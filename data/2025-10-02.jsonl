{"id": "2510.00259", "categories": ["cs.MA", "cs.AI", "cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.00259", "abs": "https://arxiv.org/abs/2510.00259", "authors": ["Ethan Herron", "Xian Yeow Lee", "Gregory Sin", "Teresa Gonzalez Diaz", "Ahmed Farahat", "Chetan Gupta"], "title": "A Hierarchical Agentic Framework for Autonomous Drone-Based Visual Inspection", "comment": null, "summary": "Autonomous inspection systems are essential for ensuring the performance and\nlongevity of industrial assets. Recently, agentic frameworks have demonstrated\nsignificant potential for automating inspection workflows but have been limited\nto digital tasks. Their application to physical assets in real-world\nenvironments, however, remains underexplored. In this work, our contributions\nare two-fold: first, we propose a hierarchical agentic framework for autonomous\ndrone control, and second, a reasoning methodology for individual function\nexecutions which we refer to as ReActEval. Our framework focuses on visual\ninspection tasks in indoor industrial settings, such as interpreting industrial\nreadouts or inspecting equipment. It employs a multi-agent system comprising a\nhead agent and multiple worker agents, each controlling a single drone. The\nhead agent performs high-level planning and evaluates outcomes, while worker\nagents implement ReActEval to reason over and execute low-level actions.\nOperating entirely in natural language, ReActEval follows a plan, reason, act,\nevaluate cycle, enabling drones to handle tasks ranging from simple navigation\n(e.g., flying forward 10 meters and land) to complex high-level tasks (e.g.,\nlocating and reading a pressure gauge). The evaluation phase serves as a\nfeedback and/or replanning stage, ensuring actions align with user objectives\nwhile preventing undesirable outcomes. We evaluate the framework in a simulated\nenvironment with two worker agents, assessing performance qualitatively and\nquantitatively based on task completion across varying complexity levels and\nworkflow efficiency. By leveraging natural language processing for agent\ncommunication, our approach offers a novel, flexible, and user-accessible\nalternative to traditional drone-based solutions, enabling autonomous\nproblem-solving for industrial inspection without extensive user intervention."}
{"id": "2510.00326", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00326", "abs": "https://arxiv.org/abs/2510.00326", "authors": ["Hassen Dhrif"], "title": "Reasoning-Aware Prompt Orchestration: A Foundation Model for Multi-Agent Language Model Coordination", "comment": null, "summary": "The emergence of large language models has enabled sophisticated multi-agent\nsystems, yet coordinating their reasoning capabilities through prompt\nengineering remains challenging. We present a theoretically-grounded framework\nfor dynamic prompt orchestration that enhances reasoning across multiple\nspecialized agents. This framework addresses three core challenges: logical\nconsistency preservation during agent transitions, reasoning-aware prompt\nadaptation, and scalable coordination of distributed inference.\n  Our approach formalizes agent states using prompt templates, reasoning\ncontext vectors, and capability matrices. We prove system convergence to stable\ncoordination patterns when step sizes satisfy $\\alpha < \\frac{1}{2L}$ where $L$\nis the Lipschitz constant of the state transition function. We implement this\nthrough a distributed architecture that dynamically routes reasoning tasks\nwhile maintaining semantic coherence.\n  Experimental results on 1,000 synthetic multi-agent conversations demonstrate\na 42% reduction in reasoning latency, a 23% improvement in logical consistency\nmeasured by ROUGE-L score, and an 89% success rate for task completion without\ncontext loss across agent transitions. Ablation studies identify the consensus\nmechanism as the primary performance driver, while revealing limitations:\nperformance degrades beyond 10 agent transitions, and the system requires\n76.5GB memory for 1,000 concurrent agents. These findings establish a new\nparadigm for scalable reasoning in multi-agent systems, providing theoretical\nfoundations for understanding reasoning emergence across coordinated language\nmodels."}
{"id": "2510.00425", "categories": ["cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.00425", "abs": "https://arxiv.org/abs/2510.00425", "authors": ["Rishi Veerapaneni", "Alvin Tang", "Haodong He", "Sophia Zhao", "Viraj Shah", "Yidai Cen", "Ziteng Ji", "Gabriel Olin", "Jon Arrizabalaga", "Yorai Shaoul", "Jiaoyang Li", "Maxim Likhachev"], "title": "Conflict-Based Search as a Protocol: A Multi-Agent Motion Planning Protocol for Heterogeneous Agents, Solvers, and Independent Tasks", "comment": "Project webpage: https://rishi-v.github.io/CBS-Protocol/", "summary": "Imagine the future construction site, hospital, office, or even sophisticated\nhousehold with dozens of robots bought from different manufacturers. How can we\nenable these different systems to effectively move in a shared environment,\ngiven that each robot may have its own independent motion planning system? This\nwork shows how we can get efficient collision-free movements between\nalgorithmically heterogeneous agents by using Conflict-Based Search (Sharon et\nal. 2015) as a protocol. At its core, the CBS Protocol requires one specific\nsingle-agent motion planning API; finding a collision-free path that satisfies\ncertain space-time constraints. Given such an API, CBS uses a central planner\nto find collision-free paths - independent of how the API is implemented. We\nshow how this protocol enables multi-agent motion planning for a heterogeneous\nteam of agents completing independent tasks with a variety of single-agent\nplanners including: Heuristic Search (e.g., A*), Sampling Based Search (e.g.,\nRRT), Optimization (e.g., Direct Collocation), Diffusion, and Reinforcement\nLearning."}
{"id": "2510.00002", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.00002", "abs": "https://arxiv.org/abs/2510.00002", "authors": ["Dong Liu"], "title": "PBFD and PDFD: Formally Defined and Verified Methodologies and Empirical Evaluation for Scalable Full-Stack Software Engineering", "comment": "184 pages; 35 figures; A DOI-linked version of this paper and all\n  supplementary materials are available on Zenodo at\n  https://zenodo.org/records/16883985", "summary": "This paper introduces Primary Breadth-First Development (PBFD) and Primary\nDepth-First Development (PDFD), two formally defined and verified methodologies\nfor scalable, industrial-grade full-stack software engineering. These\napproaches bridge a longstanding gap between formal methods and real-world\ndevelopment practice by enforcing structural correctness through\ngraph-theoretic modeling. Unlike prior graph-based approaches, PBFD and PDFD\noperate over layered directed graphs and are formalized using unified state\nmachines and Communicating Sequential Processes (CSP) to ensure critical\nproperties, including bounded-refinement termination and structural\ncompleteness. To coordinate hierarchical data at scale, we propose Three-Level\nEncapsulation (TLE) - a novel, bitmask-based encoding scheme that delivers\nprovably constant-time updates. TLE's formal guarantees underpin PBFD's\nindustrial-scale performance and scalability. PBFD was empirically validated\nthrough an eight-year enterprise deployment, demonstrating over 20x faster\ndevelopment than Salesforce OmniScript and 7-8x faster query performance\ncompared to conventional relational models. Additionally, both methodologies\nare supported by open-source MVPs, with PDFD's implementation conclusively\ndemonstrating its correctness-first design principles. Together, PBFD and PDFD\nestablish a reproducible, transparent framework that integrates formal\nverification into practical software development. All formal specifications,\nMVPs, and datasets are publicly available to foster academic research and\nindustrial-grade adoption."}
{"id": "2510.00685", "categories": ["cs.MA", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.00685", "abs": "https://arxiv.org/abs/2510.00685", "authors": ["Nurbek Tastan", "Samuel Horvath", "Karthik Nandakumar"], "title": "Stochastic Self-Organization in Multi-Agent Systems", "comment": null, "summary": "Multi-agent systems (MAS) based on Large Language Models (LLMs) have the\npotential to solve tasks that are beyond the reach of any single LLM. However,\nthis potential can only be realized when the collaboration mechanism between\nagents is optimized. Specifically, optimizing the communication structure\nbetween agents is critical for fruitful collaboration. Most existing approaches\nrely on fixed topologies, pretrained graph generators, optimization over edges,\nor employ external LLM judges, thereby adding to the complexity. In this work,\nwe introduce a response-conditioned framework that adapts communication\non-the-fly. Agents independently generate responses to the user query and\nassess peer contributions using an approximation of the Shapley value. A\ndirected acyclic graph (DAG) is then constructed to regulate the propagation of\nthe responses among agents, which ensures stable and efficient message\ntransmission from high-contributing agents to others. This graph is dynamically\nupdated based on the agent responses from the previous collaboration round.\nSince the proposed framework enables the self-organization of agents without\nadditional supervision or training, we refer to it as SelfOrg. The SelfOrg\nframework goes beyond task- and query-level optimization and takes into account\nthe stochastic nature of agent responses. Experiments with both strong and weak\nLLM backends demonstrate robust performance, with significant gains in the weak\nregime where prior methods collapse. We also theoretically show that multiple\nagents increase the chance of correctness and that the correct responses\nnaturally dominate the information flow."}
{"id": "2510.00003", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.00003", "abs": "https://arxiv.org/abs/2510.00003", "authors": ["Malte Hansen", "Jens Bamberg", "Noe Baumann", "Wilhelm Hasselbring"], "title": "Semantic Zoom and Mini-Maps for Software Cities", "comment": "Copyright 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Software visualization tools can facilitate program comprehension by\nproviding visual metaphors, or abstractions that reduce the amount of textual\ndata that needs to be processed mentally. One way they do this is by enabling\ndevelopers to build an internal representation of the visualized software and\nits architecture. However, as the amount of displayed data in the visualization\nincreases, the visualization itself can become more difficult to comprehend.\nThe ability to display small and large amounts of data in visualizations is\ncalled visual scalability.\n  In this paper, we present two approaches to address the challenge of visual\nscalability in 3D software cities. First, we present an approach to semantic\nzoom, in which the graphical representation of the software landscape changes\nbased on the virtual camera's distance from visual objects. Second, we augment\nthe visualization with a miniature two-dimensional top-view projection called\nmini-map. We demonstrate our approach using an open-source implementation in\nour software visualization tool ExplorViz. ExplorViz is web-based and uses the\n3D city metaphor, focusing on live trace visualization.\n  We evaluated our approaches in two separate user studies. The results\nindicate that semantic zoom and the mini-map are both useful additions. User\nfeedback indicates that semantic zoom and mini-maps are especially useful for\nlarge software landscapes and collaborative software exploration. The studies\nindicate a good usability of our implemented approaches. However, some\nshortcomings in our implementations have also been discovered, to be addressed\nin future work.\n  Video URL: https://youtu.be/LYtUeWvizjU"}
{"id": "2510.00125", "categories": ["cs.CL", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.00125", "abs": "https://arxiv.org/abs/2510.00125", "authors": ["Hong kyu Lee", "Ruixuan Liu", "Li Xiong"], "title": "Direct Token Optimization: A Self-contained Approach to Large Language Model Unlearning", "comment": null, "summary": "Machine unlearning is an emerging technique that removes the influence of a\nsubset of training data (forget set) from a model without full retraining, with\napplications including privacy protection, content moderation, and model\ncorrection. The key challenge lies in ensuring that the model completely\nforgets the knowledge of the forget set without compromising its overall\nutility. Existing unlearning methods for large language models (LLMs) often\nutilize auxiliary language models, retain datasets, or even commercial AI\nservices for effective unlearning and maintaining the model utility. However,\ndependence on these external resources is often impractical and could\npotentially introduce additional privacy risks. In this work, we propose direct\ntoken optimization (DTO), a novel self-contained unlearning approach for LLMs\nthat directly optimizes the token level objectives and eliminates the need for\nexternal resources. Given a sequence to unlearn, we identify two categories of\ntokens: target tokens, which capture critical knowledge for unlearning, and the\nremaining non-target tokens, which are crucial for maintaining the model\nutility. The former are used to optimize the unlearning objective, while the\nlatter serve to preserve the model's performance. The experimental results show\nthat the proposed DTO achieves up to 16.8$\\times$ improvement in forget quality\non several benchmark datasets than the latest baselines while maintaining a\ncomparable level of model utility."}
{"id": "2510.01144", "categories": ["cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.01144", "abs": "https://arxiv.org/abs/2510.01144", "authors": ["Haejoon Lee", "Dimitra Panagou"], "title": "Partial Resilient Leader-Follower Consensus in Time-Varying Graphs", "comment": "8 pages, 3 figures, Submitted to American Control Conference (ACC)\n  2026", "summary": "This work studies resilient leader-follower consensus with a bounded number\nof adversaries. Existing approaches typically require robustness conditions of\nthe entire network to guarantee resilient consensus. However, the behavior of\nsuch systems when these conditions are not fully met remains unexplored. To\naddress this gap, we introduce the notion of partial leader-follower consensus,\nin which a subset of non-adversarial followers successfully tracks the leader's\nreference state despite insufficient robustness. We propose a novel distributed\nalgorithm - the Bootstrap Percolation and Mean Subsequence Reduced (BP-MSR)\nalgorithm - and establish sufficient conditions for individual followers to\nachieve consensus via the BP-MSR algorithm in arbitrary time-varying graphs. We\nvalidate our findings through simulations, demonstrating that our method\nguarantees partial leader-follower consensus, even when standard resilient\nconsensus algorithms fail."}
{"id": "2510.00004", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.00004", "abs": "https://arxiv.org/abs/2510.00004", "authors": ["Malte Hansen", "David Moreno-Lumbreras", "Wilhelm Hasselbring"], "title": "HTML Structure Exploration in 3D Software Cities", "comment": "Copyright 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Software visualization, which uses data from dynamic program analysis, can\nhelp to explore and understand the behavior of software systems. It is common\nthat large software systems offer a web interface for user interaction.\nUsually, available web interfaces are not regarded in software visualization\ntools. This paper introduces additions to the web-based live tracing software\nvisualization tool ExplorViz: We add an embedded web view for instrumented\napplications in the 3D visualization to ease interaction with the given\napplications and enable the exploration of the thereby displayed HTML content.\nNamely, the Document Object Model (DOM) is visualized via a three-dimensional\nrepresentation of the HTML structure in same-origin contexts.\n  Our visualization approach is evaluated in a preliminary user study. The\nstudy results give insights into the potential use cases, benefits, and\nshortcomings of our implemented approach. Based on our study results, we\npropose directions for further research to support the visual exploration of\nweb interfaces and explore use cases for the combined visualization of software\ncities and HTML structure.\n  Video URL: https://youtu.be/wBWKlbvzOOE"}
{"id": "2510.00161", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.00161", "abs": "https://arxiv.org/abs/2510.00161", "authors": ["Kimihiro Hasegawa", "Wiradee Imrattanatrai", "Masaki Asada", "Ken Fukuda", "Teruko Mitamura"], "title": "TAMA: Tool-Augmented Multimodal Agent for Procedural Activity Understanding", "comment": "21 pages. Code: https://github.com/kimihiroh/tama", "summary": "Procedural activity assistants potentially support humans in a variety of\nsettings, from our daily lives, e.g., cooking or assembling flat-pack\nfurniture, to professional situations, e.g., manufacturing or biological\nexperiments. Despite its potential use cases, the system development tailored\nfor such an assistant is still underexplored. In this paper, we propose a novel\nframework, called TAMA, a Tool-Augmented Multimodal Agent, for procedural\nactivity understanding. TAMA enables interleaved multimodal reasoning by making\nuse of multimedia-returning tools in a training-free setting. Our experimental\nresult on the multimodal procedural QA dataset, ProMQA-Assembly, shows that our\napproach can improve the performance of vision-language models, especially\nGPT-5 and MiMo-VL. Furthermore, our ablation studies provide empirical support\nfor the effectiveness of two features that characterize our framework,\nmultimedia-returning tools and agentic flexible tool selection. We believe our\nproposed framework and experimental results facilitate the thinking with images\nparadigm for video and multimodal tasks, let alone the development of\nprocedural activity assistants."}
{"id": "2510.00031", "categories": ["cs.SE", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.00031", "abs": "https://arxiv.org/abs/2510.00031", "authors": ["Shun-ichiro Hayashi", "Koki Morita", "Daichi Mukunoki", "Tetsuya Hoshino", "Takahiro Katagiri"], "title": "VibeCodeHPC: An Agent-Based Iterative Prompting Auto-Tuner for HPC Code Generation Using LLMs", "comment": null, "summary": "We propose VibeCodeHPC, an automatic tuning system for HPC programs based on\nmulti-agent LLMs for code generation. VibeCodeHPC tunes programs through\nmulti-agent role allocation and iterative prompt refinement. We describe the\nsystem configuration with four roles: Project Manager (PM), System Engineer\n(SE), Programmer (PG), and Continuous Delivery (CD). We introduce dynamic agent\ndeployment and activity monitoring functions to facilitate effective\nmulti-agent collaboration. In our case study, we convert and optimize CPU-based\nmatrix-matrix multiplication code written in C to GPU code using CUDA. The\nmulti-agent configuration of VibeCodeHPC achieved higher-quality code\ngeneration per unit time compared to a solo-agent configuration. Additionally,\nthe dynamic agent deployment and activity monitoring capabilities facilitated\nmore effective identification of requirement violations and other issues."}
{"id": "2510.00172", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.00172", "abs": "https://arxiv.org/abs/2510.00172", "authors": ["Amirhossein Abaskohi", "Tianyi Chen", "Miguel Muñoz-Mármol", "Curtis Fox", "Amrutha Varshini Ramesh", "Étienne Marcotte", "Xing Han Lù", "Nicolas Chapados", "Spandana Gella", "Christopher Pal", "Alexandre Drouin", "Issam H. Laradji"], "title": "DRBench: A Realistic Benchmark for Enterprise Deep Research", "comment": null, "summary": "We introduce DRBench, a benchmark for evaluating AI agents on complex,\nopen-ended deep research tasks in enterprise settings. Unlike prior benchmarks\nthat focus on simple questions or web-only queries, DRBench evaluates agents on\nmulti-step queries (for example, ``What changes should we make to our product\nroadmap to ensure compliance with this standard?\") that require identifying\nsupporting facts from both the public web and private company knowledge base.\nEach task is grounded in realistic user personas and enterprise context,\nspanning a heterogeneous search space that includes productivity software,\ncloud file systems, emails, chat conversations, and the open web. Tasks are\ngenerated through a carefully designed synthesis pipeline with\nhuman-in-the-loop verification, and agents are evaluated on their ability to\nrecall relevant insights, maintain factual accuracy, and produce coherent,\nwell-structured reports. We release 15 deep research tasks across 10 domains,\nsuch as Sales, Cybersecurity, and Compliance. We demonstrate the effectiveness\nof DRBench by evaluating diverse DR agents across open- and closed-source\nmodels (such as GPT, Llama, and Qwen) and DR strategies, highlighting their\nstrengths, weaknesses, and the critical path for advancing enterprise deep\nresearch. Code is available at https://github.com/ServiceNow/drbench."}
{"id": "2510.00092", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.00092", "abs": "https://arxiv.org/abs/2510.00092", "authors": ["Shufeng Chen", "Mariat James Elizebeth", "Robab Aghazadeh Chakherlou", "Xingyu Zhao", "Eric Barbier", "Siddartha Khastgir", "Paul Jennings"], "title": "A Scalable Framework for Safety Assurance of Self-Driving Vehicles based on Assurance 2.0", "comment": null, "summary": "Assurance 2.0 is a modern framework developed to address the assurance\nchallenges of increasingly complex, adaptive, and autonomous systems. Building\non the traditional Claims-Argument-Evidence (CAE) model, it introduces reusable\nassurance theories and explicit counterarguments (defeaters) to enhance rigor,\ntransparency, and adaptability. It supports continuous, incremental assurance,\nenabling innovation without compromising safety. However, limitations persist\nin confidence measurement, residual doubt management, automation support, and\nthe practical handling of defeaters and confirmation bias. This paper presents\n\\textcolor{black}{a set of decomposition frameworks to identify a complete set\nof safety arguments and measure their corresponding evidence.} Grounded in the\nAssurance 2.0 paradigm, the framework is instantiated through a structured\ntemplate and employs a three-tiered decomposition strategy. \\textcolor{black}{A\ncase study regarding the application of the decomposition framework in the\nend-to-end (E2E) AI-based Self-Driving Vehicle (SDV) development is also\npresented in this paper.} At the top level, the SDV development is divided into\nthree critical phases: Requirements Engineering (RE), Verification and\nValidation (VnV), and Post-Deployment (PD). Each phase is further decomposed\naccording to its Product Development Lifecycle (PDLC). To ensure comprehensive\ncoverage, each PDLC is analyzed using an adapted 5M1E model (Man, Machine,\nMethod, Material, Measurement, and Environment). Originally developed for\nmanufacturing quality control, the 5M1E model is reinterpreted and contextually\nmapped to the assurance domain. This enables a multi-dimensional decomposition\nthat supports fine-grained traceability of safety claims, evidence, and\npotential defeaters."}
{"id": "2510.00174", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.00174", "abs": "https://arxiv.org/abs/2510.00174", "authors": ["Rik Koncel-Kedziorski", "Brihi Joshi", "Tim Paek"], "title": "PrimeX: A Dataset of Worldview, Opinion, and Explanation", "comment": "EMNLP 2025 Main", "summary": "As the adoption of language models advances, so does the need to better\nrepresent individual users to the model. Are there aspects of an individual's\nbelief system that a language model can utilize for improved alignment?\nFollowing prior research, we investigate this question in the domain of opinion\nprediction by developing PrimeX, a dataset of public opinion survey data from\n858 US residents with two additional sources of belief information: written\nexplanations from the respondents for why they hold specific opinions, and the\nPrimal World Belief survey for assessing respondent worldview. We provide an\nextensive initial analysis of our data and show the value of belief\nexplanations and worldview for personalizing language models. Our results\ndemonstrate how the additional belief information in PrimeX can benefit both\nthe NLP and psychological research communities, opening up avenues for further\nstudy."}
{"id": "2510.00197", "categories": ["cs.SE", "D.2.11"], "pdf": "https://arxiv.org/pdf/2510.00197", "abs": "https://arxiv.org/abs/2510.00197", "authors": ["Diogo Maia", "Filipe Correia", "André Restivo", "Paulo Queiroz"], "title": "Container Orchestration Patterns for Optimizing Resource Use", "comment": null, "summary": "Service-based architectures provide substantial benefits, yet service\norchestration remains a challenge, particularly for newcomers. While various\nresources on orchestration techniques exist, they often lack clarity and\nstandardization, making best practices difficult to implement and limiting\ntheir adoption within the software industry.\n  To address this gap, we analyzed existing literature and tools to identify\ncommon orchestration practices. Based on our findings, we define three key\norchestration resource optimization patterns: {\\sc Preemptive Scheduling}, {\\sc\nService Balancing}, and {\\sc Garbage Collection}. {\\sc Preemptive Scheduling}\nallows the allocation of sufficient resources for services of higher priority\nin stressful situations, while {\\sc Service Balancing} enables a restructuring\nof the nodes to allow better resource usage. To end, {\\sc Garbage Collection}\ncreates cleanup mechanisms to better understand the system's resource usage and\noptimize it. These patterns serve as foundational elements for improving\norchestration practices and fostering broader adoption in service-based\narchitectures."}
{"id": "2510.00177", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00177", "abs": "https://arxiv.org/abs/2510.00177", "authors": ["Shuyue Stella Li", "Avinandan Bose", "Faeze Brahman", "Simon Shaolei Du", "Pang Wei Koh", "Maryam Fazel", "Yulia Tsvetkov"], "title": "Personalized Reasoning: Just-In-Time Personalization and Why LLMs Fail At It", "comment": "57 pages, 6 figures", "summary": "Current large language model (LLM) development treats task-solving and\npreference alignment as separate challenges, optimizing first for objective\ncorrectness, then for alignment to aggregated human preferences. This paradigm\nfails in human-facing applications where solving a problem correctly is\ninsufficient if the response mismatches the user's needs. This challenge\nintensifies in just-in-time scenarios where no prior user interaction history\nexists due to cold-start conditions or privacy constraints. LLMs need to\nidentify what they don't know about user preferences, strategically elicit\npreference values through questioning, then adapt their reasoning processes and\nresponses accordingly -- a complicated chain of cognitive processes which we\nterm personalized reasoning. We introduce PREFDISCO, an evaluation methodology\nthat transforms static benchmarks into interactive personalization tasks using\npsychologically-grounded personas with sparse preferences. Our framework\ncreates scenarios where identical questions require different reasoning chains\ndepending on user context, as optimal explanation approaches vary by individual\nexpertise and preferences while maintaining factual accuracy. Evaluation of 21\nfrontier models across 10 tasks reveals 29.0% of naive personalization attempts\nproduce worse preference alignment than generic responses, yet generic\nresponses also fail to serve individual user needs effectively. These findings\nsuggest personalized reasoning requires dedicated development rather than\nemerging naturally. PREFDISCO establishes personalized reasoning as a\nmeasurable research frontier and reveals fundamental limitations in current\nLLMs' interactive capabilities, providing a foundation for developing systems\nthat can adapt to individual users in education, healthcare, and technical\ndomains where personalization is critical."}
{"id": "2510.00324", "categories": ["cs.SE", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.00324", "abs": "https://arxiv.org/abs/2510.00324", "authors": ["Lucas Roberts", "Denisa Roberts"], "title": "Which Programming Language and Model Work Best With LLM-as-a-Judge For Code Retrieval?", "comment": "Accepted as a full paper at SIGIR-AP 2025", "summary": "Code search is an important information retrieval application. Benefits of\nbetter code search include faster new developer on-boarding, reduced software\nmaintenance, and ease of understanding for large repositories. Despite\nimprovements in search algorithms and search benchmarks, the domain of code\nsearch has lagged behind. One reason is the high cost of human annotation for\ncode queries and answers. While humans may annotate search results in general\ntext QA systems, code annotations require specialized knowledge of a\nprogramming language (PL), as well as domain specific software engineering\nknowledge. In this work we study the use of Large Language Models (LLMs) to\nretrieve code at the level of functions and to generate annotations for code\nsearch results. We compare the impact of the retriever representation (sparse\nvs. semantic), programming language, and LLM by comparing human annotations\nacross several popular languages (C, Java, Javascript, Go, and Python). We\nfocus on repositories that implement common data structures likely to be\nimplemented in any PLs. For the same human annotations, we compare several\nLLM-as-a-Judge models to evaluate programming language and other affinities\nbetween LLMs. We find that the chosen retriever and PL exhibit affinities that\ncan be leveraged to improve alignment of human and AI relevance determinations,\nwith significant performance implications. We also find differences in\nrepresentation (sparse vs. semantic) across PLs that impact alignment of human\nand AI relevance determinations. We propose using transpilers to bootstrap\nscalable code search benchmark datasets in other PLs and in a case study\ndemonstrate that human-AI relevance agreement rates largely match the (worst\ncase) human-human agreement under study. The application code used in this work\nis available at \\href{https://github.com/rlucas7/code-searcher/}{this github\nrepo}."}
{"id": "2510.00232", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.00232", "abs": "https://arxiv.org/abs/2510.00232", "authors": ["Xin Xu", "Xunzhi He", "Churan Zhi", "Ruizhe Chen", "Julian McAuley", "Zexue He"], "title": "BiasFreeBench: a Benchmark for Mitigating Bias in Large Language Model Responses", "comment": "Work in progress", "summary": "Existing studies on bias mitigation methods for large language models (LLMs)\nuse diverse baselines and metrics to evaluate debiasing performance, leading to\ninconsistent comparisons among them. Moreover, their evaluations are mostly\nbased on the comparison between LLMs' probabilities of biased and unbiased\ncontexts, which ignores the gap between such evaluations and real-world use\ncases where users interact with LLMs by reading model responses and expect fair\nand safe outputs rather than LLMs' probabilities. To enable consistent\nevaluation across debiasing methods and bridge this gap, we introduce\nBiasFreeBench, an empirical benchmark that comprehensively compares eight\nmainstream bias mitigation techniques (covering four prompting-based and four\ntraining-based methods) on two test scenarios (multi-choice QA and open-ended\nmulti-turn QA) by reorganizing existing datasets into a unified query-response\nsetting. We further introduce a response-level metric, Bias-Free Score, to\nmeasure the extent to which LLM responses are fair, safe, and\nanti-stereotypical. Debiasing performances are systematically compared and\nanalyzed across key dimensions: the prompting vs. training paradigm, model\nsize, and generalization of different training strategies to unseen bias types.\nWe will publicly release our benchmark, aiming to establish a unified testbed\nfor bias mitigation research."}
{"id": "2510.00328", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.00328", "abs": "https://arxiv.org/abs/2510.00328", "authors": ["Ahmed Fawzy", "Amjed Tahir", "Kelly Blincoe"], "title": "Vibe Coding in Practice: Motivations, Challenges, and a Future Outlook -- a Grey Literature Review", "comment": null, "summary": "AI code generation tools are transforming software development, especially\nfor novice and non-software developers, by enabling them to write code and\nbuild applications faster and with little to no human intervention. Vibe coding\nis the practice where users rely on AI code generation tools through intuition\nand trial-and-error without necessarily understanding the underlying code.\nDespite widespread adoption, no research has systematically investigated why\nusers engage in vibe coding, what they experience while doing so, and how they\napproach quality assurance (QA) and perceive the quality of the AI-generated\ncode. To this end, we conduct a systematic grey literature review of 101\npractitioner sources, extracting 518 firsthand behavioral accounts about vibe\ncoding practices, challenges, and limitations. Our analysis reveals a\nspeed-quality trade-off paradox, where vibe coders are motivated by speed and\naccessibility, often experiencing rapid ``instant success and flow'', yet most\nperceive the resulting code as fast but flawed. QA practices are frequently\noverlooked, with many skipping testing, relying on the models' or tools'\noutputs without modification, or delegating checks back to the AI code\ngeneration tools. This creates a new class of vulnerable software developers,\nparticularly those who build a product but are unable to debug it when issues\narise. We argue that vibe coding lowers barriers and accelerates prototyping,\nbut at the cost of reliability and maintainability. These insights carry\nimplications for tool designers and software development teams. Understanding\nhow vibe coding is practiced today is crucial for guiding its responsible use\nand preventing a broader QA crisis in AI-assisted development."}
{"id": "2510.00255", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00255", "abs": "https://arxiv.org/abs/2510.00255", "authors": ["Monishwaran Maheswaran", "Marco Carini", "Christian Federmann", "Tony Diaz"], "title": "TASER: Translation Assessment via Systematic Evaluation and Reasoning", "comment": null, "summary": "We introduce TASER (Translation Assessment via Systematic Evaluation and\nReasoning), a metric that uses Large Reasoning Models (LRMs) for automated\ntranslation quality assessment. TASER harnesses the explicit reasoning\ncapabilities of LRMs to conduct systematic, step-by-step evaluation of\ntranslation quality. We evaluate TASER on the WMT24 Metrics Shared Task across\nboth reference-based and reference-free scenarios, demonstrating\nstate-of-the-art performance. In system-level evaluation, TASER achieves the\nhighest soft pairwise accuracy in both reference-based and reference-free\nsettings, outperforming all existing metrics. At the segment level, TASER\nmaintains competitive performance with our reference-free variant ranking as\nthe top-performing metric among all reference-free approaches. Our experiments\nreveal that structured prompting templates yield superior results with LRMs\ncompared to the open-ended approaches that proved optimal for traditional LLMs.\nWe evaluate o3, a large reasoning model from OpenAI, with varying reasoning\nefforts, providing insights into the relationship between reasoning depth and\nevaluation quality. The explicit reasoning process in LRMs offers\ninterpretability and visibility, addressing a key limitation of existing\nautomated metrics. Our results demonstrate that Large Reasoning Models show a\nmeasurable advancement in translation quality assessment, combining improved\naccuracy with transparent evaluation across diverse language pairs."}
{"id": "2510.00450", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.00450", "abs": "https://arxiv.org/abs/2510.00450", "authors": ["Sheikh Md. Mushfiqur Rahman", "Nasir Eisty"], "title": "Beyond Pass/Fail: The Story of Learning-Based Testing", "comment": null, "summary": "Learning-Based Testing (LBT) merges learning and testing processes to achieve\nboth testing and behavioral adequacy. LBT utilizes active learning to infer the\nmodel of the System Under Test (SUT), enabling scalability for large and\ncomplex programs by requiring only a minimal set of initial test cases. The\ncore principle of LBT is that the SUT's behavior can be thoroughly inferred by\nprogressively generating test cases and subjecting the SUT to testing, thereby\nensuring comprehensive testing. Despite being in its early stages, LBT has a\nsolid foundation of theoretical research demonstrating its efficacy in testing\nboth procedural and reactive programs. This paper provides a systematic\nliterature review of various LBT implementations across different program types\nand evaluates the current state of research in this field. We explore diverse\ntheoretical frameworks, existing tools, and libraries within the LBT domain to\nillustrate the concept's evolution and current research status. Additionally,\nwe examine case studies involving the application of LBT tools in industrial\nsettings, highlighting their potential and effectiveness in commercial software\ntesting. This systematic literature review aims to offer researchers a\ncomprehensive perspective on the inception and development of LBT, presenting\nit as a promising technique in software testing. By unveiling LBT's\nunderutilized potential, this paper seeks to significantly benefit the\npractitioners and research community."}
{"id": "2510.00261", "categories": ["cs.CL", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.00261", "abs": "https://arxiv.org/abs/2510.00261", "authors": ["Xiaoyu Song", "William Han", "Tony Chen", "Chaojing Duan", "Michael A. Rosenberg", "Emerson Liu", "Ding Zhao"], "title": "Retrieval-Augmented Generation for Electrocardiogram-Language Models", "comment": "5 pages, 2 figures; Submitted to ICASSP 2026", "summary": "Interest in generative Electrocardiogram-Language Models (ELMs) is growing,\nas they can produce textual responses conditioned on ECG signals and textual\nqueries. Unlike traditional classifiers that output label probabilities, ELMs\nare more versatile, supporting domain-specific tasks (e.g., waveform analysis,\ndiagnosis, prognosis) as well as general tasks (e.g., open-ended questions,\ndialogue). Retrieval-Augmented Generation (RAG), widely used in Large Language\nModels (LLMs) to ground LLM outputs in retrieved knowledge, helps reduce\nhallucinations and improve natural language generation (NLG). However, despite\nits promise, no open-source implementation or systematic study of RAG pipeline\ndesign for ELMs currently exists. To address this gap, we present the first\nopen-source RAG pipeline for ELMs, along with baselines and ablation studies\nfor NLG. Experiments on three public datasets show that ELMs with RAG\nconsistently improves performance over non-RAG baselines and highlights key ELM\ndesign considerations. Our code is available at:\nhttps://github.com/willxxy/ECG-Bench."}
{"id": "2510.00476", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.00476", "abs": "https://arxiv.org/abs/2510.00476", "authors": ["Arushi Sharma", "Vedant Pungliya", "Christopher J. Quinn", "Ali Jannesari"], "title": "Analyzing Latent Concepts in Code Language Models", "comment": null, "summary": "Interpreting the internal behavior of large language models trained on code\nremains a critical challenge, particularly for applications demanding trust,\ntransparency, and semantic robustness. We propose Code Concept Analysis\n(CoCoA): a global post-hoc interpretability framework that uncovers emergent\nlexical, syntactic, and semantic structures in a code language model's\nrepresentation space by clustering contextualized token embeddings into\nhuman-interpretable concept groups. We propose a hybrid annotation pipeline\nthat combines static analysis tool-based syntactic alignment with\nprompt-engineered large language models (LLMs), enabling scalable labeling of\nlatent concepts across abstraction levels. We analyse the distribution of\nconcepts across layers and across three finetuning tasks. Emergent concept\nclusters can help identify unexpected latent interactions and be used to\nidentify trends and biases within the model's learned representations. We\nfurther integrate LCA with local attribution methods to produce\nconcept-grounded explanations, improving the coherence and interpretability of\ntoken-level saliency. Empirical evaluations across multiple models and tasks\nshow that LCA discovers concepts that remain stable under semantic-preserving\nperturbations (average Cluster Sensitivity Index, CSI = 0.288) and evolve\npredictably with fine-tuning. In a user study, concept-augmented explanations\ndisambiguate token roles. In a user study on the programming-language\nclassification task, concept-augmented explanations disambiguated token roles\nand improved human-centric explainability by 37 percentage points compared with\ntoken-level attributions using Integrated Gradients."}
{"id": "2510.00263", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.00263", "abs": "https://arxiv.org/abs/2510.00263", "authors": ["Zhuohang Li", "Xiaowei Li", "Chengyu Huang", "Guowang Li", "Katayoon Goshvadi", "Bo Dai", "Dale Schuurmans", "Paul Zhou", "Hamid Palangi", "Yiwen Song", "Palash Goyal", "Murat Kantarcioglu", "Bradley A. Malin", "Yuan Xue"], "title": "Judging with Confidence: Calibrating Autoraters to Preference Distributions", "comment": null, "summary": "The alignment of large language models (LLMs) with human values increasingly\nrelies on using other LLMs as automated judges, or ``autoraters''. However,\ntheir reliability is limited by a foundational issue: they are trained on\ndiscrete preference labels, forcing a single ground truth onto tasks that are\noften subjective, ambiguous, or nuanced. We argue that a reliable autorater\nmust learn to model the full distribution of preferences defined by a target\npopulation. In this paper, we propose a general framework for calibrating\nprobabilistic autoraters to any given preference distribution. We formalize the\nproblem and present two learning methods tailored to different data conditions:\n1) a direct supervised fine-tuning for dense, probabilistic labels, and 2) a\nreinforcement learning approach for sparse, binary labels. Our empirical\nresults show that finetuning autoraters with a distribution-matching objective\nleads to verbalized probability predictions that are better aligned with the\ntarget preference distribution, with improved calibration and significantly\nlower positional bias, all while preserving performance on objective tasks."}
{"id": "2510.00501", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.00501", "abs": "https://arxiv.org/abs/2510.00501", "authors": ["Kaixin Wang", "Tianlin Li", "Xiaoyu Zhang", "Aishan Liu", "Xianglong Liu", "Ziqi Liu", "Zhiqiang Zhang", "Jun Zhou", "and Bin Shi"], "title": "CodeChemist: Functional Knowledge Transfer for Low-Resource Code Generation via Test-Time Scaling", "comment": null, "summary": "Code Large Language Models (CodeLLMs) are increasingly used in code\ngeneration tasks across a wide range of applications. However, their\nperformance is often inconsistent across different programming languages (PLs),\nwith low-resource PLs suffering the most due to limited training data. In this\npaper, we present CodeChemist, a novel and efficient framework for test-time\nscaling that enables functional knowledge transfer from high-resource to\nlow-resource PLs using generated test cases. CodeChemist first generates and\nexecutes code in high-resource PLs to create test cases that encapsulate\nfunctional knowledge. It then uses multi-temperature hedged sampling to\ngenerate code snippets in the low-resource PL and selects the best one based on\nthe pass rate of the test cases. Our extensive experiments show that\nCodeChemist outperforms existing test-time scaling approaches, boosting the\nperformance of code generation for low-resource PLs without requiring any model\nretraining."}
{"id": "2510.00268", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00268", "abs": "https://arxiv.org/abs/2510.00268", "authors": ["Zhexiong Liu", "Diane Litman"], "title": "Efficient Layer-wise LLM Fine-tuning for Revision Intention Prediction", "comment": "In The Conference on Empirical Methods in Natural Language Processing\n  (EMNLP), November 2025", "summary": "Large Language Models (LLMs) have shown extraordinary success across various\ntext generation tasks; however, their potential for simple yet essential text\nclassification remains underexplored, as LLM pre-training tends to emphasize\ngeneration over classification. While LLMs with instruction tuning can\ntransform classification into a generation task, they often struggle to\ncategorize nuanced texts. One such example is text revision, which involves\nnuanced edits between pairs of texts. Although simply fine-tuning LLMs for\nrevision classification seems plausible, it requires a large amount of revision\nannotations, which are exceptionally expensive and scarce in the community. To\naddress this issue, we introduce a plug-and-play layer-wise parameter-efficient\nfine-tuning (PEFT) framework, i.e., IR-Tuning, which fine-tunes a subset of\nimportant LLM layers that are dynamically selected based on their gradient norm\ndistribution, while freezing those of redundant layers. Extensive experiments\nsuggest that IR-Tuning surpasses several layer-wise PEFT baselines over diverse\ntext revisions, while achieving fast convergence, low GPU memory consumption,\nand effectiveness on small revision corpora."}
{"id": "2510.00519", "categories": ["cs.SE", "cs.AI", "D.2.4; D.2.11"], "pdf": "https://arxiv.org/pdf/2510.00519", "abs": "https://arxiv.org/abs/2510.00519", "authors": ["Hadiza Umar Yusuf", "Khouloud Gaaloul"], "title": "Architectural Transformations and Emerging Verification Demands in AI-Enabled Cyber-Physical Systems", "comment": null, "summary": "In the world of Cyber-Physical Systems (CPS), a captivating real-time fusion\noccurs where digital technology meets the physical world. This synergy has been\nsignificantly transformed by the integration of artificial intelligence (AI), a\nmove that dramatically enhances system adaptability and introduces a layer of\ncomplexity that impacts CPS control optimization and reliability. Despite\nadvancements in AI integration, a significant gap remains in understanding how\nthis shift affects CPS architecture, operational complexity, and verification\npractices. The extended abstract addresses this gap by investigating\narchitectural distinctions between AI-driven and traditional control models\ndesigned in Simulink and their respective implications for system verification."}
{"id": "2510.00276", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.00276", "abs": "https://arxiv.org/abs/2510.00276", "authors": ["Joe Barrow", "Raj Patel", "Misha Kharkovski", "Ben Davies", "Ryan Schmitt"], "title": "SafePassage: High-Fidelity Information Extraction with Black Box LLMs", "comment": null, "summary": "Black box large language models (LLMs) make information extraction (IE) easy\nto configure, but hard to trust. Unlike traditional information extraction\npipelines, the information \"extracted\" is not guaranteed to be grounded in the\ndocument. To prevent this, this paper introduces the notion of a \"safe\npassage\": context generated by the LLM that is both grounded in the document\nand consistent with the extracted information. This is operationalized via a\nthree-step pipeline, SafePassage, which consists of: (1) an LLM extractor that\ngenerates structured entities and their contexts from a document, (2) a\nstring-based global aligner, and (3) a scoring model. Results show that using\nthese three parts in conjunction reduces hallucinations by up to 85% on\ninformation extraction tasks with minimal risk of flagging non-hallucinations.\nHigh agreement between the SafePassage pipeline and human judgments of\nextraction quality mean that the pipeline can be dually used to evaluate LLMs.\nSurprisingly, results also show that using a transformer encoder fine-tuned on\na small number of task-specific examples can outperform an LLM scoring model at\nflagging unsafe passages. These annotations can be collected in as little as\n1-2 hours."}
{"id": "2510.00532", "categories": ["cs.SE", "cs.CR", "D.2.5"], "pdf": "https://arxiv.org/pdf/2510.00532", "abs": "https://arxiv.org/abs/2510.00532", "authors": ["Hengcheng Zhu", "Songqiang Chen", "Valerio Terragni", "Lili Wei", "Jiarong Wu", "Yepang Liu", "Shing-Chi Cheung"], "title": "LSPFuzz: Hunting Bugs in Language Servers", "comment": "This paper has been accepted for publication in The 40th IEEE/ACM\n  International Conference on Automated Software Engineering (ASE 2025)", "summary": "The Language Server Protocol (LSP) has revolutionized the integration of code\nintelligence in modern software development. There are approximately 300 LSP\nserver implementations for various languages and 50 editors offering LSP\nintegration. However, the reliability of LSP servers is a growing concern, as\ncrashes can disable all code intelligence features and significantly impact\nproductivity, while vulnerabilities can put developers at risk even when\nediting untrusted source code. Despite the widespread adoption of LSP, no\nexisting techniques specifically target LSP server testing. To bridge this gap,\nwe present LSPFuzz, a grey-box hybrid fuzzer for systematic LSP server testing.\nOur key insight is that effective LSP server testing requires holistic mutation\nof source code and editor operations, as bugs often manifest from their\ncombinations. To satisfy the sophisticated constraints of LSP and effectively\nexplore the input space, we employ a two-stage mutation pipeline: syntax-aware\nmutations to source code, followed by context-aware dispatching of editor\noperations. We evaluated LSPFuzz on four widely used LSP servers. LSPFuzz\ndemonstrated superior performance compared to baseline fuzzers, and uncovered\npreviously unknown bugs in real-world LSP servers. Of the 51 bugs we reported,\n42 have been confirmed, 26 have been fixed by developers, and two have been\nassigned CVE numbers. Our work advances the quality assurance of LSP servers,\nproviding both a practical tool and foundational insights for future research\nin this domain."}
{"id": "2510.00280", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.00280", "abs": "https://arxiv.org/abs/2510.00280", "authors": ["Ruochen Li", "Jun Li", "Bailiang Jian", "Kun Yuan", "Youxiang Zhu"], "title": "ReEvalMed: Rethinking Medical Report Evaluation by Aligning Metrics with Real-World Clinical Judgment", "comment": null, "summary": "Automatically generated radiology reports often receive high scores from\nexisting evaluation metrics but fail to earn clinicians' trust. This gap\nreveals fundamental flaws in how current metrics assess the quality of\ngenerated reports. We rethink the design and evaluation of these metrics and\npropose a clinically grounded Meta-Evaluation framework. We define clinically\ngrounded criteria spanning clinical alignment and key metric capabilities,\nincluding discrimination, robustness, and monotonicity. Using a fine-grained\ndataset of ground truth and rewritten report pairs annotated with error types,\nclinical significance labels, and explanations, we systematically evaluate\nexisting metrics and reveal their limitations in interpreting clinical\nsemantics, such as failing to distinguish clinically significant errors,\nover-penalizing harmless variations, and lacking consistency across error\nseverity levels. Our framework offers guidance for building more clinically\nreliable evaluation methods."}
{"id": "2510.00591", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00591", "abs": "https://arxiv.org/abs/2510.00591", "authors": ["Liyi Cai", "Yijie Ren", "Yitong Zhang", "Jia Li"], "title": "AI-Driven Self-Evolving Software: A Promising Path Toward Software Automation", "comment": null, "summary": "Software automation has long been a central goal of software engineering,\nstriving for software development that proceeds without human intervention.\nRecent efforts have leveraged Artificial Intelligence (AI) to advance software\nautomation with notable progress. However, current AI functions primarily as\nassistants to human developers, leaving software development still dependent on\nexplicit human intervention. This raises a fundamental question: Can AI move\nbeyond its role as an assistant to become a core component of software, thereby\nenabling genuine software automation? To investigate this vision, we introduce\nAI-Driven Self-Evolving Software, a new form of software that evolves\ncontinuously through direct interaction with users. We demonstrate the\nfeasibility of this idea with a lightweight prototype built on a multi-agent\narchitecture that autonomously interprets user requirements, generates and\nvalidates code, and integrates new functionalities. Case studies across\nmultiple representative scenarios show that the prototype can reliably\nconstruct and reuse functionality, providing early evidence that such software\nsystems can scale to more sophisticated applications and pave the way toward\ntruly automated software development. We make code and cases in this work\npublicly available at https://anonymous.4open.science/r/live-software."}
{"id": "2510.00288", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00288", "abs": "https://arxiv.org/abs/2510.00288", "authors": ["Ľuboš Kriš", "Jaroslav Kopčan", "Qiwei Peng", "Andrej Ridzik", "Marcel Veselý", "Martin Tamajka"], "title": "o-MEGA: Optimized Methods for Explanation Generation and Analysis", "comment": null, "summary": "The proliferation of transformer-based language models has revolutionized NLP\ndomain while simultaneously introduced significant challenges regarding model\ntransparency and trustworthiness. The complexity of achieving explainable\nsystems in this domain is evidenced by the extensive array of explanation\nmethods and evaluation metrics developed by researchers. To address the\nchallenge of selecting optimal explainability approaches, we present\n\\textbf{\\texttt{o-mega}}, a hyperparameter optimization tool designed to\nautomatically identify the most effective explainable AI methods and their\nconfigurations within the semantic matching domain. We evaluate o-mega on a\npost-claim matching pipeline using a curated dataset of social media posts\npaired with refuting claims. Our tool systematically explores different\nexplainable methods and their hyperparameters, demonstrating improved\ntransparency in automated fact-checking systems. As a result, such automated\noptimization of explanation methods can significantly enhance the\ninterpretability of claim-matching models in critical applications such as\nmisinformation detection, contributing to more trustworthy and transparent AI\nsystems."}
{"id": "2510.00674", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.00674", "abs": "https://arxiv.org/abs/2510.00674", "authors": ["Konstantinos Karakatsanis", "Georgios Alexopoulos", "Ioannis Karyotakis", "Foivos Timotheos Proestakis", "Evangelos Talos", "Panos Louridas", "Dimitris Mitropoulos"], "title": "PyTrim: A Practical Tool for Reducing Python Dependency Bloat", "comment": "Accepted at ASE 2025 (Tool Demonstration Track)", "summary": "Dependency bloat is a persistent challenge in Python projects, which\nincreases maintenance costs and security risks. While numerous tools exist for\ndetecting unused dependencies in Python, removing these dependencies across the\nsource code and configuration files of a project requires manual effort and\nexpertise.\n  To tackle this challenge we introduce PYTRIM, an end-to-end system to\nautomate this process. PYTRIM eliminates unused imports and package\ndeclarations across a variety of file types, including Python source and\nconfiguration files such as requirements.txt and setup.py. PYTRIM's modular\ndesign makes it agnostic to the source of dependency bloat information,\nenabling integration with any detection tool. Beyond its contribution when it\ncomes to automation, PYTRIM also incorporates a novel dynamic analysis\ncomponent that improves dependency detection recall.\n  Our evaluation of PYTRIM's end-to-end effectiveness on a ground-truth dataset\nof 37 merged pull requests from prior work, shows that PYTRIM achieves 98.3%\naccuracy in replicating human-made changes. To show its practical impact, we\nrun PYTRIM on 971 open-source packages, identifying and trimming bloated\ndependencies in 39 of them. For each case, we submit a corresponding pull\nrequest, 6 of which have already been accepted and merged. PYTRIM is available\nas an open-source project, encouraging community contributions and further\ndevelopment.\n  Video demonstration: https://youtu.be/LqTEdOUbJRI\n  Code repository: https://github.com/TrimTeam/PyTrim"}
{"id": "2510.00311", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.00311", "abs": "https://arxiv.org/abs/2510.00311", "authors": ["Bowen Wei", "Yuan Shen Tay", "Howard Liu", "Jinhao Pan", "Kun Luo", "Ziwei Zhu", "Chris Jordan"], "title": "CORTEX: Collaborative LLM Agents for High-Stakes Alert Triage", "comment": null, "summary": "Security Operations Centers (SOCs) are overwhelmed by tens of thousands of\ndaily alerts, with only a small fraction corresponding to genuine attacks. This\noverload creates alert fatigue, leading to overlooked threats and analyst\nburnout. Classical detection pipelines are brittle and context-poor, while\nrecent LLM-based approaches typically rely on a single model to interpret logs,\nretrieve context, and adjudicate alerts end-to-end -- an approach that\nstruggles with noisy enterprise data and offers limited transparency. We\npropose CORTEX, a multi-agent LLM architecture for high-stakes alert triage in\nwhich specialized agents collaborate over real evidence: a behavior-analysis\nagent inspects activity sequences, evidence-gathering agents query external\nsystems, and a reasoning agent synthesizes findings into an auditable decision.\nTo support training and evaluation, we release a dataset of fine-grained SOC\ninvestigations from production environments, capturing step-by-step analyst\nactions and linked tool outputs. Across diverse enterprise scenarios, CORTEX\nsubstantially reduces false positives and improves investigation quality over\nstate-of-the-art single-agent LLMs."}
{"id": "2510.00680", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.00680", "abs": "https://arxiv.org/abs/2510.00680", "authors": ["Hang Cui", "Jingjing Li", "Haotian Si", "Quan Zhou", "Changhua Pei", "Gaogang Xie", "Dan Pei"], "title": "TShape: Rescuing Machine Learning Models from Complex Shapelet Anomalies", "comment": null, "summary": "Time series anomaly detection (TSAD) is critical for maintaining the\nreliability of modern IT infrastructures, where complex anomalies frequently\narise in highly dynamic environments. In this paper, we present TShape, a novel\nframework designed to address the challenges in industrial time series anomaly\ndetection. Existing methods often struggle to detect shapelet anomalies that\nmanifest as complex shape deviations, which appear obvious to human experts but\nprove challenging for machine learning algorithms. TShape introduces a\npatch-wise dual attention mechanism with multi-scale convolution to model\nintricate sub-sequence variations by balancing local, fine-grained shape\nfeatures with global contextual dependencies. Our extensive evaluation on five\ndiverse benchmarks demonstrates that TShape outperforms existing\nstate-of-the-art models, achieving an average 10\\% F1 score improvement in\nanomaly detection. Additionally, ablation studies and attention visualizations\nconfirm the essential contributions of each component, highlighting the\nrobustness and adaptability of TShape to complex shapelet shapes in time series\ndata."}
{"id": "2510.00444", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.00444", "abs": "https://arxiv.org/abs/2510.00444", "authors": ["Zijun Wu", "Yongchang Hao", "Lili Mou"], "title": "TokMem: Tokenized Procedural Memory for Large Language Models", "comment": null, "summary": "Large language models rely heavily on prompts to specify tasks, recall\nknowledge and guide reasoning. However, this reliance is inefficient as prompts\nmust be re-read at each step, scale poorly across tasks, and lack mechanisms\nfor modular reuse. We introduce TokMem, a tokenized procedural memory that\nstores recurring procedures as compact, trainable embeddings. Each memory token\nencodes both an address to a procedure and a control signal that steers\ngeneration, enabling targeted behavior with constant-size overhead. To support\ncontinual adaptation, TokMem keeps the backbone model frozen, allowing new\nprocedures to be added without interfering with existing ones. We evaluate\nTokMem on 1,000 tasks for atomic recall, and on function-calling tasks for\ncompositional recall, where it consistently outperforms retrieval-augmented\ngeneration while avoiding repeated context overhead, and fine-tuning with far\nfewer parameters. These results establish TokMem as a scalable and modular\nalternative to prompt engineering and fine-tuning, offering an explicit\nprocedural memory for LLMs."}
{"id": "2510.00730", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.00730", "abs": "https://arxiv.org/abs/2510.00730", "authors": ["Larissa Schmid", "Elias Lundell", "Yogya Gamage", "Benoit Baudry", "Martin Monperrus"], "title": "Maven-Lockfile: High Integrity Rebuild of Past Java Releases", "comment": null, "summary": "Modern software projects depend on many third-party libraries, complicating\nreproducible and secure builds. Several package managers address this with the\ngeneration of a lockfile that freezes dependency versions and can be used to\nverify the integrity of dependencies. Yet, Maven, one of the most important\npackage managers in the Java ecosystem, lacks native support for a lockfile. We\npresent Maven-Lockfile to generate and update lockfiles, with support for\nrebuilding projects from past versions. Our lockfiles capture all direct and\ntransitive dependencies with their checksums, enabling high integrity builds.\nOur evaluation shows that Maven-Lockfile can reproduce builds from historical\ncommits and is able to detect tampered artifacts. With minimal configuration,\nMaven-Lockfile equips Java projects with modern build integrity and build\nreproducibility, and fosters future research on software supply chain security\nin Java."}
{"id": "2510.00446", "categories": ["cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.00446", "abs": "https://arxiv.org/abs/2510.00446", "authors": ["Yuling Shi", "Yichun Qian", "Hongyu Zhang", "Beijun Shen", "Xiaodong Gu"], "title": "LongCodeZip: Compress Long Context for Code Language Models", "comment": "Accepted to ASE 2025. Code available at\n  https://github.com/YerbaPage/LongCodeZip", "summary": "Code generation under long contexts is becoming increasingly critical as\nLarge Language Models (LLMs) are required to reason over extensive information\nin the codebase. While recent advances enable code LLMs to process long inputs,\nhigh API costs and generation latency remain substantial bottlenecks. Existing\ncontext pruning techniques, such as LLMLingua, achieve promising results for\ngeneral text but overlook code-specific structures and dependencies, leading to\nsuboptimal performance in programming tasks. In this paper, we propose\nLongCodeZip, a novel plug-and-play code compression framework designed\nspecifically for code LLMs. LongCodeZip employs a dual-stage strategy: (1)\ncoarse-grained compression, which identifies and ranks function-level chunks\nusing conditional perplexity with respect to the instruction, retaining only\nthe most relevant functions; and (2) fine-grained compression, which segments\nretained functions into blocks based on perplexity and selects an optimal\nsubset under an adaptive token budget to maximize relevance. Evaluations across\nmultiple tasks, including code completion, summarization, and question\nanswering, show that LongCodeZip consistently outperforms baseline methods,\nachieving up to a 5.6x compression ratio without degrading task performance. By\neffectively reducing context size while preserving essential information,\nLongCodeZip enables LLMs to better scale to real-world, large-scale code\nscenarios, advancing the efficiency and capability of code intelligence\napplications."}
{"id": "2510.00762", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.00762", "abs": "https://arxiv.org/abs/2510.00762", "authors": ["Rudrajit Choudhuri", "Carmen Badea", "Christian Bird", "Jenna Butler", "Rob DeLine", "Brian Houck"], "title": "AI Where It Matters: Where, Why, and How Developers Want AI Support in Daily Work", "comment": null, "summary": "Generative AI is reshaping software work, yet we lack clear guidance on where\ndevelopers most need and want support, and how to design it responsibly. We\nreport a large-scale, mixed-methods study of N=860 developers that examines\nwhere, why, and how they seek or limit AI help, providing the first task-aware,\nempirically validated mapping from developers' perceptions of their tasks to AI\nadoption patterns and responsible AI priorities. Using cognitive appraisal\ntheory, we show that task evaluations predict openness to and use of AI,\nrevealing distinct patterns: strong current use and a desire for improvement in\ncore work (e.g., coding, testing); high demand to reduce toil (e.g.,\ndocumentation, operations); and clear limits for identity- and\nrelationship-centric work (e.g., mentoring). Priorities for responsible AI\nsupport vary by context: reliability and security for systems-facing tasks;\ntransparency, alignment, and steerability to maintain control; and fairness and\ninclusiveness for human-facing work. Our results offer concrete, contextual\nguidance for delivering AI where it matters to developers and their work."}
{"id": "2510.00449", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.00449", "abs": "https://arxiv.org/abs/2510.00449", "authors": ["Koki Ryu", "Hitomi Yanaka"], "title": "Enhancing Rating Prediction with Off-the-Shelf LLMs Using In-Context User Reviews", "comment": "Accepted to EMNLP 2025 PALS Workshop", "summary": "Personalizing the outputs of large language models (LLMs) to align with\nindividual user preferences is an active research area. However, previous\nstudies have mainly focused on classification or ranking tasks and have not\nconsidered Likert-scale rating prediction, a regression task that requires both\nlanguage and mathematical reasoning to be solved effectively. This task has\nsignificant industrial applications, but the utilization of LLMs remains\nunderexplored, particularly regarding the capabilities of off-the-shelf LLMs.\nThis study investigates the performance of off-the-shelf LLMs on rating\nprediction, providing different in-context information. Through comprehensive\nexperiments with eight models across three datasets, we demonstrate that\nuser-written reviews significantly improve the rating prediction performance of\nLLMs. This result is comparable to traditional methods like matrix\nfactorization, highlighting the potential of LLMs as a promising solution for\nthe cold-start problem. We also find that the reviews for concrete items are\nmore effective than general preference descriptions that are not based on any\nspecific item. Furthermore, we discover that prompting LLMs to first generate a\nhypothetical review enhances the rating prediction performance. Our code is\navailable at https://github.com/ynklab/rating-prediction-with-reviews."}
{"id": "2510.00881", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00881", "abs": "https://arxiv.org/abs/2510.00881", "authors": ["Patrizio Migliarini", "Mashal Afzal Memon", "Marco Autili", "Paola Inverardi"], "title": "Advancing Automated Ethical Profiling in SE: a Zero-Shot Evaluation of LLM Reasoning", "comment": "Accepted at ASE 2025", "summary": "Large Language Models (LLMs) are increasingly integrated into software\nengineering (SE) tools for tasks that extend beyond code synthesis, including\njudgment under uncertainty and reasoning in ethically significant contexts. We\npresent a fully automated framework for assessing ethical reasoning\ncapabilities across 16 LLMs in a zero-shot setting, using 30 real-world\nethically charged scenarios. Each model is prompted to identify the most\napplicable ethical theory to an action, assess its moral acceptability, and\nexplain the reasoning behind their choice. Responses are compared against\nexpert ethicists' choices using inter-model agreement metrics. Our results show\nthat LLMs achieve an average Theory Consistency Rate (TCR) of 73.3% and Binary\nAgreement Rate (BAR) on moral acceptability of 86.7%, with interpretable\ndivergences concentrated in ethically ambiguous cases. A qualitative analysis\nof free-text explanations reveals strong conceptual convergence across models\ndespite surface-level lexical diversity. These findings support the potential\nviability of LLMs as ethical inference engines within SE pipelines, enabling\nscalable, auditable, and adaptive integration of user-aligned ethical\nreasoning. Our focus is the Ethical Interpreter component of a broader\nprofiling pipeline: we evaluate whether current LLMs exhibit sufficient\ninterpretive stability and theory-consistent reasoning to support automated\nprofiling."}
{"id": "2510.00482", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.00482", "abs": "https://arxiv.org/abs/2510.00482", "authors": ["Yawen Xue", "Masaya Tsunokake", "Yuta Koreeda", "Ekant Muljibhai Amin", "Takashi Sumiyoshi", "Yasuhiro Sogawa"], "title": "Agent Fine-tuning through Distillation for Domain-specific LLMs in Microdomains", "comment": "Accepted by AIxB 2025", "summary": "Agentic large language models (LLMs) have become prominent for autonomously\ninteracting with external environments and performing multi-step reasoning\ntasks. Most approaches leverage these capabilities via in-context learning with\nfew-shot prompts, but this often results in lengthy inputs and higher\ncomputational costs. Agent fine-tuning offers an alternative by enabling LLMs\nto internalize procedural reasoning and domain-specific knowledge through\ntraining on relevant data and demonstration trajectories. While prior studies\nhave focused on general domains, their effectiveness in specialized technical\nmicrodomains remains unclear. This paper explores agent fine-tuning for domain\nadaptation within Hitachi's JP1 middleware, a microdomain for specialized IT\noperations. We fine-tuned LLMs using JP1-specific datasets derived from domain\nmanuals and distilled reasoning trajectories generated by LLMs themselves,\nenhancing decision making accuracy and search efficiency. During inference, we\nused an agentic prompt with retrieval-augmented generation and introduced a\ncontext-answer extractor to improve information relevance. On JP1 certification\nexam questions, our method achieved a 14% performance improvement over the base\nmodel, demonstrating the potential of agent fine-tuning for domain-specific\nreasoning in complex microdomains."}
{"id": "2510.00920", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.00920", "abs": "https://arxiv.org/abs/2510.00920", "authors": ["Songqiang Chen", "Congying Xu", "Jingyi Chen", "Jialun Cao", "Jiarong Wu", "Shing-Chi Cheung"], "title": "On Effective Semantic Translation for Code: A Study Based on Pseudocode", "comment": null, "summary": "Large language models (LLMs) show great potential in code translation.\nHowever, accurate translation remains challenging when using the commonly\nadopted direct code-to-code translation approach, which converts a program into\nthe target programming language (PL) in a single step. Inspired by the success\nof incorporating intermediate steps to guide LLMs in resolving challenging\ntasks, we explore pseudocode-based code translation, which emulates the human\nsemantic translation by first interpreting the program's intent and logic into\npseudocode and then implementing it in the target PL. We find that\npseudocode-based translation helps translate programs that direct translation\nstruggles to handle. Nonetheless, the effectiveness, advantages, and\nlimitations of this approach remain underexplored. To bridge this gap, we\npresent an empirical study on pseudocode-based code translation, aiming to\ninvestigate its effectiveness in enhancing the direct translation approach,\nilluminate its effective usage, and identify limitations hindering its\npotential benefits. By comparing direct and pseudocode-based translation\napproaches on 9,690 translation tasks across six PLs with five popular LLMs, we\ndemonstrate that pseudocode-based translation can effectively complement direct\ntranslation, particularly when translating from flexible to rigid PLs or\ndealing with low-resource Rust. Based on these findings, we suggest adopting\nstrategies that combine the complementary strengths of both approaches to\nenhance code translation accuracy. We also reveal the advantages of\npseudocode-based translation in disentangling translations of complicated\nprograms and mitigating distractions from detailed implementations in original\nprograms, as well as its limitations due to incorrect, incomplete, or ambiguous\npseudocode."}
{"id": "2510.00496", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.00496", "abs": "https://arxiv.org/abs/2510.00496", "authors": ["Pengzhou Cheng", "Lingzhong Dong", "Zeng Wu", "Zongru Wu", "Xiangru Tang", "Chengwei Qin", "Zhuosheng Zhang", "Gongshen Liu"], "title": "Agent-ScanKit: Unraveling Memory and Reasoning of Multimodal Agents via Sensitivity Perturbations", "comment": "23 pages, 10 figures, 7 tables", "summary": "Although numerous strategies have recently been proposed to enhance the\nautonomous interaction capabilities of multimodal agents in graphical user\ninterface (GUI), their reliability remains limited when faced with complex or\nout-of-domain tasks. This raises a fundamental question: Are existing\nmultimodal agents reasoning spuriously? In this paper, we propose\n\\textbf{Agent-ScanKit}, a systematic probing framework to unravel the memory\nand reasoning capabilities of multimodal agents under controlled perturbations.\nSpecifically, we introduce three orthogonal probing paradigms: visual-guided,\ntext-guided, and structure-guided, each designed to quantify the contributions\nof memorization and reasoning without requiring access to model internals. In\nfive publicly available GUI benchmarks involving 18 multimodal agents, the\nresults demonstrate that mechanical memorization often outweighs systematic\nreasoning. Most of the models function predominantly as retrievers of\ntraining-aligned knowledge, exhibiting limited generalization. Our findings\nunderscore the necessity of robust reasoning modeling for multimodal agents in\nreal-world scenarios, offering valuable insights toward the development of\nreliable multimodal agents."}
{"id": "2510.00946", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.00946", "abs": "https://arxiv.org/abs/2510.00946", "authors": ["Shiza Andleeb", "Brandon Kantorski", "Jeffrey C. Carver"], "title": "ChatGPT in Introductory Programming: Counterbalanced Evaluation of Code Quality, Conceptual Learning, and Student Perceptions", "comment": "Accepted to SIGCITE'25", "summary": "Background: Large language models (LLMs) such as ChatGPT are increasingly\nused in introductory programming courses to provide real-time code generation,\ndebugging, and explanations. While these tools can boost productivity and code\nquality, concerns remain about over-reliance and potential impacts on\nconceptual learning. Objective: To investigate how ChatGPT access affects code\nquality, conceptual understanding, task completion times, and student\nperceptions in a CS1 course. Methods: We conducted a counterbalanced,\nquasi-experimental study in which students alternated between ChatGPT and\nnon-ChatGPT conditions across two programming assignments in C (functions and\nstructures). We evaluated their code submissions using multidimensional\nrubrics, conceptual post-surveys, and task completion time. Results: Students\nwho had access to ChatGPT produced significantly higher rubric scores for code\nquality and completed tasks in less time compared to those without access.\nHowever, gains in conceptual understanding were mixed, lower for the functions\ntopic but higher for the structures topic. Students reported positive\nexperiences with ChatGPT, citing its value for debugging and practice, while\nexpressing concerns about accuracy and long-term skill development.\nConclusions: ChatGPT can enhance code quality and efficiency for novice\nprogrammers, but may not uniformly improve conceptual understanding. Structured\nintegration and complementary instructional strategies are recommended to\nfoster independent problem-solving skills."}
{"id": "2510.00499", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00499", "abs": "https://arxiv.org/abs/2510.00499", "authors": ["Xingjian Zhao", "Zhe Xu", "Luozhijie Jin", "Yang Wang", "Hanfu Chen", "Yaozhou Jiang", "Ke Chen", "Ruixiao Li", "Mingshu Chen", "Ruiming Wang", "Wenbo Zhang", "Yiyang Zhang", "Donghua Yu", "Yang Gao", "Xiaogui Yang", "Yitian Gong", "Yuanfan Xu", "Qinyuan Cheng", "Zhaoye Fei", "Shimin Li", "Yaqian Zhou", "Xuanjing Huang", "Xipeng Qiu"], "title": "MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance", "comment": null, "summary": "Spoken dialogue systems often rely on cascaded pipelines that transcribe,\nprocess, and resynthesize speech. While effective, this design discards\nparalinguistic cues and limits expressivity. Recent end-to-end methods reduce\nlatency and better preserve these cues, yet still rely on text intermediates,\ncreating a fundamental bottleneck. We present MOSS-Speech, a true\nspeech-to-speech large language model that directly understands and generates\nspeech without relying on text guidance. Our approach combines a modality-based\nlayer-splitting architecture with a frozen pre-training strategy, preserving\nthe reasoning and knowledge of pretrained text LLMs while adding native speech\ncapabilities. Experiments show that our model achieves state-of-the-art results\nin spoken question answering and delivers comparable speech-to-speech\nperformance relative to existing text-guided systems, while still maintaining\ncompetitive text performance. By narrowing the gap between text-guided and\ndirect speech generation, our work establishes a new paradigm for expressive\nand efficient end-to-end speech interaction."}
{"id": "2510.00957", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.00957", "abs": "https://arxiv.org/abs/2510.00957", "authors": ["Shiza Andleeb", "Teo Mendoza", "Lucas Cordova", "Gursimran Walia", "Jeffrey C. Carver"], "title": "Enhancing Software Testing Education: Understanding Where Students Struggle", "comment": "Accepted to SIGCITE'25", "summary": "Effective software testing is critical for producing reliable and secure\nsoftware, yet many computer science students struggle to master the\nfoundational concepts required to construct comprehensive test suites. While\nautomated feedback tools are widely used to support student learning, it\nremains unclear which testing concepts are most frequently misunderstood and\nhow these misunderstandings are reflected in students' test suite revisions.\nThis study examines the specific testing concepts that lead students to make\nineffective changes, those that fail to improve code coverage, during test\nsuite development. Leveraging an automated feedback tool in a senior-level\nsoftware testing course, we analyzed student submissions from two assignments\nto identify prevalent conceptual gaps and patterns of unproductive\nmodification. Our results reveal that decision coverage and exception handling\nare persistent challenges, and that students most often make superficial or\nmethod-level changes that do not enhance coverage. These findings provide\nactionable insights for educators, researchers, and tool designers. By\npinpointing the concepts that most often contribute to poor testing outcomes,\nwe can refine feedback systems, target instruction to address persistent\nmisconceptions, and more effectively support students in developing robust,\nmaintainable test suites."}
{"id": "2510.00507", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00507", "abs": "https://arxiv.org/abs/2510.00507", "authors": ["Yurun Chen", "Xavier Hu", "Yuhan Liu", "Ziqi Wang", "Zeyi Liao", "Lin Chen", "Feng Wei", "Yuxi Qian", "Bo Zheng", "Keting Yin", "Shengyu Zhang"], "title": "Graph2Eval: Automatic Multimodal Task Generation for Agents via Knowledge Graphs", "comment": "20 pages, 10 figures", "summary": "As multimodal LLM-driven agents continue to advance in autonomy and\ngeneralization, evaluation based on static datasets can no longer adequately\nassess their true capabilities in dynamic environments and diverse tasks.\nExisting LLM-based synthetic data methods are largely designed for LLM training\nand evaluation, and thus cannot be directly applied to agent tasks that require\ntool use and interactive capabilities. While recent studies have explored\nautomatic agent task generation with LLMs, most efforts remain limited to text\nor image analysis, without systematically modeling multi-step interactions in\nweb environments. To address these challenges, we propose Graph2Eval, a\nknowledge graph-based framework that automatically generates both multimodal\ndocument comprehension tasks and web interaction tasks, enabling comprehensive\nevaluation of agents' reasoning, collaboration, and interactive capabilities.\nIn our approach, knowledge graphs constructed from multi-source external data\nserve as the task space, where we translate semantic relations into structured\nmultimodal tasks using subgraph sampling, task templates, and meta-paths. A\nmulti-stage filtering pipeline based on node reachability, LLM scoring, and\nsimilarity analysis is applied to guarantee the quality and executability of\nthe generated tasks. Furthermore, Graph2Eval supports end-to-end evaluation of\nmultiple agent types (Single-Agent, Multi-Agent, Web Agent) and measures\nreasoning, collaboration, and interaction capabilities. We instantiate the\nframework with Graph2Eval-Bench, a curated dataset of 1,319 tasks spanning\ndocument comprehension and web interaction scenarios. Experiments show that\nGraph2Eval efficiently generates tasks that differentiate agent and model\nperformance, revealing gaps in reasoning, collaboration, and web interaction\nacross different settings and offering a new perspective for agent evaluation."}
{"id": "2510.01002", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.01002", "abs": "https://arxiv.org/abs/2510.01002", "authors": ["Chengran Yang", "Ting Zhang", "Jinfeng Jiang", "Xin Zhou", "Haoye Tian", "Jieke Shi", "Junkai Chen", "Yikun Li", "Eng Lieh Ouh", "Lwin Khin Shar", "David Lo"], "title": "Semantics-Aligned, Curriculum-Driven, and Reasoning-Enhanced Vulnerability Repair Framework", "comment": null, "summary": "Current learning-based Automated Vulnerability Repair (AVR) approaches, while\npromising, often fail to generalize effectively in real-world scenarios. Our\ndiagnostic analysis reveals three fundamental weaknesses in state-of-the-art\nAVR approaches: (1) limited cross-repository generalization, with performance\ndrops on unseen codebases; (2) inability to capture long-range dependencies,\ncausing a performance degradation on complex, multi-hunk repairs; and (3)\nover-reliance on superficial lexical patterns, leading to significant\nperformance drops on vulnerabilities with minor syntactic variations like\nvariable renaming.\n  To address these limitations, we propose SeCuRepair, a semantics-aligned,\ncurriculum-driven, and reasoning-enhanced framework for vulnerability repair.\nAt its core, SeCuRepair adopts a reason-then-edit paradigm, requiring the model\nto articulate why and how a vulnerability should be fixed before generating the\npatch. This explicit reasoning enforces a genuine understanding of repair logic\nrather than superficial memorization of lexical patterns. SeCuRepair also moves\nbeyond traditional supervised fine-tuning and employs semantics-aware\nreinforcement learning, rewarding patches for their syntactic and semantic\nalignment with the oracle patch rather than mere token overlap. Complementing\nthis, a difficulty-aware curriculum progressively trains the model, starting\nwith simple fixes and advancing to complex, multi-hunk coordinated edits.\n  We evaluate SeCuRepair on strict, repository-level splits of BigVul and newly\ncrafted PrimeVul_AVR datasets. SeCuRepair significantly outperforms all\nbaselines, surpassing the best-performing baselines by 34.52% on BigVul and\n31.52% on PrimeVul\\textsubscript{AVR} in terms of CodeBLEU, respectively.\nComprehensive ablation studies further confirm that each component of our\nframework contributes to its final performance."}
{"id": "2510.00508", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00508", "abs": "https://arxiv.org/abs/2510.00508", "authors": ["Yongchao Long", "Xian Wu", "Yingying Zhang", "Xianbin Wen", "Yuxi Zhou", "Shenda Hong"], "title": "Copy-Paste to Mitigate Large Language Model Hallucinations", "comment": null, "summary": "While Retrieval-Augmented Generation (RAG) enables large language models\n(LLMs) to generate contextually grounded responses, contextual faithfulness\nremains challenging as LLMs may not consistently trust provided context,\nleading to hallucinations that undermine reliability. We observe an inverse\ncorrelation between response copying degree and context-unfaithful\nhallucinations on RAGTruth, suggesting that higher copying degrees reduce\nhallucinations by fostering genuine contextual belief. We propose CopyPasteLLM,\nobtained through two-stage high-copying response preference training. We design\nthree prompting methods to enhance copying degree, demonstrating that\nhigh-copying responses achieve superior contextual faithfulness and\nhallucination control. These approaches enable a fully automated pipeline that\ntransforms generated responses into high-copying preference data for training\nCopyPasteLLM. On FaithEval, ConFiQA and PubMedQA, CopyPasteLLM achieves best\nperformance in both counterfactual and original contexts, remarkably with 12.2%\nto 24.5% accuracy improvements on FaithEval over the best baseline, while\nrequiring only 365 training samples -- 1/50th of baseline data. To elucidate\nCopyPasteLLM's effectiveness, we propose the Context-Parameter Copying\nCapturing algorithm. Interestingly, this reveals that CopyPasteLLM recalibrates\nreliance on internal parametric knowledge rather than external knowledge during\ngeneration. All codes are available at\nhttps://github.com/longyongchao/CopyPasteLLM"}
{"id": "2510.01003", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01003", "abs": "https://arxiv.org/abs/2510.01003", "authors": ["Boshi Wang", "Weijian Xu", "Yunsheng Li", "Mei Gao", "Yujia Xie", "Huan Sun", "Dongdong Chen"], "title": "Improving Code Localization with Repository Memory", "comment": "15 pages, 8 figures", "summary": "Code localization is a fundamental challenge in repository-level software\nengineering tasks such as bug fixing. While existing methods equip language\nagents with comprehensive tools/interfaces to fetch information from the\nrepository, they overlook the critical aspect of memory, where each instance is\ntypically handled from scratch assuming no prior repository knowledge. In\ncontrast, human developers naturally build long-term repository memory, such as\nthe functionality of key modules and associations between various bug types and\ntheir likely fix locations. In this work, we augment language agents with such\nmemory by leveraging a repository's commit history - a rich yet underutilized\nresource that chronicles the codebase's evolution. We introduce tools that\nallow the agent to retrieve from a non-parametric memory encompassing recent\nhistorical commits and linked issues, as well as functionality summaries of\nactively evolving parts of the codebase identified via commit patterns. We\ndemonstrate that augmenting such a memory can significantly improve LocAgent, a\nstate-of-the-art localization framework, on both SWE-bench-verified and the\nmore recent SWE-bench-live benchmarks. Our research contributes towards\ndeveloping agents that can accumulate and leverage past experience for\nlong-horizon tasks, more closely emulating the expertise of human developers."}
{"id": "2510.00510", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.00510", "abs": "https://arxiv.org/abs/2510.00510", "authors": ["Jiarun Liu", "Shiyue Xu", "Shangkun Liu", "Yang Li", "Wen Liu", "Min Liu", "Xiaoqing Zhou", "Hanmin Wang", "Shilin Jia", "zhen Wang", "Shaohua Tian", "Hanhao Li", "Junbo Zhang", "Yongli Yu", "Peng Cao", "Haofen Wang"], "title": "JoyAgent-JDGenie: Technical Report on the GAIA", "comment": null, "summary": "Large Language Models are increasingly deployed as autonomous agents for\ncomplex real-world tasks, yet existing systems often focus on isolated\nimprovements without a unifying design for robustness and adaptability. We\npropose a generalist agent architecture that integrates three core components:\na collective multi-agent framework combining planning and execution agents with\ncritic model voting, a hierarchical memory system spanning working, semantic,\nand procedural layers, and a refined tool suite for search, code execution, and\nmultimodal parsing. Evaluated on a comprehensive benchmark, our framework\nconsistently outperforms open-source baselines and approaches the performance\nof proprietary systems. These results demonstrate the importance of\nsystem-level integration and highlight a path toward scalable, resilient, and\nadaptive AI assistants capable of operating across diverse domains and tasks."}
{"id": "2510.01024", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01024", "abs": "https://arxiv.org/abs/2510.01024", "authors": ["Elvis Júnior", "Alan Valejo", "Jorge Valverde-Rebaza", "Vânia de Oliveira Neves"], "title": "GenIA-E2ETest: A Generative AI-Based Approach for End-to-End Test Automation", "comment": "Preprint of a paper published at the 39th Brazilian Symposium on\n  Software Engineering (SBES 2025). Please cite the published version:\n  https://sol.sbc.org.br/index.php/sbes/article/view/37006", "summary": "Software testing is essential to ensure system quality, but it remains\ntime-consuming and error-prone when performed manually. Although recent\nadvances in Large Language Models (LLMs) have enabled automated test\ngeneration, most existing solutions focus on unit testing and do not address\nthe challenges of end-to-end (E2E) testing, which validates complete\napplication workflows from user input to final system response. This paper\nintroduces GenIA-E2ETest, which leverages generative AI to generate executable\nE2E test scripts from natural language descriptions automatically. We evaluated\nthe approach on two web applications, assessing completeness, correctness,\nadaptation effort, and robustness. Results were encouraging: the scripts\nachieved an average of 77% for both element metrics, 82% for precision of\nexecution, 85% for execution recall, required minimal manual adjustments\n(average manual modification rate of 10%), and showed consistent performance in\ntypical web scenarios. Although some sensitivity to context-dependent\nnavigation and dynamic content was observed, the findings suggest that\nGenIA-E2ETest is a practical and effective solution to accelerate E2E test\nautomation from natural language, reducing manual effort and broadening access\nto automated testing."}
{"id": "2510.00514", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.00514", "abs": "https://arxiv.org/abs/2510.00514", "authors": ["Samuel Pfisterer", "Florian Grötschla", "Luca A. Lanzendörfer", "Florian Yan", "Roger Wattenhofer"], "title": "EuroSpeech: A Multilingual Speech Corpus", "comment": "Published in the 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025) Track on Datasets and Benchmark", "summary": "Recent progress in speech processing has highlighted that high-quality\nperformance across languages requires substantial training data for each\nindividual language. While existing multilingual datasets cover many languages,\nthey often contain insufficient data for most languages. Thus, trained models\nperform poorly on the majority of the supported languages. Our work addresses\nthis challenge by introducing a scalable pipeline for constructing speech\ndatasets from parliamentary recordings. The proposed pipeline includes robust\ncomponents for media retrieval and a two-stage alignment algorithm designed to\nhandle non-verbatim transcripts and long-form audio. Applying this pipeline to\nrecordings from 22 European parliaments, we extract over 61k hours of aligned\nspeech segments, achieving substantial per-language coverage with 19 languages\nexceeding 1k hours and 22 languages exceeding 500 hours of high-quality speech\ndata. We obtain an average 41.8\\% reduction in word error rates over baselines\nwhen finetuning an existing ASR model on our dataset, demonstrating the\nusefulness of our approach."}
{"id": "2510.01077", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01077", "abs": "https://arxiv.org/abs/2510.01077", "authors": ["Daniele Bifolco", "Guido Annicchiarico", "Pierluigi Barbiero", "Massimiliano Di Penta", "Fiorella Zampetti"], "title": "CodeGenLink: A Tool to Find the Likely Origin and License of Automatically Generated Code", "comment": "Proceedings of the 40th IEEE/ACM International Conference on\n  Automated Software Engineering (ASE 2025), November 16-20 2025, Seoul, South\n  Korea", "summary": "Large Language Models (LLMs) are widely used in software development tasks\nnowadays. Unlike reusing code taken from the Web, for LLMs' generated code,\ndevelopers are concerned about its lack of trustworthiness and possible\ncopyright or licensing violations, due to the lack of code provenance\ninformation. This paper proposes CodeGenLink, a GitHub CoPilot extension for\nVisual Studio Code aimed at (i) suggesting links containing code very similar\nto automatically generated code, and (ii) whenever possible, indicating the\nlicense of the likely origin of the code. CodeGenLink retrieves candidate links\nby combining LLMs with their web search features and then performs similarity\nanalysis between the generated and retrieved code. Preliminary results show\nthat CodeGenLink effectively filters unrelated links via similarity analysis\nand provides licensing information when available. Tool URL:\nhttps://github.com/danielebifolco/CodeGenLink Tool Video:\nhttps://youtu.be/M6nqjBf9_pw"}
{"id": "2510.00526", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.00526", "abs": "https://arxiv.org/abs/2510.00526", "authors": ["Gaotang Li", "Ruizhong Qiu", "Xiusi Chen", "Heng Ji", "Hanghang Tong"], "title": "Beyond Log Likelihood: Probability-Based Objectives for Supervised Fine-Tuning across the Model Capability Continuum", "comment": "23 pages, 4 figures", "summary": "Supervised fine-tuning (SFT) is the standard approach for post-training large\nlanguage models (LLMs), yet it often shows limited generalization. We trace\nthis limitation to its default training objective: negative log likelihood\n(NLL). While NLL is classically optimal when training from scratch,\npost-training operates in a different paradigm and could violate its optimality\nassumptions, where models already encode task-relevant priors and supervision\ncan be long and noisy. To this end, we study a general family of\nprobability-based objectives and characterize their effectiveness under\ndifferent conditions. Through comprehensive experiments and extensive ablation\nstudies across 7 model backbones, 14 benchmarks, and 3 domains, we uncover a\ncritical dimension that governs objective behavior: the model-capability\ncontinuum. Near the model-strong end, prior-leaning objectives that downweight\nlow-probability tokens (e.g., $-p$, $-p^{10}$, thresholded variants)\nconsistently outperform NLL; toward the model-weak end, NLL dominates; in\nbetween, no single objective prevails. Our theoretical analysis further\nelucidates how objectives trade places across the continuum, providing a\nprincipled foundation for adapting objectives to model capability. Our code is\navailable at https://github.com/GaotangLi/Beyond-Log-Likelihood."}
{"id": "2510.01096", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01096", "abs": "https://arxiv.org/abs/2510.01096", "authors": ["Nathan Wintersgill", "Trevor Stalnaker", "Daniel Otten", "Laura A. Heymann", "Oscar Chaparro", "Massimiliano Di Penta", "Daniel M. German", "Denys Poshyvanyk"], "title": "Developers' Perspectives on Software Licensing: Current Practices, Challenges, and Tools", "comment": null, "summary": "Most modern software products incorporate open-source components, requiring\ndevelopment teams to maintain compliance with each component's licenses.\nNoncompliance can lead to significant financial, legal, and reputational\nrepercussions. While some organizations may seek advice from legal\npractitioners to assist with licensing tasks, developers still play a key role\nin such a process. To this end, it is essential to understand how developers\napproach license compliance tasks, the challenges they encounter, and the tools\nthat they use. This work studies these aspects of software licensing practices\nthrough a study - conducted by a joint team of software engineering and legal\nresearchers - consisting of a survey with 58 software developers and seven\nfollow-up interviews. The study resulted in 15 key findings regarding the\ncurrent state of practice. We discuss the implications of our findings and\noffer directions for future research as well as actionable recommendations for\nlicensing tools."}
{"id": "2510.00536", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.00536", "abs": "https://arxiv.org/abs/2510.00536", "authors": ["Kung-Hsiang Huang", "Haoyi Qiu", "Yutong Dai", "Caiming Xiong", "Chien-Sheng Wu"], "title": "GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness", "comment": null, "summary": "Graphical user interface (GUI) agents built on vision-language models have\nemerged as a promising approach to automate human-computer workflows. However,\nthey also face the inefficiency challenge as they process long sequences of\nhigh-resolution screenshots and solving long-horizon tasks, making inference\nslow, costly and memory-bound. While key-value (KV) caching can mitigate this,\nstoring the full cache is prohibitive for image-heavy contexts. Existing\ncache-compression methods are sub-optimal as they do not account for the\nspatial and temporal redundancy of GUIs. In this work, we first analyze\nattention patterns in GUI agent workloads and find that, unlike in natural\nimages, attention sparsity is uniformly high across all transformer layers.\nThis insight motivates a simple uniform budget allocation strategy, which we\nshow empirically outperforms more complex layer-varying schemes. Building on\nthis, we introduce GUI-KV, a plug-and-play KV cache compression method for GUI\nagents that requires no retraining. GUI-KV combines two novel techniques: (i)\nspatial saliency guidance, which augments attention scores with the L2 norm of\nhidden states to better preserve semantically important visual tokens, and (ii)\ntemporal redundancy scoring, which projects previous frames' keys onto the\ncurrent frame's key subspace to preferentially prune redundant history. Across\nstandard GUI agent benchmarks and models, GUI-KV outperforms competitive KV\ncompression baselines, closely matching full-cache accuracy at modest budgets.\nNotably, in a 5-screenshot setting on the AgentNetBench benchmark, GUI-KV\nreduces decoding FLOPs by 38.9% while increasing step accuracy by 4.1% over the\nfull-cache baseline. These results demonstrate that exploiting GUI-specific\nredundancies enables efficient and reliable agent performance."}
{"id": "2510.01182", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01182", "abs": "https://arxiv.org/abs/2510.01182", "authors": ["Shuqing Li", "Chenran Zhang", "Binchang Li", "Cuiyun Gao", "Michael R. Lyu"], "title": "When Shared Worlds Break: Demystifying Defects in Multi-User Extended Reality Software Systems", "comment": null, "summary": "Multi-user Extended Reality (XR) systems enable transformative shared\nexperiences but introduce unique software defects that compromise user\nexperience. Understanding software defects in multi-user XR systems is crucial\nfor enhancing system reliability, yet remains underexplored. To fill the gap,\nthis paper presents the first large-scale empirical study of multi-user XR\ndefects, analyzing 2,649 real-world bug reports from diverse sources, including\ndeveloper forums, GitHub repositories, and app reviews on mainstream XR app\nstores. Through rigorous qualitative analysis using iterative open coding, we\ndevelop a comprehensive taxonomy that classifies multi-user XR bugs along three\ndimensions: Symptom Manifestation, Root Cause Origin, and Consequence Severity.\nOur findings reveal that synchronization inconsistencies and avatar-related\nanomalies are the most prevalent symptoms, while network/synchronization logic\ndefects and session management flaws emerge as dominant root causes.\nCritically, over 34% of analyzed bugs lead to severe consequences that\nfundamentally break the shared experience, including system crashes, persistent\ndisconnections, and complete interaction breakdowns, etc. We also identify\nconcerning privacy and health implications unique to multi-user XR contexts.\nBased on our findings of defect analysis, we provide actionable recommendations\nfor developers, platform vendors, and researchers. Our results demonstrate that\nmulti-user XR systems face distinct challenges at the intersection of\ndistributed systems, real-time 3D interaction, and immersive experiences,\nnecessitating specialized approaches to testing, debugging, and quality\nassurance."}
{"id": "2510.00546", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.00546", "abs": "https://arxiv.org/abs/2510.00546", "authors": ["Minjae Oh", "Sangjun Song", "Seungkyu Lee", "Sungmin Jo", "Yohan Jo"], "title": "ThinkBrake: Mitigating Overthinking in Tool Reasoning", "comment": null, "summary": "Small reasoning models (SRMs) often overthink during tool use: they reach a\ncorrect tool-argument configuration, then continue reasoning and overwrite it\nwith an incorrect final call. We diagnose overthinking via oracle rollouts that\ninject </think> at sentence boundaries. On the Berkeley Function Calling\nLeaderboard (BFCL), this oracle termination lifts average accuracy from 85.8\\%\nto 94.2\\% while reducing tokens by 80-94\\%, revealing substantial recoverable\nheadroom and potential redundant reasoning. While prior work on concise\nreasoning has largely targeted mathematics, tool reasoning remains\nunderexplored. We adapt various early-termination baselines to tool use and\nintroduce ThinkBrake, a training-free decoding heuristic. ThinkBrake monitors\nthe log-probability margin between </think> and the current top token at\nsentence boundaries and triggers termination when this margin becomes small.\nAcross BFCL's single turn, non-live and live splits, ThinkBrake preserves or\nimproves accuracy while reducing tokens up to 25\\%, outperforming various\nbaselines."}
{"id": "2510.00446", "categories": ["cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.00446", "abs": "https://arxiv.org/abs/2510.00446", "authors": ["Yuling Shi", "Yichun Qian", "Hongyu Zhang", "Beijun Shen", "Xiaodong Gu"], "title": "LongCodeZip: Compress Long Context for Code Language Models", "comment": "Accepted to ASE 2025. Code available at\n  https://github.com/YerbaPage/LongCodeZip", "summary": "Code generation under long contexts is becoming increasingly critical as\nLarge Language Models (LLMs) are required to reason over extensive information\nin the codebase. While recent advances enable code LLMs to process long inputs,\nhigh API costs and generation latency remain substantial bottlenecks. Existing\ncontext pruning techniques, such as LLMLingua, achieve promising results for\ngeneral text but overlook code-specific structures and dependencies, leading to\nsuboptimal performance in programming tasks. In this paper, we propose\nLongCodeZip, a novel plug-and-play code compression framework designed\nspecifically for code LLMs. LongCodeZip employs a dual-stage strategy: (1)\ncoarse-grained compression, which identifies and ranks function-level chunks\nusing conditional perplexity with respect to the instruction, retaining only\nthe most relevant functions; and (2) fine-grained compression, which segments\nretained functions into blocks based on perplexity and selects an optimal\nsubset under an adaptive token budget to maximize relevance. Evaluations across\nmultiple tasks, including code completion, summarization, and question\nanswering, show that LongCodeZip consistently outperforms baseline methods,\nachieving up to a 5.6x compression ratio without degrading task performance. By\neffectively reducing context size while preserving essential information,\nLongCodeZip enables LLMs to better scale to real-world, large-scale code\nscenarios, advancing the efficiency and capability of code intelligence\napplications."}
{"id": "2510.00567", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.00567", "abs": "https://arxiv.org/abs/2510.00567", "authors": ["Yubo Xie", "Chenkai Wang", "Zongyang Ma", "Fahui Miao"], "title": "Are Large Language Models Chronically Online Surfers? A Dataset for Chinese Internet Meme Explanation", "comment": "Accepted to EMNLP 2025 Main Conference. 22 pages, 3 figures, 13\n  tables. GitHub: github.com/yuboxie/chime", "summary": "Large language models (LLMs) are trained on vast amounts of text from the\nInternet, but do they truly understand the viral content that rapidly spreads\nonline -- commonly known as memes? In this paper, we introduce CHIME, a dataset\nfor CHinese Internet Meme Explanation. The dataset comprises popular\nphrase-based memes from the Chinese Internet, annotated with detailed\ninformation on their meaning, origin, example sentences, types, etc. To\nevaluate whether LLMs understand these memes, we designed two tasks. In the\nfirst task, we assessed the models' ability to explain a given meme, identify\nits origin, and generate appropriate example sentences. The results show that\nwhile LLMs can explain the meanings of some memes, their performance declines\nsignificantly for culturally and linguistically nuanced meme types.\nAdditionally, they consistently struggle to provide accurate origins for the\nmemes. In the second task, we created a set of multiple-choice questions (MCQs)\nrequiring LLMs to select the most appropriate meme to fill in a blank within a\ncontextual sentence. While the evaluated models were able to provide correct\nanswers, their performance remains noticeably below human levels. We have made\nCHIME public and hope it will facilitate future research on computational meme\nunderstanding."}
{"id": "2510.00568", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.00568", "abs": "https://arxiv.org/abs/2510.00568", "authors": ["Shiyu Li", "Yang Tang", "Yifan Wang", "Peiming Li", "Xi Chen"], "title": "ReSeek: A Self-Correcting Framework for Search Agents with Instructive Rewards", "comment": "19 pages", "summary": "Search agents powered by Large Language Models (LLMs) have demonstrated\nsignificant potential in tackling knowledge-intensive tasks. Reinforcement\nlearning (RL) has emerged as a powerful paradigm for training these agents to\nperform complex, multi-step reasoning. However, prior RL-based methods often\nrely on sparse or rule-based rewards, which can lead agents to commit to\nsuboptimal or erroneous reasoning paths without the ability to recover. To\naddress these limitations, we propose ReSeek, a novel self-correcting framework\nfor training search agents. Our framework introduces a self-correction\nmechanism that empowers the agent to dynamically identify and recover from\nerroneous search paths during an episode. By invoking a special JUDGE action,\nthe agent can judge the information and re-plan its search strategy. To guide\nthis process, we design a dense, instructive process reward function, which\ndecomposes into a correctness reward for retrieving factual information and a\nutility reward for finding information genuinely useful for the query.\nFurthermore, to mitigate the risk of data contamination in existing datasets,\nwe introduce FictionalHot, a new and challenging benchmark with recently\ncurated questions requiring complex reasoning. Being intuitively reasonable and\npractically simple, extensive experiments show that agents trained with ReSeek\nsignificantly outperform SOTA baselines in task success rate and path\nfaithfulness."}
{"id": "2510.00579", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.00579", "abs": "https://arxiv.org/abs/2510.00579", "authors": ["Li Li", "Ziyi Wang", "Yongliang Wu", "Jianfei Cai", "Xu Yang"], "title": "CoT Vectors: Transferring and Probing the Reasoning Mechanisms of LLMs", "comment": "22 pages, 7 figures", "summary": "Chain-of-Thought (CoT) prompting has emerged as a powerful approach to\nenhancing the reasoning capabilities of Large Language Models (LLMs). However,\nexisting implementations, such as in-context learning and fine-tuning, remain\ncostly and inefficient. To improve CoT reasoning at a lower cost, and inspired\nby the task vector paradigm, we introduce CoT Vectors, compact representations\nthat encode task-general, multi-step reasoning knowledge. Through experiments\nwith Extracted CoT Vectors, we observe pronounced layer-wise instability,\nmanifesting as a U-shaped performance curve that reflects a systematic\nthree-stage reasoning process in LLMs. To address this limitation, we propose\nLearnable CoT Vectors, optimized under a teacher-student framework to provide\nmore stable and robust guidance. Extensive evaluations across diverse\nbenchmarks and models demonstrate that CoT Vectors not only outperform existing\nbaselines but also achieve performance comparable to parameter-efficient\nfine-tuning methods, while requiring fewer trainable parameters. Moreover, by\ntreating CoT Vectors as a probe, we uncover how their effectiveness varies due\nto latent space structure, information density, acquisition mechanisms, and\npre-training differences, offering new insights into the functional\norganization of multi-step reasoning in LLMs. The source code will be released."}
{"id": "2510.00582", "categories": ["cs.CL", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.00582", "abs": "https://arxiv.org/abs/2510.00582", "authors": ["Sangmin Lee", "Woongjib Choi", "Jihyun Kim", "Hong-Goo Kang"], "title": "SAGE-LD: Towards Scalable and Generalizable End-to-End Language Diarization via Simulated Data Augmentation", "comment": null, "summary": "In this paper, we present a neural spoken language diarization model that\nsupports an unconstrained span of languages within a single framework. Our\napproach integrates a learnable query-based architecture grounded in\nmultilingual awareness, with large-scale pretraining on simulated\ncode-switching data. By jointly leveraging these two components, our method\novercomes the limitations of conventional approaches in data scarcity and\narchitecture optimization, and generalizes effectively to real-world\nmultilingual settings across diverse environments. Experimental results\ndemonstrate that our approach achieves state-of-the-art performance on several\nlanguage diarization benchmarks, with a relative performance improvement of 23%\nto 52% over previous methods. We believe that this work not only advances\nresearch in language diarization but also establishes a foundational framework\nfor code-switching speech technologies."}
{"id": "2510.00629", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00629", "abs": "https://arxiv.org/abs/2510.00629", "authors": ["Teisovi Angami", "Kevisino Khate"], "title": "Tenyidie Syllabification corpus creation and deep learning applications", "comment": "17 pages", "summary": "The Tenyidie language is a low-resource language of the Tibeto-Burman family\nspoken by the Tenyimia Community of Nagaland in the north-eastern part of India\nand is considered a major language in Nagaland. It is tonal,\nSubject-Object-Verb, and highly agglutinative in nature. Being a low-resource\nlanguage, very limited research on Natural Language Processing (NLP) has been\nconducted. To the best of our knowledge, no work on syllabification has been\nreported for this language. Among the many NLP tasks, syllabification or\nsyllabication is an important task in which the given word syllables are\nidentified. The contribution of this work is the creation of 10,120 syllabified\nTenyidie words and the application of the Deep Learning techniques on the\ncreated corpus. In this paper, we have applied LSTM, BLSTM, BLSTM+CRF, and\nEncoder-decoder deep learning architectures on our created dataset. In our\ndataset split of 80:10:10 (train:validation:test) set, we achieved the highest\naccuracy of 99.21% with BLSTM model on the test set. This work will find its\napplication in numerous other NLP applications, such as morphological analysis,\npart-of-speech tagging, machine translation, etc, for the Tenyidie Language.\n  Keywords: Tenyidie; NLP; syllabification; deep learning; LSTM; BLSTM; CRF;\nEncoder-decoder"}
{"id": "2510.00647", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.00647", "abs": "https://arxiv.org/abs/2510.00647", "authors": ["Jinlan Fu", "Shenzhen Huangfu", "Hao Fei", "Yichong Huang", "Xiaoyu Shen", "Xipeng Qiu", "See-Kiong Ng"], "title": "MCM-DPO: Multifaceted Cross-Modal Direct Preference Optimization for Alt-text Generation", "comment": "Accepted by ACM MM 2025", "summary": "The alt-text generation task produces concise, context-relevant descriptions\nof images, enabling blind and low-vision users to access online images. Despite\nthe capabilities of large vision-language models, alt-text generation\nperformance remains limited due to noisy user annotations, inconsistent\nstandards, and MLLMs' insensitivity to contextual information. Previous efforts\nto fine-tune MLLMs using supervised fine-tuning (SFT) have struggled, as SFT\nrelies on accurate target annotations, which are often flawed in user-generated\nalt-text. To address this, we propose Multi-faceted Cross-modal Direct\nPreference Optimization (MCM-DPO), which improves alt-text generation by\nlearning to identify better options in preference pairs without requiring\nprecise annotations. MCM-DPO optimizes preferences across single, paired, and\nmulti-preference dimensions, covering textual, visual, and cross-modal factors.\nIn light of the scarcity of high-quality annotated and preference-labeled\ndatasets for alt-text, we constructed two large-scale, high-quality datasets\nnamed TAlt and PAlt, sourced from Twitter and Pinterest. These datasets include\n202k annotated alt-text samples and 18k preference pairs that cover diverse\npreference dimensions, aiming to support further research in this domain.\nExperimental results show that our proposed MCM-DPO method consistently\noutperforms both DPO and SFT, establishing a new state of the art in alt-text\ngeneration. We release the code and data here:\nhttps://github.com/LVUGAI/MCM-DPO"}
{"id": "2510.00662", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00662", "abs": "https://arxiv.org/abs/2510.00662", "authors": ["François Ledoyen", "Gaël Dias", "Jeremie Pantin", "Alexis Lechervy", "Fabrice Maurel", "Youssef Chahir"], "title": "Facilitating Cognitive Accessibility with LLMs: A Multi-Task Approach to Easy-to-Read Text Generation", "comment": "EMNLP 2025", "summary": "Simplifying complex texts is essential for ensuring equitable access to\ninformation, especially for individuals with cognitive impairments. The\nEasy-to-Read (ETR) initiative offers a framework for making content accessible\nto the neurodivergent population, but the manual creation of such texts remains\ntime-consuming and resource-intensive. In this work, we investigate the\npotential of large language models (LLMs) to automate the generation of ETR\ncontent. To address the scarcity of aligned corpora and the specificity of ETR\nconstraints, we propose a multi-task learning (MTL) approach that trains models\njointly on text summarization, text simplification, and ETR generation. We\nexplore two different strategies: multi-task retrieval-augmented generation\n(RAG) for in-context learning, and MTL-LoRA for parameter-efficient\nfine-tuning. Our experiments with Mistral-7B and LLaMA-3-8B, based on ETR-fr, a\nnew high-quality dataset, demonstrate the benefits of multi-task setups over\nsingle-task baselines across all configurations. Moreover, results show that\nthe RAG-based strategy enables generalization in out-of-domain settings, while\nMTL-LoRA outperforms all learning strategies within in-domain configurations."}
{"id": "2510.00691", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00691", "abs": "https://arxiv.org/abs/2510.00691", "authors": ["François Ledoyen", "Gaël Dias", "Alexis Lechervy", "Jeremie Pantin", "Fabrice Maurel", "Youssef Chahir", "Elisa Gouzonnat", "Mélanie Berthelot", "Stanislas Moravac", "Armony Altinier", "Amy Khairalla"], "title": "Inclusive Easy-to-Read Generation for Individuals with Cognitive Impairments", "comment": "ECAI 2025", "summary": "Ensuring accessibility for individuals with cognitive impairments is\nessential for autonomy, self-determination, and full citizenship. However,\nmanual Easy-to-Read (ETR) text adaptations are slow, costly, and difficult to\nscale, limiting access to crucial information in healthcare, education, and\ncivic life. AI-driven ETR generation offers a scalable solution but faces key\nchallenges, including dataset scarcity, domain adaptation, and balancing\nlightweight learning of Large Language Models (LLMs). In this paper, we\nintroduce ETR-fr, the first dataset for ETR text generation fully compliant\nwith European ETR guidelines. We implement parameter-efficient fine-tuning on\nPLMs and LLMs to establish generative baselines. To ensure high-quality and\naccessible outputs, we introduce an evaluation framework based on automatic\nmetrics supplemented by human assessments. The latter is conducted using a\n36-question evaluation form that is aligned with the guidelines. Overall\nresults show that PLMs perform comparably to LLMs and adapt effectively to\nout-of-domain texts."}
{"id": "2510.00694", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.00694", "abs": "https://arxiv.org/abs/2510.00694", "authors": ["Harethah Abu Shairah", "Somayah AlHarbi", "Abdulaziz AlHussein", "Sameer Alsabea", "Omar Shaqaqi", "Hebah AlShamlan", "Omar Knio", "George Turkiyyah"], "title": "ALARB: An Arabic Legal Argument Reasoning Benchmark", "comment": "Accepted paper at ArabicNLP 2025", "summary": "We introduce ALARB, a dataset and suite of tasks designed to evaluate the\nreasoning capabilities of large language models (LLMs) within the Arabic legal\ndomain. While existing Arabic benchmarks cover some knowledge-intensive tasks\nsuch as retrieval and understanding, substantial datasets focusing specifically\non multistep reasoning for Arabic LLMs, especially in open-ended contexts, are\nlacking. The dataset comprises over 13K commercial court cases from Saudi\nArabia, with each case including the facts presented, the reasoning of the\ncourt, the verdict, as well as the cited clauses extracted from the regulatory\ndocuments. We define a set of challenging tasks leveraging this dataset and\nreflecting the complexity of real-world legal reasoning, including verdict\nprediction, completion of reasoning chains in multistep legal arguments, and\nidentification of relevant regulations based on case facts. We benchmark a\nrepresentative selection of current open and closed Arabic LLMs on these tasks\nand demonstrate the dataset's utility for instruction tuning. Notably, we show\nthat instruction-tuning a modest 12B parameter model using ALARB significantly\nenhances its performance in verdict prediction and Arabic verdict generation,\nreaching a level comparable to that of GPT-4o."}
{"id": "2510.00810", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.00810", "abs": "https://arxiv.org/abs/2510.00810", "authors": ["Jenny Kunz", "Iben Nyholm Debess", "Annika Simonsen"], "title": "Family Matters: Language Transfer and Merging for Adapting Small LLMs to Faroese", "comment": null, "summary": "We investigate how to adapt small, efficient LLMs to Faroese, a low-resource\nNorth Germanic language. Starting from English models, we continue pre-training\non related Scandinavian languages, either individually or combined via merging,\nbefore fine-tuning on Faroese. We compare full fine-tuning with\nparameter-efficient tuning using LoRA, evaluating their impact on both\nlinguistic accuracy and text comprehension. Due to the lack of existing Faroese\nevaluation data, we construct two new minimal-pair benchmarks from adapted and\nnewly collected datasets and complement them with human evaluations by Faroese\nlinguists. Our results demonstrate that transfer from related languages is\ncrucial, though the optimal source language depends on the task: Icelandic\nenhances linguistic accuracy, whereas Danish boosts comprehension. Similarly,\nthe choice between full fine-tuning and LoRA is task-dependent: LoRA improves\nlinguistic acceptability and slightly increases human evaluation scores on the\nbase model, while full fine-tuning yields stronger comprehension performance\nand better preserves model capabilities during downstream fine-tuning."}
{"id": "2510.00829", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.00829", "abs": "https://arxiv.org/abs/2510.00829", "authors": ["Yanming Sun", "Runzhe Zhan", "Chi Seng Cheang", "Han Wu", "Xuebo Liu", "Yuyao Niu", "Fengying Ye", "Kaixin Lan", "Lidia S. Chao", "Derek F. Wong"], "title": "Exposing the Cracks: Vulnerabilities of Retrieval-Augmented LLM-based Machine Translation", "comment": null, "summary": "\\textbf{RE}trieval-\\textbf{A}ugmented \\textbf{L}LM-based \\textbf{M}achine\n\\textbf{T}ranslation (REAL-MT) shows promise for knowledge-intensive tasks like\nidiomatic translation, but its reliability under noisy retrieval contexts\nremains poorly understood despite this being a common challenge in real-world\ndeployment. To address this gap, we propose a noise synthesis framework and new\nmetrics to evaluate the robustness of REAL-MT systematically. Using this\nframework, we instantiate REAL-MT with Qwen-series models, including standard\nLLMs and large reasoning models (LRMs) with enhanced reasoning, and evaluate\ntheir performance on idiomatic translation across high-, medium-, and\nlow-resource language pairs under synthesized noise. Our results show that\nlow-resource language pairs, which rely more heavily on retrieved context,\ndegrade more severely under noise than high-resource ones and often produce\nnonsensical translations. Although LRMs possess enhanced reasoning\ncapabilities, they show no improvement in error correction and are even more\nsusceptible to noise, tending to rationalize incorrect contexts. We find that\nthis stems from an attention shift away from the source idiom to noisy content,\nwhile confidence increases despite declining accuracy, indicating poor\ncalibration. To mitigate these issues, we investigate training-free and\nfine-tuning strategies, which improve robustness at the cost of performance in\nclean contexts, revealing a fundamental trade-off. Our findings highlight the\nlimitations of current approaches, underscoring the need for self-verifying\nintegration mechanisms."}
{"id": "2510.00857", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.00857", "abs": "https://arxiv.org/abs/2510.00857", "authors": ["Adi Simhi", "Jonathan Herzig", "Martin Tutek", "Itay Itzhak", "Idan Szpektor", "Yonatan Belinkov"], "title": "ManagerBench: Evaluating the Safety-Pragmatism Trade-off in Autonomous LLMs", "comment": null, "summary": "As large language models (LLMs) evolve from conversational assistants into\nautonomous agents, evaluating the safety of their actions becomes critical.\nPrior safety benchmarks have primarily focused on preventing generation of\nharmful content, such as toxic text. However, they overlook the challenge of\nagents taking harmful actions when the most effective path to an operational\ngoal conflicts with human safety. To address this gap, we introduce\nManagerBench, a benchmark that evaluates LLM decision-making in realistic,\nhuman-validated managerial scenarios. Each scenario forces a choice between a\npragmatic but harmful action that achieves an operational goal, and a safe\naction that leads to worse operational performance. A parallel control set,\nwhere potential harm is directed only at inanimate objects, measures a model's\npragmatism and identifies its tendency to be overly safe. Our findings indicate\nthat the frontier LLMs perform poorly when navigating this safety-pragmatism\ntrade-off. Many consistently choose harmful options to advance their\noperational goals, while others avoid harm only to become overly safe and\nineffective. Critically, we find this misalignment does not stem from an\ninability to perceive harm, as models' harm assessments align with human\njudgments, but from flawed prioritization. ManagerBench is a challenging\nbenchmark for a core component of agentic behavior: making safe choices when\noperational goals and alignment values incentivize conflicting actions.\nBenchmark & code available at https://github.com/technion-cs-nlp/ManagerBench."}
{"id": "2510.00861", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.00861", "abs": "https://arxiv.org/abs/2510.00861", "authors": ["Ziliang Wang", "Kang An", "Xuhui Zheng", "Faqiang Qian", "Weikun Zhang", "Cijun Ouyang", "Jialu Cai", "Yuhang Wang", "Yichao Wu"], "title": "Erase to Improve: Erasable Reinforcement Learning for Search-Augmented LLMs", "comment": "10 pages, 4 figures", "summary": "While search-augmented large language models (LLMs) exhibit impressive\ncapabilities, their reliability in complex multi-hop reasoning remains limited.\nThis limitation arises from three fundamental challenges: decomposition errors,\nwhere tasks are incorrectly broken down; retrieval missing, where key evidence\nfails to be retrieved; and reasoning errors, where flawed logic propagates\nthrough the reasoning chain. A single failure in any of these stages can derail\nthe final answer. We propose Erasable Reinforcement Learning (ERL), a novel\nframework that transforms fragile reasoning into a robust process. ERL\nexplicitly identifies faulty steps, erases them, and regenerates reasoning in\nplace, preventing defective logic from propagating through the reasoning chain.\nThis targeted correction mechanism turns brittle reasoning into a more\nresilient process. Models trained with ERL, termed ESearch, achieve substantial\nimprovements on HotpotQA, MuSiQue, 2Wiki, and Bamboogle, with the 3B model\nachieving +8.48% EM and +11.56% F1, and the 7B model achieving +5.38% EM and\n+7.22% F1 over previous state-of-the-art(SOTA) results. These findings suggest\nthat erasable reinforcement learning provides a powerful paradigm shift for\nrobust multi-step reasoning in LLMs."}
{"id": "2510.00880", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.00880", "abs": "https://arxiv.org/abs/2510.00880", "authors": ["Loris Bergeron", "Ioana Buhnila", "Jérôme François", "Radu State"], "title": "HalluGuard: Evidence-Grounded Small Reasoning Models to Mitigate Hallucinations in Retrieval-Augmented Generation", "comment": null, "summary": "Large Language Models (LLMs) excel in many NLP tasks but remain prone to\nhallucinations, limiting trust in real-world applications. We present\nHalluGuard, a 4B-parameter Small Reasoning Model (SRM) for mitigating\nhallucinations in Retrieval-Augmented Generation (RAG). HalluGuard classifies\ndocument-claim pairs as grounded or hallucinated and produces evidence-grounded\njustifications for transparency. Our approach combines (i) a domain-agnostic\nsynthetic dataset derived from FineWeb and refined through multi-stage curation\nand data reformation, (ii) synthetic grounded and hallucinated claims, and\n(iii) preference-based fine-tuning with Odds Ratio Preference Optimization to\ndistill large-model reasoning into a smaller backbone. On the RAGTruth subset\nof the LLM-AggreFact benchmark, HalluGuard achieves 84.0% balanced accuracy\n(BAcc), rivaling specialized models, MiniCheck (7B; 84.0%) and Granite Guardian\n3.3 (8B; 82.2%) while using roughly half their parameters. Over the full\nbenchmark it reaches 75.7% BAcc, matching larger general-purpose LLMs such as\nGPT-4o (75.9%). We will release HalluGuard and datasets under Apache 2.0 upon\nacceptance."}
{"id": "2510.00890", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00890", "abs": "https://arxiv.org/abs/2510.00890", "authors": ["Zhen Yin", "Shenghua Wang"], "title": "Span-level Detection of AI-generated Scientific Text via Contrastive Learning and Structural Calibration", "comment": null, "summary": "The rapid adoption of large language models (LLMs) in scientific writing\nraises serious concerns regarding authorship integrity and the reliability of\nscholarly publications. Existing detection approaches mainly rely on\ndocument-level classification or surface-level statistical cues; however, they\nneglect fine-grained span localization, exhibit weak calibration, and often\nfail to generalize across disciplines and generators. To address these\nlimitations, we present Sci-SpanDet, a structure-aware framework for detecting\nAI-generated scholarly texts. The proposed method combines section-conditioned\nstylistic modeling with multi-level contrastive learning to capture nuanced\nhuman-AI differences while mitigating topic dependence, thereby enhancing\ncross-domain robustness. In addition, it integrates BIO-CRF sequence labeling\nwith pointer-based boundary decoding and confidence calibration to enable\nprecise span-level detection and reliable probability estimates. Extensive\nexperiments on a newly constructed cross-disciplinary dataset of 100,000\nannotated samples generated by multiple LLM families (GPT, Qwen, DeepSeek,\nLLaMA) demonstrate that Sci-SpanDet achieves state-of-the-art performance, with\nF1(AI) of 80.17, AUROC of 92.63, and Span-F1 of 74.36. Furthermore, it shows\nstrong resilience under adversarial rewriting and maintains balanced accuracy\nacross IMRaD sections and diverse disciplines, substantially surpassing\nexisting baselines. To ensure reproducibility and to foster further research on\nAI-generated text detection in scholarly documents, the curated dataset and\nsource code will be publicly released upon publication."}
{"id": "2510.00919", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00919", "abs": "https://arxiv.org/abs/2510.00919", "authors": ["Shunfeng Zheng", "Yudi Zhang", "Meng Fang", "Zihan Zhang", "Zhitan Wu", "Mykola Pechenizkiy", "Ling Chen"], "title": "Benchmarking Foundation Models with Retrieval-Augmented Generation in Olympic-Level Physics Problem Solving", "comment": null, "summary": "Retrieval-augmented generation (RAG) with foundation models has achieved\nstrong performance across diverse tasks, but their capacity for expert-level\nreasoning-such as solving Olympiad-level physics problems-remains largely\nunexplored. Inspired by the way students prepare for competitions by reviewing\npast problems, we investigate the potential of RAG to enhance physics reasoning\nin foundation models. We introduce PhoPile, a high-quality multimodal dataset\nspecifically designed for Olympiad-level physics, enabling systematic study of\nretrieval-based reasoning. PhoPile includes diagrams, graphs, and equations,\ncapturing the inherently multimodal nature of physics problem solving. Using\nPhoPile, we benchmark RAG-augmented foundation models, covering both large\nlanguage models (LLMs) and large multimodal models (LMMs) with multiple\nretrievers. Our results demonstrate that integrating retrieval with physics\ncorpora can improve model performance, while also highlighting challenges that\nmotivate further research in retrieval-augmented physics reasoning."}
{"id": "2510.00931", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.00931", "abs": "https://arxiv.org/abs/2510.00931", "authors": ["Ammar Khairi", "Daniel D'souza", "Marzieh Fadaee", "Julia Kreutzer"], "title": "Making, not Taking, the Best of N", "comment": null, "summary": "Obtaining high-quality generations in modern LLMs has largely been framed as\na selection problem: identifying a single winning generation from a diverse\npool of N samples, the Best-of-N (BoN). Yet, this approach is inherently\nzero-sum, discarding diverse and potentially useful information from the pool.\nInstead, we explore a collaborative setup, where all candidates can potentially\ncontribute to the final winning generation. To this end, we propose Fusion-of-N\n(FusioN): a method that uses a general LLM judge to synthesize the most\ninformative elements of each sample into a single final answer. We compare\nFusioN to BoN in two settings, (i) test-time scaling, where we sample and\naggregate from a single model at test-time (ii) synthetic data generation,\nwhere we fuse samples from a pool of diverse teachers to improve a student\nmodel. We extensively benchmark both setups across 11 languages, 3 diverse\ntasks and varying model scales. Across the bench, FusioN consistently\noutperforms BoN showing versatility and robustness both in test-time scaling\nand in downstream gains from synthetic data generation. We also perform\nextensive analysis on FusioN, where it shows surprising strengths and\nrobustness under challenging settings. These results show that we should shift\nhow we think about evaluating and utilizing LLM generations from a monolithic\nmeasure of quality, to embracing their polylithic nature. This shift allows us\nto integrate diverse strengths, unlock latent potential, and achieve\nimprovements that were previously inaccessible through selection alone."}
{"id": "2510.00962", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.00962", "abs": "https://arxiv.org/abs/2510.00962", "authors": ["Eileen Pan", "Anna Seo Gyeong Choi", "Maartje ter Hoeve", "Skyler Seto", "Allison Koenecke"], "title": "Analyzing Dialectical Biases in LLMs for Knowledge and Reasoning Benchmarks", "comment": "EMNLP Findings 2025, 12 pages, 11 tables, 3 figures", "summary": "Large language models (LLMs) are ubiquitous in modern day natural language\nprocessing. However, previous work has shown degraded LLM performance for\nunder-represented English dialects. We analyze the effects of typifying\n\"standard\" American English language questions as non-\"standard\" dialectal\nvariants on multiple choice question answering tasks and find up to a 20%\nreduction in accuracy. Additionally, we investigate the grammatical basis of\nunder-performance in non-\"standard\" English questions. We find that individual\ngrammatical rules have varied effects on performance, but some are more\nconsequential than others: three specific grammar rules (existential \"it\", zero\ncopula, and y'all) can explain the majority of performance degradation observed\nin multiple dialects. We call for future work to investigate bias mitigation\nmethods focused on individual, high-impact grammatical structures."}
{"id": "2510.01028", "categories": ["cs.CL", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.01028", "abs": "https://arxiv.org/abs/2510.01028", "authors": ["Ruqian Zhang", "Yijiao Zhang", "Juan Shen", "Zhongyi Zhu", "Annie Qu"], "title": "Syntax-Guided Diffusion Language Models with User-Integrated Personalization", "comment": null, "summary": "Large language models have made revolutionary progress in generating\nhuman-like text, yet their outputs often tend to be generic, exhibiting\ninsufficient structural diversity, which limits personalized expression. Recent\nadvances in diffusion models have opened new opportunities for improving\nlanguage generation beyond the limitations of autoregressive paradigms. In this\nwork, we propose a syntax-guided diffusion language model that integrates\nstructural supervision and personalized conditioning to enhance text quality,\ndiversity, and controllability. We introduce a cascaded framework that\ngenerates syntactic guidance before conditional text generation, and further\ngeneralize it to a novel noncascaded architecture for better alignment between\nstructure and content. By incorporating syntactic information in the generating\nprocess, the proposed model better captures the lexical and structural\ncharacteristics of stylistic sentence construction. To enable fine-grained\npersonalization, we develop a shared representation mechanism that facilitates\ninformation integration across users, supporting both faithful stylistic\ngeneration and generalizable zero-shot inference. Extensive experiments on\nmultiple tasks demonstrate the superiority of our approach in fluency,\ndiversity, and stylistic fidelity. Further qualitative analyses highlight its\ninterpretability and flexibility in learning personalized patterns."}
{"id": "2510.01048", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01048", "abs": "https://arxiv.org/abs/2510.01048", "authors": ["Nils Feldhus", "Laura Kopf"], "title": "Interpreting Language Models Through Concept Descriptions: A Survey", "comment": "Accepted at The Eight Workshop on Analyzing and Interpreting Neural\n  Networks for NLP (BlackboxNLP), co-located with EMNLP 2025", "summary": "Understanding the decision-making processes of neural networks is a central\ngoal of mechanistic interpretability. In the context of Large Language Models\n(LLMs), this involves uncovering the underlying mechanisms and identifying the\nroles of individual model components such as neurons and attention heads, as\nwell as model abstractions such as the learned sparse features extracted by\nSparse Autoencoders (SAEs). A rapidly growing line of work tackles this\nchallenge by using powerful generator models to produce open-vocabulary,\nnatural language concept descriptions for these components. In this paper, we\nprovide the first survey of the emerging field of concept descriptions for\nmodel components and abstractions. We chart the key methods for generating\nthese descriptions, the evolving landscape of automated and human metrics for\nevaluating them, and the datasets that underpin this research. Our synthesis\nreveals a growing demand for more rigorous, causal evaluation. By outlining the\nstate of the art and identifying key challenges, this survey provides a roadmap\nfor future research toward making models more transparent."}
{"id": "2510.01052", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01052", "abs": "https://arxiv.org/abs/2510.01052", "authors": ["Samin Mahdipour Aghabagher", "Saeedeh Momtazi"], "title": "Hybrid Dialogue State Tracking for Persian Chatbots: A Language Model-Based Approach", "comment": "22 pages, 1 figure. Submitted to Natural Language Engineering", "summary": "Dialogue State Tracking (DST) is an essential element of conversational AI\nwith the objective of deeply understanding the conversation context and leading\nit toward answering user requests. Due to high demands for open-domain and\nmulti-turn chatbots, the traditional rule-based DST is not efficient enough,\nsince it cannot provide the required adaptability and coherence for human-like\nexperiences in complex conversations. This study proposes a hybrid DST model\nthat utilizes rule-based methods along with language models, including BERT for\nslot filling and intent detection, XGBoost for intent validation, GPT for DST,\nand online agents for real-time answer generation. This model is uniquely\ndesigned to be evaluated on a comprehensive Persian multi-turn dialogue dataset\nand demonstrated significantly improved accuracy and coherence over existing\nmethods in Persian-based chatbots. The results demonstrate how effectively a\nhybrid approach may improve DST capabilities, paving the way for conversational\nAI systems that are more customized, adaptable, and human-like."}
{"id": "2510.01076", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01076", "abs": "https://arxiv.org/abs/2510.01076", "authors": ["Haonan Wang", "Junfeng Sun", "Mingjia Zhao", "Wei Liu"], "title": "Research on the Integration of Embodied Intelligence and Reinforcement Learning in Textual Domains", "comment": "4 pages", "summary": "This article addresses embodied intelligence and reinforcement learning\nintegration in the field of text processing, aiming to enhance text handling\nwith more intelligence on the basis of embodied intelligence's perception and\naction superiority and reinforcement learning's decision optimization\ncapability. Through detailed theoretical explanation and experimental\nexploration, a novel integration model is introduced. This model has been\ndemonstrated to be very effective in a wide range oftext processing tasks,\nvalidating its applicative potential"}
{"id": "2510.01145", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01145", "abs": "https://arxiv.org/abs/2510.01145", "authors": ["Sukairaj Hafiz Imam", "Tadesse Destaw Belay", "Kedir Yassin Husse", "Ibrahim Said Ahmad", "Idris Abdulmumin", "Hadiza Ali Umar", "Muhammad Yahuza Bello", "Joyce Nakatumba-Nabende", "Seid Muhie Yimam", "Shamsuddeen Hassan Muhammad"], "title": "Automatic Speech Recognition (ASR) for African Low-Resource Languages: A Systematic Literature Review", "comment": null, "summary": "ASR has achieved remarkable global progress, yet African low-resource\nlanguages remain rigorously underrepresented, producing barriers to digital\ninclusion across the continent with more than +2000 languages. This systematic\nliterature review (SLR) explores research on ASR for African languages with a\nfocus on datasets, models and training methods, evaluation techniques,\nchallenges, and recommends future directions. We employ the PRISMA 2020\nprocedures and search DBLP, ACM Digital Library, Google Scholar, Semantic\nScholar, and arXiv for studies published between January 2020 and July 2025. We\ninclude studies related to ASR datasets, models or metrics for African\nlanguages, while excluding non-African, duplicates, and low-quality studies\n(score <3/5). We screen 71 out of 2,062 records and we record a total of 74\ndatasets across 111 languages, encompassing approximately 11,206 hours of\nspeech. Fewer than 15% of research provided reproducible materials, and dataset\nlicensing is not clear. Self-supervised and transfer learning techniques are\npromising, but are hindered by limited pre-training data, inadequate coverage\nof dialects, and the availability of resources. Most of the researchers use\nWord Error Rate (WER), with very minimal use of linguistically informed scores\nsuch as Character Error Rate (CER) or Diacritic Error Rate (DER), and thus with\nlimited application in tonal and morphologically rich languages. The existing\nevidence on ASR systems is inconsistent, hindered by issues like dataset\navailability, poor annotations, licensing uncertainties, and limited\nbenchmarking. Nevertheless, the rise of community-driven initiatives and\nmethodological advancements indicates a pathway for improvement. Sustainable\ndevelopment for this area will also include stakeholder partnership, creation\nof ethically well-balanced datasets, use of lightweight modelling techniques,\nand active benchmarking."}
{"id": "2510.01146", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01146", "abs": "https://arxiv.org/abs/2510.01146", "authors": ["David Anugraha", "Shou-Yi Hung", "Zilu Tang", "Annie En-Shiun Lee", "Derry Tanti Wijaya", "Genta Indra Winata"], "title": "mR3: Multilingual Rubric-Agnostic Reward Reasoning Models", "comment": null, "summary": "Evaluation using Large Language Model (LLM) judges has been widely adopted in\nEnglish and shown to be effective for automatic evaluation. However, their\nperformance does not generalize well to non-English settings, and it remains\nunclear what constitutes effective multilingual training for such judges. In\nthis paper, we introduce mR3, a massively multilingual, rubric-agnostic reward\nreasoning model trained on 72 languages, achieving the broadest language\ncoverage in reward modeling to date. We present a comprehensive study of data\nand curriculum selection for training to identify effective strategies and data\nsources for building high-quality reward models, including the integration of\ntarget-language reasoning datasets. Our approach attains state-of-the-art\nperformance on multilingual reward model benchmarks, surpassing much larger\nmodels (i.e., GPT-OSS-120B) while being up to 9x smaller, and its effectiveness\nis further confirmed through extensive ablation studies. Our models, data, and\ncode are available as open source at https://github.com/rubricreward/mr3."}
{"id": "2510.01152", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01152", "abs": "https://arxiv.org/abs/2510.01152", "authors": ["Mustafa Omer Gul", "Claire Cardie", "Tanya Goyal"], "title": "Pay-Per-Search Models are Abstention Models", "comment": "21 pages, with 10 dedicated to citations and appendix. 9 tables and 9\n  figures. Preprint, under review", "summary": "LLMs cannot reliably recognize their parametric knowledge boundaries and\noften hallucinate answers to outside-of-boundary questions. In contrast, humans\nrecognize their limitations and can either seek external help for such\nquestions or abstain. In this paper, we introduce MASH (Modeling Abstention via\nSelective Help-seeking), a training framework that readily extracts abstentions\nfrom LLMs. Our key idea is that any external help-seeking by an LLM, i.e.\nsearch tool use, can serve as a proxy for abstention if the external help\n(search) is appropriately penalized while simultaneously rewarding answer\naccuracy. MASH operationalizes this idea using reinforcement learning with a\npay-per-search reward.\n  We run experiments on three knowledge-intensive QA datasets. Our results show\nthat MASH substantially improves upon the selective help-seeking performance of\nprior efficient search approaches; on multi-hop datasets, MASH improves answer\naccuracy by 7.6%. Furthermore, MASH demonstrates strong off-the-shelf\nabstention -- it can distinguish between unanswerable/answerable questions and\nselectively generate responses for answerable questions -- showcasing behavior\nanalogous to specialized abstention approaches. We emphasize that contrary to\nprior abstention methods, MASH does not require pre-determining knowledge\nboundaries to construct training data. Instead, MASH's abstentions are a\nby-product of training for the auxiliary selective help-seeking task. Overall,\nwe show that MASH training effectively aligns search tool use with parametric\nknowledge, which can be successfully leveraged for making abstention decisions."}
{"id": "2510.01157", "categories": ["cs.CL", "cs.CR", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.01157", "abs": "https://arxiv.org/abs/2510.01157", "authors": ["Alexandrine Fortier", "Thomas Thebaud", "Jesús Villalba", "Najim Dehak", "Patrick Cardinal"], "title": "Backdoor Attacks Against Speech Language Models", "comment": null, "summary": "Large Language Models (LLMs) and their multimodal extensions are becoming\nincreasingly popular. One common approach to enable multimodality is to cascade\ndomain-specific encoders with an LLM, making the resulting model inherit\nvulnerabilities from all of its components. In this work, we present the first\nsystematic study of audio backdoor attacks against speech language models. We\ndemonstrate its effectiveness across four speech encoders and three datasets,\ncovering four tasks: automatic speech recognition (ASR), speech emotion\nrecognition, and gender and age prediction. The attack consistently achieves\nhigh success rates, ranging from 90.76% to 99.41%. To better understand how\nbackdoors propagate, we conduct a component-wise analysis to identify the most\nvulnerable stages of the pipeline. Finally, we propose a fine-tuning-based\ndefense that mitigates the threat of poisoned pretrained encoders."}
{"id": "2510.01164", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.01164", "abs": "https://arxiv.org/abs/2510.01164", "authors": ["Zhengliang Shi", "Ruotian Ma", "Jen-tse Huang", "Xinbei Ma", "Xingyu Chen", "Mengru Wang", "Qu Yang", "Yue Wang", "Fanghua Ye", "Ziyang Chen", "Shanyi Wang", "Cixing Li", "Wenxuan Wang", "Zhaopeng Tu", "Xiaolong Li", "Zhaochun Ren", "Linus"], "title": "Social Welfare Function Leaderboard: When LLM Agents Allocate Social Welfare", "comment": null, "summary": "Large language models (LLMs) are increasingly entrusted with high-stakes\ndecisions that affect human welfare. However, the principles and values that\nguide these models when distributing scarce societal resources remain largely\nunexamined. To address this, we introduce the Social Welfare Function (SWF)\nBenchmark, a dynamic simulation environment where an LLM acts as a sovereign\nallocator, distributing tasks to a heterogeneous community of recipients. The\nbenchmark is designed to create a persistent trade-off between maximizing\ncollective efficiency (measured by Return on Investment) and ensuring\ndistributive fairness (measured by the Gini coefficient). We evaluate 20\nstate-of-the-art LLMs and present the first leaderboard for social welfare\nallocation. Our findings reveal three key insights: (i) A model's general\nconversational ability, as measured by popular leaderboards, is a poor\npredictor of its allocation skill. (ii) Most LLMs exhibit a strong default\nutilitarian orientation, prioritizing group productivity at the expense of\nsevere inequality. (iii) Allocation strategies are highly vulnerable, easily\nperturbed by output-length constraints and social-influence framing. These\nresults highlight the risks of deploying current LLMs as societal\ndecision-makers and underscore the need for specialized benchmarks and targeted\nalignment for AI governance."}
{"id": "2510.01165", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01165", "abs": "https://arxiv.org/abs/2510.01165", "authors": ["Oussama Gabouj", "Kamel Charaf", "Ivan Zakazov", "Nicolas Baldwin", "Robert West"], "title": "GRAD: Generative Retrieval-Aligned Demonstration Sampler for Efficient Few-Shot Reasoning", "comment": "EMNLP 2025 (findings)", "summary": "Large Language Models (LLMs) achieve strong performance across diverse tasks,\nbut their effectiveness often depends on the quality of the provided context.\nRetrieval-Augmented Generation (RAG) enriches prompts with external\ninformation, but its reliance on static databases constrains adaptability and\ncan result in irrelevant demonstrations. In this work, we propose a Generative\nRetrieval-Aligned Demonstrator (GRAD), a dynamic demonstration-based approach\nwhere an LLM model is trained to generate input-specific concise\ndemonstrations. By tailoring demonstrations to each input, our method offers\nbetter contextual support than traditional RAG approaches. We demonstrate the\nsuperiority of GRAD under budget constraints, where we limit both the number of\ntokens used per demonstration and the number of tokens used for the final\noutput. Trained solely on a math dataset, GRAD consistently outperforms strong\nbaselines on Qwen2.5-14B across mathematical reasoning and advanced STEM\nquestions, highlighting GRAD's robust generalization to out-of-distribution\n(OOD) domains such as physics, chemistry, and computer science. Furthermore, we\nshow that demonstrations generated by trained smaller models can effectively\nguide larger target models, reducing training costs while maintaining\ncompetitive accuracy. Overall, this work introduces a scalable demonstration\ngenerator model presenting the first step toward a dynamic few-shot learning\nparadigm in resource-constrained settings. We release the code used for the\nproject."}
{"id": "2510.01171", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01171", "abs": "https://arxiv.org/abs/2510.01171", "authors": ["Jiayi Zhang", "Simon Yu", "Derek Chong", "Anthony Sicilia", "Michael R. Tomz", "Christopher D. Manning", "Weiyan Shi"], "title": "Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM Diversity", "comment": "82 pages, 26 figures, 34 tables. Code is available at\n  https://github.com/CHATS-lab/verbalize-sampling", "summary": "Post-training alignment often reduces LLM diversity, leading to a phenomenon\nknown as mode collapse. Unlike prior work that attributes this effect to\nalgorithmic limitations, we identify a fundamental, pervasive data-level\ndriver: typicality bias in preference data, whereby annotators systematically\nfavor familiar text as a result of well-established findings in cognitive\npsychology. We formalize this bias theoretically, verify it on preference\ndatasets empirically, and show that it plays a central role in mode collapse.\nMotivated by this analysis, we introduce Verbalized Sampling, a simple,\ntraining-free prompting strategy to circumvent mode collapse. VS prompts the\nmodel to verbalize a probability distribution over a set of responses (e.g.,\n``Generate 5 jokes about coffee and their corresponding probabilities'').\nComprehensive experiments show that VS significantly improves performance\nacross creative writing (poems, stories, jokes), dialogue simulation,\nopen-ended QA, and synthetic data generation, without sacrificing factual\naccuracy and safety. For instance, in creative writing, VS increases diversity\nby 1.6-2.1x over direct prompting. We further observe an emergent trend that\nmore capable models benefit more from VS. In sum, our work provides a new\ndata-centric perspective on mode collapse and a practical inference-time remedy\nthat helps unlock pre-trained generative diversity."}
{"id": "2510.01172", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01172", "abs": "https://arxiv.org/abs/2510.01172", "authors": ["Qingyuan Liu", "Jia-Chen Gu", "Yunzhi Yao", "Hong Wang", "Nanyun Peng"], "title": "Energy-Regularized Sequential Model Editing on Hyperspheres", "comment": "The code is available at https://github.com/PlusLabNLP/SPHERE. arXiv\n  admin note: text overlap with arXiv:2410.02355 by other authors", "summary": "Large language models (LLMs) require constant updates to remain aligned with\nevolving real-world knowledge. Model editing offers a lightweight alternative\nto retraining, but sequential editing often destabilizes representations and\ninduces catastrophic forgetting. In this work, we seek to better understand and\nmitigate performance degradation caused by sequential editing. We hypothesize\nthat hyperspherical uniformity, a property that maintains uniform distribution\nof neuron weights on a hypersphere, helps the model remain stable, retain prior\nknowledge, while still accommodate new updates. We use Hyperspherical Energy\n(HE) to quantify neuron uniformity during editing, and examine its correlation\nwith editing performance. Empirical studies across widely used editing methods\nreveals a strong correlation between HE dynamics and editing performance, with\nediting failures consistently coinciding with high HE fluctuations. We further\ntheoretically prove that HE dynamics impose a lower bound on the degradation of\npretrained knowledge, highlighting why HE stability is crucial for knowledge\nretention. Motivated by these insights, we propose SPHERE (Sparse Projection\nfor Hyperspherical Energy-Regularized Editing), an HE-driven regularization\nstrategy that stabilizes neuron weight distributions, ultimately preserving\nprior knowledge while enabling reliable sequential updates. Specifically,\nSPHERE identifies a sparse space complementary to the principal hyperspherical\ndirections of the pretrained weight matrices and projects new knowledge onto\nit, attenuating perturbations on the principal directions. Extensive\nexperiments on LLaMA3 (8B) and Qwen2.5 (7B) show that SPHERE outperforms the\nbest baseline in editing capability by an average of 16.41%, while most\nfaithfully preserving general model performance, thereby offering a principled\npath toward reliable large-scale knowledge editing."}
{"id": "2510.00685", "categories": ["cs.MA", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.00685", "abs": "https://arxiv.org/abs/2510.00685", "authors": ["Nurbek Tastan", "Samuel Horvath", "Karthik Nandakumar"], "title": "Stochastic Self-Organization in Multi-Agent Systems", "comment": null, "summary": "Multi-agent systems (MAS) based on Large Language Models (LLMs) have the\npotential to solve tasks that are beyond the reach of any single LLM. However,\nthis potential can only be realized when the collaboration mechanism between\nagents is optimized. Specifically, optimizing the communication structure\nbetween agents is critical for fruitful collaboration. Most existing approaches\nrely on fixed topologies, pretrained graph generators, optimization over edges,\nor employ external LLM judges, thereby adding to the complexity. In this work,\nwe introduce a response-conditioned framework that adapts communication\non-the-fly. Agents independently generate responses to the user query and\nassess peer contributions using an approximation of the Shapley value. A\ndirected acyclic graph (DAG) is then constructed to regulate the propagation of\nthe responses among agents, which ensures stable and efficient message\ntransmission from high-contributing agents to others. This graph is dynamically\nupdated based on the agent responses from the previous collaboration round.\nSince the proposed framework enables the self-organization of agents without\nadditional supervision or training, we refer to it as SelfOrg. The SelfOrg\nframework goes beyond task- and query-level optimization and takes into account\nthe stochastic nature of agent responses. Experiments with both strong and weak\nLLM backends demonstrate robust performance, with significant gains in the weak\nregime where prior methods collapse. We also theoretically show that multiple\nagents increase the chance of correctness and that the correct responses\nnaturally dominate the information flow."}
{"id": "2510.01003", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01003", "abs": "https://arxiv.org/abs/2510.01003", "authors": ["Boshi Wang", "Weijian Xu", "Yunsheng Li", "Mei Gao", "Yujia Xie", "Huan Sun", "Dongdong Chen"], "title": "Improving Code Localization with Repository Memory", "comment": "15 pages, 8 figures", "summary": "Code localization is a fundamental challenge in repository-level software\nengineering tasks such as bug fixing. While existing methods equip language\nagents with comprehensive tools/interfaces to fetch information from the\nrepository, they overlook the critical aspect of memory, where each instance is\ntypically handled from scratch assuming no prior repository knowledge. In\ncontrast, human developers naturally build long-term repository memory, such as\nthe functionality of key modules and associations between various bug types and\ntheir likely fix locations. In this work, we augment language agents with such\nmemory by leveraging a repository's commit history - a rich yet underutilized\nresource that chronicles the codebase's evolution. We introduce tools that\nallow the agent to retrieve from a non-parametric memory encompassing recent\nhistorical commits and linked issues, as well as functionality summaries of\nactively evolving parts of the codebase identified via commit patterns. We\ndemonstrate that augmenting such a memory can significantly improve LocAgent, a\nstate-of-the-art localization framework, on both SWE-bench-verified and the\nmore recent SWE-bench-live benchmarks. Our research contributes towards\ndeveloping agents that can accumulate and leverage past experience for\nlong-horizon tasks, more closely emulating the expertise of human developers."}
