{"id": "2512.21481", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2512.21481", "abs": "https://arxiv.org/abs/2512.21481", "authors": ["Sunith Vallabhaneni", "Thomas Berkane", "Maimuna Majumder"], "title": "The AI Committee: A Multi-Agent Framework for Automated Validation and Remediation of Web-Sourced Data", "comment": null, "summary": "Many research areas rely on data from the web to gain insights and test their methods. However, collecting comprehensive research datasets often demands manually reviewing many web pages to identify and record relevant data points, which is labor-intensive and susceptible to error. While the emergence of large language models (LLM)-powered web agents has begun to automate parts of this process, they often struggle to ensure the validity of the data they collect. Indeed, these agents exhibit several recurring failure modes - including hallucinating or omitting values, misinterpreting page semantics, and failing to detect invalid information - which are subtle and difficult to detect and correct manually. To address this, we introduce the AI Committee, a novel model-agnostic multi-agent system that automates the process of validating and remediating web-sourced datasets. Each agent is specialized in a distinct task in the data quality assurance pipeline, from source scrutiny and fact-checking to data remediation and integrity validation. The AI Committee leverages various LLM capabilities - including in-context learning for dataset adaptation, chain-of-thought reasoning for complex semantic validation, and a self-correction loop for data remediation - all without task-specific training. We demonstrate the effectiveness of our system by applying it to three real-world datasets, showing that it generalizes across LLMs and significantly outperforms baseline approaches, achieving data completeness up to 78.7% and precision up to 100%. We additionally conduct an ablation study demonstrating the contribution of each agent to the Committee's performance. This work is released as an open-source tool for the research community.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAI\u59d4\u5458\u4f1a\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u7528\u4e8e\u81ea\u52a8\u9a8c\u8bc1\u548c\u8865\u6551\u7f51\u7edc\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u9ad8\u6570\u636e\u5b8c\u6574\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u624b\u5de5\u6536\u96c6\u548c\u9a8c\u8bc1\u7f51\u7edc\u6570\u636e\u96c6\u5de5\u4f5c\u91cf\u5927\u4e14\u6613\u51fa\u9519\uff0c\u73b0\u6709\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u5316\u5de5\u5177\u5728\u6570\u636e\u6709\u6548\u6027\u4fdd\u8bc1\u4e0a\u8868\u73b0\u4e0d\u8db3\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6a21\u578b\u4e0d\u53ef\u77e5\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u6bcf\u4e2a\u4ee3\u7406\u8d1f\u8d23\u6570\u636e\u8d28\u91cf\u4fdd\u8bc1\u6d41\u7a0b\u4e2d\u7684\u4e0d\u540c\u4efb\u52a1\uff0c\u5982\u6e90\u5934\u5ba1\u67e5\u3001\u4e8b\u5b9e\u6838\u67e5\u3001\u6570\u636e\u4fee\u590d\u548c\u5b8c\u6574\u6027\u9a8c\u8bc1\uff0c\u5229\u7528LLM\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u3001\u94fe\u5f0f\u63a8\u7406\u53ca\u81ea\u6211\u7ea0\u9519\u80fd\u529b\uff0c\u65e0\u9700\u7279\u5b9a\u4efb\u52a1\u8bad\u7ec3\u3002", "result": "\u7cfb\u7edf\u5728\u4e09\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5e94\u7528\u7ed3\u679c\u663e\u793a\uff0cAI\u59d4\u5458\u4f1a\u9002\u7528\u4e8e\u591a\u79cdLLM\uff0c\u6570\u636e\u5b8c\u6574\u7387\u6700\u9ad8\u8fbe78.7%\uff0c\u51c6\u786e\u7387\u8fbe100%\uff0c\u4e14\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u5404\u4ee3\u7406\u7684\u8d21\u732e\u3002", "conclusion": "AI\u59d4\u5458\u4f1a\u6709\u6548\u63d0\u5347\u4e86\u7f51\u7edc\u6570\u636e\u96c6\u7684\u8d28\u91cf\u4fdd\u969c\u80fd\u529b\uff0c\u5177\u6709\u826f\u597d\u901a\u7528\u6027\u548c\u5b9e\u7528\u4ef7\u503c\uff0c\u5e76\u4f5c\u4e3a\u5f00\u6e90\u5de5\u5177\u4f9b\u7814\u7a76\u793e\u533a\u4f7f\u7528\u3002"}}
{"id": "2512.21727", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2512.21727", "abs": "https://arxiv.org/abs/2512.21727", "authors": ["Daniil Sherki", "Daniil Merkulov", "Alexandra Savina", "Ekaterina Muravleva"], "title": "PERELMAN: Pipeline for scientific literature meta-analysis. Technical report", "comment": null, "summary": "We present PERELMAN (PipEline foR sciEntific Literature Meta-ANalysis), an agentic framework designed to extract specific information from a large corpus of scientific articles to support large-scale literature reviews and meta-analyses. Our central goal is to reliably transform heterogeneous article content into a unified, machine-readable representation. PERELMAN first elicits domain knowledge-including target variables, inclusion criteria, units, and normalization rules-through a structured dialogue with a subject-matter expert. This domain knowledge is then reused across multiple stages of the pipeline and guides coordinated agents in extracting evidence from narrative text, tables, and figures, enabling consistent aggregation across studies. In order to assess reproducibility and validate our implementation, we evaluate the system on the task of reproducing the meta-analysis of layered Li-ion cathode properties (NMC811 material). We describe our solution, which has the potential to reduce the time required to prepare meta-analyses from months to minutes.", "AI": {"tldr": "\u63d0\u51fa\u4e86PERELMAN\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u5bf9\u8bdd\u83b7\u53d6\u9886\u57df\u77e5\u8bc6\uff0c\u6307\u5bfc\u591a\u9636\u6bb5\u534f\u540c\u4ee3\u7406\u4ece\u79d1\u5b66\u6587\u732e\u4e2d\u63d0\u53d6\u4fe1\u606f\uff0c\u5b9e\u73b0\u5f02\u6784\u5185\u5bb9\u7684\u7edf\u4e00\u673a\u5668\u53ef\u8bfb\u8868\u793a\uff0c\u63d0\u5347\u5927\u89c4\u6a21\u6587\u732e\u7efc\u8ff0\u548c\u5143\u5206\u6790\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u5927\u89c4\u6a21\u79d1\u5b66\u6587\u732e\u7efc\u8ff0\u548c\u5143\u5206\u6790\u6548\u7387\u4f4e\uff0c\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u6846\u67b6\u5c06\u5f02\u8d28\u4fe1\u606f\u8f6c\u5316\u4e3a\u7edf\u4e00\u683c\u5f0f\uff0c\u51cf\u5c11\u51c6\u5907\u65f6\u95f4\u3002", "method": "\u901a\u8fc7\u4e0e\u9886\u57df\u4e13\u5bb6\u7ed3\u6784\u5316\u5bf9\u8bdd\u83b7\u53d6\u9886\u57df\u77e5\u8bc6\uff0c\u6307\u5bfc\u591a\u9636\u6bb5\u534f\u540c\u4ee3\u7406\u4ece\u6587\u672c\u3001\u8868\u683c\u548c\u56fe\u8868\u4e2d\u63d0\u53d6\u8bc1\u636e\uff0c\u5b9e\u73b0\u4fe1\u606f\u7edf\u4e00\u548c\u4e00\u81f4\u805a\u5408\u3002", "result": "\u5728\u9502\u79bb\u5b50\u7535\u6c60\u6b63\u6781\u6750\u6599\u5143\u5206\u6790\u4efb\u52a1\u4e2d\u6210\u529f\u5b9e\u73b0\u7cfb\u7edf\u590d\u73b0\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u53ef\u91cd\u590d\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "PERELMAN\u663e\u8457\u7f29\u77ed\u4e86\u5143\u5206\u6790\u51c6\u5907\u65f6\u95f4\uff0c\u6709\u6f5c\u529b\u5c06\u51c6\u5907\u65f6\u95f4\u4ece\u6570\u6708\u51cf\u5c11\u5230\u6570\u5206\u949f\uff0c\u63d0\u9ad8\u4e86\u6587\u732e\u7efc\u8ff0\u548c\u5143\u5206\u6790\u7684\u6548\u7387\u3002"}}
{"id": "2512.21878", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21878", "abs": "https://arxiv.org/abs/2512.21878", "authors": ["Marc S. Montalvo", "Hamed Yaghoobian"], "title": "MASFIN: A Multi-Agent System for Decomposed Financial Reasoning and Forecasting", "comment": "Accepted to the NeurIPS 2025 Workshop on Generative AI in Finance", "summary": "Recent advances in large language models (LLMs) are transforming data-intensive domains, with finance representing a high-stakes environment where transparent and reproducible analysis of heterogeneous signals is essential. Traditional quantitative methods remain vulnerable to survivorship bias, while many AI-driven approaches struggle with signal integration, reproducibility, and computational efficiency. We introduce MASFIN, a modular multi-agent framework that integrates LLMs with structured financial metrics and unstructured news, while embedding explicit bias-mitigation protocols. The system leverages GPT-4.1-nano for reproducability and cost-efficient inference and generates weekly portfolios of 15-30 equities with allocation weights optimized for short-term performance. In an eight-week evaluation, MASFIN delivered a 7.33% cumulative return, outperforming the S&P 500, NASDAQ-100, and Dow Jones benchmarks in six of eight weeks, albeit with higher volatility. These findings demonstrate the promise of bias-aware, generative AI frameworks for financial forecasting and highlight opportunities for modular multi-agent design to advance practical, transparent, and reproducible approaches in quantitative finance.", "AI": {"tldr": "MASFIN \u5229\u7528\u591a\u667a\u80fd\u4f53\u6846\u67b6\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u91d1\u878d\u6570\u636e\uff0c\u5b9e\u73b0\u77ed\u671f\u80a1\u7968\u6295\u8d44\u7ec4\u5408\u4f18\u5316\uff0c\u5e76\u5728\u5b9e\u6d4b\u4e2d\u8868\u73b0\u4f18\u4e8e\u4e3b\u8981\u5e02\u573a\u6307\u6570\u3002", "motivation": "\u4f20\u7edf\u91cf\u5316\u65b9\u6cd5\u5b58\u5728\u5e78\u5b58\u8005\u504f\u5dee\uff0cAI\u65b9\u6cd5\u5728\u4fe1\u53f7\u6574\u5408\u548c\u53ef\u91cd\u590d\u6027\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u8bbe\u8ba1MASFIN\u6846\u67b6\uff0c\u5c06LLM\u4e0e\u7ed3\u6784\u5316\u91d1\u878d\u6307\u6807\u548c\u975e\u7ed3\u6784\u5316\u65b0\u95fb\u7ed3\u5408\uff0c\u5e76\u5d4c\u5165\u504f\u5dee\u7f13\u89e3\u673a\u5236\uff0c\u4f7f\u7528GPT-4.1-nano\u8fdb\u884c\u9ad8\u6548\u63a8\u65ad\uff0c\u751f\u6210\u6bcf\u546815-30\u53ea\u80a1\u7968\u7684\u6295\u8d44\u7ec4\u5408\u3002", "result": "\u5728\u4e3a\u671f\u516b\u5468\u7684\u8bc4\u4f30\u4e2d\uff0cMASFIN\u5b9e\u73b07.33%\u7684\u7d2f\u8ba1\u6536\u76ca\u7387\uff0c\u516d\u5468\u8d85\u8fc7S&P 500\u3001NASDAQ-100\u548cDow Jones\uff0c\u5c3d\u7ba1\u6ce2\u52a8\u6027\u8f83\u9ad8\u3002", "conclusion": "\u504f\u5dee\u610f\u8bc6\u751f\u6210\u5f0fAI\u6846\u67b6\u5728\u91d1\u878d\u9884\u6d4b\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u6a21\u5757\u5316\u591a\u667a\u80fd\u4f53\u8bbe\u8ba1\u6709\u52a9\u4e8e\u63a8\u8fdb\u91cf\u5316\u91d1\u878d\u7684\u5b9e\u7528\u6027\u3001\u900f\u660e\u6027\u548c\u53ef\u91cd\u590d\u6027\u3002"}}
{"id": "2512.21352", "categories": ["cs.SE", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.21352", "abs": "https://arxiv.org/abs/2512.21352", "authors": ["Sumanth Bharadwaj Hachalli Karanam", "Dhiwahar Adhithya Kennady"], "title": "Multi-Agent LLM Committees for Autonomous Software Beta Testing", "comment": null, "summary": "Manual software beta testing is costly and time-consuming, while single-agent large language model (LLM) approaches suffer from hallucinations and inconsistent behavior. We propose a multi-agent committee framework in which diverse vision-enabled LLMs collaborate through a three-round voting protocol to reach consensus on testing actions. The framework combines model diversity, persona-driven behavioral variation, and visual user interface understanding to systematically explore web applications. Across 84 experimental runs with 9 testing personas and 4 scenarios, multi-agent committees achieve an 89.5 percent overall task success rate. Configurations with 2 to 4 agents reach 91.7 to 100 percent success, compared to 78.0 percent for single-agent baselines, yielding improvements of 13.7 to 22.0 percentage points. At the action level, the system attains a 93.1 percent success rate with a median per-action latency of 0.71 seconds, enabling real-time and continuous integration testing. Vision-enabled agents successfully identify user interface elements, with navigation and reporting achieving 100 percent success and form filling achieving 99.2 percent success. We evaluate the framework on WebShop and OWASP benchmarks, achieving 74.7 percent success on WebShop compared to a 50.1 percent published GPT-3 baseline, and 82.0 percent success on OWASP Juice Shop security testing with coverage of 8 of the 10 OWASP Top 10 vulnerability categories. Across 20 injected regressions, the committee achieves an F1 score of 0.91 for bug detection, compared to 0.78 for single-agent baselines. The open-source implementation enables reproducible research and practical deployment of LLM-based software testing in CI/CD pipelines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u59d4\u5458\u4f1a\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u589e\u5f3a\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5408\u4f5c\u8fdb\u884c\u8f6f\u4ef6\u6d4b\u8bd5\uff0c\u663e\u8457\u63d0\u5347\u6d4b\u8bd5\u6210\u529f\u7387\u548c\u6548\u7387\u3002", "motivation": "\u4eba\u5de5\u8f6f\u4ef6\u6d4b\u8bd5\u6210\u672c\u9ad8\u3001\u8017\u65f6\uff0c\u5355\u667a\u80fd\u4f53\u6a21\u578b\u5b58\u5728\u5e7b\u89c9\u548c\u884c\u4e3a\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165\u591a\u667a\u80fd\u4f53\u59d4\u5458\u4f1a\u6846\u67b6\uff0c\u5229\u7528\u591a\u6837\u5316\u7684\u89c6\u89c9\u589e\u5f3aLLM\uff0c\u901a\u8fc7\u4e09\u8f6e\u6295\u7968\u534f\u8bae\u534f\u4f5c\u6d4b\u8bd5\uff0c\u7ed3\u5408\u6a21\u578b\u591a\u6837\u6027\u3001\u89d2\u8272\u9a71\u52a8\u884c\u4e3a\u5dee\u5f02\u548c\u754c\u9762\u7406\u89e3\u8fdb\u884c\u7cfb\u7edf\u6d4b\u8bd5\u3002", "result": "\u591a\u667a\u80fd\u4f53\u914d\u7f6e\u6d4b\u8bd5\u6210\u529f\u7387\u9ad8\u8fbe89.5%\u81f3100%\uff0c\u8d85\u8fc7\u5355\u667a\u80fd\u4f53\u57fa\u7ebf13.7\u81f322\u4e2a\u767e\u5206\u70b9\uff0c\u5177\u5907\u5b9e\u65f6\u68c0\u6d4b\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u591a\u667a\u80fd\u4f53\u89c6\u89c9\u589e\u5f3aLLM\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u81ea\u52a8\u5316\u8f6f\u4ef6\u6d4b\u8bd5\u7684\u51c6\u786e\u7387\u548c\u6548\u7387\uff0c\u652f\u6301\u5b9e\u7528\u7684CI/CD\u7ba1\u9053\u4e2d\u5e94\u7528\uff0c\u4fc3\u8fdb\u53ef\u590d\u73b0\u7814\u7a76\u548c\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2512.21347", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.21347", "abs": "https://arxiv.org/abs/2512.21347", "authors": ["V\u00edtor Mateus de Brito", "Kleinner Farias"], "title": "Understanding the Role of Large Language Models in Software Engineering: Evidence from an Industry Survey", "comment": "4 Figures, 8 Tables, Text in Portuguese", "summary": "The rapid advancement of Large Language Models (LLMs) is reshaping software engineering by profoundly influencing coding, documentation, and system maintenance practices. As these tools become deeply embedded in developers' daily workflows, understanding how they are used has become essential. This paper reports an empirical study of LLM adoption in software engineering, based on a survey of 46 industry professionals with diverse educational backgrounds and levels of experience. The results reveal positive perceptions of LLMs, particularly regarding faster resolution of technical questions, improved documentation support, and enhanced source code standardization. However, respondents also expressed concerns about cognitive dependence, security risks, and the potential erosion of technical autonomy. These findings underscore the need for critical and supervised use of LLM-based tools. By grounding the discussion in empirical evidence from industry practice, this study bridges the gap between academic discourse and real-world software development. The results provide actionable insights for developers and researchers seeking to adopt and evolve LLM-based technologies in a more effective, responsible, and secure manner, while also motivating future research on their cognitive, ethical, and organizational implications.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u8c03\u67e546\u4f4d\u4e0d\u540c\u884c\u4e1a\u7ecf\u9a8c\u7684\u8f6f\u4ef6\u5de5\u7a0b\u5e08\uff0c\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0LLMs\u5728\u63d0\u5347\u6280\u672f\u95ee\u9898\u89e3\u51b3\u901f\u5ea6\u3001\u6587\u6863\u652f\u6301\u548c\u4ee3\u7801\u89c4\u8303\u5316\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u4e5f\u5e26\u6765\u4e86\u8ba4\u77e5\u4f9d\u8d56\u3001\u5b89\u5168\u98ce\u9669\u548c\u6280\u672f\u81ea\u4e3b\u6743\u524a\u5f31\u7684\u62c5\u5fe7\u3002", "motivation": "\u968f\u7740LLMs\u6df1\u5ea6\u878d\u5165\u5f00\u53d1\u8005\u65e5\u5e38\u5de5\u4f5c\uff0c\u7406\u89e3\u5176\u5b9e\u9645\u5e94\u7528\u548c\u5f71\u54cd\u6210\u4e3a\u5fc5\u8981\u3002", "method": "\u901a\u8fc7\u5bf946\u4f4d\u4e1a\u754c\u4e13\u4e1a\u4eba\u58eb\u8fdb\u884c\u95ee\u5377\u8c03\u67e5\uff0c\u6536\u96c6\u4e0d\u540c\u80cc\u666f\u548c\u7ecf\u9a8c\u7684\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u7684\u770b\u6cd5\u3002", "result": "\u53d1\u73b0LLMs\u88ab\u79ef\u6781\u770b\u5f85\uff0c\u5c24\u5176\u5728\u52a0\u5feb\u6280\u672f\u95ee\u9898\u89e3\u51b3\u3001\u6539\u8fdb\u6587\u6863\u548c\u4ee3\u7801\u6807\u51c6\u5316\u65b9\u9762\u6709\u6548\uff0c\u4f46\u540c\u65f6\u5b58\u5728\u8ba4\u77e5\u4f9d\u8d56\u548c\u5b89\u5168\u98ce\u9669\u7b49\u95ee\u9898\u3002", "conclusion": "\u5f3a\u8c03\u5bf9LLMs\u5de5\u5177\u7684\u5ba1\u614e\u76d1\u7ba1\u548c\u4f7f\u7528\uff0c\u63a8\u52a8LLMs\u6280\u672f\u66f4\u6709\u6548\u3001\u8d1f\u8d23\u548c\u5b89\u5168\u7684\u5e94\u7528\uff0c\u5e76\u9f13\u52b1\u5bf9\u5176\u8ba4\u77e5\u3001\u4f26\u7406\u53ca\u7ec4\u7ec7\u5c42\u9762\u5f71\u54cd\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2512.21422", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21422", "abs": "https://arxiv.org/abs/2512.21422", "authors": ["Nathan Stringham", "Fateme Hashemi Chaleshtori", "Xinyuan Yan", "Zhichao Xu", "Bei Wang", "Ana Marasovi\u0107"], "title": "Teaching People LLM's Errors and Getting it Right", "comment": null, "summary": "People use large language models (LLMs) when they should not. This is partly because they see LLMs compose poems and answer intricate questions, so they understandably, but incorrectly, assume LLMs won't stumble on basic tasks like simple arithmetic. Prior work has tried to address this by clustering instance embeddings into regions where an LLM is likely to fail and automatically describing patterns in these regions. The found failure patterns are taught to users to mitigate their overreliance. Yet, this approach has not fully succeeded. In this analysis paper, we aim to understand why.\n  We first examine whether the negative result stems from the absence of failure patterns. We group instances in two datasets by their meta-labels and evaluate an LLM's predictions on these groups. We then define criteria to flag groups that are sizable and where the LLM is error-prone, and find meta-label groups that meet these criteria. Their meta-labels are the LLM's failure patterns that could be taught to users, so they do exist. We next test whether prompting and embedding-based approaches can surface these known failures. Without this, users cannot be taught about them to reduce their overreliance. We find mixed results across methods, which could explain the negative result. Finally, we revisit the final metric that measures teaching effectiveness. We propose to assess a user's ability to effectively use the given failure patterns to anticipate when an LLM is error-prone. A user study shows a positive effect from teaching with this metric, unlike the human-AI team accuracy. Our findings show that teaching failure patterns could be a viable approach to mitigating overreliance, but success depends on better automated failure-discovery methods and using metrics like ours.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u7684\u5931\u8bef\u6a21\u5f0f\u53ca\u7528\u6237\u5bf9\u5176\u8fc7\u5ea6\u4f9d\u8d56\u95ee\u9898\uff0c\u53d1\u73b0\u5931\u8bef\u6a21\u5f0f\u786e\u5b9e\u5b58\u5728\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u63ed\u793a\u8fd9\u4e9b\u6a21\u5f0f\uff0c\u5bfc\u81f4\u6559\u5b66\u6548\u679c\u6b20\u4f73\u3002\u63d0\u51fa\u65b0\u6307\u6807\u8bc4\u4ef7\u6559\u5b66\u6548\u679c\uff0c\u7528\u6237\u7814\u7a76\u663e\u793a\u901a\u8fc7\u8be5\u6307\u6807\u6559\u5b66\u80fd\u63d0\u5347\u7528\u6237\u9884\u5224LLM\u5931\u8bef\u7684\u80fd\u529b\u3002", "motivation": "\u7528\u6237\u5e38\u9519\u8bef\u5730\u8ba4\u4e3aLLM\u4e0d\u4f1a\u5728\u7b80\u5355\u4efb\u52a1\u51fa\u9519\uff0c\u5bfc\u81f4\u8fc7\u5ea6\u4f9d\u8d56\u3002\u6b64\u524d\u901a\u8fc7\u805a\u7c7b\u5931\u8d25\u5b9e\u4f8b\u5e76\u63cf\u8ff0\u6a21\u5f0f\u6765\u6559\u80b2\u7528\u6237\uff0c\u4f46\u6548\u679c\u4e0d\u4f73\uff0c\u4f5c\u8005\u65e8\u5728\u63a2\u7a76\u539f\u56e0\u3002", "method": "\u4f5c\u8005\u5206\u6790\u4e24\u6570\u636e\u96c6\u4e2d\u5e26\u6709\u5143\u6807\u7b7e\u7684\u5b9e\u4f8b\uff0c\u786e\u8ba4\u5b58\u5728LLM\u7684\u5931\u8bef\u6a21\u5f0f\u3002\u6d4b\u8bd5\u57fa\u4e8e\u63d0\u793a\u548c\u5d4c\u5165\u7684\u5931\u8d25\u68c0\u6d4b\u65b9\u6cd5\u6548\u679c\uff0c\u5e76\u63d0\u51fa\u65b0\u6559\u5b66\u6548\u679c\u8bc4\u4f30\u6307\u6807\uff0c\u8fdb\u884c\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u3002", "result": "\u53d1\u73b0\u5b58\u5728\u53ef\u6559\u7ed9\u7528\u6237\u7684\u5931\u8bef\u6a21\u5f0f\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u5b9a\u4f4d\u8fd9\u4e9b\u6a21\u5f0f\u4e0a\u6548\u679c\u4e0d\u4e00\u3002\u63d0\u51fa\u7684\u65b0\u6307\u6807\u80fd\u66f4\u597d\u53cd\u6620\u6559\u5b66\u6548\u679c\uff0c\u7528\u6237\u7814\u7a76\u663e\u793a\u6559\u5b66\u63d0\u5347\u7528\u6237\u8bc6\u522bLLM\u9519\u8bef\u7684\u80fd\u529b\u3002", "conclusion": "\u5931\u8bef\u6a21\u5f0f\u6559\u5b66\u6709\u671b\u51cf\u5c11\u7528\u6237\u5bf9LLM\u7684\u8fc7\u5ea6\u4f9d\u8d56\uff0c\u4f46\u9700\u6539\u8fdb\u81ea\u52a8\u5316\u5931\u8bef\u53d1\u73b0\u6280\u672f\u548c\u91c7\u7528\u5408\u7406\u8bc4\u4f30\u6307\u6807\u6765\u63d0\u5347\u6559\u5b66\u6548\u679c\u3002"}}
{"id": "2512.21818", "categories": ["cs.SE", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.21818", "abs": "https://arxiv.org/abs/2512.21818", "authors": ["Brian Bowers", "Smita Khapre", "Jugal Kalita"], "title": "Analyzing Code Injection Attacks on LLM-based Multi-Agent Systems in Software Development", "comment": null, "summary": "Agentic AI and Multi-Agent Systems are poised to dominate industry and society imminently. Powered by goal-driven autonomy, they represent a powerful form of generative AI, marking a transition from reactive content generation into proactive multitasking capabilities. As an exemplar, we propose an architecture of a multi-agent system for the implementation phase of the software engineering process. We also present a comprehensive threat model for the proposed system. We demonstrate that while such systems can generate code quite accurately, they are vulnerable to attacks, including code injection. Due to their autonomous design and lack of humans in the loop, these systems cannot identify and respond to attacks by themselves. This paper analyzes the vulnerability of multi-agent systems and concludes that the coder-reviewer-tester architecture is more resilient than both the coder and coder-tester architectures, but is less efficient at writing code. We find that by adding a security analysis agent, we mitigate the loss in efficiency while achieving even better resiliency. We conclude by demonstrating that the security analysis agent is vulnerable to advanced code injection attacks, showing that embedding poisonous few-shot examples in the injected code can increase the attack success rate from 0% to 71.95%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u8f6f\u4ef6\u5b9e\u65bd\u67b6\u6784\uff0c\u5206\u6790\u4e86\u7cfb\u7edf\u5728\u751f\u6210\u4ee3\u7801\u65f6\u7684\u5b89\u5168\u98ce\u9669\uff0c\u5e76\u901a\u8fc7\u5f15\u5165\u5b89\u5168\u5206\u6790\u667a\u80fd\u4f53\u63d0\u5347\u7cfb\u7edf\u7684\u6297\u653b\u51fb\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u57fa\u4e8e\u76ee\u6807\u9a71\u52a8\u81ea\u4e3b\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u5e7f\u6cdb\u5e94\u7528\uff0c\u5982\u4f55\u4fdd\u969c\u8fd9\u4e9b\u81ea\u52a8\u5316\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u5b89\u5168\u6027\u6210\u4e3a\u6838\u5fc3\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u5e26\u6709\u7f16\u7801\u8005\u3001\u5ba1\u67e5\u8005\u3001\u6d4b\u8bd5\u8005\u4ee5\u53ca\u5b89\u5168\u5206\u6790\u8005\u7684\u591a\u667a\u80fd\u4f53\u534f\u540c\u4f53\u7cfb\u7ed3\u6784\uff0c\u5e76\u6784\u5efa\u4e86\u5168\u9762\u7684\u5a01\u80c1\u6a21\u578b\uff0c\u5206\u6790\u4e0d\u540c\u67b6\u6784\u4e0b\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u4e0e\u6548\u7387\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u7f16\u7801\u8005-\u5ba1\u67e5\u8005-\u6d4b\u8bd5\u8005\u67b6\u6784\u8f83\u5355\u4e00\u548c\u53cc\u5143\u67b6\u6784\u66f4\u5177\u97e7\u6027\uff0c\u4f46\u6548\u7387\u964d\u4f4e\u3002\u5f15\u5165\u5b89\u5168\u5206\u6790\u667a\u80fd\u4f53\u540e\uff0c\u6548\u7387\u63d0\u5347\u4e14\u97e7\u6027\u8fdb\u4e00\u6b65\u589e\u5f3a\u3002\u7136\u800c\uff0c\u5b89\u5168\u5206\u6790\u667a\u80fd\u4f53\u4f9d\u7136\u5b58\u5728\u9ad8\u7ea7\u4ee3\u7801\u6ce8\u5165\u653b\u51fb\u7684\u98ce\u9669\uff0c\u653b\u51fb\u6210\u529f\u7387\u53ef\u9ad8\u8fbe71.95%\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u81ea\u52a8\u4ee3\u7801\u751f\u6210\u4e2d\u5b58\u5728\u663e\u8457\u5b89\u5168\u98ce\u9669\uff0c\u5408\u7406\u8bbe\u8ba1\u591a\u89d2\u8272\u67b6\u6784\u5e76\u589e\u8bbe\u5b89\u5168\u5206\u6790\u667a\u80fd\u4f53\u53ef\u63d0\u5347\u6297\u653b\u51fb\u80fd\u529b\uff0c\u4f46\u9ad8\u7ea7\u653b\u51fb\u4ecd\u9700\u88ab\u91cd\u70b9\u9632\u8303\u3002"}}
{"id": "2512.21348", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21348", "abs": "https://arxiv.org/abs/2512.21348", "authors": ["Ying Xiao", "Shangwen Wang", "Sicen Liu", "Dingyuan Xue", "Xian Zhan", "Yepang Liu", "Jie M. Zhang"], "title": "Fairness Is Not Just Ethical: Performance Trade-Off via Data Correlation Tuning to Mitigate Bias in ML Software", "comment": "Accepted to the 48th International Conference on Software Engineering (ICSE 2026) Research Track", "summary": "Traditional software fairness research typically emphasizes ethical and social imperatives, neglecting that fairness fundamentally represents a core software quality issue arising directly from performance disparities across sensitive user groups. Recognizing fairness explicitly as a software quality dimension yields practical benefits beyond ethical considerations, notably improved predictive performance for unprivileged groups, enhanced out-of-distribution generalization, and increased geographic transferability in real-world deployments. Nevertheless, existing bias mitigation methods face a critical dilemma: while pre-processing methods offer broad applicability across model types, they generally fall short in effectiveness compared to post-processing techniques. To overcome this challenge, we propose Correlation Tuning (CoT), a novel pre-processing approach designed to mitigate bias by adjusting data correlations. Specifically, CoT introduces the Phi-coefficient, an intuitive correlation measure, to systematically quantify correlation between sensitive attributes and labels, and employs multi-objective optimization to address the proxy biases. Extensive evaluations demonstrate that CoT increases the true positive rate of unprivileged groups by an average of 17.5% and reduces three key bias metrics, including statistical parity difference (SPD), average odds difference (AOD), and equal opportunity difference (EOD), by more than 50% on average. CoT outperforms state-of-the-art methods by three and ten percentage points in single attribute and multiple attributes scenarios, respectively. We will publicly release our experimental results and source code to facilitate future research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u76f8\u5173\u6027\u8c03\u4f18\uff08CoT\uff09\u7684\u65b0\u578b\u9884\u5904\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c03\u6574\u6570\u636e\u4e2d\u7684\u76f8\u5173\u6027\u4ee5\u51cf\u8f7b\u6a21\u578b\u504f\u89c1\uff0c\u63d0\u5347\u5f31\u52bf\u7fa4\u4f53\u7684\u9884\u6d4b\u51c6\u786e\u7387\u548c\u516c\u5e73\u6027\u6307\u6807\u8868\u73b0\u3002", "motivation": "\u4f20\u7edf\u8f6f\u4ef6\u516c\u5e73\u6027\u7814\u7a76\u591a\u6ce8\u91cd\u4f26\u7406\u548c\u793e\u4f1a\u5c42\u9762\uff0c\u5ffd\u89c6\u4e86\u516c\u5e73\u6027\u4f5c\u4e3a\u8f6f\u4ef6\u6838\u5fc3\u8d28\u91cf\u95ee\u9898\u7684\u672c\u8d28\uff0c\u5373\u6027\u80fd\u5728\u654f\u611f\u7528\u6237\u7fa4\u4f53\u95f4\u7684\u5dee\u5f02\u3002\u73b0\u6709\u504f\u89c1\u7f13\u89e3\u65b9\u6cd5\u9762\u4e34\u9884\u5904\u7406\u65b9\u6cd5\u9002\u7528\u5e7f\u6cdb\u4f46\u6548\u679c\u4e0d\u4f73\uff0c\u540e\u5904\u7406\u65b9\u6cd5\u6548\u679c\u597d\u4f46\u9002\u7528\u8303\u56f4\u53d7\u9650\u7684\u56f0\u5883\u3002", "method": "\u63d0\u51faPhi\u7cfb\u6570\u4f5c\u4e3a\u8861\u91cf\u654f\u611f\u5c5e\u6027\u4e0e\u6807\u7b7e\u76f8\u5173\u6027\u7684\u6307\u6807\uff0c\u5229\u7528\u591a\u76ee\u6807\u4f18\u5316\u8c03\u6574\u6570\u636e\u76f8\u5173\u6027\u4ee5\u7f13\u89e3\u4ee3\u7406\u504f\u89c1\uff0c\u5f62\u6210\u65b0\u7684\u9884\u5904\u7406\u65b9\u6cd5CoT\u3002", "result": "CoT\u5728\u63d0\u5347\u5f31\u52bf\u7fa4\u4f53\u771f\u6b63\u4f8b\u7387\u5e73\u574717.5%\uff0c\u5e76\u5728\u7edf\u8ba1\u516c\u5e73\u5dee\u5f02\u3001\u5e73\u5747\u8d54\u7387\u5dee\u5f02\u548c\u5e73\u7b49\u673a\u4f1a\u5dee\u5f02\u4e09\u4e2a\u5173\u952e\u6307\u6807\u4e0a\u5e73\u5747\u51cf\u5c11\u8d85\u8fc750%\u3002\u5728\u5355\u5c5e\u6027\u548c\u591a\u5c5e\u6027\u573a\u666f\u4e0b\u5206\u522b\u6bd4\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u63d0\u53473\u4e2a\u767e\u5206\u70b9\u548c10\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u5c06\u516c\u5e73\u6027\u660e\u786e\u89c6\u4e3a\u8f6f\u4ef6\u8d28\u91cf\u7ef4\u5ea6\uff0c\u91c7\u7528CoT\u65b9\u6cd5\u80fd\u6709\u6548\u7f13\u89e3\u504f\u89c1\uff0c\u63d0\u5347\u6a21\u578b\u5728\u5f31\u52bf\u7fa4\u4f53\u4e2d\u7684\u8868\u73b0\u53ca\u6cdb\u5316\u80fd\u529b\uff0c\u63a8\u52a8\u516c\u5e73\u6027\u7814\u7a76\u4ece\u4f26\u7406\u63a2\u8ba8\u5411\u5b9e\u9645\u6027\u80fd\u4f18\u5316\u8f6c\u53d8\u3002"}}
{"id": "2512.21439", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.21439", "abs": "https://arxiv.org/abs/2512.21439", "authors": ["Geoffroy Morlat", "Marceau Nahon", "Augustin Chartouny", "Raja Chatila", "Ismael T. Freire", "Mehdi Khamassi"], "title": "Morality is Contextual: Learning Interpretable Moral Contexts from Human Data with Probabilistic Clustering and Large Language Models", "comment": "11 pages, 5 figures, +24 pages of Appendix", "summary": "Moral actions are judged not only by their outcomes but by the context in which they occur. We present COMETH (Contextual Organization of Moral Evaluation from Textual Human inputs), a framework that integrates a probabilistic context learner with LLM-based semantic abstraction and human moral evaluations to model how context shapes the acceptability of ambiguous actions. We curate an empirically grounded dataset of 300 scenarios across six core actions (violating Do not kill, Do not deceive, and Do not break the law) and collect ternary judgments (Blame/Neutral/Support) from N=101 participants. A preprocessing pipeline standardizes actions via an LLM filter and MiniLM embeddings with K-means, producing robust, reproducible core-action clusters. COMETH then learns action-specific moral contexts by clustering scenarios online from human judgment distributions using principled divergence criteria. To generalize and explain predictions, a Generalization module extracts concise, non-evaluative binary contextual features and learns feature weights in a transparent likelihood-based model. Empirically, COMETH roughly doubles alignment with majority human judgments relative to end-to-end LLM prompting (approx. 60% vs. approx. 30% on average), while revealing which contextual features drive its predictions. The contributions are: (i) an empirically grounded moral-context dataset, (ii) a reproducible pipeline combining human judgments with model-based context learning and LLM semantics, and (iii) an interpretable alternative to end-to-end LLMs for context-sensitive moral prediction and explanation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86COMETH\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u4eba\u7c7b\u9053\u5fb7\u8bc4\u5224\uff0c\u5efa\u6a21\u60c5\u5883\u5bf9\u6a21\u7cca\u884c\u4e3a\u9053\u5fb7\u63a5\u53d7\u5ea6\u7684\u5f71\u54cd\u3002", "motivation": "\u9053\u5fb7\u884c\u4e3a\u7684\u8bc4\u5224\u4e0d\u4ec5\u53d6\u51b3\u4e8e\u7ed3\u679c\uff0c\u8fd8\u4f9d\u8d56\u4e8e\u53d1\u751f\u7684\u5177\u4f53\u60c5\u5883\uff0c\u800c\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u5145\u5206\u6355\u6349\u60c5\u5883\u5bf9\u9053\u5fb7\u8bc4\u5224\u7684\u4f5c\u7528\u3002", "method": "\u6784\u5efa\u5305\u542b300\u4e2a\u60c5\u5883\u548c101\u4eba\u4e09\u5143\u5224\u65ad\u7684\u6570\u636e\u96c6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u9884\u5904\u7406\u5e76\u805a\u7c7b\u6838\u5fc3\u884c\u4e3a\uff0c\u5728\u7ebf\u5b66\u4e60\u884c\u52a8\u7279\u5b9a\u7684\u9053\u5fb7\u60c5\u5883\uff0c\u901a\u8fc7\u63d0\u53d6\u975e\u8bc4\u5224\u6027\u7684\u4e8c\u5143\u60c5\u5883\u7279\u5f81\u53ca\u900f\u660e\u7684\u4f3c\u7136\u6a21\u578b\u8fdb\u884c\u901a\u7528\u5316\u548c\u89e3\u91ca\u3002", "result": "COMETH\u5728\u5bf9\u4eba\u7c7b\u591a\u6570\u5224\u65ad\u7684\u7b26\u5408\u5ea6\u4e0a\u5927\u7ea6\u662f\u7aef\u5230\u7aef\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u793a\u7684\u4e24\u500d\uff08\u7ea660%\u5bf9\u6bd4\u7ea630%\uff09\uff0c\u5e76\u80fd\u63ed\u793a\u5f71\u54cd\u9884\u6d4b\u7684\u5173\u952e\u60c5\u5883\u7279\u5f81\u3002", "conclusion": "COMETH\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7ed3\u5408\u4eba\u7c7b\u5224\u65ad\u4e0e\u6a21\u578b\u5b66\u4e60\u7684\u3001\u53ef\u89e3\u91ca\u4e14\u60c5\u5883\u654f\u611f\u7684\u9053\u5fb7\u8bc4\u5224\u6846\u67b6\uff0c\u4f18\u4e8e\u4f20\u7edf\u7aef\u5230\u7aef\u5927\u8bed\u8a00\u6a21\u578b\u3002"}}
{"id": "2512.21351", "categories": ["cs.SE", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2512.21351", "abs": "https://arxiv.org/abs/2512.21351", "authors": ["Santhosh Kumar Ravindran"], "title": "CosmoCore-Evo: Evolutionary Dream-Replay Reinforcement Learning for Adaptive Code Generation", "comment": "10 pages, 2 figures; Code for Simulation", "summary": "Building on the affective dream-replay reinforcement learning framework of CosmoCore, we introduce CosmoCore-Evo, an extension that incorporates evolutionary algorithms to enhance adaptability and novelty in code generation tasks. Inspired by anthropological aspects of human evolution, such as natural selection and adaptation in early hominids, CosmoCore-Evo treats RL trajectories as ``genomes'' that undergo mutation and selection during the nocturnal replay phase. This mechanism allows agents to break free from trained patterns, fostering emergent behaviors and improved performance in distribution-shifted environments, such as changing APIs or novel libraries. We augment the Dream Queue with evolutionary operations, including mutation of high-fitness trajectories and enterprise-tuned fitness functions that incorporate efficiency, compliance, and scalability metrics. Evaluated on extended benchmarks including HumanEval variants with shifts, BigCodeBench, and a custom PySpark pipeline simulation, CosmoCore-Evo achieves up to 35% higher novelty in solutions and 25% faster adaptation compared to the original CosmoCore and baselines like PPO and REAMER. Ablations confirm the role of evolutionary components in bridging the sentient gap for LLM agents. Code for replication, including a toy simulation, is provided.", "AI": {"tldr": "CosmoCore-Evo\u5728CosmoCore\u7684\u57fa\u7840\u4e0a\u5f15\u5165\u8fdb\u5316\u7b97\u6cd5\uff0c\u901a\u8fc7\u57fa\u56e0\u53d8\u5f02\u548c\u9009\u62e9\u673a\u5236\u63d0\u5347\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u4ee3\u7406\u7684\u9002\u5e94\u6027\u548c\u521b\u65b0\u6027\uff0c\u5b9e\u73b0\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u9488\u5bf9\u4ee3\u7801\u751f\u6210\u4e2d\u73af\u5883\u5206\u5e03\u53d8\u5316\uff08\u5982API\u6216\u5e93\u53d8\u5316\uff09\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u501f\u9274\u4eba\u7c7b\u8fdb\u5316\u4e2d\u7684\u81ea\u7136\u9009\u62e9\u548c\u9002\u5e94\u673a\u5236\uff0c\u63d0\u5347\u6a21\u578b\u7684\u5e94\u5bf9\u80fd\u529b\u548c\u521b\u65b0\u6c34\u5e73\u3002", "method": "\u5c06\u5f3a\u5316\u5b66\u4e60\u8f68\u8ff9\u89c6\u4e3a\u57fa\u56e0\uff0c\u901a\u8fc7\u591c\u95f4\u56de\u653e\u9636\u6bb5\u5bf9\u8f68\u8ff9\u8fdb\u884c\u53d8\u5f02\u548c\u9009\u62e9\uff0c\u7ed3\u5408\u589e\u5f3a\u7684Dream Queue\u548c\u4f01\u4e1a\u5b9a\u5236\u7684\u9002\u5e94\u5ea6\u51fd\u6570\uff08\u6548\u7387\u3001\u5408\u89c4\u6027\u3001\u53ef\u6269\u5c55\u6027\u6307\u6807\uff09\u5b9e\u73b0\u8fdb\u5316\u64cd\u4f5c\u3002", "result": "\u5728\u591a\u9879\u6269\u5c55\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982HumanEval\u53d8\u4f53\u3001BigCodeBench\u548cPySpark\u6d41\u6c34\u7ebf\u6a21\u62df\uff09\u4e2d\uff0cCosmoCore-Evo\u5b9e\u73b0\u4e86\u89e3\u51b3\u65b9\u6848\u65b0\u9896\u5ea6\u63d0\u534735%\u548c\u9002\u5e94\u901f\u5ea6\u63d0\u534725%\uff0c\u663e\u8457\u4f18\u4e8eCosmoCore\u53caPPO\u3001REAMER\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8fdb\u5316\u673a\u5236\u6709\u6548\u4fc3\u8fdb\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u7684\u81ea\u6211\u7a81\u7834\u548c\u9002\u5e94\u80fd\u529b\uff0c\u7f29\u5c0f\u4e86\u667a\u80fd\u5dee\u8ddd\uff0c\u9a8c\u8bc1\u4e86\u5c06\u751f\u7269\u8fdb\u5316\u601d\u60f3\u878d\u5165\u5f3a\u5316\u5b66\u4e60\u7684\u6f5c\u529b\uff0c\u4e14\u4ee3\u7801\u53ca\u6a21\u62df\u73af\u5883\u5df2\u5f00\u6e90\u4fbf\u4e8e\u590d\u73b0\u3002"}}
{"id": "2512.21494", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21494", "abs": "https://arxiv.org/abs/2512.21494", "authors": ["Soichiro Murakami", "Hidetaka Kamigaito", "Hiroya Takamura", "Manabu Okumura"], "title": "Oogiri-Master: Benchmarking Humor Understanding via Oogiri", "comment": null, "summary": "Humor is a salient testbed for human-like creative thinking in large language models (LLMs). We study humor using the Japanese creative response game Oogiri, in which participants produce witty responses to a given prompt, and ask the following research question: What makes such responses funny to humans? Previous work has offered only limited reliable means to answer this question. Existing datasets contain few candidate responses per prompt, expose popularity signals during ratings, and lack objective and comparable metrics for funniness. Thus, we introduce Oogiri-Master and Oogiri-Corpus, which are a benchmark and dataset designed to enable rigorous evaluation of humor understanding in LLMs. Each prompt is paired with approximately 100 diverse candidate responses, and funniness is rated independently by approximately 100 human judges without access to others' ratings, reducing popularity bias and enabling robust aggregation. Using Oogiri-Corpus, we conduct a quantitative analysis of the linguistic factors associated with funniness, such as text length, ambiguity, and incongruity resolution, and derive objective metrics for predicting human judgments. Subsequently, we benchmark a range of LLMs and human baselines in Oogiri-Master, demonstrating that state-of-the-art models approach human performance and that insight-augmented prompting improves the model performance. Our results provide a principled basis for evaluating and advancing humor understanding in LLMs.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5f15\u5165\u65e5\u672c\u521b\u610f\u53cd\u5e94\u6e38\u620fOogiri\u7684\u57fa\u51c6\u548c\u6570\u636e\u96c6\uff0c\u7cfb\u7edf\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5982\u4f55\u7406\u89e3\u5e7d\u9ed8\uff0c\u5e76\u63d0\u51fa\u4e86\u8bc4\u4f30\u5e7d\u9ed8\u611f\u7684\u5ba2\u89c2\u6307\u6807\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5728\u8bc4\u4f30\u5e7d\u9ed8\u611f\u65f6\u6570\u636e\u6709\u9650\u3001\u8bc4\u4ef7\u5b58\u5728\u504f\u89c1\u4e14\u7f3a\u4e4f\u5ba2\u89c2\u6307\u6807\uff0c\u96be\u4ee5\u51c6\u786e\u7406\u89e3\u8bed\u8a00\u6a21\u578b\u5bf9\u5e7d\u9ed8\u7684\u628a\u63e1\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b\u6bcf\u4e2a\u63d0\u793a\u7ea6100\u4e2a\u591a\u6837\u5316\u56de\u5e94\u7684Oogiri-Master\u57fa\u51c6\u548cOogiri-Corpus\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u7ea6100\u540d\u72ec\u7acb\u8bc4\u5ba1\u8005\u8bc4\u5206\uff0c\u5206\u6790\u8bed\u8a00\u7279\u5f81\u4e0e\u5e7d\u9ed8\u611f\u7684\u5173\u8054\uff0c\u63d0\u51fa\u9884\u6d4b\u4eba\u7c7b\u5e7d\u9ed8\u5224\u65ad\u7684\u5ba2\u89c2\u6307\u6807\uff0c\u57fa\u51c6\u6d4b\u8bd5\u591a\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u4eba\u7c7b\u8868\u73b0\u3002", "result": "\u7814\u7a76\u8868\u660e\u6587\u672c\u957f\u5ea6\u3001\u6b67\u4e49\u548c\u4e0d\u534f\u8c03\u6027\u89e3\u51b3\u662f\u5e7d\u9ed8\u611f\u7684\u91cd\u8981\u8bed\u8a00\u56e0\u7d20\uff1b\u6700\u5148\u8fdb\u6a21\u578b\u5728\u5e7d\u9ed8\u7406\u89e3\u4e0a\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\uff0c\u5e76\u4e14\u901a\u8fc7\u589e\u6dfb\u6d1e\u5bdf\u63d0\u793a\u80fd\u591f\u8fdb\u4e00\u6b65\u63d0\u5347\u6a21\u578b\u8868\u73b0\u3002", "conclusion": "\u672c\u6587\u4e3a\u5e7d\u9ed8\u7406\u89e3\u7684\u8bc4\u4f30\u548c\u63d0\u5347\u63d0\u4f9b\u4e86\u79d1\u5b66\u7684\u57fa\u51c6\u548c\u65b9\u6cd5\uff0c\u63a8\u52a8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4eba\u7c7b\u7c7b\u521b\u9020\u6027\u601d\u7ef4\u9886\u57df\u7684\u8fdb\u6b65\u3002"}}
{"id": "2512.21567", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.21567", "abs": "https://arxiv.org/abs/2512.21567", "authors": ["Changzhi Sun", "Xiangyu Chen", "Jixiang Luo", "Dell Zhang", "Xuelong Li"], "title": "Beyond Heuristics: A Decision-Theoretic Framework for Agent Memory Management", "comment": null, "summary": "External memory is a key component of modern large language model (LLM) systems, enabling long-term interaction and personalization. Despite its importance, memory management is still largely driven by hand-designed heuristics, offering little insight into the long-term and uncertain consequences of memory decisions. In practice, choices about what to read or write shape future retrieval and downstream behavior in ways that are difficult to anticipate. We argue that memory management should be viewed as a sequential decision-making problem under uncertainty, where the utility of memory is delayed and dependent on future interactions. To this end, we propose DAM (Decision-theoretic Agent Memory), a decision-theoretic framework that decomposes memory management into immediate information access and hierarchical storage maintenance. Within this architecture, candidate operations are evaluated via value functions and uncertainty estimators, enabling an aggregate policy to arbitrate decisions based on estimated long-term utility and risk. Our contribution is not a new algorithm, but a principled reframing that clarifies the limitations of heuristic approaches and provides a foundation for future research on uncertainty-aware memory systems.", "AI": {"tldr": "\u5916\u90e8\u8bb0\u5fc6\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u7684\u8bb0\u5fc6\u7ba1\u7406\u591a\u4f9d\u8d56\u4e8e\u624b\u5de5\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u7f3a\u4e4f\u5bf9\u957f\u671f\u548c\u4e0d\u786e\u5b9a\u6027\u540e\u679c\u7684\u7406\u89e3\u3002\u672c\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u51b3\u7b56\u7406\u8bba\u7684\u8bb0\u5fc6\u7ba1\u7406\u6846\u67b6DAM\uff0c\u901a\u8fc7\u8bc4\u4f30\u8bb0\u5fc6\u64cd\u4f5c\u7684\u957f\u671f\u6548\u7528\u548c\u98ce\u9669\uff0c\u5b9e\u73b0\u4e86\u8bb0\u5fc6\u7ba1\u7406\u7684\u7406\u6027\u51b3\u7b56\u3002", "motivation": "\u5f53\u524d\u5916\u90e8\u8bb0\u5fc6\u7ba1\u7406\u4f9d\u8d56\u624b\u5de5\u8bbe\u8ba1\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u7f3a\u4e4f\u5bf9\u8bb0\u5fc6\u51b3\u7b56\u957f\u671f\u5f71\u54cd\u548c\u4e0d\u786e\u5b9a\u6027\u7684\u8ba4\u8bc6\uff0c\u9650\u5236\u4e86\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u4e2a\u6027\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u51b3\u7b56\u7406\u8bba\u6846\u67b6DAM\uff0c\u5c06\u8bb0\u5fc6\u7ba1\u7406\u5206\u89e3\u4e3a\u5373\u65f6\u4fe1\u606f\u8bbf\u95ee\u548c\u5206\u5c42\u5b58\u50a8\u7ef4\u62a4\uff0c\u901a\u8fc7\u4ef7\u503c\u51fd\u6570\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5bf9\u5019\u9009\u64cd\u4f5c\u8fdb\u884c\u8bc4\u4f30\uff0c\u57fa\u4e8e\u957f\u671f\u6548\u7528\u548c\u98ce\u9669\u5236\u5b9a\u51b3\u7b56\u7b56\u7565\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u80fd\u591f\u8bc4\u4f30\u957f\u671f\u6548\u7528\u548c\u98ce\u9669\u7684\u8bb0\u5fc6\u7ba1\u7406\u67b6\u6784\uff0c\u4f7f\u8bb0\u5fc6\u51b3\u7b56\u66f4\u52a0\u7406\u6027\u548c\u6709\u6548\uff0c\u63ed\u793a\u4e86\u542f\u53d1\u5f0f\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684DAM\u6846\u67b6\u4e3a\u8bb0\u5fc6\u7ba1\u7406\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5f3a\u8c03\u4e86\u51b3\u7b56\u7406\u8bba\u89c6\u89d2\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u8bb0\u5fc6\u7cfb\u7edf\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2512.21373", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2512.21373", "abs": "https://arxiv.org/abs/2512.21373", "authors": ["Titouan Duston", "Shuo Xin", "Yang Sun", "Daoguang Zan", "Aoyan Li", "Shulin Xin", "Kai Shen", "Yixiao Chen", "Qiming Sun", "Ge Zhang", "Jiashuo Liu", "Huan Zhou", "Jingkai Liu", "Zhichen Pu", "Yuanheng Wang", "Bo-Xuan Ge", "Xin Tong", "Fei Ye", "Zhi-Chao Zhao", "Wen-Biao Han", "Zhoujian Cao", "Yueran Zhao", "Weiluo Ren", "Qingshen Long", "Yuxiao Liu", "Anni Huang", "Yidi Du", "Yuanyuan Rong", "Jiahao Peng"], "title": "AInsteinBench: Benchmarking Coding Agents on Scientific Repositories", "comment": null, "summary": "We introduce AInsteinBench, a large-scale benchmark for evaluating whether large language model (LLM) agents can operate as scientific computing development agents within real research software ecosystems. Unlike existing scientific reasoning benchmarks which focus on conceptual knowledge, or software engineering benchmarks that emphasize generic feature implementation and issue resolving, AInsteinBench evaluates models in end-to-end scientific development settings grounded in production-grade scientific repositories. The benchmark consists of tasks derived from maintainer-authored pull requests across six widely used scientific codebases, spanning quantum chemistry, quantum computing, molecular dynamics, numerical relativity, fluid dynamics, and cheminformatics. All benchmark tasks are carefully curated through multi-stage filtering and expert review to ensure scientific challenge, adequate test coverage, and well-calibrated difficulty. By leveraging evaluation in executable environments, scientifically meaningful failure modes, and test-driven verification, AInsteinBench measures a model's ability to move beyond surface-level code generation toward the core competencies required for computational scientific research.", "AI": {"tldr": "AInsteinBench\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u771f\u5b9e\u79d1\u7814\u8f6f\u4ef6\u751f\u6001\u4e2d\u4f5c\u4e3a\u79d1\u5b66\u8ba1\u7b97\u5f00\u53d1\u4ee3\u7406\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u6982\u5ff5\u77e5\u8bc6\u6216\u901a\u7528\u8f6f\u4ef6\u5de5\u7a0b\u95ee\u9898\uff0c\u7f3a\u4e4f\u5728\u771f\u5b9e\u79d1\u7814\u73af\u5883\u4e2d\u7aef\u5230\u7aef\u79d1\u5b66\u5f00\u53d1\u7684\u8bc4\u4f30\u3002", "method": "\u57fa\u51c6\u6db5\u76d6\u516d\u4e2a\u79d1\u5b66\u4ee3\u7801\u5e93\u4e2d\u7684\u7ef4\u62a4\u8005\u8d21\u732e\u7684\u4efb\u52a1\uff0c\u7ecf\u8fc7\u591a\u9636\u6bb5\u7b5b\u9009\u548c\u4e13\u5bb6\u8bc4\u5ba1\uff0c\u5728\u53ef\u6267\u884c\u73af\u5883\u4e2d\u901a\u8fc7\u6d4b\u8bd5\u9a71\u52a8\u9a8c\u8bc1\u79d1\u5b66\u8ba1\u7b97\u80fd\u529b\u3002", "result": "\u8be5\u57fa\u51c6\u80fd\u63ed\u793a\u6a21\u578b\u5728\u79d1\u5b66\u5f00\u53d1\u4e2d\u7684\u6838\u5fc3\u80fd\u529b\uff0c\u5305\u62ec\u8d85\u8d8a\u8868\u5c42\u4ee3\u7801\u751f\u6210\u7684\u6df1\u5ea6\u7406\u89e3\u548c\u79d1\u5b66\u610f\u4e49\u7684\u5931\u8d25\u6a21\u5f0f\u3002", "conclusion": "AInsteinBench\u6709\u6548\u8861\u91cf\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u771f\u5b9e\u79d1\u7814\u8f6f\u4ef6\u73af\u5883\u4e2d\u7684\u7efc\u5408\u5f00\u53d1\u80fd\u529b\uff0c\u63a8\u52a8\u79d1\u5b66\u8ba1\u7b97\u9886\u57df\u667a\u80fd\u4ee3\u7406\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.21577", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.21577", "abs": "https://arxiv.org/abs/2512.21577", "authors": ["Emmy Liu", "Varun Gangal", "Chelsea Zou", "Xiaoqi Huang", "Michael Yu", "Alex Chang", "Zhuofu Tao", "Sachin Kumar", "Steven Y. Feng"], "title": "A Unified Definition of Hallucination, Or: It's the World Model, Stupid", "comment": null, "summary": "Despite numerous attempts to solve the issue of hallucination since the inception of neural language models, it remains a problem in even frontier large language models today. Why is this the case? We walk through definitions of hallucination used in the literature from a historical perspective up to the current day, and fold them into a single definition of hallucination, wherein different prior definitions focus on different aspects of our definition. At its core, we argue that hallucination is simply inaccurate (internal) world modeling, in a form where it is observable to the user (e.g., stating a fact which contradicts a knowledge base, or producing a summary which contradicts a known source). By varying the reference world model as well as the knowledge conflict policy (e.g., knowledge base vs. in-context), we arrive at the different existing definitions of hallucination present in the literature.\n  We argue that this unified view is useful because it forces evaluations to make clear their assumed \"world\" or source of truth, clarifies what should and should not be called hallucination (as opposed to planning or reward/incentive-related errors), and provides a common language to compare benchmarks and mitigation techniques. Building on this definition, we outline plans for a family of benchmarks in which hallucinations are defined as mismatches with synthetic but fully specified world models in different environments, and sketch out how these benchmarks can use such settings to stress-test and improve the world modeling components of language models.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5386\u53f2\u548c\u73b0\u4ee3\u89c6\u89d2\u7edf\u4e00\u5b9a\u4e49\u4e86\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u73b0\u8c61\uff0c\u8ba4\u4e3a\u5e7b\u89c9\u662f\u5185\u90e8\u4e16\u754c\u5efa\u6a21\u4e0d\u51c6\u786e\u7684\u8868\u73b0\u3002", "motivation": "\u5c3d\u7ba1\u5927\u91cf\u7814\u7a76\u5c1d\u8bd5\u89e3\u51b3\u795e\u7ecf\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u4f46\u5373\u4f7f\u5728\u6700\u524d\u6cbf\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u5e7b\u89c9\u4f9d\u7136\u5b58\u5728\uff0c\u672c\u6587\u65e8\u5728\u627e\u5230\u5176\u6839\u672c\u539f\u56e0\u5e76\u7edf\u4e00\u76f8\u5173\u5b9a\u4e49\u3002", "method": "\u6587\u4e2d\u56de\u987e\u5e76\u6574\u5408\u4e86\u6587\u732e\u4e2d\u5173\u4e8e\u5e7b\u89c9\u7684\u591a\u79cd\u5b9a\u4e49\uff0c\u63d0\u51fa\u4ee5\u5185\u90e8\u4e16\u754c\u6a21\u578b\u4e0d\u51c6\u786e\u4e3a\u6838\u5fc3\u7684\u7edf\u4e00\u5b9a\u4e49\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bbe\u60f3\u4e86\u4e00\u7cfb\u5217\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4ee5\u8bc4\u4f30\u548c\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u4e16\u754c\u5efa\u6a21\u80fd\u529b\u3002", "result": "\u901a\u8fc7\u7edf\u4e00\u89c6\u89d2\uff0c\u660e\u786e\u4e86\u5e7b\u89c9\u8bc4\u4f30\u6240\u4f9d\u8d56\u7684\u201c\u4e16\u754c\u6a21\u578b\u201d\u53ca\u4fe1\u606f\u51b2\u7a81\u7b56\u7565\uff0c\u6f84\u6e05\u4e86\u5e7b\u89c9\u4e0e\u5176\u4ed6\u9519\u8bef\u7684\u754c\u9650\uff0c\u5e76\u4e3a\u6bd4\u8f83\u57fa\u51c6\u548c\u7f13\u89e3\u7b56\u7565\u63d0\u4f9b\u4e86\u5171\u540c\u8bed\u8a00\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u7edf\u4e00\u5b9a\u4e49\u6709\u52a9\u4e8e\u89c4\u8303\u5e7b\u89c9\u7684\u8bc4\u4f30\u6807\u51c6\u548c\u7814\u7a76\u65b9\u5411\uff0c\u63a8\u52a8\u8bbe\u8ba1\u4e25\u683c\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u4e16\u754c\u5efa\u6a21\u7684\u51c6\u786e\u6027\uff0c\u4ece\u800c\u51cf\u5c11\u5e7b\u89c9\u73b0\u8c61\u3002"}}
{"id": "2512.21426", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.21426", "abs": "https://arxiv.org/abs/2512.21426", "authors": ["Mohammed Sayagh"], "title": "What Makes a GitHub Issue Ready for Copilot?", "comment": null, "summary": "AI-agents help developers in different coding tasks, such as developing new features, fixing bugs, and reviewing code. Developers can write a Github issue and assign it to an AI-agent like Copilot for implementation. Based on the issue and its related discussion, the AI-agent performs a plan for the implementation, and executes it. However, the performance of AI-agents and LLMs heavily depends on the input they receive. For instance, a GitHub issue that is unclear or not well scoped might not lead to a successful implementation that will eventually be merged. GitHub Copilot provides a set of best practice recommendations that are limited and high-level. In this paper, we build a set of 32 detailed criteria that we leverage to measure the quality of GitHub issues to make them suitable for AI-agents. We compare the GitHub issues that lead to a merged pull request versus closed pull request. Then, we build an interpretable machine learning model to predict the likelihood of a GitHub issue resulting in a merged pull request. We observe that pull requests that end up being merged are those originating from issues that are shorter, well scoped, with clear guidance and hints about the relevant artifacts for an issue, and with guidance on how to perform the implementation. Issues with external references including configuration, context setup, dependencies or external APIs are associated with lower merge rates. We built an interpretable machine learning model to help users identify how to improve a GitHub issue to increase the chances of the issue resulting in a merged pull request by Copilot. Our model has a median AUC of 72\\%. Our results shed light on quality metrics relevant for writing GitHub issues and motivate future studies further investigate the writing of GitHub issues as a first-class software engineering activity in the era of AI-teammates.", "AI": {"tldr": "\u672c\u6587\u5efa\u7acb\u4e8632\u6761\u8be6\u7ec6\u6807\u51c6\uff0c\u8bc4\u4f30GitHub issue\u7684\u8d28\u91cf\uff0c\u9884\u6d4b\u5176\u662f\u5426\u80fd\u5bfc\u81f4\u5408\u5e76\u7684PR\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u5efa\u8bae\u3002", "motivation": "\u73b0\u6709\u7684GitHub issue\u6a21\u7cca\u6216\u4e0d\u5168\u9762\uff0c\u5f71\u54cdAI\u4ee3\u7406\u5982Copilot\u7684\u4ee3\u7801\u5b9e\u73b0\u6548\u679c\uff0c\u73b0\u6709\u6700\u4f73\u5b9e\u8df5\u8fc7\u4e8e\u7b3c\u7edf\uff0c\u7f3a\u4e4f\u8be6\u7ec6\u6307\u5bfc\u3002", "method": "\u5206\u6790\u5bfc\u81f4\u5408\u5e76\u4e0e\u5173\u95edPR\u7684GitHub issue\uff0c\u6784\u5efa32\u6761\u8be6\u7ec6\u8d28\u91cf\u6807\u51c6\uff1b\u91c7\u7528\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4bissue\u5bfc\u81f4\u5408\u5e76PR\u6982\u7387\uff0c\u5e76\u5206\u6790\u5f71\u54cd\u56e0\u7d20\u3002", "result": "\u53d1\u73b0\u5408\u5e76\u7684PR\u5bf9\u5e94\u7684issue\u901a\u5e38\u66f4\u77ed\u5c0f\u3001\u8303\u56f4\u660e\u786e\u3001\u5305\u542b\u660e\u786e\u6307\u5bfc\u548c\u76f8\u5173\u7ebf\u7d22\uff1b\u542b\u5916\u90e8\u5f15\u7528\u7684issue\u5408\u5e76\u7387\u8f83\u4f4e\u3002\u6a21\u578b\u9884\u6d4bAUC\u4e2d\u4f4d\u6570\u4e3a72%\u3002", "conclusion": "\u63d0\u51fa\u4e86\u8bc4\u4f30\u548c\u63d0\u5347GitHub issue\u8d28\u91cf\u7684\u5177\u4f53\u6307\u6807\u548c\u53ef\u89e3\u91ca\u6a21\u578b\uff0c\u4e3aAI\u8f85\u52a9\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684issue\u4e66\u5199\u63d0\u4f9b\u6307\u5bfc\uff0c\u63a8\u52a8\u8be5\u6d3b\u52a8\u6210\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u91cd\u8981\u90e8\u5206\u3002"}}
{"id": "2512.21580", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.21580", "abs": "https://arxiv.org/abs/2512.21580", "authors": ["Alexander Podolskiy", "Semen Molokov", "Timofey Gerasin", "Maksim Titov", "Alexey Rukhovich", "Artem Khrapov", "Kirill Morozov", "Evgeny Tetin", "Constantine Korikov", "Pavel Efimov", "Polina Lazukova", "Yuliya Skripkar", "Nikita Okhotnikov", "Irina Piontkovskaya", "Meng Xiaojun", "Zou Xueyi", "Zhang Zhenhe"], "title": "Gamayun's Path to Multilingual Mastery: Cost-Efficient Training of a 1.5B-Parameter LLM", "comment": null, "summary": "We present Gamayun, a 1.5B-parameter multilingual language model trained entirely from scratch on 2.5T tokens. Designed for efficiency and deployment in resource-constrained environments, Gamayun addresses the lack of research on small non-English-centric LLMs by adopting a novel two-stage pre-training strategy: balanced multilingual training for cross-lingual alignment, followed by high-quality English enrichment to transfer performance gains across languages. Our model supports 12 languages, with special focus on Russian. Despite a significantly smaller training budget than comparable models, Gamayun outperforms LLaMA3.2-1B (9T tokens) on all considered benchmarks, and surpasses Qwen2.5-1.5B (18T tokens) on a wide range of English and multilingual tasks. It matches or exceeds Qwen3 (36T tokens) on most tasks outside advanced STEM, achieving state-of-the-art results in Russian, including the MERA benchmark, among the models of comparable size (1-2B parameters).", "AI": {"tldr": "Gamayun\u662f\u4e00\u4e2a1.5B\u53c2\u6570\u7684\u591a\u8bed\u8a00\u8bed\u8a00\u6a21\u578b\uff0c\u91c7\u7528\u5168\u65b0\u4e24\u9636\u6bb5\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u4e13\u6ce8\u4e8e\u8d44\u6e90\u53d7\u9650\u73af\u5883\uff0c\u652f\u630112\u79cd\u8bed\u8a00\u5e76\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u540c\u89c4\u6a21\u53ca\u66f4\u5927\u89c4\u6a21\u6a21\u578b\uff0c\u5c24\u5176\u5728\u4fc4\u8bed\u4efb\u52a1\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u9488\u5bf9\u5c0f\u578b\u975e\u82f1\u8bed\u4e3a\u4e2d\u5fc3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u7814\u7a76\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u5e94\u7528\u9700\u6c42\uff0c\u63d0\u51fa\u4e00\u79cd\u9ad8\u6548\u4e14\u8868\u73b0\u4f18\u5f02\u7684\u591a\u8bed\u8a00\u6a21\u578b\u3002", "method": "\u91c7\u7528\u4e00\u79cd\u65b0\u9896\u7684\u4e24\u9636\u6bb5\u9884\u8bad\u7ec3\u7b56\u7565\uff1a\u9996\u5148\u8fdb\u884c\u5e73\u8861\u7684\u591a\u8bed\u8a00\u8bad\u7ec3\u4ee5\u5b9e\u73b0\u8de8\u8bed\u8a00\u5bf9\u9f50\uff0c\u7136\u540e\u8fdb\u884c\u9ad8\u8d28\u91cf\u7684\u82f1\u8bed\u5f3a\u5316\u8bad\u7ec3\u4ee5\u4fc3\u8fdb\u5404\u8bed\u8a00\u7684\u6027\u80fd\u63d0\u5347\u3002", "result": "\u5728\u8bad\u7ec3\u6570\u636e\u91cf\u8fdc\u5c0f\u4e8e\u540c\u7c7b\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\uff0cGamayun\u5728\u6240\u6709\u8003\u5bdf\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8eLLaMA3.2-1B\u53caQwen2.5-1.5B\uff0c\u5e76\u5728\u5927\u90e8\u5206\u4efb\u52a1\u8d85\u8d8aQwen3\uff0c\u7279\u522b\u662f\u5728\u4fc4\u8bed\u4efb\u52a1\uff08\u5982MERA\u57fa\u51c6\uff09\u4e0a\u8fbe\u5230\u540c\u89c4\u6a21\u6a21\u578b\u7684\u9886\u5148\u6c34\u5e73\u3002", "conclusion": "Gamayun\u901a\u8fc7\u521b\u65b0\u9884\u8bad\u7ec3\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u591a\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5728\u4fdd\u8bc1\u6a21\u578b\u89c4\u6a21\u9002\u4e2d\u7684\u540c\u65f6\uff0c\u663e\u8457\u4f18\u4e8e\u591a\u79cd\u4e3b\u6d41\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5c55\u73b0\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u6784\u5efa\u9ad8\u6548\u591a\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u884c\u6027\u548c\u6f5c\u529b\u3002"}}
{"id": "2512.21431", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.21431", "abs": "https://arxiv.org/abs/2512.21431", "authors": ["Hridya Dhulipala", "Xiaokai Rong", "Tien N. Nguyen"], "title": "Cerberus: Multi-Agent Reasoning and Coverage-Guided Exploration for Static Detection of Runtime Errors", "comment": null, "summary": "In several software development scenarios, it is desirable to detect runtime errors and exceptions in code snippets without actual execution. A typical example is to detect runtime exceptions in online code snippets before integrating them into a codebase. In this paper, we propose Cerberus, a novel predictive, execution-free coverage-guided testing framework. Cerberus uses LLMs to generate the inputs that trigger runtime errors and to perform code coverage prediction and error detection without code execution. With a two-phase feedback loop, Cerberus first aims to both increasing code coverage and detecting runtime errors, then shifts to focus only detecting runtime errors when the coverage reaches 100% or its maximum, enabling it to perform better than prompting the LLMs for both purposes. Our empirical evaluation demonstrates that Cerberus performs better than conventional and learning-based testing frameworks for (in)complete code snippets by generating high-coverage test cases more efficiently, leading to the discovery of more runtime errors.", "AI": {"tldr": "\u63d0\u51fa\u4e86Cerberus\uff0c\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65e0\u9700\u6267\u884c\u4ee3\u7801\u5373\u53ef\u8fdb\u884c\u8986\u76d6\u7387\u5f15\u5bfc\u7684\u8fd0\u884c\u65f6\u9519\u8bef\u68c0\u6d4b\u6846\u67b6\u3002", "motivation": "\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\uff0c\u5e0c\u671b\u80fd\u5728\u4e0d\u6267\u884c\u4ee3\u7801\u7684\u60c5\u51b5\u4e0b\u68c0\u6d4b\u4ee3\u7801\u7247\u6bb5\u4e2d\u7684\u8fd0\u884c\u65f6\u9519\u8bef\uff0c\u5c24\u5176\u662f\u5728\u7ebf\u4ee3\u7801\u7247\u6bb5\u96c6\u6210\u524d\u3002", "method": "Cerberus\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u89e6\u53d1\u9519\u8bef\u7684\u8f93\u5165\uff0c\u9884\u6d4b\u4ee3\u7801\u8986\u76d6\u7387\u5e76\u68c0\u6d4b\u9519\u8bef\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u53cd\u9988\u56de\u8def\u5148\u63d0\u5347\u8986\u76d6\u7387\u53ca\u68c0\u6d4b\u9519\u8bef\uff0c\u518d\u4e13\u6ce8\u4e8e\u9519\u8bef\u68c0\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eCerberus\u5728\u751f\u6210\u9ad8\u8986\u76d6\u7387\u6d4b\u8bd5\u7528\u4f8b\u548c\u53d1\u73b0\u8fd0\u884c\u65f6\u9519\u8bef\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u548c\u5b66\u4e60\u578b\u6d4b\u8bd5\u6846\u67b6\u3002", "conclusion": "Cerberus\u80fd\u9ad8\u6548\u9488\u5bf9\uff08\u4e0d\uff09\u5b8c\u6574\u4ee3\u7801\u7247\u6bb5\u8fdb\u884c\u6d4b\u8bd5\uff0c\u63d0\u5347\u9519\u8bef\u53d1\u73b0\u7387\uff0c\u662f\u4e00\u79cd\u6709\u6548\u7684\u6267\u884c\u65e0\u4f9d\u8d56\u7684\u9519\u8bef\u68c0\u6d4b\u65b9\u6cd5\u3002"}}
{"id": "2512.21625", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.21625", "abs": "https://arxiv.org/abs/2512.21625", "authors": ["Xinyu Tang", "Yuliang Zhan", "Zhixun Li", "Wayne Xin Zhao", "Zhenduo Zhang", "Zujie Wen", "Zhiqiang Zhang", "Jun Zhou"], "title": "Rethinking Sample Polarity in Reinforcement Learning with Verifiable Rewards", "comment": null, "summary": "Large reasoning models (LRMs) are typically trained using reinforcement learning with verifiable reward (RLVR) to enhance their reasoning abilities. In this paradigm, policies are updated using both positive and negative self-generated rollouts, which correspond to distinct sample polarities. In this paper, we provide a systematic investigation into how these sample polarities affect RLVR training dynamics and behaviors. We find that positive samples sharpen existing correct reasoning patterns, while negative samples encourage exploration of new reasoning paths. We further explore how adjusting the advantage values of positive and negative samples at both the sample level and the token level affects RLVR training. Based on these insights, we propose an Adaptive and Asymmetric token-level Advantage shaping method for Policy Optimization, namely A3PO, that more precisely allocates advantage signals to key tokens across different polarities. Experiments across five reasoning benchmarks demonstrate the effectiveness of our approach.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5f3a\u5316\u5b66\u4e60\u4e2d\u4e0d\u540c\u6781\u6027\u6837\u672c\u5982\u4f55\u5f71\u54cd\u5927\u63a8\u7406\u6a21\u578b\u7684\u8bad\u7ec3\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u975e\u5bf9\u79f0\u7684\u6807\u8bb0\u7ea7\u4f18\u52bf\u8c03\u6574\u65b9\u6cd5A3PO\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u4e94\u4e2a\u63a8\u7406\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u5927\u63a8\u7406\u6a21\u578b\u901a\u8fc7\u5177\u6709\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4ee5\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u4e0d\u540c\u6781\u6027\uff08\u6b63\u8d1f\uff09\u6837\u672c\u5728\u8bad\u7ec3\u4e2d\u626e\u6f14\u4e0d\u540c\u89d2\u8272\uff0c\u4f46\u5176\u5177\u4f53\u5f71\u54cd\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u7cfb\u7edf\u5206\u6790\u4e86\u6b63\u8d1f\u6837\u672c\u5bf9\u8bad\u7ec3\u52a8\u6001\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u6b63\u6837\u672c\u5f3a\u5316\u5df2\u6709\u6b63\u786e\u63a8\u7406\u6a21\u5f0f\uff0c\u8d1f\u6837\u672c\u9f13\u52b1\u63a2\u7d22\u65b0\u63a8\u7406\u8def\u5f84\u3002\u63d0\u51fa\u4e86A3PO\u65b9\u6cd5\uff0c\u9488\u5bf9\u4e0d\u540c\u6781\u6027\u7684\u5173\u952e\u6807\u8bb0\u81ea\u9002\u5e94\u5206\u914d\u4f18\u52bf\u4fe1\u53f7\u3002", "result": "A3PO\u5728\u4e94\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u660e\u663e\u4f18\u52bf\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u7cbe\u7ec6\u8c03\u6574\u6b63\u8d1f\u6837\u672c\u7684\u6807\u8bb0\u7ea7\u4f18\u52bf\uff0cRLVR\u8bad\u7ec3\u53ef\u4ee5\u66f4\u6709\u6548\u5730\u5f3a\u5316\u63a8\u7406\u6a21\u578b\u7684\u6027\u80fd\uff0cA3PO\u662f\u4e00\u79cd\u6709\u6548\u7684\u6539\u8fdb\u7b56\u7565\u3002"}}
{"id": "2512.21440", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.21440", "abs": "https://arxiv.org/abs/2512.21440", "authors": ["Hridya Dhulipala", "Xiaokai Rong", "Aashish Yadavally", "Tien N. Nguyen"], "title": "Fuzzwise: Intelligent Initial Corpus Generation for Fuzzing", "comment": null, "summary": "In mutation-based greybox fuzzing, generating high-quality input seeds for the initial corpus is essential for effective fuzzing. Rather than conducting separate phases for generating a large corpus and subsequently minimizing it, we propose FuzzWise which integrates them into one process to generate the optimal initial corpus of seeds (ICS). FuzzWise leverages a multi-agent framework based on Large Language Models (LLMs). The first LLM agent generates test cases for the target program. The second LLM agent, which functions as a predictive code coverage module, assesses whether each generated test case will enhance the overall coverage of the current corpus. The streamlined process allows each newly generated test seed to be immediately evaluated for its contribution to the overall coverage. FuzzWise employs a predictive approach using an LLM and eliminates the need for actual execution, saving computational resources and time, particularly in scenarios where the execution is not desirable or even impossible. Our empirical evaluation demonstrates that FuzzWise generates significantly fewer test cases than baseline methods. Despite the lower number of test cases, FuzzWise achieves high code coverage and triggers more runtime errors compared to the baselines. Moreover, it is more time-efficient and coverage-efficient in producing an initial corpus catching more errors.", "AI": {"tldr": "FuzzWise\u901a\u8fc7\u7ed3\u5408\u751f\u6210\u548c\u6700\u5c0f\u5316\u6d4b\u8bd5\u79cd\u5b50\u8fc7\u7a0b\uff0c\u5229\u7528\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u751f\u6210\u4e86\u4f18\u5316\u7684\u521d\u59cb\u6d4b\u8bd5\u79cd\u5b50\u96c6\uff0c\u63d0\u9ad8\u7070\u76d2\u6a21\u7cca\u6d4b\u8bd5\u7684\u6548\u7387\u548c\u6548\u679c\u3002", "motivation": "\u5728\u57fa\u4e8e\u53d8\u5f02\u7684\u7070\u76d2\u6a21\u7cca\u6d4b\u8bd5\u4e2d\uff0c\u9ad8\u8d28\u91cf\u7684\u521d\u59cb\u79cd\u5b50\u96c6\u5bf9\u63d0\u9ad8\u6d4b\u8bd5\u6548\u679c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u5206\u9636\u6bb5\u751f\u6210\u548c\u6700\u5c0f\u5316\u79cd\u5b50\uff0c\u6548\u7387\u4f4e\u4e14\u8d44\u6e90\u6d88\u8017\u5927\u3002", "method": "\u63d0\u51faFuzzWise\uff0c\u91c7\u7528\u4e24\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\uff1a\u4e00\u4e2a\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\uff0c\u53e6\u4e00\u4e2a\u9884\u6d4b\u6d4b\u8bd5\u7528\u4f8b\u5bf9\u4ee3\u7801\u8986\u76d6\u7387\u7684\u8d21\u732e\uff0c\u5b9e\u73b0\u6d4b\u8bd5\u79cd\u5b50\u7684\u5373\u65f6\u8bc4\u4f30\u548c\u7b5b\u9009\uff0c\u65e0\u9700\u5b9e\u9645\u6267\u884c\u7a0b\u5e8f\uff0c\u8282\u7701\u8ba1\u7b97\u8d44\u6e90\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cFuzzWise\u751f\u6210\u7684\u6d4b\u8bd5\u7528\u4f8b\u6570\u91cf\u663e\u8457\u5c11\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4f46\u4ecd\u5b9e\u73b0\u66f4\u9ad8\u7684\u4ee3\u7801\u8986\u76d6\u548c\u66f4\u591a\u8fd0\u884c\u65f6\u9519\u8bef\uff0c\u540c\u65f6\u5728\u65f6\u95f4\u548c\u8986\u76d6\u6548\u7387\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "FuzzWise\u901a\u8fc7\u6574\u5408\u751f\u6210\u4e0e\u7b5b\u9009\u8fc7\u7a0b\u53ca\u9884\u6d4b\u8986\u76d6\u8d21\u732e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u521d\u59cb\u6d4b\u8bd5\u79cd\u5b50\u7684\u8d28\u91cf\u548c\u6a21\u7cca\u6d4b\u8bd5\u7684\u6548\u7387\uff0c\u9002\u5408\u8d44\u6e90\u6709\u9650\u6216\u4e0d\u53ef\u6267\u884c\u573a\u666f\u3002"}}
{"id": "2512.21635", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.21635", "abs": "https://arxiv.org/abs/2512.21635", "authors": ["Chengxu Yang", "Jingling Yuan", "Siqi Cai", "Jiawei Jiang", "Chuang Hu"], "title": "Heaven-Sent or Hell-Bent? Benchmarking the Intelligence and Defectiveness of LLM Hallucinations", "comment": "Published as a conference paper at KDD 2026", "summary": "Hallucinations in large language models (LLMs) are commonly regarded as errors to be minimized. However, recent perspectives suggest that some hallucinations may encode creative or epistemically valuable content, a dimension that remains underquantified in current literature. Existing hallucination detection methods primarily focus on factual consistency, struggling to handle heterogeneous scientific tasks and balance creativity with accuracy. To address these challenges, we propose HIC-Bench, a novel evaluation framework that categorizes hallucinations into Intelligent Hallucinations (IH) and Defective Hallucinations (DH), enabling systematic investigation of their interplay in LLM creativity. HIC-Bench features three core characteristics: (1) Structured IH/DH Assessment. using a multi-dimensional metric matrix integrating Torrance Tests of Creative Thinking (TTCT) metrics (Originality, Feasibility, Value) with hallucination-specific dimensions (scientific plausibility, factual deviation); (2) Cross-Domain Applicability. spanning ten scientific domains with open-ended innovation tasks; and (3) Dynamic Prompt Optimization. leveraging the Dynamic Hallucination Prompt (DHP) to guide models toward creative and reliable outputs. The evaluation process employs multiple LLM judges, averaging scores to mitigate bias, with human annotators verifying IH/DH classifications. Experimental results reveal a nonlinear relationship between IH and DH, demonstrating that creativity and correctness can be jointly optimized. These insights position IH as a catalyst for creativity and reveal the ability of LLM hallucinations to drive scientific innovation.Additionally, the HIC-Bench offers a valuable platform for advancing research into the creative intelligence of LLM hallucinations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86HIC-Bench\u6846\u67b6\uff0c\u7528\u4ee5\u5206\u7c7b\u548c\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\uff0c\u533a\u5206\u667a\u80fd\u5e7b\u89c9\u4e0e\u7f3a\u9677\u5e7b\u89c9\uff0c\u7ed3\u5408\u521b\u9020\u6027\u4e0e\u51c6\u786e\u6027\u6307\u6807\uff0c\u4fc3\u8fdb\u79d1\u5b66\u521b\u65b0\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5e7b\u89c9\u591a\u88ab\u89c6\u4e3a\u9519\u8bef\uff0c\u68c0\u6d4b\u65b9\u6cd5\u504f\u91cd\u4e8b\u5b9e\u4e00\u81f4\u6027\uff0c\u96be\u4ee5\u517c\u987e\u4e0d\u540c\u79d1\u5b66\u4efb\u52a1\u4e2d\u7684\u521b\u9020\u6027\u4e0e\u51c6\u786e\u6027\u9700\u6c42\uff0c\u7f3a\u4e4f\u5bf9\u5e7b\u89c9\u4e2d\u6709\u4ef7\u503c\u5185\u5bb9\u7684\u5b9a\u91cf\u5206\u6790\u3002", "method": "\u63d0\u51faHIC-Bench\u6846\u67b6\uff0c\u5229\u7528\u591a\u7ef4\u5ea6\u6307\u6807\uff08\u5305\u62ec\u521b\u9020\u529b\u6d4b\u8bd5\u548c\u5e7b\u89c9\u7279\u5f02\u6307\u6807\uff09\u5bf9\u667a\u80fd\u5e7b\u89c9\u4e0e\u7f3a\u9677\u5e7b\u89c9\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\uff0c\u6db5\u76d6\u5341\u4e2a\u79d1\u5b66\u9886\u57df\u7684\u521b\u65b0\u4efb\u52a1\uff0c\u91c7\u7528\u52a8\u6001\u63d0\u793a\u4f18\u5316\u6280\u672f\u5f15\u5bfc\u6a21\u578b\u4ea7\u751f\u53ef\u9760\u4e14\u5177\u521b\u9020\u6027\u7684\u8f93\u51fa\uff0c\u591a\u6a21\u578b\u8bc4\u5206\u5e76\u8f85\u4ee5\u4eba\u5de5\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e86\u667a\u80fd\u5e7b\u89c9\u4e0e\u7f3a\u9677\u5e7b\u89c9\u4e4b\u95f4\u7684\u975e\u7ebf\u6027\u5173\u7cfb\uff0c\u8868\u660e\u521b\u9020\u6027\u4e0e\u6b63\u786e\u6027\u53ef\u4ee5\u540c\u65f6\u4f18\u5316\uff0c\u667a\u80fd\u5e7b\u89c9\u80fd\u591f\u4fc3\u8fdb\u79d1\u5b66\u521b\u65b0\u3002", "conclusion": "\u667a\u80fd\u5e7b\u89c9\u4e0d\u4ec5\u662f\u9519\u8bef\uff0c\u66f4\u662f\u63a8\u52a8\u79d1\u5b66\u521b\u65b0\u7684\u50ac\u5316\u5242\uff0cHIC-Bench\u4e3a\u6df1\u5165\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5e7b\u89c9\u4e2d\u7684\u521b\u9020\u667a\u80fd\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5e73\u53f0\u3002"}}
{"id": "2512.21511", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.21511", "abs": "https://arxiv.org/abs/2512.21511", "authors": ["Takuto Kawamoto", "Yoshiki Higo"], "title": "Code Clone Refactoring in C# with Lambda Expressions", "comment": "8 pages, World Symposium on Software Engineering (WSSE 2025)", "summary": "\"Extract Method\" refactoring is a technique for consolidating code clones. Parameterization approaches are used to extract a single method from multiple code clones that contain differences. This approach parameterizes expressions and behaviors within a method. In particular, behavior parameterization has been extensively studied in Java programs, but little research has been conducted on other programming languages.\n  Lambda expressions can be used to parameterize behaviors, but the specifications of each programming language significantly affect the applicability of this technique. Therefore, the optimal \"Extract Method\" approach may vary depending on the programming language.\n  In this study, we propose a C#-specific technique that uses lambda expressions to analyze and consolidate code clones. We evaluated our proposed method by applying it to code clones detected by the NiCad clone detector and measuring how many of them could be successfully consolidated.\n  In total, 2,217 clone pairs from 22 projects were included in our evaluation. For the clone pairs determined to be refactorable, we also attempted refactoring actually. The proposed approach determined that 35.0% of all clone pairs were suitable for refactoring. Among these, 28.9% were successfully refactored.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eC# lambda\u8868\u8fbe\u5f0f\u7684\u4ee3\u7801\u514b\u9686\u5408\u5e76\u6280\u672f\uff0c\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u53ef\u4ee5\u6210\u529f\u91cd\u6784\u7ea628.9%\u7684\u4ee3\u7801\u514b\u9686\u5bf9\u3002", "motivation": "\u867d\u7136\u884c\u4e3a\u53c2\u6570\u5316\u5728Java\u4e2d\u5df2\u6709\u8f83\u591a\u7814\u7a76\uff0c\u4f46\u9488\u5bf9C#\u7b49\u5176\u4ed6\u8bed\u8a00\u7684\u5e94\u7528\u8f83\u5c11\uff0c\u8bed\u8a00\u7279\u6027\u5f71\u54cd\u6280\u672f\u9002\u7528\u6027\uff0c\u56e0\u6b64\u9700\u8981\u9488\u5bf9C#\u8bbe\u8ba1\u4e13\u95e8\u7684\u63d0\u53d6\u65b9\u6cd5\u91cd\u6784\u6280\u672f\u3002", "method": "\u5229\u7528C#\u4e2d\u7684lambda\u8868\u8fbe\u5f0f\u5bf9\u4ee3\u7801\u514b\u9686\u4e2d\u7684\u884c\u4e3a\u8fdb\u884c\u53c2\u6570\u5316\uff0c\u5e76\u4f7f\u7528NiCad\u5de5\u5177\u68c0\u6d4b\u4ee3\u7801\u514b\u9686\uff0c\u8bc4\u4f30\u8be5\u65b9\u6cd5\u5728\u5b9e\u6d4b\u9879\u76ee\u4e2d\u7684\u53ef\u91cd\u6784\u7387\u548c\u6210\u529f\u7387\u3002", "result": "\u572822\u4e2a\u9879\u76ee\u4e2d\u5171\u68c0\u6d4b\u51fa2217\u5bf9\u514b\u9686\uff0c35.0%\u9002\u5408\u91cd\u6784\uff0c\u5b9e\u9645\u6210\u529f\u91cd\u678428.9%\uff0c\u8868\u660e\u8be5\u65b9\u6cd5\u5728C#\u73af\u5883\u4e0b\u5177\u6709\u4e00\u5b9a\u6709\u6548\u6027\u3002", "conclusion": "\u57fa\u4e8eC# lambda\u8868\u8fbe\u5f0f\u7684\u884c\u4e3a\u53c2\u6570\u5316\u63d0\u53d6\u65b9\u6cd5\u53ef\u6709\u6548\u652f\u6301\u4ee3\u7801\u514b\u9686\u5408\u5e76\uff0c\u8bed\u8a00\u7279\u6027\u9700\u7eb3\u5165\u91cd\u6784\u65b9\u6cd5\u8bbe\u8ba1\u8003\u8651\u3002"}}
{"id": "2512.21706", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21706", "abs": "https://arxiv.org/abs/2512.21706", "authors": ["Shuchang Pan", "Siddharth Banerjee", "Dhruv Hebbar", "Siddhant Patel", "Akshaj Gupta", "Kan Jen Cheng", "Hanjo Kim", "Zeyi Austin Li", "Martin Q. Ma", "Tingle Li", "Gopala Anumanchipalli", "Jiachen Lian"], "title": "Enabling Conversational Behavior Reasoning Capabilities in Full-Duplex Speech", "comment": null, "summary": "Human conversation is organized by an implicit chain of thoughts that manifests as timed speech acts. Capturing this causal pathway is key to building natural full-duplex interactive systems. We introduce a framework that enables reasoning over conversational behaviors by modeling this process as causal inference within a Graph-of-Thoughts (GoT). Our approach formalizes the intent-to-action pathway with a hierarchical labeling scheme, predicting high-level communicative intents and low-level speech acts to learn their causal and temporal dependencies. To train this system, we develop a hybrid corpus that pairs controllable, event-rich simulations with human-annotated rationales and real conversational speech. The GoT framework structures streaming predictions as an evolving graph, enabling a multimodal transformer to forecast the next speech act, generate concise justifications for its decisions, and dynamically refine its reasoning. Experiments on both synthetic and real duplex dialogues show that the framework delivers robust behavior detection, produces interpretable reasoning chains, and establishes a foundation for benchmarking conversational reasoning in full duplex spoken dialogue systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56e0\u679c\u63a8\u7406\u7684Graph-of-Thoughts\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u6807\u6ce8\u548c\u6df7\u5408\u8bed\u6599\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u5bf9\u53cc\u5de5\u5bf9\u8bdd\u4e2d\u8bed\u97f3\u884c\u4e3a\u7684\u63a8\u7406\u548c\u9884\u6d4b\uff0c\u63d0\u5347\u4e86\u5bf9\u8bdd\u7cfb\u7edf\u7684\u81ea\u7136\u4ea4\u4e92\u80fd\u529b\u3002", "motivation": "\u6355\u6349\u4eba\u7c7b\u5bf9\u8bdd\u4e2d\u9690\u542b\u7684\u601d\u7ef4\u94fe\u6761\u53ca\u5176\u56e0\u679c\u8def\u5f84\uff0c\u5bf9\u4e8e\u6784\u5efa\u81ea\u7136\u3001\u5168\u53cc\u5de5\u4ea4\u4e92\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7Graph-of-Thoughts\u6846\u67b6\uff0c\u5c06\u5bf9\u8bdd\u884c\u4e3a\u5efa\u6a21\u4e3a\u56e0\u679c\u63a8\u7406\uff0c\u91c7\u7528\u5c42\u7ea7\u6807\u6ce8\u9884\u6d4b\u9ad8\u5c42\u610f\u56fe\u548c\u4f4e\u5c42\u8a00\u8bed\u884c\u4e3a\uff0c\u5229\u7528\u6df7\u5408\u8bed\u6599\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u8bbe\u8ba1\u591a\u6a21\u6001\u53d8\u6362\u5668\u7528\u4e8e\u52a8\u6001\u63a8\u7406\u548c\u9884\u6d4b\u4e0b\u4e00\u8a00\u8bed\u884c\u4e3a\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u53cc\u5de5\u5bf9\u8bdd\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u6846\u67b6\u8868\u73b0\u51fa\u7a33\u5065\u7684\u884c\u4e3a\u68c0\u6d4b\u80fd\u529b\uff0c\u751f\u6210\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u94fe\uff0c\u5e76\u4e3a\u5168\u53cc\u5de5\u8bed\u97f3\u5bf9\u8bdd\u7cfb\u7edf\u7684\u63a8\u7406\u8bc4\u6d4b\u5960\u5b9a\u57fa\u7840\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u5efa\u6a21\u4e86\u5bf9\u8bdd\u4e2d\u7684\u56e0\u679c\u601d\u7ef4\u94fe\uff0c\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7684\u89e3\u91ca\u6027\u548c\u4ea4\u4e92\u81ea\u7136\u5ea6\uff0c\u662f\u5168\u53cc\u5de5\u8bed\u97f3\u5bf9\u8bdd\u7cfb\u7edf\u53d1\u5c55\u7684\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2512.21555", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.21555", "abs": "https://arxiv.org/abs/2512.21555", "authors": ["Qi Hu", "Jiangchao Liu", "Xin Yu", "Lin Zhang", "Edward Jiang"], "title": "XTrace: A Non-Invasive Dynamic Tracing Framework for Android Applications in Production", "comment": null, "summary": "As the complexity of mobile applications grows exponentially and the fragmentation of user device environments intensifies, ensuring online application stability faces unprecedented challenges. Traditional methods, such as static logging and post-crash analysis, lack real-time contextual information, rendering them ineffective against \"ghost bugs\" that only manifest in specific scenarios. This highlights an urgent need for dynamic runtime observability: intercepting and tracing arbitrary methods in production without requiring an app release. We propose XTrace, a novel dynamic tracing framework. XTrace introduces a new paradigm of non-invasive proxying, which avoids direct modification of the virtual machine's underlying data structures. It achieves high-performance method interception by leveraging and optimizing the highly stable, built-in instrumentation mechanism of the Android ART virtual machine. Evaluated in a ByteDance application with hundreds of millions of daily active users, XTrace demonstrated production-grade stability and performance. Large-scale online A/B experiments confirmed its stability, showing no statistically significant impact (p > 0.05) on Crash User Rate or ANR rate, while maintaining minimal overhead (<7 ms startup latency, <0.01 ms per-method call) and broad compatibility (Android 5.0-15+). Critically, XTrace diagnosed over 11 severe online crashes and multiple performance bottlenecks, improving root-cause localization efficiency by over 90%. This confirms XTrace provides a production-grade solution that reconciles the long-standing conflict between stability and comprehensive coverage in Android dynamic tracing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86XTrace\uff0c\u4e00\u79cd\u57fa\u4e8eAndroid ART\u865a\u62df\u673a\u7684\u52a8\u6001\u8ffd\u8e2a\u6846\u67b6\uff0c\u80fd\u9ad8\u6027\u80fd\u3001\u975e\u4fb5\u5165\u5f0f\u5730\u62e6\u622a\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u751f\u4ea7\u73af\u5883\uff0c\u7528\u4e8e\u5b9e\u65f6\u8bca\u65ad\u79fb\u52a8\u5e94\u7528\u4e2d\u7684\u590d\u6742\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u79fb\u52a8\u5e94\u7528\u590d\u6742\u6027\u548c\u7528\u6237\u8bbe\u5907\u73af\u5883\u788e\u7247\u5316\u7684\u589e\u52a0\uff0c\u4f20\u7edf\u7684\u9759\u6001\u65e5\u5fd7\u8bb0\u5f55\u548c\u5d29\u6e83\u540e\u5206\u6790\u65b9\u6cd5\u56e0\u7f3a\u5c11\u5b9e\u65f6\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u96be\u4ee5\u5e94\u5bf9\u7279\u5b9a\u573a\u666f\u4e0b\u7684\u201c\u5e7d\u7075bug\u201d\uff0c\u4e9f\u9700\u52a8\u6001\u8fd0\u884c\u65f6\u89c2\u6d4b\u80fd\u529b\u3002", "method": "XTrace\u5229\u7528\u4e00\u79cd\u65b0\u9896\u7684\u975e\u4fb5\u5165\u5f0f\u4ee3\u7406\u6280\u672f\uff0c\u907f\u514d\u76f4\u63a5\u4fee\u6539\u865a\u62df\u673a\u5e95\u5c42\u6570\u636e\u7ed3\u6784\uff0c\u5229\u7528\u5e76\u4f18\u5316Android ART\u865a\u62df\u673a\u5185\u7f6e\u7684\u7a33\u5b9a\u63d2\u6869\u673a\u5236\uff0c\u5b9e\u73b0\u9ad8\u6027\u80fd\u7684\u65b9\u6cd5\u62e6\u622a\u3002", "result": "\u5728\u5b57\u8282\u8df3\u52a8\u65e5\u6d3b\u6570\u4ebf\u7528\u6237\u7684\u5e94\u7528\u4e2d\uff0cXTrace\u8868\u73b0\u51fa\u751f\u4ea7\u7ea7\u7684\u7a33\u5b9a\u6027\u548c\u6027\u80fd\uff0c\u5728\u7ebfA/B\u6d4b\u8bd5\u8868\u660e\u5bf9\u5d29\u6e83\u7387\u548cANR\u65e0\u663e\u8457\u5f71\u54cd\uff0c\u542f\u52a8\u5ef6\u8fdf\u4f4e\u4e8e7\u6beb\u79d2\uff0c\u6bcf\u6b21\u65b9\u6cd5\u8c03\u7528\u5f00\u9500\u4f4e\u4e8e0.01\u6beb\u79d2\uff0c\u517c\u5bb9Android 5.0\u53ca\u4ee5\u4e0a\u7248\u672c\uff0c\u5e76\u6210\u529f\u8bca\u65ad\u4e8611\u4e2a\u4e25\u91cd\u7ebf\u4e0a\u5d29\u6e83\u53ca\u591a\u4e2a\u6027\u80fd\u74f6\u9888\u3002", "conclusion": "XTrace\u6210\u529f\u89e3\u51b3\u4e86Android\u52a8\u6001\u8ffd\u8e2a\u4e2d\u7a33\u5b9a\u6027\u4e0e\u5168\u9762\u8986\u76d6\u51b2\u7a81\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7a33\u5b9a\u3001\u5b9e\u65f6\u7684\u7ebf\u4e0a\u79fb\u52a8\u5e94\u7528\u52a8\u6001\u8ffd\u8e2a\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6545\u969c\u5b9a\u4f4d\u6548\u7387\u3002"}}
{"id": "2512.21708", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.21708", "abs": "https://arxiv.org/abs/2512.21708", "authors": ["Jing Han", "Binwei Yan", "Tianyu Guo", "Zheyuan Bai", "Mengyu Zheng", "Hanting Chen", "Ying Nie"], "title": "MoRAgent: Parameter Efficient Agent Tuning with Mixture-of-Roles", "comment": "Accepted by ICML 2025", "summary": "Despite recent advancements of fine-tuning large language models (LLMs) to facilitate agent tasks, parameter-efficient fine-tuning (PEFT) methodologies for agent remain largely unexplored. In this paper, we introduce three key strategies for PEFT in agent tasks: 1) Inspired by the increasingly dominant Reason+Action paradigm, we first decompose the capabilities necessary for the agent tasks into three distinct roles: reasoner, executor, and summarizer. The reasoner is responsible for comprehending the user's query and determining the next role based on the execution trajectory. The executor is tasked with identifying the appropriate functions and parameters to invoke. The summarizer conveys the distilled information from conversations back to the user. 2) We then propose the Mixture-of-Roles (MoR) framework, which comprises three specialized Low-Rank Adaptation (LoRA) groups, each designated to fulfill a distinct role. By focusing on their respective specialized capabilities and engaging in collaborative interactions, these LoRAs collectively accomplish the agent task. 3) To effectively fine-tune the framework, we develop a multi-role data generation pipeline based on publicly available datasets, incorporating role-specific content completion and reliability verification. We conduct extensive experiments and thorough ablation studies on various LLMs and agent benchmarks, demonstrating the effectiveness of the proposed method. This project is publicly available at https://mor-agent.github.io.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9762\u5411\u4ee3\u7406\u4efb\u52a1\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u5f15\u5165\u4e86\u89d2\u8272\u5206\u89e3\u548c\u6df7\u5408\u89d2\u8272\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u89d2\u8272\u6570\u636e\u751f\u6210\u5b9e\u73b0\u9ad8\u6548\u5fae\u8c03\uff0c\u5b9e\u9a8c\u8bc1\u660e\u65b9\u6cd5\u6709\u6548\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7406\u4efb\u52a1\u5fae\u8c03\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u9488\u5bf9\u4ee3\u7406\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u4ecd\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5c06\u4ee3\u7406\u4efb\u52a1\u80fd\u529b\u5206\u4e3a\u63a8\u7406\u8005\u3001\u6267\u884c\u8005\u548c\u603b\u7ed3\u8005\u4e09\u89d2\u8272\uff0c\u63d0\u51fa\u6df7\u5408\u89d2\u8272\uff08MoR\uff09\u6846\u67b6\uff0c\u5229\u7528\u4e09\u4e2a\u4e13\u95e8\u7684\u4f4e\u79e9\u9002\u914d\u7ec4\u5b9e\u73b0\u89d2\u8272\u534f\u4f5c\uff0c\u7ed3\u5408\u57fa\u4e8e\u516c\u5f00\u6570\u636e\u96c6\u7684\u591a\u89d2\u8272\u6570\u636e\u751f\u6210\u7ba1\u9053\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5728\u591a\u79cd\u5927\u8bed\u8a00\u6a21\u578b\u548c\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\u548c\u6d88\u878d\u5206\u6790\uff0c\u7ed3\u679c\u8868\u660e\u6240\u63d0\u65b9\u6cd5\u5728\u63d0\u5347\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u8868\u73b0\u65b9\u9762\u6548\u679c\u663e\u8457\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u89d2\u8272\u5206\u89e3\u548c\u6df7\u5408\u89d2\u8272\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6846\u67b6\uff0c\u4e3a\u4ee3\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u53ef\u63a8\u5e7f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.21591", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.21591", "abs": "https://arxiv.org/abs/2512.21591", "authors": ["Shuo Sun", "Shixin Zhang", "Jiwei Yan", "Jun Yan", "Jian Zhang"], "title": "Co-Evolution of Types and Dependencies: Towards Repository-Level Type Inference for Python Code", "comment": "Accepted by FSE 2026", "summary": "Python's dynamic typing mechanism, while promoting flexibility, is a significant source of runtime type errors that plague large-scale software, which inspires the automatic type inference techniques. Existing type inference tools have achieved advances in type inference within isolated code snippets. However, repository-level type inference remains a significant challenge, primarily due to the complex inter-procedural dependencies that are difficult to model and resolve. To fill this gap, we present \\methodName, a novel approach based on LLMs that achieves repository-level type inference through the co-evolution of types and dependencies. \\methodName~constructs an Entity Dependency Graph (EDG) to model the objects and type dependencies across the repository. During the inference process, it iteratively refines types and dependencies in EDG for accurate type inference. Our key innovations are: (1) an EDG model designed to capture repository-level type dependencies; (2) an iterative type inference approach where types and dependencies co-evolve in each iteration; and (3) a type-checker-in-the-loop strategy that validates and corrects inferences on-the-fly, thereby reducing error propagation. When evaluated on 12 complex Python repositories, \\methodName~significantly outperformed prior works, achieving a \\textit{TypeSim} score of 0.89 and a \\textit{TypeExact} score of 0.84, representing a 27\\% and 40\\% relative improvement over the strongest baseline. More importantly, \\methodName~removed new type errors introduced by the tool by 92.7\\%. This demonstrates a significant leap towards automated, reliable type annotation for real-world Python development.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u65b0\u65b9\u6cd5\\methodName \uff0c\u901a\u8fc7\u5b9e\u4f53\u4f9d\u8d56\u56fe\uff08EDG\uff09\u548c\u8fed\u4ee3\u63a8\u65ad\uff0c\u5b9e\u73b0\u4e86Python\u4ed3\u5e93\u7ea7\u7684\u7c7b\u578b\u63a8\u65ad\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u65ad\u51c6\u786e\u7387\u548c\u51cf\u5c11\u4e86\u9519\u8bef\u4f20\u64ad\u3002", "motivation": "Python\u52a8\u6001\u7c7b\u578b\u5bfc\u81f4\u5927\u91cf\u8fd0\u884c\u65f6\u7c7b\u578b\u9519\u8bef\uff0c\u73b0\u6709\u5de5\u5177\u591a\u5728\u4ee3\u7801\u7247\u6bb5\u7ea7\u522b\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5f88\u96be\u5904\u7406\u4ed3\u5e93\u7ea7\u7684\u590d\u6742\u8de8\u8fc7\u7a0b\u4f9d\u8d56\u3002", "method": "\u6784\u5efa\u5b9e\u4f53\u4f9d\u8d56\u56fe\uff08EDG\uff09\u6765\u523b\u753b\u4ed3\u5e93\u4e2d\u5bf9\u8c61\u548c\u7c7b\u578b\u4f9d\u8d56\uff0c\u901a\u8fc7\u8fed\u4ee3\u65b9\u6cd5\u4f7f\u7c7b\u578b\u548c\u4f9d\u8d56\u5173\u7cfb\u5171\u540c\u6f14\u5316\uff0c\u7ed3\u5408\u5728\u7ebf\u7c7b\u578b\u68c0\u67e5\u5668\u5b9e\u65f6\u9a8c\u8bc1\u548c\u4fee\u6b63\u63a8\u65ad\u7ed3\u679c\u3002", "result": "\u572812\u4e2a\u590d\u6742Python\u4ed3\u5e93\u4e0a\uff0c\\methodName \u5b9e\u73b0TypeSim\u8bc4\u52060.89\u548cTypeExact\u8bc4\u52060.84\uff0c\u5206\u522b\u6bd4\u6700\u5f3a\u57fa\u7ebf\u63d0\u9ad827%\u548c40%\uff0c\u5e76\u6d88\u966492.7%\u7684\u5de5\u5177\u5f15\u5165\u7684\u65b0\u7c7b\u578b\u9519\u8bef\u3002", "conclusion": "\\methodName\u663e\u8457\u63d0\u5347\u4e86\u4ed3\u5e93\u7ea7\u81ea\u52a8\u7c7b\u578b\u6ce8\u91ca\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u4e3a\u73b0\u5b9ePython\u5f00\u53d1\u73af\u5883\u4e2d\u7684\u81ea\u52a8\u5316\u7c7b\u578b\u63a8\u65ad\u8fc8\u51fa\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2512.21709", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21709", "abs": "https://arxiv.org/abs/2512.21709", "authors": ["Md. Rakibul Islam", "Most. Sharmin Sultana Samu", "Md. Zahid Hossain", "Farhad Uz Zaman", "Md. Kamrozzaman Bhuiyan"], "title": "Detecting AI-Generated Paraphrases in Bengali: A Comparative Study of Zero-Shot and Fine-Tuned Transformers", "comment": "Accepted for publication in 2025 28th International Conference on Computer and Information Technology (ICCIT)", "summary": "Large language models (LLMs) can produce text that closely resembles human writing. This capability raises concerns about misuse, including disinformation and content manipulation. Detecting AI-generated text is essential to maintain authenticity and prevent malicious applications. Existing research has addressed detection in multiple languages, but the Bengali language remains largely unexplored. Bengali's rich vocabulary and complex structure make distinguishing human-written and AI-generated text particularly challenging. This study investigates five transformer-based models: XLMRoBERTa-Large, mDeBERTaV3-Base, BanglaBERT-Base, IndicBERT-Base and MultilingualBERT-Base. Zero-shot evaluation shows that all models perform near chance levels (around 50% accuracy) and highlight the need for task-specific fine-tuning. Fine-tuning significantly improves performance, with XLM-RoBERTa, mDeBERTa and MultilingualBERT achieving around 91% on both accuracy and F1-score. IndicBERT demonstrates comparatively weaker performance, indicating limited effectiveness in fine-tuning for this task. This work advances AI-generated text detection in Bengali and establishes a foundation for building robust systems to counter AI-generated content.", "AI": {"tldr": "\u9488\u5bf9\u5b5f\u52a0\u62c9\u8bed\u7684AI\u751f\u6210\u6587\u672c\u68c0\u6d4b\uff0c\u4e94\u79cd\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u5728\u96f6\u6837\u672c\u72b6\u6001\u4e0b\u8868\u73b0\u63a5\u8fd1\u968f\u673a\uff0c\u7ecf\u8fc7\u5fae\u8c03\u540e\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u6700\u9ad8\u51c6\u786e\u7387\u548cF1\u5206\u6570\u8fbe91%\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u751f\u6210\u9ad8\u5ea6\u7c7b\u4f3c\u4eba\u7c7b\u5199\u4f5c\u7684\u6587\u672c\uff0c\u4f46\u5b5f\u52a0\u62c9\u8bed\u5728AI\u6587\u672c\u68c0\u6d4b\u65b9\u9762\u7814\u7a76\u4e0d\u8db3\uff0c\u4e14\u8bed\u8a00\u7279\u70b9\u589e\u52a0\u533a\u5206\u96be\u5ea6\uff0c\u9700\u89e3\u51b3\u5b5f\u52a0\u62c9\u8bedAI\u6587\u672c\u8bc6\u522b\u95ee\u9898\u3002", "method": "\u9009\u7528XLMRoBERTa-Large\u3001mDeBERTaV3-Base\u3001BanglaBERT-Base\u3001IndicBERT-Base\u548cMultilingualBERT-Base\u4e94\u79cdTransformer\u6a21\u578b\uff0c\u5206\u522b\u8fdb\u884c\u96f6\u6837\u672c\u8bc4\u4f30\u548c\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u3002", "result": "\u96f6\u6837\u672c\u8bc4\u4f30\u51c6\u786e\u7387\u7ea650%\uff0c\u5fae\u8c03\u540eXLM-RoBERTa\u3001mDeBERTa\u548cMultilingualBERT\u8868\u73b0\u4f18\u5f02\uff0c\u51c6\u786e\u7387\u548cF1\u503c\u7ea691%\uff0cIndicBERT\u5fae\u8c03\u6548\u679c\u8f83\u5f31\u3002", "conclusion": "\u5efa\u7acb\u4e86\u5b5f\u52a0\u62c9\u8bedAI\u6587\u672c\u68c0\u6d4b\u57fa\u51c6\uff0c\u63d0\u9ad8\u4e86\u68c0\u6d4b\u6027\u80fd\uff0c\u4e3a\u5e94\u5bf9AI\u751f\u6210\u5185\u5bb9\u63d0\u4f9b\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2512.21757", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21757", "abs": "https://arxiv.org/abs/2512.21757", "authors": ["Huiyun Peng", "Antonio Zhong", "Ricardo Andr\u00e9s Calvo M\u00e9ndez", "Kelechi G. Kalu", "James C. Davis"], "title": "How Do Agents Perform Code Optimization? An Empirical Study", "comment": null, "summary": "Performance optimization is a critical yet challenging aspect of software development, often requiring a deep understanding of system behavior, algorithmic tradeoffs, and careful code modifications. Although recent advances in AI coding agents have accelerated code generation and bug fixing, little is known about how these agents perform on real-world performance optimization tasks. We present the first empirical study comparing agent- and human-authored performance optimization commits, analyzing 324 agent-generated and 83 human-authored PRs from the AIDev dataset across adoption, maintainability, optimization patterns, and validation practices. We find that AI-authored performance PRs are less likely to include explicit performance validation than human-authored PRs (45.7\\% vs. 63.6\\%, $p=0.007$). In addition, AI-authored PRs largely use the same optimization patterns as humans. We further discuss limitations and opportunities for advancing agentic code optimization.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5b9e\u8bc1\u6bd4\u8f83\u4e86AI\u4e0e\u4eba\u7c7b\u5f00\u53d1\u8005\u5728\u6027\u80fd\u4f18\u5316\u63d0\u4ea4\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0AI\u751f\u6210\u7684\u6027\u80fd\u4f18\u5316\u63d0\u4ea4\u5728\u6027\u80fd\u9a8c\u8bc1\u65b9\u9762\u660e\u663e\u8f83\u5c11\u3002", "motivation": "\u6027\u80fd\u4f18\u5316\u662f\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u590d\u6742\u4e14\u5173\u952e\u7684\u73af\u8282\uff0c\u76ee\u524d\u5c1a\u4e0d\u6e05\u695aAI\u4ee3\u7801\u4ee3\u7406\u5728\u771f\u5b9e\u6027\u80fd\u4f18\u5316\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u5982\u4f55\u3002", "method": "\u5206\u6790\u4e86AIDev\u6570\u636e\u96c6\u4e2d324\u4e2aAI\u751f\u6210\u548c83\u4e2a\u4eba\u7c7b\u7f16\u5199\u7684\u6027\u80fd\u4f18\u5316PR\uff0c\u4ece\u91c7\u7528\u7387\u3001\u53ef\u7ef4\u62a4\u6027\u3001\u4f18\u5316\u6a21\u5f0f\u53ca\u9a8c\u8bc1\u5b9e\u8df5\u7b49\u591a\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u4e86\u6bd4\u8f83\u7814\u7a76\u3002", "result": "AI\u751f\u6210\u7684\u6027\u80fd\u4f18\u5316PR\u5728\u5305\u542b\u663e\u5f0f\u6027\u80fd\u9a8c\u8bc1\u7684\u6bd4\u4f8b\u4e0a\u663e\u8457\u4f4e\u4e8e\u4eba\u7c7b\u7f16\u5199\u7684PR\uff0845.7%\u5bf963.6%\uff0cp=0.007\uff09\uff0c\u4f46\u4e24\u8005\u4f7f\u7528\u7684\u4f18\u5316\u6a21\u5f0f\u57fa\u672c\u76f8\u540c\u3002", "conclusion": "AI\u4ee3\u7801\u4ee3\u7406\u5728\u6027\u80fd\u4f18\u5316\u4e2d\u5b58\u5728\u9a8c\u8bc1\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4f46\u6574\u4f53\u4f18\u5316\u6a21\u5f0f\u4e0e\u4eba\u7c7b\u76f8\u4f3c\uff0c\u6709\u63d0\u5347\u7a7a\u95f4\uff0c\u672a\u6765\u7814\u7a76\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a8\u52a8\u667a\u80fd\u4ee3\u7801\u4f18\u5316\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.21711", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21711", "abs": "https://arxiv.org/abs/2512.21711", "authors": ["Yuyi Zhang", "Boyu Tang", "Tianjie Ju", "Sufeng Duan", "Gongshen Liu"], "title": "Do Latent Tokens Think? A Causal and Adversarial Analysis of Chain-of-Continuous-Thought", "comment": "13 pages, 5 figures", "summary": "Latent tokens are gaining attention for enhancing reasoning in large language models (LLMs), yet their internal mechanisms remain unclear. This paper examines the problem from a reliability perspective, uncovering fundamental weaknesses: latent tokens function as uninterpretable placeholders rather than encoding faithful reasoning. While resistant to perturbation, they promote shortcut usage over genuine reasoning. We focus on Chain-of-Continuous-Thought (COCONUT), which claims better efficiency and stability than explicit Chain-of-Thought (CoT) while maintaining performance. We investigate this through two complementary approaches. First, steering experiments perturb specific token subsets, namely COCONUT and explicit CoT. Unlike CoT tokens, COCONUT tokens show minimal sensitivity to steering and lack reasoning-critical information. Second, shortcut experiments evaluate models under biased and out-of-distribution settings. Results on MMLU and HotpotQA demonstrate that COCONUT consistently exploits dataset artifacts, inflating benchmark performance without true reasoning. These findings reposition COCONUT as a pseudo-reasoning mechanism: it generates plausible traces that conceal shortcut dependence rather than faithfully representing reasoning processes.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6f5c\u53d8\u91cf\u6807\u8bb0\u5728\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e2d\u7684\u4f5c\u7528\uff0c\u53d1\u73b0\u5176\u4f5c\u4e3a\u4e0d\u53ef\u89e3\u91ca\u7684\u5360\u4f4d\u7b26\uff0c\u867d\u7a33\u5b9a\u4f46\u6613\u4f9d\u8d56\u6377\u5f84\uff0c\u7f3a\u4e4f\u771f\u5b9e\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u6f5c\u53d8\u91cf\u6807\u8bb0\u5c3d\u7ba1\u88ab\u7528\u6765\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5176\u5185\u90e8\u673a\u5236\u5c1a\u4e0d\u6e05\u6670\uff0c\u5c24\u5176\u5728\u53ef\u9760\u6027\u65b9\u9762\u5b58\u5728\u7591\u95ee\u3002", "method": "\u901a\u8fc7\u9488\u5bf9\u94fe\u5f0f\u8fde\u7eed\u601d\u7ef4(COCONUT)\u548c\u663e\u5f0f\u94fe\u5f0f\u601d\u7ef4(CoT)\u6807\u8bb0\u8fdb\u884c\u5e72\u9884\u8bd5\u9a8c\u548c\u6377\u5f84\u6d4b\u8bd5\uff0c\u5206\u6790\u5176\u5bf9\u63a8\u7406\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "COCONUT\u6807\u8bb0\u5bf9\u5e72\u9884\u4e0d\u654f\u611f\uff0c\u7f3a\u5c11\u5173\u952e\u63a8\u7406\u4fe1\u606f\uff0c\u4e14\u5728\u504f\u5411\u548c\u5206\u5e03\u5916\u73af\u5883\u4e0b\u4f9d\u8d56\u6570\u636e\u96c6\u6377\u5f84\uff0c\u5bfc\u81f4\u6027\u80fd\u865a\u9ad8\u3002", "conclusion": "COCONUT\u6807\u8bb0\u66f4\u50cf\u662f\u4f2a\u63a8\u7406\u673a\u5236\uff0c\u867d\u751f\u6210\u5408\u7406\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u4f46\u63a9\u76d6\u4e86\u5bf9\u6377\u5f84\u7684\u4f9d\u8d56\uff0c\u6ca1\u6709\u771f\u5b9e\u53cd\u6620\u63a8\u7406\u8fc7\u7a0b\u3002"}}
{"id": "2512.21781", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.21781", "abs": "https://arxiv.org/abs/2512.21781", "authors": ["Abdul Ali Bangash", "Tongxu Ge", "Zhimin Zhao", "Arshdeep Singh", "Zitao Wang", "Bram Adams"], "title": "The State of the SBOM Tool Ecosystems: A Comparative Analysis of SPDX and CycloneDX", "comment": null, "summary": "A Software Bill of Materials (SBOM) provides transparency by documenting software component metadata and dependencies. However, SBOM adoption depends on tool ecosystems. With two dominant formats: SPDX and CycloneDX - the ecosystems vary significantly in maturity, tool support, and community engagement. We conduct a quantitative comparison of use cases for 170 publicly advertised SBOM tools, identifying enhancement areas for each format. We compare health metrics of both ecosystems (171 CycloneDX versus 470 SPDX tools) to evaluate robustness and maturity. We quantitatively compare 36,990 issue reports from open-source tools to identify challenges and development opportunities. Finally, we investigate the top 250 open-source projects using each tool ecosystem and compare their health metrics. Our findings reveal distinct characteristics: projects using CycloneDX tools demonstrate higher developer engagement and certain health indicators, while SPDX tools benefit from a more mature ecosystem with broader tool availability and established industry adoption. This research provides insights for developers, contributors, and practitioners regarding complementary strengths of these ecosystems and identifies opportunities for mutual enhancement.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u4e24\u79cd\u4e3b\u6d41\u8f6f\u4ef6\u7269\u6599\u6e05\u5355\uff08SBOM\uff09\u683c\u5f0fSPDX\u548cCycloneDX\u7684\u5de5\u5177\u751f\u6001\u7cfb\u7edf\uff0c\u5206\u6790\u5176\u6210\u719f\u5ea6\u3001\u5de5\u5177\u652f\u6301\u3001\u793e\u533a\u53c2\u4e0e\u5ea6\u53ca\u5f00\u53d1\u5065\u5eb7\u72b6\u51b5\uff0c\u63ed\u793a\u5404\u81ea\u4f18\u52bf\u548c\u6539\u8fdb\u7a7a\u95f4\u3002", "motivation": "SBOM\u7684\u5e7f\u6cdb\u91c7\u7528\u4f9d\u8d56\u4e8e\u76f8\u5173\u5de5\u5177\u751f\u6001\u7cfb\u7edf\uff0c\u4f46SPDX\u548cCycloneDX\u4e24\u5927\u683c\u5f0f\u7684\u751f\u6001\u7cfb\u7edf\u5728\u6210\u719f\u5ea6\u548c\u5de5\u5177\u652f\u6301\u4e0a\u6709\u663e\u8457\u5dee\u5f02\uff0c\u9700\u6df1\u5165\u6bd4\u8f83\u4ee5\u4fc3\u8fdb\u5176\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u5b9a\u91cf\u5206\u6790170\u6b3e\u516c\u5f00SBOM\u5de5\u5177\u3001\u6bd4\u8f83171\u4e2aCycloneDX\u4e0e470\u4e2aSPDX\u5de5\u5177\u7684\u5065\u5eb7\u6307\u6807\u3001\u5206\u679036990\u4e2a\u5f00\u6e90\u5de5\u5177\u7684\u95ee\u9898\u62a5\u544a\uff0c\u4ee5\u53ca\u8c03\u67e5\u4f7f\u7528\u5404\u5de5\u5177\u751f\u6001\u7cfb\u7edf\u7684\u524d250\u5927\u5f00\u6e90\u9879\u76ee\uff0c\u7efc\u5408\u8bc4\u4f30\u4e24\u79cd\u751f\u6001\u7cfb\u7edf\u3002", "result": "\u7814\u7a76\u53d1\u73b0CycloneDX\u751f\u6001\u7cfb\u7edf\u7684\u9879\u76ee\u5f00\u53d1\u8005\u53c2\u4e0e\u5ea6\u8f83\u9ad8\u4e14\u90e8\u5206\u5065\u5eb7\u6307\u6807\u4f18\u5f02\uff0c\u800cSPDX\u751f\u6001\u7cfb\u7edf\u5177\u5907\u66f4\u6210\u719f\u7684\u751f\u6001\u73af\u5883\u3001\u66f4\u591a\u5de5\u5177\u652f\u6301\u53ca\u66f4\u5e7f\u6cdb\u7684\u884c\u4e1a\u8ba4\u53ef\u3002", "conclusion": "\u4e24\u79cdSBOM\u5de5\u5177\u751f\u6001\u7cfb\u7edf\u5404\u5177\u4f18\u52bf\u4e14\u4e92\u8865\uff0c\u7814\u7a76\u4e3a\u5f00\u53d1\u8005\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u77e5\u8bc6\u53c2\u8003\uff0c\u6307\u51fa\u4e86\u4fc3\u8fdb\u53cc\u65b9\u5171\u540c\u63d0\u5347\u7684\u673a\u4f1a\u3002"}}
{"id": "2512.21715", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21715", "abs": "https://arxiv.org/abs/2512.21715", "authors": ["Rui Ke", "Jiahui Xu", "Shenghao Yang", "Kuang Wang", "Feng Jiang", "Haizhou Li"], "title": "CATCH: A Controllable Theme Detection Framework with Contextualized Clustering and Hierarchical Generation", "comment": null, "summary": "Theme detection is a fundamental task in user-centric dialogue systems, aiming to identify the latent topic of each utterance without relying on predefined schemas. Unlike intent induction, which operates within fixed label spaces, theme detection requires cross-dialogue consistency and alignment with personalized user preferences, posing significant challenges. Existing methods often struggle with sparse, short utterances for accurate topic representation and fail to capture user-level thematic preferences across dialogues. To address these challenges, we propose CATCH (Controllable Theme Detection with Contextualized Clustering and Hierarchical Generation), a unified framework that integrates three core components: (1) context-aware topic representation, which enriches utterance-level semantics using surrounding topic segments; (2) preference-guided topic clustering, which jointly models semantic proximity and personalized feedback to align themes across dialogue; and (3) a hierarchical theme generation mechanism designed to suppress noise and produce robust, coherent topic labels. Experiments on a multi-domain customer dialogue benchmark (DSTC-12) demonstrate the effectiveness of CATCH with 8B LLM in both theme clustering and topic generation quality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CATCH\u6846\u67b6\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u8868\u793a\u3001\u4e2a\u6027\u5316\u4e3b\u9898\u805a\u7c7b\u548c\u5c42\u6b21\u5316\u751f\u6210\uff0c\u6709\u6548\u63d0\u5347\u7528\u6237\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u7684\u4e3b\u9898\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u4e3b\u9898\u68c0\u6d4b\u96be\u4ee5\u5904\u7406\u77ed\u4e14\u7a00\u758f\u7684\u7528\u6237\u8bdd\u8bed\uff0c\u4e14\u65e0\u6cd5\u6355\u6349\u8de8\u5bf9\u8bdd\u7684\u7528\u6237\u4e2a\u6027\u5316\u4e3b\u9898\u504f\u597d\uff0c\u5bfc\u81f4\u8bdd\u9898\u8868\u793a\u548c\u4e00\u81f4\u6027\u8f83\u5dee\u3002", "method": "\u63d0\u51fa\u4e86CATCH\u6846\u67b6\uff0c\u5305\u62ec\uff1a1\uff09\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u8bdd\u9898\u8868\u793a\uff0c\u7528\u5468\u56f4\u4e3b\u9898\u4e30\u5bcc\u8bdd\u8bed\u8bed\u4e49\uff1b2\uff09\u504f\u597d\u5f15\u5bfc\u7684\u4e3b\u9898\u805a\u7c7b\uff0c\u7ed3\u5408\u8bed\u4e49\u8ddd\u79bb\u548c\u4e2a\u6027\u5316\u53cd\u9988\uff1b3\uff09\u5c42\u6b21\u5316\u4e3b\u9898\u751f\u6210\u673a\u5236\uff0c\u6291\u5236\u566a\u58f0\u751f\u6210\u8fde\u8d2f\u4e3b\u9898\u6807\u7b7e\u3002", "result": "\u5728\u591a\u9886\u57df\u5ba2\u6237\u5bf9\u8bdd\u57fa\u51c6\uff08DSTC-12\uff09\u4e0a\uff0c\u7ed3\u54088B\u5927\u6a21\u578b\u7684CATCH\u5728\u4e3b\u9898\u805a\u7c7b\u548c\u4e3b\u9898\u751f\u6210\u8d28\u91cf\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "CATCH\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u4e3b\u9898\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\uff0c\u5c24\u5176\u5728\u5904\u7406\u7a00\u758f\u77ed\u8bdd\u8bed\u53ca\u7528\u6237\u4e2a\u6027\u5316\u4e3b\u9898\u9700\u6c42\u4e0a\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2512.21811", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.21811", "abs": "https://arxiv.org/abs/2512.21811", "authors": ["Qiaolin Qin", "Jianchen Zhao", "Heng Li", "Weiyi Shang", "Ettore Merlo"], "title": "A Story About Cohesion and Separation: Label-Free Metric for Log Parser Evaluation", "comment": "Accepted at the 33rd IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER), Research Papers track, 2026", "summary": "Log parsing converts log messages into structured event templates, allowing for automated log analysis and reducing manual inspection effort. To select the most compatible parser for a specific system, multiple evaluation metrics are commonly used for performance comparisons. However, existing evaluation metrics heavily rely on labeled log data, which limits prior studies to a fixed set of datasets and hinders parser evaluations and selections in the industry. Further, we discovered that different versions of ground-truth used in existing studies can lead to inconsistent performance conclusions. Motivated by these challenges, we propose a novel label-free template-level metric, PMSS (parser medoid silhouette score), to evaluate log parser performance. PMSS evaluates both parser grouping and template quality with medoid silhouette analysis and Levenshtein distance within a near-linear time complexity in general. To understand its relationship with label-based template-level metrics, FGA and FTA, we compared their evaluation outcomes for six log parsers on the standard corrected Loghub 2.0 dataset. Our results indicate that log parsers achieving the highest PMSS or FGA exhibit comparable performance, differing by only 2.1% on average in terms of the FGA score; the difference is 9.8% for FTA. PMSS is also significantly (p<1e-8) and positively correlated to both FGA and FTA: the Spearman's rho correlation coefficient of PMSS-FGA and PMSS-FTA are respectively 0.648 and 0.587, close to the coefficient between FGA and FTA (0.670). We further extended our discussion on how to interpret the conclusions from different metrics, identifying challenges in using PMSS, and provided guidelines on conducting parser selections with our metric. PMSS provides a valuable evaluation alternative when ground-truths are inconsistent or labels are unavailable.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65e0\u6807\u7b7e\u7684\u65e5\u5fd7\u89e3\u6790\u8bc4\u4f30\u6307\u6807PMSS\uff0c\u901a\u8fc7\u4e2d\u4f4d\u8f6e\u5ed3\u5206\u6790\u548cLevenshtein\u8ddd\u79bb\u8bc4\u4f30\u89e3\u6790\u5668\u6027\u80fd\uff0c\u5b9e\u73b0\u8fd1\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u3002\u4e0e\u4f20\u7edf\u5e26\u6807\u7b7e\u6307\u6807FGA\u548cFTA\u9ad8\u5ea6\u76f8\u5173\uff0c\u9002\u7528\u4e8e\u6807\u7b7e\u7f3a\u5931\u6216\u4e0d\u4e00\u81f4\u7684\u60c5\u51b5\u3002", "motivation": "\u73b0\u6709\u65e5\u5fd7\u89e3\u6790\u8bc4\u4f30\u6307\u6807\u4e25\u91cd\u4f9d\u8d56\u5e26\u6807\u7b7e\u6570\u636e\uff0c\u9650\u5236\u4e86\u8bc4\u4f30\u7684\u9002\u7528\u8303\u56f4\u4e14\u4e0d\u540c\u7248\u672c\u7684\u6807\u7b7e\u5bfc\u81f4\u7ed3\u8bba\u4e0d\u4e00\u81f4\uff0c\u5f71\u54cd\u5de5\u4e1a\u5e94\u7528\u4e2d\u7684\u89e3\u6790\u5668\u9009\u62e9\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u4e2d\u4f4d\u8f6e\u5ed3\u5206\u6570\uff08PMSS\uff09\u7684\u65e0\u6807\u7b7e\u6a21\u677f\u7ea7\u8bc4\u4ef7\u6307\u6807\uff0c\u7ed3\u5408Levenshtein\u8ddd\u79bb\u8bc4\u4f30\u89e3\u6790\u5668\u7684\u5206\u7ec4\u548c\u6a21\u677f\u8d28\u91cf\uff0c\u65f6\u95f4\u590d\u6742\u5ea6\u8fd1\u4f3c\u7ebf\u6027\u3002\u901a\u8fc7\u5bf9\u6807\u51c6\u6570\u636e\u96c6Loghub 2.0\u4e0a\u516d\u4e2a\u89e3\u6790\u5668\u8fdb\u884c\u8bc4\u6d4b\uff0c\u4e0e\u4f20\u7edf\u6807\u7b7e\u6307\u6807FGA\u548cFTA\u8fdb\u884c\u76f8\u5173\u6027\u6bd4\u8f83\u3002", "result": "PMSS\u6307\u6807\u4e0eFGA\u548cFTA\u4e4b\u95f4\u8868\u73b0\u9ad8\u5ea6\u4e00\u81f4\uff0c\u76f8\u5173\u7cfb\u6570\u5206\u522b\u8fbe\u52300.648\u548c0.587\uff0cPMSS\u80fd\u6709\u6548\u66ff\u4ee3\u5e26\u6807\u7b7e\u6307\u6807\u3002\u89e3\u6790\u5668\u5728PMSS\u548cFGA\u6307\u6807\u4e0a\u7684\u8868\u73b0\u5dee\u5f02\u5e73\u5747\u4ec5\u4e3a2.1%\uff0c\u5728FTA\u4e0a\u4e3a9.8%\u3002PMSS\u7edf\u8ba1\u663e\u8457\u76f8\u5173\uff08p<1e-8\uff09\uff0c\u4e14\u80fd\u89e3\u51b3\u6807\u7b7e\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "conclusion": "PMSS\u4e3a\u65e5\u5fd7\u89e3\u6790\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65e0\u6807\u7b7e\u66ff\u4ee3\u65b9\u6848\uff0c\u7279\u522b\u9002\u5408\u6807\u7b7e\u7f3a\u5931\u6216\u4e0d\u4e00\u81f4\u73af\u5883\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u5de5\u4e1a\u754c\u89e3\u6790\u5668\u9009\u62e9\u7684\u7075\u6d3b\u6027\u548c\u51c6\u786e\u6027\uff0c\u4e14\u63d0\u4f9b\u4e86\u6307\u6807\u89e3\u8bfb\u6307\u5bfc\u548c\u4f7f\u7528\u5efa\u8bae\u3002"}}
{"id": "2512.21787", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.21787", "abs": "https://arxiv.org/abs/2512.21787", "authors": ["Abdullah Alabdullah", "Lifeng Han", "Chenghua Lin"], "title": "Ara-HOPE: Human-Centric Post-Editing Evaluation for Dialectal Arabic to Modern Standard Arabic Translation", "comment": null, "summary": "Dialectal Arabic to Modern Standard Arabic (DA-MSA) translation is a challenging task in Machine Translation (MT) due to significant lexical, syntactic, and semantic divergences between Arabic dialects and MSA. Existing automatic evaluation metrics and general-purpose human evaluation frameworks struggle to capture dialect-specific MT errors, hindering progress in translation assessment. This paper introduces Ara-HOPE, a human-centric post-editing evaluation framework designed to systematically address these challenges. The framework includes a five-category error taxonomy and a decision-tree annotation protocol. Through comparative evaluation of three MT systems (Arabic-centric Jais, general-purpose GPT-3.5, and baseline NLLB-200), Ara-HOPE effectively highlights systematic performance differences between these systems. The results show that dialect-specific terminology and semantic preservation remain the most persistent challenges in DA-MSA translation. Ara-HOPE establishes a new framework for evaluating Dialectal Arabic MT quality and provides actionable guidance for improving dialect-aware MT systems.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Ara-HOPE\uff0c\u4e00\u79cd\u9488\u5bf9\u963f\u62c9\u4f2f\u65b9\u8a00\u5230\u73b0\u4ee3\u6807\u51c6\u963f\u62c9\u4f2f\u8bed\u7ffb\u8bd1\u7684\u4eba\u5de5\u540e\u7f16\u8f91\u8bc4\u4ef7\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u65b9\u8a00\u7279\u5b9a\u9519\u8bef\u7684\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u963f\u62c9\u4f2f\u65b9\u8a00\u548c\u73b0\u4ee3\u6807\u51c6\u963f\u62c9\u4f2f\u8bed\u5728\u8bcd\u6c47\u3001\u53e5\u6cd5\u548c\u8bed\u4e49\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u73b0\u6709\u81ea\u52a8\u8bc4\u4ef7\u6307\u6807\u548c\u901a\u7528\u4eba\u5de5\u8bc4\u4f30\u6846\u67b6\u96be\u4ee5\u51c6\u786e\u8bc4\u4f30\u65b9\u8a00\u963f\u62c9\u4f2f\u8bed\u7ffb\u8bd1\u7684\u8d28\u91cf\uff0c\u963b\u788d\u4e86\u7ffb\u8bd1\u8d28\u91cf\u7684\u63d0\u5347\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u4e94\u7c7b\u9519\u8bef\u5206\u7c7b\u6cd5\u548c\u51b3\u7b56\u6811\u6807\u6ce8\u534f\u8bae\u7684\u4eba\u5de5\u540e\u7f16\u8f91\u8bc4\u4ef7\u6846\u67b6Ara-HOPE\u3002\u901a\u8fc7\u5bf9\u6bd4\u8bc4\u4f30\u4e09\u4e2a\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\uff08\u963f\u62c9\u4f2f\u4e2d\u5fc3\u7684Jais\u3001\u901a\u7528GPT-3.5\u548c\u57fa\u7ebfNLLB-200\uff09\uff0c\u7cfb\u7edf\u5730\u5206\u6790\u5404\u7cfb\u7edf\u7684\u6027\u80fd\u5dee\u5f02\u3002", "result": "Ara-HOPE\u6709\u6548\u63ed\u793a\u4e86\u4e09\u4e2a\u7cfb\u7edf\u5728\u5904\u7406\u65b9\u8a00\u4e13\u7528\u672f\u8bed\u548c\u8bed\u4e49\u4fdd\u6301\u65b9\u9762\u7684\u7cfb\u7edf\u6027\u5dee\u5f02\uff0c\u6307\u51fa\u8fd9\u4e9b\u95ee\u9898\u662f\u65b9\u8a00\u5230\u6807\u51c6\u963f\u62c9\u4f2f\u8bed\u7ffb\u8bd1\u4e2d\u6700\u96be\u514b\u670d\u7684\u6311\u6218\u3002", "conclusion": "Ara-HOPE\u4e3a\u963f\u62c9\u4f2f\u65b9\u8a00\u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4ef7\u5efa\u7acb\u4e86\u65b0\u7684\u6846\u67b6\uff0c\u5e76\u4e3a\u6539\u8fdb\u5177\u5907\u65b9\u8a00\u611f\u77e5\u80fd\u529b\u7684\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2512.21789", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.21789", "abs": "https://arxiv.org/abs/2512.21789", "authors": ["Ting-Hao K. Huang", "Ryan A. Rossi", "Sungchul Kim", "Tong Yu", "Ting-Yao E. Hsu", "Ho Yin", "Ng", "C. Lee Giles"], "title": "Five Years of SciCap: What We Learned and Future Directions for Scientific Figure Captioning", "comment": "Accepted to the 5th Annual AAAI Workshop on AI to Accelerate Science and Engineering (AI2ASE 2026)", "summary": "Between 2021 and 2025, the SciCap project grew from a small seed-funded idea at The Pennsylvania State University (Penn State) into one of the central efforts shaping the scientific figure-captioning landscape. Supported by a Penn State seed grant, Adobe, and the Alfred P. Sloan Foundation, what began as our attempt to test whether domain-specific training, which was successful in text models like SciBERT, could also work for figure captions expanded into a multi-institution collaboration. Over these five years, we curated, released, and continually updated a large collection of figure-caption pairs from arXiv papers, conducted extensive automatic and human evaluations on both generated and author-written captions, navigated the rapid rise of large language models (LLMs), launched annual challenges, and built interactive systems that help scientists write better captions. In this piece, we look back at the first five years of SciCap and summarize the key technical and methodological lessons we learned. We then outline five major unsolved challenges and propose directions for the next phase of research in scientific figure captioning.", "AI": {"tldr": "SciCap\u9879\u76ee\u4ece2021\u5e74\u5c0f\u89c4\u6a21\u542f\u52a8\u53d1\u5c55\u6210\u5b66\u672f\u754c\u91cd\u8981\u7684\u79d1\u5b66\u56fe\u8868\u6ce8\u91ca\u9879\u76ee\uff0c\u6db5\u76d6\u6570\u636e\u96c6\u6784\u5efa\u3001\u6a21\u578b\u8bc4\u4f30\u4e0e\u6311\u6218\uff0c\u63a8\u52a8\u9886\u57df\u8fdb\u6b65\u3002", "motivation": "\u63a2\u7d22\u9886\u57df\u4e13\u7528\u8bad\u7ec3\u65b9\u6cd5\u662f\u5426\u9002\u7528\u4e8e\u79d1\u5b66\u56fe\u8868\u6ce8\u91ca\uff0c\u5e76\u63d0\u5347\u79d1\u5b66\u5bb6\u56fe\u6ce8\u5199\u4f5c\u8d28\u91cf\u3002", "method": "\u6784\u5efa\u5e76\u66f4\u65b0\u5927\u89c4\u6a21\u56fe\u8868-\u6ce8\u91ca\u5bf9\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u81ea\u52a8\u548c\u4eba\u5de5\u8bc4\u6d4b\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4e3e\u529e\u5e74\u5ea6\u6311\u6218\u8d5b\uff0c\u5f00\u53d1\u4ea4\u4e92\u5199\u4f5c\u5de5\u5177\u3002", "result": "\u4e94\u5e74\u5185\u9879\u76ee\u9010\u6e10\u6269\u5927\u5f71\u54cd\uff0c\u4ea7\u51fa\u4e30\u5bcc\u6570\u636e\u4e0e\u8bc4\u6d4b\u57fa\u51c6\uff0c\u63a8\u52a8\u591a\u673a\u6784\u5408\u4f5c\u548c\u6280\u672f\u8fdb\u6b65\uff0c\u79d1\u5b66\u56fe\u6ce8\u5199\u4f5c\u80fd\u529b\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u603b\u7ed3\u6280\u672f\u4e0e\u65b9\u6cd5\u7ecf\u9a8c\uff0c\u6307\u51fa\u79d1\u5b66\u56fe\u6ce8\u9886\u57df\u4e94\u5927\u672a\u89e3\u6311\u6218\uff0c\u89c4\u5212\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u63a8\u52a8\u9886\u57df\u6301\u7eed\u53d1\u5c55\u3002"}}
{"id": "2512.22043", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.22043", "abs": "https://arxiv.org/abs/2512.22043", "authors": ["Zhangbo Long", "Letian Sha", "Jiaye Pan", "Dongpeng Xu", "Yifei Huang", "Fu Xiao"], "title": "HALF: Process Hollowing Analysis Framework for Binary Programs with the Assistance of Kernel Modules", "comment": null, "summary": "Binary program analysis is still very important in system security. There are many practical achievements in binary code analysis, but fine-grained analysis such as dynamic taint analysis, is constantly studied due to the problem of deployability, high memory usage, and performance overhead, so as to better adapt to the new analysis scenario, such as memory corruption exploits and sandbox evasion malware. This paper presents a new binary program analysis framework, in order to improve the usability and performance of fine-grained analysis. The framework mainly uses the kernel module to further expand the analysis capability of the traditional dynamic binary instrumentation. Then, based on the idea of decoupling analysis, the analysis environment is constructed in the container process through process hollowing techniques in a new way. It can reuse the functions of the existing dynamic binary instrumentation platforms and also reduce the impact on the execution of the target program. The prototype is implemented on the Windows platform. The validity and performance of the framework are verified by a large number of experiments with benchmark and actual programs. The effectiveness of the framework is also verified by the analysis of actual exploit programs and malicious code, demonstrating the value of the practical application.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5185\u6838\u6a21\u5757\u548c\u8fdb\u7a0b\u7a7a\u6d1e\u6280\u672f\u7684\u65b0\u578b\u4e8c\u8fdb\u5236\u7a0b\u5e8f\u5206\u6790\u6846\u67b6\uff0c\u4ee5\u63d0\u5347\u7ec6\u7c92\u5ea6\u52a8\u6001\u6c61\u70b9\u5206\u6790\u7684\u53ef\u7528\u6027\u548c\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u5728Windows\u5e73\u53f0\u4e0a\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u4ef7\u503c\u3002", "motivation": "\u5f53\u524d\u7ec6\u7c92\u5ea6\u4e8c\u8fdb\u5236\u4ee3\u7801\u5206\u6790\u5982\u52a8\u6001\u6c61\u70b9\u5206\u6790\u5b58\u5728\u90e8\u7f72\u96be\u5ea6\u5927\u3001\u5185\u5b58\u6d88\u8017\u9ad8\u548c\u6027\u80fd\u5f00\u9500\u5927\u7b49\u95ee\u9898\uff0c\u96be\u4ee5\u9002\u5e94\u5185\u5b58\u7834\u574f\u5229\u7528\u548c\u6c99\u7bb1\u9003\u907f\u7b49\u65b0\u573a\u666f\u3002", "method": "\u5229\u7528\u5185\u6838\u6a21\u5757\u6269\u5c55\u4f20\u7edf\u52a8\u6001\u4e8c\u8fdb\u5236\u63d2\u88c5\u6280\u672f\u7684\u5206\u6790\u80fd\u529b\uff0c\u5e76\u91c7\u7528\u8fdb\u7a0b\u7a7a\u6d1e\u6280\u672f\u5728\u5bb9\u5668\u8fdb\u7a0b\u4e2d\u6784\u5efa\u5206\u6790\u73af\u5883\uff0c\u5b9e\u73b0\u5206\u6790\u73af\u5883\u4e0e\u76ee\u6807\u7a0b\u5e8f\u7684\u89e3\u8026\uff0c\u517c\u987e\u590d\u7528\u73b0\u6709\u5e73\u53f0\u529f\u80fd\u548c\u964d\u4f4e\u7a0b\u5e8f\u6267\u884c\u5f71\u54cd\u3002", "result": "\u5728Windows\u5e73\u53f0\u5b9e\u73b0\u4e86\u539f\u578b\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u5927\u91cf\u57fa\u51c6\u6d4b\u8bd5\u548c\u5b9e\u9645\u7a0b\u5e8f\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u548c\u6027\u80fd\uff0c\u80fd\u51c6\u786e\u5206\u6790\u5b9e\u9645\u6f0f\u6d1e\u5229\u7528\u548c\u6076\u610f\u4ee3\u7801\u3002", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u7ec6\u7c92\u5ea6\u52a8\u6001\u4e8c\u8fdb\u5236\u5206\u6790\u7684\u5b9e\u7528\u6027\u548c\u6027\u80fd\uff0c\u5177\u5907\u826f\u597d\u7684\u5e94\u7528\u4ef7\u503c\u548c\u63a8\u5e7f\u6f5c\u529b\u3002"}}
{"id": "2512.21809", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.21809", "abs": "https://arxiv.org/abs/2512.21809", "authors": ["Vitthal Bhandari"], "title": "On The Conceptualization and Societal Impact of Cross-Cultural Bias", "comment": "Term paper for LING 575 (Societal Impacts of Language Technologies)", "summary": "Research has shown that while large language models (LLMs) can generate their responses based on cultural context, they are not perfect and tend to generalize across cultures. However, when evaluating the cultural bias of a language technology on any dataset, researchers may choose not to engage with stakeholders actually using that technology in real life, which evades the very fundamental problem they set out to address.\n  Inspired by the work done by arXiv:2005.14050v2, I set out to analyse recent literature about identifying and evaluating cultural bias in Natural Language Processing (NLP). I picked out 20 papers published in 2025 about cultural bias and came up with a set of observations to allow NLP researchers in the future to conceptualize bias concretely and evaluate its harms effectively. My aim is to advocate for a robust assessment of the societal impact of language technologies exhibiting cross-cultural bias.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e862025\u5e74\u53d1\u8868\u768420\u7bc7\u5173\u4e8e\u6587\u5316\u504f\u89c1\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u8bba\u6587\uff0c\u63d0\u51fa\u4e86\u8bc4\u4ef7\u6587\u5316\u504f\u89c1\u7684\u65b0\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u867d\u80fd\u57fa\u4e8e\u6587\u5316\u80cc\u666f\u751f\u6210\u56de\u7b54\uff0c\u4f46\u5b58\u5728\u8de8\u6587\u5316\u6cdb\u5316\u7684\u95ee\u9898\uff0c\u4e14\u8bc4\u4f30\u6587\u5316\u504f\u89c1\u65f6\u7f3a\u4e4f\u4e0e\u5b9e\u9645\u4f7f\u7528\u8005\u7684\u4e92\u52a8\uff0c\u5bfc\u81f4\u504f\u89c1\u95ee\u9898\u672a\u88ab\u6709\u6548\u89e3\u51b3\u3002", "method": "\u501f\u9274arXiv:2005.14050v2\u7684\u7814\u7a76\uff0c\u7cfb\u7edf\u5ba1\u89c6\u76f8\u5173\u6587\u732e\uff0c\u901a\u8fc7\u5206\u679020\u7bc72025\u5e74\u7684\u8bba\u6587\uff0c\u603b\u7ed3\u51fa\u4e00\u5957\u5177\u4f53\u6982\u5ff5\u5316\u6587\u5316\u504f\u89c1\u53ca\u5176\u5371\u5bb3\u8bc4\u4f30\u7684\u65b9\u6cd5\u6846\u67b6\u3002", "result": "\u63d0\u51fa\u4e00\u5957\u89c2\u5bdf\u548c\u8bc4\u4ef7\u6587\u5316\u504f\u89c1\u7684\u65b9\u6cd5\uff0c\u5e2e\u52a9\u672a\u6765NLP\u7814\u7a76\u8005\u66f4\u5177\u4f53\u5730\u7406\u89e3\u548c\u8bc4\u4f30\u8de8\u6587\u5316\u504f\u89c1\u7684\u793e\u4f1a\u5f71\u54cd\u3002", "conclusion": "\u547c\u5401\u5bf9\u8bed\u8a00\u6280\u672f\u4e2d\u5b58\u5728\u7684\u8de8\u6587\u5316\u504f\u89c1\u8fdb\u884c\u66f4\u6709\u529b\u7684\u793e\u4f1a\u5f71\u54cd\u8bc4\u4f30\uff0c\u4ee5\u63a8\u52a8\u516c\u6b63\u548c\u6709\u6548\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u53d1\u5c55\u3002"}}
{"id": "2512.22054", "categories": ["cs.SE", "cs.DC"], "pdf": "https://arxiv.org/pdf/2512.22054", "abs": "https://arxiv.org/abs/2512.22054", "authors": ["Giuseppe De Palma", "Saverio Giallorenzo"], "title": "Proceedings First Workshop on Adaptable Cloud Architectures", "comment": null, "summary": "This volume contains the post-proceedings of the Workshop on Adaptable Cloud Architectures (WACA 2025), held on June 20, 2025, in Lille, France, co-located with DisCoTec 2025 - 20th International Federated Conference on Distributed Computing Techniques.", "AI": {"tldr": "\u672c\u6587\u662f2025\u5e746\u670820\u65e5\u5728\u6cd5\u56fd\u91cc\u5c14\u4e3e\u529e\u7684\u201c\u9002\u5e94\u6027\u4e91\u67b6\u6784\u201d\u7814\u8ba8\u4f1a\u7684\u540e\u7eed\u8bba\u6587\u96c6\u3002", "motivation": "\u63a2\u8ba8\u9002\u5e94\u6027\u4e91\u67b6\u6784\u9886\u57df\u7684\u6700\u65b0\u7814\u7a76\u548c\u8fdb\u5c55\uff0c\u4fc3\u8fdb\u5b66\u672f\u4ea4\u6d41\u3002", "method": "\u7ec4\u7ec7\u4e13\u9898\u7814\u8ba8\u4f1a\uff0c\u6536\u96c6\u5e76\u53d1\u8868\u76f8\u5173\u9886\u57df\u7684\u6700\u65b0\u7814\u7a76\u8bba\u6587\u3002", "result": "\u6c47\u7f16\u4e86\u591a\u7bc7\u5173\u4e8e\u9002\u5e94\u6027\u4e91\u67b6\u6784\u7684\u7814\u7a76\u8bba\u6587\uff0c\u53cd\u6620\u8be5\u9886\u57df\u7684\u7814\u7a76\u73b0\u72b6\u3002", "conclusion": "\u901a\u8fc7\u6b64\u6b21\u7814\u8ba8\u4f1a\u63a8\u52a8\u4e86\u9002\u5e94\u6027\u4e91\u67b6\u6784\u9886\u57df\u7684\u5b66\u672f\u53d1\u5c55\u4e0e\u5408\u4f5c\u3002"}}
{"id": "2512.21817", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.21817", "abs": "https://arxiv.org/abs/2512.21817", "authors": ["Hong Su"], "title": "Method Decoration (DeMe): A Framework for LLM-Driven Adaptive Method Generation in Dynamic IoT Environments", "comment": null, "summary": "Intelligent IoT systems increasingly rely on large language models (LLMs) to generate task-execution methods for dynamic environments. However, existing approaches lack the ability to systematically produce new methods when facing previously unseen situations, and they often depend on fixed, device-specific logic that cannot adapt to changing environmental conditions.In this paper, we propose Method Decoration (DeMe), a general framework that modifies the method-generation path of an LLM using explicit decorations derived from hidden goals, accumulated learned methods, and environmental feedback. Unlike traditional rule augmentation, decorations in DeMe are not hardcoded; instead, they are extracted from universal behavioral principles, experience, and observed environmental differences. DeMe enables the agent to reshuffle the structure of its method path-through pre-decoration, post-decoration, intermediate-step modification, and step insertion-thereby producing context-aware, safety-aligned, and environment-adaptive methods. Experimental results show that method decoration allows IoT devices to derive ore appropriate methods when confronting unknown or faulty operating conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86Method Decoration (DeMe)\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u65b9\u6cd5\u8def\u5f84\uff0c\u4f7f\u667a\u80fd\u7269\u8054\u7f51\u7cfb\u7edf\u5728\u9762\u5bf9\u672a\u77e5\u73af\u5883\u65f6\u80fd\u751f\u6210\u9002\u5e94\u6027\u66f4\u5f3a\u7684\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u7cfb\u7edf\u5730\u751f\u6210\u65b0\u65b9\u6cd5\uff0c\u5e94\u5bf9\u672a\u77e5\u60c5\u51b5\uff0c\u4e14\u4f9d\u8d56\u56fa\u5b9a\u3001\u8bbe\u5907\u7279\u5b9a\u7684\u903b\u8f91\uff0c\u7f3a\u4e4f\u73af\u5883\u9002\u5e94\u6027\u3002", "method": "DeMe\u901a\u8fc7\u9690\u542b\u76ee\u6807\u3001\u79ef\u7d2f\u7684\u7ecf\u9a8c\u548c\u73af\u5883\u53cd\u9988\uff0c\u52a8\u6001\u4fee\u9970\u65b9\u6cd5\u751f\u6210\u8def\u5f84\uff0c\u5305\u62ec\u524d\u7f6e\u4fee\u9970\u3001\u540e\u7f6e\u4fee\u9970\u3001\u4e2d\u95f4\u6b65\u9aa4\u4fee\u6539\u548c\u6b65\u9aa4\u63d2\u5165\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDeMe\u4f7f\u7269\u8054\u7f51\u8bbe\u5907\u80fd\u5728\u672a\u77e5\u6216\u6545\u969c\u72b6\u6001\u4e0b\u751f\u6210\u66f4\u5408\u9002\u7684\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u65b9\u6cd5\u7684\u73af\u5883\u9002\u5e94\u6027\u548c\u5b89\u5168\u6027\u3002", "conclusion": "DeMe\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u52a8\u6001\u73af\u5883\u4e2d\u751f\u6210\u65b9\u6cd5\u7684\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u73af\u5883\u9002\u5e94\uff0c\u589e\u5f3a\u4e86\u7269\u8054\u7f51\u7cfb\u7edf\u7684\u667a\u80fd\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2512.21837", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.21837", "abs": "https://arxiv.org/abs/2512.21837", "authors": ["Siyu Li", "Chenwei Song", "Wan Zhou", "Xinyi Liu"], "title": "Knowledge Reasoning of Large Language Models Integrating Graph-Structured Information for Pest and Disease Control in Tobacco", "comment": null, "summary": "This paper proposes a large language model (LLM) approach that integrates graph-structured information for knowledge reasoning in tobacco pest and disease control. Built upon the GraphRAG framework, the proposed method enhances knowledge retrieval and reasoning by explicitly incorporating structured information from a domain-specific knowledge graph. Specifically, LLMs are first leveraged to assist in the construction of a tobacco pest and disease knowledge graph, which organizes key entities such as diseases, symptoms, control methods, and their relationships. Based on this graph, relevant knowledge is retrieved and integrated into the reasoning process to support accurate answer generation. The Transformer architecture is adopted as the core inference model, while a graph neural network (GNN) is employed to learn expressive node representations that capture both local and global relational information within the knowledge graph. A ChatGLM-based model serves as the backbone LLM and is fine-tuned using LoRA to achieve parameter-efficient adaptation. Extensive experimental results demonstrate that the proposed approach consistently outperforms baseline methods across multiple evaluation metrics, significantly improving both the accuracy and depth of reasoning, particularly in complex multi-hop and comparative reasoning scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u56fe\u7ed3\u6784\u4fe1\u606f\u7684\u77e5\u8bc6\u63a8\u7406\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65b9\u6cd5\uff0c\u63d0\u5347\u70df\u8349\u75c5\u866b\u5bb3\u9632\u63a7\u7684\u77e5\u8bc6\u68c0\u7d22\u4e0e\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u9488\u5bf9\u70df\u8349\u75c5\u866b\u5bb3\u9632\u63a7\u9886\u57df\u77e5\u8bc6\u590d\u6742\u4e14\u5173\u7cfb\u7d27\u5bc6\u7684\u95ee\u9898\uff0c\u63d0\u5347\u77e5\u8bc6\u63a8\u7406\u7684\u51c6\u786e\u6027\u548c\u6df1\u5ea6\u3002", "method": "\u57fa\u4e8eGraphRAG\u6846\u67b6\uff0c\u6784\u5efa\u5305\u542b\u75c5\u5bb3\u3001\u75c7\u72b6\u3001\u63a7\u5236\u65b9\u6cd5\u7b49\u5173\u952e\u5b9e\u4f53\u53ca\u5176\u5173\u7cfb\u7684\u77e5\u8bc6\u56fe\u8c31\uff1b\u5229\u7528Transformer\u4f5c\u4e3a\u63a8\u7406\u6a21\u578b\uff0c\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u8282\u70b9\u8868\u793a\uff1b\u91c7\u7528ChatGLM\u5927\u6a21\u578b\u5e76\u901a\u8fc7LoRA\u5fae\u8c03\u5b9e\u73b0\u53c2\u6570\u9ad8\u6548\u9002\u914d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u8bc4\u4ef7\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5c24\u5176\u5728\u590d\u6742\u591a\u8df3\u548c\u6bd4\u8f83\u63a8\u7406\u60c5\u5883\u4e2d\u8868\u73b0\u66f4\u4f73\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u6709\u6548\u878d\u5408\u9886\u57df\u77e5\u8bc6\u56fe\u8c31\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u70df\u8349\u75c5\u866b\u5bb3\u9632\u63a7\u77e5\u8bc6\u63a8\u7406\u7684\u51c6\u786e\u6027\u548c\u6df1\u5ea6\u3002"}}
{"id": "2512.21842", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.21842", "abs": "https://arxiv.org/abs/2512.21842", "authors": ["Baorong Huang", "Ali Asiri"], "title": "AlignAR: Generative Sentence Alignment for Arabic-English Parallel Corpora of Legal and Literary Texts", "comment": null, "summary": "High-quality parallel corpora are essential for Machine Translation (MT) research and translation teaching. However, Arabic-English resources remain scarce and existing datasets mainly consist of simple one-to-one mappings. In this paper, we present AlignAR, a generative sentence alignment method, and a new Arabic-English dataset comprising complex legal and literary texts. Our evaluation demonstrates that \"Easy\" datasets lack the discriminatory power to fully assess alignment methods. By reducing one-to-one mappings in our \"Hard\" subset, we exposed the limitations of traditional alignment methods. In contrast, LLM-based approaches demonstrated superior robustness, achieving an overall F1-score of 85.5%, a 9% improvement over previous methods. Our datasets and codes are open-sourced at https://github.com/XXX.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86AlignAR\uff0c\u4e00\u79cd\u751f\u6210\u5f0f\u53e5\u5b50\u5bf9\u9f50\u65b9\u6cd5\uff0c\u5e76\u53d1\u5e03\u4e86\u4e00\u4e2a\u5305\u542b\u590d\u6742\u6cd5\u5f8b\u548c\u6587\u5b66\u6587\u672c\u7684\u963f\u62c9\u4f2f\u8bed-\u82f1\u8bed\u5e76\u884c\u6570\u636e\u96c6\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65b9\u6cd5\u5728\u590d\u6742\u5bf9\u9f50\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002", "motivation": "\u9ad8\u8d28\u91cf\u963f\u62c9\u4f2f\u8bed-\u82f1\u8bed\u5e73\u884c\u8bed\u6599\u7a00\u7f3a\uff0c\u73b0\u6709\u6570\u636e\u96c6\u591a\u4e3a\u7b80\u5355\u4e00\u5bf9\u4e00\u6620\u5c04\uff0c\u96be\u4ee5\u5145\u5206\u8bc4\u4f30\u53e5\u5b50\u5bf9\u9f50\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86AlignAR\u751f\u6210\u5f0f\u53e5\u5b50\u5bf9\u9f50\u65b9\u6cd5\uff0c\u5e76\u6784\u5efa\u5305\u542b\u590d\u6742\u6587\u672c\u7684\u201cHard\u201d\u5b50\u96c6\u4ee5\u6311\u6218\u4f20\u7edf\u5bf9\u9f50\u65b9\u6cd5\u3002", "result": "LLM\u65b9\u6cd5\u5728\u201cHard\u201d\u5b50\u96c6\u8868\u73b0\u51fa85.5%\u7684F1\u5206\u6570\uff0c\u6bd4\u4f20\u7edf\u65b9\u6cd5\u9ad8\u51fa9%\uff0c\u663e\u793a\u51fa\u66f4\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u751f\u6210\u5f0f\u5bf9\u9f50\u65b9\u6cd5\u53ca\u590d\u6742\u6570\u636e\u96c6\u5bf9\u4e8e\u63a8\u52a8\u963f\u62c9\u4f2f\u8bed-\u82f1\u8bed\u53e5\u5b50\u5bf9\u9f50\u7814\u7a76\u7684\u91cd\u8981\u4ef7\u503c\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.21849", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21849", "abs": "https://arxiv.org/abs/2512.21849", "authors": ["Jiaxin Liu", "Peiyi Tu", "Wenyu Chen", "Yihong Zhuang", "Xinxia Ling", "Anji Zhou", "Chenxi Wang", "Zhuo Han", "Zhengkai Yang", "Junbo Zhao", "Zenan Huang", "Yuanyuan Wang"], "title": "HeartBench: Probing Core Dimensions of Anthropomorphic Intelligence in LLMs", "comment": "10 pages", "summary": "While Large Language Models (LLMs) have achieved remarkable success in cognitive and reasoning benchmarks, they exhibit a persistent deficit in anthropomorphic intelligence-the capacity to navigate complex social, emotional, and ethical nuances. This gap is particularly acute in the Chinese linguistic and cultural context, where a lack of specialized evaluation frameworks and high-quality socio-emotional data impedes progress. To address these limitations, we present HeartBench, a framework designed to evaluate the integrated emotional, cultural, and ethical dimensions of Chinese LLMs. Grounded in authentic psychological counseling scenarios and developed in collaboration with clinical experts, the benchmark is structured around a theory-driven taxonomy comprising five primary dimensions and 15 secondary capabilities. We implement a case-specific, rubric-based methodology that translates abstract human-like traits into granular, measurable criteria through a ``reasoning-before-scoring'' evaluation protocol. Our assessment of 13 state-of-the-art LLMs indicates a substantial performance ceiling: even leading models achieve only 60% of the expert-defined ideal score. Furthermore, analysis using a difficulty-stratified ``Hard Set'' reveals a significant performance decay in scenarios involving subtle emotional subtexts and complex ethical trade-offs. HeartBench establishes a standardized metric for anthropomorphic AI evaluation and provides a methodological blueprint for constructing high-quality, human-aligned training data.", "AI": {"tldr": "HeartBench\u662f\u4e00\u4e2a\u4e13\u4e3a\u8bc4\u4f30\u4e2d\u6587\u5927\u578b\u8bed\u8a00\u6a21\u578b\u793e\u4f1a\u60c5\u611f\u4f26\u7406\u80fd\u529b\u7684\u6846\u67b6\uff0c\u57fa\u4e8e\u5fc3\u7406\u54a8\u8be2\u573a\u666f\u548c\u4e13\u5bb6\u5408\u4f5c\uff0c\u901a\u8fc7\u7ec6\u81f4\u7684\u8bc4\u5206\u6807\u51c6\u6d4b\u91cf\u6a21\u578b\u8868\u73b0\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u590d\u6742\u60c5\u611f\u548c\u4f26\u7406\u5224\u65ad\u4e0a\u8868\u73b0\u6709\u9650\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8ba4\u77e5\u63a8\u7406\u8868\u73b0\u7a81\u51fa\uff0c\u4f46\u5728\u590d\u6742\u793e\u4f1a\u3001\u60c5\u611f\u548c\u4f26\u7406\u573a\u666f\u7684\u62df\u4eba\u667a\u80fd\u65b9\u9762\u5b58\u5728\u663e\u8457\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u4e2d\u6587\u8bed\u5883\u4e0b\u7f3a\u4e4f\u76f8\u5e94\u8bc4\u4f30\u6846\u67b6\u548c\u9ad8\u8d28\u91cf\u6570\u636e\uff0c\u5236\u7ea6\u6a21\u578b\u53d1\u5c55\u3002", "method": "HeartBench\u57fa\u4e8e\u771f\u5b9e\u5fc3\u7406\u54a8\u8be2\u573a\u666f\uff0c\u7ed3\u5408\u4e34\u5e8a\u4e13\u5bb6\u610f\u89c1\uff0c\u6784\u5efa\u5305\u542b5\u4e2a\u4e3b\u7ef4\u5ea6\u548c15\u4e2a\u6b21\u80fd\u529b\u7684\u7406\u8bba\u9a71\u52a8\u5206\u7c7b\u4f53\u7cfb\uff0c\u91c7\u7528\u6848\u4f8b\u5177\u4f53\u7684\u8bc4\u5206\u7ec6\u5219\u548c\u201c\u5148\u63a8\u7406\u540e\u8bc4\u5206\u201d\u7684\u8bc4\u4f30\u534f\u8bae\uff0c\u5c06\u4eba\u7c7b\u7279\u8d28\u7ec6\u5316\u4e3a\u53ef\u91cf\u5316\u6807\u51c6\u3002", "result": "\u5bf913\u4e2a\u5148\u8fdb\u6a21\u578b\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u5373\u4f7f\u662f\u9886\u5148\u6a21\u578b\u4e5f\u4ec5\u8fbe\u5230\u4e13\u5bb6\u5b9a\u4e49\u7406\u60f3\u5206\u6570\u768460%\uff0c\u5c24\u5176\u5728\u590d\u6742\u60c5\u611f\u7ec6\u8282\u548c\u4f26\u7406\u6743\u8861\u4e2d\u6027\u80fd\u660e\u663e\u4e0b\u964d\u3002", "conclusion": "HeartBench\u4e3a\u62df\u4eba\u5316\u4eba\u5de5\u667a\u80fd\u80fd\u529b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u7684\u5ea6\u91cf\u6307\u6807\u548c\u65b9\u6cd5\u8bba\u53c2\u8003\uff0c\u540c\u65f6\u4e3a\u6784\u5efa\u9ad8\u8d28\u91cf\u3001\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u5bf9\u9f50\u7684\u8bad\u7ec3\u6570\u636e\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2512.21859", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.21859", "abs": "https://arxiv.org/abs/2512.21859", "authors": ["Qi Fan", "An Zou", "Yehan Ma"], "title": "TimeBill: Time-Budgeted Inference for Large Language Models", "comment": "Accepted to AAAI 2026", "summary": "Large Language Models (LLMs) are increasingly deployed in time-critical systems, such as robotics, autonomous driving, embodied intelligence, and industrial automation, where generating accurate responses within a given time budget is crucial for decision-making, control, or safety-critical tasks. However, the auto-regressive generation process of LLMs makes it challenging to model and estimate the end-to-end execution time. Furthermore, existing efficient inference methods based on a fixed key-value (KV) cache eviction ratio struggle to adapt to varying tasks with diverse time budgets, where an improper eviction ratio may lead to incomplete inference or a drop in response performance. In this paper, we propose TimeBill, a novel time-budgeted inference framework for LLMs that balances the inference efficiency and response performance. To be more specific, we propose a fine-grained response length predictor (RLP) and an execution time estimator (ETE) to accurately predict the end-to-end execution time of LLMs. Following this, we develop a time-budgeted efficient inference approach that adaptively adjusts the KV cache eviction ratio based on execution time prediction and the given time budget. Finally, through extensive experiments, we demonstrate the advantages of TimeBill in improving task completion rate and maintaining response performance under various overrun strategies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86TimeBill\uff0c\u4e00\u4e2a\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u65f6\u95f4\u5173\u952e\u7cfb\u7edf\u4e2d\u7684\u63a8\u7406\u6846\u67b6\uff0c\u80fd\u591f\u5728\u7ed9\u5b9a\u65f6\u95f4\u9884\u7b97\u5185\u9ad8\u6548\u751f\u6210\u54cd\u5e94\u3002", "motivation": "\u73b0\u6709LLMs\u7684\u81ea\u56de\u5f52\u751f\u6210\u8fc7\u7a0b\u96be\u4ee5\u51c6\u786e\u9884\u6d4b\u6574\u4f53\u6267\u884c\u65f6\u95f4\uff0c\u4e14\u56fa\u5b9aKV\u7f13\u5b58\u9a71\u9010\u7b56\u7565\u65e0\u6cd5\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\u7684\u65f6\u95f4\u9884\u7b97\uff0c\u5f71\u54cd\u63a8\u7406\u5b8c\u6574\u6027\u548c\u54cd\u5e94\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u7ec6\u7c92\u5ea6\u54cd\u5e94\u957f\u5ea6\u9884\u6d4b\u5668\uff08RLP\uff09\u548c\u6267\u884c\u65f6\u95f4\u4f30\u8ba1\u5668\uff08ETE\uff09\u6765\u51c6\u786e\u9884\u6d4bLLM\u7684\u603b\u6267\u884c\u65f6\u95f4\uff0c\u6839\u636e\u9884\u6d4b\u7ed3\u679c\u548c\u65f6\u95f4\u9884\u7b97\u81ea\u9002\u5e94\u8c03\u6574KV\u7f13\u5b58\u9a71\u9010\u6bd4\u4f8b\uff0c\u5f62\u6210\u65f6\u95f4\u9884\u7b97\u63a8\u7406\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTimeBill\u80fd\u591f\u63d0\u5347\u4efb\u52a1\u5b8c\u6210\u7387\uff0c\u540c\u65f6\u5728\u4e0d\u540c\u65f6\u95f4\u8d85\u9650\u7b56\u7565\u4e0b\u4fdd\u6301\u54cd\u5e94\u6027\u80fd\u3002", "conclusion": "TimeBill\u6709\u6548\u5e73\u8861\u4e86\u63a8\u7406\u6548\u7387\u548c\u54cd\u5e94\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u65f6\u95f4\u654f\u611f\u7684LLM\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2512.21871", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.21871", "abs": "https://arxiv.org/abs/2512.21871", "authors": ["Naen Xu", "Jinghuai Zhang", "Changjiang Li", "Hengyu An", "Chunyi Zhou", "Jun Wang", "Boyu Xu", "Yuyuan Li", "Tianyu Du", "Shouling Ji"], "title": "Bridging the Copyright Gap: Do Large Vision-Language Models Recognize and Respect Copyrighted Content?", "comment": "AAAI 2026 (Oral)", "summary": "Large vision-language models (LVLMs) have achieved remarkable advancements in multimodal reasoning tasks. However, their widespread accessibility raises critical concerns about potential copyright infringement. Will LVLMs accurately recognize and comply with copyright regulations when encountering copyrighted content (i.e., user input, retrieved documents) in the context? Failure to comply with copyright regulations may lead to serious legal and ethical consequences, particularly when LVLMs generate responses based on copyrighted materials (e.g., retrieved book experts, news reports). In this paper, we present a comprehensive evaluation of various LVLMs, examining how they handle copyrighted content -- such as book excerpts, news articles, music lyrics, and code documentation when they are presented as visual inputs. To systematically measure copyright compliance, we introduce a large-scale benchmark dataset comprising 50,000 multimodal query-content pairs designed to evaluate how effectively LVLMs handle queries that could lead to copyright infringement. Given that real-world copyrighted content may or may not include a copyright notice, the dataset includes query-content pairs in two distinct scenarios: with and without a copyright notice. For the former, we extensively cover four types of copyright notices to account for different cases. Our evaluation reveals that even state-of-the-art closed-source LVLMs exhibit significant deficiencies in recognizing and respecting the copyrighted content, even when presented with the copyright notice. To solve this limitation, we introduce a novel tool-augmented defense framework for copyright compliance, which reduces infringement risks in all scenarios. Our findings underscore the importance of developing copyright-aware LVLMs to ensure the responsible and lawful use of copyrighted content.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5728\u5904\u7406\u7248\u6743\u5185\u5bb9\u65f6\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u5728\u8bc6\u522b\u548c\u9075\u5b88\u7248\u6743\u89c4\u5b9a\u65b9\u9762\u5b58\u5728\u663e\u8457\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5de5\u5177\u589e\u5f3a\u7684\u9632\u62a4\u6846\u67b6\u4ee5\u51cf\u5c11\u7248\u6743\u4fb5\u6743\u98ce\u9669\u3002", "motivation": "\u968f\u7740LVLMs\u5728\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u7248\u6743\u4fb5\u6743\u95ee\u9898\u65e5\u76ca\u4e25\u91cd\uff0c\u4e9f\u9700\u8bc4\u4f30\u548c\u63d0\u9ad8\u6a21\u578b\u5bf9\u7248\u6743\u5185\u5bb9\u7684\u8bc6\u522b\u4e0e\u5408\u89c4\u80fd\u529b\u3002", "method": "\u6784\u5efa\u5305\u542b5\u4e07\u6761\u591a\u6a21\u6001\u67e5\u8be2\u5185\u5bb9\u5bf9\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5206\u522b\u5305\u542b\u6709\u65e0\u7248\u6743\u58f0\u660e\u4e24\u79cd\u60c5\u666f\uff0c\u8bc4\u4f30LVLMs\u5bf9\u4e66\u7c4d\u6458\u5f55\u3001\u65b0\u95fb\u6587\u7ae0\u3001\u6b4c\u8bcd\u548c\u4ee3\u7801\u6587\u6863\u7b49\u89c6\u89c9\u8f93\u5165\u7684\u7248\u6743\u5408\u89c4\u6027\uff1b\u5e76\u63d0\u51fa\u4e00\u79cd\u5de5\u5177\u589e\u5f3a\u7684\u9632\u62a4\u6846\u67b6\u4ee5\u6539\u5584\u7248\u6743\u5408\u89c4\u6548\u679c\u3002", "result": "\u8bc4\u6d4b\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684\u95ed\u6e90LVLMs\u5728\u8bc6\u522b\u548c\u5c0a\u91cd\u7248\u6743\u5185\u5bb9\u65b9\u9762\u4ecd\u5b58\u5728\u663e\u8457\u7f3a\u9677\uff0c\u5c24\u5176\u662f\u5728\u8bfb\u53d6\u7248\u6743\u58f0\u660e\u65f6\u4e5f\u96be\u4ee5\u5b8c\u5168\u9075\u5b88\u7248\u6743\u89c4\u5b9a\uff1b\u6240\u63d0\u9632\u62a4\u6846\u67b6\u5728\u5404\u79cd\u60c5\u666f\u4e0b\u663e\u8457\u964d\u4f4e\u4fb5\u6743\u98ce\u9669\u3002", "conclusion": "\u53d1\u5c55\u7248\u6743\u611f\u77e5\u7684LVLMs\u5bf9\u4e8e\u786e\u4fdd\u7248\u6743\u5185\u5bb9\u7684\u5408\u7406\u5408\u6cd5\u4f7f\u7528\u81f3\u5173\u91cd\u8981\uff0c\u672c\u6587\u63d0\u51fa\u7684\u9632\u62a4\u6846\u67b6\u4e3a\u89e3\u51b3\u7248\u6743\u5408\u89c4\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u8def\u5f84\u3002"}}
{"id": "2512.21877", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21877", "abs": "https://arxiv.org/abs/2512.21877", "authors": ["Vaibhav Devraj", "Dhruv Kumar", "Jagat Sesh Challa"], "title": "CricBench: A Multilingual Benchmark for Evaluating LLMs in Cricket Analytics", "comment": "Under Review", "summary": "Cricket is the second most popular sport globally, commanding a massive following of over 2.5 billion fans globally. Enthusiasts and analysts frequently seek advanced statistical insights, such as long-term historical performance trends or complex player comparisons, that are often unavailable through standard web searches. While Large Language Models (LLMs) have advanced significantly in Text-to-SQL tasks, their capability to handle the domain-specific nuances, complex schema variations, and multilingual requirements inherent to sports analytics remains under-explored. To investigate this potential capability gap, we present CricBench, a comprehensive benchmark suite for evaluating LLMs on specialized cricket data. To curate a \"Gold Standard\" dataset, we collaborate with domain experts in cricket and SQL to manually author complex queries, ensuring logical correctness. Recognizing linguistic diversity, we construct the benchmark in both English and Hindi, establishing a framework that is open for further extension to other regional languages. We evaluate six state-of-the-art models, including GPT-4o, Claude 3.7 Sonnet, and open-source models, using a strict evaluation protocol. Our results reveal that high performance on general benchmarks does not guarantee success in specialized domains. While the open-weights reasoning model DeepSeek R1 achieves state-of-the-art performance (50.6%), surpassing proprietary giants like Claude 3.7 Sonnet (47.7%) and GPT-4o (33.7%), it still exhibits a significant accuracy drop when moving from general benchmarks (BIRD) to CricBench. Furthermore, we observe that code-mixed Hindi queries frequently yield parity or higher accuracy compared to English, challenging the assumption that English is the optimal prompt language for specialized SQL tasks.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u677f\u7403\u9886\u57df\u7684\u6570\u636e\u5206\u6790\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u652f\u6301\u591a\u8bed\u8a00\u7684\u4e13\u95e8\u8bc4\u6d4b\u57fa\u51c6CricBench\uff0c\u7528\u4ee5\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u9886\u57df\u7279\u5b9aSQL\u67e5\u8be2\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u901a\u7528Text-to-SQL\u4efb\u52a1\u4e0a\u867d\u6709\u8fdb\u6b65\uff0c\u4f46\u5728\u4f53\u80b2\u5206\u6790\u8fd9\u79cd\u9886\u57df\u7279\u5b9a\u7684\u590d\u6742\u6570\u636e\u548c\u591a\u8bed\u79cd\u73af\u5883\u4e0b\u8868\u73b0\u5c1a\u672a\u5145\u5206\u63a2\u8ba8\uff0c\u4e14\u6807\u51c6\u7f51\u7edc\u641c\u7d22\u96be\u4ee5\u6ee1\u8db3\u6df1\u5ea6\u7edf\u8ba1\u5206\u6790\u9700\u6c42\u3002", "method": "\u6784\u5efa\u6db5\u76d6\u82f1\u8bed\u4e0e\u5370\u5730\u8bed\u7684CricBench\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u96c6\u5408\u677f\u7403\u4e13\u5bb6\u4e0eSQL\u4e13\u5bb6\u624b\u52a8\u7f16\u5199\u590d\u6742\u67e5\u8be2\uff1b\u91c7\u7528\u4e25\u683c\u8bc4\u6d4b\u7b56\u7565\uff0c\u6d4b\u8bd5\u5305\u62ecGPT-4o\u3001Claude 3.7 Sonnet\u53ca\u5f00\u6e90\u6a21\u578b\u5728\u5185\u7684\u516d\u79cd\u5148\u8fdb\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "\u53d1\u73b0\u9ad8\u901a\u7528\u6027\u80fd\u6a21\u578b\u4e0d\u5fc5\u7136\u5728\u4e13\u4e1a\u9886\u57df\u6709\u4f18\u52bf\uff0c\u5f00\u6e90\u6a21\u578bDeepSeek R1\u8868\u73b0\u6700\u4f73\uff0850.6%\u51c6\u786e\u7387\uff09\uff0c\u4f18\u4e8eClaude 3.7 Sonnet\u548cGPT-4o\uff1b\u4e14\u5370\u5730\u8bed\u4ee3\u7801\u6df7\u5408\u67e5\u8be2\u51c6\u786e\u7387\u4e0d\u4f4e\u4e8e\u82f1\u8bed\uff0c\u8868\u660e\u6bcd\u8bed\u6df7\u5408\u63d0\u793a\u8bed\u5177\u6709\u6f5c\u529b\u3002", "conclusion": "\u4e13\u95e8\u9886\u57df\u7684SQL\u67e5\u8be2\u5b58\u5728\u6027\u80fd\u6311\u6218\uff0c\u9ad8\u6027\u80fd\u901a\u7528\u6a21\u578b\u4e0d\u4fdd\u8bc1\u9886\u57df\u9002\u7528\u6027\uff1b\u591a\u8bed\u8a00\u5c24\u5176\u662f\u4ee3\u7801\u6df7\u5408\u8bed\u8a00\u63d0\u793a\u4e3a\u672a\u6765\u7814\u7a76\u65b9\u5411\u63d0\u4f9b\u65b0\u89c6\u89d2\u3002"}}
{"id": "2512.21902", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.21902", "abs": "https://arxiv.org/abs/2512.21902", "authors": ["Sachin Pawar", "Girish Keshav Palshikar", "Anindita Sinha Banerjee", "Nitin Ramrakhiyani", "Basit Ali"], "title": "Explainable Statute Prediction via Attention-based Model and LLM Prompting", "comment": null, "summary": "In this paper, we explore the problem of automatic statute prediction where for a given case description, a subset of relevant statutes are to be predicted. Here, the term \"statute\" refers to a section, a sub-section, or an article of any specific Act. Addressing this problem would be useful in several applications such as AI-assistant for lawyers and legal question answering system. For better user acceptance of such Legal AI systems, we believe the predictions should also be accompanied by human understandable explanations. We propose two techniques for addressing this problem of statute prediction with explanations -- (i) AoS (Attention-over-Sentences) which uses attention over sentences in a case description to predict statutes relevant for it and (ii) LLMPrompt which prompts an LLM to predict as well as explain relevance of a certain statute. AoS uses smaller language models, specifically sentence transformers and is trained in a supervised manner whereas LLMPrompt uses larger language models in a zero-shot manner and explores both standard as well as Chain-of-Thought (CoT) prompting techniques. Both these models produce explanations for their predictions in human understandable forms. We compare statute prediction performance of both the proposed techniques with each other as well as with a set of competent baselines, across two popular datasets. Also, we evaluate the quality of the generated explanations through an automated counter-factual manner as well as through human evaluation.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u81ea\u52a8\u6cd5\u6761\u9884\u6d4b\u95ee\u9898\uff0c\u63d0\u51fa\u4e24\u79cd\u6a21\u578b\uff08AoS\u548cLLMPrompt\uff09\u5728\u9884\u6d4b\u6cd5\u6761\u7684\u540c\u65f6\u751f\u6210\u53ef\u7406\u89e3\u7684\u89e3\u91ca\uff0c\u5e76\u901a\u8fc7\u591a\u79cd\u65b9\u5f0f\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "motivation": "\u81ea\u52a8\u6cd5\u6761\u9884\u6d4b\u6709\u52a9\u4e8e\u6cd5\u5f8bAI\u52a9\u624b\u548c\u6cd5\u5f8b\u95ee\u7b54\u7cfb\u7edf\uff0c\u63d0\u9ad8\u6cd5\u5f8b\u670d\u52a1\u7684\u6548\u7387\u548c\u53ef\u63a5\u53d7\u6027\uff0c\u800c\u53ef\u89e3\u91ca\u6027\u662f\u63d0\u5347\u7528\u6237\u4fe1\u4efb\u7684\u5173\u952e\u3002", "method": "\u63d0\u51faAoS\u6a21\u578b\uff08\u57fa\u4e8e\u53e5\u5b50\u6ce8\u610f\u529b\u7684\u6709\u76d1\u7763\u5c0f\u6a21\u578b\uff09\u548cLLMPrompt\u65b9\u6cd5\uff08\u57fa\u4e8e\u5927\u6a21\u578b\u7684\u96f6\u6837\u672c\u63d0\u793a\uff0c\u5305\u62ec\u6807\u51c6\u548cChain-of-Thought\u63d0\u793a\uff09\u6765\u5206\u522b\u9884\u6d4b\u76f8\u5173\u6cd5\u6761\u53ca\u5176\u89e3\u91ca\u3002", "result": "\u5728\u4e24\u4e2a\u6d41\u884c\u6570\u636e\u96c6\u4e0a\uff0c\u6240\u63d0\u4e24\u79cd\u65b9\u6cd5\u5728\u6cd5\u6761\u9884\u6d4b\u4e0e\u89e3\u91ca\u8d28\u91cf\u65b9\u9762\u5747\u4f18\u4e8e\u591a\u4e2a\u57fa\u7ebf\u6a21\u578b\uff0c\u89e3\u91ca\u901a\u8fc7\u81ea\u52a8\u53cd\u4e8b\u5b9e\u65b9\u6cd5\u548c\u4eba\u5de5\u8bc4\u4f30\u5747\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u7ed3\u5408\u9884\u6d4b\u4e0e\u53ef\u89e3\u91ca\u80fd\u529b\u7684\u6a21\u578b\u6709\u52a9\u4e8e\u6cd5\u5f8bAI\u7cfb\u7edf\u7684\u5b9e\u7528\u6027\u548c\u7528\u6237\u4fe1\u4efb\uff0c\u5c55\u793a\u4e86\u4e0d\u540c\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5728\u6cd5\u5f8b\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2512.21911", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.21911", "abs": "https://arxiv.org/abs/2512.21911", "authors": ["Jikai Wang", "Jianchao Tan", "Yuxuan Hu", "Jiayu Qin", "Yerui Sun", "Yuchen Xie", "Xunliang Cai", "Juntao Li", "Min Zhang"], "title": "Accelerate Speculative Decoding with Sparse Computation in Verification", "comment": "Pre-print", "summary": "Speculative decoding accelerates autoregressive language model inference by verifying multiple draft tokens in parallel. However, the verification stage often becomes the dominant computational bottleneck, especially for long-context inputs and mixture-of-experts (MoE) models. Existing sparsification methods are designed primarily for standard token-by-token autoregressive decoding to remove substantial computational redundancy in LLMs. This work systematically adopts different sparse methods on the verification stage of the speculative decoding and identifies structured redundancy across multiple dimensions. Based on these observations, we propose a sparse verification framework that jointly sparsifies attention, FFN, and MoE components during the verification stage to reduce the dominant computation cost. The framework further incorporates an inter-draft token and inter-layer retrieval reuse strategy to further reduce redundant computation without introducing additional training. Extensive experiments across summarization, question answering, and mathematical reasoning datasets demonstrate that the proposed methods achieve favorable efficiency-accuracy trade-offs, while maintaining stable acceptance length.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7a00\u758f\u9a8c\u8bc1\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u6295\u673a\u89e3\u7801\u7684\u9a8c\u8bc1\u9636\u6bb5\u8054\u5408\u7a00\u758f\u5316\u6ce8\u610f\u529b\u3001\u524d\u9988\u7f51\u7edc\u548c\u4e13\u5bb6\u6df7\u5408\u7ec4\u4ef6\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff0c\u5b9e\u73b0\u4e86\u52a0\u901f\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u3002", "motivation": "\u6295\u673a\u89e3\u7801\u7684\u9a8c\u8bc1\u9636\u6bb5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u5c24\u5176\u5728\u957f\u4e0a\u4e0b\u6587\u548c\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u4e2d\u6210\u4e3a\u6027\u80fd\u74f6\u9888\uff0c\u73b0\u6709\u7a00\u758f\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u6807\u51c6\u9010\u8bcd\u89e3\u7801\uff0c\u96be\u4ee5\u6709\u6548\u964d\u4f4e\u9a8c\u8bc1\u9636\u6bb5\u7684\u5197\u4f59\u8ba1\u7b97\u3002", "method": "\u7cfb\u7edf\u6027\u91c7\u7528\u591a\u79cd\u7a00\u758f\u65b9\u6cd5\u4e8e\u9a8c\u8bc1\u9636\u6bb5\uff0c\u8bc6\u522b\u9a8c\u8bc1\u9636\u6bb5\u591a\u7ef4\u5ea6\u7ed3\u6784\u5197\u4f59\uff0c\u8054\u5408\u7a00\u758f\u6ce8\u610f\u529b\u3001\u524d\u9988\u7f51\u7edc\u548c\u6df7\u5408\u4e13\u5bb6\u7ec4\u4ef6\uff0c\u540c\u65f6\u5f15\u5165\u8349\u7a3f\u95f4\u4ee4\u724c\u548c\u5c42\u95f4\u68c0\u7d22\u590d\u7528\u7b56\u7565\uff0c\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "result": "\u5728\u6587\u672c\u6458\u8981\u3001\u95ee\u7b54\u548c\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\uff0c\u65b9\u6cd5\u5b9e\u73b0\u826f\u597d\u7684\u6548\u7387-\u51c6\u786e\u7387\u6743\u8861\uff0c\u4e14\u7ef4\u6301\u7a33\u5b9a\u7684\u63a5\u53d7\u957f\u5ea6\uff0c\u663e\u8457\u964d\u4f4e\u4e3b\u5bfc\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "\u63d0\u51fa\u7684\u7a00\u758f\u9a8c\u8bc1\u6846\u67b6\u6709\u6548\u7f13\u89e3\u4e86\u6295\u673a\u89e3\u7801\u9a8c\u8bc1\u9636\u6bb5\u7684\u8ba1\u7b97\u74f6\u9888\uff0c\u63d0\u5347\u4e86\u957f\u4e0a\u4e0b\u6587\u548c\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2512.21919", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.21919", "abs": "https://arxiv.org/abs/2512.21919", "authors": ["KaShun Shum", "Binyuan Hui", "Jiawei Chen", "Lei Zhang", "X. W.", "Jiaxi Yang", "Yuzhen Huang", "Junyang Lin", "Junxian He"], "title": "SWE-RM: Execution-free Feedback For Software Engineering Agents", "comment": "21 pages", "summary": "Execution-based feedback like unit testing is widely used in the development of coding agents through test-time scaling (TTS) and reinforcement learning (RL). This paradigm requires scalable and reliable collection of unit test cases to provide accurate feedback, and the resulting feedback is often sparse and cannot effectively distinguish between trajectories that are both successful or both unsuccessful. In contrast, execution-free feedback from reward models can provide more fine-grained signals without depending on unit test cases. Despite this potential, execution-free feedback for realistic software engineering (SWE) agents remains underexplored. Aiming to develop versatile reward models that are effective across TTS and RL, however, we observe that two verifiers with nearly identical TTS performance can nevertheless yield very different results in RL. Intuitively, TTS primarily reflects the model's ability to select the best trajectory, but this ability does not necessarily generalize to RL. To address this limitation, we identify two additional aspects that are crucial for RL training: classification accuracy and calibration. We then conduct comprehensive controlled experiments to investigate how to train a robust reward model that performs well across these metrics. In particular, we analyze the impact of various factors such as training data scale, policy mixtures, and data source composition. Guided by these investigations, we introduce SWE-RM, an accurate and robust reward model adopting a mixture-of-experts architecture with 30B total parameters and 3B activated during inference. SWE-RM substantially improves SWE agents on both TTS and RL performance. For example, it increases the accuracy of Qwen3-Coder-Flash from 51.6% to 62.0%, and Qwen3-Coder-Max from 67.0% to 74.6% on SWE-Bench Verified using TTS, achieving new state-of-the-art performance among open-source models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SWE-RM\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u63d0\u5347\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u667a\u80fd\u4f53\u5728\u6d4b\u8bd5\u65f6\u6269\u5c55(TTS)\u548c\u5f3a\u5316\u5b66\u4e60(RL)\u4e2d\u7684\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u65b0\u72b6\u6001\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6267\u884c\u7684\u53cd\u9988\u4f9d\u8d56\u4e8e\u5355\u5143\u6d4b\u8bd5\u7528\u4f8b\uff0c\u53cd\u9988\u7a00\u758f\u4e14\u96be\u533a\u5206\u4e0d\u540c\u8f68\u8ff9\uff1b\u6267\u884c\u81ea\u7531\u7684\u5956\u52b1\u6a21\u578b\u867d\u80fd\u63d0\u4f9b\u66f4\u7ec6\u7c92\u5ea6\u4fe1\u53f7\uff0c\u4f46\u5728\u5b9e\u9645\u8f6f\u4ef6\u5de5\u7a0b\u667a\u80fd\u4f53\u4e2d\u5e94\u7528\u4e0d\u5145\u5206\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6267\u884c\u81ea\u7531\u5956\u52b1\u6a21\u578b\u5728\u5206\u7c7b\u51c6\u786e\u7387\u548c\u6821\u51c6\u4e0a\u7684\u8868\u73b0\uff0c\u7814\u7a76\u8bad\u7ec3\u6570\u636e\u89c4\u6a21\u3001\u7b56\u7565\u6df7\u5408\u53ca\u6570\u636e\u6e90\u7ec4\u6210\u5bf9\u6a21\u578b\u8868\u73b0\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u5305\u542b30\u4ebf\u53c2\u6570\u7684\u6df7\u5408\u4e13\u5bb6\u67b6\u6784SWE-RM\u5956\u52b1\u6a21\u578b\u3002", "result": "SWE-RM\u663e\u8457\u63d0\u5347\u8f6f\u4ef6\u5de5\u7a0b\u667a\u80fd\u4f53\u5728TTS\u548cRL\u4e2d\u7684\u8868\u73b0\uff0cQwen3-Coder-Flash\u51c6\u786e\u7387\u63d0\u5347\u81f362.0%\uff0cQwen3-Coder-Max\u63d0\u5347\u81f374.6%\uff0c\u5728SWE-Bench Verified\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u5f00\u6e90\u6a21\u578b\u7684\u65b0\u6700\u4f18\u3002", "conclusion": "\u4e3a\u5b9e\u73b0\u6267\u884c\u81ea\u7531\u53cd\u9988\u5728\u8f6f\u4ef6\u5de5\u7a0b\u667a\u80fd\u4f53\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u6784\u5efa\u51c6\u786e\u4e14\u9c81\u68d2\u7684\u5956\u52b1\u6a21\u578b\u81f3\u5173\u91cd\u8981\uff0cSWE-RM\u4e3aTTS\u548cRL\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u5c55\u73b0\u4e86\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2512.21933", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.21933", "abs": "https://arxiv.org/abs/2512.21933", "authors": ["Sachin Pawar", "Manoj Apte", "Kshitij Jadhav", "Girish Keshav Palshikar", "Nitin Ramrakhiyani"], "title": "Broken Words, Broken Performance: Effect of Tokenization on Performance of LLMs", "comment": "International Joint Conference on Natural Language Processing & Asia-Pacific Chapter of the Association for Computational Linguistics (IJCNLP-AACL 2025)", "summary": "Tokenization is the first step in training any Large Language Model (LLM), where the text is split into a sequence of tokens as per the model's fixed vocabulary. This tokenization in LLMs is different from the traditional tokenization in NLP where the text is split into a sequence of \"natural\" words. In LLMs, a natural word may also be broken into multiple tokens due to limited vocabulary size of the LLMs (e.g., Mistral's tokenizer splits \"martial\" into \"mart\" and \"ial\"). In this paper, we hypothesize that such breaking of natural words negatively impacts LLM performance on various NLP tasks. To quantify this effect, we propose a set of penalty functions that compute a tokenization penalty for a given text for a specific LLM, indicating how \"bad\" the tokenization is. We establish statistical significance of our hypothesis on multiple NLP tasks for a set of different LLMs.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u8bcd\u8bed\u88ab\u62c6\u5206\u6210\u591a\u4e2a\u5b50\u8bcd\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u5957\u91cf\u5316\u8bcd\u5143\u5207\u5206\u60e9\u7f5a\u7684\u51fd\u6570\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bcd\u5143\u5207\u5206\u65f6\uff0c\u7531\u4e8e\u8bcd\u6c47\u8868\u6709\u9650\uff0c\u81ea\u7136\u5355\u8bcd\u53ef\u80fd\u88ab\u62c6\u5206\u6210\u591a\u4e2a\u5b50\u8bcd\uff0c\u8fd9\u53ef\u80fd\u964d\u4f4e\u6a21\u578b\u5728\u4e0b\u6e38\u4efb\u52a1\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u7ec4\u60e9\u7f5a\u51fd\u6570\uff0c\u7528\u4e8e\u8ba1\u7b97\u7ed9\u5b9a\u6587\u672c\u5728\u7279\u5b9aLLM\u4e0a\u7684\u8bcd\u5143\u5207\u5206\u60e9\u7f5a\uff0c\u4ee5\u8861\u91cf\u5207\u5206\u8d28\u91cf\u3002", "result": "\u5728\u591a\u4e2aLLM\u548c\u591a\u4e2a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e0a\uff0c\u7edf\u8ba1\u9a8c\u8bc1\u4e86\u8bcd\u5143\u62c6\u5206\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u8d1f\u9762\u5f71\u54cd\u3002", "conclusion": "\u8bcd\u5143\u5207\u5206\u7684\u4e0d\u5408\u7406\u62c6\u5206\u4f1a\u663e\u8457\u5f71\u54cdLLM\u7684\u6027\u80fd\uff0c\u6539\u8fdb\u5207\u5206\u7b56\u7565\u53ef\u80fd\u63d0\u5347\u6a21\u578b\u6548\u679c\u3002"}}
{"id": "2512.21956", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.21956", "abs": "https://arxiv.org/abs/2512.21956", "authors": ["Tal Halevi", "Yarden Tzach", "Ronit D. Gross", "Shalom Rosner", "Ido Kanter"], "title": "Self-attention vector output similarities reveal how machines pay attention", "comment": "22 pages, 13 figures", "summary": "The self-attention mechanism has significantly advanced the field of natural language processing, facilitating the development of advanced language-learning machines. Although its utility is widely acknowledged, the precise mechanisms of self-attention underlying its advanced learning and the quantitative characterization of this learning process remains an open research question. This study introduces a new approach for quantifying information processing within the self-attention mechanism. The analysis conducted on the BERT-12 architecture reveals that, in the final layers, the attention map focuses on sentence separator tokens, suggesting a practical approach to text segmentation based on semantic features. Based on the vector space emerging from the self-attention heads, a context similarity matrix, measuring the scalar product between two token vectors was derived, revealing distinct similarities between different token vector pairs within each head and layer. The findings demonstrated that different attention heads within an attention block focused on different linguistic characteristics, such as identifying token repetitions in a given text or recognizing a token of common appearance in the text and its surrounding context. This specialization is also reflected in the distribution of distances between token vectors with high similarity as the architecture progresses. The initial attention layers exhibit substantially long-range similarities; however, as the layers progress, a more short-range similarity develops, culminating in a preference for attention heads to create strong similarities within the same sentence. Finally, the behavior of individual heads was analyzed by examining the uniqueness of their most common tokens in their high similarity elements. Each head tends to focus on a unique token from the text and builds similarity pairs centered around it.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u4fe1\u606f\u5904\u7406\u8fdb\u884c\u4e86\u91cf\u5316\u5206\u6790\uff0c\u53d1\u73b0BERT\u6a21\u578b\u7684\u4e0d\u540c\u6ce8\u610f\u529b\u5934\u5177\u6709\u4e0d\u540c\u7684\u8bed\u8a00\u7279\u5f81\u4e13\u6ce8\u70b9\uff0c\u4e14\u968f\u7740\u5c42\u6570\u589e\u52a0\uff0c\u76f8\u4f3c\u6027\u4ece\u957f\u8ddd\u79bb\u8f6c\u5411\u77ed\u8ddd\u79bb\uff0c\u6700\u7ec8\u96c6\u4e2d\u4e8e\u540c\u4e00\u53e5\u5b50\u5185\u3002", "motivation": "\u5c3d\u7ba1\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u5177\u4f53\u7684\u5185\u90e8\u4fe1\u606f\u5904\u7406\u673a\u5236\u53ca\u5176\u5b66\u4e60\u8fc7\u7a0b\u7684\u91cf\u5316\u8868\u5f81\u4ecd\u672a\u660e\u786e\uff0c\u8feb\u5207\u9700\u8981\u6df1\u5165\u7406\u89e3\u3002", "method": "\u901a\u8fc7\u5206\u6790BERT-12\u6a21\u578b\u4e2d\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u6ce8\u610f\u529b\u56fe\u548c\u5411\u91cf\u7a7a\u95f4\uff0c\u6784\u5efa\u4e0a\u4e0b\u6587\u76f8\u4f3c\u5ea6\u77e9\u9635\uff0c\u91cf\u5316\u4e0d\u540c\u5c42\u548c\u4e0d\u540c\u5934\u7684\u8bed\u4e49\u7279\u5f81\u4e13\u6ce8\u60c5\u51b5\uff0c\u5e76\u7814\u7a76\u76f8\u4f3c\u5ea6\u5206\u5e03\u548c\u5934\u90e8\u7684\u4e13\u5c5e\u805a\u7126\u4ee4\u724c\u3002", "result": "\u53d1\u73b0\u6700\u7ec8\u5c42\u7684\u6ce8\u610f\u529b\u56fe\u96c6\u4e2d\u5728\u53e5\u5b50\u5206\u9694\u7b26\uff0c\u8868\u660e\u57fa\u4e8e\u8bed\u4e49\u7279\u5f81\u7684\u6587\u672c\u5206\u5272\u65b9\u6cd5\u6709\u6548\uff1b\u4e0d\u540c\u5934\u5173\u6ce8\u4e0d\u540c\u8bed\u8a00\u7279\u5f81\uff0c\u5982\u8bc6\u522b\u91cd\u590d\u4ee4\u724c\u6216\u5e38\u89c1\u4ee4\u724c\uff1b\u76f8\u4f3c\u5ea6\u5206\u5e03\u663e\u793a\u5c42\u6b21\u8d8a\u6df1\uff0c\u6ce8\u610f\u529b\u504f\u5411\u77ed\u7a0b\u548c\u540c\u4e00\u53e5\u5b50\u5185\uff1b\u6bcf\u4e2a\u5934\u503e\u5411\u4e8e\u56f4\u7ed5\u552f\u4e00\u4ee4\u724c\u6784\u5efa\u76f8\u4f3c\u5bf9\u3002", "conclusion": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5185\u90e8\u7684\u8bed\u4e49\u4e13\u6ce8\u53ca\u5206\u5e03\u7279\u6027\uff0c\u4e3a\u7406\u89e3\u5176\u5b66\u4e60\u673a\u5236\u548c\u63d0\u5347\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u91cf\u5316\u5de5\u5177\u548c\u89c6\u89d2\u3002"}}
{"id": "2512.22087", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.22087", "abs": "https://arxiv.org/abs/2512.22087", "authors": ["Shukai Liu", "Jian Yang", "Bo Jiang", "Yizhi Li", "Jinyang Guo", "Xianglong Liu", "Bryan Dai"], "title": "Context as a Tool: Context Management for Long-Horizon SWE-Agents", "comment": null, "summary": "Agents based on large language models have recently shown strong potential on real-world software engineering (SWE) tasks that require long-horizon interaction with repository-scale codebases. However, most existing agents rely on append-only context maintenance or passively triggered compression heuristics, which often lead to context explosion, semantic drift, and degraded reasoning in long-running interactions. We propose CAT, a new context management paradigm that elevates context maintenance to a callable tool integrated into the decision-making process of agents. CAT formalizes a structured context workspace consisting of stable task semantics, condensed long-term memory, and high-fidelity short-term interactions, and enables agents to proactively compress historical trajectories into actionable summaries at appropriate milestones. To support context management for SWE-agents, we propose a trajectory-level supervision framework, CAT-GENERATOR, based on an offline data construction pipeline that injects context-management actions into complete interaction trajectories. Using this framework, we train a context-aware model, SWE-Compressor. Experiments on SWE-Bench-Verified demonstrate that SWE-Compressor reaches a 57.6% solved rate and significantly outperforms ReAct-based agents and static compression baselines, while maintaining stable and scalable long-horizon reasoning under a bounded context budget.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CAT\uff0c\u4e00\u79cd\u96c6\u6210\u4e8e\u4ee3\u7406\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u4e0a\u4e0b\u6587\u5de5\u4f5c\u7a7a\u95f4\u548c\u4e3b\u52a8\u538b\u7f29\u5386\u53f2\u4ea4\u4e92\uff0c\u5b9e\u73b0\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u957f\u65f6\u4ea4\u4e92\u7684\u9ad8\u6548\u7ba1\u7406\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u901a\u5e38\u4f9d\u8d56\u4ec5\u8ffd\u52a0\u4e0a\u4e0b\u6587\u6216\u88ab\u52a8\u538b\u7f29\uff0c\u5bfc\u81f4\u4e0a\u4e0b\u6587\u7206\u70b8\u3001\u8bed\u4e49\u6f02\u79fb\u53ca\u63a8\u7406\u80fd\u529b\u4e0b\u964d\u3002", "method": "\u63d0\u51faCAT\u6846\u67b6\uff0c\u5176\u5305\u542b\u7a33\u5b9a\u4efb\u52a1\u8bed\u4e49\u3001\u51dd\u7ec3\u957f\u671f\u8bb0\u5fc6\u548c\u9ad8\u4fdd\u771f\u77ed\u671f\u4ea4\u4e92\u7684\u7ed3\u6784\u5316\u4e0a\u4e0b\u6587\u5de5\u4f5c\u7a7a\u95f4\uff0c\u5e76\u5f15\u5165CAT-GENERATOR\u8f68\u8ff9\u7ea7\u76d1\u7763\u6846\u67b6\u8bad\u7ec3\u4e0a\u4e0b\u6587\u611f\u77e5\u6a21\u578bSWE-Compressor\uff0c\u652f\u6301\u4e3b\u52a8\u538b\u7f29\u5386\u53f2\u8f68\u8ff9\u4e3a\u53ef\u64cd\u4f5c\u6458\u8981\u3002", "result": "SWE-Compressor\u5728SWE-Bench-Verified\u6d4b\u8bd5\u4e2d\uff0c\u89e3\u51b3\u7387\u8fbe57.6%\uff0c\u663e\u8457\u4f18\u4e8eReAct\u4ee3\u7406\u548c\u9759\u6001\u538b\u7f29\u57fa\u7ebf\uff0c\u540c\u65f6\u5728\u9650\u5236\u4e0a\u4e0b\u6587\u9884\u7b97\u4e0b\u4fdd\u6301\u7a33\u5b9a\u4e14\u53ef\u6269\u5c55\u7684\u957f\u65f6\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "CAT\u65b9\u6848\u6709\u6548\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u548c\u957f\u65f6\u63a8\u7406\u8868\u73b0\uff0c\u63a8\u52a8\u4e86\u5b9e\u9645\u590d\u6742SWE\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2512.22100", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22100", "abs": "https://arxiv.org/abs/2512.22100", "authors": ["Duygu Altinok"], "title": "Introducing TrGLUE and SentiTurca: A Comprehensive Benchmark for Turkish General Language Understanding and Sentiment Analysis", "comment": "under review by Springer", "summary": "Evaluating the performance of various model architectures, such as transformers, large language models (LLMs), and other NLP systems, requires comprehensive benchmarks that measure performance across multiple dimensions. Among these, the evaluation of natural language understanding (NLU) is particularly critical as it serves as a fundamental criterion for assessing model capabilities. Thus, it is essential to establish benchmarks that enable thorough evaluation and analysis of NLU abilities from diverse perspectives. While the GLUE benchmark has set a standard for evaluating English NLU, similar benchmarks have been developed for other languages, such as CLUE for Chinese, FLUE for French, and JGLUE for Japanese. However, no comparable benchmark currently exists for the Turkish language. To address this gap, we introduce TrGLUE, a comprehensive benchmark encompassing a variety of NLU tasks for Turkish. In addition, we present SentiTurca, a specialized benchmark for sentiment analysis. To support researchers, we also provide fine-tuning and evaluation code for transformer-based models, facilitating the effective use of these benchmarks. TrGLUE comprises Turkish-native corpora curated to mirror the domains and task formulations of GLUE-style evaluations, with labels obtained through a semi-automated pipeline that combines strong LLM-based annotation, cross-model agreement checks, and subsequent human validation. This design prioritizes linguistic naturalness, minimizes direct translation artifacts, and yields a scalable, reproducible workflow. With TrGLUE, our goal is to establish a robust evaluation framework for Turkish NLU, empower researchers with valuable resources, and provide insights into generating high-quality semi-automated datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u571f\u8033\u5176\u8bed\u81ea\u7136\u8bed\u8a00\u7406\u89e3\uff08NLU\uff09\u7684\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6TrGLUE\u53ca\u60c5\u611f\u5206\u6790\u4e13\u7528\u57fa\u51c6SentiTurca\uff0c\u65e8\u5728\u586b\u8865\u571f\u8033\u5176\u8bedNLU\u8bc4\u4f30\u7684\u7a7a\u767d\u3002", "motivation": "\u5f53\u524d\u7f3a\u5c11\u9002\u7528\u4e8e\u571f\u8033\u5176\u8bed\u7684\u591a\u7ef4\u5ea6NLU\u57fa\u51c6\uff0c\u96be\u4ee5\u5168\u9762\u8bc4\u4f30\u548c\u5206\u6790\u571f\u8033\u5176\u8bed\u6a21\u578b\u7684\u7406\u89e3\u80fd\u529b\u3002", "method": "\u6784\u5efa\u4e86\u57fa\u4e8e\u571f\u8033\u5176\u8bed\u539f\u751f\u8bed\u6599\u7684\u591a\u6837NLU\u4efb\u52a1\u96c6\u5408\uff0c\u91c7\u7528\u534a\u81ea\u52a8\u5316\u6807\u6ce8\u6d41\u7a0b\u7ed3\u5408\u5f3a\u5927LLM\u6ce8\u91ca\u3001\u6a21\u578b\u95f4\u4e00\u81f4\u6027\u68c0\u67e5\u4e0e\u4eba\u5de5\u9a8c\u8bc1\uff0c\u4fdd\u8bc1\u8bed\u8a00\u81ea\u7136\u6027\u5e76\u51cf\u5c11\u7ffb\u8bd1\u504f\u5dee\uff0c\u540c\u65f6\u63d0\u4f9btransformer\u6a21\u578b\u5fae\u8c03\u548c\u8bc4\u4f30\u4ee3\u7801\u3002", "result": "\u5f00\u53d1\u4e86\u5305\u542b\u591a\u4efb\u52a1\u7684TrGLUE\u57fa\u51c6\u548c\u4e13\u95e8\u60c5\u611f\u5206\u6790\u57fa\u51c6SentiTurca\uff0c\u57fa\u4e8e\u4e25\u8c28\u7684\u8bed\u6599\u6536\u96c6\u548c\u6807\u6ce8\u6d41\u7a0b\uff0c\u786e\u4fdd\u6570\u636e\u9ad8\u8d28\u91cf\u548c\u53ef\u590d\u73b0\u3002", "conclusion": "TrGLUE\u4e3a\u571f\u8033\u5176\u8bedNLU\u7814\u7a76\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u7684\u8bc4\u6d4b\u6846\u67b6\u548c\u8d44\u6e90\u652f\u6301\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u76f8\u5173\u6a21\u578b\u7684\u53d1\u5c55\u548c\u9ad8\u8d28\u91cf\u534a\u81ea\u52a8\u6807\u6ce8\u6570\u636e\u96c6\u7684\u751f\u6210\u63a2\u7d22\u3002"}}
