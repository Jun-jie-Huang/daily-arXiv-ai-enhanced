<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 42]
- [cs.OH](#cs.OH) [Total: 1]
- [cs.MA](#cs.MA) [Total: 6]
- [cs.SE](#cs.SE) [Total: 15]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [References Improve LLM Alignment in Non-Verifiable Domains](https://arxiv.org/abs/2602.16802)
*Kejian Shi,Yixin Liu,Peifeng Wang,Alexander R. Fabbri,Shafiq Joty,Arman Cohan*

Main category: cs.CL

TL;DR: 本研究通过参考输出引导LLM评估器，解决了无监督领域缺乏可验证奖励的问题，实现了LLM的有效自我提升，显著优于无参考的对齐方法。


<details>
  <summary>Details</summary>
Motivation: 传统RLVR因缺乏可验证的奖励而难以应用于无监督领域，如LLM对齐，探索利用参考引导的LLM评估器充当软验证器的可能性。

Method: 设计基于参考输出的评估协议，利用前沿模型或高质量人类编写的参考来增强LLM评估器，并实现基于参考指导的自我提升细调。

Result: 参考引导显著提升了弱及强版LLM评估器的准确率，在自我提升实验中分别取得较基线高20.2和17.1点的性能增益，并且达到与强奖励模型ArmoRM相媲美的表现。

Conclusion: 参考引导的LLM评估器能够显著提升LLM的对齐效果，尤其在无监督领域自我提升方面表现优异，接近使用强奖励模型的训练效果。

Abstract: While Reinforcement Learning with Verifiable Rewards (RLVR) has shown strong effectiveness in reasoning tasks, it cannot be directly applied to non-verifiable domains lacking ground-truth verifiers, such as LLM alignment. In this work, we investigate whether reference-guided LLM-evaluators can bridge this gap by serving as soft "verifiers". First, we design evaluation protocols that enhance LLM-based evaluators for LLM alignment using reference outputs. Through comprehensive experiments, we show that a reference-guided approach substantially improves the accuracy of less capable LLM-judges using references from frontier models; stronger LLM-judges can also be enhanced by high-quality (i.e., human-written) references. Building on these improved judges, we demonstrate the utility of high-quality references in alignment tuning, where LLMs guided with references are used as judges to self-improve. We show that reference-guided self-improvement yields clear gains over both direct SFT on reference outputs and self-improvement with reference-free judges, achieving performance comparable to training with ArmoRM, a strong finetuned reward model. Specifically, our method achieves 73.1% and 58.7% on AlpacaEval and Arena-Hard with Llama-3-8B-Instruct, and 70.0% and 74.1% with Qwen2.5-7B, corresponding to average absolute gains of +20.2 / +17.1 points over SFT distillation and +5.3 / +3.6 points over reference-free self-improvement on AlpacaEval / Arena-Hard. These results highlight the potential of using reference-guided LLM-evaluators to enable effective LLM post-training in non-verifiable domains.

</details>


### [2] [Evaluating Monolingual and Multilingual Large Language Models for Greek Question Answering: The DemosQA Benchmark](https://arxiv.org/abs/2602.16811)
*Charalampos Mastrokostas,Nikolaos Giarelis,Nikos Karacapilidis*

Main category: cs.CL

TL;DR: 本文针对希腊语问答任务，提出了新的数据集、评价框架，并对多种单语与多语大型语言模型进行了全面评测，填补了小语种问答研究的空白。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型多聚焦高资源语言，对低资源语言尤其是希腊语的问答效果研究较少，且多语模型存在训练数据偏差，可能导致文化等方面的失真，因此有必要针对希腊语开发专门的数据集和评测框架以改进模型表现。

Method: 开发了新的希腊语社会媒体问答数据集DemosQA，设计了适应不同问答数据和语言的内存高效的大型语言模型评估框架，并采用三种不同的提示策略，对11个单语及多语大型语言模型进行了系统评价。

Result: 发布了含社区审校答案的DemosQA数据集，搭建了支持多数据集多语言的高效评测框架，完成了11个大型语言模型在6个希腊语问答数据集上的详细评估，并公开了代码和数据促进复现。

Conclusion: 本研究构建了更具社会文化代表性的希腊语问答数据集DemosQA，提出了通用且内存高效的评测框架，并比较了11个单语和多语大型语言模型在希腊语问答任务上的表现，推动了低资源语言的问答研究。

Abstract: Recent advancements in Natural Language Processing and Deep Learning have enabled the development of Large Language Models (LLMs), which have significantly advanced the state-of-the-art across a wide range of tasks, including Question Answering (QA). Despite these advancements, research on LLMs has primarily targeted high-resourced languages (e.g., English), and only recently has attention shifted toward multilingual models. However, these models demonstrate a training data bias towards a small number of popular languages or rely on transfer learning from high- to under-resourced languages; this may lead to a misrepresentation of social, cultural, and historical aspects. To address this challenge, monolingual LLMs have been developed for under-resourced languages; however, their effectiveness remains less studied when compared to multilingual counterparts on language-specific tasks. In this study, we address this research gap in Greek QA by contributing: (i) DemosQA, a novel dataset, which is constructed using social media user questions and community-reviewed answers to better capture the Greek social and cultural zeitgeist; (ii) a memory-efficient LLM evaluation framework adaptable to diverse QA datasets and languages; and (iii) an extensive evaluation of 11 monolingual and multilingual LLMs on 6 human-curated Greek QA datasets using 3 different prompting strategies. We release our code and data to facilitate reproducibility.

</details>


### [3] [One-step Language Modeling via Continuous Denoising](https://arxiv.org/abs/2602.16813)
*Chanhyuk Lee,Jaehoon Yoo,Manan Agarwal,Sheel Shah,Jerry Huang,Aditi Raghunathan,Seunghoon Hong,Nicholas M. Boffi,Jinwoo Kim*

Main category: cs.CL

TL;DR: 本文提出基于流的连续去噪语言模型，既提升了生成质量，又实现了多步到单步的快速生成，优于现有离散扩散模型。


<details>
  <summary>Details</summary>
Motivation: 离散扩散语言模型在少步生成时质量急剧下降，限制了其加速生成的潜力。为此，作者探索基于流的连续去噪方法以同时提升速度和质量。

Method: 通过在one-hot向量上使用流式欧几里得去噪，并引入时间重参数化优化训练，最终蒸馏为少步生成的流图模型。

Result: 该论文提出了一种基于流（flow-based）的连续去噪方法来替代离散扩散模型，用于加速语言模型的生成速度，同时保持生成质量。作者设计了一种在one-hot编码空间中进行欧几里得去噪的流式语言模型（FLM），通过交叉熵目标和时间重参数化提升训练稳定性和生成质量。进而将FLM蒸馏为蒸馏流图语言模型（FMLM），实现少步生成。实验证明，FLM在LM1B和OWT数据集上达到与现有离散扩散模型相当的质量，FMLM甚至实现了单步生成超过现有模型多步生成的效果。该工作质疑了离散扩散对离散数据建模的必要性，推动了流式语言模型的加速发展。

Conclusion: 流式基于欧几里得去噪的语言模型在质量和速度上均超越了离散扩散模型，且无需离散扩散过程即可实现高效生成。

Abstract: Language models based on discrete diffusion have attracted widespread interest for their potential to provide faster generation than autoregressive models. In practice, however, they exhibit a sharp degradation of sample quality in the few-step regime, failing to realize this promise. Here we show that language models leveraging flow-based continuous denoising can outperform discrete diffusion in both quality and speed. By revisiting the fundamentals of flows over discrete modalities, we build a flow-based language model (FLM) that performs Euclidean denoising over one-hot token encodings. We show that the model can be trained by predicting the clean data via a cross entropy objective, where we introduce a simple time reparameterization that greatly improves training stability and generation quality. By distilling FLM into its associated flow map, we obtain a distilled flow map language model (FMLM) capable of few-step generation. On the LM1B and OWT language datasets, FLM attains generation quality matching state-of-the-art discrete diffusion models. With FMLM, our approach outperforms recent few-step language models across the board, with one-step generation exceeding their 8-step quality. Our work calls into question the widely held hypothesis that discrete diffusion processes are necessary for generative modeling over discrete modalities, and paves the way toward accelerated flow-based language modeling at scale. Code is available at https://github.com/david3684/flm.

</details>


### [4] [Claim Automation using Large Language Model](https://arxiv.org/abs/2602.16836)
*Zhengda Mo,Zhiyu Quan,Eli O'Donohue,Kaiwen Zhong*

Main category: cs.CL

TL;DR: 通过对大型语言模型进行保险领域的低秩适应微调，构建了一个本地化且具治理性的理赔纠正建议生成模块，有效提升理赔决策的准确性和效率


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在受监管和数据敏感领域（如保险业）的应用受限，利用大量历史保修理赔数据，旨在提升理赔人员的决策效率和准确性

Method: 提出一个本地部署的、具备治理意识的语言建模组件，采用低秩适应（LoRA）对预训练的大型语言模型进行领域特定微调，并将该模型作为理赔流程中的初步决策模块使用

Result: 领域特定微调的模型显著优于通用和基于提示的商业模型，约80%的评估案例与真实纠正措施高度匹配，验证了所提方法的实用性和预测准确度

Conclusion: 领域自适应微调能显著调整模型输出，更贴合实际操作数据，证明该方法在保险应用中具备可靠性和可治理性，是保险行业语言模型应用的重要基础。

Abstract: While Large Language Models (LLMs) have achieved strong performance on general-purpose language tasks, their deployment in regulated and data-sensitive domains, including insurance, remains limited. Leveraging millions of historical warranty claims, we propose a locally deployed governance-aware language modeling component that generates structured corrective-action recommendations from unstructured claim narratives. We fine-tune pretrained LLMs using Low-Rank Adaptation (LoRA), scoping the model to an initial decision module within the claim processing pipeline to speed up claim adjusters' decisions. We assess this module using a multi-dimensional evaluation framework that combines automated semantic similarity metrics with human evaluation, enabling a rigorous examination of both practical utility and predictive accuracy. Our results show that domain-specific fine-tuning substantially outperforms commercial general-purpose and prompt-based LLMs, with approximately 80% of the evaluated cases achieving near-identical matches to ground-truth corrective actions. Overall, this study provides both theoretical and empirical evidence to prove that domain-adaptive fine-tuning can align model output distributions more closely with real-world operational data, demonstrating its promise as a reliable and governable building block for insurance applications.

</details>


### [5] [BanglaSummEval: Reference-Free Factual Consistency Evaluation for Bangla Summarization](https://arxiv.org/abs/2602.16843)
*Ahmed Rafid,Rumman Adib,Fariya Ahmed,Ajwad Abrar,Mohammed Saidul Islam*

Main category: cs.CL

TL;DR: 本文开发了BanglaSummEval，一种无需参考摘要即可评测班加拉语文本事实一致性的问答框架，以解决资源匮乏语言中评测难题，且评测结果与专家意见高度一致。


<details>
  <summary>Details</summary>
Motivation: 现有的文本摘要事实一致性评测大多忽视了班加拉语这一资源匮乏的广泛使用语言，且多依赖参考摘要，难以在高风险领域如医疗和新闻中确保摘要可靠性。

Method: 采用单一多语言指令调优语言模型，完成问问题生成、回答、答案提取及问题权重分配；通过BERTScore-Recall进行答案语义匹配，评估事实准确性和内容覆盖度。

Result: 提出了BanglaSummEval，一个无参考、基于问答的班加拉语事实一致性评测框架；该方法通过自动生成的问题和答案评估事实准确性和内容覆盖率，并使用多语言指令调优模型简化系统复杂度和计算成本；在教育和医疗领域300篇人工摘要上验证出与专家判断高度相关（Pearson’s r=0.694，Spearman’s ρ=0.763）。

Conclusion: BanglaSummEval为低资源语言的事实一致性评测提供了一个实用、透明且解释性强的解决方案，能够有效提高班加拉语文本摘要的评测可靠性。

Abstract: Evaluating factual consistency is essential for reliable text summarization, particularly in high-stakes domains such as healthcare and news. However, most existing evaluation metrics overlook Bangla, a widely spoken yet under-resourced language, and often depend on reference summaries. We introduce BanglaSummEval, a reference-free, question-answering-based framework for evaluating factual consistency in Bangla summarization. The proposed method assesses both factual accuracy and content coverage through automatically generated questions and answers derived from the source document and the summary. A single multilingual instruction-tuned language model handles question generation, question answering, candidate answer extraction, and question importance weighting. This unified design reduces system complexity and computational cost. To capture semantic consistency beyond surface-level overlap, we use BERTScore-Recall for answer comparison. We validate BanglaSummEval on 300 human-written summaries from educational and medical domains, demonstrating strong correlation with expert human judgments (Pearson's $r = 0.694$, Spearman's $ρ= 0.763$). By providing interpretable, step-wise diagnostics alongside reliable evaluation scores, BanglaSummEval offers a practical and transparent solution for factual consistency evaluation in low-resource language settings.

</details>


### [6] [Meenz bleibt Meenz, but Large Language Models Do Not Speak Its Dialect](https://arxiv.org/abs/2602.16852)
*Minh Duc Bui,Manuel Mager,Peter Herbert Kann,Katharina von der Wense*

Main category: cs.CL

TL;DR: 首次针对德国美因茨方言Meenzerisch的NLP研究构建数字字典，发现大型语言模型对该方言的词义生成和词汇生成能力极低，强调需要更多资源和研究。


<details>
  <summary>Details</summary>
Motivation: Meenzerisch方言作为德国美因茨的传统语言，正面临消亡风险，类似许多德国方言。自然语言处理（NLP）有潜力帮助保护和复兴这些语言，但迄今无人研究Meenzerisch。

Method: 构建了一个包含2,351个Meenzerisch词汇及其标准德语释义的数字字典数据集，并基于该数据集测试了当前大型语言模型（LLMs）在为方言词汇生成释义和根据释义生成词汇两方面的能力，进一步尝试少样本学习及规则引导的提升方法。

Result: LLMs在生成方言词定义和词汇方面准确率非常低，最高分别为6.27%和1.51%；少样本学习和规则引导虽有所提升，但准确率仍低于10%。

Conclusion: 现有大型语言模型难以有效处理Meenzerisch方言，亟需更多资源和研究投入以推动德国方言的自然语言处理发展。

Abstract: Meenzerisch, the dialect spoken in the German city of Mainz, is also the traditional language of the Mainz carnival, a yearly celebration well known throughout Germany. However, Meenzerisch is on the verge of dying out-a fate it shares with many other German dialects. Natural language processing (NLP) has the potential to help with the preservation and revival efforts of languages and dialects. However, so far no NLP research has looked at Meenzerisch. This work presents the first research in the field of NLP that is explicitly focused on the dialect of Mainz. We introduce a digital dictionary-an NLP-ready dataset derived from an existing resource (Schramm, 1966)-to support researchers in modeling and benchmarking the language. It contains 2,351 words in the dialect paired with their meanings described in Standard German. We then use this dataset to answer the following research questions: (1) Can state-of-the-art large language models (LLMs) generate definitions for dialect words? (2) Can LLMs generate words in Meenzerisch, given their definitions? Our experiments show that LLMs can do neither: the best model for definitions reaches only 6.27% accuracy and the best word generation model's accuracy is 1.51%. We then conduct two additional experiments in order to see if accuracy is improved by few-shot learning and by extracting rules from the training set, which are then passed to the LLM. While those approaches are able to improve the results, accuracy remains below 10%. This highlights that additional resources and an intensification of research efforts focused on German dialects are desperately needed.

</details>


### [7] [ConvApparel: A Benchmark Dataset and Validation Framework for User Simulators in Conversational Recommenders](https://arxiv.org/abs/2602.16938)
*Ofer Meshi,Krisztian Balog,Sally Goldman,Avi Caciularu,Guy Tennenholtz,Jihwan Jeong,Amir Globerson,Craig Boutilier*

Main category: cs.CL

TL;DR: 本文提出了ConvApparel数据集和综合验证框架，有效揭示并缓解了基于大型语言模型的用户模拟器中的真实感差距，提升了对用户行为的模拟真实度。


<details>
  <summary>Details</summary>
Motivation: 当前基于大型语言模型的用户模拟器存在“真实感差距”，导致其优化的系统在现实世界表现不佳。

Method: 引入ConvApparel数据集，采用“好”与“坏”推荐者的双代理数据收集协议，并提出结合统计对齐、人类相似度评分和反事实验证的综合验证框架。

Result: 实验显示所有模拟器均存在显著真实感差距，但数据驱动的模拟器在反事实验证中表现优于提示基础模型，能够更真实地适应未见行为。

Conclusion: 通过ConvApparel数据集和综合验证框架，数据驱动的用户模拟器表现出更强的鲁棒性和现实适应性，有助于缩小真实感差距。

Abstract: The promise of LLM-based user simulators to improve conversational AI is hindered by a critical "realism gap," leading to systems that are optimized for simulated interactions, but may fail to perform well in the real world. We introduce ConvApparel, a new dataset of human-AI conversations designed to address this gap. Its unique dual-agent data collection protocol -- using both "good" and "bad" recommenders -- enables counterfactual validation by capturing a wide spectrum of user experiences, enriched with first-person annotations of user satisfaction. We propose a comprehensive validation framework that combines statistical alignment, a human-likeness score, and counterfactual validation to test for generalization. Our experiments reveal a significant realism gap across all simulators. However, the framework also shows that data-driven simulators outperform a prompted baseline, particularly in counterfactual validation where they adapt more realistically to unseen behaviors, suggesting they embody more robust, if imperfect, user models.

</details>


### [8] [When Semantic Overlap Is Not Enough: Cross-Lingual Euphemism Transfer Between Turkish and English](https://arxiv.org/abs/2602.16957)
*Hasan Can Biyik,Libby Barak,Jing Peng,Anna Feldman*

Main category: cs.CL

TL;DR: 研究了土耳其语与英语委婉语的跨语言迁移，发现语义重叠并不总带来迁移效果提升，迁移效果受标签分布和领域对齐影响。


<details>
  <summary>Details</summary>
Motivation: 委婉语依赖文化和语用上下文，跨语言建模复杂，探究跨语言等价性对迁移学习的影响有助于提升多语言委婉语检测的效果。

Method: 对土耳其语和英语中的潜在委婉语（PETs）进行功能、语用和语义上的重叠与非重叠分类，研究跨语种迁移在多语言委婉语检测中的影响。

Result: 发现迁移存在非对称性，语义重叠不保证积极迁移，尤其在资源较少的土耳其语到英语方向表现可能下降，且部分非重叠训练反而提升效果，标签分布差异部分解释这一现象。

Conclusion: 跨语言迁移效果受语义与语用对齐影响，但语义重叠不足以保障积极迁移，领域特异性对齐可能促进迁移，需考虑资源与标签差异。

Abstract: Euphemisms substitute socially sensitive expressions, often softening or reframing meaning, and their reliance on cultural and pragmatic context complicates modeling across languages. In this study, we investigate how cross-lingual equivalence influences transfer in multilingual euphemism detection. We categorize Potentially Euphemistic Terms (PETs) in Turkish and English into Overlapping (OPETs) and Non-Overlapping (NOPETs) subsets based on their functional, pragmatic, and semantic alignment. Our findings reveal a transfer asymmetry: semantic overlap is insufficient to guarantee positive transfer, particularly in low-resource Turkish-to-English direction, where performance can degrade even for overlapping euphemisms, and in some cases, improve under NOPET-based training. Differences in label distribution help explain these counterintuitive results. Category-level analysis suggests that transfer may be influenced by domain-specific alignment, though evidence is limited by sparsity.

</details>


### [9] [Eigenmood Space: Uncertainty-Aware Spectral Graph Analysis of Psychological Patterns in Classical Persian Poetry](https://arxiv.org/abs/2602.16959)
*Kourosh Shahnazari,Seyed Moein Ayyoubzadeh,Mohammadali Keshtparvar*

Main category: cs.CL

TL;DR: 该研究提出一个包含不确定性处理的规模化波斯诗歌心理分析框架，通过多标签信心加权方法和谱嵌入技术，支持可审计且谨慎的数字人文分析。


<details>
  <summary>Details</summary>
Motivation: 波斯古典诗歌通过隐喻、互文惯例和修辞间接性表达情感，需细致解读但难以大规模复现比较，有必要建立不确定认知框架支持规模化分析同时保持解释谨慎。

Method: 提出了一种基于大规模自动多标签注释的不确定性感知的诗人心理分析计算框架；每句诗关联心理概念和置信度评分，并设置弃权标志以表明证据不足；通过置信度加权汇聚为诗人与概念的矩阵，利用Jensen-Shannon和Kullback-Leibler散度衡量诗歌个性；构建置信度加权的共现图，并通过拉普拉斯谱分解定义Eigenmood嵌入。

Result: 在包含10位诗人共61573句诗的语料上，22.2%句诗被弃权，体现了不确定性的必要性；完成置信度阈值敏感性分析、选择偏差诊断及结合Eigenmood实现的从远到近的数据检索。

Conclusion: 本框架有效融合不确定性信息，实现从诗句到诗人的心理状态推断，支持可靠且可扩展的数字人文研究，兼顾解释细致性与大规模比较的需求。

Abstract: Classical Persian poetry is a historically sustained archive in which affective life is expressed through metaphor, intertextual convention, and rhetorical indirection. These properties make close reading indispensable while limiting reproducible comparison at scale. We present an uncertainty-aware computational framework for poet-level psychological analysis based on large-scale automatic multi-label annotation. Each verse is associated with a set of psychological concepts, per-label confidence scores, and an abstention flag that signals insufficient evidence. We aggregate confidence-weighted evidence into a Poet $\times$ Concept matrix, interpret each poet as a probability distribution over concepts, and quantify poetic individuality as divergence from a corpus baseline using Jensen--Shannon divergence and Kullback--Leibler divergence. To capture relational structure beyond marginals, we build a confidence-weighted co-occurrence graph over concepts and define an Eigenmood embedding through Laplacian spectral decomposition. On a corpus of 61{,}573 verses across 10 poets, 22.2\% of verses are abstained, underscoring the analytical importance of uncertainty. We further report sensitivity analysis under confidence thresholding, selection-bias diagnostics that treat abstention as a category, and a distant-to-close workflow that retrieves verse-level exemplars along Eigenmood axes. The resulting framework supports scalable, auditable digital-humanities analysis while preserving interpretive caution by propagating uncertainty from verse-level evidence to poet-level inference.

</details>


### [10] [Persona2Web: Benchmarking Personalized Web Agents for Contextual Reasoning with User History](https://arxiv.org/abs/2602.17003)
*Serin Kim,Sangam Lee,Dongha Lee*

Main category: cs.CL

TL;DR: 提出首个面向真实开放网页的个性化Web代理评测基准Persona2Web，强调通过用户历史消除查询歧义，实现细粒度个性化评估。


<details>
  <summary>Details</summary>
Motivation: 现有Web代理缺乏个性化能力，用户意图表达往往含糊，需通过推断用户偏好和上下文理解查询。

Method: 提出Persona2Web基准，用于在真实开放网页上评估个性化Web代理，基于clarify-to-personalize原则，代理需依赖用户历史推断偏好以消除查询歧义。

Result: 构建包含用户历史数据、歧义查询及细粒度个性化评估框架的Persona2Web基准，并在多种代理结构和模型上进行了广泛实验，揭示个性化Web代理面临的关键挑战。

Conclusion: Persona2Web基准有效推动个性化Web代理研究，揭示其在理解歧义查询及利用用户历史方面的难点，为后续研究提供了可靠平台。

Abstract: Large language models have advanced web agents, yet current agents lack personalization capabilities. Since users rarely specify every detail of their intent, practical web agents must be able to interpret ambiguous queries by inferring user preferences and contexts. To address this challenge, we present Persona2Web, the first benchmark for evaluating personalized web agents on the real open web, built upon the clarify-to-personalize principle, which requires agents to resolve ambiguity based on user history rather than relying on explicit instructions. Persona2Web consists of: (1) user histories that reveal preferences implicitly over long time spans, (2) ambiguous queries that require agents to infer implicit user preferences, and (3) a reasoning-aware evaluation framework that enables fine-grained assessment of personalization. We conduct extensive experiments across various agent architectures, backbone models, history access schemes, and queries with varying ambiguity levels, revealing key challenges in personalized web agent behavior. For reproducibility, our codes and datasets are publicly available at https://anonymous.4open.science/r/Persona2Web-73E8.

</details>


### [11] [ReIn: Conversational Error Recovery with Reasoning Inception](https://arxiv.org/abs/2602.17022)
*Takyoung Kim,Jinseok Nam,Chandrayee Basu,Xing Fan,Chengyuan Ma,Heng Ji,Gokhan Tur,Dilek Hakkani-Tür*

Main category: cs.CL

TL;DR: 本文提出了Reasoning Inception方法，通过外部干预实现对话错误恢复，无需模型改动，显著提升对话任务成功率和适应性。


<details>
  <summary>Details</summary>
Motivation: 当前基于大型语言模型的对话代理在固定任务上表现优异，但对用户引发的意外错误恢复能力不足，且模型微调和提示修改成本高昂，因此探索无需改动模型参数的错误恢复方法。

Method: 提出了一种测试时干预方法Reasoning Inception（ReIn），通过外部介入模块识别对话中的预定义错误并生成恢复方案，将其融入代理的内部推理过程中，引导纠正行为，无需修改模型参数或提示。

Result: 在模拟的对话失败场景中，ReIn显著提升了任务成功率，能有效应对用户的模糊和不支持请求，并优于显式提示修改方法，在不同模型和模块组合下均表现稳定且泛化能力强。

Conclusion: 本文提出的Reasoning Inception (ReIn)方法有效提升了对话代理在面对错误上下文时的恢复能力，且无需修改模型参数或提示，表现出优越的任务成功率和泛化能力。

Abstract: Conversational agents powered by large language models (LLMs) with tool integration achieve strong performance on fixed task-oriented dialogue datasets but remain vulnerable to unanticipated, user-induced errors. Rather than focusing on error prevention, this work focuses on error recovery, which necessitates the accurate diagnosis of erroneous dialogue contexts and execution of proper recovery plans. Under realistic constraints precluding model fine-tuning or prompt modification due to significant cost and time requirements, we explore whether agents can recover from contextually flawed interactions and how their behavior can be adapted without altering model parameters and prompts. To this end, we propose Reasoning Inception (ReIn), a test-time intervention method that plants an initial reasoning into the agent's decision-making process. Specifically, an external inception module identifies predefined errors within the dialogue context and generates recovery plans, which are subsequently integrated into the agent's internal reasoning process to guide corrective actions, without modifying its parameters or system prompts. We evaluate ReIn by systematically simulating conversational failure scenarios that directly hinder successful completion of user goals: user's ambiguous and unsupported requests. Across diverse combinations of agent models and inception modules, ReIn substantially improves task success and generalizes to unseen error types. Moreover, it consistently outperforms explicit prompt-modification approaches, underscoring its utility as an efficient, on-the-fly method. In-depth analysis of its operational mechanism, particularly in relation to instruction hierarchy, indicates that jointly defining recovery tools with ReIn can serve as a safe and effective strategy for improving the resilience of conversational agents without modifying the backbone models or system prompts.

</details>


### [12] [Large Language Models Persuade Without Planning Theory of Mind](https://arxiv.org/abs/2602.17045)
*Jared Moore,Rasmus Overmark,Ned Cooper,Beba Cibralic,Nick Haber,Cameron R. Jones*

Main category: cs.CL

TL;DR: 本文通过一个交互式说服任务评估人类和大型语言模型的心智理论能力，发现大型语言模型虽难以进行复杂心智状态推理，但擅长通过修辞策略影响人类信念，提示对其心智理论能力的评估应谨慎。


<details>
  <summary>Details</summary>
Motivation: 现有研究多用静态非交互问答方式评估心智理论能力，忽视了第一人称交互的重要性，可能无法真实反映心智理论能力。本文试图填补这一空白，通过实际交互说服任务更真实地评测心智理论。

Method: 设计了一种新的心智理论任务：说服者通过策略性揭示信息，影响目标选择三项政策中的一项。通过操控目标知识状态和动机状态的显隐，评估说服者的心智理论能力。进行了三个实验，分别用机器人和人类作为目标对象，测量说服效果。

Result: 实验1中，LLM在显性条件表现优异但隐性条件表现较差，人类在两种条件均表现中等偏上。实验2和3中，LLM在对真人目标的说服中表现超过人类，且能够影响真实信念。

Conclusion: 大型语言模型在显式心智理论推理方面存在困难，但擅长通过修辞策略进行有效说服。实验结果表明人类能较好地进行多步心智状态推理，而大型语言模型在隐性心智状态条件下表现较差，但在实际说服人类时表现优于人类说服者。

Abstract: A growing body of work attempts to evaluate the theory of mind (ToM) abilities of humans and large language models (LLMs) using static, non-interactive question-and-answer benchmarks. However, theoretical work in the field suggests that first-personal interaction is a crucial part of ToM and that such predictive, spectatorial tasks may fail to evaluate it. We address this gap with a novel ToM task that requires an agent to persuade a target to choose one of three policy proposals by strategically revealing information. Success depends on a persuader's sensitivity to a given target's knowledge states (what the target knows about the policies) and motivational states (how much the target values different outcomes). We varied whether these states were Revealed to persuaders or Hidden, in which case persuaders had to inquire about or infer them. In Experiment 1, participants persuaded a bot programmed to make only rational inferences. LLMs excelled in the Revealed condition but performed below chance in the Hidden condition, suggesting difficulty with the multi-step planning required to elicit and use mental state information. Humans performed moderately well in both conditions, indicating an ability to engage such planning. In Experiment 2, where a human target role-played the bot, and in Experiment 3, where we measured whether human targets' real beliefs changed, LLMs outperformed human persuaders across all conditions. These results suggest that effective persuasion can occur without explicit ToM reasoning (e.g., through rhetorical strategies) and that LLMs excel at this form of persuasion. Overall, our results caution against attributing human-like ToM to LLMs while highlighting LLMs' potential to influence people's beliefs and behavior.

</details>


### [13] [Evaluating Cross-Lingual Classification Approaches Enabling Topic Discovery for Multilingual Social Media Data](https://arxiv.org/abs/2602.17051)
*Deepak Uniyal,Md Abul Bashar,Richi Nayak*

Main category: cs.CL

TL;DR: 本文研究了跨语言文本分类在多语言社交媒体话语分析中的应用，针对氢能相关推文数据，通过四种方法筛选相关内容，最终通过主题建模发现主要讨论主题，并对方法优劣进行了对比分析。


<details>
  <summary>Details</summary>
Motivation: 多语言大规模公共讨论的社交媒体分析仍具挑战性，尤其是如何在多语种背景下准确分类相关文本以支撑全球语境下的对话分析。

Method: 数据集为2013-2022年间包含英语、日语、印地语和韩语的九百万推文，筛选相关内容的方法包括：1)将英文标注数据翻译至目标语言构建语言特定模型；2)将所有语言未标注数据翻译成英文建立单一模型；3)直接应用微调后的多语言变换器模型；4)结合翻译标注与多语言训练的混合策略。结合评估过滤效果后进行主题建模。

Result: 各方法均能够过滤出氢能相关推文但存在权衡，翻译和多语言模型各有优劣，混合策略表现更优，提供了如何优化跨语言社交媒体分析流程的实用见解。

Conclusion: 研究揭示了翻译方法与多语言模型在跨语言文本分类中的权衡，为大规模社交媒体多语言分析提供了优化策略。

Abstract: Analysing multilingual social media discourse remains a major challenge in natural language processing, particularly when large-scale public debates span across diverse languages. This study investigates how different approaches for cross-lingual text classification can support reliable analysis of global conversations. Using hydrogen energy as a case study, we analyse a decade-long dataset of over nine million tweets in English, Japanese, Hindi, and Korean (2013--2022) for topic discovery. The online keyword-driven data collection results in a significant amount of irrelevant content. We explore four approaches to filter relevant content: (1) translating English annotated data into target languages for building language-specific models for each target language, (2) translating unlabelled data appearing from all languages into English for creating a single model based on English annotations, (3) applying English fine-tuned multilingual transformers directly to each target language data, and (4) a hybrid strategy that combines translated annotations with multilingual training. Each approach is evaluated for its ability to filter hydrogen-related tweets from noisy keyword-based collections. Subsequently, topic modeling is performed to extract dominant themes within the relevant subsets. The results highlight key trade-offs between translation and multilingual approaches, offering actionable insights into optimising cross-lingual pipelines for large-scale social media analysis.

</details>


### [14] [ALPS: A Diagnostic Challenge Set for Arabic Linguistic & Pragmatic Reasoning](https://arxiv.org/abs/2602.17054)
*Hussein S. Al-Olimat,Ahmad Alshareef*

Main category: cs.CL

TL;DR: ALPS是一个由阿拉伯语言学专家构建的原生诊断数据集，用于测试模型深层语义和语用理解，结果表明尽管顶级模型流利度高，但在形态句法理解上仍有显著不足，且本土模型尚未达到人类水平。


<details>
  <summary>Details</summary>
Motivation: 现有阿拉伯语NLP基准多依赖合成或翻译数据，缺乏深入语言学验证，需一个具有文化真实性且避免翻译伪影的原生诊断数据集，检验模型的深层语言理解能力。

Method: 构建了由阿拉伯语言学专家精心设计的ALPS诊断挑战集，包含531个问题，覆盖15种任务和47个子任务，专注于深层语义与语用理解，通过评估23个不同模型的表现进行分析。

Result: 实验显示模型在对形态-句法依赖的任务错误率高达36.5%，尽管顶级商业模型性能超过平均人类，阿拉伯本土模型仍存在明显差距，专家判定的理想表现（99.2%）远超模型成绩。

Conclusion: 当前主流阿拉伯语自然语言处理模型在流利度方面表现优异，但在基本的形态-句法依赖关系理解上存在显著缺陷，且阿拉伯语本土模型性能尚不及顶级商业模型和人类水平。

Abstract: While recent Arabic NLP benchmarks focus on scale, they often rely on synthetic or translated data which may benefit from deeper linguistic verification. We introduce ALPS (Arabic Linguistic & Pragmatic Suite), a native, expert-curated diagnostic challenge set probing Deep Semantics and Pragmatics, capabilities that complement specialized large-scale benchmarks. While broad-coverage benchmarks prioritize scale and multi-task coverage, ALPS targets the depth of linguistic understanding through 531 rigorously crafted questions across 15 tasks and 47 subtasks. We developed the dataset with deep expertise in Arabic linguistics, guaranteeing cultural authenticity and eliminating translation artifacts. Evaluating 23 diverse models (commercial, open-source, and Arabic-native) against a single-pass human performance (avg. 84.6% accuracy) and an expert-adjudicated oracle (99.2%), we reveal a critical dissociation: models achieve high fluency but fail on fundamental morpho-syntactic dependencies, with elevated error rates on morpho-syntactic dependencies (36.5% across diacritics-reliant tasks) compared to compositional semantics. While top commercial models (Gemini-3-flash at 94.2%) surpass the average single human, a substantial gap persists between commercial giants and Arabic-native models, with the best Arabic-specific model (Jais-2-70B at 83.6%) approaching but not matching human performance.

</details>


### [15] [BankMathBench: A Benchmark for Numerical Reasoning in Banking Scenarios](https://arxiv.org/abs/2602.17072)
*Yunseung Lee,Subin Kim,Youngjun Kwak,Jaegul Choo*

Main category: cs.CL

TL;DR: 针对金融领域数字推理难题，提出BankMathBench数据集，显著提升LLMs在银行计算任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 目前LLMs在核心银行计算任务中存在系统性误差，现有数学和金融基准无法有效覆盖日常银行场景中的多步数值推理，亟需一个专门针对真实银行任务的领域数据集以提升模型准确率。

Method: 设计了涵盖基础、中级、高级三个难度级别的任务，分别对应单产品推理、多产品比较和多条件推理，结合数据集对开源LLMs进行训练和工具辅助微调，评估了模型在公式生成和数值推理上的表现。

Result: 该论文提出了BankMathBench，一个针对金融领域尤其是数字银行常见计算的多层次基准数据集，旨在提升大型语言模型（LLMs）在核心银行计算上的准确性。通过该数据集训练的模型在公式生成和数值推理上均表现出显著提升，准确率较零样本基线大幅度提高，表明数据集对增强领域特定推理能力的有效性。

Conclusion: BankMathBench有效填补了现有基准在金融日常银行情景中的空白，通过分层次任务设计显著提升了LLMs的数值推理能力，成为评估和推动LLMs在银行领域应用的重要工具。

Abstract: Large language models (LLMs)-based chatbots are increasingly being adopted in the financial domain, particularly in digital banking, to handle customer inquiries about products such as deposits, savings, and loans. However, these models still exhibit low accuracy in core banking computations-including total payout estimation, comparison of products with varying interest rates, and interest calculation under early repayment conditions. Such tasks require multi-step numerical reasoning and contextual understanding of banking products, yet existing LLMs often make systematic errors-misinterpreting product types, applying conditions incorrectly, or failing basic calculations involving exponents and geometric progressions. However, such errors have rarely been captured by existing benchmarks. Mathematical datasets focus on fundamental math problems, whereas financial benchmarks primarily target financial documents, leaving everyday banking scenarios underexplored. To address this limitation, we propose BankMathBench, a domain-specific dataset that reflects realistic banking tasks. BankMathBench is organized in three levels of difficulty-basic, intermediate, and advanced-corresponding to single-product reasoning, multi-product comparison, and multi-condition scenarios, respectively. When trained on BankMathBench, open-source LLMs exhibited notable improvements in both formula generation and numerical reasoning accuracy, demonstrating the dataset's effectiveness in enhancing domain-specific reasoning. With tool-augmented fine-tuning, the models achieved average accuracy increases of 57.6%p (basic), 75.1%p (intermediate), and 62.9%p (advanced), representing significant gains over zero-shot baselines. These findings highlight BankMathBench as a reliable benchmark for evaluating and advancing LLMs' numerical reasoning in real-world banking scenarios.

</details>


### [16] [Projective Psychological Assessment of Large Multimodal Models Using Thematic Apperception Tests](https://arxiv.org/abs/2602.17108)
*Anton Dzega,Aviad Elyashar,Ortal Slobodin,Odeya Cohen,Rami Puzis*

Main category: cs.CL

TL;DR: 研究利用TAT和SCORS-G评估大型多模态模型的个性特质，发现模型擅长理解人际关系和自我，但难以感知和调节攻击性，且更大更新的模型表现更好。


<details>
  <summary>Details</summary>
Motivation: 旨在探究大型多模态模型是否能通过非语言模态准确评估个性特质，尤其是区分认知与情感成分。

Method: 使用TAT框架引导模型讲故事，随后用SCORS-G标准由模型自身和人类专家对故事进行评分，比较解析结果。

Result: 该论文通过主题统觉测验（TAT）框架，结合社会认知和客体关系量表-全球版（SCORS-G），评估了大型多模态模型（LMMs）的个性特质。研究中，模型既作为故事生成主体，也作为故事评估者。结果显示，模型在理解人际关系和自我概念方面表现优秀，且评估与人类专家的结果高度一致，但对攻击性的感知和调节能力较弱。较大和较新模型在所有维度上的表现均优于较小和较旧版本。

Conclusion: 大型多模态模型能够较好地理解和分析人际动态及自我概念，其评估结果与人类专家高度一致，但普遍缺乏对攻击性的感知和调节能力。模型大小和新旧程度显著影响其性能。

Abstract: Thematic Apperception Test (TAT) is a psychometrically grounded, multidimensional assessment framework that systematically differentiates between cognitive-representational and affective-relational components of personality-like functioning. This test is a projective psychological framework designed to uncover unconscious aspects of personality. This study examines whether the personality traits of Large Multimodal Models (LMMs) can be assessed through non-language-based modalities, using the Social Cognition and Object Relations Scale - Global (SCORS-G). LMMs are employed in two distinct roles: as subject models (SMs), which generate stories in response to TAT images, and as evaluator models (EMs), who assess these narratives using the SCORS-G framework. Evaluators demonstrated an excellent ability to understand and analyze TAT responses. Their interpretations are highly consistent with those of human experts. Assessment results highlight that all models understand interpersonal dynamics very well and have a good grasp of the concept of self. However, they consistently fail to perceive and regulate aggression. Performance varied systematically across model families, with larger and more recent models consistently outperforming smaller and earlier ones across SCORS-G dimensions.

</details>


### [17] [The Emergence of Lab-Driven Alignment Signatures: A Psychometric Framework for Auditing Latent Bias and Compounding Risk in Generative AI](https://arxiv.org/abs/2602.17127)
*Dusan Bosnjakovic*

Main category: cs.CL

TL;DR: 本文提出了一种基于心理测量学的新型审计框架，通过强制选择的序数情境测试及混合线性模型分析，检测大型语言模型在多智能体系统中的持续行为偏差。实验表明，模型的潜在偏向非静态错误，而是可能导致递归意识形态回声室的复合变量。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型从独立聊天接口转向多智能体系统的基础推理层，亟需检测其持久且提供商级别的行为特征，保障安全与治理，而传统基准测试无法捕捉这一特征。

Method: 利用心理测量理论中的序数不确定性下的潜变量估计，设计强制选择的序数情境测试，并结合混合线性模型和组内相关系数分析，定量评估模型的行为倾向。

Result: 审计九个领先模型，发现高方差主要由测试条目框架引起，但显著的“实验室信号”导致行为聚类，揭示潜在偏见的复杂影响。

Conclusion: 大型语言模型在多层级AI架构中展现出稳定的行为偏见，这些偏见不仅是静态误差，更是引发递归意识形态回声室的风险因素。

Abstract: As Large Language Models (LLMs) transition from standalone chat interfaces to foundational reasoning layers in multi-agent systems and recursive evaluation loops (LLM-as-a-judge), the detection of durable, provider-level behavioral signatures becomes a critical requirement for safety and governance. Traditional benchmarks measure transient task accuracy but fail to capture stable, latent response policies -- the ``prevailing mindsets'' embedded during training and alignment that outlive individual model versions.
  This paper introduces a novel auditing framework that utilizes psychometric measurement theory -- specifically latent trait estimation under ordinal uncertainty -- to quantify these tendencies without relying on ground-truth labels. Utilizing forced-choice ordinal vignettes masked by semantically orthogonal decoys and governed by cryptographic permutation-invariance, the research audits nine leading models across dimensions including Optimization Bias, Sycophancy, and Status-Quo Legitimization.
  Using Mixed Linear Models (MixedLM) and Intraclass Correlation Coefficient (ICC) analysis, the research identifies that while item-level framing drives high variance, a persistent ``lab signal'' accounts for significant behavioral clustering. These findings demonstrate that in ``locked-in'' provider ecosystems, latent biases are not merely static errors but compounding variables that risk creating recursive ideological echo chambers in multi-layered AI architectures.

</details>


### [18] [What Makes a Good Doctor Response? An Analysis on a Romanian Telemedicine Platform](https://arxiv.org/abs/2602.17194)
*Adrian Cosma,Cosmin Dumitrache,Emilian Radoi*

Main category: cs.CL

TL;DR: 研究通过分析大规模罗马尼亚语远程医疗问答数据，发现患者满意度主要受历史行为影响，礼貌和委婉表达有助提升满意度，词汇多样性反而负面影响。


<details>
  <summary>Details</summary>
Motivation: 随着基于文本的远程医疗普及，医生在书面沟通中需有效传达医疗建议；患者反馈多反映沟通质量而非临床准确性，本研究旨在分析患者满意度的语言和行为驱动因素。

Method: 使用77334对罗马尼亚语的匿名医患文本对，提取语言无关特征、心理语言学特征及礼貌/委婉标记，应用时间序列分割训练分类器，并利用SHAP分析特征重要性。

Result: 患者和医生的历史行为特征是预测患者反馈的主要指标，文本特征虽为次要但可操作；礼貌和委婉用语与正面反馈相关，词汇多样性与负面反馈相关。

Conclusion: 医生与患者的互动中，语言风格如礼貌和委婉表达对患者满意度有积极影响，而词汇多样性则可能负向影响满意度；患者及医生的历史行为是预测满意度的主要因素。

Abstract: Text-based telemedicine has become a common mode of care, requiring clinicians to deliver medical advice clearly and effectively in writing. As platforms increasingly rely on patient ratings and feedback, clinicians face growing pressure to maintain satisfaction scores, even though these evaluations often reflect communication quality more than clinical accuracy. We analyse patient satisfaction signals in Romanian text-based telemedicine. Using a sample of 77,334 anonymised patient question--doctor response pairs, we model feedback as a binary outcome, treating thumbs-up responses as positive and grouping negative or absent feedback into the other class. We extract interpretable, predominantly language-agnostic features (e.g., length, structural characteristics, readability proxies), along with Romanian LIWC psycholinguistic features and politeness/hedging markers where available. We train a classifier with a time-based split and perform SHAP-based analyses, which indicate that patient and clinician history features dominate prediction, functioning as strong priors, while characteristics of the response text provide a smaller but, crucially, actionable signal. In subgroup correlation analyses, politeness and hedging are consistently positively associated with patient feedback, whereas lexical diversity shows a negative association.

</details>


### [19] [Quantifying and Mitigating Socially Desirable Responding in LLMs: A Desirability-Matched Graded Forced-Choice Psychometric Study](https://arxiv.org/abs/2602.17262)
*Kensuke Okada,Yui Furukawa,Kyosuke Bunji*

Main category: cs.CL

TL;DR: 该论文提出并验证了一个心理测量框架，以量化和缓解LLM问卷评估中的社交期望反应偏差，通过分级强制选择问卷设计显著提高测评的真实性和准确性。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型（LLMs）的问卷自评在真实性假设下存在偏差，尤其是在评估中的社交期望性反应（SDR），影响测评准确性，迫切需要量化和缓解此类偏差。

Method: 提出了一个基于心理测量学的框架，利用HONEST和FAKE-GOOD指令下的同一问卷，通过项目反应理论（IRT）估计潜变量评分，计算方向校正的标准化效应量量化SDR；并使用优化方法选取匹配期望性的题目对构建GFC问卷以减缓SDR。

Result: 九个指令调优的LLM在合成设定的人格问卷测试中，传统李克特量表问卷显示出显著SDR，而期望性匹配的GFC问卷显著降低了SDR，同时较好地恢复了预定的人格特征。

Conclusion: 在基于问卷的LLM评估中存在社交期望反应偏差（SDR），通过构建分级强制选择 (GFC)问卷，可以有效减少SDR并保持对目标人格特征的恢复能力。

Abstract: Human self-report questionnaires are increasingly used in NLP to benchmark and audit large language models (LLMs), from persona consistency to safety and bias assessments. Yet these instruments presume honest responding; in evaluative contexts, LLMs can instead gravitate toward socially preferred answers-a form of socially desirable responding (SDR)-biasing questionnaire-derived scores and downstream conclusions. We propose a psychometric framework to quantify and mitigate SDR in questionnaire-based evaluation of LLMs. To quantify SDR, the same inventory is administered under HONEST versus FAKE-GOOD instructions, and SDR is computed as a direction-corrected standardized effect size from item response theory (IRT)-estimated latent scores. This enables comparisons across constructs and response formats, as well as against human instructed-faking benchmarks. For mitigation, we construct a graded forced-choice (GFC) Big Five inventory by selecting 30 cross-domain pairs from an item pool via constrained optimization to match desirability. Across nine instruction-tuned LLMs evaluated on synthetic personas with known target profiles, Likert-style questionnaires show consistently large SDR, whereas desirability-matched GFC substantially attenuates SDR while largely preserving the recovery of the intended persona profiles. These results highlight a model-dependent SDR-recovery trade-off and motivate SDR-aware reporting practices for questionnaire-based benchmarking and auditing of LLMs.

</details>


### [20] [Towards Cross-lingual Values Assessment: A Consensus-Pluralism Perspective](https://arxiv.org/abs/2602.17283)
*Yukun Chen,Xinyu Zhang,Jialong Tang,Yu Wan,Baosong Yang,Yiming Li,Zhan Qin,Kui Ren*

Main category: cs.CL

TL;DR: 提出跨语言价值评估基准X-Value，发现当前大语言模型在跨语言价值理解方面存在明显不足，亟需提升其细致的价值敏感性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的评估主要关注显性有害内容，忽视了数字内容中更细微的价值维度。

Method: 构建了X-Value跨语言价值评估基准，包括5000多个问答对，涵盖18种语言和7个基于Schwartz人类基本价值理论的核心领域，并设计两阶段标注框架进行多方评价。

Result: 现有最先进的大语言模型在X-Value上的准确率低于77%，不同语言间性能差异超过20%。

Conclusion: X-Value揭示了大语言模型在价值感知上的短板，强调提升模型对多语言、多维度价值判断能力的紧迫性。

Abstract: While large language models (LLMs) have become pivotal to content safety, current evaluation paradigms primarily focus on detecting explicit harms (e.g., violence or hate speech), neglecting the subtler value dimensions conveyed in digital content. To bridge this gap, we introduce X-Value, a novel Cross-lingual Values Assessment Benchmark designed to evaluate LLMs' ability to assess deep-level values of content from a global perspective. X-Value consists of more than 5,000 QA pairs across 18 languages, systematically organized into 7 core domains grounded in Schwartz's Theory of Basic Human Values and categorized into easy and hard levels for discriminative evaluation. We further propose a unique two-stage annotation framework that first identifies whether an issue falls under global consensus (e.g., human rights) or pluralism (e.g., religion), and subsequently conducts a multi-party evaluation of the latent values embedded within the content. Systematic evaluations on X-Value reveal that current SOTA LLMs exhibit deficiencies in cross-lingual values assessment ($Acc < 77\%$), with significant performance disparities across different languages ($ΔAcc > 20\%$). This work highlights the urgent need to improve the nuanced, values-aware content assessment capability of LLMs. Our X-Value is available at: https://huggingface.co/datasets/Whitolf/X-Value.

</details>


### [21] [Representation Collapse in Machine Translation Through the Lens of Angular Dispersion](https://arxiv.org/abs/2602.17287)
*Evgeniia Tokarchuk,Maya K. Nachesa,Sergey Troshin,Vlad Niculae*

Main category: cs.CL

TL;DR: 本文研究了Transformer神经机器翻译模型中的表示崩塌问题，提出了基于角度分散的正则化方法，有效缓解崩塌并提升翻译性能，且该方法对量化模型同样有效。


<details>
  <summary>Details</summary>
Motivation: Transformer神经机器翻译模型在传统训练过程中存在表示崩塌现象，尤其是在深层表示和连续输出模型中，影响模型性能，亟需有效的缓解策略。

Method: 采用基于角度分散的正则化方法，结合训练过程中的动态分析，缓解深层Transformer的表示崩塌问题。

Result: 本文分析了基于Transformer架构的神经机器翻译模型在训练过程中表现出的表示崩塌问题，尤其是在深层Transformer层的表现。作者指出，标准的下一词预测训练策略可能导致模型表示空间的低效利用，导致表示崩塌。在连续输出神经机器翻译模型中，这一问题更为严重。通过引入基于角度分散的正则化方法，作者成功缓解了表示崩塌现象，并提升了翻译质量。此外，量化模型同样存在类似的表示崩塌，而所采用的正则化方法在量化后依然有效。

Conclusion: 引入角度分散正则化能够有效缓解Transformer模型中表示崩塌问题，提升翻译效果，并且该方法在量化后依然保持有效性。

Abstract: Modern neural translation models based on the Transformer architecture are known for their high performance, particularly when trained on high-resource datasets. A standard next-token prediction training strategy, while widely adopted in practice, may lead to overlooked artifacts such as representation collapse. Previous works have shown that this problem is especially pronounced in the representation of the deeper Transformer layers, where it often fails to efficiently utilize the geometric space. Representation collapse is even more evident in end-to-end training of continuous-output neural machine translation, where the trivial solution would be to set all vectors to the same value. In this work, we analyze the dynamics of representation collapse at different levels of discrete and continuous NMT transformers throughout training. We incorporate an existing regularization method based on angular dispersion and demonstrate empirically that it not only mitigates collapse but also improves translation quality. Furthermore, we show that quantized models exhibit similar collapse behavior and that the benefits of regularization are preserved even after quantization.

</details>


### [22] [Same Meaning, Different Scores: Lexical and Syntactic Sensitivity in LLM Evaluation](https://arxiv.org/abs/2602.17316)
*Bogdan Kostić,Conor Fallon,Julian Risch,Alexander Löser*

Main category: cs.CL

TL;DR: 研究表明LLM更依赖表层词汇模式而非深层语言能力，词汇和句法扰动显著影响性能及排名，提示需将鲁棒性测试纳入常规评估。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评测依赖标准基准，但其可靠性因对输入提示的浅层变化敏感而受质疑，需探索模型对等价语义但形式变化的鲁棒性。

Method: 采用两种语言学原则生成意义等价的词汇和句法扰动：一种是词义替换，另一种是基于依存句法分析的句法变换。对23个现代大型语言模型在MMLU、SQuAD和AMEGA三个基准上进行评估，并分析扰动对模型表现及排行榜排名的影响。

Result: 词汇扰动普遍导致明显且显著的性能下降，句法扰动效果多样，有时甚至提升表现。两种扰动都导致模型排行榜不稳定，且鲁棒性与模型规模不完全相关，依赖具体任务。

Conclusion: 大型语言模型在应对等义表层扰动时表现脆弱，表明其语言理解依赖浅层信息，未来评估应加强鲁棒性检测以全面衡量模型能力。

Abstract: The rapid advancement of Large Language Models (LLMs) has established standardized evaluation benchmarks as the primary instrument for model comparison. Yet, their reliability is increasingly questioned due to sensitivity to shallow variations in input prompts. This paper examines how controlled, truth-conditionally equivalent lexical and syntactic perturbations affect the absolute performance and relative ranking of 23 contemporary LLMs across three benchmarks: MMLU, SQuAD, and AMEGA. We employ two linguistically principled pipelines to generate meaning-preserving variations: one performing synonym substitution for lexical changes, and another using dependency parsing to determine applicable syntactic transformations. Results show that lexical perturbations consistently induce substantial, statistically significant performance degradation across nearly all models and tasks, while syntactic perturbations have more heterogeneous effects, occasionally improving results. Both perturbation types destabilize model leaderboards on complex tasks. Furthermore, model robustness did not consistently scale with model size, revealing strong task dependence. Overall, the findings suggest that LLMs rely more on surface-level lexical patterns than on abstract linguistic competence, underscoring the need for robustness testing as a standard component of LLM evaluation.

</details>


### [23] [RPDR: A Round-trip Prediction-Based Data Augmentation Framework for Long-Tail Question Answering](https://arxiv.org/abs/2602.17366)
*Yiming Zhang,Siyue Zhang,Junbo Zhao,Chen Zhao*

Main category: cs.CL

TL;DR: RPDR通过生成和筛选易学训练数据增强密集检索器，显著提升长尾问答中的稀有知识检索能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在长尾问答中难以获取和准确回忆少见知识，密集检索模型在泛化到罕见或小众知识时表现不佳，需改进检索能力。

Method: 提出RPDR数据增强框架，包括合成数据生成、Round-Trip预测选择易学数据实例和基于这些实例的检索器训练。

Result: 在PopQA和EntityQuestion两个长尾检索基准上，RPDR显著优于BM25和Contriver，特别在极端长尾类别表现突出。

Conclusion: RPDR有效提升长尾知识检索性能，动态路由机制可进一步改进检索效果。

Abstract: Long-tail question answering presents significant challenges for large language models (LLMs) due to their limited ability to acquire and accurately recall less common knowledge. Retrieval-augmented generation (RAG) systems have shown great promise in mitigating this limitation by integrating external retrieval mechanisms. However, dense retrieval models often face the same difficulties when generalizing to rare or niche knowledge. In this study, we introduce RPDR, a novel data augmentation framework that selects high-quality easy-to-learn training data, to enhance dense retrievers. Our approach is built around three core components: synthetic data generation, data selection with Round-Trip prediction to identify easy-to-learn instances, and retriever training with these instances. We evaluate RPDR on two long-tail retrieval benchmarks, PopQA and EntityQuestion, demonstrating substantial improvements over existing retrievers like BM25 and Contriver, especially on extremely long-tail categories. We identify the strengths and limitations of RPDR through detailed human analysis and propose a dynamic routing mechanism to dynamically route queries to specialized retrieval modules to further improve retrieval performance.

</details>


### [24] [The Role of the Availability Heuristic in Multiple-Choice Answering Behaviour](https://arxiv.org/abs/2602.17377)
*Leonidas Zotos,Hedderik van Rijn,Malvina Nissim*

Main category: cs.CL

TL;DR: 研究发现多项选择题中正确答案在大语料中的出现频率更高，选择最频繁出现的答案能显著提升猜题准确率，提示在学生行为建模时应考虑可用性启发法。


<details>
  <summary>Details</summary>
Motivation: 学生在答多项选择题时常通过猜测解决不确定题目，研究可用性启发法是否是有效策略。

Method: 提出基于大规模语料中概念频率计算认知可用性的方法，评估选项的易得性。

Result: 三个大规模题库中，正确答案的可用性显著高于错误选项，选择最易得选项得分比随机猜测高13.5%至32.9%。LLM生成的选项显示类似的可用性模式。

Conclusion: 可用性启发法是多项选择题猜测时的有效策略，建议将可用性纳入学生行为的计算模型。

Abstract: When students are unsure of the correct answer to a multiple-choice question (MCQ), guessing is common practice. The availability heuristic, proposed by A. Tversky and D. Kahneman in 1973, suggests that the ease with which relevant instances come to mind, typically operationalised by the mere frequency of exposure, can offer a mental shortcut for problems in which the test-taker does not know the exact answer. Is simply choosing the option that comes most readily to mind a good strategy for answering MCQs? We propose a computational method of assessing the cognitive availability of MCQ options operationalised by concepts' prevalence in large corpora. The key finding, across three large question sets, is that correct answers, independently of the question stem, are significantly more available than incorrect MCQ options. Specifically, using Wikipedia as the retrieval corpus, we find that always selecting the most available option leads to scores 13.5% to 32.9% above the random-guess baseline. We further find that LLM-generated MCQ options show similar patterns of availability compared to expert-created options, despite the LLMs' frequentist nature and their training on large collections of textual data. Our findings suggest that availability should be considered in current and future work when computationally modelling student behaviour.

</details>


### [25] [Diverse Word Choices, Same Reference: Annotating Lexically-Rich Cross-Document Coreference](https://arxiv.org/abs/2602.17424)
*Anastasia Zhukova,Felix Hamborg,Karsten Donnay,Norman Meuschke,Bela Gipp*

Main category: cs.CL

TL;DR: 本文通过改进跨文档共指标注方案，增强了新闻领域CDCR中对词汇多样性和语境变化的捕捉能力，促进相关研究。


<details>
  <summary>Details</summary>
Motivation: 现有数据集主要集中于事件解析，使用狭义的共指定义，难以处理措辞差异大的新闻报道。

Method: 通过统一的编码手册，重新注释NewsWCL50数据集及ECB+子集，并使用词汇多样性指标和同头词基线进行评估。

Result: 重新注释后的数据集在词汇多样性和注释细粒度方面表现良好，数据集间一致性较高，支持更平衡和话语感知的CDCR研究。

Conclusion: 本文提出的修订版CDCR注释方案有效捕捉新闻报道中的词汇多样性和框架变化，促进了基于话语元素的跨文档共指解析研究。

Abstract: Cross-document coreference resolution (CDCR) identifies and links mentions of the same entities and events across related documents, enabling content analysis that aggregates information at the level of discourse participants. However, existing datasets primarily focus on event resolution and employ a narrow definition of coreference, which limits their effectiveness in analyzing diverse and polarized news coverage where wording varies widely. This paper proposes a revised CDCR annotation scheme of the NewsWCL50 dataset, treating coreference chains as discourse elements (DEs) and conceptual units of analysis. The approach accommodates both identity and near-identity relations, e.g., by linking "the caravan" - "asylum seekers" - "those contemplating illegal entry", allowing models to capture lexical diversity and framing variation in media discourse, while maintaining the fine-grained annotation of DEs. We reannotate the NewsWCL50 and a subset of ECB+ using a unified codebook and evaluate the new datasets through lexical diversity metrics and a same-head-lemma baseline. The results show that the reannotated datasets align closely, falling between the original ECB+ and NewsWCL50, thereby supporting balanced and discourse-aware CDCR research in the news domain.

</details>


### [26] [Evaluating Extremely Low-Resource Machine Translation: A Comparative Study of ChrF++ and BLEU Metrics](https://arxiv.org/abs/2602.17425)
*Sanjeev Kumar,Preethi Jyothi,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 本文比较了BLEU和ChrF++评价指标在极低资源语言机器翻译中的表现，揭示BLEU在解释翻译质量方面有独特优势。


<details>
  <summary>Details</summary>
Motivation: 在极低资源语言环境中，常用的BLEU指标往往不能准确反映翻译质量，故需要评估不同指标在此情境下的有效性及其互补性。

Method: 比较分析BLEU和ChrF++两种机器翻译评价指标在极低资源语言（ELRL）场景下的表现。研究主要针对三种ELRL（Magahi, Bhojpuri和Chhattisgarhi）的大型语言模型和神经机器翻译系统的输出，分析两种指标如何应对译文中的幻觉、重复、源文本复制和变音符号变化等问题。

Result: 发现尽管BLEU得分较低，仍能提供对词汇精准度的补充见解，从而提升结果的解释性；ChrF++虽被广泛采用，但BLEU指标提供了有价值的补充视角。

Conclusion: BLEU和ChrF++各有优势，结合使用能更全面地评估极低资源语言的机器翻译质量，特别是在理解不同翻译错误类型时。

Abstract: Evaluating machine translation (MT) quality in extremely low-resource language (ELRL) scenarios poses unique challenges, as widely used metrics such as BLEU, effective in high-resource settings, often misrepresent quality in data-scarce contexts. This work presents a comparative analysis of BLEU, an n-gram-based metric, and ChrF++, a character-based metric, for MT evaluation in ELRL settings. We examine how each metric responds to translation artifacts, including hallucinations, repetition, source-text copying, and diacritic (\textit{matra}) variations across three ELRLs: Magahi, Bhojpuri, and Chhattisgarhi, with a focus on outputs from large language models (LLMs) and neural MT (NMT) systems. While recent work often relies solely on ChrF++, our findings show that BLEU, despite its lower absolute scores, provides complementary lexical-precision insights that improve interpretability.

</details>


### [27] [Fine-Grained Uncertainty Quantification for Long-Form Language Model Outputs: A Comparative Study](https://arxiv.org/abs/2602.17431)
*Dylan Bouchard,Mohit Singh Chauhan,Viren Bajaj,David Skarbrevik*

Main category: cs.CL

TL;DR: 本文针对大型语言模型的长文本生成幻觉检测，提出细粒度不确定性量化体系，验证了声明响应蕴涵评分和不确定性感知解码的有效性，提升了生成文本的事实准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要针对短文本输出设计，难以有效适用于长文本生成中的幻觉检测，亟需一种更细粒度且适用于长文本的量化方案。

Method: 设计了一种针对长文本生成的细粒度不确定性量化分类体系，涵盖响应分解、单元级评分、响应级聚合三个阶段，开发了多种基于一致性的黑盒评分方法。

Result: 在多个大型语言模型和数据集上实验，发现声明响应蕴涵评分表现优于或等同于复杂评分，声明级评分优于句子级评分，且不确定性感知解码显著提升了长文本输出的事实准确性。

Conclusion: 该论文提出的细粒度不确定性量化方法在长文本生成中表现优异，尤其是在提升事实准确性方面效果显著。

Abstract: Uncertainty quantification has emerged as an effective approach to closed-book hallucination detection for LLMs, but existing methods are largely designed for short-form outputs and do not generalize well to long-form generation. We introduce a taxonomy for fine-grained uncertainty quantification in long-form LLM outputs that distinguishes methods by design choices at three stages: response decomposition, unit-level scoring, and response-level aggregation. We formalize several families of consistency-based black-box scorers, providing generalizations and extensions of existing methods. In our experiments across multiple LLMs and datasets, we find 1) claim-response entailment consistently performs better or on par with more complex claim-level scorers, 2) claim-level scoring generally yields better results than sentence-level scoring, and 3) uncertainty-aware decoding is highly effective for improving the factuality of long-form outputs. Our framework clarifies relationships between prior methods, enables apples-to-apples comparisons, and provides practical guidance for selecting components for fine-grained UQ.

</details>


### [28] [AIDG: Evaluating Asymmetry Between Information Extraction and Containment in Multi-Turn Dialogue](https://arxiv.org/abs/2602.17443)
*Adib Sakhawat,Fardeen Sadab,Rakin Shahriar*

Main category: cs.CL

TL;DR: LLMs在动态对话中更擅长信息防守，推理能力较弱，受信息动态和约束遵守影响显著。


<details>
  <summary>Details</summary>
Motivation: 传统静态基准难以全面评价LLMs的战略推理能力，需通过动态多轮对话游戏探究信息提取与信息维护的非对称性能。

Method: 设计了AIDG游戏框架及两个任务（AIDG-I社交推理和AIDG-II结构化问答），通过439场游戏对六个顶尖LLMs进行评测，量化推理对抗与信息维护能力差异。

Result: 该论文提出了AIDG（对抗信息推理游戏）框架，用以评估大型语言模型（LLMs）在动态、多轮对话中的战略推理能力，具体通过两个任务AIDG-I和AIDG-II来分别衡量社交推理和结构化限制满足。实验显示模型在信息维护（防御）方面显著优于信息提取（推理），且揭示了信息动态和约束遵守两大瓶颈，说明LLMs擅长局部一致性防守，但难以实现全局状态跟踪。

Conclusion: LLMs表现出在维护对话状态上的优势，但在战略性信息推断中存在明显短板，主要因确认策略优于盲目信息推断及指令遵守能力受对话负载影响。

Abstract: Evaluating the strategic reasoning capabilities of Large Language Models (LLMs) requires moving beyond static benchmarks to dynamic, multi-turn interactions. We introduce AIDG (Adversarial Information Deduction Game), a game-theoretic framework that probes the asymmetry between information extraction (active deduction) and information containment (state maintenance) in dialogue. We propose two complementary tasks: AIDG-I, measuring pragmatic strategy in social deduction, and AIDG-II, measuring constraint satisfaction in a structured "20 Questions" setting. Across 439 games with six frontier LLMs, we observe a clear capability asymmetry: models perform substantially better at containment than deduction, with a 350 ELO advantage on defense;(Cohen's d = 5.47). We identify two bottlenecks driving this gap: (1) Information Dynamics, where confirmation strategies are 7.75x more effective than blind deduction (p < 0.00001), and (2) Constraint Adherence, where instruction-following degrades under conversational load, accounting for 41.3% of deductive failures. These findings suggest that while LLMs excel at local defensive coherence, they struggle with the global state tracking required for strategic inquiry.

</details>


### [29] [ABCD: All Biases Come Disguised](https://arxiv.org/abs/2602.17445)
*Mateusz Nowak,Xavier Cadet,Peter Chin*

Main category: cs.CL

TL;DR: 提出了一种减少多选题偏见的新评估协议，通过统一无序标签和句子相似度评分，提高了模型评估的公平性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有多选题评估存在标签位置和标签引导的偏见，导致对大语言模型能力的测评不够准确和鲁棒。

Method: 通过构建合成的NonsenseQA基准，观察并分析多选题中LLMs的答案位置和标签偏见，提出一个减少偏见的评估协议，即用统一无序的标签替换原有标签，并使用句子相似度模型评估答案，避免模型依赖标签位置或标签本身的偏见。

Result: 在多种基准和模型上，该协议显著提高了对答案排列变化的鲁棒性，准确率方差降低3倍，且模型性能仅有微小下降。消融实验进一步证明该方法比标准方法更稳健。

Conclusion: 该方法有效减少了多选题评估中的标签偏见，提升了评估结果的一致性和可靠性，为大语言模型能力测评提供了更准确的工具。

Abstract: Multiple-choice question (MCQ) benchmarks have been a standard evaluation practice for measuring LLMs' ability to reason and answer knowledge-based questions. Through a synthetic NonsenseQA benchmark, we observe that different LLMs exhibit varying degrees of label-position-few-shot-prompt bias, where the model either uses the answer position, the label in front of the answer, the distributions of correct answers present in the few-shot prompt, or a combination of all to answer each MCQ question. We propose a simple bias-reduced evaluation protocol that replaces the labels of each question with uniform, unordered labels and prompts the LLM to use the whole answer presented. With a simple sentence similarity model, we demonstrate improved robustness and lower standard deviation between different permutations of answers with a minimal drop in LLM's performance, exposing the LLM's capabilities under reduced evaluation artifacts, without any help from the prompt examples or the option labels. Across multiple benchmarks and models, this protocol substantially improves the robustness to answer permutations, reducing mean accuracy variance $3\times$ with only a minimal decrease in the mean model's performance. Through ablation studies on various embedding models and similarity functions, we show that the method is more robust than the standard ones.

</details>


### [30] [Entropy-Based Data Selection for Language Models](https://arxiv.org/abs/2602.17465)
*Hongming Li,Yang Liu,Chao Huang*

Main category: cs.CL

TL;DR: 该文提出一种基于熵的无监督数据选择方法，有效降低了计算资源消耗，提高了微调效率，适用于计算资源有限的语言模型微调场景。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型需要大量计算资源和数据资源，数据选择技术虽然能减少训练数据，但通常需要高计算预算，实际微调时资源有限，如何在有限计算资源下有效选择数据成为挑战。

Method: 提出基于熵的无监督数据选择框架（EUDS），利用不确定性估计进行高效数据筛选，减少计算资源消耗，同时保持数据效用。

Result: EUDS在情感分析、主题分类和问答任务中验证了其有效性，同时显著降低计算成本，提高训练时间效率，减少数据需求。

Conclusion: EUDS通过理论和实验证明，能够在计算受限的场景下实现高效的数据选择和模型微调，为语言模型的资源受限优化提供创新方案。

Abstract: Modern language models (LMs) increasingly require two critical resources: computational resources and data resources. Data selection techniques can effectively reduce the amount of training data required for fine-tuning LMs. However, their effectiveness is closely related to computational resources, which always require a high compute budget. Owing to the resource limitations in practical fine-tuning scenario, we systematically reveal the relationship between data selection and uncertainty estimation of selected data. Although large language models (LLMs) exhibit exceptional capabilities in language understanding and generation, which provide new ways to alleviate data scarcity, evaluating data usability remains a challenging task. This makes efficient data selection indispensable. To mitigate these issues, we propose Entropy-Based Unsupervised Data Selection (EUDS) framework. Empirical experiments on sentiment analysis (SA), topic classification (Topic-CLS), and question answering (Q&A) tasks validate its effectiveness. EUDS establishes a computationally efficient data-filtering mechanism. Theoretical analysis and experimental results confirm the effectiveness of our approach. EUDS significantly reduces computational costs and improves training time efficiency with less data requirement. This provides an innovative solution for the efficient fine-tuning of LMs in the compute-constrained scenarios.

</details>


### [31] [PEACE 2.0: Grounded Explanations and Counter-Speech for Combating Hate Expressions](https://arxiv.org/abs/2602.17467)
*Greta Damo,Stéphane Petiot,Elena Cabrio,Serena Villata*

Main category: cs.CL

TL;DR: PEACE 2.0通过结合检索增强生成技术，实现对仇恨言论的证据支持解释与自动反驳，提升了仇恨言论检测与应对能力。


<details>
  <summary>Details</summary>
Motivation: 在线平台上仇恨言论数量日益增加，现有技术虽能检测仇恨言论，但如何有效回应仇恨言论（即反说话）仍是挑战。

Method: 引入基于检索增强生成（RAG）的管道，通过检索事实和证据来解释仇恨言论并生成针对性的反驳言论。

Result: 开发了PEACE 2.0工具，不仅能分析和解释消息为何被认为是仇恨言论，还能自动生成基于证据的反驳言论，并分析反驳言论的特性。

Conclusion: PEACE 2.0有效整合多项功能，实现了对显性和隐性仇恨言论的深度分析和基于事实的自动反驳，为仇恨言论的识别和应对提供了新工具。

Abstract: The increasing volume of hate speech on online platforms poses significant societal challenges. While the Natural Language Processing community has developed effective methods to automatically detect the presence of hate speech, responses to it, called counter-speech, are still an open challenge. We present PEACE 2.0, a novel tool that, besides analysing and explaining why a message is considered hateful or not, also generates a response to it. More specifically, PEACE 2.0 has three main new functionalities: leveraging a Retrieval-Augmented Generation (RAG) pipeline i) to ground HS explanations into evidence and facts, ii) to automatically generate evidence-grounded counter-speech, and iii) exploring the characteristics of counter-speech replies. By integrating these capabilities, PEACE 2.0 enables in-depth analysis and response generation for both explicit and implicit hateful messages.

</details>


### [32] [Auditing Reciprocal Sentiment Alignment: Inversion Risk, Dialect Representation and Intent Misalignment in Transformers](https://arxiv.org/abs/2602.17469)
*Nusrat Jahan Lia,Shubhashis Roy Dipta*

Main category: cs.CL

TL;DR: 本文针对孟加拉语与英语之间的情感对齐问题，揭示了现有模型在情感理解上的严重缺陷，主张文化多样性驱动的对齐方法和新的评价指标以提升人机互信。


<details>
  <summary>Details</summary>
Motivation: 解决跨语言情感对齐问题，确保AI系统准确理解人类意图，增强人类对AI的信任。

Method: 通过评估四种变压器架构，特别分析压缩模型（mDistilBERT）和区域模型（IndicBERT）在孟加拉语和英语之间的表现。

Result: 发现当前对齐范式存在严重的安全性和表现缺陷，如28.7%的情感反转率和区域模型在处理正式孟加拉语时的57%对齐误差增加。还发现模型存在“非对称共情”和“现代偏见”等系统性问题。

Conclusion: 强调需要基于文化和语言多样性的多元化对齐方法，以保证情感一致性和促进人类与AI的互信，呼吁在对齐基准中引入惩罚极性反转的“情感稳定性”指标。

Abstract: The core theme of bidirectional alignment is ensuring that AI systems accurately understand human intent and that humans can trust AI behavior. However, this loop fractures significantly across language barriers. Our research addresses Cross-Lingual Sentiment Misalignment between Bengali and English by benchmarking four transformer architectures. We reveal severe safety and representational failures in current alignment paradigms. We demonstrate that compressed model (mDistilBERT) exhibits 28.7% "Sentiment Inversion Rate," fundamentally misinterpreting positive user intent as negative (or vice versa). Furthermore, we identify systemic nuances affecting human-AI trust, including "Asymmetric Empathy" where some models systematically dampen and others amplify the affective weight of Bengali text relative to its English counterpart. Finally, we reveal a "Modern Bias" in the regional model (IndicBERT), which shows a 57% increase in alignment error when processing formal (Sadhu) Bengali. We argue that equitable human-AI co-evolution requires pluralistic, culturally grounded alignment that respects language and dialectal diversity over universal compression, which fails to preserve the emotional fidelity required for reciprocal human-AI trust. We recommend that alignment benchmarks incorporate "Affective Stability" metrics that explicitly penalize polarity inversions in low-resource and dialectal contexts.

</details>


### [33] [Small LLMs for Medical NLP: a Systematic Analysis of Few-Shot, Constraint Decoding, Fine-Tuning and Continual Pre-Training in Italian](https://arxiv.org/abs/2602.17475)
*Pietro Ferrazzi,Mattia Franzin,Alberto Lavelli,Bernardo Magnini*

Main category: cs.CL

TL;DR: 本研究证明约10亿参数的小型大语言模型通过微调等策略，可在多项医疗NLP任务中超越更大模型，促进其在实际医疗中的应用，并公开了相关意大利医疗数据集和微调模型。


<details>
  <summary>Details</summary>
Motivation: 考虑到大模型的计算资源需求较高，难以在实际医疗环境广泛部署，研究小型LLM能否有效完成医疗任务且保持竞争力。

Method: 评估了来自Llama-3、Gemma-3和Qwen3三大模型家族的小型LLM，在20个临床NLP任务上比较了多种适应策略，包括推理时的少量示例提示和约束解码，训练时的有监督微调和持续预训练。

Result: 发现微调是最有效的适应策略，少量示例提示结合约束解码是强有力的低资源替代方案。基于Qwen3-1.7B的配置表现最佳，平均得分比Qwen3-32B高9.2分。

Conclusion: 小型大语言模型（约10亿参数）在多项医疗自然语言处理任务中表现出与大型模型相媲美甚至更优的性能，特别是在微调方法的帮助下达到最佳效果。

Abstract: Large Language Models (LLMs) consistently excel in diverse medical Natural Language Processing (NLP) tasks, yet their substantial computational requirements often limit deployment in real-world healthcare settings. In this work, we investigate whether "small" LLMs (around one billion parameters) can effectively perform medical tasks while maintaining competitive accuracy. We evaluate models from three major families-Llama-3, Gemma-3, and Qwen3-across 20 clinical NLP tasks among Named Entity Recognition, Relation Extraction, Case Report Form Filling, Question Answering, and Argument Mining. We systematically compare a range of adaptation strategies, both at inference time (few-shot prompting, constraint decoding) and at training time (supervised fine-tuning, continual pretraining). Fine-tuning emerges as the most effective approach, while the combination of few-shot prompting and constraint decoding offers strong lower-resource alternatives. Our results show that small LLMs can match or even surpass larger baselines, with our best configuration based on Qwen3-1.7B achieving an average score +9.2 points higher than Qwen3-32B. We release a comprehensive collection of all the publicly available Italian medical datasets for NLP tasks, together with our top-performing models. Furthermore, we release an Italian dataset of 126M words from the Emergency Department of an Italian Hospital, and 175M words from various sources that we used for continual pre-training.

</details>


### [34] [Bridging the Domain Divide: Supervised vs. Zero-Shot Clinical Section Segmentation from MIMIC-III to Obstetrics](https://arxiv.org/abs/2602.17513)
*Baris Karacan,Barbara Di Eugenio,Patrick Thornton*

Main category: cs.CL

TL;DR: 本文提出了一个新的产科临床文本分节数据集，并比较了基于监督学习和零样本大模型在医疗分节任务中的表现。发现监督模型在训练域内表现较好，但域外表现下降显著，而零样本模型在纠正幻觉后展现出较强的跨域适应性。


<details>
  <summary>Details</summary>
Motivation: 现有临床文本分节研究多基于MIMIC-III等公共语料，覆盖领域有限，缺乏对跨领域适应性和零样本模型的系统研究。

Method: 本文构建了产科临床分节数据集，系统评估了基于Transformer的监督模型在MIMIC-III和产科数据上的表现，且首次比较了监督模型与零样本大语言模型的分节效果。

Result: 基于监督的分节模型训练集内表现出色，但跨域性能大幅下降；零样本模型展现出良好的域外适应性，幻觉纠正是关键。

Conclusion: 监督模型在其训练领域表现良好，但在不同领域时性能降低；零样本大语言模型在去除幻觉后具备更好的领域适应能力。

Abstract: Clinical free-text notes contain vital patient information. They are structured into labelled sections; recognizing these sections has been shown to support clinical decision-making and downstream NLP tasks. In this paper, we advance clinical section segmentation through three key contributions. First, we curate a new de-identified, section-labeled obstetrics notes dataset, to supplement the medical domains covered in public corpora such as MIMIC-III, on which most existing segmentation approaches are trained. Second, we systematically evaluate transformer-based supervised models for section segmentation on a curated subset of MIMIC-III (in-domain), and on the new obstetrics dataset (out-of-domain). Third, we conduct the first head-to-head comparison of supervised models for medical section segmentation with zero-shot large language models. Our results show that while supervised models perform strongly in-domain, their performance drops substantially out-of-domain. In contrast, zero-shot models demonstrate robust out-of-domain adaptability once hallucinated section headers are corrected. These findings underscore the importance of developing domain-specific clinical resources and highlight zero-shot segmentation as a promising direction for applying healthcare NLP beyond well-studied corpora, as long as hallucinations are appropriately managed.

</details>


### [35] [Using LLMs for Knowledge Component-level Correctness Labeling in Open-ended Coding Problems](https://arxiv.org/abs/2602.17542)
*Zhangqi Duan,Arnav Kankaria,Dhruv Kartik,Andrew Lan*

Main category: cs.CL

TL;DR: 本文提出了一种基于大规模语言模型的自动框架，用于从学生代码中直接标注细粒度知识组件（KC）正确性，以解决传统方法中缺乏KC级正确性标签的问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据中缺乏KC级别的正确性标注，尤其是开放式编程任务中，简单将题目正确性传递到所有关联KC会掩盖部分掌握状态，影响学习曲线拟合和效果。

Method: 利用大规模语言模型直接从学生编程代码中评估每个KC的应用正确性，并引入时间上下文相关的Code-KC映射机制，提高标注的准确性。

Result: 实验中该方法生成的学习曲线更符合认知理论，预测性能优于基线模型，人类专家评估显示与LLM标注高度一致。

Conclusion: 该框架能够更准确地标注KC正确性，生成符合认知理论的学习曲线，并提升预测学生表现的效果，实验和人工评估均验证了其有效性。

Abstract: Fine-grained skill representations, commonly referred to as knowledge components (KCs), are fundamental to many approaches in student modeling and learning analytics. However, KC-level correctness labels are rarely available in real-world datasets, especially for open-ended programming tasks where solutions typically involve multiple KCs simultaneously. Simply propagating problem-level correctness to all associated KCs obscures partial mastery and often leads to poorly fitted learning curves. To address this challenge, we propose an automated framework that leverages large language models (LLMs) to label KC-level correctness directly from student-written code. Our method assesses whether each KC is correctly applied and further introduces a temporal context-aware Code-KC mapping mechanism to better align KCs with individual student code. We evaluate the resulting KC-level correctness labels in terms of learning curve fit and predictive performance using the power law of practice and the Additive Factors Model. Experimental results show that our framework leads to learning curves that are more consistent with cognitive theory and improves predictive performance, compared to baselines. Human evaluation further demonstrates substantial agreement between LLM and expert annotations.

</details>


### [36] [Learning to Stay Safe: Adaptive Regularization Against Safety Degradation during Fine-Tuning](https://arxiv.org/abs/2602.17546)
*Jyotin Goel,Souvik Maji,Pratik Mazumder*

Main category: cs.CL

TL;DR: 本文提出了一种基于安全风险自适应正则化的训练框架，有效提升指令遵循语言模型的安全性，兼顾效用，无需增加推理成本。


<details>
  <summary>Details</summary>
Motivation: 指令遵循语言模型虽被训练为有用且安全，但在微调过程中安全性容易下降，现有防护手段受限且常需在安全与效用间权衡，急需一种能动态维护模型安全同时兼顾效用的训练机制。

Method: 采用两种安全风险估计方式：一是使用法官评分系统对训练批次打分；二是利用轻量级分类器基于模型中间激活预测潜在危害意图。风险信号用于限制高风险更新，低风险更新则正常训练。

Result: 本文提出了一种自适应正则化训练框架，通过评估训练过程中的安全风险来保持语言模型在微调中的安全对齐。文中设计了两种安全风险估计方法：基于法官的安全评论机制和基于中间激活的风险预测器，分别为训练批次赋予危害评分。基于风险信号，对高风险的模型更新进行限制以保持接近安全参考策略，而低风险更新则采用常规训练。实验证明，这两种方法均能有效降低攻击成功率，保持模型性能无损且无额外推理开销。

Conclusion: 通过动态调整正则化强度以响应安全风险，语言模型在微调过程中能够保持安全性和性能的平衡，显著降低恶意攻击成功率，且无推理代价。

Abstract: Instruction-following language models are trained to be helpful and safe, yet their safety behavior can deteriorate under benign fine-tuning and worsen under adversarial updates. Existing defenses often offer limited protection or force a trade-off between safety and utility. We introduce a training framework that adapts regularization in response to safety risk, enabling models to remain aligned throughout fine-tuning. To estimate safety risk at training time, we explore two distinct approaches: a judge-based Safety Critic that assigns high-level harm scores to training batches, and an activation-based risk predictor built with a lightweight classifier trained on intermediate model activations to estimate harmful intent. Each approach provides a risk signal that is used to constrain updates deemed higher risk to remain close to a safe reference policy, while lower-risk updates proceed with standard training. We empirically verify that harmful intent signals are predictable from pre-generation activations and that judge scores provide effective high-recall safety guidance. Across multiple model families and attack scenarios, adaptive regularization with either risk estimation approach consistently lowers attack success rate compared to standard fine-tuning, preserves downstream performance, and adds no inference-time cost. This work demonstrates a principled mechanism for maintaining safety without sacrificing utility.

</details>


### [37] [Modeling Distinct Human Interaction in Web Agents](https://arxiv.org/abs/2602.17588)
*Faria Huq,Zora Zhiruo Wang,Zhanqiu Guo,Venu Arvind Arangarajan,Tianyue Ou,Frank Xu,Shuyan Zhou,Graham Neubig,Jeffrey P. Bigham*

Main category: cs.CL

TL;DR: 本文通过分析用户干预行为，训练语言模型预测干预时机，提升协作网络代理的表现。


<details>
  <summary>Details</summary>
Motivation: 当前自主网络代理缺乏理解何时及为何用户干预，导致关键决策点自动执行或过度请求确认，需改进协作机制以提升用户体验。

Method: 收集真实用户网络导航轨迹数据，识别交互模式，训练语言模型预测用户干预时机，并在实际代理中验证效果。

Result: 本文提出了建模人类干预以支持协作网络任务执行的方法。通过收集包含4200多人类与代理交互行为的CowCorpus数据集，识别了用户与代理交互的四种模式。基于此，训练语言模型预测用户何时干预，提升预测准确率约61.4-63.4%。实际部署后，用户评价代理有用性提升26.5%。

Conclusion: 结构化建模人类干预能使网络代理更适应用户需求，提升协作效率和用户满意度。

Abstract: Despite rapid progress in autonomous web agents, human involvement remains essential for shaping preferences and correcting agent behavior as tasks unfold. However, current agentic systems lack a principled understanding of when and why humans intervene, often proceeding autonomously past critical decision points or requesting unnecessary confirmation. In this work, we introduce the task of modeling human intervention to support collaborative web task execution. We collect CowCorpus, a dataset of 400 real-user web navigation trajectories containing over 4,200 interleaved human and agent actions. We identify four distinct patterns of user interaction with agents -- hands-off supervision, hands-on oversight, collaborative task-solving, and full user takeover. Leveraging these insights, we train language models (LMs) to anticipate when users are likely to intervene based on their interaction styles, yielding a 61.4-63.4% improvement in intervention prediction accuracy over base LMs. Finally, we deploy these intervention-aware models in live web navigation agents and evaluate them in a user study, finding a 26.5% increase in user-rated agent usefulness. Together, our results show structured modeling of human intervention leads to more adaptive, collaborative agents.

</details>


### [38] [The Cascade Equivalence Hypothesis: When Do Speech LLMs Behave Like ASR$\rightarrow$LLM Pipelines?](https://arxiv.org/abs/2602.17598)
*Jayadev Billa*

Main category: cs.CL

TL;DR: 当前的语音大语言模型大多只是简单的ASR与LLM的串联组合，表现高度依赖架构，在噪声环境下表现不佳。


<details>
  <summary>Details</summary>
Motivation: 验证当前语音大语言模型在解决基于转录文本的任务中的表现是否只是隐式的自动语音识别（ASR），及探究这些模型的架构依赖性。

Method: 通过对四种语音大语言模型（LLMs）和六个任务进行匹配骨干网络测试，控制了LLM骨干结构变量，分析它们与简单的Whisper到LLM的串联模型的行为和机制相似性。

Result: 发现Ultravox与其匹配的串联模型在统计学上无显著差异（κ=0.93），文本信息在隐藏状态中显现且对模型表现必不可少；而Qwen2-Audio在架构上真正有所不同，说明串联系统的等价性依赖具体架构。并且在噪声环境下，现有语音LLMs表现更差，干净条件下的优势在0 dB噪声下可逆转高达7.6%。

Conclusion: 目前大多数语音LLMs在实际应用中本质上是昂贵的串联系统，且其性能在噪声环境下明显下降，未来改进应关注架构优化以提升鲁棒性和效率。

Abstract: Current speech LLMs largely perform implicit ASR: on tasks solvable from a transcript, they are behaviorally and mechanistically equivalent to simple Whisper$\to$LLM cascades. We show this through matched-backbone testing across four speech LLMs and six tasks, controlling for the LLM backbone for the first time. Ultravox is statistically indistinguishable from its matched cascade ($κ{=}0.93$); logit lens reveals literal text emerging in hidden states; LEACE concept erasure confirms text representations are causally necessary in both architectures tested, collapsing accuracy to near-zero. Qwen2-Audio genuinely diverges, revealing cascade equivalence is architecture-dependent, not universal. For most deployed use cases, current speech LLMs are expensive cascades, and under noise, they are worse ones, with clean-condition advantages reversing by up to 7.6% at 0 dB.

</details>


### [39] [Unmasking the Factual-Conceptual Gap in Persian Language Models](https://arxiv.org/abs/2602.17623)
*Alireza Sakhaeirad,Ali Ma'manpoosh,Arshia Hemmat*

Main category: cs.CL

TL;DR: DivanBench测评显示波斯语大模型在文化推理方面存在认同偏置和推理能力不足，单纯扩充语料无法解决文化理解瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有波斯语NLP基准多关注实际记忆，缺乏对隐含社会规范和文化推理能力的有效区分，需建立专门诊断数据集进行深入分析。

Method: 设计了包含事实检索、情景对比验证和情境推理三种任务类型的315个问题，评估七个波斯语大模型在文化知识的识别与推理上的表现。

Result: 本文介绍了DivanBench，这是一个针对波斯语迷信和习俗的诊断基准，旨在区分模型对文化事实的记忆和对隐含社会规范的推理能力。通过315个问题，考察了七个波斯语大语言模型的表现，发现模型存在认同偏置、预训练加剧偏置及事实检索与应用场景推理能力存在显著差距等问题。

Conclusion: 文化理解能力不仅仅依赖于大量单语数据训练，当前模型多是模仿文化模式而非真正内化文化规范，导致推理能力不足。

Abstract: While emerging Persian NLP benchmarks have expanded into pragmatics and politeness, they rarely distinguish between memorized cultural facts and the ability to reason about implicit social norms. We introduce DivanBench, a diagnostic benchmark focused on superstitions and customs, arbitrary, context-dependent rules that resist simple logical deduction. Through 315 questions across three task types (factual retrieval, paired scenario verification, and situational reasoning), we evaluate seven Persian LLMs and reveal three critical failures: most models exhibit severe acquiescence bias, correctly identifying appropriate behaviors but failing to reject clear violations; continuous Persian pretraining amplifies this bias rather than improving reasoning, often degrading the model's ability to discern contradictions; and all models show a 21\% performance gap between retrieving factual knowledge and applying it in scenarios. These findings demonstrate that cultural competence requires more than scaling monolingual data, as current models learn to mimic cultural patterns without internalizing the underlying schemas.

</details>


### [40] [Differences in Typological Alignment in Language Models' Treatment of Differential Argument Marking](https://arxiv.org/abs/2602.17653)
*Iskar Deng,Nathalia Xu,Shane Steinert-Threlkeld*

Main category: cs.CL

TL;DR: 本文通过合成语料训练GPT-2，研究语言模型对差异性论元标记的类型学偏好，发现模型在人类标记方向偏好上表现良好，但未能重现对宾语标记的强偏好，提示不同倾向有不同来源。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型在合成语料库上训练后是否能展现出类似于人类语言的类型学偏好，特别是差异性论元标记（DAM）系统的表现。

Method: 采用受控的合成学习方法，使用18个实现不同差异性论元标记系统的合成语料库训练GPT-2模型，并通过最小对对比测试模型的泛化能力。

Result: 模型在标记方向的自然性偏好上表现出类似人类的偏好，但未能复制人类语言中在DAM中显著偏好对宾语进行显性标记的特点。

Conclusion: 不同类型学倾向可能来源于不同的内在机制，表明语言模型在模拟人类语法类型学特征时存在一定的局限性。

Abstract: Recent work has shown that language models (LMs) trained on synthetic corpora can exhibit typological preferences that resemble cross-linguistic regularities in human languages, particularly for syntactic phenomena such as word order. In this paper, we extend this paradigm to differential argument marking (DAM), a semantic licensing system in which morphological marking depends on semantic prominence. Using a controlled synthetic learning method, we train GPT-2 models on 18 corpora implementing distinct DAM systems and evaluate their generalization using minimal pairs. Our results reveal a dissociation between two typological dimensions of DAM. Models reliably exhibit human-like preferences for natural markedness direction, favoring systems in which overt marking targets semantically atypical arguments. In contrast, models do not reproduce the strong object preference in human languages, in which overt marking in DAM more often targets objects rather than subjects. These findings suggest that different typological tendencies may arise from distinct underlying sources.

</details>


### [41] [What Language is This? Ask Your Tokenizer](https://arxiv.org/abs/2602.17655)
*Clara Meister,Ahmetcan Yavuz,Pietro Lesci,Tiago Pimentel*

Main category: cs.CL

TL;DR: UniLID利用UnigramLM分词算法的概率模型，有效提升低资源语言识别准确率，支持增量训练，表现优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有语言识别系统在低资源和近似语言环境中表现不佳，亟需一种高效且适用于多语言环境的方法。

Method: 基于UnigramLM算法构建语言条件的单元分布，采用语言特异性分词，将分词和语言识别结合，支持共享词汇表和增量训练。

Result: 提出了基于UnigramLM分词算法的UniLID方法，实现了在低资源语言环境下的高准确率和高样本效率，同时支持增量添加新语言且无需重新训练。

Conclusion: UniLID是一种简单高效的语言识别方法，适合多语言和低资源场景，显著提升了语言识别性能及应用灵活性。

Abstract: Language Identification (LID) is an important component of many multilingual natural language processing pipelines, where it facilitates corpus curation, training data analysis, and cross-lingual evaluation of large language models. Despite near-perfect performance on high-resource languages, existing systems remain brittle in low-resource and closely related language settings. We introduce UniLID, a simple and efficient LID method based on the UnigramLM tokenization algorithm, leveraging its probabilistic framing, parameter estimation technique and inference strategy. In short, we learn language-conditional unigram distributions over a shared tokenizer vocabulary but treat segmentation as a language-specific phenomenon. Our formulation is data- and compute-efficient, supports incremental addition of new languages without retraining existing models, and can naturally be integrated into existing language model tokenization pipelines. Empirical evaluations against widely used baselines, including fastText, GlotLID, and CLD3, show that UniLID achieves competitive performance on standard benchmarks, substantially improves sample efficiency in low-resource settings - surpassing 70% accuracy with as few as five labeled samples per language - and delivers large gains on fine-grained dialect identification.

</details>


### [42] [Sink-Aware Pruning for Diffusion Language Models](https://arxiv.org/abs/2602.17664)
*Aidar Myrzakhan,Tianyi Li,Bowei Guo,Shengkun Tang,Zhiqiang Shen*

Main category: cs.CL

TL;DR: 本文针对扩散语言模型（DLM）推理成本高的问题，提出了一种新的剪枝方法，即“Sink-Aware Pruning”，通过识别和剪枝不稳定的注意力汇点，提高剪枝效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型推理时的高成本促使研究有效剪枝方法，且传统继承自自回归模型的剪枝启发式不适用于DLM中的注意力汇点结构。

Method: 通过分析注意力汇点在扩散语言模型中的时序变化，发现其不稳定性，基于此提出自动识别并剪枝不稳定汇点的Sink-Aware Pruning方法。

Result: Sink-Aware Pruning在保持模型性能的同时，显著降低计算资源消耗，并优于现有在相同计算条件下的剪枝方法。

Conclusion: 本文提出的Sink-Aware Pruning方法能够在不重新训练的情况下，有效提升扩散语言模型的质量与效率平衡，优于现有的剪枝基线方法。

Abstract: Diffusion Language Models (DLMs) incur high inference cost due to iterative denoising, motivating efficient pruning. Existing pruning heuristics largely inherited from autoregressive (AR) LLMs, typically preserve attention sink tokens because AR sinks serve as stable global anchors. We show that this assumption does not hold for DLMs: the attention-sink position exhibits substantially higher variance over the full generation trajectory (measured by how the dominant sink locations shift across timesteps), indicating that sinks are often transient and less structurally essential than in AR models. Based on this observation, we propose ${\bf \texttt{Sink-Aware Pruning}}$, which automatically identifies and prunes unstable sinks in DLMs (prior studies usually keep sinks for AR LLMs). Without retraining, our method achieves a better quality-efficiency trade-off and outperforms strong prior pruning baselines under matched compute. Our code is available at https://github.com/VILA-Lab/Sink-Aware-Pruning.

</details>


<div id='cs.OH'></div>

# cs.OH [[Back]](#toc)

### [43] [A Conceptual Hybrid Framework for Post-Quantum Security: Integrating BB84 QKD, AES, and Bio-inspired Mechanisms](https://arxiv.org/abs/2602.16922)
*Md. Ismiel Hossen Abir*

Main category: cs.OH

TL;DR: 本研究针对量子计算威胁，提出了结合经典和量子技术的混合安全框架保障数据安全。


<details>
  <summary>Details</summary>
Motivation: 量子计算能力提升对现有基于大数分解的RSA加密构成威胁，亟需开发适应后量子时代的数据保护方案。

Method: 设计了结合AES加密、BB84量子密钥分发、量子态比较和生物启发免疫系统的混合安全框架。

Result: 本文研究了量子计算对经典密码学中RSA的威胁，指出经典分解方法对大密钥效率低下，而Shor量子算法能多项式时间内破解RSA。基于此，提出了一个结合AES加密、BB84量子密钥分发、量子态比较认证及生物启发免疫系统的混合安全框架，用于后量子时代的数据保护。该框架在理论上能实现充分的密钥协商与高效的窃听检测，同时具备扩展性和自适应检测威胁的能力。本文为概念性框架设计，详细实现和安全性验证留待未来研究。

Conclusion: RSA在量子攻击下存在脆弱性，提出的混合框架通过结合经典加密和量子技术提供了后量子时代的数据安全保护方案。

Abstract: Quantum computing is a significant risk to classical cryptographic, especially RSA, which depends on the difficulty of factoring large numbers. Classical factorization methods, such as Trial Division and Pollard's Rho, are inefficient for large keys, while Shor's quantum algorithm can break RSA efficiently in polynomial time. This research studies RSA's vulnerabilities under both classical and quantum attacks and designs a hybrid security framework to ensure data protection in the post-quantum era. The conceptual framework combines AES encryption for classical security, BB84 Quantum Key Distribution (QKD) for secure key exchange with eavesdropping detection, quantum state comparison for lightweight authentication, and a bio-inspired immune system for adaptive threat detection. RSA is vulnerable to Shor's algorithm, BB84 achieves full key agreement in ideal conditions, and it detects eavesdropping with high accuracy. The conceptual model includes both classical and quantum security methods, providing a scalable and adaptive solution for Post-Quantum encryption data protection. This work primarily proposes a conceptual framework. Detailed implementation, security proofs, and extensive experimental validation are considered future work.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [44] [Guiding LLM-Based Human Mobility Simulation with Mobility Measures from Shared Data](https://arxiv.org/abs/2602.16726)
*Hua Yan,Heng Tan,Yu Yang*

Main category: cs.MA

TL;DR: 该论文提出一种基于移动指标引导的多提示调整框架，有效解决了大规模人类移动性模拟中缺乏群体协调的问题。


<details>
  <summary>Details</summary>
Motivation: 当前大规模人类移动性仿真方法虽能模拟个体认知过程，但缺乏群体层面的协调机制，无法捕捉集体现象。

Method: 设计了一个基于共享数据移动指标指导的多提示调整框架，采用粗粒度到细粒度的策略逐步优化个体提示，融合多重群体移动性目标。

Result: 提出了M2LSimu框架，通过基于移动性指标的多提示调整，实现个体层面与群体目标的协调，显著提升了仿真结果的现实性。

Conclusion: M2LSimu框架能在有限资源下满足多样化的群体移动性目标，显著优于现有LLM方法，实现更加真实的移动轨迹模拟。

Abstract: Large-scale human mobility simulation is critical for many science domains such as urban science, epidemiology, and transportation analysis. Recent works treat large language models (LLMs) as human agents to simulate realistic mobility trajectories by modeling individual-level cognitive processes. However, these approaches generate individual mobility trajectories independently, without any population-level coordination mechanism, and thus fail to capture the emergence of collective behaviors. To address this issue, we design M2LSimu, a mobility measures-guided multi-prompt adjustment framework that leverages mobility measures derived from shared data as guidance to refine individual-level prompts for realistic mobility generation. Our framework applies coarse-grained adjustment strategies guided by mobility measures, progressively enabling fine-grained individual-level adaptation while satisfying multiple population-level mobility objectives under a limited budget. Experiments show that M2LSimu significantly outperforms state-of-the-art LLM-based methods on two public datasets.

</details>


### [45] [Self-Evolving Multi-Agent Network for Industrial IoT Predictive Maintenance](https://arxiv.org/abs/2602.16738)
*Rebin Saleh,Khanh Pham Dinh,Balázs Villányi,Truong-Son Hy*

Main category: cs.MA

TL;DR: SEMAS是一种自我进化的层次多代理系统，分布在边缘、雾和云计算层，实现工业物联网中的实时异常检测，兼顾解释性和低延迟。


<details>
  <summary>Details</summary>
Motivation: 传统模型无法适应运营环境变化且在边缘部署时资源消耗大，需一种既能保持实时性能又具备自适应和解释能力的解决方案。

Method: 系统利用边缘代理进行轻量特征提取和预过滤，雾代理采用动态共识投票的多样化集成检测，云代理通过代理策略优化（PPO）持续优化策略，并结合联邦知识聚合和LLM响应生成提高适应性和解释性。

Result: SEMAS实现稳定且高效的异常检测，适应动态操作环境，显著降低延迟，实现真实的工业物联网实时部署。消融实验表明系统各个关键组件均对性能提升有重要贡献。

Conclusion: SEMAS在两个工业基准测试中表现出优异的异常检测性能和适应性，能够满足生产环境对实时性和解释性的严格要求。

Abstract: Industrial IoT predictive maintenance requires systems capable of real-time anomaly detection without sacrificing interpretability or demanding excessive computational resources. Traditional approaches rely on static, offline-trained models that cannot adapt to evolving operational conditions, while LLM-based monolithic systems demand prohibitive memory and latency, rendering them impractical for on-site edge deployment. We introduce SEMAS, a self-evolving hierarchical multi-agent system that distributes specialized agents across Edge, Fog, and Cloud computational tiers. Edge agents perform lightweight feature extraction and pre-filtering; Fog agents execute diversified ensemble detection with dynamic consensus voting; and Cloud agents continuously optimize system policies via Proximal Policy Optimization (PPO) while maintaining asynchronous, non-blocking inference. The framework incorporates LLM-based response generation for explainability and federated knowledge aggregation for adaptive policy distribution. This architecture enables resource-aware specialization without sacrificing real-time performance or model interpretability. Empirical evaluation on two industrial benchmarks (Boiler Emulator and Wind Turbine) demonstrates that SEMAS achieves superior anomaly detection performance with exceptional stability under adaptation, sustains prediction accuracy across evolving operational contexts, and delivers substantial latency improvements enabling genuine real-time deployment. Ablation studies confirm that PPO-driven policy evolution, consensus voting, and federated aggregation each contribute materially to system effectiveness. These findings indicate that resource-aware, self-evolving 1multi-agent coordination is essential for production-ready industrial IoT predictive maintenance under strict latency and explainability constraints.

</details>


### [46] [AdaptOrch: Task-Adaptive Multi-Agent Orchestration in the Era of LLM Performance Convergence](https://arxiv.org/abs/2602.16873)
*Geunbin Yu*

Main category: cs.MA

TL;DR: 本文提出一种动态选择多智能体协调拓扑的框架AdaptOrch，显著提升了系统性能，表明协调设计比单模型优化更重要。


<details>
  <summary>Details</summary>
Motivation: 随着各大语言模型性能趋同，单一模型选择的收益递减，亟需通过多智能体协调拓扑设计提升整体系统性能。

Method: 构建了性能收敛定律和拓扑路由算法，实现任务依赖图到最优协调拓扑的映射，并设计自适应合成协议保证多智能体输出的终止性和一致性。

Result: 本文提出了AdaptOrch框架，通过动态选择多智能体的协调拓扑结构（并行、顺序、层次和混合），显著提升系统性能。该框架包括性能收敛定律、拓扑路由算法和自适应合成协议，验证了其在编码、推理和检索生成任务中相比单一固定拓扑可提升12-23%的性能。研究表明，协调拓扑设计已成为独立于模型能力的关键优化方向。

Conclusion: 协调拓扑的设计在系统性能提升中起到主导作用，AdaptOrch框架验证了动态拓扑选择能显著超越单模型优化。

Abstract: As large language models from diverse providers converge toward comparable benchmark performance, the traditional paradigm of selecting a single best model per task yields diminishing returns. We argue that orchestration topology -- the structural composition of how multiple agents are coordinated, parallelized, and synthesized -- now dominates system-level performance over individual model capability. We present AdaptOrch, a formal framework for task-adaptive multi-agent orchestration that dynamically selects among four canonical topologies (parallel, sequential, hierarchical, and hybrid) based on task dependency graphs and empirically derived domain characteristics. Our framework introduces three key contributions: (1) a Performance Convergence Scaling Law, formalizing conditions under which orchestration selection outweighs model selection; (2) a Topology Routing Algorithm that maps task decomposition DAGs to optimal orchestration patterns in O(|V| + |E|) time; and (3) an Adaptive Synthesis Protocol with provable termination guarantees and heuristic consistency scoring for parallel agent outputs. We validate AdaptOrch across coding (SWE-bench), reasoning (GPQA), and retrieval-augmented generation tasks, demonstrating that topology-aware orchestration achieves 12-23% improvement over static single-topology baselines, even when using identical underlying models. Our results establish orchestration design as a first-class optimization target independent of model scaling.

</details>


### [47] [Safe Continuous-time Multi-Agent Reinforcement Learning via Epigraph Form](https://arxiv.org/abs/2602.17078)
*Xuefeng Wang,Lei Zhang,Henglin Pu,Husheng Li,Ahmed H. Qureshi*

Main category: cs.MA

TL;DR: 本文提出了一种连续时间约束多智能体强化学习(CT-CMDP)框架，利用物理信息神经网络(PINN)方法解决安全约束下的连续时间多智能体学习问题，提升了训练稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 当前大多数多智能体强化学习算法基于离散时间MDP，难以处理高频率或不规则时间间隔的复杂多智能体动态，且现有连续时间方法难以纳入碰撞等安全约束，导致性能下降。

Method: 通过将离散MDP重构为连续时间约束MDP(CT-CMDP)，采用物理信息神经网络(PINN)结合actor-critic方法，实现了对连续时间动态和安全约束问题的高效稳定优化。

Result: 在连续时间安全多粒子环境和安全多智能体MuJoCo基准测试中，所提方法实现了价值函数更平滑的近似、训练更稳定，并优于其他安全多智能体强化学习基线方法。

Conclusion: 本文所提的基于PINN的连续时间约束多智能体强化学习方法在安全多智能体环境中表现出更平滑的价值函数近似，更稳定的训练过程和更优的性能，验证了方法的有效性和鲁棒性。

Abstract: Multi-agent reinforcement learning (MARL) has made significant progress in recent years, but most algorithms still rely on a discrete-time Markov Decision Process (MDP) with fixed decision intervals. This formulation is often ill-suited for complex multi-agent dynamics, particularly in high-frequency or irregular time-interval settings, leading to degraded performance and motivating the development of continuous-time MARL (CT-MARL). Existing CT-MARL methods are mainly built on Hamilton-Jacobi-Bellman (HJB) equations. However, they rarely account for safety constraints such as collision penalties, since these introduce discontinuities that make HJB-based learning difficult. To address this challenge, we propose a continuous-time constrained MDP (CT-CMDP) formulation and a novel MARL framework that transforms discrete MDPs into CT-CMDPs via an epigraph-based reformulation. We then solve this by proposing a novel physics-informed neural network (PINN)-based actor-critic method that enables stable and efficient optimization in continuous time. We evaluate our approach on continuous-time safe multi-particle environments (MPE) and safe multi-agent MuJoCo benchmarks. Results demonstrate smoother value approximations, more stable training, and improved performance over safe MARL baselines, validating the effectiveness and robustness of our method.

</details>


### [48] [AgentConductor: Topology Evolution for Multi-Agent Competition-Level Code Generation](https://arxiv.org/abs/2602.17100)
*Siyu Wang,Ruotian Lu,Zhihao Yang,Yuchao Wang,Yanzhou Zhang,Lei Xu,Qimin Xu,Guojun Yin,Cailian Chen,Xinping Guan*

Main category: cs.MA

TL;DR: 提出一种基于强化学习和反馈驱动动态生成交互拓扑的多智能体系统AgentConductor，大幅提升代码生成任务的效率和准确率。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统在代码生成中未能根据任务难度调整拓扑密度，且缺乏利用执行反馈迭代优化拓扑，导致通信冗余和性能瓶颈。

Method: 设计并实现了AgentConductor，利用强化学习优化的多智能体系统，核心为基于大型语言模型的指挥者智能体，通过反馈驱动动态生成适应任务难度的多层有向无环图交互拓扑。

Result: 在三个竞赛级别和两个基础代码数据集上，AgentConductor在准确率上最高提升14.6%，拓扑密度降低13%，token成本降低68%。

Conclusion: AgentConductor通过对交互拓扑的动态调节与优化，有效减少冗余通信，提升多智能体系统的性能，在代码生成任务中达到当前最优表现。

Abstract: Large language model(LLM)-driven multi-agent systems(MAS) coordinate specialized agents through predefined interaction topologies and have shown promise for complex tasks such as competition-level code generation. Recent studies demonstrate that carefully designed multi-agent workflows and communication graphs can significantly improve code generation performance by leveraging collaborative reasoning. However, existing methods neither adapt topology density to task difficulty nor iteratively refine the topology within an instance using execution feedback, which leads to redundant communication and performance bottlenecks. To address these issues, we propose AgentConductor: a reinforcement learning-optimized MAS with an LLM-based orchestrator agent as its core, which enables end-to-end feedback-driven dynamic generation of interaction topologies. For each query, AgentConductor infers agent roles and task difficulty, then constructs a task-adapted, density-aware layered directed acyclic graph (DAG) topology, underpinned by two key innovations. First, we design a novel topological density function that captures communication-aware mathematical characterizations of multi-agent interactions. Second, we adopt difficulty interval partitioning to avoid excessive pruning for precise topological density upper bound measurement per difficulty level and finer-grained control. Empirically, across three competition-level and two foundational code datasets, AgentConductor achieves state-of-the-art accuracy, outperforming the strongest baseline by up to 14.6% in pass@1 accuracy, 13% in density reduction, and 68% in token cost reduction.

</details>


### [49] [Algorithmic Collusion at Test Time: A Meta-game Design and Evaluation](https://arxiv.org/abs/2602.17203)
*Yuhong Luo,Daniel Schoepflin,Xintong Wang*

Main category: cs.MA

TL;DR: 本文研究了算法勾结的风险，通过引入元游戏设计，在受限的测试阶段分析算法行为，探讨了在理性选择下算法是否会形成勾结，以及算法间如何协同或竞争。


<details>
  <summary>Details</summary>
Motivation: 现有关于算法勾结风险的研究多依赖于长期学习、多数假设对手理性及参数对称，本文旨在在更现实的测试时限制下，评估算法勾结的动力学和风险。

Method: 本文设计了一个元游戏框架，将算法视为拥有预训练策略的代理，结合初始策略与游戏内适应规则形成元策略，通过采样正规形式经验游戏、计算收益和遗憾值、构建最佳响应图，分析算法行为。实验包括在对称与非对称成本设置下，强化学习及基于大语言模型的策略在重复定价游戏中的表现。

Result: 研究证明算法可以在受限制的测试时间环境下形成勾结或竞争行为，提出的元游戏方法有效揭示了策略间的关系和勾结风险，实验证明定价策略在实际环境中具有不同的表现和潜在风险。

Conclusion: 研究发现算法勾结在理性选择下可能出现，且算法间会通过元策略和适应规则进行合作或竞争；实验展示了不同定价策略在实际测试环境中的有效性和勾结的可行性。

Abstract: The threat of algorithmic collusion, and whether it merits regulatory intervention, remains debated, as existing evaluations of its emergence often rely on long learning horizons, assumptions about counterparty rationality in adopting collusive strategies, and symmetry in hyperparameters and economic settings among players. To study collusion risk, we introduce a meta-game design for analyzing algorithmic behavior under test-time constraints. We model agents as possessing pretrained policies with distinct strategic characteristics (e.g., competitive, naively cooperative, robustly collusive), and formulate the problem as selecting a meta-strategy that combines a pretrained, initial policy with an in-game adaptation rule. We seek to examine whether collusion can emerge under rational choices and how agents co-adapt toward cooperation or competition. To this end, we sample normal-form empirical games over meta-strategy profiles, % across random initial game states, compute relevant game statistics (e.g., payoffs against individuals and regret against an equilibrium mixture of opponents), and construct empirical best-response graphs to uncover strategic relationships. We evaluate both reinforcement-learning and LLM-based strategies in repeated pricing games under symmetric and asymmetric cost settings, and present findings on the feasibility of algorithmic collusion and the effectiveness of pricing strategies in practical ``test-time'' environments.
  The source code and the full paper with appendix are available at: https://github.com/chailab-rutgers/CollusionMetagame.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [50] [A Construction-Phase Digital Twin Framework for Quality Assurance and Decision Support in Civil Infrastructure Projects](https://arxiv.org/abs/2602.16748)
*Md Asiful Islam,Shanto Jouerder,Md Sabit As Sami,Afia Jahin Prema*

Main category: cs.SE

TL;DR: 研究提出了一个施工阶段的数字孪生框架，通过整合多源数据，实现构件级质量的动态管理，改善了传统质量保证的时效性和决策支持。


<details>
  <summary>Details</summary>
Motivation: 传统施工质量保证依赖晚期检测结果，导致干预延迟、返工风险增加及文档分散，亟需更及时有效的质量管理方法。

Method: 通过将检测记录、材料生产及布置数据、早期感测和预测强度模型链接至单个构件，实现质量状态的动态表示和决策支持。

Result: 该框架补充了现有的检测流程，提升了数据的追溯性和早期质量评估能力，促进了从文档驱动向数据驱动的质量管理转变。

Conclusion: 该研究提出的数字孪生框架有效支持了施工阶段的质量保证，实现了构件级的质量评估和基于准备度的决策。

Abstract: Quality assurance (QA) during construction often relies on inspection records and laboratory test results that become available days or weeks after work is completed. On large highway and bridge projects, this delay limits early intervention and increases the risk of rework, schedule impacts, and fragmented documentation. This study presents a construction-phase digital twin framework designed to support element-level QA and readiness-based decision making during active construction. The framework links inspection records, material production and placement data, early-age sensing, and predictive strength models to individual construction elements. By integrating these data streams, the system represents the evolving quality state of each element and supports structured release or hold decisions before standard-age test results are available. The approach does not replace established inspection and testing procedures. Instead, it supplements existing workflows by improving traceability and enabling earlier, data-informed quality assessments. Practical considerations related to data integration, contractual constraints, and implementation challenges are also discussed. The proposed framework provides a structured pathway for transitioning construction QA from delayed, document-driven review toward proactive, element-level decision support during construction.

</details>


### [51] [Hybrid-Gym: Training Coding Agents to Generalize Across Tasks](https://arxiv.org/abs/2602.16819)
*Yiqing Xie,Emmy Liu,Gaokai Zhang,Nachiket Kotalwar,Shubham Gandhi,Sathwik Acharya,Xingyao Wang,Carolyn Rose,Graham Neubig,Daniel Fried*

Main category: cs.SE

TL;DR: 本文通过设计合成任务训练编码代理，使其掌握跨任务的可迁移技能，从而提升多样的编程任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有基准仅关注解决单一GitHub问题，无法反映编码代理在多样复杂真实任务中的能力，亟需提升其泛化和多技能掌握水平。

Method: 通过分解编程轨迹挖掘共享技能，制定辅助训练任务设计原则，构建Hybrid-Gym合成任务训练环境，指导语言模型学习多种技能。

Result: 本文提出了一种新的训练环境Hybrid-Gym，通过设计一系列可扩展的合成任务（如函数定位和依赖搜索），训练编码代理掌握在多样复杂编程任务中共享的可迁移技能。实验表明，该方法在多种真实世界任务上显著提升了基础模型的性能。

Conclusion: Hybrid-Gym设计的合成训练任务有效提升了编码代理在复杂多样编程任务中的泛化能力，显著优于现有基线。

Abstract: When assessing the quality of coding agents, predominant benchmarks focus on solving single issues on GitHub, such as SWE-Bench. In contrast, in real use, these agents solve more various and complex tasks that involve other skills such as exploring codebases, testing software, and designing architecture. In this paper, we first characterize some transferable skills that are shared across diverse tasks by decomposing trajectories into fine-grained components, and derive a set of principles for designing auxiliary training tasks to teach language models these skills. Guided by these principles, we propose a training environment, Hybrid-Gym, consisting of a set of scalable synthetic tasks, such as function localization and dependency search. Experiments show that agents trained on our synthetic tasks effectively generalize to diverse real-world tasks that are not present in training, improving a base model by 25.4% absolute gain on SWE-Bench Verified, 7.9% on SWT-Bench Verified, and 5.1% on Commit-0 Lite. Hybrid-Gym also complements datasets built for the downstream tasks (e.g., improving SWE-Play by 4.9% on SWT-Bench Verified). Code available at: https://github.com/yiqingxyq/Hybrid-Gym.

</details>


### [52] [Exploring LLMs for User Story Extraction from Mockups](https://arxiv.org/abs/2602.16997)
*Diego Firmenich,Leandro Antonelli,Bruno Pazos,Fabricio Lozada,Leonardo Morales*

Main category: cs.SE

TL;DR: 本文结合大型语言模型和高保真原型，自动生成用户故事，以提升需求定义效率。


<details>
  <summary>Details</summary>
Motivation: 用户故事定义功能需求，高保真原型促进用户参与，而结合LLM自动生成用户故事能提升敏捷性与自动化水平。

Method: 通过案例研究，分析LLM从高保真原型自动提取用户故事，同时比较包含与不包含语言扩展词汇表（LEL）提示的效果。

Result: 结果表明，加入LEL显著提高了生成用户故事的准确性和适用性，增强了用户与开发者的沟通。

Conclusion: 结合LLM和高保真原型，特别是利用LEL词汇表，有效提升了用户故事的自动生成质量，推动了AI在需求工程中的应用。

Abstract: User stories are one of the most widely used artifacts in the software industry to define functional requirements. In parallel, the use of high-fidelity mockups facilitates end-user participation in defining their needs. In this work, we explore how combining these techniques with large language models (LLMs) enables agile and automated generation of user stories from mockups. To this end, we present a case study that analyzes the ability of LLMs to extract user stories from high-fidelity mockups, both with and without the inclusion of a glossary of the Language Extended Lexicon (LEL) in the prompts. Our results demonstrate that incorporating the LEL significantly enhances the accuracy and suitability of the generated user stories. This approach represents a step forward in the integration of AI into requirements engineering, with the potential to improve communication between users and developers.

</details>


### [53] [Not Only for Developers: Exploring Plugin Maintenance for Knowledge-Centric Communities](https://arxiv.org/abs/2602.17018)
*Giovanni Rosa,David Moreno-Lumbreras,Raula Gaikovina Kula*

Main category: cs.SE

TL;DR: 本文探讨了面向非开发者知识平台Obsidian的插件生态，利用代码挖掘和主题建模发现六大插件主题，揭示其健康维护和演进机制，推动非开发者社区插件生态可持续发展研究。


<details>
  <summary>Details</summary>
Motivation: 研究非纯开发者社区中插件生态的维护挑战与机制。

Method: 通过代码库挖掘和基于大语言模型的主题建模分析396个插件，结合Pull Request分析生态系统的演进。

Result: 识别出六个与知识管理和工具相关的插件主题，发现混合社区中的插件生态系统具有可识别的工程结构。

Conclusion: 非纯开发者社区的插件生态也能健康发展，具备持续维护和演进的能力，为未来健康可持续研究提供方向。

Abstract: The adoption of third-party libraries has become integral to modern software development, leading to large ecosystems such as PyPI, NPM, and Maven, where contributors typically share the technical expertise to sustain extensions. In communities that are not exclusively composed of developers, however, maintaining plugin ecosystems can present different challenges. In this early results paper, we study Obsidian, a knowledge--centric platform whose community is focused on writing, organization, and creativity--has built a substantial plugin ecosystem despite not being developer--centric. We investigate what kinds of plugins exist within this hybrid ecosystem and establish a foundation for understanding how they are maintained. Using repository mining and LLM-based topic modeling on a representative sample of 396 plugins, we identify six topics related to knowledge management and tooling, which is (i) dynamic editing and organization, (ii) interface and layouts, (iii) creative writing and productivity, (iv) knowledge sync solutions, (v) linking and script tools, and (vi) workflow enhancements tools. Furthermore, analysis of the Pull Requests from these plugins show that much software evolution has been performed on these ecosystem. These findings suggest that even in mixed communities, plugin ecosystems can develop recognizable engineering structures, motivating future work that highlight three different research directions with six research questions related to the health and sustainability of these non-developer ecosystems.

</details>


### [54] [Wink: Recovering from Misbehaviors in Coding Agents](https://arxiv.org/abs/2602.17037)
*Rahul Nanda,Chandra Maddila,Smriti Jha,Euna Mehnaz Khan,Matteo Paltenghi,Satish Chandra*

Main category: cs.SE

TL;DR: 本文针对自主编码代理常见失误，提出并部署了一个自动纠偏系统Wink，有效提升了代理的稳定性和开发效率。


<details>
  <summary>Details</summary>
Motivation: 自主编码代理在软件开发中逐渐普及，但常出现偏离用户指令、循环卡住、错误使用工具等多种失误，影响开发效率，且手动介入成本高。

Method: 提出了一个名为Wink的轻量级异步自我干预系统，通过观察自主编码代理的行为轨迹，针对性地提供纠正指导，使代理回归正轨。

Result: Wink系统在超过一万条真实代理轨迹上测试，成功解决了90%需要单次干预的失误。在实际生产环境的A/B测试中显著减少了工具调用失败、会话中的Token数量及工程师干预次数。

Conclusion: 通过设计和部署Wink系统，验证了自动恢复代理失误的可行性与有效性，为构建大规模、健壮的自主编码代理系统提供了宝贵经验。

Abstract: Autonomous coding agents, powered by large language models (LLMs), are increasingly being adopted in the software industry to automate complex engineering tasks. However, these agents are prone to a wide range of misbehaviors, such as deviating from the user's instructions, getting stuck in repetitive loops, or failing to use tools correctly. These failures disrupt the development workflow and often require resource-intensive manual intervention. In this paper, we present a system for automatically recovering from agentic misbehaviors at scale. We first introduce a taxonomy of misbehaviors grounded in an analysis of production traffic, identifying three primary categories: Specification Drift, Reasoning Problems, and Tool Call Failures, which we find occur in about 30% of all agent trajectories.
  To address these issues, we developed a lightweight, asynchronous self-intervention system named Wink. Wink observes agent trajectories and provides targeted course-correction guidance to nudge the agent back to a productive path. We evaluated our system on over 10,000 real world agent trajectories and found that it successfully resolves 90% of the misbehaviors that require a single intervention. Furthermore, a live A/B test in our production environment demonstrated that our system leads to a statistically significant reduction in Tool Call Failures, Tokens per Session and Engineer Interventions per Session. We present our experience designing and deploying this system, offering insights into the challenges of building resilient agentic systems at scale.

</details>


### [55] [What to Cut? Predicting Unnecessary Methods in Agentic Code Generation](https://arxiv.org/abs/2602.17091)
*Kan Watanabe,Tatsuya Shirai,Yutaro Kashiwa,Hajimu Iida*

Main category: cs.SE

TL;DR: 该研究开发了一种能预测代码审查中将被删除函数的模型，帮助审查者更高效地聚焦重要代码。


<details>
  <summary>Details</summary>
Motivation: AI自动生成代码虽加速开发，但产出大量后续需审查和删除的代码，审查负担转移至审查者，缺乏辅助工具为审查者预警不必要代码。

Method: 基于函数特征建立机器学习预测模型，利用历史PR审查中函数删除的数据进行训练和验证。

Result: 提出了一种预测模型，用于识别在代码审查阶段可能被删除的函数。该模型根据不同删除原因的函数特征进行区分，取得了87.1%的AUC表现。

Conclusion: 通过预测可能被删除的函数，可以有效帮助手动审查，减少无效代码检查的时间，提高代码审查效率。

Abstract: Agentic Coding, powered by autonomous agents such as GitHub Copilot and Cursor, enables developers to generate code, tests, and pull requests from natural language instructions alone. While this accelerates implementation, it produces larger volumes of code per pull request, shifting the burden from implementers to reviewers. In practice, a notable portion of AI-generated code is eventually deleted during review, yet reviewers must still examine such code before deciding to remove it. No prior work has explored methods to help reviewers efficiently identify code that will be removed.In this paper, we propose a prediction model that identifies functions likely to be deleted during PR review. Our results show that functions deleted for different reasons exhibit distinct characteristics, and our model achieves an AUC of 87.1%. These findings suggest that predictive approaches can help reviewers prioritize their efforts on essential code.

</details>


### [56] [Multi-Ecosystem Modeling of OSS Project Sustainability](https://arxiv.org/abs/2602.17112)
*Arjun Ashok,Nafiz Imtiaz Khan,Swati Singhvi,Stefan Stanciulescu,Zhouhao Wang,Vladimir Filkov*

Main category: cs.SE

TL;DR: 本文通过分析多个基金会及GitHub项目的社会技术数据，建立了有效预测软件项目可持续性的模型，促进了项目和基金会匹配与长期发展。


<details>
  <summary>Details</summary>
Motivation: OSS项目加入不同基金会以获得治理支持和社区建设，但各基金会差异大，项目需求多样，如何匹配合适基金会并制定个性化可持续发展计划具有挑战性，促使进行基于社会技术数据的可持续性预测和分析。

Method: 通过收集和量化分析Apache、Eclipse、OSGeo基金会内和GitHub非基金会OSS项目的社会技术痕迹，构建基金会特定的可持续性预测模型和项目分诊体系，并结合案例分析验证模型效果。

Result: 本文通过对Apache、Eclipse、OSGeo基金会内孵化器项目及GitHub上非基金会OSS项目的实证和量化分析，构建了基金会特定的可持续性模型及项目分诊系统，基于项目社会技术痕迹特征有效预测项目可持续性结果。模型不仅适用于各基金会内部，也可跨基金会甚至应用于非基金会GitHub项目。结合已有恢复策略，对失败项目进行了案例研究，凸显社会技术框架在软件项目可持续性研究中的价值。

Conclusion: 社会技术框架模型和项目分诊方法能够有效预测并提高软件项目的可持续性，且具备跨基金会和非基金会项目的通用性，支持制定有效的项目恢复策略。

Abstract: Many OSS projects join foundations such as Apache, Eclipse, and OSGeo, to aid their immediate plans and improve long-term prospects by getting governance advice, incubation support, and community-building mechanisms. But foundations differ in their policies, funding models, and support strategies. Moreover, since projects joining these foundations are diverse, coming at different lifecycle stages and having different needs, it can be challenging to decide on the appropriate project-foundation match and on the project-specific plan for sustainability.
  Here, we present an empirical study and quantitative analysis of the sustainability of incubator projects in the Apache, Eclipse, and OSGeo foundations, and, additionally, of OSS projects from GitHub outside of foundations. We develop foundation-specific sustainability models and a project triage, based on projects' sociotechnical trace profiles, and demonstrate their effectiveness across the foundations. Our results show that our models with triage can effectively forecast sustainability outcomes not only within but across foundations. In addition, the generalizability of the framework allows us to apply the approach to GitHub projects outside the foundations. We complement our findings with actionable recovery strategies from previous work and apply them to case studies of failed incubator projects. Our study highlights the value of sociotechnical frameworks in characterizing and addressing software project sustainability issues.

</details>


### [57] [Quantifying Competitive Relationships Among Open-Source Software Projects](https://arxiv.org/abs/2602.17131)
*Yuki Takei,Toshiaki Aoki,Chaiyong Ragkhitwetsagul*

Main category: cs.SE

TL;DR: 本研究提出MIAO方法，通过经济学模型分析开源软件项目间的竞争，成功预测项目停止开发，助力项目维护与生态健康管理。


<details>
  <summary>Details</summary>
Motivation: 开源软件项目在激烈的竞争中存在生存风险，竞争关系对项目存续的影响尚不清楚。

Method: 提出了一种名为'Mutual Impact Analysis of OSS (MIAO)'的新自动化方法，采用结构向量自回归模型和冲击响应函数，分析开源软件项目间的竞争关系。

Result: 通过对187个开源软件项目组的实证研究，MIAO方法以81%的准确率识别出因竞争影响而停止开发的项目，预测项目停止时间有77%的准确率。

Conclusion: MIAO方法可以有效量化开源软件项目的竞争关系，帮助项目维护者理解生态系统动态并预测项目的兴衰。

Abstract: Throughout the history of software, evolution has occurred in cycles of rise and fall driven by competition, and open-source software (OSS) is no exception. This cycle is accelerating, particularly in rapidly evolving domains such as web development and deep learning. However, the impact of competitive relationships among OSS projects on their survival remains unclear, and there are risks of losing a competitive edge to rivals. To address this, this study proposes a new automated method called ``Mutual Impact Analysis of OSS (MIAO)'' to quantify these competitive relationships. The proposed method employs a structural vector autoregressive model and impulse response functions, normally used in macroeconomic analysis, to analyze the interactions among OSS projects. In an empirical analysis involving mining and analyzing 187 OSS project groups, MIAO identified projects that were forced to cease development owing to competitive influences with up to 81\% accuracy, and the resulting features supported predictive experiments that anticipate cessation one year ahead with up to 77\% accuracy. This suggests that MIAO could be a valuable tool for OSS project maintainers to understand the dynamics of OSS ecosystems and predict the rise and fall of OSS projects.

</details>


### [58] [Robustness and Reasoning Fidelity of Large Language Models in Long-Context Code Question Answering](https://arxiv.org/abs/2602.17183)
*Kishan Maharaj,Nandakishore Menon,Ashita Saxena,Srikanth Tamilselvam*

Main category: cs.SE

TL;DR: 本文评估了大型语言模型在长代码上下文中的鲁棒性，发现模型在答案格式变化和干扰信息下表现不稳定，提出了更全面的评测基准。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型日益辅助需要长代码上下文推理的软件工程任务，探索其在不同输入条件下的鲁棒性和稳定性变得重要。现有评测缺乏系统性，难以反映实际应用中的复杂场景。

Method: 通过控制消融实验，改变答案格式（多项选择顺序打乱、开放式问答）和上下文内容（引入无关或对抗性信息），在扩展的LongCodeBench数据集（包含Python、COBOL、Java）上评估模型表现。

Result: 本文系统研究了大型语言模型在处理长代码上下文中的鲁棒性，通过对答案格式、干扰项和上下文规模的控制消融试验，评估了模型对不同输入条件的敏感性。扩展了LongCodeBench Python数据集，引入COBOL和Java问答集，在三个设置下评估了先进模型：(i) 多项选择答案选项打乱，(ii) 开放式问答，(iii) 夹杂相关和对抗性无关信息的复杂上下文。结果显示模型在选项打乱和开放式问答中性能显著下降，在存在无关提示时表现脆弱。研究结果揭示了现有长上下文评估的局限，提供了一个更广泛的基准用于评估代码推理能力。

Conclusion: 当前大型语言模型在长代码上下文下对答案格式和干扰信息敏感，性能表现不稳定，需改进模型鲁棒性和评测方法。

Abstract: Large language models (LLMs) increasingly assist software engineering tasks that require reasoning over long code contexts, yet their robustness under varying input conditions remains unclear. We conduct a systematic study of long-context code question answering using controlled ablations that test sensitivity to answer format, distractors, and context scale. Extending LongCodeBench Python dataset with new COBOL and Java question-answer sets, we evaluate state-of-the-art models under three settings: (i) shuffled multiple-choice options, (ii) open-ended questions and (iii) needle-in-a-haystack contexts containing relevant and adversarially irrelevant information. Results show substantial performance drops in both shuffled multiple-choice options and open-ended questions, and brittle behavior in the presence of irrelevant cues. Our findings highlight limitations of current long-context evaluations and provide a broader benchmark for assessing code reasoning in both legacy and modern systems.

</details>


### [59] [The Case for HTML First Web Development](https://arxiv.org/abs/2602.17193)
*Juho Vepsäläinen*

Main category: cs.SE

TL;DR: HTML First开发方法强调优先使用HTML，结合服务器端逻辑和超媒体，简化代码库，提高维护和性能。


<details>
  <summary>Details</summary>
Motivation: 随着网页应用框架的发展，HTML被边缘化。HTML First旨在重新强调HTML在网页开发中的核心地位，推动简约高效的开发方式。

Method: 基于htmx项目，采用Holotype比较法对内容导向型网站进行性能评估，并通过Yle网站案例实践验证HTML First原则的效果。

Result: 采用HTML First后，代码库显著缩小，维护和开发效率提升，内容导向网站性能得到改善，案例研究支持了以上观点。

Conclusion: HTML First方法显著减少代码规模，提升维护性和性能，在内容导向型网站表现良好，但其与AI驱动开发趋势的契合度尚待探讨。

Abstract: Since its introduction in the early 90s, the web has become the largest application platform available globally. HyperText Markup Language (HTML) has been an essential part of the web since the beginning, as it allows defining webpages in a tree-like manner, including semantics and content. Although the web was never meant to be an application platform, it evolved as such, especially since the early 2000s, as web application frameworks became available. While the emergence of frameworks made it easier than ever to develop complex applications, it also put HTML on the back burner. As web standards caught up, especially with milestones such as HTML5, the gap between the web platform and frameworks was reduced. HTML First development emphasizes this shift and puts focus on literally using HTML first when possible, while encouraging minimalism familiar from the early days of the web. It seems HTML-oriented web development can provide clear benefits to developers, especially when it is combined with comple- mentary approaches, such as embracing hypermedia and moving a large part of application logic to the server side. In the context of the htmx project, it was observed that moving towards HTML can reduce the size of a codebase greatly while leading to maintenance and development benefits due to the increased conceptual simplicity. Holotype-based comparisons for content-oriented websites show performance benefits, and the same observation was confirmed by a small case study where the Yle website was converted to follow HTML First principles. In short, the HTML First approach seems to have clear advantages for web developers, while there are open questions related to the magnitude of the benefits and the alignment with the recent trend of AI-driven web development.

</details>


### [60] [Disjunction Composition of BDD Transition Systems for Model-Based Testing](https://arxiv.org/abs/2602.17237)
*Tannaz Zameni,Petra van den Bos,Arend Rensink*

Main category: cs.SE

TL;DR: 本文提出了一个基于BDD的模型测试合成方法，通过析取组合合并转移系统，保证测试能力不减弱，并在工业案例中验证有效。


<details>
  <summary>Details</summary>
Motivation: BDD作为敏捷方法，通过文本场景描述系统行为，而如何有效集成与测试这些备选行为需要新的方法支持。

Method: 将BDD中通过文本场景描述的系统行为转换为转移系统，定义了析取组合操作来合并代表备选系统行为的转移系统，利用符号语义证明合成后系统保留了原有测试能力。

Result: 提出了析取组合的形式定义，并通过符号语义论证了该组合的测试等价性，最后通过工业案例展示了方法的应用潜力。

Conclusion: 本文提出了一个合成方法，通过析取组合实现BDD转移系统的集成测试，保证了测试能力不下降，并在工业案例中验证了其有效性。

Abstract: We introduce a compositional approach to model-based test generation in Behavior-Driven Development (BDD). BDD is an agile methodology in which system behavior is specified through textual scenarios that, in our approach, are translated into transition systems used for model-based testing. This paper formally defines disjunction composition, to combine BDD transition systems that represent alternative system behaviors. Disjunction composition allows for modeling and testing the integrated behavior while ensuring that the testing power of the original set of scenarios is preserved. This is proved using a symbolic semantics for BDD transition systems, with the property that the symbolic equivalence of two BDD transition systems guarantees that they fail the same test cases. Also, we demonstrate the potential of disjunction composition by applying the composition in an industrial case study.

</details>


### [61] [Socio-Technical Well-Being of Quantum Software Communities: An Overview on Community Smells](https://arxiv.org/abs/2602.17320)
*Stefano Lambiase,Manuel De Stefano,Fabio Palomba,Filomena Ferrucci,Andrea De Lucia*

Main category: cs.SE

TL;DR: 量子开源社区存在社会技术反模式，可能损害项目质量和社区可持续性，此研究为解决这些问题提供基础分析。


<details>
  <summary>Details</summary>
Motivation: 量子计算因其超越传统计算机的潜力受到关注，但量子开源社区面临影响产品质量和社区健康的社会技术挑战，尤其是社区气味现象研究不足。

Method: 通过跨学科研究对量子开源社区的社会技术动态进行横断面分析。

Result: 通过跨学科研究分析了量子开源社区中的社会技术健康状况，揭示了社区气味对技术债务和架构、代码气味的负面影响。

Conclusion: 理解并应对量子开源社区中的社区气味对维护项目长期可持续性至关重要。

Abstract: Quantum computing has gained significant attention due to its potential to solve computational problems beyond the capabilities of classical computers. With major corporations and academic institutions investing in quantum hardware and software, there has been a rise in the development of quantum-enabled systems, particularly within open-source communities. However, despite the promising nature of quantum technologies, these communities face critical socio-technical challenges, including the emergence of socio-technical anti-patterns known as community smells. These anti-patterns, prevalent in open-source environments, have the potential to negatively impact both product quality and community health by introducing technical debt and amplifying architectural and code smells. Despite the importance of these socio-technical factors, there remains a scarcity of research investigating their influence within quantum open-source communities. This work aims to address this gap by providing a first step in analyzing the socio-technical well-being of quantum communities through a cross-sectional study. By understanding the socio-technical dynamics at play, it is expected that foundational knowledge can be established to mitigate the risks associated with community smells and ensure the long-term sustainability of open-source quantum initiatives.

</details>


### [62] [Computer-Using World Model](https://arxiv.org/abs/2602.17365)
*Yiming Guan,Rui Yu,John Zhang,Lu Wang,Chaoyun Zhang,Liqun Li,Bo Qiao,Si Qin,He Huang,Fangkai Yang,Pu Zhao,Lukas Wutschitz,Samuel Kessler,Huseyin A Inan,Robert Sim,Saravan Rajmohan,Qingwei Lin,Dongmei Zhang*

Main category: cs.SE

TL;DR: 本文针对复杂桌面软件环境中的智能代理操作，提出了一种双阶段的界面状态预测模型CUWM，通过文本描述和视觉合成预测界面变化，显著提升了决策质量和执行稳定性。


<details>
  <summary>Details</summary>
Motivation: 由于计算机使用环境中真实执行无法支持反事实探索，导致传统试错学习和规划难以进行，且单一错误的用户界面操作可能导致整个工作流程失败，因此需要建立能够预测界面状态变化的模型以支持智能代理的决策。

Method: 提出了计算机使用世界模型（CUWM），通过两阶段分解UI动态：第一阶段预测与代理相关的状态变化文本描述，第二阶段根据文本变化合成下一帧截图。CUWM基于微软Office应用中离线收集的UI状态转变训练，并通过轻量级强化学习进一步优化文本预测与环境结构的匹配。

Result: CUWM在测试阶段通过模拟和比较候选操作，改进了Office任务中的决策质量和执行的鲁棒性。

Conclusion: CUWM有效提升了复杂桌面软件环境中智能代理的操作决策能力，能够通过预测界面状态变化辅助优化用户操作。

Abstract: Agents operating in complex software environments benefit from reasoning about the consequences of their actions, as even a single incorrect user interface (UI) operation can derail long, artifact-preserving workflows. This challenge is particularly acute for computer-using scenarios, where real execution does not support counterfactual exploration, making large-scale trial-and-error learning and planning impractical despite the environment being fully digital and deterministic. We introduce the Computer-Using World Model (CUWM), a world model for desktop software that predicts the next UI state given the current state and a candidate action. CUWM adopts a two-stage factorization of UI dynamics: it first predicts a textual description of agent-relevant state changes, and then realizes these changes visually to synthesize the next screenshot. CUWM is trained on offline UI transitions collected from agents interacting with real Microsoft Office applications, and further refined with a lightweight reinforcement learning stage that aligns textual transition predictions with the structural requirements of computer-using environments. We evaluate CUWM via test-time action search, where a frozen agent uses the world model to simulate and compare candidate actions before execution. Across a range of Office tasks, world-model-guided test-time scaling improves decision quality and execution robustness.

</details>


### [63] [The Runtime Dimension of Ethics in Self-Adaptive Systems](https://arxiv.org/abs/2602.17426)
*Marco Autili,Gianluca Filippone,Mashal Afzal Memon,Patrizio Pelliccione*

Main category: cs.SE

TL;DR: 本文认为自适应系统需动态处理伦理偏好，通过运行时伦理推理和协商满足多方伦理需求，提升系统伦理自适应能力。


<details>
  <summary>Details</summary>
Motivation: 当前自适应系统与人类互动日益密切，涉及伦理决策，但现有方法多为固定规则或单一伦理理论，忽视了伦理偏好随个体、情境变化且可能冲突的复杂性。

Method: 提出将伦理偏好作为运行时需求进行动态推理与不断修正，并主张通过伦理协商处理多方伦理权衡。

Result: 识别了伦理不确定性、伦理价值冲突及多维多方协商的关键挑战，提出了构建伦理自适应系统的研究方向和问题。

Conclusion: 自适应系统应从静态伦理规则转向动态伦理推理，通过运行时伦理协商实现多主体间伦理权衡，适应不断变化的伦理需求。

Abstract: Self-adaptive systems increasingly operate in close interaction with humans, often sharing the same physical or virtual environments and making decisions with ethical implications at runtime. Current approaches typically encode ethics as fixed, rule-based constraints or as a single chosen ethical theory embedded at design time. This overlooks a fundamental property of human-system interaction settings: ethical preferences vary across individuals and groups, evolve with context, and may conflict, while still needing to remain within a legally and regulatorily defined hard-ethics envelope (e.g., safety and compliance constraints). This paper advocates a shift from static ethical rules to runtime ethical reasoning for self-adaptive systems, where ethical preferences are treated as runtime requirements that must be elicited, represented, and continuously revised as stakeholders and situations change. We argue that satisfying such requirements demands explicit ethics-based negotiation to manage ethical trade-offs among multiple humans who interact with, are represented by, or are affected by a system. We identify key challenges, ethical uncertainty, conflicts among ethical values (including human, societal, and environmental drivers), and multi-dimensional/multi-party/multi-driver negotiation, and outline research directions and questions toward ethically self-adaptive systems.

</details>


### [64] [Towards a Software Reference Architecture for Natural Language Processing Tools in Requirements Engineering](https://arxiv.org/abs/2602.17498)
*Julian Frattini,Quim Motger*

Main category: cs.SE

TL;DR: 本文提出了从单一的NLP4RE工具向可复用、互操作模块生态系统转变的愿景，并制定了实现该愿景的软件参考架构研究路线图。


<details>
  <summary>Details</summary>
Motivation: 现有NLP4RE工具功能重叠且缺乏互操作性和维护性，导致资源浪费、工具比较困难及可持续性差。

Method: 通过利益相关者主导的焦点小组讨论，收集了36条NLP4RE工具的通用系统需求，并依此制定软件参考架构的研究路线图。

Result: 确定了36条关键系统需求，证明了构建专用软件参考架构的必要性，为实现生态系统愿景奠定基础。

Conclusion: 提出的软件参考架构和研究路线图有助于提升NLP4RE工具的开发效率、复用性和长期维护性。

Abstract: Natural Language Processing (NLP) tools support requirements engineering (RE) tasks like requirements elicitation, classification, and validation. However, they are often developed from scratch despite functional overlaps, and abandoned after publication. This lack of interoperability and maintenance incurs unnecessary development effort, impedes tool comparison and benchmarking, complicates documentation, and diminishes the long-term sustainability of NLP4RE tools. To address these issues, we postulate a vision to transition from monolithic NLP4RE tools to an ecosystem of reusable, interoperable modules. We outline a research roadmap towards a software reference architecture (SRA) to realize this vision, elaborated following a standard methodological framework for SRA development. As an initial step, we conducted a stakeholder-driven focus group session to elicit generic system requirements for NLP4RE tools. This activity resulted in 36 key system requirements, further motivating the need for a dedicated SRA. Overall, the proposed vision, roadmap, and initial contribution pave the way towards improved development, reuse, and long-term maintenance of NLP4RE tools.

</details>
