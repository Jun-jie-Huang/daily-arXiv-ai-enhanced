<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 30]
- [cs.SE](#cs.SE) [Total: 17]
- [cs.MA](#cs.MA) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Teaching People LLM's Errors and Getting it Right](https://arxiv.org/abs/2512.21422)
*Nathan Stringham,Fateme Hashemi Chaleshtori,Xinyuan Yan,Zhichao Xu,Bei Wang,Ana Marasović*

Main category: cs.CL

TL;DR: 本文分析了大语言模型(LLM)的失误模式及用户对其过度依赖问题，发现失误模式确实存在，但现有方法未能有效揭示这些模式，导致教学效果欠佳。提出新指标评价教学效果，用户研究显示通过该指标教学能提升用户预判LLM失误的能力。


<details>
  <summary>Details</summary>
Motivation: 用户常错误地认为LLM不会在简单任务出错，导致过度依赖。此前通过聚类失败实例并描述模式来教育用户，但效果不佳，作者旨在探究原因。

Method: 作者分析两数据集中带有元标签的实例，确认存在LLM的失误模式。测试基于提示和嵌入的失败检测方法效果，并提出新教学效果评估指标，进行用户研究验证。

Result: 发现存在可教给用户的失误模式，传统方法在定位这些模式上效果不一。提出的新指标能更好反映教学效果，用户研究显示教学提升用户识别LLM错误的能力。

Conclusion: 失误模式教学有望减少用户对LLM的过度依赖，但需改进自动化失误发现技术和采用合理评估指标来提升教学效果。

Abstract: People use large language models (LLMs) when they should not. This is partly because they see LLMs compose poems and answer intricate questions, so they understandably, but incorrectly, assume LLMs won't stumble on basic tasks like simple arithmetic. Prior work has tried to address this by clustering instance embeddings into regions where an LLM is likely to fail and automatically describing patterns in these regions. The found failure patterns are taught to users to mitigate their overreliance. Yet, this approach has not fully succeeded. In this analysis paper, we aim to understand why.
  We first examine whether the negative result stems from the absence of failure patterns. We group instances in two datasets by their meta-labels and evaluate an LLM's predictions on these groups. We then define criteria to flag groups that are sizable and where the LLM is error-prone, and find meta-label groups that meet these criteria. Their meta-labels are the LLM's failure patterns that could be taught to users, so they do exist. We next test whether prompting and embedding-based approaches can surface these known failures. Without this, users cannot be taught about them to reduce their overreliance. We find mixed results across methods, which could explain the negative result. Finally, we revisit the final metric that measures teaching effectiveness. We propose to assess a user's ability to effectively use the given failure patterns to anticipate when an LLM is error-prone. A user study shows a positive effect from teaching with this metric, unlike the human-AI team accuracy. Our findings show that teaching failure patterns could be a viable approach to mitigating overreliance, but success depends on better automated failure-discovery methods and using metrics like ours.

</details>


### [2] [Morality is Contextual: Learning Interpretable Moral Contexts from Human Data with Probabilistic Clustering and Large Language Models](https://arxiv.org/abs/2512.21439)
*Geoffroy Morlat,Marceau Nahon,Augustin Chartouny,Raja Chatila,Ismael T. Freire,Mehdi Khamassi*

Main category: cs.CL

TL;DR: 本文提出了COMETH框架，通过结合大语言模型和人类道德评判，建模情境对模糊行为道德接受度的影响。


<details>
  <summary>Details</summary>
Motivation: 道德行为的评判不仅取决于结果，还依赖于发生的具体情境，而现有模型难以充分捕捉情境对道德评判的作用。

Method: 构建包含300个情境和101人三元判断的数据集，利用大语言模型预处理并聚类核心行为，在线学习行动特定的道德情境，通过提取非评判性的二元情境特征及透明的似然模型进行通用化和解释。

Result: COMETH在对人类多数判断的符合度上大约是端到端大语言模型提示的两倍（约60%对比约30%），并能揭示影响预测的关键情境特征。

Conclusion: COMETH提供了一个结合人类判断与模型学习的、可解释且情境敏感的道德评判框架，优于传统端到端大语言模型。

Abstract: Moral actions are judged not only by their outcomes but by the context in which they occur. We present COMETH (Contextual Organization of Moral Evaluation from Textual Human inputs), a framework that integrates a probabilistic context learner with LLM-based semantic abstraction and human moral evaluations to model how context shapes the acceptability of ambiguous actions. We curate an empirically grounded dataset of 300 scenarios across six core actions (violating Do not kill, Do not deceive, and Do not break the law) and collect ternary judgments (Blame/Neutral/Support) from N=101 participants. A preprocessing pipeline standardizes actions via an LLM filter and MiniLM embeddings with K-means, producing robust, reproducible core-action clusters. COMETH then learns action-specific moral contexts by clustering scenarios online from human judgment distributions using principled divergence criteria. To generalize and explain predictions, a Generalization module extracts concise, non-evaluative binary contextual features and learns feature weights in a transparent likelihood-based model. Empirically, COMETH roughly doubles alignment with majority human judgments relative to end-to-end LLM prompting (approx. 60% vs. approx. 30% on average), while revealing which contextual features drive its predictions. The contributions are: (i) an empirically grounded moral-context dataset, (ii) a reproducible pipeline combining human judgments with model-based context learning and LLM semantics, and (iii) an interpretable alternative to end-to-end LLMs for context-sensitive moral prediction and explanation.

</details>


### [3] [Oogiri-Master: Benchmarking Humor Understanding via Oogiri](https://arxiv.org/abs/2512.21494)
*Soichiro Murakami,Hidetaka Kamigaito,Hiroya Takamura,Manabu Okumura*

Main category: cs.CL

TL;DR: 本文通过引入日本创意反应游戏Oogiri的基准和数据集，系统研究了大型语言模型（LLMs）如何理解幽默，并提出了评估幽默感的客观指标。


<details>
  <summary>Details</summary>
Motivation: 现有研究在评估幽默感时数据有限、评价存在偏见且缺乏客观指标，难以准确理解语言模型对幽默的把握。

Method: 构建了包含每个提示约100个多样化回应的Oogiri-Master基准和Oogiri-Corpus数据集，通过约100名独立评审者评分，分析语言特征与幽默感的关联，提出预测人类幽默判断的客观指标，基准测试多种大型语言模型和人类表现。

Result: 研究表明文本长度、歧义和不协调性解决是幽默感的重要语言因素；最先进模型在幽默理解上接近人类水平，并且通过增添洞察提示能够进一步提升模型表现。

Conclusion: 本文为幽默理解的评估和提升提供了科学的基准和方法，推动了大型语言模型在人类类创造性思维领域的进步。

Abstract: Humor is a salient testbed for human-like creative thinking in large language models (LLMs). We study humor using the Japanese creative response game Oogiri, in which participants produce witty responses to a given prompt, and ask the following research question: What makes such responses funny to humans? Previous work has offered only limited reliable means to answer this question. Existing datasets contain few candidate responses per prompt, expose popularity signals during ratings, and lack objective and comparable metrics for funniness. Thus, we introduce Oogiri-Master and Oogiri-Corpus, which are a benchmark and dataset designed to enable rigorous evaluation of humor understanding in LLMs. Each prompt is paired with approximately 100 diverse candidate responses, and funniness is rated independently by approximately 100 human judges without access to others' ratings, reducing popularity bias and enabling robust aggregation. Using Oogiri-Corpus, we conduct a quantitative analysis of the linguistic factors associated with funniness, such as text length, ambiguity, and incongruity resolution, and derive objective metrics for predicting human judgments. Subsequently, we benchmark a range of LLMs and human baselines in Oogiri-Master, demonstrating that state-of-the-art models approach human performance and that insight-augmented prompting improves the model performance. Our results provide a principled basis for evaluating and advancing humor understanding in LLMs.

</details>


### [4] [Beyond Heuristics: A Decision-Theoretic Framework for Agent Memory Management](https://arxiv.org/abs/2512.21567)
*Changzhi Sun,Xiangyu Chen,Jixiang Luo,Dell Zhang,Xuelong Li*

Main category: cs.CL

TL;DR: 外部记忆对大型语言模型系统至关重要，但目前的记忆管理多依赖于手工启发式方法，缺乏对长期和不确定性后果的理解。本文提出了基于决策理论的记忆管理框架DAM，通过评估记忆操作的长期效用和风险，实现了记忆管理的理性决策。


<details>
  <summary>Details</summary>
Motivation: 当前外部记忆管理依赖手工设计的启发式方法，缺乏对记忆决策长期影响和不确定性的认识，限制了系统的性能和个性化能力。

Method: 提出决策理论框架DAM，将记忆管理分解为即时信息访问和分层存储维护，通过价值函数和不确定性估计对候选操作进行评估，基于长期效用和风险制定决策策略。

Result: 构建了一个能够评估长期效用和风险的记忆管理架构，使记忆决策更加理性和有效，揭示了启发式方法的局限性。

Conclusion: 本文提出的DAM框架为记忆管理提供了理论基础，强调了决策理论视角的重要性，为未来的不确定性感知记忆系统研究奠定了基础。

Abstract: External memory is a key component of modern large language model (LLM) systems, enabling long-term interaction and personalization. Despite its importance, memory management is still largely driven by hand-designed heuristics, offering little insight into the long-term and uncertain consequences of memory decisions. In practice, choices about what to read or write shape future retrieval and downstream behavior in ways that are difficult to anticipate. We argue that memory management should be viewed as a sequential decision-making problem under uncertainty, where the utility of memory is delayed and dependent on future interactions. To this end, we propose DAM (Decision-theoretic Agent Memory), a decision-theoretic framework that decomposes memory management into immediate information access and hierarchical storage maintenance. Within this architecture, candidate operations are evaluated via value functions and uncertainty estimators, enabling an aggregate policy to arbitrate decisions based on estimated long-term utility and risk. Our contribution is not a new algorithm, but a principled reframing that clarifies the limitations of heuristic approaches and provides a foundation for future research on uncertainty-aware memory systems.

</details>


### [5] [A Unified Definition of Hallucination, Or: It's the World Model, Stupid](https://arxiv.org/abs/2512.21577)
*Emmy Liu,Varun Gangal,Chelsea Zou,Xiaoqi Huang,Michael Yu,Alex Chang,Zhuofu Tao,Sachin Kumar,Steven Y. Feng*

Main category: cs.CL

TL;DR: 本文通过历史和现代视角统一定义了语言模型中的幻觉现象，认为幻觉是内部世界建模不准确的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管大量研究尝试解决神经语言模型中的幻觉问题，但即使在最前沿的大型语言模型中，幻觉依然存在，本文旨在找到其根本原因并统一相关定义。

Method: 文中回顾并整合了文献中关于幻觉的多种定义，提出以内部世界模型不准确为核心的统一定义，并基于此设想了一系列基准测试，用以评估和提升语言模型的世界建模能力。

Result: 通过统一视角，明确了幻觉评估所依赖的“世界模型”及信息冲突策略，澄清了幻觉与其他错误的界限，并为比较基准和缓解策略提供了共同语言。

Conclusion: 该研究提出的统一定义有助于规范幻觉的评估标准和研究方向，推动设计严格的基准测试来提升语言模型内部世界建模的准确性，从而减少幻觉现象。

Abstract: Despite numerous attempts to solve the issue of hallucination since the inception of neural language models, it remains a problem in even frontier large language models today. Why is this the case? We walk through definitions of hallucination used in the literature from a historical perspective up to the current day, and fold them into a single definition of hallucination, wherein different prior definitions focus on different aspects of our definition. At its core, we argue that hallucination is simply inaccurate (internal) world modeling, in a form where it is observable to the user (e.g., stating a fact which contradicts a knowledge base, or producing a summary which contradicts a known source). By varying the reference world model as well as the knowledge conflict policy (e.g., knowledge base vs. in-context), we arrive at the different existing definitions of hallucination present in the literature.
  We argue that this unified view is useful because it forces evaluations to make clear their assumed "world" or source of truth, clarifies what should and should not be called hallucination (as opposed to planning or reward/incentive-related errors), and provides a common language to compare benchmarks and mitigation techniques. Building on this definition, we outline plans for a family of benchmarks in which hallucinations are defined as mismatches with synthetic but fully specified world models in different environments, and sketch out how these benchmarks can use such settings to stress-test and improve the world modeling components of language models.

</details>


### [6] [Gamayun's Path to Multilingual Mastery: Cost-Efficient Training of a 1.5B-Parameter LLM](https://arxiv.org/abs/2512.21580)
*Alexander Podolskiy,Semen Molokov,Timofey Gerasin,Maksim Titov,Alexey Rukhovich,Artem Khrapov,Kirill Morozov,Evgeny Tetin,Constantine Korikov,Pavel Efimov,Polina Lazukova,Yuliya Skripkar,Nikita Okhotnikov,Irina Piontkovskaya,Meng Xiaojun,Zou Xueyi,Zhang Zhenhe*

Main category: cs.CL

TL;DR: Gamayun是一个1.5B参数的多语言语言模型，采用全新两阶段预训练策略，专注于资源受限环境，支持12种语言并在多项基准测试中超越同规模及更大规模模型，尤其在俄语任务表现出色。


<details>
  <summary>Details</summary>
Motivation: 针对小型非英语为中心的大型语言模型缺乏研究的问题，尤其是在资源受限环境中的应用需求，提出一种高效且表现优异的多语言模型。

Method: 采用一种新颖的两阶段预训练策略：首先进行平衡的多语言训练以实现跨语言对齐，然后进行高质量的英语强化训练以促进各语言的性能提升。

Result: 在训练数据量远小于同类模型的情况下，Gamayun在所有考察的基准测试中优于LLaMA3.2-1B及Qwen2.5-1.5B，并在大部分任务超越Qwen3，特别是在俄语任务（如MERA基准）上达到同规模模型的领先水平。

Conclusion: Gamayun通过创新预训练方法有效提升了多语言模型的性能，在保证模型规模适中的同时，显著优于多种主流大语言模型，展现了在资源受限环境下构建高效多语言模型的可行性和潜力。

Abstract: We present Gamayun, a 1.5B-parameter multilingual language model trained entirely from scratch on 2.5T tokens. Designed for efficiency and deployment in resource-constrained environments, Gamayun addresses the lack of research on small non-English-centric LLMs by adopting a novel two-stage pre-training strategy: balanced multilingual training for cross-lingual alignment, followed by high-quality English enrichment to transfer performance gains across languages. Our model supports 12 languages, with special focus on Russian. Despite a significantly smaller training budget than comparable models, Gamayun outperforms LLaMA3.2-1B (9T tokens) on all considered benchmarks, and surpasses Qwen2.5-1.5B (18T tokens) on a wide range of English and multilingual tasks. It matches or exceeds Qwen3 (36T tokens) on most tasks outside advanced STEM, achieving state-of-the-art results in Russian, including the MERA benchmark, among the models of comparable size (1-2B parameters).

</details>


### [7] [Rethinking Sample Polarity in Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2512.21625)
*Xinyu Tang,Yuliang Zhan,Zhixun Li,Wayne Xin Zhao,Zhenduo Zhang,Zujie Wen,Zhiqiang Zhang,Jun Zhou*

Main category: cs.CL

TL;DR: 本文研究了强化学习中不同极性样本如何影响大推理模型的训练，并提出了一种自适应非对称的标记级优势调整方法A3PO，提高了模型在五个推理基准上的表现。


<details>
  <summary>Details</summary>
Motivation: 大推理模型通过具有可验证奖励的强化学习训练以提升推理能力，不同极性（正负）样本在训练中扮演不同角色，但其具体影响尚不清楚。

Method: 系统分析了正负样本对训练动态的影响，发现正样本强化已有正确推理模式，负样本鼓励探索新推理路径。提出了A3PO方法，针对不同极性的关键标记自适应分配优势信号。

Result: A3PO在五个推理基准测试中表现出明显优势，验证了该方法在提升模型推理能力方面的有效性。

Conclusion: 通过精细调整正负样本的标记级优势，RLVR训练可以更有效地强化推理模型的性能，A3PO是一种有效的改进策略。

Abstract: Large reasoning models (LRMs) are typically trained using reinforcement learning with verifiable reward (RLVR) to enhance their reasoning abilities. In this paradigm, policies are updated using both positive and negative self-generated rollouts, which correspond to distinct sample polarities. In this paper, we provide a systematic investigation into how these sample polarities affect RLVR training dynamics and behaviors. We find that positive samples sharpen existing correct reasoning patterns, while negative samples encourage exploration of new reasoning paths. We further explore how adjusting the advantage values of positive and negative samples at both the sample level and the token level affects RLVR training. Based on these insights, we propose an Adaptive and Asymmetric token-level Advantage shaping method for Policy Optimization, namely A3PO, that more precisely allocates advantage signals to key tokens across different polarities. Experiments across five reasoning benchmarks demonstrate the effectiveness of our approach.

</details>


### [8] [Heaven-Sent or Hell-Bent? Benchmarking the Intelligence and Defectiveness of LLM Hallucinations](https://arxiv.org/abs/2512.21635)
*Chengxu Yang,Jingling Yuan,Siqi Cai,Jiawei Jiang,Chuang Hu*

Main category: cs.CL

TL;DR: 本文提出了HIC-Bench框架，用以分类和评估大语言模型中的幻觉，区分智能幻觉与缺陷幻觉，结合创造性与准确性指标，促进科学创新。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型幻觉多被视为错误，检测方法偏重事实一致性，难以兼顾不同科学任务中的创造性与准确性需求，缺乏对幻觉中有价值内容的定量分析。

Method: 提出HIC-Bench框架，利用多维度指标（包括创造力测试和幻觉特异指标）对智能幻觉与缺陷幻觉进行系统评估，涵盖十个科学领域的创新任务，采用动态提示优化技术引导模型产生可靠且具创造性的输出，多模型评分并辅以人工验证。

Result: 实验揭示了智能幻觉与缺陷幻觉之间的非线性关系，表明创造性与正确性可以同时优化，智能幻觉能够促进科学创新。

Conclusion: 智能幻觉不仅是错误，更是推动科学创新的催化剂，HIC-Bench为深入研究大语言模型幻觉中的创造智能提供了有价值的平台。

Abstract: Hallucinations in large language models (LLMs) are commonly regarded as errors to be minimized. However, recent perspectives suggest that some hallucinations may encode creative or epistemically valuable content, a dimension that remains underquantified in current literature. Existing hallucination detection methods primarily focus on factual consistency, struggling to handle heterogeneous scientific tasks and balance creativity with accuracy. To address these challenges, we propose HIC-Bench, a novel evaluation framework that categorizes hallucinations into Intelligent Hallucinations (IH) and Defective Hallucinations (DH), enabling systematic investigation of their interplay in LLM creativity. HIC-Bench features three core characteristics: (1) Structured IH/DH Assessment. using a multi-dimensional metric matrix integrating Torrance Tests of Creative Thinking (TTCT) metrics (Originality, Feasibility, Value) with hallucination-specific dimensions (scientific plausibility, factual deviation); (2) Cross-Domain Applicability. spanning ten scientific domains with open-ended innovation tasks; and (3) Dynamic Prompt Optimization. leveraging the Dynamic Hallucination Prompt (DHP) to guide models toward creative and reliable outputs. The evaluation process employs multiple LLM judges, averaging scores to mitigate bias, with human annotators verifying IH/DH classifications. Experimental results reveal a nonlinear relationship between IH and DH, demonstrating that creativity and correctness can be jointly optimized. These insights position IH as a catalyst for creativity and reveal the ability of LLM hallucinations to drive scientific innovation.Additionally, the HIC-Bench offers a valuable platform for advancing research into the creative intelligence of LLM hallucinations.

</details>


### [9] [Enabling Conversational Behavior Reasoning Capabilities in Full-Duplex Speech](https://arxiv.org/abs/2512.21706)
*Shuchang Pan,Siddharth Banerjee,Dhruv Hebbar,Siddhant Patel,Akshaj Gupta,Kan Jen Cheng,Hanjo Kim,Zeyi Austin Li,Martin Q. Ma,Tingle Li,Gopala Anumanchipalli,Jiachen Lian*

Main category: cs.CL

TL;DR: 本文提出了一种基于因果推理的Graph-of-Thoughts框架，通过分层标注和混合语料训练，实现了对双工对话中语音行为的推理和预测，提升了对话系统的自然交互能力。


<details>
  <summary>Details</summary>
Motivation: 捕捉人类对话中隐含的思维链条及其因果路径，对于构建自然、全双工交互系统至关重要。

Method: 通过Graph-of-Thoughts框架，将对话行为建模为因果推理，采用层级标注预测高层意图和低层言语行为，利用混合语料进行训练，并设计多模态变换器用于动态推理和预测下一言语行为。

Result: 在合成和真实双工对话实验中，该框架表现出稳健的行为检测能力，生成可解释的推理链，并为全双工语音对话系统的推理评测奠定基础。

Conclusion: 该框架有效建模了对话中的因果思维链，提高了系统的解释性和交互自然度，是全双工语音对话系统发展的重要方向。

Abstract: Human conversation is organized by an implicit chain of thoughts that manifests as timed speech acts. Capturing this causal pathway is key to building natural full-duplex interactive systems. We introduce a framework that enables reasoning over conversational behaviors by modeling this process as causal inference within a Graph-of-Thoughts (GoT). Our approach formalizes the intent-to-action pathway with a hierarchical labeling scheme, predicting high-level communicative intents and low-level speech acts to learn their causal and temporal dependencies. To train this system, we develop a hybrid corpus that pairs controllable, event-rich simulations with human-annotated rationales and real conversational speech. The GoT framework structures streaming predictions as an evolving graph, enabling a multimodal transformer to forecast the next speech act, generate concise justifications for its decisions, and dynamically refine its reasoning. Experiments on both synthetic and real duplex dialogues show that the framework delivers robust behavior detection, produces interpretable reasoning chains, and establishes a foundation for benchmarking conversational reasoning in full duplex spoken dialogue systems.

</details>


### [10] [MoRAgent: Parameter Efficient Agent Tuning with Mixture-of-Roles](https://arxiv.org/abs/2512.21708)
*Jing Han,Binwei Yan,Tianyu Guo,Zheyuan Bai,Mengyu Zheng,Hanting Chen,Ying Nie*

Main category: cs.CL

TL;DR: 本文提出了面向代理任务的参数高效微调方法，引入了角色分解和混合角色框架，通过多角色数据生成实现高效微调，实验证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在代理任务微调方面取得进展，但针对代理的参数高效微调方法仍未充分探索。

Method: 将代理任务能力分为推理者、执行者和总结者三角色，提出混合角色（MoR）框架，利用三个专门的低秩适配组实现角色协作，结合基于公开数据集的多角色数据生成管道进行微调。

Result: 在多种大语言模型和代理基准测试上进行了广泛实验和消融分析，结果表明所提方法在提升参数高效微调表现方面效果显著。

Conclusion: 提出的基于角色分解和混合角色的参数高效微调框架，为代理任务提供了一种有效且可推广的解决方案。

Abstract: Despite recent advancements of fine-tuning large language models (LLMs) to facilitate agent tasks, parameter-efficient fine-tuning (PEFT) methodologies for agent remain largely unexplored. In this paper, we introduce three key strategies for PEFT in agent tasks: 1) Inspired by the increasingly dominant Reason+Action paradigm, we first decompose the capabilities necessary for the agent tasks into three distinct roles: reasoner, executor, and summarizer. The reasoner is responsible for comprehending the user's query and determining the next role based on the execution trajectory. The executor is tasked with identifying the appropriate functions and parameters to invoke. The summarizer conveys the distilled information from conversations back to the user. 2) We then propose the Mixture-of-Roles (MoR) framework, which comprises three specialized Low-Rank Adaptation (LoRA) groups, each designated to fulfill a distinct role. By focusing on their respective specialized capabilities and engaging in collaborative interactions, these LoRAs collectively accomplish the agent task. 3) To effectively fine-tune the framework, we develop a multi-role data generation pipeline based on publicly available datasets, incorporating role-specific content completion and reliability verification. We conduct extensive experiments and thorough ablation studies on various LLMs and agent benchmarks, demonstrating the effectiveness of the proposed method. This project is publicly available at https://mor-agent.github.io.

</details>


### [11] [Detecting AI-Generated Paraphrases in Bengali: A Comparative Study of Zero-Shot and Fine-Tuned Transformers](https://arxiv.org/abs/2512.21709)
*Md. Rakibul Islam,Most. Sharmin Sultana Samu,Md. Zahid Hossain,Farhad Uz Zaman,Md. Kamrozzaman Bhuiyan*

Main category: cs.CL

TL;DR: 针对孟加拉语的AI生成文本检测，五种基于Transformer的模型在零样本状态下表现接近随机，经过微调后性能显著提升，最高准确率和F1分数达91%。


<details>
  <summary>Details</summary>
Motivation: 大语言模型能生成高度类似人类写作的文本，但孟加拉语在AI文本检测方面研究不足，且语言特点增加区分难度，需解决孟加拉语AI文本识别问题。

Method: 选用XLMRoBERTa-Large、mDeBERTaV3-Base、BanglaBERT-Base、IndicBERT-Base和MultilingualBERT-Base五种Transformer模型，分别进行零样本评估和任务特定微调。

Result: 零样本评估准确率约50%，微调后XLM-RoBERTa、mDeBERTa和MultilingualBERT表现优异，准确率和F1值约91%，IndicBERT微调效果较弱。

Conclusion: 建立了孟加拉语AI文本检测基准，提高了检测性能，为应对AI生成内容提供坚实基础。

Abstract: Large language models (LLMs) can produce text that closely resembles human writing. This capability raises concerns about misuse, including disinformation and content manipulation. Detecting AI-generated text is essential to maintain authenticity and prevent malicious applications. Existing research has addressed detection in multiple languages, but the Bengali language remains largely unexplored. Bengali's rich vocabulary and complex structure make distinguishing human-written and AI-generated text particularly challenging. This study investigates five transformer-based models: XLMRoBERTa-Large, mDeBERTaV3-Base, BanglaBERT-Base, IndicBERT-Base and MultilingualBERT-Base. Zero-shot evaluation shows that all models perform near chance levels (around 50% accuracy) and highlight the need for task-specific fine-tuning. Fine-tuning significantly improves performance, with XLM-RoBERTa, mDeBERTa and MultilingualBERT achieving around 91% on both accuracy and F1-score. IndicBERT demonstrates comparatively weaker performance, indicating limited effectiveness in fine-tuning for this task. This work advances AI-generated text detection in Bengali and establishes a foundation for building robust systems to counter AI-generated content.

</details>


### [12] [Do Latent Tokens Think? A Causal and Adversarial Analysis of Chain-of-Continuous-Thought](https://arxiv.org/abs/2512.21711)
*Yuyi Zhang,Boyu Tang,Tianjie Ju,Sufeng Duan,Gongshen Liu*

Main category: cs.CL

TL;DR: 本文研究了潜变量标记在大语言模型推理中的作用，发现其作为不可解释的占位符，虽稳定但易依赖捷径，缺乏真实推理能力。


<details>
  <summary>Details</summary>
Motivation: 潜变量标记尽管被用来提升大语言模型的推理能力，但其内部机制尚不清晰，尤其在可靠性方面存在疑问。

Method: 通过针对链式连续思维(COCONUT)和显式链式思维(CoT)标记进行干预试验和捷径测试，分析其对推理性能的影响。

Result: COCONUT标记对干预不敏感，缺少关键推理信息，且在偏向和分布外环境下依赖数据集捷径，导致性能虚高。

Conclusion: COCONUT标记更像是伪推理机制，虽生成合理的推理轨迹，但掩盖了对捷径的依赖，没有真实反映推理过程。

Abstract: Latent tokens are gaining attention for enhancing reasoning in large language models (LLMs), yet their internal mechanisms remain unclear. This paper examines the problem from a reliability perspective, uncovering fundamental weaknesses: latent tokens function as uninterpretable placeholders rather than encoding faithful reasoning. While resistant to perturbation, they promote shortcut usage over genuine reasoning. We focus on Chain-of-Continuous-Thought (COCONUT), which claims better efficiency and stability than explicit Chain-of-Thought (CoT) while maintaining performance. We investigate this through two complementary approaches. First, steering experiments perturb specific token subsets, namely COCONUT and explicit CoT. Unlike CoT tokens, COCONUT tokens show minimal sensitivity to steering and lack reasoning-critical information. Second, shortcut experiments evaluate models under biased and out-of-distribution settings. Results on MMLU and HotpotQA demonstrate that COCONUT consistently exploits dataset artifacts, inflating benchmark performance without true reasoning. These findings reposition COCONUT as a pseudo-reasoning mechanism: it generates plausible traces that conceal shortcut dependence rather than faithfully representing reasoning processes.

</details>


### [13] [CATCH: A Controllable Theme Detection Framework with Contextualized Clustering and Hierarchical Generation](https://arxiv.org/abs/2512.21715)
*Rui Ke,Jiahui Xu,Shenghao Yang,Kuang Wang,Feng Jiang,Haizhou Li*

Main category: cs.CL

TL;DR: 本文提出了CATCH框架，通过上下文感知表示、个性化主题聚类和层次化生成，有效提升用户对话系统中的主题检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统主题检测难以处理短且稀疏的用户话语，且无法捕捉跨对话的用户个性化主题偏好，导致话题表示和一致性较差。

Method: 提出了CATCH框架，包括：1）上下文感知的话题表示，用周围主题丰富话语语义；2）偏好引导的主题聚类，结合语义距离和个性化反馈；3）层次化主题生成机制，抑制噪声生成连贯主题标签。

Result: 在多领域客户对话基准（DSTC-12）上，结合8B大模型的CATCH在主题聚类和主题生成质量方面表现优异。

Conclusion: CATCH框架有效提升了主题检测的准确性和一致性，尤其在处理稀疏短话语及用户个性化主题需求上表现突出。

Abstract: Theme detection is a fundamental task in user-centric dialogue systems, aiming to identify the latent topic of each utterance without relying on predefined schemas. Unlike intent induction, which operates within fixed label spaces, theme detection requires cross-dialogue consistency and alignment with personalized user preferences, posing significant challenges. Existing methods often struggle with sparse, short utterances for accurate topic representation and fail to capture user-level thematic preferences across dialogues. To address these challenges, we propose CATCH (Controllable Theme Detection with Contextualized Clustering and Hierarchical Generation), a unified framework that integrates three core components: (1) context-aware topic representation, which enriches utterance-level semantics using surrounding topic segments; (2) preference-guided topic clustering, which jointly models semantic proximity and personalized feedback to align themes across dialogue; and (3) a hierarchical theme generation mechanism designed to suppress noise and produce robust, coherent topic labels. Experiments on a multi-domain customer dialogue benchmark (DSTC-12) demonstrate the effectiveness of CATCH with 8B LLM in both theme clustering and topic generation quality.

</details>


### [14] [Ara-HOPE: Human-Centric Post-Editing Evaluation for Dialectal Arabic to Modern Standard Arabic Translation](https://arxiv.org/abs/2512.21787)
*Abdullah Alabdullah,Lifeng Han,Chenghua Lin*

Main category: cs.CL

TL;DR: 本文介绍了Ara-HOPE，一种针对阿拉伯方言到现代标准阿拉伯语翻译的人工后编辑评价框架，以解决现有评估方法难以捕捉方言特定错误的问题。


<details>
  <summary>Details</summary>
Motivation: 由于阿拉伯方言和现代标准阿拉伯语在词汇、句法和语义上存在显著差异，现有自动评价指标和通用人工评估框架难以准确评估方言阿拉伯语翻译的质量，阻碍了翻译质量的提升。

Method: 提出了一个包含五类错误分类法和决策树标注协议的人工后编辑评价框架Ara-HOPE。通过对比评估三个机器翻译系统（阿拉伯中心的Jais、通用GPT-3.5和基线NLLB-200），系统地分析各系统的性能差异。

Result: Ara-HOPE有效揭示了三个系统在处理方言专用术语和语义保持方面的系统性差异，指出这些问题是方言到标准阿拉伯语翻译中最难克服的挑战。

Conclusion: Ara-HOPE为阿拉伯方言机器翻译质量评价建立了新的框架，并为改进具备方言感知能力的机器翻译系统提供了指导。

Abstract: Dialectal Arabic to Modern Standard Arabic (DA-MSA) translation is a challenging task in Machine Translation (MT) due to significant lexical, syntactic, and semantic divergences between Arabic dialects and MSA. Existing automatic evaluation metrics and general-purpose human evaluation frameworks struggle to capture dialect-specific MT errors, hindering progress in translation assessment. This paper introduces Ara-HOPE, a human-centric post-editing evaluation framework designed to systematically address these challenges. The framework includes a five-category error taxonomy and a decision-tree annotation protocol. Through comparative evaluation of three MT systems (Arabic-centric Jais, general-purpose GPT-3.5, and baseline NLLB-200), Ara-HOPE effectively highlights systematic performance differences between these systems. The results show that dialect-specific terminology and semantic preservation remain the most persistent challenges in DA-MSA translation. Ara-HOPE establishes a new framework for evaluating Dialectal Arabic MT quality and provides actionable guidance for improving dialect-aware MT systems.

</details>


### [15] [Five Years of SciCap: What We Learned and Future Directions for Scientific Figure Captioning](https://arxiv.org/abs/2512.21789)
*Ting-Hao K. Huang,Ryan A. Rossi,Sungchul Kim,Tong Yu,Ting-Yao E. Hsu,Ho Yin,Ng,C. Lee Giles*

Main category: cs.CL

TL;DR: SciCap项目从2021年小规模启动发展成学术界重要的科学图表注释项目，涵盖数据集构建、模型评估与挑战，推动领域进步。


<details>
  <summary>Details</summary>
Motivation: 探索领域专用训练方法是否适用于科学图表注释，并提升科学家图注写作质量。

Method: 构建并更新大规模图表-注释对数据集，结合自动和人工评测，利用大语言模型，举办年度挑战赛，开发交互写作工具。

Result: 五年内项目逐渐扩大影响，产出丰富数据与评测基准，推动多机构合作和技术进步，科学图注写作能力显著提升。

Conclusion: 总结技术与方法经验，指出科学图注领域五大未解挑战，规划未来研究方向，推动领域持续发展。

Abstract: Between 2021 and 2025, the SciCap project grew from a small seed-funded idea at The Pennsylvania State University (Penn State) into one of the central efforts shaping the scientific figure-captioning landscape. Supported by a Penn State seed grant, Adobe, and the Alfred P. Sloan Foundation, what began as our attempt to test whether domain-specific training, which was successful in text models like SciBERT, could also work for figure captions expanded into a multi-institution collaboration. Over these five years, we curated, released, and continually updated a large collection of figure-caption pairs from arXiv papers, conducted extensive automatic and human evaluations on both generated and author-written captions, navigated the rapid rise of large language models (LLMs), launched annual challenges, and built interactive systems that help scientists write better captions. In this piece, we look back at the first five years of SciCap and summarize the key technical and methodological lessons we learned. We then outline five major unsolved challenges and propose directions for the next phase of research in scientific figure captioning.

</details>


### [16] [On The Conceptualization and Societal Impact of Cross-Cultural Bias](https://arxiv.org/abs/2512.21809)
*Vitthal Bhandari*

Main category: cs.CL

TL;DR: 本文分析了2025年发表的20篇关于文化偏见的自然语言处理论文，提出了评价文化偏见的新方法。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型虽能基于文化背景生成回答，但存在跨文化泛化的问题，且评估文化偏见时缺乏与实际使用者的互动，导致偏见问题未被有效解决。

Method: 借鉴arXiv:2005.14050v2的研究，系统审视相关文献，通过分析20篇2025年的论文，总结出一套具体概念化文化偏见及其危害评估的方法框架。

Result: 提出一套观察和评价文化偏见的方法，帮助未来NLP研究者更具体地理解和评估跨文化偏见的社会影响。

Conclusion: 呼吁对语言技术中存在的跨文化偏见进行更有力的社会影响评估，以推动公正和有效的自然语言处理技术发展。

Abstract: Research has shown that while large language models (LLMs) can generate their responses based on cultural context, they are not perfect and tend to generalize across cultures. However, when evaluating the cultural bias of a language technology on any dataset, researchers may choose not to engage with stakeholders actually using that technology in real life, which evades the very fundamental problem they set out to address.
  Inspired by the work done by arXiv:2005.14050v2, I set out to analyse recent literature about identifying and evaluating cultural bias in Natural Language Processing (NLP). I picked out 20 papers published in 2025 about cultural bias and came up with a set of observations to allow NLP researchers in the future to conceptualize bias concretely and evaluate its harms effectively. My aim is to advocate for a robust assessment of the societal impact of language technologies exhibiting cross-cultural bias.

</details>


### [17] [Method Decoration (DeMe): A Framework for LLM-Driven Adaptive Method Generation in Dynamic IoT Environments](https://arxiv.org/abs/2512.21817)
*Hong Su*

Main category: cs.CL

TL;DR: 提出了Method Decoration (DeMe)框架，通过动态调整大语言模型生成的方法路径，使智能物联网系统在面对未知环境时能生成适应性更强的方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法系统地生成新方法，应对未知情况，且依赖固定、设备特定的逻辑，缺乏环境适应性。

Method: DeMe通过隐含目标、积累的经验和环境反馈，动态修饰方法生成路径，包括前置修饰、后置修饰、中间步骤修改和步骤插入。

Result: 实验表明，DeMe使物联网设备能在未知或故障状态下生成更合适的方法，提高了方法的环境适应性和安全性。

Conclusion: DeMe框架有效提升了大语言模型在动态环境中生成方法的能力，实现了上下文感知和环境适应，增强了物联网系统的智能性和鲁棒性。

Abstract: Intelligent IoT systems increasingly rely on large language models (LLMs) to generate task-execution methods for dynamic environments. However, existing approaches lack the ability to systematically produce new methods when facing previously unseen situations, and they often depend on fixed, device-specific logic that cannot adapt to changing environmental conditions.In this paper, we propose Method Decoration (DeMe), a general framework that modifies the method-generation path of an LLM using explicit decorations derived from hidden goals, accumulated learned methods, and environmental feedback. Unlike traditional rule augmentation, decorations in DeMe are not hardcoded; instead, they are extracted from universal behavioral principles, experience, and observed environmental differences. DeMe enables the agent to reshuffle the structure of its method path-through pre-decoration, post-decoration, intermediate-step modification, and step insertion-thereby producing context-aware, safety-aligned, and environment-adaptive methods. Experimental results show that method decoration allows IoT devices to derive ore appropriate methods when confronting unknown or faulty operating conditions.

</details>


### [18] [Knowledge Reasoning of Large Language Models Integrating Graph-Structured Information for Pest and Disease Control in Tobacco](https://arxiv.org/abs/2512.21837)
*Siyu Li,Chenwei Song,Wan Zhou,Xinyi Liu*

Main category: cs.CL

TL;DR: 本文提出了一种结合图结构信息的知识推理大型语言模型方法，提升烟草病虫害防控的知识检索与推理能力。


<details>
  <summary>Details</summary>
Motivation: 针对烟草病虫害防控领域知识复杂且关系紧密的问题，提升知识推理的准确性和深度。

Method: 基于GraphRAG框架，构建包含病害、症状、控制方法等关键实体及其关系的知识图谱；利用Transformer作为推理模型，结合图神经网络学习节点表示；采用ChatGLM大模型并通过LoRA微调实现参数高效适配。

Result: 实验表明该方法在多个评价指标上显著优于基线，尤其在复杂多跳和比较推理情境中表现更佳。

Conclusion: 所提方法有效融合领域知识图谱与大型语言模型，显著提升烟草病虫害防控知识推理的准确性和深度。

Abstract: This paper proposes a large language model (LLM) approach that integrates graph-structured information for knowledge reasoning in tobacco pest and disease control. Built upon the GraphRAG framework, the proposed method enhances knowledge retrieval and reasoning by explicitly incorporating structured information from a domain-specific knowledge graph. Specifically, LLMs are first leveraged to assist in the construction of a tobacco pest and disease knowledge graph, which organizes key entities such as diseases, symptoms, control methods, and their relationships. Based on this graph, relevant knowledge is retrieved and integrated into the reasoning process to support accurate answer generation. The Transformer architecture is adopted as the core inference model, while a graph neural network (GNN) is employed to learn expressive node representations that capture both local and global relational information within the knowledge graph. A ChatGLM-based model serves as the backbone LLM and is fine-tuned using LoRA to achieve parameter-efficient adaptation. Extensive experimental results demonstrate that the proposed approach consistently outperforms baseline methods across multiple evaluation metrics, significantly improving both the accuracy and depth of reasoning, particularly in complex multi-hop and comparative reasoning scenarios.

</details>


### [19] [AlignAR: Generative Sentence Alignment for Arabic-English Parallel Corpora of Legal and Literary Texts](https://arxiv.org/abs/2512.21842)
*Baorong Huang,Ali Asiri*

Main category: cs.CL

TL;DR: 本文提出了AlignAR，一种生成式句子对齐方法，并发布了一个包含复杂法律和文学文本的阿拉伯语-英语并行数据集。结果显示，大型语言模型方法在复杂对齐任务中表现优越。


<details>
  <summary>Details</summary>
Motivation: 高质量阿拉伯语-英语平行语料稀缺，现有数据集多为简单一对一映射，难以充分评估句子对齐方法。

Method: 提出了AlignAR生成式句子对齐方法，并构建包含复杂文本的“Hard”子集以挑战传统对齐方法。

Result: LLM方法在“Hard”子集表现出85.5%的F1分数，比传统方法高出9%，显示出更强鲁棒性。

Conclusion: 该研究展示了生成式对齐方法及复杂数据集对于推动阿拉伯语-英语句子对齐研究的重要价值，推动了该领域的发展。

Abstract: High-quality parallel corpora are essential for Machine Translation (MT) research and translation teaching. However, Arabic-English resources remain scarce and existing datasets mainly consist of simple one-to-one mappings. In this paper, we present AlignAR, a generative sentence alignment method, and a new Arabic-English dataset comprising complex legal and literary texts. Our evaluation demonstrates that "Easy" datasets lack the discriminatory power to fully assess alignment methods. By reducing one-to-one mappings in our "Hard" subset, we exposed the limitations of traditional alignment methods. In contrast, LLM-based approaches demonstrated superior robustness, achieving an overall F1-score of 85.5%, a 9% improvement over previous methods. Our datasets and codes are open-sourced at https://github.com/XXX.

</details>


### [20] [HeartBench: Probing Core Dimensions of Anthropomorphic Intelligence in LLMs](https://arxiv.org/abs/2512.21849)
*Jiaxin Liu,Peiyi Tu,Wenyu Chen,Yihong Zhuang,Xinxia Ling,Anji Zhou,Chenxi Wang,Zhuo Han,Zhengkai Yang,Junbo Zhao,Zenan Huang,Yuanyuan Wang*

Main category: cs.CL

TL;DR: HeartBench是一个专为评估中文大型语言模型社会情感伦理能力的框架，基于心理咨询场景和专家合作，通过细致的评分标准测量模型表现，发现现有模型在复杂情感和伦理判断上表现有限。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在认知推理表现突出，但在复杂社会、情感和伦理场景的拟人智能方面存在显著不足，尤其是中文语境下缺乏相应评估框架和高质量数据，制约模型发展。

Method: HeartBench基于真实心理咨询场景，结合临床专家意见，构建包含5个主维度和15个次能力的理论驱动分类体系，采用案例具体的评分细则和“先推理后评分”的评估协议，将人类特质细化为可量化标准。

Result: 对13个先进模型的评估显示，即使是领先模型也仅达到专家定义理想分数的60%，尤其在复杂情感细节和伦理权衡中性能明显下降。

Conclusion: HeartBench为拟人化人工智能能力评估提供了标准化的度量指标和方法论参考，同时为构建高质量、与人类价值观对齐的训练数据奠定基础。

Abstract: While Large Language Models (LLMs) have achieved remarkable success in cognitive and reasoning benchmarks, they exhibit a persistent deficit in anthropomorphic intelligence-the capacity to navigate complex social, emotional, and ethical nuances. This gap is particularly acute in the Chinese linguistic and cultural context, where a lack of specialized evaluation frameworks and high-quality socio-emotional data impedes progress. To address these limitations, we present HeartBench, a framework designed to evaluate the integrated emotional, cultural, and ethical dimensions of Chinese LLMs. Grounded in authentic psychological counseling scenarios and developed in collaboration with clinical experts, the benchmark is structured around a theory-driven taxonomy comprising five primary dimensions and 15 secondary capabilities. We implement a case-specific, rubric-based methodology that translates abstract human-like traits into granular, measurable criteria through a ``reasoning-before-scoring'' evaluation protocol. Our assessment of 13 state-of-the-art LLMs indicates a substantial performance ceiling: even leading models achieve only 60% of the expert-defined ideal score. Furthermore, analysis using a difficulty-stratified ``Hard Set'' reveals a significant performance decay in scenarios involving subtle emotional subtexts and complex ethical trade-offs. HeartBench establishes a standardized metric for anthropomorphic AI evaluation and provides a methodological blueprint for constructing high-quality, human-aligned training data.

</details>


### [21] [TimeBill: Time-Budgeted Inference for Large Language Models](https://arxiv.org/abs/2512.21859)
*Qi Fan,An Zou,Yehan Ma*

Main category: cs.CL

TL;DR: 本文提出了TimeBill，一个针对大语言模型（LLMs）在时间关键系统中的推理框架，能够在给定时间预算内高效生成响应。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs的自回归生成过程难以准确预测整体执行时间，且固定KV缓存驱逐策略无法适应不同任务的时间预算，影响推理完整性和响应性能。

Method: 提出了细粒度响应长度预测器（RLP）和执行时间估计器（ETE）来准确预测LLM的总执行时间，根据预测结果和时间预算自适应调整KV缓存驱逐比例，形成时间预算推理方法。

Result: 实验表明，TimeBill能够提升任务完成率，同时在不同时间超限策略下保持响应性能。

Conclusion: TimeBill有效平衡了推理效率和响应性能，适用于时间敏感的LLM应用场景。

Abstract: Large Language Models (LLMs) are increasingly deployed in time-critical systems, such as robotics, autonomous driving, embodied intelligence, and industrial automation, where generating accurate responses within a given time budget is crucial for decision-making, control, or safety-critical tasks. However, the auto-regressive generation process of LLMs makes it challenging to model and estimate the end-to-end execution time. Furthermore, existing efficient inference methods based on a fixed key-value (KV) cache eviction ratio struggle to adapt to varying tasks with diverse time budgets, where an improper eviction ratio may lead to incomplete inference or a drop in response performance. In this paper, we propose TimeBill, a novel time-budgeted inference framework for LLMs that balances the inference efficiency and response performance. To be more specific, we propose a fine-grained response length predictor (RLP) and an execution time estimator (ETE) to accurately predict the end-to-end execution time of LLMs. Following this, we develop a time-budgeted efficient inference approach that adaptively adjusts the KV cache eviction ratio based on execution time prediction and the given time budget. Finally, through extensive experiments, we demonstrate the advantages of TimeBill in improving task completion rate and maintaining response performance under various overrun strategies.

</details>


### [22] [Bridging the Copyright Gap: Do Large Vision-Language Models Recognize and Respect Copyrighted Content?](https://arxiv.org/abs/2512.21871)
*Naen Xu,Jinghuai Zhang,Changjiang Li,Hengyu An,Chunyi Zhou,Jun Wang,Boyu Xu,Yuyuan Li,Tianyu Du,Shouling Ji*

Main category: cs.CL

TL;DR: 本文评估了大型视觉语言模型（LVLMs）在处理版权内容时的表现，发现其在识别和遵守版权规定方面存在显著不足，并提出了一个基于工具增强的防护框架以减少版权侵权风险。


<details>
  <summary>Details</summary>
Motivation: 随着LVLMs在多模态推理任务上的广泛应用，版权侵权问题日益严重，亟需评估和提高模型对版权内容的识别与合规能力。

Method: 构建包含5万条多模态查询内容对的大规模基准数据集，分别包含有无版权声明两种情景，评估LVLMs对书籍摘录、新闻文章、歌词和代码文档等视觉输入的版权合规性；并提出一种工具增强的防护框架以改善版权合规效果。

Result: 评测结果表明，即使是最先进的闭源LVLMs在识别和尊重版权内容方面仍存在显著缺陷，尤其是在读取版权声明时也难以完全遵守版权规定；所提防护框架在各种情景下显著降低侵权风险。

Conclusion: 发展版权感知的LVLMs对于确保版权内容的合理合法使用至关重要，本文提出的防护框架为解决版权合规问题提供了有效路径。

Abstract: Large vision-language models (LVLMs) have achieved remarkable advancements in multimodal reasoning tasks. However, their widespread accessibility raises critical concerns about potential copyright infringement. Will LVLMs accurately recognize and comply with copyright regulations when encountering copyrighted content (i.e., user input, retrieved documents) in the context? Failure to comply with copyright regulations may lead to serious legal and ethical consequences, particularly when LVLMs generate responses based on copyrighted materials (e.g., retrieved book experts, news reports). In this paper, we present a comprehensive evaluation of various LVLMs, examining how they handle copyrighted content -- such as book excerpts, news articles, music lyrics, and code documentation when they are presented as visual inputs. To systematically measure copyright compliance, we introduce a large-scale benchmark dataset comprising 50,000 multimodal query-content pairs designed to evaluate how effectively LVLMs handle queries that could lead to copyright infringement. Given that real-world copyrighted content may or may not include a copyright notice, the dataset includes query-content pairs in two distinct scenarios: with and without a copyright notice. For the former, we extensively cover four types of copyright notices to account for different cases. Our evaluation reveals that even state-of-the-art closed-source LVLMs exhibit significant deficiencies in recognizing and respecting the copyrighted content, even when presented with the copyright notice. To solve this limitation, we introduce a novel tool-augmented defense framework for copyright compliance, which reduces infringement risks in all scenarios. Our findings underscore the importance of developing copyright-aware LVLMs to ensure the responsible and lawful use of copyrighted content.

</details>


### [23] [CricBench: A Multilingual Benchmark for Evaluating LLMs in Cricket Analytics](https://arxiv.org/abs/2512.21877)
*Vaibhav Devraj,Dhruv Kumar,Jagat Sesh Challa*

Main category: cs.CL

TL;DR: 本文针对板球领域的数据分析问题，提出了一个支持多语言的专门评测基准CricBench，用以评估大型语言模型（LLMs）在领域特定SQL查询上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在通用Text-to-SQL任务上虽有进步，但在体育分析这种领域特定的复杂数据和多语种环境下表现尚未充分探讨，且标准网络搜索难以满足深度统计分析需求。

Method: 构建涵盖英语与印地语的CricBench基准数据集，集合板球专家与SQL专家手动编写复杂查询；采用严格评测策略，测试包括GPT-4o、Claude 3.7 Sonnet及开源模型在内的六种先进模型的性能。

Result: 发现高通用性能模型不必然在专业领域有优势，开源模型DeepSeek R1表现最佳（50.6%准确率），优于Claude 3.7 Sonnet和GPT-4o；且印地语代码混合查询准确率不低于英语，表明母语混合提示语具有潜力。

Conclusion: 专门领域的SQL查询存在性能挑战，高性能通用模型不保证领域适用性；多语言尤其是代码混合语言提示为未来研究方向提供新视角。

Abstract: Cricket is the second most popular sport globally, commanding a massive following of over 2.5 billion fans globally. Enthusiasts and analysts frequently seek advanced statistical insights, such as long-term historical performance trends or complex player comparisons, that are often unavailable through standard web searches. While Large Language Models (LLMs) have advanced significantly in Text-to-SQL tasks, their capability to handle the domain-specific nuances, complex schema variations, and multilingual requirements inherent to sports analytics remains under-explored. To investigate this potential capability gap, we present CricBench, a comprehensive benchmark suite for evaluating LLMs on specialized cricket data. To curate a "Gold Standard" dataset, we collaborate with domain experts in cricket and SQL to manually author complex queries, ensuring logical correctness. Recognizing linguistic diversity, we construct the benchmark in both English and Hindi, establishing a framework that is open for further extension to other regional languages. We evaluate six state-of-the-art models, including GPT-4o, Claude 3.7 Sonnet, and open-source models, using a strict evaluation protocol. Our results reveal that high performance on general benchmarks does not guarantee success in specialized domains. While the open-weights reasoning model DeepSeek R1 achieves state-of-the-art performance (50.6%), surpassing proprietary giants like Claude 3.7 Sonnet (47.7%) and GPT-4o (33.7%), it still exhibits a significant accuracy drop when moving from general benchmarks (BIRD) to CricBench. Furthermore, we observe that code-mixed Hindi queries frequently yield parity or higher accuracy compared to English, challenging the assumption that English is the optimal prompt language for specialized SQL tasks.

</details>


### [24] [Explainable Statute Prediction via Attention-based Model and LLM Prompting](https://arxiv.org/abs/2512.21902)
*Sachin Pawar,Girish Keshav Palshikar,Anindita Sinha Banerjee,Nitin Ramrakhiyani,Basit Ali*

Main category: cs.CL

TL;DR: 本文探讨了自动法条预测问题，提出两种模型（AoS和LLMPrompt）在预测法条的同时生成可理解的解释，并通过多种方式验证其效果。


<details>
  <summary>Details</summary>
Motivation: 自动法条预测有助于法律AI助手和法律问答系统，提高法律服务的效率和可接受性，而可解释性是提升用户信任的关键。

Method: 提出AoS模型（基于句子注意力的有监督小模型）和LLMPrompt方法（基于大模型的零样本提示，包括标准和Chain-of-Thought提示）来分别预测相关法条及其解释。

Result: 在两个流行数据集上，所提两种方法在法条预测与解释质量方面均优于多个基线模型，解释通过自动反事实方法和人工评估均表现良好。

Conclusion: 结合预测与可解释能力的模型有助于法律AI系统的实用性和用户信任，展示了不同规模语言模型在法律领域的应用潜力。

Abstract: In this paper, we explore the problem of automatic statute prediction where for a given case description, a subset of relevant statutes are to be predicted. Here, the term "statute" refers to a section, a sub-section, or an article of any specific Act. Addressing this problem would be useful in several applications such as AI-assistant for lawyers and legal question answering system. For better user acceptance of such Legal AI systems, we believe the predictions should also be accompanied by human understandable explanations. We propose two techniques for addressing this problem of statute prediction with explanations -- (i) AoS (Attention-over-Sentences) which uses attention over sentences in a case description to predict statutes relevant for it and (ii) LLMPrompt which prompts an LLM to predict as well as explain relevance of a certain statute. AoS uses smaller language models, specifically sentence transformers and is trained in a supervised manner whereas LLMPrompt uses larger language models in a zero-shot manner and explores both standard as well as Chain-of-Thought (CoT) prompting techniques. Both these models produce explanations for their predictions in human understandable forms. We compare statute prediction performance of both the proposed techniques with each other as well as with a set of competent baselines, across two popular datasets. Also, we evaluate the quality of the generated explanations through an automated counter-factual manner as well as through human evaluation.

</details>


### [25] [Accelerate Speculative Decoding with Sparse Computation in Verification](https://arxiv.org/abs/2512.21911)
*Jikai Wang,Jianchao Tan,Yuxuan Hu,Jiayu Qin,Yerui Sun,Yuchen Xie,Xunliang Cai,Juntao Li,Min Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种稀疏验证框架，通过在投机解码的验证阶段联合稀疏化注意力、前馈网络和专家混合组件，显著减少计算开销，实现了加速语言模型推理。


<details>
  <summary>Details</summary>
Motivation: 投机解码的验证阶段计算成本高，尤其在长上下文和混合专家模型中成为性能瓶颈，现有稀疏方法主要针对标准逐词解码，难以有效降低验证阶段的冗余计算。

Method: 系统性采用多种稀疏方法于验证阶段，识别验证阶段多维度结构冗余，联合稀疏注意力、前馈网络和混合专家组件，同时引入草稿间令牌和层间检索复用策略，减少冗余计算，无需额外训练。

Result: 在文本摘要、问答和数学推理任务上，方法实现良好的效率-准确率权衡，且维持稳定的接受长度，显著降低主导计算开销。

Conclusion: 提出的稀疏验证框架有效缓解了投机解码验证阶段的计算瓶颈，提升了长上下文和混合专家模型的推理效率，具有广泛的应用潜力。

Abstract: Speculative decoding accelerates autoregressive language model inference by verifying multiple draft tokens in parallel. However, the verification stage often becomes the dominant computational bottleneck, especially for long-context inputs and mixture-of-experts (MoE) models. Existing sparsification methods are designed primarily for standard token-by-token autoregressive decoding to remove substantial computational redundancy in LLMs. This work systematically adopts different sparse methods on the verification stage of the speculative decoding and identifies structured redundancy across multiple dimensions. Based on these observations, we propose a sparse verification framework that jointly sparsifies attention, FFN, and MoE components during the verification stage to reduce the dominant computation cost. The framework further incorporates an inter-draft token and inter-layer retrieval reuse strategy to further reduce redundant computation without introducing additional training. Extensive experiments across summarization, question answering, and mathematical reasoning datasets demonstrate that the proposed methods achieve favorable efficiency-accuracy trade-offs, while maintaining stable acceptance length.

</details>


### [26] [SWE-RM: Execution-free Feedback For Software Engineering Agents](https://arxiv.org/abs/2512.21919)
*KaShun Shum,Binyuan Hui,Jiawei Chen,Lei Zhang,X. W.,Jiaxi Yang,Yuzhen Huang,Junyang Lin,Junxian He*

Main category: cs.CL

TL;DR: 本文提出了SWE-RM奖励模型，通过混合专家架构提升了软件工程智能体在测试时扩展(TTS)和强化学习(RL)中的性能，实现了在公开数据集上的新状态。


<details>
  <summary>Details</summary>
Motivation: 现有基于执行的反馈依赖于单元测试用例，反馈稀疏且难区分不同轨迹；执行自由的奖励模型虽能提供更细粒度信号，但在实际软件工程智能体中应用不充分。

Method: 通过分析执行自由奖励模型在分类准确率和校准上的表现，研究训练数据规模、策略混合及数据源组成对模型表现的影响，提出包含30亿参数的混合专家架构SWE-RM奖励模型。

Result: SWE-RM显著提升软件工程智能体在TTS和RL中的表现，Qwen3-Coder-Flash准确率提升至62.0%，Qwen3-Coder-Max提升至74.6%，在SWE-Bench Verified数据集上达到开源模型的新最优。

Conclusion: 为实现执行自由反馈在软件工程智能体中的广泛应用，构建准确且鲁棒的奖励模型至关重要，SWE-RM为TTS和RL提供了有效的奖励信号，展现了良好的泛化能力和实用价值。

Abstract: Execution-based feedback like unit testing is widely used in the development of coding agents through test-time scaling (TTS) and reinforcement learning (RL). This paradigm requires scalable and reliable collection of unit test cases to provide accurate feedback, and the resulting feedback is often sparse and cannot effectively distinguish between trajectories that are both successful or both unsuccessful. In contrast, execution-free feedback from reward models can provide more fine-grained signals without depending on unit test cases. Despite this potential, execution-free feedback for realistic software engineering (SWE) agents remains underexplored. Aiming to develop versatile reward models that are effective across TTS and RL, however, we observe that two verifiers with nearly identical TTS performance can nevertheless yield very different results in RL. Intuitively, TTS primarily reflects the model's ability to select the best trajectory, but this ability does not necessarily generalize to RL. To address this limitation, we identify two additional aspects that are crucial for RL training: classification accuracy and calibration. We then conduct comprehensive controlled experiments to investigate how to train a robust reward model that performs well across these metrics. In particular, we analyze the impact of various factors such as training data scale, policy mixtures, and data source composition. Guided by these investigations, we introduce SWE-RM, an accurate and robust reward model adopting a mixture-of-experts architecture with 30B total parameters and 3B activated during inference. SWE-RM substantially improves SWE agents on both TTS and RL performance. For example, it increases the accuracy of Qwen3-Coder-Flash from 51.6% to 62.0%, and Qwen3-Coder-Max from 67.0% to 74.6% on SWE-Bench Verified using TTS, achieving new state-of-the-art performance among open-source models.

</details>


### [27] [Broken Words, Broken Performance: Effect of Tokenization on Performance of LLMs](https://arxiv.org/abs/2512.21933)
*Sachin Pawar,Manoj Apte,Kshitij Jadhav,Girish Keshav Palshikar,Nitin Ramrakhiyani*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLM）中词语被拆分成多个子词对模型性能的负面影响，并提出了一套量化词元切分惩罚的函数。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在词元切分时，由于词汇表有限，自然单词可能被拆分成多个子词，这可能降低模型在下游任务的表现。

Method: 提出了一组惩罚函数，用于计算给定文本在特定LLM上的词元切分惩罚，以衡量切分质量。

Result: 在多个LLM和多个自然语言处理任务上，统计验证了词元拆分对模型性能的负面影响。

Conclusion: 词元切分的不合理拆分会显著影响LLM的性能，改进切分策略可能提升模型效果。

Abstract: Tokenization is the first step in training any Large Language Model (LLM), where the text is split into a sequence of tokens as per the model's fixed vocabulary. This tokenization in LLMs is different from the traditional tokenization in NLP where the text is split into a sequence of "natural" words. In LLMs, a natural word may also be broken into multiple tokens due to limited vocabulary size of the LLMs (e.g., Mistral's tokenizer splits "martial" into "mart" and "ial"). In this paper, we hypothesize that such breaking of natural words negatively impacts LLM performance on various NLP tasks. To quantify this effect, we propose a set of penalty functions that compute a tokenization penalty for a given text for a specific LLM, indicating how "bad" the tokenization is. We establish statistical significance of our hypothesis on multiple NLP tasks for a set of different LLMs.

</details>


### [28] [Self-attention vector output similarities reveal how machines pay attention](https://arxiv.org/abs/2512.21956)
*Tal Halevi,Yarden Tzach,Ronit D. Gross,Shalom Rosner,Ido Kanter*

Main category: cs.CL

TL;DR: 本文针对自注意力机制的信息处理进行了量化分析，发现BERT模型的不同注意力头具有不同的语言特征专注点，且随着层数增加，相似性从长距离转向短距离，最终集中于同一句子内。


<details>
  <summary>Details</summary>
Motivation: 尽管自注意力机制在自然语言处理领域取得显著进展，但其具体的内部信息处理机制及其学习过程的量化表征仍未明确，迫切需要深入理解。

Method: 通过分析BERT-12模型中自注意力机制的注意力图和向量空间，构建上下文相似度矩阵，量化不同层和不同头的语义特征专注情况，并研究相似度分布和头部的专属聚焦令牌。

Result: 发现最终层的注意力图集中在句子分隔符，表明基于语义特征的文本分割方法有效；不同头关注不同语言特征，如识别重复令牌或常见令牌；相似度分布显示层次越深，注意力偏向短程和同一句子内；每个头倾向于围绕唯一令牌构建相似对。

Conclusion: 本研究揭示了自注意力机制内部的语义专注及分布特性，为理解其学习机制和提升自然语言处理模型的可解释性提供了新的量化工具和视角。

Abstract: The self-attention mechanism has significantly advanced the field of natural language processing, facilitating the development of advanced language-learning machines. Although its utility is widely acknowledged, the precise mechanisms of self-attention underlying its advanced learning and the quantitative characterization of this learning process remains an open research question. This study introduces a new approach for quantifying information processing within the self-attention mechanism. The analysis conducted on the BERT-12 architecture reveals that, in the final layers, the attention map focuses on sentence separator tokens, suggesting a practical approach to text segmentation based on semantic features. Based on the vector space emerging from the self-attention heads, a context similarity matrix, measuring the scalar product between two token vectors was derived, revealing distinct similarities between different token vector pairs within each head and layer. The findings demonstrated that different attention heads within an attention block focused on different linguistic characteristics, such as identifying token repetitions in a given text or recognizing a token of common appearance in the text and its surrounding context. This specialization is also reflected in the distribution of distances between token vectors with high similarity as the architecture progresses. The initial attention layers exhibit substantially long-range similarities; however, as the layers progress, a more short-range similarity develops, culminating in a preference for attention heads to create strong similarities within the same sentence. Finally, the behavior of individual heads was analyzed by examining the uniqueness of their most common tokens in their high similarity elements. Each head tends to focus on a unique token from the text and builds similarity pairs centered around it.

</details>


### [29] [Context as a Tool: Context Management for Long-Horizon SWE-Agents](https://arxiv.org/abs/2512.22087)
*Shukai Liu,Jian Yang,Bo Jiang,Yizhi Li,Jinyang Guo,Xianglong Liu,Bryan Dai*

Main category: cs.CL

TL;DR: 本文提出了CAT，一种集成于代理决策过程中的上下文管理新范式，通过结构化上下文工作空间和主动压缩历史交互，实现对软件工程任务中长时交互的高效管理。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的软件工程代理通常依赖仅追加上下文或被动压缩，导致上下文爆炸、语义漂移及推理能力下降。

Method: 提出CAT框架，其包含稳定任务语义、凝练长期记忆和高保真短期交互的结构化上下文工作空间，并引入CAT-GENERATOR轨迹级监督框架训练上下文感知模型SWE-Compressor，支持主动压缩历史轨迹为可操作摘要。

Result: SWE-Compressor在SWE-Bench-Verified测试中，解决率达57.6%，显著优于ReAct代理和静态压缩基线，同时在限制上下文预算下保持稳定且可扩展的长时推理能力。

Conclusion: CAT方案有效提升了大语言模型软件工程代理的上下文管理和长时推理表现，推动了实际复杂SWE任务中的应用潜力。

Abstract: Agents based on large language models have recently shown strong potential on real-world software engineering (SWE) tasks that require long-horizon interaction with repository-scale codebases. However, most existing agents rely on append-only context maintenance or passively triggered compression heuristics, which often lead to context explosion, semantic drift, and degraded reasoning in long-running interactions. We propose CAT, a new context management paradigm that elevates context maintenance to a callable tool integrated into the decision-making process of agents. CAT formalizes a structured context workspace consisting of stable task semantics, condensed long-term memory, and high-fidelity short-term interactions, and enables agents to proactively compress historical trajectories into actionable summaries at appropriate milestones. To support context management for SWE-agents, we propose a trajectory-level supervision framework, CAT-GENERATOR, based on an offline data construction pipeline that injects context-management actions into complete interaction trajectories. Using this framework, we train a context-aware model, SWE-Compressor. Experiments on SWE-Bench-Verified demonstrate that SWE-Compressor reaches a 57.6% solved rate and significantly outperforms ReAct-based agents and static compression baselines, while maintaining stable and scalable long-horizon reasoning under a bounded context budget.

</details>


### [30] [Introducing TrGLUE and SentiTurca: A Comprehensive Benchmark for Turkish General Language Understanding and Sentiment Analysis](https://arxiv.org/abs/2512.22100)
*Duygu Altinok*

Main category: cs.CL

TL;DR: 本文提出了针对土耳其语自然语言理解（NLU）的综合基准测试套件TrGLUE及情感分析专用基准SentiTurca，旨在填补土耳其语NLU评估的空白。


<details>
  <summary>Details</summary>
Motivation: 当前缺少适用于土耳其语的多维度NLU基准，难以全面评估和分析土耳其语模型的理解能力。

Method: 构建了基于土耳其语原生语料的多样NLU任务集合，采用半自动化标注流程结合强大LLM注释、模型间一致性检查与人工验证，保证语言自然性并减少翻译偏差，同时提供transformer模型微调和评估代码。

Result: 开发了包含多任务的TrGLUE基准和专门情感分析基准SentiTurca，基于严谨的语料收集和标注流程，确保数据高质量和可复现。

Conclusion: TrGLUE为土耳其语NLU研究提供了强有力的评测框架和资源支持，有助于推动相关模型的发展和高质量半自动标注数据集的生成探索。

Abstract: Evaluating the performance of various model architectures, such as transformers, large language models (LLMs), and other NLP systems, requires comprehensive benchmarks that measure performance across multiple dimensions. Among these, the evaluation of natural language understanding (NLU) is particularly critical as it serves as a fundamental criterion for assessing model capabilities. Thus, it is essential to establish benchmarks that enable thorough evaluation and analysis of NLU abilities from diverse perspectives. While the GLUE benchmark has set a standard for evaluating English NLU, similar benchmarks have been developed for other languages, such as CLUE for Chinese, FLUE for French, and JGLUE for Japanese. However, no comparable benchmark currently exists for the Turkish language. To address this gap, we introduce TrGLUE, a comprehensive benchmark encompassing a variety of NLU tasks for Turkish. In addition, we present SentiTurca, a specialized benchmark for sentiment analysis. To support researchers, we also provide fine-tuning and evaluation code for transformer-based models, facilitating the effective use of these benchmarks. TrGLUE comprises Turkish-native corpora curated to mirror the domains and task formulations of GLUE-style evaluations, with labels obtained through a semi-automated pipeline that combines strong LLM-based annotation, cross-model agreement checks, and subsequent human validation. This design prioritizes linguistic naturalness, minimizes direct translation artifacts, and yields a scalable, reproducible workflow. With TrGLUE, our goal is to establish a robust evaluation framework for Turkish NLU, empower researchers with valuable resources, and provide insights into generating high-quality semi-automated datasets.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [31] [Multi-Agent LLM Committees for Autonomous Software Beta Testing](https://arxiv.org/abs/2512.21352)
*Sumanth Bharadwaj Hachalli Karanam,Dhiwahar Adhithya Kennady*

Main category: cs.SE

TL;DR: 提出了一个多智能体委员会框架，通过视觉增强的大型语言模型合作进行软件测试，显著提升测试成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 人工软件测试成本高、耗时，单智能体模型存在幻觉和行为不一致的问题。

Method: 引入多智能体委员会框架，利用多样化的视觉增强LLM，通过三轮投票协议协作测试，结合模型多样性、角色驱动行为差异和界面理解进行系统测试。

Result: 多智能体配置测试成功率高达89.5%至100%，超过单智能体基线13.7至22个百分点，具备实时检测能力，并在多个基准测试中表现优异。

Conclusion: 该多智能体视觉增强LLM框架有效提升了自动化软件测试的准确率和效率，支持实用的CI/CD管道中应用，促进可复现研究和实际部署。

Abstract: Manual software beta testing is costly and time-consuming, while single-agent large language model (LLM) approaches suffer from hallucinations and inconsistent behavior. We propose a multi-agent committee framework in which diverse vision-enabled LLMs collaborate through a three-round voting protocol to reach consensus on testing actions. The framework combines model diversity, persona-driven behavioral variation, and visual user interface understanding to systematically explore web applications. Across 84 experimental runs with 9 testing personas and 4 scenarios, multi-agent committees achieve an 89.5 percent overall task success rate. Configurations with 2 to 4 agents reach 91.7 to 100 percent success, compared to 78.0 percent for single-agent baselines, yielding improvements of 13.7 to 22.0 percentage points. At the action level, the system attains a 93.1 percent success rate with a median per-action latency of 0.71 seconds, enabling real-time and continuous integration testing. Vision-enabled agents successfully identify user interface elements, with navigation and reporting achieving 100 percent success and form filling achieving 99.2 percent success. We evaluate the framework on WebShop and OWASP benchmarks, achieving 74.7 percent success on WebShop compared to a 50.1 percent published GPT-3 baseline, and 82.0 percent success on OWASP Juice Shop security testing with coverage of 8 of the 10 OWASP Top 10 vulnerability categories. Across 20 injected regressions, the committee achieves an F1 score of 0.91 for bug detection, compared to 0.78 for single-agent baselines. The open-source implementation enables reproducible research and practical deployment of LLM-based software testing in CI/CD pipelines.

</details>


### [32] [Understanding the Role of Large Language Models in Software Engineering: Evidence from an Industry Survey](https://arxiv.org/abs/2512.21347)
*Vítor Mateus de Brito,Kleinner Farias*

Main category: cs.SE

TL;DR: 本文通过调查46位不同行业经验的软件工程师，研究了大型语言模型（LLMs）在软件工程中的应用，发现LLMs在提升技术问题解决速度、文档支持和代码规范化方面表现良好，但也带来了认知依赖、安全风险和技术自主权削弱的担忧。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs深度融入开发者日常工作，理解其实际应用和影响成为必要。

Method: 通过对46位业界专业人士进行问卷调查，收集不同背景和经验的软件工程师的看法。

Result: 发现LLMs被积极看待，尤其在加快技术问题解决、改进文档和代码标准化方面有效，但同时存在认知依赖和安全风险等问题。

Conclusion: 强调对LLMs工具的审慎监管和使用，推动LLMs技术更有效、负责和安全的应用，并鼓励对其认知、伦理及组织层面影响的进一步研究。

Abstract: The rapid advancement of Large Language Models (LLMs) is reshaping software engineering by profoundly influencing coding, documentation, and system maintenance practices. As these tools become deeply embedded in developers' daily workflows, understanding how they are used has become essential. This paper reports an empirical study of LLM adoption in software engineering, based on a survey of 46 industry professionals with diverse educational backgrounds and levels of experience. The results reveal positive perceptions of LLMs, particularly regarding faster resolution of technical questions, improved documentation support, and enhanced source code standardization. However, respondents also expressed concerns about cognitive dependence, security risks, and the potential erosion of technical autonomy. These findings underscore the need for critical and supervised use of LLM-based tools. By grounding the discussion in empirical evidence from industry practice, this study bridges the gap between academic discourse and real-world software development. The results provide actionable insights for developers and researchers seeking to adopt and evolve LLM-based technologies in a more effective, responsible, and secure manner, while also motivating future research on their cognitive, ethical, and organizational implications.

</details>


### [33] [Analyzing Code Injection Attacks on LLM-based Multi-Agent Systems in Software Development](https://arxiv.org/abs/2512.21818)
*Brian Bowers,Smita Khapre,Jugal Kalita*

Main category: cs.SE

TL;DR: 本文提出了一个基于多智能体系统的软件实施架构，分析了系统在生成代码时的安全风险，并通过引入安全分析智能体提升系统的抗攻击能力。


<details>
  <summary>Details</summary>
Motivation: 随着基于目标驱动自主的智能体系统广泛应用，如何保障这些自动化多智能体系统在代码生成中的安全性成为核心问题。

Method: 设计了带有编码者、审查者、测试者以及安全分析者的多智能体协同体系结构，并构建了全面的威胁模型，分析不同架构下系统的安全性与效率。

Result: 研究表明，编码者-审查者-测试者架构较单一和双元架构更具韧性，但效率降低。引入安全分析智能体后，效率提升且韧性进一步增强。然而，安全分析智能体依然存在高级代码注入攻击的风险，攻击成功率可高达71.95%。

Conclusion: 多智能体系统在自动代码生成中存在显著安全风险，合理设计多角色架构并增设安全分析智能体可提升抗攻击能力，但高级攻击仍需被重点防范。

Abstract: Agentic AI and Multi-Agent Systems are poised to dominate industry and society imminently. Powered by goal-driven autonomy, they represent a powerful form of generative AI, marking a transition from reactive content generation into proactive multitasking capabilities. As an exemplar, we propose an architecture of a multi-agent system for the implementation phase of the software engineering process. We also present a comprehensive threat model for the proposed system. We demonstrate that while such systems can generate code quite accurately, they are vulnerable to attacks, including code injection. Due to their autonomous design and lack of humans in the loop, these systems cannot identify and respond to attacks by themselves. This paper analyzes the vulnerability of multi-agent systems and concludes that the coder-reviewer-tester architecture is more resilient than both the coder and coder-tester architectures, but is less efficient at writing code. We find that by adding a security analysis agent, we mitigate the loss in efficiency while achieving even better resiliency. We conclude by demonstrating that the security analysis agent is vulnerable to advanced code injection attacks, showing that embedding poisonous few-shot examples in the injected code can increase the attack success rate from 0% to 71.95%.

</details>


### [34] [Fairness Is Not Just Ethical: Performance Trade-Off via Data Correlation Tuning to Mitigate Bias in ML Software](https://arxiv.org/abs/2512.21348)
*Ying Xiao,Shangwen Wang,Sicen Liu,Dingyuan Xue,Xian Zhan,Yepang Liu,Jie M. Zhang*

Main category: cs.SE

TL;DR: 本文提出了一种名为相关性调优（CoT）的新型预处理方法，通过调整数据中的相关性以减轻模型偏见，提升弱势群体的预测准确率和公平性指标表现。


<details>
  <summary>Details</summary>
Motivation: 传统软件公平性研究多注重伦理和社会层面，忽视了公平性作为软件核心质量问题的本质，即性能在敏感用户群体间的差异。现有偏见缓解方法面临预处理方法适用广泛但效果不佳，后处理方法效果好但适用范围受限的困境。

Method: 提出Phi系数作为衡量敏感属性与标签相关性的指标，利用多目标优化调整数据相关性以缓解代理偏见，形成新的预处理方法CoT。

Result: CoT在提升弱势群体真正例率平均17.5%，并在统计公平差异、平均赔率差异和平等机会差异三个关键指标上平均减少超过50%。在单属性和多属性场景下分别比现有最优方法提升3个百分点和10个百分点。

Conclusion: 将公平性明确视为软件质量维度，采用CoT方法能有效缓解偏见，提升模型在弱势群体中的表现及泛化能力，推动公平性研究从伦理探讨向实际性能优化转变。

Abstract: Traditional software fairness research typically emphasizes ethical and social imperatives, neglecting that fairness fundamentally represents a core software quality issue arising directly from performance disparities across sensitive user groups. Recognizing fairness explicitly as a software quality dimension yields practical benefits beyond ethical considerations, notably improved predictive performance for unprivileged groups, enhanced out-of-distribution generalization, and increased geographic transferability in real-world deployments. Nevertheless, existing bias mitigation methods face a critical dilemma: while pre-processing methods offer broad applicability across model types, they generally fall short in effectiveness compared to post-processing techniques. To overcome this challenge, we propose Correlation Tuning (CoT), a novel pre-processing approach designed to mitigate bias by adjusting data correlations. Specifically, CoT introduces the Phi-coefficient, an intuitive correlation measure, to systematically quantify correlation between sensitive attributes and labels, and employs multi-objective optimization to address the proxy biases. Extensive evaluations demonstrate that CoT increases the true positive rate of unprivileged groups by an average of 17.5% and reduces three key bias metrics, including statistical parity difference (SPD), average odds difference (AOD), and equal opportunity difference (EOD), by more than 50% on average. CoT outperforms state-of-the-art methods by three and ten percentage points in single attribute and multiple attributes scenarios, respectively. We will publicly release our experimental results and source code to facilitate future research.

</details>


### [35] [CosmoCore-Evo: Evolutionary Dream-Replay Reinforcement Learning for Adaptive Code Generation](https://arxiv.org/abs/2512.21351)
*Santhosh Kumar Ravindran*

Main category: cs.SE

TL;DR: CosmoCore-Evo在CosmoCore的基础上引入进化算法，通过基因变异和选择机制提升代码生成任务中代理的适应性和创新性，实现性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 针对代码生成中环境分布变化（如API或库变化）导致的性能下降，借鉴人类进化中的自然选择和适应机制，提升模型的应对能力和创新水平。

Method: 将强化学习轨迹视为基因，通过夜间回放阶段对轨迹进行变异和选择，结合增强的Dream Queue和企业定制的适应度函数（效率、合规性、可扩展性指标）实现进化操作。

Result: 在多项扩展基准测试（如HumanEval变体、BigCodeBench和PySpark流水线模拟）中，CosmoCore-Evo实现了解决方案新颖度提升35%和适应速度提升25%，显著优于CosmoCore及PPO、REAMER基线方法。

Conclusion: 进化机制有效促进了大型语言模型代理的自我突破和适应能力，缩小了智能差距，验证了将生物进化思想融入强化学习的潜力，且代码及模拟环境已开源便于复现。

Abstract: Building on the affective dream-replay reinforcement learning framework of CosmoCore, we introduce CosmoCore-Evo, an extension that incorporates evolutionary algorithms to enhance adaptability and novelty in code generation tasks. Inspired by anthropological aspects of human evolution, such as natural selection and adaptation in early hominids, CosmoCore-Evo treats RL trajectories as ``genomes'' that undergo mutation and selection during the nocturnal replay phase. This mechanism allows agents to break free from trained patterns, fostering emergent behaviors and improved performance in distribution-shifted environments, such as changing APIs or novel libraries. We augment the Dream Queue with evolutionary operations, including mutation of high-fitness trajectories and enterprise-tuned fitness functions that incorporate efficiency, compliance, and scalability metrics. Evaluated on extended benchmarks including HumanEval variants with shifts, BigCodeBench, and a custom PySpark pipeline simulation, CosmoCore-Evo achieves up to 35% higher novelty in solutions and 25% faster adaptation compared to the original CosmoCore and baselines like PPO and REAMER. Ablations confirm the role of evolutionary components in bridging the sentient gap for LLM agents. Code for replication, including a toy simulation, is provided.

</details>


### [36] [AInsteinBench: Benchmarking Coding Agents on Scientific Repositories](https://arxiv.org/abs/2512.21373)
*Titouan Duston,Shuo Xin,Yang Sun,Daoguang Zan,Aoyan Li,Shulin Xin,Kai Shen,Yixiao Chen,Qiming Sun,Ge Zhang,Jiashuo Liu,Huan Zhou,Jingkai Liu,Zhichen Pu,Yuanheng Wang,Bo-Xuan Ge,Xin Tong,Fei Ye,Zhi-Chao Zhao,Wen-Biao Han,Zhoujian Cao,Yueran Zhao,Weiluo Ren,Qingshen Long,Yuxiao Liu,Anni Huang,Yidi Du,Yuanyuan Rong,Jiahao Peng*

Main category: cs.SE

TL;DR: AInsteinBench是一个大规模基准测试，用于评估大型语言模型在真实科研软件生态中作为科学计算开发代理的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注概念知识或通用软件工程问题，缺乏在真实科研环境中端到端科学开发的评估。

Method: 基准涵盖六个科学代码库中的维护者贡献的任务，经过多阶段筛选和专家评审，在可执行环境中通过测试驱动验证科学计算能力。

Result: 该基准能揭示模型在科学开发中的核心能力，包括超越表层代码生成的深度理解和科学意义的失败模式。

Conclusion: AInsteinBench有效衡量语言模型在复杂真实科研软件环境中的综合开发能力，推动科学计算领域智能代理的发展。

Abstract: We introduce AInsteinBench, a large-scale benchmark for evaluating whether large language model (LLM) agents can operate as scientific computing development agents within real research software ecosystems. Unlike existing scientific reasoning benchmarks which focus on conceptual knowledge, or software engineering benchmarks that emphasize generic feature implementation and issue resolving, AInsteinBench evaluates models in end-to-end scientific development settings grounded in production-grade scientific repositories. The benchmark consists of tasks derived from maintainer-authored pull requests across six widely used scientific codebases, spanning quantum chemistry, quantum computing, molecular dynamics, numerical relativity, fluid dynamics, and cheminformatics. All benchmark tasks are carefully curated through multi-stage filtering and expert review to ensure scientific challenge, adequate test coverage, and well-calibrated difficulty. By leveraging evaluation in executable environments, scientifically meaningful failure modes, and test-driven verification, AInsteinBench measures a model's ability to move beyond surface-level code generation toward the core competencies required for computational scientific research.

</details>


### [37] [What Makes a GitHub Issue Ready for Copilot?](https://arxiv.org/abs/2512.21426)
*Mohammed Sayagh*

Main category: cs.SE

TL;DR: 本文建立了32条详细标准，评估GitHub issue的质量，预测其是否能导致合并的PR，并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 现有的GitHub issue模糊或不全面，影响AI代理如Copilot的代码实现效果，现有最佳实践过于笼统，缺乏详细指导。

Method: 分析导致合并与关闭PR的GitHub issue，构建32条详细质量标准；采用可解释的机器学习模型预测issue导致合并PR概率，并分析影响因素。

Result: 发现合并的PR对应的issue通常更短小、范围明确、包含明确指导和相关线索；含外部引用的issue合并率较低。模型预测AUC中位数为72%。

Conclusion: 提出了评估和提升GitHub issue质量的具体指标和可解释模型，为AI辅助软件开发中的issue书写提供指导，推动该活动成为软件工程中的重要部分。

Abstract: AI-agents help developers in different coding tasks, such as developing new features, fixing bugs, and reviewing code. Developers can write a Github issue and assign it to an AI-agent like Copilot for implementation. Based on the issue and its related discussion, the AI-agent performs a plan for the implementation, and executes it. However, the performance of AI-agents and LLMs heavily depends on the input they receive. For instance, a GitHub issue that is unclear or not well scoped might not lead to a successful implementation that will eventually be merged. GitHub Copilot provides a set of best practice recommendations that are limited and high-level. In this paper, we build a set of 32 detailed criteria that we leverage to measure the quality of GitHub issues to make them suitable for AI-agents. We compare the GitHub issues that lead to a merged pull request versus closed pull request. Then, we build an interpretable machine learning model to predict the likelihood of a GitHub issue resulting in a merged pull request. We observe that pull requests that end up being merged are those originating from issues that are shorter, well scoped, with clear guidance and hints about the relevant artifacts for an issue, and with guidance on how to perform the implementation. Issues with external references including configuration, context setup, dependencies or external APIs are associated with lower merge rates. We built an interpretable machine learning model to help users identify how to improve a GitHub issue to increase the chances of the issue resulting in a merged pull request by Copilot. Our model has a median AUC of 72\%. Our results shed light on quality metrics relevant for writing GitHub issues and motivate future studies further investigate the writing of GitHub issues as a first-class software engineering activity in the era of AI-teammates.

</details>


### [38] [Cerberus: Multi-Agent Reasoning and Coverage-Guided Exploration for Static Detection of Runtime Errors](https://arxiv.org/abs/2512.21431)
*Hridya Dhulipala,Xiaokai Rong,Tien N. Nguyen*

Main category: cs.SE

TL;DR: 提出了Cerberus，一种基于大语言模型的无需执行代码即可进行覆盖率引导的运行时错误检测框架。


<details>
  <summary>Details</summary>
Motivation: 在软件开发中，希望能在不执行代码的情况下检测代码片段中的运行时错误，尤其是在线代码片段集成前。

Method: Cerberus利用大语言模型生成触发错误的输入，预测代码覆盖率并检测错误，通过两阶段反馈回路先提升覆盖率及检测错误，再专注于错误检测。

Result: 实验证明Cerberus在生成高覆盖率测试用例和发现运行时错误方面优于传统和学习型测试框架。

Conclusion: Cerberus能高效针对（不）完整代码片段进行测试，提升错误发现率，是一种有效的执行无依赖的错误检测方法。

Abstract: In several software development scenarios, it is desirable to detect runtime errors and exceptions in code snippets without actual execution. A typical example is to detect runtime exceptions in online code snippets before integrating them into a codebase. In this paper, we propose Cerberus, a novel predictive, execution-free coverage-guided testing framework. Cerberus uses LLMs to generate the inputs that trigger runtime errors and to perform code coverage prediction and error detection without code execution. With a two-phase feedback loop, Cerberus first aims to both increasing code coverage and detecting runtime errors, then shifts to focus only detecting runtime errors when the coverage reaches 100% or its maximum, enabling it to perform better than prompting the LLMs for both purposes. Our empirical evaluation demonstrates that Cerberus performs better than conventional and learning-based testing frameworks for (in)complete code snippets by generating high-coverage test cases more efficiently, leading to the discovery of more runtime errors.

</details>


### [39] [Fuzzwise: Intelligent Initial Corpus Generation for Fuzzing](https://arxiv.org/abs/2512.21440)
*Hridya Dhulipala,Xiaokai Rong,Aashish Yadavally,Tien N. Nguyen*

Main category: cs.SE

TL;DR: FuzzWise通过结合生成和最小化测试种子过程，利用基于大型语言模型的多智能体框架生成了优化的初始测试种子集，提高灰盒模糊测试的效率和效果。


<details>
  <summary>Details</summary>
Motivation: 在基于变异的灰盒模糊测试中，高质量的初始种子集对提高测试效果至关重要，但传统方法分阶段生成和最小化种子，效率低且资源消耗大。

Method: 提出FuzzWise，采用两个大型语言模型代理：一个生成测试用例，另一个预测测试用例对代码覆盖率的贡献，实现测试种子的即时评估和筛选，无需实际执行程序，节省计算资源。

Result: 实验证明，FuzzWise生成的测试用例数量显著少于基线方法，但仍实现更高的代码覆盖和更多运行时错误，同时在时间和覆盖效率上表现更优。

Conclusion: FuzzWise通过整合生成与筛选过程及预测覆盖贡献，显著提升了初始测试种子的质量和模糊测试的效率，适合资源有限或不可执行场景。

Abstract: In mutation-based greybox fuzzing, generating high-quality input seeds for the initial corpus is essential for effective fuzzing. Rather than conducting separate phases for generating a large corpus and subsequently minimizing it, we propose FuzzWise which integrates them into one process to generate the optimal initial corpus of seeds (ICS). FuzzWise leverages a multi-agent framework based on Large Language Models (LLMs). The first LLM agent generates test cases for the target program. The second LLM agent, which functions as a predictive code coverage module, assesses whether each generated test case will enhance the overall coverage of the current corpus. The streamlined process allows each newly generated test seed to be immediately evaluated for its contribution to the overall coverage. FuzzWise employs a predictive approach using an LLM and eliminates the need for actual execution, saving computational resources and time, particularly in scenarios where the execution is not desirable or even impossible. Our empirical evaluation demonstrates that FuzzWise generates significantly fewer test cases than baseline methods. Despite the lower number of test cases, FuzzWise achieves high code coverage and triggers more runtime errors compared to the baselines. Moreover, it is more time-efficient and coverage-efficient in producing an initial corpus catching more errors.

</details>


### [40] [Code Clone Refactoring in C# with Lambda Expressions](https://arxiv.org/abs/2512.21511)
*Takuto Kawamoto,Yoshiki Higo*

Main category: cs.SE

TL;DR: 本文提出了一种基于C# lambda表达式的代码克隆合并技术，评估结果显示该方法可以成功重构约28.9%的代码克隆对。


<details>
  <summary>Details</summary>
Motivation: 虽然行为参数化在Java中已有较多研究，但针对C#等其他语言的应用较少，语言特性影响技术适用性，因此需要针对C#设计专门的提取方法重构技术。

Method: 利用C#中的lambda表达式对代码克隆中的行为进行参数化，并使用NiCad工具检测代码克隆，评估该方法在实测项目中的可重构率和成功率。

Result: 在22个项目中共检测出2217对克隆，35.0%适合重构，实际成功重构28.9%，表明该方法在C#环境下具有一定有效性。

Conclusion: 基于C# lambda表达式的行为参数化提取方法可有效支持代码克隆合并，语言特性需纳入重构方法设计考虑。

Abstract: "Extract Method" refactoring is a technique for consolidating code clones. Parameterization approaches are used to extract a single method from multiple code clones that contain differences. This approach parameterizes expressions and behaviors within a method. In particular, behavior parameterization has been extensively studied in Java programs, but little research has been conducted on other programming languages.
  Lambda expressions can be used to parameterize behaviors, but the specifications of each programming language significantly affect the applicability of this technique. Therefore, the optimal "Extract Method" approach may vary depending on the programming language.
  In this study, we propose a C#-specific technique that uses lambda expressions to analyze and consolidate code clones. We evaluated our proposed method by applying it to code clones detected by the NiCad clone detector and measuring how many of them could be successfully consolidated.
  In total, 2,217 clone pairs from 22 projects were included in our evaluation. For the clone pairs determined to be refactorable, we also attempted refactoring actually. The proposed approach determined that 35.0% of all clone pairs were suitable for refactoring. Among these, 28.9% were successfully refactored.

</details>


### [41] [XTrace: A Non-Invasive Dynamic Tracing Framework for Android Applications in Production](https://arxiv.org/abs/2512.21555)
*Qi Hu,Jiangchao Liu,Xin Yu,Lin Zhang,Edward Jiang*

Main category: cs.SE

TL;DR: 本文提出了XTrace，一种基于Android ART虚拟机的动态追踪框架，能高性能、非侵入式地拦截方法，适用于大规模生产环境，用于实时诊断移动应用中的复杂问题。


<details>
  <summary>Details</summary>
Motivation: 随着移动应用复杂性和用户设备环境碎片化的增加，传统的静态日志记录和崩溃后分析方法因缺少实时上下文信息，难以应对特定场景下的“幽灵bug”，亟需动态运行时观测能力。

Method: XTrace利用一种新颖的非侵入式代理技术，避免直接修改虚拟机底层数据结构，利用并优化Android ART虚拟机内置的稳定插桩机制，实现高性能的方法拦截。

Result: 在字节跳动日活数亿用户的应用中，XTrace表现出生产级的稳定性和性能，在线A/B测试表明对崩溃率和ANR无显著影响，启动延迟低于7毫秒，每次方法调用开销低于0.01毫秒，兼容Android 5.0及以上版本，并成功诊断了11个严重线上崩溃及多个性能瓶颈。

Conclusion: XTrace成功解决了Android动态追踪中稳定性与全面覆盖冲突的问题，提供了一种高效、稳定、实时的线上移动应用动态追踪解决方案，显著提升了故障定位效率。

Abstract: As the complexity of mobile applications grows exponentially and the fragmentation of user device environments intensifies, ensuring online application stability faces unprecedented challenges. Traditional methods, such as static logging and post-crash analysis, lack real-time contextual information, rendering them ineffective against "ghost bugs" that only manifest in specific scenarios. This highlights an urgent need for dynamic runtime observability: intercepting and tracing arbitrary methods in production without requiring an app release. We propose XTrace, a novel dynamic tracing framework. XTrace introduces a new paradigm of non-invasive proxying, which avoids direct modification of the virtual machine's underlying data structures. It achieves high-performance method interception by leveraging and optimizing the highly stable, built-in instrumentation mechanism of the Android ART virtual machine. Evaluated in a ByteDance application with hundreds of millions of daily active users, XTrace demonstrated production-grade stability and performance. Large-scale online A/B experiments confirmed its stability, showing no statistically significant impact (p > 0.05) on Crash User Rate or ANR rate, while maintaining minimal overhead (<7 ms startup latency, <0.01 ms per-method call) and broad compatibility (Android 5.0-15+). Critically, XTrace diagnosed over 11 severe online crashes and multiple performance bottlenecks, improving root-cause localization efficiency by over 90%. This confirms XTrace provides a production-grade solution that reconciles the long-standing conflict between stability and comprehensive coverage in Android dynamic tracing.

</details>


### [42] [Co-Evolution of Types and Dependencies: Towards Repository-Level Type Inference for Python Code](https://arxiv.org/abs/2512.21591)
*Shuo Sun,Shixin Zhang,Jiwei Yan,Jun Yan,Jian Zhang*

Main category: cs.SE

TL;DR: 本文提出了一种基于大型语言模型（LLM）的新方法\methodName ，通过实体依赖图（EDG）和迭代推断，实现了Python仓库级的类型推断，显著提升了推断准确率和减少了错误传播。


<details>
  <summary>Details</summary>
Motivation: Python动态类型导致大量运行时类型错误，现有工具多在代码片段级别取得进展，但很难处理仓库级的复杂跨过程依赖。

Method: 构建实体依赖图（EDG）来刻画仓库中对象和类型依赖，通过迭代方法使类型和依赖关系共同演化，结合在线类型检查器实时验证和修正推断结果。

Result: 在12个复杂Python仓库上，\methodName 实现TypeSim评分0.89和TypeExact评分0.84，分别比最强基线提高27%和40%，并消除92.7%的工具引入的新类型错误。

Conclusion: \methodName显著提升了仓库级自动类型注释的准确性和可靠性，为现实Python开发环境中的自动化类型推断迈出重要一步。

Abstract: Python's dynamic typing mechanism, while promoting flexibility, is a significant source of runtime type errors that plague large-scale software, which inspires the automatic type inference techniques. Existing type inference tools have achieved advances in type inference within isolated code snippets. However, repository-level type inference remains a significant challenge, primarily due to the complex inter-procedural dependencies that are difficult to model and resolve. To fill this gap, we present \methodName, a novel approach based on LLMs that achieves repository-level type inference through the co-evolution of types and dependencies. \methodName~constructs an Entity Dependency Graph (EDG) to model the objects and type dependencies across the repository. During the inference process, it iteratively refines types and dependencies in EDG for accurate type inference. Our key innovations are: (1) an EDG model designed to capture repository-level type dependencies; (2) an iterative type inference approach where types and dependencies co-evolve in each iteration; and (3) a type-checker-in-the-loop strategy that validates and corrects inferences on-the-fly, thereby reducing error propagation. When evaluated on 12 complex Python repositories, \methodName~significantly outperformed prior works, achieving a \textit{TypeSim} score of 0.89 and a \textit{TypeExact} score of 0.84, representing a 27\% and 40\% relative improvement over the strongest baseline. More importantly, \methodName~removed new type errors introduced by the tool by 92.7\%. This demonstrates a significant leap towards automated, reliable type annotation for real-world Python development.

</details>


### [43] [How Do Agents Perform Code Optimization? An Empirical Study](https://arxiv.org/abs/2512.21757)
*Huiyun Peng,Antonio Zhong,Ricardo Andrés Calvo Méndez,Kelechi G. Kalu,James C. Davis*

Main category: cs.SE

TL;DR: 本文首次实证比较了AI与人类开发者在性能优化提交上的表现，发现AI生成的性能优化提交在性能验证方面明显较少。


<details>
  <summary>Details</summary>
Motivation: 性能优化是软件开发中复杂且关键的环节，目前尚不清楚AI代码代理在真实性能优化任务中的表现如何。

Method: 分析了AIDev数据集中324个AI生成和83个人类编写的性能优化PR，从采用率、可维护性、优化模式及验证实践等多个维度进行了比较研究。

Result: AI生成的性能优化PR在包含显式性能验证的比例上显著低于人类编写的PR（45.7%对63.6%，p=0.007），但两者使用的优化模式基本相同。

Conclusion: AI代码代理在性能优化中存在验证不足的问题，但整体优化模式与人类相似，有提升空间，未来研究可以进一步推动智能代码优化的发展。

Abstract: Performance optimization is a critical yet challenging aspect of software development, often requiring a deep understanding of system behavior, algorithmic tradeoffs, and careful code modifications. Although recent advances in AI coding agents have accelerated code generation and bug fixing, little is known about how these agents perform on real-world performance optimization tasks. We present the first empirical study comparing agent- and human-authored performance optimization commits, analyzing 324 agent-generated and 83 human-authored PRs from the AIDev dataset across adoption, maintainability, optimization patterns, and validation practices. We find that AI-authored performance PRs are less likely to include explicit performance validation than human-authored PRs (45.7\% vs. 63.6\%, $p=0.007$). In addition, AI-authored PRs largely use the same optimization patterns as humans. We further discuss limitations and opportunities for advancing agentic code optimization.

</details>


### [44] [The State of the SBOM Tool Ecosystems: A Comparative Analysis of SPDX and CycloneDX](https://arxiv.org/abs/2512.21781)
*Abdul Ali Bangash,Tongxu Ge,Zhimin Zhao,Arshdeep Singh,Zitao Wang,Bram Adams*

Main category: cs.SE

TL;DR: 本文比较了两种主流软件物料清单（SBOM）格式SPDX和CycloneDX的工具生态系统，分析其成熟度、工具支持、社区参与度及开发健康状况，揭示各自优势和改进空间。


<details>
  <summary>Details</summary>
Motivation: SBOM的广泛采用依赖于相关工具生态系统，但SPDX和CycloneDX两大格式的生态系统在成熟度和工具支持上有显著差异，需深入比较以促进其发展。

Method: 通过定量分析170款公开SBOM工具、比较171个CycloneDX与470个SPDX工具的健康指标、分析36990个开源工具的问题报告，以及调查使用各工具生态系统的前250大开源项目，综合评估两种生态系统。

Result: 研究发现CycloneDX生态系统的项目开发者参与度较高且部分健康指标优异，而SPDX生态系统具备更成熟的生态环境、更多工具支持及更广泛的行业认可。

Conclusion: 两种SBOM工具生态系统各具优势且互补，研究为开发者和实践者提供了知识参考，指出了促进双方共同提升的机会。

Abstract: A Software Bill of Materials (SBOM) provides transparency by documenting software component metadata and dependencies. However, SBOM adoption depends on tool ecosystems. With two dominant formats: SPDX and CycloneDX - the ecosystems vary significantly in maturity, tool support, and community engagement. We conduct a quantitative comparison of use cases for 170 publicly advertised SBOM tools, identifying enhancement areas for each format. We compare health metrics of both ecosystems (171 CycloneDX versus 470 SPDX tools) to evaluate robustness and maturity. We quantitatively compare 36,990 issue reports from open-source tools to identify challenges and development opportunities. Finally, we investigate the top 250 open-source projects using each tool ecosystem and compare their health metrics. Our findings reveal distinct characteristics: projects using CycloneDX tools demonstrate higher developer engagement and certain health indicators, while SPDX tools benefit from a more mature ecosystem with broader tool availability and established industry adoption. This research provides insights for developers, contributors, and practitioners regarding complementary strengths of these ecosystems and identifies opportunities for mutual enhancement.

</details>


### [45] [A Story About Cohesion and Separation: Label-Free Metric for Log Parser Evaluation](https://arxiv.org/abs/2512.21811)
*Qiaolin Qin,Jianchen Zhao,Heng Li,Weiyi Shang,Ettore Merlo*

Main category: cs.SE

TL;DR: 本文提出一种无标签的日志解析评估指标PMSS，通过中位轮廓分析和Levenshtein距离评估解析器性能，实现近线性时间复杂度。与传统带标签指标FGA和FTA高度相关，适用于标签缺失或不一致的情况。


<details>
  <summary>Details</summary>
Motivation: 现有日志解析评估指标严重依赖带标签数据，限制了评估的适用范围且不同版本的标签导致结论不一致，影响工业应用中的解析器选择。

Method: 提出基于中位轮廓分数（PMSS）的无标签模板级评价指标，结合Levenshtein距离评估解析器的分组和模板质量，时间复杂度近似线性。通过对标准数据集Loghub 2.0上六个解析器进行评测，与传统标签指标FGA和FTA进行相关性比较。

Result: PMSS指标与FGA和FTA之间表现高度一致，相关系数分别达到0.648和0.587，PMSS能有效替代带标签指标。解析器在PMSS和FGA指标上的表现差异平均仅为2.1%，在FTA上为9.8%。PMSS统计显著相关（p<1e-8），且能解决标签不一致问题。

Conclusion: PMSS为日志解析评估提供了无标签替代方案，特别适合标签缺失或不一致环境，有助于提高工业界解析器选择的灵活性和准确性，且提供了指标解读指导和使用建议。

Abstract: Log parsing converts log messages into structured event templates, allowing for automated log analysis and reducing manual inspection effort. To select the most compatible parser for a specific system, multiple evaluation metrics are commonly used for performance comparisons. However, existing evaluation metrics heavily rely on labeled log data, which limits prior studies to a fixed set of datasets and hinders parser evaluations and selections in the industry. Further, we discovered that different versions of ground-truth used in existing studies can lead to inconsistent performance conclusions. Motivated by these challenges, we propose a novel label-free template-level metric, PMSS (parser medoid silhouette score), to evaluate log parser performance. PMSS evaluates both parser grouping and template quality with medoid silhouette analysis and Levenshtein distance within a near-linear time complexity in general. To understand its relationship with label-based template-level metrics, FGA and FTA, we compared their evaluation outcomes for six log parsers on the standard corrected Loghub 2.0 dataset. Our results indicate that log parsers achieving the highest PMSS or FGA exhibit comparable performance, differing by only 2.1% on average in terms of the FGA score; the difference is 9.8% for FTA. PMSS is also significantly (p<1e-8) and positively correlated to both FGA and FTA: the Spearman's rho correlation coefficient of PMSS-FGA and PMSS-FTA are respectively 0.648 and 0.587, close to the coefficient between FGA and FTA (0.670). We further extended our discussion on how to interpret the conclusions from different metrics, identifying challenges in using PMSS, and provided guidelines on conducting parser selections with our metric. PMSS provides a valuable evaluation alternative when ground-truths are inconsistent or labels are unavailable.

</details>


### [46] [HALF: Process Hollowing Analysis Framework for Binary Programs with the Assistance of Kernel Modules](https://arxiv.org/abs/2512.22043)
*Zhangbo Long,Letian Sha,Jiaye Pan,Dongpeng Xu,Yifei Huang,Fu Xiao*

Main category: cs.SE

TL;DR: 本文提出了一种基于内核模块和进程空洞技术的新型二进制程序分析框架，以提升细粒度动态污点分析的可用性和性能，验证了框架在Windows平台上的有效性和实用价值。


<details>
  <summary>Details</summary>
Motivation: 当前细粒度二进制代码分析如动态污点分析存在部署难度大、内存消耗高和性能开销大等问题，难以适应内存破坏利用和沙箱逃避等新场景。

Method: 利用内核模块扩展传统动态二进制插装技术的分析能力，并采用进程空洞技术在容器进程中构建分析环境，实现分析环境与目标程序的解耦，兼顾复用现有平台功能和降低程序执行影响。

Result: 在Windows平台实现了原型系统，并通过大量基准测试和实际程序验证了框架的有效性和性能，能准确分析实际漏洞利用和恶意代码。

Conclusion: 该框架显著提升了细粒度动态二进制分析的实用性和性能，具备良好的应用价值和推广潜力。

Abstract: Binary program analysis is still very important in system security. There are many practical achievements in binary code analysis, but fine-grained analysis such as dynamic taint analysis, is constantly studied due to the problem of deployability, high memory usage, and performance overhead, so as to better adapt to the new analysis scenario, such as memory corruption exploits and sandbox evasion malware. This paper presents a new binary program analysis framework, in order to improve the usability and performance of fine-grained analysis. The framework mainly uses the kernel module to further expand the analysis capability of the traditional dynamic binary instrumentation. Then, based on the idea of decoupling analysis, the analysis environment is constructed in the container process through process hollowing techniques in a new way. It can reuse the functions of the existing dynamic binary instrumentation platforms and also reduce the impact on the execution of the target program. The prototype is implemented on the Windows platform. The validity and performance of the framework are verified by a large number of experiments with benchmark and actual programs. The effectiveness of the framework is also verified by the analysis of actual exploit programs and malicious code, demonstrating the value of the practical application.

</details>


### [47] [Proceedings First Workshop on Adaptable Cloud Architectures](https://arxiv.org/abs/2512.22054)
*Giuseppe De Palma,Saverio Giallorenzo*

Main category: cs.SE

TL;DR: 本文是2025年6月20日在法国里尔举办的“适应性云架构”研讨会的后续论文集。


<details>
  <summary>Details</summary>
Motivation: 探讨适应性云架构领域的最新研究和进展，促进学术交流。

Method: 组织专题研讨会，收集并发表相关领域的最新研究论文。

Result: 汇编了多篇关于适应性云架构的研究论文，反映该领域的研究现状。

Conclusion: 通过此次研讨会推动了适应性云架构领域的学术发展与合作。

Abstract: This volume contains the post-proceedings of the Workshop on Adaptable Cloud Architectures (WACA 2025), held on June 20, 2025, in Lille, France, co-located with DisCoTec 2025 - 20th International Federated Conference on Distributed Computing Techniques.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [48] [The AI Committee: A Multi-Agent Framework for Automated Validation and Remediation of Web-Sourced Data](https://arxiv.org/abs/2512.21481)
*Sunith Vallabhaneni,Thomas Berkane,Maimuna Majumder*

Main category: cs.MA

TL;DR: 该论文提出了一种名为AI委员会的多代理系统，用于自动验证和补救网络数据集，显著提高数据完整性和准确性。


<details>
  <summary>Details</summary>
Motivation: 手工收集和验证网络数据集工作量大且易出错，现有利用大语言模型的自动化工具在数据有效性保证上表现不足。

Method: 设计了一个模型不可知的多代理系统，每个代理负责数据质量保证流程中的不同任务，如源头审查、事实核查、数据修复和完整性验证，利用LLM的上下文学习、链式推理及自我纠错能力，无需特定任务训练。

Result: 系统在三个真实数据集上的应用结果显示，AI委员会适用于多种LLM，数据完整率最高达78.7%，准确率达100%，且优于基线方法。消融实验验证各代理的贡献。

Conclusion: AI委员会有效提升了网络数据集的质量保障能力，具有良好通用性和实用价值，并作为开源工具供研究社区使用。

Abstract: Many research areas rely on data from the web to gain insights and test their methods. However, collecting comprehensive research datasets often demands manually reviewing many web pages to identify and record relevant data points, which is labor-intensive and susceptible to error. While the emergence of large language models (LLM)-powered web agents has begun to automate parts of this process, they often struggle to ensure the validity of the data they collect. Indeed, these agents exhibit several recurring failure modes - including hallucinating or omitting values, misinterpreting page semantics, and failing to detect invalid information - which are subtle and difficult to detect and correct manually. To address this, we introduce the AI Committee, a novel model-agnostic multi-agent system that automates the process of validating and remediating web-sourced datasets. Each agent is specialized in a distinct task in the data quality assurance pipeline, from source scrutiny and fact-checking to data remediation and integrity validation. The AI Committee leverages various LLM capabilities - including in-context learning for dataset adaptation, chain-of-thought reasoning for complex semantic validation, and a self-correction loop for data remediation - all without task-specific training. We demonstrate the effectiveness of our system by applying it to three real-world datasets, showing that it generalizes across LLMs and significantly outperforms baseline approaches, achieving data completeness up to 78.7% and precision up to 100%. We additionally conduct an ablation study demonstrating the contribution of each agent to the Committee's performance. This work is released as an open-source tool for the research community.

</details>


### [49] [PERELMAN: Pipeline for scientific literature meta-analysis. Technical report](https://arxiv.org/abs/2512.21727)
*Daniil Sherki,Daniil Merkulov,Alexandra Savina,Ekaterina Muravleva*

Main category: cs.MA

TL;DR: 提出了PERELMAN框架，通过结构化对话获取领域知识，指导多阶段协同代理从科学文献中提取信息，实现异构内容的统一机器可读表示，提升大规模文献综述和元分析效率。


<details>
  <summary>Details</summary>
Motivation: 当前大规模科学文献综述和元分析效率低，需要一种自动化框架将异质信息转化为统一格式，减少准备时间。

Method: 通过与领域专家结构化对话获取领域知识，指导多阶段协同代理从文本、表格和图表中提取证据，实现信息统一和一致聚合。

Result: 在锂离子电池正极材料元分析任务中成功实现系统复现，验证了方法的可重复性和有效性。

Conclusion: PERELMAN显著缩短了元分析准备时间，有潜力将准备时间从数月减少到数分钟，提高了文献综述和元分析的效率。

Abstract: We present PERELMAN (PipEline foR sciEntific Literature Meta-ANalysis), an agentic framework designed to extract specific information from a large corpus of scientific articles to support large-scale literature reviews and meta-analyses. Our central goal is to reliably transform heterogeneous article content into a unified, machine-readable representation. PERELMAN first elicits domain knowledge-including target variables, inclusion criteria, units, and normalization rules-through a structured dialogue with a subject-matter expert. This domain knowledge is then reused across multiple stages of the pipeline and guides coordinated agents in extracting evidence from narrative text, tables, and figures, enabling consistent aggregation across studies. In order to assess reproducibility and validate our implementation, we evaluate the system on the task of reproducing the meta-analysis of layered Li-ion cathode properties (NMC811 material). We describe our solution, which has the potential to reduce the time required to prepare meta-analyses from months to minutes.

</details>


### [50] [MASFIN: A Multi-Agent System for Decomposed Financial Reasoning and Forecasting](https://arxiv.org/abs/2512.21878)
*Marc S. Montalvo,Hamed Yaghoobian*

Main category: cs.MA

TL;DR: MASFIN 利用多智能体框架结合大语言模型和金融数据，实现短期股票投资组合优化，并在实测中表现优于主要市场指数。


<details>
  <summary>Details</summary>
Motivation: 传统量化方法存在幸存者偏差，AI方法在信号整合和可重复性方面存在挑战。

Method: 设计MASFIN框架，将LLM与结构化金融指标和非结构化新闻结合，并嵌入偏差缓解机制，使用GPT-4.1-nano进行高效推断，生成每周15-30只股票的投资组合。

Result: 在为期八周的评估中，MASFIN实现7.33%的累计收益率，六周超过S&P 500、NASDAQ-100和Dow Jones，尽管波动性较高。

Conclusion: 偏差意识生成式AI框架在金融预测中展现出潜力，模块化多智能体设计有助于推进量化金融的实用性、透明性和可重复性。

Abstract: Recent advances in large language models (LLMs) are transforming data-intensive domains, with finance representing a high-stakes environment where transparent and reproducible analysis of heterogeneous signals is essential. Traditional quantitative methods remain vulnerable to survivorship bias, while many AI-driven approaches struggle with signal integration, reproducibility, and computational efficiency. We introduce MASFIN, a modular multi-agent framework that integrates LLMs with structured financial metrics and unstructured news, while embedding explicit bias-mitigation protocols. The system leverages GPT-4.1-nano for reproducability and cost-efficient inference and generates weekly portfolios of 15-30 equities with allocation weights optimized for short-term performance. In an eight-week evaluation, MASFIN delivered a 7.33% cumulative return, outperforming the S&P 500, NASDAQ-100, and Dow Jones benchmarks in six of eight weeks, albeit with higher volatility. These findings demonstrate the promise of bias-aware, generative AI frameworks for financial forecasting and highlight opportunities for modular multi-agent design to advance practical, transparent, and reproducible approaches in quantitative finance.

</details>
