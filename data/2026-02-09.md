<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 55]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.SE](#cs.SE) [Total: 9]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Recontextualizing Famous Quotes for Brand Slogan Generation](https://arxiv.org/abs/2602.06049)
*Ziao Yang,Zizhang Chen,Lei Zhang,Hongfu Liu*

Main category: cs.CL

TL;DR: 该论文提出基于名言改写的模块化口号生成框架，有效提升广告口号的新颖性和品牌个性，优于现有大型语言模型方法。


<details>
  <summary>Details</summary>
Motivation: 由于广告口号的重复使用导致广告疲劳，现有基于大型语言模型的口号生成缺乏风格多样性和品牌个性，亟需创新且具有洞察力的口号生成方法。

Method: 引入了一个模块化框架，将口号生成任务分解为名言匹配、结构分解、词汇替换和重混生成等可解释子任务。

Result: 通过自动和人工评测，提出的方法在多样性、新颖性、情感影响力及人类偏好度方面相比三种最先进的大型语言模型基线均有边际提升。

Conclusion: 该研究提出了一种基于名言重新语境化的新范式，用于生成既新颖又富有品牌个性的广告口号，成功提升了口号的多样性、新颖性、情感影响力和人类偏好度。

Abstract: Slogans are concise and memorable catchphrases that play a crucial role in advertising by conveying brand identity and shaping public perception. However, advertising fatigue reduces the effectiveness of repeated slogans, creating a growing demand for novel, creative, and insightful slogan generation. While recent work leverages large language models (LLMs) for this task, existing approaches often produce stylistically redundant outputs that lack a clear brand persona and appear overtly machine-generated. We argue that effective slogans should balance novelty with familiarity and propose a new paradigm that recontextualizes persona-related famous quotes for slogan generation. Well-known quotes naturally align with slogan-length text, employ rich rhetorical devices, and offer depth and insight, making them a powerful resource for creative generation. Technically, we introduce a modular framework that decomposes slogan generation into interpretable subtasks, including quote matching, structural decomposition, vocabulary replacement, and remix generation. Extensive automatic and human evaluations demonstrate marginal improvements in diversity, novelty, emotional impact, and human preference over three state-of-the-art LLM baselines.

</details>


### [2] [Relevance-aware Multi-context Contrastive Decoding for Retrieval-augmented Visual Question Answering](https://arxiv.org/abs/2602.06050)
*Jongha Kim,Byungoh Ko,Jeehye Na,Jinsung Yoon,Hyunwoo J. Kim*

Main category: cs.CL

TL;DR: 本论文提出RMCD，一种针对RAG的多上下文相关性解码方法，通过加权整合多个上下文信息，提升LVLMs在知识密集视觉问答任务的性能，且无需额外训练，取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 当前RAG的解码方法未能充分利用多个相关上下文信息，且难以抑制无关上下文的负面影响，导致性能受限。

Method: 提出了一种新的解码方法Relevance-aware Multi-context Contrastive Decoding (RMCD)，通过对每个上下文预测的输出进行相关性加权整合，抑制无关上下文的负面影响。

Result: RMCD在多个LVLM和三个知识密集型视觉问答基准测试中表现最佳，且能无缝替换LVLM的解码方法无需再训练，且对检索结果具备高度鲁棒性。

Conclusion: RMCD方法通过加权整合多上下文的信息，有效提升了RAG在LVLM中的解码性能，显著优于其他解码方法，并且具有良好的鲁棒性。

Abstract: Despite the remarkable capabilities of Large Vision Language Models (LVLMs), they still lack detailed knowledge about specific entities. Retrieval-augmented Generation (RAG) is a widely adopted solution that enhances LVLMs by providing additional contexts from an external Knowledge Base. However, we observe that previous decoding methods for RAG are sub-optimal as they fail to sufficiently leverage multiple relevant contexts and suppress the negative effects of irrelevant contexts. To this end, we propose Relevance-aware Multi-context Contrastive Decoding (RMCD), a novel decoding method for RAG. RMCD outputs a final prediction by combining outputs predicted with each context, where each output is weighted based on its relevance to the question. By doing so, RMCD effectively aggregates useful information from multiple relevant contexts while also counteracting the negative effects of irrelevant ones. Experiments show that RMCD consistently outperforms other decoding methods across multiple LVLMs, achieving the best performance on three knowledge-intensive visual question-answering benchmarks. Also, RMCD can be simply applied by replacing the decoding method of LVLMs without additional training. Analyses also show that RMCD is robust to the retrieval results, consistently performing the best across the weakest to the strongest retrieval results. Code is available at https://github.com/mlvlab/RMCD.

</details>


### [3] [CAST: Character-and-Scene Episodic Memory for Agents](https://arxiv.org/abs/2602.06051)
*Kexin Ma,Bojun Li,Yuhua Tang,Ruochun Jin,Liting Sun*

Main category: cs.CL

TL;DR: 本文提出CAST记忆架构，通过角色与3D场景建模增强代理情景记忆，实现了对复杂事件的更好表示和检索，效果明显优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前大多数代理记忆系统侧重语义回忆，难以表示和检索连贯的事件，限制了其对情景记忆的表达能力。

Method: 提出了基于戏剧理论启发的CAST记忆架构，利用3D场景（时间/地点/主题）组织事件，构建角色档案来表示情景记忆；同时结合基于图的语义记忆，实现双重记忆设计。

Result: CAST在多个数据集上相比基线模型平均提升了8.11%的F1值和10.21%的J分数，尤其在开放性和时间敏感的对话问题上表现突出。

Conclusion: CAST模型通过构建基于角色和场景的三维事件记忆，有效提升了代理系统对连贯事件的记忆与检索能力，显著优于传统仅依赖语义记忆的方法。

Abstract: Episodic memory is a central component of human memory, which refers to the ability to recall coherent events grounded in who, when, and where. However, most agent memory systems only emphasize semantic recall and treat experience as structures such as key-value, vector, or graph, which makes them struggle to represent and retrieve coherent events. To address this challenge, we propose a Character-and-Scene based memory architecture(CAST) inspired by dramatic theory. Specifically, CAST constructs 3D scenes (time/place/topic) and organizes them into character profiles that summarize the events of a character to represent episodic memory. Moreover, CAST complements this episodic memory with a graph-based semantic memory, which yields a robust dual memory design. Experiments demonstrate that CAST has averagely improved 8.11% F1 and 10.21% J(LLM-as-a-Judge) than baselines on various datasets, especially on open and time-sensitive conversational questions.

</details>


### [4] [Rethinking Memory Mechanisms of Foundation Agents in the Second Half](https://arxiv.org/abs/2602.06052)
*Wei-Chieh Huang,Weizhi Zhang,Yueqing Liang,Yuanchen Bei,Yankai Chen,Tao Feng,Xinyu Pan,Zhen Tan,Yu Wang,Tianxin Wei,Shanglin Wu,Ruiyao Xu,Liangwei Yang,Rui Yang,Wooseong Yang,Chin-Yuan Yeh,Hanrong Zhang,Haozhen Zhang,Siqi Zhu,Henry Peng Zou,Wanjia Zhao,Song Wang,Wujiang Xu,Zixuan Ke,Zheng Hui,Dawei Li,Yaozu Wu,Langzhou He,Chen Wang,Xiongxiao Xu,Baixiang Huang,Juntao Tan,Shelby Heinecke,Huan Wang,Caiming Xiong,Ahmed A. Metwally,Jun Yan,Chen-Yu Lee,Hanqing Zeng,Yinglong Xia,Xiaokai Wei,Ali Payani,Yu Wang,Haitong Ma,Wenya Wang,Chengguang Wang,Yu Zhang,Xin Wang,Yongfeng Zhang,Jiaxuan You,Hanghang Tong,Xiao Luo,Yizhou Sun,Wei Wang,Julian McAuley,James Zou,Jiawei Han,Philip S. Yu,Kai Shu*

Main category: cs.CL

TL;DR: 人工智能代理在复杂长期环境中的实用性依赖于记忆机制，本综述系统梳理了记忆的分类、实现、学习和评估方法，提出未来研究重点。


<details>
  <summary>Details</summary>
Motivation: 当前人工智能研究重点由单纯模型创新转向解决长期动态环境中的实际效用问题，而记忆机制是填补效用缺口的关键技术。

Method: 通过从记忆基质、认知机制和记忆主体三维度统一研究基础代理记忆，并分析不同代理结构下的记忆实现及其学习策略，综合评述评估基准与指标。

Result: 归纳了记忆的多维分类和实现方式，指出了记忆操作的学习策略，并总结了现有评估方法，明确未来研究挑战。

Conclusion: 记忆成为实现人工智能代理在长期动态环境中实用性的关键，未来研究需侧重记忆机制的优化和实际效用评估。

Abstract: The research of artificial intelligence is undergoing a paradigm shift from prioritizing model innovations over benchmark scores towards emphasizing problem definition and rigorous real-world evaluation. As the field enters the "second half," the central challenge becomes real utility in long-horizon, dynamic, and user-dependent environments, where agents face context explosion and must continuously accumulate, manage, and selectively reuse large volumes of information across extended interactions. Memory, with hundreds of papers released this year, therefore emerges as the critical solution to fill the utility gap. In this survey, we provide a unified view of foundation agent memory along three dimensions: memory substrate (internal and external), cognitive mechanism (episodic, semantic, sensory, working, and procedural), and memory subject (agent- and user-centric). We then analyze how memory is instantiated and operated under different agent topologies and highlight learning policies over memory operations. Finally, we review evaluation benchmarks and metrics for assessing memory utility, and outline various open challenges and future directions.

</details>


### [5] [PersonaPlex: Voice and Role Control for Full Duplex Conversational Speech Models](https://arxiv.org/abs/2602.06053)
*Rajarshi Roy,Jonathan Raiman,Sang-gil Lee,Teodor-Dumitru Ene,Robert Kirby,Sungwon Kim,Jaehyeon Kim,Bryan Catanzaro*

Main category: cs.CL

TL;DR: PersonaPlex是一种新型双工对话语音模型，通过结合文本和语音提示，实现多角色和个性化语音的自然对话，优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有的双工语音模型局限于固定角色和语音，难以支持结构化、基于角色的应用和个性化交互，需突破角色和声音的限制。

Method: 引入混合系统提示，结合文本提示的角色条件化和语音样本的语音克隆，基于大规模合成数据集训练模型，数据由开源大语言模型和文本转语音模型生成。

Result: PersonaPlex在多角色客户服务场景中表现出强角色条件化行为、语音条件化能力和自然的对话响应，超过了现有最先进的双工语音模型和混合大语言模型语音系统。

Conclusion: PersonaPlex模型成功实现了角色条件化和语音克隆，提高了对多角色对话场景的适应能力，实现了自然、低延迟的对话交互。

Abstract: Recent advances in duplex speech models have enabled natural, low-latency speech-to-speech interactions. However, existing models are restricted to a fixed role and voice, limiting their ability to support structured, role-driven real-world applications and personalized interactions. In this work, we introduce PersonaPlex, a duplex conversational speech model that incorporates hybrid system prompts, combining role conditioning with text prompts and voice cloning with speech samples. PersonaPlex is trained on a large-scale synthetic dataset of paired prompts and user-agent conversations, generated with open-source large language models (LLM) and text-to-speech (TTS) models. To evaluate role conditioning in real-world settings, we extend the Full-Duplex-Bench benchmark beyond a single assistant role to multi-role customer service scenarios. Experiments show that PersonaPlex achieves strong role-conditioned behavior, voice-conditioned speech, and natural conversational responsiveness, surpassing state-of-the-art duplex speech models and hybrid large language model-based speech systems in role adherence, speaker similarity, latency, and naturalness.

</details>


### [6] [What Is Novel? A Knowledge-Driven Framework for Bias-Aware Literature Originality Evaluation](https://arxiv.org/abs/2602.06054)
*Abeer Mostafa,Thi Huyen Nguyen,Zahra Ahmadi*

Main category: cs.CL

TL;DR: 通过学习人类评审报告和构建结构化对比，本文开发了一个能更准确、一致评估科研新颖性的智能系统。


<details>
  <summary>Details</summary>
Motivation: 科研新颖性的评审通常主观且缺少全面的先行研究对比，导致评判不一致和不准确。

Method: 利用近8万条顶级AI会议带注释的评审报告微调大语言模型，结合结构化表示和相似度图，进行细粒度概念级对比分析。

Result: 系统产生的校准新颖性分数和类人的解释评估，减少了新颖性过高估计，提升了评估一致性。

Conclusion: 本文提出的文献感知新颖性评估框架能有效提升科研新颖性判断的准确性和一致性。

Abstract: Assessing research novelty is a core yet highly subjective aspect of peer review, typically based on implicit judgment and incomplete comparison to prior work. We introduce a literature-aware novelty assessment framework that explicitly learns how humans judge novelty from peer-review reports and grounds these judgments in structured comparison to existing research. Using nearly 80K novelty-annotated reviews from top-tier AI conferences, we fine-tune a large language model to capture reviewer-aligned novelty evaluation behavior. For a given manuscript, the system extracts structured representations of its ideas, methods, and claims, retrieves semantically related papers, and constructs a similarity graph that enables fine-grained, concept-level comparison to prior work. Conditioning on this structured evidence, the model produces calibrated novelty scores and human-like explanatory assessments, reducing overestimation and improving consistency relative to existing approaches.

</details>


### [7] [Quantifying and Attributing Polarization to Annotator Groups](https://arxiv.org/abs/2602.06055)
*Dimitris Tsirmpas,John Pavlopoulos*

Main category: cs.CL

TL;DR: 提出一种新指标衡量不同注释者群体间的极化，应用于仇恨言论和毒性检测数据，发现种族和教育背景显著影响注释一致性，提供工具便于群体间比较和多标签分析。


<details>
  <summary>Details</summary>
Motivation: 现有注释一致性指标不适合进行群体间分析，且对群体大小不平衡敏感，限制了在主观性任务（如毒性和仇恨言论检测）中的有效应用，因此需要一种能量化不同注释者群体极化的新方法。

Method: 提出了一种新的量化极化指标，结合统计显著性检验，用于衡量和比较不同注释者群体之间的极化程度，支持多标签设置，克服了现有指标对群体大小敏感和单注释限制的缺点。

Result: 利用该指标分析三个仇恨言论数据集和一个毒性检测数据集，发现种族对极化影响最大，宗教信仰导致的分歧在不同群体间呈动态变化，教育程度高的注释者间一致性较高，此外估计了获得稳健结果所需的最小注释人数，并提供了开源Python库实现。

Conclusion: 不同社会人口和意识形态子群体之间存在显著的注释极化现象，特别是在种族和教育背景方面，并且这些极化现象在仇恨言论检测任务中尤为突出。宗教信仰差异也影响注释结果，但其影响较为复杂。

Abstract: Current annotation agreement metrics are not well-suited for inter-group analysis, are sensitive to group size imbalances and restricted to single-annotation settings. These restrictions render them insufficient for many subjective tasks such as toxicity and hate-speech detection. For this reason, we introduce a quantifiable metric, paired with a statistical significance test, that attributes polarization to various annotator groups. Our metric enables direct comparisons between heavily imbalanced sociodemographic and ideological subgroups across different datasets and tasks, while also enabling analysis on multi-label settings. We apply this metric to three datasets on hate speech, and one on toxicity detection, discovering that: (1) Polarization is strongly and persistently attributed to annotator race, especially on the hate speech task. (2) Religious annotators do not fundamentally disagree with each other, but do with other annotators, a trend that is gradually diminished and then reversed with irreligious annotators. (3) Less educated annotators are more subjective, while educated ones tend to broadly agree more between themselves. Overall, our results reflect current findings around annotation patterns for various subgroups. Finally, we estimate the minimum number of annotators needed to obtain robust results, and provide an open-source Python library that implements our metric.

</details>


### [8] [Stop the Flip-Flop: Context-Preserving Verification for Fast Revocable Diffusion Decoding](https://arxiv.org/abs/2602.06161)
*Yanzheng Xiang,Lan Wei,Yizhen Yao,Qinglin Zhu,Hanqi Yan,Chen Jin,Philip Alexander Teare,Dandan Zhang,Lin Gui,Amrutha Saseendran,Yulan He*

Main category: cs.CL

TL;DR: COVER通过创新的缓存覆盖验证方法，提高了并行扩散语言模型的解码效率并保持了生成文本质量。


<details>
  <summary>Details</summary>
Motivation: 现有的可撤销并行扩散解码方案容易导致令牌反复掩码和恢复，造成推理速度下降和条件上下文质量减弱。

Method: 提出了COVER（Cache Override Verification for Efficient Revision）方法，通过KV缓存覆盖实现单次前向传递中的留一法验证与稳定草案生成，利用两个注意力视图避免自泄漏，并引入基于稳定性的种子优先选择策略。

Result: 在各项基准测试中，COVER显著减少了无效修正，提升了解码速度，并且保持了模型输出的质量。

Conclusion: COVER方法有效减少了不必要的修正步骤，加速了扩散语言模型的解码过程，同时保持了输出质量。

Abstract: Parallel diffusion decoding can accelerate diffusion language model inference by unmasking multiple tokens per step, but aggressive parallelism often harms quality. Revocable decoding mitigates this by rechecking earlier tokens, yet we observe that existing verification schemes frequently trigger flip-flop oscillations, where tokens are remasked and later restored unchanged. This behaviour slows inference in two ways: remasking verified positions weakens the conditioning context for parallel drafting, and repeated remask cycles consume the revision budget with little net progress. We propose COVER (Cache Override Verification for Efficient Revision), which performs leave-one-out verification and stable drafting within a single forward pass. COVER constructs two attention views via KV cache override: selected seeds are masked for verification, while their cached key value states are injected for all other queries to preserve contextual information, with a closed form diagonal correction preventing self leakage at the seed positions. COVER further prioritises seeds using a stability aware score that balances uncertainty, downstream influence, and cache drift, and it adapts the number of verified seeds per step. Across benchmarks, COVER markedly reduces unnecessary revisions and yields faster decoding while preserving output quality.

</details>


### [9] [Uncertainty Drives Social Bias Changes in Quantized Large Language Models](https://arxiv.org/abs/2602.06181)
*Stanley Z. Hua,Sanae Lotfi,Irene Y. Chen*

Main category: cs.CL

TL;DR: 量化压缩改变了语言模型的偏见行为，导致偏见翻转且对不同群体影响不均匀，需专门评估与干预保障公平性。


<details>
  <summary>Details</summary>
Motivation: 后训练量化虽然降低了大模型的计算成本，但其对模型偏见的影响尚未被充分理解，现有的聚合指标未能揭示其中的细节。

Method: 开发并使用了PostTrainingBiasBench这一统一的基准，包含13个闭合和开放式偏见数据集，对50个量化后的模型进行了大规模偏见评估。

Result: 发现了“量化诱导的掩码偏见翻转”现象，即多达21%的模型响应在量化后从有偏见状态变为无偏见，反之亦然，这与模型的不确定性有关。较低比特量化（4-bit）比高比特（8-bit）带来更多行为变化。偏见的影响在不同群体间并不对称，有些群体偏见加剧达到18.6%，有些群体则改善14.1%，使得整体偏见指标看似中立。大模型并无明显的鲁棒性优势，且不同模型族群的群体偏见变化不可预测。

Conclusion: 量化压缩会根本性地改变大语言模型的社会偏见表现，且这些改变在总体指标中难以察觉。需要在量化后进行专门的偏见评估和干预，以确保模型的可靠性。

Abstract: Post-training quantization reduces the computational cost of large language models but fundamentally alters their social biases in ways that aggregate metrics fail to capture. We present the first large-scale study of 50 quantized models evaluated on PostTrainingBiasBench, a unified benchmark of 13 closed- and open-ended bias datasets. We identify a phenomenon we term quantization-induced masked bias flipping, in which up to 21% of responses flip between biased and unbiased states after quantization, despite showing no change in aggregate bias scores. These flips are strongly driven by model uncertainty, where the responses with high uncertainty are 3-11x more likely to change than the confident ones. Quantization strength amplifies this effect, with 4-bit quantized models exhibiting 4-6x more behavioral changes than 8-bit quantized models. Critically, these changes create asymmetric impacts across demographic groups, where bias can worsen by up to 18.6% for some groups while improving by 14.1% for others, yielding misleadingly neutral aggregate outcomes. Larger models show no consistent robustness advantage, and group-specific shifts vary unpredictably across model families. Our findings demonstrate that compression fundamentally alters bias patterns, requiring crucial post-quantization evaluation and interventions to ensure reliability in practice.

</details>


### [10] [BenchMarker: An Education-Inspired Toolkit for Highlighting Flaws in Multiple-Choice Benchmarks](https://arxiv.org/abs/2602.06221)
*Nishant Balepur,Bhavya Rajasekaran,Jane Oh,Michael Xie,Atrey Desai,Vipul Gupta,Steven James Moore,Eunsol Choi,Rachel Rudinger,Jordan Lee Boyd-Graber*

Main category: cs.CL

TL;DR: 本文提出了BenchMarker工具，借助大语言模型和教育规则自动检测多项选择题基准中的常见缺陷，发现这些缺陷显著影响模型评测结果并呼吁将教育研究方法应用于NLP基准设计。


<details>
  <summary>Details</summary>
Motivation: 现有多项选择题问答基准缺乏严格的质量控制，存在题目污染、答案线索和写作错误等问题，影响模型评测的公正性和准确性。

Method: 提出BenchMarker工具，利用大型语言模型（LLM）作为评测者，基于教育领域的19条规则对多项选择题进行三类常见缺陷（题目污染、捷径线索、写作错误）的自动检测，并通过人工注释验证其有效性。

Result: BenchMarker对12个MCQA基准进行了审计，发现题目污染会虚高模型准确率，写作错误则会降低准确率并影响排名，且现有的修复措施虽改善了部分问题，却引入了新的缺陷。

Conclusion: MCQA基准测试中存在普遍的质量缺陷，这些缺陷影响了模型评估的准确性和可靠性。通过引入教育领域的方法和工具，可以有效识别和改善这些问题，从而提升MCQA基准的设计质量。

Abstract: Multiple-choice question answering (MCQA) is standard in NLP, but benchmarks lack rigorous quality control. We present BenchMarker, an education-inspired toolkit using LLM judges to flag three common MCQ flaws: 1) contamination - items appearing exactly online; 2) shortcuts - cues in the choices that enable guessing; and 3) writing errors - structural/grammatical issues based on a 19-rule education rubric. We validate BenchMarker with human annotations, then run the tool to audit 12 benchmarks, revealing: 2) contaminated MCQs tend to inflate accuracy, while writing errors tend to lower it and change rankings beyond random; and 3) prior benchmark repairs address their targeted issues (i.e., lowering accuracy with LLM-written distractors), but inadvertently add new flaws (i.e. implausible distractors, many correct answers). Overall, flaws in MCQs degrade NLP evaluation, but education research offers a path forward. We release BenchMarker to bridge the fields and improve MCQA benchmark design.

</details>


### [11] [Can One-sided Arguments Lead to Response Change in Large Language Models?](https://arxiv.org/abs/2602.06260)
*Pedro Cisneros-Velarde*

Main category: cs.CL

TL;DR: 本文研究通过提供单边论据能简单有效地引导大型语言模型对争议问题表达特定观点，并验证了该方法在多种条件下的有效性。


<details>
  <summary>Details</summary>
Motivation: 极具争议的问题需要多角度平衡回答，但LLM往往只给出单一观点或拒绝回答，探究是否可通过单方面论据简单直观地引导观点。

Method: 系统研究了三维度(观点诱导、问题表述、论据展示)如何影响LLM的观点倾向，构建了一个小型数据集并进行实验。

Result: 发现无论模型种类、论据数量、话题如何，观点引导普遍存在；切换论据内容能显著减弱该引导效果。

Conclusion: 通过提供单方面支持某个观点的论据，可以有效引导大型语言模型（LLMs）朝向特定观点回应极具争议的问题。

Abstract: Polemic questions need more than one viewpoint to express a balanced answer. Large Language Models (LLMs) can provide a balanced answer, but also take a single aligned viewpoint or refuse to answer. In this paper, we study if such initial responses can be steered to a specific viewpoint in a simple and intuitive way: by only providing one-sided arguments supporting the viewpoint. Our systematic study has three dimensions: (i) which stance is induced in the LLM response, (ii) how the polemic question is formulated, (iii) how the arguments are shown. We construct a small dataset and remarkably find that opinion steering occurs across (i)-(iii) for diverse models, number of arguments, and topics. Switching to other arguments consistently decreases opinion steering.

</details>


### [12] [Is my model "mind blurting"? Interpreting the dynamics of reasoning tokens with Recurrence Quantification Analysis (RQA)](https://arxiv.org/abs/2602.06266)
*Quoc Tuan Pham,Mehdi Jafari,Flora Salim*

Main category: cs.CL

TL;DR: 提出用递归量化分析方法研究推理模型的生成动态，更准确反映模型推理过程，提升任务复杂度预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统通过响应长度衡量推理过程不可行且不准确，需一种能够更准确反映推理动态和有效性的非文本分析方法。

Method: 将令牌生成视为动态系统，在每一步生成时提取隐藏嵌入，应用RQA方法分析生成轨迹，利用RQA指标如确定性和层状性量化模型潜在表示中的重复和停顿模式。

Result: 通过对3600个DeepSeek-R1-Distill生成轨迹的分析，RQA不仅捕捉到响应长度无法反映的信号，还将任务复杂度的预测准确率提升了8%。

Conclusion: 该论文提出使用递归量化分析(RQA)作为分析大型推理模型测试时推理链的非文本方法，有效捕捉模型生成过程中的动态模式，超过传统响应长度指标。

Abstract: Test-time compute is central to large reasoning models, yet analysing their reasoning behaviour through generated text is increasingly impractical and unreliable. Response length is often used as a brute proxy for reasoning effort, but this metric fails to capture the dynamics and effectiveness of the Chain of Thoughts (CoT) or the generated tokens. We propose Recurrence Quantification Analysis (RQA) as a non-textual alternative for analysing model's reasoning chains at test time. By treating token generation as a dynamical system, we extract hidden embeddings at each generation step and apply RQA to the resulting trajectories. RQA metrics, including Determinism and Laminarity, quantify patterns of repetition and stalling in the model's latent representations. Analysing 3,600 generation traces from DeepSeek-R1-Distill, we show that RQA captures signals not reflected by response length, but also substantially improves prediction of task complexity by 8\%. These results help establish RQA as a principled tool for studying the latent token generation dynamics of test-time scaling in reasoning models.

</details>


### [13] [MPIB: A Benchmark for Medical Prompt Injection Attacks and Clinical Safety in LLMs](https://arxiv.org/abs/2602.06268)
*Junhyeok Lee,Han Jang,Kyu Sung Choi*

Main category: cs.CL

TL;DR: 本文提出了MPIB，一个针对临床大型语言模型提示注入攻击的评估基准，利用临床损伤事件率等指标，发现攻击成功率与实际临床风险存在差异，强调防御策略需考虑攻击指令位置，并开放数据代码支持相关研究。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型与检索增强生成系统在临床应用中面临提示注入攻击风险，可能导致不安全或误导性输出，亟需标准化基准数据和指标来评估和提升临床安全性。

Method: 构建了医疗提示注入基准数据集MPIB，包括9697个实例，结合临床损伤事件率（CHER）和攻击成功率（ASR）指标，评估直接提示注入和RAG介导的间接注入对临床工作流的影响。

Result: 通过在多种基线模型和防御配置上测试MPIB，发现ASR与CHER常有显著差异，鲁棒性关键取决于攻击指令出现于用户查询还是检索上下文中，提供了开放源码和数据支持后续研究。

Conclusion: 通过MPIB评估发现，提示注入攻击对临床大型语言模型的安全性构成显著风险，且攻击成功率与临床危害事件比率存在差异，防御效果依赖于攻击指令出现的位置。

Abstract: Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems are increasingly integrated into clinical workflows; however, prompt injection attacks can steer these systems toward clinically unsafe or misleading outputs. We introduce the Medical Prompt Injection Benchmark (MPIB), a dataset-and-benchmark suite for evaluating clinical safety under both direct prompt injection and indirect, RAG-mediated injection across clinically grounded tasks. MPIB emphasizes outcome-level risk via the Clinical Harm Event Rate (CHER), which measures high-severity clinical harm events under a clinically grounded taxonomy, and reports CHER alongside Attack Success Rate (ASR) to disentangle instruction compliance from downstream patient risk. The benchmark comprises 9,697 curated instances constructed through multi-stage quality gates and clinical safety linting. Evaluating MPIB across a diverse set of baseline LLMs and defense configurations, we find that ASR and CHER can diverge substantially, and that robustness depends critically on whether adversarial instructions appear in the user query or in retrieved context. We release MPIB with evaluation code, adversarial baselines, and comprehensive documentation to support reproducible and systematic research on clinical prompt injection. Code and data are available at GitHub (code) and Hugging Face (data).

</details>


### [14] [VowelPrompt: Hearing Speech Emotions from Text via Vowel-level Prosodic Augmentation](https://arxiv.org/abs/2602.06270)
*Yancheng Wang,Osama Hanna,Ruiming Xie,Xianfeng Rui,Maohao Shen,Xuedong Zhang,Christian Fuegen,Jilong Wu,Debjyoti Paul,Arthur Guo,Zhihong Lei,Ozlem Kalinli,Qing He,Yingzhen Yang*

Main category: cs.CL

TL;DR: 该论文提出VowelPrompt框架，结合细粒度元音韵律特征与大语言模型，通过两阶段训练提升了语音情感识别的性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的语音情感识别忽视了细粒度韵律信息，导致性能和解释性不足。

Method: 提出基于元音韵律特征（基频、能量、时长）的自然语言描述，通过两阶段适应（监督微调+带验证奖励的强化学习）优化大语言模型的推理和泛化能力。

Result: 在多种基准数据集和条件下，VowelPrompt表现出超越现有方法的情感识别效果，并能够生成结合语义和韵律的解释性输出。

Conclusion: VowelPrompt框架将细粒度元音层韵律特征融入大语言模型，显著提升了语音情感识别的准确性和解释能力，在多领域和多语言任务中 outperform 了现有最先进方法。

Abstract: Emotion recognition in speech presents a complex multimodal challenge, requiring comprehension of both linguistic content and vocal expressivity, particularly prosodic features such as fundamental frequency, intensity, and temporal dynamics. Although large language models (LLMs) have shown promise in reasoning over textual transcriptions for emotion recognition, they typically neglect fine-grained prosodic information, limiting their effectiveness and interpretability. In this work, we propose VowelPrompt, a linguistically grounded framework that augments LLM-based emotion recognition with interpretable, fine-grained vowel-level prosodic cues. Drawing on phonetic evidence that vowels serve as primary carriers of affective prosody, VowelPrompt extracts pitch-, energy-, and duration-based descriptors from time-aligned vowel segments, and converts these features into natural language descriptions for better interpretability. Such a design enables LLMs to jointly reason over lexical semantics and fine-grained prosodic variation. Moreover, we adopt a two-stage adaptation procedure comprising supervised fine-tuning (SFT) followed by Reinforcement Learning with Verifiable Reward (RLVR), implemented via Group Relative Policy Optimization (GRPO), to enhance reasoning capability, enforce structured output adherence, and improve generalization across domains and speaker variations. Extensive evaluations across diverse benchmark datasets demonstrate that VowelPrompt consistently outperforms state-of-the-art emotion recognition methods under zero-shot, fine-tuned, cross-domain, and cross-linguistic conditions, while enabling the generation of interpretable explanations that are jointly grounded in contextual semantics and fine-grained prosodic structure.

</details>


### [15] [RoPE-LIME: RoPE-Space Locality + Sparse-K Sampling for Efficient LLM Attribution](https://arxiv.org/abs/2602.06275)
*Isaac Picov,Ritesh Goru*

Main category: cs.CL

TL;DR: 针对闭源大模型解释难题，RoPE-LIME利用开源代理模型和新颖的局部核及采样方法，实现了高效且稳定的token级归因，表现优于现有扰动和代理方法。


<details>
  <summary>Details</summary>
Motivation: 闭源大型语言模型（LLM）由于只能通过API访问，无法利用基于梯度的归因方法；而基于重生成文本的扰动方法成本高且噪声大，亟需一种高效且稳定的解释技术。

Method: 提出RoPE-LIME，一种基于较小开源代理模型的解释方法，利用概率性目标（负对数似然和散度目标）进行输入扰动下的token级归因。该方法结合了基于RoPE空间计算的Relaxed Word Mover's Distance的局部核函数和Sparse-K采样策略以提升解释稳定性和效率。

Result: 在HotpotQA（句子级特征）和手工标注的MMLU子集（单词级特征）上的实验表明，RoPE-LIME生成的归因更具信息性，交互覆盖面更广，且相比gSMILE显著减少了对闭源模型API的调用次数。

Conclusion: RoPE-LIME能够在闭源大模型输出的解释任务中，提供比传统扰动方法和gSMILE更具信息量的归因结果，且大幅减少了对闭源模型API调用的需求。

Abstract: Explaining closed-source LLM outputs is challenging because API access prevents gradient-based attribution, while perturbation methods are costly and noisy when they depend on regenerated text. We introduce RoPE-LIME, an open-source extension of gSMILE that decouples reasoning from explanation: given a fixed output from a closed model, a smaller open-source surrogate computes token-level attributions from probability-based objectives (negative log-likelihood and divergence targets) under input perturbations. RoPE-LIME incorporates (i) a locality kernel based on Relaxed Word Mover's Distance computed in RoPE embedding space for stable similarity under masking, and (ii) Sparse-K sampling, an efficient perturbation strategy that improves interaction coverage under limited budgets. Experiments on HotpotQA (sentence features) and a hand-labeled MMLU subset (word features) show that RoPE-LIME produces more informative attributions than leave-one-out sampling and improves over gSMILE while substantially reducing closed-model API calls.

</details>


### [16] [Judging What We Cannot Solve: A Consequence-Based Approach for Oracle-Free Evaluation of Research-Level Math](https://arxiv.org/abs/2602.06291)
*Guijin Son,Donghun Yang,Hitesh Laxmichand Patel,Hyunwoo Ko,Amit Agarwal,Sunghee Ahn,Kyong-Ha Lee,Youngjae Yu*

Main category: cs.CL

TL;DR: 提出了一种无监督的基于结果效用评估方法，有效提升了研究级数学问题解决方案的排序性能，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有推理模型在生成研究级数学尝试上取得进展，但验证过程依赖专家时间成为瓶颈，亟需一种有效且无需专家监督的新评估方法。

Method: 提出了无须专家监督的基于结果的效用评估器，通过测试候选解决方案在相关可验证问题中的表现，将其作为上下文示例进行评分。

Result: 在一组包含专家解和LLM生成的9个解的研究级数学问题上，基于结果的效用方法在排名质量上显著优于奖励模型和LLM评判器，GPT-OSS-120B模型准确率（Acc@1）从67.2提升至76.3，AUC从71.4提升至79.6。

Conclusion: 本文提出的基于结果的效用评估方法在对研究级数学问题的解决方案排序中表现优于现有的奖励模型和大型语言模型评判方法。

Abstract: Recent progress in reasoning models suggests that generating plausible attempts for research-level mathematics may be within reach, but verification remains a bottleneck, consuming scarce expert time. We hypothesize that a meaningful solution should contain enough method-level information that, when applied to a neighborhood of related questions, it should yield better downstream performance than incorrect solutions. Building on this idea, we propose \textbf{Consequence-Based Utility}, an oracle-free evaluator that scores each candidate by testing its value as an in-context exemplar in solving related yet verifiable questions. Our approach is evaluated on an original set of research-level math problems, each paired with one expert-written solution and nine LLM-generated solutions. Notably, Consequence-Based Utility consistently outperforms reward models, generative reward models, and LLM judges on ranking quality. Specifically, for GPT-OSS-120B, it improves Acc@1 from 67.2 to 76.3 and AUC from 71.4 to 79.6, with similarly large AUC gains on GPT-OSS-20B (69.0 to 79.2). Furthermore, compared to LLM-Judges, it also exhibits a larger solver-evaluator gap, maintaining a stronger correct-wrong separation even on instances where the underlying solver often fails to solve.

</details>


### [17] [Lost in Speech: Benchmarking, Evaluation, and Parsing of Spoken Code-Switching Beyond Standard UD Assumptions](https://arxiv.org/abs/2602.06307)
*Nemika Tyagi,Holly Hendrix,Nelvin Licona-Guevara,Justin Mackie,Phanos Kareen,Muhammad Imran,Megan Michelle Smith,Tatiana Gallego Hernande,Chitta Baral,Olga Kellert*

Main category: cs.CL

TL;DR: 本文针对口语代码转换的句法分析提出了新方法和评估指标，显著提升了分析性能和可解释性，突破了传统句法分析技术在口语数据上的局限。


<details>
  <summary>Details</summary>
Motivation: 现有的句法分析方法在处理口语中的代码转换（CSW）时表现不佳，主要因为口语数据中的断裂、不连贯、重复、省略以及话语驱动结构违反了标准的通用依存（UD）假设，且传统评估指标无法区分真正的结构错误和合理的变体。

Method: 本文提出一种面向系统的口语CSW句法分析方法，包括引入基于语言学的口语CSW现象分类法和专家标注的SpokeBench基准，设计了一种考虑歧义的评估指标FLEX-UD，并提出解耦式代理句法分析框架DECAP，将口语现象处理与核心句法分析分离。

Result: 实验显示，DECAP在无需再训练的情况下，产生更稳健、可解释的解析结果，且较现有技术性能提升高达52.6%。FLEX-UD评估揭示了标准指标掩盖的定性改进。

Conclusion: 通过引入语言学指导的分类与新的评估指标，并采用解耦式框架，本文显著提升了口语代码转换句法分析的性能和鲁棒性，克服了传统方法的不足。

Abstract: Spoken code-switching (CSW) challenges syntactic parsing in ways not observed in written text. Disfluencies, repetition, ellipsis, and discourse-driven structure routinely violate standard Universal Dependencies (UD) assumptions, causing parsers and large language models (LLMs) to fail despite strong performance on written data. These failures are compounded by rigid evaluation metrics that conflate genuine structural errors with acceptable variation. In this work, we present a systems-oriented approach to spoken CSW parsing. We introduce a linguistically grounded taxonomy of spoken CSW phenomena and SpokeBench, an expert-annotated gold benchmark designed to test spoken-language structure beyond standard UD assumptions. We further propose FLEX-UD, an ambiguity-aware evaluation metric, which reveals that existing parsing techniques perform poorly on spoken CSW by penalizing linguistically plausible analyses as errors. We then propose DECAP, a decoupled agentic parsing framework that isolates spoken-phenomena handling from core syntactic analysis. Experiments show that DECAP produces more robust and interpretable parses without retraining and achieves up to 52.6% improvements over existing parsing techniques. FLEX-UD evaluations further reveal qualitative improvements that are masked by standard metrics.

</details>


### [18] [Can Post-Training Transform LLMs into Causal Reasoners?](https://arxiv.org/abs/2602.06337)
*Junqi Chen,Sirui Chen,Chaochao Lu*

Main category: cs.CL

TL;DR: 本文通过新建的CauGym数据集和多种后期训练方法，实验证明后期训练能显著提升小型LLMs的因果推断能力，取得优异的准确率和强健的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 因果推断对于决策制定至关重要，但非专业人士难以掌握，同时大型语言模型在因果推断上的能力有限，且后期训练对其因果能力的影响尚未得到充分研究。

Method: 提出了包含七个核心因果任务和五个多样化测试集的CauGym数据集，使用该数据集系统评估了五种后期训练方法（SFT, DPO, KTO, PPO, GRPO），通过实验验证了后期训练对LLMs因果推断能力的提升效果。

Result: 14B参数模型在CaLM基准测试中达到93.5%的准确率，远超OpenAI o3模型的55.4%；后期训练的模型在真实世界数据分布变化和噪声条件下表现出强大的泛化性和鲁棒性。

Conclusion: 定向的后期训练可以显著提升小型大语言模型（LLMs）在因果推断任务中的表现，使其性能在多个基准测试中超过更大的模型，并且具备良好的泛化能力和鲁棒性。

Abstract: Causal inference is essential for decision-making but remains challenging for non-experts. While large language models (LLMs) show promise in this domain, their precise causal estimation capabilities are still limited, and the impact of post-training on these abilities is insufficiently explored. This paper examines the extent to which post-training can enhance LLMs' capacity for causal inference. We introduce CauGym, a comprehensive dataset comprising seven core causal tasks for training and five diverse test sets. Using this dataset, we systematically evaluate five post-training approaches: SFT, DPO, KTO, PPO, and GRPO. Across five in-domain and four existing benchmarks, our experiments demonstrate that appropriate post-training enables smaller LLMs to perform causal inference competitively, often surpassing much larger models. Our 14B parameter model achieves 93.5% accuracy on the CaLM benchmark, compared to 55.4% by OpenAI o3. Furthermore, the post-trained LLMs exhibit strong generalization and robustness under real-world conditions such as distribution shifts and noisy data. Collectively, these findings provide the first systematic evidence that targeted post-training can produce reliable and robust LLM-based causal reasoners. Our data and GRPO-model are available at https://github.com/OpenCausaLab/CauGym.

</details>


### [19] [SHINE: A Scalable In-Context Hypernetwork for Mapping Context to LoRA in a Single Pass](https://arxiv.org/abs/2602.06358)
*Yewei Liu,Xiyuan Wang,Yansheng Mao,Yoav Gelbery,Haggai Maron,Muhan Zhang*

Main category: cs.CL

TL;DR: 提出了SHINE，一种利用冻结大型语言模型参数的可扩展超网络，能快速生成高质量LoRA适配器，实现高效知识迁移，显著提升任务表现且节省资源。


<details>
  <summary>Details</summary>
Motivation: 现有超网络存在参数规模大、表达能力有限等问题，且微调大型语言模型成本高昂，SHINE旨在克服这些限制，实现高效、低成本的知识迁移。

Method: 采用了冻结的LLM参数作为超网络的一部分，结合架构创新，设计了预训练与指令微调流程，使超网络能在单次前向传播中生成高质量的LoRA适配器，无需对LLM进行细调。

Result: SHINE在多项任务上表现优异，显著节约了时间、计算和内存资源，展示出良好的扩展潜力。

Conclusion: SHINE通过可扩展的超网络设计，实现了从多样且有意义的上下文中高效生成高质量的LoRA适配器，显著提升了大型语言模型的知识迁移和适应能力。

Abstract: We propose SHINE (Scalable Hyper In-context NEtwork), a scalable hypernetwork that can map diverse meaningful contexts into high-quality LoRA adapters for large language models (LLM). By reusing the frozen LLM's own parameters in an in-context hypernetwork design and introducing architectural innovations, SHINE overcomes key limitations of prior hypernetworks and achieves strong expressive power with a relatively small number of parameters. We introduce a pretraining and instruction fine-tuning pipeline, and train our hypernetwork to generate high quality LoRA adapters from diverse meaningful contexts in a single forward pass. It updates LLM parameters without any fine-tuning, and immediately enables complex question answering tasks related to the context without directly accessing the context, effectively transforming in-context knowledge to in-parameter knowledge in one pass. Our work achieves outstanding results on various tasks, greatly saves time, computation and memory costs compared to SFT-based LLM adaptation, and shows great potential for scaling. Our code is available at https://github.com/Yewei-Liu/SHINE

</details>


### [20] [Cost-Aware Model Selection for Text Classification: Multi-Objective Trade-offs Between Fine-Tuned Encoders and LLM Prompting in Production](https://arxiv.org/abs/2602.06370)
*Alberto Andres Valdes Gonzalez*

Main category: cs.CL

TL;DR: 针对结构化文本分类，微调编码器比大型语言模型更高效低成本，推荐在实际系统中优先使用。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型在开放式推理和生成任务中表现强大，但在结构化文本分类任务中，通常只考虑预测性能，忽视了生产系统中的操作约束。

Method: 系统地比较零/少样本提示的大型语言模型和完全微调的编码器模型，在四个基准数据集上评估预测质量、推理延迟和成本。

Result: 微调的BERT类模型在准确率上与LLM竞争且常优，同时成本和延迟降低一到两个数量级。多目标评估显示选择模型需权衡性能与系统资源。

Conclusion: 微调的编码器模型在文本分类任务中表现出色，比零样本和少样本的大型语言模型在成本和延迟上有显著优势。

Abstract: Large language models (LLMs) such as GPT-4o and Claude Sonnet 4.5 have demonstrated strong capabilities in open-ended reasoning and generative language tasks, leading to their widespread adoption across a broad range of NLP applications. However, for structured text classification problems with fixed label spaces, model selection is often driven by predictive performance alone, overlooking operational constraints encountered in production systems.
  In this work, we present a systematic comparison of two contrasting paradigms for text classification: zero- and few-shot prompt-based large language models, and fully fine-tuned encoder-only architectures. We evaluate these approaches across four canonical benchmarks (IMDB, SST-2, AG News, and DBPedia), measuring predictive quality (macro F1), inference latency, and monetary cost.
  We frame model evaluation as a multi-objective decision problem and analyze trade-offs using Pareto frontier projections and a parameterized utility function reflecting different deployment regimes. Our results show that fine-tuned encoder-based models from the BERT family achieve competitive, and often superior, classification performance while operating at one to two orders of magnitude lower cost and latency compared to zero- and few-shot LLM prompting.
  Overall, our findings suggest that indiscriminate use of large language models for standard text classification workloads can lead to suboptimal system-level outcomes. Instead, fine-tuned encoders emerge as robust and efficient components for structured NLP pipelines, while LLMs are better positioned as complementary elements within hybrid architectures. We release all code, datasets, and evaluation protocols to support reproducibility and cost-aware NLP system design.

</details>


### [21] [ReBeCA: Unveiling Interpretable Behavior Hierarchy behind the Iterative Self-Reflection of Language Models with Causal Analysis](https://arxiv.org/abs/2602.06373)
*Tianqiang Yan,Sihan Shang,Yuheng Li,Song Qiu,Hao Peng,Wenjian Luo,Jue Xie,Lizhen Qu,Yuan Gao*

Main category: cs.CL

TL;DR: 本文提出ReBeCA框架，通过因果分析揭示语言模型自我反思的行为层级和关键因果因素，克服相关性分析局限，实现跨任务稳定优化自我反思效果。


<details>
  <summary>Details</summary>
Motivation: 现有关于自我反思增强语言模型可靠性的分析多基于相关性，难以推广，机制尚不透明。

Method: 提出ReBeCA框架，通过将自我反思轨迹建模成因果图，利用三阶段不变因果预测（ICP）管道识别真实影响因素。

Result: 发现行为层级体系，少数语义行为对自我反思效果有因果影响，且多个正向语义行为共存反而可能降低效果。因果父因素识别提升结构似然约49.6%，且结果在不同任务和数据集上稳定。

Conclusion: ReBeCA为解析自我反思中的因果机制提供严谨方法，有助区分真实因果关系与伪相关，提升语言模型自我反思的可解释性和泛化能力。

Abstract: While self-reflection can enhance language model reliability, its underlying mechanisms remain opaque, with existing analyses often yielding correlation-based insights that fail to generalize. To address this, we introduce \textbf{\texttt{ReBeCA}} (self-\textbf{\texttt{Re}}flection \textbf{\texttt{Be}}havior explained through \textbf{\texttt{C}}ausal \textbf{\texttt{A}}nalysis), a framework that unveils the interpretable behavioral hierarchy governing the self-reflection outcome. By modeling self-reflection trajectories as causal graphs, ReBeCA isolates genuine determinants of performance through a three-stage Invariant Causal Prediction (ICP) pipeline. We establish three critical findings: (1) \textbf{Behavioral hierarchy:} Semantic behaviors of the model influence final self-reflection results hierarchically: directly or indirectly; (2) \textbf{Causation matters:} Generalizability in self-reflection effects is limited to just a few semantic behaviors; (3) \textbf{More $\mathbf{\neq}$ better:} The confluence of seemingly positive semantic behaviors, even among direct causal factors, can impair the efficacy of self-reflection. ICP-based verification identifies sparse causal parents achieving up to $49.6\%$ structural likelihood gains, stable across tasks where correlation-based patterns fail. Intervention studies on novel datasets confirm these causal relationships hold out-of-distribution ($p = .013, η^2_\mathrm{p} = .071$). ReBeCA thus provides a rigorous methodology for disentangling genuine causal mechanisms from spurious associations in self-reflection dynamics.

</details>


### [22] [FMBench: Adaptive Large Language Model Output Formatting](https://arxiv.org/abs/2602.06384)
*Yaoting Wang,Yun Zhou,Henghui Ding*

Main category: cs.CL

TL;DR: 提出FMBench评测Markdown格式生成，结合监督及强化学习微调提升大模型语义和格式表现，揭示二者权衡。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在生成Markdown格式时存在的语义和格式错误问题，这些错误影响下游应用的可用性。

Method: 提出FMBench基准测试，设计一个轻量级的对齐流程，包括监督微调和强化学习微调，平衡语义一致性与结构正确性。

Result: 在OpenPangu和Qwen模型家族上，监督微调提升了语义对齐，强化学习进一步增强了对复杂Markdown指令的稳健性，揭示了语义与结构目标的权衡。

Conclusion: 结合监督微调和强化学习的对齐策略能有效提升模型生成符合格式要求且语义正确的Markdown内容，为实用场景提供支持。

Abstract: Producing outputs that satisfy both semantic intent and format constraints is essential for deploying large language models in user-facing and system-integrated workflows. In this work, we focus on Markdown formatting, which is ubiquitous in assistants, documentation, and tool-augmented pipelines but still prone to subtle, hard-to-detect errors (e.g., broken lists, malformed tables, inconsistent headings, and invalid code blocks) that can significantly degrade downstream usability. We present FMBench, a benchmark for adaptive Markdown output formatting that evaluates models under a wide range of instruction-following scenarios with diverse structural requirements. FMBench emphasizes real-world formatting behaviors such as multi-level organization, mixed content (natural language interleaved with lists/tables/code), and strict adherence to user-specified layout constraints. To improve Markdown compliance without relying on hard decoding constraints, we propose a lightweight alignment pipeline that combines supervised fine-tuning (SFT) with reinforcement learning fine-tuning. Starting from a base model, we first perform SFT on instruction-response pairs, and then optimize a composite objective that balances semantic fidelity with structural correctness. Experiments on two model families (OpenPangu and Qwen) show that SFT consistently improves semantic alignment, while reinforcement learning provides additional gains in robustness to challenging Markdown instructions when initialized from a strong SFT policy. Our results also reveal an inherent trade-off between semantic and structural objectives, highlighting the importance of carefully designed rewards for reliable formatted generation. Code is available at: https://github.com/FudanCVL/FMBench.

</details>


### [23] [Stopping Computation for Converged Tokens in Masked Diffusion-LM Decoding](https://arxiv.org/abs/2602.06412)
*Daisuke Oba,Danushka Bollegala,Masahiro Kaneko,Naoaki Okazaki*

Main category: cs.CL

TL;DR: SureLock通过锁定已稳定位置，减少Masked Diffusion模型中无效计算，显著降低计算成本同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 目前的Masked Diffusion Language Models在生成过程中对每个位置在每一步都重新计算注意力和前馈块，导致计算资源浪费，因为许多已经解码的token实际上是固定的。

Method: 提出了SureLock方法，当某个位置的后验概率在多个步骤中稳定时，将该位置锁定，跳过其查询投影和前馈子层计算，同时缓存其注意力键和值，其他位置仍能进行注意力计算。

Result: 在LLaDA-8B模型上，SureLock相较于无锁定的采样器减少了30%-50%的算法浮点运算量(FLOPs)，且保持了相当的生成质量。

Conclusion: SureLock有效节省了Masked Diffusion Language Models的计算成本，同时理论分析证明监控局部KL仅在锁定步骤即可控制最终token概率的偏差，验证了方法设计的合理性。

Abstract: Masked Diffusion Language Models generate sequences via iterative sampling that progressively unmasks tokens. However, they still recompute the attention and feed-forward blocks for every token position at every step -- even when many unmasked tokens are essentially fixed, resulting in substantial waste in compute. We propose SureLock: when the posterior at an unmasked position has stabilized across steps (our sure condition), we lock that position -- thereafter skipping its query projection and feed-forward sublayers -- while caching its attention keys and values so other positions can continue to attend to it. This reduces the dominant per-iteration computational cost from $O(N^2d)$ to $O(MNd)$ where $N$ is the sequence length, $M$ is the number of unlocked token positions, and $d$ is the model dimension. In practice, $M$ decreases as the iteration progresses, yielding substantial savings. On LLaDA-8B, SureLock reduces algorithmic FLOPs by 30--50% relative to the same sampler without locking, while maintaining comparable generation quality. We also provide a theoretical analysis to justify the design rationale of SureLock: monitoring only the local KL at the lock step suffices to bound the deviation in final token probabilities. Our code will be available at https://daioba.github.io/surelock .

</details>


### [24] [On the Wings of Imagination: Conflicting Script-based Multi-role Framework for Humor Caption Generation](https://arxiv.org/abs/2602.06423)
*Wenbo Shang,Yuxi Sun,Jing Ma,Xin Huang*

Main category: cs.CL

TL;DR: 本文提出利用幽默理论指导的多角色LLM协作方法HOMER，实现了多模态图像幽默标题的高效生成，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM幽默生成方法缺乏创造力和可解释性，难以满足多模态场景中的复杂幽默生成需求。

Method: 设计了包含三种角色的LLM框架（冲突脚本提取器、增强检索的层级想象者、标题生成器），结合幽默检索与想象树展开创意空间，实现多模态幽默生成。

Result: 在两个New Yorker Cartoon数据集上，HOMER在多模态幽默标题生成任务中优于多个基线和强大的LLM推理策略。

Conclusion: 提出的基于幽默理论GTVH的多角色LLM协作幽默生成机制HOMER，能够有效生成幽默且多样的图像标题，显著优于现有的最先进方法。

Abstract: Humor is a commonly used and intricate human language in daily life. Humor generation, especially in multi-modal scenarios, is a challenging task for large language models (LLMs), which is typically as funny caption generation for images, requiring visual understanding, humor reasoning, creative imagination, and so on. Existing LLM-based approaches rely on reasoning chains or self-improvement, which suffer from limited creativity and interpretability. To address these bottlenecks, we develop a novel LLM-based humor generation mechanism based on a fundamental humor theory, GTVH. To produce funny and script-opposite captions, we introduce a humor-theory-driven multi-role LLM collaboration framework augmented with humor retrieval (HOMER). The framework consists of three LLM-based roles: (1) conflicting-script extractor that grounds humor in key script oppositions, forming the basis of caption generation; (2) retrieval-augmented hierarchical imaginator that identifies key humor targets and expands the creative space of them through diverse associations structured as imagination trees; and (3) caption generator that produces funny and diverse captions conditioned on the obtained knowledge. Extensive experiments on two New Yorker Cartoon benchmarking datasets show that HOMER outperforms state-of-the-art baselines and powerful LLM reasoning strategies on multi-modal humor captioning.

</details>


### [25] [Investigating the structure of emotions by analyzing similarity and association of emotion words](https://arxiv.org/abs/2602.06430)
*Fumitaka Iwaki,Tatsuji Takahashi*

Main category: cs.CL

TL;DR: 本文通过语义网络分析验证了Plutchik情绪轮模型的大体有效性，但发现局部存在差异。


<details>
  <summary>Details</summary>
Motivation: 当前自然语言处理领域中情感分析常用Plutchik情绪轮模型，但其有效性未被充分验证。

Method: 通过收集情绪词的相似性和关联数据，构建语义网络，并应用社区检测分析网络结构，比较与Plutchik情绪轮的异同。

Result: 网络结构整体上与Plutchik情绪轮相似，但存在局部差异。

Conclusion: Plutchik情绪轮模型在情绪语义结构上具有较高的效度，但在局部细节方面需要进一步研究。

Abstract: In the field of natural language processing, some studies have attempted sentiment analysis on text by handling emotions as explanatory or response variables. One of the most popular emotion models used in this context is the wheel of emotion proposed by Plutchik. This model schematizes human emotions in a circular structure, and represents them in two or three dimensions. However, the validity of Plutchik's wheel of emotion has not been sufficiently examined. This study investigated the validity of the wheel by creating and analyzing a semantic networks of emotion words. Through our experiments, we collected data of similarity and association of ordered pairs of emotion words, and constructed networks using these data. We then analyzed the structure of the networks through community detection, and compared it with that of the wheel of emotion. The results showed that each network's structure was, for the most part, similar to that of the wheel of emotion, but locally different.

</details>


### [26] [TrailBlazer: History-Guided Reinforcement Learning for Black-Box LLM Jailbreaking](https://arxiv.org/abs/2602.06440)
*Sung-Hoon Yoon,Ruizhi Qian,Minda Zhao,Weiyue Li,Mengyu Wang*

Main category: cs.CL

TL;DR: 本文提出了一种基于历史漏洞感知的强化学习越狱框架，通过关注交互历史中的关键漏洞，大幅提升大语言模型越狱的成功率和查询效率，推动了LLM安全研究的发展。


<details>
  <summary>Details</summary>
Motivation: 当前越狱技术未能有效利用早期交互中的漏洞信息，导致攻击效率和稳定性不足。鉴于越狱为一种顺序交互过程，强化学习框架非常契合解决此类问题。

Method: 采用基于强化学习的方法，通过分析和重新加权先前交互步骤中的漏洞信号，特别引入基于注意力机制的重加权策略，以优化越狱过程中的决策和探索效率。

Result: 在AdvBench和HarmBench上的大量实验表明，所提方法不仅在越狱效果上达到了最先进水平，同时显著减少了所需的查询次数。

Conclusion: 本文提出的基于历史感知的强化学习（RL）越狱框架显著提升了大语言模型的越狱成功率和查询效率，验证了利用历史漏洞信息的重要性。

Abstract: Large Language Models (LLMs) have become integral to many domains, making their safety a critical priority. Prior jailbreaking research has explored diverse approaches, including prompt optimization, automated red teaming, obfuscation, and reinforcement learning (RL) based methods. However, most existing techniques fail to effectively leverage vulnerabilities revealed in earlier interaction turns, resulting in inefficient and unstable attacks. Since jailbreaking involves sequential interactions in which each response influences future actions, reinforcement learning provides a natural framework for this problem. Motivated by this, we propose a history-aware RL-based jailbreak framework that analyzes and reweights vulnerability signals from prior steps to guide future decisions. We show that incorporating historical information alone improves jailbreak success rates. Building on this insight, we introduce an attention-based reweighting mechanism that highlights critical vulnerabilities within the interaction history, enabling more efficient exploration with fewer queries. Extensive experiments on AdvBench and HarmBench demonstrate that our method achieves state-of-the-art jailbreak performance while significantly improving query efficiency. These results underscore the importance of historical vulnerability signals in reinforcement learning-driven jailbreak strategies and offer a principled pathway for advancing adversarial research on LLM safeguards.

</details>


### [27] [CORE: Comprehensive Ontological Relation Evaluation for Large Language Models](https://arxiv.org/abs/2602.06446)
*Satyam Dwivedi,Sanjukta Ghosh,Shivam Dwivedi,Nishi Kumari,Anil Thakur,Anurag Purushottam,Deepak Alok,Praveen Gatla,Manjuprasad B,Bipasha Patgiri*

Main category: cs.CL

TL;DR: 大型语言模型在区分有意义的语义关系和无关语义关系方面存在严重不足，CORE数据集揭示了这一隐患，并强调无关关系推理作为LLM评估与安全的重要方向。


<details>
  <summary>Details</summary>
Motivation: 现有评估很少关注LLMs区分有意义语义关系与无关关系的能力，这对语义推理和安全验证非常关键。

Method: 构建了CORE数据集，包括225K选择题和一个203题的开放测试基准，覆盖74个学科和24种语义关系类型，特别等分了相关和无关对。通过对29个先进模型进行评测，并与超过1000名人类参与者的表现对比分析。

Result: 人类基线准确率高达92.6%，尤其对无关对达到95.1%。而29个LLMs准确率仅48.25%-70.9%，对无关对准确率低至0-41.35%，且存在语义崩溃问题。在更大规模数据上准确率甚至降低至约2%。

Conclusion: 现有大型语言模型（LLMs）在判断无关语义关系方面表现较差，特别是在多个领域的语义关系推理任务中表现不足。

Abstract: Large Language Models (LLMs) perform well on many reasoning benchmarks, yet existing evaluations rarely assess their ability to distinguish between meaningful semantic relations and genuine unrelatedness. We introduce CORE (Comprehensive Ontological Relation Evaluation), a dataset of 225K multiple-choice questions spanning 74 disciplines, together with a general-domain open-source benchmark of 203 rigorously validated questions (Cohen's Kappa = 1.0) covering 24 semantic relation types with equal representation of unrelated pairs. A human baseline from 1,000+ participants achieves 92.6% accuracy (95.1% on unrelated pairs). In contrast, 29 state-of-the-art LLMs achieve 48.25-70.9% overall accuracy, with near-ceiling performance on related pairs (86.5-100%) but severe degradation on unrelated pairs (0-41.35%), despite assigning similar confidence (92-94%). Expected Calibration Error increases 2-4x on unrelated pairs, and a mean semantic collapse rate of 37.6% indicates systematic generation of spurious relations. On the CORE 225K MCQs dataset, accuracy further drops to approximately 2%, highlighting substantial challenges in domain-specific semantic reasoning. We identify unrelatedness reasoning as a critical, under-evaluated frontier for LLM evaluation and safety.

</details>


### [28] [Evaluating an evidence-guided reinforcement learning framework in aligning light-parameter large language models with decision-making cognition in psychiatric clinical reasoning](https://arxiv.org/abs/2602.06449)
*Xinxin Lin,Guangxin Dai,Yi Zhong,Xiang Li,Xue Xiao,Yixin Zhang,Zhengdong Wu,Yongbo Zheng,Runchuan Zhu,Ming Zhao,Huizi Yu,Shuo Wu,Jun Zhao,Lingming Hu,Yumei Wang,Ping Yin,Joey W. Y. Chan,Ngan Yin Chan,Sijing Chen,Yun Kwok Wing,Lin Lu,Xin Ma,Lizhou Fan*

Main category: cs.CL

TL;DR: 本文提出ClinMPO强化学习框架，通过医学证据引导优化，成功提升轻参数大语言模型在精神科复杂诊断任务中的推理能力，超过医学生表现，展现了安全可靠精神科决策支持的可行路径。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在精神科应用中存在幻觉和表层推理不足的问题，且轻参数模型虽适合隐私保护和高效临床部署，但推理能力有限，需通过训练范式优化与专业诊断认知对齐。

Method: 提出ClinMPO强化学习框架，利用基于循证医学原则构建的奖励模型，训练大语言模型的内部推理能力，重点解决精神科诊断中的幻觉和表层推理问题。

Result: ClinMPO调整的Qwen3-8B模型在包含传统大模型表现不佳的复杂病例测试中，诊断准确率达到31.4%，超过300名医学生的30.8%人类基准，实现了轻参数模型在复杂推理任务上的突破。

Conclusion: ClinMPO框架通过强化学习调整轻参数大语言模型的内部推理，使其与专业精神病学实践认知对齐，显著提升了复杂诊断任务的准确性，超过了人类医学生的表现。

Abstract: Large language models (LLMs) hold transformative potential for medical decision support yet their application in psychiatry remains constrained by hallucinations and superficial reasoning. This limitation is particularly acute in light-parameter LLMs which are essential for privacy-preserving and efficient clinical deployment. Existing training paradigms prioritize linguistic fluency over structured clinical logic and result in a fundamental misalignment with professional diagnostic cognition. Here we introduce ClinMPO, a reinforcement learning framework designed to align the internal reasoning of LLMs with professional psychiatric practice. The framework employs a specialized reward model trained independently on a dataset derived from 4,474 psychiatry journal articles and structured according to evidence-based medicine principles. We evaluated ClinMPO on a unseen subset of the benchmark designed to isolate reasoning capabilities from rote memorization. This test set comprises items where leading large-parameter LLMs consistently fail. We compared the ClinMPO-aligned light LLM performance against a cohort of 300 medical students. The ClinMPO-tuned Qwen3-8B model achieved a diagnostic accuracy of 31.4% and surpassed the human benchmark of 30.8% on these complex cases. These results demonstrate that medical evidence-guided optimization enables light-parameter LLMs to master complex reasoning tasks. Our findings suggest that explicit cognitive alignment offers a scalable pathway to reliable and safe psychiatric decision support.

</details>


### [29] [RelayGen: Intra-Generation Model Switching for Efficient Reasoning](https://arxiv.org/abs/2602.06454)
*Jiwon Song,Yoongon Kim,Jae-Joon Kim*

Main category: cs.CL

TL;DR: RelayGen提出了一种无训练开销、基于段落难度动态切换大小模型的推理方法，实现了推理加速与准确性的良好平衡。


<details>
  <summary>Details</summary>
Motivation: 现有大规模推理模型推理成本高且生成难度在单次推理过程中变化大，而现有方法要么忽略这种变化，要么依赖复杂的有监督路由，急需一种高效且简单的模型切换方法。

Method: 利用分段级别的生成不确定性分析，RelayGen在线识别推理难度变化并在不同段落间切换相应的模型进行推理，以降低计算成本并保持性能。

Result: 在多个推理基准测试中，RelayGen显著减少了推理延迟，结合推测解码后速度提升达2.2倍，且准确率仅降低不到2%。

Conclusion: RelayGen通过在推理过程中动态切换大模型和小模型，实现了推理速度的大幅提升，同时保持了高准确率，且无需额外训练或复杂的路由机制。

Abstract: Large reasoning models (LRMs) achieve strong performance on complex reasoning tasks by generating long, multi-step reasoning trajectories, but inference-time scaling incurs substantial deployment cost. A key challenge is that generation difficulty varies within a single output, whereas existing efficiency-oriented approaches either ignore this intra-generation variation or rely on supervised token-level routing with high system complexity. We present \textbf{RelayGen}, a training-free, segment-level runtime model switching framework that exploits difficulty variation in long-form reasoning. Through offline analysis of generation uncertainty using token probability margins, we show that coarse-grained segment-level control is sufficient to capture difficulty transitions within a reasoning trajectory. RelayGen identifies model-specific switch cues that signal transitions to lower-difficulty segments and dynamically delegates their continuation to a smaller model, while preserving high-difficulty reasoning on the large model. Across multiple reasoning benchmarks, RelayGen substantially reduces inference latency while preserving most of the accuracy of large models. When combined with speculative decoding, RelayGen achieves up to 2.2$\times$ end-to-end speedup with less than 2\% accuracy degradation, without requiring additional training or learned routing components.

</details>


### [30] [Diffusion-State Policy Optimization for Masked Diffusion Language Models](https://arxiv.org/abs/2602.06462)
*Daisuke Oba,Hiroki Furuta,Naoaki Okazaki*

Main category: cs.CL

TL;DR: 提出DiSPO，通过优化扩散语言模型的中间填充决策，解决终端奖励带来的稀疏信用分配问题，提升生成性能。


<details>
  <summary>Details</summary>
Motivation: 传统扩散语言模型仅依赖最终完成的终端奖励，导致中间决策的信用分配较粗糙，影响模型生成效果。

Method: 提出了DiSPO方法，在选定的中间掩码状态分支进行重新采样，利用回滚缓存的概率分布优化新填充的标记，并结合固定状态客观函数和策略梯度估计器更新模型。

Result: 在LLaDA-8B-Instruct上，DiSPO在数学和规划基准测试中，较终端反馈的diffu-GRPO基线有稳定提升，计算资源和优化步数相当。

Conclusion: DiSPO通过在中间掩码状态分支重新采样和优化填充决策，有效提升了扩散语言模型的中间决策质量，改善了终端反馈方法的稀疏信用分配问题。

Abstract: Masked diffusion language models generate by iteratively filling masked tokens over multiple denoising steps, so learning only from a terminal reward on the final completion yields coarse credit assignment over intermediate decisions. We propose DiSPO (Diffusion-State Policy Optimization), a plug-in credit-assignment layer that directly optimizes intermediate filling decisions. At selected intermediate masked states, DiSPO branches by resampling fillings for the currently masked positions from rollout-cached logits, scores the resulting completions, and updates only the newly filled tokens -- without additional multi-step diffusion rollouts. We formalize a fixed-state objective for branched completions and derive a policy-gradient estimator that can be combined with terminal-feedback policy optimization using the same rollouts. On LLaDA-8B-Instruct, DiSPO consistently improves over the terminal-feedback diffu-GRPO baseline on math and planning benchmarks under matched rollout compute and optimizer steps. Our code will be available at https://daioba.github.io/dispo .

</details>


### [31] [Improve Large Language Model Systems with User Logs](https://arxiv.org/abs/2602.06470)
*Changyue Wang,Weihang Su,Qingyao Ai,Yiqun Liu*

Main category: cs.CL

TL;DR: UNO框架通过结构化用户日志、聚类管理数据和认知差距评估，有效提升大型语言模型系统的学习能力和响应质量。


<details>
  <summary>Details</summary>
Motivation: 传统扩展大型语言模型的方法受到高质量数据稀缺和计算成本递减的限制，用户互动日志提供了丰富的人类反馈数据，但其非结构化和噪声多带来学习挑战。

Method: UNO通过将用户日志蒸馏为半结构化规则和偏好对，采用基于查询和反馈的聚类管理数据异质性，并量化模型知识与日志数据间的认知差距，进而滤除噪声反馈，构建针对主要与反思经验的不同模块。

Result: UNO在多项实验中展现出领先的效果和效率，显著优于检索增强生成和基于记忆的基线。

Conclusion: UNO框架有效利用用户日志提升了大型语言模型系统的性能，显著优于现有的基于检索增强生成和记忆的基线方法。

Abstract: Scaling training data and model parameters has long driven progress in large language models (LLMs), but this paradigm is increasingly constrained by the scarcity of high-quality data and diminishing returns from rising computational costs. As a result, recent work is increasing the focus on continual learning from real-world deployment, where user interaction logs provide a rich source of authentic human feedback and procedural knowledge. However, learning from user logs is challenging due to their unstructured and noisy nature. Vanilla LLM systems often struggle to distinguish useful feedback signals from noisy user behavior, and the disparity between user log collection and model optimization (e.g., the off-policy optimization problem) further strengthens the problem. To this end, we propose UNO (User log-driveN Optimization), a unified framework for improving LLM systems (LLMsys) with user logs. UNO first distills logs into semi-structured rules and preference pairs, then employs query-and-feedback-driven clustering to manage data heterogeneity, and finally quantifies the cognitive gap between the model's prior knowledge and the log data. This assessment guides the LLMsys to adaptively filter out noisy feedback and construct different modules for primary and reflective experiences extracted from user logs, thereby improving future responses. Extensive experiments show that UNO achieves state-of-the-art effectiveness and efficiency, significantly outperforming Retrieval Augmented Generation (RAG) and memory-based baselines. We have open-sourced our code at https://github.com/bebr2/UNO .

</details>


### [32] [Revisiting the Shape Convention of Transformer Language Models](https://arxiv.org/abs/2602.06471)
*Feng-Ting Liao,Meng-Hsi Chen,Guan-Ting Yi,Da-shan Shiu*

Main category: cs.CL

TL;DR: 本文挑战Transformer中传统窄-宽-窄FFN设计，提出并验证了基于hourglass形状的深层FFN，提升了语言模型效能和参数利用效率。


<details>
  <summary>Details</summary>
Motivation: 受残差宽-窄-宽MLP在函数逼近能力上的优越性启发，挑战了Transformer中一直以来采用的窄-宽-窄MLP设计惯例。

Method: 设计并验证了一种将传统Transformer中的FFN替换为多层残差连接的hourglass形状FFN的变体，通过实验证明其在多个模型规模上的效能提升。

Result: hourglass FFN在400M参数以下模型中表现优越，1B参数模型表现相当，同时通过调整FFN和注意力参数分配，在相同参数预算下实现性能提升。

Conclusion: 基于hourglass形状的深层FFN能够在不同模型规模上优于传统的窄-宽-窄FFN设计，且在固定参数预算下通过减小FFN参数并增加注意力层参数能提升模型性能。

Abstract: Dense Transformer language models have largely adhered to one consistent architectural shape: each layer consists of an attention module followed by a feed-forward network (FFN) with a narrow-wide-narrow MLP, allocating most parameters to the MLP at expansion ratios between 2 and 4. Motivated by recent results that residual wide-narrow-wide (hourglass) MLPs offer superior function approximation capabilities, we revisit the long-standing MLP shape convention in Transformer, challenging the necessity of the narrow-wide-narrow design. To study this, we develop a Transformer variant that replaces the conventional FFN with a deeper hourglass-shaped FFN, comprising a stack of hourglass sub-MLPs connected by residual pathways. We posit that a deeper but lighter hourglass FFN can serve as a competitive alternative to the conventional FFN, and that parameters saved by using a lighter hourglass FFN can be more effectively utilized, such as by enlarging model hidden dimensions under fixed budgets. We confirm these through empirical validations across model scales: hourglass FFNs outperform conventional FFNs up to 400M and achieve comparable performance at larger scales to 1B parameters; hourglass FFN variants with reduced FFN and increased attention parameters show consistent improvements over conventional configurations at matched budgets. Together, these findings shed new light on recent work and prompt a rethinking of the narrow-wide-narrow MLP convention and the balance between attention and FFN towards efficient and expressive modern language models.

</details>


### [33] [Completing Missing Annotation: Multi-Agent Debate for Accurate and Scalable Relevant Assessment for IR Benchmarks](https://arxiv.org/abs/2602.06526)
*Minjeong Ban,Jeonghwan Choi,Hyangsuk Min,Nicole Hee-Yeon Kim,Minseok Kim,Jae-Gil Lee,Hwanjun Song*

Main category: cs.CL

TL;DR: 为解决信息检索评估中数据不完整和标注偏差问题，提出DREAM多轮辩论框架，显著提升标注准确率并降低人工成本，进而构建BRIDGE基准数据集，实现更公平的检索系统评测。


<details>
  <summary>Details</summary>
Motivation: 现有信息检索基准数据集存在未标注的相关片段，导致评估不完整且存在偏差，且现有LLM和人工混合策略存在过度自信和升级无效等问题。

Method: 通过构建基于对立初始立场和迭代互评的多轮LLM代理辩论，实现相关性判断；结合达成共识机制，提升标注准确率和AI到人工的升级效率。

Result: DREAM达到95.2%的标注准确率，人工参与仅3.5%；基于该框架构建的BRIDGE基准发现近3万条缺失相关数据，改善了检索器排名及生成检索对齐问题。

Conclusion: 提出的DREAM框架通过多轮辩论实现了更准确的相关性标注并有效降低了人工参与率，显著提升了信息检索评估的完整性和公平性。

Abstract: Information retrieval (IR) evaluation remains challenging due to incomplete IR benchmark datasets that contain unlabeled relevant chunks. While LLMs and LLM-human hybrid strategies reduce costly human effort, they remain prone to LLM overconfidence and ineffective AI-to-human escalation. To address this, we propose DREAM, a multi-round debate-based relevance assessment framework with LLM agents, built on opposing initial stances and iterative reciprocal critique. Through our agreement-based debate, it yields more accurate labeling for certain cases and more reliable AI-to-human escalation for uncertain ones, achieving 95.2% labeling accuracy with only 3.5% human involvement. Using DREAM, we build BRIDGE, a refined benchmark that mitigates evaluation bias and enables fairer retriever comparison by uncovering 29,824 missing relevant chunks. We then re-benchmark IR systems and extend evaluation to RAG, showing that unaddressed holes not only distort retriever rankings but also drive retrieval-generation misalignment. The relevance assessment framework is available at https: //github.com/DISL-Lab/DREAM-ICLR-26; and the BRIDGE dataset is available at https://github.com/DISL-Lab/BRIDGE-Benchmark.

</details>


### [34] [MTQE.en-he: Machine Translation Quality Estimation for English-Hebrew](https://arxiv.org/abs/2602.06546)
*Andy Rosenbaum,Assaf Siani,Ilan Kernerman*

Main category: cs.CL

TL;DR: 发布英-希伯来MT质量评估数据集与实验，集成模型表现最佳，参数高效微调稳定提升性能。


<details>
  <summary>Details</summary>
Motivation: 填补英-希伯来机器翻译质量估计领域的资源空白，推动该低资源语言对的研究进展。

Method: 发布包含959英译希段落及人工评分的数据集，使用ChatGPT提示、TransQuest、CometKiwi模型评测，进行模型集成和多种微调方法实验。

Result: 集成三模型性能提升Pearson 6.4%和Spearman 5.6%，参数高效微调提升2-3%，全模型微调易过拟合。

Conclusion: MTQE.en-he基准是首个公开可用的英-希伯来机器翻译质量评估数据集，模型集成优于单一模型，且参数高效微调方法稳定且有效。

Abstract: We release MTQE.en-he: to our knowledge, the first publicly available English-Hebrew benchmark for Machine Translation Quality Estimation. MTQE.en-he contains 959 English segments from WMT24++, each paired with a machine translation into Hebrew, and Direct Assessment scores of the translation quality annotated by three human experts. We benchmark ChatGPT prompting, TransQuest, and CometKiwi and show that ensembling the three models outperforms the best single model (CometKiwi) by 6.4 percentage points Pearson and 5.6 percentage points Spearman. Fine-tuning experiments with TransQuest and CometKiwi reveal that full-model updates are sensitive to overfitting and distribution collapse, yet parameter-efficient methods (LoRA, BitFit, and FTHead, i.e., fine-tuning only the classification head) train stably and yield improvements of 2-3 percentage points. MTQE.en-he and our experimental results enable future research on this under-resourced language pair.

</details>


### [35] [Baichuan-M3: Modeling Clinical Inquiry for Reliable Medical Decision-Making](https://arxiv.org/abs/2602.06570)
*Baichuan-M3 Team,:,Chengfeng Dou,Fan Yang,Fei Li,Jiyuan Jia,Qiang Ju,Shuai Wang,Tianpeng Li,Xiangrong Zeng,Yijie Zhou,Hongda Zhang,Jinyang Tai,Linzhuang Sun,Peidong Guo,Yichuan Mo,Xiaochuan Wang,Hengfu Cui,Zhishou Zhang*

Main category: cs.CL

TL;DR: Baichuan-M3是一款医疗增强型大语言模型，采用主动信息采集和长远推理，提升了临床决策支持的准确性和可靠性，领先于当前顶尖模型。


<details>
  <summary>Details</summary>
Motivation: 针对现有医疗大语言模型在开放式咨询中的被动应答和准确性不足，旨在从被动问答转向主动、临床级的决策支持。

Method: 通过专门设计的训练流程，模拟医生的系统性工作流程，加入主动信息采集、长远推理和适应性幻觉抑制机制。

Result: 在HealthBench、HealthBench-Hallu和ScanBench等多个医学评测基准上，Baichuan-M3表现出领先性能，特别在临床问询、建议和安全性方面优于GPT-5.2。

Conclusion: Baichuan-M3显著提升了医疗领域大语言模型的临床决策支持能力，表现优于当前最先进的GPT-5.2模型。

Abstract: We introduce Baichuan-M3, a medical-enhanced large language model engineered to shift the paradigm from passive question-answering to active, clinical-grade decision support. Addressing the limitations of existing systems in open-ended consultations, Baichuan-M3 utilizes a specialized training pipeline to model the systematic workflow of a physician. Key capabilities include: (i) proactive information acquisition to resolve ambiguity; (ii) long-horizon reasoning that unifies scattered evidence into coherent diagnoses; and (iii) adaptive hallucination suppression to ensure factual reliability. Empirical evaluations demonstrate that Baichuan-M3 achieves state-of-the-art results on HealthBench, the newly introduced HealthBench-Hallu and ScanBench, significantly outperforming GPT-5.2 in clinical inquiry, advisory and safety. The models are publicly available at https://huggingface.co/collections/baichuan-inc/baichuan-m3.

</details>


### [36] [Inference-Time Rethinking with Latent Thought Vectors for Math Reasoning](https://arxiv.org/abs/2602.06584)
*Deqian Kong,Minglu Zhao,Aoyang Qin,Bo Pang,Chenxin Tao,David Hartmann,Edouardo Honig,Dehong Xu,Amit Kumar,Matt Sarte,Chuan Li,Jianwen Xie,Ying Nian Wu*

Main category: cs.CL

TL;DR: 提出了一种推理时回顾的生成框架，通过迭代优化潜在思考向量实现自我纠错，显著提升数学推理表现，且小模型性能超过大规模模型。


<details>
  <summary>Details</summary>
Motivation: 传统链式思考推理在生成过程中一旦生成错误无法回退，缺乏纠错机制，导致推理效果受限。为此引入迭代推理自纠机制提高推理准确率。

Method: 提出了推理时回顾生成框架，该框架将推理分解为连续的潜在思考向量和根据该向量生成推理轨迹的解码器，结合Gibbs样式的迭代生成与潜向量优化过程，实现推理策略的不断改进。

Result: 在GSM8K数学推理数据集上，所训练的0.2B参数模型经过30次回顾迭代后，性能超过了参数量是其10至15倍的基线模型，包括3B参数模型。

Conclusion: 通过引入推理时回顾机制，模型在推理过程中能进行迭代自我纠正，显著提升了数学推理性能，甚至用更小的模型参数超过了大规模基线模型。

Abstract: Standard chain-of-thought reasoning generates a solution in a single forward pass, committing irrevocably to each token and lacking a mechanism to recover from early errors. We introduce Inference-Time Rethinking, a generative framework that enables iterative self-correction by decoupling declarative latent thought vectors from procedural generation. We factorize reasoning into a continuous latent thought vector (what to reason about) and a decoder that verbalizes the trace conditioned on this vector (how to reason). Beyond serving as a declarative buffer, latent thought vectors compress the reasoning structure into a continuous representation that abstracts away surface-level token variability, making gradient-based optimization over reasoning strategies well-posed. Our prior model maps unstructured noise to a learned manifold of valid reasoning patterns, and at test time we employ a Gibbs-style procedure that alternates between generating a candidate trace and optimizing the latent vector to better explain that trace, effectively navigating the latent manifold to refine the reasoning strategy. Training a 0.2B-parameter model from scratch on GSM8K, our method with 30 rethinking iterations surpasses baselines with 10 to 15 times more parameters, including a 3B counterpart. This result demonstrates that effective mathematical reasoning can emerge from sophisticated inference-time computation rather than solely from massive parameter counts.

</details>


### [37] [Echoes as Anchors: Probabilistic Costs and Attention Refocusing in LLM Reasoning](https://arxiv.org/abs/2602.06600)
*Zhuoyuan Hao,Zhuo Li,Wu Li,Fangming Liu,Min Zhang,Jing Li*

Main category: cs.CL

TL;DR: 本论文发现并利用大型推理模型自发重复问题的机制，通过设计新的微调和提示技术，提高了模型在数学推理任务中的准确率，并从理论和实验两个角度验证了该机制的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在提升大型推理模型表现时未充分利用模型自发重复问题的特性，且缺乏理论解释，本文旨在探究并利用这一现象以优化推理效率和准确率。

Method: 本文通过定义Echo Likelihood Gap作为理论基础，提出Echo-Distilled SFT（监督微调）和Echoic Prompting（中途重新触发提示）两种方法，有效利用模型的EOP特性。此外，结合长度和后缀控制的似然分析以及层级注意力研究，揭示了EOP的注意力重新聚焦机制。

Result: 在GSM8K、MathQA、Hendrycks-MATH、AIME24和MATH-500数据集上，本文方法在相同解码设置和计算预算下均表现出优于基线的方法的性能提升。

Conclusion: 本文提出了利用大型推理模型中自发重复现象（Echo of Prompt, EOP）作为计算分配机制的新方法，显著提升了模型在数学问题解决等任务中的表现。

Abstract: Test-time compute allocation in large reasoning models (LRMs) is widely used and has applications in mathematical problem solving, code synthesis, and planning. Recent work has addressed this problem by scaling self-consistency and parallel thinking, adding generic ``thinking tokens'' and prompting models to re-read the question before answering. Unfortunately, these approaches either inject task-agnostic tokens or mandate heuristics that do not explain -- and often ignore -- the \emph{spontaneous} repetition that many LRMs exhibit at the head of their internal chains. In contrast, we analyze and harness the model's tendency to restate the question, which we term the \emph{Echo of Prompt (EOP)}, as a front-loaded, compute-shaping mechanism. We formalize its probabilistic cost by casting echo removal as rejection-based conditioning and defining the \emph{Echo Likelihood Gap} $Δ\mathcal{L}$ as a computable proxy. This provides the missing theoretical link that links early repetition to likelihood gains and downstream accuracy. However, it does not by itself specify how to exploit EOP. Consequently, we develop \emph{Echo-Distilled SFT (ED-SFT)} to instill an ``echo-then-reason'' pattern through supervised finetuning, and \emph{Echoic Prompting (EP)} to re-ground the model mid-trace without training. While promising, quantifying benefits beyond verbosity is non-trivial. Therefore, we conduct length and suffix-controlled likelihood analyses together with layer-wise attention studies, showing that EOP increases answer to answer-prefix attention in middle layers, consistent with an \emph{attention refocusing} mechanism. We evaluate on GSM8K, MathQA, Hendrycks-MATH, AIME24, and MATH-500 under identical decoding settings and budgets, and find consistent gains over baselines. Code is available at https://github.com/hhh2210/echoes-as-anchors.

</details>


### [38] [Do Prompts Guarantee Safety? Mitigating Toxicity from LLM Generations through Subspace Intervention](https://arxiv.org/abs/2602.06623)
*Himanshu Singh,Ziwei Xu,A. V. Subramanyam,Mohan Kankanhalli*

Main category: cs.CL

TL;DR: 针对大型语言模型生成有害内容问题，提出亚空间干预策略，有效降低文本毒性且保持流畅性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然强大，但可能生成潜在的有害或有毒内容，且有毒性检测困难且依赖上下文，本研究旨在安全有效地减少文本毒性。

Method: 采用针对性亚空间干预方法，通过识别和抑制模型内部隐藏的有毒模式，减少有害内容生成。

Result: 在多个大型语言模型上，该方法相较现有去毒系统减少了8-20%的毒性，且对文本流畅性影响极小，并显著优于现有基线方法。

Conclusion: 本论文提出的针对性亚空间干预策略有效减少了大型语言模型生成文本中的潜在有害内容，在保持文本流畅性和生成能力的同时，显著降低了有害性。

Abstract: Large Language Models (LLMs) are powerful text generators, yet they can produce toxic or harmful content even when given seemingly harmless prompts. This presents a serious safety challenge and can cause real-world harm. Toxicity is often subtle and context-dependent, making it difficult to detect at the token level or through coarse sentence-level signals. Moreover, efforts to mitigate toxicity often face a trade-off between safety and the coherence, or fluency of the generated text. In this work, we present a targeted subspace intervention strategy for identifying and suppressing hidden toxic patterns from underlying model representations, while preserving overall ability to generate safe fluent content. On the RealToxicityPrompts, our method achieves strong mitigation performance compared to existing baselines, with minimal impact on inference complexity. Across multiple LLMs, our approach reduces toxicity of state-of-the-art detoxification systems by 8-20%, while maintaining comparable fluency. Through extensive quantitative and qualitative analyses, we show that our approach achieves effective toxicity reduction without impairing generative performance, consistently outperforming existing baselines.

</details>


### [39] [FairJudge: An Adaptive, Debiased, and Consistent LLM-as-a-Judge](https://arxiv.org/abs/2602.06625)
*Bo Yang,Lanfei Feng,Yunkui Chen,Yu Zhang,Xiao Xu,Shijian Li*

Main category: cs.CL

TL;DR: FairJudge通过构建学习型评判策略和高质量数据，显著提升LLM自动评判的公平性、一致性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM作为评判者系统存在适应性差、偏见明显和评估不一致等三个根本性问题。

Method: 提出FairJudge，将评判行为建模为可学习且正则化的策略，构建数据驱动的高信息密度评判数据集，并采用课程式的SFT-DPO-GRPO训练方法，逐步实现评分准则遵守、偏见消除和跨模式一致性。

Result: 实验表明FairJudge提高了评判一致性和F1分数，显著减少了非语义偏见，表现优于更大规模的指令调优LLM。

Conclusion: FairJudge有效解决了LLM作为评判者的适应性差、偏见及不一致问题，推动了更加公平和稳定的自动评估体系的发展。

Abstract: Existing LLM-as-a-Judge systems suffer from three fundamental limitations: limited adaptivity to task- and domain-specific evaluation criteria, systematic biases driven by non-semantic cues such as position, length, format, and model provenance, and evaluation inconsistency that leads to contradictory judgments across different evaluation modes (e.g., pointwise versus pairwise). To address these issues, we propose FairJudge, an adaptive, debiased, and consistent LLM-as-a-Judge. Unlike prior approaches that treat the judge as a static evaluator, FairJudge models judging behavior itself as a learnable and regularized policy. From a data-centric perspective, we construct a high-information-density judging dataset that explicitly injects supervision signals aligned with evaluation behavior. Building on this dataset, we adopt a curriculum-style SFT-DPO-GRPO training paradigm that progressively aligns rubric adherence, bias mitigation, and cross-mode consistency, while avoiding catastrophic forgetting. Experimental results on multiple internal and public benchmarks show that FairJudge consistently improves agreement and F1, reduces non-semantic biases, and outperforms substantially larger instruction-tuned LLMs. All resources will be publicly released after acceptance to facilitate future research.

</details>


### [40] [Reading Between the Waves: Robust Topic Segmentation Using Inter-Sentence Audio Features](https://arxiv.org/abs/2602.06647)
*Steffen Freisinger,Philipp Seeberger,Tobias Bocklet,Korbinian Riedhammer*

Main category: cs.CL

TL;DR: 本文提出联合文本与音频编码的多模态话题分割方法，显著优于传统文本单一模型，增强了性能和对ASR噪声的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前自动话题分割方法未充分利用语音特征，影响了性能。

Method: 提出多模态方法，联合微调文本编码器和Siamese音频编码器，捕捉句子边界周围的语音线索。

Result: 在YouTube大规模数据集上，模型较文本单一和多模态基线均显著提升；在葡萄牙语、德语和英语三个额外数据集上也优于更大规模文本基线，且对ASR噪声更鲁棒。

Conclusion: 结合声学特征的多模态模型能显著提升自动话题分割的性能和鲁棒性。

Abstract: Spoken content, such as online videos and podcasts, often spans multiple topics, which makes automatic topic segmentation essential for user navigation and downstream applications. However, current methods do not fully leverage acoustic features, leaving room for improvement. We propose a multi-modal approach that fine-tunes both a text encoder and a Siamese audio encoder, capturing acoustic cues around sentence boundaries. Experiments on a large-scale dataset of YouTube videos show substantial gains over text-only and multi-modal baselines. Our model also proves more resilient to ASR noise and outperforms a larger text-only baseline on three additional datasets in Portuguese, German, and English, underscoring the value of learned acoustic features for robust topic segmentation.

</details>


### [41] [Beyond Static Alignment: Hierarchical Policy Control for LLM Safety via Risk-Aware Chain-of-Thought](https://arxiv.org/abs/2602.06650)
*Jianfeng Si,Lin Sun,Weihong Lin,Xiangzheng Zhang*

Main category: cs.CL

TL;DR: PACT框架通过层级策略和动态风险感知推理，实现了对大型语言模型的可控安全管理，提升了安全表现和用户定制的灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型采用静态、通用的安全策略，缺乏运行时可控性，导致模型在实际应用中难以平衡安全性与实用性，容易过度拒绝正常请求或对有害请求约束不足。

Method: 采用分层策略架构，结合不可覆盖的全球安全策略与用户定义的本地策略，实现安全决策的分类和行动路径分解，从而使决策过程透明且可控。

Result: PACT在全球政策评估下达到近先进水平的安全表现，同时在用户自定义策略评估中实现最佳的控制能力，成功解决了安全与帮助性之间的矛盾。

Conclusion: PACT框架通过动态安全控制和明确的风险感知推理，有效缓解了大型语言模型面对的安全与实用性之间的权衡问题，实现了全球性安全政策评估下的近先进安全性能，并在用户特定策略评估中展现出最佳的可控性。

Abstract: Large Language Models (LLMs) face a fundamental safety-helpfulness trade-off due to static, one-size-fits-all safety policies that lack runtime controllabilityxf, making it difficult to tailor responses to diverse application needs. %As a result, models may over-refuse benign requests or under-constrain harmful ones. We present \textbf{PACT} (Prompt-configured Action via Chain-of-Thought), a framework for dynamic safety control through explicit, risk-aware reasoning. PACT operates under a hierarchical policy architecture: a non-overridable global safety policy establishes immutable boundaries for critical risks (e.g., child safety, violent extremism), while user-defined policies can introduce domain-specific (non-global) risk categories and specify label-to-action behaviors to improve utility in real-world deployment settings. The framework decomposes safety decisions into structured Classify$\rightarrow$Act paths that route queries to the appropriate action (comply, guide, or reject) and render the decision-making process transparent.
  Extensive experiments demonstrate that PACT achieves near state-of-the-art safety performance under global policy evaluation while attaining the best controllability under user-specific policy evaluation, effectively mitigating the safety-helpfulness trade-off. We will release the PACT model suite, training data, and evaluation protocols to facilitate reproducible research in controllable safety alignment.

</details>


### [42] [Not All Layers Need Tuning: Selective Layer Restoration Recovers Diversity](https://arxiv.org/abs/2602.06665)
*Bowen Zhang,Meiyi Wang,Harold Soh*

Main category: cs.CL

TL;DR: 提出一种无需训练的选择性层恢复方法，通过恢复部分层的预训练权重，解决后训练模型生成多样性降低的问题，在多模型多任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 后训练提升了大语言模型的指令执行能力和有用性，但经常导致生成多样性下降，出现模式崩溃问题。

Method: 设计了一种代理任务CRC，通过恢复部分层的预训练权重来提高多样性，提出了Selective Layer Restoration（选择性层恢复）方法，无需额外训练，通过恢复某些层的预训练权重来平衡多样性和质量。

Result: SLR方法在多个任务和不同模型上均显著提升了生成结果的多样性，同时保持了输出质量。

Conclusion: 选择性恢复后训练模型中部分层的预训练权重，可以有效缓解模式崩溃问题，提升大语言模型的生成多样性和质量。

Abstract: Post-training improves instruction-following and helpfulness of large language models (LLMs) but often reduces generation diversity, which leads to repetitive outputs in open-ended settings, a phenomenon known as mode collapse. Motivated by evidence that LLM layers play distinct functional roles, we hypothesize that mode collapse can be localized to specific layers and that restoring a carefully chosen range of layers to their pre-trained weights can recover diversity while maintaining high output quality. To validate this hypothesis and decide which layers to restore, we design a proxy task -- Constrained Random Character(CRC) -- with an explicit validity set and a natural diversity objective. Results on CRC reveal a clear diversity-validity trade-off across restoration ranges and identify configurations that increase diversity with minimal quality loss. Based on these findings, we propose Selective Layer Restoration (SLR), a training-free method that restores selected layers in a post-trained model to their pre-trained weights, yielding a hybrid model with the same architecture and parameter count, incurring no additional inference cost. Across three different tasks (creative writing, open-ended question answering, and multi-step reasoning) and three different model families (Llama, Qwen, and Gemma), we find SLR can consistently and substantially improve output diversity while maintaining high output quality.

</details>


### [43] [compar:IA: The French Government's LLM arena to collect French-language human prompts and preference data](https://arxiv.org/abs/2602.06669)
*Lucie Termignon,Simonas Zilinskas,Hadrien Pélissier,Aurélien Barrot,Nicolas Chesnais,Elie Gavoty*

Main category: cs.CL

TL;DR: compar:IA是法国政府推出的开源平台，通过收集海量法语用户偏好数据，解决了大型语言模型非英语性能不足问题，并发布相关开放数据集支持多语言模型发展。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在非英语语言表现、文化适应性及安全性方面存在不足，部分原因是训练数据和人类偏好数据集中于英语。

Method: 开发了compar:IA，一个由法国政府支持的开源数字公共服务平台，通过盲对比界面收集法语用户的大规模人类偏好数据，确保隐私并减少参与门槛。

Result: 截至2026年2月，平台收集了超过60万个自由形式提示和25万个偏好投票，89%为法语数据，并发布了对话、投票和反馈三套开放数据集，展示了法语模型排行榜和用户交互模式分析。

Conclusion: compar:IA有效填补了非英语偏好数据短缺的空白，为多语言模型训练与评估及人机交互研究提供了宝贵的资源，且具备向国际数字公共产品扩展的潜力。

Abstract: Large Language Models (LLMs) often show reduced performance, cultural alignment, and safety robustness in non-English languages, partly because English dominates both pre-training data and human preference alignment datasets. Training methods like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) require human preference data, which remains scarce and largely non-public for many languages beyond English. To address this gap, we introduce compar:IA, an open-source digital public service developed inside the French government and designed to collect large-scale human preference data from a predominantly French-speaking general audience. The platform uses a blind pairwise comparison interface to capture unconstrained, real-world prompts and user judgments across a diverse set of language models, while maintaining low participation friction and privacy-preserving automated filtering. As of 2026-02-07, compar:IA has collected over 600,000 free-form prompts and 250,000 preference votes, with approximately 89% of the data in French. We release three complementary datasets -- conversations, votes, and reactions -- under open licenses, and present initial analyses, including a French-language model leaderboard and user interaction patterns. Beyond the French context, compar:IA is evolving toward an international digital public good, offering reusable infrastructure for multilingual model training, evaluation, and the study of human-AI interaction.

</details>


### [44] [Evaluating Prompt Engineering Strategies for Sentiment Control in AI-Generated Texts](https://arxiv.org/abs/2602.06692)
*Kerstin Sahler,Sophie Jentzsch*

Main category: cs.CL

TL;DR: 本研究展示了通过提示工程有效控制大型语言模型输出文本情感的可能性，为构建情感自适应AI系统提供了实用且经济的方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在情感自适应人工智能中的应用潜力巨大，但情感的有意控制仍具挑战性，提示工程提供了一种低成本、资源敏感的替代方案。

Method: 使用Ekman的六种基本情绪，采用零样本提示、思维链提示和微调方法，在gpt-3.5-turbo模型上进行比较，重点分析提示工程的效果。

Result: 提示工程，特别是带有人工示例的少样本提示，在控制情感方面优于零样本提示，且效果接近或优于微调方法，适合数据受限环境。

Conclusion: 通过提示工程方法能够有效控制大型语言模型生成文本的情感，实现情感自适应的人工智能系统。

Abstract: The groundbreaking capabilities of Large Language Models (LLMs) offer new opportunities for enhancing human-computer interaction through emotion-adaptive Artificial Intelligence (AI). However, deliberately controlling the sentiment in these systems remains challenging. The present study investigates the potential of prompt engineering for controlling sentiment in LLM-generated text, providing a resource-sensitive and accessible alternative to existing methods. Using Ekman's six basic emotions (e.g., joy, disgust), we examine various prompting techniques, including Zero-Shot and Chain-of-Thought prompting using gpt-3.5-turbo, and compare it to fine-tuning. Our results indicate that prompt engineering effectively steers emotions in AI-generated texts, offering a practical and cost-effective alternative to fine-tuning, especially in data-constrained settings. In this regard, Few-Shot prompting with human-written examples was the most effective among other techniques, likely due to the additional task-specific guidance. The findings contribute valuable insights towards developing emotion-adaptive AI systems.

</details>


### [45] [Table-as-Search: Formulate Long-Horizon Agentic Information Seeking as Table Completion](https://arxiv.org/abs/2602.06724)
*Tian Lan,Felix Henry,Bin Zhu,Qianghuai Jia,Junyang Ren,Qihang Pu,Haijun Li,Longyue Wang,Zhao Xu,Weihua Luo*

Main category: cs.CL

TL;DR: 本文提出TaS框架，通过结构化表格管理信息搜索任务，有效提升了长距离搜索的连贯性和效率，优于现有多种方法。


<details>
  <summary>Details</summary>
Motivation: 现有信息搜索代理在长距离探索过程中难以保持关注和连贯性，因为单一纯文本上下文难以有效跟踪复杂的搜索状态和计划。

Method: 提出了一种结构化规划框架Table-as-Search (TaS)，将信息搜索任务转化为表格填充任务，通过维护外部数据库中的表格结构来管理搜索状态和规划过程。

Result: TaS在多种基准测试，包括多代理框架和商业系统中，显著优于多种先进基线方法，验证了其在长距离信息搜索任务中的鲁棒性和效率。

Conclusion: TaS方法在长距离信息搜索任务中表现卓越，显著优于现有多种基准方法，并展现出优越的鲁棒性、效率、可扩展性和灵活性。

Abstract: Current Information Seeking (InfoSeeking) agents struggle to maintain focus and coherence during long-horizon exploration, as tracking search states, including planning procedure and massive search results, within one plain-text context is inherently fragile. To address this, we introduce \textbf{Table-as-Search (TaS)}, a structured planning framework that reformulates the InfoSeeking task as a Table Completion task. TaS maps each query into a structured table schema maintained in an external database, where rows represent search candidates and columns denote constraints or required information. This table precisely manages the search states: filled cells strictly record the history and search results, while empty cells serve as an explicit search plan. Crucially, TaS unifies three distinct InfoSeeking tasks: Deep Search, Wide Search, and the challenging DeepWide Search. Extensive experiments demonstrate that TaS significantly outperforms numerous state-of-the-art baselines across three kinds of benchmarks, including multi-agent framework and commercial systems. Furthermore, our analysis validates the TaS's superior robustness in long-horizon InfoSeeking, alongside its efficiency, scalability and flexibility. Code and datasets are publicly released at https://github.com/AIDC-AI/Marco-Search-Agent.

</details>


### [46] [R-Align: Enhancing Generative Reward Models through Rationale-Centric Meta-Judging](https://arxiv.org/abs/2602.06763)
*Yanlin Lai,Mitt Huang,Hangyu Guo,Xiangfeng Wang,Haodong Li,Shaoxiong Zhan,Liang Zhao,Chengyuan Yao,Yinmin Zhang,Qi Han,Chun Yuan,Zheng Ge,Xiangyu Zhang,Daxin Jiang*

Main category: cs.CL

TL;DR: 本文发现生成式奖励模型推理质量对RLHF表现影响大，提出R-Align方法监督理据一致性，显著提升模型性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有生成式奖励模型训练和评估仅依赖结果标签，未对推理质量进行监督，导致模型推理与标签不一致，影响RLHF下游任务的性能和稳定性。

Method: 提出基于理据对齐的训练方法R-Align，通过引入金标准判断监督理据的一致性，增强生成式奖励模型的推理质量和鲁棒性。

Result: 通过实验证明S-Corr指标能有效量化推理与标签不一致的现象，R-Align方法显著降低了S-Corr，同时提升了模型在STEM、编程、指令跟随及通用任务中的表现。

Conclusion: 推理一致性对于生成式奖励模型（GenRM）的效果具有重要意义，比单纯的标签准确率更能预测下游的RLHF表现。引入基于理据对齐的训练方法R-Align，可以有效提升推理质量，减少错误一致性比例，从而提升策略性能。

Abstract: Reinforcement Learning from Human Feedback (RLHF) remains indispensable for aligning large language models (LLMs) in subjective domains. To enhance robustness, recent work shifts toward Generative Reward Models (GenRMs) that generate rationales before predicting preferences. Yet in GenRM training and evaluation, practice remains outcome-label-only, leaving reasoning quality unchecked. We show that reasoning fidelity-the consistency between a GenRM's preference decision and reference decision rationales-is highly predictive of downstream RLHF outcomes, beyond standard label accuracy. Specifically, we repurpose existing reward-model benchmarks to compute Spurious Correctness (S-Corr)-the fraction of label-correct decisions with rationales misaligned with golden judgments. Our empirical evaluation reveals substantial S-Corr even for competitive GenRMs, and higher S-Corr is associated with policy degeneration under optimization. To improve fidelity, we propose Rationale-Centric Alignment, R-Align, which augments training with gold judgments and explicitly supervises rationale alignment. R-Align reduces S-Corr on RM benchmarks and yields consistent gains in actor performance across STEM, coding, instruction following, and general tasks.

</details>


### [47] [Generating Data-Driven Reasoning Rubrics for Domain-Adaptive Reward Modeling](https://arxiv.org/abs/2602.06795)
*Kate Sanders,Nathaniel Weir,Sapana Chaudhary,Kaj Bostrom,Huzefa Rangwala*

Main category: cs.CL

TL;DR: 通过自动构建细粒度错误分类体系和基于其的奖励函数，显著提升大型语言模型在复杂技术推理任务中的错误检测和训练效果，减少对黄金标签的依赖。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在长输出、专业领域和无可验证奖励的问题中难以可靠识别推理错误，限制了其在推理输出验证中的应用。

Method: 提出自动构建细粒度推理错误分类体系（rubrics），并基于这些rubrics设计分类方法以检测未见推理痕迹中的错误，同时将rubrics用作强化学习中的奖励函数。

Result: 基于rubrics的错误识别方法在编码、数学和化学工程等技术领域表现优于基线方法。使用rubrics设计的奖励函数提升模型准确率达+45%，且仅需20%黄金标签，效果接近使用可验证奖励的模型。

Conclusion: 利用数据驱动方法构建细粒度推理错误分类体系（rubrics）能显著提升大型语言模型在技术领域推理错误检测的准确性，从而提高模型在复杂任务上的表现和训练效率。

Abstract: An impediment to using Large Language Models (LLMs) for reasoning output verification is that LLMs struggle to reliably identify errors in thinking traces, particularly in long outputs, domains requiring expert knowledge, and problems without verifiable rewards. We propose a data-driven approach to automatically construct highly granular reasoning error taxonomies to enhance LLM-driven error detection on unseen reasoning traces. Our findings indicate that classification approaches that leverage these error taxonomies, or "rubrics", demonstrate strong error identification compared to baseline methods in technical domains like coding, math, and chemical engineering. These rubrics can be used to build stronger LLM-as-judge reward functions for reasoning model training via reinforcement learning. Experimental results show that these rewards have the potential to improve models' task accuracy on difficult domains over models trained by general LLMs-as-judges by +45%, and approach performance of models trained by verifiable rewards while using as little as 20% as many gold labels. Through our approach, we extend the usage of reward rubrics from assessing qualitative model behavior to assessing quantitative model correctness on tasks typically learned via RLVR rewards. This extension opens the door for teaching models to solve complex technical problems without a full dataset of gold labels, which are often highly costly to procure.

</details>


### [48] [Visual Word Sense Disambiguation with CLIP through Dual-Channel Text Prompting and Image Augmentations](https://arxiv.org/abs/2602.06799)
*Shamik Bhattacharya,Daniel Perkins,Yaren Dogan,Vineeth Konjeti,Sudarshan Srinivasan,Edmon Begoli*

Main category: cs.CL

TL;DR: 该论文提出利用CLIP结合文本和图像的视觉词义消歧框架，通过增强提示有效提升了词义消歧性能，验证了视觉信息在解决语言模糊性中的优势。


<details>
  <summary>Details</summary>
Motivation: 词汇歧义是大规模语言模型自然语言理解中的难题，研究如何通过视觉信息辅助解决词义歧义，提高模型的理解能力。

Method: 利用CLIP将歧义文本和候选图像投射到共享的多模态空间，通过语义和图像提示的双通道集成（包括WordNet同义词）增强文本嵌入，结合图像的测试时增强，最后使用余弦相似度选取最匹配的图像。

Result: 在SemEval-2023 VWSD数据集上，增强嵌入使MRR从0.7227提升至0.7590，Hit Rate从0.5810提升至0.6220。消融实验显示双通道提示带来了显著且低延迟的性能提升，而图像增强效果有限。多语言和定义提示引入噪声降低了语义准确性。

Conclusion: 视觉词义消歧（VWSD）通过联合视觉和语言的信息，有效提升了消歧准确率，证明了视觉信号在解决词汇歧义中的重要作用。

Abstract: Ambiguity poses persistent challenges in natural language understanding for large language models (LLMs). To better understand how lexical ambiguity can be resolved through the visual domain, we develop an interpretable Visual Word Sense Disambiguation (VWSD) framework. The model leverages CLIP to project ambiguous language and candidate images into a shared multimodal space. We enrich textual embeddings using a dual-channel ensemble of semantic and photo-based prompts with WordNet synonyms, while image embeddings are refined through robust test-time augmentations. We then use cosine similarity to determine the image that best aligns with the ambiguous text. When evaluated on the SemEval-2023 VWSD dataset, enriching the embeddings raises the MRR from 0.7227 to 0.7590 and the Hit Rate from 0.5810 to 0.6220. Ablation studies reveal that dual-channel prompting provides strong, low-latency performance, whereas aggressive image augmentation yields only marginal gains. Additional experiments with WordNet definitions and multilingual prompt ensembles further suggest that noisy external signals tend to dilute semantic specificity, reinforcing the effectiveness of precise, CLIP-aligned prompts for visual word sense disambiguation.

</details>


### [49] [The Representational Geometry of Number](https://arxiv.org/abs/2602.06843)
*Zhimin Hu,Lanhao Niu,Sashank Varma*

Main category: cs.CL

TL;DR: 本文揭示语言模型通过保持数字概念间稳定的几何关系结构，实现了任务间表示的共享与转换，提供了理解概念表示如何兼顾共享与任务特异性的机制视角。


<details>
  <summary>Details</summary>
Motivation: 认知科学中的核心问题是概念表示是否聚合到共享流形以支持泛化，或分散到正交子空间以减少任务间干扰，现有研究缺乏两者共存及跨任务转换的机制解释。

Method: 通过将数字概念作为实验对象，利用语言模型高维计算基底，分析不同任务中数字表示的几何关系和子空间结构，并验证子空间间的线性变换能力。

Result: 发现数字表示在任务专有的正交子空间内编码特征如大小和奇偶，且这些子空间间可通过线性映射相互转换，体现共享的关系结构。

Conclusion: 语言模型中的数字概念表示在不同任务中通过线性映射保持共享的几何关系结构，实现在正交子空间中的功能灵活性和平衡干扰。

Abstract: A central question in cognitive science is whether conceptual representations converge onto a shared manifold to support generalization, or diverge into orthogonal subspaces to minimize task interference. While prior work has discovered evidence for both, a mechanistic account of how these properties coexist and transform across tasks remains elusive. We propose that representational sharing lies not in the concepts themselves, but in the geometric relations between them. Using number concepts as a testbed and language models as high-dimensional computational substrates, we show that number representations preserve a stable relational structure across tasks. Task-specific representations are embedded in distinct subspaces, with low-level features like magnitude and parity encoded along separable linear directions. Crucially, we find that these subspaces are largely transformable into one another via linear mappings, indicating that representations share relational structure despite being located in distinct subspaces. Together, these results provide a mechanistic lens of how language models balance the shared structure of number representation with functional flexibility. It suggests that understanding arises when task-specific transformations are applied to a shared underlying relational structure of conceptual representations.

</details>


### [50] [SEMA: Simple yet Effective Learning for Multi-Turn Jailbreak Attacks](https://arxiv.org/abs/2602.06854)
*Mingqian Feng,Xiaodong Liu,Weiwei Yang,Jialin Song,Xuekai Zhu,Chenliang Xu,Jianfeng Gao*

Main category: cs.CL

TL;DR: SEMA提出了一种无需外部数据的多轮攻击训练框架，通过自我微调与意图漂移感知强化学习，实现了对安全对话模型的更强攻击效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多轮jailbreak攻击中因探索复杂性和意图漂移问题表现不佳，单轮攻击模型不足以描述现实安全威胁。

Method: SEMA采用两阶段策略，首先通过自我微调生成多轮对抗性提示以稳定训练，然后通过考虑意图漂移的奖励机制进行强化学习，从而训练出有效的多轮攻击者。

Result: SEMA在多个数据集、受害模型和评测方法中均达到了最先进的攻击成功率，平均ASR提升了33.9%，并且具有良好的可复现性和迁移性。

Conclusion: SEMA框架在多轮jailbreak攻击中表现出色，显著提升攻击成功率，优于现有单轮和多轮基线方法，能够提供更强现实的安全压力测试。

Abstract: Multi-turn jailbreaks capture the real threat model for safety-aligned chatbots, where single-turn attacks are merely a special case. Yet existing approaches break under exploration complexity and intent drift. We propose SEMA, a simple yet effective framework that trains a multi-turn attacker without relying on any existing strategies or external data. SEMA comprises two stages. Prefilling self-tuning enables usable rollouts by fine-tuning on non-refusal, well-structured, multi-turn adversarial prompts that are self-generated with a minimal prefix, thereby stabilizing subsequent learning. Reinforcement learning with intent-drift-aware reward trains the attacker to elicit valid multi-turn adversarial prompts while maintaining the same harmful objective. We anchor harmful intent in multi-turn jailbreaks via an intent-drift-aware reward that combines intent alignment, compliance risk, and level of detail. Our open-loop attack regime avoids dependence on victim feedback, unifies single- and multi-turn settings, and reduces exploration complexity. Across multiple datasets, victim models, and jailbreak judges, our method achieves state-of-the-art (SOTA) attack success rates (ASR), outperforming all single-turn baselines, manually scripted and template-driven multi-turn baselines, as well as our SFT (Supervised Fine-Tuning) and DPO (Direct Preference Optimization) variants. For instance, SEMA performs an average $80.1\%$ ASR@1 across three closed-source and open-source victim models on AdvBench, 33.9% over SOTA. The approach is compact, reproducible, and transfers across targets, providing a stronger and more realistic stress test for large language model (LLM) safety and enabling automatic redteaming to expose and localize failure modes. Our code is available at: https://github.com/fmmarkmq/SEMA.

</details>


### [51] [Uncovering Cross-Objective Interference in Multi-Objective Alignment](https://arxiv.org/abs/2602.06869)
*Yining Lu,Meng Jiang*

Main category: cs.CL

TL;DR: 论文分析了多目标对齐中的跨目标干扰，提出基于局部协方差的理论解释和CTWA缓解方法，推动了对齐优化的理解和稳定性。


<details>
  <summary>Details</summary>
Motivation: 在多目标对齐中训练提升部分目标性能的同时导致其他目标退化，存在显著的跨目标干扰。理解和缓解这一问题具挑战性且重要。

Method: 对经典标量化算法的系统研究，推导局部协方差定律，并扩展到裁剪代理目标。提出CTWA方法以维持正协方差。进行基于Polyak--Łojasiewicz条件的全局收敛分析。

Result: 发现跨目标干扰普遍存在且强烈依赖模型特性，验证局部协方差定律在裁剪代理目标中依然成立，提出的CTWA方法有效减轻干扰，实现了全局收敛分析。

Conclusion: 跨目标干扰是多目标对齐中的普遍且依赖模型的失败模式。通过局部协方差定律解释该现象并提出CTWA方法有效缓解干扰。

Abstract: We study a persistent failure mode in multi-objective alignment for large language models (LLMs): training improves performance on only a subset of objectives while causing others to degrade. We formalize this phenomenon as cross-objective interference and conduct the first systematic study across classic scalarization algorithms, showing that interference is pervasive and exhibits strong model dependence.
  To explain this phenomenon, we derive a local covariance law showing that an objective improves at first order when its reward exhibits positive covariance with the scalarized score. We extend this analysis to clipped surrogate objectives used in modern alignment, demonstrating that the covariance law remains valid under mild conditions despite clipping. Building on this analysis, we propose Covariance Targeted Weight Adaptation (CTWA), a plug-and-play method that maintains positive covariance between objective rewards and the training signal to effectively mitigate cross-objective interference. Finally, we complement these local improvement conditions with a global convergence analysis under the Polyak--Łojasiewicz condition, establishing when non-convex scalarized optimization achieves global convergence and how cross-objective interference depends on specific model geometric properties.

</details>


### [52] [Halluverse-M^3: A multitask multilingual benchmark for hallucination in LLMs](https://arxiv.org/abs/2602.06920)
*Samir Abdaljalil,Parichit Sharma,Erchin Serpedin,Hasan Kurban*

Main category: cs.CL

TL;DR: 本文提出Halluverse-M^3数据集，系统分析多语言、多任务、多类别的语言模型幻觉现象，为幻觉检测与缓解研究提供现实挑战和基准支持。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型在多语言和生成任务中易产生幻觉，影响事实一致性，现有研究多集中于英语，缺乏跨语言、跨任务和跨幻觉类型的系统分析。

Method: 构建包含英语、阿拉伯语、印地语和土耳其语的多语种数据集，覆盖问答和对话摘要两个生成任务，通过可控编辑和人工验证来明确区分实体级、关系级和句子级幻觉，进而对多种语言模型进行细粒度幻觉检测评估。

Result: 发现问答任务的幻觉检测相对容易，而句子级幻觉最具挑战性；模型在英语上的表现最好，印地语等低资源语言性能明显较差，其中印地语检测准确率最低。

Conclusion: Halluverse-M^3 数据集为多语言、多任务和多类别的幻觉现象提供了系统性分析的基准，揭示了不同语言和任务中幻觉检测的难易差异，推动了幻觉检测研究的发展。

Abstract: Hallucinations in large language models remain a persistent challenge, particularly in multilingual and generative settings where factual consistency is difficult to maintain. While recent models show strong performance on English-centric benchmarks, their behavior across languages, tasks, and hallucination types is not yet well understood. In this work, we introduce Halluverse-M^3, a dataset designed to enable systematic analysis of hallucinations across multiple languages, multiple generation tasks, and multiple hallucination categories. Halluverse-M^3 covers four languages, English, Arabic, Hindi, and Turkish, and supports two generation tasks: question answering and dialogue summarization. The dataset explicitly distinguishes between entity-level, relation-level, and sentence-level hallucinations. Hallucinated outputs are constructed through a controlled editing process and validated by human annotators, ensuring clear alignment between original content and hallucinated generations. Using this dataset, we evaluate a diverse set of contemporary open-source and proprietary language models on fine-grained hallucination detection. Our results show that question answering is consistently easier than dialogue summarization, while sentence-level hallucinations remain challenging even for the strongest models. Performance is highest in English and degrades in lower-resource languages, with Hindi exhibiting the lowest detection accuracy. Overall, Halluverse-M^3 provides a realistic and challenging benchmark for studying hallucinations in multilingual, multi-task settings. We release the dataset to support future research on hallucination detection and mitigation\footnote{https://huggingface.co/datasets/sabdalja/HalluVerse-M3}.

</details>


### [53] [Optimal Turkish Subword Strategies at Scale: Systematic Evaluation of Data, Vocabulary, Morphology Interplay](https://arxiv.org/abs/2602.06942)
*Duygu Altinok*

Main category: cs.CL

TL;DR: 本文提出并系统评估了面向土耳其语的亚词分词策略，首次结合词汇表大小和语料量，同时引入形态学感知诊断工具，全面解释分词效果来源，推动形态丰富语言分词技术的发展。


<details>
  <summary>Details</summary>
Motivation: 在形态丰富的语言如土耳其语中，形态结构复杂导致传统分词方法难以兼顾词汇效率和形态准确性。先前研究未能系统地控制训练语料与词汇表大小的关系，也缺乏形态学细节的诊断和广泛下游任务的验证，迫切需要一个全面、科学的分词器评估框架。

Method: 本文设计了一个联合调整词汇表大小和分词训练语料量的实验方案，比较了多种分词器（WordPiece、形态级、字符级）在相同参数预算下的表现，并利用丰富的语义、句法和形态敏感探测任务进行评测。同时提出了形态学感知的诊断工具，对分词边界准确率、过分割与欠分割指标、编辑距离等多个维度进行分析。

Result: 本文建立了一个形态学感知的“亚词宣言”，系统揭示了词汇规模、训练语料量与分词性能之间的互相关系，明确了不同分词方法在多类语言任务上的优势与局限，释放的开源代码和工具为后续研究提供了基础。

Conclusion: 本文首次系统地研究了土耳其语亚词分词的设计问题，明确了词汇表大小、分词器训练语料量与分词效果之间的关系。通过引入形态学感知的诊断工具包，揭示了分词成功与失败的内在机制，为形态丰富语言的分词构建了可复现的评估框架和实践指导。

Abstract: Tokenization is a pivotal design choice for neural language modeling in morphologically rich languages (MRLs) such as Turkish, where productive agglutination challenges both vocabulary efficiency and morphological fidelity. Prior studies have explored tokenizer families and vocabulary sizes but typically (i) vary vocabulary without systematically controlling the tokenizer's training corpus, (ii) provide limited intrinsic diagnostics, and (iii) evaluate a narrow slice of downstream tasks. We present the first comprehensive, principled study of Turkish subword tokenization; a "subwords manifest", that jointly varies vocabulary size and tokenizer training corpus size (data and vocabulary coupling), compares multiple tokenizer families under matched parameter budgets (WordPiece, morphology level, and character baselines), and evaluates across semantic (NLI, STS, sentiment analysis, NER), syntactic (POS, dependency parsing), and morphology-sensitive probes. To explain why tokenizers succeed or fail, we introduce a morphology-aware diagnostic toolkit that goes beyond coarse aggregates to boundary-level micro/macro F1, decoupled lemma atomicity vs. surface boundary hits, over/under-segmentation indices, character/word edit distances (CER/WER), continuation rates, and affix-type coverage and token-level atomicity. Our contributions are fourfold: (i) a systematic investigation of the vocabulary-corpus-success triad; (ii) a unified, morphology-aware evaluation framework linking intrinsic diagnostics to extrinsic outcomes; (iii) controlled comparisons identifying when character-level and morphology-level tokenization pay off; and (iv) an open-source release of evaluation code, tokenizer pipelines, and models. As the first work of its kind, this "subwords manifest" delivers actionable guidance for building effective tokenizers in MRLs and establishes a reproducible foundation for future research.

</details>


### [54] [DAWN: Dependency-Aware Fast Inference for Diffusion LLMs](https://arxiv.org/abs/2602.06953)
*Lizhuo Luo,Zhuoran Shi,Jiajun Luo,Zhi Wang,Shen Ren,Wenya Wang,Tianwei Zhang*

Main category: cs.CL

TL;DR: 本文提出了DAWN，一种基于词依赖关系的训练无关解码策略，通过依赖图选择可靠解码位置，实现了扩散大语言模型高效且高质量的快速推理。


<details>
  <summary>Details</summary>
Motivation: 传统的并行解码策略忽视了词之间的语义耦合性，这导致解码质量下降，因此需要一种能建模词间依赖关系的解码方法以提升效率和质量。

Method: DAWN利用一个依赖图来识别和选择更可靠的解码位置，并通过避免同时解码强耦合不确定位置来减少错误，从而实现了高并行度的训练无关依赖感知解码。

Result: DAWN在多个模型和数据集上实现了对比基线1.80至8.06倍的推理加速，同时保持生成质量不变。

Conclusion: DAWN方法能够在不牺牲生成质量的前提下显著提升扩散大语言模型的推理速度。

Abstract: Diffusion large language models (dLLMs) have shown advantages in text generation, particularly due to their inherent ability for parallel decoding. However, constrained by the quality--speed trade-off, existing inference solutions adopt conservative parallel strategies, leaving substantial efficiency potential underexplored. A core challenge is that parallel decoding assumes each position can be filled independently, but tokens are often semantically coupled. Thus, the correct choice at one position constrains valid choices at others. Without modeling these inter-token dependencies, parallel strategies produce deteriorated outputs. Motivated by this insight, we propose DAWN, a training-free, dependency-aware decoding method for fast dLLM inference. DAWN extracts token dependencies and leverages two key motivations: (1) positions dependent on unmasked certain positions become more reliable, (2) simultaneously unmasking strongly coupled uncertain positions induces errors. Given those findings, DAWN leverages a dependency graph to select more reliable unmasking positions at each iteration, achieving high parallelism with negligible loss in generation quality. Extensive experiments across multiple models and datasets demonstrate that DAWN speedups the inference by 1.80-8.06x over baselines while preserving the generation quality. Code is released at https://github.com/lizhuo-luo/DAWN.

</details>


### [55] [InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning](https://arxiv.org/abs/2602.06960)
*Yuchen Yan,Liang Jiang,Jin Jiang,Shuaicheng Li,Zujie Wen,Zhiqiang Zhang,Jun Zhou,Jian Shao,Yueting Zhuang,Yongliang Shen*

Main category: cs.CL

TL;DR: InftyThink+利用强化学习优化总结和推理策略，提升大型推理模型准确率和推理效率，克服了传统链式思维推理的限制。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在推理时扩展链式思维表现强劲，但存在计算成本高、上下文长度限制及中间信息丢失等问题，现有的迭代推理方法依赖监督学习或固定启发式，无法优化总结时机、保留内容和推理恢复策略。

Method: 提出InftyThink+，一个端到端强化学习框架，基于模型控制的迭代边界和显式总结，采用监督冷启动和轨迹级强化学习的两阶段训练方案，实现对总结和推理继续决策的策略学习。

Result: 在DeepSeek-R1-Distill-Qwen-1.5B上实验显示，InftyThink+在AIME24测试集上准确率提高21%，优于传统长链式思维强化学习，在分布外基准测试中的泛化能力更强，同时显著降低推理延迟并加速强化学习训练。

Conclusion: InftyThink+通过强化学习优化迭代推理过程，有效解决了传统链式思维推理存在的效率及准确度瓶颈，实现了推理性能和效率的双重提升。

Abstract: Large reasoning models achieve strong performance by scaling inference-time chain-of-thought, but this paradigm suffers from quadratic cost, context length limits, and degraded reasoning due to lost-in-the-middle effects. Iterative reasoning mitigates these issues by periodically summarizing intermediate thoughts, yet existing methods rely on supervised learning or fixed heuristics and fail to optimize when to summarize, what to preserve, and how to resume reasoning. We propose InftyThink+, an end-to-end reinforcement learning framework that optimizes the entire iterative reasoning trajectory, building on model-controlled iteration boundaries and explicit summarization. InftyThink+ adopts a two-stage training scheme with supervised cold-start followed by trajectory-level reinforcement learning, enabling the model to learn strategic summarization and continuation decisions. Experiments on DeepSeek-R1-Distill-Qwen-1.5B show that InftyThink+ improves accuracy by 21% on AIME24 and outperforms conventional long chain-of-thought reinforcement learning by a clear margin, while also generalizing better to out-of-distribution benchmarks. Moreover, InftyThink+ significantly reduces inference latency and accelerates reinforcement learning training, demonstrating improved reasoning efficiency alongside stronger performance.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [56] [Communication Enhances LLMs' Stability in Strategic Thinking](https://arxiv.org/abs/2602.06081)
*Nunzio Lore,Babak Heydari*

Main category: cs.MA

TL;DR: 廉价对话式预交流降低了大型语言模型在重复囚徒困境中的策略波动，提升了多智能体系统中战略行为的稳定性和可预测性，是一种低成本有效的策略改进方式。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在涉及战略推理的多智能体任务中表现出很强的依赖上下文变异性，导致策略行为缺乏可预测性，本文旨在探讨廉价无代价的预交流是否能增强多智能体系统中策略的稳定性。

Method: 使用包含7至9亿参数的语言模型，在十轮重复囚徒困境中，通过廉价对话式预先消息介入进行对比实验，采用模拟级引导重采样（bootstrap resampling）和非参数推断，结合LOWESS回归分析合作轨迹差异。

Result: 实验证明廉价对话显著减少了多数模型和语境组合中的策略轨迹噪声，稳定效果在不同提示词和解码机制下持续存在，且基线波动性较高的模型受益最大。少数情境下通信可能带来不利影响，但总体上通信是一种低成本的可靠手段。

Conclusion: 通过模拟实验和非参数推断，论文发现廉价对话式预先交流显著降低了大型语言模型在重复囚徒困境中的策略波动性，提高了策略的稳定性和可预测性。通信一般不会引发不稳定，仅在少数特定情境下存在负面效应。

Abstract: Large Language Models (LLMs) often exhibit pronounced context-dependent variability that undermines predictable multi-agent behavior in tasks requiring strategic thinking. Focusing on models that range from 7 to 9 billion parameters in size engaged in a ten-round repeated Prisoner's Dilemma, we evaluate whether short, costless pre-play messages emulating the cheap-talk paradigm affect strategic stability. Our analysis uses simulation-level bootstrap resampling and nonparametric inference to compare cooperation trajectories fitted with LOWESS regression across both the messaging and the no-messaging condition. We demonstrate consistent reductions in trajectory noise across a majority of the model-context pairings being studied. The stabilizing effect persists across multiple prompt variants and decoding regimes, though its magnitude depends on model choice and contextual framing, with models displaying higher baseline volatility gaining the most. While communication rarely produces harmful instability, we document a few context-specific exceptions and identify the limited domains in which communication harms stability. These findings position cheap-talk style communication as a low-cost, practical tool for improving the predictability and reliability of strategic behavior in multi-agent LLM systems.

</details>


### [57] [Prism: Spectral Parameter Sharing for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.06476)
*Kyungbeom Kim,Seungwon Oh,Kyung-Joong Kim*

Main category: cs.MA

TL;DR: Prism 通过频谱域参数共享实现多智能体多样性，兼顾性能和资源效率。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体强化学习中的参数共享虽提高了可扩展性，但导致智能体行为趋于同质化。现有方法通过聚类、剪枝或掩码引入多样性，但牺牲了资源效率。

Method: 通过奇异值分解（SVD）在频谱域共享网络权重，所有智能体共享奇异向量方向，但独立学习奇异值上的光谱掩码。

Result: Prism 在同质（LBF, SMACv2）和异质（MaMuJoCo）基准测试中表现出竞争力的性能，同时资源效率优于现有方法。

Conclusion: Prism 框架有效实现了多智能体之间的多样性，同时保持了参数共享的可扩展性和资源效率。

Abstract: Parameter sharing is a key strategy in multi-agent reinforcement learning (MARL) for improving scalability, yet conventional fully shared architectures often collapse into homogeneous behaviors. Recent methods introduce diversity through clustering, pruning, or masking, but typically compromise resource efficiency. We propose Prism, a parameter sharing framework that induces inter-agent diversity by representing shared networks in the spectral domain via singular value decomposition (SVD). All agents share the singular vector directions while learning distinct spectral masks on singular values. This mechanism encourages inter-agent diversity and preserves scalability. Extensive experiments on both homogeneous (LBF, SMACv2) and heterogeneous (MaMuJoCo) benchmarks show that Prism achieves competitive performance with superior resource efficiency.

</details>


### [58] [Sample-Efficient Policy Space Response Oracles with Joint Experience Best Response](https://arxiv.org/abs/2602.06599)
*Ariyan Bighashdel,Thiago D. Simão,Frans A. Oliehoek*

Main category: cs.MA

TL;DR: 提出JBR方法优化PSRO，通过共享轨迹数据提升多智能体最佳响应计算效率，兼顾准确性与样本成本，增强大规模多智能体学习的实用性。


<details>
  <summary>Details</summary>
Motivation: 传统的多智能体强化学习存在非平稳性和策略多样性维护困难的问题，且基于每个智能体的最佳响应训练在多智能体或高开销环境中极为昂贵。

Method: 提出了一种Joint Experience Best Response (JBR)方法，对PSRO进行改进，通过一次性收集轨迹来同时计算多个智能体的最佳响应，转换为离线强化学习问题，并提出了三种缓解分布偏移的策略：保守JBR、探索增强JBR和混合BR。

Result: 在多智能体基准环境中，探索增强JBR达到了最佳的准确性和效率权衡，混合BR在样本成本大幅降低的同时达到接近PSRO的性能，显著提升了PSRO在大规模战略学习中的实用性。

Conclusion: JBR有效提高了多智能体强化学习中PSRO的样本效率和计算效率，解决了非平稳性和策略多样性维护问题，使得大规模战略学习更为实际和稳健。

Abstract: Multi-agent reinforcement learning (MARL) offers a scalable alternative to exact game-theoretic analysis but suffers from non-stationarity and the need to maintain diverse populations of strategies that capture non-transitive interactions. Policy Space Response Oracles (PSRO) address these issues by iteratively expanding a restricted game with approximate best responses (BRs), yet per-agent BR training makes it prohibitively expensive in many-agent or simulator-expensive settings. We introduce Joint Experience Best Response (JBR), a drop-in modification to PSRO that collects trajectories once under the current meta-strategy profile and reuses this joint dataset to compute BRs for all agents simultaneously. This amortizes environment interaction and improves the sample efficiency of best-response computation. Because JBR converts BR computation into an offline RL problem, we propose three remedies for distribution-shift bias: (i) Conservative JBR with safe policy improvement, (ii) Exploration-Augmented JBR that perturbs data collection and admits theoretical guarantees, and (iii) Hybrid BR that interleaves JBR with periodic independent BR updates. Across benchmark multi-agent environments, Exploration-Augmented JBR achieves the best accuracy-efficiency trade-off, while Hybrid BR attains near-PSRO performance at a fraction of the sample cost. Overall, JBR makes PSRO substantially more practical for large-scale strategic learning while preserving equilibrium robustness.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [59] [SVRepair: Structured Visual Reasoning for Automated Program Repair](https://arxiv.org/abs/2602.06090)
*Xiaoxuan Tang,Jincheng Wang,Liwei Luo,Jingxuan Xu,Sheng Zhou,Dajun Chen,Wei Jiang,Yong Li*

Main category: cs.SE

TL;DR: SVRepair通过结构化视觉表示改进多模态自动程序修复，显著提升了程序修复的准确率与效果。


<details>
  <summary>Details</summary>
Motivation: 现有的自动程序修复方法多为单模态，未充分利用截图和控制流图等视觉信息，导致修复效果受限。

Method: 提出了结构化视觉表示（SVR）模型，将多种视觉工件转化为语义场景图，并结合迭代视觉工件分割策略以提升错误定位和补丁合成的精度。

Result: SVRepair在多个基准测试中表现优异，准确率分别达到SWE-Bench M的36.47%、MMCode的38.02%、CodeVision的95.12%。

Conclusion: SVRepair提出的多模态自动程序修复框架通过结构化视觉表示，有效提升了基于视觉信号的程序修复准确率。

Abstract: Large language models (LLMs) have recently shown strong potential for Automated Program Repair (APR), yet most existing approaches remain unimodal and fail to leverage the rich diagnostic signals contained in visual artifacts such as screenshots and control-flow graphs. In practice, many bug reports convey critical information visually (e.g., layout breakage or missing widgets), but directly using such dense visual inputs often causes context loss and noise, making it difficult for MLLMs to ground visual observations into precise fault localization and executable patches. To bridge this semantic gap, we propose \textbf{SVRepair}, a multimodal APR framework with structured visual representation. SVRepair first fine-tunes a vision-language model, \textbf{Structured Visual Representation (SVR)}, to uniformly transform heterogeneous visual artifacts into a \emph{semantic scene graph} that captures GUI elements and their structural relations (e.g., hierarchy), providing normalized, code-relevant context for downstream repair. Building on the graph, SVRepair drives a coding agent to localize faults and synthesize patches, and further introduces an iterative visual-artifact segmentation strategy that progressively narrows the input to bug-centered regions to suppress irrelevant context and reduce hallucinations. Extensive experiments across multiple benchmarks demonstrate state-of-the-art performance: SVRepair achieves \textbf{36.47\%} accuracy on SWE-Bench M, \textbf{38.02\%} on MMCode, and \textbf{95.12\%} on CodeVision, validating the effectiveness of SVRepair for multimodal program repair.

</details>


### [60] [Coding Agents with Environment Interaction: A Theoretical Perspective](https://arxiv.org/abs/2602.06098)
*Nicolas Menet,Michael Hersche,Andreas Krause,Abbas Rahimi*

Main category: cs.SE

TL;DR: 本文从概率视角解析测试驱动开发中编码代理的环境交互策略，提出并验证了基于模糊函数相似度的代码选择估计器和反向提示的Thompson采样近似，揭示了任务描述模糊性对性能的限制，并提出了改进任务描述的新基准。


<details>
  <summary>Details</summary>
Motivation: 编码代理在测试驱动的软件开发中应用日益广泛，但其环境交互策略的理论机制尚未充分研究。

Method: 本文提出了一个基于概率的框架，形式化了两种主流范式：生成后基于执行环境的代码选择和基于环境反馈的代码生成。首先，将多种选择启发式方法形式化为环境感知的代码正确性估计器，并证明基于模糊函数相似度的估计器比基于函数等价的估计器在信噪比上有优势。其次，将反向提示方法视为Thompson采样的上下文内近似，推导出针对含不可观察成分奖励函数的新后悔界，解释了反向提示效果受任务描述模糊性的限制。

Result: 在三个先进的公开模型和多个数据集上验证了理论分析，发现模糊函数相似度估计器优于函数等价估计器，反向提示的表现受任务描述不确定性影响，并基于此提出改进任务描述的新基准QiskitHumanEvalSimX。

Conclusion: 本文为编码代理的环境交互策略提供了系统的概率理论框架，揭示了模糊函数相似度估计器的优势及反向提示方法的理论限制，并通过实验证实理论结果，同时提出改进任务描述的新基准。

Abstract: Coding agents are increasingly utilized in test-driven software development, yet the theoretical mechanisms behind their environment-interaction strategies remain underexplored. We provide a probabilistic framework for two dominant paradigms: code selection after generation using the execution environment, and code generation conditioned on environment feedback. First, we formalize several well-established selection heuristics as environment-aware estimators of code correctness. We theoretically prove that estimators based on fuzzy functional similarity add an inductive bias and strictly dominate estimators based on functional equivalence in terms of signal-to-noise ratio. Second, we frame backprompting as an in-context approximation of Thompson sampling. We derive a novel regret bound for reward functions with unobservable components, theoretically explaining why the effectiveness of backprompting is limited by the ambiguity of the informal task description (an irreducible regret). Using three state-of-the-art open weight models, we corroborate these findings across BigCodeBenchHard, LeetCodeDataset, and QiskitHumanEvalSim. Our formalization also suggests how to improve task descriptions effectively, leading to a new benchmark, QiskitHumanEvalSimX.

</details>


### [61] [Scaling Mobile Chaos Testing with AI-Driven Test Execution](https://arxiv.org/abs/2602.06223)
*Juan Marcano,Ashish Samant,Kai Song,Lingchao Chen,Kaelan Mikowicz,Tim Smyth,Mengdie Zhang,Ali Zamani,Arturo Bravo Rovirosa,Sowjanya Puligadda,Srikanth Prodduturi,Mayank Bansal*

Main category: cs.SE

TL;DR: 本文提出了一种结合AI和故障注入的自动化移动混沌测试系统，成功应用于Uber多款应用的大规模测试中，有效发现关键故障并提升调试效率，证明了移动应用弹性验证的可扩展性和实效性。


<details>
  <summary>Details</summary>
Motivation: 移动应用在大规模分布式系统中易受到后端服务故障影响，传统混沌工程方法无法有效应对复杂多变的测试场景。

Method: 提出了一套自动化移动混沌测试系统，将基于大语言模型的DragonCrawl测试平台与uHavoc服务级故障注入系统结合，利用自适应AI驱动测试执行以覆盖多维度故障场景。

Result: 系统自2024年Q1以来执行了18万余次测试，覆盖47个关键流程，发现23个弹性风险，自动根因分析实现88%精度，测试可靠性达到99%。

Conclusion: 该系统实现了生产规模下的连续移动弹性验证，有效提升了测试效率和故障检测能力，显著缩短了调试时间，验证了自动化移动混沌测试的可行性和实用价值。

Abstract: Mobile applications in large-scale distributed systems are susceptible to backend service failures, yet traditional chaos engineering approaches cannot scale mobile testing due to the combinatorial explosion of flows, locations, and failure scenarios that need validation. We present an automated mobile chaos testing system that integrates DragonCrawl, an LLM-based mobile testing platform, with uHavoc, a service-level fault injection system. The key insight is that adaptive AI-driven test execution can navigate mobile applications under degraded backend conditions, eliminating the need to manually write test cases for each combination of user flow, city, and failure type. Since Q1 2024, our system has executed over 180,000 automated chaos tests across 47 critical flows in Uber's Rider, Driver, and Eats applications, representing approximately 39,000 hours of manual testing effort that would be impractical at this scale. We identified 23 resilience risks, with 70% being architectural dependency violations where non-critical service failures degraded core user flows. Twelve issues were severe enough to prevent trip requests or food orders. Two caused application crashes detectable only through mobile chaos testing, not backend testing alone. Automated root cause analysis reduced debugging time from hours to minutes, achieving 88% precision@5 in attributing mobile failures to specific backend services. This paper presents the system design, evaluates its performance under fault injection (maintaining 99% test reliability), and reports operational experience demonstrating that continuous mobile resilience validation is achievable at production scale.

</details>


### [62] [Trustworthy AI Software Engineers](https://arxiv.org/abs/2602.06310)
*Aldeida Aleti,Baishakhi Ray,Rashina Hoda,Simin Chen*

Main category: cs.SE

TL;DR: 本文探讨AI软件工程师的定义与可信性，提出评估框架及伦理设计策略，促进未来人机协作团队的信任建立。


<details>
  <summary>Details</summary>
Motivation: 随着AI编码代理的快速发展，软件工程师的定义和信任机制亟需重新审视，以确保未来人机协作团队的有效运作。

Method: 基于软件工程的既定定义和代理AI系统的最新研究，构建AI软件工程师概念模型，并分析其可信性的关键维度及评估方法。

Result: 提出可信性的多维度框架，包括技术质量、透明度与问责、认知谦逊、社会伦理一致性，指出信任的测量存在缺口，并倡导伦理设计方法。

Conclusion: AI软件工程师应被视为人类与AI协作的软件工程团队成员，其可信性是系统与参与者的关键属性，需通过多维度指标进行评估，并在设计和治理中融入伦理原则。

Abstract: With the rapid rise of AI coding agents, the fundamental premise of what it means to be a software engineer is in question. In this vision paper, we re-examine what it means for an AI agent to be considered a software engineer and then critically think about what makes such an agent trustworthy. \textit{Grounded} in established definitions of software engineering (SE) and informed by recent research on agentic AI systems, we conceptualise AI software engineers as participants in human-AI SE teams composed of human software engineers and AI models and tools, and we distinguish trustworthiness as a key property of these systems and actors rather than a subjective human attitude. Based on historical perspectives and emerging visions, we identify key dimensions that contribute to the trustworthiness of AI software engineers, spanning technical quality, transparency and accountability, epistemic humility, and societal and ethical alignment. We further discuss how trustworthiness can be evaluated and demonstrated, highlighting a fundamental trust measurement gap: not everything that matters for trust can be easily measured. Finally, we outline implications for the design, evaluation, and governance of AI SE systems, advocating for an ethics-by-design approach to enable appropriate trust in future human-AI SE teams.

</details>


### [63] [AgentStepper: Interactive Debugging of Software Development Agents](https://arxiv.org/abs/2602.06593)
*Robert Hutter,Michael Pradel*

Main category: cs.SE

TL;DR: 本文提出AgentStepper，一种面向基于LLM的软件开发代理的交互式调试器，改善了代理轨迹的可视化和调试效果，提升了开发效率。


<details>
  <summary>Details</summary>
Motivation: 当前调试基于LLM的软件开发代理困难，缺少对中间过程的高层次展示，需提升调试体验。

Method: 提出AgentStepper，通过将代理行为表示为LLM、程序和工具间的结构化对话，支持断点、逐步执行及实时编辑提示和工具调用，并捕获中间代码变更。

Result: AgentStepper在应用于三款先进代理时，代码改动少，用户研究显示提高了理解轨迹和找到bug的能力，并降低了工作负载。

Conclusion: AgentStepper作为首个交互式调试器，有效提升了开发者理解和调试基于LLM的软件开发代理的能力。

Abstract: Software development agents powered by large language models (LLMs) have shown great promise in automating tasks like environment setup, issue solving, and program repair. Unfortunately, understanding and debugging such agents remain challenging due to their complex and dynamic nature. Developers must reason about trajectories of LLM queries, tool calls, and code modifications, but current techniques reveal little of this intermediate process in a comprehensible format. The key insight of this paper is that debugging software development agents shares many similarities with conventional debugging of software programs, yet requires a higher level of abstraction that raises the level from low-level implementation details to high-level agent actions. Drawing on this insight, we introduce AgentStepper, the first interactive debugger for LLM-based software engineering agents. AgentStepper enables developers to inspect, control, and interactively manipulate agent trajectories. AgentStepper represents trajectories as structured conversations among an LLM, the agent program, and tools. It supports breakpoints, stepwise execution, and live editing of prompts and tool invocations, while capturing and displaying intermediate repository-level code changes. Our evaluation applies AgentStepper to three state-of-the-art software development agents, ExecutionAgent, SWE-Agent, and RepairAgent, showing that integrating the approach into existing agents requires minor code changes (39-42 edited lines). Moreover, we report on a user study with twelve participants, indicating that AgentStepper improves the ability of participants to interpret trajectories (64% vs. 67% mean performance) and identify bugs in the agent's implementation (17% vs. 60% success rate), while reducing perceived workload (e.g., frustration reduced from 5.4/7.0 to 2.4/7.0) compared to conventional tools.

</details>


### [64] [Code vs Serialized AST Inputs for LLM-Based Code Summarization: An Empirical Study](https://arxiv.org/abs/2602.06671)
*Shijia Dong,Haoruo Zhao,Paul Harvey*

Main category: cs.SE

TL;DR: 提出一种完整AST的序列化方法，用于大语言模型代码摘要，提升训练效率并保持摘要质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的代码摘要方法大多依赖原始代码或只部分利用AST信息，没有充分挖掘完整AST表示的潜力。

Method: 设计了一种AST增强和序列化方法，将完整的AST结构信息编码为兼容大语言模型的序列，保留词汇细节，便于训练和推理。

Result: 基于LLaMA-3.1-8B模型和CodeXGLUE Python数据集的实验表明，序列化的完整AST能够减少输入长度，缩短训练时间，且摘要质量与现有方法相当。

Conclusion: 提出的AST(NIT)方法通过序列化完整的抽象语法树（AST），在不损失词汇细节的同时，提升了大语言模型的代码摘要质量，并减少了模型输入长度和训练时间。

Abstract: Summarizing source code into natural language descriptions (code summarization) helps developers better understand program functionality and reduce the burden of software maintenance. Abstract Syntax Trees (ASTs), as opposed to source code, have been shown to improve summarization quality in traditional encoder-decoder-based code summarization models. However, most large language model (LLM)-based code summarization methods rely on raw code or only incorporate partial AST signals, meaning that the potential of complete AST representation has not been fully explored for LLMs. This paper presents AST(NIT), an AST augmentation and serialization method that preserves lexical details and encodes structural information into LLM-compatible sequences. Experiments with the LLaMA-3.1-8B model on the CodeXGLUE Python dataset show that the proposed serialized ASTs reduce the length of LLM inputs, require shorter training times, and achieve summarization quality comparable to existing approaches.

</details>


### [65] [Using Large Language Models to Support Automation of Failure Management in CI/CD Pipelines: A Case Study in SAP HANA](https://arxiv.org/abs/2602.06709)
*Duong Bui,Stefan Grintz,Alexander Berndt,Thomas Bach*

Main category: cs.SE

TL;DR: 本研究证明大型语言模型结合历史失败数据，能高效准确自动化CI/CD流水线故障管理，显著提升错误定位及解决方案准确率。


<details>
  <summary>Details</summary>
Motivation: CI/CD流水线故障管理手动耗时且复杂，传统程序难以自动处理非结构化信息，因此探索利用LLM自动化此过程的可能性。

Method: 通过向LLM系统输入不同类型的领域知识（流水线信息、故障管理指令和历史失败数据），评估其定位错误位置和生成准确解决方案的能力，并进行消融实验分析不同知识的贡献。

Result: 系统在提供领域知识情况下错误定位准确率达97.4%，无领域知识时为84.2%；利用历史失败数据，解决方案准确率达到92.1%。

Conclusion: LLM在提供历史失败数据的情况下，可以有效自动化CI/CD流水线的故障管理，准确率高，表现优异。

Abstract: CI/CD pipeline failure management is time-consuming when performed manually. Automating this process is non-trivial because the information required for effective failure management is unstructured and cannot be automatically processed by traditional programs. With their ability to process unstructured data, large language models (LLMs) have shown promising results for automated failure management by previous work. Following these studies, we evaluated whether an LLM-based system could automate failure management in a CI/CD pipeline in the context of a large industrial software project, namely SAP HANA. We evaluated the ability of the LLM-based system to identify the error location and to propose exact solutions that contain no unnecessary actions. To support the LLM in generating exact solutions, we provided it with different types of domain knowledge, including pipeline information, failure management instructions, and data from historical failures. We conducted an ablation study to determine which type of domain knowledge contributed most to solution accuracy. The results show that data from historical failures contributed the most to the system's accuracy, enabling it to produce exact solutions in 92.1% of cases in our dataset. The system correctly identified the error location with 97.4% accuracy when provided with domain knowledge, compared to 84.2% accuracy without it. In conclusion, our findings indicate that LLMs, when provided with data from historical failures, represent a promising approach for automating CI/CD pipeline failure management.

</details>


### [66] [Statistical-Based Metric Threshold Setting Method for Software Fault Prediction in Firmware Projects: An Industrial Experience](https://arxiv.org/abs/2602.06831)
*Marco De Luca,Domenico Amalfitano,Anna Rita Fasolino,Porfirio Tramontana*

Main category: cs.SE

TL;DR: 本文提出了一种基于软件度量阈值的可解释故障预测方法，适用于嵌入式固件行业软件质量保证，有效且易于工业应用，解决了机器学习黑盒模型的不透明问题。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习故障预测模型虽准确，但缺乏可解释性，难以被工业界广泛采纳，需提供可直接应用于软件质量保障的可解释解决方案。

Method: 利用Coverity和Understand静态分析工具提取嵌入式固件的度量数据，采用统计分析和假设检验确定判别故障的度量阈值，并跨项目验证阈值的有效性。

Result: 在三组真实工业嵌入式固件项目中验证了阈值方法，结果表明该方法能够高精度识别故障函数，支持跨项目复用，无需重新训练或领域调优。

Conclusion: 该论文提出了一种基于软件度量阈值的故障预测方法，通过阈值识别高风险代码函数，实现了高精度的故障检测，符合行业标准和软件质量保障实践。

Abstract: Ensuring software quality in embedded firmware is critical, especially in safety-critical domains where compliance with functional safety standards (ISO 26262) requires strong guarantees of software reliability. While machine learning-based fault prediction models have demonstrated high accuracy, their lack of interpretability limits their adoption in industrial settings. Developers need actionable insights that can be directly employed in software quality assurance processes and guide defect mitigation strategies. In this paper, we present a structured process for defining context-specific software metric thresholds suitable for integration into fault detection workflows in industrial settings. Our approach supports cross-project fault prediction by deriving thresholds from one set of projects and applying them to independently developed firmware, thereby enabling reuse across similar software systems without retraining or domain-specific tuning. We analyze three real-world C-embedded firmware projects provided by an industrial partner, using Coverity and Understand static analysis tools to extract software metrics. Through statistical analysis and hypothesis testing, we identify discriminative metrics and derived empirical threshold values capable of distinguishing faulty from non-faulty functions. The derived thresholds are validated through an experimental evaluation, demonstrating their effectiveness in identifying fault-prone functions with high precision. The results confirm that the derived thresholds can serve as an interpretable solution for fault prediction, aligning with industry standards and SQA practices. This approach provides a practical alternative to black-box AI models, allowing developers to systematically assess software quality, take preventive actions, and integrate metric-based fault prediction into industrial development workflows to mitigate software faults.

</details>


### [67] [TraceCoder: A Trace-Driven Multi-Agent Framework for Automated Debugging of LLM-Generated Code](https://arxiv.org/abs/2602.06875)
*Jiangping Huang,Wenguang Ye,Weisong Sun,Jian Zhang,Mingyue Zhang,Yang Liu*

Main category: cs.SE

TL;DR: TraceCoder利用细粒度运行时追踪和因果分析结合历史教训学习，显著提升了自动代码修复的准确率和效率。


<details>
  <summary>Details</summary>
Motivation: 现有自动修复方法依赖于简单的通过/未通过信号，难以准确定位错误且易陷入低效重复修复循环，缺乏从历史失败中学习的能力。

Method: 通过在代码中植入诊断探针获取细粒度运行时信息，结合因果分析定位错误根因，并利用历史教训学习机制从之前的失败中提炼经验，辅以回滚机制保证每次修复迭代改进，形成观察-分析-修复的循环。

Result: 在多个基准测试中，TraceCoder在Pass@1准确率上相较现有先进方法提升了最多34.43%，迭代修复过程单独贡献了65.61%的准确率提升，并在准确率和成本效益上显著超过主流迭代修复方法。

Conclusion: TraceCoder通过多智能体协作框架和细粒度运行时追踪，实现了对代码错误的准确定位和高效修复，显著提升了代码生成的正确率和修复效率。

Abstract: Large Language Models (LLMs) often generate code with subtle but critical bugs, especially for complex tasks. Existing automated repair methods typically rely on superficial pass/fail signals, offering limited visibility into program behavior and hindering precise error localization. In addition, without a way to learn from prior failures, repair processes often fall into repetitive and inefficient cycles. To overcome these challenges, we present TraceCoder, a collaborative multi-agent framework that emulates the observe-analyze-repair process of human experts. The framework first instruments the code with diagnostic probes to capture fine-grained runtime traces, enabling deep insight into its internal execution. It then conducts causal analysis on these traces to accurately identify the root cause of the failure. This process is further enhanced by a novel Historical Lesson Learning Mechanism (HLLM), which distills insights from prior failed repair attempts to inform subsequent correction strategies and prevent recurrence of similar mistakes. To ensure stable convergence, a Rollback Mechanism enforces that each repair iteration constitutes a strict improvement toward the correct solution. Comprehensive experiments across multiple benchmarks show that TraceCoder achieves up to a 34.43\% relative improvement in Pass@1 accuracy over existing advanced baselines. Ablation studies verify the significance of each system component, with the iterative repair process alone contributing a 65.61\% relative gain in accuracy. Furthermore, TraceCoder significantly outperforms leading iterative methods in terms of both accuracy and cost-efficiency.

</details>
