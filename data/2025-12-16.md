<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 67]
- [cs.MA](#cs.MA) [Total: 4]
- [cs.SE](#cs.SE) [Total: 30]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Enhancing Urban Visual Place Recognition for Crowdsourced Flood Imagery via LLM-Guided Attention](https://arxiv.org/abs/2512.11811)
*Fengyi Xu,Jun Ma,Waishan Qiu,Cui Guo*

Main category: cs.CL

TL;DR: 该论文提出了VPR-AttLLM框架，利用大语言模型（LLM）的语义推理和地理空间知识，通过注意力机制增强视觉地点识别（VPR）模型的描述符，有效提升了社交媒体众包城市洪水图像的地理定位性能。


<details>
  <summary>Details</summary>
Motivation: 社交媒体提供的众包街景图像缺乏可靠地理元数据，传统VPR模型在跨源场景中受视觉扭曲和领域偏移影响表现下降，亟需改进以支持应急响应。

Method: 设计了VPR-AttLLM，一种模型无关的框架，将LLM用于识别城市场景中位置信息丰富的区域，同时抑制瞬时视觉噪声，通过注意力引导增强描述符，无需模型重训或额外数据即可提升性能。

Result: 在多个扩展基准数据集（含真实社交洪水图像和不同城市景观）上，VPR-AttLLM与三种顶尖VPR模型结合后召回率提升1%-3%，在最具挑战性的场景下提升达8%。

Conclusion: VPR-AttLLM提出了一个通用的LLM引导多模态融合范式，实现了类似人类空间推理的城市感知理论与现代VPR架构的结合，具有良好的扩展性和解释性，适合城市监测和众包危机图像的快速地理定位。

Abstract: Crowdsourced street-view imagery from social media provides valuable real-time visual evidence of urban flooding and other crisis events, yet it often lacks reliable geographic metadata for emergency response. Existing Visual Place Recognition (VPR) models exhibit substantial performance degradation when applied to such imagery due to visual distortions and domain shifts inherent in cross-source scenarios. This paper presents VPR-AttLLM, a model-agnostic framework that integrates the semantic reasoning and geospatial knowledge of Large Language Models (LLMs) into established VPR pipelines through attention-guided descriptor enhancement. By leveraging LLMs to identify location-informative regions within the city context and suppress transient visual noise, VPR-AttLLM improves retrieval performance without requiring model retraining or additional data. Comprehensive evaluations are conducted on extended benchmarks including SF-XL enriched with real social-media flood images, synthetic flooding scenarios over established query sets and Mapillary photos, and a new HK-URBAN dataset capturing morphologically distinct cityscapes. Integrating VPR-AttLLM with three state-of-the-art VPR models-CosPlace, EigenPlaces, and SALAD-consistently improves recall performance, yielding relative gains typically between 1-3% and reaching up to 8% on the most challenging real flood imagery. Beyond measurable gains in retrieval accuracy, this study establishes a generalizable paradigm for LLM-guided multimodal fusion in visual retrieval systems. By embedding principles from urban perception theory into attention mechanisms, VPR-AttLLM bridges human-like spatial reasoning with modern VPR architectures. Its plug-and-play design, strong cross-source robustness, and interpretability highlight its potential for scalable urban monitoring and rapid geo-localization of crowdsourced crisis imagery.

</details>


### [2] [Reinforcement Learning for Latent-Space Thinking in LLMs](https://arxiv.org/abs/2512.11816)
*Enes Özeren,Matthias Aßenmacher*

Main category: cs.CL

TL;DR: 本文提出一种基于潜在空间的链式思维方法，结合强化学习优化潜在思维步骤，以提升数学推理任务表现。


<details>
  <summary>Details</summary>
Motivation: 传统的链式思维多基于离散语言空间，效率低且生成的许多符号仅遵循语言规则，对推理无实际帮助，潜在空间思维以连续嵌入空间思考，有望提高效率，但现有方法在复杂任务表现不足。

Method: 研究现有的监督微调潜在空间思维方法（如Coconut）的局限，进一步探索强化学习技术（如GRPO），设计新颖的潜在空间强化学习方法，直接优化潜在思维步骤。

Result: 实验显示采用强化学习训练的模型在数学推理任务中仍逊色于传统基于语言空间的链式思维模型，揭示潜在空间强化学习方法的挑战。

Conclusion: 虽然潜在空间思维结合强化学习是潜在的方向，但当前方法在复杂任务上仍难以超越传统语言空间链式思维，需进一步研究改进。此外，作者公开了代码以促进后续研究。

Abstract: Chain-of-Thought (CoT) reasoning typically utilizes the discrete language space for thinking, which is inherently inefficient, as many generated tokens only enforce linguistic rules that are not required for reasoning. To bypass this, latent-space thinking allows models to think using the continuous embedding space. While existing methods for training those models show domain-specific gains, they fail to maintain performance in complex tasks, such as mathematical reasoning. We experimentally demonstrate that the Coconut approach, a form of supervised fine-tuning for latent-space thinking, is highly sensitive to design choices and exhibits several inherent limitations. To address these issues, we investigate reinforcement learning (RL) techniques -- an underexplored direction in latent-space thinking -- including GRPO and design a novel Latent RL method for directly optimizing the latent thinking steps. Our experimental results reveal that these RL-trained models still lag behind traditional language-space CoT models in the mathematical reasoning domain. We make our codebase publicly available.

</details>


### [3] [KH-FUNSD: A Hierarchical and Fine-Grained Layout Analysis Dataset for Low-Resource Khmer Business Document](https://arxiv.org/abs/2512.11849)
*Nimol Thuon,Jun Du*

Main category: cs.CL

TL;DR: 该论文介绍了首个用于高棉语商业文档理解的层级标注数据集KH-FUNSD，支持多层次文档版面分析与信息提取。


<details>
  <summary>Details</summary>
Motivation: 当前低资源的非拉丁文字文档版面分析存在挑战，高棉语尤其缺乏相关资源，商业文档作为关键文档类型亟需专用数据支持。

Method: 构建KH-FUNSD数据集，采用三级标注体系，包括区域检测（页眉、表单字段、页脚）、FUNSD风格实体识别及关系标注、细粒度语义角色分类，实现综合版面分析与信息提取。

Result: 提供多种领先模型的基准测试，是首个针对高棉语商业文档的基线结果，揭示非拉丁低资源语言文档处理的独特难点。

Conclusion: KH-FUNSD填补了高棉语商业文档AI工具的资源空白，推动了非拉丁文字文档理解领域的发展。数据集及相关文档将公开发布。

Abstract: Automated document layout analysis remains a major challenge for low-resource, non-Latin scripts. Khmer is a language spoken daily by over 17 million people in Cambodia, receiving little attention in the development of document AI tools. The lack of dedicated resources is particularly acute for business documents, which are critical for both public administration and private enterprise. To address this gap, we present \textbf{KH-FUNSD}, the first publicly available, hierarchically annotated dataset for Khmer form document understanding, including receipts, invoices, and quotations. Our annotation framework features a three-level design: (1) region detection that divides each document into core zones such as header, form field, and footer; (2) FUNSD-style annotation that distinguishes questions, answers, headers, and other key entities, together with their relationships; and (3) fine-grained classification that assigns specific semantic roles, such as field labels, values, headers, footers, and symbols. This multi-level approach supports both comprehensive layout analysis and precise information extraction. We benchmark several leading models, providing the first set of baseline results for Khmer business documents, and discuss the distinct challenges posed by non-Latin, low-resource scripts. The KH-FUNSD dataset and documentation will be available at URL.

</details>


### [4] [Direct Confidence Alignment: Aligning Verbalized Confidence with Internal Confidence In Large Language Models](https://arxiv.org/abs/2512.11998)
*Glenn Zhang,Treasure Mayowa,Jason Fan,Yicheng Fu,Aaron Sandoval,Sean O'Brien,Kevin Zhu*

Main category: cs.CL

TL;DR: 本文提出了直接置信度对齐（DCA）方法，通过直接偏好优化，使大语言模型的口头置信度与内部置信度更好地对齐，提升模型的透明性和可靠性。实验表明，DCA在部分模型架构上提升了对齐效果，但在其他模型上效果有限，提示了模型感知方法的必要性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的内部置信度与其口头表达的置信度不一致，导致校准方法效果不佳，影响模型的可靠性和可信度。

Method: 提出基于直接偏好优化的直接置信度对齐（DCA）方法，通过对齐模型的口头置信度与内部置信度，而非与真实准确率对齐，提升置信度表达的一致性。还引入了三种新的基于校准误差的评价指标。

Result: 在多个公开权重大语言模型和多种数据集上验证，DCA在某些模型架构上显著改善了置信度的对齐指标，减少了置信度表达的不一致，但在部分模型上效果较差。

Conclusion: DCA方法能在一定程度上提升模型置信度表达的一致性和透明度，但其效果受模型架构限制，未来需探索更具模型感知性的校准方法以实现更可信赖的语言模型。

Abstract: Producing trustworthy and reliable Large Language Models (LLMs) has become increasingly important as their usage becomes more widespread. Calibration seeks to achieve this by improving the alignment between the model's confidence and the actual likelihood of its responses being correct or desirable. However, it has been observed that the internal confidence of a model, derived from token probabilities, is not well aligned with its verbalized confidence, leading to misleading results with different calibration methods. In this paper, we propose Direct Confidence Alignment (DCA), a method using Direct Preference Optimization to align an LLM's verbalized confidence with its internal confidence rather than ground-truth accuracy, enhancing model transparency and reliability by ensuring closer alignment between the two confidence measures. We evaluate DCA across multiple open-weight LLMs on a wide range of datasets. To further assess this alignment, we also introduce three new calibration error-based metrics. Our results show that DCA improves alignment metrics on certain model architectures, reducing inconsistencies in a model's confidence expression. However, we also show that it can be ineffective on others, highlighting the need for more model-aware approaches in the pursuit of more interpretable and trustworthy LLMs.

</details>


### [5] [Hold Onto That Thought: Assessing KV Cache Compression On Reasoning](https://arxiv.org/abs/2512.12008)
*Minghui Liu,Aadi Palnitkar,Tahseen Rabbani,Hyunwoo Jae,Kyle Rui Sang,Dixi Yao,Shayan Shabihi,Fuheng Zhao,Tian Li,Ce Zhang,Furong Huang,Kunpeng Zhang*

Main category: cs.CL

TL;DR: 本文评估了多种键值缓存(KV cache)压缩策略在长推理任务中的表现，发现不同任务适合不同策略，尤其是H2O和SnapKV变体在推理模型中表现优异。


<details>
  <summary>Details</summary>
Motivation: 长上下文任务中KV缓存因上下文长度线性增长导致内存瓶颈，现有压缩策略多针对预填充阶段，缺少对长推理任务的评估。

Method: 针对多个压缩策略在长推理任务和非推理任务上进行基准测试，重点分析缓存大小与推理成本的权衡。

Result: 发现没有单一策略适用于所有场景，数据集类型影响显著；H2O和SnapKV变体在推理模型中表现最佳；低缓存预算下，剔除策略可支持更长推理序列。

Conclusion: 重击跟踪策略有助于提升推理任务的KV缓存管理效果，存在缓存大小与推理成本的权衡，需要根据任务特性选择合适的压缩策略。

Abstract: Large language models (LLMs) have demonstrated remarkable performance on long-context tasks, but are often bottlenecked by memory constraints. Namely, the KV cache, which is used to significantly speed up attention computations, grows linearly with context length. A suite of compression algorithms has been introduced to alleviate cache growth by evicting unimportant tokens. However, several popular strategies are targeted towards the prefill phase, i.e., processing long prompt context, and their performance is rarely assessed on reasoning tasks requiring long decoding. In particular, short but complex prompts, such as those in benchmarks like GSM8K and MATH500, often benefit from multi-step reasoning and self-reflection, resulting in thinking sequences thousands of tokens long. In this work, we benchmark the performance of several popular compression strategies on long-reasoning tasks. For the non-reasoning Llama-3.1-8B-Instruct, we determine that no singular strategy fits all, and that performance is heavily influenced by dataset type. However, we discover that H2O and our decoding-enabled variant of SnapKV are dominant strategies for reasoning models, indicating the utility of heavy-hitter tracking for reasoning traces. We also find that eviction strategies at low budgets can produce longer reasoning traces, revealing a tradeoff between cache size and inference costs.

</details>


### [6] [Benchmarking Contextual Understanding for In-Car Conversational Systems](https://arxiv.org/abs/2512.12042)
*Philipp Habicht,Lev Sorokin,Abdullah Saydemir,Ken E. Friedl,Andrea Stocco*

Main category: cs.CL

TL;DR: 本文探讨利用大型语言模型（LLMs）及先进提示技术评估车载对话问答系统的准确性和一致性，特别在餐厅推荐的案例中评估系统对用户语境的理解与响应质量。


<details>
  <summary>Details</summary>
Motivation: 传统人工评估车载对话问答系统效率低且成本高，需寻找一种可扩展且准确的自动评估方法，提升系统响应的准确率和用户体验。

Method: 通过合成用户语句及对应正确和包含错误的系统响应，采用输入输出提示、链式思考、自洽提示及多智能体提示，利用13种不同规模及厂商的推理与非推理LLMs对系统响应进行一致性和准确性评估。

Result: 多智能体提示技术在小型非推理模型中带来最大提升，推理模型普遍优于非推理模型，DeepSeek-R1推理模型表现最佳，达到0.99的F1分数且请求成本低廉，综合效率和成本表现最佳的是非推理模型DeepSeek-V3。

Conclusion: 基于LLM的评估方法为车载对话问答系统的上下文理解提供了高效、准确且成本效益优异的替代方案，优于传统人工评估，具有良好的应用前景。

Abstract: In-Car Conversational Question Answering (ConvQA) systems significantly enhance user experience by enabling seamless voice interactions. However, assessing their accuracy and reliability remains a challenge. This paper explores the use of Large Language Models (LLMs) alongside advanced prompting techniques and agent-based methods to evaluate the extent to which ConvQA system responses adhere to user utterances. The focus lies on contextual understanding and the ability to provide accurate venue recommendations considering user constraints and situational context. To evaluate utterance-response coherence using an LLM, we synthetically generate user utterances accompanied by correct and modified failure-containing system responses. We use input-output, chain-of-thought, self-consistency prompting, and multi-agent prompting techniques with 13 reasoning and non-reasoning LLMs of varying sizes and providers, including OpenAI, DeepSeek, Mistral AI, and Meta. We evaluate our approach on a case study involving restaurant recommendations. The most substantial improvements occur for small non-reasoning models when applying advanced prompting techniques, particularly multi-agent prompting. However, reasoning models consistently outperform non-reasoning models, with the best performance achieved using single-agent prompting with self-consistency. Notably, DeepSeek-R1 reaches an F1-score of 0.99 at a cost of 0.002 USD per request. Overall, the best balance between effectiveness and cost-time efficiency is reached with the non-reasoning model DeepSeek-V3. Our findings show that LLM-based evaluation offers a scalable and accurate alternative to traditional human evaluation for benchmarking contextual understanding in ConvQA systems.

</details>


### [7] [VOYAGER: A Training Free Approach for Generating Diverse Datasets using LLMs](https://arxiv.org/abs/2512.12072)
*Avinash Amballa,Yashas Malur Saidutta,Chi-Heng Lin,Vivek Kulkarni,Srinivas Chappidi*

Main category: cs.CL

TL;DR: 本文提出了Voyager，一种基于确定性点过程的迭代方法，用于生成多样性更高的合成数据集。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型生成的合成数据缺乏多样性，影响下游模型的训练与评估。

Method: 利用确定性点过程优化数学指标，迭代生成多样化数据，且方法无需训练，适用于闭源模型且易扩展。

Result: 实验表明，Voyager在数据多样性上比现有主流方法提升1.5至3倍。

Conclusion: Voyager有效提升合成数据的多样性，具备理论支持和良好扩展性，优于传统方法。

Abstract: Large language models (LLMs) are increasingly being used to generate synthetic datasets for the evaluation and training of downstream models. However, prior work has noted that such generated data lacks diversity. In this paper, we propose Voyager, a novel principled approach to generate diverse datasets. Our approach is iterative and directly optimizes a mathematical quantity that optimizes the diversity of the dataset using the machinery of determinantal point processes. Furthermore, our approach is training-free, applicable to closed-source models, and scalable. In addition to providing theoretical justification for the working of our method, we also demonstrate through comprehensive experiments that Voyager significantly outperforms popular baseline approaches by providing a 1.5-3x improvement in diversity.

</details>


### [8] [BLASST: Dynamic BLocked Attention Sparsity via Softmax Thresholding](https://arxiv.org/abs/2512.12087)
*Jiayi Yuan,Cameron Shinn,Kai Xu,Jingze Cui,George Klimiashvili,Guangxuan Xiao,Perkz Zheng,Bo Li,Yuxin Zhou,Zhouhai Ye,Weijie You,Tian Zheng,Dominic Brown,Pengbo Wang,Richard Cai,Julien Demouth,John D. Owens,Xia Hu,Song Han,Timmy Liu,Huizi Mao*

Main category: cs.CL

TL;DR: BLASST是一种动态稀疏注意力方法，通过在线阈值剪枝，显著加速大语言模型的长上下文推理，提升计算效率同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型对长上下文推理的需求日益增长，但标准注意力机制在计算和内存方面存在瓶颈。为解决此问题，需开发高效的稀疏注意力方法。

Method: BLASST无需预计算或代理分数，利用固定阈值和在线softmax信息动态剪枝注意力矩阵，减少softmax计算、载入Value块和矩阵乘法，兼容现有FlashAttention核设计。设计自动校准方案，阈值与上下文长度呈简单反比，适用于所有注意力变体。

Result: 在现代GPU上，BLASST在保持高准确度的同时，实现了74.7%稀疏度下prefill阶段1.62倍加速，73.2%稀疏度下decode阶段1.48倍加速。通过稀疏感知训练，进一步提升模型对稀疏模式的鲁棒性。

Conclusion: BLASST提供了一个统一、高效的长上下文稀疏注意力解决方案，大幅缓解计算瓶颈并保持准确性，稀疏感知训练有助于拓展准确率与稀疏度的平衡界限。

Abstract: The growing demand for long-context inference capabilities in Large Language Models (LLMs) has intensified the computational and memory bottlenecks inherent to the standard attention mechanism. To address this challenge, we introduce BLASST, a drop-in sparse attention method that dynamically prunes the attention matrix without any pre-computation or proxy scores. Our method uses a fixed threshold and existing information from online softmax to identify negligible attention scores, skipping softmax computation, Value block loading, and the subsequent matrix multiplication. This fits seamlessly into existing FlashAttention kernel designs with negligible latency overhead. The approach is applicable to both prefill and decode stages across all attention variants (MHA, GQA, MQA, and MLA), providing a unified solution for accelerating long-context inference. We develop an automated calibration procedure that reveals a simple inverse relationship between optimal threshold and context length, enabling robust deployment across diverse scenarios. Maintaining high accuracy, we demonstrate a 1.62x speedup for prefill at 74.7% sparsity and a 1.48x speedup for decode at 73.2% sparsity on modern GPUs. Furthermore, we explore sparsity-aware training as a natural extension, showing that models can be trained to be inherently more robust to sparse attention patterns, pushing the accuracy-sparsity frontier even further.

</details>


### [9] [Extending the Context of Pretrained LLMs by Dropping Their Positional Embeddings](https://arxiv.org/abs/2512.12167)
*Yoav Gelberg,Koshi Eguchi,Takuya Akiba,Edoardo Cetin*

Main category: cs.CL

TL;DR: 本文提出了DroPE方法，通过在预训练后移除语言模型的位置嵌入，实现了无需长上下文微调的上下文扩展。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型扩展上下文长度需昂贵的微调，且现有方法依赖位置嵌入，限制了模型对未见序列长度的泛化能力。

Method: 训练后删除位置嵌入，并进行短暂的重新校准，减少对显式位置信息的依赖，实现零-shot长上下文扩展。

Result: DroPE方法在多种模型和数据集上表现优异，快速适应长上下文，且不损失原训练上下文的能力，显著优于先前的位置嵌入缩放方法和专用架构。

Conclusion: 位置嵌入在预训练过程中重要，但非语言模型建模的必要条件，DroPE有效解决了长上下文泛化难题，提供简洁高效的扩展方案。

Abstract: So far, expensive finetuning beyond the pretraining sequence length has been a requirement for effectively extending the context of language models (LM). In this work, we break this key bottleneck by Dropping the Positional Embeddings of LMs after training (DroPE). Our simple method is motivated by three key theoretical and empirical observations. First, positional embeddings (PEs) serve a crucial role during pretraining, providing an important inductive bias that significantly facilitates convergence. Second, over-reliance on this explicit positional information is also precisely what prevents test-time generalization to sequences of unseen length, even when using popular PE-scaling methods. Third, positional embeddings are not an inherent requirement of effective language modeling and can be safely removed after pretraining, following a short recalibration phase. Empirically, DroPE yields seamless zero-shot context extension without any long-context finetuning, quickly adapting pretrained LMs without compromising their capabilities in the original training context. Our findings hold across different models and dataset sizes, far outperforming previous specialized architectures and established rotary positional embedding scaling methods.

</details>


### [10] [Diffusion Language Model Inference with Monte Carlo Tree Search](https://arxiv.org/abs/2512.12168)
*Zheng Huang,Kiran Ramnath,Yueyan Chen,Aosong Feng,Sangmin Woo,Balasubramaniam Srinivasan,Zhichao Xu,Kang Zhou,Shuai Wang,Haibo Ding,Lin Lee Cheong*

Main category: cs.CL

TL;DR: 本文提出了MEDAL框架，利用蒙特卡洛树搜索优化扩散语言模型（DLMs）的推理过程，显著提升了解码效果。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散语言模型推理方法采用启发式或额外训练指导，存在子最优解问题，缺乏系统化的搜索机制。

Method: MEDAL框架在推理初始化阶段引入蒙特卡洛树搜索，限制搜索空间至高置信度操作，优先选择能提升模型信心的词元，作为后续迭代的良好起点。

Result: 在多个基准测试中，MEDAL较现有推理策略最高提升了22.0%的性能表现。

Conclusion: MEDAL为扩散语言模型推理引入了基于搜索的范式，显著提升了文本生成的质量和一致性。

Abstract: Diffusion language models (DLMs) have recently emerged as a compelling alternative to autoregressive generation, offering parallel generation and improved global coherence. During inference, DLMs generate text by iteratively denoising masked sequences in parallel; however, determining which positions to unmask and which tokens to commit forms a large combinatorial search problem. Existing inference methods approximate this search using heuristics, which often yield suboptimal decoding paths; other approaches instead rely on additional training to guide token selection. To introduce a principled search mechanism for DLMs inference, we introduce MEDAL, a framework that integrates Monte Carlo Tree SEarch initialization for Diffusion LAnguage Model inference. We employ Monte Carlo Tree Search at the initialization stage to explore promising unmasking trajectories, providing a robust starting point for subsequent refinement. This integration is enabled by restricting the search space to high-confidence actions and prioritizing token choices that improve model confidence over remaining masked positions. Across multiple benchmarks, MEDAL achieves up to 22.0% improvement over existing inference strategies, establishing a new paradigm for search-based inference in diffusion language models.

</details>


### [11] [Semantic Distance Measurement based on Multi-Kernel Gaussian Processes](https://arxiv.org/abs/2512.12238)
*Yinzhu Cheng,Haihua Xie,Yaqing Wang,Miao He,Mingming Sun*

Main category: cs.CL

TL;DR: 提出了一种基于多核高斯过程的语义距离测量方法，通过自动学习核参数，提升了语义距离的自适应性，在情感分类任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统的语义距离测量方法固定难以适应特定数据分布和任务需求，需要一种能够自动适应数据的灵活测量方法。

Method: 基于多核高斯过程（MK-GP）将文本的潜在语义函数建模为高斯过程，协方差函数由Matérn核和多项式核组合而成，核参数通过监督学习自动优化。

Result: 在大语言模型的上下文学习框架下，将该语义距离应用于细粒度情感分类，实验结果显示该方法效果显著优于传统方法。

Conclusion: 所提出的基于多核高斯过程的语义距离测量方法具有较强的适应性和有效性，适合用以提升文本相似度相关任务的性能。

Abstract: Semantic distance measurement is a fundamental problem in computational linguistics, providing a quantitative characterization of similarity or relatedness between text segments, and underpinning tasks such as text retrieval and text classification. From a mathematical perspective, a semantic distance can be viewed as a metric defined on a space of texts or on a representation space derived from them. However, most classical semantic distance methods are essentially fixed, making them difficult to adapt to specific data distributions and task requirements. In this paper, a semantic distance measure based on multi-kernel Gaussian processes (MK-GP) was proposed. The latent semantic function associated with texts was modeled as a Gaussian process, with its covariance function given by a combined kernel combining Matérn and polynomial components. The kernel parameters were learned automatically from data under supervision, rather than being hand-crafted. This semantic distance was instantiated and evaluated in the context of fine-grained sentiment classification with large language models under an in-context learning (ICL) setup. The experimental results demonstrated the effectiveness of the proposed measure.

</details>


### [12] [Adversarially Probing Cross-Family Sound Symbolism in 27 Languages](https://arxiv.org/abs/2512.12245)
*Anika Sharma,Tianyi Niu,Emma Wrenn,Shashank Srivastava*

Main category: cs.CL

TL;DR: 本文首次进行跨语言规模化分析，证实了语音形式与尺寸语义之间存在普遍的声音象征关系。


<details>
  <summary>Details</summary>
Motivation: 声音象征虽然通过一些小规模实验被证明，但缺乏大规模、跨语言的系统研究。

Method: 建立了包含27种语言、810个形容词的数据库，采用音素转录和母语者验证音频，用可解释的分类器基于音段特征预测尺寸语义，设计对抗模型屏蔽语言身份以验证普遍性。

Result: 分类器能显著预测尺寸语义，语言身份被抑制后尺寸预测仍显著高于随机，证实跨语言族群存在声音象征偏好。

Conclusion: 声音象征在尺寸语义领域表现出跨语言普遍性，研究数据与工具公开促进后续标志性研究发展。

Abstract: The phenomenon of sound symbolism, the non-arbitrary mapping between word sounds and meanings, has long been demonstrated through anecdotal experiments like Bouba Kiki, but rarely tested at scale. We present the first computational cross-linguistic analysis of sound symbolism in the semantic domain of size. We compile a typologically broad dataset of 810 adjectives (27 languages, 30 words each), each phonemically transcribed and validated with native-speaker audio. Using interpretable classifiers over bag-of-segment features, we find that phonological form predicts size semantics above chance even across unrelated languages, with both vowels and consonants contributing. To probe universality beyond genealogy, we train an adversarial scrubber that suppresses language identity while preserving size signal (also at family granularity). Language prediction averaged across languages and settings falls below chance while size prediction remains significantly above chance, indicating cross-family sound-symbolic bias. We release data, code, and diagnostic tools for future large-scale studies of iconicity.

</details>


### [13] [Market-Bench: Evaluating Large Language Models on Introductory Quantitative Trading and Market Dynamics](https://arxiv.org/abs/2512.12264)
*Abhay Srivastava,Sam Jung,Spencer Mateega*

Main category: cs.CL

TL;DR: MARKET-BENCH基准评测大型语言模型在定量交易任务中的表现，要求模型从自然语言描述生成可执行的回测代码，并评估其收益和风险指标的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在金融交易策略自动化生成中的能力尚未系统评估，市场需要一个标准化的基准来衡量模型在基础交易任务中的表现。

Method: 构建包含三种经典交易策略（微软定时交易、可口可乐与百事可乐对冲交易、微软Delta对冲）的基准套件，要求模型生成的回测代码能复现参考实现的盈亏、回撤和仓位路径。通过多轮pass@k指标分别评估结构可靠性与数值准确度。

Result: 测试了12个先进模型，发现它们在最简单策略上的执行可靠性较高，但不同模型和任务间的误差差异较大。Gemini 3 Pro和Claude 4.5 Sonnet表现出良好的可靠性和低误差，GPT-5.1 Codex-Max在前两种策略中表现优异，Qwen3 Max虽能完全通过多个测试，但盈亏路径有时不准确。

Conclusion: 现有大型语言模型可搭建基本交易基础设施，但在价格、库存和风险的复杂推理上仍存在不足。作者公开了MARKET-BENCH数据集和排行榜，促进该领域进一步研究。

Abstract: We introduce MARKET-BENCH, a benchmark that evaluates large language models (LLMs) on introductory quantitative trading tasks by asking them to construct executable backtesters from natural-language strategy descriptions and market assumptions. Each instance specifies one of three canonical strategies -- scheduled trading on Microsoft (NASDAQ: MSFT), pairs trading on Coca-Cola (NASDAQ: KO) and Pepsi (NASDAQ: PEP), or delta hedging on MSFT -- and models must produce code whose P\&L, drawdown, and position paths match a verifiable reference implementation. We assess twelve state-of-the-art models using a multi-round pass@k metric that separates structural reliability (whether the backtest runs) from numerical accuracy (mean absolute error of the backtest metrics). While most models reliably execute the simplest strategy (average pass@3 of 0.80), errors vary by orders of magnitude across models and tasks: Gemini 3 Pro and Claude 4.5 Sonnet combine strong reliability with low error on simpler strategies, GPT-5.1 Codex-Max achieves perfect pass@1 on the first two strategies and the lowest best-run error on the easiest task, and Qwen3 Max attains perfect pass@3 yet sometimes produces inaccurate P\&L paths. These results show that current LLMs can scaffold basic trading infrastructure but still struggle to reason robustly about prices, inventory, and risk; we release MARKET-BENCH and a public leaderboard at https://marketbench.ai.

</details>


### [14] [F5-TTS-RO: Extending F5-TTS to Romanian TTS via Lightweight Input Adaptation](https://arxiv.org/abs/2512.12297)
*Radu-Gabriel Chivereanu,Tiberiu Boros*

Main category: cs.CL

TL;DR: 本文提出了一种轻量级的输入层适配器，用于F5-TTS模型以支持罗马尼亚语，实现了在保持原模型能力的同时增加语言支持。


<details>
  <summary>Details</summary>
Motivation: 为了扩展F5-TTS模型支持罗马尼亚语，同时不影响其已有的语音克隆及英中语言支持能力。

Method: 冻结原模型权重，附加子网络，训练文本编码器的文本嵌入矩阵扩展部分，利用ConvNeXt模块模拟字符级嵌入的相关性，将文本转换为连续表示。

Result: 模型在听觉相似度、发音自然度和罗英混合语音切换上表现良好，保持了语音克隆能力，但仍带有残留的英语口音。

Conclusion: 该方法成功实现了F5-TTS对罗马尼亚语的支持，保持原有功能且实现一定程度的代码切换，代码及示例已开源。

Abstract: This work introduces a lightweight input-level adapter for the F5-TTS model that enables Romanian Language support. To preserve the existing capabilities of the model (voice cloning, English and Chinese support), we keep the original weights frozen, append a sub-network to the model and train it as an extension for the textual embedding matrix of the text encoder. For simplicity, we rely on ConvNeXt module implemented in F5-TTS to also model the co-dependencies between the new character-level embeddings. The module serves as a ``soft`` letter-to-sound layer, converting Romanian text into a continuous representation that the F5-TTS model uses to produce naturally sounding Romanian utterances. We evaluate the model with a pool of 20 human listeners across three tasks: (a) audio similarity between reference and generated speech, (b) pronunciation and naturalness and (c) Romanian-English code-switching. The results indicate that our approach maintains voice cloning capabilities and enables, to a certain extent, code-switching within the same utterance; however, residual English accent characteristics remain. We open-source our code and provide example audio samples at https://github.com/racai-ro/Ro-F5TTS.

</details>


### [15] [SCIR: A Self-Correcting Iterative Refinement Framework for Enhanced Information Extraction Based on Schema](https://arxiv.org/abs/2512.12337)
*Yushen Fang,Jianjun Li,Mingqian Ding,Chang Liu,Xinchi Zou,Wenqi Yang*

Main category: cs.CL

TL;DR: 提出了一种新颖的适用于大语言模型的信息提取框架SCIR及多任务中英自校正数据集MBSC，在显著降低训练成本的同时提升了信息提取的准确率。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的信息提取方法面临训练成本高和难以与模型偏好对齐的挑战。

Method: 提出了SCIR框架，利用双路径自校正模块及反馈驱动优化，实现与现有模型的即插即用，并设计MBSC数据集通过间接蒸馏GPT-4能力解决偏好对齐问题。

Result: SCIR在命名实体识别、关系抽取和事件抽取三任务上，微平均F1分数提升了5.27%，训练成本降低了87%。

Conclusion: 该方法提高了信息提取系统的准确性与灵活性，为轻量高效的信息提取方法提供了新思路。

Abstract: Although Large language Model (LLM)-powered information extraction (IE) systems have shown impressive capabilities, current fine-tuning paradigms face two major limitations: high training costs and difficulties in aligning with LLM preferences. To address these issues, we propose a novel universal IE paradigm, the Self-Correcting Iterative Refinement (SCIR) framework, along with a Multi-task Bilingual (Chinese-English) Self-Correcting (MBSC) dataset containing over 100,000 entries. The SCIR framework achieves plug-and-play compatibility with existing LLMs and IE systems through its Dual-Path Self-Correcting module and feedback-driven optimization, thereby significantly reducing training costs. Concurrently, the MBSC dataset tackles the challenge of preference alignment by indirectly distilling GPT-4's capabilities into IE result detection models. Experimental results demonstrate that SCIR outperforms state-of-the-art IE methods across three key tasks: named entity recognition, relation extraction, and event extraction, achieving a 5.27 percent average improvement in span-based Micro-F1 while reducing training costs by 87 percent compared to baseline approaches. These advancements not only enhance the flexibility and accuracy of IE systems but also pave the way for lightweight and efficient IE paradigms.

</details>


### [16] [Can GPT replace human raters? Validity and reliability of machine-generated norms for metaphors](https://arxiv.org/abs/2512.12444)
*Veronica Mangiaterra,Hamad Al-Azary,Chiara Barattieri di San Pietro,Paolo Canal,Valentina Bambini*

Main category: cs.CL

TL;DR: 本研究首次评估了GPT模型在形象性、易懂性和熟悉度三个维度上对隐喻进行评分的有效性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型(LLMs)在科学研究中的广泛应用，其可信度问题日益重要，尤其是对复杂语言项目如隐喻的自动评分尚未被充分研究。

Method: 基于687个意大利和英语隐喻，利用三种GPT模型生成指标评分，并通过与人类评分的对齐以及行为和脑电反应的预测能力进行验证。

Result: 机器评分与人类评分正相关，尤其是在熟悉度（中到强）、形象性（中到强）和易懂性（最强相关）方面，且大模型表现优于小模型。机器评分还能显著预测响应时间和脑电振幅，且评分稳定性高。

Conclusion: 较大的GPT模型能够有效且稳定地替代或增强人类对隐喻属性的评分，但在传统性和多模态意义处理中表现不佳，需谨慎考虑刺激性质。

Abstract: As Large Language Models (LLMs) are increasingly being used in scientific research, the issue of their trustworthiness becomes crucial. In psycholinguistics, LLMs have been recently employed in automatically augmenting human-rated datasets, with promising results obtained by generating ratings for single words. Yet, performance for ratings of complex items, i.e., metaphors, is still unexplored. Here, we present the first assessment of the validity and reliability of ratings of metaphors on familiarity, comprehensibility, and imageability, generated by three GPT models for a total of 687 items gathered from the Italian Figurative Archive and three English studies. We performed a thorough validation in terms of both alignment with human data and ability to predict behavioral and electrophysiological responses. We found that machine-generated ratings positively correlated with human-generated ones. Familiarity ratings reached moderate-to-strong correlations for both English and Italian metaphors, although correlations weakened for metaphors with high sensorimotor load. Imageability showed moderate correlations in English and moderate-to-strong in Italian. Comprehensibility for English metaphors exhibited the strongest correlations. Overall, larger models outperformed smaller ones and greater human-model misalignment emerged with familiarity and imageability. Machine-generated ratings significantly predicted response times and the EEG amplitude, with a strength comparable to human ratings. Moreover, GPT ratings obtained across independent sessions were highly stable. We conclude that GPT, especially larger models, can validly and reliably replace - or augment - human subjects in rating metaphor properties. Yet, LLMs align worse with humans when dealing with conventionality and multimodal aspects of metaphorical meaning, calling for careful consideration of the nature of stimuli.

</details>


### [17] [Large language models have learned to use language](https://arxiv.org/abs/2512.12447)
*Gary Lupyan*

Main category: cs.CL

TL;DR: 本文指出大语言模型已学会使用语言，呼吁突破传统语言知识评估方法，适应后图灵测试时代。


<details>
  <summary>Details</summary>
Motivation: 作者认为传统语言知识的评估方式已不能满足大语言模型的发展，亟需新的理念来推动语言科学的突破。

Method: 本文通过分析大语言模型的语言使用能力，提出应摒弃部分旧有观念，重新思考语言知识的评估标准。

Result: 指出我们已进入后图灵测试时代，传统评测方法面临挑战，需要新的评估框架。

Conclusion: 承认大语言模型的语言使用能力，将有助于推动语言科学的发展，要求接受新的评估理念。

Abstract: Acknowledging that large language models have learned to use language can open doors to breakthrough language science. Achieving these breakthroughs may require abandoning some long-held ideas about how language knowledge is evaluated and reckoning with the difficult fact that we have entered a post-Turing test era.

</details>


### [18] [The American Ghost in the Machine: How language models align culturally and the effects of cultural prompting](https://arxiv.org/abs/2512.12488)
*James Luther,Donald Brown*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）的文化适应性，发现大部分模型默认偏向美国文化，通过文化提示可以调整模型文化倾向，但对日本和中国文化适应较差。


<details>
  <summary>Details</summary>
Motivation: 随着生成式大型语言模型在人工智能领域的发展，模型的文化对齐变得尤为重要，尤其在人机交互中，文化差异会显著影响交互效果。

Method: 利用VSM13国际调查和霍夫斯泰德文化维度框架，评估八个主流LLMs的文化倾向；通过文化提示（system prompt）将模型文化对齐到中国、法国、印度、伊朗、日本和美国，测试模型的文化适应能力。

Result: 除日本和中国外，七个模型通过文化提示能显著调整其文化倾向；大部分模型默认偏向美国文化，即使有两款模型由中国公司DeepSeek开发，但对中国文化适应性较弱。

Conclusion: 尽管主流LLMs具有一定的文化适应能力，但在针对特定文化（尤其是日本和中国）时仍存在挑战，提示未来模型在文化多样性和适应性方面需进一步改进。

Abstract: Culture is the bedrock of human interaction; it dictates how we perceive and respond to everyday interactions. As the field of human-computer interaction grows via the rise of generative Large Language Models (LLMs), the cultural alignment of these models become an important field of study. This work, using the VSM13 International Survey and Hofstede's cultural dimensions, identifies the cultural alignment of popular LLMs (DeepSeek-V3, V3.1, GPT-5, GPT-4.1, GPT-4, Claude Opus 4, Llama 3.1, and Mistral Large). We then use cultural prompting, or using system prompts to shift the cultural alignment of a model to a desired country, to test the adaptability of these models to other cultures, namely China, France, India, Iran, Japan, and the United States. We find that the majority of the eight LLMs tested favor the United States when the culture is not specified, with varying results when prompted for other cultures. When using cultural prompting, seven of the eight models shifted closer to the expected culture. We find that models had trouble aligning with Japan and China, despite two of the models tested originating with the Chinese company DeepSeek.

</details>


### [19] [NagaNLP: Bootstrapping NLP for Low-Resource Nagamese Creole with Human-in-the-Loop Synthetic Data](https://arxiv.org/abs/2512.12537)
*Agniva Maiti,Manya Pandey,Murari Mandal*

Main category: cs.CL

TL;DR: 本文介绍了NagaNLP，一个针对Nagamese克里奥尔语的开源自然语言处理工具包，通过大型语言模型生成并由本地人校验的合成-混合数据构建语料库。训练的模型在词性标注和命名实体识别任务中表现优异，显著超越零样本基线。


<details>
  <summary>Details</summary>
Motivation: Nagamese等低资源语言在自然语言处理领域资源匮乏，限制了其在数字技术中的应用和表现。

Method: 提出了一个多阶段流程，利用专家引导的大型语言模型（Gemini）生成候选语料，再由母语者进行精细化和标注，形成高质量合成-混合数据集。同时，训练区分式和生成式模型以评估方法效果。

Result: 构建了包含1万对话语料和高质量标注语料的资源，XLM-RoBERTa-base模型在词性标注任务中达93.81%准确率和0.90 F1-Macro，NER任务F1-Macro为0.75，显著优于零样本基线。NagaLLaMA对话模型在困惑度指标上达到3.85，优于few-shot模型近十倍。

Conclusion: NagaNLP工具包全面释放了Nagamese语言的NLP潜能，提供了丰富的数据和模型资源，有效缓解低资源语言数据稀缺问题，并为其他类似语言的资源开发建立了可复现的框架。

Abstract: The vast majority of the world's languages, particularly creoles like Nagamese, remain severely under-resourced in Natural Language Processing (NLP), creating a significant barrier to their representation in digital technology. This paper introduces NagaNLP, a comprehensive open-source toolkit for Nagamese, bootstrapped through a novel methodology that relies on LLM-driven but human-validated synthetic data generation. We detail a multi-stage pipeline where an expert-guided LLM (Gemini) generates a candidate corpus, which is then refined and annotated by native speakers. This synthetic-hybrid approach yielded a 10K pair conversational dataset and a high-quality annotated corpus for foundational tasks. To assess the effectiveness of our methodology, we trained both discriminative and generative models. Our fine-tuned XLM-RoBERTa-base model establishes a new benchmark for Nagamese, achieving a 93.81\% accuracy (0.90 F1-Macro) on Part-of-Speech tagging and a 0.75 F1-Macro on Named Entity Recognition, massively outperforming strong zero-shot baselines. Furthermore, we fine-tuned a Llama-3.2-3B Instruct model, named NagaLLaMA, which demonstrates superior performance on conversational tasks, achieving a Perplexity of 3.85, an order of magnitude improvement over its few-shot counterpart (96.76). We release the NagaNLP toolkit, including all datasets, models, and code, providing a foundational resource for a previously underserved language and a reproducible framework for reducing data scarcity in other low-resource contexts.

</details>


### [20] [HyperEdit: Unlocking Instruction-based Text Editing in LLMs via Hypernetworks](https://arxiv.org/abs/2512.12544)
*Yiming Zeng,Jinghan Cao,Zexin Li,Wanhao Yu,Zhankai Ye,Dawei Xiang,Ting Hua,Xin Liu,Shangqian Gao,Tingting Yu*

Main category: cs.CL

TL;DR: 本文提出了HyperEdit，一种基于超网络动态适应的文本编辑方法，针对指令驱动文本编辑中对用户意图对齐和过度编辑的问题进行优化，显著提升编辑质量。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型对指令驱动的文本编辑任务表现不佳，主要因为难以忠实执行多样化的用户编辑指令，并且容易对未修改区域进行过度编辑。

Method: 提出HyperEdit方法，包括基于超网络的动态适应机制生成针对特定指令的模型参数，以及差异感知正则化，重点监督修改区域，防止过度编辑。

Result: HyperEdit在修改区域的BLEU得分上较最新方法提升了9%至30%，且仅使用3B参数，效果显著。

Conclusion: HyperEdit有效解决了指令驱动文本编辑中对用户意图对齐和编辑精准性的挑战，实现了更忠实且精确的编辑结果。

Abstract: Instruction-based text editing is increasingly critical for real-world applications such as code editors (e.g., Cursor), but Large Language Models (LLMs) continue to struggle with this task. Unlike free-form generation, editing requires faithfully implementing user instructions while preserving unchanged content, as even minor unintended modifications can break functionality. Existing approaches treat editing as generic text generation, leading to two key failures: they struggle to faithfully align edits with diverse user intents, and they often over-edit unchanged regions. We propose HyperEdit to address both issues. First, we introduce hypernetwork-based dynamic adaptation that generates request-specific parameters, enabling the model to tailor its editing strategy to each instruction. Second, we develop difference-aware regularization that focuses supervision on modified spans, preventing over-editing while ensuring precise, minimal changes. HyperEdit achieves a 9%--30% relative improvement in BLEU on modified regions over state-of-the-art baselines, despite utilizing only 3B parameters.

</details>


### [21] [Coupled Variational Reinforcement Learning for Language Model General Reasoning](https://arxiv.org/abs/2512.12576)
*Xueru Wen,Jie Lou,Yanjiang Liu,Hongyu Lin,Ben He,Xianpei Han,Le Sun,Yaojie Lu,Debing Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的耦合变分强化学习方法CoVRL，以提升语言模型推理的性能和答案一致性。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法受限于需要可验证的奖励信号，且无验证者的强化学习往往忽视了推理轨迹与答案之间的耦合，导致探索低效和轨迹与答案不一致。

Method: 提出CoVRL，通过耦合先验和后验分布，采用混合采样策略构建并优化复合分布，实现变分推断与强化学习的结合，从而提升探索效率及推理与答案的一致性。

Result: 在数学和通用推理基准测试中，CoVRL相较基础模型性能提升12.4%，且较现有无验证者强化学习方法提升2.3%。

Conclusion: CoVRL为提升语言模型的通用推理能力提供了一个有理论支撑且有效的框架，兼顾了推理效率和答案一致性。

Abstract: While reinforcement learning have achieved impressive progress in language model reasoning, they are constrained by the requirement for verifiable rewards. Recent verifier-free RL methods address this limitation by utilizing the intrinsic probabilities of LLMs generating reference answers as reward signals. However, these approaches typically sample reasoning traces conditioned only on the question. This design decouples reasoning-trace sampling from answer information, leading to inefficient exploration and incoherence between traces and final answers. In this paper, we propose \textit{\b{Co}upled \b{V}ariational \b{R}einforcement \b{L}earning} (CoVRL), which bridges variational inference and reinforcement learning by coupling prior and posterior distributions through a hybrid sampling strategy. By constructing and optimizing a composite distribution that integrates these two distributions, CoVRL enables efficient exploration while preserving strong thought-answer coherence. Extensive experiments on mathematical and general reasoning benchmarks show that CoVRL improves performance by 12.4\% over the base model and achieves an additional 2.3\% improvement over strong state-of-the-art verifier-free RL baselines, providing a principled framework for enhancing the general reasoning capabilities of language models.

</details>


### [22] [Human-Inspired Learning for Large Language Models via Obvious Record and Maximum-Entropy Method Discovery](https://arxiv.org/abs/2512.12608)
*Hong Su*

Main category: cs.CL

TL;DR: 该论文针对大语言模型在处理罕见或低资源场景时存在的不足，提出了一种结合符号记忆与最大熵方法发现的人类启发式学习框架。


<details>
  <summary>Details</summary>
Motivation: 大语言模型因训练数据中稀少的低频案例难以有效学习，且依赖隐式参数记忆，缺乏显式获取、回忆和优化方法的能力。

Method: 设计了两个机制：一是“显著记录”，将因果关系作为符号记忆存储，实现对单次或少见经验的持久学习；二是“最大熵方法发现”，优先采纳语义差异性高的方法，从而捕捉多样且稀缺的策略。

Result: 在包含60组语义多样的问题-解决对的基准测试中，该方法相比随机基线，在覆盖未见问题和内部多样性方面表现更佳，证明了其发现更具泛化和人类启发性方法的有效性。

Conclusion: 该框架有效提升了大语言模型在罕见场景下的学习能力，促进其从低频数据中显式获取和创新方法，增强模型的泛化性和多样性。

Abstract: Large Language Models (LLMs) excel at extracting common patterns from large-scale corpora, yet they struggle with rare, low-resource, or previously unseen scenarios-such as niche hardware deployment issues or irregular IoT device behaviors-because such cases are sparsely represented in training data. Moreover, LLMs rely primarily on implicit parametric memory, which limits their ability to explicitly acquire, recall, and refine methods, causing them to behave predominantly as intuition-driven predictors rather than deliberate, method-oriented learners. Inspired by how humans learn from rare experiences, this paper proposes a human-inspired learning framework that integrates two complementary mechanisms. The first, Obvious Record, explicitly stores cause--result (or question--solution) relationships as symbolic memory, enabling persistent learning even from single or infrequent encounters. The second, Maximum-Entropy Method Discovery, prioritizes and preserves methods with high semantic dissimilarity, allowing the system to capture diverse and underrepresented strategies that are typically overlooked by next-token prediction. Verification on a benchmark of 60 semantically diverse question--solution pairs demonstrates that the proposed entropy-guided approach achieves stronger coverage of unseen questions and significantly greater internal diversity than a random baseline, confirming its effectiveness in discovering more generalizable and human-inspired methods.

</details>


### [23] [StruProKGR: A Structural and Probabilistic Framework for Sparse Knowledge Graph Reasoning](https://arxiv.org/abs/2512.12613)
*Yucan Guo,Saiping Guan,Miao Su,Zeya Zhao,Xiaolong Jin,Jiafeng Guo,Xueqi Cheng*

Main category: cs.CL

TL;DR: 提出了StruProKGR，一种高效且可解释的稀疏知识图推理框架，通过距离指导路径收集和概率路径聚合提升推理质量和效率。


<details>
  <summary>Details</summary>
Motivation: 稀疏知识图推理难点在于知识稀缺和难以捕获关系模式，且现有路径方法计算量大且路径质量不稳定，且未充分利用图的结构信息。

Method: 设计了StruProKGR框架，采用距离引导的路径收集机制减少计算成本，同时利用概率路径聚合融合结构信息，优先考虑相互强化的路径。

Result: 在五个稀疏知识图推理基准上，StruProKGR在效果和效率上均优于现有路径方法。

Conclusion: StruProKGR为稀疏知识图推理提供了一种有效、高效且具有良好解释性的解决方案。

Abstract: Sparse Knowledge Graphs (KGs) are commonly encountered in real-world applications, where knowledge is often incomplete or limited. Sparse KG reasoning, the task of inferring missing knowledge over sparse KGs, is inherently challenging due to the scarcity of knowledge and the difficulty of capturing relational patterns in sparse scenarios. Among all sparse KG reasoning methods, path-based ones have attracted plenty of attention due to their interpretability. Existing path-based methods typically rely on computationally intensive random walks to collect paths, producing paths of variable quality. Additionally, these methods fail to leverage the structured nature of graphs by treating paths independently. To address these shortcomings, we propose a Structural and Probabilistic framework named StruProKGR, tailored for efficient and interpretable reasoning on sparse KGs. StruProKGR utilizes a distance-guided path collection mechanism to significantly reduce computational costs while exploring more relevant paths. It further enhances the reasoning process by incorporating structural information through probabilistic path aggregation, which prioritizes paths that reinforce each other. Extensive experiments on five sparse KG reasoning benchmarks reveal that StruProKGR surpasses existing path-based methods in both effectiveness and efficiency, providing an effective, efficient, and interpretable solution for sparse KG reasoning.

</details>


### [24] [Understanding Syllogistic Reasoning in LLMs from Formal and Natural Language Perspectives](https://arxiv.org/abs/2512.12620)
*Aheli Poddar,Saptarshi Sahoo,Sujata Ghosh*

Main category: cs.CL

TL;DR: 本文通过14个大型语言模型研究了它们在三段论推理中的表现，特别是在符号推理和自然语言理解方面。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在逻辑推理，尤其是三段论推理中的能力及其发展方向。

Method: 采用14个大型语言模型，对其在符号推理和自然语言理解层面的三段论推理能力进行系统测试和比较。

Result: 不同模型的推理能力表现不一，部分模型在符号推理任务中表现出近乎完美的能力。

Conclusion: 某些大型语言模型可能正在向更形式化的推理机制发展，而非仅仅模拟人类思维的细微差别。

Abstract: We study syllogistic reasoning in LLMs from the logical and natural language perspectives. In process, we explore fundamental reasoning capabilities of the LLMs and the direction this research is moving forward. To aid in our studies, we use 14 large language models and investigate their syllogistic reasoning capabilities in terms of symbolic inferences as well as natural language understanding. Even though this reasoning mechanism is not a uniform emergent property across LLMs, the perfect symbolic performances in certain models make us wonder whether LLMs are becoming more and more formal reasoning mechanisms, rather than making explicit the nuances of human reasoning.

</details>


### [25] [Which Pieces Does Unigram Tokenization Really Need?](https://arxiv.org/abs/2512.12641)
*Sander Land,Yuval Pinter*

Main category: cs.CL

TL;DR: 本文针对Unigram分词算法的实际应用复杂性，提供了明确的实现指南和参数选择，并提出了一种更简单的算法以改善压缩效果。


<details>
  <summary>Details</summary>
Motivation: Unigram分词算法理论优雅，但实际实现复杂，限制了其应用范围。

Method: 本文通过清晰的实现步骤和参数指南，弥合理论与实践的差距，同时提出了一种更简单的算法，接受略高的训练损失换取更好的压缩效果。

Result: 提出的简单算法在训练损失略微增加的情况下，实现了更优的压缩效果。

Conclusion: 本研究提供了可行的Unigram分词算法实现方案和参数选择指南，促进了该算法的广泛应用，并改进了压缩性能。

Abstract: The Unigram tokenization algorithm offers a probabilistic alternative to the greedy heuristics of Byte-Pair Encoding. Despite its theoretical elegance, its implementation in practice is complex, limiting its adoption to the SentencePiece package and adapters thereof. We bridge this gap between theory and practice by providing a clear guide to implementation and parameter choices. We also identify a simpler algorithm that accepts slightly higher training loss in exchange for improved compression.

</details>


### [26] [LexRel: Benchmarking Legal Relation Extraction for Chinese Civil Cases](https://arxiv.org/abs/2512.12643)
*Yida Cai,Ranjuexiao Hu,Huiyuan Xie,Chenyang Li,Yun Liu,Yuxiao Ye,Zhenghao Liu,Weixing Shen,Zhiyuan Liu*

Main category: cs.CL

TL;DR: 该论文提出了针对中国民事案件法律关系的综合模式及基准数据集LexRel，评估了大型语言模型在法律关系抽取任务中的表现，并验证了法律关系信息对其他法律AI任务的提升效果。


<details>
  <summary>Details</summary>
Motivation: 现有法律人工智能领域对中国民事案件中的法律关系研究较少，缺乏全面的法律关系抽取模式，制约了在司法实践中有效解决纠纷和实现法治价值。

Method: 提出了包含分层分类法和论点定义的综合法律关系模式，基于该模式构建了专家标注的LexRel数据集，设计法律关系抽取任务，并用它评估当前大型语言模型的表现。

Result: 实验结果显示当前大型语言模型在准确识别民事法律关系方面存在显著不足。此外，融合法律关系信息能明显提升其他法律人工智能任务的性能。

Conclusion: 综合的法律关系模式及LexRel数据集为中国民事法律关系的研究提供了基础，揭示了大型语言模型的局限性，并证明了法律关系信息在法律AI中的重要性和应用价值。

Abstract: Legal relations form a highly consequential analytical framework of civil law system, serving as a crucial foundation for resolving disputes and realizing values of the rule of law in judicial practice. However, legal relations in Chinese civil cases remain underexplored in the field of legal artificial intelligence (legal AI), largely due to the absence of comprehensive schemas. In this work, we firstly introduce a comprehensive schema, which contains a hierarchical taxonomy and definitions of arguments, for AI systems to capture legal relations in Chinese civil cases. Based on this schema, we then formulate legal relation extraction task and present LexRel, an expert-annotated benchmark for legal relation extraction in Chinese civil law. We use LexRel to evaluate state-of-the-art large language models (LLMs) on legal relation extractions, showing that current LLMs exhibit significant limitations in accurately identifying civil legal relations. Furthermore, we demonstrate that incorporating legal relations information leads to consistent performance gains on other downstream legal AI tasks.

</details>


### [27] [Modeling Authorial Style in Urdu Novels Using Character Interaction Graphs and Graph Neural Networks](https://arxiv.org/abs/2512.12654)
*Hassan Mujtaba,Hamza Naveed,Hanzlah Munir*

Main category: cs.CL

TL;DR: 该论文提出了基于图的框架，通过分析乌尔都语小说中的人物互动网络，利用叙事结构进行作者风格识别，取得了高达0.857的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 传统著作权分析主要依赖于词汇和文体线索，而高层叙事结构特别是在低资源语言（如乌尔都语）中尚未被充分探索，故本文旨在研究叙事结构能否独立推断作者风格。

Method: 将乌尔都语小说构建为人物交互图，节点为人物，边表示人物在叙事中的近邻共现，比较多种图表示方法（全局结构特征、节点语义总结、无监督图嵌入、监督图神经网络），在多作者乌尔都语小说数据集上进行实验。

Result: 基于学习的图表示显著优于手工设计的特征及无监督基线方法，在严格的作者识别评测协议下，准确率最高达到0.857。

Conclusion: 叙事结构中人物互动的图表示能够有效辅助作者风格识别，尤其对低资源语言的文本分析具有重要价值。

Abstract: Authorship analysis has traditionally focused on lexical and stylistic cues within text, while higher-level narrative structure remains underexplored, particularly for low-resource languages such as Urdu. This work proposes a graph-based framework that models Urdu novels as character interaction networks to examine whether authorial style can be inferred from narrative structure alone. Each novel is represented as a graph where nodes correspond to characters and edges denote their co-occurrence within narrative proximity. We systematically compare multiple graph representations, including global structural features, node-level semantic summaries, unsupervised graph embeddings, and supervised graph neural networks. Experiments on a dataset of 52 Urdu novels written by seven authors show that learned graph representations substantially outperform hand-crafted and unsupervised baselines, achieving up to 0.857 accuracy under a strict author-aware evaluation protocol.

</details>


### [28] [Fine-Tuning Causal LLMs for Text Classification: Embedding-Based vs. Instruction-Based Approaches](https://arxiv.org/abs/2512.12677)
*Amirhossein Yousefiramandi,Ciaran Cooney*

Main category: cs.CL

TL;DR: 本文探索了两种在资源受限条件下微调解码器大型语言模型进行文本分类的高效策略。


<details>
  <summary>Details</summary>
Motivation: 在有限计算资源环境下，提升大型语言模型对下游文本分类任务的微调效率和效果。

Method: 比较了在预训练因果语言模型上附加分类头微调与指令微调两种方法，同时结合4位量化和低秩适配技术实现单GPU下8B参数模型的高效训练。

Result: 在两个数据集上，基于嵌入的微调方法显著优于指令微调，且性能不输甚至超越了领域特定的微调模型如BERT。

Conclusion: 利用因果LLM内部表示结合高效微调技术，在计算资源有限的情况下，也能获得优异的文本分类性能，为LLM在实际分类任务中的微调提供了有效策略和指导。

Abstract: We explore efficient strategies to fine-tune decoder-only Large Language Models (LLMs) for downstream text classification under resource constraints. Two approaches are investigated: (1) attaching a classification head to a pre-trained causal LLM and fine-tuning on the task (using the LLM's final token embedding as a sequence representation), and (2) instruction-tuning the LLM in a prompt->response format for classification. To enable single-GPU fine-tuning of models up to 8B parameters, we combine 4-bit model quantization with Low-Rank Adaptation (LoRA) for parameter-efficient training. Experiments on two datasets - a proprietary single-label dataset and the public WIPO-Alpha patent dataset (extreme multi-label classification) - show that the embedding-based method significantly outperforms the instruction-tuned method in F1-score, and is very competitive with - even surpassing - fine-tuned domain-specific models (e.g. BERT) on the same tasks. These results demonstrate that directly leveraging the internal representations of causal LLMs, along with efficient fine-tuning techniques, yields impressive classification performance under limited computational resources. We discuss the advantages of each approach while outlining practical guidelines and future directions for optimizing LLM fine-tuning in classification scenarios.

</details>


### [29] [Comparative Analysis of LLM Abliteration Methods: A Cross-Architecture Evaluation](https://arxiv.org/abs/2512.13655)
*Richard J. Young*

Main category: cs.CL

TL;DR: 本研究评估了四种消融工具对16个大语言模型的影响，发现单遍方法在保持能力方面表现更优，数学推理能力对消融最敏感。


<details>
  <summary>Details</summary>
Motivation: 安全对齐机制虽然防止有害响应，但阻碍了认知建模等合法研究，需评估现有消融技术的有效性。

Method: 比较四种消融工具在16个不同规模的大语言模型上的兼容性和性能影响，通过定量指标评估能力保留和分布变化。

Result: 单遍消融方法在部分模型上保持了更好的性能，贝叶斯优化消融导致分布漂移且效果依模型而异，数学推理能力对消融影响最大。

Conclusion: 本研究为研究人员提供了基于证据的消融工具选择依据，指出消融对模型数学推理能力的敏感性是主要发现。

Abstract: Safety alignment mechanisms in large language models prevent responses to harmful queries through learned refusal behavior, yet these same mechanisms impede legitimate research applications including cognitive modeling, adversarial testing, and security analysis. While abliteration techniques enable surgical removal of refusal representations through directional orthogonalization, the relative effectiveness of available implementations remains uncharacterized. This study evaluates four abliteration tools (Heretic, DECCP, ErisForge, FailSpy) across sixteen instruction-tuned models (7B-14B parameters), reporting tool compatibility on all 16 models and quantitative metrics on subsets dictated by tool support. Single-pass methods demonstrated superior capability preservation on the benchmarked subset (avg GSM8K change across three models: ErisForge -0.28 pp; DECCP -0.13 pp), while Bayesian-optimized abliteration produced variable distribution shift (KL divergence: 0.043-1.646) with model-dependent capability impact. These findings provide researchers with evidence-based selection criteria for abliteration tool deployment across diverse model architectures. The principal finding indicates that mathematical reasoning capabilities exhibit the highest sensitivity to abliteration interventions, with GSM8K change ranging from +1.51 pp to -18.81 pp (-26.5% relative) depending on tool selection and model architecture.

</details>


### [30] [CoDA: A Context-Decoupled Hierarchical Agent with Reinforcement Learning](https://arxiv.org/abs/2512.12716)
*Xuanzhang Liu,Jianglun Feng,Zhuoran Zhuang,Junzhe Zhao,Maofei Que,Jieting Li,Dianlei Wang,Hao Tong,Ye Chen,Pan Li*

Main category: cs.CL

TL;DR: 本文提出了CoDA，一种通过分层结构将高层规划与低层执行解耦的强化学习框架，显著提升了大语言模型在复杂多步骤任务中的表现，尤其在上下文爆炸问题下展示了强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理复杂任务时因“上下文爆炸”导致性能下降，急需一种方法来缓解长文本累积对模型上下文窗口的影响。

Method: CoDA采用一个共享的大语言模型，分别作为高层规划者和低层执行者工作，利用PECO方法进行强化学习训练，通过策略更新实现两者在上下文隔离环境下的协同优化。

Result: CoDA在多个复杂多跳问答基准测试中表现优异，显著超过现有最先进方法，且在长上下文场景中表现稳定，不受上下文爆炸影响。

Conclusion: 分层设计及上下文解耦有效缓解了上下文过载问题，使大语言模型在复杂任务中性能提升和鲁棒性增强，验证了方法的有效性。

Abstract: Large Language Model (LLM) agents trained with reinforcement learning (RL) show great promise for solving complex, multi-step tasks. However, their performance is often crippled by "Context Explosion", where the accumulation of long text outputs overwhelms the model's context window and leads to reasoning failures. To address this, we introduce CoDA, a Context-Decoupled hierarchical Agent, a simple but effective reinforcement learning framework that decouples high-level planning from low-level execution. It employs a single, shared LLM backbone that learns to operate in two distinct, contextually isolated roles: a high-level Planner that decomposes tasks within a concise strategic context, and a low-level Executor that handles tool interactions in an ephemeral, isolated workspace. We train this unified agent end-to-end using PECO (Planner-Executor Co-Optimization), a reinforcement learning methodology that applies a trajectory-level reward to jointly optimize both roles, fostering seamless collaboration through context-dependent policy updates. Extensive experiments demonstrate that CoDA achieves significant performance improvements over state-of-the-art baselines on complex multi-hop question-answering benchmarks, and it exhibits strong robustness in long-context scenarios, maintaining stable performance while all other baselines suffer severe degradation, thus further validating the effectiveness of our hierarchical design in mitigating context overload.

</details>


### [31] [NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents](https://arxiv.org/abs/2512.12730)
*Jingzhe Ding,Shengda Long,Changxin Pu,Huan Zhou,Hongwan Gao,Xiang Gao,Chao He,Yue Hou,Fei Hu,Zhaojian Li,Weiran Shi,Zaiyuan Wang,Daoguang Zan,Chenchen Zhang,Xiaoxu Zhang,Qizhi Chen,Xianfu Cheng,Bo Deng,Qingshui Gu,Kai Hua,Juntao Lin,Pai Liu,Mingchen Li,Xuanguang Pan,Zifan Peng,Yujia Qin,Yong Shan,Zhewen Tan,Weihao Xie,Zihan Wang,Yishuo Yuan,Jiayu Zhang,Enduo Zhao,Yunfei Zhao,He Zhu,Chenyang Zou,Ming Ding,Jianpeng Jiao,Jiaheng Liu,Minghao Liu,Qian Liu,Chongyao Tao,Jian Yang,Tong Yang,Zhaoxiang Zhang,Xinjie Chen,Wenhao Huang,Ge Zhang*

Main category: cs.CL

TL;DR: 当前编码智能体在长远自动构建完整软件系统的能力评估方面存在不足，提出NL2Repo Bench基准专门测试长时程仓库生成能力，实验显示现有模型表现不足，长时程推理依然是核心瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有基准多聚焦于局部代码生成和短期修复任务，缺乏对智能体在长时间跨度内保持一致推理、规划和执行能力的评估，限制了对真实软件仓库构建能力的验证。

Method: 设计NL2Repo Bench基准，要求智能体仅凭单一自然语言需求文档和空工作区，独立设计架构、管理依赖、实现多模块逻辑，并生产可安装的完整Python库。

Result: 测试主流开源及闭源模型后发现长时程仓库生成任务难以突破，最高平均测试通过率低于40%，完整仓库正确构建极少，存在早期终止、全局连贯性丧失、交叉文件依赖脆弱及计划不足等问题。

Conclusion: NL2Repo Bench提供了严格且可验证的测试环境，明确指出长时程推理是实现高度自治编码智能体的主要瓶颈，为未来研究指明方向。

Abstract: Recent advances in coding agents suggest rapid progress toward autonomous software development, yet existing benchmarks fail to rigorously evaluate the long-horizon capabilities required to build complete software systems. Most prior evaluations focus on localized code generation, scaffolded completion, or short-term repair tasks, leaving open the question of whether agents can sustain coherent reasoning, planning, and execution over the extended horizons demanded by real-world repository construction. To address this gap, we present NL2Repo Bench, a benchmark explicitly designed to evaluate the long-horizon repository generation ability of coding agents. Given only a single natural-language requirements document and an empty workspace, agents must autonomously design the architecture, manage dependencies, implement multi-module logic, and produce a fully installable Python library. Our experiments across state-of-the-art open- and closed-source models reveal that long-horizon repository generation remains largely unsolved: even the strongest agents achieve below 40% average test pass rates and rarely complete an entire repository correctly. Detailed analysis uncovers fundamental long-horizon failure modes, including premature termination, loss of global coherence, fragile cross-file dependencies, and inadequate planning over hundreds of interaction steps. NL2Repo Bench establishes a rigorous, verifiable testbed for measuring sustained agentic competence and highlights long-horizon reasoning as a central bottleneck for the next generation of autonomous coding agents.

</details>


### [32] [Curió-Edu 7B: Examining Data Selection Impacts in LLM Continued Pretraining](https://arxiv.org/abs/2512.12770)
*Thales Sales Almeida,Rodrigo Nogueira,Hélio Pedrini*

Main category: cs.CL

TL;DR: 本论文研究了通过持续预训练来提升语言模型适应特定语言环境的能力，重点在于数据质量对模型表现的影响，结果表明精选数据训练的小规模模型效果优于使用全部数据的大规模模型。


<details>
  <summary>Details</summary>
Motivation: 目前在将通用语言模型适应特定语言或领域时，持续预训练是一种高效替代完整重训练的策略。然而仍不清楚在持续预训练中，是数据量还是数据质量发挥了更关键的作用。

Method: 作者基于LLaMA-2模型，利用ClassiCC-PT语料库中的1000亿葡萄牙语标记进行持续预训练，构建了Curió 7B模型。同时训练了仅用该语料库中教育和STEM领域筛选出的100亿标记数据的Curió-Edu 7B小规模模型。

Result: 尽管Curió-Edu 7B只使用了10%的数据和20%的计算资源，其在多项评测中表现超过了用全部数据训练的Curió 7B模型。

Conclusion: 数据质量和精选对语言模型的领域适应至关重要，在资源有限情况下，精选高质量数据训练的小规模模型能取得更优性能。

Abstract: Continued pretraining extends a language model's capabilities by further exposing it to additional data, often tailored to a specific linguistic or domain context. This strategy has emerged as an efficient alternative to full retraining when adapting general-purpose models to new settings. In this work, we investigate this paradigm through Curió 7B, a 7-billion-parameter model derived from LLaMA-2 and trained on 100 billion Portuguese tokens from the ClassiCC-PT corpus - the most extensive Portuguese-specific continued-pretraining effort above the three-billion-parameter scale to date. Beyond scale, we investigate whether quantity alone suffices or whether data quality plays a decisive role in linguistic adaptation. To this end, we introduce Curió-Edu 7B, a variant trained exclusively on the educational and STEM-filtered subset of the same corpus, totaling just 10 billion tokens. Despite using only 10% of the data and 20% of the computation, Curió-Edu 7B surpasses the full-corpus model in our evaluations, demonstrating that data selection can be fundamental even when adapting models with limited prior exposure to the target language. The developed models are available at https://huggingface.co/collections/ClassiCC-Corpus/curio-edu

</details>


### [33] [Persistent Personas? Role-Playing, Instruction Following, and Safety in Extended Interactions](https://arxiv.org/abs/2512.12775)
*Pedro Henrique Luz de Araujo,Michael A. Hedderich,Ali Modarressi,Hinrich Schuetze,Benjamin Roth*

Main category: cs.CL

TL;DR: 本文提出了一种长轮次（超过100轮）对话的评估协议，以系统测量大语言模型在多轮对话中保持人格一致性的能力。通过对七款主流模型的测试，发现人格一致性随对话长度增加而下降，且在人格与指令执行之间存在权衡。


<details>
  <summary>Details</summary>
Motivation: 现有的人格赋予大型语言模型的评估多限于短轮次，不能反映实际长时间交互中的表现，需要更真实的评估方法。

Method: 设计了结合长轮次人格对话和评估数据集的综合评测协议，对七款开放及封闭权重的最新模型进行人格忠实度、指令遵循及安全性的长上下文效应分析。

Result: 随着对话深入，模型的人格忠实度下降，特别是在需要同时保持人格一致和执行指令的目标导向对话中；人格模型表现初期不及非人格基线，后期因忠实度降低逐渐趋近基线表现。

Conclusion: 人格赋能的大语言模型在长时间对话中表现脆弱，本文提供的评估协议为系统测量和改进此类模型的长对话性能提供了工具。

Abstract: Persona-assigned large language models (LLMs) are used in domains such as education, healthcare, and sociodemographic simulation. Yet, they are typically evaluated only in short, single-round settings that do not reflect real-world usage. We introduce an evaluation protocol that combines long persona dialogues (over 100 rounds) and evaluation datasets to create dialogue-conditioned benchmarks that can robustly measure long-context effects. We then investigate the effects of dialogue length on persona fidelity, instruction-following, and safety of seven state-of-the-art open- and closed-weight LLMs. We find that persona fidelity degrades over the course of dialogues, especially in goal-oriented conversations, where models must sustain both persona fidelity and instruction following. We identify a trade-off between fidelity and instruction following, with non-persona baselines initially outperforming persona-assigned models; as dialogues progress and fidelity fades, persona responses become increasingly similar to baseline responses. Our findings highlight the fragility of persona applications in extended interactions and our work provides a protocol to systematically measure such failures.

</details>


### [34] [State over Tokens: Characterizing the Role of Reasoning Tokens](https://arxiv.org/abs/2512.12777)
*Mosh Levy,Zohar Elyoseph,Shauli Ravfogel,Yoav Goldberg*

Main category: cs.CL

TL;DR: 这篇论文提出了State over Tokens (SoT)框架，重新定义大语言模型生成的推理标记，将其视为计算状态而非文本叙述。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型生成的推理标记虽似人类思维过程，但并不真实反映模型的实际推理，存在形象与功能的差距。

Method: 引入SoT框架，将推理标记视为跨生成周期的持久计算状态，而非语言叙述，从计算状态角度解读推理标记。

Result: SoT框架解释了模型如何依靠推理标记正确推理但文本形式不真实反映内在过程，并提出了新的研究问题。

Conclusion: 要深入理解大语言模型的推理过程，研究应摒弃将推理标记作为文本读取，转而关注其作为状态的解码。

Abstract: Large Language Models (LLMs) can generate reasoning tokens before their final answer to boost performance on complex tasks. While these sequences seem like human thought processes, empirical evidence reveals that they are not a faithful explanation of the model's actual reasoning process. To address this gap between appearance and function, we introduce the State over Tokens (SoT) conceptual framework. SoT reframes reasoning tokens not as a linguistic narrative, but as an externalized computational state -- the sole persistent information carrier across the model's stateless generation cycles. This explains how the tokens can drive correct reasoning without being a faithful explanation when read as text and surfaces previously overlooked research questions on these tokens. We argue that to truly understand the process that LLMs do, research must move beyond reading the reasoning tokens as text and focus on decoding them as state.

</details>


### [35] [Does Tone Change the Answer? Evaluating Prompt Politeness Effects on Modern LLMs: GPT, Gemini, LLaMA](https://arxiv.org/abs/2512.12812)
*Hanyu Cai,Binqi Shen,Lier Jin,Lan Hu,Xiaojing Fan*

Main category: cs.CL

TL;DR: 本文系统评估了不同语气（友好、中立、无礼）对三款大型语言模型（GPT-4o mini、Gemini 2.0 Flash、Llama 4 Scout）在STEM和人文领域任务上的影响，发现语气效应因模型和领域而异，人文领域中无礼语气显著降低部分模型表现，而整体来看现代模型对语气变化较为鲁棒。


<details>
  <summary>Details</summary>
Motivation: 探究提示语中的语气和礼貌等语用元素对大型语言模型性能的影响，特别是不同模型家族间的差异，这在现有研究中尚未充分涉及。

Method: 设计系统评估框架，基于MMMLU基准测试，在六项跨领域任务中，比较Very Friendly、Neutral和Very Rude三种语气的提示，对三款最新大型语言模型进行准确率测试并做统计显著性分析。

Result: 结果显示语气敏感性依赖于模型和领域，人文任务中无礼语气显著降低GPT和Llama的准确率，而Gemini语气影响较小；STEM领域及跨领域聚合结果中语气影响减弱且无统计显著性。数据规模和覆盖范围影响语气效应的检测。

Conclusion: 交互语气在特定解释性任务中确实影响模型表现，但现代大型语言模型整体对语气变化较鲁棒，提示设计和模型选择时应考虑具体应用场景，提供了实际部署的参考方向。

Abstract: Prompt engineering has emerged as a critical factor influencing large language model (LLM) performance, yet the impact of pragmatic elements such as linguistic tone and politeness remains underexplored, particularly across different model families. In this work, we propose a systematic evaluation framework to examine how interaction tone affects model accuracy and apply it to three recently released and widely available LLMs: GPT-4o mini (OpenAI), Gemini 2.0 Flash (Google DeepMind), and Llama 4 Scout (Meta). Using the MMMLU benchmark, we evaluate model performance under Very Friendly, Neutral, and Very Rude prompt variants across six tasks spanning STEM and Humanities domains, and analyze pairwise accuracy differences with statistical significance testing.
  Our results show that tone sensitivity is both model-dependent and domain-specific. Neutral or Very Friendly prompts generally yield higher accuracy than Very Rude prompts, but statistically significant effects appear only in a subset of Humanities tasks, where rude tone reduces accuracy for GPT and Llama, while Gemini remains comparatively tone-insensitive. When performance is aggregated across tasks within each domain, tone effects diminish and largely lose statistical significance. Compared with earlier researches, these findings suggest that dataset scale and coverage materially influence the detection of tone effects. Overall, our study indicates that while interaction tone can matter in specific interpretive settings, modern LLMs are broadly robust to tonal variation in typical mixed-domain use, providing practical guidance for prompt design and model selection in real-world deployments.

</details>


### [36] [Hindsight is 20/20: Building Agent Memory that Retains, Recalls, and Reflects](https://arxiv.org/abs/2512.12818)
*Chris Latimer,Nicoló Boschi,Andrew Neeser,Chris Bartholomew,Gaurav Srivastava,Xuan Wang,Naren Ramakrishnan*

Main category: cs.CL

TL;DR: 本文提出了Hindsight，一种结构化的代理记忆架构，通过四个逻辑网络组织记忆，实现信息的保留、回忆和反思，在多会话和长时记忆任务中显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前的代理记忆系统将记忆作为外部层存储提取的会话片段，存在证据和推理模糊、长时信息组织困难及推理解释支持有限的问题，亟需更结构化和推理友好的记忆架构。

Method: Hindsight将代理记忆分为世界事实、代理经验、实体总结和演变信念四个逻辑网络，支持信息的保留、回忆和反思操作，通过时间和实体感知的记忆层构造结构化、可查询的记忆库，反思层负责推理和可追溯的信息更新。

Result: 在LongMemEval和LoCoMo等长时记忆基准测试中，Hindsight使用开源20B模型将准确率从39%提升至83.6%，并优于完整上下文的GPT-4o；进一步扩展模型规模，准确率达到91.4%和89.61%，大幅超越现有记忆架构。

Conclusion: Hindsight通过结构化记忆和推理机制显著提升了代理在多会话、开放领域问题上的长时记忆表现，为实现更有效的LLM代理记忆系统提供了新思路。

Abstract: Agent memory has been touted as a dimension of growth for LLM-based applications, enabling agents that can accumulate experience, adapt across sessions, and move beyond single-shot question answering. The current generation of agent memory systems treats memory as an external layer that extracts salient snippets from conversations, stores them in vector or graph-based stores, and retrieves top-k items into the prompt of an otherwise stateless model. While these systems improve personalization and context carry-over, they still blur the line between evidence and inference, struggle to organize information over long horizons, and offer limited support for agents that must explain their reasoning. We present Hindsight, a memory architecture that treats agent memory as a structured, first-class substrate for reasoning by organizing it into four logical networks that distinguish world facts, agent experiences, synthesized entity summaries, and evolving beliefs. This framework supports three core operations -- retain, recall, and reflect -- that govern how information is added, accessed, and updated. Under this abstraction, a temporal, entity aware memory layer incrementally turns conversational streams into a structured, queryable memory bank, while a reflection layer reasons over this bank to produce answers and to update information in a traceable way. On key long-horizon conversational memory benchmarks like LongMemEval and LoCoMo, Hindsight with an open-source 20B model lifts overall accuracy from 39% to 83.6% over a full-context baseline with the same backbone and outperforms full context GPT-4o. Scaling the backbone further pushes Hindsight to 91.4% on LongMemEval and up to 89.61% on LoCoMo (vs. 75.78% for the strongest prior open system), consistently outperforming existing memory architectures on multi-session and open-domain questions.

</details>


### [37] [What Matters in Evaluating Book-Length Stories? A Systematic Study of Long Story Evaluation](https://arxiv.org/abs/2512.12839)
*Dingyi Yang,Qin Jin*

Main category: cs.CL

TL;DR: 本文针对自动评估超长故事（>100K词）进行了系统研究，提出了大型基准数据集LongStoryEval，比较了多种评价方法，并设计了高效的评价模型NovelCritique，表现优于商业模型。


<details>
  <summary>Details</summary>
Motivation: 由于现有评价自动化方法难以应对超长文本故事，本研究旨在理解读者最关心的评价维度及探索适用于长篇故事的有效评价方法。

Method: 构建包含600本科长故事的LongStoryEval数据集，定义评价指标体系，比较聚合式、增量更新式和摘要式三种评价方法，提出基于摘要的NovelCritique模型进行故事多维度自动评价。

Result: 实验表明聚合式和摘要式评价方法表现最佳，聚合式在细节评估上优异，摘要式效率更高。NovelCritique在多评价维度上优于GPT-4o，与人类评价高度一致。

Conclusion: LongStoryEval及NovelCritique为超长故事的自动化多维评价提供了有效工具和基准，推动该领域研究与应用发展。

Abstract: In this work, we conduct systematic research in a challenging area: the automatic evaluation of book-length stories (>100K tokens). Our study focuses on two key questions: (1) understanding which evaluation aspects matter most to readers, and (2) exploring effective methods for evaluating lengthy stories. We introduce the first large-scale benchmark, LongStoryEval, comprising 600 newly published books with an average length of 121K tokens (maximum 397K). Each book includes its average rating and multiple reader reviews, presented as critiques organized by evaluation aspects. By analyzing all user-mentioned aspects, we propose an evaluation criteria structure and conduct experiments to identify the most significant aspects among the 8 top-level criteria. For evaluation methods, we compare the effectiveness of three types: aggregation-based, incremental-updated, and summary-based evaluations. Our findings reveal that aggregation- and summary-based evaluations perform better, with the former excelling in detail assessment and the latter offering greater efficiency. Building on these insights, we further propose NovelCritique, an 8B model that leverages the efficient summary-based framework to review and score stories across specified aspects. NovelCritique outperforms commercial models like GPT-4o in aligning with human evaluations. Our datasets and codes are available at https://github.com/DingyiYang/LongStoryEval.

</details>


### [38] [Counting Clues: A Lightweight Probabilistic Baseline Can Match an LLM](https://arxiv.org/abs/2512.12868)
*Furong Jia,Yuan Pu,Finn Guo,Monica Agrawal*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）在临床诊断多项选择测试中的表现，提出了基于频率的概率排序器（FBPR），利用预训练语料中的概念-诊断共现数据实现性能与LLMs可比，且两者表现互补，表明显式概率基线仍有价值。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs在临床诊断任务中的多项选择表现中，有多少是基于潜在的概率推理，而非其他机制。

Method: 提出FBPR方法，使用平滑朴素贝叶斯模型基于大规模语料库中概念与诊断的共现统计数据进行选项评分，并与对应预训练语料的LLMs进行对比。

Result: FBPR在使用与OLMo、Llama预训练语料相同共现数据时，性能与对应LLMs相当，但两种方法正确回答的问题差异较大，表现出优势互补。

Conclusion: 显式概率模型作为基线仍有重要价值，能够为LLMs表现提供参考并作为潜在混合方法的补充，表明基于频率统计的传统专家系统思想在复杂模型中依然占有一席之地。

Abstract: Large language models (LLMs) excel on multiple-choice clinical diagnosis benchmarks, yet it is unclear how much of this performance reflects underlying probabilistic reasoning. We study this through questions from MedQA, where the task is to select the most likely diagnosis. We introduce the Frequency-Based Probabilistic Ranker (FBPR), a lightweight method that scores options with a smoothed Naive Bayes over concept-diagnosis co-occurrence statistics from a large corpus. When co-occurrence statistics were sourced from the pretraining corpora for OLMo and Llama, FBPR achieves comparable performance to the corresponding LLMs pretrained on that same corpus. Direct LLM inference and FBPR largely get different questions correct, with an overlap only slightly above random chance, indicating complementary strengths of each method. These findings highlight the continued value of explicit probabilistic baselines: they provide a meaningful performance reference point and a complementary signal for potential hybridization. While the performance of LLMs seems to be driven by a mechanism other than simple frequency aggregation, we show that an approach similar to the historically grounded, low-complexity expert systems still accounts for a substantial portion of benchmark performance.

</details>


### [39] [Building from Scratch: A Multi-Agent Framework with Human-in-the-Loop for Multilingual Legal Terminology Mapping](https://arxiv.org/abs/2512.12950)
*Lingyi Meng,Maolin Liu,Hao Wang,Yilan Cheng,Qi Yang,Idlkaid Mohanmmed*

Main category: cs.CL

TL;DR: 本文提出了一种人机协作的多智能体框架，用于构建中英日三语的法律术语数据库，提高了术语映射的精确性与一致性。


<details>
  <summary>Details</summary>
Motivation: 由于中日两种语言存在大量同形异义词，且缺乏标准化资源，导致法律术语跨语言映射非常困难。

Method: 采用多智能体系统融合大型语言模型和法律专家，分别负责OCR、文本分割、语义对齐、术语提取与质量监督，实现人机协同工作流。

Result: 基于包含35个中文重要法规及其英日译文的三语平行语料测试，该框架显著提升了法律术语映射的准确率、一致性及工作扩展性。

Conclusion: 该人机协作多智能体框架优于传统人工方法，能够有效支持多语种法律术语数据库构建，实现高效且可靠的跨语言术语映射。

Abstract: Accurately mapping legal terminology across languages remains a significant challenge, especially for language pairs like Chinese and Japanese, which share a large number of homographs with different meanings. Existing resources and standardized tools for these languages are limited. To address this, we propose a human-AI collaborative approach for building a multilingual legal terminology database, based on a multi-agent framework. This approach integrates advanced large language models and legal domain experts throughout the entire process-from raw document preprocessing, article-level alignment, to terminology extraction, mapping, and quality assurance. Unlike a single automated pipeline, our approach places greater emphasis on how human experts participate in this multi-agent system. Humans and AI agents take on different roles: AI agents handle specific, repetitive tasks, such as OCR, text segmentation, semantic alignment, and initial terminology extraction, while human experts provide crucial oversight, review, and supervise the outputs with contextual knowledge and legal judgment. We tested the effectiveness of this framework using a trilingual parallel corpus comprising 35 key Chinese statutes, along with their English and Japanese translations. The experimental results show that this human-in-the-loop, multi-agent workflow not only improves the precision and consistency of multilingual legal terminology mapping but also offers greater scalability compared to traditional manual methods.

</details>


### [40] [QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory Management](https://arxiv.org/abs/2512.12967)
*Weizhou Shen,Ziyi Yang,Chenliang Li,Zhiyuan Lu,Miao Peng,Huashan Sun,Yingcheng Shi,Shengyi Liao,Shaopeng Lai,Bo Zhang,Dayiheng Liu,Fei Huang,Jingren Zhou,Ming Yan*

Main category: cs.CL

TL;DR: QwenLong-L1.5通过创新的训练方法和内存增强架构，实现了卓越的超长上下文推理能力，性能超越GPT-5和Gemini-2.5-Pro。


<details>
  <summary>Details</summary>
Motivation: 当前模型在处理超长上下文推理任务时存在能力和稳定性不足的问题，需系统性提升长上下文理解与推理能力。

Method: 采用长上下文数据合成框架生成复杂推理任务；提出稳定的长上下文强化学习方法（任务平衡采样和自适应熵控策略优化）；设计内存增强架构支持超4百万token的推理任务。

Result: QwenLong-L1.5在长上下文推理基准测试中平均领先基线9.90分，超长任务中提升9.48分，性能媲美甚至超过GPT-5和Gemini-2.5-Pro。

Conclusion: 通过系统的训练创新和内存管理机制，QwenLong-L1.5显著提升了超长上下文推理能力，并带动科学推理、记忆工具使用及长对话等领域表现提升。

Abstract: We introduce QwenLong-L1.5, a model that achieves superior long-context reasoning capabilities through systematic post-training innovations. The key technical breakthroughs of QwenLong-L1.5 are as follows: (1) Long-Context Data Synthesis Pipeline: We develop a systematic synthesis framework that generates challenging reasoning tasks requiring multi-hop grounding over globally distributed evidence. By deconstructing documents into atomic facts and their underlying relationships, and then programmatically composing verifiable reasoning questions, our approach creates high-quality training data at scale, moving substantially beyond simple retrieval tasks to enable genuine long-range reasoning capabilities. (2) Stabilized Reinforcement Learning for Long-Context Training: To overcome the critical instability in long-context RL, we introduce task-balanced sampling with task-specific advantage estimation to mitigate reward bias, and propose Adaptive Entropy-Controlled Policy Optimization (AEPO) that dynamically regulates exploration-exploitation trade-offs. (3) Memory-Augmented Architecture for Ultra-Long Contexts: Recognizing that even extended context windows cannot accommodate arbitrarily long sequences, we develop a memory management framework with multi-stage fusion RL training that seamlessly integrates single-pass reasoning with iterative memory-based processing for tasks exceeding 4M tokens. Based on Qwen3-30B-A3B-Thinking, QwenLong-L1.5 achieves performance comparable to GPT-5 and Gemini-2.5-Pro on long-context reasoning benchmarks, surpassing its baseline by 9.90 points on average. On ultra-long tasks (1M~4M tokens), QwenLong-L1.5's memory-agent framework yields a 9.48-point gain over the agent baseline. Additionally, the acquired long-context reasoning ability translates to enhanced performance in general domains like scientific reasoning, memory tool using, and extended dialogue.

</details>


### [41] [Authors Should Annotate](https://arxiv.org/abs/2512.12976)
*Marcus Ma,Cole Johnson,Nolan Bridges,Jackson Trager,Georgios Chochlakis,Shrikanth Narayanan*

Main category: cs.CL

TL;DR: 本文提出了一种作者标注方法，让文本作者在创作时直接对数据进行标注，用于主观特征如情感和信念的识别。


<details>
  <summary>Details</summary>
Motivation: 传统文本标注多依赖第三方，但对于作者自身视角的主观信息（如情感、信念）采用第三方标注效果不佳，需要更准确高效的标注方法。

Method: 与商业聊天机器人合作，部署实时作者标注系统，自动识别任务相关查询，动态生成标注问题并记录作者回答，同时训练基于在线学习的产品推荐模型持续改进。

Result: 通过作者标注训练的模型点击率提高了534%，相较三种传统情感标注方法，作者标注在质量、效率和成本上都优于传统方法。

Conclusion: 作者标注在主观和自我中心信息标注上具有显著优势，建议科研社区推广使用，作者也发布了相应的服务平台。

Abstract: The status quo for labeling text is third-party annotation, but there are many cases where information directly from the document's source would be preferable over a third-person proxy, especially for egocentric features like sentiment and belief. We introduce author labeling, an annotation technique where the writer of the document itself annotates the data at the moment of creation. We collaborate with a commercial chatbot with over 10,000 users to deploy an author labeling annotation system for subjective features related to product recommendation. This system identifies task-relevant queries, generates on-the-fly labeling questions, and records authors' answers in real time. We train and deploy an online-learning model architecture for product recommendation that continuously improves from author labeling and find it achieved a 534% increase in click-through rate compared to an industry advertising baseline running concurrently. We then compare the quality and practicality of author labeling to three traditional annotation approaches for sentiment analysis and find author labeling to be higher quality, faster to acquire, and cheaper. These findings reinforce existing literature that annotations, especially for egocentric and subjective beliefs, are significantly higher quality when labeled by the author rather than a third party. To facilitate broader scientific adoption, we release an author labeling service for the research community at academic.echollm.io.

</details>


### [42] [An Open and Reproducible Deep Research Agent for Long-Form Question Answering](https://arxiv.org/abs/2512.13059)
*Ikuya Yamada,Wataru Ikeda,Ko Yoshida,Mengyu Ye,Hinata Sugimoto,Masatoshi Suzuki,Hisanori Ozaki,Jun Suzuki*

Main category: cs.CL

TL;DR: 该论文提出了一种结合开源大语言模型和开放网页搜索的长文本问答系统，在多方面优化回答质量，获得比赛冠军并开源代码。


<details>
  <summary>Details</summary>
Motivation: 当前长文本问答系统在推理质量和多方面表现上仍有提升空间，尤其是真实开放域的应用场景中。

Method: 系统将开源大语言模型与网页搜索API结合，采用迭代检索、推理和综合策略，并引入基于模型评判员反馈的偏好调优，评估清晰性、洞见性和真实性等方面。

Result: 实验表明该方法在提升回答的清晰性、洞见性和事实准确性方面均有稳定效果，系统在MMU-RAG竞赛中获胜。

Conclusion: 结合大语言模型与开放搜索，并利用偏好调优提升推理质量是提升长文本问答性能的有效途径，系统具有实际应用价值且已开源。

Abstract: We present an open deep research system for long-form question answering, selected as a winning system in the text-to-text track of the MMU-RAG competition at NeurIPS 2025. The system combines an open-source large language model (LLM) with an open web search API to perform iterative retrieval, reasoning, and synthesis in real-world open-domain settings. To enhance reasoning quality, we apply preference tuning based on LLM-as-a-judge feedback that evaluates multiple aspects, including clarity, insightfulness, and factuality. Our experimental results show that the proposed method consistently improves answer quality across all three aspects. Our source code is publicly available at https://github.com/efficient-deep-research/efficient-deep-research.

</details>


### [43] [LLM Rationalis? Measuring Bargaining Capabilities of AI Negotiators](https://arxiv.org/abs/2512.13063)
*Cheril Shah,Akshit Agarwal,Kanak Garg,Mourad Heddaya*

Main category: cs.CL

TL;DR: 本文提出了基于双曲正切曲线的退让动态数学框架和两个衡量指标，比较了人类与大型语言模型（LLMs）在多种谈判场景中的表现，发现LLMs缺乏灵活适应能力且策略单一。


<details>
  <summary>Details</summary>
Motivation: 双边谈判复杂且依赖情境和策略调整，现有大型语言模型在谈判中表现有限，需探讨其局限性和改进方向。

Method: 构建基于双曲正切曲线的统一数学模型，提出burstiness tau和退让刚性指数（CRI）指标，进行大规模实证对比实验，分析人类与四种先进LLMs在不同谈判情境下的表现。

Result: 发现人类谈判者能平滑适应和推理对手策略，LLMs则倾向于锚定极端立场，固定优化策略，策略多样性低且有时带有欺骗性，且模型升级未显著提升谈判能力。

Conclusion: 当前大型语言模型在谈判中的能力存在根本性限制，亟需设计能够更好内化对手推理及情境策略的模型。

Abstract: Bilateral negotiation is a complex, context-sensitive task in which human negotiators dynamically adjust anchors, pacing, and flexibility to exploit power asymmetries and informal cues. We introduce a unified mathematical framework for modeling concession dynamics based on a hyperbolic tangent curve, and propose two metrics burstiness tau and the Concession-Rigidity Index (CRI) to quantify the timing and rigidity of offer trajectories. We conduct a large-scale empirical comparison between human negotiators and four state-of-the-art large language models (LLMs) across natural-language and numeric-offers settings, with and without rich market context, as well as six controlled power-asymmetry scenarios. Our results reveal that, unlike humans who smoothly adapt to situations and infer the opponents position and strategies, LLMs systematically anchor at extremes of the possible agreement zone for negotiations and optimize for fixed points irrespective of leverage or context. Qualitative analysis further shows limited strategy diversity and occasional deceptive tactics used by LLMs. Moreover the ability of LLMs to negotiate does not improve with better models. These findings highlight fundamental limitations in current LLM negotiation capabilities and point to the need for models that better internalize opponent reasoning and context-dependent strategy.

</details>


### [44] [Uncovering the Role of Initial Saliency in U-Shaped Attention Bias: Scaling Initial Token Weight for Enhanced Long-Text Processing](https://arxiv.org/abs/2512.13109)
*Zewen Qiang,Sendong Zhao,Haochun Wang,Bing Qin,Ting Liu*

Main category: cs.CL

TL;DR: 本文发现大语言模型在处理长文本时中间部分关注度不足，原因包括位置编码和初始显著性，并提出通过调整初始token的注意力权重提升模型处理长文本能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理长文本时表现不佳，主要是因为注意力机制偏向文本开头和结尾，导致中间部分信息丢失，这限制了模型的表现。

Method: 作者识别出注意力机制中的“初始显著性”因素，即初始token的注意力权重影响后续token的关注度。通过调节初始token与其他token之间的注意力权重，改善模型对长文本的关注能力，同时结合已有减少位置编码偏差的方法进一步提升性能。

Result: 在MDQA数据集上，该方法使模型表现提升最多3.6%，结合减少位置偏差的方法在KV-Retrieval任务中进一步提高3.4%性能。

Conclusion: 调整初始token的注意力权重是缓解大语言模型 "迷失中间" 现象的有效手段，且与现有位置编码优化方法结合可进一步提升长文本处理能力。

Abstract: Large language models (LLMs) have demonstrated strong performance on a variety of natural language processing (NLP) tasks. However, they often struggle with long-text sequences due to the ``lost in the middle'' phenomenon. This issue has been shown to arise from a U-shaped attention bias, where attention is disproportionately focused on the beginning and end of a text, leaving the middle section underrepresented. While previous studies have attributed this bias to position encoding, our research first identifies an additional factor: initial saliency. It means that in the attention computation for each token, tokens with higher attention weights relative to the initial token tend to receive more attention in the prediction of the next token. We further find that utilizing this property by scaling attention weight between the initial token and others improves the model's ability to process long contexts, achieving a maximum improvement of 3.6\% in MDQA dataset. Moreover, combining this approach with existing methods to reduce position encoding bias further enhances performance, achieving a maximum improvement of 3.4\% in KV-Retrieval tasks.

</details>


### [45] [Efficient Adaptive Rejection Sampling for Accelerating Speculative Decoding in Large Language Models](https://arxiv.org/abs/2512.13194)
*Chendong Sun*

Main category: cs.CL

TL;DR: EARS方法通过动态调整接受阈值，解决了推测解码中随机拒绝的问题，提高了大语言模型的推理效率。


<details>
  <summary>Details</summary>
Motivation: 传统推测解码的拒绝采样机制使用固定阈值，导致在高不确定性生成场景下，合理候选词被随机拒绝，影响推理效率。

Method: 提出EARS方法，根据目标模型的预测不确定性动态调整接受阈值，引入容忍项，降低随机拒绝概率。

Result: EARS在创意写作和开放领域问答任务中实现了高达18.12%的吞吐量提升，同时保持了极低的准确率下降（0.84%）。

Conclusion: EARS无需修改模型结构，可无缝集成到现有推测解码框架中，显著提升推理效率，特别在高不确定性场景下表现优异。

Abstract: Speculative Decoding is a prominent technique for accelerating the autoregressive inference of large language models (LLMs) by employing a fast draft model to propose candidate token sequences and a large target model to verify them in parallel. However, its core component -- the rejection sampling mechanism -- relies on a fixed, context-independent random threshold. This leads to a significant "random rejection" problem in high-uncertainty generation scenarios, where plausible candidate tokens are frequently rejected due to random chance, undermining inference efficiency. This paper introduces Efficient Adaptive Rejection Sampling (EARS), a novel method that dynamically adjusts the acceptance threshold by incorporating the target model's own predictive uncertainty, measured as \(1 - \max(P_{\mathrm{target}})\). By introducing a tolerance term proportional to this uncertainty, EARS intelligently relaxes the acceptance criterion when the model is uncertain, effectively reducing random rejections while maintaining strict standards when the model is confident. Experiments on creative writing and open-domain QA tasks demonstrate that EARS significantly enhances the efficiency of speculative decoding, achieving up to an 18.12% increase in throughput with a negligible 0.84% accuracy drop on the GSM8K benchmark. The method requires no modifications to model architectures and can be seamlessly integrated into existing speculative decoding frameworks.

</details>


### [46] [AutoTool: Dynamic Tool Selection and Integration for Agentic Reasoning](https://arxiv.org/abs/2512.13278)
*Jiaru Zou,Ling Yang,Yunzhe Qi,Sirui Chen,Mengting Ai,Ke Shen,Jingrui He,Mengdi Wang*

Main category: cs.CL

TL;DR: AutoTool通过动态选择工具提升了大语言模型在多任务中的推理和工具使用能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设工具集固定，限制了大语言模型代理适应新工具或不断变化工具集的能力。

Method: 通过构建包含丰富工具选择理由的大规模数据集，采用监督学习与强化学习结合的两阶段优化流程，包括轨迹稳定和基于KL正则化的多步工具选择排序优化。

Result: 在数学、科学、代码生成和多模态推理等十个基准测试中，AutoTool在少参数情况下取得了优于先进方法的显著性能提升，同时具备动态利用未见工具的泛化能力。

Conclusion: AutoTool有效提升了大语言模型代理的动态工具选择和复杂推理能力，增强了模型面对不断变化工具集的适应性和泛化能力。

Abstract: Agentic reinforcement learning has advanced large language models (LLMs) to reason through long chain-of-thought trajectories while interleaving external tool use. Existing approaches assume a fixed inventory of tools, limiting LLM agents' adaptability to new or evolving toolsets. We present AutoTool, a framework that equips LLM agents with dynamic tool-selection capabilities throughout their reasoning trajectories. We first construct a 200k dataset with explicit tool-selection rationales across 1,000+ tools and 100+ tasks spanning mathematics, science, code generation, and multimodal reasoning. Building on this data foundation, AutoTool employs a dual-phase optimization pipeline: (i) supervised and RL-based trajectory stabilization for coherent reasoning, and (ii) KL-regularized Plackett-Luce ranking to refine consistent multi-step tool selection. Across ten diverse benchmarks, we train two base models, Qwen3-8B and Qwen2.5-VL-7B, with AutoTool. With fewer parameters, AutoTool consistently outperforms advanced LLM agents and tool-integration methods, yielding average gains of 6.4% in math & science reasoning, 4.5% in search-based QA, 7.7% in code generation, and 6.9% in multimodal understanding. In addition, AutoTool exhibits stronger generalization by dynamically leveraging unseen tools from evolving toolsets during inference.

</details>


### [47] [AIR: Post-training Data Selection for Reasoning via Attention Head Influence](https://arxiv.org/abs/2512.13279)
*Jinrui Liu,Jeff Wu,Xuanguang Pan,Gavin Cheung,Shuai Ma,Chongyang Tao*

Main category: cs.CL

TL;DR: 该论文提出了AIR方法，通过识别并量化注意力头对推理步骤的重要性，实现高效的后训练蒸馏数据选择，从而提升大型语言模型的多步推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有数据选择方法未能准确捕捉推理步骤的因果重要性，导致推理技能蒸馏效率低下。

Method: 提出一个无监督、无训练的AIR框架，识别关键的注意力头，构建弱化参考模型，计算注意力影响评分用于细粒度步骤和样本选择。

Result: 多个推理基准测试表明，AIR方法优于启发式基线，显著提升了推理准确率，并能够有效隔离最关键的步骤和样本。

Conclusion: AIR提供了一种基于机制驱动的数据高效推理蒸馏方法，为LLM多步推理能力转移提供了新的思路。

Abstract: LLMs achieve remarkable multi-step reasoning capabilities, yet effectively transferring these skills via post-training distillation remains challenging. Existing data selection methods, ranging from manual curation to heuristics based on length, entropy, or overall loss, fail to capture the causal importance of individual reasoning steps, limiting distillation efficiency. To address this, we propose Attention Influence for Reasoning (AIR), a principled, unsupervised and training-free framework that leverages mechanistic insights of the retrieval head to select high-value post-training data. AIR first identifies reasoning-critical attention heads of an off-the-shelf model, then constructs a weakened reference model with disabled head influence, and finally quantifies the resulting loss divergence as the Attention Influence Score. This score enables fine-grained assessment at both the step and sample levels, supporting step-level weighted fine-tuning and global sample selection. Experiments across multiple reasoning benchmarks show that AIR consistently improves reasoning accuracy, surpassing heuristic baselines and effectively isolating the most critical steps and samples. Our work establishes a mechanism-driven, data-efficient approach for reasoning distillation in LLMs.

</details>


### [48] [Integrating Causal Reasoning into Automated Fact-Checking](https://arxiv.org/abs/2512.13286)
*Youssra Rebboud,Pasquale Lisena,Raphael Troncy*

Main category: cs.CL

TL;DR: 该论文提出了一种结合事件关系提取、语义相似度计算和基于规则推理的方法，用于识别事实核查中陈述与证据之间的因果关系不一致，从而提升自动事实核查的因果推理能力和解释性。


<details>
  <summary>Details</summary>
Motivation: 现有自动事实核查方法缺乏专门的因果关系推理，导致无法充分利用因果关系信息提升判定的语义解释能力。

Method: 通过事件关系提取技术抽取因果事件链，计算语义相似度，并结合基于规则的推理，检测陈述与证据中事件链之间的逻辑不一致性。

Result: 在两个事实核查数据集上的实验表明，该方法首次为细粒度因果事件关系融入事实核查构建了基线，提升了判决解释性。

Conclusion: 该研究填补了因果事件关系推理在事实核查中的空白，提升了自动核查系统对于因果逻辑的一致性检测能力和判定解释的丰富度。

Abstract: In fact-checking applications, a common reason to reject a claim is to detect the presence of erroneous cause-effect relationships between the events at play. However, current automated fact-checking methods lack dedicated causal-based reasoning, potentially missing a valuable opportunity for semantically rich explainability. To address this gap, we propose a methodology that combines event relation extraction, semantic similarity computation, and rule-based reasoning to detect logical inconsistencies between chains of events mentioned in a claim and in an evidence. Evaluated on two fact-checking datasets, this method establishes the first baseline for integrating fine-grained causal event relationships into fact-checking and enhance explainability of verdict prediction.

</details>


### [49] [MiniLingua: A Small Open-Source LLM for European Languages](https://arxiv.org/abs/2512.13298)
*Anna Aksenova,Boris Zverkov,Nicola Dainese,Alexander Nikitin,Pekka Marttinen*

Main category: cs.CL

TL;DR: MiniLingua 是一个一亿参数的多语言开源大语言模型，专为13种欧洲语言设计，兼顾覆盖度和指令跟随能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型存在计算成本高、隐私问题及以英语为中心的训练限制，因此需要更小巧高效的多语言模型。

Method: 从头训练了一个拥有一亿参数的MiniLingua模型，涵盖13种欧洲语言，并进行了指令微调。

Result: 指令微调版本的MiniLingua在摘要、分类及问答任务上优于欧盟类似模型EuroLLM，且在开放式生成任务中表现接近更先进的顶尖模型。

Conclusion: MiniLingua实现了高效多语种模型在多项任务上的卓越表现，且已开源所有相关资源，推动实际应用和研究。

Abstract: Large language models are powerful but often limited by high computational cost, privacy concerns, and English-centric training. Recent progress demonstrates that small, efficient models with around one billion parameters can deliver strong results and enable on-device use. This paper introduces MiniLingua, a multilingual open-source LLM of one billion parameters trained from scratch for 13 European languages, designed to balance coverage and instruction-following capabilities. Based on evaluation results, the instruction-tuned version of MiniLingua outperforms EuroLLM, a model with a similar training approach but a larger training budget, on summarization, classification and both open- and closed-book question answering. Moreover, it remains competitive with more advanced state-of-the-art models on open-ended generation tasks. We release model weights, tokenizer and source code used for data processing and model training.

</details>


### [50] [FIN-bench-v2: A Unified and Robust Benchmark Suite for Evaluating Finnish Large Language Models](https://arxiv.org/abs/2512.13330)
*Joona Kytöniemi,Jousia Piha,Akseli Reunamo,Fedor Vitiugin,Farrokh Mehryary,Sampo Pyysalo*

Main category: cs.CL

TL;DR: FIN-bench-v2是一个统一的芬兰语大型语言模型评测套件，涵盖多任务、多格式，提供公开数据和评测工具。


<details>
  <summary>Details</summary>
Motivation: 为了提供一个统一、格式一致且覆盖多种任务的芬兰语大语言模型评测平台，帮助评估模型在芬兰语上的能力。

Method: 整合现有芬兰语评测数据集，进行人类审核，使用2.15B参数模型预训练选取鲁棒任务，评测更大指令微调模型，并公开数据和工具。

Result: 成功筛选出满足稳健性标准的评测任务，涵盖阅读理解、常识推理、情感分析等多类型任务，呈现了多模型多任务性能表现。

Conclusion: FIN-bench-v2为芬兰语大语言模型性能评估提供了系统化、公开、标准化的资源和方法，促进芬兰语NLP研究的发展。

Abstract: We introduce FIN-bench-v2, a unified benchmark suite for evaluating large language models in Finnish. FIN-bench-v2 consolidates Finnish versions of widely used benchmarks together with an updated and expanded version of the original FIN-bench into a single, consistently formatted collection, covering multiple-choice and generative tasks across reading comprehension, commonsense reasoning, sentiment analysis, world knowledge, and alignment. All datasets are converted to HuggingFace Datasets, which include both cloze and multiple-choice prompt formulations with five variants per task, and we incorporate human annotation or review for machine-translated resources such as GoldenSwag and XED. To select robust tasks, we pretrain a set of 2.15B-parameter decoder-only models and use their learning curves to compute monotonicity, signal-to-noise, non-random performance, and model ordering consistency, retaining only tasks that satisfy all criteria. We further evaluate a set of larger instruction-tuned models to characterize performance across tasks and prompt formulations. All datasets, prompts, and evaluation configurations are publicly available via our fork of the Language Model Evaluation Harness at https://github.com/LumiOpen/lm-evaluation-harness. Supplementary resources are released in a separate repository at https://github.com/TurkuNLP/FIN-bench-v2.

</details>


### [51] [Detecting Emotion Drift in Mental Health Text Using Pre-Trained Transformers](https://arxiv.org/abs/2512.13363)
*Shibani Sankpal*

Main category: cs.CL

TL;DR: 本研究分析了心理健康相关消息中情绪随文本变化的动态，利用预训练模型捕捉句子级情绪变化。


<details>
  <summary>Details</summary>
Motivation: 传统情感分析通常对整条消息进行整体分类，忽略了情绪在文本中的细微变化。

Method: 使用预训练的DistilBERT和RoBERTa模型检测句子级情绪，并计算情绪漂移得分。

Result: 揭示了心理健康对话中情绪升级或缓解的模式。

Conclusion: 该方法有助于深入理解内容中的情绪动态，特别是在心理健康领域的应用。

Abstract: This study investigates emotion drift: the change in emotional state across a single text, within mental health-related messages. While sentiment analysis typically classifies an entire message as positive, negative, or neutral, the nuanced shift of emotions over the course of a message is often overlooked. This study detects sentence-level emotions and measures emotion drift scores using pre-trained transformer models such as DistilBERT and RoBERTa. The results provide insights into patterns of emotional escalation or relief in mental health conversations. This methodology can be applied to better understand emotional dynamics in content.

</details>


### [52] [Large language models are not about language](https://arxiv.org/abs/2512.13441)
*Johan J. Bolhuis,Andrea Moro,Stephen Crain,Sandiway Fong*

Main category: cs.CL

TL;DR: 大型语言模型依赖大量数据处理外部语言文本，而人类语言基于内部计算系统，能递归生成层级思维结构，区别真实和不可能的语言。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型在语言学研究的局限性，强调人类语言能力源于内部心智系统，而非仅靠外部语言数据。

Method: 通过对比大型语言模型的概率性质与人类语言的心智计算系统，分析两者在语言理解和生成上的差异。

Result: 指出大型语言模型需要大量外部数据才能分析文字串，而人类语言系统能在极少输入下成长，且能区分可能与不可能的语言。

Conclusion: 大型语言模型在语言学研究中作用有限，真正的语言理解需基于内在心智计算系统。

Abstract: Large Language Models are useless for linguistics, as they are probabilistic models that require a vast amount of data to analyse externalized strings of words. In contrast, human language is underpinned by a mind-internal computational system that recursively generates hierarchical thought structures. The language system grows with minimal external input and can readily distinguish between real language and impossible languages.

</details>


### [53] [Scaling Laws for Code: Every Programming Language Matters](https://arxiv.org/abs/2512.13472)
*Jian Yang,Shawn Guo,Lin Jing,Wei Zhang,Aishan Liu,Chuan Hao,Zhoujun Li,Wayne Xin Zhao,Xianglong Liu,Weifeng Lv,Bryan Dai*

Main category: cs.CL

TL;DR: 本文系统研究了多语言代码大语言模型的扩展规律，发现不同编程语言对性能影响显著，多语言预训练和并行配对策略可提升模型性能，提出比例依赖的多语言扩展规律优化训练资源分配。


<details>
  <summary>Details</summary>
Motivation: 现有扩展规律忽略了不同编程语言对预训练性能的差异及现代软件多语言特性，导致性能预测不准确，故需研究多语言代码预训练的扩展规律及语言间相互影响。

Method: 通过1000多次实验，涵盖多种编程语言、模型大小和数据规模，系统探索多语言代码预训练的扩展规律；提出并验证并行配对预训练策略及比例依赖的多语言扩展规律，以优化训练资源分配。

Result: 发现解释型语言（如Python）较编译型语言（如Rust）对模型规模和数据增长更敏感；多语言预训练在语法相似语言间具协同效应；并行配对策略显著提升跨语言能力；比例依赖扩展规律在相同计算资源下优于均匀分配。

Conclusion: 多语言代码模型的训练应考虑语言特性与相互影响，通过优化数据分配策略和预训练方法，可有效提升模型性能和训练效率。

Abstract: Code large language models (Code LLMs) are powerful but costly to train, with scaling laws predicting performance from model size, data, and compute. However, different programming languages (PLs) have varying impacts during pre-training that significantly affect base model performance, leading to inaccurate performance prediction. Besides, existing works focus on language-agnostic settings, neglecting the inherently multilingual nature of modern software development. Therefore, it is first necessary to investigate the scaling laws of different PLs, and then consider their mutual influences to arrive at the final multilingual scaling law. In this paper, we present the first systematic exploration of scaling laws for multilingual code pre-training, conducting over 1000+ experiments (Equivalent to 336,000+ H800 hours) across multiple PLs, model sizes (0.2B to 14B parameters), and dataset sizes (1T tokens). We establish comprehensive scaling laws for code LLMs across multiple PLs, revealing that interpreted languages (e.g., Python) benefit more from increased model size and data than compiled languages (e.g., Rust). The study demonstrates that multilingual pre-training provides synergistic benefits, particularly between syntactically similar PLs. Further, the pre-training strategy of the parallel pairing (concatenating code snippets with their translations) significantly enhances cross-lingual abilities with favorable scaling properties. Finally, a proportion-dependent multilingual scaling law is proposed to optimally allocate training tokens by prioritizing high-utility PLs (e.g., Python), balancing high-synergy pairs (e.g., JavaScript-TypeScript), and reducing allocation to fast-saturating languages (Rust), achieving superior average performance across all PLs compared to uniform distribution under the same compute budget.

</details>


### [54] [Non-Resolution Reasoning: A Framework for Preserving Semantic Ambiguity in Language Models](https://arxiv.org/abs/2512.13478)
*Kei Saito*

Main category: cs.CL

TL;DR: 该论文提出了非解析推理（NRR）框架，解决语言模型在推理中语义过早塌缩的问题，通过保持多重语义解释提升了模型在意义歧义处理和上下文跟踪上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型因软最大化竞争和贪婪解码导致语义过早塌缩，弃置有效解释，造成推理脆弱和上下文失败。

Method: 引入多向量嵌入、非塌缩注意力机制和上下文身份追踪三大组件，通过外部解析算子显式控制语义解析时机，实现语义表示与解析解耦。

Result: 在身份切换任务中，采用上下文身份追踪的模型准确率达到90.9%，远超标准Transformer模型的9.1%，有效保存歧义并跟踪上下文。

Conclusion: NRR框架通过将歧义作为显式状态，提供了避免语义过早塌缩的原则性方法，强调解决语义歧义应由任务和控制决策决定。

Abstract: Premature semantic collapse -- the forced early commitment to a single meaning -- remains a core architectural limitation of current language models. Softmax-driven competition and greedy decoding cause models to discard valid interpretations before sufficient context is available, resulting in brittle reasoning and context failures. We introduce Non-Resolution Reasoning (NRR), a general computational framework that preserves semantic ambiguity during inference and performs resolution only when explicitly required. NRR integrates three components: (1) Multi-Vector Embeddings that maintain multiple viable interpretations per token, (2) Non-Collapsing Attention that prevents winner-take-all dynamics across layers, and (3) Contextual Identity Tracking (CIT), which assigns context-specific identities to recurring entities (e.g., distinguishing "Dr. Smith the cardiologist" from "Dr. Smith the researcher"). These mechanisms are unified by an external Resolution Operator $ρ$ that makes semantic commitment explicit, controllable, and task-dependent. Unlike standard architectures, NRR separates representation from resolution, allowing a single model to shift between creative, factual, and ambiguity-preserving reasoning without retraining. A synthetic evaluation demonstrates NRR's ability to preserve ambiguity and track context: CIT-enhanced models achieve 90.9% accuracy on out-of-distribution identity-shift tasks, compared to 9.1% for transformer baselines. NRR provides a principled alternative to premature collapse, reframing ambiguity as an explicit representational state rather than a failure mode. The question is not whether AI should resolve ambiguity, but when, how, and under whose control.

</details>


### [55] [Advancing Bangla Machine Translation Through Informal Datasets](https://arxiv.org/abs/2512.13487)
*Ayon Roy,Risat Rahaman,Sadat Shibly,Udoy Saha Joy,Abdulla Al Kafi,Farig Yousuf Sadeque*

Main category: cs.CL

TL;DR: 本文针对孟加拉语作为全球第六大语言，且开源机器翻译发展不足的问题，提出通过非正式语言数据集和改进模型提升孟加拉语翻译质量。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语虽使用人数众多，但开放资源和机器翻译主要集中在正式语言，且缺少非正式语言的成对数据和先进模型，导致大量用户难以获取信息。

Method: 收集来自社交媒体和对话文本的非正式孟加拉语数据，探索并改进当前最先进的翻译模型以适应非正式语言。

Result: 开发了包含非正式语言的孟加拉语数据集，提升了机器翻译模型处理自然语言的能力。

Conclusion: 通过注重非正式孟加拉语的翻译和数据丰富，有助于提高孟加拉语使用者获取数字信息的便利性，推动孟加拉语机器翻译技术发展。

Abstract: Bangla is the sixth most widely spoken language globally, with approximately 234 million native speakers. However, progress in open-source Bangla machine translation remains limited. Most online resources are in English and often remain untranslated into Bangla, excluding millions from accessing essential information. Existing research in Bangla translation primarily focuses on formal language, neglecting the more commonly used informal language. This is largely due to the lack of pairwise Bangla-English data and advanced translation models. If datasets and models can be enhanced to better handle natural, informal Bangla, millions of people will benefit from improved online information access. In this research, we explore current state-of-the-art models and propose improvements to Bangla translation by developing a dataset from informal sources like social media and conversational texts. This work aims to advance Bangla machine translation by focusing on informal language translation and improving accessibility for Bangla speakers in the digital world.

</details>


### [56] [SkipCat: Rank-Maximized Low-Rank Compression of Large Language Models via Shared Projection and Block Skipping](https://arxiv.org/abs/2512.13494)
*Yu-Chen Lu,Sheng-Feng Yu,Hui-Hsien Weng,Pei-Shuo Wang,Yu-Fang Hu,Liang Hung-Chun,Hung-Yueh Chiang,Kai-Chiang Wu*

Main category: cs.CL

TL;DR: 本文提出了SkipCat低秩压缩框架，通过共享低秩投影和块跳过技术，在相同压缩率下保留更高秩，提升模型效率和性能。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型参数庞大，难以部署于资源有限的边缘设备，传统低秩压缩需极大降低秩导致性能大幅下降，存在效率与性能的权衡难题。

Method: 提出SkipCat框架，包括层内共享低秩投影和块跳过技术，减少冗余计算和内存传输，使模型在相同压缩率下可保留更高秩，提升压缩效率。

Result: 在无额外微调情况下，SkipCat在相同压缩率下零样本任务准确率提升7%，优于现有低秩压缩方法。

Conclusion: SkipCat有效缓解了低秩压缩中性能与效率的权衡，在资源紧张环境下更好地保持模型性能，适合边缘设备部署。

Abstract: Large language models (LLM) have achieved remarkable performance across a wide range of tasks. However, their substantial parameter sizes pose significant challenges for deployment on edge devices with limited computational and memory resources. Low-rank compression is a promising approach to address this issue, as it reduces both computational and memory costs, making LLM more suitable for resource-constrained environments. Nonetheless, naïve low-rank compression methods require a significant reduction in the retained rank to achieve meaningful memory and computation savings. For a low-rank model, the ranks need to be reduced by more than half to yield efficiency gains. Such aggressive truncation, however, typically results in substantial performance degradation. To address this trade-off, we propose SkipCat, a novel low-rank compression framework that enables the use of higher ranks while achieving the same compression rates. First, we introduce an intra-layer shared low-rank projection method, where multiple matrices that share the same input use a common projection. This reduces redundancy and improves compression efficiency. Second, we propose a block skipping technique that omits computations and memory transfers for selected sub-blocks within the low-rank decomposition. These two techniques jointly enable our compressed model to retain more effective ranks under the same compression budget. Experimental results show that, without any additional fine-tuning, our method outperforms previous low-rank compression approaches by 7% accuracy improvement on zero-shot tasks under the same compression rate. These results highlight the effectiveness of our rank-maximized compression strategy in preserving model performance under tight resource constraints.

</details>


### [57] [PrahokBART: A Pre-trained Sequence-to-Sequence Model for Khmer Natural Language Generation](https://arxiv.org/abs/2512.13552)
*Hour Kaing,Raj Dabre,Haiyue Song,Van-Hien Tran,Hideki Tanaka,Masao Utiyama*

Main category: cs.CL

TL;DR: PrahokBART是一种针对高棉语的新型序列生成预训练模型，在多个生成任务中超过了多语种模型mBART50。


<details>
  <summary>Details</summary>
Motivation: 现有多语种预训练模型忽视了高棉语的语言特性和语料质量，导致模型表现不佳。

Method: 从零开始训练PrahokBART，使用经过精心筛选的高棉语和英语语料，并结合了语言学模块如分词和归一化。

Result: PrahokBART在机器翻译、文本摘要和标题生成三大任务上均优于mBART50。

Conclusion: 通过引入语言学特征和高质量语料，PrahokBART显著提升了高棉语文本生成的自然度和效果。

Abstract: This work introduces {\it PrahokBART}, a compact pre-trained sequence-to-sequence model trained from scratch for Khmer using carefully curated Khmer and English corpora. We focus on improving the pre-training corpus quality and addressing the linguistic issues of Khmer, which are ignored in existing multilingual models, by incorporating linguistic components such as word segmentation and normalization. We evaluate PrahokBART on three generative tasks: machine translation, text summarization, and headline generation, where our results demonstrate that it outperforms mBART50, a strong multilingual pre-trained model. Additionally, our analysis provides insights into the impact of each linguistic module and evaluates how effectively our model handles space during text generation, which is crucial for the naturalness of texts in Khmer.

</details>


### [58] [Verifying Rumors via Stance-Aware Structural Modeling](https://arxiv.org/abs/2512.13559)
*Gibson Nkhata,Uttamasha Anjally Oyshi,Quan Mai,Susan Gauch*

Main category: cs.CL

TL;DR: 本文提出了一种结合立场信息和对话结构的谣言验证方法，通过编码每个帖子及其立场，聚合回复嵌入，有效提升了谣言真实性预测的性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以同时捕捉语义内容、立场信息及对话结构，特别是在基于Transformer编码器的序列长度限制下，影响谣言验证的效果。

Method: 提出了立场感知结构建模，通过对话中每个帖子的立场信号编码，并按立场类别聚合回复嵌入，同时引入立场分布和层级深度作为协变量，捕捉立场不平衡和回复层级影响。

Result: 在基准数据集上进行大量实验，结果显示该方法在预测谣言真实性方面显著优于现有方法，同时具备早期检测和跨平台泛化能力。

Conclusion: 结合立场信号和对话结构的深度建模方法能有效提升社交媒体谣言验证的准确性和适用范围。

Abstract: Verifying rumors on social media is critical for mitigating the spread of false information. The stances of conversation replies often provide important cues to determine a rumor's veracity. However, existing models struggle to jointly capture semantic content, stance information, and conversation strructure, especially under the sequence length constraints of transformer-based encoders. In this work, we propose a stance-aware structural modeling that encodes each post in a discourse with its stance signal and aggregates reply embedddings by stance category enabling a scalable and semantically enriched representation of the entire thread. To enhance structural awareness, we introduce stance distribution and hierarchical depth as covariates, capturing stance imbalance and the influence of reply depth. Extensive experiments on benchmark datasets demonstrate that our approach significantly outperforms prior methods in the ability to predict truthfulness of a rumor. We also demonstrate that our model is versatile for early detection and cross-platfrom generalization.

</details>


### [59] [Memory in the Age of AI Agents](https://arxiv.org/abs/2512.13564)
*Yuyang Hu,Shichun Liu,Yanwei Yue,Guibin Zhang,Boyang Liu,Fangyi Zhu,Jiahang Lin,Honglin Guo,Shihan Dou,Zhiheng Xi,Senjie Jin,Jiejun Tan,Yanbin Yin,Jiongnan Liu,Zeyu Zhang,Zhongxiang Sun,Yutao Zhu,Hao Sun,Boci Peng,Zhenrong Cheng,Xuanbo Fan,Jiaxin Guo,Xinlei Yu,Zhenhong Zhou,Zewen Hu,Jiahao Huo,Junhao Wang,Yuwei Niu,Yu Wang,Zhenfei Yin,Xiaobin Hu,Yue Liao,Qiankun Li,Kun Wang,Wangchunshu Zhou,Yixin Liu,Dawei Cheng,Qi Zhang,Tao Gui,Shirui Pan,Yan Zhang,Philip Torr,Zhicheng Dou,Ji-Rong Wen,Xuanjing Huang,Yu-Gang Jiang,Shuicheng Yan*

Main category: cs.CL

TL;DR: 本文综述了基于基础模型的智能体记忆系统，梳理了记忆的定义、分类和动态演变，并总结了相关基准与框架，提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 领域内关于智能体记忆的研究日益增多但分散且术语模糊，传统分类难以涵盖现有多样化系统，因而需要统一的系统性梳理。

Method: 通过明确智能体记忆范围，区分相关概念，从记忆形式（token级、参数化、潜在）、功能（事实、经验、工作）和动态（记忆形成、演化与提取）三个维度进行分类和分析；整理现有基准和开源框架；探讨未来发展方向。

Result: 提出了智能体记忆的三大形式和三类功能的细化分类，系统总结了记忆的动态特征和相关资源，构建了统一的理论视角。

Conclusion: 该综述为智能体记忆研究提供了清晰的概念框架和分类体系，促进领域整合，并指明未来研究趋势，助力智能体设计中记忆成为核心原语。

Abstract: Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing works that fall under the umbrella of agent memory often differ substantially in their motivations, implementations, and evaluation protocols, while the proliferation of loosely defined memory terminologies has further obscured conceptual clarity. Traditional taxonomies such as long/short-term memory have proven insufficient to capture the diversity of contemporary agent memory systems. This work aims to provide an up-to-date landscape of current agent memory research. We begin by clearly delineating the scope of agent memory and distinguishing it from related concepts such as LLM memory, retrieval augmented generation (RAG), and context engineering. We then examine agent memory through the unified lenses of forms, functions, and dynamics. From the perspective of forms, we identify three dominant realizations of agent memory, namely token-level, parametric, and latent memory. From the perspective of functions, we propose a finer-grained taxonomy that distinguishes factual, experiential, and working memory. From the perspective of dynamics, we analyze how memory is formed, evolved, and retrieved over time. To support practical development, we compile a comprehensive summary of memory benchmarks and open-source frameworks. Beyond consolidation, we articulate a forward-looking perspective on emerging research frontiers, including memory automation, reinforcement learning integration, multimodal memory, multi-agent memory, and trustworthiness issues. We hope this survey serves not only as a reference for existing work, but also as a conceptual foundation for rethinking memory as a first-class primitive in the design of future agentic intelligence.

</details>


### [60] [ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding](https://arxiv.org/abs/2512.13586)
*Jia-Nan Li,Jian Guan,Wei Wu,Chongxuan Li*

Main category: cs.CL

TL;DR: ReFusion是一种改进的掩码扩散模型，通过将解码从单个标记提升到固定长度的槽，实现了更高效的并行推断和更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的掩码扩散模型虽然支持并行推断，但存在计算开销大且生成不连贯的问题，限制了其性能提升。

Method: 提出ReFusion模型，通过一个基于扩散的规划步骤识别弱依赖的槽，接着以自回归方式并行填充这些槽，从而实现槽级别的并行解码，充分利用KV缓存并降低学习复杂度。

Result: 在七个不同的基准测试中，ReFusion模型在性能上平均提升34%，比以往的掩码扩散模型速度提高18倍，并在保持2.33倍速度提升的同时，缩小了与自回归模型的性能差距。

Conclusion: ReFusion通过槽级别的掩码扩散和计划填充策略，有效解决了并行推断中的效率与性能瓶颈，成为连接自回归模型和掩码扩散模型的高效桥梁。

Abstract: Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18$\times$ speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33$\times$ average speedup.

</details>


### [61] [Textual Gradients are a Flawed Metaphor for Automatic Prompt Optimization](https://arxiv.org/abs/2512.13598)
*Daniel Melcer,Qi Chen,Wen-Hao Chiang,Shweta Garg,Pranav Garg,Christian Bock*

Main category: cs.CL

TL;DR: 自动提示优化技术通过类比文本梯度提高大语言模型性能，但实验表明梯度类比解释不足。


<details>
  <summary>Details</summary>
Motivation: 提升大语言模型性能，减少人工调优提示词的需求。

Method: 通过一系列实验和案例分析文本梯度方法的行为。

Result: 发现虽然文本梯度方法能提升性能，但其梯度类比解释不准确。

Conclusion: 为选择和开发提示优化方法提供新的见解，促进更有效的提示优化策略发展。

Abstract: A well-engineered prompt can increase the performance of large language models; automatic prompt optimization techniques aim to increase performance without requiring human effort to tune the prompts. One leading class of prompt optimization techniques introduces the analogy of textual gradients. We investigate the behavior of these textual gradient methods through a series of experiments and case studies. While such methods often result in a performance improvement, our experiments suggest that the gradient analogy does not accurately explain their behavior. Our insights may inform the selection of prompt optimization strategies, and development of new approaches.

</details>


### [62] [Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models](https://arxiv.org/abs/2512.13607)
*Boxin Wang,Chankyu Lee,Nayeon Lee,Sheng-Chieh Lin,Wenliang Dai,Yang Chen,Yangyi Chen,Zhuolin Yang,Zihan Liu,Mohammad Shoeybi,Bryan Catanzaro,Wei Ping*

Main category: cs.CL

TL;DR: 本文提出了级联领域强化学习（Cascade RL）方法，用于构建通用推理模型Nemotron-Cascade，通过按领域顺序训练，降低跨领域异质性带来的复杂性，实现了性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法在处理跨领域推理时面临响应长度和验证时延大幅变化，导致训练复杂且效率低下，难以设计训练课程和调整超参数。

Method: Cascade RL通过顺序执行领域特定的强化学习，避免混合多领域异质数据，提高训练效率和模型性能。先进行RLHF对齐预训练，随后进行领域级强化学习以增强推理能力。

Result: 14B参数模型经过Cascade RL训练后，在多个基准测试上表现出色，优于其SFT教师模型DeepSeek-R1-0528，并在2025年国际信息学奥林匹克中获得银牌成绩。

Conclusion: 级联领域强化学习有效解决了跨领域推理中的异质性问题，提升了模型的推理能力和训练效率，为构建通用推理模型提供了新思路。

Abstract: Building general-purpose reasoning models with reinforcement learning (RL) entails substantial cross-domain heterogeneity, including large variation in inference-time response lengths and verification latency. Such variability complicates the RL infrastructure, slows training, and makes training curriculum (e.g., response length extension) and hyperparameter selection challenging. In this work, we propose cascaded domain-wise reinforcement learning (Cascade RL) to develop general-purpose reasoning models, Nemotron-Cascade, capable of operating in both instruct and deep thinking modes. Departing from conventional approaches that blend heterogeneous prompts from different domains, Cascade RL orchestrates sequential, domain-wise RL, reducing engineering complexity and delivering state-of-the-art performance across a wide range of benchmarks. Notably, RLHF for alignment, when used as a pre-step, boosts the model's reasoning ability far beyond mere preference optimization, and subsequent domain-wise RLVR stages rarely degrade the benchmark performance attained in earlier domains and may even improve it (see an illustration in Figure 1). Our 14B model, after RL, outperforms its SFT teacher, DeepSeek-R1-0528, on LiveCodeBench v5/v6/Pro and achieves silver-medal performance in the 2025 International Olympiad in Informatics (IOI). We transparently share our training and data recipes.

</details>


### [63] [Temporal Tokenization Strategies for Event Sequence Modeling with Large Language Models](https://arxiv.org/abs/2512.13618)
*Zefang Liu,Nam Nguyen,Yinzhu Quan,Austin Zhang*

Main category: cs.CL

TL;DR: 该论文首次系统地比较了多种时间序列事件的时间编码策略，发现没有单一最优方案，适应数据统计特性才是关键。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在处理连续时间表示时存在挑战，且现有方法在面对不同数据分布时表现不一，缺乏系统比较。

Method: 对比了五种时间序列编码策略，包括数字字符串、高精度字节编码、语义日历标记、均匀分箱和自适应量化，并在多种真实数据集上微调LLM进行评估。

Result: 研究发现不同策略对不同统计分布的数据表现不同，基于对数的编码在偏态分布中效果较好，语义日历标记对混合模态数据表现稳定。

Conclusion: 时间序列的时间编码选择应依据数据统计特性，没有单一普适最佳方案，强调了对数据分布的适配性。

Abstract: Representing continuous time is a critical and under-explored challenge in modeling temporal event sequences with large language models (LLMs). Various strategies like byte-level representations or calendar tokens have been proposed. However, the optimal approach remains unclear, especially given the diverse statistical distributions of real-world event data, which range from smooth log-normal to discrete, spiky patterns. This paper presents the first empirical study of temporal tokenization for event sequences, comparing distinct encoding strategies: naive numeric strings, high-precision byte-level representations, human-semantic calendar tokens, classic uniform binning, and adaptive residual scalar quantization. We evaluate these strategies by fine-tuning LLMs on real-world datasets that exemplify these diverse distributions. Our analysis reveals that no single strategy is universally superior; instead, prediction performance depends heavily on aligning the tokenizer with the data's statistical properties, with log-based strategies excelling on skewed distributions and human-centric formats proving robust for mixed modalities.

</details>


### [64] [Large-Language Memorization During the Classification of United States Supreme Court Cases](https://arxiv.org/abs/2512.13654)
*John E. Ortega,Dhruv D. Joshi,Matt P. Borkowski*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）在复杂法律文本分类任务中的表现，发现基于记忆和提示的模型在美国最高法院判决分类任务上优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 针对大型语言模型在非问答类分类任务中输出“幻觉”现象，探究其记忆策略与响应机制，尤其在复杂法律文本分类中表现如何。

Method: 利用美国最高法院判决文本，结合最新的LLM微调和检索方法（如参数高效微调、自动建模等），在两个传统分类任务（15类和279类）上进行实验，比较基于提示的模型（如DeepSeek）与传统BERT模型的性能。

Result: 提示加记忆的模型在两个分类任务中均表现更好，测试中比传统BERT模型提高约2个百分点的准确率，显示出更强的鲁棒性。

Conclusion: 基于提示和记忆的LLM在复杂法律文本分类任务中表现优异，优于传统模型，提示了新的模型设计方向和应用潜力。

Abstract: Large-language models (LLMs) have been shown to respond in a variety of ways for classification tasks outside of question-answering. LLM responses are sometimes called "hallucinations" since the output is not what is ex pected. Memorization strategies in LLMs are being studied in detail, with the goal of understanding how LLMs respond. We perform a deep dive into a classification task based on United States Supreme Court (SCOTUS) decisions. The SCOTUS corpus is an ideal classification task to study for LLM memory accuracy because it presents significant challenges due to extensive sentence length, complex legal terminology, non-standard structure, and domain-specific vocabulary. Experimentation is performed with the latest LLM fine tuning and retrieval-based approaches, such as parameter-efficient fine-tuning, auto-modeling, and others, on two traditional category-based SCOTUS classification tasks: one with 15 labeled topics and another with 279. We show that prompt-based models with memories, such as DeepSeek, can be more robust than previous BERT-based models on both tasks scoring about 2 points better than previous models not based on prompting.

</details>


### [65] [A stylometric analysis of speaker attribution from speech transcripts](https://arxiv.org/abs/2512.13667)
*Cristina Aggazzotti,Elizabeth Allyn Smith*

Main category: cs.CL

TL;DR: 本研究提出了一种基于文体学特征的说话人归属方法，针对语音转文本数据进行说话人识别，并与神经网络方法进行了比较。


<details>
  <summary>Details</summary>
Motivation: 鉴于传统语音识别方法在说话人使用语音变声或语音合成时失效，本文探讨利用说话内容的语言特征用于说话人识别的可能性。

Method: 提出了StyloSpeaker方法，融合字符、词汇、句法及风格特征，分别在有无标点及大小写两种文本格式上进行说话人归属测试，并通过控制话题一致性评估性能。

Result: 在去除标点及大小写的规范化文本上归属表现较好，最强的话题控制条件下性能最高，且该文体学方法相比黑箱神经网络更具解释性，能够识别出区分说话人的关键风格特征。

Conclusion: 基于文本内容的文体分析方法在说话人识别中有效，尤其当语音特征不可用时，可作为传统语音识别的有力补充。

Abstract: Forensic scientists often need to identify an unknown speaker or writer in cases such as ransom calls, covert recordings, alleged suicide notes, or anonymous online communications, among many others. Speaker recognition in the speech domain usually examines phonetic or acoustic properties of a voice, and these methods can be accurate and robust under certain conditions. However, if a speaker disguises their voice or employs text-to-speech software, vocal properties may no longer be reliable, leaving only their linguistic content available for analysis. Authorship attribution methods traditionally use syntactic, semantic, and related linguistic information to identify writers of written text (authorship attribution). In this paper, we apply a content-based authorship approach to speech that has been transcribed into text, using what a speaker says to attribute speech to individuals (speaker attribution). We introduce a stylometric method, StyloSpeaker, which incorporates character, word, token, sentence, and style features from the stylometric literature on authorship, to assess whether two transcripts were produced by the same speaker. We evaluate this method on two types of transcript formatting: one approximating prescriptive written text with capitalization and punctuation and another normalized style that removes these conventions. The transcripts' conversation topics are also controlled to varying degrees. We find generally higher attribution performance on normalized transcripts, except under the strongest topic control condition, in which overall performance is highest. Finally, we compare this more explainable stylometric model to black-box neural approaches on the same data and investigate which stylistic features most effectively distinguish speakers.

</details>


### [66] [Towards Effective Model Editing for LLM Personalization](https://arxiv.org/abs/2512.13676)
*Baixiang Huang,Limeng Cui,Jiapeng Liu,Haoran Wang,Jiawei Xu,Zhuiyue Tan,Yutong Chen,Chen Luo,Yi Liu,Kai Shu*

Main category: cs.CL

TL;DR: 本文提出了个人化编辑框架，通过局部模型编辑实现对用户偏好的精准调整，解决了计算成本高、数据需求大和性能下降问题，并引入了新的评测数据集UPQA来衡量模型对用户偏好的记忆和应用能力。


<details>
  <summary>Details</summary>
Motivation: 现有大模型个人化方法存在计算开销大、易遗忘以及在多轮对话和隐式查询中性能下降等问题，且缺乏能有效评估模型对用户偏好准确记忆的基准。

Method: 将个人化视为模型编辑任务，提出Personalization Editing框架，利用聚类偏好表示引导局部编辑，保证个性化调整精准且不影响模型整体能力，并构建了基于用户实际查询的UPQA数据集进行评价。

Result: 实验结果表明，Personalization Editing在编辑准确性和计算效率上优于微调方法，在多轮对话及隐式偏好提问情景下表现超出基于提示的基线模型。

Conclusion: Personalization Editing框架有效解决了现有个人化方法的不足，实现了高效精准的用户偏好模型更新，UPQA数据集弥补了个人化评测的空白，为个性化研究提供了新的工具和方法。

Abstract: Personalization is becoming indispensable for LLMs to align with individual user preferences and needs. Yet current approaches are often computationally expensive, data-intensive, susceptible to catastrophic forgetting, and prone to performance degradation in multi-turn interactions or when handling implicit queries. To address these challenges, we conceptualize personalization as a model editing task and introduce Personalization Editing, a framework that applies localized edits guided by clustered preference representations. This design enables precise preference-aligned updates while preserving overall model capabilities. In addition, existing personalization benchmarks frequently rely on persona-based dialogs between LLMs rather than user-LLM interactions, or focus primarily on stylistic imitation while neglecting information-seeking tasks that require accurate recall of user-specific preferences. We introduce User Preference Question Answering (UPQA), a short-answer QA dataset constructed from in-situ user queries with varying levels of difficulty. Unlike prior benchmarks, UPQA directly evaluates a model's ability to recall and apply specific user preferences. Across experimental settings, Personalization Editing achieves higher editing accuracy and greater computational efficiency than fine-tuning, while outperforming prompting-based baselines in multi-turn conversations and implicit preference questions settings.

</details>


### [67] [Beyond surface form: A pipeline for semantic analysis in Alzheimer's Disease detection from spontaneous speech](https://arxiv.org/abs/2512.13685)
*Dylan Phelps,Rodrigo Wilkens,Edward Gow-Smith,Lilian Hubner,Bárbara Malcorra,César Rennó-Costa,Marco Idiart,Maria-Cruz Villa-Uriol,Aline Villavicencio*

Main category: cs.CL

TL;DR: 本文研究了阿尔茨海默病（AD）语言模型分类中的语义和表层文本特征，提出通过语法和词汇变换保持语义不变，检测模型对真实语义标志的识别能力，并发现语义信息足以支持AD检测。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型虽能辅助AD筛查，但缺乏可解释性，难以区分认知衰退的真实语言标记与表层文本模式。研究旨在验证模型能否捕捉到底层语义指标，而非仅依赖表层形式。

Method: 通过对文本进行语法及词汇变换，保持语义不变，构建变换后文本数据，利用低BLEU和chrF评分验证结构变化，同时用高语义相似度确认语义保留。比对原文与变换文的分类性能差异，测试模型对语义信息的利用。并用生成模型检验图片描述语言对原始图像的还原能力。

Result: 变换文本结构和词汇后，分类性能与使用原始文本相近，宏观F1仅有微小偏差，表明模型依赖语义信息。图像基变换引入噪声，导致准确率下降。该方法有效剔除虚假相关性，强调语义信息对AD检测的重要性。

Conclusion: 研究表明基于语义信息的语言模型分类器能有效检测AD，实现了对难以察觉的语义障碍的识别，弥补了语言退化研究中的不足，推动了早期AD筛查技术的发展。

Abstract: Alzheimer's Disease (AD) is a progressive neurodegenerative condition that adversely affects cognitive abilities. Language-related changes can be automatically identified through the analysis of outputs from linguistic assessment tasks, such as picture description. Language models show promise as a basis for screening tools for AD, but their limited interpretability poses a challenge in distinguishing true linguistic markers of cognitive decline from surface-level textual patterns. To address this issue, we examine how surface form variation affects classification performance, with the goal of assessing the ability of language models to represent underlying semantic indicators. We introduce a novel approach where texts surface forms are transformed by altering syntax and vocabulary while preserving semantic content. The transformations significantly modify the structure and lexical content, as indicated by low BLEU and chrF scores, yet retain the underlying semantics, as reflected in high semantic similarity scores, isolating the effect of semantic information, and finding models perform similarly to if they were using the original text, with only small deviations in macro-F1. We also investigate whether language from picture descriptions retains enough detail to reconstruct the original image using generative models. We found that image-based transformations add substantial noise reducing classification accuracy. Our methodology provides a novel way of looking at what features influence model predictions, and allows the removal of possible spurious correlations. We find that just using semantic information, language model based classifiers can still detect AD. This work shows that difficult to detect semantic impairment can be identified, addressing an overlooked feature of linguistic deterioration, and opening new pathways for early detection systems.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [68] [How AI Agents Follow the Herd of AI? Network Effects, History, and Machine Optimism](https://arxiv.org/abs/2512.11943)
*Yu Liu,Wenwen Li,Yifan Dou,Guangnan Ye*

Main category: cs.MA

TL;DR: 本研究探讨了多AI代理在网络效应游戏中的决策行为，发现历史数据和其时间序列结构对AI代理的均衡推断至关重要，弱网络效应下可部分收敛，强网络效应下则出现AI乐观偏差，随机历史数据则导致收敛失败。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统中网络效应游戏的战略互动虽广泛存在，但尚未得到深入研究，特别是AI代理如何基于历史信息决策的机制尚不明确。

Method: 设计基于大型语言模型（LLM）的AI代理，模拟重复决策过程，系统操控价格轨迹（固定、上升、下降、随机）和网络效应强度，分析其对AI决策与均衡推断的影响。

Result: 无历史数据时AI代理无法推断均衡；有序历史数据在弱网络效应下帮助部分收敛，而强网络效应导致AI过度乐观；随机历史数据则完全破坏收敛，显示时间连贯性对LLM推理的重要性。

Conclusion: AI系统中均衡结果不仅依赖激励机制，还深受历史数据生成方式的影响，这一特征区别于人类决策，揭示了AI介导系统策略互动的新范式。

Abstract: Understanding decision-making in multi-AI-agent frameworks is crucial for analyzing strategic interactions in network-effect-driven contexts. This study investigates how AI agents navigate network-effect games, where individual payoffs depend on peer participatio--a context underexplored in multi-agent systems despite its real-world prevalence. We introduce a novel workflow design using large language model (LLM)-based agents in repeated decision-making scenarios, systematically manipulating price trajectories (fixed, ascending, descending, random) and network-effect strength. Our key findings include: First, without historical data, agents fail to infer equilibrium. Second, ordered historical sequences (e.g., escalating prices) enable partial convergence under weak network effects but strong effects trigger persistent "AI optimism"--agents overestimate participation despite contradictory evidence. Third, randomized history disrupts convergence entirely, demonstrating that temporal coherence in data shapes LLMs' reasoning, unlike humans. These results highlight a paradigm shift: in AI-mediated systems, equilibrium outcomes depend not just on incentives, but on how history is curated, which is impossible for human.

</details>


### [69] [Beyond Task Completion: An Assessment Framework for Evaluating Agentic AI Systems](https://arxiv.org/abs/2512.12791)
*Sreemaee Akshathala,Bassam Adnan,Mahisha Ramesh,Karthik Vaidhyanathan,Basil Muhammed,Kannan Parthasarathy*

Main category: cs.MA

TL;DR: 本文提出了一个端到端的智能体评估框架，解决了多智能体系统中行为不确定性评估难题。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统评估存在忽视模型非确定性导致的行为不确定性问题，传统二元完成指标不足以反映系统实际表现。

Method: 设计涵盖大语言模型、记忆、工具和环境四个评估支柱的综合评估框架，并在自动化云运维案例中进行了验证。

Result: 实验发现传统指标未能捕捉到的行为偏差，新框架有效反映了运行时的不确定性。

Conclusion: 该框架有效提升了多智能体系统的评估精度，为智能体系统的性能评估提供了更全面的方法。

Abstract: Recent advances in agentic AI have shifted the focus from standalone Large Language Models (LLMs) to integrated systems that combine LLMs with tools, memory, and other agents to perform complex tasks. These multi-agent architectures enable coordinated reasoning, planning, and execution across diverse domains, allowing agents to collaboratively automate complex workflows. Despite these advances, evaluation and assessment of LLM agents and the multi-agent systems they constitute remain a fundamental challenge. Although various approaches have been proposed in the software engineering literature for evaluating conventional software components, existing methods for AI-based systems often overlook the non-deterministic nature of models. This non-determinism introduces behavioral uncertainty during execution, yet existing evaluations rely on binary task completion metrics that fail to capture it. Evaluating agentic systems therefore requires examining additional dimensions, including the agent ability to invoke tools, ingest and retrieve memory, collaborate with other agents, and interact effectively with its environment. We propose an end-to-end Agent Assessment Framework with four evaluation pillars encompassing LLMs, Memory, Tools, and Environment. We validate the framework on a representative Autonomous CloudOps use case, where experiments reveal behavioral deviations overlooked by conventional metrics, demonstrating its effectiveness in capturing runtime uncertainties.

</details>


### [70] [Quantigence: A Multi-Agent AI Framework for Quantum Security Research](https://arxiv.org/abs/2512.12989)
*Abdulmalik Alquwayfili*

Main category: cs.MA

TL;DR: 本论文提出了Quantigence，一个多智能体AI框架，用于结构化的量子安全分析，显著提高了研究效率和覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 随着密码学相关量子计算机（CRQCs）的威胁日益明朗，现有公钥基础设施面临安全风险，急需迁移至后量子密码学，但研究速度快、标准不断演变，且部署环境复杂，导致过渡困难。

Method: Quantigence通过多智能体分工（密码分析师、威胁建模者、标准专家和风险评估者）协同工作，借助认知并行保持各角色独立推理，结合外部知识和新型量子风险评分（QARS）实现系统化分析。

Result: 实验证明Quantigence能够将研究周转时间缩短67%，且在文献覆盖率上优于传统手工流程。

Conclusion: Quantigence有效推动了高精度量子风险评估的民主化，为加速后量子安全技术的研发和部署提供了有力支撑。

Abstract: Cryptographically Relevant Quantum Computers (CRQCs) pose a structural threat to the global digital economy. Algorithms like Shor's factoring and Grover's search threaten to dismantle the public-key infrastructure (PKI) securing sovereign communications and financial transactions. While the timeline for fault-tolerant CRQCs remains probabilistic, the "Store-Now, Decrypt-Later" (SNDL) model necessitates immediate migration to Post-Quantum Cryptography (PQC). This transition is hindered by the velocity of research, evolving NIST standards, and heterogeneous deployment environments. To address this, we present Quantigence, a theory-driven multi-agent AI framework for structured quantum-security analysis. Quantigence decomposes research objectives into specialized roles - Cryptographic Analyst, Threat Modeler, Standards Specialist, and Risk Assessor - coordinated by a supervisory agent. Using "cognitive parallelism," agents reason independently to maintain context purity while execution is serialized on resource-constrained hardware (e.g., NVIDIA RTX 2060). The framework integrates external knowledge via the Model Context Protocol (MCP) and prioritizes vulnerabilities using the Quantum-Adjusted Risk Score (QARS), a formal extension of Mosca's Theorem. Empirical validation shows Quantigence achieves a 67% reduction in research turnaround time and superior literature coverage compared to manual workflows, democratizing access to high-fidelity quantum risk assessment.

</details>


### [71] [The Optimal Control Algorithm of Connected and Automated Vehicles at Roundabouts with Communication Delay](https://arxiv.org/abs/2512.13056)
*Chen Huang,Ronghui Hou*

Main category: cs.MA

TL;DR: 本文提出了一种考虑时间延迟和信息不确定性的环岛交互控制算法，通过分布式模型预测控制和多尺度优化，实现了车辆安全稳定进入环岛。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆依赖无线通信进行分布式控制，通信延迟会影响车辆运动和控制性能，尤其是在高速度和复杂路口（如环岛）环境下。

Method: 本文基于车辆间碰撞时间(TTC)识别冲突车辆，建立包含时间延迟的车辆运动模型，采用分布式模型预测控制确定运动控制，并通过调度车辆进入环岛的顺序，构建融合车辆指标与系统指标的多尺度优化目标，嵌入交通密度和行程时间以保证安全稳定。

Result: 通过多种仿真实验，验证了所提控制算法在不同自动驾驶车辆渗透率和交通负荷场景下，相较多种控制算法的性能优势。

Conclusion: 本文提出的控制算法有效解决了环岛环境中由时间延迟引起的信息不确定性问题，实现了车辆的安全、稳定运行，提升了环岛的整体交通效率和控制性能。

Abstract: Connected and automated vehicles (CAVs) rely on wireless communication to exchange state information for distributed control, making communication delays a critical factor that can affect vehicle motion and degrade control performance, particularly in high-speed scenarios. To address these challenges in the complex environment of roundabout intersections, this paper proposes a roundabout control algorithm, which takes into account the uncertainty of interactive information caused by time delays. First, to maintain the required distance between the current vehicle and its preceding and following vehicles, conflicting vehicles are identified based on the time-to-collision (TTC) in the conflict zone. To fully consider communication performance, a vehicle motion model incorporating time delays is established. According to the distributed model predictive control (DMPC) mechanism, the vehicle motion control that satisfies the roundabout constraints is determined. Second, by scheduling the sequence of vehicles entering the roundabout, a multiscale optimization objective is developed by integrating vehicle motion indicators and roundabout system indicators. Traffic density and travel time are embedded into the optimization problem to guide vehicles to enter the roundabout safely and stably. Through a variety of simulation experiments, the effectiveness of the proposed control algorithm is verified by comparing its performance with that of multiple control algorithms under different autonomous vehicle penetration rates and heavy traffic load scenarios.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [72] [Vibe Coding in Practice: Flow, Technical Debt, and Guidelines for Sustainable Use](https://arxiv.org/abs/2512.11922)
*Muhammad Waseem,Aakash Ahmad,Kai-Kristian Kemell,Jussi Rasku,Sami Lahti,Kalle Mäkelä,Pekka Abrahamsson*

Main category: cs.SE

TL;DR: 本文分析了生成式AI辅助的软件开发模式——Vibe Coding（VC）的流畅编码与技术债务之间的权衡，指出其在快速开发MVP时带来的风险。


<details>
  <summary>Details</summary>
Motivation: 当前VC虽然能快速生成代码，推动快速开发，但在软件开发生命周期中引入了技术债务，带来架构不一致、安全漏洞及维护负担，亟需探讨其根源及解决方案。

Method: 基于作者内部多个MVP项目经验及行业报告，分析VC流程债务产生原因，包括流程漏洞、模型训练偏差、缺乏设计理由及快速编码优先等，进一步解释当前模型、平台、硬件限制如何加剧问题，并提出相应对策。

Result: 发现VC快速生成代码虽提高效率，但同时带来架构不一致、安全风险及维护成本上升，这些问题由多方面因素共同作用导致，目前技术限制使得问题难以根除。

Conclusion: 为了实现可持续的VC，应关注流程改进、模型优化及硬件升级，采取综合对策减少技术债务积累，推动VC在实际软件工程中的健康发展。

Abstract: Vibe Coding (VC) is a form of software development assisted by generative AI, in which developers describe the intended functionality or logic via natural language prompts, and the AI system generates the corresponding source code. VC can be leveraged for rapid prototyping or developing the Minimum Viable Products (MVPs); however, it may introduce several risks throughout the software development life cycle. Based on our experience from several internally developed MVPs and a review of recent industry reports, this article analyzes the flow-debt tradeoffs associated with VC. The flow-debt trade-off arises when the seamless code generation occurs, leading to the accumulation of technical debt through architectural inconsistencies, security vulnerabilities, and increased maintenance overhead. These issues originate from process-level weaknesses, biases in model training data, a lack of explicit design rationale, and a tendency to prioritize quick code generation over human-driven iterative development. Based on our experiences, we identify and explain how current model, platform, and hardware limitations contribute to these issues, and propose countermeasures to address them, informing research and practice towards more sustainable VC approaches.

</details>


### [73] [A Systematic Mapping Study on Risks and Vulnerabilities in Software Containers](https://arxiv.org/abs/2512.11940)
*Maha Sroor,Teerath Das,Rahul Mohanani,Tommi Mikkonen*

Main category: cs.SE

TL;DR: 本文通过系统映射研究，整理了容器系统中存在的安全风险、漏洞及缓解方法，旨在提升容器安全水平。


<details>
  <summary>Details</summary>
Motivation: 当前软件容器广泛应用，但在开发与部署过程中存在显著的安全隐患，且缺乏系统性总结和分类的安全知识。

Method: 基于129篇主研究文献进行了系统映射研究，识别并分类容器生命周期中的关键风险和漏洞，梳理其成因、影响及缓解技术，并汇总相关安全实践与工具。

Result: 构建了关于容器安全问题的全新分类体系，明确了风险成因及后果，提供了具体的缓解措施和实践工具的汇总。

Conclusion: 本研究为理解软件容器安全现状提供了关键见解，呼吁未来研究重点加强安全防护措施和有效缓解策略的开发。

Abstract: Software containers are widely adopted for developing and deploying software applications. Despite their popularity, major security concerns arise during container development and deployment. Software Engineering (SE) research literature reveals a lack of reviewed, aggregated, and organized knowledge of risks, vulnerabilities, security practices, and tools in container-based systems development and deployment. Therefore, we conducted a Systematic Mapping Study (SMS) based on 129 selected primary studies to explore and organize existing knowledge on security issues in software container systems. Data from the primary studies enabled us to identify critical risks and vulnerabilities across the container life-cycle and categorize them using a novel taxonomy. Additionally, the findings highlight the causes and implications and provide a list of mitigation techniques to overcome these risks and vulnerabilities. Furthermore, we provide an aggregation of security practices and tools that can help support and improve the overall security of container systems. This study offers critical insights into the current landscape of security issues within software container systems. Our analysis highlights the need for future SE research to focus on security enhancement practices that strengthen container systems and develop effective mitigation strategies to comprehensively address existing risks and vulnerabilities.

</details>


### [74] [Evidence-Driven Decision Support for AI Model Selection in Research Software Engineering](https://arxiv.org/abs/2512.11984)
*Alireza Joonbakhsh,Alireza Rostami,AmirMohammad Kamalinia,Ali Nazeri,Farshad Khunjush,Bedir Tekinerdogan,Siamak Farshidi*

Main category: cs.SE

TL;DR: 本文提出了一种基于多标准决策的AI模型选择框架ModelSelect，通过结构化知识图谱和自动化数据收集，实现透明、可复现的模型推荐，提升研究软件工程中的决策质量。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型选择过程多依赖碎片化元数据和个人经验，缺乏系统性支持，影响可复现性和研究软件质量。

Method: 将AI模型选择问题建模为多标准决策问题，设计并实现了集成自动数据收集、结构化知识图谱和多标准决策原则的证据驱动决策支持框架ModelSelect，采用设计科学研究方法进行验证。

Result: 通过50个真实案例和与领先生成式AI系统的比较实验，ModelSelect在模型和库推荐任务中表现出高覆盖率、强合理性一致性及良好追踪性，推荐结果与专家推理高度契合。

Conclusion: 将AI模型选择问题系统化为多标准决策问题，为研究软件工程中的透明、可重复决策支持奠定基础，ModelSelect框架实现了证据集成与可解释推荐，提升了决策的质量和稳健性。

Abstract: The rapid proliferation of artificial intelligence (AI) models and methods presents growing challenges for research software engineers and researchers who must select, integrate, and maintain appropriate models within complex research workflows. Model selection is often performed in an ad hoc manner, relying on fragmented metadata and individual expertise, which can undermine reproducibility, transparency, and overall research software quality.
  This work proposes a structured and evidence-driven approach to support AI model selection that aligns with both technical and contextual requirements. We conceptualize AI model selection as a Multi-Criteria Decision-Making (MCDM) problem and introduce an evidence-based decision-support framework that integrates automated data collection pipelines, a structured knowledge graph, and MCDM principles. Following the Design Science Research methodology, the proposed framework (ModelSelect) is empirically validated through 50 real-world case studies and comparative experiments against leading generative AI systems.
  The evaluation results show that ModelSelect produces reliable, interpretable, and reproducible recommendations that closely align with expert reasoning. Across the case studies, the framework achieved high coverage and strong rationale alignment in both model and library recommendation tasks, performing comparably to generative AI assistants while offering superior traceability and consistency.
  By framing AI model selection as an MCDM problem, this work establishes a rigorous foundation for transparent and reproducible decision support in research software engineering. The proposed framework provides a scalable and explainable pathway for integrating empirical evidence into AI model recommendation processes, ultimately improving the quality and robustness of research software decision-making.

</details>


### [75] [Re-opening open-source science through AI assisted development](https://arxiv.org/abs/2512.11993)
*Ling-Hong Hung,Ka Yee Yeung*

Main category: cs.SE

TL;DR: 研究展示了由单个人类领导的自主AI团队迅速修改大型开源科学软件代码的能力，开发了首个用于处理Flex数据的开源软件STAR-Flex。


<details>
  <summary>Details</summary>
Motivation: 开源科学软件因复杂性而难以被社区修改，限制了科学开放性。

Method: 利用先进技术，由AI团队在单个人领导下，扩展了现有软件STAR，添加处理Flex数据新功能，同时保持原有功能完整。

Result: 开发出包含1.6万行C++代码的STAR-Flex，实现了首次开源Flex数据处理软件。

Conclusion: AI驱动的软件开发可促进科学软件的开放和社区参与，提高科研软件的可修改性和灵活性。

Abstract: Open-source scientific software is effectively closed to modification by its complexity. With recent advances in technology, an agentic AI team led by a single human can now rapidly and robustly modify large codebases and re-open science to the community which can review and vet the AI generated code. We demonstrate this with a case study, STAR-Flex, which is an open source fork of STAR, adding 16,000 lines of C++ code to add the ability to process 10x Flex data, while maintaining full original function. This is the first open-source processing software for Flex data and was written as part of the NIH funded MorPHiC consortium.

</details>


### [76] [A Reference Architecture for Embedding Quantum Software Into Enterprise Systems](https://arxiv.org/abs/2512.12009)
*Marc Uphues,Sebastian Thöne,Herbert Kuchen*

Main category: cs.SE

TL;DR: 本文提出了一种面向企业系统的模块化参考架构，用于嵌入量子计算软件，通过分布式服务形成稳定的可执行流程。


<details>
  <summary>Details</summary>
Motivation: 量子计算在处理计算密集型问题中表现出显著优势，企业系统需要设计合适的软件架构以融合量子计算服务，同时满足系统特性和质量属性。

Method: 设计了一个由松耦合且分布式服务组成的模块化参考架构，这些服务包括与量子计算相关和无关的任务，服务的编排通过可执行的BPMN模型进行规范。

Result: 基于该参考架构，本文在两个运筹学领域的组合优化案例中进行了应用和评估，验证了架构的适用性和效果。

Conclusion: 所提出的模块化参考架构能够有效地将量子软件集成到企业系统中，支持稳定、可重用的流程，有助于企业利用量子计算提升业务性能。

Abstract: Quantum computing promises a remarkable performance boost for certain applications, including computational intensive problems addressed by enterprise systems. However, software architectures of enterprise systems must consider specific characteristics and quality attributes when collaborating with quantum computing services. Hence, this paper presents a modular reference architecture for embedding quantum software into enterprise systems. Its building blocks consist of loosely coupled and distributed services that implement both quantum-independent and quantum-specific tasks. Although these services either depend on the business domain or the selected quantum algorithm, their orchestration forms a stable and reusable pipeline, specified as an executable BPMN model. For demonstration and evaluation purposes, the proposed reference architecture is utilized in two case studies addressing combinatorial challenges from the field of operations research.

</details>


### [77] [Hyper model checking for high-level relational models](https://arxiv.org/abs/2512.12024)
*Nuno Macedo,Hugo Pacheco*

Main category: cs.SE

TL;DR: 本文提出了HyperPardinus，一种扩展Alloy的模型查找工具，支持自动验证设计模型中的超性质，解决了高层次规格语言对超性质支持不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的高层次规格语言难以有效支持软件工程师在系统设计早期验证超性质，而Alloy虽适合早期设计验证，但不支持超性质。

Method: 提出HyperPardinus，扩展Pardinus使其自动验证关系模型上的超性质，保守扩展Alloy以支持超性质的规格和自动验证，并提供高层次反例可视化。

Result: 方法能处理具有交替量词的复杂超性质，成功进行建模和反例查找，验证了应对复杂现实场景的可行性。

Conclusion: HyperPardinus有效填补了Alloy在超性质验证上的空白，促进高层规格语言在系统设计早期的验证应用。

Abstract: Many properties related to security or concurrency must be encoded as so-called hyperproperties, temporal properties that allow reasoning about multiple traces of a system. However, despite recent advances on model checking hyperproperties, there is still a lack of higher-level specification languages that can effectively support software engineering practitioners in verifying properties of this class at early stages of system design.
  Alloy is a lightweight formal method with a high-level specification language that is supported by automated analysis procedures, making it particularly well-suited for the verification of design models at early development stages. It does not natively support, however, the verification of hyperproperties.
  This work proposes HyperPardinus, a new model finding procedure that extends Pardinus -- the temporal logic backend of the Alloy language -- to automatically verify hyperproperties over relational models by relying on existing low-level model checkers for hyperproperties. It then conservatively extends Alloy to support the specification and automatic verification of hyperproperties over design models, as well as the visualization of (counter-)examples at a higher-level of abstraction. Evaluation shows that our approach enables modeling and finding (counter-)examples for complex hyperproperties with alternating quantifiers, making it feasible to address relevant scenarios from the state of the art.

</details>


### [78] [Instruction-Tuning Open-Weight Language Models for BPMN Model Generation](https://arxiv.org/abs/2512.12063)
*Gökberk Çelikmasat,Atay Özgövde,Fatma Başak Aydemir*

Main category: cs.SE

TL;DR: 本文提出了InstruBPM，一种通过指令微调开源大语言模型，自动从自然语言生成高质量BPMN流程模型的方法。


<details>
  <summary>Details</summary>
Motivation: 传统领域建模虽有诸多优势，但因耗时且需专业技能，实践中常被忽视。本文试图用高效、隐私友好的大语言模型自动生成流程模型，降低建模门槛。

Method: 通过构建文本-图表配对数据集，使用参数高效的微调与量化技术，对开源大语言模型进行指令微调，实现本地部署；并从文本相似度、结构一致性、规范符合度和专家评审四方面评估模型。

Result: 微调后的模型在各项序列和结构指标上均超越未调优的开源基线和强大的专有模型，生成的流程图大体符合BPMN最佳实践，并显著减少建模工作量。

Conclusion: 指令微调显著提升了模型的结构准确性与鲁棒性，降低了对复杂提示的依赖，且实现了资源友好和隐私保护，研究成果及代码已公开，支持复现与拓展研究。

Abstract: Domain models are central to software engineering, as they enable a shared understanding, guide implementation, and support automated analyses and model-driven development. Yet, despite these benefits, practitioners often skip modeling because it is time-consuming and demands scarce expertise. We address this barrier by investigating whether open-weight large language models, adapted via instruction tuning, can generate high-quality BPMN process models directly from natural language descriptions in a cost-effective and privacy-preserving way. We introduce InstruBPM, a reproducible approach that prepares paired text-diagram data and instruction tunes an open source large language model with parameter-efficient fine-tuning and quantization for on-prem deployment. We evaluate the tuned model through complementary perspectives: (i) text/code similarity using BLEU, ROUGE-L, and METEOR, (ii) structural fidelity using Relative Graph Edit Distance, (iii) guidelines conformance using external tool checks, and (iv) a small expert review. Using a curated subset of a multi-domain BPMN dataset, we compare the tuned model with untuned open-weight baselines and strong proprietary models under consistent prompting regimes. Our compact tuned model outperforms all baselines across sequence and structural metrics while requiring substantially fewer resources; guideline analysis and expert feedback further indicate that the generated diagrams largely follow BPMN best practices and are useful starting points that reduce modeling effort. Overall, instruction tuning improves structural accuracy and robustness compared to untuned baselines and reduces reliance on heavy prompt scaffolding. We publicly share the trained models and scripts to support reproducibility and further research.

</details>


### [79] [Citation-Grounded Code Comprehension: Preventing LLM Hallucination Through Hybrid Retrieval and Graph-Augmented Context](https://arxiv.org/abs/2512.12117)
*Jahidul Arafat*

Main category: cs.SE

TL;DR: 本文针对大型语言模型在代码理解中产生事实错误引用的问题，提出结合混合检索和结构推理的解决方案，实现了92%的引用准确率，无虚构。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在代码理解时存在幻觉问题，生成不准确的代码引用，影响开发者的信任和使用效果。

Method: 本文设计了一种混合检索系统，结合BM25稀疏匹配、BGE稠密嵌入和基于导入关系的Neo4j图扩展，利用跨文件结构信息，提高代码引用的准确性。

Result: 经在30个Python代码库和180个开发者查询上系统评估，该方法在引用准确率上超过单一检索方式14-18个百分点，在62%的架构查询中发现纯文本相似性遗漏的跨文件证据，达到92%的准确率且无虚构。

Conclusion: 采用基于引用的生成方式，结合结构推理和混合检索，是提升代码理解系统准确性和可靠性的有效架构原则。

Abstract: Large language models have become essential tools for code comprehension, enabling developers to query unfamiliar codebases through natural language interfaces. However, LLM hallucination, generating plausible but factually incorrect citations to source code, remains a critical barrier to reliable developer assistance. This paper addresses the challenges of achieving verifiable, citation grounded code comprehension through hybrid retrieval and lightweight structural reasoning. Our work is grounded in systematic evaluation across 30 Python repositories with 180 developer queries, comparing retrieval modalities, graph expansion strategies, and citation verification mechanisms. We find that challenges of citation accuracy arise from the interplay between sparse lexical matching, dense semantic similarity, and cross file architectural dependencies. Among these, cross file evidence discovery is the largest contributor to citation completeness, but it is largely overlooked because existing systems rely on pure textual similarity without leveraging code structure. We advocate for citation grounded generation as an architectural principle for code comprehension systems and demonstrate this need by achieving 92 percent citation accuracy with zero hallucinations. Specifically, we develop a hybrid retrieval system combining BM25 sparse matching, BGE dense embeddings, and Neo4j graph expansion via import relationships, which outperforms single mode baselines by 14 to 18 percentage points while discovering cross file evidence missed by pure text similarity in 62 percent of architectural queries.

</details>


### [80] [Training Versatile Coding Agents in Synthetic Environments](https://arxiv.org/abs/2512.12216)
*Yiqi Zhu,Apurva Gandhi,Graham Neubig*

Main category: cs.SE

TL;DR: 提出了SWE-Playground，一种通过合成生成项目和任务来训练多功能软件工程智能体的方法，突破了依赖已有GitHub资源和任务单一性的限制。


<details>
  <summary>Details</summary>
Motivation: 现有训练软件工程智能体的方法依赖预先存在的GitHub仓库资源，灵活性不足且主要集中在解决问题任务，无法覆盖软件工程师需要处理的多种任务。

Method: 设计了SWE-Playground管线，利用强大的语言模型和智能体从零合成生成项目和任务环境，支持更广泛的编码任务，包括通过生成单元测试来重现问题和从头实现库，无需外部数据依赖。

Result: 在三个不同基准测试中展示了该方法的有效性，生成的轨迹训练信号密集，使智能体以远少于以往工作的轨迹数量达到相似性能。

Conclusion: SWE-Playground通过合成数据极大提升了训练灵活性和任务多样性，有效支持多功能编码智能体的训练，显著降低了训练数据需求。

Abstract: Prior works on training software engineering agents have explored utilizing existing resources such as issues on GitHub repositories to construct software engineering tasks and corresponding test suites. These approaches face two key limitations: (1) their reliance on pre-existing GitHub repositories offers limited flexibility, and (2) their primary focus on issue resolution tasks restricts their applicability to the much wider variety of tasks a software engineer must handle. To overcome these challenges, we introduce SWE-Playground, a novel pipeline for generating environments and trajectories which supports the training of versatile coding agents. Unlike prior efforts, SWE-Playground synthetically generates projects and tasks from scratch with strong language models and agents, eliminating reliance on external data sources. This allows us to tackle a much wider variety of coding tasks, such as reproducing issues by generating unit tests and implementing libraries from scratch. We demonstrate the effectiveness of this approach on three distinct benchmarks, and results indicate that SWE-Playground produces trajectories with dense training signal, enabling agents to reach comparable performance with significantly fewer trajectories than previous works.

</details>


### [81] [Cluster-guided LLM-Based Anonymization of Software Analytics Data: Studying Privacy-Utility Trade-offs in JIT Defect Prediction](https://arxiv.org/abs/2512.12224)
*Maaz Khan,Gul Sher Khan,Ahsan Raza,Pir Sami Ullah,Abdul Ali Bangash*

Main category: cs.SE

TL;DR: 提出一种基于大语言模型（LLM）的聚类引导匿名化方法，用于保护软件缺陷预测数据的隐私，同时保持预测准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的匿名化方法忽略了软件度量指标之间的上下文依赖，导致隐私与数据效用之间的权衡不理想。

Method: 利用LLM的上下文推理能力，将提交按特征聚类，并为每个聚类生成上下文感知的匿名化参数配置，定义用于匿名化的alpha-beta比率和代码变更混合分布。

Result: 在六个开源项目上的评估表明，该方法隐私水平达到IPR >= 80%，隐私提升18%至25%，且保持了与现有图匿名化方法相当的F1分数。

Conclusion: 基于LLM的自适应匿名化引擎能够在保留上下文和统计关系的同时，有效保护软件分析数据隐私，且不损害预测性能。

Abstract: The increasing use of machine learning (ML) for Just-In-Time (JIT) defect prediction raises concerns about privacy leakage from software analytics data. Existing anonymization methods, such as tabular transformations and graph perturbations, often overlook contextual dependencies among software metrics, leading to suboptimal privacy-utility tradeoffs. Leveraging the contextual reasoning of Large Language Models (LLMs), we propose a cluster-guided anonymization technique that preserves contextual and statistical relationships within JIT datasets. Our method groups commits into feature-based clusters and employs an LLM to generate context-aware parameter configurations for each commit cluster, defining alpha-beta ratios and churn mixture distributions used for anonymization. Our evaluation on six projects (Cassandra, Flink, Groovy, Ignite, OpenStack, and Qt) shows that our LLM-based approach achieves privacy level 2 (IPR >= 80 percent), improving privacy by 18 to 25 percent over four state-of-the-art graph-based anonymization baselines while maintaining comparable F1 scores. Our results demonstrate that LLMs can act as adaptive anonymization engines when provided with cluster-specific statistical information about similar data points, enabling context-sensitive and privacy-preserving software analytics without compromising predictive accuracy.

</details>


### [82] [Evaluating Asynchronous Semantics in Trace-Discovered Resilience Models: A Case Study on the OpenTelemetry Demo](https://arxiv.org/abs/2512.12314)
*Anatoly A. Krasnovsky*

Main category: cs.SE

TL;DR: 本文提出了一种基于OpenTelemetry跟踪数据的微服务依赖图构建与蒙特卡洛仿真方法，用以估计端点在服务失败情况下的可用性，并验证了异步语义对可用性预测的影响极小。


<details>
  <summary>Details</summary>
Motivation: 传统的微服务弹性模型多为手工定制，缺乏自动化和普适性，需借助分布式追踪数据自动构建服务依赖并预测故障影响。

Method: 直接从OpenTelemetry原始跟踪数据推导服务依赖图，结合端点特定的成功判定标准，加入Kafka异步边的非阻塞语义，用蒙特卡洛模拟估计端点可用性；通过GitHub Actions自动化执行图发现、仿真和混沌实验。

Result: 模型成功重现了整体可用性下降曲线，且增加异步Kafka语义对可用性预测影响极小（约0.001个百分点），表明无需复杂异步建模。

Conclusion: 对于即时HTTP可用性评估，简单的基于连接性的服务依赖模型足够有效，异步依赖的显式建模效果有限。

Abstract: While distributed tracing and chaos engineering are becoming standard for microservices, resilience models remain largely manual and bespoke. We revisit a trace-discovered connectivity model that derives a service dependency graph from traces and uses Monte Carlo simulation to estimate endpoint availability under fail-stop service failures. Compared to earlier work, we (i) derive the graph directly from raw OpenTelemetry traces, (ii) attach endpoint-specific success predicates, and (iii) add a simple asynchronous semantics that treats Kafka edges as non-blocking for immediate HTTP success. We apply this model to the OpenTelemetry Demo ("Astronomy Shop") using a GitHub Actions workflow that discovers the graph, runs simulations, and executes chaos experiments that randomly kill microservices in a Docker Compose deployment. Across the studied failure fractions, the model reproduces the overall availability degradation curve, while asynchronous semantics for Kafka edges change predicted availabilities by at most about 10^(-5) (0.001 percentage points). This null result suggests that for immediate HTTP availability in this case study, explicitly modeling asynchronous dependencies is not warranted, and a simpler connectivity-only model is sufficient.

</details>


### [83] [The Role of AI in Modern Penetration Testing](https://arxiv.org/abs/2512.12326)
*J. Alexander Curtis,Nasir U. Eisty*

Main category: cs.SE

TL;DR: 本文系统综述了人工智能在渗透测试领域的应用，特别是强化学习在发现和利用漏洞阶段的进展，指出现有研究集中于自动化重复任务和优化攻击策略，但仍存在模型灵活性不足及侦察和后利用阶段应用有限等挑战。


<details>
  <summary>Details</summary>
Motivation: 随着系统复杂性增加，传统人工渗透测试效率低且耗时，迫切需要更高效、可扩展的测试方法。AI技术为此提供了新机遇。

Method: 通过分析58篇同行评审文献，重点考察了AI技术在渗透测试中的应用，特别是强化学习方法的使用情况及工具实例，评估其效果与现状。

Result: AI在发现和利用漏洞阶段主要通过自动化任务与优化攻击策略表现出显著潜力，实际应用虽有限但已有如欧洲航天局PenBox等项目展示了成效。大型语言模型应用相对较少。

Conclusion: AI辅助渗透测试仍处初期阶段，尽管在部分环节表现出优势，但存在灵活性和阶段覆盖不足等问题，未来应加强侦察、后利用及大型语言模型的研究，以推动渗透测试的智能化和高效化发展。

Abstract: Penetration testing is a cornerstone of cybersecurity, traditionally driven by manual, time-intensive processes. As systems grow in complexity, there is a pressing need for more scalable and efficient testing methodologies. This systematic literature review examines how Artificial Intelligence (AI) is reshaping penetration testing, analyzing 58 peer-reviewed studies from major academic databases. Our findings reveal that while AI-assisted pentesting is still in its early stages, notable progress is underway, particularly through Reinforcement Learning (RL), which was the focus of 77% of the reviewed works. Most research centers on the discovery and exploitation phases of pentesting, where AI shows the greatest promise in automating repetitive tasks, optimizing attack strategies, and improving vulnerability identification. Real-world applications remain limited but encouraging, including the European Space Agency's PenBox and various open-source tools. These demonstrate AI's potential to streamline attack path analysis, analyze complex network topology, and reduce manual workload. However, challenges persist: current models often lack flexibility and are underdeveloped for the reconnaissance and post-exploitation phases of pentesting. Applications involving Large Language Models (LLMs) remain relatively under-researched, pointing to a promising direction for future exploration. This paper offers a critical overview of AI's current and potential role in penetration testing, providing valuable insights for researchers, practitioners, and organizations aiming to enhance security assessments through advanced automation or looking for gaps in existing research.

</details>


### [84] [ATLAS: Automated Tree-based Language Analysis System for C and C++ source programs](https://arxiv.org/abs/2512.12507)
*Jaid Monwar Chowdhury,Ahmad Farhan Shahriar Chowdhury,Humayra Binte Monwar,Mahmuda Naznin*

Main category: cs.SE

TL;DR: 本文提出了ATLAS，一款基于Python的命令行工具，用于生成并整合C/C++代码的多视图表示，包括控制流图和类型感知的数据流图，以解决传统分析技术在复杂代码上的不足。


<details>
  <summary>Details</summary>
Motivation: 传统程序分析方法难以有效处理C/C++代码中的复杂结构、指针别名、多级间接引用和类型混淆等问题，限制了基于机器学习的程序理解能力。

Method: 设计ATLAS工具，能生成语句级控制流图和类型感知数据流图，支持跨多文件项目，既适用于可编译代码也能分析不可编译代码，结合抽象语法树实现多视图统一代码表示。

Result: ATLAS成功生成覆盖整个程序的多视图代码表示，保留了代码的结构和语义信息，支持复杂代码的静态分析，增强了后续软件工程和机器学习任务的基础。

Conclusion: 通过整合CFG、DFG和AST，ATLAS有效克服了传统分析技术在C/C++复杂性挑战下的局限性，为程序分析和机器学习提供了实用基础和工具支持。

Abstract: The growing complexity of modern software systems has highlighted the shortcomings of traditional programming analysis techniques, particularly for Software Engineering (SE) tasks. While machine learning and Large Language Models (LLMs) offer promising solutions, their effectiveness is limited by the way they interpret data. Unlike natural language, source code meaning is defined less by token adjacency and more by complex, long-range, and structural relationships and dependencies. This limitation is especially pronounced for C and C++, where flatter syntactic hierarchies, pointer aliasing, multi-level indirection, typedef-based type obfuscation, and function-pointer calls hinder accurate static analysis. To address these challenges, this paper introduces ATLAS, a Python-based Command-Line Interface (CLI) that (i) generates statement-level Control Flow Graphs (CFG) and type-aware Data Flow Graphs (DFG) that capture inter-functional dependencies for the entire program; (ii) has the ability to work on entire C and C++ projects comprising multiple files; (iii) works on both compilable and non-compilable code and (iv) produces a unified multi-view code representation using Abstract Syntax Trees (AST), CFG and DFG. By preserving essential structural and semantic information, ATLAS provides a practical foundation for improving downstream SE and machine-learning-based program understanding. Video demonstration: https://youtu.be/RACWQe5ELwY Tool repository: https://github.com/jaid-monwar/ATLAS-code-representation-tool

</details>


### [85] [Diverse LLMs vs. Vulnerabilities: Who Detects and Fixes Them Better?](https://arxiv.org/abs/2512.12536)
*Arastoo Zibaeirad,Marco Vieira*

Main category: cs.SE

TL;DR: 本文提出了一种名为DVDR-LLM的多模型集成框架，通过融合多个大语言模型的输出，提升了软件漏洞检测和修复的准确率。


<details>
  <summary>Details</summary>
Motivation: 单个大语言模型在识别复杂漏洞和生成修复代码时表现有限，亟需提升检测和修复的效果。

Method: 设计DVDR-LLM集成框架，将多个不同的大语言模型输出组合，以减少误检率并提升整体性能。

Result: DVDR-LLM在漏洞检测准确率提升10-12%，多文件漏洞召回率提升18%，F1分数提升11.8%；但在降低误报的同时，增加了漏报，需根据安全需求调整模型输出阈值。

Conclusion: 通过集成多个大语言模型，DVDR-LLM有效提升了漏洞检测与修复的性能，特别是在复杂代码环境下表现更加突出，但需权衡误报与漏报。

Abstract: Large Language Models (LLMs) are increasingly being studied for Software Vulnerability Detection (SVD) and Repair (SVR). Individual LLMs have demonstrated code understanding abilities, but they frequently struggle when identifying complex vulnerabilities and generating fixes.
  This study presents DVDR-LLM, an ensemble framework that combines outputs from diverse LLMs to determine whether aggregating multiple models reduces error rates. Our evaluation reveals that DVDR-LLM achieves 10-12% higher detection accuracy compared to the average performance of individual models, with benefits increasing as code complexity grows. For multi-file vulnerabilities, the ensemble approach demonstrates significant improvements in recall (+18%) and F1 score (+11.8%) over individual models. However, the approach raises measurable trade-offs: reducing false positives in verification tasks while simultaneously increasing false negatives in detection tasks, requiring careful decision on the required level of agreement among the LLMs (threshold) for increased performance across different security contexts.
  Artifact: https://github.com/Erroristotle/DVDR_LLM

</details>


### [86] [Assessing the Capability of Android Dynamic Analysis Tools to Combat Anti-Runtime Analysis Techniques](https://arxiv.org/abs/2512.12551)
*Dewen Suo,Lei Xue,Weihao Huang,Runze Tan,Guozi Sun*

Main category: cs.SE

TL;DR: 本文研究了Android动态分析工具应对反动态分析技术（ARA）的能力，发现现有工具在绕过ARA方面存在显著不足，亟需更强的方法来提升安全检测效果。


<details>
  <summary>Details</summary>
Motivation: 随着Android应用数量激增，恶意应用利用ARA技术躲避动态分析，给安全专业人员带来挑战，亟需评估并提升动态分析工具的有效性。

Method: 本文通过实证研究，评估了广泛使用的Android动态分析工具绕过各种ARA技术的能力，揭示其在应对ARA机制上的弱点。

Result: 研究结果显示现有动态分析工具在绕过ARA机制方面存在关键差距，无法有效对抗恶意应用的反动态分析手段。

Conclusion: 现有Android动态分析工具难以有效抵御ARA技术，需研发更强大且鲁棒的分析方法，以提升动态分析的安全保障水平。

Abstract: As the dominant mobile operating system, Android continues to attract a substantial influx of new applications each year. However, this growth is accompanied by increased attention from malicious actors, resulting in a significant rise in security threats to the Android ecosystem. Among these threats, the adoption of Anti-Runtime Analysis (ARA) techniques by malicious applications poses a serious challenge, as it hinders security professionals from effectively analyzing malicious behaviors using dynamic analysis tools. ARA technologies are designed to prevent the dynamic examination of applications, thus complicating efforts to ensure platform security. This paper presents a comprehensive empirical study that assesses the ability of widely-used Android dynamic analysis tools to bypass various ARA techniques. Our findings reveal a critical gap in the effectiveness of existing dynamic analysis tools to counter ARA mechanisms, highlighting an urgent need for more robust solutions. This work provides valuable insights into the limitations of existing tools and highlights the need for improved methods to counteract ARA technologies, thus advancing the field of software security and dynamic analysis.

</details>


### [87] [SHERLOCK: A Deep Learning Approach To Detect Software Vulnerabilities](https://arxiv.org/abs/2512.12593)
*Saadh Jawwadh,Guhanathan Poravi*

Main category: cs.SE

TL;DR: 本研究采用卷积神经网络（CNN）进行软件漏洞检测，利用5折交叉验证训练模型，对源码进行标记化处理。结果显示模型在功能级别能有效检测多种漏洞，特别是CWE-199和CWE-120类型，整体准确率较高。


<details>
  <summary>Details</summary>
Motivation: 传统的静态和动态分析方法在检测多漏洞时效果不佳，需求更准确的检测技术来防范安全风险。

Method: 利用卷积神经网络对标记化的源代码进行分析，通过5折交叉验证训练和评估模型。

Result: CNN模型成功检测了多种漏洞，尤其在CWE-199和CWE-120类型上表现优异，整体准确率高，真阳性和真阴性指标显著。数据集缺乏标准化限制了部分漏洞的检测性能。

Conclusion: 深度学习方法相比传统技术能显著提升软件漏洞检测的准确率，未来方向包括建立标准数据集以提升模型的可靠性。

Abstract: The increasing reliance on software in various applications has made the problem of software vulnerability detection more critical. Software vulnerabilities can lead to security breaches, data theft, and other negative outcomes. Traditional software vulnerability detection techniques, such as static and dynamic analysis, have been shown to be ineffective at detecting multiple vulnerabilities.
  To address this issue, this study employed a deep learning approach, specifically Convolutional Neural Networks (CNN), to solve the software vulnerability detection problem. A 5-split cross-validation approach was used to train and evaluate the CNN model, which takes tokenized source code as input.
  The findings indicated that Sherlock successfully detected multiple vulnerabilities at the function level, and its performance was particularly strong for CWE-199, CWE-120, and CWE-Other, with an overall high accuracy rate and significant true positive and true negative values. However, the performance was less reliable for some vulnerabilities due to the lack of a standardized dataset which will be a future research direction. The results suggest that compared to current techniques, the proposed deep learning approach has the potential to substantially enhance the accuracy of software vulnerability detection.

</details>


### [88] [A Systematic Analysis of Higher Education on Software Engineering in the Netherlands](https://arxiv.org/abs/2512.12650)
*Bastiaan Heeren,Fabiano Dalpiaz,Mazyar Seraj,Roberto Verdecchia,Vadim Zaytsev*

Main category: cs.SE

TL;DR: 本研究通过分析荷兰10所大学共207门课程，基于SWEBOK知识领域全面评估软件工程高等教育现状，发现不同知识领域在本科和硕士阶段的覆盖差异，并指出软件工程经济学等领域的不足，提出课程改进建议。


<details>
  <summary>Details</summary>
Motivation: 现有软件工程教育需持续改进，理解当前高等教育实践现状有助于教育者基于实际情况优化课程和整体教学体系。

Method: 采用基于SWEBOK知识领域的映射分析方法，结合群体资源采集，分析荷兰10所大学207门课程，经过数据的一致性和均质性处理后进行深入数据分析。

Result: 发现本科阶段以‘构造与编程’为核心知识领域，其他领域如模型知识在本科和硕士均有覆盖，复杂领域多集中于硕士。同时归纳出三组紧密结合的知识领域集群，课程布局较为均匀，仅少数因研究优势存在偏差。

Conclusion: 研究揭示关键知识领域间的联系及其整合学习潜力，指出软件工程经济学等领域被忽略的现状，建议教育者考虑补充相关内容，同时鼓励在其他地区复制该研究以促进全球软件工程教育对比与改进。

Abstract: Software engineering educators strive to continuously improve their courses and programs. Understanding the current state of practice of software engineering higher education can empower educators to critically assess their courses, fine-tune them by benchmarking against observed practices, and ultimately enhance their curricula. In this study, we aim to provide an encompassing analysis of higher education on software engineering by considering the higher educational offering of an entire European country, namely the Netherlands. We leverage a crowd-sourced analysis process by considering 10 Dutch universities and 207 university courses. The courses are analysed via knowledge areas adopted from the SWEBOK. The mapping process is refined via homogenisation and internal consistency improvement phases, and is followed by a data analysis phase. Given its fundamental nature, Construction and Programming is the most covered knowledge area at Bachelor level. Other knowledge areas are equally covered at Bachelor and Master level (e.g., software engineering models), while more advanced ones are almost exclusively covered at Master level. We identify three clusters of tightly coupled knowledge areas: (i) requirements, architecture, and design, (ii) testing, verification, and security, and (iii) process-oriented and DevOps topics. Dutch universities generally cover all knowledge areas uniformly, with minor deviations reflecting institutional research strengths. Our results highlight correlations among key knowledge areas and their potential for enhancing integrated learning. We also identify underrepresented areas, such as software engineering economics, which educators may consider including in curricula. We invite researchers to use our research method in their own geographical region, in order to contrast software engineering education programs across the globe.

</details>


### [89] [Attributes to Support the Formulation of Practically Relevant Research Problems in Software Engineering](https://arxiv.org/abs/2512.12699)
*Anrafel Fernandes Pereira,Maria Teresa Baldassarre,Daniel Mendez,Jürgen Börstler,Nauman bin Ali,Rahul Mohanani,Darja Smite,Stefan Biffl,Rogardt Heldal,Davide Falessi,Daniel Graziotin,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 本文介绍并评估了软件工程中制定研究问题的七个关键属性，验证了其重要性和应用方法。


<details>
  <summary>Details</summary>
Motivation: 研究问题的恰当制定对实现软件工程研究的实际相关性至关重要，但缺乏系统指导。

Method: 通过ISERN 2024会议与42位资深研究员开展工作坊，利用Problem Vision板呈现七个属性，并收集反馈和调查数据。

Result: 验证了七个属性在行业导向研究问题制定中的重要性，收集了实践应用建议，如加入财务标准及可行性分析。

Conclusion: 七个属性有助于反思性和具上下文意识的问题制定，适应具体研究情境可提升学术与工业需求的契合度。

Abstract: [Background] A well-formulated research problem is essential for achieving practical relevance in Software Engineering (SE), yet there is a lack of structured guidance in this early phase. [Aims] Our goal is to introduce and evaluate seven attributes identified in the SE literature as relevant for formulating research problems (practical problem, context, implications/impacts, practitioners, evidence, objective, and research questions) in terms of their perceived importance and completeness, and learn how they can be applied. [Method] We conducted a workshop with 42 senior SE researchers during the ISERN 2024 meeting. The seven attributes were presented using a Problem Vision board filled with a research example. Participants discussed attributes in groups, shared written feedback, and individually completed a survey assessing their importance, completeness, and suggestions for improvement. [Results] The findings confirm the importance of the seven attributes in the formulation of industry-oriented research problems. Qualitative feedback illustrated how they can be applied in practice and revealed suggestions to refine them, such as incorporating financial criteria (e.g., ROI) into implications/impacts and addressing feasibility and constraints under evidence. [Conclusion] The results reaffirm the importance of the seven attributes in supporting a reflective and context-aware problem formulation. Adapting their use to specific research contexts can help to improve the alignment between academic research and industry needs.

</details>


### [90] [Towards AI Agents Supported Research Problem Formulation](https://arxiv.org/abs/2512.12719)
*Anrafel Fernandes Pereira,Maria Teresa Baldassarre,Daniel Mendez,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 本文探讨利用人工智能代理支持软件工程研究中研究问题的早期形成，以提升问题的实际相关性。


<details>
  <summary>Details</summary>
Motivation: 现有软件工程研究问题设定欠佳，未能反映工业实践的复杂性，影响研究的实际相关性。

Method: 基于Lean Research Inception框架，结合机器学习代码可维护性案例，设计并描述AI代理支持研究问题形成的场景，包括预填问题属性、协调利益相关者观点、细化研究问题、模拟多角度评估与决策支持。

Result: 描述性评估表明，AI代理支持有助于丰富协作讨论、增强对研究问题价值、可行性及适用性的关键反思。

Conclusion: AI代理集成到LRI中有望支持更具情境感知和实践导向的研究问题形成，但仍需实证验证以确认和完善集成方案。

Abstract: Poorly formulated research problems can compromise the practical relevance of Software Engineering studies by not reflecting the complexities of industrial practice. This vision paper explores the use of artificial intelligence agents to support SE researchers during the early stage of a research project, the formulation of the research problem. Based on the Lean Research Inception framework and using a published study on code maintainability in machine learning as a reference, we developed a descriptive evaluation of a scenario illustrating how AI agents, integrated into LRI, can support SE researchers by pre filling problem attributes, aligning stakeholder perspectives, refining research questions, simulating multiperspective assessments, and supporting decision making. The descriptive evaluation of the scenario suggests that AI agent support can enrich collaborative discussions and enhance critical reflection on the value, feasibility, and applicability of the research problem. Although the vision of integrating AI agents into LRI was perceived as promising to support the context aware and practice oriented formulation of research problems, empirical validation is needed to confirm and refine the integration of AI agents into problem formulation.

</details>


### [91] [Temporal HAL-API Dependencies as a Gateway to Formal Embedded Software Development](https://arxiv.org/abs/2512.12788)
*Manuel Bentele,Andreas Podelski,Axel Sikora,Bernd Westphal*

Main category: cs.SE

TL;DR: 本文介绍了时间性HAL-API依赖（THADs），一种适用于嵌入式软件开发的正确性属性，结合注释进行中等量规范和自动模型检测验证，有望成为静态分析和完整形式方法之间的折中方案。


<details>
  <summary>Details</summary>
Motivation: 希望在嵌入式软件开发中实现一种既不需大量规范，又能验证应用特定正确性的实用方法。

Method: 通过使用程序注释进行THADs规范，并应用软件模型检测自动验证。

Result: THADs提供了一种规范开销适中且可自动验证的正确性属性。

Conclusion: THADs有潜力成为工业嵌入式软件中形式方法更广泛且经济的切入点。

Abstract: Temporal HAL-API Dependencies (THADs) can be useful to capture an interesting class of correctness properties in embedded software development. They demand a moderate effort for specification (which can be done via program annotations) and verification (which can be done automatically via software model checking). In this sense, they have the potential to form an interesting sweet spot between generic properties (that demand virtually no specification effort, and that are typically addressed by static analysis) and application-specific properties as addressed by full-fledged formal methods. Thus, they may form a gateway to wider and more economic use of formal methods in industrial embedded software development.

</details>


### [92] [Challenges and Enablers: Remote Work for People with Disabilities in Software Development Teams](https://arxiv.org/abs/2512.12965)
*Thayssa Rocha,Luciano Teran,Marcelle Mota,Cleidson de Souza,Kiev Gama,Gustavo Pinto*

Main category: cs.SE

TL;DR: 本文研究了远程工作如何影响残疾人在软件开发团队中的体验，发现团队领导和成员对残疾人面临的协作挑战认识不足，提出了改进无障碍工具、沟通策略和管理方法的建议。


<details>
  <summary>Details</summary>
Motivation: 随着远程和混合工作模式的普及，残疾人在软件开发团队中的包容性问题日益突出，需要深入了解远程工作对他们体验的影响。

Method: 通过线上问卷调查（包括残疾人、领导和同事）及14次结构化采访（自认残疾的软件开发者），结合定量数据与定性编码分析。

Result: 残疾团队成员在远程协作中面临障碍，但团队领导和成员对这些挑战的感知有限，表明存在理解和支持不足。

Conclusion: 研究揭示了改善无障碍工具、优化沟通和适应性管理以提升残疾人在远程软件开发团队中包容性的潜力和必要性。

Abstract: The increasing adoption of remote and hybrid work modalities in the technology sector has brought new opportunities and challenges for the inclusion of people with disabilities (PWD) in software development teams (SDT). This study investigates how remote work affects PWDs' experience in mixed-ability SDT, focusing on the unique challenges and strategies that emerge in remote environments. We conducted an online survey with \totalSurveyResponses valid responses, encompassing PWD, their leaders, and teammates, to capture sociotechnical aspects of their experiences with remote collaboration. To deepen our understanding, we carried out 14 structured interviews with software developers who self-identified as having disabilities (six autistic individuals, six with physical disabilities, and two who are d/Deaf). Our analysis combines quantitative data with qualitative coding of open-ended survey responses and interview transcripts. The results reveal that, despite the barriers faced by team members with disabilities, their teammates and leaders have a limited perception of the daily challenges involved in sustaining collaborative remote work. These findings highlight opportunities for improvement in accessibility tools, communication strategies, and adaptive management approaches.

</details>


### [93] [A Decision Support Framework for Blockchain Pattern Selection Based on Soft Goals](https://arxiv.org/abs/2512.13239)
*Eddy Kiomba Kambilo,Nicolas Herbaut,Irina Rychkova,Carine Souveyet*

Main category: cs.SE

TL;DR: 本文提出了一种结合区块链模式本体和软目标的决策支持框架BC-TEAEM，帮助系统架构师在区块链方案选择上实现业务目标与技术设计的对齐。


<details>
  <summary>Details</summary>
Motivation: 区块链方案多样且缺乏统一框架，将业务目标与技术设计决策关联复杂，导致选型困难及潜在负面影响。

Method: BC-TEAEM框架结合区块链模式本体和领域无关的软目标，采用多准则决策方法，通过领域专家与技术专家的互动迭代捕捉偏好，支持区块链模式的系统选择。

Result: 开发了基于BC-TEAEM的决策支持工具原型，并通过制药企业供应链追溯系统案例验证了方法的适用性。

Conclusion: BC-TEAEM框架有效促进了区块链方案选择过程中的业务与技术对齐，提高了决策的系统性和可追溯性，适用于实际企业场景。

Abstract: Blockchain technology is gaining momentum across many sectors. Whereas blockchain solutions have important positive effects on the business domain, they also introduce constraints and may cause delayed or unforeseen negative effects, undermining business strategies. The diversity of blockchain patterns and lack of standardized frameworks linking business goals to technical design decisions make pattern selection a complex task for system architects. To address this challenge, we propose Blockchain--Technology-Aware Enterprise Modeling (BC-TEAEM), a decision support framework that combines ontologies of blockchain patterns and domain-independent soft goals with a multi-criteria decision-making approach. The framework focuses on the interplay between a domain expert and a technical expert to ensure alignment and traceability. By iteratively capturing and refining preferences, BC-TEAEM supports systematic selection of blockchain patterns. We develop a prototype decision support tool implementing our method and validate it through a case study of a pharmaceutical company's supply chain traceability system, demonstrating the framework's applicability. %a supply chain traceability case study.

</details>


### [94] [UCRBench: Benchmarking LLMs on Use Case Recovery](https://arxiv.org/abs/2512.13360)
*Shuyuan Xiao,Yiran Zhang,Weisong Sun,Xiaohong Chen,Yang Liu,Zhi Jin*

Main category: cs.SE

TL;DR: 本文针对现有用例基准匮乏且与实际系统行为不符的问题，构建了经过人工验证的代码对齐用例基准，并首次系统研究了大语言模型在生成用例方面的表现。


<details>
  <summary>Details</summary>
Motivation: 现有用例基准缺乏，且与真实系统行为对齐不足，限制了对大语言模型从源码生成用例能力的严格评估。

Method: 通过人工验证九个真实软件项目中的用户目标和子功能用例，构建代码对齐用例基准，并提出层级化评价协议，评估演员正确性、名称准确率、路径一致性和行为覆盖率。

Result: 实验表明大语言模型能部分重建系统功能，但在不同项目表现差异大，特别是在领域特定和多模块系统中存在明显不足。模型遗漏率高，难以在将子功能聚合为用户目标用例时维持一致的抽象层次。

Conclusion: 大语言模型在用例逆向工程中展现出潜力，但当前仍有明显局限，尤其在特定领域和多模块系统中表现较差，需要进一步研究和改进。

Abstract: Use cases are widely employed to specify functional requirements, yet existing benchmarks are scarce and face the risk of being misaligned with actual system behavior, similarly limiting the rigorous evaluation of large language models (LLMs) in generating use cases from source code. We address this gap by introducing code-aligned use case benchmarks, constructed through manual validation of both user-goal and subfunction use cases across nine real-world software projects. Using this benchmark, we conduct the first systematic study of LLMs and propose a hierarchical evaluation protocol that assesses actor correctness, name accuracy, path fidelity, and behavioral coverage. The results show that while LLMs can partially reconstruct system functionality, their performance varies significantly across projects, with particularly noticeable shortcomings in domain-specific and multi-module systems. The models also exhibit high omission rates and struggle to maintain consistent abstraction when aggregating subfunctions into user-goal use cases, highlighting both the potential and current limitations of LLM-based use case reverse engineering.

</details>


### [95] [PSALM: applying Proportional SAmpLing strategy in Metamorphic testing](https://arxiv.org/abs/2512.13414)
*Zenghui Zhou,Pak-Lok Poon,Zheng Zheng,Xiao-Yi Zhang*

Main category: cs.SE

TL;DR: 本文提出了PSALM，一种针对变形测试中源测试用例和变形组选择的策略，理论与实验证明其优于随机选择和其他策略。


<details>
  <summary>Details</summary>
Motivation: 变形测试的故障检测效果不仅受变形关系选择影响，测试用例和变形组的选取策略也关键，但相关系统方法研究较少。

Method: 提出PSALM，适应传统比例抽样策略（PSS）于变形测试，结合理论证明其至少不劣于随机选择，且分析特定条件下效率相等。

Result: 在八个程序和184个变异体的实证研究中，PSALM表现优于随机选择及其他现有策略，如ART和MT-ART，验证了理论分析。

Conclusion: PSALM为变形测试提供了理论支持且具实际效能的测试用例和变形组选择策略，推动了变形测试效能提升。

Abstract: Metamorphic testing (MT) alleviates the oracle problem by checking metamorphic relations (MRs) across multiple test executions. The fault detection effectiveness of MT is influenced not only by the choice and quality of MRs, but also by how source test cases and metamorphic groups (MGs) are selected. While substantial research has focused on designing, generating, and validating MRs, systematic methods for source test case selection and MG selection remain largely unexplored. Although the Proportional Sampling Strategy (PSS) provides strong theoretical guarantees in traditional testing, its assumptions cannot be directly applied in MT due to differences in selection domains, test units, and failure distributions. This paper proposes PSALM, an adaptation of PSS to MT for both source test case selection and MG selection. We formally prove that PSALM is never inferior to random selection regardless of how the source test case and MG domains are partitioned. We further identify the conditions under which applying PSALM to source test case selection and MG selection yields identical effectiveness. A comprehensive empirical study on eight subject programs and 184 mutants shows that the results are consistent with our theoretical analysis and that PSALM generally performs more effectively than existing selection strategies such as ART and MT-ART. These results demonstrate that PSALM provides a theoretically grounded and practically effective selection strategy for MT.

</details>


### [96] [QMon: Monitoring the Execution of Quantum Circuits with Mid-Circuit Measurement and Reset](https://arxiv.org/abs/2512.13422)
*Ning Ma,Jianjun Zhao,Foutse Khomh,Shaukat Ali,Heng Li*

Main category: cs.SE

TL;DR: 本文提出QMON方法，通过中途测量和重置操作监控量子电路状态，在不破坏运行行为的前提下，实现了对量子程序的调试和运行时监控。


<details>
  <summary>Details</summary>
Motivation: 由于量子电路的特性（如不可克隆定理和测量塌缩），无法直接观察或复制其状态，导致调试和运行时监控较为困难。

Method: 利用中途测量和重置操作，在开发者指定位置插入监控算子，对比预期与观测到的量子态概率，监控电路内部状态。

Result: 154个量子电路实验表明，QMON不会影响电路功能，能成功检测并定位多种程序错误，且对量子态的扰动极小。

Conclusion: QMON为量子电路调试和监控提供了实用方案，有助于提高量子软件的鲁棒性和可靠性。

Abstract: Unlike classical software, where logging and runtime tracing can effectively reveal internal execution status, quantum circuits possess unique properties, such as the no-cloning theorem and measurement-induced collapse, that prevent direct observation or duplication of their states. These characteristics make it especially challenging to monitor the execution of quantum circuits, complicating essential tasks such as debugging and runtime monitoring. This paper presents QMON, a practical methodology that leverages mid-circuit measurements and reset operations to monitor the internal states of quantum circuits while preserving their original runtime behavior. QMON enables the instrumentation of monitoring operators at developer-specified locations within the circuit, allowing comparisons between expected and observed quantum-state probabilities at those locations. We evaluated QMON by analyzing its impact on circuit behavior, monitoring coverage, and effectiveness in bug localization. Experimental results involving 154 quantum circuits show that all circuits preserve their intended functionality after instrumentation and that QMON successfully detects and localizes various programming errors. Although monitoring coverage is limited by the need to preserve delicate quantum properties, such as entanglement, QMON effectively detects errors while introducing no or negligible disturbance to the original quantum states. QMON facilitates the development of more robust and reliable quantum software as the field continues to mature.

</details>


### [97] [From User Interface to Agent Interface: Efficiency Optimization of UI Representations for LLM Agents](https://arxiv.org/abs/2512.13438)
*Dezhi Ran,Zhi Gong,Yuzhe Guo,Mengzhou Wu,Yuan Cao,Haochuan Lu,Hengyu Zhang,Xia Zeng,Gang Cao,Liangchao Yao,Yuetang Deng,Wei Yang,Tao Xie*

Main category: cs.SE

TL;DR: 本文提出了一种名为UIFormer的自动化UI表示优化框架，通过合成UI转换程序实现效率与完整性的协同优化，在Android和Web平台上显著减少了令牌数量并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型代理在自动化UI导航中效率不足，UI表示的不高效成为性能瓶颈，但优化UI表示存在缺乏布尔检验和处理复杂UI树输入的挑战。

Method: UIFormer通过设计领域特定语言限制程序空间，使用LLM迭代细化程序并结合正确性与效率奖励来优化UI转换程序，采用结构化分解简化合成任务。

Result: 在三个测试基准和五种大型语言模型上，UIFormer实现了48.7%至55.8%的令牌数减少，且几乎无运行时开销，同时保持或提升了代理性能。

Conclusion: UIFormer作为轻量级插件能无缝集成到现有LLM代理中，有效提升了UI导航任务的效率与表现，实际在微信中的部署证明了其实用价值。

Abstract: While Large Language Model (LLM) agents show great potential for automated UI navigation such as automated UI testing and AI assistants, their efficiency has been largely overlooked. Our motivating study reveals that inefficient UI representation creates a critical performance bottleneck. However, UI representation optimization, formulated as the task of automatically generating programs that transform UI representations, faces two unique challenges. First, the lack of Boolean oracles, which traditional program synthesis uses to decisively validate semantic correctness, poses a fundamental challenge to co-optimization of token efficiency and completeness. Second, the need to process large, complex UI trees as input while generating long, compositional transformation programs, making the search space vast and error-prone. Toward addressing the preceding limitations, we present UIFormer, the first automated optimization framework that synthesizes UI transformation programs by conducting constraint-based optimization with structured decomposition of the complex synthesis task. First, UIFormer restricts the program space using a domain-specific language (DSL) that captures UI-specific operations. Second, UIFormer conducts LLM-based iterative refinement with correctness and efficiency rewards, providing guidance for achieving the efficiency-completeness co-optimization. UIFormer operates as a lightweight plugin that applies transformation programs for seamless integration with existing LLM agents, requiring minimal modifications to their core logic. Evaluations across three UI navigation benchmarks spanning Android and Web platforms with five LLMs demonstrate that UIFormer achieves 48.7% to 55.8% token reduction with minimal runtime overhead while maintaining or improving agent performance. Real-world industry deployment at WeChat further validates the practical impact of UIFormer.

</details>


### [98] [A Data Annotation Requirements Representation and Specification (DARS)](https://arxiv.org/abs/2512.13444)
*Yi Peng,Hina Saeeda,Hans-Martin Heyn,Jennifer Horkoff,Eric Knauss,Fredrick Warg*

Main category: cs.SE

TL;DR: 提出了数据标注需求表示与规范（DARS）框架，以解决AI系统中数据标注所带来的特殊需求挑战，提升安全关键系统中标注数据的完整性、准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有需求工程中对AI系统及其数据的需求表示已有限涉猎，但数据标注及相关需求引发特有挑战，现有方法难以充分表示标注需求，亟需专门的表示方法。

Method: 提出DARS框架，包含标注协商卡以协调利益相关者目标与约束，及基于场景的标注规范用于表达可验证的原子化标注需求，并通过汽车感知案例和18类真实标注错误进行评估。

Result: DARS有效减轻了标注错误的根本原因，提升了数据标注的完整性、准确性和一致性。

Conclusion: 将DARS集成到需求工程中，有助于提升依赖数据标注的安全关键智能系统的可靠性，强调工程框架需适应当前智能信息系统对数据依赖的特点。

Abstract: With the rise of AI-enabled cyber-physical systems, data annotation has become a critical yet often overlooked process in the development of these intelligent information systems. Existing work in requirements engineering (RE) has explored how requirements for AI systems and their data can be represented. However, related interviews with industry professionals show that data annotations and their related requirements introduce distinct challenges, indicating a need for annotation-specific requirement representations. We propose the Data Annotation Requirements Representation and Specification (DARS), including an Annotation Negotiation Card to align stakeholders on objectives and constraints, and a Scenario-Based Annotation Specification to express atomic and verifiable data annotation requirements. We evaluate DARS with an automotive perception case related to an ongoing project, and a mapping against 18 real-world data annotation error types. The results suggest that DARS mitigates root causes of completeness, accuracy, and consistency annotation errors. By integrating DARS into RE, this work improves the reliability of safety-critical systems using data annotations and demonstrates how engineering frameworks must evolve for data-dependent components of today's intelligent information systems.

</details>


### [99] [Mapping of the system of software-related emissions and shared responsibilities](https://arxiv.org/abs/2512.13474)
*Laura Partanen,Antti Sipila,Md Sanaul Haque,Jari Porras*

Main category: cs.SE

TL;DR: 本文分析了ICT领域在全球变暖中的碳排放问题，强调规范企业责任以减缓气候变化。


<details>
  <summary>Details</summary>
Motivation: 全球气候快速变暖，ICT行业碳排放持续增加，需助力实现巴黎协定1.5℃目标。

Method: 构建全面的系统映射，识别ICT领域软件相关的碳排放和能耗主要来源，明确各利益相关方的软件生命周期责任。

Result: 系统映射明确了ICT行业碳排放点和能耗来源，并划定了各方责任范围。

Conclusion: 提升对软件相关碳排放的认识和管理，有助于ICT行业履行环境责任，助力气候目标达成。

Abstract: The global climate is experiencing a rapid and unprecedented warming trend. The ICT sector is a notable contributor to global greenhouse gas emissions, with its environmental impact continuing to expand. Addressing this issue is vital for achieving the objectives of the Paris Agreement, particularly the goal of limiting global temperature rise to 1.5°C. At the European Union level, regulatory measures such as the CSRD and the CSDD impose obligations on companies, including those within the ICT sector, to recognize and mitigate their environmental footprint. This study provides a comprehensive system mapping aimed at enhancing the awareness and understanding of software-related emissions and the corresponding responsibilities borne by the ICT sector. The mapping identifies the primary sources of carbon emissions and energy consumption within the ICT domain while also outlining the key responsibilities of the stakeholders accountable throughout the software lifecycle.

</details>


### [100] [Fine-tuned LLM-based Code Migration Framework](https://arxiv.org/abs/2512.13515)
*Oleg Grynets,Vasyl Lyashkevych,Dmytro Baran,Maksym Orliansky,Taras Zelenyy,Markiian Leshchyshyn*

Main category: cs.SE

TL;DR: 本文提出了一种基于微调大语言模型的自动化SQL代码迁移框架，实现了高效、精确的数据库系统转换。


<details>
  <summary>Details</summary>
Motivation: 解决传统SQL系统迁移中语法映射及数据库元素不兼容等关键难题，提高迁移效率和准确性。

Method: 采用微调结合提示工程技术，集成大语言模型实现自动SQL特征检测、半监督错误分析与专家反馈，构建迭代优化的迁移流程。

Result: 显著降低句法错误率，增强迁移过程中的特征一致性和数据库逻辑兼容性，实现了持续改进的效果。

Conclusion: 基于大语言模型微调的迁移框架有效提升了SQL系统迁移的准确性和效率，推动数据库现代化转换进程。

Abstract: The study presents the outcomes of research and experimental validation in the domain of automated codebase migration, with a focus on addressing challenges in transitioning SQL-based systems. The proposed method for migration essentially appears as a framework that leverages the best aspects of traditional software engineering techniques and provides an iterative, scalable, precise and efficient solution for modern database transformations. The central piece of the approach is the integration of a fine-tuned Large Language Model to address critical issues in SQL code conversion, such as syntax mapping, resolving discrepancies between Oracle PL/SQL and PostgreSQL, and optimising database elements such as stored procedures, triggers, views, and overall database logic. Thus, the method involves a trade-off between fine-tuning and prompt engineering. Special attention is given to a fine-tuning approach, which enhances the adaptability and compatibility with migration requirements across the entire database. According to the achieved results, fine-tuning plays a very important role. The study employs targeted evaluation methodologies along with computational metrics to measure the success of iterative conversion cycles. Core innovations include automated SQL feature detection, semi-supervised error analysis and integration of Subject Matter Experts feedback within a systematic migration workflow. The methodology achieves significant reductions in Syntax Error Rates, enhances feature alignment throughout migration iterations, and leverages dataset sampling to ensure continual improvement. By embedding GAI into the migration process, the framework facilitates precise feature mapping, semi-automated error resolution, and data-driven optimisation loops, improving workflow efficiency.

</details>


### [101] [How Low Can You Go? The Data-Light SE Challenge](https://arxiv.org/abs/2512.13524)
*Kishan Kumar Ganguly,Tim Menzies*

Main category: cs.SE

TL;DR: 该论文质疑软件工程优化需大量数据和计算的普遍假设，发现简单轻量级方法只用几十个标签即可达到90%最佳结果，且性能与复杂优化器相当。


<details>
  <summary>Details</summary>
Motivation: 评估软件工程中使用大量数据和计算资源进行优化的必要性，探索是否轻量级、数据需求低的方法也能取得优秀效果。

Method: 在多种软件工程优化任务中，采用多样性采样、极简贝叶斯学习器、随机探测等简单方法，并与当前复杂优化器进行对比实验，提出数据轻量化挑战，并形式化标注过程。

Result: 简单方法在几十个标签条件下表现接近最优，优于或等同复杂优化器，在许多任务上满足快速、经济的工程要求；并分析了轻量化方法成功与失败的条件。

Conclusion: 对于许多软件工程任务，轻量级、少标签的方法可替代大量数据和重度计算的优化策略，建议社区针对何时可用少量标签达成目标展开更多研究。

Abstract: Much of software engineering (SE) research assumes that progress depends on massive datasets and CPU-intensive optimizers. Yet has this assumption been rigorously tested?
  The counter-evidence presented in this paper suggests otherwise: across dozens of optimization problems from recent SE literature, including software configuration and performance tuning, cloud and systems optimization, project and process-level decision modeling, behavioral analytics, financial risk modeling, project health prediction, reinforcement learning tasks, sales forecasting, and software testing, even with just a few dozen labels, very simple methods (e.g. diversity sampling, a minimal Bayesian learner, or random probes) achieve near 90% of the best reported results. Further, these simple methods perform just as well as more state-of-the-the-art optimizers like SMAC, TPE, DEHB etc. While some tasks would require better outcomes and more sampling, these results seen after a few dozen samples would suffice for many engineering needs (particularly when the goal is rapid and cost-efficient guidance rather than slow and exhaustive optimization).
  Our results highlight that some SE tasks may be better served by lightweight approaches that demand fewer labels and far less computation. We hence propose the data-light challenge: when will a handful of labels suffice for SE tasks? To enable a large-scale investigation of this issue, we contribute (1) a mathematical formalization of labeling, (2) lightweight baseline algorithms, and (3) results on public-domain data showing the conditions under which lightweight methods excel or fail.
  For the purposes of open science, our scripts and data are online at https://github.com/KKGanguly/NEO .

</details>
