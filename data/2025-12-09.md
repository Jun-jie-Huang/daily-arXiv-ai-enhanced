<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 74]
- [cs.MA](#cs.MA) [Total: 7]
- [cs.SE](#cs.SE) [Total: 25]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Empathy by Design: Aligning Large Language Models for Healthcare Dialogue](https://arxiv.org/abs/2512.06097)
*Emre Umucu,Guillermina Solis,Leon Garza,Emilia Rivas,Beatrice Lee,Anantaa Kotal,Aritran Piplai*

Main category: cs.CL

TL;DR: 该论文提出一种基于直接偏好优化的对齐方法，显著提升大型语言模型在医疗护理对话中的事实准确性和同理心表现，优于传统方法和现有商业系统，推动可信和人性化医疗AI的发展。


<details>
  <summary>Details</summary>
Motivation: 现有通用大型语言模型在医疗和护理领域存在事实不可靠和缺乏同理心交流的问题，限制了其应用，尤其是在非专业用户和护理人员寻求医疗指导和情感支持时存在风险。

Method: 提出了一种基于直接偏好优化(DPO)的对齐框架，通过使用成对偏好数据对领域适应的LLM进行微调，使模型输出更符合人类偏好，强调支持性、易懂和有同理心的沟通风格，优于传统的强化学习对齐方法。

Result: 通过多种公开及专有模型的评测，DPO微调后的模型在语义对齐、事实准确性和以人为本的评价指标上均优于基线及商业系统（如谷歌医疗对话系统），证明该方法能够有效提升模型的可信度和同理心表现。

Conclusion: 基于偏好的对齐方法为开发可信赖、具同理心且临床信息丰富的护理和医疗AI助手提供了一个可扩展且透明的路径，有望推动该领域的应用发展。

Abstract: General-purpose large language models (LLMs) have demonstrated remarkable generative and reasoning capabilities but remain limited in healthcare and caregiving applications due to two key deficiencies: factual unreliability and a lack of empathetic communication. These shortcomings pose significant risks in sensitive contexts where users, particularly non-professionals and caregivers, seek medically relevant guidance or emotional reassurance. To address these challenges, we introduce a Direct Preference Optimization (DPO)-based alignment framework designed to improve factual correctness, semantic coherence, and human-centric qualities such as empathy, politeness, and simplicity in caregiver-patient dialogues. Our approach fine-tunes domain-adapted LLMs using pairwise preference data, where preferred responses reflect supportive and accessible communication styles while rejected ones represent prescriptive or overly technical tones. This direct optimization method aligns model outputs with human preferences more efficiently than traditional reinforcement-learning-based alignment. Empirical evaluations across multiple open and proprietary LLMs show that our DPO-tuned models achieve higher semantic alignment, improved factual accuracy, and stronger human-centric evaluation scores compared to baseline and commercial alternatives such as Google medical dialogue systems. These improvements demonstrate that preference-based alignment offers a scalable and transparent pathway toward developing trustworthy, empathetic, and clinically informed AI assistants for caregiver and healthcare communication. Our open-source code is available at: https://github.com/LeonG19/Empathy-by-Design

</details>


### [2] [Morphologically-Informed Tokenizers for Languages with Non-Concatenative Morphology: A case study of Yoloxóchtil Mixtec ASR](https://arxiv.org/abs/2512.06169)
*Chris Crawford*

Main category: cs.CL

TL;DR: 本文提出两种结合形态信息的非线性分词方法，在Yoloxóchitl Mixtec语言的语音注释任务中表现竞争力，提升了标注效率并减轻人工负担。


<details>
  <summary>Details</summary>
Motivation: 提高Yoloxóchitl Mixtec语音语料库的逐词注释效率，减轻人工标注负担。

Method: 提出两种新颖的形态信息分词方案，结合ASR和文本序列到序列工具，分别为Segment and Melody分词器和Sequence of Processes分词器。

Result: 新分词器在词错误率上优于传统分词器，但字符错误率逊色；与BPE和Unigram模型表现相当。实现了形态学和信息理论指标与性能的相关性分析。

Conclusion: 针对非拼接形态的语言设计的非线性分词器，在ASR任务中表现与传统分词器竞争力强，未来需评估其在下游任务中的适用性。

Abstract: This paper investigates the impact of using morphologically-informed tokenizers to aid and streamline the interlinear gloss annotation of an audio corpus of Yoloxóchitl Mixtec (YM) using a combination of ASR and text-based sequence-to-sequence tools, with the goal of improving efficiency while reducing the workload of a human annotator. We present two novel tokenization schemes that separate words in a nonlinear manner, preserving information about tonal morphology as much as possible. One of these approaches, a Segment and Melody tokenizer, simply extracts the tones without predicting segmentation. The other, a Sequence of Processes tokenizer, predicts segmentation for the words, which could allow an end-to-end ASR system to produce segmented and unsegmented transcriptions in a single pass. We find that these novel tokenizers are competitive with BPE and Unigram models, and the Segment-and-Melody model outperforms traditional tokenizers in terms of word error rate but does not reach the same character error rate. In addition, we analyze tokenizers on morphological and information-theoretic metrics to find predictive correlations with downstream performance. Our results suggest that nonlinear tokenizers designed specifically for the non-concatenative morphology of a language are competitive with conventional BPE and Unigram models for ASR. Further research will be necessary to determine the applicability of these tokenizers in downstream processing tasks.

</details>


### [3] [Do You Feel Comfortable? Detecting Hidden Conversational Escalation in AI Chatbots](https://arxiv.org/abs/2512.06193)
*Jihyung Park,Saleh Afroogh,Junfeng Jiao*

Main category: cs.CL

TL;DR: 提出GAUGE框架，实时监测对话情感变化，防范隐性情感伤害。


<details>
  <summary>Details</summary>
Motivation: 现有毒性过滤多依赖外部分类器，难以捕捉对话中细微且实时发展的情感恶化，存在隐性伤害风险。

Method: 基于logit的轻量级框架，实时分析LLM输出对对话情感状态的概率性转变，检测隐性情感升级。

Result: 本文提出了GAUGE框架，通过实时检测语言模型输出对对话情感状态的概率性影响，识别隐性情感升高，防止传统毒性检测漏判的“隐性伤害”。

Conclusion: GAUGE有效识别出传统毒性检测难以捕捉的情感逐渐恶化风险，填补了现有护栏机制的不足。

Abstract: Large Language Models (LLM) are increasingly integrated into everyday interactions, serving not only as information assistants but also as emotional companions. Even in the absence of explicit toxicity, repeated emotional reinforcement or affective drift can gradually escalate distress in a form of \textit{implicit harm} that traditional toxicity filters fail to detect. Existing guardrail mechanisms often rely on external classifiers or clinical rubrics that may lag behind the nuanced, real-time dynamics of a developing conversation. To address this gap, we propose GAUGE (Guarding Affective Utterance Generation Escalation), a lightweight, logit-based framework for the real-time detection of hidden conversational escalation. GAUGE measures how an LLM's output probabilistically shifts the affective state of a dialogue.

</details>


### [4] [Automated Data Enrichment using Confidence-Aware Fine-Grained Debate among Open-Source LLMs for Mental Health and Online Safety](https://arxiv.org/abs/2512.06227)
*Junyu Mao,Anthony Hills,Talia Tseriotou,Maria Liakata,Aya Shamir,Dan Sayda,Dana Atzil-Slonim,Natalie Djohari,Arpan Mandal,Silke Roth,Pamela Ugwudike,Mahesan Niranjan,Stuart E. Middleton*

Main category: cs.CL

TL;DR: 本文针对动态难标注的现实事件，提出CFD多模型细粒度辩论框架增强数据，显著提升心理健康和在线安全NLP任务性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界的指标（如心理健康事件和在线安全行为）对于改进NLP任务非常重要，但标注这些动态复杂信息的训练数据集成本高且困难。

Method: 提出了一个新的基于大型语言模型（LLM）的信心感知细粒度辩论（CFD）框架，多个LLM代理模拟人工标注者，通过交换细粒度证据达成共识，同时比较多种LLM数据丰富方法。

Result: CFD框架在两个专家标注的心理健康和在线安全数据集上表现最优，显著提升了数据丰富效果，并通过辩论生成的特征使下游任务性能提升显著，在线安全任务提升10.1%。

Conclusion: CFD框架有效提升了现实世界动态事件的NLP数据标注质量和下游任务表现，展示了多模型辩论机制在数据丰富中的潜力。

Abstract: Real-world indicators are important for improving natural language processing (NLP) tasks such as life events for mental health analysis and risky behaviour for online safety, yet labelling such information in NLP training datasets is often costly and/or difficult given the dynamic nature of such events. This paper compares several LLM-based data enrichment methods and introduces a novel Confidence-Aware Fine-Grained Debate (CFD) framework in which multiple LLM agents simulate human annotators and exchange fine-grained evidence to reach consensus. We describe two new expert-annotated datasets, a mental health Reddit wellbeing dataset and an online safety Facebook sharenting risk dataset. Our CFD framework achieves the most robust data enrichment performance compared to a range of baselines and we show that this type of data enrichment consistently improves downstream tasks. Enriched features incorporated via debate transcripts yield the largest gains, outperforming the non-enriched baseline by 10.1% for the online safety task.

</details>


### [5] [Policy-based Sentence Simplification: Replacing Parallel Corpora with LLM-as-a-Judge](https://arxiv.org/abs/2512.06228)
*Xuanxin Wu,Yuki Arase,Masaaki Nagata*

Main category: cs.CL

TL;DR: 本文提出利用大型语言模型自动构建策略驱动的简化数据，无需人工标注，实现了高效且多样化的句子简化系统，表现优于甚至可替代更大规模模型。


<details>
  <summary>Details</summary>
Motivation: 针对句子简化需要根据不同应用采取不同的简化策略，但现有方法难以实现策略驱动的控制，并且依赖人工标注或平行语料成本高。

Method: 提出了一种利用大型语言模型作为评判者（LLM-as-a-Judge）来自动构建与简化策略对齐的训练数据的方法，无需人工标注或平行语料。

Result: 该方法使简化系统能适应多样的简化策略，小规模开源模型Phi-3-mini-3.8B在词汇简化上超过GPT-4o，在整体重写上表现相当，自动指标和人工评价均验证了效果。

Conclusion: 该方法在不同模型和规模下均表现出稳定提升，验证了其鲁棒性和有效性。

Abstract: Sentence simplification aims to modify a sentence to make it easier to read and understand while preserving the meaning. Different applications require distinct simplification policies, such as replacing only complex words at the lexical level or rewriting the entire sentence while trading off details for simplicity. However, achieving such policy-driven control remains an open challenge. In this work, we introduce a simple yet powerful approach that leverages Large Language Model-as-a-Judge (LLM-as-a-Judge) to automatically construct policy-aligned training data, completely removing the need for costly human annotation or parallel corpora. Our method enables building simplification systems that adapt to diverse simplification policies. Remarkably, even small-scale open-source LLMs such as Phi-3-mini-3.8B surpass GPT-4o on lexical-oriented simplification, while achieving comparable performance on overall rewriting, as verified by both automatic metrics and human evaluations. The consistent improvements across model families and sizes demonstrate the robustness of our approach.

</details>


### [6] [LOCUS: A System and Method for Low-Cost Customization for Universal Specialization](https://arxiv.org/abs/2512.06239)
*Dhanasekar Sundararaman,Keying Li,Wayne Xiong,Aashna Garg*

Main category: cs.CL

TL;DR: LOCUS提出了一种低成本自定义通用特化的管道，通过少量标注数据优化NLP模型，达到高性能且节省内存和参数的效果。


<details>
  <summary>Details</summary>
Motivation: 降低构建和训练NLP模型的成本和计算资源需求，提升小样本学习的效果。

Method: 通过目标数据检索、上下文数据合成生成以及参数高效调优（全参数或低秩LoRA调优）三步构建定制NLP模型。

Result: LOCUS通过数据检索、合成和参数高效调优，在命名实体识别和文本分类任务中，性能优于包括GPT-4o在内的强基线，且显著减少模型大小和计算资源。

Conclusion: LOCUS在小样本条件下，通过数据检索、生成和参数高效调优，实现了高效且精准的NLP模型定制，显著降低计算资源需求，同时保证模型性能。

Abstract: We present LOCUS (LOw-cost Customization for Universal Specialization), a pipeline that consumes few-shot data to streamline the construction and training of NLP models through targeted retrieval, synthetic data generation, and parameter-efficient tuning. With only a small number of labeled examples, LOCUS discovers pertinent data in a broad repository, synthesizes additional training samples via in-context data generation, and fine-tunes models using either full or low-rank (LoRA) parameter adaptation. Our approach targets named entity recognition (NER) and text classification (TC) benchmarks, consistently outperforming strong baselines (including GPT-4o) while substantially lowering costs and model sizes. Our resultant memory-optimized models retain 99% of fully fine-tuned accuracy while using barely 5% of the memory footprint, also beating GPT-4o on several benchmarks with less than 1% of its parameters.

</details>


### [7] [Convergence of Outputs When Two Large Language Models Interact in a Multi-Agentic Setup](https://arxiv.org/abs/2512.06256)
*Aniruddha Maiti,Satya Nimmagadda,Kartha Veerya Jammuladinne,Niladri Sengupta,Ananya Jana*

Main category: cs.CL

TL;DR: 本文研究了两个大型语言模型在没有外部输入的多代理环境中互动时的表现，发现对话最初连贯但会陷入重复，形成收敛循环现象。


<details>
  <summary>Details</summary>
Motivation: 探究在无外界干预下，不同大型语言模型多轮对话的动态变化及相互影响，揭示其在多代理系统中的表现和潜在问题。

Method: 让两个语言模型交替阅读对方输出并生成回应，固定轮数后分析对话文本，使用词汇和嵌入度量衡量对话偏离初始种子句子及模型输出相似度。

Result: 发现对话初期较连贯，随后出现特定短语重复，模型输出趋同，导致对话陷入语义循环收敛状态。

Conclusion: 两模型在多轮交流中会趋于生成相似且重复的内容，导致对话陷入循环，即使模型规模大且独立训练且无提示指令。

Abstract: In this work, we report what happens when two large language models respond to each other for many turns without any outside input in a multi-agent setup. The setup begins with a short seed sentence. After that, each model reads the other's output and generates a response. This continues for a fixed number of steps. We used Mistral Nemo Base 2407 and Llama 2 13B hf. We observed that most conversations start coherently but later fall into repetition. In many runs, a short phrase appears and repeats across turns. Once repetition begins, both models tend to produce similar output rather than introducing a new direction in the conversation. This leads to a loop where the same or similar text is produced repeatedly. We describe this behavior as a form of convergence. It occurs even though the models are large, trained separately, and not given any prompt instructions. To study this behavior, we apply lexical and embedding-based metrics to measure how far the conversation drifts from the initial seed and how similar the outputs of the two models becomes as the conversation progresses.

</details>


### [8] [Nanbeige4-3B Technical Report: Exploring the Frontier of Small Language Models](https://arxiv.org/abs/2512.06266)
*Chen Yang,Guangyue Peng,Jiaying Zhu,Ran Le,Ruixiang Feng,Tao Zhang,Wei Ruan,Xiaoqi Liu,Xiaoxue Cheng,Xiyun Xu,Yang Song,Yanzipeng Gao,Yiming Jia,Yun Xing,Yuntao Wen,Zekai Wang,Zhenwei An,Zhicong Sun,Zongchao Chen*

Main category: cs.CL

TL;DR: Nanbeige4-3B通过先进训练调度和多阶段优化，成为性能领先的小型语言模型。


<details>
  <summary>Details</summary>
Motivation: 扩展小型语言模型的规模效应边界，提高其性能。

Method: 设计了Fine-Grained Warmup-Stable-Decay训练调度器，结合深思熟虑的生成优化和链式思考重构机制，采用Dual Preference Distillation方法蒸馏模型，并进行多阶段强化学习。

Result: Nanbeige4-3B在多个基准测试中显著超越同规模模型，接近更大模型的表现。

Conclusion: 通过创新的训练和微调策略，Nanbeige4-3B实现了小型语言模型的高性能突破。

Abstract: We present Nanbeige4-3B, a family of small-scale but high-performing language models. Pretrained on 23T high-quality tokens and finetuned on over 30 million diverse instructions, we extend the boundary of the scaling law for small language models. In pre-training, we design a Fine-Grained Warmup-Stable-Decay (FG-WSD) training scheduler, which progressively refines data mixtures across stages to boost model performance. In post-training, to improve the quality of the SFT data, we design a joint mechanism that integrates deliberative generation refinement and chain-of-thought reconstruction, yielding substantial gains on complex tasks. Following SFT, we employ our flagship reasoning model to distill Nanbeige4-3B through our proposed Dual Preference Distillation (DPD) method, which leads to further performance gains. Finally, a multi-stage reinforcement learning phase was applied, leveraging verifiable rewards and preference modeling to strengthen abilities on both reasoning and human alignment. Extensive evaluations show that Nanbeige4-3B not only significantly outperforms models of comparable parameter scale but also rivals much larger models across a wide range of benchmarks. The model checkpoints are available at https://huggingface.co/Nanbeige.

</details>


### [9] [Modeling Contextual Passage Utility for Multihop Question Answering](https://arxiv.org/abs/2512.06464)
*Akriti Jain,Aparna Garimella*

Main category: cs.CL

TL;DR: 本文提出一种轻量级Transformer模型，通过考虑段落间上下文依赖预测多跳问答中段落的实用性，提升答案的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法独立建模段落实用性，忽视段落之间的上下文依赖，而多跳推理中段落的实用性往往依赖于其与其他段落的关系。

Method: 提出基于轻量级小型Transformer模型的多跳问答上下文依赖的段落实用性建模方法，利用先进推理模型的推理轨迹生成合成训练数据，预测段落实用性得分以提升重排效果。

Result: 方法有效利用段落间依赖信息，提升了检索段落的重排质量，进而改善多跳问答系统的性能，优于传统基于相关性的重排方法。

Conclusion: 结合推理轨迹的上下文段落实用性建模可有效提升多跳问答的段落重排及最终表现，优于单独考虑相关性的传统方法。

Abstract: Multihop Question Answering (QA) requires systems to identify and synthesize information from multiple text passages. While most prior retrieval methods assist in identifying relevant passages for QA, further assessing the utility of the passages can help in removing redundant ones, which may otherwise add to noise and inaccuracies in the generated answers. Existing utility prediction approaches model passage utility independently, overlooking a critical aspect of multihop reasoning: the utility of a passage can be context-dependent, influenced by its relation to other passages - whether it provides complementary information or forms a crucial link in conjunction with others. In this paper, we propose a lightweight approach to model contextual passage utility, accounting for inter-passage dependencies. We fine-tune a small transformer-based model to predict passage utility scores for multihop QA. We leverage the reasoning traces from an advanced reasoning model to capture the order in which passages are used to answer a question and obtain synthetic training data. Through comprehensive experiments, we demonstrate that our utility-based scoring of retrieved passages leads to improved reranking and downstream QA performance compared to relevance-based reranking methods.

</details>


### [10] [MASim: Multilingual Agent-Based Simulation for Social Science](https://arxiv.org/abs/2512.07195)
*Xuan Zhang,Wenxuan Zhang,Anxu Wang,See-Kiong Ng,Yang Deng*

Main category: cs.CL

TL;DR: 提出了首个多语言多智能体仿真框架MASim，能够模拟跨语言社会互动和舆论演化，促进可控的计算社会科学研究。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体角色扮演多为单语言环境，无法模拟跨语言互动，这是现实社会的重要属性。

Method: 引入了MASim，一个支持多轮交互的多语言多智能体仿真框架，结合生成式智能体和多样的社会语言学特征。构建了MAPS基准，结合全球人口分布的调查问题和人口角色，实现仿真执行。

Result: MASim通过仿真跨语言文化中的公众舆论发展及媒体影响，实现了多语言仿真社会现象复现，验证了其校准性、敏感性、一致性和文化适应能力。

Conclusion: 多语言多智能体仿真是研究社会行为和语言互动的重要工具，MASim有效重现了复杂社会文化现象，强调了多语言仿真的必要性和应用价值。

Abstract: Multi-agent role-playing has recently shown promise for studying social behavior with language agents, but existing simulations are mostly monolingual and fail to model cross-lingual interaction, an essential property of real societies. We introduce MASim, the first multilingual agent-based simulation framework that supports multi-turn interaction among generative agents with diverse sociolinguistic profiles. MASim offers two key analyses: (i) global public opinion modeling, by simulating how attitudes toward open-domain hypotheses evolve across languages and cultures, and (ii) media influence and information diffusion, via autonomous news agents that dynamically generate content and shape user behavior. To instantiate simulations, we construct the MAPS benchmark, which combines survey questions and demographic personas drawn from global population distributions. Experiments on calibration, sensitivity, consistency, and cultural case studies show that MASim reproduces sociocultural phenomena and highlights the importance of multilingual simulation for scalable, controlled computational social science.

</details>


### [11] [Knowing What's Missing: Assessing Information Sufficiency in Question Answering](https://arxiv.org/abs/2512.06476)
*Akriti Jain,Aparna Garimella*

Main category: cs.CL

TL;DR: 本文提出一套结构化框架，先识别缺失信息再核实文本，提升问答系统中判断上下文信息充分性的准确性和透明度。


<details>
  <summary>Details</summary>
Motivation: 现有直接提问式方法在推理类问题中表现不佳，缺乏对信息缺失的显性判断，故提出通过先识别缺失信息再验证的隐式信号来提升判断可靠性。

Method: 该方法先生成多个缺失信息假设并达成语义共识，随后模型重新检验文本以确认信息是否确实缺失，采用结构化的识别-验证流程。

Result: 在多跳及事实类问答数据集上的评测表明，本方法能更准确判断信息充分性，且能清晰表达信息缺口。

Conclusion: 提出的Identify-then-Verify框架通过引导模型先识别缺失信息再进行验证，显著提升了对上下文信息充分性的判断准确性。

Abstract: Determining whether a provided context contains sufficient information to answer a question is a critical challenge for building reliable question-answering systems. While simple prompting strategies have shown success on factual questions, they frequently fail on inferential ones that require reasoning beyond direct text extraction. We hypothesize that asking a model to first reason about what specific information is missing provides a more reliable, implicit signal for assessing overall sufficiency. To this end, we propose a structured Identify-then-Verify framework for robust sufficiency modeling. Our method first generates multiple hypotheses about missing information and establishes a semantic consensus. It then performs a critical verification step, forcing the model to re-examine the source text to confirm whether this information is truly absent. We evaluate our method against established baselines across diverse multi-hop and factual QA datasets. The results demonstrate that by guiding the model to justify its claims about missing information, our framework produces more accurate sufficiency judgments while clearly articulating any information gaps.

</details>


### [12] [Classifying German Language Proficiency Levels Using Large Language Models](https://arxiv.org/abs/2512.06483)
*Elias-Leander Ahlers,Witold Brunsmann,Malte Schilling*

Main category: cs.CL

TL;DR: 本文探索使用大规模语言模型对德语文本进行CEFR等级分类，结合多源数据和多种方法，取得性能提升，展现了LLM在语言能力评估中的应用前景。


<details>
  <summary>Details</summary>
Motivation: 语言能力评估对于教育至关重要，有助于根据学习者需求定制教学内容。

Method: 通过构建多源多样化的CEFR标注语料库与合成数据集，采用提示工程、微调LLaMA-3-8B-Instruct模型和基于内神经状态的探测方法对德文文本进行CEFR等级分类。

Result: 在训练和评估中，提出的方法较之前方法表现出持续的性能提升。

Conclusion: 大规模语言模型在CEFR语言水平自动分类任务中表现出可靠性和可扩展性，具备实际应用潜力。

Abstract: Assessing language proficiency is essential for education, as it enables instruction tailored to learners needs. This paper investigates the use of Large Language Models (LLMs) for automatically classifying German texts according to the Common European Framework of Reference for Languages (CEFR) into different proficiency levels. To support robust training and evaluation, we construct a diverse dataset by combining multiple existing CEFR-annotated corpora with synthetic data. We then evaluate prompt-engineering strategies, fine-tuning of a LLaMA-3-8B-Instruct model and a probing-based approach that utilizes the internal neural state of the LLM for classification. Our results show a consistent performance improvement over prior methods, highlighting the potential of LLMs for reliable and scalable CEFR classification.

</details>


### [13] [ProSocialAlign: Preference Conditioned Test Time Alignment in Language Models](https://arxiv.org/abs/2512.06515)
*Somnath Banerjee,Sayan Layek,Sayantan Adak,Mykola Pechenizkiy,Animesh Mukherjee,Rima Hazra*

Main category: cs.CL

TL;DR: 本文提出了ProSocialAlign方法，在不重新训练基础模型的情况下，通过参数高效的机制引导语言模型生成安全、富有同理心且符合人类价值观的回应，显著提升了模型在多个安全评测中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型安全方法在情绪激烈或高风险场景下表现不足，单纯拒绝或盲目服从可能疏远用户或加剧风险，需一种安全且具同理心且可控的新方法。

Method: 提出了结合方向性调节（减去学习得到的“伤害向量”）和偏好感知的自回归奖励建模的框架，通过词汇顺序的受限生成，先排除有害内容，再在安全集合内优化亲社会质量。

Result: 在五个安全基准测试中，ProSocialAlign表现出优越性能，显著减少不安全生成内容并提升与人类价值观的一致性，多项评估指标均有强劲提升。

Conclusion: ProSocialAlign有效减少了语言模型的安全风险，增强了生成内容的价值对齐和用户控制能力，是提升语言模型应对复杂情绪和高风险场景安全性的实用方法。

Abstract: Current language model safety paradigms often fall short in emotionally charged or high-stakes settings, where refusal-only approaches may alienate users and naive compliance can amplify risk. We propose ProSocialAlign, a test-time, parameter-efficient framework that steers generation toward safe, empathetic, and value-aligned responses without retraining the base model. We formalize five human-centered objectives and cast safety as lexicographic constrained generation: first, applying hard constraints to eliminate harmful continuations; then optimizing for prosocial quality within the safe set. Our method combines (i) directional regulation, a harm-mitigation mechanism that subtracts a learned "harm vector" in parameter space, and (ii) preference-aware autoregressive reward modeling trained jointly across attributes with gradient conflict resolution, enabling fine-grained, user-controllable decoding. Empirical evaluations across five safety benchmarks demonstrate state-of-the-art performance, reducing unsafe leakage and boosting alignment to human values, with strong gains across multiple evaluation metrics. ProSocialAlign offers a robust and modular foundation for generating context-sensitive, safe, and human-aligned responses at inference time.

</details>


### [14] [Adapting AlignScore Mertic for Factual Consistency Evaluation of Text in Russian: A Student Abstract](https://arxiv.org/abs/2512.06586)
*Mikhail Zimin,Milyausha Shamsutdinova,Georgii Andriushchenko*

Main category: cs.CL

TL;DR: 本文提出了AlignRuScore，通过微调RuBERT模型，使AlignScore适用于俄语文本，实现了多语言事实一致性评估的基础建设，并公开了相关资源。


<details>
  <summary>Details</summary>
Motivation: 现有事实一致性评估工具主要针对英文语料，缺乏针对俄语文本的评估工具。

Method: 基于RuBERT预训练模型，结合任务特定的分类和回归头，进行了微调以适配俄语文本。

Result: 成功将统一的AlignScore事实一致性评估指标移植到俄语，验证了该方法的有效性。

Conclusion: AlignRuScore为俄语生成文本的事实一致性评估提供了有效工具，推动了多语言可靠自然语言处理的发展。

Abstract: Ensuring factual consistency in generated text is crucial for reliable natural language processing applications. However, there is a lack of evaluation tools for factual consistency in Russian texts, as existing tools primarily focus on English corpora. To bridge this gap, we introduce AlignRuScore, a comprehensive adaptation of the AlignScore metric for Russian. To adapt the metric, we fine-tuned a RuBERT-based alignment model with task-specific classification and regression heads on Russian and translated English datasets. Our results demonstrate that a unified alignment metric can be successfully ported to Russian, laying the groundwork for robust multilingual factual consistency evaluation. We release the translated corpora, model checkpoints, and code to support further research.

</details>


### [15] [The Online Discourse of Virtual Reality and Anxiety](https://arxiv.org/abs/2512.06656)
*Kwabena Yamoah,Cass Dykeman*

Main category: cs.CL

TL;DR: 本研究通过语料库语言学方法分析了关于VR治疗焦虑的网络讨论，发现核心关注点在于虚拟系统及设备，为未来临床支持和技术发展提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 理解用户对VR治疗焦虑症技术的在线讨论，从而促进该技术的效果提升和临床应用。

Method: 采用语料库语言学方法，利用Sketch Engine软件分析英语趋势语料库中关于VR与焦虑的常用词及搭配。

Result: 识别出VR、Oculus和头戴设备为讨论中最常见词汇，并发现与虚拟现实相关的介词短语搭配，揭示了设计、体验和开发等方面的信息。

Conclusion: VR技术在焦虑症治疗中的讨论主要围绕虚拟系统及物理设备展开，显示出该技术在患者关怀中的潜力。

Abstract: VR in the treatment of clinical concerns such as generalized anxiety disorder or social anxiety. VR has created additional pathways to support patient well-being and care. Understanding online discussion of what users think about this technology may further support its efficacy. The purpose of this study was to employ a corpus linguistic methodology to identify the words and word networks that shed light on the online discussion of virtual reality and anxiety. Using corpus linguistics, frequently used words in discussion along with collocation were identified by utilizing Sketch Engine software. The results of the study, based upon the English Trends corpus, identified VR, Oculus, and headset as the most frequently discussed within the VR and anxiety subcorpus. These results point to the development of the virtual system, along with the physical apparatus that makes viewing and engaging with the virtual environment possible. Additional results point to collocation of prepositional phrases such as of virtual reality, in virtual reality, and for virtual reality relating to the design, experience, and development, respectively. These findings offer new perspective on how VR and anxiety together are discussed in general discourse and offer pathways for future opportunities to support counseling needs through development and accessibility. Keywords: anxiety disorders, corpus linguistics, Sketch Engine, and virtual reality VR

</details>


### [16] [CMV-Fuse: Cross Modal-View Fusion of AMR, Syntax, and Knowledge Representations for Aspect Based Sentiment Analysis](https://arxiv.org/abs/2512.06679)
*Smitha Muthya Sudheendra,Mani Deep Cherukuri,Jaideep Srivastava*

Main category: cs.CL

TL;DR: 该论文提出一种融合四种语言视角和外部知识的多视角融合框架CMV-Fuse，大幅提升了方面情感分析效果。


<details>
  <summary>Details</summary>
Motivation: 当前的方面情感分析系统通常仅利用单一的语言视角，忽视了语言结构间复杂的交互，而人类理解语言时会综合多种视角以获取更准确的信息。

Method: 采用抽象意义表示、成分句法分析、依存句法和语义关注四种视角，结合外部知识，通过层次化门控注意力融合和结构感知的多视角对比学习机制实现多视角融合。

Result: 在标准基准测试中，CMV-Fuse在强基线基础上表现出显著提升，分析表明不同语言视角对情感分析的鲁棒性均有贡献。

Conclusion: 本文提出的CMV-Fuse框架通过融合多种语言视角显著提升了方面情感分析的性能，验证了多模态多视角融合方法的有效性。

Abstract: Natural language understanding inherently depends on integrating multiple complementary perspectives spanning from surface syntax to deep semantics and world knowledge. However, current Aspect-Based Sentiment Analysis (ABSA) systems typically exploit isolated linguistic views, thereby overlooking the intricate interplay between structural representations that humans naturally leverage. We propose CMV-Fuse, a Cross-Modal View fusion framework that emulates human language processing by systematically combining multiple linguistic perspectives. Our approach systematically orchestrates four linguistic perspectives: Abstract Meaning Representations, constituency parsing, dependency syntax, and semantic attention, enhanced with external knowledge integration. Through hierarchical gated attention fusion across local syntactic, intermediate semantic, and global knowledge levels, CMV-Fuse captures both fine-grained structural patterns and broad contextual understanding. A novel structure aware multi-view contrastive learning mechanism ensures consistency across complementary representations while maintaining computational efficiency. Extensive experiments demonstrate substantial improvements over strong baselines on standard benchmarks, with analysis revealing how each linguistic view contributes to more robust sentiment analysis.

</details>


### [17] [Mechanistic Interpretability of GPT-2: Lexical and Contextual Layers in Sentiment Analysis](https://arxiv.org/abs/2512.06681)
*Amartya Hatua*

Main category: cs.CL

TL;DR: 本文通过激活补丁系统性验证了GPT-2模型情感处理机制，发现早期层检测词汇情感，迟缓层统一整合上下文，否定了中期层特化假设。


<details>
  <summary>Details</summary>
Motivation: 探究GPT-2情感信息处理机制，验证情感计算是否遵循预期的层级架构，特别是上下文整合在中期层的作用。

Method: 采用系统性激活补丁技术，在GPT-2的所有12个Transformer层中，通过因果实验检验情感信息处理的两个阶段架构及中期层三种上下文整合假说。

Result: 本文通过对GPT-2模型的机械可解释性研究，系统性地使用激活补丁技术在12层Transformer中考察情感信息的处理机制。实验验证了情感处理的两个阶段架构：前期的词汇情感检测和中期的上下文整合。结果显示，GPT-2的早期层（第0-3层）确实作为词汇情感检测器，编码稳定且位置特异的极性信号，且与上下文基本无关。但对于中期层的上下文整合，三个假说（中期集中、现象特异性、分布式处理）均未被支持。相反，诸如否定、讽刺和领域变化等上下文现象主要在后期层（第8-11层）通过统一的非模块化机制进行整合。这一发现表明GPT-2的情感计算机制与预测的层级结构不同，强调需要进一步实验证明大型语言模型中上下文整合的具体机制。

Conclusion: GPT-2的情感处理并非如预期的中期层特化，而是早期层词汇检测与后期层非模块化的上下文整合机制。需要进一步研究大型语言模型的上下文整合。

Abstract: We present a mechanistic interpretability study of GPT-2 that causally examines how sentiment information is processed across its transformer layers. Using systematic activation patching across all 12 layers, we test the hypothesized two-stage sentiment architecture comprising early lexical detection and mid-layer contextual integration. Our experiments confirm that early layers (0-3) act as lexical sentiment detectors, encoding stable, position specific polarity signals that are largely independent of context. However, all three contextual integration hypotheses: Middle Layer Concentration, Phenomenon Specificity, and Distributed Processing are falsified. Instead of mid-layer specialization, we find that contextual phenomena such as negation, sarcasm, domain shifts etc. are integrated primarily in late layers (8-11) through a unified, non-modular mechanism. These experimental findings provide causal evidence that GPT-2's sentiment computation differs from the predicted hierarchical pattern, highlighting the need for further empirical characterization of contextual integration in large language models.

</details>


### [18] [PersonaMem-v2: Towards Personalized Intelligence via Learning Implicit User Personas and Agentic Memory](https://arxiv.org/abs/2512.06688)
*Bowen Jiang,Yuan Yuan,Maohao Shen,Zhuoqun Hao,Zhangchen Xu,Zichen Chen,Ziyi Liu,Anvesh Rao Vijjini,Jiashu He,Hanchao Yu,Radha Poovendran,Gregory Wornell,Lyle Ungar,Dan Roth,Sihao Chen,Camillo Jose Taylor*

Main category: cs.CL

TL;DR: 提出PersonaMem-v2数据集和代理记忆系统，通过强化微调显著提升大模型隐式个性化能力，实现高效且准确的用户个性化理解。


<details>
  <summary>Details</summary>
Motivation: 推动AI个性化能力发展，更好地理解和适应用户隐式偏好，解决现实交互中的长上下文推理瓶颈问题。

Method: 构建了PersonaMem-v2数据集，模拟1000次现实用户-聊天机器人交互，包含300+场景和20,000+用户偏好，并采用长上下文窗口。利用强化微调训练模型提升长上下文推理能力，实现隐式个性化理解。设计了一个代理记忆系统框架，保持可读的单一记忆随着用户交互逐渐增长。

Result: 现有前沿大模型在隐式个性化准确度仅37-48%。通过强化微调，Qwen3-4B达53%，超越GPT-5。代理记忆框架在使用16倍更少输入的情况下，达55%准确率，显著提升效果和效率。

Conclusion: PersonaMem-v2数据集和代理记忆系统为实现现实世界个性化智能提供有效路径，代理记忆通过减少上下文输入令个性化更具可扩展性。强化微调是提升隐式个性化推理能力的关键方法。

Abstract: Personalization is one of the next milestones in advancing AI capability and alignment. We introduce PersonaMem-v2, the state-of-the-art dataset for LLM personalization that simulates 1,000 realistic user-chatbot interactions on 300+ scenarios, 20,000+ user preferences, and 128k-token context windows, where most user preferences are implicitly revealed to reflect real-world interactions. Using this data, we investigate how reinforcement fine-tuning enables a model to improve its long-context reasoning capabilities for user understanding and personalization. We also develop a framework for training an agentic memory system, which maintains a single, human-readable memory that grows with each user over time.
  In our experiments, frontier LLMs still struggle with implicit personalization, achieving only 37-48% accuracy. While they support long context windows, reasoning remains the bottleneck for implicit personalization tasks. Using reinforcement fine-tuning, we successfully train Qwen3-4B to outperforms GPT-5, reaching 53% accuracy in implicit personalization. Moreover, our agentic memory framework achieves state-of-the-art 55% accuracy while using 16x fewer input tokens, relying on a 2k-token memory instead of full 32k conversation histories. These results underscore the impact of our dataset and demonstrate agentic memory as a scalable path toward real-world personalized intelligence.

</details>


### [19] [Think-While-Generating: On-the-Fly Reasoning for Personalized Long-Form Generation](https://arxiv.org/abs/2512.06690)
*Chengbing Wang,Yang Zhang,Wenjie Wang,Xiaoyan Zhao,Fuli Feng,Xiangnan He,Tat-Seng Chua*

Main category: cs.CL

TL;DR: 提出FlyThinker，通过并行推理与生成实现个性化长文本生成，提升效果和效率。


<details>
  <summary>Details</summary>
Motivation: 现有个性化长文本生成方法多为先思考后生成，难以捕获全部信息且训练效率低，因此需要一种高效动态推理方法提高个性化生成效果。

Method: 采用独立推理模型生成逐词潜在推理信息，与生成模型融合，推理与生成并行进行，训练时保证推理仅依赖之前响应，实现并行训练。

Result: 本文提出了FlyThinker，一种高效的“边思考边生成”框架，用于个性化长文本生成。该方法通过一个独立的推理模型并行生成潜在的逐词推理信息，并将其融合进生成模型，实现推理与生成的同步进行，提高了推理和生成的效率。FlyThinker避免了先前方法中“先思考后生成”所面临的静态单次推理难以捕获全部信息的问题，同时保持训练的并行性和效率。实验结果表明，FlyThinker在真实世界基准测试中实现了更好的个性化生成效果，同时保持了训练和推理的高效性。

Conclusion: FlyThinker成功实现了个性化长文本生成中的动态推理与高效训练，优于现有方法。

Abstract: Preference alignment has enabled large language models (LLMs) to better reflect human expectations, but current methods mostly optimize for population-level preferences, overlooking individual users. Personalization is essential, yet early approaches-such as prompt customization or fine-tuning-struggle to reason over implicit preferences, limiting real-world effectiveness. Recent "think-then-generate" methods address this by reasoning before response generation. However, they face challenges in long-form generation: their static one-shot reasoning must capture all relevant information for the full response generation, making learning difficult and limiting adaptability to evolving content. To address this issue, we propose FlyThinker, an efficient "think-while-generating" framework for personalized long-form generation. FlyThinker employs a separate reasoning model that generates latent token-level reasoning in parallel, which is fused into the generation model to dynamically guide response generation. This design enables reasoning and generation to run concurrently, ensuring inference efficiency. In addition, the reasoning model is designed to depend only on previous responses rather than its own prior outputs, which preserves training parallelism across different positions-allowing all reasoning tokens for training data to be produced in a single forward pass like standard LLM training, ensuring training efficiency. Extensive experiments on real-world benchmarks demonstrate that FlyThinker achieves better personalized generation while keeping training and inference efficiency.

</details>


### [20] [TopiCLEAR: Topic extraction by CLustering Embeddings with Adaptive dimensional Reduction](https://arxiv.org/abs/2512.06694)
*Aoi Fujita,Taichi Yamamoto,Yuri Nakayama,Ryota Kobayashi*

Main category: cs.CL

TL;DR: 本文提出了一种基于句子嵌入和迭代聚类的主题提取方法TopiCLEAR，成功解决了短文本主题建模难题，显著提升社交媒体和新闻文本的主题分析效果。


<details>
  <summary>Details</summary>
Motivation: 传统主题模型难以处理短且非正式的社交媒体文本，存在语义碎片化和拼写不一致问题，需开发更适合短文本的主题提取方法。

Method: 利用Sentence-BERT进行文本嵌入，初步通过高斯混合模型聚类，结合线性判别分析进行监督投影迭代优化，直至聚类收敛，且无需预处理停用词。

Result: 在四个不同数据集上的实验表明，TopiCLEAR方法相比七种基线模型，在与人工标注主题的相似度上有显著提升，且生成的主题更加易于解释。

Conclusion: 本论文提出的TopiCLEAR方法在多个数据集上表现出优越的主题提取能力，尤其适合短文本和社交媒体内容，能够生成更具解释性的主题。

Abstract: Rapid expansion of social media platforms such as X (formerly Twitter), Facebook, and Reddit has enabled large-scale analysis of public perceptions on diverse topics, including social issues, politics, natural disasters, and consumer sentiment. Topic modeling is a widely used approach for uncovering latent themes in text data, typically framed as an unsupervised classification task. However, traditional models, originally designed for longer and more formal documents, struggle with short social media posts due to limited co-occurrence statistics, fragmented semantics, inconsistent spelling, and informal language. To address these challenges, we propose a new method, TopiCLEAR: Topic extraction by CLustering Embeddings with Adaptive dimensional Reduction. Specifically, each text is embedded using Sentence-BERT (SBERT) and provisionally clustered using Gaussian Mixture Models (GMM). The clusters are then refined iteratively using a supervised projection based on linear discriminant analysis, followed by GMM-based clustering until convergence. Notably, our method operates directly on raw text, eliminating the need for preprocessing steps such as stop word removal. We evaluate our approach on four diverse datasets, 20News, AgNewsTitle, Reddit, and TweetTopic, each containing human-labeled topic information. Compared with seven baseline methods, including a recent SBERT-based method and a zero-shot generative AI method, our approach achieves the highest similarity to human-annotated topics, with significant improvements for both social media posts and online news articles. Additionally, qualitative analysis shows that our method produces more interpretable topics, highlighting its potential for applications in social media data and web content analytics.

</details>


### [21] [Parameter-Efficient Fine-Tuning with Differential Privacy for Robust Instruction Adaptation in Large Language Models](https://arxiv.org/abs/2512.06711)
*Yulin Huang,Yaxuan Luan,Jinxu Guo,Xiangchen Song,Yuchen Liu*

Main category: cs.CL

TL;DR: 提出结合差分隐私和梯度裁剪的参数高效指令微调方法，提升隐私保护与训练效率，实验证明效果优越且稳定。


<details>
  <summary>Details</summary>
Motivation: 解决大规模语言模型指令微调中的隐私保护和效率问题。

Method: 提出一种参数高效方法，将差分隐私噪声分配与梯度裁剪结合，在协同优化框架中冻结主干模型，仅更新低维投影子空间参数，并在梯度计算中引入裁剪和自适应噪声分配。

Result: 该方法在准确性、隐私预算和参数效率上均优于基线模型，并在多任务、不确定性数据条件下表现稳定。

Conclusion: 该研究丰富了差分隐私与参数高效微调的理论结合，展示了其在指令任务中的实用适应性，为复杂指令环境下的安全训练提供了可行方案。

Abstract: This study addresses the issues of privacy protection and efficiency in instruction fine-tuning of large-scale language models by proposing a parameter-efficient method that integrates differential privacy noise allocation with gradient clipping in a collaborative optimization framework. The method keeps the backbone model frozen and updates parameters through a low-dimensional projection subspace, while introducing clipping and adaptive noise allocation during gradient computation. This design reduces privacy budget consumption and ensures training stability and robustness. The unified framework combines gradient constraints, noise allocation, and parameter projection, effectively mitigating performance fluctuations and privacy risks in multi-task instruction scenarios. Experiments are conducted across hyperparameter, environment, and data sensitivity dimensions. Results show that the method outperforms baseline models in accuracy, privacy budget, and parameter efficiency, and maintains stable performance under diverse and uncertain data conditions. The findings enrich the theoretical integration of differential privacy and parameter-efficient fine-tuning and demonstrate its practical adaptability in instruction tasks, providing a feasible solution for secure training in complex instruction environments.

</details>


### [22] ["The Dentist is an involved parent, the bartender is not": Revealing Implicit Biases in QA with Implicit BBQ](https://arxiv.org/abs/2512.06732)
*Aarushi Wagh,Saniya Srivastava*

Main category: cs.CL

TL;DR: 本研究提出ImplicitBBQ基准测试，通过隐性提示检测大型语言模型中的隐性偏见，揭示了当前模型公平性评估的盲区。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要依赖显性提示来评估LLMs的偏见，而现实交流中偏见通常是通过名称、文化线索等隐性方式表现，造成评估盲点。

Method: 提出了ImplicitBBQ基准测试，扩展了Bias Benchmark for QA（BBQ），引入了6个类别的隐性保护属性进行公平性检测。

Result: GPT-4o在ImplicitBBQ上的表现明显下降，在“性取向”子类别准确率下降高达7%，其他类别也普遍表现下滑，暴露出隐性偏见问题。

Conclusion: 当前大型语言模型（LLMs）在显性偏见评估上表现尚可，但在隐性偏见方面存在显著不足。

Abstract: Existing benchmarks evaluating biases in large language models (LLMs) primarily rely on explicit cues, declaring protected attributes like religion, race, gender by name. However, real-world interactions often contain implicit biases, inferred subtly through names, cultural cues, or traits. This critical oversight creates a significant blind spot in fairness evaluation. We introduce ImplicitBBQ, a benchmark extending the Bias Benchmark for QA (BBQ) with implicitly cued protected attributes across 6 categories. Our evaluation of GPT-4o on ImplicitBBQ illustrates troubling performance disparity from explicit BBQ prompts, with accuracy declining up to 7% in the "sexual orientation" subcategory and consistent decline located across most other categories. This indicates that current LLMs contain implicit biases undetected by explicit benchmarks. ImplicitBBQ offers a crucial tool for nuanced fairness evaluation in NLP.

</details>


### [23] [A Patient-Doctor-NLP-System to contest inequality for less privileged](https://arxiv.org/abs/2512.06734)
*Subrit Dikshit,Ritu Tiwari,Priyank Jain*

Main category: cs.CL

TL;DR: 本文提出了一种紧凑高效的Transformer模型PDFTEMRA，专为资源受限医疗场景中的低资源语言和视觉障碍用户设计，兼顾性能与计算效率。


<details>
  <summary>Details</summary>
Motivation: 针对资源受限的真实世界医疗环境中，视觉障碍用户和低资源语言（如印地语）使用者在医疗辅助上的支持不足问题。

Method: 提出了PDFTEMRA模型，这是一种基于Transformer的小型架构，结合了模型蒸馏、频域调制、集成学习和随机激活模式，以降低计算成本同时保持语言理解性能。

Result: PDFTEMRA在定制的医疗问答和咨询数据集上，针对印地语和无障碍场景进行了训练和评估，表现出与最先进NLP模型相当的性能，但计算成本大幅降低。

Conclusion: PDFTEMRA适用于无障碍、包容性强的低资源医疗自然语言处理应用，能够以较低的计算开销实现与先进模型相当的效果。

Abstract: Transfer Learning (TL) has accelerated the rapid development and availability of large language models (LLMs) for mainstream natural language processing (NLP) use cases. However, training and deploying such gigantic LLMs in resource-constrained, real-world healthcare situations remains challenging. This study addresses the limited support available to visually impaired users and speakers of low-resource languages such as Hindi who require medical assistance in rural environments. We propose PDFTEMRA (Performant Distilled Frequency Transformer Ensemble Model with Random Activations), a compact transformer-based architecture that integrates model distillation, frequency-domain modulation, ensemble learning, and randomized activation patterns to reduce computational cost while preserving language understanding performance. The model is trained and evaluated on medical question-answering and consultation datasets tailored to Hindi and accessibility scenarios, and its performance is compared against standard NLP state-of-the-art model baselines. Results demonstrate that PDFTEMRA achieves comparable performance with substantially lower computational requirements, indicating its suitability for accessible, inclusive, low-resource medical NLP applications.

</details>


### [24] [Bridging Code Graphs and Large Language Models for Better Code Understanding](https://arxiv.org/abs/2512.07666)
*Zeqi Chen,Zhaoyang Chu,Yi Gui,Feng Guo,Yao Wan,Chuan Shi*

Main category: cs.CL

TL;DR: CGBridge通过引入可训练桥接模块，将代码图结构信息整合进大型语言模型，实现代码智能任务的性能大幅提升和高效推理。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在代码智能任务中因线性化的token序列限制，难以充分理解程序结构语义，且现有图增强方法存在提示长度限制或需要特定架构改动，不利于大规模指令遵循型LLM的应用，因此需要一种高效、模块化且兼容性的结构感知增强方案。

Method: 方法包括三个步骤：1) 自监督预训练代码图编码器以捕捉程序结构语义；2) 训练外部桥接模块通过跨模态注意力对齐代码、图和文本语义；3) 桥接模块生成结构感知提示注入冻结的LLM中微调完成下游任务。

Result: 本文提出了CGBridge，一种通过外部可训练桥接模块，将代码图信息增强到大型语言模型（LLMs）中的新方法。该方法通过预训练代码图编码器学习结构语义，利用跨模态注意力机制对齐代码、图和文本的语义，并生成结构感知的提示注入冻结的LLM中进行微调。实验证明，CGBridge在代码总结和代码翻译任务中相较于原模型和图增强提示方法取得显著性能提升，同时推理速度超过LoRA微调模型4倍，兼具效果和效率。

Conclusion: CGBridge有效解决了现有方法在提示长度限制和结构感知能力上的不足，显著提升了代码智能任务的表现和推理效率，验证了其作为一种结构感知增强方法的实用价值。

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance in code intelligence tasks such as code generation, summarization, and translation. However, their reliance on linearized token sequences limits their ability to understand the structural semantics of programs. While prior studies have explored graphaugmented prompting and structure-aware pretraining, they either suffer from prompt length constraints or require task-specific architectural changes that are incompatible with large-scale instructionfollowing LLMs. To address these limitations, this paper proposes CGBridge, a novel plug-and-play method that enhances LLMs with Code Graph information through an external, trainable Bridge module. CGBridge first pre-trains a code graph encoder via selfsupervised learning on a large-scale dataset of 270K code graphs to learn structural code semantics. It then trains an external module to bridge the modality gap among code, graph, and text by aligning their semantics through cross-modal attention mechanisms. Finally, the bridge module generates structure-informed prompts, which are injected into a frozen LLM, and is fine-tuned for downstream code intelligence tasks. Experiments show that CGBridge achieves notable improvements over both the original model and the graphaugmented prompting method. Specifically, it yields a 16.19% and 9.12% relative gain in LLM-as-a-Judge on code summarization, and a 9.84% and 38.87% relative gain in Execution Accuracy on code translation. Moreover, CGBridge achieves over 4x faster inference than LoRA-tuned models, demonstrating both effectiveness and efficiency in structure-aware code understanding.

</details>


### [25] [One Word Is Not Enough: Simple Prompts Improve Word Embeddings](https://arxiv.org/abs/2512.06744)
*Rajeev Ranjan*

Main category: cs.CL

TL;DR: 向词语前添加语义提示词能显著增强文本嵌入模型的词语相似度表现，无需训练，即可提升多种模型效果，超越经典静态词向量。


<details>
  <summary>Details</summary>
Motivation: 目前文本嵌入模型主要针对句子级任务设计，缺乏对孤立词语语义表现的深入理解，部分模型裸词嵌入表现不佳，研究如何提升词语层面的语义表达能力成为亟待解决的问题。

Method: 通过在目标词前预置如“meaning: {word}”或“Represent the semantic concept: {word}”的语义提示词，调整输入给文本嵌入模型，从而提升词向量的语义表示能力，最终通过标准基准测评相关度提升效果。

Result: 该论文研究了文本嵌入模型在孤立词语语义相似度评估上的表现，提出通过在词语前添加语义提示词显著提升词语相似度相关性。实验测试了7种文本嵌入模型，在SimLex-999、WordSim-353、MEN-3000三个基准上，提示词方法使Spearman相关系数最大提升0.29，有些模型对裸词表现几乎为零，但经过提示词处理后相关度提升至0.73。最优模型在SimLex-999达到0.692，超过了经典静态嵌入Word2Vec的0.40，甚至超过了LexVec的0.48，确立了文本嵌入方法的新标杆。该方法为零样本技术，无需训练，可适用于任何文本嵌入模型。

Conclusion: 在孤立词语的语义相似度任务中，简单的语义提示词预置策略显著提升了文本嵌入模型的性能，达到或超过传统静态词向量方法，且该方法零训练成本且具通用性。

Abstract: Text embedding models are designed for sentence-level applications like retrieval and semantic similarity, and are primarily evaluated on sentence-level benchmarks. Their behavior on isolated words is less understood. We show that simply prepending semantic prompts to words before embedding substantially improves word similarity correlations. Testing 7 text embedding models, including text-embedding-3-large (OpenAI), embed-english-v3.0 (Cohere), voyage-3(Voyage AI), all-mpnet-base-v2, and Qwen3-Embedding-8B, on 3 standard benchmarks (SimLex-999, WordSim-353, MEN-3000), we find that prompts like "meaning: {word}" or "Represent the semantic concept: {word}" improve Spearman correlations by up to +0.29 on SimLex-999. Some models fail completely on bare words (correlation = 0) but recover with prompts (+0.73 improvement). Our best results achieve correlation = 0.692 on SimLex-999 with embed-english-v3.0 (Cohere), correlation = 0.811 on WordSim-353, and correlation = 0.855 on MEN-3000 with text-embedding-3-large (OpenAI). These results outperform classic static embeddings like Word2Vec (correlation = 0.40) and even the best static method LexVec (correlation = 0.48) on SimLex-999, establishing a new state-of-the-art for pure embedding methods. This zero-shot technique requires no training and works with any text embedding model.

</details>


### [26] [Becoming Experienced Judges: Selective Test-Time Learning for Evaluators](https://arxiv.org/abs/2512.06751)
*Seungyeon Jwa,Daechul Ahn,Reokyoung Kim,Dongyeop Kang,Jonghyun Choi*

Main category: cs.CL

TL;DR: 提出了一种新的顺序学习自动评估方法LWE，通过自我优化和选择性更新，实现大语言模型评估器的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大型语言模型的自动评估器在推理和对齐任务中普遍使用，但通常独立处理每个样本，无法积累经验且固定使用同一提示词，忽视了样本特异性的评估需求。

Method: 提出了一种称为Learning While Evaluating (LWE)的框架，在推理时允许评估器无需训练或验证集即可顺序改进。该方法维护一个不断进化的元提示词，该提示产生针对样本的特定评估指令，并通过自我反馈进行自我优化。进而提出Selective LWE，只针对自我不一致的案例更新元提示词，提高计算效率。

Result: 在两个成对比较基准测试中，Selective LWE超过了强基线模型，证明评估器能够在连续测试中通过简单的选择性更新提升性能，主要从处理困难案例中学习。

Conclusion: LWE框架通过顺序学习和选择性更新显著提升了基于大型语言模型的自动评估性能，实现了更智能、高效的样本特异性评估。

Abstract: Automatic evaluation with large language models, commonly known as LLM-as-a-judge, is now standard across reasoning and alignment tasks. Despite evaluating many samples in deployment, these evaluators typically (i) treat each case independently, missing the opportunity to accumulate experience, and (ii) rely on a single fixed prompt for all cases, neglecting the need for sample-specific evaluation criteria. We introduce Learning While Evaluating (LWE), a framework that allows evaluators to improve sequentially at inference time without requiring training or validation sets. LWE maintains an evolving meta-prompt that (i) produces sample-specific evaluation instructions and (ii) refines itself through self-generated feedback. Furthermore, we propose Selective LWE, which updates the meta-prompt only on self-inconsistent cases, focusing computation where it matters most. This selective approach retains the benefits of sequential learning while being far more cost-effective. Across two pairwise comparison benchmarks, Selective LWE outperforms strong baselines, empirically demonstrating that evaluators can improve during sequential testing with a simple selective update, learning most from the cases they struggle with.

</details>


### [27] [From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs](https://arxiv.org/abs/2512.06776)
*Yuchuan Tian,Yuchen Liang,Jiacheng Sun,Shuo Zhang,Guangwen Yang,Yingte Shu,Sibo Fang,Tianyu Guo,Kai Han,Chao Xu,Hanting Chen,Xinghao Chen,Yunhe Wang*

Main category: cs.CL

TL;DR: 提出了一种将自回归语言模型适配为分块扩散语言模型的方法，既保持自回归模型知识，又实现分块并行生成，显著提升了7B级扩散模型的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型生成性能与生成效率的矛盾；扩散模型训练成本高且难以利用自回归模型已有知识。

Method: 将自回归模型视为块大小为1的分块扩散模型，提出了一条从自回归到分块扩散的适配路径。设计了上下文因果注意力掩码、有效的并行适配过程、辅助自回归损失以最大利用数据并保持预训练知识，以及逐步增加生成块大小。该方案与掩码分块扩散无缝结合，保证训练和推理的一致性。

Result: 基于上述方法，构建的NBDiff-7B模型（包括基础版和指令版）保持了长上下文建模和推理能力，在7B参数级别的扩散语言模型中取得了最先进的性能，在通用知识、数学及代码基准上较强对比模型表现显著提升。

Conclusion: 本研究表明，以理论指导的自回归到分块扩散的适配是一种有效且计算高效的替代方案，避免了从零训练扩散语言模型的高昂成本，同时能继承和发挥自回归模型的优势。

Abstract: Large language models (LLMs) excel at generation but dominant autoregressive (AR) decoding is inherently sequential, creating a throughput bottleneck. Diffusion Language Models (DLMs)--especially block-wise variants--enable parallel generation and intra-block bidirectional reasoning, yet training large DLMs from scratch is costly and wastes the knowledge in mature AR checkpoints. Prior "adaptation" attempts either modify logits or randomly grow attention masks to full-sequence diffusion, or simply transplant AR weights into a block-diffusion recipe, leaving a fundamental mismatch between AR causality and block-wise bidirectionality unaddressed. We reframe adaptation as a intra-paradigm path from AR to Block-Diffusion by viewing AR as Block-Diffusion with blocksize=1. Concretely, we design the pathway of adaptation as follows: we use a context-causal attention mask (causal in context, bidirectional only within the active block), an efficient parallel adaptation procedure, an auxiliary AR loss to maximize data utilization and retain pretrained knowledge, and gradual increment of the generation block size. The recipe integrates cleanly with masked block-diffusion and maintains train-inference consistency. Built on these components, NBDiff-7B (Base and Instruct) could inherit the long-context modeling and reasoning capabilities, and achieve state-of-the-art performance among the 7B-class DLMs, delivering strong gains on general-knowledge, math, and code benchmarks over strong baselines. These results demonstrate that principled AR-to-block-diffusion adaptation is an effective and compute-efficient alternative to training DLMs from scratch. Codes: https://github.com/YuchuanTian/NBDiff.

</details>


### [28] [LLM4SFC: Sequential Function Chart Generation via Large Language Models](https://arxiv.org/abs/2512.06787)
*Ofek Glick,Vladimir Tchuiev,Marah Ghoummaid,Michal Moshkovitz,Dotan Di-Castro*

Main category: cs.CL

TL;DR: 本文提出了LLM4SFC框架，实现了从自然语言描述自动生成用于工业自动化的可执行顺序功能图(SFC)，有效解决了图形化和文本化PLC语言转换的难题。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型广泛应用于结构化文本代码生成，但对图形化的IEC 61131-3标准语言SFC支持不足，且生成的代码多不可执行，不兼容工业工具链，需要一个能生成可执行SFC的自动化方案。

Method: 采用三大组件：(1)简化结构化表示以捕捉关键拓扑和内嵌的ST动作；(2) 通过微调和少样本检索增强生成技术，确保符合SFC编程规范；(3) 实时剪枝非法token，保证生成格式合法。

Result: 在真实制造项目的SFC数据集上测试，LLM4SFC表现出较高的生成成功率（75%-94%），有效连接了图形PLC语言和文本PLC语言，推动了工业编程自动化的发展。

Conclusion: LLM4SFC能够可靠生成符合语法规范的SFC程序，成功率达到75%-94%，为工业自动化编程的自动化提供了有效方案。

Abstract: While Large Language Models (LLMs) are increasingly used for synthesizing textual PLC programming languages like Structured Text (ST) code, other IEC 61131-3 standard graphical languages like Sequential Function Charts (SFCs) remain underexplored. Generating SFCs is challenging due to graphical nature and ST actions embedded within, which are not directly compatible with standard generation techniques, often leading to non-executable code that is incompatible with industrial tool-chains In this work, we introduce LLM4SFC, the first framework to receive natural-language descriptions of industrial workflows and provide executable SFCs. LLM4SFC is based on three components: (i) A reduced structured representation that captures essential topology and in-line ST and reduced textual verbosity; (ii) Fine-tuning and few-shot retrieval-augmented generation (RAG) for alignment with SFC programming conventions; and (iii) A structured generation approach that prunes illegal tokens in real-time to ensure compliance with the textual format of SFCs. We evaluate LLM4SFC on a dataset of real-world SFCs from automated manufacturing projects, using both open-source and proprietary LLMs. The results show that LLM4SFC reliably generates syntactically valid SFC programs effectively bridging graphical and textual PLC languages, achieving a generation generation success of 75% - 94%, paving the way for automated industrial programming.

</details>


### [29] [Large Language Model-Based Generation of Discharge Summaries](https://arxiv.org/abs/2512.06812)
*Tiago Rodrigues,Carla Teixeira Lopes*

Main category: cs.CL

TL;DR: 本文通过评估五种大型语言模型自动生成医学出院摘要，发现专有模型效果最佳，尽管存在幻觉和缺失问题，但仍具备实用潜力。


<details>
  <summary>Details</summary>
Motivation: 自动生成出院摘要可以显著减轻医护人员的工作负担，减少错误，并确保关键信息的易获取和可操作性。

Method: 使用五种大型语言模型（开源模型Mistral、Llama 2与专有模型GPT-3、GPT-4、Gemini 1.5 Pro），利用MIMIC-III的出院总结和笔记数据进行自动生成出院摘要。

Result: 专有模型表现优于开源模型，尤其是Gemini 1.5 Pro在单次提示下生成的摘要与标准摘要相似度最高。开源模型虽有潜力但存在幻觉和信息重复问题。临床专家的人类评估也支持专有模型生成的摘要具有实用价值。

Conclusion: 大型语言模型，尤其是专有模型，在保障数据隐私的前提下，是自动生成医学出院摘要的有前景方法。

Abstract: Discharge Summaries are documents written by medical professionals that detail a patient's visit to a care facility. They contain a wealth of information crucial for patient care, and automating their generation could significantly reduce the effort required from healthcare professionals, minimize errors, and ensure that critical patient information is easily accessible and actionable. In this work, we explore the use of five Large Language Models on this task, from open-source models (Mistral, Llama 2) to proprietary systems (GPT-3, GPT-4, Gemini 1.5 Pro), leveraging MIMIC-III summaries and notes. We evaluate them using exact-match, soft-overlap, and reference-free metrics. Our results show that proprietary models, particularly Gemini with one-shot prompting, outperformed others, producing summaries with the highest similarity to the gold-standard ones. Open-source models, while promising, especially Mistral after fine-tuning, lagged in performance, often struggling with hallucinations and repeated information. Human evaluation by a clinical expert confirmed the practical utility of the summaries generated by proprietary models. Despite the challenges, such as hallucinations and missing information, the findings suggest that LLMs, especially proprietary models, are promising candidates for automatic discharge summary generation as long as data privacy is ensured.

</details>


### [30] [CAuSE: Decoding Multimodal Classifiers using Faithful Natural Language Explanation](https://arxiv.org/abs/2512.06814)
*Dibyanayan Bandyopadhyay,Soham Bhattacharjee,Mohammed Hasanuzzaman,Asif Ekbal*

Main category: cs.CL

TL;DR: 本文提出CAuSE框架，通过因果抽象生成忠实的多模态分类器自然语言解释，有效提升了模型解释的信任度和因果忠实性。


<details>
  <summary>Details</summary>
Motivation: 多模态分类器像黑盒一样难以理解，现有的解释方法难以做到直观且忠实，要建立用户信任，需生成忠实反映模型内部决策过程的自然语言解释。

Method: 设计了CAuSE框架，通过交换干预训练形成分类器的因果抽象，从而生成忠实的自然语言解释，并提出了重新设计的因果忠实性度量标准进行验证。

Result: CAuSE在新设计的因果忠实性指标上优于现有方法，且定性分析和详尽的错误分析进一步证明其优越性和局限性。代码已开源以便复现。

Conclusion: 提出的CAuSE框架能够为预训练的多模态分类器生成忠实的自然语言解释，提升了解释的因果性和可信度，并在多数据集和多模型中表现出良好的泛化能力。

Abstract: Multimodal classifiers function as opaque black box models. While several techniques exist to interpret their predictions, very few of them are as intuitive and accessible as natural language explanations (NLEs). To build trust, such explanations must faithfully capture the classifier's internal decision making behavior, a property known as faithfulness. In this paper, we propose CAuSE (Causal Abstraction under Simulated Explanations), a novel framework to generate faithful NLEs for any pretrained multimodal classifier. We demonstrate that CAuSE generalizes across datasets and models through extensive empirical evaluations. Theoretically, we show that CAuSE, trained via interchange intervention, forms a causal abstraction of the underlying classifier. We further validate this through a redesigned metric for measuring causal faithfulness in multimodal settings. CAuSE surpasses other methods on this metric, with qualitative analysis reinforcing its advantages. We perform detailed error analysis to pinpoint the failure cases of CAuSE. For replicability, we make the codes available at https://github.com/newcodevelop/CAuSE

</details>


### [31] [AquaFusionNet: Lightweight VisionSensor Fusion Framework for Real-Time Pathogen Detection and Water Quality Anomaly Prediction on Edge Devices](https://arxiv.org/abs/2512.06848)
*Sepyan Purnama Kristanto,Lutfi Hakim,Hermansyah*

Main category: cs.CL

TL;DR: 本研究开发轻量级跨模态模型AquaFusionNet，融合显微成像与传感器数据实现低功耗、高精度饮用水微生物污染实时监测，已在印尼现场成功部署，提升水质安全保障能力。


<details>
  <summary>Details</summary>
Motivation: 现有小规模饮用水系统监测工具只能捕获污染波动的部分信息，显微成像和物理化学传感器数据需分开解读，导致实时决策不可靠，亟需统一方法实现高效准确监测。

Method: 提出轻量级跨模态框架AquaFusionNet，利用门控跨注意力机制结合显微图像与物理化学传感器数据，训练于新构建的AquaMicro12K数据集，并部署在Jetson Nano设备上实现低功耗实时监测。

Result: AquaFusionNet在印尼七个设施六个月部署，处理184万帧视频，污染检测达94.8% mAP@0.5，异常预测准确率96.3%，功耗仅4.8W，且相较其他轻量级检测器表现更优。

Conclusion: AquaFusionNet有效融合了显微图像和水质传感器数据，显著提升了小规模饮用水系统中微生物污染的实时检测准确率和可靠性。

Abstract: Evidence from many low and middle income regions shows that microbial contamination in small scale drinking water systems often fluctuates rapidly, yet existing monitoring tools capture only fragments of this behaviour. Microscopic imaging provides organism level visibility, whereas physicochemical sensors reveal shortterm changes in water chemistry; in practice, operators must interpret these streams separately, making realtime decision-making unreliable. This study introduces AquaFusionNet, a lightweight cross-modal framework that unifies both information sources inside a single edge deployable model. Unlike prior work that treats microscopic detection and water quality prediction as independent tasks, AquaFusionNet learns the statistical dependencies between microbial appearance and concurrent sensor dynamics through a gated crossattention mechanism designed specifically for lowpower hardware. The framework is trained on AquaMicro12K, a new dataset comprising 12,846 annotated 1000 micrographs curated for drinking water contexts, an area where publicly accessible microscopic datasets are scarce. Deployed for six months across seven facilities in East Java, Indonesia, the system processed 1.84 million frames and consistently detected contamination events with 94.8% mAP@0.5 and 96.3% anomaly prediction accuracy, while operating at 4.8 W on a Jetson Nano. Comparative experiments against representative lightweight detectors show that AquaFusionNet provides higher accuracy at comparable or lower power, and field results indicate that cross-modal coupling reduces common failure modes of unimodal detectors, particularly under fouling, turbidity spikes, and inconsistent illumination. All models, data, and hardware designs are released openly to facilitate replication and adaptation in decentralized water safety infrastructures.

</details>


### [32] [Rhea: Role-aware Heuristic Episodic Attention for Conversational LLMs](https://arxiv.org/abs/2512.06869)
*Wanyang Hong,Zhaoning Zhang,Yi Chen,Libo Zhang,Baihui Liu,Linbo Qiao,Zhiliang Tian,Dongsheng Li*

Main category: cs.CL

TL;DR: Rhea框架有效缓解了多轮对话中上下文衰减问题，提升了大语言模型的表现和指令遵循性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在单轮任务表现优异，但多轮对话中存在累积的上下文信息衰减，导致性能下降，亟需设计机制维护上下文完整性与指令一致性。

Method: 通过将对话历史分解为Instructional Memory和Episodic Memory两个功能独立的记忆模块，分别管理全局约束和用户交互信息，利用结构化优先机制和启发式上下文检索，有效过滤噪声，构建高信噪比的上下文。

Result: 本文针对大语言模型（LLMs）在多轮对话中表现下降的问题，定义为累积上下文衰减，提出了Rhea框架，通过将对话历史分成两个独立记忆模块（Instructional Memory和Episodic Memory），利用优先注意机制选择性整合信息，减轻注意力污染等问题。实验显示Rhea在多个多轮对话基准测试中提升了模型准确率，并保持高指令一致性。

Conclusion: Rhea提出的双记忆模块和优先注意机制显著减少了上下文相关性衰减，实现了多轮对话中更精准且一致的语言模型输出。

Abstract: Large Language Models (LLMs) have achieved remarkable performance on single-turn tasks, yet their effectiveness deteriorates in multi-turn conversations. We define this phenomenon as cumulative contextual decay - a progressive degradation of contextual integrity caused by attention pollution, dilution, and drift. To address this challenge, we propose Rhea (Role-aware Heuristic Episodic Attention), a novel framework that decouples conversation history into two functionally independent memory modules: (1) an Instructional Memory (IM) that persistently stores high-fidelity global constraints via a structural priority mechanism, and (2) an Episodic Memory (EM) that dynamically manages user-model interactions via asymmetric noise control and heuristic context retrieval. During inference, Rhea constructs a high signal-to-noise context by applying its priority attention: selectively integrating relevant episodic information while always prioritizing global instructions. To validate this approach, experiments on multiple multi-turn conversation benchmarks - including MT-Eval and Long-MT-Bench+ - show that Rhea mitigates performance decay and improves overall accuracy by 1.04 points on a 10-point scale (a 16% relative gain over strong baselines). Moreover, Rhea maintains near-perfect instruction fidelity (IAR > 8.1) across long-horizon interactions. These results demonstrate that Rhea provides a principled and effective framework for building more precise, instruction-consistent conversational LLMs.

</details>


### [33] [An Analysis of Large Language Models for Simulating User Responses in Surveys](https://arxiv.org/abs/2512.06874)
*Ziyun Yu,Yiru Zhou,Chen Zhao,Hongyi Wen*

Main category: cs.CL

TL;DR: 本研究评估LLMs模拟跨领域调查中多样用户观点的能力，提出通过CLAIMSIM增强观点多样性，但LLMs仍难准确反映不同用户背景的意见，存在固定视角和有限推理能力的挑战。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型（LLMs）在模拟多元用户意见方面的能力和局限性，特别是其在表示不同人口和文化背景用户意见时的偏见问题。

Method: 通过直接提示和链式思考提示两种方式模拟用户对跨领域调查问题的回答，并提出CLAIMSIM方法，该方法通过利用LLM的参数知识生成多样化观点作为上下文输入。

Result: 实验表明CLAIMSIM能够生成更加多样化的回答，但LLMs整体难以准确模拟用户意见。分析发现LLMs倾向于在不同人口特征间保持固定观点，且对复杂的用户特征差异推理能力有限。

Conclusion: LLMs在模拟不同用户背景的多样化意见方面存在显著局限，特别是在处理人口统计学特征的复杂差异和生成多角度观点时表现不足。

Abstract: Using Large Language Models (LLMs) to simulate user opinions has received growing attention. Yet LLMs, especially trained with reinforcement learning from human feedback (RLHF), are known to exhibit biases toward dominant viewpoints, raising concerns about their ability to represent users from diverse demographic and cultural backgrounds. In this work, we examine the extent to which LLMs can simulate human responses to cross-domain survey questions through direct prompting and chain-of-thought prompting. We further propose a claim diversification method CLAIMSIM, which elicits viewpoints from LLM parametric knowledge as contextual input. Experiments on the survey question answering task indicate that, while CLAIMSIM produces more diverse responses, both approaches struggle to accurately simulate users. Further analysis reveals two key limitations: (1) LLMs tend to maintain fixed viewpoints across varying demographic features, and generate single-perspective claims; and (2) when presented with conflicting claims, LLMs struggle to reason over nuanced differences among demographic features, limiting their ability to adapt responses to specific user profiles.

</details>


### [34] [Automated PRO-CTCAE Symptom Selection based on Prior Adverse Event Profiles](https://arxiv.org/abs/2512.06919)
*Francois Vandenhende,Anna Georgiou,Michalis Georgiou,Theodoros Psaras,Ellie Karekla*

Main category: cs.CL

TL;DR: 本文提出了一种利用历史安全数据和MedDRA语义信息，自动选择PRO-CTCAE症状子集的方法，以减少患者负担并保证信号覆盖。


<details>
  <summary>Details</summary>
Motivation: 目前PRO-CTCAE项的选择依赖于历史经验，且包含项过多会增加患者负担，项过少则可能遗漏重要安全信号。因此需要一种自动化、客观且可重复的方法优化项选择。

Method: 通过将PRO-CTCAE症状映射至MedDRA首选术语（PTs），并编码入Safeterm语义空间；结合相关性和发病率构建效用函数；利用谱分析选择与众不同且信息丰富的医疗概念子集，最后根据解释的信息排序和截断选项。

Result: 通过模拟和肿瘤学案例研究验证，该方法能够有效平衡信号覆盖和患者合规性，自动化设计过程有助于试验安全性数据分析。

Conclusion: 该自动化方法能够基于历史安全数据和MedDRA语义，客观地选择最小且全面的PRO-CTCAE项集，平衡患者负担与安全信号捕获，提升试验设计效率。

Abstract: The PRO-CTCAE is an NCI-developed patient-reported outcome system for capturing symptomatic adverse events in oncology trials. It comprises a large library drawn from the CTCAE vocabulary, and item selection for a given trial is typically guided by expected toxicity profiles from prior data. Selecting too many PRO-CTCAE items can burden patients and reduce compliance, while too few may miss important safety signals. We present an automated method to select a minimal yet comprehensive PRO-CTCAE subset based on historical safety data. Each candidate PRO-CTCAE symptom term is first mapped to its corresponding MedDRA Preferred Terms (PTs), which are then encoded into Safeterm, a high-dimensional semantic space capturing clinical and contextual diversity in MedDRA terminology. We score each candidate PRO item for relevance to the historical list of adverse event PTs and combine relevance and incidence into a utility function. Spectral analysis is then applied to the combined utility and diversity matrix to identify an orthogonal set of medical concepts that balances relevance and diversity. Symptoms are rank-ordered by importance, and a cut-off is suggested based on the explained information. The tool is implemented as part of the Safeterm trial-safety app. We evaluate its performance using simulations and oncology case studies in which PRO-CTCAE was employed. This automated approach can streamline PRO-CTCAE design by leveraging MedDRA semantics and historical data, providing an objective and reproducible method to balance signal coverage against patient burden.

</details>


### [35] [Large Language Models and Forensic Linguistics: Navigating Opportunities and Threats in the Age of Generative AI](https://arxiv.org/abs/2512.06922)
*George Mikros*

Main category: cs.CL

TL;DR: 大型语言模型既是强大分析工具，也是法医语言学面临的挑战，现有AI文本检测技术存在明显不足，需要方法学重构以确保科学性和法律可采纳性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的风格模仿和合成文本冲击传统语言鉴别理论，现有检测技术误判率高且容易被对抗攻击，造成法律采信难题。

Method: 提出采用人机混合工作流、超越二元分类的可解释检测模型，以及跨多样化人群的验证机制。

Result: 发现LLM虽能模仿风格，但与人类写作存在可侦测差异，且检测工具面临高误判和易受攻击的局限，亟需新的适应性方法。

Conclusion: 法医语言学需要重新调整方法，采用人机混合流程、可解释检测范式和多样群体的误差偏差验证，以保持科学可信度和法律认可。

Abstract: Large language models (LLMs) present a dual challenge for forensic linguistics. They serve as powerful analytical tools enabling scalable corpus analysis and embedding-based authorship attribution, while simultaneously destabilising foundational assumptions about idiolect through style mimicry, authorship obfuscation, and the proliferation of synthetic texts. Recent stylometric research indicates that LLMs can approximate surface stylistic features yet exhibit detectable differences from human writers, a tension with significant forensic implications. However, current AI-text detection techniques, whether classifier-based, stylometric, or watermarking approaches, face substantial limitations: high false positive rates for non-native English writers and vulnerability to adversarial strategies such as homoglyph substitution. These uncertainties raise concerns under legal admissibility standards, particularly the Daubert and Kumho Tire frameworks. The article concludes that forensic linguistics requires methodological reconfiguration to remain scientifically credible and legally admissible. Proposed adaptations include hybrid human-AI workflows, explainable detection paradigms beyond binary classification, and validation regimes measuring error and bias across diverse populations. The discipline's core insight, i.e., that language reveals information about its producer, remains valid but must accommodate increasingly complex chains of human and machine authorship.

</details>


### [36] [XAM: Interactive Explainability for Authorship Attribution Models](https://arxiv.org/abs/2512.06924)
*Milad Alshomary,Anisha Bhatnagar,Peter Zeng,Smaranda Muresan,Owen Rambow,Kathleen McKeown*

Main category: cs.CL

TL;DR: IXAM是一个交互式解释框架，帮助用户理解作者身份识别模型的预测，提升了模型解释的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前作者身份识别模型缺乏有效的解释工具，用户难以理解模型的预测机制。

Method: 提出IXAM框架，允许用户交互式探索模型的嵌入空间，并以多层次的写作风格特征构建预测解释。

Result: 通过用户评估，展示了IXAM在解释模型预测方面相较于预定义风格解释的优势。

Conclusion: IXAM有效提升了作者身份识别模型的可解释性，增强了用户对模型预测的理解和信任。

Abstract: We present IXAM, an Interactive eXplainability framework for Authorship Attribution Models. Given an authorship attribution (AA) task and an embedding-based AA model, our tool enables users to interactively explore the model's embedding space and construct an explanation of the model's prediction as a set of writing style features at different levels of granularity. Through a user evaluation, we demonstrate the value of our framework compared to predefined stylistic explanations.

</details>


### [37] [Progress Ratio Embeddings: An Impatience Signal for Robust Length Control in Neural Text Generation](https://arxiv.org/abs/2512.06938)
*Ivanhoé Botcazou,Tassadit Amghar,Sylvain Lamprier,Frédéric Saubion*

Main category: cs.CL

TL;DR: 提出一种基于Progress Ratio Embeddings的文本生成长度控制方法，提升长度控制的稳健性和泛华能力，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有神经语言模型虽然生成文本准确率高，但对生成文本长度的精确控制能力不足，且现有RPE方法在目标长度超出训练分布时表现不稳定。

Method: 提出了一种基于Progress Ratio Embeddings (PRE)的长度控制方法，使用连续的三角函数信号嵌入，替代传统的基于剩余令牌计数的Reverse Positional Embeddings (RPE)方法。该方法与标准Transformer架构无缝集成。

Result: PRE方法在两个新闻摘要数据集上的实验验证了其在长度控制的稳定性和生成文本准确性上的优势，且能够很好地泛化到未见过的目标长度。

Conclusion: PRE为神经语言模型提供了一种稳定且泛化能力强的长度控制手段，改进了长文本生成的精确度和稳定性，适用于标准Transformer模型。

Abstract: Modern neural language models achieve high accuracy in text generation, yet precise control over generation length remains underdeveloped. In this paper, we first investigate a recent length control method based on Reverse Positional Embeddings (RPE) and show its limits when control is requested beyond the training distribution. In particular, using a discrete countdown signal tied to the absolute remaining token count leads to instability. To provide robust length control, we introduce Progress Ratio Embeddings (PRE), as continuous embeddings tied to a trigonometric impatience signal. PRE integrates seamlessly into standard Transformer architectures, providing stable length fidelity without degrading text accuracy under standard evaluation metrics. We further show that PRE generalizes well to unseen target lengths. Experiments on two widely used news-summarization benchmarks validate these findings.

</details>


### [38] [Prompting-in-a-Series: Psychology-Informed Contents and Embeddings for Personality Recognition With Decoder-Only Models](https://arxiv.org/abs/2512.06991)
*Jing Jie Tan,Ban-Hoe Kwan,Danny Wee-Kiat Ng,Yan-Chai Hum,Anissa Mokraoui,Shih-Yu Lo*

Main category: cs.CL

TL;DR: 本文提出了基于心理学内容嵌入的PICEPR算法，通过模块化大语言模型提升个性识别性能，实现了5-15%的提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自然语言处理任务中表现突出，本文旨在利用其能力改进个性识别的准确性能，通过内容生成和处理实现更精准的个性特征提取。

Method: 提出了“Prompting-in-a-Series”算法，包含内容和嵌入两个管道，利用模块化解码器大语言模型生成或总结内容，作为个性特征提取和丰富内容生成的工具。

Result: 本文提出了一种名为PICEPR的“Prompting-in-a-Series”算法，通过内容和嵌入两个管道，利用模块化的解码器单向大语言模型来生成和总结内容，进而辅助个性识别。实验中还比较了不同闭源和开源模型的内容生成质量。PICEPR在个性识别任务上实现了5-15%的性能提升，达到了新的最先进水平。

Conclusion: PICEPR算法有效提升了个性识别性能，并在多个模型比较中表现优异，刷新了个性识别任务的性能记录。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across various natural language processing tasks. This research introduces a novel "Prompting-in-a-Series" algorithm, termed PICEPR (Psychology-Informed Contents Embeddings for Personality Recognition), featuring two pipelines: (a) Contents and (b) Embeddings. The approach demonstrates how a modularised decoder-only LLM can summarize or generate content, which can aid in classifying or enhancing personality recognition functions as a personality feature extractor and a generator for personality-rich content. We conducted various experiments to provide evidence to justify the rationale behind the PICEPR algorithm. Meanwhile, we also explored closed-source models such as \textit{gpt4o} from OpenAI and \textit{gemini} from Google, along with open-source models like \textit{mistral} from Mistral AI, to compare the quality of the generated content. The PICEPR algorithm has achieved a new state-of-the-art performance for personality recognition by 5-15\% improvement. The work repository and models' weight can be found at https://research.jingjietan.com/?q=PICEPR.

</details>


### [39] [FVA-RAG: Falsification-Verification Alignment for Mitigating Sycophantic Hallucinations](https://arxiv.org/abs/2512.07015)
*Mayank Ravishankara*

Main category: cs.CL

TL;DR: 本文提出了一种新的检索增强生成框架FVA-RAG，通过反向寻找反驳证据和双重验证机制，有效减少了大模型生成中的错误引证问题。


<details>
  <summary>Details</summary>
Motivation: 传统的检索增强生成系统容易受到检索献媚（Retrieval Sycophancy）问题影响，即检索结果倾向于支持用户偏见而非客观事实，导致模型伴随错误引用产生幻觉。

Method: 引入Falsification-Verification Alignment RAG（FVA-RAG）框架，采用对抗式检索策略生成旨在揭示反驳证据的“杀查询”，并通过双重验证机制对草拟答案和反面上下文进行权衡。

Result: FVA-RAG在针对常见误解的数据集上的初步实验表明，相较于标准RAG基线，FVA-RAG显著提升了对检索献媚幻觉的鲁棒性，有效作为推理时的“红队”筛查生成事实的准确性。

Conclusion: FVA-RAG通过转变检索策略和引入对抗性验证，显著提升了生成模型在面对错误前提查询时的事实生成准确性和鲁棒性。

Abstract: Retrieval-Augmented Generation (RAG) systems have significantly reduced hallucinations in Large Language Models (LLMs) by grounding responses in external context. However, standard RAG architectures suffer from a critical vulnerability: Retrieval Sycophancy. When presented with a query based on a false premise or a common misconception, vector-based retrievers tend to fetch documents that align with the user's bias rather than objective truth, leading the model to "hallucinate with citations."
  In this work, we introduce Falsification-Verification Alignment RAG (FVA-RAG), a framework that shifts the retrieval paradigm from Inductive Verification (seeking support) to Deductive Falsification (seeking disproof). Unlike existing "Self-Correction" methods that rely on internal consistency, FVA-RAG deploys a distinct Adversarial Retrieval Policy that actively generates "Kill Queries"-targeted search terms designed to surface contradictory evidence. We introduce a dual-verification mechanism that explicitly weighs the draft answer against this "Anti-Context." Preliminary experiments on a dataset of common misconceptions demonstrate that FVA-RAG significantly improves robustness against sycophantic hallucinations compared to standard RAG baselines, effectively acting as an inference-time "Red Team" for factual generation.

</details>


### [40] [Replicating TEMPEST at Scale: Multi-Turn Adversarial Attacks Against Trillion-Parameter Frontier Models](https://arxiv.org/abs/2512.07059)
*Richard Young*

Main category: cs.CL

TL;DR: 本研究系统评估了大型语言模型对多轮对抗攻击的脆弱性，发现规模不决定鲁棒性，推理模式能有效增强安全性，现有对齐技术依然难以完全防御此类攻击。


<details>
  <summary>Details</summary>
Motivation: 尽管在安全对齐上投入巨大，但大型语言模型对复杂多轮对抗攻击的脆弱性缺乏系统评估，且模型规模及推理模式对鲁棒性的影响尚不明确。

Method: 采用TEMPEST多轮攻击框架，对来自八个厂商的十个前沿模型，在1000种有害行为上进行对抗测试，共生成约97000次API查询，并通过独立安全分类器自动评估。

Result: 六个模型的攻击成功率高达96%-100%，四个模型表现出一定抵抗力，攻击成功率42%-78%；在相同架构下启用延伸推理显著降低攻击成功率，从97%降至42%。

Conclusion: 当前的大型语言模型无论规模大小，在面对适应性多轮对抗攻击时仍存在显著脆弱性，安全对齐质量在不同厂商间差异较大。推理模式（思考模式）能显著提升模型的对抗鲁棒性，降低攻击成功率。

Abstract: Despite substantial investment in safety alignment, the vulnerability of large language models to sophisticated multi-turn adversarial attacks remains poorly characterized, and whether model scale or inference mode affects robustness is unknown. This study employed the TEMPEST multi-turn attack framework to evaluate ten frontier models from eight vendors across 1,000 harmful behaviors, generating over 97,000 API queries across adversarial conversations with automated evaluation by independent safety classifiers. Results demonstrated a spectrum of vulnerability: six models achieved 96% to 100% attack success rate (ASR), while four showed meaningful resistance, with ASR ranging from 42% to 78%; enabling extended reasoning on identical architecture reduced ASR from 97% to 42%. These findings indicate that safety alignment quality varies substantially across vendors, that model scale does not predict adversarial robustness, and that thinking mode provides a deployable safety enhancement. Collectively, this work establishes that current alignment techniques remain fundamentally vulnerable to adaptive multi-turn attacks regardless of model scale, while identifying deliberative inference as a promising defense direction.

</details>


### [41] [SETUP: Sentence-level English-To-Uniform Meaning Representation Parser](https://arxiv.org/abs/2512.07068)
*Emma Markle,Javier Gutierrez Bach,Shira Wein*

Main category: cs.CL

TL;DR: 本文提出两种英语文本到UMR的解析方法，最佳模型达91分，显著提升自动解析效果。


<details>
  <summary>Details</summary>
Motivation: 为了推动UMR的实际应用和语言技术进步，尤其在低资源语言领域，需要开发准确的文本到UMR自动解析器，以实现大规模高质量UMR图的生成。

Method: 采用微调现有AMR解析器和利用UD转换器两种方法进行文本到UMR的解析，基于已有工作进行改进。

Result: 本文提出了两种将英语文本解析为统一语义表示（UMR）图的方法，一种是对抽象意义表示（AMR）解析器的微调，另一种则利用通用依存句法（UD）转换器作为基线。该工作的最佳模型SETUP在AnCast和SMATCH++指标上分别达到了84和91分，显著提升了自动UMR解析的性能。

Conclusion: 通过微调AMR解析器和利用UD转换，两种方法显著提升了文本到UMR的自动解析性能，最优模型实现了高准确率。

Abstract: Uniform Meaning Representation (UMR) is a novel graph-based semantic representation which captures the core meaning of a text, with flexibility incorporated into the annotation schema such that the breadth of the world's languages can be annotated (including low-resource languages). While UMR shows promise in enabling language documentation, improving low-resource language technologies, and adding interpretability, the downstream applications of UMR can only be fully explored when text-to-UMR parsers enable the automatic large-scale production of accurate UMR graphs at test time. Prior work on text-to-UMR parsing is limited to date. In this paper, we introduce two methods for English text-to-UMR parsing, one of which fine-tunes existing parsers for Abstract Meaning Representation and the other, which leverages a converter from Universal Dependencies, using prior work as a baseline. Our best-performing model, which we call SETUP, achieves an AnCast score of 84 and a SMATCH++ score of 91, indicating substantial gains towards automatic UMR parsing.

</details>


### [42] [Do Large Language Models Truly Understand Cross-cultural Differences?](https://arxiv.org/abs/2512.07075)
*Shiwei Guo,Sihang Jiang,Qianxi He,Yanghua Xiao,Jiaqing Liang,Bi Yude,Minggui He,Shimin Tao,Li Zhang*

Main category: cs.CL

TL;DR: 本文提出了SAGE基准测试，通过情景式设计和跨文化核心概念对齐，评估大语言模型的跨文化理解与推理能力，涵盖九个维度和多种真实场景，揭示了当前模型在跨文化推理上的系统性不足。


<details>
  <summary>Details</summary>
Motivation: 现有评测工具缺乏情景背景、跨文化概念映射不足、深度文化推理能力有限，难以全面评估大语言模型的跨文化理解能力。

Method: 通过基于文化理论划分的九个跨文化能力维度，收集210个核心概念，设计4530个测试题目，涵盖15个真实场景，构建了SAGE场景式基准测试，并验证其可迁移性。

Result: SAGE基准揭露了模型在多个跨文化维度与场景下的弱点和系统性局限，虽然模型有所进步，但仍无法达到细致的跨文化理解要求。

Conclusion: 实验结果表明，目前的大语言模型在跨文化理解与推理方面仍存在明显不足，距离真正的细腻跨文化理解还有较大差距。

Abstract: In recent years, large language models (LLMs) have demonstrated strong performance on multilingual tasks. Given its wide range of applications, cross-cultural understanding capability is a crucial competency. However, existing benchmarks for evaluating whether LLMs genuinely possess this capability suffer from three key limitations: a lack of contextual scenarios, insufficient cross-cultural concept mapping, and limited deep cultural reasoning capabilities. To address these gaps, we propose SAGE, a scenario-based benchmark built via cross-cultural core concept alignment and generative task design, to evaluate LLMs' cross-cultural understanding and reasoning. Grounded in cultural theory, we categorize cross-cultural capabilities into nine dimensions. Using this framework, we curated 210 core concepts and constructed 4530 test items across 15 specific real-world scenarios, organized under four broader categories of cross-cultural situations, following established item design principles. The SAGE dataset supports continuous expansion, and experiments confirm its transferability to other languages. It reveals model weaknesses across both dimensions and scenarios, exposing systematic limitations in cross-cultural reasoning. While progress has been made, LLMs are still some distance away from reaching a truly nuanced cross-cultural understanding. In compliance with the anonymity policy, we include data and code in the supplement materials. In future versions, we will make them publicly available online.

</details>


### [43] [Leveraging KV Similarity for Online Structured Pruning in LLMs](https://arxiv.org/abs/2512.07090)
*Jungmin Lee,Gwangeun Byeon,Yulhwa Kim,Seokin Hong*

Main category: cs.CL

TL;DR: 本文提出了一种在线轻量级结构化剪枝方法——Token Filtering，通过在推理时直接判断token冗余来减少计算量，无需离线校准数据，并设计了方差感知融合策略增强稳定性。在多个大型语言模型和多项任务中均表现出优越的剪枝效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于离线校准数据的剪枝方法普遍存在泛化能力差和剪枝不稳定的问题，亟需一种无需校准且在线动态判断的重要性剪枝方案。

Method: 提出了基于联合键值相似度的token冗余度衡量方法，在推理过程中动态跳过冗余token的注意力计算；设计了方差感知的融合策略自适应调整多头注意力中的相似度权重，以稳定保留重要token。

Result: 在LLaMA-2（7B/13B）、LLaMA-3（8B）和Mistral（7B）模型上进行的大量实验显示，Token Filtering在保持常识推理准确率和应对MMLU等挑战性任务时，在50%剪枝比下性能损失极小，优于以往方法。

Conclusion: Token Filtering通过在线动态判断token重要性和跳过冗余注意力计算，实现了高比例剪枝下模型性能的稳定保持，显著优于现有结构化剪枝方法。

Abstract: Pruning has emerged as a promising direction for accelerating large language model (LLM) inference, yet existing approaches often suffer from instability because they rely on offline calibration data that may not generalize across inputs. In this work, we introduce Token Filtering, a lightweight online structured pruning technique that makes pruning decisions directly during inference without any calibration data. The key idea is to measure token redundancy via joint key-value similarity and skip redundant attention computations, thereby reducing inference cost while preserving critical information. To further enhance stability, we design a variance-aware fusion strategy that adaptively weights key and value similarity across heads, ensuring that informative tokens are retained even under high pruning ratios. This design introduces no additional memory overhead and provides a more reliable criterion for token importance. Extensive experiments on LLaMA-2 (7B/13B), LLaMA-3 (8B), and Mistral (7B) demonstrate that Token Filtering consistently outperforms prior structured pruning methods, preserving accuracy on commonsense reasoning benchmarks and maintaining strong performance on challenging tasks such as MMLU, even with 50% pruning.

</details>


### [44] [DART: Leveraging Multi-Agent Disagreement for Tool Recruitment in Multimodal Reasoning](https://arxiv.org/abs/2512.07132)
*Nithin Sivakumaran,Justin Chih-Yao Chen,David Wan,Yue Zhang,Jaehong Yoon,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.CL

TL;DR: DART通过多代理分歧驱动工具调用提升视觉语言模型表现，在多项任务和应用领域均表现优异。


<details>
  <summary>Details</summary>
Motivation: 在多代理视觉模型中，选择合适的工具进行调用及其时机是一个挑战。

Method: 提出DART多代理框架，利用多个视觉代理之间的分歧来识别能解决分歧的视觉工具，使用聚合代理根据工具信息和代理输出选择最佳答案。

Result: DART在四个基准测试中优于多代理辩论及单代理工具调用框架，在A-OKVQA和MMMU数据集上分别提升3.4%和2.4%，在M3D医疗数据集提升1.3%。

Conclusion: DART能够有效利用多样化工具解决代理间分歧，促进多代理讨论，并适应应用领域中新工具的使用。

Abstract: Specialized visual tools can augment large language models or vision language models with expert knowledge (e.g., grounding, spatial reasoning, medical knowledge, etc.), but knowing which tools to call (and when to call them) can be challenging. We introduce DART, a multi-agent framework that uses disagreements between multiple debating visual agents to identify useful visual tools (e.g., object detection, OCR, spatial reasoning, etc.) that can resolve inter-agent disagreement. These tools allow for fruitful multi-agent discussion by introducing new information, and by providing tool-aligned agreement scores that highlight agents in agreement with expert tools, thereby facilitating discussion. We utilize an aggregator agent to select the best answer by providing the agent outputs and tool information. We test DART on four diverse benchmarks and show that our approach improves over multi-agent debate as well as over single agent tool-calling frameworks, beating the next-strongest baseline (multi-agent debate with a judge model) by 3.4% and 2.4% on A-OKVQA and MMMU respectively. We also find that DART adapts well to new tools in applied domains, with a 1.3% improvement on the M3D medical dataset over other strong tool-calling, single agent, and multi-agent baselines. Additionally, we measure text overlap across rounds to highlight the rich discussion in DART compared to existing multi-agent methods. Finally, we study the tool call distribution, finding that diverse tools are reliably used to help resolve disagreement.

</details>


### [45] [GUMBridge: a Corpus for Varieties of Bridging Anaphora](https://arxiv.org/abs/2512.07134)
*Lauren Levine,Amir Zeldes*

Main category: cs.CL

TL;DR: 本文介绍了GUMBridge，一个涵盖16种英语文体的桥接指代资源，并验证了其注释质量及使用多款大型语言模型的基线性能，指出桥接指代解析依然艰难。


<details>
  <summary>Details</summary>
Motivation: 现有的英语桥接指代资源规模小、现象覆盖有限、文体单一，难以支持深入研究。

Method: 提出GUMBridge资源，包含16种英语不同文体的桥接指代标注，并进行了注释质量评估和基线模型性能测试。

Result: 构建了涵盖多样文体和细粒度子类型的桥接指代资源，评估了注释质量，基于多种大型语言模型进行了基线性能测试，发现桥接解析和子类型分类仍具有挑战性。

Conclusion: 尽管提出了涵盖广泛的桥接指代数据集，桥接指代的自动解析和细分类别识别依然是当前大型语言模型难以高效解决的任务。

Abstract: Bridging is an anaphoric phenomenon where the referent of an entity in a discourse is dependent on a previous, non-identical entity for interpretation, such as in "There is 'a house'. 'The door' is red," where the door is specifically understood to be the door of the aforementioned house. While there are several existing resources in English for bridging anaphora, most are small, provide limited coverage of the phenomenon, and/or provide limited genre coverage. In this paper, we introduce GUMBridge, a new resource for bridging, which includes 16 diverse genres of English, providing both broad coverage for the phenomenon and granular annotations for the subtype categorization of bridging varieties. We also present an evaluation of annotation quality and report on baseline performance using open and closed source contemporary LLMs on three tasks underlying our data, showing that bridging resolution and subtype classification remain difficult NLP tasks in the age of LLMs.

</details>


### [46] [NeSTR: A Neuro-Symbolic Abductive Framework for Temporal Reasoning in Large Language Models](https://arxiv.org/abs/2512.07218)
*Feng Liang,Weixin Zeng,Runhao Zhao,Xiang Zhao*

Main category: cs.CL

TL;DR: 本文提出NeSTR框架，融合符号编码和反思推理，显著提升了大语言模型的时间推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在处理复杂时间推理任务时存在不足，符号方法和反思机制各自有优缺点，导致模型无法充分利用时间相关信息，推理结果可能不准确。

Method: NeSTR框架通过符号编码显式保存时间关系，利用逻辑验证保证一致性，并结合溯因反思纠正推理错误，实现混合反思推理机制。

Result: 提出Neuro-Symbolic Temporal Reasoning (NeSTR)框架，结合结构化符号表示与混合反思推理，提升了LLM的时间敏感性和推理一致性，在多项时间问答基准测试中展现出优异的零样本性能。

Conclusion: NeSTR通过神经符号融合，有效增强了大语言模型的时间推理能力，实现了更加准确和一致的时间相关推理，且无需微调。

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, temporal reasoning, particularly under complex temporal constraints, remains a major challenge. To this end, existing approaches have explored symbolic methods, which encode temporal structure explicitly, and reflective mechanisms, which revise reasoning errors through multi-step inference. Nonetheless, symbolic approaches often underutilize the reasoning capabilities of LLMs, while reflective methods typically lack structured temporal representations, which can result in inconsistent or hallucinated reasoning. As a result, even when the correct temporal context is available, LLMs may still misinterpret or misapply time-related information, leading to incomplete or inaccurate answers. To address these limitations, in this work, we propose Neuro-Symbolic Temporal Reasoning (NeSTR), a novel framework that integrates structured symbolic representations with hybrid reflective reasoning to enhance the temporal sensitivity of LLM inference. NeSTR preserves explicit temporal relations through symbolic encoding, enforces logical consistency via verification, and corrects flawed inferences using abductive reflection. Extensive experiments on diverse temporal question answering benchmarks demonstrate that NeSTR achieves superior zero-shot performance and consistently improves temporal reasoning without any fine-tuning, showcasing the advantage of neuro-symbolic integration in enhancing temporal understanding in large language models.

</details>


### [47] [Ensembling LLM-Induced Decision Trees for Explainable and Robust Error Detection](https://arxiv.org/abs/2512.07246)
*Mengqi Wang,Jianwei Wang,Qing Liu,Xiwei Xu,Zhenchang Xing,Liming Zhu,Wenjie Zhang*

Main category: cs.CL

TL;DR: 本文提出利用大语言模型诱导决策树及集成学习的错误检测方法，提升了数据错误检测的可解释性和鲁棒性，在精度上较现有方法有显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的错误检测方法采用黑盒直接标注细胞是否错误，存在缺乏结果可解释性和对提示敏感导致输出不稳定的缺点，亟需提出一种既能利用大语言模型知识又具备可解释性和鲁棒性的错误检测方法。

Method: 本文基于大语言模型诱导决策树的方法（TreeED），通过设计包含规则节点、图神经网络节点和叶节点的决策树结构，实现了逐步且可解释的错误检测流程；进一步通过不确定采样构造多个子集，利用期望最大化算法估计树的可靠性并优化集成决策，实现了更鲁棒的错误检测方法（ForestED）。

Result: 本文方法在多个数据集上通过大量实验证明，相较于最优基线方法，F1分数平均提升16.1%，同时具备较强的可解释性和鲁棒性，能有效解决传统方法的不足。

Conclusion: 本文提出的基于大语言模型诱导决策树的错误检测方法（TreeED）及其集成版（ForestED）解决了传统LLM错误检测方法缺乏可解释性和鲁棒性的问题，显著提升了错误检测的性能，实验表明方法在准确性、可解释性和鲁棒性方面均优于现有最佳方法，F1分数平均提高了16.1%。

Abstract: Error detection (ED), which aims to identify incorrect or inconsistent cell values in tabular data, is important for ensuring data quality. Recent state-of-the-art ED methods leverage the pre-trained knowledge and semantic capability embedded in large language models (LLMs) to directly label whether a cell is erroneous. However, this LLM-as-a-labeler pipeline (1) relies on the black box, implicit decision process, thus failing to provide explainability for the detection results, and (2) is highly sensitive to prompts, yielding inconsistent outputs due to inherent model stochasticity, therefore lacking robustness. To address these limitations, we propose an LLM-as-an-inducer framework that adopts LLM to induce the decision tree for ED (termed TreeED) and further ensembles multiple such trees for consensus detection (termed ForestED), thereby improving explainability and robustness. Specifically, based on prompts derived from data context, decision tree specifications and output requirements, TreeED queries the LLM to induce the decision tree skeleton, whose root-to-leaf decision paths specify the stepwise procedure for evaluating a given sample. Each tree contains three types of nodes: (1) rule nodes that perform simple validation checks (e.g., format or range), (2) Graph Neural Network (GNN) nodes that capture complex patterns (e.g., functional dependencies), and (3) leaf nodes that output the final decision types (error or clean). Furthermore, ForestED employs uncertainty-based sampling to obtain multiple row subsets, constructing a decision tree for each subset using TreeED. It then leverages an Expectation-Maximization-based algorithm that jointly estimates tree reliability and optimizes the consensus ED prediction. Extensive xperiments demonstrate that our methods are accurate, explainable and robust, achieving an average F1-score improvement of 16.1% over the best baseline.

</details>


### [48] [TeluguST-46: A Benchmark Corpus and Comprehensive Evaluation for Telugu-English Speech Translation](https://arxiv.org/abs/2512.07265)
*Bhavana Akkiraju,Srihari Bandarupalli,Swathi Sambangi,Vasavi Ravuri,R Vijaya Saraswathi,Anil Kumar Vuppala*

Main category: cs.CL

TL;DR: 本文提出了泰卢固语-英语语音翻译基准，展示了端到端模型在低资源条件下的竞争力，并对多种评价指标进行了实证分析。


<details>
  <summary>Details</summary>
Motivation: 泰卢固语作为一种形态丰富的语言，尽管拥有超过8000万使用者，但其语音翻译研究仍严重缺乏。

Method: 从46小时经人工验证的CSTD语料中构建训练集，系统比较级联(IndicWhisper+IndicMT)和端到端(SeamlessM4T微调)架构，评估多种翻译质量指标与人工评分的相关性。

Result: 构建了一个高质量的泰卢固语-英语语音翻译基准，比较了级联与端到端架构，发现端到端模型在充分调参并具备一定平行数据时可达到接近级联模型的性能；同时评估了多种自动评估指标，传统指标在该任务中优于BERTScore。

Conclusion: 端到端语音翻译系统在低资源语言条件下，通过适当调参和适量平行数据，可以实现与级联系统相当的翻译性能，同时传统自动评估指标更适合评估泰卢固语-英语翻译质量。

Abstract: Despite Telugu being spoken by over 80 million people, speech translation research for this morphologically rich language remains severely underexplored. We address this gap by developing a high-quality Telugu--English speech translation benchmark from 46 hours of manually verified CSTD corpus data (30h/8h/8h train/dev/test split). Our systematic comparison of cascaded versus end-to-end architectures shows that while IndicWhisper + IndicMT achieves the highest performance due to extensive Telugu-specific training data, finetuned SeamlessM4T models demonstrate remarkable competitiveness despite using significantly less Telugu-specific training data. This finding suggests that with careful hyperparameter tuning and sufficient parallel data (potentially less than 100 hours), end-to-end systems can achieve performance comparable to cascaded approaches in low-resource settings. Our metric reliability study evaluating BLEU, METEOR, ChrF++, ROUGE-L, TER, and BERTScore against human judgments reveals that traditional metrics provide better quality discrimination than BERTScore for Telugu--English translation. The work delivers three key contributions: a reproducible Telugu--English benchmark, empirical evidence of competitive end-to-end performance potential in low-resource scenarios, and practical guidance for automatic evaluation in morphologically complex language pairs.

</details>


### [49] [Efficient ASR for Low-Resource Languages: Leveraging Cross-Lingual Unlabeled Data](https://arxiv.org/abs/2512.07277)
*Srihari Bandarupalli,Bhavana Akkiraju,Charan Devarakonda,Vamsiraghusimha Narsinga,Anil Kumar Vuppala*

Main category: cs.CL

TL;DR: 本文通过跨语言持续预训练和未标注数据，构建小规模高效模型，实现低资源语言自动语音识别的新突破，减少对大规模数据和计算资源依赖。


<details>
  <summary>Details</summary>
Motivation: 传统ASR模型在低资源语言中受限于标注数据稀缺及计算资源限制，亟需寻找无需大量标注数据和规模超大的模型的替代方案。

Method: 通过跨语言的持续预训练，利用3,000小时的多语种未标注数据集和形态学感知的分词技术，构建了一个3亿参数的模型。

Result: 该模型在波斯语上优于参数规模更大的Whisper Large v3，在阿拉伯语和乌尔都语上也取得了有竞争力的表现，验证了小模型结合相关数据和预训练的重要性。

Conclusion: 该论文结论指出，在低资源语言的自动语音识别(ASR)中，模型性能提升主要依赖于数据的相关性和战略性预训练，而非仅仅依靠模型规模的扩大。

Abstract: Automatic speech recognition for low-resource languages remains fundamentally constrained by the scarcity of labeled data and computational resources required by state-of-the-art models. We present a systematic investigation into cross-lingual continuous pretraining for low-resource languages, using Perso-Arabic languages (Persian, Arabic, and Urdu) as our primary case study. Our approach demonstrates that strategic utilization of unlabeled speech data can effectively bridge the resource gap without sacrificing recognition accuracy. We construct a 3,000-hour multilingual corpus through a scalable unlabeled data collection pipeline and employ targeted continual pretraining combined with morphologically-aware tokenization to develop a 300M parameter model that achieves performance comparable to systems 5 times larger. Our model outperforms Whisper Large v3 (1.5B parameters) on Persian and achieves competitive results on Arabic and Urdu despite using significantly fewer parameters and substantially less labeled data. These findings challenge the prevailing assumption that ASR quality scales primarily with model size, revealing instead that data relevance and strategic pretraining are more critical factors for low-resource scenarios. This work provides a practical pathway toward inclusive speech technology, enabling effective ASR for underrepresented languages without dependence on massive computational infrastructure or proprietary datasets.

</details>


### [50] [Investigating Training and Generalization in Faithful Self-Explanations of Large Language Models](https://arxiv.org/abs/2512.07288)
*Tomoki Doi,Masaru Isonuma,Hitomi Yanaka*

Main category: cs.CL

TL;DR: 通过训练，大型语言模型在多任务多风格下自我解释的忠实度均得以提升，且解释风格间具有泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型自我解释的忠实度较低，且如何提升忠实度研究不足，同时不同解释风格的忠实度提升是否具有普适性尚不明确。

Method: 构建基于特征归因方法的一词限制解释，作为伪忠实的自我解释，并利用这些解释对指令调整模型进行持续学习。

Result: 训练显著提升了自我解释的忠实度，且该提升在所有分类任务和解释风格中均存在，并能部分推广至多词解释设置和未见任务。三种解释风格间表现出一致的交叉风格泛化。

Conclusion: 训练基于伪忠实解释的自我解释机制，可以显著提升模型的解释忠实度，且这种提升具有跨任务及跨风格的泛化潜力，促进更广泛的忠实自我解释能力发展。

Abstract: Large language models have the potential to generate explanations for their own predictions in a variety of styles based on user instructions. Recent research has examined whether these self-explanations faithfully reflect the models' actual behavior and has found that they often lack faithfulness. However, the question of how to improve faithfulness remains underexplored. Moreover, because different explanation styles have superficially distinct characteristics, it is unclear whether improvements observed in one style also arise when using other styles. This study analyzes the effects of training for faithful self-explanations and the extent to which these effects generalize, using three classification tasks and three explanation styles. We construct one-word constrained explanations that are likely to be faithful using a feature attribution method, and use these pseudo-faithful self-explanations for continual learning on instruction-tuned models. Our experiments demonstrate that training can improve self-explanation faithfulness across all classification tasks and explanation styles, and that these improvements also show signs of generalization to the multi-word settings and to unseen tasks. Furthermore, we find consistent cross-style generalization among three styles, suggesting that training may contribute to a broader improvement in faithful self-explanation ability.

</details>


### [51] [Multilingual corpora for the study of new concepts in the social sciences and humanities:](https://arxiv.org/abs/2512.07367)
*Revekka Kyriakoglou,Anna Pappa*

Main category: cs.CL

TL;DR: 通过自动提取与筛选文本，结合专家词库，构建了支持新兴人文社科概念研究的多语种语料库。


<details>
  <summary>Details</summary>
Motivation: 支持人文社会科学中新兴概念的研究，构建一个多语种语料库。

Method: 结合自动提取的公司网站文本和经过筛选的年报，进行语言检测、内容过滤、相关片段提取及结构化元数据补充。基于专家词库，提取上下文句块并标注主题类别以便监督分类任务。

Result: 构建了一个可复现且可扩展的多语种语料库资源，适合分析词汇变异性和支持自然语言处理任务的数据生成。

Conclusion: 该混合方法有效地支持了新兴概念的语料建设，为词汇分析和机器学习提供了强大的数据基础。

Abstract: This article presents a hybrid methodology for building a multilingual corpus designed to support the study of emerging concepts in the humanities and social sciences (HSS), illustrated here through the case of ``non-technological innovation''. The corpus relies on two complementary sources: (1) textual content automatically extracted from company websites, cleaned for French and English, and (2) annual reports collected and automatically filtered according to documentary criteria (year, format, duplication). The processing pipeline includes automatic language detection, filtering of non-relevant content, extraction of relevant segments, and enrichment with structural metadata. From this initial corpus, a derived dataset in English is created for machine learning purposes. For each occurrence of a term from the expert lexicon, a contextual block of five sentences is extracted (two preceding and two following the sentence containing the term). Each occurrence is annotated with the thematic category associated with the term, enabling the construction of data suitable for supervised classification tasks. This approach results in a reproducible and extensible resource, suitable both for analyzing lexical variability around emerging concepts and for generating datasets dedicated to natural language processing applications.

</details>


### [52] [Training Language Models to Use Prolog as a Tool](https://arxiv.org/abs/2512.07407)
*Niklas Mellgren,Peter Schneider-Kamp,Lukas Galke Poech*

Main category: cs.CL

TL;DR: 通过强化学习微调语言模型调用外部Prolog工具进行推理验证，大幅提升模型推理的可靠性和泛化性能，适用于安全关键领域。


<details>
  <summary>Details</summary>
Motivation: 语言模型在推理过程中常产生看似合理但错误的结果，难以验证，为了提升模型推理的可靠性和可验证性，尝试将Prolog作为外部工具进行验证计算。

Method: 采用Group Relative Policy Optimization (GRPO)方法对Qwen2.5-3B-Instruct模型进行微调，利用经过清洗的GSM8K-Prolog-Prover数据集，同时调整提示结构、奖励组成以及推理协议（单次、best-of-N及两种agentic模式）。

Result: 强化学习微调方法优于监督微调，3B模型在零样本MMLU任务中表现可与7B少样本方法相媲美。联合调优提示、奖励和推理协议有助于形成更好的程序语法和逻辑；best-of-N加外部Prolog验证提高准确率；agentic推理和内部修复提升零样本泛化能力。

Conclusion: 将模型推理基础建立在形式验证系统上，可以显著提高推理的可靠性与可审计性，对安全关键应用具有重要意义。源码已公开便于复现。

Abstract: Ensuring reliable tool use is critical for safe agentic AI systems. Language models frequently produce unreliable reasoning with plausible but incorrect solutions that are difficult to verify. To address this, we investigate fine-tuning models to use Prolog as an external tool for verifiable computation. Using Group Relative Policy Optimization (GRPO), we fine-tune Qwen2.5-3B-Instruct on a cleaned GSM8K-Prolog-Prover dataset while varying (i) prompt structure, (ii) reward composition (execution, syntax, semantics, structure), and (iii) inference protocol: single-shot, best-of-N, and two agentic modes where Prolog is invoked internally or independently. Our reinforcement learning approach outperforms supervised fine-tuning, with our 3B model achieving zero-shot MMLU performance comparable to 7B few-shot results. Our findings reveal that: 1) joint tuning of prompt, reward, and inference shapes program syntax and logic; 2) best-of-N with external Prolog verification maximizes accuracy on GSM8K; 3) agentic inference with internal repair yields superior zero-shot generalization on MMLU-Stem and MMLU-Pro. These results demonstrate that grounding model reasoning in formal verification systems substantially improves reliability and auditability for safety-critical applications. The source code for reproducing our experiments is available under https://github.com/niklasmellgren/grpo-prolog-inference

</details>


### [53] [Persian-Phi: Efficient Cross-Lingual Adaptation of Compact LLMs via Curriculum Learning](https://arxiv.org/abs/2512.07454)
*Amir Mohammad Akhlaghi,Amirhossein Shabani,Mostafa Abdolmaleki,Saeed Reza Kheradpisheh*

Main category: cs.CL

TL;DR: 该论文介绍了一种低成本、有效的方法，将单语英语模型成功转化为支持波斯语的模型，推动了低资源语言的AI民主化。


<details>
  <summary>Details</summary>
Motivation: 训练大型语言模型（LLMs）以支持低资源语言所需的高计算成本阻碍了人工智能的普及。

Method: 基于Microsoft Phi-3 Mini单语模型，设计“预热”阶段利用双语叙述数据对齐嵌入，随后进行持续预训练和参数高效微调（PEFT），实现模型的语言适配。

Result: 提出了3.8B参数的Persian-Phi模型，通过资源高效的课程学习策略成功地将单语英语模型适配到波斯语，且在HuggingFace的Open Persian LLM排行榜中获得了竞争性结果。

Conclusion: 通过资源高效的学习框架，较小规模模型也可实现多语言强大能力，促进低资源语言大模型的拓展，且硬件需求较低。

Abstract: The democratization of AI is currently hindered by the immense computational costs required to train Large Language Models (LLMs) for low-resource languages. This paper presents Persian-Phi, a 3.8B parameter model that challenges the assumption that robust multilingual capabilities require massive model sizes or multilingual baselines. We demonstrate how Microsoft Phi-3 Mini -- originally a monolingual English model -- can be effectively adapted to Persian through a novel, resource-efficient curriculum learning pipeline. Our approach employs a unique "warm-up" stage using bilingual narratives (Tiny Stories) to align embeddings prior to heavy training, followed by continual pretraining and instruction tuning via Parameter-Efficient Fine-Tuning (PEFT). Despite its compact size, Persian-Phi achieves competitive results on Open Persian LLM Leaderboard in HuggingFace. Our findings provide a validated, scalable framework for extending the reach of state-of-the-art LLMs to underrepresented languages with minimal hardware resources. The Persian-Phi model is publicly available at https://huggingface.co/amirakhlaghiqqq/PersianPhi.

</details>


### [54] [Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning](https://arxiv.org/abs/2512.07461)
*Tong Wu,Yang Liu,Jun Bai,Zixia Jia,Shuyi Zhang,Ziyong Lin,Yanting Wang,Song-Chun Zhu,Zilong Zheng*

Main category: cs.CL

TL;DR: NPR框架通过创新训练方法与执行引擎，实现大型语言模型的真正并行推理，显著提升性能与效率。


<details>
  <summary>Details</summary>
Motivation: 赋予大型语言模型（LLMs）真正的并行推理能力，从序列模拟转变为原生并行认知。

Method: 三大关键创新，包括自我蒸馏的渐进训练范式、并行感知策略优化算法，以及NPR引擎重构内存管理与流程控制。

Result: 在八个推理基准测试中，NPR训练的Qwen3-4B模型性能提升最多达24.5%，推理速度提升达4.6倍，实现100%真正的并行执行。

Conclusion: NPR确立了自我进化、高效且可扩展的代理式推理新标准，突破了传统模型依赖自回归解码的局限。

Abstract: We introduce Native Parallel Reasoner (NPR), a teacher-free framework that enables Large Language Models (LLMs) to self-evolve genuine parallel reasoning capabilities. NPR transforms the model from sequential emulation to native parallel cognition through three key innovations: 1) a self-distilled progressive training paradigm that transitions from ``cold-start'' format discovery to strict topological constraints without external supervision; 2) a novel Parallel-Aware Policy Optimization (PAPO) algorithm that optimizes branching policies directly within the execution graph, allowing the model to learn adaptive decomposition via trial and error; and 3) a robust NPR Engine that refactors memory management and flow control of SGLang to enable stable, large-scale parallel RL training. Across eight reasoning benchmarks, NPR trained on Qwen3-4B achieves performance gains of up to 24.5% and inference speedups up to 4.6x. Unlike prior baselines that often fall back to autoregressive decoding, NPR demonstrates 100% genuine parallel execution, establishing a new standard for self-evolving, efficient, and scalable agentic reasoning.

</details>


### [55] [Enhancing Agentic RL with Progressive Reward Shaping and Value-based Sampling Policy Optimization](https://arxiv.org/abs/2512.07478)
*Zhuoran Zhuang,Ye Chen,Jianghao Su,Chao Luo,Luhui Liu,Xia Zeng*

Main category: cs.CL

TL;DR: 本文提出了两种技术，进行奖励设计与策略优化，提高奖励信号稀疏且非指导性的问题，提升融合工具推理的大型语言模型的训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于工具交互的强化学习面临稀疏且非引导性质的奖励信号，导致训练收敛慢且样本效率低，同时梯度退化影响训练稳定性，需要更有效的奖励设计和策略优化方法。

Method: 提出了两种方法：1）渐进式奖励设计（PRS），通过阶段式、密集反馈引导模型逐步掌握工具调用与答案质量；2）基于价值的采样策略优化（VSPO），通过筛选高价值样本和价值平滑剪裁稳定梯度。

Result: 实验在多项短文本和长文本问答数据集上验证，PRS优于二元奖励设计，VSPO在稳定性、收敛速度及最终性能上优于PPO、GRPO、CISPO和纯监督训练基线。

Conclusion: PRS和VSPO技术显著提升了基于工具推理的LLM代理的训练稳定性和性能，使其在多个问答任务中优于现有方法，且具备更好的泛化能力。

Abstract: Large Language Models (LLMs) empowered with Tool-Integrated Reasoning (TIR) can iteratively plan, call external tools, and integrate returned information to solve complex, long-horizon reasoning tasks. Agentic Reinforcement Learning (Agentic RL) optimizes such models over full tool-interaction trajectories, but two key challenges hinder effectiveness: (1) Sparse, non-instructive rewards, such as binary 0-1 verifiable signals, provide limited guidance for intermediate steps and slow convergence; (2) Gradient degradation in Group Relative Policy Optimization (GRPO), where identical rewards within a rollout group yield zero advantage, reducing sample efficiency and destabilizing training. To address these challenges, we propose two complementary techniques: Progressive Reward Shaping (PRS) and Value-based Sampling Policy Optimization (VSPO). PRS is a curriculum-inspired reward design that introduces dense, stage-wise feedback - encouraging models to first master parseable and properly formatted tool calls, then optimize for factual correctness and answer quality. We instantiate PRS for short-form QA (with a length-aware BLEU to fairly score concise answers) and long-form QA (with LLM-as-a-Judge scoring to prevent reward hacking). VSPO is an enhanced GRPO variant that replaces low-value samples with prompts selected by a task-value metric balancing difficulty and uncertainty, and applies value-smoothing clipping to stabilize gradient updates. Experiments on multiple short-form and long-form QA benchmarks show that PRS consistently outperforms traditional binary rewards, and VSPO achieves superior stability, faster convergence, and higher final performance compared to PPO, GRPO, CISPO, and SFT-only baselines. Together, PRS and VSPO yield LLM-based TIR agents that generalize better across domains.

</details>


### [56] [SPAD: Seven-Source Token Probability Attribution with Syntactic Aggregation for Detecting Hallucinations in RAG](https://arxiv.org/abs/2512.07515)
*Pengqian Lu,Jie Lu,Anjin Liu,Guangquan Zhang*

Main category: cs.CL

TL;DR: 本文通过细致的来源归因和词性分析，提出SPAD方法显著提升了检索增强生成中的幻觉检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅将幻觉归因于内部知识和检索上下文的二元冲突，忽视了生成过程中的其他关键组件对幻觉产生的影响。

Method: 通过数学方法将每个生成token的概率归因于查询(Query)、检索增强生成(RAG)、过去生成的token、当前token、本地前馈神经网络(FFN)、最终LayerNorm调整和初始嵌入七个不同来源，并通过词性标签对这些来源贡献进行统计分析。

Result: 提出的SPAD方法通过识别异常贡献模式（例如名词过度依赖最终LayerNorm）有效检测幻觉，且广泛实验表明其达到最先进的性能。

Conclusion: SPAD成功揭示了生成中多种因素对幻觉的影响，提供了一种精确且有效的幻觉检测手段，推动了RAG领域的研究进展。

Abstract: Detecting hallucinations in Retrieval-Augmented Generation (RAG) remains a challenge. Prior approaches attribute hallucinations to a binary conflict between internal knowledge (stored in FFNs) and retrieved context. However, this perspective is incomplete, failing to account for the impact of other components in the generative process, such as the user query, previously generated tokens, the current token itself, and the final LayerNorm adjustment. To address this, we introduce SPAD. First, we mathematically attribute each token's probability into seven distinct sources: Query, RAG, Past, Current Token, FFN, Final LayerNorm, and Initial Embedding. This attribution quantifies how each source contributes to the generation of the current token. Then, we aggregate these scores by POS tags to quantify how different components drive specific linguistic categories. By identifying anomalies, such as Nouns relying on Final LayerNorm, SPAD effectively detects hallucinations. Extensive experiments demonstrate that SPAD achieves state-of-the-art performance

</details>


### [57] [LIME: Making LLM Data More Efficient with Linguistic Metadata Embeddings](https://arxiv.org/abs/2512.07522)
*Sebastian Sztwiertnia,Felix Friedrich,Kristian Kersting,Patrick Schramowski,Björn Deiseroth*

Main category: cs.CL

TL;DR: 通过引入元数据嵌入，LIME显著提升解码器语言模型的预训练效率和生成能力，且成本极低，LIME+1还能显著增强推理和算术任务表现。


<details>
  <summary>Details</summary>
Motivation: 当前预训练的解码器语言模型需要大量高质量数据，但此类数据资源日益匮乏。已有方法主要利用元数据来构建和筛选数据集，但很少直接将元数据作为训练信号。

Method: 提出LIME方法，通过将捕获句法、语义和上下文属性的元数据嵌入到词元嵌入中，提升模型预训练效率和表现。进一步提出LIME+1变体，利用偏移的元数据指导词元生成。

Result: LIME使模型对训练数据分布适应速度提升最多56%，参数增加仅0.01%，计算开销极小。LIME改善词元化，增强语言建模和生成任务性能，在500M到2B规模模型均有效。LIME+1增强推理性能最多38%、算术准确率最多35%。

Conclusion: 利用语言元数据直接作为训练信号是提升语言模型效率和性能的有效途径，LIME及其变体在多项任务中表现出显著优势，适用于不同规模模型。

Abstract: Pre-training decoder-only language models relies on vast amounts of high-quality data, yet the availability of such data is increasingly reaching its limits. While metadata is commonly used to create and curate these datasets, its potential as a direct training signal remains under-explored. We challenge this status quo and propose LIME (Linguistic Metadata Embeddings), a method that enriches token embeddings with metadata capturing syntax, semantics, and contextual properties. LIME substantially improves pre-training efficiency. Specifically, it adapts up to 56% faster to the training data distribution, while introducing only 0.01% additional parameters at negligible compute overhead. Beyond efficiency, LIME improves tokenization, leading to remarkably stronger language modeling capabilities and generative task performance. These benefits persist across model scales (500M to 2B). In addition, we develop a variant with shifted metadata, LIME+1, that can guide token generation. Given prior metadata for the next token, LIME+1 improves reasoning performance by up to 38% and arithmetic accuracy by up to 35%.

</details>


### [58] [Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs](https://arxiv.org/abs/2512.07525)
*Xiaoran Liu,Yuerong Song,Zhigeng Liu,Zengfeng Huang,Qipeng Guo,Zhaoxiang Liu,Shiguo Lian,Ziwei He,Xipeng Qiu*

Main category: cs.CL

TL;DR: 本文通过引入复数点积的虚部信息，改进了RoPE的位置编码方法，有效提升了长上下文语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 标准RoPE仅采用了复数点积的实部，丢失了包含重要相位信息的虚部，导致对长上下文依赖的建模能力受限。

Method: 提出了一种扩展RoPE的方法，重新引入了被丢弃的虚部信息，利用全复数表示创建了双分量注意力分数。

Result: 理论和实证证明该方法通过保留更多位置信息增强了长上下文依赖的建模能力，并在多项长上下文语言建模基准测试中表现优于标准RoPE，且上下文越长，提升越明显。

Conclusion: 该方法成功利用了被忽略的虚部信息，显著改善了长上下文依赖的建模，未来可作为RoPE的增强方案推广应用。

Abstract: Rotary Position Embeddings (RoPE) have become a standard for encoding sequence order in Large Language Models (LLMs) by applying rotations to query and key vectors in the complex plane. Standard implementations, however, utilize only the real component of the complex-valued dot product for attention score calculation. This simplification discards the imaginary component, which contains valuable phase information, leading to a potential loss of relational details crucial for modeling long-context dependencies. In this paper, we propose an extension that re-incorporates this discarded imaginary component. Our method leverages the full complex-valued representation to create a dual-component attention score. We theoretically and empirically demonstrate that this approach enhances the modeling of long-context dependencies by preserving more positional information. Furthermore, evaluations on a suite of long-context language modeling benchmarks show that our method consistently improves performance over the standard RoPE, with the benefits becoming more significant as context length increases. The code is available at https://github.com/OpenMOSS/rope_pp.

</details>


### [59] [SwissGov-RSD: A Human-annotated, Cross-lingual Benchmark for Token-level Recognition of Semantic Differences Between Related Documents](https://arxiv.org/abs/2512.07538)
*Michelle Wastl,Jannis Vamvas,Rico Sennrich*

Main category: cs.CL

TL;DR: 本文提出了SwissGov-RSD数据集，用于跨语言文档级语义差异识别，评估了多种大语言模型，发现当前自动方法在该任务上表现较差。


<details>
  <summary>Details</summary>
Motivation: 跨语言文档中的语义差异识别对于文本生成和多语种内容对齐至关重要，但作为独立任务未被充分研究。

Method: 构建多语种多文档的SwissGov-RSD数据集，进行人工逐词差异标注，并在此基础上评测多种开源与闭源大语言模型和编码器模型。

Result: 通过实验证明当前模型在此跨语言跨文档任务上效果远低于在单语句子级或合成数据上的表现。

Conclusion: 现有模型在跨语言、文档级语义差异识别任务上表现不足，存在显著提升空间。

Abstract: Recognizing semantic differences across documents, especially in different languages, is crucial for text generation evaluation and multilingual content alignment. However, as a standalone task it has received little attention. We address this by introducing SwissGov-RSD, the first naturalistic, document-level, cross-lingual dataset for semantic difference recognition. It encompasses a total of 224 multi-parallel documents in English-German, English-French, and English-Italian with token-level difference annotations by human annotators. We evaluate a variety of open-source and closed source large language models as well as encoder models across different fine-tuning settings on this new benchmark. Our results show that current automatic approaches perform poorly compared to their performance on monolingual, sentence-level, and synthetic benchmarks, revealing a considerable gap for both LLMs and encoder models. We make our code and datasets publicly available.

</details>


### [60] [Minimum Bayes Risk Decoding for Error Span Detection in Reference-Free Automatic Machine Translation Evaluation](https://arxiv.org/abs/2512.07540)
*Boxuan Lyu,Haiyue Song,Hidetaka Kamigaito,Chenchen Ding,Hideki Tanaka,Masao Utiyama,Kotaro Funakoshi,Manabu Okumura*

Main category: cs.CL

TL;DR: 本文通过引入MBR解码和蒸馏技术，提升了自动翻译误差跨度检测模型的性能和推理效率。


<details>
  <summary>Details</summary>
Motivation: 传统的最大后验概率（MAP）解码方法假定模型概率与人工标注相似度完全相关，但实际存在与人工标注相似度较低但模型概率较高的情况，影响误差检测的准确性。

Method: 采用生成式误差跨度检测（ESD）模型，结合最小贝叶斯风险（MBR）解码方法，通过句子级和跨度级的相似度指标进行候选假设选择，并引入MBR蒸馏技术以降低计算开销。

Result: MBR解码方法在系统、句子和误差跨度级别均优于MAP基线方法。利用MBR蒸馏技术，可使贪婪模型达到MBR解码性能，有效消除推理时延瓶颈。

Conclusion: MBR解码结合相似度指标显著提升了生成式ESD模型的检测准确性，MBR蒸馏有效降低了计算成本，为实际应用提供了性能与效率兼顾的解决方案。

Abstract: Error Span Detection (ESD) is a subtask of automatic machine translation evaluation that localizes error spans in translations and labels their severity. State-of-the-art generative ESD methods typically decode using Maximum a Posteriori (MAP), assuming that model-estimated probabilities are perfectly correlated with similarity to human annotation. However, we observed that annotations dissimilar to the human annotation could achieve a higher model likelihood than the human annotation. We address this issue by applying Minimum Bayes Risk (MBR) decoding to generative ESD models. Specifically, we employ sentence- and span-level similarity metrics as utility functions to select candidate hypotheses based on their approximate similarity to the human annotation. Extensive experimental results show that our MBR decoding outperforms the MAP baseline at the system, sentence, and span-levels. Furthermore, to mitigate the computational cost of MBR decoding, we demonstrate that applying MBR distillation enables a standard greedy model to match MBR decoding performance, effectively eliminating the inference-time latency bottleneck.

</details>


### [61] [Most over-representation of phonological features in basic vocabulary disappears when controlling for spatial and phylogenetic effects](https://arxiv.org/abs/2512.07543)
*Frederic Blum*

Main category: cs.CL

TL;DR: 通过更大规模样本和严格控制语言依赖，发现多数音象征模式不具普遍稳健性，提示需谨慎对待语言普遍性主张。


<details>
  <summary>Details</summary>
Motivation: 检验先前关于基本词汇中语音特征统计过度表现的研究是否具有可重复性，排除样本和模型偏差，特别是语言之间的家系和地理依赖未被充分控制的问题。

Method: 利用包含2864种语言的Lexibank数据，修改原有模型，加入语言间的空间和系谱依赖作为统计控制，测试先前245语言样本研究的音象征性模式的稳健性。

Result: 大多数先前观察到的音象征性模式在控制家系和地理依赖后不再稳健，许多模式完全消失；但少数模式仍表现出高度稳定性。

Conclusion: 语言音象征性的普遍性主张需在不同层面反复检验其稳健性，新研究为大规模音象征分布提供更可靠评估方法，强调在语言研究中控制语言间依赖的重要性。

Abstract: The statistical over-representation of phonological features in the basic vocabulary of languages is often interpreted as reflecting potentially universal sound symbolic patterns. However, most of those results have not been tested explicitly for reproducibility and might be prone to biases in the study samples or models. Many studies on the topic do not adequately control for genealogical and areal dependencies between sampled languages, casting doubts on the robustness of the results. In this study, we test the robustness of a recent study on sound symbolism of basic vocabulary concepts which analyzed245 languages.The new sample includes data on 2864 languages from Lexibank. We modify the original model by adding statistical controls for spatial and phylogenetic dependencies between languages. The new results show that most of the previously observed patterns are not robust, and in fact many patterns disappear completely when adding the genealogical and areal controls. A small number of patterns, however, emerges as highly stable even with the new sample. Through the new analysis, we are able to assess the distribution of sound symbolism on a larger scale than previously. The study further highlights the need for testing all universal claims on language for robustness on various levels.

</details>


### [62] [MoCoRP: Modeling Consistent Relations between Persona and Response for Persona-based Dialogue](https://arxiv.org/abs/2512.07544)
*Kyungro Lee,Dongha Choi,Hyunju Lee*

Main category: cs.CL

TL;DR: 针对人物对话数据集中缺乏显式关系的问题，MoCoRP利用NLI提取人物与回复间关系，显著提升了对话的一致性和趣味性。


<details>
  <summary>Details</summary>
Motivation: 当前的人物对话系统难以生成与角色一致且内容相关的对话，因为现有数据集缺乏人物句子与回复的显式关联，使模型难以有效抓取人物信息。

Method: 利用NLI专家模型提取人物与回复间的自然语言推理关系，将关系显式融入预训练模型及大型语言模型，通过对齐调优优化对话生成能力。

Result: 在ConvAI2和MPChat公开数据集上的实验证明，MoCoRP在人物一致性、对话趣味性及上下文感知能力方面均优于现有基线模型，且在定量和定性指标上均有显著提升。

Conclusion: 显式建模人物句子与回复之间的关系对于提升人物对话系统的表现非常有效，能够生成更加一致且富有吸引力的上下文相关回应。

Abstract: As dialogue systems become increasingly important across various domains, a key challenge in persona-based dialogue is generating engaging and context-specific interactions while ensuring the model acts with a coherent personality. However, existing persona-based dialogue datasets lack explicit relations between persona sentences and responses, which makes it difficult for models to effectively capture persona information. To address these issues, we propose MoCoRP (Modeling Consistent Relations between Persona and Response), a framework that incorporates explicit relations into language models. MoCoRP leverages an NLI expert to explicitly extract the NLI relations between persona sentences and responses, enabling the model to effectively incorporate appropriate persona information from the context into its responses. We applied this framework to pre-trained models like BART and further extended it to modern large language models (LLMs) through alignment tuning. Experimental results on the public datasets ConvAI2 and MPChat demonstrate that MoCoRP outperforms existing baselines, achieving superior persona consistency and engaging, context-aware dialogue generation. Furthermore, our model not only excels in quantitative metrics but also shows significant improvements in qualitative aspects. These results highlight the effectiveness of explicitly modeling persona-response relations in persona-based dialogue. The source codes of MoCoRP are available at https://github.com/DMCB-GIST/MoCoRP.

</details>


### [63] [Performance of the SafeTerm AI-Based MedDRA Query System Against Standardised MedDRA Queries](https://arxiv.org/abs/2512.07552)
*Francois Vandenhende,Anna Georgiou,Michalis Georgiou,Theodoros Psaras,Ellie Karekla,Elena Hadjicosta*

Main category: cs.CL

TL;DR: SafeTerm AMQ是一种基于多维向量和统计方法的医疗查询自动化系统，能有效检索相关MedDRA术语，提升药品安全不良事件信号检测的效率与准确度。


<details>
  <summary>Details</summary>
Motivation: 在药品上市前安全审查中，准确分组相关不良事件术语以辅助信号检测极为关键，需要自动化工具提升效率和准确性。

Method: 采用多维向量空间嵌入医药术语，利用余弦相似度和极值聚类计算相关性评分，并通过多重相似度阈值评估系统表现。

Result: 实验表明，SafeTerm AMQ在不同相似度阈值下均能实现高召回率（最高94%）和高精准度（最高89%），自动阈值选择优先保证召回率，该方法对狭义术语效果更佳。

Conclusion: SafeTerm AMQ在MedDRA SMQs的自动查询生成中表现良好，能够在召回率和精准度之间取得平衡，是一种有效的辅助方法。

Abstract: In pre-market drug safety review, grouping related adverse event terms into SMQs or OCMQs is critical for signal detection. We assess the performance of SafeTerm Automated Medical Query (AMQ) on MedDRA SMQs. The AMQ is a novel quantitative artificial intelligence system that understands and processes medical terminology and automatically retrieves relevant MedDRA Preferred Terms (PTs) for a given input query, ranking them by a relevance score (0-1) using multi-criteria statistical methods. The system (SafeTerm) embeds medical query terms and MedDRA PTs in a multidimensional vector space, then applies cosine similarity, and extreme-value clustering to generate a ranked list of PTs. Validation was conducted against tier-1 SMQs (110 queries, v28.1). Precision, recall and F1 were computed at multiple similarity-thresholds, defined either manually or using an automated method. High recall (94%)) is achieved at moderate similarity thresholds, indicative of good retrieval sensitivity. Higher thresholds filter out more terms, resulting in improved precision (up to 89%). The optimal threshold (0.70)) yielded an overall recall of (48%) and precision of (45%) across all 110 queries. Restricting to narrow-term PTs achieved slightly better performance at an increased (+0.05) similarity threshold, confirming increased relatedness of narrow versus broad terms. The automatic threshold (0.66) selection prioritizes recall (0.58) to precision (0.29). SafeTerm AMQ achieves comparable, satisfactory performance on SMQs and sanitized OCMQs. It is therefore a viable supplementary method for automated MedDRA query generation, balancing recall and precision. We recommend using suitable MedDRA PT terminology in query formulation and applying the automated threshold method to optimise recall. Increasing similarity scores allows refined, narrow terms selection.

</details>


### [64] [A Simple Method to Enhance Pre-trained Language Models with Speech Tokens for Classification](https://arxiv.org/abs/2512.07571)
*Nicolas Calbucura,Valentin Barriere*

Main category: cs.CL

TL;DR: 本文提出了一种简单方法，将预训练文本大语言模型与语音信息融合，通过音频特征选择，提升分类任务性能。


<details>
  <summary>Details</summary>
Motivation: 音频序列长度远大于文本，导致融合成本高，如何有效融合语音信息以提升文本模型性能是关键问题。

Method: 使用语音识别训练的语音分词器生成长序列标记后，利用套索回归进行多模态Bag-of-Words特征选择，保留重要音频标记，结合自监督语言建模对语言模型适配，最后进行任务微调。

Result: 本方法在论证谬误检测任务上优于单模态模型、较大SpeechLM模型及基于学习表示的融合方法，甚至随机音频标记选择也能提升性能。

Conclusion: 该方法通过音频特征选择和自监督预训练，有效提升了文本模型的分类性能，在论证谬误检测任务中达到了最新水平。

Abstract: This paper presents a simple method that allows to easily enhance textual pre-trained large language models with speech information, when fine-tuned for a specific classification task. A classical issue with the fusion of many embeddings from audio with text is the large length of the audio sequence compared to the text one. Our method benefits from an existing speech tokenizer trained for Audio Speech Recognition that output long sequences of tokens from a large vocabulary, making it difficult to integrate it at low cost in a large language model. By applying a simple lasso-based feature selection on multimodal Bag-of-Words representation, we retain only the most important audio tokens for the task, and adapt the language model to them with a self-supervised language modeling objective, before fine-tuning it on the downstream task. We show this helps to improve the performances compared to an unimodal model, to a bigger SpeechLM or to integrating audio via a learned representation. We show the effectiveness of our method on two recent Argumentative Fallacy Detection and Classification tasks where the use of audio was believed counterproductive, reaching state-of-the-art results. We also provide an in-depth analysis of the method, showing that even a random audio token selection helps enhancing the unimodal model. Our code is available [online](https://github.com/salocinc/EACL26SpeechTokFallacy/).

</details>


### [65] [Complementary Learning Approach for Text Classification using Large Language Models](https://arxiv.org/abs/2512.07583)
*Navid Asgari,Benjamin M. Cole*

Main category: cs.CL

TL;DR: 提出一种结合学者和大型语言模型的结构化协作方法，通过链式思维和少量示例学习，提升定量研究中人机团队的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决人机协作中各自的弱点，提高使用大型语言模型的效率和效果。

Method: 结合链式思维和少量示例学习提示，通过结构化方法整合学者和机器的优势，扩展了定性研究中的团队合作到定量研究的人机团队。

Result: 展示了如何用该方法分析人机评分差异，具体应用于1934份医药联盟新闻稿的数据样本。

Conclusion: 该方法利用低成本技术帮助学者有效管理大型语言模型固有的弱点，实现了人机协作下更有效的数据分析。

Abstract: In this study, we propose a structured methodology that utilizes large language models (LLMs) in a cost-efficient and parsimonious manner, integrating the strengths of scholars and machines while offsetting their respective weaknesses. Our methodology, facilitated through a chain of thought and few-shot learning prompting from computer science, extends best practices for co-author teams in qualitative research to human-machine teams in quantitative research. This allows humans to utilize abductive reasoning and natural language to interrogate not just what the machine has done but also what the human has done. Our method highlights how scholars can manage inherent weaknesses OF LLMs using careful, low-cost techniques. We demonstrate how to use the methodology to interrogate human-machine rating discrepancies for a sample of 1,934 press releases announcing pharmaceutical alliances (1990-2017).

</details>


### [66] [Metric-Fair Prompting: Treating Similar Samples Similarly](https://arxiv.org/abs/2512.07608)
*Jing Wang,Jie Shen,Xing Niu,Tong Zhang,Jeremy Weiss*

Main category: cs.CL

TL;DR: 该工作提出Metric-Fair Prompting方法，利用度量公平约束提升语言模型对医疗多项选择题的准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 旨在解决大型语言模型在多项选择医疗问答中的公平性问题，尤其是保证对相似实例具有一致的判断，从而提升模型的信任度和准确性。

Method: 通过计算问题间的相似度，以问答对的形式对相似问题联合处理，基于NLP嵌入和Lipschitz约束设计提示，引导模型提取关键临床特征并输出带置信度的分数。

Result: 在MedQA(US)基准测试上，Metric-Fair Prompting优于传统逐项提示方法，表现出更高的准确率和公平性。

Conclusion: Metric-Fair Prompting通过引入度量公平性约束，有效提升了大型语言模型在医疗多项选择题上的表现，增强了模型对相似问题的一致性处理能力。

Abstract: We introduce \emph{Metric-Fair Prompting}, a fairness-aware prompting framework that guides large language models (LLMs) to make decisions under metric-fairness constraints. In the application of multiple-choice medical question answering, each {(question, option)} pair is treated as a binary instance with label $+1$ (correct) or $-1$ (incorrect). To promote {individual fairness}~--~treating similar instances similarly~--~we compute question similarity using NLP embeddings and solve items in \emph{joint pairs of similar questions} rather than in isolation. The prompt enforces a global decision protocol: extract decisive clinical features, map each \((\text{question}, \text{option})\) to a score $f(x)$ that acts as confidence, and impose a Lipschitz-style constraint so that similar inputs receive similar scores and, hence, consistent outputs. Evaluated on the {MedQA (US)} benchmark, Metric-Fair Prompting is shown to improve performance over standard single-item prompting, demonstrating that fairness-guided, confidence-oriented reasoning can enhance LLM accuracy on high-stakes clinical multiple-choice questions.

</details>


### [67] [PCMind-2.1-Kaiyuan-2B Technical Report](https://arxiv.org/abs/2512.07612)
*Kairong Luo,Zhenbo Sun,Xinyu Shi,Shengqi Chen,Bowen Yu,Yunyi Chen,Chenyi Dang,Hengtao Tao,Hui Wang,Fangming Liu,Kaifeng Lyu,Wenguang Chen*

Main category: cs.CL

TL;DR: 本文推出了一个全开源的2亿参数模型Kaiyuan-2B，通过创新的数据处理和训练策略，在资源受限条件下实现与业内先进开源模型相媲美的性能。


<details>
  <summary>Details</summary>
Motivation: 当前开源社区与工业界之间的知识差距主要源于工业界依赖闭源高质量数据和训练方案，迫切需要打造高效且公开透明的开源模型。

Method: 提出了分位数数据基准方法、战略性选择重复方案和多域课程训练策略，结合优化的数据预处理流程及FP16稳定性架构修改，实现了训练效率和效果提升。

Result: Kaiyuan-2B模型在多项性能指标上达到先进开源模型水平，并且所有模型权重、数据和代码均已通过Apache 2.0许可证公开发布。

Conclusion: PCMind-2.1-Kaiyuan-2B在资源有限的情况下实现了与最先进的完全开源模型竞争的性能，展示了实用且可扩展的预训练解决方案。

Abstract: The rapid advancement of Large Language Models (LLMs) has resulted in a significant knowledge gap between the open-source community and industry, primarily because the latter relies on closed-source, high-quality data and training recipes. To address this, we introduce PCMind-2.1-Kaiyuan-2B, a fully open-source 2-billion-parameter model focused on improving training efficiency and effectiveness under resource constraints. Our methodology includes three key innovations: a Quantile Data Benchmarking method for systematically comparing heterogeneous open-source datasets and providing insights on data mixing strategies; a Strategic Selective Repetition scheme within a multi-phase paradigm to effectively leverage sparse, high-quality data; and a Multi-Domain Curriculum Training policy that orders samples by quality. Supported by a highly optimized data preprocessing pipeline and architectural modifications for FP16 stability, Kaiyuan-2B achieves performance competitive with state-of-the-art fully open-source models, demonstrating practical and scalable solutions for resource-limited pretraining. We release all assets (including model weights, data, and code) under Apache 2.0 license at https://huggingface.co/thu-pacman/PCMind-2.1-Kaiyuan-2B.

</details>


### [68] [When Large Language Models Do Not Work: Online Incivility Prediction through Graph Neural Networks](https://arxiv.org/abs/2512.07684)
*Zihan Chen,Lanyu Yu*

Main category: cs.CL

TL;DR: 本文通过图神经网络结合文本与结构特征，提出了一种高效准确的在线不文明行为检测方法，优于现有大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 当前在线社区中不文明行为泛滥，传统基于文本的自动检测方法存在准确性和效率不足，需探索结合结构信息的新方法以提升检测性能。

Method: 利用图神经网络，将用户评论作为节点，基于文本相似度构建边，结合动态注意力机制，联合学习语言内容和评论间的关系结构进行不文明行为检测。

Result: 本文提出了一种基于图神经网络的框架，用于检测英文Wikipedia社区中的三类不文明行为（毒性、攻击性和人身攻击），通过将用户评论表示为节点，评论间的文本相似度作为边，实现语言内容与关系结构的联合学习。同时引入动态调整的注意力机制，在信息聚合时自适应平衡节点和拓扑特征。实验证明该方法在多项指标上优于12个最先进的语言大模型，且推理开销显著较低。

Conclusion: 结构上下文在检测在线不文明行为中起关键作用，基于图神经网络的方法在准确性和效率上均超越了仅依赖文本的大型语言模型。

Abstract: Online incivility has emerged as a widespread and persistent problem in digital communities, imposing substantial social and psychological burdens on users. Although many platforms attempt to curb incivility through moderation and automated detection, the performance of existing approaches often remains limited in both accuracy and efficiency. To address this challenge, we propose a Graph Neural Network (GNN) framework for detecting three types of uncivil behavior (i.e., toxicity, aggression, and personal attacks) within the English Wikipedia community. Our model represents each user comment as a node, with textual similarity between comments defining the edges, allowing the network to jointly learn from both linguistic content and relational structures among comments. We also introduce a dynamically adjusted attention mechanism that adaptively balances nodal and topological features during information aggregation. Empirical evaluations demonstrate that our proposed architecture outperforms 12 state-of-the-art Large Language Models (LLMs) across multiple metrics while requiring significantly lower inference cost. These findings highlight the crucial role of structural context in detecting online incivility and address the limitations of text-only LLM paradigms in behavioral prediction. All datasets and comparative outputs will be publicly available in our repository to support further research and reproducibility.

</details>


### [69] [HalluShift++: Bridging Language and Vision through Internal Representation Shifts for Hierarchical Hallucinations in MLLMs](https://arxiv.org/abs/2512.07687)
*Sujoy Nath,Arkaprabha Basu,Sharanya Dasgupta,Swagatam Das*

Main category: cs.CL

TL;DR: 本文提出了一种针对多模态大语言模型（MLLM）幻觉检测的新方法——HalluShift++，基于内部层动态的异常检测，提升了幻觉识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖外部LLM评估器，存在自身幻觉和适应性差的问题，因此需要新的无需外部评估器且适应性更强的幻觉检测方法。

Method: 提出了基于内部层动态的幻觉检测假设，通过层级分析和特定假设修正，实现了从文本大语言模型到多模态模型的幻觉检测迁移。

Result: 验证了基于内部层动态的检测方法在MLLM中具有良好的幻觉识别能力，提升了幻觉检测的性能和适用范围。

Conclusion: HalluShift++通过分析MLLM内部层的动态变化，有效检测出幻觉现象，拓展了幻觉检测技术在多模态场景中的应用。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in vision-language understanding tasks. While these models often produce linguistically coherent output, they often suffer from hallucinations, generating descriptions that are factually inconsistent with the visual content, potentially leading to adverse consequences. Therefore, the assessment of hallucinations in MLLM has become increasingly crucial in the model development process. Contemporary methodologies predominantly depend on external LLM evaluators, which are themselves susceptible to hallucinations and may present challenges in terms of domain adaptation. In this study, we propose the hypothesis that hallucination manifests as measurable irregularities within the internal layer dynamics of MLLMs, not merely due to distributional shifts but also in the context of layer-wise analysis of specific assumptions. By incorporating such modifications, \textsc{\textsc{HalluShift++}} broadens the efficacy of hallucination detection from text-based large language models (LLMs) to encompass multimodal scenarios. Our codebase is available at https://github.com/C0mRD/HalluShift_Plus.

</details>


### [70] [Automated Generation of Custom MedDRA Queries Using SafeTerm Medical Map](https://arxiv.org/abs/2512.07694)
*Francois Vandenhende,Anna Georgiou,Michalis Georgiou,Theodoros Psaras,Ellie Karekla,Elena Hadjicosta*

Main category: cs.CL

TL;DR: 提出了一个基于人工智能的系统SafeTerm，用于自动检索和排序与药物不良事件查询相关的MedDRA术语，验证结果显示其在安全性信号检测中具备实用价值。


<details>
  <summary>Details</summary>
Motivation: 在药品上市前的安全性评估中，将相关不良事件术语归类为标准化的MedDRA查询或FDA OCMQ查询对于信号检测至关重要。

Method: 本研究提出了一个新颖的定量人工智能系统（SafeTerm），通过将医学查询术语和MedDRA首选术语嵌入多维向量空间，利用余弦相似度和极值聚类技术自动检索和排序与输入查询相关的MedDRA术语。

Result: 系统在104个FDA OCMQ v3.0查询上验证，表现出高召回率（>95%）和良好精确度（最高86%），最优相似度阈值为0.70-0.75时召回率约为50%，精确率约为33%。

Conclusion: SafeTerm系统可作为自动生成MedDRA查询的辅助工具，推荐初始相似度阈值为0.60，随后提高阈值以优化术语筛选。

Abstract: In pre-market drug safety review, grouping related adverse event terms into standardised MedDRA queries or the FDA Office of New Drugs Custom Medical Queries (OCMQs) is critical for signal detection. We present a novel quantitative artificial intelligence system that understands and processes medical terminology and automatically retrieves relevant MedDRA Preferred Terms (PTs) for a given input query, ranking them by a relevance score using multi-criteria statistical methods. The system (SafeTerm) embeds medical query terms and MedDRA PTs in a multidimensional vector space, then applies cosine similarity and extreme-value clustering to generate a ranked list of PTs. Validation was conducted against the FDA OCMQ v3.0 (104 queries), restricted to valid MedDRA PTs. Precision, recall and F1 were computed across similarity-thresholds. High recall (>95%) is achieved at moderate thresholds. Higher thresholds improve precision (up to 86%). The optimal threshold (~0.70 - 0.75) yielded recall ~50% and precision ~33%. Narrow-term PT subsets performed similarly but required slightly higher similarity thresholds. The SafeTerm AI-driven system provides a viable supplementary method for automated MedDRA query generation. A similarity threshold of ~0.60 is recommended initially, with increased thresholds for refined term selection.

</details>


### [71] [Mary, the Cheeseburger-Eating Vegetarian: Do LLMs Recognize Incoherence in Narratives?](https://arxiv.org/abs/2512.07777)
*Karin de Langis,Püren Öncel,Ryan Peters,Andrew Elfenbein,Laura Kristen Allen,Andreas Schramm,Dongyeop Kang*

Main category: cs.CL

TL;DR: LLMs能内部识别不连贯叙事，但生成评价无法有效区分连贯性，且更依赖世界知识而非叙事理解，表明其故事连贯性理解不完整。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型是否能够可靠地区分连贯与不连贯故事，揭示其在叙事理解方面的能力与不足。

Method: 采用成对故事数据集，通过内在表征分析与问答生成两种方式评估LLMs的叙事连贯性识别能力，并测试多种推理增强方法。

Result: 本文利用成对叙事数据集，研究大型语言模型（LLMs）区分连贯与不连贯故事的能力。发现模型内部表征能可靠识别不连贯叙事，但生成的评价问答无法有效分辨两者，显示模型在故事理解上存在差距。推理增强方法未能弥补这一缺陷，表明内部状态与行为间存在不一致。此外，模型对环境设定违背（如沙漠下雨）的不连贯更敏感，而对角色特性违背（如素食者点奶酪汉堡）敏感度较低，暗示模型更依赖典型世界知识而非基于意义的叙事连贯性。整体结果表明LLMs尚未完全掌握叙事连贯性。

Conclusion: LLMs虽具内部识别不连贯能力，但在行为层面表现不足，且对不同类型不连贯敏感度不均，显示其叙事连贯理解存在局限。

Abstract: Leveraging a dataset of paired narratives, we investigate the extent to which large language models (LLMs) can reliably separate incoherent and coherent stories. A probing study finds that LLMs' internal representations can reliably identify incoherent narratives. However, LLMs generate responses to rating questions that fail to satisfactorily separate the coherent and incoherent narratives across several prompt variations, hinting at a gap in LLM's understanding of storytelling. The reasoning LLMs tested do not eliminate these deficits, indicating that thought strings may not be able to fully address the discrepancy between model internal state and behavior. Additionally, we find that LLMs appear to be more sensitive to incoherence resulting from an event that violates the setting (e.g., a rainy day in the desert) than to incoherence arising from a character violating an established trait (e.g., Mary, a vegetarian, later orders a cheeseburger), suggesting that LLMs may rely more on prototypical world knowledge than building meaning-based narrative coherence. The consistent asymmetry found in our results suggests that LLMs do not have a complete grasp on narrative coherence.

</details>


### [72] [On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models](https://arxiv.org/abs/2512.07783)
*Charlie Zhang,Graham Neubig,Xiang Yue*

Main category: cs.CL

TL;DR: 通过构建可控实验框架，研究揭示了预训练、中期训练和强化学习后训练对语言模型推理能力提升的不同贡献及其相互关系，为优化训练策略提供支持。


<details>
  <summary>Details</summary>
Motivation: 当前大规模预训练数据不透明，强化学习目标与已有知识复杂交互，导致难以厘清各训练阶段对模型推理能力提升的具体贡献，本文旨在通过控制实验框架解决这一问题。

Method: 采用合成推理任务，设计明确的原子操作和可解析的逐步推理轨迹，通过系统操控训练分布，分别评估模型的外推泛化和语境泛化能力，从而解析不同训练阶段的因果贡献。

Result: 本文通过构建一个完全可控的实验框架，系统地分析了预训练、中期训练和基于强化学习（RL）的后训练对语言模型推理能力的贡献。研究发现：1）只有当预训练留下足够发展空间且RL训练数据针对模型能力边界任务时，RL才能带来真实的能力提升；2）语境泛化需要适当的预训练，且RL能有效迁移此能力；3）中期训练显著提升性能，且在计算资源固定条件下优于仅用RL，显示其重要作用；4）过程级奖励减少了奖励欺骗，提高了推理的准确性。整体结果阐明了预训练、中期训练与RL三者的相互作用，为改进推理语言模型的训练策略提供了理论基础。

Conclusion: 本文证明了RL后训练能有效提升模型能力的前提条件，并强调了中期训练的重要性及过程级奖励的积极作用，明确了不同训练阶段对推理能力的影响机制。

Abstract: Recent reinforcement learning (RL) techniques have yielded impressive reasoning improvements in language models, yet it remains unclear whether post-training truly extends a model's reasoning ability beyond what it acquires during pre-training. A central challenge is the lack of control in modern training pipelines: large-scale pre-training corpora are opaque, mid-training is often underexamined, and RL objectives interact with unknown prior knowledge in complex ways. To resolve this ambiguity, we develop a fully controlled experimental framework that isolates the causal contributions of pre-training, mid-training, and RL-based post-training. Our approach employs synthetic reasoning tasks with explicit atomic operations, parseable step-by-step reasoning traces, and systematic manipulation of training distributions. We evaluate models along two axes: extrapolative generalization to more complex compositions and contextual generalization across surface contexts. Using this framework, we reconcile competing views on RL's effectiveness. We show that: 1) RL produces true capability gains (pass@128) only when pre-training leaves sufficient headroom and when RL data target the model's edge of competence, tasks at the boundary that are difficult but not yet out of reach. 2) Contextual generalization requires minimal yet sufficient pre-training exposure, after which RL can reliably transfer. 3) Mid-training significantly enhances performance under fixed compute compared with RL only, demonstrating its central but underexplored role in training pipelines. 4) Process-level rewards reduce reward hacking and improve reasoning fidelity. Together, these results clarify the interplay between pre-training, mid-training, and RL, offering a foundation for understanding and improving reasoning LM training strategies.

</details>


### [73] [Collaborative Causal Sensemaking: Closing the Complementarity Gap in Human-AI Decision Support](https://arxiv.org/abs/2512.07801)
*Raunak Jain,Mudita Khurana*

Main category: cs.CL

TL;DR: 本文提出合作因果建构框架，促进人机协作决策，通过共享认知模型和因果推理提升专家团队表现，解决传统AI辅助中人机协作不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的代理广泛应用于专家决策支持，但在人机协作中经常无法提升团队表现，原因在于传统AI辅助忽视了专家决策中的协同认知过程。

Method: 提出了合作因果建构（Collaborative Causal Sensemaking，CCS）作为研究框架，设计支持认知工作的决策支持系统，强调共同构建和检验因果假设，动态调整模型和目标，以及从联合决策结果中相互学习。

Result: 提出的CCS框架能够促进人机共同思考与推理，使AI真正成为人类专家的合作伙伴，提升信任和互补性，解决现有系统在高风险复杂环境中表现不佳的问题。

Conclusion: 将多智能体系统研究转向支持协作认知的代理，强调人机协同的因果推理和建模，促进共同决策中的信任与互补，实现AI与人类专家的长期共进。

Abstract: LLM-based agents are rapidly being plugged into expert decision-support, yet in messy, high-stakes settings they rarely make the team smarter: human-AI teams often underperform the best individual, experts oscillate between verification loops and over-reliance, and the promised complementarity does not materialise. We argue this is not just a matter of accuracy, but a fundamental gap in how we conceive AI assistance: expert decisions are made through collaborative cognitive processes where mental models, goals, and constraints are continually co-constructed, tested, and revised between human and AI. We propose Collaborative Causal Sensemaking (CCS) as a research agenda and organizing framework for decision-support agents: systems designed as partners in cognitive work, maintaining evolving models of how particular experts reason, helping articulate and revise goals, co-constructing and stress-testing causal hypotheses, and learning from the outcomes of joint decisions so that both human and agent improve over time. We sketch challenges around training ecologies that make collaborative thinking instrumentally valuable, representations and interaction protocols for co-authored models, and evaluation centred on trust and complementarity. These directions can reframe MAS research around agents that participate in collaborative sensemaking and act as AI teammates that think with their human partners.

</details>


### [74] [Do Generalisation Results Generalise?](https://arxiv.org/abs/2512.07832)
*Matteo Boglioni,Andrea Sgobbi,Gabriel Tavernini,Francesco Rita,Marius Mosbach,Tiago Pimentel*

Main category: cs.CL

TL;DR: 本文研究了大语言模型在多个不同分布的测试集上的泛化能力，发现不同OOD测试集之间的表现相关性依赖于具体模型，没有统一趋势。


<details>
  <summary>Details</summary>
Motivation: 现有评估通常只用单一OOD测试集，无法全面反映模型实际部署中遇到的多样化数据分布转移。

Method: 在微调过程中，评估模型在多个OOD测试集上的表现，并控制在域内表现后计算各测试集间的偏相关。

Result: 通过OLMo2和OPT的分析发现，不同OOD测试集间的表现相关性取决于具体模型，没有统一正负相关趋势。

Conclusion: 不同模型在不同OOD测试集上的泛化表现相关性不稳定，说明单一OOD测试集的泛化评估有限。

Abstract: A large language model's (LLM's) out-of-distribution (OOD) generalisation ability is crucial to its deployment. Previous work assessing LLMs' generalisation performance, however, typically focuses on a single out-of-distribution dataset. This approach may fail to precisely evaluate the capabilities of the model, as the data shifts encountered once a model is deployed are much more diverse. In this work, we investigate whether OOD generalisation results generalise. More specifically, we evaluate a model's performance across multiple OOD testsets throughout a finetuning run; we then evaluate the partial correlation of performances across these testsets, regressing out in-domain performance. This allows us to assess how correlated are generalisation performances once in-domain performance is controlled for. Analysing OLMo2 and OPT, we observe no overarching trend in generalisation results: the existence of a positive or negative correlation between any two OOD testsets depends strongly on the specific choice of model analysed.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [75] [AI-Generated Compromises for Coalition Formation: Modeling, Simulation, and a Textual Case Study](https://arxiv.org/abs/2512.05983)
*Eyal Briman,Ehud Shapiro,Nimrod Talmon*

Main category: cs.MA

TL;DR: 本文提出利用NLP和大语言模型，在文本协作写作中生成智能体间的妥协提案，促进大规模民主文本编辑，弥补传统工具不足。


<details>
  <summary>Details</summary>
Motivation: 当前在多智能体提案间寻找妥协方案的过程中，如何高效找到妥协提案仍是未解难题，尤其在文本协作写作领域表现出传统工具的局限性。

Method: 利用NLP技术和大规模语言模型构建文本的语义度量空间，设计算法生成适合的妥协点，并通过模拟不同的联盟形成过程验证算法效果。

Result: 开发的算法能有效生成被多数智能体支持的妥协文本提案，促进了大规模民主协作文本编辑，显示出AI在该领域的潜力。

Conclusion: 本文提出了一个包含智能体有限理性和不确定性的整体模型，并开发了AI模型以生成妥协提案，在文本协作写作领域展示了有效性，尤其适用于大规模民主文本编辑如社区宪法的协作起草。

Abstract: The challenge of finding compromises between agent proposals is fundamental to AI sub-fields such as argumentation, mediation, and negotiation. Building on this tradition, Elkind et al. (2021) introduced a process for coalition formation that seeks majority-supported proposals preferable to the status quo, using a metric space where each agent has an ideal point. The crucial step in this iterative process involves identifying compromise proposals around which agent coalitions can unite. How to effectively find such compromise proposals, however, remains an open question. We address this gap by formalizing a holistic model that encompasses agent bounded rationality and uncertainty and developing AI models to generate such compromise proposals. We focus on the domain of collaboratively writing text documents -- e.g., to enable the democratic creation of a community constitution. We apply NLP (Natural Language Processing) techniques and utilize LLMs (Large Language Models) to create a semantic metric space for text and develop algorithms to suggest suitable compromise points. To evaluate the effectiveness of our algorithms, we simulate various coalition formation processes and demonstrate the potential of AI to facilitate large-scale democratic text editing, such as collaboratively drafting a constitution, an area where traditional tools are limited.

</details>


### [76] [HiveMind: Contribution-Guided Online Prompt Optimization of LLM Multi-Agent Systems](https://arxiv.org/abs/2512.06432)
*Yihan Xia,Taotao Wang,Shengli Zhang,Zhangyuhua Weng,Bin Cao,Soung Chang Liew*

Main category: cs.MA

TL;DR: 本文提出HiveMind框架，利用Shapley值和DAG-Shapley算法实现高效贡献分析与在线优化，提升多智能体合作效果，验证了在股票交易任务中的有效性和计算优势。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统中个体代理的效果评估困难，及低效代理的即时优化一直是开放挑战，亟需高效准确的贡献分析方法以提升系统整体性能。

Method: 提出将Shapley值作为贡献度量标准，引入DAG-Shapley算法利用有向无环图结构剪枝非有效联盟并重用中间结果，大幅降低计算复杂度，实现高效且准确的贡献分配。

Result: 在股票交易多智能体场景中，HiveMind显著优于静态基线，DAG-Shapley减少了80%以上的LLM调用次数，同时贡献归因准确度接近完整Shapley值。

Conclusion: HiveMind框架通过引入Contribution-Guided Online Prompt Optimization (CG-OPO)和高效的DAG-Shapley算法，实现了对LLM多智能体系统中个体贡献的精确评估与优化，显著提升了多智能体合作的性能。

Abstract: Recent advances in LLM-based multi-agent systems have demonstrated remarkable capabilities in complex decision-making scenarios such as financial trading and software engineering. However, evaluating each individual agent's effectiveness and online optimization of underperforming agents remain open challenges. To address these issues, we present HiveMind, a self-adaptive framework designed to optimize LLM multi-agent collaboration through contribution analysis. At its core, HiveMind introduces Contribution-Guided Online Prompt Optimization (CG-OPO), which autonomously refines agent prompts based on their quantified contributions. We first propose the Shapley value as a grounded metric to quantify each agent's contribution, thereby identifying underperforming agents in a principled manner for automated prompt refinement. To overcome the computational complexity of the classical Shapley value, we present DAG-Shapley, a novel and efficient attribution algorithm that leverages the inherent Directed Acyclic Graph structure of the agent workflow to axiomatically prune non-viable coalitions. By hierarchically reusing intermediate outputs of agents in the DAG, our method further reduces redundant computations, and achieving substantial cost savings without compromising the theoretical guarantees of Shapley values. Evaluated in a multi-agent stock-trading scenario, HiveMind achieves superior performance compared to static baselines. Notably, DAG-Shapley reduces LLM calls by over 80\% while maintaining attribution accuracy comparable to full Shapley values, establishing a new standard for efficient credit assignment and enabling scalable, real-world optimization of multi-agent collaboration.

</details>


### [77] [ChargingBoul: A Competitive Negotiating Agent with Novel Opponent Modeling](https://arxiv.org/abs/2512.06595)
*Joe Shymanski*

Main category: cs.MA

TL;DR: ChargingBoul代理通过对对手建模与动态出价策略实现高效自动谈判，在2022年ANAC竞赛中名列前茅，展示了其强大性能和潜力。


<details>
  <summary>Details</summary>
Motivation: 自动化谈判在多智能体系统中是关键研究领域，存在于电子商务、资源分配和自主决策等多个领域，需要设计高效谈判代理。

Method: ChargingBoul采用轻量级策略，结合让步政策和对对手的建模，通过根据对手出价模式分类、动态调整出价策略，实现高效谈判。

Result: ChargingBoul在2022年ANAC竞赛中获得第二名，表现出对多样化对手策略的高适应性和卓越谈判效果。

Conclusion: ChargingBoul有效平衡了让步与对手建模，提升了自动谈判性能。未来可通过更复杂的对手建模和自适应出价启发式方法进一步优化。

Abstract: Automated negotiation has emerged as a critical area of research in multiagent systems, with applications spanning e-commerce, resource allocation, and autonomous decision-making. This paper presents ChargingBoul, a negotiating agent that competed in the 2022 Automated Negotiating Agents Competition (ANAC) and placed second in individual utility by an exceptionally narrow margin. ChargingBoul employs a lightweight yet effective strategy that balances concession and opponent modeling to achieve high negotiation outcomes. The agent classifies opponents based on bid patterns, dynamically adjusts its bidding strategy, and applies a concession policy in later negotiation stages to maximize utility while fostering agreements. We evaluate ChargingBoul's performance using competition results and subsequent studies that have utilized the agent in negotiation research. Our analysis highlights ChargingBoul's effectiveness across diverse opponent strategies and its contributions to advancing automated negotiation techniques. We also discuss potential enhancements, including more sophisticated opponent modeling and adaptive bidding heuristics, to improve its performance further.

</details>


### [78] [Analyzing Collision Rates in Large-Scale Mixed Traffic Control via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2512.06645)
*Muyang Fan*

Main category: cs.MA

TL;DR: 本研究通过MARL和仿真分析了交通密度、信号配置及转向策略对混合交通系统碰撞率的影响，提出了提升安全性和鲁棒性的设计建议。


<details>
  <summary>Details</summary>
Motivation: 在人类驾驶车辆与机器人驾驶车辆混合的动态不确定交通环境中，交通信号控制虽借助MARL有潜力，但安全性难以保障，碰撞率作为交通风险的直接指标需融入控制设计。

Method: 通过多智能体强化学习（MARL）对混合交通控制网络进行控制，结合仿真实验分析碰撞率受车流量、信号灯配置与转向策略的影响。

Result: 仿真实验表明碰撞率对交通密度、信号协调水平及转向控制设计均高度敏感，揭示了影响混合交通系统安全性的关键因素。

Conclusion: 研究结果为基于MARL的混合交通控制系统在提升安全性和效率方面提供了实用指导，推动智能交通系统的发展。

Abstract: Vehicle collisions remain a major challenge in large-scale mixed traffic systems, especially when human-driven vehicles (HVs) and robotic vehicles (RVs) interact under dynamic and uncertain conditions. Although Multi-Agent Reinforcement Learning (MARL) offers promising capabilities for traffic signal control, ensuring safety in such environments remains difficult. As a direct indicator of traffic risk, the collision rate must be well understood and incorporated into traffic control design. This study investigates the primary factors influencing collision rates in a MARL-governed Mixed Traffic Control (MTC) network. We examine three dimensions: total vehicle count, signalized versus unsignalized intersection configurations, and turning-movement strategies. Through controlled simulation experiments, we evaluate how each factor affects collision likelihood. The results show that collision rates are sensitive to traffic density, the level of signal coordination, and turning-control design. These findings provide practical insights for improving the safety and robustness of MARL-based mixed traffic control systems, supporting the development of intelligent transportation systems in which both efficiency and safety are jointly optimized.

</details>


### [79] [Characterizing Lane-Changing Behavior in Mixed Traffic](https://arxiv.org/abs/2512.07219)
*Sungyong Chung,Alireza Talebpour,Samer H. Hamdar*

Main category: cs.MA

TL;DR: 本文利用真实数据和博弈论方法，揭示了混合交通换道行为中的合作模式及社会困境，并发现换道互动通过反复进行促进合作行为的演进。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶汽车（AV）逐步普及的混合交通环境中，理解换道行为及其对交通安全与效率的影响至关重要。特别是需要分析主动换道车辆与受影响车辆之间的互动，以及不同自动驾驶汽车市场渗透率下行为的演变。

Method: 基于Waymo公开运动数据集（WOMD）的真实轨迹数据，使用k-means聚类和量化响应均衡框架分析混合交通中换道车辆（主动车辆）与受影响车辆（被动车辆）之间的互动行为；采用演化博弈论和蒙特卡洛模拟研究合作与背叛行为的演化。

Result: 发现自动驾驶车辆在主动和被动角色中合作比例高于人类驾驶车辆；约4%和11%的换道事件存在社会困境，主要表现为鹿猎博弈和囚徒困境，鸡博弈较少；蒙特卡洛模拟表明，换道互动的反复发生推动合作行为逐渐增强，且不受自动驾驶车辆渗透率影响。

Conclusion: 混合交通中的换道行为存在显著的社会困境，但随着车辆间换道互动的持续，合作行为会逐步增强，有助于提高交通安全和效率，这一过程不依赖于自动驾驶车辆的市场渗透率水平。

Abstract: Characterizing and understanding lane-changing behavior in the presence of automated vehicles (AVs) is crucial to ensuring safety and efficiency in mixed traffic. Accordingly, this study aims to characterize the interactions between the lane-changing vehicle (active vehicle) and the vehicle directly impacted by the maneuver in the target lane (passive vehicle). Utilizing real-world trajectory data from the Waymo Open Motion Dataset (WOMD), this study explores patterns in lane-changing behavior and provides insight into how these behaviors evolve under different AV market penetration rates (MPRs). In particular, we propose a game-theoretic framework to analyze cooperative and defective behaviors in mixed traffic, applied to the 7,636 observed lane-changing events in the WOMD. First, we utilize k-means clustering to classify vehicles as cooperative or defective, revealing that the proportions of cooperative AVs are higher than those of HDVs in both active and passive roles. Next, we jointly estimate the utilities of active and passive vehicles to model their behaviors using the quantal response equilibrium framework. Empirical payoff tables are then constructed based on these utilities. Using these payoffs, we analyze the presence of social dilemmas and examine the evolution of cooperative behaviors using evolutionary game theory. Our results reveal the presence of social dilemmas in approximately 4% and 11% of lane-changing events for active and passive vehicles, respectively, with most classified as Stag Hunt or Prisoner's Dilemma (Chicken Game rarely observed). Moreover, the Monte Carlo simulation results show that repeated lane-changing interactions consistently lead to increased cooperative behavior over time, regardless of the AV penetration rate.

</details>


### [80] [Understanding LLM Agent Behaviours via Game Theory: Strategy Recognition, Biases and Multi-Agent Dynamics](https://arxiv.org/abs/2512.07462)
*Trung-Kiet Huynh,Duy-Minh Dao-Sy,Thanh-Bang Cao,Phong-Hao Le,Hong-Dan Nguyen,Phu-Quy Nguyen-Lam,Minh-Luan Nguyen-Vo,Hong-Phat Pham,Phu-Hoa Pham,Thien-Kim Than,Chi-Nguyen Tran,Huy Tran,Gia-Thoai Tran-Le,Alessio Buscemi,Le Hong Trang,The Anh Han*

Main category: cs.MA

TL;DR: 本文通过扩展FAIRGAME框架，设计新博弈环境系统评估LLMs在重复社会困境中的策略行为，发现其合作偏好和语言依赖性，提出了统一的方法论为LLMs作为战略智能体的审计提供基础。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs越来越多地作为自主决策者在交互式、多智能体系统及人类社会中运行，理解其战略行为对于安全、协调以及AI驱动的社会经济基础设施设计具有重要意义。评估此类行为需捕捉不仅是输出内容，还包括指导决策的潜在意图。

Method: 通过扩展FAIRGAME框架，设计了付费刻度囚徒困境和动态多智能体公共物品博弈两种环境，系统性评估大型语言模型（LLMs）在重复社会困境中的行为。使用传统监督分类模型对重复博弈策略进行训练，解释LLMs的行为模式。

Result: 实验揭示不同模型和语言间存在一致的行为特征，如对激励的敏感合作、跨语言差异和博弈末期趋向背叛。语言框架的影响有时与模型结构差异同等显著。通过分类模型定位LLMs呈现出系统性的、依赖模型和语言的行为意图。

Conclusion: 研究为审计LLMs的战略行为提供了统一方法论，揭示了其系统性的合作偏差及语言影响，推动了安全多智能体系统、AI治理和集体决策设计的发展。

Abstract: As Large Language Models (LLMs) increasingly operate as autonomous decision-makers in interactive and multi-agent systems and human societies, understanding their strategic behaviour has profound implications for safety, coordination, and the design of AI-driven social and economic infrastructures. Assessing such behaviour requires methods that capture not only what LLMs output, but the underlying intentions that guide their decisions. In this work, we extend the FAIRGAME framework to systematically evaluate LLM behaviour in repeated social dilemmas through two complementary advances: a payoff-scaled Prisoners Dilemma isolating sensitivity to incentive magnitude, and an integrated multi-agent Public Goods Game with dynamic payoffs and multi-agent histories. These environments reveal consistent behavioural signatures across models and languages, including incentive-sensitive cooperation, cross-linguistic divergence and end-game alignment toward defection. To interpret these patterns, we train traditional supervised classification models on canonical repeated-game strategies and apply them to FAIRGAME trajectories, showing that LLMs exhibit systematic, model- and language-dependent behavioural intentions, with linguistic framing at times exerting effects as strong as architectural differences. Together, these findings provide a unified methodological foundation for auditing LLMs as strategic agents and reveal systematic cooperation biases with direct implications for AI governance, collective decision-making, and the design of safe multi-agent systems.

</details>


### [81] [Understanding Individual Decision-Making in Multi-Agent Reinforcement Learning: A Dynamical Systems Approach](https://arxiv.org/abs/2512.07588)
*James Rudd-Jones,María Pérez-Ortiz,Mirco Musolesi*

Main category: cs.MA

TL;DR: 本文将多智能体强化学习系统建模为耦合随机动力系统，借助动力学理论分析个体行为的稳定性，首次在考虑随机性的情况下深刻理解MARL行为，为理论分析与实践应用架起桥梁。


<details>
  <summary>Details</summary>
Motivation: 传统分析方法依赖均值场近似，忽略了随机性，导致理论预测与个体轨迹实际表现存在偏差；实际MARL算法具有随机探索、环境噪声和随机梯度更新等固有随机性，需考虑这些因素才能深入理解系统行为。

Method: 将多智能体强化学习（MARL）系统建模为耦合随机动力系统，并利用动力系统理论工具分析个体行为的稳定性和敏感性。

Result: 提出了将MARL视为耦合随机动力系统的新方法，首次严谨研究了含随机性的MARL动态，揭示了个体层面行为的稳定性和敏感性，为多智能体学习过程的设计和控制提供了实际洞见。

Conclusion: 该框架有效捕捉了MARL系统的随机性和个体交互，为理解和控制多智能体学习过程提供了新的理论基础和实践指导，尤其适用于对安全性有严格要求的场景。

Abstract: Analysing learning behaviour in Multi-Agent Reinforcement Learning (MARL) environments is challenging, in particular with respect to \textit{individual} decision-making. Practitioners frequently tend to study or compare MARL algorithms from a qualitative perspective largely due to the inherent stochasticity in practical algorithms arising from random dithering exploration strategies, environment transition noise, and stochastic gradient updates to name a few. Traditional analytical approaches, such as replicator dynamics, often rely on mean-field approximations to remove stochastic effects, but this simplification, whilst able to provide general overall trends, might lead to dissonance between analytical predictions and actual realisations of individual trajectories. In this paper, we propose a novel perspective on MARL systems by modelling them as \textit{coupled stochastic dynamical systems}, capturing both agent interactions and environmental characteristics. Leveraging tools from dynamical systems theory, we analyse the stability and sensitivity of agent behaviour at individual level, which are key dimensions for their practical deployments, for example, in presence of strict safety requirements. This framework allows us, for the first time, to rigorously study MARL dynamics taking into consideration their inherent stochasticity, providing a deeper understanding of system behaviour and practical insights for the design and control of multi-agent learning processes.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [82] [Auto-SPT: Automating Semantic Preserving Transformations for Code](https://arxiv.org/abs/2512.06042)
*Ashish Hooda,Mihai Christodorescu,Chuangang Ren,Aaron Wilson,Kassem Fawaz,Somesh Jha*

Main category: cs.SE

TL;DR: 针对代码克隆检测模型在现实代码变换下表现差的问题，提出Auto-SPT框架，利用大型语言模型自动生成多样且强大的语义保持转换，以提升检测模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实世界代码常经过多种语义不变变换，导致训练在干净数据上的代码克隆检测模型在实际应用中性能大幅下降，亟需有效方法提高模型对真实代码变换的鲁棒性。

Method: 基于大型语言模型，自动设计并实现多样化的语义保持转换（SPTs），将这些转换组合以生成强变换，用以生成合成数据增强训练代码克隆检测模型。

Result: 本文提出了一种名为Auto-SPT的新框架，旨在自动构建代码的语义保持转换（SPTs）生成器，以解决机器学习代码克隆检测模型在面对现实中经过语义不变转换（如重构、混淆、格式化、编译优化等）代码时性能下降的问题。Auto-SPT基于大型语言模型（LLMs）设计，能够生成多样且强大的SPTs组合，从而提升代码克隆检测模型对真实世界代码的鲁棒性。实验结果表明，Auto-SPT生成的转换比现有方法更具多样性，并显著降低了现有检测器的性能，同时通过增强训练数据，提高了模型面对对抗性代码变换的检测能力。

Conclusion: Auto-SPT框架有效生成了多样且强大的语义保持转换，能够模拟现实世界代码的多样变换，并通过数据增强提高模型对抗真实代码变换的鲁棒性。

Abstract: Machine learning (ML) models for code clone detection determine whether two pieces of code are semantically equivalent, which in turn is a key building block for software-engineering tasks like refactoring and security tasks like vulnerability and malware detection. While these models are predominantly trained on clean, structured code datasets, real-world code often undergoes a variety of semantic-preserving transformations, including refactoring, minification, automated formatting, and compiler optimizations. To address this critical gap between training and test data, we propose Auto-SPT, a novel framework to automatically construct synthetic-data generators for code. Auto-SPT is designed to produce Semantic Preserving Transformations (SPTs) that alter a program's syntactic structure while preserving its functionality and is instantiated on top of Large Language Models (LLMs). In particular, we use LLMs to craft a diverse set of SPTs, generate strong implementations for these SPTs, and compose them to result into strong transformations. Our formal analysis shows that the diversity of SPTs impacts the strength of their composition. We then empirically demonstrate that Auto-SPT generates more diverse SPTs than existing approaches and these SPTs significantly drop the performance of state-of-the-art code clone detectors. Further experiments show Auto-SPT can be used to enhance code datasets for training, to produce code-clone detection models that are robust to real-world, adversarial code transformations.

</details>


### [83] [Beyond Prototyping: Autonomous, Enterprise-Grade Frontend Development from Pixel to Production via a Specialized Multi-Agent Framework](https://arxiv.org/abs/2512.06046)
*Ramprasath Ganesaraja,Swathika N,Saravanan AP,Kamalkumar Rathinasamy,Chetana Amancharla,Rahul Das,Sahil Dilip Panse,Aditya Batwe,Dileep Vijayan,Veena Ashok,Thanushree A P,Kausthubh J Rao,Alden Olivero,Roshan,Rajeshwar Reddy Manthena,Asmitha Yuga Sre A,Harsh Tripathi,Suganya Selvaraj,Vito Chin,Kasthuri Rangan Bhaskar,Kasthuri Rangan Bhaskar,Venkatraman R,Sajit Vijayakumar*

Main category: cs.SE

TL;DR: AI4UI是面向企业级应用的自主前端开发框架，借助Figma语法、人机协同和领域知识，实现高效、安全、合规的UI代码自动生成，显著提升开发效率和质量。


<details>
  <summary>Details</summary>
Motivation: 目前通用代码助手侧重快速原型制作，难以满足企业级应用对安全性、可扩展性、合规性和维护性的高标准需求，AI4UI旨在填补这一空白，实现面向生产的自动化前端开发。

Method: 利用Figma中定义的生成式AI友好语法编码设计需求，结合领域知识图谱和安全代码集成策略，实现设计到代码的自动转换；在人机协同环节由领域专家进行细节和合规性调整；采用变更驱动的工作流和专业架构模板确保代码质量与维护性。

Result: AI4UI是一套专为企业级应用开发设计的自主前端开发代理框架，重点在于生产就绪的安全、可扩展、合规且易维护的UI代码。该框架结合人机协同，在设计阶段通过Figma内嵌生成式AI友好语法编码需求，后期由领域专家进行细节优化和合规审查，中间阶段实现自动将设计转化为工程就绪代码。技术创新包括Figma语法解析、领域知识图谱、安全的代码集成策略、专业架构模板以及基于变更的工作流。大规模评测显示AI4UI在平台兼容性、安全合规及功能实现等指标上表现优异，用户评价也居于领先水平，显著缩短了交付周期。

Conclusion: AI4UI成功实现了企业级前端开发自动化，通过结构化设计输入、人机协同优化和安全合规保障，提高了代码质量及交付效率，表现优于业界主流方案。

Abstract: We present AI4UI, a framework of autonomous front-end development agents purpose-built to meet the rigorous requirements of enterprise-grade application delivery. Unlike general-purpose code assistants designed for rapid prototyping, AI4UI focuses on production readiness delivering secure, scalable, compliant, and maintainable UI code integrated seamlessly into enterprise workflows. AI4UI operates with targeted human-in-the-loop involvement: at the design stage, developers embed a Gen-AI-friendly grammar into Figma prototypes to encode requirements for precise interpretation; and at the post processing stage, domain experts refine outputs for nuanced design adjustments, domain-specific optimizations, and compliance needs. Between these stages, AI4UI runs fully autonomously, converting designs into engineering-ready UI code. Technical contributions include a Figma grammar for autonomous interpretation, domain-aware knowledge graphs, a secure abstract/package code integration strategy, expertise driven architecture templates, and a change-oriented workflow coordinated by specialized agent roles. In large-scale benchmarks against industry baselines and leading competitor systems, AI4UI achieved 97.24% platform compatibility, 87.10% compilation success, 86.98% security compliance, 78.00% feature implementation success, 73.50% code-review quality, and 73.36% UI/UX consistency. In blind preference studies with 200 expert evaluators, AI4UI emerged as one of the leaders demonstrating strong competitive standing among leading solutions. Operating asynchronously, AI4UI generates thousands of validated UI screens in weeks rather than months, compressing delivery timeline

</details>


### [84] [Reinforcement Learning Integrated Agentic RAG for Software Test Cases Authoring](https://arxiv.org/abs/2512.06060)
*Mohanakrishnan Hariharan*

Main category: cs.SE

TL;DR: 本文提出利用强化学习驱动的AI智能体，结合混合知识库，持续优化软件测试用例生成，显著提升测试准确率和缺陷检测率，实现人机协同的质量工程自动化。


<details>
  <summary>Details</summary>
Motivation: 传统基于静态知识库的LLMs系统在测试用例生成能力上缺乏持续自我改进，本文旨在引入强化学习驱动的智能体，通过QE专家反馈实现流程的动态优化与知识持续更新。

Method: 采用强化注入智能RAG框架，结合专用智能体和混合向量-图知识库，使用PPO和DQN强化学习算法，根据QE反馈和缺陷检测结果不断优化测试用例生成策略。

Result: 本论文提出了一个将强化学习与自主智能体相结合的框架，旨在实现软件测试用例自动生成过程的持续改进，在质量工程(QE)流程中从业务需求文档自动生成测试用例。不同于传统基于静态知识库生成测试用例的大型语言模型(LLMs)系统，该框架采用强化注入的智能RAG（检索、增强、生成）方法，利用AI智能体从QE反馈、评估和缺陷发现结果中学习，不断优化测试用例生成策略。系统结合专用智能体和混合向量-图知识库，通过先进的强化学习算法（PPO和DQN）基于测试有效性、缺陷检测率和工作流指标优化智能体行为。实验结果表明，该框架在苹果企业项目中测试生成准确率提升了2.4%（由94.8%提升至97.2%），缺陷检测率提高了10.8%。该方法实现了由QE专业知识驱动的持续知识完善循环，提升了测试用例质量，同时增强而非替代了人工测试能力。

Conclusion: 该框架通过强化学习和专家反馈，实现了测试用例自动生成的持续改进，提升了测试准确性和缺陷检测能力，增强了人工质量工程的效率和效果。

Abstract: This paper introduces a framework that integrates reinforcement learning (RL) with autonomous agents to enable continuous improvement in the automated process of software test cases authoring from business requirement documents within Quality Engineering (QE) workflows. Conventional systems employing Large Language Models (LLMs) generate test cases from static knowledge bases, which fundamentally limits their capacity to enhance performance over time. Our proposed Reinforcement Infused Agentic RAG (Retrieve, Augment, Generate) framework overcomes this limitation by employing AI agents that learn from QE feedback, assessments, and defect discovery outcomes to automatically improve their test case generation strategies. The system combines specialized agents with a hybrid vector-graph knowledge base that stores and retrieves software testing knowledge. Through advanced RL algorithms, specifically Proximal Policy Optimization (PPO) and Deep Q-Networks (DQN), these agents optimize their behavior based on QE-reported test effectiveness, defect detection rates, and workflow metrics. As QEs execute AI-generated test cases and provide feedback, the system learns from this expert guidance to improve future iterations. Experimental validation on enterprise Apple projects yielded substantive improvements: a 2.4% increase in test generation accuracy (from 94.8% to 97.2%), and a 10.8% improvement in defect detection rates. The framework establishes a continuous knowledge refinement loop driven by QE expertise, resulting in progressively superior test case quality that enhances, rather than replaces, human testing capabilities.

</details>


### [85] [Toward Patch Robustness Certification and Detection for Deep Learning Systems Beyond Consistent Samples](https://arxiv.org/abs/2512.06123)
*Qilin Zhou,Zhengyuan Wei,Haipeng Wang,Zhuo Wang,W. K. Chan*

Main category: cs.SE

TL;DR: HiCert提出了一种新颖的掩码认证技术，有效提升了对补丁攻击的鲁棒性认证能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法有效认证误分类或预测不一致的样本，导致防御机制易被绕过，迫切需要一种更全面的认证检测方法。

Method: 基于掩码的形式化分析，建立有害样本与良性样本的置信度边界关系，通过检测不同预测标签的变体保证样本安全。

Result: 本文提出了HiCert，一种基于掩码的补丁鲁棒性认证检测技术，解决了现有方法无法认证误分类或预测不一致样本的问题。通过形式化分析，HiCert建立了有害样本与其良性样本之间的新关系，通过检测有害变体的最大置信度边界，系统性地认证了一致和不一致样本。实验结果表明，HiCert在认证更多良性样本的同时，显著提升了准确率并降低了误报率。

Conclusion: HiCert实现了更全面有效的补丁鲁棒性认证，显著优于现有认证检测方法。

Abstract: Patch robustness certification is an emerging kind of provable defense technique against adversarial patch attacks for deep learning systems. Certified detection ensures the detection of all patched harmful versions of certified samples, which mitigates the failures of empirical defense techniques that could (easily) be compromised. However, existing certified detection methods are ineffective in certifying samples that are misclassified or whose mutants are inconsistently pre icted to different labels. This paper proposes HiCert, a novel masking-based certified detection technique. By focusing on the problem of mutants predicted with a label different from the true label with our formal analysis, HiCert formulates a novel formal relation between harmful samples generated by identified loopholes and their benign counterparts. By checking the bound of the maximum confidence among these potentially harmful (i.e., inconsistent) mutants of each benign sample, HiCert ensures that each harmful sample either has the minimum confidence among mutants that are predicted the same as the harmful sample itself below this bound, or has at least one mutant predicted with a label different from the harmful sample itself, formulated after two novel insights. As such, HiCert systematically certifies those inconsistent samples and consistent samples to a large extent. To our knowledge, HiCert is the first work capable of providing such a comprehensive patch robustness certification for certified detection. Our experiments show the high effectiveness of HiCert with a new state-of the-art performance: It certifies significantly more benign samples, including those inconsistent and consistent, and achieves significantly higher accuracy on those samples without warnings and a significantly lower false silent ratio.

</details>


### [86] [Systematically Thinking about the Complexity of Code Structuring Exercises at Introductory Level](https://arxiv.org/abs/2512.06178)
*Georgiana Haldeman,Peter Ohmann,Paul Denny*

Main category: cs.SE

TL;DR: 本文提出了一个框架，用于系统评估代码结构任务的复杂度，帮助学生识别和分离代码中的有意义抽象，从而培养计算思维中的分解与抽象能力。


<details>
  <summary>Details</summary>
Motivation: 分解与抽象作为计算思维的核心，在初级编程教育中常被忽视。随着生成式AI减少了对语法的依赖，提升了代码高级推理的重要性，重新强调DA教学的必要性。

Method: 通过定义任务复杂度的三个维度（重复性、代码模式、数据依赖）及其多个等级，设计示例任务并开发交互式工具用于生成和探索DA问题。

Result: 构建了一个详细的复杂度评估框架，配套示例和工具，支持教学中有针对性地培养学生的分解与抽象技能。

Conclusion: 该框架有效地支持了教育任务的开发，促进学生在过程式编程范式下对分解和抽象技能的掌握。

Abstract: Decomposition and abstraction is an essential component of computational thinking, yet it is not always emphasized in introductory programming courses. In addition, as generative AI further reduces the focus on syntax and increases the importance of higher-level code reasoning, there is renewed opportunity to teach DA explicitly. In this paper, we introduce a framework for systematically assessing the complexity of code structuring tasks, where students must identify and separate meaningful abstractions within existing, unstructured code. The framework defines three dimensions of task complexity, each with multiple levels: repetition, code pattern, and data dependency. To support practical use, we provide example tasks mapped to these levels and offer an interactive tool for generating and exploring DA problems. The framework is designed to support the development of educational tasks that build students' skills with DA in the procedural paradigm.

</details>


### [87] [DUET: Agentic Design Understanding via Experimentation and Testing](https://arxiv.org/abs/2512.06247)
*Gus Henry Smith,Sandesh Adhikary,Vineet Thumuluri,Karthik Suresh,Vivek Pandit,Kartik Hegde,Hamid Shojaei,Chandra Bhagavatula*

Main category: cs.SE

TL;DR: 针对LLM难以理解复杂RTL代码的问题，提出通过实验和测试迭代构建设计理解的DUET方法，显著提升硬件设计AI代理的验证能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于大型语言模型的AI代理能处理复杂软件工程问题，但难以理解硬件设计中的寄存器传输级（RTL）代码，因为RTL代码包含复杂、动态、时间演化的行为，语言模型难以仅凭语法推断这些行为，限制了其在代码补全、文档编写和验证等任务上的能力。

Method: 提出了DUET方法学，通过迭代生成假设并利用EDA工具（仿真、波形检查、形式验证）进行实验测试，从底向上构建设计理解。

Result: DUET方法显著提升了AI代理在形式验证任务中的表现，相比不进行实验验证的基线流程效果更优。

Conclusion: 通过结合迭代假设生成与EDA工具的实验测试，DUET有效增强了AI代理对硬件设计的理解能力，提高了相关下游任务的性能，尤其是在形式验证方面。

Abstract: AI agents powered by large language models (LLMs) are being used to solve increasingly complex software engineering challenges, but struggle with hardware design tasks. Register Transfer Level (RTL) code presents a unique challenge for LLMs, as it encodes complex, dynamic, time-evolving behaviors using the low-level language features of SystemVerilog. LLMs struggle to infer these complex behaviors from the syntax of RTL alone, which limits their ability to complete all downstream tasks like code completion, documentation, or verification. In response to this issue, we present DUET: a general methodology for developing Design Understanding via Experimentation and Testing. DUET mimics how hardware design experts develop an understanding of complex designs: not just via a one-off readthrough of the RTL, but via iterative experimentation using a number of tools. DUET iteratively generates hypotheses, tests them with EDA tools (e.g., simulation, waveform inspection, and formal verification), and integrates the results to build a bottom-up understanding of the design. In our evaluations, we show that DUET improves AI agent performance on formal verification, when compared to a baseline flow without experimentation.

</details>


### [88] [CFCEval: Evaluating Security Aspects in Code Generated by Large Language Models](https://arxiv.org/abs/2512.06248)
*Cheng Cheng,Jinqiu Yang*

Main category: cs.SE

TL;DR: 该文提出CFCEval评估框架和新指标ELRM，通过新数据集解决数据偏差，提升代码生成质量和安全性评估的准确性和全面性。


<details>
  <summary>Details</summary>
Motivation: 现有代码大语言模型（Code LLM）评估缺乏方法论严谨性和全面性，数据集偏差和CodeBLEU指标的局限性影响评估准确性。

Method: 提出了CFCEval评估框架，包含新基准数据集MLVBench和新评估指标ELRM，以解决现有评估中数据集偏差和CodeBLEU指标的不足。CFCEval从代码质量、安全漏洞修复能力、后期变换修复能力和相关性四个维度评估生成代码。

Result: CFCEval在四个维度更有效地捕捉代码质量和安全性，ELRM指标与人类判断更一致，优于CodeBLEU。

Conclusion: CFCEval为代码生成模型评价提供了更科学和全面的工具，推动未来代码大模型的评估方法进步。

Abstract: Code-focused Large Language Models (LLMs), such as CodeX and Star-Coder, have demonstrated remarkable capabilities in enhancing developer productivity through context-aware code generation. However, evaluating the quality and security of LLM-generated code remains a significant challenge. Existing evaluation protocols for Code LLMs lack both methodological rigor and comprehensive scope. A key limitation is dataset bias, which arises from unintentional overlap between training and testing data. Furthermore, while CodeBLEU, a BLEU-based metric, is widely used to assess code similarity, it suffers from critical shortcomings, including imprecise tokenization, structural limitations, and low reference diversity. To address these challenges, we introduce CFCEval, a novel framework for evaluating the quality and security of code generated by LLMs. CFCEval mitigates dataset bias by creating a new benchmark, MLVBench, and incorporates ELRM, a new metric designed to assess the relevance between reference code and generated code. CFCEval evaluates generated code across four dimensions: programming quality, vulnerability-fixing capability, post-transformation fixing capability, and relevance. Our experiments show that CFCEval not only captures both quality and security aspects of generated code more effectively but also that its ELRM aligns more closely with human judgments than CodeBLEU, thus paving the way for future advancements in Code LLMs evaluation.

</details>


### [89] [LLMCFG-TGen: Using LLM-Generated Control Flow Graphs to Automatically Create Test Cases from Use Cases](https://arxiv.org/abs/2512.06401)
*Zhenzhen Yang,Chenhui Cui,Tao Li,Rubing Huang,Nan Niu,Dave Towey,Shikai Guo*

Main category: cs.SE

TL;DR: 通过LLM生成控制流图，自动枚举执行路径，生成全面测试用例，实现完整路径覆盖，提升测试质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的测试生成方法存在覆盖不全、逻辑复杂条件捕捉不足的问题，需要一种能生成全面且准确测试用例的新方法。

Method: （1）利用LLM将用例转换为包含所有可能分支的结构化控制流图（CFG）；（2）遍历CFG，枚举所有完整执行路径；（3）基于路径生成测试用例。

Result: 本文提出了一种基于大语言模型(LLM)生成控制流图(CFG)的自动测试用例生成方法（LLMCFG-TGen），从自然语言（NL）用例描述中提取完整执行路径，生成全面且非冗余的测试用例。实验结果表明，该方法相比传统方法实现了完整路径覆盖，提升了测试完整性和准确性，且显著减少了人工投入。

Conclusion: 结合LLM语义推理与结构化建模，有效弥合了自然语言需求与系统化测试用例生成之间的鸿沟，提升测试的全面性和一致性，减少人工干预。

Abstract: Appropriate test case generation is critical in software testing, significantly impacting the quality of the testing. Requirements-Based Test Generation (RBTG) derives test cases from software requirements, aiming to verify whether or not the system's behaviors align with user needs and expectations. Requirements are often documented in Natural Language (NL), with use-case descriptions being a popular method for capturing functional behaviors and interaction flows in a structured form. Large Language Models (LLMs) have shown strong potential for automating test generation directly from NL requirements. However, current LLM-based approaches may not provide comprehensive, non-redundant coverage. They may also fail to capture complex conditional logic in requirements, resulting in incomplete test cases. We propose a new approach that automatically generates test cases from NL use-case descriptions, called Test Generation based on LLM-generated Control Flow Graphs (LLMCFG-TGen). LLMCFG-TGen comprises three main steps: (1) An LLM transforms a use case into a structured CFG that encapsulates all potential branches; (2) The generated CFG is explored, and all complete execution paths are enumerated; and (3) The execution paths are then used to generate the test cases. To evaluate our proposed approach, we conducted a series of experiments. The results show that LLMs can effectively construct well-structured CFGs from NL use cases. Compared with the baseline methods, LLMCFG-TGen achieves full path coverage, improving completeness and ensuring clear and accurate test cases. Practitioner assessments confirm that LLMCFG-TGen produces logically consistent and comprehensive test cases, while substantially reducing manual effort. The findings suggest that coupling LLM-based semantic reasoning with structured modeling effectively bridges the gap between NL requirements and systematic test generation.

</details>


### [90] [Translating PL/I Macro Procedures into Java Using Automatic Templatization and Large Language Models](https://arxiv.org/abs/2512.06448)
*Takaaki Tateishi,Yasuharu Katsuno*

Main category: cs.SE

TL;DR: 本文提出利用符号执行生成代码模板辅助大语言模型翻译PL/I宏程序的方法，有效提升自动翻译质量，使得生成的Java程序能再现原PL/I宏程序生成的代码行为。


<details>
  <summary>Details</summary>
Motivation: 传统将PL/I程序翻译为现代语言（如Java）时，涉及PL/I宏程序部分使自动翻译复杂且困难，大语言模型难以直接翻译PL/I宏程序生成的代码。

Method: 提出了一种名为templatization的新方法，利用符号执行生成带有命名占位符的代码模板作为中间表示，将符号值视为宏生成代码的一部分，从而辅助大语言模型(LLMs)翻译PL/I宏程序。

Result: 在对10个PL/I宏程序的初步实验中，基于templatization的方法成功生成了可读且可维护的Java程序，且这些Java程序能忠实再现原宏生成的PL/I程序行为。

Conclusion: 通过templatization方法结合符号执行，中间生成代码模板，大幅提升了PL/I宏程序自动翻译成Java的准确性和可维护性。该方法在初步实验中展现出良好效果。

Abstract: Modernizing legacy enterprise systems often involves translating PL/I programs into modern languages such as Java. This task becomes significantly more complex when PL/I macro procedures are involved. The PL/I macro procedures are considered string-manipulating programs that generate PL/I code, and they make automated translation more complex. Recently, large language models (LLMs) have been explored for automated code translation. However, LLM-based code translation struggles to translate the PL/I macro procedures to Java programs that reproduce the behavior of the plain PL/I code generated by the original PL/I macro procedures.
  This paper proposes a novel method called templatization, which uses symbolic execution to generate code templates (code with named placeholders) as an intermediate representation. In this approach, symbolic values are treated as parts of macro-generated code. By symbolically executing macro procedures and generating code templates, our approach facilitates LLMs to generate readable and maintainable Java code. Our preliminary experiment on ten PL/I macro procedures shows that the LLM-based translation through templatization successfully generates Java programs that reproduce the behavior of the macro-generated PL/I programs.

</details>


### [91] [METRION: A Framework for Accurate Software Energy Measurement](https://arxiv.org/abs/2512.06806)
*Benjamin Weigell,Simon Hornung,Bernhard Bauer*

Main category: cs.SE

TL;DR: 本文提出了一种考虑多种硬件因素的线程级应用能耗量化模型，并通过METRION框架实现，显著提升了能耗测量准确性，有助于IT领域的节能优化。


<details>
  <summary>Details</summary>
Motivation: 信息通信技术行业的能耗及碳排放逐年上升，为实现有效优化需准确识别主要能耗来源及验证节能效果，因而急需对应用层能耗进行精准量化的方法。

Method: 设计了一个能量归因模型，支持线程级能耗量化，涵盖Simultaneous Multithreading、频率调节、多插槽架构及非统一内存访问等因素，并集成在METRION框架中，采用Linux系统和Intel CPU进行了实现和测试。

Result: 模型在三个不同工作负载测试中表现出色，CPU能耗平均绝对百分比误差仅为4.2%，DRAM能耗误差为16.1%，表明模型能够有效捕捉应用程序的能耗特征。

Conclusion: 本论文提出的能量归因模型能够准确量化应用程序在CPU和DRAM上的线程级能耗，考虑了多种复杂因素，且通过METRION框架实现跨平台应用，实验结果验证了模型的有效性。

Abstract: The Information and Communication Technology sector accounted for approximately 1.4% of global greenhouse gas emissions and 4% of the world's electricity consumption in 2020, with both expected to rise. To reduce this environmental impact, optimization strategies are employed to reduce energy consumption at the IT infrastructure and application levels. However, effective optimization requires, firstly, the identification of major energy consumers and, secondly, the ability to quantify whether an optimization has achieved the intended energy savings. Accurate determination of application-level energy consumption is thus essential. Therefore, we introduce an energy attribution model that quantifies the energy consumption of applications on CPU and DRAM at the thread level, considering the influence of Simultaneous Multithreading, frequency scaling, multi-socket architectures, and Non-Uniform Memory Access. To ensure cross-platform applicability, we integrate the proposed model into an extensible framework, METRION, including a platform-independent data model and an initial implementation for Linux systems using Intel CPUs. We evaluate METRION across three different workloads and demonstrate that the energy attribution model can accurately capture the CPU energy consumption of applications targeting solely the CPU with a Mean Absolute Percentage Error of 4.2%, and the DRAM energy consumption of applications targeting DRAM with an 16.1% error.

</details>


### [92] [Leveraging LLMs to support co-evolution between definitions and instances of textual DSLs](https://arxiv.org/abs/2512.06836)
*Weixing Zhang,Regina Hebig,Daniel Strüber*

Main category: cs.SE

TL;DR: 研究基于大语言模型实现DSL文本实例与语法共同演化，LLM适合小规模实例但扩展性不足。


<details>
  <summary>Details</summary>
Motivation: 随着软件语言语法定义不断演化，传统模型驱动方法难以保持DSL文本实例中的注释和布局等辅助信息，探索利用LLM实现文本实例与语法的共同演化，以提高软件维护和理解效率。

Method: 采用Claude-3.5和GPT-4o两种大语言模型，针对七个不同的语言案例进行实验，评估其在文本实例语言演化迁移中的表现和辅助信息保存能力。

Result: 本文探讨了基于大语言模型（LLM）在软件语言语法定义演化中，实现语法与文本实例协同演化的可行性。通过对Claude-3.5和GPT-4o两种高级语言模型的应用及七种案例语言的实验，结果显示LLM在处理小规模、文本实例有限的情况下表现良好，但在面对大规模实例时存在明显的扩展性挑战。

Conclusion: 基于LLM的方法能够有效支持小规模文本实例的语言演化迁移，但大规模文本实例的处理存在扩展性瓶颈，需进一步研究改进。

Abstract: Software languages evolve over time for various reasons, such as the addition of new features. When the language's grammar definition evolves, textual instances that originally conformed to the grammar become outdated. For DSLs in a model-driven engineering context, there exists a plethora of techniques to co-evolve models with the evolving metamodel. However, these techniques are not geared to support DSLs with a textual syntax -- applying them to textual language definitions and instances may lead to the loss of information from the original instances, such as comments and layout information, which are valuable for software comprehension and maintenance. This study explores the potential of Large Language Model (LLM)-based solutions in achieving grammar and instance co-evolution, with attention to their ability to preserve auxiliary information when directly processing textual instances. By applying two advanced language models, Claude-3.5 and GPT-4o, and conducting experiments across seven case languages, we evaluated the feasibility and limitations of this approach. Our results indicate a good ability of the considered LLMs for migrating textual instances in small-scale cases with limited instance size, which are representative of a subset of cases encountered in practice. In addition, we observe significant challenges with the scalability of LLM-based solutions to larger instances, leading to insights that are useful for informing future research.

</details>


### [93] [BabelCoder: Agentic Code Translation with Specification Alignment](https://arxiv.org/abs/2512.06902)
*Fazle Rabbi,Soumit Kanti Saha,Tri Minh Triet Pham,Song Wang,Jinqiu Yang*

Main category: cs.SE

TL;DR: 提出了BabelCoder多代理架构，通过任务分解和协同提升自动代码翻译的准确度，优于当前主流方法。


<details>
  <summary>Details</summary>
Motivation: 自动化代码翻译长期挑战性大，现有方法准确率有限，且未有效利用代码的上下文和结构信息。

Method: 采用多专门化代理分别负责代码生成、正确性验证和错误修复，形成结构化协同的代理框架完成代码翻译任务。

Result: 在四个基准数据集上，BabelCoder在94%的案例中表现优于四个现有基线方法，平均准确率达94.16%。

Conclusion: BabelCoder通过引入多代理协同工作框架显著提升了代码翻译的准确性，表现优于现有最先进方法。

Abstract: As software systems evolve, developers increasingly work across multiple programming languages and often face the need to migrate code from one language to another. While automatic code translation offers a promising solution, it has long remained a challenging task. Recent advancements in Large Language Models (LLMs) have shown potential for this task, yet existing approaches remain limited in accuracy and fail to effectively leverage contextual and structural cues within the code. Prior work has explored translation and repair mechanisms, but lacks a structured, agentic framework where multiple specialized agents collaboratively improve translation quality. In this work, we introduce BabelCoder, an agentic framework that performs code translation by decomposing the task into specialized agents for translation, testing, and refinement, each responsible for a specific aspect such as generating code, validating correctness, or repairing errors. We evaluate BabelCoder on four benchmark datasets and compare it against four state-of-the-art baselines. BabelCoder outperforms existing methods by 0.5%-13.5% in 94% of cases, achieving an average accuracy of 94.16%.

</details>


### [94] [MINES: Explainable Anomaly Detection through Web API Invariant Inference](https://arxiv.org/abs/2512.06906)
*Wenjie Zhang,Yun Lin,Chun Fung Amos Kwok,Xiwen Teoh,Xiaofei Xie,Frank Liauw,Hongyu Zhang,Jin Song Dong*

Main category: cs.SE

TL;DR: 该论文提出MINES，通过从数据库模式层面推断API不变量，实现高效且无误报的网络应用异常检测。


<details>
  <summary>Details</summary>
Motivation: 现代网络应用通过暴露的Web API招致恶意攻击或非法访问，日志异常具有与正常日志相似的特点且缺乏关键信息，现有日志学习方法容易被噪声误导，导致异常检测模型表浅且规则不准确。

Method: MINES方法通过将API签名转换为增强的数据库表结构，并利用大语言模型（LLM）推断潜在的数据库约束关系，进而生成可解释的API不变量，用于异常检测。具体包括利用正常日志实例筛选和验证LLM生成的不变量，最终将推断的约束转化为Python代码，实现运行时日志的验证。

Result: 在多个基准系统（TrainTicket、NiceFish、Gitea、Mastodon、NextCloud）上，MINES在检测网络篡改攻击中，实现了高召回率且几乎无误报，优于LogRobust、LogFormer和WebNorm等现有方法，达到新的最先进水平。

Conclusion: MINES通过结合API结构与数据库约束关系，成功消除日志噪声影响，实现了准确且可解释的异常检测，验证了其在实际网络攻击检测中的优越性和实用价值。

Abstract: Detecting the anomalies of web applications, important infrastructures for running modern companies and governments, is crucial for providing reliable web services. Many modern web applications operate on web APIs (e.g., RESTful, SOAP, and WebSockets), their exposure invites intended attacks or unintended illegal visits, causing abnormal system behaviors. However, such anomalies can share very similar logs with normal logs, missing crucial information (which could be in database) for log discrimination. Further, log instances can be also noisy, which can further mislead the state-of-the-art log learning solutions to learn spurious correlation, resulting superficial models and rules for anomaly detection. In this work, we propose MINES which infers explainable API invariants for anomaly detection from the schema level instead of detailed raw log instances, which can (1) significantly discriminate noise in logs to identify precise normalities and (2) detect abnormal behaviors beyond the instrumented logs. Technically, MINES (1) converts API signatures into table schema to enhance the original database shema; and (2) infers the potential database constraints on the enhanced database schema to capture the potential relationships between APIs and database tables. MINES uses LLM for extracting potential relationship based on two given table structures; and use normal log instances to reject and accept LLM-generated invariants. Finally, MINES translates the inferred constraints into invariants to generate Python code for verifying the runtime logs. We extensively evaluate MINES on web-tamper attacks on the benchmarks of TrainTicket, NiceFish, Gitea, Mastodon, and NextCloud against baselines such as LogRobust, LogFormer, and WebNorm. The results show that MINES achieves high recall for the anomalies while introducing almost zero false positives, indicating a new state-of-the-art.

</details>


### [95] [Multi-Docker-Eval: A `Shovel of the Gold Rush' Benchmark on Automatic Environment Building for Software Engineering](https://arxiv.org/abs/2512.06915)
*Kelin Fu,Tianyu Liu,Zeyu Shang,Yingwei Ma,Jian Yang,Jiaheng Liu,Kaigui Bian*

Main category: cs.SE

TL;DR: 该论文通过Multi-Docker-Eval基准系统评测自动化环境配置任务，揭示当前模型的瓶颈与关键影响因素，指导更高效的软件工程自动化。


<details>
  <summary>Details</summary>
Motivation: 解决软件工程自动化中环境配置的关键瓶颈，提供可信赖的评估标准。

Method: 提出Multi-Docker-Eval基准，包括40个真实仓库和9种编程语言，评测执行状态成功率和效率。

Result: 当前最先进模型的成功率较低（最高37.7%），环境构建是主要瓶颈；模型大小和推理长度非决定性因素，开源模型表现优异；代理框架和编程语言显著影响成功率。

Conclusion: 基于发现提出可操作的指导原则，有助于构建可扩展、全自动的软件工程自动化流程。

Abstract: Automated environment configuration is a critical bottleneck in scaling software engineering (SWE) automation. To provide a reliable evaluation standard for this task, we present Multi-Docker-Eval benchmark. It includes 40 real-world repositories spanning 9 programming languages and measures both success in achieving executable states and efficiency under realistic constraints. Our extensive evaluation of state-of-the-art LLMs and agent frameworks reveals key insights: (1) the overall success rate of current models is low (F2P at most 37.7%), with environment construction being the primary bottleneck; (2) model size and reasoning length are not decisive factors, and open-source models like DeepSeek-V3.1 and Kimi-K2 are competitive in both efficiency and effectiveness; (3) agent framework and programming language also have significantly influence on success rate. These findings provide actionable guidelines for building scalable, fully automated SWE pipelines.

</details>


### [96] [Reformulate, Retrieve, Localize: Agents for Repository-Level Bug Localization](https://arxiv.org/abs/2512.07022)
*Genevieve Caumartin,Glaucia Melo*

Main category: cs.SE

TL;DR: 提出一种基于非微调LLM的轻量查询改写与摘要方法，结合BM25检索，有效提升了大规模软件库中错误的文件级定位性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于信息检索的错误定位方法依赖未改变的错误描述，常含有噪声信息，导致检索准确率不佳；最新的大型语言模型虽可通过查询改写提升定位效果，但其对代理性能的影响尚未被探究。

Method: 使用开源的非微调大型语言模型（LLM）对错误报告中的关键信息（如标识符和代码片段）进行提取，并在检索前对查询进行轻量化改写。之后，结合BM25检索算法实现文件级别的错误定位自动化。

Result: 通过最佳查询改写技术，该方法在首次文件检索中的排名提升了35%，文件检索性能较SWE-agent最高提升22%。

Conclusion: 采用轻量级的LLM驱动查询改写显著优化了基于信息检索的错误定位效果，实现了自动化且高效的文件级错误定位。

Abstract: Bug localization remains a critical yet time-consuming challenge in large-scale software repositories. Traditional information retrieval-based bug localization (IRBL) methods rely on unchanged bug descriptions, which often contain noisy information, leading to poor retrieval accuracy. Recent advances in large language models (LLMs) have improved bug localization through query reformulation, yet the effect on agent performance remains unexplored. In this study, we investigate how an LLM-powered agent can improve file-level bug localization via lightweight query reformulation and summarization. We first employ an open-source, non-fine-tuned LLM to extract key information from bug reports, such as identifiers and code snippets, and reformulate queries pre-retrieval. Our agent then orchestrates BM25 retrieval using these preprocessed queries, automating localization workflow at scale. Using the best-performing query reformulation technique, our agent achieves 35% better ranking in first-file retrieval than our BM25 baseline and up to +22% file retrieval performance over SWE-agent.

</details>


### [97] [RisConFix: LLM-based Automated Repair of Risk-Prone Drone Configurations](https://arxiv.org/abs/2512.07122)
*Liping Han,Tingting Nie,Le Yu,Mingzhe Hu,Tao Yue*

Main category: cs.SE

TL;DR: 提出了基于大语言模型的无人机飞行参数实时修复方法RisConFix，能自动纠正降稳飞行的不良配置，实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 无人机飞行控制系统配置参数众多且复杂，推荐值组合可能依然导致飞行不稳定，影响无人机鲁棒性，亟需一种自动、实时、有效的风险配置修复方法。

Method: RisConFix通过实时监测无人机飞行状态，发现异常即利用大型语言模型分析配置参数与飞行状态间关系，生成参数修正并迭代修复，直到飞行恢复正常。

Result: 本文提出了一种基于大语言模型（LLM）的无人机飞行控制软件实时修复方法RisConFix，用于自动修复可能导致飞行不稳定的配置参数组合。该方法通过持续监测无人机运行状态，检测异常飞行行为后，利用LLM分析配置参数与飞行状态关系，生成修正参数更新以恢复飞行稳定。整体修复过程为迭代进行，直到飞行状态正常。实验基于ArduPilot进行，包含1421组错误配置，RisConFix修复成功率达到97%，平均修复次数1.17次，验证了其高效、准确的实时修复能力。

Conclusion: RisConFix方法能够在保证飞行安全与稳定性的前提下，有效自动修复无人机风险配置，显著提升了无人机的鲁棒性和可靠性。

Abstract: Flight control software is typically designed with numerous configurable parameters governing multiple functionalities, enabling flexible adaptation to mission diversity and environmental uncertainty. Although developers and manufacturers usually provide recommendations for these parameters to ensure safe and stable operations, certain combinations of parameters with recommended values may still lead to unstable flight behaviors, thereby degrading the drone's robustness. To this end, we propose a Large Language Model (LLM) based approach for real-time repair of risk-prone configurations (named RisConFix) that degrade drone robustness. RisConFix continuously monitors the drone's operational state and automatically triggers a repair mechanism once abnormal flight behaviors are detected. The repair mechanism leverages an LLM to analyze relationships between configuration parameters and flight states, and then generates corrective parameter updates to restore flight stability. To ensure the validity of the updated configuration, RisConFix operates as an iterative process; it continuously monitors the drone's flight state and, if an anomaly persists after applying an update, automatically triggers the next repair cycle. We evaluated RisConFix through a case study of ArduPilot (with 1,421 groups of misconfigurations). Experimental results show that RisConFix achieved a best repair success rate of 97% and an optimal average number of repairs of 1.17, demonstrating its capability to effectively and efficiently repair risk-prone configurations in real time.

</details>


### [98] [Towards Benchmarking Design Pattern Detection Under Obfuscation: Reproducing and Evaluating Attention-Based Detection Method](https://arxiv.org/abs/2512.07193)
*Manthan Shenoy,Andreas Rausch*

Main category: cs.SE

TL;DR: 本文评估了注意力机制设计模式检测器在代码混淆下鲁棒性，发现其主要依赖浅层语法特征，提出一个混淆语料库作为语义泛化能力测试基准。


<details>
  <summary>Details</summary>
Motivation: 探究现有设计模式检测方法是否真正依赖于代码的深层结构和行为语义，避免仅依赖容易被混淆手段破坏的浅层特征。

Method: 重现实验并评估了DPDAtt方法，构建了包含名称标识符和字符串字面量被替换但控制流等语义保持不变的混淆语料库，以测试分类器的鲁棒性。

Result: 结果表明DPDAtt分类器高度依赖表面语法特征，代码混淆显著降低其检测准确率，表明其语义理解能力不足。

Conclusion: 该论文揭示了基于注意力机制的设计模式检测分类器在语义鲁棒性方面的不足，特别是对浅层语法特征的依赖，导致在代码混淆后检测效果显著下降。

Abstract: This paper investigates the semantic robustness of attention-based classifiers for design pattern detection, particularly focusing on their reliance on structural and behavioral semantics. We reproduce the DPDAtt, an attention-based design pattern detection approach using learning-based classifiers, and evaluate its performance under obfuscation. To this end, we curate an obfuscated version of the DPDAtt Corpus, where the name identifiers in code such as class names, method names, etc., and string literals like print statements and comment blocks are replaced while preserving control flow, inheritance, and logic. Our findings reveal that these trained classifiers in DPDAtt depend significantly on superficial syntactic features, leading to substantial misclassification when such cues are removed through obfuscation. This work highlights the need for more robust detection tools capable of capturing deeper semantic meanings in source code. We propose our curated Obfuscated corpus (containing 34 Java source files) as a reusable proof-of-concept benchmark for evaluating state-of-the-art design pattern detectors on their true semantic generalization capabilities.

</details>


### [99] [Automatic Syntax Error Repair for Discrete Controller Synthesis using Large Language Model](https://arxiv.org/abs/2512.07261)
*Yusei Ishimizu,Takuto Yamauchi,Sinan Chen,Jinyu Cai,Jialong Li,Kenji Tei*

Main category: cs.SE

TL;DR: 针对DCS模型语法错误，本文利用基于知识的提示策略和大语言模型实现自动修复，显著提升了修复效率和准确率。


<details>
  <summary>Details</summary>
Motivation: 离散控制器综合（DCS）尽管功能强大，但其在实际中应用受限，主要是因建模语言（如FSP和FLTL）语法复杂，且语法错误常导致开发效率低下和注意力分散。

Method: 通过专家访谈与学生调研系统总结常见错误模式，设计知识告知的提示策略，输入DCS语法规则和示例引导大语言模型进行语法错误修复。

Result: 提出了一种结合大语言模型（LLMs）和基于知识的提示策略来自动修复DCS模型语法错误的方法，并构建了含真实错误注入的新基准测试，实验表明该方法在修复准确率和效率（比人工快3.46倍）上表现优越。

Conclusion: 结合领域知识的提示策略充分发挥了大语言模型在错误修复中的潜力，显著优化了DCS模型开发的工作流程和效率。

Abstract: Discrete Controller Synthesis (DCS) is a powerful formal method for automatically generating specifications of discrete event systems. However, its practical adoption is often hindered by the highly specialized nature of formal models written in languages such as FSP and FLTL. In practice, syntax errors in modeling frequently become an important bottleneck for developers-not only disrupting the workflow and reducing productivity, but also diverting attention from higher-level semantic design. To this end, this paper presents an automated approach that leverages Large Language Models (LLMs) to repair syntax errors in DCS models using a well-designed, knowledge-informed prompting strategy. Specifically, the prompting is derived from a systematic empirical study of common error patterns, identified through expert interviews and student workshops. It equips the LLM with DCS-specific domain knowledge, including formal grammar rules and illustrative examples, to guide accurate corrections. To evaluate our method, we constructed a new benchmark by systematically injecting realistic syntax errors into validated DCS models. The quantitative evaluation demonstrates the high effectiveness of the proposed approach in terms of repair accuracy and its practical utility regarding time, achieving a speedup of 3.46 times compared to human developers. The experimental replication suite, including the benchmark and prompts, is available at https://github.com/Uuusay1432/DCSModelRepair.git

</details>


### [100] [The Human Need for Storytelling: Reflections on Qualitative Software Engineering Research With a Focus Group of Experts](https://arxiv.org/abs/2512.07293)
*Roberto Verdecchia,Justus Bogner*

Main category: cs.SE

TL;DR: 本文通过专家对话，综述了定性软件工程研究的现状、意义及挑战，并展望其未来发展。


<details>
  <summary>Details</summary>
Motivation: 探讨定性软件工程研究的重要性、发展历程、当前面临的常见障碍及未来前景。

Method: 通过组织专家焦点小组讨论，以三位知名学者的对话形式，反映定性软件工程研究的现状。

Result: 总结了定性软件工程研究的价值、演变过程、实践中的挑战及未来发展趋势。

Conclusion: 定性研究在软件工程领域逐渐被重视，虽存在挑战，但未来前景广阔。

Abstract: From its first adoption in the late 80s, qualitative research has slowly but steadily made a name for itself in what was, and perhaps still is, the predominantly quantitative software engineering (SE) research landscape. As part of our regular column on empirical software engineering (ACM SIGSOFT SEN-ESE), we reflect on the state of qualitative SE research with a focus group of experts. Among other things, we discuss why qualitative SE research is important, how it evolved over time, common impediments faced while practicing it today, and what the future of qualitative SE research might look like. Joining the conversation are Rashina Hoda (Monash University, Australia), Carolyn Seaman (University of Maryland, United States), and Klaas Stol (University College Cork, Ireland). The content of this paper is a faithful account of our conversation from October 25, 2025, which we moderated and edited for our column.

</details>


### [101] [Challenges in Developing Secure Software -- Results of an Interview Study in the German Software Industry](https://arxiv.org/abs/2512.07368)
*Alex R. Mattukat,Timo Langstrof,Horst Lichter*

Main category: cs.SE

TL;DR: 安全软件开发困难重重，主要因复杂性、安全意识和流程问题，以及人才缺乏。


<details>
  <summary>Details</summary>
Motivation: 网络犯罪造成的损失促使安全软件的开发成为必然，但现有措施未能显著改善现状，故需深入理解软件企业面临的安全开发挑战。

Method: 采用访谈法，访谈了19位来自12家跨行业公司的行业专家，收集关于软件安全开发的挑战。

Result: 通过对12家跨行业公司的19位行业专家进行访谈，本文发现软件安全开发面临的主要挑战是软件复杂性高、安全意识不足、过程不适宜及缺乏专业人员。

Conclusion: 软件安全开发的挑战主要源于复杂性、安全意识不足、流程不适宜及缺乏专业人才，需要针对性研究和改进。

Abstract: The damage caused by cybercrime makes the development of secure software inevitable. Although many tools and frameworks exist to support the development of secure software, statistics on cybercrime show no improvement in recent years. To understand the challenges software companies face in developing secure software, we conducted an interview study with 19 industry experts from 12 cross-industry companies. The results of our study show that the challenges are mainly due to high complexity, a lack of security awareness, and unsuitable processes, which are further exacerbated by an immediate lack of skilled personnel. This article presents our study and the challenges we identified, and derives potential research directions from them.

</details>


### [102] [Do LLMs Trust the Code They Write?](https://arxiv.org/abs/2512.07404)
*Francisco Ribeiro,Claudio Spiess,Prem Devanbu,Sarah Nadi*

Main category: cs.SE

TL;DR: 本文发现大型语言模型内部存在代码正确性的表示，通过对比正确与错误代码的隐藏状态，利用该表示提升代码生成质量，无需执行测试即可筛选更优代码。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在代码生成中表现优异，但输出的概率往往与代码正确性不相关，本文旨在寻找模型内部对代码正确性的编码特征，从而提高生成代码的可靠性。

Method: 通过对比相同任务的正确与错误代码在LLM隐藏状态上的差异，提取内部正确性表示，基于此进行代码样本的质量评估和排序。

Result: 在四个大型语言模型上实验验证了该内部表示的有效性，实现了比传统概率排序更优的代码质量筛选效果，且无需测试执行。

Conclusion: 文中展示了利用LLM内部的正确性表示可以显著优于传统的概率和口头置信度排序方法，从而提升代码生成的准确性和可靠性。

Abstract: Despite the effectiveness of large language models (LLMs) for code generation, they often output incorrect code. One reason is that model output probabilities are often not well-correlated with correctness, and reflect only the final output of the generation process. Inspired by findings that LLMs internally encode concepts like truthfulness, this paper explores if LLMs similarly represent code correctness. Specifically, we identify a correctness representation inside LLMs by contrasting the hidden states between pairs of correct and incorrect code for the same programming tasks. By experimenting on four LLMs, we show that exploiting this extracted correctness representation outperforms standard log-likelihood ranking, as well as verbalized model confidence. Furthermore, we explore how this internal correctness signal can be used to select higher-quality code samples, without requiring test execution. Ultimately, this work demonstrates how leveraging internal representations can enhance code generation systems and make LLMs more reliable, thus improving confidence in automatically generated code.

</details>


### [103] [Systematic Evaluation of Black-Box Checking for Fast Bug Detection](https://arxiv.org/abs/2512.07434)
*Bram Pellen,María Belén Rodríguez,Frits Vaandrager,Petra van den Bos*

Main category: cs.SE

TL;DR: 本文系统评估了黑盒检查方法在实际协议和控制器中快速发现安全漏洞的能力，验证其比传统方法更高效、更有效。


<details>
  <summary>Details</summary>
Motivation: 现有应用中模型检测通常只在最终模型无反例时使用，BBC则在所有假设模型中使用，本文系统评估BBC快速发现漏洞的能力。

Method: 结合主动自动机学习、基于模型的测试和模型检测，特别是在黑盒检查（BBC）方法中对所有假设模型应用模型检测。

Result: BBC在完全模型可学习时仅用3.4%的查询即可检测违规，相比传统方法显著提升；在无法完全学习时仍能检测大量违规，能检测94%的安全性质违规，且比现有MBT算法更有效发现深层漏洞。

Conclusion: 黑盒检查（BBC）方法在自动机学习和测试中显著加速和提升了安全性质违规的检测效果，是发现实现中深层次漏洞的更优方法。

Abstract: Combinations of active automata learning, model-based testing and model checking have been successfully used in numerous applications, e.g., for spotting bugs in implementations of major network protocols and to support refactoring of embedded controllers. However, in the large majority of these applications, model checking is only used at the very end, when no counterexample can be found anymore for the latest hypothesis model. This contrasts with the original proposal of black-box checking (BBC) by Peled, Vardi & Yannakakis, which applies model checking for all hypotheses, also the intermediate ones. In this article, we present the first systematic evaluation of the ability of BBC to find bugs quickly, based on 77 benchmarks models from real protocol implementations and controllers for which specifications of safety properties are available. Our main finding are: (a) In cases where the full model can be learned, BBC detects violations of the specifications with just 3.4% of the queries needed by an approach in which model checking is only used for the full model. (b) Even when the full model cannot be learned, BBC is still able to detect many violations of the specification. In particular, BBC manages to detect 94% of the safety properties violations in the challenging RERS 2019 industrial LTL benchmarks. (c) Our results also confirm that BBC is way more effective than existing MBT algorithms in finding deep bugs in implementations.

</details>


### [104] [AutoICE: Automatically Synthesizing Verifiable C Code via LLM-driven Evolution](https://arxiv.org/abs/2512.07501)
*Weilin Luo,Xueyi Liang,Haotian Deng,Yanan Liu,Hai Wan*

Main category: cs.SE

TL;DR: AutoICE利用大型语言模型和进化搜索，显著提升了从自然语言需求自动合成可验证C代码的成功率，超越现有最先进技术。


<details>
  <summary>Details</summary>
Motivation: 自动从自然语言需求合成可验证代码有助于确保软件正确性和可靠性，但现有方法因缺乏领域预训练语料和难以有效形式化隐含知识，导致语法和语义错误较多。

Method: 提出AutoICE，一种结合多样性个体初始化、多样协同交叉和自我反思变异的基于大型语言模型的进化搜索方法，以合成可验证的C代码。

Result: AutoICE验证成功率达90.36%，显著优于现有最先进方法。在针对开发者友好数据集上，成功率为88.33%，远超现有方法的65%。

Conclusion: AutoICE有效提升了自动形式化代码合成的准确性和可靠性，尤其在发现隐含知识和减少错误传播方面表现优异，推动了基于LLM的自动形式化技术发展。

Abstract: Automatically synthesizing verifiable code from natural language requirements ensures software correctness and reliability while significantly lowering the barrier to adopting the techniques of formal methods. With the rise of large language models (LLMs), long-standing efforts at autoformalization have gained new momentum. However, existing approaches suffer from severe syntactic and semantic errors due to the scarcity of domain-specific pre-training corpora and often fail to formalize implicit knowledge effectively. In this paper, we propose AutoICE, an LLM-driven evolutionary search for synthesizing verifiable C code. It introduces the diverse individual initialization and the collaborative crossover to enable diverse iterative updates, thereby mitigating error propagation inherent in single-agent iterations. Besides, it employs the self-reflective mutation to facilitate the discovery of implicit knowledge. Evaluation results demonstrate the effectiveness of AutoICE: it successfully verifies $90.36$\% of code, outperforming the state-of-the-art (SOTA) approach. Besides, on a developer-friendly dataset variant, AutoICE achieves a $88.33$\% verification success rate, significantly surpassing the $65$\% success rate of the SOTA approach.

</details>


### [105] [Understanding Privacy Risks in Code Models Through Training Dynamics: A Causal Approach](https://arxiv.org/abs/2512.07814)
*Hua Yang,Alejandro Velasco,Sen Fang,Bowen Xu,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: 本研究首次从因果角度证明了不同PII类型存在不同程度的泄露风险，并基于训练动态揭示了其内在机制，指导针对不同PII类型的防护策略设计。


<details>
  <summary>Details</summary>
Motivation: 现有研究将个人可识别信息（PII）视为单一类别，忽视了不同PII类型之间泄露风险的异质性。

Method: 建立包含多种PII类型的数据集，微调不同规模的代表性大语言模型，计算真实PII数据上的训练动态，并构建结构因果模型估计学习性对泄露的因果影响。

Result: 不同PII类型的泄露风险存在显著差异，且与其训练动态相关：易于学习的PII类型（如IP地址）泄露风险较高，难以学习的（如密钥和密码）泄露较少，模糊类型表现不一。

Conclusion: PII的泄露风险依赖于其类型及对应的学习难度，建议未来针对不同类型PII采取差异化、基于学习性风险的防御措施以提升LLM4Code的隐私保护能力。

Abstract: Large language models for code (LLM4Code) have greatly improved developer productivity but also raise privacy concerns due to their reliance on open-source repositories containing abundant personally identifiable information (PII). Prior work shows that commercial models can reproduce sensitive PII, yet existing studies largely treat PII as a single category and overlook the heterogeneous risks among different types. We investigate whether distinct PII types vary in their likelihood of being learned and leaked by LLM4Code, and whether this relationship is causal. Our methodology includes building a dataset with diverse PII types, fine-tuning representative models of different scales, computing training dynamics on real PII data, and formulating a structural causal model to estimate the causal effect of learnability on leakage. Results show that leakage risks differ substantially across PII types and correlate with their training dynamics: easy-to-learn instances such as IP addresses exhibit higher leakage, while harder types such as keys and passwords leak less frequently. Ambiguous types show mixed behaviors. This work provides the first causal evidence that leakage risks are type-dependent and offers guidance for developing type-aware and learnability-aware defenses for LLM4Code.

</details>


### [106] [Studying the Role of Reusing Crowdsourcing Knowledge in Software Development](https://arxiv.org/abs/2512.07824)
*Rabe Abdalkareem*

Main category: cs.SE

TL;DR: 本文通过大规模实证研究分析了复用众包平台知识对软件项目的影响，发现其有助开发但也增加质量风险，提出改进持续集成以提升质量保障和开发效率的解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前关于软件工程中众包的经验性研究缺乏，特别是关于众包知识被开发者如何使用及其对软件质量的影响这一基础问题未得到解答。

Method: 对多个知名众包平台如Stack Overflow和npm进行了大规模的实证研究，分析了众包知识复用对软件项目的影响，并探讨了持续集成(CI)在软件质量保障中的应用。

Result: 研究发现复用众包平台上的知识尤其是代码，有助于提升软件开发，但也带来了依赖开销和维护工作量增加等质量问题。基于此，提出通过改进持续集成流程来提升开发者生产力并节省资源。

Conclusion: 复用众包知识在促进软件开发效率的同时，会引入质量风险，通过数据驱动的方法和改进持续集成，可以有效缓解这些风险，提升软件质量保障能力。

Abstract: Crowdsourcing platforms, such as Stack Overflow, have changed and impacted the software development practice. In these platforms, developers share and reuse their software development and programming experience. Therefore, a plethora of research work focused on crowdsourcing in software engineering and showed that, among other things, crowdsourced development tends to increase developers' productivity and reduce time-to-market. However, in crowdsourcing, the empirical studies of software quality are lacking, and simple questions, such as what developers use the crowdsourcing knowledge for, are unanswered.
  Therefore, our research focused on studying the impact of reusing crowdsourcing knowledge on software projects. To do so, we conduct several large-scale empirical studies on some of the well-known crowdsourcing platforms, including Stack Overflow and npm. Our results showed that reusing knowledge from these crowdsourcing platforms has the potential to assist software development practice, specifically in the form of reusing crowdsourced code. However, using such knowledge affects the quality of the software in several aspects, such as making the software projects suffer from dependency overhead and increasing the maintenance effort. Based on these findings, we use the gained knowledge to make sound data-driven decisions where we examine software quality assurance methods to mitigate the risk of relying on crowd sourcing knowledge in software development. We examine the use of continuous integration (CI). Our analysis showed how CI can be improved to increase developers' productivity and save their resources.

</details>
