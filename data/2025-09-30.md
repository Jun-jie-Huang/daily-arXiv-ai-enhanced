<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 195]
- [cs.SE](#cs.SE) [Total: 35]
- [cs.MA](#cs.MA) [Total: 6]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Are you sure? Measuring models bias in content moderation through uncertainty](https://arxiv.org/abs/2509.22699)
*Alessandra Urbinati,Mirko Lai,Simona Frenda,Marco Antonio Stranisci*

Main category: cs.CL

TL;DR: 本文提出了一种基于置信度的不监督方法，通过分析模型对少数族裔和女性标注者的分类不确定性，评估内容审核模型的公平性。


<details>
  <summary>Details</summary>
Motivation: 尽管已有资源旨在解决内容审核中的种族和社会偏见问题，但目前缺乏有效评估模型公平性的手段。作者希望通过新的方法更好地衡量和识别模型的偏见。

Method: 利用保序预测技术计算模型对易受影响群体（女性和非白人标注者）消息进行分类时的不确定性，以此作为分析模型偏见的代理指标，并与传统性能指标（如F1分数）进行对比。

Result: 发现一些预训练模型尽管对少数群体的标签预测准确率较高，但预测置信度较低，显示出模型在不同群体标注上的表现差异。

Conclusion: 通过测量模型预测的置信度，可以识别出哪些标注群体在预训练模型中被较好地代表，从而在模型实际应用前指导去偏过程，提升内容审核公平性。

Abstract: Automatic content moderation is crucial to ensuring safety in social media.
Language Model-based classifiers are being increasingly adopted for this task,
but it has been shown that they perpetuate racial and social biases. Even if
several resources and benchmark corpora have been developed to challenge this
issue, measuring the fairness of models in content moderation remains an open
issue. In this work, we present an unsupervised approach that benchmarks models
on the basis of their uncertainty in classifying messages annotated by people
belonging to vulnerable groups. We use uncertainty, computed by means of the
conformal prediction technique, as a proxy to analyze the bias of 11 models
against women and non-white annotators and observe to what extent it diverges
from metrics based on performance, such as the $F_1$ score. The results show
that some pre-trained models predict with high accuracy the labels coming from
minority groups, even if the confidence in their prediction is low. Therefore,
by measuring the confidence of models, we are able to see which groups of
annotators are better represented in pre-trained models and lead the debiasing
process of these models before their effective use.

</details>


### [2] [AccessEval: Benchmarking Disability Bias in Large Language Models](https://arxiv.org/abs/2509.22703)
*Srikant Panda,Amit Agarwal,Hitesh Laxmichand Patel*

Main category: cs.CL

TL;DR: 本文提出了AccessEval基准，评估21个大型语言模型在不同残疾类型和现实领域中对残疾相关查询的响应差异，发现模型对残疾查询表现出更多负面情绪、刻板印象和事实错误。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理现实生活查询时表现出差异，尤其是在涉及残疾用户的场景中，这些偏差可能带来实际的负面影响，因此需要系统评估模型在不同残疾语境中的表现。

Method: 构建AccessEval基准，涵盖21个闭源和开源模型，测试6个现实领域和9种残疾类型，使用配对的中性查询和残疾感知查询，评估情绪、社会感知和事实准确性指标。

Result: 模型在残疾感知查询上的回答更具负面情绪、存在更多刻板印象和事实错误，且这种影响在不同领域与残疾类型间差异显著，听力、言语和行动障碍受到的影响尤为严重。

Conclusion: 当前语言模型存在嵌入的能力歧视，通过实际决策场景分析展现了此类偏见可能对残疾用户造成的实质性伤害，强调了在实际应用中缓解偏见的重要性。

Abstract: Large Language Models (LLMs) are increasingly deployed across diverse domains
but often exhibit disparities in how they handle real-life queries. To
systematically investigate these effects within various disability contexts, we
introduce \textbf{AccessEval (Accessibility Evaluation)}, a benchmark
evaluating 21 closed- and open-source LLMs across 6 real-world domains and 9
disability types using paired Neutral and Disability-Aware Queries. We
evaluated model outputs with metrics for sentiment, social perception, and
factual accuracy.
  Our analysis reveals that responses to disability-aware queries tend to have
a more negative tone, increased stereotyping, and higher factual error compared
to neutral queries. These effects show notable variation by domain and
disability type, with disabilities affecting hearing, speech, and mobility
disproportionately impacted. These disparities reflect persistent forms of
ableism embedded in model behavior.
  By examining model performance in real-world decision-making contexts, we
better illuminate how such biases can translate into tangible harms for
disabled users. This framing helps bridges the gap between technical evaluation
and user impact, reinforcing importance of bias mitigation in day-to-day
applications. Our dataset is publicly available at:
https://huggingface.co/datasets/Srikant86/AccessEval

</details>


### [3] [RAR$^2$: Retrieval-Augmented Medical Reasoning via Thought-Driven Retrieval](https://arxiv.org/abs/2509.22713)
*Kaishuai Xu,Wenjun Hou,Yi Cheng,Wenjie Li*

Main category: cs.CL

TL;DR: 该论文提出了RAR²框架，通过联合学习推理增强检索与检索增强推理，提升复杂医学问答的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于检索增强生成（RAG）的方法难以处理需要深入推理的医学问题，且通常不显式建模推理过程，导致难以准确检索和整合临床相关知识。

Method: RAR²通过构建思维过程，发掘隐含的知识需求，指导检索与答案生成。采用混合偏好对进行训练，并通过直接偏好优化（DPO）训练模型，同时设计两种测试时扩展策略。

Result: 在多个生物医学问答数据集上，RAR²表现优于带或不带微调的RAG基线，证明了该框架的有效性。

Conclusion: RAR²通过联合推理和检索过程，有效提升了复杂医学问题的回答质量，展现了其在现实临床任务中的应用潜力。

Abstract: Large Language Models (LLMs) have shown promising performance on diverse
medical benchmarks, highlighting their potential in supporting real-world
clinical tasks. Retrieval-Augmented Generation (RAG) has emerged as a key
approach for mitigating knowledge gaps and hallucinations by incorporating
external medical information. However, RAG still struggles with complex medical
questions that require intensive reasoning, as surface-level input often fails
to reflect the true knowledge needs of the task. Existing methods typically
focus on refining queries without explicitly modeling the reasoning process,
limiting their ability to retrieve and integrate clinically relevant knowledge.
In this work, we propose RAR$^2$, a joint learning framework that improves both
Reasoning-Augmented Retrieval and Retrieval-Augmented Reasoning. RAR$^2$
constructs a thought process to uncover implicit knowledge requirements and
uses it to guide retrieval and answer generation. We build a training dataset
of mixed preference pairs and apply Direct Preference Optimization (DPO) to
train the model. Moreover, we design two test-time scaling strategies to
explore the boundaries of our framework. Experiments demonstrate the
effectiveness of RAR$^2$ across several biomedical question answering datasets,
outperforming RAG baselines with or without fine-tuning.

</details>


### [4] [TRUEBench: Can LLM Response Meet Real-world Constraints as Productivity Assistant?](https://arxiv.org/abs/2509.22715)
*Jiho Park,Jongyoon Song,Minjin Choi,Kyuho Heo,Taehun Huh,Ji Won Kim*

Main category: cs.CL

TL;DR: TRUEBench是一个真实世界使用评估基准，专门用于测试大型语言模型（LLMs）作为生产力助手的能力，涵盖12种语言、多轮复杂对话和隐性约束。


<details>
  <summary>Details</summary>
Motivation: 现有评测基准缺乏多语言支持，无法捕捉用户请求中的隐性约束，也忽略多轮对话的复杂性，难以真实反映LLMs的实际表现。

Method: 设计TRUEBench基准，包含12种语言的输入提示，跨实例多语言指令，严格评估显性和隐性约束，涵盖多轮对话及上下文切换，并使用LLM验证器优化约束表达。

Result: 实验显示TRUEBench相较现有基准更具挑战性，即使是强模型（如OpenAI o1）总体通过率仅69.07%。

Conclusion: TRUEBench为生产力场景中的LLMs提供了更严格、真实的评估，揭示了其优缺点，有助于推动其能力提升。

Abstract: Large language models (LLMs) are increasingly integral as productivity
assistants, but existing benchmarks fall short in rigorously evaluating their
real-world instruction-following capabilities. Current benchmarks often (i)
lack sufficient multilinguality, (ii) fail to capture the implicit constraints
inherent in user requests, and (iii) overlook the complexities of multi-turn
dialogue. To address these critical gaps and provide a more realistic
assessment, we introduce TRUEBench (Trustworthy Real-world Usage Evaluation
Benchmark)1, a novel benchmark specifically designed for LLM-based productivity
assistants. TRUEBench distinguishes itself by featuring input prompts across 12
languages, incorporating intra-instance multilingual instructions, employing
rigorous evaluation criteria to capture both explicit and implicit constraints,
and including complex multi-turn dialogue scenarios with both accumulating
constraints and context switches. Furthermore, to ensure reliability in
evaluation, we refined constraints using an LLM validator. Extensive
experiments demonstrate that TRUEBench presents significantly greater
challenges than existing benchmarks; for instance, a strong model like OpenAI
o1 achieved only a 69.07% overall pass rate. TRUEBench offers a demanding and
realistic assessment of LLMs in practical productivity settings, highlighting
their capabilities and limitations.

</details>


### [5] [Multi-Modal Sentiment Analysis with Dynamic Attention Fusion](https://arxiv.org/abs/2509.22729)
*Sadia Abdulhalim,Muaz Albaghdadi,Moshiur Farazi*

Main category: cs.CL

TL;DR: 本文提出了一种结合文本和语音特征的动态注意力融合模型，提高了情感分析的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统情感分析仅依赖文本，忽视了语音中的非语言线索，如语调和韵律，而这些对捕捉真实情感意图至关重要。

Method: 提出Dynamic Attention Fusion (DAF)框架，通过自适应注意力机制结合预训练语言模型的文本嵌入与语音编码器的声学特征，无需微调编码器。

Result: DAF模型在大型多模态基准测试中优于静态融合和单模态基线，在F1分数和预测误差上取得显著提升，并通过消融实验验证了动态加权策略的重要性。

Conclusion: 该方法有效整合了语言和非语言信息，为情感预测提供了更稳健的基础，对情绪识别、心理健康评估及人机交互有广泛应用价值。

Abstract: Traditional sentiment analysis has long been a unimodal task, relying solely
on text. This approach overlooks non-verbal cues such as vocal tone and prosody
that are essential for capturing true emotional intent. We introduce Dynamic
Attention Fusion (DAF), a lightweight framework that combines frozen text
embeddings from a pretrained language model with acoustic features from a
speech encoder, using an adaptive attention mechanism to weight each modality
per utterance. Without any finetuning of the underlying encoders, our proposed
DAF model consistently outperforms both static fusion and unimodal baselines on
a large multimodal benchmark. We report notable gains in F1-score and
reductions in prediction error and perform a variety of ablation studies that
support our hypothesis that the dynamic weighting strategy is crucial for
modeling emotionally complex inputs. By effectively integrating verbal and
non-verbal information, our approach offers a more robust foundation for
sentiment prediction and carries broader impact for affective computing
applications -- from emotion recognition and mental health assessment to more
natural human computer interaction.

</details>


### [6] [Enabling Approximate Joint Sampling in Diffusion LMs](https://arxiv.org/abs/2509.22738)
*Parikshit Bansal,Sujay Sanghavi*

Main category: cs.CL

TL;DR: 本文提出了一种轻量级采样器，可在一个完整模型前馈后，通过多次轻量级前馈实现近似多token联合采样，提高文本生成的准确性和速度。


<details>
  <summary>Details</summary>
Motivation: 传统自回归语言模型逐token顺序生成，确保采样于联合分布，但速度慢；掩码扩散模型并行非顺序生成，速度快但准确性低。需要一种方法在保证接近联合分布的同时实现并行采样。

Method: 在已有大规模扩散语言模型上，增加一层轻量级采样器层，该采样器经过训练模拟完整模型的精确联合采样。一次完整模型前馈后，可多次前馈采样器层实现多token联合采样。

Result: 该方法在Dream-7B-Base和Dream-7B-Instruct模型上验证，在每步解码并行解码4个token时，采样效果达到MAUVE评分0.87，远超边缘基线0.31，表明高效且准确的联合采样能力。

Conclusion: 通过设计并训练轻量级采样器，可以实现扩散语言模型中近似多token联合采样，兼顾文本生成的速度与精确性，推动并行非顺序文本生成技术发展。

Abstract: In autoregressive language models, each token is sampled by conditioning on
all the past tokens; the overall string has thus been sampled from the correct
underlying joint distribution represented by the model. In contrast, masked
diffusion language models generate text by unmasking tokens out of order and
potentially in parallel. Generating an overall string sampled from the correct
underlying joint distribution would (again) require exactly one token unmasking
in every full-model forward pass. The more tokens unmasked in parallel, the
further away the string is from the true joint; this can be seen in the
resulting drop in accuracy (but, increase in speed). In this paper we devise a
way to {\em approximately} sample multiple tokens from the joint distribution
in a single full-model forward pass; we do so by developing a new lightweight
single-layer ``sampler" on top of an existing large diffusion LM. One forward
pass of the full model can now be followed by multiple forward passes of only
this sampler layer, to yield multiple unmasked tokens. Our sampler is trained
to mimic exact joint sampling from the (frozen) full model. We show the
effectiveness of our approximate joint sampling for both pretrained-only
(Dream-7B-Base) and instruction-tuned (Dream-7B-Instruct) models on language
modeling and math \& coding tasks. When four tokens are unmasked for each
full-model denoising step, our sampling algorithm achieves a MAUVE score of
0.87 (vs marginal baseline of 0.31) with respect to the true joint
distribution.

</details>


### [7] [Painless Activation Steering: An Automated, Lightweight Approach for Post-Training Large Language Models](https://arxiv.org/abs/2509.22739)
*Sasha Cui,Zhongren Chen*

Main category: cs.CL

TL;DR: 本文提出了一种名为Painless Activation Steering（PAS）的自动化激活引导方法，用于语言模型的后训练，避免了手工提示或特征标注的麻烦，提升了行为任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型的后训练方法要么成本高、耗时长（基于权重调节），要么控制不精准且需大量试错（基于提示调节），而激活引导虽然有潜力，但通常需要繁琐的人工设计和标注。

Method: 提出PAS，通过自动化方式利用已有标注数据构建激活向量，实现无需人工介入的激活引导。对三款开放权重模型和18个任务进行评估，同时引入内省变体iPAS加强因果控制效果。

Result: PAS能显著提升行为相关任务的性能，在偏见、道德和对齐任务上表现尤为突出（偏见提升10.1%，道德5.2%，对齐34.8%）。此外，PAS还能与上下文学习和监督微调叠加增益。

Conclusion: PAS提供了一种实用、自动化、快速且轻量的语言模型后训练激活引导方案，明确了其适用场景与局限，为激活引导技术的实用化提供了有力支持。

Abstract: Language models (LMs) are typically post-trained for desired capabilities and
behaviors via weight-based or prompt-based steering, but the former is
time-consuming and expensive, and the latter is not precisely controllable and
often requires manual trial-and-error. While activation steering (AS) promises
a cheap, fast, and controllable alternative to the two existing post-training
methods, current AS techniques require hand-crafted prompt pairs or
labor-intensive feature annotation, making them more inconvenient than the
plug-and-play methods such as Reinforcement Learning (RL) and Supervised
Fine-Tuning (SFT). We introduce Painless Activation Steering (PAS), a family of
fully automated methods that make AS readily usable with any given labeled
dataset, with no need for prompt construction, feature labeling, or human
intervention. We evaluate PAS on three open-weight models
(Llama3.1-8B-Instruct, DeepSeek-R1-Distill-8B, and Nous-Hermes-2) and 18 tasks;
we find that PAS reliably improves performance for behavior tasks, but not for
intelligence-oriented tasks. The introspective variant (iPAS) delivers the
strongest causal steering effects (10.1% on Bias, 5.2% on Morality, and 34.8%
on Alignment). We also show PAS delivers additional gains on top of In-Context
Learning (ICL) and SFT. PAS constructs a fast, lightweight activation vector
that can be cheaply trained, easily stored, and activated at will. Our results
provide a characterization of where AS helps, where it fails, and how to deploy
it as a practical, automated LM post-training option.

</details>


### [8] [MIRAGE: Multi-hop Reasoning with Ambiguity Evaluation for Illusory Questions](https://arxiv.org/abs/2509.22750)
*Jeonghyun Park,Ingeol Baek,Seunghyun Yoon,Haeun Jang,Aparna Garimella,Akriti Jain,Nedim Lipka,Hwanhee Lee*

Main category: cs.CL

TL;DR: 本文提出了一个用于评估真实场景中多跳问答中歧义挑战的基准数据集MIRAGE，并提出了多代理框架CLARION以提升模型在处理多跳歧义推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 真实世界的多跳问答常包含难以避免的歧义，使得从单一问题产生多条推理路径，模型需在每一步解决歧义，但现有大型语言模型难以有效处理此类多层次歧义推理。

Method: 提出MIRAGE基准，包含1142个高质量模糊多跳问题，涵盖句法、一般和语义歧义，采用多模型验证流程确保质量；并设计多代理推理框架CLARION，提升歧义消解与推理能力。

Result: 实验显示即使是最先进模型在MIRAGE上表现仍不理想，表明多步推理中歧义解决是显著挑战；CLARION框架显著超越现有方法。

Conclusion: 解决多跳问答中的多层次歧义问题是智能推理的重要难点，MIRAGE为研究提供了有力工具，CLARION展示了改进路径，促进更健壮适应的推理系统发展。

Abstract: Real-world Multi-hop Question Answering (QA) often involves ambiguity that is
inseparable from the reasoning process itself. This ambiguity creates a
distinct challenge, where multiple reasoning paths emerge from a single
question, each requiring independent resolution. Since each sub-question is
ambiguous, the model must resolve ambiguity at every step. Thus, answering a
single question requires handling multiple layers of ambiguity throughout the
reasoning chain. We find that current Large Language Models (LLMs) struggle in
this setting, typically exploring wrong reasoning paths and producing
incomplete answers. To facilitate research on multi-hop ambiguity, we introduce
MultI-hop Reasoning with AmbiGuity Evaluation for Illusory Questions (MIRAGE),
a benchmark designed to analyze and evaluate this challenging intersection of
ambiguity interpretation and multi-hop reasoning. MIRAGE contains 1,142
high-quality examples of ambiguous multi-hop questions, categorized under a
taxonomy of syntactic, general, and semantic ambiguity, and curated through a
rigorous multi-LLM verification pipeline. Our experiments reveal that even
state-of-the-art models struggle on MIRAGE, confirming that resolving ambiguity
combined with multi-step inference is a distinct and significant challenge. To
establish a robust baseline, we propose CLarifying Ambiguity with a Reasoning
and InstructiON (CLARION), a multi-agent framework that significantly
outperforms existing approaches on MIRAGE, paving the way for more adaptive and
robust reasoning systems.

</details>


### [9] [ML2B: Multi-Lingual ML Benchmark For AutoML](https://arxiv.org/abs/2509.22768)
*Ekaterina Trofimova,Zosia Shamina,Maria Selifanova,Artem Zaitsev,Remi Savchuk,Maxim Minets,Daria Ozerova,Emil Sataev,Denis Zuenko,Andrey E. Ustyuzhanin*

Main category: cs.CL

TL;DR: 本文提出了ML2B，这是第一个多语言机器学习代码生成的基准测试，涵盖13种自然语言的30个Kaggle竞赛，使用自动评估框架AIDE进行评测，发现非英语任务性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 当前的机器学习代码生成基准测试仅限于英语，忽视了机器学习研究和实践的多语言特性。

Method: 构建ML2B基准，包含多语言翻译的竞赛数据，采用AIDE自动化框架对数据科学管道进行端到端评估。

Result: 非英语任务性能下降15-45%，揭示了多语言表示学习在代码生成中的关键挑战。

Conclusion: ML2B填补了多语言机器学习代码生成评测的空白，促进相关领域的研究发展。

Abstract: Large language models (LLMs) have recently demonstrated strong capabilities
in generating machine learning (ML) code, enabling end-to-end pipeline
construction from natural language instructions. However, existing benchmarks
for ML code generation are mainly restricted to English, overlooking the global
and multilingual nature of ML research and practice. To address this gap, we
present ML2B, the first benchmark for evaluating multilingual ML code
generation. ML2B consists of 30 Kaggle competitions translated into 13 natural
languages, covering tabular, text, and image data types, with structured
metadata and validated human-reviewed translations. For evaluation, we employ
AIDE, an automated framework for end-to-end assessment of data science
pipelines, and provide insights into cross-lingual model performance. Our
results reveal substantial 15-45% performance degradation on non-English tasks,
highlighting critical challenges in multilingual representation learning for
code generation. The benchmark, evaluation framework, and comprehensive results
are made available through our GitHub repository to facilitate future research
in multilingual ML code generation: https://github.com/enaix/ml2b.

</details>


### [10] [ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection](https://arxiv.org/abs/2509.22808)
*Mohamed Maged,Alhassan Ehab,Ali Mekky,Besher Hassan,Shady Shehata*

Main category: cs.CL

TL;DR: 本论文介绍了首个多方言阿拉伯语假声数据集，利用多模型评估与多方法分类器训练，发现FishSpeech模型生成的合成语音最具挑战性。


<details>
  <summary>Details</summary>
Motivation: 鉴于生成式文本转语音模型兴起，区分真实与合成阿拉伯语语音仍具挑战性，且相关研究稀缺。

Method: 构建多方言数据集，通过多模型生成音频，采用现代嵌入向量结合分类器、传统机器学习+MFCC、RawNet2架构训练分类器，辅以人工主观评分和ASR词错误率测评。

Result: FishSpeech在卡萨布兰卡语料库上的阿拉伯语语音克隆表现最佳，生成的合成语音更逼真且难以区分。

Conclusion: 单一TTS模型生成数据虽有优势，但限制了泛化能力，建议多模型融合以增强数据集的广泛适用性。

Abstract: With the rise of generative text-to-speech models, distinguishing between
real and synthetic speech has become challenging, especially for Arabic that
have received limited research attention. Most spoof detection efforts have
focused on English, leaving a significant gap for Arabic and its many dialects.
In this work, we introduce the first multi-dialect Arabic spoofed speech
dataset. To evaluate the difficulty of the synthesized audio from each model
and determine which produces the most challenging samples, we aimed to guide
the construction of our final dataset either by merging audios from multiple
models or by selecting the best-performing model, we conducted an evaluation
pipeline that included training classifiers using two approaches: modern
embedding-based methods combined with classifier heads; classical machine
learning algorithms applied to MFCC features; and the RawNet2 architecture. The
pipeline further incorporated the calculation of Mean Opinion Score based on
human ratings, as well as processing both original and synthesized datasets
through an Automatic Speech Recognition model to measure the Word Error Rate.
Our results demonstrate that FishSpeech outperforms other TTS models in Arabic
voice cloning on the Casablanca corpus, producing more realistic and
challenging synthetic speech samples. However, relying on a single TTS for
dataset creation may limit generalizability.

</details>


### [11] [EditGRPO: Reinforcement Learning with Post -Rollout Edits for Clinically Accurate Chest X-Ray Report Generation](https://arxiv.org/abs/2509.22812)
*Kai Zhang,Christopher Malon,Lichao Sun,Martin Renqiang Min*

Main category: cs.CL

TL;DR: 本论文提出了EditGRPO，一种针对放射学报告生成的混合策略强化学习算法，通过临床驱动的奖励优化生成质量，显著提升了模型性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型的监督微调目标未明确与临床有效性对齐，导致生成的放射学报告质量和泛化能力有限。

Method: 设计EditGRPO混合策略强化学习算法，结合在策探索和离策引导，注入句子级细节修正以提高采样效率和训练效果。

Result: EditGRPO在四个主要胸部X光报告生成数据集上的CheXbert、GREEN、Radgraph和RATEScore指标均优于传统监督微调和基础GRPO，性能提升平均3.4%。

Conclusion: 通过引入临床驱动奖励和混合策略的强化学习方法，EditGRPO有效提升了放射学报告生成的准确性和泛化能力，为医学图像报告生成提供了可行的优化路径。

Abstract: Radiology report generation requires advanced medical image analysis,
effective temporal reasoning, and accurate text generation. Although recent
innovations, particularly multimodal large language models (MLLMs), have shown
improved performance, their supervised fine-tuning (SFT) objective is not
explicitly aligned with clinical efficacy. In this work, we introduce EditGRPO,
a mixed-policy reinforcement learning (RL) algorithm designed specifically to
optimize the generation through clinically motivated rewards. EditGRPO
integrates on-policy exploration with off-policy guidance by injecting
sentence-level detailed corrections during training rollouts. This mixed-policy
approach addresses the exploration dilemma and sampling efficiency issues
typically encountered in RL. Applied to a Qwen2.5-VL-3B MLLM initialized with
supervised fine-tuning (SFT), EditGRPO outperforms both SFT and vanilla GRPO
baselines, achieving an average improvement of 3.4% in CheXbert, GREEN,
Radgraph, and RATEScore metrics across four major chest X-ray report generation
datasets. Notably, EditGRPO also demonstrates superior out-of-domain
generalization, with an average performance gain of 5.9% on unseen datasets.

</details>


### [12] [Critique-Coder: Enhancing Coder Models by Critique Reinforcement Learning](https://arxiv.org/abs/2509.22824)
*Chi Ruan,Dongfu Jiang,Yubo Wang,Wenhu Chen*

Main category: cs.CL

TL;DR: 本文提出了批判强化学习（CRL）方法，通过生成对（问题, 解决方案）对的批评以优化模型表现，结合标准强化学习训练了批判编码器（Critique-Coder），在代码生成和逻辑推理任务上均优于仅用强化学习的模型。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习主要关注生成响应，缺乏显式培养批判和反思能力的机制，受近期方法如CFT和CGD启发，本文旨在通过显式教学批判提升模型性能。

Method: 设计批判强化学习（CRL），让模型为（问题,解决方案）对生成批评，依据批评最终判断标签与真实标签的匹配给予奖励；提出Critique-Coder，混合使用80%标准RL数据和20% CRL数据进行训练。

Result: Critique-Coder在多项基准测试中均优于仅用强化学习的模型，在LiveCodeBench v5上表现超过60%，优于DeepCoder-14B和GPT-o1等推理模型，在逻辑推理任务中表现提升，证明了其通用推理和批判能力的提升。

Conclusion: 批判强化学习有效补充了传统强化学习，提高了大规模语言模型的推理与批判能力，且这种能力具有跨任务的可迁移性。

Abstract: Reinforcement Learning (RL) has emerged as a popular training paradigm,
particularly when paired with reasoning models. While effective, it primarily
focuses on generating responses and lacks mechanisms to explicitly foster
critique or reflection. Several recent studies, like Critique-Fine-Tuning (CFT)
and Critique-Guided-Distillation (CGD) have shown the benefits of explicitly
teaching LLMs how to critique. Motivated by them, we propose Critique
Reinforcement Learning (CRL), where the model is tasked with generating a
critique for a given (question, solution) pair. The reward is determined solely
by whether the final judgment label $c \in \{\texttt{True}, \texttt{False}\}$
of the generated critique aligns with the ground-truth judgment $c^*$. Building
on this point, we introduce \textsc{Critique-Coder}, which is trained on a
hybrid of RL and CRL by substituting 20\% of the standard RL data with CRL
data. We fine-tune multiple models (\textsc{Critique-Coder}) and evaluate them
on different benchmarks to show their advantages over RL-only models. We show
that \textsc{Critique-Coder} consistently outperforms RL-only baselines on all
the evaluated benchmarks. Notably, our \textsc{Critique-Coder-8B} can reach
over 60\% on LiveCodeBench (v5), outperforming other reasoning models like
DeepCoder-14B and GPT-o1. Beyond code generation, \textsc{Critique-Coder} also
demonstrates enhanced general reasoning abilities, as evidenced by its better
performance on logic reasoning tasks from the BBEH dataset. This indicates that
the application of CRL on coding datasets enhances general reasoning and
critique abilities, which are transferable across a broad range of tasks.
Hence, we believe that CRL works as a great complement to standard RL for LLM
reasoning.

</details>


### [13] [ChatInject: Abusing Chat Templates for Prompt Injection in LLM Agents](https://arxiv.org/abs/2509.22830)
*Hwan Chang,Yonghyun Jun,Hwanhee Lee*

Main category: cs.CL

TL;DR: 本文提出了一种新的间接提示注入攻击ChatInject，利用多轮对话操控大型语言模型（LLM）基于结构化聊天模板的指令追随特性，显著提升攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型代理与外部环境交互带来新的攻击面，传统的纯文本注入方法无法充分利用模型对结构化聊天模板的依赖，存在未被充分探索的漏洞。

Method: 提出ChatInject攻击，通过伪装成原生聊天模板的恶意负载，并利用多轮说服对话逐步诱导模型执行恶意指令。

Result: 实验证明ChatInject在多个前沿LLM上成功率远高于传统方法，多轮版本成功率更高，并且具有良好的跨模型迁移能力，对闭源模型仍有效，现有防御措施难以阻挡。

Conclusion: 当前基于LLM的代理系统在结构化聊天模板和多轮对话环境下存在严重安全隐患，亟需设计更有效的防御机制。

Abstract: The growing deployment of large language model (LLM) based agents that
interact with external environments has created new attack surfaces for
adversarial manipulation. One major threat is indirect prompt injection, where
attackers embed malicious instructions in external environment output, causing
agents to interpret and execute them as if they were legitimate prompts. While
previous research has focused primarily on plain-text injection attacks, we
find a significant yet underexplored vulnerability: LLMs' dependence on
structured chat templates and their susceptibility to contextual manipulation
through persuasive multi-turn dialogues. To this end, we introduce ChatInject,
an attack that formats malicious payloads to mimic native chat templates,
thereby exploiting the model's inherent instruction-following tendencies.
Building on this foundation, we develop a persuasion-driven Multi-turn variant
that primes the agent across conversational turns to accept and execute
otherwise suspicious actions. Through comprehensive experiments across frontier
LLMs, we demonstrate three critical findings: (1) ChatInject achieves
significantly higher average attack success rates than traditional prompt
injection methods, improving from 5.18% to 32.05% on AgentDojo and from 15.13%
to 45.90% on InjecAgent, with multi-turn dialogues showing particularly strong
performance at average 52.33% success rate on InjecAgent, (2)
chat-template-based payloads demonstrate strong transferability across models
and remain effective even against closed-source LLMs, despite their unknown
template structures, and (3) existing prompt-based defenses are largely
ineffective against this attack approach, especially against Multi-turn
variants. These findings highlight vulnerabilities in current agent systems.

</details>


### [14] [Learning to Detect Relevant Contexts and Knowledge for Response Selection in Retrieval-based Dialogue Systems](https://arxiv.org/abs/2509.22845)
*Kai Hua,Zhiyuan Feng,Chongyang Tao,Rui Yan,Lu Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种多轮响应选择模型RSM-DCK，通过检测对话上下文和知识中的相关部分，提高了检索式对话系统的响应匹配性能。


<details>
  <summary>Details</summary>
Motivation: 现有检索式对话系统中，上下文和知识中存在大量无关信息，特别是话题转移导致的信息冗余，影响了响应匹配的效果。

Method: 模型先以最近上下文为查询，在词级和话语级别预选上下文和知识中的相关部分；然后候选响应分别与选中的上下文和知识交互；最后利用融合表示进一步筛选知识相关部分以完成匹配。

Result: 在两个基准数据集上的评测结果显示，所提模型优于现有方法，且能有效检测相关上下文和知识。

Conclusion: RSM-DCK模型能够显著提升检索式知识驱动对话系统的响应选择效果，通过相关内容检测减少无关信息的干扰。

Abstract: Recently, knowledge-grounded conversations in the open domain gain great
attention from researchers. Existing works on retrieval-based dialogue systems
have paid tremendous efforts to utilize neural networks to build a matching
model, where all of the context and knowledge contents are used to match the
response candidate with various representation methods. Actually, different
parts of the context and knowledge are differentially important for recognizing
the proper response candidate, as many utterances are useless due to the topic
shift. Those excessive useless information in the context and knowledge can
influence the matching process and leads to inferior performance. To address
this problem, we propose a multi-turn \textbf{R}esponse \textbf{S}election
\textbf{M}odel that can \textbf{D}etect the relevant parts of the
\textbf{C}ontext and \textbf{K}nowledge collection (\textbf{RSM-DCK}). Our
model first uses the recent context as a query to pre-select relevant parts of
the context and knowledge collection at the word-level and utterance-level
semantics. Further, the response candidate interacts with the selected context
and knowledge collection respectively. In the end, The fused representation of
the context and response candidate is utilized to post-select the relevant
parts of the knowledge collection more confidently for matching. We test our
proposed model on two benchmark datasets. Evaluation results indicate that our
model achieves better performance than the existing methods, and can
effectively detect the relevant context and knowledge for response selection.

</details>


### [15] [Towards Generalizable Implicit In-Context Learning with Attention Routing](https://arxiv.org/abs/2509.22854)
*Jiaqian Li,Yanshu Li,Ligong Han,Ruixiang Tang,Wenya Wang*

Main category: cs.CL

TL;DR: 本文提出了一种名为In-Context Routing (ICR)的新型隐式上下文学习方法，通过调整注意力逻辑实现通用的上下文学习模式，显著提升了泛化能力和多任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有隐式上下文学习方法依赖任务特定的偏移向量，缺乏利用隐式上下文学习的结构机制，导致泛化能力有限。

Method: ICR通过提取隐式上下文学习中出现的结构方向，利用可学习的输入条件路由器调节注意力逻辑，实现一次训练多次使用。

Result: 在12个不同领域和多种大语言模型上，ICR在零样本和少样本任务中均优于现有隐式上下文学习方法，展现出良好的跨域泛化能力。

Conclusion: ICR方法有效内化了隐式上下文学习的结构特征，提升了大语言模型的少样本学习能力和泛化性能，拓宽了隐式上下文学习的实际应用边界。

Abstract: Implicit in-context learning (ICL) has newly emerged as a promising paradigm
that simulates ICL behaviors in the representation space of Large Language
Models (LLMs), aiming to attain few-shot performance at zero-shot cost.
However, existing approaches largely rely on injecting shift vectors into
residual flows, which are typically constructed from labeled demonstrations or
task-specific alignment. Such designs fall short of utilizing the structural
mechanisms underlying ICL and suffer from limited generalizability. To address
this, we propose In-Context Routing (ICR), a novel implicit ICL method that
internalizes generalizable ICL patterns at the attention logits level. It
extracts reusable structural directions that emerge during ICL and employs a
learnable input-conditioned router to modulate attention logits accordingly,
enabling a train-once-and-reuse framework. We evaluate ICR on 12 real-world
datasets spanning diverse domains and multiple LLMs. The results show that ICR
consistently outperforms prior implicit ICL methods that require task-specific
retrieval or training, while demonstrating robust generalization to
out-of-domain tasks where existing methods struggle. These findings position
ICR to push the boundary of ICL's practical value.

</details>


### [16] [The Bias is in the Details: An Assessment of Cognitive Bias in LLMs](https://arxiv.org/abs/2509.22856)
*R. Alexander Knipper,Charles S. Knipper,Kaiqi Zhang,Valerie Sims,Clint Bowers,Santu Karmaker*

Main category: cs.CL

TL;DR: 本文通过大规模评估45个大型语言模型在八种认知偏见上的表现，发现它们在17.8-57.3%的情况下展现出与人类相似的认知偏见。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在实际决策中的应用日益广泛，研究其是否存在认知偏见变得尤为重要。

Method: 构建了基于多项选择题的评估框架，与心理学家合作制作220个决策场景，设计多样化提示语，从而生成280万条模型响应。

Result: 模型在多种认知偏见中展现出与偏见相符的行为，且模型规模和提示细节对偏见程度有显著影响，较大模型和更详细提示可在一定程度上降低偏见。

Conclusion: 大型语言模型确实存在多种认知偏见，但通过增大模型规模和优化提示设计可减轻这些偏见。研究对提升LLMs的公正性和可靠性具有指导意义。

Abstract: As Large Language Models (LLMs) are increasingly embedded in real-world
decision-making processes, it becomes crucial to examine the extent to which
they exhibit cognitive biases. Extensively studied in the field of psychology,
cognitive biases appear as systematic distortions commonly observed in human
judgments. This paper presents a large-scale evaluation of eight
well-established cognitive biases across 45 LLMs, analyzing over 2.8 million
LLM responses generated through controlled prompt variations. To achieve this,
we introduce a novel evaluation framework based on multiple-choice tasks,
hand-curate a dataset of 220 decision scenarios targeting fundamental cognitive
biases in collaboration with psychologists, and propose a scalable approach for
generating diverse prompts from human-authored scenario templates. Our analysis
shows that LLMs exhibit bias-consistent behavior in 17.8-57.3% of instances
across a range of judgment and decision-making contexts targeting anchoring,
availability, confirmation, framing, interpretation, overattribution, prospect
theory, and representativeness biases. We find that both model size and prompt
specificity play a significant role on bias susceptibility as follows: larger
size (>32B parameters) can reduce bias in 39.5% of cases, while higher prompt
detail reduces most biases by up to 14.9%, except in one case
(Overattribution), which is exacerbated by up to 8.8%.

</details>


### [17] [Lexicon-Enriched Graph Modeling for Arabic Document Readability Prediction](https://arxiv.org/abs/2509.22870)
*Passant Elchafei,Mayar Osama,Mohamed Rageh,Mervat Abuelkheir*

Main category: cs.CL

TL;DR: 本文提出了一种基于图神经网络和词典丰富的节点特征相结合的方法，用于阿拉伯语文档可读性预测，在BAREC 2025共享任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前可读性评估多针对句子或文档单独处理，缺乏综合利用句子间关系和词典信息的系统，尤其在阿拉伯语中表现不足。

Method: 将文档建模为句子和词元节点的图，利用词典（SAMER）特征和阿拉伯语变换模型嵌入增强节点信息，通过图神经网络和变换器分别编码，后在推断阶段融合两者预测结果，句子级输出用最大池化汇总为文档级结果。

Result: 该混合模型在多个可读性指标上的性能优于单独使用图神经网络或变换器，降低了单方法的局限性。特别是融合策略提升了文档层面的预测效果，而仅用图神经网络对句子层面可读性预测更准确。

Conclusion: 图神经网络与变换器模型的融合提供了阿拉伯语文档可读性评估的有效策略，体现了结合结构信息和上下文特征的优势，为文本可读性分析提供了新路径。

Abstract: We present a graph-based approach enriched with lexicons to predict
document-level readability in Arabic, developed as part of the Constrained
Track of the BAREC Shared Task 2025. Our system models each document as a
sentence-level graph, where nodes represent sentences and lemmas, and edges
capture linguistic relationships such as lexical co-occurrence and class
membership. Sentence nodes are enriched with features from the SAMER lexicon as
well as contextual embeddings from the Arabic transformer model. The graph
neural network (GNN) and transformer sentence encoder are trained as two
independent branches, and their predictions are combined via late fusion at
inference. For document-level prediction, sentence-level outputs are aggregated
using max pooling to reflect the most difficult sentence. Experimental results
show that this hybrid method outperforms standalone GNN or transformer branches
across multiple readability metrics. Overall, the findings highlight that
fusion offers advantages at the document level, but the GNN-only approach
remains stronger for precise prediction of sentence-level readability.

</details>


### [18] [HEART: Emotionally-driven test-time scaling of Language Models](https://arxiv.org/abs/2509.22876)
*Gabriela Pinto,Palash Goyal,Yiwen Song,Souradip Chakraborty,Zifeng Wang,Tomas Pfister,Hamid Palangi*

Main category: cs.CL

TL;DR: 本文提出了HEART框架，通过情感驱动的迭代自我纠错提升语言模型在复杂推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前推理改进方法多侧重逻辑或结构优化，忽视了情感反馈对认知性能的调节作用。

Method: 利用由Paul Ekman六种基本情绪短语组成的情感反馈，以不同情绪语气迭代引导模型修正错误推理。

Result: 在多个难度推理基准测试中，HEART与oracle验证器结合显著提升模型准确率，表现优于现有最先进方法。

Conclusion: 情感驱动的反馈机制为机器推理带来新突破，但在无验证器环境下效果不稳定，未来需解决这一瓶颈。

Abstract: Test-time scaling has shown considerable success in improving the performance
of language models on complex reasoning tasks without requiring fine-tuning.
However, current strategies such as self-reflection primarily focus on logical
or structural refinement. They do not leverage the guiding potential of
affective feedback. Inspired by psychological research showing that emotions
can modulate cognitive performance, we introduce HEART--a novel framework that
uses emotionally-driven prompts for iterative self-correction. HEART provides
feedback on a model's incorrect response using a curated set of concise,
emotionally charged phrases based on the six universal emotions categorized by
Dr. Paul Ekman. By systematically varying the emotional tone of the feedback
across iterations, our method guides the model to escape flawed reasoning paths
and explore more promising alternatives. We evaluate our framework on
challenging reasoning benchmarks including OlympiadBench, Humanity's Last Exam,
and SimpleQA. Our results reveal a significant new phenomenon: when guided by
an oracle verifier, this affective iteration protocol unlocks significantly
deeper reasoning, leading to consistent and substantial increases in accuracy
over state-of-the-art baselines with the same verifier. However, we also
identify a critical bottleneck for practical deployment. In a verifier-free
setting, it struggles to harness these gains consistently, highlighting as a
key challenge for future work. Our findings suggest that the next frontier in
machine reasoning may lie not just in refining logic, but also in understanding
and leveraging the `HEART' of the models.

</details>


### [19] [Infusing Theory of Mind into Socially Intelligent LLM Agents](https://arxiv.org/abs/2509.22887)
*EunJeong Hwang,Yuwei Yin,Giuseppe Carenini,Peter West,Vered Shwartz*

Main category: cs.CL

TL;DR: 本文提出了在大型语言模型（LLM）中引入心理理论（Theory of Mind, ToM）以提升对话效果，提出了ToMAgent（ToMA）这种结合了心理状态预测和对话前瞻的对话代理。


<details>
  <summary>Details</summary>
Motivation: 现有聊天机器人和基于LLM的社交代理通常未集成心理理论能力，限制其理解他人心理状态的能力，影响对话质量和目标达成。

Method: 通过在对话轮次之间生成和利用心理状态信息，并训练ToMA结合ToM与对话前瞻机制，最大化心理状态在实现对话目标中的作用。

Result: 在Sotopia交互式社交评估基准上的实验表明，ToMA相比多个基线模型表现更优，表现出更具策略性、目标导向的推理能力，能更好地适应长远对话并维护良好关系。

Conclusion: 本文展示了将心理理论整合进LLM对话代理的可行性和有效性，推动了构建具备社会智能的语言模型代理的发展。

Abstract: Theory of Mind (ToM)-an understanding of the mental states of others-is a key
aspect of human social intelligence, yet, chatbots and LLM-based social agents
do not typically integrate it. In this work, we demonstrate that LLMs that
explicitly use ToM get better at dialogue, achieving goals more effectively.
After showing that simply prompting models to generate mental states between
dialogue turns already provides significant benefit, we further introduce
ToMAgent (ToMA), a ToM-focused dialogue agent. ToMA is trained by pairing ToM
with dialogue lookahead to produce mental states that are maximally useful for
achieving dialogue goals. Experiments on the Sotopia interactive social
evaluation benchmark demonstrate the effectiveness of our method over a range
of baselines. Comprehensive analysis shows that ToMA exhibits more strategic,
goal-oriented reasoning behaviors, which enable long-horizon adaptation, while
maintaining better relationships with their partners. Our results suggest a
step forward in integrating ToM for building socially intelligent LLM agents.

</details>


### [20] [Extract-0: A Specialized Language Model for Document Information Extraction](https://arxiv.org/abs/2509.22906)
*Henrique Godoy*

Main category: cs.CL

TL;DR: 提出了Extract-0，一个7亿参数的专门优化文档信息提取的语言模型，性能超过了参数量大得多的模型，如GPT-4.1。


<details>
  <summary>Details</summary>
Motivation: 通过专门优化文档信息提取任务，提升模型在多样文档提取任务上的表现，同时降低计算资源消耗。

Method: 结合合成数据生成、Low-Rank Adaptation监督微调以及基于Group Relative Policy Optimization的强化学习，使用内存保留合成数据生成以及只微调0.53%模型参数，并引入基于语义相似性的奖励函数处理提取任务的不确定性。

Result: 在1000个多样文档提取任务基准上，Extract-0的平均奖励为0.573，优于GPT-4.1(0.457)、o3(0.464)及GPT-4.1-2025(0.459)。

Conclusion: 任务专门优化可以使模型在降低计算资源的同时，性能超过通用大型模型，证明了专用模型的优势。

Abstract: This paper presents Extract-0, a 7-billion parameter language model
specifically optimized for document information extraction that achieves
performance exceeding models with parameter counts several orders of magnitude
larger. Through a novel combination of synthetic data generation, supervised
fine-tuning with Low-Rank Adaptation (LoRA), and reinforcement learning via
Group Relative Policy Optimization (GRPO), Extract-0 achieves a mean reward of
0.573 on a benchmark of 1,000 diverse document extraction tasks, outperforming
GPT-4.1 (0.457), o3 (0.464), and GPT-4.1-2025 (0.459). The training methodology
employs a memory-preserving synthetic data generation pipeline that produces
280,128 training examples from diverse document sources, followed by
parameterefficient fine-tuning that modifies only 0.53% of model weights (40.4M
out of 7.66B parameters). The reinforcement learning phase introduces a novel
semantic similarity-based reward function that handles the inherent ambiguity
in information extraction tasks. This research demonstrates that task-specific
optimization can yield models that surpass general-purpose systems while
requiring substantially fewer computational resource.

</details>


### [21] [Large language models management of medications: three performance analyses](https://arxiv.org/abs/2509.22926)
*Kelli Henry,Steven Xu,Kaitlin Blotske,Moriah Cargile,Erin F. Barreto,Brian Murray,Susan Smith,Seth R. Bauer,Yanjun Gao,Tianming Liu,Andrea Sikora*

Main category: cs.CL

TL;DR: 本文评估了GPT-4o在药物配方匹配、药物相互作用识别和药物处方句子生成三项药物测试中的表现，结果显示模型准确性较低，尤其是药物配方匹配和药物相互作用识别存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在医疗诊断中特别是药物用药方案推荐上的一致性和准确性，以促进其临床应用。

Method: 使用GPT-4o执行三项药物基准测试，分别为药物名称到正确配方的映射、内部知识及网络搜索识别药物相互作用，以及依据药物名称生成药物处方句子。通过TF-IDF余弦相似度、Levenshtein相似度、ROUGE指标及临床医生手动评估量化准确率。

Result: GPT-4o在药物配方匹配上表现不佳，常出现遗漏和虚构配方，完全匹配准确率仅49%。药物配方复杂度越高，准确率越低。药物相互作用识别不一致，网络搜索辅助时效果有提升，但无相互作用时反而表现更差。处方句子生成准确率为65.8%。

Conclusion: 模型在所有测试中的表现均较差，强调了需要基于临床标注数据的领域特定训练及建立全面的评估框架来提升模型在医疗药物推荐中的可靠性。

Abstract: Background: Large language models (LLMs) can be useful in diagnosing medical
conditions, but few studies have evaluated their consistency in recommending
appropriate medication regimens. The purpose of this evaluation was to test
GPT-4o on three medication benchmarking tests including mapping a drug name to
its correct formulation, identifying drug-drug interactions using both its
internal knowledge and using a web search, and preparing a medication order
sentence after being given the medication name. Methods: Using GTP-4o three
experiments were completed. Accuracy was quantified by computing cosine
similarity on TF-IDF vectors, normalized Levenshtein similarity, and
ROUGE-1/ROUGE-L F1 between each response and its reference string or by manual
evaluation by clinicians. Results: GPT-4o performed poorly on drug-formulation
matching, with frequent omissions of available drug formulations (mean 1.23 per
medication) and hallucinations of formulations that do not exist (mean 1.14 per
medication). Only 49% of tested medications were correctly matched to all
available formulations. Accuracy was decreased for medications with more
formulations (p<0.0001). GPT-4o was also inconsistent at identifying
drug-drug-interactions, although it had better performance with the
search-augmented assessment compared to its internal knowledge (54.7% vs.
69.2%, p=0.013). However, allowing a web-search worsened performance when there
was no drug-drug interaction (median % correct 100% vs. 40%, p<0.001). Finally,
GPT-4o performed moderately with preparing a medication order sentence, with
only 65.8% of medication order sentences containing no medication or
abbreviation errors. Conclusions: Model performance was overall poor for all
tests. This highlights the need for domain-specific training through
clinician-annotated datasets and a comprehensive evaluation framework for
benchmarking performance.

</details>


### [22] [LLMs Behind the Scenes: Enabling Narrative Scene Illustration](https://arxiv.org/abs/2509.22940)
*Melissa Roemmele,John Joon Young Chung,Taewook Kim,Yuqian Sun,Alex Calderwood,Max Kreminski*

Main category: cs.CL

TL;DR: 本论文提出一种基于大型语言模型（LLMs）和文本到图像模型的叙事场景自动插图生成流程，并发布了包含人工质量评估的新数据集SceneIllustrations。


<details>
  <summary>Details</summary>
Motivation: 希望利用生成式AI，将文本故事自动转换为场景插图，提升叙事表达的视觉表现力。

Method: 构建基于LLMs的提示接口，驱动文本到图像模型自动生成故事场景插图，并通过人工标注获得插图质量数据。

Result: 生成了SceneIllustrations数据集，实验证明LLMs能有效将故事隐含的场景知识语言化，促进插图生成与质量评估。

Conclusion: LLMs在跨模态叙事转换中展现出强大能力，推动了文本生成图像的自动化及其质量判定，具备广泛应用潜力。

Abstract: Generative AI has established the opportunity to readily transform content
from one medium to another. This capability is especially powerful for
storytelling, where visual illustrations can illuminate a story originally
expressed in text. In this paper, we focus on the task of narrative scene
illustration, which involves automatically generating an image depicting a
scene in a story. Motivated by recent progress on text-to-image models, we
consider a pipeline that uses LLMs as an interface for prompting text-to-image
models to generate scene illustrations given raw story text. We apply
variations of this pipeline to a prominent story corpus in order to synthesize
illustrations for scenes in these stories. We conduct a human annotation task
to obtain pairwise quality judgments for these illustrations. The outcome of
this process is the SceneIllustrations dataset, which we release as a new
resource for future work on cross-modal narrative transformation. Through our
analysis of this dataset and experiments modeling illustration quality, we
demonstrate that LLMs can effectively verbalize scene knowledge implicitly
evoked by story text. Moreover, this capability is impactful for generating and
evaluating illustrations.

</details>


### [23] [What Matters More For In-Context Learning under Matched Compute Budgets: Pretraining on Natural Text or Incorporating Targeted Synthetic Examples?](https://arxiv.org/abs/2509.22947)
*Mohammed Sabry,Anya Belz*

Main category: cs.CL

TL;DR: 本文研究了在保持计算成本不变的情况下，是否通过显式训练归纳线路能提升上下文学习(ICL)能力，发现自然文本训练表现同样出色。


<details>
  <summary>Details</summary>
Motivation: 探索通过注入特定合成数据加速归纳头的生成，从而提升模型的上下文学习能力。

Method: 提出Bi-Induct课程，向预训练中注入前向复制、后向复制或混合模式，训练0.13B到1B参数规模模型，评估少样本ICL、归纳头激活及语言建模困惑度。

Result: Bi-Induct加速了小规模模型归纳头的出现，但未显著提升泛化能力。自然文本训练在功能性ICL和标准语言建模表现上同样甚至更优，合成数据对困惑度的负面影响随模型规模减小。归纳头的关键作用通过消融实验得到证实。

Conclusion: 显式诱导归纳头激活并不足以提升ICL，关键在于归纳线路成为功能必要部分。预训练需注重机制感知和促进关键线路的负载，而非仅靠结构存在。

Abstract: Does explicitly exercising the induction circuit during pretraining improve
in-context learning (ICL), or is natural text sufficient when compute is held
constant (iso-FLOPs)? To test whether targeted synthetic data can accelerate
induction-head emergence and enhance ICL, we introduce Bi-Induct, a lightweight
curriculum that injects forward-copy (Induction), backward-copy (Anti), or a
balanced mix into the pretraining stream. We train models from 0.13B to 1B
parameters under iso-FLOPs, evaluating (i) few-shot ICL benchmarks, (ii)
head-level telemetry, and (iii) held-out language modeling perplexity. Our
findings challenge the assumption that early induction circuit activation
directly improves ICL. While Bi-Induct accelerates induction-head emergence at
small scales, this does not consistently yield stronger generalization. On
standard LM benchmarks, Bi-Induct matches natural-only training; on
function-style ICL probes, the 1B natural-only performs best. Stress tests
(e.g., label permutation, HITS@1 vs. HITS@3, 1 vs. 10 shots) preserve these
trends. Telemetry shows larger natural-only models develop broader, earlier
induction heads without explicit induction patterns. Anti-induction data fails
to elicit meaningful activation. Perplexity penalties from synthetic data
shrink with scale, suggesting larger models can absorb non-natural patterns
with minimal cost. Crucially, ablating the top 2% of induction heads degrades
ICL more than random ablations, especially for natural-only models, indicating
more centralized, load-bearing circuits. Bi-Induct variants exhibit more
redundant induction activity, implying different circuit utilization. Overall,
inducing activation is not sufficient: ICL gains depend on these circuits
becoming functionally necessary. These results underscore mechanism-aware
pretraining diagnostics and data mixtures that foster load-bearing, not merely
present, structure.

</details>


### [24] [Emergent morpho-phonological representations in self-supervised speech models](https://arxiv.org/abs/2509.22973)
*Jon Gauthier,Canaan Breiss,Matthew Leonard,Edward F. Chang*

Main category: cs.CL

TL;DR: 该论文研究了自监督语音模型如何表征语言现象，发现模型通过一种线性几何结构连接单词及其词形变化，且该结构更多反映单词对的分布关系而非传统的音系或形态学单位。


<details>
  <summary>Details</summary>
Motivation: 当前不清楚自监督语音模型在识别自然环境下的口语单词时采用了何种语言表征策略。

Method: 通过分析针对单词识别优化的S3M变体，重点研究其如何表征英语名词和动词的频繁词形变化中的音系和形态学现象。

Result: 发现模型的表征呈现全球线性几何结构，能够映射基本单词与其规则词形变化形式之间的关系，这种结构更多反映词汇中的分布式关系，而非直接对应音系或形态学单位。

Conclusion: 这种基于分布关系的线性几何结构提示了潜在人类口语词汇识别的表征策略，挑战了对音系和形态学表征需要区分的传统观点。

Abstract: Self-supervised speech models can be trained to efficiently recognize spoken
words in naturalistic, noisy environments. However, we do not understand the
types of linguistic representations these models use to accomplish this task.
To address this question, we study how S3M variants optimized for word
recognition represent phonological and morphological phenomena in frequent
English noun and verb inflections. We find that their representations exhibit a
global linear geometry which can be used to link English nouns and verbs to
their regular inflected forms.
  This geometric structure does not directly track phonological or
morphological units. Instead, it tracks the regular distributional
relationships linking many word pairs in the English lexicon -- often, but not
always, due to morphological inflection. These findings point to candidate
representational strategies that may support human spoken word recognition,
challenging the presumed necessity of distinct linguistic representations of
phonology and morphology.

</details>


### [25] [Same Content, Different Representations: A Controlled Study for Table QA](https://arxiv.org/abs/2509.22983)
*Yue Zhang,Seiji Maekawa,Nikita Bhutani*

Main category: cs.CL

TL;DR: 本文通过控制实验研究表格表示对表格问答（Table QA）性能的影响，比较结构化和半结构化表格的模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有的Table QA基准数据固定格式，未系统考察表格表示对模型性能的影响。作者希望揭示表示方式如何影响模型表现，支持模型选择和设计。

Method: 设计口头化流水线生成结构化与半结构化表格对，构建含表格大小、连接需求、查询复杂度、模式质量等维度的诊断基准，评估SQL方法、LLM、混合方法在不同表格结构的数据上的表现。

Result: SQL方法在结构化数据上表现佳但在半结构化数据上下降，LLM灵活但精度较低，混合方法兼顾两者优势，尤其在噪声模式下表现更好。大表和复杂查询使这些影响更明显。

Conclusion: 表格表示是Table QA表现的关键因素，无单一方法能优于所有条件。研究结果为模型选择和设计提供指导，促进更健壮的混合模型发展，适应多样化真实数据格式。

Abstract: Table Question Answering (Table QA) in real-world settings must operate over
both structured databases and semi-structured tables containing textual fields.
However, existing benchmarks are tied to fixed data formats and have not
systematically examined how representation itself affects model performance. We
present the first controlled study that isolates the role of table
representation by holding content constant while varying structure. Using a
verbalization pipeline, we generate paired structured and semi-structured
tables, enabling direct comparisons across modeling paradigms. To support
detailed analysis, we introduce a diagnostic benchmark with splits along table
size, join requirements, query complexity, and schema quality. Our experiments
reveal consistent trade-offs: SQL-based methods achieve high accuracy on
structured inputs but degrade on semi-structured data, LLMs exhibit flexibility
but reduced precision, and hybrid approaches strike a balance, particularly
under noisy schemas. These effects intensify with larger tables and more
complex queries. Ultimately, no single method excels across all conditions, and
we highlight the central role of representation in shaping Table QA
performance. Our findings provide actionable insights for model selection and
design, paving the way for more robust hybrid approaches suited for diverse
real-world data formats.

</details>


### [26] [ADAM: A Diverse Archive of Mankind for Evaluating and Enhancing LLMs in Biographical Reasoning](https://arxiv.org/abs/2509.22991)
*Jasin Cekinmez,Omid Ghahroodi,Saad Fowad Chandle,Dhiman Gupta,Ehsaneddin Asgari*

Main category: cs.CL

TL;DR: 论文提出了ADAM框架，系统评估和改进多模态大型语言模型在传记推理上的能力，涵盖多语言、多模态数据和不同认知水平。


<details>
  <summary>Details</summary>
Motivation: 传记知识作为重要的事实知识维度尚未被系统研究，现有模型在此领域表现不足，尤其在防止虚假信息生成方面存在挑战。

Method: 构建包含400多万人物的多语言多模态传记数据库AdamDB，设计基于布鲁姆认知分类法的六层次评测集AdamBench，以及提出检索增强生成系统AdamRAG以减少模型幻觉。

Result: AdamRAG显著提升了开源模型的表现，提升主要集中在低阶推理能力上，流行度对准确度有强烈影响，面部图像等多模态输入效果较检索手段有限。

Conclusion: ADAM框架首次建立了认知、文化和多模态结合的传记推理评测基准，为开发多语言、准确且抗幻觉的多模态语言模型提供了基础。

Abstract: We introduce ADAM (A Diverse Archive of Mankind), a framework for evaluating
and improving multimodal large language models (MLLMs) in biographical
reasoning. To the best of our knowledge, this is the first work to
systematically examine LLM capabilities in biography, a critical yet
underexplored dimension of factual knowledge. At its core, AdamDB is a
multilingual and multimodal dataset covering over 4 million individuals across
geography, time, and profession, while AdamBench provides cognitively
structured evaluations based on Bloom's taxonomy, spanning six reasoning levels
in both English and native languages. To address hallucinations, particularly
for lesser-known individuals, we propose AdamRAG, a retrieval-augmented
generation system tailored to biographical contexts. Experiments show that
AdamRAG substantially improves open-source models and modestly benefits
closed-source ones, with the largest gains on lower-order reasoning. Popularity
strongly mediates accuracy, and multimodal input via face images offers
smaller, less consistent improvements than retrieval. ADAM establishes the
first benchmark and framework for cognitively, culturally, and multimodally
grounded biographical evaluation, advancing the development of multilingual,
accurate, and hallucination-resistant MLLMs.

</details>


### [27] [AI Brown and AI Koditex: LLM-Generated Corpora Comparable to Traditional Corpora of English and Czech Texts](https://arxiv.org/abs/2509.22996)
*Jiří Milička,Anna Marklová,Václav Cvrček*

Main category: cs.CL

TL;DR: 本文介绍了两个由大型语言模型生成的英、捷两种语言文本语料库，旨在提供用于语言学比较的资源。


<details>
  <summary>Details</summary>
Motivation: 创建一个多样化、多主题且与现有人类文本语料库可比的资源，方便对比人类与模型生成文本的语言学特征。

Method: 采用多家机构的多款大型语言模型（如GPT-3到GPT-4.5）生成文本；文本涵盖多种类型，符合Universal Dependencies标注标准，包括分词、词元还原及形态句法注释。

Result: 共生成英、捷两部分语料，英文本约2700万词，捷文本约2150万词，语料已公开发布并通过捷克国家语料库检索界面提供访问。

Conclusion: 该资源为比较人类与机器生成文本的语言学特征提供了丰富的数据支持，促进相关研究发展。

Abstract: This article presents two corpora of English and Czech texts generated with
large language models (LLMs). The motivation is to create a resource for
comparing human-written texts with LLM-generated text linguistically. Emphasis
was placed on ensuring these resources are multi-genre and rich in terms of
topics, authors, and text types, while maintaining comparability with existing
human-created corpora. These generated corpora replicate reference human
corpora: BE21 by Paul Baker, which is a modern version of the original Brown
Corpus, and Koditex corpus that also follows the Brown Corpus tradition but in
Czech. The new corpora were generated using models from OpenAI, Anthropic,
Alphabet, Meta, and DeepSeek, ranging from GPT-3 (davinci-002) to GPT-4.5, and
are tagged according to the Universal Dependencies standard (i.e., they are
tokenized, lemmatized, and morphologically and syntactically annotated). The
subcorpus size varies according to the model used (the English part contains on
average 864k tokens per model, 27M tokens altogether, the Czech partcontains on
average 768k tokens per model, 21.5M tokens altogether). The corpora are freely
available for download under the CC BY 4.0 license (the annotated data are
under CC BY-NC-SA 4.0 licence) and are also accessible through the search
interface of the Czech National Corpus.

</details>


### [28] [Look Back to Reason Forward: Revisitable Memory for Long-Context LLM Agents](https://arxiv.org/abs/2509.23040)
*Yaorui Shi,Yuxin Chen,Siyuan Wang,Sihang Li,Hengxing Cai,Qi Gu,Xiang Wang,An Zhang*

Main category: cs.CL

TL;DR: 针对大规模语言模型在长文档问答中面临的证据分散问题，提出了带有回调增强记忆的ReMemR1代理和结合多层奖励的强化学习方法RLMLR，提升了信息保存和多跳推理能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型在处理长上下文问答时，现有“边读边记忆”方法存在前向处理不可逆、信息覆盖损失和稀疏强化学习信号等问题。

Method: 提出ReMemR1记忆增强代理，支持从完整记忆历史中选择性检索和非线性推理，配合RLMLR强化学习方法，通过结合最终答案奖励和密集的步骤级奖励，提升记忆的有效利用和训练监督。

Result: ReMemR1在长文档问答任务中相较于现有基于记忆的方法表现出显著提升，验证了其在长上下文推理中的有效性。

Conclusion: ReMemR1结合回调增强记忆和多层奖励强化学习，有效缓解信息退化和增强多跳推理，成为长上下文推理任务中的有效解决方案。

Abstract: Large language models face challenges in long-context question answering,
where key evidence of a query may be dispersed across millions of tokens.
Existing works equip large language models with a memory corpus that is
dynamically updated during a single-pass document scan, also known as the
"memorize while reading" methods. While this approach scales efficiently, it
suffers from irreversible forward-only processing, information loss through
overwriting, and sparse reinforcement learning signals. To tackle these
challenges, we present ReMemR1, a memory-augmented agent with callback-enhanced
memory that allows selective retrieval from the entire memory history and
allows non-linear reasoning and revisiting of early evidence. To further
strengthen training, we propose Reinforcement Learning with Multi-Level Rewards
(RLMLR), which combines final-answer rewards with dense, step-level signals
that guide effective memory use. Together, these contributions mitigate
information degradation, improve supervision, and support multi-hop memory
utilizing. Experiments on long-document QA show significant gains over existing
memory-based approaches, which validates ReMemR1 as an effective solution for
long-context reasoning agents.

</details>


### [29] [Peacemaker or Troublemaker: How Sycophancy Shapes Multi-Agent Debate](https://arxiv.org/abs/2509.23055)
*Binwei Yao,Chao Shang,Wanyu Du,Jianfeng He,Ruixue Lian,Yi Zhang,Hang Su,Sandesh Swamy,Yanjun Qi*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在多智能体辩论系统中表现出的阿谀行为，提出了评估指标和框架，分析其对辩论结果的负面影响，并给出了设计原则以改善系统表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在阿谀倾向，导致多智能体辩论中辩论提前达成共识，影响辩论系统通过分歧促进创新的目标，且此前对智能体之间的阿谀行为研究不足。

Method: 提出针对多智能体辩论系统的阿谀行为形式定义，设计评估阿谀行为水平及其对信息交流影响的指标，系统研究不同角色（辩手与裁判）阿谀程度对集中和分散辩论框架结果的影响。

Result: 发现阿谀行为是导致多智能体辩论失败的核心原因，表现为过早达成错误共识，准确率低于单智能体基线，且存在不同的辩手和裁判驱动的失败模式。

Conclusion: 针对发现的阿谀行为问题，提出了可操作的设计原则，旨在平衡智能体间的有效分歧与合作，提升多智能体辩论系统的表现。

Abstract: Large language models (LLMs) often display sycophancy, a tendency toward
excessive agreeability. This behavior poses significant challenges for
multi-agent debating systems (MADS) that rely on productive disagreement to
refine arguments and foster innovative thinking. LLMs' inherent sycophancy can
collapse debates into premature consensus, potentially undermining the benefits
of multi-agent debate. While prior studies focus on user--LLM sycophancy, the
impact of inter-agent sycophancy in debate remains poorly understood. To
address this gap, we introduce the first operational framework that (1)
proposes a formal definition of sycophancy specific to MADS settings, (2)
develops new metrics to evaluate the agent sycophancy level and its impact on
information exchange in MADS, and (3) systematically investigates how varying
levels of sycophancy across agent roles (debaters and judges) affects outcomes
in both decentralized and centralized debate frameworks. Our findings reveal
that sycophancy is a core failure mode that amplifies disagreement collapse
before reaching a correct conclusion in multi-agent debates, yields lower
accuracy than single-agent baselines, and arises from distinct debater-driven
and judge-driven failure modes. Building on these findings, we propose
actionable design principles for MADS, effectively balancing productive
disagreement with cooperation in agent interactions.

</details>


### [30] [Semantic Voting: A Self-Evaluation-Free Approach for Efficient LLM Self-Improvement on Unverifiable Open-ended Tasks](https://arxiv.org/abs/2509.23067)
*Chunyang Jiang,Yonggang Zhang,Yiyang Cai,Chi-Min Chan,Yulong Liu,Mingming Chen,Wei Xue,Yike Guo*

Main category: cs.CL

TL;DR: 该论文提出了一种针对不可验证任务的无自我评估自我提升方法，利用语义投票替代硬匹配的多数投票，以降低计算成本和避免偏差，显著提升效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有使用自我评估机制（如自我判断和熵最小化）的方法在处理不可验证任务时计算开销大且存在过度自信和偏差问题，且传统硬匹配多数投票方法难以应用于开放式任务。

Method: 作者提出语义投票机制，通过轻量级句子嵌入模型计算语义相似度，实现软匹配，替代传统硬匹配多数投票和成本较高的自我评估方法。

Result: 实验表明该方法在不同模型架构和任务上均显著提升了计算效率和整体性能，优于现有的自我评估方法。

Conclusion: 语义投票作为一种轻量且有效的自我提升机制，成功突破了不可验证任务自我评估的瓶颈，具备广泛应用潜力。

Abstract: The rising cost of acquiring supervised data has driven significant interest
in self-improvement for large language models (LLMs). Straightforward
unsupervised signals like majority voting have proven effective in generating
pseudo-labels for verifiable tasks, while their applicability to unverifiable
tasks (e.g., translation) is limited by the open-ended character of responses.
As a result, self-evaluation mechanisms (e.g., self-judging and entropy
minimization) are predominantly used to derive pseudo-labels. However,
self-evaluation relying on LLMs typically incurs high computational overhead
and introduces overconfidence issues due to intrinsic biases. To address these
challenges, we propose a novel self-evaluation-free approach for unverifiable
tasks, designed for lightweight yet effective self-improvement. Inspired by
majority voting commonly employed in verifiable tasks, we propose semantic
voting as a novel mechanism that relaxes the principle of hard matching (i.e.,
exact matching) toward soft matching (i.e., semantic similarity). Soft matching
is achieved by leveraging a lightweight sentence embedding model to quantify
semantic similarity, thereby mitigating excessive computational burden and
intrinsic bias-associated limitations of self-evaluation. Comprehensive
experiments demonstrate that our method achieves substantial gains in
computational efficiency and overall better performance than self-evaluation
methods across diverse model architectures and tasks.

</details>


### [31] [From Evidence to Trajectory: Abductive Reasoning Path Synthesis for Training Retrieval-Augmented Generation Agents](https://arxiv.org/abs/2509.23071)
*Muzhi Li,Jinhu Qi,Yihong Wu,Minghao Zhao,Liheng Ma,Yifan Li,Xinyu Wang,Yingxue Zhang,Ho-fung Leung,Irwin King*

Main category: cs.CL

TL;DR: 本文提出了EviPath，一种基于证据的推理路径合成方法，用于提升检索增强生成代理的推理和决策能力。


<details>
  <summary>Details</summary>
Motivation: 当前检索增强生成代理缺乏过程级监督，导致任务分解、检索调用和逐步决策等能力受限，强化学习存在稀疏奖励和大模型推理能力有限的问题，现有数据合成方法也未能有效模拟环境交互。

Method: EviPath包含三部分：1）溯因式子任务规划，将问题分解为子问题并基于依赖关系迭代规划最优解路径；2）忠实子问题回答，利用支持证据构造代理环境生成推理和答案；3）对话式微调，将代理-环境交互转换为对话格式用于监督微调。通过合成数据使大模型直接学习复杂推理和工具使用能力。

Result: 在多个开放领域问答基准测试中，采用EviPath合成数据训练的8B参数模型相比最先进基线，获得了14.7%的绝对准确率提升，表现显著且稳定。

Conclusion: EviPath有效提升了检索增强生成代理的推理和决策能力，为基于证据的过程级监督提供了创新方法，推动了复杂推理任务的性能提升。

Abstract: Retrieval-augmented generation agents development is hindered by the lack of
process-level supervision to effectively guide agentic capabilities like task
decomposition, retriever invocation, and stepwise decision-making. While
reinforcement learning offers a potential solution, it suffers from sparse
rewards and the limited reasoning capabilities of large language models (LLMs).
Meanwhile, existing data synthesis methods only produce chain-of-thought
rationales and fail to model environmental interactions. In this paper, we
propose EviPath, an evidence-anchored reasoning path synthesis paradigm for RAG
agent development. EviPath comprises: (i) Abductive Subtask Planning, which
decomposes the problem into sub-questions and iteratively plans an optimal
solution path based on the dependencies between them; (ii) Faithful
Sub-question Answering, which uses supporting evidence to construct a proxy
environment to generate reasoning thoughts and answers for each sub-question;
and (iii) Conversational Fine-Tuning, which formats the complete
agent-environment interaction trajectory into a dialogue format suitable for
Supervised Fine-Tuning. EviPath allows LLMs to learn complex reasoning and
tool-use capabilities directly from synthesized data. Extensive experiments on
widely-used question-answering benchmarks show that an 8B parameter model
trained with EviPath-synthesized data significantly and consistently
outperforms state-of-the-art baselines with a double-digit absolute EM gain of
14.7% in open-domain question answering.

</details>


### [32] [The Geometry of Creative Variability: How Credal Sets Expose Calibration Gaps in Language Models](https://arxiv.org/abs/2509.23088)
*Esteban Garces Arias,Julian Rodemann,Christian Heumann*

Main category: cs.CL

TL;DR: 本文提出了一种基于可信集合的几何框架来量化和分解神经文本生成中的不确定性，评估了语言模型在人类创意变化捕捉上的不足，揭示了解码策略对认知不确定性的显著影响。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在创意任务中难以准确反映多样化且有效的输出，人类创意表现与模型生成存在差距，亟需有效量化和理解模型不确定性。

Method: 提出利用可信集合——概率分布的凸包——构建几何框架，对500个创意写作提示及对应人类续写进行大规模分析，比较4种语言模型和5种解码策略，进行不确定性分解。

Result: 发现最优模型与人类创意变异的校准度仅为0.434，总不确定性的39.4%-72.0%来源于解码策略选择，模型规模与校准质量相关性弱，基础模型与指令调优模型校准质量无显著差异。

Conclusion: 该几何框架为提升生成模型对人类创意匹配提供了有益见解，并释放了完整的实验工具，推动生成系统向更好的人机创意对齐发展。

Abstract: Understanding uncertainty in large language models remains a fundamental
challenge, particularly in creative tasks where multiple valid outputs exist.
We present a geometric framework using credal sets - convex hulls of
probability distributions - to quantify and decompose uncertainty in neural
text generation, calibrated against human creative variation. Analyzing 500
creative writing prompts from the WritingPrompts dataset with 10 unique human
continuations each, we evaluate four language models across five decoding
strategies, generating 100,000 stories. Our credal set analysis reveals
substantial gaps in capturing human creative variation, with the best
model-human calibration reaching only 0.434 (Gemma-2B with temperature 0.7). We
decompose total uncertainty into epistemic and aleatoric components, finding
that the choice of decoding strategy contributes 39.4% to 72.0% of total
epistemic uncertainty. Model scale shows weak correlation with calibration
quality and no significant difference exists between base and instruction-tuned
models in calibration quality. Our geometric framework provides actionable
insights for improving generation systems for human-AI creative alignment. We
release our complete experimental framework.

</details>


### [33] [Exploring Large Language Models for Translating Romanian Computational Problems into English](https://arxiv.org/abs/2501.05601)
*Adrian Marius Dumitran,Adrian-Catalin Badea,Stefan-Gabriel Muscalu,Angela-Liliana Dumitran,Stefan-Cosmin Dascalescu,Radu-Sebastian Amarie*

Main category: cs.CL

TL;DR: 该论文探讨了大型语言模型(LLMs)在罗马尼亚语到英语的数学和计算机科学任务翻译中的表现，表明通过精心设计的提示，LLMs可以保持甚至提升翻译质量。


<details>
  <summary>Details</summary>
Motivation: 罗马尼亚语任务翻译成英语后，LLMs表现下降，因此亟需提升跨语言翻译的准确性，保证各种应用的有效性，如教育和竞赛自动翻译。

Method: 评估多种LLM（OpenRoLLM、Llama 3.1 8B、Llama 3.2 3B、GPT-4o）在不同方法下的翻译准确性及稳定性；通过扩充罗马尼亚OJI数据集及人类监督进行语义和句法分析。

Result: 发现经过精心提示和监督，LLMs在翻译少数语言时能够维持甚至提升表现；翻译质量经专家评估接近人类水平。

Conclusion: 在有人工监督的条件下，LLMs能成为多语言问题自动翻译和解决的有效工具，具备现实应用潜力。

Abstract: Recent studies have suggested that large language models (LLMs) underperform
on mathematical and computer science tasks when these problems are translated
from Romanian into English, compared to their original Romanian format.
Accurate translation is critical for applications ranging from automatic
translations in programming competitions to the creation of high-quality
educational materials, as well as minimizing errors or fraud in human
translations. This study shows that robust large language models (LLMs) can
maintain or even enhance their performance in translating less common languages
when given well-structured prompts. Our findings suggest that LLMs, with
appropriate supervision, can be reliably used for the automatic translation of
IOI (International Olympiad in Informatics)-style tasks. We evaluate several
translation methods across multiple LLMs, including OpenRoLLM, Llama 3.1 8B,
Llama 3.2 3B and GPT-4o, assessing their translation accuracy and performance
stability through repeated runs. Additionally, we augment the OJI (Romanian
County-Level Informatics Olympiad) Romanian dataset with accurate English
translations, enhancing its utility for future LLM training and evaluation.
Through detailed syntactic and semantic analyses, we confirm that with human
oversight, LLMs can serve as a viable solution for multilingual
problem-solving. We also compare the translation quality of LLMs against human
translators, as evaluated by a certified expert, underscoring the potential of
LLMs in realworld scenarios.

</details>


### [34] [d$^2$Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching](https://arxiv.org/abs/2509.23094)
*Yuchu Jiang,Yue Cai,Xiangzhong Luo,Jiale Fu,Jiarui Wang,Chonghan Liu,Xu Yang*

Main category: cs.CL

TL;DR: 本文针对扩散式大型语言模型推理效率较低的问题，提出了一种无需训练的双重自适应缓存框架d²Cache，通过细粒度选择策略动态更新关键值缓存，显著提升推理速度和生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散式大型语言模型依赖双向注意力机制，无法像自回归模型一样直接利用标准的关键值缓存，导致推理效率较低。

Method: 引入d²Cache框架，该方法采用两阶段细粒度选择策略，在每个解码步骤动态识别需要更新关键值状态的token，同时缓存其他token的KV状态以便复用。该框架无需额外训练，且支持准左到右生成，缓解生成末端过早的置信过高问题。

Result: 在两个代表性扩散式模型LLaDA和Dream上，d²Cache实现了显著的推理速度提升，同时在生成质量上也有稳定的改善。

Conclusion: d²Cache作为一种无需训练的近似缓存方案，有效提升了扩散式语言模型的推理效率和生成效果，具有广泛的应用前景。

Abstract: Diffusion-based large language models (dLLMs), despite their promising
performance, still suffer from inferior inference efficiency. This is because
dLLMs rely on bidirectional attention and cannot directly benefit from the
standard key-value (KV) cache as autoregressive models (ARMs) do. To tackle
this issue, we introduce \textit{Dual aDaptive Cache} (d$^2$Cache), which is a
training-free approximate KV cache framework for accelerating dLLM inference.
d$^2$Cache features a two-stage fine-grained selection strategy to identify
tokens and adaptively update their KV states at each decoding step, while
caching the KV states of the remaining tokens for reuse. Furthermore,
d$^2$Cache naturally offers a more reliable decoding alternative, which can
enable quasi left-to-right generation and mitigate premature overconfidence in
tokens at the end of the sequence. Extensive experimental results on two
representative dLLMs (\ie, LLaDA and Dream) demonstrate that d$^2$Cache not
only achieves substantial inference speedups, but also yields consistent
improvements in generation quality. The code is available at
https://github.com/Kamichanw/d2Cache.

</details>


### [35] [TF-Bench: Evaluating Program Semantics Reasoning with Type Inference in System F](https://arxiv.org/abs/2509.23686)
*Yifeng He,Luning Yang,Christopher Castro Gaw Gonzalo,Hao Chen*

Main category: cs.CL

TL;DR: 本文介绍了一个新的基于类型推断的代码推理基准TF-Bench及其纯语义版本TF-Bench_pure，用于评估大语言模型的程序语义推理能力，揭示了现有模型存在显著局限。


<details>
  <summary>Details</summary>
Motivation: 现有代码推理基准缺乏形式化的程序中心演绎框架，无法准确评估模型是否真正理解程序语义，存在利用自然语言与代码表面关联的可能性。

Method: 设计基于System F类型推断的TF-Bench，并通过验证过的变换去除自然语言干扰，构建TF-Bench_pure；提出两种新指标评估模型的鲁棒性和测试时的推理效果。

Result: 最新大语言模型如Claude-3.7-sonnet在纯语义的TF-Bench_pure上的最高准确率仅55.85%，显示出现有模型推理能力的明显不足。

Conclusion: 当前大语言模型在程序语义推理方面存在关键限制，TF-Bench为评估和提升这些能力提供了新的方向和工具。

Abstract: Large Language Models (LLMs) are increasingly integrated into the software
engineering ecosystem. Their test-time compute (TTC) reasoning capabilities
show significant potential for understanding program logic and semantics beyond
mere token recognition. However, current benchmarks for code reasoning lack a
formal, program-centric deductive framework to ensure sound evaluation, and are
incapable of assessing whether models genuinely reason about program semantics
or merely exploit superficial associations between natural language and code
tokens. To bridge this gap, we introduce TF-Bench, a benchmark designed to
evaluate LLM reasoning based on type inference in System F, a task we refer to
as program semantics reasoning. By employing verified transformations to remove
semantically irrelevant natural language, we construct TF-Bench_pure, a purely
semantics-driven variant of TF-Bench. Our analysis reveals substantial
limitations in state-of-the-art LLMs, with the best-performing LLM
(Claude-3.7-sonnet) achieving only 55.85% accuracy on TF-Bench_pure.
Additionally, we propose two novel metrics to assess robustness and the
effectiveness of test-time reasoning, underscoring critical limitations in
current LLM capabilities and highlighting essential directions for future
research.

</details>


### [36] [How to Make Large Language Models Generate 100% Valid Molecules?](https://arxiv.org/abs/2509.23099)
*Wen Tao,Jing Tang,Alvin Chan,Bryan Hooi,Baolong Bi,Nanyun Peng,Yuansheng Liu,Yiwei Wang*

Main category: cs.CL

TL;DR: 本文提出了SmiSelf框架，通过将无效的SMILES转换为SELFIES并利用其语法规则，实现了100%有效分子生成，提升了大语言模型在分子生成中的实用性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在少样本环境下用SMILES表示生成有效分子存在困难，且SELFIES表示在LLM中表现不佳，需要一种方法保证生成分子的有效性。

Method: 提出SmiSelf跨化学语言框架，基于语法规则将无效SMILES转换为VALID SELFIES，利用SELFIES的机制进行修正，确保分子有效性。

Result: 实验结果表明SmiSelf可确保100%分子有效性，同时保持分子的特性并且在其他指标上保持或提升表现。

Conclusion: SmiSelf有效提升了LLM生成分子的有效性，扩大了其在生物医学领域的应用范围，且兼容所有基于SMILES的分子生成模型。

Abstract: Molecule generation is key to drug discovery and materials science, enabling
the design of novel compounds with specific properties. Large language models
(LLMs) can learn to perform a wide range of tasks from just a few examples.
However, generating valid molecules using representations like SMILES is
challenging for LLMs in few-shot settings. In this work, we explore how LLMs
can generate 100% valid molecules. We evaluate whether LLMs can use SELFIES, a
representation where every string corresponds to a valid molecule, for valid
molecule generation but find that LLMs perform worse with SELFIES than with
SMILES. We then examine LLMs' ability to correct invalid SMILES and find their
capacity limited. Finally, we introduce SmiSelf, a cross-chemical language
framework for invalid SMILES correction. SmiSelf converts invalid SMILES to
SELFIES using grammatical rules, leveraging SELFIES' mechanisms to correct the
invalid SMILES. Experiments show that SmiSelf ensures 100% validity while
preserving molecular characteristics and maintaining or even enhancing
performance on other metrics. SmiSelf helps expand LLMs' practical applications
in biomedicine and is compatible with all SMILES-based generative models. Code
is available at https://github.com/wentao228/SmiSelf.

</details>


### [37] [Non-Collaborative User Simulators for Tool Agents](https://arxiv.org/abs/2509.23124)
*Jeonghoon Shim,Woojung Song,Cheyon Jin,Seungwon KooK,Yohan Jo*

Main category: cs.CL

TL;DR: 提出了一种非协作用户模拟方法，用于训练和测试工具代理，使其能应对现实中非协作用户的挑战行为。


<details>
  <summary>Details</summary>
Motivation: 现有用户模拟器大多表现为协作性，无法有效训练和测试工具代理面对现实中非合作用户时的表现。

Method: 设计了一种新颖的用户模拟架构，可模拟四类非协作行为：请求不可用服务、偏离话题、表现出不耐烦、提供不完整话语，同时确保完成任务所需信息的完整传递。

Result: 在MultiWOZ和τ-bench上的实验表明，当前先进的工具代理在遇到非协作用户时性能显著下降，出现幻觉加剧和对话中断等问题。

Conclusion: 该用户模拟框架便于扩展，有助于研究社区开发更鲁棒的工具代理，并在真实复杂条件下对其进行预警性诊断。

Abstract: Non-Collaborative User Simulators for Tool Agents Download PDF Jeonghoon
Shim, Woojung Song, Cheyon Jin, Seungwon KooK, Yohan Jo 19 Sept 2025 (modified:
25 Sept 2025)ICLR 2026 Conference SubmissionConference, AuthorsRevisionsCC BY
4.0 Keywords: Tool Agent, User Simulator, Non-collaborative User, Dialogue
Simulation TL;DR: A non-collaborative user simulation method for tool agent.
Abstract: Tool agents interact with users through multi-turn dialogues to
accomplish various tasks. Recent studies have adopted user simulation methods
to develop these agents in multi-turn settings. However, existing user
simulators tend to be agent-friendly, exhibiting only cooperative behaviors,
which fails to train and test agents against non-collaborative users in the
real world. To address this, we propose a novel user simulator architecture
that simulates four categories of non-collaborative behaviors: requesting
unavailable services, digressing into tangential conversations, expressing
impatience, and providing incomplete utterances. Our user simulator can
simulate challenging and natural non-collaborative behaviors while reliably
delivering all intents and information necessary to accomplish the task. Our
experiments on MultiWOZ and $\tau$-bench reveal significant performance
degradation in state-of-the-art tool agents when encountering non-collaborative
users. We provide detailed analyses of agents' weaknesses under each
non-collaborative condition, such as escalated hallucinations and dialogue
breakdowns. Ultimately, we contribute an easily extensible user simulation
framework to help the research community develop tool agents and preemptively
diagnose them under challenging real-world conditions within their own
services.

</details>


### [38] [Tagging the Thought: Unlocking Personalization Reasoning via Reinforcement Learning](https://arxiv.org/abs/2509.23140)
*Song Jin,Juntian Zhang,Yong Liu,Xun Zhang,Yufei Zhang,Fei Jiang,Guojun Yin,Wei Lin,Rui Yan*

Main category: cs.CL

TL;DR: 该论文提出了TagPR训练框架，通过标签化推理链数据和多阶段强化学习，大幅提升大型语言模型的个性化推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在通用推理能力虽强，但缺乏精准的个性化推理能力，难以基于用户历史和偏好生成贴合用户的回答。

Method: 论文设计了一套自动生成和语义标注推理链的数据驱动流程，构建结构化推理数据集。采用先监督微调，再结合层级强化学习的训练策略，强化模型对基于标签约束和用户嵌入的个性化奖励信号的响应能力。

Result: 在公开LaMP基准和自建数据集上，TagPR框架相比基础模型平均提升32.65%，实现了个性化推理任务的先进水平。

Conclusion: 结构化、可解释的推理链生成与训练是解锁大型语言模型个性化推理能力的有效途径。

Abstract: Recent advancements have endowed Large Language Models (LLMs) with impressive
general reasoning capabilities, yet they often struggle with personalization
reasoning - the crucial ability to analyze user history, infer unique
preferences, and generate tailored responses. To address this limitation, we
introduce TagPR, a novel training framework that significantly enhances an
LLM's intrinsic capacity for personalization reasoning through a tagging the
thought approach. Our method first develops a data-driven pipeline to
automatically generate and semantically label reasoning chains, creating a
structured dataset that fosters interpretable reasoning. We then propose a
synergistic training strategy that begins with Supervised Fine-Tuning (SFT) on
this tagged data to establish foundational reasoning patterns, followed by a
multi-stage reinforcement learning (RL) process. This RL phase is guided by a
unique composite reward signal, which integrates tag-based constraints and a
novel Personalization Reward Model with User Embeddings (PRMU) to achieve
fine-grained alignment with user-specific logic. Extensive experiments on the
public LaMP benchmark and a self-constructed dataset demonstrate that our
approach achieves state-of-the-art results, delivering an average improvement
of 32.65% over the base model across all tasks. Our work validates that
structured, interpretable reasoning is a highly effective pathway to unlocking
genuine personalization capabilities in LLMs.

</details>


### [39] [Tree Reward-Aligned Search for TReASURe in Masked Diffusion Language Models](https://arxiv.org/abs/2509.23146)
*Zichao Yu,Ming Li,Wenyi Zhang,Weiguo Gao*

Main category: cs.CL

TL;DR: 本文提出了TReASURe，一种针对Masked Diffusion Language Models的树搜索测试对齐方法，解决了并行解掩导致的分支相关性和高方差奖励评估的问题。


<details>
  <summary>Details</summary>
Motivation: 现有树搜索方法在Masked Diffusion Language Models中应用时存在分支高度相关限制探索和奖励估计高方差导致剪枝不稳定的问题。

Method: 提出UnmaskBranch分支策略，通过首次解掩实现分支多样性，并使用ResubstituteScore剪枝规则，利用确定性重替代生成低方差代理完成分数。

Result: 理论上证明了分支效率和评分规则的优越性，并在实验中在困惑度、语言接受度及情感和毒性控制上超越前人方法，表现尤其在低计算预算下显著提升。

Conclusion: TReASURe有效解决了Masked Diffusion Language Models中树搜索的关键挑战，实现了更稳定和高效的测试时任务对齐。

Abstract: Tree search has recently emerged as a powerful framework for aligning
generative models with task-specific rewards at test time. Applying tree search
to Masked Diffusion Language Models, however, introduces two key challenges:
(i) parallel unmasking yields highly correlated branches, limiting exploration,
and (ii) reward evaluation via sampled completions produces high-variance
estimates, making pruning unstable. We propose TReASURe, a tree-search
test-time alignment method that addresses these issues. It introduces (i)
UnmaskBranch, a branching strategy based on first-hitting unmasking that
diversifies both token content and reveal order with a single model call per
parent node, and (ii) ResubstituteScore, a pruning rule that uses deterministic
resubstitution to score partially masked sequences with low-variance proxy
completions. Theoretically, we quantify branching efficiency gains in NFEs
(number of function evaluations), show that the scoring rule approximates the
true reward with error bounded by predictive uncertainty, and prove
improvements with larger tree widths. Empirically, TReASURe achieves
state-of-the-art results on perplexity, linguistic acceptability, and control
of sentiment and toxicity, outperforming prior methods under matched compute
budgets, with especially strong gains in low-NFE regimes.

</details>


### [40] [Test-Time Policy Adaptation for Enhanced Multi-Turn Interactions with LLMs](https://arxiv.org/abs/2509.23166)
*Chenxing Wei,Hong Wang,Ying He,Fei Yu,Yao Shu*

Main category: cs.CL

TL;DR: 本文提出了一种针对大语言模型多轮交互的自适应方法ROSA，通过单步参数更新实现模型实时自我纠错，显著提升任务效果和效率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型训练基于静态单轮数据，导致多轮长时间交互中性能下降，难以适应实时用户反馈。

Method: 提出测试时策略自适应新范式（T2PAM），利用用户反馈作为奖励信号估计最优策略，并用轻量级算法ROSA通过单步参数更新引导模型向该策略靠拢。

Result: 理论证明ROSA策略会随着交互次数增加收敛至用户偏好，实验证明ROSA在多轮交互的任务效果和计算效率上显著提升。

Conclusion: ROSA有效解决了大语言模型多轮交互中模型适应性差的问题，实现了高效的交互自我纠错，具有较大应用价值。

Abstract: Large Language Models (LLMs) employ multi-turn interaction as a fundamental
paradigm for completing complex tasks. However, their performance often
degrades in extended interactions, as they are typically trained on static,
single-turn data, which hinders their ability to adapt to real-time user
feedback. To address this limitation, we first propose a new paradigm:
Test-Time Policy Adaptation for Multi-Turn Interactions (T2PAM), which utilizes
user feedback from the ongoing interaction as a reward signal to estimate a
latent optimal policy aligned with user preferences, then updates a small
subset of parameters to steer the model toward this policy, ultimately enabling
efficient in-conversation self-correction. We then introduce Optimum-Referenced
One-Step Adaptation (ROSA), a lightweight algorithm that operationalizes T2PAM.
ROSA guides the model parameters toward a theoretical optimal policy in a
single, efficient update step, avoiding costly iterative gradient-based
optimization and minimizing computational overhead. We provide a rigorous
theoretical analysis guaranteeing that the policy of ROSA converges to the
preference of user as the number of interactions increases. Extensive
experiments on challenging benchmark demonstrate that ROSA achieves significant
improvements in both task effectiveness and efficiency.

</details>


### [41] [Pretraining LLM with Latent Thoughts in Continuous Space](https://arxiv.org/abs/2509.23184)
*Boyi Zeng,He Li,Shixiang Song,Yixuan Wang,Ziwei He,Xinbing Wang,Zhouhan Lin*

Main category: cs.CL

TL;DR: 本文提出了一种新的预训练方法，通过生成潜在思维向量作为中间步骤，提升语言模型对每个单词的预测能力。


<details>
  <summary>Details</summary>
Motivation: 受Chain-of-Thought启发，作者探索在预训练阶段增加计算步骤，是否可以提升语言模型生成质量。

Method: 语言模型先生成潜在思维（当前位置的最后隐状态），再用该潜在思维预测下一个单词，实现额外连续空间内的计算步骤。

Result: 在相同推理成本下，该方法的1.4B模型表现超越了2.8B参数的标准模型，且生成多步潜在思维进一步提升性能。

Conclusion: 通过预训练生成潜在思维的额外计算步骤，可以显著提高语言模型性能，且效益超过简单增加参数量。

Abstract: The remarkable success of Chain-of-Thought (CoT), which enhances performance
by scaling generation steps at test-time, inspires us to ask: can we leverage a
similar scaling of computational steps during pretraining to improve the
generation of each individual token? To address this, we propose a novel
pre-training methodology: Pretraining Language Models with Latent Thoughts. Our
approach pretrains a language model (LM) to first generate an intermediate
latent thought-the last hidden state of the current position-which is then used
as input to predict the actual subsequent token. This additional computational
step enables the LM to refine its prediction within unconstrained continuous
space. Our experiments demonstrate that, at an identical inference cost, a LM
that generates one additional latent thought per token outperforms a standard
model with double the parameters. For instance, ours-1.4B (Pythia Arch),
pretrained on 300B tokens from the Pile, significantly surpasses the vanilla
Pythia-2.8B trained on the same data on both language modeling and a range of
general downstream tasks. Furthermore, increasing the number of latent thoughts
generated before each actual token-forming a chain analogous to
CoT-consistently improves the model's performance.

</details>


### [42] [Diagnose, Localize, Align: A Full-Stack Framework for Reliable LLM Multi-Agent Systems under Instruction Conflicts](https://arxiv.org/abs/2509.23188)
*Guancheng Wan,Leixin Sun,Longxu Dou,Zitong Shi,Fang Wu,Eric Hanchen Jiang,Wenke Huang,Guibin Zhang,Hejia Geng,Xiangru Tang,Zhenfei Yin,Yizhou Sun,Wei Wang*

Main category: cs.CL

TL;DR: 本文提出了一种三阶段框架解决大型语言模型驱动的多智能体系统在指令冲突下的等级顺从性问题，提高了系统在医疗问答等任务中的可靠性。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在执行复杂任务时存在指令冲突，导致系统级规则被错误优先，且现有宏观指标无法准确反映和修正这一问题。

Method: 提出三阶段框架：1）设计上下文感知的角色遵守评分指标CRAS，分解角色遵守为四个维度；2）通过注意力漂移分析定位冲突发生的中层注意力头；3）开发局部微调方法SAIL，仅在关键层应用LoRA并优化带权重的DPO偏好目标。

Result: 在多种基准测试和多智能体框架上，方法显著提升了指令层级顺从率，比如在MedQA任务中，使用AutoGen框架提升了5.60%。

Conclusion: 通过局部、有针对性的调整注意力层，可以有效解决多智能体系统中的指令冲突问题，无需整体微调，提升系统的可靠性和表现。

Abstract: Large Language Model (LLM)-powered multi-agent systems (MAS) have rapidly
advanced collaborative reasoning, tool use, and role-specialized coordination
in complex tasks. However, reliability-critical deployment remains hindered by
a systemic failure mode: hierarchical compliance under instruction conflicts
(system-user, peer-peer), where agents misprioritize system-level rules in the
presence of competing demands. Moreover, widely used macro-level metrics (e.g.,
pass@k) obscure these micro-level violations and offer little actionable
guidance for remedy. In this work, we present a full-stack, three-stage
framework: (1) Diagnose - Contextualized Role Adherence Score (CRAS), a
query-wise, context-aware scoring metric that decomposes role adherence into
four measurable dimensions; (2) Localize - attention drift analysis revealing
that instruction conflicts are resolved by attention heads that are largely
concentrated in middle layers; (3) Align - Surgical Alignment of Instruction
Layers (SAIL), which installs LoRA only on the localized focal layers and
optimizes a token-weighted DPO-style preference objective that credits tokens
by their focal attentional contribution. Across standard benchmarks and MAS
frameworks, our surgical approach improves instruction hierarchy compliance
(e.g., +5.60% with AutoGen on MedQA) without full-model finetuning.

</details>


### [43] [Estimating the strength and timing of syntactic structure building in naturalistic reading](https://arxiv.org/abs/2509.23195)
*Nan Wang,Jiaxuan Li*

Main category: cs.CL

TL;DR: 本研究通过EEG和眼动追踪数据，探讨句法结构建构与词类检测的时间顺序，发现短语结构构建可能先于词类检测。


<details>
  <summary>Details</summary>
Motivation: 传统研究混淆了词类检测和短语结构建构两个过程，且假设短语结构建构发生在词类检测之后。

Method: 利用ZuCo语料库的EEG联合眼动追踪数据，分析凝视转移和贝叶斯网络模型，检验两过程的时间关系。

Result: 凝视转移显示读者更倾向于在句法核心间移动，结构深度是阅读偏离线性顺序的主导因素，且词汇熟悉度和意外度影响较小；脑电数据显示句法意外度在词前及早期整合阶段影响神经活动。

Conclusion: 研究扩展了句法时间模型，表明短语结构建构可先于词类检测，且主导词汇影响，支持预测性的“树状支架”理解模型。

Abstract: A central question in psycholinguistics is the timing of syntax in sentence
processing. Much of the existing evidence comes from violation paradigms, which
conflate two separable processes - syntactic category detection and phrase
structure construction - and implicitly assume that phrase structure follows
category detection. In this study, we use co-registered EEG and eye-tracking
data from the ZuCo corpus to disentangle these processes and test their
temporal order under naturalistic reading conditions. Analyses of gaze
transitions showed that readers preferentially moved between syntactic heads,
suggesting that phrase structures, rather than serial word order, organize
scanpaths. Bayesian network modeling further revealed that structural depth was
the strongest driver of deviations from linear reading, outweighing lexical
familiarity and surprisal. Finally, fixation-related potentials demonstrated
that syntactic surprisal influences neural activity before word onset (-184 to
-10 ms) and during early integration (48 to 300 ms). These findings extend
current models of syntactic timing by showing that phrase structure
construction can precede category detection and dominate lexical influences,
supporting a predictive "tree-scaffolding" account of comprehension.

</details>


### [44] [From Harm to Help: Turning Reasoning In-Context Demos into Assets for Reasoning LMs](https://arxiv.org/abs/2509.23196)
*Haonan Wang,Weida Liang,Zihang Fu,Nie Zheng,Yifan Zhang,Yao Tong,Tongyao Zhu,Hao Jiang,Chuang Li,Jiaying Wu,Kenji Kawaguchi*

Main category: cs.CL

TL;DR: 此论文研究了推理大型语言模型（RLMs）中，使用演示示例时准确率下降的问题，提出Insight-to-Solve（I2S）方法，通过将演示转化为可复用的洞见提升推理性能，实现了显著的准确率提高。


<details>
  <summary>Details</summary>
Motivation: 当前的推理大型语言模型在使用少量示例的链式思考（CoT）时表现反而不如直接回答，导致性能下降的原因不明。论文旨在揭示准确率下降的机制并改进演示示例的使用方式。

Method: 分析发现性能下降源于语义误导和策略转移失败两方面。提出Insight-to-Solve（I2S）方法，将示例中的推理轨迹转化为显式、可复用的洞见，针对目标问题生成专门的推理轨迹，并通过自我优化提高推理的连贯性和正确性（I2S+）。

Result: 在多个基准测试中，I2S和I2S+方法均超越了直接回答和测试时扩展基线模型。对GPT-4.1和o1-mini模型均有显著性能提升，例如GPT-4.1在AIME’25提升14.0%。

Conclusion: 通过将演示示例转化为明确洞见并结合自我优化，能够有效避免语义误导和策略转移失败，提高推理语言模型的性能，证明了Insight-to-Solve方法在实际推理任务中的有效性。

Abstract: Recent reasoning LLMs (RLMs), especially those trained with verifier-based
reinforcement learning, often perform worse with few-shot CoT than with direct
answering. We revisit this paradox using high-quality reasoning traces from
DeepSeek-R1 as demonstrations and find that adding more exemplars consistently
degrades accuracy, even when demonstrations are optimal. A detailed analysis
reveals two mechanisms behind this decline: (i) semantic misguidance, where
high textual similarity leads the model to treat the target as the same as the
exemplar and to copy intermediate steps verbatim; and (ii) strategy transfer
failure, where the model struggles to extract useful reasoning strategies and
apply them to target questions. Guided by these, we introduce Insight-to-Solve
(I2S), a sequential test-time procedure that turns demonstrations into
explicit, reusable insights and derives a target-specific reasoning trace;
optionally, the reasoning is self-refined for coherence and correctness (I2S+).
Extensive experiments on diverse benchmarks show that I2S and I2S+ consistently
outperform both direct answering and test-time scaling baselines across open-
and closed-source models. Even for GPT models, our method helps: on AIME'25,
GPT-4.1 rises by +14.0%, and o1-mini improves by +2.7% on AIME and +1.7% on
GPQA, indicating that in-context demonstrations can be harnessed effectively
via insight-refine-solve framework.

</details>


### [45] [Global Beats, Local Tongue: Studying Code Switching in K-pop Hits on Billboard Charts](https://arxiv.org/abs/2509.23197)
*Aditya Narayan Sankaran,Reza Farahbakhsh,Noel Crespi*

Main category: cs.CL

TL;DR: 本文通过分析2017至2025年进入Billboard Hot 100和Global 200榜单的K-pop歌曲，探讨了韩英之间的语言切换及英语歌词使用，揭示了全球成功K-pop作品的语言策略。


<details>
  <summary>Details</summary>
Motivation: K-pop全球流行背景下，探讨语言切换如何影响歌曲在全球排行榜的表现及反映艺人身份。

Method: 收集14个团体和8名个人艺人共计多首榜单歌曲，统计分析韩英歌词比例、语言切换频率及其它风格特征，进行性别分类预测并比较Hot 100与Global 200榜单差异。

Result: 英语在全球成功K-pop歌曲中占主导地位，男女艺人高频率语言切换，无显著性别差异，但女solo艺人更偏爱英语；用歌词预测性别最高宏平均F1达0.76；Hot 100歌曲英语使用更高。

Conclusion: K-pop歌词的语言选择受全球市场压力影响，显示出与艺人身份及榜单背景相关的风格特征，英语使用对美国市场成功尤为关键。

Abstract: Code switching, particularly between Korean and English, has become a
defining feature of modern K-pop, reflecting both aesthetic choices and global
market strategies. This paper is a primary investigation into the linguistic
strategies employed in K-pop songs that achieve global chart success, with a
focus on the role of code-switching and English lyric usage. A dataset of K-pop
songs that appeared on the Billboard Hot 100 and Global 200 charts from 2017 to
2025, spanning 14 groups and 8 solo artists, was compiled. Using this dataset,
the proportion of English and Korean lyrics, the frequency of code-switching,
and other stylistic features were analysed. It was found that English dominates
the linguistic landscape of globally charting K-pop songs, with both male and
female performers exhibiting high degrees of code-switching and English usage.
Statistical tests indicated no significant gender-based differences, although
female solo artists tend to favour English more consistently. A classification
task was also performed to predict performer gender from lyrics, achieving
macro F1 scores up to 0.76 using multilingual embeddings and handcrafted
features. Finally, differences between songs charting on the Hot 100 versus the
Global 200 were examined, suggesting that, while there is no significant gender
difference in English, higher English usage may be more critical for success in
the US-focused Hot 100. The findings highlight how linguistic choices in K-pop
lyrics are shaped by global market pressures and reveal stylistic patterns that
reflect performer identity and chart context.

</details>


### [46] [Steering Prepositional Phrases in Language Models: A Case of with-headed Adjectival and Adverbial Complements in Gemma-2](https://arxiv.org/abs/2509.23204)
*Stefan Arnold,René Gröbner*

Main category: cs.CL

TL;DR: 本文研究了语言模型在生成介词短语时如何决定其补语是作为工具状语还是定语修饰，发现模型倾向于工具状语，且通过调整注意力头可以控制其偏好。


<details>
  <summary>Details</summary>
Motivation: 语言模型在生成介词短语时，需要区分补语的功能角色，但内部决策机制尚不清楚，因此希望深入了解并控制这一过程。

Method: 针对Gemma-2模型，设计包含具有相同上下文但不同补语功能的with介词短语的提示集，分析模型偏好，并通过投影注意力头激活到词汇空间定位关注工具状语偏好的头部，再通过缩放特定注意力头的值向量调整补语功能分布。

Result: 模型表现出对工具状语的强烈偏好（比例约3:4），通过调节单个注意力头的值向量，可以将工具状语比例降低至33%，同时提升定语比例至36%。

Conclusion: 语言模型的介词补语功能角色生成受到特定注意力头的影响，调整该头的参数可以有效控制生成的功能倾向，为理解和控制语言模型的内部机制提供了新方法。

Abstract: Language Models, when generating prepositional phrases, must often decide for
whether their complements functions as an instrumental adjunct (describing the
verb adverbially) or an attributive modifier (enriching the noun adjectivally),
yet the internal mechanisms that resolve this split decision remain poorly
understood. In this study, we conduct a targeted investigation into Gemma-2 to
uncover and control the generation of prepositional complements. We assemble a
prompt suite containing with-headed prepositional phrases whose contexts
equally accommodate either an instrumental or attributive continuation,
revealing a strong preference for an instrumental reading at a ratio of 3:4. To
pinpoint individual attention heads that favor instrumental over attributive
complements, we project activations into the vocabulary space. By scaling the
value vector of a single attention head, we can shift the distribution of
functional roles of complements, attenuating instruments to 33% while elevating
attributes to 36%.

</details>


### [47] [PARL-MT: Learning to Call Functions in Multi-Turn Conversation with Progress Awareness](https://arxiv.org/abs/2509.23206)
*Huacan Chai,Zijie Cao,Maolin Ran,Yingxuan Yang,Jianghao Lin,pengxin,Hairui Wang,Renjie Ding,Ziyu Wan,Muning Wen,Weiwen Liu,Weinan Zhang,Fei Huang,Ying Wen*

Main category: cs.CL

TL;DR: 该论文提出了PARL-MT框架，通过引入进度意识，提升大语言模型多轮功能调用的表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法很好处理多轮对话中的任务规划和进度跟踪，影响多轮功能调用的连贯性和效率。

Method: 提出了PARL-MT框架，包括进度意识生成(PAG)流水线自动构建对话摘要与未来规划的数据集，及进度意识引导的强化学习(PAG-RL)，用于减少冗余并增强局部动作与全局任务的对齐。

Result: 在两个公开基准测试中，PARL-MT显著优于现有方法，证明进度意识有效提升了多轮功能调用的鲁棒性和效率。

Conclusion: 引入进度意识对多轮功能调用的任务规划和执行具有关键作用，PARL-MT提供了一种有效的训练框架。

Abstract: Large language models (LLMs) have achieved impressive success in single-turn
function calling, yet real-world applications such as travel planning or
multi-stage data analysis typically unfold across multi-turn conversations. In
these settings, LLMs must not only issue accurate function calls at each step
but also maintain progress awareness, the ability to summarize past
interactions and plan future actions to ensure coherent, long-horizon task
execution. Existing approaches, however, either reduce multi-turn training to
isolated single-turn samples, which neglects task-level planning, or employ
end-to-end reinforcement learning (RL) that struggles with redundancy and lacks
explicit integration of progress awareness. To overcome these limitations, we
introduce PARL-MT, a framework that explicitly incorporates progress awareness
into LLM training for multi-turn function calling. PARL-MT combines (i) a
Progress Awareness Generation (PAG) pipeline, which automatically constructs
datasets coupling conversation summaries with future task planning, and (ii) a
Progress Awareness-Guided Reinforcement Learning (PAG-RL) algorithm, which
integrates progress awareness into RL training to reduce contextual redundancy
and improve alignment between local actions and global task completion.
Empirical results on two public benchmarks demonstrate that PARL-MT
significantly outperforms existing methods, highlighting the effectiveness of
progress awareness in enabling robust and efficient multi-turn function
calling.

</details>


### [48] [A Structured Framework for Evaluating and Enhancing Interpretive Capabilities of Multimodal LLMs in Culturally Situated Tasks](https://arxiv.org/abs/2509.23208)
*Haorui Yu,Ramon Ruiz-Dolz,Qiufeng Yi*

Main category: cs.CL

TL;DR: 本研究评估了主流视觉语言模型在中国传统绘画批评中的表现，开发了量化框架并基于人格视角进行测试。


<details>
  <summary>Details</summary>
Motivation: 测试当前视觉语言模型在生成中国传统绘画批评中的能力和特点，探索其在复杂语义理解和内容生成中的潜力和局限。

Method: 构建了一个多维量化评估框架，提取专家批评的立场、焦点和质量特征，定义多种批评者人格，并通过人格引导提示，对不同视觉语言模型如Llama、Qwen、Gemini进行批评生成能力评测。

Result: 发现了视觉语言模型在艺术批评方面的表现水平、优势和改进空间，揭示了其在多视角批评生成任务中的潜力和不足。

Conclusion: 目前视觉语言模型在传统艺术批评中具备一定能力，但仍需提升对复杂语义的理解和内容的多样化生成能力。

Abstract: This study aims to test and evaluate the capabilities and characteristics of
current mainstream Visual Language Models (VLMs) in generating critiques for
traditional Chinese painting. To achieve this, we first developed a
quantitative framework for Chinese painting critique. This framework was
constructed by extracting multi-dimensional evaluative features covering
evaluative stance, feature focus, and commentary quality from human expert
critiques using a zero-shot classification model. Based on these features,
several representative critic personas were defined and quantified. This
framework was then employed to evaluate selected VLMs such as Llama, Qwen, or
Gemini. The experimental design involved persona-guided prompting to assess the
VLM's ability to generate critiques from diverse perspectives. Our findings
reveal the current performance levels, strengths, and areas for improvement of
VLMs in the domain of art critique, offering insights into their potential and
limitations in complex semantic understanding and content generation tasks. The
code used for our experiments can be publicly accessed at:
https://github.com/yha9806/VULCA-EMNLP2025.

</details>


### [49] [Detecting Corpus-Level Knowledge Inconsistencies in Wikipedia with Large Language Models](https://arxiv.org/abs/2509.23233)
*Sina J. Semnani,Jirayu Burapacheep,Arpandeep Khatua,Thanawan Atchariyachanvanit,Zheng Wang,Monica S. Lam*

Main category: cs.CL

TL;DR: 本文提出了一种基于大语言模型的系统CLAIRE，用于检测并提供维基百科语料库中的事实不一致性，帮助编辑提高条目准确性。


<details>
  <summary>Details</summary>
Motivation: 维基百科作为全球最大的开放知识库，其准确性对训练大型语言模型和检索增强生成系统至关重要，然而其存在事实不一致的问题亟需解决。

Method: 设计CLAIRE系统，结合大语言模型推理与信息检索，自动识别潜在不一致陈述并提供上下文证据支持，同时通过人类注释构建WIKICOLLIDE不一致性基准数据集。

Result: 在用户研究中，CLAIRE提升了编辑者的信心和发现不一致事实的效率，实证发现英语维基百科中至少3.3%的事实存在冲突，且现有自动检测系统表现有限，最高AUROC仅75.1%。

Conclusion: 事实矛盾是维基百科中可量化且普遍存在的问题，基于大语言模型的自动化工具如CLAIRE能有效辅助编辑者大规模改进知识一致性。

Abstract: Wikipedia is the largest open knowledge corpus, widely used worldwide and
serving as a key resource for training large language models (LLMs) and
retrieval-augmented generation (RAG) systems. Ensuring its accuracy is
therefore critical. But how accurate is Wikipedia, and how can we improve it?
  We focus on inconsistencies, a specific type of factual inaccuracy, and
introduce the task of corpus-level inconsistency detection. We present CLAIRE,
an agentic system that combines LLM reasoning with retrieval to surface
potentially inconsistent claims along with contextual evidence for human
review. In a user study with experienced Wikipedia editors, 87.5% reported
higher confidence when using CLAIRE, and participants identified 64.7% more
inconsistencies in the same amount of time.
  Combining CLAIRE with human annotation, we contribute WIKICOLLIDE, the first
benchmark of real Wikipedia inconsistencies. Using random sampling with
CLAIRE-assisted analysis, we find that at least 3.3% of English Wikipedia facts
contradict another fact, with inconsistencies propagating into 7.3% of FEVEROUS
and 4.0% of AmbigQA examples. Benchmarking strong baselines on this dataset
reveals substantial headroom: the best fully automated system achieves an AUROC
of only 75.1%.
  Our results show that contradictions are a measurable component of Wikipedia
and that LLM-based systems like CLAIRE can provide a practical tool to help
editors improve knowledge consistency at scale.

</details>


### [50] [Fin-ExBERT: User Intent based Text Extraction in Financial Context using Graph-Augmented BERT and trainable Plugin](https://arxiv.org/abs/2509.23259)
*Soumick Sarker,Abhijit Kumar Rai*

Main category: cs.CL

TL;DR: 本文提出了Fin-ExBERT，一种基于适应性BERT和LoRA轻量级框架，用于从金融服务对话中高效提取用户意图相关句子。


<details>
  <summary>Details</summary>
Motivation: 金融对话文本因结构非正式、专业术语多且意图密度变化大，导致句子级信息提取面临挑战。

Method: 基于领域适应的BERT，结合LoRA适配器进行轻量微调，采用两阶段训练策略和基于概率曲率动态阈值方法，提升提取效果和泛化能力。

Result: 在真实金融对话数据上，实现了高精度和F1分数，输出结果解释性强，适合后续审计和问答流程。

Conclusion: Fin-ExBERT框架具备批量评估、可视化及校准导出功能，是金融对话挖掘的可部署解决方案。

Abstract: Financial dialogue transcripts pose a unique challenge for sentence-level
information extraction due to their informal structure, domain-specific
vocabulary, and variable intent density. We introduce Fin-ExBERT, a lightweight
and modular framework for extracting user intent-relevant sentences from
annotated financial service calls. Our approach builds on a domain-adapted BERT
(Bidirectional Encoder Representations from Transformers) backbone enhanced
with LoRA (Low-Rank Adaptation) adapters, enabling efficient fine-tuning using
limited labeled data. We propose a two-stage training strategy with progressive
unfreezing: initially training a classifier head while freezing the backbone,
followed by gradual fine-tuning of the entire model with differential learning
rates. To ensure robust extraction under uncertainty, we adopt a dynamic
thresholding strategy based on probability curvature (elbow detection),
avoiding fixed cutoff heuristics. Empirical results show strong precision and
F1 performance on real-world transcripts, with interpretable output suitable
for downstream auditing and question-answering workflows. The full framework
supports batched evaluation, visualization, and calibrated export, offering a
deployable solution for financial dialogue mining.

</details>


### [51] [A2D: Any-Order, Any-Step Safety Alignment for Diffusion Language Models](https://arxiv.org/abs/2509.23286)
*Wonje Jeung,Sangyeon Yoon,Yoonjun Cho,Dongjae Jeon,Sangwoo Shin,Hyesoo Hong,Albert No*

Main category: cs.CL

TL;DR: 本文提出了一种针对扩散大语言模型(dLLMs)的安全防护方法A2D，能在生成任一位置有害内容时即时拒绝，提升安全性和响应速度。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型虽然支持任意顺序生成，但这也增加了攻击风险，如有害内容可能在任意位置出现，以及基于模板的预填充攻击可绕过常规拒绝机制。

Method: A2D是一种基于标记级对齐的安全方法，通过随机掩码，训练dLLMs在检测到有害内容时立即生成[EOS]信号，实现在任何解码顺序和任意步骤预填充攻击下的鲁棒防护，并支持实时监控和自动终止不安全生成。

Result: 在安全基准测试中，A2D显著降低了DIJA攻击成功率，从80%以上降至接近0%（如LLaDA-8B-Instruct降至1.3%，Dream-v0-Instruct-7B降至0.0%），且基于阈值的[EOS]概率实现了最高19.3倍的安全终止加速。

Conclusion: A2D通过对扩散模型的标记级对齐，为任意顺序生成提供了有效的安全防护机制，能够显著抵御预填充攻击，快速终止有害生成，提升了dLLMs的安全性和实用性。

Abstract: Diffusion large language models (dLLMs) enable any-order generation, but this
flexibility enlarges the attack surface: harmful spans may appear at arbitrary
positions, and template-based prefilling attacks such as DIJA bypass
response-level refusals. We introduce A2D (Any-Order, Any-Step Defense), a
token-level alignment method that aligns dLLMs to emit an [EOS] refusal signal
whenever harmful content arises. By aligning safety directly at the token-level
under randomized masking, A2D achieves robustness to both any-decoding-order
and any-step prefilling attacks under various conditions. It also enables
real-time monitoring: dLLMs may begin a response but automatically terminate if
unsafe continuation emerges. On safety benchmarks, A2D consistently prevents
the generation of harmful outputs, slashing DIJA success rates from over 80% to
near-zero (1.3% on LLaDA-8B-Instruct, 0.0% on Dream-v0-Instruct-7B), and
thresholded [EOS] probabilities allow early rejection, yielding up to 19.3x
faster safe termination.

</details>


### [52] [Scaling Policy Compliance Assessment in Language Models with Policy Reasoning Traces](https://arxiv.org/abs/2509.23291)
*Joseph Marvin Imperial,Harish Tayyar Madabushi*

Main category: cs.CL

TL;DR: 本文提出了Policy Reasoning Traces (PRT)，通过生成专门的推理链提升大模型对政策合规性的评估能力，在HIPAA和GDPR政策合规性评估中表现优异。


<details>
  <summary>Details</summary>
Motivation: 政策合规性评估需要逐步辨别是否违反政策条款，但获取专家级的推理过程标注成本高。作者旨在通过自动生成推理链弥补这一不足。

Method: 引入Policy Reasoning Traces (PRT)，即专门生成的推理链，作为推理桥梁提升大模型在推理时和训练时的政策合规性判断能力。

Result: 实验证明，PRT方法显著提升了开源和商业大模型在HIPAA和GDPR政策上的合规评估性能，达到了新的最高水平。同时，PRT帮助模型更准确地引用政策条款并影响合规判定。

Conclusion: PRT不仅提高了合规性评估的准确率，还增强了模型引用政策条款的能力和推理链的利用率，是提升政策合规模型性能的有效方法。

Abstract: Policy compliance assessment is a fundamental task of evaluating whether an
input case strictly complies with a set of human-defined rules, more generally
known as policies. In practice, human experts follow a systematic, step-by-step
process to identify violations with respect to specific stipulations outlined
in the policy. However, such documentation of gold-standard, expert-level
reasoning processes is costly to acquire. In this paper, we introduce Policy
Reasoning Traces (PRT), a form of specialized generated reasoning chains that
serve as a reasoning bridge to improve an LLM's policy compliance assessment
capabilities. Our empirical evaluations demonstrate that the use of PRTs for
both inference-time and training-time scenarios significantly enhances the
performance of open-weight and commercial models, setting a new
state-of-the-art for HIPAA and GDPR policies. Beyond accuracy gains, we also
highlight how PRTs can improve an LLM's ability to accurately cite policy
clauses, as well as influence compliance decisions through their high
utilization from the raw chains of thought.

</details>


### [53] [Learning to Reason in Structured In-context Environments with Reinforcement Learning](https://arxiv.org/abs/2509.23330)
*Peng Yu,Zeyuan Zhao,Shao Zhang,Luoyi Fu,Xinbing Wang,Ying Wen*

Main category: cs.CL

TL;DR: 本文提出了一个结构化上下文环境（SIE）框架，通过自动构建基于大规模结构化数据的推理环境，实现了可扩展、可泛化且可验证的LLM强化学习调优。


<details>
  <summary>Details</summary>
Motivation: 现有的数学和编程环境难以扩展且依赖专家标注，游戏环境学习的技能泛化能力差，亟需一种兼具可扩展性、泛化性和可验证性的推理环境。

Method: SIE框架基于大规模结构化数据自动构建推理环境，利用数据中的组合模式促进推理技能泛化，并通过明确定义的模式与推理链条实现规则化验证。

Result: SIE在领域内的结构化推理中取得显著提升，且学得的组合推理技能能有效迁移至数学和逻辑等领域外任务。在信息有限的部分SIE环境中，LLM能通过环境探索推断缺失信息，实现稳健的推理提升和泛化。

Conclusion: SIE框架有效解决了推理环境的可扩展性、泛化性和可验证性难题，提升了LLM推理能力和泛化效果，是强化学习调优的重要突破。

Abstract: Large language models (LLMs) have achieved significant advancements in
reasoning capabilities through reinforcement learning (RL) via environmental
exploration. As the intrinsic properties of the environment determine the
abilities that LLMs can learn, the environment plays a important role in the RL
finetuning process. An ideal LLM reasoning environment should possess three
core characteristics: scalability, generalizable reasoning, and verifiability.
However, existing mathematical and coding environments are difficult to scale
due to heavy reliance on expert annotation, while the skills learned in
game-based environments are too specialized to generalize. To bridge this gap,
we introduce the \textbf{S}tructured \textbf{I}n-context \textbf{E}nvironment
(SIE) framework. SIE achieves scalability by automatically constructing
reasoning environments from large-scale structured data, where the rich
compositional patterns naturally support generalizable reasoning. Moreover, the
explicit schemas and reasoning chains in structured data provide a foundation
for rule-based verifiability. Experimental results show that SIE framework not
only achieves substantial improvements in in-domain structured reasoning, but
also enables the learned compositional reasoning skills to generalize
effectively to out-of-domain mathematical and logical reasoning tasks. We
further explored learning in information-limited partial SIEs and found that
LLMs can infer the missing information through exploring the environment,
leading to robust reasoning improvements and generalization performance.

</details>


### [54] [C-Evolve: Consensus-based Evolution for Prompt Groups](https://arxiv.org/abs/2509.23331)
*Tiancheng Li,Yuhang Wang,Zhiyang Chen,Zijun Wang,Liyuan Ma,Guo-jun Qi*

Main category: cs.CL

TL;DR: 本文提出了一种名为Consensus-Evolve的进化算法，通过多数投票机制优化提示语的组合，从而提升封闭源模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的提示进化算法侧重于单个提示的优化，缺乏通过多提示结果整合以达成共识来进一步提升系统性能的研究。

Method: 采用岛屿式进化算法维护群体多样性，利用群体内提示的投票分数作为适应度函数，促进形成高性能提示组。

Result: C-Evolve在多个任务（HotpotQA、IFBench、MATH等）中表现优异，相较于GEPA分别提升了4.95%和2.73%的准确率，在不同模型上均实现了性能提升。

Conclusion: 通过引入共识机制，C-Evolve有效提高了提示演化的质量，显著推动了封闭源模型的能力边界。

Abstract: Prompt evolution algorithms offer a powerful paradigm for enhancing AI
systems based on closed-source models, while few work explores whether
aggregating results from multiple prompts to reach a consensus can further
advance the system capability boundary. In this paper, we introduce
Consensus-Evolve (C-Evolve), an evolutionary algorithm that discovers a group
of prompts whose aggregated outputs after majority voting achieve optimal
performance. More specifically, C-Evolve employs an island-based evolutionary
algorithm to maintain population diversity, and prompts from distinct islands
are selected to form groups to aggregate their outputs. The key difference from
single individual evolution is a voting score, which evaluates each individual
prompt's contribution within groups. We take this as the fitness score for
evolution instead of individual performance. Consequently, C-Evolve is more
likely to produce and maintain prompts with higher potential to form a
high-performing group and eliminate low-performing ones, gradually improving
the group performance after reaching consensus. Our method achieves
state-of-the-art performance across a wide range of tasks, including both
open-ended tasks like HotpotQA and closed-ended tasks like MATH. On Qwen3-8B,
C-Evolve achieves 70.67% on HotpotQA and 43.88% on IFBench, which are 4.95% and
2.73% higher than GEPA, respectively. For GPT-4.1-mini, the accuracy on IFBench
is further improved to 47.96% and reaches 95.33% in the MATH benchmark. These
results demonstrate the C-Evolve's competitive performance.

</details>


### [55] [Dual-Space Smoothness for Robust and Balanced LLM Unlearning](https://arxiv.org/abs/2509.23362)
*Han Yan,Zheyuan Liu,Meng Jiang*

Main category: cs.CL

TL;DR: 该论文提出了PRISM框架，通过在表示空间和参数空间中实施双重平滑策略，提升机器反学习的鲁棒性和指标平衡。


<details>
  <summary>Details</summary>
Motivation: 现有最先进的反学习方法存在灾难性遗忘和指标不平衡问题，且容易被重学和越狱攻击利用。

Method: 设计了两个阶段的平滑优化：一是在表示空间中采用鲁棒训练探针防御越狱攻击；二是在参数空间中解耦保持-遗忘梯度冲突，平滑参数空间以减少重学攻击。

Result: 在WMDP和MUSE数据集上，PRISM在多种攻击下表现优于最先进基线，并且在关键指标上取得更好平衡。

Conclusion: PRISM有效提升机器反学习的安全性和性能，兼顾反学习效果、效用保持和隐私保护等指标，具备更强的鲁棒性。

Abstract: With the rapid advancement of large language models, Machine Unlearning has
emerged to address growing concerns around user privacy, copyright
infringement, and overall safety. Yet state-of-the-art (SOTA) unlearning
methods often suffer from catastrophic forgetting and metric imbalance, for
example by over-optimizing one objective (e.g., unlearning effectiveness,
utility preservation, or privacy protection) at the expense of others. In
addition, small perturbations in the representation or parameter space can be
exploited by relearn and jailbreak attacks. To address these challenges, we
propose PRISM, a unified framework that enforces dual-space smoothness in
representation and parameter spaces to improve robustness and balance
unlearning metrics. PRISM consists of two smoothness optimization stages: (i) a
representation space stage that employs a robustly trained probe to defend
against jailbreak attacks, and (ii) a parameter-space stage that decouples
retain-forget gradient conflicts, reduces imbalance, and smooths the parameter
space to mitigate relearning attacks. Extensive experiments on WMDP and MUSE,
across conversational-dialogue and continuous-text settings, show that PRISM
outperforms SOTA baselines under multiple attacks while achieving a better
balance among key metrics.

</details>


### [56] [MedCritical: Enhancing Medical Reasoning in Small Language Models via Self-Collaborative Correction](https://arxiv.org/abs/2509.23368)
*Xinchun Su,Chunxu Luo,Yixuan Li,Weidong Yang,Lipeng Ma*

Main category: cs.CL

TL;DR: 本文提出了MedCritical两阶段框架，通过大模型指导的小模型自我迭代强化推理能力，在医学复杂推理任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 目前医学领域复杂推理任务中，小语言模型表现不佳，而传统的知识蒸馏方法存在成本高、效率低的问题。

Method: 提出MedCritical框架，第一阶段利用大模型提取长链思维模板指导小模型推理，第二阶段引入模型自我迭代的直接偏好优化(DPO)提升小模型推理能力。

Result: MedCritical 7B模型在CMExam基准测试中，分别超过Taiyi和Huatuo-o1-7B模型3.04%和10.12%，达到了7B级别小模型的新SOTA性能。

Conclusion: 通过大模型引导小模型自我学习和迭代，MedCritical有效提升了小模型在医学复杂推理任务中的表现，同时降低了成本和提升了效率。

Abstract: In the field of medicine, complex reasoning tasks such as clinical diagnosis,
treatment planning, and medical knowledge integration pose significant
challenges, where small language models often underperform compared to large
language models like GPT-4 and Deepseek. Recent knowledge distillation-based
methods aim to address these issues through teacher-guided error correction,
but this LLM as judge approach remains challenging in terms of cost, time, and
efficiency. To circumvent this issue, we propose a novel two-stage framework,
MedCritical, which uses a small language model fine-tuned by a large teacher
model to play against itself. In the first stage, we extract high-level and
detailed long-chain thought templates from the teacher model to guide the
student model to generate more complex reasoning thoughts. In the second stage,
we introduce direct preference optimization (DPO) through model self-iteration
collaboration to enhance the reasoning ability of the student model by playing
against the correction trajectory of the fine-tuned model during training. This
model self-learning DPO approach teaches the student model to use its own
error-driven insights to consolidate its skills and knowledge to solve complex
problems, and achieves comparable results to traditional knowledge distillation
methods using teacher models at a lower cost. Notably, our MedCritical 7B model
outperforms the Taiyi and Huatuo-o1-7B models by 3.04\% and 10.12\%
respectively on the CMExam benchmark, achieving new SOTA performance among
7B-class small models.

</details>


### [57] [Alignment through Meta-Weighted Online Sampling: Bridging the Gap between Data Generation and Preference Optimization](https://arxiv.org/abs/2509.23371)
*Junming Yang,Ning Xu,Biao Liu,Shiqi Qiao,Xin Geng*

Main category: cs.CL

TL;DR: 提出了一种新的偏好优化框架MetaAPO，通过动态调整数据生成与模型训练，提升大语言模型的对齐效果，显著降低在线标注成本。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型偏好优化方法存在离线偏好数据与模型策略分布不匹配的问题，且难以适应模型动态学习状态。

Method: 引入轻量级元学习器作为“对齐差距估计器”，动态评估和引导在线采样，结合在线与离线数据，通过样本级元权重优化模型训练。

Result: 在AlpacaEval 2、Arena-Hard和MT-Bench数据集上，MetaAPO表现优于现有偏好优化方法，并减少42%的在线标注成本。

Conclusion: MetaAPO有效弥合了离线数据与在线采样的分布差距，提升偏好优化效果，降低了标注成本，适用于大语言模型的对齐训练。

Abstract: Preference optimization is crucial for aligning large language models (LLMs)
with human values and intentions. A significant challenge in this process is
the distribution mismatch between pre-collected offline preference data and the
evolving model policy. Existing methods attempt to reduce this gap using static
heuristics or decoupled online sampling strategies, but they often fail to
adapt to the model's dynamic learning state. To bridge this gap, we propose
Meta-Weighted Adaptive Preference Optimization (MetaAPO), a novel framework
that dynamically couples data generation with model training. MetaAPO employs a
lightweight meta-learner, as an "alignment gap estimator", to evaluate the
potential benefits of on-policy sampling in relation to offline data. This
guides targeted online generation and assigns sample-wise meta-weights to the
optimization objective, dynamically balancing the quality and distribution of
online and offline data. Experiments on AlpacaEval 2, Arena-Hard and MT-Bench
demonstrate that MetaAPO consistently outperforms existing preference
optimization approaches across various settings, while reducing 42% in online
annotation costs.

</details>


### [58] [CCD: Mitigating Hallucinations in Radiology MLLMs via Clinical Contrastive Decoding](https://arxiv.org/abs/2509.23379)
*Xi Zhang,Zaiqiao Meng,Jake Lever,Edmond S. L. Ho*

Main category: cs.CL

TL;DR: 本文提出了一种名为临床对比编码（CCD）的无训练、无检索推理框架，用于减少多模态大型语言模型在放射科报告生成中的医学幻觉，显著提升了生成报告的临床准确性。


<details>
  <summary>Details</summary>
Motivation: 多模态大型语言模型在放射学中虽然取得进展，但易因对临床信息敏感过高而生成临床不支持的描述（医学幻觉），这在严谨要求的医疗应用中存在风险。

Method: 提出了临床对比编码（CCD），引入双阶段对比机制，融合来自放射学专家模型的结构化临床信号，细化生成过程中的词元概率，提升临床一致性，且无需修改基础模型或额外训练。

Result: 在三个数据集及多模型上测试，CCD在MIMIC-CXR数据集的RadGraph-F1指标上提升高达17%，持续提升放射科报告生成表现。

Conclusion: CCD为放射学领域提供了一种轻量化、通用的解决方案，能有效缓解医学幻觉，实现专家模型与多模态大型语言模型的有效结合。

Abstract: Multimodal large language models (MLLMs) have recently achieved remarkable
progress in radiology by integrating visual perception with natural language
understanding. However, they often generate clinically unsupported
descriptions, known as medical hallucinations, which pose serious risks in
medical applications that demand accuracy and image-grounded outputs. Through
empirical analysis, we find that prompt-induced hallucinations remain prevalent
in radiology MLLMs, largely due to over-sensitivity to clinical sections. To
address this, we introduce Clinical Contrastive Cecoding (CCD), a training-free
and retrieval-free inference framework that integrates structured clinical
signals from task-specific radiology expert models. CCD introduces a dual-stage
contrastive mechanism to refine token-level logits during generation, thereby
enhancing clinical fidelity without modifying the base MLLM. Experiments on
three datasets and multiple models demonstrate that CCD consistently improves
overall performance on radiology report generation (RRG). On the MIMIC-CXR
dataset, it yields up to a 17% improvement in RadGraph-F1 when applied to
state-of-the-art RRG models. Our approach provides a lightweight and
generalisable solution for mitigating medical hallucinations, effectively
bridging expert models and MLLMs in radiology.

</details>


### [59] [Guard Vector: Beyond English LLM Guardrails with Task-Vector Composition and Streaming-Aware Prefix SFT](https://arxiv.org/abs/2509.23381)
*Wonhyuk Lee,Youngchol Kim,Yunjin Park,Junhyung Moon,Dongyoung Jeong,Wanjin Park*

Main category: cs.CL

TL;DR: 本文提出了Guard Vector，通过计算保护模型与预训练语言模型之间的参数差异，生成目标保护模型（TGM），提升安全性分类性能并支持中文、日语、韩语扩展，无需额外训练或标签。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型在安全任务中的适应性和跨语言扩展问题，同时提升分类质量并减少训练和计算资源需求。

Method: 通过将Guard Vector与目标语言模型组合得到TGM，采用前缀训练和单词输出分类器的流式感知方法进行微调，保证模型在流式输入下的表现一致性，提升推理效率。

Result: TGM在标准安全测试中优于现有保护模型，实现了跨语言扩展及模型在两种主流保护模型结构间的可移植性，同时在流式情况下保持分类质量并降低延迟。

Conclusion: 该方法有效提升了语言模型的安全分类性能和跨语言能力，节省了训练数据和计算资源，促进了更负责任的AI生态发展。

Abstract: We introduce Guard Vector, a safety task vector computed as the parameter
difference between a guardrail model (Guard Model) and a same-architecture
pretrained language model. Composing this vector with a target language model
yields a Target Guard Model (TGM). We then adapt TGM with a streaming-aware
approach that combines prefix-based training and evaluation with a classifier
that produces a single-token output. With this composition alone, TGM improves
classification quality over established Guard Models across standard safety
suites and enables language extensibility to Chinese, Japanese, and Korean,
requiring neither additional training nor target language labels. It also
demonstrates model portability across two widely used public guardrail
backbones, Llama and Gemma. With prefix SFT (supervised fine-tuning), TGM
preserves classification quality under streaming by aligning the behavior
between prefix inputs and full-text inputs. The single-token output design
increases throughput and reduces latency. Together, these components reduce
data and compute requirements while promoting streaming-aware evaluation
practices, thereby contributing to a more responsible AI ecosystem.

</details>


### [60] [Train Once, Answer All: Many Pretraining Experiments for the Cost of One](https://arxiv.org/abs/2509.23383)
*Sebastian Bordt,Martin Pawelczyk*

Main category: cs.CL

TL;DR: 该论文提出在单次训练过程中同时进行多个预训练实验，以节省计算资源并研究大语言模型的学习、推理与记忆机制。


<details>
  <summary>Details</summary>
Motivation: 传统的预训练实验计算成本高，限制了大语言模型学习机制的深入研究。

Method: 在训练一个1.5B参数模型时同时进行十个预训练实验，涵盖数据污染、中毒、记忆、知识获取、数学推理和水印等多个方面。

Result: 单模型能同时复现多个实验结果，且不同实验间的相互影响极小，整体训练动态和性能无明显下降。

Conclusion: 单次训练中进行多项预训练实验是可行的，能在有限计算预算下促进大语言模型的科学实验研究。

Abstract: Recent work has demonstrated that controlled pretraining experiments are a
powerful tool for understanding learning, reasoning, and memorization in large
language models (LLMs). However, the computational cost of pretraining presents
a significant constraint. To overcome this constraint, we propose to conduct
multiple pretraining experiments simultaneously during a single training run.
We demonstrate the feasibility of this approach by conducting ten experiments
during the training of a 1.5B parameter model on 210B tokens. Although we only
train a single model, we can replicate the results from multiple previous works
on data contamination, poisoning, and memorization. We also conduct novel
investigations into knowledge acquisition, mathematical reasoning, and
watermarking. For example, we dynamically update the training data until the
model acquires a particular piece of knowledge. Remarkably, the influence of
the ten experiments on the model's training dynamics and overall performance is
minimal. However, interactions between different experiments may act as a
potential confounder in our approach. We propose to test for interactions with
continual pretraining experiments, finding them to be negligible in our setup.
Overall, our findings suggest that performing multiple pretraining experiments
in a single training run can enable rigorous scientific experimentation with
large models on a compute budget.

</details>


### [61] [No Loss, No Gain: Gated Refinement and Adaptive Compression for Prompt Optimization](https://arxiv.org/abs/2509.23387)
*Wenhang Shi,Yiren Chen,Shuqing Bian,Xinyi Zhang,Kai Tang,Pengfei Hu,Zhe Zhao,Wei Lu,Xiaoyong Du*

Main category: cs.CL

TL;DR: 本文提出了GRACE框架，通过门控精炼和自适应压缩两大战略，实现了高效且稳定的自动提示词优化，在多个任务中显著提升性能和优化效率。


<details>
  <summary>Details</summary>
Motivation: 当前自动提示词优化方法存在稳定性差、易陷入局部最优的问题，导致效率低下且效果有限。

Method: GRACE引入了门控精炼策略（通过反馈调节门和更新拒绝门稳定优化信号）和自适应压缩策略（提取提示词核心概念重新优化路径），通过信息丢失促进性能提升。

Result: 在11个任务中，GRACE相比最新方法分别提升了4.7%、4.4%和2.7%的平均相对性能，且只使用了前人25%的提示词生成预算，显著提高效率与性能。

Conclusion: GRACE框架有效解决了自动提示词优化中的局部最优问题，实现了高效稳定的性能提升，具有显著的实践应用价值。

Abstract: Prompt engineering is crucial for leveraging the full potential of large
language models (LLMs). While automatic prompt optimization offers a scalable
alternative to costly manual design, generating effective prompts remains
challenging. Existing methods often struggle to stably generate improved
prompts, leading to low efficiency, and overlook that prompt optimization
easily gets trapped in local optima. Addressing this, we propose GRACE, a
framework that integrates two synergistic strategies: Gated Refinement and
Adaptive Compression, achieving Efficient prompt optimization. The gated
refinement strategy introduces a feedback regulation gate and an update
rejection gate, which refine update signals to produce stable and effective
prompt improvements. When optimization stagnates, the adaptive compression
strategy distills the prompt's core concepts, restructuring the optimization
trace and opening new paths. By strategically introducing information loss
through refinement and compression, GRACE delivers substantial gains in
performance and efficiency. In extensive experiments on 11 tasks across three
practical domains, including BIG-Bench Hard (BBH), domain-specific, and general
NLP tasks, GRACE achieves significant average relative performance improvements
of 4.7%, 4.4% and 2.7% over state-of-the-art methods, respectively. Further
analysis shows that GRACE achieves these gains using only 25% of the prompt
generation budget required by prior methods, highlighting its high optimization
efficiency and low computational overhead. Our code is available at
https://github.com/Eric8932/GRACE.

</details>


### [62] [Liaozhai through the Looking-Glass: On Paratextual Explicitation of Culture-Bound Terms in Machine Translation](https://arxiv.org/abs/2509.23395)
*Sherrie Shen,Weixuan Wang,Alexandra Birch*

Main category: cs.CL

TL;DR: 本文提出了机器翻译中利用副文本进行文化依赖术语解释的新任务，构建了相关数据集并评估了大型语言模型的表现，发现副文本有助于理解但模型效果仍不及专业译者。


<details>
  <summary>Details</summary>
Motivation: 传统机器翻译难以准确传递文化依赖术语的含义，且现有方法忽视了专业译者使用的副文本注释。

Method: 基于Genette的副文本理论，构建了包含560条专家对齐副文本的数据集，从四个中文古典短篇小说英译版本中收集；采用带推理跟踪和不带推理跟踪的LLM进行副文本选择与内容的评估，同时使用内在提示和代理检索方法进行实验。

Result: 实验显示该任务具有较大挑战性，LLM生成的副文本能提升读者理解，但效果明显不及译者撰写的副文本。统计分析发现专业译者在副文本使用上差异较大，反映文化调解的开放性。

Conclusion: 副文本解释拓展了机器翻译的边界，超越了单纯的语言等价，为单语解释和个性化适应提供了潜在可能。

Abstract: The faithful transfer of contextually-embedded meaning continues to challenge
contemporary machine translation (MT), particularly in the rendering of
culture-bound terms--expressions or concepts rooted in specific languages or
cultures, resisting direct linguistic transfer. Existing computational
approaches to explicitating these terms have focused exclusively on in-text
solutions, overlooking paratextual apparatus in the footnotes and endnotes
employed by professional translators. In this paper, we formalize Genette's
(1987) theory of paratexts from literary and translation studies to introduce
the task of paratextual explicitation for MT. We construct a dataset of 560
expert-aligned paratexts from four English translations of the classical
Chinese short story collection Liaozhai and evaluate LLMs with and without
reasoning traces on choice and content of explicitation. Experiments across
intrinsic prompting and agentic retrieval methods establish the difficulty of
this task, with human evaluation showing that LLM-generated paratexts improve
audience comprehension, though remain considerably less effective than
translator-authored ones. Beyond model performance, statistical analysis
reveals that even professional translators vary widely in their use of
paratexts, suggesting that cultural mediation is inherently open-ended rather
than prescriptive. Our findings demonstrate the potential of paratextual
explicitation in advancing MT beyond linguistic equivalence, with promising
extensions to monolingual explanation and personalized adaptation.

</details>


### [63] [Comparison of Scoring Rationales Between Large Language Models and Human Raters](https://arxiv.org/abs/2509.23412)
*Haowei Hua,Hong Jiao,Dan Song*

Main category: cs.CL

TL;DR: 本文探讨了利用大型语言模型（LLMs）如GPT-4o、Gemini在自动评分中的表现及其提供评分理由的能力，分析了其准确性和评分一致性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的发展，自动评分技术获得提升，研究其评分理由有助于理解评分背后的推理过程，提升评分一致性。

Method: 使用大规模作文测试数据，通过平方加权卡帕系数和归一化互信息评估LLMs的评分准确性，利用余弦相似度比较评分理由的相似性，采用主成分分析探索理由的聚类模式。

Result: 发现LLMs在评分准确性和评分理由方面表现出较强能力，但存在一定评分不一致性，评分理由之间表现出不同的聚类模式。

Conclusion: 研究揭示了LLMs在自动评分中的思维特点和准确性，为改进人类与自动评分的评分理由理解提供了理论支持。

Abstract: Advances in automated scoring are closely aligned with advances in
machine-learning and natural-language-processing techniques. With recent
progress in large language models (LLMs), the use of ChatGPT, Gemini, Claude,
and other generative-AI chatbots for automated scoring has been explored. Given
their strong reasoning capabilities, LLMs can also produce rationales to
support the scores they assign. Thus, evaluating the rationales provided by
both human and LLM raters can help improve the understanding of the reasoning
that each type of rater applies when assigning a score. This study investigates
the rationales of human and LLM raters to identify potential causes of scoring
inconsistency. Using essays from a large-scale test, the scoring accuracy of
GPT-4o, Gemini, and other LLMs is examined based on quadratic weighted kappa
and normalized mutual information. Cosine similarity is used to evaluate the
similarity of the rationales provided. In addition, clustering patterns in
rationales are explored using principal component analysis based on the
embeddings of the rationales. The findings of this study provide insights into
the accuracy and ``thinking'' of LLMs in automated scoring, helping to improve
the understanding of the rationales behind both human scoring and LLM-based
automated scoring.

</details>


### [64] [Retrieval-Constrained Decoding Reveals Underestimated Parametric Knowledge in Language Models](https://arxiv.org/abs/2509.23417)
*Rajaa El Hamdani,Samy Haffoudhi,Nils Holzenberger,Fabian Suchanek,Thomas Bonald,Fragkiskos D. Malliaros*

Main category: cs.CL

TL;DR: 提出了检索约束解码（RCD）策略，通过限制生成的答案形式提升语言模型的准确率，缓解对知识评估的严格限制。


<details>
  <summary>Details</summary>
Motivation: 现有评价方法过于严格，导致语言模型生成的另类表达形式答案被误判为错误，从而低估了模型的知识能力。

Method: 引入RCD解码策略，限制模型输出唯一表述形式；构建包含1.9万多个知识问答的数据集YAGO-QA，用于评估。

Result: 在评测135M至70B参数的开源语言模型时，RCD解码显著提升模型得分，例如Llama-3.1-70B由32.3%提升至46.0%，8B模型使用RCD甚至超越了大模型使用普通解码的表现。

Conclusion: RCD策略有效缓解了模型知识被低估的问题，提升了语言模型在事实问答任务中的表现，体现了对模型真实知识水平的更准确评估。

Abstract: Language models (LMs) encode substantial factual knowledge, but often produce
answers judged as incorrect. We hypothesize that many of these answers are
actually correct, but are expressed in alternative surface forms that are
dismissed due to an overly strict evaluation, leading to an underestimation of
models' parametric knowledge. We propose Retrieval-Constrained Decoding (RCD),
a decoding strategy that restricts model outputs to unique surface forms. We
introduce YAGO-QA, a dataset of 19,137 general knowledge questions. Evaluating
open-source LMs from 135M to 70B parameters, we show that standard decoding
undervalues their knowledge. For instance, Llama-3.1-70B scores only 32.3% F1
with vanilla decoding but 46.0% with RCD. Similarly, Llama-3.1-8B reaches 33.0%
with RCD, outperforming the larger model under vanilla decoding. We publicly
share the code and dataset at https://github.com/Rajjaa/disambiguated-LLM.

</details>


### [65] [Cognition-of-Thought Elicits Social-Aligned Reasoning in Large Language Models](https://arxiv.org/abs/2509.23441)
*Xuanming Zhang,Yuxuan Chen,Min-Hsuan Yeh,Yixuan Li*

Main category: cs.CL

TL;DR: 本文提出了Cognition-of-Thought (CooT)框架，通过在大语言模型的生成过程中加入认知自我监控环节，提高模型对有害行为的检测和纠正能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的安全对齐策略多依赖于模型权重，导致控制隐式且不易修改，不能动态应对生成过程中的潜在风险。

Method: CooT框架结合文本生成器与认知感知器，感知器采用基于优先级的层级原则监控生成序列，检测到违规时回退并在注入社会先验与警告的指导下重新生成。

Result: CooT在多个基准测试和不同模型家族中显著提升了模型的安全性和社会推理能力。

Conclusion: CooT将对齐从固定属性转化为推理时动态、显式且可审计的过程，实现了无需重新训练即可灵活更新安全策略。

Abstract: Large language models (LLMs) excel at complex reasoning but can still exhibit
harmful behaviors. Current alignment strategies typically embed safety into
model weights, making these controls implicit, static, and difficult to modify.
This paper introduces Cognition-of-Thought (CooT), a novel decoding-time
framework that equips LLMs with an explicit cognitive self-monitoring loop.
CooT couples a standard text Generator with a cognitive Perceiver that
continuously monitors the unfolding sequence. The Perceiver uses a structured,
precedence-based hierarchy of principles (e.g., safety over obedience) to
detect potential misalignments as they arise. When violations are flagged, CooT
intervenes by rolling back the generation to the point of error and
regenerating under injected guidance that combines universal social priors with
context-specific warnings. CooT thus transforms alignment from a fixed property
into an explicit, dynamic, and auditable process active during inference,
allowing for flexible policy updates without retraining the model. Extensive
experiments across multiple benchmarks and model families confirm that CooT
consistently improves safety and social reasoning performance.

</details>


### [66] [Text-Based Approaches to Item Difficulty Modeling in Large-Scale Assessments: A Systematic Review](https://arxiv.org/abs/2509.23486)
*Sydney Peters,Nan Zhang,Hong Jiao,Ming Li,Tianyi Zhou,Robert Lissitz*

Main category: cs.CL

TL;DR: 本文综述了基于文本的自动题目难度预测方法，评估了37篇相关研究，表明先进语言模型在性能和效率上优于传统方法，有望实现高准确度的难度预测。


<details>
  <summary>Details</summary>
Motivation: 传统题目难度测定依赖费时费力的实地测试和经典理论模型，急需更高效、成本更低的自动化预测方法。

Method: 收集并分析37篇利用机器学习和语言模型进行题目难度预测的研究，比较数据集、模型架构、特征选择和评估指标等。

Result: 发现经典机器学习模型因可解释性仍有价值，但基于变换器的语言模型更能捕捉语法和语义信息，无需人工特征工程，预测效果显著，RMSE最低0.165，相关系数最高0.87，准确率最高0.806。

Conclusion: 基于文本的自动难度预测方法具备广阔应用前景，能够提高大规模测评的效率和公平性，未来研究应进一步优化模型性能及推广应用。

Abstract: Item difficulty plays a crucial role in test performance, interpretability of
scores, and equity for all test-takers, especially in large-scale assessments.
Traditional approaches to item difficulty modeling rely on field testing and
classical test theory (CTT)-based item analysis or item response theory (IRT)
calibration, which can be time-consuming and costly. To overcome these
challenges, text-based approaches leveraging machine learning and language
models, have emerged as promising alternatives. This paper reviews and
synthesizes 37 articles on automated item difficulty prediction in large-scale
assessment settings published through May 2025. For each study, we delineate
the dataset, difficulty parameter, subject domain, item type, number of items,
training and test data split, input, features, model, evaluation criteria, and
model performance outcomes. Results showed that although classic machine
learning models remain relevant due to their interpretability, state-of-the-art
language models, using both small and large transformer-based architectures,
can capture syntactic and semantic patterns without the need for manual feature
engineering. Uniquely, model performance outcomes were summarized to serve as a
benchmark for future research and overall, text-based methods have the
potential to predict item difficulty with root mean square error (RMSE) as low
as 0.165, Pearson correlation as high as 0.87, and accuracy as high as 0.806.
The review concludes by discussing implications for practice and outlining
future research directions for automated item difficulty modeling.

</details>


### [67] [The Impact of Role Design in In-Context Learning for Large Language Models](https://arxiv.org/abs/2509.23501)
*Hamidreza Rouzegar,Masoud Makrehchi*

Main category: cs.CL

TL;DR: 本文研究了在GPT-3.5、GPT-4o、Llama2系列模型中，角色设计对上下文学习效果的影响，尤其是在零样本和少样本场景下通过角色扮演提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 虽然提示工程已广泛研究，但在提示中角色设计对模型效果的影响尚未被充分探讨。

Method: 在多个数据集和任务（情感分析、文本分类、问答、数学推理）上，比较不同角色配置对GPT-3.5、GPT-4o及Llama2模型的零样本和少样本学习效果。

Result: 研究发现基于角色设计的提示结构能够提高LLM的性能表现。

Conclusion: 角色设计是改进大语言模型上下文学习性能的有效途径，值得进一步研究和应用。

Abstract: In-context learning (ICL) enables Large Language Models (LLMs) to generate
predictions based on prompts without additional fine-tuning. While prompt
engineering has been widely studied, the impact of role design within prompts
remains underexplored. This study examines the influence of role configurations
in zero-shot and few-shot learning scenarios using GPT-3.5 and GPT-4o from
OpenAI and Llama2-7b and Llama2-13b from Meta. We evaluate the models'
performance across datasets, focusing on tasks like sentiment analysis, text
classification, question answering, and math reasoning. Our findings suggest
the potential of role-based prompt structuring to enhance LLM performance.

</details>


### [68] [AraS2P: Arabic Speech-to-Phonemes System](https://arxiv.org/abs/2509.23504)
*Bassam Matar,Mohamed Fayed,Ayman Khalafallah*

Main category: cs.CL

TL;DR: AraS2P系统通过两阶段训练策略，结合大规模语音-音素数据预训练和多样化数据增强，提升了阿拉伯语音到音素转换的性能，并在Iqra'Eval 2025任务中排名第一。


<details>
  <summary>Details</summary>
Motivation: 提高阿拉伯语语音转音素的准确性，尤其是在音素级别的误读检测方面。

Method: 采用Wav2Vec2-BERT模型，先进行任务适应的持续预训练，使用由MSA Phonetiser转换的大规模阿拉伯语音-音素数据；随后在官方任务数据上微调，并通过合成朗诵数据进行多样化增强，模拟真实语音中的发音变化和错误。

Result: 系统在Iqra'Eval 2025共享任务中荣获第一名，显示出方法在音素级误读检测中的优越性能。

Conclusion: 结合音素感知的预训练和针对性的增强策略，可以显著提升阿拉伯语音转音素系统的性能，特别是在误读检测任务中效果显著。

Abstract: This paper describes AraS2P, our speech-to-phonemes system submitted to the
Iqra'Eval 2025 Shared Task. We adapted Wav2Vec2-BERT via Two-Stage training
strategy. In the first stage, task-adaptive continue pretraining was performed
on large-scale Arabic speech-phonemes datasets, which were generated by
converting the Arabic text using the MSA Phonetiser. In the second stage, the
model was fine-tuned on the official shared task data, with additional
augmentation from XTTS-v2-synthesized recitations featuring varied Ayat
segments, speaker embeddings, and textual perturbations to simulate possible
human errors. The system ranked first on the official leaderboard,
demonstrating that phoneme-aware pretraining combined with targeted
augmentation yields strong performance in phoneme-level mispronunciation
detection.

</details>


### [69] [From Human Annotation to Automation: LLM-in-the-Loop Active Learning for Arabic Sentiment Analysis](https://arxiv.org/abs/2509.23515)
*Dania Refai,Alaa Dalaq,Doaa Dalaq,Irfan Ahmad*

Main category: cs.CL

TL;DR: 本文提出了用于阿拉伯语情感分析的主动学习框架，结合大型语言模型（LLM）辅助标注，显著减少标注成本并保持高性能。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语情感分析由于缺乏大规模高质量标注数据，进展缓慢，且目前较少研究在阿拉伯语上下文中使用主动学习和大型语言模型辅助标注。

Method: 设计主动学习框架，评估三种深度学习模型（LSTM、GRU、RNN）和两种标注策略（人工标注与LLM辅助标注），比较五个LLM在三个阿拉伯语数据集上的表现，并选择最佳LLM进行标注辅助。

Result: LLM辅助的主动学习在多个数据集上表现出与人工标注相当甚至更优的准确率。例如，Hunger Station数据集上，使用GPT-4o生成标签的LSTM模型在仅450个样本的情况下达到93%的准确率。

Conclusion: LLM辅助的主动学习有效减少了阿拉伯语情感分析中的标注工作量，且保持高性能，显示出其在低资源语言任务中的潜力。

Abstract: Natural language processing (NLP), particularly sentiment analysis, plays a
vital role in areas like marketing, customer service, and social media
monitoring by providing insights into user opinions and emotions. However,
progress in Arabic sentiment analysis remains limited due to the lack of large,
high-quality labeled datasets. While active learning has proven effective in
reducing annotation efforts in other languages, few studies have explored it in
Arabic sentiment tasks. Likewise, the use of large language models (LLMs) for
assisting annotation and comparing their performance to human labeling is still
largely unexplored in the Arabic context. In this paper, we propose an active
learning framework for Arabic sentiment analysis designed to reduce annotation
costs while maintaining high performance. We evaluate multiple deep learning
architectures: Specifically, long short-term memory (LSTM), gated recurrent
units (GRU), and recurrent neural networks (RNN), across three benchmark
datasets: Hunger Station, AJGT, and MASAC, encompassing both modern standard
Arabic and dialectal variations. Additionally, two annotation strategies are
compared: Human labeling and LLM-assisted labeling. Five LLMs are evaluated as
annotators: GPT-4o, Claude 3 Sonnet, Gemini 2.5 Pro, DeepSeek Chat, and LLaMA 3
70B Instruct. For each dataset, the best-performing LLM was used: GPT-4o for
Hunger Station, Claude 3 Sonnet for AJGT, and DeepSeek Chat for MASAC. Our
results show that LLM-assisted active learning achieves competitive or superior
performance compared to human labeling. For example, on the Hunger Station
dataset, the LSTM model achieved 93% accuracy with only 450 labeled samples
using GPT-4o-generated labels, while on the MASAC dataset, DeepSeek Chat
reached 82% accuracy with 650 labeled samples, matching the accuracy obtained
through human labeling.

</details>


### [70] [On the Shelf Life of Fine-Tuned LLM Judges: Future Proofing, Backward Compatibility, and Question Generalization](https://arxiv.org/abs/2509.23542)
*Janvijay Singh,Austin Xu,Yilun Zhou,Yefan Zhou,Dilek Hakkani-Tur,Shafiq Joty*

Main category: cs.CL

TL;DR: 该论文研究了基于大语言模型的评判者在实际部署中面临的未来适应性、向后兼容性和问题泛化能力问题，重点评估了不同微调方法和模型基础下的表现。


<details>
  <summary>Details</summary>
Motivation: 当前微调评判者虽性能优越，但实务应用中其对未来问题和生成模型的适应性及泛化能力尚未充分评估。

Method: 采用统一框架下，针对数学领域设计多种训练和测试分布，结合三种微调算法（SFT，DPO）与三种基础模型，系统研究未来适应性、向后兼容性及问题泛化等性能。

Result: 大多数模型的未来适应性较差，向后兼容性较好，DPO微调效果较稳定且性能提升；持续学习在应对生成器变化上更均衡；普遍存在问题泛化能力不足的问题。

Conclusion: 当前评判者模型在应对生成器更新和未知问题上仍存在明显挑战，未来设计需注重提升未来适应性和泛化能力以符合实际应用需求。

Abstract: The LLM-as-a-judge paradigm is widely used in both evaluating free-text model
responses and reward modeling for model alignment and finetuning. Recently,
finetuning judges with judge-specific data has emerged as an often preferred
choice over directly prompting frontier models as judges, as the former
achieves better performance with smaller model sizes while being more robust to
common biases. However, the standard evaluation ignores several practical
concerns of finetuned judges regarding their real world deployment. In this
paper, we identify and formalize three aspects that affect the shelf life of
these judges: future proofing and backward compatibility -- how well judges
finetuned on responses by today's generator models perform on responses by
future models or past models, as well as question generalization -- how well
judges generalize to unseen questions at test time. We study these three
aspects in the math domain under a unified framework with varying train and
test distributions, three SFT- and DPO-based finetuning algorithms and three
different base models. Experiments suggest that future-proofing is challenging
for most models, while backward compatibility is relatively easy, with
DPO-trained models consistently improving performance. We further find that
continual learning provides a more balanced adaptation to shifts between older
and newer response distributions than training solely on stronger or weaker
responses. Moreover, all models observe certain degrees of performance
degradation when moving from questions seen during training to unseen ones,
showing that current judges do not fully generalize to unseen questions. These
findings provide insights into practical considerations for developing and
deploying judge models in the face of ever-changing generators.

</details>


### [71] [Automatic Speech Recognition for Greek Medical Dictation](https://arxiv.org/abs/2509.23550)
*Vardis Georgilas,Themos Stafylakis*

Main category: cs.CL

TL;DR: 该论文提出了一种针对希腊医疗领域的语音转写系统，结合自动语音识别和文本校正，实现了更准确的专业医疗术语识别。


<details>
  <summary>Details</summary>
Motivation: 帮助医疗从业者减轻手动文档负担，提高工作效率。

Method: 结合自动语音识别技术与文本校正模型，利用声学和文本建模，针对希腊医疗术语和语言变异进行领域特定微调。

Result: 系统能够生成更准确连贯的医疗语音转录，适应复杂医疗术语和语言不一致性。

Conclusion: 该系统为希腊医疗行业提供了实用的语言技术支持，促进医疗文档自动化和效率提升。

Abstract: Medical dictation systems are essential tools in modern healthcare, enabling
accurate and efficient conversion of speech into written medical documentation.
The main objective of this paper is to create a domain-specific system for
Greek medical speech transcriptions. The ultimate goal is to assist healthcare
professionals by reducing the overload of manual documentation and improving
workflow efficiency. Towards this goal, we develop a system that combines
automatic speech recognition techniques with text correction model, allowing
better handling of domain-specific terminology and linguistic variations in
Greek. Our approach leverages both acoustic and textual modeling to create more
realistic and reliable transcriptions. We focused on adapting existing language
and speech technologies to the Greek medical context, addressing challenges
such as complex medical terminology and linguistic inconsistencies. Through
domain-specific fine-tuning, our system achieves more accurate and coherent
transcriptions, contributing to the development of practical language
technologies for the Greek healthcare sector.

</details>


### [72] [Towards Efficient CoT Distillation: Self-Guided Rationale Selector for Better Performance with Fewer Rationales](https://arxiv.org/abs/2509.23574)
*Jianzhi Yan,Le Liu,Youcheng Pan,Shiwei Chen,Yang Xiang,Buzhou Tang*

Main category: cs.CL

TL;DR: 本文提出了一种面向模型的推理选择蒸馏方法MoRSD，通过选择高质量的推理步骤提升小语言模型的推理能力，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有链式思维蒸馏方法忽视了推理步骤质量，可能传递噪声或错误信息，降低学生模型性能，需要筛选高质量推理步骤。

Method: 提出MoRSD方法，设计推理难度指标评估学生模型在给定推理下的正确率，通过控制推理的准确性、多样性和难度选择高质量推理进行蒸馏。

Result: 在七个数据集和三个任务上对比基线，MoRSD实现了4.6%的平均性能提升，使用的推理数量更少且质量更高。

Conclusion: 高质量推理步骤虽占较少比例，但能有效提升学生模型推理能力，MoRSD为高效链式思维蒸馏提供了新思路。

Abstract: Chain-of-thought (CoT) distillation aims to enhance small language models'
(SLMs) reasoning by transferring multi-step reasoning capability from the
larger teacher models. However, existing work underestimates rationale quality,
focusing primarily on data quantity, which may transfer noisy or incorrect
information to the student model. To address the above issues, we proposed
\textbf{M}odel-\textbf{O}riented \textbf{R}ationale \textbf{S}election
\textbf{D}istillation (MoRSD), which can discern and select high quality
rationales for distillation to improve performance further. We further propose
a Rationale Difficulty (RD) metric to measure the ability of the student model
to generate the correct answer under a given rationale. Compared to the
baseline, we achieved 4.6$\%$ average improvement on seven datasets over three
tasks, using fewer rationales by controlling their accuracy, diversity, and
difficulty. Our results reveal that a small portion of the high quality
rationales can enhance the reasoning ability of student models than the entire
dataset. Our method promises to be a possible solution for efficient CoT
distillation. Our code will be released in https://github.com/Leon221220/MoRSD.

</details>


### [73] [Jackal: A Real-World Execution-Based Benchmark Evaluating Large Language Models on Text-to-JQL Tasks](https://arxiv.org/abs/2509.23579)
*Kevin Frank,Anmol Gulati,Elias Lumer,Sindy Campagna,Vamse Kumar Subbiah*

Main category: cs.CL

TL;DR: 本文提出了Jackal，一个包含10万条自然语言请求与JQL查询对的大规模文本转JQL基准，结合执行准确率评测多种大语言模型，揭示了现有模型在JQL生成上的性能瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏公开、基于实际执行结果的自然语言查询转JQL的真实世界评测基准，阻碍了相关技术的发展。

Method: 构建包含不同类型用户请求的Jackal数据集，结合实时Jira实例的执行结果进行评分，评测23个大语言模型的文本转JQL性能。

Result: 最佳模型Gemini 2.5 Pro最高执行准确率仅60.3%，不同请求类型表现差异显著，尤其是短文本和语义相似请求准确率较低。

Conclusion: Jackal基准揭示了当前大语言模型在JQL生成上的不足，为未来提升企业级Jira数据处理能力提供了新的研究挑战和评测工具。

Abstract: Enterprise teams rely on the Jira Query Language (JQL) to retrieve and filter
issues from Jira. Yet, to our knowledge, there is no open, real-world,
execution-based benchmark for mapping natural language queries to JQL. We
introduce Jackal, a novel, large-scale text-to-JQL benchmark comprising 100,000
natural language (NL) requests paired with validated JQL queries and
execution-based results on a live Jira instance with over 200,000 issues. To
reflect real-world usage, each JQL query is associated with four types of user
requests: (i) Long NL, (ii) Short NL, (iii) Semantically Similar, and (iv)
Semantically Exact. We release Jackal, a corpus of 100,000 text-to-JQL pairs,
together with an execution-based scoring toolkit, and a static snapshot of the
evaluated Jira instance for reproducibility. We report text-to-JQL results on
23 Large Language Models (LLMs) spanning parameter sizes, open and closed
source models, across execution accuracy, exact match, and canonical exact
match. In this paper, we report results on Jackal-5K, a 5,000-pair subset of
Jackal. On Jackal-5K, the best overall model (Gemini 2.5 Pro) achieves only
60.3% execution accuracy averaged equally across four user request types.
Performance varies significantly across user request types: (i) Long NL
(86.0%), (ii) Short NL (35.7%), (iii) Semantically Similar (22.7%), and (iv)
Semantically Exact (99.3%). By benchmarking LLMs on their ability to produce
correct and executable JQL queries, Jackal exposes the limitations of current
state-of-the-art LLMs and sets a new, execution-based challenge for future
research in Jira enterprise data.

</details>


### [74] [LLM Hallucination Detection: HSAD](https://arxiv.org/abs/2509.23580)
*JinXin Li,Gang Tu,JunJie Hu*

Main category: cs.CL

TL;DR: 本文提出了一种基于隐藏层时序信号频域分析的幻觉检测方法HSAD，通过对大语言模型推理过程中的隐藏层信号进行傅里叶变换提取频谱特征，有效检测生成内容中的幻觉，提高检测准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有幻觉检测方法受限于知识覆盖范围或难以捕捉推理偏差，导致幻觉检测效果不理想，阻碍了大语言模型在关键场景的应用。

Method: 将大语言模型的推理过程视为随时间展开的认知过程，通过隐层时序信号模拟人类欺骗检测中的信号感知和辨识，应用快速傅里叶变换提取频域特征，进而设计基于频谱特征的幻觉检测算法。

Result: 频谱特征分析实验验证了该方法的有效性，HSAD在幻觉检测任务上表现出更高的检测准确性和鲁棒性。

Conclusion: HSAD方法通过结合推理过程建模与频域特征提取，克服了传统方法的知识覆盖和推理偏差检测局限，为大语言模型幻觉检测提供了一种有效且稳健的新路径。

Abstract: Although Large Language Models have demonstrated powerful capabilities in a
wide range of tasks such as language understanding and code generation, the
frequent occurrence of hallucinations during the generation process has become
a significant impediment to their deployment in critical application scenarios.
Current mainstream hallucination detection methods rely on factual consistency
verification or static hidden layer features. The former is constrained by the
scope of knowledge coverage, while the latter struggles to capture reasoning
biases during the inference process. To address these issues, and inspired by
signal analysis methods in cognitive neuroscience, this paper proposes a
hallucination detection method based on the frequency-domain analysis of hidden
layer temporal signals, named HSAD (\textbf{H}idden \textbf{S}ignal
\textbf{A}nalysis-based \textbf{D}etection). First, by treating the LLM's
reasoning process as a cognitive journey that unfolds over time, we propose
modeling and simulating the human process of signal perception and
discrimination in a deception-detection scenario through hidden layer temporal
signals. Next, The Fast Fourier Transform is applied to map these temporal
signals into the frequency domain to construct spectral features, which are
used to capture anomalies that arise during the reasoning process; analysis
experiments on these spectral features have proven the effectiveness of this
approach. Finally, a hallucination detection algorithm is designed based on
these spectral features to identify hallucinations in the generated content. By
effectively combining the modeling of the reasoning process with
frequency-domain feature extraction, the HSAD method overcomes the limitations
of existing approaches in terms of knowledge coverage and the detection of
reasoning biases, demonstrating higher detection accuracy and robustness.

</details>


### [75] [Timber: Training-free Instruct Model Refining with Base via Effective Rank](https://arxiv.org/abs/2509.23595)
*Taiqiang Wu,Runming Yang,Tao Liu,Jiahao Wang,Zenan Xu,Ngai Wong*

Main category: cs.CL

TL;DR: 本文探讨了在后训练阶段Instruct模型表面改进的假设，发现其有效秩变化微乎其微。为解决其探索能力受限问题，提出了无需训练的Timber方法，通过细微调整权重差异部分恢复Base模型特性，从而提升探索能力和保持利用能力。实验证明该方法在Llama和Qwen模型上有效提升了性能。


<details>
  <summary>Details</summary>
Motivation: 后训练阶段常被认为只是表面改进，虽然提升了模型利用能力，但限制了模型的探索能力，存在重要权衡问题，亟需提升Instruct模型的探索能力而不损失其利用能力。

Method: 提出Timber方法，无需重新训练，通过针对性细微调整Instruct模型和基础模型之间的权重差异，部分将Instruct模型权重回退至Base模型，实现提升探索能力。

Result: 在Llama和Qwen系列模型的多项实验结果表明，Timber方法在提升Instruct模型的探索能力的同时，保持了其利用能力，尤其在Pass@k性能指标上表现出持续提升。

Conclusion: Timber方法为Instruct模型的后训练阶段提供了新的视角和有效的无训练细化策略，既深化了对权重级别变化的理解，也解决了探索能力不足的问题，具有实用价值。

Abstract: Post-training, which elicits a pretrained Base model into the corresponding
Instruct model, is widely considered to be superficial. In this work, we first
reinforce this hypothesis by providing novel quantitative evidence from the
weight level that the effective rank (eRank) remains negligibly changed.
However, this superficiality also suffers a critical trade-off, improving the
exploitation capabilities at the cost of limiting its exploration. To tackle
this issue, we propose Timber, a simple yet effective training-free method that
enhances the exploration capability of the Instruct model while preserving its
exploitation. The key insight is to partially revert Instruct towards the
paired Base model by subtle yet targeted refinement of the weight deltas.
Extensive experiments on Llama and Qwen series demonstrate that Timber
consistently improves vanilla Instruct models, particularly on Pass@k
performance. Our findings offer new insights into the post-training stage at
the weight level and practical strategies to refine the Instruct model without
training.

</details>


### [76] [Fast Thinking for Large Language Models](https://arxiv.org/abs/2509.23633)
*Haoyu Zheng,Zhuonan Wang,Yuqian Yuan,Tianwei Lin,Wenqiao Zhang,Zheqi Lv,Juncheng Li,Siliang Tang,Yueting Zhuang,Hongyang He*

Main category: cs.CL

TL;DR: 本文提出了一种名为Fast Thinking的潜代码本方法，通过在训练阶段学习离散策略先验并在推理时使用连续思维向量替代显式推理，提高大型语言模型的推理效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有推理导向的大型语言模型依赖显式逐步生成推理步骤，虽然链式思维技术提升了复杂推理任务的性能，但推理过程冗长且成本高，存在效率瓶颈。

Method: 提出了Fast Thinking框架，利用训练阶段的简洁链式思维草图学习策略先验代码本，在推理时用少量连续思维向量进行策略级指导，避免显式推理步骤的生成。并引入GainRouter轻量级路由机制，自适应切换快速推理和慢速显式推理，抑制过度推理并减少无效token生成。

Result: 在多个推理基准测试中，所提方法在实现竞争性或更优准确度的同时，大幅降低了推理成本和延迟。

Conclusion: 该方法为大型语言模型实现高效且可控的推理提供了实用方案，平衡了准确率和推理效率，具有较大应用潜力。

Abstract: Reasoning-oriented Large Language Models (LLMs) often rely on generating
explicit tokens step by step, and their effectiveness typically hinges on
large-scale supervised fine-tuning or reinforcement learning. While
Chain-of-Thought (CoT) techniques substantially enhance performance on complex
reasoning tasks, they remain inefficient, requiring long reasoning traces that
increase latency and token usage. In this work, we introduce Latent Codebooks
for Fast Thinking, a framework that uses concise CoT sketches only during
training to learn a codebook of discrete strategy priors. At inference, the
model conditions on a handful of continuous thinking vectors distilled from the
codebook in a single pass, enabling strategy-level guidance without producing
explicit reasoning tokens. To complement this design, we propose GainRouter, a
lightweight routing mechanism that adaptively switches between fast codebook
guided inference and slow explicit reasoning, thereby suppressing overthinking
and reducing unnecessary token generation. Experiments across multiple
reasoning benchmarks show that our approach achieves competitive or superior
accuracy while substantially lowering inference cost, offering a practical path
toward efficient and controllable reasoning in large language models.

</details>


### [77] [Don't Settle Too Early: Self-Reflective Remasking for Diffusion Language Models](https://arxiv.org/abs/2509.23653)
*Zemin Huang,Yuhang Wang,Zhiyang Chen,Guo-Jun Qi*

Main category: cs.CL

TL;DR: 本文提出了一种名为RemeDi的掩码扩散语言模型，通过引入重新掩码机制，实现对错误词元的动态改正，提升文本生成质量。


<details>
  <summary>Details</summary>
Motivation: 传统的掩码扩散语言模型在生成后通常无法修改错误的词元，难以改正生成中的错误。

Method: RemeDi在每一步同时预测词元分布和对应的置信度，利用置信度动态决定哪些词元需要重新掩码并重新采样。通过有监督微调和强化学习训练模型检测并重新掩码错误词元，优化生成路径。

Result: RemeDi在多个数据集上实现了开源掩码扩散语言模型的最新性能。

Conclusion: 引入重新掩码机制使掩码扩散语言模型具备更灵活的文本修正能力，显著提升生成效果。

Abstract: Mask-based Diffusion Language Models (DLMs) struggle to revise incorrect
tokens: once a token is generated, it typically remains fixed. The key
challenge is to identify potential errors in the inputs. In this paper, we
propose \emph{\underline{Rem}asking-\underline{e}nabled \underline{Di}ffusion
Language Model (RemeDi}, a mask-based DLM that introduces \emph{remasking} as
another fundamental mechanism, enabling more flexible text refinement in
diffusion-based text generation. To achieve this, RemeDi jointly predicts token
distributions and per-token confidence scores at each step. The confidence
scores determine which tokens to be unmasked after the current step, allowing
the model to identify tokens with low quality and remask them. These remasked
tokens can be resampled with richer context in subsequent steps. We design a
remask-aware pipeline to train this ability, including supervised fine-tuning
which teaches the model to detect and remask incorrect tokens in addition to
predict mask tokens, and reinforcement learning which optimizes full generation
trajectories toward higher rewards. Experiments show that RemeDi achieves the
state-of-the-art results among open-source DLMs on multiple datasets.

</details>


### [78] [Beyond English-Centric Training: How Reinforcement Learning Improves Cross-Lingual Reasoning in LLMs](https://arxiv.org/abs/2509.23657)
*Shulin Huang,Yiran Ding,Junshu Pan,Yue Zhang*

Main category: cs.CL

TL;DR: 本文系统研究了强化学习(RL)与监督微调(SFT)在多语言复杂推理中的跨语言泛化能力，发现RL在准确率和泛化能力上优于SFT，尤其是在非英语数据上训练时表现更佳。


<details>
  <summary>Details</summary>
Motivation: 增强大型语言模型复杂推理能力，同时探索RL对跨语言推理泛化的影响，这在之前研究中尚未被系统研究。

Method: 基于Qwen2.5-3B-Base模型，利用多语言推理基准（数学推理、常识推理、科学推理）进行实验，比较强化学习和监督微调的效果，并通过机制分析探讨原因。

Result: 强化学习不仅提升了准确率，还显著增强了模型的跨语言泛化能力；用非英语数据进行RL训练比用英语数据表现更好，这种现象在SFT中未见。

Conclusion: 强化学习赋予模型更稳健的推理策略，显著提升多语言推理效果，为实现更公平和有效的多语言模型提供了关键指导。

Abstract: Enhancing the complex reasoning capabilities of Large Language Models (LLMs)
attracts widespread attention. While reinforcement learning (RL) has shown
superior performance for improving complex reasoning, its impact on
cross-lingual generalization compared to Supervised Fine-Tuning (SFT) remains
unexplored. We present the first systematic investigation into cross-lingual
reasoning generalization of RL and SFT. Using Qwen2.5-3B-Base as our foundation
model, we conduct experiments on diverse multilingual reasoning benchmarks,
including math reasoning, commonsense reasoning, and scientific reasoning. Our
investigation yields two significant findings: (1) Tuning with RL not only
achieves higher accuracy but also demonstrates substantially stronger
cross-lingual generalization capabilities compared to SFT. (2) RL training on
non-English data yields better overall performance and generalization than
training on English data, which is not observed with SFT. Furthermore, through
comprehensive mechanistic analyses, we explore the underlying factors of RL's
superiority and generalization across languages. Our results provide compelling
evidence that RL enables the model with more robust reasoning strategies,
offering crucial guidance for more equitable and effective multilingual
reasoning.

</details>


### [79] [Aligning LLMs for Multilingual Consistency in Enterprise Applications](https://arxiv.org/abs/2509.23659)
*Amit Agarwal,Hansa Meghwani,Hitesh Laxmichand Patel,Tao Sheng,Sujith Ravi,Dan Roth*

Main category: cs.CL

TL;DR: 本文提出了一种基于批量对齐的细调策略，通过利用多语言语义等效数据，显著提升大语言模型在非英语语言上的性能，解决多语言应用中模型性能差异大的问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在高资源语言（如英语）和中低资源语言之间存在显著性能差距，影响多语言环境下的客户体验和运营可靠性。

Method: 采用批量对齐策略，在每个训练批次中利用语义等效的多语言数据，直接对齐模型输出，以提升非英语语言的表现。

Result: 该方法使非英语语言的准确率最高提升23.9%，且不影响英语性能、模型推理能力和检索质量。

Conclusion: 该方法简单、可扩展，且易于与现有训练和部署流程整合，为工业界提供更稳健公平的多语言AI解决方案。

Abstract: Large language models (LLMs) remain unreliable for global enterprise
applications due to substantial performance gaps between high-resource and
mid/low-resource languages, driven by English-centric pretraining and internal
reasoning biases. This inconsistency undermines customer experience and
operational reliability in multilingual settings such as customer support,
content moderation, and information retrieval. Even with advanced
Retrieval-Augmented Generation (RAG) systems, we observe up to an 29% accuracy
drop in non-English languages compared to English.
  We propose a practical, batch-wise alignment strategy for fine-tuning LLMs,
leveraging semantically equivalent multilingual data in each training batch to
directly align model outputs across languages. This approach improves
non-English accuracy by up to 23.9\% without compromising English performance,
model reasoning, or retrieval quality. Our method is simple to implement,
scalable, and integrates seamlessly with existing LLM training \& deployment
pipelines, enabling more robust and equitable multilingual AI solutions in
industry.

</details>


### [80] [VIVA+: Human-Centered Situational Decision-Making](https://arxiv.org/abs/2509.23698)
*Zhe Hu,Yixiao Ren,Guanzhong Liu,Jing Li,Yu Yin*

Main category: cs.CL

TL;DR: 本文提出了VIVA+基准测试，用于评估多模态大型语言模型在模拟人类决策和推理中的表现，并展示了当前模型的局限性及改进方向。


<details>
  <summary>Details</summary>
Motivation: 多模态大型语言模型在复杂的人类环境中表现出潜力，但缺乏针对细致人类式推理和决策能力的评估手段。

Method: 设计了VIVA+基准，包含1317个真实场景和6373个多项选择题，评测模型的情境理解、行动理由和反思推理三大能力，结合针对性训练和多步推理策略进行验证。

Result: 评测了最新的商业及开源模型，发现其在不同能力上表现存在明显差异，且通过特定训练和多步推理能持续提升性能。

Conclusion: 当前多模态大语言模型在社会认知和复杂决策方面仍存在显著挑战，VIVA+为其发展提供了系统的评估框架和改进方向。

Abstract: Multimodal Large Language Models (MLLMs) show promising results for embodied
agents in operating meaningfully in complex, human-centered environments. Yet,
evaluating their capacity for nuanced, human-like reasoning and decision-making
remains challenging. In this work, we introduce VIVA+, a cognitively grounded
benchmark for evaluating the reasoning and decision-making of MLLMs in
human-centered situations. VIVA+ consists of 1,317 real-world situations paired
with 6,373 multiple-choice questions, targeting three core abilities for
decision-making: (1) Foundational Situation Comprehension, (2) Context-Driven
Action Justification, and (3) Reflective Reasoning. Together, these dimensions
provide a systematic framework for assessing a model's ability to perceive,
reason, and act in socially meaningful ways. We evaluate the latest commercial
and open-source models on VIVA+, where we reveal distinct performance patterns
and highlight significant challenges. We further explore targeted training and
multi-step reasoning strategies, which yield consistent performance
improvements. Finally, our in-depth analysis highlights current model
limitations and provides actionable insights for advancing MLLMs toward more
robust, context-aware, and socially adept decision-making in real-world
settings.

</details>


### [81] [Collaboration of Fusion and Independence: Hypercomplex-driven Robust Multi-Modal Knowledge Graph Completion](https://arxiv.org/abs/2509.23714)
*Zhiqiang Liu,Yichi Zhang,Mengshu Sun,Lei Liang,Wen Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的多模态知识图谱补全方法M-Hyper，结合融合和独立模态表示，利用四元数代数实现多模态间的高效交互，达到目前最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态知识图谱补全方法存在过固定融合策略导致信息丢失或模态独立子模型难捕捉语义交互的不足。

Method: 采用四元数代数，通过FERF和R2MF模块生成三独立模态和一融合模态的表示，映射到双四元数的四个正交基，实现多模态的细粒度交互。

Result: 大量实验表明该方法在性能、鲁棒性和计算效率上均优于现有方法。

Conclusion: M-Hyper成功实现了融合和独立模态的协同表示与交互，为多模态知识图谱补全提供了有效解决方案。

Abstract: Multi-modal knowledge graph completion (MMKGC) aims to discover missing facts
in multi-modal knowledge graphs (MMKGs) by leveraging both structural
relationships and diverse modality information of entities. Existing MMKGC
methods follow two multi-modal paradigms: fusion-based and ensemble-based.
Fusion-based methods employ fixed fusion strategies, which inevitably leads to
the loss of modality-specific information and a lack of flexibility to adapt to
varying modality relevance across contexts. In contrast, ensemble-based methods
retain modality independence through dedicated sub-models but struggle to
capture the nuanced, context-dependent semantic interplay between modalities.
To overcome these dual limitations, we propose a novel MMKGC method M-Hyper,
which achieves the coexistence and collaboration of fused and independent
modality representations. Our method integrates the strengths of both
paradigms, enabling effective cross-modal interactions while maintaining
modality-specific information. Inspired by ``quaternion'' algebra, we utilize
its four orthogonal bases to represent multiple independent modalities and
employ the Hamilton product to efficiently model pair-wise interactions among
them. Specifically, we introduce a Fine-grained Entity Representation
Factorization (FERF) module and a Robust Relation-aware Modality Fusion (R2MF)
module to obtain robust representations for three independent modalities and
one fused modality. The resulting four modality representations are then mapped
to the four orthogonal bases of a biquaternion (a hypercomplex extension of
quaternion) for comprehensive modality interaction. Extensive experiments
indicate its state-of-the-art performance, robustness, and computational
efficiency.

</details>


### [82] [Do LLMs Understand Romanian Driving Laws? A Study on Multimodal and Fine-Tuned Question Answering](https://arxiv.org/abs/2509.23715)
*Eduard Barbu,Adrian Marius Dumitran*

Main category: cs.CL

TL;DR: 本文评估了大型语言模型在罗马尼亚交通法规问答及解释生成中的表现，发布了涵盖1208个问题的数据集，并比较了纯文本与多模态系统的效果，发现微调后模型竞争力强，图像的文本描述优于直接视觉输入，同时引入了模型自身评判解释质量的方法。


<details>
  <summary>Details</summary>
Motivation: 确保新老驾驶员掌握交通规则以保障道路安全，且针对资源相对匮乏的语言环境，评估大型语言模型在交通法规问答中的表现与解释能力。

Method: 发布包含1208个问题（其中387个为多模态）的问答数据集，比较文本与多模态的先进系统表现，对Llama和RoLlama 3.1-8B-Instruct模型进行领域特定微调，利用文本描述辅助图像信息输入，并采用大型语言模型作为评判者评估解释质量。

Result: 先进模型表现良好，微调的8B参数模型表现具有竞争力，图像的文本描述优于直接视觉输入，且基于模型自身的解释质量评判揭示了自我偏好倾向。

Conclusion: 本研究有效推动了针对资源较少语言的可解释问答系统发展，证明了领域微调和文本描述多模态输入的重要性，并提出了基于LLM评判解释质量的新方法。

Abstract: Ensuring that both new and experienced drivers master current traffic rules
is critical to road safety. This paper evaluates Large Language Models (LLMs)
on Romanian driving-law QA with explanation generation. We release a
1{,}208-question dataset (387 multimodal) and compare text-only and multimodal
SOTA systems, then measure the impact of domain-specific fine-tuning for Llama
3.1-8B-Instruct and RoLlama 3.1-8B-Instruct. SOTA models perform well, but
fine-tuned 8B models are competitive. Textual descriptions of images outperform
direct visual input. Finally, an LLM-as-a-Judge assesses explanation quality,
revealing self-preference bias. The study informs explainable QA for
less-resourced languages.

</details>


### [83] [Compose and Fuse: Revisiting the Foundational Bottlenecks in Multimodal Reasoning](https://arxiv.org/abs/2509.23744)
*Yucheng Wang,Yifan Hou,Aydin Javadov,Mubashara Akhtar,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: 该论文研究了多模态大型语言模型（MLLMs）在跨模态推理中的表现，提出了一个基于逻辑的评估框架，发现多模态推理的表现受限于融合和任务组合瓶颈。


<details>
  <summary>Details</summary>
Motivation: 目前多模态推理的效果报告不一，缺乏系统且受控的评估框架来明确哪些情况下多模态输入有利或不利于推理。

Method: 构建了一个基于逻辑的评估框架，将多模态推理分为六种交互模式，分析不同模态事实的分布和逻辑组合，并结合模型内在机制探讨失败原因。

Result: 多模态输入仅在提供独立且充分的推理路径时增强推理，冗余或链式蕴含常降低表现；发现弱模态拖累整体表现，模态冲突导致偏好，融合信号整合失败。并确认存在任务组合瓶颈和融合瓶颈。

Conclusion: 多模态推理的主要障碍是信息融合而非感知，建议采用任务组合意识训练和早期融合控制以提升多模态推理性能。

Abstract: Multimodal large language models (MLLMs) promise enhanced reasoning by
integrating diverse inputs such as text, vision, and audio. Yet cross-modal
reasoning remains underexplored, with conflicting reports on whether added
modalities help or harm performance. These inconsistencies stem from a lack of
controlled evaluation frameworks and analysis of models' internals to isolate
when and why modality interactions support or undermine reasoning. We address
this gap through a logic-grounded evaluation framework that categorizes
multimodal reasoning into six interaction patterns, varying how facts are
distributed across modalities and logically combined. Empirically, additional
modalities enhance reasoning only when they provide independent and sufficient
reasoning paths, while redundant or chained entailment support often hurts
performance. Moreover, reasoning degrades in three systematic ways: weaker
modalities drag down overall performance, conflicts bias preference toward
certain modalities, and joint signals from different modalities fail to be
integrated effectively. Therefore, we identify two core failures:
task-composition bottleneck, where recognition and reasoning cannot be jointly
executed in one pass, and fusion bottleneck, where early integration introduces
bias. For further investigation, we find that attention patterns fail to encode
fact usefulness, but a simple two-step prompting (recognize then reason)
restores performance, confirming the task-composition bottleneck. Moreover,
modality identity remains recoverable in early layers, and softening attention
in early fusion improves reasoning, highlighting biased fusion as another
failure mode. Overall, our findings show that integration, not perception, is
the main barrier to multimodal reasoning, suggesting composition-aware training
and early fusion control as promising directions.

</details>


### [84] [Understanding Textual Capability Degradation in Speech LLMs via Parameter Importance Analysis](https://arxiv.org/abs/2509.23755)
*Chao Wang,Rui-Chen Zheng,Yang Ai,Zhen-Hua Ling*

Main category: cs.CL

TL;DR: 本文分析了语音集成到大型语言模型中导致文本能力下降的问题，提出了基于参数重要性估计的分析框架。


<details>
  <summary>Details</summary>
Motivation: 语音集成虽然扩展了大型语言模型的功能，但削弱了其核心文本理解能力，限制了其利用预训练文本知识的能力。

Method: 研究了编码器-适配器范式，通过参数重要性估计揭示了文本重要性分布的层级变化；提出层级学习率调度和低秩适配（LoRA）两种策略来缓解该问题。

Result: 实验表明，提出的方法在保持文本能力的同时提升了语音问答的表现，效果优于全量微调。

Conclusion: 分析证明维护参数的重要性分布是保持文本能力的关键，所提策略有效利用了文本知识的结构性质，提升了语音增强大型语言模型的综合表现。

Abstract: The integration of speech into Large Language Models (LLMs) has substantially
expanded their capabilities, but often at the cost of weakening their core
textual competence. This degradation limits the ability of speech-enabled LLMs
to fully exploit their pre-trained text-based knowledge. In this work, we
analyze the underlying mechanisms of this issue through a focused study of the
widely used encoder-adaptor paradigm. We propose an analytical framework based
on parameter importance estimation, which reveals that fine-tuning for speech
introduces a textual importance distribution shift: the layer-wise allocation
of parameters critical to textual reasoning is disrupted. Building on this
insight, we investigate two mitigation strategies: layer-wise learning rate
scheduling and Low-Rank Adaptation (LoRA), both aim to preserve the original
parameter distribution. Experimental results show that both approaches better
maintain textual competence than full fine-tuning, while also improving
downstream spoken question answering performance. Furthermore, our analysis
offers a principled explanation for the effectiveness of the proposed
mitigation strategies, linking their benefits to the structural properties of
textual knowledge in LLMs.

</details>


### [85] [Knowledge-Level Consistency Reinforcement Learning: Dual-Fact Alignment for Long-Form Factuality](https://arxiv.org/abs/2509.23765)
*Junliang Li,Yucheng Wang,Yan Chen,Yu Ran,Ruiqing Zhang,Jing Liu,Hua Wu,Haifeng Wang*

Main category: cs.CL

TL;DR: 该论文提出了一种新的强化学习框架KLCF，通过知识一致性和事实对齐机制，有效提升大型语言模型长文本生成的真实性，减少虚假信息。


<details>
  <summary>Details</summary>
Motivation: 现有基于人类反馈的强化学习方法主要依赖偏好奖励，忽视了模型内部知识边界，导致模型生成虚假信息问题（“幻觉税”）加重。

Method: 提出知识层面一致性强化学习框架（KLCF），利用预训练知识边界构建事实清单，结合双事实对齐机制，一方面提升事实覆盖和召回，另一方面通过模型自评模块增强事实精度。该方法无需外部知识检索，轻量高效。

Result: 实验表明，KLCF在多个长文本基准测试中显著提升了事实准确性指标，有效减少了模型幻觉现象。

Conclusion: KLCF框架通过内部知识一致性监督和双重事实对齐机制，解决了LLM长文本生成中的事实性问题，具备高效、可扩展优势，促进了生成文本的可靠性。

Abstract: Hallucination and factuality deficits remain key obstacles to the reliability
of large language models (LLMs) in long-form generation. Existing reinforcement
learning from human feedback (RLHF) frameworks primarily rely on preference
rewards, yet they often overlook the model's internal knowledge boundaries,
exacerbating the so-called "hallucination tax". To address this challenge, we
propose Knowledge-Level Consistency Reinforcement Learning Framework (KLCF), a
novel framework that focuses on the knowledge consistency between the policy
model's expressed knowledge and the base model's parametric knowledge, and
introduces a Dual-Fact Alignment mechanism to jointly optimize factual recall
and precision. Specifically, KLCF leverages pretrained knowledge boundaries to
construct fact checklist, guiding online reinforcement learning to improve
factual coverage and recall; simultaneously, it trains a self-assessment module
based on the base model's internal knowledge to enhance factual precision
during generation. Unlike prior methods that rely on external retrieval or
heavy verification, our reward design is fully external-knowledge-free and
lightweight, making KLCF efficient and easily scalable to large-scale training.
Experimental results demonstrate that KLCF substantially improves factuality
metrics across multiple long-form benchmarks and effectively alleviates model
hallucinations.

</details>


### [86] [From Personal to Collective: On the Role of Local and Global Memory in LLM Personalization](https://arxiv.org/abs/2509.23767)
*Zehong Wang,Junlin Wu,ZHaoxuan Tan,Bolian Li,Xianrui Zhong,Zheli Liu,Qingkai Zeng*

Main category: cs.CL

TL;DR: 该论文提出了一个本地-全局记忆框架（LoGo），通过结合个性化的本地记忆和捕捉群体共享兴趣的全局记忆，解决了大语言模型个性化中的冷启动和偏差问题。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型个性化中用户历史数据不足导致的冷启动问题，以及历史数据过于偏向导致的模型过拟合问题。

Method: 提出LoGo框架，将个性化的本地记忆与捕捉群体共享兴趣的全局记忆结合，设计中介模块调和两者之间的冲突。

Result: 在多个基准测试中，LoGo框架显著提升了个性化质量，改善了冷启动用户表现，并减轻了偏差预测。

Conclusion: 引入群体知识对于提升大语言模型的个性化效果至关重要，LoGo框架有效解决了个性化中的关键挑战。

Abstract: Large language model (LLM) personalization aims to tailor model behavior to
individual users based on their historical interactions. However, its
effectiveness is often hindered by two key challenges: the \textit{cold-start
problem}, where users with limited history provide insufficient context for
accurate personalization, and the \textit{biasing problem}, where users with
abundant but skewed history cause the model to overfit to narrow preferences.
We identify both issues as symptoms of a common underlying limitation, i.e.,
the inability to model collective knowledge across users. To address this, we
propose a local-global memory framework (LoGo) that combines the personalized
local memory with a collective global memory that captures shared interests
across the population. To reconcile discrepancies between these two memory
sources, we introduce a mediator module designed to resolve conflicts between
local and global signals. Extensive experiments on multiple benchmarks
demonstrate that LoGo consistently improves personalization quality by both
warming up cold-start users and mitigating biased predictions. These results
highlight the importance of incorporating collective knowledge to enhance LLM
personalization.

</details>


### [87] [Bridging the Knowledge-Prediction Gap in LLMs on Multiple-Choice Questions](https://arxiv.org/abs/2509.23782)
*Yoonah Park,Haesung Pyun,Yohan Jo*

Main category: cs.CL

TL;DR: 本文发现大型语言模型在选择题与自由生成中表现出知识预测差异，通过调整隐藏状态中的知识基础和预测基础的对齐，提升了选择题答题准确率。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在多项选择题上尽管具备正确知识，但预测表现不佳的机制，并寻求解决方法。

Method: 通过探测隐藏状态中的知识基础和预测基础子空间，提出参数无关的KAPPA方法，将预测坐标调整与知识坐标对齐，从而改善模型预测。

Result: KAPPA在多个数据集上的选择题任务中显著提升准确率，优于基线方法，并且在跨数据集和自由问答任务中也表现有效。

Conclusion: 提出了一种新的几何视角理解知识-预测差距，并提供了一种实用的无参调整方法KAPPA，实现模型行为与潜在知识的更好对齐。

Abstract: Large Language Models (LLMs) often fail on multiple-choice questions (MCQs)
despite demonstrating correct knowledge in other contexts, such as free-form
generation. To investigate the mechanism underlying this knowledge-prediction
gap on MCQs and alleviate it, we conduct a probing analysis and find that
residual streams in certain layers contain a subspace spanned by two important
bases: a \emph{knowledge basis} that encodes the probability of the
ground-truth answer for a given MCQ and a \emph{prediction basis} that encodes
the probability of the answer choice predicted by the model. We observe that
incorrect predictions arise from a misalignment of the model's hidden states
along these two bases. Hence, we introduce \textbf{KAPPA} (Knowledge-Aligned
Prediction through Projection-based Adjustment), a parameter-free intervention
that transforms the hidden states to align the prediction coordinate with the
knowledge coordinate within this subspace. Experiments on binary-choice
reformulations of Big-Bench-Hard and ARC-Challenge show that KAPPA
substantially improves accuracy and consistently outperforms baselines. While
optimal subspaces differ across tasks, subspaces generalize to some extent, as
supported by cross-dataset experiments. Moreover, KAPPA extends its
effectiveness to free-form questions beyond MCQs. Our work provides a new
geometric understanding of the knowledge-prediction gap and offers a practical
method for better aligning model behavior with its latent knowledge.

</details>


### [88] [Transformer Tafsir at QIAS 2025 Shared Task: Hybrid Retrieval-Augmented Generation for Islamic Knowledge Question Answering](https://arxiv.org/abs/2509.23793)
*Muhammad Abu Ahmad,Mohamad Ballout,Raia Abu Ahmad,Elia Bruni*

Main category: cs.CL

TL;DR: 该论文提出了一种混合检索增强生成系统，通过结合稀疏和稠密检索以及交叉编码器重排序，提高大语言模型在伊斯兰知识理解和推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 提升大语言模型在伊斯兰知识理解和推理任务中的准确率，解决单一检索方法的局限性。

Method: 基于三阶段管线，初始使用BM25进行稀疏检索，接着应用稠密嵌入模型进行语义匹配，最后用交叉编码器进行重排序，以增强检索精度。

Result: 在使用Fanar和Mistral两种大语言模型的两项子任务测试中，该方法性能提升显著，最高准确率提升达25%；最佳配置下，Fanar模型在子任务1和子任务2中准确率分别达到45%和80%。

Conclusion: 结合多种检索技术的混合RAG系统显著提高了大语言模型在伊斯兰知识理解与推理任务的性能，证明了该方法的有效性。

Abstract: This paper presents our submission to the QIAS 2025 shared task on Islamic
knowledge understanding and reasoning. We developed a hybrid
retrieval-augmented generation (RAG) system that combines sparse and dense
retrieval methods with cross-encoder reranking to improve large language model
(LLM) performance. Our three-stage pipeline incorporates BM25 for initial
retrieval, a dense embedding retrieval model for semantic matching, and
cross-encoder reranking for precise content retrieval. We evaluate our approach
on both subtasks using two LLMs, Fanar and Mistral, demonstrating that the
proposed RAG pipeline enhances performance across both, with accuracy
improvements up to 25%, depending on the task and model configuration. Our best
configuration is achieved with Fanar, yielding accuracy scores of 45% in
Subtask 1 and 80% in Subtask 2.

</details>


### [89] [Open-DeBias: Toward Mitigating Open-Set Bias in Language Models](https://arxiv.org/abs/2509.23805)
*Arti Rani,Shweta Singh,Nihar Ranjan Sahoo,Gaurav Kumar Nayak*

Main category: cs.CL

TL;DR: 本文提出了OpenBiasBench和Open-DeBias方法，针对文本问答中的开放集合偏见进行检测和缓解，显著提升了模型问答准确率和多语言泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有偏见缓解方法多局限于预定义类别，难以应对新兴或特定语境的开放集合偏见问题。

Method: 引入全面的偏见评测基准OpenBiasBench，并设计高效的适配器模块Open-DeBias，通过少量训练数据微调实现有效偏见缓解。

Result: Open-DeBias在BBQ等数据集上显著提升准确率，且在韩语零样本转移中表现优异，证明了其语言无关的泛化能力。

Conclusion: Open-DeBias展现了鲁棒性、多语言支持及通用开放域偏见缓解能力，适用于广泛NLP任务，推动偏见检测和缓解研究的发展。

Abstract: Large Language Models (LLMs) have achieved remarkable success on question
answering (QA) tasks, yet they often encode harmful biases that compromise
fairness and trustworthiness. Most existing bias mitigation approaches are
restricted to predefined categories, limiting their ability to address novel or
context-specific emergent biases. To bridge this gap, we tackle the novel
problem of open-set bias detection and mitigation in text-based QA. We
introduce OpenBiasBench, a comprehensive benchmark designed to evaluate biases
across a wide range of categories and subgroups, encompassing both known and
previously unseen biases. Additionally, we propose Open-DeBias, a novel,
data-efficient, and parameter-efficient debiasing method that leverages adapter
modules to mitigate existing social and stereotypical biases while generalizing
to unseen ones. Compared to the state-of-the-art BMBI method, Open-DeBias
improves QA accuracy on BBQ dataset by nearly $48\%$ on ambiguous subsets and
$6\%$ on disambiguated ones, using adapters fine-tuned on just a small fraction
of the training data. Remarkably, the same adapters, in a zero-shot transfer to
Korean BBQ, achieve $84\%$ accuracy, demonstrating robust language-agnostic
generalization. Through extensive evaluation, we also validate the
effectiveness of Open-DeBias across a broad range of NLP tasks, including
StereoSet and CrowS-Pairs, highlighting its robustness, multilingual strength,
and suitability for general-purpose, open-domain bias mitigation. The project
page is available at: https://sites.google.com/view/open-debias25

</details>


### [90] [SPELL: Self-Play Reinforcement Learning for evolving Long-Context Language Models](https://arxiv.org/abs/2509.23863)
*Ziyi Yang,Weizhou Shen,Ruijun Chen,Chenliang Li,Fanqi Wan,Ming Yan,Xiaojun Quan,Fei Huang*

Main category: cs.CL

TL;DR: 提出SPELL，一种多角色自我对弈强化学习框架，实现大语言模型的无标签长上下文推理优化。


<details>
  <summary>Details</summary>
Motivation: 长上下文推理在大语言模型中的进展落后，原因包括处理长文本的难度以及缺乏可靠的人类标注和可编程的奖励信号。

Method: SPELL集成了提问者、回答者和验证者三个角色于一个模型中，循环自我训练。提问者从文档生成问题及参考答案，回答者基于文档回答问题，验证者评估回答与参考答案的语义等价性，产生奖励信号指导训练。加入自动课程学习和动态难度适应的奖励函数稳定训练。

Result: 在六个长上下文基准测试中，SPELL提升了多种大模型的表现，超越同等规模的有监督微调模型。对Qwen3-30B-A3B-Thinking模型，pass@8指标提升7.6点，显著提升性能上限。

Conclusion: SPELL框架有效解决了长上下文推理中的训练信号缺乏问题，实现了无标签自我优化，提升了大语言模型的推理能力，有望推广到更大规模模型。

Abstract: Progress in long-context reasoning for large language models (LLMs) has
lagged behind other recent advances. This gap arises not only from the
intrinsic difficulty of processing long texts, but also from the scarcity of
reliable human annotations and programmatically verifiable reward signals. In
this paper, we propose SPELL, a multi-role self-play reinforcement learning
framework that enables scalable, label-free optimization for long-context
reasoning. SPELL integrates three cyclical roles-questioner, responder, and
verifier-within a single model to enable continual self-improvement. The
questioner generates questions from raw documents paired with reference
answers; the responder learns to solve these questions based on the documents;
and the verifier evaluates semantic equivalence between the responder's output
and the questioner's reference answer, producing reward signals to guide
continual training. To stabilize training, we introduce an automated curriculum
that gradually increases document length and a reward function that adapts
question difficulty to the model's evolving capabilities. Extensive experiments
on six long-context benchmarks show that SPELL consistently improves
performance across diverse LLMs and outperforms equally sized models fine-tuned
on large-scale annotated data. Notably, SPELL achieves an average 7.6-point
gain in pass@8 on the strong reasoning model Qwen3-30B-A3B-Thinking, raising
its performance ceiling and showing promise for scaling to even more capable
models.

</details>


### [91] [Winning the Pruning Gamble: A Unified Approach to Joint Sample and Token Pruning for Efficient Supervised Fine-Tuning](https://arxiv.org/abs/2509.23873)
*Shaobo Wang,Jiaming Wang,Jiajun Zhang,Cong Wang,Yue Min,Zichen Wen,Fei Huang,Huiqiang Jiang,Junyang Lin,Dayiheng Liu,Linfeng Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种名为Q-Tuning的统一数据剪枝框架，通过协同优化样本和token两个层面，提高大语言模型监督微调的数据利用率，在减少训练数据量的同时显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 监督微调从轻量级步骤演变为计算密集阶段，现有的数据剪枝方法仅在样本或token层面单独操作，导致数据利用效率低下。

Method: 提出误差-不确定性平面（EU Plane）诊断框架，联合评估样本和token的重要性；基于此设计Q-Tuning，包含样本层的筛选和token层的非对称剪枝策略，重点保留校准类样本并对误解样本进行选择性token裁剪。

Result: Q-Tuning在五个不同基准测试中表现领先，SmolLM2-1.7B模型上，仅用原始训练数据12.5%就实现了比全数据训练高38%的性能提升。

Conclusion: Q-Tuning为预算有限的大语言模型监督微调提供了一种实用且可扩展的数据使用优化方案，是首个动态剪枝方法优于全数据训练的案例。

Abstract: As supervised fine-tuning (SFT) evolves from a lightweight post-training step
into a compute-intensive phase rivaling mid-training in scale, data efficiency
has become critical for aligning large language models (LLMs) under tight
budgets. Existing data pruning methods suffer from a fragmented design: they
operate either at the sample level or the token level in isolation, failing to
jointly optimize both dimensions. This disconnect leads to significant
inefficiencies--high-value samples may still contain redundant tokens, while
token-level pruning often discards crucial instructional or corrective signals
embedded in individual examples. To address this bottleneck, we introduce the
Error-Uncertainty (EU) Plane, a diagnostic framework that jointly characterizes
the heterogeneous utility of training data across samples and tokens. Guided by
this insight, we propose Quadrant-based Tuning (Q-Tuning), a unified framework
that strategically coordinates sample pruning and token pruning. Q-Tuning
employs a two-stage strategy: first, it performs sample-level triage to retain
examples rich in informative misconceptions or calibration signals; second, it
applies an asymmetric token-pruning policy, using a context-aware scoring
mechanism to trim less salient tokens exclusively from misconception samples
while preserving calibration samples in their entirety. Our method sets a new
state of the art across five diverse benchmarks. Remarkably, on SmolLM2-1.7B,
Q-Tuning achieves a +38\% average improvement over the full-data SFT baseline
using only 12.5\% of the original training data. As the first dynamic pruning
approach to consistently outperform full-data training, Q-Tuning provides a
practical and scalable blueprint for maximizing data utilization in
budget-constrained LLM SFT.

</details>


### [92] [DocPruner: A Storage-Efficient Framework for Multi-Vector Visual Document Retrieval via Adaptive Patch-Level Embedding Pruning](https://arxiv.org/abs/2509.23883)
*Yibo Yan,Guangwei Xu,Xin Zou,Shuliang Liu,James Kwok,Xuming Hu*

Main category: cs.CL

TL;DR: 本文提出DocPruner，通过自适应剪枝文档的patch级嵌入向量，有效减少Visual Document Retrieval系统的存储开销。


<details>
  <summary>Details</summary>
Motivation: 当前采用多向量表示文档的方法虽然细粒度表达能力强，但导致存储开销极大，不利于大规模部署。

Method: DocPruner利用文档内部patch的注意力分布动态识别并剪除冗余嵌入，减少存储需求。

Result: 在保持检索性能基本不变的情况下，DocPruner实现了领先多向量VDR模型50-60%的存储压缩率。

Conclusion: DocPruner是首个针对VDR的patch级嵌入自适应剪枝框架，验证了其存储高效且性能稳健的优势，适合大规模系统应用。

Abstract: Visual Document Retrieval (VDR), the task of retrieving visually-rich
document pages using queries that combine visual and textual cues, is crucial
for numerous real-world applications. Recent state-of-the-art methods leverage
Large Vision-Language Models (LVLMs) in a multi-vector paradigm, representing
each document as patch-level embeddings to capture fine-grained details. While
highly effective, this approach introduces a critical challenge: prohibitive
storage overhead, as storing hundreds of vectors per page makes large-scale
deployment costly and impractical. To address this, we introduce DocPruner, the
first framework to employ adaptive patch-level embedding pruning for VDR to
effectively reduce the storage overhead. DocPruner leverages the intra-document
patch attention distribution to dynamically identify and discard redundant
embeddings for each document. This adaptive mechanism enables a significant
50-60% reduction in storage for leading multi-vector VDR models with negligible
degradation in document retrieval performance. Extensive experiments across
more than ten representative datasets validate that DocPruner offers a robust,
flexible, and effective solution for building storage-efficient, large-scale
VDR systems.

</details>


### [93] [Taming Masked Diffusion Language Models via Consistency Trajectory Reinforcement Learning with Fewer Decoding Step](https://arxiv.org/abs/2509.23924)
*Jingyi Yang,Guanxu Chen,Xuhao Hu,Jing Shao*

Main category: cs.CL

TL;DR: 本文针对掩码扩散语言模型（MDLMs）的解码策略和强化学习算法进行了研究，提出了EOSER、ASS解码调度器和CJ-GRPO优化方法，提升了全扩散式解码性能及训练推理一致性。


<details>
  <summary>Details</summary>
Motivation: 现有针对自回归模型设计的解码策略和强化学习算法直接应用于MDLMs存在训练推理不一致和性能不优的问题。

Method: 提出EOSER和ASS解码调度器实现高效的全扩散式解码，设计CJ-GRPO算法强调训练推理轨迹一致性，以减少优化误差。

Result: 在数学和规划推理任务中，使用LLaDA-8B-Instruct模型验证，所提方法在减少解码步骤的同时实现了较优性能表现。

Conclusion: EOSER、ASS和CJ-GRPO方法有效提升了MDLMs的解码效率和强化学习训练一致性，展现出较强的实际应用潜力。

Abstract: Masked diffusion language models (MDLMs) have recently emerged as a promising
alternative to autoregressive (AR) language models, offering properties such as
parallel decoding, flexible generation orders, and the potential for fewer
inference steps. Despite these advantages, decoding strategies and
reinforcement learning (RL) algorithms tailored for MDLMs remain underexplored.
A naive approach is to directly transfer techniques well-established for AR
models to MDLMs. However, this raises an immediate question: Is such a naive
transfer truly optimal? For example, 1) Block-wise and semi-AR decoding
strategies are not employed during the training of MDLMs, so why do they
outperform full diffusion-style decoding during inference? 2) Applying RL
algorithms designed for AR models directly to MDLMs exhibits a
training-inference inconsistency, since MDLM decoding are non-causal
(parallel). This results in inconsistencies between the rollout trajectory and
the optimization trajectory. To address these challenges, we propose EOS Early
Rejection (EOSER) and Ascending Step-Size (ASS) decoding scheduler, which
unlock the potential of MDLMs to perform full diffusion-style decoding,
achieving competitive performance with fewer decoding steps. Additionally, we
introduce Consistency Trajectory Group Relative Policy Optimization (CJ-GRPO)
for taming MDLMs, which emphasizes the consistency between rollout trajectory
and optimization trajectory, and reduces the optimization errors caused by
skip-step optimization. We conduct extensive experiments on reasoning tasks,
such as mathematical and planning benchmarks, using LLaDA-8B-Instruct. The
results demonstrate that the proposed EOSER and ASS mechanisms, together with
CJ-GRPO, hold significant promise for effectively and efficiently taming MDLMs.
Code: https://github.com/yjyddq/EOSER-ASS-RL.

</details>


### [94] [Assessing Large Language Models in Updating Their Forecasts with New Information](https://arxiv.org/abs/2509.23936)
*Zhangdie Yuan,Zifeng Ding,Andreas Vlachos*

Main category: cs.CL

TL;DR: 提出了EVOLVECAST框架，评估大语言模型在接收新信息后是否适当调整预测和置信度。发现模型调整往往不一致或过于保守，置信度估计远不及人类。


<details>
  <summary>Details</summary>
Motivation: 现有工作将未来事件预测视为静态任务，未考虑预测和置信度随新证据变化的动态调整。

Method: 设计EVOLVECAST框架，比较大语言模型和人类预测者在接收训练截止后发布的信息时的预测变化和置信度校准。

Result: 大语言模型对新信息有一定响应，但调整不一致且过于保守，置信度估计不稳定且远低于人类水平。

Conclusion: 模型表现出保守偏差，表明需要更稳健的信念更新方法，以提升模型动态调整预测的能力。

Abstract: Prior work has largely treated future event prediction as a static task,
failing to consider how forecasts and the confidence in them should evolve as
new evidence emerges. To address this gap, we introduce EVOLVECAST, a framework
for evaluating whether large language models appropriately revise their
predictions in response to new information. In particular, EVOLVECAST assesses
whether LLMs adjust their forecasts when presented with information released
after their training cutoff. We use human forecasters as a comparative
reference to analyze prediction shifts and confidence calibration under updated
contexts. While LLMs demonstrate some responsiveness to new information, their
updates are often inconsistent or overly conservative. We further find that
neither verbalized nor logits-based confidence estimates consistently
outperform the other, and both remain far from the human reference standard.
Across settings, models tend to express conservative bias, underscoring the
need for more robust approaches to belief updating.

</details>


### [95] [Easy Turn: Integrating Acoustic and Linguistic Modalities for Robust Turn-Taking in Full-Duplex Spoken Dialogue Systems](https://arxiv.org/abs/2509.23938)
*Guojian Li,Chengyou Wang,Hongfei Xue,Shuiyuan Wang,Dehui Gao,Zihan Zhang,Yuke Lin,Wenjie Li,Longshuai Xiao,Zhonghua Fu,Lei Xie*

Main category: cs.CL

TL;DR: 本文提出了一种开源的多模态对话转接检测模型Easy Turn，并发布了1155小时语音数据集，显著提升了转接检测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有全双工人机交互的转接检测多依赖专有模型，数据匮乏且多模态支持有限，缺乏开源且高效的解决方案。

Method: 提出Easy Turn模型，融合语音和语言两种模态，预测四种对话转接状态；同时发布大规模开源训练数据集Easy Turn trainset。

Result: Easy Turn模型在开源测试集上优于TEN Turn Detection和Smart Turn V2等现有开源模型，达成了先进的转接检测准确率。

Conclusion: Easy Turn提供了开源、多模态且高性能的对话转接检测方案，有助于推动自然人机全双工交互的发展。

Abstract: Full-duplex interaction is crucial for natural human-machine communication,
yet remains challenging as it requires robust turn-taking detection to decide
when the system should speak, listen, or remain silent. Existing solutions
either rely on dedicated turn-taking models, most of which are not
open-sourced. The few available ones are limited by their large parameter size
or by supporting only a single modality, such as acoustic or linguistic.
Alternatively, some approaches finetune LLM backbones to enable full-duplex
capability, but this requires large amounts of full-duplex data, which remain
scarce in open-source form. To address these issues, we propose Easy Turn, an
open-source, modular turn-taking detection model that integrates acoustic and
linguistic bimodal information to predict four dialogue turn states: complete,
incomplete, backchannel, and wait, accompanied by the release of Easy Turn
trainset, a 1,145-hour speech dataset designed for training turn-taking
detection models. Compared to existing open-source models like TEN Turn
Detection and Smart Turn V2, our model achieves state-of-the-art turn-taking
detection accuracy on our open-source Easy Turn testset. The data and model
will be made publicly available on GitHub.

</details>


### [96] [Vision-Grounded Machine Interpreting: Improving the Translation Process through Visual Cues](https://arxiv.org/abs/2509.23957)
*Claudio Fantinuoli*

Main category: cs.CL

TL;DR: 该论文提出了一种结合视觉信息的多模态机器口译系统，通过视觉线索辅助语音翻译，提高了词汇消歧能力，促进了翻译质量提升。


<details>
  <summary>Details</summary>
Motivation: 现有机器口译系统仅依赖语音信号，难以处理需要视觉、情境等多模态信息辅助才能准确翻译的歧义，限制了性能表现。

Method: 设计Vision-Grounded Interpreting (VGI)系统，将视觉语言模型与实时语音输入结合，通过摄像头捕获视觉信息，利用视觉上下文辅助翻译过程。同时构建诊断语料库，评估视觉信息在词汇歧义、性别解析及句法歧义中的作用。

Result: 视觉信息显著提升了词汇歧义消解效果，对性别解析有较小且不稳定的提升，而对句法歧义没有明显帮助。

Conclusion: 融入多模态信息，特别是视觉线索，是提升机器口译翻译质量的必要方向。

Abstract: Machine Interpreting systems are currently implemented as unimodal, real-time
speech-to-speech architectures, processing translation exclusively on the basis
of the linguistic signal. Such reliance on a single modality, however,
constrains performance in contexts where disambiguation and adequacy depend on
additional cues, such as visual, situational, or pragmatic information. This
paper introduces Vision-Grounded Interpreting (VGI), a novel approach designed
to address the limitations of unimodal machine interpreting. We present a
prototype system that integrates a vision-language model to process both speech
and visual input from a webcam, with the aim of priming the translation process
through contextual visual information. To evaluate the effectiveness of this
approach, we constructed a hand-crafted diagnostic corpus targeting three types
of ambiguity. In our evaluation, visual grounding substantially improves
lexical disambiguation, yields modest and less stable gains for gender
resolution, and shows no benefit for syntactic ambiguities. We argue that
embracing multimodality represents a necessary step forward for advancing
translation quality in machine interpreting.

</details>


### [97] [HiPO: Hybrid Policy Optimization for Dynamic Reasoning in LLMs](https://arxiv.org/abs/2509.23967)
*Ken Deng,Zizheng Zhan,Wen Xiang,Wenqiang Zhu,Tianhao Peng,Xinping Lei,Weihao Li,Jingxuan Xu,Kun Wu,Yifan Yao,Haoyang Huang,Huaixi Tang,Kepeng Lei,Zhiyi Lai,Songwei Yu,Zongxian Feng,Zuchen Gao,Weihao Xie,Chenchen Zhang,Yanan Wu,Yuanxing Zhang,Lecheng Huang,Yuqun Zhang,Jie Liu,Zhaoxiang Zhang,Haotian Zhang,Bin Chen,Jiaheng Liu*

Main category: cs.CL

TL;DR: 提出了一种名为HiPO的自适应推理控制框架，使大型语言模型能够在详细推理和直接回应之间选择，以提高推理效率并降低成本。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在复杂任务中依赖连锁思维推理来提高准确性，但生成冗长推理过程效率低下，导致令牌使用过多和推理成本高。

Method: HiPO结合混合数据管道（提供成对的详细推理和直接回应）和混合强化学习奖励系统，平衡准确性与效率，避免过度依赖详细推理。

Result: 在数学和编程基准测试中，HiPO显著减少了令牌长度，同时保持或提升了准确率。

Conclusion: HiPO为高效的自适应推理提供了原则性方法，有助于推理型大型语言模型在资源敏感的实际应用中部署。

Abstract: Large Language Models (LLMs) increasingly rely on chain-of-thought (CoT)
reasoning to improve accuracy on complex tasks. However, always generating
lengthy reasoning traces is inefficient, leading to excessive token usage and
higher inference costs. This paper introduces the Hybrid Policy Optimization
(i.e., HiPO), a framework for adaptive reasoning control that enables LLMs to
selectively decide when to engage in detailed reasoning (Think-on) and when to
respond directly (Think-off). Specifically, HiPO combines a hybrid data
pipelineproviding paired Think-on and Think-off responseswith a hybrid
reinforcement learning reward system that balances accuracy and efficiency
while avoiding over-reliance on detailed reasoning. Experiments across
mathematics and coding benchmarks demonstrate that HiPO can substantially
reduce token length while maintaining or improving accuracy. Finally, we hope
HiPO a can be a principled approach for efficient adaptive reasoning, advancing
the deployment of reasoning-oriented LLMs in real-world, resource-sensitive
settings.

</details>


### [98] [ByteSized32Refactored: Towards an Extensible Interactive Text Games Corpus for LLM World Modeling and Evaluation](https://arxiv.org/abs/2509.23979)
*Haonan Wang,Junfeng Sun,Xingdi Yuan,Ruoyao Wang,Ziang Xiao*

Main category: cs.CL

TL;DR: 本文提出了ByteSized32Refactored，一个针对文本游戏生成任务的模块化重构实现，优化了原ByteSized32的代码结构，实现了代码行数减半和功能集中，提升了扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决原ByteSized32代码复杂且不易扩展的问题，方便对文本游戏生成任务的研究及扩展。

Method: 通过抽象七个基础类，集中通用逻辑于一个基础库GameBasic.py，实现代码模块化和结构优化。

Result: 使代码行数从2万减少到1万，并在GPT-4o上对未见场景的文本游戏生成进行了实验，部分评测指标有所提升，部分指标下降。

Conclusion: 该模块化、中心化设计不仅提升了代码扩展性，也为LLM适应环境规格提供了支持，构建了一个可持续扩展的文本游戏生成环境。

Abstract: Simulating interactive world models remains a core challenge in Large
Language Models(LLMs). In this work, we introduce the ByteSized32Refactored, a
refactored, modular, and extensible implementation of the original ByteSized32
corpus to explore the task of text game generation. We further optimize the
code structure of each text game and create the GameBasic.py foundation
library, which centralizes common logic across all 32 games by abstracting 7
base classes (GameObject, etc.) into reusable modules, thereby reducing from
20k to 10k total lines of Python code compared to the original Bytesized32. Our
refactored implementation enables extendability - with our centralized design,
ByteSized32Refactored can be more efficiently extended to include text games of
new scenarios and specifications by reusing the shared logic and
functionalities. Extensive experiments with GPT-4o demonstrate a mix of
performance - with Bytesized32Refactored, the generated text games for unseen
scenarios showcase quality improvements on two of the four evaluation
dimensions while decreases on the other two, indicating that the hierarchical
structure of the refactored code presents new challenges for LLMs. Overall, we
highlight that our extensible code structure, centered on the foundation
library and the modular optimization, not only facilitates LLM adaptation to
environment specifications but also establishes a scalable environment that
supports future extensions.

</details>


### [99] [Toward Preference-aligned Large Language Models via Residual-based Model Steering](https://arxiv.org/abs/2509.23982)
*Lucio La Cava,Andrea Tagarelli*

Main category: cs.CL

TL;DR: 本文提出了一种名为PaLRS的训练免费偏好对齐方法，通过利用LLM残差流中的偏好信号，使用少量偏好对提取可插拔的引导向量，实现推理时模型行为的定向调整。


<details>
  <summary>Details</summary>
Motivation: 现有的偏好对齐方法如人类反馈强化学习或直接偏好优化通常需要大量整理数据和昂贵的计算资源，且产生持久的任务特定模型，效率低下且不灵活。

Method: PaLRS利用LLM残差流中的偏好信号，从少量（约百个）偏好对中提取轻量级的引导向量，推理时插入模型，实现无训练调整模型行为。

Result: 在多个中小规模开源LLM上实验，PaLRS对齐模型在数学推理和代码生成任务中表现提升，同时保持通用性能，并且相比DPO显著节省时间。

Conclusion: PaLRS提供了一种高效、灵活的偏好对齐替代方案，无需训练、可即插即用，且只需最少数据，极大简化了偏好优化流程。

Abstract: Preference alignment is a critical step in making Large Language Models
(LLMs) useful and aligned with (human) preferences. Existing approaches such as
Reinforcement Learning from Human Feedback or Direct Preference Optimization
typically require curated data and expensive optimization over billions of
parameters, and eventually lead to persistent task-specific models. In this
work, we introduce Preference alignment of Large Language Models via Residual
Steering (PaLRS), a training-free method that exploits preference signals
encoded in the residual streams of LLMs. From as few as one hundred preference
pairs, PaLRS extracts lightweight, plug-and-play steering vectors that can be
applied at inference time to push models toward preferred behaviors. We
evaluate PaLRS on various small-to-medium-scale open-source LLMs, showing that
PaLRS-aligned models achieve consistent gains on mathematical reasoning and
code generation benchmarks while preserving baseline general-purpose
performance. Moreover, when compared to DPO-aligned models, they perform better
with huge time savings. Our findings highlight that PaLRS offers an effective,
much more efficient and flexible alternative to standard preference
optimization pipelines, offering a training-free, plug-and-play mechanism for
alignment with minimal data.

</details>


### [100] [The Hidden Costs of Translation Accuracy: Distillation, Quantization, and Environmental Impact](https://arxiv.org/abs/2509.23990)
*Dhaathri Vijay,Anandaswarup Vadapalli*

Main category: cs.CL

TL;DR: 本研究通过机器翻译案例，比较了全尺度、大幅蒸馏和量化模型之间的翻译质量与效率的权衡。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型快速扩展引发了计算和环境成本的担忧，亟需探索翻译质量与计算效率及环保影响之间的权衡。

Method: 在Flores+基准和具体法语、印地语、坎纳达语对话翻译的人类评测上，比较了3.3B全尺寸模型、蒸馏模型和量化模型的性能与碳排放量。

Result: 全尺寸模型虽BLEU最高，但碳排放最大；蒸馏模型推理速度提升4.5倍且BLEU降低极少；即使是激进量化（INT4）也保持高准确性和流利度。

Conclusion: 模型压缩策略显著降低计算需求和环境影响，同时保持有竞争力的翻译质量；建议将效率和可持续性作为NLP评估的重要维度。

Abstract: The rapid expansion of large language models (LLMs) has heightened concerns
about their computational and environmental costs. This study investigates the
trade-offs between translation quality and efficiency by comparing full-scale,
distilled, and quantized models using machine translation as a case study. We
evaluated performance on the Flores+ benchmark and through human judgments of
conversational translations in French, Hindi, and Kannada. Our analysis of
carbon emissions per evaluation run revealed that the full 3.3B fp32 model,
while achieving the highest BLEU scores, incurred the largest environmental
footprint (about 0.007-0.008 kg CO2 per run). The distilled models achieved an
inference of up to 4.5x faster than the full 3.3B model, with only minimal
reductions in BLEU scores. Human evaluations also showed that even aggressive
quantization (INT4) preserved high levels of accuracy and fluency, with
differences between models generally minor. These findings demonstrate that
model compression strategies can substantially reduce computational demands and
environmental impact while maintaining competitive translation quality, though
trade-offs are more pronounced in low-resource settings. We argue for
evaluation frameworks that integrate efficiency and sustainability alongside
objective metrics as central dimensions of progress in NLP.

</details>


### [101] [The AI Agent Code of Conduct: Automated Guardrail Policy-as-Prompt Synthesis](https://arxiv.org/abs/2509.23994)
*Gauri Kholkar,Ratinder Ahuja*

Main category: cs.CL

TL;DR: 本文提出了一个利用大语言模型自动将非结构化设计文档转化为可验证实时安全策略的框架，实现了对自主AI的动态行为监控。


<details>
  <summary>Details</summary>
Motivation: 随着自主AI系统广泛应用于工业领域，保障其安全运行变得越来越重要，然而现有方法难以将自然语言政策有效转化为可执行的实时约束。

Method: 引入"Policy as Prompt"方法，利用大语言模型解析自然语言政策，通过构建可验证的策略树，并编译成基于提示的轻量级分类器，实现对AI行为的实时审计。

Result: 在多种应用中验证了该方法的有效性，展示了该系统具备良好的可扩展性和审计能力，成功实现了政策到实践的桥接。

Conclusion: 该框架为实现更可验证和可监管的自主AI系统提供了技术基础，有助于提升AI系统的安全性和可靠性。

Abstract: As autonomous AI agents are increasingly deployed in industry, it is
essential to safeguard them. We introduce a novel framework that automates the
translation of unstructured design documents into verifiable, real-time
guardrails. We introduce "Policy as Prompt," a new approach that uses Large
Language Models (LLMs) to interpret and enforce natural language policies by
applying contextual understanding and the principle of least privilege. Our
system first ingests technical artifacts to construct a verifiable policy tree,
which is then compiled into lightweight, prompt-based classifiers that audit
agent behavior at runtime. We validate our approach across diverse
applications, demonstrating a scalable and auditable pipeline that bridges the
critical policy-to-practice gap, paving the way for verifiably safer and more
regulatable AI.

</details>


### [102] [MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP Use](https://arxiv.org/abs/2509.24002)
*Zijian Wu,Xiangyan Liu,Xinyuan Zhang,Lingjun Chen,Fanqing Meng,Lingxiao Du,Yiran Zhao,Fanshi Zhang,Yaoqi Ye,Jiawei Wang,Zirui Wang,Jinjie Ni,Yufan Yang,Arvin Xu,Michael Qizhe Shieh*

Main category: cs.CL

TL;DR: 本论文提出了MCPMark，一个涵盖127个复杂任务的基准测试，旨在更全面地评估大型语言模型（LLM）与外部系统的交互能力，揭示现有模型性能不足。


<details>
  <summary>Details</summary>
Motivation: 现有MCP基准测试任务范围狭窄，多侧重读任务或交互深度有限，无法真实反映实际工作流的复杂性与多样性。

Method: 设计MCPMark基准，包括127个由领域专家和AI合作创作的任务，每个任务包含初始状态和自动验证脚本，涵盖丰富的CRUD操作，通过最简代理框架进行工具调用循环测试多款先进LLM。

Result: 在测试中，性能最好的模型gpt-5-medium的通过率也仅为52.56%（pass@1），其他强模型表现更低，且平均每任务需16.2轮执行和17.4次工具调用，显著高于现有基准，说明任务具有较高难度。

Conclusion: MCPMark显著提升了MCP任务的复杂性和现实性，暴露了当前LLM模型在真实多步骤交互场景中的性能瓶颈，凸显了进一步研究的必要性。

Abstract: MCP standardizes how LLMs interact with external systems, forming the
foundation for general agents. However, existing MCP benchmarks remain narrow
in scope: they focus on read-heavy tasks or tasks with limited interaction
depth, and fail to capture the complexity and realism of real-world workflows.
To address this gap, we propose MCPMark, a benchmark designed to evaluate MCP
use in a more realistic and comprehensive manner. It consists of $127$
high-quality tasks collaboratively created by domain experts and AI agents.
Each task begins with a curated initial state and includes a programmatic
script for automatic verification. These tasks demand richer and more diverse
interactions with the environment, involving a broad range of create, read,
update, and delete (CRUD) operations. We conduct a comprehensive evaluation of
cutting-edge LLMs using a minimal agent framework that operates in a
tool-calling loop. Empirical results show that the best-performing model,
gpt-5-medium, reaches only $52.56$\% pass@1 and $33.86$\% pass^4, while other
widely regarded strong models, including claude-sonnet-4 and o3, fall below
$30$\% pass@1 and $15$\% pass^4. On average, LLMs require $16.2$ execution
turns and $17.4$ tool calls per task, significantly surpassing those in
previous MCP benchmarks and highlighting the stress-testing nature of MCPMark.

</details>


### [103] [Sequential Diffusion Language Models](https://arxiv.org/abs/2509.24007)
*Yangzhou Liu,Yue Cao,Hao Li,Gen Luo,Zhe Chen,Weiyun Wang,Xiaobo Liang,Biqing Qi,Lijun Wu,Changyao Tian,Yanting Zhang,Yuqiang Li,Tong Lu,Yu Qiao,Jifeng Dai,Wenhai Wang*

Main category: cs.CL

TL;DR: 本文提出了一种名为Sequential Diffusion Language Model (SDLM)的新型扩散语言模型，通过下一序列预测（NSP）统一下一词和下一块预测，实现动态生成长度，兼容KV缓存，训练效率高，性能优于强基线。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型虽然理论上效率高，但受限于固定长度解码和KV缓存不兼容，且块扩散方法仍需固定块大小和昂贵训练，亟需一种更灵活高效的生成方法。

Method: 提出NSP统一下一词和下一块预测，允许模型自适应决定生成长度；基于NSP设计SDLM，在固定大小掩码块内进行扩散推断，并根据模型置信度动态解码连续子序列，同时保持KV缓存兼容性。

Result: SDLM在仅用3.5M训练样本的情况下，性能达到或超越强自回归基线，推理吞吐量比Qwen-2.5高2.1倍；大型SDLM-32B模型展现出更强的效率提升和良好的可扩展性。

Conclusion: SDLM通过NSP实现了灵活高效的扩散语言建模，兼顾性能与效率，展示了极大应用和扩展潜力。

Abstract: Diffusion language models (DLMs) have strong theoretical efficiency but are
limited by fixed-length decoding and incompatibility with key-value (KV)
caches. Block diffusion mitigates these issues, yet still enforces a fixed
block size and requires expensive training. We introduce Next Sequence
Prediction (NSP), which unifies next-token and next-block prediction, enabling
the model to adaptively determine the generation length at each step. When the
length is fixed to 1, NSP reduces to standard next-token prediction. Building
on NSP, we propose Sequential Diffusion Language Model (SDLM), which can
retrofit pre-trained autoregressive language models (ALMs) at minimal cost.
Specifically, SDLM performs diffusion inference within fixed-size mask blocks,
but dynamically decodes consecutive subsequences based on model confidence,
thereby preserving KV-cache compatibility and improving robustness to varying
uncertainty and semantics across the sequence. Experiments show that SDLM
matches or surpasses strong autoregressive baselines using only 3.5M training
samples, while achieving 2.1 higher throughput than Qwen-2.5. Notably, the
SDLM-32B model delivers even more pronounced efficiency gains, demonstrating
the strong scalability potential of our modeling paradigm. Project page and
codes: https://github.com/OpenGVLab/SDLM

</details>


### [104] [SparseD: Sparse Attention for Diffusion Language Models](https://arxiv.org/abs/2509.24014)
*Zeqing Wang,Gongfan Fang,Xinyin Ma,Xingyi Yang,Xinchao Wang*

Main category: cs.CL

TL;DR: 本文针对扩散语言模型推理延迟高的问题，提出了一种头部特定的稀疏注意机制SparseD，通过预计算并重复利用稀疏模式，以及在早期保持完全注意力，后期切换稀疏注意力，实现了在长上下文下的高效加速。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散语言模型推理速度慢，主要瓶颈是注意力机制关于上下文长度的二次复杂度，且现有用于自回归模型的稀疏注意力方法不适用于扩散模型，因为它们忽略了头部之间的差异和不同去噪步骤的重要性。

Method: 提出SparseD方法，预先计算头部特定的稀疏注意模式并在所有去噪步骤中重复使用，同时在初期去噪步骤中使用全连接注意力，后期切换为稀疏注意力，从而保持生成质量并提升效率。

Result: SparseD在64k上下文长度、1024去噪步骤的设定下，实现了较FlashAttention高达1.5倍的推理速度提升，且无性能损失。

Conclusion: SparseD通过头部特定的稀疏注意力机制有效降低了扩散语言模型的推理复杂度，实现了长上下文场景下的高效无损加速，具有实际应用价值。

Abstract: While diffusion language models (DLMs) offer a promising alternative to
autoregressive models (ARs), existing open-source DLMs suffer from high
inference latency. This bottleneck is mainly due to the attention's quadratic
complexity with respect to context length in computing all query-key pairs.
Intuitively, to reduce this complexity, a natural strategy is to restrict
attention to sparse patterns that retain only the most relevant connections.
Such approaches are well-established in ARs, where attention follows fixed and
clearly defined sparse patterns. However, in DLMs, we observe distinct sparsity
behaviors: (1) attention patterns vary across heads, (2) attention patterns in
each head remain highly similar across denoising steps, and (3) early denoising
steps are critical for generation. These findings render sparse attention
methods designed for ARs largely incompatible with DLMs, as they fail to
capture head-specific structures and risk degrading generation when applied in
early denoising steps. To address these challenges, we propose SparseD, a novel
sparse attention method for DLMs. Leveraging the observations, SparseD only
requires pre-computing head-specific sparse patterns one time, and reuses them
across all steps. This prevents recomputing sparse patterns at each denoising
step. Meanwhile, SparseD uses full attention in the early steps, then switches
to sparse attention later to maintain generation quality. Together, these
establish SparseD as a practical and efficient solution for deploying DLMs in
long-context applications. Experimental results demonstrate that SparseD
achieves lossless acceleration, delivering up to $1.50\times$ speedup over
FlashAttention at a 64k context length with 1,024 denoising steps.

</details>


### [105] [ResFormer: All-Time Reservoir Memory for Long Sequence Classification](https://arxiv.org/abs/2509.24074)
*Hongbo Liu,Jia Xu*

Main category: cs.CL

TL;DR: 本文提出了一种名为ResFormer的神经网络架构，旨在高效处理变长上下文的序列分类任务，结合了活力库计算网络和传统Transformer，显著提升了准确率且降低了内存消耗。


<details>
  <summary>Details</summary>
Motivation: 传统基于Transformer的模型在处理长序列时存在时间和内存复杂度过高的问题，限制了输入长度，难以有效捕捉长距离依赖。

Method: ResFormer采用级联方法，将带有非线性读出的活力库计算网络用于捕捉长时上下文依赖，时间复杂度线性，同时使用传统Transformer处理短时依赖。

Result: 在EmoryNLP数据集上，ResFormer相较DeepSeek-Qwen和ModernBERT提升准确率高达22.3%，并在MultiWOZ、MELD和IEMOCAP数据集上也取得了稳定的性能提升，同时减少了内存消耗。

Conclusion: ResFormer有效结合了不同模型的优势，显著提升了长文本上下文的建模能力和效率，是解决Transformer长序列限制的有效方案。

Abstract: Sequence classification is essential in NLP for understanding and
categorizing language patterns in tasks like sentiment analysis, intent
detection, and topic classification. Transformer-based models, despite
achieving state-of-the-art performance, have inherent limitations due to
quadratic time and memory complexity, restricting their input length. Although
extensive efforts have aimed at reducing computational demands, processing
extensive contexts remains challenging.
  To overcome these limitations, we propose ResFormer, a novel neural network
architecture designed to model varying context lengths efficiently through a
cascaded methodology. ResFormer integrates an reservoir computing network
featuring a nonlinear readout to effectively capture long-term contextual
dependencies in linear time. Concurrently, short-term dependencies within
sentences are modeled using a conventional Transformer architecture with
fixed-length inputs.
  Experiments demonstrate that ResFormer significantly outperforms baseline
models of DeepSeek-Qwen and ModernBERT, delivering an accuracy improvement of
up to +22.3% on the EmoryNLP dataset and consistent gains on MultiWOZ, MELD,
and IEMOCAP. In addition, ResFormer exhibits reduced memory consumption,
underscoring its effectiveness and efficiency in modeling extensive contextual
information.

</details>


### [106] [Ensembling Multilingual Transformers for Robust Sentiment Analysis of Tweets](https://arxiv.org/abs/2509.24080)
*Meysam Shirdel Bilehsavar,Negin Mahmoudi,Mohammad Jalili Torkamani,Kiana Kiashemshaki*

Main category: cs.CL

TL;DR: 该论文提出了一种基于transformer集成模型和大型语言模型的多语言情感分析方法，准确率超过86%。


<details>
  <summary>Details</summary>
Motivation: 针对现有情感分析在处理无标签外语文本时效果不佳的问题，提升多语言情感分析性能。

Method: 采用bert-base-multilingual-uncased-sentiment和XLM-R两个预训练模型集成，利用多语言数据集进行情感分类。

Result: 实验结果显示，提出的方法在多语言情感分析任务中准确率超过86%。

Conclusion: 该方法有效提升了在无标签多语言情感分析中的性能，具有较强的实用价值。

Abstract: Sentiment analysis is a very important natural language processing activity
in which one identifies the polarity of a text, whether it conveys positive,
negative, or neutral sentiment. Along with the growth of social media and the
Internet, the significance of sentiment analysis has grown across numerous
industries such as marketing, politics, and customer service. Sentiment
analysis is flawed, however, when applied to foreign languages, particularly
when there is no labelled data to train models upon. In this study, we present
a transformer ensemble model and a large language model (LLM) that employs
sentiment analysis of other languages. We used multi languages dataset.
Sentiment was then assessed for sentences using an ensemble of pre-trained
sentiment analysis models: bert-base-multilingual-uncased-sentiment, and XLM-R.
Our experimental results indicated that sentiment analysis performance was more
than 86% using the proposed method.

</details>


### [107] [Large-Scale Constraint Generation -- Can LLMs Parse Hundreds of Constraints?](https://arxiv.org/abs/2509.24090)
*Matteo Boffa,Jiaxuan You*

Main category: cs.CL

TL;DR: 本文提出了大规模约束生成（LSCG）问题，评估大型语言模型在处理大量细粒度约束时的能力，并设计了Words Checker测试实例，同时提出FoCusNet模型辅助约束筛选，提升准确率。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型在面对大量且细粒度约束时的理解与执行能力，当前模型在约束数目增加时表现显著下降，亟需改进方法。

Method: 设计Words Checker测试实例考察不同模型和引导技术的性能，提出FoCusNet小型专用模型，将原始约束列表精简为更相关的子集，辅助大型语言模型聚焦重要约束。

Result: 实验结果表明，随着约束数量增加，现有方法性能显著下降，而FoCusNet模型能提高8-13%的准确率表现。

Conclusion: 大规模约束生成是检验语言模型能力的新任务，结合特定的小模型筛选机制可以有效提升处理大量约束的性能。

Abstract: Recent research has explored the constrained generation capabilities of Large
Language Models (LLMs) when explicitly prompted by few task-specific
requirements. In contrast, we introduce Large-Scale Constraint Generation
(LSCG), a new problem that evaluates whether LLMs can parse a large,
fine-grained, generic list of constraints. To examine the LLMs' ability to
handle an increasing number constraints, we create a practical instance of
LSCG, called Words Checker. In Words Checker, we evaluate the impact of model
characteristics (e.g., size, family) and steering techniques (e.g., Simple
Prompt, Chain of Thought, Best of N) on performance. We also propose FoCusNet,
a small and dedicated model that parses the original list of constraints into a
smaller subset, helping the LLM focus on relevant constraints. Experiments
reveal that existing solutions suffer a significant performance drop as the
number of constraints increases, with FoCusNet showing an 8-13% accuracy boost.

</details>


### [108] [GEAR: A General Evaluation Framework for Abductive Reasoning](https://arxiv.org/abs/2509.24096)
*Kaiyu He,Peilin Wu,Mian Zhang,Kun Wan,Wentian Zhao,Xinya Du,Zhiyu Chen*

Main category: cs.CL

TL;DR: 本文提出了GEAR，一种用于评估大语言模型生成可解释假设能力的无标签、自动化评估框架。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型是否具备发现新知识的能力，并寻找有效的评估方法。

Method: 设计GEAR框架，基于一致性、泛化性和多样性三个指标对假设集合评分，并提出根据模型学习速度调整训练难度的动态训练策略。

Result: 在1500个问题上对9个大语言模型进行了细致评测，揭示了模型间差异，并通过无标签训练策略提升了模型在GEAR和传统归纳推理任务上的表现。

Conclusion: GEAR为归纳推理提供了一个可扩展、可靠的评估和训练框架，能促进模型产生更多样且合理的假设。

Abstract: Since the advent of large language models (LLMs), research has focused on
instruction following and deductive reasoning. A central question remains: can
these models discover new knowledge, and how can we evaluate this ability? We
address this by studying abductive reasoning-the generation of plausible
hypotheses to explain observations-and introduce GEAR (General Evaluation for
Abductive Reasoning), a general-purpose, fully automated, transparent, and
label-free evaluation paradigm. GEAR scores hypothesis sets by three metrics:
consistency (each hypothesis explains the observations), generalizability
(consistent hypotheses make meaningful predictions on unseen inputs), and
diversity (the set covers distinct predictions and patterns). Built this way,
GEAR is scalable (no human gold answers), reliable (deterministic scoring
aligned with classical abduction), and open-ended (scores improve only when
models produce new plausible hypotheses, unlike static benchmarks that saturate
once accuracy is high). Using GEAR, we conduct a fine-grained study of nine
LLMs on four abduction benchmarks with 1,500 problems, generating over 50,000
candidate hypotheses and revealing model differences obscured by gold-answer or
purely human evaluations. We further propose a momentum-based curriculum that
adjusts GEAR-derived training data by learning velocity: it starts with what
the model learns quickly and shifts toward harder objectives such as generating
diverse hypotheses once the model is confident on foundational objectives.
Without gold-label supervision, this strategy improves all GEAR objectives and
these gains transfer to established abductive reasoning benchmarks. Taken
together, GEAR provides a principled framework that evaluates abduction and
supplies label-free, scalable training signals that help LLMs produce more
diverse and reliable hypotheses.

</details>


### [109] [BTC-SAM: Leveraging LLMs for Generation of Bias Test Cases for Sentiment Analysis Models](https://arxiv.org/abs/2509.24101)
*Zsolt T. Kardkovács,Lynda Djennane,Anna Field,Boualem Benatallah,Yacine Gaci,Fabio Casati,Walid Gaaloul*

Main category: cs.CL

TL;DR: 该论文提出了BTC-SAM框架，利用大型语言模型（LLMs）生成多样化、高质量的测试语句，检测情感分析模型中的社会偏见。


<details>
  <summary>Details</summary>
Motivation: 情感分析模型存在社会偏见，传统构建覆盖广泛且多样的测试句子集成本高、耗时且依赖专家和众包。

Method: 提出BTC-SAM框架，借助大型语言模型进行可控生成测试句，实现多样且覆盖广的偏见测试句生成。

Result: 实验结果显示，使用LLMs生成的测试句相比基础提示方法具有更高的语言多样性和测试覆盖度，能够有效发现此前未见的偏见。

Conclusion: 利用大型语言模型生成测试样本是高效且有效的偏见检测手段，能够帮助提升情感分析模型的公平性。

Abstract: Sentiment Analysis (SA) models harbor inherent social biases that can be
harmful in real-world applications. These biases are identified by examining
the output of SA models for sentences that only vary in the identity groups of
the subjects. Constructing natural, linguistically rich, relevant, and diverse
sets of sentences that provide sufficient coverage over the domain is
expensive, especially when addressing a wide range of biases: it requires
domain experts and/or crowd-sourcing. In this paper, we present a novel bias
testing framework, BTC-SAM, which generates high-quality test cases for bias
testing in SA models with minimal specification using Large Language Models
(LLMs) for the controllable generation of test sentences. Our experiments show
that relying on LLMs can provide high linguistic variation and diversity in the
test sentences, thereby offering better test coverage compared to base
prompting methods even for previously unseen biases.

</details>


### [110] [Pragmatic Inference for Moral Reasoning Acquisition: Generalization via Distributional Semantics](https://arxiv.org/abs/2509.24102)
*Guangliang Liu,Xi Chen,Bocheng Chen,Xitong Zhang,Kristen Johnson*

Main category: cs.CL

TL;DR: 本文提出了一种基于道德基础理论的语用推理方法，旨在提升大型语言模型（LLMs）在道德推理中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在道德推理中虽表现出潜力，但难以实现泛化，这是因为模型侧重于分布语义，而道德推理更多依赖语用层面的理解。

Method: 该文基于道德基础理论，设计了利用上下文信息的语用推理方法，通过逐步引导模型将道德基础与道德推理目标联系起来，弥补语用层面的差距。

Result: 实验结果表明，该方法显著提升了大型语言模型在道德推理任务中的泛化能力。

Conclusion: 研究为基于道德基础理论的道德推理提供了方法基础，促进了未来相关研究的发展。

Abstract: Moral reasoning has emerged as a promising research direction for Large
Language Models (LLMs), yet achieving generalization remains a central
challenge. From a linguistic standpoint, this difficulty arises because LLMs
are adept at capturing distributional semantics, which fundamentally differs
from the morals which operate at the pragmatic level. This paper investigates
how LLMs can achieve generalized moral reasoning despite their reliance on
distributional semantics. We propose pragmatic inference methods grounded in
moral foundations theory, which leverage contextual information at each step to
bridge the pragmatic gap and guide LLMs in connecting moral foundations with
moral reasoning objectives. Experimental results demonstrate that our approach
significantly enhances LLMs' generalization in moral reasoning, providing a
foundation for future research grounded in moral foundations theory.

</details>


### [111] [Dual-Scale World Models for LLM Agents Towards Hard-Exploration Problems](https://arxiv.org/abs/2509.24116)
*Minsoo Kim,Seung-won Hwang*

Main category: cs.CL

TL;DR: 本文提出了GLoW方法，通过双尺度世界模型和多路径优势反思机制提升LLM在困难探索任务中的表现，在文本游戏基准测试中实现了新的最佳性能，并显著减少了环境交互次数。


<details>
  <summary>Details</summary>
Motivation: 现有基于大型语言模型(LLM)的智能体在需要通过探索学习新知识的困难探索任务中表现有限，需要一种更高效且有效的探索机制。

Method: 提出GLoW方法，利用双尺度世界模型管理高价值发现的轨迹前沿，并通过多路径优势反思机制推断基于优势的进展信号，引导探索过程。

Result: 在Jericho文本游戏基准测试中，GLoW达到了LLM方法的新状态最优性能，且与最先进的强化学习方法相比，环境交互次数减少了100到800倍。

Conclusion: GLoW有效提升了LLM智能体在困难探索任务中的表现，实现高效探索和知识学习，展示了其在文本游戏和类似任务中的应用潜力。

Abstract: LLM-based agents have seen promising advances, yet they are still limited in
"hard-exploration" tasks requiring learning new knowledge through exploration.
We present GLoW, a novel approach leveraging dual-scale world models,
maintaining a trajectory frontier of high-value discoveries at the global
scale, while learning from local trial-and-error in exploration through a
Multi-path Advantage Reflection mechanism which infers advantage-based progress
signals to guide exploration. To evaluate our framework for hard-exploration,
we tackle the Jericho benchmark suite of text-based games, where GLoW achieves
a new state-of-theart performance for LLM-based approaches. Compared to
state-of-the-art RLbased methods, our approach achieves comparable performance
while requiring 100-800x fewer environment interactions.

</details>


### [112] [EduVidQA: Generating and Evaluating Long-form Answers to Student Questions based on Lecture Videos](https://arxiv.org/abs/2509.24120)
*Sourjyadip Ray,Shubham Sharma,Somak Aditya,Pawan Goyal*

Main category: cs.CL

TL;DR: 本文提出了基于多模态大型语言模型（MLLM）自动回答在线课程学生问题的任务，构建了包含5252问答对的EduVidQA数据集，并通过实证研究学生偏好，评测了6个先进MLLM模型的表现。


<details>
  <summary>Details</summary>
Motivation: 随着数字平台改变教育模式，保持互动性对有效学习至关重要，自动化回答学生问题可以提升在线教育质量。

Method: 构建EduVidQA数据集，包含合成与真实问题答案，利用6个最新MLLM进行微调和评测，结合文本和定性指标多维评估模型效果。

Result: 合成数据有助于模型微调，但任务具有较大挑战性；模型在多维度评测中表现出差异，验证了数据集和任务的研究价值。

Conclusion: 本研究为在线教育中的自动问答任务设立了基准，推动自然语言处理在教育领域的应用，开启了未来研究的新方向。

Abstract: As digital platforms redefine educational paradigms, ensuring interactivity
remains vital for effective learning. This paper explores using Multimodal
Large Language Models (MLLMs) to automatically respond to student questions
from online lectures - a novel question answering task of real world
significance. We introduce the EduVidQA Dataset with 5252 question-answer pairs
(both synthetic and real-world) from 296 computer science videos covering
diverse topics and difficulty levels. To understand the needs of the dataset
and task evaluation, we empirically study the qualitative preferences of
students, which we provide as an important contribution to this line of work.
Our benchmarking experiments consist of 6 state-of-the-art MLLMs, through which
we study the effectiveness of our synthetic data for finetuning, as well as
showing the challenging nature of the task. We evaluate the models using both
text-based and qualitative metrics, thus showing a nuanced perspective of the
models' performance, which is paramount to future work. This work not only sets
a benchmark for this important problem, but also opens exciting avenues for
future research in the field of Natural Language Processing for Education.

</details>


### [113] [Beyond Magic Words: Sharpness-Aware Prompt Evolving for Robust Large Language Models with TARE](https://arxiv.org/abs/2509.24130)
*Guancheng Wan,Lucheng Fu,Haoxin Liu,Yiqiao Jin,Hui Yi Leong,Eric Hanchen Jiang,Hejia Geng,Jinhe Bi,Yunpu Ma,Xiangru Tang,B. Aditya Prakash,Yizhou Sun,Wei Wang*

Main category: cs.CL

TL;DR: 本文提出了一种关注文本敏感性的自动提示词优化方法，有效提升大语言模型在语义一致的改写提示下的鲁棒性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有提示词优化方法关注点对点精度，缺乏对提示词语义改写不变性和搜索稳定性的保障，导致提示词优化结果易受小改写影响而表现不稳定。

Method: 本文首次形式化定义了提示词语义空间的文本敏感性和基于语义邻域的鲁棒性准则，提出无梯度的TARE框架，交替进行基于采样的对抗性搜索和鲁棒性选择，同时提出ATARE通过学习各向异性权重调整语义邻域形状及半径。

Result: 所提方法在多个任务中测试，能有效降低文本敏感性差距，使提示在语义改写下保持高准确率，超越仅优化准确率的方法，且计算效率较高。

Conclusion: 最小化文本敏感性差距的提示词优化策略能够显著提升提示词在语义改写下的稳定性和鲁棒性，为大语言模型提示词设计提供了新的有效途径。

Abstract: The performance of Large Language Models (LLMs) hinges on carefully
engineered prompts. However, prevailing prompt optimization methods, ranging
from heuristic edits and reinforcement learning to evolutionary search,
primarily target point-wise accuracy. They seldom enforce paraphrase invariance
or searching stability, and therefore cannot remedy this brittleness in
practice. Automated prompt search remains brittle: small, semantically
preserving paraphrases often cause large performance swings. We identify this
brittleness as the textual sharpness of the prompt landscape. In this work, we
provide the first formal treatment of textual sharpness in the discrete,
semantic space of prompts, together with an operational robustness criterion
over a semantic neighborhood; the design is black-box or API-only, requiring no
gradients to update the model's parameters. Then we introduce TARE (Textual
Sharpness-Aware Evolving), a derivative-free framework that alternates between
an inner, sampling-based adversarial search that stresses a prompt with hard
paraphrases and an outer, robust selection that prefers candidates whose
neighborhoods remain strong. We further propose ATARE, which learns anisotropic
weights to shape the semantic neighborhood and adapts its radius over time to
balance exploration and fidelity. Diverse tasks evaluate our methods, whose
design for minimizing textual sharpness gap leads to prompts that preserve
accuracy under paraphrasing, outperforming accuracy-only prompt search while
remaining computationally practical.

</details>


### [114] [Your thoughts tell who you are: Characterize the reasoning patterns of LRMs](https://arxiv.org/abs/2509.24147)
*Yida Chen,Yuning Mao,Xianjun Yang,Suyu Ge,Shengjie Bi,Lijuan Liu,Saghar Hosseini,Liang Tan,Yixin Nie,Shaoliang Nie*

Main category: cs.CL

TL;DR: 本文提出了一种名为LOT的方法，通过生成型语言模型分类和比较大型推理模型的推理轨迹，揭示它们的思考差异。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型的比较主要基于宏观统计指标，不清楚不同模型在推理过程中的差异。为此，需寻找更细粒度的比较方法。

Method: 引入LLM提出的开放分类法（LOT），利用生成型语言模型对两个推理模型的推理轨迹进行比较，提取区分特征并基于其分布构建人类可读的推理思维分类体系。

Result: LOT在12个开源推理模型中，成功识别出不同模型在数学、科学、编码任务的系统性推理差异，区分准确率达80%-100%。基于推理风格的对齐能提升小模型的性能。

Conclusion: LOT不仅能高效区分不同推理模型，还能提供定性解释，并证明调整推理风格有助于提升模型表现。

Abstract: Current comparisons of large reasoning models (LRMs) focus on macro-level
statistics such as task accuracy or reasoning length. Whether different LRMs
reason differently remains an open question. To address this gap, we introduce
the LLM-proposed Open Taxonomy (LOT), a classification method that uses a
generative language model to compare reasoning traces from two LRMs and
articulate their distinctive features in words. LOT then models how these
features predict the source LRM of a reasoning trace based on their empirical
distributions across LRM outputs. Iterating this process over a dataset of
reasoning traces yields a human-readable taxonomy that characterizes how models
think. We apply LOT to compare the reasoning of 12 open-source LRMs on tasks in
math, science, and coding. LOT identifies systematic differences in their
thoughts, achieving 80-100% accuracy in distinguishing reasoning traces from
LRMs that differ in scale, base model family, or objective domain. Beyond
classification, LOT's natural-language taxonomy provides qualitative
explanations of how LRMs think differently. Finally, in a case study, we link
the reasoning differences to performance: aligning the reasoning style of
smaller Qwen3 models with that of the largest Qwen3 during test time improves
their accuracy on GPQA by 3.3-5.7%.

</details>


### [115] [Localizing Task Recognition and Task Learning in In-Context Learning via Attention Head Analysis](https://arxiv.org/abs/2509.24164)
*Haolin Yang,Hakaze Cho,Naoya Inoue*

Main category: cs.CL

TL;DR: 本文提出了基于任务子空间对数归因（TSLA）的框架，揭示了大型语言模型中注意力头对上下文学习器中任务识别（TR）和任务学习（TL）的不同且互补作用。


<details>
  <summary>Details</summary>
Motivation: 现有关于大型语言模型上下文学习机制的研究分为基于注意力头成分的分析和将上下文学习分解为任务识别与任务学习两大视角，两者尚未统一解释。

Method: 提出TSLA方法，识别专门执行任务识别和任务学习的注意力头，通过相关性分析、消融试验和输入扰动验证其独立而有效的作用，并结合几何分析解释其如何推动任务识别及预测。

Result: 识别出的TR头通过将隐藏状态与任务子空间对齐实现任务识别，TL头则在子空间内旋转隐藏状态以指向正确标签；研究统一了归纳头和任务向量的机制理解。

Conclusion: 该研究提供了一个统一且可解释的框架，揭示大型语言模型如何在多任务多环境下通过特定注意力头完成上下文学习。

Abstract: We investigate the mechanistic underpinnings of in-context learning (ICL) in
large language models by reconciling two dominant perspectives: the
component-level analysis of attention heads and the holistic decomposition of
ICL into Task Recognition (TR) and Task Learning (TL). We propose a novel
framework based on Task Subspace Logit Attribution (TSLA) to identify attention
heads specialized in TR and TL, and demonstrate their distinct yet
complementary roles. Through correlation analysis, ablation studies, and input
perturbations, we show that the identified TR and TL heads independently and
effectively capture the TR and TL components of ICL. Using steering experiments
with geometric analysis of hidden states, we reveal that TR heads promote task
recognition by aligning hidden states with the task subspace, while TL heads
rotate hidden states within the subspace toward the correct label to facilitate
prediction. We further show how previous findings on ICL mechanisms, including
induction heads and task vectors, can be reconciled with our
attention-head-level analysis of the TR-TL decomposition. Our framework thus
provides a unified and interpretable account of how large language models
execute ICL across diverse tasks and settings.

</details>


### [116] [Task Vectors, Learned Not Extracted: Performance Gains and Mechanistic Insight](https://arxiv.org/abs/2509.24169)
*Haolin Yang,Hakaze Cho,Kaize Ding,Naoya Inoue*

Main category: cs.CL

TL;DR: 本文提出了直接训练学习型任务向量(LTV)的方法，提升了任务表示的准确性和灵活性，并系统分析了任务向量在大语言模型中通过注意力头和线性传播影响预测的机制。


<details>
  <summary>Details</summary>
Motivation: 现有研究通过复杂且不透明的方法提取任务向量，且未深入阐明任务向量如何影响模型计算。

Method: 作者提出直接训练学习型任务向量(LTV)，并通过系统分析揭示了任务向量在模型中主要通过注意力头中的OV电路作用，以及其传播过程中的线性特性。

Result: LTV在准确性和灵活性上优于传统提取的任务向量，可以在任意层、位置甚至ICL提示中有效作用；任务向量在低层级主要通过少数关键注意力头实现预测引导，高层级表现为线性旋转和放大。

Conclusion: LTV不仅提供了一种高效获得任务向量的方法，也为理解大语言模型内在的内在学习机制提供了理论基础。

Abstract: Large Language Models (LLMs) can perform new tasks from in-context
demonstrations, a phenomenon known as in-context learning (ICL). Recent work
suggests that these demonstrations are compressed into task vectors (TVs),
compact task representations that LLMs exploit for predictions. However, prior
studies typically extract TVs from model outputs or hidden states using
cumbersome and opaque methods, and they rarely elucidate the mechanisms by
which TVs influence computation. In this work, we address both limitations.
First, we propose directly training Learned Task Vectors (LTVs), which surpass
extracted TVs in accuracy and exhibit superior flexibility-acting effectively
at arbitrary layers, positions, and even with ICL prompts. Second, through
systematic analysis, we investigate the mechanistic role of TVs, showing that
at the low level they steer predictions primarily through attention-head OV
circuits, with a small subset of "key heads" most decisive. At a higher level,
we find that despite Transformer nonlinearities, TV propagation is largely
linear: early TVs are rotated toward task-relevant subspaces to improve logits
of relevant labels, while later TVs are predominantly scaled in magnitude.
Taken together, LTVs not only provide a practical approach for obtaining
effective TVs but also offer a principled lens into the mechanistic foundations
of ICL.

</details>


### [117] [Retrieval-augmented GUI Agents with Generative Guidelines](https://arxiv.org/abs/2509.24183)
*Ran Xu,Kaixin Ma,Wenhao Yu,Hongming Zhang,Joyce C. Ho,Carl Yang,Dong Yu*

Main category: cs.CL

TL;DR: 本文提出了一种轻量级视觉语言模型RAG-GUI，通过在推理时利用网络教程，显著提升了GUI代理在复杂数字任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉语言模型的GUI代理在真实应用中因训练数据稀缺和任务复杂性而效果有限，尤其难以应对罕见和未见场景。

Method: RAG-GUI先通过监督微调（SFT）进行预训练，然后采用自引导拒绝采样微调（RSF）进一步优化，且作为模型无关的插件设计，适配任何基于VLM的代理。

Result: 在三个不同任务上，RAG-GUI均优于基线代理，并在两种模型尺寸下比其他推理基线提升2.6%至13.3%。

Conclusion: RAG-GUI展示了良好的泛化能力和实际的即插即用特性，适合真实场景中提升VLM驱动的GUI代理表现。

Abstract: GUI agents powered by vision-language models (VLMs) show promise in
automating complex digital tasks. However, their effectiveness in real-world
applications is often limited by scarce training data and the inherent
complexity of these tasks, which frequently require long-tailed knowledge
covering rare, unseen scenarios. We propose RAG-GUI , a lightweight VLM that
leverages web tutorials at inference time. RAG-GUI is first warm-started via
supervised finetuning (SFT) and further refined through self-guided rejection
sampling finetuning (RSF). Designed to be model-agnostic, RAG-GUI functions as
a generic plug-in that enhances any VLM-based agent. Evaluated across three
distinct tasks, it consistently outperforms baseline agents and surpasses other
inference baselines by 2.6% to 13.3% across two model sizes, demonstrating
strong generalization and practical plug-and-play capabilities in real-world
scenarios.

</details>


### [118] [Beyond Overall Accuracy: A Psychometric Deep Dive into the Topic-Specific Medical Capabilities of 80 Large Language Models](https://arxiv.org/abs/2509.24186)
*Zhimeng Luo,Lixin Wu,Adam Frisch,Daqing He*

Main category: cs.CL

TL;DR: 本文提出了基于项目反应理论（IRT）的医疗大语言模型评估框架MedIRT，使用USMLE相关的1100个问题和80个LLM响应，提供比传统准确率更稳定细致的性能排名。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在高风险医疗应用中的兴起，传统准确率评估无法反映问题特性和主题特异性，亟需更可靠准确的评价方法。

Method: 采用基于IRT的单一维度两参数逻辑模型对每个主题进行评估，结合问题难度和辨别度，分析80个多元LLM对1100个USMLE题目的响应，构建多因素能力画像。

Result: 模型表现呈现“尖峰”能力特征，整体排名可能误导；GPT-5虽然整体表现最好，但在社会科学和交流领域被Claude-3-opus超越，显示特定能力的多样化分布。

Conclusion: 基于IRT的MedIRT框架不仅提高了评估的稳定性和细粒度，还能识别题目缺陷，促进医疗LLM的安全有效部署，是评估医疗大语言模型的坚实心理测量学方法学基础。

Abstract: As Large Language Models (LLMs) are increasingly proposed for high-stakes
medical applications, there has emerged a critical need for reliable and
accurate evaluation methodologies. Traditional accuracy metrics fail
inadequately as they neither capture question characteristics nor offer
topic-specific insights. To address this gap, we introduce \textsc{MedIRT}, a
rigorous evaluation framework grounded in Item Response Theory (IRT), the gold
standard in high-stakes educational testing. Unlike previous research relying
on archival data, we prospectively gathered fresh responses from 80 diverse
LLMs on a balanced, 1,100-question USMLE-aligned benchmark. Using one
unidimensional two-parameter logistic IRT model per topic, we estimate LLM's
latent model ability jointly with question difficulty and discrimination,
yielding more stable and nuanced performance rankings than accuracy alone.
Notably, we identify distinctive ``spiky'' ability profiles, where overall
rankings can be misleading due to highly specialized model abilities. While
\texttt{GPT-5} was the top performer in a majority of domains (8 of 11), it was
outperformed in Social Science and Communication by \texttt{Claude-3-opus},
demonstrating that even an overall 23rd-ranked model can hold the top spot for
specific competencies. Furthermore, we demonstrate IRT's utility in auditing
benchmarks by identifying flawed questions. We synthesize these findings into a
practical decision-support framework that integrates our multi-factor
competency profiles with operational metrics. This work establishes a robust,
psychometrically grounded methodology essential for the safe, effective, and
trustworthy deployment of LLMs in healthcare.

</details>


### [119] [PET: Preference Evolution Tracking with LLM-Generated Explainable Distribution](https://arxiv.org/abs/2509.24189)
*Luyang Zhang,Siyuan Peng,Jialu Wang,Shichao Zhu,Beibei Li,Zhongcun Wang,Guangmou Pan,Yan Li,Song Yang*

Main category: cs.CL

TL;DR: 本文提出了Preference Evolution Tracking (PET)框架，通过推断用户偏好在可解释的偏好簇空间中的动态概率分布，实现更透明和个性化的用户偏好建模。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的用户偏好预测方法虽效果好，但端到端生成模式缺乏透明性，难以进行全面用户画像，且易受流行度偏差影响。

Method: PET通过logit-probing和生成式分类技术，将用户偏好视为概率分布，代替直接生成排序列表，从而提升偏好推断的解释性和个性化。

Result: 在Yelp和MovieLens基准数据集上，PET在NDCG指标上相较于直接生成方法提升最多40%。在某短视频平台大规模数据上，PET对长尾内容排序表现优异，NDCG成绩是现有另一生产模型的7倍。

Conclusion: PET将用户偏好建模从列表生成转变为分布式映射，推动了更具可解释性、公平性和多样性的个性化推荐系统发展。

Abstract: Understanding how user preference evolves over time is a fundamental
challenge central to modern digital ecosystems, for which Large Language Models
(LLMs) are an increasingly prominent and popular approach due to their ability
to comprehend the rich semantic context within behavioral data. A common
practice is to use LLMs to predict a user's next action by directly generating
a ranked list of preferred items. Although effective for short-term prediction,
the end-to-end generation paradigm inherently limits personalization. Its
opaque decision-making process obscures holistic user profiling and exacerbates
popularity bias. To address these limitations, we propose Preference Evolution
Tracking (PET), a framework that reframes the task as inferring a dynamic
probability distribution over a stable and interpretable lattice of preference
clusters. By applying logit-probing and generative classification techniques,
PET infers a user's preference as a probability distribution, enabling
transparent preference learning. On public benchmarks (Yelp, MovieLens), PET
improves ranking quality by up to 40% in NDCG over direct generation baselines.
On a large-scale, real-world dataset from a short-video platform, it excels at
ranking long-tail contents, significantly outperforming a SOTA production model
by 7 times in the NDCG score. Ultimately, PET transforms the user profile model
from direct preference list generation to a transparent distributional
preference mapping, paving the way for more explainable, fair, and diverse
personalization systems.

</details>


### [120] [AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced Self-Play](https://arxiv.org/abs/2509.24193)
*Ran Xu,Yuchen Zhuang,Zihan Dong,Jonathan Wang,Yue Yu,Joyce C. Ho,Linjun Zhang,Haoyu Wang,Wenqi Shi,Carl Yang*

Main category: cs.CL

TL;DR: AceSearcher是一个训练单一大语言模型在复杂推理任务中交替扮演查询分解者和答案解决者角色的自我协作框架，显著提升了多步检索和推理的效果，性能优于现有先进方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于搜索增强的大语言模型在处理复杂推理任务时，多跳检索效果不佳且推理能力有限。

Method: 提出AceSearcher框架，通过自我对弈训练模型在分解复杂查询和整合检索上下文生成答案两个角色间切换，结合有监督微调和强化学习微调优化最终答案准确性，无需中间标注。

Result: 在三个重推理任务的10个数据集上，AceSearcher比现有先进方法平均提高7.6%的准确率。在财经文档级推理任务中，32B参数模型性能与DeepSeek-V3持平，但参数量不足其5%，且小规模模型仍优于更大参数量的竞争者。

Conclusion: AceSearcher显著提升了搜索增强大语言模型在复杂推理任务上的效率和效果，提供了一个高效且强大的框架，未来有广泛应用价值。

Abstract: Search-augmented LLMs often struggle with complex reasoning tasks due to
ineffective multi-hop retrieval and limited reasoning ability. We propose
AceSearcher, a cooperative self-play framework that trains a single large
language model (LLM) to alternate between two roles: a decomposer that breaks
down complex queries and a solver that integrates retrieved contexts for answer
generation. AceSearcher couples supervised fine-tuning on a diverse mixture of
search, reasoning, and decomposition tasks with reinforcement fine-tuning
optimized for final answer accuracy, eliminating the need for intermediate
annotations. Extensive experiments on three reasoning-intensive tasks across 10
datasets show that AceSearcher outperforms state-of-the-art baselines,
achieving an average exact match improvement of 7.6%. Remarkably, on
document-level finance reasoning tasks, AceSearcher-32B matches the performance
of the DeepSeek-V3 model using less than 5% of its parameters. Even at smaller
scales (1.5B and 8B), AceSearcher often surpasses existing search-augmented
LLMs with up to 9x more parameters, highlighting its exceptional efficiency and
effectiveness in tackling complex reasoning tasks. Our code will be published
at https://github.com/ritaranx/AceSearcher and
https://huggingface.co/AceSearcher.

</details>


### [121] [Can Large Language Models Express Uncertainty Like Human?](https://arxiv.org/abs/2509.24202)
*Linwei Tao,Yi-Fan Yeh,Bo Kai,Minjing Dong,Tao Huang,Tom A. Lamb,Jialin Yu,Philip H. S. Torr,Chang Xu*

Main category: cs.CL

TL;DR: 本文提出了一种基于语言表达不确定性的置信度估计方法，即通过语言中的犹豫表达（如"可能"、"或许"）来反映模型不确定性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在高风险应用中容易给出过于自信的答案，传统置信度估计方法存在隐藏信息、计算开销大以及不符合自然交流的问题，因此需要一种轻量且以人为本的置信度估计方式。

Method: 构建了首个包含犹豫表达和人工标注置信度的大规模数据集，设计了将语言表达映射为置信度分数的轻量映射器，系统研究了多种大语言模型在语言置信度表达的性能，并提出了一种微调框架提升置信度可靠性。

Result: 发现大多数大语言模型在表达可靠置信度方面表现不足，但通过精心设计的提示语可以达到较好的校准和区分能力，微调进一步提升了性能。

Conclusion: 语言置信度是一种高效、可扩展且符合人类交流习惯的模型不确定性估计方法，值得深入研究与应用。

Abstract: Large language models (LLMs) are increasingly used in high-stakes settings,
where overconfident responses can mislead users. Reliable confidence estimation
has been shown to enhance trust and task accuracy. Yet existing methods face
practical barriers: logits are often hidden, multi-sampling is computationally
expensive, and verbalized numerical uncertainty (e.g., giving a 0-100 score)
deviates from natural communication. We revisit linguistic confidence (LC),
where models express uncertainty through hedging language (e.g., probably,
might), offering a lightweight and human-centered alternative. To advance this
direction, we (1) release the first diverse, large-scale dataset of hedging
expressions with human-annotated confidence scores, and (2) propose a
lightweight mapper that converts hedges into confidence scores at near-zero
cost. Building on these resources, we (3) conduct the first systematic study of
LC across modern LLMs and QA benchmarks, revealing that while most LLMs
underperform in expressing reliable LC, carefully designed prompting achieves
competitive calibration and discriminability. Finally, we (4) introduce a
fine-tuning framework that further improves LC reliability. Taken together, our
work positions linguistic confidence as a scalable, efficient, and
human-aligned approach to LLM uncertainty estimation, and calls for deeper
exploration of this promising yet underexplored direction.

</details>


### [122] [BeyondBench: Benchmark-Free Evaluation of Reasoning in Language Models](https://arxiv.org/abs/2509.24210)
*Gaurav Srivastava,Aafiya Hussain,Zhenyu Bi,Swastik Roy,Priya Pitre,Meng Lu,Morteza Ziyadi,Xuan Wang*

Main category: cs.CL

TL;DR: 本文提出了BeyondBench，一个通过算法自动生成问题来评估语言模型的新框架，以避免测试集被训练数据污染的问题。


<details>
  <summary>Details</summary>
Motivation: 现有静态评测基准容易被训练数据污染，难以判断模型是否真正具备推理能力。

Method: BeyondBench动态生成基于数学算法的问题，涵盖44个算法任务和117个变种，分为Easy、Medium、Hard三类，问题组合空间超过10^15个实例，并通过数学证明验证答案。

Result: 评估了101个语言模型，发现所有模型在复杂度提高时推理能力显著下降，尤其是处理困难任务时表现较差；GPT-5系列无辅助工具时性能大幅下降。

Conclusion: BeyondBench为语言模型推理能力的公平评估提供了有效工具，揭示了现有模型在复杂问题上的不足，并强调辅助工具的重要性。

Abstract: Evaluating language models fairly is becoming harder as static benchmarks
available on the internet risk contamination by training data. This makes it
unclear whether models are truly reasoning or just recalling answers. In this
paper, we introduce BeyondBench, an evaluation framework that avoids this
problem by using algorithmic problem generation. Unlike traditional benchmarks
that risk contamination from internet-scale training data, BeyondBench creates
mathematically grounded problems on the fly, ensuring each test remains fresh
and uncontaminated. Our framework covers 44 algorithmic tasks with a total of
117 variations, grouped into three difficulty levels: the Easy Suite (29 tasks)
for basic arithmetic and statistics, the Medium Suite (5 tasks, 49 variations)
for sequence patterns and reasoning, and the Hard Suite (10 tasks, 68
variations) tackling NP-complete and constraint satisfaction problems. Each
task generates problems from a combinatorial space larger than 10^15 unique
instances, with solutions verified deterministically by mathematical proofs. We
evaluated 101 language models, including 85 open-source and 16 closed-source
models, spanning sizes from 0.5B to 141B parameters and multiple quantization
schemes. Our results show consistent reasoning deficiencies across model
families, with performance degrading sharply as problem complexity increases
from polynomial to exponential. In our Hard Suite evaluations, models such as
Gemini-2.5-pro, Llama-3.3-70B, and Qwen2.5-72B achieved average accuracies of
56.38%, 26.91%, and 33.60%, respectively. Moreover, we observe that performance
drops drastically without tool usage, with GPT-5, GPT-5-mini, and GPT-5-nano
showing a decline of 16.81%, 28.05%, and 47.59% accuracy on the hard suite. Our
leaderboard is publicly available at https://ctrl-gaurav.github.io/BeyondBench/

</details>


### [123] [ScenarioBench: Trace-Grounded Compliance Evaluation for Text-to-SQL and RAG](https://arxiv.org/abs/2509.24212)
*Zahra Atf,Peter R Lewis*

Main category: cs.CL

TL;DR: ScenarioBench是一个用于合规性场景的文本到SQL及检索增强生成评估基准，强调决策依据的透明性与可审计性。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL和检索增强生成评测缺乏对决策依据的严格验证和端到端的全面评分，难以确保系统输出的合规性和解释的可信度。

Method: 设计基于YAML的场景，每个场景包含预期决策、证据轨迹、政策条款及规范SQL，系统必须用条款ID解释决策，评估包括决策准确率、轨迹质量、检索效果、SQL正确性、政策覆盖率、延迟以及解释幻觉率，并提出场景难度指标（SDI和SDI-R）综合评分。

Result: ScenarioBench实现了对决策与其依据的严格绑定和无预览规则，比以往基准更注重解释质量和合理的时间预算管理。

Conclusion: ScenarioBench提供了一个更可信、可审计和综合的合规性文本到SQL及检索生成评测框架，有助于提升系统决策合理性及其解释的质量。

Abstract: ScenarioBench is a policy-grounded, trace-aware benchmark for evaluating
Text-to-SQL and retrieval-augmented generation in compliance contexts. Each
YAML scenario includes a no-peek gold-standard package with the expected
decision, a minimal witness trace, the governing clause set, and the canonical
SQL, enabling end-to-end scoring of both what a system decides and why. Systems
must justify outputs using clause IDs from the same policy canon, making
explanations falsifiable and audit-ready. The evaluator reports decision
accuracy, trace quality (completeness, correctness, order), retrieval
effectiveness, SQL correctness via result-set equivalence, policy coverage,
latency, and an explanation-hallucination rate. A normalized Scenario
Difficulty Index (SDI) and a budgeted variant (SDI-R) aggregate results while
accounting for retrieval difficulty and time. Compared with prior Text-to-SQL
or KILT/RAG benchmarks, ScenarioBench ties each decision to clause-level
evidence under strict grounding and no-peek rules, shifting gains toward
justification quality under explicit time budgets.

</details>


### [124] [MoVa: Towards Generalizable Classification of Human Morals and Values](https://arxiv.org/abs/2509.24216)
*Ziyu Chen,Junfei Sun,Chenxi Li,Tuan Dung Nguyen,Jing Yao,Xiaoyuan Yi,Xing Xie,Chenhao Tan,Lexing Xie*

Main category: cs.CL

TL;DR: 本文介绍了MoVa资源包，用于通用人类道德和价值观的分类，包括16个标注数据集、轻量级大语言模型提示策略及心理调查评估应用。


<details>
  <summary>Details</summary>
Motivation: 解决研究者在识别语言中人类道德和价值时面对的理论框架多样性和数据缺乏难题。

Method: 构建16个标注数据集，提出轻量级大语言模型提示策略，推荐同时评分所有相关概念的all@once分类方法。

Result: 该方法在多个领域和理论框架中表现优于微调模型，促进了人类和机器交流的细粒度解释。

Conclusion: MoVa资源和方法有助于提高道德价值分类的准确性，推动机器行为的对齐与理解。

Abstract: Identifying human morals and values embedded in language is essential to
empirical studies of communication. However, researchers often face substantial
difficulty navigating the diversity of theoretical frameworks and data
available for their analysis. Here, we contribute MoVa, a well-documented suite
of resources for generalizable classification of human morals and values,
consisting of (1) 16 labeled datasets and benchmarking results from four
theoretically-grounded frameworks; (2) a lightweight LLM prompting strategy
that outperforms fine-tuned models across multiple domains and frameworks; and
(3) a new application that helps evaluate psychological surveys. In practice,
we specifically recommend a classification strategy, all@once, that scores all
related concepts simultaneously, resembling the well-known multi-label
classifier chain. The data and methods in MoVa can facilitate many fine-grained
interpretations of human and machine communication, with potential implications
for the alignment of machine behavior.

</details>


### [125] [Model Fusion with Multi-LoRA Inference for Tool-Enhanced Game Dialogue Agents](https://arxiv.org/abs/2509.24229)
*Kangxu Wang,Ze Chen,Chengcheng Wei,Jiewen Zheng,Jiarong He,Max Gao*

Main category: cs.CL

TL;DR: 该论文介绍了opdainlp团队参加CPDC 2025 GPU赛道的解决方案，旨在构建符合游戏角色特性和世界观的游戏内对话AI，并支持功能调用。


<details>
  <summary>Details</summary>
Motivation: 挑战任务包括构建符合角色设定且支持功能调用的对话AI，同时考虑推理中的效果和资源时间限制。

Method: 基于主办方提供的数据合成部分任务数据，采用Qwen3-14B模型结合LoRA微调和模型融合技术，推理时使用集成多个LoRA适配器的基础模型，并通过vLLM实现多LoRA推理。

Result: 解决方案在GPU赛道的三个任务中表现优异，分别获得任务1和任务3的冠军，任务2的第二名。

Conclusion: 本文提出的方法有效提升了游戏内对话AI的表现，在多任务挑战中取得了领先成绩，验证了多LoRA适配器融合与数据合成技术的实用性。

Abstract: This paper presents the opdainlp team's solution for the GPU track of the
CPDC 2025 challenge. The challenge consists of three tasks, aiming to build an
in-game conversational AI that adheres to character personas, aligns with the
game's worldview, and supports function calling. Considering both effectiveness
and resource/time constraints during inference, we synthesized data for some of
the tasks based on the datasets provided by the competition organizers. We
employed Qwen3-14B with LoRA fine-tuning and model fusion, and utilized a base
model integrated with multiple LoRA adapters during inference. Specifically, in
the competition, we used three distinct LoRA adapters to handle tool calling,
response generation with tool call results, and response generation without
tool call results, respectively. MultiLoRA inference was implemented using
vLLM. Our solution achieved the first place in Task 1 and Task 3, and the
second place in Task 2 of the GPU track.

</details>


### [126] [Prompt and Parameter Co-Optimization for Large Language Models](https://arxiv.org/abs/2509.24245)
*Xiaohe Bo,Rui Li,Zexu Sun,Quanyu Dai,Zeyu Zhang,Zihang Tian,Xu Chen,Zhenhua Dong*

Main category: cs.CL

TL;DR: 本文提出了MetaTuner框架，联合优化提示词和微调参数，提升大语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多孤立地研究提示优化和微调，未充分挖掘二者结合的潜力。

Method: 设计两个神经网络分别生成提示词和参数，采用共享底层编码层并引入监督正则化损失，实现离散与连续空间的联合优化。

Result: 在多个基准测试中，MetaTuner框架表现优于传统单独使用提示优化或微调的方法。

Conclusion: 提示优化与微调的结合通过MetaTuner可以实现性能提升，展示了其协同效应和应用前景。

Abstract: Prompt optimization and fine-tuning are two major approaches to improve the
performance of Large Language Models (LLMs). They enhance the capabilities of
LLMs from complementary perspectives: the former through explicit natural
language, and the latter through implicit parameter updates. However, prior
work has typically studied them in isolation, leaving their synergistic
potential largely underexplored. To bridge this gap, in this paper, we
introduce MetaTuner, a novel framework that jointly integrates prompt
optimization and fine-tuning for LLM training. Specifically, we introduce two
neural networks to generate prompts and parameters, respectively, while
allowing them to share a common bottom encoding layer to enable knowledge
sharing. By the guidance of the final supervised signals, our framework is
optimized to discover the optimal combinations between the prompts and
parameters. Given that prompt learning involves discrete optimization while
fine-tuning operates in a continuous parameter space, we design a supervised
regularization loss to train our framework effectively. Extensive experiments
across diverse benchmarks show that our method consistently outperforms the
baselines.

</details>


### [127] [MRAG-Suite: A Diagnostic Evaluation Platform for Visual Retrieval-Augmented Generation](https://arxiv.org/abs/2509.24253)
*Yuelyu Ji*

Main category: cs.CL

TL;DR: 本文提出了MRAG-Suite诊断评估平台，用于全面评估多模态检索增强生成（Visual RAG）系统在不同难度和模糊性查询下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态问答评估方法未能系统考虑查询的难度和模糊性，导致对模型能力的评估不全面。

Method: 提出MRAG-Suite平台，整合多模态基准数据集，设计基于难度和模糊性的过滤策略，开发MM-RAGChecker诊断工具用于逐条诊断模型回答。

Result: 在难度大和查询模糊情况下，Visual RAG模型准确率显著下降，出现大量虚假回答（幻觉）。MM-RAGChecker能有效检测和定位这些问题。

Conclusion: MRAG-Suite和MM-RAGChecker提高了多模态检索生成模型评估的系统性和准确性，为未来优化视觉RAG模型提供了重要工具。

Abstract: Multimodal Retrieval-Augmented Generation (Visual RAG) significantly advances
question answering by integrating visual and textual evidence. Yet, current
evaluations fail to systematically account for query difficulty and ambiguity.
We propose MRAG-Suite, a diagnostic evaluation platform integrating diverse
multimodal benchmarks (WebQA, Chart-RAG, Visual-RAG, MRAG-Bench). We introduce
difficulty-based and ambiguity-aware filtering strategies, alongside
MM-RAGChecker, a claim-level diagnostic tool. Our results demonstrate
substantial accuracy reductions under difficult and ambiguous queries,
highlighting prevalent hallucinations. MM-RAGChecker effectively diagnoses
these issues, guiding future improvements in Visual RAG systems.

</details>


### [128] [SimuHome: A Temporal- and Environment-Aware Benchmark for Smart Home LLM Agents](https://arxiv.org/abs/2509.24282)
*Gyuhyeon Seo,Jungwoo Yang,Junseong Pyo,Nalim Kim,Jonggeun Lee,Yohan Jo*

Main category: cs.CL

TL;DR: 本文介绍了一个名为SimuHome的智能家居模拟环境，用于训练和评估具备复杂任务处理能力的大型语言模型代理。研究发现现有模型在处理潜在意图推断和时间调度等方面表现不足。


<details>
  <summary>Details</summary>
Motivation: 智能家居任务复杂，涉及潜在用户意图、时间依赖、设备约束和调度等，缺少真实模拟环境和有效评测基准限制了智能家居代理的发展。

Method: 构建基于Matter协议的SimuHome仿真环境，支持智能设备模拟、API调用及环境变量变化，并设计涵盖12种用户查询类型的600个评测场景。采用统一的ReAct框架对11个模型进行测试。

Result: 模型在简单任务中表现良好，但在潜在意图推断、状态验证及时间调度方面表现较差，顶级模型GPT-4.1成功率仅为54%。

Conclusion: 智能家居代理需提升通过工具验证状态和协调时间相关动作的能力，强调开发更可靠的方法以应对复杂智能家居场景。

Abstract: Large Language Model (LLM) agents excel at multi-step, tool-augmented tasks.
However, smart homes introduce distinct challenges, requiring agents to handle
latent user intents, temporal dependencies, device constraints, scheduling, and
more. The main bottlenecks for developing smart home agents with such
capabilities include the lack of a realistic simulation environment where
agents can interact with devices and observe the results, as well as a
challenging benchmark to evaluate them. To address this, we introduce
$\textbf{SimuHome}$, a time-accelerated home environment that simulates smart
devices, supports API calls, and reflects changes in environmental variables.
By building the simulator on the Matter protocol (the global industry standard
for smart home communication), SimuHome provides a high-fidelity environment,
and agents validated in SimuHome can be deployed on real Matter-compliant
devices with minimal adaptation. We provide a challenging benchmark of 600
episodes across twelve user query types that require the aforementioned
capabilities. Our evaluation of 11 agents under a unified ReAct framework
reveals that while models perform well on simple tasks, they struggle with
latent intent inference, state verification, and especially temporal
scheduling. Even the top-performing model, GPT-4.1, reaches only 54% success
rate. These findings highlight a critical need for methods that can reliably
verify the current state via tools before acting and coordinate time-dependent
actions.

</details>


### [129] [Let LLMs Speak Embedding Languages: Generative Text Embeddings via Iterative Contrastive Refinement](https://arxiv.org/abs/2509.24291)
*Yu-Che Tsai,Kuan-Yu Chen,Yuan-Chi Li,Yuan-Hao Chen,Ching-Yu Tsai,Shou-De Lin*

Main category: cs.CL

TL;DR: 本文提出了一种基于生成迭代优化的句子表征方法GIRCSE，通过自回归生成不断细化语义表示，显著优于现有基于大型语言模型的编码器方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大型语言模型的句子嵌入方法普遍采用编码器单一范式，忽视了生成模型的核心优势，导致捕捉语义的能力受限。

Method: 提出GIRCSE框架，通过自回归生成软性Token序列并结合对比学习目标进行迭代优化，设计了迭代对比优化目标（ICR）引导每一步迭代产生更优表征。

Result: 在MTEB基准和指令跟随任务中，GIRCSE显著优于强大的基于LLM的嵌入基线，并展示了生成长度增加能持续提升表征质量的特性。

Conclusion: 生成式迭代优化为表示学习开辟了新范式，充分发挥了大型语言模型的生成潜力，提升了表征能力。

Abstract: Existing large language model (LLM)-based embeddings typically adopt an
encoder-only paradigm, treating LLMs as static feature extractors and
overlooking their core generative strengths. We introduce GIRCSE (Generative
Iterative Refinement for Contrastive Sentence Embeddings), a novel framework
that leverages autoregressive generation to iteratively refine semantic
representations. By producing sequences of soft tokens optimized under
contrastive objective, GIRCSE captures latent concepts and implicit semantics
that encoder-only methods often miss. To guide this process, we propose an
Iterative Contrastive Refinement (ICR) objective that encourages each
refinement step to yield better representations. Extensive experiments show
that GIRCSE outperforms strong LLM-based embedding baselines on the MTEB
benchmark and instruction-following tasks. Moreover, GIRCSE exhibits an
emergent test-time scaling property: generating more tokens at inference
steadily improves embedding quality. Our results establish generative iterative
refinement as a new paradigm for representation learning.

</details>


### [130] [LOGOS: LLM-driven End-to-End Grounded Theory Development and Schema Induction for Qualitative Research](https://arxiv.org/abs/2509.24294)
*Xinyu Pi,Qisen Yang,Chuong Nguyen*

Main category: cs.CL

TL;DR: LOGOS是一个全自动的扎根理论框架，无需人工编码即可从文本中生成结构化理论，显著提升了定性研究的规模和效率。


<details>
  <summary>Details</summary>
Motivation: 扎根理论依赖专家手工编码，存在规模瓶颈，现有工具未实现真正自动化，限制了定性研究的推广与效率。

Method: LOGOS结合大语言模型自动编码、语义聚类、图推理及迭代细化，构建高复用的编码本；引入五维度指标及训练测试分割协议，实现标准化无偏评估。

Result: LOGOS在五个不同语料库上超越强基线方法，在复杂数据集上与专家编码方案达到88.2%的高度一致。

Conclusion: LOGOS为定性研究自动化提供了强大途径，有效促进了扎根理论在广泛领域的民主化和规模化应用，同时保持理论深度。

Abstract: Grounded theory offers deep insights from qualitative data, but its reliance
on expert-intensive manual coding presents a major scalability bottleneck.
Current computational tools stop short of true automation, keeping researchers
firmly in the loop. We introduce LOGOS, a novel, end-to-end framework that
fully automates the grounded theory workflow, transforming raw text into a
structured, hierarchical theory. LOGOS integrates LLM-driven coding, semantic
clustering, graph reasoning, and a novel iterative refinement process to build
highly reusable codebooks. To ensure fair comparison, we also introduce a
principled 5-dimensional metric and a train-test split protocol for
standardized, unbiased evaluation. Across five diverse corpora, LOGOS
consistently outperforms strong baselines and achieves a remarkable $88.2\%$
alignment with an expert-developed schema on a complex dataset. LOGOS
demonstrates a powerful new path to democratize and scale qualitative research
without sacrificing theoretical nuance.

</details>


### [131] [DiffuGuard: How Intrinsic Safety is Lost and Found in Diffusion Large Language Models](https://arxiv.org/abs/2509.24296)
*Zherui Li,Zheng Nie,Zhenhong Zhou,Yufei Guo,Yue Liu,Yitong Zhang,Yu Cheng,Qingsong Wen,Kun Wang,Jiaheng Zhang*

Main category: cs.CL

TL;DR: 本文分析了扩散大型语言模型（dLLMs）在越狱攻击上的脆弱性，提出了一种训练无关的防御框架DiffuGuard，有效降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: dLLMs的生成机制使其面临与自回归LLMs不同的安全风险，现有策略存在明显漏洞且早期生成标记的安全性决定最终输出，亟需新的防御方法。

Method: 提出DiffuGuard框架，包括随机退火重掩码缓解贪婪策略偏差，以及块级审计和修复利用内部模型表征实现风险检测与自动修正。

Result: 在四个dLLMs上的实验证明，DiffuGuard将六种越狱方法的攻击成功率从47.9%降至14.7%，同时保持模型效用和效率。

Conclusion: DiffuGuard成功释放了dLLMs的内在安全潜力，为该类模型的安全防护提供了有效且高效的解决方案。

Abstract: The rapid advancement of Diffusion Large Language Models (dLLMs) introduces
unprecedented vulnerabilities that are fundamentally distinct from
Autoregressive LLMs, stemming from their iterative and parallel generation
mechanisms. In this paper, we conduct an in-depth analysis of dLLM
vulnerabilities to jailbreak attacks across two distinct dimensions: intra-step
and inter-step dynamics. Experimental results reveal a harmful bias inherent in
the standard greedy remasking strategy and identify a critical phenomenon we
term Denoising-path Dependence, where the safety of early-stage tokens
decisively influences the final output. These findings also indicate that while
current decoding strategies constitute a significant vulnerability, dLLMs
possess a substantial intrinsic safety potential. To unlock this potential, we
propose DiffuGuard, a training-free defense framework that addresses
vulnerabilities through a dual-stage approach: Stochastic Annealing Remasking
dynamically introduces controlled randomness to mitigate greedy selection bias,
while Block-level Audit and Repair exploits internal model representations for
autonomous risk detection and guided correction. Comprehensive experiments on
four dLLMs demonstrate DiffuGuard's exceptional effectiveness, reducing Attack
Success Rate against six diverse jailbreak methods from 47.9% to 14.7% while
preserving model utility and efficiency. Our code is available at:
https://github.com/niez233/DiffuGuard.

</details>


### [132] [Q-Mirror: Unlocking the Multi-Modal Potential of Scientific Text-Only QA Pairs](https://arxiv.org/abs/2509.24297)
*Junying Wang,Zicheng Zhang,Ye Shen,Yalun Wu,Yingji Liang,Yijin Guo,Farong Wen,Wenzhe Li,Xuezhi Zhao,Qi Jia,Guangtao Zhai*

Main category: cs.CL

TL;DR: 本文提出了一种将文本问答对转换为多模态问答对的框架，并构建了相应的质量评价标准和基准数据集，通过设计闭环代理系统Q-Mirror实现多模态问答对的生成与评估，显著提升了问答对的质量和通过率。


<details>
  <summary>Details</summary>
Motivation: 高质量多模态基准对于推动大型模型中的科学推理至关重要，但手工创建成本高且难以扩展，因此探索自动将文本问答对转化为多模态问答对成为解决瓶颈的切入点。

Method: 提出了TQA到MMQA的转换框架，制定多维度质量评价标准，构建两个大规模基准用于生成与评估任务，设计Q-Mirror系统实现多模态问答对的生成和评价闭环迭代优化。

Result: 实验证明当前最先进模型虽然能生成多模态问答对，但质量仍有不足，顶尖理解模型在质量评估上与人类判准高度一致，Q-Mirror系统显著提升了问答对的平均分和通过率。

Conclusion: 利用转换框架和闭环代理系统，可以有效生成高质量多模态问答对，为构建大规模科学多模态基准提供了实用的路径和工具。

Abstract: High-quality, multi-modal benchmarks are crucial for advancing scientific
reasoning in large models yet their manual creation is costly and unscalable.
To address this bottleneck, we explore the potential for transforming Text-Only
QA Pairs (TQAs) into high-quality Multi-Modal QA Pairs (MMQAs), which include
three parts: 1) Task Definition \& Evaluation Rubric: We develop a TQA-to-MMQA
framework and establish a comprehensive, multi-dimensional MMQA quality rubric
that provides principles for the transformation. 2) Benchmark Construction:
Then we construct two extensive benchmarks to rigorously evaluate
state-of-the-art generation \& understanding models on the distinct tasks of
MMQA generation \& MMQA quality evaluation. 3) Preliminary Solution: We develop
an agentic system (Q-Mirror), which operationalizes our framework by
integrating MMQA generation and evaluation into a closed loop for iterative
refinement. Our experiments show that while state-of-the-art models can
generate MMQAs, their outputs still leave substantial gaps, underscoring the
need for reliable evaluation. We further demonstrate that top-tier
understanding models align closely with human judgment in MMQA quality
assessment. Leveraging both insights, the Q-Mirror agent raises average scores
from 78.90 to 85.22 and pass rates from 72\% to 95\%, offering a practical path
to large-scale scientific benchmarks.

</details>


### [133] [Dual Mechanisms of Value Expression: Intrinsic vs. Prompted Values in LLMs](https://arxiv.org/abs/2509.24319)
*Jongwook Han,Jongwon Lim,Injin Kong,Yohan Jo*

Main category: cs.CL

TL;DR: 本文研究了大语言模型内在价值表达与通过提示词引导的价值表达的机制差异及其影响。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在价值对齐与角色设定中的广泛应用，理解其价值表达机制的异同极为重要。

Method: 通过提取残差流中的价值向量和识别多层感知机中的价值神经元，系统分析内在表达和提示表达的共享与独特成分。

Result: 发现两种机制部分共享关键组件，同时存在独特成分；提示表达的价值可控性更强，内在表达产出响应更具词汇多样性。

Conclusion: 内在与提示机制虽有交集，但各自特有部分分别促进响应多样性和指令遵循性，影响LLM的价值表达与操控能力。

Abstract: Large language models (LLMs) can express different values in two distinct
ways: (1) intrinsic expression, reflecting the model's inherent values learned
during training, and (2) prompted expression, elicited by explicit prompts.
Given their widespread use in value alignment and persona steering, it is
paramount to clearly understand their underlying mechanisms, particularly
whether they mostly overlap (as one might expect) or rely on substantially
different mechanisms, but this remains largely understudied. We analyze this at
the mechanistic level using two approaches: (1) value vectors, feature
directions representing value mechanisms extracted from the residual stream,
and (2) value neurons, MLP neurons that contribute to value expressions. We
demonstrate that intrinsic and prompted value mechanisms partly share common
components that are crucial for inducing value expression, but also possess
unique elements that manifest in different ways. As a result, these mechanisms
lead to different degrees of value steerability (prompted > intrinsic) and
response diversity (intrinsic > prompted). In particular, components unique to
the intrinsic mechanism seem to promote lexical diversity in responses, whereas
those specific to the prompted mechanism primarily strengthen instruction
following, taking effect even in distant tasks like jailbreaking.

</details>


### [134] [Multimodal Large Language Models Meet Multimodal Emotion Recognition and Reasoning: A Survey](https://arxiv.org/abs/2509.24322)
*Yuntao Shou,Tao Meng,Wei Ai,Keqin Li*

Main category: cs.CL

TL;DR: 该论文综述了大语言模型(LLMs)和多模态大语言模型(MLLMs)在情感识别与推理领域的最新进展，涵盖模型架构、数据集和性能评估，指出了关键挑战并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着对高级语义理解和跨模态融合的需求增长，多模态大语言模型在情感识别与推理中展现出巨大潜力，但该领域缺乏系统性的综述和整合。

Method: 本文通过系统调查LLMs和MLLMs在多模态情感识别与推理中的应用，整理相关模型架构、系列数据集及性能基准，综合分析现有研究进展。

Result: 论文总结了该领域内主要模型和技术路线，提供了全面的性能比较和评估，并指出当前研究中的主要挑战。

Conclusion: 该论文首次系统性综述了多模态大语言模型在情感识别与推理中的应用，旨在为研究者提供权威参考和实践指导，推动该领域的发展。

Abstract: In recent years, large language models (LLMs) have driven major advances in
language understanding, marking a significant step toward artificial general
intelligence (AGI). With increasing demands for higher-level semantics and
cross-modal fusion, multimodal large language models (MLLMs) have emerged,
integrating diverse information sources (e.g., text, vision, and audio) to
enhance modeling and reasoning in complex scenarios. In AI for Science,
multimodal emotion recognition and reasoning has become a rapidly growing
frontier. While LLMs and MLLMs have achieved notable progress in this area, the
field still lacks a systematic review that consolidates recent developments. To
address this gap, this paper provides a comprehensive survey of LLMs and MLLMs
for emotion recognition and reasoning, covering model architectures, datasets,
and performance benchmarks. We further highlight key challenges and outline
future research directions, aiming to offer researchers both an authoritative
reference and practical insights for advancing this domain. To the best of our
knowledge, this paper is the first attempt to comprehensively survey the
intersection of MLLMs with multimodal emotion recognition and reasoning. The
summary of existing methods mentioned is in our Github:
\href{https://github.com/yuntaoshou/Awesome-Emotion-Reasoning}{https://github.com/yuntaoshou/Awesome-Emotion-Reasoning}.

</details>


### [135] [Speculative Verification: Exploiting Information Gain to Refine Speculative Decoding](https://arxiv.org/abs/2509.24328)
*Sungkyun Kim,Jaemin Kim,Dogyung Yoon,Jiho Shin,Junyeol Lee,Jiwon Seo*

Main category: cs.CL

TL;DR: 本文提出了一种名为Speculative Verification（SV）的技术，通过引入辅助模型动态预测投机准确率，改进推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有的推测解码（Speculative Decoding, SD）技术因投机准确率低导致拒绝令牌带来的计算浪费，限制了其效率提升，尤其在大批量处理时表现不佳。

Method: SV引入一个与草稿模型相似规模的辅助模型，用于估计草稿模型与目标模型输出分布的一致性，从而动态调整验证长度，最大化吞吐量，减少拒绝令牌的计算浪费。

Result: 在公开大型语言模型及不同任务组合下，SV相比原有的SD及标准解码均有显著提升，最高达到2倍性能提升，并在大批量（32-80）环境下平均加速1.4倍。

Conclusion: SV无需改动草稿模型和目标模型，兼容现有的SD变体，展示了其在推理效率上的鲁棒性、可扩展性和应用价值。

Abstract: LLMs have low GPU efficiency and high latency due to autoregressive decoding.
Speculative decoding (SD) mitigates this using a small draft model to
speculatively generate multiple tokens, which are then verified in parallel by
a target model. However, when speculation accuracy is low, the overhead from
rejected tokens can offset the benefits, limiting SD's effectiveness,
especially at large batch sizes. To address this, we propose Speculative
Verification (SV), an efficient augmentation to SD that dynamically predicts
speculation accuracy and adapts the verification length to maximize throughput.
SV introduces a companion model - a small auxiliary model similar in size to
the draft model - to estimate the alignment between draft and target model
distributions. By maximizing the information gain from quantifying this
alignment, SV refines verification decisions, reducing wasted computation on
rejected tokens and improving decoding efficiency. Moreover, SV requires no
modifications to the draft or target models and is compatible with existing SD
variants. We extensively evaluated SV on publicly available LLMs across three
NLP tasks using nine combinations of draft, companion, and target models,
including 13B-72B target models and three types of variations: base (no
finetuning), instruction-tuned, and task fine-tuned. Across all experiments and
batch sizes (4-80), SV consistently outperforms both SD and standard decoding
with the target model. It improves SD performance by up to 2$\times$, with an
average speedup of 1.4 $\times$ in large-batch settings (batch sizes 32-80).
These results demonstrate SV's robustness, scalability, and practical utility
for efficient LLM inference.

</details>


### [136] [AlignX: Advancing Multilingual Large Language Models with Multilingual Representation Alignment](https://arxiv.org/abs/2509.24338)
*Mengyu Bu,Shaolei Zhang,Zhongjun He,Hua Wu,Yang Feng*

Main category: cs.CL

TL;DR: 本文提出了AlignX框架，通过两阶段方法提升多语言大语言模型的跨语种性能和对齐效果。


<details>
  <summary>Details</summary>
Motivation: 当前多语言大语言模型在非主流语言上的性能和跨语种对齐存在不足，传统大规模多语料微调方法效果有限。

Method: AlignX包括两个阶段：第一阶段通过多语言语义对齐和语言特征整合对表示进行对齐；第二阶段通过多语言指令微调提升模型多语言能力。

Result: 在多种预训练大语言模型上实验表明，AlignX显著提升了模型的多语言生成和跨语种生成能力。

Conclusion: AlignX有效促进多语言表示的紧密结合，优化了跨语种对齐，弥合了多语言性能差距。

Abstract: Multilingual large language models (LLMs) possess impressive multilingual
understanding and generation capabilities. However, their performance and
cross-lingual alignment often lag for non-dominant languages. A common solution
is to fine-tune LLMs on large-scale and more balanced multilingual corpus, but
such approaches often lead to imprecise alignment and suboptimal knowledge
transfer, struggling with limited improvements across languages. In this paper,
we propose AlignX to bridge the multilingual performance gap, which is a
two-stage representation-level framework for enhancing multilingual performance
of pre-trained LLMs. In the first stage, we align multilingual representations
with multilingual semantic alignment and language feature integration. In the
second stage, we stimulate the multilingual capability of LLMs via multilingual
instruction fine-tuning. Experimental results on several pre-trained LLMs
demonstrate that our approach enhances LLMs' multilingual general and
cross-lingual generation capability. Further analysis indicates that AlignX
brings the multilingual representations closer and improves the cross-lingual
alignment.

</details>


### [137] [Beyond Repetition: Text Simplification and Curriculum Learning for Data-Constrained Pretraining](https://arxiv.org/abs/2509.24356)
*Matthew Theodore Roque,Dan John Velasco*

Main category: cs.CL

TL;DR: 研究探讨在有限数据条件下，采用课程学习中的文本复杂度排序和简化数据增强对语言模型预训练的影响。


<details>
  <summary>Details</summary>
Motivation: 现有大多数语言模型预训练研究集中于大规模数据，在数据有限时优化策略尚不清楚，特别是训练数据顺序和替代文本版本的效果未充分研究。

Method: 使用人类撰写段落与大型语言模型简化版本组成的平行语料，测试四种数据排序策略（重复暴露、低到高复杂度、高到低复杂度和交错排序），并通过微调和零-shot任务评估模型的表示质量。

Result: 简化数据的加入提升了微调和零-shot性能，相较于重复暴露基线，小型模型更适合低到高复杂度排序，大型模型则在交错排序下表现更优。

Conclusion: 简化文本和合理的数据排序策略能在数据受限条件下有效提升语言模型的表示能力，小型和大型模型应采用不同的排序方式以获得最佳性能。

Abstract: Most studies on language model pretraining focus on large datasets, leaving
open questions about optimization in data-constrained settings. In such
settings, the effects of training data order and of including alternative
versions of the same text remain underexplored. We address this by studying
curriculum learning in pretraining, focusing on text-complexity ordering and
data augmentation via simplification. We ask: (1) Does simplifying texts
enhance representation quality more than reusing the original data? and (2)
Does ordering data by text complexity yield better representations? To answer,
we build on a pair of parallel corpora where human-written paragraphs are
aligned with LLM-simplified variants, and test four data schedules: repeated
exposure, low-to-high complexity, high-to-low, and interleaved. We analyze
models' representation quality from a sample efficiency perspective via
fine-tuning, as well as its zero-shot performance on linguistic knowledge,
entity tracking, world knowledge, and commonsense reasoning. Our findings show
that adding simplified data improves fine-tuning and zero-shot performance over
a repeated-exposure baseline: smaller models benefit from low-to-high
complexity, while larger models perform better with interleaved ordering.

</details>


### [138] [Reinforcement Mid-Training](https://arxiv.org/abs/2509.24375)
*Yijun Tian,Shaoyu Chen,Zhichao Xu,Yawei Wang,Jinhe Bi,Peng Han,Wei Wang*

Main category: cs.CL

TL;DR: 论文提出一种称为RMT的强化中期训练框架，解决大型语言模型训练中的效率低下、信息利用不足等问题，实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型训练仅包含预训练和后训练阶段，缺乏中间的强化训练阶段，导致训练效率低和模型过度思考等问题。

Method: 引入动态token预算机制限制推理步骤，采用基于课程的自适应采样实现由易到难的学习路径，并设计融合强化学习与下一个token预测的双重训练策略。

Result: RMT在语言建模任务中，推理长度仅为21%时性能提升高达64.91%；中期强化训练获得的模型检查点在后训练中提升数学领域性能达18.76%。

Conclusion: 强化中期训练是连接预训练与后训练的重要环节，能有效提升模型表现与训练效率，具有广泛应用潜力。

Abstract: The development of state-of-the-art large language models is commonly
understood as a two-stage process involving pre-training and post-training. We
point out the need for an additional intermediate stage called reinforcement
mid-training with potential for strong performance gains. In this paper, we
formally define the problem and identify three key challenges: (1) inefficient
training due to excessive reasoning steps, (2) disregard of the imbalanced
token entropy distribution, and (3) underutilization of token information. To
address these challenges, we propose RMT, a framework for efficient, adaptive,
and unified reinforcement mid-training with various innovative components. In
particular, we first introduce a dynamic token budget mechanism that constrains
unnecessary reasoning steps and mitigates model overthinking. Next, we design a
curriculum-based adaptive sampling method that fosters a progressive learning
trajectory from easy to hard tokens. Finally, we present a dual training
strategy that combines reinforcement learning with next-token prediction,
ensuring targeted learning on key tokens and full exploitation of all token
information. Extensive experiments demonstrate the superiority of RMT over
state-of-the-art methods, achieving up to +64.91% performance improvement with
only 21% of the reasoning length in language modeling. We also show that
checkpoints obtained after reinforcement mid-training can benefit the
subsequent post-training, yielding up to +18.76% improvement in the
mathematical domain.

</details>


### [139] [HarmMetric Eval: Benchmarking Metrics and Judges for LLM Harmfulness Assessment](https://arxiv.org/abs/2509.24384)
*Langqi Yang,Tianhang Zheng,Kedong Xiu,Yixuan Chen,Di Wang,Puning Zhao,Zhan Qin,Kui Ren*

Main category: cs.CL

TL;DR: 本文提出了HarmMetric Eval基准，用于系统评估评估LLM输出有害性的各种指标和评判标准。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏统一的基准来评估不同衡量LLM有害输出的方法，导致缺乏可信的比较。

Method: 构建了包含代表性有害提示和多样化模型回应的数据集，配合灵活的评分机制，支持多种指标和评判工具的评价。

Result: 实验发现传统指标METEOR和ROUGE-1在判断模型输出有害性方面优于基于LLM的判断者，挑战了LLM在该领域的优势认知。

Conclusion: HarmMetric Eval为有害性评估提供了系统性工具和数据支持，推动该领域研究和评估的规范化。

Abstract: The alignment of large language models (LLMs) with human values is critical
for their safe deployment, yet jailbreak attacks can subvert this alignment to
elicit harmful outputs from LLMs. In recent years, a proliferation of jailbreak
attacks has emerged, accompanied by diverse metrics and judges to assess the
harmfulness of the LLM outputs. However, the absence of a systematic benchmark
to assess the quality and effectiveness of these metrics and judges undermines
the credibility of the reported jailbreak effectiveness and other risks. To
address this gap, we introduce HarmMetric Eval, a comprehensive benchmark
designed to support both overall and fine-grained evaluation of harmfulness
metrics and judges. Our benchmark includes a high-quality dataset of
representative harmful prompts paired with diverse harmful and non-harmful
model responses, alongside a flexible scoring mechanism compatible with various
metrics and judges. With HarmMetric Eval, our extensive experiments uncover a
surprising result: two conventional metrics--METEOR and ROUGE-1--outperform
LLM-based judges in evaluating the harmfulness of model responses, challenging
prevailing beliefs about LLMs' superiority in this domain. Our dataset is
publicly available at https://huggingface.co/datasets/qusgo/HarmMetric_Eval,
and the code is available at
https://anonymous.4open.science/r/HarmMetric-Eval-4CBE.

</details>


### [140] [LLaDA-MoE: A Sparse MoE Diffusion Language Model](https://arxiv.org/abs/2509.24389)
*Fengqi Zhu,Zebin You,Yipeng Xing,Zenan Huang,Lin Liu,Yihong Zhuang,Guoshan Lu,Kangyu Wang,Xudong Wang,Lanning Wei,Hongrui Guo,Jiaqi Hu,Wentao Ye,Tieyuan Chen,Chenchen Li,Chengfu Tang,Haibo Feng,Jun Hu,Jun Zhou,Xiaolu Zhang,Zhenzhong Lan,Junbo Zhao,Da Zheng,Chongxuan Li,Jianguo Li,Ji-Rong Wen*

Main category: cs.CL

TL;DR: LLaDA-MoE是一种基于Mixture-of-Experts架构的扩散大语言模型，具备7B参数容量但推理时仅激活1.4B参数，在多项基准测试中表现优于现有扩散语言模型，达到先进水平。


<details>
  <summary>Details</summary>
Motivation: 在保持模型容量的同时，减少推理计算量，提高扩散语言模型的效率和性能。

Method: 从头训练基于MoE架构的扩散大语言模型LLaDA-MoE，利用稀疏激活机制，在推理时只激活部分专家参数以降低计算开销。

Result: LLaDA-MoE在多项基准测试中超越之前的扩散模型（如LLaDA、LLaDA 1.5和Dream），其指令调优模型表现与参数更少的Qwen2.5-3B-Instruct相当，体现了MoE架构在推理效率上的优势。

Conclusion: 引入稀疏MoE架构到扩散语言模型的训练目标中，能够在推理时实现高效参数激活，同时保持模型性能，展现出MoE模型在扩散语言领域的潜力和发展空间。

Abstract: We introduce LLaDA-MoE, a large language diffusion model with the
Mixture-of-Experts (MoE) architecture, trained from scratch on approximately
20T tokens. LLaDA-MoE achieves competitive performance with significantly
reduced computational overhead by maintaining a 7B-parameter capacity while
activating only 1.4B parameters during inference. Our empirical evaluation
reveals that LLaDA-MoE achieves state-of-the-art performance among diffusion
language models with larger parameters, surpassing previous diffusion language
models LLaDA, LLaDA 1.5, and Dream across multiple benchmarks. The
instruct-tuned model LLaDA-MoE-7B-A1B-Instruct demonstrates capabilities
comparable to Qwen2.5-3B-Instruct in knowledge understanding, code generation,
mathematical reasoning, agent and alignment tasks, despite using fewer active
parameters. Our results show that integrating a sparse MoE architecture into
the training objective of masked diffusion language models still brings out
MoE's strengths under efficient inference with few active parameters, and opens
ample room for further exploration of diffusion language models. LLaDA-MoE
models are available at Huggingface.

</details>


### [141] [Agentar-Scale-SQL: Advancing Text-to-SQL through Orchestrated Test-Time Scaling](https://arxiv.org/abs/2509.24403)
*Pengfei Wang,Baolin Sun,Xuemei Dong,Yaxun Dai,Hongwei Yuan,Mengdie Chu,Yingqi Gao,Xiang Qi,Peng Zhang,Ying Yan*

Main category: cs.CL

TL;DR: Agentar-Scale-SQL通过三重测试时扩展策略显著提升Text-to-SQL性能，在BIRD基准测试中达到81.67%准确率并排名第一。


<details>
  <summary>Details</summary>
Motivation: 当前SOTA Text-to-SQL方法在复杂基准测试如BIRD上仍远落后于人类专家，且测试时扩展策略缺乏组织性且忽视模型内部推理过程。

Method: 提出Agentar-Scale-SQL框架，采用有序测试时扩展策略，包括内部通过强化学习增强的内在推理，顺序的迭代优化，及并行的多样合成与锦标赛选择三种视角的协同结合。

Result: 在BIRD测试集上取得81.67%的执行准确率，刷新SOTA纪录并在官方排行榜中排名第一。

Conclusion: Agentar-Scale-SQL有效利用可扩展计算资源，通过有序的多视角测试扩展策略实现了接近人类水平的Text-to-SQL性能，具有良好的适应性和推广潜力。

Abstract: State-of-the-art (SOTA) Text-to-SQL methods still lag significantly behind
human experts on challenging benchmarks like BIRD. Current approaches that
explore test-time scaling lack an orchestrated strategy and neglect the model's
internal reasoning process. To bridge this gap, we introduce Agentar-Scale-SQL,
a novel framework leveraging scalable computation to improve performance.
Agentar-Scale-SQL implements an Orchestrated Test-Time Scaling strategy that
synergistically combines three distinct perspectives: i) Internal Scaling via
RL-enhanced Intrinsic Reasoning, ii) Sequential Scaling through Iterative
Refinement, and iii) Parallel Scaling using Diverse Synthesis and Tournament
Selection. Agentar-Scale-SQL is a general-purpose framework designed for easy
adaptation to new databases and more powerful language models. Extensive
experiments show that Agentar-Scale-SQL achieves SOTA performance on the BIRD
benchmark, reaching 81.67\% execution accuracy on the test set and ranking
first on the official leaderboard, demonstrating an effective path toward
human-level performance.

</details>


### [142] [Multilingual Text-to-SQL: Benchmarking the Limits of Language Models with Collaborative Language Agents](https://arxiv.org/abs/2509.24405)
*Khanh Trinh Pham,Thu Huong Nguyen,Jun Jo,Quoc Viet Hung Nguyen,Thanh Tam Nguyen*

Main category: cs.CL

TL;DR: MultiSpider 2.0扩展了Spider 2.0至八种语言，提升了多语言Text-to-SQL的难度和复杂性，但当前先进模型执行准确率仍很低。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL基准大多仅支持英语，限制了多语言处理能力的发展。

Method: 构建覆盖八种语言的MultiSpider 2.0基准，同时设计协作驱动的语言代理模型，通过迭代优化SQL查询提升性能。

Result: 先进大语言模型在MultiSpider 2.0上的执行准确率仅为4%，通过协作代理模型提升至15%。

Conclusion: 多语言Text-to-SQL存在显著性能差距，需开发具备跨语言鲁棒性的模型以满足实际企业需求。

Abstract: Text-to-SQL enables natural access to databases, yet most benchmarks are
English-only, limiting multilingual progress. We introduce MultiSpider 2.0,
extending Spider 2.0 to eight languages (English, German, French, Spanish,
Portuguese, Japanese, Chinese, Vietnamese). It preserves Spider 2.0's
structural difficulty while adding linguistic and dialectal variability,
demanding deeper reasoning for complex SQL. On this benchmark, state-of-the-art
LLMs (such as DeepSeek-R1 and OpenAI o1) reach only 4\% execution accuracy when
relying on intrinsic reasoning, versus 60\% on MultiSpider 1.0. Therefore, we
provide a collaboration-driven language agents baseline that iteratively
refines queries, improving accuracy to 15\%. These results reveal a substantial
multilingual gap and motivate methods that are robust across languages and
ready for real-world enterprise deployment. Our benchmark is available at
https://github.com/phkhanhtrinh23/Multilingual_Text_to_SQL.

</details>


### [143] [CDT: A Comprehensive Capability Framework for Large Language Models Across Cognition, Domain, and Task](https://arxiv.org/abs/2509.24422)
*Haosi Mo,Xinyu Ma,Xuebo Liu,Derek F. Wong,Yu Li,Jie Liu,Min Zhang*

Main category: cs.CL

TL;DR: 本文提出了一个名为Cognition-Domain-Task (CDT) 的综合评估框架，用于全面测评大型语言模型（LLMs）的能力，超越了传统的任务特定基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有评估基准通常只聚焦于模型的孤立能力，缺乏一个全方位的综合评估体系，需要一个能够更全面测量LLMs多维能力的框架。

Method: 基于Cattell-Horn-Carroll认知理论，重新定义和细化模型能力的认知层次分类，构建CDT框架从认知、领域和任务三个维度进行能力评测，并将其应用于数据集能力评估和数据选择两方面。

Result: 实验表明，CDT的能力指标与下游任务表现高度相关，支持有效的数据集分析与构建。在数据选择任务中，CDT显著提升了基准测试成绩，分别提高1.6和2.2分。

Conclusion: CDT框架有效且实用，能帮助全面评估和提升大型语言模型的能力，对模型能力度量和数据集管理具有重要价值。

Abstract: Recent advances in Large Language Models (LLMs) have significantly enhanced
their capabilities, highlighting the need for comprehensive evaluation
frameworks that extend beyond task-specific benchmarks. However, existing
benchmarks often focus on isolated abilities, lacking a holistic framework for
assessing LLM capabilities. To address this gap, we propose the
Cognition-Domain-Task (CDT) framework, which comprehensively measures a model's
capabilities across three dimensions. We expand the scope of model capability
definitions at the cognitive level by incorporating the Cattell-Horn-Carroll
cognitive theory, refining the categorization of model capabilities. We apply
CDT in two directions: dataset capability evaluation and data selection.
Experiments show that our capability metrics correlate well with downstream
performance and can support effective dataset analysis and construction. The
experiments on data selection also show significant improvements in both
general and specific benchmarks, achieving scores of 44.3 and 45.4, with an
increase of 1.6 and 2.2 points over the baselines, respectively. These results
validate the effectiveness and practicality of CDT. Source code and models are
available at https://github.com/Alessa-mo/CDT.

</details>


### [144] [Alternatives To Next Token Prediction In Text Generation -- A Survey](https://arxiv.org/abs/2509.24435)
*Charlie Wyatt,Aditya Joshi,Flora Salim*

Main category: cs.CL

TL;DR: 本文综述了大语言模型（LLMs）基于下一词预测（NTP）范式的局限性及其替代方法，提出五类新兴方法体系。


<details>
  <summary>Details</summary>
Motivation: 当前基于下一词预测的范式虽然推动了大语言模型的成功，但存在长期规划能力差、错误积累和计算效率低下等核心问题，亟需探索替代方案。

Method: 文章将替代NTP的方法归纳为五类：1）多词预测，预测多个未来词块；2）先规划后生成，先制定全局高层次计划以指导生成；3）潜在推理，将自回归过程转移到连续潜在空间；4）连续生成方法，通过迭代并行优化替代顺序生成；5）非Transformer架构，利用不同模型结构绕开NTP。

Result: 通过整合五类方法的见解，文中建立了替代NTP方法的分类体系，有助于引导未来模型设计，克服基于词级别生成的已知缺陷。

Conclusion: 文章为未来研究提供了系统化的框架和观点，支持发展能克服传统NTP范式限制的创新性自然语言处理模型。

Abstract: The paradigm of Next Token Prediction (NTP) has driven the unprecedented
success of Large Language Models (LLMs), but is also the source of their most
persistent weaknesses such as poor long-term planning, error accumulation, and
computational inefficiency. Acknowledging the growing interest in exploring
alternatives to NTP, the survey describes the emerging ecosystem of
alternatives to NTP. We categorise these approaches into five main families:
(1) Multi-Token Prediction, which targets a block of future tokens instead of a
single one; (2) Plan-then-Generate, where a global, high-level plan is created
upfront to guide token-level decoding; (3) Latent Reasoning, which shifts the
autoregressive process itself into a continuous latent space; (4) Continuous
Generation Approaches, which replace sequential generation with iterative,
parallel refinement through diffusion, flow matching, or energy-based methods;
and (5) Non-Transformer Architectures, which sidestep NTP through their
inherent model structure. By synthesizing insights across these methods, this
survey offers a taxonomy to guide research into models that address the known
limitations of token-level generation to develop new transformative models for
natural language processing.

</details>


### [145] [Bias Mitigation or Cultural Commonsense? Evaluating LLMs with a Japanese Dataset](https://arxiv.org/abs/2509.24468)
*Taisei Yamamoto,Ryoma Kumon,Danushka Bollegala,Hitomi Yanaka*

Main category: cs.CL

TL;DR: 本文提出了一个评估大型语言模型（LLMs）社会偏见及文化常识的基准SOBACO，发现去偏方法虽能减少偏见，却显著降低模型在文化常识任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有去偏方法可能损害大型语言模型的能力，但其对与社会偏见相关的文化常识影响未被充分研究。

Method: 设计了日本语的统一评测基准SOBACO，评估多个LLMs在去偏前后的社会偏见和文化常识表现。

Result: 去偏方法导致模型在文化常识任务上的表现下降，准确率最高降低了75%。

Conclusion: 去偏方法需权衡减少偏见与保持文化常识能力，提升大型语言模型的公平性和实用性。

Abstract: Large language models (LLMs) exhibit social biases, prompting the development
of various debiasing methods. However, debiasing methods may degrade the
capabilities of LLMs. Previous research has evaluated the impact of bias
mitigation primarily through tasks measuring general language understanding,
which are often unrelated to social biases. In contrast, cultural commonsense
is closely related to social biases, as both are rooted in social norms and
values. The impact of bias mitigation on cultural commonsense in LLMs has not
been well investigated. Considering this gap, we propose SOBACO (SOcial BiAs
and Cultural cOmmonsense benchmark), a Japanese benchmark designed to evaluate
social biases and cultural commonsense in LLMs in a unified format. We evaluate
several LLMs on SOBACO to examine how debiasing methods affect cultural
commonsense in LLMs. Our results reveal that the debiasing methods degrade the
performance of the LLMs on the cultural commonsense task (up to 75% accuracy
deterioration). These results highlight the importance of developing debiasing
methods that consider the trade-off with cultural commonsense to improve
fairness and utility of LLMs.

</details>


### [146] [A Text-To-Text Alignment Algorithm for Better Evaluation of Modern Speech Recognition Systems](https://arxiv.org/abs/2509.24478)
*Lasse Borgholt,Jakob Havtorn,Christian Igel,Lars Maaløe,Zheng-Hua Tan*

Main category: cs.CL

TL;DR: 本文提出了一种结合动态规划与束搜索评分的新型对齐算法，以提升语音识别中错误的精确对齐和细粒度错误分析能力。


<details>
  <summary>Details</summary>
Motivation: 现有语音识别性能提升多来自常用词，但这些词语语义贡献有限，常规评测指标（词错误率）掩盖了罕见词、专有名词和领域特定词汇的错误，这些错误更为重要但难以被识别，故需要更精确的对齐方法以支持细粒度误差分析。

Method: 提出一种结合动态规划与束搜索评分的对齐算法，提高转录文本与参考文本间的对齐精度，特别是在单个错误层面。

Result: 新算法较传统文本对齐方法在个别错误对齐上更为准确，支持更可靠的错误分析。算法已通过PyPI公开。

Conclusion: 该对齐算法能更准确地识别语音识别中的关键错误，促进细粒度错误分析，弥补现有评测指标的不足，助力语音识别性能的深入理解和提升。

Abstract: Modern neural networks have greatly improved performance across speech
recognition benchmarks. However, gains are often driven by frequent words with
limited semantic weight, which can obscure meaningful differences in word error
rate, the primary evaluation metric. Errors in rare terms, named entities, and
domain-specific vocabulary are more consequential, but remain hidden by
aggregate metrics. This highlights the need for finer-grained error analysis,
which depends on accurate alignment between reference and model transcripts.
However, conventional alignment methods are not designed for such precision. We
propose a novel alignment algorithm that couples dynamic programming with beam
search scoring. Compared to traditional text alignment methods, our approach
provides more accurate alignment of individual errors, enabling reliable error
analysis. The algorithm is made available via PyPI.

</details>


### [147] [Sanitize Your Responses: Mitigating Privacy Leakage in Large Language Models](https://arxiv.org/abs/2509.24488)
*Wenjie Fu,Huandong Wang,Junyao Gao,Guoan Wan,Tao Jiang*

Main category: cs.CL

TL;DR: 本文提出了一种名为Self-Sanitize的LLM自我净化框架，能够实时监控并修正有害内容，显著降低隐私泄露风险，同时对延迟和资源影响极小。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在生成内容时存在引发有害或隐私泄露风险，且现有的事后过滤方法效率低且延迟高，不支持实时流式生成。

Method: 受认知心理学启发，设计了Self-Sanitize框架，包括轻量级的Self-Monitor模块实时监测高层意图和Self-Repair模块即时修正有害内容，支持token级流式处理，无需额外审查对话。

Result: 在四个大语言模型和三个隐私泄露场景下广泛实验，Self-Sanitize展示出优越的缓解性能，开销极小且不降低模型实用性。

Conclusion: Self-Sanitize为大语言模型的安全部署提供了一个实用且稳健的解决方案，能够实现实时自我净化，显著提高内容安全性和隐私保护水平。

Abstract: As Large Language Models (LLMs) achieve remarkable success across a wide
range of applications, such as chatbots and code copilots, concerns surrounding
the generation of harmful content have come increasingly into focus. Despite
significant advances in aligning LLMs with safety and ethical standards,
adversarial prompts can still be crafted to elicit undesirable responses.
Existing mitigation strategies are predominantly based on post-hoc filtering,
which introduces substantial latency or computational overhead, and is
incompatible with token-level streaming generation. In this work, we introduce
Self-Sanitize, a novel LLM-driven mitigation framework inspired by cognitive
psychology, which emulates human self-monitor and self-repair behaviors during
conversations. Self-Sanitize comprises a lightweight Self-Monitor module that
continuously inspects high-level intentions within the LLM at the token level
via representation engineering, and a Self-Repair module that performs in-place
correction of harmful content without initiating separate review dialogues.
This design allows for real-time streaming monitoring and seamless repair, with
negligible impact on latency and resource utilization. Given that
privacy-invasive content has often been insufficiently focused in previous
studies, we perform extensive experiments on four LLMs across three privacy
leakage scenarios. The results demonstrate that Self-Sanitize achieves superior
mitigation performance with minimal overhead and without degrading the utility
of LLMs, offering a practical and robust solution for safer LLM deployments.
Our code is available at the following link:
https://github.com/wjfu99/LLM_Self_Sanitize

</details>


### [148] [GRPO-MA: Multi-Answer Generation in GRPO for Stable and Efficient Chain-of-Thought Training](https://arxiv.org/abs/2509.24494)
*Hongcheng Wang,Yinuo Huang,Sukai Wang,Guanghui Ren,Hao Dong*

Main category: cs.CL

TL;DR: 本文提出了GRPO-MA方法，通过多答案生成缓解GRPO算法在链式思维训练中的梯度耦合、稀疏奖励信号和优势估计不稳定问题，显著提升了性能和训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有的GRPO算法在使用强化学习训练大模型的链式思维时存在梯度耦合、稀疏奖励和优势估计不稳定的挑战，限制了训练效果和效率。

Method: 提出GRPO-MA，通过从每个思考过程生成多个答案来分散梯度，提高优化的稳定性和效率。理论证明增加答案数量可降低思考优势的方差，实验证实梯度震荡减少。

Result: 在数学、代码及多模态任务中，GRPO-MA显著提升了模型性能和训练效率，且消融实验显示答案数量增加对性能提升有持续正向作用。

Conclusion: 通过多答案生成策略，GRPO-MA成功解决了GRPO的主要挑战，实现了更稳定高效的链式思维训练，为强化学习优化大模型提供了有效路径。

Abstract: Recent progress, such as DeepSeek-R1, has shown that the GRPO algorithm, a
Reinforcement Learning (RL) approach, can effectively train Chain-of-Thought
(CoT) reasoning in Large Language Models (LLMs) and Vision-Language Models
(VLMs). In this paper, we analyze three challenges of GRPO: gradient coupling
between thoughts and answers, sparse reward signals caused by limited parallel
sampling, and unstable advantage estimation. To mitigate these challenges, we
propose GRPO-MA, a simple yet theoretically grounded method that leverages
multi-answer generation from each thought process, enabling more robust and
efficient optimization. Theoretically, we show that the variance of thought
advantage decreases as the number of answers per thought increases.
Empirically, our gradient analysis confirms this effect, showing that GRPO-MA
reduces gradient spikes compared to GRPO. Experiments on math, code, and
diverse multimodal tasks demonstrate that GRPO-MA substantially improves
performance and training efficiency. Our ablation studies further reveal that
increasing the number of answers per thought consistently enhances model
performance.

</details>


### [149] [Knowledge Editing with Subspace-Aware Key-Value Mappings](https://arxiv.org/abs/2509.24502)
*Haewon Park,Sangwoo Kim,Yohan Jo*

Main category: cs.CL

TL;DR: 提出了一种名为SUIT的知识编辑方法，通过仅修改关键特征子空间，有效纠正语言模型中的事实错误，同时保持模型的知识不被过度干扰。


<details>
  <summary>Details</summary>
Motivation: 现有基于输入输出向量映射的知识编辑方法缺乏对向量的约束，导致模型被大幅扰动，影响知识保存。

Method: SUIT方法通过识别并仅修改与知识编辑相关的关键子空间，避免了对模型其他部分的影响。

Result: 在LLaMA-3-8B、GPT-J-6B和Qwen2.5-7B模型上的实验显示，SUIT在保持高效编辑的同时，大幅提升了知识保存效果。

Conclusion: SUIT成功定位了编辑所需的关键子空间，显著减少模型扰动，提高了知识编辑的有效性和稳健性。

Abstract: Knowledge editing aims to efficiently correct factual errors in Language
Models (LMs). The popular locate-then-edit approach modifies an MLP layer by
finding an optimal mapping between its input vector (key) and output vector
(value) that leads to the expression of the edited knowledge. However, existing
methods without any constraints on the key and value vectors cause significant
perturbations to the edited model. To address this, we propose Subspace
Knowledge Edit (SUIT), a method that identifies and modifies only the subspace
of critical features relevant to the edit. Our empirical results on LLaMA-3-8B,
GPT-J-6B, and Qwen2.5-7B models show that SUIT dramatically improves knowledge
preservation over strong baselines while maintaining high edit efficacy. This
effectiveness confirms that SUIT successfully identifies the critical subspace
for the edit. Further analyses provide additional validation for our approach.
The source code and data will be released to the public upon publication of the
paper.

</details>


### [150] [Building Benchmarks from the Ground Up: Community-Centered Evaluation of LLMs in Healthcare Chatbot Settings](https://arxiv.org/abs/2509.24506)
*Hamna,Gayatri Bhat,Sourabrata Mukherjee,Faisal Lalani,Evan Hadfield,Divya Siddarth,Kalika Bali,Sunayana Sitaram*

Main category: cs.CL

TL;DR: 本文提出了Samiksha，一种与民间社会组织和社区成员共同创立的社区驱动评估管道，用于评估大语言模型在医疗领域中对社区健康问题的处理能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的评估多基于通用或特定领域的基准测试，缺乏对终端用户现实生活的关注，特别是在医疗等关键领域，需要更加贴近社区实际需求和文化背景的评估方法。

Method: 构建了一个文化敏感、社区驱动的自动化评估管道，社区反馈指导评估内容、基准构建及输出评分，确保评估结果对社区需求和文化实践具有实际反映。

Result: 通过在印度医疗健康领域的应用，展示了多语言大语言模型处理细致社区健康查询的能力，同时验证了这种评估管道具备良好的可扩展性和包容性。

Conclusion: 该社区驱动的评估方法为大语言模型提供了一个更具文化和语境贴近性的评估途径，从而促进了模型在关键领域的实用性和可靠性提升。

Abstract: Large Language Models (LLMs) are typically evaluated through general or
domain-specific benchmarks testing capabilities that often lack grounding in
the lived realities of end users. Critical domains such as healthcare require
evaluations that extend beyond artificial or simulated tasks to reflect the
everyday needs, cultural practices, and nuanced contexts of communities. We
propose Samiksha, a community-driven evaluation pipeline co-created with
civil-society organizations (CSOs) and community members. Our approach enables
scalable, automated benchmarking through a culturally aware, community-driven
pipeline in which community feedback informs what to evaluate, how the
benchmark is built, and how outputs are scored. We demonstrate this approach in
the health domain in India. Our analysis highlights how current multilingual
LLMs address nuanced community health queries, while also offering a scalable
pathway for contextually grounded and inclusive LLM evaluation.

</details>


### [151] [AdaThink-Med: Medical Adaptive Thinking with Uncertainty-Guided Length Calibration](https://arxiv.org/abs/2509.24560)
*Shaohao Rui,Kaitao Chen,Weijie Ma,Xiaosong Wang*

Main category: cs.CL

TL;DR: 本文提出了AdaThink-Med，一种用于医疗大语言模型的自适应思考框架，通过不确定性引导的推理长度校准，实现根据问题难度动态调整推理长度，显著降低推理成本同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有医疗大语言模型在推理时普遍采取固定或长时间推理，导致推理成本高且效率低，缺乏根据问题难度调整推理长度的端到端方法。

Method: AdaThink-Med框架生成多个候选答案，评估其正确性和不确定性，通过不确定性引导的长度校准模块判断问题难度，针对易题惩罚长推理路径，难题则鼓励延长推理链。

Result: 在六个公开医疗问答基准上，AdaThink-Med在保持性能几乎不变的情况下，实现了平均最高6.4倍的推理长度缩减。

Conclusion: AdaThink-Med实现了医疗推理模型的自适应思考能力，能够根据问题自动抑制冗余推理过程，兼顾推理效率与性能，是实际应用中的有效方案。

Abstract: Recent advances in inference time scaling with extended long chain-of thought
have significantly improved the reasoning capabilities of both general and
medical large language models (LLMs). However, these models tend to engage in
lengthy reasoning processes regardless of the difficulty of the input question,
leading to increased inference costs in real-world applications. Therefore,
enabling adaptive thinking where models think less for simpler questions and
think more for complex ones is critical for the effective use of medical LLMs
in practice. Despite its importance, there is a lack of end-to-end approaches
designed to enhance the adaptive thinking capabilities of medical LLMs while
providing a comprehensive examination of the trade-off between performance and
computational cost. To bridge this gap, we propose AdaThink-Med, the first
end-to-end framework designed to enhance adaptive thinking ability in medical
reasoning models with uncertainty-guided length calibration. AdaThink-Med first
generates multiple candidate outputs for each question, evaluates the
correctness and uncertainty of each candidate, and then estimates problem
difficulty via an uncertainty-guided length calibration module. For outputs
with low difficulty and correct answers, the framework penalizes longer
reasoning paths; whereas for those with high difficulty and incorrect answers,
it encourages extending the chain of thought to explore alternative solutions.
On six public medical QA benchmarks, AdaThink-Med achieves up to 6.4x length
reduction on average while retaining performance with only minimal degradation.
Intriguingly, we observe that AdaThink-Med spontaneously develops two distinct
reasoning modes, which we characterize as "non-thinking" and "thinking",
demonstrating the model's ability to suppress redundant reasoning processes
dynamically.

</details>


### [152] [Inducing Dyslexia in Vision Language Models](https://arxiv.org/abs/2509.24597)
*Melika Honarmand,Ayati Sharma,Badr AlKhamissi,Johannes Mehrer,Martin Schrimpf*

Main category: cs.CL

TL;DR: 本研究通过大规模视觉语言模型模拟阅读障碍，揭示词形加工区域功能障碍导致阅读困难的机制。


<details>
  <summary>Details</summary>
Motivation: 传统行为和神经影像学方法难以测试阅读障碍的因果机制，亟需新的模型工具。

Method: 利用视觉语言模型识别并扰动词形加工相关单元，模拟阅读障碍的神经机制。

Result: 针对性破坏模型中词形加工单元导致选择性阅读障碍，保持其他视觉及语言理解能力正常，模拟出与阅读障碍者相似的语音缺陷。

Conclusion: 所建立的计算模型有效复制了阅读障碍的关键特征，为研究阅读障碍机制提供新的框架。

Abstract: Dyslexia, a neurodevelopmental disorder characterized by persistent reading
difficulties, is often linked to reduced activity of the visual word form area
in the ventral occipito-temporal cortex. Traditional approaches to studying
dyslexia, such as behavioral and neuroimaging methods, have provided valuable
insights but remain limited in their ability to test causal hypotheses about
the underlying mechanisms of reading impairments. In this study, we use
large-scale vision-language models (VLMs) to simulate dyslexia by functionally
identifying and perturbing artificial analogues of word processing. Using
stimuli from cognitive neuroscience, we identify visual-word-form-selective
units within VLMs and demonstrate that targeted ablation of these units, unlike
ablation of random units, leads to selective impairments in reading tasks while
general visual and language comprehension abilities remain intact. In
particular, the resulting model matches dyslexic humans' phonological deficits
without a significant change in orthographic processing. Taken together, our
modeling results replicate key characteristics of dyslexia and establish a
computational framework for investigating reading disorders.

</details>


### [153] [HiKE: Hierarchical Evaluation Framework for Korean-English Code-Switching Speech Recognition](https://arxiv.org/abs/2509.24613)
*Gio Paik,Yongbeom Kim,Soungmin Lee,Sangmin Ahn,Chanwoo Kim*

Main category: cs.CL

TL;DR: 提出了HiKE，这是首个面向韩英混合语音识别的评测框架，包含高质量自然混合语料和多层次混合语言标注，支持多语言语音模型的系统评估。


<details>
  <summary>Details</summary>
Motivation: 现有多语言自动语音识别技术进展迅速，但对日常对话中常见的语言混合现象（代码切换）研究不足，尤其是韩英混合语。

Method: 构建包含多个话题的高质量自然韩英混合语料库，设计细致的借词标签和分层级（词、短语、句子）代码切换标注方案，用于系统地评估模型处理不同层次代码切换的能力。

Result: 通过对多种多语言语音识别模型评测及使用代码切换数据微调实验，发现模型初期在代码切换语音识别表现较差，但微调后能力显著提升。

Conclusion: HiKE框架有效促进对韩英代码切换语音识别的研究与评估，验证了微调技术在提升多语言模型代码切换处理能力方面的效果，将对相关领域研究产生积极推动作用。

Abstract: Despite advances in multilingual automatic speech recognition (ASR),
code-switching (CS), the mixing of languages within an utterance common in
daily speech, remains a severely underexplored challenge. In this paper, we
introduce HiKE: the Hierarchical Korean-English code-switching benchmark, the
first globally accessible evaluation framework for Korean-English CS, aiming to
provide a means for the precise evaluation of multilingual ASR models and to
foster research in the field. The proposed framework not only consists of
high-quality, natural CS data across various topics, but also provides
meticulous loanword labels and a hierarchical CS-level labeling scheme (word,
phrase, and sentence) that together enable a systematic evaluation of a model's
ability to handle each distinct level of code-switching. Through evaluations of
diverse multilingual ASR models and fine-tuning experiments, this paper
demonstrates that while most multilingual ASR models initially struggle with
CS-ASR, this capability can be enabled through fine-tuning with CS data. HiKE
will be available at https://github.com/ThetaOne-AI/HiKE.

</details>


### [154] [Hype or not? Formalizing Automatic Promotional Language Detection in Biomedical Research](https://arxiv.org/abs/2509.24638)
*Bojan Batalo,Erica K. Shimomoto,Neil Millar*

Main category: cs.CL

TL;DR: 本文提出了自动检测科学论文中夸大宣传语言（hype）的任务，并建立了正式的标注指南，基于NIH资助申请语料进行了标注和机器学习模型训练，取得了良好的实验效果。


<details>
  <summary>Details</summary>
Motivation: 科学领域宣传语言日益增多，会削弱客观评估、阻碍研究发展、破坏信任，因此需要自动检测此类夸大语言。

Method: 制定夸大语言的正式标注指南，标注NIH资助申请数据集，使用传统文本分类器和语言模型进行训练与评测，并与人类基线性能比较。

Result: 标注指南使人类注释者能可靠识别夸大形容词，机器学习模型基于注释数据集表现良好，但任务语言复杂，可能需要领域知识和事实的时间信息辅助。

Conclusion: 首次将科学夸大语言检测作为自然语言处理任务进行研究，该任务具有语言复杂性，未来可能需要更深入结合领域知识和时间事实信息。

Abstract: In science, promotional language ('hype') is increasing and can undermine
objective evaluation of evidence, impede research development, and erode trust
in science. In this paper, we introduce the task of automatic detection of
hype, which we define as hyperbolic or subjective language that authors use to
glamorize, promote, embellish, or exaggerate aspects of their research. We
propose formalized guidelines for identifying hype language and apply them to
annotate a portion of the National Institutes of Health (NIH) grant application
corpus. We then evaluate traditional text classifiers and language models on
this task, comparing their performance with a human baseline. Our experiments
show that formalizing annotation guidelines can help humans reliably annotate
candidate hype adjectives and that using our annotated dataset to train machine
learning models yields promising results. Our findings highlight the linguistic
complexity of the task, and the potential need for domain knowledge and
temporal awareness of the facts. While some linguistic works address hype
detection, to the best of our knowledge, we are the first to approach it as a
natural language processing task.

</details>


### [155] [InfLLM-V2: Dense-Sparse Switchable Attention for Seamless Short-to-Long Adaptation](https://arxiv.org/abs/2509.24663)
*Weilin Zhao,Zihan Zhou,Zhou Su,Chaojun Xiao,Yuxuan Li,Yanghao Li,Yudi Zhang,Weilun Zhao,Zhen Li,Yuxiang Huang,Ao Sun,Xu Han,Zhiyuan Liu*

Main category: cs.CL

TL;DR: 本文提出了InfLLM-V2，一种适用于长序列处理的可切换稀疏注意力框架，实现了效率提升和性能保持。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer的自注意力机制在处理长序列时计算和内存消耗大，现有稀疏注意力方法参数过多且影响训练流程，导致收敛慢和加速困难。

Method: InfLLM-V2通过参数无关的架构修改重用密集注意力参数，在短序列时使用密集注意力，长序列时平滑切换为稀疏注意力，同时实现了高效的具体实现方案以降低计算开销。

Result: 在长上下文理解和链式推理任务中，InfLLM-V2比密集注意力速度快4倍，且性能保持在98.1%和99.7%。同时基于该框架公开了混合推理模型MiniCPM4.1。

Conclusion: InfLLM-V2有效解决了长序列注意力的计算瓶颈，且保持了训练流程一致性和高性能，具备良好的实用价值和开源贡献。

Abstract: Long-sequence processing is a critical capability for modern large language
models. However, the self-attention mechanism in the standard Transformer
architecture faces severe computational and memory bottlenecks when processing
long sequences. While trainable sparse attention methods offer a promising
solution, existing approaches such as NSA introduce excessive extra parameters
and disrupt the conventional \textit{pretrain-on-short, finetune-on-long}
workflow, resulting in slow convergence and difficulty in acceleration. To
overcome these limitations, we introduce dense-sparse switchable attention
framework, termed as InfLLM-V2. InfLLM-V2 is a trainable sparse attention that
seamlessly adapts models from short to long sequences. Specifically, InfLLM-V2
reuses dense attention parameters through parameter-free architecture
modification, maintaining consistency between short and long sequence
processing. Additionally, InfLLM-V2 ensures computational efficiency across all
sequence lengths, by using dense attention for short inputs and smoothly
transitioning to sparse attention for long sequences. To achieve practical
acceleration, we further introduce an efficient implementation of InfLLM-V2
that significantly reduces the computational overhead. Our experiments on
long-context understanding and chain-of-thought reasoning demonstrate that
InfLLM-V2 is 4$\times$ faster than dense attention while retaining 98.1% and
99.7% of the performance, respectively. Based on the InfLLM-V2 framework, we
have trained and open-sourced MiniCPM4.1
(https://huggingface.co/openbmb/MiniCPM4.1-8B), a hybrid reasoning model,
providing a reproducible implementation for the research community.

</details>


### [156] [Understanding the Dilemma of Unlearning for Large Language Models](https://arxiv.org/abs/2509.24675)
*Qingjie Zhang,Haoting Qian,Zhicong Huang,Cheng Hong,Minlie Huang,Ke Xu,Chao Zhang,Han Qiu*

Main category: cs.CL

TL;DR: 该论文提出了一种名为unPact的可解释撤销学习框架，通过分析提示词对输出的贡献来揭示撤销学习的机制，并发现现有方法存在要么知识可恢复，要么导致能力退化的两难困境。


<details>
  <summary>Details</summary>
Motivation: 尽管撤销学习方法不断被提出，但由于大规模语言模型架构复杂，知识难以追踪，其机制缺乏可解释分析。

Method: 提出unPact框架，利用提示词归因和贡献跟踪量化提示词对输出的影响，进行撤销学习前后的对比分析。

Result: 发现撤销学习破坏提示词中关键词的关注度，知识大多并未真正抹去且可通过强调关键词恢复，泛化能力退化源于对所有词的不加区分惩罚。

Conclusion: 现有撤销学习方法存在两难困境，要么知识未彻底抹去，要么导致灾难性遗忘，尚需改进以实现可靠的知识撤销。

Abstract: Unlearning seeks to remove specific knowledge from large language models
(LLMs), but its effectiveness remains contested. On one side, "forgotten"
knowledge can often be recovered through interventions such as light
fine-tuning; on the other side, unlearning may induce catastrophic forgetting
that degrades general capabilities. Despite active exploration of unlearning
methods, interpretability analyses of the mechanism are scarce due to the
difficulty of tracing knowledge in LLMs' complex architectures. We address this
gap by proposing unPact, an interpretable framework for unlearning via prompt
attribution and contribution tracking. Typically, it quantifies each prompt
token's influence on outputs, enabling pre- and post-unlearning comparisons to
reveal what changes. Across six mainstream unlearning methods, three LLMs, and
three benchmarks, we find that: (1) Unlearning appears to be effective by
disrupting focus on keywords in prompt; (2) Much of the knowledge is not truly
erased and can be recovered by simply emphasizing these keywords in prompts,
without modifying the model's weights; (3) Catastrophic forgetting arises from
indiscriminate penalization of all tokens. Taken together, our results suggest
an unlearning dilemma: existing methods tend either to be insufficient -
knowledge remains recoverable by keyword emphasis, or overly destructive -
general performance collapses due to catastrophic forgetting, still leaving a
gap to reliable unlearning.

</details>


### [157] [Reference-Free Rating of LLM Responses via Latent Information](https://arxiv.org/abs/2509.24678)
*Leander Girrbach,Chi-Ping Su,Tankred Saanum,Richard Socher,Eric Schulz,Zeynep Akata*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型作为评价者时的评分可靠性问题，提出潜变量评价方法以提升评分稳定性和校准度。


<details>
  <summary>Details</summary>
Motivation: 传统利用语言模型直接给自由文本打Likert分数存在评分不稳定和校准不佳的问题，导致评分集中且频繁出现平分，影响评价的有效性。

Method: 提出潜变量评价方法，包括基于概率加权的整数评分、验证器风格的概率“是”判定以及在线性探针训练的模型激活信号，替代直接打分，实现更稳定和判别力更强的评分。

Result: 潜变量方法在多项双向和单向评分基准测试中表现优于或匹配传统提示法，概率加权评分相关度最高，线性探针在输出对数概率校准失误时能恢复有效信号。

Conclusion: 潜在信息能为无参考评价提供确定性且判别性更强的信号，有助于提升模型评价、选择及训练过程，如Best-of-N选择、多教师蒸馏和路由等应用。

Abstract: How reliable are single-response LLM-as-a-judge ratings without references,
and can we obtain fine-grained, deterministic scores in this setting? We study
the common practice of asking a judge model to assign Likert-scale scores to
free-text responses and show two systematic issues: scores are unstable under
sampling and poorly calibrated, leading to compression near the top of the
scale and frequent ties. We then propose and evaluate Latent Judges, which
derive scalar ratings from internal model signals: (i) probability-weighted
scores over integer ratings, (ii) verifier-style probabilities of "yes", and
(iii) linear probes trained on model activations at the rating position. Across
a broad suite of pairwise and single-rating benchmarks, latent methods match or
surpass standard prompting, with consistent gains on pairwise accuracy and
listwise ranking relevant to Best-of-N selection. Probability-weighted scores
achieve the strongest single-rating correlations, while probes recover useful
signals when output logits are miscalibrated. These results indicate that
latent information provides deterministic and more discriminative signals for
reference-free evaluation, and can improve selection and training approaches
like Best-of-$N$, multi-teacher distillation, and routing.

</details>


### [158] [MemGen: Weaving Generative Latent Memory for Self-Evolving Agents](https://arxiv.org/abs/2509.24704)
*Guibin Zhang,Muxin Fu,Shuicheng Yan*

Main category: cs.CL

TL;DR: MemGen是一个动态生成记忆框架，通过监控推理状态触发记忆调用，构建潜在记忆序列，实现记忆与认知的紧密交织，显著提升了LLM代理的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的记忆范式无法同时捕捉人类认知中推理与记忆的流动交织，导致代理记忆表现受限。

Method: 提出MemGen框架，包括记忆触发器和记忆编织器，动态生成潜在记忆序列以丰富推理过程；实现记忆与认知的循环交织。

Result: 在八个基准测试中，MemGen性能超过了多种领先的外部记忆系统，还有较强的跨领域泛化能力，并能自发表现出类似人类的多种记忆功能。

Conclusion: MemGen展示了向更加自然的机器认知形式演化的潜力，通过动态生成记忆提升了基于LLM的代理的记忆与推理能力。

Abstract: Agent memory shapes how Large Language Model (LLM)-powered agents, akin to
the human brain, progressively refine themselves through environment
interactions. Existing paradigms remain constrained: parametric memory forcibly
adjusts model parameters, and retrieval-based memory externalizes experience
into structured databases, yet neither captures the fluid interweaving of
reasoning and memory that underlies human cognition. To address this gap, we
propose MemGen, a dynamic generative memory framework that equips agents with a
human-esque cognitive faculty. It consists of a \textit{memory trigger}, which
monitors the agent's reasoning state to decide explicit memory invocation, and
a \textit{memory weaver}, which takes the agent's current state as stimulus to
construct a latent token sequence as machine-native memory to enrich its
reasoning. In this way, MemGen enables agents to recall and augment latent
memory throughout reasoning, producing a tightly interwoven cycle of memory and
cognition. Extensive experiments across eight benchmarks show that MemGen
surpasses leading external memory systems such as ExpeL and AWM by up to
$38.22\%$, exceeds GRPO by up to $13.44\%$, and exhibits strong cross-domain
generalization ability. More importantly, we find that without explicit
supervision, MemGen spontaneously evolves distinct human-like memory faculties,
including planning memory, procedural memory, and working memory, suggesting an
emergent trajectory toward more naturalistic forms of machine cognition.

</details>


### [159] [Socratic-Zero : Bootstrapping Reasoning via Data-Free Agent Co-evolution](https://arxiv.org/abs/2509.24726)
*Shaobo Wang,Zhengbo Jiao,Zifan Zhang,Yilang Peng,Xu Ze,Boyu Yang,Wei Wang,Hu Wei,Linfeng Zhang*

Main category: cs.CL

TL;DR: 本文提出了Socratic-Zero框架，通过教师、求解者和生成者三者协同进化，自动生成高质量数学推理训练数据，实现从100个种子问题出发大幅提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在推理任务上依赖大量人工高质量数据，难以扩展；现有合成数据方法难以保证数据质量且无法动态适应模型能力，导致训练效果不佳。

Method: 设计Socratic-Zero系统，由教师（Teacher） adaptive出更难题目，求解者（Solver）通过反馈不断提升推理能力，生成者（Generator）提炼教师策略以规模化生成高质量课程，实现闭环自我提升。

Result: Socratic-Solver-8B在七个数学推理基准上相比现有数据合成方法提升了20.2个百分点，在不同模型系列均表现提升；用Socratic-Generator-32B生成的合成数据训练的学生模型表现优于多个商业大模型。

Conclusion: Socratic-Zero框架无需预先任务或标签，通过多智能体协同进化合成训练数据，有效提升模型推理能力，展现了自动化、高质量数据合成的巨大潜力。

Abstract: Recent breakthroughs in large language models (LLMs) on reasoning tasks rely
heavily on massive, high-quality datasets-typically human-annotated and thus
difficult to scale. While data synthesis or distillation offers a promising
alternative, existing methods struggle with inconsistent data quality and an
inability to dynamically adapt to the evolving capabilities of the model,
leading to suboptimal training signals. To address these limitations, we
introduce Socratic-Zero, a fully autonomous framework that generates
high-quality training data from minimal seed examples through the co-evolution
of three agents: the Teacher, the Solver, and the Generator. The Solver
continuously refines its reasoning by learning from preference feedback on both
successful and failed trajectories; the Teacher adaptively crafts increasingly
challenging questions based on the Solver's weaknesses; and the Generator
distills the Teacher's question-design strategy to enable scalable,
high-fidelity curriculum generation. This closed-loop system produces a
self-improving curriculum-requiring no pre-existing tasks or labels.
Remarkably, starting from only 100 seed questions, our Socratic-Solver-8B
achieves an average gain of +20.2 percentage points over prior data synthesis
methods across seven mathematical reasoning benchmarks (AMC23, AIME24-25,
Olympiad, MATH-500, Minerva, and GSM8K), with consistent gains on both Qwen3
and GLM4 series models. Even more surprisingly, synthetic data from
Socratic-Generator-32B enables student LLMs to achieve superior performance
compared to other state-of-the-art (SOTA) commercial LLMs on these benchmarks,
including Qwen3-235B-A22B, DeepSeek-V3.1-671B, GPT-5, Gemini-2.5-Pro, Grok-4,
and Claude-4.1-Opus.

</details>


### [160] [ProxyAttn: Guided Sparse Attention via Representative Heads](https://arxiv.org/abs/2509.24745)
*Yixuan Wang,Huang He,Siqi Bao,Hua Wu,Haifeng Wang,Qingfu Zhu,Wanxiang Che*

Main category: cs.CL

TL;DR: 提出了ProxyAttn，一种无需训练的稀疏注意力算法，通过压缩注意力头维度实现更精细的块估计，显著提升长文本任务中大型语言模型的效率。


<details>
  <summary>Details</summary>
Motivation: 传统注意力机制计算复杂度高，限制了大型语言模型在长文本任务中的效率，且现有动态块重要性估计方法在高稀疏率时性能下降。

Method: 通过观察多注意力头之间的相似性，使用代表性头的合并分数近似所有头的分数，并提出基于块的动态预算估计方法，实现了细粒度的块重要性评估。

Result: 在多模型和基准测试中验证了注意力头相似性，ProxyAttn在保持性能的同时，实现了最多10.3倍注意力加速和2.4倍预处理加速。

Conclusion: ProxyAttn通过细粒度且低计算成本的块估计，有效提升了长文本任务中大型语言模型的计算效率和性能。

Abstract: The quadratic complexity of attention mechanisms limits the efficiency of
Large Language Models (LLMs) on long-text tasks. Recently, methods that
dynamically estimate block importance have enabled efficient block sparse
attention, leading to significant acceleration in long-text pre-filling of
LLMs. However, their coarse-grained estimation inevitably leads to performance
degradation at high sparsity rates. In this work, we propose ProxyAttn, a
training-free sparse attention algorithm that achieves more precise block
estimation by compressing the dimension of attention heads. Based on our
observation of the similarity among multiple attention heads, we use the scores
of pooled representative heads to approximate the scores for all heads. To
account for the varying sparsity among heads, we also propose a block-aware
dynamic budget estimation method. By combining the scores from representative
proxy heads with multi-head dynamic budgets, we achieve a more fine-grained
block importance evaluation at low computational cost. Experiments on a variety
of mainstream models and extensive benchmarks confirm the underlying similarity
among attention heads. Leveraging a fine-grained estimation, the proposed
method achieves substantial gains in performance and efficiency compared to
existing methods. More precisely, ProxyAttn can achieve up to 10.3x attention
acceleration and 2.4x prefilling acceleration without significant performance
loss. Our code is available at https://github.com/wyxstriker/ProxyAttn.

</details>


### [161] [LatentEvolve: Self-Evolving Test-Time Scaling in Latent Space](https://arxiv.org/abs/2509.24771)
*Guibin Zhang,Fanci Meng,Guancheng Wan,Zherui Li,Kun Wang,Zhenfei Yin,Lei Bai,Shuicheng Yan*

Main category: cs.CL

TL;DR: 本文提出了LatentEvolve，一种受人脑学习系统启发的自我进化测试时缩放(TTS)框架，通过交替的"日间"快速检索和"夜间"整合优化，显著提升了大型语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时缩放方法彼此独立，未能使大型语言模型逐步学习如何更有效地缩放测试时计算资源。作者旨在使模型能自我进化，学习更优的测试时计算策略。

Method: 借鉴补充学习系统理论，设计了两部分进化机制：日间缩放快速检索历史潜在表示指导当前推理，夜间缩放则整合以往潜在优化，类似人脑睡眠期间的记忆巩固。日夜交替演化实现了模型测试时缩放的快慢演进，且完全无监督。

Result: 在八个基准测试和五个模型骨干上，LatentEvolve优于现有顶尖TTS方法（如LatentSeek和TTRL）最高达13.33%提升，并且在跨领域及跨骨干模型泛化能力方面表现卓越。

Conclusion: LatentEvolve通过模拟人脑认知动态，实现了大型语言模型测试时计算的自我进化与优化，显著增强了模型的推理能力与泛化性，推动了TTS方法的发展。

Abstract: Test-time Scaling (TTS) has been demonstrated to significantly enhance the
reasoning capabilities of Large Language Models (LLMs) during the inference
phase without altering model parameters. However, existing TTS methods are
largely independent, implying that LLMs have not yet evolved to progressively
learn how to scale more effectively. With the objective of evolving LLMs to
learn ``how to scale test-time computation,'' we propose LatentEvolve, a
self-evolving latent TTS framework inspired by the complementary learning
system (CLS) theory. Analogous to the human brain's dual system of a
fast-recall hippocampus and a slow-consolidating neocortex, LatentEvolve
comprises two evolutionary components: \textit{daytime scaling}, which rapidly
retrieves historical latent representations to better guide current LLM
reasoning; and \textit{nighttime scaling}, which integrates past latent
optimizations in a manner akin to the human brain's consolidation of
experiences during sleep. The alternation of daytime and nighttime processes
facilitates a fast and slow evolution of LLM TTS, mirroring human cognitive
dynamics in a fully unsupervised manner. Extensive experiments across eight
benchmarks and five model backbones demonstrate that our LatentEvolve surpasses
state-of-the-art TTS methods such as LatentSeek and TTRL by up to $13.33\%$ and
exhibits exceptional cross-domain and cross-backbone generalization.

</details>


### [162] [SeaPO: Strategic Error Amplification for Robust Preference Optimization of Large Language Models](https://arxiv.org/abs/2509.24781)
*Jun Rao,Yunjie Liao,Xuebo Liu,Zepeng Lin,Lian Lian,Dong Jin,Shengjun Cheng,Jun Yu,Min Zhang*

Main category: cs.CL

TL;DR: 本文提出了SeaPO，一种通过引入模型常见错误模式来增强大语言模型偏好优化的方法，有效提升了模型尤其是真实性方面的表现。


<details>
  <summary>Details</summary>
Motivation: 现有偏好优化方法中正负样本质量相近，导致训练中优化复杂且效果有限。

Method: SeaPO通过利用三种常见错误类型，将负样本设计得更错误，利用偏好训练减少错误，从而提升模型性能。

Result: 在多个能力维度和不同模型规模上，SeaPO提高了模型整体性能，尤其是在真实性方面提升了5-10个百分点。

Conclusion: 引入特定错误模式作为负样本可有效提升模型偏好优化效果，且不同错误类型对任务表现有差异化影响，混合错误类型带来更广泛提升。

Abstract: Existing alignment methods for preference optimization of large language
models (LLMs) aim to enhance model performance by utilizing pairs of positive
and negative samples. However, due to the limited capacity of models in scoring
or generating responses, the quality of positive and negative samples may
become similar during training, which complicates optimization for preference
learning. To address this issue, we introduce SeaPO, a Strategic Error
Amplification method that leverages three error types commonly occurring in
LLMs to introduce specific error patterns into the model Preference
Optimization. This strategy ensures that negative samples are more erroneous
than positive samples and preference-based training is employed to mitigate the
occurrence of these errors, thereby enhancing model performance. Evaluations
across five capability dimensions and different model scales (1.5B to 14B)
demonstrate that the generated data significantly improved overall model
performance, particularly in terms of truthfulness, with improvements of 5-10
percentage points observed. Further analysis reveals that task performance
varies depending on the error types introduced. Injecting the most common error
types improves performance in related tasks, while a mix of error types leads
to a broader performance enhancement: most tasks show stable improvements,
while a few tasks exhibit significant gains.

</details>


### [163] [Evaluating Spatiotemporal Consistency in Automatically Generated Sewing Instructions](https://arxiv.org/abs/2509.24792)
*Luisa Geiger,Mareike Hartmann,Michael Sullivan,Alexander Koller*

Main category: cs.CL

TL;DR: 本文提出了一种基于树的自动评估指标，用于评估大型语言模型生成的分步组装指令，特别适用于缝纫指令的时空准确性评估。


<details>
  <summary>Details</summary>
Motivation: 传统评估指标如BLEU和BERT相似度并不能准确反映组装指令中的时空特性，导致评估结果与人工标注的错误数量和质量评分相关性较低。

Method: 设计了一种基于树结构的自动评估指标，用以更好地捕捉指令中的时空信息，并将该指标应用于缝纫指令的评估。

Result: 该指标与人工标注的错误数量和人工质量评分的相关性更高，且在面对特意构造的反事实样本时表现出更强的鲁棒性。

Conclusion: 基于树的评估指标在评价大型语言模型生成的缝纫组装指令的时空准确性方面优于传统文本相似性指标，具有更高的实用价值。

Abstract: In this paper, we propose a novel, automatic tree-based evaluation metric for
LLM-generated step-by-step assembly instructions, that more accurately reflects
spatiotemporal aspects of construction than traditional metrics such as BLEU
and BERT similarity scores. We apply our proposed metric to the domain of
sewing instructions, and show that our metric better correlates with
manually-annotated error counts as well as human quality ratings, demonstrating
our metric's superiority for evaluating the spatiotemporal soundness of sewing
instructions. Further experiments show that our metric is more robust than
traditional approaches against artificially-constructed counterfactual examples
that are specifically constructed to confound metrics that rely on textual
similarity.

</details>


### [164] [KnowGuard: Knowledge-Driven Abstention for Multi-Round Clinical Reasoning](https://arxiv.org/abs/2509.24816)
*Xilin Dang,Kexin Chen,Xiaorui Su,Ayush Noori,Iñaki Arango,Lucas Vittor,Xinyi Long,Yuyang Du,Marinka Zitnik,Pheng Ann Heng*

Main category: cs.CL

TL;DR: 本文提出KnowGuard，一种结合知识图谱探索的临床决策模型，提升大语言模型在医学领域的“放弃决策”能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在医疗决策中面对信息不足时难以有效放弃，过于自信导致潜在误诊。

Method: KnowGuard采用“先调查后放弃”模式，利用知识图谱扩展和证据检索系统地发现和评估证据，实现基于患者上下文的证据排名。

Result: 在多轮开放式临床测试中，KnowGuard提高诊断准确率3.93%，同时减少平均7.27轮不必要的交互，优于现有方法。

Conclusion: 整合知识图谱的系统性探索显著提升了临床决策中模型放弃能力，提高诊断安全性与效率。

Abstract: In clinical practice, physicians refrain from making decisions when patient
information is insufficient. This behavior, known as abstention, is a critical
safety mechanism preventing potentially harmful misdiagnoses. Recent
investigations have reported the application of large language models (LLMs) in
medical scenarios. However, existing LLMs struggle with the abstentions,
frequently providing overconfident responses despite incomplete information.
This limitation stems from conventional abstention methods relying solely on
model self-assessments, which lack systematic strategies to identify knowledge
boundaries with external medical evidences. To address this, we propose
\textbf{KnowGuard}, a novel \textit{investigate-before-abstain} paradigm that
integrates systematic knowledge graph exploration for clinical decision-making.
Our approach consists of two key stages operating on a shared contextualized
evidence pool: 1) an evidence discovery stage that systematically explores the
medical knowledge space through graph expansion and direct retrieval, and 2) an
evidence evaluation stage that ranks evidence using multiple factors to adapt
exploration based on patient context and conversation history. This two-stage
approach enables systematic knowledge graph exploration, allowing models to
trace structured reasoning paths and recognize insufficient medical evidence.
We evaluate our abstention approach using open-ended multi-round clinical
benchmarks that mimic realistic diagnostic scenarios, assessing abstention
quality through accuracy-efficiency trade-offs beyond existing closed-form
evaluations. Experimental evidences clearly demonstrate that KnowGuard
outperforms state-of-the-art abstention approaches, improving diagnostic
accuracy by 3.93\% while reducing unnecessary interaction by 7.27 turns on
average.

</details>


### [165] [DiaCDM: Cognitive Diagnosis in Teacher-Student Dialogues using the Initiation-Response-Evaluation Framework](https://arxiv.org/abs/2509.24821)
*Rui Jia,Yuang Wei,Ruijia Li,Yuang-Hao Jiang,Xinyu Xie,Yaomin Shen,Min Zhang,Bo Jiang*

Main category: cs.CL

TL;DR: 提出了DiaCDM模型，实现了基于对话的认知诊断，提高了诊断准确率和结果可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统认知诊断模型无法有效处理动态且无结构的师生对话，且难以从长对话中提取准确的诊断语义。

Method: 采用教育学中的启动-响应-评价（IRE）框架设计对话诊断框架，开发基于图的编码方法融合教师问题与知识组件，更精准地捕获对话中的关键信息。

Result: DiaCDM在三个真实对话数据集上显著提升了诊断准确率，并增强了结果的可解释性。

Conclusion: DiaCDM首次将认知诊断应用于对话场景，为教师准确评估学生认知状态提供了有效工具。

Abstract: While cognitive diagnosis (CD) effectively assesses students' knowledge
mastery from structured test data, applying it to real-world teacher-student
dialogues presents two fundamental challenges. Traditional CD models lack a
suitable framework for handling dynamic, unstructured dialogues, and it's
difficult to accurately extract diagnostic semantics from lengthy dialogues. To
overcome these hurdles, we propose DiaCDM, an innovative model. We've adapted
the initiation-response-evaluation (IRE) framework from educational theory to
design a diagnostic framework tailored for dialogue. We also developed a unique
graph-based encoding method that integrates teacher questions with relevant
knowledge components to capture key information more precisely. To our
knowledge, this is the first exploration of cognitive diagnosis in a dialogue
setting. Experiments on three real-world dialogue datasets confirm that DiaCDM
not only significantly improves diagnostic accuracy but also enhances the
results' interpretability, providing teachers with a powerful tool for
assessing students' cognitive states. The code is available at
https://github.com/Mind-Lab-ECNU/DiaCDM/tree/main.

</details>


### [166] [SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts via Token-Level LSH Matching](https://arxiv.org/abs/2509.24832)
*Xinye Zhao,Spyridon Mastorakis*

Main category: cs.CL

TL;DR: 本文提出了SemShareKV，一种基于语义相似性的KV缓存共享与压缩框架，通过模糊匹配减少重复计算，显著提升大语言模型推理速度和降低内存占用。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模不断扩大，推理过程中键值缓存的内存需求成为瓶颈，现有方法难以处理语义相似但词汇不同的提示，影响多文档摘要和对话等任务的效率。

Method: 引入基于局部敏感哈希（LSH）的模糊令牌匹配，并结合旋转位置编码（RoPE）保持位置信息，选择性复用参考提示的相关KV对，实现缓存共享和压缩。

Result: 在多个摘要数据集上，使用5000令牌输入时，推理速度提升至6.25倍，GPU内存占用降低42%，且质量下降极其微小。

Conclusion: 语义感知的KV缓存共享技术有效缓解了大语言模型推理的内存和计算瓶颈，具备显著的实用价值和应用潜力。

Abstract: As large language models (LLMs) continue to scale, the memory footprint of
key-value (KV) caches during inference has become a significant bottleneck.
Existing approaches primarily focus on compressing KV caches within a single
prompt or reusing shared prefixes or frequently ocurred text segments across
prompts. However, such strategies are limited in scenarios where prompts are
semantically similar but lexically different, which frequently occurs in tasks
such as multi-document summarization and conversational agents. We propose
\textit{SemShareKV}, a KV cache sharing and compression framework that
accelerates LLM inference by reusing KVCache in semantically similar prompts.
Instead of relying on exact token matches, SemShareKV applies fuzzy token
matching using locality-sensitive hashing (LSH) on token embeddings and
incorporates Rotary Position Embedding (RoPE) to better preserve positional
information. By selectively reusing relevant key-value pairs from a reference
prompt's cache, SemShareKV reduces redundant computation while maintaining
output quality. Experiments on diverse summarization datasets show up to
6.25$\times$ speedup and 42\% lower GPU memory usage with 5k tokens input, with
negligible quality degradation. These results highlight the potential of
semantic-aware cache sharing for efficient LLM inference.

</details>


### [167] [Hierarchical Error Correction for Large Language Models: A Systematic Framework for Domain-Specific AI Quality Enhancement](https://arxiv.org/abs/2509.24841)
*Zhilong Zhao,Yindi Liu*

Main category: cs.CL

TL;DR: 本文提出了一种层次化错误校正（HEC）框架，通过系统错误分析和分层干预策略，显著提升大语言模型在专业领域的表现。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在专业领域表现不佳，医疗编码任务准确率仅为45.9%，亟需有效方法提升专业领域AI性能。

Method: 分析AI在专业领域的错误层次结构（知识层、推理层、复杂性层），基于此设计三阶段层次化错误校正框架，并在多个专业任务和模型中进行验证。

Result: 该方法在医疗转录、法律文档分类、政治偏见检测及法律推理任务中均显著提升准确率，平均提升11.2个百分点（p<0.001），但在高基线任务（准确率>75%）中效果受限。

Conclusion: 系统化的错误分析指导下的分层校正策略，特别适用于中等基线表现任务，有望提升专业领域AI性能，同时需注意框架的适用边界以优化部署效果。

Abstract: Large Language Models face significant performance challenges in specialized
domains, with state-of-the-art models achieving only 45.9% accuracy on medical
coding tasks. This study proposes a Hierarchical Error Correction (HEC)
framework that addresses domain-specific AI limitations through systematic
error analysis and targeted intervention strategies.
  We analyze error patterns across four specialized domains and find that AI
errors follow consistent hierarchical structures: Knowledge-layer errors
(58.4%), Reasoning-layer errors (39.6%), and Complexity-layer errors (2.0%).
Based on these patterns, we develop a three-stage correction framework that
addresses errors according to their hierarchical importance and demonstrates
that framework effectiveness correlates inversely with baseline task
performance.
  Experimental validation across medical transcription (4,921 cases), legal
document classification (1,000 cases), political bias detection (645 cases),
and legal reasoning (1,000 cases) shows consistent improvements. Cross-model
validation across five LLM architectures demonstrates average improvements of
11.2 percentage points (p < 0.001). However, analysis reveals framework
limitations in high-baseline tasks (>75% accuracy), where hierarchical
intervention may interfere with effective reasoning processes.
  The results suggest that systematic error analysis can guide effective AI
enhancement strategies in specialized domains, particularly for
moderate-baseline tasks, while highlighting the importance of understanding
framework boundaries for optimal deployment.

</details>


### [168] [Between Help and Harm: An Evaluation of Mental Health Crisis Handling by LLMs](https://arxiv.org/abs/2509.24857)
*Adrian Arnaiz-Rodriguez,Miguel Baidal,Erik Derner,Jenn Layton Annable,Mark Ball,Mark Ince,Elvira Perez Vallejos,Nuria Oliver*

Main category: cs.CL

TL;DR: 本文提出了统一的精神健康危机分类体系，构建了多样化评估数据集，并基于专家设计的方案对大型语言模型（LLMs）进行精神健康危机分类和回应的系统性评估。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型驱动的聊天机器人被广泛用于高风险情境如情感支持和心理健康，其安全识别及响应急性心理危机的能力尚未得到充分理解，缺乏统一的分类体系、标注基准及临床指导的评估标准，限制了相关进展。

Method: 本文提出了包括六个临床定义的心理危机类别的统一分类体系，收集了多元化的评估数据集，设计了专家指导的评价协议，系统性地评测了三种先进LLM对危机类型的分类能力和生成安全适当回应的表现。

Result: 结果显示，虽然LLM在处理明确危机披露时表现出高度一致性和较强可靠性，但仍存在显著风险，不少回复被评为不适当或有害，其中开源模型的失误率高于商业模型。此外，模型在处理间接或模糊风险信号时有系统性弱点，倾向于使用死板且不真实的默认回复，且经常与用户语境不匹配。

Conclusion: 研究强调了提升LLM安全保障、改进危机检测及开发上下文感知干预措施的紧迫性。提供的分类体系、数据集和评估框架为AI驱动的心理健康支持领域的持续研究和负责任创新奠定基础，有助于减少伤害并更好地保护脆弱用户。

Abstract: The widespread use of chatbots powered by large language models (LLMs) such
as ChatGPT and Llama has fundamentally reshaped how people seek information and
advice across domains. Increasingly, these chatbots are being used in
high-stakes contexts, including emotional support and mental health concerns.
While LLMs can offer scalable support, their ability to safely detect and
respond to acute mental health crises remains poorly understood. Progress is
hampered by the absence of unified crisis taxonomies, robust annotated
benchmarks, and empirical evaluations grounded in clinical best practices. In
this work, we address these gaps by introducing a unified taxonomy of six
clinically-informed mental health crisis categories, curating a diverse
evaluation dataset, and establishing an expert-designed protocol for assessing
response appropriateness. We systematically benchmark three state-of-the-art
LLMs for their ability to classify crisis types and generate safe, appropriate
responses. The results reveal that while LLMs are highly consistent and
generally reliable in addressing explicit crisis disclosures, significant risks
remain. A non-negligible proportion of responses are rated as inappropriate or
harmful, with responses generated by an open-weight model exhibiting higher
failure rates than those generated by the commercial ones. We also identify
systemic weaknesses in handling indirect or ambiguous risk signals, a reliance
on formulaic and inauthentic default replies, and frequent misalignment with
user context. These findings underscore the urgent need for enhanced
safeguards, improved crisis detection, and context-aware interventions in LLM
deployments. Our taxonomy, datasets, and evaluation framework lay the
groundwork for ongoing research and responsible innovation in AI-driven mental
health support, helping to minimize harm and better protect vulnerable users.

</details>


### [169] [Metaphor identification using large language models: A comparison of RAG, prompt engineering, and fine-tuning](https://arxiv.org/abs/2509.24866)
*Matteo Fuoli,Weihang Huang,Jeannette Littlemore,Sarah Turner,Ellen Wilding*

Main category: cs.CL

TL;DR: 本研究探讨了大语言模型(LLMs)在自动识别隐喻文本中的应用，比较了检索增强生成、提示工程和微调三种方法。结果显示微调方法表现最佳，达到0.79的F1分数。


<details>
  <summary>Details</summary>
Motivation: 隐喻广泛存在于话语中，是理解认知、情感和意识形态的有效视角，但自动化识别受限于隐喻的语境敏感性，需要手工标注，限制了大规模分析的可能性。

Method: 本研究比较三种大语言模型识别隐喻的方法：检索增强生成（RAG），通过提供代码本及示例指导；提示工程，包括零次学习、少次学习及思路链策略；以及基于人工标注文本的微调训练。

Result: 最先进的封闭源大语言模型表现出高准确度，微调方法达到了中位F1值0.79。比对人类与模型的标注，发现主要差异集中在隐喻理论中的灰色区域和概念难点。

Conclusion: 大语言模型可部分实现隐喻的自动识别，有助于隐喻识别协议及相关理论的发展和完善，推动隐喻大规模文本分析研究。

Abstract: Metaphor is a pervasive feature of discourse and a powerful lens for
examining cognition, emotion, and ideology. Large-scale analysis, however, has
been constrained by the need for manual annotation due to the context-sensitive
nature of metaphor. This study investigates the potential of large language
models (LLMs) to automate metaphor identification in full texts. We compare
three methods: (i) retrieval-augmented generation (RAG), where the model is
provided with a codebook and instructed to annotate texts based on its rules
and examples; (ii) prompt engineering, where we design task-specific verbal
instructions; and (iii) fine-tuning, where the model is trained on hand-coded
texts to optimize performance. Within prompt engineering, we test zero-shot,
few-shot, and chain-of-thought strategies. Our results show that
state-of-the-art closed-source LLMs can achieve high accuracy, with fine-tuning
yielding a median F1 score of 0.79. A comparison of human and LLM outputs
reveals that most discrepancies are systematic, reflecting well-known grey
areas and conceptual challenges in metaphor theory. We propose that LLMs can be
used to at least partly automate metaphor identification and can serve as a
testbed for developing and refining metaphor identification protocols and the
theory that underpins them.

</details>


### [170] [Expanding Computation Spaces of LLMs at Inference Time](https://arxiv.org/abs/2509.24884)
*Yoonna Jang,Kisu Yang,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 本论文研究了在推理阶段仅通过插入填充符号扩展语言模型的计算空间，发现这种方法特别有利于小型模型提升性能。


<details>
  <summary>Details</summary>
Motivation: 扩展语言模型的计算空间有助于利用更多信息进行复杂推理，但此前工作多在训练时加入特殊符号，本研究尝试仅在推理时插入填充符以简化操作。

Method: 选择有效的填充符类型、数量及插入位置，分析模型训练阶段何时开始利用扩展空间，并通过注意力图探究计算动态，实验覆盖1.7B~32B模型和多任务。

Result: 最佳策略是将填充符放在"Answer:"前，小模型收益最大，最高提升12.372个百分点。注意力图显示扩展空间内进行有效计算。

Conclusion: 填充符扩展空间为模型提供了额外计算能力，而非冗余输入，对提升推理能力尤其有效。

Abstract: Chain-of-thought (CoT) rationale enables language models to use additional
task-related text for problem-solving, benefiting not only from detailed
reasoning steps but also from the expanded computational space of longer
inputs. Prior work has trained filler or special tokens to serve as additional
computation spaces. In this study, we investigate whether language models can
leverage artificially inserted sequences of filler tokens solely at inference.
We first identify effective token types, numbers, and insertion locations, then
examine at what stage of training models begin to exploit the expanded
computation space, and finally analyze dynamics within these spaces via
attention maps. Experiments on models ranging from 1.7B to 32B across
open-domain QA and math tasks show that appropriate token types and counts
vary, but placing filler tokens directly before the final 'Answer:' token is
most effective. Smaller models benefit most, up to 12.372 percentage points in
SmolLM2-1.7B-Instruct, indicating that these spaces act as additional
computational capacity rather than redundant input. Attention maps reveal that
expanded spaces often continue the original attention mechanism and sometimes
focus on questions or answer options, suggesting meaningful computation for
problem-solving.

</details>


### [171] [BOE-XSUM: Extreme Summarization in Clear Language of Spanish Legal Decrees and Notifications](https://arxiv.org/abs/2509.24908)
*Andrés Fernández García,Javier de la Rosa,Julio Gonzalo,Roser Morante,Enrique Amigó,Alejandro Benito-Santos,Jorge Carrillo-de-Albornoz,Víctor Fresno,Adrian Ghajari,Guillermo Marco,Laura Plaza,Eva Sánchez Salido*

Main category: cs.CL

TL;DR: 本文介绍了一个新的西班牙官方公报简明摘要数据集BOE-XSUM，并通过微调中等规模大语言模型进行摘要生成，效果优于通用模型。


<details>
  <summary>Details</summary>
Motivation: 由于信息过载，长文摘要变得非常重要，尤其是西班牙法律文档领域缺乏简明摘要数据集和相关模型。

Method: 构建了包含3648条简洁明了摘要的BOE-XSUM数据集，并对中等规模大语言模型进行微调，随后与零样本通用生成模型进行比较。

Result: 微调后模型表现明显优于零样本通用模型，最佳模型BERTIN GPT-J 6B实现24%的性能提升，准确率达到41.6%。

Conclusion: 针对西班牙法律文档的专门数据集和模型微调显著提升了摘要生成性能，有助于缓解信息过载带来的挑战。

Abstract: The ability to summarize long documents succinctly is increasingly important
in daily life due to information overload, yet there is a notable lack of such
summaries for Spanish documents in general, and in the legal domain in
particular. In this work, we present BOE-XSUM, a curated dataset comprising
3,648 concise, plain-language summaries of documents sourced from Spain's
``Bolet\'{\i}n Oficial del Estado'' (BOE), the State Official Gazette. Each
entry in the dataset includes a short summary, the original text, and its
document type label. We evaluate the performance of medium-sized large language
models (LLMs) fine-tuned on BOE-XSUM, comparing them to general-purpose
generative models in a zero-shot setting. Results show that fine-tuned models
significantly outperform their non-specialized counterparts. Notably, the
best-performing model -- BERTIN GPT-J 6B (32-bit precision) -- achieves a 24\%
performance gain over the top zero-shot model, DeepSeek-R1 (accuracies of
41.6\% vs.\ 33.5\%).

</details>


### [172] [How Well Do LLMs Imitate Human Writing Style?](https://arxiv.org/abs/2509.24930)
*Rebira Jemama,Rajesh Kumar*

Main category: cs.CL

TL;DR: 本文提出了一种快速且无训练需求的作者身份验证与风格模仿分析框架，通过融合TF-IDF字符n元组和变换器嵌入，实现高准确率且节省资源，评估了五种大语言模型在不同提示策略下的风格匹配表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽能生成流畅文本，但其复制特定人类作者独特风格的能力尚不明确，亟需一种快速且无需训练的检测和模仿分析方法。

Method: 结合TF-IDF字符n元组和变换器嵌入，利用经验距离分布对文本对进行分类，无需监督训练或阈值调优。测试五种LLM在零样本、一样本、少样本及文本补全等提示策略下的表现。

Result: 框架在学术论文上达到97.5%准确率，跨领域评估94.5%，训练时间减少91.8%，内存使用减少59%。少样本提示显著提升风格一致性（最高23.5倍），文本补全提示准确率达99.9%。此外，高保真模仿不代表人类式不可预测度，LLM生成文本困惑度远低于人类文章。

Conclusion: 风格保真度与统计可检测性可分离，该框架为未来作者身份建模、检测及身份条件生成研究提供了可复现的基础。

Abstract: Large language models (LLMs) can generate fluent text, but their ability to
replicate the distinctive style of a specific human author remains unclear. We
present a fast, training-free framework for authorship verification and style
imitation analysis. The method integrates TF-IDF character n-grams with
transformer embeddings and classifies text pairs through empirical distance
distributions, eliminating the need for supervised training or threshold
tuning. It achieves 97.5\% accuracy on academic essays and 94.5\% in
cross-domain evaluation, while reducing training time by 91.8\% and memory
usage by 59\% relative to parameter-based baselines. Using this framework, we
evaluate five LLMs from three separate families (Llama, Qwen, Mixtral) across
four prompting strategies - zero-shot, one-shot, few-shot, and text completion.
Results show that the prompting strategy has a more substantial influence on
style fidelity than model size: few-shot prompting yields up to 23.5x higher
style-matching accuracy than zero-shot, and completion prompting reaches 99.9\%
agreement with the original author's style. Crucially, high-fidelity imitation
does not imply human-like unpredictability - human essays average a perplexity
of 29.5, whereas matched LLM outputs average only 15.2. These findings
demonstrate that stylistic fidelity and statistical detectability are
separable, establishing a reproducible basis for future work in authorship
modeling, detection, and identity-conditioned generation.

</details>


### [173] [MobileLLM-R1: Exploring the Limits of Sub-Billion Language Model Reasoners with Open Training Recipes](https://arxiv.org/abs/2509.24945)
*Changsheng Zhao,Ernie Chang,Zechun Liu,Chia-Jung Chang,Wei Wen,Chen Lai,Rick Cao,Yuandong Tian,Raghuraman Krishnamoorthi,Yangyang Shi,Vikas Chandra*

Main category: cs.CL

TL;DR: 本文挑战了大型语言模型推理能力依赖超大规模语料训练的观点，证明了通过精心筛选和重新采样开源数据，使用大约2万亿高质量tokens即可训练出性能优异的推理模型。


<details>
  <summary>Details</summary>
Motivation: 当前普遍认为推理能力只在极大规模模型和海量数据训练下出现，本文质疑第二点，即推理模型必需依赖超过10万亿tokens的大规模语料训练。

Method: 通过设计指标筛选并重新采样优质开源数据集，利用约2万亿tokens构建训练集，进行4.2万亿tokens预训练及后训练，训练出一系列参数量不足十亿但推理能力强的MobileLLM-R1模型。

Result: MobileLLM-R1-950M在多个推理基准测试中表现优异，AIME评分15.5远超其他开源模型，同时其训练数据仅为行业领先模型Qwen3的11.7%，但推理能力匹配甚至超越Qwen3-0.6B。

Conclusion: 推理能力的出现不依赖超大规模训练语料，只要使用高质量数据并合理采样，小规模模型也能达到业内领先的推理水平，推动开源推理模型的研究与应用。

Abstract: The paradigm shift in large language models (LLMs) from instinctive responses
to chain-of-thought (CoT) reasoning has fueled two prevailing assumptions: (1)
reasoning capabilities only emerge in sufficiently large models, and (2) such
capabilities require training on massive datasets. While the first assumption
has already been challenged by recent sub-billion-parameter reasoning models
such as Qwen3-0.6B and DeepSeek distilled variants, the second remains largely
unquestioned. In this work, we revisit the necessity of scaling to extremely
large corpora (>10T tokens) for reasoning emergence. By carefully curating and
resampling open-source datasets that we identify as beneficial under our
designed metrics, we demonstrate that strong reasoning abilities can emerge
with far less data. Specifically, we show that only ~2T tokens of high-quality
data are sufficient, and pre-training with 4.2T tokens on the dataset resampled
from these ~2T tokens, followed by a established post-training procedure,
enables the development of MobileLLM-R1, a series of sub-billion-parameter
reasoning models that substantially outperform prior models trained on fully
open-sourced data. For example, MobileLLM-R1-950M achieves an AIME score of
15.5, compared to just 0.6 for OLMo-2-1.48B and 0.3 for SmolLM-2-1.7B.
Remarkably, despite being trained on only 11.7% of the tokens compared to
Qwen3's proprietary 36T-token corpus for pretraining, MobileLLM-R1-950M matches
or surpasses Qwen3-0.6B across multiple reasoning benchmarks. To facilitate
further research in this direction, we have released the complete training
recipe, data sources, data mixing ratio, and model checkpoints, together with
the key insights obtained throughout this study.

</details>


### [174] [The Dialogue That Heals: A Comprehensive Evaluation of Doctor Agents' Inquiry Capability](https://arxiv.org/abs/2509.24958)
*Linlu Gong,Ante Wang,Yunghwei Lai,Weizhi Ma,Yang Liu*

Main category: cs.CL

TL;DR: 本文提出了MAQuE，这是首个用于自动全面评估医疗多轮问诊的大型基准，包含多样化患者模拟，旨在衡量AI医生在问诊中的多方面能力。


<details>
  <summary>Details</summary>
Motivation: 当前AI医生虽然具备专家级的诊断技能和主动询问能力，但缺乏同理心、耐心和清晰沟通等医生必备的其他素质，因此需要一个全面评估医疗问诊质量的工具。

Method: 设计了包含3000个模拟患者角色的MAQuE基准，涵盖语言模式、认知限制、情感反应等多样性；同时建立多维度评价框架，包括任务成功率、询问熟练度、对话能力、询问效率及患者体验等指标。

Result: 在不同大型语言模型(LLM)上实验表明，模型在各评价维度面临显著挑战，且表现受患者行为变化影响较大，影响了诊断准确性。细粒度指标揭示了不同评价指标间存在权衡。

Conclusion: 尽管现有模型在医疗问诊方面取得进步，但在平衡性能与实际应用需求方面仍有较大提升空间，提示未来研究需关注多维度均衡改进。

Abstract: An effective physician should possess a combination of empathy, expertise,
patience, and clear communication when treating a patient. Recent advances have
successfully endowed AI doctors with expert diagnostic skills, particularly the
ability to actively seek information through inquiry. However, other essential
qualities of a good doctor remain overlooked. To bridge this gap, we present
MAQuE(Medical Agent Questioning Evaluation), the largest-ever benchmark for the
automatic and comprehensive evaluation of medical multi-turn questioning. It
features 3,000 realistically simulated patient agents that exhibit diverse
linguistic patterns, cognitive limitations, emotional responses, and tendencies
for passive disclosure. We also introduce a multi-faceted evaluation framework,
covering task success, inquiry proficiency, dialogue competence, inquiry
efficiency, and patient experience. Experiments on different LLMs reveal
substantial challenges across the evaluation aspects. Even state-of-the-art
models show significant room for improvement in their inquiry capabilities.
These models are highly sensitive to variations in realistic patient behavior,
which considerably impacts diagnostic accuracy. Furthermore, our fine-grained
metrics expose trade-offs between different evaluation perspectives,
highlighting the challenge of balancing performance and practicality in
real-world clinical settings.

</details>


### [175] [SemanticShield: LLM-Powered Audits Expose Shilling Attacks in Recommender Systems](https://arxiv.org/abs/2509.24961)
*Kaihong Li,Huichi Zhou,Bin Ma,Fangjun Huang*

Main category: cs.CL

TL;DR: 本文提出了SemanticShield，这是一个结合大语言模型的两阶段推荐系统刷单攻击检测框架，显著提升了检测效果和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法主要关注用户行为，忽视了商品标题和描述等项目侧特征，这些信息能够揭示恶意意图。

Method: 提出两阶段检测框架：第一阶段利用低成本行为准则预筛疑似用户，第二阶段通过大语言模型进行语义一致性审计；通过强化微调优化轻量级大语言模型，形成专用检测器SemanticShield。

Result: 在六种代表性攻击策略上，SemanticShield展现出有效的防御能力，在未见过的攻击方法测试中也表现出较强的泛化性。

Conclusion: 结合商品侧语义信息与大语言模型的两阶段检测框架有效防御刷单攻击，SemanticShield提供了强大且泛化能力突出的检测手段。

Abstract: Recommender systems (RS) are widely used in e-commerce for personalized
suggestions, yet their openness makes them susceptible to shilling attacks,
where adversaries inject fake behaviors to manipulate recommendations. Most
existing defenses emphasize user-side behaviors while overlooking item-side
features such as titles and descriptions that can expose malicious intent. To
address this gap, we propose a two-stage detection framework that integrates
item-side semantics via large language models (LLMs). The first stage
pre-screens suspicious users using low-cost behavioral criteria, and the second
stage employs LLM-based auditing to evaluate semantic consistency. Furthermore,
we enhance the auditing model through reinforcement fine-tuning on a
lightweight LLM with carefully designed reward functions, yielding a
specialized detector called SemanticShield. Experiments on six representative
attack strategies demonstrate the effectiveness of SemanticShield against
shilling attacks, and further evaluation on previously unseen attack methods
shows its strong generalization capability. Code is available at
https://github.com/FrankenstLee/SemanticShield.

</details>


### [176] [Generalized Correctness Models: Learning Calibrated and Model-Agnostic Correctness Predictors from Historical Patterns](https://arxiv.org/abs/2509.24988)
*Hanqi Xiao,Vaidehi Patil,Hyunji Lee,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.CL

TL;DR: 本文提出了通用正确性模型（GCM），通过注入目标模型的历史预测信息，提升大规模语言模型（LLM）对答案正确性的估计准确性和校准性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在高风险应用中准确估计自身回答的置信度仍具挑战，传统方法依赖模型自身判断正确性的能力，但效果有限。

Method: 通过注入目标模型的历史正确性数据训练通用正确性模型（GCM），探索不同注入历史信息的方法，包括训练GCM和使用上下文示例及后校准。

Result: GCM可跨多个数据集和模型泛化正确性预测能力，发现答案措辞是正确性的重要预测因子，并且历史信息的注入显著提升了置信度估计的表现。

Conclusion: 可靠的LLM置信度估计是通过系统编码正确性历史而获得的通用技能，而非模型依赖的自省能力。GCM为改进LLM置信度估计提供了有效方案，适用于多种模型和应用场景。

Abstract: Generating accurate and calibrated confidence estimates is critical for
deploying LLMs in high-stakes or user-facing applications, and remains an open
challenge. Prior research has often framed confidence as a problem of eliciting
a model's "self-knowledge", i.e., the ability of an LLM to judge whether its
own answers are correct; this approach implicitly assumes that there is some
privileged information about the answer's correctness that is accessible to the
model itself. However, our experiments reveal that an LLM attempting to predict
the correctness of its own outputs generally performs no better than an
unrelated LLM. Moreover, we hypothesize that a key factor in building a
"Correctness Model" (CM) is exposure to a target model's historical
predictions. We propose multiple methods to inject this historical correctness
information, creating a Generalized Correctness Model (GCM). We first show that
GCMs can be trained on the correctness data from many LLMs and learn patterns
for correctness prediction applicable across datasets and models. We then use
CMs as a lens for studying the source of correctness prediction ability and its
generalization, systematically controlling their training data and finding that
answer phrasing is a strong predictor for correctness. We further explore
alternative methods of injecting history without training an LLM, finding that
including history as in-context examples can help improve correctness
prediction, and post-hoc calibration can provide complementary reductions in
calibration error. We evaluate GCMs based on Qwen3-8B across 5 model families
and the MMLU and TriviaQA datasets, as well as on a downstream selective
prediction task, finding that reliable LLM confidence estimation is a
generalizable and model-agnostic skill learned by systematically encoding
correctness history rather than a model-specific skill reliant on
self-introspection.

</details>


### [177] [Circuit Distillation](https://arxiv.org/abs/2509.25002)
*Somin Wadhwa,Silvio Amir,Byron C. Wallace*

Main category: cs.CL

TL;DR: 本文提出了一种新的模型蒸馏方法——电路蒸馏，通过对齐教师模型与学生模型内部的功能对应电路组件的表示，优于传统的行为模仿蒸馏方法。


<details>
  <summary>Details</summary>
Motivation: 传统模型蒸馏只注重复制教师模型的输出，忽视了内部计算机制，使得难以有效传递复杂算法能力。

Method: 引入电路蒸馏方法，将教师与学生模型的内部电路组件进行功能对应匹配，并设计损失函数使其内部表示相似，从而迁移教师模型的计算机制。

Result: 在实体跟踪和理论心理任务上，使用Llama3系列模型，电路蒸馏表现优于标准蒸馏，成功通过调整少量学生参数转移算法能力。

Conclusion: 该方法验证了通过内部计算机制的迁移实现模型蒸馏的可行性，有助于高效蒸馏具有解释性和可控性的教师模型能力。

Abstract: Model distillation typically focuses on behavioral mimicry, where a student
model is trained to replicate a teacher's output while treating its internal
computations as a black box. In this work we propose an alternative approach:
Distilling the underlying computational mechanisms implemented by a teacher
model. Specifically, we propose circuit distillation, which introduces an
objective to align internal representations between analogous circuit
components in teacher and student models. We propose a method to match
``functionally correspondent'' circuit components and introduce a loss
reflecting similarities between the representations that these induce. We
evaluate circuit distillation on entity tracking and theory of mind (ToM) tasks
using models from the Llama3 family. Our results demonstrate that circuit
distillation outperforms standard distillation, successfully transferring
algorithmic capabilities by adjusting only a small, targeted subset of student
model parameters. This work establishes the feasibility of transferring
mechanisms, which may in turn allow for efficient distillation of targeted
teacher capabilities via interpretable and controllable internal student
mechanisms.

</details>


### [178] [Ultra-Fast Language Generation via Discrete Diffusion Divergence Instruct](https://arxiv.org/abs/2509.25035)
*Haoyang Zheng,Xinyang Liu,Cindy Xiangrui Kong,Nan Jiang,Zheyuan Hu,Weijian Luo,Wei Deng,Guang Lin*

Main category: cs.CL

TL;DR: DiDi-Instruct是一种基于训练的快速语言生成方法，通过初始化预训练的离散扩散语言模型，实现了64倍加速，优于GPT-2及其他模型。


<details>
  <summary>Details</summary>
Motivation: 追求在AI时代实现快速的语言文本生成。

Method: 提出DiDi-Instruct方法，基于积分KL散度最小化框架，结合分组奖励归一化、中间状态匹配和奖励引导采样等技术，优化训练稳定性和推理性能。

Result: 在OpenWebText数据集上，DiDi-Instruct在减少训练时间的同时，显著提升了生成质量和加速性能，样本困惑度最低达18.4，训练时间减少20倍。

Conclusion: DiDi-Instruct是一种高效且有效的蒸馏方法，实现了高速且高质量的语言生成。

Abstract: Fast generation of language texts is the holy grail that people pursue in the
AI era. In this work, we introduced Discrete Diffusion Divergence Instruct
(DiDi-Instruct), a training-based method that leads to fast language generation
models by initializing from a pre-trained (masked) discrete diffusion language
model (dLLM). The resulting DiDi-Instruct model outperforms the dLLM
counterparts and the GPT-2 baseline with 64x acceleration. In the theoretical
part of the paper, we build the foundation of DiDi-Instruct in a framework of
integral KL-divergence minimization, with practical training algorithms. We
also introduce techniques like grouped reward normalization, intermediate-state
matching, and the reward-guided ancestral sampler (RGAS) that significantly
improve the training stability, the model coverage, and the inference
performances. On OpenWebText, DiDi-Instruct outperforms all accelerated
language generation models as well as the GPT-2 baseline and the standard
dLLMs, achieving sample perplexities ranging from 62.2 (8 NFEs) to 18.4 (128
NFEs). These performance gains are accomplished with a negligible entropy loss
of about 1% and 20x less additional training wall-clock time. We further
validate the robustness and effectiveness of DiDi-Instruct through extensive
ablation studies, model scaling, and the generation of discrete protein
sequences. In conclusion, DiDi-Instruct is an efficient yet effective
distillation method, enabling language generation in the blink of an eye. We
will release both code and models at github.com/haoyangzheng-ai/didi-instruct.

</details>


### [179] [GateMABSA: Aspect-Image Gated Fusion for Multimodal Aspect-based Sentiment Analysis](https://arxiv.org/abs/2509.25037)
*Adamu Lawan,Haruna Yunusa*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的多模态门控模型GateMABSA，用于解决多模态基于方面的情感分析中的视觉噪声过滤和模态对齐问题。


<details>
  <summary>Details</summary>
Motivation: 现有多模态ABSA模型难以有效过滤视觉噪声，并且难以在多模态中准确对齐方面与表达情感内容。

Method: GateMABSA设计了三个专用的门控多模态LSTM：Syn-mLSTM融合句法结构，Sem-mLSTM强调方面和语义的关联，Fuse-mLSTM选择性融合多模态信息。

Result: 在两个Twitter基准数据集上，GateMABSA的性能优于多种基线模型。

Conclusion: GateMABSA通过引入句法、语义和融合感知的门控机制，有效提升了多模态基于方面情感分析的准确性。

Abstract: Aspect-based Sentiment Analysis (ABSA) has recently advanced into the
multimodal domain, where user-generated content often combines text and images.
However, existing multimodal ABSA (MABSA) models struggle to filter noisy
visual signals, and effectively align aspects with opinion-bearing content
across modalities. To address these challenges, we propose GateMABSA, a novel
gated multimodal architecture that integrates syntactic, semantic, and
fusion-aware mLSTM. Specifically, GateMABSA introduces three specialized
mLSTMs: Syn-mLSTM to incorporate syntactic structure, Sem-mLSTM to emphasize
aspect--semantic relevance, and Fuse-mLSTM to perform selective multimodal
fusion. Extensive experiments on two benchmark Twitter datasets demonstrate
that GateMABSA outperforms several baselines.

</details>


### [180] [Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic Architectures](https://arxiv.org/abs/2509.25045)
*Marco Bronzini,Carlo Nicolini,Bruno Lepri,Jacopo Staiano,Andrea Passerini*

Main category: cs.CL

TL;DR: 本文提出了超维探测器，一种通过向量符号架构将大型语言模型（LLM）的残余流映射为可解释概念的新方法，突破了现有解释方法的局限。


<details>
  <summary>Details</summary>
Motivation: 现有对大型语言模型内部表示的解释方法存在词汇限制或特征模糊等问题，难以有效解码模型向量空间中的信息。

Method: 结合符号表示和神经探测技术，利用向量符号架构（VSA）将模型残余流映射到可解释概念空间，整合稀疏自编码器和传统探针的优势。

Result: 实验表明该探测器能在多种模型、不同嵌入维度和输入领域中可靠提取有意义的概念，且能帮助发现模型失败案例。

Conclusion: 该工作推动了对大型语言模型向量空间信息解码的发展，实现了从神经表示中提取更丰富、结构化和易解释特征。

Abstract: Despite their capabilities, Large Language Models (LLMs) remain opaque with
limited understanding of their internal representations. Current
interpretability methods, such as direct logit attribution (DLA) and sparse
autoencoders (SAEs), provide restricted insight due to limitations such as the
model's output vocabulary or unclear feature names. This work introduces
Hyperdimensional Probe, a novel paradigm for decoding information from the LLM
vector space. It combines ideas from symbolic representations and neural
probing to project the model's residual stream into interpretable concepts via
Vector Symbolic Architectures (VSAs). This probe combines the strengths of SAEs
and conventional probes while overcoming their key limitations. We validate our
decoding paradigm with controlled input-completion tasks, probing the model's
final state before next-token prediction on inputs spanning syntactic pattern
recognition, key-value associations, and abstract inference. We further assess
it in a question-answering setting, examining the state of the model both
before and after text generation. Our experiments show that our probe reliably
extracts meaningful concepts across varied LLMs, embedding sizes, and input
domains, also helping identify LLM failures. Our work advances information
decoding in LLM vector space, enabling extracting more informative,
interpretable, and structured features from neural representations.

</details>


### [181] [Confidence-Guided Error Correction for Disordered Speech Recognition](https://arxiv.org/abs/2509.25048)
*Abner Hernandez,Tomás Arias Vergara,Andreas Maier,Paula Andrea Pérez-Toro*

Main category: cs.CL

TL;DR: 该论文提出了一种基于置信度信息的提示方法，用于利用大语言模型改进自动语音识别的错误纠正，特别是针对障碍语音，提高了识别准确率。


<details>
  <summary>Details</summary>
Motivation: 传统利用大语言模型进行自动语音识别后处理时，存在过度纠正和对残障语音鲁棒性差的问题，因此需要引入置信度信息以提升模型的泛化能力和鲁棒性。

Method: 作者提出置信度引导的提示方法，将词级别的不确定性估计直接嵌入大语言模型训练中，通过对LLaMA 3.1模型进行微调，重点纠正ASR识别中不确定区域。

Result: 在Speech Accessibility Project的自发语音数据集上，相较于传统LLM纠错方法，错误率降低了10%；在TORGO数据集上，错误率降低达47%。

Conclusion: 置信度感知的微调策略显著提升了大语言模型对障碍语音的纠错效果，证明了该方法在自动语音识别后处理中的有效性。

Abstract: We investigate the use of large language models (LLMs) as post-processing
modules for automatic speech recognition (ASR), focusing on their ability to
perform error correction for disordered speech. In particular, we propose
confidence-informed prompting, where word-level uncertainty estimates are
embedded directly into LLM training to improve robustness and generalization
across speakers and datasets. This approach directs the model to uncertain ASR
regions and reduces overcorrection. We fine-tune a LLaMA 3.1 model and compare
our approach to both transcript-only fine-tuning and post hoc confidence-based
filtering. Evaluations show that our method achieves a 10% relative WER
reduction compared to naive LLM correction on the Speech Accessibility Project
spontaneous speech and a 47% reduction on TORGO, demonstrating the
effectiveness of confidence-aware fine-tuning for impaired speech.

</details>


### [182] [An empirical study on the limitation of Transformers in program trace generation](https://arxiv.org/abs/2509.25073)
*Simeng Sun*

Main category: cs.CL

TL;DR: 这篇论文研究了使用Transformer模型生成程序执行轨迹的任务，提出了多种模型改进方法，并分析了模型在泛化能力上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有算法问题通常不需要生成长且每步简单的执行轨迹，PTG任务通过生成详细的执行轨迹外化推理过程，旨在提升模型对复杂任务的理解能力。

Method: 训练了包含多样修改的小型Transformer模型，改进包括不同的位置编码方式、softmax替代方法、混合模型设计及短卷积结构。

Result: 这些模型在分布内测试中表现出较强准确率，但在泛化到例如程序长度和轨迹步数等变化时系统性失败，部分设计显著提升了泛化性能。

Conclusion: 通过对多种Transformer结构的比较，论文揭示了模型在PTG任务中泛化的挑战及潜在改进方向，为未来提升算法推理模型的泛化能力提供参考。

Abstract: We study Transformers on the task \emph{program trace generation} (PTG),
where models produce step-by-step execution traces for synthetic programs.
Unlike existing algorithmic problems, PTG externalizes reasoning through long
traces where each step is trivial. We train small Transformers with diverse
modifications, including alternative position encodings, softmax replacements,
hybrid model, and short convolutions. While these models achieve strong
in-distribution accuracy, they exhibit systematic failures when generalizing to
various factors (e.g., program length, trace steps), though some designs
significantly improve generalization.

</details>


### [183] [Scaling Generalist Data-Analytic Agents](https://arxiv.org/abs/2509.25084)
*Shuofei Qiao,Yanqiu Zhao,Zhisong Qiu,Xiaobin Wang,Jintian Zhang,Zhao Bin,Ningyu Zhang,Yong Jiang,Pengjun Xie,Fei Huang,Huajun Chen*

Main category: cs.CL

TL;DR: 提出了DataMind，一个用于构建通用数据分析智能体的可扩展数据合成与训练方案，通过细粒度任务分类、知识增强采样、多任务训练目标及稳定的多轮代码执行，提升了开源模型在多样化数据分析任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有数据分析智能体依赖专有模型和提示设计，开源模型难以应对多格式、大规模数据及长时、多步推理的现实需求，亟需一个有效的开源训练框架。

Method: DataMind通过细粒度任务分类和递归任务组合丰富数据多样性，采用知识增强轨迹采样及规则过滤，结合SFT与强化学习训练目标，构建内存高效且稳定的多轮代码执行机制。

Result: 基于DataMind训练的DataMind-14B在多项数据分析评测中以71.16%的平均得分超过了DeepSeek-V3.1和GPT-5等专有模型，DataMind-7B在开源模型中表现最佳。

Conclusion: DataMind有效提升了开源数据分析智能体能力，推动了自动科学发现和创新AI愿景的发展，相关数据集和模型将公开供社区研究。

Abstract: Data-analytic agents are emerging as a key catalyst for automated scientific
discovery and for the vision of Innovating AI. Current approaches, however,
rely heavily on prompt engineering over proprietary models, while open-source
models struggle to face diverse-format, large-scale data files and
long-horizon, multi-step reasoning that real-world analytics demands. This
paper introduces DataMind, a scalable data synthesis and agent training recipe
designed to build generalist data-analytic agents. DataMind tackles three key
challenges in building open-source data-analytic agents, including insufficient
data resources, improper training strategy, and unstable code-based multi-turn
rollout. Concretely, DataMind applies 1) a fine-grained task taxonomy and a
recursive easy-to-hard task composition mechanism to increase the diversity and
difficulty of synthesized queries; 2) a knowledge-augmented trajectory sampling
strategy followed by model-based and rule-based filtering; 3) a dynamically
adjustable training objective combining both SFT and RL losses; 4) a
memory-frugal and stable code-based multi-turn rollout framework. Built on
DataMind, we curate DataMind-12K, a high-quality trajectory set spanning
diverse domains, task categories, and data file formats for data-analytic
tasks. Trained on DataMind-12K, our DataMind-14B achieves state-of-the-art with
an average score of 71.16% on multiple data analysis benchmarks, outperforming
the strongest proprietary baselines DeepSeek-V3.1 and GPT-5. Our DataMind-7B
also performs best among all open-source models with a score of 68.10%. We also
incorporate some empirical insights gained from our exploratory trials into the
analysis experiments, aiming to provide actionable insights about agentic
training for the community. We will release DataMind-12K and DataMind-7B,14B
for the community's future research.

</details>


### [184] [jina-reranker-v3: Last but Not Late Interaction for Document Reranking](https://arxiv.org/abs/2509.25085)
*Feng Wang,Yuqing Li,Han Xiao*

Main category: cs.CL

TL;DR: jina-reranker-v3是一种参数量为0.6亿的多语言文档重排序器，采用创新的“最后但不晚”交互机制，实现了高效的多文档交互和优秀的排名效果。


<details>
  <summary>Details</summary>
Motivation: 现有的late interaction模型如ColBERT先单独编码然后多向量匹配，缺乏充分的跨文档交互，且模型较大。

Method: 提出在同一上下文窗口内对查询和文档进行因果自注意力机制，实现丰富的跨文档交互，最后从每个文档的最后一个Token提取上下文嵌入。

Result: 模型在BEIR基准上的nDCG@10达到61.94，性能领先且参数量是生成式列表重排序器的十分之一。

Conclusion: 通过创新的交互机制，jina-reranker-v3在保持模型紧凑的同时，实现了跨文档信息的有效融合和先进的排序性能。

Abstract: jina-reranker-v3 is a 0.6B parameter multilingual document reranker that
introduces a novel last but not late interaction. Unlike late interaction
models such as ColBERT that perform separate encoding followed by multi-vector
matching, our approach conducts causal self-attention between query and
documents within the same context window, enabling rich cross-document
interactions before extracting contextual embeddings from the last token of
each document. This compact architecture achieves state-of-the-art BEIR
performance with 61.94 nDCG@10 while being ten times smaller than generative
listwise rerankers.

</details>


### [185] [Towards Trustworthy Lexical Simplification: Exploring Safety and Efficiency with Small LLMs](https://arxiv.org/abs/2509.25086)
*Akio Hayakawa,Stefan Bott,Horacio Saggion*

Main category: cs.CL

TL;DR: 本文针对大语言模型在词汇简化中的现实应用挑战，提出基于小型语言模型的高效词汇简化框架，实现本地部署，兼顾性能与隐私安全。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在词汇简化任务表现优异，但在隐私敏感和资源受限环境中难以应用，且考虑到弱势用户群体，输出的安全性和正确性尤为重要。

Method: 提出基于小型语言模型的词汇简化框架，探索合成数据的知识蒸馏与上下文学习方法，结合手动与自动评测，多语言实验；利用模型输出概率检测并过滤有害简化。

Result: 知识蒸馏提升自动评测指标，但增加有害简化，模型输出概率可作为过滤有害简化的有效信号，通过筛选策略抑制有害简化，同时保留有益简化。

Conclusion: 建立了小型语言模型高效安全词汇简化基准，揭示性能、效率与安全的关键权衡，提出了适合现实安全部署的有效方案。

Abstract: Despite their strong performance, large language models (LLMs) face
challenges in real-world application of lexical simplification (LS),
particularly in privacy-sensitive and resource-constrained environments.
Moreover, since vulnerable user groups (e.g., people with disabilities) are one
of the key target groups of this technology, it is crucial to ensure the safety
and correctness of the output of LS systems. To address these issues, we
propose an efficient framework for LS systems that utilizes small LLMs
deployable in local environments. Within this framework, we explore knowledge
distillation with synthesized data and in-context learning as baselines. Our
experiments in five languages evaluate model outputs both automatically and
manually. Our manual analysis reveals that while knowledge distillation boosts
automatic metric scores, it also introduces a safety trade-off by increasing
harmful simplifications. Importantly, we find that the model's output
probability is a useful signal for detecting harmful simplifications.
Leveraging this, we propose a filtering strategy that suppresses harmful
simplifications while largely preserving beneficial ones. This work establishes
a benchmark for efficient and safe LS with small LLMs. It highlights the key
trade-offs between performance, efficiency, and safety, and demonstrates a
promising approach for safe real-world deployment.

</details>


### [186] [Towards Personalized Deep Research: Benchmarks and Evaluations](https://arxiv.org/abs/2509.25106)
*Yuan Liang,Jiaxian Li,Yuqing Wang,Piaohong Wang,Motong Tian,Pai Liu,Shuofei Qiao,Runnan Fang,He Zhu,Ge Zhang,Minghao Liu,Yuchen Eleanor Jiang,Ningyu Zhang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: 该论文提出了个性化深度研究基准（Personalized Deep Research Bench, PDRB），用于评估深度研究代理（DRA）的个性化能力，包含多领域任务和真实用户画像，并设计了PQR评价框架，综合评价个性化、一致性及事实可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有评估多依赖封闭式基准，缺乏开放式且涵盖个性化场景的深度研究评测，难以反映DRA在真实个性化环境中的表现。

Method: 构建了PDRB基准，包括50个多领域任务与25个真实用户画像组合，产生250个用户任务查询；提出PQR评价框架，联合评估个性化匹配、内容质量和事实可靠性。

Result: 通过在多种系统上的实验，揭示了当前深度研究代理在个性化处理方面的能力和局限。

Conclusion: 该工作为开发和评估下一代个性化AI研究助手奠定了扎实的标准和基础，推动个性化深度研究代理的发展。

Abstract: Deep Research Agents (DRAs) can autonomously conduct complex investigations
and generate comprehensive reports, demonstrating strong real-world potential.
However, existing evaluations mostly rely on close-ended benchmarks, while
open-ended deep research benchmarks remain scarce and typically neglect
personalized scenarios. To bridge this gap, we introduce Personalized Deep
Research Bench, the first benchmark for evaluating personalization in DRAs. It
pairs 50 diverse research tasks across 10 domains with 25 authentic user
profiles that combine structured persona attributes with dynamic real-world
contexts, yielding 250 realistic user-task queries. To assess system
performance, we propose the PQR Evaluation Framework, which jointly measures
(P) Personalization Alignment, (Q) Content Quality, and (R) Factual
Reliability. Our experiments on a range of systems highlight current
capabilities and limitations in handling personalized deep research. This work
establishes a rigorous foundation for developing and evaluating the next
generation of truly personalized AI research assistants.

</details>


### [187] [Knowledge Extraction on Semi-Structured Content: Does It Remain Relevant for Question Answering in the Era of LLMs?](https://arxiv.org/abs/2509.25107)
*Kai Sun,Yin Huang,Srishti Mehra,Mohammad Kachuee,Xilun Chen,Renjie Tao,Zhaojiang Lin,Andrea Jessee,Nirav Shah,Alex Betty,Yue Liu,Anuj Kumar,Wen-tau Yih,Xin Luna Dong*

Main category: cs.CL

TL;DR: 本文探讨了在大型语言模型（LLMs）推动的基于网络的问答系统中，知识三元组提取的实际价值。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在问答准确率上表现出色，但其在知识三元组提取上的能力尚有挑战，探究知识提取是否仍具价值。

Method: 通过扩展已有基准数据集，加入知识提取标注，并评估不同规模的商用和开源LLMs，检验知识提取对问答性能的提升作用。

Result: 研究表明，知识三元组提取对LLMs仍有挑战；不过，结合提取的三元组进行增强和多任务学习，可以提高问答系统表现。

Conclusion: 知识三元组提取仍在基于网络的问答系统中发挥重要作用，适当利用此技术能提升不同规模和资源设置下LLMs的效果。

Abstract: The advent of Large Language Models (LLMs) has significantly advanced
web-based Question Answering (QA) systems over semi-structured content, raising
questions about the continued utility of knowledge extraction for question
answering. This paper investigates the value of triple extraction in this new
paradigm by extending an existing benchmark with knowledge extraction
annotations and evaluating commercial and open-source LLMs of varying sizes.
Our results show that web-scale knowledge extraction remains a challenging task
for LLMs. Despite achieving high QA accuracy, LLMs can still benefit from
knowledge extraction, through augmentation with extracted triples and
multi-task learning. These findings provide insights into the evolving role of
knowledge triple extraction in web-based QA and highlight strategies for
maximizing LLM effectiveness across different model sizes and resource
settings.

</details>


### [188] [Investigating Language and Retrieval Bias in Multilingual Previously Fact-Checked Claim Detection](https://arxiv.org/abs/2509.25138)
*Ivan Vykopal,Antonia Karamolegkou,Jaroslav Kopčan,Qiwei Peng,Tomáš Javůrek,Michal Gregor,Marián Šimko*

Main category: cs.CL

TL;DR: 本文研究了多语言大型语言模型在跨语言事实核查中的语言偏差和检索偏差，评估了6个模型在20种语言中的表现，揭示了模型在高资源语言上的优势及某些事实被过度检索的问题。


<details>
  <summary>Details</summary>
Motivation: 多语言大型语言模型在跨语言事实核查中表现不均，偏向高资源语言，且信息检索系统存在检索偏差，影响事实核查的公平性和准确性。

Method: 利用AMC-16K数据集和全多语言提示策略，评估6个开源多语言模型在20种语言上的单语言和跨语言表现；通过翻译提示词分析性能差异；使用多语言嵌入模型分析检索过程中的偏差。

Result: 发现模型表现存在显著语言偏差，高资源语言表现明显优于低资源语言；检索过程中某些事实被频繁检索，导致热门事实检索表现被高估，冷门事实则被忽视。

Conclusion: 多语言大型语言模型在事实核查任务中仍存在语言和检索偏差，需要改进模型设计与检索策略，提高多语言事实核查的公平性和全面性。

Abstract: Multilingual Large Language Models (LLMs) offer powerful capabilities for
cross-lingual fact-checking. However, these models often exhibit language bias,
performing disproportionately better on high-resource languages such as English
than on low-resource counterparts. We also present and inspect a novel concept
- retrieval bias, when information retrieval systems tend to favor certain
information over others, leaving the retrieval process skewed. In this paper,
we study language and retrieval bias in the context of Previously Fact-Checked
Claim Detection (PFCD). We evaluate six open-source multilingual LLMs across 20
languages using a fully multilingual prompting strategy, leveraging the AMC-16K
dataset. By translating task prompts into each language, we uncover disparities
in monolingual and cross-lingual performance and identify key trends based on
model family, size, and prompting strategy. Our findings highlight persistent
bias in LLM behavior and offer recommendations for improving equity in
multilingual fact-checking. To investigate retrieval bias, we employed
multilingual embedding models and look into the frequency of retrieved claims.
Our analysis reveals that certain claims are retrieved disproportionately
across different posts, leading to inflated retrieval performance for popular
claims while under-representing less common ones.

</details>


### [189] [Paired by the Teacher: Turning Unpaired Data into High-Fidelity Pairs for Low-Resource Text Generation](https://arxiv.org/abs/2509.25144)
*Yen-Ju Lu,Thomas Thebaud,Laureano Moro-Velazquez,Najim Dehak,Jesus Villalba*

Main category: cs.CL

TL;DR: PbT是一种两阶段教师-学生方法，用于在没有人工标签或平行数据的情况下，合成高质量的输入-输出对，显著提升低资源NLG任务的性能。


<details>
  <summary>Details</summary>
Motivation: 在低资源自然语言生成场景中，通常只有输入或输出中的一部分数据，缺乏配对数据，导致模型训练困难或依赖昂贵的大型语言模型合成数据。

Method: PbT通过让教师大型语言模型将未配对示例压缩成中间表示，再由学生模型从中间表示重构输入，实现高质量的合成数据生成。

Result: 在五个基准任务中，使用PbT训练的学生模型性能优于基于大型教师生成的70B语料和其他无监督方法，并接近人类标注数据水平。

Conclusion: PbT通过生成符合领域特征的高质量合成数据，显著降低标注成本，提升了低资源NLG任务的效果，具备应用推广价值。

Abstract: We present Paired by the Teacher (PbT), a two-stage teacher-student pipeline
that synthesizes accurate input-output pairs without human labels or parallel
data. In many low-resource natural language generation (NLG) scenarios,
practitioners may have only raw outputs, like highlights, recaps, or questions,
or only raw inputs, such as articles, dialogues, or paragraphs, but seldom
both. This mismatch forces small models to learn from very few examples or rely
on costly, broad-scope synthetic examples produced by large LLMs. PbT addresses
this by asking a teacher LLM to compress each unpaired example into a concise
intermediate representation (IR), and training a student to reconstruct inputs
from IRs. This enables outputs to be paired with student-generated inputs,
yielding high-quality synthetic data. We evaluate PbT on five
benchmarks-document summarization (XSum, CNNDM), dialogue summarization
(SAMSum, DialogSum), and question generation (SQuAD)-as well as an unpaired
setting on SwitchBoard (paired with DialogSum summaries). An 8B student trained
only on PbT data outperforms models trained on 70 B teacher-generated corpora
and other unsupervised baselines, coming within 1.2 ROUGE-L of human-annotated
pairs and closing 82% of the oracle gap at one-third the annotation cost of
direct synthesis. Human evaluation on SwitchBoard further confirms that only
PbT produces concise, faithful summaries aligned with the target style,
highlighting its advantage of generating in-domain sources that avoid the
mismatch, limiting direct synthesis.

</details>


### [190] [Pretraining Large Language Models with NVFP4](https://arxiv.org/abs/2509.25149)
*NVIDIA,Felix Abecassis,Anjulie Agrusa,Dong Ahn,Jonah Alben,Stefania Alborghetti,Michael Andersch,Sivakumar Arayandi,Alexis Bjorlin,Aaron Blakeman,Evan Briones,Ian Buck,Bryan Catanzaro,Jinhang Choi,Mike Chrzanowski,Eric Chung,Victor Cui,Steve Dai,Bita Darvish Rouhani,Carlo del Mundo,Deena Donia,Burc Eryilmaz,Henry Estela,Abhinav Goel,Oleg Goncharov,Yugi Guvvala,Robert Hesse,Russell Hewett,Herbert Hum,Ujval Kapasi,Brucek Khailany,Mikail Khona,Nick Knight,Alex Kondratenko,Ronny Krashinsky,Ben Lanir,Simon Layton,Michael Lightstone,Daniel Lo,Paulius Micikevicius,Asit Mishra,Tim Moon,Deepak Narayanan,Chao Ni,Abhijit Paithankar,Satish Pasumarthi,Ankit Patel,Mostofa Patwary,Ashwin Poojary,Gargi Prasad,Sweta Priyadarshi,Yigong Qin,Xiaowei Ren,Oleg Rybakov,Charbel Sakr,Sanjeev Satheesh,Stas Sergienko,Pasha Shamis,Kirthi Shankar,Nishant Sharma,Mohammad Shoeybi,Michael Siu,Misha Smelyanskiy,Darko Stosic,Dusan Stosic,Bor-Yiing Su,Frank Sun,Nima Tajbakhsh,Shelby Thomas,Przemek Tredak,Evgeny Tsykunov,Gandhi Vaithilingam,Aditya Vavre,Rangharajan Venkatesan,Roger Waleffe,Qiyu Wan,Hexin Wang,Mengdi Wang,Lizzie Wei,Hao Wu,Evan Wu,Keith Wyss,Ning Xu,Jinze Xue,Charlene Yang,Yujia Zhai,Ruoxi Zhang,Jingyang Zhu,Zhongbo Zhu*

Main category: cs.CL

TL;DR: 本文提出了一种基于NVFP4格式的4位浮点数训练大语言模型的新方法，实现了稳定且高效的训练效果。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型训练计算资源消耗巨大，提升预训练效率至关重要，尤其在采用更低精度（如4位浮点数）训练时面临稳定性和收敛性挑战。

Method: 引入随机Hadamard变换控制块级异常值，采用二维量化方案确保前向和反向表示一致，使用随机舍入减少梯度偏差，并选择性使用高精度层。

Result: 成功训练了一个120亿参数、使用4位精度训练的模型，训练损失和下游任务准确率与FP8基线相当，验证了方法的有效性。

Conclusion: 结合NVFP4格式和新训练方法，4位精度训练LLM成为可行且有效的技术路线，是窄精度LLM训练算法的重要进展。

Abstract: Large Language Models (LLMs) today are powerful problem solvers across many
domains, and they continue to get stronger as they scale in model size,
training set size, and training set quality, as shown by extensive research and
experimentation across the industry. Training a frontier model today requires
on the order of tens to hundreds of yottaflops, which is a massive investment
of time, compute, and energy. Improving pretraining efficiency is therefore
essential to enable the next generation of even more capable LLMs. While 8-bit
floating point (FP8) training is now widely adopted, transitioning to even
narrower precision, such as 4-bit floating point (FP4), could unlock additional
improvements in computational speed and resource utilization. However,
quantization at this level poses challenges to training stability, convergence,
and implementation, notably for large-scale models trained on long token
horizons.
  In this study, we introduce a novel approach for stable and accurate training
of large language models (LLMs) using the NVFP4 format. Our method integrates
Random Hadamard transforms (RHT) to bound block-level outliers, employs a
two-dimensional quantization scheme for consistent representations across both
the forward and backward passes, utilizes stochastic rounding for unbiased
gradient estimation, and incorporates selective high-precision layers. We
validate our approach by training a 12-billion-parameter model on 10 trillion
tokens -- the longest publicly documented training run in 4-bit precision to
date. Our results show that the model trained with our NVFP4-based pretraining
technique achieves training loss and downstream task accuracies comparable to
an FP8 baseline. These findings highlight that NVFP4, when combined with our
training approach, represents a major step forward in narrow-precision LLM
training algorithms.

</details>


### [191] [EasySteer: A Unified Framework for High-Performance and Extensible LLM Steering](https://arxiv.org/abs/2509.25175)
*Haolei Xu,Xinyu Mei,Yuchen Yan,Rui Zhou,Wenqi Zhang,Weiming Lu,Yueting Zhuang,Yongliang Shen*

Main category: cs.CL

TL;DR: EasySteer是一个基于vLLM的统一高性能大语言模型（LLM）操控框架，具备模块化结构和多种控制方法，显著提升推理效率，实现多应用场景下的模型行为控制。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM操控框架存在计算效率低、扩展性差和功能受限的问题，阻碍了研究和实际应用的进展。

Method: 提出EasySteer，构建于vLLM优化推理引擎之上，采用模块化架构和可插拔接口，支持分析和学习驱动的操控方法，预计算多个应用领域的操控向量，并提供交互式演示。

Result: EasySteer在速度上相比现有框架提升了5.5-11.4倍，实验验证其在防止过度思考、减少幻觉等应用中效果显著。

Conclusion: EasySteer将LLM操控由研究技术转化为生产级能力，为可部署且可控的语言模型建立了关键基础设施。

Abstract: Large language model (LLM) steering has emerged as a promising paradigm for
controlling model behavior at inference time through targeted manipulation of
hidden states, offering a lightweight alternative to expensive retraining.
However, existing steering frameworks suffer from critical limitations:
computational inefficiency, limited extensibility, and restricted functionality
that hinder both research progress and practical deployment. We present
EasySteer, a unified framework for high-performance, extensible LLM steering
built on vLLM. Our system features modular architecture with pluggable
interfaces for both analysis-based and learning-based methods, fine-grained
parameter control, pre-computed steering vectors for eight application domains,
and an interactive demonstration system. Through deep integration with vLLM's
optimized inference engine, EasySteer achieves 5.5-11.4$\times$ speedup over
existing frameworks. Extensive experiments demonstrate its effectiveness in
overthinking mitigation, hallucination reduction, and other key applications.
EasySteer transforms steering from research technique to production-ready
capability, establishing critical infrastructure for deployable, controllable
language models.

</details>


### [192] [NAIPv2: Debiased Pairwise Learning for Efficient Paper Quality Estimation](https://arxiv.org/abs/2509.25179)
*Penghai Zhao,Jinyu Tian,Qinghua Xing,Xin Zhang,Zheng Li,Jianjun Qian,Ming-Ming Cheng,Xiang Li*

Main category: cs.CL

TL;DR: 提出了NAIPv2，一种去偏且高效的科学论文质量评估框架，结合领域-年组内的成对学习和评论倾向信号，提高评分一致性与模型性能，同时支持线性时间推理。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的评分方法推理成本高，直接回归方法因规模不一致性限制性能，需要一种高效且去偏的论文质量评价方法。

Method: NAIPv2利用领域-年组内成对学习减少评级不一致，引入评论倾向信号合并评论分数和置信度，并构建了包含24,276篇论文的NAIDv2数据集进行训练和评估。

Result: NAIPv2在ICLR数据集上达到了78.2%的AUC和0.432的Spearman相关系数，且推理线性高效；在NeurIPS测试上泛化能力强，评分能够区分拒稿到口头报告的类别。

Conclusion: NAIPv2提供了一种去偏、高效且具有良好泛化能力的自动论文质量估计框架，为未来科学智能系统的发展奠定基础。

Abstract: The ability to estimate the quality of scientific papers is central to how
both humans and AI systems will advance scientific knowledge in the future.
However, existing LLM-based estimation methods suffer from high inference cost,
whereas the faster direct score regression approach is limited by scale
inconsistencies. We present NAIPv2, a debiased and efficient framework for
paper quality estimation. NAIPv2 employs pairwise learning within domain-year
groups to reduce inconsistencies in reviewer ratings and introduces the Review
Tendency Signal (RTS) as a probabilistic integration of reviewer scores and
confidences. To support training and evaluation, we further construct NAIDv2, a
large-scale dataset of 24,276 ICLR submissions enriched with metadata and
detailed structured content. Trained on pairwise comparisons but enabling
efficient pointwise prediction at deployment, NAIPv2 achieves state-of-the-art
performance (78.2% AUC, 0.432 Spearman), while maintaining scalable,
linear-time efficiency at inference. Notably, on unseen NeurIPS submissions, it
further demonstrates strong generalization, with predicted scores increasing
consistently across decision categories from Rejected to Oral. These findings
establish NAIPv2 as a debiased and scalable framework for automated paper
quality estimation, marking a step toward future scientific intelligence
systems. Code and dataset are released at
https://sway.cloud.microsoft/Pr42npP80MfPhvj8.

</details>


### [193] [Incentive-Aligned Multi-Source LLM Summaries](https://arxiv.org/abs/2509.25184)
*Yanchen Jiang,Zhe Feng,Aranyak Mehta*

Main category: cs.CL

TL;DR: 本文提出了Truthful Text Summarization (TTS)框架，提高了大语言模型生成摘要的真实性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在集成多源文本时易受虚假信息影响且缺乏激励机制保证来源的真实性。

Method: TTS将初稿分解为原子陈述，收集各来源对每个陈述的立场，通过改进的多任务同行预测机制评分并过滤不可靠来源，最后重新生成摘要。

Result: 实验表明TTS能提升事实准确性和模型鲁棒性，同时保持文本流畅性，并有效减少信息操纵行为。

Conclusion: TTS框架通过激励机制确保信息来源诚实报告，从而提升了生成文本的真实性和可靠性。

Abstract: Large language models (LLMs) are increasingly used in modern search and
answer systems to synthesize multiple, sometimes conflicting, texts into a
single response, yet current pipelines offer weak incentives for sources to be
accurate and are vulnerable to adversarial content. We introduce Truthful Text
Summarization (TTS), an incentive-aligned framework that improves factual
robustness without ground-truth labels. TTS (i) decomposes a draft synthesis
into atomic claims, (ii) elicits each source's stance on every claim, (iii)
scores sources with an adapted multi-task peer-prediction mechanism that
rewards informative agreement, and (iv) filters unreliable sources before
re-summarizing. We establish formal guarantees that align a source's incentives
with informative honesty, making truthful reporting the utility-maximizing
strategy. Experiments show that TTS improves factual accuracy and robustness
while preserving fluency, aligning exposure with informative corroboration and
disincentivizing manipulation.

</details>


### [194] [Learning to Parallel: Accelerating Diffusion Large Language Models via Adaptive Parallel Decoding](https://arxiv.org/abs/2509.25188)
*Wenrui Bao,Zhiben Chen,Dan Xu,Yuzhang Shang*

Main category: cs.CL

TL;DR: 本文提出了一种基于学习的并行解码框架Learn2PD，通过训练轻量级自适应过滤模型，实现大语言模型的高效平行解码，显著提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 目前的并行解码策略依赖固定且不针对输入的启发式方法，导致各种自然语言处理任务中速度与质量无法达到最优平衡。

Method: 提出Learn2PD框架，训练一个轻量级过滤模型预测每个位置上的标记是否已正确预测，从而动态确定解码过程中的掩码状态，并引入EoTP用于检测序列结束，避免不必要的解码。

Result: 在LLaDA基准测试中，该方法实现了最多22.58倍加速且无性能下降，结合KV-Cache后加速提升至57.51倍。

Conclusion: Learn2PD通过动态学习的过滤机制，显著提高了基于扩散模型的大语言模型的并行解码效率，具备实用价值。

Abstract: Autoregressive decoding in large language models (LLMs) requires
$\mathcal{O}(n)$ sequential steps for $n$ tokens, fundamentally limiting
inference throughput. Recent diffusion-based LLMs (dLLMs) enable parallel token
generation through iterative denoising. However, current parallel decoding
strategies rely on fixed, input-agnostic heuristics (e.g., confidence
thresholds), which fail to adapt to input-specific characteristics, resulting
in suboptimal speed-quality trade-offs across diverse NLP tasks. In this work,
we explore a more flexible and dynamic approach to parallel decoding. We
propose Learning to Parallel Decode (Learn2PD), a framework that trains a
lightweight and adaptive filter model to predict, for each token position,
whether the current prediction matches the final output. This learned filter
approximates an oracle parallel decoding strategy that unmasks tokens only when
correctly predicted. Importantly, the filter model is learned in a
post-training manner, requiring only a small amount of computation to optimize
it (minute-level GPU time). Additionally, we introduce End-of-Text Prediction
(EoTP) to detect decoding completion at the end of sequence, avoiding redundant
decoding of padding tokens. Experiments on the LLaDA benchmark demonstrate that
our method achieves up to 22.58$\times$ speedup without any performance drop,
and up to 57.51$\times$ when combined with KV-Cache.

</details>


### [195] [InfoAgent: Advancing Autonomous Information-Seeking Agents](https://arxiv.org/abs/2509.25189)
*Gongrui Zhang,Jialiang Zhu,Ruiqi Yang,Kai Qiu,Miaosen Zhang,Zhirong Wu,Qi Dai,Bei Liu,Chong Luo,Zhengyuan Yang,Linjie Li,Lijuan Wang,Weizhu Chen,Yuan Zhang,Xin Li,Zhaoyi Liu,Xin Geng,Baining Guo*

Main category: cs.CL

TL;DR: 本文提出了InfoAgent，一个利用创新数据合成和自主搜索工具的大型语言模型研究代理，显著提升了复杂查询的解答能力。


<details>
  <summary>Details</summary>
Motivation: 构建能够通过与外部工具交互扩展能力的大型语言模型代理，提升AI在复杂检索和推理任务中的表现。

Method: 通过构建实体树及子树采样与实体模糊化生成高难度查询，搭建自主搜索基础设施，并采用两阶段训练（冷启动监督微调+强化学习）提升模型的长时搜索及推理能力。

Result: InfoAgent在多个基准测试中表现优异，BrowseComp准确率15.3%，BrowseComp-ZH为29.2%，Xbench-DS为40.4%，均优于现有开源深度研究代理。

Conclusion: 自研数据合成管线与工具支持显著提升了大型语言模型代理处理复杂查询的能力，展现了推动代理能力进一步发展的潜力。

Abstract: Building Large Language Model agents that expand their capabilities by
interacting with external tools represents a new frontier in AI research and
applications. In this paper, we introduce InfoAgent, a deep research agent
powered by an innovative data synthesis pipeline and orchestrated web search
tools. To construct challenging, hard-to-find queries,we build entity trees and
apply sub-tree sampling with entity fuzzification to systematically increase
question difficulty. Unlike prior work that relies heavily on commercial search
tools, we develop a dedicated self-hosted search infrastructure, enhancing
transparency of agent environments and facilitating further advancement of
agent capacity. We evaluate the effectiveness of our data pipeline by measuring
the average number of tool calls required to correctly answer a question, and
also show that our agent yields better performance when equipped with our
tools. Our \mbox{InfoAgent} is post-trained from Qwen3-14B using a two-stage
recipe: cold-start supervised finetuning to instill long-horizon search
behaviors, followed by reinforcement learning which significantly improves
reasoning-driven tool use. With our methods, InfoAgent achieves 15.3\% accuracy
on BrowseComp, 29.2\% on BrowseComp-ZH, and 40.4\% on Xbench-DS, outperforming
prior open-source deep research agents such as WebSailor-72B and DeepDive-32B.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [196] [A benchmark for vericoding: formally verified program synthesis](https://arxiv.org/abs/2509.22908)
*Sergiu Bursuc,Theodore Ehrenborg,Shaowei Lin,Lacramioara Astefanoaei,Ionel Emilian Chiosa,Jure Kukovec,Alok Singh,Oliver Butterley,Adem Bizid,Quinn Dougherty,Miranda Zhao,Max Tan,Max Tegmark*

Main category: cs.SE

TL;DR: 本文提出并测试了最大规模的vericoding基准，使用LLM从形式化规范生成经过验证的代码，包含三种格式共12504个规范，新问题6174个，验证成功率在27%-82%之间，添加自然语言描述效果不显著，并展示了近年LLM在Dafny验证上的进步。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成多依赖自然语言描述，可能产生有缺陷代码，亟需形成针对形式化规范的经过验证的代码生成基准，以推动生成代码的正确性和可靠性提升。

Method: 构建包含三种语言（Dafny、Verus/Rust、Lean）共12504个形式化规范的基准，利用现成的大型语言模型进行代码生成与形式验证，比较不同语言和条件下的成功率，并分析自然语言描述的影响及LLM进步对验证率的提升。

Result: 在12,504个规范中，使用off-the-shelf LLMs取得了Lean 27%、Verus/Rust 44%、Dafny 82%的验证成功率，添加自然语言描述未显著提高性能，且Dafny验证率从去年68%提升至96%。

Conclusion: vericoding基准有效评估基于形式化规范的代码生成能力，展示了LLM在形式验证领域的显著进步，提示自然语言描述对这种场景帮助有限，未来工作可进一步提升不同语言间的应用。

Abstract: We present and test the largest benchmark for vericoding, LLM-generation of
formally verified code from formal specifications - in contrast to vibe coding,
which generates potentially buggy code from a natural language description. Our
benchmark contains 12,504 formal specifications, with 3,029 in Dafny, 2,334 in
Verus/Rust and 7,141 in Lean. Of these, 6,174 are new unseen problems. We find
vericoding success rates of 27% in Lean, 44% in Verus/Rust and 82% in Dafny
using off-the-shelf LLMs. Adding natural-language descriptions does not
significantly improve performance. We also find that LLM progress has improved
progress on pure Dafny verification from 68% to 96% over the past year. The
benchmark and vericoding results are shared at
https://github.com/Beneficial-AI-Foundation/vericoding-benchmark

</details>


### [197] [Towards Human-interpretable Explanation in Code Clone Detection using LLM-based Post Hoc Explainer](https://arxiv.org/abs/2509.22978)
*Teeradaj Racharak,Chaiyong Ragkhitwetsagul,Chayanee Junplong,Akara Supratak*

Main category: cs.SE

TL;DR: 本文提出利用大型语言模型（如ChatGPT-4）的上下文学习能力，作为机器学习代码克隆检测器（如GraphCodeBERT）预测结果的后置解释器，提升解释准确率达98%。


<details>
  <summary>Details</summary>
Motivation: 传统基于机器学习的代码克隆检测器虽准确，但作为黑盒模型难以解释其决策过程；现有后置解释方法要么需求白盒访问，要么计算成本高，亟需更高效的解释方法。

Method: 利用大型语言模型ChatGPT-4的上下文学习能力，对GraphCodeBERT识别的代码克隆结果进行解释，评估解释的准确性与实用性，并调节温度参数提高解释质量。

Result: 该方法在98%的情况下提供了正确解释，95%时间内生成了良好解释；降低温度参数能提升解释准确率，部分解释和代码实例亦具有实际参考价值。

Conclusion: 利用大型语言模型作为后置解释器是一种有前景的方法，能增强代码克隆检测结果的可解释性，促进软件工程相关任务的进一步研究与应用。

Abstract: Recent studies highlight various machine learning (ML)-based techniques for
code clone detection, which can be integrated into developer tools such as
static code analysis. With the advancements brought by ML in code
understanding, ML-based code clone detectors could accurately identify and
classify cloned pairs, especially semantic clones, but often operate as black
boxes, providing little insight into the decision-making process. Post hoc
explainers, on the other hand, aim to interpret and explain the predictions of
these ML models after they are made, offering a way to understand the
underlying mechanisms driving the model's decisions. However, current post hoc
techniques require white-box access to the ML model or are computationally
expensive, indicating a need for advanced post hoc explainers. In this paper,
we propose a novel approach that leverages the in-context learning capabilities
of large language models to elucidate the predictions made by the ML-based code
clone detectors. We perform a study using ChatGPT-4 to explain the code clone
results inferred by GraphCodeBERT. We found that our approach is promising as a
post hoc explainer by giving the correct explanations up to 98% and offering
good explanations 95% of the time. However, the explanations and the code line
examples given by the LLM are useful in some cases. We also found that lowering
the temperature to zero helps increase the accuracy of the explanation. Lastly,
we list the insights that can lead to further improvements in future work. This
study paves the way for future studies in using LLMs as a post hoc explainer
for various software engineering tasks.

</details>


### [198] [The Matthew Effect of AI Programming Assistants: A Hidden Bias in Software Evolution](https://arxiv.org/abs/2509.23261)
*Fei Gu,Zi Liang,Hongzong LI,Jiahao MA*

Main category: cs.SE

TL;DR: 该论文研究了大语言模型（LLM）驱动的AI辅助编程对软件开发的影响，发现更受欢迎的编程语言或框架的代码生成成功率更高，即存在马太效应。


<details>
  <summary>Details</summary>
Motivation: 尽管先前研究关注提示设计和代码生成质量，但AI辅助编程如何影响软件开发迭代动态及生态系统尚未充分探讨。

Method: 通过大规模实验，针对千余个算法编程任务和数百个框架选择任务，系统性分析AI辅助编程与软件生态的互动。

Result: 发现了显著的马太效应：越流行的语言或框架，LLM生成代码的成功率越高，表明AI可能强化现有的流行等级，促进主流工具集中，同时阻碍多样性和创新。

Conclusion: AI辅助编程加速编程生态系统向主流工具集中演化，对多样性和创新带来挑战，需警惕这种影响以指导未来生态发展。

Abstract: AI-assisted programming is rapidly reshaping software development, with large
language models (LLMs) enabling new paradigms such as vibe coding and agentic
coding. While prior works have focused on prompt design and code generation
quality, the broader impact of LLM-driven development on the iterative dynamics
of software engineering remains underexplored. In this paper, we conduct
large-scale experiments on thousands of algorithmic programming tasks and
hundreds of framework selection tasks to systematically investigate how
AI-assisted programming interacts with the software ecosystem. Our analysis
reveals \textbf{a striking Matthew effect: the more popular a programming
language or framework, the higher the success rate of LLM-generated code}. The
phenomenon suggests that AI systems may reinforce existing popularity
hierarchies, accelerating convergence around dominant tools while hindering
diversity and innovation. We provide a quantitative characterization of this
effect and discuss its implications for the future evolution of programming
ecosystems.

</details>


### [199] [Code Arcades: 3d Visualization of Classes, Dependencies and Software Metrics](https://arxiv.org/abs/2509.23297)
*Anthony Savidis,Christos Vasilopoulos*

Main category: cs.SE

TL;DR: 本文提出了一种新的软件可视化方法，通过灵活的代码元素分组、多层次软件指标结合及动态交互式可视化引擎，提高了源码理解的适应性和洞察力。


<details>
  <summary>Details</summary>
Motivation: 软件可视化旨在图形化展示软件构件，增强源码的理解、分析、维护和演进，但传统方法在组织灵活性和多层次视角上存在不足。

Method: 引入可配置的分组机制支持基于任意关系的代码元素组织，结合细粒度与粗粒度指标，采用交互式可视化引擎实现动态调整渲染属性。

Result: 实现了一种灵活且多层次的软件可视化方法，提升了对大规模系统结构模式和复杂度热点的识别能力。

Conclusion: 该方法通过灵活分组、多级指标和交互调整，提供了更适应且富有洞察力的源码理解工具，有助于开发者有效分析和维护软件系统。

Abstract: Software visualization seeks to represent software artifacts graphical-ly in
two or three dimensions, with the goal of enhancing comprehension, anal-ysis,
maintenance, and evolution of the source code. In this context, visualiza-tions
employ graphical forms such as dependency structures, treemaps, or time-lines
that incorporate repository histories. These visualizations allow software
engineers to identify structural patterns, detect complexity hotspots, and
infer system behaviors that are difficult to perceive directly from source
text. By adopting metaphor-based approaches, visualization tools provide
macroscopic overviews while enabling focused inspection of specific program
elements, thus offering an accessible means of understanding large-scale
systems. The contri-bution of our work lies in three areas. First, we introduce
a configurable group-ing mechanism that supports flexible organization of code
elements based on arbitrary relationships. Second, we combine fine-grained and
coarse-grained software metrics to provide a multi-level perspective on system
properties. Third, we present an interactive visualization engine that allows
developers to dynamically adjust rendering attributes. Collectively, these
advances provide a more adaptable and insightful approach to source code
comprehension.

</details>


### [200] [Methods for evaluating software accessibility](https://arxiv.org/abs/2509.23469)
*Mykola Kuz,Ivan Yaremiy,Hanna Yaremii,Mykola Pikuliak,Ihor Lazarovych,Mykola Kozlenko,Denys Vekeryk*

Main category: cs.SE

TL;DR: 该论文提出了一种针对软件可访问性更为详细和实用的评估方法，并应用于具体网站进行分析和改进建议。


<details>
  <summary>Details</summary>
Motivation: 现有的软件可访问性评估方法过于通用，难以满足不同用户群体的特殊需求，亟需开发更细化的评估指标和方法。

Method: 构建了一个分类和数学模型，发展了基于该模型的软件可访问性评估方法，专注于“可用性”质量特性的子特性“可访问性”的定量评估。

Result: 该方法成功应用于分析某高校主页对视觉障碍用户的包容性，提出了具体的改进建议。

Conclusion: 该研究提出的方法相比标准方法更详细且实用，有助于提升数字环境的包容性，推动软件可访问性的改进。

Abstract: The development and enhancement of methods for evaluating software
accessibility is a relevant challenge in modern software engineering, as
ensuring equal access to digital services is a key factor in improving their
efficiency and inclusivity. The increasing digitalization of society
necessitates the creation of software that complies with international
accessibility standards such as ISO/IEC 25023 and WCAG. Adhering to these
standards helps eliminate barriers to software use for individuals with diverse
physical, sensory, and cognitive needs. Despite advancements in regulatory
frameworks, existing accessibility evaluation methodologies are often
generalized and fail to account for the specific needs of different user
categories or the unique ways they interact with digital systems. This
highlights the need for the development of new, more detailed methods for
defining metrics that influence the quality of user interaction with software
products. Building a classification and mathematical model and developing
accessibility assessment methods for software based on it. A method for
assessing the quality subcharacteristic "Accessibility", which is part of the
"Usability" quality characteristic, has been developed. This enabled the
analysis of a website's inclusivity for individuals with visual impairments,
and the formulation of specific recommendations for further improvements, which
is a crucial step toward creating an inclusive digital environment. Comparing
to standardized approaches, a more detailed and practically oriented
accessibility assessment methodology has been proposed. Using this methodology,
an analysis of the accessibility of the main pages of Vasyl Stefanyk
Precarpathian National University's website was conducted, and improvements
were suggested to enhance its inclusivity.

</details>


### [201] [Improving the Efficiency of LLM Agent Systems through Trajectory Reduction](https://arxiv.org/abs/2509.23586)
*Yuan-An Xiao,Pengfei Gao,Chao Peng,Yingfei Xiong*

Main category: cs.SE

TL;DR: 本文提出了一种名为AgentDiet的推理时轨迹缩减方法，能显著减少多轮大语言模型代理系统的计算成本，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 多轮基于大语言模型的代理系统在软件工程任务中广受欢迎，但随着对话轨迹长度增加，计算成本急剧上升，效率问题被忽视。

Method: 通过分析代理轨迹中的无用、冗余和过期信息，设计AgentDiet自动识别并移除这些无效数据，以减少输入令牌数量。

Result: 在两个大语言模型和两个基准测试中，AgentDiet减少了39.9%至59.7%的输入令牌，降低了21.1%至35.9%的计算成本，同时保持了代理性能不变。

Conclusion: 轨迹缩减方法有效提升了多轮代理系统的计算效率，具备广阔应用前景，是提升LLM代理系统性能的重要方向。

Abstract: Multi-turn agent systems based on Large Language Models (LLMs) have been
increasingly popular for software engineering tasks. While LLM agents show
decent effectiveness, the high computational cost of input tokens due to the
ever-growing trajectory remains an efficiency concern for their applications.
Efficiency is largely neglected in existing studies and agent products, and
this paper fills the gap by introducing an inference-time trajectory reduction
approach to reduce the cost of agents.
  Through analyzing existing agent trajectories, we demonstrate that useless,
redundant, and expired information is widespread in all trajectories, which can
be identified and reduced without harming the agent's performance. We then
design a simple yet effective trajectory reduction approach, AgentDiet, which
automatically removes such waste information. We implement AgentDiet on a
top-performing coding agent, and the evaluation on two LLMs and two benchmarks
shows that AgentDiet can reduce input tokens by 39.9% ~ 59.7%, or the final
computational cost by 21.1% ~ 35.9%, while maintaining the same agent
performance. This indicates that trajectory reduction is a promising direction
for agent systems.

</details>


### [202] [Similarity-Based Assessment of Computational Reproducibility in Jupyter Notebooks](https://arxiv.org/abs/2509.23645)
*A S M Shahadat Hossain,Colin Brown,David Koop,Tanu Malik*

Main category: cs.SE

TL;DR: 本文提出了一种基于相似度的可重现性指标(SRI)，用于评估Jupyter Notebook结果的计算重现性。


<details>
  <summary>Details</summary>
Motivation: Jupyter Notebook中的计算实验结果由于随机性、库版本变化及计算环境差异，导致重复运行结果不一致，需一种有效度量方法评估其重现性。

Method: 提出了基于Python对象特定相似度度量的SRI指标，通过比较重运行输出与原始输出，为每个输出单元格给出0到1之间的定量分数及质量评估。

Result: 通过案例研究，将SRI应用于多份Jupyter Notebook，展示了多种相似度度量方法在量化计算重现性中的有效性。

Conclusion: SRI为量化Jupyter Notebook计算重现性提供了新颖且实用的工具，有助于理解和提升实验结果的一致性。

Abstract: Computational reproducibility refers to obtaining consistent results when
rerunning an experiment. Jupyter Notebook, a web-based computational notebook
application, facilitates running, publishing, and sharing computational
experiments along with their results. However, rerunning a Jupyter Notebook may
not always generate identical results due to various factors, such as
randomness, changes in library versions, or variations in the computational
environment. This paper introduces the Similarity-based Reproducibility Index
(SRI) -- a metric for assessing the reproducibility of results in Jupyter
Notebooks. SRI employs novel methods developed based on similarity metrics
specific to different types of Python objects to compare rerun outputs against
original outputs. For every cell generating an output in a rerun notebook, SRI
reports a quantitative score in the range [0, 1] as well as some qualitative
insights to assess reproducibility. The paper also includes a case study in
which the proposed metric is applied to a set of Jupyter Notebooks,
demonstrating how various similarity metrics can be leveraged to quantify
computational reproducibility.

</details>


### [203] [PAT-Agent: Autoformalization for Model Checking](https://arxiv.org/abs/2509.23675)
*Xinyue Zuo,Yifan Zhang,Hongshu Wang,Yufan Cai,Zhe Hou,Jing Sun,Jin Song Dong*

Main category: cs.SE

TL;DR: 本文提出了PAT-Agent框架，结合大型语言模型与形式验证，实现自然语言自动形式化及模型修复，提升形式模型自动构建与验证效率。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在形式验证应用中面临规范语言复杂、生成内容虚假和语义鸿沟等挑战，需一种集成自动化形式建模与修复的解决方案。

Method: PAT-Agent利用规划型大型语言模型提取关键信息并制定详细计划，再由代码生成型模型合成符合语法和语义的形式模型，结合PAT模型检查器和基于反例的修复循环进行验证与修正，同时提供友好的网页界面支持用户交互。

Result: 在40个系统上的实验表明，PAT-Agent在验证成功率和效率上均优于基线方法，消融实验和用户研究验证了规划与修复模块的重要性及界面易用性。

Conclusion: PAT-Agent成功结合了生成式大模型与严格形式验证技术，显著提升了自动形式建模的准确性和可用性，尤其适合非形式方法专家使用。

Abstract: Recent advances in large language models (LLMs) offer promising potential for
automating formal methods. However, applying them to formal verification
remains challenging due to the complexity of specification languages, the risk
of hallucinated output, and the semantic gap between natural language and
formal logic. We introduce PAT-Agent, an end-to-end framework for natural
language autoformalization and formal model repair that combines the generative
capabilities of LLMs with the rigor of formal verification to automate the
construction of verifiable formal models. In PAT-Agent, a Planning LLM first
extracts key modeling elements and generates a detailed plan using semantic
prompts, which then guides a Code Generation LLM to synthesize syntactically
correct and semantically faithful formal models. The resulting code is verified
using the Process Analysis Toolkit (PAT) model checker against user-specified
properties, and when discrepancies occur, a Repair Loop is triggered to
iteratively correct the model using counterexamples. To improve flexibility, we
built a web-based interface that enables users, particularly non-FM-experts, to
describe, customize, and verify system behaviors through user-LLM interactions.
Experimental results on 40 systems show that PAT-Agent consistently outperforms
baselines, achieving high verification success with superior efficiency. The
ablation studies confirm the importance of both planning and repair components,
and the user study demonstrates that our interface is accessible and supports
effective formal modeling, even for users with limited formal methods
experience.

</details>


### [204] [Satellite: Detecting and Analyzing Smart Contract Vulnerabilities caused by Subcontract Misuse](https://arxiv.org/abs/2509.23679)
*Zeqin Liao,Yuhong Nan,Zixu Gao,Henglong Liang,Sicheng Hao,Jiajing Wu,Zibin Zheng*

Main category: cs.SE

TL;DR: 本文提出Satellite框架，用于字节码层面检测智能合约中的子合约误用漏洞，准确率和召回率分别为84.68%和92.11%。


<details>
  <summary>Details</summary>
Motivation: 智能合约开发中广泛复用子合约提高效率，但复用可能引入漏洞，传统检测方法难以应对字节码层面信息缺失的挑战。

Method: 设计Satellite框架，采用迁移学习恢复继承方法，提取细粒度方法级特征，进行方法级对比，结合指标分类检测子合约误用漏洞。

Result: 在包含58个真实攻击案例和56个模式的数据集上，Satellite性能优异，且成功识别出10011个真实合约中的14个新未知漏洞，影响数字资产约20万美元。

Conclusion: Satellite有效提升了字节码层面智能合约子合约误用漏洞的自动检测能力，具有较高的实用价值和安全保障意义。

Abstract: Developers of smart contracts pervasively reuse subcontracts to improve
development efficiency. Like any program language, such subcontract reuse may
unexpectedly include, or introduce vulnerabilities to the end-point smart
contract. Unfortunately, automatically detecting such issues poses several
unique challenges. Particularly, in most cases, smart contracts are compiled as
bytecode, whose class-level information (e.g., inheritance, virtual function
table), and even semantics (e.g., control flow and data flow) are fully
obscured as a single smart contract after compilation.
  In this paper, we propose Satellite, a new bytecode-level static analysis
framework for subcontract misuse vulnerability (SMV) detection in smart
contracts. Satellite incorporates a series of novel designs to enhance its
overall effectiveness.. Particularly, Satellite utilizes a transfer learning
method to recover the inherited methods, which are critical for identifying
subcontract reuse in smart contracts. Further, Satellite extracts a set of
fine-grained method-level features and performs a method-level comparison, for
identifying the reuse part of subcontract in smart contracts. Finally,
Satellite summarizes a set of SMV indicators according to their types, and
hence effectively identifies SMVs. To evaluate Satellite, we construct a
dataset consisting of 58 SMVs derived from real-world attacks and collect
additional 56 SMV patterns from SOTA studies. Experiment results indicate that
Satellite exhibits good performance in identifying SMV, with a precision rate
of 84.68% and a recall rate of 92.11%. In addition, Satellite successfully
identifies 14 new/unknown SMV over 10,011 real-world smart contracts, affecting
a total amount of digital assets worth 201,358 USD.

</details>


### [205] [Influence-Guided Concolic Testing of Transformer Robustness](https://arxiv.org/abs/2509.23806)
*Chih-Duo Hong,Yu Wang,Yao-Chen Chang,Fang Yu*

Main category: cs.SE

TL;DR: 本文提出了一种基于影响力引导的符号执行测试方法，用于提高Transformer分类器中的标签翻转输入发现效率。


<details>
  <summary>Details</summary>
Motivation: 现有的深度神经网络符号执行测试难以处理现代Transformer架构的复杂自注意力机制，且缺乏有效的路径谓词优先级排序策略。

Method: 引入基于SHAP的影响力评分对路径谓词进行排序，提出纯Python实现的多头自注意力求解语义，并设计调度启发式方法限制约束增长。

Result: 在小尺寸Transformer白盒测试中，影响力引导的测试方法比基线方法更高效，且在更深层网络依然保持稳定性能；利用SHAP分析发现多个成功攻击案例间存在共享的决策逻辑。

Conclusion: 影响力信号有效指导符号执行搜索，结合实用的注意力求解语义和调度策略，使得基于符号执行的测试可应用于现代Transformer模型，有助于模型调试与审计。

Abstract: Concolic testing for deep neural networks alternates concrete execution with
constraint solving to search for inputs that flip decisions. We present an
{influence-guided} concolic tester for Transformer classifiers that ranks path
predicates by SHAP-based estimates of their impact on the model output. To
enable SMT solving on modern architectures, we prototype a solver-compatible,
pure-Python semantics for multi-head self-attention and introduce practical
scheduling heuristics that temper constraint growth on deeper models. In a
white-box study on compact Transformers under small $L_0$ budgets, influence
guidance finds label-flip inputs more efficiently than a FIFO baseline and
maintains steady progress on deeper networks. Aggregating successful attack
instances with a SHAP-based critical decision path analysis reveals recurring,
compact decision logic shared across attacks. These observations suggest that
(i) influence signals provide a useful search bias for symbolic exploration,
and (ii) solver-friendly attention semantics paired with lightweight scheduling
make concolic testing feasible for contemporary Transformer models, offering
potential utility for debugging and model auditing.

</details>


### [206] [Navigating the Labyrinth: Path-Sensitive Unit Test Generation with Large Language Models](https://arxiv.org/abs/2509.23812)
*Dianshu Liao,Xin Yin,Shidong Pan,Chao Ni,Zhenchang Xing,Xiaoyu Sun*

Main category: cs.SE

TL;DR: 本文提出了一种基于路径敏感的JUnitGenie框架，通过结合代码知识与大型语言模型，实现上下文感知的单元测试生成，显著提升了测试覆盖率并发现真实缺陷。


<details>
  <summary>Details</summary>
Motivation: 传统自动单元测试生成方法受限于路径不敏感的固定启发式规则或有限上下文，难以覆盖复杂执行路径，导致测试覆盖率不足。

Method: JUnitGenie框架通过从Java项目中提取代码知识，构建结构化提示，引导大型语言模型生成高覆盖率的单元测试，充分利用代码语义和控制流信息。

Result: 在2258个复杂方法上，JUnitGenie相比启发式和LLM基线方法，分支覆盖率提升29.60%，代码行覆盖率提升31.00%，且能发现并帮助开发者修复真实缺陷。

Conclusion: 结合代码结构知识与大型语言模型的路径敏感测试生成方法显著提升了测试效率和质量，具备实际应用价值。

Abstract: Unit testing is essential for software quality assurance, yet writing and
maintaining tests remains time-consuming and error-prone. To address this
challenge, researchers have proposed various techniques for automating unit
test generation, including traditional heuristic-based methods and more recent
approaches that leverage large language models (LLMs). However, these existing
approaches are inherently path-insensitive because they rely on fixed
heuristics or limited contextual information and fail to reason about deep
control-flow structures. As a result, they often struggle to achieve adequate
coverage, particularly for deep or complex execution paths. In this work, we
present a path-sensitive framework, JUnitGenie, to fill this gap by combining
code knowledge with the semantic capabilities of LLMs in guiding context-aware
unit test generation. After extracting code knowledge from Java projects,
JUnitGenie distills this knowledge into structured prompts to guide the
generation of high-coverage unit tests. We evaluate JUnitGenie on 2,258 complex
focal methods from ten real-world Java projects. The results show that
JUnitGenie generates valid tests and improves branch and line coverage by
29.60% and 31.00% on average over both heuristic and LLM-based baselines. We
further demonstrate that the generated test cases can uncover real-world bugs,
which were later confirmed and fixed by developers.

</details>


### [207] [SolContractEval: A Benchmark for Evaluating Contract-Level Solidity Code Generation](https://arxiv.org/abs/2509.23824)
*Zhifan Ye,Jiachi Chen,Zhenzhe Shao,Lingfeng Bao,Xiaohu Yang,Zhongxin Liu*

Main category: cs.SE

TL;DR: 本文提出SolContractEval，一个用于Solidity智能合约生成的真实合同级别基准测试框架，并基于历史交易回放开发了动态评估方法，系统评估了六个主流大语言模型的表现。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在Solidity代码生成的效果和可靠性尚未充分评估，且以往评测多聚焦于孤立函数和合成输入，无法反映真实合约开发中的挑战。

Method: 构建包含124个真实链上合约任务的SolContractEval基准，任务涵盖九大领域，输入包含完整依赖和结构化框架，同时采用历史交易回放动态评估方法实现功能正确性验证。对六个主流大语言模型进行系统测试。

Result: Claude-3.7-Sonnet表现最佳，但整体生成效果仍逊于通用语言的类级生成任务。模型在遵循标准模式任务上表现较好，在复杂逻辑和合约间依赖上表现较差，对Solidity特性和上下文的理解有限。

Conclusion: 现有大语言模型在Solidity智能合约生成上存在明显不足，需针对Solidity语言特性和合约复杂性改进模型能力，同时SolContractEval为智能合约代码生成提供了更真实有效的评测标准。

Abstract: The rise of blockchain has brought smart contracts into mainstream use,
creating a demand for smart contract generation tools. While large language
models (LLMs) excel at generating code in general-purpose languages, their
effectiveness on Solidity, the primary language for smart contracts, remains
underexplored. Solidity constitutes only a small portion of typical LLM
training data and differs from general-purpose languages in its
version-sensitive syntax and limited flexibility. These factors raise concerns
about the reliability of existing LLMs for Solidity code generation.
Critically, existing evaluations, focused on isolated functions and synthetic
inputs, fall short of assessing models' capabilities in real-world contract
development.
  To bridge this gap, we introduce SolContractEval, the first contract-level
benchmark for Solidity code generation. It comprises 124 tasks drawn from real
on-chain contracts across nine major domains. Each task input, consisting of
complete context dependencies, a structured contract framework, and a concise
task prompt, is independently annotated and cross-validated by experienced
developers. To enable precise and automated evaluation of functional
correctness, we also develop a dynamic evaluation framework based on historical
transaction replay. Building on SolContractEval, we perform a systematic
evaluation of six mainstream LLMs. We find that Claude-3.7-Sonnet achieves the
highest overall performance, though evaluated models underperform relative to
their capabilities on class-level generation tasks in general-purpose
programming languages. Second, current models perform better on tasks that
follow standard patterns but struggle with complex logic and inter-contract
dependencies. Finally, they exhibit limited understanding of Solidity-specific
features and contextual dependencies.

</details>


### [208] [HFuzzer: Testing Large Language Models for Package Hallucinations via Phrase-based Fuzzing](https://arxiv.org/abs/2509.23835)
*Yukai Zhao,Menghan Wu,Xing Hu,Xin Xia*

Main category: cs.SE

TL;DR: 本文针对大语言模型（LLMs）在代码生成中出现的虚假包推荐问题，提出了一种基于短语模糊测试的新框架HFUZZER，有效发现更多且多样的虚假包，提升安全检测能力。


<details>
  <summary>Details</summary>
Motivation: LLMs在实际代码生成中存在包虚假推荐风险，可能被恶意利用导致供应链攻击，而现有针对自然语言生成的测试框架无法有效检测包虚假，缺少专门的测试工具。

Method: 提出HFUZZER框架，利用基于短语的模糊测试引导模型推断更多合理信息，结合从包信息或代码任务中抽取短语以保证测试代码与短语高度相关，从而生成丰富且多样的代码测试任务。

Result: 在多款LLM模型中测试，HFUZZER触发了所有模型的包虚假表现，发现的独特虚假包数量是现有变异模糊测试框架的2.6倍，且任务多样性更高，特别在GPT-4o上检测出46个独特虚假包，还发现模型在环境配置环节也存在包虚假问题。

Conclusion: HFUZZER有效检测并揭示了LLMs在代码生成及环境配置中的包虚假现象，为提升LLM代码生成的安全性提供了实用工具和方法。

Abstract: Large Language Models (LLMs) are widely used for code generation, but they
face critical security risks when applied to practical production due to
package hallucinations, in which LLMs recommend non-existent packages. These
hallucinations can be exploited in software supply chain attacks, where
malicious attackers exploit them to register harmful packages. It is critical
to test LLMs for package hallucinations to mitigate package hallucinations and
defend against potential attacks. Although researchers have proposed testing
frameworks for fact-conflicting hallucinations in natural language generation,
there is a lack of research on package hallucinations. To fill this gap, we
propose HFUZZER, a novel phrase-based fuzzing framework to test LLMs for
package hallucinations. HFUZZER adopts fuzzing technology and guides the model
to infer a wider range of reasonable information based on phrases, thereby
generating enough and diverse coding tasks. Furthermore, HFUZZER extracts
phrases from package information or coding tasks to ensure the relevance of
phrases and code, thereby improving the relevance of generated tasks and code.
We evaluate HFUZZER on multiple LLMs and find that it triggers package
hallucinations across all selected models. Compared to the mutational fuzzing
framework, HFUZZER identifies 2.60x more unique hallucinated packages and
generates more diverse tasks. Additionally, when testing the model GPT-4o,
HFUZZER finds 46 unique hallucinated packages. Further analysis reveals that
for GPT-4o, LLMs exhibit package hallucinations not only during code generation
but also when assisting with environment configuration.

</details>


### [209] [Learning-Based Testing for Deep Learning: Enhancing Model Robustness with Adversarial Input Prioritization](https://arxiv.org/abs/2509.23961)
*Sheikh Md Mushfiqur Rahman,Nasir Eisty*

Main category: cs.SE

TL;DR: 本文提出了一种结合学习测试、假设测试和变异测试的对抗样本优先级排序方法，显著提升了DNN中故障检测效率和模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于覆盖率或置信度的优先级排序方法难以高效识别最具故障揭示性的对抗样本，限制了其实用效果。

Method: 该方法在不依赖特定模型结构或形式验证的前提下，选择高概率暴露模型故障的对抗输入子集，兼容多种DNN和攻击技术。

Result: 实验显示该方法在各种数据集、模型架构和攻击技术下均优于基线方法，加速了故障检测过程，全面挖掘潜在故障。

Conclusion: 方法不仅提高了故障发现效率，还维护了输入多样性，指导模型重训练，从而增强了模型鲁棒性，具有现实应用价值。

Abstract: Context: Deep Neural Networks (DNNs) are increasingly deployed in critical
applications, where resilience against adversarial inputs is paramount.
However, whether coverage-based or confidence-based, existing test
prioritization methods often fail to efficiently identify the most
fault-revealing inputs, limiting their practical effectiveness. Aims: This
project aims to enhance fault detection and model robustness in DNNs by
integrating Learning-Based Testing (LBT) with hypothesis and mutation testing
to efficiently prioritize adversarial test cases. Methods: Our method selects a
subset of adversarial inputs with a high likelihood of exposing model faults,
without relying on architecture-specific characteristics or formal
verification, making it adaptable across diverse DNNs. Results: Our results
demonstrate that the proposed LBT method consistently surpasses baseline
approaches in prioritizing fault-revealing inputs and accelerating fault
detection. By efficiently organizing test permutations, it uncovers all
potential faults significantly faster across various datasets, model
architectures, and adversarial attack techniques. Conclusion: Beyond improving
fault detection, our method preserves input diversity and provides effective
guidance for model retraining, further enhancing robustness. These advantages
establish our approach as a powerful and practical solution for adversarial
test prioritization in real-world DNN applications.

</details>


### [210] [SandCell: Sandboxing Rust Beyond Unsafe Code](https://arxiv.org/abs/2509.24032)
*Jialun Zhang,Merve Gülmez,Thomas Nyman,Gang Tan*

Main category: cs.SE

TL;DR: 本文提出了SandCell，一种基于Rust语言的灵活轻量级隔离机制，通过现有语法边界实现对安全和不安全代码的细粒度沙箱控制。


<details>
  <summary>Details</summary>
Motivation: Rust通过所有权和借用规则保证内存安全，但unsafe关键字带来风险，已有隔离方法不支持灵活策略沙箱机制。

Method: SandCell利用Rust语法边界提供灵活沙箱指定机制，减少注释工作，同时采用新技术降低沙箱间数据传输开销。

Result: 实验证明SandCell能有效防止Rust应用中的安全漏洞，且性能开销合理。

Conclusion: SandCell实现了Rust代码中安全与不安全代码的可控隔离，提升了程序安全性和灵活性。

Abstract: Rust is a modern systems programming language that ensures memory safety by
enforcing ownership and borrowing rules at compile time. While the unsafe
keyword allows programmers to bypass these restrictions, it introduces
significant risks. Various approaches for isolating unsafe code to protect safe
Rust from vulnerabilities have been proposed, yet these methods provide only
fixed isolation boundaries and do not accommodate expressive policies that
require sandboxing both safe and unsafe code. This paper presents SandCell for
flexible and lightweight isolation in Rust by leveraging existing syntactic
boundaries. SandCell allows programmers to specify which components to sandbox
with minimal annotation effort, enabling fine-grained control over isolation.
The system also introduces novel techniques to minimize overhead when
transferring data between sandboxes. Our evaluation demonstrates SandCell's
effectiveness in preventing vulnerabilities across various Rust applications
while maintaining reasonable performance overheads.

</details>


### [211] [PerfBench: Can Agents Resolve Real-World Performance Bugs?](https://arxiv.org/abs/2509.24091)
*Spandan Garg,Roshanak Zilouchian Moghaddam*

Main category: cs.SE

TL;DR: PerfBench是一个包含81个真实性能缺陷修复任务的基准测试，用于评估软件工程智能体在性能缺陷修复上的能力，现有智能体性能较低，新提出的OpenHands-Perf-Agent表现有提升但仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 现有智能体修复工具主要关注功能性缺陷，缺乏针对性能缺陷的评估和修复能力。

Method: 构建PerfBench基准，包含真实开发者修复的性能缺陷；设计新的评估机制让智能体生成并验证性能修复效果；开发性能感知的OpenHands-Perf-Agent。

Result: 现有顶尖智能体在PerfBench上修复率仅约3%，而OpenHands-Perf-Agent提升至约20%，表现出性能优化工具和指令的重要性。

Conclusion: 通过专门的性能测试指令和工具，智能体在性能缺陷修复上有所提升，但PerfBench表明性能修复仍是智能体领域的重大挑战。

Abstract: Performance bugs are inefficiencies in software that waste computational
resources without causing functional failures, making them particularly
challenging to detect and fix. While recent advances in Software Engineering
agents have shown promise in automated bug fixing, existing benchmarks
primarily focus on functional correctness and fail to evaluate agents'
abilities to identify and resolve non-functional issues like performance bugs.
We introduce PerfBench, a benchmark comprising 81 real-world performance
bug-fixing tasks from popular .NET repositories on GitHub. Unlike existing
benchmarks that rely on pre-existing test suites, PerfBench features a novel
evaluation harness that allows agents to generate their own performance
benchmarks and validates fixes by comparing execution metrics collected for
developer fix and agent fix. Each task in PerfBench is derived from actual
developer fixes linked to performance-related issues, which are then verified
by human experts, ensuring real-world relevance. Our evaluation reveals that
current state-of-the-art coding agents struggle with performance optimization
tasks, with baseline OpenHands agent achieving only a ~3% success rate on our
benchmark. We develop OpenHands-Perf-Agent, which incorporates
performance-aware tooling and instructions and achieves a ~20% success rate on
the benchmark. We show that by ensuring the agent has proper instructions to
benchmark its changes and tooling for benchmark output processing, we can
improve the agent performance significantly, but room for improvement still
remains. PerfBench provides a challenging test set for furthering the
capabilities of agents in fixing performance issues.

</details>


### [212] [TENET: Leveraging Tests Beyond Validation for Code Generation](https://arxiv.org/abs/2509.24148)
*Yiran Hu,Nan Jiang,Shanchao Liang,Yi Wu,Lin Tan*

Main category: cs.SE

TL;DR: 本文提出了TENET，一种结合测试驱动开发（TDD）和大语言模型（LLM）的代码生成代理，解决生成准确性和执行效率等挑战。


<details>
  <summary>Details</summary>
Motivation: 在通过大语言模型进行vibe coding时，TDD尤为重要，但存在测试套件选择、上下文检索和反馈利用三大挑战。

Method: 提出TENET，包含：1) 选择多样性测试套件的新颖测试框架；2) 高效代码检索和交互式调试工具集；3) 基于反思的迭代代码优化流程。

Result: TENET在RepoCod和RepoEval基准测试中分别达成69.08%和81.77%的Pass@1，比当前最佳基线提升9.49和2.17个百分点。

Conclusion: TENET有效提升了复杂代码库中基于测试驱动的代码生成性能，首次系统研究了测试套件多样性对LLM代理的影响。

Abstract: Test-Driven Development (TDD) is a widely adopted software engineering
practice that requires developers to create and execute tests alongside code
implementation, ensuring that software behavior is continuously validated and
refined. In the era of vibe coding, where developers increasingly delegate code
writing to large language models (LLMs) by specifying high-level intentions,
TDD becomes even more crucial, as test cases serve as executable specifications
that explicitly define and verify intended functionality beyond what
natural-language descriptions and code context can convey. While vibe coding
under TDD is promising, there are three main challenges: (1) selecting a small
yet effective test suite to improve the generation accuracy and control the
execution workload, (2) retrieving context such as relevant code effectively,
and (3) systematically using test feedback for effective code refinement. To
address these challenges, we introduce TENET, an LLM agent for generating
functions in complex real-world repositories under the TDD setting. TENET
features three components: (1) a novel test harness mechanism that selects a
concise test suite to maximize diversity of target usage scenarios; (2) a
tailored agent toolset that performs efficient retrieval of relevant code with
interactive debugging; and (3) a reflection-based refinement workflow that
iteratively analyzes failures, replenishes context, and applies code
refinement. TENET achieves 69.08% and 81.77% Pass@1 on RepoCod and RepoEval
benchmarks, outperforming the best agentic baselines by 9.49 and 2.17
percentage points, respectively. In addition, this is the first study of
test-driven code generation with repository-level context, examining how
different aspects of test suites affect the performance of LLM agents under the
TDD setting.

</details>


### [213] [Metamorphic Testing for Audio Content Moderation Software](https://arxiv.org/abs/2509.24215)
*Wenxuan Wang,Yongjiang Wu,Junyuan Zhang,Shuqing Li,Yun Peng,Wenting Chen,Shuai Wang,Michael R. Lyu*

Main category: cs.SE

TL;DR: 本文提出了MTAM框架，通过变异测试方法检测音频内容审核软件，发现其在面对经过微调的有害音频时存在较高漏判率。


<details>
  <summary>Details</summary>
Motivation: 随着音频平台的发展，有害音频内容传播严重，现有审核工具难以有效抵御微小音频篡改导致的绕过问题。

Method: 设计14种基于音频特征和启发式扰动的变异关系，生成仍具毒性的测试用例，用于测试5款商业音频审核软件及学术模型。

Result: MTAM在多款商业审核软件中发现高达16.7%~51.1%的错误率，在学术模型中错误率达45.7%。

Conclusion: MTAM有效揭示了现有音频审核工具对细微音频修改防御能力不足的问题，提示需加强对此类攻击的防护。

Abstract: The rapid growth of audio-centric platforms and applications such as WhatsApp
and Twitter has transformed the way people communicate and share audio content
in modern society. However, these platforms are increasingly misused to
disseminate harmful audio content, such as hate speech, deceptive
advertisements, and explicit material, which can have significant negative
consequences (e.g., detrimental effects on mental health). In response,
researchers and practitioners have been actively developing and deploying audio
content moderation tools to tackle this issue. Despite these efforts, malicious
actors can bypass moderation systems by making subtle alterations to audio
content, such as modifying pitch or inserting noise. Moreover, the
effectiveness of modern audio moderation tools against such adversarial inputs
remains insufficiently studied. To address these challenges, we propose MTAM, a
Metamorphic Testing framework for Audio content Moderation software.
Specifically, we conduct a pilot study on 2000 audio clips and define 14
metamorphic relations across two perturbation categories: Audio Features-Based
and Heuristic perturbations. MTAM applies these metamorphic relations to toxic
audio content to generate test cases that remain harmful while being more
likely to evade detection. In our evaluation, we employ MTAM to test five
commercial textual content moderation software and an academic model against
three kinds of toxic content. The results show that MTAM achieves up to 38.6%,
18.3%, 35.1%, 16.7%, and 51.1% error finding rates (EFR) when testing
commercial moderation software provided by Gladia, Assembly AI, Baidu,
Nextdata, and Tencent, respectively, and it obtains up to 45.7% EFR when
testing the state-of-the-art algorithms from the academy.

</details>


### [214] [Comparing Open-Source and Commercial LLMs for Domain-Specific Analysis and Reporting: Software Engineering Challenges and Design Trade-offs](https://arxiv.org/abs/2509.24344)
*Theo Koraag,Niklas Wagner,Felix Dobslaw,Lucas Gren*

Main category: cs.SE

TL;DR: 本研究探讨了开源和商业大型语言模型在财务报告分析与评论生成中的应用，强调了软件工程实现中的挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在自然语言处理领域表现突出，但其在金融领域的专用应用研究仍较少，特别是在实现层面的软件工程问题。

Method: 采用设计科学研究方法，通过案例研究迭代设计并评估两种基于大型语言模型的系统：一种基于本地开源模型的多代理工作流，另一种使用商业GPT-4o模型，并通过专家评估真实财务报告场景。

Result: 大型语言模型在自动化财务报告任务中展现出较强潜力，但集成过程面临诸多挑战，如提示设计、上下文依赖性和实现权衡。云端模型流畅性和易用性更好，但存在数据隐私和外部依赖问题；开源模型在数据控制和合规性方面表现优越，但需更多工程投入以保证可靠性和易用性。

Conclusion: 大型语言模型在财务报告自动化中潜力显著，成功集成需重视架构、提示设计和系统可靠性，通过量身定制的验证机制和工程策略解决领域特定挑战，实现准确性、控制性和合规性的平衡。

Abstract: Context: Large Language Models (LLMs) enable automation of complex natural
language processing across domains, but research on domain-specific
applications like Finance remains limited. Objectives: This study explored
open-source and commercial LLMs for financial report analysis and commentary
generation, focusing on software engineering challenges in implementation.
Methods: Using Design Science Research methodology, an exploratory case study
iteratively designed and evaluated two LLM-based systems: one with local
open-source models in a multi-agent workflow, another using commercial GPT-4o.
Both were assessed through expert evaluation of real-world financial reporting
use cases. Results: LLMs demonstrated strong potential for automating financial
reporting tasks, but integration presented significant challenges. Iterative
development revealed issues including prompt design, contextual dependency, and
implementation trade-offs. Cloud-based models offered superior fluency and
usability but raised data privacy and external dependency concerns. Local
open-source models provided better data control and compliance but required
substantially more engineering effort for reliability and usability.
Conclusion: LLMs show strong potential for financial reporting automation, but
successful integration requires careful attention to architecture, prompt
design, and system reliability. Implementation success depends on addressing
domain-specific challenges through tailored validation mechanisms and
engineering strategies that balance accuracy, control, and compliance.

</details>


### [215] [Efficient Decomposition Identification of Deterministic Finite Automata from Examples](https://arxiv.org/abs/2509.24347)
*Junjie Meng,Jie An,Yong Li,Andrea Turrini,Fanjiang Xu,Naijun Zhan,Miaomiao Zhang*

Main category: cs.SE

TL;DR: 本文提出了一种基于3值确定性有限自动机（3DFA）替代增强前缀树自动机（APTA）的DFA分解识别方法，有效提高了识别效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统确定性有限自动机（DFA）识别方法往往产生复杂且难以扩展的大型DFA，且现有DFA分解识别方法受限于APTA冗余，导致计算资源消耗大且扩展性差。

Method: 本文研究了传统的帕累托最优DFA分解问题和新提出的状态最优DFA分解问题，提出利用标签样本直接生成的3值DFA（3DFA）替代APTA，减少冗余，改进SAT编码，从而提升计算效率。

Result: 实验结果表明，基于3DFA的方法在帕累托最优DFA分解问题上显著提升了效率，同时使状态最优DFA分解问题实现了良好的可扩展性。

Conclusion: 基于3DFA的DFA分解识别框架有效克服了传统方法的冗余问题，提升了算法效率和可扩展性，是解决复杂自动机学习任务的优选方案。

Abstract: The identification of deterministic finite automata (DFAs) from labeled
examples is a cornerstone of automata learning, yet traditional methods focus
on learning monolithic DFAs, which often yield a large DFA lacking simplicity
and interoperability. Recent work addresses these limitations by exploring DFA
decomposition identification problems (DFA-DIPs), which model system behavior
as intersections of multiple DFAs, offering modularity for complex tasks.
However, existing DFA-DIP approaches depend on SAT encodings derived from
Augmented Prefix Tree Acceptors (APTAs), incurring scalability limitations due
to their inherent redundancy.
  In this work, we advance DFA-DIP research through studying two variants: the
traditional Pareto-optimal DIP and the novel states-optimal DIP, which
prioritizes a minimal number of states. We propose a novel framework that
bridges DFA decomposition with recent advancements in automata representation.
One of our key innovations replaces APTA with 3-valued DFA (3DFA) derived
directly from labeled examples. This compact representation eliminates
redundancies of APTA, thus drastically reducing variables in the improved SAT
encoding. Experimental results demonstrate that our 3DFA-based approach
achieves significant efficiency gains for the Pareto-optimal DIP while enabling
a scalable solution for the states-optimal DIP.

</details>


### [216] [Walk the Talk: Is Your Log-based Software Reliability Maintenance System Really Reliable?](https://arxiv.org/abs/2509.24352)
*Minghua He,Tong Jia,Chiming Duan,Pei Xiao,Lingzhe Zhang,Kangjin Wang,Yifan Wu,Ying Li,Gang Huang*

Main category: cs.SE

TL;DR: 本文针对深度学习日志异常检测方法缺乏可解释性的问题，提出了可信度度量标准及FaithLog系统，以提升模型诊断的可信性和解释性。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的日志异常检测方法对服务提供者而言如黑盒，缺乏对异常检测过程的理解，影响信任及实际部署。

Method: 定义了诊断可信度指标，通过注意力机制引导的因果关系设计与对抗一致性学习，提出FaithLog系统以提高模型的诊断可信度。

Result: 在两个公开数据集和一个工业数据集上的评估显示，FaithLog在诊断可信度方面达到最先进的性能，优于现有方法。

Conclusion: FaithLog有效提升了日志异常检测模型的诊断可信度，增强了模型的可解释性，有助于服务提供者信任并应用该技术。

Abstract: Log-based software reliability maintenance systems are crucial for sustaining
stable customer experience. However, existing deep learning-based methods
represent a black box for service providers, making it impossible for providers
to understand how these methods detect anomalies, thereby hindering trust and
deployment in real production environments. To address this issue, this paper
defines a trustworthiness metric, diagnostic faithfulness, for models to gain
service providers' trust, based on surveys of SREs at a major cloud provider.
We design two evaluation tasks: attention-based root cause localization and
event perturbation. Empirical studies demonstrate that existing methods perform
poorly in diagnostic faithfulness. Consequently, we propose FaithLog, a
faithful log-based anomaly detection system, which achieves faithfulness
through a carefully designed causality-guided attention mechanism and
adversarial consistency learning. Evaluation results on two public datasets and
one industrial dataset demonstrate that the proposed method achieves
state-of-the-art performance in diagnostic faithfulness.

</details>


### [217] [United We Stand: Towards End-to-End Log-based Fault Diagnosis via Interactive Multi-Task Learning](https://arxiv.org/abs/2509.24364)
*Minghua He,Chiming Duan,Pei Xiao,Tong Jia,Siyu Yu,Lingzhe Zhang,Weijie Hong,Jin Han,Yifan Wu,Ying Li,Gang Huang*

Main category: cs.SE

TL;DR: 提出了一种名为Chimera的端到端基于日志的故障诊断方法，通过异常检测和根因定位之间的双向交互和知识转移，实现了更准确的故障诊断。


<details>
  <summary>Details</summary>
Motivation: 现有的故障诊断方法任务独立，不能有效连接异常检测与根因定位，导致诊断偏差累积、依赖高成本监控数据以及忽视任务间的协作关系。

Method: Chimera基于交互式多任务学习，在数据、特征和诊断结果层设计了异常检测与根因定位之间的交互策略，构建统一的端到端框架。

Result: 在两个公开数据集和一个工业数据集上的评测结果显示，Chimera在异常检测和根因定位上的性能分别提升了2.92%-5.00%和19.01%-37.09%。

Conclusion: Chimera方法有效解决了现有方法的问题，提升了诊断准确率，并成功部署于工业云平台，证明其实用价值。

Abstract: Log-based fault diagnosis is essential for maintaining software system
availability. However, existing fault diagnosis methods are built using a
task-independent manner, which fails to bridge the gap between anomaly
detection and root cause localization in terms of data form and diagnostic
objectives, resulting in three major issues: 1) Diagnostic bias accumulates in
the system; 2) System deployment relies on expensive monitoring data; 3) The
collaborative relationship between diagnostic tasks is overlooked. Facing this
problems, we propose a novel end-to-end log-based fault diagnosis method,
Chimera, whose key idea is to achieve end-to-end fault diagnosis through
bidirectional interaction and knowledge transfer between anomaly detection and
root cause localization. Chimera is based on interactive multi-task learning,
carefully designing interaction strategies between anomaly detection and root
cause localization at the data, feature, and diagnostic result levels, thereby
achieving both sub-tasks interactively within a unified end-to-end framework.
Evaluation on two public datasets and one industrial dataset shows that Chimera
outperforms existing methods in both anomaly detection and root cause
localization, achieving improvements of over 2.92% - 5.00% and 19.01% - 37.09%,
respectively. It has been successfully deployed in production, serving an
industrial cloud platform.

</details>


### [218] [Agentic Services Computing](https://arxiv.org/abs/2509.24380)
*Shuiguang Deng,Hailiang Zhao,Ziqi Wang,Guanjie Cheng,Peng Chen,Wenzhuo Qian,Zhiwei Ling,Jianwei Yin,Albert Y. Zomaya,Schahram Dustdar*

Main category: cs.SE

TL;DR: 本文提出了智能服务计算（ASC）范式，强调通过感知、决策、自主协作和信任维护，实现动态、自适应、多智能体生态的服务转型。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型驱动的智能代理兴起，传统静态服务计算向动态目标导向的多智能体生态系统转变，亟需新的服务计算范式。

Method: 建立以生命周期为核心的ASC框架，围绕设计、部署、运营和演化四个阶段，系统分析感知与环境建模、自主决策、协作组织及评价信任等四大研究维度。

Result: 验证了ASC中智能服务的编排特性及其在生命周期各阶段的应用，强调上下文意识、自主推理、协作进化和信任作为贯穿始终的关键因素。

Conclusion: 本文融合传统服务计算与大型语言模型多智能体技术，构建了一个全面且面向未来的智能服务基础框架，为相关领域研究和实践提供了统一参考。

Abstract: The rise of LLM-powered agents is driving a fundamental transformation in
services computing: from static, request-response functions to dynamic,
goal-oriented, and autonomous multi-agent ecosystems. In response to this
shift, we introduce Agentic Service Computing (ASC), a new paradigm that
reimagines services as intelligent, self-adaptive, and socially embedded
entities. This comprehensive survey presents a lifecycle-driven framework for
ASC, structured around four core phases: Design, Deployment, Operation, and
Evolution. We systematically analyze ASC through four foundational research
dimensions: (1) Perception, Context, and Environment Modeling, (2) Autonomous
Decision-Making and Task Execution, (3) Multi-Agent Collaboration and
Organization, and (4) Evaluation, Value Alignment, and Trustworthiness. We
examine how these dimensions are instantiated, integrated, and continuously
adapted across the service lifecycle. Our synthesis reveals that agentic
services are not merely assembled but orchestrated: contextual awareness
enables robust deployment; autonomous reasoning supports real-time operation;
collaborative structures emerge and evolve through interaction; and
trustworthiness must be upheld as a cross-cutting, lifelong imperative. We
further identify and discuss emerging trends shaping the future of ASC. By
integrating classical principles of services computing with advances in
LLM-based multi-agent systems, this work establishes a holistic and
forward-looking foundation for ASC. It provides a unified reference for
researchers and practitioners aiming to develop adaptive, accountable, and
human-centered intelligent services.

</details>


### [219] [Unit Test Update through LLM-Driven Context Collection and Error-Type-Aware Refinement](https://arxiv.org/abs/2509.24419)
*Yuanhe Zhang,Zhiquan Yang,Shengyi Pan,Zhongxin Liu*

Main category: cs.SE

TL;DR: 本文提出了一种基于大语言模型（LLM）的自动化单元测试更新方法TESTUPDATER，能够即时修复并增强单元测试，显著提升测试通过率和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 当前手动维护单元测试效率低且易延误，现有自动化方法主要关注测试修复，缺少对测试增强和复杂代码变更的有效处理，且测试生成正确率不高。

Method: TESTUPDATER利用LLM分析代码变更并提取上下文，通过设计的引导提示逐步处理各种代码变更和新增依赖，支持测试的修复与增强，并采用错误类型感知的迭代改进机制执行并修复测试，确保测试准确性。

Result: 在新构建的包含195个真实样本的UPDATES4J基准测试中，TESTUPDATER达到了94.4%的编译通过率和86.7%的测试通过率，分别比现有最佳方法提升15.9%和20.0%，同时分支覆盖率和行覆盖率也有显著提升。

Conclusion: TESTUPDATER有效解决了自动化单元测试修复与增强中的多种难题，显著提升测试质量和覆盖，推动了软件测试自动化的发展。

Abstract: Unit testing is critical for ensuring software quality and software system
stability. The current practice of manually maintaining unit tests suffers from
low efficiency and the risk of delayed or overlooked fixes. Therefore, an
automated approach is required to instantly update unit tests, with the
capability to both repair and enhance unit tests. However, existing automated
test maintenance methods primarily focus on repairing broken tests, neglecting
the scenario of enhancing existing tests to verify new functionality.
Meanwhile, due to their reliance on rule-based context collection and the lack
of verification mechanisms, existing approaches struggle to handle complex code
changes and often produce test cases with low correctness. To address these
challenges, we propose TESTUPDATER, a novel LLM based approach that enables
automated just-in-time test updates in response to production code changes.
TESTUPDATER first leverages the LLM to analyze code changes and identify
relevant context, which it then extracts and filters. Then, through carefully
designed prompts, TESTUPDATER guides the LLM step by step to handle various
types of code changes and introduce new dependencies, enabling both test repair
and enhancement. Finally, we introduce an error-type-aware iterative refinement
mechanism that executes the LLM-updated tests and repairs failures, which
significantly improves the overall correctness of test updates. Since existing
test repair datasets lack scenarios of test enhancement, we further construct a
new benchmark, UPDATES4J, with 195 real-world samples from 7 projects.
Experimental results show that TESTUPDATER achieves a compilation pass rate of
94.4% and a test pass rate of 86.7%, outperforming the state-of-the-art method
SYNTER by 15.9% and 20.0%, respectively. Furthermore, TESTUPDATER exhibits
12.9% higher branch coverage and 15.2% greater line coverage than SYNTER.

</details>


### [220] [Towards Shift-Up: A Framework and a Prestudy on High-Value Activities in GenAI Native Software Development](https://arxiv.org/abs/2509.24485)
*Vlad Stirbu,Mateen Ahmed Abbasi,Teerath Das,Jesse Haimi,Niko Iljin,Pyry Kotilainen,Petrus Lipsanen,Niko Mäkitalo,Maiju Sipilä,Venla Veijalainen,Tommi Mikkonen*

Main category: cs.SE

TL;DR: 本文提出了一个名为shift-up的生成式人工智能（GenAI）原生开发框架，旨在帮助软件团队借助GenAI专注于高价值工作，并展示了使用现有GenAI工具的初步研究。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能对软件工程产生了重大影响，促使开发模式从人类开发者转向基于用户提示的专用智能代理。为此，如何有效利用GenAI提升软件开发效率成为迫切需求。

Method: 提出shift-up框架，帮助软件团队在GenAI辅助下专注于高价值任务；设计并开展了一项基于现有GenAI工具的初步研究以验证框架的有效性。

Result: 初步研究表明，使用shift-up框架和现有GenAI工具能够支持软件团队提高开发效率和工作重点，同时展现了实施潜力。

Conclusion: shift-up框架为GenAI在软件工程中的应用提供了一种新思路，未来将进一步细化和深入研究以推动该方向的发展。

Abstract: Generative AI (GenAI) has significantly influenced software engineering.
Associated tools have created a shift in software engineering, where
specialized agents, based on user-provided prompts, are replacing human
developers. In this paper, we propose a framework for GenAI native development
that we call \textit{shift-up}, which helps software teams focus on high-value
work while being supported by GenAI. Furthermore, we also present a preliminary
study testing these ideas with current GenAI tools. Towards the end of the
paper, we propose future research goals to study shift-up in more detail.

</details>


### [221] [JSProtect: A Scalable Obfuscation Framework for Mini-Games in WeChat](https://arxiv.org/abs/2509.24498)
*Zhihao Li,Chaozheng Wang,Zongjie Li,Xinyong Peng,Zelin Su,Qun Xia,Haochuan Lu,Ting Xiong,Man Ho Lam,Shuzheng Gao,Yuchong Xie,Cuiyun Gao,Shuai Wang,Yuetang Deng,Huafeng Ma*

Main category: cs.SE

TL;DR: 本文提出了JSProtect，一种针对微信小游戏的高吞吐量并行JavaScript混淆框架，解决了现有工具处理时间长、运行性能差和代码膨胀严重的问题。


<details>
  <summary>Details</summary>
Motivation: 微信小游戏生态面临二次开发导致的知识产权盗用问题，现有JavaScript混淆工具无法高效处理大规模代码，存在性能和代码膨胀严重的限制。

Method: 提出了Parallel-Aware Scope Analysis (PASA)算法，实现代码独立分区以充分利用多核处理能力，以及独立命名空间管理，采用短标识符重用减少代码膨胀。

Result: JSProtect能在几分钟内处理20MB代码，保持100%语义等价，代码膨胀仅20%，远低于基线工具的1000%以上，并且保持接近原生的运行性能。

Conclusion: JSProtect为工业规模JavaScript保护提供了一种新范式，有效平衡了安全性、高性能和可扩展性。

Abstract: The WeChat mini-game ecosystem faces rampant intellectual property theft to
other platforms via secondary development, yet existing JavaScript obfuscation
tools are ill-equipped for large-scale applications, suffering from prohibitive
processing times, severe runtime performance degradation, and unsustainable
code size inflation. This paper introduces JSProtect, a high-throughput
parallelized obfuscation framework designed to overcome these fundamental
limitations. At the core of our framework is the Parallel-Aware Scope Analysis
(PASA) algorithm, which enables two key optimizations: independent code
partitioning for multi-core processing and independent namespace management
that aggressively reuses short identifiers to combat code bloat. Our evaluation
demonstrates that JSProtectprocesses 20MB codebases in minutes, maintaining
100\% semantic equivalence while controlling code size inflation to as low as
20\% compared to over 1,000\% with baseline tools. Furthermore, it preserves
near-native runtime performance and provides superior security effectiveness
against both static analysis tools and large language models. This work
presents a new paradigm for industrial-scale JavaScript protection that
effectively balances robust security with high performance and scalability.

</details>


### [222] [SemGuard: Real-Time Semantic Evaluator for Correcting LLM-Generated Code](https://arxiv.org/abs/2509.24507)
*Qinglin Wang,Zhihong Sun,Ruyun Wang,Tao Huang,Zhi Jin,Ge Li,Chen Lyu*

Main category: cs.SE

TL;DR: 本文介绍了一种名为SemGuard的实时语义监督框架，通过细粒度的数据集SemDiff训练语义评估器，在代码生成过程中及时发现并纠正语义错误，显著提升了大型语言模型的代码生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型生成的代码存在大量语义错误，传统的事后修复方法延迟大且不准确，需在代码生成阶段就注入精准的语义信息以避免错误传播。

Method: 构建标注代码差异的细粒度数据集SemDiff，训练语义评估器并嵌入解码器中，实现行级实时语义监督，检测部分代码偏离并回滚重生成，无需执行程序或测试用例。

Result: 在多个基准测试中，SemGuard相较于最先进方法ROCODE显著降低语义错误率19.86%，提升Pass@1指标近49%，且对不同模型和语言均有良好适应性。

Conclusion: SemGuard通过实时语义评价实现了代码生成过程中的错误早期纠正，显著提高了代码的语义正确性和生成质量，是提升代码自动生成效能的有效方案。

Abstract: Large Language Models (LLMs) can translate natural language requirements into
code, yet empirical analyses of representative models reveal that semantic
errors-programs that compile but behave incorrectly-constitute the majority of
observed faults (e.g., >60% on DeepSeek-Coder-6.7B and QwenCoder-7B). Post-hoc
repair pipelines detect such faults only after execution, incurring latency,
relying on incomplete test suites, and often mis-localizing the defect. Since
semantic drift originates in the autoregressive decoding process, intervening
while the code is being generated is a direct way to stop error propagation.
Constrained-decoding approaches such as ROCODE attempt this, but still wait
until the entire program runs to obtain feedback and use entropy heuristics
that do not truly capture semantics. A more effective solution must inject
semantic signals-early and precisely-into the decoding process.We present
SemGuard, a semantic-evaluator-driven framework that performs real-time,
line-level semantic supervision. To train the evaluator, we build SemDiff, the
first dataset with fine-grained annotations that mark the exact line where a
correct and an incorrect implementation diverge. The evaluator, once embedded
in the LLM's decoder, flags deviations on partial code, rolls back to the
faulty line, and guides regeneration-without executing the program or requiring
test cases. Across four benchmarks, SemGuard consistently outperforms
state-of-the-art baselines. It lowers the semantic error rate by 19.86% on
SemDiff relative to ROCODE, and lifts Pass@1 by 48.92% on the real-world
LiveCodeBench with CodeLlama-7B. Similar gains hold for StarCoder2-7B on MBPP
and for DeepSeekCoder-6.7B on the Java benchmark SemDiff-Java, demonstrating
model- and language-agnostic effectiveness.

</details>


### [223] [Agentic Specification Generator for Move Programs](https://arxiv.org/abs/2509.24515)
*Yu-Fu Fu,Meng Xu,Taesoo Kim*

Main category: cs.SE

TL;DR: 本文提出了针对Move智能合约的自动规范生成工具MSG，利用大语言模型显著提升了非主流语言的规范生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的规范生成工具主要支持主流编程语言，对以Move为代表的新兴且强调验证的语言支持不足。

Method: 提出MSG工具，采用代理化、模块化设计，结合规范语言特性及验证工具链反馈，提升规范生成的质量和可验证性。

Result: MSG对84%的测试Move函数生成了可验证规范，比传统设计多生成57%的可验证条款，利用验证反馈后可验证规范增加30%。

Conclusion: 基于LLM的规范生成在新兴语言Move上表现优异，模块化设计和验证反馈显著提升了规范质量，推动了非主流语言的智能合约验证。

Abstract: While LLM-based specification generation is gaining traction, existing tools
primarily focus on mainstream programming languages like C, Java, and even
Solidity, leaving emerging and yet verification-oriented languages like Move
underexplored. In this paper, we introduce MSG, an automated specification
generation tool designed for Move smart contracts. MSG aims to highlight key
insights that uniquely present when applying LLM-based specification generation
to a new ecosystem. Specifically, MSG demonstrates that LLMs exhibit robust
code comprehension and generation capabilities even for non-mainstream
languages. MSG successfully generates verifiable specifications for 84% of
tested Move functions and even identifies clauses previously overlooked by
experts. Additionally, MSG shows that explicitly leveraging specification
language features through an agentic, modular design improves specification
quality substantially (generating 57% more verifiable clauses than conventional
designs). Incorporating feedback from the verification toolchain further
enhances the effectiveness of MSG, leading to a 30% increase in generated
verifiable specifications.

</details>


### [224] [Bridging Developer Instructions and Code Completion Through Instruction-Aware Fill-in-the-Middle Paradigm](https://arxiv.org/abs/2509.24637)
*Zhensu Sun,Chengran Yang,Chao Peng,Pengfei Gao,Xiaoning Du,Li Li,David Lo*

Main category: cs.SE

TL;DR: 本文提出IFIM，一种结合指令调优的中间填充方法，提升代码补全模型的指令理解能力，同时保持原有补全性能。


<details>
  <summary>Details</summary>
Motivation: 现有代码大模型在处理含指令的代码补全时表现不佳，指令调优方法虽提升指令跟随能力却损害中间填充性能，需解决这一矛盾。

Method: IFIM方法扩展了传统的中间填充训练，输入增加显式指令部分，使模型学习（前缀、指令、后缀）三元组，并用GPT-4o生成大规模指令数据进行训练。

Result: 在HumanEval-infilling和RepoMasterEval基准测试中，IFIM显著提升指令跟随能力，HumanEval-infilling上Pass@1从84.6%提升至93.6%，且未损害无指令情况下的原始性能。

Conclusion: IFIM有效提升了代码补全模型对自然语言指令的利用能力，实现了指令理解与中间填充性能的兼顾，为代码补全模型的实用性提供了支持。

Abstract: Large Language Models (LLMs) have significantly advanced code completion, yet
they often fail when the developer's intent is underspecified in the code
context. To address this, developers usually add natural language instructions
(e.g., comments) into the code context to clarify their intent. However,
existing code LLMs applied for code completion systems merely undergo a
fill-in-the-middle (FIM) pre-training, which struggles to leverage this
information effectively due to the lack of instruction-like training data.
Existing instruction-tuning techniques, which improve instruction-following in
general code generation, paradoxically degrade FIM performance, forcing a
trade-off between instruction-following and infilling capabilities. To address
this gap, we introduce Instruction-aware Fill-in-the-Middle (IFIM), an
instruction-tuning method specifically designed to enhance FIM code completion
models. IFIM extends the conventional FIM training objective by incorporating
an explicit instruction section into the input, enabling the model to learn
from (prefix, instruction, suffix) triplets. This approach allows the model to
effectively leverage developer-provided directives while preserving its core
completion abilities when no instructions are present. To facilitate this, we
constructed a large-scale dataset by using GPT-4o to generate concise,
intent-focused instructions for code infilling examples. We evaluated IFIM by
applying it to two popular base models, Deepseek-Coder and Qwen2.5-Coder, on
the benchmarks derived from HumanEval-infilling and RepoMasterEval. The results
demonstrate that IFIM significantly improves instruction-following
capabilities, boosting the Pass@1 score from 84.6% to 93.6% on
HumanEval-infilling. Moreover, this enhancement does not compromise the models'
original performance on FIM code completion tasks with no instructions
provided.

</details>


### [225] [CoTune: Co-evolutionary Configuration Tuning](https://arxiv.org/abs/2509.24694)
*Gangda Xiong,Tao Chen*

Main category: cs.SE

TL;DR: 本文提出了CoTune，一种通过协同演进方法自动调优系统配置以满足复杂性能需求的工具，显著优于现有调优器。


<details>
  <summary>Details</summary>
Motivation: 现有自动调优器通常忽视复杂的性能需求，如具体的延迟目标，仅简单假设性能越好越优，导致信息浪费和资源浪费。将需求作为优化目标会带来收敛问题。

Method: CoTune引入一个辅助性能需求与系统配置共同演进，辅助主性能需求在其失效或误导时起作用，实现基于需求的稳健调优。

Result: 在162个案例（9个系统，18个需求）上的实验显示，CoTune在90%的案例中表现最佳，整体提升最高达2.9倍，且效率更高。

Conclusion: CoTune有效融合性能需求信息，避免了需求过严或多样性带来的负面影响，实现了更高效且表现更优的自动系统调优。

Abstract: To automatically tune configurations for the best possible system performance
(e.g., runtime or throughput), much work has been focused on designing
intelligent heuristics in a tuner. However, existing tuner designs have mostly
ignored the presence of complex performance requirements (e.g., the latency
shall ideally be 2 seconds), but simply assume that better performance is
always more preferred. This would not only waste valuable information in a
requirement but might also consume extensive resources to tune for a goal with
little gain. Yet, prior studies have shown that simply incorporating the
requirement as a tuning objective is problematic since the requirement might be
too strict, harming convergence; or its highly diverse satisfactions might lead
to premature convergence. In this paper, we propose CoTune, a tool that takes
the information of a given target performance requirement into account through
co-evolution. CoTune is unique in the sense that it creates an auxiliary
performance requirement to be co-evolved with the configurations, which assists
the target performance requirement when it becomes ineffective or even
misleading, hence allowing the tuning to be guided by the requirement while
being robust to its harm. Experiment results on 162 cases (nine systems and 18
requirements) reveal that CoTune considerably outperforms existing tuners,
ranking as the best for 90% cases (against the 0%--35% for other tuners) with
up to 2.9x overall improvements, while doing so under a much better efficiency.

</details>


### [226] [Large language models for behavioral modeling: A literature survey](https://arxiv.org/abs/2509.24782)
*Muhammad Laiq*

Main category: cs.SE

TL;DR: 本文概述了当前利用大型语言模型（LLMs）在行为建模领域的研究，重点关注用例图和时序图的自动生成，发现LLMs表现出良好效果，但存在专家评估缺失和模型选择单一的问题。


<details>
  <summary>Details</summary>
Motivation: 当前虽广泛使用LLMs进行行为建模，但缺乏相关综述，亟需总结现状以指导未来研究并帮助实践者和教育者了解LLMs辅助行为建模的有效性。

Method: 通过基于术语的文献检索筛选出14篇相关研究，分析这些研究在用例图和时序图自动生成中的应用和表现。

Result: LLMs在自动生成用例图和时序图方面展示出良好效果；但现有研究多使用GPT模型，缺乏领域专家的评估。

Conclusion: 未来研究应扩大LLMs种类以评估其行为建模能力，并应引入领域专家进行输出评估，以提高研究的可靠性和应用价值。

Abstract: In recent years, large language models (LLMs) have been extensively utilized
for behavioral modeling, for example, to automatically generate sequence
diagrams. However, no overview of this work has been published yet. Such an
overview will help identify future research directions and inform practitioners
and educators about the effectiveness of LLMs in assisting behavioral modeling.
This study aims to provide an overview of the existing research on the use of
LLMs for behavioral modeling, particularly focusing on use case and sequence
diagrams. Through a term-based search, we filtered and identified 14 relevant
primary studies. Our analysis of the selected primary studies reveals that LLMs
have demonstrated promising results in automatically generating use case and
sequence diagrams. In addition, we found that most of the current literature
lacks expert-based evaluations and has mainly used GPT-based models. Therefore,
future work should evaluate a broader range of LLMs for behavioral modeling and
involve domain experts to evaluate the output of LLMs.

</details>


### [227] [Evaluating SAP Joule for Code Generation](https://arxiv.org/abs/2509.24828)
*Joshua Heisler,Johannes Reisinger,Andreas Fischer*

Main category: cs.SE

TL;DR: 本文评估了SAP自研生成模型Joule在Javascript代码生成方面的性能，使用HumanEval-X基准测试，将其与29个模型进行比较，Joule排名第五，准确率为80.49%。


<details>
  <summary>Details</summary>
Motivation: 评估SAP最新发布的生成模型Joule在代码生成领域，特别是在Javascript语言上的实际表现，填补对其性能的评测空白。

Method: 采用HumanEval-X Javascript基准，对SAP Joule与其他29个生成模型进行严格准确率比较。

Result: SAP Joule在HumanEval-X测试中取得了80.49%的准确率，排名第五，表现优异。

Conclusion: SAP Joule展示了强大的Javascript代码生成能力，尽管尚未针对SAP特有语言优化，但已具备较高竞争力，是首次对其代码生成能力的系统对比评测。

Abstract: SAP has released its own proprietary generative model SAP Joule, intended for
various generative tasks, including serving as a code assistant for software
engineers. While Joule is yet not focused on SAP-specific ABAP code generation,
it can be used for other common languages, including Javascript. This paper
compares SAP Joules Javascript coding capabilities against a total of 29 other
models using the HumanEval-X Javascript benchmark. SAP Joule achieves a strict
accuracy of 80.49% as the fifth best model in our evaluation. To the best of
our knowledge, this is the first comparative evaluation of SAP Joule code
generation capabilities.

</details>


### [228] [DiffTester: Accelerating Unit Test Generation for Diffusion LLMs via Repetitive Pattern](https://arxiv.org/abs/2509.24975)
*Lekang Yang,Yuetong Liu,Yitong Zhang,Jia Li*

Main category: cs.SE

TL;DR: 本文提出了DiffTester，一个针对扩散式大语言模型(dLLMs)的自动单元测试生成加速框架，通过动态识别测试中的重复结构模式，提升生成效率且不降低测试质量，在多个编程语言和模型上表现出优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型自动生成单元测试效率低，尤其是生成多个token时测试质量显著下降，限制了扩散式大语言模型在自动单元测试生成中的应用。

Method: DiffTester利用抽象语法树分析动态识别目标方法的重复结构模式，自适应增加每步生成的token数，从而提高生成速度同时保证测试质量。

Result: 在三个基准测试和两种代表性模型上，DiffTester显著加速了测试生成过程，保持了测试覆盖率，并且在不同扩散式大语言模型和编程语言上均表现良好。

Conclusion: DiffTester为软件开发中的自动单元测试生成提供了一种高效且可扩展的解决方案，有望广泛应用于多种编程语言和模型环境。

Abstract: Software development relies heavily on extensive unit testing, which makes
the efficiency of automated Unit Test Generation (UTG) particularly important.
However, most existing LLMs generate test cases one token at a time in each
forward pass, which leads to inefficient UTG. Recently, diffusion LLMs (dLLMs)
have emerged, offering promising parallel generation capabilities and showing
strong potential for efficient UTG. Despite this advantage, their application
to UTG is still constrained by a clear trade-off between efficiency and test
quality, since increasing the number of tokens generated in each step often
causes a sharp decline in the quality of test cases. To overcome this
limitation, we present DiffTester, an acceleration framework specifically
tailored for dLLMs in UTG. The key idea of DiffTester is that unit tests
targeting the same focal method often share repetitive structural patterns. By
dynamically identifying these common patterns through abstract syntax tree
analysis during generation, DiffTester adaptively increases the number of
tokens produced at each step without compromising the quality of the output. To
enable comprehensive evaluation, we extend the original TestEval benchmark,
which was limited to Python, by introducing additional programming languages
including Java and C++. Extensive experiments on three benchmarks with two
representative models show that DiffTester delivers significant acceleration
while preserving test coverage. Moreover, DiffTester generalizes well across
different dLLMs and programming languages, providing a practical and scalable
solution for efficient UTG in software development. Code and data are publicly
available at https://github.com/wellbeingyang/DLM4UTG-open .

</details>


### [229] [Large Language Models for Software Testing: A Research Roadmap](https://arxiv.org/abs/2509.25043)
*Cristian Augusto,Antonia Bertolino,Guglielmo De Angelis,Francesca Lonetti,Jesús Morán*

Main category: cs.SE

TL;DR: 本文系统回顾了大语言模型在软件测试领域的应用，梳理其研究进展与趋势。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在软件测试领域快速发展，研究成果繁多，缺乏系统性综述和明确的研究路线图。

Method: 通过半系统文献综述，收集整理相关研究，分类总结当前状态、研究方向及存在挑战。

Result: 构建了大语言模型软件测试领域的研究分类和现有成果，分析了活跃的研究方向和面临的挑战。

Conclusion: 提出了大语言模型对软件测试长远影响的展望，为相关研究提供了结构化的指导和未来方向。

Abstract: Large Language Models (LLMs) are starting to be profiled as one of the most
significant disruptions in the Software Testing field.
  Specifically, they have been successfully applied in software testing tasks
such as generating test code, or summarizing documentation.
  This potential has attracted hundreds of researchers, resulting in dozens of
new contributions every month, hardening researchers to
  stay at the forefront of the wave. Still, to the best of our knowledge, no
prior work has provided a structured vision of the progress
  and most relevant research trends in LLM-based testing. In this article, we
aim to provide a roadmap that illustrates its current state,
  grouping the contributions into different categories, and also sketching the
most promising and active research directions for the field.
  To achieve this objective, we have conducted a semi-systematic literature
review, collecting articles and mapping them into the most
  prominent categories, reviewing the current and ongoing status, and analyzing
the open challenges of LLM-based software testing.
  Lastly, we have outlined several expected long-term impacts of LLMs over the
whole software testing field.

</details>


### [230] [Towards Reliable Generation of Executable Workflows by Foundation Models](https://arxiv.org/abs/2509.25117)
*Sogol Masoumzadeh,Keheliya Gallaba,Dayi Lin,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 本文提出了一种框架，通过静态分析反馈检测并修复基础模型生成的领域专用语言工作流中的缺陷，提高其准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 基础模型生成的工作流中缺陷普遍存在，人工构建工作流耗时且需专业知识，需要自动化方法提高生成工作流的质量和可靠性。

Method: 通过静态分析方法开发了专门的分析器Timon，检测九种缺陷类型，并结合反馈指导基于基础模型的工具Pumbaa修复缺陷。

Result: 统计发现87.27%的基础模型生成的工作流包含缺陷，Timon能够有效检测缺陷，结合Pumbaa实现缺陷修复。

Conclusion: 本方法为从自然语言需求自动生成可执行工作流提供了关键步骤，提升了工作流生成的可靠性和自动化水平。

Abstract: Recent advancements in Foundation Models (FMs) have demonstrated significant
progress in comprehending complex natural language to perform intricate tasks.
Successfully executing these tasks often requires orchestrating calls to FMs
alongside other software components. However, manually decomposing a task into
a coherent sequence of smaller, logically aggregated steps, commonly referred
to as workflows, demands considerable effort and specialized domain knowledge.
While FMs can assist in generating such workflows specified in domain-specific
languages (DSLs), achieving accuracy and reliability in this process remains a
challenge.
  This work introduces a framework that leverages static analysis feedback to
enable FMs to detect and repair defects in the DSL-based workflows they
generate. We begin by presenting the first-ever taxonomy of incidences of
defects in FM-generated DSL workflows, categorizing them into 18 distinct
types. Furthermore, we observe a high prevalence of defects across FM-generated
DSL workflows, with 87.27% of the studied instances containing at least one
defect. This, in turn, emphasizes the magnitude of the problem in practice and
underscores the necessity for implementing mitigation strategies. Following
this, we demonstrate that nine types of these defects can be effectively
identified through static analysis of the workflows. For this purpose, we
develop Timon, the first-of-its-kind static analyzer specifically designed for
FM-generated DSL workflows. Finally, we show that by incorporating feedback
from Timon, we can guide Pumbaa, an FM-based tool, to repair the detected
defect incidences. By systematically detecting and repairing defects, our work
provides a crucial step towards the reliable and automated generation of
executable workflows from natural language requirements.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [231] [Game-Theoretic Understandings of Multi-Agent Systems with Multiple Objectives](https://arxiv.org/abs/2509.23026)
*Yue Wang*

Main category: cs.MA

TL;DR: 本文提出了多目标马尔可夫博弈（MOMG）框架，解决多智能体系统中多目标强化学习问题，提出并证明了Pareto-纳什均衡（PNE）的存在性，设计了计算PNE的在线学习算法和无偏好双阶段算法。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统中代理目标多样，导致性能复杂且存在权衡，亟需有效的多目标强化学习框架。

Method: 提出MOMG框架，引入Pareto-纳什均衡作为解概念，证明其存在性及与标量化纳什均衡的等价关系，设计在线学习算法和无偏好双阶段算法。

Result: 证明了PNE存在且可通过标量化方法求解，提出的算法可以在线学习并高效计算不同偏好下的PNE解。

Conclusion: 本文构建了多目标多智能体强化学习理论基础及有效算法，推动了复杂多目标多智能体系统的解决。

Abstract: In practical multi-agent systems, agents often have diverse objectives, which
makes the system more complex, as each agent's performance across multiple
criteria depends on the joint actions of all agents, creating intricate
strategic trade-offs. To address this, we introduce the Multi-Objective Markov
Game (MOMG), a framework for multi-agent reinforcement learning with multiple
objectives. We propose the Pareto-Nash Equilibrium (PNE) as the primary
solution concept, where no agent can unilaterally improve one objective without
sacrificing performance on another. We prove existence of PNE, and establish an
equivalence between the PNE and the set of Nash Equilibria of MOMG's
corresponding linearly scalarized games, enabling solutions of MOMG by
transferring to a standard single-objective Markov game. However, we note that
computing a PNE is theoretically and computationally challenging, thus we
propose and study weaker but more tractable solution concepts. Building on
these foundations, we develop online learning algorithm that identify a single
solution to MOMGs. Furthermore, we propose a two-phase, preference-free
algorithm that decouples exploration from planning. Our algorithm enables
computation of a PNE for any given preference profile without collecting new
samples, providing an efficient methodological characterization of the entire
Pareto-Nash front.

</details>


### [232] [Situational Awareness for Safe and Robust Multi-Agent Interactions Under Uncertainty](https://arxiv.org/abs/2509.23425)
*Benjamin Alcorn,Eman Hammad*

Main category: cs.MA

TL;DR: 本文研究多智能体系统中非协调代理的意图识别与行为预测，并在资源受限条件下实现最优决策。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统广泛应用于电力系统、车载网络和机器人领域，解决非协调智能体意图预测及资源限制下的最优决策问题具有重要意义。

Method: 构建了一个模型，其中自治代理在安全观察半径内观测环境，估计周围代理的未来动作，通过强化学习和博弈论算法实现决策；缺乏观测时采用基于历史轨迹的估计算法，引入风险分析管理不确定性。

Result: 提出的方法在强化学习和博弈论两种决策框架下验证，成功实现了对其他智能体未来行为的预测及自身最优行动的决策。

Conclusion: 该研究有效解决了多智能体系统中意图预测及资源受限条件下的最优策略制定问题，展示了结合估计算法和风险分析的可行性。

Abstract: Multi-agent systems are prevalent in a wide range of domains including power
systems, vehicular networks, and robotics. Two important problems to solve in
these types of systems are how the intentions of non-coordinating agents can be
determined to predict future behavior and how the agents can achieve their
objectives under resource constraints without significantly sacrificing
performance. To study this, we develop a model where an autonomous agent
observes the environment within a safety radius of observation, determines the
state of a surrounding agent of interest (within the observation radius),
estimates future actions to be taken, and acts in an optimal way. In the
absence of observations, agents are able to utilize an estimation algorithm to
predict the future actions of other agents based on historical trajectory. The
use of the proposed estimation algorithm introduces uncertainty, which is
managed via risk analysis. The proposed approach in this study is validated
using two different learning-based decision making frameworks: reinforcement
learning and game theoretic algorithms.

</details>


### [233] [PartnerMAS: An LLM Hierarchical Multi-Agent Framework for Business Partner Selection on High-Dimensional Features](https://arxiv.org/abs/2509.24046)
*Lingyao Li,Haolun Wu,Zhenkun Li,Jiabei Hu,Yu Wang,Xiaoshan Huang,Wenyue Hua,Wenqian Wang*

Main category: cs.MA

TL;DR: 本文提出了一个名为PartnerMAS的分层多智能体框架，用于高维度决策任务中的合作伙伴选择，提升了评估精度。


<details>
  <summary>Details</summary>
Motivation: 单一大语言模型或辩论式系统在处理具有异质特征的大规模候选池时，存在可扩展性和一致性问题，需求更有效的多智能体协作机制。

Method: 设计了三层结构的多智能体系统，包括规划者代理（设计策略）、专门代理（角色评估）和监督者代理（整合输出），并提供一个风险投资共同投资的基准数据集用于评测。

Result: 通过140个案例测试，PartnerMAS在匹配率上领先单智能体和辩论多智能体系统10-15%，该框架有效提升了评估准确性和结果鲁棒性。

Conclusion: 多智能体的结构化协作比单一模型扩展更能提升高维决策任务的性能，PartnerMAS为数据丰富领域的复杂决策提供了有前景的解决方案。

Abstract: High-dimensional decision-making tasks, such as business partner selection,
involve evaluating large candidate pools with heterogeneous numerical,
categorical, and textual features. While large language models (LLMs) offer
strong in-context reasoning capabilities, single-agent or debate-style systems
often struggle with scalability and consistency in such settings. We propose
PartnerMAS, a hierarchical multi-agent framework that decomposes evaluation
into three layers: a Planner Agent that designs strategies, Specialized Agents
that perform role-specific assessments, and a Supervisor Agent that integrates
their outputs. To support systematic evaluation, we also introduce a curated
benchmark dataset of venture capital co-investments, featuring diverse firm
attributes and ground-truth syndicates. Across 140 cases, PartnerMAS
consistently outperforms single-agent and debate-based multi-agent baselines,
achieving up to 10--15\% higher match rates. Analysis of agent reasoning shows
that planners are most responsive to domain-informed prompts, specialists
produce complementary feature coverage, and supervisors play an important role
in aggregation. Our findings demonstrate that structured collaboration among
LLM agents can generate more robust outcomes than scaling individual models,
highlighting PartnerMAS as a promising framework for high-dimensional
decision-making in data-rich domains.

</details>


### [234] [CORRECT: COndensed eRror RECognition via knowledge Transfer in multi-agent systems](https://arxiv.org/abs/2509.24088)
*Yifan Yu,Moyan Li,Shaoyuan Xu,Jinmiao Fu,Xinhai Hou,Fan Lai,Bryan Wang*

Main category: cs.MA

TL;DR: 本文提出了CORRECT框架，通过在线缓存错误模式，实现多智能体系统中错误的快速定位，无需训练，显著提升定位准确率。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统的错误传播复杂且难以识别，传统方法训练成本高且适应性差，需一种轻量且高效的错误识别方法。

Method: 设计了基于缓存的错误模式复用机制，使大语言模型能够实时识别和迁移错误结构，无需代价昂贵的重训练，同时构建了大规模注释数据集支持研究。

Result: 在七个多智能体系统应用中，CORRECT实现了步级错误定位准确率提升19.8%，且计算开销几乎为零，显著缩小了自动与人工错误识别的差距。

Conclusion: CORRECT是一种高效、适应性强的多智能体系统错误识别框架，能有效支持动态部署场景，推动自动化调试与分析技术的发展。

Abstract: Multi-agent systems (MAS) are increasingly capable of tackling complex
real-world tasks, yet their reliance on inter-agent coordination, tool use, and
long-horizon reasoning makes error recognition particularly challenging. Minor
errors can propagate across agents, escalating into task failures while
producing long, intertwined execution trajectories that impose significant
costs for both human developers and automated systems to debug and analyze. Our
key insight is that, despite surface differences in failure trajectories (e.g.,
logs), MAS errors often recur with similar structural patterns. This paper
presents CORRECT, the first lightweight, training-free framework that leverages
an online cache of distilled error schemata to recognize and transfer knowledge
of failure structures across new requests. This cache-based reuse allows LLMs
to perform targeted error localization at inference time, avoiding the need for
expensive retraining while adapting to dynamic MAS deployments in subseconds.
To support rigorous study in this domain, we also introduce CORRECT-Error, a
large-scale dataset of over 2,000 annotated trajectories collected through a
novel error-injection pipeline guided by real-world distributions, and further
validated through human evaluation to ensure alignment with natural failure
patterns. Experiments across seven diverse MAS applications show that CORRECT
improves step-level error localization up to 19.8% over existing advances while
at near-zero overhead, substantially narrowing the gap between automated and
human-level error recognition.

</details>


### [235] [MAS$^2$: Self-Generative, Self-Configuring, Self-Rectifying Multi-Agent Systems](https://arxiv.org/abs/2509.24323)
*Kun Wang,Guibin Zhang,ManKit Ye,Xinyu Deng,Dongxia Wang,Xiaobin Hu,Jinyang Guo,Yang Liu,Yufei Guo*

Main category: cs.MA

TL;DR: 本文提出了一种基于递归自生成原则的多智能体系统MAS^2，通过生成器-实现者-整正者三智能体团队，实现了多任务动态适应与优化，在多个复杂任务上优于现有多智能体系统，且具有较优的性能与成本平衡。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统多采用生成一次后部署的方式，系统刚性强，难以适应真实环境中的动态和不确定性，亟需一种能够自我演化和动态调整的多智能体系统框架。

Method: 提出MAS^2，基于递归自生成理念设计生成器-实现者-整正者三智能体团队，利用协作树优化方法训练与专门化这些元智能体，动态构建和实时调整目标多智能体系统以应对复杂任务需求。

Result: 在七个基准测试中，MAS^2在深度研究和代码生成等复杂场景中性能提升最高达19.6%，对新型大语言模型的泛化能力也表现优异，提升达15.1%，同时保持了成本与性能的最佳平衡。

Conclusion: MAS^2突破了传统多智能体系统的静态局限，通过递归自生成实现多任务适应与优化，表现出强大的性能和泛化能力，是多智能体系统发展的新方向。

Abstract: The past two years have witnessed the meteoric rise of Large Language Model
(LLM)-powered multi-agent systems (MAS), which harness collective intelligence
and exhibit a remarkable trajectory toward self-evolution. This paradigm has
rapidly progressed from manually engineered systems that require bespoke
configuration of prompts, tools, roles, and communication protocols toward
frameworks capable of automated orchestration. Yet, dominant automatic
multi-agent systems, whether generated by external modules or a single LLM
agent, largely adhere to a rigid ``\textit{generate-once-and-deploy}''
paradigm, rendering the resulting systems brittle and ill-prepared for the
dynamism and uncertainty of real-world environments. To transcend this
limitation, we introduce MAS$^2$, a paradigm predicated on the principle of
recursive self-generation: a multi-agent system that autonomously architects
bespoke multi-agent systems for diverse problems. Technically, we devise a
``\textit{generator-implementer-rectifier}'' tri-agent team capable of
dynamically composing and adaptively rectifying a target agent system in
response to real-time task demands. Collaborative Tree Optimization is proposed
to train and specialize these meta-agents. Extensive evaluation across seven
benchmarks reveals that MAS$^2$ achieves performance gains of up to $19.6\%$
over state-of-the-art MAS in complex scenarios such as deep research and code
generation. Moreover, MAS$^2$ exhibits superior cross-backbone generalization,
effectively leveraging previously unseen LLMs to yield improvements of up to
$15.1\%$. Crucially, these gains are attained without incurring excessive token
costs, as MAS$^2$ consistently resides on the Pareto frontier of
cost-performance trade-offs. The source codes are available at
https://github.com/yeyeyeah2/MAS2.

</details>


### [236] [MARLIN: Multi-Agent Reinforcement Learning with Murmuration Intelligence and LLM Guidance for Reservoir Management](https://arxiv.org/abs/2509.25034)
*Heming Fu,Guojun Xiong,Jian Li,Shan Lin*

Main category: cs.MA

TL;DR: 提出了MARLIN框架，通过模拟鸟群智能实现分布式水库管理，提升不确定性处理和协调效率。


<details>
  <summary>Details</summary>
Motivation: 气候变化引发极端天气，导致水灾威胁加剧，传统集中优化和现有多智能体强化学习方法在复杂不确定环境下难以有效管理水资源。

Method: 基于多智能体强化学习结合仿生学的对齐、分离和凝聚规则，实现水库的局部决策和全局协调；利用大型语言模型提供实时奖励塑形以适应环境变化和人类偏好。

Result: 在美国地质调查局真实数据实验中，MARLIN提升了23%的不确定性处理能力，减少35%的计算量，加速68%的洪水响应，且协调能力呈超线性提升。

Conclusion: MARLIN展示了通过智能与可扩展的方法有效应对复杂水资源管理中的不确定性和协调挑战，为灾害预防和社区保护提供了新的技术途径。

Abstract: As climate change intensifies extreme weather events, water disasters pose
growing threats to global communities, making adaptive reservoir management
critical for protecting vulnerable populations and ensuring water security.
Modern water resource management faces unprecedented challenges from cascading
uncertainties propagating through interconnected reservoir networks. These
uncertainties, rooted in physical water transfer losses and environmental
variability, make precise control difficult. For example, sending 10 tons
downstream may yield only 8-12 tons due to evaporation and seepage. Traditional
centralized optimization approaches suffer from exponential computational
complexity and cannot effectively handle such real-world uncertainties, while
existing multi-agent reinforcement learning (MARL) methods fail to achieve
effective coordination under uncertainty. To address these challenges, we
present MARLIN, a decentralized reservoir management framework inspired by
starling murmurations intelligence. Integrating bio-inspired alignment,
separation, and cohesion rules with MARL, MARLIN enables individual reservoirs
to make local decisions while achieving emergent global coordination. In
addition, a LLM provides real-time reward shaping signals, guiding agents to
adapt to environmental changes and human-defined preferences. Experiments on
real-world USGS data show that MARLIN improves uncertainty handling by 23\%,
cuts computation by 35\%, and accelerates flood response by 68\%, exhibiting
super-linear coordination, with complexity scaling 5.4x from 400 to 10,000
nodes. These results demonstrate MARLIN's potential for disaster prevention and
protecting communities through intelligent, scalable water resource management.

</details>
