{"id": "2510.23627", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23627", "abs": "https://arxiv.org/abs/2510.23627", "authors": ["Fred Zimmerman"], "title": "AI-Driven Development of a Publishing Imprint: Xynapse Traces", "comment": null, "summary": "Xynapse Traces is an experimental publishing imprint created via a fusion of\nhuman and algorithmic methods using a configuration-driven architecture and a\nmulti-model AI integration framework. The system achieved a remarkable 90%\nreduction in time-to-market (from a typical 6-12 months to just 2-4 weeks),\nwith 80% cost reduction compared to traditional imprint development, while\npublishing 52 books in its first year and maintaining exceptional quality\nmetrics, including 99% citation accuracy and 100% validation success after\ninitial corrections. Key technical innovations include a continuous ideation\npipeline with tournament-style evaluation, a novel codex design for\ntranscriptive meditation practice, comprehensive automation spanning from\nideation through production and distribution, and publisher personas that\ndefine and guide the imprint's mission. The system also integrates automated\nverification with human oversight, ensuring that gains in speed do not\ncompromise publishing standards. This effort has significant implications for\nthe future of book publishing, suggesting new paradigms for human-AI\ncollaboration that democratize access to sophisticated publishing capabilities\nand make previously unviable niche markets accessible.", "AI": {"tldr": "Xynapse Traces\u901a\u8fc7\u4eba\u673a\u878d\u5408\u548c\u591a\u6a21\u578bAI\u6846\u67b6\uff0c\u5b9e\u73b0\u4e66\u7c4d\u51fa\u7248\u6548\u7387\u548c\u6210\u672c\u7684\u6781\u5927\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u51fa\u7248\u5468\u671f\u957f\u3001\u6210\u672c\u9ad8\u4e14\u6548\u7387\u4f4e\uff0c\u4e14\u96be\u4ee5\u6ee1\u8db3\u591a\u6837\u5316\u5e02\u573a\u9700\u6c42\u3002", "method": "\u91c7\u7528\u914d\u7f6e\u9a71\u52a8\u67b6\u6784\u548c\u591a\u6a21\u578bAI\u96c6\u6210\uff0c\u6784\u5efa\u8fde\u7eed\u521b\u610f\u6d41\u6c34\u7ebf\u3001\u7ade\u8d5b\u5f0f\u8bc4\u4f30\u548c\u81ea\u52a8\u5316\u751f\u4ea7\u5206\u53d1\u6d41\u7a0b\uff0c\u5e76\u7ed3\u5408\u4eba\u5de5\u76d1\u7763\u786e\u4fdd\u8d28\u91cf\u3002", "result": "\u5b9e\u73b0\u4e0a\u5e02\u65f6\u95f4\u7f29\u77ed90%\u81f32-4\u5468\uff0c\u6210\u672c\u964d\u4f4e80%\uff0c\u9996\u5e74\u51fa\u724852\u672c\u4e66\uff0c\u5f15\u7528\u51c6\u786e\u7387\u8fbe99%\uff0c\u6821\u9a8c\u6210\u529f\u7387100%\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5c55\u793a\u4e86\u4eba\u673a\u534f\u4f5c\u51fa\u7248\u7684\u65b0\u8303\u5f0f\uff0c\u63a8\u52a8\u51fa\u7248\u6c11\u4e3b\u5316\u548c\u7ec6\u5206\u5e02\u573a\u5f00\u53d1\u7684\u53ef\u80fd\u3002"}}
{"id": "2510.23642", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.23642", "abs": "https://arxiv.org/abs/2510.23642", "authors": ["Yuansheng Ni", "Songcheng Cai", "Xiangchao Chen", "Jiarong Liang", "Zhiheng Lyu", "Jiaqi Deng", "Kai Zou", "Ping Nie", "Fei Yuan", "Xiang Yue", "Wenhu Chen"], "title": "VisCoder2: Building Multi-Language Visualization Coding Agents", "comment": null, "summary": "Large language models (LLMs) have recently enabled coding agents capable of\ngenerating, executing, and revising visualization code. However, existing\nmodels often fail in practical workflows due to limited language coverage,\nunreliable execution, and lack of iterative correction mechanisms. Progress has\nbeen constrained by narrow datasets and benchmarks that emphasize single-round\ngeneration and single-language tasks. To address these challenges, we introduce\nthree complementary resources for advancing visualization coding agents.\nVisCode-Multi-679K is a large-scale, supervised dataset containing 679K\nvalidated and executable visualization samples with multi-turn correction\ndialogues across 12 programming languages. VisPlotBench is a benchmark for\nsystematic evaluation, featuring executable tasks, rendered outputs, and\nprotocols for both initial generation and multi-round self-debug. Finally, we\npresent VisCoder2, a family of multi-language visualization models trained on\nVisCode-Multi-679K. Experiments show that VisCoder2 significantly outperforms\nstrong open-source baselines and approaches the performance of proprietary\nmodels like GPT-4.1, with further gains from iterative self-debug, reaching\n82.4% overall execution pass rate at the 32B scale, particularly in symbolic or\ncompiler-dependent languages.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u9488\u5bf9\u53ef\u89c6\u5316\u7f16\u7801\u4ee3\u7406\u5b58\u5728\u7684\u8bed\u8a00\u8986\u76d6\u6709\u9650\u3001\u6267\u884c\u4e0d\u53ef\u9760\u53ca\u7f3a\u5c11\u8fed\u4ee3\u4fee\u6b63\u673a\u5236\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u6784\u5efa\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u6570\u636e\u96c6VisCode-Multi-679K\u3001\u5efa\u7acb\u8bc4\u6d4b\u57fa\u51c6VisPlotBench\u4ee5\u53ca\u63d0\u51fa\u591a\u8bed\u8a00\u6a21\u578bVisCoder2\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u548c\u6267\u884c\u53ef\u89c6\u5316\u4ee3\u7801\u65f6\u5b58\u5728\u591a\u79cd\u5c40\u9650\uff0c\u5305\u62ec\u8bed\u8a00\u8986\u76d6\u8303\u56f4\u72ed\u7a84\u3001\u6267\u884c\u7ed3\u679c\u4e0d\u7a33\u5b9a\u3001\u7f3a\u5c11\u591a\u8f6e\u7ea0\u6b63\u673a\u5236\uff0c\u5e76\u4e14\u73b0\u6709\u6570\u636e\u96c6\u4e0e\u8bc4\u6d4b\u591a\u805a\u7126\u5355\u8f6e\u6216\u5355\u8bed\u8a00\u4efb\u52a1\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5b9e\u7528\u6027\u548c\u8fdb\u6b65\u3002", "method": "\u6784\u5efa\u5305\u542b679K\u7ecf\u9a8c\u8bc1\u4e14\u53ef\u6267\u884c\u53ef\u89c6\u5316\u6837\u4f8b\u53ca\u591a\u8f6e\u4fee\u6b63\u5bf9\u8bdd\u7684\u591a\u8bed\u8a00\u5927\u89c4\u6a21\u6570\u636e\u96c6VisCode-Multi-679K\uff1b\u8bbe\u8ba1\u7cfb\u7edf\u5316\u8bc4\u6d4b\u57fa\u51c6VisPlotBench\uff0c\u652f\u6301\u521d\u59cb\u751f\u6210\u53ca\u591a\u8f6e\u81ea\u6211\u8c03\u8bd5\u7684\u8bc4\u4f30\uff1b\u63d0\u51fa\u57fa\u4e8e\u8be5\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u591a\u8bed\u8a00\u53ef\u89c6\u5316\u6a21\u578b\u7cfb\u5217VisCoder2\u3002", "result": "VisCoder2\u5728\u591a\u4e2a\u7f16\u7a0b\u8bed\u8a00\u548c\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5f3a\u5f00\u6e90\u57fa\u7ebf\uff0c\u6027\u80fd\u63a5\u8fd1GPT-4\uff0c\u7279\u522b\u5728\u7b26\u53f7\u6216\u4f9d\u8d56\u7f16\u8bd1\u5668\u8bed\u8a00\u4e2d\u8868\u73b0\u7a81\u51fa\u3002\u901a\u8fc7\u591a\u8f6e\u81ea\u6211\u8c03\u8bd5\u7b56\u7565\uff0c32B\u53c2\u6570\u89c4\u6a21\u6a21\u578b\u8fbe\u523082.4%\u7684\u6267\u884c\u901a\u8fc7\u7387\u3002", "conclusion": "\u7ed3\u5408\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u6570\u636e\u96c6\u4e0e\u7cfb\u7edf\u5316\u8bc4\u6d4b\uff0c\u914d\u5408\u591a\u8f6e\u81ea\u6211\u4fee\u6b63\u673a\u5236\uff0c\u63d0\u5347\u4e86\u53ef\u89c6\u5316\u7f16\u7801\u6a21\u578b\u7684\u6267\u884c\u80fd\u529b\u4e0e\u6cdb\u5316\u6027\u80fd\uff0c\u63a8\u52a8\u4e86\u591a\u8bed\u8a00\u53ef\u89c6\u5316\u751f\u6210\u4e0e\u8c03\u8bd5\u4ee3\u7406\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.23664", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23664", "abs": "https://arxiv.org/abs/2510.23664", "authors": ["Eranga Bandara", "Ross Gore", "Xueping Liang", "Sachini Rajapakse", "Isurunima Kularathne", "Pramoda Karunarathna", "Peter Foytik", "Sachin Shetty", "Ravi Mukkamala", "Abdul Rahman", "Amin Hass", "Ng Wee Keong", "Kasun De Zoysa", "Aruna Withanage", "Nilaan Loganathan"], "title": "Agentsway -- Software Development Methodology for AI Agents-based Teams", "comment": null, "summary": "The emergence of Agentic AI is fundamentally transforming how software is\ndesigned, developed, and maintained. Traditional software development\nmethodologies such as Agile, Kanban, ShapeUp, etc, were originally designed for\nhuman-centric teams and are increasingly inadequate in environments where\nautonomous AI agents contribute to planning, coding, testing, and continuous\nlearning. To address this methodological gap, we present \"Agentsway\" a novel\nsoftware development framework designed for ecosystems where AI agents operate\nas first-class collaborators. Agentsway introduces a structured lifecycle\ncentered on human orchestration, and privacy-preserving collaboration among\nspecialized AI agents. The framework defines distinct roles for planning,\nprompting, coding, testing, and fine-tuning agents, each contributing to\niterative improvement and adaptive learning throughout the development process.\nBy integrating fine-tuned LLMs that leverage outputs and feedback from\ndifferent agents throughout the development cycle as part of a retrospective\nlearning process, Agentsway enhances domain-specific reasoning, and explainable\ndecision-making across the entire software development lifecycle. Responsible\nAI principles are further embedded across the agents through the coordinated\nuse of multiple fine-tuned LLMs and advanced reasoning models, ensuring\nbalanced, transparent, and accountable decision-making. This work advances\nsoftware engineering by formalizing agent-centric collaboration, integrating\nprivacy-by-design principles, and defining measurable metrics for productivity\nand trust. Agentsway represents a foundational step toward the next generation\nof AI-native, self-improving software development methodologies. To the best of\nour knowledge, this is the first research effort to introduce a dedicated\nmethodology explicitly designed for AI agent-based software engineering teams.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Agentsway\uff0c\u4e00\u79cd\u4e13\u4e3aAI\u4ee3\u7406\u534f\u4f5c\u8bbe\u8ba1\u7684\u8f6f\u4ef6\u5f00\u53d1\u6846\u67b6\uff0c\u901a\u8fc7\u89d2\u8272\u5206\u5de5\u548c\u9690\u79c1\u4fdd\u62a4\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u5b66\u4e60\u548c\u8d1f\u8d23\u4efb\u7684AI\u51b3\u7b56\uff0c\u63a8\u52a8AI\u539f\u751f\u7684\u8f6f\u4ef6\u5f00\u53d1\u65b9\u6cd5\u5b66\u3002", "motivation": "\u4f20\u7edf\u7684\u8f6f\u4ef6\u5f00\u53d1\u65b9\u6cd5\u9488\u5bf9\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u56e2\u961f\u8bbe\u8ba1\uff0c\u65e0\u6cd5\u9002\u5e94\u81ea\u4e3bAI\u4ee3\u7406\u53c2\u4e0e\u7684\u8f6f\u4ef6\u5f00\u53d1\u73af\u5883\uff0c\u4e9f\u9700\u65b0\u7684\u65b9\u6cd5\u5b66\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "Agentsway\u6846\u67b6\u5b9a\u4e49\u4e86\u8ba1\u5212\u3001\u63d0\u793a\u3001\u7f16\u7801\u3001\u6d4b\u8bd5\u548c\u5fae\u8c03\u7b49AI\u4ee3\u7406\u89d2\u8272\uff0c\u91c7\u7528\u7ec6\u5316\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u4e92\u76f8\u53cd\u9988\uff0c\u5f62\u6210\u95ed\u73af\u7684\u8fed\u4ee3\u4e0e\u5b66\u4e60\u8fc7\u7a0b\uff0c\u5e76\u5d4c\u5165\u9690\u79c1\u8bbe\u8ba1\u548c\u591a\u6a21\u578b\u534f\u8c03\u786e\u4fdd\u8d1f\u8d23\u4efb\u51b3\u7b56\u3002", "result": "Agentsway\u5b9e\u73b0\u4e86\u57fa\u4e8eAI\u4ee3\u7406\u534f\u4f5c\u7684\u8f6f\u4ef6\u5f00\u53d1\u89c4\u8303\uff0c\u63d0\u5347\u4e86\u9886\u57df\u63a8\u7406\u80fd\u529b\u548c\u51b3\u7b56\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5efa\u7acb\u4e86\u751f\u4ea7\u529b\u548c\u4fe1\u4efb\u7684\u53ef\u5ea6\u91cf\u6807\u51c6\uff0c\u6807\u5fd7\u7740\u8f6f\u4ef6\u5de5\u7a0b\u65b9\u6cd5\u7684\u521b\u65b0\u7a81\u7834\u3002", "conclusion": "Agentsway\u4e3aAI\u4ee3\u7406\u9a71\u52a8\u7684\u8f6f\u4ef6\u5de5\u7a0b\u56e2\u961f\u63d0\u51fa\u4e86\u9996\u4e2a\u4e13\u95e8\u65b9\u6cd5\u5b66\uff0c\u5f00\u542f\u4e86AI\u81ea\u4e3b\u6539\u8fdb\u548c\u539f\u751f\u534f\u4f5c\u7684\u8f6f\u4ef6\u5f00\u53d1\u65b0\u65f6\u4ee3\u3002"}}
{"id": "2510.23674", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.23674", "abs": "https://arxiv.org/abs/2510.23674", "authors": ["Bin Wang", "Hui Li", "AoFan Liu", "BoTao Yang", "Ao Yang", "YiLu Zhong", "Weixiang Huang", "Yanping Zhang", "Runhuai Huang", "Weimin Zeng"], "title": "RefleXGen:The unexamined code is not worth using", "comment": null, "summary": "Security in code generation remains a pivotal challenge when applying large\nlanguage models (LLMs). This paper introduces RefleXGen, an innovative method\nthat significantly enhances code security by integrating Retrieval-Augmented\nGeneration (RAG) techniques with guided self-reflection mechanisms inherent in\nLLMs. Unlike traditional approaches that rely on fine-tuning LLMs or developing\nspecialized secure code datasets - processes that can be resource-intensive -\nRefleXGen iteratively optimizes the code generation process through\nself-assessment and reflection without the need for extensive resources. Within\nthis framework, the model continuously accumulates and refines its knowledge\nbase, thereby progressively improving the security of the generated code.\nExperimental results demonstrate that RefleXGen substantially enhances code\nsecurity across multiple models, achieving a 13.6% improvement with GPT-3.5\nTurbo, a 6.7% improvement with GPT-4o, a 4.5% improvement with CodeQwen, and a\n5.8% improvement with Gemini. Our findings highlight that improving the quality\nof model self-reflection constitutes an effective and practical strategy for\nstrengthening the security of AI-generated code.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86RefleXGen\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u5f15\u5bfc\u81ea\u6211\u53cd\u601d\u673a\u5236\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4ee3\u7801\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u5b89\u5168\u6027\u96be\u9898\uff0c\u907f\u514d\u4f20\u7edf\u5fae\u8c03\u548c\u4e13\u7528\u5b89\u5168\u4ee3\u7801\u6570\u636e\u96c6\u7684\u8d44\u6e90\u6d88\u8017\u3002", "method": "\u5f15\u5165RefleXGen\uff0c\u4f7f\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\u548c\u6a21\u578b\u81ea\u6211\u8bc4\u4f30\u3001\u53cd\u601d\u673a\u5236\uff0c\u8fed\u4ee3\u4f18\u5316\u751f\u6210\u4ee3\u7801\uff0c\u6301\u7eed\u79ef\u7d2f\u548c\u4f18\u5316\u77e5\u8bc6\u5e93\u3002", "result": "\u5b9e\u9a8c\u663e\u793aRefleXGen\u5728\u591a\u79cd\u6a21\u578b\u4e2d\u63d0\u5347\u4ee3\u7801\u5b89\u5168\u6027\uff0c\u5982GPT-3.5 Turbo\u63d0\u534713.6%\uff0cGPT-4o\u63d0\u53476.7%\uff0cCodeQwen\u63d0\u53474.5%\uff0cGemini\u63d0\u53475.8%\u3002", "conclusion": "\u63d0\u5347\u6a21\u578b\u81ea\u6211\u53cd\u601d\u8d28\u91cf\u662f\u589e\u5f3aAI\u751f\u6210\u4ee3\u7801\u5b89\u5168\u6027\u7684\u6709\u6548\u4e14\u5b9e\u7528\u7b56\u7565\u3002"}}
{"id": "2510.23615", "categories": ["cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23615", "abs": "https://arxiv.org/abs/2510.23615", "authors": ["Nishant Doshi"], "title": "Logic-based Task Representation and Reward Shaping in Multiagent Reinforcement Learning", "comment": null, "summary": "This paper presents an approach for accelerated learning of optimal plans for\na given task represented using Linear Temporal Logic (LTL) in multi-agent\nsystems. Given a set of options (temporally abstract actions) available to each\nagent, we convert the task specification into the corresponding Buchi Automaton\nand proceed with a model-free approach which collects transition samples and\nconstructs a product Semi Markov Decision Process (SMDP) on-the-fly.\nValue-based Reinforcement Learning algorithms can then be used to synthesize a\ncorrect-by-design controller without learning the underlying transition model\nof the multi-agent system. The exponential sample complexity due to multiple\nagents is dealt with using a novel reward shaping approach. We test the\nproposed algorithm in a deterministic gridworld simulation for different tasks\nand find that the reward shaping results in significant reduction in\nconvergence times. We also infer that using options becomes increasing more\nrelevant as the state and action space increases in multi-agent systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\u4efb\u52a1\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u52a0\u901f\u5b66\u4e60\u6700\u4f18\u8ba1\u5212\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efaBuchi\u81ea\u52a8\u673a\u548c\u534a\u9a6c\u5c14\u79d1\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u7ed3\u5408\u5956\u52b1\u5851\u9020\u6280\u672f\u663e\u8457\u63d0\u5347\u5b66\u4e60\u6548\u7387\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u4efb\u52a1\u590d\u6742\u4e14\u72b6\u6001\u7a7a\u95f4\u5927\uff0c\u5c24\u5176\u4f7f\u7528\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\u63cf\u8ff0\u4efb\u52a1\u65f6\uff0c\u5b66\u4e60\u6700\u4f18\u8ba1\u5212\u7684\u6837\u672c\u590d\u6742\u5ea6\u5448\u6307\u6570\u589e\u957f\u3002", "method": "\u5c06\u4efb\u52a1\u89c4\u683c\u8f6c\u6362\u4e3aBuchi\u81ea\u52a8\u673a\uff0c\u6784\u5efa\u4ea7\u54c1\u534a\u9a6c\u5c14\u79d1\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u91c7\u7528\u65e0\u6a21\u578b\u57fa\u4e8e\u4ef7\u503c\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u5e76\u5f15\u5165\u5956\u52b1\u5851\u9020\u65b9\u6cd5\u4ee5\u964d\u4f4e\u6837\u672c\u590d\u6742\u5ea6\u3002", "result": "\u5728\u786e\u5b9a\u6027\u7f51\u683c\u4e16\u754c\u4e2d\u591a\u4e2a\u4efb\u52a1\u6d4b\u8bd5\u7ed3\u679c\u8868\u660e\uff0c\u5956\u52b1\u5851\u9020\u663e\u8457\u51cf\u5c11\u4e86\u6536\u655b\u65f6\u95f4\uff0c\u540c\u65f6\u9009\u9879\u673a\u5236\u5728\u5927\u89c4\u6a21\u72b6\u6001\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u66f4\u663e\u91cd\u8981\u6027\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u65f6\u5e8f\u903b\u8f91\u4efb\u52a1\u8868\u793a\u3001\u9009\u9879\u548c\u5956\u52b1\u5851\u9020\uff0c\u6210\u529f\u52a0\u901f\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u6700\u4f18\u8ba1\u5212\u5b66\u4e60\uff0c\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u5b9e\u7528\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2510.23730", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23730", "abs": "https://arxiv.org/abs/2510.23730", "authors": ["Alessandra Terranova", "Bj\u00f6rn Ross", "Alexandra Birch"], "title": "Evaluating Long-Term Memory for Long-Context Question Answering", "comment": "14 pages including appendix, 3 figures. Submitted to October ARR and\n  to Metacognition in Generative AI EurIPS workshop (under review for both)", "summary": "In order for large language models to achieve true conversational continuity\nand benefit from experiential learning, they need memory. While research has\nfocused on the development of complex memory systems, it remains unclear which\ntypes of memory are most effective for long-context conversational tasks. We\npresent a systematic evaluation of memory-augmented methods using LoCoMo, a\nbenchmark of synthetic long-context dialogues annotated for question-answering\ntasks that require diverse reasoning strategies. We analyse full-context\nprompting, semantic memory through retrieval-augmented generation and agentic\nmemory, episodic memory through in-context learning, and procedural memory\nthrough prompt optimization. Our findings show that memory-augmented approaches\nreduce token usage by over 90% while maintaining competitive accuracy. Memory\narchitecture complexity should scale with model capability, with small\nfoundation models benefitting most from RAG, and strong instruction-tuned\nreasoning model gaining from episodic learning through reflections and more\ncomplex agentic semantic memory. In particular, episodic memory can help LLMs\nrecognise the limits of their own knowledge.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc4\u4f30\u4e86\u591a\u79cd\u8bb0\u5fc6\u589e\u5f3a\u65b9\u6cd5\u5728\u957f\u4e0a\u4e0b\u6587\u5bf9\u8bdd\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5408\u7406\u7684\u8bb0\u5fc6\u67b6\u6784\u80fd\u591f\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u5e76\u7ef4\u6301\u8f83\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u4e3a\u4e86\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u771f\u6b63\u7684\u5bf9\u8bdd\u8fde\u7eed\u6027\u5e76\u4ece\u7ecf\u9a8c\u4e2d\u5b66\u4e60\uff0c\u7814\u7a76\u4e86\u4e0d\u540c\u7c7b\u578b\u7684\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u63a2\u7a76\u5b83\u4eec\u5728\u957f\u4e0a\u4e0b\u6587\u5bf9\u8bdd\u4efb\u52a1\u4e2d\u7684\u6548\u679c\u3002", "method": "\u4f7f\u7528LoCoMo\u57fa\u51c6\u5bf9\u591a\u79cd\u8bb0\u5fc6\u589e\u5f3a\u65b9\u6cd5\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\uff0c\u5305\u62ec\u5168\u4e0a\u4e0b\u6587\u63d0\u793a\u3001\u57fa\u4e8e\u68c0\u7d22\u7684\u8bed\u4e49\u8bb0\u5fc6\u3001\u60c5\u666f\u8bb0\u5fc6\uff08\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\uff09\u548c\u7a0b\u5e8f\u8bb0\u5fc6\uff08\u901a\u8fc7\u63d0\u793a\u4f18\u5316\uff09\u3002", "result": "\u8bb0\u5fc6\u589e\u5f3a\u65b9\u6cd5\u80fd\u591f\u51cf\u5c1190%\u4ee5\u4e0a\u7684token\u4f7f\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u51c6\u786e\u7387\u3002\u5c0f\u578b\u57fa\u7840\u6a21\u578b\u4ece\u57fa\u4e8e\u68c0\u7d22\u751f\u6210\uff08RAG\uff09\u53d7\u76ca\u6700\u5927\uff0c\u800c\u5f3a\u5927\u7684\u63a8\u7406\u6a21\u578b\u5219\u901a\u8fc7\u60c5\u666f\u5b66\u4e60\u548c\u66f4\u590d\u6742\u7684\u8bed\u4e49\u8bb0\u5fc6\u83b7\u5f97\u63d0\u5347\u3002", "conclusion": "\u8bb0\u5fc6\u67b6\u6784\u7684\u590d\u6742\u6027\u5e94\u4e0e\u6a21\u578b\u80fd\u529b\u5339\u914d\uff0c\u4e0d\u540c\u89c4\u6a21\u548c\u80fd\u529b\u7684\u6a21\u578b\u5e94\u91c7\u7528\u4e0d\u540c\u7684\u8bb0\u5fc6\u7cfb\u7edf\u4ee5\u5b9e\u73b0\u6700\u4f18\u6027\u80fd\uff0c\u60c5\u666f\u8bb0\u5fc6\u7279\u522b\u6709\u52a9\u4e8e\u6a21\u578b\u8ba4\u8bc6\u81ea\u8eab\u77e5\u8bc6\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.23761", "categories": ["cs.SE", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.23761", "abs": "https://arxiv.org/abs/2510.23761", "authors": ["Kevin Han", "Siddharth Maddikayala", "Tim Knappe", "Om Patel", "Austen Liao", "Amir Barati Farimani"], "title": "TDFlow: Agentic Workflows for Test Driven Software Engineering", "comment": null, "summary": "We introduce TDFlow, a novel test-driven agentic workflow that frames\nrepository-scale software engineering as a test-resolution task, specifically\ndesigned to solve human-written tests. Given a set of tests, TDFlow repeatedly\nproposes, revises, and debugs repository-scale patches using precisely\nengineered sub-agents and tightly constrained tools. The workflow decomposes\nsoftware engineering program repair into four components governed by respective\nsub-agents. This simple, forced decoupling of patch proposing, debugging, patch\nrevision, and optional test generation (1) reduces long-context burden on any\nindividual sub-agent, (2) focuses each sub-agent on specific, pre-defined\nsub-tasks, and (3) allows for specialized performance improvement on specific\nsub-tasks. When provided human-written tests, TDFlow attains 88.8% pass rate on\nSWE-Bench Lite (an absolute improvement of 27.8% over the next best system) and\n94.3% on SWE-Bench Verified. Manual inspection of the 800 TDFlow runs within\nSWE-Bench Lite and Verified uncover only 7 instances of test hacking, which\nwere subsequently counted as failures. Furthermore, we show that the primary\nobstacle to human-level software engineering performance lies within writing\nsuccessful reproduction tests. We envision a human-LLM interactive system\npowered by TDFlow where human developers write tests solved by LLM systems.\nTogether, these results indicate that modern LLMs, when embedded in a narrowly\nengineered, test-driven workflow, already achieve human-level test resolution\n-- with the final frontier for fully autonomous repository repair being the\naccurate generation of valid reproduction tests.", "AI": {"tldr": "TDFlow \u662f\u4e00\u79cd\u521b\u65b0\u7684\u6d4b\u8bd5\u9a71\u52a8\u667a\u80fd\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5c06\u5927\u89c4\u6a21\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u8f6c\u5316\u4e3a\u901a\u8fc7\u6d4b\u8bd5\u89e3\u51b3\u7684\u8865\u4e01\u751f\u6210\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4fee\u590d\u6548\u7387\u548c\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u8f6f\u4ef6\u5de5\u7a0b\u4fee\u590d\u65b9\u6cd5\u96be\u4ee5\u9ad8\u6548\u89e3\u51b3\u590d\u6742\u4eba\u7c7b\u7f16\u5199\u7684\u6d4b\u8bd5\uff0c\u4e14\u5355\u4e00\u667a\u80fd\u4f53\u627f\u62c5\u8fc7\u591a\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5f71\u54cd\u6027\u80fd\u3002", "method": "\u8bbe\u8ba1\u4e86\u5206\u4e3a\u8865\u4e01\u63d0\u51fa\u3001\u8c03\u8bd5\u3001\u4fee\u8ba2\u548c\u6d4b\u8bd5\u751f\u6210\u56db\u4e2a\u5b50\u4ee3\u7406\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u964d\u4f4e\u5404\u5b50\u4ee3\u7406\u7684\u4efb\u52a1\u590d\u6742\u5ea6\u5e76\u5b9e\u73b0\u4e13\u95e8\u5316\u4f18\u5316\u3002", "result": "TDFlow \u5728 SWE-Bench Lite \u548c Verified \u6d4b\u8bd5\u96c6\u4e0a\u5206\u522b\u8fbe\u523088.8%\u548c94.3%\u7684\u6d4b\u8bd5\u901a\u8fc7\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u7cfb\u7edf27.8%\u3002\u5e76\u4e14\u624b\u52a8\u68c0\u67e5\u53d1\u73b0\u6781\u4f4e\u7684\u6d4b\u8bd5\u7be1\u6539\u60c5\u51b5\u3002", "conclusion": "\u5d4c\u5165\u7cbe\u7ec6\u5de5\u7a0b\u5316\u7684\u6d4b\u8bd5\u9a71\u52a8\u6d41\u7a0b\u4e2d\u7684\u73b0\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u5df2\u8fbe\u5230\u4eba\u7c7b\u6c34\u5e73\u7684\u6d4b\u8bd5\u4fee\u590d\u80fd\u529b\uff0c\u672a\u6765\u5173\u952e\u5728\u4e8e\u51c6\u786e\u751f\u6210\u6709\u6548\u7684\u590d\u73b0\u6d4b\u8bd5\u3002"}}
{"id": "2510.23899", "categories": ["cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23899", "abs": "https://arxiv.org/abs/2510.23899", "authors": ["Maria G. Mendoza", "Addison Kalanther", "Daniel Bostwick", "Emma Stephan", "Chinmay Maheshwari", "Shankar Sastry"], "title": "Coordinated Autonomous Drones for Human-Centered Fire Evacuation in Partially Observable Urban Environments", "comment": "Accepted to IEEE Global Humanitarian Technology Conference (GHTC\n  2025). 8 pages, 4 figures", "summary": "Autonomous drone technology holds significant promise for enhancing search\nand rescue operations during evacuations by guiding humans toward safety and\nsupporting broader emergency response efforts. However, their application in\ndynamic, real-time evacuation support remains limited. Existing models often\noverlook the psychological and emotional complexity of human behavior under\nextreme stress. In real-world fire scenarios, evacuees frequently deviate from\ndesignated safe routes due to panic and uncertainty. To address these\nchallenges, this paper presents a multi-agent coordination framework in which\nautonomous Unmanned Aerial Vehicles (UAVs) assist human evacuees in real-time\nby locating, intercepting, and guiding them to safety under uncertain\nconditions. We model the problem as a Partially Observable Markov Decision\nProcess (POMDP), where two heterogeneous UAV agents, a high-level rescuer (HLR)\nand a low-level rescuer (LLR), coordinate through shared observations and\ncomplementary capabilities. Human behavior is captured using an agent-based\nmodel grounded in empirical psychology, where panic dynamically affects\ndecision-making and movement in response to environmental stimuli. The\nenvironment features stochastic fire spread, unknown evacuee locations, and\nlimited visibility, requiring UAVs to plan over long horizons to search for\nhumans and adapt in real-time. Our framework employs the Proximal Policy\nOptimization (PPO) algorithm with recurrent policies to enable robust\ndecision-making in partially observable settings. Simulation results\ndemonstrate that the UAV team can rapidly locate and intercept evacuees,\nsignificantly reducing the time required for them to reach safety compared to\nscenarios without UAV assistance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u5229\u7528\u591a\u667a\u80fd\u4f53\u65e0\u4eba\u673a\u534f\u8c03\u6846\u67b6\uff0c\u5b9e\u65f6\u8f85\u52a9\u706b\u707e\u4e2d\u7684\u4eba\u5458\u758f\u6563\uff0c\u901a\u8fc7\u5b9a\u4f4d\u3001\u62e6\u622a\u5e76\u5f15\u5bfc\u4eba\u5458\u5b89\u5168\u64a4\u79bb\uff0c\u6709\u6548\u5e94\u5bf9\u9003\u751f\u8fc7\u7a0b\u4e2d\u56e0\u6050\u614c\u548c\u4e0d\u786e\u5b9a\u6027\u5bfc\u81f4\u7684\u884c\u4e3a\u504f\u79bb\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u65e0\u4eba\u673a\u8f85\u52a9\u758f\u6563\u7cfb\u7edf\u5f80\u5f80\u5ffd\u7565\u4e86\u6781\u7aef\u5e94\u6fc0\u4e0b\u4eba\u7c7b\u884c\u4e3a\u7684\u5fc3\u7406\u548c\u60c5\u611f\u590d\u6742\u6027\uff0c\u5b9e\u9645\u706b\u707e\u9003\u751f\u4e2d\u4eba\u5458\u5e38\u56e0\u6050\u614c\u504f\u79bb\u5b89\u5168\u8def\u5f84\uff0c\u6025\u9700\u80fd\u591f\u9002\u5e94\u52a8\u6001\u73af\u5883\u7684\u5b9e\u65f6\u8f85\u52a9\u7cfb\u7edf\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u8bbe\u8ba1\u4e24\u7c7b\u5f02\u6784\u65e0\u4eba\u673a\uff08\u9ad8\u5c42\u6551\u63f4\u8005\u548c\u4f4e\u5c42\u6551\u63f4\u8005\uff09\u534f\u4f5c\u6846\u67b6\uff0c\u7ed3\u5408\u57fa\u4e8e\u5fc3\u7406\u5b66\u7684\u4ee3\u7406\u4eba\u6a21\u578b\u6a21\u62df\u4eba\u7684\u6050\u614c\u884c\u4e3a\uff0c\u5229\u7528\u57fa\u4e8ePPO\u7b97\u6cd5\u7684\u9012\u5f52\u7b56\u7565\u5b9e\u73b0\u957f\u671f\u89c4\u5212\u548c\u52a8\u6001\u9002\u5e94\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u534f\u4f5c\u65e0\u4eba\u673a\u56e2\u961f\u80fd\u5feb\u901f\u5b9a\u4f4d\u5e76\u62e6\u622a\u53d7\u56f0\u4eba\u5458\uff0c\u663e\u8457\u7f29\u77ed\u4eba\u5458\u5b89\u5168\u64a4\u79bb\u6240\u9700\u65f6\u95f4\uff0c\u76f8\u8f83\u65e0\u65e0\u4eba\u673a\u8f85\u52a9\u65b9\u6848\u6548\u679c\u4f18\u8d8a\u3002", "conclusion": "\u6240\u63d0\u591a\u667a\u80fd\u4f53\u65e0\u4eba\u673a\u534f\u8c03\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u706b\u707e\u7d27\u6025\u758f\u6563\u7684\u6548\u7387\u548c\u5b89\u5168\u6027\uff0c\u4e3a\u52a8\u6001\u590d\u6742\u73af\u5883\u4e0b\u7684\u65e0\u4eba\u673a\u5e94\u6025\u8f85\u52a9\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6cd5\u548c\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2510.23766", "categories": ["cs.CL", "68T05", "I.2.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.23766", "abs": "https://arxiv.org/abs/2510.23766", "authors": ["Ramshankar Bhuvaneswaran", "Handan Liu"], "title": "BitSkip: An Empirical Analysis of Quantization and Early Exit Composition", "comment": "Submitted to JMLR", "summary": "The pursuit of efficient Large Language Models (LLMs) has led to increasingly\ncomplex techniques like extreme quantization and dynamic routing. While\nindividual benefits of these methods are well-documented, their compositional\neffects remain poorly understood. This paper introduces BitSkip, a hybrid\narchitectural framework for systematically exploring these interactions.\nCounter-intuitively, our findings reveal that a simple 8-bit quantized model\nwithout Hadamard transform (BitSkip-V1) not only outperforms its more complex\n4-bit and Hadamard-enhanced counterparts but also competes the full-precision\nbaseline in quality (perplexity of 1.13 vs 1.19) . The introduction of Hadamard\ntransforms, even at 8-bit precision, catastrophically degraded performance by\nover 37,000%, tracing fundamental training instability. Our BitSkip-V1 recipe\ndemonstrates superior early-exit characteristics, with layer 18 providing\noptimal 32.5% speed gain for minimal 4% quality loss.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86BitSkip\u6846\u67b6\uff0c\u7814\u7a76\u6781\u7aef\u91cf\u5316\u548c\u52a8\u6001\u8def\u7531\u6280\u672f\u7684\u590d\u5408\u6548\u679c\uff0c\u53d1\u73b0\u7b80\u5355\u76848\u4f4d\u91cf\u5316\u6a21\u578b\u6027\u80fd\u4f18\u4e8e\u590d\u67424\u4f4d\u6a21\u578b\uff0c\u4e14\u63a5\u8fd1\u5168\u7cbe\u5ea6\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u9ad8\u6548\u5927\u8bed\u8a00\u6a21\u578b\u6280\u672f\u590d\u6742\u4e14\u7ec4\u5408\u6548\u679c\u4e0d\u660e\uff0c\u672c\u7814\u7a76\u63a2\u7d22\u5176\u4ea4\u4e92\u4f5c\u7528\u3002", "method": "\u8bbe\u8ba1BitSkip\u6df7\u5408\u67b6\u6784\uff0c\u7cfb\u7edf\u5730\u8bc4\u4f30\u4e0d\u540c\u91cf\u5316\u4f4d\u6570\u548cHadamard\u53d8\u6362\u7684\u5f71\u54cd\u3002", "result": "8\u4f4d\u65e0Hadamard\u53d8\u6362\u6a21\u578b\uff08BitSkip-V1\uff09\u6027\u80fd\u6700\u4f73\uff0c\u8d85\u8d8a\u590d\u6742\u65b9\u6848\u4e14\u6027\u80fd\u63a5\u8fd1\u5168\u7cbe\u5ea6\u3002Hadamard\u53d8\u6362\u53cd\u800c\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u6027\u80fd\u5927\u5e45\u4e0b\u964d\u3002", "conclusion": "\u7b80\u5316\u91cf\u5316\u8bbe\u8ba1\u6709\u5229\u4e8e\u6a21\u578b\u6027\u80fd\u548c\u7a33\u5b9a\u6027\uff0cBitSkip-V1\u5b9e\u73b0\u4e86\u901f\u5ea6\u4e0e\u8d28\u91cf\u7684\u6700\u4f73\u5e73\u8861\u3002"}}
{"id": "2510.23893", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23893", "abs": "https://arxiv.org/abs/2510.23893", "authors": ["Rodrigo Falc\u00e3o", "Stefan Schweitzer", "Julien Siebert", "Emily Calvet", "Frank Elberzhager"], "title": "Evaluating the effectiveness of LLM-based interoperability", "comment": null, "summary": "Background: Systems of systems are becoming increasingly dynamic and\nheterogeneous, and this adds pressure on the long-standing challenge of\ninteroperability. Besides its technical aspect, interoperability has also an\neconomic side, as development time efforts are required to build the\ninteroperability artifacts. Objectives: With the recent advances in the field\nof large language models (LLMs), we aim at analyzing the effectiveness of\nLLM-based strategies to make systems interoperate autonomously, at runtime,\nwithout human intervention. Method: We selected 13 open source LLMs and curated\nfour versions of a dataset in the agricultural interoperability use case. We\nperformed three runs of each model with each version of the dataset, using two\ndifferent strategies. Then we compared the effectiveness of the models and the\nconsistency of their results across multiple runs. Results: qwen2.5-coder:32b\nwas the most effective model using both strategies DIRECT (average pass@1 >=\n0.99) and CODEGEN (average pass@1 >= 0.89) in three out of four dataset\nversions. In the fourth dataset version, which included an unit conversion, all\nmodels using the strategy DIRECT failed, whereas using CODEGEN\nqwen2.5-coder:32b succeeded with an average pass@1 = 0.75. Conclusion: Some\nLLMs can make systems interoperate autonomously. Further evaluation in\ndifferent domains is recommended, and further research on reliability\nstrategies should be conducted.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b(LLMs)\u5b9e\u73b0\u7cfb\u7edf\u81ea\u4e3b\u7ba1\u7406\u4e92\u64cd\u4f5c\u6027\u7684\u6548\u679c\uff0c\u9009\u53d613\u4e2a\u5f00\u6e90\u6a21\u578b\uff0c\u57fa\u4e8e\u519c\u4e1a\u4e92\u64cd\u4f5c\u6027\u6570\u636e\u96c6\u8fdb\u884c\u6d4b\u8bd5\uff0cqwen2.5-coder:32b\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u52a8\u6001\u5f02\u6784\u7684\u7cfb\u7edf\u73af\u5883\u5bf9\u4e92\u64cd\u4f5c\u6027\u63d0\u51fa\u4e86\u66f4\u9ad8\u8981\u6c42\uff0c\u4e14\u5f00\u53d1\u4e92\u64cd\u4f5c\u6027\u5de5\u5177\u6210\u672c\u9ad8\u3002\u672c\u6587\u65e8\u5728\u8bc4\u4f30\u5229\u7528LLMs\u5728\u8fd0\u884c\u65f6\u5b9e\u73b0\u7cfb\u7edf\u81ea\u4e3b\u7ba1\u7406\u4e92\u64cd\u4f5c\u7684\u53ef\u884c\u6027\u3002", "method": "\u6d4b\u8bd513\u4e2a\u5f00\u6e90LLMs\uff0c\u57fa\u4e8e\u519c\u4e1a\u4e92\u64cd\u4f5c\u6027\u56db\u4e2a\u7248\u672c\u7684\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u4e24\u79cd\u7b56\u7565\uff08DIRECT\u548cCODEGEN\uff09\uff0c\u591a\u6b21\u8fd0\u884c\u5e76\u6bd4\u8f83\u6a21\u578b\u6548\u679c\u4e0e\u7ed3\u679c\u4e00\u81f4\u6027\u3002", "result": "qwen2.5-coder:32b\u5728DIRECT\u548cCODEGEN\u7b56\u7565\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u524d\u4e09\u4e2a\u6570\u636e\u96c6\u7248\u672c\u7684\u5e73\u5747pass@1\u5747\u9ad8\u4e8e0.89\uff0c\u5728\u6d89\u53ca\u5355\u4f4d\u8f6c\u6362\u7684\u7248\u672c\u4e2d\uff0cDIRECT\u7b56\u7565\u5931\u8d25\uff0cCODEGEN\u7b56\u7565\u4ecd\u8fbe0.75\u3002", "conclusion": "\u90e8\u5206LLMs\u5177\u5907\u81ea\u4e3b\u7ba1\u7406\u7cfb\u7edf\u4e92\u64cd\u4f5c\u6027\u80fd\u529b\uff0c\u5efa\u8bae\u5728\u4e0d\u540c\u9886\u57df\u7ee7\u7eed\u8bc4\u4f30\u5e76\u52a0\u5f3a\u53ef\u9760\u6027\u7b56\u7565\u7814\u7a76\u3002"}}
{"id": "2510.24030", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2510.24030", "abs": "https://arxiv.org/abs/2510.24030", "authors": ["Ahmet Akkaya Melih", "Yamuna Singh", "Kunal L. Agarwal", "Priya Mukherjee", "Kiran Pattnaik", "Hanuman Bhatia"], "title": "Human Machine Social Hybrid Intelligence:A Collaborative Decision Making Framework for Large Model Agent Groups and Human Experts", "comment": null, "summary": "The rapid advancements in large foundation models and multi-agent systems\noffer unprecedented capabilities, yet current Human-in-the-Loop (HiTL)\nparadigms inadequately integrate human expertise, often leading to cognitive\noverload and decision-making bottlenecks in complex, high-stakes environments.\nWe propose the \"Human-Machine Social Hybrid Intelligence\" (HMS-HI) framework, a\nnovel architecture designed for deep, collaborative decision-making between\ngroups of human experts and LLM-powered AI agents. HMS-HI is built upon three\ncore pillars: (1) a \\textbf{Shared Cognitive Space (SCS)} for unified,\nmulti-modal situational awareness and structured world modeling; (2) a\n\\textbf{Dynamic Role and Task Allocation (DRTA)} module that adaptively assigns\ntasks to the most suitable agent (human or AI) based on capabilities and\nworkload; and (3) a \\textbf{Cross-Species Trust Calibration (CSTC)} protocol\nthat fosters transparency, accountability, and mutual adaptation through\nexplainable declarations and structured feedback. Validated in a high-fidelity\nurban emergency response simulation, HMS-HI significantly reduced civilian\ncasualties by 72\\% and cognitive load by 70\\% compared to traditional HiTL\napproaches, demonstrating superior decision quality, efficiency, and human-AI\ntrust. An ablation study confirms the critical contribution of each module,\nhighlighting that engineered trust and shared context are foundational for\nscalable, synergistic human-AI collaboration.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a\"\u4eba\u673a\u793e\u4f1a\u6df7\u5408\u667a\u80fd\"\u7684\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u5171\u4eab\u8ba4\u77e5\u7a7a\u95f4\u3001\u52a8\u6001\u4efb\u52a1\u5206\u914d\u548c\u8de8\u7269\u79cd\u4fe1\u4efb\u6821\u51c6\uff0c\u5b9e\u73b0\u4eba\u7c7b\u4e13\u5bb6\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684AI\u4ee3\u7406\u4e4b\u95f4\u7684\u6df1\u5ea6\u534f\u4f5c\u51b3\u7b56\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u5e94\u6025\u54cd\u5e94\u6a21\u62df\u4e2d\u663e\u8457\u51cf\u5c11\u4e86\u6b7b\u4ea1\u7387\u548c\u8ba4\u77e5\u8d1f\u8377\uff0c\u63d0\u5347\u4e86\u51b3\u7b56\u8d28\u91cf\u548c\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u4eba\u673a\u4ea4\u4e92\u8303\u5f0f\u672a\u80fd\u6709\u6548\u6574\u5408\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\uff0c\u5bfc\u81f4\u5728\u590d\u6742\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\u8ba4\u77e5\u8fc7\u8f7d\u548c\u51b3\u7b56\u74f6\u9888\u3002\u9700\u8981\u4e00\u4e2a\u80fd\u4fc3\u8fdb\u7fa4\u4f53\u4eba\u7c7b\u4e13\u5bb6\u4e0eAI\u4ee3\u7406\u6df1\u5165\u534f\u4f5c\u7684\u67b6\u6784\u3002", "method": "\u8bbe\u8ba1\u4e86\"\u4eba\u673a\u793e\u4f1a\u6df7\u5408\u667a\u80fd\"\u6846\u67b6\uff0c\u5305\u542b\u5171\u4eab\u8ba4\u77e5\u7a7a\u95f4\uff08\u7edf\u4e00\u591a\u6a21\u6001\u60c5\u5883\u611f\u77e5\u548c\u7ed3\u6784\u5316\u4e16\u754c\u5efa\u6a21\uff09\u3001\u52a8\u6001\u89d2\u8272\u4e0e\u4efb\u52a1\u5206\u914d\uff08\u6839\u636e\u80fd\u529b\u548c\u8d1f\u8f7d\u81ea\u9002\u5e94\u5206\u914d\u4efb\u52a1\uff09\uff0c\u4ee5\u53ca\u8de8\u7269\u79cd\u4fe1\u4efb\u6821\u51c6\u534f\u8bae\uff08\u901a\u8fc7\u53ef\u89e3\u91ca\u58f0\u660e\u548c\u7ed3\u6784\u5316\u53cd\u9988\u4fc3\u8fdb\u900f\u660e\u6027\u548c\u4e92\u9002\u5e94\uff09\u3002", "result": "\u5728\u9ad8\u4fdd\u771f\u57ce\u5e02\u5e94\u6025\u54cd\u5e94\u6a21\u62df\u4e2d\uff0c\u8be5\u6846\u67b6\u76f8\u6bd4\u4f20\u7edf\u4eba\u673a\u4ea4\u4e92\u65b9\u5f0f\uff0c\u964d\u4f4e\u4e8672%\u7684\u5e73\u6c11\u4f24\u4ea1\u548c70%\u7684\u8ba4\u77e5\u8d1f\u8377\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u51b3\u7b56\u8d28\u91cf\u3001\u6548\u7387\u548c\u4eba\u673a\u4fe1\u4efb\u5ea6\u3002\u6d88\u878d\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5404\u6a21\u5757\u7684\u91cd\u8981\u8d21\u732e\u3002", "conclusion": "\u57fa\u4e8e\u6784\u5efa\u4fe1\u4efb\u548c\u5171\u4eab\u60c5\u5883\u7684\u673a\u5236\u662f\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u9ad8\u6548\u534f\u540c\u4eba\u673a\u51b3\u7b56\u7684\u57fa\u7840\uff0c\u672c\u6587\u63d0\u51fa\u7684\u6846\u67b6\u4e3a\u672a\u6765\u590d\u6742\u9ad8\u98ce\u9669\u73af\u5883\u4e0b\u7684\u4eba\u673a\u6df1\u5ea6\u534f\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u8303\u5f0f\u3002"}}
{"id": "2510.23828", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23828", "abs": "https://arxiv.org/abs/2510.23828", "authors": ["Mena Attia", "Aashiq Muhamed", "Mai Alkhamissi", "Thamar Solorio", "Mona Diab"], "title": "Beyond Understanding: Evaluating the Pragmatic Gap in LLMs' Cultural Processing of Figurative Language", "comment": null, "summary": "We present a comprehensive evaluation of the ability of large language models\n(LLMs) to process culturally grounded language, specifically to understand and\npragmatically use figurative expressions that encode local knowledge and\ncultural nuance. Using figurative language as a proxy for cultural nuance and\nlocal knowledge, we design evaluation tasks for contextual understanding,\npragmatic use, and connotation interpretation in Arabic and English. We\nevaluate 22 open- and closed-source LLMs on Egyptian Arabic idioms,\nmultidialectal Arabic proverbs, and English proverbs. Our results show a\nconsistent hierarchy: the average accuracy for Arabic proverbs is 4.29% lower\nthan for English proverbs, and performance for Egyptian idioms is 10.28% lower\nthan for Arabic proverbs. For the pragmatic use task, accuracy drops by 14.07%\nrelative to understanding, though providing contextual idiomatic sentences\nimproves accuracy by 10.66%. Models also struggle with connotative meaning,\nreaching at most 85.58% agreement with human annotators on idioms with 100%\ninter-annotator agreement. These findings demonstrate that figurative language\nserves as an effective diagnostic for cultural reasoning: while LLMs can often\ninterpret figurative meaning, they face challenges in using it appropriately.\nTo support future research, we release Kinayat, the first dataset of Egyptian\nArabic idioms designed for both figurative understanding and pragmatic use\nevaluation.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5904\u7406\u6587\u5316\u7279\u5b9a\u8bed\u8a00\u7684\u80fd\u529b\uff0c\u5c24\u5176\u662f\u7406\u89e3\u548c\u4f7f\u7528\u5305\u542b\u5730\u65b9\u77e5\u8bc6\u7684\u6bd4\u55bb\u8868\u8fbe\u3002", "motivation": "\u6bd4\u55bb\u8bed\u8a00\u4f5c\u4e3a\u6587\u5316\u7ec6\u5fae\u5dee\u522b\u548c\u5730\u65b9\u77e5\u8bc6\u7684\u4ee3\u7406\uff0c\u7528\u4e8e\u8861\u91cf\u6a21\u578b\u7684\u6587\u5316\u63a8\u7406\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u4e86\u963f\u62c9\u4f2f\u8bed\u548c\u82f1\u8bed\u7684\u6bd4\u55bb\u8bed\u8a00\u7406\u89e3\u3001\u8bed\u7528\u548c\u5185\u6db5\u89e3\u91ca\u4efb\u52a1\uff0c\u8bc4\u4f30\u4e8622\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u57c3\u53ca\u963f\u62c9\u4f2f\u8bed\u6210\u8bed\u3001\u591a\u65b9\u8a00\u963f\u62c9\u4f2f\u8c1a\u8bed\u548c\u82f1\u8bed\u8c1a\u8bed\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u53d1\u73b0\u963f\u62c9\u4f2f\u8c1a\u8bed\u7684\u51c6\u786e\u7387\u6bd4\u82f1\u8bed\u8c1a\u8bed\u4f4e4.29%\uff0c\u57c3\u53ca\u6210\u8bed\u6bd4\u963f\u62c9\u4f2f\u8c1a\u8bed\u4f4e10.28%\uff1b\u8bed\u7528\u4efb\u52a1\u51c6\u786e\u7387\u6bd4\u7406\u89e3\u4efb\u52a1\u4f4e14.07%\uff0c\u4f46\u5728\u63d0\u4f9b\u4e0a\u4e0b\u6587\u540e\u63d0\u5347\u4e8610.66%\uff1b\u6a21\u578b\u5728\u5185\u6db5\u610f\u4e49\u4e0a\u7684\u8868\u73b0\u8f83\u5f31\uff0c\u6700\u9ad8\u4e0e\u4eba\u5de5\u6ce8\u91ca\u4e00\u81f4\u7387\u4e3a85.58%\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u80fd\u7406\u89e3\u6bd4\u55bb\u610f\u4e49\uff0c\u4f46\u5728\u6070\u5f53\u4f7f\u7528\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002\u6bd4\u55bb\u8bed\u8a00\u662f\u6587\u5316\u63a8\u7406\u7684\u6709\u6548\u8bca\u65ad\u5de5\u5177\u3002\u7814\u7a76\u8fd8\u53d1\u5e03\u4e86\u9996\u4e2a\u7528\u4e8e\u7406\u89e3\u548c\u8bed\u7528\u8bc4\u4f30\u7684\u57c3\u53ca\u963f\u62c9\u4f2f\u8bed\u6210\u8bed\u6570\u636e\u96c6Kinayat\u3002"}}
{"id": "2510.23970", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.23970", "abs": "https://arxiv.org/abs/2510.23970", "authors": ["Maria C. Borges", "Julian Legler", "Lucca Di Benedetto"], "title": "Validating Alerts in Cloud-Native Observability", "comment": "16th Symposium on Software Performance (SSP'25)", "summary": "Observability and alerting form the backbone of modern reliability\nengineering. Alerts help teams catch faults early before they turn into\nproduction outages and serve as first clues for troubleshooting. However,\ndesigning effective alerts is challenging. They need to strike a fine balance\nbetween catching issues early and minimizing false alarms. On top of this,\nalerts often cover uncommon faults, so the code is rarely executed and\ntherefore rarely checked. To address these challenges, several industry\npractitioners advocate for testing alerting code with the same rigor as\napplication code. Still, there's a lack of tools that support such systematic\ndesign and validation of alerts.\n  This paper introduces a new alerting extension for the observability\nexperimentation tool OXN. It lets engineers experiment with alerts early during\ndevelopment. With OXN, engineers can now tune rules at design time and\nroutinely validate the firing behavior of their alerts, avoiding future\nproblems at runtime.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u76d1\u6d4b\u8b66\u62a5\u8bbe\u8ba1\u4e0e\u9a8c\u8bc1\u7684\u5de5\u5177\u6269\u5c55\uff0c\u5e2e\u52a9\u5de5\u7a0b\u5e08\u5728\u5f00\u53d1\u65e9\u671f\u6d4b\u8bd5\u548c\u8c03\u6574\u8b66\u62a5\u89c4\u5219\uff0c\u4ece\u800c\u51cf\u5c11\u8fd0\u884c\u65f6\u9519\u8bef\u3002", "motivation": "\u62a5\u8b66\u8bbe\u8ba1\u9700\u5728\u5c3d\u65e9\u53d1\u73b0\u95ee\u9898\u548c\u51cf\u5c11\u8bef\u62a5\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u4e14\u62a5\u8b66\u4ee3\u7801\u5f88\u5c11\u6267\u884c\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u5316\u8bbe\u8ba1\u548c\u9a8c\u8bc1\u5de5\u5177\u3002", "method": "\u5728\u73b0\u6709\u89c2\u6d4b\u5b9e\u9a8c\u5de5\u5177OXN\u4e0a\u6269\u5c55\u8b66\u62a5\u529f\u80fd\uff0c\u652f\u6301\u5de5\u7a0b\u5e08\u5728\u5f00\u53d1\u9636\u6bb5\u8fdb\u884c\u89c4\u5219\u8c03\u4f18\u548c\u8b66\u62a5\u884c\u4e3a\u9a8c\u8bc1\u3002", "result": "\u5de5\u7a0b\u5e08\u80fd\u591f\u5728\u8bbe\u8ba1\u65f6\u8c03\u6574\u548c\u6d4b\u8bd5\u8b66\u62a5\uff0c\u63d0\u5347\u8b66\u62a5\u7684\u6709\u6548\u6027\u548c\u53ef\u9760\u6027\uff0c\u51cf\u5c11\u751f\u4ea7\u6545\u969c\u3002", "conclusion": "\u901a\u8fc7\u5c06\u8b66\u62a5\u4ee3\u7801\u7684\u8bbe\u8ba1\u4e0e\u9a8c\u8bc1\u7cfb\u7edf\u5316\u5e76\u7eb3\u5165\u5f00\u53d1\u6d41\u7a0b\uff0c\u80fd\u591f\u63d0\u524d\u53d1\u73b0\u5e76\u907f\u514d\u6f5c\u5728\u8fd0\u884c\u65f6\u95ee\u9898\uff0c\u63d0\u9ad8\u7cfb\u7edf\u53ef\u9760\u6027\u3002"}}
{"id": "2510.23842", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23842", "abs": "https://arxiv.org/abs/2510.23842", "authors": ["Saki Imai", "Lee Kezar", "Laurel Aichler", "Mert Inan", "Erin Walker", "Alicia Wooten", "Lorna Quandt", "Malihe Alikhani"], "title": "How Pragmatics Shape Articulation: A Computational Case Study in STEM ASL Discourse", "comment": null, "summary": "Most state-of-the-art sign language models are trained on interpreter or\nisolated vocabulary data, which overlooks the variability that characterizes\nnatural dialogue. However, human communication dynamically adapts to contexts\nand interlocutors through spatiotemporal changes and articulation style. This\nspecifically manifests itself in educational settings, where novel vocabularies\nare used by teachers, and students. To address this gap, we collect a motion\ncapture dataset of American Sign Language (ASL) STEM (Science, Technology,\nEngineering, and Mathematics) dialogue that enables quantitative comparison\nbetween dyadic interactive signing, solo signed lecture, and interpreted\narticles. Using continuous kinematic features, we disentangle dialogue-specific\nentrainment from individual effort reduction and show spatiotemporal changes\nacross repeated mentions of STEM terms. On average, dialogue signs are\n24.6%-44.6% shorter in duration than the isolated signs, and show significant\nreductions absent in monologue contexts. Finally, we evaluate sign embedding\nmodels on their ability to recognize STEM signs and approximate how entrained\nthe participants become over time. Our study bridges linguistic analysis and\ncomputational modeling to understand how pragmatics shape sign articulation and\nits representation in sign language technologies.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6536\u96c6\u4e86\u7f8e\u56fd\u624b\u8bedSTEM\u9886\u57df\u7684\u52a8\u6001\u5bf9\u8bdd\u52a8\u4f5c\u6355\u6349\u6570\u636e\uff0c\u5206\u6790\u4e86\u5bf9\u8bdd\u8bed\u5883\u4e0b\u624b\u8bed\u52a8\u4f5c\u7684\u65f6\u7a7a\u53d8\u5316\u53ca\u7b80\u5316\u7279\u5f81\uff0c\u5e76\u8bc4\u4f30\u4e86\u624b\u8bed\u5d4c\u5165\u6a21\u578b\u5bf9\u4e13\u4e1a\u8bcd\u6c47\u7684\u8bc6\u522b\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u624b\u8bed\u6a21\u578b\u591a\u57fa\u4e8e\u53e3\u8bd1\u6216\u5b64\u7acb\u8bcd\u6c47\uff0c\u5ffd\u89c6\u81ea\u7136\u5bf9\u8bdd\u4e2d\u624b\u8bed\u7684\u53d8\u5316\u548c\u9002\u5e94\u6027\uff0c\u5c24\u5176\u5728\u6559\u80b2\u573a\u666f\u4e2d\uff0c\u6559\u5e08\u548c\u5b66\u751f\u4f7f\u7528\u7684\u65b0\u8bcd\u6c47\u5e26\u6765\u4e86\u989d\u5916\u6311\u6218\u3002", "method": "\u6536\u96c6\u7f8e\u56fd\u624b\u8bedSTEM\u9886\u57df\u7684\u52a8\u4f5c\u6355\u6349\u6570\u636e\uff0c\u6bd4\u8f83\u53cc\u4eba\u4e92\u52a8\u624b\u8bed\u3001\u5355\u4eba\u8bb2\u5ea7\u624b\u8bed\u548c\u53e3\u8bd1\u624b\u8bed\uff0c\u901a\u8fc7\u8fde\u7eed\u7684\u8fd0\u52a8\u5b66\u7279\u5f81\u89e3\u6790\u5bf9\u8bdd\u7279\u5f02\u7684\u540c\u6b65\u6027\u548c\u4e2a\u4f53\u52a8\u4f5c\u7b80\u5316\uff0c\u5206\u6790STEM\u672f\u8bed\u91cd\u590d\u51fa\u73b0\u65f6\u624b\u8bed\u7684\u65f6\u7a7a\u53d8\u5316\uff0c\u5e76\u8bc4\u4f30\u624b\u8bed\u5d4c\u5165\u6a21\u578b\u8bc6\u522b\u4e13\u4e1a\u8bcd\u6c47\u548c\u6355\u6349\u540c\u6b65\u6027\u7684\u8868\u73b0\u3002", "result": "\u53d1\u73b0\u5bf9\u8bdd\u4e2d\u7684\u624b\u8bed\u52a8\u4f5c\u6bd4\u5b64\u7acb\u624b\u8bed\u65f6\u957f\u7f29\u77ed24.6%-44.6%\uff0c\u4e14\u5728\u5355\u4eba\u8bb2\u5ea7\u4e2d\u65e0\u6b64\u7f29\u77ed\u73b0\u8c61\uff1b\u901a\u8fc7\u6a21\u578b\u8bc4\u4f30\u5c55\u793a\u4e86\u53c2\u4e0e\u8005\u968f\u7740\u65f6\u95f4\u63a8\u79fb\u7684\u52a8\u4f5c\u540c\u6b65\u7a0b\u5ea6\uff1b\u63ed\u793a\u4e86\u624b\u8bed\u52a8\u4f5c\u5728\u5bf9\u8bdd\u4e2d\u7684\u65f6\u7a7a\u53d8\u5316\u7279\u70b9\u3002", "conclusion": "\u7814\u7a76\u7ed3\u5408\u8bed\u8a00\u5b66\u548c\u8ba1\u7b97\u5efa\u6a21\uff0c\u9610\u91ca\u4e86\u8bed\u7528\u56e0\u7d20\u5982\u4f55\u5f71\u54cd\u624b\u8bed\u8868\u8fbe\u53ca\u5176\u6280\u672f\u8868\u793a\uff0c\u4e3a\u589e\u5f3a\u624b\u8bed\u6280\u672f\u5bf9\u52a8\u6001\u4ea4\u6d41\u7684\u9002\u5e94\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u6570\u636e\u652f\u6301\u3002"}}
{"id": "2510.24019", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24019", "abs": "https://arxiv.org/abs/2510.24019", "authors": ["Xing Xing", "Wei Wang", "Lipeng Ma", "Weidong Yang", "Junjie Zheng"], "title": "Lifecycle-Aware code generation: Leveraging Software Engineering Phases in LLMs", "comment": null, "summary": "Recent progress in large language models (LLMs) has advanced automatic code\ngeneration, yet most approaches rely on direct, single-step translation from\nproblem descriptions to code, disregarding structured software engineering\npractices. We introduce a lifecycle-aware framework that systematically\nincorporates intermediate artifacts such as requirements analysis, state\nmachine modeling, and pseudocode into both the training and inference stages.\nThis design aligns code generation with standard software development phases\nand enables more structured reasoning. Experiments show that lifecycle-level\nfine-tuning improves code correctness by up to 75% over the same model before\nfine-tuning, with performance gains compounding across intermediate stages.\nMulti-step inference consistently surpasses single-step generation,\ndemonstrating the effectiveness of intermediate scaffolding. Notably,\nopen-source LLMs, once fine-tuned under our framework, match or slightly\noutperform models pretrained on code. When applied to DeepSeek-Coder-1.3B, our\nframework yields relative CodeBLEU improvements of 34.3%, 20.0%, 11.2%, and\n22.3% over ChatGPT-3.5, ChatGPT-4o-mini, DeepSeek-R1, and LLaMA-8B,\nrespectively. Our pipeline also proves robust with up to 80\\% less training\ndata, confirming its resilience. Ablation studies further reveal that each\nintermediate artifact contributes distinctly to final code quality, with state\nmachine modeling yielding the most substantial impact. Our source code and\ndetailed experimental data are available at\nhttps://anonymous.4open.science/r/Lifecycle-Aware-3CCB.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u751f\u547d\u5468\u671f\u611f\u77e5\u6846\u67b6\uff0c\u5c06\u8f6f\u4ef6\u5f00\u53d1\u7684\u4e2d\u95f4\u4ea7\u7269\uff08\u5982\u9700\u6c42\u5206\u6790\u3001\u72b6\u6001\u673a\u5efa\u6a21\u3001\u4f2a\u4ee3\u7801\uff09\u7eb3\u5165\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u81ea\u52a8\u4ee3\u7801\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u7ed3\u6784\u5316\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u4ee3\u7801\u751f\u6210\u591a\u4f9d\u8d56\u4e8e\u5355\u6b65\u4ece\u95ee\u9898\u63cf\u8ff0\u76f4\u63a5\u7ffb\u8bd1\u6210\u4ee3\u7801\uff0c\u5ffd\u89c6\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u7684\u7ed3\u6784\u5316\u6d41\u7a0b\uff0c\u5bfc\u81f4\u751f\u6210\u4ee3\u7801\u8d28\u91cf\u6709\u9650\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8f6f\u4ef6\u5f00\u53d1\u751f\u547d\u5468\u671f\u7684\u591a\u6b65\u9aa4\u751f\u6210\u6846\u67b6\uff0c\u5c06\u9700\u6c42\u5206\u6790\u3001\u72b6\u6001\u673a\u5efa\u6a21\u3001\u4f2a\u4ee3\u7801\u7b49\u4f5c\u4e3a\u4e2d\u95f4\u4ea7\u7269\u5f15\u5165\u8bad\u7ec3\u548c\u63a8\u7406\uff0c\u5141\u8bb8\u6a21\u578b\u8fdb\u884c\u5206\u6b65\u63a8\u7406\u548c\u8c03\u4f18\u3002", "result": "\u751f\u547d\u5468\u671f\u7ea7\u5fae\u8c03\u4f7f\u4ee3\u7801\u6b63\u786e\u7387\u63d0\u5347\u6700\u9ad8\u8fbe75%\uff0c\u591a\u6b65\u63a8\u7406\u4f18\u4e8e\u5355\u6b65\u751f\u6210\uff0c\u5f00\u6e90\u6a21\u578b\u7ecf\u5fae\u8c03\u53ef\u4e0e\u6216\u8d85\u8d8a\u9884\u8bad\u7ec3\u4ee3\u7801\u6a21\u578b\u3002\u5177\u4f53\u5728DeepSeek-Coder-1.3B\u4e0a\u5bf9\u6bd4\u591a\u4e2a\u6a21\u578b\u6709\u660e\u663eCodeBLEU\u63d0\u5347\uff0c\u4e14\u5bf9\u8bad\u7ec3\u6570\u636e\u91cf\u5927\u5e45\u51cf\u5c11\u5177\u5907\u9c81\u68d2\u6027\u3002", "conclusion": "\u5f15\u5165\u4e2d\u95f4\u8f6f\u4ef6\u5de5\u7a0b\u4ea7\u7269\u4e3a\u81ea\u52a8\u4ee3\u7801\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7ed3\u6784\u5316\u652f\u6491\uff0c\u663e\u8457\u63d0\u5347\u4ee3\u7801\u8d28\u91cf\u548c\u6a21\u578b\u6027\u80fd\uff0c\u5c24\u5176\u72b6\u6001\u673a\u5efa\u6a21\u5bf9\u6700\u7ec8\u6210\u679c\u8d21\u732e\u6700\u5927\u3002\u8be5\u6846\u67b6\u9002\u7528\u6027\u5f3a\u4e14\u516c\u5f00\u4e86\u6e90\u7801\u548c\u5b9e\u9a8c\u6570\u636e\u3002"}}
{"id": "2510.24438", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.24438", "abs": "https://arxiv.org/abs/2510.24438", "authors": ["Abdullah Mushtaq", "Rafay Naeem", "Ezieddin Elmahjub", "Ibrahim Ghaznavi", "Shawqi Al-Maliki", "Mohamed Abdallah", "Ala Al-Fuqaha", "Junaid Qadir"], "title": "Can LLMs Write Faithfully? An Agent-Based Evaluation of LLM-generated Islamic Content", "comment": "Accepted at 39th Conference on Neural Information Processing Systems\n  (NeurIPS 2025) Workshop: 5th Muslims in Machine Learning (MusIML) Workshop", "summary": "Large language models are increasingly used for Islamic guidance, but risk\nmisquoting texts, misapplying jurisprudence, or producing culturally\ninconsistent responses. We pilot an evaluation of GPT-4o, Ansari AI, and Fanar\non prompts from authentic Islamic blogs. Our dual-agent framework uses a\nquantitative agent for citation verification and six-dimensional scoring (e.g.,\nStructure, Islamic Consistency, Citations) and a qualitative agent for\nfive-dimensional side-by-side comparison (e.g., Tone, Depth, Originality).\nGPT-4o scored highest in Islamic Accuracy (3.93) and Citation (3.38), Ansari AI\nfollowed (3.68, 3.32), and Fanar lagged (2.76, 1.82). Despite relatively strong\nperformance, models still fall short in reliably producing accurate Islamic\ncontent and citations -- a paramount requirement in faith-sensitive writing.\nGPT-4o had the highest mean quantitative score (3.90/5), while Ansari AI led\nqualitative pairwise wins (116/200). Fanar, though trailing, introduces\ninnovations for Islamic and Arabic contexts. This study underscores the need\nfor community-driven benchmarks centering Muslim perspectives, offering an\nearly step toward more reliable AI in Islamic knowledge and other high-stakes\ndomains such as medicine, law, and journalism.", "AI": {"tldr": "\u8bc4\u4f30\u4e86GPT-4o\u3001Ansari AI\u548cFanar\u5728\u4f0a\u65af\u5170\u6307\u5bfc\u6587\u672c\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0GPT-4o\u8868\u73b0\u6700\u597d\uff0c\u4f46\u5404\u6a21\u578b\u4ecd\u96be\u4ee5\u4fdd\u8bc1\u5185\u5bb9\u548c\u5f15\u7528\u7684\u7edd\u5bf9\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4f0a\u65af\u5170\u6307\u5bfc\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u5b58\u5728\u8bef\u5f15\u7ecf\u5178\u3001\u6cd5\u5b66\u8bef\u7528\u53ca\u6587\u5316\u4e0d\u4e00\u81f4\u98ce\u9669\uff0c\u9700\u8bc4\u4f30\u5176\u8868\u73b0\u3002", "method": "\u57fa\u4e8e\u771f\u5b9e\u4f0a\u65af\u5170\u535a\u5ba2\u6587\u672c\uff0c\u91c7\u7528\u53cc\u4ee3\u7406\u6846\u67b6\uff1a\u91cf\u5316\u4ee3\u7406\u8fdb\u884c\u5f15\u6587\u6838\u9a8c\u53ca\u516d\u7ef4\u8bc4\u5206\uff0c\u8d28\u6027\u4ee3\u7406\u8fdb\u884c\u4e94\u7ef4\u5ea6\u5bf9\u6bd4\u8bc4\u4ef7\u3002", "result": "GPT-4o\u5728\u4f0a\u65af\u5170\u51c6\u786e\u5ea6\u548c\u5f15\u7528\u8bc4\u5206\u6700\u9ad8\uff0cAnsari AI\u6b21\u4e4b\uff0cFanar\u8868\u73b0\u8f83\u5f31\uff1b\u6a21\u578b\u6574\u4f53\u5c1a\u672a\u8fbe\u5230\u5b8c\u5168\u51c6\u786e\u4e0e\u53ef\u9760\u3002", "conclusion": "\u5f3a\u8c03\u5efa\u7acb\u4ee5\u7a46\u65af\u6797\u89c6\u89d2\u4e3a\u6838\u5fc3\u7684\u793e\u533a\u9a71\u52a8\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63a8\u52a8\u5728\u5b97\u6559\u3001\u533b\u5b66\u3001\u6cd5\u5f8b\u7b49\u5173\u952e\u9886\u57df\u7684\u53ef\u9760AI\u53d1\u5c55\u3002"}}
{"id": "2510.23845", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23845", "abs": "https://arxiv.org/abs/2510.23845", "authors": ["Grace Byun", "Rebecca Lipschutz", "Sean T. Minton", "Abigail Lott", "Jinho D. Choi"], "title": "CRADLE Bench: A Clinician-Annotated Benchmark for Multi-Faceted Mental Health Crisis and Safety Risk Detection", "comment": null, "summary": "Detecting mental health crisis situations such as suicide ideation, rape,\ndomestic violence, child abuse, and sexual harassment is a critical yet\nunderexplored challenge for language models. When such situations arise during\nuser--model interactions, models must reliably flag them, as failure to do so\ncan have serious consequences. In this work, we introduce CRADLE BENCH, a\nbenchmark for multi-faceted crisis detection. Unlike previous efforts that\nfocus on a limited set of crisis types, our benchmark covers seven types\ndefined in line with clinical standards and is the first to incorporate\ntemporal labels. Our benchmark provides 600 clinician-annotated evaluation\nexamples and 420 development examples, together with a training corpus of\naround 4K examples automatically labeled using a majority-vote ensemble of\nmultiple language models, which significantly outperforms single-model\nannotation. We further fine-tune six crisis detection models on subsets defined\nby consensus and unanimous ensemble agreement, providing complementary models\ntrained under different agreement criteria.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CRADLE BENCH\uff0c\u4e00\u4e2a\u6db5\u76d6\u4e03\u79cd\u7b26\u5408\u4e34\u5e8a\u6807\u51c6\u5371\u673a\u7c7b\u578b\u5e76\u5305\u542b\u65f6\u95f4\u6807\u7b7e\u7684\u591a\u9762\u5411\u5371\u673a\u68c0\u6d4b\u57fa\u51c6\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u68c0\u6d4b\u7cbe\u795e\u5065\u5eb7\u5371\u673a\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "motivation": "\u7cbe\u795e\u5065\u5eb7\u5371\u673a\uff08\u5982\u81ea\u6740\u610f\u5ff5\u3001\u5f3a\u5978\u3001\u5bb6\u5ead\u66b4\u529b\u7b49\uff09\u7684\u68c0\u6d4b\u5bf9\u8bed\u8a00\u6a21\u578b\u6765\u8bf4\u662f\u4e00\u9879\u5173\u952e\u4f46\u672a\u5145\u5206\u63a2\u7d22\u7684\u6311\u6218\uff0c\u4e14\u672a\u80fd\u53ca\u65f6\u8bc6\u522b\u4f1a\u9020\u6210\u4e25\u91cd\u540e\u679c\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u5168\u9762\u4e14\u4e34\u5e8a\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u6784\u5efa\u4e86\u6db5\u76d6\u4e03\u79cd\u5371\u673a\u7c7b\u578b\u5e76\u5e26\u65f6\u95f4\u6807\u7b7e\u7684CRADLE BENCH\u57fa\u51c6\uff0c\u5305\u542b600\u4e2a\u4e34\u5e8a\u533b\u751f\u6807\u6ce8\u7684\u8bc4\u4f30\u793a\u4f8b\u548c420\u4e2a\u5f00\u53d1\u793a\u4f8b\uff0c\u4ee5\u53ca\u7ea64000\u4e2a\u901a\u8fc7\u591a\u6a21\u578b\u591a\u6570\u6295\u7968\u81ea\u52a8\u6807\u6ce8\u7684\u8bad\u7ec3\u6837\u672c\u3002\u57fa\u4e8e\u4e0d\u540c\u6a21\u578b\u4e00\u81f4\u6027\u7a0b\u5ea6\uff0c\u5bf9\u516d\u4e2a\u5371\u673a\u68c0\u6d4b\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u8bad\u7ec3\u3002", "result": "\u901a\u8fc7\u591a\u6a21\u578b\u591a\u6570\u6295\u7968\u751f\u6210\u8bad\u7ec3\u6570\u636e\u663e\u8457\u4f18\u4e8e\u5355\u6a21\u578b\u6807\u6ce8\uff0c\u5fae\u8c03\u516d\u4e2a\u6a21\u578b\u5b9e\u73b0\u4e0d\u540c\u7684\u4e00\u81f4\u6027\u8bad\u7ec3\u7b56\u7565\uff0c\u63d0\u5347\u5371\u673a\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "CRADLE BENCH\u4e3a\u5371\u673a\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u591a\u7ef4\u5ea6\u3001\u4e34\u5e8a\u6807\u51c6\u7684\u6807\u6ce8\u6570\u636e\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u652f\u6301\u6784\u5efa\u66f4\u4e3a\u6709\u6548\u548c\u53ef\u9760\u7684\u7cbe\u795e\u5065\u5eb7\u5371\u673a\u8bed\u8a00\u6a21\u578b\u68c0\u6d4b\u5de5\u5177\u3002"}}
{"id": "2510.24142", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24142", "abs": "https://arxiv.org/abs/2510.24142", "authors": ["Joran Leest", "Ilias Gerostathopoulos", "Patricia Lago", "Claudia Raibulet"], "title": "Monitoring and Observability of Machine Learning Systems: Current Practices and Gaps", "comment": null, "summary": "Production machine learning (ML) systems fail silently -- not with crashes,\nbut through wrong decisions. While observability is recognized as critical for\nML operations, there is a lack empirical evidence of what practitioners\nactually capture. This study presents empirical results on ML observability in\npractice through seven focus group sessions in several domains. We catalog the\ninformation practitioners systematically capture across ML systems and their\nenvironment and map how they use it to validate models, detect and diagnose\nfaults, and explain observed degradations. Finally, we identify gaps in current\npractice and outline implications for tooling design and research to establish\nML observability practices.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u4e03\u4e2a\u7126\u70b9\u5c0f\u7ec4\u7814\u7a76\u4e86\u751f\u4ea7\u73af\u5883\u4e2d\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u7684\u53ef\u89c2\u6d4b\u6027\u5b9e\u8df5\uff0c\u5206\u6790\u4e86\u4ece\u6a21\u578b\u9a8c\u8bc1\u5230\u6545\u969c\u8bca\u65ad\u7684\u7cfb\u7edf\u6355\u83b7\u4fe1\u606f\u53ca\u5176\u5e94\u7528\uff0c\u5e76\u6307\u51fa\u4e86\u5f53\u524d\u5b9e\u8df5\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u5728\u751f\u4ea7\u4e2d\u7ecf\u5e38\u65e0\u58f0\u5931\u8d25\uff0c\u7f3a\u5c11\u5bf9\u5b9e\u8df5\u4e2d\u53ef\u89c2\u6d4b\u6027\u771f\u5b9e\u60c5\u51b5\u7684\u7ecf\u9a8c\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u4e03\u4e2a\u4e0d\u540c\u9886\u57df\u7684\u7126\u70b9\u5c0f\u7ec4\u4f1a\u8bae\uff0c\u6536\u96c6\u548c\u5206\u6790\u5b9e\u8df5\u8005\u5bf9ML\u7cfb\u7edf\u53ca\u5176\u73af\u5883\u7684\u7cfb\u7edf\u6027\u4fe1\u606f\u6355\u83b7\u53ca\u4f7f\u7528\u60c5\u51b5\u3002", "result": "\u7cfb\u7edf\u5730\u6574\u7406\u4e86\u5b9e\u8df5\u8005\u91c7\u96c6\u7684\u4fe1\u606f\u7c7b\u522b\u548c\u4f7f\u7528\u65b9\u5f0f\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u9a8c\u8bc1\u3001\u6545\u969c\u68c0\u6d4b\u8bca\u65ad\u548c\u6027\u80fd\u4e0b\u964d\u89e3\u91ca\u8fc7\u7a0b\u4e2d\u7684\u4fe1\u606f\u9700\u6c42\u548c\u5e94\u7528\u65b9\u5f0f\u3002", "conclusion": "\u5f53\u524d\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u53ef\u89c2\u6d4b\u6027\u5b58\u5728\u7f3a\u53e3\uff0c\u7814\u7a76\u7ed3\u679c\u4e3a\u5de5\u5177\u8bbe\u8ba1\u548c\u65b9\u6cd5\u6539\u8fdb\u63d0\u4f9b\u6307\u5bfc\uff0c\u63a8\u52a8\u5efa\u7acb\u5b8c\u5584\u7684ML\u53ef\u89c2\u6d4b\u6027\u5b9e\u8df5\u4f53\u7cfb\u3002"}}
{"id": "2510.24701", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.24701", "abs": "https://arxiv.org/abs/2510.24701", "authors": ["Tongyi DeepResearch Team", "Baixuan Li", "Bo Zhang", "Dingchu Zhang", "Fei Huang", "Guangyu Li", "Guoxin Chen", "Huifeng Yin", "Jialong Wu", "Jingren Zhou", "Kuan Li", "Liangcai Su", "Litu Ou", "Liwen Zhang", "Pengjun Xie", "Rui Ye", "Wenbiao Yin", "Xinmiao Yu", "Xinyu Wang", "Xixi Wu", "Xuanzhong Chen", "Yida Zhao", "Zhen Zhang", "Zhengwei Tao", "Zhongwang Zhang", "Zile Qiao", "Chenxi Wang", "Donglei Yu", "Gang Fu", "Haiyang Shen", "Jiayin Yang", "Jun Lin", "Junkai Zhang", "Kui Zeng", "Li Yang", "Hailong Yin", "Maojia Song", "Ming Yan", "Peng Xia", "Qian Xiao", "Rui Min", "Ruixue Ding", "Runnan Fang", "Shaowei Chen", "Shen Huang", "Shihang Wang", "Shihao Cai", "Weizhou Shen", "Xiaobin Wang", "Xin Guan", "Xinyu Geng", "Yingcheng Shi", "Yuning Wu", "Zhuo Chen", "Zijian Li", "Yong Jiang"], "title": "Tongyi DeepResearch Technical Report", "comment": "https://tongyi-agent.github.io/blog", "summary": "We present Tongyi DeepResearch, an agentic large language model, which is\nspecifically designed for long-horizon, deep information-seeking research\ntasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is\ndeveloped through an end-to-end training framework that combines agentic\nmid-training and agentic post-training, enabling scalable reasoning and\ninformation seeking across complex tasks. We design a highly scalable data\nsynthesis pipeline that is fully automatic, without relying on costly human\nannotation, and empowers all training stages. By constructing customized\nenvironments for each stage, our system enables stable and consistent\ninteractions throughout. Tongyi DeepResearch, featuring 30.5 billion total\nparameters, with only 3.3 billion activated per token, achieves\nstate-of-the-art performance across a range of agentic deep research\nbenchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH,\nWebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We\nopen-source the model, framework, and complete solutions to empower the\ncommunity.", "AI": {"tldr": "Tongyi DeepResearch\u662f\u4e00\u6b3e\u9762\u5411\u957f\u8fdc\u3001\u6df1\u5ea6\u4fe1\u606f\u68c0\u7d22\u4efb\u52a1\u7684\u81ea\u4e3b\u4ee3\u7406\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u8bad\u7ec3\u6846\u67b6\u548c\u81ea\u52a8\u5316\u6570\u636e\u5408\u6210\uff0c\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u548c\u4fe1\u606f\u68c0\u7d22\uff0c\u8868\u73b0\u51fa\u8272\u5e76\u5f00\u6e90\u3002", "motivation": "\u9488\u5bf9\u957f\u5468\u671f\u3001\u6df1\u5ea6\u4fe1\u606f\u641c\u7d22\u4efb\u52a1\u9700\u6c42\uff0c\u8bbe\u8ba1\u4e00\u79cd\u5177\u5907\u81ea\u4e3b\u4ee3\u7406\u80fd\u529b\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u63d0\u5347\u590d\u6742\u4efb\u52a1\u7684\u63a8\u7406\u548c\u4fe1\u606f\u68c0\u7d22\u80fd\u529b\u3002", "method": "\u91c7\u7528\u7aef\u5230\u7aef\u8bad\u7ec3\u6846\u67b6\uff0c\u7ed3\u5408\u4ee3\u7406\u4e2d\u671f\u8bad\u7ec3\u4e0e\u540e\u671f\u8bad\u7ec3\uff0c\u5229\u7528\u5168\u81ea\u52a8\u3001\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u7684\u6570\u636e\u5408\u6210\u6d41\u6c34\u7ebf\uff0c\u8bbe\u8ba1\u5b9a\u5236\u73af\u5883\u4ee5\u4fdd\u8bc1\u4ea4\u4e92\u7a33\u5b9a\u6027\u3002", "result": "\u62e5\u6709305\u4ebf\u53c2\u6570\u7684\u6a21\u578b\uff0c\u6fc0\u6d3b\u53c2\u6570\u4ec53.3\u4ebf\uff0c\u8fbe\u6210\u591a\u9879\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\u6d4b\u8bd5\u7684\u6700\u5148\u8fdb\u8868\u73b0\uff0c\u5305\u62ec\u591a\u8bed\u8a00\u548c\u591a\u573a\u666f\u4efb\u52a1\u3002", "conclusion": "\u8be5\u6a21\u578b\u53ca\u8bad\u7ec3\u6846\u67b6\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u7a33\u5b9a\u3001\u53ef\u6269\u5c55\u7684\u6df1\u5c42\u4fe1\u606f\u4ee3\u7406\u7814\u7a76\u80fd\u529b\uff0c\u4e14\u5df2\u5f00\u6e90\uff0c\u4fc3\u8fdb\u793e\u533a\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2510.23853", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23853", "abs": "https://arxiv.org/abs/2510.23853", "authors": ["Yize Cheng", "Arshia Soltani Moakhar", "Chenrui Fan", "Kazem Faghih", "Parsa Hosseini", "Wenxiao Wang", "Soheil Feizi"], "title": "Temporal Blindness in Multi-Turn LLM Agents: Misaligned Tool Use vs. Human Time Perception", "comment": "preliminary work in progress", "summary": "Large language model agents are increasingly used in multi-turn\nconversational settings to interact with and execute tasks in dynamic\nenvironments. However, a key limitation is their temporal blindness: they, by\ndefault, operate with a stationary context, failing to account for the\nreal-world time elapsed between messages. This becomes a critical liability\nwhen an agent must decide whether to invoke a tool based on how much time has\npassed since the last observation. Without temporal awareness, agents often\neither over-rely on previous context (skipping necessary tool calls), or\nunder-rely on it (unnecessarily repeating tool calls). To study this challenge,\nwe introduce TicToc-v1, a test set of multi-turn user-agent trajectories across\n34 scenarios with varying time sensitivity. Each trajectory ends with a user\nquestion, where the need for a tool call depends on the amount of time elapsed\nsince the last message. To give LLMs temporal context, we augment dialogue\nmessages with explicit timestamps, bridging the gap between static dialogue and\nevolving environments. We then collected human preferences for these samples,\ncreating two subsets: one where humans preferred relying on the previous\nobservation (prefer-noTool), and another where they preferred a new tool call\n(prefer-Tool). We evaluated how well LLM tool-calling decisions align with\nhuman preferences under varying time intervals on TicToc-v1. Our analysis show\nthat without time information, most models perform only slightly better than\nrandom, with the top alignment rate being just over 60%. While adding\ntimestamps leads to a slight improvement, particularly for larger models, the\nimprovement is modest, peaking at around 65%. We also show that naive,\nprompt-based alignment have limited effectiveness. Our findings highlight the\nneed for specific post-training alignment to align multi-turn LLM tool use with\nhuman temporal perception.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86TicToc-v1\u6d4b\u8bd5\u96c6\uff0c\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7f3a\u4e4f\u65f6\u95f4\u611f\u77e5\u7684\u95ee\u9898\u53ca\u5176\u5bf9\u5de5\u5177\u8c03\u7528\u51b3\u7b56\u7684\u5f71\u54cd\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u6ca1\u6709\u65f6\u95f4\u611f\u77e5\uff0c\u5bfc\u81f4\u5de5\u5177\u8c03\u7528\u51b3\u7b56\u4e0d\u51c6\u786e\uff0c\u5f71\u54cd\u4e0e\u52a8\u6001\u73af\u5883\u7684\u4ea4\u4e92\u6548\u679c\u3002", "method": "\u6784\u5efa\u5305\u542b\u663e\u5f0f\u65f6\u95f4\u6233\u7684\u5bf9\u8bdd\u6570\u636e\u96c6TicToc-v1\uff0c\u91c7\u96c6\u4eba\u7c7b\u504f\u597d\u6570\u636e\uff0c\u5c06\u5bf9\u8bdd\u5206\u4e3a\u504f\u5411\u91cd\u590d\u4f7f\u7528\u5de5\u5177\u548c\u504f\u5411\u65b0\u5de5\u5177\u8c03\u7528\u4e24\u7c7b\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u4e0d\u540c\u65f6\u95f4\u95f4\u9694\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u6ca1\u6709\u65f6\u95f4\u4fe1\u606f\u65f6\u6a21\u578b\u8868\u73b0\u63a5\u8fd1\u968f\u673a\uff1b\u52a0\u5165\u65f6\u95f4\u6233\u540e\u8868\u73b0\u7565\u6709\u63d0\u5347\uff0c\u5c24\u5176\u662f\u5927\u6a21\u578b\uff0c\u4f46\u63d0\u5347\u6709\u9650\uff1b\u7b80\u5355\u7684\u63d0\u793a\u5bf9\u9f50\u65b9\u6cd5\u6548\u679c\u6709\u9650\u3002", "conclusion": "\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u65f6\u95f4\u611f\u77e5\u5bf9\u5de5\u5177\u8c03\u7528\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u9700\u91c7\u7528\u4e13\u95e8\u7684\u8bad\u7ec3\u540e\u5bf9\u9f50\u65b9\u6cd5\u63d0\u9ad8\u6a21\u578b\u4e0e\u4eba\u7c7b\u65f6\u95f4\u611f\u77e5\u7684\u5339\u914d\u5ea6\u3002"}}
{"id": "2510.24188", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24188", "abs": "https://arxiv.org/abs/2510.24188", "authors": ["C\u00e9sar Santos", "Ermeson Andrade", "Roberto Natella"], "title": "Investigating Software Aging in LLM-Generated Software Systems", "comment": "Presented at the 17th International Workshop on Software Aging and\n  Rejuvenation (WoSAR), 2025", "summary": "Automatically generated software, especially code produced by Large Language\nModels (LLMs), is increasingly adopted to accelerate development and reduce\nmanual effort. However, little is known about the long-term reliability of such\nsystems under sustained execution. In this paper, we experimentally investigate\nthe phenomenon of software aging in applications generated by LLM-based tools.\nUsing the Bolt platform and standardized prompts from Baxbench, we generated\nfour service-oriented applications and subjected them to 50-hour load tests.\nResource usage, response time, and throughput were continuously monitored to\ndetect degradation patterns. The results reveal significant evidence of\nsoftware aging, including progressive memory growth, increased response time,\nand performance instability across all applications. Statistical analyzes\nconfirm these trends and highlight variability in the severity of aging\naccording to the type of application. Our findings show the need to consider\naging in automatically generated software and provide a foundation for future\nstudies on mitigation strategies and long-term reliability evaluation.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5bf9\u56db\u4e2a\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u670d\u52a1\u578b\u5e94\u7528\u8fdb\u884c\u957f\u8fbe50\u5c0f\u65f6\u7684\u8d1f\u8f7d\u6d4b\u8bd5\uff0c\u53d1\u73b0\u8f6f\u4ef6\u8001\u5316\u73b0\u8c61\uff0c\u5305\u62ec\u5185\u5b58\u589e\u957f\u3001\u54cd\u5e94\u65f6\u95f4\u589e\u52a0\u548c\u6027\u80fd\u4e0d\u7a33\u5b9a\uff0c\u5e76\u5f3a\u8c03\u81ea\u52a8\u751f\u6210\u8f6f\u4ef6\u7684\u957f\u671f\u53ef\u9760\u6027\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u4ee3\u7801\u88ab\u5e7f\u6cdb\u91c7\u7528\uff0c\u957f\u671f\u8fd0\u884c\u7684\u53ef\u9760\u6027\u95ee\u9898\u5c1a\u4e0d\u6e05\u695a\uff0c\u9700\u7814\u7a76\u81ea\u52a8\u751f\u6210\u8f6f\u4ef6\u7684\u8001\u5316\u73b0\u8c61\u3002", "method": "\u5229\u7528Bolt\u5e73\u53f0\u548cBaxbench\u6807\u51c6\u5316\u63d0\u793a\u8bed\u751f\u6210\u56db\u4e2a\u5e94\u7528\uff0c\u8fdb\u884c50\u5c0f\u65f6\u8d1f\u8f7d\u6d4b\u8bd5\uff0c\u6301\u7eed\u76d1\u6d4b\u8d44\u6e90\u4f7f\u7528\u3001\u54cd\u5e94\u65f6\u95f4\u548c\u541e\u5410\u91cf\uff0c\u7edf\u8ba1\u5206\u6790\u6027\u80fd\u9000\u5316\u3002", "result": "\u6240\u6709\u5e94\u7528\u8868\u73b0\u51fa\u663e\u8457\u7684\u8f6f\u4ef6\u8001\u5316\u8ff9\u8c61\uff0c\u5982\u5185\u5b58\u6301\u7eed\u589e\u957f\u3001\u54cd\u5e94\u65f6\u95f4\u63d0\u5347\u548c\u6027\u80fd\u6ce2\u52a8\uff0c\u4e14\u4e0d\u540c\u5e94\u7528\u8001\u5316\u4e25\u91cd\u7a0b\u5ea6\u5b58\u5728\u5dee\u5f02\u3002", "conclusion": "\u81ea\u52a8\u751f\u6210\u7684\u8f6f\u4ef6\u5b58\u5728\u8f6f\u4ef6\u8001\u5316\u98ce\u9669\uff0c\u5e94\u91cd\u89c6\u5176\u957f\u671f\u53ef\u9760\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u7f13\u89e3\u7b56\u7565\u548c\u8bc4\u4f30\u65b9\u6cd5\u7684\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2510.23854", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23854", "abs": "https://arxiv.org/abs/2510.23854", "authors": ["Jyotika Singh", "Weiyi Sun", "Amit Agarwal", "Viji Krishnamurthy", "Yassine Benajiba", "Sujith Ravi", "Dan Roth"], "title": "Can LLMs Narrate Tabular Data? An Evaluation Framework for Natural Language Representations of Text-to-SQL System Outputs", "comment": "Accepted at EMNLP 2025", "summary": "In modern industry systems like multi-turn chat agents, Text-to-SQL\ntechnology bridges natural language (NL) questions and database (DB) querying.\nThe conversion of tabular DB results into NL representations (NLRs) enables the\nchat-based interaction. Currently, NLR generation is typically handled by large\nlanguage models (LLMs), but information loss or errors in presenting tabular\nresults in NL remains largely unexplored. This paper introduces a novel\nevaluation method - Combo-Eval - for judgment of LLM-generated NLRs that\ncombines the benefits of multiple existing methods, optimizing evaluation\nfidelity and achieving a significant reduction in LLM calls by 25-61%.\nAccompanying our method is NLR-BIRD, the first dedicated dataset for NLR\nbenchmarking. Through human evaluations, we demonstrate the superior alignment\nof Combo-Eval with human judgments, applicable across scenarios with and\nwithout ground truth references.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Combo-Eval\uff0c\u4e00\u79cd\u7ed3\u5408\u591a\u79cd\u73b0\u6709\u65b9\u6cd5\u7684\u6587\u672c\u5316\u6570\u636e\u5e93\u7ed3\u679c\u81ea\u7136\u8bed\u8a00\u8868\u73b0\uff08NLR\uff09\u751f\u6210\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u53d1\u5e03\u4e86\u9996\u4e2a\u4e13\u7528NLR\u57fa\u51c6\u6570\u636e\u96c6NLR-BIRD\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bc4\u4f30\u51c6\u786e\u6027\u4e0e\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5c06\u6570\u636e\u5e93\u67e5\u8be2\u7ed3\u679c\u8f6c\u6362\u4e3a\u81ea\u7136\u8bed\u8a00\u65f6\uff0c\u5b58\u5728\u4fe1\u606f\u4e22\u5931\u548c\u9519\u8bef\u7684\u95ee\u9898\uff0c\u4f46\u76f8\u5173\u7684\u8bc4\u4f30\u65b9\u6cd5\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u63d0\u51faCombo-Eval\u8bc4\u4f30\u65b9\u6cd5\uff0c\u878d\u5408\u591a\u79cd\u73b0\u6709\u8bc4\u4f30\u65b9\u5f0f\uff0c\u4f18\u5316\u8bc4\u4f30\u7684\u771f\u5b9e\u6027\uff0c\u5e76\u51cf\u5c1125-61%\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8c03\u7528\u6b21\u6570\uff0c\u540c\u65f6\u53d1\u5e03\u4e86\u7528\u4e8eNLR\u6d4b\u8bd5\u7684NLR-BIRD\u6570\u636e\u96c6\u3002", "result": "\u901a\u8fc7\u4eba\u5de5\u8bc4\u4f30\u9a8c\u8bc1\uff0cCombo-Eval\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u5339\u914d\u5ea6\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u6709\u65e0\u53c2\u8003\u7b54\u6848\u7684\u591a\u79cd\u573a\u666f\u3002", "conclusion": "Combo-Eval\u53caNLR-BIRD\u4e3aNLR\u751f\u6210\u8bc4\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4fc3\u8fdb\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6587\u8868\u8f6c\u7801\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.24241", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24241", "abs": "https://arxiv.org/abs/2510.24241", "authors": ["Zixian Zhang", "Takfarinas Saber"], "title": "MAGNET: A Multi-Graph Attentional Network for Code Clone Detection", "comment": null, "summary": "Code clone detection is a fundamental task in software engineering that\nunderpins refactoring, debugging, plagiarism detection, and vulnerability\nanalysis. Existing methods often rely on singular representations such as\nabstract syntax trees (ASTs), control flow graphs (CFGs), and data flow graphs\n(DFGs), which capture only partial aspects of code semantics. Hybrid approaches\nhave emerged, but their fusion strategies are typically handcrafted and\nineffective. In this study, we propose MAGNET, a multi-graph attentional\nframework that jointly leverages AST, CFG, and DFG representations to capture\nsyntactic and semantic features of source code. MAGNET integrates residual\ngraph neural networks with node-level self-attention to learn both local and\nlong-range dependencies, introduces a gated cross-attention mechanism for\nfine-grained inter-graph interactions, and employs Set2Set pooling to fuse\nmulti-graph embeddings into unified program-level representations. Extensive\nexperiments on BigCloneBench and Google Code Jam demonstrate that MAGNET\nachieves state-of-the-art performance with an overall F1 score of 96.5\\% and\n99.2\\% on the two datasets, respectively. Ablation studies confirm the critical\ncontributions of multi-graph fusion and each attentional component. Our code is\navailable at https://github.com/ZixianReid/Multigraph_match", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMAGNET\u7684\u591a\u56fe\u6ce8\u610f\u529b\u6846\u67b6\uff0c\u7528\u4e8e\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\uff0c\u901a\u8fc7\u878d\u5408AST\u3001CFG\u548cDFG\u4e09\u79cd\u4ee3\u7801\u8868\u793a\uff0c\u63d0\u5347\u4ee3\u7801\u8bed\u4e49\u6355\u83b7\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u68c0\u6d4b\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u65b9\u6cd5\u591a\u4f9d\u8d56\u5355\u4e00\u4ee3\u7801\u8868\u793a\uff0c\u65e0\u6cd5\u5168\u9762\u6355\u83b7\u4ee3\u7801\u8bed\u4e49\uff0c\u4e14\u73b0\u6709\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\u878d\u5408\u7b56\u7565\u6548\u679c\u6709\u9650\u3002", "method": "\u63d0\u51faMAGNET\u6846\u67b6\uff0c\u8054\u5408\u4f7f\u7528AST\u3001CFG\u548cDFG\u4e09\u79cd\u56fe\u7ed3\u6784\uff0c\u91c7\u7528\u6b8b\u5dee\u56fe\u795e\u7ecf\u7f51\u7edc\u7ed3\u5408\u8282\u70b9\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ee5\u53ca\u95e8\u63a7\u4ea4\u53c9\u6ce8\u610f\u529b\u5b9e\u73b0\u591a\u56fe\u7ec6\u7c92\u5ea6\u4ea4\u4e92\uff0c\u7528Set2Set\u6c60\u5316\u878d\u5408\u591a\u56fe\u5d4c\u5165\u3002", "result": "\u5728BigCloneBench\u548cGoogle Code Jam\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523096.5%\u548c99.2%\u7684F1\u5206\u6570\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u591a\u56fe\u878d\u5408\u548c\u6ce8\u610f\u529b\u7ec4\u4ef6\u7684\u91cd\u8981\u4f5c\u7528\u3002", "conclusion": "MAGNET\u6709\u6548\u878d\u5408\u591a\u79cd\u4ee3\u7801\u56fe\u8868\u793a\uff0c\u63d0\u5347\u4e86\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u6027\u80fd\uff0c\u8bf4\u660e\u591a\u56fe\u795e\u7ecf\u7f51\u7edc\u4e0e\u6ce8\u610f\u529b\u673a\u5236\u7684\u7ed3\u5408\u5bf9\u6355\u83b7\u4ee3\u7801\u8bed\u4e49\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2510.23870", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23870", "abs": "https://arxiv.org/abs/2510.23870", "authors": ["Marianne Menglin Liu", "Sai Ashish Somayajula", "Syed Fahad Allam Shah", "Sujith Ravi", "Dan Roth"], "title": "OraPlan-SQL: A Planning-Centric Framework for Complex Bilingual NL2SQL Reasoning", "comment": null, "summary": "We present OraPlan-SQL, our system for the Archer NL2SQL Evaluation Challenge\n2025, a bilingual benchmark requiring complex reasoning such as arithmetic,\ncommonsense, and hypothetical inference. OraPlan-SQL ranked first, exceeding\nthe second-best system by more than 6% in execution accuracy (EX), with 55.0%\nin English and 56.7% in Chinese, while maintaining over 99% SQL validity (VA).\nOur system follows an agentic framework with two components: Planner agent that\ngenerates stepwise natural language plans, and SQL agent that converts these\nplans into executable SQL. Since SQL agent reliably adheres to the plan, our\nrefinements focus on the planner. Unlike prior methods that rely on multiple\nsub-agents for planning and suffer from orchestration overhead, we introduce a\nfeedback-guided meta-prompting strategy to refine a single planner. Failure\ncases from a held-out set are clustered with human input, and an LLM distills\nthem into corrective guidelines that are integrated into the planner's system\nprompt, improving generalization without added complexity. For the multilingual\nscenario, to address transliteration and entity mismatch issues, we incorporate\nentity-linking guidelines that generate alternative surface forms for entities\nand explicitly include them in the plan. Finally, we enhance reliability\nthrough plan diversification: multiple candidate plans are generated for each\nquery, with the SQL agent producing a query for each plan, and final output\nselected via majority voting over their executions.", "AI": {"tldr": "OraPlan-SQL\u662f\u4e00\u79cd\u53cc\u8bedNL2SQL\u7cfb\u7edf\uff0c\u57282025\u5e74Archer\u6311\u6218\u8d5b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6267\u884c\u51c6\u786e\u7387\u8fbe55%\u4ee5\u4e0a\uff0cSQL\u6709\u6548\u6027\u8d8599%\u3002", "motivation": "\u9488\u5bf9\u590d\u6742\u63a8\u7406\u9700\u6c42\uff0c\u63d0\u5347\u81ea\u7136\u8bed\u8a00\u5230SQL\u7684\u8f6c\u6362\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u91c7\u7528\u53cc\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u5305\u62ec\u89c4\u5212\u8005\u548cSQL\u8f6c\u6362\u8005\u3002\u901a\u8fc7\u53cd\u9988\u9a71\u52a8\u7684\u5143\u63d0\u793a\u7b56\u7565\u6539\u8fdb\u5355\u4e00\u89c4\u5212\u8005\uff0c\u52a0\u5165\u5b9e\u4f53\u94fe\u63a5\u6307\u5bfc\u5904\u7406\u591a\u8bed\u8a00\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u751f\u6210\u591a\u79cd\u8ba1\u5212\u548c\u591a\u6570\u8868\u51b3\u63d0\u9ad8\u7ed3\u679c\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u6311\u6218\u8d5b\u4e2d\uff0cOraPlan-SQL\u6bd4\u7b2c\u4e8c\u540d\u9ad8\u51fa6%\u4ee5\u4e0a\uff0c\u82f1\u8bed\u548c\u6c49\u8bed\u6267\u884c\u51c6\u786e\u7387\u5206\u522b\u8fbe\u523055.0%\u548c56.7%\uff0cSQL\u6709\u6548\u6027\u8d85\u8fc799%\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u6709\u6548\u63d0\u5347\u4e86\u590d\u6742\u63a8\u7406\u7684NL2SQL\u6027\u80fd\u548c\u591a\u8bed\u8a00\u652f\u6301\uff0c\u4e14\u901a\u8fc7\u4f18\u5316\u89c4\u5212\u8005\u548c\u591a\u8ba1\u5212\u7b56\u7565\u63d0\u5347\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u7ed3\u679c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2510.24265", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.24265", "abs": "https://arxiv.org/abs/2510.24265", "authors": ["Sadia Afroz", "Zixuan Feng", "Katie Kimura", "Bianca Trinkenreich", "Igor Steinmacher", "Anita Sarma"], "title": "Developer Productivity with GenAI", "comment": null, "summary": "Generative AI (GenAI) tools are increasingly being adopted in software\ndevelopment as productivity aids. However, evidence regarding where and when\nthese tools actually enhance productivity is unclear. In this paper, we\ninvestigate how GenAI adoption affects different dimensions of developer\nproductivity. We surveyed 415 software practitioners to capture their\nperceptions of productivity changes associated with AI-assisted development\nusing the SPACE framework - Satisfaction and well-being, Performance, Activity,\nCommunication and collaboration, and Efficiency and flow. Our results,\ndisaggregated by frequency of AI usage, reveal limited overall productivity\nchange, highlighting the productivity paradox in which developers become faster\nbut do not necessarily create better software or feel more fulfilled.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5bf9415\u540d\u8f6f\u4ef6\u5f00\u53d1\u8005\u7684\u8c03\u67e5\uff0c\u7814\u7a76\u751f\u6210\u5f0fAI\u5de5\u5177\u5728\u5f00\u53d1\u4e2d\u7684\u4f7f\u7528\u9891\u7387\u5bf9\u751f\u4ea7\u529b\u7684\u591a\u7ef4\u5f71\u54cd\uff0c\u7ed3\u679c\u663e\u793a\u867d\u5f00\u53d1\u901f\u5ea6\u52a0\u5feb\uff0c\u4f46\u8f6f\u4ef6\u8d28\u91cf\u548c\u5f00\u53d1\u8005\u6ee1\u610f\u5ea6\u672a\u660e\u663e\u63d0\u5347\uff0c\u4f53\u73b0\u4e86\u751f\u4ea7\u529b\u6096\u8bba\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u5de5\u5177\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u666e\u53ca\uff0c\u5176\u5b9e\u9645\u63d0\u5347\u751f\u4ea7\u529b\u7684\u573a\u666f\u548c\u6548\u679c\u4ecd\u4e0d\u660e\u6717\uff0c\u7814\u7a76\u65e8\u5728\u660e\u786e\u8fd9\u4e9b\u5de5\u5177\u5bf9\u5f00\u53d1\u8005\u591a\u7ef4\u751f\u4ea7\u529b\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528SPACE\u6846\u67b6\uff08\u6ee1\u610f\u5ea6\u4e0e\u798f\u7949\u3001\u8868\u73b0\u3001\u6d3b\u52a8\u3001\u6c9f\u901a\u534f\u4f5c\u3001\u6548\u7387\u4e0e\u6d41\u7a0b\uff09\u5bf9415\u540d\u8f6f\u4ef6\u4ece\u4e1a\u8005\u8fdb\u884c\u4e86\u95ee\u5377\u8c03\u67e5\uff0c\u5e76\u6839\u636eAI\u4f7f\u7528\u9891\u7387\u5206\u6790\u5f71\u54cd\u3002", "result": "\u603b\u4f53\u751f\u4ea7\u529b\u53d8\u5316\u6709\u9650\uff0c\u5f00\u53d1\u901f\u5ea6\u6709\u6240\u52a0\u5feb\uff0c\u4f46\u5e76\u672a\u663e\u8457\u63d0\u9ad8\u8f6f\u4ef6\u8d28\u91cf\u6216\u5f00\u53d1\u8005\u6ee1\u8db3\u611f\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u5de5\u5177\u867d\u80fd\u63d0\u5347\u5f00\u53d1\u901f\u5ea6\uff0c\u4f46\u4e0d\u4e00\u5b9a\u5e26\u6765\u66f4\u9ad8\u8d28\u91cf\u7684\u8f6f\u4ef6\u6216\u66f4\u5f3a\u7684\u5f00\u53d1\u8005\u6ee1\u8db3\u611f\uff0c\u4f53\u73b0\u751f\u4ea7\u529b\u6096\u8bba\u3002"}}
{"id": "2510.23884", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23884", "abs": "https://arxiv.org/abs/2510.23884", "authors": ["Tananun Songdechakraiwut", "Michael Lutz"], "title": "Language Models for Longitudinal Clinical Prediction", "comment": null, "summary": "We explore a lightweight framework that adapts frozen large language models\nto analyze longitudinal clinical data. The approach integrates patient history\nand context within the language model space to generate accurate forecasts\nwithout model fine-tuning. Applied to neuropsychological assessments, it\nachieves accurate and reliable performance even with minimal training data,\nshowing promise for early-stage Alzheimer's monitoring.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u5229\u7528\u51bb\u7ed3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5206\u6790\u7eb5\u5411\u4e34\u5e8a\u6570\u636e\uff0c\u5b9e\u73b0\u51c6\u786e\u9884\u6d4b\uff0c\u65e0\u9700\u5fae\u8c03\u3002", "motivation": "\u5f53\u524d\u6a21\u578b\u5fae\u8c03\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u6709\u6548\u5229\u7528\u60a3\u8005\u5386\u53f2\u6570\u636e\u8fdb\u884c\u957f\u671f\u9884\u6d4b\u3002", "method": "\u5728\u8bed\u8a00\u6a21\u578b\u7a7a\u95f4\u4e2d\u878d\u5408\u60a3\u8005\u5386\u53f2\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u9002\u914d\u51bb\u7ed3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u5728\u795e\u7ecf\u5fc3\u7406\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u4e86\u9ad8\u51c6\u786e\u7387\u548c\u53ef\u9760\u6027\uff0c\u4e14\u8bad\u7ec3\u6570\u636e\u9700\u6c42\u6781\u5c11\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5bf9\u65e9\u671f\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u76d1\u6d4b\u5177\u6709\u6f5c\u529b\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u6a21\u578b\u5fae\u8c03\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24358", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24358", "abs": "https://arxiv.org/abs/2510.24358", "authors": ["Lingyue Fu", "Bolun Zhang", "Hao Guan", "Yaoming Zhu", "Lin Qiu", "Weiwen Liu", "Xuezhi Cao", "Xunliang Cai", "Weinan Zhang", "Yong Yu"], "title": "Automatically Benchmarking LLM Code Agents through Agent-Driven Annotation and Evaluation", "comment": null, "summary": "Recent advances in code agents have enabled automated software development at\nthe project level, supported by large language models (LLMs) and widely adopted\ntools. However, existing benchmarks for code agent evaluation face two major\nlimitations: high annotation cost and expertise requirements, and rigid\nevaluation metrics that rely primarily on unit tests. To address these\nchallenges, we propose an agent-driven benchmark construction pipeline that\nleverages human supervision to efficiently generate diverse and challenging\nproject-level tasks. Based on this approach, we introduce PRDBench, a novel\nbenchmark comprising 50 real-world Python projects across 20 domains, each with\nstructured Product Requirement Document (PRD) requirements, comprehensive\nevaluation criteria, and reference implementations. PRDBench features rich data\nsources, high task complexity, and flexible metrics. We further employ an\nAgent-as-a-Judge paradigm to score agent outputs, enabling the evaluation of\nvarious test types beyond unit tests. Extensive experiments on PRDBench\ndemonstrate its effectiveness in assessing the capabilities of both code agents\nand evaluation agents, providing a scalable and robust framework for annotation\nand evaluation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ee3\u7406\u9a71\u52a8\u7684\u57fa\u51c6\u6784\u5efa\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u4e86\u5305\u542b50\u4e2a\u771f\u5b9ePython\u9879\u76ee\u7684PRDBench\u57fa\u51c6\uff0c\u652f\u6301\u591a\u7ef4\u5ea6\u8bc4\u6d4b\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u4ee3\u7406\u7684\u8bc4\u6d4b\u57fa\u51c6\u5b58\u5728\u6807\u6ce8\u6210\u672c\u9ad8\u4e14\u4f9d\u8d56\u5355\u4e00\u6d4b\u8bd5\u8bc4\u4ef7\u6307\u6807\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165\u4ee3\u7406\u9a71\u52a8\u7684\u57fa\u51c6\u6784\u5efa\u6d41\u7a0b\u548cAgent-as-a-Judge\u8bc4\u5206\u8303\u5f0f\uff0c\u751f\u6210\u591a\u6837\u4e14\u590d\u6742\u7684\u9879\u76ee\u7ea7\u4efb\u52a1\uff0c\u5e76\u91c7\u7528\u7075\u6d3b\u591a\u6837\u7684\u8bc4\u6d4b\u6807\u51c6\u3002", "result": "PRDBench\u6db5\u76d620\u4e2a\u9886\u57df\u7684\u771f\u5b9e\u9879\u76ee\uff0c\u652f\u6301\u4e30\u5bcc\u6570\u636e\u6e90\u548c\u590d\u6742\u4efb\u52a1\uff0c\u80fd\u6709\u6548\u8bc4\u4f30\u591a\u79cd\u4ee3\u7801\u4ee3\u7406\u53ca\u8bc4\u6d4b\u4ee3\u7406\u7684\u80fd\u529b\u3002", "conclusion": "PRDBench\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u9c81\u68d2\u7684\u9879\u76ee\u7ea7\u4ee3\u7801\u4ee3\u7406\u8bc4\u6d4b\u6846\u67b6\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u57fa\u51c6\u7684\u4e0d\u8db3\uff0c\u63a8\u52a8\u4ee3\u7801\u4ee3\u7406\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.23896", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23896", "abs": "https://arxiv.org/abs/2510.23896", "authors": ["Kosei Uemura", "Miaoran Zhang", "David Ifeoluwa Adelani"], "title": "AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages", "comment": null, "summary": "Text embeddings are an essential building component of several NLP tasks such\nas retrieval-augmented generation which is crucial for preventing\nhallucinations in LLMs. Despite the recent release of massively multilingual\nMTEB (MMTEB), African languages remain underrepresented, with existing tasks\noften repurposed from translation benchmarks such as FLORES clustering or\nSIB-200. In this paper, we introduce AfriMTEB -- a regional expansion of MMTEB\ncovering 59 languages, 14 tasks, and 38 datasets, including six newly added\ndatasets. Unlike many MMTEB datasets that include fewer than five languages,\nthe new additions span 14 to 56 African languages and introduce entirely new\ntasks, such as hate speech detection, intent detection, and emotion\nclassification, which were not previously covered. Complementing this, we\npresent AfriE5, an adaptation of the instruction-tuned mE5 model to African\nlanguages through cross-lingual contrastive distillation. Our evaluation shows\nthat AfriE5 achieves state-of-the-art performance, outperforming strong\nbaselines such as Gemini-Embeddings and mE5.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86AfriMTEB\uff0c\u4e00\u4e2a\u8986\u76d6\u975e\u6d3259\u79cd\u8bed\u8a00\u548c\u591a\u79cdNLP\u4efb\u52a1\u7684\u65b0\u578b\u591a\u8bed\u8a00\u8bc4\u6d4b\u57fa\u51c6\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u9488\u5bf9\u975e\u6d32\u8bed\u8a00\u4f18\u5316\u7684AfriE5\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u8bed\u8a00\u6587\u672c\u5d4c\u5165\u8bc4\u6d4b\u57fa\u51c6\u4e2d\u975e\u6d32\u8bed\u8a00\u4e25\u91cd\u4e0d\u8db3\uff0c\u4e14\u7f3a\u4e4f\u9488\u5bf9\u975e\u6d32\u8bed\u8a00\u7684\u65b0\u4efb\u52a1\uff0c\u8fd9\u9650\u5236\u4e86\u975e\u6d32\u8bed\u8a00NLP\u6280\u672f\u7684\u53d1\u5c55\u3002", "method": "\u6784\u5efaAfriMTEB\u57fa\u51c6\uff0c\u6db5\u76d659\u79cd\u975e\u6d32\u8bed\u8a00\u548c14\u4e2a\u4efb\u52a1\uff0c\u65b0\u589e\u591a\u79cd\u4efb\u52a1\u5982\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u3001\u610f\u56fe\u68c0\u6d4b\u548c\u60c5\u611f\u5206\u7c7b\uff1b\u540c\u65f6\u901a\u8fc7\u8de8\u8bed\u8a00\u5bf9\u6bd4\u84b8\u998f\u65b9\u6cd5\u5bf9instruction-tuned\u7684mE5\u6a21\u578b\u8fdb\u884c\u9002\u914d\uff0c\u5f97\u5230AfriE5\u6a21\u578b\u3002", "result": "AfriE5\u5728\u975e\u6d32\u8bed\u8a00\u6587\u672c\u5d4c\u5165\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86Gemini-Embeddings\u548c\u539f\u59cbmE5\u7b49\u5f3a\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "AfriMTEB\u548cAfriE5\u4e3a\u975e\u6d32\u8bed\u8a00\u7684NLP\u7814\u7a76\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u4fc3\u8fdb\u4e86\u975e\u6d32\u8bed\u8a00\u6587\u672c\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.24367", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24367", "abs": "https://arxiv.org/abs/2510.24367", "authors": ["Junda He", "Jieke Shi", "Terry Yue Zhuo", "Christoph Treude", "Jiamou Sun", "Zhenchang Xing", "Xiaoning Du", "David Lo"], "title": "LLM-as-a-Judge for Software Engineering: Literature Review, Vision, and the Road Ahead", "comment": null, "summary": "The rapid integration of Large Language Models (LLMs) into software\nengineering (SE) has revolutionized tasks like code generation, producing a\nmassive volume of software artifacts. This surge has exposed a critical\nbottleneck: the lack of scalable, reliable methods to evaluate these outputs.\nHuman evaluation is costly and time-consuming, while traditional automated\nmetrics like BLEU fail to capture nuanced quality aspects. In response, the\nLLM-as-a-Judge paradigm - using LLMs for automated evaluation - has emerged.\nThis approach leverages the advanced reasoning of LLMs, offering a path toward\nhuman-like nuance at automated scale. However, LLM-as-a-Judge research in SE is\nstill in its early stages. This forward-looking SE 2030 paper aims to steer the\ncommunity toward advancing LLM-as-a-Judge for evaluating LLM-generated software\nartifacts. We provide a literature review of existing SE studies, analyze their\nlimitations, identify key research gaps, and outline a detailed roadmap. We\nenvision these frameworks as reliable, robust, and scalable human surrogates\ncapable of consistent, multi-faceted artifact evaluation by 2030. Our work aims\nto foster research and adoption of LLM-as-a-Judge frameworks, ultimately\nimproving the scalability of software artifact evaluation.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u4f5c\u4e3a\u81ea\u52a8\u8bc4\u4f30\u5de5\u5177\u6765\u89e3\u51b3\u4eba\u5de5\u8bc4\u4f30\u6210\u672c\u9ad8\u548c\u4f20\u7edf\u6307\u6807\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u968f\u7740LLMs\u751f\u6210\u5927\u91cf\u8f6f\u4ef6\u5de5\u4ef6\uff0c\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u6548\u7387\u4f4e\u4e14\u51c6\u786e\u5ea6\u6709\u9650\uff0c\u4e9f\u9700\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u53ef\u9760\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u4f9b\u4e86\u5bf9\u73b0\u6709\u8f6f\u4ef6\u5de5\u7a0b\u4e2dLLM\u4f5c\u4e3a\u8bc4\u5224\u5de5\u5177\u7684\u7814\u7a76\u7efc\u8ff0\uff0c\u5206\u6790\u73b0\u5b58\u4e0d\u8db3\uff0c\u8bc6\u522b\u5173\u952e\u7814\u7a76\u7a7a\u767d\uff0c\u5e76\u63d0\u51fa\u672a\u6765\u53d1\u5c55\u8def\u7ebf\u56fe\u3002", "result": "\u7814\u7a76\u8868\u660e\u73b0\u6709\u65b9\u6cd5\u5c1a\u5904\u4e8e\u521d\u7ea7\u9636\u6bb5\uff0c\u5c1a\u672a\u5b9e\u73b0\u5b8c\u5168\u53ef\u9760\u4e0e\u591a\u7ef4\u5ea6\u8bc4\u4f30\u3002", "conclusion": "\u672c\u6587\u5c55\u671b\u52302030\u5e74\uff0cLLM\u4f5c\u4e3a\u8bc4\u5224\u5de5\u5177\u5c06\u6210\u4e3a\u4eba\u7c7b\u8bc4\u4f30\u7684\u6709\u6548\u66ff\u4ee3\uff0c\u63a8\u52a8\u8f6f\u4ef6\u5de5\u4ef6\u8bc4\u4f30\u7684\u89c4\u6a21\u5316\u548c\u591a\u6837\u5316\u3002"}}
{"id": "2510.23921", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23921", "abs": "https://arxiv.org/abs/2510.23921", "authors": ["Kaveh Eskandari Miandoab", "Mahammed Kamruzzaman", "Arshia Gharooni", "Gene Louis Kim", "Vasanth Sarathy", "Ninareh Mehrabi"], "title": "Breaking the Benchmark: Revealing LLM Bias via Minimal Contextual Augmentation", "comment": "9 pages, 3 figures, 3 tables", "summary": "Large Language Models have been shown to demonstrate stereotypical biases in\ntheir representations and behavior due to the discriminative nature of the data\nthat they have been trained on. Despite significant progress in the development\nof methods and models that refrain from using stereotypical information in\ntheir decision-making, recent work has shown that approaches used for bias\nalignment are brittle. In this work, we introduce a novel and general\naugmentation framework that involves three plug-and-play steps and is\napplicable to a number of fairness evaluation benchmarks. Through application\nof augmentation to a fairness evaluation dataset (Bias Benchmark for Question\nAnswering (BBQ)), we find that Large Language Models (LLMs), including\nstate-of-the-art open and closed weight models, are susceptible to\nperturbations to their inputs, showcasing a higher likelihood to behave\nstereotypically. Furthermore, we find that such models are more likely to have\nbiased behavior in cases where the target demographic belongs to a community\nless studied by the literature, underlining the need to expand the fairness and\nsafety research to include more diverse communities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u6570\u636e\u589e\u5f3a\u6846\u67b6\uff0c\u7528\u4e8e\u516c\u5e73\u6027\u8bc4\u4f30\uff0c\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bb9\u6613\u53d7\u5230\u8f93\u5165\u6270\u52a8\u5f71\u54cd\uff0c\u4ece\u800c\u8868\u73b0\u51fa\u523b\u677f\u504f\u89c1\uff0c\u5c24\u5176\u662f\u5728\u8f83\u5c11\u7814\u7a76\u7684\u7fa4\u4f53\u4e0a\u8868\u73b0\u66f4\u4e3a\u660e\u663e\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u56e0\u8bad\u7ec3\u6570\u636e\u7684\u6b67\u89c6\u6027\u5b58\u5728\u523b\u677f\u504f\u89c1\uff0c\u4e14\u73b0\u6709\u7684\u504f\u89c1\u6821\u6b63\u65b9\u6cd5\u7f3a\u4e4f\u9c81\u68d2\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u3001\u901a\u7528\u7684\u516c\u5e73\u6027\u589e\u5f3a\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5305\u542b\u4e09\u6b65\u7684\u5373\u63d2\u5373\u7528\u6570\u636e\u589e\u5f3a\u6846\u67b6\uff0c\u5e94\u7528\u4e8e\u516c\u5e73\u6027\u8bc4\u4f30\u57fa\u51c6\u6570\u636e\u96c6BBQ\uff0c\u901a\u8fc7\u5bf9\u8f93\u5165\u8fdb\u884c\u6270\u52a8\u6765\u8bc4\u4f30\u6a21\u578b\u7684\u504f\u89c1\u8868\u73b0\u3002", "result": "\u53d1\u73b0\u5305\u62ec\u6700\u65b0\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\u5728\u5185\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8f93\u5165\u6270\u52a8\u4e0b\u66f4\u53ef\u80fd\u8868\u73b0\u51fa\u523b\u677f\u504f\u89c1\uff0c\u5e76\u4e14\u9488\u5bf9\u6587\u732e\u4e2d\u8f83\u5c11\u7814\u7a76\u7684\u7fa4\u4f53\u504f\u89c1\u66f4\u4e25\u91cd\u3002", "conclusion": "\u8be5\u589e\u5f3a\u6846\u67b6\u6709\u6548\u63ed\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u504f\u89c1\u8106\u5f31\u6027\uff0c\u5f3a\u8c03\u4e86\u5c06\u516c\u5e73\u6027\u4e0e\u5b89\u5168\u7814\u7a76\u62d3\u5c55\u5230\u66f4\u591a\u591a\u6837\u5316\u793e\u533a\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2510.24428", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24428", "abs": "https://arxiv.org/abs/2510.24428", "authors": ["Nguyen Hoang Anh", "Minh Le-Anh", "Bach Le", "Nghi D. Q. Bui"], "title": "CodeWiki: Automated Repository-Level Documentation at Scale", "comment": null, "summary": "Developers spend nearly 58% of their time understanding codebases, yet\nmaintaining comprehensive documentation remains challenging due to complexity\nand manual effort. While recent Large Language Models (LLMs) show promise for\nfunction-level documentation, they fail at the repository level, where\ncapturing architectural patterns and cross-module interactions is essential. We\nintroduce CodeWiki, the first open-source framework for holistic\nrepository-level documentation across seven programming languages. CodeWiki\nemploys three innovations: (i) hierarchical decomposition that preserves\narchitectural context, (ii) recursive agentic processing with dynamic\ndelegation, and (iii) synthesis of textual and visual artifacts including\narchitecture diagrams and data flows. We also present CodeWikiBench, the first\nrepository-level documentation benchmark with multi-level rubrics and agentic\nassessment. CodeWiki achieves 68.79% quality score with proprietary models and\n64.80% with open-source alternatives, outperforming existing closed-source\nsystems and demonstrating scalable, accurate documentation for real-world\nrepositories.", "AI": {"tldr": "CodeWiki\u662f\u4e00\u4e2a\u5f00\u6e90\u6846\u67b6\uff0c\u65e8\u5728\u81ea\u52a8\u751f\u6210\u8de8\u4e03\u79cd\u7f16\u7a0b\u8bed\u8a00\u7684\u6574\u4ed3\u5e93\u7ea7\u522b\u6587\u6863\uff0c\u901a\u8fc7\u5c42\u6b21\u5206\u89e3\u3001\u9012\u5f52\u4ee3\u7406\u5904\u7406\u548c\u6587\u672c\u52a0\u89c6\u89c9\u7efc\u5408\u63d0\u9ad8\u6587\u6863\u8d28\u91cf\uff0c\u4f18\u4e8e\u73b0\u6709\u7cfb\u7edf\u3002", "motivation": "\u5f00\u53d1\u8005\u82b1\u8d39\u5927\u91cf\u65f6\u95f4\u7406\u89e3\u4ee3\u7801\u5e93\uff0c\u4f46\u73b0\u6709\u81ea\u52a8\u6587\u6863\u5de5\u5177\u4e3b\u8981\u805a\u7126\u4e8e\u51fd\u6570\u7ea7\uff0c\u65e0\u6cd5\u6709\u6548\u6355\u6349\u4ed3\u5e93\u5c42\u9762\u7684\u67b6\u6784\u6a21\u5f0f\u548c\u8de8\u6a21\u5757\u4ea4\u4e92\uff0c\u5bfc\u81f4\u6587\u6863\u7ef4\u62a4\u56f0\u96be\u3002", "method": "CodeWiki\u5f15\u5165\u5c42\u6b21\u5206\u89e3\u4fdd\u5b58\u67b6\u6784\u4e0a\u4e0b\u6587\uff0c\u9012\u5f52\u4ee3\u7406\u5904\u7406\u673a\u5236\u7ed3\u5408\u52a8\u6001\u4efb\u52a1\u5206\u914d\uff0c\u4ee5\u53ca\u6587\u672c\u548c\u67b6\u6784\u56fe\u3001\u6570\u636e\u6d41\u7b49\u89c6\u89c9\u4ea7\u7269\u7684\u7efc\u5408\u751f\u6210\uff0c\u652f\u6301\u4e03\u79cd\u7f16\u7a0b\u8bed\u8a00\u3002\u5e76\u8bbe\u8ba1CodeWikiBench\u4f5c\u4e3a\u8bc4\u4ef7\u57fa\u51c6\u3002", "result": "CodeWiki\u4f7f\u7528\u4e13\u6709\u6a21\u578b\u8fbe\u523068.79%\u7684\u6587\u6863\u8d28\u91cf\u5206\uff0c\u5f00\u6e90\u6a21\u578b\u8fbe\u523064.80%\uff0c\u5747\u4f18\u4e8e\u73b0\u6709\u95ed\u6e90\u7cfb\u7edf\uff0c\u8868\u73b0\u51fa\u5bf9\u771f\u5b9e\u4ed3\u5e93\u7684\u826f\u597d\u6269\u5c55\u6027\u548c\u51c6\u786e\u6027\u3002", "conclusion": "CodeWiki\u5b9e\u73b0\u4e86\u8de8\u8bed\u8a00\u3001\u5168\u4ed3\u5e93\u8303\u56f4\u7684\u9ad8\u8d28\u91cf\u81ea\u52a8\u6587\u6863\u751f\u6210\uff0c\u4e3a\u89e3\u51b3\u590d\u6742\u8f6f\u4ef6\u7cfb\u7edf\u7684\u7ef4\u62a4\u548c\u7406\u89e3\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2510.23924", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23924", "abs": "https://arxiv.org/abs/2510.23924", "authors": ["Dina Pisarevskaya", "Arkaitz Zubiaga"], "title": "Agent-based Automated Claim Matching with Instruction-following LLMs", "comment": "Accepted for the International Joint Conference on Natural Language\n  Processing & Asia-Pacific Chapter of the Association for Computational\n  Linguistics (2025) Findings", "summary": "We present a novel agent-based approach for the automated claim matching task\nwith instruction-following LLMs. We propose a two-step pipeline that first\ngenerates prompts with LLMs, to then perform claim matching as a binary\nclassification task with LLMs. We demonstrate that LLM-generated prompts can\noutperform SOTA with human-generated prompts, and that smaller LLMs can do as\nwell as larger ones in the generation process, allowing to save computational\nresources. We also demonstrate the effectiveness of using different LLMs for\neach step of the pipeline, i.e. using an LLM for prompt generation, and another\nfor claim matching. Our investigation into the prompt generation process in\nturn reveals insights into the LLMs' understanding of claim matching.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6307\u4ee4\u8ddf\u968f\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u7684\u81ea\u52a8\u5316\u8bba\u65ad\u5339\u914d\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u6b65\u6d41\u6c34\u7ebf\u751f\u6210\u63d0\u793a\u5e76\u5206\u7c7b\u8bba\u65ad\uff0c\u7ed3\u679c\u663e\u793aLLM\u751f\u6210\u7684\u63d0\u793a\u4f18\u4e8e\u4eba\u5de5\u63d0\u793a\uff0c\u4e14\u5c0f\u578bLLM\u8868\u73b0\u4e0d\u900a\u4e8e\u5927\u578bLLM\uff0c\u8282\u7701\u8ba1\u7b97\u8d44\u6e90\u3002", "motivation": "\u73b0\u6709\u8bba\u65ad\u5339\u914d\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u63d0\u793a\u6216\u5927\u578b\u6a21\u578b\uff0c\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u5927\uff0c\u4e14\u63d0\u793a\u8d28\u91cf\u5f71\u54cd\u6548\u679c\uff0c\u8feb\u5207\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u81ea\u52a8\u5316\u63d0\u793a\u751f\u6210\u53ca\u5339\u914d\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e24\u6b65\u6d41\u6c34\u7ebf\uff0c\u7b2c\u4e00\u6b65\u7528LLM\u81ea\u52a8\u751f\u6210\u63d0\u793a\uff0c\u7b2c\u4e8c\u6b65\u7528LLM\u8fdb\u884c\u4e8c\u5206\u7c7b\u8bba\u65ad\u5339\u914d\uff1b\u63a2\u7d22\u4e0d\u540cLLM\u7ec4\u5408\u4f7f\u7528\u4ee5\u63d0\u5347\u6548\u7387\u548c\u6548\u679c\u3002", "result": "LLM\u81ea\u52a8\u751f\u6210\u63d0\u793a\u4f18\u4e8e\u4eba\u5de5\u751f\u6210\u63d0\u793a\uff0c\u5c0f\u578bLLM\u5728\u63d0\u793a\u751f\u6210\u4e2d\u8868\u73b0\u4e0d\u6bd4\u5927\u578bLLM\u5dee\uff1b\u4e0d\u540c\u6a21\u578b\u7ec4\u5408\u4f7f\u7528\u663e\u793a\u53ef\u4f18\u5316\u6027\u80fd\u4e0e\u8d44\u6e90\u6d88\u8017\u3002", "conclusion": "\u81ea\u52a8\u751f\u6210\u7684\u63d0\u793a\u7ed3\u5408\u4e0d\u540cLLM\u642d\u914d\u80fd\u6709\u6548\u63d0\u5347\u8bba\u65ad\u5339\u914d\u4efb\u52a1\uff0c\u8282\u7701\u8d44\u6e90\u5e76\u63d0\u4f9b\u5bf9LLM\u7406\u89e3\u80fd\u529b\u7684\u6d1e\u5bdf\u3002"}}
{"id": "2510.24483", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24483", "abs": "https://arxiv.org/abs/2510.24483", "authors": ["Michele Lanza"], "title": "The Divine Software Engineering Comedy -- Inferno: The Okinawa Files", "comment": null, "summary": "In June 2024 I co-organized the FUture of Software Engineering symposium in\nOkinawa, Japan. Me, Andrian Marcus, Takashi Kobayashi and Shinpei Hayashi were\ngeneral chairs, Nicole Novielli, Kevin Moran, Yutaro Kashiwa and Masanari Kondo\nwere program chairs, some members of my group, Carmen Armenti, Stefano\nCampanella, Roberto Minelli, were the tables, can't have a room with only\nchairs, after all. We invited a crowd of people to discuss what future software\nengineering has. FUSE became a 3-day marathon on whether there is actually a\nfuture at all for SE. This essay is a slightly dark take about what I saw at\nthat event, very loosely based on the discussions that took place, adding some\nhealthy sarcasm and cynicism, the intellectual salt and pepper I never seem to\nrun out of. I listened to the brilliant people who gathered to talk about where\nwe're headed, and distilled three nightmares headed in our direction: software\nmakers who don't know what they're doing, but get the job done anyway, a field\nmoving so fast it can't remember its own lessons, and technologies multiplying\nlike rabbits in Spring. So, let's start. The future, eh? The future of software\nengineering looks like a car crash in slow motion: you can see it coming but\nyou can't look away. The thing is...", "AI": {"tldr": "\u672c\u6587\u662f\u4f5c\u8005\u5bf92024\u5e74\u5728\u65e5\u672c\u51b2\u7ef3\u4e3e\u529e\u7684\u8f6f\u4ef6\u5de5\u7a0b\u672a\u6765\u7814\u8ba8\u4f1a\u7684\u53cd\u601d\u4e0e\u89c2\u5bdf\u3002", "motivation": "\u7814\u8ba8\u4f1a\u65e8\u5728\u63a2\u8ba8\u8f6f\u4ef6\u5de5\u7a0b\u7684\u672a\u6765\u53ca\u5176\u53d1\u5c55\u65b9\u5411\u3002", "method": "\u901a\u8fc7\u7ec4\u7ec7\u548c\u53c2\u4e0e\u591a\u4f4d\u4e13\u5bb6\u7684\u8ba8\u8bba\uff0c\u6536\u96c6\u5e76\u5206\u6790\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u672a\u6765\u7684\u4e0d\u540c\u89c2\u70b9\u3002", "result": "\u603b\u7ed3\u51fa\u8f6f\u4ef6\u5de5\u7a0b\u9762\u4e34\u7684\u4e09\u5927\u201c\u5669\u68a6\u201d\uff1a\u4e00\u662f\u8f6f\u4ef6\u5f00\u53d1\u8005\u6280\u80fd\u4e0d\u8db3\u5374\u80fd\u5b8c\u6210\u5de5\u4f5c\uff0c\u4e8c\u662f\u9886\u57df\u53d1\u5c55\u8fc7\u5feb\u96be\u4ee5\u5438\u53d6\u6559\u8bad\uff0c\u4e09\u662f\u6280\u672f\u5feb\u901f\u589e\u6b96\u5e26\u6765\u7684\u6311\u6218\u3002", "conclusion": "\u8f6f\u4ef6\u5de5\u7a0b\u7684\u672a\u6765\u5145\u6ee1\u4e0d\u786e\u5b9a\u6027\u548c\u98ce\u9669\uff0c\u867d\u7136\u4ee4\u4eba\u62c5\u5fe7\uff0c\u4f46\u8fd9\u79cd\u8ba8\u8bba\u6709\u52a9\u4e8e\u6b63\u89c6\u95ee\u9898\u5e76\u63a8\u52a8\u9886\u57df\u8fdb\u6b65\u3002"}}
{"id": "2510.23941", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23941", "abs": "https://arxiv.org/abs/2510.23941", "authors": ["Soham Satyadharma", "Fatemeh Sheikholeslami", "Swati Kaul", "Aziz Umit Batur", "Suleiman A. Khan"], "title": "Auto prompting without training labels: An LLM cascade for product quality assessment in e-commerce catalogs", "comment": null, "summary": "We introduce a novel, training free cascade for auto-prompting Large Language\nModels (LLMs) to assess product quality in e-commerce. Our system requires no\ntraining labels or model fine-tuning, instead automatically generating and\nrefining prompts for evaluating attribute quality across tens of thousands of\nproduct category-attribute pairs. Starting from a seed of human-crafted\nprompts, the cascade progressively optimizes instructions to meet\ncatalog-specific requirements. This approach bridges the gap between general\nlanguage understanding and domain-specific knowledge at scale in complex\nindustrial catalogs. Our extensive empirical evaluations shows the auto-prompt\ncascade improves precision and recall by $8-10\\%$ over traditional\nchain-of-thought prompting. Notably, it achieves these gains while reducing\ndomain expert effort from 5.1 hours to 3 minutes per attribute - a $99\\%$\nreduction. Additionally, the cascade generalizes effectively across five\nlanguages and multiple quality assessment tasks, consistently maintaining\nperformance gains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u81ea\u52a8\u63d0\u793a\u7ea7\u8054\u65b9\u6cd5\uff0c\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u7535\u5546\u4ea7\u54c1\u8d28\u91cf\uff0c\u5b9e\u73b0\u51c6\u786e\u7387\u548c\u53ec\u56de\u7387\u63d0\u53478-10%\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\u5728\u5927\u89c4\u6a21\u590d\u6742\u5de5\u4e1a\u76ee\u5f55\u4e2d\uff0c\u7f3a\u4e4f\u9488\u5bf9\u7279\u5b9a\u9886\u57df\u77e5\u8bc6\u548c\u9ad8\u6548\u6027\u7684\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u4eba\u5de5\u8bbe\u8ba1\u7684\u521d\u59cb\u63d0\u793a\uff0c\u81ea\u52a8\u751f\u6210\u548c\u4f18\u5316\u63d0\u793a\u8bed\uff0c\u4ee5\u9002\u5e94\u5404\u7c7b\u76ee\u5c5e\u6027\u7684\u8d28\u91cf\u8bc4\u4ef7\u9700\u6c42\uff0c\u5f62\u6210\u8bad\u7ec3\u81ea\u7531\u7684\u81ea\u52a8\u63d0\u793a\u7ea7\u8054\u3002", "result": "\u8be5\u65b9\u6cd5\u76f8\u6bd4\u4f20\u7edf\u94fe\u5f0f\u601d\u7ef4\u63d0\u5347\u4e868-10%\u7684\u51c6\u786e\u7387\u548c\u53ec\u56de\u7387\uff0c\u540c\u65f6\u5c06\u9886\u57df\u4e13\u5bb6\u7684\u5de5\u4f5c\u65f6\u95f4\u4ece5.1\u5c0f\u65f6\u51cf\u5c11\u81f33\u5206\u949f\uff0c\u5b9e\u73b099%\u7684\u6548\u7387\u63d0\u5347\u3002\u5e76\u4e14\u80fd\u591f\u8de8\u8bed\u8a00\u548c\u591a\u4efb\u52a1\u6cdb\u5316\u3002", "conclusion": "\u81ea\u52a8\u63d0\u793a\u7ea7\u8054\u65b9\u6cd5\u6709\u6548\u6865\u63a5\u4e86\u901a\u7528\u8bed\u8a00\u7406\u89e3\u548c\u9886\u57df\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u4ea7\u54c1\u8d28\u91cf\u8bc4\u4f30\u7684\u6548\u7387\u4e0e\u51c6\u786e\u6027\uff0c\u5177\u5907\u826f\u597d\u7684\u591a\u8bed\u8a00\u548c\u591a\u4efb\u52a1\u9002\u7528\u6027\u3002"}}
{"id": "2510.24706", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24706", "abs": "https://arxiv.org/abs/2510.24706", "authors": ["Shuqing Li", "Jiayi Yan", "Chenyu Niu", "Jen-tse Huang", "Yun Peng", "Wenxuan Wang", "Yepang Liu", "Michael R. Lyu"], "title": "ComboBench: Can LLMs Manipulate Physical Devices to Play Virtual Reality Games?", "comment": null, "summary": "Virtual Reality (VR) games require players to translate high-level semantic\nactions into precise device manipulations using controllers and head-mounted\ndisplays (HMDs). While humans intuitively perform this translation based on\ncommon sense and embodied understanding, whether Large Language Models (LLMs)\ncan effectively replicate this ability remains underexplored. This paper\nintroduces a benchmark, ComboBench, evaluating LLMs' capability to translate\nsemantic actions into VR device manipulation sequences across 262 scenarios\nfrom four popular VR games: Half-Life: Alyx, Into the Radius, Moss: Book II,\nand Vivecraft. We evaluate seven LLMs, including GPT-3.5, GPT-4, GPT-4o,\nGemini-1.5-Pro, LLaMA-3-8B, Mixtral-8x7B, and GLM-4-Flash, compared against\nannotated ground truth and human performance. Our results reveal that while\ntop-performing models like Gemini-1.5-Pro demonstrate strong task decomposition\ncapabilities, they still struggle with procedural reasoning and spatial\nunderstanding compared to humans. Performance varies significantly across\ngames, suggesting sensitivity to interaction complexity. Few-shot examples\nsubstantially improve performance, indicating potential for targeted\nenhancement of LLMs' VR manipulation capabilities. We release all materials at\nhttps://sites.google.com/view/combobench.", "AI": {"tldr": "\u672c\u6587\u8bbe\u8ba1\u4e86ComboBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u5c06\u8bed\u4e49\u52a8\u4f5c\u8f6c\u6362\u4e3a\u865a\u62df\u73b0\u5b9e\u8bbe\u5907\u64cd\u4f5c\u5e8f\u5217\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u9876\u7ea7\u6a21\u578b\u867d\u8868\u73b0\u4f18\u79c0\uff0c\u4f46\u5728\u7a0b\u5e8f\u63a8\u7406\u548c\u7a7a\u95f4\u7406\u89e3\u4e0a\u4ecd\u900a\u4e8e\u4eba\u7c7b\u3002", "motivation": "\u865a\u62df\u73b0\u5b9e\u6e38\u620f\u4e2d\u73a9\u5bb6\u9700\u5c06\u9ad8\u5c42\u8bed\u4e49\u52a8\u4f5c\u7cbe\u51c6\u8f6c\u6362\u4e3a\u8bbe\u5907\u64cd\u4f5c\uff0c\u7136\u800c\u73b0\u6709\u7814\u7a76\u5c1a\u672a\u5145\u5206\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8fd9\u65b9\u9762\u7684\u6548\u80fd\u3002", "method": "\u6784\u5efa\u5305\u542b262\u573a\u666f\u7684ComboBench\uff0c\u6db5\u76d6\u56db\u6b3e\u4e3b\u6d41VR\u6e38\u620f\uff0c\u901a\u8fc7\u5bf9\u6bd4\u4e03\u4e2aLLM\u53ca\u4eba\u7c7b\u8868\u73b0\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u8bed\u4e49\u52a8\u4f5c\u8f6c\u8bbe\u5907\u64cd\u4f5c\u7684\u80fd\u529b\u3002", "result": "Gemini-1.5-Pro\u8868\u73b0\u6700\u4f73\uff0c\u663e\u793a\u826f\u597d\u7684\u4efb\u52a1\u5206\u89e3\u80fd\u529b\uff0c\u4f46\u603b\u4f53\u5728\u7a0b\u5e8f\u63a8\u7406\u548c\u7a7a\u95f4\u7406\u89e3\u65b9\u9762\u4ecd\u4e0d\u53ca\u4eba\u7c7b\uff1b\u4e0d\u540c\u6e38\u620f\u95f4\u8868\u73b0\u5dee\u5f02\u660e\u663e\uff0c\u5c11\u6837\u672c\u5b66\u4e60\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "LLMs\u5728VR\u8bbe\u5907\u64cd\u4f5c\u8bed\u4e49\u8f6c\u6362\u5177\u6f5c\u529b\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u63d0\u5347\u63a8\u7406\u548c\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\uff0c\u5c11\u6837\u672c\u5b66\u4e60\u4e3a\u589e\u5f3a\u8be5\u80fd\u529b\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2510.23946", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23946", "abs": "https://arxiv.org/abs/2510.23946", "authors": ["Tananun Songdechakraiwut"], "title": "Leveraging LLMs for Early Alzheimer's Prediction", "comment": null, "summary": "We present a connectome-informed LLM framework that encodes dynamic fMRI\nconnectivity as temporal sequences, applies robust normalization, and maps\nthese data into a representation suitable for a frozen pre-trained LLM for\nclinical prediction. Applied to early Alzheimer's detection, our method\nachieves sensitive prediction with error rates well below clinically recognized\nmargins, with implications for timely Alzheimer's intervention.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8111\u8fde\u63a5\u7ec4\u4fe1\u606f\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001fMRI\u8fde\u63a5\u6027\u6570\u636e\u8fdb\u884c\u65e9\u671f\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u4e34\u5e8a\u9884\u6d4b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7075\u654f\u5ea6\u548c\u4f4e\u8bef\u5dee\u7387\u3002", "motivation": "\u5f53\u524d\u65e9\u671f\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u4e34\u5e8a\u9884\u6d4b\u9762\u4e34\u51c6\u786e\u7387\u548c\u53ca\u65f6\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4e9f\u9700\u5229\u7528\u8111\u8fde\u63a5\u7ec4\u52a8\u6001\u4fe1\u606f\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002", "method": "\u5c06\u52a8\u6001fMRI\u8fde\u63a5\u6027\u6570\u636e\u7f16\u7801\u4e3a\u65f6\u95f4\u5e8f\u5217\uff0c\u91c7\u7528\u9c81\u68d2\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u5e76\u6620\u5c04\u5230\u9884\u8bad\u7ec3\u51bb\u7ed3\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8868\u793a\u7a7a\u95f4\u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u65e9\u671f\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u68c0\u6d4b\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u7075\u654f\u5ea6\uff0c\u9519\u8bef\u7387\u8fdc\u4f4e\u4e8e\u4e34\u5e8a\u8ba4\u53ef\u7684\u9608\u503c\u3002", "conclusion": "\u57fa\u4e8e\u8fde\u63a5\u7ec4\u4fe1\u606f\u7684LLM\u6846\u67b6\u4e3a\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u65e9\u671f\u53ca\u65f6\u5e72\u9884\u63d0\u4f9b\u4e86\u53ef\u884c\u4e14\u6709\u6548\u7684\u4e34\u5e8a\u9884\u6d4b\u5de5\u5177\u3002"}}
{"id": "2510.23949", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23949", "abs": "https://arxiv.org/abs/2510.23949", "authors": ["Kyomin Hwang", "Hyeonjin Kim", "Seungyeon Kim", "Sunghyun Wee", "Nojun Kwak"], "title": "Uncovering the Potential Risks in Unlearning: Danger of English-only Unlearning in Multilingual LLMs", "comment": null, "summary": "There have been a couple of studies showing that attempting to erase\nmultilingual knowledge using only English data is insufficient for multilingual\nLLMs. However, their analyses remain highly performance-oriented. In this\npaper, we switch the point of view to evaluation, and address an additional\nblind spot which reveals itself when the multilingual LLM is fully finetuned\nwith parallel multilingual dataset before unlearning. Here, language confusion\noccurs whereby a model responds in language different from that of the input\nprompt. Language confusion is a problematic phenomenon in unlearning, causing\nthe standard reference-based metrics to fail. We tackle this phenomenon in\nthree steps: (1) introduce N-gram-based Language-Mix (N-Mix) score to\nquantitatively show the language confusion is pervasive and consistent in\nmultilingual LLMs, (2) demonstrate that reference-based metrics result in false\nnegatives when N-Mix score is high, and(3) suggest the need of new type of\nunlearning evaluation that can directly assess the content of the generated\nsentences. We call this type of metrics as semantic-based metric.", "AI": {"tldr": "\u672c\u6587\u53d1\u73b0\u591a\u8bed\u8a00\u5927\u6a21\u578b\u5728\u5fae\u8c03\u540e\u5b58\u5728\u8bed\u8a00\u6df7\u6dc6\u73b0\u8c61\uff0c\u4e14\u6807\u51c6\u8bc4\u4ef7\u6307\u6807\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u6a21\u578b\u7684\u9057\u5fd8\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u591a\u8bed\u8a00\u6a21\u578b\u7684\u9057\u5fd8\u7814\u7a76\u4e3b\u8981\u57fa\u4e8e\u6027\u80fd\u6307\u6807\uff0c\u5ffd\u89c6\u8bed\u8a00\u6df7\u6dc6\u73b0\u8c61\uff0c\u5bfc\u81f4\u8bc4\u4ef7\u7ed3\u679c\u4e0d\u51c6\u786e\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eN-gram\u7684\u8bed\u8a00\u6df7\u6dc6\u8bc4\u5206\uff08N-Mix\uff09\u91cf\u5316\u8bed\u8a00\u6df7\u6dc6\uff0c\u5206\u6790\u6807\u51c6\u53c2\u8003\u6307\u6807\u5728\u9ad8N-Mix\u60c5\u51b5\u4e0b\u8bef\u5224\u7684\u539f\u56e0\uff0c\u5e76\u5021\u5bfc\u8bed\u4e49\u5185\u5bb9\u8bc4\u4f30\u7684\u65b0\u578b\u9057\u5fd8\u8bc4\u4ef7\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u591a\u8bed\u8a00\u5927\u6a21\u578b\u5e7f\u6cdb\u5b58\u5728\u8bed\u8a00\u6df7\u6dc6\u95ee\u9898\uff0c\u4e14\u4f20\u7edf\u6307\u6807\u5728\u6b64\u60c5\u51b5\u4e0b\u4f1a\u4ea7\u751f\u5047\u9634\u6027\u8bc4\u4ef7\u3002", "conclusion": "\u9700\u8981\u91c7\u7528\u57fa\u4e8e\u8bed\u4e49\u7684\u8bc4\u4ef7\u6307\u6807\u6765\u51c6\u786e\u8861\u91cf\u591a\u8bed\u8a00\u5927\u6a21\u578b\u7684\u9057\u5fd8\u6548\u679c\uff0c\u907f\u514d\u8bed\u8a00\u6df7\u6dc6\u5e26\u6765\u7684\u8bef\u5224\u3002"}}
{"id": "2510.23995", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23995", "abs": "https://arxiv.org/abs/2510.23995", "authors": ["Mengzhou Sun", "Sendong Zhao", "Jianyu Chen", "Haochun Wang", "Bin Qin"], "title": "M-Eval: A Heterogeneity-Based Framework for Multi-evidence Validation in Medical RAG Systems", "comment": null, "summary": "Retrieval-augmented Generation (RAG) has demonstrated potential in enhancing\nmedical question-answering systems through the integration of large language\nmodels (LLMs) with external medical literature. LLMs can retrieve relevant\nmedical articles to generate more professional responses efficiently. However,\ncurrent RAG applications still face problems. They generate incorrect\ninformation, such as hallucinations, and they fail to use external knowledge\ncorrectly. To solve these issues, we propose a new method named M-Eval. This\nmethod is inspired by the heterogeneity analysis approach used in\nEvidence-Based Medicine (EBM). Our approach can check for factual errors in RAG\nresponses using evidence from multiple sources. First, we extract additional\nmedical literature from external knowledge bases. Then, we retrieve the\nevidence documents generated by the RAG system. We use heterogeneity analysis\nto check whether the evidence supports different viewpoints in the response. In\naddition to verifying the accuracy of the response, we also assess the\nreliability of the evidence provided by the RAG system. Our method shows an\nimprovement of up to 23.31% accuracy across various LLMs. This work can help\ndetect errors in current RAG-based medical systems. It also makes the\napplications of LLMs more reliable and reduces diagnostic errors.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\u7684\u533b\u5b66\u95ee\u7b54\u4e2d\u7684\u9519\u8bef\u68c0\u6d4b\u65b9\u6cd5M-Eval\uff0c\u901a\u8fc7\u591a\u6e90\u8bc1\u636e\u7684\u5f02\u8d28\u6027\u5206\u6790\uff0c\u6709\u6548\u63d0\u5347\u4e86\u7cfb\u7edf\u56de\u7b54\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524dRAG\u7cfb\u7edf\u5728\u533b\u5b66\u95ee\u7b54\u5e94\u7528\u4e2d\u5b58\u5728\u9519\u8bef\u751f\u6210\uff08\u5982\u5e7b\u89c9\u4fe1\u606f\uff09\u548c\u672a\u80fd\u6b63\u786e\u5229\u7528\u5916\u90e8\u77e5\u8bc6\u7684\u95ee\u9898\uff0c\u4e9f\u9700\u4e00\u79cd\u65b9\u6cd5\u6765\u9a8c\u8bc1\u56de\u7b54\u7684\u771f\u5b9e\u6027\u548c\u8bc1\u636e\u7684\u53ef\u9760\u6027\u3002", "method": "\u8be5\u65b9\u6cd5\u501f\u9274\u5faa\u8bc1\u533b\u5b66\u4e2d\u7684\u5f02\u8d28\u6027\u5206\u6790\uff0c\u5148\u4ece\u5916\u90e8\u77e5\u8bc6\u5e93\u63d0\u53d6\u989d\u5916\u533b\u5b66\u6587\u732e\uff0c\u68c0\u7d22RAG\u7cfb\u7edf\u751f\u6210\u7684\u8bc1\u636e\u6587\u6863\uff0c\u5229\u7528\u591a\u6e90\u8bc1\u636e\u7684\u5f02\u8d28\u6027\u5206\u6790\u5224\u65ad\u56de\u7b54\u4e2d\u89c2\u70b9\u662f\u5426\u88ab\u8bc1\u636e\u652f\u6301\uff0c\u4ece\u800c\u68c0\u6d4b\u4e8b\u5b9e\u9519\u8bef\u5e76\u8bc4\u4f30\u8bc1\u636e\u53ef\u9760\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u63d0\u9ad8\u4e86\u56de\u7b54\u51c6\u786e\u7387\uff0c\u6700\u9ad8\u63d0\u5347\u8fbe23.31%\u3002", "conclusion": "M-Eval\u65b9\u6cd5\u6709\u6548\u51cf\u5c11\u4e86RAG\u7cfb\u7edf\u4e2d\u7684\u8bca\u65ad\u9519\u8bef\uff0c\u63d0\u9ad8\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u533b\u5b66\u95ee\u7b54\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2510.23998", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23998", "abs": "https://arxiv.org/abs/2510.23998", "authors": ["Mengzhou Sun", "Sendong Zhao", "Jianyu Chen", "Bin Qin"], "title": "PICOs-RAG: PICO-supported Query Rewriting for Retrieval-Augmented Generation in Evidence-Based Medicine", "comment": null, "summary": "Evidence-based medicine (EBM) research has always been of paramount\nimportance. It is important to find appropriate medical theoretical support for\nthe needs from physicians or patients to reduce the occurrence of medical\naccidents. This process is often carried out by human querying relevant\nliterature databases, which lacks objectivity and efficiency. Therefore,\nresearchers utilize retrieval-augmented generation (RAG) to search for evidence\nand generate responses automatically. However, current RAG methods struggle to\nhandle complex queries in real-world clinical scenarios. For example, when\nqueries lack certain information or use imprecise language, the model may\nretrieve irrelevant evidence and generate unhelpful answers. To address this\nissue, we present the PICOs-RAG to expand the user queries into a better\nformat. Our method can expand and normalize the queries into professional ones\nand use the PICO format, a search strategy tool present in EBM, to extract the\nmost important information used for retrieval. This approach significantly\nenhances retrieval efficiency and relevance, resulting in up to an 8.8\\%\nimprovement compared to the baseline evaluated by our method. Thereby the\nPICOs-RAG improves the performance of the large language models into a helpful\nand reliable medical assistant in EBM.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86PICOs-RAG\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u7528\u6237\u67e5\u8be2\u6269\u5c55\u548c\u89c4\u8303\u5316\u4e3aPICO\u683c\u5f0f\uff0c\u63d0\u9ad8\u4e86\u8bc1\u636e\u68c0\u7d22\u7684\u6548\u7387\u548c\u76f8\u5173\u6027\uff0c\u589e\u5f3a\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5faa\u8bc1\u533b\u5b66\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u4eba\u5de5\u7684\u6587\u732e\u68c0\u7d22\u7f3a\u4e4f\u5ba2\u89c2\u6027\u548c\u6548\u7387\uff0c\u73b0\u6709\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\u5728\u5904\u7406\u4e34\u5e8a\u590d\u6742\u67e5\u8be2\u65f6\u6548\u679c\u6709\u9650\uff0c\u5bfc\u81f4\u68c0\u7d22\u7ed3\u679c\u4e0d\u76f8\u5173\u3001\u56de\u7b54\u4e0d\u51c6\u786e\u3002", "method": "\u8be5\u65b9\u6cd5\u5c06\u7528\u6237\u67e5\u8be2\u6269\u5c55\u4e3a\u4e13\u4e1a\u7684\u3001\u7b26\u5408PICO\u683c\u5f0f\u7684\u67e5\u8be2\uff0c\u5229\u7528PICO\u4f5c\u4e3a\u5faa\u8bc1\u533b\u5b66\u7684\u68c0\u7d22\u7b56\u7565\u5de5\u5177\uff0c\u63d0\u5347\u68c0\u7d22\u5173\u952e\u5185\u5bb9\u7684\u83b7\u53d6\u80fd\u529b\u3002", "result": "\u91c7\u7528PICOs-RAG\u65b9\u6cd5\u7684\u68c0\u7d22\u6548\u7387\u548c\u76f8\u5173\u6027\u663e\u8457\u63d0\u5347\uff0c\u76f8\u8f83\u57fa\u7ebf\u65b9\u6cd5\u6700\u9ad8\u63d0\u53478.8%\u3002", "conclusion": "PICOs-RAG\u65b9\u6cd5\u6709\u6548\u6539\u5584\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5faa\u8bc1\u533b\u5b66\u9886\u57df\u7684\u68c0\u7d22\u548c\u751f\u6210\u80fd\u529b\uff0c\u4f7f\u5176\u6210\u4e3a\u66f4\u6709\u7528\u3001\u53ef\u9760\u7684\u533b\u5b66\u52a9\u624b\u3002"}}
{"id": "2510.24003", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24003", "abs": "https://arxiv.org/abs/2510.24003", "authors": ["Mengzhou Sun", "Sendong Zhao", "Jianyu Chen", "Haochun Wang", "Bin Qin"], "title": "META-RAG: Meta-Analysis-Inspired Evidence-Re-Ranking Method for Retrieval-Augmented Generation in Evidence-Based Medicine", "comment": null, "summary": "Evidence-based medicine (EBM) holds a crucial role in clinical application.\nGiven suitable medical articles, doctors effectively reduce the incidence of\nmisdiagnoses. Researchers find it efficient to use large language models (LLMs)\ntechniques like RAG for EBM tasks. However, the EBM maintains stringent\nrequirements for evidence, and RAG applications in EBM struggle to efficiently\ndistinguish high-quality evidence. Therefore, inspired by the meta-analysis\nused in EBM, we provide a new method to re-rank and filter the medical\nevidence. This method presents multiple principles to filter the best evidence\nfor LLMs to diagnose. We employ a combination of several EBM methods to emulate\nthe meta-analysis, which includes reliability analysis, heterogeneity analysis,\nand extrapolation analysis. These processes allow the users to retrieve the\nbest medical evidence for the LLMs. Ultimately, we evaluate these high-quality\narticles and show an accuracy improvement of up to 11.4% in our experiments and\nresults. Our method successfully enables RAG to extract higher-quality and more\nreliable evidence from the PubMed dataset. This work can reduce the infusion of\nincorrect knowledge into responses and help users receive more effective\nreplies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5faa\u8bc1\u533b\u5b66\u65b9\u6cd5\u7684\u65b0\u578b\u8bc1\u636e\u91cd\u65b0\u6392\u5e8f\u548c\u7b5b\u9009\u7b56\u7565\uff0c\u4ee5\u63d0\u9ad8\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u8bca\u65ad\u4e2d\u7684\u8bc1\u636e\u8d28\u91cf\uff0c\u4ece\u800c\u63d0\u5347\u8bca\u65ad\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5faa\u8bc1\u533b\u5b66\u4efb\u52a1\u96be\u4ee5\u6709\u6548\u533a\u5206\u9ad8\u8d28\u91cf\u533b\u7597\u8bc1\u636e\uff0c\u5f71\u54cd\u8bca\u65ad\u51c6\u786e\u6027\u3002", "method": "\u501f\u9274\u5faa\u8bc1\u533b\u5b66\u4e2d\u7684\u5143\u5206\u6790\u7406\u5ff5\uff0c\u8bbe\u8ba1\u4e86\u7ed3\u5408\u53ef\u9760\u6027\u5206\u6790\u3001\u5f02\u8d28\u6027\u5206\u6790\u548c\u5916\u63a8\u5206\u6790\u7684\u591a\u539f\u5219\u8bc1\u636e\u7b5b\u9009\u65b9\u6cd5\uff0c\u7528\u4ee5\u63d0\u5347\u8bc1\u636e\u8d28\u91cf\u5e76\u91cd\u65b0\u6392\u5e8f\u533b\u7597\u6587\u732e\u3002", "result": "\u8be5\u65b9\u6cd5\u5728PubMed\u6570\u636e\u96c6\u4e0a\u5e94\u7528\uff0c\u5b9e\u73b0\u4e86\u6700\u592711.4%\u7684\u51c6\u786e\u7387\u63d0\u5347\uff0c\u6709\u6548\u63d0\u53d6\u5230\u66f4\u9ad8\u8d28\u91cf\u548c\u66f4\u53ef\u9760\u7684\u533b\u7597\u8bc1\u636e\u3002", "conclusion": "\u672c\u6587\u65b9\u6cd5\u6210\u529f\u63d0\u5347\u4e86\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\u7684\u8bc1\u636e\u8d28\u91cf\uff0c\u51cf\u5c11\u9519\u8bef\u77e5\u8bc6\u7684\u6ce8\u5165\uff0c\u5e2e\u52a9\u7528\u6237\u83b7\u5f97\u66f4\u6709\u6548\u7684\u533b\u7597\u8bca\u65ad\u56de\u590d\u3002"}}
{"id": "2510.24014", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24014", "abs": "https://arxiv.org/abs/2510.24014", "authors": ["Yizhu Jiao", "Sha Li", "Sizhe Zhou", "Heng Ji", "Jiawei Han"], "title": "TEXT2DB: Integration-Aware Information Extraction with Large Language Model Agents", "comment": "ACL 2025. Source code: https://github.com/yzjiao/Text2DB", "summary": "The task of information extraction (IE) is to extract structured knowledge\nfrom text. However, it is often not straightforward to utilize IE output due to\nthe mismatch between the IE ontology and the downstream application needs. We\npropose a new formulation of IE TEXT2DB that emphasizes the integration of IE\noutput and the target database (or knowledge base). Given a user instruction, a\ndocument set, and a database, our task requires the model to update the\ndatabase with values from the document set to satisfy the user instruction.\nThis task requires understanding user instructions for what to extract and\nadapting to the given DB/KB schema for how to extract on the fly. To evaluate\nthis new task, we introduce a new benchmark featuring common demands such as\ndata infilling, row population, and column addition. In addition, we propose an\nLLM agent framework OPAL (Observe-PlanAnalyze LLM) which includes an Observer\ncomponent that interacts with the database, the Planner component that\ngenerates a code-based plan with calls to IE models, and the Analyzer component\nthat provides feedback regarding code quality before execution. Experiments\nshow that OPAL can successfully adapt to diverse database schemas by generating\ndifferent code plans and calling the required IE models. We also highlight\ndifficult cases such as dealing with large databases with complex dependencies\nand extraction hallucination, which we believe deserve further investigation.\nSource code: https://github.com/yzjiao/Text2DB", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u4fe1\u606f\u62bd\u53d6\u4efb\u52a1TEXT2DB\uff0c\u5f3a\u8c03\u5c06\u4fe1\u606f\u62bd\u53d6\u8f93\u51fa\u4e0e\u76ee\u6807\u6570\u636e\u5e93\u96c6\u6210\uff0c\u5229\u7528\u7528\u6237\u6307\u4ee4\u5bf9\u6570\u636e\u5e93\u8fdb\u884c\u66f4\u65b0\u3002\u8bbe\u8ba1\u4e86\u65b0\u7684\u57fa\u51c6\u548cLLM\u4ee3\u7406\u6846\u67b6OPAL\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u80fd\u9002\u5e94\u4e0d\u540c\u6570\u636e\u5e93\u67b6\u6784\u5e76\u6709\u6548\u8c03\u7528\u4fe1\u606f\u62bd\u53d6\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u4fe1\u606f\u62bd\u53d6\u4efb\u52a1\u4e0e\u4e0b\u6e38\u5e94\u7528\u7684\u6570\u636e\u5e93\u9700\u6c42\u4e0d\u5339\u914d\uff0c\u5bfc\u81f4\u4fe1\u606f\u96be\u4ee5\u76f4\u63a5\u5229\u7528\uff0c\u9700\u8bbe\u8ba1\u65b0\u7684\u4efb\u52a1\u4ee5\u6865\u63a5\u4fe1\u606f\u62bd\u53d6\u8f93\u51fa\u4e0e\u6570\u636e\u5e93\u66f4\u65b0\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u63d0\u51faTEXT2DB\u4efb\u52a1\uff0c\u7ed3\u5408\u7528\u6237\u6307\u4ee4\u3001\u6587\u6863\u96c6\u548c\u6570\u636e\u5e93\uff0c\u8981\u6c42\u6a21\u578b\u5b9e\u65f6\u9002\u5e94\u6570\u636e\u5e93\u67b6\u6784\u66f4\u65b0\u6570\u636e\uff1b\u5e76\u8bbe\u8ba1OPAL\u6846\u67b6\uff0c\u5305\u62ecObserver\u3001Planner\u548cAnalyzer\u7ec4\u4ef6\uff0c\u5206\u522b\u8d1f\u8d23\u6570\u636e\u5e93\u4ea4\u4e92\u3001\u751f\u6210\u4ee3\u7801\u8ba1\u5212\u53ca\u4ee3\u7801\u8d28\u91cf\u53cd\u9988\u3002", "result": "\u5b9e\u9a8c\u8868\u660eOPAL\u80fd\u6839\u636e\u4e0d\u540c\u6570\u636e\u5e93\u67b6\u6784\u751f\u6210\u7075\u6d3b\u7684\u4ee3\u7801\u8ba1\u5212\uff0c\u6210\u529f\u8c03\u7528\u6240\u9700\u7684\u4fe1\u606f\u62bd\u53d6\u6a21\u578b\uff0c\u5b9e\u73b0\u5bf9\u6570\u636e\u5e93\u7684\u6709\u6548\u66f4\u65b0\u3002\u90e8\u5206\u590d\u6742\u6570\u636e\u5e93\u53ca\u62bd\u53d6\u5e7b\u89c9\u95ee\u9898\u503c\u5f97\u540e\u7eed\u7814\u7a76\u3002", "conclusion": "TEXT2DB\u4efb\u52a1\u53caOPAL\u6846\u67b6\u4e3a\u4fe1\u606f\u62bd\u53d6\u4e0e\u6570\u636e\u5e93\u66f4\u65b0\u7684\u7ed3\u5408\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u7075\u6d3b\u9002\u5e94\u4e0d\u540c\u6570\u636e\u5e93\u9700\u6c42\u7684\u80fd\u529b\uff0c\u5e76\u6307\u51fa\u672a\u6765\u6311\u6218\u4e0e\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2510.24020", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24020", "abs": "https://arxiv.org/abs/2510.24020", "authors": ["Hao An", "Yang Xu"], "title": "Teaching LLMs to Abstain via Fine-Grained Semantic Confidence Reward", "comment": "23pages, 4figures", "summary": "Mitigating hallucinations in Large Language Models (LLMs) is critical for\ntheir reliable deployment. Existing methods typically fine-tune LLMs to abstain\nfrom answering questions beyond their knowledge scope. However, these methods\noften rely on coarse-grained signals to guide LLMs to abstain, such as overall\nconfidence or uncertainty scores on multiple sampled answers, which may result\nin an imprecise awareness of the model's own knowledge boundaries. To this end,\nwe propose a novel reinforcement learning framework built on\n$\\textbf{\\underline{Fi}ne-grained \\underline{S}emantic \\underline{Co}nfidence\n\\underline{Re}ward (\\Ours)}$, which guides LLMs to abstain via sample-specific\nconfidence. Specifically, our method operates by sampling multiple candidate\nanswers and conducting semantic clustering, then training the LLM to retain\nanswers within high-confidence clusters and discard those within low-confidence\nones, thereby promoting accurate post-hoc abstention. Additionally, we propose\na new metric for evaluating the reliability of abstention fine-tuning tasks\nmore comprehensively. Our method significantly enhances reliability in both\nin-domain and out-of-distribution benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ec6\u7c92\u5ea6\u8bed\u4e49\u7f6e\u4fe1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u805a\u7c7b\u5f15\u5bfc\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u51c6\u786e\u7684\u540e\u9a8c\u5f03\u7b54\uff0c\u4ece\u800c\u51cf\u8f7b\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u901a\u8fc7\u6574\u4f53\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\u6a21\u578b\u5f03\u7b54\u7684\u65b9\u6cd5\u5f80\u5f80\u8fc7\u4e8e\u7c97\u7cd9\uff0c\u65e0\u6cd5\u7cbe\u51c6\u8bc6\u522b\u6a21\u578b\u7684\u77e5\u8bc6\u8fb9\u754c\uff0c\u5f71\u54cd\u6a21\u578b\u7684\u7b54\u6848\u53ef\u9760\u6027\u3002", "method": "\u8be5\u65b9\u6cd5\u5bf9\u591a\u4e2a\u5019\u9009\u7b54\u6848\u8fdb\u884c\u8bed\u4e49\u805a\u7c7b\uff0c\u57fa\u4e8e\u805a\u7c7b\u7ed3\u679c\u7684\u7f6e\u4fe1\u5ea6\u8bad\u7ec3\u6a21\u578b\u4fdd\u7559\u9ad8\u7f6e\u4fe1\u5ea6\u7b54\u6848\uff0c\u653e\u5f03\u4f4e\u7f6e\u4fe1\u5ea6\u7b54\u6848\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u5f03\u7b54\u63a7\u5236\u3002", "result": "\u5728\u57df\u5185\u548c\u5206\u5e03\u504f\u79fb\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5f03\u7b54\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u901a\u8fc7\u7ec6\u7c92\u5ea6\u8bed\u4e49\u7f6e\u4fe1\u5956\u52b1\u7684\u7b56\u7565\uff0c\u80fd\u591f\u66f4\u7cbe\u51c6\u5730\u6307\u5bfc\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u53ef\u9760\u7684\u5f03\u7b54\uff0c\u6709\u6548\u7f13\u89e3\u5e7b\u89c9\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u5b9e\u9645\u90e8\u7f72\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2510.24021", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24021", "abs": "https://arxiv.org/abs/2510.24021", "authors": ["Haiduo Huang", "Jiangcheng Song", "Yadong Zhang", "Pengju Ren"], "title": "SpecKD: Speculative Decoding for Effective Knowledge Distillation of LLMs", "comment": null, "summary": "Knowledge Distillation (KD) has become a cornerstone technique for\ncompressing Large Language Models (LLMs) into smaller, more efficient student\nmodels. However, conventional KD approaches typically apply the distillation\nloss uniformly across all tokens, regardless of the teacher's confidence. This\nindiscriminate mimicry can introduce noise, as the student is forced to learn\nfrom the teacher's uncertain or high-entropy predictions, which may ultimately\nharm student performance-especially when the teacher is much larger and more\npowerful. To address this, we propose Speculative Knowledge Distillation\n(SpecKD), a novel, plug-and-play framework that introduces a dynamic,\ntoken-level gating mechanism inspired by the \"propose-and-verify\" paradigm of\nspeculative decoding. At each step, the student's token proposal is verified\nagainst the teacher's distribution; the distillation loss is selectively\napplied only to \"accepted\" tokens, while \"rejected\" tokens are masked out.\nExtensive experiments on diverse text generation tasks show that SpecKD\nconsistently and significantly outperforms strong KD baselines, leading to more\nstable training and more capable student models, and achieving state-of-the-art\nresults.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a8\u6001token\u7ea7\u522b\u95e8\u63a7\u673a\u5236\u7684\u77e5\u8bc6\u84b8\u998f\u65b0\u65b9\u6cd5Speculative Knowledge Distillation (SpecKD)\uff0c\u901a\u8fc7\u5bf9\u5b66\u751f\u6a21\u578b\u7684token\u63d0\u8bae\u8fdb\u884c\u6559\u5e08\u5206\u5e03\u9a8c\u8bc1\uff0c\u6709\u9009\u62e9\u5730\u5e94\u7528\u84b8\u998f\u635f\u5931\uff0c\u63d0\u5347\u4e86\u84b8\u998f\u6548\u679c\u548c\u5b66\u751f\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u5bf9\u6240\u6709token\u5747\u5300\u65bd\u52a0\u84b8\u998f\u635f\u5931\uff0c\u5ffd\u7565\u4e86\u6559\u5e08\u6a21\u578b\u5bf9\u4e0d\u540ctoken\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u7684\u5dee\u5f02\uff0c\u5bfc\u81f4\u5b66\u751f\u6a21\u578b\u88ab\u8feb\u5b66\u4e60\u6559\u5e08\u7684\u4e0d\u786e\u5b9a\u6216\u8005\u9ad8\u71b5\u9884\u6d4b\uff0c\u8fdb\u800c\u5f15\u5165\u566a\u58f0\uff0c\u5f71\u54cd\u5b66\u751f\u6a21\u578b\u6027\u80fd\uff0c\u5c24\u5176\u5f53\u6559\u5e08\u6a21\u578b\u89c4\u6a21\u8fdc\u5927\u65f6\u3002", "method": "\u63d0\u51fa\u4e86SpecKD\u6846\u67b6\uff0c\u501f\u9274\u6295\u673a\u89e3\u7801\u4e2d\u7684\u201c\u63d0\u8bae-\u9a8c\u8bc1\u201d\u673a\u5236\uff0c\u5728\u6bcf\u4e2a\u751f\u6210\u6b65\u9aa4\u4e2d\u5148\u7531\u5b66\u751f\u6a21\u578b\u63d0\u51fatoken\uff0c\u7136\u540e\u901a\u8fc7\u6559\u5e08\u6a21\u578b\u5206\u5e03\u8fdb\u884c\u9a8c\u8bc1\uff0c\u53ea\u6709\u88ab\u63a5\u53d7\u7684token\u624d\u5e94\u7528\u84b8\u998f\u635f\u5931\uff0c\u88ab\u62d2\u7edd\u7684token\u5219\u5c4f\u853d\uff0c\u4e0d\u53c2\u4e0e\u635f\u5931\u8ba1\u7b97\u3002", "result": "\u5728\u591a\u4e2a\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e2d\uff0cSpecKD\u76f8\u8f83\u4e8e\u4f20\u7edf\u7684\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\uff0c\u8bad\u7ec3\u66f4\u7a33\u5b9a\u4e14\u5b66\u751f\u6a21\u578b\u80fd\u529b\u66f4\u5f3a\uff0c\u53d6\u5f97\u4e86\u6700\u65b0\u7684\u6027\u80fd\u6307\u6807\u3002", "conclusion": "Speculative Knowledge Distillation\u901a\u8fc7\u5f15\u5165\u52a8\u6001token\u7ea7\u95e8\u63a7\u673a\u5236\u6709\u6548\u51cf\u5c11\u4e86\u84b8\u998f\u8fc7\u7a0b\u4e2d\u7684\u566a\u58f0\uff0c\u63d0\u9ad8\u4e86\u77e5\u8bc6\u84b8\u998f\u7684\u6548\u679c\uff0c\u662f\u63d0\u5347\u5b66\u751f\u6a21\u578b\u6027\u80fd\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2510.24023", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24023", "abs": "https://arxiv.org/abs/2510.24023", "authors": ["Saujas Vaduguru", "Yilun Hua", "Yoav Artzi", "Daniel Fried"], "title": "Success and Cost Elicit Convention Formation for Efficient Communication", "comment": null, "summary": "Humans leverage shared conversational context to become increasingly\nsuccessful and efficient at communicating over time. One manifestation of this\nis the formation of ad hoc linguistic conventions, which allow people to\ncoordinate on short, less costly utterances that are understood using shared\nconversational context. We present a method to train large multimodal models to\nform conventions, enabling efficient communication. Our approach uses simulated\nreference games between models, and requires no additional human-produced data.\nIn repeated reference games involving photographs and tangram images, our\nmethod enables models to communicate efficiently with people: reducing the\nmessage length by up to 41% while increasing success by 15% over the course of\nthe interaction. Human listeners respond faster when interacting with our model\nthat forms conventions. We also show that training based on success or cost\nalone is insufficient - both are necessary to elicit convention formation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bad\u7ec3\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5f62\u6210\u8bed\u8a00\u7ea6\u5b9a\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u578b\u4e4b\u95f4\u7684\u6a21\u62df\u53c2\u7167\u6e38\u620f\uff0c\u5b9e\u73b0\u9ad8\u6548\u6c9f\u901a\u3002", "motivation": "\u4eba\u7c7b\u901a\u8fc7\u5171\u4eab\u7684\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u5f62\u6210\u4e34\u65f6\u8bed\u8a00\u7ea6\u5b9a\uff0c\u4ece\u800c\u66f4\u9ad8\u6548\u5730\u4ea4\u6d41\u3002\u5982\u4f55\u8ba9\u6a21\u578b\u4e5f\u80fd\u5f62\u6210\u7c7b\u4f3c\u7ea6\u5b9a\u4ee5\u63d0\u5347\u4ea4\u6d41\u6548\u7387\uff1f", "method": "\u5229\u7528\u6a21\u578b\u95f4\u7684\u91cd\u590d\u53c2\u7167\u6e38\u620f\uff0c\u4e0d\u4f9d\u8d56\u989d\u5916\u7684\u4eba\u5de5\u6570\u636e\uff0c\u8bad\u7ec3\u6a21\u578b\u5f62\u6210\u8bed\u8a00\u7ea6\u5b9a\u3002", "result": "\u5728\u6d89\u53ca\u7167\u7247\u548c\u62fc\u56fe\u56fe\u50cf\u7684\u91cd\u590d\u53c2\u7167\u6e38\u620f\u4e2d\uff0c\u6240\u63d0\u65b9\u6cd5\u4f7f\u6a21\u578b\u4e0e\u4eba\u7c7b\u6c9f\u901a\u65f6\u4fe1\u606f\u957f\u5ea6\u7f29\u77ed41%\uff0c\u6210\u529f\u7387\u63d0\u9ad815%\uff0c\u4eba\u7c7b\u542c\u8005\u54cd\u5e94\u66f4\u5feb\u3002", "conclusion": "\u4ec5\u57fa\u4e8e\u6210\u529f\u7387\u6216\u6210\u672c\u7684\u8bad\u7ec3\u4e0d\u8db3\u4ee5\u5f62\u6210\u8bed\u8a00\u7ea6\u5b9a\uff0c\u6210\u529f\u7387\u548c\u6210\u672c\u53cc\u91cd\u76ee\u6807\u540c\u65f6\u4f18\u5316\u624d\u80fd\u4fc3\u4f7f\u5f62\u6210\u6709\u6548\u7ea6\u5b9a\u3002"}}
{"id": "2510.24051", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24051", "abs": "https://arxiv.org/abs/2510.24051", "authors": ["In Gim", "Zhiyao Ma", "Seung-seob Lee", "Lin Zhong"], "title": "Pie: A Programmable Serving System for Emerging LLM Applications", "comment": "SOSP 2025. Source code available at\n  https://github.com/pie-project/pie", "summary": "Emerging large language model (LLM) applications involve diverse reasoning\nstrategies and agentic workflows, straining the capabilities of existing\nserving systems built on a monolithic token generation loop. This paper\nintroduces Pie, a programmable LLM serving system designed for flexibility and\nefficiency. Pie decomposes the traditional generation loop into fine-grained\nservice handlers exposed via an API and delegates control of the generation\nprocess to user-provided programs, called inferlets. This enables applications\nto implement new KV cache strategies, bespoke generation logic, and seamlessly\nintegrate computation and I/O-entirely within the application, without\nrequiring modifications to the serving system. Pie executes inferlets using\nWebAssembly, benefiting from its lightweight sandboxing. Our evaluation shows\nPie matches state-of-the-art performance on standard tasks (3-12% latency\noverhead) while significantly improving latency and throughput (1.3x-3.4x\nhigher) on agentic workflows by enabling application-specific optimizations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Pie\uff0c\u4e00\u79cd\u9762\u5411\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7075\u6d3b\u9ad8\u6548\u7684\u53ef\u7f16\u7a0b\u670d\u52a1\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c06\u4f20\u7edf\u751f\u6210\u5faa\u73af\u62c6\u89e3\u4e3a\u7ec6\u7c92\u5ea6\u7684\u670d\u52a1\u5904\u7406\u5668\u548c\u7528\u6237\u63d0\u4f9b\u7684inferlets\u7a0b\u5e8f\uff0c\u5b9e\u73b0\u5e94\u7528\u5c42\u7684\u65b0KV\u7f13\u5b58\u7b56\u7565\u548c\u5b9a\u5236\u751f\u6210\u903b\u8f91\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u63a8\u7406\u5de5\u4f5c\u6d41\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5355\u4e00\u751f\u6210\u5faa\u73af\u7684\u5927\u8bed\u8a00\u6a21\u578b\u670d\u52a1\u7cfb\u7edf\u96be\u4ee5\u6ee1\u8db3\u591a\u6837\u5316\u63a8\u7406\u7b56\u7565\u548c\u590d\u6742\u4ee3\u7406\u5de5\u4f5c\u6d41\u5bf9\u7075\u6d3b\u6027\u548c\u6548\u7387\u7684\u9700\u6c42\u3002", "method": "\u5c06\u4f20\u7edf\u7684\u751f\u6210\u5faa\u73af\u62c6\u5206\u6210\u7ec6\u9897\u7c92\u5ea6\u7684\u670d\u52a1\u5904\u7406\u5668\u7ec4\u4ef6\uff0c\u901a\u8fc7API\u66b4\u9732\u7ed9\u7528\u6237\uff0c\u5e76\u7531\u7528\u6237\u5b9e\u73b0\u7684inferlets\u7a0b\u5e8f\u63a7\u5236\u751f\u6210\u8fc7\u7a0b\uff0c\u5229\u7528WebAssembly\u6280\u672f\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u6c99\u7bb1\u6267\u884c\uff0c\u65b9\u4fbf\u5b9a\u5236\u7f13\u5b58\u548c\u751f\u6210\u903b\u8f91\u3002", "result": "\u5728\u6807\u51c6\u4efb\u52a1\u4e0a\uff0cPie\u7684\u6027\u80fd\u4e0e\u6700\u5148\u8fdb\u7cfb\u7edf\u76f8\u5f53\uff0c\u4ec5\u67093-12%\u7684\u5ef6\u8fdf\u5f00\u9500\uff1b\u5728\u590d\u6742\u4ee3\u7406\u5de5\u4f5c\u6d41\u4e0a\uff0c\u5219\u5b9e\u73b0\u4e861.3\u500d\u81f33.4\u500d\u7684\u5ef6\u8fdf\u548c\u541e\u5410\u91cf\u63d0\u5347\u3002", "conclusion": "Pie\u901a\u8fc7\u53ef\u7f16\u7a0b\u8bbe\u8ba1\u548c\u7ec6\u7c92\u5ea6\u89e3\u8026\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u7075\u6d3b\u7684LLM\u670d\u52a1\uff0c\u9002\u5e94\u591a\u6837\u5316\u5e94\u7528\u9700\u6c42\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u5de5\u4f5c\u6d41\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2510.24073", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24073", "abs": "https://arxiv.org/abs/2510.24073", "authors": ["Xinwei Wu", "Heng Liu", "Jiang Zhou", "Xiaohu Zhao", "Linlong Xu", "Longyue Wang", "Weihua Luo", "Kaifu Zhang"], "title": "Challenging Multilingual LLMs: A New Taxonomy and Benchmark for Unraveling Hallucination in Translation", "comment": null, "summary": "Large Language Models (LLMs) have advanced machine translation but remain\nvulnerable to hallucinations. Unfortunately, existing MT benchmarks are not\ncapable of exposing failures in multilingual LLMs. To disclose hallucination in\nmultilingual LLMs, we introduce a diagnostic framework with a taxonomy that\nseparates Instruction Detachment from Source Detachment. Guided by this\ntaxonomy, we create HalloMTBench, a multilingual, human-verified benchmark\nacross 11 English-to-X directions. We employed 4 frontier LLMs to generate\ncandidates and scrutinize these candidates with an ensemble of LLM judges, and\nexpert validation. In this way, we curate 5,435 high-quality instances. We have\nevaluated 17 LLMs on HalloMTBench. Results reveal distinct ``hallucination\ntriggers'' -- unique failure patterns reflecting model scale, source length\nsensitivity, linguistic biases, and Reinforcement-Learning (RL) amplified\nlanguage mixing. HalloMTBench offers a forward-looking testbed for diagnosing\nLLM translation failures. HalloMTBench is available in\nhttps://huggingface.co/collections/AIDC-AI/marco-mt.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86HalloMTBench\uff0c\u4e00\u4e2a\u4eba\u7c7b\u9a8c\u8bc1\u7684\u591a\u8bed\u8a00\u673a\u5668\u7ffb\u8bd1\u57fa\u51c6\uff0c\u7528\u4e8e\u68c0\u6d4b\u591a\u8bed\u8a00\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u673a\u5668\u7ffb\u8bd1\u57fa\u51c6\u65e0\u6cd5\u6709\u6548\u63ed\u793a\u591a\u8bed\u8a00LLMs\u4e2d\u7684\u5e7b\u89c9\u5931\u8d25\uff0c\u9700\u8981\u4e00\u4e2a\u65b0\u7684\u8bca\u65ad\u6846\u67b6\u6765\u533a\u5206\u4e0d\u540c\u7c7b\u578b\u7684\u5e7b\u89c9\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5305\u542b\u6307\u4ee4\u8131\u8282\u548c\u6e90\u8bed\u8131\u8282\u7684\u5e7b\u89c9\u5206\u7c7b\u6cd5\uff0c\u521b\u5efa\u4e86\u8986\u76d611\u4e2a\u82f1\u8bd1\u591a\u8bed\u8a00\u65b9\u5411\u7684HalloMTBench\uff0c\u5229\u75284\u4e2a\u524d\u6cbfLLM\u751f\u6210\u7ffb\u8bd1\u5019\u9009\uff0c\u5e76\u901a\u8fc7\u591a\u4e2aLLM\u8bc4\u4ef7\u5668\u548c\u4e13\u5bb6\u9a8c\u8bc1\u7b5b\u9009\u51fa5435\u4e2a\u9ad8\u8d28\u91cf\u6837\u672c\u3002", "result": "\u8bc4\u4f30\u4e8617\u4e2aLLMs\uff0c\u53d1\u73b0\u4e86\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u3001\u6e90\u6587\u672c\u957f\u5ea6\u654f\u611f\u6027\u3001\u8bed\u8a00\u504f\u89c1\u53ca\u5f3a\u5316\u5b66\u4e60\u5f15\u53d1\u7684\u8bed\u8a00\u6df7\u5408\u7b49\u5e7b\u89c9\u89e6\u53d1\u56e0\u7d20\uff0c\u63ed\u793a\u4e86\u591a\u6837\u7684\u5931\u8d25\u6a21\u5f0f\u3002", "conclusion": "HalloMTBench\u4e3a\u672a\u6765\u8bca\u65ad\u591a\u8bed\u8a00LLMs\u7ffb\u8bd1\u5931\u8d25\u63d0\u4f9b\u4e86\u524d\u77bb\u6027\u6d4b\u8bd5\u5e73\u53f0\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u66f4\u53ef\u9760\u7684\u673a\u5668\u7ffb\u8bd1\u53d1\u5c55\u3002"}}
{"id": "2510.24081", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24081", "abs": "https://arxiv.org/abs/2510.24081", "authors": ["Tyler A. Chang", "Catherine Arnett", "Abdelrahman Eldesokey", "Abdelrahman Sadallah", "Abeer Kashar", "Abolade Daud", "Abosede Grace Olanihun", "Adamu Labaran Mohammed", "Adeyemi Praise", "Adhikarinayum Meerajita Sharma", "Aditi Gupta", "Afitab Iyigun", "Afonso Simpl\u00edcio", "Ahmed Essouaied", "Aicha Chorana", "Akhil Eppa", "Akintunde Oladipo", "Akshay Ramesh", "Aleksei Dorkin", "Alfred Malengo Kondoro", "Alham Fikri Aji", "Ali Eren \u00c7etinta\u015f", "Allan Hanbury", "Alou Dembele", "Alp Niksarli", "\u00c1lvaro Arroyo", "Amin Bajand", "Amol Khanna", "Ana Chkhaidze", "Ana Condez", "Andiswa Mkhonto", "Andrew Hoblitzell", "Andrew Tran", "Angelos Poulis", "Anirban Majumder", "Anna Vacalopoulou", "Annette Kuuipolani Kanahele Wong", "Annika Simonsen", "Anton Kovalev", "Ashvanth. S", "Ayodeji Joseph Lana", "Barkin Kinay", "Bashar Alhafni", "Benedict Cibalinda Busole", "Bernard Ghanem", "Bharti Nathani", "Biljana Stojanovska \u0110uri\u0107", "Bola Agbonile", "Bragi Bergsson", "Bruce Torres Fischer", "Burak Tutar", "Burcu Alaku\u015f \u00c7\u0131nar", "Cade J. Kanoniakapueo Kane", "Can Udomcharoenchaikit", "Catherine Arnett", "Chadi Helwe", "Chaithra Reddy Nerella", "Chen Cecilia Liu", "Chiamaka Glory Nwokolo", "Cristina Espa\u00f1a-Bonet", "Cynthia Amol", "DaeYeop Lee", "Dana Arad", "Daniil Dzenhaliou", "Daria Pugacheva", "Dasol Choi", "Daud Abolade", "David Liu", "David Semedo", "Deborah Popoola", "Deividas Mataciunas", "Delphine Nyaboke", "Dhyuthy Krishna Kumar", "Diogo Gl\u00f3ria-Silva", "Diogo Tavares", "Divyanshu Goyal", "DongGeon Lee", "Ebele Nwamaka Anajemba", "Egonu Ngozi Grace", "Elena Mickel", "Elena Tutubalina", "Elias Herranen", "Emile Anand", "Emmanuel Habumuremyi", "Emuobonuvie Maria Ajiboye", "Eryawan Presma Yulianrifat", "Esther Adenuga", "Ewa Rudnicka", "Faith Olabisi Itiola", "Faran Taimoor Butt", "Fathima Thekkekara", "Fatima Haouari", "Filbert Aurelian Tjiaranata", "Firas Laakom", "Francesca Grasso", "Francesco Orabona", "Francesco Periti", "Gbenga Kayode Solomon", "Gia Nghia Ngo", "Gloria Udhehdhe-oze", "Gon\u00e7alo Martins", "Gopi Naga Sai Ram Challagolla", "Guijin Son", "Gulnaz Abdykadyrova", "Hafsteinn Einarsson", "Hai Hu", "Hamidreza Saffari", "Hamza Zaidi", "Haopeng Zhang", "Harethah Abu Shairah", "Harry Vuong", "Hele-Andra Kuulmets", "Houda Bouamor", "Hwanjo Yu", "Iben Nyholm Debess", "\u0130brahim Ethem Deveci", "Ikhlasul Akmal Hanif", "Ikhyun Cho", "In\u00eas Calvo", "In\u00eas Vieira", "Isaac Manzi", "Ismail Daud", "Itay Itzhak", "Iuliia", "Alekseenko", "Ivan Belashkin", "Ivan Spada", "Ivan Zhelyazkov", "Jacob Brinton", "Jafar Isbarov", "Jaka \u010cibej", "Jan \u010cuhel", "Jan Koco\u0144", "Jauza Akbar Krito", "Jebish Purbey", "Jennifer Mickel", "Jennifer Za", "Jenny Kunz", "Jihae Jeong", "Jimena Tena D\u00e1valos", "Jinu Lee", "Jo\u00e3o Magalh\u00e3es", "John Yi", "Jongin Kim", "Joseph Chataignon", "Joseph Marvin Imperial", "Jubeerathan Thevakumar", "Judith Land", "Junchen Jiang", "Jungwhan Kim", "Kairit Sirts", "Kamesh R", "Kamesh V", "Kanda Patrick Tshinu", "K\u00e4triin Kukk", "Kaustubh Ponkshe", "Kavsar Huseynova", "Ke He", "Kelly Buchanan", "Kengatharaiyer Sarveswaran", "Kerem Zaman", "Khalil Mrini", "Kian Kyars", "Krister Kruusmaa", "Kusum Chouhan", "Lainitha Krishnakumar", "Laura Castro S\u00e1nchez", "Laura Porrino Moscoso", "Leshem Choshen", "Levent Sencan", "Lilja \u00d8vrelid", "Lisa Alazraki", "Lovina Ehimen-Ugbede", "Luheerathan Thevakumar", "Luxshan Thavarasa", "Mahnoor Malik", "Mamadou K. Keita", "Mansi Jangid", "Marco De Santis", "Marcos Garc\u00eda", "Marek Suppa", "Mariam D'Ciofalo", "Marii Ojastu", "Maryam Sikander", "Mausami Narayan", "Maximos Skandalis", "Mehak Mehak", "Mehmet \u0130lteri\u015f Bozkurt", "Melaku Bayu Workie", "Menan Velayuthan", "Michael Leventhal", "Micha\u0142 Marci\u0144czuk", "Mirna Poto\u010dnjak", "Mohammadamin Shafiei", "Mridul Sharma", "Mrityunjaya Indoria", "Muhammad Ravi Shulthan Habibi", "Murat Koli\u0107", "Nada Galant", "Naphat Permpredanun", "Narada Maugin", "Nicholas Kluge Corr\u00eaa", "Nikola Ljube\u0161i\u0107", "Nirmal Thomas", "Nisansa de Silva", "Nisheeth Joshi", "Nitish Ponkshe", "Nizar Habash", "Nneoma C. Udeze", "Noel Thomas", "No\u00e9mi Ligeti-Nagy", "Nouhoum Coulibaly", "Nsengiyumva Faustin", "Odunayo Kareemat Buliaminu", "Odunayo Ogundepo", "Oghojafor Godswill Fejiro", "Ogundipe Blessing Funmilola", "Okechukwu God'spraise", "Olanrewaju Samuel", "Olaoye Deborah Oluwaseun", "Olasoji Akindejoye", "Olga Popova", "Olga Snissarenko", "Onyinye Anulika Chiemezie", "Orkun Kinay", "Osman Tursun", "Owoeye Tobiloba Moses", "Oyelade Oluwafemi Joshua", "Oyesanmi Fiyinfoluwa", "Pablo Gamallo", "Pablo Rodr\u00edguez Fern\u00e1ndez", "Palak Arora", "Pedro Valente", "Peter Rupnik", "Philip Oghenesuowho Ekiugbo", "Pramit Sahoo", "Prokopis Prokopidis", "Pua Niau-Puhipau", "Quadri Yahya", "Rachele Mignone", "Raghav Singhal", "Ram Mohan Rao Kadiyala", "Raphael Merx", "Rapheal Afolayan", "Ratnavel Rajalakshmi", "Rishav Ghosh", "Romina Oji", "Ron Kekeha Solis", "Rui Guerra", "Rushikesh Zawar", "Sa'ad Nasir Bashir", "Saeed Alzaabi", "Sahil Sandeep", "Sai Pavan Batchu", "SaiSandeep Kantareddy", "Salsabila Zahirah Pranida", "Sam Buchanan", "Samuel Rutunda", "Sander Land", "Sarah Sulollari", "Sardar Ali", "Saroj Sapkota", "Saulius Tautvaisas", "Sayambhu Sen", "Sayantani Banerjee", "Sebastien Diarra", "SenthilNathan. M", "Sewoong Lee", "Shaan Shah", "Shankar Venkitachalam", "Sharifa Djurabaeva", "Sharon Ibejih", "Shivanya Shomir Dutta", "Siddhant Gupta", "Silvia Paniagua Su\u00e1rez", "Sina Ahmadi", "Sivasuthan Sukumar", "Siyuan Song", "Snegha A.", "Sokratis Sofianopoulos", "Sona Elza Simon", "Sonja Ben\u010dina", "Sophie Gvasalia", "Sphurti Kirit More", "Spyros Dragazis", "Stephan P. Kaufhold", "Suba. S", "Sultan AlRashed", "Surangika Ranathunga", "Taiga Someya", "Taja Kuzman Punger\u0161ek", "Tal Haklay", "Tasi'u Jibril", "Tatsuya Aoyama", "Tea Abashidze", "Terenz Jomar Dela Cruz", "Terra Blevins", "Themistoklis Nikas", "Theresa Dora Idoko", "Thu Mai Do", "Tilek Chubakov", "Tommaso Gargiani", "Uma Rathore", "Uni Johannesen", "Uwuma Doris Ugwu", "Vallerie Alexandra Putra", "Vanya Bannihatti Kumar", "Varsha Jeyarajalingam", "Varvara Arzt", "Vasudevan Nedumpozhimana", "Viktoria Ondrejova", "Viktoryia Horbik", "Vishnu Vardhan Reddy Kummitha", "Vuk Dini\u0107", "Walelign Tewabe Sewunetie", "Winston Wu", "Xiaojing Zhao", "Yacouba Diarra", "Yaniv Nikankin", "Yash Mathur", "Yixi Chen", "Yiyuan Li", "Yolanda Xavier", "Yonatan Belinkov", "Yusuf Ismail Abayomi", "Zaid Alyafeai", "Zhengyang Shan", "Zhi Rui Tam", "Zilu Tang", "Zuzana Nadova", "Baber Abbasi", "Stella Biderman", "David Stap", "Duygu Ataman", "Fabian Schmidt", "Hila Gonen", "Jiayi Wang", "David Ifeoluwa Adelani"], "title": "Global PIQA: Evaluating Physical Commonsense Reasoning Across 100+ Languages and Cultures", "comment": "Preprint", "summary": "To date, there exist almost no culturally-specific evaluation benchmarks for\nlarge language models (LLMs) that cover a large number of languages and\ncultures. In this paper, we present Global PIQA, a participatory commonsense\nreasoning benchmark for over 100 languages, constructed by hand by 335\nresearchers from 65 countries around the world. The 116 language varieties in\nGlobal PIQA cover five continents, 14 language families, and 23 writing\nsystems. In the non-parallel split of Global PIQA, over 50% of examples\nreference local foods, customs, traditions, or other culturally-specific\nelements. We find that state-of-the-art LLMs perform well on Global PIQA in\naggregate, but they exhibit weaker performance in lower-resource languages (up\nto a 37% accuracy gap, despite random chance at 50%). Open models generally\nperform worse than proprietary models. Global PIQA highlights that in many\nlanguages and cultures, everyday knowledge remains an area for improvement,\nalongside more widely-discussed capabilities such as complex reasoning and\nexpert knowledge. Beyond its uses for LLM evaluation, we hope that Global PIQA\nprovides a glimpse into the wide diversity of cultures in which human language\nis embedded.", "AI": {"tldr": "\u672c\u8bba\u6587\u6784\u5efa\u4e86\u6db5\u76d6100\u591a\u79cd\u8bed\u8a00\u548c\u6587\u5316\u7684Global PIQA\u5e38\u8bc6\u63a8\u7406\u57fa\u51c6\uff0c\u5c55\u793a\u4e86\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8bed\u8a00\u6587\u5316\u80cc\u666f\u4e0b\u7684\u8868\u73b0\u5dee\u5f02\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u6db5\u76d6\u591a\u8bed\u8a00\u591a\u6587\u5316\u7684\u6587\u5316\u7279\u5b9a\u8bc4\u4f30\u57fa\u51c6\uff0c\u8feb\u5207\u9700\u8981\u6784\u5efa\u4e00\u4e2a\u5168\u7403\u6027\u7684\u5e38\u8bc6\u63a8\u7406\u8bc4\u6d4b\u4ee5\u5168\u9762\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "method": "\u901a\u8fc7335\u540d\u7814\u7a76\u4eba\u5458\u572865\u4e2a\u56fd\u5bb6\u5408\u4f5c\uff0c\u624b\u5de5\u6784\u5efa\u4e86\u5305\u62ec116\u79cd\u8bed\u8a00\u53d8\u4f53\u7684Global PIQA\u57fa\u51c6\uff0c\u6db5\u76d6\u4e94\u5927\u6d32\u300114\u4e2a\u8bed\u7cfb\u548c23\u79cd\u4e66\u5199\u7cfb\u7edf\u3002\u8be5\u57fa\u51c6\u5305\u62ec\u5927\u91cf\u6d89\u53ca\u672c\u5730\u98df\u54c1\u3001\u4e60\u4fd7\u548c\u6587\u5316\u5143\u7d20\u7684\u9898\u76ee\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6700\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u603b\u4f53\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u8d44\u6e90\u8f83\u5c11\u7684\u8bed\u8a00\u4e2d\u8868\u73b0\u660e\u663e\u8f83\u5f31\uff0c\u51c6\u786e\u7387\u5dee\u8ddd\u9ad8\u8fbe37%\u3002\u5f00\u6e90\u6a21\u578b\u666e\u904d\u52a3\u4e8e\u4e13\u6709\u6a21\u578b\u3002", "conclusion": "Global PIQA\u63ed\u793a\u4e86\u4e0d\u540c\u8bed\u8a00\u4e0e\u6587\u5316\u4e2d\u65e5\u5e38\u77e5\u8bc6\u4ecd\u5b58\u5728\u663e\u8457\u77ed\u677f\uff0c\u5f3a\u8c03\u4e86\u591a\u5143\u6587\u5316\u80cc\u666f\u4e0b\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u63d0\u5347\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u4e3a\u8bc4\u4f30\u591a\u6837\u6587\u5316\u73af\u5883\u4e0b\u7684\u8bed\u8a00\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2510.24096", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24096", "abs": "https://arxiv.org/abs/2510.24096", "authors": ["Md. Rezuwan Hassan", "Azmol Hossain", "Kanij Fatema", "Rubayet Sabbir Faruque", "Tanmoy Shome", "Ruwad Naswan", "Trina Chakraborty", "Md. Foriduzzaman Zihad", "Tawsif Tashwar Dipto", "Nazia Tasnim", "Nazmuddoha Ansary", "Md. Mehedi Hasan Shawon", "Ahmed Imtiaz Humayun", "Md. Golam Rabiul Alam", "Farig Sadeque", "Asif Sushmit"], "title": "RegSpeech12: A Regional Corpus of Bengali Spontaneous Speech Across Dialects", "comment": "26 pages", "summary": "The Bengali language, spoken extensively across South Asia and among\ndiasporic communities, exhibits considerable dialectal diversity shaped by\ngeography, culture, and history. Phonological and pronunciation-based\nclassifications broadly identify five principal dialect groups: Eastern\nBengali, Manbhumi, Rangpuri, Varendri, and Rarhi. Within Bangladesh, further\ndistinctions emerge through variation in vocabulary, syntax, and morphology, as\nobserved in regions such as Chittagong, Sylhet, Rangpur, Rajshahi, Noakhali,\nand Barishal. Despite this linguistic richness, systematic research on the\ncomputational processing of Bengali dialects remains limited. This study seeks\nto document and analyze the phonetic and morphological properties of these\ndialects while exploring the feasibility of building computational models\nparticularly Automatic Speech Recognition (ASR) systems tailored to regional\nvarieties. Such efforts hold potential for applications in virtual assistants\nand broader language technologies, contributing to both the preservation of\ndialectal diversity and the advancement of inclusive digital tools for\nBengali-speaking communities. The dataset created for this study is released\nfor public use.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5b5f\u52a0\u62c9\u8bed\u4e94\u5927\u65b9\u8a00\u7684\u8bed\u97f3\u548c\u5f62\u6001\u7279\u5f81\uff0c\u63a2\u7d22\u4e86\u9488\u5bf9\u533a\u57df\u65b9\u8a00\u6784\u5efa\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u7684\u53ef\u80fd\u6027\uff0c\u5e76\u516c\u5f00\u4e86\u76f8\u5173\u6570\u636e\u96c6\u3002", "motivation": "\u5b5f\u52a0\u62c9\u8bed\u5b58\u5728\u663e\u8457\u7684\u65b9\u8a00\u591a\u6837\u6027\uff0c\u4f46\u9488\u5bf9\u8fd9\u4e9b\u65b9\u8a00\u7684\u8ba1\u7b97\u5904\u7406\u7814\u7a76\u8f83\u5c11\uff0c\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u8bb0\u5f55\u548c\u5206\u6790\u4e0d\u540c\u5730\u533a\u5b5f\u52a0\u62c9\u65b9\u8a00\u7684\u8bed\u97f3\u548c\u5f62\u6001\u7279\u5f81\uff0c\u5c1d\u8bd5\u6784\u5efa\u9002\u5408\u5404\u65b9\u8a00\u7684\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u3002", "result": "\u7814\u7a76\u6210\u529f\u63d0\u53d6\u5e76\u5206\u6790\u4e86\u5404\u65b9\u8a00\u7684\u8bed\u8a00\u7279\u5f81\uff0c\u63a2\u7d22\u4e86\u53ef\u884c\u7684ASR\u6a21\u578b\uff0c\u4e14\u53d1\u5e03\u4e86\u6570\u636e\u96c6\u4f9b\u516c\u5f00\u4f7f\u7528\u3002", "conclusion": "\u8be5\u7814\u7a76\u4fc3\u8fdb\u4e86\u5b5f\u52a0\u62c9\u8bed\u65b9\u8a00\u591a\u6837\u6027\u7684\u4fdd\u62a4\u548c\u6570\u5b57\u6280\u672f\u7684\u5305\u5bb9\u6027\u53d1\u5c55\uff0c\u4e3a\u76f8\u5173\u5e94\u7528\u5982\u865a\u62df\u52a9\u624b\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.24102", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24102", "abs": "https://arxiv.org/abs/2510.24102", "authors": ["Yihan Wang", "Peiyu Liu", "Runyu Chen", "Jiaxing Pu", "Wei Xu"], "title": "Squrve: A Unified and Modular Framework for Complex Real-World Text-to-SQL Tasks", "comment": null, "summary": "Text-to-SQL technology has evolved rapidly, with diverse academic methods\nachieving impressive results. However, deploying these techniques in real-world\nsystems remains challenging due to limited integration tools. Despite these\nadvances, we introduce Squrve, a unified, modular, and extensive Text-to-SQL\nframework designed to bring together research advances and real-world\napplications. Squrve first establishes a universal execution paradigm that\nstandardizes invocation interfaces, then proposes a multi-actor collaboration\nmechanism based on seven abstracted effective atomic actor components.\nExperiments on widely adopted benchmarks demonstrate that the collaborative\nworkflows consistently outperform the original individual methods, thereby\nopening up a new effective avenue for tackling complex real-world queries. The\ncodes are available at https://github.com/Satissss/Squrve.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u4e14\u6a21\u5757\u5316\u7684Text-to-SQL\u6846\u67b6Squrve\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u6267\u884c\u8303\u5f0f\u548c\u591a\u89d2\u8272\u534f\u4f5c\u673a\u5236\uff0c\u63d0\u9ad8\u4e86\u590d\u6742\u67e5\u8be2\u7684\u5904\u7406\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1Text-to-SQL\u6280\u672f\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u5b9e\u9645\u7cfb\u7edf\u4e2d\u90e8\u7f72\u4ecd\u53d7\u9650\u4e8e\u96c6\u6210\u5de5\u5177\u4e0d\u8db3\u3002", "method": "Squrve\u5efa\u7acb\u4e86\u7edf\u4e00\u7684\u6267\u884c\u8303\u5f0f\u53ca\u57fa\u4e8e\u4e03\u4e2a\u62bd\u8c61\u539f\u5b50\u89d2\u8272\u7ec4\u4ef6\u7684\u591a\u89d2\u8272\u534f\u4f5c\u673a\u5236\u3002", "result": "\u5728\u591a\u4e2a\u4e3b\u6d41\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u534f\u4f5c\u5de5\u4f5c\u6d41\u7684\u8868\u73b0\u4f18\u4e8e\u5355\u4e00\u65b9\u6cd5\u3002", "conclusion": "Squrve\u4e3a\u5e94\u5bf9\u590d\u6742\u7684\u73b0\u5b9e\u67e5\u8be2\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u89e3\u51b3\u8def\u5f84\uff0c\u5177\u6709\u8f83\u5f3a\u7684\u5b9e\u7528\u4ef7\u503c\u548c\u63a8\u5e7f\u6f5c\u529b\u3002"}}
{"id": "2510.24126", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24126", "abs": "https://arxiv.org/abs/2510.24126", "authors": ["Vivek Kalyan", "Martin Andrews"], "title": "Reinforcement Learning for Long-Horizon Multi-Turn Search Agents", "comment": "4 pages plus references and appendices. Accepted into the First\n  Workshop on Multi-Turn Interactions in Large Language Models at NeurIPS 2025", "summary": "Large Language Model (LLM) agents can leverage multiple turns and tools to\nsolve complex tasks, with prompt-based approaches achieving strong performance.\nThis work demonstrates that Reinforcement Learning (RL) can push capabilities\nsignificantly further by learning from experience. Through experiments on a\nlegal document search benchmark, we show that our RL-trained 14 Billion\nparameter model outperforms frontier class models (85% vs 78% accuracy). In\naddition, we explore turn-restricted regimes, during training and at test-time,\nthat show these agents achieve better results if allowed to operate over longer\nmulti-turn horizons.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c55\u793a\u4e86\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6cd5\u5f8b\u6587\u6863\u641c\u7d22\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u524d\u6cbf\u6a21\u578b\uff0c\u5e76\u4e14\u591a\u8f6e\u64cd\u4f5c\u80fd\u529b\u63d0\u5347\u4e86\u6a21\u578b\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u63d0\u793a\u7684\u591a\u8f6e\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u901a\u8fc7\u7ecf\u9a8c\u5b66\u4e60\u7684\u5f3a\u5316\u5b66\u4e60\u6709\u6f5c\u529b\u8fdb\u4e00\u6b65\u63d0\u5347\u80fd\u529b\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u8bad\u7ec3\u4e00\u4e2a\u62e5\u6709140\u4ebf\u53c2\u6570\u7684\u6a21\u578b\uff0c\u5728\u6cd5\u5f8b\u6587\u6863\u641c\u7d22\u4efb\u52a1\u4e2d\u8fdb\u884c\u591a\u8f6e\u5bf9\u8bdd\u548c\u5de5\u5177\u4f7f\u7528\uff0c\u5e76\u8bc4\u4f30\u591a\u8f6e\u6b21\u6570\u9650\u5236\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u6a21\u578b\u51c6\u786e\u7387\u8fbe\u523085%\uff0c\u4f18\u4e8e\u73b0\u6709\u9886\u5148\u6a21\u578b\u768478%\uff1b\u540c\u65f6\u5141\u8bb8\u66f4\u957f\u591a\u8f6e\u5bf9\u8bdd\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6548\u679c\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u591a\u8f6e\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5f53\u6a21\u578b\u5141\u8bb8\u5728\u66f4\u957f\u5bf9\u8bdd\u8f6e\u6570\u4e2d\u64cd\u4f5c\u65f6\u8868\u73b0\u6700\u4f73\u3002"}}
{"id": "2510.24139", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24139", "abs": "https://arxiv.org/abs/2510.24139", "authors": ["Chanwoo Park", "Suyoung Park", "Yelim Ahn", "Jongmin Kim", "Jongyeon Park", "Jaejin Lee"], "title": "Beyond Line-Level Filtering for the Pretraining Corpora of LLMs", "comment": "submitted to ACL ARR Rolling Review", "summary": "While traditional line-level filtering techniques, such as line-level\ndeduplication and trailing-punctuation filters, are commonly used, these basic\nmethods can sometimes discard valuable content, negatively affecting downstream\nperformance. In this paper, we introduce two methods-pattern-aware line-level\ndeduplication (PLD) and pattern-aware trailing punctuation filtering (PTF)-by\nenhancing the conventional filtering techniques. Our approach not only\nconsiders line-level signals but also takes into account their sequential\ndistribution across documents, enabling us to retain structurally important\ncontent that might otherwise be removed. We evaluate these proposed methods by\ntraining small language models (1 B parameters) in both English and Korean. The\nresults demonstrate that our methods consistently improve performance on\nmultiple-choice benchmarks and significantly enhance generative\nquestion-answering accuracy on both SQuAD v1 and KorQuAD v1.", "AI": {"tldr": "\u5f15\u5165\u4e86\u4e24\u79cd\u57fa\u4e8e\u6a21\u5f0f\u7684\u884c\u7ea7\u6587\u672c\u8fc7\u6ee4\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u8868\u73b0\u3002", "motivation": "\u4f20\u7edf\u884c\u7ea7\u8fc7\u6ee4\u65b9\u6cd5\u6709\u65f6\u4f1a\u4e22\u5f03\u6709\u4ef7\u503c\u5185\u5bb9\uff0c\u5f71\u54cd\u4e0b\u6e38\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u6a21\u5f0f\u611f\u77e5\u884c\u7ea7\u53bb\u91cd\uff08PLD\uff09\u548c\u5c3e\u90e8\u6807\u70b9\u8fc7\u6ee4\uff08PTF\uff09\uff0c\u8003\u8651\u4e86\u884c\u7684\u5e8f\u5217\u5206\u5e03\uff0c\u4fdd\u7559\u7ed3\u6784\u6027\u91cd\u8981\u5185\u5bb9\u3002", "result": "\u5728\u82f1\u8bed\u548c\u97e9\u8bed1\u4ebf\u53c2\u6570\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u4e0a\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u63d0\u5347\u4e86\u591a\u9879\u9009\u62e9\u9898\u57fa\u51c6\u6d4b\u8bd5\u548c\u751f\u6210\u5f0f\u95ee\u7b54\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6a21\u5f0f\u611f\u77e5\u8fc7\u6ee4\u65b9\u6cd5\u6709\u6548\u6539\u8fdb\u4e86\u884c\u7ea7\u6587\u672c\u9884\u5904\u7406\uff0c\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u3002"}}
{"id": "2510.24150", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24150", "abs": "https://arxiv.org/abs/2510.24150", "authors": ["Chanwoo Park", "Suyoung Park", "JiA Kang", "Jongyeon Park", "Sangho Kim", "Hyunji M. Park", "Sumin Bae", "Mingyu Kang", "Jaejin Lee"], "title": "Ko-MuSR: A Multistep Soft Reasoning Benchmark for LLMs Capable of Understanding Korean", "comment": "submitted to ACL ARR Rolling Review", "summary": "We present Ko-MuSR, the first benchmark to comprehensively evaluate\nmultistep, soft reasoning in long Korean narratives while minimizing data\ncontamination. Built following MuSR, Ko-MuSR features fully Korean narratives,\nreasoning chains, and multiple-choice questions verified by human annotators\nfor logical consistency and answerability. Evaluations of four large language\nmodels -- two multilingual and two Korean-specialized -- show that multilingual\nmodels outperform Korean-focused ones even in Korean reasoning tasks,\nindicating cross-lingual generalization of reasoning ability. Carefully\ndesigned prompting strategies, which combine few-shot examples, reasoning\ntraces, and task-specific hints, further boost accuracy, approaching\nhuman-level performance. Ko-MuSR offers a solid foundation for advancing Korean\nNLP by enabling systematic evaluation of long-context reasoning and prompting\nstrategies.", "AI": {"tldr": "Ko-MuSR\u662f\u9996\u4e2a\u9488\u5bf9\u97e9\u8bed\u957f\u7bc7\u53d9\u4e8b\u8fdb\u884c\u591a\u6b65\u8f6f\u63a8\u7406\u7684\u7efc\u5408\u8bc4\u6d4b\u57fa\u51c6\uff0c\u5c55\u793a\u8de8\u8bed\u8a00\u63a8\u7406\u80fd\u529b\u548c\u63d0\u793a\u7b56\u7565\u63d0\u5347\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u4e13\u95e8\u9488\u5bf9\u97e9\u8bed\u957f\u7bc7\u53d9\u4e8b\u4e2d\u591a\u6b65\u8f6f\u63a8\u7406\u7684\u8bc4\u6d4b\u57fa\u51c6\uff0c\u4e14\u9700\u8981\u51cf\u5c11\u6570\u636e\u6c61\u67d3\uff0c\u63a8\u52a8\u97e9\u8bedNLP\u53d1\u5c55\u3002", "method": "\u6784\u5efaKo-MuSR\u57fa\u51c6\uff0c\u5305\u542b\u97e9\u8bed\u53d9\u8ff0\u3001\u63a8\u7406\u94fe\u548c\u4eba\u5de5\u9a8c\u8bc1\u7684\u591a\u9009\u9898\uff1b\u8bc4\u6d4b\u4e24\u7c7b\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5229\u7528\u8bbe\u8ba1\u7684\u63d0\u793a\u7b56\u7565\u63d0\u5347\u6a21\u578b\u8868\u73b0\u3002", "result": "\u591a\u8bed\u8a00\u6a21\u578b\u5728\u97e9\u8bed\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u97e9\u8bed\u4e13\u7528\u6a21\u578b\uff0c\u8bbe\u8ba1\u7684\u63d0\u793a\u7b56\u7565\u663e\u8457\u63d0\u5347\u51c6\u786e\u7387\uff0c\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\u3002", "conclusion": "Ko-MuSR\u4e3a\u7cfb\u7edf\u8bc4\u4f30\u97e9\u8bed\u957f\u6587\u672c\u63a8\u7406\u548c\u63d0\u793a\u6280\u672f\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u4fc3\u8fdb\u97e9\u8bed\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7684\u8fdb\u6b65\u3002"}}
{"id": "2510.24178", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24178", "abs": "https://arxiv.org/abs/2510.24178", "authors": ["Aaron Scott", "Maike Z\u00fcfle", "Jan Niehues"], "title": "MuSaG: A Multimodal German Sarcasm Dataset with Full-Modal Annotations", "comment": null, "summary": "Sarcasm is a complex form of figurative language in which the intended\nmeaning contradicts the literal one. Its prevalence in social media and popular\nculture poses persistent challenges for natural language understanding,\nsentiment analysis, and content moderation. With the emergence of multimodal\nlarge language models, sarcasm detection extends beyond text and requires\nintegrating cues from audio and vision. We present MuSaG, the first German\nmultimodal sarcasm detection dataset, consisting of 33 minutes of manually\nselected and human-annotated statements from German television shows. Each\ninstance provides aligned text, audio, and video modalities, annotated\nseparately by humans, enabling evaluation in unimodal and multimodal settings.\nWe benchmark nine open-source and commercial models, spanning text, audio,\nvision, and multimodal architectures, and compare their performance to human\nannotations. Our results show that while humans rely heavily on audio in\nconversational settings, models perform best on text. This highlights a gap in\ncurrent multimodal models and motivates the use of MuSaG for developing models\nbetter suited to realistic scenarios. We release MuSaG publicly to support\nfuture research on multimodal sarcasm detection and human-model alignment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u5fb7\u8bed\u591a\u6a21\u6001\u8bbd\u523a\u68c0\u6d4b\u6570\u636e\u96c6MuSaG\uff0c\u5305\u542b\u6587\u672c\u3001\u97f3\u9891\u548c\u89c6\u9891\u4e09\u79cd\u6a21\u6001\u768433\u5206\u949f\u7535\u89c6\u8282\u76ee\u7247\u6bb5\uff0c\u5e76\u9488\u5bf9\u4e5d\u4e2a\u6a21\u578b\u8fdb\u884c\u4e86\u6027\u80fd\u8bc4\u6d4b\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728\u6587\u672c\u4e0a\u8868\u73b0\u6700\u597d\uff0c\u4f46\u4e0e\u4eba\u7c7b\u4f9d\u8d56\u97f3\u9891\u4fe1\u53f7\u7684\u65b9\u5f0f\u5b58\u5728\u5dee\u8ddd\u3002", "motivation": "\u8bbd\u523a\u662f\u4e00\u79cd\u590d\u6742\u7684\u8bed\u8a00\u73b0\u8c61\uff0c\u96be\u4ee5\u901a\u8fc7\u6587\u672c\u5355\u6a21\u6001\u51c6\u786e\u8bc6\u522b\uff0c\u5c24\u5176\u5728\u793e\u4ea4\u5a92\u4f53\u548c\u6d41\u884c\u6587\u5316\u4e2d\u51fa\u73b0\u9891\u7e41\uff0c\u591a\u6a21\u6001\u6a21\u578b\u7684\u53d1\u5c55\u9700\u8981\u65b0\u7684\u6570\u636e\u652f\u6301\u3002", "method": "\u6784\u5efa\u4e86MuSaG\u6570\u636e\u96c6\uff0c\u5305\u542b\u5bf9\u5fb7\u8bed\u7535\u89c6\u8282\u76ee\u4e2d\u8a00\u8bba\u7684\u591a\u6a21\u6001\u6570\u636e\u548c\u4eba\u5de5\u6ce8\u91ca\uff0c\u8bc4\u6d4b\u4e86\u4e5d\u4e2a\u5f00\u6e90\u4e0e\u5546\u4e1a\u6a21\u578b\u5728\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u4e0a\u7684\u8bbd\u523a\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u4e0e\u4eba\u5de5\u6ce8\u91ca\u7ed3\u679c\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u4eba\u7c7b\u5728\u4f1a\u8bdd\u73af\u5883\u4e2d\u66f4\u4f9d\u8d56\u97f3\u9891\u7ebf\u7d22\u8bc6\u522b\u8bbd\u523a\uff0c\u800c\u73b0\u6709\u6a21\u578b\u5728\u6587\u672c\u6a21\u6001\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u8bf4\u660e\u591a\u6a21\u6001\u6a21\u578b\u5c1a\u672a\u5145\u5206\u5229\u7528\u97f3\u9891\u4fe1\u606f\u3002", "conclusion": "MuSaG\u586b\u8865\u4e86\u5fb7\u8bed\u591a\u6a21\u6001\u8bbd\u523a\u68c0\u6d4b\u6570\u636e\u7a7a\u767d\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u4e0d\u8db3\uff0c\u4fc3\u8fdb\u66f4\u9002\u5408\u771f\u5b9e\u573a\u666f\u7684\u591a\u6a21\u6001\u8bbd\u523a\u68c0\u6d4b\u6a21\u578b\u7684\u5f00\u53d1\u3002\u8be5\u6570\u636e\u96c6\u5df2\u516c\u5f00\u4ee5\u652f\u6301\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2510.24179", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24179", "abs": "https://arxiv.org/abs/2510.24179", "authors": ["Iv\u00e1n Mart\u00ednez-Murillo", "Paloma Moreda", "Elena Lloret"], "title": "Exploring the Influence of Relevant Knowledge for Natural Language Generation Interpretability", "comment": null, "summary": "This paper explores the influence of external knowledge integration in\nNatural Language Generation (NLG), focusing on a commonsense generation task.\nWe extend the CommonGen dataset by creating KITGI, a benchmark that pairs input\nconcept sets with retrieved semantic relations from ConceptNet and includes\nmanually annotated outputs. Using the T5-Large model, we compare sentence\ngeneration under two conditions: with full external knowledge and with filtered\nknowledge where highly relevant relations were deliberately removed. Our\ninterpretability benchmark follows a three-stage method: (1) identifying and\nremoving key knowledge, (2) regenerating sentences, and (3) manually assessing\noutputs for commonsense plausibility and concept coverage. Results show that\nsentences generated with full knowledge achieved 91\\% correctness across both\ncriteria, while filtering reduced performance drastically to 6\\%. These\nfindings demonstrate that relevant external knowledge is critical for\nmaintaining both coherence and concept coverage in NLG. This work highlights\nthe importance of designing interpretable, knowledge-enhanced NLG systems and\ncalls for evaluation frameworks that capture the underlying reasoning beyond\nsurface-level metrics.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5916\u90e8\u77e5\u8bc6\u6574\u5408\u5bf9\u81ea\u7136\u8bed\u8a00\u751f\u6210\uff08NLG\uff09\u4e2d\u5e38\u8bc6\u751f\u6210\u4efb\u52a1\u7684\u5f71\u54cd\uff0c\u6784\u5efa\u4e86KITGI\u57fa\u51c6\u5e76\u9a8c\u8bc1\u4e86\u77e5\u8bc6\u5bf9\u751f\u6210\u8d28\u91cf\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u63a2\u7d22\u5916\u90e8\u77e5\u8bc6\u5bf9\u63d0\u9ad8NLG\u751f\u6210\u53e5\u5b50\u5e38\u8bc6\u5408\u7406\u6027\u548c\u6982\u5ff5\u8986\u76d6\u7684\u4f5c\u7528\uff0c\u89e3\u51b3\u73b0\u6709\u7cfb\u7edf\u7f3a\u4e4f\u89e3\u91ca\u6027\u548c\u77e5\u8bc6\u6574\u5408\u8bc4\u4f30\u7684\u95ee\u9898\u3002", "method": "\u6269\u5c55CommonGen\u6570\u636e\u96c6\uff0c\u6784\u5efaKITGI\u57fa\u51c6\uff0c\u7ed3\u5408ConceptNet\u8bed\u4e49\u5173\u7cfb\uff1b\u91c7\u7528T5-Large\u6a21\u578b\u6bd4\u8f83\u5b8c\u6574\u77e5\u8bc6\u548c\u8fc7\u6ee4\u77e5\u8bc6\u4e24\u79cd\u6761\u4ef6\u4e0b\u7684\u53e5\u5b50\u751f\u6210\u8868\u73b0\uff0c\u8bbe\u8ba1\u4e09\u9636\u6bb5\u53ef\u89e3\u91ca\u6027\u8bc4\u6d4b\u65b9\u6cd5\u3002", "result": "\u5b8c\u6574\u77e5\u8bc6\u6761\u4ef6\u4e0b\u751f\u6210\u53e5\u5b50\u6b63\u786e\u7387\u8fbe\u523091%\uff0c\u8fc7\u6ee4\u6389\u5173\u952e\u4fe1\u606f\u65f6\u6b63\u786e\u7387\u9aa4\u964d\u81f36%\uff0c\u8bc1\u660e\u5173\u7cfb\u6027\u77e5\u8bc6\u5bf9\u751f\u6210\u7684\u8fde\u8d2f\u6027\u548c\u8986\u76d6\u5ea6\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u76f8\u5173\u7684\u5916\u90e8\u77e5\u8bc6\u5bf9\u63d0\u5347NLG\u7684\u5e38\u8bc6\u5408\u7406\u6027\u548c\u8986\u76d6\u5ea6\u5177\u6709\u5173\u952e\u4f5c\u7528\uff0c\u5f3a\u8c03\u8bbe\u8ba1\u89e3\u91ca\u6027\u5f3a\u7684\u77e5\u8bc6\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u548c\u66f4\u6df1\u5165\u7684\u8bc4\u6d4b\u6846\u67b6\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2510.24208", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24208", "abs": "https://arxiv.org/abs/2510.24208", "authors": ["Jian Gu", "Aldeida Aleti", "Chunyang Chen", "Hongyu Zhang"], "title": "Beyond Neural Incompatibility: Easing Cross-Scale Knowledge Transfer in Large Language Models through Latent Semantic Alignment", "comment": "an early-stage version", "summary": "Large Language Models (LLMs) encode vast amounts of knowledge in their\nmassive parameters, which is accessible to locate, trace, and analyze. Despite\nadvances in neural interpretability, it is still not clear how to transfer\nknowledge in a fine-grained manner, namely parametric knowledge transfer (PKT).\nA key problem is enabling effective and efficient knowledge transfer across\nLLMs of different scales, which is essential for achieving greater flexibility\nand broader applicability in transferring knowledge between LLMs. Due to neural\nincompatibility, referring to the architectural and parametric differences\nbetween LLMs of varying scales, existing methods that directly reuse layer\nparameters are severely limited. In this paper, we identify the semantic\nalignment in latent space as the fundamental prerequisite for LLM cross-scale\nknowledge transfer. Instead of directly using the layer parameters, our\napproach takes activations as the medium of layer-wise knowledge transfer.\nLeveraging the semantics in latent space, our approach is simple and\noutperforms prior work, better aligning model behaviors across varying scales.\nEvaluations on four benchmarks demonstrate the efficacy of our method. Further\nanalysis reveals the key factors easing cross-scale knowledge transfer and\nprovides insights into the nature of latent semantic alignment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u6f5c\u5728\u7a7a\u95f4\u8bed\u4e49\u5bf9\u9f50\u7684\u7ec6\u7c92\u5ea6\u8de8\u5c3a\u5ea6\u5927\u578b\u8bed\u8a00\u6a21\u578b\u77e5\u8bc6\u4f20\u9012\u65b9\u6cd5\uff0c\u901a\u8fc7\u6fc0\u6d3b\u4f5c\u4e3a\u5c42\u95f4\u77e5\u8bc6\u4f20\u9012\u5a92\u4ecb\uff0c\u89e3\u51b3\u4e86\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u95f4\u53c2\u6570\u548c\u7ed3\u6784\u4e0d\u517c\u5bb9\u95ee\u9898\uff0c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4e0d\u540c\u89c4\u6a21\u95f4\u8fdb\u884c\u7ec6\u7c92\u5ea6\u53c2\u6570\u77e5\u8bc6\u4f20\u9012\u5b58\u5728\u56f0\u96be\uff0c\u4e3b\u8981\u7531\u4e8e\u6a21\u578b\u67b6\u6784\u548c\u53c2\u6570\u5dee\u5f02\u5bfc\u81f4\u7684\u795e\u7ecf\u4e0d\u517c\u5bb9\u95ee\u9898\u3002\u5982\u4f55\u5b9e\u73b0\u9ad8\u6548\u6709\u6548\u7684\u8de8\u5c3a\u5ea6\u77e5\u8bc6\u8f6c\u79fb\u6210\u4e3a\u96be\u70b9\u3002", "method": "\u672c\u6587\u63d0\u51fa\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u8bed\u4e49\u5bf9\u9f50\uff0c\u5229\u7528\u6fc0\u6d3b\u800c\u975e\u76f4\u63a5\u91cd\u7528\u5c42\u53c2\u6570\uff0c\u4f5c\u4e3a\u5c42\u95f4\u77e5\u8bc6\u4f20\u9012\u7684\u5a92\u4ecb\uff0c\u6539\u5584\u4e86\u6a21\u578b\u95f4\u884c\u4e3a\u7684\u4e00\u81f4\u6027\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u8de8\u5c3a\u5ea6\u77e5\u8bc6\u8fc1\u79fb\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u8868\u73b0\u4f18\u8d8a\uff0c\u8bc1\u660e\u4e86\u5229\u7528\u6f5c\u5728\u8bed\u4e49\u5bf9\u9f50\u8fdb\u884c\u77e5\u8bc6\u8f6c\u79fb\u7684\u6709\u6548\u6027\u3002\u8fdb\u4e00\u6b65\u7684\u5206\u6790\u63ed\u793a\u4e86\u4fc3\u8fdb\u8de8\u5c3a\u5ea6\u77e5\u8bc6\u8f6c\u79fb\u7684\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "\u6f5c\u5728\u7a7a\u95f4\u7684\u8bed\u4e49\u5bf9\u9f50\u662f\u5b9e\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8de8\u5c3a\u5ea6\u77e5\u8bc6\u4f20\u9012\u7684\u57fa\u7840\uff0c\u6fc0\u6d3b\u4f5c\u4e3a\u77e5\u8bc6\u4f20\u9012\u5a92\u4ecb\u80fd\u6709\u6548\u514b\u670d\u53c2\u6570\u548c\u7ed3\u6784\u5dee\u5f02\u5e26\u6765\u7684\u9650\u5236\uff0c\u4e3a\u795e\u7ecf\u7f51\u7edc\u89e3\u91ca\u6027\u548c\u7075\u6d3b\u77e5\u8bc6\u8f6c\u79fb\u63d0\u4f9b\u65b0\u8def\u5f84\u3002"}}
{"id": "2510.24222", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.24222", "abs": "https://arxiv.org/abs/2510.24222", "authors": ["Adi Simhi", "Jonathan Herzig", "Itay Itzhak", "Dana Arad", "Zorik Gekhman", "Roi Reichart", "Fazl Barez", "Gabriel Stanovsky", "Idan Szpektor", "Yonatan Belinkov"], "title": "HACK: Hallucinations Along Certainty and Knowledge Axes", "comment": "The code is available at\n  https://github.com/technion-cs-nlp/HACK_Hallucinations_Along_Certainty_and_Knowledge_axes", "summary": "Hallucinations in LLMs present a critical barrier to their reliable usage.\nExisting research usually categorizes hallucination by their external\nproperties rather than by the LLMs' underlying internal properties. This\nexternal focus overlooks that hallucinations may require tailored mitigation\nstrategies based on their underlying mechanism. We propose a framework for\ncategorizing hallucinations along two axes: knowledge and certainty. Since\nparametric knowledge and certainty may vary across models, our categorization\nmethod involves a model-specific dataset construction process that\ndifferentiates between those types of hallucinations. Along the knowledge axis,\nwe distinguish between hallucinations caused by a lack of knowledge and those\noccurring despite the model having the knowledge of the correct response. To\nvalidate our framework along the knowledge axis, we apply steering mitigation,\nwhich relies on the existence of parametric knowledge to manipulate model\nactivations. This addresses the lack of existing methods to validate knowledge\ncategorization by showing a significant difference between the two\nhallucination types. We further analyze the distinct knowledge and\nhallucination patterns between models, showing that different hallucinations do\noccur despite shared parametric knowledge. Turning to the certainty axis, we\nidentify a particularly concerning subset of hallucinations where models\nhallucinate with certainty despite having the correct knowledge internally. We\nintroduce a new evaluation metric to measure the effectiveness of mitigation\nmethods on this subset, revealing that while some methods perform well on\naverage, they fail disproportionately on these critical cases. Our findings\nhighlight the importance of considering both knowledge and certainty in\nhallucination analysis and call for targeted mitigation approaches that\nconsider the hallucination underlying factors.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u77e5\u8bc6\u548c\u786e\u5b9a\u6027\u4e24\u4e2a\u7ef4\u5ea6\u7684LLM\u5e7b\u89c9\u5206\u7c7b\u6846\u67b6\uff0c\u5e76\u9488\u5bf9\u4e0d\u540c\u7c7b\u578b\u7684\u5e7b\u89c9\u8bbe\u8ba1\u4e86\u5dee\u5f02\u5316\u7684\u7f13\u89e3\u7b56\u7565\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u4ece\u5916\u90e8\u5c5e\u6027\u5206\u7c7bLLM\u5e7b\u89c9\uff0c\u5ffd\u89c6\u4e86\u5e7b\u89c9\u7684\u5185\u5728\u673a\u5236\uff0c\u5bfc\u81f4\u7f13\u89e3\u63aa\u65bd\u7f3a\u4e4f\u9488\u5bf9\u6027\u3002", "method": "\u6784\u5efa\u6a21\u578b\u7279\u5b9a\u6570\u636e\u96c6\uff0c\u4ece\u77e5\u8bc6\u7f3a\u5931\u53ca\u77e5\u8bc6\u5b58\u5728\u4f46\u5e7b\u89c9\u4e24\u79cd\u7c7b\u578b\u5206\u7c7b\u5e7b\u89c9\uff0c\u5e94\u7528\u6fc0\u6d3b\u64cd\u63a7\u9a8c\u8bc1\u5206\u7c7b\u6709\u6548\u6027\uff0c\u5e76\u5206\u6790\u4e0d\u540c\u6a21\u578b\u7684\u77e5\u8bc6\u548c\u5e7b\u89c9\u6a21\u5f0f\uff1b\u63d0\u51fa\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u8861\u91cf\u7f13\u89e3\u65b9\u6cd5\u5bf9\u9ad8\u786e\u5b9a\u6027\u5e7b\u89c9\u7684\u6548\u679c\u3002", "result": "\u663e\u8457\u533a\u5206\u4e86\u4e24\u7c7b\u5e7b\u89c9\uff0c\u5c55\u793a\u4e0d\u540c\u6a21\u578b\u5373\u4f7f\u5171\u4eab\u77e5\u8bc6\u4ecd\u6709\u4e0d\u540c\u5e7b\u89c9\u8868\u73b0\uff0c\u7f13\u89e3\u65b9\u6cd5\u5728\u6574\u4f53\u8868\u73b0\u826f\u597d\u4f46\u5bf9\u9ad8\u786e\u5b9a\u6027\u5e7b\u89c9\u6548\u679c\u6709\u9650\u3002", "conclusion": "\u5f3a\u8c03\u77e5\u8bc6\u4e0e\u786e\u5b9a\u6027\u4e24\u4e2a\u7ef4\u5ea6\u5728\u5e7b\u89c9\u5206\u7c7b\u548c\u7f13\u89e3\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u547c\u5401\u5f00\u53d1\u9488\u5bf9\u5e7b\u89c9\u5185\u5728\u673a\u5236\u7684\u5b9a\u5411\u7f13\u89e3\u7b56\u7565\u3002"}}
{"id": "2510.24236", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24236", "abs": "https://arxiv.org/abs/2510.24236", "authors": ["Teague McMillan", "Gabriele Dominici", "Martin Gjoreski", "Marc Langheinrich"], "title": "Towards Transparent Reasoning: What Drives Faithfulness in Large Language Models?", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025) Workshop: NeurIPS 2025 Workshop on Evaluating the Evolving LLM\n  Lifecycle: Benchmarks, Emergent Abilities, and Scaling", "summary": "Large Language Models (LLMs) often produce explanations that do not\nfaithfully reflect the factors driving their predictions. In healthcare\nsettings, such unfaithfulness is especially problematic: explanations that omit\nsalient clinical cues or mask spurious shortcuts can undermine clinician trust\nand lead to unsafe decision support. We study how inference and training-time\nchoices shape explanation faithfulness, focusing on factors practitioners can\ncontrol at deployment. We evaluate three LLMs (GPT-4.1-mini, LLaMA 70B, LLaMA\n8B) on two datasets-BBQ (social bias) and MedQA (medical licensing questions),\nand manipulate the number and type of few-shot examples, prompting strategies,\nand training procedure. Our results show: (i) both the quantity and quality of\nfew-shot examples significantly impact model faithfulness; (ii) faithfulness is\nsensitive to prompting design; (iii) the instruction-tuning phase improves\nmeasured faithfulness on MedQA. These findings offer insights into strategies\nfor enhancing the interpretability and trustworthiness of LLMs in sensitive\ndomains.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u89e3\u91ca\u5176\u9884\u6d4b\u65f6\u7684\u5fe0\u5b9e\u5ea6\u95ee\u9898\uff0c\u7279\u522b\u662f\u533b\u7597\u9886\u57df\u4e2d\u4e0d\u5fe0\u5b9e\u89e3\u91ca\u53ef\u80fd\u5bfc\u81f4\u4fe1\u4efb\u548c\u5b89\u5168\u98ce\u9669\uff0c\u5e76\u63a2\u7a76\u4e86\u63a8\u7406\u548c\u8bad\u7ec3\u9009\u62e9\u5bf9\u5fe0\u5b9e\u5ea6\u7684\u5f71\u54cd\u3002", "motivation": "\u533b\u7597\u9886\u57df\u5bf9\u89e3\u91ca\u7684\u5fe0\u5b9e\u5ea6\u8981\u6c42\u5f88\u9ad8\uff0c\u4e0d\u51c6\u786e\u7684\u89e3\u91ca\u4f1a\u524a\u5f31\u4e34\u5e8a\u533b\u751f\u7684\u4fe1\u4efb\u5e76\u5e26\u6765\u5b89\u5168\u9690\u60a3\uff0c\u56e0\u6b64\u63a2\u7d22\u6a21\u578b\u63a8\u7406\u548c\u8bad\u7ec3\u4e2d\u53ef\u63a7\u56e0\u7d20\u5bf9\u89e3\u91ca\u5fe0\u5b9e\u5ea6\u7684\u5f71\u54cd\u975e\u5e38\u91cd\u8981\u3002", "method": "\u8bc4\u4f30\u4e86\u4e09\u4e2aLLMs\uff08GPT-4.1-mini, LLaMA 70B, LLaMA 8B\uff09\u5728\u4e24\u4e2a\u6570\u636e\u96c6\uff08BBQ\u793e\u4f1a\u504f\u89c1\u548cMedQA\u533b\u7597\u8bb8\u53ef\u95ee\u7b54\uff09\u4e0a\u7684\u8868\u73b0\uff0c\u64cd\u63a7\u5c11\u6837\u672c\u793a\u4f8b\u6570\u91cf\u4e0e\u8d28\u91cf\u3001\u63d0\u793a\u8bbe\u8ba1\u3001\u8bad\u7ec3\u8fc7\u7a0b\u7b49\u53d8\u91cf\uff0c\u5206\u6790\u5176\u5bf9\u89e3\u91ca\u5fe0\u5b9e\u5ea6\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u5c11\u6837\u672c\u793a\u4f8b\u7684\u6570\u91cf\u548c\u8d28\u91cf\u663e\u8457\u5f71\u54cd\u89e3\u91ca\u5fe0\u5b9e\u5ea6\uff0c\u63d0\u793a\u8bbe\u8ba1\u5bf9\u5fe0\u5b9e\u5ea6\u654f\u611f\uff0c\u6307\u4ee4\u8c03\u4f18\u8bad\u7ec3\u9636\u6bb5\u63d0\u5347\u4e86MedQA\u4e0a\u7684\u5fe0\u5b9e\u5ea6\u8868\u73b0\u3002", "conclusion": "\u901a\u8fc7\u8c03\u6574\u5c11\u6837\u672c\u793a\u4f8b\u3001\u63d0\u793a\u7b56\u7565\u548c\u8bad\u7ec3\u6d41\u7a0b\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347LLMs\u5728\u654f\u611f\u9886\u57df\uff08\u5982\u533b\u7597\uff09\u7684\u89e3\u91ca\u5fe0\u5b9e\u5ea6\uff0c\u4ece\u800c\u589e\u5f3a\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u7528\u6237\u4fe1\u4efb\u3002"}}
{"id": "2510.24247", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24247", "abs": "https://arxiv.org/abs/2510.24247", "authors": ["Ahmad Ghannam", "Naif Alharthi", "Faris Alasmary", "Kholood Al Tabash", "Shouq Sadah", "Lahouari Ghouti"], "title": "Abjad AI at NADI 2025: CATT-Whisper: Multimodal Diacritic Restoration Using Text and Speech Representations", "comment": null, "summary": "In this work, we tackle the Diacritic Restoration (DR) task for Arabic\ndialectal sentences using a multimodal approach that combines both textual and\nspeech information. We propose a model that represents the text modality using\nan encoder extracted from our own pre-trained model named CATT. The speech\ncomponent is handled by the encoder module of the OpenAI Whisper base model.\nOur solution is designed following two integration strategies. The former\nconsists of fusing the speech tokens with the input at an early stage, where\nthe 1500 frames of the audio segment are averaged over 10 consecutive frames,\nresulting in 150 speech tokens. To ensure embedding compatibility, these\naveraged tokens are processed through a linear projection layer prior to\nmerging them with the text tokens. Contextual encoding is guaranteed by the\nCATT encoder module. The latter strategy relies on cross-attention, where text\nand speech embeddings are fused. The cross-attention output is then fed to the\nCATT classification head for token-level diacritic prediction. To further\nimprove model robustness, we randomly deactivate the speech input during\ntraining, allowing the model to perform well with or without speech. Our\nexperiments show that the proposed approach achieves a word error rate (WER) of\n0.25 and a character error rate (CER) of 0.9 on the development set. On the\ntest set, our model achieved WER and CER scores of 0.55 and 0.13, respectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6587\u672c\u548c\u8bed\u97f3\u4fe1\u606f\u7684\u963f\u62c9\u4f2f\u65b9\u8a00\u52a0\u5143\u97f3\u7b26\u53f7\u8fd8\u539f\u6a21\u578b\uff0c\u5229\u7528\u591a\u6a21\u6001\u878d\u5408\u63d0\u5347\u51c6\u786e\u7387\u3002", "motivation": "\u963f\u62c9\u4f2f\u65b9\u8a00\u7684\u52a0\u5143\u97f3\u7b26\u53f7\u8fd8\u539f\u4efb\u52a1\u5177\u6709\u6311\u6218\u6027\uff0c\u5355\u4e00\u6587\u672c\u4fe1\u606f\u96be\u4ee5\u51c6\u786e\u6062\u590d\uff0c\u7ed3\u5408\u8bed\u97f3\u4fe1\u606f\u6709\u671b\u63d0\u5347\u6027\u80fd\u3002", "method": "\u91c7\u7528\u6587\u672c\u7f16\u7801\u5668\uff08CATT\uff09\u548c\u8bed\u97f3\u7f16\u7801\u5668\uff08OpenAI Whisper\u57fa\u6a21\u578b\uff09\uff0c\u8bbe\u8ba1\u4e24\u79cd\u878d\u5408\u7b56\u7565\uff1a\u65e9\u671f\u878d\u5408\u548c\u57fa\u4e8e\u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u878d\u5408\uff0c\u5e76\u901a\u8fc7\u8bad\u7ec3\u65f6\u968f\u673a\u505c\u7528\u8bed\u97f3\u8f93\u5165\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\u3002", "result": "\u5728\u5f00\u53d1\u96c6\u4e0a\uff0c\u6a21\u578b\u5b9e\u73b0\u4e86\u8bcd\u9519\u8bef\u73870.25\u548c\u5b57\u7b26\u9519\u8bef\u73870.9\uff1b\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u8bcd\u9519\u8bef\u7387\u4e3a0.55\uff0c\u5b57\u7b26\u9519\u8bef\u7387\u4e3a0.13\u3002", "conclusion": "\u7ed3\u5408\u6587\u672c\u4e0e\u8bed\u97f3\u7684\u591a\u6a21\u6001\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u963f\u62c9\u4f2f\u65b9\u8a00\u52a0\u5143\u97f3\u7b26\u53f7\u8fd8\u539f\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u4e14\u5177\u6709\u8f83\u597d\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.24250", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24250", "abs": "https://arxiv.org/abs/2510.24250", "authors": ["Syed Zohaib Hassan", "P\u00e5l Halvorsen", "Miriam S. Johnson", "Pierre Lison"], "title": "Evaluating LLMs on Generating Age-Appropriate Child-Like Conversations", "comment": "11 pages excluding references and appendix. 3 figures and 6 tables", "summary": "Large Language Models (LLMs), predominantly trained on adult conversational\ndata, face significant challenges when generating authentic, child-like\ndialogue for specialized applications. We present a comparative study\nevaluating five different LLMs (GPT-4, RUTER-LLAMA-2-13b, GPTSW, NorMistral-7b,\nand NorBloom-7b) to generate age-appropriate Norwegian conversations for\nchildren aged 5 and 9 years. Through a blind evaluation by eleven education\nprofessionals using both real child interview data and LLM-generated text\nsamples, we assessed authenticity and developmental appropriateness. Our\nresults show that evaluators achieved strong inter-rater reliability (ICC=0.75)\nand demonstrated higher accuracy in age prediction for younger children\n(5-year-olds) compared to older children (9-year-olds). While GPT-4 and\nNorBloom-7b performed relatively well, most models generated language perceived\nas more linguistically advanced than the target age groups. These findings\nhighlight critical data-related challenges in developing LLM systems for\nspecialized applications involving children, particularly in low-resource\nlanguages where comprehensive age-appropriate lexical resources are scarce.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u4e94\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u9002\u9f84\u513f\u7ae5\uff085\u5c81\u548c9\u5c81\uff09\u632a\u5a01\u8bed\u5bf9\u8bdd\u4e2d\u7684\u8868\u73b0\u3002\u901a\u8fc7\u6559\u80b2\u4e13\u5bb6\u7684\u76f2\u8bc4\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u8f83\u5c0f\u5e74\u9f84\u7ec4\u7684\u9884\u6d4b\u51c6\u786e\u6027\u8f83\u9ad8\uff0c\u4f46\u5927\u591a\u6570\u6a21\u578b\u751f\u6210\u7684\u8bed\u8a00\u504f\u5411\u66f4\u6210\u719f\u3002", "motivation": "\u7531\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u57fa\u4e8e\u6210\u4eba\u5bf9\u8bdd\u6570\u636e\u8bad\u7ec3\uff0c\u751f\u6210\u771f\u5b9e\u513f\u7ae5\u5bf9\u8bdd\u5b58\u5728\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u4e13\u95e8\u5e94\u7528\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e0b\u3002", "method": "\u9009\u53d6\u4e94\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u62105\u5c81\u548c9\u5c81\u513f\u7ae5\u632a\u5a01\u8bed\u5bf9\u8bdd\u6587\u672c\uff0c\u9080\u8bf711\u540d\u6559\u80b2\u4e13\u5bb6\u8fdb\u884c\u76f2\u8bc4\uff0c\u57fa\u4e8e\u771f\u5b9e\u513f\u7ae5\u9762\u8bd5\u6570\u636e\u5bf9\u751f\u6210\u6587\u672c\u7684\u771f\u5b9e\u6027\u548c\u53d1\u5c55\u9002\u9f84\u6027\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u8005\u8868\u73b0\u51fa\u8f83\u9ad8\u7684\u4e00\u81f4\u6027\uff08ICC=0.75\uff09\uff0c\u5bf95\u5c81\u513f\u7ae5\u7684\u5e74\u9f84\u9884\u6d4b\u51c6\u786e\u6027\u9ad8\u4e8e9\u5c81\u7ec4\u3002GPT-4\u548cNorBloom-7b\u8868\u73b0\u8f83\u597d\uff0c\u5927\u591a\u6570\u6a21\u578b\u751f\u6210\u7684\u8bed\u8a00\u76f8\u6bd4\u76ee\u6807\u5e74\u9f84\u66f4\u6210\u719f\u3002", "conclusion": "\u5728\u513f\u7ae5\u4e13\u7528\u5bf9\u8bdd\u751f\u6210\u4e2d\uff0c\u5c24\u5176\u662f\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e2d\uff0c\u6570\u636e\u76f8\u5173\u95ee\u9898\u5bf9LLM\u7684\u6027\u80fd\u5f71\u54cd\u663e\u8457\uff0c\u5f3a\u8c03\u4e86\u5f00\u53d1\u9002\u9f84\u513f\u7ae5\u5bf9\u8bdd\u7cfb\u7edf\u65f6\u9700\u8981\u5173\u6ce8\u6570\u636e\u7684\u9488\u5bf9\u6027\u548c\u5145\u5206\u6027\u3002"}}
{"id": "2510.24256", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24256", "abs": "https://arxiv.org/abs/2510.24256", "authors": ["Jack Merullo", "Srihita Vatsavaya", "Lucius Bushnaq", "Owen Lewis"], "title": "From Memorization to Reasoning in the Spectrum of Loss Curvature", "comment": null, "summary": "We characterize how memorization is represented in transformer models and\nshow that it can be disentangled in the weights of both language models (LMs)\nand vision transformers (ViTs) using a decomposition based on the loss\nlandscape curvature. This insight is based on prior theoretical and empirical\nwork showing that the curvature for memorized training points is much sharper\nthan non memorized, meaning ordering weight components from high to low\ncurvature can reveal a distinction without explicit labels. This motivates a\nweight editing procedure that suppresses far more recitation of untargeted\nmemorized data more effectively than a recent unlearning method\n(BalancedSubnet), while maintaining lower perplexity. Since the basis of\ncurvature has a natural interpretation for shared structure in model weights,\nwe analyze the editing procedure extensively on its effect on downstream tasks\nin LMs, and find that fact retrieval and arithmetic are specifically and\nconsistently negatively affected, even though open book fact retrieval and\ngeneral logical reasoning is conserved. We posit these tasks rely heavily on\nspecialized directions in weight space rather than general purpose mechanisms,\nregardless of whether those individual datapoints are memorized. We support\nthis by showing a correspondence between task data's activation strength with\nlow curvature components that we edit out, and the drop in task performance\nafter the edit. Our work enhances the understanding of memorization in neural\nnetworks with practical applications towards removing it, and provides evidence\nfor idiosyncratic, narrowly-used structures involved in solving tasks like math\nand fact retrieval.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u57fa\u4e8e\u635f\u5931\u66f2\u7387\u7684\u6743\u91cd\u5206\u89e3\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86Transformer\u6a21\u578b\u4e2d\u8bb0\u5fc6\u7684\u8868\u793a\u5f62\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6743\u91cd\u7f16\u8f91\u7b56\u7565\u6765\u51cf\u5c11\u975e\u76ee\u6807\u8bb0\u5fc6\u5185\u5bb9\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7406\u8bba\u548c\u5b9e\u9a8c\u8bc1\u660e\u8bb0\u5fc6\u70b9\u7684\u635f\u5931\u66f2\u7387\u8f83\u9510\u5229\uff0c\u5982\u4f55\u5229\u7528\u8fd9\u4e00\u6027\u8d28\u5206\u79bb\u6a21\u578b\u8bb0\u5fc6\u5e76\u6709\u6548\u53bb\u9664\u672a\u76ee\u6807\u8bb0\u5fc6\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u5b9e\u7528\u6027\u6210\u4e3a\u7814\u7a76\u91cd\u70b9\u3002", "method": "\u5229\u7528\u635f\u5931\u66f2\u7387\u5bf9Transformer\u6a21\u578b\u6743\u91cd\u8fdb\u884c\u6392\u5e8f\u548c\u5206\u89e3\uff0c\u901a\u8fc7\u6291\u5236\u9ad8\u66f2\u7387\u6743\u91cd\u5206\u91cf\u6765\u5b9e\u73b0\u8bb0\u5fc6\u7684\u6743\u91cd\u7f16\u8f91\uff0c\u5e76\u5bf9\u7f16\u8f91\u540e\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u5f71\u54cd\u8fdb\u884c\u7cfb\u7edf\u5206\u6790\u3002", "result": "\u63d0\u51fa\u7684\u6743\u91cd\u7f16\u8f91\u65b9\u6cd5\u6bd4\u73b0\u6709\u7684\u53bb\u5b66\u4e60\u65b9\u6cd5\uff08\u5982BalancedSubnet\uff09\u66f4\u6709\u6548\u5730\u51cf\u5c11\u672a\u76ee\u6807\u8bb0\u5fc6\u5185\u5bb9\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u4f4e\u56f0\u60d1\u5ea6\uff1b\u7f16\u8f91\u8fc7\u7a0b\u4e2d\u7279\u5b9a\u4efb\u52a1\u5982\u4e8b\u5b9e\u68c0\u7d22\u548c\u7b97\u672f\u80fd\u529b\u660e\u663e\u4e0b\u964d\uff0c\u5408\u7406\u89e3\u91ca\u4e86\u4efb\u52a1\u4f9d\u8d56\u4e8e\u7279\u6b8a\u65b9\u5411\u7684\u6743\u91cd\u3002", "conclusion": "\u8be5\u7814\u7a76\u52a0\u6df1\u4e86\u5bf9\u795e\u7ecf\u7f51\u7edc\u8bb0\u5fc6\u673a\u5236\u7684\u7406\u89e3\uff0c\u5c55\u793a\u4e86\u8bb0\u5fc6\u53ef\u4ee5\u5728\u6743\u91cd\u4e2d\u88ab\u533a\u5206\u548c\u7f16\u8f91\uff0c\u652f\u6301\u4e86\u4efb\u52a1\u76f8\u5173\u7684\u7279\u5b9a\u7a84\u57df\u7ed3\u6784\u5f71\u54cd\u6a21\u578b\u8868\u73b0\uff0c\u4e3a\u8bb0\u5fc6\u53bb\u9664\u63d0\u4f9b\u7406\u8bba\u548c\u5b9e\u9645\u6307\u5bfc\u3002"}}
{"id": "2510.24259", "categories": ["cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24259", "abs": "https://arxiv.org/abs/2510.24259", "authors": ["Ziqi Ma", "Sao Mai Nguyen", "Philippe Xu"], "title": "Can LLMs Translate Human Instructions into a Reinforcement Learning Agent's Internal Emergent Symbolic Representation?", "comment": null, "summary": "Emergent symbolic representations are critical for enabling developmental\nlearning agents to plan and generalize across tasks. In this work, we\ninvestigate whether large language models (LLMs) can translate human natural\nlanguage instructions into the internal symbolic representations that emerge\nduring hierarchical reinforcement learning. We apply a structured evaluation\nframework to measure the translation performance of commonly seen LLMs -- GPT,\nClaude, Deepseek and Grok -- across different internal symbolic partitions\ngenerated by a hierarchical reinforcement learning algorithm in the Ant Maze\nand Ant Fall environments. Our findings reveal that although LLMs demonstrate\nsome ability to translate natural language into a symbolic representation of\nthe environment dynamics, their performance is highly sensitive to partition\ngranularity and task complexity. The results expose limitations in current LLMs\ncapacity for representation alignment, highlighting the need for further\nresearch on robust alignment between language and internal agent\nrepresentations.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u5426\u5c06\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8f6c\u6362\u4e3a\u5c42\u7ea7\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7b26\u53f7\u8868\u793a\uff0c\u53d1\u73b0\u5176\u53d7\u4efb\u52a1\u590d\u6742\u5ea6\u548c\u7b26\u53f7\u7c92\u5ea6\u5f71\u54cd\u8f83\u5927\uff0c\u5b58\u5728\u8868\u793a\u5bf9\u9f50\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4fc3\u8fdb\u5b66\u4e60\u4ee3\u7406\u901a\u8fc7\u7b26\u53f7\u8868\u793a\u66f4\u597d\u5730\u89c4\u5212\u548c\u6cdb\u5316\u4efb\u52a1\uff0c\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u4e0e\u5185\u90e8\u7b26\u53f7\u8868\u793a\u8f6c\u6362\u4e2d\u7684\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u7ed3\u6784\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u6bd4\u8f83\u591a\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08GPT\u3001Claude\u3001Deepseek\u3001Grok\uff09\u5bf9\u5f3a\u5316\u5b66\u4e60\u4e2d\u7b26\u53f7\u5206\u5272\u7684\u7ffb\u8bd1\u8868\u73b0\uff0c\u6d4b\u8bd5\u73af\u5883\u4e3aAnt Maze\u548cAnt Fall\u3002", "result": "\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5177\u5907\u4e00\u5b9a\u7684\u7ffb\u8bd1\u80fd\u529b\u4f46\u8868\u73b0\u53d7\u7b26\u53f7\u7c92\u5ea6\u548c\u4efb\u52a1\u590d\u6742\u5ea6\u5f71\u54cd\u663e\u8457\uff0c\u5b58\u5728\u8868\u793a\u5bf9\u9f50\u7684\u9650\u5236\u3002", "conclusion": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u8a00\u4e0e\u5185\u90e8\u7b26\u53f7\u8868\u793a\u5bf9\u9f50\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u5b9e\u73b0\u66f4\u7a33\u5065\u7684\u8868\u793a\u5bf9\u9f50\u3002"}}
{"id": "2510.24295", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24295", "abs": "https://arxiv.org/abs/2510.24295", "authors": ["M\u0103d\u0103lina Zgreab\u0103n", "Tejaswini Deoskar", "Lasha Abzianidze"], "title": "MERGE: Minimal Expression-Replacement GEneralization Test for Natural Language Inference", "comment": "Pre-print", "summary": "In recent years, many generalization benchmarks have shown language models'\nlack of robustness in natural language inference (NLI). However, manually\ncreating new benchmarks is costly, while automatically generating high-quality\nones, even by modifying existing benchmarks, is extremely difficult. In this\npaper, we propose a methodology for automatically generating high-quality\nvariants of original NLI problems by replacing open-class words, while\ncrucially preserving their underlying reasoning. We dub our generalization test\nas MERGE (Minimal Expression-Replacements GEneralization), which evaluates the\ncorrectness of models' predictions across reasoning-preserving variants of the\noriginal problem. Our results show that NLI models' perform 4-20% worse on\nvariants, suggesting low generalizability even on such minimally altered\nproblems. We also analyse how word class of the replacements, word probability,\nand plausibility influence NLI models' performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u751f\u6210\u81ea\u7136\u8bed\u8a00\u63a8\u7406(NLI)\u95ee\u9898\u9ad8\u8d28\u91cf\u53d8\u4f53\u7684\u65b9\u6cd5\uff0c\u5373MERGE\uff0c\u901a\u8fc7\u66ff\u6362\u5f00\u653e\u7c7b\u8bcd\u6c47\u4f46\u4fdd\u6301\u63a8\u7406\u4e0d\u53d8\uff0c\u8bc4\u4f30\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709NLI\u6a21\u578b\u5728\u6cdb\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u800c\u624b\u52a8\u6784\u5efa\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u6210\u672c\u9ad8\u6602\uff0c\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u6d4b\u8bd5\u96be\u5ea6\u5927\u3002", "method": "\u63d0\u51faMERGE\u65b9\u6cd5\uff1a\u901a\u8fc7\u6700\u5c0f\u8868\u8fbe\u66ff\u6362\uff0c\u66ff\u6362\u539f\u95ee\u9898\u4e2d\u7684\u5f00\u653e\u7c7b\u8bcd\u6c47\uff0c\u751f\u6210\u4fdd\u6301\u63a8\u7406\u903b\u8f91\u4e0d\u53d8\u7684NLI\u53d8\u4f53\uff0c\u7528\u4ee5\u6d4b\u8bd5\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cNLI\u6a21\u578b\u5728\u7ecf\u8fc7\u6700\u5c0f\u53d8\u6362\u7684\u53d8\u4f53\u4e0a\u6027\u80fd\u4e0b\u964d4-20%\uff0c\u8868\u660e\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u8f83\u5f31\u3002\u5206\u6790\u4e86\u66ff\u6362\u8bcd\u6c47\u7c7b\u522b\u3001\u8bcd\u9891\u53ca\u5408\u7406\u6027\u5bf9\u6a21\u578b\u8868\u73b0\u7684\u5f71\u54cd\u3002", "conclusion": "MERGE\u65b9\u6cd5\u6709\u6548\u63ed\u793a\u4e86\u5f53\u524dNLI\u6a21\u578b\u5728\u8f7b\u5fae\u53d8\u52a8\u4e0b\u7684\u6cdb\u5316\u4e0d\u8db3\uff0c\u5f3a\u8c03\u4e86\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.24302", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24302", "abs": "https://arxiv.org/abs/2510.24302", "authors": ["Shangyu Xing", "Siyuan Wang", "Chenyuan Yang", "Xinyu Dai", "Xiang Ren"], "title": "Lookahead Tree-Based Rollouts for Enhanced Trajectory-Level Exploration in Reinforcement Learning with Verifiable Rewards", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR), particularly with\nalgorithms like Group Relative Policy Optimization (GRPO), has proven highly\neffective in enhancing the reasoning capabilities of large language models.\nHowever, a critical bottleneck in current pipelines lies in the limited\ndiversity of sampled trajectories during group rollouts. Homogeneous\ntrajectories and their associated rewards would diminish the return signals for\npolicy updates, thereby hindering effective policy learning. This lack of\ndiversity stems primarily from token-level stochastic sampling, where local\nvariations are likely to collapse into near-identical reasoning paths. To\naddress this limitation, we propose Lookahead Tree-Based Rollouts (LATR), a\nnovel rollout strategy designed to explicitly promotes trajectory-level\ndiversity by enforcing branching into different candidate tokens likely to\nyield distinct continuations. Specifically, LATR iteratively operates in three\nstages: (1) branching at high-uncertainty generation steps, (2) performing\nlookahead simulation for each new branch, and (3) pruning branches that\nexhibits prolonged similarity during simulation. Compared with stochastic\nSampling, LATR accelerates policy learning by 131% on average and improves\nfinal pass@1 performance by 4.2% on both GRPO and Dynamic sAmpling Policy\nOptimization (DAPO) algorithms across different reasoning tasks. Our code and\ndata are publicly available at https://github.com/starreeze/latr.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u91c7\u6837\u7b56\u7565\u2014\u2014\u524d\u77bb\u6811\u5f62\u5c55\u5f00\uff08LATR\uff09\uff0c\u65e8\u5728\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u4e2d\u7684\u91c7\u6837\u591a\u6837\u6027\uff0c\u4ece\u800c\u52a0\u901f\u7b56\u7565\u5b66\u4e60\u5e76\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u7531\u4e8e\u57fa\u4e8e\u7ec4\u7684\u7b56\u7565\u4f18\u5316\uff08\u5982GRPO\uff09\u5728\u91c7\u6837\u8f68\u8ff9\u65f6\u5b58\u5728\u591a\u6837\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u56de\u62a5\u4fe1\u53f7\u51cf\u5f31\uff0c\u5f71\u54cd\u7b56\u7565\u66f4\u65b0\u6548\u679c\u3002\u4e3b\u8981\u539f\u56e0\u662f\u57fa\u4e8etoken\u5c42\u9762\u7684\u968f\u673a\u91c7\u6837\uff0c\u5bb9\u6613\u4ea7\u751f\u8fd1\u4f3c\u76f8\u540c\u7684\u63a8\u7406\u8def\u5f84\u3002", "method": "\u63d0\u51faLATR\u65b9\u6cd5\uff0c\u5728\u9ad8\u4e0d\u786e\u5b9a\u6027\u751f\u6210\u6b65\u9aa4\u8fdb\u884c\u5206\u652f\uff0c\u9488\u5bf9\u6bcf\u4e2a\u5206\u652f\u6267\u884c\u524d\u77bb\u4eff\u771f\uff0c\u5e76\u526a\u679d\u5728\u4eff\u771f\u8fc7\u7a0b\u4e2d\u5c55\u73b0\u51fa\u957f\u65f6\u95f4\u76f8\u4f3c\u6027\u7684\u5206\u652f\uff0c\u4ece\u800c\u663e\u8457\u589e\u52a0\u8f68\u8ff9\u5c42\u6b21\u7684\u591a\u6837\u6027\u3002", "result": "\u4e0e\u968f\u673a\u91c7\u6837\u76f8\u6bd4\uff0cLATR\u5e73\u5747\u52a0\u901f\u653f\u7b56\u5b66\u4e60131%\uff0c\u5e76\u5728GRPO\u548cDAPO\u7b97\u6cd5\u4e0b\u7684\u591a\u4e2a\u63a8\u7406\u4efb\u52a1\u4e2d\u5c06\u6700\u7ec8pass@1\u6027\u80fd\u63d0\u53474.2%\u3002", "conclusion": "LATR\u6709\u6548\u89e3\u51b3\u4e86RLVR\u4e2d\u91c7\u6837\u591a\u6837\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6548\u7387\u548c\u6027\u80fd\u3002\u65b9\u6cd5\u53ca\u6570\u636e\u5df2\u5f00\u6e90\u3002"}}
{"id": "2510.24320", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24320", "abs": "https://arxiv.org/abs/2510.24320", "authors": ["Zhiheng Xi", "Jixuan Huang", "Xin Guo", "Boyang Hong", "Dingwen Yang", "Xiaoran Fan", "Shuo Li", "Zehui Chen", "Junjie Ye", "Siyu Yuan", "Zhengyin Du", "Xuesong Yao", "Yufei Xu", "Jiecao Chen", "Rui Zheng", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "Critique-RL: Training Language Models for Critiquing through Two-Stage Reinforcement Learning", "comment": "Preprint, 25 pages, 9 figures. Code:\n  https://github.com/WooooDyy/Critique-RL", "summary": "Training critiquing language models to assess and provide feedback on model\noutputs is a promising way to improve LLMs for complex reasoning tasks.\nHowever, existing approaches typically rely on stronger supervisors for\nannotating critique data. To address this, we propose Critique-RL, an online RL\napproach for developing critiquing language models without stronger\nsupervision. Our approach operates on a two-player paradigm: the actor\ngenerates a response, the critic provides feedback, and the actor refines the\nresponse accordingly. We first reveal that relying solely on indirect reward\nsignals from the actor's outputs for RL optimization often leads to\nunsatisfactory critics: while their helpfulness (i.e., providing constructive\nfeedback) improves, the discriminability (i.e., determining whether a response\nis high-quality or not) remains poor, resulting in marginal performance gains.\nTo overcome this, Critique-RL adopts a two-stage optimization strategy. In\nstage I, it reinforces the discriminability of the critic with direct\nrule-based reward signals; in stage II, it introduces indirect rewards based on\nactor refinement to improve the critic's helpfulness, while maintaining its\ndiscriminability via appropriate regularization. Extensive experiments across\nvarious tasks and models show that Critique-RL delivers substantial performance\nimprovements. For example, it achieves a 9.02% gain on in-domain tasks and a\n5.70% gain on out-of-domain tasks for Qwen2.5-7B, highlighting its potential.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Critique-RL\uff0c\u4e00\u79cd\u65e0\u9700\u5f3a\u76d1\u7763\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u89d2\u8272\uff08\u751f\u6210\u8005\u4e0e\u6279\u8bc4\u8005\uff09\u6a21\u578b\u6765\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u6279\u8bc4\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4f9d\u8d56\u5f3a\u76d1\u7763\u8d44\u6e90\uff0c\u800c\u8fd9\u9650\u5236\u4e86\u5176\u5e94\u7528\u548c\u6548\u679c\u63d0\u5347\u3002", "method": "Critique-RL\u91c7\u7528\u4e24\u9636\u6bb5\u4f18\u5316\uff0c\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u89c4\u5219\u5956\u52b1\u63d0\u5347\u6279\u8bc4\u8005\u7684\u5224\u522b\u80fd\u529b\uff0c\u7b2c\u4e8c\u9636\u6bb5\u57fa\u4e8e\u751f\u6210\u8005\u6539\u8fdb\u5f15\u5165\u95f4\u63a5\u5956\u52b1\u63d0\u5347\u6279\u8bc4\u7684\u5efa\u8bbe\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u5224\u522b\u80fd\u529b\u3002", "result": "\u5728\u591a\u4efb\u52a1\u548c\u591a\u6a21\u578b\u6d4b\u8bd5\u4e2d\uff0cCritique-RL\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u6bd4\u5982\u5728Qwen2.5-7B\u6a21\u578b\u4e0a\u57df\u5185\u4efb\u52a1\u63d0\u53479.02%\uff0c\u57df\u5916\u4efb\u52a1\u63d0\u53475.70%\u3002", "conclusion": "Critique-RL\u6709\u6548\u89e3\u51b3\u4e86\u6279\u8bc4\u6a21\u578b\u8bad\u7ec3\u4e2d\u5224\u522b\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u8868\u73b0\uff0c\u5c55\u793a\u4e86\u65e0\u5f3a\u76d1\u7763\u6279\u8bc4\u8bad\u7ec3\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.24328", "categories": ["cs.CL", "cs.AI", "68T50", "F.2.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.24328", "abs": "https://arxiv.org/abs/2510.24328", "authors": ["Hunzalah Hassan Bhatti", "Firoj Alam"], "title": "Beyond MCQ: An Open-Ended Arabic Cultural QA Benchmark with Dialect Variants", "comment": "Cultural Knowledge, Everyday Knowledge, Open-Ended Question,\n  Chain-of-Thought, Large Language Models, Native, Multilingual, Language\n  Diversity", "summary": "Large Language Models (LLMs) are increasingly used to answer everyday\nquestions, yet their performance on culturally grounded and dialectal content\nremains uneven across languages. We propose a comprehensive method that (i)\ntranslates Modern Standard Arabic (MSA) multiple-choice questions (MCQs) into\nEnglish and several Arabic dialects, (ii) converts them into open-ended\nquestions (OEQs), (iii) benchmarks a range of zero-shot and fine-tuned LLMs\nunder both MCQ and OEQ settings, and (iv) generates chain-of-thought (CoT)\nrationales to fine-tune models for step-by-step reasoning. Using this method,\nwe extend an existing dataset in which QAs are parallelly aligned across\nmultiple language varieties, making it, to our knowledge, the first of its\nkind. We conduct extensive experiments with both open and closed models. Our\nfindings show that (i) models underperform on Arabic dialects, revealing\npersistent gaps in culturally grounded and dialect-specific knowledge; (ii)\nArabic-centric models perform well on MCQs but struggle with OEQs; and (iii)\nCoT improves judged correctness while yielding mixed n-gram-based metrics. The\ndeveloped dataset will be publicly released to support further research on\nculturally and linguistically inclusive evaluation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u65b9\u6cd5\uff0c\u5c06\u6807\u51c6\u963f\u62c9\u4f2f\u8bed\u7684\u9009\u62e9\u9898\u7ffb\u8bd1\u6210\u591a\u79cd\u963f\u62c9\u4f2f\u65b9\u8a00\u548c\u82f1\u8bed\uff0c\u8f6c\u6362\u4e3a\u5f00\u653e\u5f0f\u95ee\u9898\uff0c\u5e76\u4f7f\u7528\u591a\u79cd\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7ed3\u5408\u94fe\u5f0f\u601d\u7ef4\u6cd5\u4f18\u5316\u6a21\u578b\u8868\u73b0\u3002", "motivation": "\u76ee\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u6587\u5316\u80cc\u666f\u548c\u65b9\u8a00\u5185\u5bb9\u4e0a\u8868\u73b0\u4e0d\u5747\u8861\uff0c\u5c24\u5176\u662f\u963f\u62c9\u4f2f\u8bed\u7684\u65b9\u8a00\u5185\u5bb9\u7f3a\u4e4f\u6709\u6548\u8bc4\u4f30\u6570\u636e\u548c\u65b9\u6cd5\u3002", "method": "\u5c06\u6807\u51c6\u963f\u62c9\u4f2f\u8bed\u7684\u9009\u62e9\u9898\u7ffb\u8bd1\u6210\u82f1\u8bed\u548c\u591a\u79cd\u963f\u62c9\u4f2f\u65b9\u8a00\uff0c\u8f6c\u6362\u6210\u5f00\u653e\u5f0f\u95ee\u9898\uff0c\u6bd4\u8f83\u96f6\u6837\u672c\u548c\u5fae\u8c03\u6a21\u578b\u5728\u9009\u62e9\u9898\u548c\u5f00\u653e\u9898\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u901a\u8fc7\u751f\u6210\u94fe\u5f0f\u601d\u7ef4\u7406\u7531\u5fae\u8c03\u6a21\u578b\u4ee5\u63d0\u5347\u9010\u6b65\u63a8\u7406\u80fd\u529b\u3002", "result": "\u6a21\u578b\u5728\u963f\u62c9\u4f2f\u65b9\u8a00\u4e0a\u7684\u8868\u73b0\u8f83\u5dee\uff0c\u963f\u62c9\u4f2f\u8bed\u4e2d\u5fc3\u6a21\u578b\u5728\u9009\u62e9\u9898\u4e0a\u8868\u73b0\u826f\u597d\u4f46\u5f00\u653e\u5f0f\u95ee\u9898\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u94fe\u5f0f\u601d\u7ef4\u5fae\u8c03\u63d0\u5347\u4e86\u6b63\u786e\u7387\u4f46\u57fa\u4e8en-gram\u7684\u6307\u6807\u8868\u73b0\u4e0d\u4e00\u3002", "conclusion": "\u6240\u5f00\u53d1\u7684\u591a\u8bed\u8a00\u3001\u591a\u65b9\u8a00\u5e73\u884c\u6570\u636e\u96c6\u4e3a\u6587\u5316\u548c\u8bed\u8a00\u5305\u5bb9\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u76f8\u5173\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2510.24345", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24345", "abs": "https://arxiv.org/abs/2510.24345", "authors": ["Zikai Xiao", "Fei Huang", "Jianhong Tu", "Jianhui Wei", "Wen Ma", "Yuxuan Zhou", "Jian Wu", "Bowen Yu", "Zuozhu Liu", "Junyang Lin"], "title": "LongWeave: A Long-Form Generation Benchmark Bridging Real-World Relevance and Verifiability", "comment": "EMNLP Findings 2025", "summary": "Generating long, informative, and factual outputs remains a major challenge\nfor Large Language Models (LLMs). Existing benchmarks for long-form generation\ntypically assess real-world queries with hard-to-verify metrics or use\nsynthetic setups that ease evaluation but overlook real-world intricacies. In\nthis paper, we introduce \\textbf{LongWeave}, which balances real-world and\nverifiable assessment with Constraint-Verifier Evaluation (CoV-Eval). CoV-Eval\nconstructs tasks by first defining verifiable targets within real-world\nscenarios, then systematically generating corresponding queries, textual\nmaterials, and constraints based on these targets. This ensures that tasks are\nboth realistic and objectively assessable, enabling rigorous assessment of\nmodel capabilities in meeting complex real-world constraints. LongWeave\nsupports customizable input/output lengths (up to 64K/8K tokens) across seven\ndistinct tasks. Evaluation on 23 LLMs shows that even state-of-the-art models\nencounter significant challenges in long-form generation as real-world\ncomplexity and output length increase.", "AI": {"tldr": "LongWeave\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u73b0\u5b9e\u573a\u666f\u548c\u53ef\u9a8c\u8bc1\u8bc4\u4f30\u7684\u65b0\u65b9\u6cd5\uff0c\u65e8\u5728\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u957f\u6587\u672c\u751f\u6210\u65b9\u9762\u7684\u80fd\u529b\uff0c\u7ed3\u679c\u663e\u793a\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684\u6a21\u578b\u4e5f\u5b58\u5728\u663e\u8457\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u7684\u957f\u6587\u672c\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u5728\u8bc4\u4f30\u957f\u6587\u672c\u751f\u6210\u8d28\u91cf\u65f6\u8981\u4e48\u4f9d\u8d56\u96be\u4ee5\u9a8c\u8bc1\u7684\u6307\u6807\uff0c\u8981\u4e48\u4f7f\u7528\u7b80\u5316\u7684\u5408\u6210\u73af\u5883\uff0c\u7f3a\u4e4f\u5bf9\u73b0\u5b9e\u4e16\u754c\u590d\u6742\u6027\u7684\u5145\u5206\u8003\u91cf\u3002", "method": "\u63d0\u51faLongWeave\u6846\u67b6\uff0c\u5229\u7528Constraint-Verifier Evaluation(CoV-Eval)\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u5305\u542b\u53ef\u9a8c\u8bc1\u76ee\u6807\u7684\u771f\u5b9e\u573a\u666f\u4efb\u52a1\uff0c\u7cfb\u7edf\u751f\u6210\u67e5\u8be2\u3001\u6587\u672c\u6750\u6599\u548c\u7ea6\u675f\u6761\u4ef6\uff0c\u652f\u6301\u53ef\u5b9a\u5236\u7684\u8f93\u5165\u8f93\u51fa\u957f\u5ea6\u3002", "result": "\u572823\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0a\u8bc4\u4f30\uff0c\u53d1\u73b0\u968f\u7740\u73b0\u5b9e\u590d\u6742\u6027\u548c\u8f93\u51fa\u957f\u5ea6\u589e\u52a0\uff0c\u6a21\u578b\u5728\u957f\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u663e\u8457\u4e0b\u964d\uff0c\u5b58\u5728\u8f83\u5927\u6311\u6218\u3002", "conclusion": "LongWeave\u4e3a\u957f\u6587\u672c\u751f\u6210\u8bc4\u4f30\u63d0\u4f9b\u4e86\u73b0\u5b9e\u6027\u4e0e\u5ba2\u89c2\u6027\u517c\u5177\u7684\u6807\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5e94\u5bf9\u590d\u6742\u7ea6\u675f\u548c\u957f\u6587\u672c\u8f93\u51fa\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u672a\u6765\u6a21\u578b\u6539\u8fdb\u4ecd\u9700\u91cd\u70b9\u7a81\u7834\u8fd9\u65b9\u9762\u95ee\u9898\u3002"}}
{"id": "2510.24365", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24365", "abs": "https://arxiv.org/abs/2510.24365", "authors": ["Matthew Shardlow"], "title": "Text Simplification with Sentence Embeddings", "comment": null, "summary": "Sentence embeddings can be decoded to give approximations of the original\ntexts used to create them. We explore this effect in the context of text\nsimplification, demonstrating that reconstructed text embeddings preserve\ncomplexity levels. We experiment with a small feed forward neural network to\neffectively learn a transformation between sentence embeddings representing\nhigh-complexity and low-complexity texts. We provide comparison to a Seq2Seq\nand LLM-based approach, showing encouraging results in our much smaller\nlearning setting. Finally, we demonstrate the applicability of our\ntransformation to an unseen simplification dataset (MedEASI), as well as\ndatasets from languages outside the training data (ES,DE). We conclude that\nlearning transformations in sentence embedding space is a promising direction\nfor future research and has potential to unlock the ability to develop small,\nbut powerful models for text simplification and other natural language\ngeneration tasks.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u5b66\u4e60\u53e5\u5b50\u5d4c\u5165\u7a7a\u95f4\u7684\u53d8\u6362\uff0c\u5b9e\u73b0\u6587\u672c\u590d\u6742\u5ea6\u7684\u7b80\u5316\uff0c\u5c55\u793a\u4e86\u5728\u5c0f\u89c4\u6a21\u795e\u7ecf\u7f51\u7edc\u548c\u591a\u8bed\u8a00\u6570\u636e\u4e0a\u7684\u6709\u6548\u6027\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5229\u7528\u53e5\u5b50\u5d4c\u5165\u89e3\u7801\u6765\u4fdd\u7559\u5e76\u8c03\u6574\u6587\u672c\u590d\u6742\u5ea6\uff0c\u63a8\u52a8\u6587\u672c\u7b80\u5316\u7814\u7a76\u3002", "method": "\u4f7f\u7528\u5c0f\u578b\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u9ad8\u590d\u6742\u5ea6\u4e0e\u4f4e\u590d\u6742\u5ea6\u53e5\u5b50\u5d4c\u5165\u4e4b\u95f4\u7684\u53d8\u6362\uff0c\u5e76\u4e0eSeq2Seq\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u793a\u8303\u4e86\u8be5\u65b9\u6cd5\u5728\u672a\u89c1\u7b80\u5316\u6570\u636e\u96c6\uff08MedEASI\uff09\u53ca\u897f\u73ed\u7259\u8bed\u548c\u5fb7\u8bed\u6570\u636e\u96c6\u4e0a\u7684\u6709\u6548\u6027\uff0c\u8868\u73b0\u4f18\u4e8e\u5c0f\u89c4\u6a21\u6a21\u578b\u7684\u9884\u671f\u3002", "conclusion": "\u5728\u53e5\u5b50\u5d4c\u5165\u7a7a\u95f4\u5b66\u4e60\u53d8\u6362\u662f\u6587\u672c\u7b80\u5316\u548c\u81ea\u7136\u8bed\u8a00\u751f\u6210\u9886\u57df\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u53ef\u5f00\u53d1\u5c0f\u578b\u4e14\u9ad8\u6548\u7684\u6a21\u578b\u3002"}}
{"id": "2510.24425", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24425", "abs": "https://arxiv.org/abs/2510.24425", "authors": ["Guangyu Xie", "Yice Zhang", "Jianzhu Bao", "Qianlong Wang", "Yang Sun", "Bingbing Wang", "Ruifeng Xu"], "title": "Comprehensive and Efficient Distillation for Lightweight Sentiment Analysis Models", "comment": "Accepted by EMNLP 2025. 22 pages, 9 figures. The first two authors\n  contribute equally", "summary": "Recent efforts leverage knowledge distillation techniques to develop\nlightweight and practical sentiment analysis models. These methods are grounded\nin human-written instructions and large-scale user texts. Despite the promising\nresults, two key challenges remain: (1) manually written instructions are\nlimited in diversity and quantity, making them insufficient to ensure\ncomprehensive coverage of distilled knowledge; (2) large-scale user texts incur\nhigh computational cost, hindering the practicality of these methods. To this\nend, we introduce COMPEFFDIST, a comprehensive and efficient distillation\nframework for sentiment analysis. Our framework consists of two key modules:\nattribute-based automatic instruction construction and difficulty-based data\nfiltering, which correspondingly tackle the aforementioned challenges. Applying\nour method across multiple model series (Llama-3, Qwen-3, and Gemma-3), we\nenable 3B student models to match the performance of 20x larger teacher models\non most tasks. In addition, our approach greatly outperforms baseline methods\nin data efficiency, attaining the same performance level with only 10% of the\ndata.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aCOMPEFFDIST\u7684\u9ad8\u6548\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u7528\u4e8e\u8f7b\u91cf\u5316\u60c5\u611f\u5206\u6790\u6a21\u578b\u7684\u8bad\u7ec3\uff0c\u5b9e\u73b03B\u53c2\u6570\u6a21\u578b\u8fbe\u523020\u500d\u5927\u6a21\u578b\u6027\u80fd\uff0c\u4e14\u6570\u636e\u6548\u7387\u63d0\u5347\u81f3\u4ec5\u970010%\u7684\u6570\u636e\u91cf\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u77e5\u8bc6\u84b8\u998f\u7684\u60c5\u611f\u5206\u6790\u65b9\u6cd5\u4f9d\u8d56\u6570\u91cf\u548c\u591a\u6837\u6027\u6709\u9650\u7684\u624b\u5199\u6307\u4ee4\u4e0e\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u5927\u89c4\u6a21\u7528\u6237\u6587\u672c\uff0c\u5bfc\u81f4\u8986\u76d6\u4e0d\u8db3\u548c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u5c5e\u6027\u7684\u81ea\u52a8\u6307\u4ee4\u6784\u9020\u548c\u57fa\u4e8e\u96be\u5ea6\u7684\u6570\u636e\u8fc7\u6ee4\u4e24\u5927\u6a21\u5757\uff0c\u5206\u522b\u89e3\u51b3\u6307\u4ee4\u591a\u6837\u6027\u4e0d\u8db3\u548c\u5927\u89c4\u6a21\u6570\u636e\u8ba1\u7b97\u6210\u672c\u95ee\u9898\u3002", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u7cfb\u5217\uff08Llama-3\u3001Qwen-3\u3001Gemma-3\uff09\u4e0a\uff0c3B\u5b66\u751f\u6a21\u578b\u6027\u80fd\u5ab2\u7f8e20\u500d\u5927\u6a21\u578b\uff0c\u5e76\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6570\u636e\u4f7f\u7528\u91cf\u4ec5\u4e3a10%\u3002", "conclusion": "COMPEFFDIST\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u60c5\u611f\u5206\u6790\u77e5\u8bc6\u84b8\u998f\u7684\u6548\u7387\u548c\u6027\u80fd\uff0c\u63a8\u52a8\u8f7b\u91cf\u7ea7\u6a21\u578b\u7684\u5b9e\u7528\u5316\u3002"}}
{"id": "2510.24427", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24427", "abs": "https://arxiv.org/abs/2510.24427", "authors": ["Ken Gu", "Advait Bhat", "Mike A Merrill", "Robert West", "Xin Liu", "Daniel McDuff", "Tim Althoff"], "title": "SynthWorlds: Controlled Parallel Worlds for Disentangling Reasoning and Knowledge in Language Models", "comment": null, "summary": "Evaluating the reasoning ability of language models (LMs) is complicated by\ntheir extensive parametric world knowledge, where benchmark performance often\nreflects factual recall rather than genuine reasoning. Existing datasets and\napproaches (e.g., temporal filtering, paraphrasing, adversarial substitution)\ncannot cleanly separate the two. We present SynthWorlds, a framework that\ndisentangles task reasoning complexity from factual knowledge. In SynthWorlds,\nwe construct parallel corpora representing two worlds with identical\ninterconnected structure: a real-mapped world, where models may exploit\nparametric knowledge, and a synthetic-mapped world, where such knowledge is\nmeaningless. On top of these corpora, we design two mirrored tasks as case\nstudies: multi-hop question answering and page navigation, which maintain equal\nreasoning difficulty across worlds. Experiments in parametric-only (e.g.,\nclosed-book QA) and knowledge-augmented (e.g., retrieval-augmented) LM settings\nreveal a persistent knowledge advantage gap, defined as the performance boost\nmodels gain from memorized parametric world knowledge. Knowledge acquisition\nand integration mechanisms reduce but do not eliminate this gap, highlighting\nopportunities for system improvements. Fully automatic and scalable,\nSynthWorlds provides a controlled environment for evaluating LMs in ways that\nwere previously challenging, enabling precise and testable comparisons of\nreasoning and memorization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SynthWorlds\u6846\u67b6\uff0c\u7528\u4ee5\u533a\u5206\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u548c\u4e8b\u5b9e\u77e5\u8bc6\u8bb0\u5fc6\uff0c\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u7b49\u96be\u5ea6\u7684\u5e73\u884c\u4efb\u52a1\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8bb0\u5fc6\u77e5\u8bc6\u5e26\u6765\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u6307\u51fa\u4e86\u5f53\u524d\u6a21\u578b\u5728\u77e5\u8bc6\u6574\u5408\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524d\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u65f6\uff0c\u96be\u4ee5\u5265\u79bb\u6a21\u578b\u8bb0\u5fc6\u7684\u4e8b\u5b9e\u77e5\u8bc6\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u533a\u5206\u63a8\u7406\u548c\u8bb0\u5fc6\u8868\u73b0\u3002", "method": "\u6784\u5efa\u4e24\u4e2a\u5177\u6709\u76f8\u540c\u7ed3\u6784\u7684\u5e73\u884c\u8bed\u6599\u5e93\uff08\u771f\u5b9e\u6620\u5c04\u4e16\u754c\u548c\u5408\u6210\u6620\u5c04\u4e16\u754c\uff09\uff0c\u8bbe\u8ba1\u591a\u8df3\u95ee\u7b54\u548c\u9875\u9762\u5bfc\u822a\u4e24\u4e2a\u4efb\u52a1\uff0c\u4fdd\u6301\u63a8\u7406\u96be\u5ea6\u4e00\u81f4\uff0c\u901a\u8fc7\u5bf9\u6bd4\u4e24\u8005\u8868\u73b0\u5206\u79bb\u8bb0\u5fc6\u4e0e\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5b58\u5728\u663e\u8457\u7684\u77e5\u8bc6\u4f18\u52bf\u5dee\u8ddd\uff0c\u6a21\u578b\u4f9d\u8d56\u8bb0\u5fc6\u77e5\u8bc6\u83b7\u5f97\u6027\u80fd\u63d0\u5347\uff0c\u800c\u77e5\u8bc6\u83b7\u53d6\u548c\u6574\u5408\u673a\u5236\u867d\u6709\u6240\u6539\u5584\uff0c\u4f46\u65e0\u6cd5\u5b8c\u5168\u6d88\u9664\u8be5\u5dee\u8ddd\u3002", "conclusion": "SynthWorlds\u6846\u67b6\u4e3a\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u4e0e\u8bb0\u5fc6\u80fd\u529b\u63d0\u4f9b\u4e86\u53ef\u63a7\u73af\u5883\uff0c\u4fc3\u8fdb\u5bf9\u4e24\u8005\u8fdb\u884c\u7cbe\u786e\u6d4b\u8bd5\u548c\u6bd4\u8f83\uff0c\u6709\u52a9\u4e8e\u7cfb\u7edf\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2510.24434", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24434", "abs": "https://arxiv.org/abs/2510.24434", "authors": ["Julian Valline", "Cedric Lothritz", "Jordi Cabot"], "title": "LuxIT: A Luxembourgish Instruction Tuning Dataset from Monolingual Seed Data", "comment": null, "summary": "The effectiveness of instruction-tuned Large Language Models (LLMs) is often\nlimited in low-resource linguistic settings due to a lack of high-quality\ntraining data. We introduce LuxIT, a novel, monolingual instruction tuning\ndataset for Luxembourgish developed to mitigate this challenge. We synthesize\nthe dataset from a corpus of native Luxembourgish texts, utilizing\nDeepSeek-R1-0528, chosen for its shown proficiency in Luxembourgish. Following\ngeneration, we apply a quality assurance process, employing an LLM-as-a-judge\napproach. To investigate the practical utility of the dataset, we fine-tune\nseveral smaller-scale LLMs on LuxIT. Subsequent benchmarking against their base\nmodels on Luxembourgish language proficiency examinations, however, yields\nmixed results, with performance varying significantly across different models.\nLuxIT represents a critical contribution to Luxembourgish natural language\nprocessing and offers a replicable monolingual methodology, though our findings\nhighlight the need for further research to optimize its application.", "AI": {"tldr": "LuxIT\u662f\u9488\u5bf9\u5362\u68ee\u5821\u8bed\u7684\u5355\u8bed\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u96c6\uff0c\u65e8\u5728\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e2d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6548\u679c\u3002", "motivation": "\u7531\u4e8e\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u532e\u4e4f\uff0c\u4f4e\u8d44\u6e90\u8bed\u8a00\u5982\u5362\u68ee\u5821\u8bed\u7684\u6307\u4ee4\u8c03\u4f18\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6548\u679c\u53d7\u9650\uff0c\u9700\u5f00\u53d1\u4e13\u95e8\u6570\u636e\u96c6\u3002", "method": "\u5229\u7528DeepSeek-R1-0528\u6a21\u578b\u5408\u6210\u57fa\u4e8e\u5362\u68ee\u5821\u8bed\u539f\u751f\u6587\u672c\u7684LuxIT\u6570\u636e\u96c6\uff0c\u5e76\u91c7\u7528LLM\u8bc4\u5224\u673a\u5236\u8fdb\u884c\u8d28\u91cf\u4fdd\u8bc1\u3002", "result": "\u9488\u5bf9LuxIT\u5fae\u8c03\u7684\u5c0f\u578b\u6a21\u578b\u5728\u5362\u68ee\u5821\u8bed\u8bed\u8a00\u80fd\u529b\u6d4b\u8bc4\u4e2d\u7684\u8868\u73b0\u53c2\u5dee\u4e0d\u9f50\uff0c\u6548\u679c\u4e0d\u4e00\u3002", "conclusion": "LuxIT\u4e3a\u5362\u68ee\u5821\u8bed\u81ea\u7136\u8bed\u8a00\u5904\u7406\u63d0\u4f9b\u91cd\u8981\u8d44\u6e90\u548c\u53ef\u590d\u5236\u65b9\u6cd5\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u63d0\u5347\u5176\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002"}}
{"id": "2510.24446", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24446", "abs": "https://arxiv.org/abs/2510.24446", "authors": ["Viktoriia Zinkovich", "Anton Antonov", "Andrei Spiridonov", "Denis Shepelev", "Andrey Moskalenko", "Daria Pugacheva", "Elena Tutubalina", "Andrey Kuznetsov", "Vlad Shakhuro"], "title": "SPARTA: Evaluating Reasoning Segmentation Robustness through Black-Box Adversarial Paraphrasing in Text Autoencoder Latent Space", "comment": null, "summary": "Multimodal large language models (MLLMs) have shown impressive capabilities\nin vision-language tasks such as reasoning segmentation, where models generate\nsegmentation masks based on textual queries. While prior work has primarily\nfocused on perturbing image inputs, semantically equivalent textual\nparaphrases-crucial in real-world applications where users express the same\nintent in varied ways-remain underexplored. To address this gap, we introduce a\nnovel adversarial paraphrasing task: generating grammatically correct\nparaphrases that preserve the original query meaning while degrading\nsegmentation performance. To evaluate the quality of adversarial paraphrases,\nwe develop a comprehensive automatic evaluation protocol validated with human\nstudies. Furthermore, we introduce SPARTA-a black-box, sentence-level\noptimization method that operates in the low-dimensional semantic latent space\nof a text autoencoder, guided by reinforcement learning. SPARTA achieves\nsignificantly higher success rates, outperforming prior methods by up to 2x on\nboth the ReasonSeg and LLMSeg-40k datasets. We use SPARTA and competitive\nbaselines to assess the robustness of advanced reasoning segmentation models.\nWe reveal that they remain vulnerable to adversarial paraphrasing-even under\nstrict semantic and grammatical constraints. All code and data will be released\npublicly upon acceptance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5bf9\u6297\u6027\u6539\u5199\u4efb\u52a1\uff0c\u751f\u6210\u8bed\u6cd5\u6b63\u786e\u4e14\u4fdd\u7559\u539f\u610f\u7684\u6587\u672c\u6539\u5199\u4ee5\u7834\u574f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5206\u5272\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86SPARTA\u65b9\u6cd5\u8fdb\u884c\u4f18\u5316\uff0c\u9a8c\u8bc1\u4e86\u5f53\u524d\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e0b\u7684\u8106\u5f31\u6027\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u56fe\u50cf\u8f93\u5165\u7684\u6270\u52a8\uff0c\u800c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u7528\u6237\u7528\u4e0d\u540c\u65b9\u5f0f\u8868\u8fbe\u540c\u4e00\u610f\u56fe\uff0c\u8bed\u4e49\u7b49\u4ef7\u7684\u6587\u672c\u6539\u5199\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6587\u672c\u81ea\u7f16\u7801\u5668\u8bed\u4e49\u6f5c\u53d8\u91cf\u7a7a\u95f4\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u6307\u5bfc\u7684\u9ed1\u76d2\u53e5\u5b50\u7ea7\u4f18\u5316\u65b9\u6cd5SPARTA\uff0c\u7528\u4e8e\u751f\u6210\u5bf9\u6297\u6027\u6587\u672c\u6539\u5199\uff1b\u5e76\u8bbe\u8ba1\u4e86\u7efc\u5408\u81ea\u52a8\u8bc4\u4f30\u534f\u8bae\u7ed3\u5408\u4eba\u5de5\u9a8c\u8bc1\u3002", "result": "SPARTA\u5728ReasonSeg\u548cLLMSeg-40k\u6570\u636e\u96c6\u4e0a\u6210\u529f\u7387\u8f83\u4ee5\u5f80\u65b9\u6cd5\u63d0\u5347\u7ea62\u500d\uff0c\u663e\u793a\u51fa\u66f4\u5f3a\u7684\u653b\u51fb\u80fd\u529b\u3002\u57fa\u4e8eSPARTA\u548c\u7ade\u4e89\u6027\u57fa\u7ebf\u8bc4\u4f30\uff0c\u73b0\u6709\u63a8\u7406\u5206\u5272\u6a21\u578b\u5bf9\u5bf9\u6297\u6027\u6587\u672c\u6539\u5199\u4ecd\u6709\u660e\u663e\u8106\u5f31\u6027\u3002", "conclusion": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u5206\u5272\u4efb\u52a1\u4e2d\u5b58\u5728\u5bf9\u8bed\u4e49\u7b49\u4ef7\u6587\u672c\u6539\u5199\u7684\u8106\u5f31\u6027\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\uff0c\u672a\u6765\u5de5\u4f5c\u9700\u52a0\u5f3a\u6a21\u578b\u5bf9\u6587\u672c\u591a\u6837\u8868\u8fbe\u7684\u9002\u5e94\u80fd\u529b\u3002"}}
{"id": "2510.24450", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24450", "abs": "https://arxiv.org/abs/2510.24450", "authors": ["\u0160pela Vintar", "Taja Kuzman Punger\u0161ek", "Mojca Brglez", "Nikola Ljube\u0161i\u0107"], "title": "Charting the European LLM Benchmarking Landscape: A New Taxonomy and a Set of Best Practices", "comment": "12 pages, 1 figure. Submitted to the LREC 2026 conference", "summary": "While new benchmarks for large language models (LLMs) are being developed\ncontinuously to catch up with the growing capabilities of new models and AI in\ngeneral, using and evaluating LLMs in non-English languages remains a\nlittle-charted landscape. We give a concise overview of recent developments in\nLLM benchmarking, and then propose a new taxonomy for the categorization of\nbenchmarks that is tailored to multilingual or non-English use scenarios. We\nfurther propose a set of best practices and quality standards that could lead\nto a more coordinated development of benchmarks for European languages. Among\nother recommendations, we advocate for a higher language and culture\nsensitivity of evaluation methods.", "AI": {"tldr": "\u672c\u6587\u6982\u8ff0\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u63d0\u51fa\u4e86\u9488\u5bf9\u591a\u8bed\u8a00\u6216\u975e\u82f1\u8bed\u573a\u666f\u7684\u57fa\u51c6\u5206\u7c7b\u65b9\u6cd5\uff0c\u5e76\u5efa\u8bae\u6b27\u6d32\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u7684\u53d1\u5c55\u5e94\u6ce8\u91cd\u8bed\u8a00\u548c\u6587\u5316\u7684\u654f\u611f\u6027\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u7684\u63d0\u5347\uff0c\u5982\u4f55\u6709\u6548\u5730\u5728\u975e\u82f1\u8bed\u8bed\u8a00\u73af\u5883\u4e2d\u4f7f\u7528\u548c\u8bc4\u4f30\u8fd9\u4e9b\u6a21\u578b\u4f9d\u7136\u7f3a\u4e4f\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u672c\u6587\u56de\u987e\u4e86\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5\u7684\u53d1\u5c55\uff0c\u63d0\u51fa\u4e86\u9002\u7528\u4e8e\u591a\u8bed\u8a00\u73af\u5883\u7684\u57fa\u51c6\u5206\u7c7b\u6cd5\uff0c\u5e76\u5236\u5b9a\u4e86\u4e00\u7cfb\u5217\u9ad8\u8d28\u91cf\u6807\u51c6\u4e0e\u6700\u4f73\u5b9e\u8df5\u3002", "result": "\u57fa\u4e8e\u65b0\u7684\u5206\u7c7b\u6cd5\u548c\u6807\u51c6\uff0c\u63a8\u52a8\u4e86\u66f4\u534f\u8c03\u7684\u6b27\u6d32\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u53d1\u5c55\uff0c\u5e76\u5f3a\u8c03\u8bc4\u6d4b\u65b9\u6cd5\u9700\u8981\u66f4\u597d\u5730\u9002\u5e94\u4e0d\u540c\u8bed\u8a00\u548c\u6587\u5316\u7279\u70b9\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u591a\u8bed\u8a00\u4e13\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\u548c\u63d0\u5347\u8bc4\u6d4b\u7684\u8bed\u8a00\u6587\u5316\u654f\u611f\u5ea6\uff0c\u80fd\u591f\u4fc3\u8fdb\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u975e\u82f1\u8bed\u73af\u5883\u4e2d\u7684\u6709\u6548\u8bc4\u4f30\u548c\u516c\u5e73\u53d1\u5c55\u3002"}}
{"id": "2510.24469", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.24469", "abs": "https://arxiv.org/abs/2510.24469", "authors": ["Durga Prasad Maram", "Dhruvin Gandhi", "Zonghai Yao", "Gayathri Akkinapalli", "Franck Dernoncourt", "Yu Wang", "Ryan A. Rossi", "Nesreen K. Ahmed"], "title": "Iterative Critique-Refine Framework for Enhancing LLM Personalization", "comment": null, "summary": "Personalized text generation requires models not only to produce coherent\ntext but also to align with a target user's style, tone, and topical focus.\nExisting retrieval-augmented approaches such as LaMP and PGraphRAG enrich\nprofiles with user and neighbor histories, but they stop at generation and\noften yield outputs that drift in tone, topic, or style. We present PerFine, a\nunified, training-free critique-refine framework that enhances personalization\nthrough iterative, profile-grounded feedback. In each iteration, an LLM\ngenerator produces a draft conditioned on the retrieved profile, and a critic\nLLM - also conditioned on the same profile - provides structured feedback on\ntone, vocabulary, sentence structure, and topicality. The generator then\nrevises, while a novel knockout strategy retains the stronger draft across\niterations. We further study additional inference-time strategies such as\nBest-of-N and Topic Extraction to balance quality and efficiency. Across Yelp,\nGoodreads, and Amazon datasets, PerFine consistently improves personalization\nover PGraphRAG, with GEval gains of +7-13%, steady improvements over 3-5\nrefinement iterations, and scalability with increasing critic size. These\nresults highlight that post-hoc, profile-aware feedback offers a powerful\nparadigm for personalized LLM generation that is both training-free and\nmodel-agnostic.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86PerFine\uff0c\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u57fa\u4e8e\u8fed\u4ee3\u53cd\u9988\u7684\u4e2a\u6027\u5316\u6587\u672c\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u6279\u5224\u8005\u6a21\u578b\u5bf9\u751f\u6210\u6587\u672c\u5728\u8bed\u6c14\u3001\u8bcd\u6c47\u3001\u53e5\u5f0f\u548c\u4e3b\u9898\u7b49\u65b9\u9762\u8fdb\u884c\u53cd\u9988\uff0c\u663e\u8457\u63d0\u5347\u4e2a\u6027\u5316\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u5982LaMP\u548cPGraphRAG\u867d\u7136\u5229\u7528\u7528\u6237\u53ca\u90bb\u5c45\u7684\u5386\u53f2\u6570\u636e\u4e30\u5bcc\u7528\u6237\u753b\u50cf\uff0c\u4f46\u751f\u6210\u6587\u672c\u5bb9\u6613\u5728\u8bed\u6c14\u3001\u4e3b\u9898\u548c\u98ce\u683c\u4e0a\u504f\u79bb\u76ee\u6807\u7528\u6237\uff0c\u96be\u4ee5\u5b9e\u73b0\u7cbe\u786e\u4e2a\u6027\u5316\u3002", "method": "PerFine\u91c7\u7528\u8fed\u4ee3\u7684\u751f\u6210-\u6279\u5224-\u4fee\u6539\u6d41\u7a0b\uff0c\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u8349\u7a3f\uff0c\u53e6\u4e00\u4e2a\u8bed\u8a00\u6a21\u578b\u57fa\u4e8e\u540c\u4e00\u7528\u6237\u753b\u50cf\u5bf9\u8349\u7a3f\u7684\u591a\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u7ed3\u6784\u5316\u53cd\u9988\uff0c\u751f\u6210\u5668\u636e\u6b64\u4fee\u8ba2\u6587\u672c\uff0c\u5e76\u901a\u8fc7\u4e00\u79cd\u65b0\u9896\u7684knockout\u7b56\u7565\u5728\u8fed\u4ee3\u4e2d\u4fdd\u7559\u6700\u4f18\u6587\u672c\u3002", "result": "\u5728Yelp\u3001Goodreads\u548cAmazon\u6570\u636e\u96c6\u4e0a\uff0cPerFine\u76f8\u6bd4PGraphRAG\u5728\u4e2a\u6027\u5316\u6307\u6807\u4e0a\u63d0\u5347\u4e867-13%\uff0c\u4e14\u57283-5\u8f6e\u8fed\u4ee3\u4e2d\u6301\u7eed\u6539\u8fdb\uff0c\u968f\u7740\u6279\u5224\u8005\u6a21\u578b\u89c4\u6a21\u589e\u5927\u6027\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u3002", "conclusion": "PerFine\u5c55\u793a\u4e86\u540e\u671f\u57fa\u4e8e\u7528\u6237\u753b\u50cf\u7684\u53cd\u9988\u673a\u5236\u5728\u4e2a\u6027\u5316\u6587\u672c\u751f\u6210\u4e2d\u7684\u5f3a\u5927\u4f5c\u7528\uff0c\u8be5\u65b9\u6cd5\u65e0\u9700\u8bad\u7ec3\u4e14\u5bf9\u6a21\u578b\u65e0\u7279\u5b9a\u4f9d\u8d56\uff0c\u5177\u5907\u826f\u597d\u7684\u6269\u5c55\u6027\u548c\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.24476", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24476", "abs": "https://arxiv.org/abs/2510.24476", "authors": ["Yihan Li", "Xiyuan Fu", "Ghanshyam Verma", "Paul Buitelaar", "Mingming Liu"], "title": "Mitigating Hallucination in Large Language Models (LLMs): An Application-Oriented Survey on RAG, Reasoning, and Agentic Systems", "comment": "25 pages, 7 figures, 3 tables", "summary": "Hallucination remains one of the key obstacles to the reliable deployment of\nlarge language models (LLMs), particularly in real-world applications. Among\nvarious mitigation strategies, Retrieval-Augmented Generation (RAG) and\nreasoning enhancement have emerged as two of the most effective and widely\nadopted approaches, marking a shift from merely suppressing hallucinations to\nbalancing creativity and reliability. However, their synergistic potential and\nunderlying mechanisms for hallucination mitigation have not yet been\nsystematically examined. This survey adopts an application-oriented perspective\nof capability enhancement to analyze how RAG, reasoning enhancement, and their\nintegration in Agentic Systems mitigate hallucinations. We propose a taxonomy\ndistinguishing knowledge-based and logic-based hallucinations, systematically\nexamine how RAG and reasoning address each, and present a unified framework\nsupported by real-world applications, evaluations, and benchmarks.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5229\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u63a8\u7406\u589e\u5f3a\u65b9\u6cd5\u7f13\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u5e7b\u89c9\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u77e5\u8bc6\u578b\u548c\u903b\u8f91\u578b\u5e7b\u89c9\u7684\u5206\u7c7b\uff0c\u5e76\u5206\u6790\u4e86\u4e24\u79cd\u65b9\u6cd5\u53ca\u5176\u7ed3\u5408\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u90e8\u7f72\u9762\u4e34\u5e7b\u89c9\u95ee\u9898\uff0c\u9700\u4ece\u63d0\u5347\u521b\u9020\u6027\u548c\u53ef\u9760\u6027\u89d2\u5ea6\u5bfb\u627e\u6709\u6548\u7f13\u89e3\u7b56\u7565\u3002", "method": "\u4ece\u5e94\u7528\u89c6\u89d2\u51fa\u53d1\uff0c\u6784\u5efa\u77e5\u8bc6\u578b\u548c\u903b\u8f91\u578b\u5e7b\u89c9\u5206\u7c7b\u4f53\u7cfb\uff0c\u7cfb\u7edf\u5206\u6790RAG\u548c\u63a8\u7406\u589e\u5f3a\u6280\u672f\u7684\u51cf\u5e7b\u89c9\u673a\u5236\u53ca\u5176\u5728Agentic\u7cfb\u7edf\u4e2d\u7684\u6574\u5408\u3002", "result": "\u603b\u7ed3\u4e86RAG\u548c\u63a8\u7406\u589e\u5f3a\u5728\u4e0d\u540c\u5e7b\u89c9\u7c7b\u578b\u4e0a\u7684\u4f5c\u7528\uff0c\u63d0\u51fa\u4e86\u7edf\u4e00\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u5b9e\u9645\u5e94\u7528\u548c\u8bc4\u6d4b\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "conclusion": "RAG\u4e0e\u63a8\u7406\u589e\u5f3a\u7684\u534f\u540c\u5e94\u7528\u662f\u7f13\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u5e7b\u89c9\u7684\u6709\u6548\u7b56\u7565\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u6a21\u578b\u7684\u53ef\u9760\u6027\u4e0e\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.24478", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24478", "abs": "https://arxiv.org/abs/2510.24478", "authors": ["Frederik Broy", "Maike Z\u00fcfle", "Jan Niehues"], "title": "Talk2Ref: A Dataset for Reference Prediction from Scientific Talks", "comment": null, "summary": "Scientific talks are a growing medium for disseminating research, and\nautomatically identifying relevant literature that grounds or enriches a talk\nwould be highly valuable for researchers and students alike. We introduce\nReference Prediction from Talks (RPT), a new task that maps long, and\nunstructured scientific presentations to relevant papers. To support research\non RPT, we present Talk2Ref, the first large-scale dataset of its kind,\ncontaining 6,279 talks and 43,429 cited papers (26 per talk on average), where\nrelevance is approximated by the papers cited in the talk's corresponding\nsource publication. We establish strong baselines by evaluating\nstate-of-the-art text embedding models in zero-shot retrieval scenarios, and\npropose a dual-encoder architecture trained on Talk2Ref. We further explore\nstrategies for handling long transcripts, as well as training for domain\nadaptation. Our results show that fine-tuning on Talk2Ref significantly\nimproves citation prediction performance, demonstrating both the challenges of\nthe task and the effectiveness of our dataset for learning semantic\nrepresentations from spoken scientific content. The dataset and trained models\nare released under an open license to foster future research on integrating\nspoken scientific communication into citation recommendation systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u4efb\u52a1\u2014\u2014\u4ece\u79d1\u5b66\u6f14\u8bb2\u4e2d\u81ea\u52a8\u8bc6\u522b\u76f8\u5173\u6587\u732e\uff0c\u6784\u5efa\u4e86\u7b2c\u4e00\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6Talk2Ref\uff0c\u5305\u542b6279\u4e2a\u6f14\u8bb2\u548c43429\u7bc7\u5f15\u7528\u8bba\u6587\uff0c\u5efa\u7acb\u4e86\u5f3a\u57fa\u7ebf\u5e76\u8bbe\u8ba1\u4e86\u53cc\u7f16\u7801\u5668\u6a21\u578b\u8fdb\u884c\u4f18\u5316\u3002", "motivation": "\u79d1\u5b66\u6f14\u8bb2\u4f5c\u4e3a\u4f20\u64ad\u7814\u7a76\u6210\u679c\u7684\u91cd\u8981\u5a92\u4ecb\uff0c\u81ea\u52a8\u8bc6\u522b\u652f\u6491\u6216\u4e30\u5bcc\u6f14\u8bb2\u5185\u5bb9\u7684\u76f8\u5173\u6587\u732e\uff0c\u5bf9\u4e8e\u7814\u7a76\u8005\u548c\u5b66\u751f\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b\u5927\u91cf\u6f14\u8bb2\u53ca\u5176\u5f15\u7528\u8bba\u6587\u7684Talk2Ref\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u591a\u79cd\u6587\u672c\u5d4c\u5165\u6a21\u578b\uff0c\u8bbe\u8ba1\u5e76\u8bad\u7ec3\u4e86\u57fa\u4e8e\u53cc\u7f16\u7801\u5668\u7684\u6a21\u578b\uff0c\u63a2\u7d22\u4e86\u5904\u7406\u957f\u7bc7\u6f14\u8bb2\u6587\u672c\u548c\u9886\u57df\u9002\u5e94\u7684\u7b56\u7565\u3002", "result": "\u5728Talk2Ref\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\u663e\u8457\u63d0\u5347\u4e86\u5f15\u7528\u9884\u6d4b\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u4efb\u52a1\u7684\u6311\u6218\u6027\u548c\u6570\u636e\u96c6\u5bf9\u4ece\u53e3\u8bed\u79d1\u5b66\u5185\u5bb9\u4e2d\u5b66\u4e60\u8bed\u4e49\u8868\u793a\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u4efb\u52a1\u548c\u6570\u636e\u96c6\u4e3a\u7814\u7a76\u5c06\u53e3\u8bed\u79d1\u5b66\u4f20\u64ad\u4e0e\u6587\u732e\u63a8\u8350\u7cfb\u7edf\u7ed3\u5408\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5f00\u653e\u53d1\u5e03\u8d44\u6e90\u4fc3\u8fdb\u672a\u6765\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2510.24488", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24488", "abs": "https://arxiv.org/abs/2510.24488", "authors": ["Katherine Abramski", "Giulio Rossetti", "Massimo Stella"], "title": "A word association network methodology for evaluating implicit biases in LLMs compared to humans", "comment": "24 pages, 13 figures, 3 tables", "summary": "As Large language models (LLMs) become increasingly integrated into our\nlives, their inherent social biases remain a pressing concern. Detecting and\nevaluating these biases can be challenging because they are often implicit\nrather than explicit in nature, so developing evaluation methods that assess\nthe implicit knowledge representations of LLMs is essential. We present a novel\nword association network methodology for evaluating implicit biases in LLMs\nbased on simulating semantic priming within LLM-generated word association\nnetworks. Our prompt-based approach taps into the implicit relational\nstructures encoded in LLMs, providing both quantitative and qualitative\nassessments of bias. Unlike most prompt-based evaluation methods, our method\nenables direct comparisons between various LLMs and humans, providing a\nvaluable point of reference and offering new insights into the alignment of\nLLMs with human cognition. To demonstrate the utility of our methodology, we\napply it to both humans and several widely used LLMs to investigate social\nbiases related to gender, religion, ethnicity, sexual orientation, and\npolitical party. Our results reveal both convergences and divergences between\nLLM and human biases, providing new perspectives on the potential risks of\nusing LLMs. Our methodology contributes to a systematic, scalable, and\ngeneralizable framework for evaluating and comparing biases across multiple\nLLMs and humans, advancing the goal of transparent and socially responsible\nlanguage technologies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bcd\u8bed\u8054\u60f3\u7f51\u7edc\u6a21\u62df\u8bed\u4e49\u542f\u52a8\u7684\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u9690\u6027\u504f\u89c1\u7684\u65b0\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9a\u91cf\u548c\u5b9a\u6027\u5730\u6bd4\u8f83\u6a21\u578b\u4e0e\u4eba\u7c7b\u7684\u793e\u4f1a\u504f\u89c1\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u9690\u6027\u793e\u4f1a\u504f\u89c1\u6210\u4e3a\u4e9f\u9700\u89e3\u51b3\u7684\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u8bc4\u4f30\u6a21\u578b\u9690\u542b\u7684\u504f\u89c1\u77e5\u8bc6\uff0c\u9700\u53d1\u5c55\u80fd\u53cd\u6620\u6a21\u578b\u9690\u6027\u8ba4\u77e5\u7ed3\u6784\u7684\u8bc4\u4f30\u624b\u6bb5\u3002", "method": "\u672c\u6587\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u63d0\u793a\u8bcd\u89e6\u53d1\u7684\u8bcd\u8bed\u8054\u60f3\u7f51\u7edc\u6784\u5efa\u548c\u8bed\u4e49\u542f\u52a8\u6a21\u62df\u65b9\u6cd5\uff0c\u4ece\u6a21\u578b\u8f93\u51fa\u7684\u8054\u60f3\u8bcd\u4e2d\u63d0\u53d6\u9690\u6027\u504f\u89c1\u4fe1\u606f\uff0c\u540c\u65f6\u652f\u6301\u4eba\u4e0e\u6a21\u578b\u95f4\u7684\u76f4\u63a5\u6bd4\u8f83\u3002", "result": "\u901a\u8fc7\u5bf9\u591a\u578b\u53f7\u5927\u8bed\u8a00\u6a21\u578b\u548c\u4eba\u7c7b\u6837\u672c\u8fdb\u884c\u6027\u522b\u3001\u5b97\u6559\u3001\u65cf\u88d4\u3001\u6027\u53d6\u5411\u53ca\u653f\u6cbb\u7b49\u504f\u89c1\u6d4b\u8bd5\uff0c\u53d1\u73b0\u6a21\u578b\u4e0e\u4eba\u7c7b\u504f\u89c1\u65e2\u6709\u76f8\u4f3c\u4e5f\u6709\u5dee\u5f02\uff0c\u5c55\u73b0\u4e86\u6a21\u578b\u6f5c\u5728\u98ce\u9669\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6784\u5efa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u6027\u3001\u53ef\u6269\u5c55\u3001\u53ef\u63a8\u5e7f\u7684\u6846\u67b6\uff0c\u52a9\u529b\u591a\u6a21\u578b\u548c\u4eba\u7c7b\u504f\u89c1\u7684\u8bc4\u4f30\u4e0e\u6bd4\u8f83\uff0c\u4fc3\u8fdb\u8bed\u8a00\u6280\u672f\u7684\u900f\u660e\u5316\u548c\u793e\u4f1a\u8d23\u4efb\u3002"}}
{"id": "2510.24505", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24505", "abs": "https://arxiv.org/abs/2510.24505", "authors": ["Qing Zong", "Jiayu Liu", "Tianshi Zheng", "Chunyang Li", "Baixuan Xu", "Haochen Shi", "Weiqi Wang", "Zhaowei Wang", "Chunkit Chan", "Yangqiu Song"], "title": "CritiCal: Can Critique Help LLM Uncertainty or Confidence Calibration?", "comment": null, "summary": "Accurate confidence calibration in Large Language Models (LLMs) is critical\nfor safe use in high-stakes domains, where clear verbalized confidence enhances\nuser trust. Traditional methods that mimic reference confidence expressions\noften fail to capture the reasoning needed for accurate confidence assessment.\nWe propose natural language critiques as a solution, ideally suited for\nconfidence calibration, as precise gold confidence labels are hard to obtain\nand often require multiple generations. This paper studies how natural language\ncritiques can enhance verbalized confidence, addressing: (1) What to critique:\nuncertainty (question-focused) or confidence (answer-specific)? Analysis shows\nconfidence suits multiple-choice tasks, while uncertainty excels in open-ended\nscenarios. (2) How to critique: self-critique or critique calibration training?\nWe propose Self-Critique, enabling LLMs to critique and optimize their\nconfidence beyond mere accuracy, and CritiCal, a novel Critique Calibration\ntraining method that leverages natural language critiques to improve confidence\ncalibration, moving beyond direct numerical optimization. Experiments show that\nCritiCal significantly outperforms Self-Critique and other competitive\nbaselines, even surpassing its teacher model, GPT-4o, in complex reasoning\ntasks. CritiCal also shows robust generalization in out-of-distribution\nsettings, advancing LLM's reliability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u5229\u7528\u81ea\u7136\u8bed\u8a00\u6279\u8bc4\u6765\u63d0\u9ad8\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7f6e\u4fe1\u5ea6\u6821\u51c6\uff0c\u63d0\u51fa\u81ea\u6211\u6279\u8bc4\u548c\u6279\u8bc4\u6821\u51c6\u8bad\u7ec3\u4e24\u79cd\u65b9\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e86\u6279\u8bc4\u6821\u51c6\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u5728\u9ad8\u98ce\u9669\u9886\u57df\u4e2d\uff0c\u51c6\u786e\u7684\u7f6e\u4fe1\u5ea6\u6821\u51c6\u5bf9\u7528\u6237\u4fe1\u4efb\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u6a21\u62df\u7f6e\u4fe1\u8868\u8fbe\u7684\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u53cd\u6620\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\u548c\u5b9e\u9645\u7f6e\u4fe1\u5ea6\u3002", "method": "\u7814\u7a76\u4e86\u6279\u8bc4\u5185\u5bb9\u5e94\u805a\u7126\u4e8e\u4e0d\u786e\u5b9a\u6027\u8fd8\u662f\u7f6e\u4fe1\u5ea6\uff0c\u63d0\u51fa\u81ea\u6211\u6279\u8bc4\uff08Self-Critique\uff09\u548c\u6279\u8bc4\u6821\u51c6\u8bad\u7ec3\uff08CritiCal\uff09\u4e24\u79cd\u65b9\u6cd5\uff0c\u540e\u8005\u5229\u7528\u81ea\u7136\u8bed\u8a00\u6279\u8bc4\u8d85\u8d8a\u6570\u503c\u4f18\u5316\u8fdb\u884c\u7f6e\u4fe1\u5ea6\u6821\u51c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCritiCal\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u81ea\u6211\u6279\u8bc4\u548c\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\uff0c\u751a\u81f3\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8d85\u8fc7\u4e86GPT-4o\u6a21\u578b\uff0c\u5e76\u5728\u5206\u5e03\u5916\u8bbe\u5b9a\u4e2d\u8868\u73b0\u51fa\u5f3a\u5065\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u81ea\u7136\u8bed\u8a00\u6279\u8bc4\u662f\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7f6e\u4fe1\u5ea6\u6821\u51c6\u7684\u6709\u6548\u7b56\u7565\uff0cCritiCal\u65b9\u6cd5\u63a8\u52a8\u4e86LLM\u7684\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\uff0c\u9002\u5408\u9ad8\u98ce\u9669\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2510.24530", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24530", "abs": "https://arxiv.org/abs/2510.24530", "authors": ["Eric G. C. Laporte"], "title": "Lev\u00e9e d'ambigu\u00eft\u00e9s par grammaires locales", "comment": "in French language", "summary": "Many words are ambiguous in terms of their part of speech (POS). However,\nwhen a word appears in a text, this ambiguity is generally much reduced.\nDisambiguating POS involves using context to reduce the number of POS\nassociated with words, and is one of the main challenges of lexical tagging.\nThe problem of labeling words by POS frequently arises in natural language\nprocessing, for example for spelling correction, grammar or style checking,\nexpression recognition, text-to-speech conversion, text corpus analysis, etc.\nLexical tagging systems are thus useful as an initial component of many natural\nlanguage processing systems. A number of recent lexical tagging systems produce\nmultiple solutions when the text is lexically ambiguous or the uniquely correct\nsolution cannot be found. These contributions aim to guarantee a zero silence\nrate: the correct tag(s) for a word must never be discarded. This objective is\nunrealistic for systems that tag each word uniquely. This article concerns a\nlexical disambiguation method adapted to the objective of a zero silence rate\nand implemented in Silberztein's INTEX system (1993). We present here a formal\ndescription of this method. We show that to verify a local disambiguation\ngrammar in this framework, it is not sufficient to consider the transducer\npaths separately: one needs to verify their interactions. Similarly, if a\ncombination of multiple transducers is used, the result cannot be predicted by\nconsidering them in isolation. Furthermore, when examining the initial labeling\nof a text as produced by INTEX, ideas for disambiguation rules come\nspontaneously, but grammatical intuitions may turn out to be inaccurate, often\ndue to an unforeseen construction or ambiguity. If a zero silence rate is\ntargeted, local grammars must be carefully tested. This is where a detailed\nspecification of what a grammar will do once applied to texts would be\nnecessary.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u8bcd\u6027\u6807\u6ce8\u4e2d\u4fdd\u6301\u96f6\u9057\u6f0f\u7387\u7684\u6d88\u6b67\u65b9\u6cd5\uff0c\u5e76\u5728INTEX\u7cfb\u7edf\u4e2d\u5b9e\u73b0\uff0c\u5f3a\u8c03\u4e86\u5c40\u90e8\u6d88\u6b67\u8bed\u6cd5\u9a8c\u8bc1\u9700\u8003\u8651\u8def\u5f84\u4ea4\u4e92\u53ca\u591a\u91cd\u81ea\u52a8\u673a\u7ec4\u5408\u3002", "motivation": "\u8bcd\u6027\u6b67\u4e49\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u4e14\u8bcd\u6027\u6d88\u6b67\u662f\u8bcd\u6c47\u6807\u6ce8\u7684\u4e3b\u8981\u6311\u6218\uff0c\u5c24\u5176\u5728\u9700\u8981\u4fdd\u8bc1\u6b63\u786e\u8bcd\u6027\u6807\u7b7e\u4e0d\u88ab\u9057\u6f0f\u7684\u5e94\u7528\u4e2d\u3002", "method": "\u57fa\u4e8eSilberztein\u7684INTEX\u7cfb\u7edf\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u6d88\u6b67\u65b9\u6cd5\uff0c\u5f3a\u8c03\u5bf9\u5c40\u90e8\u6d88\u6b67\u8bed\u6cd5\u7684\u5f62\u5f0f\u5316\u63cf\u8ff0\u53ca\u9700\u9a8c\u8bc1\u8def\u5f84\u4e4b\u95f4\u4ea4\u4e92\u548c\u591a\u81ea\u52a8\u673a\u7ec4\u5408\u7684\u6574\u4f53\u6027\u5f71\u54cd\u3002", "result": "\u6307\u51fa\u5355\u72ec\u8003\u8651\u8f6c\u5bfc\u5668\u8def\u5f84\u6216\u81ea\u52a8\u673a\u96be\u4ee5\u9884\u6d4b\u6d88\u6b67\u7ed3\u679c\uff0c\u6d88\u6b67\u89c4\u5219\u5e94\u7ecf\u8fc7\u7ec6\u81f4\u6d4b\u8bd5\uff0c\u786e\u4fdd\u96f6\u9057\u6f0f\u7387\u3002", "conclusion": "\u4e3a\u5b9e\u73b0\u96f6\u9057\u6f0f\u7387\uff0c\u6d88\u6b67\u5c40\u90e8\u8bed\u6cd5\u5fc5\u987b\u4e25\u683c\u89c4\u8303\u548c\u6d4b\u8bd5\uff0c\u7406\u8bba\u9a8c\u8bc1\u4e0e\u5b9e\u8df5\u5e94\u7528\u7d27\u5bc6\u7ed3\u5408\u624d\u80fd\u6709\u6548\u8bbe\u8ba1\u6d88\u6b67\u89c4\u5219\u3002"}}
{"id": "2510.24538", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24538", "abs": "https://arxiv.org/abs/2510.24538", "authors": ["Venkata S Govindarajan", "Laura Biester"], "title": "Dark & Stormy: Modeling Humor in the Worst Sentences Ever Written", "comment": null, "summary": "Textual humor is enormously diverse and computational studies need to account\nfor this range, including intentionally bad humor. In this paper, we curate and\nanalyze a novel corpus of sentences from the Bulwer-Lytton Fiction Contest to\nbetter understand \"bad\" humor in English. Standard humor detection models\nperform poorly on our corpus, and an analysis of literary devices finds that\nthese sentences combine features common in existing humor datasets (e.g., puns,\nirony) with metaphor, metafiction and simile. LLMs prompted to synthesize\ncontest-style sentences imitate the form but exaggerate the effect by\nover-using certain literary devices, and including far more novel\nadjective-noun bigrams than human writers. Data, code and analysis are\navailable at https://github.com/venkatasg/bulwer-lytton", "AI": {"tldr": "\u672c\u6587\u521b\u5efa\u5e76\u5206\u6790\u4e86\u4e00\u4e2a\u65b0\u7684\u6765\u81eaBulwer-Lytton\u7535\u5f71\u7ade\u8d5b\u7684\u201c\u7cdf\u7cd5\u201d\u5e7d\u9ed8\u8bed\u6599\u5e93\uff0c\u63a2\u8ba8\u82f1\u6587\u4e2d\u7684\u201c\u574f\u201d\u5e7d\u9ed8\u7279\u5f81\u3002", "motivation": "\u6587\u672c\u5e7d\u9ed8\u5f62\u5f0f\u591a\u6837\uff0c\u5c24\u5176\u662f\u6545\u610f\u5236\u9020\u7684\u4f4e\u52a3\u5e7d\u9ed8\uff0c\u8ba1\u7b97\u673a\u7814\u7a76\u5bf9\u6b64\u7406\u89e3\u4e0d\u8db3\uff0c\u9700\u6df1\u5165\u5206\u6790\u5176\u7279\u70b9\u3002", "method": "\u6536\u96c6\u5e76\u5206\u6790Bulwer-Lytton\u7ade\u8d5b\u53e5\u5b50\uff0c\u8bc4\u4f30\u6807\u51c6\u5e7d\u9ed8\u68c0\u6d4b\u6a21\u578b\u8868\u73b0\uff0c\u5206\u6790\u5176\u4e2d\u7684\u6587\u5b66\u624b\u6cd5\uff0c\u5e76\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u7ade\u8d5b\u98ce\u683c\u53e5\u5b50\u8fdb\u884c\u5bf9\u6bd4\u7814\u7a76\u3002", "result": "\u6807\u51c6\u5e7d\u9ed8\u68c0\u6d4b\u6a21\u578b\u5728\u8be5\u8bed\u6599\u5e93\u8868\u73b0\u4e0d\u4f73\uff0c\u7cdf\u7cd5\u5e7d\u9ed8\u7ed3\u5408\u4e86\u53cc\u5173\u3001\u8bbd\u523a\u3001\u9690\u55bb\u3001\u5143\u5c0f\u8bf4\u53ca\u660e\u55bb\u7b49\u591a\u79cd\u6587\u5b66\u624b\u6cd5\u3002\u5927\u6a21\u578b\u867d\u80fd\u6a21\u4eff\u5f62\u5f0f\u4f46\u8fc7\u5ea6\u4f7f\u7528\u67d0\u4e9b\u624b\u6cd5\uff0c\u4ea7\u751f\u66f4\u591a\u65b0\u9896\u7684\u5f62\u5bb9\u8bcd-\u540d\u8bcd\u642d\u914d\u3002", "conclusion": "\u7cdf\u7cd5\u5e7d\u9ed8\u5177\u6709\u72ec\u7279\u7684\u6587\u5b66\u7279\u5f81\uff0c\u73b0\u6709\u5e7d\u9ed8\u68c0\u6d4b\u6a21\u578b\u96be\u4ee5\u51c6\u786e\u8bc6\u522b\uff0c\u4e14\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5185\u5bb9\u5b58\u5728\u8fc7\u5ea6\u6a21\u4eff\u7684\u95ee\u9898\uff0c\u4e3a\u5e7d\u9ed8\u8ba1\u7b97\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u6570\u636e\u548c\u5206\u6790\u89c6\u89d2\u3002"}}
{"id": "2510.24541", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24541", "abs": "https://arxiv.org/abs/2510.24541", "authors": ["Seyoung Song", "Nawon Kim", "Songeun Chae", "Kiwoong Park", "Jiho Jin", "Haneul Yoo", "Kyunghyun Cho", "Alice Oh"], "title": "Open Korean Historical Corpus: A Millennia-Scale Diachronic Collection of Public Domain Texts", "comment": "Dataset and code available at https://github.com/seyoungsong/OKHC", "summary": "The history of the Korean language is characterized by a discrepancy between\nits spoken and written forms and a pivotal shift from Chinese characters to the\nHangul alphabet. However, this linguistic evolution has remained largely\nunexplored in NLP due to a lack of accessible historical corpora. To address\nthis gap, we introduce the Open Korean Historical Corpus, a large-scale, openly\nlicensed dataset spanning 1,300 years and 6 languages, as well as\nunder-represented writing systems like Korean-style Sinitic (Idu) and\nHanja-Hangul mixed script. This corpus contains 18 million documents and 5\nbillion tokens from 19 sources, ranging from the 7th century to 2025. We\nleverage this resource to quantitatively analyze major linguistic shifts: (1)\nIdu usage peaked in the 1860s before declining sharply; (2) the transition from\nHanja to Hangul was a rapid transformation starting around 1890; and (3) North\nKorea's lexical divergence causes modern tokenizers to produce up to 51 times\nhigher out-of-vocabulary rates. This work provides a foundational resource for\nquantitative diachronic analysis by capturing the history of the Korean\nlanguage. Moreover, it can serve as a pre-training corpus for large language\nmodels, potentially improving their understanding of Sino-Korean vocabulary in\nmodern Hangul as well as archaic writing systems.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5f00\u653e\u97e9\u6587\u5386\u53f2\u8bed\u6599\u5e93\uff0c\u6db5\u76d61300\u5e74\u30016\u79cd\u8bed\u8a00\u548c\u591a\u79cd\u4e66\u5199\u7cfb\u7edf\uff0c\u5305\u542b1800\u4e07\u6587\u6863\u548c50\u4ebf\u8bcd\u6761\uff0c\u7528\u4e8e\u5b9a\u91cf\u5206\u6790\u97e9\u8bed\u8bed\u8a00\u6f14\u53d8\u53ca\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "\u97e9\u8bed\u53e3\u8bed\u4e0e\u4e66\u5199\u5f62\u5f0f\u5dee\u5f02\u5927\uff0c\u4e14\u4e66\u5199\u7cfb\u7edf\u7ecf\u5386\u4ece\u6c49\u5b57\u5411\u97e9\u6587\u5b57\u6bcd\u7684\u91cd\u5927\u8f6c\u53d8\uff0c\u4f46\u76f8\u5173\u5386\u53f2\u8bed\u6599\u8d44\u6e90\u7f3a\u4e4f\uff0c\u9650\u5236\u4e86\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u5bf9\u97e9\u8bed\u8bed\u8a00\u6f14\u53d8\u7684\u7814\u7a76\u3002", "method": "\u6784\u5efa\u5e76\u516c\u5f00\u4e86\u4e00\u4e2a\u5927\u578b\u7684\u5386\u53f2\u97e9\u8bed\u8bed\u6599\u5e93\uff0c\u5305\u542b1300\u5e74\u95f4\u7684\u591a\u79cd\u8bed\u8a00\u4e0e\u4e66\u5199\u7cfb\u7edf\uff0c\u6db5\u76d62700\u5e74\u52302025\u5e74\u7684\u5927\u91cf\u6587\u732e\uff0c\u901a\u8fc7\u5b9a\u91cf\u5206\u6790\u5177\u4f53\u8bed\u8a00\u53d8\u5316\uff0c\u5982Idu\u4f7f\u7528\u3001\u6c49\u5b57\u5230\u97e9\u6587\u7684\u8fc7\u6e21\u53ca\u671d\u97e9\u8bcd\u6c47\u5206\u5316\u60c5\u51b5\u3002", "result": "\u53d1\u73b0Idu\u4e66\u5199\u57281860\u5e74\u4ee3\u8fbe\u5230\u9876\u5cf0\u540e\u8fc5\u901f\u4e0b\u964d\uff1b\u6c49\u5b57\u5411\u97e9\u6587\u7684\u8f6c\u53d8\u81ea1890\u5e74\u5f00\u59cb\u5feb\u901f\u5b8c\u6210\uff1b\u5317\u97e9\u8bcd\u6c47\u5dee\u5f02\u5bfc\u81f4\u73b0\u4ee3\u5206\u8bcd\u5668\u51fa\u73b0\u9ad8\u8fbe51\u500d\u7684\u672a\u767b\u5f55\u8bcd\u7387\u3002", "conclusion": "\u5f00\u653e\u97e9\u6587\u5386\u53f2\u8bed\u6599\u5e93\u4e3a\u97e9\u8bed\u8bed\u8a00\u6f14\u53d8\u7684\u91cf\u5316\u5386\u65f6\u5206\u6790\u5960\u5b9a\u57fa\u7840\uff0c\u4e5f\u53ef\u4f5c\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u8d44\u6e90\uff0c\u63d0\u5347\u6a21\u578b\u5bf9\u73b0\u4ee3\u97e9\u6587\u4e2d\u7684\u6c49\u5b57\u8bcd\u6c47\u53ca\u53e4\u4e66\u5199\u7cfb\u7edf\u7684\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2510.24570", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24570", "abs": "https://arxiv.org/abs/2510.24570", "authors": ["Rapha\u00ebl Bagat", "Irina Illina", "Emmanuel Vincent"], "title": "BEST-RQ-Based Self-Supervised Learning for Whisper Domain Adaptation", "comment": "Submitted to ICASSP 2026", "summary": "Automatic Speech Recognition (ASR) systems, despite large multilingual\ntraining, struggle in out-of-domain and low-resource scenarios where labeled\ndata is scarce. We propose BEARD (BEST-RQ Encoder Adaptation with Re-training\nand Distillation), a novel framework designed to adapt Whisper's encoder using\nunlabeled data. Unlike traditional self-supervised learning methods, BEARD\nuniquely combines a BEST-RQ objective with knowledge distillation from a frozen\nteacher encoder, ensuring the encoder's complementarity with the pre-trained\ndecoder. Our experiments focus on the ATCO2 corpus from the challenging Air\nTraffic Control (ATC) communications domain, characterized by non-native\nspeech, noise, and specialized phraseology. Using about 5,000 hours of\nuntranscribed speech for BEARD and 2 hours of transcribed speech for\nfine-tuning, the proposed approach significantly outperforms previous baseline\nand fine-tuned model, achieving a relative improvement of 12% compared to the\nfine-tuned model. To the best of our knowledge, this is the first work to use a\nself-supervised learning objective for domain adaptation of Whisper.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86BEARD\u6846\u67b6\uff0c\u901a\u8fc7\u65e0\u6807\u7b7e\u6570\u636e\u81ea\u76d1\u7763\u5b66\u4e60\u7ed3\u5408\u77e5\u8bc6\u84b8\u998f\uff0c\u6210\u529f\u5b9e\u73b0\u4e86Whisper\u7f16\u7801\u5668\u5728\u4e13\u4e1a\u9886\u57df\u7684\u9002\u5e94\uff0c\u63d0\u9ad8\u4e86\u8bed\u97f3\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709ASR\u7cfb\u7edf\u5728\u6807\u7b7e\u6570\u636e\u7a00\u7f3a\u7684\u9886\u57df\u548c\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u8db3\uff0c\u8feb\u5207\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u6a21\u578b\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51faBEARD\u6846\u67b6\uff0c\u7ed3\u5408BEST-RQ\u81ea\u76d1\u7763\u76ee\u6807\u548c\u56fa\u5b9a\u6559\u5e08\u7f16\u7801\u5668\u7684\u77e5\u8bc6\u84b8\u998f\uff0c\u5bf9Whisper\u7f16\u7801\u5668\u8fdb\u884c\u8bad\u7ec3\u4ee5\u9002\u5e94\u7279\u5b9a\u9886\u57df\u7684\u65e0\u6807\u7b7e\u6570\u636e\u3002", "result": "\u5728ATCO2\u7a7a\u4e2d\u4ea4\u901a\u7ba1\u5236\u9886\u57df\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528\u5927\u7ea65000\u5c0f\u65f6\u65e0\u6807\u7b7e\u6570\u636e\u8bad\u7ec3BEARD\uff0c\u7ed3\u54082\u5c0f\u65f6\u8f6c\u5f55\u6570\u636e\u5fae\u8c03\uff0c\u4f7f\u6a21\u578b\u6027\u80fd\u8f83\u5fae\u8c03\u6a21\u578b\u63d0\u534712%\u3002", "conclusion": "BEARD\u9996\u6b21\u5c06\u81ea\u76d1\u7763\u5b66\u4e60\u76ee\u6807\u5e94\u7528\u4e8eWhisper\u7684\u9886\u57df\u9002\u5e94\uff0c\u6709\u6548\u514b\u670d\u4e86\u6807\u7b7e\u7a00\u7f3a\u95ee\u9898\uff0c\u63d0\u5347\u4e86ASR\u7cfb\u7edf\u5728\u4e13\u4e1a\u9886\u57df\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u7387\u3002"}}
{"id": "2510.24591", "categories": ["cs.CL", "astro-ph.IM"], "pdf": "https://arxiv.org/pdf/2510.24591", "abs": "https://arxiv.org/abs/2510.24591", "authors": ["Christine Ye", "Sihan Yuan", "Suchetha Cooray", "Steven Dillmann", "Ian L. V. Roque", "Dalya Baron", "Philipp Frank", "Sergio Martin-Alvarez", "Nolan Koblischke", "Frank J Qu", "Diyi Yang", "Risa Wechsler", "Ioana Ciuca"], "title": "ReplicationBench: Can AI Agents Replicate Astrophysics Research Papers?", "comment": null, "summary": "Frontier AI agents show increasing promise as scientific research assistants,\nand may eventually be useful for extended, open-ended research workflows.\nHowever, in order to use agents for novel research, we must first assess the\nunderlying faithfulness and correctness of their work. To evaluate agents as\nresearch assistants, we introduce ReplicationBench, an evaluation framework\nthat tests whether agents can replicate entire research papers drawn from the\nastrophysics literature. Astrophysics, where research relies heavily on\narchival data and computational study while requiring little real-world\nexperimentation, is a particularly useful testbed for AI agents in scientific\nresearch. We split each paper into tasks which require agents to replicate the\npaper's core contributions, including the experimental setup, derivations, data\nanalysis, and codebase. Each task is co-developed with the original paper\nauthors and targets a key scientific result, enabling objective evaluation of\nboth faithfulness (adherence to original methods) and correctness (technical\naccuracy of results). ReplicationBench is extremely challenging for current\nfrontier language models: even the best-performing language models score under\n20%. We analyze ReplicationBench trajectories in collaboration with domain\nexperts and find a rich, diverse set of failure modes for agents in scientific\nresearch. ReplicationBench establishes the first benchmark of paper-scale,\nexpert-validated astrophysics research tasks, reveals insights about agent\nperformance generalizable to other domains of data-driven science, and provides\na scalable framework for measuring AI agents' reliability in scientific\nresearch.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ReplicationBench\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u79d1\u7814\u52a9\u7406\u5728\u5929\u4f53\u7269\u7406\u5b66\u8bba\u6587\u590d\u5236\u4e0a\u7684\u80fd\u529b\uff0c\u76ee\u524d\u6700\u5148\u8fdb\u7684\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u4e0d\u8db320%\u3002", "motivation": "\u968f\u7740AI\u79d1\u7814\u52a9\u7406\u7684\u53d1\u5c55\uff0c\u9700\u8981\u8bc4\u4f30\u5176\u5728\u521b\u65b0\u79d1\u7814\u4e2d\u7684\u5fe0\u5b9e\u6027\u548c\u51c6\u786e\u6027\uff0c\u786e\u4fdd\u5176\u5de5\u4f5c\u53ef\u9760\u3002", "method": "\u8bbe\u8ba1ReplicationBench\uff0c\u5c06\u5929\u4f53\u7269\u7406\u5b66\u8bba\u6587\u62c6\u5206\u4e3a\u591a\u4e2a\u4efb\u52a1\uff0c\u6db5\u76d6\u5b9e\u9a8c\u8bbe\u8ba1\u3001\u63a8\u5bfc\u3001\u6570\u636e\u5206\u6790\u548c\u4ee3\u7801\u5b9e\u73b0\uff0c\u5e76\u4e0e\u8bba\u6587\u4f5c\u8005\u5171\u540c\u5236\u5b9a\uff0c\u8fdb\u884c\u5ba2\u89c2\u8bc4\u4f30\u3002", "result": "\u5f53\u524d\u9876\u5c16\u8bed\u8a00\u6a21\u578b\u5728\u8be5\u6d4b\u8bd5\u4e2d\u7684\u8868\u73b0\u4f4e\u4e8e20%\uff0c\u5e76\u63ed\u793a\u4e86\u79d1\u5b66\u7814\u7a76\u4e2dAI\u52a9\u7406\u7684\u591a\u6837\u5316\u5931\u8d25\u6a21\u5f0f\u3002", "conclusion": "ReplicationBench\u4e3aAI\u5728\u79d1\u5b66\u7814\u7a76\u4e2d\u7684\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u9996\u4e2a\u4e13\u5bb6\u9a8c\u8bc1\u7684\u57fa\u51c6\u6846\u67b6\uff0c\u80fd\u63a8\u5e7f\u81f3\u5176\u4ed6\u6570\u636e\u9a71\u52a8\u79d1\u5b66\u9886\u57df\uff0c\u4fc3\u8fdbAI\u79d1\u7814\u52a9\u7406\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2510.24592", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24592", "abs": "https://arxiv.org/abs/2510.24592", "authors": ["Guoxin Chen", "Jing Wu", "Xinjie Chen", "Wayne Xin Zhao", "Ruihua Song", "Chengxi Li", "Kai Fan", "Dayiheng Liu", "Minpeng Liao"], "title": "ReForm: Reflective Autoformalization with Prospective Bounded Sequence Optimization", "comment": "Ongoing Work", "summary": "Autoformalization, which translates natural language mathematics into\nmachine-verifiable formal statements, is critical for using formal mathematical\nreasoning to solve math problems stated in natural language. While Large\nLanguage Models can generate syntactically correct formal statements, they\noften fail to preserve the original problem's semantic intent. This limitation\narises from the LLM approaches' treating autoformalization as a simplistic\ntranslation task which lacks mechanisms for self-reflection and iterative\nrefinement that human experts naturally employ. To address these issues, we\npropose ReForm, a Reflective Autoformalization method that tightly integrates\nsemantic consistency evaluation into the autoformalization process. This\nenables the model to iteratively generate formal statements, assess its\nsemantic fidelity, and self-correct identified errors through progressive\nrefinement. To effectively train this reflective model, we introduce\nProspective Bounded Sequence Optimization (PBSO), which employs different\nrewards at different sequence positions to ensure that the model develops both\naccurate autoformalization and correct semantic validations, preventing\nsuperficial critiques that would undermine the purpose of reflection. Extensive\nexperiments across four autoformalization benchmarks demonstrate that ReForm\nachieves an average improvement of 17.2 percentage points over the strongest\nbaselines. To further ensure evaluation reliability, we introduce\nConsistencyCheck, a benchmark of 859 expert-annotated items that not only\nvalidates LLMs as judges but also reveals that autoformalization is inherently\ndifficult: even human experts produce semantic errors in up to 38.5% of cases.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aReForm\u7684\u53cd\u601d\u5f0f\u81ea\u52a8\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u4e00\u81f4\u6027\u8bc4\u4f30\u5b9e\u73b0\u81ea\u52a8\u5f62\u5f0f\u5316\u7684\u8fed\u4ee3\u751f\u6210\u548c\u81ea\u6211\u7ea0\u9519\uff0c\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u63d0\u534717.2\u4e2a\u767e\u5206\u70b9\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5c06\u81ea\u7136\u8bed\u8a00\u6570\u5b66\u8bed\u53e5\u8f6c\u5316\u4e3a\u5f62\u5f0f\u5316\u8bed\u53e5\u65f6\uff0c\u5f80\u5f80\u65e0\u6cd5\u4fdd\u6301\u539f\u95ee\u9898\u7684\u8bed\u4e49\u610f\u56fe\uff0c\u56e0\u4e3a\u5b83\u4eec\u4ec5\u89c6\u81ea\u52a8\u5f62\u5f0f\u5316\u4e3a\u7b80\u5355\u7684\u7ffb\u8bd1\u4efb\u52a1\uff0c\u7f3a\u4e4f\u4eba\u7c7b\u4e13\u5bb6\u81ea\u7136\u5177\u5907\u7684\u53cd\u601d\u548c\u8fed\u4ee3\u6539\u8fdb\u673a\u5236\u3002", "method": "\u63d0\u51faReForm\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u8bed\u4e49\u4e00\u81f4\u6027\u8bc4\u4f30\u7d27\u5bc6\u6574\u5408\u5165\u81ea\u52a8\u5f62\u5f0f\u5316\u8fc7\u7a0b\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u8fed\u4ee3\u751f\u6210\u5f62\u5f0f\u5316\u8bed\u53e5\u3001\u81ea\u6211\u8bc4\u4f30\u8bed\u4e49\u4e00\u81f4\u6027\u5e76\u9010\u6b65\u4fee\u6b63\u9519\u8bef\u3002\u91c7\u7528\u524d\u77bb\u6709\u754c\u5e8f\u5217\u4f18\u5316(PBSO)\u8bad\u7ec3\u6a21\u578b\uff0c\u4e0d\u540c\u5e8f\u5217\u4f4d\u7f6e\u8d4b\u4e88\u4e0d\u540c\u5956\u52b1\uff0c\u786e\u4fdd\u6a21\u578b\u540c\u65f6\u63d0\u9ad8\u5f62\u5f0f\u5316\u51c6\u786e\u5ea6\u548c\u8bed\u4e49\u9a8c\u8bc1\u80fd\u529b\uff0c\u907f\u514d\u8868\u9762\u6279\u8bc4\u3002", "result": "\u5728\u56db\u4e2a\u81ea\u52a8\u5f62\u5f0f\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cReForm\u8f83\u6700\u5f3a\u57fa\u7ebf\u5e73\u5747\u63d0\u9ad817.2\u4e2a\u767e\u5206\u70b9\u6548\u679c\u8868\u73b0\u663e\u8457\u3002\u5f15\u5165ConsistencyCheck\u6807\u51c6\uff0c\u57fa\u4e8e859\u4e2a\u4e13\u5bb6\u6807\u6ce8\u9879\u9a8c\u8bc1\u8bc4\u6d4b\u53ef\u9760\u6027\uff0c\u5e76\u63ed\u793a\u81ea\u52a8\u5f62\u5f0f\u5316\u5177\u6709\u8f83\u5927\u96be\u5ea6\uff0c\u4e13\u5bb6\u4e5f\u5b58\u5728\u9ad8\u8fbe38.5%\u7684\u8bed\u4e49\u9519\u8bef\u7387\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u8bed\u4e49\u4e00\u81f4\u6027\u8bc4\u4f30\u7684\u53cd\u601d\u673a\u5236\uff0cReForm\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u5f62\u5f0f\u5316\u7684\u8bed\u4e49\u4fdd\u771f\u5ea6\u548c\u6574\u4f53\u6027\u80fd\uff0c\u4fc3\u8fdb\u4e86\u81ea\u7136\u8bed\u8a00\u6570\u5b66\u8bed\u53e5\u5230\u673a\u5668\u53ef\u6821\u9a8c\u5f62\u5f0f\u5316\u8868\u8fbe\u7684\u51c6\u786e\u8f6c\u6362\u3002"}}
{"id": "2510.24605", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24605", "abs": "https://arxiv.org/abs/2510.24605", "authors": ["Yicun Yang", "Cong Wang", "Shaobo Wang", "Zichen Wen", "Biqing Qi", "Hanlin Xu", "Linfeng Zhang"], "title": "Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead the Way", "comment": null, "summary": "Diffusion-based large language models (dLLMs) have exhibited substantial\npotential for parallel text generation, which may enable more efficient\ngeneration compared to autoregressive models. However, current dLLMs suffer\nfrom fixed generation lengths, which indicates the generation lengths of dLLMs\nhave to be determined before decoding as a hyper-parameter, leading to issues\nin efficiency and flexibility. To solve these problems, in this work, we\npropose to train a diffusion LLM with native variable generation lengths,\nabbreviated as dLLM-Var. Concretely, we aim to train a model to accurately\npredict the [EOS] token in the generated text, which makes a dLLM be able to\nnatively infer in a block diffusion manner, while still maintaining the ability\nof global bi-directional (full) attention and high parallelism. Experiments on\nstandard benchmarks demonstrate that our method achieves a 30.1x speedup over\ntraditional dLLM inference paradigms and a 2.4x speedup relative to\nautoregressive models such as Qwen and Llama. Our method achieves higher\naccuracy and faster inference, elevating dLLMs beyond mere academic novelty and\nsupporting their practical use in real-world applications. Codes and models\nhave been released.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u53ef\u53d8\u751f\u6210\u957f\u5ea6\u7684\u6269\u6563\u5f0f\u5927\u8bed\u8a00\u6a21\u578bdLLM-Var\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfdLLM\u751f\u6210\u957f\u5ea6\u56fa\u5b9a\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u751f\u6210\u6548\u7387\u548c\u7075\u6d3b\u6027\u3002", "motivation": "\u4f20\u7edf\u6269\u6563\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u957f\u5ea6\u56fa\u5b9a\uff0c\u9700\u4e8b\u5148\u8bbe\u5b9a\u8d85\u53c2\u6570\uff0c\u9650\u5236\u4e86\u6548\u7387\u548c\u7075\u6d3b\u6027\u3002", "method": "\u8bad\u7ec3\u6a21\u578b\u51c6\u786e\u9884\u6d4b\u751f\u6210\u6587\u672c\u4e2d\u7684[EOS]\u7ed3\u675f\u6807\u8bb0\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u539f\u751f\u5b9e\u73b0\u53d8\u957f\u751f\u6210\uff0c\u540c\u65f6\u4fdd\u6301\u5168\u5c40\u53cc\u5411\u6ce8\u610f\u529b\u548c\u9ad8\u5e76\u884c\u6027\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u65b9\u6cd5\u6bd4\u4f20\u7edf\u6269\u6563\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u5feb30.1\u500d\uff0c\u6bd4\u81ea\u56de\u5f52\u6a21\u578b\uff08\u5982Qwen\u548cLlama\uff09\u5feb2.4\u500d\uff0c\u4e14\u51c6\u786e\u5ea6\u66f4\u9ad8\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b9e\u7528\u6027\u548c\u6548\u7387\uff0c\u63a8\u52a8\u5176\u5e94\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u3002"}}
{"id": "2510.24606", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24606", "abs": "https://arxiv.org/abs/2510.24606", "authors": ["Siheng Xiong", "Joe Zou", "Faramarz Fekri", "Yae Jee Cho"], "title": "Long-Context Modeling with Dynamic Hierarchical Sparse Attention for On-Device LLMs", "comment": "Accepted to NeurIPS 2025 Workshop on Efficient Reasoning", "summary": "The quadratic cost of attention hinders the scalability of long-context LLMs,\nespecially in resource-constrained settings. Existing static sparse methods\nsuch as sliding windows or global tokens utilizes the sparsity of attention to\nreduce the cost of attention, but poorly adapts to the content-dependent\nvariations in attention due to their staticity. While previous work has\nproposed several dynamic approaches to improve flexibility, they still depend\non predefined templates or heuristic mechanisms. Such strategies reduce\ngenerality and prune tokens that remain contextually important, limiting their\naccuracy across diverse tasks. To tackle these bottlenecks of existing methods\nfor long-context modeling, we introduce Dynamic Hierarchical Sparse Attention\n(DHSA), a data-driven framework that dynamically predicts attention sparsity\nonline without retraining. Our proposed DHSA adaptively segments sequences into\nvariable-length chunks, then computes chunk representations by aggregating the\ntoken embeddings within each chunk. To avoid the bias introduced by varying\nchunk lengths, we apply length-normalized aggregation that scales the averaged\nembeddings by the square root of the chunk size. Finally, DHSA upsamples the\nchunk-level similarity scores to token level similarities to calculate\nimportance scores that determine which token-level interactions should be\npreserved. Our experiments on Gemma2 with Needle-in-a-Haystack Test and\nLongBench show that DHSA matches dense attention in accuracy, while reducing\nprefill latency by 20-60% and peak memory usage by 35%. Compared to other\nrepresentative baselines such as block sparse attention, DHSA achieves\nconsistently higher accuracy (6-18% relative gains) with comparable or lower\ncost, offering an efficient and adaptable solution for long-context on-device\nLLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u52a8\u6001\u5206\u5c42\u7a00\u758f\u6ce8\u610f\u529b(DHSA)\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u5927\u6a21\u578b\u4e2d\u6ce8\u610f\u529b\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u9002\u5e94\u6027\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7684\u9759\u6001\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u867d\u7136\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u4f46\u7531\u4e8e\u5176\u56fa\u5b9a\u6a21\u5f0f\uff0c\u65e0\u6cd5\u7075\u6d3b\u9002\u5e94\u5185\u5bb9\u53d8\u5316\uff0c\u5f71\u54cd\u51c6\u786e\u6027\u3002\u73b0\u6709\u52a8\u6001\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u6a21\u677f\uff0c\u9650\u5236\u4e86\u901a\u7528\u6027\u548c\u51c6\u786e\u6027\u3002", "method": "DHSA\u901a\u8fc7\u52a8\u6001\u5728\u7ebf\u9884\u6d4b\u6ce8\u610f\u529b\u7a00\u758f\u6027\uff0c\u81ea\u9002\u5e94\u5730\u5c06\u5e8f\u5217\u5206\u5272\u6210\u53ef\u53d8\u957f\u5ea6\u5757\uff0c\u7528\u957f\u5ea6\u5f52\u4e00\u5316\u7684\u805a\u5408\u65b9\u5f0f\u8ba1\u7b97\u5757\u8868\u793a\uff0c\u518d\u5c06\u5757\u7ea7\u76f8\u4f3c\u5ea6\u4e0a\u91c7\u6837\u5230\u4ee4\u724c\u7ea7\uff0c\u4ee5\u786e\u5b9a\u4fdd\u7559\u7684\u4ea4\u4e92\uff0c\u4ece\u800c\u6709\u6548\u51cf\u5c11\u8ba1\u7b97\u91cf\u4e14\u4e0d\u9700\u91cd\u8bad\u7ec3\u3002", "result": "\u5728Gemma2\u7684\u6d4b\u8bd5\u4e2d\uff0cDHSA\u5728\u51c6\u786e\u7387\u4e0a\u4e0e\u7a20\u5bc6\u6ce8\u610f\u529b\u6301\u5e73\uff0c\u540c\u65f6\u9884\u586b\u5145\u5ef6\u8fdf\u51cf\u5c1120-60%\uff0c\u5cf0\u503c\u5185\u5b58\u4f7f\u7528\u964d\u4f4e35%\u3002\u4e0e\u533a\u5757\u7a00\u758f\u6ce8\u610f\u529b\u7b49\u57fa\u7ebf\u76f8\u6bd4\uff0cDHSA\u5728\u51c6\u786e\u7387\u4e0a\u63d0\u53476-18%\uff0c\u8ba1\u7b97\u6210\u672c\u76f8\u5f53\u6216\u66f4\u4f4e\u3002", "conclusion": "DHSA\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u957f\u4e0a\u4e0b\u6587\u8868\u793a\u65b9\u6cd5\uff0c\u9002\u5408\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u517c\u987e\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u3002"}}
{"id": "2510.24619", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.24619", "abs": "https://arxiv.org/abs/2510.24619", "authors": ["Snegha A", "Sayambhu Sen", "Piyush Singh Pasi", "Abhishek Singhania", "Preethi Jyothi"], "title": "Zero-Shot Cross-Lingual Transfer using Prefix-Based Adaptation", "comment": "12 Pages", "summary": "With the release of new large language models (LLMs) like Llama and Mistral,\nzero-shot cross-lingual transfer has become increasingly feasible due to their\nmultilingual pretraining and strong generalization capabilities. However,\nadapting these decoder-only LLMs to new tasks across languages remains\nchallenging. While parameter-efficient fine-tuning (PeFT) techniques like\nLow-Rank Adaptation (LoRA) are widely used, prefix-based techniques such as\nsoft prompt tuning, prefix tuning, and Llama Adapter are less explored,\nespecially for zero-shot transfer in decoder-only models. We present a\ncomprehensive study of three prefix-based methods for zero-shot cross-lingual\ntransfer from English to 35+ high- and low-resource languages. Our analysis\nfurther explores transfer across linguistic families and scripts, as well as\nthe impact of scaling model sizes from 1B to 24B. With Llama 3.1 8B, prefix\nmethods outperform LoRA-baselines by up to 6% on the Belebele benchmark.\nSimilar improvements were observed with Mistral v0.3 7B as well. Despite using\nonly 1.23M learning parameters with prefix tuning, we achieve consistent\nimprovements across diverse benchmarks. These findings highlight the potential\nof prefix-based techniques as an effective and scalable alternative to LoRA,\nparticularly in low-resource multilingual settings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u9488\u5bf9\u89e3\u7801\u5668\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u96f6-shot\u8de8\u8bed\u8a00\u8fc1\u79fb\uff0c\u91cd\u70b9\u6bd4\u8f83\u4e86\u524d\u7f00\u8c03\u4f18\u65b9\u6cd5\u4e0eLoRA\u6280\u672f\uff0c\u5728Llama\u548cMistral\u6a21\u578b\u4e0a\u5bf935\u79cd\u4ee5\u4e0a\u8bed\u8a00\u7684\u9002\u5e94\u6548\u679c\uff0c\u53d1\u73b0\u524d\u7f00\u65b9\u6cd5\u5728\u591a\u8bed\u8a00\u548c\u591a\u8d44\u6e90\u73af\u5883\u4e0b\u8868\u73b0\u4f18\u4e8eLoRA\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5177\u5907\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u548c\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u89e3\u7801\u5668\u7ed3\u6784\u7684\u6a21\u578b\u5728\u8de8\u8bed\u8a00\u4efb\u52a1\u7684\u96f6-shot\u5fae\u8c03\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u7279\u522b\u662f\u524d\u7f00\u8c03\u4f18\u65b9\u6cd5\u7684\u5e94\u7528\u8f83\u5c11\u4e9f\u9700\u7814\u7a76\u3002", "method": "\u7cfb\u7edf\u6bd4\u8f83\u4e86\u4e09\u79cd\u57fa\u4e8e\u524d\u7f00\u7684\u8c03\u4f18\u6280\u672f\uff08\u8f6f\u63d0\u793a\u8c03\u4f18\u3001\u524d\u7f00\u8c03\u4f18\u548cLlama Adapter\uff09\uff0c\u5728Llama 3.1 8B\u548cMistral 7B\u6a21\u578b\u4e0a\uff0c\u9488\u5bf9\u82f1\u6587\u5230\u8d85\u8fc735\u79cd\u8bed\u8a00\u7684\u8de8\u8bed\u8a00\u8fc1\u79fb\u8fdb\u884c\u5b9e\u9a8c\uff0c\u8fd8\u5206\u6790\u4e86\u4e0d\u540c\u8bed\u8a00\u5bb6\u65cf\u3001\u6587\u5b57\u4f53\u7cfb\u53ca\u6a21\u578b\u89c4\u6a21\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4f7f\u7528Llama 3.1 8B\u6a21\u578b\u65f6\uff0c\u524d\u7f00\u8c03\u4f18\u65b9\u6cd5\u5728Belebele\u57fa\u51c6\u4e0a\u76f8\u6bd4LoRA\u63d0\u5347\u6700\u9ad8\u8fbe6%\uff1b\u5728Mistral 7B\u6a21\u578b\u4e0a\u4e5f\u89c2\u5bdf\u5230\u7c7b\u4f3c\u6539\u8fdb\u3002\u524d\u7f00\u8c03\u4f18\u4ec5\u97001.23M\u53c2\u6570\uff0c\u4ecd\u80fd\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7a33\u5b9a\u63d0\u5347\u8868\u73b0\u3002", "conclusion": "\u524d\u7f00\u8c03\u4f18\u6280\u672f\u4f5c\u4e3a\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u8c03\u4f18\u65b9\u6cd5\uff0c\u5728\u89e3\u7801\u5668LLMs\u7684\u96f6-shot\u8de8\u8bed\u8a00\u8fc1\u79fb\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u5c24\u5176\u9002\u5408\u4f4e\u8d44\u6e90\u591a\u8bed\u8a00\u73af\u5883\uff0c\u5c55\u793a\u4e86\u5176\u4f5c\u4e3aLoRA\u66ff\u4ee3\u65b9\u6848\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.24626", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24626", "abs": "https://arxiv.org/abs/2510.24626", "authors": ["William Held", "David Hall", "Percy Liang", "Diyi Yang"], "title": "Relative Scaling Laws for LLMs", "comment": null, "summary": "Scaling laws describe how language models improve with additional data,\nparameters, and compute. While widely used, they are typically measured on\naggregate test sets. Aggregate evaluations yield clean trends but average over\nheterogeneous subpopulations, obscuring performance disparities. We introduce\nrelative scaling laws, which track how performance gaps between test\ndistributions evolve with scale rather than focusing solely on absolute error.\nUsing 255 decoder-only Transformers trained under matched-compute (IsoFLOP)\nbudgets from $10^{18}$--$10^{20}$ FLOPs on standard pretraining datasets, we\nfind diverse trajectories: academic domains on MMLU converge toward parity;\nregional English dialects shift depending on population size; and clusters of\nAI risk behaviours split, with capability- and influence-related risks\nincreasing during pretraining while adversarial risks do not. These results\nshow that although scaling improves overall performance, it is not a universal\nequalizer. To support further study, we release all model checkpoints from this\nwork to enable practitioners to measure relative alongside traditional scaling\nlaws, in order to better prioritize robustness challenges in light of the\nbitter lesson.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u76f8\u5bf9\u89c4\u6a21\u6cd5\u5219\uff0c\u7528\u4e8e\u5206\u6790\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u6d4b\u8bd5\u5206\u5e03\u4e2d\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u53d1\u73b0\u89c4\u6a21\u589e\u52a0\u5e76\u4e0d\u4e00\u5b9a\u51cf\u5c11\u6240\u6709\u6027\u80fd\u5dee\u8ddd\u3002", "motivation": "\u4f20\u7edf\u7684\u89c4\u6a21\u6cd5\u5219\u901a\u8fc7\u805a\u5408\u6d4b\u8bd5\u96c6\u8861\u91cf\u6a21\u578b\u6027\u80fd\u63d0\u5347\uff0c\u5ffd\u7565\u4e86\u4e0d\u540c\u5b50\u7fa4\u4f53\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u53ef\u80fd\u63a9\u76d6\u4e86\u4e0d\u5e73\u7b49\u73b0\u8c61\u3002", "method": "\u4f5c\u8005\u4f7f\u7528255\u4e2a\u5728\u76f8\u540c\u8ba1\u7b97\u9884\u7b97\uff08IsoFLOP\uff09\u4e0b\u8bad\u7ec3\u7684\u89e3\u7801\u5668\u5f0fTransformer\u6a21\u578b\uff0c\u8986\u76d6\u4ece10^{18}\u523010^{20} FLOPs\u7684\u89c4\u6a21\uff0c\u901a\u8fc7\u76f8\u5bf9\u89c4\u6a21\u6cd5\u5219\u8ffd\u8e2a\u4e0d\u540c\u6d4b\u8bd5\u5206\u5e03\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u968f\u89c4\u6a21\u53d8\u5316\u7684\u8d8b\u52bf\u3002", "result": "\u53d1\u73b0\u4e0d\u540c\u9886\u57df\u8868\u73b0\u8f68\u8ff9\u5404\u5f02\uff1a\u5b66\u672f\u9886\u57df\u8d8b\u5411\u6027\u80fd\u5747\u8861\uff0c\u533a\u57df\u82f1\u8bed\u65b9\u8a00\u8868\u73b0\u4f9d\u4eba\u53e3\u89c4\u6a21\u53d8\u5316\u800c\u53d8\u5316\uff0cAI\u98ce\u9669\u884c\u4e3a\u8868\u73b0\u51fa\u80fd\u529b\u548c\u5f71\u54cd\u76f8\u5173\u98ce\u9669\u968f\u8bad\u7ec3\u589e\u52a0\u800c\u52a0\u5267\uff0c\u800c\u5bf9\u6297\u6027\u98ce\u9669\u5219\u65e0\u660e\u663e\u53d8\u5316\u3002", "conclusion": "\u867d\u7136\u6a21\u578b\u89c4\u6a21\u6269\u5927\u6574\u4f53\u6027\u80fd\u63d0\u5347\uff0c\u4f46\u5e76\u975e\u6240\u6709\u6027\u80fd\u5dee\u5f02\u90fd\u4f1a\u6d88\u5931\u3002\u5f15\u5165\u76f8\u5bf9\u89c4\u6a21\u6cd5\u5219\u6709\u52a9\u4e8e\u63ed\u793a\u5e76\u4f18\u5148\u89e3\u51b3\u6a21\u578b\u9c81\u68d2\u6027\u7684\u6311\u6218\u3002\u4f5c\u8005\u8fd8\u516c\u5f00\u4e86\u6240\u6709\u6a21\u578b\u68c0\u67e5\u70b9\u4f9b\u540e\u7eed\u7814\u7a76\u4f7f\u7528\u3002"}}
{"id": "2510.24628", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24628", "abs": "https://arxiv.org/abs/2510.24628", "authors": ["Anh Ngo", "Nicolas Rollet", "Catherine Pelachaud", "Chloe Clavel"], "title": "\"Mm, Wat?\" Detecting Other-initiated Repair Requests in Dialogue", "comment": "9 pages", "summary": "Maintaining mutual understanding is a key component in human-human\nconversation to avoid conversation breakdowns, in which repair, particularly\nOther-Initiated Repair (OIR, when one speaker signals trouble and prompts the\nother to resolve), plays a vital role. However, Conversational Agents (CAs)\nstill fail to recognize user repair initiation, leading to breakdowns or\ndisengagement. This work proposes a multimodal model to automatically detect\nrepair initiation in Dutch dialogues by integrating linguistic and prosodic\nfeatures grounded in Conversation Analysis. The results show that prosodic cues\ncomplement linguistic features and significantly improve the results of\npretrained text and audio embeddings, offering insights into how different\nfeatures interact. Future directions include incorporating visual cues,\nexploring multilingual and cross-context corpora to assess the robustness and\ngeneralizability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8bed\u8a00\u548c\u8bed\u97f3\u7279\u5f81\u7684\u591a\u6a21\u6001\u6a21\u578b\uff0c\u7528\u4e8e\u81ea\u52a8\u68c0\u6d4b\u8377\u5170\u8bed\u5bf9\u8bdd\u4e2d\u7684\u4fee\u590d\u542f\u52a8\uff0c\u4ee5\u63d0\u9ad8\u5bf9\u8bdd\u8fde\u7eed\u6027\u3002", "motivation": "\u5f53\u524d\u5bf9\u8bdd\u4ee3\u7406\u65e0\u6cd5\u6709\u6548\u8bc6\u522b\u7528\u6237\u7684\u4fee\u590d\u542f\u52a8\uff0c\u5bfc\u81f4\u5bf9\u8bdd\u4e2d\u65ad\u548c\u7528\u6237\u6d41\u5931\uff0c\u4e9f\u9700\u63d0\u5347\u5bf9\u4fee\u590d\u542f\u52a8\u7684\u68c0\u6d4b\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u4f1a\u8bdd\u5206\u6790\u7406\u8bba\uff0c\u7ed3\u5408\u8bed\u8a00\u7279\u5f81\u548c\u97f5\u5f8b\u7279\u5f81\uff0c\u63d0\u51fa\u4e00\u4e2a\u591a\u6a21\u6001\u6a21\u578b\u7528\u4e8e\u81ea\u52a8\u68c0\u6d4b\u4fee\u590d\u542f\u52a8\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u97f5\u5f8b\u7ebf\u7d22\u80fd\u663e\u8457\u63d0\u5347\u5df2\u9884\u8bad\u7ec3\u6587\u672c\u4e0e\u97f3\u9891\u5d4c\u5165\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u4e0d\u540c\u7279\u5f81\u95f4\u7684\u4ea4\u4e92\u4f5c\u7528\u3002", "conclusion": "\u7ed3\u5408\u8bed\u8a00\u548c\u97f5\u5f8b\u7279\u5f81\u7684\u591a\u6a21\u6001\u6a21\u578b\u80fd\u6709\u6548\u63d0\u5347\u5bf9\u8bdd\u4e2d\u4fee\u590d\u542f\u52a8\u7684\u8bc6\u522b\u6548\u679c\uff0c\u672a\u6765\u53ef\u62d3\u5c55\u81f3\u89c6\u89c9\u4fe1\u606f\u548c\u591a\u8bed\u79cd\u591a\u573a\u666f\u4ee5\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.24636", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24636", "abs": "https://arxiv.org/abs/2510.24636", "authors": ["Ziyou Hu", "Zhengliang Shi", "Minghang Zhu", "Haitao Li", "Teng Sun", "Pengjie Ren", "Suzan Verberne", "Zhaochun Ren"], "title": "OpenReward: Learning to Reward Long-form Agentic Tasks via Reinforcement Learning", "comment": null, "summary": "Reward models (RMs) have become essential for aligning large language models\n(LLMs), serving as scalable proxies for human evaluation in both training and\ninference. However, existing RMs struggle on knowledge-intensive and long-form\ntasks, where evaluating correctness requires grounding beyond the model's\ninternal knowledge. This limitation hinders them from reliably discriminating\nsubtle quality differences, especially when external evidence is necessary. To\naddress this, we introduce OpenRM, a tool-augmented long-form reward model that\nsystematically judges open-ended responses by invoking external tools to gather\nrelevant evidence. We train OpenRM with Group Relative Policy Optimization\n(GRPO) on over 27K synthesized pairwise examples generated through a\ncontrollable data synthesis framework. The training objective jointly\nsupervises intermediate tool usage and final outcome accuracy, incentivizing\nour reward model to learn effective evidence-based judgment strategies.\nExtensive experiments on three newly-collected datasets and two widely-used\nbenchmarks demonstrate that OpenRM substantially outperforms existing reward\nmodeling approaches. As a further step, we integrate OpenRM into both\ninference-time response selection and training-time data selection. This yields\nconsistent gains in downstream LLM alignment tasks, highlighting the potential\nof tool-augmented reward models for scaling reliable long-form evaluation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86OpenRM\uff0c\u4e00\u79cd\u7ed3\u5408\u5916\u90e8\u5de5\u5177\u4ee5\u6536\u96c6\u8bc1\u636e\uff0c\u63d0\u5347\u957f\u6587\u672c\u5956\u52b1\u6a21\u578b\u5224\u65ad\u5f00\u6e90\u56de\u7b54\u6b63\u786e\u6027\u7684\u5de5\u5177\u589e\u5f3a\u5956\u52b1\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u5956\u52b1\u6a21\u578b\u5728\u77e5\u8bc6\u5bc6\u96c6\u4e14\u9700\u5916\u90e8\u8bc1\u636e\u652f\u6301\u7684\u957f\u6587\u672c\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u96be\u4ee5\u7ec6\u81f4\u533a\u5206\u56de\u7b54\u8d28\u91cf\u3002", "method": "\u5f15\u5165OpenRM\uff0c\u5229\u7528\u5916\u90e8\u5de5\u5177\u6536\u96c6\u76f8\u5173\u8bc1\u636e\uff0c\u7ed3\u540827K\u5bf9\u5408\u6210\u6570\u636e\u8bad\u7ec3\uff0c\u91c7\u7528Group Relative Policy Optimization\uff0c\u8054\u5408\u76d1\u7763\u5de5\u5177\u8c03\u7528\u548c\u6700\u7ec8\u51c6\u786e\u5ea6\u3002", "result": "\u5728\u4e09\u7ec4\u65b0\u6536\u96c6\u6570\u636e\u548c\u4e24\u4e2a\u5e38\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cOpenRM\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5956\u52b1\u6a21\u578b\u3002", "conclusion": "OpenRM\u901a\u8fc7\u5de5\u5177\u589e\u5f3a\u5956\u52b1\u8bc4\u4f30\uff0c\u6539\u5584\u4e86\u957f\u6587\u672c\u4efb\u52a1\u7684\u6a21\u578b\u5bf9\u9f50\u4e0e\u8bc4\u5224\uff0c\u5c55\u73b0\u51fa\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u5747\u663e\u8457\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.24647", "categories": ["cs.CL", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.24647", "abs": "https://arxiv.org/abs/2510.24647", "authors": ["Hugo Rydel-Johnston", "Alex Kafkas"], "title": "Quantifying the Effects of Word Length, Frequency, and Predictability on Dyslexia", "comment": null, "summary": "We ask where, and under what conditions, dyslexic reading costs arise in a\nlarge-scale naturalistic reading dataset. Using eye-tracking aligned to\nword-level features (word length, frequency, and predictability), we model how\neach feature influences dyslexic time costs. We find that all three features\nrobustly change reading times in both typical and dyslexic readers, and that\ndyslexic readers show stronger sensitivities to each, especially\npredictability. Counterfactual manipulations of these features substantially\nnarrow the dyslexic-control gap by about one third, with predictability showing\nthe strongest effect, followed by length and frequency. These patterns align\nwith dyslexia theories that posit heightened demands on linguistic working\nmemory and phonological encoding, and they motivate further work on lexical\ncomplexity and parafoveal preview benefits to explain the remaining gap. In\nshort, we quantify when extra dyslexic costs arise, how large they are, and\noffer actionable guidance for interventions and computational models for\ndyslexics.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5bf9\u5927\u89c4\u6a21\u81ea\u7136\u9605\u8bfb\u6570\u636e\u7684\u773c\u52a8\u8ffd\u8e2a\u5206\u6790\uff0c\u7814\u7a76\u4e86\u9605\u8bfb\u969c\u788d\u8005\u5728\u9605\u8bfb\u4e2d\u53d7\u5230\u7684\u65f6\u95f4\u6210\u672c\u5f71\u54cd\uff0c\u53d1\u73b0\u8bcd\u957f\u3001\u8bcd\u9891\u548c\u9884\u6d4b\u6027\u5747\u80fd\u663e\u8457\u5f71\u54cd\u9605\u8bfb\u65f6\u95f4\uff0c\u4e14\u9605\u8bfb\u969c\u788d\u8005\u5bf9\u8fd9\u4e9b\u7279\u5f81\u5c24\u5176\u654f\u611f\u3002", "motivation": "\u63a2\u7d22\u9605\u8bfb\u969c\u788d\u8005\u9605\u8bfb\u65f6\u95f4\u589e\u52a0\u7684\u5177\u4f53\u6761\u4ef6\u548c\u5f71\u54cd\u56e0\u7d20\uff0c\u4ee5\u7406\u89e3\u9605\u8bfb\u969c\u788d\u4ea7\u751f\u7684\u673a\u5236\u3002", "method": "\u5229\u7528\u773c\u52a8\u8ffd\u8e2a\u6570\u636e\uff0c\u7ed3\u5408\u8bcd\u957f\u3001\u8bcd\u9891\u548c\u9884\u6d4b\u6027\u7279\u5f81\uff0c\u6784\u5efa\u6a21\u578b\u5206\u6790\u8fd9\u4e9b\u7279\u5f81\u5bf9\u9605\u8bfb\u969c\u788d\u8005\u548c\u666e\u901a\u9605\u8bfb\u8005\u9605\u8bfb\u65f6\u95f4\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u6240\u6709\u8bcd\u7ea7\u7279\u5f81\u90fd\u663e\u8457\u5f71\u54cd\u9605\u8bfb\u65f6\u95f4\uff0c\u9605\u8bfb\u969c\u788d\u8005\u5bf9\u8fd9\u4e9b\u7279\u5f81\u7684\u654f\u611f\u5ea6\u66f4\u9ad8\uff0c\u5c24\u5176\u662f\u8bcd\u7684\u9884\u6d4b\u6027\uff1b\u5bf9\u8fd9\u4e9b\u7279\u5f81\u7684\u53cd\u4e8b\u5b9e\u64cd\u63a7\u53ef\u7f29\u5c0f\u9605\u8bfb\u969c\u788d\u8005\u4e0e\u666e\u901a\u8bfb\u8005\u95f4\u7ea6\u4e09\u5206\u4e4b\u4e00\u7684\u65f6\u95f4\u5dee\u8ddd\u3002", "conclusion": "\u8bcd\u7684\u9884\u6d4b\u6027\u3001\u957f\u5ea6\u548c\u9891\u7387\u662f\u89e3\u91ca\u9605\u8bfb\u969c\u788d\u8005\u989d\u5916\u9605\u8bfb\u65f6\u95f4\u6210\u672c\u7684\u91cd\u8981\u56e0\u7d20\uff0c\u7ed3\u679c\u652f\u6301\u4e86\u5bf9\u8bed\u8a00\u5de5\u4f5c\u8bb0\u5fc6\u548c\u97f3\u7d20\u7f16\u7801\u9700\u6c42\u589e\u52a0\u7684\u7406\u8bba\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u548c\u5e72\u9884\u63aa\u65bd\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2510.24652", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.24652", "abs": "https://arxiv.org/abs/2510.24652", "authors": ["Jiawei Zhou", "Lei Chen"], "title": "Optimizing Retrieval for RAG via Reinforced Contrastive Learning", "comment": null, "summary": "As retrieval-augmented generation (RAG) becomes increasingly widespread, the\nrole of information retrieval (IR) is shifting from retrieving information for\nhuman users to retrieving contextual knowledge for artificial intelligence (AI)\nsystems, where relevance becomes difficult to define or annotate beforehand. To\naddress this challenge, we propose R3, a Retrieval framework optimized for RAG\nthrough trialand-feedback Reinforced contrastive learning. Unlike prior\napproaches that rely on annotated or synthetic data for supervised fine-tuning,\nR3 enables the retriever to dynamically explore and optimize relevance within\nthe RAG environment. During training, the retrieved results interact with the\nenvironment to produce contrastive signals that automatically guide the\nretriever's self-improvement. Extensive experiments across diverse tasks\ndemonstrate that R3 improves RAG performance by 5.2% over the original\nretriever and surpasses state-of-the-art retrievers by 4.9%, while achieving\ncomparable results to LLM-augmented retrieval and RAG systems built on\npost-trained or instruction-tuned LLMs. It is both efficient and practical,\nrequiring only 4 GPUs and completing training within a single day.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aR3\u7684\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5bf9\u6bd4\u5b66\u4e60\u4f18\u5316\u68c0\u7d22\u751f\u6210\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u68c0\u7d22\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u968f\u7740\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6280\u672f\u7684\u53d1\u5c55\uff0c\u4f20\u7edf\u7684\u4fe1\u606f\u68c0\u7d22\u4e0d\u518d\u4ec5\u4ec5\u662f\u4e3a\u4eba\u7c7b\u7528\u6237\u68c0\u7d22\u4fe1\u606f\uff0c\u800c\u662f\u4e3aAI\u7cfb\u7edf\u68c0\u7d22\u4e0a\u4e0b\u6587\u77e5\u8bc6\uff0c\u76f8\u5173\u6027\u5b9a\u4e49\u53d8\u5f97\u56f0\u96be\u4e14\u96be\u4ee5\u9884\u5148\u6807\u6ce8\u3002", "method": "\u63d0\u51faR3\u6846\u67b6\uff0c\u901a\u8fc7\u8bd5\u9a8c\u4e0e\u53cd\u9988\u7684\u5f3a\u5316\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f7f\u68c0\u7d22\u5668\u80fd\u5728RAG\u73af\u5883\u4e2d\u52a8\u6001\u63a2\u7d22\u548c\u4f18\u5316\u76f8\u5173\u6027\uff0c\u5229\u7528\u73af\u5883\u4ea4\u4e92\u4ea7\u751f\u5bf9\u6bd4\u4fe1\u53f7\u4fc3\u8fdb\u68c0\u7d22\u5668\u7684\u81ea\u6211\u63d0\u5347\uff0c\u65e0\u9700\u4f9d\u8d56\u6807\u6ce8\u6216\u5408\u6210\u6570\u636e\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\u3002", "result": "\u5728\u591a\u79cd\u4efb\u52a1\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0cR3\u63d0\u5347\u4e865.2%\u7684RAG\u6027\u80fd\uff0c\u9886\u5148\u73b0\u6709\u6700\u5148\u8fdb\u68c0\u7d22\u56684.9%\uff0c\u6027\u80fd\u4e0e\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u540e\u8bad\u7ec3\u6216\u6307\u4ee4\u8c03\u4f18\u7684RAG\u7cfb\u7edf\u76f8\u5f53\u3002", "conclusion": "R3\u6846\u67b6\u9ad8\u6548\u5b9e\u7528\uff0c\u4ec5\u97004\u5757GPU\uff0c\u4e00\u5929\u5185\u5b8c\u6210\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347RAG\u7cfb\u7edf\u68c0\u7d22\u6548\u679c\uff0c\u5177\u6709\u826f\u597d\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2510.24654", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24654", "abs": "https://arxiv.org/abs/2510.24654", "authors": ["Pengcheng Qiu", "Chaoyi Wu", "Junwei Liu", "Qiaoyu Zheng", "Yusheng Liao", "Haowen Wang", "Yun Yue", "Qianrui Fan", "Shuai Zhen", "Jian Wang", "Jinjie Gu", "Yanfeng Wang", "Ya Zhang", "Weidi Xie"], "title": "Evolving Diagnostic Agents in a Virtual Clinical Environment", "comment": null, "summary": "In this paper, we present a framework for training large language models\n(LLMs) as diagnostic agents with reinforcement learning, enabling them to\nmanage multi-turn diagnostic processes, adaptively select examinations, and\ncommit to final diagnoses. Unlike instruction-tuned models trained on static\ncase summaries, our method acquires diagnostic strategies through interactive\nexploration and outcome-based feedback. Our contributions are fourfold: (i) We\npresent DiagGym, a diagnostics world model trained with electronic health\nrecords that emits examination outcomes conditioned on patient history and\nrecommended examination, serving as a virtual clinical environment for\nrealistic diagnosis training and evaluation; (ii) We train DiagAgent via\nend-to-end, multi-turn reinforcement learning to learn diagnostic policies that\noptimize both information yield and diagnostic accuracy; (iii) We introduce\nDiagBench, a diagnostic benchmark comprising 750 cases with physician-validated\nexamination recommendations and 99 cases annotated with 973 physician-written\nrubrics on diagnosis process; (iv) we demonstrate superior performance across\ndiverse diagnostic settings. DiagAgent significantly outperforms 10\nstate-of-the-art LLMs, including DeepSeek-v3 and GPT-4o, as well as two\nprompt-engineered agents. In single-turn settings, DiagAgent achieves 9.34%\nhigher diagnostic accuracy and 44.03% improvement in examination recommendation\nhit ratio. In end-to-end settings, it delivers 15.12% increase in diagnostic\naccuracy and 23.09% boost in examination recommendation F1 score. In\nrubric-based evaluation, it surpasses the next-best model, Claude-sonnet-4, by\n7.1% in weighted rubric score. These findings indicate that learning policies\nin interactive clinical environments confers dynamic and clinically meaningful\ndiagnostic management abilities unattainable through passive training alone.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u8bca\u65ad\u4ee3\u7406\u7684\u6846\u67b6\uff0c\u5b9e\u73b0\u591a\u56de\u5408\u8bca\u65ad\u8fc7\u7a0b\u7ba1\u7406\u548c\u52a8\u6001\u68c0\u67e5\u9009\u62e9\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u591a\u57fa\u4e8e\u9759\u6001\u75c5\u4f8b\u603b\u7ed3\u8fdb\u884c\u8bad\u7ec3\uff0c\u7f3a\u4e4f\u52a8\u6001\u4ea4\u4e92\u8bca\u65ad\u7b56\u7565\u7684\u5b66\u4e60\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u5957\u6574\u4f53\u65b9\u6848\uff0c\u5305\u62ec\u8bad\u7ec3\u865a\u62df\u8bca\u65ad\u73af\u5883DiagGym\uff0c\u91c7\u7528\u591a\u56de\u5408\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8bca\u65ad\u4ee3\u7406DiagAgent\uff0c\u6784\u5efa\u8bca\u65ad\u57fa\u51c6DiagBench\uff0c\u5e76\u8fdb\u884c\u591a\u7ef4\u5ea6\u6027\u80fd\u8bc4\u4f30\u3002", "result": "DiagAgent\u5728\u8bca\u65ad\u51c6\u786e\u7387\u3001\u68c0\u67e5\u63a8\u8350\u547d\u4e2d\u7387\u3001F1\u5206\u6570\u53ca\u57fa\u4e8e\u533b\u751f\u8bc4\u5206\u7684\u6307\u6807\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u591a\u6b3e\u5148\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5305\u62ecGPT-4o\u548cClaude-sonnet-4\u3002", "conclusion": "\u901a\u8fc7\u5728\u4ea4\u4e92\u5f0f\u4e34\u5e8a\u73af\u5883\u4e2d\u5b66\u4e60\u8bca\u65ad\u7b56\u7565\uff0c\u6a21\u578b\u5177\u5907\u4e86\u52a8\u6001\u4e14\u5177\u6709\u4e34\u5e8a\u610f\u4e49\u7684\u8bca\u65ad\u7ba1\u7406\u80fd\u529b\uff0c\u8d85\u8d8a\u4e86\u88ab\u52a8\u8bad\u7ec3\u65b9\u6cd5\u7684\u5c40\u9650\u3002"}}
{"id": "2510.24664", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24664", "abs": "https://arxiv.org/abs/2510.24664", "authors": ["Parker Riley", "Daniel Deutsch", "Mara Finkelstein", "Colten DiIanni", "Juraj Juraska", "Markus Freitag"], "title": "MQM Re-Annotation: A Technique for Collaborative Evaluation of Machine Translation", "comment": null, "summary": "Human evaluation of machine translation is in an arms race with translation\nmodel quality: as our models get better, our evaluation methods need to be\nimproved to ensure that quality gains are not lost in evaluation noise. To this\nend, we experiment with a two-stage version of the current state-of-the-art\ntranslation evaluation paradigm (MQM), which we call MQM re-annotation. In this\nsetup, an MQM annotator reviews and edits a set of pre-existing MQM\nannotations, that may have come from themselves, another human annotator, or an\nautomatic MQM annotation system. We demonstrate that rater behavior in\nre-annotation aligns with our goals, and that re-annotation results in\nhigher-quality annotations, mostly due to finding errors that were missed\nduring the first pass.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MQM\u91cd\u6807\u6ce8\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u5df2\u6709\u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u7684\u6ce8\u91ca\u8fdb\u884c\u5ba1\u67e5\u548c\u7f16\u8f91\uff0c\u63d0\u5347\u4e86\u8bc4\u4f30\u6ce8\u91ca\u7684\u8d28\u91cf\u3002", "motivation": "\u968f\u7740\u7ffb\u8bd1\u6a21\u578b\u8d28\u91cf\u63d0\u5347\uff0c\u73b0\u6709\u7684\u4eba\u7c7b\u7ffb\u8bd1\u8bc4\u4f30\u65b9\u6cd5\u9700\u8981\u6539\u8fdb\u4ee5\u51cf\u5c11\u8bc4\u4f30\u566a\u58f0\uff0c\u786e\u4fdd\u8d28\u91cf\u63d0\u5347\u88ab\u51c6\u786e\u53cd\u6620\u3002", "method": "\u5b9e\u9a8c\u91c7\u7528\u4e24\u9636\u6bb5\u7684MQM\u91cd\u6807\u6ce8\u65b9\u6cd5\uff0c\u5373\u8ba9\u8bc4\u4f30\u8005\u91cd\u65b0\u5ba1\u67e5\u5e76\u4fee\u6b63\u5df2\u6709\u7684MQM\u6ce8\u91ca\uff0c\u8fd9\u4e9b\u6ce8\u91ca\u53ef\u80fd\u6765\u81ea\u4e0d\u540c\u6765\u6e90\uff0c\u5305\u62ec\u81ea\u52a8\u7cfb\u7edf\u6216\u5176\u4ed6\u8bc4\u4f30\u8005\u3002", "result": "\u91cd\u65b0\u6807\u6ce8\u540e\u8bc4\u4f30\u8005\u884c\u4e3a\u7b26\u5408\u9884\u671f\uff0c\u4e14\u6ce8\u91ca\u8d28\u91cf\u663e\u8457\u63d0\u5347\uff0c\u4e3b\u8981\u662f\u7ea0\u6b63\u4e86\u7b2c\u4e00\u8f6e\u8bc4\u4f30\u4e2d\u9057\u6f0f\u7684\u9519\u8bef\u3002", "conclusion": "MQM\u91cd\u6807\u6ce8\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u673a\u5668\u7ffb\u8bd1\u8bc4\u4f30\u7684\u51c6\u786e\u6027\uff0c\u6709\u52a9\u4e8e\u66f4\u7cbe\u786e\u5730\u53cd\u6620\u7ffb\u8bd1\u6a21\u578b\u8d28\u91cf\u7684\u63d0\u5347\u3002"}}
{"id": "2510.24668", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24668", "abs": "https://arxiv.org/abs/2510.24668", "authors": ["Mingyi Deng", "Lijun Huang", "Yani Fan", "Jiayi Zhang", "Fashen Ren", "Jinyi Bai", "Fuzhen Yang", "Dayi Miao", "Zhaoyang Yu", "Yifan Wu", "Yanfei Zhang", "Fengwei Teng", "Yingjia Wan", "Song Hu", "Yude Li", "Xin Jin", "Conghao Hu", "Haoyu Li", "Qirui Fu", "Tai Zhong", "Xinyu Wang", "Xiangru Tang", "Nan Tang", "Chenglin Wu", "Yuyu Luo"], "title": "InteractComp: Evaluating Search Agents With Ambiguous Queries", "comment": null, "summary": "Language agents have demonstrated remarkable potential in web search and\ninformation retrieval. However, these search agents assume user queries are\ncomplete and unambiguous, an assumption that diverges from reality where users\nbegin with incomplete queries requiring clarification through interaction. Yet\nmost agents lack interactive mechanisms during the search process, and existing\nbenchmarks cannot assess this capability. To address this gap, we introduce\nInteractComp, a benchmark designed to evaluate whether search agents can\nrecognize query ambiguity and actively interact to resolve it during search.\nFollowing the principle of easy to verify, interact to disambiguate, we\nconstruct 210 expert-curated questions across 9 domains through a\ntarget-distractor methodology that creates genuine ambiguity resolvable only\nthrough interaction. Evaluation of 17 models reveals striking failure: the best\nmodel achieves only 13.73% accuracy despite 71.50% with complete context,\nexposing systematic overconfidence rather than reasoning deficits. Forced\ninteraction produces dramatic gains, demonstrating latent capability current\nstrategies fail to engage. Longitudinal analysis shows interaction capabilities\nstagnated over 15 months while search performance improved seven-fold,\nrevealing a critical blind spot. This stagnation, coupled with the immediate\nfeedback inherent to search tasks, makes InteractComp a valuable resource for\nboth evaluating and training interaction capabilities in search agents. The\ncode is available at https://github.com/FoundationAgents/InteractComp.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86InteractComp\u57fa\u51c6\uff0c\u8bc4\u4f30\u641c\u7d22\u4ee3\u7406\u8bc6\u522b\u67e5\u8be2\u6b67\u4e49\u5e76\u901a\u8fc7\u4ea4\u4e92\u89e3\u51b3\u7684\u80fd\u529b\uff0c\u63ed\u793a\u73b0\u6709\u6a21\u578b\u5728\u4ea4\u4e92\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u641c\u7d22\u4ee3\u7406\u5047\u8bbe\u7528\u6237\u67e5\u8be2\u5b8c\u6574\u4e14\u65e0\u6b67\u4e49\uff0c\u73b0\u5b9e\u4e2d\u7528\u6237\u67e5\u8be2\u5f80\u5f80\u4e0d\u5b8c\u6574\u4e14\u9700\u4ea4\u4e92\u6f84\u6e05\uff0c\u4f46\u5927\u591a\u6570\u4ee3\u7406\u7f3a\u4e4f\u4ea4\u4e92\u673a\u5236\uff0c\u4e14\u65e0\u76f8\u5e94\u57fa\u51c6\u8fdb\u884c\u8bc4\u4f30\u3002", "method": "\u6784\u5efa\u4e86210\u4e2a\u8de89\u4e2a\u9886\u57df\u7684\u4e13\u5bb6\u8bbe\u8ba1\u95ee\u9898\uff0c\u95ee\u9898\u8bbe\u8ba1\u5305\u542b\u771f\u6b63\u7684\u6b67\u4e49\uff0c\u4ec5\u901a\u8fc7\u4ea4\u4e92\u624d\u80fd\u89e3\u51b3\uff0c\u5e76\u752817\u4e2a\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\uff0c\u540c\u65f6\u5206\u6790\u6a21\u578b\u7684\u8868\u73b0\u548c\u4ea4\u4e92\u80fd\u529b\u53d1\u5c55\u3002", "result": "\u6700\u4f73\u6a21\u578b\u4ec513.73%\u51c6\u786e\u7387\uff08\u5b8c\u5168\u4e0a\u4e0b\u658771.5%\uff09\uff0c\u8868\u660e\u6a21\u578b\u8fc7\u4e8e\u81ea\u4fe1\u4e14\u7f3a\u4e4f\u4ea4\u4e92\u5229\u7528\u80fd\u529b\uff0c\u5f3a\u5236\u4ea4\u4e92\u663e\u8457\u63d0\u5347\u8868\u73b0\uff0c\u4ea4\u4e92\u80fd\u529b15\u4e2a\u6708\u672a\u6539\u8fdb\u800c\u641c\u7d22\u6027\u80fd\u63d0\u53477\u500d\u3002", "conclusion": "InteractComp\u63ed\u793a\u4e86\u8bed\u8a00\u641c\u7d22\u4ee3\u7406\u5728\u4ea4\u4e92\u8bc6\u522b\u4e0e\u89e3\u51b3\u6b67\u4e49\u65b9\u9762\u7684\u4e25\u91cd\u77ed\u677f\uff0c\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u7528\u4e8e\u8bc4\u4f30\u4e0e\u8bad\u7ec3\u641c\u7d22\u4ee3\u7406\u7684\u4ea4\u4e92\u80fd\u529b\u3002"}}
{"id": "2510.24677", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24677", "abs": "https://arxiv.org/abs/2510.24677", "authors": ["Xun Liang", "Huayi Lai", "Hanyu Wang", "Wentao Zhang", "Linfeng Zhang", "Yanfang Chen", "Feiyu Xiong", "Zhiyu Li"], "title": "Dissecting Role Cognition in Medical LLMs via Neuronal Ablation", "comment": "15 pages, 9 figures", "summary": "Large language models (LLMs) have gained significant traction in medical\ndecision support systems, particularly in the\n  context of medical question answering and role-playing simulations. A common\npractice, Prompt-Based Role Playing (PBRP),\n  instructs models to adopt different clinical roles (e.g., medical students,\nresidents, attending physicians) to simulate varied\n  professional behaviors. However, the impact of such role prompts on model\nreasoning capabilities remains unclear. This\n  study introduces the RP-Neuron-Activated Evaluation Framework(RPNA) to\nevaluate whether role prompts induce distinct,\n  role-specific cognitive processes in LLMs or merely modify linguistic style.\nWe test this framework on three medical QA\n  datasets, employing neuron ablation and representation analysis techniques to\nassess changes in reasoning pathways. Our\n  results demonstrate that role prompts do not significantly enhance the\nmedical reasoning abilities of LLMs. Instead, they\n  primarily affect surface-level linguistic features, with no evidence of\ndistinct reasoning pathways or cognitive differentiation\n  across clinical roles. Despite superficial stylistic changes, the core\ndecision-making mechanisms of LLMs remain uniform\n  across roles, indicating that current PBRP methods fail to replicate the\ncognitive complexity found in real-world medical\n  practice. This highlights the limitations of role-playing in medical AI and\nemphasizes the need for models that simulate genuine\n  cognitive processes rather than linguistic imitation.We have released the\nrelated code in the following repository:https:\n  //github.com/IAAR-Shanghai/RolePlay_LLMDoctor", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u89d2\u8272\u626e\u6f14\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u89d2\u8272\u63d0\u793a\u4e3b\u8981\u5f71\u54cd\u8bed\u8a00\u98ce\u683c\uff0c\u800c\u975e\u63a8\u7406\u673a\u5236\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u89d2\u8272\u626e\u6f14\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u5176\u89d2\u8272\u63d0\u793a\u662f\u5426\u771f\u6b63\u5f71\u54cd\u6a21\u578b\u63a8\u7406\u80fd\u529b\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u63d0\u51faRP-Neuron-Activated\u8bc4\u4f30\u6846\u67b6\uff0c\u7ed3\u5408\u795e\u7ecf\u5143\u5207\u9664\u548c\u8868\u793a\u5206\u6790\uff0c\u8bc4\u4f30\u533b\u7597QA\u6570\u636e\u96c6\u4e2d\u4e0d\u540c\u4e34\u5e8a\u89d2\u8272\u63d0\u793a\u5bf9\u63a8\u7406\u8def\u5f84\u7684\u5f71\u54cd\u3002", "result": "\u89d2\u8272\u63d0\u793a\u672a\u663e\u8457\u63d0\u5347\u6a21\u578b\u7684\u533b\u7597\u63a8\u7406\u80fd\u529b\uff0c\u4ec5\u6539\u53d8\u4e86\u8868\u9762\u8bed\u8a00\u7279\u5f81\uff0c\u672a\u53d1\u73b0\u4e0d\u540c\u89d2\u8272\u4e4b\u95f4\u7684\u8ba4\u77e5\u5dee\u5f02\u6216\u63a8\u7406\u8def\u5f84\u53d8\u5316\u3002", "conclusion": "\u5f53\u524d\u57fa\u4e8e\u89d2\u8272\u7684\u89d2\u8272\u626e\u6f14\u65b9\u6cd5\u672a\u80fd\u6a21\u62df\u771f\u5b9e\u533b\u7597\u8ba4\u77e5\u590d\u6742\u6027\uff0c\u63d0\u793a\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6a21\u62df\u771f\u5b9e\u8ba4\u77e5\u8fc7\u7a0b\u7684\u65b0\u6a21\u578b\u3002"}}
{"id": "2510.24684", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24684", "abs": "https://arxiv.org/abs/2510.24684", "authors": ["Bo Liu", "Chuanyang Jin", "Seungone Kim", "Weizhe Yuan", "Wenting Zhao", "Ilia Kulikov", "Xian Li", "Sainbayar Sukhbaatar", "Jack Lanchantin", "Jason Weston"], "title": "SPICE: Self-Play In Corpus Environments Improves Reasoning", "comment": null, "summary": "Self-improving systems require environmental interaction for continuous\nadaptation. We introduce SPICE (Self-Play In Corpus Environments), a\nreinforcement learning framework where a single model acts in two roles: a\nChallenger that mines documents from a large corpus to generate diverse\nreasoning tasks, and a Reasoner that solves them. Through adversarial dynamics,\nthe Challenger creates an automatic curriculum at the frontier of the\nReasoner's capability, while corpus grounding provides the rich,\nnear-inexhaustible external signal necessary for sustained improvement. Unlike\nexisting ungrounded self-play methods that offer more limited benefits, SPICE\nachieves consistent gains across mathematical (+8.9%) and general reasoning\n(+9.8%) benchmarks on multiple model families. Our analysis reveals how\ndocument grounding is a key ingredient in SPICE to continuously generate its\nown increasingly challenging goals and achieve them, enabling sustained\nself-improvement.", "AI": {"tldr": "SPICE\u662f\u4e00\u79cd\u57fa\u4e8e\u8bed\u6599\u5e93\u73af\u5883\u7684\u81ea\u6211\u5bf9\u6297\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6587\u6863\u6316\u6398\u751f\u6210\u591a\u6837\u7684\u63a8\u7406\u4efb\u52a1\uff0c\u5b9e\u73b0\u6a21\u578b\u7684\u6301\u7eed\u81ea\u6211\u63d0\u5347\u3002", "motivation": "\u81ea\u6211\u63d0\u5347\u7cfb\u7edf\u9700\u8981\u4e0d\u65ad\u4e0e\u73af\u5883\u4ea4\u4e92\u4ee5\u5b9e\u73b0\u6301\u7eed\u9002\u5e94\uff0c\u73b0\u6709\u65e0\u73af\u5883\u57fa\u7840\u7684\u81ea\u6211\u5bf9\u6297\u65b9\u6cd5\u6548\u679c\u6709\u9650\u3002", "method": "\u63d0\u51faSPICE\u6846\u67b6\uff0c\u6a21\u578b\u626e\u6f14\u6311\u6218\u8005\u548c\u63a8\u7406\u8005\u4e24\u4e2a\u89d2\u8272\uff1a\u6311\u6218\u8005\u4ece\u5927\u89c4\u6a21\u8bed\u6599\u5e93\u4e2d\u6316\u6398\u6587\u6863\u751f\u6210\u591a\u6837\u63a8\u7406\u4efb\u52a1\uff0c\u63a8\u7406\u8005\u8fdb\u884c\u89e3\u7b54\uff0c\u901a\u8fc7\u5bf9\u6297\u52a8\u6001\u81ea\u52a8\u751f\u6210\u9002\u5408\u63a8\u7406\u8005\u80fd\u529b\u8fb9\u754c\u7684\u8bfe\u7a0b\u3002", "result": "SPICE\u5728\u6570\u5b66\u548c\u901a\u7528\u63a8\u7406\u57fa\u51c6\u4e0a\u5206\u522b\u63d0\u5347\u4e868.9%\u548c9.8%\uff0c\u8d85\u8fc7\u73b0\u6709\u65e0\u57fa\u73af\u5883\u7684\u81ea\u6211\u5bf9\u6297\u65b9\u6cd5\u3002", "conclusion": "\u8bed\u6599\u5e93\u57fa\u7840\u6210\u4e3a\u5173\u952e\uff0c\u4f7fSPICE\u80fd\u6301\u7eed\u751f\u6210\u5e76\u8fbe\u5230\u8d8a\u6765\u8d8a\u5177\u6311\u6218\u6027\u7684\u76ee\u6807\uff0c\u4fc3\u8fdb\u6a21\u578b\u6301\u7eed\u81ea\u6211\u63d0\u5347\u3002"}}
{"id": "2510.24694", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24694", "abs": "https://arxiv.org/abs/2510.24694", "authors": ["Yida Zhao", "Kuan Li", "Xixi Wu", "Liwen Zhang", "Dingchu Zhang", "Baixuan Li", "Maojia Song", "Zhuo Chen", "Chenxi Wang", "Xinyu Wang", "Kewei Tu", "Pengjun Xie", "Jingren Zhou", "Yong Jiang"], "title": "Repurposing Synthetic Data for Fine-grained Search Agent Supervision", "comment": null, "summary": "LLM-based search agents are increasingly trained on entity-centric synthetic\ndata to solve complex, knowledge-intensive tasks. However, prevailing training\nmethods like Group Relative Policy Optimization (GRPO) discard this rich entity\ninformation, relying instead on sparse, outcome-based rewards. This critical\nlimitation renders them unable to distinguish informative \"near-miss\"\nsamples-those with substantially correct reasoning but a flawed final\nanswer-from complete failures, thus discarding valuable learning signals. We\naddress this by leveraging the very entities discarded during training. Our\nempirical analysis reveals a strong positive correlation between the number of\nground-truth entities identified during an agent's reasoning process and final\nanswer accuracy. Building on this insight, we introduce Entity-aware Group\nRelative Policy Optimization (E-GRPO), a novel framework that formulates a\ndense entity-aware reward function. E-GRPO assigns partial rewards to incorrect\nsamples proportional to their entity match rate, enabling the model to\neffectively learn from these \"near-misses\". Experiments on diverse\nquestion-answering (QA) and deep research benchmarks show that E-GRPO\nconsistently and significantly outperforms the GRPO baseline. Furthermore, our\nanalysis reveals that E-GRPO not only achieves superior accuracy but also\ninduces more efficient reasoning policies that require fewer tool calls,\ndemonstrating a more effective and sample-efficient approach to aligning search\nagents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b9e\u4f53\u4fe1\u606f\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u65b9\u6cd5E-GRPO\uff0c\u901a\u8fc7\u5bf9\u90e8\u5206\u6b63\u786e\u7684\u201c\u8fd1\u4e4e\u6210\u529f\u201d\u6837\u672c\u8d4b\u4e88\u5956\u52b1\uff0c\u63d0\u5347\u4e86LLM\u641c\u7d22\u4ee3\u7406\u5728\u77e5\u8bc6\u5bc6\u96c6\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u8bad\u7ec3\u65b9\u6cd5GRPO\u5ffd\u7565\u4e86\u4e30\u5bcc\u7684\u5b9e\u4f53\u4fe1\u606f\uff0c\u5bfc\u81f4\u65e0\u6cd5\u533a\u5206\u6709\u4ef7\u503c\u7684\u201c\u8fd1\u4e4e\u6210\u529f\u201d\u6837\u672c\u548c\u5931\u8d25\u6837\u672c\uff0c\u4e22\u5931\u4e86\u91cd\u8981\u7684\u5b66\u4e60\u4fe1\u53f7\u3002", "method": "\u63d0\u51fa\u4e86\u5b9e\u4f53\u611f\u77e5\u7684Group Relative Policy Optimization\uff08E-GRPO\uff09\uff0c\u5f15\u5165\u57fa\u4e8e\u5b9e\u4f53\u5339\u914d\u7387\u7684\u7a20\u5bc6\u5956\u52b1\u51fd\u6570\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u4ece\u201c\u8fd1\u4e4e\u6210\u529f\u201d\u7684\u9519\u8bef\u6837\u672c\u4e2d\u6709\u6548\u5b66\u4e60\u3002", "result": "\u5728\u591a\u79cd\u95ee\u7b54\u548c\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cE-GRPO\u663e\u8457\u4f18\u4e8eGRPO\uff0c\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u7387\u548c\u66f4\u9ad8\u6548\u7684\u63a8\u7406\u7b56\u7565\uff0c\u51cf\u5c11\u4e86\u5de5\u5177\u8c03\u7528\u6b21\u6570\u3002", "conclusion": "\u5229\u7528\u5b9e\u4f53\u4fe1\u606f\u7684\u7a20\u5bc6\u5956\u52b1\u51fd\u6570\u80fd\u591f\u6539\u5584\u641c\u7d22\u4ee3\u7406\u7684\u8bad\u7ec3\u6548\u679c\uff0c\u63d0\u9ad8\u5176\u51c6\u786e\u7387\u548c\u63a8\u7406\u6548\u7387\uff0c\u662f\u4e00\u79cd\u66f4\u6709\u6548\u4e14\u6837\u672c\u9ad8\u6548\u7684\u5bf9\u9f50\u65b9\u6cd5\u3002"}}
{"id": "2510.24695", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24695", "abs": "https://arxiv.org/abs/2510.24695", "authors": ["Xuanzhong Chen", "Zile Qiao", "Guoxin Chen", "Liangcai Su", "Zhen Zhang", "Xinyu Wang", "Pengjun Xie", "Fei Huang", "Jingren Zhou", "Yong Jiang"], "title": "AgentFrontier: Expanding the Capability Frontier of LLM Agents with ZPD-Guided Data Synthesis", "comment": "https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/", "summary": "Training large language model agents on tasks at the frontier of their\ncapabilities is key to unlocking advanced reasoning. We introduce a data\nsynthesis approach inspired by the educational theory of the Zone of Proximal\nDevelopment (ZPD), which defines this frontier as tasks an LLM cannot solve\nalone but can master with guidance. To operationalize this, we present the\nAgentFrontier Engine, an automated pipeline that synthesizes high-quality,\nmultidisciplinary data situated precisely within the LLM's ZPD. This engine\nsupports both continued pre-training with knowledge-intensive data and targeted\npost-training on complex reasoning tasks. From the same framework, we derive\nthe ZPD Exam, a dynamic and automated benchmark designed to evaluate agent\ncapabilities on these frontier tasks. We train AgentFrontier-30B-A3B model on\nour synthesized data, which achieves state-of-the-art results on demanding\nbenchmarks like Humanity's Last Exam, even surpassing some leading proprietary\nagents. Our work demonstrates that a ZPD-guided approach to data synthesis\noffers a scalable and effective path toward building more capable LLM agents.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u201c\u6700\u8fd1\u53d1\u5c55\u533a\u201d(ZPD)\u6559\u80b2\u7406\u8bba\u7684\u6570\u636e\u5408\u6210\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u7684\u9ad8\u7ea7\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\u6709\u9650\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u5b9a\u4f4d\u4efb\u52a1\u96be\u5ea6\u4e8e\u6a21\u578b\u7684\u201c\u6700\u8fd1\u53d1\u5c55\u533a\u201d\u6765\u7a81\u7834\u80fd\u529b\u8fb9\u754c\u3002", "method": "\u8bbe\u8ba1\u4e86AgentFrontier\u5f15\u64ce\uff0c\u81ea\u52a8\u5408\u6210\u4f4d\u4e8eLLM\u6700\u8fd1\u53d1\u5c55\u533a\u7684\u9ad8\u8d28\u91cf\u8de8\u5b66\u79d1\u6570\u636e\uff0c\u652f\u6301\u6301\u7eed\u9884\u8bad\u7ec3\u548c\u9488\u5bf9\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u540e\u8bad\u7ec3\uff0c\u540c\u65f6\u57fa\u4e8e\u6b64\u8bbe\u8ba1\u4e86ZPD\u8003\u8bd5\u8bc4\u6d4b\u6a21\u578b\u80fd\u529b\u3002", "result": "\u8bad\u7ec3\u7684AgentFrontier-30B-A3B\u6a21\u578b\u5728\u591a\u4e2a\u9ad8\u96be\u5ea6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u751a\u81f3\u8d85\u8fc7\u90e8\u5206\u9886\u5148\u7684\u4e13\u6709\u6a21\u578b\u3002", "conclusion": "ZPD\u6307\u5bfc\u4e0b\u7684\u6570\u636e\u5408\u6210\u65b9\u6cd5\u4e3a\u6784\u5efa\u66f4\u5f3a\u5927\u7684LLM\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u8def\u5f84\u3002"}}
{"id": "2510.24697", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24697", "abs": "https://arxiv.org/abs/2510.24697", "authors": ["Zhengwei Tao", "Haiyang Shen", "Baixuan Li", "Wenbiao Yin", "Jialong Wu", "Kuan Li", "Zhongwang Zhang", "Huifeng Yin", "Rui Ye", "Liwen Zhang", "Xinyu Wang", "Pengjun Xie", "Jingren Zhou", "Yong Jiang"], "title": "WebLeaper: Empowering Efficiency and Efficacy in WebAgent via Enabling Info-Rich Seeking", "comment": null, "summary": "Large Language Model (LLM)-based agents have emerged as a transformative\napproach for open-ended problem solving, with information seeking (IS) being a\ncore capability that enables autonomous reasoning and decision-making. While\nprior research has largely focused on improving retrieval depth, we observe\nthat current IS agents often suffer from low search efficiency, which in turn\nconstrains overall performance. A key factor underlying this inefficiency is\nthe sparsity of target entities in training tasks, which limits opportunities\nfor agents to learn and generalize efficient search behaviors. To address these\nchallenges, we propose WebLeaper, a framework for constructing high-coverage IS\ntasks and generating efficient solution trajectories. We formulate IS as a\ntree-structured reasoning problem, enabling a substantially larger set of\ntarget entities to be embedded within a constrained context. Leveraging curated\nWikipedia tables, we propose three variants for synthesizing IS tasks, Basic,\nUnion, and Reverse-Union, to systematically increase both IS efficiency and\nefficacy. Finally, we curate training trajectories by retaining only those that\nare simultaneously accurate and efficient, ensuring that the model is optimized\nfor both correctness and search performance. Extensive experiments on both\nbasic and comprehensive settings, conducted on five IS benchmarks, BrowserComp,\nGAIA, xbench-DeepSearch, WideSearch, and Seal-0, demonstrate that our method\nconsistently achieves improvements in both effectiveness and efficiency over\nstrong baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86WebLeaper\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u9ad8\u8986\u76d6\u7387\u7684\u4fe1\u606f\u641c\u7d22\u4efb\u52a1\u548c\u751f\u6210\u9ad8\u6548\u641c\u7d22\u8f68\u8ff9\uff0c\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5728\u4fe1\u606f\u641c\u7d22\u4e2d\u7684\u6548\u7387\u548c\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u4fe1\u606f\u641c\u7d22\u4ee3\u7406\u6548\u7387\u4f4e\uff0c\u4e3b\u8981\u56e0\u8bad\u7ec3\u4efb\u52a1\u4e2d\u76ee\u6807\u5b9e\u4f53\u7a00\u758f\uff0c\u9650\u5236\u4e86\u5b66\u4e60\u9ad8\u6548\u641c\u7d22\u884c\u4e3a\u7684\u673a\u4f1a\u3002", "method": "\u5c06\u4fe1\u606f\u641c\u7d22\u5efa\u6a21\u4e3a\u6811\u7ed3\u6784\u63a8\u7406\u95ee\u9898\uff0c\u5229\u7528\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u7ef4\u57fa\u767e\u79d1\u8868\u683c\u5408\u6210\u4e09\u79cd\u4efb\u52a1\u53d8\u4f53\uff08Basic\u3001Union\u548cReverse-Union\uff09\uff0c\u5e76\u7b5b\u9009\u51c6\u786e\u9ad8\u6548\u7684\u8bad\u7ec3\u8f68\u8ff9\u4ee5\u4f18\u5316\u6a21\u578b\u8868\u73b0\u3002", "result": "\u5728\u4e94\u4e2a\u4fe1\u606f\u641c\u7d22\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cWebLeaper\u5728\u641c\u7d22\u6548\u7387\u548c\u6548\u679c\u4e0a\u5747\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u4efb\u52a1\u8bbe\u8ba1\u548c\u8bad\u7ec3\u8f68\u8ff9\u7b5b\u9009\u7b56\u7565\uff0cWebLeaper\u663e\u8457\u63d0\u5347\u4e86\u4fe1\u606f\u641c\u7d22\u4ee3\u7406\u7684\u6574\u4f53\u6027\u80fd\uff0c\u63a8\u52a8\u4e86\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5f00\u653e\u5f0f\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3002"}}
{"id": "2510.24698", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24698", "abs": "https://arxiv.org/abs/2510.24698", "authors": ["Baixuan Li", "Dingchu Zhang", "Jialong Wu", "Wenbiao Yin", "Zhengwei Tao", "Yida Zhao", "Liwen Zhang", "Haiyang Shen", "Runnan Fang", "Pengjun Xie", "Jingren Zhou", "Yong Jiang"], "title": "ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking", "comment": null, "summary": "Parallel thinking expands exploration breadth, complementing the deep\nexploration of information-seeking (IS) agents to further enhance\nproblem-solving capability. However, conventional parallel thinking faces two\nkey challenges in this setting: inefficiency from repeatedly rolling out from\nscratch, and difficulty in integrating long-horizon reasoning trajectories\nduring answer generation, as limited context capacity prevents full\nconsideration of the reasoning process. To address these issues, we propose\nParallelMuse, a two-stage paradigm designed for deep IS agents. The first\nstage, Functionality-Specified Partial Rollout, partitions generated sequences\ninto functional regions and performs uncertainty-guided path reuse and\nbranching to enhance exploration efficiency. The second stage, Compressed\nReasoning Aggregation, exploits reasoning redundancy to losslessly compress\ninformation relevant to answer derivation and synthesize a coherent final\nanswer. Experiments across multiple open-source agents and benchmarks\ndemonstrate up to 62% performance improvement with a 10--30% reduction in\nexploratory token consumption.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faParallelMuse\uff0c\u901a\u8fc7\u5e76\u884c\u601d\u7ef4\u6269\u5c55\u4fe1\u606f\u641c\u7d22\u4ee3\u7406\u7684\u63a2\u7d22\u6df1\u5ea6\u548c\u5e7f\u5ea6\uff0c\u63d0\u5347\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u5e76\u884c\u601d\u7ef4\u5728\u4fe1\u606f\u641c\u7d22\u4ee3\u7406\u4e2d\u5b58\u5728\u542f\u52a8\u6548\u7387\u4f4e\u4e0b\u548c\u957f\u65f6\u7a0b\u63a8\u7406\u96be\u4ee5\u6574\u5408\u4e24\u5927\u96be\u9898\uff0c\u9650\u5236\u4e86\u95ee\u9898\u89e3\u51b3\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u7b2c\u4e00\u9636\u6bb5\u90e8\u5206\u5c55\u5f00\u529f\u80fd\u533a\u5206\u5e76\u590d\u7528\u8def\u5f84\u63d0\u5347\u6548\u7387\uff0c\u7b2c\u4e8c\u9636\u6bb5\u65e0\u635f\u538b\u7f29\u63a8\u7406\u4fe1\u606f\u7efc\u5408\u7b54\u6848\u3002", "result": "\u591a\u9879\u5f00\u6e90\u4ee3\u7406\u548c\u57fa\u51c6\u6d4b\u8bd5\u8868\u660eParallelMuse\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe62%\uff0c\u540c\u65f6\u63a2\u7d22\u4ee4\u724c\u6d88\u8017\u964d\u4f4e10-30%\u3002", "conclusion": "ParallelMuse\u6709\u6548\u63d0\u5347\u4fe1\u606f\u641c\u7d22\u4ee3\u7406\u7684\u5e76\u884c\u63a2\u7d22\u6548\u7387\u548c\u63a8\u7406\u6574\u5408\u80fd\u529b\uff0c\u663e\u8457\u589e\u5f3a\u95ee\u9898\u89e3\u51b3\u8868\u73b0\u3002"}}
{"id": "2510.24699", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24699", "abs": "https://arxiv.org/abs/2510.24699", "authors": ["Rui Ye", "Zhongwang Zhang", "Kuan Li", "Huifeng Yin", "Zhengwei Tao", "Yida Zhao", "Liangcai Su", "Liwen Zhang", "Zile Qiao", "Xinyu Wang", "Pengjun Xie", "Fei Huang", "Siheng Chen", "Jingren Zhou", "Yong Jiang"], "title": "AgentFold: Long-Horizon Web Agents with Proactive Context Management", "comment": "26 pages, 9 figures", "summary": "LLM-based web agents show immense promise for information seeking, yet their\neffectiveness on long-horizon tasks is hindered by a fundamental trade-off in\ncontext management. Prevailing ReAct-based agents suffer from context\nsaturation as they accumulate noisy, raw histories, while methods that fixedly\nsummarize the full history at each step risk the irreversible loss of critical\ndetails. Addressing these, we introduce AgentFold, a novel agent paradigm\ncentered on proactive context management, inspired by the human cognitive\nprocess of retrospective consolidation. AgentFold treats its context as a\ndynamic cognitive workspace to be actively sculpted, rather than a passive log\nto be filled. At each step, it learns to execute a `folding' operation, which\nmanages its historical trajectory at multiple scales: it can perform granular\ncondensations to preserve vital, fine-grained details, or deep consolidations\nto abstract away entire multi-step sub-tasks. The results on prominent\nbenchmarks are striking: with simple supervised fine-tuning (without continual\npre-training or RL), our AgentFold-30B-A3B agent achieves 36.2% on BrowseComp\nand 47.3% on BrowseComp-ZH. Notably, this performance not only surpasses or\nmatches open-source models of a dramatically larger scale, such as the\nDeepSeek-V3.1-671B-A37B, but also surpasses leading proprietary agents like\nOpenAI's o4-mini.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86AgentFold\uff0c\u4e00\u79cd\u57fa\u4e8e\u4e3b\u52a8\u4e0a\u4e0b\u6587\u7ba1\u7406\u7684\u65b0\u578b\u5927\u8bed\u8a00\u6a21\u578b(Large Language Model, LLM)\u7f51\u9875\u4ee3\u7406\u3002\u5b83\u901a\u8fc7\u52a8\u6001\u6298\u53e0\u5386\u53f2\u4fe1\u606f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u4efb\u52a1\u4e2d\u4e0a\u4e0b\u6587\u9971\u548c\u4e0e\u5173\u952e\u4fe1\u606f\u4e22\u5931\u7684\u77db\u76fe\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eReAct\u7684\u7f51\u9875\u4ee3\u7406\u5728\u6267\u884c\u957f\u65f6\u95f4\u8de8\u5ea6\u4efb\u52a1\u65f6\uff0c\u9762\u4e34\u4e0a\u4e0b\u6587\u4fe1\u606f\u8fc7\u8f7d\u548c\u5173\u952e\u4fe1\u606f\u4e22\u5931\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u524a\u5f31\u4e86\u5176\u4efb\u52a1\u6548\u679c\u3002", "method": "AgentFold\u5f15\u5165\u4e3b\u52a8\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u7b56\u7565\uff0c\u6a21\u62df\u4eba\u7c7b\u7684\u9006\u5411\u6574\u5408\u8ba4\u77e5\u8fc7\u7a0b\uff0c\u5728\u6bcf\u4e00\u6b65\u901a\u8fc7\u6267\u884c\u201c\u6298\u53e0\u201d\u64cd\u4f5c\u591a\u5c3a\u5ea6\u5730\u7ba1\u7406\u5386\u53f2\u4fe1\u606f\uff0c\u65e2\u80fd\u7cbe\u7ec6\u4fdd\u7559\u91cd\u8981\u7ec6\u8282\uff0c\u53c8\u80fd\u9ad8\u5c42\u62bd\u8c61\u591a\u6b65\u5b50\u4efb\u52a1\uff0c\u5b9e\u73b0\u52a8\u6001\u3001\u4e3b\u52a8\u7684\u5386\u53f2\u4fe1\u606f\u7ba1\u7406\u3002", "result": "\u5728BrowseComp\u548cBrowseComp-ZH\u4e24\u4e2a\u91cd\u8981\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAgentFold-30B-A3B\u5206\u522b\u53d6\u5f97\u4e8636.2%\u548c47.3%\u7684\u6210\u7ee9\uff0c\u4e0d\u4ec5\u8d85\u8fc7\u4e86\u89c4\u6a21\u5927\u5f97\u591a\u7684\u5f00\u6e90\u6a21\u578bDeepSeek-V3.1-671B-A37B\uff0c\u8fd8\u4f18\u4e8eOpenAI\u7684\u9886\u5148\u4e13\u6709\u4ee3\u7406o4-mini\u3002", "conclusion": "AgentFold\u9a8c\u8bc1\u4e86\u4e3b\u52a8\u4e0a\u4e0b\u6587\u7ba1\u7406\u7b56\u7565\u5728\u957f\u4efb\u52a1LLM\u7f51\u9875\u4ee3\u7406\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u89e3\u51b3\u4e0a\u4e0b\u6587\u7ba1\u7406\u96be\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2510.24702", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24702", "abs": "https://arxiv.org/abs/2510.24702", "authors": ["Yueqi Song", "Ketan Ramaneti", "Zaid Sheikh", "Ziru Chen", "Boyu Gou", "Tianbao Xie", "Yiheng Xu", "Danyang Zhang", "Apurva Gandhi", "Fan Yang", "Joseph Liu", "Tianyue Ou", "Zhihao Yuan", "Frank Xu", "Shuyan Zhou", "Xingyao Wang", "Xiang Yue", "Tao Yu", "Huan Sun", "Yu Su", "Graham Neubig"], "title": "Agent Data Protocol: Unifying Datasets for Diverse, Effective Fine-tuning of LLM Agents", "comment": null, "summary": "Public research results on large-scale supervised finetuning of AI agents\nremain relatively rare, since the collection of agent training data presents\nunique challenges. In this work, we argue that the bottleneck is not a lack of\nunderlying data sources, but that a large variety of data is fragmented across\nheterogeneous formats, tools, and interfaces. To this end, we introduce the\nagent data protocol (ADP), a light-weight representation language that serves\nas an \"interlingua\" between agent datasets in diverse formats and unified agent\ntraining pipelines downstream. The design of ADP is expressive enough to\ncapture a large variety of tasks, including API/tool use, browsing, coding,\nsoftware engineering, and general agentic workflows, while remaining simple to\nparse and train on without engineering at a per-dataset level. In experiments,\nwe unified a broad collection of 13 existing agent training datasets into ADP\nformat, and converted the standardized ADP data into training-ready formats for\nmultiple agent frameworks. We performed SFT on these data, and demonstrated an\naverage performance gain of ~20% over corresponding base models, and delivers\nstate-of-the-art or near-SOTA performance on standard coding, browsing, tool\nuse, and research benchmarks, without domain-specific tuning. All code and data\nare released publicly, in the hope that ADP could help lower the barrier to\nstandardized, scalable, and reproducible agent training.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u4ee3\u7406\u6570\u636e\u534f\u8bae\uff08ADP\uff09\uff0c\u89e3\u51b3\u4e86\u4ee3\u7406\u8bad\u7ec3\u6570\u636e\u683c\u5f0f\u591a\u6837\u4e14\u5206\u6563\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u591a\u79cd\u4ee3\u7406\u4efb\u52a1\u6570\u636e\u7684\u7edf\u4e00\u8868\u8fbe\u548c\u8bad\u7ec3\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u516c\u5f00\u7684\u5927\u89c4\u6a21\u76d1\u7763\u5fae\u8c03\u4ee3\u7406\u8bad\u7ec3\u6570\u636e\u8f83\u5c11\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u6570\u636e\u6765\u6e90\u867d\u591a\u4f46\u683c\u5f0f\u548c\u63a5\u53e3\u5206\u6563\uff0c\u96be\u4ee5\u7edf\u4e00\u7ba1\u7406\u548c\u5229\u7528\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86ADP\uff0c\u4e00\u79cd\u53ef\u8868\u8fbe\u591a\u79cd\u4efb\u52a1\u7684\u7edf\u4e00\u6570\u636e\u683c\u5f0f\uff0c\u5c0613\u4e2a\u73b0\u6709\u4ee3\u7406\u8bad\u7ec3\u6570\u636e\u96c6\u6574\u5408\u8f6c\u6362\u4e3aADP\u683c\u5f0f\uff0c\u5e76\u7528\u8be5\u6570\u636e\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u3002", "result": "\u7edf\u4e00\u7684\u6570\u636e\u683c\u5f0f\u4fc3\u8fdb\u4e86\u8bad\u7ec3\uff0c\u5fae\u8c03\u540e\u6a21\u578b\u6027\u80fd\u5e73\u5747\u63d0\u5347\u7ea620%\uff0c\u5728\u7f16\u7801\u3001\u6d4f\u89c8\u3001\u5de5\u5177\u4f7f\u7528\u548c\u7814\u7a76\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u63a5\u8fd1\u6216\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u7684\u6548\u679c\u3002", "conclusion": "ADP\u4f5c\u4e3a\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e2d\u95f4\u8bed\u8a00\uff0c\u964d\u4f4e\u4e86\u4ee3\u7406\u8bad\u7ec3\u6570\u636e\u7684\u5229\u7528\u95e8\u69db\uff0c\u4fc3\u8fdb\u4e86\u4ee3\u7406\u8bad\u7ec3\u7684\u6807\u51c6\u5316\u3001\u53ef\u6269\u5c55\u548c\u53ef\u590d\u73b0\u6027\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5747\u5df2\u516c\u5f00\u3002"}}
{"id": "2510.24707", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24707", "abs": "https://arxiv.org/abs/2510.24707", "authors": ["Juraj Juraska", "Tobias Domhan", "Mara Finkelstein", "Tetsuji Nakagawa", "Geza Kovacs", "Daniel Deutsch", "Pidong Wang", "Markus Freitag"], "title": "MetricX-25 and GemSpanEval: Google Translate Submissions to the WMT25 Evaluation Shared Task", "comment": "Accepted to WMT25", "summary": "In this paper, we present our submissions to the unified WMT25 Translation\nEvaluation Shared Task. For the Quality Score Prediction subtask, we create a\nnew generation of MetricX with improvements in the input format and the\ntraining protocol, while for the Error Span Detection subtask we develop a new\nmodel, GemSpanEval, trained to predict error spans along with their severities\nand categories. Both systems are based on the state-of-the-art multilingual\nopen-weights model Gemma 3, fine-tuned on publicly available WMT data. We\ndemonstrate that MetricX-25, adapting Gemma 3 to an encoder-only architecture\nwith a regression head on top, can be trained to effectively predict both MQM\nand ESA quality scores, and significantly outperforms its predecessor. Our\ndecoder-only GemSpanEval model, on the other hand, we show to be competitive in\nerror span detection with xCOMET, a strong encoder-only sequence-tagging\nbaseline. With error span detection formulated as a generative task, we\ninstruct the model to also output the context for each predicted error span,\nthus ensuring that error spans are identified unambiguously.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86WMT25\u7ffb\u8bd1\u8bc4\u4f30\u5171\u4eab\u4efb\u52a1\u4e2d\u63d0\u4ea4\u7684\u4e24\u4e2a\u7cfb\u7edf\uff0c\u5206\u522b\u7528\u4e8e\u8d28\u91cf\u8bc4\u5206\u9884\u6d4b\u548c\u9519\u8bef\u8de8\u5ea6\u68c0\u6d4b\u3002", "motivation": "\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u548c\u9519\u8bef\u68c0\u6d4b\u7684\u6709\u6548\u6027\u3002", "method": "\u91c7\u7528\u591a\u8bed\u8a00\u5f00\u6e90\u6a21\u578bGemma 3\uff0c\u5bf9\u5176\u8fdb\u884c\u5fae\u8c03\uff0c\u6784\u5efaMetricX-25\u7528\u4e8e\u8d28\u91cf\u8bc4\u5206\u9884\u6d4b\uff0c\u6784\u5efaGemSpanEval\u7528\u4e8e\u9519\u8bef\u8de8\u5ea6\u68c0\u6d4b\u3002", "result": "MetricX-25\u663e\u8457\u4f18\u4e8e\u524d\u4ee3\u6a21\u578b\uff0c\u5728\u9884\u6d4bMQM\u548cESA\u8d28\u91cf\u8bc4\u5206\u4e0a\u6548\u679c\u7a81\u51fa\uff1bGemSpanEval\u5728\u9519\u8bef\u8de8\u5ea6\u68c0\u6d4b\u4e0a\u5177\u5907\u4e0e\u5f3a\u57fa\u7ebfxCOMET\u7ade\u4e89\u7684\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u6539\u8fdb\u8f93\u5165\u683c\u5f0f\u548c\u8bad\u7ec3\u534f\u8bae\uff0c\u4ee5\u53ca\u5c06\u9519\u8bef\u8de8\u5ea6\u68c0\u6d4b\u4efb\u52a1\u751f\u6210\u5316\uff0c\u63d0\u5347\u4e86\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4ef7\u548c\u9519\u8bef\u68c0\u6d4b\u7684\u6027\u80fd\u548c\u660e\u786e\u6027\u3002"}}
