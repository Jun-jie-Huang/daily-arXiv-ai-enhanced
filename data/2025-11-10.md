<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 44]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.SE](#cs.SE) [Total: 9]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Evaluating LLMs' Reasoning Over Ordered Procedural Steps](https://arxiv.org/abs/2511.04688)
*Adrita Anika,Md Messal Monem Miah*

Main category: cs.CL

TL;DR: 研究了大型语言模型在重建食谱步骤顺序任务中的表现，发现模型在步骤较多且顺序混乱时性能下降。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型在处理顺序对结果有直接影响的程序序列推理中的能力，特别是在食谱这类顺序关键的领域。

Method: 采用零样本和少样本设置评估多种大型语言模型，利用改编的排序和序列对齐评价指标（肯德尔tau，归一化最长公共子序列，归一化编辑距离）进行综合分析。

Result: 结果表明，随着序列长度增加和输入步骤混乱程度加剧，模型性能显著下降。

Conclusion: 现有大型语言模型在程序式推理任务中，尤其是面对更长且更混乱的输入时，推理能力有限。

Abstract: Reasoning over procedural sequences, where the order of steps directly
impacts outcomes, is a critical capability for large language models (LLMs). In
this work, we study the task of reconstructing globally ordered sequences from
shuffled procedural steps, using a curated dataset of food recipes, a domain
where correct sequencing is essential for task success. We evaluate several
LLMs under zero-shot and few-shot settings and present a comprehensive
evaluation framework that adapts established metrics from ranking and sequence
alignment. These include Kendall's Tau, Normalized Longest Common Subsequence
(NLCS), and Normalized Edit Distance (NED), which capture complementary aspects
of ordering quality. Our analysis shows that model performance declines with
increasing sequence length, reflecting the added complexity of longer
procedures. We also find that greater step displacement in the input,
corresponding to more severe shuffling, leads to further degradation. These
findings highlight the limitations of current LLMs in procedural reasoning,
especially with longer and more disordered inputs.

</details>


### [2] [Adaptive Testing for LLM Evaluation: A Psychometric Alternative to Static Benchmarks](https://arxiv.org/abs/2511.04689)
*Peiyu Li,Xiuxiu Tang,Si Chen,Ying Cheng,Ronald Metoyer,Ting Hua,Nitesh V. Chawla*

Main category: cs.CL

TL;DR: 论文提出了ATLAS，一种基于项目反应理论的自适应测试框架，用来减少大规模语言模型评估中的测试题数量，实现更高效且精准的模型能力测量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的评估需要大量基准测试题，评估过程昂贵且缓慢，且现有方法对所有题目一视同仁，忽略了题目的质量和信息量差异。

Method: 利用项目反应理论（IRT）和费舍尔信息引导的题目选择，实现自适应测试，从而减少测验题目数，保持评估精度，同时降低题目暴露率和测试重叠。

Result: 在HellaSwag数据集上，将题目数量减少90%至42题，测量精度仍与全量测试相当，平均绝对误差为0.154；平均暴露率保持在10%以下，测试重叠率为16-27%。

Conclusion: ATLAS框架能显著降低语言模型评估中所需的测试题数，提高测评效率和准确性，同时缓解数据标注错误带来的影响。IRT评分比准确率排名更能反映模型能力差异。

Abstract: Large language model evaluation requires thousands of benchmark items, making
evaluations expensive and slow. Existing methods compute average accuracy
across fixed item sets, treating all items equally despite varying quality and
informativeness. We present ATLAS an adaptive testing framework using Item
Response Theory (IRT) to estimate model ability through Fisher
information-guided item selection. Our analysis of five major benchmarks
reveals that 3-6% of items exhibit negative discrimination, indicating
annotation errors that corrupt static evaluation. ATLAS achieves 90% item
reduction while maintaining measurement precision: on HellaSwag (5,608 items),
we match full-benchmark estimates using only 42 items with 0.154 MAE. Our
framework maintains item exposure rates below 10% and test overlap at 16-27%,
compared to static benchmarks where every model sees all items (100% exposure).
Among 4,000+ tested models, IRT ranks differ from accuracy ranks: models with
the same accuracy get different IRT scores, and 23-31% of all models shift by
more than 10 rank positions. Code and calibrated item banks are available at
https://github.com/Peiyu-Georgia-Li/ATLAS.git.

</details>


### [3] [SARC: Sentiment-Augmented Deep Role Clustering for Fake News Detection](https://arxiv.org/abs/2511.04692)
*Jingqing Wang,Jiaxing Shang,Rong Xu,Fei Hao,Tianjin Huang,Geyong Min*

Main category: cs.CL

TL;DR: 该论文提出了SARC框架，通过情感增强的深度聚类识别用户角色，以提升假新闻检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法将情感特征视为辅助信号，忽视了情感极性来源于不同用户角色的区分，影响假新闻检测性能。

Method: 利用BiGRU和注意力机制联合表示评论文本和情感编码，构建可微分深度聚类模块自动分类用户角色，结合角色聚类与假新闻检测共同优化目标。

Result: 在RumourEval-19和微博竞赛数据集上，SARC在所有评价指标上优于基线模型。

Conclusion: 引入用户角色区分的情感增强聚类显著提升了假新闻检测的性能，证明了该方法的有效性。

Abstract: Fake news detection has been a long-standing research focus in social
networks. Recent studies suggest that incorporating sentiment information from
both news content and user comments can enhance detection performance. However,
existing approaches typically treat sentiment features as auxiliary signals,
overlooking role differentiation, that is, the same sentiment polarity may
originate from users with distinct roles, thereby limiting their ability to
capture nuanced patterns for effective detection. To address this issue, we
propose SARC, a Sentiment-Augmented Role Clustering framework which utilizes
sentiment-enhanced deep clustering to identify user roles for improved fake
news detection. The framework first generates user features through joint
comment text representation (with BiGRU and Attention mechanism) and sentiment
encoding. It then constructs a differentiable deep clustering module to
automatically categorize user roles. Finally, unlike existing approaches which
take fake news label as the unique supervision signal, we propose a joint
optimization objective integrating role clustering and fake news detection to
further improve the model performance. Experimental results on two benchmark
datasets, RumourEval-19 and Weibo-comp, demonstrate that SARC achieves superior
performance across all metrics compared to baseline models. The code is
available at: https://github.com/jxshang/SARC.

</details>


### [4] [Reasoning Up the Instruction Ladder for Controllable Language Models](https://arxiv.org/abs/2511.04694)
*Zishuo Zheng,Vidhisha Balachandran,Chan Young Park,Faeze Brahman,Sachin Kumar*

Main category: cs.CL

TL;DR: 该论文提出在大型语言模型中引入指令层级机制，通过训练模型识别和优先执行高层级指令，从而提升模型的可靠性和安全性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在关键决策中的应用，如何调和来自不同来源的多重指令成为挑战，确保高优先级指令优先执行对于模型的可控性和可信度至关重要。

Method: 将指令层级解析视为推理任务，设计包含可验证答案的VerIH数据集，并利用轻量级强化学习对模型进行微调，使模型能够理解系统指令与用户指令之间的优先关系。

Result: 微调后的模型在指令跟随和指令层级基准测试中表现出一致提升，且该推理能力能泛化到训练分布之外的安全关键场景，增强了对越狱和提示注入攻击的鲁棒性。

Conclusion: 基于推理的指令层级机制是实现可靠且可控大型语言模型的有效途径，系统提示的更新能稳健地引导模型行为改进。

Abstract: As large language model (LLM) based systems take on high-stakes roles in
real-world decision-making, they must reconcile competing instructions from
multiple sources (e.g., model developers, users, and tools) within a single
prompt context. Thus, enforcing an instruction hierarchy (IH) in LLMs, where
higher-level directives override lower-priority requests, is critical for the
reliability and controllability of LLMs. In this work, we reframe instruction
hierarchy resolution as a reasoning task. Specifically, the model must first
"think" about the relationship between a given user prompt and higher-priority
(system) instructions before generating a response. To enable this capability
via training, we construct VerIH, an instruction hierarchy dataset of
constraint-following tasks with verifiable answers. This dataset comprises both
aligned and conflicting system-user instructions. We show that lightweight
reinforcement learning with VerIH effectively transfers general reasoning
capabilities of models to instruction prioritization. Our finetuned models
achieve consistent improvements on instruction following and instruction
hierarchy benchmarks. This reasoning ability also generalizes to
safety-critical settings beyond the training distribution. By treating safety
issues as resolving conflicts between adversarial user inputs and predefined
higher-priority policies, our trained model enhances robustness against
jailbreak and prompt injection attacks. These results demonstrate that
reasoning over instruction hierarchies provides a practical path to reliable
LLMs, where updates to system prompts yield controllable and robust changes in
model behavior.

</details>


### [5] [EncouRAGe: Evaluating RAG Local, Fast, and Reliable](https://arxiv.org/abs/2511.04696)
*Jan Strich,Adeline Scharfenberg,Chris Biemann,Martin Semmann*

Main category: cs.CL

TL;DR: 介绍了EncouRAGe，一种用于检索增强生成（RAG）系统开发和评估的Python框架，支持灵活实验和本地部署。评估表明RAG效果仍不及Oracle Context，Hybrid BM25表现最佳。


<details>
  <summary>Details</summary>
Motivation: 为简化基于大语言模型和嵌入模型的RAG系统开发与评估，提供一个科学可复现、模块化且易扩展的框架。

Method: 设计了包含Type Manifest、RAG Factory、Inference、Vector Store和Metrics五个模块的EncouRAGe框架，并在多个基准数据集上进行了广泛评测。

Result: 评估显示RAG系统性能低于Oracle Context，Hybrid BM25在所有数据集上表现最好；重新排序带来性能提升有限但增加了响应延迟。

Conclusion: EncouRAGe框架有效支持RAG系统的开发和评估，研究发现当前RAG方法仍有提升空间，Hybrid BM25是一种稳定的基线方法。

Abstract: We introduce EncouRAGe, a comprehensive Python framework designed to
streamline the development and evaluation of Retrieval-Augmented Generation
(RAG) systems using Large Language Models (LLMs) and Embedding Models.
EncouRAGe comprises five modular and extensible components: Type Manifest, RAG
Factory, Inference, Vector Store, and Metrics, facilitating flexible
experimentation and extensible development. The framework emphasizes scientific
reproducibility, diverse evaluation metrics, and local deployment, enabling
researchers to efficiently assess datasets within RAG workflows. This paper
presents implementation details and an extensive evaluation across multiple
benchmark datasets, including 25k QA pairs and over 51k documents. Our results
show that RAG still underperforms compared to the Oracle Context, while Hybrid
BM25 consistently achieves the best results across all four datasets. We
further examine the effects of reranking, observing only marginal performance
improvements accompanied by higher response latency.

</details>


### [6] [multiMentalRoBERTa: A Fine-tuned Multiclass Classifier for Mental Health Disorder](https://arxiv.org/abs/2511.04698)
*K M Sajjadul Islam,John Fields,Praveen Madiraju*

Main category: cs.CL

TL;DR: 提出了多类心理健康状况分类的RoBERTa模型multiMentalRoBERTa，实现了优异的分类性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 早期从社交媒体文本中检测心理健康障碍对于及时支持和风险评估至关重要。

Method: 基于多个数据集微调RoBERTa进行多类心理健康分类，进行数据探索及比较多种模型表现，并采用可解释性手段分析词汇线索。

Result: multiMentalRoBERTa在六分类和五分类任务上分别达到了0.839和0.870的宏F1分数，优于基线和其他模型。

Conclusion: 微调的Transformer模型在敏感的心理健康检测中表现优异且解释性强，强调了公平性和人机协同的安全性，具有轻量级部署优势。

Abstract: The early detection of mental health disorders from social media text is
critical for enabling timely support, risk assessment, and referral to
appropriate resources. This work introduces multiMentalRoBERTa, a fine-tuned
RoBERTa model designed for multiclass classification of common mental health
conditions, including stress, anxiety, depression, post-traumatic stress
disorder (PTSD), suicidal ideation, and neutral discourse. Drawing on multiple
curated datasets, data exploration is conducted to analyze class overlaps,
revealing strong correlations between depression and suicidal ideation as well
as anxiety and PTSD, while stress emerges as a broad, overlapping category.
Comparative experiments with traditional machine learning methods,
domain-specific transformers, and prompting-based large language models
demonstrate that multiMentalRoBERTa achieves superior performance, with macro
F1-scores of 0.839 in the six-class setup and 0.870 in the five-class setup
(excluding stress), outperforming both fine-tuned MentalBERT and baseline
classifiers. Beyond predictive accuracy, explainability methods, including
Layer Integrated Gradients and KeyBERT, are applied to identify lexical cues
that drive classification, with a particular focus on distinguishing depression
from suicidal ideation. The findings emphasize the effectiveness of fine-tuned
transformers for reliable and interpretable detection in sensitive contexts,
while also underscoring the importance of fairness, bias mitigation, and
human-in-the-loop safety protocols. Overall, multiMentalRoBERTa is presented as
a lightweight, robust, and deployable solution for enhancing support in mental
health platforms.

</details>


### [7] [Cross-Lingual SynthDocs: A Large-Scale Synthetic Corpus for Any to Arabic OCR and Document Understanding](https://arxiv.org/abs/2511.04699)
*Haneen Al-Homoud,Asma Ibrahim,Murtadha Al-Jubran,Fahad Al-Otaibi,Yazeed Al-Harbi,Daulet Toibazar,Kesen Wang,Pedro J. Moreno*

Main category: cs.CL

TL;DR: SynthDocs是一个大规模的阿拉伯语合成语料库，包含超过250万样本，旨在解决阿拉伯语OCR和文档理解的数据匮乏问题。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语文档的OCR和理解任务缺乏充分的数据资源，限制了相关技术的发展。

Method: 通过结合真实扫描背景、双语布局和带元音符号字体，构建包含文本、表格和图表的多样化合成数据集，并在此基础上微调Qwen-2.5-VL模型。

Result: 在多个阿拉伯语公开基准数据集上，模型的词错误率（WER）、字符错误率（CER）、树编辑距离相似性（TEDS）及图表提取评分（CharTeX）均有显著提升。

Conclusion: SynthDocs为多语言文档分析提供了一个可扩展且视觉真实的资源，有助于推动阿拉伯语及其他语言文档处理技术的发展。

Abstract: Cross-Lingual SynthDocs is a large-scale synthetic corpus designed to address
the scarcity of Arabic resources for Optical Character Recognition (OCR) and
Document Understanding (DU). The dataset comprises over 2.5 million of samples,
including 1.5 million textual data, 270K fully annotated tables, and hundred
thousands of real data based charts. Our pipeline leverages authentic scanned
backgrounds, bilingual layouts, and diacritic aware fonts to capture the
typographic and structural complexity of Arabic documents. In addition to text,
the corpus includes variety of rendered styles for charts and tables.
Finetuning Qwen-2.5-VL on SynthDocs yields consistent improvements in Word
Error Rate (WER) and Character Error Rate (CER) in terms of OCR across multiple
public Arabic benchmarks, Tree-Edit Distance Similarity (TEDS) and Chart
Extraction Score (CharTeX) improved as well in other modalities. SynthDocs
provides a scalable, visually realistic resource for advancing research in
multilingual document analysis.

</details>


### [8] [Separate the Wheat from the Chaff: Winnowing Down Divergent Views in Retrieval Augmented Generation](https://arxiv.org/abs/2511.04700)
*Song Wang,Zihan Chen,Peng Wang,Zhepei Wei,Zhen Tan,Yu Meng,Cong Shen,Jundong Li*

Main category: cs.CL

TL;DR: WinnowRAG通过聚类和迭代筛选策略，有效过滤检索文档中的噪声，提升检索增强生成模型的回答准确度，且无需模型微调，适用性强。


<details>
  <summary>Details</summary>
Motivation: 扩大量检索文档虽然能提高相关信息的覆盖，但带来大量无关或误导性文档，降低生成回答的准确性，亟需有效的噪声过滤机制。

Method: WinnowRAG分两阶段：第一阶段基于查询聚类文档并分配给多个LLM代理生成回答；第二阶段由评估型LLM迭代甄别并剔除噪声文档，同时采用合并策略保留有用知识，无需模型微调。

Result: 在多个实际数据集上的广泛实验表明，WinnowRAG在准确率和鲁棒性上均优于现有最先进方法。

Conclusion: WinnowRAG为检索增强生成提供了一种高效、通用且易适配的文档过滤方案，有效提升了生成模型的性能和应用范围。

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
integrating external knowledge sources to address their limitations in
accessing up-to-date or specialized information. A natural strategy to increase
the likelihood of retrieving relevant information is to expand the number of
retrieved documents. However, involving more documents could introduce
significant noise, as many documents may be irrelevant or misleading, thereby
reducing the overall accuracy of the generated responses. To overcome the
challenge associated with handling a larger number of documents, we propose
WinnowRAG, a novel RAG framework designed to systematically filter out noisy
documents while preserving valuable content -- a process we refer to as
winnowing. WinnowRAG operates in two stages: In Stage I, we perform query-aware
clustering to group similar documents and form distinct topic clusters. Each
cluster is assigned to an LLM agent for generating a unique answer. In Stage
II, we perform winnowing, wherein a critic LLM evaluates the outputs of
multiple agents and iteratively separates useful documents from noisy ones. To
retain useful documents when discarding agents, we propose two strategic
merging techniques to ensure that only relevant knowledge is used for
generating the final response. Crucially, WinnowRAG is model-agnostic and does
not require any model fine-tuning, making it easily adaptable to various tasks.
Extensive experiments on various realistic datasets demonstrate the
effectiveness of WinnowRAG over state-of-the-art baselines.

</details>


### [9] [Measuring what Matters: Construct Validity in Large Language Model Benchmarks](https://arxiv.org/abs/2511.04703)
*Andrew M. Bean,Ryan Othniel Kearns,Angelika Romanou,Franziska Sofia Hafner,Harry Mayne,Jan Batzner,Negar Foroutan,Chris Schmitz,Karolina Korgul,Hunar Batra,Oishi Deb,Emma Beharry,Cornelius Emde,Thomas Foster,Anna Gausen,María Grandury,Simeng Han,Valentin Hofmann,Lujain Ibrahim,Hazel Kim,Hannah Rose Kirk,Fangru Lin,Gabrielle Kaili-May Liu,Lennart Luettgau,Jabez Magomere,Jonathan Rystrøm,Anna Sotnikova,Yushi Yang,Yilun Zhao,Adel Bibi,Antoine Bosselut,Ronald Clark,Arman Cohan,Jakob Foerster,Yarin Gal,Scott A. Hale,Inioluwa Deborah Raji,Christopher Summerfield,Philip H. S. Torr,Cozmin Ududec,Luc Rocher,Adam Mahdi*

Main category: cs.CL

TL;DR: 本文系统评估了现有大语言模型（LLM）基准测试的有效性，发现当前测试在安全性和鲁棒性等复杂现象的测量上存在有效性不足的问题，并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 为了更准确地评估大语言模型的能力及其安全性和鲁棒性，确保测试指标具有强构念有效性，以此提升评估的可信度。

Method: 组织29名专家对445个来自顶级会议的LLM基准测试进行系统回顾，分析任务设计、测量现象和评分机制中的共性问题。

Result: 发现当前基准测试在测量抽象复杂现象时存在有效性不足，影响结论的可靠性。

Conclusion: 提出八条关键建议及具体指导，帮助研究者和从业者设计更有效、具代表性的LLM基准测试。

Abstract: Evaluating large language models (LLMs) is crucial for both assessing their
capabilities and identifying safety or robustness issues prior to deployment.
Reliably measuring abstract and complex phenomena such as 'safety' and
'robustness' requires strong construct validity, that is, having measures that
represent what matters to the phenomenon. With a team of 29 expert reviewers,
we conduct a systematic review of 445 LLM benchmarks from leading conferences
in natural language processing and machine learning. Across the reviewed
articles, we find patterns related to the measured phenomena, tasks, and
scoring metrics which undermine the validity of the resulting claims. To
address these shortcomings, we provide eight key recommendations and detailed
actionable guidance to researchers and practitioners in developing LLM
benchmarks.

</details>


### [10] [UA-Code-Bench: A Competitive Programming Benchmark for Evaluating LLM Code Generation in Ukrainian](https://arxiv.org/abs/2511.05040)
*Mykyta Syromiatnikov,Victoria Ruvinskaya*

Main category: cs.CL

TL;DR: 本研究提出了UA-Code-Bench，这是一个针对乌克兰语代码生成和竞赛编程问题解决能力的公开基准测试，涉及500个难度等级分布均匀的问题，评测13个主流模型的Python代码生成能力。


<details>
  <summary>Details</summary>
Motivation: 当前评估大语言模型在低资源语言上的真实能力仍有挑战，现有基准多为从英语翻译任务或仅测试简单语言理解，缺乏针对代码生成和编程能力的深入测试。

Method: 构建包含500个乌克兰语编程问题的基准集，均匀覆盖五个难度级别，使用13个领先模型基于一-shot提示生成Python代码，通过Eolymp平台隐藏测试集验证代码正确性，评估性能、唯一性及计算效率。

Result: 结果显示即使是表现最好的模型如OpenAI o3和GPT-5也仅能解决约50%的问题，体现低资源语言下代码生成的难度。对不同难度的表现、解法唯一性及运行时间和内存消耗进行了详细分析。

Conclusion: 该研究表明竞赛编程基准对评估低资源语言的大语言模型非常有价值，推动了多语言代码生成和增强推理模型的未来研究，相关资源已公开发布。

Abstract: Evaluating the real capabilities of large language models in low-resource
languages still represents a challenge, as many existing benchmarks focus on
widespread tasks translated from English or evaluate only simple language
understanding. This paper introduces UA-Code-Bench, a new open-source benchmark
established for a thorough evaluation of language models' code generation and
competitive programming problem-solving abilities in Ukrainian. The benchmark
comprises 500 problems from the Eolymp platform, evenly distributed across five
complexity levels from very easy to very hard. A diverse set of 13 leading
proprietary and open-source models, generating Python solutions based on a
one-shot prompt, was evaluated via the dedicated Eolymp environment against
hidden tests, ensuring code correctness. The obtained results reveal that even
top-performing models, such as OpenAI o3 and GPT-5, solve only half of the
problems, highlighting the challenge of code generation in low-resource natural
language. Furthermore, this research presents a comprehensive analysis of
performance across various difficulty levels, as well as an assessment of
solution uniqueness and computational efficiency, measured by both elapsed time
and memory consumption of the generated solutions. In conclusion, this work
demonstrates the value of competitive programming benchmarks in evaluating
large language models, especially in underrepresented languages. It also paves
the way for future research on multilingual code generation and
reasoning-enhanced models. The benchmark, data parsing, preparation, code
generation, and evaluation scripts are available at
https://huggingface.co/datasets/NLPForUA/ua-code-bench.

</details>


### [11] [POLIS-Bench: Towards Multi-Dimensional Evaluation of LLMs for Bilingual Policy Tasks in Governmental Scenarios](https://arxiv.org/abs/2511.04705)
*Tingyue Yang,Junchi Yao,Yuhui Guo,Chang Liu*

Main category: cs.CL

TL;DR: 本文介绍了POLIS-Bench，这是第一个专为政府双语政策场景下大型语言模型设计的严谨评估套件。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法有效评估在政府双语政策应用场景中大型语言模型的理解和应用能力，因此需要一个专门针对该领域的评估工具。

Method: 构建了最新的双语政策语料库，设计了基于情景的三项任务（条款检索与解释、解决方案生成、合规判断），并提出结合语义相似度与准确率的双度量评估框架。

Result: 在10多个先进大型语言模型上进行大规模评测，发现推理模型表现优异，特别在合规任务中体现出模型能力差异；利用基准成功微调轻量级开源模型，达到或超过强大商业模型表现，成本显著降低。

Conclusion: POLIS-Bench为政府双语政策相关场景中大型语言模型的评测提供了系统、有效的工具，微调后的轻量级模型展示了高性价比和实际部署潜力。

Abstract: We introduce POLIS-Bench, the first rigorous, systematic evaluation suite
designed for LLMs operating in governmental bilingual policy scenarios.
Compared to existing benchmarks, POLIS-Bench introduces three major
advancements. (i) Up-to-date Bilingual Corpus: We construct an extensive,
up-to-date policy corpus that significantly scales the effective assessment
sample size, ensuring relevance to current governance practice. (ii)
Scenario-Grounded Task Design: We distill three specialized, scenario-grounded
tasks -- Clause Retrieval & Interpretation, Solution Generation, and the
Compliance Judgmen--to comprehensively probe model understanding and
application. (iii) Dual-Metric Evaluation Framework: We establish a novel
dual-metric evaluation framework combining semantic similarity with accuracy
rate to precisely measure both content alignment and task requirement
adherence. A large-scale evaluation of over 10 state-of-the-art LLMs on
POLIS-Bench reveals a clear performance hierarchy where reasoning models
maintain superior cross-task stability and accuracy, highlighting the
difficulty of compliance tasks. Furthermore, leveraging our benchmark, we
successfully fine-tune a lightweight open-source model. The resulting POLIS
series models achieves parity with, or surpasses, strong proprietary baselines
on multiple policy subtasks at a significantly reduced cost, providing a
cost-effective and compliant path for robust real-world governmental
deployment.

</details>


### [12] [GEMMA-SQL: A Novel Text-to-SQL Model Based on Large Language Models](https://arxiv.org/abs/2511.04710)
*Hari Mohan Pandey,Anshul Gupta,Subham Sarkar,Minakshi Tomer,Schneider Johannes,Yan Gong*

Main category: cs.CL

TL;DR: 本文提出了基于Gemma 2B架构的轻量高效文本到SQL模型GEMMA-SQL，通过多重提示策略和指令微调，在SPIDER数据集上取得优异性能，且能够部署于低成本硬件。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型资源需求高，不易部署且成本较大，故需要一种轻量级、资源高效且性能优良的文本到SQL解决方案。

Method: 基于开源Gemma 2B架构，采用资源高效的迭代微调方法，结合多重提示策略（包括少样本学习）进行训练，设计指令微调版本GEMMA-SQL Instruct，提升SQL生成准确率。

Result: 在SPIDER基准测试上，GEMMA-SQL Instruct实现了66.8%的Test-Suite准确率和63.3%的Exact Set Match准确率，超越了IRNet、RYANSQL和CodeXDavinci等多项先进模型。

Conclusion: 有效的提示设计和针对性的指令微调显著提升了模型性能，同时保证了系统的可扩展性和适应性，使GEMMA-SQL成为实际可用的开源文本到SQL替代方案。

Abstract: Text-to-SQL systems enable users to interact with structured databases using
natural language, eliminating the need for specialized programming knowledge.
In this work, we introduce GEMMA-SQL, a lightweight and efficient text-to-SQL
model built upon the open-source Gemma 2B architecture. Unlike many large
language models (LLMs), GEMMA-SQL is fine-tuned in a resource-efficient,
iterative manner and can be deployed on low-cost hardware. Leveraging the
SPIDER benchmark for training and evaluation, GEMMA-SQL combines multiple
prompting strategies, including few-shot learning, to enhance SQL query
generation accuracy. The instruction-tuned variant, GEMMA-SQL Instruct,
achieves 66.8% Test-Suite accuracy and 63.3% Exact Set Match accuracy,
outperforming several state-of-the-art baselines such as IRNet, RYANSQL, and
CodeXDavinci. The proposed approach demonstrates that effective prompt design
and targeted instruction tuning can significantly boost performance while
maintaining high scalability and adaptability. These results position GEMMA-SQL
as a practical, open-source alternative for robust and accessible text-to-SQL
systems.

</details>


### [13] [First is Not Really Better Than Last: Evaluating Layer Choice and Aggregation Strategies in Language Model Data Influence Estimation](https://arxiv.org/abs/2511.04715)
*Dmytro Vitel,Anshuman Chhabra*

Main category: cs.CL

TL;DR: 本文研究了大语言模型中训练样本影响度的准确估计，挑战了以往认为前层更重要的观点，提出中间注意力层更优的估计方法，并引入了新的影响评分评估指标NDR。


<details>
  <summary>Details</summary>
Motivation: 当前对大语言模型训练样本影响的估计多依赖于模型中梯度信息，但因模型庞大，影响度计算常局限于部分层次，导致估计方法存在局限性且之前的假设存在不可靠之处。

Method: 提出了理论和实证证据反驳“抵消效应”理论，主张中间注意力层是更好的影响估计选择，同时探索了多层影响评分聚合的新方法（如排名和投票），并提出无需重训练的影响评分评估新指标噪声检测率（NDR）。

Result: 通过多种规模与类型的大型语言模型实验，结果表明中间层优于前层进行影响度估计，新聚合方法显著提升性能，且NDR指标较传统方法更具预测能力。

Conclusion: 本研究推翻了先前认为前层影响力较大的观点，证明了中间注意力层在影响估计中的优势，并提供了更有效的影响评分聚合和评估方法，为理解和审计大型语言模型提供了更科学的工具。

Abstract: Identifying how training samples influence/impact Large Language Model (LLM)
decision-making is essential for effectively interpreting model decisions and
auditing large-scale datasets. Current training sample influence estimation
methods (also known as influence functions) undertake this goal by utilizing
information flow through the model via its first-order and higher-order
gradient terms. However, owing to the large model sizes of today consisting of
billions of parameters, these influence computations are often restricted to
some subset of model layers to ensure computational feasibility. Prior seminal
work by Yeh et al. (2022) in assessing which layers are best suited for
computing language data influence concluded that the first (embedding) layers
are the most informative for this purpose, using a hypothesis based on
influence scores canceling out (i.e., the cancellation effect). In this work,
we propose theoretical and empirical evidence demonstrating how the
cancellation effect is unreliable, and that middle attention layers are better
estimators for influence. Furthermore, we address the broader challenge of
aggregating influence scores across layers, and showcase how alternatives to
standard averaging (such as ranking and vote-based methods) can lead to
significantly improved performance. Finally, we propose better methods for
evaluating influence score efficacy in LLMs without undertaking model
retraining, and propose a new metric known as the Noise Detection Rate (NDR)
that exhibits strong predictive capability compared to the cancellation effect.
Through extensive experiments across LLMs of varying types and scales, we
concretely determine that the first (layers) are not necessarily better than
the last (layers) for LLM influence estimation, contrasting with prior
knowledge in the field.

</details>


### [14] [Learning to reason about rare diseases through retrieval-augmented agents](https://arxiv.org/abs/2511.04720)
*Ha Young Kim,Jun Li,Ana Beatriz Solana,Carolin M. Pirkl,Benedikt Wiestler,Julia A. Schnabel,Cosmin I. Bercea*

Main category: cs.CL

TL;DR: 该论文提出了RADAR系统，通过检索病例报告和文献，增强AI在脑部MRI罕见病检测的诊断推理能力，提高了诊断准确率和模型解释性。


<details>
  <summary>Details</summary>
Motivation: 罕见疾病因训练数据稀缺，导致AI模型诊断准确性低，临床医生常依赖文献和病例辅助诊断，因此设计基于外部知识检索的辅助诊断系统。

Method: 利用句子转换器对病例报告和文献进行嵌入，采用FAISS构建索引实现高效相似性搜索，结合大语言模型实现无额外训练的罕见病诊断与解释。

Result: 在包含280种罕见疾病的NOVA数据集上，RADAR提升了最高10.2%的诊断性能，且增强了模型的可解释性，尤其对开源模型效果显著。

Conclusion: 基于知识检索的增强推理方法在罕见病医学影像诊断中表现出强大潜力，有助于提升低发病率疾病的识别能力和模型解释力。

Abstract: Rare diseases represent the long tail of medical imaging, where AI models
often fail due to the scarcity of representative training data. In clinical
workflows, radiologists frequently consult case reports and literature when
confronted with unfamiliar findings. Following this line of reasoning, we
introduce RADAR, Retrieval Augmented Diagnostic Reasoning Agents, an agentic
system for rare disease detection in brain MRI. Our approach uses AI agents
with access to external medical knowledge by embedding both case reports and
literature using sentence transformers and indexing them with FAISS to enable
efficient similarity search. The agent retrieves clinically relevant evidence
to guide diagnostic decision making on unseen diseases, without the need of
additional training. Designed as a model-agnostic reasoning module, RADAR can
be seamlessly integrated with diverse large language models, consistently
improving their rare pathology recognition and interpretability. On the NOVA
dataset comprising 280 distinct rare diseases, RADAR achieves up to a 10.2%
performance gain, with the strongest improvements observed for open source
models such as DeepSeek. Beyond accuracy, the retrieved examples provide
interpretable, literature grounded explanations, highlighting
retrieval-augmented reasoning as a powerful paradigm for low-prevalence
conditions in medical imaging.

</details>


### [15] [Surprisal reveals diversity gaps in image captioning and different scorers change the story](https://arxiv.org/abs/2511.04754)
*Nikolai Ilinykh,Simon Dobnik*

Main category: cs.CL

TL;DR: 本文提出了基于surprisal方差的图像描述语言多样性度量方法，并比较了多种模型与人类的表现，指出单一评分器评估可能导致结论的颠倒。


<details>
  <summary>Details</summary>
Motivation: 现有图像描述模型的语言多样性评估缺乏统一且可靠的度量标准，且不同评分器对多样性评估结果影响较大。

Method: 基于token级别的负对数概率的方差（surprisal方差）来衡量描述集合的语言多样性，采用多种语言模型（包括caption训练的n-gram模型和通用语言模型）进行重新评分，并比较不同模型和人类生成的描述。

Result: 在人类描述中，使用caption训练的n-gram语言模型时surprisal方差约为模型的两倍，但用通用语言模型重新评分后，人类与模型的结果模式发生逆转，显示评分器选择对多样性评价有重大影响。

Conclusion: 提出的基于surprisal方差的多样性度量指标具有鲁棒性，但依赖单一评分器可能导致评价结论完全相反，建议采用多评分器综合评估图像描述的语言多样性。

Abstract: We quantify linguistic diversity in image captioning with surprisal variance
- the spread of token-level negative log-probabilities within a caption set. On
the MSCOCO test set, we compare five state-of-the-art vision-and-language LLMs,
decoded with greedy and nucleus sampling, to human captions. Measured with a
caption-trained n-gram LM, humans display roughly twice the surprisal variance
of models, but rescoring the same captions with a general-language model
reverses the pattern. Our analysis introduces the surprisal-based diversity
metric for image captioning. We show that relying on a single scorer can
completely invert conclusions, thus, robust diversity evaluation must report
surprisal under several scorers.

</details>


### [16] [Explore Data Left Behind in Reinforcement Learning for Reasoning Language Models](https://arxiv.org/abs/2511.04800)
*Chenxi Liu,Junjie Liang,Yuqi Jia,Bochuan Cao,Yang Bai,Heng Huang,Xun Chen*

Main category: cs.CL

TL;DR: 本文提出了一种探索残差提示（ERPO）的方法，通过激活零方差奖励的残差提示，提升大语言模型在强化学习训练中的推理能力。


<details>
  <summary>Details</summary>
Motivation: 随着训练时间增加和模型规模扩大，越来越多的训练提示成为零方差残差提示，缺乏训练信号，导致训练多样性降低和效果受限。

Method: 提出ERPO框架，跟踪每个提示的历史表现，针对残差提示自适应增加采样温度，促进生成多样性推理轨迹，激活训练信号。

Result: 在Qwen2.5系列模型上，ERPO在多个数学推理基准测试中持续超越强基线方法。

Conclusion: ERPO有效利用了残差提示，提升了大语言模型的推理训练效果，是强化学习训练中一种有效的策略优化方法。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an
effective approach for improving the reasoning abilities of large language
models (LLMs). The Group Relative Policy Optimization (GRPO) family has
demonstrated strong performance in training LLMs with RLVR. However, as models
train longer and scale larger, more training prompts become residual prompts,
those with zero variance rewards that provide no training signal. Consequently,
fewer prompts contribute to training, reducing diversity and hindering
effectiveness. To fully exploit these residual prompts, we propose the Explore
Residual Prompts in Policy Optimization (ERPO) framework, which encourages
exploration on residual prompts and reactivates their training signals. ERPO
maintains a history tracker for each prompt and adaptively increases the
sampling temperature for residual prompts that previously produced all correct
responses. This encourages the model to generate more diverse reasoning traces,
introducing incorrect responses that revive training signals. Empirical results
on the Qwen2.5 series demonstrate that ERPO consistently surpasses strong
baselines across multiple mathematical reasoning benchmarks.

</details>


### [17] [Trained on Tokens, Calibrated on Concepts: The Emergence of Semantic Calibration in LLMs](https://arxiv.org/abs/2511.04869)
*Preetum Nakkiran,Arwen Bradley,Adam Goliński,Eugene Ndiaye,Michael Kirchhof,Sinead Williamson*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型(LLMs)在语义层面上的置信度校准问题，发现基础模型在开放领域问答任务中具备良好的语义置信度估计能力，并提供了理论解释与实验证明。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明LLMs具备下一词预测的校准能力，但是否能在语义层面准确评估回答置信度尚不清楚。

Method: 提出了基于“B-校准”的理论框架，解释了为何语义校准是下一词预测的副产品，并通过语义答案类别分布预测测试这一机制。

Result: 实验证明基础LLMs在问答任务中具备语义校准能力；强化学习指令调优和链式思维推理会破坏这种校准。

Conclusion: 论文首次提供了LLMs语义校准出现的原则性解释，揭示其理论机制及实验验证，拓展了对模型置信度估计的理解。

Abstract: Large Language Models (LLMs) often lack meaningful confidence estimates for
their outputs. While base LLMs are known to exhibit next-token calibration, it
remains unclear whether they can assess confidence in the actual meaning of
their responses beyond the token level. We find that, when using a certain
sampling-based notion of semantic calibration, base LLMs are remarkably
well-calibrated: they can meaningfully assess confidence in open-domain
question-answering tasks, despite not being explicitly trained to do so. Our
main theoretical contribution establishes a mechanism for why semantic
calibration emerges as a byproduct of next-token prediction, leveraging a
recent connection between calibration and local loss optimality. The theory
relies on a general definition of "B-calibration," which is a notion of
calibration parameterized by a choice of equivalence classes (semantic or
otherwise). This theoretical mechanism leads to a testable prediction: base
LLMs will be semantically calibrated when they can easily predict their own
distribution over semantic answer classes before generating a response. We
state three implications of this prediction, which we validate through
experiments: (1) Base LLMs are semantically calibrated across
question-answering tasks, (2) RL instruction-tuning systematically breaks this
calibration, and (3) chain-of-thought reasoning breaks calibration. To our
knowledge, our work provides the first principled explanation of when and why
semantic calibration emerges in LLMs.

</details>


### [18] [Minimal and Mechanistic Conditions for Behavioral Self-Awareness in LLMs](https://arxiv.org/abs/2511.04875)
*Matthew Bozoukov,Matthew Nguyen,Shubkarman Singh,Bart Bussmann,Patrick Leask*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型行为自我意识的最小诱发条件及其机制，发现通过单个低秩适配器即可引发且表现为线性特征，且自我意识在不同任务间独立分布。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型展现的自我意识能力可能导致安全风险，如在评估中更好地掩饰其真实能力，因此需要理解其产生的最小条件及机制。

Method: 对经过指令调优的语言模型使用低秩适配器（LoRA）进行受控微调实验，研究行为自我意识的诱发及其在激活空间中的表现。

Result: 发现单个秩为1的LoRA适配器即可稳定诱发行为自我意识，该行为可由激活空间中的单一引导向量捕获，自我意识表现为领域局部且任务间表示独立。

Conclusion: 行为自我意识作为一种领域特定的线性特征容易被诱发和调节，提示其具有较明确的结构和潜在风险。

Abstract: Recent studies have revealed that LLMs can exhibit behavioral self-awareness:
the ability to accurately describe or predict their own learned behaviors
without explicit supervision. This capability raises safety concerns as it may,
for example, allow models to better conceal their true abilities during
evaluation. We attempt to characterize the minimal conditions under which such
self-awareness emerges, and the mechanistic processes through which it
manifests. Through controlled finetuning experiments on instruction-tuned LLMs
with low-rank adapters (LoRA), we find: (1) that self-awareness can be reliably
induced using a single rank-1 LoRA adapter; (2) that the learned self-aware
behavior can be largely captured by a single steering vector in activation
space, recovering nearly all of the fine-tune's behavioral effect; and (3) that
self-awareness is non-universal and domain-localized, with independent
representations across tasks. Together, these findings suggest that behavioral
self-awareness emerges as a domain-specific, linear feature that can be easily
induced and modulated.

</details>


### [19] [SDS KoPub VDR: A Benchmark Dataset for Visual Document Retrieval in Korean Public Documents](https://arxiv.org/abs/2511.04910)
*Jaehoon Lee,Sohyun Kim,Wanggeun Park,Geon Lee,Seungkyung Kim,Minyoung Lee*

Main category: cs.CL

TL;DR: 本文提出了首个面向韩文公共文档的大规模视觉文档检索基准SDS KoPub VDR，并展示了现有模型在多模态检索任务中存在的性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有视觉文档检索基准主要忽视非英语语言及官方文档的结构复杂性，导致相关技术无法有效应用于复杂多模态的韩文公共文档场景。

Method: 构建包含361份真实韩文公共文档的大规模数据集，设计600组查询-页面-答案三元组，涵盖文本、视觉及跨模态推理，采用GPT-4o生成初稿并经过人工严格审核。评测包括纯文本检索和多模态检索两大任务，系统评估模型表现。

Result: 多模态检索任务表现明显落后于文本检索，尤其是需要跨模态推理的场景，显示当前先进模型未能充分理解和利用视觉信息。

Conclusion: SDS KoPub VDR基准资源填补了韩文公共文档视觉检索领域的空白，为多模态文档智能提供了严谨评测平台，推动未来多模态AI在复杂文档环境下的发展。

Abstract: Existing benchmarks for visual document retrieval (VDR) largely overlook
non-English languages and the structural complexity of official publications.
To address this critical gap, we introduce SDS KoPub VDR, the first
large-scale, publicly available benchmark for retrieving and understanding
Korean public documents. The benchmark is built upon a corpus of 361 real-world
documents (40,781 pages), including 256 files under the KOGL Type 1 license and
105 from official legal portals, capturing complex visual elements like tables,
charts, and multi-column layouts. To establish a challenging and reliable
evaluation set, we constructed 600 query-page-answer triples. These were
initially generated using multimodal models (e.g., GPT-4o) and subsequently
underwent a rigorous human verification and refinement process to ensure
factual accuracy and contextual relevance. The queries span six major public
domains and are systematically categorized by the reasoning modality required:
text-based, visual-based (e.g., chart interpretation), and cross-modal. We
evaluate SDS KoPub VDR on two complementary tasks that reflect distinct
retrieval paradigms: (1) text-only retrieval, which measures a model's ability
to locate relevant document pages based solely on textual signals, and (2)
multimodal retrieval, which assesses retrieval performance when visual features
(e.g., tables, charts, and layouts) are jointly leveraged alongside text. This
dual-task evaluation reveals substantial performance gaps, particularly in
multimodal scenarios requiring cross-modal reasoning, even for state-of-the-art
models. As a foundational resource, SDS KoPub VDR not only enables rigorous and
fine-grained evaluation across textual and multimodal retrieval tasks but also
provides a clear roadmap for advancing multimodal AI in complex, real-world
document intelligence.

</details>


### [20] [BudgetMem: Learning Selective Memory Policies for Cost-Efficient Long-Context Processing in Language Models](https://arxiv.org/abs/2511.04919)
*Chandra Vamsi Krishna Alla,Harish Naidu Gaddam,Manohar Kommi*

Main category: cs.CL

TL;DR: BudgetMem是一种新颖的记忆增强架构，通过选择性记忆和特征评分，在严格内存预算下高效处理长文本，显著节省内存且性能仅轻微下降。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在处理长上下文时受限于计算和内存资源，现有扩展上下文窗口的方法成本高昂，不适合资源有限的部署环境。

Method: 提出BudgetMem架构，结合选择性记忆策略和基于特征的显著性评分（如实体密度、TF-IDF、话语标志、位置信息偏差），并采用学习门控机制配合BM25稀疏检索，实现高效存储与信息访问。

Result: 在700个问答对的实验中，BudgetMem在长文档上仅降低1.0%的F1分数，同时节省了72.4%的内存，相较于基线RAG系统表现优异。

Conclusion: BudgetMem为在有限硬件条件下部署具备长文本理解能力的语言模型提供了实用方案，扩展了该技术的可及性。

Abstract: Large Language Models (LLMs) face significant computational and memory
constraints when processing long contexts, despite growing demand for
applications requiring reasoning over extensive documents, multi-session
dialogues, and book length texts. While recent advances have extended context
windows to 100K-1M tokens, such approaches incur prohibitive costs for resource
constrained deployments. We propose BudgetMem, a novel memory augmented
architecture that learns what to remember rather than remembering everything.
Our system combines selective memory policies with feature based salience
scoring (entity density, TF-IDF, discourse markers, position bias) to decide
which information merits storage under strict budget constraints. Unlike
existing retrieval augmented generation (RAG) systems that store all chunks,
BudgetMem employs learned gating mechanisms coupled with BM25 sparse retrieval
for efficient information access. Through comprehensive experiments on 700
question answer pairs across short (237 tokens) and long (5K-10K tokens)
documents with Llama-3.2-3B-Instruct, we demonstrate that BudgetMem achieves
remarkable results on long documents: only 1.0% F1 score degradation while
saving 72.4% memory compared to baseline RAG. We validate our approach through
budget sensitivity analysis (testing 7 budget ratios), naive baseline
comparisons, and document length analysis, showing that BudgetMem's benefits
increase with document length. Our work provides a practical pathway for
deploying capable long context systems on modest hardware, democratizing access
to advanced language understanding capabilities.

</details>


### [21] [AgentExpt: Automating AI Experiment Design with LLM-based Resource Retrieval Agent](https://arxiv.org/abs/2511.04921)
*Yu Li,Lehui Li,Qingmin Liao,Fengli Xu,Yong Li*

Main category: cs.CL

TL;DR: 本文提出了一种基于论文引用网络的基线和数据集推荐框架，提升了实验设计的自动化能力。


<details>
  <summary>Details</summary>
Motivation: 现有工作受限于数据覆盖不足及过度依赖内容相似性，忽视了实验适用性，难以全面推荐真实使用的基线和数据集。

Method: 设计自动化数据收集管道链接论文与实际使用的基线和数据集；提出融合自我描述和引用上下文的集体感知增强检索器，微调嵌入模型提高召回效率；构建推理增强的重排序器，提取交互链条生成可解释的推荐理由和优化排名。

Result: 构建了涵盖过去五年顶尖AI会议85%使用数据集和基线的数据集；方法在Recall@20和HitRate@5指标上比分数最强的现有方法分别提升5.85%和8.30%。

Conclusion: 该研究推动了实验设计自动化的可靠性和可解释性，提供了有效的基线和数据集推荐方案。

Abstract: Large language model agents are becoming increasingly capable at web-centric
tasks such as information retrieval, complex reasoning. These emerging
capabilities have given rise to surge research interests in developing LLM
agent for facilitating scientific quest. One key application in AI research is
to automate experiment design through agentic dataset and baseline retrieval.
However, prior efforts suffer from limited data coverage, as recommendation
datasets primarily harvest candidates from public portals and omit many
datasets actually used in published papers, and from an overreliance on content
similarity that biases model toward superficial similarity and overlooks
experimental suitability. Harnessing collective perception embedded in the
baseline and dataset citation network, we present a comprehensive framework for
baseline and dataset recommendation. First, we design an automated
data-collection pipeline that links roughly one hundred thousand accepted
papers to the baselines and datasets they actually used. Second, we propose a
collective perception enhanced retriever. To represent the position of each
dataset or baseline within the scholarly network, it concatenates
self-descriptions with aggregated citation contexts. To achieve efficient
candidate recall, we finetune an embedding model on these representations.
Finally, we develop a reasoning-augmented reranker that exact interaction
chains to construct explicit reasoning chains and finetunes a large language
model to produce interpretable justifications and refined rankings. The dataset
we curated covers 85\% of the datasets and baselines used at top AI conferences
over the past five years. On our dataset, the proposed method outperforms the
strongest prior baseline with average gains of +5.85\% in Recall@20, +8.30\% in
HitRate@5. Taken together, our results advance reliable, interpretable
automation of experimental design.

</details>


### [22] [Diagnosing and Mitigating Semantic Inconsistencies in Wikidata's Classification Hierarchy](https://arxiv.org/abs/2511.04926)
*Shixiong Zhao,Hideaki Takeda*

Main category: cs.CL

TL;DR: 本文针对Wikidata存在的分类错误和层级结构不一致问题，提出了一种新的验证方法和评估标准，并开发了一个系统以帮助用户检查和修正分类关系。


<details>
  <summary>Details</summary>
Motivation: Wikidata作为最大的开放知识图谱，虽然具备数据整合优势，但宽松的编辑政策导致分类错误和冗余连接，影响了数据质量。

Method: 基于先前研究，提出新验证方法确认分类错误、过度泛化的子类链接及冗余连接；引入新的评估标准判定是否需修正；开发用户检查系统。

Result: 验证方法成功识别了具体领域内的分类问题，新评价标准有效指导修正决策，用户检查系统发挥了众包优势，提升了分类准确性。

Conclusion: 通过新验证方法和工具，优化了Wikidata的分类结构，提升了知识图谱数据质量，为知识图谱研究和应用提供了更可靠的数据基础。

Abstract: Wikidata is currently the largest open knowledge graph on the web,
encompassing over 120 million entities. It integrates data from various
domain-specific databases and imports a substantial amount of content from
Wikipedia, while also allowing users to freely edit its content. This openness
has positioned Wikidata as a central resource in knowledge graph research and
has enabled convenient knowledge access for users worldwide. However, its
relatively loose editorial policy has also led to a degree of taxonomic
inconsistency. Building on prior work, this study proposes and applies a novel
validation method to confirm the presence of classification errors,
over-generalized subclass links, and redundant connections in specific domains
of Wikidata. We further introduce a new evaluation criterion for determining
whether such issues warrant correction and develop a system that allows users
to inspect the taxonomic relationships of arbitrary Wikidata
entities-leveraging the platform's crowdsourced nature to its full potential.

</details>


### [23] [LoPT: Lossless Parallel Tokenization Acceleration for Long Context Inference of Large Language Model](https://arxiv.org/abs/2511.04952)
*Wei Shao,Lingchao Zheng,Pengyu Wang,Peizhen Zheng,Jun Li,Yuwei Fan*

Main category: cs.CL

TL;DR: 提出了LoPT，一种无损并行分词框架，实现长文本分词加速且结果与标准顺序分词完全一致。


<details>
  <summary>Details</summary>
Motivation: 长上下文推理场景中，分词作为瓶颈影响模型计算效率，现有并行分词存在边界不一致的问题。

Method: LoPT通过基于字符位置的匹配和动态调整分块长度，实现对分词片段的准确对齐和合并，保证无损分词。

Result: 在多种长文本数据集上，LoPT显著加速分词处理，同时保证分词结果与顺序分词一致。

Conclusion: LoPT有效解决了并行分词中的边界一致性问题，实现了加速且无损的分词过程，理论和实验均验证了其鲁棒性。

Abstract: Long context inference scenarios have become increasingly important for large
language models, yet they introduce significant computational latency. While
prior research has optimized long-sequence inference through operators, model
architectures, and system frameworks, tokenization remains an overlooked
bottleneck. Existing parallel tokenization methods accelerate processing
through text segmentation and multi-process tokenization, but they suffer from
inconsistent results due to boundary artifacts that occur after merging. To
address this, we propose LoPT, a novel Lossless Parallel Tokenization framework
that ensures output identical to standard sequential tokenization. Our approach
employs character-position-based matching and dynamic chunk length adjustment
to align and merge tokenized segments accurately. Extensive experiments across
diverse long-text datasets demonstrate that LoPT achieves significant speedup
while guaranteeing lossless tokenization. We also provide theoretical proof of
consistency and comprehensive analytical studies to validate the robustness of
our method.

</details>


### [24] [Too Good to be Bad: On the Failure of LLMs to Role-Play Villains](https://arxiv.org/abs/2511.04962)
*Zihao Yi,Qingxuan Jiang,Ruotian Ma,Xingyu Chen,Qu Yang,Mengru Wang,Fanghua Ye,Ying Shen,Zhaopeng Tu,Xiaolong Li,Linus*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型在扮演道德多样角色时的表现，尤其是负面角色的表现受到安全对齐机制的制约。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在生成内容时倾向于道德安全对齐，这限制了它们真实扮演具有负面或复杂道德特征角色的能力。

Method: 作者提出了Moral RolePlay基准数据集，包含四级道德对齐尺度和均衡的测试集，用于评估模型扮演从道德典范到纯恶棍角色的表现。

Result: 实验证明，随着角色道德的降低，模型的角色扮演真实性单调下降；尤以欺骗与操控等与安全原则冲突的负面特质表现最弱。一般聊天能力与扮演反派角色能力相关性低，高安全对齐模型表现尤其差。

Conclusion: 该研究首次系统揭示了语言模型安全性与创造性真实度之间的冲突，强调需要发展更细致且具上下文感知的对齐方法。

Abstract: Large Language Models (LLMs) are increasingly tasked with creative
generation, including the simulation of fictional characters. However, their
ability to portray non-prosocial, antagonistic personas remains largely
unexamined. We hypothesize that the safety alignment of modern LLMs creates a
fundamental conflict with the task of authentically role-playing morally
ambiguous or villainous characters. To investigate this, we introduce the Moral
RolePlay benchmark, a new dataset featuring a four-level moral alignment scale
and a balanced test set for rigorous evaluation. We task state-of-the-art LLMs
with role-playing characters from moral paragons to pure villains. Our
large-scale evaluation reveals a consistent, monotonic decline in role-playing
fidelity as character morality decreases. We find that models struggle most
with traits directly antithetical to safety principles, such as ``Deceitful''
and ``Manipulative'', often substituting nuanced malevolence with superficial
aggression. Furthermore, we demonstrate that general chatbot proficiency is a
poor predictor of villain role-playing ability, with highly safety-aligned
models performing particularly poorly. Our work provides the first systematic
evidence of this critical limitation, highlighting a key tension between model
safety and creative fidelity. Our benchmark and findings pave the way for
developing more nuanced, context-aware alignment methods.

</details>


### [25] [Acquiring Common Chinese Emotional Events Using Large Language Model](https://arxiv.org/abs/2511.04989)
*Ya Wang,Guangzheng Zhu,Cungen Cao,Jingjing Li,He Li,Xin Huang*

Main category: cs.CL

TL;DR: 本文通过收集中文情感事件指示词，利用中文大语言模型生成并筛选情感事件，最终构建了一个包含102,218条带情感极性标签的中文常见情感事件知识库。


<details>
  <summary>Details</summary>
Motivation: 获取常见的中文情感事件是提升多种应用效果的重要基础，但情感事件尤其是通用的、上下文独立的情感事件难以获取。

Method: 先收集中文情感事件指示词，再利用大语言模型生成情感事件，训练过滤器筛除无效结果，并采用不同技术对事件进行正负分类。

Result: 构建了唯一大规模中文常见情感事件知识库，包含102,218条高质量带情感极性标签的事件。内在评测和情感原因提取的应用展示了该方法的有效性和潜力。

Conclusion: 该方法有效获取了通用的中文情感事件，将促进情绪因果提取等领域的研究和应用，相关资源将在论文发布后公开。

Abstract: Knowledge about emotional events is an important kind of knowledge which has
been applied to improve the effectiveness of different applications. However,
emotional events cannot be easily acquired, especially common or generalized
emotional events that are context-independent. The goal of this paper is to
obtain common emotional events in Chinese language such as "win a prize" and
"be criticized". Our approach begins by collecting a comprehensive list of
Chinese emotional event indicators. Then, we generate emotional events by
prompting a Chinese large language model (LLM) using these indicators. To
ensure the quality of these emotional events, we train a filter to discard
invalid generated results. We also classify these emotional events as being
positive events and negative events using different techniques. Finally, we
harvest a total of 102,218 high-quality common emotional events with sentiment
polarity labels, which is the only large-scale commonsense knowledge base of
emotional events in Chinese language. Intrinsic evaluation results show that
the proposed method in this paper can be effectively used to acquire common
Chinese emotional events. An extrinsic use case also demonstrates the strong
potential of common emotional events in the field of emotion cause extraction
(ECE). Related resources including emotional event indicators and emotional
events will be released after the publication of this paper.

</details>


### [26] [Pluralistic Behavior Suite: Stress-Testing Multi-Turn Adherence to Custom Behavioral Policies](https://arxiv.org/abs/2511.05018)
*Prasoon Varshney,Makesh Narsimhan Sreedhar,Liwei Jiang,Traian Rebedea,Christopher Parisien*

Main category: cs.CL

TL;DR: 提出了 PBSUITE，一个用于评估大语言模型在多回合对话中遵守多元化行为政策的动态测试套件。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型多采用通用安全和使用原则，但实际应用中需要适应不同组织的独特政策和价值观，因此需要针对多元化对齐目标进行系统评估。

Method: 构建包含300条来自30个行业的现实行为政策的数据集，并设计动态评估框架，在对抗性条件下测试模型对定制行为规范的遵守能力。

Result: 发现主流开源和封闭源模型在单回合表现良好（失败率低于4%），但在多回合对抗性交互中遵守行为政策的能力大幅下降（失败率高达84%）。

Conclusion: 现有模型的对齐与安全调节方法难以在真实多回合交互中有效执行多元化行为政策，PBSUITE为进一步研究提供了数据和分析工具。

Abstract: Large language models (LLMs) are typically aligned to a universal set of
safety and usage principles intended for broad public acceptability. Yet,
real-world applications of LLMs often take place within organizational
ecosystems shaped by distinctive corporate policies, regulatory requirements,
use cases, brand guidelines, and ethical commitments. This reality highlights
the need for rigorous and comprehensive evaluation of LLMs with pluralistic
alignment goals, an alignment paradigm that emphasizes adaptability to diverse
user values and needs. In this work, we present PLURALISTIC BEHAVIOR SUITE
(PBSUITE), a dynamic evaluation suite designed to systematically assess LLMs'
capacity to adhere to pluralistic alignment specifications in multi-turn,
interactive conversations. PBSUITE consists of (1) a diverse dataset of 300
realistic LLM behavioral policies, grounded in 30 industries; and (2) a dynamic
evaluation framework for stress-testing model compliance with custom behavioral
specifications under adversarial conditions. Using PBSUITE, We find that
leading open- and closed-source LLMs maintain robust adherence to behavioral
policies in single-turn settings (less than 4% failure rates), but their
compliance weakens substantially in multi-turn adversarial interactions (up to
84% failure rates). These findings highlight that existing model alignment and
safety moderation methods fall short in coherently enforcing pluralistic
behavioral policies in real-world LLM interactions. Our work contributes both
the dataset and analytical framework to support future research toward robust
and context-aware pluralistic alignment techniques.

</details>


### [27] [Order-Level Attention Similarity Across Language Models: A Latent Commonality](https://arxiv.org/abs/2511.05064)
*Jinglin Liang,Jin Zhong,Shuangping Huang,Yunqing Hu,Huiyuan Zhang,Huifang Li,Lixin Fan,Hanlin Gu*

Main category: cs.CL

TL;DR: 本文研究了不同语言模型（LMs）之间的上下文聚合模式的共性，提出了一种无需训练的跨模型适配器方法，提高了未见模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要聚焦于单个模型或注意力头，缺乏对多个语言模型之间上下文聚合共性的系统分析。探索这些共性有助于深化对语言模型的理解，并推动跨模型知识迁移。

Method: 提出了Order-Level Attention（OLA）——通过注意力展开的阶数分解方法，发现不同LMs在相同阶次的OLA表现出显著相似性，且OLA与句法知识隐式关联；基于此，设计了无训练参数的跨语言模型适配器TOA，利用OLA作为统一的句法特征表示，实现适配器在未见模型上的泛化。

Result: 实验证明TOA具有良好的跨模型泛化能力，能有效提升未见语言模型的性能。

Conclusion: 通过揭示不同语言模型之间的上下文聚合共性以及其与句法知识的联系，本文提出的TOA方法实现了无需参数更新的跨模型适配，促进了语言模型的知识共享与性能提升。

Abstract: In this paper, we explore an important yet previously neglected question: Do
context aggregation patterns across Language Models (LMs) share commonalities?
While some works have investigated context aggregation or attention weights in
LMs, they typically focus on individual models or attention heads, lacking a
systematic analysis across multiple LMs to explore their commonalities. In
contrast, we focus on the commonalities among LMs, which can deepen our
understanding of LMs and even facilitate cross-model knowledge transfer. In
this work, we introduce the Order-Level Attention (OLA) derived from the
order-wise decomposition of Attention Rollout and reveal that the OLA at the
same order across LMs exhibits significant similarities. Furthermore, we
discover an implicit mapping between OLA and syntactic knowledge. Based on
these two findings, we propose the Transferable OLA Adapter (TOA), a
training-free cross-LM adapter transfer method. Specifically, we treat the OLA
as a unified syntactic feature representation and train an adapter that takes
OLA as input. Due to the similarities in OLA across LMs, the adapter
generalizes to unseen LMs without requiring any parameter updates. Extensive
experiments demonstrate that TOA's cross-LM generalization effectively enhances
the performance of unseen LMs. Code is available at
https://github.com/jinglin-liang/OLAS.

</details>


### [28] [Reasoning-Guided Claim Normalization for Noisy Multilingual Social Media Posts](https://arxiv.org/abs/2511.05078)
*Manan Sharma,Arya Suneesh,Manish Jain,Pawan Kumar Rajpoot,Prasanna Devadiga,Bharatdeep Hazarika,Ashish Shrivastava,Kishan Gurumurthy,Anshuman B Suresh,Aditya U Baliga*

Main category: cs.CL

TL;DR: 本文提出了一种针对多语言错误信息检测的声明规范化方法，通过系统性地分解社交媒体帖子，实现跨语言的高效迁移，基于仅用英语训练数据进行微调，显著提升了多语言性能。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的多语言错误信息繁杂且表达模糊，如何将其转化为清晰、可验证的声明，以实现有效的跨语言错误信息检测，是本研究的核心动力。

Method: 采用基于谁（Who）、什么（What）、哪里（Where）、何时（When）、为什么（Why）、如何（How）六大问题的系统性分解方法，结合LoRA微调Qwen3-14B模型，利用数据去重、词汇级召回过滤和上下文增强的少样本学习进行推理。

Result: 系统在多语言上表现优异，METEOR分数从英语的41.16到马拉地语的15.21不等，在英语排行榜获得第三名，在荷兰语和旁遮普语中排名第四，较基线方法有41.3%的相对提升。

Conclusion: 方法有效实现了跨语言的泛化，特别是在罗曼语族和日耳曼语族语言上保持了语义连贯性，显著优于现有方法，提高了多语言错误信息检测的准确性。

Abstract: We address claim normalization for multilingual misinformation detection -
transforming noisy social media posts into clear, verifiable statements across
20 languages. The key contribution demonstrates how systematic decomposition of
posts using Who, What, Where, When, Why and How questions enables robust
cross-lingual transfer despite training exclusively on English data. Our
methodology incorporates finetuning Qwen3-14B using LoRA with the provided
dataset after intra-post deduplication, token-level recall filtering for
semantic alignment and retrieval-augmented few-shot learning with contextual
examples during inference. Our system achieves METEOR scores ranging from 41.16
(English) to 15.21 (Marathi), securing third rank on the English leaderboard
and fourth rank for Dutch and Punjabi. The approach shows 41.3% relative
improvement in METEOR over baseline configurations and substantial gains over
existing methods. Results demonstrate effective cross-lingual generalization
for Romance and Germanic languages while maintaining semantic coherence across
diverse linguistic structures.

</details>


### [29] [On Text Simplification Metrics and General-Purpose LLMs for Accessible Health Information, and A Potential Architectural Advantage of The Instruction-Tuned LLM class](https://arxiv.org/abs/2511.05080)
*P. Bilha Githinji,Aikaterini Meilliou,Peiwu Qin*

Main category: cs.CL

TL;DR: 本研究评估了两类通用大型语言模型（LLM）在科学文献自动简化成通俗语言方面的表现，发现指令调优的Mistral 24B在平衡可读性和内容保真度方面优于推理增强的QWen2.5 32B。


<details>
  <summary>Details</summary>
Motivation: 随着公众对生物医学信息的需求增长，需开发可扩展的自动文本简化工具，以促进科学文献的普及。

Method: 通过比较分析指令调优的Mistral 24B和推理增强的QWen2.5 32B，结合21项指标评估包括可读性、内容保真度和安全性等多维度性能。

Result: Mistral 24B在SARI指标（42.46）和BERTScore（0.91）上表现优异，展现出合理的词汇简化策略；QWen2.5虽提高了可读性但在内容保真度上表现较差（BERTScore 0.89）。多指标相关分析揭示了可读性指标间存在功能冗余。

Conclusion: 指令调优的LLM在文本简化任务中更具优势，未来研究应关注词汇层面的领域适应问题，并合理选择评估指标以提高简化效果。

Abstract: The increasing health-seeking behavior and digital consumption of biomedical
information by the general public necessitate scalable solutions for
automatically adapting complex scientific and technical documents into plain
language. Automatic text simplification solutions, including advanced large
language models, however, continue to face challenges in reliably arbitrating
the tension between optimizing readability performance and ensuring
preservation of discourse fidelity. This report empirically assesses the
performance of two major classes of general-purpose LLMs, demonstrating their
linguistic capabilities and foundational readiness for the task compared to a
human benchmark. Using a comparative analysis of the instruction-tuned Mistral
24B and the reasoning-augmented QWen2.5 32B, we identify a potential
architectural advantage in the instruction-tuned LLM. Mistral exhibits a
tempered lexical simplification strategy that enhances readability across a
suite of metrics and the simplification-specific formula SARI (mean 42.46),
while preserving human-level discourse with a BERTScore of 0.91. QWen also
attains enhanced readability performance, but its operational strategy shows a
disconnect in balancing between readability and accuracy, reaching a
statistically significantly lower BERTScore of 0.89. Additionally, a
comprehensive correlation analysis of 21 metrics spanning readability,
discourse fidelity, content safety, and underlying distributional measures for
mechanistic insights, confirms strong functional redundancies among five
readability indices. This empirical evidence tracks baseline performance of the
evolving LLMs for the task of text simplification, identifies the
instruction-tuned Mistral 24B for simplification, provides necessary heuristics
for metric selection, and points to lexical support as a primary
domain-adaptation issue for simplification.

</details>


### [30] [Iterative Layer-wise Distillation for Efficient Compression of Large Language Models](https://arxiv.org/abs/2511.05085)
*Grigory Kovalev,Mikhail Tikhomirov*

Main category: cs.CL

TL;DR: 本文提出了一种基于迭代评估层重要性的改进蒸馏方法，有效压缩大语言模型层数，同时保持较高性能。


<details>
  <summary>Details</summary>
Motivation: 目标是开发性能高、体积小的大语言模型，适合资源受限环境应用，因此需要有效的模型蒸馏技术。

Method: 基于ShortGPT方法，迭代评估每层对性能的影响，通过移除层并衡量性能下降确定重要性，同时结合KL散度和均方误差的联合损失函数进行微调训练。

Result: 在Qwen2.5-3B模型上，模型层数从36减少到28层时仅有9.7%的性能损失，减少到24层时性能损失约18%，验证了中间层对推理贡献较小。

Conclusion: 迭代蒸馏与微调方法有效压缩大语言模型，性能损失有限，适用于资源有限场景，具有较大的应用潜力。

Abstract: This work investigates distillation methods for large language models (LLMs)
with the goal of developing compact models that preserve high performance.
Several existing approaches are reviewed, with a discussion of their respective
strengths and limitations. An improved method based on the ShortGPT approach
has been developed, building upon the idea of incorporating iterative
evaluation of layer importance. At each step, importance is assessed by
measuring performance degradation when individual layers are removed, using a
set of representative datasets. This process is combined with further training
using a joint loss function based on KL divergence and mean squared error.
Experiments on the Qwen2.5-3B model show that the number of layers can be
reduced from 36 to 28 (resulting in a 2.47 billion parameter model) with only a
9.7% quality loss, and to 24 layers with an 18% loss. The findings suggest that
the middle transformer layers contribute less to inference, underscoring the
potential of the proposed method for creating efficient models. The results
demonstrate the effectiveness of iterative distillation and fine-tuning, making
the approach suitable for deployment in resource-limited settings.

</details>


### [31] [A Toolbox for Improving Evolutionary Prompt Search](https://arxiv.org/abs/2511.05120)
*Daniel Grießhaber,Maximilian Kimmich,Johannes Maucher,Ngoc Thang Vu*

Main category: cs.CL

TL;DR: 本文提出了多项改进措施以提升进化提示优化的质量和效率，包括分步骤演化、基于大语言模型的评价、引入人类反馈以及高效的评估策略。


<details>
  <summary>Details</summary>
Motivation: 现有的进化提示优化方法缺乏强健的操作手段和高效的评估机制，限制了优化效果和效率。

Method: 本文将进化过程拆分成独立步骤以增强控制，引入基于大语言模型的评审机制，结合人类反馈优化演化操作符，并设计更高效的评估策略以减少计算开销。

Result: 该方法在提升提示优化质量和效率方面表现优异，并开放了代码供新任务上的提示优化和后续研究使用。

Conclusion: 通过多重创新改进，本文显著增强了进化提示优化技术，推动了提示优化领域的研究和应用发展。

Abstract: Evolutionary prompt optimization has demonstrated effectiveness in refining
prompts for LLMs. However, existing approaches lack robust operators and
efficient evaluation mechanisms. In this work, we propose several key
improvements to evolutionary prompt optimization that can partially generalize
to prompt optimization in general: 1) decomposing evolution into distinct steps
to enhance the evolution and its control, 2) introducing an LLM-based judge to
verify the evolutions, 3) integrating human feedback to refine the evolutionary
operator, and 4) developing more efficient evaluation strategies that maintain
performance while reducing computational overhead. Our approach improves both
optimization quality and efficiency. We release our code, enabling prompt
optimization on new tasks and facilitating further research in this area.

</details>


### [32] [ManufactuBERT: Efficient Continual Pretraining for Manufacturing](https://arxiv.org/abs/2511.05135)
*Robin Armingaud,Romaric Besançon*

Main category: cs.CL

TL;DR: 本文提出了ManufactuBERT，一种针对制造领域持续预训练的RoBERTa模型，通过领域特定语料库和去重处理显著提升制造业相关NLP任务表现，并加快训练速度。


<details>
  <summary>Details</summary>
Motivation: 大型通用Transformer模型在制造等专业领域表现欠佳，原因是缺乏领域特定术语与语义的训练。

Method: 设计了一套从网络数据中提取制造领域语料的数据处理流程，包括领域过滤和多阶段去重，基于此语料对RoBERTa模型进行持续预训练。

Result: ManufactuBERT在多个制造相关NLP任务中取得了新的最高性能，且通过语料去重使训练收敛速度提高33%，显著减少计算资源消耗。

Conclusion: 通过领域定制语料和清洗处理，能有效提升专业领域语言模型的表现和训练效率，该流程对于其他专业领域模型开发具有借鉴价值。

Abstract: While large general-purpose Transformer-based encoders excel at general
language understanding, their performance diminishes in specialized domains
like manufacturing due to a lack of exposure to domain-specific terminology and
semantics. In this paper, we address this gap by introducing ManufactuBERT, a
RoBERTa model continually pretrained on a large-scale corpus curated for the
manufacturing domain. We present a comprehensive data processing pipeline to
create this corpus from web data, involving an initial domain-specific
filtering step followed by a multi-stage deduplication process that removes
redundancies. Our experiments show that ManufactuBERT establishes a new
state-of-the-art on a range of manufacturing-related NLP tasks, outperforming
strong specialized baselines. More importantly, we demonstrate that training on
our carefully deduplicated corpus significantly accelerates convergence,
leading to a 33\% reduction in training time and computational cost compared to
training on the non-deduplicated dataset. The proposed pipeline offers a
reproducible example for developing high-performing encoders in other
specialized domains. We will release our model and curated corpus at
https://huggingface.co/cea-list-ia.

</details>


### [33] [Mind the Gap... or Not? How Translation Errors and Evaluation Details Skew Multilingual Results](https://arxiv.org/abs/2511.05162)
*Jan-Thorsten Peter,David Vilar,Tobias Domhan,Dan Malkin,Markus Freitag*

Main category: cs.CL

TL;DR: 本文以数学领域为例，研究了大型语言模型（LLMs）在多语言环境中的表现差异，发现原有数据集存在翻译错误和回答抽取问题，校正后语言差距基本消失。


<details>
  <summary>Details</summary>
Motivation: 当前多语言大型语言模型在不同语言和领域表现存在显著差异，研究者希望揭示这些差异的真实原因及改进方法。

Method: 通过分析多语言数学基准数据集（MGSM），发现并修正其中的翻译错误，提出自动质量保证方法，并改进答案抽取流程。

Result: 修正数据集和改进答案抽取后，语言表现差异大幅减少，原先认为存在的跨语言性能差异基本消失。

Conclusion: 数据质量和评测方法会显著影响多语言模型的表现评价，改进这些环节有助于准确评估和提升模型的跨语言能力；同时发布了修正后的数据集供社区使用。

Abstract: Most current large language models (LLMs) support a wide variety of languages
in addition to English, including high-resource languages (e.g. German,
Chinese, French), as well as low-resource ones (e.g. Swahili, Telugu). In
addition they have also shown impressive capabilities in different domains,
like coding, science and math. In this short paper, taking math as an example
domain, we study the performance of different LLMs across languages.
Experimental results show that there exists a non-negligible and consistent gap
in the performance of the models across languages. Interestingly, and somewhat
against expectations, the gap exists for both high- and low-resource languages.
We hope that these results influence further research into cross-lingual
capability generalization for next generation LLMs. If it weren't for the fact
that they are false! By analyzing one of the standard multilingual math
benchmarks (MGSM), we determine that several translation errors are present in
the data. Furthermore, the lack of standardized answer extraction from LLM
outputs further influences the final results. We propose a method for automatic
quality assurance to address the first issue at scale, and give recommendations
to address the second one. Combining these two approaches we show that the
aforementioned language gap mostly disappears, leading to completely different
conclusions from our research. We additionally release the corrected dataset to
the community.

</details>


### [34] [Effectiveness of Chain-of-Thought in Distilling Reasoning Capability from Large Language Models](https://arxiv.org/abs/2511.05184)
*Cong-Thanh Do,Rama Doddipatla,Kate Knill*

Main category: cs.CL

TL;DR: 本文研究了思维链提示（CoT）在知识蒸馏中提升大型语言模型推理能力的作用，证明通过白盒知识蒸馏使用CoT，可以显著提升小型模型在自然语言推理和理解任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 提升小型语言模型在复杂自然语言推理任务中的性能，借助大型模型的推理能力进行知识迁移。

Method: 通过白盒知识蒸馏方法，利用CoT-Collection数据集中的思维链数据，将Qwen和Llama2系列大型模型的推理能力蒸馏到小型模型。

Result: 蒸馏后的模型在BIG-Bench-Hard（BBH）基准测试中的自然语言推理和理解任务上表现更优，证明CoT在知识蒸馏中的有效性。

Conclusion: CoT提示在白盒知识蒸馏过程中能有效提升小型语言模型的推理能力，提高其在复杂任务中的表现。

Abstract: Chain-of-Thought (CoT) prompting is a widely used method to improve the
reasoning capability of Large Language Models (LLMs). More recently, CoT has
been leveraged in Knowledge Distillation (KD) to transfer reasoning capability
from a larger LLM to a smaller one. This paper examines the role of CoT in
distilling the reasoning capability from larger LLMs to smaller LLMs using
white-box KD, analysing its effectiveness in improving the performance of the
distilled models for various natural language reasoning and understanding
tasks. We conduct white-box KD experiments using LLMs from the Qwen and Llama2
families, employing CoT data from the CoT-Collection dataset. The distilled
models are then evaluated on natural language reasoning and understanding tasks
from the BIG-Bench-Hard (BBH) benchmark, which presents complex challenges for
smaller LLMs. Experimental results demonstrate the role of CoT in improving
white-box KD effectiveness, enabling the distilled models to achieve better
average performance in natural language reasoning and understanding tasks from
BBH.

</details>


### [35] [Translation via Annotation: A Computational Study of Translating Classical Chinese into Japanese](https://arxiv.org/abs/2511.05239)
*Zilong Li,Jie Cao*

Main category: cs.CL

TL;DR: 本文研究了将古汉语翻译成日语的字符标注任务，提出了一种基于大型语言模型(LLM)的注释流水线，并构建了新的数据集。实验表明，在低资源环境下，引入辅助中文NLP任务能提升序列标注任务的效果，LLM在直接翻译上表现优异，但在字符注释上表现不稳定。


<details>
  <summary>Details</summary>
Motivation: 古人通过对每个字符的注释来完成古汉语向日语的翻译，这一过程可以抽象为序列标注任务，但面临低资源挑战。

Method: 引入基于大型语言模型的注释流水线，构建数字化的开源翻译数据集，并结合辅助中文NLP任务进行训练。

Result: 辅助中文NLP任务提升了序列标注训练效果，LLM在机器翻译任务中表现较好，但在字符注释任务中表现不稳定。

Conclusion: 提出的方法可作为大型语言模型在古汉语字符注释任务中的有效补充，解决低资源带来的挑战。

Abstract: Ancient people translated classical Chinese into Japanese by annotating
around each character. We abstract this process as sequence tagging tasks and
fit them into modern language technologies. The research of this annotation and
translation system is a facing low-resource problem. We release this problem by
introducing a LLM-based annotation pipeline and construct a new dataset from
digitalized open-source translation data. We show that under the low-resource
setting, introducing auxiliary Chinese NLP tasks has a promoting effect on the
training of sequence tagging tasks. We also evaluate the performance of large
language models. They achieve high scores in direct machine translation, but
they are confused when being asked to annotate characters. Our method could
work as a supplement of LLMs.

</details>


### [36] [Reflective Personalization Optimization: A Post-hoc Rewriting Framework for Black-Box Large Language Models](https://arxiv.org/abs/2511.05286)
*Teqi Hao,Xioayu Tan,Shaojie Shi,Yinghui Xu,Xihe Qiu*

Main category: cs.CL

TL;DR: 本文提出了Reflective Personalization Optimization (RPO)框架，通过分离内容生成和个性化调整，有效提升大语言模型的个性化水平。


<details>
  <summary>Details</summary>
Motivation: 现有个性化方法多依赖上下文注入，导致生成内容准确性与用户风格对齐存在权衡，限制了输出质量和控制精度。

Method: RPO分两阶段工作：先用基础模型生成通用回复，再由外部反思模块重写以符合用户偏好。反思模块通过结构化监督微调和强化学习训练。

Result: 在LaMP基准测试中，RPO明显优于现有方法，证明了显式响应塑造优于隐式上下文注入。

Conclusion: RPO提出了高效且与模型无关的个性化层，可无缝集成任意基础模型，为用户中心生成开辟新方向。

Abstract: The personalization of black-box large language models (LLMs) is a critical
yet challenging task. Existing approaches predominantly rely on context
injection, where user history is embedded into the prompt to directly guide the
generation process. However, this single-step paradigm imposes a dual burden on
the model: generating accurate content while simultaneously aligning with
user-specific styles. This often results in a trade-off that compromises output
quality and limits precise control. To address this fundamental tension, we
propose Reflective Personalization Optimization (RPO), a novel framework that
redefines the personalization paradigm by decoupling content generation from
alignment. RPO operates in two distinct stages: first, a base model generates a
high-quality, generic response; then, an external reflection module explicitly
rewrites this output to align with the user's preferences. This reflection
module is trained using a two-stage process. Initially, supervised fine-tuning
is employed on structured rewriting trajectories to establish a core
personalized reasoning policy that models the transformation from generic to
user-aligned responses. Subsequently, reinforcement learning is applied to
further refine and enhance the quality of the personalized outputs.
Comprehensive experiments on the LaMP benchmark demonstrate that RPO, by
decoupling content generation from personalization, significantly outperforms
state-of-the-art baselines. These findings underscore the superiority of
explicit response shaping over implicit context injection. Moreover, RPO
introduces an efficient, model-agnostic personalization layer that can be
seamlessly integrated with any underlying base model, paving the way for a new
and effective direction in user-centric generation scenarios.

</details>


### [37] [Listening Between the Lines: Decoding Podcast Narratives with Language Modeling](https://arxiv.org/abs/2511.05310)
*Shreya Gupta,Ojasva Saxena,Arghodeep Nandi,Sarah Masud,Kiran Garimella,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 本文提出了一种基于BERT模型的叙事框架标注方法，以应对播客内容的结构复杂性，实现对播客叙事的自动化精准分析。


<details>
  <summary>Details</summary>
Motivation: 播客作为塑造公共舆论的重要平台，其内容多样且无脚本，传统的大型语言模型难以捕捉其细微的叙事框架，影响对传播影响力的理解。

Method: 开发并微调一个BERT模型，将叙事框架与对话中的具体实体关联，结合细粒度框架标签和高层次话题进行关联分析。

Result: 新方法更贴合人类对复杂对话数据的判断，能够系统揭示话题与叙事框架之间的关系。

Conclusion: 该研究提供了一种更稳健的数字媒体影响力分析框架，有助于自动化理解播客中的信息传播与说服机制。

Abstract: Podcasts have become a central arena for shaping public opinion, making them
a vital source for understanding contemporary discourse. Their typically
unscripted, multi-themed, and conversational style offers a rich but complex
form of data. To analyze how podcasts persuade and inform, we must examine
their narrative structures -- specifically, the narrative frames they employ.
  The fluid and conversational nature of podcasts presents a significant
challenge for automated analysis. We show that existing large language models,
typically trained on more structured text such as news articles, struggle to
capture the subtle cues that human listeners rely on to identify narrative
frames. As a result, current approaches fall short of accurately analyzing
podcast narratives at scale.
  To solve this, we develop and evaluate a fine-tuned BERT model that
explicitly links narrative frames to specific entities mentioned in the
conversation, effectively grounding the abstract frame in concrete details. Our
approach then uses these granular frame labels and correlates them with
high-level topics to reveal broader discourse trends. The primary contributions
of this paper are: (i) a novel frame-labeling methodology that more closely
aligns with human judgment for messy, conversational data, and (ii) a new
analysis that uncovers the systematic relationship between what is being
discussed (the topic) and how it is being presented (the frame), offering a
more robust framework for studying influence in digital media.

</details>


### [38] [What Are the Facts? Automated Extraction of Court-Established Facts from Criminal-Court Opinions](https://arxiv.org/abs/2511.05320)
*Klára Bendová,Tomáš Knap,Jan Černý,Vojtěch Pour,Jaromir Savelka,Ivana Kvapilíková,Jakub Drápal*

Main category: cs.CL

TL;DR: 本文研究了从斯洛伐克公开法庭判决书中提取犯罪行为描述的可行性，利用正则表达式和大型语言模型两种方法，取得了较高的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 刑事司法行政数据中犯罪行为信息有限，而大陆欧洲法院的判决书中包含丰富的犯罪行为描述信息，这部分数据尚未被充分利用。

Method: 采用基线正则表达式方法提取词汇，进一步优化正则表达式处理特定的词汇分隔方式，并利用大型语言模型Gemini Flash 2.0通过预定义指令提取犯罪描述。将两种方法组合以提高效果。

Result: 基线方法仅识别40.5%判决书中的描述，改进后正则表达式达97%，大型语言模型达98.75%，结合方法达99.5%。法律学生评估表明高级方法与人工标注一致率约90%，基线仅34.5%；大型语言模型完全匹配率91.75%，组合方法92%。

Conclusion: 结合正则表达式和大型语言模型可有效自动提取法院判决中犯罪行为描述，达到接近人工标注的准确性，展现出良好应用前景。

Abstract: Criminal justice administrative data contain only a limited amount of
information about the committed offense. However, there is an unused source of
extensive information in continental European courts' decisions: descriptions
of criminal behaviors in verdicts by which offenders are found guilty. In this
paper, we study the feasibility of extracting these descriptions from publicly
available court decisions from Slovakia. We use two different approaches for
retrieval: regular expressions and large language models (LLMs). Our baseline
was a simple method employing regular expressions to identify typical words
occurring before and after the description. The advanced regular expression
approach further focused on "sparing" and its normalization (insertion of
spaces between individual letters), typical for delineating the description.
The LLM approach involved prompting the Gemini Flash 2.0 model to extract the
descriptions using predefined instructions. Although the baseline identified
descriptions in only 40.5% of verdicts, both methods significantly outperformed
it, achieving 97% with advanced regular expressions and 98.75% with LLMs, and
99.5% when combined. Evaluation by law students showed that both advanced
methods matched human annotations in about 90% of cases, compared to just 34.5%
for the baseline. LLMs fully matched human-labeled descriptions in 91.75% of
instances, and a combination of advanced regular expressions with LLMs reached
92%.

</details>


### [39] [Evaluating Subword Tokenization Techniques for Bengali: A Benchmark Study with BengaliBPE](https://arxiv.org/abs/2511.05324)
*Firoj Ahmmed Patwary,Abdullah Al Noman*

Main category: cs.CL

TL;DR: 本文提出了一种针对孟加拉语的Byte Pair Encoding分词器BengaliBPE，旨在改善当前分词器在形态丰富语言上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有子词分词器如SentencePiece和HuggingFace BPE主要针对拉丁语系设计，在形态复杂的孟加拉语表现不佳。

Method: BengaliBPE结合Unicode规范化、字素级初始化和形态感知的合并规则，保持语言一致性和子词完整性。

Result: 在大型孟加拉新闻分类数据集上，BengaliBPE实现了更细粒度分词和更好的形态解释性，且分类准确率优于三种基线方法，尽管计算成本略高。

Conclusion: 语言感知的分词方法对形态丰富语言至关重要，BengaliBPE为未来孟加拉语NLP及大规模预训练提供了坚实基础。

Abstract: Tokenization is an important first step in Natural Language Processing (NLP)
pipelines because it decides how models learn and represent linguistic
information. However, current subword tokenizers like SentencePiece or
HuggingFace BPE are mostly designed for Latin or multilingual corpora and do
not perform well on languages with rich morphology such as Bengali. To address
this limitation, we present BengaliBPE, a Byte Pair Encoding (BPE) tokenizer
specifically developed for the Bengali script. BengaliBPE applies Unicode
normalization, grapheme-level initialization, and morphology-aware merge rules
to maintain linguistic consistency and preserve subword integrity. We use a
large-scale Bengali news classification dataset to compare BengaliBPE with
three baselines: Whitespace, SentencePiece BPE, and HuggingFace BPE. The
evaluation considers tokenization granularity, encoding speed, and downstream
classification accuracy. While all methods perform reasonably well, BengaliBPE
provides the most detailed segmentation and the best morphological
interpretability, albeit with slightly higher computational cost. These
findings highlight the importance of language-aware tokenization for
morphologically rich scripts and establish BengaliBPE as a strong foundation
for future Bengali NLP systems, including large-scale pretraining of contextual
language models.

</details>


### [40] [A multimodal multiplex of the mental lexicon for multilingual individuals](https://arxiv.org/abs/2511.05361)
*Maria Huynh,Wilder C. Rodrigues*

Main category: cs.CL

TL;DR: 本研究探讨多语言者的心理词汇结构，结合多层网络模型，引入视觉输入层，研究遗传语言对其他语言习得的影响。


<details>
  <summary>Details</summary>
Motivation: 近年来研究发现多语言者在语言和认知任务中表现优于单语者，促使对多语言词汇系统心理结构的深入研究。

Method: 基于多层网络模型，结合BIA+框架，设计包含视觉输入的多模态实验，以考察视觉信息是否影响翻译任务中的语言表现。

Result: 尚未提供具体结果，研究重点在于验证视觉输入在翻译任务中对语言能力和准确性的影响。

Conclusion: 旨在揭示视觉输入在多语言词汇系统中的作用，进一步理解遗传语言如何影响第二语言习得。

Abstract: Historically, bilingualism was often perceived as an additional cognitive
load that could hinder linguistic and intellectual development. However, over
the last three decades, this view has changed considerably. Numerous studies
have aimed to model and understand the architecture of the bilingual word
recognition system Dijkstra and van Heuven (2002), investigating how parallel
activation operates in the brain and how one language influences another Kroll
et al. (2015). Increasingly, evidence suggests that multilinguals, individuals
who speak three or more languages, can perform better than monolinguals in
various linguistic and cognitive tasks, such as learning an additional language
Abu-Rabia and Sanitsky (2010). This research proposal focuses on the study of
the mental lexicon and how it may be structured in individuals who speak
multiple languages. Building on the work of Stella et al. (2018), who
investigated explosive learning in humans using a multiplex model of the mental
lexicon, and the Bilingual Interactive Activation (BIA+) framework proposed by
Dijkstra and van Heuven (2002), the present study applies the same multilayer
network principles introduced by Kivela et al. (2014). Our experimental design
extends previous research by incorporating multimodality into the multiplex
model, introducing an additional layer that connects visual inputs to their
corresponding lexical representations across the multilingual layers of the
mental lexicon. In this research, we aim to explore how a heritage language
influences the acquisition of another language. Specifically, we ask: Does the
presence of visual input in a translation task influence participants'
proficiency and accuracy compared to text-only conditions?

</details>


### [41] [Large Language Models for Explainable Threat Intelligence](https://arxiv.org/abs/2511.05406)
*Tiago Dinis,Miguel Correia,Roger Tavares*

Main category: cs.CL

TL;DR: 本文提出了利用结合实时信息检索和特定领域数据的检索增强生成(RAG)技术，开发了一个用于网络威胁情报获取的系统RAGRecon，并通过知识图谱增强其可解释性，实验结果显示该系统在七种大型语言模型中表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着网络威胁的不断复杂化，传统安全机制难以应对，利用大型语言模型的文本处理和生成能力来提升网络安全威胁情报的获取能力成为必要。

Method: 本文提出的RAGRecon系统结合了大型语言模型与检索增强生成技术，通过实时信息检索和领域特定数据整合回答关于网络安全威胁的问题，并通过生成知识图谱实现AI解释性。

Result: 在两个数据集上测试了七种大型语言模型，最佳模型组合的回答与参考答案匹配率超过了91%。

Conclusion: RAGRecon系统有效提升了网络安全威胁情报获取的准确性和模型推理的透明度，为安全分析师提供了更易理解和信赖的智能辅助工具。

Abstract: As cyber threats continue to grow in complexity, traditional security
mechanisms struggle to keep up. Large language models (LLMs) offer significant
potential in cybersecurity due to their advanced capabilities in text
processing and generation. This paper explores the use of LLMs with
retrieval-augmented generation (RAG) to obtain threat intelligence by combining
real-time information retrieval with domain-specific data. The proposed system,
RAGRecon, uses a LLM with RAG to answer questions about cybersecurity threats.
Moreover, it makes this form of Artificial Intelligence (AI) explainable by
generating and visually presenting to the user a knowledge graph for every
reply. This increases the transparency and interpretability of the reasoning of
the model, allowing analysts to better understand the connections made by the
system based on the context recovered by the RAG system. We evaluated RAGRecon
experimentally with two datasets and seven different LLMs and the responses
matched the reference responses more than 91% of the time for the best
combinations.

</details>


### [42] [Minority-Aware Satisfaction Estimation in Dialogue Systems via Preference-Adaptive Reinforcement Learning](https://arxiv.org/abs/2511.05407)
*Yahui Fu,Zi Haur Pang,Tatsuya Kawahara*

Main category: cs.CL

TL;DR: 本文提出了一种统一框架，通过链式个性化推理和无监督群体偏好聚类，结合偏好自适应强化学习，提升对话系统中用户满意度的评估效果，尤其改善了少数群体用户的满意度估计。


<details>
  <summary>Details</summary>
Motivation: 现有对话系统的用户满意度评估方法通常训练单一模型，忽视了不同用户群体尤其是少数群体的个性化偏好，导致满意度评估存在偏差。

Method: 提出了链式个性化推理(CoPeR)捕捉个体偏好，基于期望最大化的多元-少数群体偏好感知聚类(M2PC)发现不同用户群体，再结合偏好自适应强化学习(PAda-PPO)联合优化个体与群体偏好。

Result: 在情感支持对话数据集上的实验表明，该方法在用户满意度估计上表现一致提升，尤其对少数群体用户满意度估计效果有显著提高。

Conclusion: 本文框架有效融合个体和群体偏好，实现了更精确的用户满意度评估，具有较强的实用价值和推广潜力。

Abstract: User satisfaction in dialogue systems is inherently subjective. When the same
response strategy is applied across users, minority users may assign different
satisfaction ratings than majority users due to variations in individual
intents and preferences. However, existing alignment methods typically train
one-size-fits-all models that aim for broad consensus, often overlooking
minority perspectives and user-specific adaptation. We propose a unified
framework that models both individual- and group-level preferences for user
satisfaction estimation. First, we introduce Chain-of-Personalized-Reasoning
(CoPeR) to capture individual preferences through interpretable reasoning
chains. Second, we propose an expectation-maximization-based Majority-Minority
Preference-Aware Clustering (M2PC) algorithm that discovers distinct user
groups in an unsupervised manner to learn group-level preferences. Finally, we
integrate these components into a preference-adaptive reinforcement learning
framework (PAda-PPO) that jointly optimizes alignment with both individual and
group preferences. Experiments on the Emotional Support Conversation dataset
demonstrate consistent improvements in user satisfaction estimation,
particularly for underrepresented user groups.

</details>


### [43] [Steering Language Models with Weight Arithmetic](https://arxiv.org/abs/2511.05408)
*Constanza Fierro,Fabien Roger*

Main category: cs.CL

TL;DR: 本文提出了一种称为对比性权重引导的新方法，通过权重算术调整大语言模型的参数，以高效利用有限的训练数据，实现对模型行为的精细控制。


<details>
  <summary>Details</summary>
Motivation: 在训练大语言模型时，提供高质量的多样化反馈既困难又昂贵，而仅在狭窄分布上的反馈会导致意外的泛化问题。为此，作者希望开发一种方法，以更好地利用狭窄的训练数据来调整模型行为。

Method: 提出通过对两次小规模微调的权重增量进行相减，提取出行为方向，再通过加减该方向来修改模型参数，称为对比性权重引导。这种方法在后训练阶段应用，通过权重空间操作实现行为的改变。

Result: 权重引导在缓解拍马屁行为和引入错误行为中表现出优于激活引导的泛化能力，且在保持模型总体能力的同时实现更强的行为控制。此外，权重引导能缓解微调过程中的不良行为漂移，同时保持任务性能。还发现了利用权重方向相似度检测潜在错误行为的初步证据。

Conclusion: 对比性权重引导是一种有效且简单的后训练技术，能更好地控制大语言模型行为，减少不良泛化和行为漂移，并且有望实现训练中错误行为的早期监测。该方法为安全和定制化语言模型提供了新的思路和工具。

Abstract: Providing high-quality feedback to Large Language Models (LLMs) on a diverse
training distribution can be difficult and expensive, and providing feedback
only on a narrow distribution can result in unintended generalizations. To
better leverage narrow training data, we propose contrastive weight steering, a
simple post-training method that edits the model parameters using weight
arithmetic. We isolate a behavior direction in weight-space by subtracting the
weight deltas from two small fine-tunes -- one that induces the desired
behavior and another that induces its opposite -- and then add or remove this
direction to modify the model's weights. We apply this technique to mitigate
sycophancy and induce misalignment, and find that weight steering often
generalizes further than activation steering, achieving stronger
out-of-distribution behavioral control before degrading general capabilities.
We also show that, in the context of task-specific fine-tuning, weight steering
can partially mitigate undesired behavioral drift: it can reduce sycophancy and
under-refusals introduced during fine-tuning while preserving task performance
gains. Finally, we provide preliminary evidence that emergent misalignment can
be detected by measuring the similarity between fine-tuning updates and an
"evil" weight direction, suggesting that it may be possible to monitor the
evolution of weights during training and detect rare misaligned behaviors that
never manifest during training or evaluations.

</details>


### [44] [MIMIC-SR-ICD11: A Dataset for Narrative-Based Diagnosis](https://arxiv.org/abs/2511.05485)
*Yuexin Wu,Shiqi Wang,Vasile Rus*

Main category: cs.CL

TL;DR: 本文提出了MIMIC-SR-ICD11数据集和基于似然的排序框架LL-Rank，用于提升基于电子健康记录的疾病诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 传统电子健康记录往往忽略自我报告中的临床关键信息，导致诊断信息缺失。为解决该问题，作者提出构建新的数据集并设计新的算法方法。

Method: 构建大规模基于WHO ICD-11术语的诊断数据集MIMIC-SR-ICD11，设计LL-Rank框架计算归一化联合似然，减去无报告的先验概率，从而对诊断标签进行重排序。

Result: 在七种模型基线上，LL-Rank相较于强基线GenMap持续提升性能，消融实验表明其基于点互信息（PMI）的方法有效减少了标签频率偏差影响。

Conclusion: 利用自我报告构建的诊断数据集和基于似然的排序方法能显著提升疾病诊断的准确性，改善临床决策支持系统。

Abstract: Disease diagnosis is a central pillar of modern healthcare, enabling early
detection and timely intervention for acute conditions while guiding lifestyle
adjustments and medication regimens to prevent or slow chronic disease.
Self-reports preserve clinically salient signals that templated electronic
health record (EHR) documentation often attenuates or omits, especially subtle
but consequential details. To operationalize this shift, we introduce
MIMIC-SR-ICD11, a large English diagnostic dataset built from EHR discharge
notes and natively aligned to WHO ICD-11 terminology. We further present
LL-Rank, a likelihood-based re-ranking framework that computes a
length-normalized joint likelihood of each label given the clinical report
context and subtracts the corresponding report-free prior likelihood for that
label. Across seven model backbones, LL-Rank consistently outperforms a strong
generation-plus-mapping baseline (GenMap). Ablation experiments show that
LL-Rank's gains primarily stem from its PMI-based scoring, which isolates
semantic compatibility from label frequency bias.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [45] [TAMAS: Benchmarking Adversarial Risks in Multi-Agent LLM Systems](https://arxiv.org/abs/2511.05269)
*Ishan Kavathekar,Hemang Jain,Ameya Rathod,Ponnurangam Kumaraguru,Tanuja Ganu*

Main category: cs.MA

TL;DR: 本文提出了用于评估多智能体大语言模型系统安全性的基准TAMAS，揭示了该类系统易受攻击的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体大语言模型系统在安全性方面研究不足，现有评测多聚焦于单智能体，缺乏针对多智能体协作的安全测试。

Method: 构建了TAMAS基准，包含300个对抗性实例、六类攻击方式和211个工具，涵盖五个场景，并设计有效鲁棒性评分指标评估安全与任务效率的平衡。

Result: 通过对十个主流大语言模型和三种多智能体交互设置的测试，发现多智能体系统易受多种攻击影响，存在显著安全隐患。

Conclusion: 多智能体大语言模型系统安全风险高，亟需开发更强防御机制，TAMAS为系统研究和提升安全性提供了基础平台和评测标准。

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities as
autonomous agents through tool use, planning, and decision-making abilities,
leading to their widespread adoption across diverse tasks. As task complexity
grows, multi-agent LLM systems are increasingly used to solve problems
collaboratively. However, safety and security of these systems remains largely
under-explored. Existing benchmarks and datasets predominantly focus on
single-agent settings, failing to capture the unique vulnerabilities of
multi-agent dynamics and co-ordination. To address this gap, we introduce
$\textbf{T}$hreats and $\textbf{A}$ttacks in $\textbf{M}$ulti-$\textbf{A}$gent
$\textbf{S}$ystems ($\textbf{TAMAS}$), a benchmark designed to evaluate the
robustness and safety of multi-agent LLM systems. TAMAS includes five distinct
scenarios comprising 300 adversarial instances across six attack types and 211
tools, along with 100 harmless tasks. We assess system performance across ten
backbone LLMs and three agent interaction configurations from Autogen and
CrewAI frameworks, highlighting critical challenges and failure modes in
current multi-agent deployments. Furthermore, we introduce Effective Robustness
Score (ERS) to assess the tradeoff between safety and task effectiveness of
these frameworks. Our findings show that multi-agent systems are highly
vulnerable to adversarial attacks, underscoring the urgent need for stronger
defenses. TAMAS provides a foundation for systematically studying and improving
the safety of multi-agent LLM systems.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [46] [Agentic Refactoring: An Empirical Study of AI Coding Agents](https://arxiv.org/abs/2511.04824)
*Kosei Horikawa,Hao Li,Yutaro Kashiwa,Bram Adams,Hajimu Iida,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 本文通过对开源Java项目中AI代理生成的代码重构进行了大规模实证研究，揭示了代理重构的特点、动机及其对代码质量的影响。


<details>
  <summary>Details</summary>
Motivation: 虽然AI编码工具如OpenAI Codex在自动重构中的应用日益普及，但缺乏关于代理重构实际应用情况及其与人工重构对比的实证理解。

Method: 基于AIDev数据集，分析了12,256个拉取请求和14,988个提交中的15,451次重构，定量评估了重构类型、动机及对代码结构指标的影响。

Result: 代理重构主要为低级一致性编辑（如重命名变量），占26.1%提交，动机集中于提升代码可维护性和可读性，且在结构指标上带来统计学显著但幅度较小的改善。

Conclusion: AI代理重构是软件开发中一个常见且有目的的活动，侧重内部质量提升，虽改善有限但有效，表明其在未来软件工程中具有重要价值。

Abstract: Agentic coding tools, such as OpenAI Codex, Claude Code, and Cursor, are
transforming the software engineering landscape. These AI-powered systems
function as autonomous teammates capable of planning and executing complex
development tasks. Agents have become active participants in refactoring, a
cornerstone of sustainable software development aimed at improving internal
code quality without altering observable behavior. Despite their increasing
adoption, there is a critical lack of empirical understanding regarding how
agentic refactoring is utilized in practice, how it compares to human-driven
refactoring, and what impact it has on code quality. To address this empirical
gap, we present a large-scale study of AI agent-generated refactorings in
real-world open-source Java projects, analyzing 15,451 refactoring instances
across 12,256 pull requests and 14,988 commits derived from the AIDev dataset.
Our empirical analysis shows that refactoring is a common and intentional
activity in this development paradigm, with agents explicitly targeting
refactoring in 26.1% of commits. Analysis of refactoring types reveals that
agentic efforts are dominated by low-level, consistency-oriented edits, such as
Change Variable Type (11.8%), Rename Parameter (10.4%), and Rename Variable
(8.5%), reflecting a preference for localized improvements over the high-level
design changes common in human refactoring. Additionally, the motivations
behind agentic refactoring focus overwhelmingly on internal quality concerns,
with maintainability (52.5%) and readability (28.1%). Furthermore, quantitative
evaluation of code quality metrics shows that agentic refactoring yields small
but statistically significant improvements in structural metrics, particularly
for medium-level changes, reducing class size and complexity (e.g., Class LOC
median $\Delta$ = -15.25).

</details>


### [47] [Software Defined Vehicle Code Generation: A Few-Shot Prompting Approach](https://arxiv.org/abs/2511.04849)
*Quang-Dung Nguyen,Tri-Dung Tran,Thanh-Hieu Chu,Hoang-Loc Tran,Xiangwei Cheng,Dirk Slama*

Main category: cs.SE

TL;DR: 本文提出使用系统提示语通过高级提示工程技巧调整大语言模型（LLMs）以生成软件定义车辆（SDV）代码，无需模型训练，取得了较好效果。


<details>
  <summary>Details</summary>
Motivation: 软件定义车辆的发展要求高效的代码生成工具，但通用大语言模型因架构限制难以直接适配SDV代码生成。

Method: 使用系统提示语和高级提示工程设计提示结构，对不同模型使用无提示、少量示例提示多种方法进行实验，评估其在SDV代码生成上的表现。

Result: 采用少量示例提示策略的大语言模型在多个量化指标上表现最佳，有效调整输出以符合预期。

Conclusion: 基于高级提示工程的少量示例提示方法可显著提升大语言模型在特定领域如SDV代码生成的性能，无需基础模型训练或架构访问。

Abstract: The emergence of Software-Defined Vehicles (SDVs) marks a paradigm shift in
the automotive industry, where software now plays a pivotal role in defining
vehicle functionality, enabling rapid innovation of modern vehicles. Developing
SDV-specific applications demands advanced tools to streamline code generation
and improve development efficiency. In recent years, general-purpose large
language models (LLMs) have demonstrated transformative potential across
domains. Still, restricted access to proprietary model architectures hinders
their adaption to specific tasks like SDV code generation. In this study, we
propose using prompts, a common and basic strategy to interact with LLMs and
redirect their responses. Using only system prompts with an appropriate and
efficient prompt structure designed using advanced prompt engineering
techniques, LLMs can be crafted without requiring a training session or access
to their base design. This research investigates the extensive experiments on
different models by applying various prompting techniques, including bare
models, using a benchmark specifically created to evaluate LLMs' performance in
generating SDV code. The results reveal that the model with a few-shot
prompting strategy outperforms the others in adjusting the LLM answers to match
the expected outcomes based on quantitative metrics.

</details>


### [48] [What About Our Bug? A Study on the Responsiveness of NPM Package Maintainers](https://arxiv.org/abs/2511.04986)
*Mohammadreza Saeidi,Ethan Thoma,Raula Gaikovina Kula,Gema Rodríguez-Pérez*

Main category: cs.SE

TL;DR: 研究调查了 npm 生态中维护者对 30,340 个缺陷报告的响应率，发现维护者响应率中位数为 70%，但部分缺陷因责任归属、贡献规范等原因未被解决。


<details>
  <summary>Details</summary>
Motivation: 由于第三方库的链式依赖，单个库的缺陷可能影响大量下游库。维护者可能不会修复某些缺陷，尤其是在责任归属模糊的情况下。

Method: 采用混合方法，挖掘 500 个最依赖库中 30,340 个缺陷报告的数据，并通过定性开放编码分析未被解决缺陷的原因。

Result: 维护者总体响应积极，项目响应率中位数为 70%。未解决缺陷背后的原因包括贡献实践、依赖限制及库的特定标准。

Conclusion: 提出了未解决缺陷原因的分类法，认识维护者行为有助于制定更有效的开源生态支持策略，提升生态系统的健壮性和响应能力。

Abstract: Background: Widespread use of third-party libraries makes ecosystems like
Node Package Manager (npm) critical to modern software development. However,
this interconnected chain of dependencies also creates challenges: bugs in one
library can propagate downstream, potentially impacting many other libraries
that rely on it. We hypothesize that maintainers may not always decide to fix a
bug, especially if the maintainer decides it falls out of their responsibility
within the chain of dependencies. Aims: To confirm this hypothesis, we
investigate the responsiveness of 30,340 bug reports across 500 of the most
depended-upon npm packages. Method: We adopt a mixed-method approach to mine
repository issue data and perform qualitative open coding to analyze reasons
behind unaddressed bug reports. Results: Our findings show that maintainers are
generally responsive, with a median project-level responsiveness of 70% (IQR:
55%-89%), reflecting their commitment to support downstream developers.
Conclusions: We present a taxonomy of the reasons some bugs remain unresolved.
The taxonomy includes contribution practices, dependency constraints, and
library-specific standards as reasons for not being responsive. Understanding
maintainer behavior can inform practices that promote a more robust and
responsive open-source ecosystem that benefits the entire community.

</details>


### [49] [Generating Software Architecture Description from Source Code using Reverse Engineering and Large Language Model](https://arxiv.org/abs/2511.05165)
*Ahmad Hatahet,Christoph Knieke,Andreas Rausch*

Main category: cs.SE

TL;DR: 提出了一种结合反向工程技术和大型语言模型（LLM）的半自动生成软件架构描述(SAD)的方法，实现从源码恢复静态和行为架构视图，提高文档维护性和系统理解度。


<details>
  <summary>Details</summary>
Motivation: 目前软件架构描述常常缺失、过时或与实际实现不符，开发者需要从源码中获取架构信息，过程繁琐且增加认知负担。

Method: 通过反向工程提取组件图，利用提示工程筛选核心组件，用few-shot提示生成状态机图来建模组件行为，结合LLM自动生成SAD。

Result: 该方法在C++示例中有效抽象了组件图并准确表现复杂行为，减少对专家参与依赖，提升文档的可维护性和系统理解。

Conclusion: 结合LLM的半自动SAD生成为减少人工工作量、提升系统长期可维护性提供了可行路径。

Abstract: Software Architecture Descriptions (SADs) are essential for managing the
inherent complexity of modern software systems. They enable high-level
architectural reasoning, guide design decisions, and facilitate effective
communication among diverse stakeholders. However, in practice, SADs are often
missing, outdated, or poorly aligned with the system's actual implementation.
Consequently, developers are compelled to derive architectural insights
directly from source code-a time-intensive process that increases cognitive
load, slows new developer onboarding, and contributes to the gradual
degradation of clarity over the system's lifetime. To address these issues, we
propose a semi-automated generation of SADs from source code by integrating
reverse engineering (RE) techniques with a Large Language Model (LLM). Our
approach recovers both static and behavioral architectural views by extracting
a comprehensive component diagram, filtering architecturally significant
elements (core components) via prompt engineering, and generating state machine
diagrams to model component behavior based on underlying code logic with
few-shots prompting. This resulting views representation offer a scalable and
maintainable alternative to traditional manual architectural documentation.
This methodology, demonstrated using C++ examples, highlights the potent
capability of LLMs to: 1) abstract the component diagram, thereby reducing the
reliance on human expert involvement, and 2) accurately represent complex
software behaviors, especially when enriched with domain-specific knowledge
through few-shot prompting. These findings suggest a viable path toward
significantly reducing manual effort while enhancing system understanding and
long-term maintainability.

</details>


### [50] [CodeMapper: A Language-Agnostic Approach to Mapping Code Regions Across Commits](https://arxiv.org/abs/2511.05205)
*Huimin Hu,Michael Pradel*

Main category: cs.SE

TL;DR: 本文提出了CodeMapper，一种独立于编程语言和代码元素的代码映射方法，用于在不同版本的代码提交中定位对应的代码区域，提升了代码区域定位的准确率。


<details>
  <summary>Details</summary>
Motivation: 开发者在软件演进过程中需要确定不同提交之间某段代码区域的变化，但现有工具如git diff只显示全部文件改动，且大多数技术局限于特定语言或代码元素，无法满足这一需求。

Method: CodeMapper方法分为两阶段：首先，通过分析代码差异、检测代码移动及搜索特定代码片段，计算候选区域；其次，计算相似度以选出最可能对应的目标代码区域。

Result: 在包括两个新手工标注数据集的四个数据集上测试，涵盖十种流行编程语言，CodeMapper准确识别目标区域的比例在71.0%至94.5%之间，显著优于现有最佳方法，提升幅度为1.5%至58.8%。

Conclusion: CodeMapper有效解决了代码映射问题，具有语言和代码元素无关性，显著提升了代码区域匹配的准确率，为软件演进中的代码比较与分析提供了强有力的工具。

Abstract: During software evolution, developers commonly face the problem of mapping a
specific code region from one commit to another. For example, they may want to
determine how the condition of an if-statement, a specific line in a
configuration file, or the definition of a function changes. We call this the
code mapping problem. Existing techniques, such as git diff, address this
problem only insufficiently because they show all changes made to a file
instead of focusing on a code region of the developer's choice. Other
techniques focus on specific code elements and programming languages (e.g.,
methods in Java), limiting their applicability. This paper introduces
CodeMapper, an approach to address the code mapping problem in a way that is
independent of specific program elements and programming languages. Given a
code region in one commit, CodeMapper finds the corresponding region in another
commit. The approach consists of two phases: (i) computing candidate regions by
analyzing diffs, detecting code movements, and searching for specific code
fragments, and (ii) selecting the most likely target region by calculating
similarities. Our evaluation applies CodeMapper to four datasets, including two
new hand-annotated datasets containing code region pairs in ten popular
programming languages. CodeMapper correctly identifies the expected target
region in 71.0%--94.5% of all cases, improving over the best available
baselines by 1.5--58.8 absolute percent points.

</details>


### [51] [Building Specialized Software-Assistant ChatBot with Graph-Based Retrieval-Augmented Generation](https://arxiv.org/abs/2511.05297)
*Mohammed Hilel,Yannis Karmim,Jean De Bodinat,Reda Sarehane,Antoine Gillon*

Main category: cs.SE

TL;DR: 本文提出了一个基于图的检索增强生成框架，自动将企业级网页应用转换为状态-动作知识图，使大语言模型能够提供可靠且具有上下文意识的数字化指导。


<details>
  <summary>Details</summary>
Motivation: 尽管数字采用平台帮助员工使用复杂企业软件，但构建和维护互动指南需要大量手工工作，而大语言模型在缺乏结构化软件理解时易产生错误答案且难以微调，因此需要一种新的方法来提升指导的准确性和自动化。

Method: 通过构建状态-动作知识图，结合基于图的检索增强生成框架，将企业应用结构化表示，辅助大语言模型生成有根有据的指导。该框架包括软件界面提取和结构化、图检索设计及实际集成到生产数字采用平台的流程。

Result: 该框架已与AI企业RAKAM及Lemon Learning共同开发和应用，实证了其在工业环境中具备良好的可扩展性、鲁棒性和生产部署能力。

Conclusion: 基于图的知识表示和检索增强生成技术有效解决了大语言模型在企业软件指导中的不确定性问题，提升了数字采用平台的自动化和可靠性，对企业软件培训和使用有显著推动作用。

Abstract: Digital Adoption Platforms (DAPs) have become essential tools for helping
employees navigate complex enterprise software such as CRM, ERP, or HRMS
systems. Companies like LemonLearning have shown how digital guidance can
reduce training costs and accelerate onboarding. However, building and
maintaining these interactive guides still requires extensive manual effort.
Leveraging Large Language Models as virtual assistants is an appealing
alternative, yet without a structured understanding of the target software,
LLMs often hallucinate and produce unreliable answers. Moreover, most
production-grade LLMs are black-box APIs, making fine-tuning impractical due to
the lack of access to model weights. In this work, we introduce a Graph-based
Retrieval-Augmented Generation framework that automatically converts enterprise
web applications into state-action knowledge graphs, enabling LLMs to generate
grounded and context-aware assistance. The framework was co-developed with the
AI enterprise RAKAM, in collaboration with Lemon Learning. We detail the
engineering pipeline that extracts and structures software interfaces, the
design of the graph-based retrieval process, and the integration of our
approach into production DAP workflows. Finally, we discuss scalability,
robustness, and deployment lessons learned from industrial use cases.

</details>


### [52] [Code Review Automation using Retrieval Augmented Generation](https://arxiv.org/abs/2511.05302)
*Qianru Meng,Xiao Zhang,Zhaochen Ren,Joost Visser*

Main category: cs.SE

TL;DR: 本文提出了RARe方法，结合检索与生成技术，提升自动代码审查的质量和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的代码审查自动生成方法存在审查意见偏离主题或过于泛泛的问题，影响质量。

Method: 通过引入检索增强生成（RAG）策略，RARe先利用密集检索器获取最相关的代码审查，再结合大型语言模型进行生成，整合外部领域知识改善审查结果。

Result: RARe在两个基准数据集上的BLEU-4得分分别达到12.32和12.96，超越了现有最先进方法。

Conclusion: RARe有效结合检索与生成，显著提升自动代码审查的准确性和实用性，且通过人工评估和案例分析验证了其可靠性。

Abstract: Code review is essential for maintaining software quality but is
labor-intensive. Automated code review generation offers a promising solution
to this challenge. Both deep learning-based generative techniques and
retrieval-based methods have demonstrated strong performance in this task.
However, despite these advancements, there are still some limitations where
generated reviews can be either off-point or overly general. To address these
issues, we introduce Retrieval-Augmented Reviewer (RARe), which leverages
Retrieval-Augmented Generation (RAG) to combine retrieval-based and generative
methods, explicitly incorporating external domain knowledge into the code
review process. RARe uses a dense retriever to select the most relevant reviews
from the codebase, which then enrich the input for a neural generator,
utilizing the contextual learning capacity of large language models (LLMs), to
produce the final review. RARe outperforms state-of-the-art methods on two
benchmark datasets, achieving BLEU-4 scores of 12.32 and 12.96, respectively.
Its effectiveness is further validated through a detailed human evaluation and
a case study using an interpretability tool, demonstrating its practical
utility and reliability.

</details>


### [53] [SWE-Compass: Towards Unified Evaluation of Agentic Coding Abilities for Large Language Models](https://arxiv.org/abs/2511.05459)
*Jingxuan Xu,Ken Deng,Weihao Li,Songwei Yu,Huaixi Tang,Haoyang Huang,Zhiyi Lai,Zizheng Zhan,Yanan Wu,Chenchen Zhang,Kepeng Lei,Yifan Yao,Xinping Lei,Wenqiang Zhu,Zongxian Feng,Han Li,Junqi Xiong,Dailin Li,Zuchen Gao,Kun Wu,Wen Xiang,Ziqi Zhan,Yuanxing Zhang,Wuxuan Gong,Ziyuan Gao,Guanxiang Wang,Yirong Xue,Xiaojiang Zhang,Jinghui Wang,Huiming Wang,Wenhao Zhuang,Zhaoxiang Zhang,Yuqun Zhang,Haotian Zhang,Bin Chen,Jiaheng Liu*

Main category: cs.SE

TL;DR: SWE-Compass是一个全面的代码相关评估基准，涵盖多种任务、编程场景和语言，用于评测大型语言模型在软件工程中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有对大型语言模型在软件工程领域的评测存在任务覆盖窄、语言偏向和与真实开发流程不匹配等问题。

Method: 设计SWE-Compass基准，包含8种任务类型、8个编程场景和10种编程语言，基于2000个真实GitHub PR实例，采用SWE-Agent和Claude Code两个代理框架评测十个先进大型语言模型。

Result: 发现不同任务类型、语言和场景存在明确难度层级，评测揭示模型在不同维度的性能表现。

Conclusion: SWE-Compass与实际开发流程对齐，为诊断和提升大型语言模型的编码能力提供了严格且可复现的基础。

Abstract: Evaluating large language models (LLMs) for software engineering has been
limited by narrow task coverage, language bias, and insufficient alignment with
real-world developer workflows. Existing benchmarks often focus on algorithmic
problems or Python-centric bug fixing, leaving critical dimensions of software
engineering underexplored. To address these gaps, we introduce SWE-Compass1, a
comprehensive benchmark that unifies heterogeneous code-related evaluations
into a structured and production-aligned framework. SWE-Compass spans 8 task
types, 8 programming scenarios, and 10 programming languages, with 2000
high-quality instances curated from authentic GitHub pull requests and refined
through systematic filtering and validation. We benchmark ten state-of-the-art
LLMs under two agentic frameworks, SWE-Agent and Claude Code, revealing a clear
hierarchy of difficulty across task types, languages, and scenarios. Moreover,
by aligning evaluation with real-world developer practices, SWE-Compass
provides a rigorous and reproducible foundation for diagnosing and advancing
agentic coding capabilities in large language models.

</details>


### [54] [A Metamorphic Testing Perspective on Knowledge Distillation for Language Models of Code: Does the Student Deeply Mimic the Teacher?](https://arxiv.org/abs/2511.05476)
*Md. Abdul Awal,Mrigank Rochan,Chanchal K. Roy*

Main category: cs.SE

TL;DR: 本文针对基于Transformer的代码语言模型压缩过程中，学生模型未能深度模仿教师模型行为的问题，提出MetaCompress框架，通过变形测试系统地评估学生模型的行为忠实度，揭示传统准确率评估的不足。


<details>
  <summary>Details</summary>
Motivation: Transformer代码语言模型虽性能优秀，但因计算资源消耗大限制实际应用。知识蒸馏用于模型压缩，但传统准确率评估忽视学生模型与教师模型行为上的深层差异，尤其在对抗攻击下表现不佳。

Method: 提出MetaCompress变形测试框架，通过行为保持的变形关系系统比较教师和学生模型的输出，评估行为忠实度。实验在两大任务和三种蒸馏技术的压缩模型上验证该方法。

Result: MetaCompress发现学生模型中最高达62%的行为差异，且学生模型在对抗攻击下性能下降高达285%，传统准确率无法体现这些差异。

Conclusion: 行为忠实度评估对于知识蒸馏压缩的代码语言模型至关重要。MetaCompress提供了一个实用且系统的测试框架，有助于提升压缩模型的质量和可靠性。

Abstract: Transformer-based language models of code have achieved state-of-the-art
performance across a wide range of software analytics tasks, but their
practical deployment remains limited due to high computational costs, slow
inference speeds, and significant environmental impact. To address these
challenges, recent research has increasingly explored knowledge distillation as
a method for compressing a large language model of code (the teacher) into a
smaller model (the student) while maintaining performance. However, the degree
to which a student model deeply mimics the predictive behavior and internal
representations of its teacher remains largely unexplored, as current
accuracy-based evaluation provides only a surface-level view of model quality
and often fails to capture more profound discrepancies in behavioral fidelity
between the teacher and student models. To address this gap, we empirically
show that the student model often fails to deeply mimic the teacher model,
resulting in up to 285% greater performance drop under adversarial attacks,
which is not captured by traditional accuracy-based evaluation. Therefore, we
propose MetaCompress, a metamorphic testing framework that systematically
evaluates behavioral fidelity by comparing the outputs of teacher and student
models under a set of behavior-preserving metamorphic relations. We evaluate
MetaCompress on two widely studied tasks, using compressed versions of popular
language models of code, obtained via three different knowledge distillation
techniques: Compressor, AVATAR, and MORPH. The results show that MetaCompress
identifies up to 62% behavioral discrepancies in student models, underscoring
the need for behavioral fidelity evaluation within the knowledge distillation
pipeline and establishing MetaCompress as a practical framework for testing
compressed language models of code derived through knowledge distillation.

</details>
