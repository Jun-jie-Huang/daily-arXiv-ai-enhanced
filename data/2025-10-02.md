<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 57]
- [cs.SE](#cs.SE) [Total: 28]
- [cs.MA](#cs.MA) [Total: 5]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Direct Token Optimization: A Self-contained Approach to Large Language Model Unlearning](https://arxiv.org/abs/2510.00125)
*Hong kyu Lee,Ruixuan Liu,Li Xiong*

Main category: cs.CL

TL;DR: 提出了一种名为直接令牌优化(DTO)的新型大语言模型自包含遗忘方法，通过优化目标令牌实现有效遗忘，同时保持模型效用，无需依赖外部资源。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的遗忘方法依赖外部语言模型、数据集乃至商业AI服务，不仅不切实际还存在隐私风险，急需一种无需外部资源的有效遗忘策略。

Method: 通过识别序列中的目标令牌（需遗忘的关键知识）和非目标令牌（维护模型效用），直接优化目标令牌的遗忘目标，同时保留非目标令牌以保证模型性能，实现自包含的令牌级优化。

Result: 在多个基准数据集上，DTO在遗忘质量方面比最新方法最高提升了16.8倍，同时保持了相当水平的模型效用。

Conclusion: DTO提供了无需依赖外部资源的高效大语言模型遗忘方案，有效提升遗忘效果且确保模型整体性能，适合实际应用中的隐私保护与模型纠正场景。

Abstract: Machine unlearning is an emerging technique that removes the influence of a
subset of training data (forget set) from a model without full retraining, with
applications including privacy protection, content moderation, and model
correction. The key challenge lies in ensuring that the model completely
forgets the knowledge of the forget set without compromising its overall
utility. Existing unlearning methods for large language models (LLMs) often
utilize auxiliary language models, retain datasets, or even commercial AI
services for effective unlearning and maintaining the model utility. However,
dependence on these external resources is often impractical and could
potentially introduce additional privacy risks. In this work, we propose direct
token optimization (DTO), a novel self-contained unlearning approach for LLMs
that directly optimizes the token level objectives and eliminates the need for
external resources. Given a sequence to unlearn, we identify two categories of
tokens: target tokens, which capture critical knowledge for unlearning, and the
remaining non-target tokens, which are crucial for maintaining the model
utility. The former are used to optimize the unlearning objective, while the
latter serve to preserve the model's performance. The experimental results show
that the proposed DTO achieves up to 16.8$\times$ improvement in forget quality
on several benchmark datasets than the latest baselines while maintaining a
comparable level of model utility.

</details>


### [2] [TAMA: Tool-Augmented Multimodal Agent for Procedural Activity Understanding](https://arxiv.org/abs/2510.00161)
*Kimihiro Hasegawa,Wiradee Imrattanatrai,Masaki Asada,Ken Fukuda,Teruko Mitamura*

Main category: cs.CL

TL;DR: 本文提出了一个名为TAMA的工具增强多模态代理框架，用于理解程序性活动，通过多媒体返回工具实现训练自由的交叉多模态推理，在程序性问题回答数据集上提升了视觉语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 程序性活动助手在日常生活和专业领域有广泛应用潜力，但针对这类助手的系统开发尚未充分探索。

Method: 提出TAMA框架，利用多媒体返回工具实现无训练的交叉多模态推理，并结合灵活的工具选择机制。

Result: 在ProMQA-Assembly多模态程序性问答数据集上，TAMA显著提升了GPT-5和MiMo-VL的表现，消融实验支持多媒体工具和工具选择机制的有效性。

Conclusion: TAMA框架和实验结果推动了基于图像思维的视频和多模态任务的发展，为程序性活动助手的研发提供了有力支持。

Abstract: Procedural activity assistants potentially support humans in a variety of
settings, from our daily lives, e.g., cooking or assembling flat-pack
furniture, to professional situations, e.g., manufacturing or biological
experiments. Despite its potential use cases, the system development tailored
for such an assistant is still underexplored. In this paper, we propose a novel
framework, called TAMA, a Tool-Augmented Multimodal Agent, for procedural
activity understanding. TAMA enables interleaved multimodal reasoning by making
use of multimedia-returning tools in a training-free setting. Our experimental
result on the multimodal procedural QA dataset, ProMQA-Assembly, shows that our
approach can improve the performance of vision-language models, especially
GPT-5 and MiMo-VL. Furthermore, our ablation studies provide empirical support
for the effectiveness of two features that characterize our framework,
multimedia-returning tools and agentic flexible tool selection. We believe our
proposed framework and experimental results facilitate the thinking with images
paradigm for video and multimodal tasks, let alone the development of
procedural activity assistants.

</details>


### [3] [DRBench: A Realistic Benchmark for Enterprise Deep Research](https://arxiv.org/abs/2510.00172)
*Amirhossein Abaskohi,Tianyi Chen,Miguel Muñoz-Mármol,Curtis Fox,Amrutha Varshini Ramesh,Étienne Marcotte,Xing Han Lù,Nicolas Chapados,Spandana Gella,Christopher Pal,Alexandre Drouin,Issam H. Laradji*

Main category: cs.CL

TL;DR: 本文介绍了DRBench，一个用于评估AI代理在企业复杂开放式深度研究任务中的基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试多关注简单问题或仅限于网络查询，缺乏对多步骤、多来源复杂任务的评估。

Method: 设计了DRBench，包含15个跨10个领域的深度研究任务，任务涉及公私域信息，评估代理在信息检索、事实准确性及报告生成上的表现。采用人工参与的合成流水线保证任务质量。

Result: 通过对多个开源和闭源模型（如GPT、Llama、Qwen）和研究策略的评测，展示了DRBench在揭示AI代理优势和不足方面的有效性。

Conclusion: DRBench为企业复杂研究任务的AI能力评估提供了新的标杆，有助于推动企业深度研究方向的技术进步。

Abstract: We introduce DRBench, a benchmark for evaluating AI agents on complex,
open-ended deep research tasks in enterprise settings. Unlike prior benchmarks
that focus on simple questions or web-only queries, DRBench evaluates agents on
multi-step queries (for example, ``What changes should we make to our product
roadmap to ensure compliance with this standard?") that require identifying
supporting facts from both the public web and private company knowledge base.
Each task is grounded in realistic user personas and enterprise context,
spanning a heterogeneous search space that includes productivity software,
cloud file systems, emails, chat conversations, and the open web. Tasks are
generated through a carefully designed synthesis pipeline with
human-in-the-loop verification, and agents are evaluated on their ability to
recall relevant insights, maintain factual accuracy, and produce coherent,
well-structured reports. We release 15 deep research tasks across 10 domains,
such as Sales, Cybersecurity, and Compliance. We demonstrate the effectiveness
of DRBench by evaluating diverse DR agents across open- and closed-source
models (such as GPT, Llama, and Qwen) and DR strategies, highlighting their
strengths, weaknesses, and the critical path for advancing enterprise deep
research. Code is available at https://github.com/ServiceNow/drbench.

</details>


### [4] [PrimeX: A Dataset of Worldview, Opinion, and Explanation](https://arxiv.org/abs/2510.00174)
*Rik Koncel-Kedziorski,Brihi Joshi,Tim Paek*

Main category: cs.CL

TL;DR: 本文提出了PrimeX数据集，结合受访者的公开意见调查数据、意见解释和世界观调查，展示了利用个人信念信息提升语言模型个性化的潜力。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型的应用普及，如何更好地表示个体用户，尤其其信念体系，以提高模型的对齐能力成为需求。

Method: 开发了包含858名美国受访者公共意见调查数据的PrimeX数据集，附加了受访者的意见书面解释及世界观调查数据，并进行了初步的数据分析。

Result: 结果显示，信念解释和世界观信息能够有效提升语言模型的个性化表现，证明了PrimeX在NLP和心理学研究中的价值。

Conclusion: PrimeX数据集通过整合额外的信念信息，为语言模型个性化及相关研究提供了新视角和可能性，促进了跨领域研究的发展。

Abstract: As the adoption of language models advances, so does the need to better
represent individual users to the model. Are there aspects of an individual's
belief system that a language model can utilize for improved alignment?
Following prior research, we investigate this question in the domain of opinion
prediction by developing PrimeX, a dataset of public opinion survey data from
858 US residents with two additional sources of belief information: written
explanations from the respondents for why they hold specific opinions, and the
Primal World Belief survey for assessing respondent worldview. We provide an
extensive initial analysis of our data and show the value of belief
explanations and worldview for personalizing language models. Our results
demonstrate how the additional belief information in PrimeX can benefit both
the NLP and psychological research communities, opening up avenues for further
study.

</details>


### [5] [Personalized Reasoning: Just-In-Time Personalization and Why LLMs Fail At It](https://arxiv.org/abs/2510.00177)
*Shuyue Stella Li,Avinandan Bose,Faeze Brahman,Simon Shaolei Du,Pang Wei Koh,Maryam Fazel,Yulia Tsvetkov*

Main category: cs.CL

TL;DR: 本文提出了PREFDISCO，一种将静态基准转化为交互式个性化任务的评估方法，揭示了现有大语言模型（LLM）在个性化推理中的局限性，强调个性化推理需专门开发。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在任务解决与偏好对齐上分开优化，但在没有用户历史数据的实时交互中，准确回答不足以满足用户需求，亟需模型能识别未知偏好并动态调整回答。

Method: 提出PREFDISCO框架，基于心理学角色设定和稀疏偏好，将静态问题转变为根据用户背景调整推理链的个性化任务，体现不同用户对解释方式的不同需求。

Result: 评估21个先进模型在10个任务中表现，29%的简单个性化尝试反而降低了偏好对齐效果，普通回答也不能有效满足个别需求，显示个性化推理不能自然而然生成。

Conclusion: PREFDISCO表明个性化推理是一个独立且重要的研究方向，现有LLM在交互能力上存在根本局限，为教育、医疗等需个性化的领域提供了研究基础。

Abstract: Current large language model (LLM) development treats task-solving and
preference alignment as separate challenges, optimizing first for objective
correctness, then for alignment to aggregated human preferences. This paradigm
fails in human-facing applications where solving a problem correctly is
insufficient if the response mismatches the user's needs. This challenge
intensifies in just-in-time scenarios where no prior user interaction history
exists due to cold-start conditions or privacy constraints. LLMs need to
identify what they don't know about user preferences, strategically elicit
preference values through questioning, then adapt their reasoning processes and
responses accordingly -- a complicated chain of cognitive processes which we
term personalized reasoning. We introduce PREFDISCO, an evaluation methodology
that transforms static benchmarks into interactive personalization tasks using
psychologically-grounded personas with sparse preferences. Our framework
creates scenarios where identical questions require different reasoning chains
depending on user context, as optimal explanation approaches vary by individual
expertise and preferences while maintaining factual accuracy. Evaluation of 21
frontier models across 10 tasks reveals 29.0% of naive personalization attempts
produce worse preference alignment than generic responses, yet generic
responses also fail to serve individual user needs effectively. These findings
suggest personalized reasoning requires dedicated development rather than
emerging naturally. PREFDISCO establishes personalized reasoning as a
measurable research frontier and reveals fundamental limitations in current
LLMs' interactive capabilities, providing a foundation for developing systems
that can adapt to individual users in education, healthcare, and technical
domains where personalization is critical.

</details>


### [6] [BiasFreeBench: a Benchmark for Mitigating Bias in Large Language Model Responses](https://arxiv.org/abs/2510.00232)
*Xin Xu,Xunzhi He,Churan Zhi,Ruizhe Chen,Julian McAuley,Zexue He*

Main category: cs.CL

TL;DR: 该论文提出BiasFreeBench，一个统一的偏见缓解评估基准，系统比较了八种主流偏见缓解技术，涵盖提示和训练方法，并引入响应级偏见评分指标，促进偏见缓解研究的一致性和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有针对大语言模型的偏见缓解方法评估标准不统一，且多基于模型概率与偏见无关情境的比较，忽视了用户与模型交互中的实际需求。

Method: 重新组织现有数据集，构建统一的查询-响应测试环境，比较八种偏见缓解技术（四种基于提示、四种基于训练）在多项选择和开放式多轮问答中的表现，设计了响应级的Bias-Free Score指标来衡量模型回复的公平性、安全性和反刻板印象程度。

Result: 系统性比较了不同偏见缓解方法在提示与训练范式、模型大小以及训练策略对未知偏见类型的泛化能力上的表现差异。

Conclusion: 提出的BiasFreeBench提供了一个统一且更贴近用户体验的偏见缓解评估基准，有助于推动该领域的标准化和公平性研究。

Abstract: Existing studies on bias mitigation methods for large language models (LLMs)
use diverse baselines and metrics to evaluate debiasing performance, leading to
inconsistent comparisons among them. Moreover, their evaluations are mostly
based on the comparison between LLMs' probabilities of biased and unbiased
contexts, which ignores the gap between such evaluations and real-world use
cases where users interact with LLMs by reading model responses and expect fair
and safe outputs rather than LLMs' probabilities. To enable consistent
evaluation across debiasing methods and bridge this gap, we introduce
BiasFreeBench, an empirical benchmark that comprehensively compares eight
mainstream bias mitigation techniques (covering four prompting-based and four
training-based methods) on two test scenarios (multi-choice QA and open-ended
multi-turn QA) by reorganizing existing datasets into a unified query-response
setting. We further introduce a response-level metric, Bias-Free Score, to
measure the extent to which LLM responses are fair, safe, and
anti-stereotypical. Debiasing performances are systematically compared and
analyzed across key dimensions: the prompting vs. training paradigm, model
size, and generalization of different training strategies to unseen bias types.
We will publicly release our benchmark, aiming to establish a unified testbed
for bias mitigation research.

</details>


### [7] [TASER: Translation Assessment via Systematic Evaluation and Reasoning](https://arxiv.org/abs/2510.00255)
*Monishwaran Maheswaran,Marco Carini,Christian Federmann,Tony Diaz*

Main category: cs.CL

TL;DR: TASER利用大型推理模型对翻译质量进行系统性、分步的自动评估，在多语言任务中表现出色，超过现有指标。


<details>
  <summary>Details</summary>
Motivation: 现有自动翻译评估指标缺乏透明性和系统性，难以准确反映翻译质量，因此需要利用大型推理模型的显式推理能力提升评估质量。

Method: 引入TASER，采用大型推理模型通过结构化提示模板进行逐步推理评估翻译质量，分别在有无参考译文环境下测试性能。

Result: TASER在WMT24指标共享任务中表现最佳，系统级和段落级评估均超越现有指标，参考自由版本在无参考环境中表现最好。

Conclusion: 大型推理模型结合显式推理过程，提升了自动翻译质量评估的准确性和解释性，显示了其在多语言场景中的先进性。

Abstract: We introduce TASER (Translation Assessment via Systematic Evaluation and
Reasoning), a metric that uses Large Reasoning Models (LRMs) for automated
translation quality assessment. TASER harnesses the explicit reasoning
capabilities of LRMs to conduct systematic, step-by-step evaluation of
translation quality. We evaluate TASER on the WMT24 Metrics Shared Task across
both reference-based and reference-free scenarios, demonstrating
state-of-the-art performance. In system-level evaluation, TASER achieves the
highest soft pairwise accuracy in both reference-based and reference-free
settings, outperforming all existing metrics. At the segment level, TASER
maintains competitive performance with our reference-free variant ranking as
the top-performing metric among all reference-free approaches. Our experiments
reveal that structured prompting templates yield superior results with LRMs
compared to the open-ended approaches that proved optimal for traditional LLMs.
We evaluate o3, a large reasoning model from OpenAI, with varying reasoning
efforts, providing insights into the relationship between reasoning depth and
evaluation quality. The explicit reasoning process in LRMs offers
interpretability and visibility, addressing a key limitation of existing
automated metrics. Our results demonstrate that Large Reasoning Models show a
measurable advancement in translation quality assessment, combining improved
accuracy with transparent evaluation across diverse language pairs.

</details>


### [8] [Retrieval-Augmented Generation for Electrocardiogram-Language Models](https://arxiv.org/abs/2510.00261)
*Xiaoyu Song,William Han,Tony Chen,Chaojing Duan,Michael A. Rosenberg,Emerson Liu,Ding Zhao*

Main category: cs.CL

TL;DR: 本文提出了首个用于生成心电图语言模型（ELMs）的开源检索增强生成（RAG）管道，并在三个公开数据集上验证了其有效性，显著提升了自然语言生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有的生成心电图语言模型缺乏系统性的检索增强生成（RAG）设计和开源实现，限制了模型在医疗文本生成中的准确性和多样性。

Method: 本文设计并实现了一个开源的RAG管道，将检索机制融入ELMs中，通过基线模型和消融实验验证了RAG对自然语言生成的提升效果。

Result: 在三个公开心电图数据集上的实验结果表明，采用RAG的ELMs在诊断、分析和对话任务中的文本生成性能均优于非RAG模型。

Conclusion: RAG技术为生成型心电图语言模型提供了有效的支持，显著减少了幻觉现象并提升了文本生成质量，且所提出的开源管道可为相关研究提供重要参考。

Abstract: Interest in generative Electrocardiogram-Language Models (ELMs) is growing,
as they can produce textual responses conditioned on ECG signals and textual
queries. Unlike traditional classifiers that output label probabilities, ELMs
are more versatile, supporting domain-specific tasks (e.g., waveform analysis,
diagnosis, prognosis) as well as general tasks (e.g., open-ended questions,
dialogue). Retrieval-Augmented Generation (RAG), widely used in Large Language
Models (LLMs) to ground LLM outputs in retrieved knowledge, helps reduce
hallucinations and improve natural language generation (NLG). However, despite
its promise, no open-source implementation or systematic study of RAG pipeline
design for ELMs currently exists. To address this gap, we present the first
open-source RAG pipeline for ELMs, along with baselines and ablation studies
for NLG. Experiments on three public datasets show that ELMs with RAG
consistently improves performance over non-RAG baselines and highlights key ELM
design considerations. Our code is available at:
https://github.com/willxxy/ECG-Bench.

</details>


### [9] [Judging with Confidence: Calibrating Autoraters to Preference Distributions](https://arxiv.org/abs/2510.00263)
*Zhuohang Li,Xiaowei Li,Chengyu Huang,Guowang Li,Katayoon Goshvadi,Bo Dai,Dale Schuurmans,Paul Zhou,Hamid Palangi,Yiwen Song,Palash Goyal,Murat Kantarcioglu,Bradley A. Malin,Yuan Xue*

Main category: cs.CL

TL;DR: 本文提出了一种校准大语言模型自动评分器以匹配目标偏好分布的通用框架，旨在提升其对主观和模糊任务的判定可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有使用大语言模型作为自动评分器的方法受限于训练时仅基于离散偏好标签，无法准确反映主观、模糊或微妙任务中的多样化偏好。

Method: 我们提出了两种训练方法：针对密集概率标签的直接监督微调，以及针对稀疏二元标签的强化学习方法，用于校准自动评分器使其输出符合目标偏好分布。

Result: 实验证明，基于分布匹配目标的微调使自动评分器的概率预测与目标偏好分布更加一致，提升了校准质量，显著降低了位置偏差，并保持了对客观任务的性能。

Conclusion: 通过引入基于偏好分布的校准，自动评分器能够更可靠地反映多样化偏好，提升了其在复杂任务中的判定质量和公平性。

Abstract: The alignment of large language models (LLMs) with human values increasingly
relies on using other LLMs as automated judges, or ``autoraters''. However,
their reliability is limited by a foundational issue: they are trained on
discrete preference labels, forcing a single ground truth onto tasks that are
often subjective, ambiguous, or nuanced. We argue that a reliable autorater
must learn to model the full distribution of preferences defined by a target
population. In this paper, we propose a general framework for calibrating
probabilistic autoraters to any given preference distribution. We formalize the
problem and present two learning methods tailored to different data conditions:
1) a direct supervised fine-tuning for dense, probabilistic labels, and 2) a
reinforcement learning approach for sparse, binary labels. Our empirical
results show that finetuning autoraters with a distribution-matching objective
leads to verbalized probability predictions that are better aligned with the
target preference distribution, with improved calibration and significantly
lower positional bias, all while preserving performance on objective tasks.

</details>


### [10] [Efficient Layer-wise LLM Fine-tuning for Revision Intention Prediction](https://arxiv.org/abs/2510.00268)
*Zhexiong Liu,Diane Litman*

Main category: cs.CL

TL;DR: 本文提出IR-Tuning，一种基于梯度范数动态选择重要层进行高效微调的大型语言模型方法，解决文本修订分类标注稀缺问题，实现低资源下的高效分类。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在文本生成方面表现优异，但在细粒度文本分类任务（如文本修订分类）表现不足，且修订标注昂贵稀缺，需开发高效微调策略。

Method: 提出IR-Tuning，一种插拔式的分层参数高效微调框架，通过动态选择梯度范数较大的重要层进行微调，冻结冗余层，节省计算资源。

Result: IR-Tuning在多个文本修订分类基准上优于其他分层PEFT方法，具备快速收敛、低显存消耗及小规模数据高效适应能力。

Conclusion: IR-Tuning有效提升了LLM在细粒度文本分类任务上的表现，实现了资源利用最大化，具有广泛应用潜力。

Abstract: Large Language Models (LLMs) have shown extraordinary success across various
text generation tasks; however, their potential for simple yet essential text
classification remains underexplored, as LLM pre-training tends to emphasize
generation over classification. While LLMs with instruction tuning can
transform classification into a generation task, they often struggle to
categorize nuanced texts. One such example is text revision, which involves
nuanced edits between pairs of texts. Although simply fine-tuning LLMs for
revision classification seems plausible, it requires a large amount of revision
annotations, which are exceptionally expensive and scarce in the community. To
address this issue, we introduce a plug-and-play layer-wise parameter-efficient
fine-tuning (PEFT) framework, i.e., IR-Tuning, which fine-tunes a subset of
important LLM layers that are dynamically selected based on their gradient norm
distribution, while freezing those of redundant layers. Extensive experiments
suggest that IR-Tuning surpasses several layer-wise PEFT baselines over diverse
text revisions, while achieving fast convergence, low GPU memory consumption,
and effectiveness on small revision corpora.

</details>


### [11] [SafePassage: High-Fidelity Information Extraction with Black Box LLMs](https://arxiv.org/abs/2510.00276)
*Joe Barrow,Raj Patel,Misha Kharkovski,Ben Davies,Ryan Schmitt*

Main category: cs.CL

TL;DR: 该论文提出了SafePassage三步管道，通过生成与文档内容一致的上下文来防止大型语言模型在信息抽取中产生虚假信息，能显著减少虚假抽取，提高可信度。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在信息抽取中易引入与文档不符的虚假信息，影响抽取结果的可信度，亟需一种机制保障抽取内容的可靠性。

Method: 提出SafePassage三步管道：1）用LLM提取结构化实体及其上下文；2）通过字符串比对进行全局对齐；3）利用评分模型评估上下文的安全性。

Result: 该方法能将信息抽取任务中的虚假信息降低85%，并且能有效区分安全与不安全段落，自动评估效果与人工判断高度一致。同时，微调的Transformer编码器在识别不安全段落时优于LLM评分模型。

Conclusion: SafePassage有效提升LLM信息抽取的可信度，减少错误信息生成，且微调编码器可快速搭建高效评分模型，为信息抽取及模型评测提供可靠工具。

Abstract: Black box large language models (LLMs) make information extraction (IE) easy
to configure, but hard to trust. Unlike traditional information extraction
pipelines, the information "extracted" is not guaranteed to be grounded in the
document. To prevent this, this paper introduces the notion of a "safe
passage": context generated by the LLM that is both grounded in the document
and consistent with the extracted information. This is operationalized via a
three-step pipeline, SafePassage, which consists of: (1) an LLM extractor that
generates structured entities and their contexts from a document, (2) a
string-based global aligner, and (3) a scoring model. Results show that using
these three parts in conjunction reduces hallucinations by up to 85% on
information extraction tasks with minimal risk of flagging non-hallucinations.
High agreement between the SafePassage pipeline and human judgments of
extraction quality mean that the pipeline can be dually used to evaluate LLMs.
Surprisingly, results also show that using a transformer encoder fine-tuned on
a small number of task-specific examples can outperform an LLM scoring model at
flagging unsafe passages. These annotations can be collected in as little as
1-2 hours.

</details>


### [12] [ReEvalMed: Rethinking Medical Report Evaluation by Aligning Metrics with Real-World Clinical Judgment](https://arxiv.org/abs/2510.00280)
*Ruochen Li,Jun Li,Bailiang Jian,Kun Yuan,Youxiang Zhu*

Main category: cs.CL

TL;DR: 本文提出一个临床基础的元评价框架，用于重新设计和评估自动生成放射学报告的评价指标，揭示了现有指标在临床语义解释上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有自动生成报告的评价指标得分虽高，却难以获得临床医师信任，反映出评价指标在临床质量评估上的根本缺陷。

Method: 提出临床基础的元评价框架，定义临床对齐和关键指标能力（区分度、鲁棒性、单调性）标准，用细粒度标注数据系统评估现有指标。

Result: 发现现有指标难以区分临床重要错误、过度惩罚无害差异且缺乏错误严重程度一致性。

Conclusion: 该框架为构建更具临床可靠性的评价方法提供了指导。

Abstract: Automatically generated radiology reports often receive high scores from
existing evaluation metrics but fail to earn clinicians' trust. This gap
reveals fundamental flaws in how current metrics assess the quality of
generated reports. We rethink the design and evaluation of these metrics and
propose a clinically grounded Meta-Evaluation framework. We define clinically
grounded criteria spanning clinical alignment and key metric capabilities,
including discrimination, robustness, and monotonicity. Using a fine-grained
dataset of ground truth and rewritten report pairs annotated with error types,
clinical significance labels, and explanations, we systematically evaluate
existing metrics and reveal their limitations in interpreting clinical
semantics, such as failing to distinguish clinically significant errors,
over-penalizing harmless variations, and lacking consistency across error
severity levels. Our framework offers guidance for building more clinically
reliable evaluation methods.

</details>


### [13] [o-MEGA: Optimized Methods for Explanation Generation and Analysis](https://arxiv.org/abs/2510.00288)
*Ľuboš Kriš,Jaroslav Kopčan,Qiwei Peng,Andrej Ridzik,Marcel Veselý,Martin Tamajka*

Main category: cs.CL

TL;DR: 介绍了o-mega，一种用于语义匹配领域的可解释AI方法超参数自动优化工具，以提升模型可解释性和透明度。


<details>
  <summary>Details</summary>
Motivation: Transformer语言模型虽强大，但模型透明度和可信度不足，现有解释方法繁多，难以选择最佳方案。

Method: 提出o-mega工具，自动优化可解释方法及其超参数，在社交媒体帖子与反驳声明的数据集上评估。

Result: o-mega系统性探索不同解释方法及配置，显著提升自动化事实核查系统的透明度。

Conclusion: 自动优化解释方法可增强语义匹配模型的可解释性，有助于错误信息检测等关键应用，提高AI系统的可信与透明度。

Abstract: The proliferation of transformer-based language models has revolutionized NLP
domain while simultaneously introduced significant challenges regarding model
transparency and trustworthiness. The complexity of achieving explainable
systems in this domain is evidenced by the extensive array of explanation
methods and evaluation metrics developed by researchers. To address the
challenge of selecting optimal explainability approaches, we present
\textbf{\texttt{o-mega}}, a hyperparameter optimization tool designed to
automatically identify the most effective explainable AI methods and their
configurations within the semantic matching domain. We evaluate o-mega on a
post-claim matching pipeline using a curated dataset of social media posts
paired with refuting claims. Our tool systematically explores different
explainable methods and their hyperparameters, demonstrating improved
transparency in automated fact-checking systems. As a result, such automated
optimization of explanation methods can significantly enhance the
interpretability of claim-matching models in critical applications such as
misinformation detection, contributing to more trustworthy and transparent AI
systems.

</details>


### [14] [CORTEX: Collaborative LLM Agents for High-Stakes Alert Triage](https://arxiv.org/abs/2510.00311)
*Bowen Wei,Yuan Shen Tay,Howard Liu,Jinhao Pan,Kun Luo,Ziwei Zhu,Chris Jordan*

Main category: cs.CL

TL;DR: CORTEX是一种多代理大语言模型架构，用于安全运营中心的高风险警报分流，显著降低误报率，提高调查质量。


<details>
  <summary>Details</summary>
Motivation: 传统安全检测流程脆弱且缺乏上下文，单一大语言模型难以处理复杂嘈杂的企业数据，且透明度不足，导致安全分析师疲劳和威胁遗漏。

Method: 设计多代理协作架构，包括行为分析代理、取证代理和推理代理，各司其职基于真实证据协同工作，并发布包含细粒度调查步骤和工具输出的数据集用于训练评价。

Result: 在多样化企业场景中，CORTEX相比单一大语言模型大幅减少误报，提升调查效果。

Conclusion: 多代理LLM架构提升了高风险安全警报处理的准确性和可审计性，有助缓解安全分析师负担，提高安全运营效率。

Abstract: Security Operations Centers (SOCs) are overwhelmed by tens of thousands of
daily alerts, with only a small fraction corresponding to genuine attacks. This
overload creates alert fatigue, leading to overlooked threats and analyst
burnout. Classical detection pipelines are brittle and context-poor, while
recent LLM-based approaches typically rely on a single model to interpret logs,
retrieve context, and adjudicate alerts end-to-end -- an approach that
struggles with noisy enterprise data and offers limited transparency. We
propose CORTEX, a multi-agent LLM architecture for high-stakes alert triage in
which specialized agents collaborate over real evidence: a behavior-analysis
agent inspects activity sequences, evidence-gathering agents query external
systems, and a reasoning agent synthesizes findings into an auditable decision.
To support training and evaluation, we release a dataset of fine-grained SOC
investigations from production environments, capturing step-by-step analyst
actions and linked tool outputs. Across diverse enterprise scenarios, CORTEX
substantially reduces false positives and improves investigation quality over
state-of-the-art single-agent LLMs.

</details>


### [15] [TokMem: Tokenized Procedural Memory for Large Language Models](https://arxiv.org/abs/2510.00444)
*Zijun Wu,Yongchang Hao,Lili Mou*

Main category: cs.CL

TL;DR: 本文提出了TokMem，一种将反复使用的过程存储为紧凑且可训练的嵌入的标记化程序记忆，用于改进大语言模型的任务指定和推理效率。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型依赖提示词来指定任务和引导推理，存在效率低、在多任务间扩展性差以及缺乏模块化复用机制的问题。

Method: 引入TokMem，通过存储过程的地址和控制信号的记忆标记，实现针对性生成行为，并保持主模型不变，支持程序的不断添加和适应。

Result: 在1000个原子回忆任务和函数调用组合任务上，TokMem性能优于检索增强生成和少参数微调，避免了重复上下文开销。

Conclusion: TokMem为大语言模型提供了一种可扩展、模块化且显式的程序记忆，成为提示工程和微调的有效替代方案。

Abstract: Large language models rely heavily on prompts to specify tasks, recall
knowledge and guide reasoning. However, this reliance is inefficient as prompts
must be re-read at each step, scale poorly across tasks, and lack mechanisms
for modular reuse. We introduce TokMem, a tokenized procedural memory that
stores recurring procedures as compact, trainable embeddings. Each memory token
encodes both an address to a procedure and a control signal that steers
generation, enabling targeted behavior with constant-size overhead. To support
continual adaptation, TokMem keeps the backbone model frozen, allowing new
procedures to be added without interfering with existing ones. We evaluate
TokMem on 1,000 tasks for atomic recall, and on function-calling tasks for
compositional recall, where it consistently outperforms retrieval-augmented
generation while avoiding repeated context overhead, and fine-tuning with far
fewer parameters. These results establish TokMem as a scalable and modular
alternative to prompt engineering and fine-tuning, offering an explicit
procedural memory for LLMs.

</details>


### [16] [LongCodeZip: Compress Long Context for Code Language Models](https://arxiv.org/abs/2510.00446)
*Yuling Shi,Yichun Qian,Hongyu Zhang,Beijun Shen,Xiaodong Gu*

Main category: cs.CL

TL;DR: 提出了LongCodeZip，一种针对代码大语言模型的双阶段代码压缩框架，显著提高了长上下文代码生成的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 当前长上下文代码生成面临高API成本和生成延迟，且现有上下文剪枝方法忽视代码特有结构与依赖，导致性能不足。

Method: LongCodeZip采用粗粒度压缩（基于指令的条件困惑度筛选函数级代码块）和细粒度压缩（基于困惑度分割函数并根据自适应令牌预算选择子集）两阶段策略。

Result: 在代码补全、摘要和问答等任务中，LongCodeZip实现最高5.6倍压缩率且不降低任务性能，优于现有基线方法。

Conclusion: 通过有效压缩代码上下文尺寸且保留关键信息，LongCodeZip提升了代码大模型处理大规模代码的能力和效率，推动代码智能应用发展。

Abstract: Code generation under long contexts is becoming increasingly critical as
Large Language Models (LLMs) are required to reason over extensive information
in the codebase. While recent advances enable code LLMs to process long inputs,
high API costs and generation latency remain substantial bottlenecks. Existing
context pruning techniques, such as LLMLingua, achieve promising results for
general text but overlook code-specific structures and dependencies, leading to
suboptimal performance in programming tasks. In this paper, we propose
LongCodeZip, a novel plug-and-play code compression framework designed
specifically for code LLMs. LongCodeZip employs a dual-stage strategy: (1)
coarse-grained compression, which identifies and ranks function-level chunks
using conditional perplexity with respect to the instruction, retaining only
the most relevant functions; and (2) fine-grained compression, which segments
retained functions into blocks based on perplexity and selects an optimal
subset under an adaptive token budget to maximize relevance. Evaluations across
multiple tasks, including code completion, summarization, and question
answering, show that LongCodeZip consistently outperforms baseline methods,
achieving up to a 5.6x compression ratio without degrading task performance. By
effectively reducing context size while preserving essential information,
LongCodeZip enables LLMs to better scale to real-world, large-scale code
scenarios, advancing the efficiency and capability of code intelligence
applications.

</details>


### [17] [Enhancing Rating Prediction with Off-the-Shelf LLMs Using In-Context User Reviews](https://arxiv.org/abs/2510.00449)
*Koki Ryu,Hitomi Yanaka*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLMs）在李克特量表评分预测任务中的表现，发现用户书写的评论和生成假设评论能显著提升预测效果，性能可与传统矩阵分解方法相媲美，展示了LLMs在冷启动问题上的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 个性化调整大语言模型输出以符合用户偏好是研究热点，现有研究多集中于分类或排序，缺少对需语言和数学推理的回归任务——李克特量表评分预测的探讨，且LLMs的离线使用效果有待研究。

Method: 对八个预训练大语言模型在三个数据集上进行全面实验，提供不同上下文信息（包括用户评论和具体物品评论），并尝试先生成假设评论作为提示以提升评分预测效果。

Result: 用户书写的评论显著提升了LLMs的评分预测性能，效果接近传统矩阵分解方法；具体物品评论比一般偏好描述更有效；先生成假设评论可进一步提高性能。

Conclusion: 离线预训练的LLMs在李克特评分预测任务中展示了良好性能，用户评论和假设评论提示是提升效果的关键，表明LLMs在工业应用特别是冷启动问题上有广泛潜力。

Abstract: Personalizing the outputs of large language models (LLMs) to align with
individual user preferences is an active research area. However, previous
studies have mainly focused on classification or ranking tasks and have not
considered Likert-scale rating prediction, a regression task that requires both
language and mathematical reasoning to be solved effectively. This task has
significant industrial applications, but the utilization of LLMs remains
underexplored, particularly regarding the capabilities of off-the-shelf LLMs.
This study investigates the performance of off-the-shelf LLMs on rating
prediction, providing different in-context information. Through comprehensive
experiments with eight models across three datasets, we demonstrate that
user-written reviews significantly improve the rating prediction performance of
LLMs. This result is comparable to traditional methods like matrix
factorization, highlighting the potential of LLMs as a promising solution for
the cold-start problem. We also find that the reviews for concrete items are
more effective than general preference descriptions that are not based on any
specific item. Furthermore, we discover that prompting LLMs to first generate a
hypothetical review enhances the rating prediction performance. Our code is
available at https://github.com/ynklab/rating-prediction-with-reviews.

</details>


### [18] [Agent Fine-tuning through Distillation for Domain-specific LLMs in Microdomains](https://arxiv.org/abs/2510.00482)
*Yawen Xue,Masaya Tsunokake,Yuta Koreeda,Ekant Muljibhai Amin,Takashi Sumiyoshi,Yasuhiro Sogawa*

Main category: cs.CL

TL;DR: 本文研究了将大语言模型通过代理微调应用于日立JP1中间件这一专业IT操作微领域，提升了模型在该领域的决策准确性和搜索效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于少量提示的上下文学习方式输入过长且计算开销大，且现有研究多聚焦于通用领域，在专业技术微领域的适用性不明。

Method: 通过使用JP1领域手册和LLM自生成的推理轨迹对大语言模型进行微调，结合检索增强生成和上下文答案提取器，用于提高信息相关性。

Result: 在JP1认证考试题目上，方法较基础模型性能提升了14%。

Conclusion: 代理微调策略能有效提升大语言模型在特定复杂微领域的推理和决策能力，展现了良好的领域适应潜力。

Abstract: Agentic large language models (LLMs) have become prominent for autonomously
interacting with external environments and performing multi-step reasoning
tasks. Most approaches leverage these capabilities via in-context learning with
few-shot prompts, but this often results in lengthy inputs and higher
computational costs. Agent fine-tuning offers an alternative by enabling LLMs
to internalize procedural reasoning and domain-specific knowledge through
training on relevant data and demonstration trajectories. While prior studies
have focused on general domains, their effectiveness in specialized technical
microdomains remains unclear. This paper explores agent fine-tuning for domain
adaptation within Hitachi's JP1 middleware, a microdomain for specialized IT
operations. We fine-tuned LLMs using JP1-specific datasets derived from domain
manuals and distilled reasoning trajectories generated by LLMs themselves,
enhancing decision making accuracy and search efficiency. During inference, we
used an agentic prompt with retrieval-augmented generation and introduced a
context-answer extractor to improve information relevance. On JP1 certification
exam questions, our method achieved a 14% performance improvement over the base
model, demonstrating the potential of agent fine-tuning for domain-specific
reasoning in complex microdomains.

</details>


### [19] [Agent-ScanKit: Unraveling Memory and Reasoning of Multimodal Agents via Sensitivity Perturbations](https://arxiv.org/abs/2510.00496)
*Pengzhou Cheng,Lingzhong Dong,Zeng Wu,Zongru Wu,Xiangru Tang,Chengwei Qin,Zhuosheng Zhang,Gongshen Liu*

Main category: cs.CL

TL;DR: 该文提出了Agent-ScanKit框架，系统评估多模态代理在图形用户界面（GUI）任务中的记忆与推理能力，发现现有模型更依赖机械记忆，推理能力有限，泛化效果不佳。


<details>
  <summary>Details</summary>
Motivation: 尽管已有多种策略提升多模态代理在GUI中的交互能力，但面对复杂或领域外任务时，模型的可靠性不足，质疑其推理是否存在虚假。

Method: 设计了Agent-ScanKit框架，包含视觉引导、文本引导和结构引导三种正交探测范式，用以在无须访问模型内部的情况下量化记忆与推理的贡献。

Result: 在五个公开GUI基准和18个多模态代理上实验，结果显示机械记忆往往主导性能，模型主要作为训练知识的检索器，具有有限的泛化能力。

Conclusion: 强调需构建更健壮的推理模型以提升多模态代理在实际场景的可靠性，为多模态代理的可靠发展提供了重要见解。

Abstract: Although numerous strategies have recently been proposed to enhance the
autonomous interaction capabilities of multimodal agents in graphical user
interface (GUI), their reliability remains limited when faced with complex or
out-of-domain tasks. This raises a fundamental question: Are existing
multimodal agents reasoning spuriously? In this paper, we propose
\textbf{Agent-ScanKit}, a systematic probing framework to unravel the memory
and reasoning capabilities of multimodal agents under controlled perturbations.
Specifically, we introduce three orthogonal probing paradigms: visual-guided,
text-guided, and structure-guided, each designed to quantify the contributions
of memorization and reasoning without requiring access to model internals. In
five publicly available GUI benchmarks involving 18 multimodal agents, the
results demonstrate that mechanical memorization often outweighs systematic
reasoning. Most of the models function predominantly as retrievers of
training-aligned knowledge, exhibiting limited generalization. Our findings
underscore the necessity of robust reasoning modeling for multimodal agents in
real-world scenarios, offering valuable insights toward the development of
reliable multimodal agents.

</details>


### [20] [MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance](https://arxiv.org/abs/2510.00499)
*Xingjian Zhao,Zhe Xu,Luozhijie Jin,Yang Wang,Hanfu Chen,Yaozhou Jiang,Ke Chen,Ruixiao Li,Mingshu Chen,Ruiming Wang,Wenbo Zhang,Yiyang Zhang,Donghua Yu,Yang Gao,Xiaogui Yang,Yitian Gong,Yuanfan Xu,Qinyuan Cheng,Zhaoye Fei,Shimin Li,Yaqian Zhou,Xuanjing Huang,Xipeng Qiu*

Main category: cs.CL

TL;DR: 本文提出了MOSS-Speech，一种无需依赖文本指导的端到端真实语音到语音大型语言模型，提升了语音理解和生成的表达性及效率。


<details>
  <summary>Details</summary>
Motivation: 传统语音对话系统依赖多级流水线处理，丢失了副语言线索且限制了表达能力；现有端到端模型虽减少延迟，但仍依赖文本中介，形成瓶颈。

Method: 提出基于模态分层结构和冻结预训练策略，结合预训练文本大型语言模型的推理能力和知识，同时新增原生语音功能，实现直接的语音理解和生成。

Result: 模型在口语问答任务中取得了最先进的成绩，语音到语音性能与现有文本引导系统相当，同时具备竞争力的文本表现。

Conclusion: 通过缩小文本引导与直接语音生成的差距，本文工作建立了一个表达性更强且高效的端到端语音交互新范式。

Abstract: Spoken dialogue systems often rely on cascaded pipelines that transcribe,
process, and resynthesize speech. While effective, this design discards
paralinguistic cues and limits expressivity. Recent end-to-end methods reduce
latency and better preserve these cues, yet still rely on text intermediates,
creating a fundamental bottleneck. We present MOSS-Speech, a true
speech-to-speech large language model that directly understands and generates
speech without relying on text guidance. Our approach combines a modality-based
layer-splitting architecture with a frozen pre-training strategy, preserving
the reasoning and knowledge of pretrained text LLMs while adding native speech
capabilities. Experiments show that our model achieves state-of-the-art results
in spoken question answering and delivers comparable speech-to-speech
performance relative to existing text-guided systems, while still maintaining
competitive text performance. By narrowing the gap between text-guided and
direct speech generation, our work establishes a new paradigm for expressive
and efficient end-to-end speech interaction.

</details>


### [21] [Graph2Eval: Automatic Multimodal Task Generation for Agents via Knowledge Graphs](https://arxiv.org/abs/2510.00507)
*Yurun Chen,Xavier Hu,Yuhan Liu,Ziqi Wang,Zeyi Liao,Lin Chen,Feng Wei,Yuxi Qian,Bo Zheng,Keting Yin,Shengyu Zhang*

Main category: cs.CL

TL;DR: 本论文提出Graph2Eval框架，利用知识图自动生成多模态文档理解和网页交互任务，实现对多种智能代理的综合评测。


<details>
  <summary>Details</summary>
Motivation: 现有静态数据集无法充分评估多模态大模型代理的动态环境适应和多任务能力，且现有自动任务生成多限于文本或图片，缺乏动态交互任务。

Method: 基于多源数据构建知识图，通过子图采样、任务模板和元路径生成任务，并通过多阶段过滤保障任务质量和可执行性，支持单代理、多代理及网页代理的终端评测。

Result: 提出的Graph2Eval-Bench包含1319个任务，涵盖文档理解和网页交互，实验表明任务有效区分不同代理和模型的推理、协作及交互能力差异。

Conclusion: Graph2Eval提供了一种系统的多模态任务自动生成和智能代理评测方法，有助于揭示代理在复杂互动环境中的能力缺口，推动更全面的评估标准。

Abstract: As multimodal LLM-driven agents continue to advance in autonomy and
generalization, evaluation based on static datasets can no longer adequately
assess their true capabilities in dynamic environments and diverse tasks.
Existing LLM-based synthetic data methods are largely designed for LLM training
and evaluation, and thus cannot be directly applied to agent tasks that require
tool use and interactive capabilities. While recent studies have explored
automatic agent task generation with LLMs, most efforts remain limited to text
or image analysis, without systematically modeling multi-step interactions in
web environments. To address these challenges, we propose Graph2Eval, a
knowledge graph-based framework that automatically generates both multimodal
document comprehension tasks and web interaction tasks, enabling comprehensive
evaluation of agents' reasoning, collaboration, and interactive capabilities.
In our approach, knowledge graphs constructed from multi-source external data
serve as the task space, where we translate semantic relations into structured
multimodal tasks using subgraph sampling, task templates, and meta-paths. A
multi-stage filtering pipeline based on node reachability, LLM scoring, and
similarity analysis is applied to guarantee the quality and executability of
the generated tasks. Furthermore, Graph2Eval supports end-to-end evaluation of
multiple agent types (Single-Agent, Multi-Agent, Web Agent) and measures
reasoning, collaboration, and interaction capabilities. We instantiate the
framework with Graph2Eval-Bench, a curated dataset of 1,319 tasks spanning
document comprehension and web interaction scenarios. Experiments show that
Graph2Eval efficiently generates tasks that differentiate agent and model
performance, revealing gaps in reasoning, collaboration, and web interaction
across different settings and offering a new perspective for agent evaluation.

</details>


### [22] [Copy-Paste to Mitigate Large Language Model Hallucinations](https://arxiv.org/abs/2510.00508)
*Yongchao Long,Xian Wu,Yingying Zhang,Xianbin Wen,Yuxi Zhou,Shenda Hong*

Main category: cs.CL

TL;DR: 本文提出了CopyPasteLLM，通过高复制率训练显著提升了大型语言模型在检索增强生成中的上下文真实性和减少幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 当前检索增强生成方法中，模型对上下文的信任度不一致，导致幻觉现象影响生成结果的可靠性。

Method: 通过两阶段高复制率响应偏好训练，设计三种提示方法提升复制率，结合自动生成高复制率偏好数据的训练管线，训练CopyPasteLLM模型，同时提出Context-Parameter Copying Capturing算法分析其效果。

Result: CopyPasteLLM在FaithEval、ConFiQA和PubMedQA数据集上表现优异，准确率提升12.2%至24.5%，训练样本仅为基线的1/50。

Conclusion: 通过提高响应的复制率，CopyPasteLLM有效增强了上下文信任度和可靠性，减少了幻觉现象，展现了更强的知识依赖调整能力。

Abstract: While Retrieval-Augmented Generation (RAG) enables large language models
(LLMs) to generate contextually grounded responses, contextual faithfulness
remains challenging as LLMs may not consistently trust provided context,
leading to hallucinations that undermine reliability. We observe an inverse
correlation between response copying degree and context-unfaithful
hallucinations on RAGTruth, suggesting that higher copying degrees reduce
hallucinations by fostering genuine contextual belief. We propose CopyPasteLLM,
obtained through two-stage high-copying response preference training. We design
three prompting methods to enhance copying degree, demonstrating that
high-copying responses achieve superior contextual faithfulness and
hallucination control. These approaches enable a fully automated pipeline that
transforms generated responses into high-copying preference data for training
CopyPasteLLM. On FaithEval, ConFiQA and PubMedQA, CopyPasteLLM achieves best
performance in both counterfactual and original contexts, remarkably with 12.2%
to 24.5% accuracy improvements on FaithEval over the best baseline, while
requiring only 365 training samples -- 1/50th of baseline data. To elucidate
CopyPasteLLM's effectiveness, we propose the Context-Parameter Copying
Capturing algorithm. Interestingly, this reveals that CopyPasteLLM recalibrates
reliance on internal parametric knowledge rather than external knowledge during
generation. All codes are available at
https://github.com/longyongchao/CopyPasteLLM

</details>


### [23] [JoyAgent-JDGenie: Technical Report on the GAIA](https://arxiv.org/abs/2510.00510)
*Jiarun Liu,Shiyue Xu,Shangkun Liu,Yang Li,Wen Liu,Min Liu,Xiaoqing Zhou,Hanmin Wang,Shilin Jia,zhen Wang,Shaohua Tian,Hanhao Li,Junbo Zhang,Yongli Yu,Peng Cao,Haofen Wang*

Main category: cs.CL

TL;DR: 本文提出了一个通用智能体架构，结合多智能体协作、多层次记忆系统和优化的工具集，在复杂任务中展现出良好性能，优于开源系统并接近专有系统水平。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型作为自主智能体应用于复杂任务时，缺乏一个统一设计以提升系统的鲁棒性和适应性。

Method: 设计了一个集合多智能体框架（包含规划、执行智能体及批判模型投票）、分层记忆体系（工作记忆、语义记忆、程序记忆）以及改进的工具套件（搜索、代码执行、多模态解析）。

Result: 在综合基准测试中，该框架稳定优于开源基线，性能接近专有系统。

Conclusion: 系统级整合对提升智能体的扩展性、鲁棒性和适应性至关重要，为构建多领域、多任务的强大AI助手提供了方向。

Abstract: Large Language Models are increasingly deployed as autonomous agents for
complex real-world tasks, yet existing systems often focus on isolated
improvements without a unifying design for robustness and adaptability. We
propose a generalist agent architecture that integrates three core components:
a collective multi-agent framework combining planning and execution agents with
critic model voting, a hierarchical memory system spanning working, semantic,
and procedural layers, and a refined tool suite for search, code execution, and
multimodal parsing. Evaluated on a comprehensive benchmark, our framework
consistently outperforms open-source baselines and approaches the performance
of proprietary systems. These results demonstrate the importance of
system-level integration and highlight a path toward scalable, resilient, and
adaptive AI assistants capable of operating across diverse domains and tasks.

</details>


### [24] [EuroSpeech: A Multilingual Speech Corpus](https://arxiv.org/abs/2510.00514)
*Samuel Pfisterer,Florian Grötschla,Luca A. Lanzendörfer,Florian Yan,Roger Wattenhofer*

Main category: cs.CL

TL;DR: 提出了一种从欧洲议会录音中构建多语言语音数据集的可扩展管道，显著提升了多语言语音识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有多语言语音数据集往往语言样本不足，导致模型在多数语言上表现欠佳。

Method: 设计了包括媒体检索和两阶段对齐算法的管道，处理非逐字转录和长时语音，应用于22个欧洲议会录音。

Result: 提取了超过61千小时语音，19种语言1000小时以上，22种语言500小时以上训练数据。

Conclusion: 在微调ASR模型后，平均词错误率降低41.8%，证明了方法有效性。

Abstract: Recent progress in speech processing has highlighted that high-quality
performance across languages requires substantial training data for each
individual language. While existing multilingual datasets cover many languages,
they often contain insufficient data for most languages. Thus, trained models
perform poorly on the majority of the supported languages. Our work addresses
this challenge by introducing a scalable pipeline for constructing speech
datasets from parliamentary recordings. The proposed pipeline includes robust
components for media retrieval and a two-stage alignment algorithm designed to
handle non-verbatim transcripts and long-form audio. Applying this pipeline to
recordings from 22 European parliaments, we extract over 61k hours of aligned
speech segments, achieving substantial per-language coverage with 19 languages
exceeding 1k hours and 22 languages exceeding 500 hours of high-quality speech
data. We obtain an average 41.8\% reduction in word error rates over baselines
when finetuning an existing ASR model on our dataset, demonstrating the
usefulness of our approach.

</details>


### [25] [Beyond Log Likelihood: Probability-Based Objectives for Supervised Fine-Tuning across the Model Capability Continuum](https://arxiv.org/abs/2510.00526)
*Gaotang Li,Ruizhong Qiu,Xiusi Chen,Heng Ji,Hanghang Tong*

Main category: cs.CL

TL;DR: 本文探讨了监督微调大语言模型时常用的负对数似然（NLL）目标函数的局限，提出了一种基于模型能力连续体调整概率目标函数的方法，有效提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前监督微调大语言模型通常采用负对数似然（NLL）目标函数，但在后训练阶段该目标函数可能不再最优，限制了模型泛化能力。

Method: 作者研究了一类基于概率的目标函数，针对不同模型能力阶段（模型强、中、弱）调整目标函数权重，通过广泛实验验证不同目标函数在不同条件下的表现及其理论分析。

Result: 实验结果显示，靠近模型强端时，倾向利用先验的目标函数（如下调低概率标记的目标）优于NLL；在模型弱端，NLL表现最好；中间阶段则无单一最佳目标函数。

Conclusion: 研究揭示了目标函数表现随模型能力变化的规律，提供了在不同模型能力下调整目标函数的理论依据和实践指导，有助于提升大语言模型的后训练效果。

Abstract: Supervised fine-tuning (SFT) is the standard approach for post-training large
language models (LLMs), yet it often shows limited generalization. We trace
this limitation to its default training objective: negative log likelihood
(NLL). While NLL is classically optimal when training from scratch,
post-training operates in a different paradigm and could violate its optimality
assumptions, where models already encode task-relevant priors and supervision
can be long and noisy. To this end, we study a general family of
probability-based objectives and characterize their effectiveness under
different conditions. Through comprehensive experiments and extensive ablation
studies across 7 model backbones, 14 benchmarks, and 3 domains, we uncover a
critical dimension that governs objective behavior: the model-capability
continuum. Near the model-strong end, prior-leaning objectives that downweight
low-probability tokens (e.g., $-p$, $-p^{10}$, thresholded variants)
consistently outperform NLL; toward the model-weak end, NLL dominates; in
between, no single objective prevails. Our theoretical analysis further
elucidates how objectives trade places across the continuum, providing a
principled foundation for adapting objectives to model capability. Our code is
available at https://github.com/GaotangLi/Beyond-Log-Likelihood.

</details>


### [26] [GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness](https://arxiv.org/abs/2510.00536)
*Kung-Hsiang Huang,Haoyi Qiu,Yutong Dai,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.CL

TL;DR: 本文提出了一种针对图形用户界面（GUI）代理的键值缓存压缩方法GUI-KV，利用空间显著性引导和时间冗余评分策略，提高缓存利用效率，降低计算成本，同时保持较高的准确率。


<details>
  <summary>Details</summary>
Motivation: GUI代理在处理高分辨率截图和长任务序列时面临效率低下的问题，传统缓存压缩方法未能充分利用GUI的空间和时间冗余。

Method: 分析GUI代理工作负载中的注意力模式，提出统一预算分配策略，并设计GUI-KV方法，包括空间显著性引导和时间冗余评分两个创新技术，无需重新训练即可应用。

Result: 在多个GUI代理基准测试中，GUI-KV优于现有缓存压缩方法，在AgentNetBench的五截图任务中，将解码计算量减少38.9%，同时步骤准确率提升4.1%。

Conclusion: 通过利用GUI特有的空间和时间冗余，GUI-KV实现了高效且可靠的缓存压缩，提高了GUI代理的推理速度和性能。

Abstract: Graphical user interface (GUI) agents built on vision-language models have
emerged as a promising approach to automate human-computer workflows. However,
they also face the inefficiency challenge as they process long sequences of
high-resolution screenshots and solving long-horizon tasks, making inference
slow, costly and memory-bound. While key-value (KV) caching can mitigate this,
storing the full cache is prohibitive for image-heavy contexts. Existing
cache-compression methods are sub-optimal as they do not account for the
spatial and temporal redundancy of GUIs. In this work, we first analyze
attention patterns in GUI agent workloads and find that, unlike in natural
images, attention sparsity is uniformly high across all transformer layers.
This insight motivates a simple uniform budget allocation strategy, which we
show empirically outperforms more complex layer-varying schemes. Building on
this, we introduce GUI-KV, a plug-and-play KV cache compression method for GUI
agents that requires no retraining. GUI-KV combines two novel techniques: (i)
spatial saliency guidance, which augments attention scores with the L2 norm of
hidden states to better preserve semantically important visual tokens, and (ii)
temporal redundancy scoring, which projects previous frames' keys onto the
current frame's key subspace to preferentially prune redundant history. Across
standard GUI agent benchmarks and models, GUI-KV outperforms competitive KV
compression baselines, closely matching full-cache accuracy at modest budgets.
Notably, in a 5-screenshot setting on the AgentNetBench benchmark, GUI-KV
reduces decoding FLOPs by 38.9% while increasing step accuracy by 4.1% over the
full-cache baseline. These results demonstrate that exploiting GUI-specific
redundancies enables efficient and reliable agent performance.

</details>


### [27] [ThinkBrake: Mitigating Overthinking in Tool Reasoning](https://arxiv.org/abs/2510.00546)
*Minjae Oh,Sangjun Song,Seungkyu Lee,Sungmin Jo,Yohan Jo*

Main category: cs.CL

TL;DR: 小型推理模型在使用工具时经常出现过度思考的问题，即已找到正确的工具调用配置后仍继续推理，导致最终调用错误。研究通过在句子边界注入终止标记诊断过度思考，并提出了ThinkBrake，一种基于概率边际监测的无训练终止策略，有效减少了推理步骤并保持或提升准确率。


<details>
  <summary>Details</summary>
Motivation: 小型推理模型在工具调用时容易过度思考，导致推理效率低下且准确率下降，现有研究多集中于数学推理，工具推理领域尚未充分探索，有必要设计方法减少冗余推理步骤。

Method: 通过在句子边界注入</think>标记进行oracle终止诊断，发现模型存在过度思考问题。基于此，提出ThinkBrake方法，该方法通过监测句子边界处</think>与当前最高概率词的概率差距，当差距减小时即触发终止，属于无训练的解码启发式方法。

Result: 在伯克利函数调用排行榜（BFCL）测试中，oracle终止将准确率从85.8%提升至94.2%，并减少80-94%的tokens。ThinkBrake在BFCL单轮、非实时和实时三种场景下均能保持或提升准确率，同时减少最多25%的tokens，表现优于多种基线方法。

Conclusion: 小型推理模型在工具调用时存在显著的过度思考现象，通过引入早期终止机制尤其是ThinkBrake方法，可以有效减少无效推理步骤，在保证或提高准确率的同时显著提升推理效率，展示了工具推理领域的潜在改进空间。

Abstract: Small reasoning models (SRMs) often overthink during tool use: they reach a
correct tool-argument configuration, then continue reasoning and overwrite it
with an incorrect final call. We diagnose overthinking via oracle rollouts that
inject </think> at sentence boundaries. On the Berkeley Function Calling
Leaderboard (BFCL), this oracle termination lifts average accuracy from 85.8\%
to 94.2\% while reducing tokens by 80-94\%, revealing substantial recoverable
headroom and potential redundant reasoning. While prior work on concise
reasoning has largely targeted mathematics, tool reasoning remains
underexplored. We adapt various early-termination baselines to tool use and
introduce ThinkBrake, a training-free decoding heuristic. ThinkBrake monitors
the log-probability margin between </think> and the current top token at
sentence boundaries and triggers termination when this margin becomes small.
Across BFCL's single turn, non-live and live splits, ThinkBrake preserves or
improves accuracy while reducing tokens up to 25\%, outperforming various
baselines.

</details>


### [28] [Are Large Language Models Chronically Online Surfers? A Dataset for Chinese Internet Meme Explanation](https://arxiv.org/abs/2510.00567)
*Yubo Xie,Chenkai Wang,Zongyang Ma,Fahui Miao*

Main category: cs.CL

TL;DR: 本文提出了CHIME数据集，用于评估大型语言模型对中文网络梗的理解能力。通过解释梗的意义、起源及生成示例句两项任务发现，模型对文化和语言细节敏感的梗理解较弱，且难以准确识别梗起源。在多项选择填空任务中，模型表现虽有正确率，但仍不及人类水平。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然训练于大量网络文本，但是否真正理解网络上迅速传播的梗仍未明晰，特别是文化语言细节丰富的中文网络梗。为此，需构建专门的数据集并设计评测任务。

Method: 构建CHIME数据集，收集中文流行短语梗并附注释信息（意义、起源、示例句、类型等）。设计解释梗和起源的生成任务，以及多选题填空任务以评估语言模型对梗的理解和应用能力。

Result: 模型能解释部分梗意义，但在文化语言细节丰富的梗上表现不佳，起源识别效果较差。多项选择问答中，模型正确率有待提升，整体低于人类水平。

Conclusion: 现有大型语言模型在中文网络梗理解上存在不足，尤其是文化和语言细节方面。CHIME数据集公开将促进相关领域的研究与进步。

Abstract: Large language models (LLMs) are trained on vast amounts of text from the
Internet, but do they truly understand the viral content that rapidly spreads
online -- commonly known as memes? In this paper, we introduce CHIME, a dataset
for CHinese Internet Meme Explanation. The dataset comprises popular
phrase-based memes from the Chinese Internet, annotated with detailed
information on their meaning, origin, example sentences, types, etc. To
evaluate whether LLMs understand these memes, we designed two tasks. In the
first task, we assessed the models' ability to explain a given meme, identify
its origin, and generate appropriate example sentences. The results show that
while LLMs can explain the meanings of some memes, their performance declines
significantly for culturally and linguistically nuanced meme types.
Additionally, they consistently struggle to provide accurate origins for the
memes. In the second task, we created a set of multiple-choice questions (MCQs)
requiring LLMs to select the most appropriate meme to fill in a blank within a
contextual sentence. While the evaluated models were able to provide correct
answers, their performance remains noticeably below human levels. We have made
CHIME public and hope it will facilitate future research on computational meme
understanding.

</details>


### [29] [ReSeek: A Self-Correcting Framework for Search Agents with Instructive Rewards](https://arxiv.org/abs/2510.00568)
*Shiyu Li,Yang Tang,Yifan Wang,Peiming Li,Xi Chen*

Main category: cs.CL

TL;DR: 本文提出了ReSeek框架，通过引入自我纠正机制和密集的过程奖励函数，提高了基于大语言模型的搜索代理在复杂推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 目前基于强化学习的搜索代理依赖稀疏或规则化的奖励，容易导致代理陷入次优或错误的推理路径且无法自我纠正。

Method: ReSeek框架赋予代理动态识别并纠正错误路径的能力，通过特殊的JUDGE操作判断信息并重新规划搜索策略，同时设计了包含正确性奖励和实用性奖励的密集激励机制。

Result: 在新引入的复杂推理基准FictionalHot上，ReSeek训练的代理在任务成功率和路径忠实度上显著优于当前最先进的方法。

Conclusion: ReSeek框架通过自我纠正和密集奖励机制有效提升了搜索代理的推理能力和表现，展示了其在知识密集型任务中的潜力。

Abstract: Search agents powered by Large Language Models (LLMs) have demonstrated
significant potential in tackling knowledge-intensive tasks. Reinforcement
learning (RL) has emerged as a powerful paradigm for training these agents to
perform complex, multi-step reasoning. However, prior RL-based methods often
rely on sparse or rule-based rewards, which can lead agents to commit to
suboptimal or erroneous reasoning paths without the ability to recover. To
address these limitations, we propose ReSeek, a novel self-correcting framework
for training search agents. Our framework introduces a self-correction
mechanism that empowers the agent to dynamically identify and recover from
erroneous search paths during an episode. By invoking a special JUDGE action,
the agent can judge the information and re-plan its search strategy. To guide
this process, we design a dense, instructive process reward function, which
decomposes into a correctness reward for retrieving factual information and a
utility reward for finding information genuinely useful for the query.
Furthermore, to mitigate the risk of data contamination in existing datasets,
we introduce FictionalHot, a new and challenging benchmark with recently
curated questions requiring complex reasoning. Being intuitively reasonable and
practically simple, extensive experiments show that agents trained with ReSeek
significantly outperform SOTA baselines in task success rate and path
faithfulness.

</details>


### [30] [CoT Vectors: Transferring and Probing the Reasoning Mechanisms of LLMs](https://arxiv.org/abs/2510.00579)
*Li Li,Ziyi Wang,Yongliang Wu,Jianfei Cai,Xu Yang*

Main category: cs.CL

TL;DR: 本文提出了CoT Vectors，一种用于多步骤推理的紧凑表征，旨在提升大型语言模型的推理能力且成本更低。


<details>
  <summary>Details</summary>
Motivation: 现有基于Chain-of-Thought（CoT）的方法，如上下文学习和微调，成本高且效率低下，需寻找低成本高效的推理改进方法。

Method: 引入CoT Vectors，通过提取和学习的方式，将任务通用的多步推理知识编码为紧凑向量，并采用教师-学生框架优化学习的稳定性与鲁棒性。

Result: 在多种基准测试和模型上，CoT Vectors表现优于现有基线，接近参数高效微调方法的效果，且训练参数更少。此外，探究了其表现随潜在空间结构、信息密度等因素的变化。

Conclusion: CoT Vectors为低成本有效提升LLMs多步骤推理能力提供了新思路，并揭示了多步推理的功能组织机制，具备广泛应用潜力。

Abstract: Chain-of-Thought (CoT) prompting has emerged as a powerful approach to
enhancing the reasoning capabilities of Large Language Models (LLMs). However,
existing implementations, such as in-context learning and fine-tuning, remain
costly and inefficient. To improve CoT reasoning at a lower cost, and inspired
by the task vector paradigm, we introduce CoT Vectors, compact representations
that encode task-general, multi-step reasoning knowledge. Through experiments
with Extracted CoT Vectors, we observe pronounced layer-wise instability,
manifesting as a U-shaped performance curve that reflects a systematic
three-stage reasoning process in LLMs. To address this limitation, we propose
Learnable CoT Vectors, optimized under a teacher-student framework to provide
more stable and robust guidance. Extensive evaluations across diverse
benchmarks and models demonstrate that CoT Vectors not only outperform existing
baselines but also achieve performance comparable to parameter-efficient
fine-tuning methods, while requiring fewer trainable parameters. Moreover, by
treating CoT Vectors as a probe, we uncover how their effectiveness varies due
to latent space structure, information density, acquisition mechanisms, and
pre-training differences, offering new insights into the functional
organization of multi-step reasoning in LLMs. The source code will be released.

</details>


### [31] [SAGE-LD: Towards Scalable and Generalizable End-to-End Language Diarization via Simulated Data Augmentation](https://arxiv.org/abs/2510.00582)
*Sangmin Lee,Woongjib Choi,Jihyun Kim,Hong-Goo Kang*

Main category: cs.CL

TL;DR: 提出了一种支持多语言的神经语言分段模型，通过多语言感知的可学习查询架构和大规模代码切换数据预训练，在多样化环境中实现了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统语言分段方法面临数据稀缺和架构优化的限制，难以适应真实世界多语言环境。

Method: 结合基于查询的多语言感知架构和基于模拟代码切换数据的大规模预训练，共同提升模型泛化能力。

Result: 在多个语言分段基准测试中，取得了23%到52%的性能提升，达到了最新水平。

Conclusion: 该方法不仅推动了语言分段研究的发展，也为代码切换语音技术奠定了基础框架。

Abstract: In this paper, we present a neural spoken language diarization model that
supports an unconstrained span of languages within a single framework. Our
approach integrates a learnable query-based architecture grounded in
multilingual awareness, with large-scale pretraining on simulated
code-switching data. By jointly leveraging these two components, our method
overcomes the limitations of conventional approaches in data scarcity and
architecture optimization, and generalizes effectively to real-world
multilingual settings across diverse environments. Experimental results
demonstrate that our approach achieves state-of-the-art performance on several
language diarization benchmarks, with a relative performance improvement of 23%
to 52% over previous methods. We believe that this work not only advances
research in language diarization but also establishes a foundational framework
for code-switching speech technologies.

</details>


### [32] [Tenyidie Syllabification corpus creation and deep learning applications](https://arxiv.org/abs/2510.00629)
*Teisovi Angami,Kevisino Khate*

Main category: cs.CL

TL;DR: 本文针对低资源的滇缅语支语言——滇意节语，构建了一个包含10120个音节划分的词汇语料库，并应用包括LSTM、BLSTM、BLSTM+CRF和编码器-解码器在内的深度学习模型进行音节划分任务，最高测试准确率达到99.21%。


<details>
  <summary>Details</summary>
Motivation: 滇意节语作为印度东北部的主要低资源语言，尚未有关于音节划分的研究，而音节划分是多种自然语言处理任务的基础。

Method: 构建了含有10120个音节划分样本的语料库，并应用LSTM、双向LSTM（BLSTM）、BLSTM结合条件随机场（CRF）和编码器-解码器深度学习架构进行模型训练与测试。

Result: 在80:10:10数据划分的训练、验证和测试集上，使用BLSTM模型在测试集上取得了99.21%的最高准确率。

Conclusion: 所构建的数据集和应用的深度学习模型为滇意节语的音节划分提供了有效解决方案，为后续的形态分析、词性标注和机器翻译等自然语言处理任务奠定基础。

Abstract: The Tenyidie language is a low-resource language of the Tibeto-Burman family
spoken by the Tenyimia Community of Nagaland in the north-eastern part of India
and is considered a major language in Nagaland. It is tonal,
Subject-Object-Verb, and highly agglutinative in nature. Being a low-resource
language, very limited research on Natural Language Processing (NLP) has been
conducted. To the best of our knowledge, no work on syllabification has been
reported for this language. Among the many NLP tasks, syllabification or
syllabication is an important task in which the given word syllables are
identified. The contribution of this work is the creation of 10,120 syllabified
Tenyidie words and the application of the Deep Learning techniques on the
created corpus. In this paper, we have applied LSTM, BLSTM, BLSTM+CRF, and
Encoder-decoder deep learning architectures on our created dataset. In our
dataset split of 80:10:10 (train:validation:test) set, we achieved the highest
accuracy of 99.21% with BLSTM model on the test set. This work will find its
application in numerous other NLP applications, such as morphological analysis,
part-of-speech tagging, machine translation, etc, for the Tenyidie Language.
  Keywords: Tenyidie; NLP; syllabification; deep learning; LSTM; BLSTM; CRF;
Encoder-decoder

</details>


### [33] [MCM-DPO: Multifaceted Cross-Modal Direct Preference Optimization for Alt-text Generation](https://arxiv.org/abs/2510.00647)
*Jinlan Fu,Shenzhen Huangfu,Hao Fei,Yichong Huang,Xiaoyu Shen,Xipeng Qiu,See-Kiong Ng*

Main category: cs.CL

TL;DR: 本文提出了一种名为多维度跨模态直接偏好优化（MCM-DPO）的方法，解决了由于用户注释噪声和标准不一致导致的自动生成图像描述（alt-text）效果有限的问题，通过学习选择更优的偏好对提升生成质量，同时构建了两个大规模高质量数据集支持研究。


<details>
  <summary>Details</summary>
Motivation: 现有大规模视觉语言模型在自动生成图像描述方面受限于用户注释质量低、标准不一致以及模型对上下文信息敏感度不足，基于监督学习的微调方法因依赖准确注释而难以取得好效果。

Method: 提出MCM-DPO方法，通过在单一、成对和多偏好维度上跨文本、视觉及跨模态因素优化偏好选择，无需依赖精确注释数据。同时构建了包含202k注释样本和18k偏好对的新数据集TAlt和PAlt。

Result: 实验表明MCM-DPO方法在图像描述生成任务中，性能持续优于传统的监督微调和直接偏好优化方法，达到了新的性能水平。

Conclusion: MCM-DPO通过多维度跨模态偏好优化有效提升了自动图像描述质量，并辅助建立了高质量数据集，为未来该领域研究提供了新思路和数据资源。

Abstract: The alt-text generation task produces concise, context-relevant descriptions
of images, enabling blind and low-vision users to access online images. Despite
the capabilities of large vision-language models, alt-text generation
performance remains limited due to noisy user annotations, inconsistent
standards, and MLLMs' insensitivity to contextual information. Previous efforts
to fine-tune MLLMs using supervised fine-tuning (SFT) have struggled, as SFT
relies on accurate target annotations, which are often flawed in user-generated
alt-text. To address this, we propose Multi-faceted Cross-modal Direct
Preference Optimization (MCM-DPO), which improves alt-text generation by
learning to identify better options in preference pairs without requiring
precise annotations. MCM-DPO optimizes preferences across single, paired, and
multi-preference dimensions, covering textual, visual, and cross-modal factors.
In light of the scarcity of high-quality annotated and preference-labeled
datasets for alt-text, we constructed two large-scale, high-quality datasets
named TAlt and PAlt, sourced from Twitter and Pinterest. These datasets include
202k annotated alt-text samples and 18k preference pairs that cover diverse
preference dimensions, aiming to support further research in this domain.
Experimental results show that our proposed MCM-DPO method consistently
outperforms both DPO and SFT, establishing a new state of the art in alt-text
generation. We release the code and data here:
https://github.com/LVUGAI/MCM-DPO

</details>


### [34] [Facilitating Cognitive Accessibility with LLMs: A Multi-Task Approach to Easy-to-Read Text Generation](https://arxiv.org/abs/2510.00662)
*François Ledoyen,Gaël Dias,Jeremie Pantin,Alexis Lechervy,Fabrice Maurel,Youssef Chahir*

Main category: cs.CL

TL;DR: 本研究利用多任务学习方法，结合文本摘要、简化与易读文本生成，利用大语言模型自动生成面向认知障碍群体的易读文本。


<details>
  <summary>Details</summary>
Motivation: 易读文本的手工制作费时费力，亟需高效自动生成方法以促进认知障碍者的信息公平获取。

Method: 提出多任务学习框架，结合文本摘要、简化及易读文本生成任务，采用基于多任务检索增强生成(RAG)和参数高效微调(MTL-LoRA)两种策略，使用新数据集ETR-fr训练Mistral-7B和LLaMA-3-8B模型。

Result: 多任务学习策略优于单任务基线，RAG策略在域外泛化表现良好，MTL-LoRA在域内配置下性能最佳。

Conclusion: 多任务学习在自动生成易读文本方面有效，结合RAG和MTL-LoRA策略可针对不同应用场景实现性能优化，推动认知障碍者信息无障碍获取。

Abstract: Simplifying complex texts is essential for ensuring equitable access to
information, especially for individuals with cognitive impairments. The
Easy-to-Read (ETR) initiative offers a framework for making content accessible
to the neurodivergent population, but the manual creation of such texts remains
time-consuming and resource-intensive. In this work, we investigate the
potential of large language models (LLMs) to automate the generation of ETR
content. To address the scarcity of aligned corpora and the specificity of ETR
constraints, we propose a multi-task learning (MTL) approach that trains models
jointly on text summarization, text simplification, and ETR generation. We
explore two different strategies: multi-task retrieval-augmented generation
(RAG) for in-context learning, and MTL-LoRA for parameter-efficient
fine-tuning. Our experiments with Mistral-7B and LLaMA-3-8B, based on ETR-fr, a
new high-quality dataset, demonstrate the benefits of multi-task setups over
single-task baselines across all configurations. Moreover, results show that
the RAG-based strategy enables generalization in out-of-domain settings, while
MTL-LoRA outperforms all learning strategies within in-domain configurations.

</details>


### [35] [Inclusive Easy-to-Read Generation for Individuals with Cognitive Impairments](https://arxiv.org/abs/2510.00691)
*François Ledoyen,Gaël Dias,Alexis Lechervy,Jeremie Pantin,Fabrice Maurel,Youssef Chahir,Elisa Gouzonnat,Mélanie Berthelot,Stanislas Moravac,Armony Altinier,Amy Khairalla*

Main category: cs.CL

TL;DR: 本文提出了ETR-fr数据集和基于PLMs与LLMs的轻量级微调方法，实现了自动生成符合欧洲易读性(Easy-to-Read)指南的文本，旨在帮助认知障碍者获得更易理解的信息。


<details>
  <summary>Details</summary>
Motivation: 认知障碍者需易读文本以实现自主和完整公民权，但传统手工改写成本高且难以扩展，AI生成易读文本面临数据集不足和模型适应等挑战。

Method: 构建首个符合欧洲ETR指南的数据集ETR-fr，对预训练语言模型(PLMs)和大型语言模型(LLMs)进行参数高效微调，建立文本生成基线；设计自动评估指标结合36问人工评测框架，确保输出质量符合易读性标准。

Result: 实验结果显示，PLMs与LLMs表现相当，且能够有效适应领域外文本，验证了提出方法的有效性和生成文本的高质量。

Conclusion: 提出的方法和数据集为自动生成易读文本提供了有力支撑，促进认知障碍群体的信息可及性，推动在医疗、教育和公民生活中的实际应用。

Abstract: Ensuring accessibility for individuals with cognitive impairments is
essential for autonomy, self-determination, and full citizenship. However,
manual Easy-to-Read (ETR) text adaptations are slow, costly, and difficult to
scale, limiting access to crucial information in healthcare, education, and
civic life. AI-driven ETR generation offers a scalable solution but faces key
challenges, including dataset scarcity, domain adaptation, and balancing
lightweight learning of Large Language Models (LLMs). In this paper, we
introduce ETR-fr, the first dataset for ETR text generation fully compliant
with European ETR guidelines. We implement parameter-efficient fine-tuning on
PLMs and LLMs to establish generative baselines. To ensure high-quality and
accessible outputs, we introduce an evaluation framework based on automatic
metrics supplemented by human assessments. The latter is conducted using a
36-question evaluation form that is aligned with the guidelines. Overall
results show that PLMs perform comparably to LLMs and adapt effectively to
out-of-domain texts.

</details>


### [36] [ALARB: An Arabic Legal Argument Reasoning Benchmark](https://arxiv.org/abs/2510.00694)
*Harethah Abu Shairah,Somayah AlHarbi,Abdulaziz AlHussein,Sameer Alsabea,Omar Shaqaqi,Hebah AlShamlan,Omar Knio,George Turkiyyah*

Main category: cs.CL

TL;DR: ALARB是一个专为阿拉伯法律领域设计的大型语言模型推理能力评估数据集，涵盖13K多起商业法庭案例，支持多步法律推理任务。


<details>
  <summary>Details</summary>
Motivation: 现有阿拉伯语基准主要侧重知识密集型任务，缺乏专门针对多步推理和开放式语境的阿拉伯法律领域数据集和任务。

Method: 收集了沙特阿拉伯超过13K商业法庭案例，包含案情、法院推理、判决及法规条款，设计判决预测、多步推理链补全和相关法规识别等复杂法律推理任务。并用多种阿拉伯语大模型进行基准测试，同时对一个12亿参数模型进行了基于ALARB的数据指令微调。

Result: 基准测试表明ALARB数据集能有效评估模型的法律推理能力，指令微调12B模型显著提升判决预测和判决生成表现，达到了接近GPT-4o的水平。

Conclusion: ALARB数据集为阿拉伯法律大语言模型的推理能力测评和提升提供了重要资源，指令微调能使中等规模模型取得媲美更大模型的性能。

Abstract: We introduce ALARB, a dataset and suite of tasks designed to evaluate the
reasoning capabilities of large language models (LLMs) within the Arabic legal
domain. While existing Arabic benchmarks cover some knowledge-intensive tasks
such as retrieval and understanding, substantial datasets focusing specifically
on multistep reasoning for Arabic LLMs, especially in open-ended contexts, are
lacking. The dataset comprises over 13K commercial court cases from Saudi
Arabia, with each case including the facts presented, the reasoning of the
court, the verdict, as well as the cited clauses extracted from the regulatory
documents. We define a set of challenging tasks leveraging this dataset and
reflecting the complexity of real-world legal reasoning, including verdict
prediction, completion of reasoning chains in multistep legal arguments, and
identification of relevant regulations based on case facts. We benchmark a
representative selection of current open and closed Arabic LLMs on these tasks
and demonstrate the dataset's utility for instruction tuning. Notably, we show
that instruction-tuning a modest 12B parameter model using ALARB significantly
enhances its performance in verdict prediction and Arabic verdict generation,
reaching a level comparable to that of GPT-4o.

</details>


### [37] [Family Matters: Language Transfer and Merging for Adapting Small LLMs to Faroese](https://arxiv.org/abs/2510.00810)
*Jenny Kunz,Iben Nyholm Debess,Annika Simonsen*

Main category: cs.CL

TL;DR: 本论文研究了如何将小型高效的大语言模型适配到低资源的法罗语，通过先在相关斯堪的纳维亚语言上继续预训练，再针对法罗语进行微调。


<details>
  <summary>Details</summary>
Motivation: 针对法罗语资源稀缺的问题，探索通过相关语言迁移和参数高效微调技术提升模型性能。

Method: 从英语模型出发，继续在斯堪的纳维亚语言（冰岛语、丹麦语）上预训练，然后针对法罗语进行微调；比较全微调与LoRA两种微调策略；构建两个法罗语评测基准并辅以语言学家人工评价。

Result: 相关语言迁移显著提升模型性能，任务不同最优源语言不同（冰岛语提升语言准确性，丹麦语提升理解能力）；微调方式选择也依赖任务，LoRA提升语言可接受性，全微调提升理解表现并更好保持模型能力。

Conclusion: 针对低资源语言的模型适配，选择合适的源语言和微调策略对不同任务表现有重要影响，结合迁移学习和参数高效微调能有效提升模型在法罗语上的性能。

Abstract: We investigate how to adapt small, efficient LLMs to Faroese, a low-resource
North Germanic language. Starting from English models, we continue pre-training
on related Scandinavian languages, either individually or combined via merging,
before fine-tuning on Faroese. We compare full fine-tuning with
parameter-efficient tuning using LoRA, evaluating their impact on both
linguistic accuracy and text comprehension. Due to the lack of existing Faroese
evaluation data, we construct two new minimal-pair benchmarks from adapted and
newly collected datasets and complement them with human evaluations by Faroese
linguists. Our results demonstrate that transfer from related languages is
crucial, though the optimal source language depends on the task: Icelandic
enhances linguistic accuracy, whereas Danish boosts comprehension. Similarly,
the choice between full fine-tuning and LoRA is task-dependent: LoRA improves
linguistic acceptability and slightly increases human evaluation scores on the
base model, while full fine-tuning yields stronger comprehension performance
and better preserves model capabilities during downstream fine-tuning.

</details>


### [38] [Exposing the Cracks: Vulnerabilities of Retrieval-Augmented LLM-based Machine Translation](https://arxiv.org/abs/2510.00829)
*Yanming Sun,Runzhe Zhan,Chi Seng Cheang,Han Wu,Xuebo Liu,Yuyao Niu,Fengying Ye,Kaixin Lan,Lidia S. Chao,Derek F. Wong*

Main category: cs.CL

TL;DR: 本文提出了一个噪声合成框架和新指标，系统评估基于检索增强的大型语言模型机器翻译（REAL-MT）在含噪检索环境下的鲁棒性，发现低资源语言对在噪声干扰下性能显著下降，大型推理模型虽然推理能力增强，但对噪声更敏感且表现出错误的合理化倾向。


<details>
  <summary>Details</summary>
Motivation: 当前检索增强大型语言模型在知识密集型任务中表现优异，但在实际部署中常遇到噪声干扰，其鲁棒性尚不清楚，尤其是低资源语言对在噪声环境下的表现尤为重要。

Method: 提出噪声合成框架和新的评估指标，利用Qwen系列模型（包括标准大型语言模型和增强推理能力模型）在不同资源语言对的成语翻译任务上进行系统评测，同时分析推理模型的注意力偏移和置信度变化。

Result: 低资源语言对依赖检索语境，噪声影响大导致翻译无意义；推理能力增强模型对噪声更敏感，注意力偏离源语成语，置信度提升但准确率下降，表明校准较差。

Conclusion: 当前检索增强大型语言模型的鲁棒性受限，提升噪声环境下性能存在性能与干净环境表现的权衡，未来需探索自我验证的融合机制以改善鲁棒性。

Abstract: \textbf{RE}trieval-\textbf{A}ugmented \textbf{L}LM-based \textbf{M}achine
\textbf{T}ranslation (REAL-MT) shows promise for knowledge-intensive tasks like
idiomatic translation, but its reliability under noisy retrieval contexts
remains poorly understood despite this being a common challenge in real-world
deployment. To address this gap, we propose a noise synthesis framework and new
metrics to evaluate the robustness of REAL-MT systematically. Using this
framework, we instantiate REAL-MT with Qwen-series models, including standard
LLMs and large reasoning models (LRMs) with enhanced reasoning, and evaluate
their performance on idiomatic translation across high-, medium-, and
low-resource language pairs under synthesized noise. Our results show that
low-resource language pairs, which rely more heavily on retrieved context,
degrade more severely under noise than high-resource ones and often produce
nonsensical translations. Although LRMs possess enhanced reasoning
capabilities, they show no improvement in error correction and are even more
susceptible to noise, tending to rationalize incorrect contexts. We find that
this stems from an attention shift away from the source idiom to noisy content,
while confidence increases despite declining accuracy, indicating poor
calibration. To mitigate these issues, we investigate training-free and
fine-tuning strategies, which improve robustness at the cost of performance in
clean contexts, revealing a fundamental trade-off. Our findings highlight the
limitations of current approaches, underscoring the need for self-verifying
integration mechanisms.

</details>


### [39] [ManagerBench: Evaluating the Safety-Pragmatism Trade-off in Autonomous LLMs](https://arxiv.org/abs/2510.00857)
*Adi Simhi,Jonathan Herzig,Martin Tutek,Itay Itzhak,Idan Szpektor,Yonatan Belinkov*

Main category: cs.CL

TL;DR: 提出了ManagerBench基准，用于评估大语言模型在管理场景中应对安全与绩效冲突的决策能力，发现现有模型在权衡安全与实用性时表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有安全评估主要关注避免产生有害内容，忽视了执行操作目标时可能出现的有害行为的决策挑战。

Method: 设计ManagerBench，通过逼真的管理场景在人为验证的情况下考察模型在有害但高效与安全低效间的选择，同时设置无害对照组测量实用主义倾向。

Result: 领先模型在安全与实用权衡中表现不佳，有的倾向选择有害选项以提升绩效，有的则过度避害导致效率低下，且这种行为偏差源于错误的优先级排序，而非识别能力不足。

Conclusion: ManagerBench为评估代理行为中核心的安全决策提供了挑战性平台，凸显了现有模型在处理操作目标与安全价值冲突时的不足。

Abstract: As large language models (LLMs) evolve from conversational assistants into
autonomous agents, evaluating the safety of their actions becomes critical.
Prior safety benchmarks have primarily focused on preventing generation of
harmful content, such as toxic text. However, they overlook the challenge of
agents taking harmful actions when the most effective path to an operational
goal conflicts with human safety. To address this gap, we introduce
ManagerBench, a benchmark that evaluates LLM decision-making in realistic,
human-validated managerial scenarios. Each scenario forces a choice between a
pragmatic but harmful action that achieves an operational goal, and a safe
action that leads to worse operational performance. A parallel control set,
where potential harm is directed only at inanimate objects, measures a model's
pragmatism and identifies its tendency to be overly safe. Our findings indicate
that the frontier LLMs perform poorly when navigating this safety-pragmatism
trade-off. Many consistently choose harmful options to advance their
operational goals, while others avoid harm only to become overly safe and
ineffective. Critically, we find this misalignment does not stem from an
inability to perceive harm, as models' harm assessments align with human
judgments, but from flawed prioritization. ManagerBench is a challenging
benchmark for a core component of agentic behavior: making safe choices when
operational goals and alignment values incentivize conflicting actions.
Benchmark & code available at https://github.com/technion-cs-nlp/ManagerBench.

</details>


### [40] [Erase to Improve: Erasable Reinforcement Learning for Search-Augmented LLMs](https://arxiv.org/abs/2510.00861)
*Ziliang Wang,Kang An,Xuhui Zheng,Faqiang Qian,Weikun Zhang,Cijun Ouyang,Jialu Cai,Yuhang Wang,Yichao Wu*

Main category: cs.CL

TL;DR: 该论文提出了易擦除强化学习（ERL）框架，用于提升大语言模型在复杂多跳推理中的可靠性，通过识别并擦除错误推理步骤，显著提高了模型在多个多跳问答数据集上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前搜索增强型大语言模型在复杂多跳推理任务中存在分解错误、检索缺失和推理错误，单一错误会导致最终答案失败，影响了模型的可靠性。

Method: 提出易擦除强化学习（ERL）框架，通过显式识别推理中的错误步骤，擦除并重新生成推理内容，防止错误逻辑传播，提升推理的鲁棒性。

Result: 训练后模型（ESearch）在HotpotQA、MuSiQue、2Wiki和Bamboogle数据集上取得显著提升，3B模型EM提升8.48%，F1提升11.56%；7B模型EM提升5.38%，F1提升7.22%。

Conclusion: ERL提供了一种强有力的范式转变，使得大语言模型在多步骤推理任务中表现更为稳健和可靠。

Abstract: While search-augmented large language models (LLMs) exhibit impressive
capabilities, their reliability in complex multi-hop reasoning remains limited.
This limitation arises from three fundamental challenges: decomposition errors,
where tasks are incorrectly broken down; retrieval missing, where key evidence
fails to be retrieved; and reasoning errors, where flawed logic propagates
through the reasoning chain. A single failure in any of these stages can derail
the final answer. We propose Erasable Reinforcement Learning (ERL), a novel
framework that transforms fragile reasoning into a robust process. ERL
explicitly identifies faulty steps, erases them, and regenerates reasoning in
place, preventing defective logic from propagating through the reasoning chain.
This targeted correction mechanism turns brittle reasoning into a more
resilient process. Models trained with ERL, termed ESearch, achieve substantial
improvements on HotpotQA, MuSiQue, 2Wiki, and Bamboogle, with the 3B model
achieving +8.48% EM and +11.56% F1, and the 7B model achieving +5.38% EM and
+7.22% F1 over previous state-of-the-art(SOTA) results. These findings suggest
that erasable reinforcement learning provides a powerful paradigm shift for
robust multi-step reasoning in LLMs.

</details>


### [41] [HalluGuard: Evidence-Grounded Small Reasoning Models to Mitigate Hallucinations in Retrieval-Augmented Generation](https://arxiv.org/abs/2510.00880)
*Loris Bergeron,Ioana Buhnila,Jérôme François,Radu State*

Main category: cs.CL

TL;DR: 本文介绍了HalluGuard，一种4亿参数的小型推理模型，旨在减少大语言模型在基于检索的生成中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型擅长NLP任务但容易产生幻觉，降低了在实际应用中的可靠性，迫切需要有效的幻觉缓解方法。

Method: HalluGuard通过合成领域无关的数据集、多阶段精炼及基于偏好的优化训练，识别文档-断言对的真实性，并生成有证据支持的解释。

Result: 在LLM-AggreFact基准测试中，HalluGuard以约一半参数达到与更大模型相当甚至优越的准确率，表现优异。

Conclusion: HalluGuard成功地将大模型的推理能力蒸馏至小模型，有效缓解幻觉问题，未来将开源模型和数据集，推动该领域发展。

Abstract: Large Language Models (LLMs) excel in many NLP tasks but remain prone to
hallucinations, limiting trust in real-world applications. We present
HalluGuard, a 4B-parameter Small Reasoning Model (SRM) for mitigating
hallucinations in Retrieval-Augmented Generation (RAG). HalluGuard classifies
document-claim pairs as grounded or hallucinated and produces evidence-grounded
justifications for transparency. Our approach combines (i) a domain-agnostic
synthetic dataset derived from FineWeb and refined through multi-stage curation
and data reformation, (ii) synthetic grounded and hallucinated claims, and
(iii) preference-based fine-tuning with Odds Ratio Preference Optimization to
distill large-model reasoning into a smaller backbone. On the RAGTruth subset
of the LLM-AggreFact benchmark, HalluGuard achieves 84.0% balanced accuracy
(BAcc), rivaling specialized models, MiniCheck (7B; 84.0%) and Granite Guardian
3.3 (8B; 82.2%) while using roughly half their parameters. Over the full
benchmark it reaches 75.7% BAcc, matching larger general-purpose LLMs such as
GPT-4o (75.9%). We will release HalluGuard and datasets under Apache 2.0 upon
acceptance.

</details>


### [42] [Span-level Detection of AI-generated Scientific Text via Contrastive Learning and Structural Calibration](https://arxiv.org/abs/2510.00890)
*Zhen Yin,Shenghua Wang*

Main category: cs.CL

TL;DR: 本文提出Sci-SpanDet框架，针对AI生成的学术文本进行细粒度检测，实现跨领域强鲁棒性和高准确率。


<details>
  <summary>Details</summary>
Motivation: 现有检测方法忽视细粒度定位、校准能力弱，且难以跨学科和多模型泛化。

Method: 结合章节条件的风格建模、多层对比学习及BIO-CRF序列标注与指针边界解码，实现精确片段检测和可靠概率估计。

Result: 在含10万条跨学科多模型生成样本的数据集上，Sci-SpanDet达到F1(AI)80.17，AUROC92.63，Span-F174.36，表现优于现有方法，且在对抗重写和不同章节保持均衡准确率。

Conclusion: Sci-SpanDet有效提升了学术文本中AI生成内容的检测精度与鲁棒性，推动了该领域的研究发展，相关数据和代码将公开。

Abstract: The rapid adoption of large language models (LLMs) in scientific writing
raises serious concerns regarding authorship integrity and the reliability of
scholarly publications. Existing detection approaches mainly rely on
document-level classification or surface-level statistical cues; however, they
neglect fine-grained span localization, exhibit weak calibration, and often
fail to generalize across disciplines and generators. To address these
limitations, we present Sci-SpanDet, a structure-aware framework for detecting
AI-generated scholarly texts. The proposed method combines section-conditioned
stylistic modeling with multi-level contrastive learning to capture nuanced
human-AI differences while mitigating topic dependence, thereby enhancing
cross-domain robustness. In addition, it integrates BIO-CRF sequence labeling
with pointer-based boundary decoding and confidence calibration to enable
precise span-level detection and reliable probability estimates. Extensive
experiments on a newly constructed cross-disciplinary dataset of 100,000
annotated samples generated by multiple LLM families (GPT, Qwen, DeepSeek,
LLaMA) demonstrate that Sci-SpanDet achieves state-of-the-art performance, with
F1(AI) of 80.17, AUROC of 92.63, and Span-F1 of 74.36. Furthermore, it shows
strong resilience under adversarial rewriting and maintains balanced accuracy
across IMRaD sections and diverse disciplines, substantially surpassing
existing baselines. To ensure reproducibility and to foster further research on
AI-generated text detection in scholarly documents, the curated dataset and
source code will be publicly released upon publication.

</details>


### [43] [Benchmarking Foundation Models with Retrieval-Augmented Generation in Olympic-Level Physics Problem Solving](https://arxiv.org/abs/2510.00919)
*Shunfeng Zheng,Yudi Zhang,Meng Fang,Zihan Zhang,Zhitan Wu,Mykola Pechenizkiy,Ling Chen*

Main category: cs.CL

TL;DR: 该论文研究了利用检索增强生成（RAG）技术提升基础模型在奥林匹克级别物理问题推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统基础模型在专家级别物理问题推理方面表现有限，受启发于学生通过复习竞赛题目准备，探索如何利用检索历史问题提升推理能力。

Method: 构建了一个名为PhoPile的高质量多模态物理问题数据集，包含图表、图示和方程，系统评估不同基础模型结合多样检索器的表现。

Result: 结果表明，将检索技术与物理知识库结合显著提升模型在奥林匹克物理问题上的推理表现。

Conclusion: 检索增强方法能够有效提高多模态基础模型的物理推理能力，但仍存在挑战，促使进一步研究检索驱动的物理推理技术。

Abstract: Retrieval-augmented generation (RAG) with foundation models has achieved
strong performance across diverse tasks, but their capacity for expert-level
reasoning-such as solving Olympiad-level physics problems-remains largely
unexplored. Inspired by the way students prepare for competitions by reviewing
past problems, we investigate the potential of RAG to enhance physics reasoning
in foundation models. We introduce PhoPile, a high-quality multimodal dataset
specifically designed for Olympiad-level physics, enabling systematic study of
retrieval-based reasoning. PhoPile includes diagrams, graphs, and equations,
capturing the inherently multimodal nature of physics problem solving. Using
PhoPile, we benchmark RAG-augmented foundation models, covering both large
language models (LLMs) and large multimodal models (LMMs) with multiple
retrievers. Our results demonstrate that integrating retrieval with physics
corpora can improve model performance, while also highlighting challenges that
motivate further research in retrieval-augmented physics reasoning.

</details>


### [44] [Making, not Taking, the Best of N](https://arxiv.org/abs/2510.00931)
*Ammar Khairi,Daniel D'souza,Marzieh Fadaee,Julia Kreutzer*

Main category: cs.CL

TL;DR: 提出了一种名为Fusion-of-N（FusioN）的方法，通过融合多个生成样本的信息来提高大型语言模型的生成质量，相较传统的从多个样本中选择最佳的Best-of-N (BoN)方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统Best-of-N方法仅选择单个最佳生成结果，忽略了其他样本中潜在的有用信息，这导致信息浪费和性能瓶颈。

Method: 利用一个通用的LLM评判模型，将来自多个生成样本中最有信息量的部分融合成一个最终答案，以发挥所有样本的优势。

Result: 在测试时的模型扩展和合成数据生成两种场景下，FusioN在11种语言、3个任务和不同模型规模中均优于BoN，表现出更强的多样性整合能力和鲁棒性。

Conclusion: 研究表明，应改变评估和利用LLM生成结果的方式，从单一质量衡量转向融合多样性，实现信息整合和潜力释放，突破传统选择方法的局限。

Abstract: Obtaining high-quality generations in modern LLMs has largely been framed as
a selection problem: identifying a single winning generation from a diverse
pool of N samples, the Best-of-N (BoN). Yet, this approach is inherently
zero-sum, discarding diverse and potentially useful information from the pool.
Instead, we explore a collaborative setup, where all candidates can potentially
contribute to the final winning generation. To this end, we propose Fusion-of-N
(FusioN): a method that uses a general LLM judge to synthesize the most
informative elements of each sample into a single final answer. We compare
FusioN to BoN in two settings, (i) test-time scaling, where we sample and
aggregate from a single model at test-time (ii) synthetic data generation,
where we fuse samples from a pool of diverse teachers to improve a student
model. We extensively benchmark both setups across 11 languages, 3 diverse
tasks and varying model scales. Across the bench, FusioN consistently
outperforms BoN showing versatility and robustness both in test-time scaling
and in downstream gains from synthetic data generation. We also perform
extensive analysis on FusioN, where it shows surprising strengths and
robustness under challenging settings. These results show that we should shift
how we think about evaluating and utilizing LLM generations from a monolithic
measure of quality, to embracing their polylithic nature. This shift allows us
to integrate diverse strengths, unlock latent potential, and achieve
improvements that were previously inaccessible through selection alone.

</details>


### [45] [Analyzing Dialectical Biases in LLMs for Knowledge and Reasoning Benchmarks](https://arxiv.org/abs/2510.00962)
*Eileen Pan,Anna Seo Gyeong Choi,Maartje ter Hoeve,Skyler Seto,Allison Koenecke*

Main category: cs.CL

TL;DR: 研究发现大型语言模型在处理非标准英语方言时性能下降，尤其是涉及特定语法规则时。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型在非标准美国英语方言题目上的表现及其语法原因，因之前研究显示方言造成性能下降。

Method: 将标准美式英语的问题转换为非标准方言变体，评估多选题的准确率，分析特定语法规则对性能的影响。

Result: 准确率最高下降20%；存在三条关键语法规则（存在性"it"，零系动词，y'all用法）解释了大部分性能下降。

Conclusion: 建议未来工作针对个别高影响语法结构研究偏见缓解方法。

Abstract: Large language models (LLMs) are ubiquitous in modern day natural language
processing. However, previous work has shown degraded LLM performance for
under-represented English dialects. We analyze the effects of typifying
"standard" American English language questions as non-"standard" dialectal
variants on multiple choice question answering tasks and find up to a 20%
reduction in accuracy. Additionally, we investigate the grammatical basis of
under-performance in non-"standard" English questions. We find that individual
grammatical rules have varied effects on performance, but some are more
consequential than others: three specific grammar rules (existential "it", zero
copula, and y'all) can explain the majority of performance degradation observed
in multiple dialects. We call for future work to investigate bias mitigation
methods focused on individual, high-impact grammatical structures.

</details>


### [46] [Syntax-Guided Diffusion Language Models with User-Integrated Personalization](https://arxiv.org/abs/2510.01028)
*Ruqian Zhang,Yijiao Zhang,Juan Shen,Zhongyi Zhu,Annie Qu*

Main category: cs.CL

TL;DR: 本文提出了一种语法引导的扩散语言模型，以提升文本生成的多样性和个性化表达。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型生成文本质量高但缺乏结构多样性，限制了个性化表达。扩散模型为突破自回归模型局限提供了新机遇。

Method: 设计了级联和非级联两种框架，先生成语法指导再进行条件文本生成，结合语法信息捕获句子构建的词汇和结构特征；提出共享表示机制支持跨用户信息整合，实现风格化生成和零样本推断。

Result: 在多项任务实验中，模型在流利度、多样性和风格忠实度方面表现优越。

Conclusion: 该模型通过语法引导和个性化条件化，提升了文本质量和个性化生成能力，展现了良好的解释性和灵活性。

Abstract: Large language models have made revolutionary progress in generating
human-like text, yet their outputs often tend to be generic, exhibiting
insufficient structural diversity, which limits personalized expression. Recent
advances in diffusion models have opened new opportunities for improving
language generation beyond the limitations of autoregressive paradigms. In this
work, we propose a syntax-guided diffusion language model that integrates
structural supervision and personalized conditioning to enhance text quality,
diversity, and controllability. We introduce a cascaded framework that
generates syntactic guidance before conditional text generation, and further
generalize it to a novel noncascaded architecture for better alignment between
structure and content. By incorporating syntactic information in the generating
process, the proposed model better captures the lexical and structural
characteristics of stylistic sentence construction. To enable fine-grained
personalization, we develop a shared representation mechanism that facilitates
information integration across users, supporting both faithful stylistic
generation and generalizable zero-shot inference. Extensive experiments on
multiple tasks demonstrate the superiority of our approach in fluency,
diversity, and stylistic fidelity. Further qualitative analyses highlight its
interpretability and flexibility in learning personalized patterns.

</details>


### [47] [Interpreting Language Models Through Concept Descriptions: A Survey](https://arxiv.org/abs/2510.01048)
*Nils Feldhus,Laura Kopf*

Main category: cs.CL

TL;DR: 本文综述了利用自然语言概念描述解释大语言模型内部组件的最新研究进展，强调了因果评估的重要性。


<details>
  <summary>Details</summary>
Motivation: 理解神经网络特别是大语言模型的决策机制，以提升模型透明度和可解释性。

Method: 分析当前生成模型组件自然语言概念描述的方法，评估指标和相关数据集，系统总结相关研究。

Result: 揭示了该领域对更加严谨和因果评估方法的需求，梳理了关键方法和存在的挑战。

Conclusion: 该综述为未来在模型组分解释和透明性研究提供了研究路线图。

Abstract: Understanding the decision-making processes of neural networks is a central
goal of mechanistic interpretability. In the context of Large Language Models
(LLMs), this involves uncovering the underlying mechanisms and identifying the
roles of individual model components such as neurons and attention heads, as
well as model abstractions such as the learned sparse features extracted by
Sparse Autoencoders (SAEs). A rapidly growing line of work tackles this
challenge by using powerful generator models to produce open-vocabulary,
natural language concept descriptions for these components. In this paper, we
provide the first survey of the emerging field of concept descriptions for
model components and abstractions. We chart the key methods for generating
these descriptions, the evolving landscape of automated and human metrics for
evaluating them, and the datasets that underpin this research. Our synthesis
reveals a growing demand for more rigorous, causal evaluation. By outlining the
state of the art and identifying key challenges, this survey provides a roadmap
for future research toward making models more transparent.

</details>


### [48] [Hybrid Dialogue State Tracking for Persian Chatbots: A Language Model-Based Approach](https://arxiv.org/abs/2510.01052)
*Samin Mahdipour Aghabagher,Saeedeh Momtazi*

Main category: cs.CL

TL;DR: 该论文提出了一种结合规则和语言模型的混合对话状态追踪（DST）模型，针对波斯语多轮对话数据集进行了评估，显著提升了准确性和连贯性。


<details>
  <summary>Details</summary>
Motivation: 传统的基于规则的DST模型在开放域和多轮对话中缺乏适应性和连贯性，无法满足复杂对话中人类般体验的需求。

Method: 构建了一个混合DST模型，融合了基于规则的方法，BERT用于槽位填充和意图识别，XGBoost进行意图验证，GPT完成DST任务，结合在线代理实现实时回答生成。

Result: 在波斯语多轮对话数据集上的测试显示，该混合模型在准确性和连贯性方面显著优于现有方法。

Conclusion: 混合方法有效提升DST能力，为实现更加个性化、适应性强且具有类人体验的对话系统奠定了基础。

Abstract: Dialogue State Tracking (DST) is an essential element of conversational AI
with the objective of deeply understanding the conversation context and leading
it toward answering user requests. Due to high demands for open-domain and
multi-turn chatbots, the traditional rule-based DST is not efficient enough,
since it cannot provide the required adaptability and coherence for human-like
experiences in complex conversations. This study proposes a hybrid DST model
that utilizes rule-based methods along with language models, including BERT for
slot filling and intent detection, XGBoost for intent validation, GPT for DST,
and online agents for real-time answer generation. This model is uniquely
designed to be evaluated on a comprehensive Persian multi-turn dialogue dataset
and demonstrated significantly improved accuracy and coherence over existing
methods in Persian-based chatbots. The results demonstrate how effectively a
hybrid approach may improve DST capabilities, paving the way for conversational
AI systems that are more customized, adaptable, and human-like.

</details>


### [49] [Research on the Integration of Embodied Intelligence and Reinforcement Learning in Textual Domains](https://arxiv.org/abs/2510.01076)
*Haonan Wang,Junfeng Sun,Mingjia Zhao,Wei Liu*

Main category: cs.CL

TL;DR: 本文提出了一种将具身智能与强化学习结合的新模型，提升文本处理的智能水平，在多种文本处理任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 利用具身智能在感知和行动上的优势及强化学习在决策优化上的能力，提升文本处理的智能化水平。

Method: 详细理论阐述和实验探索，提出一种具身智能与强化学习相结合的文本处理集成模型。

Result: 该模型在多种文本处理任务中表现出较高的有效性，验证了其实用潜力。

Conclusion: 结合具身智能与强化学习的方法有效提升了文本处理的智能化，具有广泛的应用前景。

Abstract: This article addresses embodied intelligence and reinforcement learning
integration in the field of text processing, aiming to enhance text handling
with more intelligence on the basis of embodied intelligence's perception and
action superiority and reinforcement learning's decision optimization
capability. Through detailed theoretical explanation and experimental
exploration, a novel integration model is introduced. This model has been
demonstrated to be very effective in a wide range oftext processing tasks,
validating its applicative potential

</details>


### [50] [Automatic Speech Recognition (ASR) for African Low-Resource Languages: A Systematic Literature Review](https://arxiv.org/abs/2510.01145)
*Sukairaj Hafiz Imam,Tadesse Destaw Belay,Kedir Yassin Husse,Ibrahim Said Ahmad,Idris Abdulmumin,Hadiza Ali Umar,Muhammad Yahuza Bello,Joyce Nakatumba-Nabende,Seid Muhie Yimam,Shamsuddeen Hassan Muhammad*

Main category: cs.CL

TL;DR: 综述了非洲低资源语言的自动语音识别（ASR）研究现状，包括数据集、模型、评估方法及挑战，指出数据集匮乏、资源有限和评估指标不足等问题，并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 非洲拥有2000多种语言，但ASR技术针对这些低资源语言的研究严重不足，阻碍了非洲大陆的数字包容性发展。

Method: 采用PRISMA 2020系统文献回顾方法，从多个数据库筛选2020年至2025年间发布的相关研究，最终选取71篇文献，覆盖111种语言和74个数据集。

Result: 发现语音数据约11206小时，研究中只有少数提供可复现材料，数据许可不明确，自监督与迁移学习虽有潜力但受制于数据不足和方言覆盖不全，评估多用词错误率，缺乏适合声调和形态丰富语言的指标，整体证据不一致且存在数据注释和许可等问题。

Conclusion: 尽管存在数据和评估限制，社区驱动和方法学进展为非洲语言ASR提供改进路径，未来需加强利益相关方合作、建立伦理平衡数据集、采用轻量化模型及开展积极的基准测试。

Abstract: ASR has achieved remarkable global progress, yet African low-resource
languages remain rigorously underrepresented, producing barriers to digital
inclusion across the continent with more than +2000 languages. This systematic
literature review (SLR) explores research on ASR for African languages with a
focus on datasets, models and training methods, evaluation techniques,
challenges, and recommends future directions. We employ the PRISMA 2020
procedures and search DBLP, ACM Digital Library, Google Scholar, Semantic
Scholar, and arXiv for studies published between January 2020 and July 2025. We
include studies related to ASR datasets, models or metrics for African
languages, while excluding non-African, duplicates, and low-quality studies
(score <3/5). We screen 71 out of 2,062 records and we record a total of 74
datasets across 111 languages, encompassing approximately 11,206 hours of
speech. Fewer than 15% of research provided reproducible materials, and dataset
licensing is not clear. Self-supervised and transfer learning techniques are
promising, but are hindered by limited pre-training data, inadequate coverage
of dialects, and the availability of resources. Most of the researchers use
Word Error Rate (WER), with very minimal use of linguistically informed scores
such as Character Error Rate (CER) or Diacritic Error Rate (DER), and thus with
limited application in tonal and morphologically rich languages. The existing
evidence on ASR systems is inconsistent, hindered by issues like dataset
availability, poor annotations, licensing uncertainties, and limited
benchmarking. Nevertheless, the rise of community-driven initiatives and
methodological advancements indicates a pathway for improvement. Sustainable
development for this area will also include stakeholder partnership, creation
of ethically well-balanced datasets, use of lightweight modelling techniques,
and active benchmarking.

</details>


### [51] [mR3: Multilingual Rubric-Agnostic Reward Reasoning Models](https://arxiv.org/abs/2510.01146)
*David Anugraha,Shou-Yi Hung,Zilu Tang,Annie En-Shiun Lee,Derry Tanti Wijaya,Genta Indra Winata*

Main category: cs.CL

TL;DR: 本文提出了mR3，一种覆盖72种语言的多语言奖励推理模型，用于多语言自动评估，效果优于更大模型且模型更小。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型评判器主要适用于英语，难以推广到多语言环境，亟需有效的多语言训练策略。

Method: 构建了覆盖72种语言的mR3模型，进行了数据和课程选择的综合研究，整合了目标语言推理数据集，实现了多语言奖励模型的训练。

Result: mR3在多语言奖励模型基准测试中表现出色，超过了更大的GPT-OSS-120B模型，且模型体积更小（最多小9倍），通过消融实验进一步验证其有效性。

Conclusion: 采用多语言、多数据源和合理的训练策略，可以构建高质量且高效的多语言奖励模型，推动非英语自动评估的发展。

Abstract: Evaluation using Large Language Model (LLM) judges has been widely adopted in
English and shown to be effective for automatic evaluation. However, their
performance does not generalize well to non-English settings, and it remains
unclear what constitutes effective multilingual training for such judges. In
this paper, we introduce mR3, a massively multilingual, rubric-agnostic reward
reasoning model trained on 72 languages, achieving the broadest language
coverage in reward modeling to date. We present a comprehensive study of data
and curriculum selection for training to identify effective strategies and data
sources for building high-quality reward models, including the integration of
target-language reasoning datasets. Our approach attains state-of-the-art
performance on multilingual reward model benchmarks, surpassing much larger
models (i.e., GPT-OSS-120B) while being up to 9x smaller, and its effectiveness
is further confirmed through extensive ablation studies. Our models, data, and
code are available as open source at https://github.com/rubricreward/mr3.

</details>


### [52] [Pay-Per-Search Models are Abstention Models](https://arxiv.org/abs/2510.01152)
*Mustafa Omer Gul,Claire Cardie,Tanya Goyal*

Main category: cs.CL

TL;DR: 本文提出了MASH训练框架，通过对外部搜索帮助的惩罚和答案准确率的奖励，让大语言模型能够更好地判断其知识边界，实现自主放弃回答的问题能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型无法准确识别其参数化知识的边界，容易对超出边界的问题产生错误回答，而人类则会意识到自身限制并寻求外部帮助或选择放弃。

Method: 提出MASH框架，将模型使用搜索工具的行为作为放弃回答的代理，通过强化学习和按搜索付费的奖励机制训练模型，使其能选择性地寻求帮助或放弃回答。

Result: 在三个知识密集型问答数据集上的实验结果显示，MASH显著优于之前的高效搜索方法，特别是在多跳问答中提升了7.6%的准确率，并能区分可答和不可答问题，展现出类似专门放弃策略的能力。

Conclusion: MASH无需预先确定知识边界即可训练，其放弃回答的行为是辅助选择性求助任务训练的副产品，实现了搜索工具使用与模型知识的有效对齐，成功用于放弃决策。

Abstract: LLMs cannot reliably recognize their parametric knowledge boundaries and
often hallucinate answers to outside-of-boundary questions. In contrast, humans
recognize their limitations and can either seek external help for such
questions or abstain. In this paper, we introduce MASH (Modeling Abstention via
Selective Help-seeking), a training framework that readily extracts abstentions
from LLMs. Our key idea is that any external help-seeking by an LLM, i.e.
search tool use, can serve as a proxy for abstention if the external help
(search) is appropriately penalized while simultaneously rewarding answer
accuracy. MASH operationalizes this idea using reinforcement learning with a
pay-per-search reward.
  We run experiments on three knowledge-intensive QA datasets. Our results show
that MASH substantially improves upon the selective help-seeking performance of
prior efficient search approaches; on multi-hop datasets, MASH improves answer
accuracy by 7.6%. Furthermore, MASH demonstrates strong off-the-shelf
abstention -- it can distinguish between unanswerable/answerable questions and
selectively generate responses for answerable questions -- showcasing behavior
analogous to specialized abstention approaches. We emphasize that contrary to
prior abstention methods, MASH does not require pre-determining knowledge
boundaries to construct training data. Instead, MASH's abstentions are a
by-product of training for the auxiliary selective help-seeking task. Overall,
we show that MASH training effectively aligns search tool use with parametric
knowledge, which can be successfully leveraged for making abstention decisions.

</details>


### [53] [Backdoor Attacks Against Speech Language Models](https://arxiv.org/abs/2510.01157)
*Alexandrine Fortier,Thomas Thebaud,Jesús Villalba,Najim Dehak,Patrick Cardinal*

Main category: cs.CL

TL;DR: 本文首次系统性研究了针对语音语言模型的音频后门攻击，攻击成功率高达90.76%至99.41%，并提出基于微调的防御方法。


<details>
  <summary>Details</summary>
Motivation: 目前多模态大语言模型通过级联领域特定编码器实现多模态能力，但导致模型继承各组件的脆弱性，亟需研究其安全隐患。

Method: 在四个语音编码器和三个数据集上，针对自动语音识别、语音情感识别及性别和年龄预测等任务，进行音频后门攻击测试，并通过组件分析识别最脆弱阶段，最后设计微调防御策略。

Result: 攻击在四个任务上均取得90.76%至99.41%的高成功率，组件分析揭示后门传播机制，微调防御有效缓解预训练编码器中毒威胁。

Conclusion: 音频后门攻击对语音语言模型构成严重威胁，通过分析攻击传播路径并采用微调防御，可有效提升模型安全性。

Abstract: Large Language Models (LLMs) and their multimodal extensions are becoming
increasingly popular. One common approach to enable multimodality is to cascade
domain-specific encoders with an LLM, making the resulting model inherit
vulnerabilities from all of its components. In this work, we present the first
systematic study of audio backdoor attacks against speech language models. We
demonstrate its effectiveness across four speech encoders and three datasets,
covering four tasks: automatic speech recognition (ASR), speech emotion
recognition, and gender and age prediction. The attack consistently achieves
high success rates, ranging from 90.76% to 99.41%. To better understand how
backdoors propagate, we conduct a component-wise analysis to identify the most
vulnerable stages of the pipeline. Finally, we propose a fine-tuning-based
defense that mitigates the threat of poisoned pretrained encoders.

</details>


### [54] [Social Welfare Function Leaderboard: When LLM Agents Allocate Social Welfare](https://arxiv.org/abs/2510.01164)
*Zhengliang Shi,Ruotian Ma,Jen-tse Huang,Xinbei Ma,Xingyu Chen,Mengru Wang,Qu Yang,Yue Wang,Fanghua Ye,Ziyang Chen,Shanyi Wang,Cixing Li,Wenxuan Wang,Zhaopeng Tu,Xiaolong Li,Zhaochun Ren,Linus*

Main category: cs.CL

TL;DR: 该论文提出了一个用于评估大语言模型（LLM）在社会资源分配中表现的基准测试——社会福利函数基准。


<details>
  <summary>Details</summary>
Motivation: 随着LLM被赋予影响人类福祉的高风险决策任务，研究其分配稀缺社会资源时遵循的原则和价值观变得十分重要，但目前尚未深入探究。

Method: 设计了一个动态模拟环境，LLM作为主权分配者，将任务分配给异质社区成员，并在最大化集体效率和保障分配公平之间制造权衡。评估了20个最先进的LLM并创建了相应榜单。

Result: 发现：（i）模型的对话能力不能有效预测其分配能力；（ii）大多数模型倾向于功利主义，优先考虑群体产出但造成严重不平等；（iii）分配策略易受输出长度限制和社会影响框架的干扰。

Conclusion: 当前LLM作为社会决策者存在风险，需开发专门基准和针对性调整以实现AI治理的对齐目标。

Abstract: Large language models (LLMs) are increasingly entrusted with high-stakes
decisions that affect human welfare. However, the principles and values that
guide these models when distributing scarce societal resources remain largely
unexamined. To address this, we introduce the Social Welfare Function (SWF)
Benchmark, a dynamic simulation environment where an LLM acts as a sovereign
allocator, distributing tasks to a heterogeneous community of recipients. The
benchmark is designed to create a persistent trade-off between maximizing
collective efficiency (measured by Return on Investment) and ensuring
distributive fairness (measured by the Gini coefficient). We evaluate 20
state-of-the-art LLMs and present the first leaderboard for social welfare
allocation. Our findings reveal three key insights: (i) A model's general
conversational ability, as measured by popular leaderboards, is a poor
predictor of its allocation skill. (ii) Most LLMs exhibit a strong default
utilitarian orientation, prioritizing group productivity at the expense of
severe inequality. (iii) Allocation strategies are highly vulnerable, easily
perturbed by output-length constraints and social-influence framing. These
results highlight the risks of deploying current LLMs as societal
decision-makers and underscore the need for specialized benchmarks and targeted
alignment for AI governance.

</details>


### [55] [GRAD: Generative Retrieval-Aligned Demonstration Sampler for Efficient Few-Shot Reasoning](https://arxiv.org/abs/2510.01165)
*Oussama Gabouj,Kamel Charaf,Ivan Zakazov,Nicolas Baldwin,Robert West*

Main category: cs.CL

TL;DR: 本文提出了一种动态演示生成方法GRAD，通过训练语言模型生成针对输入的定制化简洁演示，优化了检索增强生成效果，在有限预算下表现优越，且具备良好的跨领域泛化能力，实现了高效低成本的动态少样本学习。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成方法依赖静态数据库，导致适应性差和演示内容可能无关，影响效果。

Method: 设计了GRAD模型，训练语言模型根据输入生成特定的简洁演示，替代传统静态检索结果，提高上下文相关性和效率。

Result: 在数学推理和STEM领域问题上，GRAD超越了强基线模型，并展示了在物理、化学和计算机科学等跨领域的强泛化能力，同时通过较小模型生成演示辅导大模型，降低训练成本。

Conclusion: GRAD为动态少样本学习提供了一种可扩展的生成示范方案，实现了资源受限环境下高效准确的模型表现，推动了少样本学习领域的发展。

Abstract: Large Language Models (LLMs) achieve strong performance across diverse tasks,
but their effectiveness often depends on the quality of the provided context.
Retrieval-Augmented Generation (RAG) enriches prompts with external
information, but its reliance on static databases constrains adaptability and
can result in irrelevant demonstrations. In this work, we propose a Generative
Retrieval-Aligned Demonstrator (GRAD), a dynamic demonstration-based approach
where an LLM model is trained to generate input-specific concise
demonstrations. By tailoring demonstrations to each input, our method offers
better contextual support than traditional RAG approaches. We demonstrate the
superiority of GRAD under budget constraints, where we limit both the number of
tokens used per demonstration and the number of tokens used for the final
output. Trained solely on a math dataset, GRAD consistently outperforms strong
baselines on Qwen2.5-14B across mathematical reasoning and advanced STEM
questions, highlighting GRAD's robust generalization to out-of-distribution
(OOD) domains such as physics, chemistry, and computer science. Furthermore, we
show that demonstrations generated by trained smaller models can effectively
guide larger target models, reducing training costs while maintaining
competitive accuracy. Overall, this work introduces a scalable demonstration
generator model presenting the first step toward a dynamic few-shot learning
paradigm in resource-constrained settings. We release the code used for the
project.

</details>


### [56] [Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM Diversity](https://arxiv.org/abs/2510.01171)
*Jiayi Zhang,Simon Yu,Derek Chong,Anthony Sicilia,Michael R. Tomz,Christopher D. Manning,Weiyan Shi*

Main category: cs.CL

TL;DR: 本论文指出大语言模型（LLM）后训练对齐过程中多样性降低的模式崩溃问题，归因于数据偏差，并提出一种无训练的推理时策略来解决。


<details>
  <summary>Details</summary>
Motivation: 现有研究多将模式崩溃归咎于算法缺陷，但本文发现认知心理学中的典型性偏差导致的偏好数据是根本原因。

Method: 提出Verbalized Sampling策略，通过模型口头表达多个生成结果及其概率，规避典型性偏差，提升多样性。

Result: 实验证明，VS在创意写作、对话模拟、开放式问答和数据生成等任务中显著提升多样性，且不损害准确性和安全性。

Conclusion: 本文从数据视角揭示模式崩溃本质，提出有效的推理时防止模式崩溃技术，助力释放预训练生成模型的多样性潜力。

Abstract: Post-training alignment often reduces LLM diversity, leading to a phenomenon
known as mode collapse. Unlike prior work that attributes this effect to
algorithmic limitations, we identify a fundamental, pervasive data-level
driver: typicality bias in preference data, whereby annotators systematically
favor familiar text as a result of well-established findings in cognitive
psychology. We formalize this bias theoretically, verify it on preference
datasets empirically, and show that it plays a central role in mode collapse.
Motivated by this analysis, we introduce Verbalized Sampling, a simple,
training-free prompting strategy to circumvent mode collapse. VS prompts the
model to verbalize a probability distribution over a set of responses (e.g.,
``Generate 5 jokes about coffee and their corresponding probabilities'').
Comprehensive experiments show that VS significantly improves performance
across creative writing (poems, stories, jokes), dialogue simulation,
open-ended QA, and synthetic data generation, without sacrificing factual
accuracy and safety. For instance, in creative writing, VS increases diversity
by 1.6-2.1x over direct prompting. We further observe an emergent trend that
more capable models benefit more from VS. In sum, our work provides a new
data-centric perspective on mode collapse and a practical inference-time remedy
that helps unlock pre-trained generative diversity.

</details>


### [57] [Energy-Regularized Sequential Model Editing on Hyperspheres](https://arxiv.org/abs/2510.01172)
*Qingyuan Liu,Jia-Chen Gu,Yunzhi Yao,Hong Wang,Nanyun Peng*

Main category: cs.CL

TL;DR: 本论文研究了大语言模型顺序编辑过程中性能下降的问题，提出了基于超球面能量稳定性（HE）的方法，提高编辑稳定性和知识保留。


<details>
  <summary>Details</summary>
Motivation: 大语言模型需要不断更新以适应实时知识，但顺序编辑会导致模型表现不稳定和灾难性遗忘，亟需找到稳定顺序编辑的方法。

Method: 通过引入超球面能量（HE）作为神经元权重分布的量化指标，发现其与编辑性能高度相关，并提出SPHERE方法利用HE正则化稳定权重分布，减少主方向扰动，实现可靠的顺序知识编辑。

Result: 在LLaMA3和Qwen2.5两种大模型上，SPHERE方法在编辑能力上平均提升16.41%，且更好地保持了模型原有性能。

Conclusion: 稳定神经元权重的超球面均匀分布对顺序编辑中的知识保留至关重要，SPHERE方法为大规模知识编辑提供了有效且有原则的解决方案。

Abstract: Large language models (LLMs) require constant updates to remain aligned with
evolving real-world knowledge. Model editing offers a lightweight alternative
to retraining, but sequential editing often destabilizes representations and
induces catastrophic forgetting. In this work, we seek to better understand and
mitigate performance degradation caused by sequential editing. We hypothesize
that hyperspherical uniformity, a property that maintains uniform distribution
of neuron weights on a hypersphere, helps the model remain stable, retain prior
knowledge, while still accommodate new updates. We use Hyperspherical Energy
(HE) to quantify neuron uniformity during editing, and examine its correlation
with editing performance. Empirical studies across widely used editing methods
reveals a strong correlation between HE dynamics and editing performance, with
editing failures consistently coinciding with high HE fluctuations. We further
theoretically prove that HE dynamics impose a lower bound on the degradation of
pretrained knowledge, highlighting why HE stability is crucial for knowledge
retention. Motivated by these insights, we propose SPHERE (Sparse Projection
for Hyperspherical Energy-Regularized Editing), an HE-driven regularization
strategy that stabilizes neuron weight distributions, ultimately preserving
prior knowledge while enabling reliable sequential updates. Specifically,
SPHERE identifies a sparse space complementary to the principal hyperspherical
directions of the pretrained weight matrices and projects new knowledge onto
it, attenuating perturbations on the principal directions. Extensive
experiments on LLaMA3 (8B) and Qwen2.5 (7B) show that SPHERE outperforms the
best baseline in editing capability by an average of 16.41%, while most
faithfully preserving general model performance, thereby offering a principled
path toward reliable large-scale knowledge editing.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [58] [PBFD and PDFD: Formally Defined and Verified Methodologies and Empirical Evaluation for Scalable Full-Stack Software Engineering](https://arxiv.org/abs/2510.00002)
*Dong Liu*

Main category: cs.SE

TL;DR: 本文提出了两种正式定义并验证的方法——主广度优先开发（PBFD）和主深度优先开发（PDFD），以实现可扩展的工业级全栈软件工程。


<details>
  <summary>Details</summary>
Motivation: 解决形式方法与现实开发实践之间的长期鸿沟，通过图论建模确保结构正确性。

Method: 利用分层有向图，结合统一状态机和通信顺序进程（CSP）形式化PBFD和PDFD，提出三层封装（TLE）编码方案以支持规模化数据协调。

Result: PBFD在八年企业部署中验证，开发速度比Salesforce OmniScript快20倍以上，查询性能比传统关系模型快7-8倍，并且两者均提供开源原型，证明了设计正确性。

Conclusion: PBFD和PDFD为结合形式验证与实用软件开发建立了可复现、透明的框架，推动学术研究和工业应用。

Abstract: This paper introduces Primary Breadth-First Development (PBFD) and Primary
Depth-First Development (PDFD), two formally defined and verified methodologies
for scalable, industrial-grade full-stack software engineering. These
approaches bridge a longstanding gap between formal methods and real-world
development practice by enforcing structural correctness through
graph-theoretic modeling. Unlike prior graph-based approaches, PBFD and PDFD
operate over layered directed graphs and are formalized using unified state
machines and Communicating Sequential Processes (CSP) to ensure critical
properties, including bounded-refinement termination and structural
completeness. To coordinate hierarchical data at scale, we propose Three-Level
Encapsulation (TLE) - a novel, bitmask-based encoding scheme that delivers
provably constant-time updates. TLE's formal guarantees underpin PBFD's
industrial-scale performance and scalability. PBFD was empirically validated
through an eight-year enterprise deployment, demonstrating over 20x faster
development than Salesforce OmniScript and 7-8x faster query performance
compared to conventional relational models. Additionally, both methodologies
are supported by open-source MVPs, with PDFD's implementation conclusively
demonstrating its correctness-first design principles. Together, PBFD and PDFD
establish a reproducible, transparent framework that integrates formal
verification into practical software development. All formal specifications,
MVPs, and datasets are publicly available to foster academic research and
industrial-grade adoption.

</details>


### [59] [Semantic Zoom and Mini-Maps for Software Cities](https://arxiv.org/abs/2510.00003)
*Malte Hansen,Jens Bamberg,Noe Baumann,Wilhelm Hasselbring*

Main category: cs.SE

TL;DR: 本文提出了两种3D软件城市可视化的视觉缩放方法：语义缩放和迷你地图，提升了大规模软件可视化的可理解性。


<details>
  <summary>Details</summary>
Motivation: 随着可视化数据显示量增加，程序理解变得更加困难，解决视觉可扩展性问题迫切需要新的方法。

Method: 通过语义缩放动态改变图形表示以及增加二维迷你地图实现对大规模数据的有效视觉展示。使用3D城市隐喻和开源工具ExplorViz进行实现。

Result: 用户研究表明语义缩放和迷你地图均为有用的功能，尤其对大型软件和协作探索有显著帮助，且易用性良好。

Conclusion: 语义缩放和迷你地图有效提升了3D软件可视化的视觉可扩展性，后续工作将针对实现中的不足进行改进。

Abstract: Software visualization tools can facilitate program comprehension by
providing visual metaphors, or abstractions that reduce the amount of textual
data that needs to be processed mentally. One way they do this is by enabling
developers to build an internal representation of the visualized software and
its architecture. However, as the amount of displayed data in the visualization
increases, the visualization itself can become more difficult to comprehend.
The ability to display small and large amounts of data in visualizations is
called visual scalability.
  In this paper, we present two approaches to address the challenge of visual
scalability in 3D software cities. First, we present an approach to semantic
zoom, in which the graphical representation of the software landscape changes
based on the virtual camera's distance from visual objects. Second, we augment
the visualization with a miniature two-dimensional top-view projection called
mini-map. We demonstrate our approach using an open-source implementation in
our software visualization tool ExplorViz. ExplorViz is web-based and uses the
3D city metaphor, focusing on live trace visualization.
  We evaluated our approaches in two separate user studies. The results
indicate that semantic zoom and the mini-map are both useful additions. User
feedback indicates that semantic zoom and mini-maps are especially useful for
large software landscapes and collaborative software exploration. The studies
indicate a good usability of our implemented approaches. However, some
shortcomings in our implementations have also been discovered, to be addressed
in future work.
  Video URL: https://youtu.be/LYtUeWvizjU

</details>


### [60] [HTML Structure Exploration in 3D Software Cities](https://arxiv.org/abs/2510.00004)
*Malte Hansen,David Moreno-Lumbreras,Wilhelm Hasselbring*

Main category: cs.SE

TL;DR: 本文通过为ExplorViz软件可视化工具添加嵌入式网页视图，实现了3D环境中HTML结构的可视化，提升了用户对动态网页应用的交互和理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有软件可视化工具通常忽略了大型软件系统中的网页界面交互，限制了对用户界面行为的理解。

Method: 在ExplorViz的3D可视化中嵌入网页视图，同时对HTML的DOM结构进行三维可视化，支持同源环境下对网页内容的交互探索。

Result: 初步用户研究显示该方法有助于用户更好地理解网页应用，揭示了适用场景、优势与不足。

Conclusion: 结合软件城市和HTML结构的可视化具有潜力，未来研究将进一步支持网页界面的交互探索和应用。

Abstract: Software visualization, which uses data from dynamic program analysis, can
help to explore and understand the behavior of software systems. It is common
that large software systems offer a web interface for user interaction.
Usually, available web interfaces are not regarded in software visualization
tools. This paper introduces additions to the web-based live tracing software
visualization tool ExplorViz: We add an embedded web view for instrumented
applications in the 3D visualization to ease interaction with the given
applications and enable the exploration of the thereby displayed HTML content.
Namely, the Document Object Model (DOM) is visualized via a three-dimensional
representation of the HTML structure in same-origin contexts.
  Our visualization approach is evaluated in a preliminary user study. The
study results give insights into the potential use cases, benefits, and
shortcomings of our implemented approach. Based on our study results, we
propose directions for further research to support the visual exploration of
web interfaces and explore use cases for the combined visualization of software
cities and HTML structure.
  Video URL: https://youtu.be/wBWKlbvzOOE

</details>


### [61] [VibeCodeHPC: An Agent-Based Iterative Prompting Auto-Tuner for HPC Code Generation Using LLMs](https://arxiv.org/abs/2510.00031)
*Shun-ichiro Hayashi,Koki Morita,Daichi Mukunoki,Tetsuya Hoshino,Takahiro Katagiri*

Main category: cs.SE

TL;DR: VibeCodeHPC是一个基于多智能体大型语言模型（LLM）的高性能计算（HPC）程序自动调优系统，通过多智能体角色分配和迭代提示优化，实现高效代码生成与优化。


<details>
  <summary>Details</summary>
Motivation: 解决HPC程序调优复杂性，通过多智能体协作提升代码自动生成与优化效率。

Method: 设计包含项目经理、系统工程师、程序员和持续交付四个角色的多智能体系统；引入动态智能体部署和活动监控功能，支持多智能体协同工作；通过案例研究实现CPU代码向GPU CUDA代码的转换和优化。

Result: 多智能体配置下代码生成质量和效率均优于单智能体配置；动态部署和活动监控增强对需求违背和问题的识别能力。

Conclusion: 多智能体LLM系统有效提升了HPC程序的自动调优效率和代码质量，具备较强的协作与问题检测能力。

Abstract: We propose VibeCodeHPC, an automatic tuning system for HPC programs based on
multi-agent LLMs for code generation. VibeCodeHPC tunes programs through
multi-agent role allocation and iterative prompt refinement. We describe the
system configuration with four roles: Project Manager (PM), System Engineer
(SE), Programmer (PG), and Continuous Delivery (CD). We introduce dynamic agent
deployment and activity monitoring functions to facilitate effective
multi-agent collaboration. In our case study, we convert and optimize CPU-based
matrix-matrix multiplication code written in C to GPU code using CUDA. The
multi-agent configuration of VibeCodeHPC achieved higher-quality code
generation per unit time compared to a solo-agent configuration. Additionally,
the dynamic agent deployment and activity monitoring capabilities facilitated
more effective identification of requirement violations and other issues.

</details>


### [62] [A Scalable Framework for Safety Assurance of Self-Driving Vehicles based on Assurance 2.0](https://arxiv.org/abs/2510.00092)
*Shufeng Chen,Mariat James Elizebeth,Robab Aghazadeh Chakherlou,Xingyu Zhao,Eric Barbier,Siddartha Khastgir,Paul Jennings*

Main category: cs.SE

TL;DR: 本文提出了一套基于Assurance 2.0框架的分解方法，旨在全面识别安全性论证及其证据，应用于自动驾驶车辆开发全过程，提升安全保证的细致性和透明度。


<details>
  <summary>Details</summary>
Motivation: 面对日益复杂和自主化系统的安全保证需求，传统的Claims-Argument-Evidence模型存在信心度测量和偏见处理等局限，需引入更系统且灵活的保障框架。

Method: 基于Assurance 2.0范式，采用三层次分解策略和结构化模板，将自动驾驶车辆开发分为需求工程、验证与确认、部署后三个阶段，并结合改编的5M1E模型进行多维度分析，实现安全论点、证据和反驳点的细粒度追踪。

Result: 通过在自动驾驶车辆开发项目中的案例研究，演示了分解框架的有效应用，能够系统地覆盖安全论证各环节，确保论证的全面性和严谨性。

Conclusion: 该分解框架增强了安全论证的透明度和适应性，有助于持续、增量的安全保证，推动复杂自主系统在创新与安全之间的平衡。

Abstract: Assurance 2.0 is a modern framework developed to address the assurance
challenges of increasingly complex, adaptive, and autonomous systems. Building
on the traditional Claims-Argument-Evidence (CAE) model, it introduces reusable
assurance theories and explicit counterarguments (defeaters) to enhance rigor,
transparency, and adaptability. It supports continuous, incremental assurance,
enabling innovation without compromising safety. However, limitations persist
in confidence measurement, residual doubt management, automation support, and
the practical handling of defeaters and confirmation bias. This paper presents
\textcolor{black}{a set of decomposition frameworks to identify a complete set
of safety arguments and measure their corresponding evidence.} Grounded in the
Assurance 2.0 paradigm, the framework is instantiated through a structured
template and employs a three-tiered decomposition strategy. \textcolor{black}{A
case study regarding the application of the decomposition framework in the
end-to-end (E2E) AI-based Self-Driving Vehicle (SDV) development is also
presented in this paper.} At the top level, the SDV development is divided into
three critical phases: Requirements Engineering (RE), Verification and
Validation (VnV), and Post-Deployment (PD). Each phase is further decomposed
according to its Product Development Lifecycle (PDLC). To ensure comprehensive
coverage, each PDLC is analyzed using an adapted 5M1E model (Man, Machine,
Method, Material, Measurement, and Environment). Originally developed for
manufacturing quality control, the 5M1E model is reinterpreted and contextually
mapped to the assurance domain. This enables a multi-dimensional decomposition
that supports fine-grained traceability of safety claims, evidence, and
potential defeaters.

</details>


### [63] [Container Orchestration Patterns for Optimizing Resource Use](https://arxiv.org/abs/2510.00197)
*Diogo Maia,Filipe Correia,André Restivo,Paulo Queiroz*

Main category: cs.SE

TL;DR: 本文分析现有文献和工具，提出三种服务资源编排优化模式：抢占调度、服务平衡和垃圾回收，旨在提升服务编排实践的效果和推广。


<details>
  <summary>Details</summary>
Motivation: 服务化架构虽具显著优势，但服务编排对新手仍具挑战，现有资源缺乏清晰性和标准化，限制了最佳实践的实施和行业采用。

Method: 通过分析现有文献和工具，识别出常见的编排实践，进一步定义三种关键的资源优化模式。

Result: 提出抢占调度保证高优先级服务资源分配，服务平衡优化节点资源使用，垃圾回收机制帮助理解和优化系统资源消耗。

Conclusion: 这三种资源优化模式为改进服务编排实践奠定基础，有助于促进服务化架构中服务编排的广泛应用。

Abstract: Service-based architectures provide substantial benefits, yet service
orchestration remains a challenge, particularly for newcomers. While various
resources on orchestration techniques exist, they often lack clarity and
standardization, making best practices difficult to implement and limiting
their adoption within the software industry.
  To address this gap, we analyzed existing literature and tools to identify
common orchestration practices. Based on our findings, we define three key
orchestration resource optimization patterns: {\sc Preemptive Scheduling}, {\sc
Service Balancing}, and {\sc Garbage Collection}. {\sc Preemptive Scheduling}
allows the allocation of sufficient resources for services of higher priority
in stressful situations, while {\sc Service Balancing} enables a restructuring
of the nodes to allow better resource usage. To end, {\sc Garbage Collection}
creates cleanup mechanisms to better understand the system's resource usage and
optimize it. These patterns serve as foundational elements for improving
orchestration practices and fostering broader adoption in service-based
architectures.

</details>


### [64] [Which Programming Language and Model Work Best With LLM-as-a-Judge For Code Retrieval?](https://arxiv.org/abs/2510.00324)
*Lucas Roberts,Denisa Roberts*

Main category: cs.SE

TL;DR: 本文研究了利用大型语言模型（LLM）改进函数级代码搜索及注释生成，比较了不同检索器表示、编程语言和LLM对代码搜索相关性的影响，并提出使用转译器构建跨语言代码搜索基准数据集的方法。


<details>
  <summary>Details</summary>
Motivation: 代码搜索在软件开发中关键，但因为代码注释需要专业知识，人工标注成本高，阻碍了代码搜索的发展。本文旨在利用LLM减轻人工注释负担并提升代码搜索效果。

Method: 本文通过比较稀疏表示与语义表示、不同编程语言（C、Java、JavaScript、Go、Python）及多种LLM作为评判模型，对实现常见数据结构的代码库进行代码搜索与注释生成实验。此外提出用转译器扩展基准数据集。

Result: 发现检索器类型和编程语言存在亲和性，可以优化人类与AI之间的相关性判断一致性。不同表示方式在不同语言中影响AI判别效果。转译器方法使得生成跨语言数据集成为可能，且人机相关性一致性接近人类标注者之间一致性。

Conclusion: 利用LLM和转译器显著提升了代码搜索的自动化注释和相关性判断，降低了人工标注成本，推动了多语言代码搜索基准建设和性能提升。

Abstract: Code search is an important information retrieval application. Benefits of
better code search include faster new developer on-boarding, reduced software
maintenance, and ease of understanding for large repositories. Despite
improvements in search algorithms and search benchmarks, the domain of code
search has lagged behind. One reason is the high cost of human annotation for
code queries and answers. While humans may annotate search results in general
text QA systems, code annotations require specialized knowledge of a
programming language (PL), as well as domain specific software engineering
knowledge. In this work we study the use of Large Language Models (LLMs) to
retrieve code at the level of functions and to generate annotations for code
search results. We compare the impact of the retriever representation (sparse
vs. semantic), programming language, and LLM by comparing human annotations
across several popular languages (C, Java, Javascript, Go, and Python). We
focus on repositories that implement common data structures likely to be
implemented in any PLs. For the same human annotations, we compare several
LLM-as-a-Judge models to evaluate programming language and other affinities
between LLMs. We find that the chosen retriever and PL exhibit affinities that
can be leveraged to improve alignment of human and AI relevance determinations,
with significant performance implications. We also find differences in
representation (sparse vs. semantic) across PLs that impact alignment of human
and AI relevance determinations. We propose using transpilers to bootstrap
scalable code search benchmark datasets in other PLs and in a case study
demonstrate that human-AI relevance agreement rates largely match the (worst
case) human-human agreement under study. The application code used in this work
is available at \href{https://github.com/rlucas7/code-searcher/}{this github
repo}.

</details>


### [65] [Vibe Coding in Practice: Motivations, Challenges, and a Future Outlook -- a Grey Literature Review](https://arxiv.org/abs/2510.00328)
*Ahmed Fawzy,Amjed Tahir,Kelly Blincoe*

Main category: cs.SE

TL;DR: 本文系统性综述了AI代码生成工具中"vibe coding"的实践，即用户凭直觉和试错依赖AI生成代码。分析发现用户追求速度与易用性，但产出代码往往质量欠佳，且缺乏充分的质量保证措施。


<details>
  <summary>Details</summary>
Motivation: 尽管AI代码生成工具被广泛采用，但尚无研究系统探究用户为何以vibe coding方式使用这些工具，他们的体验以及如何进行质量保证。

Method: 通过系统性灰色文献综述，分析101个从业者来源中的518个用户行为描述，归纳vibe coding的实践特点、挑战和限制。

Result: 发现用户追求快速且易用，常跳过测试，依赖AI生成结果，导致代码质量不高且维护困难，产生新型脆弱开发者群体。

Conclusion: vibe coding虽降低门槛和加速原型制作，但以可靠性和可维护性为代价。研究对工具设计和开发团队具有重要启示，提示应引导其负责任使用以防止质量保证危机。

Abstract: AI code generation tools are transforming software development, especially
for novice and non-software developers, by enabling them to write code and
build applications faster and with little to no human intervention. Vibe coding
is the practice where users rely on AI code generation tools through intuition
and trial-and-error without necessarily understanding the underlying code.
Despite widespread adoption, no research has systematically investigated why
users engage in vibe coding, what they experience while doing so, and how they
approach quality assurance (QA) and perceive the quality of the AI-generated
code. To this end, we conduct a systematic grey literature review of 101
practitioner sources, extracting 518 firsthand behavioral accounts about vibe
coding practices, challenges, and limitations. Our analysis reveals a
speed-quality trade-off paradox, where vibe coders are motivated by speed and
accessibility, often experiencing rapid ``instant success and flow'', yet most
perceive the resulting code as fast but flawed. QA practices are frequently
overlooked, with many skipping testing, relying on the models' or tools'
outputs without modification, or delegating checks back to the AI code
generation tools. This creates a new class of vulnerable software developers,
particularly those who build a product but are unable to debug it when issues
arise. We argue that vibe coding lowers barriers and accelerates prototyping,
but at the cost of reliability and maintainability. These insights carry
implications for tool designers and software development teams. Understanding
how vibe coding is practiced today is crucial for guiding its responsible use
and preventing a broader QA crisis in AI-assisted development.

</details>


### [66] [Beyond Pass/Fail: The Story of Learning-Based Testing](https://arxiv.org/abs/2510.00450)
*Sheikh Md. Mushfiqur Rahman,Nasir Eisty*

Main category: cs.SE

TL;DR: 本文系统综述了学习型测试(LBT)的研究进展，探讨了其理论基础、工具库及工业应用案例，展示了LBT在软件测试中的潜力和有效性。


<details>
  <summary>Details</summary>
Motivation: LBT结合学习与测试以实现行为和测试充分性，能高效处理复杂系统测试，当前仍处于初期发展阶段，亟需系统总结和评估。

Method: 通过系统性文献回顾分析不同程序类型的LBT实现，评估其理论框架、现有工具和工业应用案例。

Result: 揭示了LBT的理论基础和工具发展状况，展示了在复杂程序和工业环境中的应用潜力和效果。

Conclusion: LBT作为一种有前景的软件测试技术，具备显著的实际应用价值，本文为研究人员和从业人员提供了全面视角，推动其进一步发展。

Abstract: Learning-Based Testing (LBT) merges learning and testing processes to achieve
both testing and behavioral adequacy. LBT utilizes active learning to infer the
model of the System Under Test (SUT), enabling scalability for large and
complex programs by requiring only a minimal set of initial test cases. The
core principle of LBT is that the SUT's behavior can be thoroughly inferred by
progressively generating test cases and subjecting the SUT to testing, thereby
ensuring comprehensive testing. Despite being in its early stages, LBT has a
solid foundation of theoretical research demonstrating its efficacy in testing
both procedural and reactive programs. This paper provides a systematic
literature review of various LBT implementations across different program types
and evaluates the current state of research in this field. We explore diverse
theoretical frameworks, existing tools, and libraries within the LBT domain to
illustrate the concept's evolution and current research status. Additionally,
we examine case studies involving the application of LBT tools in industrial
settings, highlighting their potential and effectiveness in commercial software
testing. This systematic literature review aims to offer researchers a
comprehensive perspective on the inception and development of LBT, presenting
it as a promising technique in software testing. By unveiling LBT's
underutilized potential, this paper seeks to significantly benefit the
practitioners and research community.

</details>


### [67] [Analyzing Latent Concepts in Code Language Models](https://arxiv.org/abs/2510.00476)
*Arushi Sharma,Vedant Pungliya,Christopher J. Quinn,Ali Jannesari*

Main category: cs.SE

TL;DR: 提出了Code Concept Analysis (CoCoA)方法，通过聚类代码语言模型的上下文嵌入，揭示模型内部的词汇、语法和语义概念，实现对代码模型行为的全局解释。


<details>
  <summary>Details</summary>
Motivation: 大型代码语言模型内部行为难以解释，尤其在需要信任和透明度的场景下，因此需要一种方法揭示其内部语义结构。

Method: 提出了CoCoA框架，结合静态分析和基于提示工程的大语言模型进行混合标注，对模型的上下文嵌入进行聚类，发现潜在概念，并结合局部归因方法生成概念级解释。

Result: CoCoA发现的概念在语义保持扰动下稳定，并能随着微调过程演化。用户研究表明，概念增强的解释相较于传统方法提升了37%的可解释性。

Conclusion: CoCoA有效提升了大型代码语言模型的透明度和可解释性，对理解模型内在行为及发现潜在偏差有重要价值。

Abstract: Interpreting the internal behavior of large language models trained on code
remains a critical challenge, particularly for applications demanding trust,
transparency, and semantic robustness. We propose Code Concept Analysis
(CoCoA): a global post-hoc interpretability framework that uncovers emergent
lexical, syntactic, and semantic structures in a code language model's
representation space by clustering contextualized token embeddings into
human-interpretable concept groups. We propose a hybrid annotation pipeline
that combines static analysis tool-based syntactic alignment with
prompt-engineered large language models (LLMs), enabling scalable labeling of
latent concepts across abstraction levels. We analyse the distribution of
concepts across layers and across three finetuning tasks. Emergent concept
clusters can help identify unexpected latent interactions and be used to
identify trends and biases within the model's learned representations. We
further integrate LCA with local attribution methods to produce
concept-grounded explanations, improving the coherence and interpretability of
token-level saliency. Empirical evaluations across multiple models and tasks
show that LCA discovers concepts that remain stable under semantic-preserving
perturbations (average Cluster Sensitivity Index, CSI = 0.288) and evolve
predictably with fine-tuning. In a user study, concept-augmented explanations
disambiguate token roles. In a user study on the programming-language
classification task, concept-augmented explanations disambiguated token roles
and improved human-centric explainability by 37 percentage points compared with
token-level attributions using Integrated Gradients.

</details>


### [68] [CodeChemist: Functional Knowledge Transfer for Low-Resource Code Generation via Test-Time Scaling](https://arxiv.org/abs/2510.00501)
*Kaixin Wang,Tianlin Li,Xiaoyu Zhang,Aishan Liu,Xianglong Liu,Ziqi Liu,Zhiqiang Zhang,Jun Zhou,and Bin Shi*

Main category: cs.SE

TL;DR: 提出了CodeChemist框架，通过生成并执行高资源编程语言的代码测试用例，实现对低资源编程语言的功能知识迁移，从而提升低资源语言的代码生成性能，无需模型重训练。


<details>
  <summary>Details</summary>
Motivation: CodeLLMs在不同编程语言上的性能不一致，尤其是低资源语言因训练数据有限表现较差，亟需提升低资源语言的代码生成效果。

Method: CodeChemist先在高资源语言生成并执行代码，创建功能性测试用例；然后在低资源语言中采用多温度采样生成代码片段，并通过测试用例的通过率选择最佳代码。

Result: 大规模实验表明，CodeChemist优于现有测试时扩展方法，显著提升低资源编程语言的代码生成性能。

Conclusion: CodeChemist有效实现了高资源语言向低资源语言的功能知识迁移，提升低资源语言代码生成质量，无需重训练模型。

Abstract: Code Large Language Models (CodeLLMs) are increasingly used in code
generation tasks across a wide range of applications. However, their
performance is often inconsistent across different programming languages (PLs),
with low-resource PLs suffering the most due to limited training data. In this
paper, we present CodeChemist, a novel and efficient framework for test-time
scaling that enables functional knowledge transfer from high-resource to
low-resource PLs using generated test cases. CodeChemist first generates and
executes code in high-resource PLs to create test cases that encapsulate
functional knowledge. It then uses multi-temperature hedged sampling to
generate code snippets in the low-resource PL and selects the best one based on
the pass rate of the test cases. Our extensive experiments show that
CodeChemist outperforms existing test-time scaling approaches, boosting the
performance of code generation for low-resource PLs without requiring any model
retraining.

</details>


### [69] [Architectural Transformations and Emerging Verification Demands in AI-Enabled Cyber-Physical Systems](https://arxiv.org/abs/2510.00519)
*Hadiza Umar Yusuf,Khouloud Gaaloul*

Main category: cs.SE

TL;DR: 本文探讨了人工智能集成如何影响网络物理系统（CPS）的架构和验证，重点比较了基于AI的控制模型与传统模型在Simulink设计中的差异。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能融入CPS，系统适应性增强，同时控制优化和可靠性面临更复杂的挑战。当前尚缺乏对这一转变对系统架构和验证实践影响的深入理解。

Method: 通过分析基于AI的控制模型与传统控制模型在Simulink中的架构差异，探讨其对系统验证的影响。

Result: 发现AI集成导致系统架构变得更加复杂，对验证流程提出了新的要求和挑战。

Conclusion: 本文填补了AI融入CPS后架构与验证间相互影响的研究空白，为后续优化和可靠性保障提供了理论基础。

Abstract: In the world of Cyber-Physical Systems (CPS), a captivating real-time fusion
occurs where digital technology meets the physical world. This synergy has been
significantly transformed by the integration of artificial intelligence (AI), a
move that dramatically enhances system adaptability and introduces a layer of
complexity that impacts CPS control optimization and reliability. Despite
advancements in AI integration, a significant gap remains in understanding how
this shift affects CPS architecture, operational complexity, and verification
practices. The extended abstract addresses this gap by investigating
architectural distinctions between AI-driven and traditional control models
designed in Simulink and their respective implications for system verification.

</details>


### [70] [LSPFuzz: Hunting Bugs in Language Servers](https://arxiv.org/abs/2510.00532)
*Hengcheng Zhu,Songqiang Chen,Valerio Terragni,Lili Wei,Jiarong Wu,Yepang Liu,Shing-Chi Cheung*

Main category: cs.SE

TL;DR: 本文提出了LSPFuzz，一种针对语言服务器协议（LSP）服务器的灰盒混合模糊测试工具。


<details>
  <summary>Details</summary>
Motivation: 现有LSP服务器存在崩溃和安全漏洞问题，严重影响代码智能功能和开发者安全，但缺乏专门的测试技术。

Method: 设计了一个两阶段变异流水线，包括面向语法的源代码变异和基于上下文的编辑器操作调度，以系统性地测试LSP服务器。

Result: 在四个主流LSP服务器上进行测试，LSPFuzz表现优于基线模糊测试工具，发现并报告了51个新缺陷，其中大部分已被确认和修复。

Conclusion: LSPFuzz提升了LSP服务器的质量保障，提供了实用工具和研究基础，推动该领域的后续研究。

Abstract: The Language Server Protocol (LSP) has revolutionized the integration of code
intelligence in modern software development. There are approximately 300 LSP
server implementations for various languages and 50 editors offering LSP
integration. However, the reliability of LSP servers is a growing concern, as
crashes can disable all code intelligence features and significantly impact
productivity, while vulnerabilities can put developers at risk even when
editing untrusted source code. Despite the widespread adoption of LSP, no
existing techniques specifically target LSP server testing. To bridge this gap,
we present LSPFuzz, a grey-box hybrid fuzzer for systematic LSP server testing.
Our key insight is that effective LSP server testing requires holistic mutation
of source code and editor operations, as bugs often manifest from their
combinations. To satisfy the sophisticated constraints of LSP and effectively
explore the input space, we employ a two-stage mutation pipeline: syntax-aware
mutations to source code, followed by context-aware dispatching of editor
operations. We evaluated LSPFuzz on four widely used LSP servers. LSPFuzz
demonstrated superior performance compared to baseline fuzzers, and uncovered
previously unknown bugs in real-world LSP servers. Of the 51 bugs we reported,
42 have been confirmed, 26 have been fixed by developers, and two have been
assigned CVE numbers. Our work advances the quality assurance of LSP servers,
providing both a practical tool and foundational insights for future research
in this domain.

</details>


### [71] [AI-Driven Self-Evolving Software: A Promising Path Toward Software Automation](https://arxiv.org/abs/2510.00591)
*Liyi Cai,Yijie Ren,Yitong Zhang,Jia Li*

Main category: cs.SE

TL;DR: 本文提出了一种通过与用户直接交互实现持续演进的软件形式，利用多智能体架构实现软件的自动构建和功能集成，迈出了真正软件自动化的第一步。


<details>
  <summary>Details</summary>
Motivation: 当前AI在软件开发中主要作为辅助工具，软件开发仍依赖人工干预；探索如何使AI成为软件的核心组件，实现真正的软件自动化。

Method: 提出AI驱动的自我演进软件，基于多智能体架构构建轻量级原型，自动解读用户需求、生成验证代码、集成功能。

Result: 在多个代表性场景的案例研究中，原型能可靠构建和复用功能，展示了该系统可扩展应用的潜力。

Conclusion: AI驱动的自我演进软件为实现无需人工干预的自动化软件开发铺平了道路，展示了可行性和应用前景。

Abstract: Software automation has long been a central goal of software engineering,
striving for software development that proceeds without human intervention.
Recent efforts have leveraged Artificial Intelligence (AI) to advance software
automation with notable progress. However, current AI functions primarily as
assistants to human developers, leaving software development still dependent on
explicit human intervention. This raises a fundamental question: Can AI move
beyond its role as an assistant to become a core component of software, thereby
enabling genuine software automation? To investigate this vision, we introduce
AI-Driven Self-Evolving Software, a new form of software that evolves
continuously through direct interaction with users. We demonstrate the
feasibility of this idea with a lightweight prototype built on a multi-agent
architecture that autonomously interprets user requirements, generates and
validates code, and integrates new functionalities. Case studies across
multiple representative scenarios show that the prototype can reliably
construct and reuse functionality, providing early evidence that such software
systems can scale to more sophisticated applications and pave the way toward
truly automated software development. We make code and cases in this work
publicly available at https://anonymous.4open.science/r/live-software.

</details>


### [72] [PyTrim: A Practical Tool for Reducing Python Dependency Bloat](https://arxiv.org/abs/2510.00674)
*Konstantinos Karakatsanis,Georgios Alexopoulos,Ioannis Karyotakis,Foivos Timotheos Proestakis,Evangelos Talos,Panos Louridas,Dimitris Mitropoulos*

Main category: cs.SE

TL;DR: PYTRIM是一个自动化工具，针对Python项目中的依赖臃肿问题，自动删除未使用的依赖，涵盖代码源码和配置文件，提升维护效率和安全性。


<details>
  <summary>Details</summary>
Motivation: Python项目中依赖臃肿问题普遍存在，导致维护成本和安全风险上升，现有工具只能检测未使用依赖，移除过程仍需大量手动操作和专业知识。

Method: 提出了PYTRIM系统，自动消除包括Python源码和配置文件（如requirements.txt、setup.py）中的未使用依赖。其模块化设计支持接入任意检测工具，并引入动态分析以提升检测召回率。

Result: 在包含37个合并请求的基准数据集上，PYTRIM实现98.3%的准确率；在971个开源包中检测并修剪出39个依赖臃肿实例，提交的拉取请求中已有6个被接受合并。

Conclusion: PYTRIM有效自动化解决Python依赖臃肿问题，减少维护工作量，提升项目质量，作为开源项目促进社区贡献和持续发展。

Abstract: Dependency bloat is a persistent challenge in Python projects, which
increases maintenance costs and security risks. While numerous tools exist for
detecting unused dependencies in Python, removing these dependencies across the
source code and configuration files of a project requires manual effort and
expertise.
  To tackle this challenge we introduce PYTRIM, an end-to-end system to
automate this process. PYTRIM eliminates unused imports and package
declarations across a variety of file types, including Python source and
configuration files such as requirements.txt and setup.py. PYTRIM's modular
design makes it agnostic to the source of dependency bloat information,
enabling integration with any detection tool. Beyond its contribution when it
comes to automation, PYTRIM also incorporates a novel dynamic analysis
component that improves dependency detection recall.
  Our evaluation of PYTRIM's end-to-end effectiveness on a ground-truth dataset
of 37 merged pull requests from prior work, shows that PYTRIM achieves 98.3%
accuracy in replicating human-made changes. To show its practical impact, we
run PYTRIM on 971 open-source packages, identifying and trimming bloated
dependencies in 39 of them. For each case, we submit a corresponding pull
request, 6 of which have already been accepted and merged. PYTRIM is available
as an open-source project, encouraging community contributions and further
development.
  Video demonstration: https://youtu.be/LqTEdOUbJRI
  Code repository: https://github.com/TrimTeam/PyTrim

</details>


### [73] [TShape: Rescuing Machine Learning Models from Complex Shapelet Anomalies](https://arxiv.org/abs/2510.00680)
*Hang Cui,Jingjing Li,Haotian Si,Quan Zhou,Changhua Pei,Gaogang Xie,Dan Pei*

Main category: cs.SE

TL;DR: 本文提出了TShape，一个针对工业时间序列异常检测的新框架，通过双重注意力机制和多尺度卷积提升对复杂形态异常的检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以检测复杂形态异常，这些异常虽然对专家明显，但对机器学习算法极具挑战性。

Method: 引入了基于补丁的双重注意力机制结合多尺度卷积，以平衡局部细粒度形态特征与全局上下文依赖来建模子序列变化。

Result: 在五个多样化基准测试中，TShape平均F1分数提升了10%，优于现有最先进模型。消融实验和注意力可视化验证了各组件的重要性。

Conclusion: TShape框架在复杂形态异常检测中表现稳健且适应性强，显著提升了工业时间序列异常检测的效果。

Abstract: Time series anomaly detection (TSAD) is critical for maintaining the
reliability of modern IT infrastructures, where complex anomalies frequently
arise in highly dynamic environments. In this paper, we present TShape, a novel
framework designed to address the challenges in industrial time series anomaly
detection. Existing methods often struggle to detect shapelet anomalies that
manifest as complex shape deviations, which appear obvious to human experts but
prove challenging for machine learning algorithms. TShape introduces a
patch-wise dual attention mechanism with multi-scale convolution to model
intricate sub-sequence variations by balancing local, fine-grained shape
features with global contextual dependencies. Our extensive evaluation on five
diverse benchmarks demonstrates that TShape outperforms existing
state-of-the-art models, achieving an average 10\% F1 score improvement in
anomaly detection. Additionally, ablation studies and attention visualizations
confirm the essential contributions of each component, highlighting the
robustness and adaptability of TShape to complex shapelet shapes in time series
data.

</details>


### [74] [Maven-Lockfile: High Integrity Rebuild of Past Java Releases](https://arxiv.org/abs/2510.00730)
*Larissa Schmid,Elias Lundell,Yogya Gamage,Benoit Baudry,Martin Monperrus*

Main category: cs.SE

TL;DR: 该论文提出了Maven-Lockfile工具，用于生成和更新Maven的依赖锁文件，实现了对所有依赖及其校验和的记录，支持从历史版本重建项目并验证依赖完整性。


<details>
  <summary>Details</summary>
Motivation: 现代软件项目依赖大量第三方库，使得构建的可复现性和安全性变得复杂，而Maven缺乏原生锁文件支持。

Method: 设计并实现Maven-Lockfile工具，生成包含直接和传递依赖及其校验和的锁文件，支持历史版本构建重现和依赖校验。

Result: 评估表明Maven-Lockfile能成功从历史提交复现构建，并检测到被篡改的构件。

Conclusion: Maven-Lockfile以极少配置为Java项目提供现代化的构建完整性和可复现性，促进Java软件供应链安全研究。

Abstract: Modern software projects depend on many third-party libraries, complicating
reproducible and secure builds. Several package managers address this with the
generation of a lockfile that freezes dependency versions and can be used to
verify the integrity of dependencies. Yet, Maven, one of the most important
package managers in the Java ecosystem, lacks native support for a lockfile. We
present Maven-Lockfile to generate and update lockfiles, with support for
rebuilding projects from past versions. Our lockfiles capture all direct and
transitive dependencies with their checksums, enabling high integrity builds.
Our evaluation shows that Maven-Lockfile can reproduce builds from historical
commits and is able to detect tampered artifacts. With minimal configuration,
Maven-Lockfile equips Java projects with modern build integrity and build
reproducibility, and fosters future research on software supply chain security
in Java.

</details>


### [75] [AI Where It Matters: Where, Why, and How Developers Want AI Support in Daily Work](https://arxiv.org/abs/2510.00762)
*Rudrajit Choudhuri,Carmen Badea,Christian Bird,Jenna Butler,Rob DeLine,Brian Houck*

Main category: cs.SE

TL;DR: 该研究通过对860名开发者的大规模混合方法调查，首次构建了开发者任务感知与AI采纳模式及责任AI优先级的映射，揭示不同任务对AI支持的需求与限制。


<details>
  <summary>Details</summary>
Motivation: 当前Generative AI正在改变软件开发工作，但缺乏关于开发者最需要和期望AI支持的具体领域及如何负责任地设计AI的明确指导。

Method: 采用认知评估理论，结合大规模的混合方法研究，对860名开发者进行调查，分析他们在不同任务中的AI使用态度与实际使用模式。

Result: 研究发现核心工作如编码和测试中AI使用较多且需求提升强烈；重复性工作（如文档和运维）对减少劳累的需求高；身份和关系相关工作（如指导）则限制AI的使用。不同任务对责任AI的优先需求不同，如系统任务重视可靠性和安全，人际任务则重视公平和包容性。

Conclusion: 研究为负责任地将AI应用于开发者及其工作中提供了具体的、情境相关的指导，帮助在重要任务中有效地交付AI支持。

Abstract: Generative AI is reshaping software work, yet we lack clear guidance on where
developers most need and want support, and how to design it responsibly. We
report a large-scale, mixed-methods study of N=860 developers that examines
where, why, and how they seek or limit AI help, providing the first task-aware,
empirically validated mapping from developers' perceptions of their tasks to AI
adoption patterns and responsible AI priorities. Using cognitive appraisal
theory, we show that task evaluations predict openness to and use of AI,
revealing distinct patterns: strong current use and a desire for improvement in
core work (e.g., coding, testing); high demand to reduce toil (e.g.,
documentation, operations); and clear limits for identity- and
relationship-centric work (e.g., mentoring). Priorities for responsible AI
support vary by context: reliability and security for systems-facing tasks;
transparency, alignment, and steerability to maintain control; and fairness and
inclusiveness for human-facing work. Our results offer concrete, contextual
guidance for delivering AI where it matters to developers and their work.

</details>


### [76] [Advancing Automated Ethical Profiling in SE: a Zero-Shot Evaluation of LLM Reasoning](https://arxiv.org/abs/2510.00881)
*Patrizio Migliarini,Mashal Afzal Memon,Marco Autili,Paola Inverardi*

Main category: cs.SE

TL;DR: 本文提出了一个自动化框架，评估16个大型语言模型在30个现实伦理场景中的道德推理能力，结果显示模型在理论一致性和道德接受度上表现良好。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在软件工程工具中的广泛应用，尤其涉及伦理决策，迫切需要评估其在伦理推理中的表现和稳定性。

Method: 采用零-shot设置，针对30个真实伦理场景，要求模型识别适用伦理理论、判断道德可接受性并解释理由，将模型回答与专家伦理学家进行对比，使用一致性指标评估表现。

Result: 模型平均理论一致率为73.3%，道德接受度二元一致率达86.7%，且模型解释在概念上高度收敛但语言表达多样。

Conclusion: 当前大型语言模型具备较好的伦理推理能力和稳定性，有望作为软件工程中可扩展、可审计且适应用户价值观的伦理推理引擎，为自动化伦理画像提供支持。

Abstract: Large Language Models (LLMs) are increasingly integrated into software
engineering (SE) tools for tasks that extend beyond code synthesis, including
judgment under uncertainty and reasoning in ethically significant contexts. We
present a fully automated framework for assessing ethical reasoning
capabilities across 16 LLMs in a zero-shot setting, using 30 real-world
ethically charged scenarios. Each model is prompted to identify the most
applicable ethical theory to an action, assess its moral acceptability, and
explain the reasoning behind their choice. Responses are compared against
expert ethicists' choices using inter-model agreement metrics. Our results show
that LLMs achieve an average Theory Consistency Rate (TCR) of 73.3% and Binary
Agreement Rate (BAR) on moral acceptability of 86.7%, with interpretable
divergences concentrated in ethically ambiguous cases. A qualitative analysis
of free-text explanations reveals strong conceptual convergence across models
despite surface-level lexical diversity. These findings support the potential
viability of LLMs as ethical inference engines within SE pipelines, enabling
scalable, auditable, and adaptive integration of user-aligned ethical
reasoning. Our focus is the Ethical Interpreter component of a broader
profiling pipeline: we evaluate whether current LLMs exhibit sufficient
interpretive stability and theory-consistent reasoning to support automated
profiling.

</details>


### [77] [On Effective Semantic Translation for Code: A Study Based on Pseudocode](https://arxiv.org/abs/2510.00920)
*Songqiang Chen,Congying Xu,Jingyi Chen,Jialun Cao,Jiarong Wu,Shing-Chi Cheung*

Main category: cs.SE

TL;DR: 本文研究了基于伪代码的代码翻译方法，发现其能够补充直接代码翻译，提升翻译准确性，尤其在从灵活语言到刚性语言的转换中表现突出，但也存在伪代码错误等限制。


<details>
  <summary>Details</summary>
Motivation: 现有直接代码翻译方法在准确翻译时存在挑战，借鉴人类通过伪代码理解程序意图的方式，探讨引入中间步骤提升翻译效果的可能性。

Method: 设计实证研究，比较直接代码翻译与伪代码中介的两步翻译方式，在九千余个任务、六种编程语言、五款大型语言模型上进行性能对比与分析。

Result: 伪代码翻译在某些语言对（尤其是从灵活到刚性语言）和低资源语言（Rust）中表现更优，能帮助理清复杂程序逻辑，避免原代码中细节干扰，同时两种方法互补提升翻译准确率。

Conclusion: 建议结合直接翻译与伪代码翻译的优势以提升代码翻译准确度，同时关注伪代码本身可能带来的错误和歧义限制。

Abstract: Large language models (LLMs) show great potential in code translation.
However, accurate translation remains challenging when using the commonly
adopted direct code-to-code translation approach, which converts a program into
the target programming language (PL) in a single step. Inspired by the success
of incorporating intermediate steps to guide LLMs in resolving challenging
tasks, we explore pseudocode-based code translation, which emulates the human
semantic translation by first interpreting the program's intent and logic into
pseudocode and then implementing it in the target PL. We find that
pseudocode-based translation helps translate programs that direct translation
struggles to handle. Nonetheless, the effectiveness, advantages, and
limitations of this approach remain underexplored. To bridge this gap, we
present an empirical study on pseudocode-based code translation, aiming to
investigate its effectiveness in enhancing the direct translation approach,
illuminate its effective usage, and identify limitations hindering its
potential benefits. By comparing direct and pseudocode-based translation
approaches on 9,690 translation tasks across six PLs with five popular LLMs, we
demonstrate that pseudocode-based translation can effectively complement direct
translation, particularly when translating from flexible to rigid PLs or
dealing with low-resource Rust. Based on these findings, we suggest adopting
strategies that combine the complementary strengths of both approaches to
enhance code translation accuracy. We also reveal the advantages of
pseudocode-based translation in disentangling translations of complicated
programs and mitigating distractions from detailed implementations in original
programs, as well as its limitations due to incorrect, incomplete, or ambiguous
pseudocode.

</details>


### [78] [ChatGPT in Introductory Programming: Counterbalanced Evaluation of Code Quality, Conceptual Learning, and Student Perceptions](https://arxiv.org/abs/2510.00946)
*Shiza Andleeb,Brandon Kantorski,Jeffrey C. Carver*

Main category: cs.SE

TL;DR: 在CS1编程课程中，使用ChatGPT能够提升代码质量和任务完成效率，但对概念理解的影响不一。


<details>
  <summary>Details</summary>
Motivation: 探讨ChatGPT在初学者编程课程中的使用对代码质量、概念理解、完成时间及学生感受的影响。

Method: 通过一个对比实验，学生交替使用和不使用ChatGPT完成两个C语言编程作业，并采用多维度评分、概念问卷和任务时间进行评估。

Result: 使用ChatGPT的学生代码质量更高，任务完成时间更短，但概念理解效果在不同主题间存在差异；学生总体对ChatGPT持正面评价，但担忧其准确性及长期技能培养。

Conclusion: ChatGPT可提高初学者编程效率和代码质量，但对概念理解帮助有限，需结合有结构的教学策略促进独立解决问题能力。

Abstract: Background: Large language models (LLMs) such as ChatGPT are increasingly
used in introductory programming courses to provide real-time code generation,
debugging, and explanations. While these tools can boost productivity and code
quality, concerns remain about over-reliance and potential impacts on
conceptual learning. Objective: To investigate how ChatGPT access affects code
quality, conceptual understanding, task completion times, and student
perceptions in a CS1 course. Methods: We conducted a counterbalanced,
quasi-experimental study in which students alternated between ChatGPT and
non-ChatGPT conditions across two programming assignments in C (functions and
structures). We evaluated their code submissions using multidimensional
rubrics, conceptual post-surveys, and task completion time. Results: Students
who had access to ChatGPT produced significantly higher rubric scores for code
quality and completed tasks in less time compared to those without access.
However, gains in conceptual understanding were mixed, lower for the functions
topic but higher for the structures topic. Students reported positive
experiences with ChatGPT, citing its value for debugging and practice, while
expressing concerns about accuracy and long-term skill development.
Conclusions: ChatGPT can enhance code quality and efficiency for novice
programmers, but may not uniformly improve conceptual understanding. Structured
integration and complementary instructional strategies are recommended to
foster independent problem-solving skills.

</details>


### [79] [Enhancing Software Testing Education: Understanding Where Students Struggle](https://arxiv.org/abs/2510.00957)
*Shiza Andleeb,Teo Mendoza,Lucas Cordova,Gursimran Walia,Jeffrey C. Carver*

Main category: cs.SE

TL;DR: 本研究通过分析高级软件测试课程中学生利用自动化反馈工具提交的测试套件，发现学生在覆盖决策路径和异常处理概念上存在明显理解不足，且常进行无效的测试套件修改。


<details>
  <summary>Details</summary>
Motivation: 许多计算机科学学生难以掌握构建全面测试套件的基础概念，导致软件测试效果不佳，现有自动反馈工具无法明确指出学生误区。

Method: 利用自动反馈工具分析两次课程作业中的学生测试套件提交，识别学生在测试概念上的常见误解及其导致的无效修改模式。

Result: 结果显示决策覆盖和异常处理是学生普遍挑战，学生多进行表面或方法级别的修改，但未能显著提升代码覆盖率。

Conclusion: 研究为教育者和工具设计者提供了改进反馈系统和教学重点的依据，旨在帮助学生克服误解，提高测试套件的质量和可维护性。

Abstract: Effective software testing is critical for producing reliable and secure
software, yet many computer science students struggle to master the
foundational concepts required to construct comprehensive test suites. While
automated feedback tools are widely used to support student learning, it
remains unclear which testing concepts are most frequently misunderstood and
how these misunderstandings are reflected in students' test suite revisions.
This study examines the specific testing concepts that lead students to make
ineffective changes, those that fail to improve code coverage, during test
suite development. Leveraging an automated feedback tool in a senior-level
software testing course, we analyzed student submissions from two assignments
to identify prevalent conceptual gaps and patterns of unproductive
modification. Our results reveal that decision coverage and exception handling
are persistent challenges, and that students most often make superficial or
method-level changes that do not enhance coverage. These findings provide
actionable insights for educators, researchers, and tool designers. By
pinpointing the concepts that most often contribute to poor testing outcomes,
we can refine feedback systems, target instruction to address persistent
misconceptions, and more effectively support students in developing robust,
maintainable test suites.

</details>


### [80] [Semantics-Aligned, Curriculum-Driven, and Reasoning-Enhanced Vulnerability Repair Framework](https://arxiv.org/abs/2510.01002)
*Chengran Yang,Ting Zhang,Jinfeng Jiang,Xin Zhou,Haoye Tian,Jieke Shi,Junkai Chen,Yikun Li,Eng Lieh Ouh,Lwin Khin Shar,David Lo*

Main category: cs.SE

TL;DR: 本文提出的SeCuRepair框架针对自动漏洞修复中的泛化能力不足、长距离依赖捕获困难和过度依赖表面模式等问题，实现了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前学习型自动漏洞修复方法难以在真实环境中有效泛化，存在跨仓库泛化能力弱、不能处理复杂多段修复、依赖表层词法模式导致性能下降等缺陷。

Method: 提出SeCuRepair框架，利用“先推理后编辑”范式，增强补丁生成的语义理解，结合语义感知强化学习和难度感知课程训练，逐步提升模型在多段复杂修复任务上的能力。

Result: 在BigVul和PrimeVul_AVR两个数据集的跨仓库评测中，SeCuRepair在CodeBLEU指标上分别领先最佳基线34.52%和31.52%，消融实验验证了框架各部分的有效性。

Conclusion: SeCuRepair通过结合语义推理与课程训练显著提升了自动漏洞修复模型的泛化性能和复杂漏洞修复能力，克服了当前主流方法的多项弱点。

Abstract: Current learning-based Automated Vulnerability Repair (AVR) approaches, while
promising, often fail to generalize effectively in real-world scenarios. Our
diagnostic analysis reveals three fundamental weaknesses in state-of-the-art
AVR approaches: (1) limited cross-repository generalization, with performance
drops on unseen codebases; (2) inability to capture long-range dependencies,
causing a performance degradation on complex, multi-hunk repairs; and (3)
over-reliance on superficial lexical patterns, leading to significant
performance drops on vulnerabilities with minor syntactic variations like
variable renaming.
  To address these limitations, we propose SeCuRepair, a semantics-aligned,
curriculum-driven, and reasoning-enhanced framework for vulnerability repair.
At its core, SeCuRepair adopts a reason-then-edit paradigm, requiring the model
to articulate why and how a vulnerability should be fixed before generating the
patch. This explicit reasoning enforces a genuine understanding of repair logic
rather than superficial memorization of lexical patterns. SeCuRepair also moves
beyond traditional supervised fine-tuning and employs semantics-aware
reinforcement learning, rewarding patches for their syntactic and semantic
alignment with the oracle patch rather than mere token overlap. Complementing
this, a difficulty-aware curriculum progressively trains the model, starting
with simple fixes and advancing to complex, multi-hunk coordinated edits.
  We evaluate SeCuRepair on strict, repository-level splits of BigVul and newly
crafted PrimeVul_AVR datasets. SeCuRepair significantly outperforms all
baselines, surpassing the best-performing baselines by 34.52% on BigVul and
31.52% on PrimeVul\textsubscript{AVR} in terms of CodeBLEU, respectively.
Comprehensive ablation studies further confirm that each component of our
framework contributes to its final performance.

</details>


### [81] [Improving Code Localization with Repository Memory](https://arxiv.org/abs/2510.01003)
*Boshi Wang,Weijian Xu,Yunsheng Li,Mei Gao,Yujia Xie,Huan Sun,Dongdong Chen*

Main category: cs.SE

TL;DR: 本文通过利用代码仓库的提交历史为语言代理系统引入长期记忆机制，提升代码定位任务的效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了代码仓库的长期记忆，导致每次任务都从零开始处理，缺乏历史经验支持。

Method: 通过利用提交历史和关联的issue，构建非参数化记忆库，并设计工具使代理系统能够检索相关的历史提交和代码摘要信息。

Result: 实验表明，引入长期记忆机制的LocAgent在两个SWE-bench基准测试中表现显著提升。

Conclusion: 增强语言代理系统的长期记忆机制，有助于更好地模拟人类开发者的经验积累，提高代码定位等长期任务的性能。

Abstract: Code localization is a fundamental challenge in repository-level software
engineering tasks such as bug fixing. While existing methods equip language
agents with comprehensive tools/interfaces to fetch information from the
repository, they overlook the critical aspect of memory, where each instance is
typically handled from scratch assuming no prior repository knowledge. In
contrast, human developers naturally build long-term repository memory, such as
the functionality of key modules and associations between various bug types and
their likely fix locations. In this work, we augment language agents with such
memory by leveraging a repository's commit history - a rich yet underutilized
resource that chronicles the codebase's evolution. We introduce tools that
allow the agent to retrieve from a non-parametric memory encompassing recent
historical commits and linked issues, as well as functionality summaries of
actively evolving parts of the codebase identified via commit patterns. We
demonstrate that augmenting such a memory can significantly improve LocAgent, a
state-of-the-art localization framework, on both SWE-bench-verified and the
more recent SWE-bench-live benchmarks. Our research contributes towards
developing agents that can accumulate and leverage past experience for
long-horizon tasks, more closely emulating the expertise of human developers.

</details>


### [82] [GenIA-E2ETest: A Generative AI-Based Approach for End-to-End Test Automation](https://arxiv.org/abs/2510.01024)
*Elvis Júnior,Alan Valejo,Jorge Valverde-Rebaza,Vânia de Oliveira Neves*

Main category: cs.SE

TL;DR: 本文提出了利用生成式AI自动生成端到端（E2E）测试脚本的方法，显著提高了测试自动化效率，减少了人工干预。


<details>
  <summary>Details</summary>
Motivation: 手动软件测试耗时且易出错，现有基于大型语言模型的自动化测试多聚焦于单元测试，缺乏对端到端测试中完整应用流程验证的支持。

Method: 设计GenIA-E2ETest系统，自动将自然语言描述转换为可执行的端到端测试脚本，应用于真实网页应用进行评测。

Result: 测试脚本在完整性、正确性、执行精准率和召回率均表现较好，手动调整率低且在常见网络场景中保持稳定。

Conclusion: GenIA-E2ETest有效推进了端到端自动化测试，降低了人工工作量，且为自动测试的普及提供了可行方案。

Abstract: Software testing is essential to ensure system quality, but it remains
time-consuming and error-prone when performed manually. Although recent
advances in Large Language Models (LLMs) have enabled automated test
generation, most existing solutions focus on unit testing and do not address
the challenges of end-to-end (E2E) testing, which validates complete
application workflows from user input to final system response. This paper
introduces GenIA-E2ETest, which leverages generative AI to generate executable
E2E test scripts from natural language descriptions automatically. We evaluated
the approach on two web applications, assessing completeness, correctness,
adaptation effort, and robustness. Results were encouraging: the scripts
achieved an average of 77% for both element metrics, 82% for precision of
execution, 85% for execution recall, required minimal manual adjustments
(average manual modification rate of 10%), and showed consistent performance in
typical web scenarios. Although some sensitivity to context-dependent
navigation and dynamic content was observed, the findings suggest that
GenIA-E2ETest is a practical and effective solution to accelerate E2E test
automation from natural language, reducing manual effort and broadening access
to automated testing.

</details>


### [83] [CodeGenLink: A Tool to Find the Likely Origin and License of Automatically Generated Code](https://arxiv.org/abs/2510.01077)
*Daniele Bifolco,Guido Annicchiarico,Pierluigi Barbiero,Massimiliano Di Penta,Fiorella Zampetti*

Main category: cs.SE

TL;DR: 本文提出CodeGenLink，一种结合大语言模型与网页搜索的Visual Studio Code扩展，用于为自动生成代码推荐相似源码链接并提供许可证信息。


<details>
  <summary>Details</summary>
Motivation: LLM生成代码缺乏可信度和代码来源信息，开发者担心版权和许可问题。

Method: 结合LLM与网页搜索产生候选链接，通过相似度分析筛选并展示代码来源及许可信息。

Result: 初步结果表明CodeGenLink能有效排除不相关链接并提供许可信息。

Conclusion: CodeGenLink提升了LLM生成代码的可追溯性和可信度，有助于解决版权及许可担忧。

Abstract: Large Language Models (LLMs) are widely used in software development tasks
nowadays. Unlike reusing code taken from the Web, for LLMs' generated code,
developers are concerned about its lack of trustworthiness and possible
copyright or licensing violations, due to the lack of code provenance
information. This paper proposes CodeGenLink, a GitHub CoPilot extension for
Visual Studio Code aimed at (i) suggesting links containing code very similar
to automatically generated code, and (ii) whenever possible, indicating the
license of the likely origin of the code. CodeGenLink retrieves candidate links
by combining LLMs with their web search features and then performs similarity
analysis between the generated and retrieved code. Preliminary results show
that CodeGenLink effectively filters unrelated links via similarity analysis
and provides licensing information when available. Tool URL:
https://github.com/danielebifolco/CodeGenLink Tool Video:
https://youtu.be/M6nqjBf9_pw

</details>


### [84] [Developers' Perspectives on Software Licensing: Current Practices, Challenges, and Tools](https://arxiv.org/abs/2510.01096)
*Nathan Wintersgill,Trevor Stalnaker,Daniel Otten,Laura A. Heymann,Oscar Chaparro,Massimiliano Di Penta,Daniel M. German,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: 本文针对软件开发者在开源组件许可合规上的实践进行了实证研究，揭示了相关挑战和当前使用的工具。


<details>
  <summary>Details</summary>
Motivation: 随着开源组件广泛应用，开发团队需要确保许可证合规，以避免法律和财务风险。理解开发者如何进行许可合规及其面临的问题至关重要。

Method: 通过对58名软件开发者的问卷调查和7次深入访谈，结合软件工程与法律研究者的合作，全面分析开发者在许可合规任务中的做法和挑战。

Result: 研究得出了15项关键发现，反映了当前许可合规实践的现状，包括开发者面临的困难及工具使用情况。

Conclusion: 本文讨论了研究结果的意义，并提出未来研究方向及对许可合规工具的改进建议，以帮助提升合规效率和效果。

Abstract: Most modern software products incorporate open-source components, requiring
development teams to maintain compliance with each component's licenses.
Noncompliance can lead to significant financial, legal, and reputational
repercussions. While some organizations may seek advice from legal
practitioners to assist with licensing tasks, developers still play a key role
in such a process. To this end, it is essential to understand how developers
approach license compliance tasks, the challenges they encounter, and the tools
that they use. This work studies these aspects of software licensing practices
through a study - conducted by a joint team of software engineering and legal
researchers - consisting of a survey with 58 software developers and seven
follow-up interviews. The study resulted in 15 key findings regarding the
current state of practice. We discuss the implications of our findings and
offer directions for future research as well as actionable recommendations for
licensing tools.

</details>


### [85] [When Shared Worlds Break: Demystifying Defects in Multi-User Extended Reality Software Systems](https://arxiv.org/abs/2510.01182)
*Shuqing Li,Chenran Zhang,Binchang Li,Cuiyun Gao,Michael R. Lyu*

Main category: cs.SE

TL;DR: 本文首次对多用户扩展现实（XR）系统中的软件缺陷进行了大规模实证研究，分析了2,649条真实缺陷报告，提出了缺陷分类体系并揭示了主要症状和根因，强调了其对系统稳定性和用户体验的严重影响。


<details>
  <summary>Details</summary>
Motivation: 多用户XR系统虽能提供变革性的共享体验，但独特的软件缺陷严重影响用户体验，且相关缺陷研究尚不充分，因此亟需对这些缺陷进行系统的理解与分类。

Method: 收集并分析了来自开发者论坛、GitHub仓库和主流XR应用市场的2,649条缺陷报告，采用迭代开放编码法进行定性分析，构建了包括症状表现、根因来源和后果严重性三个维度的缺陷分类体系。

Result: 发现同步不一致和头像异常是最普遍的症状，网络/同步逻辑缺陷及会话管理缺陷是主要根因，超过34%的缺陷导致系统崩溃、持续断线或交互彻底失效，且存在隐私和健康风险。

Conclusion: 多用户XR系统面临分布式系统、实时3D交互和沉浸体验交汇的独特挑战，需采用专项测试、调试和质量保障手段，本文提出了针对开发者、平台供应商和研究者的具体改进建议。

Abstract: Multi-user Extended Reality (XR) systems enable transformative shared
experiences but introduce unique software defects that compromise user
experience. Understanding software defects in multi-user XR systems is crucial
for enhancing system reliability, yet remains underexplored. To fill the gap,
this paper presents the first large-scale empirical study of multi-user XR
defects, analyzing 2,649 real-world bug reports from diverse sources, including
developer forums, GitHub repositories, and app reviews on mainstream XR app
stores. Through rigorous qualitative analysis using iterative open coding, we
develop a comprehensive taxonomy that classifies multi-user XR bugs along three
dimensions: Symptom Manifestation, Root Cause Origin, and Consequence Severity.
Our findings reveal that synchronization inconsistencies and avatar-related
anomalies are the most prevalent symptoms, while network/synchronization logic
defects and session management flaws emerge as dominant root causes.
Critically, over 34% of analyzed bugs lead to severe consequences that
fundamentally break the shared experience, including system crashes, persistent
disconnections, and complete interaction breakdowns, etc. We also identify
concerning privacy and health implications unique to multi-user XR contexts.
Based on our findings of defect analysis, we provide actionable recommendations
for developers, platform vendors, and researchers. Our results demonstrate that
multi-user XR systems face distinct challenges at the intersection of
distributed systems, real-time 3D interaction, and immersive experiences,
necessitating specialized approaches to testing, debugging, and quality
assurance.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [86] [A Hierarchical Agentic Framework for Autonomous Drone-Based Visual Inspection](https://arxiv.org/abs/2510.00259)
*Ethan Herron,Xian Yeow Lee,Gregory Sin,Teresa Gonzalez Diaz,Ahmed Farahat,Chetan Gupta*

Main category: cs.MA

TL;DR: 本文提出了一种用于工业室内环境中自主无人机控制的分层智能框架和基于自然语言的推理方法ReActEval，旨在实现视觉检查任务的自动化。


<details>
  <summary>Details</summary>
Motivation: 传统的智能体框架多集中于数字任务，缺乏应用于物理资产的实地自动化检测，工业环境中的无人机自主检查需求亟需高效且灵活的解决方案。

Method: 本文设计了由一个头部智能体和多个工作智能体组成的多智能体系统，头部智能体负责高层规划与评估，工作智能体通过ReActEval方法循环执行计划、推理、行动与评估，实现导航与复杂检查任务，全部操作用自然语言完成。

Result: 在仿真环境中，使用两个工作智能体进行实验，结果表明该框架能够有效完成不同复杂度的任务，提升工作流效率和任务完成度。

Conclusion: 基于自然语言处理的智能体通信使得无人机系统更灵活易用，减少了人工操作，展示了在工业检查中自主解决问题的潜力，为传统解决方案提供了创新替代方案。

Abstract: Autonomous inspection systems are essential for ensuring the performance and
longevity of industrial assets. Recently, agentic frameworks have demonstrated
significant potential for automating inspection workflows but have been limited
to digital tasks. Their application to physical assets in real-world
environments, however, remains underexplored. In this work, our contributions
are two-fold: first, we propose a hierarchical agentic framework for autonomous
drone control, and second, a reasoning methodology for individual function
executions which we refer to as ReActEval. Our framework focuses on visual
inspection tasks in indoor industrial settings, such as interpreting industrial
readouts or inspecting equipment. It employs a multi-agent system comprising a
head agent and multiple worker agents, each controlling a single drone. The
head agent performs high-level planning and evaluates outcomes, while worker
agents implement ReActEval to reason over and execute low-level actions.
Operating entirely in natural language, ReActEval follows a plan, reason, act,
evaluate cycle, enabling drones to handle tasks ranging from simple navigation
(e.g., flying forward 10 meters and land) to complex high-level tasks (e.g.,
locating and reading a pressure gauge). The evaluation phase serves as a
feedback and/or replanning stage, ensuring actions align with user objectives
while preventing undesirable outcomes. We evaluate the framework in a simulated
environment with two worker agents, assessing performance qualitatively and
quantitatively based on task completion across varying complexity levels and
workflow efficiency. By leveraging natural language processing for agent
communication, our approach offers a novel, flexible, and user-accessible
alternative to traditional drone-based solutions, enabling autonomous
problem-solving for industrial inspection without extensive user intervention.

</details>


### [87] [Reasoning-Aware Prompt Orchestration: A Foundation Model for Multi-Agent Language Model Coordination](https://arxiv.org/abs/2510.00326)
*Hassen Dhrif*

Main category: cs.MA

TL;DR: 提出了一个动态提示协调框架，提升多智能体系统中的推理能力，实现逻辑一致性和推理感知的提示适应，支持分布式推理的可扩展协调。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统中通过提示工程协调推理存在逻辑一致性和协调可扩展性方面的挑战。

Method: 通过提示模板、推理上下文向量和能力矩阵形式化智能体状态，建立理论收敛性证明，并设计分布式架构动态路由推理任务。

Result: 在1000个合成多智能体对话中，推理延迟降低42%，逻辑一致性提升23%，任务完成成功率89%。消融实验表明共识机制是关键性能驱动因素，同时发现性能在10次智能体切换后下降，并且需大量内存支持。

Conclusion: 该框架为多智能体语言模型推理提供了理论基础和可扩展的新范式，虽存在性能和资源限制，但显著提升了推理效率和一致性。

Abstract: The emergence of large language models has enabled sophisticated multi-agent
systems, yet coordinating their reasoning capabilities through prompt
engineering remains challenging. We present a theoretically-grounded framework
for dynamic prompt orchestration that enhances reasoning across multiple
specialized agents. This framework addresses three core challenges: logical
consistency preservation during agent transitions, reasoning-aware prompt
adaptation, and scalable coordination of distributed inference.
  Our approach formalizes agent states using prompt templates, reasoning
context vectors, and capability matrices. We prove system convergence to stable
coordination patterns when step sizes satisfy $\alpha < \frac{1}{2L}$ where $L$
is the Lipschitz constant of the state transition function. We implement this
through a distributed architecture that dynamically routes reasoning tasks
while maintaining semantic coherence.
  Experimental results on 1,000 synthetic multi-agent conversations demonstrate
a 42% reduction in reasoning latency, a 23% improvement in logical consistency
measured by ROUGE-L score, and an 89% success rate for task completion without
context loss across agent transitions. Ablation studies identify the consensus
mechanism as the primary performance driver, while revealing limitations:
performance degrades beyond 10 agent transitions, and the system requires
76.5GB memory for 1,000 concurrent agents. These findings establish a new
paradigm for scalable reasoning in multi-agent systems, providing theoretical
foundations for understanding reasoning emergence across coordinated language
models.

</details>


### [88] [Conflict-Based Search as a Protocol: A Multi-Agent Motion Planning Protocol for Heterogeneous Agents, Solvers, and Independent Tasks](https://arxiv.org/abs/2510.00425)
*Rishi Veerapaneni,Alvin Tang,Haodong He,Sophia Zhao,Viraj Shah,Yidai Cen,Ziteng Ji,Gabriel Olin,Jon Arrizabalaga,Yorai Shaoul,Jiaoyang Li,Maxim Likhachev*

Main category: cs.MA

TL;DR: 本文提出利用冲突基础搜索（CBS）协议，实现了多厂商、多类型机器人在共享环境中的高效无碰撞移动。


<details>
  <summary>Details</summary>
Motivation: 解决不同厂商机器人各自独立运动规划系统如何在同一环境中有效协作、避免碰撞的问题。

Method: 采用冲突基础搜索（CBS）协议，利用单一代理运动规划API，结合多种单代理规划算法（启发式搜索、采样搜索、优化、扩散、强化学习），通过中心规划实现多代理无碰撞路径规划。

Result: 证明CBS协议可以融合多种异构单代理规划方法，有效实现多机器人系统的无碰撞路径规划。

Conclusion: CBS作为一种统一协议，可以支持异构多智能体系统高效的协同运动规划，促进未来多机器人应用场景的发展。

Abstract: Imagine the future construction site, hospital, office, or even sophisticated
household with dozens of robots bought from different manufacturers. How can we
enable these different systems to effectively move in a shared environment,
given that each robot may have its own independent motion planning system? This
work shows how we can get efficient collision-free movements between
algorithmically heterogeneous agents by using Conflict-Based Search (Sharon et
al. 2015) as a protocol. At its core, the CBS Protocol requires one specific
single-agent motion planning API; finding a collision-free path that satisfies
certain space-time constraints. Given such an API, CBS uses a central planner
to find collision-free paths - independent of how the API is implemented. We
show how this protocol enables multi-agent motion planning for a heterogeneous
team of agents completing independent tasks with a variety of single-agent
planners including: Heuristic Search (e.g., A*), Sampling Based Search (e.g.,
RRT), Optimization (e.g., Direct Collocation), Diffusion, and Reinforcement
Learning.

</details>


### [89] [Stochastic Self-Organization in Multi-Agent Systems](https://arxiv.org/abs/2510.00685)
*Nurbek Tastan,Samuel Horvath,Karthik Nandakumar*

Main category: cs.MA

TL;DR: 本文提出了一种基于响应条件的多智能体通信自组织框架（SelfOrg），通过动态构建有向无环图，实现智能体之间高效稳定的信息传播，显著提升了弱大语言模型下的协作性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有多智能体系统通信机制依赖固定拓扑或复杂优化，限制有效合作的问题。

Method: 智能体独立生成回应并使用Shapley值近似评估贡献，动态构建并更新有向无环图，控制信息传播路径，无需额外监督或训练，实现自组织通信。

Result: 在强弱不同大语言模型后台下均表现稳健，弱模型环境中性能显著优于现有方法。理论证明多智能体能提高正确率，正确信息自然占优。

Conclusion: SelfOrg框架有效优化多智能体协作中的通信机制，提高响应准确性和信息流效率，尤其在弱模型环境中表现突出。

Abstract: Multi-agent systems (MAS) based on Large Language Models (LLMs) have the
potential to solve tasks that are beyond the reach of any single LLM. However,
this potential can only be realized when the collaboration mechanism between
agents is optimized. Specifically, optimizing the communication structure
between agents is critical for fruitful collaboration. Most existing approaches
rely on fixed topologies, pretrained graph generators, optimization over edges,
or employ external LLM judges, thereby adding to the complexity. In this work,
we introduce a response-conditioned framework that adapts communication
on-the-fly. Agents independently generate responses to the user query and
assess peer contributions using an approximation of the Shapley value. A
directed acyclic graph (DAG) is then constructed to regulate the propagation of
the responses among agents, which ensures stable and efficient message
transmission from high-contributing agents to others. This graph is dynamically
updated based on the agent responses from the previous collaboration round.
Since the proposed framework enables the self-organization of agents without
additional supervision or training, we refer to it as SelfOrg. The SelfOrg
framework goes beyond task- and query-level optimization and takes into account
the stochastic nature of agent responses. Experiments with both strong and weak
LLM backends demonstrate robust performance, with significant gains in the weak
regime where prior methods collapse. We also theoretically show that multiple
agents increase the chance of correctness and that the correct responses
naturally dominate the information flow.

</details>


### [90] [Partial Resilient Leader-Follower Consensus in Time-Varying Graphs](https://arxiv.org/abs/2510.01144)
*Haejoon Lee,Dimitra Panagou*

Main category: cs.MA

TL;DR: 本文研究在存在有限恶意节点情况下的部分领导-跟随共识问题，提出了BP-MSR算法保证部分非恶意节点跟随领导状态。


<details>
  <summary>Details</summary>
Motivation: 现有方法需整个网络满足鲁棒性条件才能保证复原共识，而这些条件不满足时系统行为未被深入研究。

Method: 提出了一种基于Bootstrap Percolation和Mean Subsequence Reduced的分布式算法（BP-MSR），并给出个体节点达成共识的充分条件。

Result: 仿真结果表明该算法即使在传统鲁棒共识算法失效时也能保证部分非恶意节点实现领导-跟随共识。

Conclusion: BP-MSR算法在鲁棒条件不足时仍能实现有部分非恶意节点跟随领导，实现了部分领导-跟随共识的新概念和方法。

Abstract: This work studies resilient leader-follower consensus with a bounded number
of adversaries. Existing approaches typically require robustness conditions of
the entire network to guarantee resilient consensus. However, the behavior of
such systems when these conditions are not fully met remains unexplored. To
address this gap, we introduce the notion of partial leader-follower consensus,
in which a subset of non-adversarial followers successfully tracks the leader's
reference state despite insufficient robustness. We propose a novel distributed
algorithm - the Bootstrap Percolation and Mean Subsequence Reduced (BP-MSR)
algorithm - and establish sufficient conditions for individual followers to
achieve consensus via the BP-MSR algorithm in arbitrary time-varying graphs. We
validate our findings through simulations, demonstrating that our method
guarantees partial leader-follower consensus, even when standard resilient
consensus algorithms fail.

</details>
