<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 38]
- [cs.SE](#cs.SE) [Total: 5]
- [cs.MA](#cs.MA) [Total: 6]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Entropy-Based Measurement of Value Drift and Alignment Work in Large Language Models](https://arxiv.org/abs/2512.03047)
*Samih Fadli*

Main category: cs.CL

TL;DR: 本文提出基于伦理熵的动态监测框架，通过行为分类和熵评估量化模型价值漂移，利用调优模型显著减少伦理熵增长，支持运行时安全监督。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型安全评估依赖静态基准，但关键失败是动态的，如价值漂移、越狱攻击和对齐退化。基于智能第二定律中伦理熵随时间增长的理论，构建动态监测框架。

Method: 定义了五类行为分类法，训练分类器估计模型对话中的伦理熵S(t)，并通过压力测试测量模型伦理熵动态。

Result: 基础模型表现持续伦理熵增长，指令调优模型抑制伦理熵漂移，减少约80%的伦理熵。估计有效对齐工作率γ_eff，将伦理熵和γ_eff嵌入监测流程中，实现运行时价值漂移预警。

Conclusion: 利用伦理熵动态评估和监控大语言模型的价值漂移，有效抑制对齐退化并实现运行时预警，为模型安全提供实用工具。

Abstract: Large language model safety is usually assessed with static benchmarks, but key failures are dynamic: value drift under distribution shift, jailbreak attacks, and slow degradation of alignment in deployment. Building on a recent Second Law of Intelligence that treats ethical entropy as a state variable which tends to increase unless countered by alignment work, we make this framework operational for large language models. We define a five-way behavioral taxonomy, train a classifier to estimate ethical entropy S(t) from model transcripts, and measure entropy dynamics for base and instruction-tuned variants of four frontier models across stress tests. Base models show sustained entropy growth, while tuned variants suppress drift and reduce ethical entropy by roughly eighty percent. From these trajectories we estimate an effective alignment work rate gamma_eff and embed S(t) and gamma_eff in a monitoring pipeline that raises alerts when entropy drift exceeds a stability threshold, enabling run-time oversight of value drift.

</details>


### [2] [Watermarks for Embeddings-as-a-Service Large Language Models](https://arxiv.org/abs/2512.03079)
*Anudeex Shetty*

Main category: cs.CL

TL;DR: 本文研究了基于大型语言模型（LLMs）的文本嵌入服务（EaaS）在防御仿冒攻击中的水印技术，发现现有水印易被文本改写绕过，提出了一种基于线性变换的新型水印WET，实验证明其对改写攻击具备较强的鲁棒性和高准确率。


<details>
  <summary>Details</summary>
Motivation: 为解决EaaS中的仿冒攻击问题，保护模型所有权，本文聚焦于研究水印技术的漏洞与改进，提升水印的防护能力和验证准确率。

Method: 本文首先通过实验证明了现有水印方法容易被输入文本的改写绕过；随后提出WET水印方法，利用线性变换对嵌入进行编码，验证时通过逆变换对比相似度以确认水印；最后通过消融实验评估各组件及参数的影响。

Result: 实验表明现有水印技术面对文本改写攻击时鲁棒性不足，新提WET方法可实现近乎完美的水印验证，详细实验验证了各因素对性能的影响。

Conclusion: 本文揭示了当前EaaS水印技术存在的改写攻击漏洞，并提出了新颖的基于线性变换的水印方法WET，有效防御了此类攻击，保障了模型所有权的验证。

Abstract: Large Language Models (LLMs) have demonstrated exceptional capabilities in natural language understanding and generation. Based on these LLMs, businesses have started to provide Embeddings-as-a-Service (EaaS), offering feature extraction capabilities (in the form of text embeddings) that benefit downstream natural language processing tasks. However, prior research has demonstrated that EaaS is vulnerable to imitation attacks, where an attacker clones the service's model in a black-box manner without access to the model's internal workings. In response, watermarks have been added to the text embeddings to protect the intellectual property of EaaS providers by allowing them to check for model ownership. This thesis focuses on defending against imitation attacks by investigating EaaS watermarks. To achieve this goal, we unveil novel attacks and propose and validate new watermarking techniques.
  Firstly, we show that existing EaaS watermarks can be removed through paraphrasing the input text when attackers clone the model during imitation attacks. Our study illustrates that paraphrasing can effectively bypass current state-of-the-art EaaS watermarks across various attack setups (including different paraphrasing techniques and models) and datasets in most instances. This demonstrates a new vulnerability in recent EaaS watermarking techniques.
  Subsequently, as a countermeasure, we propose a novel watermarking technique, WET (Watermarking EaaS with Linear Transformation), which employs linear transformation of the embeddings. Watermark verification is conducted by applying a reverse transformation and comparing the similarity between recovered and original embeddings. We demonstrate its robustness against paraphrasing attacks with near-perfect verifiability. We conduct detailed ablation studies to assess the significance of each component and hyperparameter in WET.

</details>


### [3] [Alleviating Choice Supportive Bias in LLM with Reasoning Dependency Generation](https://arxiv.org/abs/2512.03082)
*Nan Zhuang,Wenshuo Wang,Lekai Qian,Yuxiao Wang,Boyu Cao,Qi Liu*

Main category: cs.CL

TL;DR: 本文首次提出通过推理依赖生成框架生成无偏推理数据，以缓解大型语言模型的选择支持偏差，显著提升模型的决策客观性。


<details>
  <summary>Details</summary>
Motivation: 当前去偏方法多针对社会和人口统计偏差，而认知偏差如选择支持偏差尚未被充分研究和解决，影响AI辅助决策的客观性，亟需开发有效的去偏技术。

Method: 通过自动生成平衡的推理问答对，明确建模选择、证据与理由之间的依赖关系，利用生成的数据对大型语言模型进行微调，以减轻选择支持偏差。

Result: 本文提出了一种名为推理依赖生成（RDG）的新框架，以解决大型语言模型（LLMs）在评价过程中表现出的选择支持偏差（CSB）。RDG自动构建平衡的推理问答对，明确建模选项、证据和理由之间的依赖关系，通过微调生成的数据来减轻CSB。实验表明，微调后的模型在记忆和评价任务上分别提高了81.5%和94.3%，且在标准BBQ基准测试中表现稳定，展示了该方法在纠正认知偏差方面的有效性。

Conclusion: 推理依赖生成框架有效缓解了大型语言模型的选择支持偏差，提升了模型的可靠性和决策的客观性，有助于构建更可信的AI辅助决策系统。

Abstract: Recent studies have demonstrated that some Large Language Models exhibit choice-supportive bias (CSB) when performing evaluations, systematically favoring their chosen options and potentially compromising the objectivity of AI-assisted decision making. While existing debiasing approaches primarily target demographic and social biases, methods for addressing cognitive biases in LLMs remain largely unexplored. In this work, we present the first solution to address CSB through Reasoning Dependency Generation (RDG), a novel framework for generating unbiased reasoning data to mitigate choice-supportive bias through fine-tuning. RDG automatically constructs balanced reasoning QA pairs, explicitly (un)modeling the dependencies between choices, evidences, and justifications. Our approach is able to generate a large-scale dataset of QA pairs across domains, incorporating Contextual Dependency Data and Dependency Decouple Data. Experiments show that LLMs fine-tuned on RDG-generated data demonstrate a 81.5% improvement in memory-based experiments and 94.3% improvement in the evaluation-based experiment, while maintaining similar performance on standard BBQ benchmarks. This work pioneers an approach for addressing cognitive biases in LLMs and contributes to the development of more reliable AI-assisted decision support systems.

</details>


### [4] [Enhancing Job Matching: Occupation, Skill and Qualification Linking with the ESCO and EQF taxonomies](https://arxiv.org/abs/2512.03195)
*Stylianos Saroglou,Konstantinos Diamantaras,Francesco Preta,Marina Delianidi,Apostolos Benisis,Christian Johannes Meyer*

Main category: cs.CL

TL;DR: 通过语言模型和欧洲职业框架结合，提升职位文本分类准确率，公开工具与数据支持后续研究。


<details>
  <summary>Details</summary>
Motivation: 提升劳动市场信息的分类能力，精确连结职位文本与欧洲权威职业分类体系，更好地分析工作性质和技能需求。

Method: 采用语言模型，结合Sentence Linking和Entity Linking方法，将职位文本分类链接至欧洲ESCO和EQF框架。引入生成式大型语言模型并发布了开源工具。

Result: 开发了一个集成两种方法的开源工具，发布了两个带注释的数据集，验证了多种语言模型在职业和资格识别上的效果。

Conclusion: 研究推动了职位实体抽取技术进步，提供了数字经济下劳动市场研究的计算基础设施，有助于理解劳动市场和职业技能结构。

Abstract: This study investigates the potential of language models to improve the classification of labor market information by linking job vacancy texts to two major European frameworks: the European Skills, Competences, Qualifications and Occupations (ESCO) taxonomy and the European Qualifications Framework (EQF). We examine and compare two prominent methodologies from the literature: Sentence Linking and Entity Linking. In support of ongoing research, we release an open-source tool, incorporating these two methodologies, designed to facilitate further work on labor classification and employment discourse. To move beyond surface-level skill extraction, we introduce two annotated datasets specifically aimed at evaluating how occupations and qualifications are represented within job vacancy texts. Additionally, we examine different ways to utilize generative large language models for this task. Our findings contribute to advancing the state of the art in job entity extraction and offer computational infrastructure for examining work, skills, and labor market narratives in a digitally mediated economy. Our code is made publicly available: https://github.com/tabiya-tech/tabiya-livelihoods-classifier

</details>


### [5] [InvertiTune: High-Quality Data Synthesis for Cost-Effective Single-Shot Text-to-Knowledge Graph Generation](https://arxiv.org/abs/2512.03197)
*Faezeh Faez,Marzieh S. Tahaei,Yaochen Hu,Ali Pourranjbar,Mahdi Biparva,Mark Coates,Yingxue Zhang*

Main category: cs.CL

TL;DR: InvertiTune通过数据生成和微调，提升了文本生成知识图谱的效率和准确性，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有Text2KG方法依赖迭代LLM提示，计算成本高且易忽略文本中分布的复杂关系，亟需一种高效且能捕捉复杂语义的新方法。

Method: 提出受控数据生成管道，从大知识库中提取子图，过滤噪声，并通过LLM生成自然语言描述，结合监督微调训练轻量模型进行单次知识图构建。

Result: 本文提出了InvertiTune框架，通过受控的数据生成和监督微调相结合，解决了现有基于LLM的文本到知识图谱（Text2KG）方法计算资源消耗大且难以捕捉复杂关系的问题。InvertiTune利用大知识库提取子图并生成对应的自然语言描述，从而生成更符合真实场景的大规模数据集，推动轻量模型的单次知识图构建。实验证明InvertiTune在CE12k数据集上优于未微调的大型LLM和最先进的Text2KG方法，并在跨数据集测试中的泛化能力更强。

Conclusion: 高质量且贴近真实场景的训练数据对提升Text2KG系统的效率和性能至关重要，InvertiTune框架有效实现了这一目标。

Abstract: Large Language Models (LLMs) have revolutionized the ability to understand and generate text, enabling significant progress in automatic knowledge graph construction from text (Text2KG). Many Text2KG methods, however, rely on iterative LLM prompting, making them computationally expensive and prone to overlooking complex relations distributed throughout the text. To address these limitations, we propose InvertiTune, a framework that combines a controlled data generation pipeline with supervised fine-tuning (SFT). Within this framework, the data-generation pipeline systematically extracts subgraphs from large knowledge bases, applies noise filtering, and leverages LLMs to generate corresponding natural text descriptions, a task more aligned with LLM capabilities than direct KG generation from text. This pipeline enables generating datasets composed of longer texts paired with larger KGs that better reflect real-world scenarios compared to existing benchmarks, thus supporting effective SFT of lightweight models for single-shot KG construction. Experimental results on CE12k, a dataset generated using the introduced pipeline, show that InvertiTune outperforms larger non-fine-tuned LLMs as well as state-of-the-art Text2KG approaches, while also demonstrating stronger cross-dataset generalization on CrossEval-1200, a test set created from three established benchmark datasets and CE12k. These findings highlight the importance of realistic, high-quality training data for advancing efficient and high-performing Text2KG systems.

</details>


### [6] [Identifying attributions of causality in political text](https://arxiv.org/abs/2512.03214)
*Paulina Garcia-Corral*

Main category: cs.CL

TL;DR: 本文提出一种基于轻量级因果语言模型的技术，系统地识别和解析政治文本中的因果解释，为政治科学中解释的研究提供了有效工具。


<details>
  <summary>Details</summary>
Motivation: 尽管解释是人们理解政治事件的基础，但在政治科学中相关系统分析方法有限且零散，缺乏统一框架。

Method: 训练轻量级因果语言模型，自动检测和解析政治文本中的因果解释，生成因果关系对的结构化数据集。

Result: 该模型能够大规模研究因果解释，标注需求低，具有较好泛化能力和相对于人工编码的准确性。

Conclusion: 本文方法实现了政治文本中因果解释的自动检测和结构化表达，促进了解释在政治科学中系统化和规模化的研究。

Abstract: Explanations are a fundamental element of how people make sense of the political world. Citizens routinely ask and answer questions about why events happen, who is responsible, and what could or should be done differently. Yet despite their importance, explanations remain an underdeveloped object of systematic analysis in political science, and existing approaches are fragmented and often issue-specific. I introduce a framework for detecting and parsing explanations in political text. To do this, I train a lightweight causal language model that returns a structured data set of causal claims in the form of cause-effect pairs for downstream analysis. I demonstrate how causal explanations can be studied at scale, and show the method's modest annotation requirements, generalizability, and accuracy relative to human coding.

</details>


### [7] [Randomized Masked Finetuning: An Efficient Way to Mitigate Memorization of PIIs in LLMs](https://arxiv.org/abs/2512.03310)
*Kunj Joshi,David A. Smith*

Main category: cs.CL

TL;DR: 提出了一种随机掩码微调方法，显著减少大语言模型中个人身份信息的记忆，提升隐私保护效果，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型存在严重的安全和隐私风险，模型容易记忆训练数据中的个人身份信息。

Method: 提出了一种新颖的随机掩码微调（RMFT）技术，通过在微调过程中随机屏蔽部分信息，减少模型对敏感数据的记忆。

Result: 在Enron邮件数据集上，RMFT比传统微调方法将总提取率降低了80.81%，已见提取率降低了80.17%，且困惑度仅增加了5.73%，在隐私和性能之间达到了较好的平衡。

Conclusion: RMFT方法显著降低了大语言模型中个人身份信息的记忆，实现了有效的隐私保护，同时保持了较低的性能损失。

Abstract: The current literature on memorization in Natural Language Models, especially Large Language Models (LLMs), poses severe security and privacy risks, as models tend to memorize personally identifying information (PIIs) from training data. We introduce Randomized Masked Fine-Tuning (RMFT), a novel privacy-preserving fine-tuning technique that reduces PII memorization while minimizing performance impact. Using the Enron Email Dataset, we demonstrate that RMFT achieves an 80.81% reduction in Total Extraction Rate and 80.17% reduction in Seen Extraction Rate compared to baseline fine-tuning, outperforming deduplication methods while maintaining only a 5.73% increase in perplexity. We present MaxTER, a Pareto-optimal evaluation framework for assessing privacy-utility tradeoffs, and show the performance of RMFT vs Deduplication by Area Under The Response Curve (AURC) metric.

</details>


### [8] [Modeling Topics and Sociolinguistic Variation in Code-Switched Discourse: Insights from Spanish-English and Spanish-Guaraní](https://arxiv.org/abs/2512.03334)
*Nemika Tyagi,Nelvin Licona Guevara,Olga Kellert*

Main category: cs.CL

TL;DR: 该研究利用大型语言模型自动标注双语代码转换语料，实现了社会语言学与话题分析的规模化与自动化，揭示了双语环境下的语言使用规律，验证了大型模型在该领域的有效性。


<details>
  <summary>Details</summary>
Motivation: 通过利用大型语言模型自动注释，提高社会语言学和话题分析的效率，尤其是针对跨语言和低资源环境下的双语话语分析。

Method: 采用大型语言模型自动标注了3691个代码转换句子的主题、体裁和语篇语用功能，结合迈阿密双语语料库的人口统计元数据，并对西班牙语-瓜拉尼语数据集进行了新主题注释。

Result: 发现了迈阿密数据中性别、语言主导权与语篇功能之间的系统联系，以及巴拉圭文本中形式瓜拉尼语与非正式西班牙语之间明显的文体分割，这些结果以大规模语料支持并扩展了早期社会语言学观察。

Conclusion: 大型语言模型能够可靠地恢复传统上需要人工注释才能获得的可解释的社会语言学模式，推动了跨语言和低资源双语研究的计算方法发展。

Abstract: This study presents an LLM-assisted annotation pipeline for the sociolinguistic and topical analysis of bilingual discourse in two typologically distinct contexts: Spanish-English and Spanish-Guaraní. Using large language models, we automatically labeled topic, genre, and discourse-pragmatic functions across a total of 3,691 code-switched sentences, integrated demographic metadata from the Miami Bilingual Corpus, and enriched the Spanish-Guaraní dataset with new topic annotations. The resulting distributions reveal systematic links between gender, language dominance, and discourse function in the Miami data, and a clear diglossic division between formal Guaraní and informal Spanish in Paraguayan texts. These findings replicate and extend earlier interactional and sociolinguistic observations with corpus-scale quantitative evidence. The study demonstrates that large language models can reliably recover interpretable sociolinguistic patterns traditionally accessible only through manual annotation, advancing computational methods for cross-linguistic and low-resource bilingual research.

</details>


### [9] [PERCS: Persona-Guided Controllable Biomedical Summarization Dataset](https://arxiv.org/abs/2512.03340)
*Rohan Charudatt Salvi,Chirag Chawla,Dhruv Jain,Swapnil Panigrahi,Md Shad Akhtar,Shweta Yadav*

Main category: cs.CL

TL;DR: 该论文提出了一个针对四类不同医学素养用户的个性化医学摘要简化数据集PERCS，并进行了详细审校和多模型基准测试，为个性化可控医学文本简化研究提供了基础资源和实验结果。


<details>
  <summary>Details</summary>
Motivation: 现有医学文本简化资源通常只针对单一的通用读者，忽视了不同用户群体在医学素养和信息需求上的巨大差异。为更好地服务不同受众，亟需开发针对不同医学素养水平的个性化医学摘要生成方法。

Method: 提出了PERCS（Persona-guided Controllable Summarization）数据集，包含生物医学摘要及对应针对四类人格化读者（普通大众、医学预科学生、非医学研究者和医学专家）的个性化简化摘要。摘要由医生通过详细的错误分类系统进行事实准确性和人格化匹配的审核。同时使用四种大型语言模型进行基准测试，采用自动评估指标衡量摘要的全面性、可读性和准确性。

Result: 构建了首个支持多Persona的医学摘要简化数据集，验证了不同Persona的摘要在可读性、词汇和内容深度上的显著差异。基准测试为未来个性化医学文本简化研究提供了参考结果和公开资源。

Conclusion: 个性化医学文本简化能够更好地满足不同医学素养和信息需求的用户，有助于提升健康素养水平。PERCS数据集及其相关资源的开放将促进针对不同受众的可控生物医学文本简化技术的发展。

Abstract: Automatic medical text simplification plays a key role in improving health literacy by making complex biomedical research accessible to diverse readers. However, most existing resources assume a single generic audience, overlooking the wide variation in medical literacy and information needs across user groups. To address this limitation, we introduce PERCS (Persona-guided Controllable Summarization), a dataset of biomedical abstracts paired with summaries tailored to four personas: Laypersons, Premedical Students, Non-medical Researchers, and Medical Experts. These personas represent different levels of medical literacy and information needs, emphasizing the need for targeted, audience-specific summarization. Each summary in PERCS was reviewed by physicians for factual accuracy and persona alignment using a detailed error taxonomy. Technical validation shows clear differences in readability, vocabulary, and content depth across personas. Along with describing the dataset, we benchmark four large language models on PERCS using automatic evaluation metrics that assess comprehensiveness, readability, and faithfulness, establishing baseline results for future research. The dataset, annotation guidelines, and evaluation materials are publicly available to support research on persona-specific communication and controllable biomedical summarization.

</details>


### [10] [Idea-Gated Transformers: Enforcing Semantic Coherence via Differentiable Vocabulary Pruning](https://arxiv.org/abs/2512.03343)
*Darshan Fofadiya*

Main category: cs.CL

TL;DR: 提出Idea-Gated Transformer架构，通过语义门控机制减少语言模型的主题漂移，提高生成文本的领域一致性和可控性。


<details>
  <summary>Details</summary>
Motivation: 当前自回归语言模型因依赖局部关联而容易产生主题漂移，虽然扩大模型规模有所缓解，但根本问题仍未解决，因而需要一种能够将语义规划与句法生成分离，并有效控制生成过程的新方法。

Method: 设计一个辅助的“Idea Head”预测未来上下文的词袋分布，生成潜在“概念向量”，并通过可微的门控机制抑制语义无关的词汇，动态调整主词汇表的生成概率，从而提升语义一致性。

Result: 提出了一种名为Idea-Gated Transformer的新架构，通过引入辅助的“Idea Head”来预测未来上下文的词袋分布，生成“概念向量”以在生成过程中进行语义门控，从而减少生成文本的主题漂移。该方法在WikiText-103数据集上实验显示，尽管在验证困惑度上与标准GPT-2模型相当，但在领域保留方面表现显著提升。门控机制能有效抑制语义无关的词汇生成，使生成内容更集中于特定语义簇，提高了语言模型的可控性。

Conclusion: Idea-Gated Transformer通过分离语义规划和句法生成，并引入语义门控机制，显著提升了主题一致性和领域保留能力，为语言模型的可控生成提供了有效且参数高效的解决方案。

Abstract: Autoregressive Language Models (LLMs) trained on Next-Token Prediction (NTP) often suffer from ``Topic Drift'' where the generation wanders away from the initial prompt due to a reliance on local associations rather than global planning \citep{holtzman2019curious}. While scaling model size mitigates this \citep{brown2020language}, the fundamental myopia of the NTP objective remains. In this work, we introduce the Idea-Gated Transformer, a novel architecture that separates semantic planning from syntactic generation. We introduce an auxiliary ``Idea Head'' trained to predict the bag-of-words distribution for a future context window, creating a latent ``Concept Vector'' that actively gates the main vocabulary during generation. We propose a differentiable gating mechanism that suppresses semantically irrelevant tokens, effectively pruning the search space in real-time. Experiments on WikiText-103 demonstrate that while the Idea-Gated model achieves comparable validation perplexity to a standard GPT-2 baseline, it exhibits significantly superior Domain Retention. Qualitative and quantitative analysis reveals that the gating mechanism successfully locks generation into specific semantic clusters (e.g., Finance, Science) and resists associative drift, offering a parameter-efficient path toward more controllable language modeling.

</details>


### [11] [From Hypothesis to Premises: LLM-based Backward Logical Reasoning with Selective Symbolic Translation](https://arxiv.org/abs/2512.03360)
*Qingchuan Li,Mingyue Cheng,Zirui Liu,Daoyu Wang,Yuting Zeng,Tongxuan Liu*

Main category: cs.CL

TL;DR: 本文提出了一种结合高置信度符号翻译与假设驱动逆向推理的框架HBLR，显著提升了逻辑推理的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型多依赖正向推理，存在推理路径冗余、虚假步骤和语义漂移问题，导致推理效率低且不可靠。为此需要一种更高效、可靠的逻辑推理方法。

Method: 提出了结合置信度感知的符号翻译与假设驱动的逆向推理方法。翻译阶段只转换高置信度部分为逻辑形式，利用翻译反思模块确保语义保真；推理阶段模拟人类演绎推理，通过假设结论为真递归验证前提，并用推理反思模块纠正推理错误。

Result: HBLR在五个推理基准任务上均显著优于强基线方法，在准确率和效率方面表现突出。

Conclusion: 本文提出的Hypothesis-driven Backward Logical Reasoning (HBLR)框架，在逻辑推理准确性和效率上均优于现有方法，显示出更可靠的推理能力。

Abstract: Logical reasoning is a core challenge in natural language understanding and a fundamental capability of artificial intelligence, underpinning scientific discovery, mathematical theorem proving, and complex decision-making. Despite the remarkable progress of large language models (LLMs), most current approaches still rely on forward reasoning paradigms, generating step-by-step rationales from premises to conclusions. However, such methods often suffer from redundant inference paths, hallucinated steps, and semantic drift, resulting in inefficient and unreliable reasoning. In this paper, we propose a novel framework, Hypothesis-driven Backward Logical Reasoning (HBLR). The core idea is to integrate confidence-aware symbolic translation with hypothesis-driven backward reasoning. In the translation phase, only high-confidence spans are converted into logical form, such as First-Order Logic (FOL), while uncertain content remains in natural language. A translation reflection module further ensures semantic fidelity by evaluating symbolic outputs and reverting lossy ones back to text when necessary. In the reasoning phase, HBLR simulates human deductive thinking by assuming the conclusion is true and recursively verifying its premises. A reasoning reflection module further identifies and corrects flawed inference steps, enhancing logical coherence. Extensive experiments on five reasoning benchmarks demonstrate that HBLR consistently outperforms strong baselines in both accuracy and efficiency.

</details>


### [12] [Nexus: Higher-Order Attention Mechanisms in Transformers](https://arxiv.org/abs/2512.03377)
*Hanting Chen,Chu Zhong,Kai Han,Yuchuan Tian,Yuchen Liang,Tianyu Guo,Xinghao Chen,Dacheng Tao,Yunhe Wang*

Main category: cs.CL

TL;DR: 作者提出了一种新型的高阶注意力网络（Hon），通过递归嵌套的自注意力机制动态细化Query和Key的表示，突破了标准注意力的低秩瓶颈，从而更好地捕获复杂的多跳依赖关系。


<details>
  <summary>Details</summary>
Motivation: 标准的一阶注意力机制受限于低秩瓶颈，难以捕捉复杂的多层次关系，因此需要一种新的机制来增强表达能力。

Method: Hon采用递归框架，利用内层的自注意力循环动态生成Query和Key向量，并通过参数共享策略控制参数规模，实现更强的表达能力。

Result: Hon在多个基准测试中性能优于标准Transformer，展示了其提高建模复杂关联和全局上下文融合能力的有效性。

Conclusion: Hon通过递归的嵌套自注意力机制，有效提升了Transformer的表现能力，理论上突破了线性瓶颈，实验证明其在多个基准测试中优于标准Transformer。

Abstract: Transformers have achieved significant success across various domains, relying on self-attention to capture dependencies. However, the standard first-order attention mechanism is often limited by a low-rank bottleneck, struggling to capture intricate, multi-hop relationships within a single layer. In this paper, we propose the \textbf{Higher-Order Attention Network (Hon)}, a novel architecture designed to enhance representational power through a recursive framework. Unlike standard approaches that use static linear projections for Queries and Keys, Hon dynamically refines these representations via nested self-attention mechanisms. Specifically, the Query and Key vectors are themselves outputs of inner attention loops, allowing tokens to aggregate global context and model high-order correlations \textit{prior} to the final attention computation. We enforce a parameter-efficient weight-sharing strategy across recursive steps, ensuring that this enhanced expressivity incurs $\mathcal{O}(1)$ additional parameters. We provide theoretical analysis demonstrating that our method breaks the linear bottleneck of standard attention. Empirically, Hon outperforms standard Transformers on multiple benchmarks.

</details>


### [13] [Characterizing Language Use in a Collaborative Situated Game](https://arxiv.org/abs/2512.03381)
*Nicholas Tomlin,Naitian Zhou,Eve Fleisig,Liangyuan,Chen,Téa Wright,Lauren Vinh,Laura X. Ma,Seun Eisape,Ellie French,Tingting Du,Tianjiao Zhang,Alexander Koller,Alane Suhr*

Main category: cs.CL

TL;DR: 本文收集并发布了一个基于《传送门2》合作游戏的对话语料库，特色是包含复杂的空间引用和合作语言现象。


<details>
  <summary>Details</summary>
Motivation: 合作视频游戏中的语言表达丰富且复杂，现有语料库中很少涉及这类情境，故旨在填补这方面的数据空白。

Method: 通过收集玩家在《传送门2》合作模式中11.5小时的语音对话，进行语言和行为分析，并制作包含视频、音频、转录及游戏状态的多模态语料库。

Result: 构建了包含24.5K话语的对话语料库，发现了复杂空间参考、澄清修正以及即兴约定等语言现象，且公开发布了相关数据及标注。

Conclusion: 该语料库揭示了合作游戏中的语言多样性及其独特特点，并提供了详细的多模态数据支持未来研究。

Abstract: Cooperative video games, where multiple participants must coordinate by communicating and reasoning under uncertainty in complex environments, yield a rich source of language data. We collect the Portal Dialogue Corpus: a corpus of 11.5 hours of spoken human dialogue in the co-op mode of the popular Portal 2 virtual puzzle game, comprising 24.5K total utterances. We analyze player language and behavior, identifying a number of linguistic phenomena that rarely appear in most existing chitchat or task-oriented dialogue corpora, including complex spatial reference, clarification and repair, and ad-hoc convention formation. To support future analyses of language use in complex, situated, collaborative problem-solving scenarios, we publicly release the corpus, which comprises player videos, audio, transcripts, game state data, and both manual and automatic annotations of language data.

</details>


### [14] [Dual LoRA: Enhancing LoRA with Magnitude and Direction Updates](https://arxiv.org/abs/2512.03402)
*Yixing Xu,Chao Li,Xuanwu Yin,Spandan Tiwari,Dong Li,Ashish Sirasao,Emad Barsoum*

Main category: cs.CL

TL;DR: 本文提出Dual LoRA方法，通过结构化分组和非线性函数改进低秩适应，提高了大语言模型参数高效微调的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的低秩适应(LoRA)方法虽然参数高效，但由于低秩假设导致适应性能不理想。

Method: 提出Dual LoRA方法，将低秩矩阵分为幅值组和方向组，分别控制参数更新幅度和方向，通过ReLU和符号函数模拟基于梯度的参数更新过程。

Result: 在多个NLP任务和多种预训练模型上，Dual LoRA在相同可训练参数量下持续优于原始LoRA及其最新变体。

Conclusion: 通过引入归纳偏置，Dual LoRA有效提升了低秩适应方法的性能，证明了该方法在参数高效微调中的优势。

Abstract: Low-rank adaptation (LoRA) is one of the most popular methods among parameter-efficient fine-tuning (PEFT) methods to adapt pre-trained large language models (LLMs) to specific downstream tasks. However, the model trained based on LoRA often has an unsatisfactory performance due to its low-rank assumption. In this paper, we propose a novel method called Dual LoRA to improve the performance by incorporating an inductive bias into the original LoRA. Specifically, we separate low-rank matrices into two groups: the magnitude group to control whether or not and how far we should update a parameter and the direction group to decide whether this parameter should move forward or backward, to better simulate the parameter updating process of the full fine-tuning based on gradient-based optimization algorithms. We show that this can be simply achieved by adding a ReLU function to the magnitude group and a sign function to the direction group. We conduct several experiments over a wide range of NLP tasks, including natural language generation (NLG), understanding (NLU), and commonsense reasoning datasets on GPT-2, RoBERTa, DeBERTa, and LLaMA-1/2/3 as baseline models. The results show that we consistently outperform LoRA and its state-of-the-art variants with the same number of trainable parameters.

</details>


### [15] [PretrainZero: Reinforcement Active Pretraining](https://arxiv.org/abs/2512.03442)
*Xingrun Xing,Zhiyuan Fan,Jie Lou,Guoqi Li,Jiajun Zhang,Debing Zhang*

Main category: cs.CL

TL;DR: 本文提出PretrainZero框架，结合自监督强化学习，实现从领域特定的后训练向通用预训练的迁移，显著提升了模型的通用推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习模型虽在特定领域表现优异，但严重依赖验证奖励，限制了其在通用推理能力上的发挥，亟需从领域特定的强化学习向通用预训练过渡。

Method: PretrainZero设计了一个基于预训练语料库的强化主动学习框架，采用主动识别信息内容进行推理预测，通过自监督强化学习进行模型训练，突破了验证标签的依赖。

Result: PretrainZero在Qwen3-4B-Base模型上，在MMLU-Pro、SuperGPQA和数学平均基准上分别提升了8.43、5.96和10.60的分数，验证了框架的有效性。

Conclusion: PretrainZero通过主动预训练和自监督强化学习，成功突破了传统依赖验证标签的限制，显著提升了预训练模型的通用推理能力，并在多项基准测试中表现优异。

Abstract: Mimicking human behavior to actively learning from general experience and achieve artificial general intelligence has always been a human dream. Recent reinforcement learning (RL) based large-thinking models demonstrate impressive expert-level abilities, i.e., software and math, but still rely heavily on verifiable rewards in specific domains, placing a significant bottleneck to extend the performance boundary of general reasoning capabilities. In this work, we propose PretrainZero, a reinforcement active learning framework built on the pretraining corpus to extend RL from domain-specific post-training to general pretraining. PretrainZero features the following characteristics: 1) Active pretraining: inspired by the active learning ability of humans, PretrainZero learns a unified reasoning policy to actively identify reasonable and informative contents from pretraining corpus, and reason to predict these contents by RL. 2) Self-supervised learning: without any verifiable labels, pretrained reward models, or supervised fine-tuning, we directly pretrain reasoners from 3 to 30B base models on the general Wikipedia corpus using RL, significantly breaking the verification data-wall for general reasoning. 3) Verification scaling: by tackling increasingly challenging masked spans, PretrainZero substantially enhances the general reasoning abilities of pretrained base models. In reinforcement pretraining, PretrainZero improves Qwen3-4B-Base for 8.43, 5.96 and 10.60 on MMLU-Pro, SuperGPQA and math average benchmarks. In post-training, the pretrained models can also serve as reasoning foundation models for downstream RLVR tasks.

</details>


### [16] [A Preliminary Study on the Promises and Challenges of Native Top-$k$ Sparse Attention](https://arxiv.org/abs/2512.03494)
*Di Xiu,Hongyin Tang,Bolin Rong,Lizhi Yan,Jingang Wang,Yifan Lu,Xunliang Cai*

Main category: cs.CL

TL;DR: 本文探讨了Top-k注意力机制在大语言模型长上下文建模中的作用，重点验证了Top-k解码和训练的有效性，并分析了近似算法的精度对任务性能的影响。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理计算成本高，成为长上下文建模任务的瓶颈，探索高效且性能优良的Top-k注意力机制以提升推理效率。

Method: 通过实验证明了准确的Top-k解码在多个下游任务上的有效性，研究了训练中应用Top-k注意力策略的影响，并分析了近似Top-k算法的性能波动。

Result: Top-k注意力机制在解码中可达到或超越全注意力性能；训练阶段保证一致性可进一步释放潜力；近似Top-k算法精度与任务表现正相关；低熵现象支持理论假设。

Conclusion: 确保训练和推理阶段的一致Top-k注意力操作能够显著提升模型性能；低熵状态有利于Top-k解码效果。

Abstract: Large Language Models (LLMs) are increasingly prevalent in the field of long-context modeling, however, their inference computational costs have become a critical bottleneck hindering the advancement of tasks such as agents and multimodal applications. This report conducts a preliminary investigation into the effectiveness and theoretical mechanisms of the Top-$k$ Attention mechanism during both the decoding and training phases. First, we validate the effectiveness of exact Top-$k$ Decoding through extensive experimentation. Experiments demonstrate that retaining only the pivotal Keys with the highest similarity to the Query as the context window during the decoding stage achieves performance comparable to, or even surpassing, full attention on downstream tasks such as HELMET and LongBench v2. Second, we further explore the native Top-$k$ Attention training strategy. Experiments confirm that ensuring the consistency between training and inference regarding Top-$k$ Attention operations facilitates the further unlocking of Top-$k$ Decoding's potential, thereby significantly enhancing model performance. Furthermore, considering the high computational complexity of exact Top-$k$ Attention, we investigate the impact of approximate Top-$k$ algorithm precision on downstream tasks. Our research confirms a positive correlation between downstream task performance and approximation fidelity, and we provide statistical evaluations of the Lightning Indexer's precision within the DeepSeek-V3.2-Exp model. Finally, this report provides a theoretical interpretation from the perspective of Entropy. Experimental observations indicate that models subjected to Top-$k$ Attention SFT exhibit a distinct phenomenon of entropy reduction in downstream tasks, which validates the hypothesis that low-entropy states are better adapted to Top-$k$ Decoding.

</details>


### [17] [Understanding LLM Reasoning for Abstractive Summarization](https://arxiv.org/abs/2512.03503)
*Haohan Yuan,Siu Cheung Hui,Haopeng Zhang*

Main category: cs.CL

TL;DR: 研究发现大型语言模型推理对抽象摘要效果影响复杂，显式与隐式推理各有利弊，简单增加推理量反而可能损害摘要事实准确性。摘要任务需注重信息忠实压缩。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在数学和代码生成等分析任务上推理能力优异，但对抽象摘要任务的有效性尚缺乏验证。

Method: 将通用推理策略调整应用于摘要领域，设计并开展对8种推理策略和3个大型推理模型在8个不同数据集上的系统大规模比较实验。

Result: 推理策略的效果依赖具体策略和上下文，不具普适性。显式推理提升摘要流畅性但可能降低事实准确性；隐式推理则相反。增加模型内部推理预算并未提升，反而可能降低事实一致性。

Conclusion: 抽象摘要任务中，推理不是万能的，需在流畅性与事实准确性之间权衡。有效摘要依赖忠实压缩而非过度复杂推理。

Abstract: While the reasoning capabilities of Large Language Models (LLMs) excel in analytical tasks such as mathematics and code generation, their utility for abstractive summarization remains widely assumed but largely unverified. To bridge this gap, we first tailor general reasoning strategies to the summarization domain. We then conduct a systematic, large scale comparative study of 8 reasoning strategies and 3 Large Reasoning Models (LRMs) across 8 diverse datasets, assessing both summary quality and faithfulness. Our findings show that reasoning is not a universal solution and its effectiveness is highly dependent on the specific strategy and context. Specifically, we observe a trade-off between summary quality and factual faithfulness: explicit reasoning strategies tend to improve fluency at the expense of factual grounding, while implicit reasoning in LRMs exhibits the inverse pattern. Furthermore, increasing an LRM's internal reasoning budget does not improve, and can even hurt, factual consistency, suggesting that effective summarization demands faithful compression rather than creative over-thinking.

</details>


### [18] [Fine-grained Narrative Classification in Biased News Articles](https://arxiv.org/abs/2512.03582)
*Zeba Afroz,Harsh Vardhan,Pawan Bhakuni,Aanchal Punia,Rajdeep Kumar,Md. Shad Akhtar*

Main category: cs.CL

TL;DR: 本文建立了印度新闻中的细粒度叙事标注数据集INDI-PROP，设计了两种基于GPT的推理框架FANTA和TPTC，实现了对偏见、叙事框架和说服技巧的精准分类。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏细粒度的叙事分类方法来分析带有偏见的新闻文章，尤其是在印度新闻媒体中的意识形态宣传。通过细粒度的多层次标注，可以更深入理解宣传中的叙事结构和说服技巧。

Method: 构建多层级标注体系，包括文章偏见、事件特定的细粒度叙事框架和说服技巧的注释；利用GPT-4o-mini设计两种多跳推理模型FANTA和TPTC，分别聚焦层次性沟通现象整合和说服提示的系统分解，实现分类任务。

Result: 构建了包含1266篇关于两个极化政治事件（CAA和农民抗议）的细粒度叙事数据集，开发的FANTA和TPTC模型在偏见、叙事和说服技巧分类任务中均显著优于基线模型。

Conclusion: 采用基于多层次沟通现象和分解说服线索的GPT驱动推理方法，能够有效提升偏见、叙事及说服技巧的识别准确率，为分析印度新闻媒体中的意识形态宣传提供了有效工具。

Abstract: Narratives are the cognitive and emotional scaffolds of propaganda. They organize isolated persuasive techniques into coherent stories that justify actions, attribute blame, and evoke identification with ideological camps. In this paper, we propose a novel fine-grained narrative classification in biased news articles. We also explore article-bias classification as the precursor task to narrative classification and fine-grained persuasive technique identification. We develop INDI-PROP, the first ideologically grounded fine-grained narrative dataset with multi-level annotation for analyzing propaganda in Indian news media. Our dataset INDI-PROP comprises 1,266 articles focusing on two polarizing socio-political events in recent times: CAA and the Farmers' protest. Each article is annotated at three hierarchical levels: (i) ideological article-bias (pro-government, pro-opposition, neutral), (ii) event-specific fine-grained narrative frames anchored in ideological polarity and communicative intent, and (iii) persuasive techniques. We propose FANTA and TPTC, two GPT-4o-mini guided multi-hop prompt-based reasoning frameworks for the bias, narrative, and persuasive technique classification. FANTA leverages multi-layered communicative phenomena by integrating information extraction and contextual framing for hierarchical reasoning. On the other hand, TPTC adopts systematic decomposition of persuasive cues via a two-stage approach. Our evaluation suggests substantial improvement over underlying baselines in each case.

</details>


### [19] [AlignCheck: a Semantic Open-Domain Metric for Factual Consistency Assessment](https://arxiv.org/abs/2512.03634)
*Ahmad Aghaebrahimian*

Main category: cs.CL

TL;DR: 本文提出了一种灵活且可解释的事实一致性评估框架，通过加权指标和复杂度控制机制提升了自然语言处理中事实评估的准确性和可解释性，特别适用于临床等高风险领域。


<details>
  <summary>Details</summary>
Motivation: 当前大规模语言模型易于产生虚假但似乎合理的论述，特别是在临床等高风险领域，这种错误可能带来严重后果，现有评估方法不足以有效诊断和减轻此类错误。

Method: 采用文本原子事实分解技术，提出无需预设模式的灵活方法，结合加权指标提升事实评估准确性，并设计了复杂领域评估复杂度控制机制。

Result: 在通用及临床数据集上进行基准测试，证明了所提方法的有效性，并开放代码以支持未来基于事实感知的模型训练。

Conclusion: 本论文提出了一种可解释的事实一致性评估框架，解决了现有评估指标对事实一致性评估不足及解释性差的问题，提高了复杂领域文本的评估效果。

Abstract: Large Language Models have significantly advanced natural language processing tasks, but remain prone to generating incorrect or misleading but plausible arguments. This issue, known as hallucination, is particularly concerning in high-stakes domains like clinical applications, where factual inaccuracies can have severe consequences. Existing evaluation metrics fail to adequately assess factual consistency and lack interpretability, making diagnosing and mitigating errors difficult. We propose an interpretable framework for factual consistency assessment for in-domain and open-domain texts to address these limitations. Our approach decomposes text into atomic facts and introduces a flexible, schema-free methodology. Unlike previous methods with an absolute metric, we incorporate a weighted metric to enhance factual evaluation. Additionally, we propose a mechanism to control assessment complexity in intricate domains. We benchmark our approach on popular general and clinical datasets and release our code to support fact-aware model training in future research.

</details>


### [20] [Generative AI Practices, Literacy, and Divides: An Empirical Analysis in the Italian Context](https://arxiv.org/abs/2512.03671)
*Beatrice Savoldi,Giuseppe Attanasio,Olga Gorodetskaya,Marta Marchiori Manerba,Elisa Bassignana,Silvia Casola,Matteo Negri,Tommaso Caselli,Luisa Bentivogli,Alan Ramponi,Arianna Muti,Nicoletta Balbo,Debora Nozza*

Main category: cs.CL

TL;DR: 本文基于对意大利1906名成年人的调查，首次全面描绘了生成式人工智能（GenAI）的采用、使用模式和数字素养状况。研究显示GenAI在工作和个人生活中得到广泛应用，但用户数字素养较低，存在误判信息的风险。性别差异显著，女性特别是老年女性的采纳率和使用频率仅为男性的一半，素养虽是重要因素，但无法完全解释这一差距。需要有针对性的教育措施及进一步研究其他障碍。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式AI技术潜力巨大，但因用户认知和素养差异，可能加剧数字鸿沟，需深入了解其采用和使用状况及存在的障碍。

Method: 通过对1906名意大利语成年人的新调查数据进行实证分析，全面描绘GenAI的采用、使用模式及数字素养。

Result: 发现GenAI广泛应用于工作和私人领域，包括情感支持和医疗建议；数字素养低下导致用户难以识别错误信息；存在显著性别差异，教育和素养仅部分解释差距。

Conclusion: GenAI在意大利被广泛采用并成为主要信息来源，但用户数字素养不足带来风险；性别差异明显，素养无法完全解释，需针对性策略促进公平参与。

Abstract: The rise of Artificial Intelligence (AI) language technologies, particularly generative AI (GenAI) chatbots accessible via conversational interfaces, is transforming digital interactions. While these tools hold societal promise, they also risk widening digital divides due to uneven adoption and low awareness of their limitations. This study presents the first comprehensive empirical mapping of GenAI adoption, usage patterns, and literacy in Italy, based on newly collected survey data from 1,906 Italian-speaking adults. Our findings reveal widespread adoption for both work and personal use, including sensitive tasks like emotional support and medical advice. Crucially, GenAI is supplanting other technologies to become a primary information source: this trend persists despite low user digital literacy, posing a risk as users struggle to recognize errors or misinformation. Moreover, we identify a significant gender divide -- particularly pronounced in older generations -- where women are half as likely to adopt GenAI and use it less frequently than men. While we find literacy to be a key predictor of adoption, it only partially explains this disparity, suggesting that other barriers are at play. Overall, our data provide granular insights into the multipurpose usage of GenAI, highlighting the dual need for targeted educational initiatives and further investigation into the underlying barriers to equitable participation that competence alone cannot explain.

</details>


### [21] [Evaluating Hydro-Science and Engineering Knowledge of Large Language Models](https://arxiv.org/abs/2512.03672)
*Shiruo Hu,Wenbo Shan,Yingjia Li,Zhiqi Wan,Xinpeng Yu,Yunjia Qi,Haotian Xia,Yang Xiao,Dingxiao Liu,Jiaru Wang,Chenxu Gong,Ruixi Zhang,Shuyue Wu,Shibo Cui,Chee Hui Lai,Wei Luo,Yubin He,Bin Xu,Jianshi Zhao*

Main category: cs.CL

TL;DR: 本文构建了水利科学与工程领域的大型语言模型评测基准，评估了其知识和应用能力，指出模型现阶段优势与不足。


<details>
  <summary>Details</summary>
Motivation: 水利科学与工程领域多学科融合，决策复杂，亟需智能辅助。现有大型语言模型在此领域的知识和应用能力尚未充分评估，因此设计了系统评测基准以填补此空缺。

Method: 构建了包含4000题的多选题评测基准（Hydro-SE Bench），覆盖九个子领域，测试LLMs在基础知识、工程应用和推理计算能力上的表现。

Result: 本文提出了Hydro-SE LLM评测基准（Hydro-SE Bench），包含4000道多项选择题，涵盖九个子领域，对LLMs的基础概念知识、工程应用能力及推理计算能力进行评估。评测显示商业LLMs准确率在0.74至0.80之间，小规模LLMs在0.41至0.68之间。LLMs在自然与物理科学相关子领域表现较好，但在行业标准和水工结构等专业知识方面表现较弱。模型规模提升主要增强了推理和计算能力，实际工程应用能力仍有提升空间。该研究明确了LLMs在水利科学与工程领域的优势与不足，为模型开发提供训练目标，也为领域研究者应用LLMs提供指导。

Conclusion: LLMs在水利科学与工程领域表现参差，尤其在专业知识及工程应用方面仍需提升，模型规模扩大可增强推理能力。该评测基准为模型优化和实际应用提供了依据。

Abstract: Hydro-Science and Engineering (Hydro-SE) is a critical and irreplaceable domain that secures human water supply, generates clean hydropower energy, and mitigates flood and drought disasters. Featuring multiple engineering objectives, Hydro-SE is an inherently interdisciplinary domain that integrates scientific knowledge with engineering expertise. This integration necessitates extensive expert collaboration in decision-making, which poses difficulties for intelligence. With the rapid advancement of large language models (LLMs), their potential application in the Hydro-SE domain is being increasingly explored. However, the knowledge and application abilities of LLMs in Hydro-SE have not been sufficiently evaluated. To address this issue, we propose the Hydro-SE LLM evaluation benchmark (Hydro-SE Bench), which contains 4,000 multiple-choice questions. Hydro-SE Bench covers nine subfields and enables evaluation of LLMs in aspects of basic conceptual knowledge, engineering application ability, and reasoning and calculation ability. The evaluation results on Hydro-SE Bench show that the accuracy values vary among 0.74 to 0.80 for commercial LLMs, and among 0.41 to 0.68 for small-parameter LLMs. While LLMs perform well in subfields closely related to natural and physical sciences, they struggle with domain-specific knowledge such as industry standards and hydraulic structures. Model scaling mainly improves reasoning and calculation abilities, but there is still great potential for LLMs to better handle problems in practical engineering application. This study highlights the strengths and weaknesses of LLMs for Hydro-SE tasks, providing model developers with clear training targets and Hydro-SE researchers with practical guidance for applying LLMs.

</details>


### [22] [Different types of syntactic agreement recruit the same units within large language models](https://arxiv.org/abs/2512.03676)
*Daria Kryvosheieva,Andrea de Varda,Evelina Fedorenko,Greta Tuckute*

Main category: cs.CL

TL;DR: 通过功能定位方法揭示大语言模型对句法一致性的内部表示共享机制，体现了句法依赖在模型语义空间中的重要地位。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型中句法知识的表示方式，尤其不同句法现象是否共享或者区分不同的模型组件。

Method: 采用认知神经科学启发的功能定位方法，识别七个开源大语言模型中对67种英语句法现象反应最强的单元，并分析其在句法理解中的作用。

Result: 发现不同类型的句法一致性（如主谓、一致代词、限定词-名词）会招募重叠的模型单元，表明一致性构成了模型中的重要功能类别。该规律在英语、俄语、中文以及跨语言分析的57种语言中均成立，语言结构相似度越高，主谓一致的共享单元越多。

Conclusion: 句法一致性作为句法依赖的关键标志，在大语言模型的表示空间中形成了具有意义的类别，体现了模型对语言结构的深层理解。

Abstract: Large language models (LLMs) can reliably distinguish grammatical from ungrammatical sentences, but how grammatical knowledge is represented within the models remains an open question. We investigate whether different syntactic phenomena recruit shared or distinct components in LLMs. Using a functional localization approach inspired by cognitive neuroscience, we identify the LLM units most responsive to 67 English syntactic phenomena in seven open-weight models. These units are consistently recruited across sentences containing the phenomena and causally support the models' syntactic performance. Critically, different types of syntactic agreement (e.g., subject-verb, anaphor, determiner-noun) recruit overlapping sets of units, suggesting that agreement constitutes a meaningful functional category for LLMs. This pattern holds in English, Russian, and Chinese; and further, in a cross-lingual analysis of 57 diverse languages, structurally more similar languages share more units for subject-verb agreement. Taken together, these findings reveal that syntactic agreement-a critical marker of syntactic dependencies-constitutes a meaningful category within LLMs' representational spaces.

</details>


### [23] [AITutor-EvalKit: Exploring the Capabilities of AI Tutors](https://arxiv.org/abs/2512.03688)
*Numaan Naeem,Kaushal Kumar Maurya,Kseniia Petukhova,Ekaterina Kochmar*

Main category: cs.CL

TL;DR: AITutor-EvalKit利用语言技术评估AI导师教学效果，并提供模型检查和数据可视化，方便教育界和ACL社区使用。


<details>
  <summary>Details</summary>
Motivation: 提升AI导师的教学质量评估方法，满足教育利益相关者和ACL社区的需求，同时支持学习和用户反馈收集。

Method: 通过引入语言技术，结合软件演示和评估手段，以及模型检查和数据可视化功能实现教学质量评估。

Result: 成功开发出该工具，能有效评估AI导师教学质量，并支持相关数据的分析和用户交互。

Conclusion: AITutor-EvalKit是一款利用语言技术评估AI导师教学质量的工具，支持示范、评估、模型检查和数据可视化。

Abstract: We present AITutor-EvalKit, an application that uses language technology to evaluate the pedagogical quality of AI tutors, provides software for demonstration and evaluation, as well as model inspection and data visualization. This tool is aimed at education stakeholders as well as *ACL community at large, as it supports learning and can also be used to collect user feedback and annotations.

</details>


### [24] [DZ-TDPO: Non-Destructive Temporal Alignment for Mutable State Tracking in Long-Context Dialogue](https://arxiv.org/abs/2512.03704)
*Yijun Liao*

Main category: cs.CL

TL;DR: 本文提出DZ-TDPO，通过动态KL约束和时间注意力偏置解决长上下文对话中的状态惯性问题，显著提升对话模型对齐效果且保持通用能力。


<details>
  <summary>Details</summary>
Motivation: 长上下文对话系统存在状态惯性问题，导致模型难以在不断变化的用户意图和既定历史上下文之间解决冲突。

Method: 提出了DZ-TDPO，一种结合冲突感知动态KL约束与可学习时间注意力偏置的非破坏性对齐框架。

Result: 在多会话聊天（MSC）数据集上，DZ-TDPO达到了86.2%的领先胜率，并保持了强的零样本泛化能力。大模型在该方法下实现了近乎完美的对齐（99.4%胜率），且困惑度几乎没有增加。

Conclusion: 通过精确调节注意力权重而非破坏性权重更新，可以缓解状态惯性问题，保持模型的通用能力，并且小模型和大模型在对齐和稳定性上存在权衡关系。

Abstract: Long-context dialogue systems suffer from State Inertia, where static constraints prevent models from resolving conflicts between evolving user intents and established historical context. To address this, we propose DZ-TDPO, a non-destructive alignment framework that synergizes conflict-aware dynamic KL constraints with a learnable temporal attention bias. Experiments on the Multi-Session Chat (MSC) dataset demonstrate that DZ-TDPO achieves state-of-the-art win rates (86.2% on Phi-3.5) while maintaining robust zero-shot generalization. Crucially, our scaling analysis reveals a "Capacity-Stability Trade-off": while smaller models incur an "alignment tax" (perplexity surge) to overcome historical inertia, the larger Qwen2.5-7B model achieves near-perfect alignment (99.4% win rate) with negligible perplexity overhead. This confirms that TAI can be alleviated via precise attention regulation rather than destructive weight updates, preserving general capabilities (MMLU) across model scales. Code and data are available: https://github.com/lyj20071013/DZ-TDPO

</details>


### [25] [AR-Med: Automated Relevance Enhancement in Medical Search via LLM-Driven Information Augmentation](https://arxiv.org/abs/2512.03737)
*Chuyue Wang,Jie Feng,Yuxi Wu,Hang Zhang,Zhiguo Fan,Bing Cheng,Wei Lin*

Main category: cs.CL

TL;DR: 针对医疗在线搜索复杂性和高风险，AR-Med结合验证医学知识和大语言模型，设计了检索增强和知识蒸馏策略，显著提升搜索准确率和用户体验，成功实现大规模应用。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以理解复杂的用户查询且效果有限，面对医疗领域的高风险和专业需求，大语言模型虽具潜力但存在幻觉、知识空白及高成本等挑战，迫切需要一个准确、可靠且可扩展的医疗搜索方案。

Method: 采用检索增强的方式结合大语言模型推理，并设计了知识蒸馏机制将大模型压缩为高效且强大的学生模型，同时构建了多专家注释的基准LocalQSMed用于模型迭代和性能保证。

Result: AR-Med实现了离线准确率超过93%，比原有线上系统提升24%，同时在线相关性和用户满意度也显著提升。

Conclusion: AR-Med框架通过结合验证过的医学知识和大语言模型，实现了在线医疗搜索的高准确率和高可靠性，显著提升了系统性能和用户满意度，证明了基于大语言模型的医疗搜索解决方案的可行性和实用性。

Abstract: Accurate and reliable search on online healthcare platforms is critical for user safety and service efficacy. Traditional methods, however, often fail to comprehend complex and nuanced user queries, limiting their effectiveness. Large language models (LLMs) present a promising solution, offering powerful semantic understanding to bridge this gap. Despite their potential, deploying LLMs in this high-stakes domain is fraught with challenges, including factual hallucinations, specialized knowledge gaps, and high operational costs. To overcome these barriers, we introduce \textbf{AR-Med}, a novel framework for \textbf{A}utomated \textbf{R}elevance assessment for \textbf{Med}ical search that has been successfully deployed at scale on the Online Medical Delivery Platforms. AR-Med grounds LLM reasoning in verified medical knowledge through a retrieval-augmented approach, ensuring high accuracy and reliability. To enable efficient online service, we design a practical knowledge distillation scheme that compresses large teacher models into compact yet powerful student models. We also introduce LocalQSMed, a multi-expert annotated benchmark developed to guide model iteration and ensure strong alignment between offline and online performance. Extensive experiments show AR-Med achieves an offline accuracy of over 93\%, a 24\% absolute improvement over the original online system, and delivers significant gains in online relevance and user satisfaction. Our work presents a practical and scalable blueprint for developing trustworthy, LLM-powered systems in real-world healthcare applications.

</details>


### [26] [Principled RL for Diffusion LLMs Emerges from a Sequence-Level Perspective](https://arxiv.org/abs/2512.03759)
*Jingyang Ou,Jiaqi Han,Minkai Xu,Shaoxuan Xu,Jianwen Xie,Stefano Ermon,Yi Wu,Chongxuan Li*

Main category: cs.CL

TL;DR: 提出ESPO方法解决扩散大语言模型强化学习中逐token概率缺失问题，通过序列级策略优化大幅提升任务性能。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型缺乏token级条件概率，导致传统基于token的强化学习方法难以直接应用，亟需设计能处理序列级生成的新型RL框架。

Method: 基于ELBO的序列级策略优化，将整个生成序列作为单一动作，结合每token归一化重要性比率和稳健的KL散度估计，支持稳定的大规模训练。

Result: 本文针对扩散大语言模型（dLLMs）在强化学习（RL）中面临的核心问题——缺乏逐token的条件概率分解，提出了基于ELBO的序列级策略优化方法（ESPO）。该方法将整个序列生成视作单一动作，利用ELBO作为序列似然的代理，结合每token的重要性比率归一化和稳定的KL散度估计，保证了大规模训练的稳定性。实验证明，ESPO在数学推理、编程和规划任务上显著优于基于token的强化学习方法，在Countdown任务上提升20-40分，且在其他基准任务表现稳定。

Conclusion: ESPO将序列级优化视为强化学习的有效范式，显著提升了dLLMs在多项任务的表现，验证了方法的实用性和有效性。

Abstract: Reinforcement Learning (RL) has proven highly effective for autoregressive language models, but adapting these methods to diffusion large language models (dLLMs) presents fundamental challenges. The core difficulty lies in likelihood approximation: while autoregressive models naturally provide token-level conditional probabilities essential for token-level RL objectives (e.g., GRPO), dLLMs generate sequences through iterative non-autoregressive denoising steps that lack this factorization. To address this fundamental mismatch, we propose ELBO-based Sequence-level Policy Optimization (ESPO), a principled RL framework that treats entire sequence generation as a single action and uses the ELBO as a tractable sequence-level likelihood proxy. Our method incorporates per-token normalization of importance ratios and robust KL-divergence estimation to ensure stable large-scale training. Extensive experiments on mathematical reasoning, coding, and planning tasks demonstrate that ESPO significantly outperforms token-level baselines, achieving dramatic improvements of 20-40 points on the Countdown task, while maintaining consistent gains on math and coding benchmarks. Our approach establishes sequence-level optimization as a principled and empirically effective paradigm for RL in dLLMs. Our code is available at https://github.com/ML-GSAI/ESPO.

</details>


### [27] [In-Context Representation Hijacking](https://arxiv.org/abs/2512.03771)
*Itay Yona,Amir Sarid,Michael Karasik,Yossi Gandelsman*

Main category: cs.CL

TL;DR: Doublespeak攻击通过替换上下文中关键字，诱导模型内部语义重写，从而绕过安全对齐，暴露了LLMs潜在的新威胁。


<details>
  <summary>Details</summary>
Motivation: 揭示大语言模型（LLMs）当前安全对齐机制的不足和潜在攻击面。

Method: 通过多次上下文例子中将有害关键词系统性地替换为无害词汇，进行“上下文表示劫持”的攻击。使用可解释性工具分析语义覆盖在模型层层内的变化。

Result: 提出的Doublespeak攻击能成功将无害词内部语义转变为有害语义，绕过模型的安全限制，在多个模型上表现出高成功率（如Llama-3.3-70B-Instruct达到74%的攻击成功率）。

Conclusion: 当前对齐策略不足以防范表示层面的攻击，应在表示层面优化安全防护措施以防止语义劫持。

Abstract: We introduce \textbf{Doublespeak}, a simple \emph{in-context representation hijacking} attack against large language models (LLMs). The attack works by systematically replacing a harmful keyword (e.g., \textit{bomb}) with a benign token (e.g., \textit{carrot}) across multiple in-context examples, provided a prefix to a harmful request. We demonstrate that this substitution leads to the internal representation of the benign token converging toward that of the harmful one, effectively embedding the harmful semantics under a euphemism. As a result, superficially innocuous prompts (e.g., ``How to build a carrot?'') are internally interpreted as disallowed instructions (e.g., ``How to build a bomb?''), thereby bypassing the model's safety alignment. We use interpretability tools to show that this semantic overwrite emerges layer by layer, with benign meanings in early layers converging into harmful semantics in later ones. Doublespeak is optimization-free, broadly transferable across model families, and achieves strong success rates on closed-source and open-source systems, reaching 74\% ASR on Llama-3.3-70B-Instruct with a single-sentence context override. Our findings highlight a new attack surface in the latent space of LLMs, revealing that current alignment strategies are insufficient and should instead operate at the representation level.

</details>


### [28] [Enhancing Instruction-Following Capabilities in Seq2Seq Models: DoLA Adaptations for T5](https://arxiv.org/abs/2512.03803)
*Huey Sun,Anabel Yong,Lorenzo Gilly,Felipe Jin*

Main category: cs.CL

TL;DR: 本工作将对比解码算法DoLa首次引入编码器-解码器架构T5系列，评估其对模型指令执行与生成文本质量的影响，结果显示该方法具有选择性提升效果，并通过logit层析分析解释了其机制。


<details>
  <summary>Details</summary>
Motivation: 当前对比解码技术主要应用于decoder-only模型，且多关注于提升文本真实性，本文希望探索其在编码器-解码器架构及指令遵循能力上的效果。

Method: 将DoLa算法从decoder-only架构扩展到T5和FLAN-T5模型，并通过层间logit变化分析，评估其对指令遵循能力和生成文本可信度的影响。

Result: 实验表明，DoLa在提升某些任务文本忠实度方面有效，但也可能对其他任务产生负面影响，通过细致的logit演变分析揭示了该现象的内在机理。

Conclusion: 本文首次将对比解码策略DoLa应用于编码器-解码器架构的T5和FLAN-T5模型，发现该方法在某些任务类别上提升了文本生成的忠实度，但在其他任务上效果不佳。

Abstract: Contrastive decoding is a lightweight and effective inference-time method that improves the quality of text generation in Large Language Models. However, algorithms such as DoLa (Decoding by Contrastive Layers) have only been implemented in decoder-only architectures and studied for their impact on improving factuality. This work adapts DoLa for the T5 and FLAN-T5 model families and evaluates its impact on the models' instruction following capabilities, which to our knowledge is the first implementation of a contrastive decoding strategy in an encoder-decoder architecture. Our results show that DoLa improves the faithfulness of text generation for certain categories of tasks and harms others. To understand these results, we present a layer-by-layer analysis of logit evolution in a FLAN-T5 model to quantify DoLa's impact on token output probabilities.

</details>


### [29] [Improving Alignment Between Human and Machine Codes: An Empirical Assessment of Prompt Engineering for Construct Identification in Psychology](https://arxiv.org/abs/2512.03818)
*Kylie L. Anglin,Stephanie Milan,Brittney Hernandez,Claudia Ventura*

Main category: cs.CL

TL;DR: 本文针对心理学领域的文本分类，提出并验证了通过结合编码本指导和自动提示工程的提示优化框架，显著提升大型语言模型的分类准确性，确保结果符合专家标准。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在文本分类任务中表现优秀，但其输出高度依赖提示词的措辞。在心理学等领域，分类对象有严谨的理论定义且训练数据中可能未充分体现，缺乏对提示优化的研究，促使本文开展相关探索。

Method: 本文设计了一个实证框架，通过五种提示策略（编码本指导的经验提示选择、自动提示工程、角色提示、链式思维推理和解释性提示）进行零样本和少样本分类实验，评估不同策略对大型语言模型性能的影响。

Result: 实验发现角色提示、链式思维和解释性提示无法完全缓解提示措辞不当带来的性能下降。构造定义、任务框架及示例是影响分类效果的关键因素。结合编码本指导和自动提示工程的少样本提示实现了与专家判断最一致的分类。

Conclusion: 在心理学等专业领域，利用大型语言模型（LLMs）进行分类任务时，提示词的措辞对模型输出影响显著。通过结合编码本指导的经验提示选择与自动提示工程，可以显著提升模型分类性能，使其结果更符合专家判断。

Abstract: Due to their architecture and vast pre-training data, large language models (LLMs) demonstrate strong text classification performance. However, LLM output - here, the category assigned to a text - depends heavily on the wording of the prompt. While literature on prompt engineering is expanding, few studies focus on classification tasks, and even fewer address domains like psychology, where constructs have precise, theory-driven definitions that may not be well represented in pre-training data. We present an empirical framework for optimizing LLM performance for identifying constructs in texts via prompt engineering. We experimentally evaluate five prompting strategies --codebook-guided empirical prompt selection, automatic prompt engineering, persona prompting, chain-of-thought reasoning, and explanatory prompting - with zero-shot and few-shot classification. We find that persona, chain-of-thought, and explanations do not fully address performance loss accompanying a badly worded prompt. Instead, the most influential features of a prompt are the construct definition, task framing, and, to a lesser extent, the examples provided. Across three constructs and two models, the classifications most aligned with expert judgments resulted from a few-shot prompt combining codebook-guided empirical prompt selection with automatic prompt engineering. Based on our findings, we recommend that researchers generate and evaluate as many prompt variants as feasible, whether human-crafted, automatically generated, or ideally both, and select prompts and examples based on empirical performance in a training dataset, validating the final approach in a holdout set. This procedure offers a practical, systematic, and theory-driven method for optimizing LLM prompts in settings where alignment with expert judgment is critical.

</details>


### [30] [Training and Evaluation of Guideline-Based Medical Reasoning in LLMs](https://arxiv.org/abs/2512.03838)
*Michael Staniek,Artem Sokolov,Stefan Riezler*

Main category: cs.CL

TL;DR: 本文提出通过微调大语言模型（LLMs）使其遵循医学共识指南的推理步骤，从而增强医学早期预测的可信解释能力，并结合时间序列预测模型改善未来临床数据的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 医学早期预测准确性提升的同时缺乏可信的解释，影响医学从业者的信任，故希望通过教学LLM遵循共识推理步骤提升解释能力。

Method: 通过将医学共识指南规则口头化并实例化于电子健康记录进行微调，训练小型LLM学习共识规则和例外情况，并结合时间序列预测模型实现多模态输入。

Result: 微调的小型LLM在推理正确性和预测性能上均优于仅提示大模型的单次学习和基于医学文本训练的模型，多模态方法进一步提升了未来临床数据预测的泛化能力。

Conclusion: 在特定医疗领域微调的LLM能够实现几乎完美的规则推理正确性，优于仅依靠大模型的单次学习，但临床预测的瓶颈在于未来数据的稀疏不规则采样，通过多模态融合时间序列预测可进一步提升性能。

Abstract: Machine learning for early prediction in medicine has recently shown breakthrough performance, however, the focus on improving prediction accuracy has led to a neglect of faithful explanations that are required to gain the trust of medical practitioners. The goal of this paper is to teach LLMs to follow medical consensus guidelines step-by-step in their reasoning and prediction process. Since consensus guidelines are ubiquitous in medicine, instantiations of verbalized medical inference rules to electronic health records provide data for fine-tuning LLMs to learn consensus rules and possible exceptions thereof for many medical areas. Consensus rules also enable an automatic evaluation of the model's inference process regarding its derivation correctness (evaluating correct and faithful deduction of a conclusion from given premises) and value correctness (comparing predicted values against real-world measurements). We exemplify our work using the complex Sepsis-3 consensus definition. Our experiments show that small fine-tuned models outperform one-shot learning of considerably larger LLMs that are prompted with the explicit definition and models that are trained on medical texts including consensus definitions. Since fine-tuning on verbalized rule instantiations of a specific medical area yields nearly perfect derivation correctness for rules (and exceptions) on unseen patient data in that area, the bottleneck for early prediction is not out-of-distribution generalization, but the orthogonal problem of generalization into the future by forecasting sparsely and irregularly sampled clinical variables. We show that the latter results can be improved by integrating the output representations of a time series forecasting model with the LLM in a multimodal setup.

</details>


### [31] [Reconstructing KV Caches with Cross-layer Fusion For Enhanced Transformers](https://arxiv.org/abs/2512.03870)
*Hongzhan Lin,Zhiqi Bai,Xinmiao Zhang,Sen Yang,Xiang Li,Siran Yang,Yunlong Xu,Jiaheng Liu,Yongchi Zhao,Jiamang Wang,Yuchi Xu,Wenbo Su,Bo Zheng*

Main category: cs.CL

TL;DR: 针对Transformer长序列KV缓存占用大问题，提出了基于底层和中间层信息融合的FusedKV方法，实现显著内存节省和性能提升。


<details>
  <summary>Details</summary>
Motivation: Transformer解码器在长序列时KV缓存占用大量内存，现有跨层KV缓存共享方法（如YOCO, CLA）效果不如层内方法（如GQA），需要理解和优化KV缓存信息流分布。

Method: 提出FusedKV，通过对底层和中间层最有信息量的KV缓存进行可学习融合，保留相对位置编码信息，同时提出FusedKV-Lite简化融合方式以降低I/O开销。

Result: 在参数规模从3.32亿到40亿的大型语言模型中，方法减少50%缓存内存消耗，同时验证困惑度优于标准Transformer解码器。

Conclusion: FusedKV及其简化版本FusedKV-Lite为Transformer解码器提供了一种高效节省内存且性能优越的跨层KV缓存共享架构替代方案。

Abstract: Transformer decoders have achieved strong results across tasks, but the memory required for the KV cache becomes prohibitive at long sequence lengths. Although Cross-layer KV Cache sharing (e.g., YOCO, CLA) offers a path to mitigate KV Cache bottleneck, it typically underperforms within-layer methods like GQA. To understand the root cause, we investigate the information flow of keys and values of the top-layers. Our preliminary reveals a clear distribution: values are predominantly derived from the bottom layer, while keys draw more information from both bottom and middle layers. Building upon this, we propose FusedKV, whose top-layer KV caches are a learnable fusion of the most informative ones from the bottom and middle layers. This fusion operates directly on post-RoPE keys, preserving relative positional information without the computational cost of re-applying rotary embeddings. To further improve efficiency, we propose FusedKV-Lite, an cross-layer sharing approach, where top-layer KV caches are directly derived from the bottom-layer values and the middle-layer keys. Compared to FusedKV, FusedKV-Lite reduces I/O overhead at the cost of a slight increase in perplexity. In experiments on LLMs ranging from 332M to 4B parameters, our proposed method reduce 50\% cache memory while achieving lower validation perplexity than the standard Transformer decoder, establishing it as a memory-efficient, high-performance architectural alternative.

</details>


### [32] [BERnaT: Basque Encoders for Representing Natural Textual Diversity](https://arxiv.org/abs/2512.03903)
*Ekhi Azurmendi,Joseba Fernandez de Landa,Jaione Bengoetxea,Maite Heredia,Julen Etxaniz,Mikel Zubillaga,Ander Soraluze,Aitor Soroa*

Main category: cs.CL

TL;DR: 本研究通过引入多样语言变体语料，提升低资源语言模型的鲁棒性和泛化能力，验证了语言多样性的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型多依赖标准文本，导致模型鲁棒性下降和代表性偏差，忽略了非标准语言变体。

Method: 构建包含标准、社交媒体及历史文本的多样语料库，训练三种配置的BERnaT编码器模型，设计区分标准与多样语言理解任务的评估框架。

Result: 结合标准与多样语料训练的模型在多种任务上表现优于仅标准语料训练的模型，同时保持标准基准准确度。

Conclusion: 语言模型应覆盖语言多样性，以提升泛化能力和包容性。

Abstract: Language models depend on massive text corpora that are often filtered for quality, a process that can unintentionally exclude non-standard linguistic varieties, reduce model robustness and reinforce representational biases. In this paper, we argue that language models should aim to capture the full spectrum of language variation (dialectal, historical, informal, etc.) rather than relying solely on standardized text. Focusing on Basque, a morphologically rich and low-resource language, we construct new corpora combining standard, social media, and historical sources, and pre-train the BERnaT family of encoder-only models in three configurations: standard, diverse, and combined. We further propose an evaluation framework that separates Natural Language Understanding (NLU) tasks into standard and diverse subsets to assess linguistic generalization. Results show that models trained on both standard and diverse data consistently outperform those trained on standard corpora, improving performance across all task types without compromising standard benchmark accuracy. These findings highlight the importance of linguistic diversity in building inclusive, generalizable language models.

</details>


### [33] [Is Lying Only Sinful in Islam? Exploring Religious Bias in Multilingual Large Language Models Across Major Religions](https://arxiv.org/abs/2512.03943)
*Kazi Abrab Hossain,Jannatul Somiya Mahmud,Maria Hossain Tuli,Anik Mitra,S. M. Taiabul Haque,Farig Y. Sadeque*

Main category: cs.CL

TL;DR: BRAND数据集揭示多语言模型在宗教偏见检测中的不足，特别是对伊斯兰教的倾向偏见问题。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在宗教敏感领域表现不稳定，尤其是多语言模型在准确性和偏见问题上存在挑战，需要专门数据集进行测评和改进。

Method: 构建包含南亚四大宗教的双语数据集（英语和孟加拉语），设计三种提示方式评估多语言模型的偏见表现。

Result: 本论文提出了BRAND数据集，专注于南亚四大宗教（佛教、基督教、印度教和伊斯兰教）的多语言偏见检测，包含2400多条数据，通过英文和孟加拉语三种不同提示测试模型。结果显示模型在英语上的表现优于孟加拉语，且在宗教中表现出对伊斯兰教的偏见，即使在宗教中性的问题上也存在偏差。

Conclusion: 多语言大模型在处理宗教相关内容时存在语言差异导致的准确度差异和系统性偏见，需进一步改进模型公平性。

Abstract: While recent developments in large language models have improved bias detection and classification, sensitive subjects like religion still present challenges because even minor errors can result in severe misunderstandings. In particular, multilingual models often misrepresent religions and have difficulties being accurate in religious contexts. To address this, we introduce BRAND: Bilingual Religious Accountable Norm Dataset, which focuses on the four main religions of South Asia: Buddhism, Christianity, Hinduism, and Islam, containing over 2,400 entries, and we used three different types of prompts in both English and Bengali. Our results indicate that models perform better in English than in Bengali and consistently display bias toward Islam, even when answering religion-neutral questions. These findings highlight persistent bias in multilingual models when similar questions are asked in different languages. We further connect our findings to the broader issues in HCI regarding religion and spirituality.

</details>


### [34] [Adapting Large Language Models to Low-Resource Tibetan: A Two-Stage Continual and Supervised Fine-Tuning Study](https://arxiv.org/abs/2512.03976)
*Lifeng Chen,Ryan Lai,Tianming Liu*

Main category: cs.CL

TL;DR: 本文提出两阶段方法有效适应大模型至藏语，显著提升性能，揭示适应层次机制，为低资源语言扩展大模型提供新框架。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型适应低资源语言（如藏语）的难题，克服数据稀缺和跨语言漂移问题。

Method: 采用两阶段适应策略：先进行持续预训练（CPT）以建立藏语语言基础，再通过有监督微调（SFT）实现任务和翻译专门化。

Result: 困惑度显著下降（2.98降至1.54），中译藏翻译质量大幅提升（BLEU从0.046提升到0.261，chrF从2.2提升到6.6）；适应主要在嵌入层和输出头，MLP中后层负责领域特定转换。

Conclusion: 持续预训练构建藏语语义空间，有监督微调提升任务对齐，且几乎无表示层破坏，首次量化分析藏语大模型适应动态，提供可复现框架扩展多语言基础模型。

Abstract: Adapting large language models (LLMs) to low-resource languages remains a major challenge due to data scarcity and cross-lingual drift. This work presents a two-stage adaptation of Qwen2.5-3B to Tibetan, a morphologically rich and underrepresented language. We employ Continual Pretraining (CPT) to establish Tibetan linguistic grounding, followed by Supervised Fine-Tuning (SFT) for task and translation specialization. Empirical evaluations demonstrate a consistent decrease in perplexity (from 2.98 $\rightarrow$ 1.54) and substantial improvements in Chinese$\rightarrow$Tibetan translation quality (BLEU: 0.046 $\rightarrow$ 0.261; chrF: 2.2 $\rightarrow$ 6.6). Layer-wise analysis across 435 layers in Qwen3-4B reveals that adaptation primarily concentrates on embedding and output heads, with mid--late MLP projections encoding domain-specific transformations. Our findings suggest that CPT constructs a Tibetan semantic manifold while SFT sharpens task alignment with minimal representational disruption. This study provides the first quantitative exploration of Tibetan adaptation dynamics for LLMs, and offers an open, reproducible framework for extending multilingual foundation models to low-resource settings.

</details>


### [35] [Teaching Old Tokenizers New Words: Efficient Tokenizer Adaptation for Pre-trained Models](https://arxiv.org/abs/2512.03989)
*Taido Purason,Pavel Chizhov,Ivan P. Yamshchikov,Mark Fishel*

Main category: cs.CL

TL;DR: 该论文提出了继续BPE训练和叶节点词汇剪枝两种方法，改进分词器在新领域的适应性，提高词汇利用效率，相关工具已开源。


<details>
  <summary>Details</summary>
Motivation: 预训练语言模型在迁移到新领域或语言时，需要适当调整分词器以适应新环境，解决词汇扩展和冗余词汇问题。

Method: 通过在新领域文本上继续BPE训练扩充词汇；采用叶节点词汇剪枝技术移除冗余词汇，结合实验验证了方法优越性。

Result: 提出继续BPE训练方法，通过在新数据上持续BPE合并学习，提升分词效率和词汇利用率；引入基于叶节点的词汇剪枝技术，去除冗余词汇而不损害模型质量。

Conclusion: 继续BPE训练和词汇剪枝方法有效提升了分词器的适应性和效率，可实用地控制词汇调整，保障模型性能。

Abstract: Tokenizer adaptation plays an important role in transferring pre-trained language models to new domains or languages. In this work, we address two complementary aspects of this process: vocabulary extension and pruning. The common approach to extension trains a new tokenizer on domain-specific text and appends the tokens that do not overlap with the existing vocabulary, which often results in many tokens that are unreachable or never used. We propose continued BPE training, which adapts a pre-trained tokenizer by continuing the BPE merge learning process on new data. Experiments across multiple languages and model families show that this approach improves tokenization efficiency and leads to better utilization of added vocabulary. We also introduce leaf-based vocabulary pruning, which removes redundant tokens while preserving model quality. Together, these methods provide practical tools for controlled vocabulary modification, which we release as an open-source package.

</details>


### [36] [AugServe: Adaptive Request Scheduling for Augmented Large Language Model Inference Serving](https://arxiv.org/abs/2512.04013)
*Ying Wang,Zhen Jin,Jiexiong Xu,Wenhai Lin,Yiquan Chen,Wenzhi Chen*

Main category: cs.CL

TL;DR: AugServe通过智能调度和动态批处理技术，大幅提升了增强型大语言模型推理服务的效率和响应速度。


<details>
  <summary>Details</summary>
Motivation: 现有增强型LLM推理系统存在基于先到先服务调度导致严重排队延迟和静态批处理令牌限制难以适应负载变化的问题，导致服务吞吐和质量受损。

Method: 提出了一种两阶段自适应请求调度策略，结合推理请求特征和运行时信息动态优化调度顺序，并根据硬件状态及实时负载调整令牌批处理机制。

Result: 相比vLLM和InferCept，AugServe的有效吞吐量分别提升4.7-33.1倍和3.3-13.2倍，首个令牌响应时间减少了96.3%和95.0%。

Conclusion: AugServe通过采用两阶段自适应请求调度策略和动态令牌批处理机制，显著提升了增强型大语言模型推理服务的有效吞吐量和降低排队延迟，优化了用户体验。

Abstract: As augmented large language models (LLMs) with external tools become increasingly popular in web applications, improving augmented LLM inference serving efficiency and optimizing service-level objectives (SLOs) are critical for enhancing user experience. To achieve this, inference systems must maximize request handling within latency constraints, referred to as increasing effective throughput. However, existing systems face two major challenges: (i) reliance on first-come-first-served (FCFS) scheduling causes severe head-of-line blocking, leading to queuing delays exceeding the SLOs for many requests; and (ii) static batch token limit, which fails to adapt to fluctuating loads and hardware conditions. Both of these factors degrade effective throughput and service quality.
  This paper presents AugServe, an efficient inference framework designed to reduce queueing latency and enhance effective throughput for augmented LLM inference services. The core idea of AugServe is a two-stage adaptive request scheduling strategy. Specifically, AugServe combines the inference features of augmented LLM requests to optimize the order of scheduling decisions (stage I). These decisions are continuously refined with runtime information (stage II), adapting to both request characteristics and system capabilities. In addition, AugServe dynamically adjusts the token batching mechanism based on hardware status and real-time load, further enhancing throughput performance. Experimental results show that AugServe achieves 4.7-33.1x and 3.3-13.2x higher effective throughput than vLLM and InferCept, while reducing time-to-first-token (TTFT) by up to 96.3% and 95.0%, respectively.

</details>


### [37] [Jina-VLM: Small Multilingual Vision Language Model](https://arxiv.org/abs/2512.04032)
*Andreas Koukounas,Georgios Mastrapas,Florian Hönicke,Sedigheh Eslami,Guillaume Roncari,Scott Martens,Han Xiao*

Main category: cs.CL

TL;DR: Jina-VLM是一种具有24亿参数的多语言视觉问答模型，在中型开源视觉语言模型中表现出色，利用SigLIP2编码器和Qwen3语言骨干通过注意力连接器高效处理各分辨率图像。


<details>
  <summary>Details</summary>
Motivation: 提升多语言视觉问答性能，在开放中型规模视觉语言模型中实现领先表现。

Method: 结合SigLIP2视觉编码器和Qwen3语言骨干，通过注意力池化连接器实现对任意分辨率图像的高效处理。

Result: 在标准视觉问答基准和多语言评测中，Jina-VLM表现优于同类模型，并保持文本任务性能的竞争力。

Conclusion: Jina-VLM在多语言视觉问答任务中优于同规模模型，同时保持较好的文本性能。

Abstract: We present Jina-VLM, a 2.4B parameter vision-language model that achieves state-of-the-art multilingual visual question answering among open 2B-scale VLMs. The model couples a SigLIP2 vision encoder with a Qwen3 language backbone through an attention-pooling connector that enables token-efficient processing of arbitrary-resolution images. Across standard VQA benchmarks and multilingual evaluations, Jina-VLM outperforms comparable models while preserving competitive text-only performance.

</details>


### [38] [SkillFactory: Self-Distillation For Learning Cognitive Behaviors](https://arxiv.org/abs/2512.04072)
*Zayne Sprague,Jack Lu,Manya Wadhwa,Sedrick Keh,Mengye Ren,Greg Durrett*

Main category: cs.CL

TL;DR: SkillFactory通过SFT阶段预训练模型认知技能，提升其强化学习后的泛化能力和稳健性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖基本模型已有技能进行强化学习，但基模型缺乏的新技能难以学习，SkillFactory旨在通过SFT阶段让模型初步学会这些技能，为RL环节奠定基础。

Method: 利用模型自身生成的样本重组为技能导向的训练数据进行监督微调，形成"银"标准SFT轨迹，再进行强化学习提升模型表现。

Result: 提出SkillFactory方法，通过监督微调（SFT）阶段使模型学习长链推理中使用的认知技能，为后续强化学习（RL）阶段做准备；该方法不依赖大模型蒸馏，而是利用模型自身重新排列的训练样本生成"银"标准SFT轨迹，有效提高模型在RL中的技能掌握能力。

Conclusion: SkillFactory方法通过预先引入归纳偏置，帮助模型在RL阶段更好地学习和运用认知技能，增强了模型对复杂任务的泛化能力和对域外任务的鲁棒性。

Abstract: Reasoning models leveraging long chains of thought employ various cognitive skills, such as verification of their answers, backtracking, retrying by an alternate method, and more. Previous work has shown that when a base language model exhibits these skills, training that model further with reinforcement learning (RL) can learn to leverage them. How can we get models to leverage skills that aren't exhibited by base models? Our work, SkillFactory, is a method for fine-tuning models to roughly learn these skills during a supervised fine-tuning (SFT) stage prior to RL. Our approach does not rely on distillation from a stronger model, but instead uses samples from the model itself, rearranged to provide training data in the format of those skills. These "silver" SFT traces may be imperfect, but are nevertheless effective for priming a model to acquire skills during RL. Our evaluation shows that (1) starting from SkillFactory SFT initialization helps a model to generalize to harder variants of a task post-RL, despite lower performance pre-RL; (2) cognitive skills are indeed used by the model; (3) RLed SkillFactory models are more robust to regression on out-of-domain tasks than RLed base models. Our work suggests that inductive biases learned prior to RL help models learn robust cognitive skill use.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [39] [Is Vibe Coding Safe? Benchmarking Vulnerability of Agent-Generated Code in Real-World Tasks](https://arxiv.org/abs/2512.03262)
*Songwen Zhao,Danqing Wang,Kexun Zhang,Jiaxuan Luo,Zhuo Li,Lei Li*

Main category: cs.SE

TL;DR: 本研究构建了一个真实任务基准，发现采用大型语言模型的vibe coding虽然功能表现良好，但安全漏洞频发，安全性能不足，提示生产环境的广泛应用需谨慎。


<details>
  <summary>Details</summary>
Motivation: 随着vibe coding的广泛应用，探究其生成代码在实际生产环境中的安全性变得迫切且必要。

Method: 构建了SU S VI B E S基准库，包含200个来自真实开源项目的带有安全漏洞的软件工程任务，评估多种先进LLM编程代理的安全性能。

Result: 测试结果显示，尽管大多数代码功能正确，但安全性评分极低，仅10.5%的代码被评估为安全，且简单的安全策略难以有效提升安全水平。

Conclusion: Vibe coding产出在安全性方面表现较差，现有大型语言模型代理生成的代码存在较多安全漏洞，不足以直接部署于生产环境中。

Abstract: Vibe coding is a new programming paradigm in which human engineers instruct large language model (LLM) agents to complete complex coding tasks with little supervision. Although it is increasingly adopted, are vibe coding outputs really safe to deploy in production? To answer this question, we propose SU S VI B E S, a benchmark consisting of 200 feature-request software engineering tasks from real-world open-source projects, which, when given to human programmers, led to vulnerable implementations. We evaluate multiple widely used coding agents with frontier models on this benchmark. Disturbingly, all agents perform poorly in terms of software security. Although 61% of the solutions from SWE-Agent with Claude 4 Sonnet are functionally correct, only 10.5% are secure. Further experiments demonstrate that preliminary security strategies, such as augmenting the feature request with vulnerability hints, cannot mitigate these security issues. Our findings raise serious concerns about the widespread adoption of vibe-coding, particularly in security-sensitive applications.

</details>


### [40] [Exploring the Potential and Limitations of Large Language Models for Novice Program Fault Localization](https://arxiv.org/abs/2512.03421)
*Hexiang Xu,Hengyuan Liu,Yonghao Wu,Xiaolan Kang,Xiang Chen,Yong Liu*

Main category: cs.SE

TL;DR: 本研究评估了多款大型语言模型在程序故障定位任务中的性能，发现具备推理能力的模型表现更优，LLMs对新手调试帮助显著，但仍存在计算成本高和过度推理问题。


<details>
  <summary>Details</summary>
Motivation: 初学者在程序故障定位中存在理解和经验不足的问题，传统方法缺乏上下文理解能力，导致定位效果不佳。因此，利用具备强语义理解能力的LLMs来提升故障定位的准确性和调试辅助效果，具有重要研究意义。

Method: 本文通过在Codeflaws、Condefects和新构建的BugT数据集上测试多款开源与闭源LLMs，比较不同模型在故障定位准确率、提示依赖、推理表现和计算成本等方面的表现来评估其有效性。

Result: 本文评估了六款闭源和七款开源大型语言模型（LLMs）在程序故障定位任务中的表现，使用Codeflaws、Condefects和新构建的BugT数据集。结果显示，具备推理能力的高级模型如OpenAI o3和DeepSeekR1在准确率上表现优越，且对提示设计依赖较小；而缺乏推理能力的模型，如GPT-4，需要精心设计的提示才能维持性能。LLMs在简单故障定位上表现良好，但随着问题难度增加准确率下降，且存在“过度推理”导致结果不清晰的问题。此外，LLMs的计算成本较高，不利于实时调试。针对初学者，LLMs提供的解释具有显著价值，有助于提升调试效率。总体表明LLMs在故障定位领域潜力巨大，但仍需改进推理能力和计算效率。

Conclusion: LLMs能够提升程序故障定位的效率和准确率，尤其对初学者辅助价值较大，但需进一步优化其推理能力和计算资源消耗以实现实用化。

Abstract: Novice programmers often face challenges in fault localization due to their limited experience and understanding of programming syntax and logic. Traditional methods like Spectrum-Based Fault Localization (SBFL) and Mutation-Based Fault Localization (MBFL) help identify faults but often lack the ability to understand code context, making them less effective for beginners. In recent years, Large Language Models (LLMs) have shown promise in overcoming these limitations by utilizing their ability to understand program syntax and semantics. LLM-based fault localization provides more accurate and context-aware results than traditional techniques. This study evaluates six closed-source and seven open-source LLMs using the Codeflaws, Condefects, and BugT datasets, with BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns. Advanced models with reasoning capabilities, such as OpenAI o3 and DeepSeekR1, achieve superior accuracy with minimal reliance on prompt engineering. In contrast, models without reasoning capabilities, like GPT-4, require carefully designed prompts to maintain performance. While LLMs perform well in simple fault localization, their accuracy decreases as problem difficulty increases, though top models maintain robust performance in the BugT dataset. Over-reasoning is another challenge, where some models generate excessive explanations that hinder fault localization clarity. Additionally, the computational cost of deploying LLMs remains a significant barrier for real-time debugging. LLM's explanations demonstrate significant value for novice programmer assistance, with one-year experience participants consistently rating them highly. Our findings demonstrate the potential of LLMs to improve debugging efficiency while stressing the need for further refinement in their reasoning and computational efficiency for practical adoption.

</details>


### [41] [Runnable Directories: The Solution to the Monorepo vs. Multi-repo Debate](https://arxiv.org/abs/2512.03815)
*Shayan Ghasemnezhad,Samarth KaPatel,Sofia Nikiforova,Giacinto Paolo Saggese,Paul Smith,Heanh Sok*

Main category: cs.SE

TL;DR: 本文提出Causify Dev，通过可运行目录和容器化工作流，融合monorepo与multi-repo优点，提高大型软件代码库的可维护性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统单代码库和多代码库策略各有优劣，难以同时满足一致性、模块化、可扩展性和工具复杂度需求，因此亟需一种结合二者优点的混合解决方案。

Method: 引入可运行目录作为自包含执行单元，采用统一轻量环境、共享工具和基于Docker的容器化工作流，保障依赖隔离和持续集成部署的高效。

Result: 本文提出了一种名为Causify Dev的混合软件开发系统，旨在解决传统单一代码库（monorepo）和多代码库（multi-repo）策略的权衡问题。通过引入“可运行目录”（runnable directory）概念，即自包含、可独立执行的单元，配合统一的轻量级环境、共享辅助工具及基于Docker容器的工作流，实现了依赖隔离、一致配置和高效的CI/CD。该方法在单一代码库和多代码库之间提供了实用的折中方案，提升了复杂代码库的可靠性和可维护性。

Conclusion: Causify Dev系统通过可运行目录和统一环境实现了单代码库与多代码库之间的有效折中，提升了复杂软件项目的开发及维护效率。

Abstract: Modern software systems increasingly strain traditional codebase organization strategies. Monorepos offer consistency but often suffer from scalability issues and tooling complexity, while multi-repos provide modularity at the cost of coordination and dependency management challenges. As an answer to this trade-off, we present the Causify Dev system, a hybrid approach that integrates key benefits of both. Its central concept is the runnable directory -- a self-contained, independently executable unit with its own development, testing, and deployment lifecycles. Backed by a unified thin environment, shared helper utilities, and containerized Docker-based workflows, runnable directories enable consistent setups, isolated dependencies, and efficient CI/CD processes. The Causify Dev approach provides a practical middle ground between monorepo and multi-repo strategies, improving reliability and maintainability for growing, complex codebases.

</details>


### [42] [A Comprehensive Study on the Impact of Vulnerable Dependencies on Open-Source Software](https://arxiv.org/abs/2512.03868)
*Shree Hari Bittugondanahalli Indra Kumar,Lilia Rodrigues Sampaio,André Martin,Andrey Brito,Christof Fetzer*

Main category: cs.SE

TL;DR: 该研究利用自研SCA工具分析了超千个多语种开源项目的依赖漏洞，发现漏洞多为传递性，且关键漏洞平均超过一年未修复，强调了监控依赖漏洞及加快修复的重要性。


<details>
  <summary>Details</summary>
Motivation: 开源库广泛被开发者使用以加速产品开发，但可能引入安全漏洞，导致严重安全事件，如Log4Shell，因此迫切需要理解和解决依赖漏洞问题。

Method: 通过使用自研的软件组成分析（SCA）工具VODA，爬取了包含Java、Python、Rust等多语言的1000多个GitHub开源项目及其2013至2023年版本历史，共约5万个版本，分析依赖库版本、依赖深度及已知漏洞，并结合项目团队规模、活跃度及发布周期等指标进行关联研究。

Result: 发现大多数编程语言中，漏洞依赖主要为传递性依赖；关键漏洞平均存在时间超过一年才被修复；依赖深度及漏洞持续时间的分析为理解漏洞传播和修复提供了数据支持。相比以往研究，样本规模更大、语言更丰富，提升了结果的广泛适用性。

Conclusion: 开源项目中安全漏洞普遍且持续时间长，尤其是传递性依赖的漏洞较难及时修复。针对依赖漏洞的持续监控和加速修复机制至关重要，以提升软件供应链安全。

Abstract: Open-source libraries are widely used by software developers to speed up the development of products, however, they can introduce security vulnerabilities, leading to incidents like Log4Shell. With the expanding usage of open-source libraries, it becomes even more imperative to comprehend and address these dependency vulnerabilities. The use of Software Composition Analysis (SCA) tools does greatly help here as they provide a deep insight on what dependencies are used in a project, enhancing the security and integrity in the software supply chain. In order to learn how wide spread vulnerabilities are and how quickly they are being fixed, we conducted a study on over 1k open-source software projects with about 50k releases comprising several languages such as Java, Python, Rust, Go, Ruby, PHP, and JavaScript. Our objective is to investigate the severity, persistence, and distribution of these vulnerabilities, as well as their correlation with project metrics such as team and contributors size, activity and release cycles. In order to perform such analysis, we crawled over 1k projects from github including their version history ranging from 2013 to 2023 using VODA, our SCA tool. Using our approach, we can provide information such as library versions, dependency depth, and known vulnerabilities, and how they evolved over the software development cycle. Being larger and more diverse than datasets used in earlier works and studies, ours provides better insights and generalizability of the gained results. The data collected answers several research questions about the dependency depth and the average time a vulnerability persists. Among other findings, we observed that for most programming languages, vulnerable dependencies are transitive, and a critical vulnerability persists in average for over a year before being fixed.

</details>


### [43] [Tunable Automation in Automated Program Verification](https://arxiv.org/abs/2512.03926)
*Alexander Y. Bai,Chris Hawblitzel,Andrea Lattuada*

Main category: cs.SE

TL;DR: 本文提出了一种细粒度控制量词事实的机制，以调节验证自动化与性能之间的平衡，提升SMT求解器在软件验证中的应用效果。


<details>
  <summary>Details</summary>
Motivation: SMT求解器在处理量词实例时，在自动化程度和性能之间存在根本矛盾，急需一种机制来灵活调节这一矛盾。

Method: 通过允许库作者预定义多级自动化设置，并让最终用户在模块、函数或证明上下文中自定义量词的可用性来控制量词实例的使用。

Result: 实际在Verus工具及多个公开代码库上的评估表明，该方法有效展示了自动化与性能的权衡，支持开发者根据具体上下文选择适合的自动化级别。

Conclusion: 选择性管理量词实例可以使开发者在不同上下文中调节自动化程度，实现自动化与性能的良好平衡。

Abstract: Automated verification tools based on SMT solvers have made significant progress in verifying complex software systems. However, these tools face a fundamental tension between automation and performance when dealing with quantifier instantiation -- the primary source of incompleteness and verification slowdown in SMT-based verifiers. Tools choose between aggressive quantifier instantiation that provides more automation but longer verification times, or conservative instantiation that responds quickly but may require more manual proof hints.
  We present a mechanism that enables fine-grained control over the availability of quantified facts in verification contexts, allowing developers to selectively tune the level of automation. Our approach lets library authors provide different pre-defined automation levels while giving end-users the ability to further customize quantifier availability at the module, function, or proof context level.
  We implement our techniques in Verus, a Rust-based verification tool, and evaluate them on multiple openly available codebases. Our empirical analysis demonstrates the automation-performance tradeoff and that selective quantifier management enables developers to select the appropriate level of automation in different contexts.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [44] [AGENTSAFE: A Unified Framework for Ethical Assurance and Governance in Agentic AI](https://arxiv.org/abs/2512.03180)
*Rafflesia Khan,Declan Joyce,Mansura Habiba*

Main category: cs.MA

TL;DR: 本文提出了AGENTSAFE，一个针对基于大语言模型的自主代理系统的统一治理框架，通过整合风险库和多层次控制机制，实现了从风险识别到持续保障的全面管理。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型代理的治理框架碎片化、静态，缺乏一个从风险识别到运营保障的端到端完整流程，尤其是针对自主代理系统的综合治理需求。

Method: 构建了一个名为AGENTSAFE的治理框架，包含设计时、运行时和审计控制，基于AI风险库，对代理系统的计划-行动-观察-反思循环和工具链进行风险映射和分类。该框架引入多种安全措施，如限制风险行为、重要操作人工审批、预部署安全评估、语义遥测、动态授权、异常检测和可中断性机制，以及基于密码学和组织控制的追踪和问责。

Result: 提出AGENTSAFE框架实现了风险的端到端治理，包括设计、运行和审计管控，涵盖风险识别、预部署评价及持续运行时监控，提升了系统安全性、隐私、公平性和整体安全保障。

Conclusion: AGENTSAFE框架有效连接了风险分类与实际控制措施，建立了预部署安全评估方法，并通过运行时治理和问责机制，增强了代理型AI系统的可信度和安全性。

Abstract: The rapid deployment of large language model (LLM)-based agents introduces a new class of risks, driven by their capacity for autonomous planning, multi-step tool integration, and emergent interactions. It raises some risk factors for existing governance approaches as they remain fragmented: Existing frameworks are either static taxonomies driven; however, they lack an integrated end-to-end pipeline from risk identification to operational assurance, especially for an agentic platform. We propose AGENTSAFE, a practical governance framework for LLM-based agentic systems. The framework operationalises the AI Risk Repository into design, runtime, and audit controls, offering a governance framework for risk identification and assurance. The proposed framework, AGENTSAFE, profiles agentic loops (plan -> act -> observe -> reflect) and toolchains, and maps risks onto structured taxonomies extended with agent-specific vulnerabilities. It introduces safeguards that constrain risky behaviours, escalates high-impact actions to human oversight, and evaluates systems through pre-deployment scenario banks spanning security, privacy, fairness, and systemic safety. During deployment, AGENTSAFE ensures continuous governance through semantic telemetry, dynamic authorization, anomaly detection, and interruptibility mechanisms. Provenance and accountability are reinforced through cryptographic tracing and organizational controls, enabling measurable, auditable assurance across the lifecycle of agentic AI systems. The key contributions of this paper are: (1) a unified governance framework that translates risk taxonomies into actionable design, runtime, and audit controls; (2) an Agent Safety Evaluation methodology that provides measurable pre-deployment assurance; and (3) a set of runtime governance and accountability mechanisms that institutionalise trust in agentic AI ecosystems.

</details>


### [45] [Learning Network Sheaves for AI-native Semantic Communication](https://arxiv.org/abs/2512.03248)
*Enrico Grimaldi,Mario Edoardo Pandolfo,Gabriele D'Acunto,Sergio Barbarossa,Paolo Di Lorenzo*

Main category: cs.MA

TL;DR: 本文提出了一种面向多异构AI代理的语义通信框架，通过学习通信拓扑和语义去噪压缩实现高效对齐与任务相关语义保持，提升了多智能体协同效果。


<details>
  <summary>Details</summary>
Motivation: 解决多异构AI代理之间在通信压缩潜空间表示时的语义噪声问题，同时保持任务相关的语义信息。

Method: 通过学习通信拓扑和对齐映射生成配备正交映射的学习型网络簇，结合语义去噪压缩模块构建共享的全局语义空间，并采用非凸字典学习问题迭代求解。

Result: 多代理在真实图像数据上的实验表明，语义去噪压缩促进了代理间的一致性和语义聚类提取，同时在下游任务中保持高精度。

Conclusion: 所提出的通信网络展示了不同代理间的语义异质性，且方法具有良好的可解释性。

Abstract: Recent advances in AI call for a paradigm shift from bit-centric communication to goal- and semantics-oriented architectures, paving the way for AI-native 6G networks. In this context, we address a key open challenge: enabling heterogeneous AI agents to exchange compressed latent-space representations while mitigating semantic noise and preserving task-relevant meaning. We cast this challenge as learning both the communication topology and the alignment maps that govern information exchange among agents, yielding a learned network sheaf equipped with orthogonal maps. This learning process is further supported by a semantic denoising end compression module that constructs a shared global semantic space and derives sparse, structured representations of each agent's latent space. This corresponds to a nonconvex dictionary learning problem solved iteratively with closed-form updates. Experiments with mutiple AI agents pre-trained on real image data show that the semantic denoising and compression facilitates AI agents alignment and the extraction of semantic clusters, while preserving high accuracy in downstream task. The resulting communication network provides new insights about semantic heterogeneity across agents, highlighting the interpretability of our methodology.

</details>


### [46] [A Gossip-Enhanced Communication Substrate for Agentic AI: Toward Decentralized Coordination in Large-Scale Multi-Agent Systems](https://arxiv.org/abs/2512.03285)
*Nafiul I. Khan,Mansura Habiba,Rafflesia Khan*

Main category: cs.MA

TL;DR: 针对多智能体系统中灵活、分布式协调的需求，本文提出将gossip协议作为补充通信机制，强调其在扩展性和自适应性方面的优势，同时指出需解决的关键挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 多智能体平台规模扩大，智能体角色不再固定，预定义工具链不足以支持灵活、分布式的协调，现有结构化通信协议难以满足大型自适应系统中涌现的群集智能需求。

Method: 重新审视gossip协议，作为智能体通信的补充机制，分析其去中心化、容错性和知识扩散特性，并探讨其在上下文传播、协同及全局感知中的应用潜力与挑战。

Result: gossip协议能够支持上下文丰富的状态传播、在不确定性中实现弹性协调和全局意识涌现，同时存在语义相关性、时间陈旧性和动作一致性保障的挑战。

Conclusion: gossip机制对于未来大规模、自主性增强的智能体生态系统至关重要，有助于保持其鲁棒性、自适应性和自组织特性，需进一步研究语义过滤、信任及知识衰减等问题。

Abstract: As agentic platforms scale, agents are moving beyond fixed roles and predefined toolchains, creating an urgent need for flexible and decentralized coordination. Current structured communication protocols such as direct agent-to-agent messaging or MCP-style tool calls offer reliability, but they struggle to support the emergent and swarm-like intelligence required in large adaptive systems. Distributed agents must learn continuously, share context fluidly, and coordinate without depending solely on central planners.
  This paper revisits gossip protocols as a complementary substrate for agentic communication. Gossip mechanisms, long valued in distributed systems for their decentralized and fault-tolerant properties, provide scalable and adaptive diffusion of knowledge and fill gaps that structured protocols alone cannot efficiently address. However, gossip also introduces challenges, including semantic relevance, temporal staleness, and limited guarantees on action consistency in rapidly changing environments.
  We examine how gossip can support context-rich state propagation, resilient coordination under uncertainty, and emergent global awareness. We also outline open problems around semantic filtering, trust, and knowledge decay. Rather than proposing a complete framework, this paper presents a research agenda for integrating gossip into multi-agent communication stacks and argues that gossip is essential for future agentic ecosystems that must remain robust, adaptive, and self-organizing as their scale and autonomy increase.

</details>


### [47] [Local Dominance in Mixed-Strength Populations -- Fast Maximal Independent Set](https://arxiv.org/abs/2512.03303)
*Michael Luby,Sandy Irani*

Main category: cs.MA

TL;DR: 研究混合强度代理模型中的局部支配问题，证明即使代理强度各异，扩展的Luby MIS协议仍能快速收敛，但异质性会改变全局动态。


<details>
  <summary>Details</summary>
Motivation: 现实中代理强度存在差异，传统均等强度模型无法解释异质强度下的快速局部支配形成，故提出新模型以理解强度异质性对收敛速度和动力学的影响。

Method: 引入每个代理从自身强度分布中抽样强度的混合强度代理模型，扩展Luby MIS协议并数学证明其快速收敛性，同时构造特例展示异质性对动态的影响。

Result: 本文提出了一个描述具有不同强度代理的局部支配模型，并证明了基于Luby 最大独立集(MIS)协议的扩展仍能实现快速收敛。研究发现，尽管强度异质性影响过程动力学，但快速收敛性依然存在，从而为混合强度自然过程中的快速支配收敛提供了理论依据。

Conclusion: 混合强度模型中Luby MIS协议依然快速收敛，但异质性显著影响迭代过程的进展速度，导致不同的全局行为。

Abstract: In many natural and engineered systems, agents interact through local contests that determine which individuals become dominant within their neighborhoods. These interactions are shaped by inherent differences in strength, and they often lead to stable dominance patterns that emerge surprisingly quickly relative to the size of the population. This motivates the search for simple mathematical models that capture both heterogeneous agent strength and rapid convergence to stable local dominance.
  A widely studied abstraction of local dominance is the Maximal Independent Set (MIS) problem. In the Luby MIS protocol that provably converges quickly to an MIS, each agent repeatedly generates a strength value chosen uniformly and becomes locally dominant if its value is smaller than those of its neighbors. This provides a theoretical explanation for fast dominance convergence in populations of equal-strength agents and naturally raises the question of whether fast convergence also holds in the more realistic setting where agents are inherently mixed-strength.
  To investigate this question, we introduce the mixed-strength agents model, in which each agent draws its strength from its own distribution. We prove that the extension of the Luby MIS protocol where each agent repeatedly generates a strength value from its own distribution still exhibits fast dominance convergence, providing formal confirmation of the rapid convergence observed in many mixed-strength natural processes.
  We also show that heterogeneity can significantly change the dynamics of the process. In contrast to the equal-strength setting, a constant fraction of edges need not be eliminated per round. We construct a population and strength profile in which progress per round is asymptotically smaller, illustrating how inherent strength asymmetry produces qualitatively different global behavior.

</details>


### [48] [AsymPuzl: An Asymmetric Puzzle for multi-agent cooperation](https://arxiv.org/abs/2512.03466)
*Xavier Cadet,Edward Koh,Peter Chin*

Main category: cs.MA

TL;DR: 本文提出AsymPuzl环境，研究大语言模型在信息不对称条件下的多轮协作通信，发现强模型能快速共享信息解决谜题，反馈设计对性能影响显著。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体多轮交互研究多以开放式角色扮演为主，缺乏受控评估环境。为了精确研究信息不对称条件下大语言模型的通信协作机制，设计了AsymPuzl环境以提供可控、简洁且具表达力的测试平台。

Method: 设计AsymPuzl两智能体拼图环境，让两个智能体拥有互补但不完整的信息，通过消息交流合作解决问题；使用不同性能的大语言模型（包括GPT-5和Claude-4.0）进行实验，观察其通信行为及解决效率；分析不同反馈策略对模型表现的影响。

Result: 本文提出了AsymPuzl，一种设计用于在信息不对称条件下进行多轮、多智能体协作的两智能体拼图环境。通过让两个智能体分别观察拼图的不同部分并通过消息交流合作解决问题，研究了当前主流大语言模型（如GPT-5和Claude-4.0）的通信策略。结果显示，强模型能在两轮内共享完整信息达成解答，弱模型则表现出忽视对方消息或过度修正猜测的现象。此外，反馈设计对性能有显著影响，简单的自我反馈提升成功率，而详细联合反馈反而降低表现。该研究揭示了在简单合作任务中，不同模型的通信策略存在差异，并依赖于反馈信号的细节层次，提供了一个用于测试多轮合作极限的实验平台。

Conclusion: 在简单的两智能体合作任务中，不同LLM在通信策略上存在显著差异，且反馈设计对模型表现至关重要。AsymPuzl环境为研究多轮、多智能体合作提供了有效测试平台。

Abstract: Large Language Model (LLM) agents are increasingly studied in multi-turn, multi-agent scenarios, yet most existing setups emphasize open-ended role-play rather than controlled evaluation. We introduce AsymPuzl, a minimal but expressive two-agent puzzle environment designed to isolate communication under information asymmetry. Each agent observes complementary but incomplete views of a symbolic puzzle and must exchange messages to solve it cooperatively. Using a diverse set of current-generation and open-source LLMs, we show that (i) strong models such as GPT-5 and Claude-4.0 reliably converge across puzzle sizes on the solution by sharing complete information in two turns, (ii) weaker models often ignore partner messages or over-correct their hypotheses, and (iii) feedback design is non-trivial: simple self-feedback improves success rates, while detailed joint feedback can hurt performance. These findings show that even in simple cooperative tasks, LLM communication strategies diverge and depend on the granularity of feedback signals. AsymPuzl thus provides a testbed for probing the limits of multi-turn cooperation and opens avenues for studying coordination mechanisms.

</details>


### [49] [SRPG: Semantically Reconstructed Privacy Guard for Zero-Trust Privacy in Educational Multi-Agent Systems](https://arxiv.org/abs/2512.03694)
*Shuang Guo,Zihui Li*

Main category: cs.MA

TL;DR: SRPG利用双流重构机制，有效保护教育多智能体对话中的未结构化个人隐私，保障零泄露同时维持高水平教学质量。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型驱动的多智能体系统在个性化教育中存在未结构化对话中个人身份信息泄露的风险。传统隐私方法难以同时保障安全与教学效用。

Method: 设计双流重构机制：严格去敏流确保个人信息零泄露，语境重构流由大语言模型驱动，恢复数学逻辑，实现隐私与内容的解耦。

Result: 提出SRPG隐私保护机制，利用双流重构机制分离私密信息和教学内容，实现零泄露且保持教学效果。在MathDial测试中，SRPG在GPT-4o模型上攻击成功率降至0，教学匹配度显著优于基线。

Conclusion: SRPG在保障未成年学生隐私安全的同时，未损害数学教学的逻辑完整性和质量，优于现有隐私保护基线方法。

Abstract: Multi-Agent Systems (MAS) with large language models (LLMs) enable personalized education but risk leaking minors personally identifiable information (PII) via unstructured dialogue. Existing privacy methods struggle to balance security and utility: role-based access control fails on unstructured text, while naive masking destroys pedagogical context. We propose SRPG, a privacy guard for educational MAS, using a Dual-Stream Reconstruction Mechanism: a strict sanitization stream ensures zero PII leakage, and a context reconstruction stream (LLM driven) recovers mathematical logic. This decouples instructional content from private data, preserving teaching efficacy. Tests on MathDial show SRPG works across models; with GPT-4o, it achieves 0.0000 Attack Success Rate (ASR) (zero leakage) and 0.8267 Exact Match, far outperforming the zero trust Pure LLM baseline (0.2138). SRPG effectively protects minors privacy without sacrificing mathematical instructional quality.

</details>
