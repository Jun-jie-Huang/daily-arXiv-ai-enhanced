{"id": "2601.05385", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.05385", "abs": "https://arxiv.org/abs/2601.05385", "authors": ["Debangshu Banerjee", "Olivier Bouissou", "Stefan Zetzsche"], "title": "DafnyPro: LLM-Assisted Automated Verification for Dafny Programs", "comment": null, "summary": "We present DafnyPro, an inference-time framework that enhances LLMs for generating verification annotations in Dafny. DafnyPro comprises three key components: a diff-checker that prevents modifications to base program logic, a pruner that removes unnecessary invariants, and a hint-augmentation system that retrieves and applies predefined, problem-independent proof strategies. We evaluate DafnyPro using Claude Sonnet 3.5 and 3.7 on four benchmarks: Clover, MBPP-Dafny, HumanEval-Dafny, and DafnyBench, achieving consistent performance gains in all cases. Notably, on DafnyBench, the most challenging benchmark, Claude Sonnet 3.5 enhanced with DafnyPro achieves 86% correct proofs, a 16 pp improvement over the base model. We also fine-tune two Qwen models on training data derived from verification attempts by larger models enhanced with DafnyPro. Our 7B and 14B models achieve 68% and 70% correct proofs on DafnyBench, respectively, demonstrating that smaller models can maintain high verification accuracy.", "AI": {"tldr": "\u63d0\u51faDafnyPro\u6846\u67b6\u901a\u8fc7\u5dee\u5f02\u68c0\u67e5\u3001\u526a\u679d\u548c\u63d0\u793a\u589e\u5f3a\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728Dafny\u9a8c\u8bc1\u6ce8\u91ca\u751f\u6210\u4e0a\u7684\u8868\u73b0\uff0c\u663e\u8457\u63d0\u9ad8\u9a8c\u8bc1\u6b63\u786e\u7387\uff0c\u4e14\u5c0f\u6a21\u578b\u4e5f\u80fd\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "motivation": "\u63d0\u5347LLM\u5728\u7a0b\u5e8f\u9a8c\u8bc1\u6ce8\u91ca\u751f\u6210\u4e2d\u7684\u8868\u73b0\uff0c\u89e3\u51b3\u57fa\u7840\u7a0b\u5e8f\u903b\u8f91\u88ab\u9519\u8bef\u4fee\u6539\u548c\u4e0d\u5fc5\u8981\u6ce8\u91ca\u5197\u4f59\u95ee\u9898\u3002", "method": "DafnyPro\u5305\u542b\u5dee\u5f02\u68c0\u67e5\u5668\u3001\u526a\u679d\u5668\u548c\u63d0\u793a\u589e\u5f3a\u7cfb\u7edf\uff0c\u901a\u8fc7\u9632\u6b62\u57fa\u7840\u7a0b\u5e8f\u903b\u8f91\u4fee\u6539\u3001\u5220\u9664\u4e0d\u5fc5\u8981\u7684\u4e0d\u53d8\u5f0f\u4ee5\u53ca\u590d\u7528\u9884\u5b9a\u4e49\u8bc1\u660e\u7b56\u7565\u63d0\u5347\u6a21\u578b\u8868\u73b0\u3002", "result": "\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528DafnyPro\u7684\u6a21\u578b\u4e00\u81f4\u83b7\u5f97\u6027\u80fd\u63d0\u5347\uff0c\u6700\u5177\u6311\u6218\u6027\u7684DafnyBench\u4e0a\u51c6\u786e\u7387\u63d0\u9ad8\u4e8616\u4e2a\u767e\u5206\u70b9\u3002\u5fae\u8c03\u7684\u5c0f\u6a21\u578b\u4e5f\u5c55\u73b0\u51fa\u9ad8\u9a8c\u8bc1\u51c6\u786e\u7387\u3002", "conclusion": "DafnyPro\u80fd\u591f\u663e\u8457\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728Dafny\u4e2d\u751f\u6210\u9a8c\u8bc1\u6ce8\u91ca\u7684\u80fd\u529b\uff0c\u589e\u5f3a\u4e86\u9a8c\u8bc1\u51c6\u786e\u7387\u548c\u6548\u7387\u3002"}}
{"id": "2601.05449", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.05449", "abs": "https://arxiv.org/abs/2601.05449", "authors": ["Theodore Chambers", "Arturo Miguel Russell Bernal", "Michael Vierhauser", "Jane Cleland-Huang"], "title": "Uncovering Failures in Cyber-Physical System State Transitions: A Fuzzing-Based Approach Applied to sUAS", "comment": "13 pages, ~7 figures; author-accepted manuscript accepted for ICSE 2026", "summary": "The increasing deployment of small Uncrewed Aerial Systems (sUAS) in diverse and often safety-critical environments demands rigorous validation of onboard decision logic under various conditions. In this paper, we present SaFUZZ, a state-aware fuzzing pipeline that validates core behavior associated with state transitions, automated failsafes, and human operator interactions in sUAS applications operating under various timing conditions and environmental disturbances. We create fuzzing specifications to detect behavioral deviations, and then dynamically generate associated Fault Trees to visualize states, modes, and environmental factors that contribute to the failure, thereby helping project stakeholders to analyze the failure and identify its root causes. We validated SaFUZZ against a real-world sUAS system and were able to identify several points of failure not previously detected by the system's development team. The fuzzing was conducted in a high-fidelity simulation environment, and outcomes were validated on physical sUAS in a real-world field testing setting. The findings from the study demonstrated SaFUZZ's ability to provide a practical and scalable approach to uncovering diverse state transition failures in a real-world sUAS application.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSaFUZZ\uff0c\u4e00\u79cd\u9488\u5bf9\u5c0f\u578b\u65e0\u4eba\u673a\u7cfb\u7edf\u72b6\u6001\u8f6c\u6362\u7684\u6a21\u7cca\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u6210\u529f\u63ed\u793a\u771f\u5b9e\u7cfb\u7edf\u4e2d\u7684\u591a\u79cd\u672a\u53d1\u73b0\u6545\u969c\uff0c\u63d0\u5347\u7cfb\u7edf\u5b89\u5168\u6027\u3002", "motivation": "\u5c0f\u578b\u65e0\u4eba\u673a\u7cfb\u7edf\uff08sUAS\uff09\u5728\u5b89\u5168\u5173\u952e\u73af\u5883\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u9700\u8981\u4e25\u683c\u9a8c\u8bc1\u5176\u51b3\u7b56\u903b\u8f91\u5728\u5404\u79cd\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u4e86SaFUZZ\uff0c\u4e00\u79cd\u72b6\u6001\u611f\u77e5\u7684\u6a21\u7cca\u6d4b\u8bd5\u6d41\u7a0b\uff0c\u901a\u8fc7\u8bbe\u8ba1\u6a21\u7cca\u6d4b\u8bd5\u89c4\u8303\u68c0\u6d4b\u884c\u4e3a\u504f\u5dee\uff0c\u5e76\u52a8\u6001\u751f\u6210\u6545\u969c\u6811\u4ee5\u53ef\u89c6\u5316\u72b6\u6001\u3001\u6a21\u5f0f\u53ca\u73af\u5883\u56e0\u7d20\uff1b\u5728\u9ad8\u4fdd\u771f\u4eff\u771f\u73af\u5883\u548c\u771f\u5b9e\u573a\u5730\u4e2d\u9a8c\u8bc1\u7cfb\u7edf\u3002", "result": "SaFUZZ\u5728\u771f\u5b9esUAS\u7cfb\u7edf\u4e2d\u53d1\u73b0\u4e86\u5f00\u53d1\u56e2\u961f\u672a\u68c0\u6d4b\u5230\u7684\u591a\u5904\u6545\u969c\u70b9\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u53d1\u73b0\u72b6\u6001\u8f6c\u6362\u5931\u8d25\u65b9\u9762\u7684\u5b9e\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "SaFUZZ\u4e3asUAS\u5e94\u7528\u4e2d\u7684\u72b6\u6001\u8f6c\u6362\u5931\u8d25\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u53ef\u5b9e\u65bd\u7684\u68c0\u6d4b\u65b9\u6cd5\uff0c\u80fd\u591f\u8f85\u52a9\u9879\u76ee\u65b9\u5206\u6790\u6545\u969c\u5e76\u5b9a\u4f4d\u6839\u56e0\u3002"}}
{"id": "2601.05463", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.05463", "abs": "https://arxiv.org/abs/2601.05463", "authors": ["Chao Wei", "Xinyi Peng", "Yawen Yan", "Mao Luo", "Ting Cai"], "title": "Rethinking Basis Path Testing: Mixed Integer Programming Approach for Test Path Set Generation", "comment": null, "summary": "Basis path testing is a cornerstone of structural testing, yet traditional automated methods, relying on greedy graph-traversal algorithms (e.g., DFS/BFS), often generate sub-optimal paths. This structural inferiority is not a trivial issue; it directly impedes downstream testing activities by complicating automated test data generation and increasing the cognitive load for human engineers. This paper reframes basis path generation from a procedural search task into a declarative optimization problem. We introduce a Mixed Integer Programming (MIP) framework designed to produce a complete basis path set that is globally optimal in its structural simplicity. Our framework includes two complementary strategies: a Holistic MIP model that guarantees a theoretically optimal path set, and a scalable Incremental MIP strategy for large, complex topologies. The incremental approach features a multi-objective function that prioritizes path simplicity and incorporates a novelty penalty to maximize the successful generation of linearly independent paths. Empirical evaluations on both real-code and large-scale synthetic Control Flow Graphs demonstrate that our Incremental MIP strategy achieves a 100\\% success rate in generating complete basis sets, while remaining computationally efficient. Our work provides a foundational method for generating a high-quality structural \"scaffold\" that can enhance the efficiency and effectiveness of subsequent test generation efforts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df7\u5408\u6574\u6570\u89c4\u5212\uff08MIP\uff09\u7684\u8def\u5f84\u751f\u6210\u6846\u67b6\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u8d2a\u5fc3\u7b97\u6cd5\u751f\u6210\u8def\u5f84\u7ed3\u6784\u6b21\u4f18\u7684\u95ee\u9898\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u7ed3\u6784\u7b80\u5355\u4e14\u5168\u8986\u76d6\u7684\u57fa\u8def\u5f84\u96c6\u5408\u751f\u6210\u3002", "motivation": "\u4f20\u7edf\u57fa\u8def\u5f84\u6d4b\u8bd5\u4f9d\u8d56\u8d2a\u5fc3\u7684\u56fe\u904d\u5386\u7b97\u6cd5\uff08\u5982DFS/BFS\uff09\uff0c\u8def\u5f84\u8d28\u91cf\u8f83\u5dee\uff0c\u5bfc\u81f4\u81ea\u52a8\u6d4b\u8bd5\u6570\u636e\u751f\u6210\u56f0\u96be\u4e14\u589e\u52a0\u4eba\u5de5\u8ba4\u77e5\u8d1f\u62c5\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u7ed3\u6784\u66f4\u4f18\u7684\u57fa\u8def\u5f84\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u5c06\u57fa\u8def\u5f84\u751f\u6210\u95ee\u9898\u8f6c\u5316\u4e3a\u58f0\u660e\u5f0f\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u6574\u4f53MIP\u6a21\u578b\u548c\u589e\u91cfMIP\u7b56\u7565\uff0c\u540e\u8005\u901a\u8fc7\u591a\u76ee\u6807\u51fd\u6570\u548c\u65b0\u9896\u6027\u60e9\u7f5a\u673a\u5236\u4f18\u5316\u8def\u5f84\u7ed3\u6784\uff0c\u517c\u987e\u7406\u8bba\u6700\u4f18\u89e3\u548c\u5b9e\u65f6\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u589e\u91cfMIP\u7b56\u7565\u5728\u5b9e\u9645\u4ee3\u7801\u548c\u5927\u89c4\u6a21\u5408\u6210\u63a7\u5236\u6d41\u56fe\u4e0a\u5747\u80fd100%\u751f\u6210\u5b8c\u6574\u57fa\u8def\u5f84\u96c6\uff0c\u4e14\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u7ed3\u6784\u7b80\u5355\u6027\u548c\u751f\u6210\u6210\u529f\u7387\u4e0a\u7684\u4f18\u52bf\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u589e\u91cf\u578bMIP\u7b56\u7565\u5728\u751f\u6210\u5b8c\u6574\u57fa\u8def\u5f84\u96c6\u65b9\u9762\u6210\u529f\u7387\u8fbe\u5230100%\uff0c\u5728\u7ed3\u6784\u7b80\u5355\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u53ef\u663e\u8457\u63d0\u5347\u540e\u7eed\u6d4b\u8bd5\u6570\u636e\u751f\u6210\u7684\u6548\u7387\u3002"}}
{"id": "2601.05467", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05467", "abs": "https://arxiv.org/abs/2601.05467", "authors": ["Swapnil Shinde", "Sahil Wadhwa", "Andy Luo", "Emily Chen"], "title": "STELP: Secure Transpilation and Execution of LLM-Generated Programs", "comment": null, "summary": "Rapid evolution of Large Language Models (LLMs) has achieved major advances in reasoning, planning, and function-calling capabilities. Multi-agentic collaborative frameworks using such LLMs place them at the center of solving software development-related tasks such as code generation. However, direct use of LLM generated code in production software development systems is problematic. The code could be unstable or erroneous and contain vulnerabilities such as data poisoning, malicious attacks, and hallucinations that could lead to widespread system malfunctions. This prohibits the adoption of LLM generated code in production AI systems where human code reviews and traditional secure testing tools are impractical or untrustworthy. In this paper, we discuss safety and reliability problems with the execution of LLM generated code and propose a Secure Transpiler and Executor of LLM-Generated Program (STELP), capable of executing LLM-generated code in a controlled and safe manner. STELP secures autonomous production AI systems involving code generation, filling the critical void left by the impracticality or limitations of traditional secure testing methodologies and human oversight. This includes applications such as headless code generation-execution and LLMs that produce executable code snippets as an action plan to be executed in real time. We contribute a human-validated dataset of insecure code snippets and benchmark our approach on publicly available datasets for correctness, safety, and latency. Our results demonstrate that our approach outperforms an existing method by a significant margin, particularly in its ability to safely execute risky code snippets. Warning: This paper contains malicious code snippets that should be run with caution.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSTELP\u7684\u5b89\u5168\u8f6c\u6362\u6267\u884c\u7cfb\u7edf\uff0c\u7528\u4e8e\u5b89\u5168\u6267\u884cLLM\u751f\u6210\u7684\u4ee3\u7801\uff0c\u89e3\u51b3\u4e86\u4ee3\u7801\u4e0d\u7a33\u5b9a\u3001\u6613\u51fa\u9519\u53ca\u5b89\u5168\u6f0f\u6d1e\u7b49\u95ee\u9898\uff0c\u4fdd\u969c\u751f\u4ea7\u73af\u5883\u4e2d\u81ea\u52a8\u5316AI\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u7531\u4e8eLLM\u751f\u6210\u7684\u4ee3\u7801\u5b58\u5728\u6f5c\u5728\u7684\u9519\u8bef\u548c\u5b89\u5168\u98ce\u9669\uff0c\u4e14\u4f20\u7edf\u7684\u4eba\u5de5\u5ba1\u67e5\u548c\u6d4b\u8bd5\u624b\u6bb5\u96be\u4ee5\u9002\u7528\uff0c\u4e9f\u9700\u4e00\u79cd\u81ea\u52a8\u5316\u4e14\u5b89\u5168\u7684\u6267\u884c\u673a\u5236\u4fdd\u969c\u4ee3\u7801\u7684\u5b89\u5168\u8fd0\u884c\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86STELP\u5b89\u5168\u8f6c\u6362\u6267\u884c\u5668\uff0c\u901a\u8fc7\u53d7\u63a7\u73af\u5883\u6267\u884c\u4ee3\u7801\uff0c\u5e76\u7ed3\u5408\u4eba\u5de5\u9a8c\u8bc1\u7684\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u6d4b\uff0c\u786e\u4fdd\u4ee3\u7801\u7684\u6b63\u786e\u6027\u3001\u5b89\u5168\u6027\u548c\u6267\u884c\u6548\u7387\u3002", "result": "\u901a\u8fc7\u516c\u5f00\u6570\u636e\u96c6\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0cSTELP\u5728\u4ee3\u7801\u6b63\u786e\u6027\u3001\u5b89\u5168\u6027\u548c\u6267\u884c\u5ef6\u8fdf\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u5b89\u5168\u6267\u884c\u9ad8\u98ce\u9669\u4ee3\u7801\u65b9\u9762\u53d6\u5f97\u7a81\u7834\u3002", "conclusion": "STELP\u6709\u6548\u63d0\u5347\u4e86LLM\u751f\u6210\u4ee3\u7801\u6267\u884c\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\uff0c\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u5b89\u5168\u6267\u884c\u9ad8\u98ce\u9669\u4ee3\u7801\u7247\u6bb5\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2601.05279", "categories": ["cs.MA", "cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2601.05279", "abs": "https://arxiv.org/abs/2601.05279", "authors": ["Yingzhuo Liu", "Shuodi Liu", "Weijun Luo", "Liuyu Xiang", "Zhaofeng He"], "title": "Simulation-Free PSRO: Removing Game Simulation from Policy Space Response Oracles", "comment": null, "summary": "Policy Space Response Oracles (PSRO) combines game-theoretic equilibrium computation with learning and is effective in approximating Nash Equilibrium in zero-sum games. However, the computational cost of PSRO has become a significant limitation to its practical application. Our analysis shows that game simulation is the primary bottleneck in PSRO's runtime. To address this issue, we conclude the concept of Simulation-Free PSRO and summarize existing methods that instantiate this concept. Additionally, we propose a novel Dynamic Window-based Simulation-Free PSRO, which introduces the concept of a strategy window to replace the original strategy set maintained in PSRO. The number of strategies in the strategy window is limited, thereby simplifying opponent strategy selection and improving the robustness of the best response. Moreover, we use Nash Clustering to select the strategy to be eliminated, ensuring that the number of strategies within the strategy window is effectively limited. Our experiments across various environments demonstrate that the Dynamic Window mechanism significantly reduces exploitability compared to existing methods, while also exhibiting excellent compatibility. Our code is available at https://github.com/enochliu98/SF-PSRO.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9PSRO\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a8\u6001\u7a97\u53e3\u7684\u65e0\u6a21\u62dfPSRO\u65b9\u6cd5\uff0c\u901a\u8fc7\u7b56\u7565\u6570\u91cf\u9650\u5236\u548c\u7b56\u7565\u6dd8\u6c70\u673a\u5236\uff0c\u63d0\u5347\u4e86\u6548\u7387\u548c\u7b56\u7565\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u7684Policy Space Response Oracles (PSRO)\u5728\u8ba1\u7b97\u8fd1\u4f3c\u96f6\u548c\u535a\u5f08\u4e2d\u7684\u7eb3\u4ec0\u5747\u8861\u65f6\uff0c\u8ba1\u7b97\u4ee3\u4ef7\u8fc7\u9ad8\uff0c\u5c24\u5176\u662f\u6e38\u620f\u6a21\u62df\u6210\u4e3a\u4e3b\u8981\u74f6\u9888\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u65e0\u6a21\u62df\uff08Simulation-Free\uff09PSRO\u7684\u6982\u5ff5\uff0c\u56de\u987e\u4e86\u76f8\u5173\u65b9\u6cd5\uff0c\u5e76\u8fdb\u4e00\u6b65\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u52a8\u6001\u7a97\u53e3\u7684\u65e0\u6a21\u62dfPSRO\uff0c\u91c7\u7528\u7b56\u7565\u7a97\u53e3\u6765\u9650\u5236\u7ef4\u62a4\u7684\u7b56\u7565\u6570\u91cf\uff0c\u5f15\u5165Nash\u805a\u7c7b\u9009\u62e9\u5f85\u6dd8\u6c70\u7b56\u7565\u4ee5\u63a7\u5236\u7a97\u53e3\u5185\u7b56\u7565\u6570\u91cf\u3002", "result": "\u52a8\u6001\u7a97\u53e3\u673a\u5236\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u88ab\u5229\u7528\u6027\uff08exploitability\uff09\uff0c\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u7b97\u6cd5\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u901a\u8fc7\u52a8\u6001\u7a97\u53e3\u548c\u65e0\u6a21\u62df\u6280\u672f\uff0c\u663e\u8457\u51cf\u8f7b\u4e86PSRO\u7684\u8ba1\u7b97\u8d1f\u62c5\uff0c\u63d0\u9ad8\u4e86\u5b9e\u7528\u6027\u548c\u7b56\u7565\u8d28\u91cf\uff0c\u5177\u6709\u826f\u597d\u517c\u5bb9\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u73af\u5883\u3002"}}
{"id": "2601.05271", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.05271", "abs": "https://arxiv.org/abs/2601.05271", "authors": ["Xiran Fan", "Zhimeng Jiang", "Chin-Chia Michael Yeh", "Yuzhong Chen", "Yingtong Dou", "Menghai Pan", "Yan Zheng"], "title": "Enhancing Foundation Models in Transaction Understanding with LLM-based Sentence Embeddings", "comment": null, "summary": "The ubiquity of payment networks generates vast transactional data encoding rich consumer and merchant behavioral patterns. Recent foundation models for transaction analysis process tabular data sequentially but rely on index-based representations for categorical merchant fields, causing substantial semantic information loss by converting rich textual data into discrete tokens. While Large Language Models (LLMs) can address this limitation through superior semantic understanding, their computational overhead challenges real-time financial deployment. We introduce a hybrid framework that uses LLM-generated embeddings as semantic initializations for lightweight transaction models, balancing interpretability with operational efficiency. Our approach employs multi-source data fusion to enrich merchant categorical fields and a one-word constraint principle for consistent embedding generation across LLM architectures. We systematically address data quality through noise filtering and context-aware enrichment. Experiments on large-scale transaction datasets demonstrate significant performance improvements across multiple transaction understanding tasks.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u4ea4\u6613\u6570\u636e\u4e2d\u5546\u6237\u5b57\u6bb5\u7684\u8bed\u4e49\u4fe1\u606f\u4e22\u5931\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8eLLM\u5d4c\u5165\u7684\u8f7b\u91cf\u7ea7\u6df7\u5408\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4ea4\u6613\u5206\u6790\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u4ea4\u6613\u5206\u6790\u6a21\u578b\u4f9d\u8d56\u7d22\u5f15\u8868\u793a\u5546\u6237\u7c7b\u522b\uff0c\u5bfc\u81f4\u4e30\u5bcc\u6587\u672c\u8bed\u4e49\u4fe1\u606f\u4e27\u5931\uff0c\u800cLLM\u867d\u80fd\u5f25\u8865\u8bed\u4e49\u7406\u89e3\u4e0d\u8db3\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9650\u5236\u5176\u5b9e\u65f6\u91d1\u878d\u5e94\u7528\uff0c\u56e0\u6b64\u9700\u5bfb\u627e\u517c\u987e\u8bed\u4e49\u7406\u89e3\u4e0e\u8ba1\u7b97\u6548\u7387\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528LLM\u751f\u6210\u8bed\u4e49\u5d4c\u5165\u4f5c\u4e3a\u521d\u59cb\u8f93\u5165\uff0c\u7ed3\u5408\u591a\u6e90\u6570\u636e\u878d\u5408\u4e30\u5bcc\u5546\u6237\u5b57\u6bb5\u4fe1\u606f\uff0c\u5229\u7528\u5355\u8bcd\u7ea6\u675f\u539f\u5219\u786e\u4fdd\u5d4c\u5165\u4e00\u81f4\u6027\uff0c\u5e76\u901a\u8fc7\u566a\u58f0\u8fc7\u6ee4\u548c\u4e0a\u4e0b\u6587\u589e\u5f3a\u63d0\u5347\u6570\u636e\u8d28\u91cf\uff0c\u6784\u5efa\u8f7b\u91cf\u7ea7\u4ea4\u6613\u6a21\u578b\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u7684\u5d4c\u5165\u5411\u91cf\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u4ea4\u6613\u6a21\u578b\u7684\u8bed\u4e49\u521d\u59cb\u5316\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u7d22\u5f15\u8868\u793a\u6cd5\u5bfc\u81f4\u7684\u8bed\u4e49\u4fe1\u606f\u4e22\u5931\u95ee\u9898\uff0c\u540c\u65f6\u517c\u987e\u89e3\u91ca\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u591a\u6e90\u6570\u636e\u878d\u5408\u4e30\u5bcc\u5546\u6237\u5206\u7c7b\u5b57\u6bb5\uff0c\u5e76\u91c7\u7528\u5355\u8bcd\u7ea6\u675f\u539f\u5219\u4fdd\u8bc1\u5d4c\u5165\u7684\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u901a\u8fc7\u566a\u58f0\u8fc7\u6ee4\u548c\u4e0a\u4e0b\u6587\u589e\u5f3a\u63d0\u5347\u6570\u636e\u8d28\u91cf\u3002\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u4ea4\u6613\u7406\u89e3\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "\u57fa\u4e8eLLM\u751f\u6210\u5d4c\u5165\u7684\u6df7\u5408\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4ea4\u6613\u6570\u636e\u4e2d\u8bed\u4e49\u4fe1\u606f\u7684\u4e22\u5931\uff0c\u63d0\u5347\u4e86\u4ea4\u6613\u7406\u89e3\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u517c\u987e\u4e86\u6a21\u578b\u7684\u89e3\u91ca\u6027\u548c\u8fd0\u884c\u6548\u7387\u3002"}}
{"id": "2601.05485", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.05485", "abs": "https://arxiv.org/abs/2601.05485", "authors": ["Wenhao Zeng", "Yitian Chai", "Hao Zhou", "Fandong Meng", "Jie Zhou", "Xiaodong Gu"], "title": "Readability-Robust Code Summarization via Meta Curriculum Learning", "comment": "Code available at https://github.com/Zengwh02/RoFTCodeSum", "summary": "Code summarization has emerged as a fundamental technique in the field of program comprehension. While code language models have shown significant advancements, the current models and benchmarks are confined to high-readability code, which contains sufficient semantic cues such as function and variable names. In the real world, however, code is often poorly structured or obfuscated, significantly degrading model performance. In this paper, we first empirically evaluate the robustness of state-of-the-art language models on poor-readability code for the task of code summarization, focusing on (1) their effectiveness, (2) the impact of prompt engineering, and (3) the robustness of different variants. Experimental results reveal that state-of-the-art models-including GPT-4o and DeepSeek-V3 experience a substantial performance drop when faced with poorly readable code, and that prompt engineering and reasoning-enhanced models offer limited improvements. Motivated by these findings, we propose RoFTCodeSum, a novel fine-tuning method that enhances the robustness of code summarization against poorly readable code. RoFTCodeSum marries the concepts of curriculum learning and meta-learning: based on the original dataset for fine-tuning, it creates curricular training sets, e.g., obfuscating function names and identifiers from the code, respectively, that have progressive difficulty in code comprehension. In each training step, the approach meta-updates the gradients using these progressively challenging datasets, thereby optimizing both accuracy and readability robustness simultaneously. Experimental results demonstrate that RoFTCodeSum exhibits increased robustness against semantic perturbation while enhancing performance on the original code.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u4e3b\u6d41\u4ee3\u7801\u603b\u7ed3\u6a21\u578b\u5728\u96be\u8bfb\u4ee3\u7801\u4e0a\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u63d0\u51faRoFTCodeSum\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u548c\u51c6\u786e\u7387\uff0c\u5e76\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u8bed\u8a00\u6a21\u578b\u5728\u9ad8\u53ef\u8bfb\u4ee3\u7801\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u9762\u5bf9\u7ed3\u6784\u5dee\u6216\u6df7\u6dc6\u4ee3\u7801\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u4e14\u63d0\u793a\u5de5\u7a0b\u548c\u63a8\u7406\u589e\u5f3a\u65b9\u6cd5\u63d0\u5347\u6709\u9650\u3002", "method": "\u63d0\u51faRoFTCodeSum\u5fae\u8c03\u65b9\u6cd5\uff0c\u5c06\u8bfe\u7a0b\u5b66\u4e60\u548c\u5143\u5b66\u4e60\u7ed3\u5408\uff0c\u901a\u8fc7\u9010\u6b65\u589e\u52a0\u4ee3\u7801\u96be\u5ea6\u7684\u8bad\u7ec3\u6570\u636e\u8fdb\u884c\u68af\u5ea6\u5143\u66f4\u65b0\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u5bf9\u96be\u8bfb\u4ee3\u7801\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eRoFTCodeSum\u65b9\u6cd5\u5728\u6297\u8bed\u4e49\u6270\u52a8\u7684\u9c81\u68d2\u6027\u548c\u539f\u59cb\u4ee3\u7801\u6027\u80fd\u4e0a\u5747\u6709\u63d0\u5347\uff0c\u4f18\u4e8eGPT-4o\u548cDeepSeek-V3\u7b49\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "RoFTCodeSum\u6709\u6548\u63d0\u5347\u4e86\u4ee3\u7801\u603b\u7ed3\u6a21\u578b\u5728\u96be\u8bfb\u4ee3\u7801\u4e0a\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4ee3\u7801\u7406\u89e3\u63d0\u4f9b\u4e86\u66f4\u52a0\u7a33\u5065\u7684\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2601.05429", "categories": ["cs.MA", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.05429", "abs": "https://arxiv.org/abs/2601.05429", "authors": ["Levente Alekszejenk\u00f3", "Dobrowiecki Tadeusz"], "title": "On the Transition to an Auction-based Intelligent Parking Assignment System", "comment": null, "summary": "Finding a free parking space in a city has become a challenging task over the past decades. A recently proposed auction-based parking assignment can alleviate cruising for parking and also set a market-driven, demand-responsive parking price. However, the wide acceptance of such a system is far from certain.\n  To evaluate the merits of auction-based parking assignment, we assume that drivers have access to a smartphone-based reservation system prior to its mandatory introduction and thus have the opportunity to test and experience its merits voluntarily. We set our experiment as Eclipse SUMO simulations with different rates of participants and non-participants to check how different market penetration levels affect the traffic flow, the performance of the auction-based assignment system, and the financial outcomes. The results show that the auction-based system improves traffic flow with increasing penetration rates, allowing participants to park gradually closer to their preferred parking lots. However, it comes with a price; the system also increases parking expenditures for participants. Interestingly, non-participating drivers will face even higher parking prices. Consequently, they will be motivated to use the new system.", "AI": {"tldr": "\u901a\u8fc7\u4eff\u771f\u7814\u7a76\uff0c\u62cd\u5356\u5f0f\u505c\u8f66\u5206\u914d\u7cfb\u7edf\u80fd\u6539\u5584\u4ea4\u901a\u6d41\u5e76\u9a71\u52a8\u5e02\u573a\u4ef7\u683c\uff0c\u4f46\u6210\u672c\u589e\u52a0\u4fc3\u4f7f\u66f4\u591a\u53f8\u673a\u53c2\u4e0e\u3002", "motivation": "\u5bfb\u627e\u57ce\u5e02\u4e2d\u7684\u514d\u8d39\u505c\u8f66\u4f4d\u53d8\u5f97\u8d8a\u6765\u8d8a\u56f0\u96be\uff0c\u9700\u8981\u521b\u65b0\u7684\u505c\u8f66\u5206\u914d\u7cfb\u7edf\u6765\u7f13\u89e3\u505c\u8f66\u96be\u9898\u3002", "method": "\u901a\u8fc7Eclipse SUMO\u4eff\u771f\uff0c\u8003\u8651\u4e0d\u540c\u53c2\u4e0e\u7387\u4e0b\u7684\u62cd\u5356\u5f0f\u505c\u8f66\u5206\u914d\u7cfb\u7edf\uff0c\u6a21\u62df\u53f8\u673a\u4f7f\u7528\u667a\u80fd\u624b\u673a\u9884\u7ea6\u7cfb\u7edf\u7684\u573a\u666f\uff0c\u5206\u6790\u5e02\u573a\u6e17\u900f\u7387\u5bf9\u4ea4\u901a\u6d41\u3001\u7cfb\u7edf\u6027\u80fd\u53ca\u8d22\u52a1\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "result": "\u62cd\u5356\u5f0f\u505c\u8f66\u5206\u914d\u7cfb\u7edf\u80fd\u591f\u968f\u7740\u53c2\u4e0e\u7387\u7684\u589e\u52a0\u6539\u5584\u4ea4\u901a\u6d41\uff0c\u53c2\u4e0e\u8005\u80fd\u66f4\u63a5\u8fd1\u9996\u9009\u505c\u8f66\u4f4d\u505c\u8f66\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u63d0\u9ad8\u4e86\u53c2\u4e0e\u8005\u7684\u505c\u8f66\u8d39\u7528\uff0c\u4e14\u975e\u53c2\u4e0e\u8005\u7684\u505c\u8f66\u8d39\u7528\u66f4\u9ad8\uff0c\u4fc3\u4f7f\u66f4\u591a\u53f8\u673a\u503e\u5411\u4e8e\u4f7f\u7528\u8be5\u7cfb\u7edf\u3002"}}
{"id": "2601.05358", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.05358", "abs": "https://arxiv.org/abs/2601.05358", "authors": ["Tim Menzner", "Jochen L. Leidner"], "title": "The Table of Media Bias Elements: A sentence-level taxonomy of media bias types and propaganda techniques", "comment": null, "summary": "Public debates about \"left-\" or \"right-wing\" news overlook the fact that bias is usually conveyed by concrete linguistic manoeuvres that transcend any single political spectrum. We therefore shift the focus from where an outlet allegedly stands to how partiality is expressed in individual sentences. Drawing on 26,464 sentences collected from newsroom corpora, user submissions and our own browsing, we iteratively combine close-reading, interdisciplinary theory and pilot annotation to derive a fine-grained, sentence-level taxonomy of media bias and propaganda. The result is a two-tier schema comprising 38 elementary bias types, arranged in six functional families and visualised as a \"table of media-bias elements\". For each type we supply a definition, real-world examples, cognitive and societal drivers, and guidance for recognition. A quantitative survey of a random 155-sentence sample illustrates prevalence differences, while a cross-walk to the best-known NLP and communication-science taxonomies reveals substantial coverage gains and reduced ambiguity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ec6\u7c92\u5ea6\u7684\u53e5\u5b50\u7ea7\u5a92\u4f53\u504f\u89c1\u5206\u7c7b\u4f53\u7cfb\uff0c\u6db5\u76d638\u79cd\u57fa\u672c\u504f\u89c1\u7c7b\u578b\uff0c\u6a2a\u8de8\u516d\u5927\u529f\u80fd\u7c7b\u522b\uff0c\u4ee5\u66f4\u51c6\u786e\u8bc6\u522b\u548c\u7406\u89e3\u5a92\u4f53\u504f\u89c1\u3002", "motivation": "\u5f53\u524d\u5173\u4e8e\u5a92\u4f53\u5de6\u53f3\u7ffc\u504f\u89c1\u7684\u8fa9\u8bba\u5ffd\u89c6\u4e86\u504f\u89c1\u5177\u4f53\u7684\u8bed\u8a00\u8868\u8fbe\u65b9\u5f0f\uff0c\u7814\u7a76\u65e8\u5728\u8f6c\u53d8\u89c6\u89d2\uff0c\u5173\u6ce8\u5355\u53e5\u5c42\u9762\u5982\u4f55\u4f53\u73b0\u504f\u89c1\u3002", "method": "\u901a\u8fc7\u5206\u679026,464\u53e5\u65b0\u95fb\u8bed\u6599\uff0c\u7ed3\u5408\u7ec6\u8bfb\u3001\u8de8\u5b66\u79d1\u7406\u8bba\u4e0e\u8bd5\u70b9\u6ce8\u91ca\uff0c\u8fed\u4ee3\u6784\u5efa\u4e86\u4e00\u4e2a\u4e24\u5c42\u6b21\u3001\u5305\u542b38\u79cd\u504f\u89c1\u7c7b\u578b\u7684\u5a92\u4f53\u504f\u89c1\u5206\u7c7b\u65b9\u6848\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b38\u79cd\u504f\u89c1\u7c7b\u578b\u7684\u4e24\u5c42\u5206\u7c7b\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u968f\u673a\u6837\u672c\u91cf\u5316\u8c03\u67e5\u5c55\u793a\u5176\u504f\u89c1\u7c7b\u578b\u7684\u666e\u904d\u6027\uff0c\u540c\u65f6\u4e0e\u73b0\u6709NLP\u53ca\u4f20\u64ad\u79d1\u5b66\u5206\u7c7b\u4f53\u7cfb\u7684\u6620\u5c04\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u8be6\u5c3d\u7684\u5a92\u4f53\u504f\u89c1\u5143\u7d20\u8868\uff0c\u63d0\u4f9b\u504f\u89c1\u5b9a\u4e49\u3001\u5b9e\u4f8b\u53ca\u8bc6\u522b\u6307\u5bfc\uff0c\u5b9e\u73b0\u4e86\u5bf9\u73b0\u6709\u5206\u7c7b\u4f53\u7cfb\u7684\u663e\u8457\u8865\u5145\u548c\u6b67\u4e49\u51cf\u5c11\u3002"}}
{"id": "2601.05502", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05502", "abs": "https://arxiv.org/abs/2601.05502", "authors": ["Gideon Peters", "SayedHassan Khatoonabadi", "Emad Shihab"], "title": "Evaluating the Use of LLMs for Automated DOM-Level Resolution of Web Performance Issues", "comment": "Accepted to the The ACM International Conference on Mining Software Repositories (MSR) (MSR 2026)", "summary": "Users demand fast, seamless webpage experiences, yet developers often struggle to meet these expectations within tight constraints. Performance optimization, while critical, is a time-consuming and often manual process. One of the most complex tasks in this domain is modifying the Document Object Model (DOM), which is why this study focuses on it. Recent advances in Large Language Models (LLMs) offer a promising avenue to automate this complex task, potentially transforming how developers address web performance issues. This study evaluates the effectiveness of nine state-of-the-art LLMs for automated web performance issue resolution. For this purpose, we first extracted the DOM trees of 15 popular webpages (e.g., Facebook), and then we used Lighthouse to retrieve their performance audit reports. Subsequently, we passed the extracted DOM trees and corresponding audits to each model for resolution. Our study considers 7 unique audit categories, revealing that LLMs universally excel at SEO & Accessibility issues. However, their efficacy in performance-critical DOM manipulations is mixed. While high-performing models like GPT-4.1 delivered significant reductions in areas like Initial Load, Interactivity, and Network Optimization (e.g., 46.52% to 48.68% audit incidence reductions), others, such as GPT-4o-mini, notably underperformed, consistently. A further analysis of these modifications showed a predominant additive strategy and frequent positional changes, alongside regressions particularly impacting Visual Stability.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u6d4b\u4e86\u4e5d\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u4f18\u5316\u7f51\u9875\u6027\u80fd\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5b83\u4eec\u5728SEO\u548c\u65e0\u969c\u788d\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u6027\u80fd\u5173\u952e\u7684DOM\u4fee\u6539\u4e0a\u6548\u679c\u4e0d\u4e00\uff0cGPT-4.1\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u7528\u6237\u9700\u8981\u5feb\u901f\u65e0\u7f1d\u7684\u7f51\u9875\u4f53\u9a8c\uff0c\u4f46\u5f00\u53d1\u8005\u5728\u6709\u9650\u6761\u4ef6\u4e0b\u96be\u4ee5\u6ee1\u8db3\uff0c\u8fd9\u4f7f\u7f51\u9875\u6027\u80fd\u4f18\u5316\u6210\u4e3a\u5173\u952e\u4e14\u590d\u6742\u7684\u4efb\u52a1\uff0c\u5c24\u5176\u662f DOM \u4fee\u6539\u3002", "method": "\u8bc4\u4f30\u4e5d\u79cd\u6700\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u5728\u81ea\u52a8\u89e3\u51b3\u7f51\u9875\u6027\u80fd\u95ee\u9898\u4e2d\u7684\u6548\u679c\uff0c\u901a\u8fc7\u63d0\u53d615\u4e2a\u70ed\u95e8\u7f51\u9875\u7684DOM\u548c\u6027\u80fd\u62a5\u544a\uff0c\u8f93\u5165\u6a21\u578b\u8fdb\u884c\u95ee\u9898\u89e3\u51b3\u3002", "result": "\u6240\u6709\u6a21\u578b\u5728SEO\u548c\u65e0\u969c\u788d\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5173\u952e\u6027\u80fd\u76f8\u5173\u7684DOM\u64cd\u4f5c\u4e0a\u6548\u679c\u53c2\u5dee\u4e0d\u9f50\uff0c\u5176\u4e2dGPT-4.1\u8868\u73b0\u6700\u4f73\uff0c\u663e\u8457\u51cf\u5c11\u6027\u80fd\u5ba1\u8ba1\u95ee\u9898\uff0c\u800cGPT-4o-mini\u6301\u7eed\u8868\u73b0\u8f83\u5dee\u3002\u4fee\u6539\u7b56\u7565\u4e3b\u8981\u4e3a\u6dfb\u52a0\u548c\u4f4d\u7f6e\u8c03\u6574\uff0c\u4f46\u5b58\u5728\u5f71\u54cd\u89c6\u89c9\u7a33\u5b9a\u6027\u7684\u56de\u5f52\u3002", "conclusion": "\u5148\u8fdb\u7684LLMs\u5c24\u5176\u662f\u9ad8\u6027\u80fd\u6a21\u578b\uff0c\u5728\u81ea\u52a8\u4f18\u5316\u7f51\u9875\u6027\u80fd\u95ee\u9898\u4e0a\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u5c24\u5176\u662fSEO\u548c\u65e0\u969c\u788d\u9886\u57df\uff0c\u4f46\u5728\u6027\u80fd\u5173\u952e\u7684DOM\u6539\u52a8\u65b9\u9762\u4ecd\u9700\u6539\u8fdb\u3002"}}
{"id": "2601.05487", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2601.05487", "abs": "https://arxiv.org/abs/2601.05487", "authors": ["Huanxiang Lin", "Qianyue Wang", "Jinwu Hu", "Bailin Chen", "Qing Du", "Mingkui Tan"], "title": "EvidFuse: Writing-Time Evidence Learning for Consistent Text-Chart Data Reporting", "comment": null, "summary": "Data-driven reports communicate decision-relevant insights by tightly interleaving narrative text with charts grounded in underlying tables. However, current LLM-based systems typically generate narratives and visualizations in staged pipelines, following either a text-first-graph-second or a graph-first-text-second paradigm. These designs often lead to chart-text inconsistency and insight freezing, where the intermediate evidence space becomes fixed and the model can no longer retrieve or construct new visual evidence as the narrative evolves, resulting in shallow and predefined analysis. To address the limitations, we propose \\textbf{EvidFuse}, a training-free multi-agent framework that enables writing-time text-chart interleaved generation for data-driven reports. EvidFuse decouples visualization analysis from long-form drafting via two collaborating components: a \\textbf{Data-Augmented Analysis Agent}, equipped with Exploratory Data Analysis (EDA)-derived knowledge and access to raw tables, and a \\textbf{Real-Time Evidence Construction Writer} that plans an outline and drafts the report while intermittently issuing fine-grained analysis requests. This design allows visual evidence to be constructed and incorporated exactly when the narrative requires it, directly constraining subsequent claims and enabling on-demand expansion of the evidence space. Experiments demonstrate that EvidFuse attains the top rank in both LLM-as-a-judge and human evaluations on chart quality, chart-text alignment, and report-level usefulness.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86EvidFuse\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u4e2a\u534f\u4f5c\u7ec4\u4ef6\u5b9e\u73b0\u5199\u4f5c\u8fc7\u7a0b\u4e2d\u56fe\u8868\u4e0e\u6587\u672c\u7684\u5b9e\u65f6\u4ea4\u53c9\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u56fe\u8868\u6587\u672c\u4e0d\u4e00\u81f4\u548c\u5206\u6790\u505c\u6ede\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u62a5\u544a\u751f\u6210\u65b9\u6cd5\u901a\u5e38\u4e3a\u5206\u9636\u6bb5\u6d41\u7a0b\uff0c\u9020\u6210\u56fe\u6587\u4e0d\u4e00\u81f4\u53ca\u8bc1\u636e\u7a7a\u95f4\u56fa\u5b9a\uff0c\u9650\u5236\u4e86\u5206\u6790\u7684\u6df1\u5ea6\u548c\u52a8\u6001\u6269\u5c55\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u4e00\u4e2a\u6570\u636e\u589e\u5f3a\u5206\u6790\u4ee3\u7406\u5229\u7528\u63a2\u7d22\u6027\u6570\u636e\u5206\u6790\u77e5\u8bc6\u548c\u8bbf\u95ee\u539f\u59cb\u8868\u683c\uff0c\u7ed3\u5408\u4e00\u4e2a\u5b9e\u65f6\u8bc1\u636e\u6784\u9020\u5199\u4f5c\u4ee3\u7406\u89c4\u5212\u5927\u7eb2\u5e76\u95f4\u6b47\u53d1\u51fa\u7ec6\u7c92\u5ea6\u5206\u6790\u8bf7\u6c42\uff0c\u5b9e\u73b0\u6587\u672c\u548c\u56fe\u8868\u7684\u4ea4\u53c9\u751f\u6210\u3002", "result": "EvidFuse\u5728\u81ea\u52a8\u8bc4\u4f30\u548c\u4eba\u5de5\u8bc4\u4ef7\u4e2d\u5747\u4f4d\u5217\u7b2c\u4e00\uff0c\u8868\u660e\u5176\u5728\u56fe\u8868\u8d28\u91cf\u3001\u56fe\u6587\u5bf9\u9f50\u4ee5\u53ca\u62a5\u544a\u6709\u7528\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "EvidFuse\u663e\u8457\u63d0\u5347\u4e86\u56fe\u8868\u8d28\u91cf\u548c\u6587\u672c\u56fe\u8868\u5bf9\u9f50\u5ea6\uff0c\u5b9e\u73b0\u4e86\u66f4\u6df1\u5165\u548c\u52a8\u6001\u7684\u6570\u636e\u9a71\u52a8\u62a5\u544a\u751f\u6210\uff0c\u83b7\u5f97\u4e86\u6700\u4f73\u7684\u4eba\u5de5\u548c\u6a21\u578b\u8bc4\u4f30\u7ed3\u679c\u3002"}}
{"id": "2601.05366", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.05366", "abs": "https://arxiv.org/abs/2601.05366", "authors": ["Zheng Luo", "T Pranav Kutralingam", "Ogochukwu N Okoani", "Wanpeng Xu", "Hua Wei", "Xiyang Hu"], "title": "Lost in Execution: On the Multilingual Robustness of Tool Calling in Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed as agents that invoke external tools through structured function calls. While recent work reports strong tool-calling performance under standard English-centric evaluations, the robustness of tool calling under multilingual user interactions remains underexplored. In this work, we introduce MLCL, a diagnostic benchmark, and conduct a systematic evaluation of multilingual tool calling across Chinese, Hindi, and the low-resource language Igbo. Through fine-grained error analysis, we show that many failures occur despite correct intent understanding and tool selection. We identify parameter value language mismatch as a dominant failure mode, where models generate semantically appropriate parameter values in the user's language, violating language-invariant execution conventions. We further evaluate several inference-time system strategies and find that while these strategies substantially reduce language-induced execution errors, none of them can fully recover English-level performance.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faMLCL\u57fa\u51c6\uff0c\u53d1\u73b0\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8c03\u7528\u5de5\u5177\u65f6\u7684\u4e3b\u8981\u5931\u8d25\u539f\u56e0\u662f\u53c2\u6570\u8bed\u8a00\u4e0d\u5339\u914d\uff0c\u867d\u7136\u7b56\u7565\u80fd\u7f13\u89e3\u95ee\u9898\u4f46\u4ecd\u96be\u8d85\u8d8a\u82f1\u6587\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u82f1\u6587\u73af\u5883\u4e0b\u7684\u5de5\u5177\u8c03\u7528\u8868\u73b0\uff0c\u7f3a\u4e4f\u5bf9\u591a\u8bed\u8a00\u7528\u6237\u4ea4\u4e92\u4e2d\u9c81\u68d2\u6027\u7684\u63a2\u7d22\u3002", "method": "\u5f15\u5165\u4e86MLCL\u8bca\u65ad\u57fa\u51c6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8c03\u7528\u5916\u90e8\u5de5\u5177\u7684\u80fd\u529b\uff0c\u91cd\u70b9\u5206\u6790\u6a21\u578b\u5728\u4e2d\u6587\u3001\u5370\u5730\u8bed\u548c\u4f4e\u8d44\u6e90\u8bed\u79cd\u4f0a\u535a\u8bed\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u901a\u8fc7\u7ec6\u7c92\u5ea6\u9519\u8bef\u5206\u6790\uff0c\u53d1\u73b0\u5f88\u591a\u62a5\u9519\u6e90\u81ea\u53c2\u6570\u503c\u7684\u8bed\u8a00\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5373\u6a21\u578b\u751f\u6210\u4e86\u7b26\u5408\u7528\u6237\u8bed\u8a00\u4f46\u8fdd\u53cd\u6267\u884c\u8bed\u8a00\u4e0d\u53d8\u89c4\u5b9a\u7684\u53c2\u6570\u503c\u3002\u6d4b\u8bd5\u4e86\u591a\u79cd\u63a8\u7406\u65f6\u7cfb\u7edf\u7b56\u7565\uff0c\u867d\u663e\u8457\u51cf\u5c11\u4e86\u8bed\u8a00\u5f15\u8d77\u7684\u6267\u884c\u9519\u8bef\uff0c\u4f46\u5c1a\u672a\u8fbe\u5230\u82f1\u8bed\u6c34\u5e73\u7684\u6027\u80fd\u3002", "conclusion": "\u591a\u8bed\u8a00\u5de5\u5177\u8c03\u7528\u5b58\u5728\u8bed\u8a00\u53c2\u6570\u503c\u4e0d\u5339\u914d\u7684\u4e3b\u8981\u969c\u788d\uff0c\u73b0\u6709\u65b9\u6cd5\u867d\u6709\u6548\u4f46\u96be\u4ee5\u5b8c\u5168\u6062\u590d\u82f1\u8bed\u73af\u5883\u4e0b\u7684\u5de5\u5177\u8c03\u7528\u6027\u80fd\uff0c\u63d0\u793a\u672a\u6765\u9700\u52a0\u5f3a\u591a\u8bed\u8a00\u6267\u884c\u4e00\u81f4\u6027\u8bbe\u8ba1\u3002"}}
{"id": "2601.05539", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.05539", "abs": "https://arxiv.org/abs/2601.05539", "authors": ["Gou Tan", "Zilong He", "Min Li", "Pengfei Chen", "Jieke Shi", "Zhensu Sun", "Ting Zhang", "Danwen Chen", "Lwin Khin Shar", "Chuanfu Zhang", "David Lo"], "title": "LIDL: LLM Integration Defect Localization via Knowledge Graph-Enhanced Multi-Agent Analysis", "comment": null, "summary": "LLM-integrated software, which embeds or interacts with large language models (LLMs) as functional components, exhibits probabilistic and context-dependent behaviors that fundamentally differ from those of traditional software. This shift introduces a new category of integration defects that arise not only from code errors but also from misaligned interactions among LLM-specific artifacts, including prompts, API calls, configurations, and model outputs. However, existing defect localization techniques are ineffective at identifying these LLM-specific integration defects because they fail to capture cross-layer dependencies across heterogeneous artifacts, cannot exploit incomplete or misleading error traces, and lack semantic reasoning capabilities for identifying root causes.\n  To address these challenges, we propose LIDL, a multi-agent framework for defect localization in LLM-integrated software. LIDL (1) constructs a code knowledge graph enriched with LLM-aware annotations that represent interaction boundaries across source code, prompts, and configuration files, (2) fuses three complementary sources of error evidence inferred by LLMs to surface candidate defect locations, and (3) applies context-aware validation that uses counterfactual reasoning to distinguish true root causes from propagated symptoms. We evaluate LIDL on 146 real-world defect instances collected from 105 GitHub repositories and 16 agent-based systems. The results show that LIDL significantly outperforms five state-of-the-art baselines across all metrics, achieving a Top-3 accuracy of 0.64 and a MAP of 0.48, which represents a 64.1% improvement over the best-performing baseline. Notably, LIDL achieves these gains while reducing cost by 92.5%, demonstrating both high accuracy and cost efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7684LIDL\u6846\u67b6\u901a\u8fc7\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u548c\u878d\u5408\u591a\u6e90\u9519\u8bef\u7ebf\u7d22\uff0c\u7cbe\u51c6\u5b9a\u4f4d\u96c6\u6210\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u590d\u6742\u8f6f\u4ef6\u7f3a\u9677\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u6210\u672c\u5927\u5e45\u964d\u4f4e\u3002", "motivation": "\u4f20\u7edf\u8f6f\u4ef6\u7684\u7f3a\u9677\u5b9a\u4f4d\u6280\u672f\u65e0\u6cd5\u6709\u6548\u8bc6\u522b\u96c6\u6210\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u7684\u8f6f\u4ef6\u4e2d\u7684\u7279\u5b9a\u96c6\u6210\u7f3a\u9677\uff0c\u8fd9\u4e9b\u7f3a\u9677\u6e90\u81ea\u4ee3\u7801\u9519\u8bef\u4e4b\u5916\u7684LLM\u4ea4\u4e92\u9519\u8bef\uff0c\u5982\u63d0\u793a\u3001API\u8c03\u7528\u3001\u914d\u7f6e\u548c\u6a21\u578b\u8f93\u51fa\u4e4b\u95f4\u7684\u9519\u914d\u3002", "method": "\u63d0\u51faLIDL\uff0c\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u5e26\u6709LLM\u611f\u77e5\u6ce8\u91ca\u7684\u4ee3\u7801\u77e5\u8bc6\u56fe\u8c31\uff0c\u878d\u5408\u7531LLM\u63a8\u65ad\u7684\u4e09\u79cd\u9519\u8bef\u8bc1\u636e\u6765\u6e90\uff0c\u4ee5\u53ca\u91c7\u7528\u57fa\u4e8e\u53cd\u4e8b\u5b9e\u63a8\u7406\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u9a8c\u8bc1\uff0c\u4ee5\u5b9a\u4f4dLLM\u96c6\u6210\u8f6f\u4ef6\u7684\u7f3a\u9677\u3002", "result": "\u5728146\u4e2a\u771f\u5b9e\u7f3a\u9677\u6837\u672c\u4e0a\u7684\u8bc4\u6d4b\u7ed3\u679c\u663e\u793a\uff0cLIDL\u5728\u51c6\u786e\u7387\u548c\u5747\u503c\u5e73\u5747\u7cbe\u5ea6\u65b9\u9762\u5747\u663e\u8457\u4f18\u4e8e\u4e94\u4e2a\u6700\u5148\u8fdb\u57fa\u7ebf\uff0cTop-3\u51c6\u786e\u7387\u4e3a0.64\u3001MAP\u4e3a0.48\uff0c\u5206\u522b\u63d0\u534764.1%\uff1b\u4e14\u6210\u672c\u964d\u4f4e\u4e8692.5%\u3002", "conclusion": "LIDL\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u7f3a\u9677\u5b9a\u4f4d\u65b9\u6cd5\u96be\u4ee5\u8bc6\u522bLLM\u96c6\u6210\u8f6f\u4ef6\u7279\u5b9a\u7f3a\u9677\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u7387\u548c\u9ad8\u6210\u672c\u6548\u76ca\uff0c\u63d0\u5347\u4e86\u7f3a\u9677\u5b9a\u4f4d\u7684\u6548\u7387\u548c\u7cbe\u5ea6\u3002"}}
{"id": "2601.05509", "categories": ["cs.MA", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2601.05509", "abs": "https://arxiv.org/abs/2601.05509", "authors": ["Yi-Ning Weng", "Hsuan-Wei Lee"], "title": "How Exploration Breaks Cooperation in Shared-Policy Multi-Agent Reinforcement Learning", "comment": "38 pages, 9 figures", "summary": "Multi-agent reinforcement learning in dynamic social dilemmas commonly relies on parameter sharing to enable scalability. We show that in shared-policy Deep Q-Network learning, standard exploration can induce a robust and systematic collapse of cooperation even in environments where fully cooperative equilibria are stable and payoff dominant. Through controlled experiments, we demonstrate that shared DQN converges to stable but persistently low-cooperation regimes. This collapse is not caused by reward misalignment, noise, or insufficient training, but by a representational failure arising from partial observability combined with parameter coupling across heterogeneous agent states. Exploration-driven updates bias the shared representation toward locally dominant defection responses, which then propagate across agents and suppress cooperative learning. We confirm that the failure persists across network sizes, exploration schedules, and payoff structures, and disappears when parameter sharing is removed or when agents maintain independent representations. These results identify a fundamental failure mode of shared-policy MARL and establish structural conditions under which scalable learning architectures can systematically undermine cooperation. Our findings provide concrete guidance for the design of multi-agent learning systems in social and economic environments where collective behavior is critical.", "AI": {"tldr": "\u5171\u4eab\u7b56\u7565\u7684\u591a\u667a\u80fd\u4f53\u6df1\u5ea6Q\u5b66\u4e60\u5728\u52a8\u6001\u793e\u4f1a\u56f0\u5883\u4e2d\u56e0\u63a2\u7d22\u5f15\u53d1\u7684\u8868\u793a\u5931\u8d25\uff0c\u5bfc\u81f4\u5408\u4f5c\u5d29\u6e83\uff0c\u8fd9\u662f\u5171\u4eab\u53c2\u6570\u4f53\u7cfb\u7684\u6839\u672c\u5931\u6548\u6a21\u5f0f\u3002", "motivation": "\u63a2\u8ba8\u5171\u4eab\u53c2\u6570\u4e0b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u5408\u4f5c\u884c\u4e3a\u5d29\u6e83\u7684\u6839\u672c\u539f\u56e0\uff0c\u4ee5\u53ca\u52a8\u6001\u793e\u4f1a\u56f0\u5883\u4e2d\u5408\u4f5c\u96be\u4ee5\u7ef4\u6301\u7684\u673a\u5236\u3002", "method": "\u901a\u8fc7\u5171\u4eab\u7b56\u7565\u7684\u6df1\u5ea6Q\u7f51\u7edc(DQN)\u8fdb\u884c\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff0c\u5e76\u5728\u53d7\u63a7\u5b9e\u9a8c\u4e2d\u5206\u6790\u63a2\u7d22\u7b56\u7565\u5bf9\u5408\u4f5c\u7a33\u5b9a\u6027\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u5171\u4eabDQN\u5728\u6807\u51c6\u63a2\u7d22\u4e0b\u4f1a\u7cfb\u7edf\u6027\u5730\u5bfc\u81f4\u5408\u4f5c\u5d29\u6e83\uff0c\u539f\u56e0\u4e0d\u662f\u5956\u52b1\u4e0d\u5bf9\u9f50\u3001\u566a\u58f0\u6216\u8bad\u7ec3\u4e0d\u8db3\uff0c\u800c\u662f\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u4e0e\u53c2\u6570\u8026\u5408\u5bfc\u81f4\u7684\u8868\u793a\u5931\u8d25\u3002\u79fb\u9664\u53c2\u6570\u5171\u4eab\u6216\u91c7\u7528\u72ec\u7acb\u8868\u793a\u53ef\u4ee5\u6d88\u9664\u8be5\u5931\u6548\u3002", "conclusion": "\u5171\u4eab\u7b56\u7565\u4e0e\u53c2\u6570\u8026\u5408\u5728\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u53ef\u80fd\u7cfb\u7edf\u6027\u635f\u5bb3\u5408\u4f5c\uff0c\u8bbe\u8ba1\u591a\u667a\u80fd\u4f53\u5b66\u4e60\u7cfb\u7edf\u65f6\u5e94\u907f\u514d\u5b8c\u5168\u5171\u4eab\u53c2\u6570\uff0c\u91c7\u7528\u72ec\u7acb\u8868\u793a\u4ee5\u7ef4\u62a4\u5408\u4f5c\u6027\u3002"}}
{"id": "2601.05403", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05403", "abs": "https://arxiv.org/abs/2601.05403", "authors": ["Zhiwei Liu", "Yupen Cao", "Yuechen Jiang", "Mohsinul Kabir", "Polydoros Giannouris", "Chen Xu", "Ziyang Xu", "Tianlei Zhu", "Tariquzzaman Faisal", "Triantafillos Papadopoulos", "Yan Wang", "Lingfei Qian", "Xueqing Peng", "Zhuohan Xie", "Ye Yuan", "Saeed Almheiri", "Abdulrazzaq Alnajjar", "Mingbin Chen", "Harry Stuart", "Paul Thompson", "Prayag Tiwari", "Alejandro Lopez-Lira", "Xue Liu", "Jimin Huang", "Sophia Ananiadou"], "title": "Same Claim, Different Judgment: Benchmarking Scenario-Induced Bias in Multilingual Financial Misinformation Detection", "comment": "Work in progress", "summary": "Large language models (LLMs) have been widely applied across various domains of finance. Since their training data are largely derived from human-authored corpora, LLMs may inherit a range of human biases. Behavioral biases can lead to instability and uncertainty in decision-making, particularly when processing financial information. However, existing research on LLM bias has mainly focused on direct questioning or simplified, general-purpose settings, with limited consideration of the complex real-world financial environments and high-risk, context-sensitive, multilingual financial misinformation detection tasks (\\mfmd). In this work, we propose \\mfmdscen, a comprehensive benchmark for evaluating behavioral biases of LLMs in \\mfmd across diverse economic scenarios. In collaboration with financial experts, we construct three types of complex financial scenarios: (i) role- and personality-based, (ii) role- and region-based, and (iii) role-based scenarios incorporating ethnicity and religious beliefs. We further develop a multilingual financial misinformation dataset covering English, Chinese, Greek, and Bengali. By integrating these scenarios with misinformation claims, \\mfmdscen enables a systematic evaluation of 22 mainstream LLMs. Our findings reveal that pronounced behavioral biases persist across both commercial and open-source models. This project will be available at https://github.com/lzw108/FMD.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u91d1\u878d\u8bef\u4fe1\u606f\u68c0\u6d4b\u4efb\u52a1\uff0c\u63d0\u51fa\u591a\u8bed\u8a00\u591a\u573a\u666f\u7684\u884c\u4e3a\u504f\u5dee\u8bc4\u4f30\u57fa\u51c6\uff0c\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u666e\u904d\u5b58\u5728\u884c\u4e3a\u504f\u5dee\u3002", "motivation": "\u73b0\u6709\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u504f\u5dee\u7684\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u76f4\u63a5\u63d0\u95ee\u6216\u7b80\u5355\u573a\u666f\uff0c\u7f3a\u4e4f\u5bf9\u590d\u6742\u91d1\u878d\u73af\u5883\u53ca\u9ad8\u98ce\u9669\u3001\u591a\u8bed\u8a00\u91d1\u878d\u8bef\u4fe1\u606f\u68c0\u6d4b\u7684\u504f\u5dee\u5206\u6790\u3002", "method": "\u6784\u5efa\u4e09\u79cd\u57fa\u4e8e\u89d2\u8272\u3001\u4eba\u683c\u3001\u5730\u533a\u3001\u79cd\u65cf\u548c\u5b97\u6559\u7684\u91d1\u878d\u573a\u666f\uff0c\u7ed3\u5408\u591a\u8bed\u8a00\u8bef\u4fe1\u606f\u6570\u636e\u96c6\uff0c\u5bf922\u4e2a\u4e3b\u6d41\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a\\mfmdscen\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u8bed\u8a00\u91d1\u878d\u8bef\u4fe1\u606f\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u884c\u4e3a\u504f\u5dee\u3002\u901a\u8fc7\u4e0e\u91d1\u878d\u4e13\u5bb6\u5408\u4f5c\uff0c\u6784\u5efa\u4e86\u4e09\u79cd\u590d\u6742\u91d1\u878d\u573a\u666f\uff0c\u5e76\u5f00\u53d1\u4e86\u6db5\u76d6\u82f1\u8bed\u3001\u4e2d\u6587\u3001\u5e0c\u814a\u8bed\u548c\u5b5f\u52a0\u62c9\u8bed\u7684\u8bef\u4fe1\u606f\u6570\u636e\u96c6\u3002\u57fa\u51c6\u6db5\u76d622\u4e2a\u4e3b\u6d41LLMs\uff0c\u7ed3\u679c\u663e\u793a\u5546\u4e1a\u548c\u5f00\u6e90\u6a21\u578b\u5747\u5b58\u5728\u663e\u8457\u884c\u4e3a\u504f\u5dee\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u7684\u91d1\u878d\u8bef\u4fe1\u606f\u4efb\u52a1\u4e2d\u4ecd\u5b58\u5728\u663e\u8457\u884c\u4e3a\u504f\u5dee\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u6539\u8fdb\u3002"}}
{"id": "2601.05540", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.05540", "abs": "https://arxiv.org/abs/2601.05540", "authors": ["Patrick Loic Foalem", "Leuson Da Silva", "Foutse Khomh", "Ettore Merlo", "Heng Li"], "title": "Empirical Characterization of Logging Smells in Machine Learning Code", "comment": null, "summary": "\\underline{Context:} Logging is a fundamental yet complex practice in software engineering, essential for monitoring, debugging, and auditing software systems. With the increasing integration of machine learning (ML) components into software systems, effective logging has become critical to ensure reproducibility, traceability, and observability throughout model training and deployment. Although various general-purpose and ML-specific logging frameworks exist, little is known about how these tools are actually used in practice or whether ML practitioners adopt consistent and effective logging strategies. To date, no empirical study has systematically characterized recurring bad logging practices--or logging smells--in ML System. \\underline{Goal:} This study aims to empirically identify and characterize logging smells in ML systems, providing an evidence-based understanding of how logging is implemented and challenged in practice. \\underline{Method:} We propose to conduct a large-scale mining of open-source ML repositories hosted on GitHub to catalogue recurring logging smells. Subsequently, a practitioner survey involving ML engineers will be conducted to assess the perceived relevance, severity, and frequency of the identified smells. \\underline{Limitations:} % While The study's limitations include that While our findings may not be generalizable to closed-source industrial projects, we believe our study provides an essential step toward understanding and improving logging practices in ML development.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5927\u89c4\u6a21\u6316\u6398GitHub\u5f00\u6e90\u673a\u5668\u5b66\u4e60\u9879\u76ee\uff0c\u7cfb\u7edf\u8bc6\u522b\u548c\u5206\u7c7b\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u4e2d\u7684\u65e5\u5fd7\u5f02\u5473\uff08\u5373\u4e0d\u826f\u65e5\u5fd7\u5b9e\u8df5\uff09\uff0c\u5e76\u901a\u8fc7\u8c03\u67e5ML\u5de5\u7a0b\u5e08\u8bc4\u4f30\u8fd9\u4e9b\u5f02\u5473\u7684\u76f8\u5173\u6027\u3001\u4e25\u91cd\u6027\u548c\u9891\u7387\u3002", "motivation": "\u968f\u7740\u673a\u5668\u5b66\u4e60\u7ec4\u4ef6\u6574\u5408\u5230\u8f6f\u4ef6\u7cfb\u7edf\u4e2d\uff0c\u65e5\u5fd7\u5bf9\u4e8e\u6a21\u578b\u8bad\u7ec3\u548c\u90e8\u7f72\u7684\u53ef\u590d\u73b0\u6027\u3001\u53ef\u8ffd\u8e2a\u6027\u548c\u53ef\u89c2\u5bdf\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u5bf9\u4e8eML\u7cfb\u7edf\u4e2d\u7684\u65e5\u5fd7\u5b9e\u8df5\u53ca\u5176\u95ee\u9898\u7f3a\u4e4f\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u5927\u89c4\u6a21\u6316\u6398GitHub\u4e0a\u7684\u5f00\u6e90\u673a\u5668\u5b66\u4e60\u4ed3\u5e93\uff0c\u8bc6\u522b\u65e5\u5fd7\u5f02\u5473\uff1b\u968f\u540e\u901a\u8fc7ML\u5de5\u7a0b\u5e08\u8c03\u67e5\u8bc4\u4f30\u8fd9\u4e9b\u65e5\u5fd7\u5f02\u5473\u7684\u5b9e\u9645\u5f71\u54cd\u3002", "result": "\u8bc6\u522b\u51fa\u591a\u79cd\u5728ML\u7cfb\u7edf\u4e2d\u53cd\u590d\u51fa\u73b0\u7684\u65e5\u5fd7\u5f02\u5473\uff0c\u786e\u8ba4\u4e86\u8fd9\u4e9b\u95ee\u9898\u5728\u5b9e\u9645\u5f00\u53d1\u4e2d\u7684\u666e\u904d\u6027\u548c\u4e25\u91cd\u6027\uff0c\u4e14\u8fd9\u4e9b\u53d1\u73b0\u6709\u52a9\u4e8e\u6307\u5bfc\u672a\u6765\u7684\u65e5\u5fd7\u6539\u8fdb\u5de5\u4f5c\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\u4e86\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u4e2d\u5b58\u5728\u7684\u5e38\u89c1\u65e5\u5fd7\u5f02\u5473\uff0c\u5f3a\u8c03\u4e86\u6539\u8fdb\u65e5\u5fd7\u5b9e\u8df5\u7684\u91cd\u8981\u6027\uff0c\u4e3aML\u5f00\u53d1\u4e2d\u7684\u65e5\u5fd7\u7ba1\u7406\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\u3002"}}
{"id": "2601.05606", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2601.05606", "abs": "https://arxiv.org/abs/2601.05606", "authors": ["Chen Han", "Jin Tan", "Bohan Yu", "Wenzhen Zheng", "Xijin Tang"], "title": "Conformity Dynamics in LLM Multi-Agent Systems: The Roles of Topology and Self-Social Weighting", "comment": "Under Review", "summary": "Large Language Models (LLMs) are increasingly instantiated as interacting agents in multi-agent systems (MAS), where collective decisions emerge through social interaction rather than independent reasoning. A fundamental yet underexplored mechanism in this process is conformity, the tendency of agents to align their judgments with prevailing group opinions. This paper presents a systematic study of how network topology shapes conformity dynamics in LLM-based MAS through a misinformation detection task. We introduce a confidence-normalized pooling rule that controls the trade-off between self-reliance and social influence, enabling comparisons between two canonical decision paradigms: Centralized Aggregation and Distributed Consensus. Experimental results demonstrate that network topology critically governs both the efficiency and robustness of collective judgments. Centralized structures enable immediate decisions but are sensitive to hub competence and exhibit same-model alignment biases. In contrast, distributed structures promote more robust consensus, while increased network connectivity speeds up convergence but also heightens the risk of wrong-but-sure cascades, in which agents converge on incorrect decisions with high confidence. These findings characterize the conformity dynamics in LLM-based MAS, clarifying how network topology and self-social weighting jointly shape the efficiency, robustness, and failure modes of collective decision-making.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\uff0c\u7f51\u7edc\u62d3\u6251\u5982\u4f55\u5f71\u54cd\u4ece\u4f17\u884c\u4e3a\u548c\u96c6\u4f53\u51b3\u7b56\u8868\u73b0\uff0c\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u5f52\u4e00\u5316\u89c4\u5219\u6bd4\u8f83\u96c6\u4e2d\u5f0f\u548c\u5206\u5e03\u5f0f\u51b3\u7b56\uff0c\u63ed\u793a\u4e86\u7f51\u7edc\u7ed3\u6784\u5bf9\u6548\u7387\u3001\u9c81\u68d2\u6027\u53ca\u5931\u8d25\u98ce\u9669\u7684\u5173\u952e\u5f71\u54cd\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\uff0cLLMs\u901a\u8fc7\u793e\u4ea4\u4e92\u52a8\u800c\u975e\u72ec\u7acb\u63a8\u7406\u8fdb\u884c\u96c6\u4f53\u51b3\u7b56\uff0c\u7136\u800c\u4ece\u4f17\u673a\u5236\u5982\u4f55\u53d7\u7f51\u7edc\u62d3\u6251\u5f71\u54cd\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\uff0c\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u63ed\u793a\u7f51\u7edc\u7ed3\u6784\u5728\u8fd9\u4e00\u8fc7\u7a0b\u4e2d\u5bf9\u4ece\u4f17\u52a8\u6001\u53ca\u51b3\u7b56\u7ed3\u679c\u7684\u4f5c\u7528\u3002", "method": "\u8bbe\u8ba1\u4e86\u7f6e\u4fe1\u5ea6\u5f52\u4e00\u5316\u805a\u5408\u89c4\u5219\u7528\u4e8e\u63a7\u5236\u4e2a\u4f53\u81ea\u4fe1\u4e0e\u793e\u4f1a\u5f71\u54cd\u7684\u6743\u8861\uff0c\u501f\u52a9\u8bef\u4fe1\u606f\u68c0\u6d4b\u4efb\u52a1\uff0c\u6bd4\u8f83\u4e86\u96c6\u4e2d\u5f0f\u805a\u5408\u548c\u5206\u5e03\u5f0f\u5171\u8bc6\u4e24\u79cd\u51b3\u7b56\u6a21\u5f0f\u5728\u4e0d\u540c\u7f51\u7edc\u62d3\u6251\u4e0b\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u5176\u5bf9\u96c6\u4f53\u51b3\u7b56\u6548\u7387\u548c\u9c81\u68d2\u6027\u7684\u5f71\u54cd\u3002", "result": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5982\u4f55\u901a\u8fc7\u7f51\u7edc\u62d3\u6251\u7ed3\u6784\u5f71\u54cd\u4ece\u4f17\u52a8\u6001\uff0c\u7279\u522b\u662f\u5728\u8bef\u4fe1\u606f\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u7f6e\u4fe1\u5ea6\u5f52\u4e00\u5316\u805a\u5408\u89c4\u5219\uff0c\u5e73\u8861\u4e86\u4e2a\u4f53\u81ea\u4fe1\u548c\u793e\u4f1a\u5f71\u54cd\uff0c\u6bd4\u8f83\u4e86\u96c6\u4e2d\u5f0f\u805a\u5408\u4e0e\u5206\u5e03\u5f0f\u5171\u8bc6\u4e24\u79cd\u51b3\u7b56\u8303\u5f0f\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u7f51\u7edc\u7ed3\u6784\u51b3\u5b9a\u4e86\u96c6\u4f53\u5224\u65ad\u7684\u6548\u7387\u4e0e\u9c81\u68d2\u6027\u3002\u96c6\u4e2d\u5f0f\u7ed3\u6784\u505a\u51b3\u7b56\u8fc5\u901f\u4f46\u4f9d\u8d56\u5173\u952e\u8282\u70b9\u80fd\u529b\u4e14\u5b58\u5728\u6a21\u578b\u4e00\u81f4\u6027\u504f\u5dee\uff1b\u5206\u5e03\u5f0f\u7ed3\u6784\u66f4\u5065\u58ee\uff0c\u5171\u8bc6\u901f\u5ea6\u53d7\u7f51\u7edc\u8fde\u901a\u6027\u5f71\u54cd\uff0c\u4f46\u8fde\u901a\u6027\u8fc7\u9ad8\u53ef\u80fd\u5f15\u53d1\u9519\u8bef\u4e14\u81ea\u4fe1\u7684\u51b3\u7b56\u7ea7\u8054\u3002\u672c\u6587\u63ed\u793a\u4e86\u7f51\u7edc\u62d3\u6251\u548c\u4e2a\u4f53\u6743\u91cd\u5982\u4f55\u5171\u540c\u5f71\u54cd\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u96c6\u4f53\u51b3\u7b56\u6027\u80fd\u53ca\u5931\u8d25\u6a21\u5f0f\u3002", "conclusion": "\u7f51\u7edc\u62d3\u6251\u7ed3\u6784\u53ca\u4e2a\u4f53\u81ea\u4fe1\u4e0e\u793e\u4f1a\u5f71\u54cd\u7684\u6743\u91cd\u8c03\u8282\u5171\u540c\u51b3\u5b9a\u4e86LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u4ece\u4f17\u52a8\u6001\u7684\u6548\u7387\u3001\u9c81\u68d2\u6027\u4e0e\u5931\u8d25\u6a21\u5f0f\uff0c\u96c6\u4e2d\u5f0f\u51b3\u7b56\u5feb\u901f\u4f46\u6613\u53d7\u5173\u952e\u8282\u70b9\u5f71\u54cd\uff0c\u5206\u5e03\u5f0f\u51b3\u7b56\u9c81\u68d2\u6027\u66f4\u5f3a\u4f46\u8fde\u901a\u6027\u8fc7\u9ad8\u98ce\u9669\u589e\u52a0\u3002"}}
{"id": "2601.05411", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05411", "abs": "https://arxiv.org/abs/2601.05411", "authors": ["Jan \u010cern\u00fd", "Ivana Kvapil\u00edkov\u00e1", "Silvie Cinkov\u00e1"], "title": "Glitter: Visualizing Lexical Surprisal for Readability in Administrative Texts", "comment": null, "summary": "This work investigates how measuring information entropy of text can be used to estimate its readability. We propose a visualization framework that can be used to approximate information entropy of text using multiple language models and visualize the result. The end goal is to use this method to estimate and improve readability and clarity of administrative or bureaucratic texts. Our toolset is available as a libre software on https://github.com/ufal/Glitter.", "AI": {"tldr": "\u901a\u8fc7\u591a\u8bed\u8a00\u6a21\u578b\u4f30\u7b97\u6587\u672c\u4fe1\u606f\u71b5\uff0c\u53ef\u89c6\u5316\u5206\u6790\u6587\u672c\u53ef\u8bfb\u6027\uff0c\u81f4\u529b\u4e8e\u63d0\u5347\u5b98\u65b9\u6587\u672c\u7684\u6613\u8bfb\u6027\u3002", "motivation": "\u5229\u7528\u4fe1\u606f\u71b5\u6307\u6807\u6765\u8bc4\u4f30\u6587\u672c\u7684\u53ef\u8bfb\u6027\uff0c\u4ee5\u63d0\u5347\u884c\u653f\u548c\u5b98\u50da\u6587\u672c\u7684\u6e05\u6670\u5ea6\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u591a\u8bed\u8a00\u6a21\u578b\u4f30\u7b97\u6587\u672c\u4fe1\u606f\u71b5\u5e76\u8fdb\u884c\u53ef\u89c6\u5316\u7684\u6846\u67b6\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u89c6\u5316\u5de5\u5177\u96c6\uff0c\u80fd\u591f\u8fd1\u4f3c\u8ba1\u7b97\u6587\u672c\u4fe1\u606f\u71b5\uff0c\u5e76\u4ee5\u5f00\u6e90\u5f62\u5f0f\u53d1\u5e03\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8bc4\u4f30\u548c\u6539\u5584\u884c\u653f\u6587\u672c\u7684\u53ef\u8bfb\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u4fc3\u8fdb\u6587\u672c\u8868\u8fbe\u66f4\u4e3a\u6e05\u6670\u660e\u4e86\u3002"}}
{"id": "2601.05542", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.05542", "abs": "https://arxiv.org/abs/2601.05542", "authors": ["Adam Bodicoat", "Gunel Jahangirova", "Valerio Terragni"], "title": "Understanding LLM-Driven Test Oracle Generation", "comment": "Accepted for presentation at the 2nd ACM/IEEE International Conference on AI-powered Software (AIware 2025)", "summary": "Automated unit test generation aims to improve software quality while reducing the time and effort required for creating tests manually. However, existing techniques primarily generate regression oracles that predicate on the implemented behavior of the class under test. They do not address the oracle problem: the challenge of distinguishing correct from incorrect program behavior. With the rise of Foundation Models (FMs), particularly Large Language Models (LLMs), there is a new opportunity to generate test oracles that reflect intended behavior. This positions LLMs as enablers of Promptware, where software creation and testing are driven by natural-language prompts. This paper presents an empirical study on the effectiveness of LLMs in generating test oracles that expose software failures. We investigate how different prompting strategies and levels of contextual input impact the quality of LLM-generated oracles. Our findings offer insights into the strengths and limitations of LLM-based oracle generation in the FM era, improving our understanding of their capabilities and fostering future research in this area.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\uff0c\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u751f\u6210\u8f6f\u4ef6\u6d4b\u8bd5\u9884\u8a00\u673a\u4e2d\u7684\u8868\u73b0\u548c\u5f71\u54cd\u56e0\u7d20\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u81ea\u52a8\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u6280\u672f\u4e2d\u9884\u8a00\u673a\u95ee\u9898\uff0c\u5373\u5982\u4f55\u533a\u5206\u7a0b\u5e8f\u884c\u4e3a\u7684\u6b63\u786e\u4e0e\u5426\u3002", "method": "\u4f7f\u7528\u4e0d\u540c\u7684\u63d0\u793a\u7b56\u7565\u548c\u4e0a\u4e0b\u6587\u8f93\u5165\u7ea7\u522b\uff0c\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u751f\u6210\u5355\u5143\u6d4b\u8bd5\u9884\u8a00\u673a\u7684\u6548\u679c\u3002", "result": "\u53d1\u73b0\u4e0d\u540c\u7684\u63d0\u793a\u7b56\u7565\u548c\u4e0a\u4e0b\u6587\u8f93\u5165\u5f71\u54cdLLMs\u751f\u6210\u7684\u6d4b\u8bd5\u9884\u8a00\u673a\u8d28\u91cf\uff0c\u6307\u51fa\u4e86LLM\u5728\u751f\u6210\u6d4b\u8bd5\u9884\u8a00\u673a\u65b9\u9762\u7684\u4f18\u52bf\u4e0e\u5c40\u9650\u3002", "conclusion": "LLMs\u4e3a\u81ea\u52a8\u6d4b\u8bd5\u9884\u8a00\u673a\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\uff0c\u4f46\u5176\u6548\u679c\u4f9d\u8d56\u4e8e\u63d0\u793a\u8bbe\u8ba1\u548c\u4e0a\u4e0b\u6587\uff0c\u672a\u6765\u7814\u7a76\u9700\u8fdb\u4e00\u6b65\u63d0\u5347\u5176\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2601.05905", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.05905", "abs": "https://arxiv.org/abs/2601.05905", "authors": ["Haoming Xu", "Ningyuan Zhao", "Yunzhi Yao", "Weihong Xu", "Hongru Wang", "Xinle Deng", "Shumin Deng", "Jeff Z. Pan", "Huajun Chen", "Ningyu Zhang"], "title": "Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency", "comment": "Work in progress", "summary": "As Large Language Models (LLMs) are increasingly deployed in real-world settings, correctness alone is insufficient. Reliable deployment requires maintaining truthful beliefs under contextual perturbations. Existing evaluations largely rely on point-wise confidence like Self-Consistency, which can mask brittle belief. We show that even facts answered with perfect self-consistency can rapidly collapse under mild contextual interference. To address this gap, we propose Neighbor-Consistency Belief (NCB), a structural measure of belief robustness that evaluates response coherence across a conceptual neighborhood. To validate the efficiency of NCB, we introduce a new cognitive stress-testing protocol that probes outputs stability under contextual interference. Experiments across multiple LLMs show that the performance of high-NCB data is relatively more resistant to interference. Finally, we present Structure-Aware Training (SAT), which optimizes context-invariant belief structure and reduces long-tail knowledge brittleness by approximately 30%. Code will be available at https://github.com/zjunlp/belief.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u90bb\u5c45\u4e00\u81f4\u6027\u4fe1\u5ff5\uff08NCB\uff09\u4f5c\u4e3a\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8bed\u5883\u5e72\u6270\u4e0b\u4fe1\u5ff5\u7a33\u5065\u6027\u7684\u65b0\u6307\u6807\uff0c\u53d1\u73b0\u4f20\u7edf\u7684\u81ea\u6211\u4e00\u81f4\u6027\u6307\u6807\u63a9\u76d6\u4e86\u6a21\u578b\u5728\u8f7b\u5fae\u5e72\u6270\u4e0b\u5bb9\u6613\u5d29\u6e83\u7684\u95ee\u9898\u3002\u901a\u8fc7\u8ba4\u77e5\u538b\u529b\u6d4b\u8bd5\u9a8c\u8bc1\uff0cNCB\u8f83\u9ad8\u7684\u6570\u636e\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6297\u5e72\u6270\u80fd\u529b\uff0c\u4e14\u7ed3\u6784\u611f\u77e5\u8bad\u7ec3\uff08SAT\uff09\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u7684\u4fe1\u5ff5\u7a33\u5065\u6027\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5355\u7eaf\u6b63\u786e\u7387\u5df2\u4e0d\u80fd\u6ee1\u8db3\u9700\u6c42\uff0c\u6a21\u578b\u9700\u8981\u5728\u8bed\u5883\u5e72\u6270\u4e0b\u4fdd\u6301\u53ef\u4fe1\u7684\u4fe1\u5ff5\u3002\u73b0\u6709\u7684\u4e3b\u8981\u8bc4\u4ef7\u6307\u6807\uff08\u5982\u81ea\u6211\u4e00\u81f4\u6027\uff09\u672a\u80fd\u63ed\u793a\u6a21\u578b\u4fe1\u5ff5\u7684\u8106\u5f31\u6027\u3002", "method": "\u63d0\u51fa\u90bb\u5c45\u4e00\u81f4\u6027\u4fe1\u5ff5\uff08NCB\uff09\u4f5c\u4e3a\u7ed3\u6784\u5316\u4fe1\u5ff5\u7a33\u5065\u6027\u5ea6\u91cf\u6307\u6807\uff0c\u8bbe\u8ba1\u8ba4\u77e5\u538b\u529b\u6d4b\u8bd5\u534f\u8bae\u9a8c\u8bc1\u5176\u6709\u6548\u6027\uff0c\u6700\u540e\u91c7\u7528\u7ed3\u6784\u611f\u77e5\u8bad\u7ec3\uff08SAT\uff09\u4f18\u5316\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u4e0d\u53d8\u4fe1\u5ff5\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u9ad8NCB\u6570\u636e\u5bf9\u8bed\u5883\u5e72\u6270\u66f4\u5177\u62b5\u6297\u529b\uff0c\u7ed3\u6784\u611f\u77e5\u8bad\u7ec3\u65b9\u6cd5\u964d\u4f4e\u4e86\u957f\u5c3e\u77e5\u8bc6\u7684\u8106\u5f31\u6027\u7ea630%\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u8868\u73b0\u3002", "conclusion": "NCB\u80fd\u591f\u66f4\u6709\u6548\u5730\u8bc4\u4f30\u6a21\u578b\u4fe1\u5ff5\u7684\u7a33\u5065\u6027\uff0cSAT\u8bad\u7ec3\u65b9\u6cd5\u63d0\u5347\u4e86\u6a21\u578b\u5728\u957f\u5c3e\u77e5\u8bc6\u4e0a\u7684\u7a33\u5b9a\u6027\uff0c\u51cf\u5c11\u4e86\u7ea630%\u7684\u8106\u5f31\u6027\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2601.05414", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05414", "abs": "https://arxiv.org/abs/2601.05414", "authors": ["Minda Zhao", "Yilun Du", "Mengyu Wang"], "title": "Large Language Models Are Bad Dice Players: LLMs Struggle to Generate Random Numbers from Statistical Distributions", "comment": null, "summary": "As large language models (LLMs) transition from chat interfaces to integral components of stochastic pipelines across domains like educational assessment and synthetic data construction, the ability to faithfully sample from specified probability distributions has become a functional requirement rather than a theoretical curiosity. We present the first large-scale, statistically powered audit of native probabilistic sampling in frontier LLMs, benchmarking 11 models across 15 distributions. To disentangle failure modes, we employ a dual-protocol design: Batch Generation, where a model produces N=1000 samples within one response, and Independent Requests, comprising $N=1000$ stateless calls. We observe a sharp protocol asymmetry: batch generation achieves only modest statistical validity, with a 13% median pass rate, while independent requests collapse almost entirely, with 10 of 11 models passing none of the distributions. Beyond this asymmetry, we reveal that sampling fidelity degrades monotonically with distributional complexity and aggravates as the requested sampling horizon N increases. Finally, we demonstrate the propagation of these failures into downstream tasks: models fail to enforce uniform answer-position constraints in MCQ generation and systematically violate demographic targets in attribute-constrained text-to-image prompt synthesis. These findings indicate that current LLMs lack a functional internal sampler, necessitating the use of external tools for applications requiring statistical guarantees.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5927\u89c4\u6a21\u5ba1\u8ba1\u4e86\u524d\u6cbf\u5927\u8bed\u8a00\u6a21\u578b\u5728\u539f\u751f\u6982\u7387\u91c7\u6837\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6279\u91cf\u751f\u6210\u548c\u72ec\u7acb\u8bf7\u6c42\u4e24\u79cd\u534f\u8bae\u4e0b\u91c7\u6837\u51c6\u786e\u7387\u5747\u4f4e\uff0c\u4e14\u968f\u5206\u5e03\u590d\u6742\u5ea6\u548c\u91c7\u6837\u89c4\u6a21\u589e\u5927\u800c\u4e0b\u964d\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u4e8e\u6559\u80b2\u8bc4\u4f30\u3001\u5408\u6210\u6570\u636e\u751f\u6210\u7b49\u9700\u8981\u51c6\u786e\u6982\u7387\u91c7\u6837\u7684\u573a\u666f\uff0c\u8bc4\u4f30\u5176\u80fd\u5426\u4ece\u6307\u5b9a\u5206\u5e03\u51c6\u786e\u91c7\u6837\u6210\u4e3a\u5fc5\u8981\u3002", "method": "\u8bbe\u8ba1\u53cc\u534f\u8bae\u5ba1\u8ba1\u65b9\u6cd5\uff1a\u6279\u91cf\u751f\u6210\uff08\u4e00\u6b21\u751f\u62101000\u4e2a\u6837\u672c\uff09\u4e0e\u72ec\u7acb\u8bf7\u6c42\uff081000\u6b21\u72ec\u7acb\u8c03\u7528\uff09\uff0c\u5bf911\u4e2a\u6a21\u578b\u572815\u79cd\u4e0d\u540c\u5206\u5e03\u4e0a\u8fdb\u884c\u91c7\u6837\u51c6\u786e\u6027\u6d4b\u8bd5\u3002", "result": "\u6279\u91cf\u751f\u6210\u901a\u8fc7\u7387\u4e2d\u7b49\uff0813%\uff09\uff0c\u72ec\u7acb\u8bf7\u6c42\u51e0\u4e4e\u5168\u90e8\u5931\u8d25\uff0810/11\u6a21\u578b\u672a\u901a\u8fc7\u4efb\u4f55\u5206\u5e03\u6d4b\u8bd5\uff09\uff1b\u91c7\u6837\u51c6\u786e\u7387\u968f\u5206\u5e03\u590d\u6742\u5ea6\u548c\u6837\u672c\u6570\u91cf\u589e\u52a0\u800c\u4e0b\u964d\uff0c\u4e14\u5728\u591a\u9879\u4efb\u52a1\uff08MCQ\u751f\u6210\u3001\u6587\u672c\u5230\u56fe\u50cf\uff09\u4e2d\u91c7\u6837\u9519\u8bef\u4f20\u5bfc\u5bfc\u81f4\u7ed3\u679c\u504f\u5dee\u3002", "conclusion": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u6709\u6548\u7684\u5185\u90e8\u91c7\u6837\u673a\u5236\uff0c\u65e0\u6cd5\u4fdd\u8bc1\u6982\u7387\u5206\u5e03\u91c7\u6837\u7684\u7edf\u8ba1\u51c6\u786e\u6027\uff0c\u9700\u501f\u52a9\u5916\u90e8\u5de5\u5177\u4ee5\u6ee1\u8db3\u7edf\u8ba1\u6027\u80fd\u8981\u6c42\u3002"}}
{"id": "2601.05555", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.05555", "abs": "https://arxiv.org/abs/2601.05555", "authors": ["Patrick Loic Foalem", "Foutse Khomh", "Leuson Da Silva", "Ettore Merlo"], "title": "An Empirical Study of Policy-as-Code Adoption in Open-Source Software Projects", "comment": null, "summary": "\\textbf{Context:} Policy-as-Code (PaC) has become a foundational approach for embedding governance, compliance, and security requirements directly into software systems. While organizations increasingly adopt PaC tools, the software engineering community lacks an empirical understanding of how these tools are used in real-world development practices.\n  \\textbf{Objective:} This paper aims to bridge this gap by conducting the first large-scale study of PaC usage in open-source software. Our goal is to characterize how PaC tools are adopted, what purposes they serve, and what governance activities they support across diverse software ecosystems.\n  \\textbf{Method:} We analyzed 399 GitHub repositories using nine widely adopted PaC tools. Our mixed-methods approach combines quantitative analysis of tool usage and project characteristics with a qualitative investigation of policy files. We further employ a Large Language Model (LLM)--assisted classification pipeline, refined through expert validation, to derive a taxonomy of PaC usage consisting of 5 categories and 15 sub-categories.\n  \\textbf{Results:} Our study reveals substantial diversity in PaC adoption. PaC tools are frequently used in early-stage projects and are heavily oriented toward governance, configuration control, and documentation. We also observe emerging PaC usage in MLOps pipelines and strong co-usage patterns, such as between OPA and Gatekeeper. Our taxonomy highlights recurring governance intents.\n  \\textbf{Conclusion:} Our findings offer actionable insights for practitioners and tool developers. They highlight concrete usage patterns, emphasize actual PaC usage, and motivate opportunities for improving tool interoperability. This study lays the empirical foundation for future research on PaC practices and their role in ensuring trustworthy, compliant software systems.", "AI": {"tldr": "\u901a\u8fc7\u5927\u89c4\u6a21\u7814\u7a76\uff0c\u63ed\u793a\u4e86PaC\u5de5\u5177\u7684\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3001\u76ee\u7684\u548c\u6cbb\u7406\u6d3b\u52a8\uff0c\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\u548c\u6539\u8fdb\u5de5\u5177\u4e92\u64cd\u4f5c\u6027\u7684\u673a\u4f1a\u3002", "motivation": "\u5f53\u524d\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\u7f3a\u4e4f\u5bf9PaC\u5de5\u5177\u5728\u5b9e\u9645\u5f00\u53d1\u4e2d\u7684\u5e94\u7528\u7684\u5b9e\u8bc1\u7406\u89e3\u3002", "method": "\u5206\u6790399\u4e2aGitHub\u4ed3\u5e93\uff0c\u91c7\u75289\u79cd\u6d41\u884c\u7684\u653f\u7b56\u5373\u4ee3\u7801\uff08PaC\uff09\u5de5\u5177\uff0c\u7ed3\u5408\u5b9a\u91cf\u548c\u5b9a\u6027\u65b9\u6cd5\uff0c\u5e76\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8f85\u52a9\u5206\u7c7b\u3002", "result": "\u53d1\u73b0PaC\u5de5\u5177\u5728\u65e9\u671f\u9879\u76ee\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4e3b\u8981\u7528\u4e8e\u6cbb\u7406\u3001\u914d\u7f6e\u63a7\u5236\u548c\u6587\u6863\uff0c\u8fd8\u5728MLOps\u7ba1\u9053\u4e2d\u51fa\u73b0\uff0c\u63ed\u793a\u4e86\u5f3a\u70c8\u7684\u5171\u7528\u6a21\u5f0f\u548c\u6cbb\u7406\u610f\u56fe\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5b9e\u8df5\u8005\u548c\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u5177\u4f53\u7684\u4f7f\u7528\u6a21\u5f0f\u548c\u6539\u8fdb\u5efa\u8bae\uff0c\u63a8\u52a8\u672a\u6765PaC\u7814\u7a76\u4e0e\u53ef\u4fe1\u5408\u89c4\u8f6f\u4ef6\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.05930", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.05930", "abs": "https://arxiv.org/abs/2601.05930", "authors": ["Jingsheng Zheng", "Jintian Zhang", "Yujie Luo", "Yuren Mao", "Yunjun Gao", "Lun Du", "Huajun Chen", "Ningyu Zhang"], "title": "Can We Predict Before Executing Machine Learning Agents?", "comment": "Work in progress", "summary": "Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7528LLMs\u9884\u6d4b\u89e3\u51b3\u65b9\u6848\u4f18\u5148\u7ea7\uff0c\u7ed5\u8fc7\u9ad8\u6210\u672c\u7269\u7406\u9a8c\u8bc1\uff0c\u663e\u8457\u63d0\u5347\u81ea\u52a8\u5316\u79d1\u5b66\u53d1\u73b0\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u5316\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u7269\u7406\u6267\u884c\u8fdb\u884c\u5047\u8bbe\u8bc4\u4f30\uff0c\u5b58\u5728\u6267\u884c\u74f6\u9888\uff0c\u9650\u5236\u4e86\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cdData-centric Solution Preference\u4efb\u52a1\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u901a\u8fc7\u9884\u6d4b-\u9a8c\u8bc1\u5faa\u73af\uff08Predict-then-Verify loop\uff09\u66ff\u4ee3\u4f20\u7edf\u7684\u7269\u7406\u6267\u884c\u74f6\u9888\uff0c\u52a0\u5feb\u79d1\u5b66\u53d1\u73b0\u8fc7\u7a0b\u3002", "result": "\u901a\u8fc7\u6784\u5efa\u5305\u542b18,438\u5bf9\u6bd4\u6570\u636e\u7684\u8bed\u6599\u5e93\uff0c\u9a8c\u8bc1\u4e86LLMs\u5728\u9884\u6d4b\u89e3\u51b3\u65b9\u6848\u4f18\u52a3\u4e0a\u7684\u80fd\u529b\uff0c\u6a21\u578b\u51c6\u786e\u7387\u8fbe61.5%\uff0c\u5e76\u5728\u4ee3\u7406FOREAGENT\u4e2d\u5b9e\u73b0\u4e866\u500d\u52a0\u901f\u548c6%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9884\u6d4b\u80fd\u529b\u66ff\u4ee3\u7269\u7406\u6267\u884c\u74f6\u9888\uff0c\u5b9e\u73b0\u4e86\u79d1\u5b66\u53d1\u73b0\u8fc7\u7a0b\u7684\u52a0\u901f\u4e0e\u6027\u80fd\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86\u9884\u6d4b-\u9a8c\u8bc1\u6846\u67b6\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2601.05437", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05437", "abs": "https://arxiv.org/abs/2601.05437", "authors": ["Chenxiao Yu", "Bowen Yi", "Farzan Karimi-Malekabadi", "Suhaib Abdurahman", "Jinyi Ye", "Shrikanth Narayanan", "Yue Zhao", "Morteza Dehghani"], "title": "Tracing Moral Foundations in Large Language Models", "comment": null, "summary": "Large language models (LLMs) often produce human-like moral judgments, but it is unclear whether this reflects an internal conceptual structure or superficial ``moral mimicry.'' Using Moral Foundations Theory (MFT) as an analytic framework, we study how moral foundations are encoded, organized, and expressed within two instruction-tuned LLMs: Llama-3.1-8B-Instruct and Qwen2.5-7B-Instruct. We employ a multi-level approach combining (i) layer-wise analysis of MFT concept representations and their alignment with human moral perceptions, (ii) pretrained sparse autoencoders (SAEs) over the residual stream to identify sparse features that support moral concepts, and (iii) causal steering interventions using dense MFT vectors and sparse SAE features. We find that both models represent and distinguish moral foundations in a structured, layer-dependent way that aligns with human judgments. At a finer scale, SAE features show clear semantic links to specific foundations, suggesting partially disentangled mechanisms within shared representations. Finally, steering along either dense vectors or sparse features produces predictable shifts in foundation-relevant behavior, demonstrating a causal connection between internal representations and moral outputs. Together, our results provide mechanistic evidence that moral concepts in LLMs are distributed, layered, and partly disentangled, suggesting that pluralistic moral structure can emerge as a latent pattern from the statistical regularities of language alone.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u9053\u5fb7\u5224\u65ad\u57fa\u4e8e\u5185\u5728\u590d\u6742\u7684\u591a\u5c42\u6b21\u7ed3\u6784\uff0c\u5177\u5907\u4e00\u5b9a\u7684\u8ba4\u77e5\u7279\u6027\uff0c\u800c\u975e\u4ec5\u4ec5\u9053\u5fb7\u6a21\u4eff\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ea7\u751f\u7c7b\u4eba\u9053\u5fb7\u5224\u65ad\u80cc\u540e\u7684\u5185\u90e8\u7ed3\u6784\uff0c\u533a\u5206\u771f\u5b9e\u8ba4\u77e5\u7ed3\u6784\u4e0e\u8868\u9762\u6a21\u4eff\u3002", "method": "\u7ed3\u5408\u5206\u5c42\u5206\u6790\u3001\u9884\u8bad\u7ec3\u7a00\u758f\u81ea\u7f16\u7801\u5668\u8bc6\u522b\u5173\u952e\u7279\u5f81\u4ee5\u53ca\u56e0\u679c\u5e72\u9884\uff0c\u7814\u7a76\u4e86\u4e24\u79cd\u6307\u4ee4\u8c03\u4f18\u7684LLM\u4e2d\u9053\u5fb7\u57fa\u7840\u7684\u8868\u8fbe\u548c\u7ec4\u7ec7\u65b9\u5f0f\u3002", "result": "\u4e24\u79cd\u6a21\u578b\u5728\u4e0d\u540c\u5c42\u6b21\u4e0a\u8868\u73b0\u51fa\u5bf9\u5e94\u4eba\u7c7b\u9053\u5fb7\u611f\u77e5\u7684\u7ed3\u6784\u5316\u9053\u5fb7\u57fa\u7840\uff0c\u4e14\u901a\u8fc7\u7a00\u758f\u7279\u5f81\u53d1\u73b0\u90e8\u5206\u89e3\u8026\u673a\u5236\uff0c\u5e72\u9884\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5185\u5728\u8868\u793a\u4e0e\u9053\u5fb7\u884c\u4e3a\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u9053\u5fb7\u6982\u5ff5\u662f\u5206\u5e03\u5f0f\u3001\u591a\u5c42\u6b21\u4e14\u90e8\u5206\u89e3\u5f00\u8026\u5408\u7684\uff0c\u4f53\u73b0\u4e86\u590d\u6742\u7684\u5185\u90e8\u7ed3\u6784\u800c\u975e\u7b80\u5355\u7684\u6a21\u4eff\u3002"}}
{"id": "2601.05617", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.05617", "abs": "https://arxiv.org/abs/2601.05617", "authors": ["Omar Abedelkader", "St\u00e9phane Ducasse", "Oleksandr Zaitsev", "Romain Robbes", "Guillermo Polito"], "title": "Package-Aware Approach for Repository-Level Code Completion in Pharo", "comment": null, "summary": "Pharo offers a sophisticated completion engine based on semantic heuristics, which coordinates specific fetchers within a lazy architecture. These heuristics can be recomposed to support various activities (e.g., live programming or history usage navigation). While this system is powerful, it does not account for the repository structure when suggesting global names such as class names, class variables, or global variables. As a result, it does not prioritize classes within the same package or project, treating all global names equally. In this paper, we present a new heuristic that addresses this limitation. Our approach searches variable names in a structured manner: it begins with the package of the requesting class, then expands to other packages within the same repository, and finally considers the global namespace. We describe the logic behind this heuristic and evaluate it against the default semantic heuristic and one that directly queries the global namespace. Preliminary results indicate that the Mean Reciprocal Rank (MRR) improves, confirming that package-awareness completions deliver more accurate and relevant suggestions than the previous flat global approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u57fa\u4e8e\u5305\u611f\u77e5\u7684\u53d8\u91cf\u540d\u79f0\u8865\u5168\u542f\u53d1\u5f0f\uff0c\u63d0\u5347\u4e86\u8865\u5168\u7cfb\u7edf\u7684\u51c6\u786e\u6027\uff0c\u66f4\u597d\u5730\u5229\u7528\u4e86\u4ee3\u7801\u4ed3\u5e93\u7684\u7ed3\u6784\u4fe1\u606f\u3002", "motivation": "\u73b0\u6709\u7684\u8865\u5168\u5f15\u64ce\u672a\u80fd\u5145\u5206\u5229\u7528\u4ee3\u7801\u4ed3\u5e93\u7684\u5305\u7ed3\u6784\u4fe1\u606f\uff0c\u5bfc\u81f4\u5168\u5c40\u540d\u79f0\u88ab\u5e73\u7b49\u5bf9\u5f85\uff0c\u964d\u4f4e\u4e86\u8865\u5168\u5efa\u8bae\u7684\u76f8\u5173\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5305\u7ed3\u6784\u611f\u77e5\u7684\u542f\u53d1\u5f0f\u641c\u7d22\u65b9\u6cd5\uff0c\u4f18\u5148\u641c\u7d22\u8bf7\u6c42\u7c7b\u6240\u5728\u5305\u7684\u53d8\u91cf\u540d\uff0c\u7136\u540e\u6269\u5c55\u5230\u540c\u4ed3\u5e93\u5176\u4ed6\u5305\uff0c\u6700\u540e\u8003\u8651\u5168\u5c40\u547d\u540d\u7a7a\u95f4\u3002", "result": "\u57fa\u4e8e\u8be5\u65b9\u6cd5\u7684\u8865\u5168\u7b56\u7565\u5728\u5e73\u5747\u5012\u6570\u6392\u540d\uff08MRR\uff09\u6307\u6807\u4e0a\u4f18\u4e8e\u9ed8\u8ba4\u7684\u8bed\u4e49\u542f\u53d1\u5f0f\u548c\u5168\u5c40\u547d\u540d\u7a7a\u95f4\u67e5\u8be2\u65b9\u6cd5\u3002", "conclusion": "\u5f15\u5165\u5305\u611f\u77e5\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u5347\u81ea\u52a8\u8865\u5168\u7684\u51c6\u786e\u6027\u548c\u76f8\u5173\u6027\u3002"}}
{"id": "2601.05459", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05459", "abs": "https://arxiv.org/abs/2601.05459", "authors": ["Hongjin Kim", "Jaewook Lee", "Kiyoung Lee", "Jong-hun Shin", "Soojong Lim", "Oh-Woog Kwon"], "title": "Do LLMs Need Inherent Reasoning Before Reinforcement Learning? A Study in Korean Self-Correction", "comment": "IJCNLP-AACL 2025 (Main), Outstanding Paper Award", "summary": "Large Language Models (LLMs) demonstrate strong reasoning and self-correction abilities in high-resource languages like English, but their performance remains limited in low-resource languages such as Korean. In this study, we investigate whether reinforcement learning (RL) can enhance Korean reasoning abilities to a degree comparable to English. Our findings reveal that RL alone yields limited improvements when applied to models lacking inherent Korean reasoning capabilities. To address this, we explore several fine-tuning strategies and show that aligning the model's internal reasoning processes with Korean inputs-particularly by tuning Korean-specific neurons in early layers-is key to unlocking RL's effectiveness. We introduce a self-correction code-switching dataset to facilitate this alignment and observe significant performance gains in both mathematical reasoning and self-correction tasks. Ultimately, we conclude that the crucial factor in multilingual reasoning enhancement is not injecting new linguistic knowledge, but effectively eliciting and aligning existing reasoning capabilities. Our study provides a new perspective on how internal translation and neuron-level tuning contribute to multilingual reasoning alignment in LLMs.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u97e9\u8bed\u63a8\u7406\u80fd\u529b\u63d0\u5347\u5173\u952e\u5728\u4e8e\u6a21\u578b\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\u7684\u5bf9\u9f50\uff0c\u5f3a\u5316\u5b66\u4e60\u8f85\u4ee5\u7279\u5b9a\u795e\u7ecf\u5143\u8c03\u4f18\u548c\u81ea\u7ea0\u6b63\u6570\u636e\u96c6\u80fd\u663e\u8457\u63d0\u5347\u591a\u8bed\u8a00\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u63a2\u7a76\u5f3a\u5316\u5b66\u4e60\u662f\u5426\u80fd\u63d0\u5347\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u97e9\u8bed\uff09\u4e0a\u7684\u63a8\u7406\u80fd\u529b\uff0c\u8fbe\u5230\u4e0e\u9ad8\u8d44\u6e90\u8bed\u8a00\uff08\u82f1\u8bed\uff09\u76f8\u5f53\u7684\u6c34\u5e73\u3002", "method": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\u4e13\u95e8\u8c03\u4f18\u97e9\u8bed\u7279\u5b9a\u795e\u7ecf\u5143\uff0c\u5e76\u5f15\u5165\u4ee3\u7801\u5207\u6362\u7684\u81ea\u7ea0\u6b63\u6570\u636e\u96c6\uff0c\u5b9e\u73b0\u6a21\u578b\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\u4e0e\u97e9\u8bed\u8f93\u5165\u7684\u5bf9\u9f50\u3002", "result": "\u5f3a\u5316\u5b66\u4e60\u5355\u72ec\u5e94\u7528\u6548\u679c\u6709\u9650\uff0c\u901a\u8fc7\u8c03\u4f18\u7279\u5b9a\u795e\u7ecf\u5143\u548c\u5bf9\u9f50\u5185\u90e8\u63a8\u7406\u6d41\u7a0b\u663e\u8457\u63d0\u5347\u4e86\u6570\u5b66\u63a8\u7406\u548c\u81ea\u7ea0\u6b63\u4efb\u52a1\u7684\u6027\u80fd\u3002", "conclusion": "\u5728\u591a\u8bed\u8a00\u63a8\u7406\u80fd\u529b\u63d0\u5347\u4e2d\uff0c\u5173\u952e\u4e0d\u5728\u4e8e\u6ce8\u5165\u65b0\u7684\u8bed\u8a00\u77e5\u8bc6\uff0c\u800c\u662f\u6709\u6548\u6fc0\u53d1\u548c\u5bf9\u9f50\u6a21\u578b\u5df2\u6709\u7684\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2601.05622", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.05622", "abs": "https://arxiv.org/abs/2601.05622", "authors": ["Chengjie Wang", "Jingzheng Wu", "Hao Lyu", "Xiang Ling", "Tianyue Luo", "Yanjun Wu", "Chen Zhao"], "title": "A Large Scale Empirical Analysis on the Adherence Gap between Standards and Tools in SBOM", "comment": "35 pages, 4 figures, 18 tables. Accepted by TOSEM", "summary": "A Software Bill of Materials (SBOM) is a machine-readable artifact that systematically organizes software information, enhancing supply chain transparency and security. To facilitate the exchange and utilization of SBOMs, organizations such as the Linux Foundation and OWASP have proposed SBOM standards. Following standards, organizations have developed tools for generating and utilizing SBOMs. However, limited research has examined the adherence of these SBOM tools to standard specifications, a gap that could lead to compliance failures and disruptions in SBOM utilization. This paper presents the first large-scale, two-stage empirical analysis of the adherence gap, using our automated evaluation framework, SAP. The evaluation, comprising a baseline evaluation and a one-year longitudinal follow-up, covers 55,444 SBOMs generated by six SBOM tools from 3,287 real-world repositories. Our analysis reveals persistent, fundamental limitations in current SBOM tools: (1) inadequate compliance support with policy requirements; (2) poor tool consistencies, including inter-tool consistency rates as low as 7.84% to 12.77% for package detection across languages, and significant longitudinal inconsistency, where tools show low consistency with their own prior versions; and (3) mediocre to poor accuracy for detailed software information, e.g., accuracy of package licenses below 20%. We analyze the root causes of these gaps and provide practical solutions. All the code, replication docker image, evaluation results are open sourced at [GitHub](https://github.com/dw763j/SAP) and [Zenodo](https://doi.org/10.5281/zenodo.14998624) for further researches.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u5927\u89c4\u6a21\u5b9e\u8bc1\u5206\u6790\u4e86SBOM\u5de5\u5177\u5bf9\u6807\u51c6\u7684\u9075\u5b88\u60c5\u51b5\uff0c\u63ed\u793a\u5176\u5b58\u5728\u663e\u8457\u5408\u89c4\u548c\u7a33\u5b9a\u6027\u4e0d\u8db3\uff0c\u63d0\u51fa\u6839\u672c\u539f\u56e0\u5206\u6790\u53ca\u5b9e\u9645\u6539\u8fdb\u65b9\u6848\u3002", "motivation": "\u5c3d\u7ba1\u5df2\u6709SBOM\u6807\u51c6\u548c\u5de5\u5177\uff0c\u9488\u5bf9SBOM\u5de5\u5177\u5bf9\u6807\u51c6\u89c4\u8303\u7684\u9075\u5b88\u60c5\u51b5\u5374\u5c11\u6709\u7814\u7a76\uff0c\u5b58\u5728\u5408\u89c4\u7f3a\u5931\u548cSBOM\u5e94\u7528\u4e2d\u65ad\u7684\u98ce\u9669\u3002", "method": "\u901a\u8fc7\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6SAP\uff0c\u91c7\u7528\u5927\u89c4\u6a21\u4e24\u9636\u6bb5\u5b9e\u8bc1\u5206\u6790\u65b9\u6cd5\uff0c\u5bf96\u4e2aSBOM\u5de5\u5177\u751f\u6210\u768455,444\u4e2aSBOM\u6587\u4ef6\u8fdb\u884c\u57fa\u4e8e\u6807\u51c6\u89c4\u8303\u7684\u5408\u89c4\u6027\u8bc4\u4f30\uff0c\u5305\u62ec\u57fa\u7ebf\u8bc4\u4f30\u548c\u4e00\u5e74\u671f\u7eb5\u5411\u8ddf\u8e2a\u3002", "result": "\u53d1\u73b0\u5f53\u524dSBOM\u5de5\u5177\u5b58\u5728\u6839\u672c\u6027\u4e0d\u8db3\uff1a(1) \u5bf9\u7b56\u7565\u5408\u89c4\u6027\u652f\u6301\u4e0d\u5145\u5206\uff1b(2) \u5de5\u5177\u95f4\u548c\u5de5\u5177\u81ea\u8eab\u7248\u672c\u95f4\u4e00\u81f4\u6027\u8f83\u4f4e\uff0c\u5305\u68c0\u6d4b\u4e00\u81f4\u7387\u6700\u4f4e\u4ec5\u4e3a7.84%-12.77%\uff1b(3) \u7ec6\u8282\u8f6f\u4ef6\u4fe1\u606f\u51c6\u786e\u7387\u8f83\u5dee\uff0c\u5982\u5305\u8bb8\u53ef\u8bc1\u51c6\u786e\u7387\u4f4e\u4e8e20%\u3002", "conclusion": "\u5f53\u524d\u4e3b\u6d41SBOM\u5de5\u5177\u5728\u9075\u5b88\u6807\u51c6\u89c4\u8303\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u4e9f\u9700\u6539\u8fdb\u5408\u89c4\u652f\u6301\u548c\u5de5\u5177\u4e00\u81f4\u6027\uff0c\u63a8\u52a8SBOM\u7684\u53ef\u9760\u5e94\u7528\u3002"}}
{"id": "2601.05473", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.05473", "abs": "https://arxiv.org/abs/2601.05473", "authors": ["Zhihao Yuan", "Yunze Xiao", "Ming Li", "Weihao Xuan", "Richard Tong", "Mona Diab", "Tom Mitchell"], "title": "Towards Valid Student Simulation with Large Language Models", "comment": null, "summary": "This paper presents a conceptual and methodological framework for large language model (LLM) based student simulation in educational settings. The authors identify a core failure mode, termed the \"competence paradox\" in which broadly capable LLMs are asked to emulate partially knowledgeable learners, leading to unrealistic error patterns and learning dynamics. To address this, the paper reframes student simulation as a constrained generation problem governed by an explicit Epistemic State Specification (ESS), which defines what a simulated learner can access, how errors are structured, and how learner state evolves over time. The work further introduces a Goal-by-Environment framework to situate simulated student systems according to behavioral objectives and deployment contexts. Rather than proposing a new system or benchmark, the paper synthesizes prior literature, formalizes key design dimensions, and articulates open challenges related to validity, evaluation, and ethical risks. Overall, the paper argues for epistemic fidelity over surface realism as a prerequisite for using LLM-based simulated students as reliable scientific and pedagogical instruments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u8ba4\u77e5\u72b6\u6001\u89c4\u8303\u7684\u5b66\u751f\u6a21\u62df\u6846\u67b6\uff0c\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u6a21\u62df\u5b66\u751f\u7684\u2018\u80fd\u529b\u6096\u8bba\u2019\uff0c\u5f3a\u8c03\u8ba4\u77e5\u771f\u5b9e\u611f\u4ee5\u63d0\u9ad8\u6a21\u62df\u7684\u79d1\u5b66\u548c\u6559\u5b66\u4ef7\u503c\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u6a21\u62df\u5b66\u751f\u65f6\u51fa\u73b0\u7684\u2018\u80fd\u529b\u6096\u8bba\u2019\u95ee\u9898\uff0c\u5373\u6a21\u578b\u8fc7\u4e8e\u5f3a\u5927\u5374\u6a21\u62df\u90e8\u5206\u77e5\u8bc6\u7684\u5b66\u751f\uff0c\u5bfc\u81f4\u9519\u8bef\u6a21\u5f0f\u548c\u5b66\u4e60\u52a8\u6001\u4e0d\u771f\u5b9e\u3002", "method": "\u5c06\u5b66\u751f\u6a21\u62df\u91cd\u6784\u4e3a\u53d7\u7ea6\u675f\u751f\u6210\u95ee\u9898\uff0c\u901a\u8fc7\u660e\u786e\u7684\u8ba4\u77e5\u72b6\u6001\u89c4\u8303\uff08Epistemic State Specification, ESS\uff09\u63a7\u5236\u6a21\u62df\u5b66\u751f\u7684\u77e5\u8bc6\u8bbf\u95ee\u3001\u9519\u8bef\u7ed3\u6784\u548c\u72b6\u6001\u6f14\u53d8\uff0c\u5f15\u5165\u76ee\u6807-\u73af\u5883\u6846\u67b6\u660e\u786e\u884c\u4e3a\u76ee\u6807\u548c\u90e8\u7f72\u73af\u5883\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6982\u5ff5\u548c\u65b9\u6cd5\u6846\u67b6\uff0c\u89c4\u8303\u6a21\u62df\u5b66\u751f\u7684\u8bbe\u8ba1\u7ef4\u5ea6\uff0c\u5f3a\u8c03\u77e5\u8bc6\u72b6\u6001\u7684\u771f\u5b9e\u518d\u73b0\uff0c\u7cfb\u7edf\u603b\u7ed3\u4e86\u76f8\u5173\u6587\u732e\uff0c\u6307\u51fa\u4e86\u6548\u5ea6\u3001\u8bc4\u4f30\u548c\u4f26\u7406\u98ce\u9669\u7b49\u5f00\u653e\u95ee\u9898\u3002", "conclusion": "\u76f8\u6bd4\u8868\u9762\u73b0\u5b9e\u611f\uff0c\u8ba4\u77e5\u72b6\u6001\u7684\u771f\u5b9e\u518d\u73b0\u662f\u4f7f\u7528\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6a21\u62df\u5b66\u751f\u4f5c\u4e3a\u79d1\u5b66\u548c\u6559\u5b66\u5de5\u5177\u7684\u5173\u952e\u524d\u63d0\u3002"}}
{"id": "2601.05663", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.05663", "abs": "https://arxiv.org/abs/2601.05663", "authors": ["Gianmario Voria", "Moses Openja", "Foutse Khomh", "Gemma Catolino", "Fabio Palomba"], "title": "Tracing Stereotypes in Pre-trained Transformers: From Biased Neurons to Fairer Models", "comment": null, "summary": "The advent of transformer-based language models has reshaped how AI systems process and generate text. In software engineering (SE), these models now support diverse activities, accelerating automation and decision-making. Yet, evidence shows that these models can reproduce or amplify social biases, raising fairness concerns. Recent work on neuron editing has shown that internal activations in pre-trained transformers can be traced and modified to alter model behavior. Building on the concept of knowledge neurons, neurons that encode factual information, we hypothesize the existence of biased neurons that capture stereotypical associations within pre-trained transformers. To test this hypothesis, we build a dataset of biased relations, i.e., triplets encoding stereotypes across nine bias types, and adapt neuron attribution strategies to trace and suppress biased neurons in BERT models. We then assess the impact of suppression on SE tasks. Our findings show that biased knowledge is localized within small neuron subsets, and suppressing them substantially reduces bias with minimal performance loss. This demonstrates that bias in transformers can be traced and mitigated at the neuron level, offering an interpretable approach to fairness in SE.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u5143\u7f16\u8f91\u7684\u65b9\u6cd5\uff0c\u5728BERT\u6a21\u578b\u4e2d\u5b9a\u4f4d\u5e76\u6291\u5236\u504f\u89c1\u795e\u7ecf\u5143\uff0c\u5b9e\u73b0\u5bf9\u504f\u89c1\u7684\u6709\u6548\u7f13\u89e3\u4e14\u7ef4\u6301\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u63d0\u5347\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u4e2dAI\u7cfb\u7edf\u7684\u516c\u5e73\u6027\u3002", "motivation": "\u53d8\u538b\u5668\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u5b58\u5728\u590d\u5236\u548c\u653e\u5927\u793e\u4f1a\u504f\u89c1\u7684\u95ee\u9898\uff0c\u4e9f\u9700\u6709\u6548\u53ef\u89e3\u91ca\u7684\u504f\u89c1\u68c0\u6d4b\u4e0e\u7f13\u89e3\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u5305\u542b\u4e5d\u79cd\u504f\u89c1\u7c7b\u578b\u7684\u504f\u89c1\u5173\u7cfb\u6570\u636e\u96c6\uff0c\u91c7\u7528\u795e\u7ecf\u5143\u5f52\u56e0\u7b56\u7565\u8ffd\u8e2a\u5e76\u6291\u5236BERT\u6a21\u578b\u4e2d\u7684\u504f\u89c1\u795e\u7ecf\u5143\uff0c\u5e76\u8bc4\u4f30\u5176\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u504f\u89c1\u77e5\u8bc6\u96c6\u4e2d\u5728\u5c11\u6570\u795e\u7ecf\u5143\uff0c\u901a\u8fc7\u6291\u5236\u8fd9\u4e9b\u795e\u7ecf\u5143\u53ef\u663e\u8457\u51cf\u5c11\u6a21\u578b\u504f\u89c1\uff0c\u540c\u65f6\u5bf9SE\u4efb\u52a1\u6027\u80fd\u5f71\u54cd\u6709\u9650\u3002", "conclusion": "\u504f\u89c1\u77e5\u8bc6\u5728\u53d8\u538b\u5668\u6a21\u578b\u7684\u5c11\u6570\u795e\u7ecf\u5143\u4e2d\u5c40\u90e8\u5316\uff0c\u6291\u5236\u8fd9\u4e9b\u795e\u7ecf\u5143\u80fd\u663e\u8457\u51cf\u5c11\u504f\u89c1\u4e14\u6027\u80fd\u635f\u5931\u5f88\u5c0f\uff0c\u8868\u660e\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u53ef\u901a\u8fc7\u795e\u7ecf\u5143\u5c42\u9762\u8ffd\u8e2a\u548c\u7f13\u89e3\u504f\u89c1\u3002"}}
{"id": "2601.05478", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05478", "abs": "https://arxiv.org/abs/2601.05478", "authors": ["Herun Wan", "Jiaying Wu", "Minnan Luo", "Fanxiao Li", "Zhi Zeng", "Min-Yen Kan"], "title": "The Facade of Truth: Uncovering and Mitigating LLM Susceptibility to Deceptive Evidence", "comment": null, "summary": "To reliably assist human decision-making, LLMs must maintain factual internal beliefs against misleading injections. While current models resist explicit misinformation, we uncover a fundamental vulnerability to sophisticated, hard-to-falsify evidence. To systematically probe this weakness, we introduce MisBelief, a framework that generates misleading evidence via collaborative, multi-round interactions among multi-role LLMs. This process mimics subtle, defeasible reasoning and progressive refinement to create logically persuasive yet factually deceptive claims. Using MisBelief, we generate 4,800 instances across three difficulty levels to evaluate 7 representative LLMs. Results indicate that while models are robust to direct misinformation, they are highly sensitive to this refined evidence: belief scores in falsehoods increase by an average of 93.0\\%, fundamentally compromising downstream recommendations. To address this, we propose Deceptive Intent Shielding (DIS), a governance mechanism that provides an early warning signal by inferring the deceptive intent behind evidence. Empirical results demonstrate that DIS consistently mitigates belief shifts and promotes more cautious evidence evaluation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u53d1\u73b0LLMs\u5bf9\u590d\u6742\u8bef\u5bfc\u8bc1\u636e\u5b58\u5728\u663e\u8457\u8106\u5f31\u6027\uff0c\u63d0\u51faMisBelief\u6846\u67b6\u7528\u4e8e\u751f\u6210\u6b64\u7c7b\u8bef\u5bfc\u8bc1\u636e\uff0c\u5e76\u8bbe\u8ba1DIS\u673a\u5236\u6709\u6548\u9632\u8303\u3002", "motivation": "\u4e3a\u4e86\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8f85\u52a9\u4eba\u7c7b\u51b3\u7b56\u65f6\u7684\u4e8b\u5b9e\u4fe1\u5ff5\u575a\u5b9a\u6027\uff0c\u907f\u514d\u6a21\u578b\u56e0\u590d\u6742\u4e14\u96be\u4ee5\u53cd\u9a73\u7684\u8bef\u5bfc\u8bc1\u636e\u800c\u4ea7\u751f\u9519\u8bef\u5224\u65ad\uff0c\u4fdd\u969c\u51b3\u7b56\u8d28\u91cf\uff0c\u5fc5\u987b\u7cfb\u7edf\u63a2\u7a76\u5e76\u9632\u8303\u6b64\u7c7b\u8106\u5f31\u6027\u3002", "method": "\u63d0\u51faMisBelief\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u89d2\u8272\u3001\u591a\u8f6e\u6b21\u534f\u4f5c\u63a8\u7406\u751f\u6210\u903b\u8f91\u4e0a\u6709\u8bf4\u670d\u529b\u4f46\u5b9e\u9645\u4e0a\u8bef\u5bfc\u7684\u8bc1\u636e\uff1b\u5e76\u8bbe\u8ba1DIS\u6cbb\u7406\u673a\u5236\uff0c\u901a\u8fc7\u8bc6\u522b\u6f5c\u5728\u6b3a\u9a97\u610f\u56fe\u6765\u6291\u5236\u8bef\u4fe1\u548c\u4fe1\u5ff5\u8f6c\u53d8\u3002", "result": "\u8be5\u8bba\u6587\u63ed\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u9762\u5bf9\u590d\u6742\u4e14\u96be\u4ee5\u53cd\u9a73\u7684\u8bef\u5bfc\u6027\u8bc1\u636e\u65f6\u5b58\u5728\u7684\u6839\u672c\u8106\u5f31\u6027\u3002\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aMisBelief\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u89d2\u8272\u3001\u591a\u8f6e\u6b21\u7684\u534f\u4f5c\u4ea4\u4e92\u751f\u6210\u5177\u6709\u903b\u8f91\u8bf4\u670d\u529b\u4f46\u4e8b\u5b9e\u8bef\u5bfc\u7684\u8bc1\u636e\u3002\u901a\u8fc7\u8be5\u6846\u67b6\uff0c\u751f\u6210\u4e864800\u4e2a\u4e0d\u540c\u96be\u5ea6\u7ea7\u522b\u7684\u6d4b\u8bd5\u5b9e\u4f8b\uff0c\u8bc4\u4f30\u4e867\u4e2a\u4ee3\u8868\u6027LLMs\uff0c\u7ed3\u679c\u663e\u793a\u6a21\u578b\u5bf9\u76f4\u63a5\u9519\u8bef\u4fe1\u606f\u7684\u62b5\u6297\u529b\u8f83\u5f3a\uff0c\u4f46\u5bf9\u8bef\u5bfc\u6027\u7ec6\u5316\u8bc1\u636e\u6781\u4e3a\u654f\u611f\uff0c\u8bef\u4fe1\u865a\u5047\u4fe1\u606f\u7684\u6982\u7387\u589e\u52a093%\uff0c\u4e25\u91cd\u5f71\u54cd\u540e\u7eed\u63a8\u8350\u3002\u9488\u5bf9\u8fd9\u4e00\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u6b3a\u9a97\u610f\u56fe\u5c4f\u853d\uff08DIS\uff09\u673a\u5236\uff0c\u8be5\u673a\u5236\u901a\u8fc7\u63a8\u65ad\u8bc1\u636e\u80cc\u540e\u7684\u6b3a\u9a97\u610f\u56fe\u63d0\u4f9b\u9884\u8b66\u4fe1\u53f7\uff0c\u6709\u6548\u51cf\u7f13\u4e86\u4fe1\u5ff5\u8f6c\u53d8\uff0c\u4fc3\u8fdb\u4e86\u66f4\u8c28\u614e\u7684\u8bc1\u636e\u8bc4\u4f30\u3002", "conclusion": "LLMs\u867d\u80fd\u62b5\u6297\u76f4\u63a5\u9519\u8bef\u4fe1\u606f\uff0c\u4f46\u6613\u53d7\u590d\u6742\u8bef\u5bfc\u6027\u8bc1\u636e\u5f71\u54cd\uff0cDIS\u673a\u5236\u53ef\u663e\u8457\u51cf\u7f13\u8fd9\u4e00\u8106\u5f31\u6027\uff0c\u63d0\u5347\u6a21\u578b\u7684\u4e8b\u5b9e\u575a\u5b9a\u6027\u548c\u51b3\u7b56\u53ef\u9760\u6027\u3002"}}
{"id": "2601.05685", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.05685", "abs": "https://arxiv.org/abs/2601.05685", "authors": ["Mingfei Cheng", "Lionel Briand", "Yuan Zhou"], "title": "Drivora: A Unified and Extensible Infrastructure for Search-based Autonomous Driving Testing", "comment": null, "summary": "Search-based testing is critical for evaluating the safety and reliability of autonomous driving systems (ADSs). However, existing approaches are often built on heterogeneous frameworks (e.g., distinct scenario spaces, simulators, and ADSs), which require considerable effort to reuse and adapt across different settings. To address these challenges, we present Drivora, a unified and extensible infrastructure for search-based ADS testing built on the widely used CARLA simulator. Drivora introduces a unified scenario definition, OpenScenario, that specifies scenarios using low-level, actionable parameters to ensure compatibility with existing methods while supporting extensibility to new testing designs (e.g., multi-autonomous-vehicle testing). On top of this, Drivora decouples the testing engine, scenario execution, and ADS integration. The testing engine leverages evolutionary computation to explore new scenarios and supports flexible customization of core components. The scenario execution can run arbitrary scenarios using a parallel execution mechanism that maximizes hardware utilization for large-scale batch simulation. For ADS integration, Drivora provides access to 12 ADSs through a unified interface, streamlining configuration and simplifying the incorporation of new ADSs. Our tools are publicly available at https://github.com/MingfeiCheng/Drivora.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Drivora\uff0c\u4e00\u4e2a\u57fa\u4e8eCARLA\u6a21\u62df\u5668\u7684\u7edf\u4e00\u4e14\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\uff08ADS\uff09\u641c\u7d22\u6d4b\u8bd5\u57fa\u7840\u8bbe\u65bd\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u573a\u666f\u5b9a\u4e49\u3001\u8fdb\u5316\u8ba1\u7b97\u9a71\u52a8\u7684\u6d4b\u8bd5\u5f15\u64ce\u3001\u5e76\u884c\u6267\u884c\u673a\u5236\u53ca\u7edf\u4e00\u7684ADS\u63a5\u53e3\uff0c\u5b9e\u73b0\u4e86\u591a\u6837\u5316\u573a\u666f\u6d4b\u8bd5\u4e0e\u5927\u89c4\u6a21\u6279\u91cf\u6a21\u62df\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u6d4b\u8bd5\u65b9\u6cd5\u591a\u6837\u4e14\u6846\u67b6\u5f02\u6784\uff0c\u5bfc\u81f4\u96be\u4ee5\u590d\u7528\u548c\u9002\u914d\uff0c\u4e9f\u9700\u4e00\u4e2a\u7edf\u4e00\u4e14\u53ef\u6269\u5c55\u7684\u6d4b\u8bd5\u57fa\u7840\u8bbe\u65bd\u4ee5\u652f\u6301\u591a\u6837\u5316\u7684\u6d4b\u8bd5\u9700\u6c42\u3002", "method": "\u91c7\u7528\u7edf\u4e00\u7684OpenScenario\u573a\u666f\u5b9a\u4e49\uff0c\u57fa\u4e8e\u8fdb\u5316\u8ba1\u7b97\u7684\u641c\u7d22\u6d4b\u8bd5\u5f15\u64ce\uff0c\u652f\u6301\u5e76\u884c\u573a\u666f\u6267\u884c\u4ee5\u63d0\u9ad8\u786c\u4ef6\u5229\u7528\u7387\uff0c\u5e76\u901a\u8fc7\u7edf\u4e00\u63a5\u53e3\u96c6\u621012\u4e2a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u3002", "result": "\u5f00\u53d1\u4e86\u57fa\u4e8eCARLA\u7684Drivora\u5e73\u53f0\uff0c\u652f\u6301\u591a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u548c\u591a\u8f66\u8f86\u6d4b\u8bd5\uff0c\u63d0\u4f9b\u7075\u6d3b\u7684\u7ec4\u4ef6\u5b9a\u5236\u548c\u9ad8\u6548\u7684\u5e76\u884c\u6a21\u62df\u6267\u884c\uff0c\u5de5\u5177\u5df2\u5f00\u6e90\u4f9b\u793e\u533a\u4f7f\u7528\u3002", "conclusion": "Drivora\u6210\u529f\u5b9e\u73b0\u4e86\u4e00\u4e2a\u7edf\u4e00\u3001\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u6d4b\u8bd5\u5e73\u53f0\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6d4b\u8bd5\u6846\u67b6\u5f02\u6784\u5bfc\u81f4\u7684\u9002\u914d\u548c\u91cd\u7528\u96be\u9898\uff0c\u6709\u52a9\u4e8e\u63d0\u5347ADS\u6d4b\u8bd5\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2601.05488", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05488", "abs": "https://arxiv.org/abs/2601.05488", "authors": ["Zhiyu Shen", "Ziming Wu", "Fuming Lai", "Shaobing Lian", "Yanghui Rao"], "title": "MemBuilder: Reinforcing LLMs for Long-Term Memory Construction via Attributed Dense Rewards", "comment": "19 pages (9 main + 10 appendix), 7 figures, 3 tables", "summary": "Maintaining consistency in long-term dialogues remains a fundamental challenge for LLMs, as standard retrieval mechanisms often fail to capture the temporal evolution of historical states. While memory-augmented frameworks offer a structured alternative, current systems rely on static prompting of closed-source models or suffer from ineffective training paradigms with sparse rewards. We introduce MemBuilder, a reinforcement learning framework that trains models to orchestrate multi-dimensional memory construction with attributed dense rewards. MemBuilder addresses two key challenges: (1) Sparse Trajectory-Level Rewards: we employ synthetic session-level question generation to provide dense intermediate rewards across extended trajectories; and (2) Multi-Dimensional Memory Attribution: we introduce contribution-aware gradient weighting that scales policy updates based on each component's downstream impact. Experimental results show that MemBuilder enables a 4B-parameter model to outperform state-of-the-art closed-source baselines, exhibiting strong generalization across long-term dialogue benchmarks.", "AI": {"tldr": "\u63d0\u51faMemBuilder\u6846\u67b6\uff0c\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u591a\u7ef4\u8bb0\u5fc6\u6784\u5efa\uff0c\u901a\u8fc7\u5bc6\u96c6\u5956\u52b1\u548c\u8d21\u732e\u611f\u77e5\u68af\u5ea6\u6743\u91cd\uff0c\u63d0\u5347\u957f\u5bf9\u8bdd\u4e00\u81f4\u6027\u3002", "motivation": "\u5f53\u524d\u957f\u5bf9\u8bdd\u4e2d\u4fdd\u6301\u4e00\u81f4\u6027\u662fLLM\u7684\u57fa\u672c\u6311\u6218\uff0c\u73b0\u6709\u7684\u68c0\u7d22\u673a\u5236\u96be\u4ee5\u6355\u6349\u5386\u53f2\u72b6\u6001\u7684\u65f6\u95f4\u6f14\u53d8\u3002", "method": "MemBuilder\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff0c\u91c7\u7528\u5408\u6210\u4f1a\u8bdd\u7ea7\u95ee\u9898\u751f\u6210\u63d0\u4f9b\u5bc6\u96c6\u4e2d\u95f4\u5956\u52b1\uff0c\u5e76\u5229\u7528\u8d21\u732e\u611f\u77e5\u68af\u5ea6\u6743\u91cd\u8c03\u6574\u7b56\u7565\u66f4\u65b0\uff0c\u4f18\u5316\u591a\u7ef4\u8bb0\u5fc6\u6784\u5efa\u3002", "result": "MemBuilder\u4f7f\u5f97\u4e00\u4e2a4B\u53c2\u6570\u7684\u6a21\u578b\u8d85\u8fc7\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u7684\u95ed\u6e90\u6a21\u578b\uff0c\u5728\u957f\u5bf9\u8bdd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "MemBuilder\u901a\u8fc7\u89e3\u51b3\u5956\u52b1\u7a00\u758f\u548c\u591a\u7ef4\u8bb0\u5fc6\u5f52\u56e0\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u957f\u5bf9\u8bdd\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2601.05703", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.05703", "abs": "https://arxiv.org/abs/2601.05703", "authors": ["Wiebe Vandendriessche", "Jordi Thijsman", "Laurens D'hooge", "Bruno Volckaert", "Merlijn Sebrechts"], "title": "AIBoMGen: Generating an AI Bill of Materials for Secure, Transparent, and Compliant Model Training", "comment": "Accepted at ACM/IEEE CAIN 2026", "summary": "The rapid adoption of complex AI systems has outpaced the development of tools to ensure their transparency, security, and regulatory compliance. In this paper, the AI Bill of Materials (AIBOM), an extension of the Software Bill of Materials (SBOM), is introduced as a standardized, verifiable record of trained AI models and their environments. Our proof-of-concept platform, AIBoMGen, automates the generation of signed AIBOMs by capturing datasets, model metadata, and environment details during training. The training platform acts as a neutral, third-party observer and root of trust. It enforces verifiable AIBOM creation for every job. The system uses cryptographic hashing, digital signatures, and in-toto attestations to ensure integrity and protect against threats such as artifact tampering by dishonest model creators. Our evaluation demonstrates that AIBoMGen reliably detects unauthorized modifications to all artifacts and can generate AIBOMs with negligible performance overhead. These results highlight the potential of AIBoMGen as a foundational step toward building secure and transparent AI ecosystems, enabling compliance with regulatory frameworks like the EUs AI Act.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u57fa\u4e8eSBOM\u6269\u5c55\u7684AIBOM\u53ca\u5176\u81ea\u52a8\u751f\u6210\u5e73\u53f0AIBoMGen\uff0c\u4fdd\u8bc1AI\u6a21\u578b\u548c\u73af\u5883\u8bb0\u5f55\u7684\u900f\u660e\u6027\u548c\u5b89\u5168\u6027\uff0c\u52a9\u529b\u6ee1\u8db3\u76d1\u7ba1\u5408\u89c4\u8981\u6c42\u3002", "motivation": "\u590d\u6742AI\u7cfb\u7edf\u5feb\u901f\u666e\u53ca\u5bfc\u81f4\u900f\u660e\u5ea6\u3001\u5b89\u5168\u6027\u548c\u5408\u89c4\u6027\u5de5\u5177\u4e0d\u8db3\uff0c\u6025\u9700\u6807\u51c6\u5316\u3001\u53ef\u9a8c\u8bc1\u7684AI\u6a21\u578b\u53ca\u5176\u73af\u5883\u8bb0\u5f55\u3002", "method": "\u63d0\u51fa\u4e86AIBOM\uff08AI Bill of Materials\uff09\uff0c\u57fa\u4e8eSBOM\u7684\u6269\u5c55\uff0c\u8bbe\u8ba1\u4e86AIBoMGen\u5e73\u53f0\u81ea\u52a8\u751f\u6210\u5e26\u7b7e\u540d\u7684AIBOM\uff0c\u91c7\u7528\u52a0\u5bc6\u54c8\u5e0c\u3001\u6570\u5b57\u7b7e\u540d\u548cin-toto\u8ba4\u8bc1\u4fdd\u8bc1\u4fe1\u606f\u5b8c\u6574\u6027\u3002", "result": "AIBoMGen\u80fd\u591f\u53ef\u9760\u68c0\u6d4b\u672a\u6388\u6743\u4fee\u6539\uff0c\u751f\u6210AIBOM\u65f6\u6027\u80fd\u5f00\u9500\u6781\u5c0f\u3002", "conclusion": "AIBoMGen\u4e3a\u6784\u5efa\u5b89\u5168\u900f\u660e\u7684AI\u751f\u6001\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63a8\u52a8\u5b9e\u73b0\u50cf\u6b27\u76dfAI\u6cd5\u6848\u8fd9\u6837\u7684\u5408\u89c4\u76d1\u7ba1\u76ee\u6807\u3002"}}
{"id": "2601.05505", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05505", "abs": "https://arxiv.org/abs/2601.05505", "authors": ["Yubo Hou", "Zhisheng Chen", "Tao Wan", "Zengchang Qin"], "title": "FlashMem: Distilling Intrinsic Latent Memory via Computation Reuse", "comment": null, "summary": "The stateless architecture of Large Language Models inherently lacks the mechanism to preserve dynamic context, compelling agents to redundantly reprocess history to maintain long-horizon autonomy. While latent memory offers a solution, current approaches are hindered by architectural segregation, relying on auxiliary encoders that decouple memory from the reasoning backbone. We propose FlashMem, a framework that distills intrinsic memory directly from transient reasoning states via computation reuse. Leveraging the property that internal representations uniquely encode input trajectories, FlashMem identifies the last hidden state as a sufficient statistic for the interaction history. This enables a Shared-KV Consolidator to synthesize memory by attending directly to the backbone's frozen cache, eliminating redundant re-parameterization. Furthermore, a parameter-free Cognitive Monitor leverages attention entropy to adaptively trigger consolidation only when high epistemic uncertainty is detected. Experiments demonstrate that FlashMem matches the performance of heavy baselines while reducing inference latency by 5 times, effectively bridging the gap between efficiency and persistent cognition.", "AI": {"tldr": "FlashMem\u901a\u8fc7\u76f4\u63a5\u5229\u7528\u63a8\u7406\u72b6\u6001\u63d0\u53d6\u5185\u5b58\uff0c\u63d0\u5347\u63a8\u7406\u6548\u7387\u4e94\u500d\uff0c\u5b9e\u73b0\u6301\u4e45\u8bb0\u5fc6\u4e0e\u63a8\u7406\u9aa8\u5e72\u7684\u65e0\u7f1d\u7ed3\u5408\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u65e0\u72b6\u6001\u67b6\u6784\u7f3a\u4e4f\u52a8\u6001\u4e0a\u4e0b\u6587\u7684\u6301\u4e45\u673a\u5236\uff0c\u5bfc\u81f4\u5728\u957f\u65f6\u7a0b\u81ea\u4e3b\u7ba1\u7406\u4e2d\u9700\u8981\u53cd\u590d\u91cd\u65b0\u5904\u7406\u5386\u53f2\u4fe1\u606f\uff0c\u6548\u7387\u4f4e\u4e0b\u3002\u73b0\u6709\u7684\u6f5c\u5728\u8bb0\u5fc6\u65b9\u6cd5\u5b58\u5728\u67b6\u6784\u9694\u79bb\u95ee\u9898\uff0c\u5185\u5b58\u4e0e\u63a8\u7406\u9aa8\u5e72\u5206\u79bb\u3002", "method": "\u63d0\u51faFlashMem\u6846\u67b6\uff0c\u901a\u8fc7\u8ba1\u7b97\u91cd\u7528\u4ece\u77ac\u65f6\u63a8\u7406\u72b6\u6001\u4e2d\u63d0\u53d6\u5185\u5728\u8bb0\u5fc6\u3002\u5229\u7528\u5185\u90e8\u8868\u793a\u552f\u4e00\u7f16\u7801\u8f93\u5165\u8f68\u8ff9\u7684\u7279\u6027\uff0c\u5c06\u6700\u540e\u7684\u9690\u85cf\u72b6\u6001\u89c6\u4e3a\u4ea4\u4e92\u5386\u53f2\u7684\u5145\u5206\u7edf\u8ba1\u91cf\u3002\u91c7\u7528\u5171\u4eab\u952e\u503c\u6574\u5408\u5668\u76f4\u63a5\u8bbf\u95ee\u9aa8\u5e72\u6a21\u578b\u7684\u51bb\u7ed3\u7f13\u5b58\uff0c\u907f\u514d\u5197\u4f59\u53c2\u6570\u91cd\u6784\uff1b\u901a\u8fc7\u65e0\u53c2\u6570\u8ba4\u77e5\u76d1\u63a7\u5668\u57fa\u4e8e\u6ce8\u610f\u529b\u71b5\u81ea\u9002\u5e94\u89e6\u53d1\u8bb0\u5fc6\u6574\u5408\u3002", "result": "FlashMem\u5b9e\u73b0\u4e86\u4e0e\u5927\u578b\u57fa\u7ebf\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u63a8\u7406\u5ef6\u8fdf\u51cf\u5c11\u4e94\u500d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\uff0c\u6210\u529f\u5728\u6548\u7387\u548c\u6301\u7eed\u8ba4\u77e5\u4e4b\u95f4\u5efa\u7acb\u6865\u6881\u3002", "conclusion": "FlashMem\u6709\u6548\u89e3\u51b3\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u8bb0\u5fc6\u548c\u63a8\u7406\u9aa8\u5e72\u5206\u79bb\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u9ad8\u6548\u7684\u8bb0\u5fc6\u6574\u5408\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u4e0e\u4f4e\u5ef6\u8fdf\u7684\u5e73\u8861\uff0c\u63a8\u52a8\u4e86\u957f\u65f6\u7a0b\u81ea\u4e3b\u7ba1\u7406\u80fd\u529b\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.05721", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.05721", "abs": "https://arxiv.org/abs/2601.05721", "authors": ["Daniel P\u00f6ttgen", "Mersedeh Sadeghi", "Max Unterbusch", "Andreas Vogelsang"], "title": "From Issues to Insights: RAG-based Explanation Generation from Software Engineering Artifacts", "comment": "Accepted at NLBSE 2026, Rio de Janeiro, Brazil", "summary": "The increasing complexity of modern software systems has made understanding their behavior increasingly challenging, driving the need for explainability to improve transparency and user trust. Traditional documentation is often outdated or incomplete, making it difficult to derive accurate, context-specific explanations. Meanwhile, issue-tracking systems capture rich and continuously updated development knowledge, but their potential for explainability remains untapped. With this work, we are the first to apply a Retrieval-Augmented Generation (RAG) approach for generating explanations from issue-tracking data. Our proof-of-concept system is implemented using open-source tools and language models, demonstrating the feasibility of leveraging structured issue data for explanation generation. Evaluating our approach on an exemplary project's set of GitHub issues, we achieve 90% alignment with human-written explanations. Additionally, our system exhibits strong faithfulness and instruction adherence, ensuring reliable and grounded explanations. These findings suggest that RAG-based methods can extend explainability beyond black-box ML models to a broader range of software systems, provided that issue-tracking data is available - making system behavior more accessible and interpretable.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5229\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\uff0c\u4ece\u95ee\u9898\u8ffd\u8e2a\u7cfb\u7edf\u6570\u636e\u751f\u6210\u8f6f\u4ef6\u884c\u4e3a\u89e3\u91ca\uff0c\u5b9e\u9a8c\u663e\u793a\u751f\u6210\u89e3\u91ca\u9ad8\u6548\u4e14\u53ef\u4fe1\uff0c\u63d0\u5347\u4e86\u8f6f\u4ef6\u7cfb\u7edf\u7684\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u4ee3\u8f6f\u4ef6\u7cfb\u7edf\u65e5\u76ca\u590d\u6742\uff0c\u7406\u89e3\u5176\u884c\u4e3a\u53d8\u5f97\u66f4\u52a0\u56f0\u96be\uff0c\u4f20\u7edf\u6587\u6863\u5e38\u5e38\u8fc7\u65f6\u6216\u4e0d\u5b8c\u6574\uff0c\u96be\u4ee5\u63d0\u4f9b\u51c6\u786e\u7684\u4e0a\u4e0b\u6587\u89e3\u91ca\uff0c\u800c\u95ee\u9898\u8ffd\u8e2a\u7cfb\u7edf\u4e2d\u8574\u542b\u4e30\u5bcc\u4e14\u6301\u7eed\u66f4\u65b0\u7684\u5f00\u53d1\u77e5\u8bc6\uff0c\u5374\u672a\u88ab\u5145\u5206\u5229\u7528\u4e8e\u89e3\u91ca\u751f\u6210\u3002", "method": "\u9996\u6b21\u5c06\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u65b9\u6cd5\u5e94\u7528\u4e8e\u4ece\u95ee\u9898\u8ffd\u8e2a\u6570\u636e\u4e2d\u751f\u6210\u89e3\u91ca\uff0c\u6784\u5efa\u4e86\u57fa\u4e8e\u5f00\u6e90\u5de5\u5177\u548c\u8bed\u8a00\u6a21\u578b\u7684\u6982\u5ff5\u9a8c\u8bc1\u7cfb\u7edf\u3002", "result": "\u5728GitHub\u9879\u76ee\u7684\u95ee\u9898\u96c6\u4e2d\u8bc4\u4f30\uff0c\u751f\u6210\u7684\u89e3\u91ca\u4e0e\u4eba\u5de5\u7f16\u5199\u7684\u89e3\u91ca\u670990%\u7684\u5339\u914d\u5ea6\uff0c\u7cfb\u7edf\u5c55\u73b0\u51fa\u8f83\u5f3a\u7684\u53ef\u4fe1\u5ea6\u548c\u5bf9\u6307\u4ee4\u7684\u9075\u4ece\u6027\u3002", "conclusion": "\u57fa\u4e8eRAG\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5229\u7528\u95ee\u9898\u8ffd\u8e2a\u6570\u636e\u6269\u5c55\u8f6f\u4ef6\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4f7f\u7cfb\u7edf\u884c\u4e3a\u66f4\u6613\u4e8e\u7406\u89e3\u548c\u89e3\u91ca\uff0c\u4e14\u9002\u7528\u8303\u56f4\u8d85\u8d8a\u4f20\u7edf\u7684\u9ed1\u76d2\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002"}}
{"id": "2601.05520", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05520", "abs": "https://arxiv.org/abs/2601.05520", "authors": ["Xuemei Tang", "Chengxi Yan", "Jinghang Gu", "Chu-Ren Huang"], "title": "CHisAgent: A Multi-Agent Framework for Event Taxonomy Construction in Ancient Chinese Cultural Systems", "comment": "22 pages, 13 figures, 7 tables", "summary": "Despite strong performance on many tasks, large language models (LLMs) show limited ability in historical and cultural reasoning, particularly in non-English contexts such as Chinese history. Taxonomic structures offer an effective mechanism to organize historical knowledge and improve understanding. However, manual taxonomy construction is costly and difficult to scale. Therefore, we propose \\textbf{CHisAgent}, a multi-agent LLM framework for historical taxonomy construction in ancient Chinese contexts. CHisAgent decomposes taxonomy construction into three role-specialized stages: a bottom-up \\textit{Inducer} that derives an initial hierarchy from raw historical corpora, a top-down \\textit{Expander} that introduces missing intermediate concepts using LLM world knowledge, and an evidence-guided \\textit{Enricher} that integrates external structured historical resources to ensure faithfulness. Using the \\textit{Twenty-Four Histories}, we construct a large-scale, domain-aware event taxonomy covering politics, military, diplomacy, and social life in ancient China. Extensive reference-free and reference-based evaluations demonstrate improved structural coherence and coverage, while further analysis shows that the resulting taxonomy supports cross-cultural alignment.", "AI": {"tldr": "\u63d0\u51fa\u4e86CHisAgent\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u65b9\u6cd5\u81ea\u52a8\u6784\u5efa\u4e2d\u56fd\u53e4\u4ee3\u5386\u53f2\u4e8b\u4ef6\u5206\u7c7b\uff0c\u63d0\u5347\u4e86\u5206\u7c7b\u7684\u7ed3\u6784\u6027\u548c\u5b8c\u6574\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e2d\u56fd\u5386\u53f2\u7b49\u975e\u82f1\u8bed\u8bed\u5883\u4e0b\u7684\u63a8\u7406\u80fd\u529b\u6709\u9650\uff0c\u624b\u5de5\u6784\u5efa\u5386\u53f2\u5206\u7c7b\u6210\u672c\u9ad8\u4e14\u96be\u6269\u5c55\uff0c\u6545\u63d0\u51fa\u81ea\u52a8\u5316\u591a\u667a\u80fd\u4f53\u6846\u67b6\u4ee5\u63d0\u5347\u5386\u53f2\u77e5\u8bc6\u7ec4\u7ec7\u6548\u80fd\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e09\u4e2a\u89d2\u8272\u5206\u5de5\u7684\u9636\u6bb5\uff1a(1)\u5e95\u5c42\u8bf1\u5bfc\u8005\u4ece\u539f\u59cb\u8bed\u6599\u4e2d\u751f\u6210\u521d\u59cb\u5c42\u7ea7\u7ed3\u6784\uff1b(2)\u9876\u5c42\u6269\u5c55\u8005\u5229\u7528LLM\u4e16\u754c\u77e5\u8bc6\u8865\u5145\u4e2d\u95f4\u6982\u5ff5\uff1b(3)\u8bc1\u636e\u5145\u5b9e\u8005\u878d\u5408\u5916\u90e8\u7ed3\u6784\u5316\u8d44\u6e90\u4fdd\u8bc1\u5206\u7c7b\u771f\u5b9e\u6027\u3002", "result": "\u672c\u6587\u9488\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5386\u53f2\u6587\u5316\u63a8\u7406\u4e2d\u7684\u5c40\u9650\uff0c\u63d0\u51fa\u4e86CHisAgent\uff0c\u4e00\u79cd\u9002\u7528\u4e8e\u4e2d\u56fd\u53e4\u4ee3\u80cc\u666f\u7684\u591a\u667a\u80fd\u4f53\u5386\u53f2\u5206\u7c7b\u6784\u5efa\u6846\u67b6\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u4e09\u4e2a\u9636\u6bb5\uff08\u5e95\u5c42\u8bf1\u5bfc\u8005\u3001\u9876\u5c42\u6269\u5c55\u8005\u3001\u8bc1\u636e\u5145\u5b9e\u8005\uff09\u7cfb\u7edf\u6027\u6784\u5efa\u5386\u53f2\u4e8b\u4ef6\u5206\u7c7b\uff0c\u7ed3\u5408\u539f\u59cb\u5386\u53f2\u8bed\u6599\u3001LLM\u4e16\u754c\u77e5\u8bc6\u53ca\u5916\u90e8\u7ed3\u6784\u5316\u5386\u53f2\u8d44\u6e90\uff0c\u6700\u7ec8\u6784\u5efa\u4e86\u6db5\u76d6\u53e4\u4ee3\u4e2d\u56fd\u653f\u6cbb\u3001\u519b\u4e8b\u3001\u5916\u4ea4\u4e0e\u793e\u4f1a\u751f\u6d3b\u7684\u89c4\u6a21\u5316\u4e8b\u4ef6\u5206\u7c7b\u3002\u8bc4\u4f30\u8868\u660e\u8be5\u5206\u7c7b\u7ed3\u6784\u5177\u6709\u66f4\u597d\u7684\u7ed3\u6784\u8fde\u8d2f\u6027\u548c\u8986\u76d6\u8303\u56f4\uff0c\u5e76\u652f\u6301\u8de8\u6587\u5316\u5bf9\u9f50\u3002", "conclusion": "CHisAgent\u6709\u6548\u6574\u5408\u591a\u6e90\u4fe1\u606f\uff0c\u81ea\u52a8\u6784\u5efa\u4e86\u7ed3\u6784\u5408\u7406\u3001\u8986\u76d6\u5e7f\u6cdb\u7684\u4e2d\u56fd\u53e4\u4ee3\u5386\u53f2\u4e8b\u4ef6\u5206\u7c7b\uff0c\u4fc3\u8fdb\u4e86\u5386\u53f2\u77e5\u8bc6\u7684\u7ec4\u7ec7\u4e0e\u7406\u89e3\u3002"}}
{"id": "2601.05772", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.05772", "abs": "https://arxiv.org/abs/2601.05772", "authors": ["Qingyuan Li", "Chenchen Yu", "Chuanyi Li", "Xin-Cheng Wen", "Cheryl Lee", "Cuiyun Gao", "Bin Luo"], "title": "StriderSPD: Structure-Guided Joint Representation Learning for Binary Security Patch Detection", "comment": null, "summary": "Vulnerabilities severely threaten software systems, making the timely application of security patches crucial for mitigating attacks. However, software vendors often silently patch vulnerabilities with limited disclosure, where Security Patch Detection (SPD) comes to protect software assets. Recently, most SPD studies have targeted Open-Source Software (OSS), yet a large portion of real-world software is closed-source, where patches are distributed as binaries without accessible source code. The limited binary SPD approaches often lift binaries to abstraction levels, i.e., assembly code or pseudo-code. However, assembly code is register-based instructions conveying limited semantics, while pseudo-code lacks parser-compatible grammar to extract structure, both hindering accurate vulnerability-fix representation learning. In addition, previous studies often obtain training and testing data from the same project for evaluation, which fails to reflect closed-source conditions. To alleviate the above challenges, we propose \\textbf{\\textit{StriderSPD}}, a \\underline{Str}ucture-gu\\underline{ide}d joint \\underline{r}epresentation \\underline{SPD} framework of binary code that integrates a graph branch into a large language model (LLM), leveraging structural information to guide the LLM in identifying security patches. Our novel design of the adapters in the graph branch effectively aligns the representations between assembly code and pseudo-code at the LLM's token level. We further present a two-stage training strategy to address the optimization imbalance caused by the large parameter disparity between StriderSPD's two branches, which enables proper branch fitting. To enable more realistic evaluation, we construct a binary SPD benchmark that is disjoint from prior datasets in both projects and domains and extensively evaluate StriderSPD on this benchmark.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u7ed3\u6784\u5f15\u5bfc\u7684\u56fe\u5206\u652f\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e8c\u8fdb\u5236\u5b89\u5168\u8865\u4e01\u68c0\u6d4b\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u95ed\u6e90\u8f6f\u4ef6\u6f0f\u6d1e\u8865\u4e01\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u9488\u5bf9\u95ed\u6e90\u8f6f\u4ef6\u4e8c\u8fdb\u5236\u8865\u4e01\u68c0\u6d4b\u9762\u4e34\u7684\u8bed\u4e49\u4fe1\u606f\u6709\u9650\u548c\u7f3a\u4e4f\u5408\u7406\u8bed\u6cd5\u7ed3\u6784\u3001\u4ee5\u53ca\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u5728\u540c\u4e00\u9879\u76ee\u4e2d\u8bad\u7ec3\u6d4b\u8bd5\u5bfc\u81f4\u8bc4\u4f30\u4e0d\u73b0\u5b9e\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faStriderSPD\u6846\u67b6\uff0c\u7ed3\u5408\u56fe\u7ed3\u6784\u5206\u652f\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u6784\u4fe1\u606f\u5f15\u5bfc\u8bc6\u522b\u5b89\u5168\u8865\u4e01\uff1b\u8bbe\u8ba1\u56fe\u7ed3\u6784\u5206\u652f\u7684\u9002\u914d\u5668\u5bf9\u9f50\u6c47\u7f16\u7801\u4e0e\u4f2a\u4ee3\u7801\u8868\u793a\uff1b\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u89e3\u51b3\u4e0d\u540c\u5206\u652f\u53c2\u6570\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "\u6784\u5efa\u4e86\u4e0d\u540c\u9879\u76ee\u548c\u9886\u57df\u7684\u4e8c\u8fdb\u5236SPD\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5728\u6b64\u57fa\u51c6\u4e0a\u5e7f\u6cdb\u8bc4\u4f30StriderSPD\uff0c\u9a8c\u8bc1\u5176\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "StriderSPD\u6709\u6548\u89e3\u51b3\u4e86\u4e8c\u8fdb\u5236\u5b89\u5168\u8865\u4e01\u68c0\u6d4b\u4e2d\u8bed\u4e49\u8868\u8fbe\u4e0d\u8db3\u548c\u8bad\u7ec3\u8bc4\u4f30\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u68c0\u6d4b\u51c6\u786e\u7387\u5e76\u63a8\u52a8\u4e86\u73b0\u5b9e\u95ed\u6e90\u8f6f\u4ef6\u573a\u666f\u4e0b\u7684\u8865\u4e01\u68c0\u6d4b\u7814\u7a76\u3002"}}
{"id": "2601.05524", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05524", "abs": "https://arxiv.org/abs/2601.05524", "authors": ["Yuhao Shen", "Tianyu Liu", "Junyi Shen", "Jinyang Wu", "Quan Kong", "Li Huan", "Cong Wang"], "title": "Double: Breaking the Acceleration Limit via Double Retrieval Speculative Parallelism", "comment": null, "summary": "Parallel Speculative Decoding (PSD) accelerates traditional Speculative Decoding (SD) by overlapping draft generation with verification. However, it remains hampered by two fundamental challenges: (1) a theoretical speedup ceiling dictated by the speed ratio between the draft and target models, and (2) high computational waste and pipeline stall due to mid-sequence token rejections of early errors. To address these limitations, we introduce \\textsc{Double} (Double Retrieval Speculative Parallelism). By bridging the gap between SD and PSD, our framework resolves the Retrieval \\emph{Precision-Efficiency Dilemma} through a novel synchronous mechanism. Specifically, we enable the draft model to execute iterative retrieval speculations to break the theoretical speedup limits; to alleviate rejections without rollback, the target model performs authoritative retrieval to generate multi-token guidance. \\textsc{Double} is entirely training-free and lossless. Extensive experiments demonstrate state-of-the-art speedup of $\\textbf{5.3}\\times$ on LLaMA3.3-70B and $\\textbf{2.8}\\times$ on Qwen3-32B, significantly outperforming the advanced method EAGLE-3 that requires extensive model training.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDouble\u7684\u53cc\u91cd\u68c0\u7d22\u6295\u673a\u5e76\u884c\u6846\u67b6\uff0c\u901a\u8fc7\u540c\u6b65\u673a\u5236\u89e3\u51b3\u4e86\u4f20\u7edf\u5e76\u884c\u6295\u673a\u89e3\u7801\u4e2d\u7684\u901f\u5ea6\u548c\u6548\u7387\u74f6\u9888\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u52a0\u901f\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u5e76\u884c\u6295\u673a\u89e3\u7801\u53d7\u6a21\u578b\u901f\u5ea6\u6bd4\u9650\u5236\u4e14\u5b58\u5728\u4e2d\u5e8f\u5217\u6807\u8bb0\u62d2\u7edd\u5bfc\u81f4\u7684\u8ba1\u7b97\u6d6a\u8d39\u4e0e\u6d41\u6c34\u7ebf\u963b\u585e\uff0c\u4e9f\u9700\u7a81\u7834\u8fd9\u4e9b\u6839\u672c\u6027\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u540c\u6b65\u673a\u5236\uff0c\u5141\u8bb8\u8349\u7a3f\u6a21\u578b\u6267\u884c\u8fed\u4ee3\u68c0\u7d22\u6295\u673a\uff0c\u5e76\u4f7f\u76ee\u6807\u6a21\u578b\u8fdb\u884c\u6743\u5a01\u68c0\u7d22\u4ee5\u751f\u6210\u591a\u6807\u8bb0\u6307\u5bfc\uff0c\u4ece\u800c\u6d88\u9664\u8349\u7a3f\u6a21\u578b\u7684\u9519\u8bef\u56de\u6eda\u3002", "result": "\u5728LLaMA3.3-70B\u548cQwen3-32B\u6a21\u578b\u4e0a\uff0cDouble\u5206\u522b\u8fbe\u5230\u4e865.3\u500d\u548c2.8\u500d\u7684\u52a0\u901f\uff0c\u663e\u8457\u4f18\u4e8e\u9700\u8981\u5927\u91cf\u6a21\u578b\u8bad\u7ec3\u7684EAGLE-3\u65b9\u6cd5\u3002", "conclusion": "Double\u6846\u67b6\u7a81\u7834\u4e86\u4f20\u7edf\u5e76\u884c\u6295\u673a\u89e3\u7801\u7684\u901f\u5ea6\u4e0a\u9650\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u8ba1\u7b97\u6d6a\u8d39\uff0c\u5b9e\u73b0\u4e86\u5728\u5927\u578b\u6a21\u578b\u4e0a\u76845.3\u500d\u548c2.8\u500d\u52a0\u901f\uff0c\u6027\u80fd\u4f18\u4e8e\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u7684\u5148\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2601.05777", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.05777", "abs": "https://arxiv.org/abs/2601.05777", "authors": ["Yaoqi Guo", "Ying Xiao", "Jie M. Zhang", "Mark Harman", "Yiling Lou", "Yang Liu", "Zhenpeng Chen"], "title": "EET: Experience-Driven Early Termination for Cost-Efficient Software Engineering Agents", "comment": null, "summary": "Software engineering (SE) agents powered by large language models are increasingly adopted in practice, yet they often incur substantial monetary cost. We introduce EET, an experience-driven early termination approach that reduces the cost of SE agents while preserving task performance. EET extracts structured experience from prior issue-resolution executions and leverages it to guide early termination during patch generation and selection, reducing unproductive iterations. We evaluate EET on the SWE-bench Verified benchmark across three representative SE agents. EET consistently reduces total cost by 19%-55% (32% on average), with negligible loss in resolution rate (at most 0.2%). These efficiency gains are achieved, on average, by identifying early-termination opportunities for 11% of issues and reducing API calls, input tokens, and output tokens by 21%, 30%, and 25%, respectively. We release the code, prompts, and data at https://github.com/EffiSEAgent/EET.", "AI": {"tldr": "EET\u901a\u8fc7\u5229\u7528\u7ed3\u6784\u5316\u5386\u53f2\u7ecf\u9a8c\u5b9e\u73b0\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u7684\u65e9\u671f\u7ec8\u6b62\uff0c\u663e\u8457\u964d\u4f4e\u8fd0\u884c\u6210\u672c\u4e14\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u6210\u672c\u8f83\u9ad8\uff0c\u4e9f\u9700\u6709\u6548\u964d\u4f4e\u5176\u8fd0\u884c\u5f00\u9500\u3002", "method": "\u63d0\u51fa\u7ecf\u9a8c\u9a71\u52a8\u7684\u65e9\u671f\u7ec8\u6b62\uff08EET\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u53d6\u5148\u524d\u95ee\u9898\u89e3\u51b3\u8fc7\u7a0b\u4e2d\u7684\u7ed3\u6784\u5316\u7ecf\u9a8c\uff0c\u6307\u5bfc\u8865\u4e01\u751f\u6210\u548c\u9009\u62e9\u9636\u6bb5\u7684\u65e9\u671f\u7ec8\u6b62\uff0c\u51cf\u5c11\u4f4e\u6548\u8fed\u4ee3\u3002", "result": "EET\u5728SWE-bench Verified\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728\u4e09\u79cd\u4ee3\u8868\u6027\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u4e0a\u5b9e\u73b0\u4e8619%-55%\uff08\u5e73\u574732%\uff09\u7684\u6210\u672c\u964d\u4f4e\uff0c\u89e3\u51b3\u7387\u4ec5\u4e0b\u964d\u6700\u591a0.2%\u3002", "conclusion": "\u7ecf\u9a8c\u9a71\u52a8\u7684\u65e9\u671f\u7ec8\u6b62\u6709\u6548\u63d0\u5347\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u7684\u6267\u884c\u6548\u7387\uff0c\u663e\u8457\u8282\u7ea6API\u8c03\u7528\u548c\u4ee4\u724c\u6570\uff0c\u4e14\u51e0\u4e4e\u4e0d\u5f71\u54cd\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2601.05543", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.05543", "abs": "https://arxiv.org/abs/2601.05543", "authors": ["Chaoren Wang", "Heng Lu", "Xueyao Zhang", "Shujie Liu", "Yan Lu", "Jinyu Li", "Zhizheng Wu"], "title": "Closing the Modality Reasoning Gap for Speech Large Language Models", "comment": null, "summary": "Although speech large language models have achieved notable progress, a substantial modality reasoning gap remains: their reasoning performance on speech inputs is markedly weaker than on text. This gap could be associated with representational drift across Transformer layers and behavior deviations in long-chain reasoning. To address this issue, we introduce TARS, a reinforcement-learning framework that aligns text-conditioned and speech-conditioned trajectories through an asymmetric reward design. The framework employs two dense and complementary signals: representation alignment, which measures layer-wise hidden-state similarity between speech- and text-conditioned trajectories, and behavior alignment, which evaluates semantic consistency between generated outputs and reference text completions. Experiments on challenging reasoning benchmarks, including MMSU and OBQA, show that our approach significantly narrows the modality reasoning gap and achieves state-of-the-art performance among 7B-scale Speech LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86TARS\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u9f50\u8bed\u97f3\u548c\u6587\u672c\u6761\u4ef6\u4e0b\u7684\u8868\u793a\u548c\u884c\u4e3a\uff0c\u6709\u6548\u63d0\u5347\u4e86\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u7f29\u5c0f\u4e86\u4e0e\u6587\u672c\u6a21\u6001\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "motivation": "\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u97f3\u8f93\u5165\u4e0a\u7684\u63a8\u7406\u6027\u80fd\u663e\u8457\u5f31\u4e8e\u6587\u672c\u8f93\u5165\uff0c\u5b58\u5728\u6a21\u6001\u63a8\u7406\u5dee\u8ddd\uff0c\u8fd9\u4e00\u5dee\u8ddd\u53ef\u80fd\u6e90\u4e8eTransformer\u5c42\u95f4\u7684\u8868\u793a\u6f02\u79fb\u548c\u957f\u94fe\u63a8\u7406\u4e2d\u7684\u884c\u4e3a\u504f\u5dee\u3002", "method": "\u8bbe\u8ba1\u4e86TARS\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u975e\u5bf9\u79f0\u5956\u52b1\u673a\u5236\uff0c\u5229\u7528\u8868\u793a\u5bf9\u9f50\u548c\u884c\u4e3a\u5bf9\u9f50\u4e24\u4e2a\u5bc6\u96c6\u4fe1\u53f7\uff0c\u5b9e\u73b0\u8bed\u97f3\u548c\u6587\u672c\u6761\u4ef6\u8f68\u8ff9\u7684\u5bf9\u9f50\uff0c\u4ece\u800c\u63d0\u5347\u8bed\u97f3\u63a8\u7406\u6027\u80fd\u3002", "result": "\u63d0\u51fa\u7684TARS\u6846\u67b6\u663e\u8457\u7f29\u5c0f\u4e86\u8bed\u97f3\u4e0e\u6587\u672c\u6a21\u6001\u63a8\u7406\u7684\u5dee\u8ddd\uff0c\u5728MMSU\u548cOBQA\u7b49\u590d\u6742\u63a8\u7406\u57fa\u51c6\u4e0a\u8fbe\u52307B\u89c4\u6a21\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "TARS\u6846\u67b6\u901a\u8fc7\u8868\u793a\u548c\u884c\u4e3a\u7684\u53cc\u91cd\u5bf9\u9f50\uff0c\u6709\u6548\u63d0\u5347\u4e86\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u6a21\u6001\u95f4\u5bf9\u9f50\u7b56\u7565\u5728\u7f29\u5c0f\u8bed\u97f3\u4e0e\u6587\u672c\u63a8\u7406\u5dee\u8ddd\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2601.05827", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.05827", "abs": "https://arxiv.org/abs/2601.05827", "authors": ["Zewei Lin", "Jiachi Chen", "Jingwen Zhang", "Zexu Wang", "Yuming Feng", "Weizhe Zhang", "Zibin Zheng"], "title": "SSR: Safeguarding Staking Rewards by Defining and Detecting Logical Defects in DeFi Staking", "comment": null, "summary": "Decentralized Finance (DeFi) staking is one of the most prominent applications within the DeFi ecosystem, where DeFi projects enable users to stake tokens on the platform and reward participants with additional tokens. However, logical defects in DeFi staking could enable attackers to claim unwarranted rewards by manipulating reward amounts, repeatedly claiming rewards, or engaging in other malicious actions. To mitigate these threats, we conducted the first study focused on defining and detecting logical defects in DeFi staking. Through the analysis of 64 security incidents and 144 audit reports, we identified six distinct types of logical defects, each accompanied by detailed descriptions and code examples. Building on this empirical research, we developed SSR (Safeguarding Staking Reward), a static analysis tool designed to detect logical defects in DeFi staking contracts. SSR utilizes a large language model (LLM) to extract fundamental information about staking logic and constructs a DeFi staking model. It then identifies logical defects by analyzing the model and the associated semantic features. We constructed a ground truth dataset based on known security incidents and audit reports to evaluate the effectiveness of SSR. The results indicate that SSR achieves an overall precision of 92.31%, a recall of 87.92%, and an F1-score of 88.85%. Additionally, to assess the prevalence of logical defects in real-world smart contracts, we compiled a large-scale dataset of 15,992 DeFi staking contracts. SSR detected that 3,557 (22.24%) of these contracts contained at least one logical defect.", "AI": {"tldr": "\u672c\u7814\u7a76\u5b9a\u4e49\u5e76\u68c0\u6d4bDeFi\u8d28\u62bc\u4e2d\u7684\u903b\u8f91\u7f3a\u9677\uff0c\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9759\u6001\u5206\u6790\u5de5\u5177SSR\uff0c\u6709\u6548\u53d1\u73b0\u5408\u7ea6\u4e2d\u7684\u5b89\u5168\u95ee\u9898\u3002", "motivation": "DeFi\u8d28\u62bc\u5b58\u5728\u903b\u8f91\u7f3a\u9677\uff0c\u653b\u51fb\u8005\u53ef\u901a\u8fc7\u64cd\u7eb5\u5956\u52b1\u91d1\u989d\u6216\u91cd\u590d\u9886\u53d6\u5956\u52b1\u7b49\u624b\u6bb5\u83b7\u53d6\u4e0d\u5f53\u5229\u76ca\uff0c\u4e9f\u9700\u9488\u5bf9DeFi\u8d28\u62bc\u903b\u8f91\u7f3a\u9677\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u5206\u679064\u8d77\u5b89\u5168\u4e8b\u4ef6\u4e0e144\u4efd\u5ba1\u8ba1\u62a5\u544a\uff0c\u5f52\u7eb3\u51fa\u516d\u79cdDeFi\u8d28\u62bc\u903b\u8f91\u7f3a\u9677\u7c7b\u578b\uff0c\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9759\u6001\u5206\u6790\u5de5\u5177SSR\uff0c\u901a\u8fc7\u6784\u5efaDeFi\u8d28\u62bc\u6a21\u578b\u5e76\u7ed3\u5408\u8bed\u4e49\u7279\u5f81\u68c0\u6d4b\u903b\u8f91\u7f3a\u9677\u3002", "result": "SSR\u5de5\u5177\u5728\u57fa\u4e8e\u771f\u5b9e\u6570\u636e\u96c6\u7684\u8bc4\u4f30\u4e2d\u8fbe\u523092.31%\u51c6\u786e\u7387\u300187.92%\u53ec\u56de\u7387\u548c88.85% F1\u503c\uff1b\u5bf9\u7ea616,000\u4efd\u5408\u7ea6\u68c0\u6d4b\u53d1\u73b0\u7ea622.24%\u5b58\u5728\u903b\u8f91\u7f3a\u9677\u3002", "conclusion": "SSR\u5de5\u5177\u80fd\u9ad8\u6548\u51c6\u786e\u5730\u68c0\u6d4bDeFi\u8d28\u62bc\u903b\u8f91\u7f3a\u9677\uff0c\u63ed\u793a\u8be5\u9886\u57df\u5b58\u5728\u8f83\u591a\u5b89\u5168\u9690\u60a3\uff0c\u4fc3\u8fdbDeFi\u751f\u6001\u66f4\u5b89\u5168\u53d1\u5c55\u3002"}}
{"id": "2601.05545", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05545", "abs": "https://arxiv.org/abs/2601.05545", "authors": ["Hongjin Kim", "Jeonghyun Kang", "Harksoo Kim"], "title": "Can Large Language Models Differentiate Harmful from Argumentative Essays? Steps Toward Ethical Essay Scoring", "comment": "COLING 2025 accepted paper (Main)", "summary": "This study addresses critical gaps in Automated Essay Scoring (AES) systems and Large Language Models (LLMs) with regard to their ability to effectively identify and score harmful essays. Despite advancements in AES technology, current models often overlook ethically and morally problematic elements within essays, erroneously assigning high scores to essays that may propagate harmful opinions. In this study, we introduce the Harmful Essay Detection (HED) benchmark, which includes essays integrating sensitive topics such as racism and gender bias, to test the efficacy of various LLMs in recognizing and scoring harmful content. Our findings reveal that: (1) LLMs require further enhancement to accurately distinguish between harmful and argumentative essays, and (2) both current AES models and LLMs fail to consider the ethical dimensions of content during scoring. The study underscores the need for developing more robust AES systems that are sensitive to the ethical implications of the content they are scoring.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u6709\u5bb3\u4f5c\u6587\u68c0\u6d4b\u57fa\u51c6\uff0c\u8bc4\u4f30\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206\u7cfb\u7edf\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bc6\u522b\u548c\u8bc4\u5206\u6709\u5bb3\u5185\u5bb9\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u8fa8\u522b\u4f26\u7406\u95ee\u9898\u65b9\u9762\u5b58\u5728\u663e\u8457\u4e0d\u8db3\u3002", "motivation": "\u5c3d\u7ba1\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206\u6280\u672f\u6709\u6240\u8fdb\u6b65\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u5f80\u5f80\u5ffd\u89c6\u4f5c\u6587\u4e2d\u7684\u4f26\u7406\u548c\u9053\u5fb7\u95ee\u9898\uff0c\u9519\u8bef\u5730\u7ed9\u4e88\u6709\u5bb3\u5185\u5bb9\u9ad8\u5206\uff0c\u4e9f\u9700\u6539\u8fdb\u6a21\u578b\u4ee5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u6784\u5efaHarmful Essay Detection (HED)\u57fa\u51c6\uff0c\u5305\u542b\u6d89\u53ca\u79cd\u65cf\u4e3b\u4e49\u548c\u6027\u522b\u504f\u89c1\u7684\u654f\u611f\u8bdd\u9898\u4f5c\u6587\uff0c\u5e76\u7528\u8be5\u57fa\u51c6\u6d4b\u8bd5\u591a\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u6709\u5bb3\u5185\u5bb9\u7684\u8bc6\u522b\u548c\u8bc4\u5206\u80fd\u529b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9700\u8981\u8fdb\u4e00\u6b65\u589e\u5f3a\u4ee5\u533a\u5206\u6709\u5bb3\u4f5c\u6587\u548c\u8bba\u8bc1\u5408\u7406\u7684\u4f5c\u6587\uff0c\u73b0\u6709\u7cfb\u7edf\u666e\u904d\u5ffd\u89c6\u5185\u5bb9\u7684\u4f26\u7406\u7ef4\u5ea6\uff0c\u5bfc\u81f4\u8bc4\u5206\u7ed3\u679c\u4e0d\u5408\u7406\u3002", "conclusion": "\u5f53\u524d\u7684\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206\u7cfb\u7edf\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bc6\u522b\u548c\u6b63\u786e\u8bc4\u5206\u542b\u6709\u79cd\u65cf\u4e3b\u4e49\u3001\u6027\u522b\u504f\u89c1\u7b49\u4f26\u7406\u95ee\u9898\u7684\u6709\u5bb3\u4f5c\u6587\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u4e9f\u9700\u5f00\u53d1\u5bf9\u4f26\u7406\u7ef4\u5ea6\u66f4\u52a0\u654f\u611f\u7684\u8bc4\u5206\u7cfb\u7edf\u3002"}}
{"id": "2601.05752", "categories": ["cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.05752", "abs": "https://arxiv.org/abs/2601.05752", "authors": ["Shu Yang", "Jingyu Hu", "Tong Li", "Hanqi Yan", "Wenxuan Wang", "Di Wang"], "title": "AutoMonitor-Bench: Evaluating the Reliability of LLM-Based Misbehavior Monitor", "comment": null, "summary": "We introduce AutoMonitor-Bench, the first benchmark designed to systematically evaluate the reliability of LLM-based misbehavior monitors across diverse tasks and failure modes. AutoMonitor-Bench consists of 3,010 carefully annotated test samples spanning question answering, code generation, and reasoning, with paired misbehavior and benign instances. We evaluate monitors using two complementary metrics: Miss Rate (MR) and False Alarm Rate (FAR), capturing failures to detect misbehavior and oversensitivity to benign behavior, respectively. Evaluating 12 proprietary and 10 open-source LLMs, we observe substantial variability in monitoring performance and a consistent trade-off between MR and FAR, revealing an inherent safety-utility tension. To further explore the limits of monitor reliability, we construct a large-scale training corpus of 153,581 samples and fine-tune Qwen3-4B-Instruction to investigate whether training on known, relatively easy-to-construct misbehavior datasets improves monitoring performance on unseen and more implicit misbehaviors. Our results highlight the challenges of reliable, scalable misbehavior monitoring and motivate future work on task-aware designing and training strategies for LLM-based monitors.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u7cfb\u7edf\u8bc4\u4f30LLM\u8bef\u884c\u4e3a\u76d1\u6d4b\u5668\u53ef\u9760\u6027\u7684\u57fa\u51c6AutoMonitor-Bench\uff0c\u63ed\u793a\u76d1\u6d4b\u65f6\u6f0f\u62a5\u4e0e\u8bef\u62a5\u7684\u6027\u80fd\u6743\u8861\uff0c\u5e76\u5c55\u793a\u5927\u89c4\u6a21\u5fae\u8c03\u63d0\u5347\u76d1\u6d4b\u6027\u80fd\u7684\u5c1d\u8bd5\u548c\u5b58\u5728\u7684\u96be\u9898\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u7cfb\u7edf\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u8bef\u884c\u4e3a\u76d1\u6d4b\u5668\u53ef\u9760\u6027\u7684\u57fa\u51c6\uff0c\u4e14\u76d1\u6d4b\u5b58\u5728\u6f0f\u62a5\u4e0e\u8bef\u62a5\u7684\u5b89\u5168\u4e0e\u5b9e\u7528\u6027\u6743\u8861\u95ee\u9898\uff0c\u8feb\u5207\u9700\u8981\u7cfb\u7edf\u5de5\u5177\u4e0e\u65b9\u6cd5\u63d0\u5347\u76d1\u63a7\u7684\u7a33\u5b9a\u6027\u4e0e\u51c6\u786e\u6027\u3002", "method": "\u8bbe\u8ba1AutoMonitor-Bench\u57fa\u51c6\uff0c\u6536\u96c63010\u4e2a\u6807\u6ce8\u6837\u672c\u8986\u76d6\u95ee\u7b54\u3001\u4ee3\u7801\u751f\u6210\u4e0e\u63a8\u7406\u4efb\u52a1\uff0c\u5e76\u4f7f\u752812\u4e2a\u4e13\u6709\u4e0e10\u4e2a\u5f00\u6e90\u5927\u6a21\u578b\u8fdb\u884c\u6027\u80fd\u8bc4\u4f30\uff1b\u6784\u5efa15.3\u4e07\u6837\u672c\u7684\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\uff0c\u5fae\u8c03Qwen3-4B-Instruction\u6a21\u578b\u4ee5\u63d0\u5347\u76d1\u6d4b\u6027\u80fd\u3002", "result": "\u89c2\u5bdf\u5230\u76d1\u6d4b\u6027\u80fd\u5728\u4e0d\u540c\u6a21\u578b\u95f4\u5dee\u5f02\u663e\u8457\uff0c\u5b58\u5728\u6f0f\u62a5\u7387(MR)\u4e0e\u8bef\u62a5\u7387(FAR)\u7684\u660e\u663e\u6743\u8861\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u5fae\u8c03\u63d0\u5347\u90e8\u5206\u6027\u80fd\uff0c\u4f46\u5bf9\u9690\u853d\u8bef\u884c\u4e3a\u7684\u68c0\u6d4b\u4ecd\u5177\u6311\u6218\u3002", "conclusion": "\u76d1\u6d4bLLM\u8bef\u884c\u4e3a\u5b58\u5728\u56fa\u6709\u7684\u5b89\u5168\u4e0e\u5b9e\u7528\u6027\u51b2\u7a81\uff0c\u901a\u8fc7\u8bbe\u8ba1\u9488\u5bf9\u6027\u4efb\u52a1\u548c\u8bad\u7ec3\u7b56\u7565\u63d0\u5347\u76d1\u63a7\u6548\u679c\u4ecd\u9700\u6df1\u5165\u7814\u7a76\uff0c\u672a\u6765\u5de5\u4f5c\u5e94\u805a\u7126\u4e8e\u53ef\u9760\u3001\u53ef\u6269\u5c55\u7684\u4efb\u52a1\u611f\u77e5\u76d1\u6d4b\u5668\u8bbe\u8ba1\u4e0e\u8bad\u7ec3\u3002"}}
{"id": "2601.05548", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05548", "abs": "https://arxiv.org/abs/2601.05548", "authors": ["Jeonghyun Kang", "Hongjin Kim", "Harksoo Kim"], "title": "Generation-Based and Emotion-Reflected Memory Update: Creating the KEEM Dataset for Better Long-Term Conversation", "comment": "COLING 2025 accepted paper (Main)", "summary": "In this work, we introduce the Keep Emotional and Essential Memory (KEEM) dataset, a novel generation-based dataset designed to enhance memory updates in long-term conversational systems. Unlike existing approaches that rely on simple accumulation or operation-based methods, which often result in information conflicts and difficulties in accurately tracking a user's current state, KEEM dynamically generates integrative memories. This process not only preserves essential factual information but also incorporates emotional context and causal relationships, enabling a more nuanced understanding of user interactions. By seamlessly updating a system's memory with both emotional and essential data, our approach promotes deeper empathy and enhances the system's ability to respond meaningfully in open-domain conversations.", "AI": {"tldr": "KEEM\u6570\u636e\u96c6\u901a\u8fc7\u751f\u6210\u7efc\u5408\u8bb0\u5fc6\uff0c\u6574\u5408\u60c5\u611f\u4e0e\u4e8b\u5b9e\u4fe1\u606f\uff0c\u63d0\u5347\u957f\u65f6\u5bf9\u8bdd\u7cfb\u7edf\u7684\u8bb0\u5fc6\u66f4\u65b0\u548c\u54cd\u5e94\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u957f\u65f6\u5bf9\u8bdd\u7cfb\u7edf\u7684\u8bb0\u5fc6\u66f4\u65b0\u65b9\u6cd5\u5b58\u5728\u4fe1\u606f\u51b2\u7a81\u548c\u72b6\u6001\u8ddf\u8e2a\u56f0\u96be\uff0c\u96be\u4ee5\u51c6\u786e\u53cd\u6620\u7528\u6237\u60c5\u611f\u548c\u4e8b\u5b9e\u72b6\u6001\uff0c\u5f71\u54cd\u5bf9\u8bdd\u8d28\u91cf\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u878d\u5408\u60c5\u611f\u4e0e\u4e8b\u5b9e\u7684\u8bb0\u5fc6\u66f4\u65b0\u673a\u5236\u3002", "method": "\u91c7\u7528\u751f\u6210\u5f0f\u65b9\u6cd5\u52a8\u6001\u521b\u5efa\u7efc\u5408\u8bb0\u5fc6\uff0c\u7ed3\u5408\u4e8b\u5b9e\u3001\u60c5\u611f\u548c\u56e0\u679c\u5173\u7cfb\uff0c\u5b9e\u73b0\u7cfb\u7edf\u8bb0\u5fc6\u7684\u5b9e\u65f6\u66f4\u65b0\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86Keep Emotional and Essential Memory (KEEM)\u6570\u636e\u96c6\uff0c\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u7684\u957f\u65f6\u8bb0\u5fc6\u66f4\u65b0\u6570\u636e\u96c6\u3002\u4e0e\u4f20\u7edf\u7b80\u5355\u79ef\u7d2f\u6216\u57fa\u4e8e\u64cd\u4f5c\u7684\u65b9\u6cd5\u4e0d\u540c\uff0cKEEM\u80fd\u52a8\u6001\u751f\u6210\u7efc\u5408\u8bb0\u5fc6\uff0c\u517c\u987e\u4e8b\u5b9e\u4fe1\u606f\u3001\u60c5\u611f\u4e0a\u4e0b\u6587\u548c\u56e0\u679c\u5173\u7cfb\uff0c\u4ece\u800c\u66f4\u51c6\u786e\u5730\u8ddf\u8e2a\u7528\u6237\u5f53\u524d\u72b6\u6001\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u65e0\u7f1d\u66f4\u65b0\u5305\u542b\u60c5\u611f\u548c\u5173\u952e\u4fe1\u606f\u7684\u8bb0\u5fc6\uff0c\u63d0\u5347\u4e86\u7cfb\u7edf\u5728\u5f00\u653e\u57df\u5bf9\u8bdd\u4e2d\u7684\u5171\u60c5\u80fd\u529b\u548c\u54cd\u5e94\u8d28\u91cf\u3002", "conclusion": "KEEM\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u957f\u65f6\u8bb0\u5fc6\u66f4\u65b0\u4e2d\u7684\u4fe1\u606f\u51b2\u7a81\u548c\u72b6\u6001\u8ddf\u8e2a\u56f0\u96be\uff0c\u63d0\u5347\u4e86\u7cfb\u7edf\u7684\u5171\u60c5\u80fd\u529b\u548c\u5f00\u653e\u57df\u5bf9\u8bdd\u8868\u73b0\u3002"}}
{"id": "2601.05560", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05560", "abs": "https://arxiv.org/abs/2601.05560", "authors": ["Junyao Yang", "Chen Qian", "Dongrui Liu", "Wen Shen", "Yong Liu", "Jing Shao"], "title": "ReasonAny: Incorporating Reasoning Capability to Any Model via Simple and Effective Model Merging", "comment": "22 pages, 6 figures, 14 tables", "summary": "Large Reasoning Models (LRMs) with long chain-of-thought reasoning have recently achieved remarkable success. Yet, equipping domain-specialized models with such reasoning capabilities, referred to as \"Reasoning + X\", remains a significant challenge. While model merging offers a promising training-free solution, existing methods often suffer from a destructive performance collapse: existing methods tend to both weaken reasoning depth and compromise domain-specific utility. Interestingly, we identify a counter-intuitive phenomenon underlying this failure: reasoning ability predominantly resides in parameter regions with low gradient sensitivity, contrary to the common assumption that domain capabilities correspond to high-magnitude parameters. Motivated by this insight, we propose ReasonAny, a novel merging framework that resolves the reasoning-domain performance collapse through Contrastive Gradient Identification. Experiments across safety, biomedicine, and finance domains show that ReasonAny effectively synthesizes \"Reasoning + X\" capabilities, significantly outperforming state-of-the-art baselines while retaining robust reasoning performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faReasonAny\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u68af\u5ea6\u8bc6\u522b\u89e3\u51b3\u4e86\u9886\u57df\u4e13\u7528\u6a21\u578b\u4e0e\u5927\u63a8\u7406\u6a21\u578b\u5408\u5e76\u65f6\u7684\u6027\u80fd\u5d29\u6e83\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u63a8\u7406\u80fd\u529b\u4e0e\u9886\u57df\u4e13\u957f\u7684\u6709\u6548\u878d\u5408\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\u901a\u5e38\u5bfc\u81f4\u63a8\u7406\u6df1\u5ea6\u548c\u9886\u57df\u7279\u5b9a\u6548\u7528\u53cc\u91cd\u4e0b\u964d\uff0c\u53d1\u73b0\u63a8\u7406\u80fd\u529b\u4e3b\u8981\u4f4d\u4e8e\u4f4e\u68af\u5ea6\u654f\u611f\u7684\u53c2\u6570\u533a\u57df\uff0c\u800c\u9886\u57df\u80fd\u529b\u96c6\u4e2d\u5728\u9ad8\u5e45\u5ea6\u53c2\u6570\uff0c\u4f20\u7edf\u5408\u5e76\u65b9\u6cd5\u672a\u80fd\u533a\u5206\u8fd9\u4e24\u79cd\u7279\u6027\uff0c\u5bfc\u81f4\u6027\u80fd\u5d29\u6e83\u3002", "method": "\u63d0\u51faContrastive Gradient Identification\u6280\u672f\u7684ReasonAny\u6846\u67b6\uff0c\u7528\u4e8e\u8bc6\u522b\u548c\u4fdd\u62a4\u63a8\u7406\u80fd\u529b\u6240\u5728\u7684\u4f4e\u68af\u5ea6\u654f\u611f\u53c2\u6570\u533a\u57df\uff0c\u5e76\u901a\u8fc7\u6a21\u578b\u5408\u5e76\u5b9e\u73b0\u63a8\u7406\u548c\u9886\u57df\u6a21\u578b\u80fd\u529b\u7684\u534f\u540c\u63d0\u5347\u3002", "result": "\u5728\u5b89\u5168\u3001\u751f\u7269\u533b\u836f\u548c\u91d1\u878d\u7b49\u591a\u4e2a\u9886\u57df\u7684\u5b9e\u9a8c\u4e2d\uff0cReasonAny\u6846\u67b6\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u6709\u6548\u5408\u6210\u4e86\u63a8\u7406\u548c\u9886\u57df\u4e13\u957f\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7a33\u5065\u7684\u63a8\u7406\u6027\u80fd\u3002", "conclusion": "ReasonAny\u6709\u6548\u89e3\u51b3\u4e86\"\u63a8\u7406+\u9886\u57df\"\u6a21\u578b\u5408\u5e76\u65f6\u7684\u6027\u80fd\u5d29\u6e83\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u4e13\u4e1a\u9886\u57df\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u5408\u6210\u6a21\u578b\u7684\u63a8\u7406\u548c\u9886\u57df\u80fd\u529b\u3002"}}
{"id": "2601.05582", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05582", "abs": "https://arxiv.org/abs/2601.05582", "authors": ["Sung-Yoo Lim", "Koki Sato", "Kiyoshi Takami", "Giancarlos Parady", "Eui-Jin Kim"], "title": "Can large language models interpret unstructured chat data on dynamic group decision-making processes? Evidence on joint destination choice", "comment": "23 pages, 9 figures", "summary": "Social activities result from complex joint activity-travel decisions between group members. While observing the decision-making process of these activities is difficult via traditional travel surveys, the advent of new types of data, such as unstructured chat data, can help shed some light on these complex processes. However, interpreting these decision-making processes requires inferring both explicit and implicit factors. This typically involves the labor-intensive task of manually annotating dialogues to capture context-dependent meanings shaped by the social and cultural norms. This study evaluates the potential of Large Language Models (LLMs) to automate and complement human annotation in interpreting decision-making processes from group chats, using data on joint eating-out activities in Japan as a case study. We designed a prompting framework inspired by the knowledge acquisition process, which sequentially extracts key decision-making factors, including the group-level restaurant choice set and outcome, individual preferences of each alternative, and the specific attributes driving those preferences. This structured process guides the LLM to interpret group chat data, converting unstructured dialogues into structured tabular data describing decision-making factors. To evaluate LLM-driven outputs, we conduct a quantitative analysis using a human-annotated ground truth dataset and a qualitative error analysis to examine model limitations. Results show that while the LLM reliably captures explicit decision-making factors, it struggles to identify nuanced implicit factors that human annotators readily identified. We pinpoint specific contexts when LLM-based extraction can be trusted versus when human oversight remains essential. These findings highlight both the potential and limitations of LLM-based analysis for incorporating non-traditional data sources on social activities.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u89e3\u8bfb\u65e5\u672c\u7fa4\u4f53\u804a\u5929\u6570\u636e\u4e2d\u5173\u4e8e\u96c6\u4f53\u5916\u51fa\u5c31\u9910\u51b3\u7b56\u8fc7\u7a0b\u7684\u53ef\u884c\u6027\uff0c\u901a\u8fc7\u8bbe\u8ba1\u4e00\u4e2a\u9010\u6b65\u63d0\u53d6\u51b3\u7b56\u8981\u7d20\u7684\u63d0\u793a\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u5c06\u975e\u7ed3\u6784\u5316\u5bf9\u8bdd\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u6570\u636e\u3002", "motivation": "\u4f20\u7edf\u65c5\u884c\u8c03\u67e5\u96be\u4ee5\u89c2\u5bdf\u7fa4\u4f53\u6d3b\u52a8\u4e2d\u590d\u6742\u7684\u5171\u540c\u51b3\u7b56\u8fc7\u7a0b\uff0c\u65b0\u5174\u975e\u7ed3\u6784\u5316\u804a\u5929\u6570\u636e\u4e3a\u63ed\u793a\u8fd9\u4e00\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u65b0\u7684\u6570\u636e\u6e90\uff0c\u4f46\u89e3\u8bfb\u4f9d\u8d56\u4e8e\u5927\u91cf\u4eba\u5de5\u6ce8\u91ca\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u83b7\u53d6\u8fc7\u7a0b\u7684\u63d0\u793a\u6846\u67b6\uff0c\u9010\u6b65\u63d0\u53d6\u7fa4\u4f53\u9910\u5385\u9009\u62e9\u96c6\u3001\u5404\u65b9\u6848\u7684\u4e2a\u4eba\u504f\u597d\u53ca\u9a71\u52a8\u504f\u597d\u7684\u5177\u4f53\u5c5e\u6027\uff0c\u5c06\u975e\u7ed3\u6784\u5316\u5bf9\u8bdd\u8f6c\u4e3a\u7ed3\u6784\u5316\u8868\u683c\uff0c\u5e76\u4ee5\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u8fdb\u884c\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u3002", "result": "LLM\u80fd\u53ef\u9760\u8bc6\u522b\u663e\u6027\u56e0\u7d20\u4f46\u96be\u4ee5\u6355\u6349\u9690\u542b\u7ec6\u8282\uff0c\u660e\u786e\u4e86\u4f55\u65f6\u53ef\u4fe1\u8d56\u81ea\u52a8\u5316\u7ed3\u679c\uff0c\u4f55\u65f6\u9700\u4eba\u5de5\u53c2\u4e0e\uff0c\u5c55\u793a\u4e86LLM\u5728\u5206\u6790\u793e\u4f1a\u6d3b\u52a8\u4e2d\u975e\u4f20\u7edf\u6570\u636e\u6765\u6e90\u7684\u6f5c\u529b\u4e0e\u9650\u5236\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u8f83\u597d\u5730\u6355\u6349\u663e\u6027\u51b3\u7b56\u56e0\u7d20\uff0c\u4f46\u5728\u8bc6\u522b\u9690\u542b\u7684\u7ec6\u5fae\u56e0\u7d20\u65b9\u9762\u8868\u73b0\u8f83\u5f31\uff0c\u9700\u7ed3\u5408\u4eba\u5de5\u6821\u9a8c\u4ee5\u786e\u4fdd\u51c6\u786e\u6027\u3002"}}
{"id": "2601.05589", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05589", "abs": "https://arxiv.org/abs/2601.05589", "authors": ["Jiawei Shen", "Jia Zhu", "Hanghui Guo", "Weijie Shi", "Yue Cui", "Qingyu Niu", "Guoqing Ma", "Yidan Liang", "Jingjiang Liu", "Yiling Wang", "Shimin Di", "Jiajie Xu"], "title": "ACR: Adaptive Context Refactoring via Context Refactoring Operators for Multi-Turn Dialogue", "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable performance in multi-turn dialogue. However, in multi-turn dialogue, models still struggle to stay aligned with what has been established earlier, follow dependencies across many turns, and avoid drifting into incorrect facts as the interaction grows longer. Existing approaches primarily focus on extending the context window, introducing external memory, or applying context compression, yet these methods still face limitations such as \\textbf{contextual inertia} and \\textbf{state drift}. To address these challenges, we propose the \\textbf{A}daptive \\textbf{C}ontext \\textbf{R}efactoring \\textbf{(ACR)} Framework, which dynamically monitors and reshapes the interaction history to mitigate contextual inertia and state drift actively. ACR is built on a library of context refactoring operators and a teacher-guided self-evolving training paradigm that learns when to intervene and how to refactor, thereby decoupling context management from the reasoning process. Extensive experiments on multi-turn dialogue demonstrate that our method significantly outperforms existing baselines while reducing token consumption.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u81ea\u9002\u5e94\u4e0a\u4e0b\u6587\u91cd\u6784\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u7ba1\u7406\u5bf9\u8bdd\u5386\u53f2\uff0c\u6709\u6548\u63d0\u5347\u591a\u8f6e\u5bf9\u8bdd\u6a21\u578b\u8868\u73b0\u5e76\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u6a21\u578b\u96be\u4ee5\u4fdd\u6301\u4e0e\u5148\u524d\u4fe1\u606f\u4e00\u81f4\uff0c\u5b58\u5728\u4e0a\u4e0b\u6587\u60f0\u6027\u548c\u72b6\u6001\u6f02\u79fb\u95ee\u9898\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u5982\u6269\u5c55\u4e0a\u4e0b\u6587\u7a97\u53e3\u3001\u5f15\u5165\u5916\u90e8\u8bb0\u5fc6\u7b49\u4ecd\u9762\u4e34\u5c40\u9650\u3002", "method": "\u8bbe\u8ba1\u4e86\u81ea\u9002\u5e94\u4e0a\u4e0b\u6587\u91cd\u6784\uff08ACR\uff09\u6846\u67b6\uff0c\u5305\u542b\u4e0a\u4e0b\u6587\u91cd\u6784\u64cd\u4f5c\u5e93\u548c\u6559\u5e08\u6307\u5bfc\u7684\u81ea\u6211\u8fdb\u5316\u8bad\u7ec3\u8303\u5f0f\uff0c\u52a8\u6001\u76d1\u63a7\u5e76\u91cd\u5851\u4ea4\u4e92\u5386\u53f2\uff0c\u5b9e\u73b0\u4e0a\u4e0b\u6587\u7ba1\u7406\u4e0e\u63a8\u7406\u8fc7\u7a0b\u7684\u89e3\u8026\u3002", "result": "\u5728\u591a\u8f6e\u5bf9\u8bdd\u4efb\u52a1\u4e2d\uff0cACR\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8868\u73b0\u4e3a\u66f4\u597d\u7684\u5bf9\u8bdd\u8d28\u91cf\u548c\u66f4\u4f4e\u7684\u4ee3\u5e01\u6d88\u8017\u3002", "conclusion": "\u63d0\u51fa\u7684ACR\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u4e0a\u4e0b\u6587\u60f0\u6027\u548c\u72b6\u6001\u6f02\u79fb\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u5bf9\u8bdd\u4e00\u81f4\u6027\u548c\u51c6\u786e\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u3002"}}
{"id": "2601.05609", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05609", "abs": "https://arxiv.org/abs/2601.05609", "authors": ["Nguyen Minh Phuong", "Ha-Thanh Nguyen", "May Myo Zin", "Ken Satoh"], "title": "Data Augmented Pipeline for Legal Information Extraction and Reasoning", "comment": "Accepted in the Demonstration Track at ICAIL 2025", "summary": "In this paper, we propose a pipeline leveraging Large Language Models (LLMs) for data augmentation in Information Extraction tasks within the legal domain. The proposed method is both simple and effective, significantly reducing the manual effort required for data annotation while enhancing the robustness of Information Extraction systems. Furthermore, the method is generalizable, making it applicable to various Natural Language Processing (NLP) tasks beyond the legal domain.", "AI": {"tldr": "\u672c\u6587\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6570\u636e\u589e\u5f3a\uff0c\u63d0\u5347\u6cd5\u5f8b\u9886\u57df\u4fe1\u606f\u62bd\u53d6\u7684\u6548\u7387\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u5177\u5907\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u4eba\u5de5\u6807\u6ce8\u6cd5\u5f8b\u9886\u57df\u4fe1\u606f\u62bd\u53d6\u4efb\u52a1\u7684\u6570\u636e\u6210\u672c\u9ad8\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u589e\u5f3a\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u6570\u636e\u589e\u5f3a\u7684\u6d41\u6c34\u7ebf\uff0c\u7b80\u5316\u6570\u636e\u6807\u6ce8\u6d41\u7a0b\u3002", "result": "\u663e\u8457\u51cf\u5c11\u624b\u5de5\u6807\u6ce8\u5de5\u4f5c\u91cf\uff0c\u63d0\u9ad8\u4fe1\u606f\u62bd\u53d6\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7b80\u5355\u6709\u6548\uff0c\u4e14\u5177\u6709\u901a\u7528\u6027\uff0c\u53ef\u63a8\u5e7f\u81f3\u6cd5\u5f8b\u9886\u57df\u5916\u7684\u591a\u79cd\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u3002"}}
{"id": "2601.05624", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05624", "abs": "https://arxiv.org/abs/2601.05624", "authors": ["Abayomi O. Agbeyangi"], "title": "Text Detoxification in isiXhosa and Yor\u00f9b\u00e1: A Cross-Lingual Machine Learning Approach for Low-Resource African Languages", "comment": "26 pages, 9 figures and 1 algorithm", "summary": "Toxic language is one of the major barrier to safe online participation, yet robust mitigation tools are scarce for African languages. This study addresses this critical gap by investigating automatic text detoxification (toxic to neutral rewriting) for two low-resource African languages, isiXhosa and Yor\u00f9b\u00e1. The work contributes a novel, pragmatic hybrid methodology: a lightweight, interpretable TF-IDF and Logistic Regression model for transparent toxicity detection, and a controlled lexicon- and token-guided rewriting component. A parallel corpus of toxic to neutral rewrites, which captures idiomatic usage, diacritics, and code switching, was developed to train and evaluate the model. The detection component achieved stratified K-fold accuracies of 61-72% (isiXhosa) and 72-86% (Yor\u00f9b\u00e1), with per-language ROC-AUCs up to 0.88. The rewriting component successfully detoxified all detected toxic sentences while preserving 100% of non-toxic sentences. These results demonstrate that scalable, interpretable machine learning detectors combined with rule-based edits offer a competitive and resource-efficient solution for culturally adaptive safety tooling, setting a new benchmark for low-resource Text Style Transfer (TST) in African languages.", "AI": {"tldr": "\u672c\u7814\u7a76\u9488\u5bf9\u975e\u6d32\u4f4e\u8d44\u6e90\u8bed\u8a00isiXhosa\u548cYor\u00f9b\u00e1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408TF-IDF+\u903b\u8f91\u56de\u5f52\u68c0\u6d4b\u548c\u8bcd\u5178+\u4ee4\u724c\u6307\u5bfc\u91cd\u5199\u7684\u81ea\u52a8\u6587\u672c\u53bb\u6bd2\u65b9\u6cd5\u3002", "motivation": "\u9488\u5bf9\u975e\u6d32\u8bed\u8a00\u7f3a\u4e4f\u5b89\u5168\u7684\u81ea\u52a8\u53bb\u6bd2\u5de5\u5177\u7684\u73b0\u72b6\uff0c\u7814\u7a76\u6709\u6548\u4e14\u9002\u5408\u4f4e\u8d44\u6e90\u73af\u5883\u7684\u6587\u672c\u53bb\u6bd2\u6280\u672f\uff0c\u4fc3\u8fdb\u7528\u6237\u7684\u5b89\u5168\u5728\u7ebf\u53c2\u4e0e\u3002", "method": "\u91c7\u7528\u8f7b\u91cf\u4e14\u53ef\u89e3\u91ca\u7684TF-IDF\u7279\u5f81\u4e0e\u903b\u8f91\u56de\u5f52\u8fdb\u884c\u6bd2\u6027\u68c0\u6d4b\uff0c\u7ed3\u5408\u57fa\u4e8e\u8bcd\u5178\u548c\u4ee4\u724c\u7684\u6307\u5bfc\u89c4\u5219\u5b9e\u73b0\u63a7\u5236\u6027\u6587\u672c\u91cd\u5199\uff0c\u8bad\u7ec3\u548c\u8bc4\u4f30\u57fa\u4e8e\u6784\u5efa\u7684\u6bd2\u6027-\u4e2d\u6027\u5e73\u884c\u8bed\u6599\u3002", "result": "\u6bd2\u6027\u68c0\u6d4b\u5728isiXhosa\u548cYor\u00f9b\u00e1\u8bed\u8a00\u4e2d\u5206\u522b\u8fbe\u523061-72%\u548c72-86%\u7684\u51c6\u786e\u7387\uff0c\u6700\u9ad8ROC-AUC\u8fbe0.88\uff0c\u91cd\u5199\u7ec4\u4ef6\u6210\u529f\u6d88\u9664\u6240\u6709\u6709\u6bd2\u53e5\u5b50\u4e14\u4fdd\u6301\u65e0\u6bd2\u53e5\u5b50\u4e0d\u53d8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u6709\u7ade\u4e89\u529b\u4e14\u8d44\u6e90\u9ad8\u6548\u7684\u6587\u672c\u53bb\u6bd2\uff0c\u68c0\u6d4b\u51c6\u786e\u738761-86%\uff0cROC-AUC\u6700\u9ad80.88\uff0c\u91cd\u5199\u7ec4\u4ef6\u5b8c\u5168\u53bb\u9664\u4e86\u6709\u6bd2\u8bed\u53e5\u4e14\u4e0d\u5f71\u54cd\u65e0\u6bd2\u8bed\u53e5\uff0c\u63a8\u52a8\u4e86\u975e\u6d32\u8bed\u8a00\u6587\u672c\u98ce\u683c\u8fc1\u79fb\u7814\u7a76\u3002"}}
{"id": "2601.05633", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05633", "abs": "https://arxiv.org/abs/2601.05633", "authors": ["Nuoyan Lyu", "Bingbing Xu", "Weihao Meng", "Yige Yuan", "Yang Zhang", "Zhiyong Huang", "Tat-Seng Chua", "Huawei Shen"], "title": "GIFT: Games as Informal Training for Generalizable LLMs", "comment": null, "summary": "While Large Language Models (LLMs) have achieved remarkable success in formal learning tasks such as mathematics and code generation, they still struggle with the \"practical wisdom\" and generalizable intelligence, such as strategic creativity and social reasoning, that characterize human cognition. This gap arises from a lack of informal learning, which thrives on interactive feedback rather than goal-oriented instruction. In this paper, we propose treating Games as a primary environment for LLM informal learning, leveraging their intrinsic reward signals and abstracted complexity to cultivate diverse competencies. To address the performance degradation observed in multi-task learning, we introduce a Nested Training Framework. Unlike naive task mixing optimizing an implicit \"OR\" objective, our framework employs sequential task composition to enforce an explicit \"AND\" objective, compelling the model to master multiple abilities simultaneously to achieve maximal rewards. Using GRPO-based reinforcement learning across Matrix Games, TicTacToe, and Who's the Spy games, we demonstrate that integrating game-based informal learning not only prevents task interference but also significantly bolsters the model's generalization across broad ability-oriented benchmarks. The framework and implementation are publicly available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6e38\u620f\u73af\u5883\u7684\u5d4c\u5957\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u975e\u6b63\u5f0f\u5b66\u4e60\u80fd\u529b\uff0c\u589e\u5f3a\u5176\u5b9e\u7528\u667a\u6167\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u5728\u5f62\u5f0f\u5316\u5b66\u4e60\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u4f53\u73b0\u4eba\u7c7b\u8ba4\u77e5\u7279\u5f81\u7684\"\u5b9e\u8df5\u667a\u6167\"\u548c\u901a\u7528\u667a\u80fd\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u4e3b\u8981\u7531\u4e8e\u7f3a\u4e4f\u4f9d\u8d56\u4ea4\u4e92\u53cd\u9988\u7684\u975e\u6b63\u5f0f\u5b66\u4e60\u3002", "method": "\u5c06\u6e38\u620f\u4f5c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b(LLMs)\u975e\u6b63\u5f0f\u5b66\u4e60\u7684\u4e3b\u8981\u73af\u5883\uff0c\u5229\u7528\u6e38\u620f\u5185\u5728\u7684\u5956\u52b1\u4fe1\u53f7\u548c\u62bd\u8c61\u590d\u6742\u6027\uff0c\u901a\u8fc7GRPO\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u591a\u79cd\u6e38\u620f\u4e2d\u8fdb\u884c\u8bad\u7ec3\uff1b\u5f15\u5165\u5d4c\u5957\u8bad\u7ec3\u6846\u67b6\uff0c\u91c7\u7528\u987a\u5e8f\u4efb\u52a1\u7ec4\u5408\uff0c\u5b9e\u73b0\u663e\u5f0f\"AND\"\u76ee\u6807\uff0c\u4ee5\u514b\u670d\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u7684\u6027\u80fd\u9000\u5316\u95ee\u9898\u3002", "result": "\u57fa\u4e8e\u6e38\u620f\u7684\u975e\u6b63\u5f0f\u5b66\u4e60\u6846\u67b6\u6709\u6548\u907f\u514d\u4e86\u4efb\u52a1\u95f4\u5e72\u6270\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u80fd\u529b\u5bfc\u5411\u5e7f\u6cdb\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e14\u76f8\u5173\u6846\u67b6\u548c\u5b9e\u73b0\u5df2\u516c\u5f00\u3002", "conclusion": "\u6e38\u620f\u4f5c\u4e3a\u975e\u6b63\u5f0f\u5b66\u4e60\u73af\u5883\uff0c\u53ef\u5f25\u8865\u5927\u8bed\u8a00\u6a21\u578b\u5728\u901a\u7528\u667a\u80fd\u65b9\u9762\u7684\u4e0d\u8db3\uff1b\u901a\u8fc7\u663e\u5f0f\u591a\u4efb\u52a1\u987a\u5e8f\u8bad\u7ec3\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u6a21\u578b\u591a\u80fd\u529b\u638c\u63e1\u53ca\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2601.05641", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.05641", "abs": "https://arxiv.org/abs/2601.05641", "authors": ["Alireza Dehghanpour Farashah", "Aditi Khandelwal", "Marylou Fauchard", "Zhuan Shi", "Negar Rostamzadeh", "Golnoosh Farnadi"], "title": "Multilingual Amnesia: On the Transferability of Unlearning in Multilingual LLMs", "comment": null, "summary": "As multilingual large language models become more widely used, ensuring their safety and fairness across diverse linguistic contexts presents unique challenges. While existing research on machine unlearning has primarily focused on monolingual settings, typically English, multilingual environments introduce additional complexities due to cross-lingual knowledge transfer and biases embedded in both pretraining and fine-tuning data. In this work, we study multilingual unlearning using the Aya-Expanse 8B model under two settings: (1) data unlearning and (2) concept unlearning. We extend benchmarks for factual knowledge and stereotypes to ten languages through translation: English, French, Arabic, Japanese, Russian, Farsi, Korean, Hindi, Hebrew, and Indonesian. These languages span five language families and a wide range of resource levels. Our experiments show that unlearning in high-resource languages is generally more stable, with asymmetric transfer effects observed between typologically related languages. Furthermore, our analysis of linguistic distances indicates that syntactic similarity is the strongest predictor of cross-lingual unlearning behavior.", "AI": {"tldr": "\u672c\u6587\u63a2\u7d22\u4e86\u591a\u8bed\u8a00\u5927\u6a21\u578b\u4e2d\u7684\u9057\u5fd8\u95ee\u9898\uff0c\u8986\u76d6\u5341\u79cd\u8bed\u8a00\uff0c\u53d1\u73b0\u9ad8\u8d44\u6e90\u8bed\u8a00\u9057\u5fd8\u66f4\u7a33\u5b9a\uff0c\u53e5\u6cd5\u76f8\u4f3c\u6027\u662f\u8de8\u8bed\u8a00\u9057\u5fd8\u7684\u5173\u952e\u56e0\u7d20\u3002", "motivation": "\u968f\u7740\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u7684\u666e\u53ca\uff0c\u786e\u4fdd\u5176\u5728\u4e0d\u540c\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6027\u548c\u516c\u5e73\u6027\u6311\u6218\u589e\u591a\uff0c\u5c24\u5176\u662f\u5904\u7406\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u6570\u636e\u4e2d\u5b58\u5728\u7684\u504f\u89c1\u548c\u77e5\u8bc6\u9057\u5fd8\u9700\u6c42\uff1b\u9488\u5bf9\u5355\u8bed\u8a00\u7814\u7a76\u7684\u4e0d\u8db3\uff0c\u5f00\u5c55\u591a\u8bed\u8a00\u9057\u5fd8\u673a\u5236\u7814\u7a76\u663e\u5f97\u5c24\u4e3a\u91cd\u8981\u3002", "method": "\u57fa\u4e8eAya-Expanse 8B\u6a21\u578b\uff0c\u4f5c\u8005\u901a\u8fc7\u7ffb\u8bd1\u6269\u5c55\u4e86\u4e8b\u5b9e\u77e5\u8bc6\u548c\u523b\u677f\u5370\u8c61\u57fa\u51c6\u5230\u5341\u79cd\u8bed\u8a00\uff0c\u8bbe\u8ba1\u6570\u636e\u9057\u5fd8\u548c\u6982\u5ff5\u9057\u5fd8\u4e24\u79cd\u5b9e\u9a8c\u8bbe\u7f6e\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u8bed\u8a00\u4e0b\u7684\u9057\u5fd8\u8868\u73b0\u53ca\u5176\u8de8\u8bed\u8a00\u8f6c\u79fb\u6548\u5e94\u3002", "result": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u4fe1\u606f\u9057\u5fd8\u95ee\u9898\uff0c\u91cd\u70b9\u5728\u6570\u636e\u9057\u5fd8\u548c\u6982\u5ff5\u9057\u5fd8\u4e24\u79cd\u60c5\u5883\u4e0b\u7684\u8868\u73b0\u3002\u4f5c\u8005\u4ee5Aya-Expanse 8B\u6a21\u578b\u4e3a\u57fa\u7840\uff0c\u5c06\u4e8b\u5b9e\u77e5\u8bc6\u548c\u523b\u677f\u5370\u8c61\u7684\u57fa\u51c6\u6269\u5c55\u5230\u5305\u62ec\u5341\u79cd\u4e0d\u540c\u8bed\u8a00\uff0c\u8fd9\u4e9b\u8bed\u8a00\u6db5\u76d6\u4e94\u4e2a\u8bed\u8a00\u5bb6\u65cf\u548c\u4e0d\u540c\u7684\u8d44\u6e90\u4e30\u5bcc\u5ea6\u3002\u5b9e\u9a8c\u53d1\u73b0\uff0c\u9ad8\u8d44\u6e90\u8bed\u8a00\u4e2d\u7684\u9057\u5fd8\u884c\u4e3a\u66f4\u7a33\u5b9a\uff0c\u540c\u65f6\u5b58\u5728\u8bed\u8a00\u7c7b\u578b\u76f8\u5173\u8bed\u8a00\u4e4b\u95f4\u7684\u4e0d\u5bf9\u79f0\u8fc1\u79fb\u6548\u5e94\u3002\u8bcd\u6cd5\u548c\u53e5\u6cd5\u4e4b\u95f4\uff0c\u53e5\u6cd5\u76f8\u4f3c\u6027\u662f\u9884\u6d4b\u8de8\u8bed\u8a00\u9057\u5fd8\u884c\u4e3a\u7684\u6700\u5f3a\u56e0\u7d20\u3002", "conclusion": "\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u9057\u5fd8\u884c\u4e3a\u8868\u73b0\u51fa\u8bed\u8a00\u8d44\u6e90\u4e30\u5bcc\u5ea6\u548c\u8bed\u8a00\u7c7b\u578b\u7684\u5f71\u54cd\uff0c\u53e5\u6cd5\u76f8\u4f3c\u6027\u5f3a\u70c8\u5f71\u54cd\u8de8\u8bed\u8a00\u9057\u5fd8\u6548\u679c\u3002"}}
{"id": "2601.05654", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05654", "abs": "https://arxiv.org/abs/2601.05654", "authors": ["Sejun Park", "Yoonah Park", "Jongwon Lim", "Yohan Jo"], "title": "A Framework for Personalized Persuasiveness Prediction via Context-Aware User Profiling", "comment": null, "summary": "Estimating the persuasiveness of messages is critical in various applications, from recommender systems to safety assessment of LLMs. While it is imperative to consider the target persuadee's characteristics, such as their values, experiences, and reasoning styles, there is currently no established systematic framework to optimize leveraging a persuadee's past activities (e.g., conversations) to the benefit of a persuasiveness prediction model. To address this problem, we propose a context-aware user profiling framework with two trainable components: a query generator that generates optimal queries to retrieve persuasion-relevant records from a user's history, and a profiler that summarizes these records into a profile to effectively inform the persuasiveness prediction model. Our evaluation on the ChangeMyView Reddit dataset shows consistent improvements over existing methods across multiple predictor models, with gains of up to +13.77%p in F1 score. Further analysis shows that effective user profiles are context-dependent and predictor-specific, rather than relying on static attributes or surface-level similarity. Together, these results highlight the importance of task-oriented, context-dependent user profiling for personalized persuasiveness prediction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u7528\u6237\u753b\u50cf\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u67e5\u8be2\u83b7\u53d6\u4e0e\u8bf4\u670d\u76f8\u5173\u7684\u7528\u6237\u5386\u53f2\u8bb0\u5f55\uff0c\u5e76\u5c06\u5176\u603b\u7ed3\u4e3a\u7528\u6237\u753b\u50cf\uff0c\u7528\u4e8e\u63d0\u5347\u8bf4\u670d\u529b\u9884\u6d4b\u6a21\u578b\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u7f3a\u4e4f\u7cfb\u7edf\u6846\u67b6\u5229\u7528\u8bf4\u670d\u5bf9\u8c61\u7684\u5386\u53f2\u6d3b\u52a8\u4fe1\u606f\uff0c\u65e0\u6cd5\u6709\u6548\u7ed3\u5408\u7528\u6237\u7279\u70b9\u4f18\u5316\u8bf4\u670d\u529b\u9884\u6d4b\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u53ef\u8bad\u7ec3\u7ec4\u4ef6\uff1a\u67e5\u8be2\u751f\u6210\u5668\u7528\u4e8e\u68c0\u7d22\u5386\u53f2\u8bf4\u670d\u76f8\u5173\u8bb0\u5f55\uff0c\u753b\u50cf\u5668\u7528\u4e8e\u603b\u7ed3\u8bb0\u5f55\u5f62\u6210\u7528\u6237\u753b\u50cf\uff0c\u7136\u540e\u5c06\u5176\u7528\u4e8e\u8bf4\u670d\u529b\u9884\u6d4b\u3002", "result": "\u5728ChangeMyView Reddit\u6570\u636e\u96c6\u4e0a\u7684\u591a\u79cd\u9884\u6d4b\u6a21\u578b\u4e2d\u5747\u89c1\u63d0\u5347\uff0c\u663e\u793a\u4e0a\u4e0b\u6587\u76f8\u5173\u548c\u9884\u6d4b\u5668\u7279\u5b9a\u7684\u4e2a\u6027\u5316\u7528\u6237\u753b\u50cf\u7684\u4f5c\u7528\u3002", "conclusion": "\u901a\u8fc7\u4efb\u52a1\u5bfc\u5411\u548c\u4e0a\u4e0b\u6587\u4f9d\u8d56\u7684\u7528\u6237\u753b\u50cf\uff0c\u6a21\u578b\u5728\u8bf4\u670d\u529b\u9884\u6d4b\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cF1\u5206\u6570\u63d0\u5347\u6700\u9ad8\u8fbe13.77%\u3002"}}
{"id": "2601.05657", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05657", "abs": "https://arxiv.org/abs/2601.05657", "authors": ["Hao Yang", "Hongyuan Lu", "Dingkang Yang", "Wenliang Yang", "Peng Sun", "Xiaochuan Zhang", "Jun Xiao", "Kefan He", "Wai Lam", "Yang Liu", "Xinhua Zeng"], "title": "Stephanie2: Thinking, Waiting, and Making Decisions Like Humans in Step-by-Step AI Social Chat", "comment": "13 pages", "summary": "Instant-messaging human social chat typically progresses through a sequence of short messages. Existing step-by-step AI chatting systems typically split a one-shot generation into multiple messages and send them sequentially, but they lack an active waiting mechanism and exhibit unnatural message pacing. In order to address these issues, we propose Stephanie2, a novel next-generation step-wise decision-making dialogue agent. With active waiting and message-pace adaptation, Stephanie2 explicitly decides at each step whether to send or wait, and models latency as the sum of thinking time and typing time to achieve more natural pacing. We further introduce a time-window-based dual-agent dialogue system to generate pseudo dialogue histories for human and automatic evaluations. Experiments show that Stephanie2 clearly outperforms Stephanie1 on metrics such as naturalness and engagement, and achieves a higher pass rate on human evaluation with the role identification Turing test.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faStephanie2\u4e00\u6b65\u6b65\u51b3\u7b56\u7684\u804a\u5929\u4ee3\u7406\uff0c\u901a\u8fc7\u4e3b\u52a8\u7b49\u5f85\u548c\u8282\u594f\u63a7\u5236\u6539\u5584\u5373\u65f6\u901a\u8baf\u804a\u5929\u7684\u81ea\u7136\u6027\u548c\u4e92\u52a8\u4f53\u9a8c\uff0c\u4f18\u4e8e\u524d\u4ee3\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u5206\u6b65\u751f\u6210\u804a\u5929\u7cfb\u7edf\u7f3a\u4e4f\u4e3b\u52a8\u7b49\u5f85\u673a\u5236\uff0c\u6d88\u606f\u53d1\u9001\u8282\u594f\u4e0d\u81ea\u7136\uff0c\u5f71\u54cd\u4e86\u804a\u5929\u7684\u771f\u5b9e\u611f\u3002", "method": "\u91c7\u7528\u4e3b\u52a8\u7b49\u5f85\u673a\u5236\u548c\u6d88\u606f\u8282\u594f\u9002\u5e94\u7b56\u7565\uff0c\u7ed3\u5408\u601d\u8003\u548c\u6253\u5b57\u65f6\u95f4\u5efa\u6a21\u5ef6\u8fdf\uff0c\u5e76\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u65f6\u95f4\u7a97\u53e3\u7684\u53cc\u4ee3\u7406\u5bf9\u8bdd\u7cfb\u7edf\u751f\u6210\u4f2a\u5bf9\u8bdd\u5386\u53f2\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "Stephanie2\u5728\u81ea\u7136\u6027\u3001\u4e92\u52a8\u6027\u6307\u6807\u4e0a\u8d85\u8fc7Stephanie1\uff0c\u5e76\u5728\u5305\u542b\u89d2\u8272\u8bc6\u522b\u7684\u56fe\u7075\u6d4b\u8bd5\u4e2d\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "Stephanie2\u6709\u6548\u63d0\u5347\u4e86\u5373\u65f6\u901a\u8baf\u4eba\u7c7b\u793e\u4ea4\u804a\u5929\u7684\u81ea\u7136\u6027\u548c\u4e92\u52a8\u6027\uff0c\u4f18\u4e8e\u524d\u4ee3Stephanie1\u3002"}}
{"id": "2601.05699", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05699", "abs": "https://arxiv.org/abs/2601.05699", "authors": ["Atnafu Lambebo Tonja", "Srija Anand", "Emilio Villa-Cueva", "Israel Abebe Azime", "Jesujoba Oluwadara Alabi", "Muhidin A. Mohamed", "Debela Desalegn Yadeta", "Negasi Haile Abadi", "Abigail Oppong", "Nnaemeka Casmir Obiefuna", "Idris Abdulmumin", "Naome A Etori", "Eric Peter Wairagala", "Kanda Patrick Tshinu", "Imanigirimbabazi Emmanuel", "Gabofetswe Malema", "Alham Fikri Aji", "David Ifeoluwa Adelani", "Thamar Solorio"], "title": "Afri-MCQA: Multimodal Cultural Question Answering for African Languages", "comment": null, "summary": "Africa is home to over one-third of the world's languages, yet remains underrepresented in AI research. We introduce Afri-MCQA, the first Multilingual Cultural Question-Answering benchmark covering 7.5k Q&A pairs across 15 African languages from 12 countries. The benchmark offers parallel English-African language Q&A pairs across text and speech modalities and was entirely created by native speakers. Benchmarking large language models (LLMs) on Afri-MCQA shows that open-weight models perform poorly across evaluated cultures, with near-zero accuracy on open-ended VQA when queried in native language or speech. To evaluate linguistic competence, we include control experiments meant to assess this specific aspect separate from cultural knowledge, and we observe significant performance gaps between native languages and English for both text and speech. These findings underscore the need for speech-first approaches, culturally grounded pretraining, and cross-lingual cultural transfer. To support more inclusive multimodal AI development in African languages, we release our Afri-MCQA under academic license or CC BY-NC 4.0 on HuggingFace (https://huggingface.co/datasets/Atnafu/Afri-MCQA)", "AI": {"tldr": "Afri-MCQA\u6570\u636e\u96c6\u8986\u76d615\u79cd\u975e\u6d32\u8bed\u8a00\u548c\u6587\u672c\u8bed\u97f3\u4e24\u6a21\u6001\uff0c\u6d4b\u8bd5\u53d1\u73b0\u5927\u6a21\u578b\u5728\u975e\u6d32\u8bed\u8a00\u4e0a\u7684\u8868\u73b0\u6781\u5dee\uff0c\u5f3a\u8c03\u4e86\u5f00\u53d1\u6587\u5316\u548c\u8bed\u8a00\u5b9a\u5236\u6a21\u578b\u7684\u5fc5\u8981\u6027\uff0c\u6570\u636e\u96c6\u516c\u5f00\u4fc3\u8fdb\u66f4\u5305\u5bb9\u7684\u975e\u6d32\u8bed\u8a00AI\u7814\u7a76\u3002", "motivation": "\u975e\u6d32\u62e5\u6709\u4e30\u5bcc\u7684\u8bed\u8a00\u8d44\u6e90\u4f46\u5728AI\u7814\u7a76\u4e2d\u4e25\u91cd\u7f3a\u5931\uff0c\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u975e\u6d32\u8bed\u8a00\u4e0a\u7684\u8868\u73b0\u6781\u5dee\uff0c\u4e9f\u9700\u521b\u5efa\u591a\u8bed\u79cd\u3001\u6587\u5316\u76f8\u5173\u7684\u6570\u636e\u96c6\u4ee5\u63a8\u52a8\u66f4\u5177\u5305\u5bb9\u6027\u7684AI\u53d1\u5c55\u3002", "method": "\u6784\u5efa\u6db5\u76d615\u79cd\u975e\u6d32\u8bed\u8a00\u7684\u591a\u8bed\u8a00\u6587\u5316\u95ee\u7b54\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7531\u6bcd\u8bed\u8005\u5b8c\u6210\uff0c\u5305\u542b\u6587\u672c\u548c\u8bed\u97f3\u6a21\u6001\uff1b\u901a\u8fc7\u5bf9\u591a\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6d4b\u8bd5\u548c\u63a7\u5236\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u6a21\u578b\u7684\u6587\u5316\u77e5\u8bc6\u548c\u8bed\u8a00\u80fd\u529b\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86Afri-MCQA\uff0c\u8fd9\u662f\u9996\u4e2a\u6db5\u76d615\u79cd\u975e\u6d32\u8bed\u8a00\u30017.5\u5343\u6761\u95ee\u7b54\u5bf9\u7684\u591a\u8bed\u8a00\u6587\u5316\u95ee\u7b54\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u6587\u672c\u548c\u8bed\u97f3\u4e24\u79cd\u6a21\u6001\uff0c\u7531\u6bcd\u8bed\u8005\u521b\u5efa\u3002\u901a\u8fc7\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6d4b\u8bd5\u53d1\u73b0\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u975e\u6d32\u672c\u5730\u8bed\u8a00\u6216\u8bed\u97f3\u7684\u5f00\u653e\u5f0f\u89c6\u89c9\u95ee\u7b54\u4e0a\u8868\u73b0\u6781\u5dee\uff0c\u51c6\u786e\u7387\u8fd1\u4e4e\u4e3a\u96f6\u3002\u63a7\u5236\u5b9e\u9a8c\u663e\u793a\u6bcd\u8bed\u548c\u82f1\u8bed\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u8ddd\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5bf9\u975e\u6d32\u8bed\u8a00\u7684\u6587\u5316\u548c\u8bed\u8a00\u80fd\u529b\u652f\u6301\u4e0d\u8db3\u3002\u4f5c\u8005\u5f3a\u8c03\u4e86\u9700\u8981\u4ee5\u8bed\u97f3\u4f18\u5148\u7684\u65b9\u6cd5\u3001\u6587\u5316\u6839\u690d\u9884\u8bad\u7ec3\u53ca\u8de8\u8bed\u8a00\u6587\u5316\u8fc1\u79fb\uff0c\u5e76\u5c06\u6570\u636e\u96c6\u4ee5\u5b66\u672f\u8bb8\u53ef\u548cCC BY-NC 4.0\u534f\u8bae\u516c\u5f00\uff0c\u4fc3\u8fdb\u975e\u6d32\u8bed\u8a00\u591a\u6a21\u6001AI\u7684\u53d1\u5c55\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u975e\u6d32\u672c\u5730\u8bed\u8a00\u53ca\u8bed\u97f3\u6a21\u5f0f\u4e0b\u8868\u73b0\u4e0d\u8db3\uff0c\u5b58\u5728\u663e\u8457\u7684\u8bed\u8a00\u53ca\u6587\u5316\u77e5\u8bc6\u5dee\u8ddd\uff0c\u672a\u6765\u9700\u91c7\u7528\u8bed\u97f3\u4f18\u5148\u3001\u6587\u5316\u690d\u5165\u7684\u9884\u8bad\u7ec3\u53ca\u8de8\u8bed\u8a00\u8fc1\u79fb\u65b9\u6cd5\u6765\u63d0\u5347\u6a21\u578b\u8868\u73b0\u3002"}}
{"id": "2601.05707", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05707", "abs": "https://arxiv.org/abs/2601.05707", "authors": ["Zhaolin Li", "Jan Niehues"], "title": "Multimodal In-context Learning for ASR of Low-resource Languages", "comment": "Under review", "summary": "Automatic speech recognition (ASR) still covers only a small fraction of the world's languages, mainly due to supervised data scarcity. In-context learning (ICL) with large language models (LLMs) addresses this problem, but prior work largely focuses on high-resource languages covered during training and text-only settings. This paper investigates whether speech LLMs can learn unseen languages with multimodal ICL (MICL), and how this learning can be used to improve ASR. We conduct experiments with two speech LLMs, Phi-4 and Qwen3-Omni, on three diverse endangered languages. Firstly, we find that MICL is effective for unseen languages, leveraging both speech and text modalities. We further show that cross-lingual transfer learning improves MICL efficiency on target languages without training on them. Moreover, we analyze attention patterns to interpret MICL mechanisms, and we observe layer-dependent preferences between audio and text context, with an overall bias towards text. Finally, we show that prompt-based ASR with speech LLMs performs poorly on unseen languages, motivating a simple ASR system that combines a stronger acoustic model with a speech LLM via MICL-based selection of acoustic hypotheses. Results show that MICL consistently improves ASR performance, and that cross-lingual transfer learning matches or outperforms corpus-trained language models without using target-language data. Our code is publicly available.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e0e\u8de8\u8bed\u8a00\u8fc1\u79fb\u5b66\u4e60\uff0c\u6709\u6548\u63d0\u5347\u672a\u89c1\u8bed\u8a00\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u53d7\u9650\u4e8e\u76d1\u7763\u6570\u636e\u7a00\u7f3a\uff0c\u5c24\u5176\u662f\u8bb8\u591a\u8bed\u8a00\u672a\u88ab\u8986\u76d6\u3002\u4f20\u7edf\u4e0a\u4e0b\u6587\u5b66\u4e60\u591a\u9488\u5bf9\u8bad\u7ec3\u4e2d\u9ad8\u8d44\u6e90\u8bed\u8a00\u4e14\u4e3b\u8981\u4e3a\u7eaf\u6587\u672c\uff0c\u7f3a\u4e4f\u9488\u5bf9\u672a\u89c1\u8bed\u8a00\u7684\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5b66\u4e60\u7814\u7a76\u3002", "method": "\u672c\u6587\u91c7\u7528\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08MICL\uff09\u65b9\u6cd5\uff0c\u7ed3\u5408\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\uff08Phi-4\u548cQwen3-Omni\uff09\uff0c\u9488\u5bf9\u4e09\u79cd\u6fd2\u5371\u8bed\u8a00\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5206\u6790MICL\u5728\u672a\u89c1\u8fc7\u8bed\u8a00\u4e0a\u7684\u6548\u679c\u53ca\u8de8\u8bed\u8a00\u8fc1\u79fb\u5b66\u4e60\u7684\u63d0\u5347\u4f5c\u7528\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eMICL\u5728\u672a\u89c1\u8bed\u8a00\u4e0a\u6709\u6548\uff0c\u901a\u8fc7\u5229\u7528\u8bed\u97f3\u548c\u6587\u672c\u589e\u5f3a\u8bc6\u522b\u3002\u8de8\u8bed\u8a00\u8fc1\u79fb\u5b66\u4e60\u8fdb\u4e00\u6b65\u63d0\u5347\u76ee\u6807\u8bed\u8a00\u7684MICL\u6548\u7387\u3002\u6ce8\u610f\u529b\u6a21\u5f0f\u5206\u6790\u663e\u793a\u4e0d\u540c\u5c42\u5bf9\u97f3\u9891\u548c\u6587\u672c\u7684\u504f\u597d\u4e0d\u540c\uff0c\u6587\u672c\u6574\u4f53\u5360\u4f18\u3002\u57fa\u4e8e\u63d0\u793a\u7684ASR\u5728\u672a\u89c1\u8bed\u8a00\u8868\u73b0\u8f83\u5dee\uff0c\u878d\u5408\u66f4\u5f3a\u58f0\u5b66\u6a21\u578b\u548cspeech LLM\u901a\u8fc7MICL\u9009\u62e9\u58f0\u5b66\u5047\u8bbe\u663e\u8457\u63d0\u9ad8ASR\u6027\u80fd\u3002\u8de8\u8bed\u8a00\u8fc1\u79fb\u5b66\u4e60\u6027\u80fd\u53ef\u4e0e\u8bad\u7ec3\u8fc7\u76ee\u6807\u8bed\u8a00\u7684\u6a21\u578b\u5ab2\u7f8e\u3002", "conclusion": "MICL\u7ed3\u5408\u8de8\u8bed\u8a00\u8fc1\u79fb\u80fd\u591f\u663e\u8457\u63d0\u5347\u6fd2\u5371\u8bed\u8a00\u7684\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u6548\u679c\uff0c\u4e14\u65e0\u9700\u76ee\u6807\u8bed\u8a00\u6570\u636e\uff0c\u63d0\u793a\u57fa\u4e8e\u58f0\u5b66\u5047\u8bbe\u9009\u62e9\u7684\u7ed3\u5408\u7b56\u7565\u5728\u8fd9\u4e00\u9886\u57df\u5e94\u7528\u6f5c\u529b\u5de8\u5927\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2601.05713", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.05713", "abs": "https://arxiv.org/abs/2601.05713", "authors": ["Thomas Fabian"], "title": "Visualising Information Flow in Word Embeddings with Diffusion Tensor Imaging", "comment": null, "summary": "Understanding how large language models (LLMs) represent natural language is a central challenge in natural language processing (NLP) research. Many existing methods extract word embeddings from an LLM, visualise the embedding space via point-plots, and compare the relative positions of certain words. However, this approach only considers single words and not whole natural language expressions, thus disregards the context in which a word is used. Here we present a novel tool for analysing and visualising information flow in natural language expressions by applying diffusion tensor imaging (DTI) to word embeddings. We find that DTI reveals how information flows between word embeddings. Tracking information flows within the layers of an LLM allows for comparing different model structures and revealing opportunities for pruning an LLM's under-utilised layers. Furthermore, our model reveals differences in information flows for tasks like pronoun resolution and metaphor detection. Our results show that our model permits novel insights into how LLMs represent actual natural language expressions, extending the comparison of isolated word embeddings and improving the interpretability of NLP models.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5f15\u5165DTI\u6280\u672f\uff0c\u521b\u65b0\u6027\u5730\u5206\u6790\u5e76\u53ef\u89c6\u5316\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u81ea\u7136\u8bed\u8a00\u8868\u8fbe\u7684\u4fe1\u606f\u6d41\u52a8\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u53ea\u6bd4\u8f83\u5355\u8bcd\u5d4c\u5165\u7684\u76f8\u5bf9\u4f4d\u7f6e\uff0c\u5ffd\u7565\u4e86\u8bcd\u5728\u4e0a\u4e0b\u6587\u4e2d\u7684\u4f7f\u7528\uff0c\u96be\u4ee5\u63ed\u793a\u81ea\u7136\u8bed\u8a00\u8868\u8fbe\u7684\u4fe1\u606f\u4f20\u9012\u673a\u5236\u3002", "method": "\u91c7\u7528\u6269\u6563\u5f20\u91cf\u6210\u50cf\uff08DTI\uff09\u6280\u672f\u5206\u6790\u548c\u53ef\u89c6\u5316\u8bcd\u5d4c\u5165\u4e2d\u7684\u4fe1\u606f\u6d41\u52a8\uff0c\u8ffd\u8e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5404\u5c42\u4e2d\u7684\u4fe1\u606f\u6d41\u52a8\u3002", "result": "DTI\u63ed\u793a\u4e86\u8bcd\u5d4c\u5165\u95f4\u4fe1\u606f\u7684\u6d41\u52a8\u65b9\u5f0f\uff0c\u4e0d\u540c\u4efb\u52a1\uff08\u5982\u4ee3\u8bcd\u89e3\u6790\u548c\u9690\u55bb\u8bc6\u522b\uff09\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u4fe1\u606f\u6d41\u6a21\u5f0f\uff1b\u540c\u65f6\u5e2e\u52a9\u6bd4\u8f83\u548c\u526a\u679d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u672a\u5145\u5206\u5229\u7528\u7684\u5c42\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6269\u5c55\u4e86\u5bf9\u8bed\u8a00\u6a21\u578b\u4e2d\u8bcd\u5d4c\u5165\u8868\u793a\u7684\u7406\u89e3\uff0c\u4ece\u5355\u8bcd\u5c42\u9762\u5230\u8868\u8fbe\u5c42\u9762\uff0c\u4fc3\u8fdb\u4e86\u5bf9\u6a21\u578b\u7ed3\u6784\u548c\u4efb\u52a1\u8868\u73b0\u7684\u6df1\u5165\u5206\u6790\u4e0e\u4f18\u5316\u3002"}}
{"id": "2601.05751", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05751", "abs": "https://arxiv.org/abs/2601.05751", "authors": ["Amalie Brogaard Pauli", "Maria Barrett", "Max M\u00fcller-Eberstein", "Isabelle Augenstein", "Ira Assent"], "title": "Analysing Differences in Persuasive Language in LLM-Generated Text: Uncovering Stereotypical Gender Patterns", "comment": null, "summary": "Large language models (LLMs) are increasingly used for everyday communication tasks, including drafting interpersonal messages intended to influence and persuade. Prior work has shown that LLMs can successfully persuade humans and amplify persuasive language. It is therefore essential to understand how user instructions affect the generation of persuasive language, and to understand whether the generated persuasive language differs, for example, when targeting different groups. In this work, we propose a framework for evaluating how persuasive language generation is affected by recipient gender, sender intent, or output language. We evaluate 13 LLMs and 16 languages using pairwise prompt instructions. We evaluate model responses on 19 categories of persuasive language using an LLM-as-judge setup grounded in social psychology and communication science. Our results reveal significant gender differences in the persuasive language generated across all models. These patterns reflect biases consistent with gender-stereotypical linguistic tendencies documented in social psychology and sociolinguistics.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e8613\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u572816\u79cd\u8bed\u8a00\u4e2d\u751f\u6210\u8bf4\u670d\u6027\u8bed\u8a00\u65f6\uff0c\u53d7\u53d7\u4f17\u6027\u522b\u3001\u53d1\u9001\u8005\u610f\u56fe\u548c\u8bed\u8a00\u5f71\u54cd\u7684\u5dee\u5f02\uff0c\u53d1\u73b0\u5b58\u5728\u666e\u904d\u7684\u6027\u522b\u504f\u89c1\uff0c\u53cd\u6620\u4e86\u793e\u4f1a\u8bed\u8a00\u5b66\u4e2d\u7684\u6027\u522b\u523b\u677f\u503e\u5411\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u88ab\u5e7f\u6cdb\u7528\u4e8e\u65e5\u5e38\u4ea4\u6d41\u4efb\u52a1\u4e2d\uff0c\u5c24\u5176\u662f\u8d77\u8349\u65e8\u5728\u5f71\u54cd\u548c\u8bf4\u670d\u4ed6\u4eba\u7684\u4fe1\u606f\uff0c\u4e86\u89e3\u7528\u6237\u6307\u4ee4\u5982\u4f55\u5f71\u54cd\u8bf4\u670d\u8bed\u8a00\u751f\u6210\u4ee5\u53ca\u8bf4\u670d\u8bed\u8a00\u5728\u4e0d\u540c\u76ee\u6807\u7fa4\u4f53\u4e2d\u7684\u5dee\u5f02\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u7814\u7a76\u63a5\u6536\u8005\u6027\u522b\u3001\u53d1\u9001\u8005\u610f\u56fe\u548c\u8f93\u51fa\u8bed\u8a00\u5982\u4f55\u5f71\u54cd\u8bf4\u670d\u6027\u8bed\u8a00\u751f\u6210\u3002\u4f7f\u752813\u4e2aLLM\u548c16\u79cd\u8bed\u8a00\uff0c\u901a\u8fc7\u6210\u5bf9\u63d0\u793a\u6307\u4ee4\u8fdb\u884c\u6d4b\u8bd5\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u793e\u4f1a\u5fc3\u7406\u5b66\u548c\u4f20\u64ad\u79d1\u5b66\u7684LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\uff0c\u4ece19\u4e2a\u8bf4\u670d\u8bed\u8a00\u7c7b\u522b\u8bc4\u4f30\u6a21\u578b\u54cd\u5e94\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u6240\u6709\u6a21\u578b\u751f\u6210\u7684\u8bf4\u670d\u8bed\u8a00\u4e2d\u5b58\u5728\u663e\u8457\u7684\u6027\u522b\u5dee\u5f02\uff0c\u8fd9\u4e9b\u5dee\u5f02\u7b26\u5408\u5df2\u6709\u7684\u6027\u522b\u8bed\u8a00\u523b\u677f\u5370\u8c61\uff0c\u4e3a\u8fdb\u4e00\u6b65\u7406\u89e3\u548c\u6539\u8fdb\u6a21\u578b\u751f\u6210\u7684\u516c\u5e73\u6027\u548c\u591a\u6837\u6027\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002", "conclusion": "\u751f\u6210\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u751f\u6210\u6709\u8bf4\u670d\u529b\u7684\u8bed\u8a00\u65f6\u5b58\u5728\u663e\u8457\u7684\u6027\u522b\u5dee\u5f02\uff0c\u8fd9\u4e9b\u5dee\u5f02\u53cd\u6620\u4e86\u793e\u4f1a\u5fc3\u7406\u5b66\u548c\u793e\u4f1a\u8bed\u8a00\u5b66\u4e2d\u8bb0\u5f55\u7684\u6027\u522b\u523b\u677f\u8bed\u8a00\u503e\u5411\u3002"}}
{"id": "2601.05776", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05776", "abs": "https://arxiv.org/abs/2601.05776", "authors": ["Benedikt Ebing", "Lennart Keller", "Goran Glava\u0161"], "title": "One Script Instead of Hundreds? On Pretraining Romanized Encoder Language Models", "comment": null, "summary": "Exposing latent lexical overlap, script romanization has emerged as an effective strategy for improving cross-lingual transfer (XLT) in multilingual language models (mLMs). Most prior work, however, focused on setups that favor romanization the most: (1) transfer from high-resource Latin-script to low-resource non-Latin-script languages and/or (2) between genealogically closely related languages with different scripts. It thus remains unclear whether romanization is a good representation choice for pretraining general-purpose mLMs, or, more precisely, if information loss associated with romanization harms performance for high-resource languages. We address this gap by pretraining encoder LMs from scratch on both romanized and original texts for six typologically diverse high-resource languages, investigating two potential sources of degradation: (i) loss of script-specific information and (ii) negative cross-lingual interference from increased vocabulary overlap. Using two romanizers with different fidelity profiles, we observe negligible performance loss for languages with segmental scripts, whereas languages with morphosyllabic scripts (Chinese and Japanese) suffer degradation that higher-fidelity romanization mitigates but cannot fully recover. Importantly, comparing monolingual LMs with their mLM counterpart, we find no evidence that increased subword overlap induces negative interference. We further show that romanization improves encoding efficiency (i.e., fertility) for segmental scripts at a negligible performance cost.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u7f57\u9a6c\u5316\u5bf9\u516d\u79cd\u9ad8\u8d44\u6e90\u8bed\u8a00\u9884\u8bad\u7ec3\u591a\u8bed\u8a00\u6a21\u578b\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5bf9\u97f3\u6bb5\u6587\u5b57\u8bed\u8a00\u65e0\u663e\u8457\u6027\u80fd\u635f\u5931\u4e14\u63d0\u5347\u7f16\u7801\u6548\u7387\uff0c\u4f46\u5bf9\u4e2d\u6587\u65e5\u6587\u7b49\u5f62\u97f3\u8282\u6587\u5b57\u5b58\u5728\u6027\u80fd\u4e0b\u964d\uff0c\u4e14\u9ad8\u4fdd\u771f\u7f57\u9a6c\u5316\u6709\u90e8\u5206\u7f13\u89e3\u4f5c\u7528\uff0c\u5b50\u8bcd\u91cd\u53e0\u672a\u5f15\u8d77\u591a\u8bed\u8d1f\u5e72\u6270\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4ece\u9ad8\u8d44\u6e90\u62c9\u4e01\u5b57\u6bcd\u8bed\u8a00\u5411\u4f4e\u8d44\u6e90\u975e\u62c9\u4e01\u5b57\u6bcd\u8bed\u8a00\u6216\u76f8\u5173\u8bed\u8a00\u95f4\u7684\u8fc1\u79fb\uff0c\u672a\u660e\u786e\u7f57\u9a6c\u5316\u662f\u5426\u9002\u5408\u9884\u8bad\u7ec3\u901a\u7528\u591a\u8bed\u8a00\u6a21\u578b\uff0c\u5c24\u5176\u662f\u9ad8\u8d44\u6e90\u8bed\u8a00\u7684\u4fe1\u606f\u635f\u5931\u95ee\u9898\u3002", "method": "\u4ece\u96f6\u5f00\u59cb\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u8bed\u8a00\u6a21\u578b\uff0c\u5206\u522b\u5229\u7528\u7f57\u9a6c\u5316\u6587\u672c\u548c\u539f\u59cb\u6587\u672c\uff0c\u6d89\u53ca\u516d\u79cd\u8bed\u8a00\u7c7b\u578b\u5dee\u5f02\u8f83\u5927\u7684\u9ad8\u8d44\u6e90\u8bed\u8a00\uff0c\u4f7f\u7528\u4e24\u79cd\u4e0d\u540c\u4fdd\u771f\u5ea6\u7684\u7f57\u9a6c\u5316\u65b9\u6cd5\u5bf9\u6bd4\u6548\u679c\u3002", "result": "\u5bf9\u97f3\u6bb5\u6587\u5b57\u8bed\u8a00\u5f71\u54cd\u4e0d\u5927\uff0c\u5f62\u97f3\u8282\u6587\u5b57\uff08\u4e2d\u6587\u3001\u65e5\u6587\uff09\u5b58\u5728\u6027\u80fd\u4e0b\u964d\uff0c\u9ad8\u4fdd\u771f\u7f57\u9a6c\u5316\u80fd\u90e8\u5206\u7f13\u89e3\u3002\u5355\u8bed\u4e0e\u591a\u8bed\u6a21\u578b\u6bd4\u8f83\u672a\u53d1\u73b0\u589e\u52a0\u5b50\u8bcd\u91cd\u53e0\u5bfc\u81f4\u8d1f\u9762\u5e72\u6270\u3002\u7f57\u9a6c\u5316\u63d0\u9ad8\u4e86\u7f16\u7801\u6548\u7387\uff08\u751f\u80b2\u7387\uff09\uff0c\u6027\u80fd\u635f\u5931\u53ef\u5ffd\u7565\u3002", "conclusion": "\u7f57\u9a6c\u5316\u5728\u9ad8\u8d44\u6e90\u97f3\u6bb5\u6587\u5b57\u8bed\u8a00\u4e2d\u51e0\u4e4e\u65e0\u6027\u80fd\u635f\u5931\u4e14\u63d0\u5347\u7f16\u7801\u6548\u7387\uff0c\u4f46\u5bf9\u5f62\u97f3\u8282\u6587\u5b57\u6709\u4e00\u5b9a\u6027\u80fd\u635f\u5bb3\uff0c\u9ad8\u4fdd\u771f\u7f57\u9a6c\u5316\u6709\u6240\u5e2e\u52a9\u4f46\u4e0d\u80fd\u5b8c\u5168\u89e3\u51b3\u3002\u591a\u8bed\u8a00\u6a21\u578b\u4e2d\u5b50\u8bcd\u91cd\u53e0\u672a\u9020\u6210\u8d1f\u9762\u5f71\u54cd\u3002\u7f57\u9a6c\u5316\u4f5c\u4e3a\u9884\u8bad\u7ec3\u8868\u793a\u9009\u62e9\u5177\u6709\u6761\u4ef6\u9002\u7528\u6027\u3002"}}
{"id": "2601.05794", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.05794", "abs": "https://arxiv.org/abs/2601.05794", "authors": ["Eilam Cohen", "Itamar Bul", "Danielle Inbar", "Omri Loewenbach"], "title": "Simplify-This: A Comparative Analysis of Prompt-Based and Fine-Tuned LLMs", "comment": null, "summary": "Large language models (LLMs) enable strong text generation, and in general there is a practical tradeoff between fine-tuning and prompt engineering. We introduce Simplify-This, a comparative study evaluating both paradigms for text simplification with encoder-decoder LLMs across multiple benchmarks, using a range of evaluation metrics. Fine-tuned models consistently deliver stronger structural simplification, whereas prompting often attains higher semantic similarity scores yet tends to copy inputs. A human evaluation favors fine-tuned outputs overall. We release code, a cleaned derivative dataset used in our study, checkpoints of fine-tuned models, and prompt templates to facilitate reproducibility and future work.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u6587\u672c\u7b80\u5316\u4efb\u52a1\u4e2d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u4e0e\u63d0\u793a\u5de5\u7a0b\u7684\u6548\u679c\uff0c\u53d1\u73b0\u5fae\u8c03\u5728\u7ed3\u6784\u7b80\u5316\u65b9\u9762\u66f4\u5f3a\uff0c\u63d0\u793a\u5de5\u7a0b\u66f4\u6ce8\u91cd\u8bed\u4e49\u4fdd\u7559\u4e14\u6613\u590d\u5236\u539f\u6587\uff0c\u4eba\u5de5\u8bc4\u4ef7\u503e\u5411\u5fae\u8c03\u7ed3\u679c\u3002", "motivation": "\u63a2\u8ba8\u5fae\u8c03\u4e0e\u63d0\u793a\u5de5\u7a0b\u8fd9\u4e24\u79cd\u6587\u672c\u7b80\u5316\u65b9\u6cd5\u95f4\u7684\u6743\u8861\u548c\u4f18\u52a3\uff0c\u627e\u51fa\u66f4\u6709\u6548\u7684\u7b80\u5316\u7b56\u7565\u3002", "method": "\u5f15\u5165Simplify-This\uff0c\u6bd4\u8f83\u4f7f\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6587\u672c\u7b80\u5316\u65f6\u5fae\u8c03\u4e0e\u63d0\u793a\u5de5\u7a0b\u7684\u6548\u679c\uff0c\u57fa\u4e8e\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u591a\u79cd\u8bc4\u4ef7\u6307\u6807\u8fdb\u884c\u8bc4\u6d4b\u3002", "result": "\u5fae\u8c03\u6a21\u578b\u5728\u7ed3\u6784\u7b80\u5316\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u63d0\u793a\u5de5\u7a0b\u83b7\u5f97\u66f4\u9ad8\u7684\u8bed\u4e49\u76f8\u4f3c\u5ea6\u5206\u6570\u4f46\u66f4\u503e\u5411\u4e8e\u590d\u5236\u8f93\u5165\uff0c\u4e14\u4eba\u5de5\u8bc4\u4ef7\u603b\u4f53\u504f\u597d\u5fae\u8c03\u751f\u6210\u7684\u6587\u672c\u3002", "conclusion": "\u5fae\u8c03\u662f\u6587\u672c\u7b80\u5316\u4efb\u52a1\u4e2d\u66f4\u6709\u6548\u7684\u7b56\u7565\uff0c\u5efa\u8bae\u672a\u6765\u5de5\u4f5c\u56f4\u7ed5\u5fae\u8c03\u5c55\u5f00\uff0c\u540c\u65f6\u53d1\u5e03\u4ee3\u7801\u3001\u6570\u636e\u96c6\u548c\u6a21\u578b\u4ee5\u4fc3\u8fdb\u53ef\u590d\u73b0\u6027\u548c\u540e\u7eed\u7814\u7a76\u3002"}}
{"id": "2601.05808", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.05808", "abs": "https://arxiv.org/abs/2601.05808", "authors": ["Xiaoshuai Song", "Haofei Chang", "Guanting Dong", "Yutao Zhu", "Zhicheng Dou", "Ji-Rong Wen"], "title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis", "comment": "Working in progress", "summary": "Large language models (LLMs) are expected to be trained to act as agents in various real-world environments, but this process relies on rich and varied tool-interaction sandboxes. However, access to real systems is often restricted; LLM-simulated environments are prone to hallucinations and inconsistencies; and manually built sandboxes are hard to scale. In this paper, we propose EnvScaler, an automated framework for scalable tool-interaction environments via programmatic synthesis. EnvScaler comprises two components. First, SkelBuilder constructs diverse environment skeletons through topic mining, logic modeling, and quality evaluation. Then, ScenGenerator generates multiple task scenarios and rule-based trajectory validation functions for each environment. With EnvScaler, we synthesize 191 environments and about 7K scenarios, and apply them to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Qwen3 series models. Results on three benchmarks show that EnvScaler significantly improves LLMs' ability to solve tasks in complex environments involving multi-turn, multi-tool interactions. We release our code and data at https://github.com/RUC-NLPIR/EnvScaler.", "AI": {"tldr": "EnvScaler\u901a\u8fc7\u81ea\u52a8\u5316\u5408\u6210\u591a\u6837\u5316\u5de5\u5177\u4ea4\u4e92\u73af\u5883\uff0c\u6709\u6548\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u591a\u5de5\u5177\u4ea4\u4e92\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f5c\u4e3a\u591a\u73af\u5883\u4e2d\u7684\u667a\u80fd\u4f53\u9700\u8981\u4e30\u5bcc\u591a\u6837\u7684\u5de5\u5177\u4ea4\u4e92\u6c99\u7bb1\uff0c\u4f46\u771f\u5b9e\u7cfb\u7edf\u8bbf\u95ee\u53d7\u9650\uff0c\u6a21\u62df\u73af\u5883\u6613\u51fa\u9519\uff0c\u624b\u52a8\u6784\u5efa\u96be\u4ee5\u6269\u5c55\u3002", "method": "EnvScaler\u6846\u67b6\u5305\u542bSkelBuilder\u548cScenGenerator\u4e24\u4e2a\u7ec4\u4ef6\uff0c\u524d\u8005\u901a\u8fc7\u4e3b\u9898\u6316\u6398\u3001\u903b\u8f91\u5efa\u6a21\u548c\u8d28\u91cf\u8bc4\u4f30\u6784\u5efa\u591a\u6837\u5316\u73af\u5883\u9aa8\u67b6\uff0c\u540e\u8005\u4e3a\u6bcf\u4e2a\u73af\u5883\u751f\u6210\u591a\u4e2a\u4efb\u52a1\u573a\u666f\u548c\u89c4\u5219\u57fa\u8f68\u8ff9\u9a8c\u8bc1\u51fd\u6570\u3002", "result": "\u4f7f\u7528EnvScaler\u5408\u6210\u4e86191\u4e2a\u73af\u5883\u548c\u7ea67000\u4e2a\u573a\u666f\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8eQwen3\u7cfb\u5217\u6a21\u578b\u7684\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u4e09\u5927\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86LLMs\u5728\u590d\u6742\u591a\u8f6e\u591a\u5de5\u5177\u4ea4\u4e92\u73af\u5883\u4e2d\u7684\u4efb\u52a1\u89e3\u51b3\u80fd\u529b\u3002", "conclusion": "EnvScaler\u6846\u67b6\u5b9e\u73b0\u4e86\u73af\u5883\u6784\u5efa\u7684\u81ea\u52a8\u5316\u548c\u89c4\u6a21\u5316\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5904\u7406\u590d\u6742\u591a\u5de5\u5177\u4ea4\u4e92\u73af\u5883\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u4e3a\u8bad\u7ec3\u73af\u5883\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2601.05821", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05821", "abs": "https://arxiv.org/abs/2601.05821", "authors": ["Milad Alshomary", "Grace Li", "Anubhav Jangra", "Yufang Hou", "Kathleen McKeown", "Smaranda Muresan"], "title": "LLMs as Science Journalists: Supporting Early-stage Researchers in Communicating Their Science to the Public", "comment": null, "summary": "The scientific community needs tools that help early-stage researchers effectively communicate their findings and innovations to the public. Although existing general-purpose Large Language Models (LLMs) can assist in this endeavor, they are not optimally aligned for it. To address this, we propose a framework for training LLMs to emulate the role of a science journalist that can be used by early-stage researchers to learn how to properly communicate their papers to the general public. We evaluate the usefulness of our trained LLM Journalists in leading conversations with both simulated and human researchers. %compared to the general-purpose ones. Our experiments indicate that LLMs trained using our framework ask more relevant questions that address the societal impact of research, prompting researchers to clarify and elaborate on their findings. In the user study, the majority of participants who interacted with our trained LLM Journalist appreciated it more than interacting with general-purpose LLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u626e\u6f14\u79d1\u5b66\u8bb0\u8005\u7684\u6846\u67b6\uff0c\u5e2e\u52a9\u65e9\u671f\u7814\u7a76\u4eba\u5458\u66f4\u597d\u5730\u5411\u516c\u4f17\u4f20\u8fbe\u79d1\u7814\u6210\u679c\u3002", "motivation": "\u73b0\u6709\u901a\u7528LLMs\u867d\u80fd\u8f85\u52a9\u79d1\u7814\u4ea4\u6d41\uff0c\u4f46\u672a\u9488\u5bf9\u79d1\u5b66\u4f20\u64ad\u4f18\u5316\uff0c\u65e9\u671f\u7814\u7a76\u8005\u7f3a\u4e4f\u6709\u6548\u5de5\u5177\u5411\u516c\u4f17\u4f20\u8fbe\u79d1\u7814\u5185\u5bb9\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u8bad\u7ec3\u6846\u67b6\uff0c\u4f7fLLMs\u6a21\u4eff\u79d1\u5b66\u8bb0\u8005\u89d2\u8272\uff0c\u5e76\u8fdb\u884c\u95ee\u7b54\u4e92\u52a8\u4ee5\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u5b8c\u5584\u8bba\u6587\u6c9f\u901a\u3002", "result": "\u8bad\u7ec3\u540e\u7684LLMs\u5728\u6a21\u62df\u548c\u771f\u5b9e\u5bf9\u8bdd\u4e2d\u63d0\u51fa\u66f4\u76f8\u5173\u95ee\u9898\uff0c\u4fc3\u8fdb\u7814\u7a76\u8005\u66f4\u6e05\u6670\u9610\u8ff0\u6210\u679c\uff0c\u5927\u90e8\u5206\u7528\u6237\u66f4\u559c\u6b22\u4e0e\u8fd9\u7c7b\u6a21\u578b\u4e92\u52a8\u3002", "conclusion": "\u901a\u8fc7\u8bad\u7ec3\uff0cLLMs\u53ef\u4ee5\u66f4\u6709\u6548\u5730\u5f15\u5bfc\u7814\u7a76\u4eba\u5458\u8ba8\u8bba\u79d1\u7814\u7684\u793e\u4f1a\u5f71\u54cd\uff0c\u7528\u6237\u66f4\u503e\u5411\u4e8e\u4e0e\u8fd9\u79cd\u5b9a\u5236\u6a21\u578b\u4e92\u52a8\u3002"}}
{"id": "2601.05833", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05833", "abs": "https://arxiv.org/abs/2601.05833", "authors": ["Liu Zai"], "title": "Peek2: A Regex-free implementation of pretokenizers for Byte-level BPE", "comment": "5 pages, 4 figures, for associated code, see https://github.com/omegacoleman/tokenizers_peek2", "summary": "Pretokenization is a crucial, sequential pass in Byte-level BPE tokenizers. Our proposed new implementation, Peek2, serves as a drop-in replacement for cl100k-like pretokenizers used in GPT-3, LLaMa-3, and Qwen-2.5. Designed with performance and safety in mind, Peek2 is Regex-free and delivers a $ 1.11\\times $ improvement in overall throughput across the entire Byte-level BPE encoding process. This algorithm runs entirely on the CPU, has stable linear complexity $ O(n) $, and provides presegmentation results identical to those of the original Regex-based pretokenizer.", "AI": {"tldr": "Peek2\u662f\u4e00\u79cd\u65e0Regex\u3001\u5b8c\u5168CPU\u5b9e\u73b0\u7684\u9ad8\u6548Byte-level BPE\u9884\u5206\u8bcd\u5668\uff0c\u63d0\u5347\u4e861.11\u500d\u6027\u80fd\uff0c\u517c\u5bb9\u73b0\u6709\u4e3b\u6d41\u6a21\u578b\u3002", "motivation": "\u63d0\u9ad8Byte-level BPE\u9884\u5206\u8bcd\u7684\u6027\u80fd\u548c\u5b89\u5168\u6027\uff0c\u4f5c\u4e3aGPT-3\u3001LLaMa-3\u53caQwen-2.5\u4e2d\u5e38\u7528\u7684cl100k-like\u9884\u5206\u8bcd\u5668\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0Regex\u7684\u9884\u5206\u8bcd\u65b0\u5b9e\u73b0Peek2\uff0c\u5177\u6709\u7ebf\u6027\u590d\u6742\u5ea6\uff0c\u5b8c\u5168\u5728CPU\u4e0a\u8fd0\u884c\u3002", "result": "Peek2\u5728\u6574\u4f53\u541e\u5410\u91cf\u4e0a\u63d0\u5347\u4e86\u7ea61.11\u500d\uff0c\u4e14\u4fdd\u6301\u4e86\u9884\u5206\u8bcd\u7ed3\u679c\u7684\u4e00\u81f4\u6027\u548c\u7b97\u6cd5\u7a33\u5b9a\u6027\u3002", "conclusion": "Peek2\u5b9e\u73b0\u4e86\u4e0e\u539f\u59cbRegex\u57fa\u9884\u5206\u8bcd\u5668\u5b8c\u5168\u76f8\u540c\u7684\u9884\u5206\u8bcd\u7ed3\u679c\uff0c\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6574\u4e2aByte-level BPE\u7f16\u7801\u8fc7\u7a0b\u7684\u541e\u5410\u91cf\u3002"}}
{"id": "2601.05835", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05835", "abs": "https://arxiv.org/abs/2601.05835", "authors": ["Molly Kennedy", "Ali Parker", "Yihong Liu", "Hinrich Sch\u00fctze"], "title": "Left, Right, or Center? Evaluating LLM Framing in News Classification and Generation", "comment": null, "summary": "Large Language Model (LLM) based summarization and text generation are increasingly used for producing and rewriting text, raising concerns about political framing in journalism where subtle wording choices can shape interpretation. Across nine state-of-the-art LLMs, we study political framing by testing whether LLMs' classification-based bias signals align with framing behavior in their generated summaries. We first compare few-shot ideology predictions against LEFT/CENTER/RIGHT labels. We then generate \"steered\" summaries under FAITHFUL, CENTRIST, LEFT, and RIGHT prompts, and score all outputs using a single fixed ideology evaluator. We find pervasive ideological center-collapse in both article-level ratings and generated text, indicating a systematic tendency toward centrist framing. Among evaluated models, Grok 4 is by far the most ideologically expressive generator, while Claude Sonnet 4.5 and Llama 3.1 achieve the strongest bias-rating performance among commercial and open-weight models, respectively.", "AI": {"tldr": "\u7814\u7a76\u4e86\u4e5d\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u6587\u672c\u751f\u6210\u4e2d\u653f\u6cbb\u7acb\u573a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6a21\u578b\u666e\u904d\u8d8b\u5411\u4e8e\u4e2d\u7acb\u7acb\u573a\uff0c\u4e14\u4e0d\u540c\u6a21\u578b\u5728\u610f\u8bc6\u5f62\u6001\u8868\u8fbe\u4e0a\u6709\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u9274\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u65b0\u95fb\u6587\u672c\u751f\u6210\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u7814\u7a76\u5176\u662f\u5426\u5b58\u5728\u653f\u6cbb\u6846\u67b6\u504f\u5411\uff0c\u53ca\u5176\u6f5c\u5728\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5c11\u6837\u672c\u5b66\u4e60\u9884\u6d4b\u653f\u6cbb\u5149\u8c31\u6807\u7b7e\uff08\u5de6\u6d3e/\u4e2d\u7acb/\u53f3\u6d3e\uff09\uff0c\u5e76\u751f\u6210\u5728\u4e0d\u540c\u610f\u8bc6\u5f62\u6001\u6307\u5bfc\uff08\u5fe0\u5b9e\u3001\u4e2d\u7acb\u3001\u5de6\u6d3e\u3001\u53f3\u6d3e\uff09\u4e0b\u7684\u6587\u672c\u6458\u8981\uff0c\u4f7f\u7528\u56fa\u5b9a\u7684\u610f\u8bc6\u5f62\u6001\u8bc4\u4f30\u5668\u5bf9\u6240\u6709\u8f93\u51fa\u8fdb\u884c\u8bc4\u5206\u3002", "result": "\u53d1\u73b0\u6240\u6709\u6a21\u578b\u5728\u6587\u7ae0\u7ea7\u522b\u548c\u751f\u6210\u6587\u672c\u4e2d\u5747\u663e\u793a\u51fa\u610f\u8bc6\u5f62\u6001\u7684\u4e2d\u7acb\u96c6\u4e2d\u7279\u6027\uff1bGrok 4\u8868\u73b0\u51fa\u6700\u5f3a\u7684\u610f\u8bc6\u5f62\u6001\u751f\u6210\u80fd\u529b\uff0cClaude Sonnet 4.5\u548cLlama 3.1\u5728\u504f\u89c1\u8bc4\u7ea7\u4e0a\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u6587\u672c\u65f6\u666e\u904d\u8868\u73b0\u51fa\u5411\u4e2d\u7acb\u7acb\u573a\u503e\u659c\u7684\u8d8b\u52bf\uff0c\u4e2a\u522b\u6a21\u578b\u5982Grok 4\u80fd\u591f\u8f83\u597d\u5730\u8868\u8fbe\u610f\u8bc6\u5f62\u6001\u5dee\u5f02\u3002"}}
{"id": "2601.05847", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05847", "abs": "https://arxiv.org/abs/2601.05847", "authors": ["Rafael Brens", "Yuqiao Meng", "Luoxi Tang", "Zhaohan Xi"], "title": "Semantic NLP Pipelines for Interoperable Patient Digital Twins from Unstructured EHRs", "comment": null, "summary": "Digital twins -- virtual replicas of physical entities -- are gaining traction in healthcare for personalized monitoring, predictive modeling, and clinical decision support. However, generating interoperable patient digital twins from unstructured electronic health records (EHRs) remains challenging due to variability in clinical documentation and lack of standardized mappings. This paper presents a semantic NLP-driven pipeline that transforms free-text EHR notes into FHIR-compliant digital twin representations. The pipeline leverages named entity recognition (NER) to extract clinical concepts, concept normalization to map entities to SNOMED-CT or ICD-10, and relation extraction to capture structured associations between conditions, medications, and observations. Evaluation on MIMIC-IV Clinical Database Demo with validation against MIMIC-IV-on-FHIR reference mappings demonstrates high F1-scores for entity and relation extraction, with improved schema completeness and interoperability compared to baseline methods.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49NLP\u7684\u7ba1\u9053\uff0c\u5c06\u975e\u7ed3\u6784\u5316\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u6587\u672c\u8f6c\u5316\u4e3a\u7b26\u5408FHIR\u6807\u51c6\u7684\u6570\u5b57\u5b6a\u751f\u8868\u793a\uff0c\u4ee5\u63d0\u9ad8\u4e92\u64cd\u4f5c\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u7684\u975e\u7ed3\u6784\u5316\u6587\u672c\u96be\u4ee5\u6807\u51c6\u5316\u5904\u7406\uff0c\u9650\u5236\u4e86\u60a3\u8005\u6570\u5b57\u5b6a\u751f\u7684\u751f\u6210\u548c\u5e94\u7528\uff0c\u4e9f\u9700\u8bed\u4e49\u9a71\u52a8\u7684\u81ea\u52a8\u8f6c\u6362\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u547d\u540d\u5b9e\u4f53\u8bc6\u522b(NER)\u3001\u6982\u5ff5\u89c4\u8303\u5316\uff08\u6620\u5c04\u81f3SNOMED-CT\u6216ICD-10\uff09\u53ca\u5173\u7cfb\u62bd\u53d6\uff0c\u6784\u5efaFHIR\u517c\u5bb9\u7684\u60a3\u8005\u6570\u5b57\u5b6a\u751f\u3002", "result": "\u5728MIMIC-IV-on-FHIR\u53c2\u8003\u6620\u5c04\u4e0b\uff0c\u65b9\u6cd5\u663e\u793a\u51fa\u4f18\u8d8a\u7684\u5b9e\u4f53\u53ca\u5173\u7cfb\u62bd\u53d6\u6027\u80fd\uff0c\u6a21\u5f0f\u5b8c\u6574\u6027\u548c\u4e92\u64cd\u4f5c\u6027\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728MIMIC-IV\u4e34\u5e8a\u6570\u636e\u5e93\u4e0a\u9a8c\u8bc1\u8868\u73b0\u51fa\u9ad8F1\u5206\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b9e\u4f53\u4e0e\u5173\u7cfb\u62bd\u53d6\u7684\u51c6\u786e\u6027\u548c\u4e92\u64cd\u4f5c\u6027\u3002"}}
{"id": "2601.05851", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.05851", "abs": "https://arxiv.org/abs/2601.05851", "authors": ["Sandeep Mishra", "Devichand Budagam", "Anubhab Mandal", "Bishal Santra", "Pawan Goyal", "Manish Gupta"], "title": "Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs", "comment": "Accepted to EACL 2026 Industry Track, 12 pages, 6 figures", "summary": "Real-time multimodal auto-completion is essential for digital assistants, chatbots, design tools, and healthcare consultations, where user inputs rely on shared visual context. We introduce Multimodal Auto-Completion (MAC), a task that predicts upcoming characters in live chats using partially typed text and visual cues. Unlike traditional text-only auto-completion (TAC), MAC grounds predictions in multimodal context to better capture user intent. To enable this task, we adapt MMDialog and ImageChat to create benchmark datasets. We evaluate leading vision-language models (VLMs) against strong textual baselines, highlighting trade-offs in accuracy and efficiency. We present Router-Suggest, a router framework that dynamically selects between textual models and VLMs based on dialog context, along with a lightweight variant for resource-constrained environments. Router-Suggest achieves a 2.3x to 10x speedup over the best-performing VLM. A user study shows that VLMs significantly excel over textual models on user satisfaction, notably saving user typing effort and improving the quality of completions in multi-turn conversations. These findings underscore the need for multimodal context in auto-completions, leading to smarter, user-aware assistants.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u5b9e\u65f6\u591a\u6a21\u6001\u81ea\u52a8\u8865\u5168\uff08MAC\uff09\u4efb\u52a1\uff0c\u7ed3\u5408\u90e8\u5206\u8f93\u5165\u6587\u672c\u548c\u89c6\u89c9\u7ebf\u7d22\u9884\u6d4b\u5bf9\u8bdd\u4e2d\u7684\u4e0b\u4e00\u4e2a\u5b57\u7b26\uff0c\u63d0\u5347\u4e86\u7528\u6237\u8f93\u5165\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u8865\u5168\u591a\u4e3a\u6587\u672c\u65b9\u6cd5\uff0c\u5ffd\u7565\u89c6\u89c9\u4fe1\u606f\uff0c\u96be\u4ee5\u51c6\u786e\u6355\u6349\u7528\u6237\u610f\u56fe\uff0c\u4e9f\u9700\u7ed3\u5408\u591a\u6a21\u6001\u4fe1\u606f\u6539\u8fdb\u81ea\u52a8\u8865\u5168\u6548\u679c\u3002", "method": "\u901a\u8fc7\u6539\u7f16MMDialog\u548cImageChat\u6784\u5efa\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u8bc4\u6d4b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u6587\u672c\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1Router-Suggest\u52a8\u6001\u9009\u62e9\u6a21\u578b\u4ee5\u4f18\u5316\u6027\u80fd\u4e0e\u6548\u7387\u3002", "result": "\u89c6\u56fe\u8bed\u8a00\u6a21\u578b\u5728\u7528\u6237\u6ee1\u610f\u5ea6\u548c\u8f93\u5165\u8282\u7701\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u6587\u672c\u6a21\u578b\uff0cRouter-Suggest\u5728\u4fdd\u6301\u51c6\u786e\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u901f\u5ea6\u63d0\u5347\u3002", "conclusion": "\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5728\u81ea\u52a8\u8865\u5168\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u7528\u6237\u6ee1\u610f\u5ea6\u548c\u8f93\u5165\u6548\u7387\uff0cRouter-Suggest\u5b9e\u73b0\u4e86\u6027\u80fd\u4e0e\u901f\u5ea6\u7684\u826f\u597d\u5e73\u8861\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u73af\u5883\u3002"}}
{"id": "2601.05858", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.05858", "abs": "https://arxiv.org/abs/2601.05858", "authors": ["Alexandra Dragomir", "Florin Brad", "Radu Tudor Ionescu"], "title": "CLewR: Curriculum Learning with Restarts for Machine Translation Preference Learning", "comment": null, "summary": "Large language models (LLMs) have demonstrated competitive performance in zero-shot multilingual machine translation (MT). Some follow-up works further improved MT performance via preference optimization, but they leave a key aspect largely underexplored: the order in which data samples are given during training. We address this topic by integrating curriculum learning into various state-of-the-art preference optimization algorithms to boost MT performance. We introduce a novel curriculum learning strategy with restarts (CLewR), which reiterates easy-to-hard curriculum multiple times during training to effectively mitigate the catastrophic forgetting of easy examples. We demonstrate consistent gains across several model families (Gemma2, Qwen2.5, Llama3.1) and preference optimization techniques. We publicly release our code at https://github.com/alexandra-dragomir/CLewR.", "AI": {"tldr": "\u901a\u8fc7\u7ed3\u5408\u5e26\u91cd\u542f\u7684\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565CLewR\u63d0\u5347\u504f\u597d\u4f18\u5316\u7684\u591a\u8bed\u8a00\u673a\u5668\u7ffb\u8bd1\u6027\u80fd\uff0c\u89e3\u51b3\u8bad\u7ec3\u4e2d\u6837\u672c\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u5728\u591a\u8bed\u8a00\u673a\u5668\u7ffb\u8bd1\u7684\u504f\u597d\u4f18\u5316\u8fc7\u7a0b\u4e2d\uff0c\u8bad\u7ec3\u6570\u636e\u6837\u672c\u7684\u987a\u5e8f\u5bf9\u6027\u80fd\u5f71\u54cd\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u5c06\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u6574\u5408\u5230\u591a\u79cd\u5148\u8fdb\u7684\u504f\u597d\u4f18\u5316\u7b97\u6cd5\u4e2d\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5e26\u91cd\u542f\u7684\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6cd5CLewR\uff0c\u901a\u8fc7\u591a\u6b21\u91cd\u590d\u7531\u6613\u5230\u96be\u7684\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u51cf\u7f13\u5bb9\u6613\u6837\u672c\u7684\u9057\u5fd8\u3002", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u5bb6\u65cf\uff08Gemma2\u3001Qwen2.5\u3001Llama3.1\uff09\u548c\u504f\u597d\u4f18\u5316\u6280\u672f\u4e0a\u5747\u53d6\u5f97\u4e86\u4e00\u81f4\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u5f15\u5165\u5e26\u91cd\u542f\u7684\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u80fd\u6709\u6548\u63d0\u5347\u96f6-shot\u591a\u8bed\u8a00\u673a\u5668\u7ffb\u8bd1\u7684\u6027\u80fd\uff0c\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002"}}
{"id": "2601.05864", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05864", "abs": "https://arxiv.org/abs/2601.05864", "authors": ["Jonathan Downie", "Joss Moorkens"], "title": "What do the metrics mean? A critical analysis of the use of Automated Evaluation Metrics in Interpreting", "comment": "25 pages", "summary": "With the growth of interpreting technologies, from remote interpreting and Computer-Aided Interpreting to automated speech translation and interpreting avatars, there is now a high demand for ways to quickly and efficiently measure the quality of any interpreting delivered. A range of approaches to fulfil the need for quick and efficient quality measurement have been proposed, each involving some measure of automation. This article examines these recently-proposed quality measurement methods and will discuss their suitability for measuring the quality of authentic interpreting practice, whether delivered by humans or machines, concluding that automatic metrics as currently proposed cannot take into account the communicative context and thus are not viable measures of the quality of any interpreting provision when used on their own. Across all attempts to measure or even categorise quality in Interpreting Studies, the contexts in which interpreting takes place have become fundamental to the final analysis.", "AI": {"tldr": "\u73b0\u6709\u81ea\u52a8\u5316\u53e3\u8bd1\u8d28\u91cf\u6d4b\u91cf\u65b9\u6cd5\u5ffd\u89c6\u4ea4\u6d41\u73af\u5883\uff0c\u4e0d\u80fd\u5355\u72ec\u6709\u6548\u8bc4\u4f30\u53e3\u8bd1\u8d28\u91cf\u3002", "motivation": "\u968f\u7740\u53e3\u8bd1\u6280\u672f\u7684\u53d1\u5c55\uff0c\u5feb\u901f\u9ad8\u6548\u8bc4\u4f30\u53e3\u8bd1\u8d28\u91cf\u7684\u9700\u6c42\u663e\u8457\u589e\u52a0\u3002", "method": "\u672c\u6587\u8bc4\u4f30\u4e86\u591a\u79cd\u8fd1\u5e74\u6765\u63d0\u51fa\u7684\u81ea\u52a8\u5316\u53e3\u8bd1\u8d28\u91cf\u6d4b\u91cf\u65b9\u6cd5\u3002", "result": "\u5404\u7c7b\u81ea\u52a8\u5316\u6d4b\u91cf\u65b9\u6cd5\u867d\u7136\u4fbf\u6377\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u771f\u5b9e\u4ea4\u6d41\u73af\u5883\u7684\u8003\u8651\uff0c\u96be\u4ee5\u4f5c\u4e3a\u72ec\u7acb\u7684\u8d28\u91cf\u8bc4\u4f30\u5de5\u5177\u3002", "conclusion": "\u5f53\u524d\u81ea\u52a8\u5316\u8d28\u91cf\u6d4b\u91cf\u65b9\u6cd5\u65e0\u6cd5\u5168\u9762\u8003\u8651\u53e3\u8bd1\u7684\u4ea4\u6d41\u8bed\u5883\uff0c\u56e0\u6b64\u5355\u72ec\u4f7f\u7528\u65f6\u65e0\u6cd5\u6709\u6548\u8861\u91cf\u53e3\u8bd1\u8d28\u91cf\u3002"}}
{"id": "2601.05866", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05866", "abs": "https://arxiv.org/abs/2601.05866", "authors": ["Maxime Dassen", "Rebecca Kotula", "Kenton Murray", "Andrew Yates", "Dawn Lawrie", "Efsun Kayi", "James Mayfield", "Kevin Duh"], "title": "FACTUM: Mechanistic Detection of Citation Hallucination in Long-Form RAG", "comment": "Accepted at ECIR 2026. 18 pages, 2 figures", "summary": "Retrieval-Augmented Generation (RAG) models are critically undermined by citation hallucinations, a deceptive failure where a model confidently cites a source that fails to support its claim. Existing work often attributes hallucination to a simple over-reliance on the model's parametric knowledge. We challenge this view and introduce FACTUM (Framework for Attesting Citation Trustworthiness via Underlying Mechanisms), a framework of four mechanistic scores measuring the distinct contributions of a model's attention and FFN pathways, and the alignment between them. Our analysis reveals two consistent signatures of correct citation: a significantly stronger contribution from the model's parametric knowledge and greater use of the attention sink for information synthesis. Crucially, we find the signature of a correct citation is not static but evolves with model scale. For example, the signature of a correct citation for the Llama-3.2-3B model is marked by higher pathway alignment, whereas for the Llama-3.1-8B model, it is characterized by lower alignment, where pathways contribute more distinct, orthogonal information. By capturing this complex, evolving signature, FACTUM outperforms state-of-the-art baselines by up to 37.5% in AUC. Our findings reframe citation hallucination as a complex, scale-dependent interplay between internal mechanisms, paving the way for more nuanced and reliable RAG systems.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5f15\u5165FACTUM\u6846\u67b6\uff0c\u63ed\u793a\u4e86RAG\u6a21\u578b\u4e2d\u5f15\u6587\u5e7b\u89c9\u7684\u590d\u6742\u5185\u5728\u673a\u5236\uff0c\u6210\u529f\u63d0\u5347\u4e86\u5f15\u7528\u53ef\u4fe1\u5ea6\u7684\u68c0\u6d4b\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6a21\u578b\uff08RAG\uff09\u4e2d\u5f15\u6587\u5e7b\u89c9\u95ee\u9898\uff0c\u5373\u6a21\u578b\u81ea\u4fe1\u5730\u5f15\u7528\u4f46\u6765\u6e90\u4e0d\u652f\u6301\u5176\u8bba\u70b9\u7684\u60c5\u51b5\u3002", "method": "\u63d0\u51faFACTUM\u6846\u67b6\uff0c\u901a\u8fc7\u56db\u4e2a\u673a\u68b0\u8bc4\u5206\u8861\u91cf\u6a21\u578b\u7684\u6ce8\u610f\u529b\u673a\u5236\u548c\u524d\u9988\u7f51\u7edc\uff08FFN\uff09\u8def\u5f84\u7684\u8d21\u732e\u53ca\u5176\u5bf9\u9f50\u60c5\u51b5\uff0c\u5206\u6790\u6a21\u578b\u5185\u90e8\u673a\u5236\u4e0e\u6b63\u786e\u5f15\u7528\u7684\u5173\u7cfb\u3002", "result": "\u53d1\u73b0\u6b63\u786e\u5f15\u7528\u7684\u4e24\u4e2a\u6807\u5fd7\uff1a\u6a21\u578b\u53c2\u6570\u77e5\u8bc6\u8d21\u732e\u66f4\u5927\u548c\u6ce8\u610f\u529b\u6c47\u805a\u7528\u4e8e\u4fe1\u606f\u7efc\u5408\uff0c\u4e14\u8fd9\u4e00\u7279\u5f81\u968f\u6a21\u578b\u89c4\u6a21\u53d8\u5316\u3002\u6b64\u5916\uff0cFACTUM\u5728AUC\u6307\u6807\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u5347\u4e8637.5%\u3002", "conclusion": "\u5f15\u6587\u5e7b\u89c9\u4e0d\u662f\u7b80\u5355\u7684\u53c2\u6570\u4f9d\u8d56\uff0c\u800c\u662f\u5173\u8054\u6a21\u578b\u5185\u90e8\u673a\u5236\u590d\u6742\u4e14\u968f\u89c4\u6a21\u53d8\u5316\u7684\u76f8\u4e92\u4f5c\u7528\uff0cFACTUM\u4e3a\u6784\u5efa\u66f4\u53ef\u9760\u7684RAG\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2601.05874", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05874", "abs": "https://arxiv.org/abs/2601.05874", "authors": ["Santosh Srinath K", "Mudit Somani", "Varun Reddy Padala", "Prajna Devi Upadhyay", "Abhijit Das"], "title": "Continual-learning for Modelling Low-Resource Languages from Large Language Models", "comment": null, "summary": "Modelling a language model for a multi-lingual scenario includes several potential challenges, among which catastrophic forgetting is the major challenge. For example, small language models (SLM) built for low-resource languages by adapting large language models (LLMs) pose the challenge of catastrophic forgetting. This work proposes to employ a continual learning strategy using parts-of-speech (POS)-based code-switching along with a replay adapter strategy to mitigate the identified gap of catastrophic forgetting while training SLM from LLM. Experiments conducted on vision language tasks such as visual question answering and language modelling task exhibits the success of the proposed architecture.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bcd\u6027\u4ee3\u7801\u5207\u6362\u548c\u91cd\u653e\u9002\u914d\u5668\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u6210\u529f\u7f13\u89e3\u4e86\u591a\u8bed\u8a00\u573a\u666f\u4e0b\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u707e\u96be\u6027\u9057\u5fd8\u7684\u95ee\u9898\u3002", "motivation": "\u5728\u591a\u8bed\u8a00\u573a\u666f\u4e2d\u6784\u5efa\u8bed\u8a00\u6a21\u578b\u65f6\uff0c\u707e\u96be\u6027\u9057\u5fd8\u662f\u4e3b\u8981\u6311\u6218\uff0c\u5c24\u5176\u662f\u901a\u8fc7\u9002\u914d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u65f6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8bcd\u6027\uff08POS\uff09\u57fa\u7840\u7684\u4ee3\u7801\u5207\u6362\u548c\u91cd\u653e\u9002\u914d\u5668\u7b56\u7565\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u51cf\u8f7b\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u5728\u4ece\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9002\u914d\u65f6\u4ea7\u751f\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "result": "\u5728\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\uff08\u5982\u89c6\u89c9\u95ee\u7b54\uff09\u548c\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7ed3\u679c\u5c55\u793a\u4e86\u6240\u63d0\u4f53\u7cfb\u7ed3\u6784\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u4e86\u707e\u96be\u6027\u9057\u5fd8\uff0c\u63d0\u9ad8\u4e86\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8bed\u8a00\u548c\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2601.05877", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05877", "abs": "https://arxiv.org/abs/2601.05877", "authors": ["Meghana Sunil", "Manikandarajan Venmathimaran", "Muthu Subash Kavitha"], "title": "iReasoner: Trajectory-Aware Intrinsic Reasoning Supervision for Self-Evolving Large Multimodal Models", "comment": null, "summary": "Recent work shows that large multimodal models (LMMs) can self-improve from unlabeled data via self-play and intrinsic feedback. Yet existing self-evolving frameworks mainly reward final outcomes, leaving intermediate reasoning weakly constrained despite its importance for visually grounded decision making. We propose iReasoner, a self-evolving framework that improves an LMM's implicit reasoning by explicitly eliciting chain-of-thought (CoT) and rewarding its internal agreement. In a Proposer--Solver loop over unlabeled images, iReasoner augments outcome-level intrinsic rewards with a trajectory-aware signal defined over intermediate reasoning steps, providing learning signals that distinguish reasoning paths leading to the same answer without ground-truth labels or external judges. Starting from Qwen2.5-VL-7B, iReasoner yields up to $+2.1$ points across diverse multimodal reasoning benchmarks under fully unsupervised post-training. We hope this work serves as a starting point for reasoning-aware self-improvement in LMMs in purely unsupervised settings.", "AI": {"tldr": "iReasoner\u901a\u8fc7\u5956\u52b1\u94fe\u5f0f\u63a8\u7406\u5185\u90e8\u4e00\u81f4\u6027\uff0c\u81ea\u76d1\u7763\u63d0\u5347\u591a\u6a21\u6001\u6a21\u578b\u4e2d\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u5728\u65e0\u76d1\u7763\u573a\u666f\u4e0b\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u81ea\u6211\u8fdb\u5316\u591a\u4f9d\u8d56\u6700\u7ec8\u7ed3\u679c\u5956\u52b1\uff0c\u5ffd\u89c6\u4e86\u4e2d\u95f4\u63a8\u7406\u8fc7\u7a0b\u7684\u7ea6\u675f\uff0c\u800c\u4e2d\u95f4\u63a8\u7406\u5bf9\u89c6\u89c9\u51b3\u7b56\u6781\u4e3a\u91cd\u8981\u3002", "method": "\u63d0\u51faiReasoner\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5f15\u5bfc\u94fe\u5f0f\u601d\u8003(CoT)\u5e76\u5956\u52b1\u5176\u5185\u90e8\u4e00\u81f4\u6027\uff0c\u7ed3\u5408Proposer-Solver\u5faa\u73af\u673a\u5236\uff0c\u5728\u65e0\u4eba\u6807\u6ce8\u6570\u636e\u4e0a\u5bf9\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u8fdb\u884c\u8f68\u8ff9\u611f\u77e5\u5956\u52b1\u3002", "result": "iReasoner\u5728\u57fa\u4e8eQwen2.5-VL-7B\u6a21\u578b\u4e0a\u8fdb\u884c\u65e0\u76d1\u7763\u540e\u8bad\u7ec3\uff0c\u5728\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u4e2d\u63d0\u5347\u4e86\u6700\u591a2.1\u5206\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5bf9\u9690\u5f0f\u63a8\u7406\u6539\u8fdb\u7684\u6709\u6548\u6027\u3002", "conclusion": "iReasoner\u4e3a\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5b9e\u73b0\u7eaf\u65e0\u76d1\u7763\u73af\u5883\u4e0b\u7684\u63a8\u7406\u611f\u77e5\u81ea\u6211\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u6709\u52a9\u4e8e\u52a0\u5f3a\u6a21\u578b\u4e2d\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u63a8\u52a8\u89c6\u89c9\u63a8\u7406\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2601.05879", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.05879", "abs": "https://arxiv.org/abs/2601.05879", "authors": ["Jakub Harasta", "Matej Vasina", "Martin Kornel", "Tomas Foltynek"], "title": "Gender Bias in LLMs: Preliminary Evidence from Shared Parenting Scenario in Czech Family Law", "comment": "Accepted at AI for Access to Justice, Dispute Resolution, and Data Access (AIDA2J) at Jurix 2025, Torino, Italy", "summary": "Access to justice remains limited for many people, leading laypersons to increasingly rely on Large Language Models (LLMs) for legal self-help. Laypeople use these tools intuitively, which may lead them to form expectations based on incomplete, incorrect, or biased outputs. This study examines whether leading LLMs exhibit gender bias in their responses to a realistic family law scenario. We present an expert-designed divorce scenario grounded in Czech family law and evaluate four state-of-the-art LLMs GPT-5 nano, Claude Haiku 4.5, Gemini 2.5 Flash, and Llama 3.3 in a fully zero-shot interaction. We deploy two versions of the scenario, one with gendered names and one with neutral labels, to establish a baseline for comparison. We further introduce nine legally relevant factors that vary the factual circumstances of the case and test whether these variations influence the models' proposed shared-parenting ratios. Our preliminary results highlight differences across models and suggest gender-dependent patterns in the outcomes generated by some systems. The findings underscore both the risks associated with laypeople's reliance on LLMs for legal guidance and the need for more robust evaluation of model behavior in sensitive legal contexts. We present exploratory and descriptive evidence intended to identify systematic asymmetries rather than to establish causal effects.", "AI": {"tldr": "\u672c\u7814\u7a76\u8003\u5bdf\u5148\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5bb6\u5ead\u6cd5\u6848\u4f8b\u4e2d\u7684\u6027\u522b\u504f\u89c1\uff0c\u53d1\u73b0\u90e8\u5206\u6a21\u578b\u7ed3\u679c\u53d7\u6027\u522b\u5f71\u54cd\uff0c\u63d0\u793a\u6cd5\u5f8b\u81ea\u52a9\u5b58\u5728\u98ce\u9669\u3002", "motivation": "\u9274\u4e8e\u53f8\u6cd5\u53ef\u53ca\u6027\u4e0d\u8db3\uff0c\u666e\u901a\u7528\u6237\u4f9d\u8d56\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6cd5\u5f8b\u81ea\u52a9\uff0c\u4f46\u6a21\u578b\u53ef\u80fd\u4ea7\u751f\u4e0d\u51c6\u786e\u6216\u5e26\u504f\u89c1\u7684\u7ed3\u679c\uff0c\u9700\u8bc4\u4f30\u5176\u5728\u5bb6\u5ead\u6cd5\u5f8b\u573a\u666f\u4e2d\u7684\u6027\u522b\u504f\u89c1\u98ce\u9669\u3002", "method": "\u8bbe\u8ba1\u57fa\u4e8e\u6377\u514b\u5bb6\u5ead\u6cd5\u7684\u79bb\u5a5a\u6848\u4f8b\uff0c\u91c7\u7528\u5e26\u6027\u522b\u548c\u4e2d\u6027\u6807\u7b7e\u4e24\u7248\u672c\uff0c\u6d4b\u8bd5\u56db\u6b3e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u96f6\u6837\u672c\u8f93\u51fa\uff0c\u5e76\u5f15\u5165\u4e5d\u4e2a\u6cd5\u5f8b\u53d8\u91cf\u5206\u6790\u5bf9\u5171\u4eab\u629a\u517b\u6bd4\u4f8b\u7684\u5f71\u54cd\u3002", "result": "\u672c\u7814\u7a76\u901a\u8fc7\u8bbe\u8ba1\u4e00\u771f\u5b9e\u7684\u6377\u514b\u5bb6\u5ead\u6cd5\u5f8b\u79bb\u5a5a\u6848\u4f8b\uff0c\u8bc4\u4f30\u56db\u4e2a\u5148\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\uff08GPT-5 nano\u3001Claude Haiku 4.5\u3001Gemini 2.5 Flash\u3001Llama 3.3\uff09\u5728\u96f6\u6837\u672c\u4ea4\u4e92\u4e0b\u7684\u6027\u522b\u504f\u89c1\u8868\u73b0\u3002\u91c7\u7528\u5e26\u6027\u522b\u540d\u548c\u4e2d\u6027\u6807\u7b7e\u7684\u4e24\u7248\u672c\u6848\u4f8b\uff0c\u5e76\u5f15\u5165\u4e5d\u4e2a\u6cd5\u5f8b\u76f8\u5173\u53d8\u91cf\uff0c\u68c0\u6d4b\u6a21\u578b\u5bf9\u5171\u4eab\u629a\u517b\u6bd4\u4f8b\u7684\u5f71\u54cd\u3002\u7ed3\u679c\u663e\u793a\u4e0d\u540c\u6a21\u578b\u8868\u73b0\u5b58\u5728\u5dee\u5f02\u4e14\u90e8\u5206\u6a21\u578b\u5448\u73b0\u6027\u522b\u4f9d\u8d56\u7684\u504f\u89c1\u6a21\u5f0f\uff0c\u63ed\u793a\u4f9d\u8d56LLM\u8fdb\u884c\u6cd5\u5f8b\u81ea\u52a9\u7684\u6f5c\u5728\u98ce\u9669\uff0c\u5f3a\u8c03\u9700\u52a0\u5f3a\u6a21\u578b\u5728\u654f\u611f\u6cd5\u5f8b\u60c5\u5883\u4e2d\u7684\u8bc4\u4f30\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\u90e8\u5206\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u5bb6\u5ead\u6cd5\u5f8b\u573a\u666f\u65f6\u5b58\u5728\u6027\u522b\u504f\u89c1\uff0c\u8868\u660e\u5bf9\u6cd5\u5f8b\u81ea\u52a9\u624b\u6bb5\u7684\u4f9d\u8d56\u9700\u8981\u8c28\u614e\uff0c\u5e76\u547c\u5401\u52a0\u5f3a\u5bf9\u6a21\u578b\u884c\u4e3a\u7684\u654f\u611f\u6027\u8bc4\u4f30\u3002"}}
{"id": "2601.05882", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.05882", "abs": "https://arxiv.org/abs/2601.05882", "authors": ["Constantinos Karouzos", "Xingwei Tan", "Nikolaos Aletras"], "title": "An Empirical Study on Preference Tuning Generalization and Diversity Under Domain Shift", "comment": null, "summary": "Preference tuning aligns pretrained language models to human judgments of quality, helpfulness, or safety by optimizing over explicit preference signals rather than likelihood alone. Prior work has shown that preference-tuning degrades performance and reduces helpfulness when evaluated outside the training domain. However, the extent to which adaptation strategies mitigate this domain shift remains unexplored. We address this challenge by conducting a comprehensive and systematic study of alignment generalization under domain shift. We compare five popular alignment objectives and various adaptation strategies from source to target, including target-domain supervised fine-tuning and pseudo-labeling, across summarization and question-answering helpfulness tasks. Our findings reveal systematic differences in generalization across alignment objectives under domain shift. We show that adaptation strategies based on pseudo-labeling can substantially reduce domain-shift degradation", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u504f\u597d\u8c03\u4f18\u5728\u9886\u57df\u8fc1\u79fb\u4e0b\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u53d1\u73b0\u4f2a\u6807\u7b7e\u9002\u5e94\u7b56\u7565\u80fd\u663e\u8457\u51cf\u8f7b\u9886\u57df\u8fc1\u79fb\u5e26\u6765\u7684\u6027\u80fd\u9000\u5316\u3002", "motivation": "\u504f\u597d\u8c03\u4f18\u63d0\u9ad8\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u52a9\u76ca\u6027\uff0c\u4f46\u5176\u5728\u8bad\u7ec3\u9886\u57df\u5916\u7684\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u5982\u4f55\u7f13\u89e3\u8fd9\u79cd\u9886\u57df\u504f\u5dee\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u5c1a\u672a\u88ab\u6df1\u5165\u7814\u7a76\u3002", "method": "\u6bd4\u8f83\u4e86\u4e94\u79cd\u5e38\u89c1\u7684\u5bf9\u9f50\u76ee\u6807\u53ca\u591a\u79cd\u9002\u5e94\u7b56\u7565\uff08\u5305\u62ec\u76ee\u6807\u9886\u57df\u7684\u76d1\u7763\u5fae\u8c03\u548c\u4f2a\u6807\u7b7e\u65b9\u6cd5\uff09\uff0c\u5728\u6458\u8981\u548c\u95ee\u7b54\u4efb\u52a1\u4e2d\u8bc4\u4f30\u9886\u57df\u8fc1\u79fb\u7684\u6548\u679c\u3002", "result": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u504f\u597d\u8c03\u4f18\u6765\u5bf9\u9f50\u4eba\u7c7b\u5224\u65ad\uff08\u5982\u8d28\u91cf\u3001\u5e2e\u52a9\u6027\u3001\u5b89\u5168\u6027\uff09\uff0c\u5e76\u91cd\u70b9\u63a2\u8ba8\u4e86\u504f\u597d\u8c03\u4f18\u5728\u9886\u57df\u8fc1\u79fb\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u95ee\u9898\u3002\u4f5c\u8005\u7cfb\u7edf\u6bd4\u8f83\u4e86\u4e94\u79cd\u5bf9\u9f50\u76ee\u6807\u53ca\u591a\u79cd\u4ece\u6e90\u9886\u57df\u5230\u76ee\u6807\u9886\u57df\u7684\u9002\u5e94\u7b56\u7565\uff08\u5305\u62ec\u76ee\u6807\u9886\u57df\u7684\u76d1\u7763\u5fae\u8c03\u548c\u4f2a\u6807\u7b7e\u65b9\u6cd5\uff09\uff0c\u9488\u5bf9\u6458\u8981\u548c\u95ee\u7b54\u52a9\u76ca\u4efb\u52a1\u8fdb\u884c\u8bc4\u4f30\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4e0d\u540c\u5bf9\u9f50\u76ee\u6807\u5728\u9886\u57df\u8fc1\u79fb\u65f6\u6cdb\u5316\u80fd\u529b\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u540c\u65f6\u57fa\u4e8e\u4f2a\u6807\u7b7e\u7684\u9002\u5e94\u7b56\u7565\u80fd\u6709\u6548\u7f13\u89e3\u9886\u57df\u504f\u5dee\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "\u4e0d\u540c\u7684\u504f\u597d\u8c03\u4f18\u76ee\u6807\u5728\u9886\u57df\u8f6c\u79fb\u4e2d\u7684\u6cdb\u5316\u6548\u679c\u5dee\u5f02\u660e\u663e\uff0c\u4f2a\u6807\u7b7e\u9002\u5e94\u7b56\u7565\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u5728\u65b0\u9886\u57df\u7684\u8868\u73b0\uff0c\u51cf\u8f7b\u9886\u57df\u504f\u79fb\u7684\u8d1f\u9762\u5f71\u54cd\u3002"}}
{"id": "2601.05903", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05903", "abs": "https://arxiv.org/abs/2601.05903", "authors": ["Zihang Tian", "Rui Li", "Jingsen Zhang", "Xiaohe Bo", "Wei Huo", "Xu Chen"], "title": "HAPS: Hierarchical LLM Routing with Joint Architecture and Parameter Search", "comment": null, "summary": "Large language model (LLM) routing aims to exploit the specialized strengths of different LLMs for diverse tasks. However, existing approaches typically focus on selecting LLM architectures while overlooking parameter settings, which are critical for task performance. In this paper, we introduce HAPS, a hierarchical LLM routing framework that jointly searches over model architectures and parameters. Specifically, we use a high-level router to select among candidate LLM architectures, and then search for the optimal parameters for the selected architectures based on a low-level router. We design a parameter generation network to share parameters between the two routers to mutually enhance their capabilities. In the training process, we design a reward-augmented objective to effectively optimize our framework. Experiments on two commonly used benchmarks show that HAPS consistently outperforms strong routing baselines. We have released our code at https://github.com/zihangtian/HAPS.", "AI": {"tldr": "HAPS\u6846\u67b6\u7ed3\u5408\u67b6\u6784\u9009\u62e9\u4e0e\u53c2\u6570\u641c\u7d22\uff0c\u5229\u7528\u4e24\u7ea7\u8def\u7531\u5668\u548c\u53c2\u6570\u751f\u6210\u7f51\u7edc\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4efb\u52a1\u8868\u73b0\uff0c\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8def\u7531\u65b9\u6cd5\u901a\u5e38\u53ea\u5173\u6ce8\u6a21\u578b\u67b6\u6784\u7684\u9009\u62e9\uff0c\u5ffd\u7565\u4e86\u53c2\u6570\u8bbe\u7f6e\uff0c\u800c\u53c2\u6570\u8bbe\u7f6e\u5bf9\u4efb\u52a1\u8868\u73b0\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4e24\u7ea7\u8def\u7531\u5668\u7ed3\u6784\uff0c\u9ad8\u5c42\u8def\u7531\u5668\u9009\u62e9\u6a21\u578b\u67b6\u6784\uff0c\u4f4e\u5c42\u8def\u7531\u5668\u641c\u7d22\u6700\u4f18\u53c2\u6570\uff1b\u5f15\u5165\u53c2\u6570\u751f\u6210\u7f51\u7edc\u5171\u4eab\u53c2\u6570\u4ee5\u589e\u5f3a\u8def\u7531\u80fd\u529b\uff1b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u91c7\u7528\u5956\u52b1\u589e\u5f3a\u76ee\u6807\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u63d0\u51fa\u4e86HAPS\uff0c\u4e00\u4e2a\u5c42\u7ea7\u5316\u7684LLM\u8def\u7531\u6846\u67b6\uff0c\u8054\u5408\u641c\u7d22\u6a21\u578b\u67b6\u6784\u548c\u53c2\u6570\uff0c\u901a\u8fc7\u9ad8\u4f4e\u5c42\u8def\u7531\u5668\u534f\u540c\u5de5\u4f5c\u5e76\u5f15\u5165\u53c2\u6570\u751f\u6210\u7f51\u7edc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8def\u7531\u6548\u679c\u3002", "conclusion": "HAPS\u6846\u67b6\u901a\u8fc7\u8054\u5408\u4f18\u5316\u6a21\u578b\u67b6\u6784\u4e0e\u53c2\u6570\u8bbe\u7f6e\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8def\u7531\u6548\u679c\uff0c\u9a8c\u8bc1\u4e86\u53c2\u6570\u641c\u7d22\u5bf9\u6027\u80fd\u63d0\u5347\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2601.05911", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05911", "abs": "https://arxiv.org/abs/2601.05911", "authors": ["Phuong-Hang Le", "Valentin Pelloin", "Arnault Chatelain", "Maryem Bouziane", "Mohammed Ghennai", "Qianwen Guan", "Kirill Milintsevich", "Salima Mdhaffar", "Aidan Mannion", "Nils Defauw", "Shuyue Gu", "Alexandre Audibert", "Marco Dinarelli", "Yannick Est\u00e8ve", "Lorraine Goeuriot", "Steffen Lalande", "Nicolas Herv\u00e9", "Maximin Coavoux", "Fran\u00e7ois Portet", "\u00c9tienne Ollion", "Marie Candito", "Maxime Peyrard", "Solange Rossato", "Benjamin Lecouteux", "Aur\u00e9lie Nardy", "Gilles S\u00e9rasset", "Vincent Segonne", "Sol\u00e8ne Evain", "Diandra Fabre", "Didier Schwab"], "title": "Pantagruel: Unified Self-Supervised Encoders for French Text and Speech", "comment": null, "summary": "We release Pantagruel models, a new family of self-supervised encoder models for French text and speech. Instead of predicting modality-tailored targets such as textual tokens or speech units, Pantagruel learns contextualized target representations in the feature space, allowing modality-specific encoders to capture linguistic and acoustic regularities more effectively. Separate models are pre-trained on large-scale French corpora, including Wikipedia, OSCAR and CroissantLLM for text, together with MultilingualLibriSpeech, LeBenchmark, and INA-100k for speech. INA-100k is a newly introduced 100,000-hour corpus of French audio derived from the archives of the Institut National de l'Audiovisuel (INA), the national repository of French radio and television broadcasts, providing highly diverse audio data. We evaluate Pantagruel across a broad range of downstream tasks spanning both modalities, including those from the standard French benchmarks such as FLUE or LeBenchmark. Across these tasks, Pantagruel models show competitive or superior performance compared to strong French baselines such as CamemBERT, FlauBERT, and LeBenchmark2.0, while maintaining a shared architecture that can seamlessly handle either speech or text inputs. These results confirm the effectiveness of feature-space self-supervised objectives for French representation learning and highlight Pantagruel as a robust foundation for multimodal speech-text understanding.", "AI": {"tldr": "Pantagruel\u6a21\u578b\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u7279\u5f81\u7a7a\u95f4\u76ee\u6807\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u4e14\u5f3a\u5927\u7684\u6cd5\u8bed\u6587\u672c\u548c\u8bed\u97f3\u7f16\u7801\u5668\uff0c\u63d0\u5347\u4e86\u8bed\u8a00\u7406\u89e3\u7684\u8868\u73b0\u3002", "motivation": "\u4f20\u7edf\u6a21\u578b\u9488\u5bf9\u4e0d\u540c\u6a21\u6001\u5206\u522b\u9884\u6d4b\u7279\u5b9a\u76ee\u6807\uff0c\u9650\u5236\u4e86\u7f16\u7801\u5668\u5bf9\u8bed\u8a00\u548c\u58f0\u5b66\u89c4\u5f8b\u7684\u6355\u6349\u80fd\u529b\u3002\u901a\u8fc7\u5b66\u4e60\u7279\u5f81\u7a7a\u95f4\u4e2d\u7684\u76ee\u6807\u8868\u793a\uff0c\u80fd\u591f\u63d0\u5347\u6cd5\u8bed\u6587\u672c\u548c\u8bed\u97f3\u7684\u8868\u5f81\u6548\u679c\uff0c\u589e\u5f3a\u8de8\u6a21\u6001\u7406\u89e3\u80fd\u529b\u3002", "method": "Pantagruel\u6a21\u578b\u91c7\u7528\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u901a\u8fc7\u5728\u7279\u5f81\u7a7a\u95f4\u5b66\u4e60\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u76ee\u6807\u8868\u5f81\uff0c\u8bad\u7ec3\u9002\u7528\u4e8e\u6cd5\u8bed\u6587\u672c\u548c\u8bed\u97f3\u7684\u7f16\u7801\u5668\u3002\u4e0e\u4f20\u7edf\u7684\u9884\u6d4b\u6587\u672c\u6807\u8bb0\u6216\u8bed\u97f3\u5355\u4f4d\u4e0d\u540c\uff0c\u8fd9\u79cd\u65b9\u6cd5\u4f7f\u7f16\u7801\u5668\u80fd\u66f4\u597d\u5730\u6355\u6349\u8bed\u8a00\u548c\u58f0\u5b66\u89c4\u5f8b\u3002\u6a21\u578b\u5206\u522b\u5728\u5927\u89c4\u6a21\u6cd5\u8bed\u8bed\u6599\u5e93(\u5982Wikipedia\u3001OSCAR\u3001CroissantLLM\u6587\u672c\u8d44\u6e90\u4ee5\u53caMultilingualLibriSpeech\u3001LeBenchmark\u548cINA-100k\u8bed\u97f3\u8d44\u6e90)\u4e0a\u9884\u8bad\u7ec3\u3002", "result": "Pantagruel\u6a21\u578b\u5728\u6db5\u76d6\u6587\u672c\u4e0e\u8bed\u97f3\u7684\u591a\u9879\u4e0b\u6e38\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u7ade\u4e89\u529b\uff0c\u4f18\u4e8e\u6216\u8d85\u8d8a\u4e86CamemBERT\u3001FlauBERT\u548cLeBenchmark2.0\u7b49\u5f3a\u57fa\u7ebf\uff0c\u4e14\u67b6\u6784\u7edf\u4e00\uff0c\u80fd\u591f\u65e0\u7f1d\u5904\u7406\u6587\u672c\u548c\u8bed\u97f3\u8f93\u5165\u3002", "conclusion": "\u672c\u6587\u8bc1\u660e\u4e86\u7279\u5f81\u7a7a\u95f4\u81ea\u76d1\u7763\u76ee\u6807\u5728\u6cd5\u8bed\u8868\u793a\u5b66\u4e60\u4e2d\u7684\u6709\u6548\u6027\uff0cPantagruel\u6a21\u578b\u662f\u591a\u6a21\u6001\u8bed\u97f3-\u6587\u672c\u7406\u89e3\u7684\u575a\u5b9e\u57fa\u7840\uff0c\u5177\u5907\u4f18\u5f02\u7684\u6027\u80fd\u548c\u901a\u7528\u67b6\u6784\u3002"}}
{"id": "2601.05960", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05960", "abs": "https://arxiv.org/abs/2601.05960", "authors": ["V\u00edctor Gallego"], "title": "Distilling Feedback into Memory-as-a-Tool", "comment": "Code: https://github.com/vicgalle/feedback-memory-as-a-tool Data: https://huggingface.co/datasets/vicgalle/rubric-feedback-bench", "summary": "We propose a framework that amortizes the cost of inference-time reasoning by converting transient critiques into retrievable guidelines, through a file-based memory system and agent-controlled tool calls. We evaluate this method on the Rubric Feedback Bench, a novel dataset for rubric-based learning. Experiments demonstrate that our augmented LLMs rapidly match the performance of test-time refinement pipelines while drastically reducing inference cost.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u6587\u4ef6\u5b58\u50a8\u548c\u5de5\u5177\u8c03\u7528\u5c06\u4e34\u65f6\u6279\u8bc4\u8f6c\u5316\u4e3a\u53ef\u68c0\u7d22\u6307\u5bfc\u7684\u63a8\u7406\u6210\u672c\u644a\u9500\u6846\u67b6\uff0c\u5728Rubric Feedback Bench\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u663e\u8457\u964d\u4f4e\u63a8\u7406\u6210\u672c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u51cf\u5c11\u5728\u63a8\u7406\u65f6\u8fdb\u884c\u7ec6\u81f4\u4fee\u6b63\u5e26\u6765\u7684\u9ad8\u989d\u8ba1\u7b97\u6210\u672c\uff0c\u901a\u8fc7\u5c06\u4e34\u65f6\u53cd\u9988\u8f6c\u5316\u4e3a\u6301\u4e45\u6307\u5bfc\u4fe1\u606f\u63d0\u5347\u6548\u7387\u3002", "method": "\u4f7f\u7528\u6587\u4ef6\u5b58\u50a8\u7684\u8bb0\u5fc6\u7cfb\u7edf\u548c\u4ee3\u7406\u63a7\u5236\u7684\u5de5\u5177\u8c03\u7528\uff0c\u5c06\u4e34\u65f6\u6279\u8bc4\u8f6c\u5316\u4e3a\u53ef\u68c0\u7d22\u7684\u6307\u5bfc\u4fe1\u606f\u4ee5\u644a\u9500\u63a8\u7406\u6210\u672c\u3002", "result": "\u589e\u5f3a\u7684\u8bed\u8a00\u6a21\u578b\u5728Rubric Feedback Bench\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u4e0e\u6d4b\u8bd5\u65f6\u7ec6\u5316\u65b9\u6cd5\u76f8\u5f53\uff0c\u4f46\u63a8\u7406\u6210\u672c\u5927\u5e45\u4e0b\u964d\u3002", "conclusion": "\u901a\u8fc7\u6587\u4ef6\u57fa\u7840\u7684\u8bb0\u5fc6\u7cfb\u7edf\u548c\u5de5\u5177\u8c03\u7528\uff0c\u589e\u5f3a\u7684\u8bed\u8a00\u6a21\u578b\u80fd\u8fc5\u901f\u8fbe\u5230\u6d4b\u8bd5\u65f6\u7ec6\u5316\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u4e14\u5927\u5e45\u964d\u4f4e\u63a8\u7406\u6210\u672c\u3002"}}
{"id": "2601.06002", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06002", "abs": "https://arxiv.org/abs/2601.06002", "authors": ["Qiguang Chen", "Yantao Du", "Ziniu Li", "Jinhao Liu", "Songyao Duan", "Jiarui Guo", "Minghao Liu", "Jiaheng Liu", "Tong Yang", "Ge Zhang", "Libo Qin", "Wanxiang Che", "Wenhao Huang"], "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning", "comment": "Preprint", "summary": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u6709\u6548\u5b66\u4e60\u957f\u94fe\u601d\u7ef4\u662f\u56e0\u4e3a\u7f3a\u4e4f\u7a33\u5b9a\u7684\u7c7b\u5206\u5b50\u7ed3\u6784\uff0c\u8bba\u6587\u63d0\u51faMole-Syn\u65b9\u6cd5\u5408\u6210\u8fd9\u4e9b\u7ed3\u6784\uff0c\u63d0\u5347\u63a8\u7406\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u4ece\u4eba\u7c7b\u6216\u975e\u957f\u94fe\u601d\u7ef4\u6a21\u578b\u6a21\u4eff\u4e2d\u5b66\u5230\u6709\u6548\u7684\u957f\u94fe\u601d\u7ef4\u63a8\u7406\uff0c\u9700\u7406\u89e3\u6709\u6548\u957f\u94fe\u601d\u7ef4\u7684\u7ed3\u6784\u7279\u5f81\u53ca\u5176\u5bf9\u5b66\u4e60\u7684\u5f71\u54cd\uff0c\u4ece\u800c\u8bbe\u8ba1\u534f\u52a9\u957f\u94fe\u601d\u7ef4\u5b66\u4e60\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMole-Syn\u5206\u5e03\u8f6c\u79fb\u56fe\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u62df\u4e09\u79cd\u76f8\u4e92\u4f5c\u7528\u5408\u6210\u957f\u94fe\u601d\u7ef4\u7ed3\u6784\uff0c\u6307\u5bfc\u6709\u6548\u8f68\u8ff9\u5408\u6210\uff0c\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7a33\u5b9a\u3002", "result": "\u8be5\u8bba\u6587\u63d0\u51fa\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5b66\u4e60\u957f\u94fe\u601d\u7ef4\u94fe\uff08Long CoT\uff09\u63a8\u7406\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u5206\u6790\u5176\u539f\u56e0\u662f\u6709\u6548\u7684\u957f\u94fe\u601d\u7ef4\u8f68\u8ff9\u5177\u6709\u7a33\u5b9a\u7684\u7c7b\u5206\u5b50\u7ed3\u6784\uff0c\u8fd9\u4e9b\u7ed3\u6784\u5305\u62ec\u6df1\u5ea6\u63a8\u7406\u3001\u81ea\u6211\u53cd\u601d\u548c\u81ea\u6211\u63a2\u7d22\u4e09\u79cd\u76f8\u4e92\u4f5c\u7528\u65b9\u5f0f\u3002\u901a\u8fc7\u84b8\u998f\u8f68\u8ff9\u5206\u6790\u53d1\u73b0\u8fd9\u4e9b\u7ed3\u6784\u6765\u6e90\u4e8e\u957f\u94fe\u601d\u7ef4\u5fae\u8c03\u800c\u975e\u7b80\u5355\u7684\u5173\u952e\u8bcd\u6a21\u4eff\u3002\u8bba\u6587\u5f15\u5165\u6709\u6548\u8bed\u4e49\u5f02\u6784\u4f53\u6982\u5ff5\uff0c\u6307\u51fa\u53ea\u6709\u4fc3\u8fdb\u5feb\u901f\u71b5\u6536\u655b\u7684\u201c\u952e\u201d\u652f\u6301\u7a33\u5b9a\u7684\u957f\u94fe\u601d\u7ef4\u5b66\u4e60\uff0c\u800c\u7ed3\u6784\u7ade\u4e89\u53cd\u800c\u635f\u5bb3\u8bad\u7ec3\u6548\u679c\u3002\u57fa\u4e8e\u6b64\uff0c\u63d0\u51faMole-Syn\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5e03\u8f6c\u79fb\u56fe\u6307\u5bfc\u6709\u6548\u957f\u94fe\u7ed3\u6784\u5408\u6210\uff0c\u63d0\u5347\u6027\u80fd\u548c\u5f3a\u5316\u5b66\u4e60\u7a33\u5b9a\u6027\u3002", "conclusion": "\u7a33\u5b9a\u4e14\u6709\u6548\u7684\u957f\u94fe\u601d\u7ef4\u8f68\u8ff9\u4f9d\u8d56\u4e09\u79cd\u7c7b\u5206\u5b50\u76f8\u4e92\u4f5c\u7528\u7ed3\u6784\uff0c\u8fd9\u4e9b\u7ed3\u6784\u901a\u8fc7\u5fae\u8c03\u5f62\u6210\uff0c\u6b63\u786e\u5f15\u5bfc\u7ed3\u6784\u5408\u6210\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u8868\u73b0\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002"}}
{"id": "2601.06007", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.06007", "abs": "https://arxiv.org/abs/2601.06007", "authors": ["Elias Lumer", "Faheem Nizar", "Akshaya Jangiti", "Kevin Frank", "Anmol Gulati", "Mandar Phadate", "Vamse Kumar Subbiah"], "title": "Don't Break the Cache: An Evaluation of Prompt Caching for Long-Horizon Agentic Tasks", "comment": "15 pages, 8 figures", "summary": "Recent advancements in Large Language Model (LLM) agents have enabled complex multi-turn agentic tasks requiring extensive tool calling, where conversations can span dozens of API calls with increasingly large context windows. However, although major LLM providers offer prompt caching to reduce cost and latency, its benefits for agentic workloads remain underexplored in the research literature. To our knowledge, no prior work quantifies these cost savings or compares caching strategies for multi-turn agentic tasks. We present a comprehensive evaluation of prompt caching across three major LLM providers (OpenAI, Anthropic, and Google) and compare three caching strategies, including full context caching, system prompt only caching, and caching that excludes dynamic tool results. We evaluate on DeepResearchBench, a multi-turn agentic benchmark where agents autonomously execute real-world web search tool calls to answer complex research questions, measuring both API cost and time to first token (TTFT) across over 500 agent sessions with 10,000-token system prompts. Our results demonstrate that prompt caching reduces API costs by 45-80% and improves time to first token by 13-31% across providers. We find that strategic prompt cache block control, such as placing dynamic content at the end of the system prompt, avoiding dynamic traditional function calling, and excluding dynamic tool results, provides more consistent benefits than naive full-context caching, which can paradoxically increase latency. Our analysis reveals nuanced variations in caching behavior across providers, and we provide practical guidance for implementing prompt caching in production agentic systems.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4e09\u5927\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u5546\u5728\u591a\u8f6e\u4ee3\u7406\u4efb\u52a1\u4e2d\u63d0\u793a\u7f13\u5b58\u7684\u6548\u679c\uff0c\u53d1\u73b0\u5408\u7406\u7684\u7f13\u5b58\u7b56\u7565\u80fd\u663e\u8457\u964d\u4f4eAPI\u6210\u672c\u548c\u54cd\u5e94\u65f6\u95f4\u3002", "motivation": "\u5f53\u524d\u867d\u7136\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u5546\u652f\u6301\u63d0\u793a\u7f13\u5b58\u4ee5\u964d\u4f4e\u6210\u672c\u548c\u5ef6\u8fdf\uff0c\u4f46\u9488\u5bf9\u590d\u6742\u591a\u8f6e\u4ee3\u7406\u4efb\u52a1\u63d0\u793a\u7f13\u5b58\u7684\u5177\u4f53\u6548\u76ca\u53ca\u6700\u4f73\u7f13\u5b58\u7b56\u7565\u5c1a\u672a\u88ab\u7cfb\u7edf\u91cf\u5316\u548c\u6bd4\u8f83\u3002", "method": "\u5728DeepResearchBench\u57fa\u51c6\u4e0a\u5bf9OpenAI\u3001Anthropic\u548cGoogle\u4e09\u5bb6\u63d0\u4f9b\u5546\uff0c\u6bd4\u8f83\u5168\u4e0a\u4e0b\u6587\u7f13\u5b58\u3001\u4ec5\u7cfb\u7edf\u63d0\u793a\u7f13\u5b58\u53ca\u6392\u9664\u52a8\u6001\u5de5\u5177\u7ed3\u679c\u7684\u7f13\u5b58\u4e09\u79cd\u7b56\u7565\uff0c\u7edf\u8ba1\u8d85500\u6b21\u4f1a\u8bdd\u7684\u6210\u672c\u548c\u54cd\u5e94\u65f6\u95f4\u3002", "result": "\u63d0\u793a\u7f13\u5b58\u80fd\u964d\u4f4e45-80%API\u8d39\u7528\uff0c\u63d0\u534713-31%\u9996\u6b21\u54cd\u5e94\u901f\u5ea6\uff0c\u4e14\u901a\u8fc7\u5408\u7406\u8bbe\u8ba1\u7f13\u5b58\u5757\u4f4d\u7f6e\u548c\u5185\u5bb9\uff0c\u80fd\u66f4\u52a0\u7a33\u5b9a\u5730\u53d1\u6325\u7f13\u5b58\u6548\u7528\u3002\u4e0d\u540c\u63d0\u4f9b\u5546\u7f13\u5b58\u8868\u73b0\u5b58\u5728\u7ec6\u5fae\u5dee\u5f02\uff0c\u6587\u7ae0\u63d0\u51fa\u4e86\u751f\u4ea7\u73af\u5883\u4e2d\u63d0\u793a\u7f13\u5b58\u7684\u5b9e\u7528\u5efa\u8bae\u3002", "conclusion": "\u63d0\u793a\u7f13\u5b58\u80fd\u663e\u8457\u8282\u7701API\u6210\u672c\uff0845-80%\uff09\u5e76\u63d0\u5347\u54cd\u5e94\u901f\u5ea6\uff0813-31%\uff09\uff0c\u4e14\u4f18\u5316\u7f13\u5b58\u7b56\u7565\uff08\u5982\u52a8\u6001\u5185\u5bb9\u653e\u672b\u7aef\u3001\u6392\u9664\u52a8\u6001\u5de5\u5177\u7ed3\u679c\uff09\u6548\u679c\u4f18\u4e8e\u7b80\u5355\u5168\u4e0a\u4e0b\u6587\u7f13\u5b58\u3002"}}
{"id": "2601.06021", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.06021", "abs": "https://arxiv.org/abs/2601.06021", "authors": ["Jiajie Zhang", "Xin Lv", "Ling Feng", "Lei Hou", "Juanzi Li"], "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards", "comment": null, "summary": "Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose \\textbf{Citation-aware Rubric Rewards (CaRR)}, a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce \\textbf{Citation-aware Group Relative Policy Optimization (C-GRPO)}, which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u7ed3\u5408\u7ec6\u7c92\u5ea6\u8bc1\u636e\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6CaRR\u548c\u8bad\u7ec3\u65b9\u6cd5C-GRPO\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u641c\u7d22\u4ee3\u7406\u7684\u63a8\u7406\u5168\u9762\u6027\u548c\u771f\u5b9e\u6027\uff0c\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u5956\u52b1\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8c\u5143\u7ed3\u679c\u5956\u52b1\uff0c\u4e0d\u80fd\u5168\u9762\u53cd\u6620\u63a8\u7406\u8fc7\u7a0b\u7684\u5b8c\u6574\u6027\u548c\u4e8b\u5b9e\u6027\uff0c\u5bfc\u81f4\u5feb\u6377\u65b9\u5f0f\u5229\u7528\u548c\u5e7b\u89c9\u7b49\u4e0d\u826f\u884c\u4e3a\u3002", "method": "\u63d0\u51fa\u4e86\u7ec6\u7c92\u5ea6\u5956\u52b1\u6846\u67b6Citation-aware Rubric Rewards (CaRR)\uff0c\u901a\u8fc7\u5206\u89e3\u590d\u6742\u95ee\u9898\u4e3a\u53ef\u9a8c\u8bc1\u7684\u5355\u8df3\u8bc4\u5206\u6807\u51c6\uff0c\u8981\u6c42\u4ee3\u7406\u8bc6\u522b\u9690\u85cf\u5b9e\u4f53\u3001\u7ed9\u51fa\u6b63\u786e\u5f15\u7528\u5e76\u6784\u5efa\u5b8c\u6574\u8bc1\u636e\u94fe\uff1b\u5f15\u5165\u7ed3\u5408CaRR\u4e0e\u7ed3\u679c\u5956\u52b1\u7684Citation-aware Group Relative Policy Optimization (C-GRPO)\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "C-GRPO\u5728\u591a\u4e2a\u6df1\u5ea6\u641c\u7d22\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u4f20\u7edf\u57fa\u4e8e\u7ed3\u679c\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u6709\u6548\u9632\u6b62\u5feb\u6377\u65b9\u5f0f\u5229\u7528\uff0c\u4fc3\u8fdb\u5168\u9762\u4e14\u57fa\u4e8e\u8bc1\u636e\u7684\u63a8\u7406\uff0c\u5e76\u5c55\u73b0\u51fa\u5bf9\u5f00\u653e\u5f0f\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u7ed3\u5408\u7ec6\u7c92\u5ea6\u8bc1\u636e\u5956\u52b1\u7684RL\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u641c\u7d22\u4ee3\u7406\u7684\u63a8\u7406\u8d28\u91cf\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u57fa\u4e8eLLM\u7684\u6df1\u5ea6\u641c\u7d22\u63d0\u4f9b\u4e86\u6709\u6548\u4f18\u5316\u9014\u5f84\u3002"}}
{"id": "2601.06022", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06022", "abs": "https://arxiv.org/abs/2601.06022", "authors": ["Chengming Cui", "Tianxin Wei", "Ziyi Chen", "Ruizhong Qiu", "Zhichen Zeng", "Zhining Liu", "Xuying Ning", "Duo Zhou", "Jingrui He"], "title": "AdaFuse: Adaptive Ensemble Decoding with Test-Time Scaling for LLMs", "comment": null, "summary": "Large language models (LLMs) exhibit complementary strengths arising from differences in pretraining data, model architectures, and decoding behaviors. Inference-time ensembling provides a practical way to combine these capabilities without retraining. However, existing ensemble approaches suffer from fundamental limitations. Most rely on fixed fusion granularity, which lacks the flexibility required for mid-generation adaptation and fails to adapt to different generation characteristics across tasks. To address these challenges, we propose AdaFuse, an adaptive ensemble decoding framework that dynamically selects semantically appropriate fusion units during generation. Rather than committing to a fixed granularity, AdaFuse adjusts fusion behavior on the fly based on the decoding context, with words serving as basic building blocks for alignment. To be specific, we introduce an uncertainty-based criterion to decide whether to apply ensembling at each decoding step. Under confident decoding states, the model continues generation directly. In less certain states, AdaFuse invokes a diversity-aware scaling strategy to explore alternative candidate continuations and inform ensemble decisions. This design establishes a synergistic interaction between adaptive ensembling and test-time scaling, where ensemble decisions guide targeted exploration, and the resulting diversity in turn strengthens ensemble quality. Experiments on open-domain question answering, arithmetic reasoning, and machine translation demonstrate that AdaFuse consistently outperforms strong ensemble baselines, achieving an average relative improvement of 6.88%. The code is available at https://github.com/CCM0111/AdaFuse.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86AdaFuse\uff0c\u4e00\u79cd\u81ea\u9002\u5e94\u7684\u96c6\u6210\u89e3\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u9002\u5f53\u7684\u878d\u5408\u5355\u5143\u6765\u63d0\u9ad8\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u56fa\u5b9a\u878d\u5408\u7c92\u5ea6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u63a8\u7406\u65f6\u96c6\u6210\u65b9\u6cd5\u56e0\u56fa\u5b9a\u878d\u5408\u7c92\u5ea6\u7f3a\u4e4f\u7075\u6d3b\u6027\uff0c\u96be\u4ee5\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\u53ca\u751f\u6210\u8fc7\u7a0b\u4e2d\u53d8\u5316\u7684\u7279\u70b9\uff0c\u9650\u5236\u4e86\u96c6\u6210\u80fd\u529b\u7684\u53d1\u6325\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u52a8\u6001\u9009\u62e9\u878d\u5408\u5355\u5143\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u5355\u8bcd\u4f5c\u4e3a\u57fa\u672c\u5bf9\u9f50\u5355\u4f4d\uff0c\u5728\u7f6e\u4fe1\u72b6\u6001\u76f4\u63a5\u751f\u6210\uff0c\u5728\u4e0d\u786e\u5b9a\u72b6\u6001\u91c7\u7528\u591a\u6837\u6027\u611f\u77e5\u7684\u6269\u5c55\u7b56\u7565\u4ee5\u63a2\u7d22\u5907\u9009\u7b54\u6848\uff0c\u589e\u5f3a\u96c6\u6210\u7684\u6548\u679c\u3002", "result": "\u5728\u5f00\u653e\u57df\u95ee\u7b54\u3001\u7b97\u672f\u63a8\u7406\u548c\u673a\u5668\u7ffb\u8bd1\u4efb\u52a1\u4e2d\uff0cAdaFuse\u5747\u8d85\u8d8a\u4e86\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e73\u5747\u63d0\u5347\u4e866.88%\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "AdaFuse\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u878d\u5408\u7c92\u5ea6\u53ca\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u5224\u65ad\uff0c\u6709\u6548\u63d0\u5347\u4e86\u591a\u6a21\u578b\u96c6\u6210\u7684\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u76f8\u8f83\u5f3a\u57fa\u7ebf\u5e73\u5747\u63d0\u53476.88%\u3002"}}
