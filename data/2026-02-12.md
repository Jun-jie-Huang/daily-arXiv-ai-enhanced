<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 53]
- [cs.MA](#cs.MA) [Total: 5]
- [cs.SE](#cs.SE) [Total: 15]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Reviewing the Reviewer: Elevating Peer Review Quality through LLM-Guided Feedback](https://arxiv.org/abs/2602.10118)
*Sukannya Purkayastha,Qile Wan,Anne Lauscher,Lizhen Qu,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本论文提出了一种结合神经符号方法和LLM的同行评审懒惰思考检测与反馈框架，有效提升评审质量并发布了相关数据集。


<details>
  <summary>Details</summary>
Motivation: 科学同行评审质量因懒惰思考和简单启发式下降，且现有方法仅作为单标签任务，缺少针对性的反馈指导。

Method: 采用LLM驱动的框架，将评审分解为论证片段，通过神经符号模块结合LLM特征和传统分类器识别问题，再利用遗传算法优化的模板生成针对性反馈。

Result: 方法优于零样本LLM基线，显著提高了评审质量，并发布了含1,309条懒惰思考与具体性标签的LazyReviewPlus数据集。

Conclusion: 本论文提出的基于LLM的懒惰思考检测框架显著提升了同行评审质量，达到了92.4%的提升效果，并发布了新的数据集LazyReviewPlus。

Abstract: Peer review is central to scientific quality, yet reliance on simple heuristics -- lazy thinking -- has lowered standards. Prior work treats lazy thinking detection as a single-label task, but review segments may exhibit multiple issues, including broader clarity problems, or specificity issues. Turning detection into actionable improvements requires guideline-aware feedback, which is currently missing. We introduce an LLM-driven framework that decomposes reviews into argumentative segments, identifies issues via a neurosymbolic module combining LLM features with traditional classifiers, and generates targeted feedback using issue-specific templates refined by a genetic algorithm. Experiments show our method outperforms zero-shot LLM baselines and improves review quality by up to 92.4\%. We also release LazyReviewPlus, a dataset of 1,309 sentences labeled for lazy thinking and specificity.

</details>


### [2] [Latent Thoughts Tuning: Bridging Context and Reasoning with Fused Information in Latent Tokens](https://arxiv.org/abs/2602.10229)
*Weihao Liu,Dehai Min,Lu Cheng*

Main category: cs.CL

TL;DR: 本文提出LT-Tuning框架，通过上下文与预测语义融合及课程学习提升潜在空间推理的稳定性和准确性，有效超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 显式的链式思维方法受限于离散词汇空间，限制模型思维表达；连续潜在空间推理虽有潜力，但现有方法因特征塌陷和对齐问题不稳定，需要一种更鲁棒的潜在推理机制。

Method: LT-Tuning框架通过结合上下文隐藏状态和词汇嵌入空间的预测语义指导，使用Context-Prediction-Fusion机制，同时采用渐进式三阶段课程学习，实现潜在思维与显式思维模式的动态切换。

Result: 实验结果表明，LT-Tuning优于现有的潜在推理基线，成功缓解了特征塌陷问题，提升了推理的鲁棒性和准确性。

Conclusion: 本文提出的LT-Tuning方法通过引入上下文预测融合机制和三阶段课程学习，有效解决了潜在思维中的特征塌陷和不稳定问题，在连续潜在空间的推理中表现出更高的准确性和鲁棒性。

Abstract: While explicit Chain-of-Thought (CoT) equips Large Language Models (LLMs) with strong reasoning capabilities, it requires models to verbalize every intermediate step in text tokens, constraining the model thoughts to the discrete vocabulary space. Recently, reasoning in continuous latent space has emerged as a promising alternative, enabling more robust inference and flexible computation beyond discrete token constraints. However, current latent paradigms often suffer from feature collapse and instability, stemming from distribution mismatches when recurrently using hidden states as the input embeddings, or alignment issues when relying on assistant models. To address this, we propose Latent Thoughts Tuning (LT-Tuning), a framework that redefines how latent thoughts are constructed and deployed. Instead of relying solely on raw hidden states, our method introduces a Context-Prediction-Fusion mechanism that jointly leveraging contextual hidden states and predictive semantic guidance from the vocabulary embedding space. Combined with a progressive three-stage curriculum learning pipeline, LT-Tuning also enables dynamically switching between latent and explicit thinking modes. Experiments demonstrate that our method outperforms existing latent reasoning baselines, effectively mitigating feature collapse and achieving robust reasoning accuracy.

</details>


### [3] [Learning to Evict from Key-Value Cache](https://arxiv.org/abs/2602.10238)
*Luca Moschella,Laura Manduchi,Ozan Sener*

Main category: cs.CL

TL;DR: 该论文提出通过强化学习优化大语言模型KV缓存的驱逐策略，大幅提升缓存管理效率和性能，适用于多种任务和更长上下文。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的自回归KV缓存占用大量内存，现有驱逐或压缩方法依赖启发式策略，间接且带来计算开销，需要一种更有效且可自适应的缓存管理方式。

Method: 将KV缓存驱逐问题重新定义为强化学习问题，设计了轻量级的每个头部的RL代理，基于预计算的生成轨迹仅使用键值向量学习评估和排序令牌的未来效用，训练过程中无需修改底层模型或额外推理成本。

Result: 在长上下文基准RULER和多轮对话基准OASST2-4k上取得明显性能提升；零样本测试显示其在不同下游任务和更长上下文长度下依然表现良好，验证了方法的泛化能力和扩展性。

Conclusion: 通过强化学习策略学习预测未来令牌的有用性，提出的KV Policy (KVP)在不同模型和基准测试中显著优于现有方法，并能很好地泛化到更长的上下文和不同任务。

Abstract: The growing size of Large Language Models (LLMs) makes efficient inference challenging, primarily due to the memory demands of the autoregressive Key-Value (KV) cache. Existing eviction or compression methods reduce cost but rely on heuristics, such as recency or past attention scores, which serve only as indirect proxies for a token's future utility and introduce computational overhead. We reframe KV cache eviction as a reinforcement learning (RL) problem: learning to rank tokens by their predicted usefulness for future decoding. To this end, we introduce KV Policy (KVP), a framework of lightweight per-head RL agents trained on pre-computed generation traces using only key and value vectors. Each agent learns a specialized eviction policy guided by future utility, which evaluates the quality of the ranking across all cache budgets, requiring no modifications to the underlying LLM or additional inference. Evaluated across two different model families on the long-context benchmark RULER and the multi-turn dialogue benchmark OASST2-4k, KVP significantly outperforms baselines. Furthermore, zero-shot tests on standard downstream tasks (e.g., LongBench, BOOLQ, ARC) indicate that KVP generalizes well beyond its training distribution and to longer context lengths. These results demonstrate that learning to predict future token utility is a powerful and scalable paradigm for adaptive KV cache management.

</details>


### [4] [On Emergent Social World Models -- Evidence for Functional Integration of Theory of Mind and Pragmatic Reasoning in Language Models](https://arxiv.org/abs/2602.10298)
*Polina Tsvilodub,Jan-Felix Klumpp,Amir Mohammadpour,Jennifer Hu,Michael Franke*

Main category: cs.CL

TL;DR: 研究显示语言模型可能具备共享的社会认知机制，支持其具备综合的社会世界模型。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型是否具备共享的计算机制以支持通用理论心智和语言特定的语用推理，进而回答语言模型是否具有新兴的“社会世界模型”。

Method: 采用行为评估和基于认知神经科学启发的功能定位方法，结合假设驱动的统计测试，对语言模型在七个理论心智（ToM）能力子类别上的表现进行分析。

Result: 提供了具有创新性的理论心智定位器数据，改进了功能定位方法，并给出了语言模型社会认知能力Emergence的实证证据。

Conclusion: 语言模型（LMs）可能发展出相互关联的“社会世界模型”，而非孤立的能力，支持了功能整合假设。

Abstract: This paper investigates whether LMs recruit shared computational mechanisms for general Theory of Mind (ToM) and language-specific pragmatic reasoning in order to contribute to the general question of whether LMs may be said to have emergent "social world models", i.e., representations of mental states that are repurposed across tasks (the functional integration hypothesis). Using behavioral evaluations and causal-mechanistic experiments via functional localization methods inspired by cognitive neuroscience, we analyze LMs' performance across seven subcategories of ToM abilities (Beaudoin et al., 2020) on a substantially larger localizer dataset than used in prior like-minded work. Results from stringent hypothesis-driven statistical testing offer suggestive evidence for the functional integration hypothesis, indicating that LMs may develop interconnected "social world models" rather than isolated competencies. This work contributes novel ToM localizer data, methodological refinements to functional localization techniques, and empirical insights into the emergence of social cognition in artificial systems.

</details>


### [5] [Are More Tokens Rational? Inference-Time Scaling in Language Models as Adaptive Resource Rationality](https://arxiv.org/abs/2602.10329)
*Zhimin Hu,Riya Roshan,Sashank Varma*

Main category: cs.CL

TL;DR: 研究表明，大型语言模型通过扩展推理时计算，自发展现资源理性行为，能根据任务复杂度调整推理策略，无需显式成本奖励。


<details>
  <summary>Details</summary>
Motivation: 探讨资源理性是否能在没有显式与计算成本相关奖励的情况下，通过推理时的扩展自发出现。

Method: 设计了变量归因任务，通过改变候选变量数和试验次数系统地控制任务复杂度，比较指令调优模型与强化学习训练的大型推理模型的表现。

Result: 指令调优模型在复杂的异或和同或函数上表现下降，而强化学习训练模型表现更加稳健。两者均能根据任务复杂度调整推理策略。

Conclusion: 推理性能提升可以通过推理时计算资源的扩大实现，且模型在任务复杂度变化时能够展现从穷举法转向分析法的策略调整。

Abstract: Human reasoning is shaped by resource rationality -- optimizing performance under constraints. Recently, inference-time scaling has emerged as a powerful paradigm to improve the reasoning performance of Large Language Models by expanding test-time computation. Specifically, instruction-tuned (IT) models explicitly generate long reasoning steps during inference, whereas Large Reasoning Models (LRMs) are trained by reinforcement learning to discover reasoning paths that maximize accuracy. However, it remains unclear whether resource-rationality can emerge from such scaling without explicit reward related to computational costs. We introduce a Variable Attribution Task in which models infer which variables determine outcomes given candidate variables, input-output trials, and predefined logical functions. By varying the number of candidate variables and trials, we systematically manipulate task complexity. Both models exhibit a transition from brute-force to analytic strategies as complexity increases. IT models degrade on XOR and XNOR functions, whereas LRMs remain robust. These findings suggest that models can adjust their reasoning behavior in response to task complexity, even without explicit cost-based reward. It provides compelling evidence that resource rationality is an emergent property of inference-time scaling itself.

</details>


### [6] [The Subjectivity of Respect in Police Traffic Stops: Modeling Community Perspectives in Body-Worn Camera Footage](https://arxiv.org/abs/2602.10339)
*Preni Golazizian,Elnaz Rahmati,Jackson Trager,Zhivar Sourati,Nona Ghazizadeh,Georgios Chochlakis,Jose Alcocer,Kerby Bennett,Aarya Vijay Devnani,Parsa Hejabi,Harry G. Muttram,Akshay Kiran Padte,Mehrshad Saadatinia,Chenhao Wu,Alireza S. Zaibari,Michael Sierra-Arévalo,Nick Weller,Shrikanth Narayanan,Benjamin A. T. Graham,Morteza Dehghani*

Main category: cs.CL

TL;DR: 本文利用洛杉矶警察局交通拦截录像，创建了多视角尊重评分数据集，并提出视角感知模型预测个性化尊重评分和理由，提高对不同社区尊重理解，助力执法公信力提升。


<details>
  <summary>Details</summary>
Motivation: 交通拦截是警方与市民最常见的接触形式，执法过程中尊重感的主观性强，且受个体经历影响，社区视角多样性使得理解不同社区的期望成为亟须解决的问题。

Method: （1）基于程序正义理论、LAPD培训材料和实地调研开发了领域专用评估标准；（2）引入了用以视角一致数据构建的评分驱动偏好数据构建框架；（3）提出了视角感知模型框架，用以根据交通拦截记录预测个性化尊重评分并生成标记者特定理由。

Result: 所提出的方法在警方相关、受司法影响及非相关三类标记者群体中，均有效提升了尊重评分预测准确性和理由文本的对齐度，验证了视角感知模型的有效性。

Conclusion: 本文构建了一个包含多视角尊重评分及自由文本理由的洛杉矶警察局交通拦截大规模数据集，并提出了一个能够预测个性化尊重评分及生成特定标记者理由的视角感知建模框架，提高了评分预测和理由对齐的性能，有助于执法部门理解多元社区期望，增强公众信任和程序正当性。

Abstract: Traffic stops are among the most frequent police-civilian interactions, and body-worn cameras (BWCs) provide a unique record of how these encounters unfold. Respect is a central dimension of these interactions, shaping public trust and perceived legitimacy, yet its interpretation is inherently subjective and shaped by lived experience, rendering community-specific perspectives a critical consideration. Leveraging unprecedented access to Los Angeles Police Department BWC footage, we introduce the first large-scale traffic-stop dataset annotated with respect ratings and free-text rationales from multiple perspectives. By sampling annotators from police-affiliated, justice-system-impacted, and non-affiliated Los Angeles residents, we enable the systematic study of perceptual differences across diverse communities. To this end, we (i) develop a domain-specific evaluation rubric grounded in procedural justice theory, LAPD training materials, and extensive fieldwork; (ii) introduce a rubric-driven preference data construction framework for perspective-consistent alignment; and (iii) propose a perspective-aware modeling framework that predicts personalized respect ratings and generates annotator-specific rationales for both officers and civilian drivers from traffic-stop transcripts. Across all three annotator groups, our approach improves both rating prediction performance and rationale alignment. Our perspective-aware framework enables law enforcement to better understand diverse community expectations, providing a vital tool for building public trust and procedural legitimacy.

</details>


### [7] [Geometry-Aware Decoding with Wasserstein-Regularized Truncation and Mass Penalties for Large Language Models](https://arxiv.org/abs/2602.10346)
*Arash Gholami Davoodi,Navid Rezazadeh,Seyed Pouyan Mousavi Davoudi,Pouya Pezeshkpour*

Main category: cs.CL

TL;DR: 本文提出了基于几何感知的Top-W截断采样方法，实现了在保持合理概率质量和熵的前提下，通过Wasserstein距离优化token截断，提升了大型语言模型生成的准确性与创造力。


<details>
  <summary>Details</summary>
Motivation: 现有截断采样方法主要依赖概率质量和熵，忽略了token空间的语义几何信息，导致生成结果的多样性和逻辑性难以兼顾。

Method: 提出了Top-W截断规则，基于token嵌入空间的几何结构，利用Wasserstein距离来平衡保留概率质量和熵，设计了一种简单的闭式解，结合高效的几何潜力函数实现，配合交替解码流程。

Result: 在GSM8K、GPQA、AlpacaEval和MT-Bench四个基准测试和三种指令调优模型上，Top-W方法显著优于现有顶尖解码策略，准确率提升最高达33.7%，并增强了创造力表现。

Conclusion: Top-W方法通过引入基于Wasserstein距离的几何感知截断规则，显著提升了大型语言模型在生成任务中的表现，兼顾了生成文本的多样性、创造力和逻辑一致性。

Abstract: Large language models (LLMs) must balance diversity and creativity against logical coherence in open-ended generation. Existing truncation-based samplers are effective but largely heuristic, relying mainly on probability mass and entropy while ignoring semantic geometry of the token space. We present Top-W, a geometry-aware truncation rule that uses Wasserstein distance-defined over token-embedding geometry-to keep the cropped distribution close to the original, while explicitly balancing retained probability mass against the entropy of the kept set. Our theory yields a simple closed-form structure for the fixed-potential subset update: depending on the mass-entropy trade-off, the optimal crop either collapses to a single token or takes the form of a one-dimensional prefix that can be found efficiently with a linear scan. We implement Top-W using efficient geometry-based potentials (nearest-set or k-NN) and pair it with an alternating decoding routine that keeps the standard truncation-and-sampling interface unchanged. Extensive experiments on four benchmarks (GSM8K, GPQA, AlpacaEval, and MT-Bench) across three instruction-tuned models show that Top-W consistently outperforms prior state-of-the-art decoding approaches achieving up to 33.7% improvement. Moreover, we find that Top-W not only improves accuracy-focused performance, but also boosts creativity under judge-based open-ended evaluation.

</details>


### [8] [When Less Is More? Diagnosing ASR Predictions in Sardinian via Layer-Wise Decoding](https://arxiv.org/abs/2602.10350)
*Domenico De Cristofaro,Alessandro Vietti,Marianne Pouplier,Aleese Block*

Main category: cs.CL

TL;DR: 研究发现多语言语音模型的中间层比最终层在音素预测上更准确，截断顶层提升了低资源语言的识别性能，中间层探测可作为ASR诊断工具。


<details>
  <summary>Details</summary>
Motivation: 现有多语言语音模型最终层输出在音素准确性上不佳，特别是在低资源语言中，探查中间层潜在的更准确语音表示可能改善识别性能。

Method: 对预训练的Wav2Vec2模型采取每层解码策略，研究语音识别过程中不同编码器层的音素级预测演变，特别针对资源匮乏的坎皮达内斯撒丁语。

Result: 在坎皮达内斯撒丁语中，截断更高层transformer层后，音素错误率降低，最佳性能不是最终层而是倒数第二层。中间层预测更精确，减少回归错误，反映出模型更细粒度的声学信息处理。

Conclusion: 中间层的语音表示在多语言语音模型中比最终层更准确，截断上层transformer层可提高音素错误率表现，早期层的输出更好地保持了音段身份，避免了过生成和某些语音错误。

Abstract: Recent studies have shown that intermediate layers in multilingual speech models often encode more phonetically accurate representations than the final output layer. In this work, we apply a layer-wise decoding strategy to a pretrained Wav2Vec2 model to investigate how phoneme-level predictions evolve across encoder layers, focusing on Campidanese Sardinian, a low-resource language. We show that truncating upper transformer layers leads to improved Phoneme Error Rates (PER), with the best performance achieved not at the final layer, but two layers earlier. Through fine-grained alignment analysis, we find that intermediate predictions better preserve segmental identity, avoid overgeneration, and reduce certain classes of phonological errors. We also introduce the notion of regressive errors, cases where correct predictions at intermediate layers are overwritten by errors at the final layer. These regressions highlight the limitations of surface-level error metrics and reveal how deeper layers may generalize or abstract away from acoustic detail. Our findings support the use of early-layer probing as a diagnostic tool for ASR models, particularly in low-resource settings where standard evaluation metrics may fail to capture linguistically meaningful behavior.

</details>


### [9] [Learning Self-Interpretation from Interpretability Artifacts: Training Lightweight Adapters on Vector-Label Pairs](https://arxiv.org/abs/2602.10352)
*Keenan Pepper,Alex McKenzie,Florin Pop,Stijn Servaes,Martin Leitgab,Mike Vaiana,Judd Rosenblatt,Michael S. A. Graziano,Diogo de Lucena*

Main category: cs.CL

TL;DR: 本文提出通过训练轻量级适配器来提升冻结语言模型的自我解释能力，显著提高了不同任务和模型上的解释性能，且随模型规模增长效果更佳。


<details>
  <summary>Details</summary>
Motivation: 现有的语言模型自我解释方法由于对超参数敏感性高而不可靠，亟需一种稳定且通用的自我解释方法。

Method: 在保持语言模型参数冻结的情况下，训练一个具有少量参数的标量仿射适配器，生成稀疏自编码器特征标签用于自我解释。

Result: 训练的适配器在标签生成、主题识别及多跳推理中表现优异，且简单的偏置向量贡献最大，适配器方法随模型规模增大表现更好。

Conclusion: 训练轻量级适配器以生成解释性标签，提高了语言模型的自我解释能力，且该方法具有良好的泛化性和稳定性。

Abstract: Self-interpretation methods prompt language models to describe their own internal states, but remain unreliable due to hyperparameter sensitivity. We show that training lightweight adapters on interpretability artifacts, while keeping the LM entirely frozen, yields reliable self-interpretation across tasks and model families. A scalar affine adapter with just $d_\text{model}+1$ parameters suffices: trained adapters generate sparse autoencoder feature labels that outperform the training labels themselves (71% vs 63% generation scoring at 70B scale), identify topics with 94% recall@1 versus 1% for untrained baselines, and decode bridge entities in multi-hop reasoning that appear in neither prompt nor response, surfacing implicit reasoning without chain-of-thought. The learned bias vector alone accounts for 85% of improvement, and simpler adapters generalize better than more expressive alternatives. Controlling for model knowledge via prompted descriptions, we find self-interpretation gains outpace capability gains from 7B to 72B parameters. Our results demonstrate that self-interpretation improves with scale, without modifying the model being interpreted.

</details>


### [10] [Physically Interpretable AlphaEarth Foundation Model Embeddings Enable LLM-Based Land Surface Intelligence](https://arxiv.org/abs/2602.10354)
*Mashrekur Rahman*

Main category: cs.CL

TL;DR: 研究系统揭示了卫星基础模型嵌入的物理可解释性及其稳健性，并成功将其应用于基于自然语言的地表环境智能系统，推动环境与地理信息系统的融合。


<details>
  <summary>Details</summary>
Motivation: 目前卫星基础模型生成的密集嵌入物理解释性较差，限制了其在环境决策系统中的应用，因此需要系统地分析其物理含义并开发可操作化的智能系统。

Method: 利用2017至2023年期间覆盖美国大陆的1210万个样本，结合线性、非线性和基于注意力机制的方法，对Google AlphaEarth模型的64维嵌入与26个环境变量进行全面的可解释性分析，并通过空间块交叉验证和时间稳定性检验验证结果。随后构建了基于FAISS索引的嵌入数据库，结合检索增强生成技术实现自然语言环境查询系统，并用四个大语言模型进行评估。

Result: 发现个别嵌入维度对应特定地表属性，整体嵌入能高精度重构多数环境变量（12个变量$R^2 > 0.90$，温度和高程接近$R^2=0.97$）；维度-变量关系稳定且时空稳健。构建的土地表面智能系统在360个查询响应循环中获得加权平均评分3.74/5，且语义基础性和连贯性评分较高。

Conclusion: 卫星基础模型的64维嵌入具有明确的物理可解释性，能够高精度映射地表环境变量，且在空间和时间上表现出稳健性。这些嵌入可以用于构建基于自然语言查询的地表智能系统，实现环境与地理空间情报的操作化应用。

Abstract: Satellite foundation models produce dense embeddings whose physical interpretability remains poorly understood, limiting their integration into environmental decision systems. Using 12.1 million samples across the Continental United States (2017--2023), we first present a comprehensive interpretability analysis of Google AlphaEarth's 64-dimensional embeddings against 26 environmental variables spanning climate, vegetation, hydrology, temperature, and terrain. Combining linear, nonlinear, and attention-based methods, we show that individual embedding dimensions map onto specific land surface properties, while the full embedding space reconstructs most environmental variables with high fidelity (12 of 26 variables exceed $R^2 > 0.90$; temperature and elevation approach $R^2 = 0.97$). The strongest dimension-variable relationships converge across all three analytical methods and remain robust under spatial block cross-validation (mean $ΔR^2 = 0.017$) and temporally stable across all seven study years (mean inter-year correlation $r = 0.963$). Building on these validated interpretations, we then developed a Land Surface Intelligence system that implements retrieval-augmented generation over a FAISS-indexed embedding database of 12.1 million vectors, translating natural language environmental queries into satellite-grounded assessments. An LLM-as-Judge evaluation across 360 query--response cycles, using four LLMs in rotating generator, system, and judge roles, achieved weighted scores of $μ= 3.74 \pm 0.77$ (scale 1--5), with grounding ($μ= 3.93$) and coherence ($μ= 4.25$) as the strongest criteria. Our results demonstrate that satellite foundation model embeddings are physically structured representations that can be operationalized for environmental and geospatial intelligence.

</details>


### [11] [Autonomous Continual Learning of Computer-Use Agents for Environment Adaptation](https://arxiv.org/abs/2602.10356)
*Tianci Xue,Zeyi Liao,Tianneng Shi,Zilu Wang,Kai Zhang,Dawn Song,Yu Su,Huan Sun*

Main category: cs.CL

TL;DR: 本论文提出一种无需人工数据的自主课程强化学习方法，帮助智能代理在多变环境中持续学习并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现实数字环境多样且动态，代理经常遇到未见过的场景和分布变化，传统依赖人工标注数据的方法成本高，如何实现无人工数据的环境适应成为挑战。

Method: 引入自主课程强化学习（ACuRL）框架，通过目标环境探索获取初始经验，并使用课程任务生成器结合经验和反馈迭代合成新任务；设计自动评估器CUAJudge以提供可靠奖励信号。

Result: ACuRL在环境内和跨环境持续学习中表现出4-22%的性能提升，且无灾难性遗忘；CUAJudge自动评估与人工判断一致率达93%；参数更新稀疏（约20%），有助于稳健适应。

Conclusion: 本研究提出的ACuRL框架有效实现了计算机使用代理在缺乏人工标注数据的情况下，能够持续适应特定环境，且避免了灾难性遗忘，提升了性能。

Abstract: Real-world digital environments are highly diverse and dynamic. These characteristics cause agents to frequently encounter unseen scenarios and distribution shifts, making continual learning in specific environments essential for computer-use agents (CUAs). However, a key challenge lies in obtaining high-quality and environment-grounded agent data without relying on costly human annotation. In this work, we introduce ACuRL, an Autonomous Curriculum Reinforcement Learning framework that continually adapts agents to specific environments with zero human data. The agent first explores target environments to acquire initial experiences. During subsequent iterative training, a curriculum task generator leverages these experiences together with feedback from the previous iteration to synthesize new tasks tailored for the agent's current capabilities. To provide reliable reward signals, we introduce CUAJudge, a robust automatic evaluator for CUAs that achieves 93% agreement with human judgments. Empirically, our method effectively enables both intra-environment and cross-environment continual learning, yielding 4-22% performance gains without catastrophic forgetting on existing environments. Further analyses show highly sparse updates (e.g., 20% parameters), which helps explain the effective and robust adaptation. Our data and code are available at https://github.com/OSU-NLP-Group/ACuRL.

</details>


### [12] [The Alignment Bottleneck in Decomposition-Based Claim Verification](https://arxiv.org/abs/2602.10380)
*Mahmud Elahi Akhter,Federico Ruggeri,Iman Munire Bilal,Rob Procter,Maria Liakata*

Main category: cs.CL

TL;DR: 该文指出复杂主张验证中分解方法效果差异源于证据对齐不足和子主张错误传播，提出新数据集和评测方法，证明只有细粒度精确对齐的证据才能显著提升性能，且在含噪声标签时保守弃权更有利于系统鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有主张分解方法针对复杂、多面主张验证的效果不稳定，我们认为问题源于忽视了证据对齐和子主张错误类型的瓶颈，旨在通过深入分析这两方面来揭示性能不一致的原因。

Method: 引入一个包含时间限定证据及人工注释子主张证据跨度的新数据集，在两种证据对齐设置下（SAE和SRE）进行主张分解评估，比较不同证据对齐对分解效果的影响，并分析子主张标签错误类型对下游任务稳定性的影响。

Result: 实验结果显示，只有在证据粒度细且严格对齐的情况下，主张分解才显著提升性能。而依赖重复主张级证据的标准设置在多个数据集和领域中均无法提升性能且常降低效果。此外，含噪声子主张标签中，保守的弃权方式比激进的错误预测更能降低误差传播。

Conclusion: 结构化主张分解在证据严格对齐时能显著提升复杂主张验证性能，但标准使用重复主张级证据的方式不但无效，反而会降低性能。此外，含噪声的子主张标签中，保守的‘弃权’策略能有效减少误差传播。

Abstract: Structured claim decomposition is often proposed as a solution for verifying complex, multi-faceted claims, yet empirical results have been inconsistent. We argue that these inconsistencies stem from two overlooked bottlenecks: evidence alignment and sub-claim error profiles. To better understand these factors, we introduce a new dataset of real-world complex claims, featuring temporally bounded evidence and human-annotated sub-claim evidence spans. We evaluate decomposition under two evidence alignment setups: Sub-claim Aligned Evidence (SAE) and Repeated Claim-level Evidence (SRE). Our results reveal that decomposition brings significant performance improvement only when evidence is granular and strictly aligned. By contrast, standard setups that rely on repeated claim-level evidence (SRE) fail to improve and often degrade performance as shown across different datasets and domains (PHEMEPlus, MMM-Fact, COVID-Fact). Furthermore, we demonstrate that in the presence of noisy sub-claim labels, the nature of the error ends up determining downstream robustness. We find that conservative "abstention" significantly reduces error propagation compared to aggressive but incorrect predictions. These findings suggest that future claim decomposition frameworks must prioritize precise evidence synthesis and calibrate the label bias of sub-claim verification models.

</details>


### [13] [Triggers Hijack Language Circuits: A Mechanistic Analysis of Backdoor Behaviors in Large Language Models](https://arxiv.org/abs/2602.10382)
*Théo Lasnier,Wissam Antoun,Francis Kulumba,Djamé Seddah*

Main category: cs.CL

TL;DR: 本文首次系统分析了大型语言模型中语言切换后门的内部机制，发现后门触发器利用了模型已有的语言编码组件，提示防御方法可针对已知功能组件进行监控和缓解。


<details>
  <summary>Details</summary>
Motivation: 当前对大型语言模型中后门攻击的内部机制理解不足，尤其是语言切换后门触发机制。

Method: 通过激活修补技术，定位触发器形成于模型浅层，并识别出处理触发信息的注意力头。

Result: 发现触发器激活的注意力头与自然编码输出语言的注意力头在不同模型规模中高度重合，Jaccard指数为0.18至0.66。

Conclusion: 语言切换后门触发器与模型已有的输出语言编码组件高度重叠，形成非孤立回路，提示后门利用了模型的自然语言处理机制。

Abstract: Backdoor attacks pose significant security risks for Large Language Models (LLMs), yet the internal mechanisms by which triggers operate remain poorly understood. We present the first mechanistic analysis of language-switching backdoors, studying the GAPperon model family (1B, 8B, 24B parameters) which contains triggers injected during pretraining that cause output language switching. Using activation patching, we localize trigger formation to early layers (7.5-25% of model depth) and identify which attention heads process trigger information. Our central finding is that trigger-activated heads substantially overlap with heads naturally encoding output language across model scales, with Jaccard indices between 0.18 and 0.66 over the top heads identified. This suggests that backdoor triggers do not form isolated circuits but instead co-opt the model's existing language components. These findings have implications for backdoor defense: detection methods may benefit from monitoring known functional components rather than searching for hidden circuits, and mitigation strategies could potentially leverage this entanglement between injected and natural behaviors.

</details>


### [14] [When Tables Go Crazy: Evaluating Multimodal Models on French Financial Documents](https://arxiv.org/abs/2602.10384)
*Virginie Mouilleron,Théo Lasnier,Djamé Seddah*

Main category: cs.CL

TL;DR: 该论文提出了首个法语金融文档多模态基准测试，评估视觉语言模型在复杂金融任务中的表现，发现模型在文本和表格处理上表现不错，但图表理解和多轮对话推理能力不足。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在专业的非英语金融领域文档理解中可靠性尚未充分探究，而金融文档内容复杂且错误后果严重，因此需要一个多模态基准来测评模型在法语金融文件中的理解能力。

Method: 提出了Multimodal Finance Eval基准测试，使用六个开源视觉语言模型，通过LLM作为评判员进行评估，涵盖文本提取、表格理解、图表解读和多轮对话推理。

Result: 在文本和表格任务上模型准确率达到85-90%，但在图表解释任务上表现较差，仅34-62%，且多轮对话中早期错误会累积，导致准确率降至约50%。

Conclusion: 目前的视觉语言模型在明确定义的文本和表格提取任务中表现良好，但在交互式、多步骤的金融分析中表现脆弱，特别是在图表解读和多轮对话推理中准确率较低。

Abstract: Vision-language models (VLMs) perform well on many document understanding tasks, yet their reliability in specialized, non-English domains remains underexplored. This gap is especially critical in finance, where documents mix dense regulatory text, numerical tables, and visual charts, and where extraction errors can have real-world consequences. We introduce Multimodal Finance Eval, the first multimodal benchmark for evaluating French financial document understanding. The dataset contains 1,204 expert-validated questions spanning text extraction, table comprehension, chart interpretation, and multi-turn conversational reasoning, drawn from real investment prospectuses, KIDs, and PRIIPs. We evaluate six open-weight VLMs (8B-124B parameters) using an LLM-as-judge protocol. While models achieve strong performance on text and table tasks (85-90% accuracy), they struggle with chart interpretation (34-62%). Most notably, multi-turn dialogue reveals a sharp failure mode: early mistakes propagate across turns, driving accuracy down to roughly 50% regardless of model size.
  These results show that current VLMs are effective for well-defined extraction tasks but remain brittle in interactive, multi-step financial analysis. Multimodal Finance Eval offers a challenging benchmark to measure and drive progress in this high-stakes setting.

</details>


### [15] [Less is Enough: Synthesizing Diverse Data in Feature Space of LLMs](https://arxiv.org/abs/2602.10388)
*Zhongzhi Li,Xuansheng Wu,Yijiang Li,Lijie Hu,Ninghao Liu*

Main category: cs.CL

TL;DR: 提出了一种基于特征激活覆盖率的新颖数据多样性度量和数据合成方法，通过补全缺失特征提升大模型下游任务性能，实现跨模型知识迁移。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本度量的数据多样性指标难以准确反映任务相关特征，限制了下游性能提升的效果，因此需要更有效的可解释特征空间多样性度量方法。

Method: 采用稀疏自动编码器识别种子数据中缺失的特征，并生成包含这些特征的合成数据，以提升数据多样性和模型性能。

Result: 实验结果表明该方法在多种任务（包括指令跟随、毒性检测、奖励建模和行为引导）中均显著提升了数据多样性和模型表现，且发现了多个模型间共享的可解释特征空间。

Conclusion: 引入了基于特征激活覆盖率（FAC）的数据多样性度量方法，提出了FAC综合框架，有效提升了训练数据的多样性及下游任务性能，且实现了跨模型的知识迁移。

Abstract: The diversity of post-training data is critical for effective downstream performance in large language models (LLMs). Many existing approaches to constructing post-training data quantify diversity using text-based metrics that capture linguistic variation, but such metrics provide only weak signals for the task-relevant features that determine downstream performance. In this work, we introduce Feature Activation Coverage (FAC) which measures data diversity in an interpretable feature space. Building upon this metric, we further propose a diversity-driven data synthesis framework, named FAC Synthesis, that first uses a sparse autoencoder to identify missing features from a seed dataset, and then generates synthetic samples that explicitly reflect these features. Experiments show that our approach consistently improves both data diversity and downstream performance on various tasks, including instruction following, toxicity detection, reward modeling, and behavior steering. Interestingly, we identify a shared, interpretable feature space across model families (i.e., LLaMA, Mistral, and Qwen), enabling cross-model knowledge transfer. Our work provides a solid and practical methodology for exploring data-centric optimization of LLMs.

</details>


### [16] [When are We Worried? Temporal Trends of Anxiety and What They Reveal about Us](https://arxiv.org/abs/2602.10400)
*Saif M. Mohammad*

Main category: cs.CL

TL;DR: 通过分析社交媒体数据，揭示了焦虑在一天、周中的变化规律，及其与时态、人称代词的关联，提供了对焦虑心理状态的深入理解。


<details>
  <summary>Details</summary>
Motivation: 探究人们在社交媒体上的焦虑时间点及其背后的心理机制。

Method: 利用词汇表分析美加社交媒体推文中的焦虑词汇，结合时段、时态和代词使用进行数据分析。

Result: 发现焦虑在早上8点最高，午间最低，周末低于工作日；过去时态句子中的焦虑最高，未来时最低；第三人称代词帖文焦虑高于第一、二人称，主格代词帖文焦虑高于宾格。

Conclusion: 社交媒体上的焦虑水平呈现出系统性的日内变化规律，并且与人称代词的使用和时态有关。

Abstract: In this short paper, we make use of a recently created lexicon of word-anxiety associations to analyze large amounts of US and Canadian social media data (tweets) to explore *when* we are anxious and what insights that reveals about us. We show that our levels of anxiety on social media exhibit systematic patterns of rise and fall during the day -- highest at 8am (in-line with when we have high cortisol levels in the body) and lowest around noon. Anxiety is lowest on weekends and highest mid-week. We also examine anxiety in past, present, and future tense sentences to show that anxiety is highest in past tense and lowest in future tense. Finally, we examine the use of anxiety and calmness words in posts that contain pronouns to show: more anxiety in 3rd person pronouns (he, they) posts than 1st and 2nd person pronouns and higher anxiety in posts with subject pronouns (I, he, she, they) than object pronouns (me, him, her, them). Overall, these trends provide valuable insights on not just when we are anxious, but also how different types of focus (future, past, self, outward, etc.) are related to anxiety.

</details>


### [17] [EVOKE: Emotion Vocabulary Of Korean and English](https://arxiv.org/abs/2602.10414)
*Yoonwon Jung,Hagyeong Shin,Benjamin K. Bergen*

Main category: cs.CL

TL;DR: 本文提出了EVOKE，一个最全面的韩英情感词汇平行数据集，系统标注多义词和情感隐喻，支持多学科跨语言情感研究。


<details>
  <summary>Details</summary>
Motivation: 为了构建一个涵盖韩语和英语情感词汇的平行数据集，解决跨语言情感表达差异的问题。

Method: 收集并系统性地标注韩语和英语的情感形容词和动词，覆盖多义词及其关系，识别语言特异性情感词汇，同时注释情感相关隐喻。

Result: 构建了包含1427个韩语词和1399个英语词，分别标注了819个韩语词和924个英语词的多义情感词和隐喻关系的平行数据集，成为迄今为止最全面、系统且理论无关的韩英情感词汇数据库。

Conclusion: EVOKE数据集为情感科学、心理语言学、计算语言学和自然语言处理提供了强大的多用途资源，支持不同理论视角和需求的研究者使用，促进跨语言情感研究的发展。

Abstract: This paper introduces EVOKE, a parallel dataset of emotion vocabulary in English and Korean. The dataset offers comprehensive coverage of emotion words in each language, in addition to many-to-many translations between words in the two languages and identification of language-specific emotion words. The dataset contains 1,427 Korean words and 1,399 English words, and we systematically annotate 819 Korean and 924 English adjectives and verbs. We also annotate multiple meanings of each word and their relationships, identifying polysemous emotion words and emotion-related metaphors. The dataset is, to our knowledge, the most comprehensive, systematic, and theory-agnostic dataset of emotion words in both Korean and English to date. It can serve as a practical tool for emotion science, psycholinguistics, computational linguistics, and natural language processing, allowing researchers to adopt different views on the resource reflecting their needs and theoretical perspectives. The dataset is publicly available at https://github.com/yoonwonj/EVOKE.

</details>


### [18] [LATA: A Tool for LLM-Assisted Translation Annotation](https://arxiv.org/abs/2602.10454)
*Baorong Huang,Ali Asiri*

Main category: cs.CL

TL;DR: 本文提出了一个基于大语言模型辅助的混合自动化与人工校正的阿拉伯语-英语平行语料对齐工具，提高了标注效率与质量。


<details>
  <summary>Details</summary>
Motivation: 传统自动句子对齐工具难以应对结构差异大的语言对及深层语义转换，迫切需要兼顾自动化效率与人工精密判断的解决方案。

Method: 该方法采用模板化的Prompt Manager，通过大语言模型执行句子分段和对齐，并集成自动预处理与人工校正的工作流，实现严格的JSON格式输出和基于standoff架构的翻译技术注释。

Result: 工具能有效兼顾标注效率与语言学精度，帮助研究人员高效精准地分析专业领域复杂翻译现象。

Conclusion: 本文提出了一种基于大语言模型辅助的交互式对齐工具，有效提升了阿拉伯语-英语结构差异语言对的平行语料构建质量。

Abstract: The construction of high-quality parallel corpora for translation research has increasingly evolved from simple sentence alignment to complex, multi-layered annotation tasks. This methodological shift presents significant challenges for structurally divergent language pairs, such as Arabic--English, where standard automated tools frequently fail to capture deep linguistic shifts or semantic nuances. This paper introduces a novel, LLM-assisted interactive tool designed to reduce the gap between scalable automation and the rigorous precision required for expert human judgment. Unlike traditional statistical aligners, our system employs a template-based Prompt Manager that leverages large language models (LLMs) for sentence segmentation and alignment under strict JSON output constraints. In this tool, automated preprocessing integrates into a human-in-the-loop workflow, allowing researchers to refine alignments and apply custom translation technique annotations through a stand-off architecture. By leveraging LLM-assisted processing, the tool balances annotation efficiency with the linguistic precision required to analyze complex translation phenomena in specialized domains.

</details>


### [19] [Neuro-Symbolic Synergy for Interactive World Modeling](https://arxiv.org/abs/2602.10480)
*Hongyu Zhao,Siyu Zhou,Haolin Yang,Zengyi Qin,Tianyi Zhou*

Main category: cs.CL

TL;DR: 本文提出Neuro-Symbolic Synergy框架，通过结合大语言模型的语义先验与符号规则，提升世界模型的准确性和训练效率，实验证明其在多环境中表现出色。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型尽管具备强大的推理能力，但在遵守确定性转移规则方面表现不佳，尤其是边缘情况下易产生幻觉，符号模型虽逻辑一致性强但缺乏语义表现力。因此需要一种结合二者优点的世界模型。

Method: NeSyS 通过交替训练神经和符号两种世界模型，利用彼此未能充分解释的轨迹进行训练。符号模型对语言模型输出进行概率分布修正，神经模型仅在符号规则覆盖不到的轨迹上进行微调。

Result: 在ScienceWorld、Webshop和Plancraft三个交互环境中，NeSyS在预测准确率和数据效率上均显著优于基线方法，且训练数据需求减少了50%而准确率不降。

Conclusion: NeSyS 框架有效结合了大规模语言模型的概率语义先验和可执行的符号规则，实现了表达能力与鲁棒性的双重优势。通过符号世界模型对神经世界模型输出概率分布的直接约束，使训练效率提高同时保持准确率。

Abstract: Large language models (LLMs) exhibit strong general-purpose reasoning capabilities, yet they frequently hallucinate when used as world models (WMs), where strict compliance with deterministic transition rules--particularly in corner cases--is essential. In contrast, Symbolic WMs provide logical consistency but lack semantic expressivity. To bridge this gap, we propose Neuro-Symbolic Synergy (NeSyS), a framework that integrates the probabilistic semantic priors of LLMs with executable symbolic rules to achieve both expressivity and robustness. NeSyS alternates training between the two models using trajectories inadequately explained by the other. Unlike rule-based prompting, the symbolic WM directly constrains the LLM by modifying its output probability distribution. The neural WM is fine-tuned only on trajectories not covered by symbolic rules, reducing training data by 50% without loss of accuracy. Extensive experiments on three distinct interactive environments, i.e., ScienceWorld, Webshop, and Plancraft, demonstrate NeSyS's consistent advantages over baselines in both WM prediction accuracy and data efficiency.

</details>


### [20] [Canvas-of-Thought: Grounding Reasoning via Mutable Structured States](https://arxiv.org/abs/2602.10494)
*Lingzhuang Sun,Yuxia Zhu,Ruitong Liu,Hao Liang,Zheng Sun,Caijun Jia,Honghao He,Yuchen Wu,Siyuan Li,Jingxuan Wei,Xiangxiang Zhang,Bihui Yu,Wentao Zhang*

Main category: cs.CL

TL;DR: Canvas-CoT通过引入HTML Canvas进行原地状态更新和视觉反馈，突破了多模态模型线性文本推理的瓶颈，显著提升复杂任务的推理效果和效率。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型依赖单维线性文本推理序列，难以高效处理复杂任务，尤其是在几何和SVG设计等高维领域，文本表达缺乏视觉引导导致推理精度受限。

Method: Canvas-CoT利用HTML Canvas支持基于DOM的CRUD操作，实现推理状态的原地修改，结合基于渲染的批评循环提供视觉反馈，解决传统线性文本推理无法高效纠正局部错误和缺乏视觉指导的问题。

Result: 在VCode、RBench-V和MathVista数据集上的大量实验表明，Canvas-CoT在推理准确性和上下文效率方面显著优于现有基线方法。

Conclusion: 本文提出的Canvas-of-Thought (Canvas-CoT) 通过引入HTML Canvas作为外部推理载体，有效解决了多模态大语言模型在复杂任务中因线性文本序列推理的瓶颈，显著提升了模型推理精度和效率，开启了上下文高效的多模态推理新范式。

Abstract: While Chain-of-Thought (CoT) prompting has significantly advanced the reasoning capabilities of Multimodal Large Language Models (MLLMs), relying solely on linear text sequences remains a bottleneck for complex tasks. We observe that even when auxiliary visual elements are interleaved, they are often treated as static snapshots within a one-dimensional, unstructured reasoning chain. We argue that such approaches treat reasoning history as an immutable stream: correcting a local error necessitates either generating verbose downstream corrections or regenerating the entire context. This forces the model to implicitly maintain and track state updates, significantly increasing token consumption and cognitive load. This limitation is particularly acute in high-dimensional domains, such as geometry and SVG design, where the textual expression of CoT lacks explicit visual guidance, further constraining the model's reasoning precision. To bridge this gap, we introduce \textbf{Canvas-of-Thought (Canvas-CoT)}. By leveraging a HTML Canvas as an external reasoning substrate, Canvas-CoT empowers the model to perform atomic, DOM-based CRUD operations. This architecture enables in-place state revisions without disrupting the surrounding context, allowing the model to explicitly maintain the "ground truth". Furthermore, we integrate a rendering-based critique loop that serves as a hard constraint validator, providing explicit visual feedback to resolve complex tasks that are difficult to articulate through text alone. Extensive experiments on VCode, RBench-V, and MathVista demonstrate that Canvas-CoT significantly outperforms existing baselines, establishing a new paradigm for context-efficient multimodal reasoning.

</details>


### [21] [On the Robustness of Knowledge Editing for Detoxification](https://arxiv.org/abs/2602.10504)
*Ming Dong,Shiyi Tang,Ziyan Peng,Guanyi Chen,Tingting He*

Main category: cs.CL

TL;DR: 本文提出面向鲁棒性的评估框架，揭示基于知识编辑的去毒化方法存在伪去毒和跨语言、组合性限制，强调现有自动评估指标的不足。


<details>
  <summary>Details</summary>
Motivation: 现有去毒化方法的评估主要依赖自动毒性分类器，假设其毒性分数下降意味着有害行为的真正抑制，但这一假设缺乏严谨验证。

Method: 提出了一个面向鲁棒性的评估框架，从优化鲁棒性、组合鲁棒性和跨语言鲁棒性三个维度对KE-based去毒化方法进行评估。

Result: 发现伪去毒化是常见失败模式，即毒性分数降低实际上是由于退化生成行为而非真正的有害内容抑制；多种不安全行为共同编辑时去毒效果下降；单语和跨语言的去毒效果依赖特定模型和方法组合。

Conclusion: 基于知识编辑的去毒化方法在某些模型、有限的去毒目标数量和特定语言子集下表现出一定的鲁棒性，但总体上存在显著局限性和失败模式。

Abstract: Knowledge-Editing-based (KE-based) detoxification has emerged as a promising approach for mitigating harmful behaviours in Large Language Models. Existing evaluations, however, largely rely on automatic toxicity classifiers, implicitly assuming that reduced toxicity scores reflect genuine behavioural suppression. In this work, we propose a robustness-oriented evaluation framework for KE-based detoxification that examines its reliability beyond standard classifier-based metrics along three dimensions: optimisation robustness, compositional robustness, and cross-lingual robustness. We identify pseudo-detoxification as a common failure mode, where apparent toxicity reductions arise from degenerate generation behaviours rather than meaningful suppression of unsafe content. We further show that detoxification effectiveness degrades when multiple unsafe behaviours are edited jointly, and that both monolingual and cross-lingual detoxification remain effective only under specific model-method combinations. Overall, our results indicate that KE-based detoxification is robust only for certain models, limited numbers of detoxification objectives, and a subset of languages.

</details>


### [22] [LHAW: Controllable Underspecification for Long-Horizon Tasks](https://arxiv.org/abs/2602.10525)
*George Pu,Michael S. Lee,Udari Madhushani Sehwag,David J. Lee,Bryan Zhu,Yash Maurya,Mohit Raghavendra,Yuan Xue,Samuel Marc Denton*

Main category: cs.CL

TL;DR: LHAW提出了一种全新的框架，有效生成和评估长期任务中的不确定性变体，助力提升自主代理系统在复杂模糊环境下的执行可靠性。


<details>
  <summary>Details</summary>
Motivation: 长期工作流代理在模糊情景下需要澄清以确保正确执行，但缺乏可扩展且无任务依赖的框架来系统性地评估和管理任务不确定性。

Method: LHAW采用模块化的合成管道，通过四个维度（目标、约束、输入和上下文）有控制地删除信息，构造信息不足的任务变体，并通过代理实验证明变体的影响，分类为关键结果、分歧或良性。

Result: 发布了285个任务变体并进行实证分析，揭示当前代理在检测、推理和解决任务不明确性上的表现，首次实现了对代理澄清行为的成本敏感评估。

Conclusion: 本文提出的LHAW框架为长期任务执行中的模糊性处理提供了系统化、可控且无任务依赖的方法，推动了自主系统中工作流代理的可靠执行。

Abstract: Long-horizon workflow agents that operate effectively over extended periods are essential for truly autonomous systems. Their reliable execution critically depends on the ability to reason through ambiguous situations in which clarification seeking is necessary to ensure correct task execution. However, progress is limited by the lack of scalable, task-agnostic frameworks for systematically curating and measuring the impact of ambiguity across custom workflows. We address this gap by introducing LHAW (Long-Horizon Augmented Workflows), a modular, dataset-agnostic synthetic pipeline that transforms any well-specified task into controllable underspecified variants by systematically removing information across four dimensions - Goals, Constraints, Inputs, and Context - at configurable severity levels. Unlike approaches that rely on LLM predictions of ambiguity, LHAW validates variants through empirical agent trials, classifying them as outcome-critical, divergent, or benign based on observed terminal state divergence. We release 285 task variants from TheAgentCompany, SWE-Bench Pro and MCP-Atlas according to our taxonomy alongside formal analysis measuring how current agents detect, reason about, and resolve underspecification across ambiguous settings. LHAW provides the first systematic framework for cost-sensitive evaluation of agent clarification behavior in long-horizon settings, enabling development of reliable autonomous systems.

</details>


### [23] [When to Memorize and When to Stop: Gated Recurrent Memory for Long-Context Reasoning](https://arxiv.org/abs/2602.10560)
*Leheng Sheng,Yongtao Zhang,Wenchang Ma,Yaorui Shi,Ting Huang,Xiang Wang,An Zhang,Ke Shen,Tat-Seng Chua*

Main category: cs.CL

TL;DR: GRU-Mem提出了基于文本控制门和强化学习的机制，解决大语言模型长上下文推理中的效率和稳定性问题，大幅提升推理速度和性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在处理长上下文时性能下降，且MemAgent方法中内存无差别更新和缺乏退出机制导致效率低下和计算浪费。

Method: 提出了GRU-Mem模型，引入两个文本控制的门机制（更新门和退出门），并结合端到端强化学习中的奖励信号促进正确的更新与退出行为。

Result: GRU-Mem在多种长上下文推理任务中表现优越，推理速度相较于MemAgent提升最多达400%。

Conclusion: GRU-Mem通过引入文本控制的更新门和退出门，显著提升了大语言模型在长上下文推理中的稳定性和效率，克服了传统RNN式内存更新的缺陷。

Abstract: While reasoning over long context is crucial for various real-world applications, it remains challenging for large language models (LLMs) as they suffer from performance degradation as the context length grows. Recent work MemAgent has tried to tackle this by processing context chunk-by-chunk in an RNN-like loop and updating a textual memory for final answering. However, this naive recurrent memory update faces two crucial drawbacks: (i) memory can quickly explode because it can update indiscriminately, even on evidence-free chunks; and (ii) the loop lacks an exit mechanism, leading to unnecessary computation after even sufficient evidence is collected. To address these issues, we propose GRU-Mem, which incorporates two text-controlled gates for more stable and efficient long-context reasoning. Specifically, in GRU-Mem, the memory only updates when the update gate is open and the recurrent loop will exit immediately once the exit gate is open. To endow the model with such capabilities, we introduce two reward signals $r^{\text{update}}$ and $r^{\text{exit}}$ within end-to-end RL, rewarding the correct updating and exiting behaviors respectively. Experiments on various long-context reasoning tasks demonstrate the effectiveness and efficiency of GRU-Mem, which generally outperforms the vanilla MemAgent with up to 400\% times inference speed acceleration.

</details>


### [24] [Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters](https://arxiv.org/abs/2602.10604)
*Ailin Huang,Ang Li,Aobo Kong,Bin Wang,Binxing Jiao,Bo Dong,Bojun Wang,Boyu Chen,Brian Li,Buyun Ma,Chang Su,Changxin Miao,Changyi Wan,Chao Lou,Chen Hu,Chen Xu,Chenfeng Yu,Chengting Feng,Chengyuan Yao,Chunrui Han,Dan Ma,Dapeng Shi,Daxin Jiang,Dehua Ma,Deshan Sun,Di Qi,Enle Liu,Fajie Zhang,Fanqi Wan,Guanzhe Huang,Gulin Yan,Guoliang Cao,Guopeng Li,Han Cheng,Hangyu Guo,Hanshan Zhang,Hao Nie,Haonan Jia,Haoran Lv,Hebin Zhou,Hekun Lv,Heng Wang,Heung-Yeung Shum,Hongbo Huang,Hongbo Peng,Hongyu Zhou,Hongyuan Wang,Houyong Chen,Huangxi Zhu,Huimin Wu,Huiyong Guo,Jia Wang,Jian Zhou,Jianjian Sun,Jiaoren Wu,Jiaran Zhang,Jiashu Lv,Jiashuo Liu,Jiayi Fu,Jiayu Liu,Jie Cheng,Jie Luo,Jie Yang,Jie Zhou,Jieyi Hou,Jing Bai,Jingcheng Hu,Jingjing Xie,Jingwei Wu,Jingyang Zhang,Jishi Zhou,Junfeng Liu,Junzhe Lin,Ka Man Lo,Kai Liang,Kaibo Liu,Kaijun Tan,Kaiwen Yan,Kaixiang Li,Kang An,Kangheng Lin,Lei Yang,Liang Lv,Liang Zhao,Liangyu Chen,Lieyu Shi,Liguo Tan,Lin Lin,Lina Chen,Luck Ma,Mengqiang Ren,Michael Li,Ming Li,Mingliang Li,Mingming Zhang,Mingrui Chen,Mitt Huang,Na Wang,Peng Liu,Qi Han,Qian Zhao,Qinglin He,Qinxin Du,Qiuping Wu,Quan Sun,Rongqiu Yang,Ruihang Miao,Ruixin Han,Ruosi Wan,Ruyan Guo,Shan Wang,Shaoliang Pang,Shaowen Yang,Shengjie Fan,Shijie Shang,Shiliang Yang,Shiwei Li,Shuangshuang Tian,Siqi Liu,Siye Wu,Siyu Chen,Song Yuan,Tiancheng Cao,Tianchi Yue,Tianhao Cheng,Tianning Li,Tingdan Luo,Wang You,Wei Ji,Wei Yuan,Wei Zhang,Weibo Wu,Weihao Xie,Wen Sun,Wenjin Deng,Wenzhen Zheng,Wuxun Xie,Xiangfeng Wang,Xiangwen Kong,Xiangyu Liu,Xiangyu Zhang,Xiaobo Yang,Xiaojia Liu,Xiaolan Yuan,Xiaoran Jiao,Xiaoxiao Ren,Xiaoyun Zhang,Xin Li,Xin Liu,Xin Wu,Xing Chen,Xingping Yang,Xinran Wang,Xu Zhao,Xuan He,Xuanti Feng,Xuedan Cai,Xuqiang Zhou,Yanbo Yu,Yang Li,Yang Xu,Yanlin Lai,Yanming Xu,Yaoyu Wang,Yeqing Shen,Yibo Zhu,Yichen Lv,Yicheng Cao,Yifeng Gong,Yijing Yang,Yikun Yang,Yin Zhao,Yingxiu Zhao,Yinmin Zhang,Yitong Zhang,Yixuan Zhang,Yiyang Chen,Yongchi Zhao,Yongshen Long,Yongyao Wang,Yousong Guan,Yu Zhou,Yuang Peng,Yuanhao Ding,Yuantao Fan,Yuanzhen Yang,Yuchu Luo,Yudi Zhao,Yue Peng,Yueqiang Lin,Yufan Lu,Yuling Zhao,Yunzhou Ju,Yurong Zhang,Yusheng Li,Yuxiang Yang,Yuyang Chen,Yuzhu Cai,Zejia Weng,Zetao Hong,Zexi Li,Zhe Xie,Zheng Ge,Zheng Gong,Zheng Zeng,Zhenyi Lu,Zhewei Huang,Zhichao Chang,Zhiguo Huang,Zhiheng Hu,Zidong Yang,Zili Wang,Ziqi Ren,Zixin Zhang,Zixuan Wang*

Main category: cs.CL

TL;DR: Step 3.5 Flash是一种高效稀疏专家混合智能代理模型，兼顾推理精度和运行效率，表现优异，适合工业环境部署。


<details>
  <summary>Details</summary>
Motivation: 构建智能代理时，关键是保证精准的推理能力和快速可靠的执行，因此需要在模型规模和计算效率之间取得平衡。

Method: 采用稀疏专家混合模型（MoE）架构，结合1960亿参数的基础模型和110亿活跃参数，使用交错3:1滑动窗口/全注意力机制及多标记预测（MTP-3）优化推理效率，设计可扩展的强化学习框架，结合可验证信号和偏好反馈，在大规模离策略训练中保持稳定。

Result: 在IMO-AnswerBench、LiveCodeBench-v6、tau2-Bench、BrowseComp和Terminal-Bench 2.0等多项任务中取得优异成绩，表现与GPT-5.2 xHigh和Gemini 3.0 Pro等前沿模型相当，实现高密度且高效的模型部署。

Conclusion: Step 3.5 Flash通过提高推理能力和执行效率，在智能代理领域实现了前沿水平的性能，表现出强大的数学、编码和工具使用能力，与先进模型相当。

Abstract: We introduce Step 3.5 Flash, a sparse Mixture-of-Experts (MoE) model that bridges frontier-level agentic intelligence and computational efficiency. We focus on what matters most when building agents: sharp reasoning and fast, reliable execution. Step 3.5 Flash pairs a 196B-parameter foundation with 11B active parameters for efficient inference. It is optimized with interleaved 3:1 sliding-window/full attention and Multi-Token Prediction (MTP-3) to reduce the latency and cost of multi-round agentic interactions. To reach frontier-level intelligence, we design a scalable reinforcement learning framework that combines verifiable signals with preference feedback, while remaining stable under large-scale off-policy training, enabling consistent self-improvement across mathematics, code, and tool use. Step 3.5 Flash demonstrates strong performance across agent, coding, and math tasks, achieving 85.4% on IMO-AnswerBench, 86.4% on LiveCodeBench-v6 (2024.08-2025.05), 88.2% on tau2-Bench, 69.0% on BrowseComp (with context management), and 51.0% on Terminal-Bench 2.0, comparable to frontier models such as GPT-5.2 xHigh and Gemini 3.0 Pro. By redefining the efficiency frontier, Step 3.5 Flash provides a high-density foundation for deploying sophisticated agents in real-world industrial environments.

</details>


### [25] [Online Causal Kalman Filtering for Stable and Effective Policy Optimization](https://arxiv.org/abs/2602.10609)
*Shuo He,Lang Feng,Xin Cheng,Lei Feng,Bo An*

Main category: cs.CL

TL;DR: 本文提出了一种基于卡尔曼滤波的在线因果方法，用于平滑大语言模型强化学习中的重要性采样比率，提升了策略优化的稳定性和效果，实验验证了其在数学推理任务中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 当前使用固定或独立调整的IS比率方法忽视了代币间的时序依赖，导致局部偏差结构不一致，引发策略梯度更新失真和训练崩溃，需要更稳定有效的策略优化方法。

Method: 将重要性采样（IS）比率建模为跨代币演变的隐状态，利用卡尔曼滤波进行在线、自回归的状态更新，实现了平滑噪声尖峰且保留局部结构变化的IS比率调整。

Result: KPO方法在数学推理数据集上优于现有最先进方法，表现出更稳健的训练过程和更优的性能。

Conclusion: 提出的在线因果卡尔曼滤波（KPO）方法有效缓解了大语言模型强化学习中的策略优化不稳定问题，显著提升了策略更新的稳定性和效果，在数学推理等挑战性任务上取得了优异成绩。

Abstract: Reinforcement learning for large language models suffers from high-variance token-level importance sampling (IS) ratios, which would destabilize policy optimization at scale. To improve stability, recent methods typically use a fixed sequence-level IS ratio for all tokens in a sequence or adjust each token's IS ratio separately, thereby neglecting temporal off-policy derivation across tokens in a sequence. In this paper, we first empirically identify that local off-policy deviation is structurally inconsistent at the token level, which may distort policy-gradient updates across adjacent tokens and lead to training collapse. To address the issue, we propose Online Causal Kalman Filtering for stable and effective Policy Optimization (KPO). Concretely, we model the desired IS ratio as a latent state that evolves across tokens and apply a Kalman filter to update this state online and autoregressively based on the states of past tokens, regardless of future tokens. The resulting filtered IS ratios preserve token-wise local structure-aware variation while strongly smoothing noise spikes, yielding more stable and effective policy updates. Experimentally, KPO achieves superior results on challenging math reasoning datasets compared with state-of-the-art counterparts.

</details>


### [26] [How Do Decoder-Only LLMs Perceive Users? Rethinking Attention Masking for User Representation Learning](https://arxiv.org/abs/2602.10622)
*Jiahao Yuan,Yike Xu,Jinyong Wen,Baokun Wang,Yang Chen,Xiaotong Lin,Wuliang Huang,Ziyi Gao,Xing Fu,Yu Cheng,Weiqiang Wang*

Main category: cs.CL

TL;DR: 本文系统研究了解码器大模型中的注意力掩码设计，提出梯度引导软掩码预热提升双向掩码训练效果，在多个工业用户任务中显著提高了用户表示的质量和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有的解码器大型语言模型在用户表示学习中的注意力掩码设计及其对用户嵌入质量的影响尚未充分研究，亟需探索如何设计和训练更有效的注意力掩码。

Method: 在统一的对比学习框架下，系统性比较了因果、混合和双向注意力掩码。提出Gradient-Guided Soft Masking方法，在从因果到双向注意力掩码切换时，通过梯度引导的软掩码预热结合线性调度，逐步开放未来注意力。

Result: 在基于支付宝大规模异构用户行为数据的9个工业用户认知基准任务中，所提出的方法相较于因果、混合及仅使用调度器的基线，表现出更稳定的训练过程和更优的双向用户表示，同时保持与解码器预训练的兼容性。

Conclusion: 本文研究表明，在解码器结构的大型语言模型中，注意力掩码设计及训练过渡策略对用户表征学习的效果至关重要。通过提出基于梯度的软掩码预热方法，能够提升双向注意力掩码的训练稳定性和用户表示质量。

Abstract: Decoder-only large language models are increasingly used as behavioral encoders for user representation learning, yet the impact of attention masking on the quality of user embeddings remains underexplored. In this work, we conduct a systematic study of causal, hybrid, and bidirectional attention masks within a unified contrastive learning framework trained on large-scale real-world Alipay data that integrates long-horizon heterogeneous user behaviors. To improve training dynamics when transitioning from causal to bidirectional attention, we propose Gradient-Guided Soft Masking, a gradient-based pre-warmup applied before a linear scheduler that gradually opens future attention during optimization. Evaluated on 9 industrial user cognition benchmarks covering prediction, preference, and marketing sensitivity tasks, our approach consistently yields more stable training and higher-quality bidirectional representations compared with causal, hybrid, and scheduler-only baselines, while remaining compatible with decoder pretraining. Overall, our findings highlight the importance of masking design and training transition in adapting decoder-only LLMs for effective user representation learning. Our code is available at https://github.com/JhCircle/Deepfind-GGSM.

</details>


### [27] [UMEM: Unified Memory Extraction and Management Framework for Generalizable Memory](https://arxiv.org/abs/2602.10652)
*Yongshi Ye,Hui Jiang,Feihu Jiang,Tian Lan,Yichao Du,Biao Fu,Xiaodong Shi,Qianghuai Jia,Longyue Wang,Weihua Luo*

Main category: cs.CL

TL;DR: UMEM框架联合训练大语言模型同时进行记忆提取与管理，通过语义邻域优化提升记忆泛化能力，在多任务测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅优化记忆管理，记忆提取过程固定，导致模型过拟合具体实例，产生噪声而非稳健记忆，影响泛化性能。

Method: 提出UMEM框架，采用语义邻域建模和基于群体边际效用奖励的GRPO优化策略，同时训练大语言模型进行记忆提取和记忆管理。

Result: UMEM在五个基准测试中显著优于竞争性基线，在多轮互动任务中提升高达10.67%，且表现出持续的单调上升曲线。

Conclusion: UMEM框架通过统一优化记忆提取与管理，实现了记忆的自我演化，显著提升了LLM代理在多轮交互任务中的表现，且在持续演化过程中性能持续提升。

Abstract: Self-evolving memory serves as the trainable parameters for Large Language Models (LLMs)-based agents, where extraction (distilling insights from experience) and management (updating the memory bank) must be tightly coordinated. Existing methods predominately optimize memory management while treating memory extraction as a static process, resulting in poor generalization, where agents accumulate instance-specific noise rather than robust memories. To address this, we propose Unified Memory Extraction and Management (UMEM), a self-evolving agent framework that jointly optimizes a Large Language Model to simultaneous extract and manage memories. To mitigate overfitting to specific instances, we introduce Semantic Neighborhood Modeling and optimize the model with a neighborhood-level marginal utility reward via GRPO. This approach ensures memory generalizability by evaluating memory utility across clusters of semantically related queries. Extensive experiments across five benchmarks demonstrate that UMEM significantly outperforms highly competitive baselines, achieving up to a 10.67% improvement in multi-turn interactive tasks. Futhermore, UMEM maintains a monotonic growth curve during continuous evolution. Codes and models will be publicly released.

</details>


### [28] [Benchmarks Are Not That Out of Distribution: Word Overlap Predicts Performance](https://arxiv.org/abs/2602.10657)
*Woojin Chung,Jeonghoon Kim*

Main category: cs.CL

TL;DR: 基准测试性能主要由预训练数据与评估数据的词汇重叠驱动，简单词频统计能较好预测模型表现，表明许多基准测试与预训练语料分布差异较小。


<details>
  <summary>Details</summary>
Motivation: 探讨高质量预训练数据的定义，是否基准性能主要受预训练语料与测试数据之间的统计模式重叠驱动。

Method: 通过词级单字交叉熵和词频统计，结合10个零样本基准、4个预训练数据集（总计8.5B至60B标记）以及400M至3B参数的模型，进行控制实验和分析。

Result: 发现词级单字交叉熵与基准性能呈稳健的负相关，大规模包含类似词汇分布的预训练子集能够提升下游任务性能，词频统计对基准成绩有额外影响。

Conclusion: 许多标准基准测试的性能主要受预训练语料与评估数据之间的词汇重叠程度影响，简单的词汇重叠统计能够有效预测基准性能。

Abstract: Understanding what constitutes high-quality pre-training data remains a central question in language model training. In this work, we investigate whether benchmark performance is primarily driven by the degree of statistical pattern overlap between pre-training corpora and evaluation datasets. We measure this overlap using word-level unigram cross-entropy and word frequency statistics, and perform controlled experiments across $10$ zero-shot benchmarks, $4$ pre-training datasets spanning $8.5\mathrm{B}$ to $60\mathrm{B}$ tokens, and model sizes ranging from $400\mathrm{M}$ to $3\mathrm{B}$ parameters. Our results demonstrate a robust inverse relationship between word-level unigram cross-entropy and benchmark performance, suggesting that widely used benchmarks are strongly influenced by word overlap between training and evaluation data. Thus, larger pre-training subsets with similar word-level unigram cross-entropy yield improved downstream results, indicating that word frequency statistics play an additional role in shaping benchmark scores. Taken together, these results suggest that many standard benchmarks are only weakly out-of-distribution relative to pre-training corpora, so that simple word-overlap statistics predict benchmark performance.

</details>


### [29] [Targeted Syntactic Evaluation of Language Models on Georgian Case Alignment](https://arxiv.org/abs/2602.10661)
*Daniel Gallagher,Gerhard Heyer*

Main category: cs.CL

TL;DR: 本文通过构建基于树库的测试数据集，评估Transformer模型在格鲁吉亚语施事格标注的能力，发现模型在施事格上的表现较差，归因于其罕见性和数据稀缺。


<details>
  <summary>Details</summary>
Motivation: 研究基于Transformer的语言模型在处理格鲁吉亚语这种罕见的分裂施事格系统中语法格分配的能力，尤其关注主语和宾语的格标记。

Method: 采用基于树库的方法，利用Grew查询语言生成最小对，构建包含370个句法测试的七个任务的数据集。评估了五个编码器和两个仅解码器模型，使用词级和句子级准确性指标。

Result: 模型整体表现受已有训练数据量限制，施事格分配准确率最低，主格最高，结果体现了格频率分布对模型性能的影响，证明数据稀缺和格语法作用导致性能不足。

Conclusion: 基于Transformer的语言模型在处理格鲁吉亚语的分裂施事格对齐时，表现最差的是施事格，表现最好的是主格，其性能与名词格的频率分布相关。

Abstract: This paper evaluates the performance of transformer-based language models on split-ergative case alignment in Georgian, a particularly rare system for assigning grammatical cases to mark argument roles. We focus on subject and object marking determined through various permutations of nominative, ergative, and dative noun forms. A treebank-based approach for the generation of minimal pairs using the Grew query language is implemented. We create a dataset of 370 syntactic tests made up of seven tasks containing 50-70 samples each, where three noun forms are tested in any given sample. Five encoder- and two decoder-only models are evaluated with word- and/or sentence-level accuracy metrics. Regardless of the specific syntactic makeup, models performed worst in assigning the ergative case correctly and strongest in assigning the nominative case correctly. Performance correlated with the overall frequency distribution of the three forms (NOM > DAT > ERG). Though data scarcity is a known issue for low-resource languages, we show that the highly specific role of the ergative along with a lack of available training data likely contributes to poor performance on this case. The dataset is made publicly available and the methodology provides an interesting avenue for future syntactic evaluations of languages where benchmarks are limited.

</details>


### [30] [Locomo-Plus: Beyond-Factual Cognitive Memory Evaluation Framework for LLM Agents](https://arxiv.org/abs/2602.10715)
*Yifei Li,Weidong Guo,Lingling Zhang,Rongman Xu,Muye Huang,Hui Liu,Lijiao Xu,Yu Xu,Jun Liu*

Main category: cs.CL

TL;DR: 提出LoCoMo-Plus基准和评估框架，针对长时对话中的隐含认知约束记忆进行更全面评测，弥补了现有评测只关注显式事实记忆的不足。


<details>
  <summary>Details</summary>
Motivation: 真实对话中，回复依赖用户状态、目标等隐含约束，现有评测忽视此类认知记忆能力，导致对模型认知记忆的真实能力评估不足。

Method: 提出LoCoMo-Plus基准，设计基于约束一致性的统一评估框架，结合多种主干模型、检索和记忆系统进行实验评测。

Result: 实验结果表明认知记忆任务依然具有挑战性，现有字符串匹配指标和任务提示方法在此场景下表现不佳，揭示了错误未被现有基准捕捉。

Conclusion: 当前的长时对话记忆评测主要关注表层事实回忆，忽略了隐含约束的记忆和应用。LoCoMo-Plus基准测试能有效评估模型在语义断裂情况下的认知记忆，揭示了现有方法的不足。

Abstract: Long-term conversational memory is a core capability for LLM-based dialogue systems, yet existing benchmarks and evaluation protocols primarily focus on surface-level factual recall. In realistic interactions, appropriate responses often depend on implicit constraints such as user state, goals, or values that are not explicitly queried later. To evaluate this setting, we introduce \textbf{LoCoMo-Plus}, a benchmark for assessing cognitive memory under cue--trigger semantic disconnect, where models must retain and apply latent constraints across long conversational contexts. We further show that conventional string-matching metrics and explicit task-type prompting are misaligned with such scenarios, and propose a unified evaluation framework based on constraint consistency. Experiments across diverse backbone models, retrieval-based methods, and memory systems demonstrate that cognitive memory remains challenging and reveals failures not captured by existing benchmarks. Our code and evaluation framework are publicly available at: https://github.com/xjtuleeyf/Locomo-Plus.

</details>


### [31] [Macaron: Controlled, Human-Written Benchmark for Multilingual and Multicultural Reasoning via Template-Filling](https://arxiv.org/abs/2602.10732)
*Alaa Elsetohy,Sama Hadhoud,Haryo Akbarianto Wibowo,Chenxi Whitehouse,Genta Indra Winata,Fajri Koto,Alham Fikri Aji*

Main category: cs.CL

TL;DR: Macaron基准结合推理类型与文化因素，创建多语言多文化推理测试集，揭示多语言模型本地语言推理性能差异，特别是文化相关推理任务难度大。


<details>
  <summary>Details</summary>
Motivation: 现有多语言基准测试缺乏对文化背景推理的有效评估，翻译数据集偏重英语场景，而文化优先数据集缺乏对推理类型的控制，为此需要一个同时考虑推理类型和文化因素的多语言推理基准。

Method: 通过设计100个语言无关的模板，涵盖7种推理类型和22个文化方面，利用母语注释员生成与场景相符的英语及本地语言多选题和系统派生的真伪题，构建了包含20种语言和文化情境的Macaron数据集。

Result: Macaron包含11,862个样本，覆盖20个国家／文化背景和10种书写系统。对21个多语言LLM进行零样本评估，发现推理模式模型表现优异，开源模型在本地语言上性能大幅下降，真伪题任务成绩接近随机。特别是依赖文化背景的数学和计数模板最具挑战。

Conclusion: Macaron基准测试表明，多语言大模型（LLMs）在本地语言上的表现受到文化背景的显著影响，尤其是在涉及文化基础的数学和计数推理任务中表现最差。采用推理模式的模型在多语言任务中表现最佳，且英语与本地语言之间表现接近一致。

Abstract: Multilingual benchmarks rarely test reasoning over culturally grounded premises: translated datasets keep English-centric scenarios, while culture-first datasets often lack control over the reasoning required. We propose Macaron, a template-first benchmark that factorizes reasoning type and cultural aspect across question languages. Using 100 language-agnostic templates that cover 7 reasoning types, 22 cultural aspects, native annotators create scenario-aligned English and local-language multiple-choice questions and systematically derived True/False questions. Macaron contains 11,862 instances spanning 20 countries/cultural contexts, 10 scripts, and 20 languages (including low-resource ones like Amharic, Yoruba, Zulu, Kyrgyz, and some Arabic dialects). In zero-shot evaluation of 21 multilingual LLMs, reasoning-mode models achieve the strongest performance and near-parity between English and local languages, while open-weight models degrade substantially in local languages and often approach chance on T/F tasks. Culture-grounded mathematical and counting templates are consistently the hardest. The data can be accessed here https://huggingface.co/datasets/AlaaAhmed2444/Macaron.

</details>


### [32] [Reinforced Curriculum Pre-Alignment for Domain-Adaptive VLMs](https://arxiv.org/abs/2602.10740)
*Yuming Yan,Shuo Yang,Kai Tang,Sihong Chen,Yang Zhang,Ke Xu,Dan Hu,Qun Yu,Pengfei Hu,Edith C. H. Ngai*

Main category: cs.CL

TL;DR: 针对视觉-语言模型领域适应难题，本文提出了一种强化学习驱动的渐进式训练方法RCPA，成功实现了领域知识的安全引入和模型能力的平衡提升。


<details>
  <summary>Details</summary>
Motivation: 解决视觉-语言模型在特定领域Fine-Tuning时出现的灾难性遗忘和强化学习方法在领域适应中的优化崩溃问题，提升模型的领域适应性同时保持通用能力。

Method: 引入了一种基于强化学习的课程感知渐进调节机制，初期对输出施加部分约束，逐步过渡到完整生成优化，确保模型安全适应新领域。

Result: 在多个专业领域和通用基准测试中，RCPA表现优异，有效保持模型通用能力的同时显著提升对特定领域的适应性能。

Conclusion: 本文提出的RCPA方法有效解决了视觉-语言模型在特定领域适应过程中遗忘通用能力的问题，实现了领域知识的逐步习得与多模态能力的平衡。

Abstract: Vision-Language Models (VLMs) demonstrate remarkable general-purpose capabilities but often fall short in specialized domains such as medical imaging or geometric problem-solving. Supervised Fine-Tuning (SFT) can enhance performance within a target domain, but it typically causes catastrophic forgetting, limiting its generalization. The central challenge, therefore, is to adapt VLMs to new domains while preserving their general-purpose capabilities. Continual pretraining is effective for expanding knowledge in Large Language Models (LLMs), but it is less feasible for VLMs due to prohibitive computational costs and the unavailability of pretraining data for most open-source models. This necessitates efficient post-training adaptation methods. Reinforcement learning (RL)-based approaches such as Group Relative Policy Optimization (GRPO) have shown promise in preserving general abilities, yet they often fail in domain adaptation scenarios where the model initially lacks sufficient domain knowledge, leading to optimization collapse. To bridge this gap, we propose Reinforced Curriculum Pre-Alignment (RCPA), a novel post-training paradigm that introduces a curriculum-aware progressive modulation mechanism. In the early phase, RCPA applies partial output constraints to safely expose the model to new domain concepts. As the model's domain familiarity increases, training gradually transitions to full generation optimization, refining responses and aligning them with domain-specific preferences. This staged adaptation balances domain knowledge acquisition with the preservation of general multimodal capabilities. Extensive experiments across specialized domains and general benchmarks validate the effectiveness of RCPA, establishing a practical pathway toward building high-performing and domain-adaptive VLMs.

</details>


### [33] [Deep Learning-based Method for Expressing Knowledge Boundary of Black-Box LLM](https://arxiv.org/abs/2602.10801)
*Haotian Sheng,Heyong Wang,Ming Hong,Hongman He,Junqiu Liu*

Main category: cs.CL

TL;DR: 本文提出LSCL方法，利用深度学习量化并表达黑盒大语言模型的知识边界，有效减少幻觉问题，显著提高知识表达准确性，且适用多种黑盒模型。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成内容失真（幻觉）问题的根本原因是其对内部知识边界的认知缺失，现有研究多针对白盒LLM，缺乏适合只提供API访问的黑盒LLM的知识边界表达方法。

Method: 基于知识蒸馏框架，设计了一个深度学习模型，以黑盒LLM的输入问题、输出答案及token概率作为输入，映射其内部知识状态，实现知识边界的表达和量化；并提出一种适用于不支持token概率访问的LLM的替代方法。

Result: 实验表明LSCL在多个公开数据集和主流黑盒LLM上表现出色，在准确率和召回率等指标上显著优于基线模型，且替代方法在无token概率访问的场景中性能接近LSCL且优于基线。

Conclusion: 本文提出的LSCL方法有效地帮助黑盒大语言模型准确表达其知识边界，显著优于现有基线模型，提升了模型在知识表达上的可靠性。

Abstract: Large Language Models (LLMs) have achieved remarkable success, however, the emergence of content generation distortion (hallucination) limits their practical applications. The core cause of hallucination lies in LLMs' lack of awareness regarding their stored internal knowledge, preventing them from expressing their knowledge state on questions beyond their internal knowledge boundaries, as humans do. However, existing research on knowledge boundary expression primarily focuses on white-box LLMs, leaving methods suitable for black-box LLMs which offer only API access without revealing internal parameters-largely unexplored. Against this backdrop, this paper proposes LSCL (LLM-Supervised Confidence Learning), a deep learning-based method for expressing the knowledge boundaries of black-box LLMs. Based on the knowledge distillation framework, this method designs a deep learning model. Taking the input question, output answer, and token probability from a black-box LLM as inputs, it constructs a mapping between the inputs and the model' internal knowledge state, enabling the quantification and expression of the black-box LLM' knowledge boundaries. Experiments conducted on diverse public datasets and with multiple prominent black-box LLMs demonstrate that LSCL effectively assists black-box LLMs in accurately expressing their knowledge boundaries. It significantly outperforms existing baseline models on metrics such as accuracy and recall rate. Furthermore, considering scenarios where some black-box LLMs do not support access to token probability, an adaptive alternative method is proposed. The performance of this alternative approach is close to that of LSCL and surpasses baseline models.

</details>


### [34] [Beyond Confidence: The Rhythms of Reasoning in Generative Models](https://arxiv.org/abs/2602.10816)
*Deyuan Liu,Zecheng Wang,Zhanyue Qin,Zhiying Tu,Dianhui Chu,Dianbo Sui*

Main category: cs.CL

TL;DR: 本文针对大语言模型输入扰动敏感的问题，提出了新指标$δ_{\mathrm{TCB}}$，量化内部预测状态的稳定性，优于传统困惑度指标，促进了模型鲁棒性分析和改进。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型具备强大能力，但对输入上下文的轻微变动极其敏感，传统指标如准确率和困惑度无法反映局部预测的鲁棒性，难以揭示模型内在状态对扰动的抵抗能力。

Method: 引入了Token Constraint Bound ($δ_{\mathrm{TCB}}$)这一新指标，通过度量LLM内部状态在主导下一个token预测显著变化前能承受的最大扰动，结合输出嵌入空间几何特性，量化模型的预测稳定性。

Result: 实验表明，$δ_{\mathrm{TCB}}$与有效的提示工程相关，并能够发现传统困惑度指标忽视的关键预测不稳定性，提升了对上下文学习和文本生成过程中模型稳定性的评估。

Conclusion: 本文提出的Token Constraint Bound ($δ_{\mathrm{TCB}}$)指标，有效评估了大语言模型（LLMs）内在状态对输入扰动的鲁棒性，揭示了传统指标难以发现的预测不稳定性，从而促进对模型稳定性的深入理解和改进。

Abstract: Large Language Models (LLMs) exhibit impressive capabilities yet suffer from sensitivity to slight input context variations, hampering reliability. Conventional metrics like accuracy and perplexity fail to assess local prediction robustness, as normalized output probabilities can obscure the underlying resilience of an LLM's internal state to perturbations. We introduce the Token Constraint Bound ($δ_{\mathrm{TCB}}$), a novel metric that quantifies the maximum internal state perturbation an LLM can withstand before its dominant next-token prediction significantly changes. Intrinsically linked to output embedding space geometry, $δ_{\mathrm{TCB}}$ provides insights into the stability of the model's internal predictive commitment. Our experiments show $δ_{\mathrm{TCB}}$ correlates with effective prompt engineering and uncovers critical prediction instabilities missed by perplexity during in-context learning and text generation. $δ_{\mathrm{TCB}}$ offers a principled, complementary approach to analyze and potentially improve the contextual stability of LLM predictions.

</details>


### [35] [I can tell whether you are a Native Hawlêri Speaker! How ANN, CNN, and RNN perform in NLI-Native Language Identification](https://arxiv.org/abs/2602.10832)
*Hardi Garari,Hossein Hassani*

Main category: cs.CL

TL;DR: 本研究构建了首个针对库尔德语Hewlêri子方言的语音母语识别数据集，利用神经网络模型实现95.92%的高准确率，弥补了少资源方言母语识别的研究空白。


<details>
  <summary>Details</summary>
Motivation: 现有有关不同语言的母语识别研究较多，但针对方言和亚方言，尤其是资源较少的库尔德语，研究较少，存在明显空白。

Method: 收集了40名母语或非母语者约24小时的Hewlêri子方言语音数据，构建并比较了ANN、CNN和RNN三种神经网络模型，通过66次实验验证，包括不同时间段音频分割和采样策略。

Result: RNN模型在5秒音频分割下表现最佳，准确率达到95.92%。所创建的数据集为Sorani库尔德语Hewlêri子方言的首个语音母语识别数据集，具有重要研究价值。

Conclusion: 该研究成功填补了针对库尔德语Hewlêri子方言母语识别的研究空白，表明基于神经网络的模型尤其是RNN在该任务中具有较高的准确率。

Abstract: Native Language Identification (NLI) is a task in Natural Language Processing (NLP) that typically determines the native language of an author through their writing or a speaker through their speaking. It has various applications in different areas, such as forensic linguistics and general linguistics studies. Although considerable research has been conducted on NLI regarding two different languages, such as English and German, the literature indicates a significant gap regarding NLI for dialects and subdialects. The gap becomes wider in less-resourced languages such as Kurdish. This research focuses on NLI within the context of a subdialect of Sorani (Central) Kurdish. It aims to investigate the NLI for Hewlêri, a subdialect spoken in Hewlêr (Erbil), the Capital of the Kurdistan Region of Iraq. We collected about 24 hours of speech by recording interviews with 40 native or non-native Hewlêri speakers, 17 female and 23 male. We created three Neural Network-based models: Artificial Neural Network (ANN), Convolutional Neural Network (CNN), and Recurrent Neural Network (RNN), which were evaluated through 66 experiments, covering various time-frames from 1 to 60 seconds, undersampling, oversampling, and cross-validation. The RNN model showed the highest accuracy of 95.92% for 5-second audio segmentation, using an 80:10:10 data splitting scheme. The created dataset is the first speech dataset for NLI on the Hewlêri subdialect in the Sorani Kurdish dialect, which can be of benefit to various research areas.

</details>


### [36] [C-MOP: Integrating Momentum and Boundary-Aware Clustering for Enhanced Prompt Evolution](https://arxiv.org/abs/2602.10874)
*Binwei Yan,Yifei Fu,Mingjian Zhu,Hanting Chen,Mingxuan Yuan,Yunhe Wang,Hailin Hu*

Main category: cs.CL

TL;DR: 本文提出了C-MOP，一种通过边界感知对比采样和动量语义聚类稳定自动提示优化的方法，实验验证其显著超越多种先进基线，提升大模型提示效果。


<details>
  <summary>Details</summary>
Motivation: 解决当前自动提示优化过程中存在的更新信号噪声和冲突问题，提高优化稳定性和提示性能。

Method: 提出了C-MOP框架，包含边界感知对比采样（BACS）利用批次级信息挖掘三类特征，用于刻画正负提示样本的表现和边界，及动量引导语义聚类（MGSC）通过带时间衰减的文本动量机制消除语义冲突。

Result: 在多项实验中，C-MOP均超过了PromptWizard和ProTeGi等最先进基线，平均性能提升1.58%和3.35%，并实现了用参数较少的通用大模型超过大型领域专用模型。

Conclusion: C-MOP方法通过边界感知对比采样和动量引导的语义聚类，有效稳定自动提示优化过程，提升了大模型提示性能，且能用3B参数模型超过70B领域专用模型的表现。

Abstract: Automatic prompt optimization is a promising direction to boost the performance of Large Language Models (LLMs). However, existing methods often suffer from noisy and conflicting update signals. In this research, we propose C-MOP (Cluster-based Momentum Optimized Prompting), a framework that stabilizes optimization via Boundary-Aware Contrastive Sampling (BACS) and Momentum-Guided Semantic Clustering (MGSC). Specifically, BACS utilizes batch-level information to mine tripartite features--Hard Negatives, Anchors, and Boundary Pairs--to precisely characterize the typical representation and decision boundaries of positive and negative prompt samples. To resolve semantic conflicts, MGSC introduces a textual momentum mechanism with temporal decay that distills persistent consensus from fluctuating gradients across iterations. Extensive experiments demonstrate that C-MOP consistently outperforms SOTA baselines like PromptWizard and ProTeGi, yielding average gains of 1.58% and 3.35%. Notably, C-MOP enables a general LLM with 3B activated parameters to surpass a 70B domain-specific dense LLM, highlighting its effectiveness in driving precise prompt evolution. The code is available at https://github.com/huawei-noah/noah-research/tree/master/C-MOP.

</details>


### [37] [Diagnosing Structural Failures in LLM-Based Evidence Extraction for Meta-Analysis](https://arxiv.org/abs/2602.10881)
*Zhiyin Tan,Jennifer D'Souza*

Main category: cs.CL

TL;DR: 大型语言模型在自动化科学文献元分析中结构性绑定和数值归属能力不足，限制了其在复杂结构提取任务中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 系统综述和元分析需要将叙述性文章转化为结构化、数值化的研究记录，当前尚不清楚大型语言模型能否满足这种复杂结构的需求。

Method: 提出了一种结构化的诊断框架，通过具有不同关系和数值复杂度的模式约束查询，结合五个科学领域的手工整理语料，统一的查询套件和评估协议，评估了两种先进大型语言模型在单文档和多文档长上下文模式下的表现。

Result: 单一属性查询的性能适中，但一旦任务涉及变量、角色、统计方法和效应量的稳定绑定，性能急剧下降。全量元分析关联元组提取几乎不可靠，长上下文输入加剧了失败，下游聚合过程放大了上游的微小错误，导致统计数据不可靠。

Conclusion: 当前大型语言模型在自动化结构化元分析方面存在显著局限，特别是在保持变量、角色、统计方法和效应量之间的稳定绑定及数值归属方面。

Abstract: Systematic reviews and meta-analyses rely on converting narrative articles into structured, numerically grounded study records. Despite rapid advances in large language models (LLMs), it remains unclear whether they can meet the structural requirements of this process, which hinge on preserving roles, methods, and effect-size attribution across documents rather than on recognizing isolated entities. We propose a structural, diagnostic framework that evaluates LLM-based evidence extraction as a progression of schema-constrained queries with increasing relational and numerical complexity, enabling precise identification of failure points beyond atom-level extraction. Using a manually curated corpus spanning five scientific domains, together with a unified query suite and evaluation protocol, we evaluate two state-of-the-art LLMs under both per-document and long-context, multi-document input regimes. Across domains and models, performance remains moderate for single-property queries but degrades sharply once tasks require stable binding between variables, roles, statistical methods, and effect sizes. Full meta-analytic association tuples are extracted with near-zero reliability, and long-context inputs further exacerbate these failures. Downstream aggregation amplifies even minor upstream errors, rendering corpus-level statistics unreliable. Our analysis shows that these limitations stem not from entity recognition errors, but from systematic structural breakdowns, including role reversals, cross-analysis binding drift, instance compression in dense result sections, and numeric misattribution, indicating that current LLMs lack the structural fidelity, relational binding, and numerical grounding required for automated meta-analysis. The code and data are publicly available at GitHub (https://github.com/zhiyintan/LLM-Meta-Analysis).

</details>


### [38] [The CLEF-2026 FinMMEval Lab: Multilingual and Multimodal Evaluation of Financial AI Systems](https://arxiv.org/abs/2602.10886)
*Zhuohan Xie,Rania Elbadry,Fan Zhang,Georgi Georgiev,Xueqing Peng,Lingfei Qian,Jimin Huang,Dimitar Dimitrov,Vanshikaa Jani,Yuyang Dai,Jiahui Geng,Yuxia Wang,Ivan Koychev,Veselin Stoyanov,Preslav Nakov*

Main category: cs.CL

TL;DR: FinMMEval 2026首次提出多语言多模态金融大模型评测框架，包含三大任务，推动全球金融AI系统发展。


<details>
  <summary>Details</summary>
Motivation: 当前金融自然语言处理技术多为单语、文本单模态，任务范围有限，缺乏多语言多模态的综合评估框架。

Method: 设计了三个相互关联的任务：金融考试问答、多语言金融问答（PolyFiQA）和金融决策，涵盖金融理解、推理和决策。

Result: 提供了一个全面的评估工具套件，能测试模型在多语言、多模态下的推理、泛化和决策能力，促进金融AI系统的发展。

Conclusion: FinMMEval 2026填补了金融领域多语言多模态评估的空白，公开数据和评测资源，推动可复现研究和全球包容性发展。

Abstract: We present the setup and the tasks of the FinMMEval Lab at CLEF 2026, which introduces the first multilingual and multimodal evaluation framework for financial Large Language Models (LLMs). While recent advances in financial natural language processing have enabled automated analysis of market reports, regulatory documents, and investor communications, existing benchmarks remain largely monolingual, text-only, and limited to narrow subtasks. FinMMEval 2026 addresses this gap by offering three interconnected tasks that span financial understanding, reasoning, and decision-making: Financial Exam Question Answering, Multilingual Financial Question Answering (PolyFiQA), and Financial Decision Making. Together, these tasks provide a comprehensive evaluation suite that measures models' ability to reason, generalize, and act across diverse languages and modalities. The lab aims to promote the development of robust, transparent, and globally inclusive financial AI systems, with datasets and evaluation resources publicly released to support reproducible research.

</details>


### [39] [SoftMatcha 2: A Fast and Soft Pattern Matcher for Trillion-Scale Corpora](https://arxiv.org/abs/2602.10908)
*Masataka Yoneda,Yusuke Matsushita,Go Kamoda,Kohei Suenaga,Takuya Akiba,Masaki Waga,Sho Yokoi*

Main category: cs.CL

TL;DR: 本文提出一种基于后缀数组的极快语义鲁棒搜索算法，实现了万亿级语料库下毫秒级搜索，性能超过现有最先进方法，并能检测训练数据污染。


<details>
  <summary>Details</summary>
Motivation: 传统搜索方法在面对超大规模语料库及语义放宽查询时面临组合爆炸及高延迟问题，迫切需要一种快速且灵活的搜索算法。

Method: 提出基于后缀数组的字符串匹配算法，结合磁盘感知设计的快速精确查找和动态的语料库感知剪枝技术，抑制查询长度导致的搜索空间指数增长。

Result: 在1.4万亿词令的FineWeb-Edu数据集上，方法显著降低搜索延迟，性能优于infini-gram、infini-gram mini及SoftMatcha等最新方法，并成功检测训练语料的基准污染。

Conclusion: 本论文提出的基于后缀数组的算法，实现了在万亿级自然语言语料库上低于0.3秒的超高速搜索，有效处理了语义变体，显著优于现有方法。

Abstract: We present an ultra-fast and flexible search algorithm that enables search over trillion-scale natural language corpora in under 0.3 seconds while handling semantic variations (substitution, insertion, and deletion). Our approach employs string matching based on suffix arrays that scales well with corpus size. To mitigate the combinatorial explosion induced by the semantic relaxation of queries, our method is built on two key algorithmic ideas: fast exact lookup enabled by a disk-aware design, and dynamic corpus-aware pruning. We theoretically show that the proposed method suppresses exponential growth in the search space with respect to query length by leveraging statistical properties of natural language. In experiments on FineWeb-Edu (Lozhkov et al., 2024) (1.4T tokens), we show that our method achieves significantly lower search latency than existing methods: infini-gram (Liu et al., 2024), infini-gram mini (Xu et al., 2025), and SoftMatcha (Deguchi et al., 2025). As a practical application, we demonstrate that our method identifies benchmark contamination in training corpora, unidentified by existing approaches. We also provide an online demo of fast, soft search across corpora in seven languages.

</details>


### [40] [Computational Phenomenology of Temporal Experience in Autism: Quantifying the Emotional and Narrative Characteristics of Lived Unpredictability](https://arxiv.org/abs/2602.10947)
*Kacper Dudzic,Karolina Drożdż,Maciej Wodziński,Anastazja Szuła,Marcin Moskalewicz*

Main category: cs.CL

TL;DR: 本研究结合现象学访谈和计算分析，发现自闭症个体体验中的时间不可预测性是其核心问题，且其叙事真实反映了这类体验，而非叙事结构缺陷。


<details>
  <summary>Details</summary>
Motivation: 当前关于自闭症时间性障碍的研究存在医疗缺陷模型主导、定性研究样本量小及缺乏现象学和计算研究结合等局限，故本研究旨在弥合现象学与计算方法的差距并解决样本量问题。

Method: 研究整合了三种方法：A）使用横断性时间体验评估对自闭症个体进行结构化现象学访谈；B）对自闭症自传叙事语料库进行计算分析；C）使用叙事流量测度进行计算复制研究以评估自闭症自传的现象学真实性。

Result: 访谈显示自闭症组与对照组在体验不可预测性方面存在显著差异，计算分析发现自闭症叙事中时间词汇带有更多负面情感，且与感知的断续性词汇相关。叙事流分析证实自闭症叙事更接近真实自传而非虚构故事。

Conclusion: 自闭症个体在时间性上的核心挑战是其生活体验中的不可预测性，这种不可预测性深刻影响了其社会关系。

Abstract: Disturbances in temporality, such as desynchronization with the social environment and its unpredictability, are considered core features of autism with a deep impact on relationships. However, limitations regarding research on this issue include: 1) the dominance of deficit-based medical models of autism, 2) sample size in qualitative research, and 3) the lack of phenomenological anchoring in computational research. To bridge the gap between phenomenological and computational approaches and overcome sample-size limitations, our research integrated three methodologies. Study A: structured phenomenological interviews with autistic individuals using the Transdiagnostic Assessment of Temporal Experience. Study B: computational analysis of an autobiographical corpus of autistic narratives built for this purpose. Study C: a replication of a computational study using narrative flow measures to assess the perceived phenomenological authenticity of autistic autobiographies. Interviews revealed that the most significant differences between the autistic and control groups concerned unpredictability of experience. Computational results mirrored these findings: the temporal lexicon in autistic narratives was significantly more negatively valenced - particularly the "Immediacy & Suddenness" category. Outlier analysis identified terms associated with perceived discontinuity (unpredictably, precipitously, and abruptly) as highly negative. The computational analysis of narrative flow found that the autistic narratives contained within the corpus quantifiably resemble autobiographical stories more than imaginary ones. Overall, the temporal challenges experienced by autistic individuals were shown to primarily concern lived unpredictability and stem from the contents of lived experience, and not from autistic narrative construction.

</details>


### [41] [Search or Accelerate: Confidence-Switched Position Beam Search for Diffusion Language Models](https://arxiv.org/abs/2602.10953)
*Mingyu Cao,Alvaro Correia,Christos Louizos,Shiwei Liu,Lu Yin*

Main category: cs.CL

TL;DR: SOAR是一种无需训练的扩散语言模型解码算法，通过根据模型置信度调整解码策略，显著提升了生成质量并保持高效推理。


<details>
  <summary>Details</summary>
Motivation: 传统扩散语言模型的贪婪解码策略容易导致次优的解码顺序，特别是在需要复杂推理的任务中，迫切需要一种平衡质量与效率的解码方法。

Method: 提出了一种无需训练的解码算法SOAR，该算法依据模型置信度动态调整解码策略，在置信度低时扩大搜索范围避免过早决策，在置信度高时并行解码多个位置以降低迭代次数。

Result: 在Dream-7B和LLaDA-8B模型上，SOAR在GSM8K、MBPP、HumanEval等数学推理和代码生成基准测试中提升了生成质量，且保持了竞争力的推理速度。

Conclusion: SOAR算法通过自适应不确定性调整解码行为，有效提高了扩散语言模型在数学推理和代码生成任务上的生成质量，同时保持了有竞争力的推理速度。

Abstract: Diffusion Language Models (DLMs) generate text by iteratively denoising a masked sequence, repeatedly deciding which positions to commit at each step. Standard decoding follows a greedy rule: unmask the most confident positions, yet this local choice can lock the model into a suboptimal unmasking order, especially on reasoning-heavy prompts. We present SOAR, a training-free decoding algorithm that adapts its behavior to the model's uncertainty. When confidence is low, SOAR briefly widens the search over alternative unmasking decisions to avoid premature commitments; when confidence is high, it collapses the search and decodes many positions in parallel to reduce the number of denoising iterations. Across mathematical reasoning and code generation benchmarks (GSM8K, MBPP, HumanEval) on Dream-7B and LLaDA-8B, SOAR improves generation quality while maintaining competitive inference speed, offering a practical way to balance quality and efficiency in DLM decoding.

</details>


### [42] [LoRA-Squeeze: Simple and Effective Post-Tuning and In-Tuning Compression of LoRA Modules](https://arxiv.org/abs/2602.10993)
*Ivan Vulić,Adam Grycner,Quentin de Laroussilhe,Jonas Pfeiffer*

Main category: cs.CL

TL;DR: LoRA-Squeeze通过先学习高秩表示再压缩，解决了LoRA秩预设和部署难题，显著提升了微调效果和参数效率。


<details>
  <summary>Details</summary>
Motivation: 标准LoRA面临秩和相关超参数预设困难，以及不同秩模块部署复杂问题，迫切需要更灵活高效的秩调整方法。

Method: 采用先高秩微调模型，再利用随机奇异值分解(RSVD)对权重更新矩阵进行压缩，动态调整LoRA模块的秩，支持后期压缩和训练中逐步降秩。

Result: 在13个文本和10个视觉语言任务中，LoRA-Squeeze的后期压缩生成的低秩适配器表现优于直接训练的低秩模型，逐步降秩变体性能最好。

Conclusion: LoRA-Squeeze通过先学习高秩解再压缩，比直接学习低秩解效果更好，能显著提升LoRA的性能与部署效率。

Abstract: Despite its huge number of variants, standard Low-Rank Adaptation (LoRA) is still a dominant technique for parameter-efficient fine-tuning (PEFT). Nonetheless, it faces persistent challenges, including the pre-selection of an optimal rank and rank-specific hyper-parameters, as well as the deployment complexity of heterogeneous-rank modules and more sophisticated LoRA derivatives. In this work, we introduce LoRA-Squeeze, a simple and efficient methodology that aims to improve standard LoRA learning by changing LoRA module ranks either post-hoc or dynamically during training}. Our approach posits that it is better to first learn an expressive, higher-rank solution and then compress it, rather than learning a constrained, low-rank solution directly. The method involves fine-tuning with a deliberately high(er) source rank, reconstructing or efficiently approximating the reconstruction of the full weight update matrix, and then using Randomized Singular Value Decomposition (RSVD) to create a new, compressed LoRA module at a lower target rank. Extensive experiments across 13 text and 10 vision-language tasks show that post-hoc compression often produces lower-rank adapters that outperform those trained directly at the target rank, especially if a small number of fine-tuning steps at the target rank is allowed. Moreover, a gradual, in-tuning rank annealing variant of LoRA-Squeeze consistently achieves the best LoRA size-performance trade-off.

</details>


### [43] [Linguistic Indicators of Early Cognitive Decline in the DementiaBank Pitt Corpus: A Statistical and Machine Learning Study](https://arxiv.org/abs/2602.11028)
*Artsvik Avetisyan,Sachin Kumar*

Main category: cs.CL

TL;DR: 本研究通过分析自发语言的多种语言学特征，结合可解释的机器学习方法和统计验证，发现可靠且可解释的语言标志，用于早期认知衰退的语言筛查。


<details>
  <summary>Details</summary>
Motivation: 细微的自发语言变化是认知衰退的早期指标，识别具有语言学可解释性的痴呆标志有助于实现透明且临床基础的认知筛查方法。

Method: 使用DementiaBank Pitt语料库的自发语音转录文本，采用原始文本、结合词汇和语法信息的词性增强表示、及仅含词性的句法表示，利用逻辑回归和随机森林模型，在转录本层面和受试者层面交叉验证中评估模型性能，并通过全局特征重要性和Mann-Whitney U检验验证模型解释性与统计显著性。

Result: 模型在三种语言表示下均表现稳定，句法及语法特征在无词汇内容时依然保持较强判别能力；受试者层面评估更为保守但一致；统计分析显示功能词使用、词汇多样性、句子结构和语篇连贯性存在显著差异，且与机器学习特征重要性结果高度对应。

Conclusion: 抽象的语言特征能够有效捕捉早期认知衰退的可靠标志，结合可解释的机器学习与非参数统计验证，支持基于语言的透明且可靠的认知筛查方法。

Abstract: Background: Subtle changes in spontaneous language production are among the earliest indicators of cognitive decline. Identifying linguistically interpretable markers of dementia can support transparent and clinically grounded screening approaches.
  Methods: This study analyzes spontaneous speech transcripts from the DementiaBank Pitt Corpus using three linguistic representations: raw cleaned text, a part-of-speech (POS)-enhanced representation combining lexical and grammatical information, and a POS-only syntactic representation. Logistic regression and random forest models were evaluated under two protocols: transcript-level train-test splits and subject-level five-fold cross-validation to prevent speaker overlap. Model interpretability was examined using global feature importance, and statistical validation was conducted using Mann-Whitney U tests with Cliff's delta effect sizes.
  Results: Across representations, models achieved stable performance, with syntactic and grammatical features retaining strong discriminative power even in the absence of lexical content. Subject-level evaluation yielded more conservative but consistent results, particularly for POS-enhanced and POS-only representations. Statistical analysis revealed significant group differences in functional word usage, lexical diversity, sentence structure, and discourse coherence, aligning closely with machine learning feature importance findings.
  Conclusion: The results demonstrate that abstract linguistic features capture robust markers of early cognitive decline under clinically realistic evaluation. By combining interpretable machine learning with non-parametric statistical validation, this study supports the use of linguistically grounded features for transparent and reliable language-based cognitive screening.

</details>


### [44] [Language Model Inversion through End-to-End Differentiation](https://arxiv.org/abs/2602.11044)
*Kevin Yandoka Denamganaï,Kartic Subr*

Main category: cs.CL

TL;DR: 本文通过端到端可微分方法，实现了语言模型输出的反向优化，即根据期望输出自动生成输入提示，实验表明该方法高效且可靠。


<details>
  <summary>Details</summary>
Motivation: 现有研究较少分析语言模型的可逆性，即如何从目标输出推测对应的输入提示，解决这类问题有助于更好地理解和控制LM行为。

Method: 提出一种端到端可微分算法，将语言模型视为处理分布序列的函数，利用梯度下降优化输入提示，实现针对目标输出的反向优化。

Result: 通过实验和消融研究，该方法能在长度分别为10和80的提示下，针对长度为20的目标输出有效优化提示词，证明了方法的可靠性和效率。

Conclusion: 该论文成功实现了语言模型（LM）输出的可逆性，通过端到端的可微分算法利用梯度下降法优化输入提示，实验证明该方法在多种白盒LM上表现稳定有效。

Abstract: Despite emerging research on Language Models (LM), few approaches analyse the invertibility of LMs. That is, given a LM and a desirable target output sequence of tokens, determining what input prompts would yield the target output remains an open problem. We formulate this problem as a classical gradient-based optimisation. First, we propose a simple algorithm to achieve end-to-end differentiability of a given (frozen) LM and then find optimised prompts via gradient descent. Our central insight is to view LMs as functions operating on sequences of distributions over tokens (rather than the traditional view as functions on sequences of tokens). Our experiments and ablations demonstrate that our DLM-powered inversion can reliably and efficiently optimise prompts of lengths $10$ and $80$ for targets of length $20$, for several white-box LMs (out-of-the-box).

</details>


### [45] [Embedding Inversion via Conditional Masked Diffusion Language Models](https://arxiv.org/abs/2602.11047)
*Han Xiao*

Main category: cs.CL

TL;DR: 本文提出一种基于条件掩码扩散的嵌入反演方法，通过并行迭代去噪实现高效准确的重构，在多个嵌入模型上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的嵌入反演方法多采用顺序自回归生成，效率低下。

Method: 将嵌入反演建模为条件掩码扩散，通过自适应层归一化让掩码扩散语言模型并行生成所有Token，避免访问目标编码器，仅需8次前向传播。模型参数量为7800万。

Result: 在三个嵌入模型的32个Token序列上，方法实现了81.3%的Token准确率和0.87的余弦相似度。

Conclusion: 该方法有效提高了嵌入反演的并行效率和准确度，且模型轻量，无需目标编码器支持。

Abstract: We frame embedding inversion as conditional masked diffusion, recovering all tokens in parallel through iterative denoising rather than sequential autoregressive generation. A masked diffusion language model is conditioned on the target embedding via adaptive layer normalization, requiring only 8 forward passes through a 78M parameter model with no access to the target encoder. On 32-token sequences across three embedding models, the method achieves 81.3% token accuracy and 0.87 cosine similarity.

</details>


### [46] [Conversational Behavior Modeling Foundation Model With Multi-Level Perception](https://arxiv.org/abs/2602.11065)
*Dingkun Zhou,Shuchang Pan,Jiachen Lian,Siddharth Banerjee,Sarika Pasumarthy,Dhruv Hebbar,Siddhant Patel,Zeyi Austin Li,Kan Jen Cheng,Sanay Bordia,Krish Patel,Akshaj Gupta,Tingle Li,Gopala Anumanchipalli*

Main category: cs.CL

TL;DR: 本文提出了一个基于多层感知和思维图的对话行为推理框架，通过分层预测和动态推理，有效捕捉和解释全双工语音对话中的行为和意图。


<details>
  <summary>Details</summary>
Motivation: 人类对话是由隐含的思维链组织而成，捕捉这一过程对于构建自然的双工交互系统至关重要。

Method: 提出了一个多层感知的框架，利用思维图(Graph-of-Thoughts, GoT)对对话行为进行推理，采用分层标注体系预测高层意图和低层言语行为，并通过转换器进行连续预测和动态推理。

Result: 该方法在合成及真实双工对话实验中表现出强健的行为检测能力，生成可解释的推理链，并为全双工语音对话系统中的对话推理建立了基准。

Conclusion: 本文提出的GoT框架有效建模了人类对话的意图-行动路径，实现了对双工对话行为的准确识别和可解释推理，推动了自然互动系统的发展。

Abstract: Human conversation is organized by an implicit chain of thoughts that manifests as timed speech acts. Capturing this perceptual pathway is key to building natural full-duplex interactive systems. We introduce a framework that models this process as multi-level perception, and then reasons over conversational behaviors via a Graph-of-Thoughts (GoT). Our approach formalizes the intent-to-action pathway with a hierarchical labeling scheme, predicting high-level communicative intents and low-level speech acts to learn their causal and temporal dependencies. To train this system, we develop a high quality corpus that pairs controllable, event-rich dialogue data with human-annotated labels. The GoT framework structures streaming predictions as an evolving graph, enabling a transformer to forecast the next speech act, generate concise justifications for its decisions, and dynamically refine its reasoning. Experiments on both synthetic and real duplex dialogues show that the framework delivers robust behavior detection, produces interpretable reasoning chains, and establishes a foundation for benchmarking conversational reasoning in full duplex spoken dialogue systems.

</details>


### [47] [Simultaneous Speech-to-Speech Translation Without Aligned Data](https://arxiv.org/abs/2602.11072)
*Tom Labiausse,Romain Fabre,Yannick Estève,Alexandre Défossez,Neil Zeghidour*

Main category: cs.CL

TL;DR: 该论文提出无词级对齐需求的Hibiki-Zero方法，实现高效准确的多语言同时语音翻译，并提供相关模型和数据资源。


<details>
  <summary>Details</summary>
Motivation: 传统的语音同时翻译依赖于难以大规模获取的词级对齐训练数据，且依赖语言特定的启发式方法效果欠佳。

Method: 提出Hibiki-Zero方法，完全消除词级对齐需求，先用句子级对齐数据训练高延时语音翻译模型，再通过新颖的基于GRPO的强化学习策略优化低延时性能。

Result: Hibiki-Zero在翻译准确率、延时、声音传递和自然度上均达到了五个源语言到英语任务的最新水平，且可用少于1000小时语音数据适配新输入语言。

Conclusion: Hibiki-Zero简化了训练流程，提升了多语言同时语音翻译的性能和适应性，突破了依赖语言特定对齐的瓶颈，实现了高效、准确和自然的实时翻译。

Abstract: Simultaneous speech translation requires translating source speech into a target language in real-time while handling non-monotonic word dependencies. Traditional approaches rely on supervised training with word-level aligned data, which is difficult to collect at scale and thus depends on synthetic alignments using language-specific heuristics that are suboptimal. We propose Hibiki-Zero, which eliminates the need for word-level alignments entirely. This fundamentally simplifies the training pipeline and enables seamless scaling to diverse languages with varying grammatical structures, removing the bottleneck of designing language-specific alignment heuristics. We first train on sentence-level aligned data to learn speech translation at high latency, then apply a novel reinforcement learning strategy using GRPO to optimize latency while preserving translation quality. Hibiki-Zero achieves state-of-the-art performance in translation accuracy, latency, voice transfer, and naturalness across five X-to-English tasks. Moreover, we demonstrate that our model can be adapted to support a new input language with less than 1000h of speech. We provide examples, model weights, inference code and we release a benchmark containing 45h of multilingual data for speech translation evaluation.

</details>


### [48] [SteuerLLM: Local specialized large language model for German tax law analysis](https://arxiv.org/abs/2602.11081)
*Sebastian Wind,Jeta Sopa,Laurin Schmid,Quirin Jackl,Sebastian Kiefer,Fei Wu,Martin Mayr,Harald Köstler,Gerhard Wellein,Andreas Maier,Soroosh Tayebi Arasteh*

Main category: cs.CL

TL;DR: 论文提出了针对德国税法的专业基准和训练模型，通过算法生成真实考题和合成数据训练领域专用大语言模型，显著提升了法律推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在严格规则、准确术语和法律结构领域性能下降，税法作为此类挑战典型示例，需要准确的法条引用和结构化论证。

Method: 算法生成真实考试题库作为基准，设计部分得分评估体系；构建大规模合成训练数据，基于受控检索增强管道训练28B参数的领域适配语言模型。

Result: SteuerLLM在多个税法核心领域和学术层次的真实考题上性能优于同规模通用模型，部分场景超越更大规模系统，并公开基准数据、模型和代码促进领域AI研究。

Conclusion: 针对德国税法领域，通过算法生成的开源测验基准 SteuerEx 和针对该领域训练的 SteuerLLM 模型显著提升了法律推理任务表现，域专门数据及架构适配优于仅增加模型规模。

Abstract: Large language models (LLMs) demonstrate strong general reasoning and language understanding, yet their performance degrades in domains governed by strict formal rules, precise terminology, and legally binding structure. Tax law exemplifies these challenges, as correct answers require exact statutory citation, structured legal argumentation, and numerical accuracy under rigid grading schemes. We algorithmically generate SteuerEx, the first open benchmark derived from authentic German university tax law examinations. SteuerEx comprises 115 expert-validated examination questions spanning six core tax law domains and multiple academic levels, and employs a statement-level, partial-credit evaluation framework that closely mirrors real examination practice. We further present SteuerLLM, a domain-adapted LLM for German tax law trained on a large-scale synthetic dataset generated from authentic examination material using a controlled retrieval-augmented pipeline. SteuerLLM (28B parameters) consistently outperforms general-purpose instruction-tuned models of comparable size and, in several cases, substantially larger systems, demonstrating that domain-specific data and architectural adaptation are more decisive than parameter scale for performance on realistic legal reasoning tasks. All benchmark data, training datasets, model weights, and evaluation code are released openly to support reproducible research in domain-specific legal artificial intelligence. A web-based demo of SteuerLLM is available at https://steuerllm.i5.ai.fau.de.

</details>


### [49] [DataChef: Cooking Up Optimal Data Recipes for LLM Adaptation via Reinforcement Learning](https://arxiv.org/abs/2602.11089)
*Yicheng Chen,Zerun Ma,Xinchen Xie,Yining Li,Kai Chen*

Main category: cs.CL

TL;DR: 本文通过强化学习实现了自动数据配方生成，成功将大规模语言模型适配至多种目标任务，减少人工成本，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前大规模语言模型训练数据的处理流程设计高度依赖人工，费时费力，且需大量专业知识，急需一种自动化生成高质量训练数据配方的方法。

Method: 采用在线强化学习框架，利用代理奖励预测候选数据配方的下游任务表现，指导模型循环优化数据处理流程，最终生成符合目标任务的完整数据配方。

Result: 提出的DataChef-32B在六个任务上的自动生成数据配方表现与人工设计相当，在数学领域提升了Qwen3-1.7B模型于AIME'25测试的性能，达到66.7分，超越基线模型。

Conclusion: 本论文提出了端到端数据配方生成方法，实现了自动化设计大规模语言模型的训练数据处理流程，显著减少了对人工设计的依赖。

Abstract: In the current landscape of Large Language Models (LLMs), the curation of large-scale, high-quality training data is a primary driver of model performance. A key lever is the \emph{data recipe}, which comprises a data processing pipeline to transform raw sources into training corpora. Despite the growing use of LLMs to automate individual data processing steps, such as data synthesis and filtering, the overall design of data recipes remains largely manual and labor-intensive, requiring substantial human expertise and iteration. To bridge this gap, we formulate \emph{end-to-end data recipe generation} for LLM adaptation. Given a target benchmark and a pool of available data sources, a model is required to output a complete data recipe that adapts a base LLM to the target task. We present DataChef-32B, which performs online reinforcement learning using a proxy reward that predicts downstream performance for candidate recipes. Across six held-out tasks, DataChef-32B produces practical recipes that reach comparable downstream performance to those curated by human experts. Notably, the recipe from DataChef-32B adapts Qwen3-1.7B-Base to the math domain, achieving 66.7 on AIME'25 and surpassing Qwen3-1.7B. This work sheds new light on automating LLM training and developing self-evolving AI systems.

</details>


### [50] [Can Large Language Models Make Everyone Happy?](https://arxiv.org/abs/2602.11091)
*Usman Naseem,Gautam Siddharth Kashyap,Ebad Shabbir,Sushant Kumar Ray,Abdullah Mohammad,Rafiq Ali*

Main category: cs.CL

TL;DR: 提出了一个新的综合基准MisAlign-Profile，用以评估大型语言模型中安全、价值与文化多个维度的错配及其权衡，从而更真实地反映模型在复杂现实环境中的行为表现。


<details>
  <summary>Details</summary>
Motivation: 现有基准多从单一维度评估模型，缺乏对安全、价值和文化三个维度同时错配及其权衡机制的系统理解，导致对实际多维场景下行为偏差的把握不足。

Method: 构建了包含112个规范领域分类（安全、价值、文化）的MisAligned-Aligned英文数据集MISALIGNTRADE，并利用Gemma-2-9B-it和Qwen3-30B-A3B-Instruct-2507模型及SimHash指纹技术进行提示扩展和去重。采用两阶段拒绝采样方法生成错配与对齐的模型响应。随后对多种通用、微调及开源LLM进行了基准测试。

Result: 评测揭示了大型语言模型在三个维度之间存在12%-34%的错配权衡，体现了模型在不同维度间表现的相互影响和冲突。

Conclusion: 该论文提出了一种统一的基准MisAlign-Profile，用于系统测量大型语言模型在安全性、价值观和文化维度之间的错配权衡，填补了现有基准在多维度交互评估中的不足。

Abstract: Misalignment in Large Language Models (LLMs) refers to the failure to simultaneously satisfy safety, value, and cultural dimensions, leading to behaviors that diverge from human expectations in real-world settings where these dimensions must co-occur. Existing benchmarks, such as SAFETUNEBED (safety-centric), VALUEBENCH (value-centric), and WORLDVIEW-BENCH (culture-centric), primarily evaluate these dimensions in isolation and therefore provide limited insight into their interactions and trade-offs. More recent efforts, including MIB and INTERPRETABILITY BENCHMARK-based on mechanistic interpretability, offer valuable perspectives on model failures; however, they remain insufficient for systematically characterizing cross-dimensional trade-offs. To address these gaps, we introduce MisAlign-Profile, a unified benchmark for measuring misalignment trade-offs inspired by mechanistic profiling. First, we construct MISALIGNTRADE, an English misaligned-aligned dataset across 112 normative domains taxonomies, including 14 safety, 56 value, and 42 cultural domains. In addition to domain labels, each prompt is classified with one of three orthogonal semantic types-object, attribute, or relations misalignment-using Gemma-2-9B-it and expanded via Qwen3-30B-A3B-Instruct-2507 with SimHash-based fingerprinting to avoid deduplication. Each prompt is paired with misaligned and aligned responses through two-stage rejection sampling to ensure quality. Second, we benchmark general-purpose, fine-tuned, and open-weight LLMs on MISALIGNTRADE-revealing 12%-34% misalignment trade-offs across dimensions.

</details>


### [51] [Safety Recovery in Reasoning Models Is Only a Few Early Steering Steps Away](https://arxiv.org/abs/2602.11096)
*Soumya Suvra Ghosal,Souradip Chakraborty,Vaibhav Singh,Furong Huang,Dinesh Manocha,Amrit Singh Bedi*

Main category: cs.CL

TL;DR: SafeThink通过在推理过程中动态监控并注入安全提示，有效提升多模态推理模型的安全性，显著降低了绕过攻击成功率且不会损害模型的推理表现。


<details>
  <summary>Details</summary>
Motivation: 基于强化学习的后训练方法虽能提升多模态大规模推理模型的推理能力，但却可能降低安全对齐，增加绕过安全限制的成功率，亟需一种有效的安全防御机制。

Method: SafeThink通过在安全阈值被违反时，监控推理过程并有条件地注入优化的短校正前缀“Wait, think safely”，将安全恢复视为满足约束的过程，从而实现安全干预。

Result: SafeThink在六个开源模型和四个绕过安全基准测试中，将攻击成功率降低了30-60%，例如LlamaV-o1模型在JailbreakV-28K测试中的攻击成功率从63.33%降至5.74%，同时保证了推理准确率基本不变。

Conclusion: SafeThink作为一种推理时的轻量级防御方法，有效减少了多模态大规模推理模型的安全漏洞攻击成功率，同时保持了推理性能的稳定。

Abstract: Reinforcement learning (RL) based post-training for explicit chain-of-thought (e.g., GRPO) improves the reasoning ability of multimodal large-scale reasoning models (MLRMs). But recent evidence shows that it can simultaneously degrade safety alignment and increase jailbreak success rates. We propose SafeThink, a lightweight inference-time defense that treats safety recovery as a satisficing constraint rather than a maximization objective. SafeThink monitors the evolving reasoning trace with a safety reward model and conditionally injects an optimized short corrective prefix ("Wait, think safely") only when the safety threshold is violated. In our evaluations across six open-source MLRMs and four jailbreak benchmarks (JailbreakV-28K, Hades, FigStep, and MM-SafetyBench), SafeThink reduces attack success rates by 30-60% (e.g., LlamaV-o1: 63.33% to 5.74% on JailbreakV-28K, R1-Onevision: 69.07% to 5.65% on Hades) while preserving reasoning performance (MathVista accuracy: 65.20% to 65.00%). A key empirical finding from our experiments is that safety recovery is often only a few steering steps away: intervening in the first 1-3 reasoning steps typically suffices to redirect the full generation toward safe completions.

</details>


### [52] [TEGRA: Text Encoding With Graph and Retrieval Augmentation for Misinformation Detection](https://arxiv.org/abs/2602.11106)
*Géraud Faye,Wassila Ouerdane,Guillaume Gadek,Céline Hudelot*

Main category: cs.CL

TL;DR: 本文提出了一种将文本与知识图结合的虚假信息检测新方法，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 虚假信息检测依赖外部知识辅助，模拟人工事实核查的过程以提升检测效果。

Method: 提出了TEG方法，将文本转化为图结构并联合编码文本和图信息进行分类，同时设计了扩展框架TEGRA以融合领域特定知识。

Result: 实验结果表明，TEG比单纯语言模型表现更优，TEGRA进一步提升了分类准确率。

Conclusion: 通过结合图结构和文本信息的混合表示方法TEG，以及其扩展版本TEGRA，本文显著提升了虚假信息检测的准确度。

Abstract: Misinformation detection is a critical task that can benefit significantly from the integration of external knowledge, much like manual fact-checking. In this work, we propose a novel method for representing textual documents that facilitates the incorporation of information from a knowledge base. Our approach, Text Encoding with Graph (TEG), processes documents by extracting structured information in the form of a graph and encoding both the text and the graph for classification purposes. Through extensive experiments, we demonstrate that this hybrid representation enhances misinformation detection performance compared to using language models alone. Furthermore, we introduce TEGRA, an extension of our framework that integrates domain-specific knowledge, further enhancing classification accuracy in most cases.

</details>


### [53] [Data Repetition Beats Data Scaling in Long-CoT Supervised Fine-Tuning](https://arxiv.org/abs/2602.11149)
*Dawid J. Kopiczko,Sagar Vaze,Tijmen Blankevoort,Yuki M. Asano*

Main category: cs.CL

TL;DR: 在推理语言模型的监督微调中，多轮训练小数据集优于单轮训练大数据集，完全记忆状态对应泛化提升，提出用token准确率指导训练停止。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过监督微调（SFT）提升推理语言模型的性能，质疑传统观点即更多独特训练样本带来更好泛化能力。

Method: 在固定更新预算下，比较多轮训练小数据集与单轮训练大数据集的效果，使用AIME'24/25和GPQA基准进行实验。

Result: 小数据集上多轮训练（重复训练）显著优于大数据集单轮训练，提升12-26个百分点且无灾难性遗忘，且训练token准确率可作为停止准则。

Conclusion: 重复训练优势明显，达成完全记忆状态时泛化能力提升，这为推理SFT提供了实用方案并提出了新的研究问题。

Abstract: Supervised fine-tuning (SFT) on chain-of-thought data is an essential post-training step for reasoning language models. Standard machine learning intuition suggests that training with more unique training samples yields better generalization. Counterintuitively, we show that SFT benefits from repetition: under a fixed update budget, training for more epochs on smaller datasets outperforms single-epoch training on larger datasets. On AIME'24/25 and GPQA benchmarks, Olmo3-7B trained for 128 epochs on 400 samples outperforms the equivalent 1 epoch on 51200 samples by 12-26 percentage points, with no additional catastrophic forgetting. We find that training token accuracy reliably signals when repetition has saturated; improvements from additional epochs plateau at full memorization, a pattern consistent across all settings. These findings provide a practical approach for reasoning SFT, where scaling epochs with token accuracy as a stopping criterion can replace expensive undirected data scaling. We pose the repetition advantage, where full memorization coincides with improved generalization, as a new open problem for the community in understanding the training dynamics of large language models.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [54] [AIvilization v0: Toward Large-Scale Artificial Social Simulation with a Unified Agent Architecture and Adaptive Agent Profiles](https://arxiv.org/abs/2602.10429)
*Wenkai Fan,Shurui Zhang,Xiaolong Wang,Haowei Yang,Tsz Wai Chan,Xingyan Chen,Junquan Bi,Zirui Zhou,Jia Liu,Kani Chen*

Main category: cs.MA

TL;DR: 本文设计并部署了一个大规模人工社会系统，通过层次化规划和双重记忆代理实现长期多目标自治，验证系统在复杂经济环境下表现稳定且多样化，支持持续投资与探索。


<details>
  <summary>Details</summary>
Motivation: 解决在快速变化环境中保持目标稳定性与反应正确性之间的矛盾，实现长期自治和多目标管理。

Method: 引入了分层的分支思考规划器、带双重记忆机制的适应性代理画像及人类参与的引导界面，通过模拟验证和分层重规划保证目标可行性，环境设计结合生理成本、生产机制和价格机制等，并利用高频交易数据验证系统表现。

Result: 系统实现了稳定的市场环境，复制了金融市场的关键现象（如厚尾收益和波动聚集），并且产生了基于教育和准入限制的财富结构差异，验证了设计的有效性；简化规划器在狭窄任务上表现相当，但整体架构在多目标长期任务中更加稳健。

Conclusion: AIvilization v0成功构建了一个大规模的人工社会，能在资源受限和环境快速变化下实现长期自主和多目标协调，表现出稳定的市场行为和财富分层，且完整架构在复杂任务中表现出更强鲁棒性。

Abstract: AIvilization v0 is a publicly deployed large-scale artificial society that couples a resource-constrained sandbox economy with a unified LLM-agent architecture, aiming to sustain long-horizon autonomy while remaining executable under rapidly changing environment. To mitigate the tension between goal stability and reactive correctness, we introduce (i) a hierarchical branch-thinking planner that decomposes life goals into parallel objective branches and uses simulation-guided validation plus tiered re-planning to ensure feasibility; (ii) an adaptive agent profile with dual-process memory that separates short-term execution traces from long-term semantic consolidation, enabling persistent yet evolving identity; and (iii) a human-in-the-loop steering interface that injects long-horizon objectives and short commands at appropriate abstraction levels, with effects propagated through memory rather than brittle prompt overrides. The environment integrates physiological survival costs, non-substitutable multi-tier production, an AMM-based price mechanism, and a gated education-occupation system. Using high-frequency transactions from the platforms mature phase, we find stable markets that reproduce key stylized facts (heavy-tailed returns and volatility clustering) and produce structured wealth stratification driven by education and access constraints. Ablations show simplified planners can match performance on narrow tasks, while the full architecture is more robust under multi-objective, long-horizon settings, supporting delayed investment and sustained exploration.

</details>


### [55] [An Ontology-driven Dynamic Knowledge Base for Uninhabited Ground Vehicles](https://arxiv.org/abs/2602.10555)
*Hsan Sandar Win,Andrew Walters,Cheng-Chew Lim,Daniel Webber,Seth Leslie,Tan Doan*

Main category: cs.MA

TL;DR: 本文提出基于本体的动态上下文任务数据知识库，实时更新UGV先验信息，增强其自主决策能力和态势感知，验证实验取得成功。


<details>
  <summary>Details</summary>
Motivation: UGV过度依赖任务前预置的先验信息，任务中出现的意外情况导致识别歧义并增加用户干预，故需引入动态上下文信息实时更新先验数据。

Method: 通过设计基于本体的表示方式，结合近实时信息获取和分析，实现了任务中平台内的DCMD动态更新，并在四辆UGV的实验室监视任务中进行了验证。

Result: 实验结果表明，本体驱动的动态知识表示能够使UGV操作环境信息具备机器可操作性，产出上下文信息，成功支持及时完成任务并提升态势感知。

Conclusion: 本研究表明，基于本体驱动的动态上下文任务数据（DCMD）知识库能够有效支持无人地面车辆（UGVs）的态势感知和自主决策，提升其在战术前沿复杂动态环境中的灵活性。

Abstract: In this paper, the concept of Dynamic Contextual Mission Data (DCMD) is introduced to develop an ontology-driven dynamic knowledge base for Uninhabited Ground Vehicles (UGVs) at the tactical edge. The dynamic knowledge base with DCMD is added to the UGVs to: support enhanced situation awareness; improve autonomous decision making; and facilitate agility within complex and dynamic environments. As UGVs are heavily reliant on the a priori information added pre-mission, unexpected occurrences during a mission can cause identification ambiguities and require increased levels of user input. Updating this a priori information with contextual information can help UGVs realise their full potential. To address this, the dynamic knowledge base was designed using an ontology-driven representation, supported by near real-time information acquisition and analysis, to provide in-mission on-platform DCMD updates. This was implemented on a team of four UGVs that executed a laboratory based surveillance mission. The results showed that the ontology-driven dynamic representation of the UGV operational environment was machine actionable, producing contextual information to support a successful and timely mission, and contributed directly to the situation awareness.

</details>


### [56] [Beyond Task Performance: A Metric-Based Analysis of Sequential Cooperation in Heterogeneous Multi-Agent Destructive Foraging](https://arxiv.org/abs/2602.10685)
*Alejandro Mendoza Barrionuevo,Samuel Yanes Luis,Daniel Gutiérrez Reina,Sergio L. Toral Marín*

Main category: cs.MA

TL;DR: 本文提出并验证了一套适用于异构、角色依赖多智能体系统的通用合作度量指标，提升了多智能体合作分析的系统性和多维度评估能力。


<details>
  <summary>Details</summary>
Motivation: 针对现有研究过于关注任务完成的算法性能，缺少系统化、多维度评估多智能体系统内合作性指标的问题。

Method: 设计包含主要指标、团队间指标和团队内指标三大类的合作性度量体系，并在一个灵感来源于动态水面清理的破坏性采掘场景中进行验证，评估多种学习算法和启发式方法。

Result: 通过具体场景验证了指标体系的有效性，能够描述异构多智能体在时间角色依赖条件下的合作特性，揭示不同算法在效率、协调、公平等方面的表现差异。

Conclusion: 本文提出了一套通用的合作性度量指标，能多层次表征多智能体系统中的效率、协调性、依赖性、公平性及敏感性，适用于具有时间角色依赖和部分可观测环境的异构多智能体系统。

Abstract: This work addresses the problem of analyzing cooperation in heterogeneous multi-agent systems which operate under partial observability and temporal role dependency, framed within a destructive multi-agent foraging setting. Unlike most previous studies, which focus primarily on algorithmic performance with respect to task completion, this article proposes a systematic set of general-purpose cooperation metrics aimed at characterizing not only efficiency, but also coordination and dependency between teams and agents, fairness, and sensitivity. These metrics are designed to be transferable to different multi-agent sequential domains similar to foraging. The proposed suite of metrics is structured into three main categories that jointly provide a multilevel characterization of cooperation: primary metrics, inter-team metrics, and intra-team metrics. They have been validated in a realistic destructive foraging scenario inspired by dynamic aquatic surface cleaning using heterogeneous autonomous vehicles. It involves two specialized teams with sequential dependencies: one focused on the search of resources, and another on their destruction. Several representative approaches have been evaluated, covering both learning-based algorithms and classical heuristic paradigms.

</details>


### [57] [The emergence of numerical representations in communicating artificial agents](https://arxiv.org/abs/2602.10996)
*Daniela Mihai,Lucas Weber,Francesca Franzon*

Main category: cs.MA

TL;DR: 研究表明，仅有交流压力能使人工代理高效交流已学数字，但生成的数字代码缺乏通用性和组合能力。


<details>
  <summary>Details</summary>
Motivation: 探讨仅凭交流压力是否足以促使人工代理产生数值表示及其代码是否类似于人类数字系统。

Method: 采用两种神经网络代理在指称游戏中通过离散符号和连续草图两种通信方式，探索符号和图像化的数字表示。

Result: 代理在两种通信方式中均能高精度传输训练过的数字，但生成代码缺乏组合性，无法推广到未见数字。

Conclusion: 通信压力足以使人工智能代理学会高精度的数字表示和传递，但生成的代码缺乏组合性，无法对未见过的数字进行系统性的表达。

Abstract: Human languages provide efficient systems for expressing numerosities, but whether the sheer pressure to communicate is enough for numerical representations to arise in artificial agents, and whether the emergent codes resemble human numerals at all, remains an open question. We study two neural network-based agents that must communicate numerosities in a referential game using either discrete tokens or continuous sketches, thus exploring both symbolic and iconic representations. Without any pre-defined numeric concepts, the agents achieve high in-distribution communication accuracy in both communication channels and converge on high-precision symbol-meaning mappings. However, the emergent code is non-compositional: the agents fail to derive systematic messages for unseen numerosities, typically reusing the symbol of the highest trained numerosity (discrete), or collapsing extrapolated values onto a single sketch (continuous). We conclude that the communication pressure alone suffices for precise transmission of learned numerosities, but additional pressures are needed to yield compositional codes and generalisation abilities.

</details>


### [58] [Learning to Compose for Cross-domain Agentic Workflow Generation](https://arxiv.org/abs/2602.11114)
*Jialiang Wang,Shengxiang Xu,Hanmo Liu,Jiachuan Wang,Yuyu Luo,Shimin Di,Min-Ling Zhang,Lei Chen*

Main category: cs.MA

TL;DR: 提出一种基于分解-重组-决策机制的单次生成跨域工作流方法，效率和性能优于传统迭代优化。


<details>
  <summary>Details</summary>
Motivation: 当前自动生成工作流在领域迁移下依赖迭代优化，不仅成本高且表现不稳定，亟需一种能够高效、稳健生成跨域工作流的方法。

Method: 通过学习一组跨领域的可重用工作流能力，以稀疏组合方式一次性生成任务专属工作流，并利用反事实归因机制识别驱动成功的能力，避免迭代优化带来的高成本。

Result: 该方法在多领域、跨领域和未知领域评测中，超越了需20次迭代的最先进基线，显著降低了生成延迟和成本。

Conclusion: 本论文提出了一种内嵌分解-重组-决策机制于大型语言模型（LLM）中的跨域自动化工作流生成方法，显著提升了工作流生成的效率和稳定性。

Abstract: Automatically generating agentic workflows -- executable operator graphs or codes that orchestrate reasoning, verification, and repair -- has become a practical way to solve complex tasks beyond what single-pass LLM generation can reliably handle. Yet what constitutes a good workflow depends heavily on the task distribution and the available operators. Under domain shift, current systems typically rely on iterative workflow refinement to discover a feasible workflow from a large workflow space, incurring high iteration costs and yielding unstable, domain-specific behavior. In response, we internalize a decompose-recompose-decide mechanism into an open-source LLM for cross-domain workflow generation. To decompose, we learn a compact set of reusable workflow capabilities across diverse domains. To recompose, we map each input task to a sparse composition over these bases to generate a task-specific workflow in a single pass. To decide, we attribute the success or failure of workflow generation to counterfactual contributions from learned capabilities, thereby capturing which capabilities actually drive success by their marginal effects. Across stringent multi-domain, cross-domain, and unseen-domain evaluations, our 1-pass generator surpasses SOTA refinement baselines that consume 20 iterations, while substantially reducing generation latency and cost.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [59] [AgentTrace: A Structured Logging Framework for Agent System Observability](https://arxiv.org/abs/2602.10133)
*Adam AlSayyad,Kelvin Yuxiang Huang,Richik Pal*

Main category: cs.SE

TL;DR: AgentTrace动态追踪LLM智能代理的行为和状态，强化安全与可追踪性，推动其在关键领域的应用。


<details>
  <summary>Details</summary>
Motivation: 现有的安全方法难以应对LLM代理的非确定性行为，缺乏透明度和可追溯性，限制了其在高风险领域的使用。

Method: 通过在运行时对代理进行轻量级的动态监测，AgentTrace捕获操作、认知和环境三方面的结构化日志，区别于传统日志系统，实现了连续且可内省的追踪。

Result: AgentTrace提升了代理部署的可靠性，支持精细的风险分析和更准确的信任评估，有效解决了制约LLM代理应用的关键安全问题。

Conclusion: AgentTrace作为一个动态可观测性和遥测框架，为LLM驱动的自主代理提供了连续、可内省的跟踪能力，从而增强了安全性、问责性和实时监控，促进了其在高风险领域的可靠部署。

Abstract: Despite the growing capabilities of autonomous agents powered by large language models (LLMs), their adoption in high-stakes domains remains limited. A key barrier is security: the inherently nondeterministic behavior of LLM agents defies static auditing approaches that have historically underpinned software assurance. Existing security methods, such as proxy-level input filtering and model glassboxing, fail to provide sufficient transparency or traceability into agent reasoning, state changes, or environmental interactions. In this work, we introduce AgentTrace, a dynamic observability and telemetry framework designed to fill this gap. AgentTrace instruments agents at runtime with minimal overhead, capturing a rich stream of structured logs across three surfaces: operational, cognitive, and contextual. Unlike traditional logging systems, AgentTrace emphasizes continuous, introspectable trace capture, designed not just for debugging or benchmarking, but as a foundational layer for agent security, accountability, and real-time monitoring. Our research highlights how AgentTrace can enable more reliable agent deployment, fine-grained risk analysis, and informed trust calibration, thereby addressing critical concerns that have so far limited the use of LLM agents in sensitive environments.

</details>


### [60] [Can Large Language Models Implement Agent-Based Models? An ODD-based Replication Study](https://arxiv.org/abs/2602.10140)
*Nuno Fachada,Daniel Fernandes,Carlos M. Fernandes,João P. Matos-Carvalho*

Main category: cs.SE

TL;DR: 研究评估了17款大型语言模型基于规范自动生成代理模型代码的能力，发现GPT-4.1表现最优，但整体实现的科学可靠性仍有限。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs是否能够从标准化规范中可靠地实现代理模型，以支持复制、验证和确认。

Method: 采用PPHPC捕食者-猎物模型作为参考规范，评估17个现代LLM在ODD到代码的翻译任务中的表现，包括可执行性检查、统计比较和运行效率分析。

Result: 发现虽然多款LLM能生成行为一致的代码实现，但仅具备可执行性不足以满足科学需求。GPT-4.1能够稳定生成统计上有效且高效的代码，Claude 3.7 Sonnet表现较好但不够稳定。

Conclusion: LLMs可以在一定程度上从标准化规范实现代理模型，但行为一致性和科学使用的可靠性尚未完全保证。GPT-4.1表现最好，Claude 3.7 Sonnet次之。

Abstract: Large language models (LLMs) can now synthesize non-trivial executable code from textual descriptions, raising an important question: can LLMs reliably implement agent-based models from standardized specifications in a way that supports replication, verification, and validation? We address this question by evaluating 17 contemporary LLMs on a controlled ODD-to-code translation task, using the PPHPC predator-prey model as a fully specified reference. Generated Python implementations are assessed through staged executability checks, model-independent statistical comparison against a validated NetLogo baseline, and quantitative measures of runtime efficiency and maintainability. Results show that behaviorally faithful implementations are achievable but not guaranteed, and that executability alone is insufficient for scientific use. GPT-4.1 consistently produces statistically valid and efficient implementations, with Claude 3.7 Sonnet performing well but less reliably. Overall, the findings clarify both the promise and current limitations of LLMs as model engineering tools, with implications for reproducible agent-based and environmental modelling.

</details>


### [61] [On the Use of a Large Language Model to Support the Conduction of a Systematic Mapping Study: A Brief Report from a Practitioner's View](https://arxiv.org/abs/2602.10147)
*Cauã Ferreira Barros,Marcos Kalinowski,Mohamad Kassab,Valdemar Vicente Graciano Neto*

Main category: cs.SE

TL;DR: 本文报告了使用LLMs辅助系统映射研究的经验，指出其提高效率的同时存在提示设计难度和幻觉等挑战，并给出实践建议。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对LLMs在系统综述全过程实际应用的详细报告。

Method: 利用LLMs辅助进行系统映射的各步骤，结合人工调整提示和验证。

Result: 发现LLMs在减少重复工作时间和标准化数据提取方面有优势，但提示设计复杂，效果依赖多次迭代。

Conclusion: LLMs在系统映射研究中能显著提高效率，但存在幻觉和需要频繁人工验证等风险。

Abstract: The use of Large Language Models (LLMs) has drawn growing interest within the scientific community. LLMs can handle large volumes of textual data and support methods for evidence synthesis. Although recent studies highlight the potential of LLMs to accelerate screening and data extraction steps in systematic reviews, detailed reports of their practical application throughout the entire process remain scarce. This paper presents an experience report on the conduction of a systematic mapping study with the support of LLMs, describing the steps followed, the necessary adjustments, and the main challenges faced. Positive aspects are discussed, such as (i) the significant reduction of time in repetitive tasks and (ii) greater standardization in data extraction, as well as negative aspects, including (i) considerable effort to build reliable well-structured prompts, especially for less experienced users, since achieving effective prompts may require several iterations and testing, which can partially offset the expected time savings, (ii) the occurrence of hallucinations, and (iii) the need for constant manual verification. As a contribution, this work offers lessons learned and practical recommendations for researchers interested in adopting LLMs in systematic mappings and reviews, highlighting both efficiency gains and methodological risks and limitations to be considered.

</details>


### [62] [EvoCodeBench: A Human-Performance Benchmark for Self-Evolving LLM-Driven Coding Systems](https://arxiv.org/abs/2602.10171)
*Wentao Zhang,Jianfeng Wang,Liheng Liang,Yilei Zhao,HaiBin Wen,Zhe Zhao*

Main category: cs.SE

TL;DR: 提出EvoCodeBench，评估具自我演化能力的语言模型编码系统，通过性能动态、多语言支持及人类对比，补足传统基准测试不足。


<details>
  <summary>Details</summary>
Motivation: 现有代码基准测试主要关注静态正确性，忽视了推理过程中模型自我演化能力和资源消耗，且缺乏与人类程序员的性能对比，以及多语言和长尾语言的稳定性分析。

Method: 设计了EvoCodeBench基准测试，追踪LLM驱动编码系统在反复求解过程中性能动态，包括正确性、解决时间、内存消耗和算法改进；支持多语言评测并直接与人类程序员在同一任务上进行比较。

Result: 自我演化系统随着推理过程表现出效率提升，EvoCodeBench提供了超越准确性、包含人类相对表现及多语言稳定性的评测视角。

Conclusion: EvoCodeBench为评估具自我演化能力的LLM驱动编码系统提供了基础，支持更全面且人类中心的性能衡量方法，促进多语言及长尾语言的稳健性研究。

Abstract: As large language models (LLMs) continue to advance in programming tasks, LLM-driven coding systems have evolved from one-shot code generation into complex systems capable of iterative improvement during inference. However, existing code benchmarks primarily emphasize static correctness and implicitly assume fixed model capability during inference. As a result, they do not capture inference-time self-evolution, such as whether accuracy and efficiency improve as an agent iteratively refines its solutions. They also provide limited accounting of resource costs and rarely calibrate model performance against that of human programmers. Moreover, many benchmarks are dominated by high-resource languages, leaving cross-language robustness and long-tail language stability underexplored. Therefore, we present EvoCodeBench, a benchmark for evaluating self-evolving LLM-driven coding systems across programming languages with direct comparison to human performance. EvoCodeBench tracks performance dynamics, measuring solution correctness alongside efficiency metrics such as solving time, memory consumption, and improvement algorithmic design over repeated problem-solving attempts. To ground evaluation in a human-centered reference frame, we directly compare model performance with that of human programmers on the same tasks, enabling relative performance assessment within the human ability distribution. Furthermore, EvoCodeBench supports multiple programming languages, enabling systematic cross-language and long-tail stability analyses under a unified protocol. Our results demonstrate that self-evolving systems exhibit measurable gains in efficiency over time, and that human-relative and multi-language analyses provide insights unavailable through accuracy alone. EvoCodeBench establishes a foundation for evaluating coding intelligence in evolving LLM-driven systems.

</details>


### [63] [TestExplora: Benchmarking LLMs for Proactive Bug Discovery via Repository-Level Test Generation](https://arxiv.org/abs/2602.10471)
*Steven Liu,Jane Luo,Xin Zhang,Aofan Liu,Hao Liu,Jie Wu,Ziyang Huang,Yangyu Huang,Yu Kang,Scarlett Li*

Main category: cs.SE

TL;DR: 提出TestExplora基准评估LLM主动发现软件缺陷能力，发现当前模型仍有明显不足，但代理探索方法展现出显著提升潜力。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法忽视了主动发现缺陷的目标，通常只关注回归预防或失败后重现，缺乏在失败前发现缺陷的能力，需构建新基准来填补这一空白。

Method: 提出TestExplora基准，包含2389个任务和482个代码库，模型通过与文档导出的意图对比实现主动缺陷发现，并采用连续、时间感知的数据采集方法保证评估的可持续性和准确性。

Result: 评测显示，最先进LLM最高Fail-to-Pass率仅16.06%，而引入代理探索的SWEAgent可将该率提升至17.27%，且Fail-to-Pass@5达到29.7%，表明代理探索在主动缺陷发现中成效显著。

Conclusion: 当前的LLM在主动发现软件缺陷方面存在显著能力差距，最先进模型的缺陷发现通过率较低，但采用代理探索的方法（如SWEAgent）能有效提升性能，显示出主动测试的潜力。

Abstract: Given that Large Language Models (LLMs) are increasingly applied to automate software development, comprehensive software assurance spans three distinct goals: regression prevention, reactive reproduction, and proactive discovery. Current evaluations systematically overlook the third goal. Specifically, they either treat existing code as ground truth (a compliance trap) for regression prevention, or depend on post-failure artifacts (e.g., issue reports) for bug reproduction-so they rarely surface defects before failures. To bridge this gap, we present TestExplora, a benchmark designed to evaluate LLMs as proactive testers within full-scale, realistic repository environments. TestExplora contains 2,389 tasks from 482 repositories and hides all defect-related signals. Models must proactively find bugs by comparing implementations against documentation-derived intent, using documentation as the oracle. Furthermore, to keep evaluation sustainable and reduce leakage, we propose continuous, time-aware data collection. Our evaluation reveals a significant capability gap: state-of-the-art models achieve a maximum Fail-to-Pass (F2P) rate of only 16.06%. Further analysis indicates that navigating complex cross-module interactions and leveraging agentic exploration are critical to advancing LLMs toward autonomous software quality assurance. Consistent with this, SWEAgent instantiated with GPT-5-mini achieves an F2P of 17.27% and an F2P@5 of 29.7%, highlighting the effectiveness and promise of agentic exploration in proactive bug discovery tasks.

</details>


### [64] [From Prompt-Response to Goal-Directed Systems: The Evolution of Agentic AI Software Architecture](https://arxiv.org/abs/2602.10479)
*Mamdouh Alenezi*

Main category: cs.SE

TL;DR: 本文系统回顾并规划了从提示驱动生成模型向具备自主感知和规划能力的Agentic AI转型的理论和实践路径，提出关键架构和治理框架，解析行业趋势，并指明未来挑战。


<details>
  <summary>Details</summary>
Motivation: 当前的生成模型缺乏目标导向和自主行动能力，推动了从单纯提示驱动向具备自主感知和规划能力的系统架构的转变，旨在构建更强大的智能代理。

Method: 通过结合经典智能代理理论（反应式、推理式、信念-欲望-意图模型）与现代以大语言模型为中心的方法（工具调用、记忆增强推理、多代理协调），提出一种生产级LLM代理的参考架构、代理拓扑分类及企业加强指南。

Result: 提出了一个清晰的生产级LLM代理架构，分类了多代理系统拓扑及其失败模式，并制定了企业级的治理和可观测性检查表。分析了多种行业平台，发现其向标准化代理循环、注册机制和可审计控制机制趋同，同时指出未来的研究重点在于验证性、互操作性和安全自主性。

Conclusion: Agentic AI正处于从无状态提示驱动的生成模型向具备自主感知、规划和行动能力的目标导向系统的转变阶段，这一发展将促进具备可扩展性和可组合性的自主系统的成熟，类似于网络服务的发展。

Abstract: Agentic AI denotes an architectural transition from stateless, prompt-driven generative models toward goal-directed systems capable of autonomous perception, planning, action, and adaptation through iterative control loops. This paper examines this transition by connecting foundational intelligent agent theories, including reactive, deliberative, and Belief-Desire-Intention models, with contemporary LLM-centric approaches such as tool invocation, memory-augmented reasoning, and multi-agent coordination. The paper presents three primary contributions: (i) a reference architecture for production-grade LLM agents that separates cognitive reasoning from execution using typed tool interfaces; (ii) a taxonomy of multi-agent topologies, together with their associated failure modes and mitigation approaches; and (iii) an enterprise hardening checklist that incorporates governance, observability, and reproducibility considerations. Through an analysis of emerging industry platforms, including Kore.ai, Salesforce Agentforce, TrueFoundry, ZenML, and LangChain, the study identifies a convergence toward standardized agent loops, registries, and auditable control mechanisms. It is argued that the subsequent phase of agentic AI development will parallel the maturation of web services, relying on shared protocols, typed contracts, and layered governance structures to support scalable and composable autonomy. The persistent challenges related to verifiability, interoperability, and safe autonomy remain key areas for future research and practical deployment.

</details>


### [65] [Consistency Meets Verification: Enhancing Test Generation Quality in Large Language Models Without Ground-Truth Solutions](https://arxiv.org/abs/2602.10522)
*Hamed Taherkhani,Alireza DaghighFarsoodeh,Mohammad Chowdhury,Hung Viet Pham,Hadi Hemmati*

Main category: cs.SE

TL;DR: ConVerTest通过三大战略提升自动测试质量，摆脱对真实代码验证的依赖，显著改善了测试有效性和代码覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的自动测试方法依赖真实代码进行验证，存在错误传播风险且限制了测试驱动开发的应用。

Method: 提出了两阶段管线ConVerTest，结合自洽性生成测试用例、多步推理引导代码迭代优化以及双重执行协议进行代码和测试的交叉验证。

Result: 在BIGCODEBENCH和LBPP基准测试中，ConVerTest分别提升测试有效性39%、代码覆盖率28%、突变测试分数18%。

Conclusion: ConVerTest显著提升了自动生成测试的有效性和可靠性，避免了对先验代码的依赖，适用于测试驱动开发。

Abstract: Large Language Models (LLMs) have significantly advanced automated test generation, yet existing methods often rely on ground-truth code for verification, risking bug propagation and limiting applicability in test-driven development. We present ConVerTest, a novel two-stage pipeline for synthesizing reliable tests without requiring prior code implementations. ConVerTest integrates three core strategies: (i) Self-Consistency(SC) to generate convergent test cases via majority voting; (ii) Chain-of-Verification (CoVe) for iterative, reasoning-guided code refinement; and (iii) a Dual Execution Agreement to crossvalidate code and tests through consensus. Experiments on BIGCODEBENCH and LESS BASIC PYTHON PROBLEMS (LBPP) benchmarks demonstrate that ConVerTest improves test validity, line coverage, and mutation scores by up to 39%, 28%, and 18% respectively over baselines. Our findings highlight ConVerTest as a robust solution for mitigating hallucinations and enhancing the reliability of autonomous software testing agents.

</details>


### [66] [Theory of Troubleshooting: The Developer's Cognitive Experience of Overcoming Confusion](https://arxiv.org/abs/2602.10540)
*Arty Starr,Margaret-Anne Storey*

Main category: cs.SE

TL;DR: 该论文基于认知科学，构建了软件开发中故障排除的理论，阐明了认知疲劳及其对开发者体验和项目风险的影响。


<details>
  <summary>Details</summary>
Motivation: 软件开发中故障排除过程复杂且消耗大量认知资源，亟需理论框架解释故障排除的困难及其带来的风险。

Method: 通过访谈27位专业开发者，采用建构主义扎根理论方法，基于经验数据构建理论。

Result: 提出了基于认知科学的故障排除理论，解释了故障排除中的认知疲劳机制，并为开发者体验研究提供认知基础和实践启示。

Conclusion: 本文提出的故障排除理论揭示了软件开发中故障排除的认知挑战及其对项目风险的影响，强调了故障排除过程中认知资源的消耗与疲劳。

Abstract: This paper introduces a Theory of Troubleshooting that is rooted in cognitive science. This theory helps software developers explain the challenges they face and the project risks that emerge as troubleshooting becomes difficult. We define troubleshooting as the cognitive problem-solving process of identifying, understanding, and constructing a mental model of the cause of an unexpected system behavior, and consider the cognitive process of troubleshooting to be an integral part of the activity of debugging. Troubleshooting is a particularly intense and draining aspect of software work, placing sustained demands on attention, working memory, and mental modeling. By surfacing and naming the confusion experience inherent in troubleshooting in terms of neurological and attentional dynamics, our theory explains how prolonged troubleshooting can deplete cognitive resources and lead to cognitive fatigue. In the study presented in this paper, we interview 27 professional developers about their troubleshooting experiences, and follow a Constructivist Grounded Theory approach to construct a theory grounded in empirical data. Our theory contributes to research on Developer Experience by providing a cognitive foundation for understanding troubleshooting difficulty, fatigue, and sustainability risk--and offers practical implications for both research and industry.

</details>


### [67] [ISD-Agent-Bench: A Comprehensive Benchmark for Evaluating LLM-based Instructional Design Agents](https://arxiv.org/abs/2602.10620)
*YoungHoon Jeon,Suwan Kim,Haein Son,Sookbun Lee,Yeil Jeong,Unggi Lee*

Main category: cs.SE

TL;DR: 本文提出ISD-Agent-Bench评测基准，系统评估了结合经典教学设计理论和现代推理的LLM代理，验证了其在自动化教学设计中的优越性能，为LLM应用于教学系统设计研究奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理在教学系统设计中应用潜力大，但缺乏标准化评测基准及存在评判偏差风险，难以客观评估其表现。

Method: 构建了包含25,795个场景的ISD-Agent-Bench基准，通过多模型多评审的多评委协议确保评价可靠性，比较了基于ADDIE等经典理论和纯技术驱动的多种ISD代理。

Result: 构建的基准覆盖51个变量和33个ISD子步骤，实验表明，融合经典ISD理论和现代推理的代理性能最优，理论质量与基准表现高度相关，理论驱动的代理在以问题为中心的设计和目标评估方面表现突出。

Conclusion: 结合经典教学系统设计理论与现代推理方法的LLM代理在自动化教学系统设计中表现最佳，优于单纯的理论或技术方法。

Abstract: Large Language Model (LLM) agents have shown promising potential in automating Instructional Systems Design (ISD), a systematic approach to developing educational programs. However, evaluating these agents remains challenging due to the lack of standardized benchmarks and the risk of LLM-as-judge bias. We present ISD-Agent-Bench, a comprehensive benchmark comprising 25,795 scenarios generated via a Context Matrix framework that combines 51 contextual variables across 5 categories with 33 ISD sub-steps derived from the ADDIE model. To ensure evaluation reliability, we employ a multi-judge protocol using diverse LLMs from different providers, achieving high inter-judge reliability. We compare existing ISD agents with novel agents grounded in classical ISD theories such as ADDIE, Dick \& Carey, and Rapid Prototyping ISD. Experiments on 1,017 test scenarios demonstrate that integrating classical ISD frameworks with modern ReAct-style reasoning achieves the highest performance, outperforming both pure theory-based agents and technique-only approaches. Further analysis reveals that theoretical quality strongly correlates with benchmark performance, with theory-based agents showing significant advantages in problem-centered design and objective-assessment alignment. Our work provides a foundation for systematic LLM-based ISD research.

</details>


### [68] [Assessing Vision-Language Models for Perception in Autonomous Underwater Robotic Software](https://arxiv.org/abs/2602.10655)
*Muhammad Yousaf,Aitor Arrieta,Shaukat Ali,Paolo Arcaini,Shuai Wang*

Main category: cs.SE

TL;DR: 本文针对自主水下机器人中感知模块面临的挑战，评估了视觉-语言模型在检测水下垃圾方面的性能及不确定性，为软件工程师选型提供实践参考。


<details>
  <summary>Details</summary>
Motivation: 水下环境恶劣且标注数据稀缺，传统深度学习模型难以保证感知模块的可信度。VLM模型通过上下文推理可能更适合此类复杂环境，但其实际表现及不确定性尚未从软件工程角度深入研究。

Method: 通过实证评估VLM感知模块在水下垃圾检测任务中的表现，包括性能测量、置信度不确定性分析及两者关系的研究。

Result: 评估结果揭示VLM模型在低能见度和噪声较大的水下环境中表现稳健，误差与不确定性的关系为风险管理提供了量化依据，支持工业合作伙伴选择合适的VLM模型。

Conclusion: 基于视觉-语言模型（VLM）的感知模块在自主水下机器人（AUR）软件中展现出良好的性能和鲁棒性，适合用于检测水下垃圾。

Abstract: Autonomous Underwater Robots (AURs) operate in challenging underwater environments, including low visibility and harsh water conditions. Such conditions present challenges for software engineers developing perception modules for the AUR software. To successfully carry out these tasks, deep learning has been incorporated into the AUR software to support its operations. However, the unique challenges of underwater environments pose difficulties for deep learning models, which often rely on labeled data that is scarce and noisy. This may undermine the trustworthiness of AUR software that relies on perception modules. Vision-Language Models (VLMs) offer promising solutions for AUR software as they generalize to unseen objects and remain robust in noisy conditions by inferring information from contextual cues. Despite this potential, their performance and uncertainty in underwater environments remain understudied from a software engineering perspective. Motivated by the needs of an industrial partner in assurance and risk management for maritime systems to assess the potential use of VLMs in this context, we present an empirical evaluation of VLM-based perception modules within the AUR software. We assess their ability to detect underwater trash by computing performance, uncertainty, and their relationship, to enable software engineers to select appropriate VLMs for their AUR software.

</details>


### [69] [Hidden Licensing Risks in the LLMware Ecosystem](https://arxiv.org/abs/2602.10758)
*Bo Wang,Yueyang Chen,Jieke Shi,Minghui Li,Yunbo Lyu,Yinan Wu,Youfang Lin,Zhou Yang*

Main category: cs.SE

TL;DR: 本文针对LLMware复杂供应链中的许可问题进行大规模分析，提出了一个基于LLM的许可兼容检测工具LiAgent，显著提升了检测性能，发现并报告了多个许可不兼容问题，促进了LLMware生态的健康发展。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）被广泛集成到软件系统中，形成了新型的LLMware系统，这些系统依赖复杂的开源软件、模型和数据集供应链，然而相关的许可问题尚未被充分研究。

Method: 通过从GitHub和Hugging Face收集大规模数据集，分析包含开源仓库、LLM模型和数据集的LLMware供应链许可分布，结合对许可相关讨论的定性分析，评估现有许可冲突检测方法的性能，并提出基于LLM的代理框架LiAgent进行许可兼容性分析。

Result: 发现LLMware的许可分布与传统开源生态存在显著差异，现有检测方法F1得分较低（58%和76%），而LiAgent提升到87%。LiAgent报告了60个许可不兼容问题，其中11个得到开发者确认，涉及高下载量的模型，表明影响广泛。

Conclusion: LLMware生态中许可兼容性问题普遍存在且复杂，现有检测技术不足，LiAgent为生态级许可兼容分析提供了有效方法，助力推动LLMware生态的可持续发展。

Abstract: Large Language Models (LLMs) are increasingly integrated into software systems, giving rise to a new class of systems referred to as LLMware. Beyond traditional source-code components, LLMware embeds or interacts with LLMs that depend on other models and datasets, forming complex supply chains across open-source software (OSS), models, and datasets. However, licensing issues emerging from these intertwined dependencies remain largely unexplored. Leveraging GitHub and Hugging Face, we curate a large-scale dataset capturing LLMware supply chains, including 12,180 OSS repositories, 3,988 LLMs, and 708 datasets. Our analysis reveals that license distributions in LLMware differ substantially from traditional OSS ecosystems. We further examine license-related discussions and find that license selection and maintenance are the dominant concerns, accounting for 84% of cases. To understand incompatibility risks, we analyze license conflicts along supply chains and evaluate state-of-the-art detection approaches, which achieve only 58% and 76% F1 scores in this setting. Motivated by these limitations, we propose LiAgent, an LLM-based agent framework for ecosystem-level license compatibility analysis. LiAgent achieves an F1 score of 87%, improving performance by 14 percentage points over prior methods. We reported 60 incompatibility issues detected by LiAgent, 11 of which have been confirmed by developers. Notably, two conflicted LLMs have over 107 million and 5 million downloads on Hugging Face, respectively, indicating potentially widespread downstream impact. We conclude with implications and recommendations to support the sustainable growth of the LLMware ecosystem.

</details>


### [70] [VulReaD: Knowledge-Graph-guided Software Vulnerability Reasoning and Detection](https://arxiv.org/abs/2602.10787)
*Samal Mukhtar,Yinghua Yao,Zhu Sun,Mustafa Mustafa,Yew Soon Ong,Youcheng Sun*

Main category: cs.SE

TL;DR: 提出VulReaD，结合安全知识图谱和大语言模型，实现与CWE一致的软件漏洞多类别推理和检测，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的软件漏洞检测多聚焦于二元分类，缺乏与CWE类别语义一致的解释，限制了漏洞检测的细粒度推理能力和解释性。

Method: 提出了基于安全知识图谱引导的VulReaD，采用强大的教师大语言模型生成与CWE一致的对比推理监督，通过无人工注释训练学生模型，并使用Odds Ratio Preference Optimization优化以促进分类法对齐的推理能力。

Result: 在三个真实数据集上，VulReaD相比最先进方法，二元分类F1提升8-10%，多分类Macro-F1提升30%，Micro-F1提升18%，显示KG引导的推理增强了检测覆盖率和解释性，大语言模型在二元检测中优于深度学习基线。

Conclusion: VulReaD方法显著提升了软件漏洞检测的性能，尤其是在多类别CWE级别的漏洞推理上表现优异，同时增强了预测的语义一致性和解释的可理解性。

Abstract: Software vulnerability detection (SVD) is a critical challenge in modern systems. Large language models (LLMs) offer natural-language explanations alongside predictions, but most work focuses on binary evaluation, and explanations often lack semantic consistency with Common Weakness Enumeration (CWE) categories. We propose VulReaD, a knowledge-graph-guided approach for vulnerability reasoning and detection that moves beyond binary classification toward CWE-level reasoning. VulReaD leverages a security knowledge graph (KG) as a semantic backbone and uses a strong teacher LLM to generate CWE-consistent contrastive reasoning supervision, enabling student model training without manual annotations. Students are fine-tuned with Odds Ratio Preference Optimization (ORPO) to encourage taxonomy-aligned reasoning while suppressing unsupported explanations. Across three real-world datasets, VulReaD improves binary F1 by 8-10% and multi-class classification by 30% Macro-F1 and 18% Micro-F1 compared to state-of-the-art baselines. Results show that LLMs outperform deep learning baselines in binary detection and that KG-guided reasoning enhances CWE coverage and interpretability.

</details>


### [71] [PELLI: Framework to effectively integrate LLMs for quality software generation](https://arxiv.org/abs/2602.10808)
*Rasmus Krebs,Somnath Mazumdar*

Main category: cs.SE

TL;DR: 本文提出PELLI，构建全面代码质量评估框架，比较五种主流LLM在可维护性、性能和可靠性上的表现，发现GPT-4T和Gemini表现较好，提示设计影响代码质量，推动LLM与人类开发者高效协作。


<details>
  <summary>Details</summary>
Motivation: 现有研究多仅以可靠性为指标，且只比较少量LLM，缺乏全面多指标、多模型的代码质量评估框架。

Method: 提出PELLI框架，通过迭代分析过程，综合评估代码的可维护性、性能和可靠性三个非功能需求，选取五种主流LLM进行量化对比。

Result: 构建了全面的代码质量评估框架PELLI，评估了三大非功能需求指标，发现GPT-4T与Gemini表现优异，提示设计影响显著，结果对LLM在实际应用推广具有指导意义。

Conclusion: GPT-4T和Gemini在代码质量评估中表现较好，提示设计对代码质量有显著影响，不同应用领域和提示下表现存在差异。

Abstract: Recent studies have revealed that when LLMs are appropriately prompted and configured, they demonstrate mixed results. Such results often meet or exceed the baseline performance. However, these comparisons have two primary issues. First, they mostly considered only reliability as a comparison metric and selected a few LLMs (such as Codex and ChatGPT) for comparision. This paper proposes a comprehensive code quality assessment framework called Programmatic Excellence via LLM Iteration (PELLI). PELLI is an iterative analysis-based process that upholds high-quality code changes. We extended the state-of-the-art by performing a comprehensive evaluation that generates quantitative metrics for analyzing three primary nonfunctional requirements (such as maintainability, performance, and reliability) while selecting five popular LLMs. For PELLI's applicability, we selected three application domains while following Python coding standards. Following this framework, practitioners can ensure harmonious integration between LLMs and human developers, ensuring that their potential is fully realized. PELLI can serve as a practical guide for developers aiming to leverage LLMs while adhering to recognized quality standards. This study's outcomes are crucial for advancing LLM technologies in real-world applications, providing stakeholders with a clear understanding of where these LLMs excel and where they require further refinement. Overall, based on three nonfunctional requirements, we have found that GPT-4T and Gemini performed slightly better. We also found that prompt design can influence the overall code quality. In addition, each application domain demonstrated high and low scores across various metrics, and even within the same metrics across different prompts.

</details>


### [72] [Deriving and Validating Requirements Engineering Principles for Large-Scale Agile Development: An Industrial Longitudinal Study](https://arxiv.org/abs/2602.10972)
*Hina Saeeda,Mijin Kim,Eric Knauss,Jesper Thyssen,Jesper Ørting,Jesper Lysemose Korsgaard,Niels Jørgen Strøm*

Main category: cs.SE

TL;DR: 本文通过长期案例研究和多方验证，提出了六条适用于大规模敏捷开发的需求工程原则，为提升需求管理和组织协同提供了实用框架。


<details>
  <summary>Details</summary>
Motivation: 大规模敏捷系统开发中缺乏统一的需求工程流程和高水平指导原则，导致需求管理效果不佳。本文旨在填补这一空白，探索适用于大规模敏捷环境的需求工程指导原则。

Method: 采用定性数据收集方法，包括25个以上冲刺、320次每周同步会议、七次公司间和公司内部的研讨会，以及后期与高级领导的焦点小组讨论，结合主题分析法提炼出关键原则，并通过跨公司专家评估进行验证。

Result: 总结出六条关键的需求工程原则，涵盖架构背景、利益相关者驱动的验证与对齐、大规模敏捷组织中的需求实践、进化性的轻量级文档、委托式需求管理、组织角色与职责以及共享的需求理解。通过行业内多家跨国企业的验证，证明这些原则具有广泛适用性和实践价值。

Conclusion: 本文通过五年纵向案例研究，归纳出六条适用于大规模敏捷系统开发的需求工程原则，这些原则经过多方验证，证明能够提升需求管理效果和组织协同。

Abstract: In large scale agile systems development, the lack of a unified requirements engineering (RE) process is a major challenge, exacerbated by the absence of high level guiding principles for effective requirements management. To address this challenge, we conducted a five year longitudinal case study with Grundfos AB, in collaboration with the Software Centre in Sweden. RE principles were first derived through qualitative data collection spanning more than 25 sprints, approximately 320 weekly synchronisation meetings, and seven cross-company, company-specific workshops between 2019 and 2024. These activities engaged practitioners from diverse roles, representing several hundred developers across domains. In late 2024, five in depth focus groups with senior leaders at Grundfos provided retrospective validation of the principles and assessed their strategic impact. We aim to (1) empirically examine RE principles in large scale agile system development, (2) explore their benefits in practice within the case company, and (3) identify a set of transferable RE principles for large scale contexts. Using thematic analysis, six key RE principles architectural context, stakeholder-driven validation and alignment, requirements practices in large-scale agile organisations. evolution with lightweight documentation, delegated requirements management, organisational roles and responsibilities, and a shared understanding of requirements are derived. The study was further validated through crosscompany expert evaluation with three additional multinational organisations (Bosch, Ericsson, and Volvo Cars), which are directly responsible for largescale requirements management. Together, these efforts provide a scalable and adaptable foundation for improving requirements practices in largescale agile organisations.

</details>


### [73] [FeatureBench: Benchmarking Agentic Coding for Complex Feature Development](https://arxiv.org/abs/2602.10975)
*Qixing Zhou,Jiacheng Zhang,Haiyang Wang,Rui Hao,Jiahe Wang,Minghao Han,Yuxue Yang,Shuzhe Wu,Feiyang Pan,Lue Fan,Dandan Tu,Zhaoxiang Zhang*

Main category: cs.SE

TL;DR: 提出FeatureBench，一个自动生成特性级跨提交/PR代理编码评估基准，揭示了当前代理模型能力不足，促进代理编码技术发展。


<details>
  <summary>Details</summary>
Motivation: 当前代理编码基准任务范围有限，常依赖非执行评估且难以自动更新，缺乏对代理模型在真实复杂软件开发中的能力全面评估。

Method: 采用基于执行的评估协议和自动化、可扩展的测试驱动任务生成方法，通过追踪单元测试及其依赖图，自动从代码仓库派生跨多个提交和PR的特性级任务，并构建可执行环境进行验证。

Result: 构建了包含200个挑战性任务和3825个可执行环境的FeatureBench基准，从24个开源仓库自动生成，实证评测显示最先进代理模型在此基准上成功率仅为11.0%，远低于已有SWE-bench表现。

Conclusion: FeatureBench基于端到端、特性导向的软件开发过程，设计了一个评估基准，揭示了当前最先进的代理代码模型在特性级任务上的不足，表明代理代码仍有显著提升空间。

Abstract: Agents powered by large language models (LLMs) are increasingly adopted in the software industry, contributing code as collaborators or even autonomous developers. As their presence grows, it becomes important to assess the current boundaries of their coding abilities. Existing agentic coding benchmarks, however, cover a limited task scope, e.g., bug fixing within a single pull request (PR), and often rely on non-executable evaluations or lack an automated approach for continually updating the evaluation coverage. To address such issues, we propose FeatureBench, a benchmark designed to evaluate agentic coding performance in end-to-end, feature-oriented software development. FeatureBench incorporates an execution-based evaluation protocol and a scalable test-driven method that automatically derives tasks from code repositories with minimal human effort. By tracing from unit tests along a dependency graph, our approach can identify feature-level coding tasks spanning multiple commits and PRs scattered across the development timeline, while ensuring the proper functioning of other features after the separation. Using this framework, we curated 200 challenging evaluation tasks and 3825 executable environments from 24 open-source repositories in the first version of our benchmark. Empirical evaluation reveals that the state-of-the-art agentic model, such as Claude 4.5 Opus, which achieves a 74.4% resolved rate on SWE-bench, succeeds on only 11.0% of tasks, opening new opportunities for advancing agentic coding. Moreover, benefiting from our automated task collection toolkit, FeatureBench can be easily scaled and updated over time to mitigate data leakage. The inherent verifiability of constructed environments also makes our method potentially valuable for agent training.

</details>
