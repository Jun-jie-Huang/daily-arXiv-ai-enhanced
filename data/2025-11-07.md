<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 44]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.SE](#cs.SE) [Total: 16]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Activation-Space Personality Steering: Hybrid Layer Selection for Stable Trait Control in LLMs](https://arxiv.org/abs/2511.03738)
*Pranav Bhandari,Nicolas Fay,Sanjeevan Selvaganapathy,Amitava Datta,Usman Naseem,Mehwish Nasim*

Main category: cs.CL

TL;DR: 本文提出了一种基于大五人格特质的低秩子空间发现方法，挖掘并注入LLM中隐藏的个性特质表示，实现了精确的个性化输出控制。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型虽然隐含人格特质，但如何稳定控制和对齐这些特质以满足具体需求仍是未解决的挑战。需要有效的行为调控机制来填补这一空白。

Method: 利用大五人格框架提取变换器层的隐藏激活，采用低秩子空间发现技术，确定不同模型架构中的特征层，实现个性特质的稳健注入。并通过动态层选择的灵活控制框架调节特质表达。

Result: 实验发现人格特质在模型中存在低秩共享子空间，这些结构通过适当扰动可实现有效行为引导，同时不影响模型的流畅性、多样性和通用性能。

Conclusion: 该方法连接了心理学理论与模型实际对齐，提供了一种实用的个性化行为调控机制，为LLM个性化和行为操控开辟了新路径。

Abstract: Large Language Models exhibit implicit personalities in their generation, but
reliably controlling or aligning these traits to meet specific needs remains an
open challenge. The need for effective mechanisms for behavioural manipulation
of the model during generation is a critical gap in the literature that needs
to be fulfilled. Personality-aware LLMs hold a promising direction towards this
objective. However, the relationship between these psychological constructs and
their representations within LLMs remains underexplored and requires further
investigation. Moreover, it is intriguing to understand and study the use of
these representations to steer the models' behaviour. We propose a novel
pipeline that extracts hidden state activations from transformer layers using
the Big Five Personality Traits (Openness, Conscientiousness, Extraversion,
Agreeableness and Neuroticism), which is a comprehensive and empirically
validated framework to model human personality applies low-rank subspace
discovery methods, and identifies trait-specific optimal layers across
different model architectures for robust injection. The resulting
personality-aligned directions are then operationalised through a flexible
steering framework with dynamic layer selection, enabling precise control of
trait expression in LLM outputs. Our findings reveal that personality traits
occupy a low-rank shared subspace, and that these latent structures can be
transformed into actionable mechanisms for effective steering through careful
perturbations without impacting the fluency, variance and general capabilities,
helping to bridge the gap between psychological theory and practical model
alignment.

</details>


### [2] [TextualVerifier: Verify TextGrad Step-by-Step](https://arxiv.org/abs/2511.03739)
*Eugenius Mario Situmorang,Adila Alfa Krisnadhi,Ari Wibisono*

Main category: cs.CL

TL;DR: 本文提出了TextualVerifier，一种基于大语言模型的自验证框架，用于提升TextGrad文本自动微分系统的推理有效性和优化性能。


<details>
  <summary>Details</summary>
Motivation: 虽然TextGrad支持文本自动微分优化，但缺乏推理有效性的自验证机制，限制了其决策可靠性。

Method: TextualVerifier采用链式思维分解、变体生成、多数投票和共识聚合四阶段流程，结合大语言模型实现推理步骤的验证，非侵入式集成于TextGrad的损失函数和优化结果验证环节。

Result: 在PRM800K数据集的单独评测中，推理有效性提升29%；与TextGrad集成后，在多个基准测试上准确率提升2.2个百分点，且多轮调用大语言模型的平均开销较低。不同版本的TextualVerifier在多个测试集上均带来显著性能提升。

Conclusion: TextualVerifier开创了基于大语言模型的TextGrad自验证机制，无需数值梯度即可实现更可靠的文本推理和优化，为文本基优化中的验证问题开启新方向。

Abstract: TextGrad is a novel approach to text-based automatic differentiation that
enables composite AI systems to perform optimization without explicit numerical
equations. However, it currently lacks self-verification mechanisms that ensure
reasoning validity in text-based decision making. This research introduces
TextualVerifier, a verification framework that leverages chain-of-thought
reasoning and majority voting with large language models to address this
verification gap. TextualVerifier implements a four-stage workflow:
chain-of-thought decomposition, variant generation, majority voting, and
consensus aggregation. It integrates non-invasively with TextGrad at both the
loss function and optimization result verification stages. Experimental
evaluation using the Gemini 1.5 Pro model is conducted in two phases: (1)
standalone evaluation on PRM800K, and (2) integrated evaluation with TextGrad
on GPQA-Diamond, MMLU-ML, and MMLU-CP benchmarks. Results show statistically
significant improvements (p < 0.001). In phase one, TextualVerifier improves
the validity of reasoning steps by 29 percent. In phase two, integration into
TextGrad loss function yields a 2.2 percentage point gain from 68.2 to 70.4
percent with a moderate overhead of 5.9 LLM calls on average. Further
evaluations of TextualVerifier versioning yield 8.08, 10.71, and 3.92
percentage point improvements on GPQA, MMLU-ML, and MMLU-CP respectively.
TextualVerifier thus presents the first self-verification framework for
TextGrad through LLM-based techniques without requiring numerical gradients,
enabling more reliable reasoning and opening new directions for verification in
text-based optimization.

</details>


### [3] [GRDD+: An Extended Greek Dialectal Dataset with Cross-Architecture Fine-tuning Evaluation](https://arxiv.org/abs/2511.03772)
*Stergios Chatzikyriakidis,Dimitris Papadakis,Sevasti-Ioanna Papaioannou,Erofili Psaltaki*

Main category: cs.CL

TL;DR: 该论文介绍了扩展的希腊方言数据集GRDD+，包含10种方言和超过630万词汇，并通过微调多种大语言模型评估方言数据质量的影响。


<details>
  <summary>Details</summary>
Motivation: 现有希腊方言数据集有限，缺乏多样性和规模，难以支持大语言模型的方言识别与生成能力。

Method: 构建了包含10种希腊方言、共计637万词的GRDD+数据集，并对三种模型架构（Llama-3-8B、Llama-3.1-8B、Krikri-8B）进行微调，同时与多种先进模型（Claude-3.7-Sonnet、Gemini-2.5、ChatGPT-5）结果进行比较。

Result: GRDD+数据集显著提升了模型在希腊方言理解和生成上的表现，证明了高质量多样化方言数据对于提升大语言模型性能的重要性。

Conclusion: GRDD+是迄今为止规模最大、种类最丰富的希腊方言数据集，其在推动希腊方言相关自然语言处理研究及大语言模型微调方面具有重要价值。

Abstract: We present an extended Greek Dialectal Dataset (GRDD+) 1that complements the
existing GRDD dataset with more data from Cretan, Cypriot, Pontic and Northern
Greek, while we add six new varieties: Greco-Corsican, Griko (Southern Italian
Greek), Maniot, Heptanesian, Tsakonian, and Katharevusa Greek. The result is a
dataset with total size 6,374,939 words and 10 varieties. This is the first
dataset with such variation and size to date. We conduct a number of
fine-tuning experiments to see the effect of good quality dialectal data on a
number of LLMs. We fine-tune three model architectures (Llama-3-8B,
Llama-3.1-8B, Krikri-8B) and compare the results to frontier models
(Claude-3.7-Sonnet, Gemini-2.5, ChatGPT-5).

</details>


### [4] [PLLuM: A Family of Polish Large Language Models](https://arxiv.org/abs/2511.03823)
*Jan Kocoń,Maciej Piasecki,Arkadiusz Janz,Teddy Ferdinan,Łukasz Radliński,Bartłomiej Koptyra,Marcin Oleksy,Stanisław Woźniak,Paweł Walkowiak,Konrad Wojtasik,Julia Moska,Tomasz Naskręt,Bartosz Walkowiak,Mateusz Gniewkowski,Kamil Szyc,Dawid Motyka,Dawid Banach,Jonatan Dalasiński,Ewa Rudnicka,Bartłomiej Alberski,Tomasz Walkowiak,Aleksander Szczęsny,Maciej Markiewicz,Tomasz Bernaś,Hubert Mazur,Kamil Żyta,Mateusz Tykierko,Grzegorz Chodak,Tomasz Kajdanowicz,Przemysław Kazienko,Agnieszka Karlińska,Karolina Seweryn,Anna Kołos,Maciej Chrabąszcz,Katarzyna Lorenc,Aleksandra Krasnodębska,Artur Wilczek,Katarzyna Dziewulska,Paula Betscher,Zofia Cieślińska,Katarzyna Kowol,Daria Mikoś,Maciej Trzciński,Dawid Krutul,Marek Kozłowski,Sławomir Dadas,Rafał Poświata,Michał Perełkiewicz,Małgorzata Grębowiec,Maciej Kazuła,Marcin Białas,Roman Roszko,Danuta Roszko,Jurgita Vaičenonienė,Andrius Utka,Paweł Levchuk,Paweł Kowalski,Irena Prawdzic-Jankowska,Maciej Ogrodniczuk,Monika Borys,Anna Bulińska,Wiktoria Gumienna,Witold Kieraś,Dorota Komosińska,Katarzyna Krasnowska-Kieraś,Łukasz Kobyliński,Martyna Lewandowska,Marek Łaziński,Mikołaj Łątkowski,Dawid Mastalerz,Beata Milewicz,Agnieszka Anna Mykowiecka,Angelika Peljak-Łapińska,Sandra Penno,Zuzanna Przybysz,Michał Rudolf,Piotr Rybak,Karolina Saputa,Aleksandra Tomaszewska,Aleksander Wawer,Marcin Woliński,Joanna Wołoszyn,Alina Wróblewska,Bartosz Żuk,Filip Żarnecki,Konrad Kaczyński,Anna Cichosz,Zuzanna Deckert,Monika Garnys,Izabela Grabarczyk,Wojciech Janowski,Sylwia Karasińska,Aleksandra Kujawiak,Piotr Misztela,Maria Szymańska,Karolina Walkusz,Igor Siek,Jakub Kwiatkowski,Piotr Pęzik*

Main category: cs.CL

TL;DR: PLLuM 是专门针对波兰语开发的最大开源大语言模型系列，包含1400亿词的预训练语料库和多种定制数据集，实现了高质量和文化相关的语言处理，支持波兰公共行政任务，并注重负责任的AI框架和安全过滤。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型主要以英语为中心，难以满足波兰语用户对高质量、透明且文化相关的语言模型的需求。

Method: 构建了包含1400亿词的波兰语文本语料库，设计了77k定制指令数据集和100k偏好优化数据集，采用负责任的AI框架进行数据治理与安全过滤，详细阐述模型架构、训练及对齐技术。

Result: 所开发的PLLuM模型在波兰语任务，尤其是公共行政领域表现出良好效果，支持多种基线和指令调优变体，模型公开发布促进开放研究。

Conclusion: PLLuM填补了波兰语大语言模型的空白，推动语言模型多语种发展，强化波兰自主AI技术，实现高质量且负责任的语言模型建设。

Abstract: Large Language Models (LLMs) play a central role in modern artificial
intelligence, yet their development has been primarily focused on English,
resulting in limited support for other languages. We present PLLuM (Polish
Large Language Model), the largest open-source family of foundation models
tailored specifically for the Polish language. Developed by a consortium of
major Polish research institutions, PLLuM addresses the need for high-quality,
transparent, and culturally relevant language models beyond the English-centric
commercial landscape. We describe the development process, including the
construction of a new 140-billion-token Polish text corpus for pre-training, a
77k custom instructions dataset, and a 100k preference optimization dataset. A
key component is a Responsible AI framework that incorporates strict data
governance and a hybrid module for output correction and safety filtering. We
detail the models' architecture, training procedures, and alignment techniques
for both base and instruction-tuned variants, and demonstrate their utility in
a downstream task within public administration. By releasing these models
publicly, PLLuM aims to foster open research and strengthen sovereign AI
technologies in Poland.

</details>


### [5] [BAPPA: Benchmarking Agents, Plans, and Pipelines for Automated Text-to-SQL Generation](https://arxiv.org/abs/2511.04153)
*Fahim Ahmed,Md Mubtasim Ahasan,Jahir Sadik Monon,Muntasir Wahed,M Ashraful Amin,A K M Mahbubur Rahman,Amin Ahsan Ali*

Main category: cs.CL

TL;DR: 本文研究了三种基于多智能体的大型语言模型（LLM）流水线用于文本到SQL的生成，以应对现有模型在大规模数据库和复杂推理上的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型难以有效处理大规模数据库结构和复杂SQL生成任务，且大多采用复杂且不实用的流水线，小型高效模型的潜力被忽视。

Method: 设计并测试了三种多智能体流水线：多智能体讨论流水线（迭代批评与改进SQL）、规划者-编码者流水线（规划者生成分步计划，编码者合成SQL）、编码者-聚合者流水线（多编码者生成SQL，推理者选择最佳查询）；并对多种模型进行系统性能评估。

Result: 在Bird-Bench Mini-Dev测试中，多轮讨论提升了小模型Qwen2.5-7b-Instruct的执行准确率最高达10.6%。LLM推理者-编码者流水线表现最好，DeepSeek-R1-32B和QwQ-32B提升Gemma 3 27B IT的准确率从52.4%到56.4%。

Conclusion: 结合多智能体协作机制的LLM流水线，尤其是推理者-编码者方案，能有效提升中小型模型在文本到SQL生成任务中的表现，为高效实用的自然语言数据库接口提供了有力支持。

Abstract: Text-to-SQL systems provide a natural language interface that can enable even
laymen to access information stored in databases. However, existing Large
Language Models (LLM) struggle with SQL generation from natural instructions
due to large schema sizes and complex reasoning. Prior work often focuses on
complex, somewhat impractical pipelines using flagship models, while smaller,
efficient models remain overlooked. In this work, we explore three multi-agent
LLM pipelines, with systematic performance benchmarking across a range of small
to large open-source models: (1) Multi-agent discussion pipeline, where agents
iteratively critique and refine SQL queries, and a judge synthesizes the final
answer; (2) Planner-Coder pipeline, where a thinking model planner generates
stepwise SQL generation plans and a coder synthesizes queries; and (3)
Coder-Aggregator pipeline, where multiple coders independently generate SQL
queries, and a reasoning agent selects the best query. Experiments on the
Bird-Bench Mini-Dev set reveal that Multi-Agent discussion can improve small
model performance, with up to 10.6% increase in Execution Accuracy for
Qwen2.5-7b-Instruct seen after three rounds of discussion. Among the pipelines,
the LLM Reasoner-Coder pipeline yields the best results, with DeepSeek-R1-32B
and QwQ-32B planners boosting Gemma 3 27B IT accuracy from 52.4% to the highest
score of 56.4%. Codes are available at
https://github.com/treeDweller98/bappa-sql.

</details>


### [6] [STARS: Segment-level Token Alignment with Rejection Sampling in Large Language Models](https://arxiv.org/abs/2511.03827)
*Mohammad Atif Quamar,Mohammad Areeb,Mikhail Kuznetsov,Muslum Ozgur Ozmen,Z. Berkay Celik*

Main category: cs.CL

TL;DR: 提出了一种名为STARS的解码时算法，通过分段采样和拒绝采样，提高大语言模型与人类价值对齐的效率和效果。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法要么计算昂贵（微调），要么推理时计算难以承受（Best-of-N采样），需要一种高效且精确的对齐方法。

Method: STARS算法通过迭代地采样、评分并拒绝/接受固定长度的短Token段，实现对生成过程的早期纠正，从而提升计算效率和对齐质量。

Result: 在六个大语言模型上，STARS相比监督微调提升最多14.9个百分点，优于直接偏好优化最多4.3个百分点，且性能与Best-of-N基线持平。

Conclusion: 基于分段粒度的奖励引导采样为对齐大语言模型提供了一种泛化性强、鲁棒且高效的替代方案。

Abstract: Aligning large language models with human values is crucial for their safe
deployment; however, existing methods, such as fine-tuning, are computationally
expensive and suboptimal. In contrast, inference-time approaches like Best-of-N
sampling require practically infeasible computation to achieve optimal
alignment. We propose STARS: Segment-level Token Alignment with Rejection
Sampling, a decoding-time algorithm that steers model generation by iteratively
sampling, scoring, and rejecting/accepting short, fixed-size token segments.
This allows for early correction of the generation path, significantly
improving computational efficiency and boosting alignment quality. Across a
suite of six LLMs, we show that STARS outperforms Supervised Fine-Tuning (SFT)
by up to 14.9 percentage points and Direct Preference Optimization (DPO) by up
to 4.3 percentage points on win-rates, while remaining highly competitive with
strong Best-of-N baselines. Our work establishes granular, reward-guided
sampling as a generalizable, robust, and efficient alternative to traditional
fine-tuning and full-sequence ranking methods for aligning LLMs.

</details>


### [7] [Computational Turing Test Reveals Systematic Differences Between Human and AI Language](https://arxiv.org/abs/2511.04195)
*Nicolò Pagan,Petter Törnberg,Christopher A. Bail,Anikó Hannák,Christopher Barrie*

Main category: cs.CL

TL;DR: 本文提出了一种结合多项指标和语言特征的计算图灵测试框架，用以验证大型语言模型（LLMs）生成文本的真实性，并比较了九种开源LLMs在五种校准策略下的表现，发现其生成文本仍与人类文本明显不同，且优化人类相似度与语义忠实度存在权衡。


<details>
  <summary>Details</summary>
Motivation: 当前社会科学中广泛使用的LLMs假设其能生成真实且类人文本，但缺乏有效的验证工具，现有人类评判方法不可靠，亟需一种客观、可扩展的验证和校准框架。

Method: 提出了一个结合BERT检测性、语义相似度、风格标记及话题模式的计算图灵测试框架；系统比较了九种开源LLMs在五种校准策略（如微调、风格提示、上下文检索）下模拟X、Bluesky和Reddit用户交互的能力。

Result: 校准后LLM输出仍能与人类文本明显区分，尤其在情感语调和情绪表达上；指令微调模型表现不及基础模型，增大模型规模也未提升类人性；优化人类相似度与语义忠实度存在明显权衡。

Conclusion: 本文提供了一个可扩展的用于LLMs文本真实性验证及校准的框架，揭示了当前模型在捕捉人类交流上的局限性以及人类相似度和语义忠实度之间的权衡问题，为未来改进提供了重要参考。

Abstract: Large language models (LLMs) are increasingly used in the social sciences to
simulate human behavior, based on the assumption that they can generate
realistic, human-like text. Yet this assumption remains largely untested.
Existing validation efforts rely heavily on human-judgment-based evaluations --
testing whether humans can distinguish AI from human output -- despite evidence
that such judgments are blunt and unreliable. As a result, the field lacks
robust tools for assessing the realism of LLM-generated text or for calibrating
models to real-world data. This paper makes two contributions. First, we
introduce a computational Turing test: a validation framework that integrates
aggregate metrics (BERT-based detectability and semantic similarity) with
interpretable linguistic features (stylistic markers and topical patterns) to
assess how closely LLMs approximate human language within a given dataset.
Second, we systematically compare nine open-weight LLMs across five calibration
strategies -- including fine-tuning, stylistic prompting, and context retrieval
-- benchmarking their ability to reproduce user interactions on X (formerly
Twitter), Bluesky, and Reddit. Our findings challenge core assumptions in the
literature. Even after calibration, LLM outputs remain clearly distinguishable
from human text, particularly in affective tone and emotional expression.
Instruction-tuned models underperform their base counterparts, and scaling up
model size does not enhance human-likeness. Crucially, we identify a trade-off:
optimizing for human-likeness often comes at the cost of semantic fidelity, and
vice versa. These results provide a much-needed scalable framework for
validation and calibration in LLM simulations -- and offer a cautionary note
about their current limitations in capturing human communication.

</details>


### [8] [Divide, Cache, Conquer: Dichotomic Prompting for Efficient Multi-Label LLM-Based Classification](https://arxiv.org/abs/2511.03830)
*Mikołaj Langner,Jan Eliasz,Ewa Rudnicka,Jan Kocoń*

Main category: cs.CL

TL;DR: 提出了一种将多标签文本分类任务转化为一系列二分决策的方法，实现了高效且准确的分类。


<details>
  <summary>Details</summary>
Motivation: 传统多标签分类生成所有标签效率低下，特别是在利用大型语言模型时。

Method: 将多标签分类拆分为多个独立的是/否查询，结合前缀缓存机制提高短文本推理效率。通过利用强大的注释模型DeepSeek-V3标注多维度情感数据，再对小模型进行蒸馏微调。

Result: 微调后的小模型在训练过的标签维度上性能显著优于零样本基线，展示了方法的有效性和效率。

Conclusion: 将多标签分类转化为二分查询、结合蒸馏和缓存推理是一种可扩展且高效的基于大语言模型的分类框架，适用于多领域。

Abstract: We introduce a method for efficient multi-label text classification with
large language models (LLMs), built on reformulating classification tasks as
sequences of dichotomic (yes/no) decisions. Instead of generating all labels in
a single structured response, each target dimension is queried independently,
which, combined with a prefix caching mechanism, yields substantial efficiency
gains for short-text inference without loss of accuracy. To demonstrate the
approach, we focus on affective text analysis, covering 24 dimensions including
emotions and sentiment. Using LLM-to-SLM distillation, a powerful annotator
model (DeepSeek-V3) provides multiple annotations per text, which are
aggregated to fine-tune smaller models (HerBERT-Large, CLARIN-1B, PLLuM-8B,
Gemma3-1B). The fine-tuned models show significant improvements over zero-shot
baselines, particularly on the dimensions seen during training. Our findings
suggest that decomposing multi-label classification into dichotomic queries,
combined with distillation and cache-aware inference, offers a scalable and
effective framework for LLM-based classification. While we validate the method
on affective states, the approach is general and applicable across domains.

</details>


### [9] [Evaluating Machine Translation Datasets for Low-Web Data Languages: A Gendered Lens](https://arxiv.org/abs/2511.03880)
*Hellina Hailu Nigatu,Bethelhem Yemane Mamo,Bontu Fufa Balcha,Debora Taye Tesfaye,Elbethel Daniel Zewdie,Ikram Behiru Nesiru,Jitu Ewnetu Hailu,Senait Mengesha Yayo*

Main category: cs.CL

TL;DR: 本文研究了三种低资源语言的机器翻译数据集质量，重点分析了性别偏见及有害内容。


<details>
  <summary>Details</summary>
Motivation: 随着低资源语言在NLP研究中的重要性增加，大量数据集被收集，但过分注重数量可能导致技术性能不佳及传播社会偏见。

Method: 分析三个低资源语言（Afan Oromo, Amharic, Tigrinya）机器翻译训练和测试数据的领域分布及性别代表性，包括人名、动词语法性别及刻板印象。

Result: 发现训练数据偏重政治和宗教文本，测试数据偏向新闻、健康和体育，数据中男性形象严重占比，有害和有毒的针对女性的内容明显，且数据量最多的语言问题最为突出。

Conclusion: 仅增加数据量无法保证数据质量，强调需关注低资源语言数据集的偏见和有害内容，促进早期干预和改进。

Abstract: As low-resourced languages are increasingly incorporated into NLP research,
there is an emphasis on collecting large-scale datasets. But in prioritizing
quantity over quality, we risk 1) building language technologies that perform
poorly for these languages and 2) producing harmful content that perpetuates
societal biases. In this paper, we investigate the quality of Machine
Translation (MT) datasets for three low-resourced languages--Afan Oromo,
Amharic, and Tigrinya, with a focus on the gender representation in the
datasets. Our findings demonstrate that while training data has a large
representation of political and religious domain text, benchmark datasets are
focused on news, health, and sports. We also found a large skew towards the
male gender--in names of persons, the grammatical gender of verbs, and in
stereotypical depictions in the datasets. Further, we found harmful and toxic
depictions against women, which were more prominent for the language with the
largest amount of data, underscoring that quantity does not guarantee quality.
We hope that our work inspires further inquiry into the datasets collected for
low-resourced languages and prompts early mitigation of harmful content.
WARNING: This paper contains discussion of NSFW content that some may find
disturbing.

</details>


### [10] [GRAD: Graph-Retrieved Adaptive Decoding for Hallucination Mitigation](https://arxiv.org/abs/2511.03900)
*Manh Nguyen,Sunil Gupta,Dai Do,Hung Le*

Main category: cs.CL

TL;DR: 提出了GRAD方法，通过在解码时利用语料库中词汇的转移图来减少大语言模型的幻觉现象，提高生成内容的真实性与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有减少大语言模幻觉的方法依赖外部知识库和提示，存在脆弱性和高成本问题，故需一种轻量且效果好的解决方案。

Method: GRAD在解码时构建稀疏的词汇转移图，将检索语料中的next-token logits累积后进行归一化，并融合模型原始logits，实现生成词汇的证据驱动自适应解码。

Result: 在三个模型和多个问答基准测试中，GRAD提升内在准确率9.7%，降低幻觉率8.6%，正确率提升6.9%，在真理与信息性综合评分中表现最佳。

Conclusion: GRAD作为一种无需重训练的轻量级解码策略，成功利用语料库的统计证据引导生成过程，有效减少幻觉，提升生成文本的真实性和可验证性。

Abstract: Hallucination mitigation remains a persistent challenge for large language
models (LLMs), even as model scales grow. Existing approaches often rely on
external knowledge sources, such as structured databases or knowledge graphs,
accessed through prompting or retrieval. However, prompt-based grounding is
fragile and domain-sensitive, while symbolic knowledge integration incurs heavy
retrieval and formatting costs. Motivated by knowledge graphs, we introduce
Graph-Retrieved Adaptive Decoding (GRAD), a decoding-time method that grounds
generation in corpus-derived evidence without retraining. GRAD constructs a
sparse token transition graph by accumulating next-token logits across a small
retrieved corpus in a single forward pass. During decoding, graph-retrieved
logits are max-normalized and adaptively fused with model logits to favor
high-evidence continuations while preserving fluency. Across three models and a
range of question-answering benchmarks spanning intrinsic, extrinsic
hallucination, and factuality tasks, GRAD consistently surpasses baselines,
achieving up to 9.7$\%$ higher intrinsic accuracy, 8.6$\%$ lower hallucination
rates, and 6.9$\%$ greater correctness compared to greedy decoding, while
attaining the highest truth--informativeness product score among all methods.
GRAD offers a lightweight, plug-and-play alternative to contrastive decoding
and knowledge graph augmentation, demonstrating that statistical evidence from
corpus-level token transitions can effectively steer generation toward more
truthful and verifiable outputs.

</details>


### [11] [Context informs pragmatic interpretation in vision-language models](https://arxiv.org/abs/2511.03908)
*Alvin Wei Ming Tan,Ben Prystawski,Veronica Boyce,Michael C. Frank*

Main category: cs.CL

TL;DR: 本文评估了人类与视觉语言模型在迭代指称游戏中的上下文敏感语用推理能力。


<details>
  <summary>Details</summary>
Motivation: 迭代指称游戏要求代理在多轮语言环境中进行上下文敏感的语用推理，测试其语言理解和实践能力。

Method: 通过调整上下文的信息量、顺序和相关性，比较人类与视觉语言模型在多轮指称游戏中的表现。

Result: 在无相关上下文时，模型表现虽高于随机但远逊于人类；有相关上下文时，模型性能明显提高，但抽象指称的少样本游戏仍困难。

Conclusion: 视觉语言模型在上下文丰富的情况下能显著提升语用推理能力，但面对抽象少样本任务仍存在挑战。

Abstract: Iterated reference games - in which players repeatedly pick out novel
referents using language - present a test case for agents' ability to perform
context-sensitive pragmatic reasoning in multi-turn linguistic environments. We
tested humans and vision-language models on trials from iterated reference
games, varying the given context in terms of amount, order, and relevance.
Without relevant context, models were above chance but substantially worse than
humans. However, with relevant context, model performance increased
dramatically over trials. Few-shot reference games with abstract referents
remain a difficult task for machine learning models.

</details>


### [12] [The Human Flourishing Geographic Index: A County-Level Dataset for the United States, 2013--2023](https://arxiv.org/abs/2511.03915)
*Stefano M. Iacus,Devika Jain,Andrea Nasuto,Giuseppe Porro,Marcello Carammia,Andrea Vezzulli*

Main category: cs.CL

TL;DR: 本文提出了基于推特数据和大语言模型分析的人类繁荣地理指数（HFGI），用于细致量化美国社会的多维度繁荣状况。


<details>
  <summary>Details</summary>
Motivation: 传统的人类繁荣测量缺乏细致的空间和时间分辨率，难以深入理解社会福祉的动态变化。

Method: 利用约26亿条带地理定位的美国推特数据（2013-2023），通过微调大语言模型对48个指标进行分类，这些指标依据哈佛全球繁荣研究框架，并结合对移民态度和腐败感知的分析。

Result: 生成了每月和每年层级的县及州级别繁荣指标，数据经过验证显示能准确反映繁荣的相关构念，并与现有指标有预期的相关性。

Conclusion: 该指数提供了高分辨率、多维度的社会福祉测量工具，支持跨学科研究社会变迁和不平等，揭示过去十年美国社交媒体中人类繁荣的动态特征。

Abstract: Quantifying human flourishing, a multidimensional construct including
happiness, health, purpose, virtue, relationships, and financial stability, is
critical for understanding societal well-being beyond economic indicators.
Existing measures often lack fine spatial and temporal resolution. Here we
introduce the Human Flourishing Geographic Index (HFGI), derived from analyzing
approximately 2.6 billion geolocated U.S. tweets (2013-2023) using fine-tuned
large language models to classify expressions across 48 indicators aligned with
Harvard's Global Flourishing Study framework plus attitudes towards migration
and perception of corruption. The dataset offers monthly and yearly county- and
state-level indicators of flourishing-related discourse, validated to confirm
that the measures accurately represent the underlying constructs and show
expected correlations with established indicators. This resource enables
multidisciplinary analyses of well-being, inequality, and social change at
unprecedented resolution, offering insights into the dynamics of human
flourishing as reflected in social media discourse across the United States
over the past decade.

</details>


### [13] [Direct Semantic Communication Between Large Language Models via Vector Translation](https://arxiv.org/abs/2511.03945)
*Fu-Chun Yang,Jason Eshraghian*

Main category: cs.CL

TL;DR: 本文提出通过向量翻译在大语言模型间建立潜在语义桥梁，实现语义级的信息交换。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体交互中，模型仅传递文本令牌，丢失大量潜在语义，限制信息传递效率且增加计算负担。

Method: 采用双编码器翻译器训练模型间的向量映射，在Llama-2-7B与Mistral-7B-Instruct间进行语义向量转换，并以30%强度注入目标模型。

Result: 双编码器实现平均余弦对齐0.538，注入向量稳定生成效果，且评估显示通用模型间语义转移效率优于指令调优模型。

Conclusion: 通过保守的语义向量注入，证明了跨模型潜在语义通信的可行性，为多模型协同AI系统提供了共享语义的新途径。

Abstract: In multi-agent settings, such as debate, reflection, or tool-calling, large
language models (LLMs) pass messages as plain tokens, discarding most latent
semantics. This constrains information transfer and adds unnecessary
computational overhead. We form a latent bridge via vector translations, which
use learned mappings that enable direct semantic exchange between
representation spaces. A dual-encoder translator trained between Llama-2-7B and
Mistral-7B-Instruct attains an average cosine alignment of 0.538. Injecting the
translated vectors at 30 percent blending strength steers the target model's
generation without destabilizing logits. Bidirectional evaluation shows a
2.01:1 transfer asymmetry, indicating that general-purpose models yield more
transferable representations than instruction-tuned variants. This conservative
injection preserves computational stability while demonstrating that
cross-model latent communication is feasible, enabling collaborative AI systems
that share meaning rather than tokens.

</details>


### [14] [Abductive Inference in Retrieval-Augmented Language Models: Generating and Validating Missing Premises](https://arxiv.org/abs/2511.04020)
*Shiyin Lin*

Main category: cs.CL

TL;DR: 本文提出了一种将溯因推理融入基于检索增强的大型语言模型的方法，以解决检索证据不完整导致推理缺失的问题。


<details>
  <summary>Details</summary>
Motivation: 检索增强的大型语言模型在知识密集型任务中表现突出，但当检索到的证据不完整时，会导致推理过程出现断裂，需要引入溯因推理来生成合理的缺失前提以弥补推理空白。

Method: 该方法通过检测证据不足，生成候选缺失前提，并通过一致性和合理性检查对其进行验证，将溯因推理集成到检索增强的语言模型中。

Result: 在溯因推理和多跳问答基准测试中，该方法提高了答案准确性和推理的可信度。

Conclusion: 溯因推理为提升检索增强生成系统的鲁棒性和可解释性提供了有前景的方向。

Abstract: Large Language Models (LLMs) enhanced with retrieval -- commonly referred to
as Retrieval-Augmented Generation (RAG) -- have demonstrated strong performance
in knowledge-intensive tasks. However, RAG pipelines often fail when retrieved
evidence is incomplete, leaving gaps in the reasoning process. In such cases,
\emph{abductive inference} -- the process of generating plausible missing
premises to explain observations -- offers a principled approach to bridge
these gaps. In this paper, we propose a framework that integrates abductive
inference into retrieval-augmented LLMs. Our method detects insufficient
evidence, generates candidate missing premises, and validates them through
consistency and plausibility checks. Experimental results on abductive
reasoning and multi-hop QA benchmarks show that our approach improves both
answer accuracy and reasoning faithfulness. This work highlights abductive
inference as a promising direction for enhancing the robustness and
explainability of RAG systems.

</details>


### [15] [WST: Weakly Supervised Transducer for Automatic Speech Recognition](https://arxiv.org/abs/2511.04035)
*Dongji Gao,Chenda Liao,Changliang Liu,Matthew Wiesner,Leibny Paola Garcia,Daniel Povey,Sanjeev Khudanpur,Jian Wu*

Main category: cs.CL

TL;DR: 该论文提出了一种弱监督转录器（WST），用以解决自动语音识别中对高质量标注数据依赖严重的问题，通过设计灵活的训练图来处理转录错误，提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 自动语音识别中，RNN-T模型对大规模高质量标注数据依赖大，获取此类数据成本高且困难。需设计一种能够容忍转录错误的弱监督方法以减轻这种依赖。

Method: 提出一种弱监督转录器（WST），采用灵活的训练图来鲁棒地处理转录错误，不依赖额外的置信度估计或预训练辅助模型。

Result: 在合成和工业数据集上实验证明WST在转录错误率高达70%时依然表现良好，且优于基于CTC的弱监督方法如BTC和OTC。

Conclusion: WST方法在实际自动语音识别环境中展现了良好的实用性和鲁棒性，显著降低了对高质量标注数据的需求。

Abstract: The Recurrent Neural Network-Transducer (RNN-T) is widely adopted in
end-to-end (E2E) automatic speech recognition (ASR) tasks but depends heavily
on large-scale, high-quality annotated data, which are often costly and
difficult to obtain. To mitigate this reliance, we propose a Weakly Supervised
Transducer (WST), which integrates a flexible training graph designed to
robustly handle errors in the transcripts without requiring additional
confidence estimation or auxiliary pre-trained models. Empirical evaluations on
synthetic and industrial datasets reveal that WST effectively maintains
performance even with transcription error rates of up to 70%, consistently
outperforming existing Connectionist Temporal Classification (CTC)-based weakly
supervised approaches, such as Bypass Temporal Classification (BTC) and
Omni-Temporal Classification (OTC). These results demonstrate the practical
utility and robustness of WST in realistic ASR settings. The implementation
will be publicly available.

</details>


### [16] [T-FIX: Text-Based Explanations with Features Interpretable to eXperts](https://arxiv.org/abs/2511.04070)
*Shreya Havaldar,Helen Jin,Chaehyeon Kim,Anton Xue,Weiqiu You,Marco Gatti,Bhuvnesh Jain,Helen Qu,Daniel A Hashimoto,Amin Madani,Rajat Deo,Sameed Ahmed M. Khatana,Gary E. Weissman,Lyle Ungar,Eric Wong*

Main category: cs.CL

TL;DR: 本论文提出了T-FIX基准，用于评估大型语言模型解释是否与领域专家的直觉相符，涵盖七个知识密集型领域，并开发了新的评估指标。


<details>
  <summary>Details</summary>
Motivation: 在知识密集型领域中，用户（通常是领域专家）除了答案外，还需要具有专家级推理的有意义解释。然而现有评估方法侧重于解释的表面合理性，无法体现解释内容与专家直觉的一致性。

Method: 提出了专家对齐的评价标准，设计了T-FIX基准，涵盖七个知识密集领域，并与领域专家合作开发度量解释与专家判断一致性的指标。

Result: 成功建立了T-FIX基准和多项新的专家对齐评估指标，能够更准确地衡量大型语言模型解释的专家一致性。

Conclusion: 通过引入专家对齐评价标准，本研究推动了对LLM解释质量的更深层次评估，满足了知识密集场景中领域专家的需求。

Abstract: As LLMs are deployed in knowledge-intensive settings (e.g., surgery,
astronomy, therapy), users expect not just answers, but also meaningful
explanations for those answers. In these settings, users are often domain
experts (e.g., doctors, astrophysicists, psychologists) who require
explanations that reflect expert-level reasoning. However, current evaluation
schemes primarily emphasize plausibility or internal faithfulness of the
explanation, which fail to capture whether the content of the explanation truly
aligns with expert intuition. We formalize expert alignment as a criterion for
evaluating explanations with T-FIX, a benchmark spanning seven
knowledge-intensive domains. In collaboration with domain experts, we develop
novel metrics to measure the alignment of LLM explanations with expert
judgment.

</details>


### [17] [Plan of Knowledge: Retrieval-Augmented Large Language Models for Temporal Knowledge Graph Question Answering](https://arxiv.org/abs/2511.04072)
*Xinying Qian,Ying Zhang,Yu Zhao,Baohang Zhou,Xuhui Sui,Xiaojie Yuan*

Main category: cs.CL

TL;DR: 本论文提出了PoK框架，通过分解复杂时间问题和对比时序检索显著提升大语言模型在时间知识图问答中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分理解时间约束的复杂语义信息，大语言模型在时间推理上能力有限且易产生幻觉。

Method: 设计了知识规划模块，将复杂问题拆为子目标，并构建对比检索的时序知识库，实现结构化规划与时序知识检索结合。

Result: 在四个基准数据集上，PoK提升了大语言模型的检索精度和推理准确率，最高超过现有最先进方法56%。

Conclusion: PoK框架通过结构化规划和对比时序检索有效增强了时间知识图问答中大语言模型的解释性和事实一致性。

Abstract: Temporal Knowledge Graph Question Answering (TKGQA) aims to answer
time-sensitive questions by leveraging factual information from Temporal
Knowledge Graphs (TKGs). While previous studies have employed pre-trained TKG
embeddings or graph neural networks to inject temporal knowledge, they fail to
fully understand the complex semantic information of time constraints.
Recently, Large Language Models (LLMs) have shown remarkable progress,
benefiting from their strong semantic understanding and reasoning
generalization capabilities. However, their temporal reasoning ability remains
limited. LLMs frequently suffer from hallucination and a lack of knowledge. To
address these limitations, we propose the Plan of Knowledge framework with a
contrastive temporal retriever, which is named PoK. Specifically, the proposed
Plan of Knowledge module decomposes a complex temporal question into a sequence
of sub-objectives from the pre-defined tools, serving as intermediate guidance
for reasoning exploration. In parallel, we construct a Temporal Knowledge Store
(TKS) with a contrastive retrieval framework, enabling the model to selectively
retrieve semantically and temporally aligned facts from TKGs. By combining
structured planning with temporal knowledge retrieval, PoK effectively enhances
the interpretability and factual consistency of temporal reasoning. Extensive
experiments on four benchmark TKGQA datasets demonstrate that PoK significantly
improves the retrieval precision and reasoning accuracy of LLMs, surpassing the
performance of the state-of-the-art TKGQA methods by 56.0% at most.

</details>


### [18] [The truth is no diaper: Human and AI-generated associations to emotional words](https://arxiv.org/abs/2511.04077)
*Špela Vintar,Jan Jona Javoršek*

Main category: cs.CL

TL;DR: 本研究比较了人类与大型语言模型（LLMs）对词语联想的行为，特别是情感负载词的联想，发现LLMs的联想与人类有一定重叠，但更具情感放大效应，且联想更可预测、创造性较低。


<details>
  <summary>Details</summary>
Motivation: 传统的人类词语联想方法虽能揭示心理词汇，但受个体经验、情绪和认知风格影响较大。探索LLMs是否能模拟人类联想特别是在情感词方面，对于理解其创造力机制有重要意义。

Method: 通过比较人类与LLMs对带有情感负载词语的联想响应，分析二者联想的相似度、情感放大效应及创造性特征。

Result: 发现LLMs与人类联想有中等程度的重叠，LLMs倾向于增强词语的情感负载，其联想更加可预测且创造性低于人类。

Conclusion: 虽然LLMs能模拟部分人类词语联想，但其联想表现出更强的情感放大和较低创造性，提示其联想机制与人类仍存在显著差异。

Abstract: Human word associations are a well-known method of gaining insight into the
internal mental lexicon, but the responses spontaneously offered by human
participants to word cues are not always predictable as they may be influenced
by personal experience, emotions or individual cognitive styles. The ability to
form associative links between seemingly unrelated concepts can be the driving
mechanisms of creativity. We perform a comparison of the associative behaviour
of humans compared to large language models. More specifically, we explore
associations to emotionally loaded words and try to determine whether large
language models generate associations in a similar way to humans. We find that
the overlap between humans and LLMs is moderate, but also that the associations
of LLMs tend to amplify the underlying emotional load of the stimulus, and that
they tend to be more predictable and less creative than human ones.

</details>


### [19] [Improving the Performance of Radiology Report De-identification with Large-Scale Training and Benchmarking Against Cloud Vendor Methods](https://arxiv.org/abs/2511.04079)
*Eva Prakash,Maayane Attias,Pierre Chambon,Justin Xu,Steven Truong,Jean-Benoit Delbrouck,Tessa Cook,Curtis Langlotz*

Main category: cs.CL

TL;DR: 基于Transformer的放射学报告自动脱敏模型，通过大规模多模态数据训练，实现了跨机构的高精度PHI检测，优于现有学术和商业系统。


<details>
  <summary>Details</summary>
Motivation: 提升放射学报告自动脱敏的准确性与跨机构通用性，保障受保护健康信息（PHI）隐私，同时实现数据使用价值最大化。

Method: 基于Transformer架构，利用斯坦福大学两大放射学语料库进行微调，扩展PHI类别引入年龄信息，在斯坦福和宾夕法尼亚大学测试集上评估性能，并与商业云服务系统比较，同时验证合成PHI生成的稳定性。

Result: 模型在宾夕法尼亚数据集F1得分达到0.973，斯坦福数据集达到0.996，超过或匹配了先前最先进模型。对合成PHI的检测稳定且准确，优于所有商业系统（F1: 0.960 vs. 0.632-0.754）。

Conclusion: 多模态大规模训练显著提升了模型跨机构泛化能力和稳健性，合成PHI技术兼顾隐私保护与数据利用，提出的Transformer脱敏模型树立了临床文本隐私保护的新标杆。

Abstract: Objective: To enhance automated de-identification of radiology reports by
scaling transformer-based models through extensive training datasets and
benchmarking performance against commercial cloud vendor systems for protected
health information (PHI) detection. Materials and Methods: In this
retrospective study, we built upon a state-of-the-art, transformer-based, PHI
de-identification pipeline by fine-tuning on two large annotated radiology
corpora from Stanford University, encompassing chest X-ray, chest CT,
abdomen/pelvis CT, and brain MR reports and introducing an additional PHI
category (AGE) into the architecture. Model performance was evaluated on test
sets from Stanford and the University of Pennsylvania (Penn) for token-level
PHI detection. We further assessed (1) the stability of synthetic PHI
generation using a "hide-in-plain-sight" method and (2) performance against
commercial systems. Precision, recall, and F1 scores were computed across all
PHI categories. Results: Our model achieved overall F1 scores of 0.973 on the
Penn dataset and 0.996 on the Stanford dataset, outperforming or maintaining
the previous state-of-the-art model performance. Synthetic PHI evaluation
showed consistent detectability (overall F1: 0.959 [0.958-0.960]) across 50
independently de-identified Penn datasets. Our model outperformed all vendor
systems on synthetic Penn reports (overall F1: 0.960 vs. 0.632-0.754).
Discussion: Large-scale, multimodal training improved cross-institutional
generalization and robustness. Synthetic PHI generation preserved data utility
while ensuring privacy. Conclusion: A transformer-based de-identification model
trained on diverse radiology datasets outperforms prior academic and commercial
systems in PHI detection and establishes a new benchmark for secure clinical
text processing.

</details>


### [20] [A Characterization of List Language Identification in the Limit](https://arxiv.org/abs/2511.04103)
*Moses Charikar,Chirag Pabbaraju,Ambuj Tewari*

Main category: cs.CL

TL;DR: 本文研究了在语言识别极限问题中文本语言识别的可能性，提出并精确刻画了允许输出k个猜测列表的新模型。


<details>
  <summary>Details</summary>
Motivation: 经典理论证明语言识别在极限情况下通常不可能，但近期语言生成问题取得了积极进展，启发作者引入多猜测列表模型重新探讨语言识别极限问题。

Method: 基于Angluin的单猜测列表识别理论，提出递归版本的k列表识别理论，给出语言集合可k列表识别的精确刻画，还推导了统计设置下的识别速率。

Result: 表明一个语言集合能k列表识别当且仅当它能被分解为k个可单列表识别的子集合，且k列表识别在统计设置下的识别速率最佳为指数级；非k列表识别集合无法实现任何趋零的错误率。

Conclusion: k列表识别扩展了经典语言识别的理论边界，提供了一种有效解决语言识别极限问题的方法，且在统计学习框架下达到了理论最优的识别速率。

Abstract: We study the problem of language identification in the limit, where given a
sequence of examples from a target language, the goal of the learner is to
output a sequence of guesses for the target language such that all the guesses
beyond some finite time are correct. Classical results of Gold showed that
language identification in the limit is impossible for essentially any
interesting collection of languages. Later, Angluin gave a precise
characterization of language collections for which this task is possible.
Motivated by recent positive results for the related problem of language
generation, we revisit the classic language identification problem in the
setting where the learner is given the additional power of producing a list of
$k$ guesses at each time step. The goal is to ensure that beyond some finite
time, one of the guesses is correct at each time step.
  We give an exact characterization of collections of languages that can be
$k$-list identified in the limit, based on a recursive version of Angluin's
characterization (for language identification with a list of size $1$). This
further leads to a conceptually appealing characterization: A language
collection can be $k$-list identified in the limit if and only if the
collection can be decomposed into $k$ collections of languages, each of which
can be identified in the limit (with a list of size $1$). We also use our
characterization to establish rates for list identification in the statistical
setting where the input is drawn as an i.i.d. stream from a distribution
supported on some language in the collection. Our results show that if a
collection is $k$-list identifiable in the limit, then the collection can be
$k$-list identified at an exponential rate, and this is best possible. On the
other hand, if a collection is not $k$-list identifiable in the limit, then it
cannot be $k$-list identified at any rate that goes to zero.

</details>


### [21] [Batch Prompting Suppresses Overthinking Reasoning Under Constraint: How Batch Prompting Suppresses Overthinking in Reasoning Models](https://arxiv.org/abs/2511.04108)
*Wenmo Qiu,Saurabh Srivastava*

Main category: cs.CL

TL;DR: 本文研究了批处理提示在大型语言模型中的作用，发现批处理不仅降低推理成本，还能改善多步骤推理的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 探索批处理提示在大型推理模型中的额外优势，超越传统的成本节约效果。

Method: 在13个多样化基准测试中，比较批处理与非批处理的表现，通过行为分析揭示批处理对模型决策和推理过程的影响。

Result: 批处理显著提升了准确率，减少了推理所需的token数3到5倍，抑制了过度思考和语言犹豫，提高了回答的果断性，并展现出批次内样例相互促进的集体现象。

Conclusion: 批处理不仅是提升吞吐量的优化手段，更是推理时段有效的正则化策略，有助于构建高效可靠的推理模型。

Abstract: Recent work has explored batch prompting as a strategy to amortize inference
cost in large language models (LLMs). In this paper, we show that batching
offers an additional, underappreciated benefit: it regularizes model behavior
during multi-step reasoning for Large Reasoning Models (LRMs). We conduct a
comprehensive study across 13 diverse benchmarks and observe that batching
improves accuracy while substantially reducing reasoning token usage, often by
3x-5x. Through detailed behavioral analysis, we find that batching suppresses
overthinking, reduces hedging language (e.g., repetitive self-corrections), and
encourages more decisive answers. Surprisingly, we also observe emergent
collective effects in batched inference: models often generalize patterns from
earlier examples to solve harder ones in the same batch. These findings
position batching not just as a throughput optimization, but as a powerful
inference-time regularizer for more efficient and reliable LLM reasoning.

</details>


### [22] [RIDE: Difficulty Evolving Perturbation with Item Response Theory for Mathematical Reasoning](https://arxiv.org/abs/2511.04120)
*Xinyuan Li,Murong Xu,Wenbiao Tao,Hanlun Zhu,Yike Zhao,Jipeng Zhang,Yunshi Lan*

Main category: cs.CL

TL;DR: 本文提出了基于对抗性问题重写的评估框架RIDE，用以更准确地测量大语言模型在数学推理上的真实能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在数学推理上的高表现可能因训练数据泄露或浅层模式匹配而夸大，传统基于规则的扰动方法存在生成不恰当问题的问题，难以系统评估题目难度和基准发展。

Method: 提出RIDE框架，结合项目反应理论（IRT）通过35个大语言模型模拟学生回答构建难度排序器，利用强化学习指导问题重写模型，生成难度适中的、合理的问题变体。

Result: RIDE在竞赛级数学基准测试中生成的扰动问题使26个大语言模型的平均表现下降21.73%，揭示了模型在数学推理上的鲁棒性不足。

Conclusion: RIDE有效暴露了当前大语言模型数学推理的局限性，验证了基于对抗扰动的评估方法的合理性和必要性，为后续提升模型能力提供了有力工具。

Abstract: Large language models (LLMs) achieve high performance on mathematical
reasoning, but these results can be inflated by training data leakage or
superficial pattern matching rather than genuine reasoning. To this end, an
adversarial perturbation-based evaluation is needed to measure true
mathematical reasoning ability. Current rule-based perturbation methods often
generate ill-posed questions and impede the systematic evaluation of question
difficulty and the evolution of benchmarks. To bridge this gap, we propose
RIDE, a novel adversarial question-rewriting framework that leverages Item
Response Theory (IRT) to rigorously measure question difficulty and to generate
intrinsically more challenging, well-posed variations of mathematical problems.
We employ 35 LLMs to simulate students and build a difficulty ranker from their
responses. This ranker provides a reward signal during reinforcement learning
and guides a question-rewriting model to reformulate existing questions across
difficulty levels. Applying RIDE to competition-level mathematical benchmarks
yields perturbed versions that degrade advanced LLM performance, with
experiments showing an average 21.73% drop across 26 models, thereby exposing
limited robustness in mathematical reasoning and confirming the validity of our
evaluation approach.

</details>


### [23] [CantoASR: Prosody-Aware ASR-LALM Collaboration for Low-Resource Cantonese](https://arxiv.org/abs/2511.04139)
*Dazhong Chen,Yi-Cheng Lin,Yuchen Huang,Ziwei Gong,Di Jiang,Zeying Xie,Yi R.,Fung*

Main category: cs.CL

TL;DR: 本文提出CantoASR，一种结合强制对齐、LoRA微调的Whisper和指令微调Qwen-Audio的粤语自动语音识别错误纠正框架，显著提升了粤语低资源环境下的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 粤语低资源环境下，受限的标注数据、多音调及口音变化导致现有自动语音识别模型表现较差。

Method: 通过强制对齐提取声学特征，LoRA微调Whisper改善音调辨识能力，并用指令调优的Qwen-Audio进行韵律感知纠错，构建协作的ASR-LALM错误纠正框架。

Result: 在自发粤语语料上的评测显示，所提方法在字符错误率（CER）上较Whisper-Large-V3有显著提升。

Conclusion: 融合声学线索与大规模音频语言模型的推理能力，为低资源、多音调和方言的自动语音识别提供了可扩展的有效策略。

Abstract: Automatic speech recognition (ASR) is critical for language accessibility,
yet low-resource Cantonese remains challenging due to limited annotated data,
six lexical tones, tone sandhi, and accent variation. Existing ASR models, such
as Whisper, often suffer from high word error rates. Large audio-language
models (LALMs), in contrast, can leverage broader contextual reasoning but
still require explicit tonal and prosodic acoustic cues. We introduce CantoASR,
a collaborative ASR-LALM error correction framework that integrates forced
alignment for acoustic feature extraction, a LoRA-finetuned Whisper for
improved tone discrimination, and an instruction-tuned Qwen-Audio for
prosody-aware correction. Evaluations on spontaneous Cantonese data show
substantial CER gains over Whisper-Large-V3. These findings suggest that
integrating acoustic cues with LALM reasoning provides a scalable strategy for
low-resource tonal and dialectal ASR.

</details>


### [24] [Trustworthy LLM-Mediated Communication: Evaluating Information Fidelity in LLM as a Communicator (LAAC) Framework in Multiple Application Domains](https://arxiv.org/abs/2511.04184)
*Mohammed Musthafa Rafi,Adarsh Krishnamurthy,Aditya Balu*

Main category: cs.CL

TL;DR: 该论文提出了LAAC，即将大型语言模型（LLM）作为智能沟通中介，解决AI生成内容膨胀和压缩带来的沟通虚假问题，促进真实知识交流。


<details>
  <summary>Details</summary>
Motivation: 现有的AI生成内容使沟通变得冗长且失真，发送者和接收者均未与真实内容互动，需一种新范式改善沟通真实性和效果。

Method: LAAC通过结构化对话捕捉发送者意图，作为智能中介促进多领域真实沟通，并系统评估信息捕获准确性、一致性和响应可靠性三大信任维度。

Result: 通过多场景受控实验，发现当前LAAC在信息捕获、知识一致性及回复完整性方面存在显著信任缺口。

Conclusion: 在解决信任缺口前，LAAC尚无法在高风险沟通场景中可靠部署，但该方法为真实可信的AI辅助沟通提供了新的发展方向。

Abstract: The proliferation of AI-generated content has created an absurd communication
theater where senders use LLMs to inflate simple ideas into verbose content,
recipients use LLMs to compress them back into summaries, and as a consequence
neither party engage with authentic content. LAAC (LLM as a Communicator)
proposes a paradigm shift - positioning LLMs as intelligent communication
intermediaries that capture the sender's intent through structured dialogue and
facilitate genuine knowledge exchange with recipients. Rather than perpetuating
cycles of AI-generated inflation and compression, LAAC enables authentic
communication across diverse contexts including academic papers, proposals,
professional emails, and cross-platform content generation. However, deploying
LLMs as trusted communication intermediaries raises critical questions about
information fidelity, consistency, and reliability. This position paper
systematically evaluates the trustworthiness requirements for LAAC's deployment
across multiple communication domains. We investigate three fundamental
dimensions: (1) Information Capture Fidelity - accuracy of intent extraction
during sender interviews across different communication types, (2)
Reproducibility - consistency of structured knowledge across multiple
interaction instances, and (3) Query Response Integrity - reliability of
recipient-facing responses without hallucination, source conflation, or
fabrication. Through controlled experiments spanning multiple LAAC use cases,
we assess these trust dimensions using LAAC's multi-agent architecture.
Preliminary findings reveal measurable trust gaps that must be addressed before
LAAC can be reliably deployed in high-stakes communication scenarios.

</details>


### [25] [LLM-as-a-Judge is Bad, Based on AI Attempting the Exam Qualifying for the Member of the Polish National Board of Appeal](https://arxiv.org/abs/2511.04205)
*Michał Karp,Anna Kubaszewska,Magdalena Król,Robert Król,Aleksander Smywiński-Pohl,Mateusz Szymański,Witold Wydmański*

Main category: cs.CL

TL;DR: 该研究评估当前大型语言模型（LLMs）能否通过波兰国家申诉法庭的官方资格考试。结果显示模型在知识测试中表现尚可，但未通过实务书面考试，且模型间评判出现偏差。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在法律领域尤其是公共采购法律考试中的应用潜力及局限，评估其是否能替代人类法官或独立考官。

Method: 设计包含多选知识测试和书面判决的考试，测试包括GPT-4.1等多种LLM模型，采用闭卷和检索增强生成方法，并尝试"LLM作为法官"自动评判模型答案的方案。

Result: LLMs在知识测试中得分尚可，但实务书面判决部分未达合格线，且由其他模型自动评价的结果与官方评审组存在明显出入，暴露出模型错误引用法律条款、逻辑推理薄弱等缺陷。

Conclusion: 尽管技术快速发展，现有LLMs仍无法取代波兰公共采购裁决中的人类法官或独立考官，需法律专家与技术团队密切合作改进模型表现。

Abstract: This study provides an empirical assessment of whether current large language
models (LLMs) can pass the official qualifying examination for membership in
Poland's National Appeal Chamber (Krajowa Izba Odwo{\l}awcza). The authors
examine two related ideas: using LLM as actual exam candidates and applying the
'LLM-as-a-judge' approach, in which model-generated answers are automatically
evaluated by other models. The paper describes the structure of the exam, which
includes a multiple-choice knowledge test on public procurement law and a
written judgment, and presents the hybrid information recovery and extraction
pipeline built to support the models. Several LLMs (including GPT-4.1, Claude 4
Sonnet and Bielik-11B-v2.6) were tested in closed-book and various
Retrieval-Augmented Generation settings. The results show that although the
models achieved satisfactory scores in the knowledge test, none met the passing
threshold in the practical written part, and the evaluations of the
'LLM-as-a-judge' often diverged from the judgments of the official examining
committee. The authors highlight key limitations: susceptibility to
hallucinations, incorrect citation of legal provisions, weaknesses in logical
argumentation, and the need for close collaboration between legal experts and
technical teams. The findings indicate that, despite rapid technological
progress, current LLMs cannot yet replace human judges or independent examiners
in Polish public procurement adjudication.

</details>


### [26] [REMIND: Input Loss Landscapes Reveal Residual Memorization in Post-Unlearning LLMs](https://arxiv.org/abs/2511.04228)
*Liran Cohen,Yaniv Nemcovesky,Avi Mendelson*

Main category: cs.CL

TL;DR: REMIND提出了一种检测模型未遗忘数据残留影响的新方法，通过分析模型在小范围输入变化下的损失变化，区分已遗忘和未遗忘的数据。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法只关注单一输入，可能忽视语义相似例子中的残留影响，威胁隐私和信息安全。

Method: REMIND通过分析模型在输入变动下的损失曲线形状，判别数据是否被有效遗忘，且仅需基于查询的访问方式。

Result: 实验表明，未遗忘数据损失曲线更平缓，而保留或无关数据损失曲线更尖锐且波动更大，REMIND表现优于现有方法且具有鲁棒性。

Conclusion: REMIND提供了一种灵敏且可解释的遗忘效果评估框架，为语言模型的去记忆提供可靠测量，具备实际应用价值。

Abstract: Machine unlearning aims to remove the influence of specific training data
from a model without requiring full retraining. This capability is crucial for
ensuring privacy, safety, and regulatory compliance. Therefore, verifying
whether a model has truly forgotten target data is essential for maintaining
reliability and trustworthiness. However, existing evaluation methods often
assess forgetting at the level of individual inputs. This approach may overlook
residual influence present in semantically similar examples. Such influence can
compromise privacy and lead to indirect information leakage. We propose REMIND
(Residual Memorization In Neighborhood Dynamics), a novel evaluation method
aiming to detect the subtle remaining influence of unlearned data and classify
whether the data has been effectively forgotten. REMIND analyzes the model's
loss over small input variations and reveals patterns unnoticed by single-point
evaluations. We show that unlearned data yield flatter, less steep loss
landscapes, while retained or unrelated data exhibit sharper, more volatile
patterns. REMIND requires only query-based access, outperforms existing methods
under similar constraints, and demonstrates robustness across different models,
datasets, and paraphrased inputs, making it practical for real-world
deployment. By providing a more sensitive and interpretable measure of
unlearning effectiveness, REMIND provides a reliable framework to assess
unlearning in language models. As a result, REMIND offers a novel perspective
on memorization and unlearning.

</details>


### [27] [Reusing Pre-Training Data at Test Time is a Compute Multiplier](https://arxiv.org/abs/2511.04234)
*Alex Fang,Thomas Voice,Ruoming Pang,Ludwig Schmidt,Tom Gunter*

Main category: cs.CL

TL;DR: 本文利用检索增强生成方法及测试时计算资源评估预训练数据集的信息利用效率，发现现有预训练方法未充分挖掘数据价值。


<details>
  <summary>Details</summary>
Motivation: 尽管预训练数据集持续优化，但缺乏量化预训练过程中信息提取效率的研究。

Method: 通过检索增强生成和测试时额外计算资源，对比预训练与检索结合的效果，评估数据集价值利用率。

Result: 在MMLU、Math-500和SimpleQA数据集中，检索增强显著提升准确率，MMLU中检索相当于预训练的约5倍计算效应，利用更多测试计算资源进一步提升性能。

Conclusion: 当前预训练方法未充分利用已有数据中的信息，存在显著提升空间。

Abstract: Large language models learn from their vast pre-training corpora, gaining the
ability to solve an ever increasing variety of tasks; yet although researchers
work to improve these datasets, there is little effort to understand how
efficient the pre-training apparatus is at extracting ideas and knowledge from
the data. In this work, we use retrieval augmented generation along with
test-time compute as a way to quantify how much dataset value was left behind
by the process of pre-training, and how this changes across scale. We
demonstrate that pre-training then retrieving from standard and largely
open-sourced datasets results in significant accuracy gains in MMLU, Math-500,
and SimpleQA, which persist through decontamination. For MMLU we observe that
retrieval acts as a ~5x compute multiplier versus pre-training alone. We show
that these results can be further improved by leveraging additional compute at
test time to parse the retrieved context, demonstrating a 10 percentage point
improvement on MMLU for the public LLaMA 3.1 8B model. Overall, our results
suggest that today's pre-training methods do not make full use of the
information in existing pre-training datasets, leaving significant room for
progress.

</details>


### [28] [Efficient Topic Extraction via Graph-Based Labeling: A Lightweight Alternative to Deep Models](https://arxiv.org/abs/2511.04248)
*Salma Mekaoui,Hiba Sofyan,Imane Amaaz,Imane Benchrif,Arsalane Zarghili,Ilham Chaker,Nikola S. Nikolov*

Main category: cs.CL

TL;DR: 本文提出了一种基于图的主题标注方法，通过丰富主题词汇并分析词汇间的关系，实现对主题的有效命名，兼顾了计算效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现代文本数据快速增长，提取主题已成为关键任务，但现有方法通常计算代价高；使用统计主题建模虽节省资源，但生成的主题难以直观理解，因此需要有效且计算友好的主题标注方法。

Method: 设计了一种基于图的主题标注方法，通过引入语义相关词并构建词汇关系图，分析词间连接获取准确且有意义的标签，无需复杂计算模型。

Result: 在两个数据集上与传统基准方法及ChatGPT-3.5比较，所提方法在BERTScore和余弦相似度指标上表现优于传统方法，达到与ChatGPT-3.5相媲美的效果且计算效率更高。

Conclusion: 提出的图模型主题标注方法在解释性和计算效率上均表现出优势，未来可进一步探索提升自动化和解释性的研究方向。

Abstract: Extracting topics from text has become an essential task, especially with the
rapid growth of unstructured textual data. Most existing works rely on highly
computational methods to address this challenge. In this paper, we argue that
probabilistic and statistical approaches, such as topic modeling (TM), can
offer effective alternatives that require fewer computational resources. TM is
a statistical method that automatically discovers topics in large collections
of unlabeled text; however, it produces topics as distributions of
representative words, which often lack clear interpretability. Our objective is
to perform topic labeling by assigning meaningful labels to these sets of
words. To achieve this without relying on computationally expensive models, we
propose a graph-based approach that not only enriches topic words with
semantically related terms but also explores the relationships among them. By
analyzing these connections within the graph, we derive suitable labels that
accurately capture each topic's meaning. We present a comparative study between
our proposed method and several benchmarks, including ChatGPT-3.5, across two
different datasets. Our method achieved consistently better results than
traditional benchmarks in terms of BERTScore and cosine similarity and produced
results comparable to ChatGPT-3.5, while remaining computationally efficient.
Finally, we discuss future directions for topic labeling and highlight
potential research avenues for enhancing interpretability and automation.

</details>


### [29] [SSPO: Subsentence-level Policy Optimization](https://arxiv.org/abs/2511.04256)
*Kun Yang,Zikang chen,Yanmeng Wang,Zhigen Li*

Main category: cs.CL

TL;DR: 本文提出了基于句子级别重要性比例的强化学习算法SSPO，旨在解决现有LLM后训练方法中GRPO训练不稳定和GSPO采样数据利用率低的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于验证奖励的强化学习算法在训练时存在策略更新不稳定和采样数据利用率低的缺陷，如GRPO和GSPO，影响大语言模型的推理能力提升。

Method: SSPO引入句子级重要性比例以平衡GRPO的高方差和GSPO的采样数据弃用问题，同时利用句子熵动态调整PPO-CLIP的裁剪范围，鼓励高熵令牌探索，限制低熵令牌。

Result: SSPO在五个数据集上平均得分46.57，显著优于GRPO的43.01和GSPO的44.42，并在三个数据集上实现了最先进的性能。

Conclusion: SSPO通过句子级别的策略权衡和熵调节，有效提升了生成数据的利用率和训练稳定性，显著增强了大语言模型的推理能力。

Abstract: As a significant part of post-training of the Large Language Models (LLMs),
Reinforcement Learning from Verifiable Reward (RLVR) has greatly improved LLMs'
reasoning skills. However, some RLVR algorithms, such as GRPO (Group Relative
Policy Optimization) and GSPO (Group Sequence Policy Optimization), are
observed to suffer from unstable policy updates and low usage of sampling data,
respectively. The importance ratio of GRPO is calculated at the token level,
which focuses more on optimizing a single token. This will be easily affected
by outliers, leading to model training collapse. GSPO proposed the calculation
of the response level importance ratio, which solves the problem of high
variance and training noise accumulation in the calculation of the GRPO
importance ratio. However, since all the response tokens share a common
importance ratio, extreme values can easily raise or lower the overall mean,
leading to the entire response being mistakenly discarded, resulting in a
decrease in the utilization of sampled data. This paper introduces SSPO, which
applies sentence-level importance ratio, taking the balance between GRPO and
GSPO. SSPO not only avoids training collapse and high variance, but also
prevents the whole response tokens from being abandoned by the clipping
mechanism. Furthermore, we apply sentence entropy to PPO-CLIP to steadily
adjust the clipping bounds, encouraging high-entropy tokens to explore and
narrow the clipping range of low-entropy tokens. In particular, SSPO achieves
an average score of 46.57 across five datasets, surpassing GRPO (43.01) and
GSPO (44.42), and wins state-of-the-art performance on three datasets. These
results highlight SSPO's effectiveness in leveraging generated data by taking
the essence of GSPO but rejecting its shortcomings.

</details>


### [30] [Dynamic Jointly Batch Selection for Data Efficient Machine Translation Fine-Tuning](https://arxiv.org/abs/2511.04406)
*Mohammad Amin Ghanizadeh,Mohammad Javad Dousti*

Main category: cs.CL

TL;DR: 本文提出了一种面向机器翻译微调的数据选择方法，通过学习模型与预训练参考模型的协同利用，定义了可学习性得分来评估数据实用性，并采用批量选择策略，显著提升训练效率与翻译质量。


<details>
  <summary>Details</summary>
Motivation: 提升机器翻译模型性能依赖于高质量且有效的数据选择，而当前方法未充分利用学习模型与参考模型间的协同效应。

Method: 本文通过定义学习得分评估数据点的训练价值，结合批量选择策略考虑数据间关联，优化数据选择过程以提升训练效率。

Result: 在英语-波斯语及其他语言对的mBART模型实验中，相较于iid基线，方法提升数据利用效率达5倍，计算效率提升24%，且翻译性能优于随机选择。

Conclusion: 该方法通过协同模型与系统化数据筛选显著提高了机器翻译微调的数据效率和性能，为构建高效翻译系统提供了有效途径。

Abstract: Data quality and its effective selection are fundamental to improving the
performance of machine translation models, serving as cornerstones for
achieving robust and reliable translation systems. This paper presents a data
selection methodology specifically designed for fine-tuning machine translation
systems, which leverages the synergy between a learner model and a pre-trained
reference model to enhance overall training effectiveness. By defining a
learnability score, our approach systematically evaluates the utility of data
points for training, ensuring that only the most relevant and impactful
examples contribute to the fine-tuning process. Furthermore, our method employs
a batch selection strategy which considers interdependencies among data points,
optimizing the efficiency of the training process while maintaining a focus on
data relevance. Experiments on English to Persian and several other language
pairs using an mBART model fine-tuned on the CCMatrix dataset demonstrate that
our method can achieve up to a fivefold improvement in data efficiency compared
to an iid baseline. Experimental results indicate that our approach improves
computational efficiency by 24 when utilizing cached embeddings, as it requires
fewer training data points. Additionally, it enhances generalization, resulting
in superior translation performance compared to random selection method.

</details>


### [31] [If I Could Turn Back Time: Temporal Reframing as a Historical Reasoning Task for LLMs](https://arxiv.org/abs/2511.04432)
*Lars Bungum,Charles Yijia Huang,Abeer Kashar*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLMs）在时间推理上的能力，通过使用1940年挪威的问答书籍，模拟当时背景回答问题，并比较了英语与挪威语提示效果。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在不同语言和时间背景下对历史信息的处理与推理能力。

Method: 使用1940年挪威问答书籍中的问题，以英语和挪威语分别提示多款LLM回答问题，并通过LLM作为判分工具及人工抽查进行评分。

Result: 英文提示效果优于挪威语，且较大模型表现更佳。测试了DeepSeek-R1、Gemma3、Qwen3、Llama3.1及最大挪威语专用模型。

Conclusion: 提示语言和模型大小显著影响LLMs的时间推理表现，且英文提示在该实验中优于挪威语，提示未来在语言选择和模型规模方面的考虑。

Abstract: In this study, we experiment with the ability of LLMs to do temporal
reasoning. Using a Norwegian book from 1940 containing trivia questions, we
prompt the LLMs to answer the questions as if it were 1940. We also pose the
questions in both English and Norwegian. Correct answers are often presented as
sentences, and grading is done by means of LLM-as-judge, with sampled checks by
a native speaker. Prompting in English consistently gave better results than in
Norwegian, an unexpected result. In contrast, using larger LLMs improved
results. We tested the DeepSeek-R1, Gemma3, Qwen3, and Llama3.1 model families,
and also the largest available LLM especially crafted for Norwegian.

</details>


### [32] [Probabilistic Textual Time Series Depression Detection](https://arxiv.org/abs/2511.04476)
*Fabian Schmidt,Seyedehmoniba Ravan,Vladimir Vlassov*

Main category: cs.CL

TL;DR: 本文提出了一种名为PTTSD的概率文本时间序列抑郁检测框架，利用临床访谈中的语句级文本预测抑郁严重程度，并同时建模时间上的不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有抑郁严重度预测模型缺乏不确定性估计和时间序列建模，限制了临床决策支持的准确性和可解释性。

Method: PTTSD设计了序列到序列和序列到单一输出两种变体，结合双向LSTM、自注意力机制和残差连接，以及高斯或Student-t输出头，通过负对数似然进行训练。

Result: 在E-DAIC和DAIC-WOZ数据集上，PTTSD在仅用文本的系统中实现了最先进的性能（如E-DAIC上的MAE为3.85），并产生了良好校准的预测区间。消融实验显示了注意力机制和概率建模的重要性，与MentalBERT的对比验证了模型的通用性。

Conclusion: PTTSD框架有效提升了抑郁严重度预测的准确性和不确定性建模，且其产生的解释性结果在临床上具有重要参考价值。

Abstract: Accurate and interpretable predictions of depression severity are essential
for clinical decision support, yet existing models often lack uncertainty
estimates and temporal modeling. We propose PTTSD, a Probabilistic Textual Time
Series Depression Detection framework that predicts PHQ-8 scores from
utterance-level clinical interviews while modeling uncertainty over time. PTTSD
includes sequence-to-sequence and sequence-to-one variants, both combining
bidirectional LSTMs, self-attention, and residual connections with Gaussian or
Student-t output heads trained via negative log-likelihood. Evaluated on E-DAIC
and DAIC-WOZ, PTTSD achieves state-of-the-art performance among text-only
systems (e.g., MAE = 3.85 on E-DAIC, 3.55 on DAIC) and produces well-calibrated
prediction intervals. Ablations confirm the value of attention and
probabilistic modeling, while comparisons with MentalBERT establish generality.
A three-part calibration analysis and qualitative case studies further
highlight the interpretability and clinical relevance of uncertainty-aware
forecasting.

</details>


### [33] [ThaiOCRBench: A Task-Diverse Benchmark for Vision-Language Understanding in Thai](https://arxiv.org/abs/2511.04479)
*Surapon Nonesung,Teetouch Jaknamon,Sirinya Chaiophat,Natapong Nitarach,Chanakan Wittayasakpan,Warit Sirichotedumrong,Adisai Na-Thalang,Kunat Pipatanakul*

Main category: cs.CL

TL;DR: 本文提出了ThaiOCRBench，一种用于评估泰语文本视觉语言模型的综合基准，涵盖13类任务，包含2808个样本。评测显示专有模型优于开源模型，后者在细粒度文字识别和手写内容提取上表现较差。


<details>
  <summary>Details</summary>
Motivation: 现有多模态基准主要关注高资源语言，忽视了泰语，尤其是需要理解文档结构的任务。作者旨在填补这一空白，推动泰语视觉语言模型的发展。

Method: 构建了一个多样且人工标注的泰语文本数据集，包括13个任务类别和2808个样本，并在零样本设置下评估多种先进视觉语言模型（包括专有和开源）。通过详细错误分析揭示模型表现瓶颈。

Result: 评测结果显示专有模型如Gemini 2.5 Pro表现明显优于开源模型，后者在细粒度文本识别和手写内容提取任务上性能下降最显著。错误分析指出语言偏见、结构不匹配及虚假内容生成是主要挑战。

Conclusion: ThaiOCRBench为低资源复杂文字脚本场景下评估视觉语言模型提供了标准化框架，同时为改进泰语文档理解提供了有价值的洞见。

Abstract: We present ThaiOCRBench, the first comprehensive benchmark for evaluating
vision-language models (VLMs) on Thai text-rich visual understanding tasks.
Despite recent progress in multimodal modeling, existing benchmarks
predominantly focus on high-resource languages, leaving Thai underrepresented,
especially in tasks requiring document structure understanding. ThaiOCRBench
addresses this gap by offering a diverse, human-annotated dataset comprising
2,808 samples across 13 task categories. We evaluate a wide range of
state-of-the-art VLMs in a zero-shot setting, spanning both proprietary and
open-source systems. Results show a significant performance gap, with
proprietary models (e.g., Gemini 2.5 Pro) outperforming open-source
counterparts. Notably, fine-grained text recognition and handwritten content
extraction exhibit the steepest performance drops among open-source models.
Through detailed error analysis, we identify key challenges such as language
bias, structural mismatch, and hallucinated content. ThaiOCRBench provides a
standardized framework for assessing VLMs in low-resource, script-complex
settings, and provides actionable insights for improving Thai-language document
understanding.

</details>


### [34] [RUST-BENCH: Benchmarking LLM Reasoning on Unstructured Text within Structured Tables](https://arxiv.org/abs/2511.04491)
*Nikhil Abhyankar,Purvi Chaurasia,Sanchit Kabra,Ananya Srivastava,Vivek Gupta,Chandan K. Reddy*

Main category: cs.CL

TL;DR: 本文提出了针对现实世界复杂表格的推理基准RUST-BENCH，包含近8000个问题，涵盖科学和体育两个领域。实验表明现有大语言模型在复杂异构表格和多跳推理方面表现不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试表格推理的任务多为小型、统一的表格，难以反映现实数据的复杂性和大语言模型的真实推理能力。

Method: 构建RUST-BENCH数据集，包含2031个真实表格和7966个问题，涉及科学（NSF资助记录）和体育（NBA统计）两个领域，评估模型在规模、异构性、领域特性和推理复杂度上的表现。

Result: 实验结果显示，无论开源还是专有大语言模型都难以处理异构模式和复杂的多跳推理，暴露当前模型结构和推理策略的不足。

Conclusion: RUST-BENCH为表格推理研究提供了新的具有挑战性的测试平台，推动该领域向更现实和复杂任务发展。

Abstract: Existing tabular reasoning benchmarks mostly test models on small, uniform
tables, underrepresenting the complexity of real-world data and giving an
incomplete view of Large Language Models' (LLMs) reasoning abilities. Real
tables are long, heterogeneous, and domain-specific, mixing structured fields
with free text and requiring multi-hop reasoning across thousands of tokens. To
address this gap, we introduce RUST-BENCH, a benchmark of 7966 questions from
2031 real-world tables spanning two domains: i) RB-Science (NSF grant records)
and ii) RB-Sports (NBA statistics). Unlike prior work, RUST-BENCH evaluates
LLMs jointly across scale, heterogeneity, domain specificity, and reasoning
complexity. Experiments with open-source and proprietary models show that LLMs
struggle with heterogeneous schemas and complex multi-hop inference, revealing
persistent weaknesses in current architectures and prompting strategies.
RUST-BENCH establishes a challenging new testbed for advancing tabular
reasoning research.

</details>


### [35] [OUNLP at TSAR 2025 Shared Task: Multi-Round Text Simplifier via Code Generation](https://arxiv.org/abs/2511.04495)
*Cuong Huynh,Jie Cao*

Main category: cs.CL

TL;DR: 本文提出基于大语言模型（LLM）提示生成的多轮文本简化方法，针对不同的CEFR水平差异进行可读性控制，系统在TSAR-2025竞赛中获得第7名。


<details>
  <summary>Details</summary>
Motivation: 发现文本简化性能与源文本和目标文本的CEFR水平差异密切相关，基于此设计针对性简化方法。

Method: 提出两种多轮简化方法：规则驱动简化（MRS-Rule）和结合规则与LLM的简化（MRS-Joint），均通过GPT-4o模型生成文本。

Result: 提交的系统在TSAR-2025共享任务中排名第7，后续改进的MRS-Joint方法通过将LLM简化结果作为起点进一步提升性能。

Conclusion: 多轮简化结合规则与LLM提示生成能有效提升阅读难度控制的文本简化效果，CEFR水平差异作为简化设计的重要参考。

Abstract: This paper describes the OUNLP system submitted to the TSAR-2025 Shared Task
(Alva-Manchego et al., 2025), designed for readability-controlled text
simplification using LLM-prompting-based generation. Based on the analysis of
prompt-based text simplification methods, we discovered an interesting finding
that text simplification performance is highly related to the gap between the
source CEFR (Arase et al., 2022) level and the target CEFR level. Inspired by
this finding, we propose two multi-round simplification methods and generate
them via GPT-4o: rule-based simplification (MRS-Rule) and jointly rule-based
LLM simplification (MRS-Joint). Our submitted systems ranked 7 out of 20 teams.
Later improvements with MRS-Joint show that taking the LLM simplified
candidates as the starting point could further boost the multi-round
simplification performance.

</details>


### [36] [Decoding Emergent Big Five Traits in Large Language Models: Temperature-Dependent Expression and Architectural Clustering](https://arxiv.org/abs/2511.04499)
*Christos-Nikolaos Zacharopoulos,Revekka Kyriakoglou*

Main category: cs.CL

TL;DR: 该论文系统评估了六个大型语言模型（LLM）的类人格特征，使用了大五人格量表-2（BFI-2）框架，发现不同采样温度对神经质和外向性有显著影响，并根据模型架构进行了聚类分析。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在人机交互中的应用增多，理解它们的人格特征有助于实现更负责任的开发和部署。

Method: 针对六个LLM应用BFI-2人格量表，调整采样温度，评估其在五大人格维度上的表现，并通过层次聚类分析模型特征的稳定性。

Result: 发现四个人格维度存在显著差异，尤其是神经质和外向性受温度调整影响明显；聚类分析显示模型架构可能影响人格特征的稳定性。

Conclusion: 研究揭示了LLM中类人格特征的出现方式，为模型调优、选择和AI伦理治理提供了新视角，且已公开了数据和代码。

Abstract: As Large Language Models (LLMs) become integral to human-centered
applications, understanding their personality-like behaviors is increasingly
important for responsible development and deployment. This paper systematically
evaluates six LLMs, applying the Big Five Inventory-2 (BFI-2) framework, to
assess trait expressions under varying sampling temperatures. We find
significant differences across four of the five personality dimensions, with
Neuroticism and Extraversion susceptible to temperature adjustments. Further,
hierarchical clustering reveals distinct model clusters, suggesting that
architectural features may predispose certain models toward stable trait
profiles. Taken together, these results offer new insights into the emergence
of personality-like patterns in LLMs and provide a new perspective on model
tuning, selection, and the ethical governance of AI systems. We share the data
and code for this analysis here:
https://osf.io/bsvzc/?view_only=6672219bede24b4e875097426dc3fac1

</details>


### [37] [RAGalyst: Automated Human-Aligned Agentic Evaluation for Domain-Specific RAG](https://arxiv.org/abs/2511.04502)
*Joshua Gao,Quoc Huy Pham,Subin Varghese,Silwal Saurav,Vedhus Hoskere*

Main category: cs.CL

TL;DR: 本文介绍了RAGalyst，一种自动化且与人类判断一致的评估框架，用于专业领域的检索增强生成系统（RAG）评估，解决了现有评估方法在专业、安全关键领域中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统评估方法多依赖启发式指标或LLM作为评判者，缺乏与人类判断的验证对齐，难以捕捉专业领域的细节。

Method: 提出RAGalyst框架，利用代理链条生成高质量的合成问答数据，并通过代理过滤确保数据质量，同时优化LLM作为评判者的指标（答案正确性和可回答性），使其与人类注释高度相关。

Result: 在军事、网络安全和桥梁工程三个领域测试多种RAG组件，发现性能强依赖具体上下文，没有通用最佳模型或配置；并分析了回答正确率低的主要原因。

Conclusion: RAGalyst为系统地评估专业领域RAG系统提供有效工具，帮助研究者理解领域特定权衡并做出可靠设计选择，提升RAG系统的实用性和可信度。

Abstract: Retrieval-Augmented Generation (RAG) is a critical technique for grounding
Large Language Models (LLMs) in factual evidence, yet evaluating RAG systems in
specialized, safety-critical domains remains a significant challenge. Existing
evaluation frameworks often rely on heuristic-based metrics that fail to
capture domain-specific nuances and other works utilize LLM-as-a-Judge
approaches that lack validated alignment with human judgment. This paper
introduces RAGalyst, an automated, human-aligned agentic framework designed for
the rigorous evaluation of domain-specific RAG systems. RAGalyst features an
agentic pipeline that generates high-quality, synthetic question-answering (QA)
datasets from source documents, incorporating an agentic filtering step to
ensure data fidelity. The framework refines two key LLM-as-a-Judge
metrics-Answer Correctness and Answerability-using prompt optimization to
achieve a strong correlation with human annotations. Applying this framework to
evaluate various RAG components across three distinct domains (military
operations, cybersecurity, and bridge engineering), we find that performance is
highly context-dependent. No single embedding model, LLM, or hyperparameter
configuration proves universally optimal. Additionally, we provide an analysis
on the most common low Answer Correctness reasons in RAG. These findings
highlight the necessity of a systematic evaluation framework like RAGalyst,
which empowers practitioners to uncover domain-specific trade-offs and make
informed design choices for building reliable and effective RAG systems.
RAGalyst is available on our Github.

</details>


### [38] [Modeling Clinical Uncertainty in Radiology Reports: from Explicit Uncertainty Markers to Implicit Reasoning Pathways](https://arxiv.org/abs/2511.04506)
*Paloma Rabaey,Jong Hak Moon,Jung-Oh Lee,Min Gwan Kim,Hangyul Yoon,Thomas Demeester,Edward Choi*

Main category: cs.CL

TL;DR: 本文提出了一种两部分框架量化放射科报告中的显式和隐式不确定性，并基于此发布了一个扩展的结构化放射科报告基准数据集。


<details>
  <summary>Details</summary>
Motivation: 放射科报告中的不确定性使得基于规则的系统难以准确量化诊断结果的不确定程度，且隐式不确定性导致诊断判断不完整，影响自动化分析和临床决策。

Method: （1）显式不确定性通过专家验证的大型语言模型（LLM）基础参考排名映射到概率值；（2）隐式不确定性通过扩展框架，结合专家定义的14种常见诊断路径系统性添加特征子发现。

Result: 发布了Lunguage++，一个包含不确定性信息的细粒度结构化放射科报告基准，支持不确定性感知的图像分类和诊断推理。

Conclusion: 该方法有效量化和建模了放射科报告中的两种不确定性，为临床诊断的不确定性感知分析提供了新的资源和工具。

Abstract: Radiology reports are invaluable for clinical decision-making and hold great
potential for automated analysis when structured into machine-readable formats.
These reports often contain uncertainty, which we categorize into two distinct
types: (i) Explicit uncertainty reflects doubt about the presence or absence of
findings, conveyed through hedging phrases. These vary in meaning depending on
the context, making rule-based systems insufficient to quantify the level of
uncertainty for specific findings; (ii) Implicit uncertainty arises when
radiologists omit parts of their reasoning, recording only key findings or
diagnoses. Here, it is often unclear whether omitted findings are truly absent
or simply unmentioned for brevity. We address these challenges with a two-part
framework. We quantify explicit uncertainty by creating an expert-validated,
LLM-based reference ranking of common hedging phrases, and mapping each finding
to a probability value based on this reference. In addition, we model implicit
uncertainty through an expansion framework that systematically adds
characteristic sub-findings derived from expert-defined diagnostic pathways for
14 common diagnoses. Using these methods, we release Lunguage++, an expanded,
uncertainty-aware version of the Lunguage benchmark of fine-grained structured
radiology reports. This enriched resource enables uncertainty-aware image
classification, faithful diagnostic reasoning, and new investigations into the
clinical impact of diagnostic uncertainty.

</details>


### [39] [Are language models aware of the road not taken? Token-level uncertainty and hidden state dynamics](https://arxiv.org/abs/2511.04527)
*Amir Zur,Atticus Geiger,Ekdeep Singh Lubana,Eric Bigelow*

Main category: cs.CL

TL;DR: 本文研究了语言模型在生成文本时如何通过隐藏激活表示其推理过程中的不确定性和可替代路径。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型在链式推理过程中是否隐含地表示了多条可能的推理路径，以及如何量化和操控模型的不确定性。

Method: 通过分析语言模型的隐藏激活，控制和预测模型在推理时的不确定性，并观察激活干预对模型输出的影响。

Result: 发现模型在不同的token上不确定性与其激活控制的可调节性存在明显相关，隐含激活能预测模型未来的结果分布。

Conclusion: 语言模型不仅生成单一路径答案，还隐式表示多种可能推理路径的分布，激活干预在模型未确定最终答案时更有效。

Abstract: When a language model generates text, the selection of individual tokens
might lead it down very different reasoning paths, making uncertainty difficult
to quantify. In this work, we consider whether reasoning language models
represent the alternate paths that they could take during generation. To test
this hypothesis, we use hidden activations to control and predict a language
model's uncertainty during chain-of-thought reasoning. In our experiments, we
find a clear correlation between how uncertain a model is at different tokens,
and how easily the model can be steered by controlling its activations. This
suggests that activation interventions are most effective when there are
alternate paths available to the model -- in other words, when it has not yet
committed to a particular final answer. We also find that hidden activations
can predict a model's future outcome distribution, demonstrating that models
implicitly represent the space of possible paths.

</details>


### [40] [IntelliProof: An Argumentation Network-based Conversational Helper for Organized Reflection](https://arxiv.org/abs/2511.04528)
*Kaveh Eskandari Miandoab,Katharine Kowalyshyn,Kabir Pamnani,Anesu Gavhera,Vasanth Sarathy,Matthias Scheutz*

Main category: cs.CL

TL;DR: IntelliProof是一个基于大语言模型的交互式论证性文章分析系统，通过构建论证图来提升用户对文章结构和逻辑的理解。


<details>
  <summary>Details</summary>
Motivation: 现有自动作文评分系统缺乏良好的用户体验和可解读性，难以帮助用户深入理解论证结构和逻辑关系。

Method: IntelliProof将论文结构化为包含论点节点和支持证据的论证图，使用大语言模型自动分类和评分论证关系，并通过可视化展示和自然语言解释增强用户体验。

Result: 系统能够提供每个论证关系的分类依据和量化的文章连贯性指标，支持快速探索论证质量，且保留人工监督的可能性。

Conclusion: IntelliProof有效桥接了论证文章结构语义与用户理解之间的鸿沟，为分析和改进论证性文章提供了一种交互式且透明的工具。

Abstract: We present IntelliProof, an interactive system for analyzing argumentative
essays through LLMs. IntelliProof structures an essay as an argumentation
graph, where claims are represented as nodes, supporting evidence is attached
as node properties, and edges encode supporting or attacking relations. Unlike
existing automated essay scoring systems, IntelliProof emphasizes the user
experience: each relation is initially classified and scored by an LLM, then
visualized for enhanced understanding. The system provides justifications for
classifications and produces quantitative measures for essay coherence. It
enables rapid exploration of argumentative quality while retaining human
oversight. In addition, IntelliProof provides a set of tools for a better
understanding of an argumentative essay and its corresponding graph in natural
language, bridging the gap between the structural semantics of argumentative
essays and the user's understanding of a given text. A live demo and the system
are available here to try: \textbf{https://intelliproof.vercel.app}

</details>


### [41] [From Model to Breach: Towards Actionable LLM-Generated Vulnerabilities Reporting](https://arxiv.org/abs/2511.04538)
*Cyril Vallez,Alexander Sternfeld,Andrei Kucharavy,Ljiljana Dolamic*

Main category: cs.CL

TL;DR: 本文提出了一个新的针对大语言模型生成代码中漏洞风险的严重性度量方法，并利用该方法评估了开源代码生成模型的安全性。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的编码助手在软件开发中作用的提升，其生成代码中潜在漏洞对网络安全的影响也变得更加重要。现有的安全基准和改进方法尚未有效解决广泛使用的编码模型中的漏洞问题。

Method: 提出了反映漏洞风险的新指标——提示暴露（Prompt Exposure，PE），结合漏洞严重性、生成概率和提示语的影响，进一步定义模型暴露（Model Exposure，ME）分数，用以衡量模型生成漏洞的严重性和普遍性。

Result: 发现即使是最新的开源模型，在现实使用场景下仍易受到早期报道的漏洞攻击，表明安全与功能的权衡阻碍了漏洞的有效修补。利用PE和ME指标揭示了漏洞的风险和流行程度。

Conclusion: 提出的PE和ME指标为评估和改进代码生成模型的安全性提供了新工具，有助于重点缓解最严重和最普遍的漏洞风险。

Abstract: As the role of Large Language Models (LLM)-based coding assistants in
software development becomes more critical, so does the role of the bugs they
generate in the overall cybersecurity landscape. While a number of LLM code
security benchmarks have been proposed alongside approaches to improve the
security of generated code, it remains unclear to what extent they have
impacted widely used coding LLMs. Here, we show that even the latest
open-weight models are vulnerable in the earliest reported vulnerability
scenarios in a realistic use setting, suggesting that the safety-functionality
trade-off has until now prevented effective patching of vulnerabilities. To
help address this issue, we introduce a new severity metric that reflects the
risk posed by an LLM-generated vulnerability, accounting for vulnerability
severity, generation chance, and the formulation of the prompt that induces
vulnerable code generation - Prompt Exposure (PE). To encourage the mitigation
of the most serious and prevalent vulnerabilities, we use PE to define the
Model Exposure (ME) score, which indicates the severity and prevalence of
vulnerabilities a model generates.

</details>


### [42] [BanglaMedQA and BanglaMMedBench: Evaluating Retrieval-Augmented Generation Strategies for Bangla Biomedical Question Answering](https://arxiv.org/abs/2511.04560)
*Sadia Sultana,Saiyma Sittul Muna,Mosammat Zannatul Samarukh,Ajwad Abrar,Tareque Mohmud Chowdhury*

Main category: cs.CL

TL;DR: 本文介绍了首个大规模孟加拉语生物医学多项选择题数据集BanglaMedQA和BanglaMMedBench，并提出多种基于检索增强生成（RAG）策略以提升医学问答系统的准确性。


<details>
  <summary>Details</summary>
Motivation: 由于低资源语言的发展受限，孟加拉语医学问答系统难以实现高准确率，亟需构建相关数据集和有效算法，促进医疗知识的公平获取。

Method: 本文构建了孟加拉语医学教科书语料库并采用光学字符识别（OCR），结合传统与多种创新RAG策略（零样本回退、代理式、迭代反馈、集成RAG），动态选择检索与推理方法，提高事实准确性。

Result: 实验表明，代理式RAG策略结合openai/gpt-oss-120b模型达到89.54%的最高准确率，优于其它方法且推理质量更佳。

Conclusion: 基于RAG的方法显著提升了孟加拉语医学问答的可靠性和可访问性，为多语言医学人工智能的发展奠定了基础。

Abstract: Developing accurate biomedical Question Answering (QA) systems in
low-resource languages remains a major challenge, limiting equitable access to
reliable medical knowledge. This paper introduces BanglaMedQA and
BanglaMMedBench, the first large-scale Bangla biomedical Multiple Choice
Question (MCQ) datasets designed to evaluate reasoning and retrieval in medical
artificial intelligence (AI). The study applies and benchmarks several
Retrieval-Augmented Generation (RAG) strategies, including Traditional,
Zero-Shot Fallback, Agentic, Iterative Feedback, and Aggregate RAG, combining
textbook-based and web retrieval with generative reasoning to improve factual
accuracy. A key novelty lies in integrating a Bangla medical textbook corpus
through Optical Character Recognition (OCR) and implementing an Agentic RAG
pipeline that dynamically selects between retrieval and reasoning strategies.
Experimental results show that the Agentic RAG achieved the highest accuracy
89.54% with openai/gpt-oss-120b, outperforming other configurations and
demonstrating superior rationale quality. These findings highlight the
potential of RAG-based methods to enhance the reliability and accessibility of
Bangla medical QA, establishing a foundation for future research in
multilingual medical artificial intelligence.

</details>


### [43] [When retrieval outperforms generation: Dense evidence retrieval for scalable fake news detection](https://arxiv.org/abs/2511.04643)
*Alamgir Munir Qazi,John P. McCrae,Jamal Abdul Nasir*

Main category: cs.CL

TL;DR: DeReC框架通过结合密集检索和专用分类技术，替代了基于大型语言模型的事实核查方法，在保持更高准确率的同时，大幅提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 当前基于大型语言模型的事实核查方法虽然提供了解释性推理，但存在计算资源消耗大和生成虚假信息的风险，不适合实际应用。

Method: 提出轻量级的DeReC框架，利用通用文本嵌入结合密集检索与分类技术，替代自回归LLM方法，实现高效事实核查。

Result: DeReC在RAWFC数据集上的F1得分为65.58%，优于L-Defense的61.20%；运行时间分别减少了95%和92%，展现了优异的效率和精度。

Conclusion: 经过精心设计的基于检索的系统在特定任务上能达到甚至超过大型语言模型的性能，且更适合实际部署。

Abstract: The proliferation of misinformation necessitates robust yet computationally
efficient fact verification systems. While current state-of-the-art approaches
leverage Large Language Models (LLMs) for generating explanatory rationales,
these methods face significant computational barriers and hallucination risks
in real-world deployments. We present DeReC (Dense Retrieval Classification), a
lightweight framework that demonstrates how general-purpose text embeddings can
effectively replace autoregressive LLM-based approaches in fact verification
tasks. By combining dense retrieval with specialized classification, our system
achieves better accuracy while being significantly more efficient. DeReC
outperforms explanation-generating LLMs in efficiency, reducing runtime by 95%
on RAWFC (23 minutes 36 seconds compared to 454 minutes 12 seconds) and by 92%
on LIAR-RAW (134 minutes 14 seconds compared to 1692 minutes 23 seconds),
showcasing its effectiveness across varying dataset sizes. On the RAWFC
dataset, DeReC achieves an F1 score of 65.58%, surpassing the state-of-the-art
method L-Defense (61.20%). Our results demonstrate that carefully engineered
retrieval-based systems can match or exceed LLM performance in specialized
tasks while being significantly more practical for real-world deployment.

</details>


### [44] [Logit-Entropy Adaptive Stopping Heuristic for Efficient Chain-of-Thought Reasoning](https://arxiv.org/abs/2511.04654)
*Mohammad Atif Quamar,Mohammad Areeb*

Main category: cs.CL

TL;DR: 本文提出了一种名为LEASH的自适应停顿算法，用于减少连锁思维提示中不必要的推理步骤，从而节省计算资源和时间。


<details>
  <summary>Details</summary>
Motivation: 现有连锁思维提示生成固定长度的推理过程，导致计算资源浪费和延迟增加。

Method: 提出LEASH算法，通过监测令牌级别熵的变化率和最高概率差值的改进，当两者趋于平稳时停止推理生成。

Result: 在四个模型和两个基准测试上，LEASH平均减少30-35%的生成令牌和27%的延迟，但准确率下降约10个百分点。

Conclusion: LEASH无需额外训练即可有效减少推理步骤，提供了一种简单高效的替代方案。

Abstract: Chain-of-Thought (CoT) prompting is a key technique for enabling complex
reasoning in large language models. However, generating full, fixed-length
rationales is computationally wasteful, inflating both token usage and latency.
We introduce LEASH: Logit-Entropy Adaptive Stopping Heuristic, a training-free
decoding algorithm that adaptively halts rationale generation. LEASH monitors
two intrinsic signals: the slope of token-level entropy and the improvement in
the top-logit margin. It terminates the generation once both signals plateau,
indicating the model has reached a stable reasoning state. Across four
instruction-tuned models on the GSM8K and AQuA-RAT benchmarks, LEASH reduces
average token generation by 30--35% and latency by 27%, while incurring a 10
p.p. accuracy drop relative to CoT. LEASH is model-agnostic and requires no
additional training or supervision, offering a simple and efficient alternative
to CoT decoding.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [45] [OptiMA: A Transaction-Based Framework with Throughput Optimization for Very Complex Multi-Agent Systems](https://arxiv.org/abs/2511.03761)
*Umut Çalıkyılmaz,Nitin Nayak,Jinghua Groppe,Sven Groppe*

Main category: cs.MA

TL;DR: 本文针对多智能体系统在规模和复杂度增加时出现的故障易感性和性能瓶颈问题，提出了基于事务的框架及事务调度方法，开发了OptiMA框架，验证了该框架支持百余智能体系统并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统趋向于更大规模和更复杂模型，带来故障易感性和性能瓶颈两大问题。

Method: 提出基于事务的设计框架以提高系统容错性，并融入事务调度以缓解性能瓶颈，开发OptiMA框架实现以上方法。

Result: OptiMA框架可支持超过100个智能体的复杂系统运行，事务调度提升性能超过16%。

Conclusion: 基于事务的设计框架和调度机制有效提升了超大规模复杂多智能体系统的容错性和性能，具备理论和实践价值。

Abstract: In recent years, the research of multi-agent systems has taken a direction to
explore larger and more complex models to fulfill sophisticated tasks. We point
out two possible pitfalls that might be caused by increasing complexity;
susceptibilities to faults, and performance bottlenecks. To prevent the former
threat, we propose a transaction-based framework to design very complex
multi-agent systems (VCMAS). To address the second threat, we offer to
integrate transaction scheduling into the proposed framework. We implemented
both of these ideas to develop the OptiMA framework and show that it is able to
facilitate the execution of VCMAS with more than a hundred agents. We also
demonstrate the effect of transaction scheduling on such a system by showing
improvements up to more than 16\%. Furthermore, we also performed a theoretical
analysis on the transaction scheduling problem and provided practical tools
that can be used for future research on it.

</details>


### [46] [ASAP: an Agentic Solution to Auto-optimize Performance of Large-Scale LLM Training](https://arxiv.org/abs/2511.03844)
*Yuran Ding,Xinwei Chen,Xiaofan Zhang,Zongwei Zhou*

Main category: cs.MA

TL;DR: 本文提出ASAP系统，通过多智能体方法自动优化分布式大规模大型语言模型训练的性能，有效减少训练时间，提高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖手工调优或资源密集型搜索，无法满足快速发展的大型语言模型训练性能优化需求。

Method: 设计了包含协调者、分析者和提议者的多智能体系统，结合LLM推理、性能分析工具和专家知识库，实现性能瓶颈自动诊断和分片配置优化。

Result: ASAP自动生成的分片配置使训练步长时间减少28%，吞吐量提高1.43倍；结合人工优化吞吐量可达2.58倍。

Conclusion: ASAP提供了一种可扩展且具解释性的AI辅助性能优化方法，显著提升了大规模LLM训练效率。

Abstract: Optimizing large-language model (LLM) training on distributed domain-specific
accelerator systems presents significant challenges due to its complex
optimization space. Existing optimization methods, however, rely on
time-consuming manual tuning or resource-intensive black-box searches, which
struggle to keep pace with the rapidly evolving LLM domain, leading to slow
development and underutilized resources. To address this, we introduce ASAP, an
Agentic Solution to Auto-optimize Performance of Large-Scale LLM Training. It
is a multi-agent system, featuring Coordinator, Analyzer, and Proposal agents,
which integrates LLM reasoning with insights from performance profiling tools,
roofline analysis, and a knowledge base of best practices and successful past
optimizations from human experts. Our proposed design can automate the
diagnosis of performance bottlenecks and recommend optimized sharding
configurations with reasoning, thus effectively improving the efficiency of
distributed LLM training. Experiments have shown that the ASAP-generated
sharding configurations can contribute up to 28% training step time reduction
and 1.43 times throughput improvement. When combined with additional
optimization from human experts, throughput can be further increased to 2.58
times. The proposed ASAP promises to provide a scalable and explainable
methodology for AI-assisted performance engineering in large-scale LLM
training.

</details>


### [47] [Multi-Agent Collaborative Framework For Math Problem Generation](https://arxiv.org/abs/2511.03958)
*Kia Karbasi,Kevin Hong,Mohammad Amin Samadi,Gregory Pottie*

Main category: cs.MA

TL;DR: 提出了一种用于数学教育中自动问题生成（AQG）的协作多智能体框架，有效提升生成问题的复杂度控制和认知需求平衡。


<details>
  <summary>Details</summary>
Motivation: 现有预训练变换器语言模型难以精确控制问题复杂度和认知负担，限制了智能辅导系统在数学教育中自动题目生成的应用。

Method: 引入多智能体合作框架，通过多个智能体迭代优化生成的问题-答案对，实现对复杂度和认知需求的动态调整。

Result: 系统在相关性、重要性、清晰度、难度匹配度及可答性五个指标上的评估显示生成题目质量显著提升，认知挑战和清晰度达成更好平衡。

Conclusion: 协作多智能体框架能够更好地控制生成教育内容的复杂度和质量，有助于推进自动化教育内容生成与个性化学习环境的发展。

Abstract: Automatic question generation (AQG) for mathematics education remains an
elusive goal for Intelligent Tutoring Systems and educators. While pre-trained
transformer-based language models have significantly advanced natural language
generation, they often struggle to precisely control problem complexity and
cognitive demands. In this paper, we introduce a collaborative multi-agent
framework as a novel method of incorporating inference-time computation into
AQG. This approach leverages multiple agents that iteratively refine generated
question-answer pairs to better balance complexity and cognitive demand. We
evaluate the generated questions on five meta-evaluation criteria: relevance,
importance, clarity, difficulty matching, answerability, to assess the system's
ability to control the required complexity and quality of the questions.
Preliminary evaluations show that this collaborative multi-agent framework
elevates the quality of generated educational content by fostering a more
nuanced balance between cognitive challenge and clarity. These promising
outcomes suggest that integrating collaborative multi-agent workflows can yield
more controlled, pedagogically valuable content that can help advance automated
educational content generation and adaptive learning environments.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [48] [Tutorial Debriefing: Applied Statistical Causal Inference in Requirements Engineering](https://arxiv.org/abs/2511.03875)
*Julian Frattini,Hans-Martin Heyn,Robert Feldt,Richard Torkar*

Main category: cs.SE

TL;DR: 软件工程研究旨在通过工具、流程和指导原则改善软件开发，但需因果证据验证其有效性。随机对照试验难以实施时，需通过观察数据进行统计因果推断。


<details>
  <summary>Details</summary>
Motivation: 软件工程研究要将成果实际应用于软件开发以提升性能指标，但随机对照试验难以实现，需寻找替代的因果推断方法。

Method: 采用统计因果推断方法，从观察性数据中推断新工具、流程或指导原则与性能结果间的因果关系。

Result: 提出利用统计因果推断作为在不适合进行随机对照试验时验证软件工程创新有效性的替代方法。

Conclusion: 统计因果推断是连接软件工程研究成果与实际改进效果的重要桥梁，促进知识向实践转化。

Abstract: As any scientific discipline, the software engineering (SE) research
community strives to contribute to the betterment of the target population of
our research: software producers and consumers. We will only achieve this
betterment if we manage to transfer the knowledge acquired during research into
practice. This transferal of knowledge may come in the form of tools,
processes, and guidelines for software developers. However, the value of these
contributions hinges on the assumption that applying them causes an improvement
of the development process, user experience, or other performance metrics. Such
a promise requires evidence of causal relationships between an exposure or
intervention (i.e., the contributed tool, process or guideline) and an outcome
(i.e., performance metrics). A straight-forward approach to obtaining this
evidence is via controlled experiments in which a sample of a population is
randomly divided into a group exposed to the new tool, process, or guideline,
and a control group. However, such randomized control trials may not be
legally, ethically, or logistically feasible. In these cases, we need a
reliable process for statistical causal inference (SCI) from observational
data.

</details>


### [49] [Collaborative Agents for Automated Program Repair in Ruby](https://arxiv.org/abs/2511.03925)
*Nikta Akbarpour,Mahdieh Sadat Benis,Fatemeh Hendijani Fard,Ali Ouni,Mohamed Aymen Saied*

Main category: cs.SE

TL;DR: 本文提出了RAMP，一个针对Ruby的轻量级自动程序修复框架，通过多代理协作，实现基于反馈的迭代修复，显著提升了修复效果。


<details>
  <summary>Details</summary>
Motivation: 现有自动程序修复方法计算成本高且多集中于少数语言，Ruby作为广泛使用的开发语言却少有相关研究。

Method: RAMP通过多代理团队生成测试、错误反思和修正候选方案，利用轻量提示和测试驱动反馈迭代修复，不依赖大型数据库或昂贵微调。

Result: 在XCodeEval基准测试中，RAMP对Ruby的pass@1达67%，优于现有方法，且五次迭代内快速收敛，测试生成和自我反思对性能贡献显著。

Conclusion: RAMP展示了多代理修复策略的有效性，为基于大模型的调试工具拓展至少研究语言如Ruby提供了基础。

Abstract: Automated Program Repair (APR) has advanced rapidly with Large Language
Models (LLMs), but most existing methods remain computationally expensive, and
focused on a small set of languages. Ruby, despite its widespread use in web
development and the persistent challenges faced by its developers, has received
little attention in APR research. In this paper, we introduce RAMP, a novel
lightweight framework that formulates program repair as a feedback-driven,
iterative process for Ruby. RAMP employs a team of collaborative agents that
generate targeted tests, reflect on errors, and refine candidate fixes until a
correct solution is found. Unlike prior approaches, RAMP is designed to avoid
reliance on large multilingual repair databases or costly fine-tuning, instead
operating directly on Ruby through lightweight prompting and test-driven
feedback. Evaluation on the XCodeEval benchmark shows that RAMP achieves a
pass@1 of 67% on Ruby, outper-forming prior approaches. RAMP converges quickly
within five iterations, and ablation studies confirm that test generation and
self-reflection are key drivers of its performance. Further analysis shows that
RAMP is particularly effective at repairing wrong answers, compilation errors,
and runtime errors. Our approach provides new insights into multi-agent repair
strategies, and establishes a foundation for extending LLM-based debugging
tools to under-studied languages.

</details>


### [50] [PEFA-AI: Advancing Open-source LLMs for RTL generation using Progressive Error Feedback Agentic-AI](https://arxiv.org/abs/2511.03934)
*Athma Narayanan,Mahesh Subedar,Omesh Tickoo*

Main category: cs.SE

TL;DR: 本文提出了一种由多个智能体组成的流程，结合了专门的语言大模型（LLMs）和硬件仿真工具，实现无人工干预的寄存器传输级（RTL）代码生成。


<details>
  <summary>Details</summary>
Motivation: 解决自动生成高质量RTL代码中的复杂性和错误反馈问题，提高生成代码的功能正确性和综合性。

Method: 引入渐进式错误反馈系统（PEFA），通过迭代错误反馈机制自我纠正，逐步提升生成代码的复杂度；结合开源和闭源的LLM，实现多智能体协作完成任务。

Result: 在两个开源的自然语言转RTL数据集上进行基准测试，方法在通过率和效率（token使用量）上均优于现有方法，缩小了开源与闭源LLM的性能差距。

Conclusion: 所提方法有效提升了自动RTL生成的准确性和效率，达到了最新的性能基准，展示了多智能体结合专门LLM和仿真工具完成复杂任务的潜力。

Abstract: We present an agentic flow consisting of multiple agents that combine
specialized LLMs and hardware simulation tools to collaboratively complete the
complex task of Register Transfer Level (RTL) generation without human
intervention. A key feature of the proposed flow is the progressive error
feedback system of agents (PEFA), a self-correcting mechanism that leverages
iterative error feedback to progressively increase the complexity of the
approach. The generated RTL includes checks for compilation, functional
correctness, and synthesizable constructs. To validate this adaptive approach
to code generation, benchmarking is performed using two opensource natural
language-to-RTL datasets. We demonstrate the benefits of the proposed approach
implemented on an open source agentic framework, using both open- and
closed-source LLMs, effectively bridging the performance gap between them.
Compared to previously published methods, our approach sets a new benchmark,
providing state-of-the-art pass rates while being efficient in token counts.

</details>


### [51] [PSD2Code: Automated Front-End Code Generation from Design Files via Multimodal Large Language Models](https://arxiv.org/abs/2511.04012)
*Yongxi Chen,Lei Chen*

Main category: cs.SE

TL;DR: 本文提出了PSD2Code方法，利用PSD文件解析和资产对齐技术，实现从设计文件到生产级React+SCSS代码的自动生成。


<details>
  <summary>Details</summary>
Motivation: 现有设计到代码的方法存在结构不一致、资源错位和产出代码难以直接使用的问题，缺乏生产级代码生成能力。

Method: 提出ParseAlignGenerate流水线，通过解析PSD文件提取层次结构、图层属性及元数据，结合基于约束的资产对齐策略，保证设计资源与生成代码的一致性，并通过结构化提示提升代码质量。

Result: 评估显示PSD2Code在代码相似度、视觉一致性和生产准备度等多项指标上明显优于现有方法，并且具备良好的模型独立性。

Conclusion: 将结构化设计信息与多模态大语言模型结合，实现了工业级自动前端代码生成，推动了基于设计驱动的自动化前端开发进程。

Abstract: Design-to-code generation has emerged as a promising approach to bridge the
gap between design prototypes and deployable frontend code. However, existing
methods often suffer from structural inconsistencies, asset misalignment, and
limited production readiness. This paper presents PSD2Code, a novel multi-modal
approach that leverages PSD file parsing and asset alignment to generate
production-ready React+SCSS code. Our method introduces a ParseAlignGenerate
pipeline that extracts hierarchical structures, layer properties, and metadata
from PSD files, providing large language models with precise spatial
relationships and semantic groupings for frontend code generation. The system
employs a constraint-based alignment strategy that ensures consistency between
generated elements and design resources, while a structured prompt construction
enhances controllability and code quality. Comprehensive evaluation
demonstrates significant improvements over existing methods across multiple
metrics including code similarity, visual fidelity, and production readiness.
The method exhibits strong model independence across different large language
models, validating the effectiveness of integrating structured design
information with multimodal large language models for industrial-grade code
generation, marking an important step toward design-driven automated frontend
development.

</details>


### [52] [Specification-Guided Vulnerability Detection with Large Language Models](https://arxiv.org/abs/2511.04014)
*Hao Zhu,Jia Li,Cuiyun Gao,Jiaru Qian,Yihong Dong,Huanyu Liu,Lecheng Wang,Ziliang Wang,Xiaolong Hu,Ge Li*

Main category: cs.SE

TL;DR: 本文提出VulInstruct方法，通过构建安全规范知识库，提升大语言模型在漏洞检测中的性能，实现更准确的漏洞识别和推理。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在漏洞检测中表现有限，缺乏对安全规范的理解，难以区分漏洞代码与修补代码，原因是训练数据中缺乏明确的安全行为期望，导致模型难以推理安全缺陷。

Method: VulInstruct系统性地从历史漏洞中抽取安全规范，构建知识库，包括(i)跨项目的通用安全规范和(ii)针对特定代码库的领域专属规范，结合过去案例供模型检索和推理，从行为期望角度检测新漏洞。

Result: 在严格评测标准下，VulInstruct在PrimeVul数据集上的F1值达到45.0%，召回率37.7%，分别提升32.7%和50.8%，且独立检测的漏洞数是其他方法的2.4倍。

Conclusion: VulInstruct显著提升了LLM在漏洞检测中的准确性和推理能力，成功发现真实未公开的高危漏洞，具备实际应用价值。

Abstract: Large language models (LLMs) have achieved remarkable progress in code
understanding tasks. However, they demonstrate limited performance in
vulnerability detection and struggle to distinguish vulnerable code from
patched code. We argue that LLMs lack understanding of security specifications
-- the expectations about how code should behave to remain safe. When code
behavior differs from these expectations, it becomes a potential vulnerability.
However, such knowledge is rarely explicit in training data, leaving models
unable to reason about security flaws. We propose VulInstruct, a
specification-guided approach that systematically extracts security
specifications from historical vulnerabilities to detect new ones. VulInstruct
constructs a specification knowledge base from two perspectives: (i) General
specifications from high-quality patches across projects, capturing fundamental
safe behaviors; and (ii) Domain-specific specifications from repeated
violations in particular repositories relevant to the target code. VulInstruct
retrieves relevant past cases and specifications, enabling LLMs to reason about
expected safe behaviors rather than relying on surface patterns. We evaluate
VulInstruct under strict criteria requiring both correct predictions and valid
reasoning. On PrimeVul, VulInstruct achieves 45.0% F1-score (32.7% improvement)
and 37.7% recall (50.8% improvement) compared to baselines, while uniquely
detecting 24.3% of vulnerabilities -- 2.4x more than any baseline. In pair-wise
evaluation, VulInstruct achieves 32.3% relative improvement. VulInstruct also
discovered a previously unknown high-severity vulnerability (CVE-2025-56538) in
production code, demonstrating practical value for real-world vulnerability
discovery. All code and supplementary materials are available at
https://github.com/zhuhaopku/VulInstruct-temp.

</details>


### [53] [LLM-Driven Adaptive Source-Sink Identification and False Positive Mitigation for Static Analysis](https://arxiv.org/abs/2511.04023)
*Shiyin Lin*

Main category: cs.SE

TL;DR: 本文提出了一种基于大语言模型（LLM）的污点分析框架AdaTaint，通过神经符号推理自适应推断源/汇规范并过滤误报，显著提高静态漏洞分析的准确性和召回率。


<details>
  <summary>Details</summary>
Motivation: 静态分析在发现软件漏洞方面有效，但因源-汇规范不完整和误报率高而存在局限。

Method: 设计AdaTaint框架，结合LLM的推断能力和程序事实与约束验证的符号推理，实现规范自适应推断和误报过滤。

Result: 在多个基准测试和真实项目中，AdaTaint将误报平均减少43.7%，召回率提升11.2%，且运行时开销仍具竞争力。

Conclusion: 将LLM推断与符号验证结合，为更准确、可靠的静态漏洞分析提供了实用路径。

Abstract: Static analysis is effective for discovering software vulnerabilities but
notoriously suffers from incomplete source--sink specifications and excessive
false positives (FPs). We present \textsc{AdaTaint}, an LLM-driven taint
analysis framework that adaptively infers source/sink specifications and
filters spurious alerts through neuro-symbolic reasoning. Unlike LLM-only
detectors, \textsc{AdaTaint} grounds model suggestions in program facts and
constraint validation, ensuring both adaptability and determinism.
  We evaluate \textsc{AdaTaint} on Juliet 1.3, SV-COMP-style C benchmarks, and
three large real-world projects. Results show that \textsc{AdaTaint} reduces
false positives by \textbf{43.7\%} on average and improves recall by
\textbf{11.2\%} compared to state-of-the-art baselines (CodeQL, Joern, and
LLM-only pipelines), while maintaining competitive runtime overhead. These
findings demonstrate that combining LLM inference with symbolic validation
offers a practical path toward more accurate and reliable static vulnerability
analysis.

</details>


### [54] [Benchmarking and Studying the LLM-based Agent System in End-to-End Software Development](https://arxiv.org/abs/2511.04064)
*Zhengran Zeng,Yixin Li,Rui Xie,Wei Ye,Shikun Zhang*

Main category: cs.SE

TL;DR: 本文构建了一个挑战性的软件开发基准测试集E2EDevBench，并提出了结合测试用例和大语言模型的混合评估框架，评测三种自主软件开发代理体系结构，发现现有代理成功率约为50%，主要瓶颈为需求遗漏和自我验证不足。


<details>
  <summary>Details</summary>
Motivation: 现有LLM自主软件开发系统评测存在基准过于简单和工作架构比较不公平的问题，难以科学评估其性能。

Method: 构建动态更新的E2EDevBench，设计结合功能测试和LLM需求验证的混合评估框架，并统一基础实现，对三种代理架构进行控制变量实验。

Result: 发现顶尖代理能完成约50%需求，成功率受任务分解和协作架构策略影响最大，需求遗漏和自我验证不足是主要瓶颈。

Conclusion: 本文提供了更现实的基准测试和评估框架，揭示了当前软件开发代理的核心瓶颈，指导未来研究提升需求理解和任务规划能力。

Abstract: The development of LLM-based autonomous agents for end-to-end software
development represents a significant paradigm shift in software engineering.
However, the scientific evaluation of these systems is hampered by significant
challenges, including overly simplistic benchmarks and the difficulty of
conducting fair comparisons between different agent architectures due to
confounding implementation variables. To address these limitations, we first
construct a challenging and dynamically curated E2EDevBench to simulate
realistic development scenarios. Second, we propose a hybrid evaluation
framework that combines test-case-based functional assessment with
fine-grained, LLM-based requirement verification. Using this framework, we
conduct a controlled empirical study on three representative agent
architectures implemented upon a unified foundation to isolate the impact of
workflow design. Our findings reveal that state-of-the-art agents can fulfill
approximately 50\% of requirements on \bench{}, but their success is critically
dependent on the architectural strategy for task decomposition and
collaboration. Furthermore, our analysis indicates that the primary bottleneck
is the omission of requirements and inadequate self-verification. This work
provides the community with a more realistic benchmark, a comprehensive
evaluation framework, and crucial insights into the current capabilities and
core challenges of software development agents, guiding future research toward
enhancing requirement comprehension and planning.

</details>


### [55] [How Natural Language Proficiency Shapes GenAI Code for Software Engineering Tasks](https://arxiv.org/abs/2511.04115)
*Ruksit Rojpaisarnkit,Youmei Fan,Kenichi Matsumoto,Raula Gaikovina Kula*

Main category: cs.SE

TL;DR: 本文研究了自然语言提示中的英语语言水平对大语言模型生成代码质量的影响，发现更高的英语水平提示能显著提升代码正确性。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型在软件工程中的广泛应用，提示的自然语言能力对生成代码质量的影响尚未充分探究。

Method: 使用HumanEval数据集，对164个编程任务，系统调整提示的英语水平，从基础到高级，分析其对代码能力和正确性的影响。

Result: 发现大语言模型默认采用中级（B2）英语水平，尽管代码能力依赖于具体模型，但高级英语提示始终带来更多正确代码。

Conclusion: 自然语言的英语水平是控制代码生成的关键因素，开发者可通过提升提示的语言水平来提高AI生成代码的可靠性和质量。

Abstract: With the widespread adoption of Foundation Model (FM)-powered tools in
software engineering, the natural language prompt has become a critical
interface between developers and Large Language Models (LLMs). While much
research has focused on prompt structure, the natural language proficiency is
an underexplored factor that can influence the quality of generated code. This
paper investigates whether the English language proficiency itself independent
of the prompting technique affects the proficiency and correctness of code
generated by LLMs. Using the HumanEval dataset, we systematically varied the
English proficiency of prompts from basic to advanced for 164 programming tasks
and measured the resulting code proficiency and correctness. Our findings show
that LLMs default to an intermediate (B2) natural language level. While the
effect on the resulting code proficiency was model-dependent, we found that
higher-proficiency prompts consistently yielded more correct code across all
models. These results demonstrate that natural language proficiency is a key
lever for controlling code generation, helping developers tailor AI output and
improve the reliability of solutions.

</details>


### [56] [Are We Aligned? A Preliminary Investigation of the Alignment of Responsible AI Values between LLMs and Human Judgment](https://arxiv.org/abs/2511.04157)
*Asma Yamani,Malak Baslyman,Moataz Ahmed*

Main category: cs.SE

TL;DR: 本文研究了大型语言模型(LLMs)在软件工程中的价值取向与两类人群的对比，揭示了其在责任AI价值观上的一致性与不足。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在软件工程任务中广泛应用，理解其价值观与人类判断的对齐程度对于确保责任AI至关重要。

Method: 评估23个LLMs在四个任务上的表现，包括选择关键责任AI价值、评估价值重要性、解决价值冲突以及优先排序体现价值的软件需求。

Result: LLMs的价值观总体上更接近AI从业人员而非美国代表样本，强调公平、隐私、透明、安全和问责，但在声明的价值观与实际需求优先级之间存在不一致。

Conclusion: 依赖LLMs进行需求工程存在风险，需结合人工监督，建立系统方法来基准测试和监控AI辅助软件开发中的价值对齐。

Abstract: Large Language Models (LLMs) are increasingly employed in software
engineering tasks such as requirements elicitation, design, and evaluation,
raising critical questions regarding their alignment with human judgments on
responsible AI values. This study investigates how closely LLMs' value
preferences align with those of two human groups: a US-representative sample
and AI practitioners. We evaluate 23 LLMs across four tasks: (T1) selecting key
responsible AI values, (T2) rating their importance in specific contexts, (T3)
resolving trade-offs between competing values, and (T4) prioritizing software
requirements that embody those values. The results show that LLMs generally
align more closely with AI practitioners than with the US-representative
sample, emphasizing fairness, privacy, transparency, safety, and
accountability. However, inconsistencies appear between the values that LLMs
claim to uphold (Tasks 1-3) and the way they prioritize requirements (Task 4),
revealing gaps in faithfulness between stated and applied behavior. These
findings highlight the practical risk of relying on LLMs in requirements
engineering without human oversight and motivate the need for systematic
approaches to benchmark, interpret, and monitor value alignment in AI-assisted
software development.

</details>


### [57] [Explaining Software Vulnerabilities with Large Language Models](https://arxiv.org/abs/2511.04179)
*Oshando Johnson,Alexandra Fomina,Ranjith Krishnamurthy,Vaibhav Chaudhari,Rohith Kumar Shanmuganathan,Eric Bodden*

Main category: cs.SE

TL;DR: 本文提出了SAFE，一个利用GPT-4o生成安全漏洞解释的IDE插件，显著提升了静态应用安全测试工具的可用性。


<details>
  <summary>Details</summary>
Motivation: 现有静态应用安全测试（SAST）工具因警告信息过于通用，导致开发者难以理解或忽视关键漏洞。

Method: 设计了SAFE插件，结合大型语言模型GPT-4o，为SAST检测出的漏洞生成原因、影响及修复策略的详细解释。

Result: 专家用户研究显示，SAFE生成的解释有效帮助初中级开发者理解和解决安全漏洞。

Conclusion: 通过整合LLM技术，SAFE显著提升了SAST工具的可用性，促进安全漏洞的有效处理。

Abstract: The prevalence of security vulnerabilities has prompted companies to adopt
static application security testing (SAST) tools for vulnerability detection.
Nevertheless, these tools frequently exhibit usability limitations, as their
generic warning messages do not sufficiently communicate important information
to developers, resulting in misunderstandings or oversight of critical
findings. In light of recent developments in Large Language Models (LLMs) and
their text generation capabilities, our work investigates a hybrid approach
that uses LLMs to tackle the SAST explainability challenges. In this paper, we
present SAFE, an Integrated Development Environment (IDE) plugin that leverages
GPT-4o to explain the causes, impacts, and mitigation strategies of
vulnerabilities detected by SAST tools. Our expert user study findings indicate
that the explanations generated by SAFE can significantly assist beginner to
intermediate developers in understanding and addressing security
vulnerabilities, thereby improving the overall usability of SAST tools.

</details>


### [58] [GITER: A Git-Based Declarative Exchange Model Using Kubernetes-Style Custom Resources](https://arxiv.org/abs/2511.04182)
*Christos Tranoris*

Main category: cs.SE

TL;DR: 本文提出了一种基于Git的轻量级异步信息交换方法，用于分布式实体间的协调，替代传统API和消息代理。


<details>
  <summary>Details</summary>
Motivation: 解决传统跨域和跨组织异步通信复杂且不易审计的问题，扩展GitOps使用场景。

Method: 利用Kubernetes Operator和Custom Resources构建Git通信模型，通过共享仓库管理发布者和消费者的状态，结合Git的版本控制、签名和访问控制。

Result: 实现了透明、可追溯、可复现的异步通信机制，保持系统间的松耦合和自主性。

Conclusion: Git作为声明式通信基础能够支持复杂合作场景，具有独特优势但需权衡其实现复杂度和性能。

Abstract: This paper introduces a lightweight and auditable method for asynchronous
information exchange between distributed entities using Git as the coordination
medium. The proposed approach replaces traditional APIs and message brokers
with a Git-based communication model built on the principles of Kubernetes
Operators and Custom Resources (CRs). Each participating entity, designated as
a Publisher or Consumer, interacts through a shared repository that serves as a
single source of truth, where the spec field captures the desired state and the
status field reflects the observed outcome. This pattern extends GitOps beyond
infrastructure management to support cross-domain, inter-organizational, and
air-gapped collaboration scenarios. By leveraging Git native features
(versioning, commit signing, and access control) the model ensures
transparency, traceability, and reproducibility while preserving loose coupling
and autonomy between systems. The paper discusses architectural principles,
implementation considerations, and comparisons with RESTful and broker-based
integrations, highlighting both the advantages and trade-offs of adopting Git
as a declarative communication substrate.

</details>


### [59] [A Tool for Benchmarking Large Language Models' Robustness in Assessing the Realism of Driving Scenarios](https://arxiv.org/abs/2511.04267)
*Jiahui Wu,Chengjie Lu,Aitor Arrieta,Shaukat Ali*

Main category: cs.SE

TL;DR: 本文提出了DriveRLR工具，利用大型语言模型评估自动驾驶场景仿真真实性的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统安全性仍是挑战，模拟测试要求有效评估仿真场景真实性，现有方法难以做到。

Method: 开发DriveRLR，通过生成变异场景变体和构造文本提示，评估LLM识别场景真实性的能力和鲁棒性。

Result: 在DeepScenario数据集上测试GPT-5、Llama 4 Maverick和Mistral Small 3.2，DriveRLR揭示了不同LLM的鲁棒性差异，有效评估场景真实性。

Conclusion: DriveRLR不仅能评估LLM的鲁棒性，还可用作场景生成的客观函数，支持基于仿真的自动驾驶系统测试。

Abstract: In recent years, autonomous driving systems have made significant progress,
yet ensuring their safety remains a key challenge. To this end, scenario-based
testing offers a practical solution, and simulation-based methods have gained
traction due to the high cost and risk of real-world testing. However,
evaluating the realism of simulated scenarios remains difficult, creating
demand for effective assessment methods. Recent advances show that Large
Language Models (LLMs) possess strong reasoning and generalization
capabilities, suggesting their potential in assessing scenario realism through
scenario-related textual prompts. Motivated by this, we propose DriveRLR, a
benchmark tool to assess the robustness of LLMs in evaluating the realism of
driving scenarios. DriveRLR generates mutated scenario variants, constructs
prompts, which are then used to assess a given LLM's ability and robustness in
determining the realism of driving scenarios. We validate DriveRLR on the
DeepScenario dataset using three state-of-the-art LLMs: GPT-5, Llama 4
Maverick, and Mistral Small 3.2. Results show that DriveRLR effectively reveals
differences in the robustness of various LLMs, demonstrating its effectiveness
and practical value in scenario realism assessment. Beyond LLM robustness
evaluation, DriveRLR can serve as a practical component in applications such as
an objective function to guide scenario generation, supporting simulation-based
ADS testing workflows.

</details>


### [60] [Where Do LLMs Still Struggle? An In-Depth Analysis of Code Generation Benchmarks](https://arxiv.org/abs/2511.04355)
*Amir Molzam Sharifloo,Maedeh Heydari,Parsa Kazerooni,Daniel Maninger,Mira Mezini*

Main category: cs.SE

TL;DR: 本文分析了大型语言模型（LLMs）在代码生成中的失败任务，发现了其弱点和导致失败的常见复杂因素。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试提供的排名无法揭示LLMs持续失败的任务，而这些信息对理解模型限制和改进至关重要。

Method: 研究四个流行基准中的代码生成任务，通过分析失败任务，检查代码复杂性，并系统地审查114个持续失败的任务。

Result: 发现了LLMs的四种典型弱点模式，以及基准任务中常见导致失败的复杂问题。

Conclusion: 揭示了LLMs在代码生成中的局限，以指导未来模型的改进和更有效的基准设计。

Abstract: Large Language Models (LLMs) have achieved remarkable success in code
generation, and the race to improve their performance has become a central
focus of AI research. Benchmarks and leaderboards are increasingly popular,
offering quantitative rankings of LLMs. However, they provide limited insight
into the tasks that LLMs consistently fail to solve - information that is
crucial for understanding current limitations and guiding the development of
more capable models. To address this gap, we examined code generation tasks
across four popular benchmarks, identifying those that major LLMs are most
likely to fail. To understand the causes of these failures, we investigated
whether the static complexity of solution code contributes to them, followed by
a systematic inspection of 114 tasks that LLMs consistently struggled with. Our
analysis revealed four recurring patterns of weaknesses in LLMs, as well as
common complications within benchmark tasks that most often lead to failure.

</details>


### [61] [Speed at the Cost of Quality? The Impact of LLM Agent Assistance on Software Development](https://arxiv.org/abs/2511.04427)
*Hao He,Courtney Miller,Shyam Agarwal,Christian Kästner,Bogdan Vasilescu*

Main category: cs.SE

TL;DR: 本文评估了流行大语言模型助手Cursor对软件开发速度和代码质量的因果影响，发现Cursor显著提升开发速度但导致代码复杂度和警告增加。


<details>
  <summary>Details</summary>
Motivation: 尽管业界普遍声称LLM助手能大幅提升开发效率，但缺乏实证数据支持这些主张。

Method: 利用差分中之差分设计，对比采用Cursor的GitHub项目与未采用的匹配控制组项目，结合面板数据广义矩估计法进行因果推断。

Result: Cursor的采用带来显著且短暂的开发速度提升，同时带来静态分析警告和代码复杂度的显著且持久增加，后者成为长期速度减缓的主要因素。

Conclusion: LLM助手虽然短期内提升开发效率，但带来的代码质量下降可能引发长期生产力问题，提醒开发者和设计者关注平衡效率与质量。

Abstract: Large language models (LLMs) have demonstrated the promise to revolutionize
the field of software engineering. Among other things, LLM agents are rapidly
gaining momentum in their application to software development, with
practitioners claiming a multifold productivity increase after adoption. Yet,
empirical evidence is lacking around these claims. In this paper, we estimate
the causal effect of adopting a widely popular LLM agent assistant, namely
Cursor, on development velocity and software quality. The estimation is enabled
by a state-of-the-art difference-in-differences design comparing
Cursor-adopting GitHub projects with a matched control group of similar GitHub
projects that do not use Cursor. We find that the adoption of Cursor leads to a
significant, large, but transient increase in project-level development
velocity, along with a significant and persistent increase in static analysis
warnings and code complexity. Further panel generalized method of moments
estimation reveals that the increase in static analysis warnings and code
complexity acts as a major factor causing long-term velocity slowdown. Our
study carries implications for software engineering practitioners, LLM agent
assistant designers, and researchers.

</details>


### [62] [EDIT-Bench: Evaluating LLM Abilities to Perform Real-World Instructed Code Edits](https://arxiv.org/abs/2511.04486)
*Wayne Chi,Valerie Chen,Ryan Shar,Aditya Mittal,Jenny Liang,Wei-Lin Chiang,Anastasios Nikolas Angelopoulos,Ion Stoica,Graham Neubig,Ameet Talwalkar,Chris Donahue*

Main category: cs.SE

TL;DR: 本文介绍了EDIT-Bench，一个基于真实用户指令和代码上下文的大型语言模型代码编辑能力评测基准。


<details>
  <summary>Details</summary>
Motivation: 现有代码编辑能力评测基准多依赖人工合成数据，缺乏对真实使用场景的适应性和覆盖。

Method: 收集实际用户指令和代码上下文，构建包含545个多语言代码编辑问题的EDIT-Bench，涵盖错误修复、功能添加等多种真实使用场景。评测40个LLM在不同上下文信息和问题类别下的表现。

Result: 仅5个模型得分超过60%，性能在不同问题类别和上下文信息层级间变化显著，上下文信息的变化导致成功率波动达11%。

Conclusion: 真实上下文信息对代码编辑任务成败影响甚大，强调使用真实场景数据评测LLM代码编辑能力的重要性。

Abstract: Instructed code editing, where LLMs directly modify a developer's existing
code based on a user instruction, is becoming a widely used interaction mode in
AI coding assistants. However, few benchmarks directly evaluate this capability
and current datasets often rely on artificial sources. We introduce EDIT-Bench,
a benchmark for evaluating LLM code editing capabilities grounded in real-world
usage, i.e., user instructions and code contexts collected in the wild.
EDIT-Bench comprises of 545 problems, multiple natural and programming
languages, and a diverse set of real-world use cases, ranging from resolving
errors to adding features. EDIT-Bench introduces context-dependent problems
that require the model to understand code context, highlighted code, and cursor
position in addition to the user instruction. We evaluate 40 diverse LLMs and
observe that EDIT-Bench is a challenging set of problems where only 5 models
score over 60%. We find that model performance varies across different
categories of user instructions. Further, we find that varying levels of
contextual information greatly affect task success rate, with performance
varying up to 11%, indicating the importance of evaluating with realistic
context.

</details>


### [63] [Microservices Is Dying, A New Method for Module Division Based on Universal Interfaces](https://arxiv.org/abs/2511.04548)
*Qing Wang,Yong Zhang*

Main category: cs.SE

TL;DR: 本文提出了一种计算模块独立性的概念方法，通过设计通用接口，实现模块间零依赖的架构，从而提升系统可动态修改性。


<details>
  <summary>Details</summary>
Motivation: 微服务虽然实现了物理隔离，但未能防止模块间依赖传播，这影响系统的模块独立性和维护性。

Method: 基于影响评估方法，计算模块独立性，设计通用接口模式，形成通用模块边界，最终构建EIGHT平台架构。

Result: EIGHT架构保证模块独立性，使得即使是单进程单体应用也能动态加载、卸载及修改任意模块。

Conclusion: 该架构为复杂系统设计提供了新思路，超越了传统微服务和单体架构，实现了高度模块独立与动态灵活性。

Abstract: Although microservices have physically isolated modules, they have failed to
prevent the propagation and diffusion of dependencies. To trace the root cause
of the inter-module coupling, this paper, starting from the impact assessment
approach for module changes, proposes a conceptual method for calculating
module independence and utilizes this method to derive the necessary conditions
for module independence. Then, a new system design philosophy and software
engineering methodology is proposed, aimed at eliminating dependencies between
modules. A specific pattern is employed to design a set of universal
interfaces, serving as a universal boundary between modules. Subsequently, this
method is used to implement a platform architecture named EIGHT, demonstrating
that, as long as module independence is guaranteed, even a monolithic
application within a single process can dynamically load, unload, or modify any
part at runtime. Finally, the paper concludes that this architecture aims to
explore a novel path for increasingly complex systems, beyond microservice and
monolithic architectures.

</details>
