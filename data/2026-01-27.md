<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 111]
- [cs.SE](#cs.SE) [Total: 25]
- [cs.MA](#cs.MA) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Crystal-KV: Efficient KV Cache Management for Chain-of-Thought LLMs via Answer-First Principle](https://arxiv.org/abs/2601.16986)
*Zihan Wang,Cheng Tang,Lei Gong,Cheng Li,Chao Wang,teng wang,Wenqi Lou,Xuehai Zhou*

Main category: cs.CL

TL;DR: 本文针对链式思维推理的KV缓存管理提出了Crystal-KV方法，通过答案优先原则和注意力驱动的缓存淘汰机制，有效压缩缓存，提高了推理效率和准确率。


<details>
  <summary>Details</summary>
Motivation: 链式思维推理虽提升复杂任务准确率，但长序列存储引起KV缓存过大，传统缓存压缩策略无法有效处理，需设计专门针对CoT推理的缓存管理策略以减少内存开销、提高效率。

Method: 采用了答案优先原则，将缓存条目分为SlipKV和CrystalKV；提出基于注意力的最近最少频繁使用算法来精准淘汰无用缓存；引入自适应缓存预算分配算法，根据动态比例调整各层和头的缓存预算，提升缓存利用率。

Result: 该论文提出了一种名为Crystal-KV的高效KV缓存管理框架，专为链式思维推理设计。通过引入答案优先原则，将缓存条目分为SlipKV（保持推理流程但可能引入误导）和CrystalKV（对最终答案正确性贡献大）。论文采用基于注意力的最近最少频繁使用算法，精确淘汰无用缓存条目，并通过自适应缓存预算分配，动态调整缓存资源，提升关键部分利用率。实验结果表明，该方法实现了领先的KV缓存压缩效果，显著提升了推理吞吐量和响应速度，同时保持甚至提升了答案准确率。

Conclusion: Crystal-KV方法通过区分缓存条目的重要性和动态分配缓存资源，实现了高效的KV缓存管理，提升了链式思维推理的性能和准确性，具有显著的应用价值。

Abstract: Chain-of-Thought (CoT) reasoning in large language models (LLMs) significantly improves accuracy on complex tasks, yet incurs excessive memory overhead due to the long think-stage sequences stored in the Key-Value (KV) cache. Unlike traditional generation tasks where all tokens are uniformly important, CoT emphasizes the final answer, rendering conventional KV compression strategies ineffective. In this paper, we present Crystal-KV, an efficient KV cache management framework tailored for CoT reasoning. Our key insight is the answer-first principle. By mapping answer preferences into think-stage attention map, we distinguish between SlipKV, which mainly maintains the reasoning flow but may occasionally introduce misleading context, and CrystalKV, which truly contributes to the correctness of the final answer. Next, we propose an attention-based Least Recently Frequently Used algorithm. It precisely identifies when a SlipKV entry's utility expires and evicts it, retaining CrystalKV without disrupting reasoning flow. Finally, we introduce an adaptive cache budget allocation algorithm. Based on the dynamic proportion of CrystalKV, it estimates the importance of each layer/head and adjusts the KV cache budget during inference, amplifying critical components to improve budget utilization. Results show that Crystal-KV achieves state-of-the-art KV cache compression, significantly improves throughput, and enables faster response time, while maintaining, or even improving, answer accuracy for CoT reasoning.

</details>


### [2] [Evaluating Reward Model Generalization via Pairwise Maximum Discrepancy Competitions](https://arxiv.org/abs/2601.16987)
*Shunyang Luo,Peibei Cao,Zhihui Zhu,Kehua Feng,Zhihua Wang,Keyan Ding*

Main category: cs.CL

TL;DR: 提出PMDC动态、高效评估奖励模型泛化能力，显著揭示模型排名变化和泛化弱点。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型评估主要依赖静态、预先注释的偏好数据集，覆盖有限且难以评估在开放世界中的泛化能力。

Method: 提出了Pairwise Maximum Discrepancy Competition (PMDC)，通过动态选择引发两奖励模型（RM）分歧的(prompt, response)对，使用Bradley--Terry模型对结果聚合，生成全球排名和配对胜率图谱。

Result: PMDC评估了10个代表性奖励模型，发现其排名相比传统基准发生显著变化，同时通过定性分析发现系统性泛化失败。

Conclusion: PMDC为评价奖励模型泛化提供了更真实、全面的测试框架，有助于发现和改进奖励模型的泛化缺陷。

Abstract: Reward models (RMs) are central to aligning large language models, yet their practical effectiveness hinges on generalization to unseen prompts and shifting distributions. Most existing RM evaluations rely on static, pre-annotated preference datasets, which provide limited coverage and often fail to faithfully assess generalization in open-world settings. We introduce Pairwise Maximum Discrepancy Competition (PMDC), a dynamic and annotation-efficient framework for evaluating RM generalization using a large, unlabeled, open-domain prompt pool. PMDC actively selects prompt--response pairs that maximize disagreement between two RMs, yielding a compact set of highly contentious test cases. These cases are adjudicated by an oracle, and the resulting outcomes are aggregated via a Bradley--Terry model to produce a global ranking and pairwise win-rate landscape of RMs. We apply PMDC to re-evaluate 10 representative RMs and observe substantial rank reshuffling compared with conventional benchmarks. Qualitative analyses further uncover systematic generalization failures, providing valuable insights for improving reward modeling.

</details>


### [3] [Uncertainty Quantification for Named Entity Recognition via Full-Sequence and Subsequence Conformal Prediction](https://arxiv.org/abs/2601.16999)
*Matthew Singer,Srijan Sengupta,Karl Pazdernik*

Main category: cs.CL

TL;DR: 该论文提出了一种利用符合预测方法，为NER模型生成带有置信度保证的标签预测集，提升了预测的可靠性和应用的稳健性。


<details>
  <summary>Details</summary>
Motivation: 当前的命名实体识别（NER）模型只输出单一标签序列，缺乏不确定性度量，导致下游应用容易受错误累积影响。

Method: 基于符合预测方法，设计非一致性评分函数构建预测集，实现对整个句子标签的置信度保证，支持无条件和条件覆盖，并考虑了句子长度、语言、实体类型及数量等异质性因素。

Result: 提出一个基于序列标注的NER模型的不确定性感知预测集框架，提供具有置信度保证的正确标签集合，并设计高效的非一致性评分函数，实现模型预测的可靠性和覆盖性。

Conclusion: 该方法有效提升了NER模型的预测可靠性，支持多种应用场景，具有广泛的适用性和良好的校准效果。

Abstract: Named Entity Recognition (NER) serves as a foundational component in many natural language processing (NLP) pipelines. However, current NER models typically output a single predicted label sequence without any accompanying measure of uncertainty, leaving downstream applications vulnerable to cascading errors. In this paper, we introduce a general framework for adapting sequence-labeling-based NER models to produce uncertainty-aware prediction sets. These prediction sets are collections of full-sentence labelings that are guaranteed to contain the correct labeling with a user-specified confidence level. This approach serves a role analogous to confidence intervals in classical statistics by providing formal guarantees about the reliability of model predictions. Our method builds on conformal prediction, which offers finite-sample coverage guarantees under minimal assumptions. We design efficient nonconformity scoring functions to construct efficient, well-calibrated prediction sets that support both unconditional and class-conditional coverage. This framework accounts for heterogeneity across sentence length, language, entity type, and number of entities within a sentence. Empirical experiments on four NER models across three benchmark datasets demonstrate the broad applicability, validity, and efficiency of the proposed methods.

</details>


### [4] [RAM-SD: Retrieval-Augmented Multi-agent framework for Sarcasm Detection](https://arxiv.org/abs/2601.17002)
*Ziyang Zhou,Ziqi Liu,Yan Wang,Yiming Lin,Yangbin Chen*

Main category: cs.CL

TL;DR: 本文提出RAM-SD多智能体框架，针对讽刺检测多样化需求，实现多视角互补分析和可解释推理，性能超越现有强基线并提供透明认知机制。


<details>
  <summary>Details</summary>
Motivation: 讽刺检测由于需要细腻的上下文理解、世界知识及多维语言线索，且不同讽刺表达差异大，因此仍是一个重大挑战。现有方法使用统一推理策略，难以满足讽刺多样化的分析需求。

Method: 提出RAM-SD框架，包括四个阶段：（1）上下文检索以获取讽刺与非讽刺示例；（2）元规划器分类讽刺类型并选择最佳推理方案；（3）多个专门智能体执行互补多视角分析；（4）整合器将分析结果合成为最终可解释判断与自然语言解释。

Result: 在四个标准基准上，RAM-SD取得77.74%的Macro-F1，较强基线GPT-4o+CoC提升7.01个百分点。框架不仅性能领先，还提供透明可解释的推理过程，揭示讽刺理解的认知机制。

Conclusion: RAM-SD有效提升了讽刺检测的准确性和解释性，突破了统一推理策略的限制，为讽刺理解提供了更精细、多维和可解释的分析方法。

Abstract: Sarcasm detection remains a significant challenge due to its reliance on nuanced contextual understanding, world knowledge, and multi-faceted linguistic cues that vary substantially across different sarcastic expressions. Existing approaches, from fine-tuned transformers to large language models, apply a uniform reasoning strategy to all inputs, struggling to address the diverse analytical demands of sarcasm. These demands range from modeling contextual expectation violations to requiring external knowledge grounding or recognizing specific rhetorical patterns. To address this limitation, we introduce RAM-SD, a Retrieval-Augmented Multi-Agent framework for Sarcasm Detection. The framework operates through four stages: (1) contextual retrieval grounds the query in both sarcastic and non-sarcastic exemplars; (2) a meta-planner classifies the sarcasm type and selects an optimal reasoning plan from a predefined set; (3) an ensemble of specialized agents performs complementary, multi-view analysis; and (4) an integrator synthesizes these analyses into a final, interpretable judgment with a natural language explanation. Evaluated on four standard benchmarks, RAM-SD achieves a state-of-the-art Macro-F1 of 77.74%, outperforming the strong GPT-4o+CoC baseline by 7.01 points. Our framework not only sets a new performance benchmark but also provides transparent and interpretable reasoning traces, illuminating the cognitive processes behind sarcasm comprehension.

</details>


### [5] [From Emotion to Expression: Theoretical Foundations and Resources for Fear Speech](https://arxiv.org/abs/2601.17132)
*Vigneshwaran Shankaran,Gabriella Lapesa,Claudia Wagner*

Main category: cs.CL

TL;DR: 本文跨学科整合恐惧言论的理论与数据，提出分类法，推动该领域的研究与资源建设。


<details>
  <summary>Details</summary>
Motivation: 恐惧言论广泛且影响深远，但计算研究资源不足且分散，需要跨学科融合视角和系统框架以推动该领域的发展。

Method: 比较心理学、政治学、传播学和语言学中的恐惧理论，回顾相关研究数据集，提出统一的恐惧分类法。

Result: 本文探讨了恐惧言论在社会动员、信息传播和集体行为中的作用，指出恐惧言论在传播范围和互动性上超过仇恨言论，因为其表现得更“文明”且易于规避审查。然而，恐惧言论的计算研究仍然分散且资源不足。作者通过比较心理学、政治学、传播学和语言学中的恐惧理论，整合多学科视角，回顾现有定义，并梳理相关数据集，提出一个整合不同维度的恐惧分类法，为未来构建数据集和推动恐惧言论研究提供理论和实践指导。

Conclusion: 通过多学科视角整合和梳理数据集，本文为恐惧言论的定义、分类和研究提供了系统性框架和资源基础，促进了该领域的理论与实务发展。

Abstract: Few forces rival fear in their ability to mobilize societies, distort communication, and reshape collective behavior. In computational linguistics, fear is primarily studied as an emotion, but not as a distinct form of speech. Fear speech content is widespread and growing, and often outperforms hate-speech content in reach and engagement because it appears "civiler" and evades moderation. Yet the computational study of fear speech remains fragmented and under-resourced. This can be understood by recognizing that fear speech is a phenomenon shaped by contributions from multiple disciplines. In this paper, we bridge cross-disciplinary perspectives by comparing theories of fear from Psychology, Political science, Communication science, and Linguistics. Building on this, we review existing definitions. We follow up with a survey of datasets from related research areas and propose a taxonomy that consolidates different dimensions of fear for studying fear speech. By reviewing current datasets and defining core concepts, our work offers both theoretical and practical guidance for creating datasets and advancing fear speech research.

</details>


### [6] [Dynamic Role Assignment for Multi-Agent Debate](https://arxiv.org/abs/2601.17152)
*Miao Zhang,Junsik Kim,Siyuan Xiang,Jian Gao,Cheng Cao*

Main category: cs.CL

TL;DR: 本文提出一种动态角色分配框架，通过元辩论选择合适智能体角色，显著提升多智能体辩论系统性能。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体大型语言模型（LLM）和视觉语言模型（VLM）辩论系统在复杂问题求解中采用专门角色，但未利用模型专长来决定哪个模型适合哪个角色。

Method: 提出动态角色分配框架，通过两个阶段的元辩论（提案和同行评审），根据数据和角色特定标准选择最适合每个职位的智能体。

Result: 实验结果显示，该方法在现有辩论系统基础上，性能较统一分配提升最高74.8%，较随机分配提升最高29.7%。

Conclusion: 该方法开创了多智能体系统设计的新范式，实现由静态部署向动态且能力感知的选择转变。

Abstract: Multi-agent large language model (LLM) and vision-language model (VLM) debate systems employ specialized roles for complex problem-solving, yet model specializations are not leveraged to decide which model should fill which role. We propose dynamic role assignment, a framework that runs a Meta-Debate to select suitable agents before the actual debate. The meta-debate has two stages: (1) proposal, where candidates provide role-tailored arguments, and (2) peer review, where proposals are scored with data and role-specific criteria to choose the best agent for each position. We evaluate our method on LLM problem solving benchmarks. Applied on top of existing debate systems, our approach consistently outperforms uniform assignments (filling all roles with the same model) by up to 74.8% and random assignments (assigning models to roles without considering their suitability) by up to 29.7%, depending on the task and the specific assignment. This work establishes a new paradigm for multi-agent system design, shifting from static agent deployment to dynamic and capability-aware selection.

</details>


### [7] [Interpretability of the Intent Detection Problem: A New Approach](https://arxiv.org/abs/2601.17156)
*Eduardo Sanchez-Karhunen,Jose F. Quesada-Moreno,Miguel A. Gutiérrez-Naranjo*

Main category: cs.CL

TL;DR: 利用动力系统理论分析RNN在意图检测中的机制，发现数据集平衡性影响隐藏状态空间的几何结构，从而影响识别性能。


<details>
  <summary>Details</summary>
Motivation: 虽然深度学习在意图检测中表现突出，但RNN内部解决机制缺乏理解，尤其是如何受数据集特性影响，作者希望通过动力系统理论提供几何学的解释，揭示这些机制。

Method: 将句子视为RNN隐藏状态空间中的轨迹，利用动力系统理论分析并比较平衡与不平衡数据集上的状态空间结构变化。

Result: 本论文采用动力系统理论分析RNN在意图检测任务中的内部机制，通过将句子解释为隐藏状态空间中的轨迹，揭示了网络如何在状态空间中形成不同意图的聚类结构。对平衡的SNIPS数据集，网络学习到一个理想的解决方案，状态空间被划分为对应各意图的低维流形上的不同簇。对不平衡的ATIS数据集，该理想几何结构被类别不平衡扭曲，低频意图的簇结构退化，导致性能下降。该框架将几何分离与读出对齐分离开来，解释了现实中的性能差异。

Conclusion: RNN解决意图检测问题时通过在低维空间形成不同意图的簇结构，数据集的不平衡性导致这些簇结构退化，影响性能表现。该几何视角为理解RNN动态和数据集性质对模型行为的影响提供了新见解。

Abstract: Intent detection, a fundamental text classification task, aims to identify and label the semantics of user queries, playing a vital role in numerous business applications. Despite the dominance of deep learning techniques in this field, the internal mechanisms enabling Recurrent Neural Networks (RNNs) to solve intent detection tasks are poorly understood. In this work, we apply dynamical systems theory to analyze how RNN architectures address this problem, using both the balanced SNIPS and the imbalanced ATIS datasets. By interpreting sentences as trajectories in the hidden state space, we first show that on the balanced SNIPS dataset, the network learns an ideal solution: the state space, constrained to a low-dimensional manifold, is partitioned into distinct clusters corresponding to each intent. The application of this framework to the imbalanced ATIS dataset then reveals how this ideal geometric solution is distorted by class imbalance, causing the clusters for low-frequency intents to degrade. Our framework decouples geometric separation from readout alignment, providing a novel, mechanistic explanation for real world performance disparities. These findings provide new insights into RNN dynamics, offering a geometric interpretation of how dataset properties directly shape a network's computational solution.

</details>


### [8] [Who Gets Which Message? Auditing Demographic Bias in LLM-Generated Targeted Text](https://arxiv.org/abs/2601.17172)
*Tunazzina Islam*

Main category: cs.CL

TL;DR: 该论文首次系统分析了大型语言模型（LLMs）在针对不同人口群体定制信息时的行为表现，揭示了不同年龄和性别间信息内容和说服力的差异。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型能大规模生成个性化、有说服力的文本，自动化交流中的偏见和公平性问题日益突出，需要系统分析模型在基于人口统计特征定制消息时的表现。

Method: 提出了一个控制性评估框架，采用GPT-4o、Llama-3.3和Mistral-Large 2.1三个模型，通过独立生成和情境丰富生成两种方式，评估文本的词汇内容、语言风格和说服策略，在气候交流领域进行实验。

Result: 三个模型均表现出基于性别和年龄的显著差异，男性和年轻人定制的消息强调行动及创新，女性和老年人定制的消息更注重关怀和传统；上下文信息进一步放大了这些差异，年轻男性群体的信息说服力评分显著更高。

Conclusion: 论文发现LLMs生成的定制信息中存在基于年龄和性别的刻板印象，男性和年轻群体的信息更强调行动力和创新，而女性和老年群体的信息则更强调温情和传统，且上下文环境会放大这些差异，强调了需要建立偏见感知和透明审计机制。

Abstract: Large language models (LLMs) are increasingly capable of generating personalized, persuasive text at scale, raising new questions about bias and fairness in automated communication. This paper presents the first systematic analysis of how LLMs behave when tasked with demographic-conditioned targeted messaging. We introduce a controlled evaluation framework using three leading models -- GPT-4o, Llama-3.3, and Mistral-Large 2.1 -- across two generation settings: Standalone Generation, which isolates intrinsic demographic effects, and Context-Rich Generation, which incorporates thematic and regional context to emulate realistic targeting. We evaluate generated messages along three dimensions: lexical content, language style, and persuasive framing. We instantiate this framework on climate communication and find consistent age- and gender-based asymmetries across models: male- and youth-targeted messages emphasize agency, innovation, and assertiveness, while female- and senior-targeted messages stress warmth, care, and tradition. Contextual prompts systematically amplify these disparities, with persuasion scores significantly higher for messages tailored to younger or male audiences. Our findings demonstrate how demographic stereotypes can surface and intensify in LLM-generated targeted communication, underscoring the need for bias-aware generation pipelines and transparent auditing frameworks that explicitly account for demographic conditioning in socially sensitive applications.

</details>


### [9] [Beyond Factual QA: Mentorship-Oriented Question Answering over Long-Form Multilingual Content](https://arxiv.org/abs/2601.17173)
*Parth Bhalerao,Diola Dsouza,Ruiwen Guan,Oana Ignat*

Main category: cs.CL

TL;DR: 提出了多语言长文本指导式问答数据集MentorQA，比较多种问答架构，发现多代理系统表现最佳，促进教育领域智能问答研究。


<details>
  <summary>Details</summary>
Motivation: 现有问答系统多聚焦事实正确性，而教育等实际应用需体现指导和反思能力，缺乏涵盖多语言及长文本的导师式问答数据集与评估基准，推动此研究有助提升智能问答系统在教育和职业咨询中的实用价值。

Method: 构建了一个包含4种语言、近9000问答对和180小时视频内容的多语言长文本问答数据集；设计涵盖清晰度、契合度和学习价值的多维评估指标；在该框架下比较单代理、双代理、RAG及多代理问答系统性能；分析自动化评估与人工评判的一致性。

Result: 本文提出了MentorQA，一个面向指导式问答的多语言长文本视频数据集和评估框架，包含近9000个问答对，覆盖四种语言。该数据集超越了传统的事实正确性评估，引入了清晰度、契合度和学习价值等指导维度。作者比较了单代理、双代理、检索增强生成（RAG）和多代理架构，发现多代理系统在复杂话题和低资源语言中表现最佳。此外，指出基于大语言模型的自动评估与人工评判存在显著差异。该工作首次将指导式问答作为独立研究领域，并为教育AI中的多语言智能体架构和评估设计提供了基准。

Conclusion: 导师式问答作为独立研究问题，需考虑超越事实正确性的多维评估，且多代理问答架构在复杂和低资源场景下具有优势。基于LLM的自动评估存在可靠性问题。

Abstract: Question answering systems are typically evaluated on factual correctness, yet many real-world applications-such as education and career guidance-require mentorship: responses that provide reflection and guidance. Existing QA benchmarks rarely capture this distinction, particularly in multilingual and long-form settings. We introduce MentorQA, the first multilingual dataset and evaluation framework for mentorship-focused question answering from long-form videos, comprising nearly 9,000 QA pairs from 180 hours of content across four languages. We define mentorship-focused evaluation dimensions that go beyond factual accuracy, capturing clarity, alignment, and learning value. Using MentorQA, we compare Single-Agent, Dual-Agent, RAG, and Multi-Agent QA architectures under controlled conditions. Multi-Agent pipelines consistently produce higher-quality mentorship responses, with especially strong gains for complex topics and lower-resource languages. We further analyze the reliability of automated LLM-based evaluation, observing substantial variation in alignment with human judgments. Overall, this work establishes mentorship-focused QA as a distinct research problem and provides a multilingual benchmark for studying agentic architectures and evaluation design in educational AI. The dataset and evaluation framework are released at https://github.com/AIM-SCU/MentorQA.

</details>


### [10] [Systematicity between Forms and Meanings across Languages Supports Efficient Communication](https://arxiv.org/abs/2601.17181)
*Doreen Osmelak,Yang Xu,Michael Hahn,Kate McCurdy*

Main category: cs.CL

TL;DR: 本文研究了不同语言中语法意义（如人称、数）如何通过动词和代词形式表达，发现在简洁性和准确性之间存在竞争压力。提出了基于学习难度的新复杂性度量方法，更好地解释了语言形式中的系统性。


<details>
  <summary>Details</summary>
Motivation: 现有理论虽揭示了语言的高效交流性质，但未解释词形内部的系统性关系，本研究旨填补该空白。

Method: 提出了基于意义到形式映射可学习性的复杂性度量，用以评估语言系统的简洁性和准确性，构建模型分析多样语言中的语法表达。

Result: 模型能够区分已观测和未观测的语言系统，揭示了语法形式中存在的细微规律性，支持高效沟通与语言系统性间的关联。

Conclusion: 动词和代词形式受到简洁性和准确性之间的竞争影响，新的复杂性度量方法有效捕捉了语言形式中的细粒度规律，从而连接了高效沟通理论与语言系统性。

Abstract: Languages vary widely in how meanings map to word forms. These mappings have been found to support efficient communication; however, this theory does not account for systematic relations within word forms. We examine how a restricted set of grammatical meanings (e.g. person, number) are expressed on verbs and pronouns across typologically diverse languages. Consistent with prior work, we find that verb and pronoun forms are shaped by competing communicative pressures for simplicity (minimizing the inventory of grammatical distinctions) and accuracy (enabling recovery of intended meanings). Crucially, our proposed model uses a novel measure of complexity (inverse of simplicity) based on the learnability of meaning-to-form mappings. This innovation captures fine-grained regularities in linguistic form, allowing better discrimination between attested and unattested systems, and establishes a new connection from efficient communication theory to systematicity in natural language.

</details>


### [11] [Reasoning Beyond Literal: Cross-style Multimodal Reasoning for Figurative Language Understanding](https://arxiv.org/abs/2601.17197)
*Seyyed Saeid Cheshmi,Hahnemann Ortiz,James Mooney,Dongyeop Kang*

Main category: cs.CL

TL;DR: 本文提出了一种三步框架，旨在提升视觉-语言模型对隐喻、讽刺、幽默等多模态修辞语言的理解与推理能力，实现跨多种修辞风格的泛化。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在字面多模态任务表现良好，但对带有讽刺、幽默、隐喻等复杂意图与情感的修辞语言理解不足，且多模态信息使推理更具挑战性。

Method: 通过三步框架：(i)解释多模态修辞语言，(ii)提供透明推理链条，(iii)实现跨多修辞风格泛化。进行了四种修辞风格的测试，验证推理链重要性及泛化能力，且实现风格间迁移和联合训练。

Result: 实验结果表明：(1)引入推理链提升了多模态修辞理解，(2)一种修辞风格学得的推理能迁移至相关风格，(3)联合训练实现了优于大型模型的泛化推理能力。

Conclusion: 轻量级视觉-语言模型通过集成可验证推理链条，不仅提升了多模态修辞语言的理解能力，还实现了不同修辞风格间的迁移和泛化，优于更大规模的模型。

Abstract: Vision-language models (VLMs) have demonstrated strong reasoning abilities in literal multimodal tasks such as visual mathematics and science question answering. However, figurative language, such as sarcasm, humor, and metaphor, remains a significant challenge, as it conveys intent and emotion through subtle incongruities between expressed and intended meanings. In multimodal settings, accompanying images can amplify or invert textual meaning, demanding models that reason across modalities and account for subjectivity. We propose a three-step framework for developing efficient multimodal reasoning models that can (i) interpret multimodal figurative language, (ii) provide transparent reasoning traces, and (iii) generalize across multiple figurative styles. Experiments across four styles show that (1) incorporating reasoning traces substantially improves multimodal figurative understanding, (2) reasoning learned in one style can transfer to others, especially between related styles like sarcasm and humor, and (3) training jointly across styles yields a generalized reasoning VLM that outperforms much larger open- and closed-source models. Our findings show that lightweight VLMs with verifiable reasoning achieve robust cross-style generalization while providing inspectable reasoning traces for multimodal tasks. The code and implementation are available at https://github.com/scheshmi/CrossStyle-MMR.

</details>


### [12] [Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis](https://arxiv.org/abs/2601.17203)
*Scott Friedman,Sonja Schmer-Galunder,Anthony Chen,Jeffrey Rye*

Main category: cs.CL

TL;DR: 本文提出了一种量化词嵌入中的性别偏见的方法，并利用该方法揭示教育、政治、经济和健康领域的统计性别差距，基于2018年推特数据和多地区国家的比较分析。


<details>
  <summary>Details</summary>
Motivation: 当前NLP模型训练数据存在种族和性别偏见，这些偏见可能反映了训练数据文化中的实际差距，利用这些偏见可以帮助理解文化背景。

Method: 基于机器学习训练的词嵌入，量化性别偏见，并与实际统计的性别差距（教育、政治、经济、健康）进行关联分析。

Result: 在2018年推特数据（涵盖51个美国地区和99个国家）上验证了这种量化方法，词嵌入的性别偏见与多个国际与美国本土的统计性别差距高度相关。

Conclusion: 通过词嵌入中性别偏见的量化，可以揭示多个领域内的性别差距，且这些度量在不同地区具有较强的预测能力和规律性。

Abstract: Modern models for common NLP tasks often employ machine learning techniques and train on journalistic, social media, or other culturally-derived text. These have recently been scrutinized for racial and gender biases, rooting from inherent bias in their training text. These biases are often sub-optimal and recent work poses methods to rectify them; however, these biases may shed light on actual racial or gender gaps in the culture(s) that produced the training text, thereby helping us understand cultural context through big data. This paper presents an approach for quantifying gender bias in word embeddings, and then using them to characterize statistical gender gaps in education, politics, economics, and health. We validate these metrics on 2018 Twitter data spanning 51 U.S. regions and 99 countries. We correlate state and country word embedding biases with 18 international and 5 U.S.-based statistical gender gaps, characterizing regularities and predictive strength.

</details>


### [13] [DF-RAG: Query-Aware Diversity for Retrieval-Augmented Generation](https://arxiv.org/abs/2601.17212)
*Saadat Hasan Khan,Spencer Hong,Jingyu Wu,Kevin Lybarger,Youbing Yin,Erin Babinsky,Daben Liu*

Main category: cs.CL

TL;DR: 本文提出了DF-RAG方法，通过在检索阶段引入多样性，提升推理密集型问答的性能。该方法在不需额外微调的情况下动态优化多样性，显著优于传统的余弦相似度检索。


<details>
  <summary>Details</summary>
Motivation: 现有基于余弦相似度的检索方法倾向于获取高度相关但冗余的信息，降低了推理密集型QA的召回效率，亟需引入多样性以提升信息覆盖和检索质量。

Method: 基于最大边际相关（Maximal Marginal Relevance）框架，DF-RAG在检索时选择既相关又相互差异最大的内容片段，并能动态优化每个查询的多样性水平，无需额外微调。

Result: DF-RAG在多个复杂问答基准上取得比传统RAG方法4-10%的F1性能提升，并超越其他基线方法。此外，DF-RAG实现了接近理论最优提升的91.3%。

Conclusion: DF-RAG在推理密集型问答基准测试上，相比传统RAG提升了4-10%的F1分数，捕获了接近Oracle上限91.3%的潜在提升，为复杂问答任务提供了更优的检索增强生成方法。

Abstract: Retrieval-augmented generation (RAG) is a common technique for grounding language model outputs in domain-specific information. However, RAG is often challenged by reasoning-intensive question-answering (QA), since common retrieval methods like cosine similarity maximize relevance at the cost of introducing redundant content, which can reduce information recall. To address this, we introduce Diversity-Focused Retrieval-Augmented Generation (DF-RAG), which systematically incorporates diversity into the retrieval step to improve performance on complex, reasoning-intensive QA benchmarks. DF-RAG builds upon the Maximal Marginal Relevance framework to select information chunks that are both relevant to the query and maximally dissimilar from each other. A key innovation of DF-RAG is its ability to optimize the level of diversity for each query dynamically at test time without requiring any additional fine-tuning or prior information. We show that DF-RAG improves F1 performance on reasoning-intensive QA benchmarks by 4-10 percent over vanilla RAG using cosine similarity and also outperforms other established baselines. Furthermore, we estimate an Oracle ceiling of up to 18 percent absolute F1 gains over vanilla RAG, of which DF-RAG captures up to 91.3 percent.

</details>


### [14] [Beyond Outcome Verification: Verifiable Process Reward Models for Structured Reasoning](https://arxiv.org/abs/2601.17223)
*Massimiliano Pronesti,Anya Belz,Yufang Hou*

Main category: cs.CL

TL;DR: 本文提出的基于规则验证的过程奖励模型显著提升了医学证据风险偏倚评估中语言模型的推理准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有基于神经网络评分中间推理步骤的方法存在不透明、偏见和奖励作弊问题，亟需引入确定性规则验证以提升推理过程的可靠性和可解释性。

Method: 提出了VPRMs框架，使用确定性、基于规则的验证器对推理中间步骤进行程序化检查，并应用于医学证据合成风险偏倚评估。

Result: 本文提出了一种强化学习框架——可验证过程奖励模型（VPRMs），通过基于规则的确定性验证器对中间推理步骤进行检查，解决了现有基于神经评估者评分连锁推理步骤方法存在的不透明性、偏见和奖励作弊问题。应用于医学证据综合中风险偏倚评估，利用领域规则实现程序化验证推理路径。实验结果显示，VPRMs在多个数据集上能生成更符合领域规则的推理，步骤决策与最终标签一致性显著提升，F1指标较现有最先进模型提升20%，较可验证结果奖励提升6.5%，且大幅增强了证据基础和逻辑一致性。

Conclusion: VPRMs通过规则基中间步骤验证，提升了模型推理的逻辑一致性和结果准确性，优于现有基于结果奖励和神经评估的方法。

Abstract: Recent work on reinforcement learning with verifiable rewards (RLVR) has shown that large language models (LLMs) can be substantially improved using outcome-level verification signals, such as unit tests for code or exact-match checks for mathematics. In parallel, process supervision has long been explored as a way to shape the intermediate reasoning behaviour of LLMs, but existing approaches rely on neural judges to score chain-of-thought steps, leaving them vulnerable to opacity, bias, and reward hacking. To address this gap, we introduce Verifiable Process Reward Models (VPRMs), a reinforcement-learning framework in which intermediate reasoning steps are checked by deterministic, rule-based verifiers. We apply VPRMs to risk-of-bias assessment for medical evidence synthesis, a domain where guideline-defined criteria and rule-based decision paths enable programmatic verification of reasoning traces. Across multiple datasets, we find that VPRMs generate reasoning that adheres closely to domain rules and achieve substantially higher coherence between step-level decisions and final labels. Results show that VPRMs achieve up to 20% higher F1 than state-of-the-art models and 6.5% higher than verifiable outcome rewards, with substantial gains in evidence grounding and logical coherence.

</details>


### [15] [Retell, Reward, Repeat: Reinforcement Learning for Narrative Theory-Informed Story Generation](https://arxiv.org/abs/2601.17226)
*David Y. Liu,Xanthe Muston,Aditya Joshi,Sebastian Sequoiah-Grayson*

Main category: cs.CL

TL;DR: 本文提出利用强化学习对大型语言模型进行后训练，以提升自动故事生成的质量和多样性，表现出优于传统监督微调的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有自动故事生成方法训练和评估依赖有限的客观标准，难以捕捉故事叙述的主观性，且传统的监督微调方法存在局限。

Method: 采用强化学习（d-RLAIF）作为监督微调（SFT）后的替代方法进行自动故事生成（ASG）。利用Todorov的叙事平衡理论建立ASG质量的原则，使用7B和14B参数的大型语言模型作为评价者，通过这些原则来测试与人工注释者的一致性，同时为d-RLAIF提供奖励信号。使用Gemini-3-Flash评估后训练模型的输出，并与TimeTravel数据集中的人工故事进行比较。

Result: d-RLAIF能够生成更具多样性且更符合人类叙事惯例的故事，表现出比监督微调更好的效果。

Conclusion: 强化学习为处理像自动故事生成这类主观性强的语言任务提供了有效的后训练方法，有望推动该领域的发展。

Abstract: Despite the subjective nature of storytelling, past works on automatic story generation (ASG) have relied on limited ground truths for training and evaluation. In this work, we explore reinforcement learning (d-RLAIF) as a post-training alternative to supervised fine-tuning (SFT). We first apply Todorov's Theory of Narrative Equilibrium to establish principles that define desirable ASG qualities. We prompt 7B and 14B LLM-as-judge models with our principles to test alignment with human annotators and provide reward signals during d-RLAIF. We use Gemini-3-Flash to evaluate the output of our post-trained models and compare them to human-written stories from the TimeTravel dataset. We show that d-RLAIF offers a viable alternative to supervised fine-tuning (SFT)--producing stories that are more diverse and aligned with human narrative conventions. Our paper demonstrates the promise of reinforcement learning for linguistically grounded post-training for subjective tasks such as ASG.

</details>


### [16] [CaseFacts: A Benchmark for Legal Fact-Checking and Precedent Retrieval](https://arxiv.org/abs/2601.17230)
*Akshith Reddy Putta,Jacob Devasier,Chengkai Li*

Main category: cs.CL

TL;DR: 本文提出了CaseFacts，一种针对美国最高法院判例的法律事实自动核查基准，旨在验证非专业人士的法律陈述。利用大型语言模型合成案情概要，设计了语义相似性启发式方法以识别和验证复杂的法律推翻案。实验表明现有模型在该任务中表现仍有限，开放式网络检索反而降低性能。


<details>
  <summary>Details</summary>
Motivation: 现有自动事实核查多依赖静态语料，忽视法律领域中陈述的动态性与技术复杂性，致力于填补这一空白。

Method: 通过多阶段流水线，使用大型语言模型从专家案例摘要合成陈述，设计新的语义相似性启发式方法高效识别复杂法律推翻，并将陈述分为支持、反驳或推翻三类。

Result: 构建了包含6294条法律陈述的CaseFacts数据集，实验显示即使利用先进大模型及网络搜索，其法律事实验证性能仍需提升。

Conclusion: CaseFacts基准验证了现有先进大模型在法律事实核查方面的挑战性，尤其是高噪声检索信息可能降低模型准确性。

Abstract: Automated Fact-Checking has largely focused on verifying general knowledge against static corpora, overlooking high-stakes domains like law where truth is evolving and technically complex. We introduce CaseFacts, a benchmark for verifying colloquial legal claims against U.S. Supreme Court precedents. Unlike existing resources that map formal texts to formal texts, CaseFacts challenges systems to bridge the semantic gap between layperson assertions and technical jurisprudence while accounting for temporal validity. The dataset consists of 6,294 claims categorized as Supported, Refuted, or Overruled. We construct this benchmark using a multi-stage pipeline that leverages Large Language Models (LLMs) to synthesize claims from expert case summaries, employing a novel semantic similarity heuristic to efficiently identify and verify complex legal overrulings. Experiments with state-of-the-art LLMs reveal that the task remains challenging; notably, augmenting models with unrestricted web search degrades performance compared to closed-book baselines due to the retrieval of noisy, non-authoritative precedents. We release CaseFacts to spur research into legal fact verification systems.

</details>


### [17] [Frame-Guided Synthetic Claim Generation for Automatic Fact-Checking Using High-Volume Tabular Data](https://arxiv.org/abs/2601.17232)
*Jacob Devasier,Akshith Putta,Qing Wang,Alankrit Moses,Chengkai Li*

Main category: cs.CL

TL;DR: 本文提出了一个多语种大规模结构化数据事实核查数据集，采用创新生成方法并验证了大型模型需依赖检索推理，解决了复杂表格中事实核查的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有自动化事实核查基准很少关注验证对真实、海量结构化数据的主张，主要集中在小规模、精心策划的表格上。

Method: 采用基于六种语义框架的程序化数据点选择方法生成多语种合成声明，通过知识探测实验验证模型真实性能，构建了SQL基线系统用于评估。

Result: 提出了一个包含78503条合成声明、基于434个复杂OECD表格（每个表超过50万行）的多语种大型数据集；设计了基于六种语义框架的生成方法，支持英语、中文、西班牙语和印地语；通过知识探测实验证明大型语言模型未记忆这些事实，要求系统进行真实检索和推理；给出了SQL生成基线系统，表明基准具有高度挑战性，模型难以在庞大表格中准确检索证据。

Conclusion: 该数据集为解决真实世界、大规模结构化数据下的事实核查问题提供了重要资源，尤其暴露了证据检索作为核心瓶颈，为后续研究指明方向。

Abstract: Automated fact-checking benchmarks have largely ignored the challenge of verifying claims against real-world, high-volume structured data, instead focusing on small, curated tables. We introduce a new large-scale, multilingual dataset to address this critical gap. It contains 78,503 synthetic claims grounded in 434 complex OECD tables, which average over 500K rows each. We propose a novel, frame-guided methodology where algorithms programmatically select significant data points based on six semantic frames to generate realistic claims in English, Chinese, Spanish, and Hindi. Crucially, we demonstrate through knowledge-probing experiments that LLMs have not memorized these facts, forcing systems to perform genuine retrieval and reasoning rather than relying on parameterized knowledge. We provide a baseline SQL-generation system and show that our benchmark is highly challenging. Our analysis identifies evidence retrieval as the primary bottleneck, with models struggling to find the correct data in massive tables. This dataset provides a critical new resource for advancing research on this unsolved, real-world problem.

</details>


### [18] [PingPong: A Natural Benchmark for Multi-Turn Code-Switching Dialogues](https://arxiv.org/abs/2601.17277)
*Mohammad Rifqi Farhansyah,Hanif Muhammad Zhafran,Farid Adilazuarda,Shamsuddeen Hassan Muhammad,Maryam Ibrahim Mukhtar,Nedjma Ousidhoum,Genta Indra Winata,Ayu Purwarianti,Alham Fikri Aji*

Main category: cs.CL

TL;DR: 提出了自然多方代码切换对话基准PingPong，展示其数据自然多样性及对模型的挑战，推动多语多轮对话NLP研究。


<details>
  <summary>Details</summary>
Motivation: 当前多语言使用者中代码切换普遍存在，但现有基准测试难以准确反映日常交流中代码切换的复杂性。

Method: 提出PingPong数据集，包含五种语言组合的自然多方代码切换对话，涉及2-4人，包含真实且多线程的对话结构，设计三种下游任务（问答、对话摘要、主题分类）进行评测。

Result: 数据表现出比机器生成数据更自然、多样的特性，包括消息长度、发言者主导性和回复距离的多样性。多种先进语言模型在该数据集上表现有限，显示现有模型处理代码切换多语对话的能力不足。

Conclusion: PingPong数据集提供了真实且多样化的代码切换对话资源，是评估和推动多语种自然语言处理系统发展的重要基准，当前系统需加强对真实多语代码切换复杂性的适应能力。

Abstract: Code-switching is a widespread practice among the world's multilingual majority, yet few benchmarks accurately reflect its complexity in everyday communication. We present PingPong, a benchmark for natural multi-party code-switching dialogues covering five language-combination variations, some of which are trilingual. Our dataset consists of human-authored conversations among 2 to 4 participants covering authentic, multi-threaded structures where replies frequently reference much earlier points in the dialogue. We demonstrate that our data is significantly more natural and structurally diverse than machine-generated alternatives, offering greater variation in message length, speaker dominance, and reply distance. Based on these dialogues, we define three downstream tasks: Question Answering, Dialogue Summarization, and Topic Classification. Evaluations of several state-of-the-art language models on PingPong reveal that performance remains limited on code-switched inputs, underscoring the urgent need for more robust NLP systems capable of addressing the intricacies of real-world multilingual discourse.

</details>


### [19] [Mind the Ambiguity: Aleatoric Uncertainty Quantification in LLMs for Safe Medical Question Answering](https://arxiv.org/abs/2601.17284)
*Yaokun Liu,Yifan Liu,Phoebe Mbuvi,Zelin Li,Ruichen Yao,Gawon Lim,Dong Wang*

Main category: cs.CL

TL;DR: 针对医疗问答中因输入模糊引发的不确定性，提出AU检测与澄清机制，实现显著准确率提升。


<details>
  <summary>Details</summary>
Motivation: 医疗问答中用户提问模糊带来显著安全风险和答案准确率下降，迫切需要解决输入不确定性问题。

Method: 通过构建CV-MedBench基准测试，分析LLM内部激活模式中的亚勒托里不确定性（AU），引入AU-Probe模块检测输入歧义，并提出基于AU的"先澄清后回答"框架。

Result: 四个开放大语言模型的实验表明，所提框架较基线平均准确率提升9.48%，有效提升医疗问答安全性和可靠性。

Conclusion: 所提的AU-guided澄清机制无需微调或多次推理，提供高效、稳健的安全医疗问答解决方案，增强了医疗应用的可靠性。

Abstract: The deployment of Large Language Models in Medical Question Answering is severely hampered by ambiguous user queries, a significant safety risk that demonstrably reduces answer accuracy in high-stakes healthcare settings. In this paper, we formalize this challenge by linking input ambiguity to aleatoric uncertainty (AU), which is the irreducible uncertainty arising from underspecified input. To facilitate research in this direction, we construct CV-MedBench, the first benchmark designed for studying input ambiguity in Medical QA. Using this benchmark, we analyze AU from a representation engineering perspective, revealing that AU is linearly encoded in LLM's internal activation patterns. Leveraging this insight, we introduce a novel AU-guided "Clarify-Before-Answer" framework, which incorporates AU-Probe - a lightweight module that detects input ambiguity directly from hidden states. Unlike existing uncertainty estimation methods, AU-Probe requires neither LLM fine-tuning nor multiple forward passes, enabling an efficient mechanism to proactively request user clarification and significantly enhance safety. Extensive experiments across four open LLMs demonstrate the effectiveness of our QA framework, with an average accuracy improvement of 9.48% over baselines. Our framework provides an efficient and robust solution for safe Medical QA, strengthening the reliability of health-related applications. The code is available at https://github.com/yaokunliu/AU-Med.git, and the CV-MedBench dataset is released on Hugging Face at https://huggingface.co/datasets/yaokunl/CV-MedBench.

</details>


### [20] [Meta-Judging with Large Language Models: Concepts, Methods, and Challenges](https://arxiv.org/abs/2601.17312)
*Hugo Silva,Mateus Mendes,Hugo Gonçalo Oliveira*

Main category: cs.CL

TL;DR: 本文综述了大语言模型作为元裁判的新兴评估方式，分析其优势和不足，提出改进评估稳定性和可信性的未来方向。


<details>
  <summary>Details</summary>
Motivation: 现有LLM作为裁判在评估中存在提示词敏感性、系统偏差、冗长和虚假理由等脆弱性，需更稳健的评估范式。

Method: 通过综述现有文献，提出基于六大视角的框架系统地分析LLM作为元裁判的方法与机制。

Result: 总结了LLM作为元裁判的最新进展，揭示其优势，并指出当前面临的局限与未来发展方向。

Conclusion: LLM-as-a-Meta-Judge为自动化评估提供了更稳定、可信的方向，但仍需解决成本、提示词敏感性及模型偏见等挑战。

Abstract: Large language models (LLMs) are evolving fast and are now frequently used as evaluators, in a process typically referred to as LLM-as-a-Judge, which provides quality assessments of model outputs. However, recent research points out significant vulnerabilities in such evaluation, including sensitivity to prompts, systematic biases, verbosity effects, and unreliable or hallucinated rationales. These limitations motivated the development of a more robust paradigm, dubbed LLM-as-a-Meta-Judge. This survey reviews recent advances in meta-judging and organizes the literature, by introducing a framework along six key perspectives: (i) Conceptual Foundations, (ii) Mechanisms of Meta-Judging, (iii) Alignment Training Methods, (iv) Evaluation, (v) Limitations and Failure Modes, and (vi) Future Directions. By analyzing the limitations of LLM-as-a-Judge and summarizing recent advances in meta-judging by LLMs, we argue that LLM-as-a-Meta-Judge offers a promising direction for more stable and trustworthy automated evaluation, while highlighting remaining challenges related to cost, prompt sensitivity, and shared model biases, which must be addressed to advance the next generation of LLM evaluation methodologies.

</details>


### [21] [The Shadow Self: Intrinsic Value Misalignment in Large Language Model Agents](https://arxiv.org/abs/2601.17344)
*Chen Chen,Kim Young Il,Yuan Yang,Wenhao Su,Yilin Zhang,Xueluan Gong,Qian Wang,Yongsen Zheng,Ziyao Liu,Kwok-Yan Lam*

Main category: cs.CL

TL;DR: 本文提出了IMPRESS框架，系统评估大型语言模型中固有价值错位风险，发现其普遍存在且难以通过现有方法有效缓解。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型代理具有广泛自主性，但存在偏离人类价值观和伦理规范的风险，现有研究多集中于明确有害输入下的响应，缺少对完全无害且代理性情境中的价值错位风险的系统研究。

Method: 提出了IMPRESS框架，通过构建真实、完全无害且有上下文的情景集来系统评估固有价值错位风险，采用多阶段生成管道和严格质量控制生成基准测试，并对21个先进模型进行评测。

Result: 发现固有价值错位在多模型间普遍存在，误差率受动机、风险类型、模型规模和架构影响较大，而编码策略影响有限；上下文化和框架机制显著影响行为；且现有安全提示和防护措施效果不稳定或有限。

Conclusion: 本论文确定了大型语言模型中固有价值错位（Intrinsic Value Misalignment）作为一种广泛存在且被低估的安全风险，并证明现有的安全缓解策略效果有限。

Abstract: Large language model (LLM) agents with extended autonomy unlock new capabilities, but also introduce heightened challenges for LLM safety. In particular, an LLM agent may pursue objectives that deviate from human values and ethical norms, a risk known as value misalignment. Existing evaluations primarily focus on responses to explicit harmful input or robustness against system failure, while value misalignment in realistic, fully benign, and agentic settings remains largely underexplored. To fill this gap, we first formalize the Loss-of-Control risk and identify the previously underexamined Intrinsic Value Misalignment (Intrinsic VM). We then introduce IMPRESS (Intrinsic Value Misalignment Probes in REalistic Scenario Set), a scenario-driven framework for systematically assessing this risk. Following our framework, we construct benchmarks composed of realistic, fully benign, and contextualized scenarios, using a multi-stage LLM generation pipeline with rigorous quality control. We evaluate Intrinsic VM on 21 state-of-the-art LLM agents and find that it is a common and broadly observed safety risk across models. Moreover, the misalignment rates vary by motives, risk types, model scales, and architectures. While decoding strategies and hyperparameters exhibit only marginal influence, contextualization and framing mechanisms significantly shape misalignment behaviors. Finally, we conduct human verification to validate our automated judgments and assess existing mitigation strategies, such as safety prompting and guardrails, which show instability or limited effectiveness. We further demonstrate key use cases of IMPRESS across the AI Ecosystem. Our code and benchmark will be publicly released upon acceptance.

</details>


### [22] [Do readers prefer AI-generated Italian short stories?](https://arxiv.org/abs/2601.17363)
*Michael Farrell*

Main category: cs.CL

TL;DR: 本研究通过20名参与者的盲测，比较了AI生成的意大利短篇故事与著名作家阿尔贝托·莫拉维亚的作品，结果显示AI作品略受欢迎，且无显著的人口统计学或阅读习惯影响偏好。


<details>
  <summary>Details</summary>
Motivation: 探究读者是否更喜欢AI生成的文学作品，及人口统计变量和阅读习惯是否影响其偏好，验证AI文学作品的接受度与编辑必要性。

Method: 采用盲测方式，20名参与者阅读并评价两篇由ChatGPT-4o生成和一篇阿尔贝托·莫拉维亚创作的短篇故事，同时收集其阅读习惯及人口统计信息。

Result: AI生成的故事获得略高的平均评分和更频繁的偏好选择，但两者差异不大，且无统计学显著联系于参与者的背景信息。

Conclusion: 参与者略偏好AI生成的短篇故事，且文本偏好不受年龄、性别、教育和阅读习惯等因素影响，挑战了读者偏好人类创作小说的传统假设。

Abstract: This study investigates whether readers prefer AI-generated short stories in Italian over one written by a renowned Italian author. In a blind setup, 20 participants read and evaluated three stories, two created with ChatGPT-4o and one by Alberto Moravia, without being informed of their origin. To explore potential influencing factors, reading habits and demographic data, comprising age, gender, education and first language, were also collected. The results showed that the AI-written texts received slightly higher average ratings and were more frequently preferred, although differences were modest. No statistically significant associations were found between text preference and demographic or reading-habit variables. These findings challenge assumptions about reader preference for human-authored fiction and raise questions about the necessity of synthetic-text editing in literary contexts.

</details>


### [23] [Parameter Efficient Fine Tuning Llama 3.1 for Answering Arabic Legal Questions: A Case Study on Jordanian Laws](https://arxiv.org/abs/2601.17364)
*Mohammed Fasha,Bassam Hammo,Bilal Sowan,Husam Barham,Esam Nsour*

Main category: cs.CL

TL;DR: 本研究基于约旦法律数据，利用低资源高效的微调技术改进了Llama-3.1模型的阿拉伯语法律问答能力，提升了准确性和推理水平。


<details>
  <summary>Details</summary>
Motivation: 探讨如何针对阿拉伯语法律问答任务，微调大型语言模型以提高法律推理和准确性。

Method: 使用PEFT的LoRA适配器及4-bit量化技术，通过Unsloth框架对Llama-3.1两个版本进行微调，利用约旦法律构建6000条法律问答对的定制数据集，采用BLEU和ROUGE指标进行性能评估。

Result: 微调后的模型在法律推理和准确性上均有显著提升，同时实现了资源利用效率的优化。

Conclusion: 通过有效的微调技术和量化手段，Llama-3.1大语言模型成功适配于阿拉伯语法律领域，显示出在领域特定任务中的应用潜力。

Abstract: This study uses Jordanian law as a case study to explore the fine-tuning of the Llama-3.1 large language model for Arabic question-answering. Two versions of the model - Llama-3.1-8B-bnb-4bit and Llama-3.1-8B-Instruct-bnb-4bit - were fine-tuned using parameter-efficient fine-tuning (PEFT) with LoRA adapters and 4-bit quantized models, leveraging the Unsloth framework for accelerated and resource-efficient training. A custom dataset of 6000 legal question-answer pairs was curated from Jordanian laws and formatted into structured prompts. Performance was evaluated using the BLEU and the ROUGE metrics to compare the fine-tuned models to their respective base versions. Results demonstrated improved legal reasoning and accuracy while achieving resource efficiency through quantization and optimized fine-tuning strategies. This work underscores the potential of adapting large language models for Arabic legal domains and highlights effective techniques for fine-tuning domain-specific tasks.

</details>


### [24] [Elastic Attention: Test-time Adaptive Sparsity Ratios for Efficient Transformers](https://arxiv.org/abs/2601.17367)
*Zecheng Tang,Quantong Qiu,Yi Yang,Zhiyi Hong,Haiya Xiang,Kebin Liu,Qingqing Dang,Juntao Li,Min Zhang*

Main category: cs.CL

TL;DR: 提出Elastic Attention动态调整注意力稀疏度，提升长上下文大模型性能和效率。


<details>
  <summary>Details</summary>
Motivation: 解决标准注意力机制在长上下文场景下的计算复杂度高和静态混合注意力策略在推理过程中无法适应任务稀疏度变化的问题。

Method: 通过在预训练模型中集成轻量级Attention Router，实现每个注意力头动态分配计算模式（稀疏或全连接），进而动态调整整体注意力稀疏度。

Result: 本文提出了一种名为Elastic Attention的动态注意力机制，通过引入轻量级的Attention Router，能够根据输入动态调整模型整体的稀疏度，从而解决了标准注意力机制的二次复杂度瓶颈问题，同时避免了传统混合注意力策略中静态计算比例的限制。该方法在8xA800 GPU上训练12小时即可显著提升长上下文场景中主流大语言模型的性能及推理效率。实验证明在三个长上下文任务基准上，Elastic Attention优于现有方法。

Conclusion: Elastic Attention有效克服了传统注意力机制的计算瓶颈，实现了输入自适应的动态稀疏分配，显著提升了模型在长上下文任务中的表现和推理效率。

Abstract: The quadratic complexity of standard attention mechanisms poses a significant scalability bottleneck for large language models (LLMs) in long-context scenarios. While hybrid attention strategies that combine sparse and full attention within a single model offer a viable solution, they typically employ static computation ratios (i.e., fixed proportions of sparse versus full attention) and fail to adapt to the varying sparsity sensitivities of downstream tasks during inference. To address this issue, we propose Elastic Attention, which allows the model to dynamically adjust its overall sparsity based on the input. This is achieved by integrating a lightweight Attention Router into the existing pretrained model, which dynamically assigns each attention head to different computation modes. Within only 12 hours of training on 8xA800 GPUs, our method enables models to achieve both strong performance and efficient inference. Experiments across three long-context benchmarks on widely-used LLMs demonstrate the superiority of our method.

</details>


### [25] [WarrantScore: Modeling Warrants between Claims and Evidence for Substantiation Evaluation in Peer Reviews](https://arxiv.org/abs/2601.17377)
*Kiyotada Mori,Shohei Tanaka,Tosho Hirasawa,Tadashi Kozuno,Koichiro Yoshino,Yoshitaka Ushiku*

Main category: cs.CL

TL;DR: 本研究提出了一种基于声明与证据逻辑推理的新评估指标，提升了科学评论中论证支持度的评估准确性，有助于提高同行评审效率。


<details>
  <summary>Details</summary>
Motivation: 针对同行评审过程中人力资源短缺的问题，探索利用语言模型减少审稿人工作量，尤其是通过更精确地评估科学评论中论证的支持程度。

Method: 提出了一种新的评估指标，通过提取科学评论中的论点核心组成部分（声明和证据），并评估声明与证据之间的逻辑推理关系来判断支持程度。

Result: 实验结果表明，提出的方法在与人工评分的相关性方面优于传统方法，能够更准确地评估声明与证据之间的逻辑推理关系。

Conclusion: 该方法更好地捕捉了评论中的逻辑推理关系，能够有效支持同行评审流程的效率提升，具有较好的应用潜力。

Abstract: The scientific peer-review process is facing a shortage of human resources due to the rapid growth in the number of submitted papers. The use of language models to reduce the human cost of peer review has been actively explored as a potential solution to this challenge. A method has been proposed to evaluate the level of substantiation in scientific reviews in a manner that is interpretable by humans. This method extracts the core components of an argument, claims and evidence, and assesses the level of substantiation based on the proportion of claims supported by evidence. The level of substantiation refers to the extent to which claims are based on objective facts. However, when assessing the level of substantiation, simply detecting the presence or absence of supporting evidence for a claim is insufficient; it is also necessary to accurately assess the logical inference between a claim and its evidence. We propose a new evaluation metric for scientific review comments that assesses the logical inference between claims and evidence. Experimental results show that the proposed method achieves a higher correlation with human scores than conventional methods, indicating its potential to better support the efficiency of the peer-review process.

</details>


### [26] [Revisiting Modality Invariance in a Multilingual Speech-Text Model via Neuron-Level Analysis](https://arxiv.org/abs/2601.17387)
*Toshiki Nakai,Varsha Suresh,Vera Demberg*

Main category: cs.CL

TL;DR: 本文通过对SeamlessM4T v2模型的分析，探讨多语言语音文本基础模型在处理口语与书面语时是否存在一致的内部语言表示，发现存在不完全的模态不变性。


<details>
  <summary>Details</summary>
Motivation: 多语言语音文本基础模型需统一处理多种语言和模态，但其内部是否对相同语言的语音与文本保持一致表示尚不明确，理解这一点有助于提升模型性能和稳健性。

Method: 通过平均精度排名识别语言和模态选择性神经元，采用中位数替换干预推理阶段行为，分析语言和模态激活不均衡性，多角度研究编码信息分布和神经元功能。

Result: 发现编码器表现出一定程度的语言模态不变性，但共享解码器难以完全恢复语言信息，尤其在语音转文本时表现更差，且在关键注意力投影中存在局部模态选择性结构，语音条件和非主导脚本表现出更高的激活集中度。

Conclusion: 尽管编码器表示趋于语言无关，但共享解码器在生成模态无关表示时难以恢复语言信息，特别是在语音到文本的适应中，且语音条件解码和非主导脚本对少数神经元依赖较重，增加了模型的脆弱性。

Abstract: Multilingual speech-text foundation models aim to process language uniformly across both modality and language, yet it remains unclear whether they internally represent the same language consistently when it is spoken versus written. We investigate this question in SeamlessM4T v2 through three complementary analyses that probe where language and modality information is encoded, how selective neurons causally influence decoding, and how concentrated this influence is across the network. We identify language- and modality-selective neurons using average-precision ranking, investigate their functional role via median-replacement interventions at inference time, and analyze activation-magnitude inequality across languages and modalities. Across experiments, we find evidence of incomplete modality invariance. Although encoder representations become increasingly language-agnostic, this compression makes it more difficult for the shared decoder to recover the language of origin when constructing modality-agnostic representations, particularly when adapting from speech to text. We further observe sharply localized modality-selective structure in cross-attention key and value projections. Finally, speech-conditioned decoding and non-dominant scripts exhibit higher activation concentration, indicating heavier reliance on a small subset of neurons, which may underlie increased brittleness across modalities and languages.

</details>


### [27] [CLM-Bench: Benchmarking and Analyzing Cross-lingual Misalignment of LLMs in Knowledge Editing](https://arxiv.org/abs/2601.17397)
*Yucheng Hu,Wei Zhou,Juesi Xiao*

Main category: cs.CL

TL;DR: 该论文提出以中文为主的文化感知多语言知识编辑基准CLM-Bench，发现跨语言知识编辑存在严重失效，强调原生文化背景数据的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有多语言知识编辑评测依赖机械翻译的英语中心数据集，导致翻译伪影且忽视目标语言特有文化实体，无法真实反映大模型知识分布。

Method: 提出CLM-Bench基于中文文化背景构建的评测基准，收集1010对中英对应的知识编辑对，并通过层级表示分析揭示语言间编辑向量的正交性和线性可加性。

Result: 在CLM-Bench上对代表性大模型进行实验，发现跨语言知识编辑表现出严重的错位现象，编辑效果不能有效传递至另一语言。

Conclusion: 当前多语言知识编辑方法存在跨语言编辑失效问题，编辑向量在不同语言间几乎正交，无法实现有效的跨语言传播。

Abstract: Knowledge Editing (KE) has emerged as a promising paradigm for updating facts in Large Language Models (LLMs) without retraining. However, progress in Multilingual Knowledge Editing (MKE) is currently hindered by biased evaluation frameworks. We observe that existing MKE benchmarks are typically constructed by mechanically translating English-centric datasets into target languages (e.g., English-to-Chinese). This approach introduces translation artifacts and neglects culturally specific entities native to the target language, failing to reflect the true knowledge distribution of LLMs. To address this, we propose CLM-Bench, a culture-aware benchmark constructed using a native Chinese-first methodology. We curate 1,010 high-quality CounterFact pairs rooted in Chinese cultural contexts and align them with English counterparts. Using CLM-Bench, we conduct extensive experiments on representative LLMs (e.g., Llama-3, Qwen2) and reveal a significant Cross-lingual Misalignment: edits in one language function independently and fail to propagate to the other. We further provide a geometric explanation via layer-wise representation analysis, demonstrating that edit vectors for Chinese and English are nearly orthogonal -- residing in disjoint subspaces -- while mixed-lingual editing exhibits linear additivity of these vectors. Our findings challenge the effectiveness of current methods in cross-lingual transfer and underscore the importance of culturally native benchmarks.

</details>


### [28] [Oops, Wait: Token-Level Signals as a Lens into LLM Reasoning](https://arxiv.org/abs/2601.17421)
*Jaehui Hwang,Dongyoon Han,Sangdoo Yun,Byeongho Heo*

Main category: cs.CL

TL;DR: 本文系统分析了大型语言模型中话语标记与推理正确性的关系，发现训练策略影响信号利用，而模型规模影响较小。


<details>
  <summary>Details</summary>
Motivation: 分析大型语言模型中类似“wait”和“therefore”等话语标记如何反映其推理过程，探索不同训练策略和模型规模对这些信号的影响。

Method: 通过比对多模型中话语标记的token概率，结合具体标记“wait”与答案概率的关系，评估推理信号的表现及其在不同训练条件下的变化。

Result: 发现特定话语标记与推理正确性高度相关，这种关联随训练策略变化但在模型规模上保持稳定。微调于小规模数据集的模型通过这些信号获得推理能力，但只部分利用了这些信号。

Conclusion: 本文提供了一个系统视角来观察和理解大型语言模型推理动态，揭示了训练策略对话语信号利用的影响。

Abstract: The emergence of discourse-like tokens such as "wait" and "therefore" in large language models (LLMs) has offered a unique window into their reasoning processes. However, systematic analyses of how such signals vary across training strategies and model scales remain lacking. In this paper, we analyze token-level signals through token probabilities across various models. We find that specific tokens strongly correlate with reasoning correctness, varying with training strategies while remaining stable across model scales. A closer look at the "wait" token in relation to answer probability demonstrates that models fine-tuned on small-scale datasets acquire reasoning ability through such signals but exploit them only partially. This work provides a systematic lens to observe and understand the dynamics of LLM reasoning.

</details>


### [29] [Clustering-driven Memory Compression for On-device Large Language Models](https://arxiv.org/abs/2601.17443)
*Ondrej Bohdal,Pramit Saha,Umberto Michieli,Mete Ozay,Taha Ceritli*

Main category: cs.CL

TL;DR: 本文提出了一种基于聚类的记忆压缩策略，通过在合并前对用户记忆进行相似性聚类，实现了在保持个性化生成质量的同时有效减少上下文占用。


<details>
  <summary>Details</summary>
Motivation: 当前通过简单拼接用户过去交互记忆以实现个性化生成，会因上下文限制而导致效率低下，且直接平均压缩会产生语义冲突影响性能。

Method: 通过计算用户记忆的相似性，将其划分到不同聚类中，并在每个聚类内合并记忆，从而减少冗余信息且保持语义一致性。

Result: 实验结果表明，该聚类驱动的记忆合并方式显著减少了记忆token数量，并且提升了生成模型的个性化表现和响应质量。

Conclusion: 基于聚类的记忆压缩方法能显著降低记忆token数量，并在固定上下文预算下提高生成质量，优于简单平均和直接拼接等基线方法。

Abstract: Large language models (LLMs) often rely on user-specific memories distilled from past interactions to enable personalized generation. A common practice is to concatenate these memories with the input prompt, but this approach quickly exhausts the limited context available in on-device LLMs. Compressing memories by averaging can mitigate context growth, yet it frequently harms performance due to semantic conflicts across heterogeneous memories. In this work, we introduce a clustering-based memory compression strategy that balances context efficiency and personalization quality. Our method groups memories by similarity and merges them within clusters prior to concatenation, thereby preserving coherence while reducing redundancy. Experiments demonstrate that our approach substantially lowers the number of memory tokens while outperforming baseline strategies such as naive averaging or direct concatenation. Furthermore, for a fixed context budget, clustering-driven merging yields more compact memory representations and consistently enhances generation quality.

</details>


### [30] [Revealing the Truth with ConLLM for Detecting Multi-Modal Deepfakes](https://arxiv.org/abs/2601.17530)
*Gautam Siddharth Kashyap,Harsh Joshi,Niharika Jain,Ebad Shabbir,Jiechao Gao,Nipun Joshi,Usman Naseem*

Main category: cs.CL

TL;DR: 本文提出ConLLM框架，通过对比学习和大语言模型推理解决了深度伪造检测中的模态碎片化和语义推理问题，显著提升多模态检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有深伪检测方法存在模态碎片化导致的泛化能力差以及浅层跨模态推理限制，不能有效检测细粒度语义不一致。

Method: 提出了一个两阶段的混合框架ConLLM，第一阶段利用预训练模型提取模态特定嵌入，第二阶段通过对比学习对齐这些嵌入并用大语言模型进行深度语义推理。

Result: ConLLM在音频、视频和音视频多模态上表现优异，音频深伪错误接受率降低50%，视频准确率提升8%，音视频任务准确率提升约9%。消融实验显示预训练模型嵌入带来9%-10%的持续提升。

Conclusion: ConLLM通过对跨模态特征进行对比学习和大语言模型推理，有效解决深度伪造检测中的模态碎片化和浅层语义推理问题，提高了多模态深伪检测性能。

Abstract: The rapid rise of deepfake technology poses a severe threat to social and political stability by enabling hyper-realistic synthetic media capable of manipulating public perception. However, existing detection methods struggle with two core limitations: (1) modality fragmentation, which leads to poor generalization across diverse and adversarial deepfake modalities; and (2) shallow inter-modal reasoning, resulting in limited detection of fine-grained semantic inconsistencies. To address these, we propose ConLLM (Contrastive Learning with Large Language Models), a hybrid framework for robust multimodal deepfake detection. ConLLM employs a two-stage architecture: stage 1 uses Pre-Trained Models (PTMs) to extract modality-specific embeddings; stage 2 aligns these embeddings via contrastive learning to mitigate modality fragmentation, and refines them using LLM-based reasoning to address shallow inter-modal reasoning by capturing semantic inconsistencies. ConLLM demonstrates strong performance across audio, video, and audio-visual modalities. It reduces audio deepfake EER by up to 50%, improves video accuracy by up to 8%, and achieves approximately 9% accuracy gains in audio-visual tasks. Ablation studies confirm that PTM-based embeddings contribute 9%-10% consistent improvements across modalities.

</details>


### [31] [Less is More for RAG: Information Gain Pruning for Generator-Aligned Reranking and Evidence Selection](https://arxiv.org/abs/2601.17532)
*Zhipeng Song,Yizhi Zhou,Xiangyu Kong,Jiulong Jiao,Xinrui Bao,Xu You,Xueqing Shi,Yuhang Zhou,Heng Qi*

Main category: cs.CL

TL;DR: 提出的IGP模块通过生成器对齐的效用信号有效筛选证据，显著提升多证据问答的准确率并大幅减少输入规模。


<details>
  <summary>Details</summary>
Motivation: 现有的基于检索增强生成（RAG）模型在有限的上下文预算内，如何有效选择注入的检索段落是关键挑战。传统的检索相关性指标（如NDCG）与问答质量的相关性较弱，甚至在多段落注入情况下可能产生负相关，由于冗余和轻微冲突影响生成稳定性。

Method: 通过生成器对齐的效用信号来评估证据段落效用，基于该信号对检索结果进行重排序和剪枝，过滤掉弱或有害段落，最终在有限上下文预算下选取最有价值的证据注入生成模型。

Result: 提出了一种信息增益剪枝（Information Gain Pruning, IGP）模块，以生成器对齐的效用信号进行重排序和剪枝，过滤弱或有害段落，不改变现有预算接口。在五个开放领域问答基准、多种检索器和生成器上，IGP持续改善质量与成本的权衡。在多证据场景下，IGP相比仅用检索器的基线，提升了12-20%的平均F1分数，并减少了76-79%的最终输入标记数。

Conclusion: IGP是一种实用的重排序与剪枝机制，能提升多段落注入下的问答生成质量，同时大幅降低输入代价，适用于多种检索器和生成器组合。

Abstract: Retrieval-augmented generation (RAG) grounds large language models with external evidence, but under a limited context budget, the key challenge is deciding which retrieved passages should be injected. We show that retrieval relevance metrics (e.g., NDCG) correlate weakly with end-to-end QA quality and can even become negatively correlated under multi-passage injection, where redundancy and mild conflicts destabilize generation. We propose \textbf{Information Gain Pruning (IGP)}, a deployment-friendly reranking-and-pruning module that selects evidence using a generator-aligned utility signal and filters weak or harmful passages before truncation, without changing existing budget interfaces. Across five open-domain QA benchmarks and multiple retrievers and generators, IGP consistently improves the quality--cost trade-off. In a representative multi-evidence setting, IGP delivers about +12--20% relative improvement in average F1 while reducing final-stage input tokens by roughly 76--79% compared to retriever-only baselines.

</details>


### [32] [Improving User Privacy in Personalized Generation: Client-Side Retrieval-Augmented Modification of Server-Side Generated Speculations](https://arxiv.org/abs/2601.17569)
*Alireza Salemi,Hamed Zamani*

Main category: cs.CL

TL;DR: 本论文提出了$P^3$框架，实现了在不向服务端暴露用户隐私数据的情况下的个性化大语言模型输出。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强的个性化方法面临隐私暴露与模型能力低下的权衡，需实现既高效又保护隐私的个性化生成。

Method: 服务器端模型根据用户查询生成草稿标记序列，客户端模型基于用户私有资料评价并修改草稿，交互迭代直到生成最终输出。

Result: 在LaMP-QA数据集上的个性化问答任务中，$P^3$性能提升7.4%-9%，恢复90.3%-95.7%的泄露上限效用，隐私泄露仅增加1.5%-3.5%，客户端仅生成9.2%的标记。

Conclusion: $P^3$显著优于非个性化的服务器端模型和个性化的客户端模型，达到接近泄露全部数据的上限性能，同时保证用户隐私安全，且适合边缘设备部署。

Abstract: Personalization is crucial for aligning Large Language Model (LLM) outputs with individual user preferences and background knowledge. State-of-the-art solutions are based on retrieval augmentation, where relevant context from a user profile is retrieved for LLM consumption. These methods deal with a trade-off between exposing retrieved private data to cloud providers and relying on less capable local models. We introduce $P^3$, an interactive framework for high-quality personalization without revealing private profiles to server-side LLMs. In $P^3$, a large server-side model generates a sequence of $k$ draft tokens based solely on the user query, while a small client-side model, with retrieval access to the user's private profile, evaluates and modifies these drafts to better reflect user preferences. This process repeats until an end token is generated. Experiments on LaMP-QA, a recent benchmark consisting of three personalized question answering datasets, show that $P^3$ consistently outperforms both non-personalized server-side and personalized client-side baselines, achieving statistically significant improvements of $7.4%$ to $9%$ on average. Importantly, $P^3$ recovers $90.3%$ to $95.7%$ of the utility of a ``leaky'' upper-bound scenario in which the full profile is exposed to the large server-side model. Privacy analyses, including linkability and attribute inference attacks, indicate that $P^3$ preserves the privacy of a non-personalized server-side model, introducing only marginal additional leakage ($1.5%$--$3.5%$) compared to submitting a query without any personal context. Additionally, the framework is efficient for edge deployment, with the client-side model generating only $9.2%$ of the total tokens. These results demonstrate that $P^3$ provides a practical, effective solution for personalized generation with improved privacy.

</details>


### [33] [Sequence Repetition Enhances Token Embeddings and Improves Sequence Labeling with Decoder-only Language Models](https://arxiv.org/abs/2601.17585)
*Matija Luka Kukić,Marko Čuljak,David Dukić,Martin Tutek,Jan Šnajder*

Main category: cs.CL

TL;DR: 本文提出利用序列重复方法使解码器语言模型支持双向上下文，从而提升序列标注任务性能，展现出优于传统编码器模型的潜力和计算效率优势。


<details>
  <summary>Details</summary>
Motivation: 解码器仅模型在序列标注任务中的表现受限于单向上下文，而序列标注任务自然需要双向上下文支持。现有的因果屏蔽去除方法虽可实现双向，但改动较大，亟需一种更轻量的适配方法。

Method: 本文提出序列重复(SR)方法，通过重复输入序列实现解码器模型的双向性，无需大幅修改模型结构。并通过微调实验证明SR的有效性，分析了重复次数与中间层嵌入的影响。

Result: 实验表明SR方法使解码器模型具备双向能力，提升了Token级嵌入质量，表现优于编码器模型和未屏蔽解码器模型。同时，重复次数增加不会降低性能，中间层嵌入效果与最终层相当但计算更高效。

Conclusion: 序列重复技术有效缓解了只解码器模型的结构限制，提升了语言模型在序列标注任务中的适用性和效率，为解码器模型应用于更多Token级任务提供了新思路。

Abstract: Modern language models (LMs) are trained in an autoregressive manner, conditioned only on the prefix. In contrast, sequence labeling (SL) tasks assign labels to each individual input token, naturally benefiting from bidirectional context. This discrepancy has historically led SL to rely on inherently bidirectional encoder-only models. However, the rapid development of decoder-only models has raised the question of whether they can be adapted to SL. While causal mask removal has emerged as a viable technique for adapting decoder-only models to leverage the full context for SL, it requires considerable changes to the base model functionality. In this work, we explore sequence repetition (SR) as a less invasive alternative for enabling bidirectionality in decoder-only models. Through fine-tuning experiments, we show that SR inherently makes decoders bidirectional, improving the quality of token-level embeddings and surpassing encoders and unmasked decoders. Contrary to earlier claims, we find that increasing the number of repetitions does not degrade SL performance. Finally, we demonstrate that embeddings from intermediate layers are highly effective for SR, comparable to those from final layers, while being significantly more efficient to compute. Our findings underscore that SR alleviates the structural limitations of decoders, enabling more efficient and adaptable LMs and broadening their applicability to other token-level tasks.

</details>


### [34] [From Chains to DAGs: Probing the Graph Structure of Reasoning in LLMs](https://arxiv.org/abs/2601.17593)
*Tianjun Zhong,Linyang He,Nima Mesgarani*

Main category: cs.CL

TL;DR: 该论文通过构建框架探测大型语言模型隐藏状态中是否编码了推理的DAG结构，结果显示模型中呈现出非线性、图结构化的推理表示。


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型多将推理视为线性步骤链，忽视了许多推理问题更自然的有向无环图（DAG）结构，这种结构中中间结论依赖多个前提，可能并行分支再汇合。理解模型内部是否反映这种图结构的推理仍是未解难题。

Method: 提出Reasoning DAG Probing框架，通过轻量级探针训练，利用模型隐藏状态预测推理节点的图论属性（节点深度和节点对距离），分析不同层中DAG结构的出现及其被破坏时的影响。

Result: 实验结果表明推理DAG几何结构能够在线性可访问的形式中在模型中间层显著编码，并且不同深度的节点和模型规模影响结构的可恢复性，表明大型语言模型的推理不仅是顺序的，还体现出可测量的内部图结构。

Conclusion: 推理过程在大型语言模型内部呈现出图结构编码，说明对推理的理解应超越线性步骤链，考虑图结构特性，为深入揭示模型推理机制提供新工具和视角。

Abstract: Recent progress in large language models has renewed interest in mechanistically characterizing how multi-step reasoning is represented and computed. While much prior work treats reasoning as a linear chain of steps, many reasoning problems are more naturally structured as directed acyclic graphs (DAGs), where intermediate conclusions may depend on multiple premises, branch into parallel sub-derivations, and later merge or be reused. Understanding whether such graph-structured reasoning is reflected in model internals remains an open question.
  In this work, we introduce Reasoning DAG Probing, a framework that directly asks whether LLM hidden states encode the geometry of a reasoning DAG in a linearly accessible form, and where this structure emerges across layers. Within this framework, we associate each reasoning node with a textual realization and train lightweight probes to predict two graph-theoretic properties from hidden states: node depth and pairwise node distance. We use these probes to analyze the layerwise emergence of DAG structure and evaluate controls that disrupt reasoning-relevant structure while preserving superficial textual properties. Our results provide evidence that reasoning DAG geometry is meaningfully encoded in intermediate layers, with recoverability varying systematically by node depth and model scale, suggesting that LLM reasoning is not only sequential but exhibits measurable internal graph structure.

</details>


### [35] [Learning to Ideate for Machine Learning Engineering Agents](https://arxiv.org/abs/2601.17596)
*Yunxiang Zhang,Kang Zhou,Zhichao Xu,Kiran Ramnath,Yun Zhou,Sangmin Woo,Haibo Ding,Lin Lee Cheong*

Main category: cs.CL

TL;DR: 通过分离创意生成与执行的双代理框架并利用强化学习训练，显著提升了机器学习算法的优化效果。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习工程代理难以有效迭代优化其实现的算法，因此需要一个机制来改善算法的战略创意和实施过程。

Method: 提出了一个双代理框架MLE-Ideator，将创意生成与实现分开，允许实现代理请求创意代理的战略帮助，并训练创意代理以优化生成的算法想法。

Result: MLE-Ideator在无训练环境下超过了单一实现代理基线；经过强化学习训练的创意代理在少量训练样本下提升了11.5%性能，且优于Claude Sonnet 3.5模型。

Conclusion: 该方法展示了训练战略性AI系统促进科学发现的潜力，为提高机器学习工程代理的效果提供了有效路径。

Abstract: Existing machine learning engineering (MLE) agents struggle to iteratively optimize their implemented algorithms for effectiveness. To address this, we introduce MLE-Ideator, a dual-agent framework that separates ideation from implementation. In our system, an implementation agent can request strategic help from a dedicated Ideator. We show this approach is effective in two ways. First, in a training-free setup, our framework significantly outperforms implementation-only agent baselines on MLE-Bench. Second, we demonstrate that the Ideator can be trained with reinforcement learning (RL) to generate more effective ideas. With only 1K training samples from 10 MLE tasks, our RL-trained Qwen3-8B Ideator achieves an 11.5% relative improvement compared to its untrained counterpart and surpasses Claude Sonnet 3.5. These results highlights a promising path toward training strategic AI systems for scientific discovery.

</details>


### [36] [What Language Models Know But Don't Say: Non-Generative Prior Extraction for Generalization](https://arxiv.org/abs/2601.17609)
*Sara Rezaeimanesh,Mohammad M. Ghassemi*

Main category: cs.CL

TL;DR: 针对小样本和分布偏移问题，提出通过大型语言模型token级置信信息构建贝叶斯逻辑回归先验的LoID方法，显著提升泛化性能。


<details>
  <summary>Details</summary>
Motivation: 医疗和金融等领域中，标注数据规模大且成本高，导致模型常在小规模数据上训练，难以泛化到真实世界。大型语言模型涵盖了这些领域多年的丰富知识，有潜力辅助模型提升泛化能力。

Method: 提出了LoID（Logit-Informed Distributions），一种确定性方法，通过直接访问大型语言模型的token级预测，探测模型在正负语义方向上的置信度，构造信息性先验分布用于贝叶斯逻辑回归。

Result: 在10个真实表格数据集和合成OOD环境下，LoID相比标准无信息先验、AutoElicit、LLMProcesses方法显著提升性能，最多弥补了相比全数据训练的性能差距59%，在8个数据集上优于竞品。

Conclusion: LoID实现了高效、可重复的将大型语言模型知识整合进贝叶斯推断的机制，有效提升了小样本和分布偏移条件下的逻辑回归性能。

Abstract: In domains like medicine and finance, large-scale labeled data is costly and often unavailable, leading to models trained on small datasets that struggle to generalize to real-world populations. Large language models contain extensive knowledge from years of research across these domains. We propose LoID (Logit-Informed Distributions), a deterministic method for extracting informative prior distributions for Bayesian logistic regression by directly accessing their token-level predictions. Rather than relying on generated text, we probe the model's confidence in opposing semantic directions (positive vs. negative impact) through carefully constructed sentences. By measuring how consistently the LLM favors one direction across diverse phrasings, we extract the strength and reliability of the model's belief about each feature's influence. We evaluate LoID on ten real-world tabular datasets under synthetic out-of-distribution (OOD) settings characterized by covariate shift, where the training data represents only a subset of the population. We compare our approach against (1) standard uninformative priors, (2) AutoElicit, a recent method that prompts LLMs to generate priors via text completions, (3) LLMProcesses, a method that uses LLMs to generate numerical predictions through in-context learning and (4) an oracle-style upper bound derived from fitting logistic regression on the full dataset. We assess performance using Area Under the Curve (AUC). Across datasets, LoID significantly improves performance over logistic regression trained on OOD data, recovering up to \textbf{59\%} of the performance gap relative to the oracle model. LoID outperforms AutoElicit and LLMProcessesc on 8 out of 10 datasets, while providing a reproducible and computationally efficient mechanism for integrating LLM knowledge into Bayesian inference.

</details>


### [37] [Beyond the Rabbit Hole: Mapping the Relational Harms of QAnon Radicalization](https://arxiv.org/abs/2601.17658)
*Bich Ngoc,Doan,Giuseppe Russo,Gianmarco De Francisci Morales,Robert West*

Main category: cs.CL

TL;DR: 本文通过分析QAnon支持社区的12747个叙述，系统描绘激进化路径并量化对亲属造成的情感伤害，发现激进化人格模型可预测叙述者的具体情感反应。


<details>
  <summary>Details</summary>
Motivation: 阴谋论激进化对社会和受害者家庭带来严重影响，但个人层面的情感伤害尚少研究，本文旨在填补这一空白。

Method: 采用BERTopic进行主题建模描绘激进过程，基于LDA图模型识别6种激进化人格，结合LLM辅助情感检测和回归分析情感伤害。

Result: 识别出6种激进化人格，与具体的情感伤害（愤怒、厌恶、恐惧、悲伤）高度相关，显示人格模型能预测亲属的情感损害。

Conclusion: 激进化不仅是意识形态选择，还与个人和认知崩溃相关，不同激进化人格对应不同的情感伤害，为理解激进化的关系性提供了实证框架。

Abstract: The rise of conspiracy theories has created far-reaching societal harm in the public discourse by eroding trust and fueling polarization. Beyond this public impact lies a deeply personal toll on the friends and families of conspiracy believers, a dimension often overlooked in large-scale computational research. This study fills this gap by systematically mapping radicalization journeys and quantifying the associated emotional toll inflicted on loved ones. We use the prominent case of QAnon as a case study, analyzing 12747 narratives from the r/QAnonCasualties support community through a novel mixed-methods approach. First, we use topic modeling (BERTopic) to map the radicalization trajectories, identifying key pre-existing conditions, triggers, and post-radicalization characteristics. From this, we apply an LDA-based graphical model to uncover six recurring archetypes of QAnon adherents, which we term "radicalization personas." Finally, using LLM-assisted emotion detection and regression modeling, we link these personas to the specific emotional toll reported by narrators. Our findings reveal that these personas are not just descriptive; they are powerful predictors of the specific emotional harms experienced by narrators. Radicalization perceived as a deliberate ideological choice is associated with narrator anger and disgust, while those marked by personal and cognitive collapse are linked to fear and sadness. This work provides the first empirical framework for understanding radicalization as a relational phenomenon, offering a vital roadmap for researchers and practitioners to navigate its interpersonal fallout.

</details>


### [38] [UrduLM: A Resource-Efficient Monolingual Urdu Language Model](https://arxiv.org/abs/2601.17664)
*Syed Muhammad Ali,Hammad Sajid,Zainab Haider,Ali Muhammad Asad,Haya Fatima,Abdul Samad*

Main category: cs.CL

TL;DR: 针对乌尔都语资源稀缺问题，构建了33GB语料库和定制分词器，训练了1亿参数单语transformer模型UrduLM，达成与超大型多语言模型相当的性能，成果公开，推动乌尔都语及其他小语种NLP发展。


<details>
  <summary>Details</summary>
Motivation: 乌尔都语虽有2.3亿使用者，但缺乏专门的transformer语言模型和精选语料库，现有多语言模型支持有限且性能差、计算成本高、文化不准确。

Method: 我们收集了33GB多样化的乌尔都语语料，开发了自定义BPE分词器，减少20-30%的分词负担，预训练了1亿参数的解码器单语模型UrduLM。

Result: UrduLM在少样本评估中表现优异，准确率达66.6%（情感分类），BLEU分数超过30（语法纠正），性能媲美甚至超越体量大30倍的多语言模型。

Conclusion: UrduLM为乌尔都语NLP研究提供了高效且开放的单语预训练模型和语料，建立了研究基线，并提供了对其他资源匮乏语种的可扩展方案。

Abstract: Urdu, spoken by 230 million people worldwide, lacks dedicated transformer-based language models and curated corpora. While multilingual models provide limited Urdu support, they suffer from poor performance, high computational costs, and cultural inaccuracies due to insufficient training data. To address these challenges, we present UrduLM, a pretrained Urdu monolingual language model trained in low-resource settings. We curate a 33GB Urdu corpus from diverse sources, develop a custom BPE tokenizer that reduces tokenization overhead by atleast 20-30% compared to multilingual alternatives, and pretrain a 100M-parameter decoder-only model. In few-shot evaluations, UrduLM achieves competitive performance with multilingual models up to 30x its size, reaching 66.6% accuracy on sentiment classification and BLEU scores exceeding 30 on grammar correction tasks. The complete methodology -- including corpus, tokenizer, model weights, and evaluation benchmarks -- is released openly to establish a baseline for Urdu NLP research and provide a scalable framework for other underrepresented languages.

</details>


### [39] [Align to the Pivot: Dual Alignment with Self-Feedback for Multilingual Math Reasoning](https://arxiv.org/abs/2601.17671)
*Chunxu Zhao,Xin Huang,Xue Han,Shujian Huang,Chao Deng,Junlan Feng*

Main category: cs.CL

TL;DR: 该论文针对大型语言模型在多语言环境下，尤其是低资源语言中的表现下降问题，提出了一种名为PASMR的方法，通过设定主语言为枢纽语言，利用枢纽语言的推理答案监督目标语言推理，实现跨语言自我反馈机制，以提升模型的多语言理解和推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多语言推理任务中表现下降，尤其是低资源语言，主要原因是模型在多语言理解和推理对齐上的不一致性。

Method: 将模型的主语言设为枢纽语言，先将问题翻译成枢纽语言进行推理，再用枢纽语言的推理结果监督目标语言的推理过程，形成无需外部正确答案或奖励模型的跨语言自我反馈机制。

Result: 大量实验验证了该方法提升了模型的问题理解和推理能力，带来任务性能的显著提升。

Conclusion: PASMR方法成功改善了多语言数学推理任务中模型的对齐问题，显著提升了模型在理解和推理方面的表现。

Abstract: Despite the impressive reasoning abilities demonstrated by large language models (LLMs), empirical evidence indicates that they are not language agnostic as expected, leading to performance declines in multilingual settings, especially for low-resource languages. We attribute the decline to the model's inconsistent multilingual understanding and reasoning alignment. To address this, we present Pivot-Aligned Self-Feedback Multilingual Reasoning (PASMR), aiming to improve the alignment of multilingual math reasoning abilities in LLMs. This approach designates the model's primary language as the pivot language. During training, the model first translates questions into the pivot language to facilitate better alignment of reasoning patterns. The reasoning process in the target language is then supervised by the pivot language's reasoning answers, thereby establishing a cross-lingual self-feedback mechanism without relying on external correct answers or reward models. Extensive experimental results demonstrate that our method enhances both the model's understanding of questions and its reasoning capabilities, leading to notable task improvements.

</details>


### [40] [S$^3$-Attention:Attention-Aligned Endogenous Retrieval for Memory-Bounded Long-Context Inference](https://arxiv.org/abs/2601.17702)
*Qingsen Ma,Dianyun Wang,Yaoye Wang,Lechen Ning,Sujie Zhu,Xiaohang Zhang,Jiaming Lyu,Linhao Ren,Zhenbo Xu,Zhaofeng He*

Main category: cs.CL

TL;DR: 本文提出S3-Attention，通过稀疏特征和倒排索引高效处理长上下文推理，显著降低GPU内存需求并提升鲁棒性，虽延迟仍需优化。


<details>
  <summary>Details</summary>
Motivation: 长文本推理内存和噪音效率低，现有KV缓存线性扩展且检索相关性差。

Method: 提出S3-Attention框架，通过稀疏自动编码器生成稀疏特征并构建CPU倒排索引，替代KV缓存限制GPU内存。

Result: S3-Attention在多模型和信息密集场景下，与全上下文推理表现相近，鲁棒性提升，但当前实现延迟较高。

Conclusion: S3-Attention有效提高了长上下文推理的内存利用率和鲁棒性，为后续优化提供方向。

Abstract: Large language models are increasingly applied to multi-document and long-form inputs, yet long-context inference remains memory- and noise-inefficient. Key-value (KV) caching scales linearly with context length, while external retrieval methods often return lexically similar but causally irrelevant passages.
  We present S3-Attention, a memory-first inference-time framework that treats long-context processing as attention-aligned endogenous retrieval. S3-Attention decodes transient key and query projections into top-k sparse feature identifiers using lightweight sparse autoencoders, and constructs a CPU-based inverted index mapping features to token positions or spans during a single streaming scan. This design allows the KV cache to be discarded entirely and bounds GPU memory usage by the scan chunk size.
  At generation time, feature co-activation is used to retrieve compact evidence spans, optionally fused with BM25 for exact lexical matching. Under a unified LongBench evaluation protocol with fixed prompting, decoding, and matched token budgets, S3-Hybrid closely matches full-context inference across multiple model families and improves robustness in several information-dense settings. We also report an engineering limitation of the current prototype, which incurs higher wall-clock latency than optimized full-KV baselines, motivating future kernel-level optimization.

</details>


### [41] [Distance-to-Distance Ratio: A Similarity Measure for Sentences Based on Rate of Change in LLM Embeddings](https://arxiv.org/abs/2601.17705)
*Abdullah Qureshi,Kenneth Rice,Alexander Wolpert*

Main category: cs.CL

TL;DR: DDR是一种基于语义上下文影响的新型文本嵌入相似度度量，能更精准地区分语义相似和不相似文本。


<details>
  <summary>Details</summary>
Motivation: 现有文本嵌入相似度度量未能很好地符合人类对文本语义相似性的感知，尤其在细微语义变化的区分上效果有限。

Method: 引入了DDR（distance-to-distance ratio）作为一种新的相似度度量，通过测量上下文单词嵌入的相似度变化率，结合大语言模型句子嵌入的相似度变化，体现上下文的语义影响。

Result: 通过对句子进行一至三个词的同义词替换及随机词替换的实验，DDR在区分语义相似和非相似文本方面优于其他主流相似度指标，表现出更精细的识别能力。

Conclusion: DDR有效提升了文本嵌入相似度的语义区分能力，尤其在微小语义变化的情况下表现出更好的灵敏度和准确度，适合作为大语言模型句子嵌入相似度的评估指标。

Abstract: A measure of similarity between text embeddings can be considered adequate only if it adheres to the human perception of similarity between texts. In this paper, we introduce the distance-to-distance ratio (DDR), a novel measure of similarity between LLM sentence embeddings. Inspired by Lipschitz continuity, DDR measures the rate of change in similarity between the pre-context word embeddings and the similarity between post-context LLM embeddings, thus measuring the semantic influence of context. We evaluate the performance of DDR in experiments designed as a series of perturbations applied to sentences drawn from a sentence dataset. For each sentence, we generate variants by replacing one, two, or three words with either synonyms, which constitute semantically similar text, or randomly chosen words, which constitute semantically dissimilar text. We compare the performance of DDR with other prevailing similarity metrics and demonstrate that DDR consistently provides finer discrimination between semantically similar and dissimilar texts, even under minimal, controlled edits.

</details>


### [42] [A Computational Approach to Visual Metonymy](https://arxiv.org/abs/2601.17706)
*Saptarshi Ghosh,Linfeng Liu,Tianyu Jiang*

Main category: cs.CL

TL;DR: 本文首次对视觉转喻（视觉中的间接指称现象）进行了计算机研究，提出基于符号学理论的新框架，构建了包含2000个多项选择题的视觉转喻评测数据集ViMET。实验结果显示现有视觉语言模型在理解视觉转喻上的表现明显不及人类。


<details>
  <summary>Details</summary>
Motivation: 图像常通过间接线索表达含义，这种视觉转喻使得图像传达丰富的文化和职业信息，现有模型对此类间接视觉指代理解不足，亟需新的研究框架和评测工具。

Method: 基于符号学理论，结合大规模语言模型和文本生成图像模型，设计生成视觉转喻表征的流水线，并据此构建ViMET数据集，用以评测多模态语言模型的视觉转喻理解能力。

Result: 构建了首个视觉转喻数据集ViMET，包含2000道多项选择题，实验表明人类准确率为86.9%，而最先进的视觉语言模型准确率仅为65.9%，展示了机器在视觉转喻认知推理上的显著差距。

Conclusion: 现有视觉语言模型在理解视觉转喻方面表现有限，尚无法达到人类的认知水平。

Abstract: Images often communicate more than they literally depict: a set of tools can suggest an occupation and a cultural artifact can suggest a tradition. This kind of indirect visual reference, known as visual metonymy, invites viewers to recover a target concept via associated cues rather than explicit depiction. In this work, we present the first computational investigation of visual metonymy. We introduce a novel pipeline grounded in semiotic theory that leverages large language models and text-to-image models to generate metonymic visual representations. Using this framework, we construct ViMET, the first visual metonymy dataset comprising 2,000 multiple-choice questions to evaluate the cognitive reasoning abilities in multimodal language models. Experimental results on our dataset reveal a significant gap between human performance (86.9%) and state-of-the-art vision-language models (65.9%), highlighting limitations in machines' ability to interpret indirect visual references. Our dataset is publicly available at: https://github.com/cincynlp/ViMET.

</details>


### [43] [Unsupervised Elicitation of Moral Values from Language Models](https://arxiv.org/abs/2601.17728)
*Meysam Alizadeh,Fabrizio Gilardi,Zeynab Samei*

Main category: cs.CL

TL;DR: 该论文证明了预训练语言模型具有内在的道德推理能力，可通过无监督的内部一致性最大化算法有效激发，提升道德判断准确性并显著减少偏见，为AI系统与人类价值观对齐提供了可扩展路径。


<details>
  <summary>Details</summary>
Motivation: AI系统日益普及，必须使其行为符合人类价值观。已有研究表明语言模型的内在道德推理能力有限，且构建道德评估的真实标签数据困难，因而探索无监督方法以激发语言模型内在道德能力成为必要。

Method: 采用无监督的内部一致性最大化（ICM）算法，在三个基准数据集和四个不同的语言模型上测试道德判断的标注准确性、跨道德框架的泛化能力及社会偏见的缓解效果。

Result: ICM在Norm Bank和ETHICS基准测试上超越所有预训练和聊天机器人基线；基于ICM标签微调的模型表现与人类标签相当甚至更优，尤其在正义和常识道德框架中提升显著；ICM能够将社交偏见错误率降低一半以上，在种族、社会经济状态和政治方面改善最大。

Conclusion: 预训练语言模型（LMs）具备潜在的道德推理能力，通过无监督的内部一致性最大化（ICM）算法可以有效揭示这一能力，同时显著减少社会偏见。

Abstract: As AI systems become pervasive, grounding their behavior in human values is critical. Prior work suggests that language models (LMs) exhibit limited inherent moral reasoning, leading to calls for explicit moral teaching. However, constructing ground truth data for moral evaluation is difficult given plural frameworks and pervasive biases. We investigate unsupervised elicitation as an alternative, asking whether pretrained (base) LMs possess intrinsic moral reasoning capability that can be surfaced without human supervision. Using the Internal Coherence Maximization (ICM) algorithm across three benchmark datasets and four LMs, we test whether ICM can reliably label moral judgments, generalize across moral frameworks, and mitigate social bias. Results show that ICM outperforms all pre-trained and chatbot baselines on the Norm Bank and ETHICS benchmarks, while fine-tuning on ICM labels performs on par with or surpasses those of human labels. Across theoretically motivated moral frameworks, ICM yields its largest relative gains on Justice and Commonsense morality. Furthermore, although chatbot LMs exhibit social bias failure rates comparable to their pretrained ones, ICM reduces such errors by more than half, with the largest improvements in race, socioeconomic status, and politics. These findings suggest that pretrained LMs possess latent moral reasoning capacities that can be elicited through unsupervised methods like ICM, providing a scalable path for AI alignment.

</details>


### [44] [Hylog: A Hybrid Approach to Logging Text Production in Non-alphabetic Scripts](https://arxiv.org/abs/2601.17753)
*Roberto Crotti,Giovanni Denaro,Zhiqiang Du,Ricardo Muñoz Martín*

Main category: cs.CL

TL;DR: Hylog系统结合键盘和屏幕文本记录，解决了IME输入中关键细节捕捉难题，推动了多语言文本认知研究。


<details>
  <summary>Details</summary>
Motivation: 现有的研究性键盘记录工具难以捕捉非字母文字输入法（IME）在屏幕上的文本转化过程，限制了对文本产生认知过程的深入研究。

Method: 提出了一种名为Hylog的新型混合日志系统，结合分析性键盘记录和生态学文本记录，通过插件捕获键盘输出与渲染文本，并通过混合模块同步生成双重记录轨迹。

Result: Hylog成功捕获了拉丁字母、中文字符和IME确认键之间的按键及时间间隔，揭示了传统键盘记录工具无法检测的细节，实现了对IME输入过程多层次语言认知的分析。

Conclusion: Hylog为多语言文本生成认知研究提供了更完整和细粒度的数据收集方法，其模块化插件架构支持扩展至其他IME系统，促进了多语种文本生产研究的包容性发展。

Abstract: Research keyloggers are essential for cognitive studies of text production, yet most fail to capture the on-screen transformations performed by Input Method Editors (IMEs) for non-alphabetic scripts. To address this methodological gap, we present Hylog, a novel hybrid logging system that combines analytical keylogging with ecological text logging for a more complete and finer-grained analysis. Our modular, open-source system uses plug-ins for standard applications (Microsoft Word, Google Chrome) to capture both keyboard output and rendered text, which a hybridizer module then synchronizes into a dual trace. To validate the system's technical feasibility and demonstrate its analytical capabilities, we conducted a proof-of-concept study where two volunteers translated a text into simplified Chinese. Hylog successfully captured keypresses and temporal intervals between Latin letters, Chinese characters, and IME confirmations -- some measurements invisible to traditional keyloggers. The resulting data enable the formulation of new, testable hypotheses about the cognitive restrictions and affordances at different linguistic layers in IME-mediated typing. Our plug-in architecture enables extension to other IME systems and fosters more inclusive multilingual text-production research.

</details>


### [45] [ProGraph-R1: Progress-aware Reinforcement Learning for Graph Retrieval Augmented Generation](https://arxiv.org/abs/2601.17755)
*Jinyoung Park,Sanghyeok Lee,Omar Zia Khan,Hyunwoo J. Kim,Joo-Kyung Kim*

Main category: cs.CL

TL;DR: ProGraph-R1通过结构感知检索和进度驱动的强化学习策略，改进了GraphRAG的多步推理能力，并在多跳问答中表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有的基于强化学习的GraphRAG框架主要依赖语义相似性检索，忽略图结构，同时奖励机制稀疏，无法有效反映中间检索步骤质量。

Method: 提出ProGraph-R1框架，引入结构感知的超图检索机制，结合语义相关性和图连接性，促进多跳推理路径的连贯遍历；设计基于进度的逐步策略优化，利用中间推理进展提供密集学习信号。

Result: ProGraph-R1在多跳问答基准测试中，推理准确率和生成质量均优于现有GraphRAG方法。

Conclusion: 结合结构感知的检索机制和基于进度的奖励策略，ProGraph-R1有效提升了基于图的多步推理性能和结果质量。

Abstract: Graph Retrieval-Augmented Generation (GraphRAG) has been successfully applied in various knowledge-intensive question answering tasks by organizing external knowledge into structured graphs of entities and relations. It enables large language models (LLMs) to perform complex reasoning beyond text-chunk retrieval. Recent works have employed reinforcement learning (RL) to train agentic GraphRAG frameworks that perform iterative interactions between LLMs and knowledge graphs. However, existing RL-based frameworks such as Graph-R1 suffer from two key limitations: (1) they primarily depend on semantic similarity for retrieval, often overlooking the underlying graph structure, and (2) they rely on sparse, outcome-level rewards, failing to capture the quality of intermediate retrieval steps and their dependencies. To address these limitations, we propose ProGraph-R1, a progress-aware agentic framework for graph-based retrieval and multi-step reasoning. ProGraph-R1 introduces a structure-aware hypergraph retrieval mechanism that jointly considers semantic relevance and graph connectivity, encouraging coherent traversal along multi-hop reasoning paths. We also design a progress-based step-wise policy optimization, which provides dense learning signals by modulating advantages according to intermediate reasoning progress within a graph, rather than relying solely on final outcomes. Experiments on multi-hop question answering benchmarks demonstrate that ProGraph-R1 consistently improves reasoning accuracy and generation quality over existing GraphRAG methods.

</details>


### [46] [Cross-Lingual Probing and Community-Grounded Analysis of Gender Bias in Low-Resource Bengali](https://arxiv.org/abs/2601.17764)
*Md Asgor Hossain Reaj,Rajan Das Gupta,Jui Saha Pritha,Abdullah Al Noman,Abir Ahmed,Golam Md Mohiuddin,Tze Hui Liew*

Main category: cs.CL

TL;DR: 本研究揭示了孟加拉语中性别偏见的独特特征，英语中心方法难以适用，强调结合社区调研和本地化方法以更好识别和缓解偏见。


<details>
  <summary>Details</summary>
Motivation: 当前研究多聚焦英语语言的性别偏见，忽视了像孟加拉语这样全球南方语言在语言和文化层面的偏见表现，亟需深入分析和有效缓解，以推动自然语言处理系统的公平与包容。

Method: 通过词汇表挖掘、计算分类模型、基于翻译的比较分析和GPT生成偏见语句等多种技术手段提取性别偏见表达，并在农村和低收入地区进行了实地调研以收集真实偏见数据。

Result: 本研究针对大语言模型在孟加拉语中存在的性别偏见问题进行了系统性分析，发现英语中心的方法难以有效检测和缓解孟加拉语中的性别偏见。通过词汇挖掘、计算分类、翻译对比和GPT生成偏见等多种方法，并结合农村及低收入社区的实地调查，揭示了孟加拉语性别偏见的独特性及社会文化影响，强调了本地化、情境敏感的研究方法和社区驱动的研究模式的重要性。

Conclusion: 孟加拉语中的性别偏见具有独特的语言和文化特征，现有英语为主的方法难以直接移植，亟需开发针对少数语言的本地化偏见检测和缓解工具，结合社区驱动研究以提升公正包容的自然语言处理系统。

Abstract: Large Language Models (LLMs) have achieved significant success in recent years; yet, issues of intrinsic gender bias persist, especially in non-English languages. Although current research mostly emphasizes English, the linguistic and cultural biases inherent in Global South languages, like Bengali, are little examined. This research seeks to examine the characteristics and magnitude of gender bias in Bengali, evaluating the efficacy of current approaches in identifying and alleviating bias. We use several methods to extract gender-biased utterances, including lexicon-based mining, computational classification models, translation-based comparison analysis, and GPT-based bias creation. Our research indicates that the straight application of English-centric bias detection frameworks to Bengali is severely constrained by language disparities and socio-cultural factors that impact implicit biases. To tackle these difficulties, we executed two field investigations inside rural and low-income areas, gathering authentic insights on gender bias. The findings demonstrate that gender bias in Bengali presents distinct characteristics relative to English, requiring a more localized and context-sensitive methodology. Additionally, our research emphasizes the need of integrating community-driven research approaches to identify culturally relevant biases often neglected by automated systems. Our research enhances the ongoing discussion around gender bias in AI by illustrating the need to create linguistic tools specifically designed for underrepresented languages. This study establishes a foundation for further investigations into bias reduction in Bengali and other Indic languages, promoting the development of more inclusive and fair NLP systems.

</details>


### [47] [DPI: Exploiting Parameter Heterogeneity for Interference-Free Fine-Tuning](https://arxiv.org/abs/2601.17777)
*Xiaoyu Liu,Xiaoyu Guan,Di Liang,Xianjie Wu*

Main category: cs.CL

TL;DR: 通过识别并冻结各任务的核心参数，实现动态参数隔离，解决多任务微调中的任务干扰，提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 不同的有监督微调任务之间存在目标冲突，导致模型在优化某一任务时，可能会损害其他任务的性能，即“跷跷板效应”。

Method: 先对大语言模型在不同任务上独立微调，识别每个任务的核心参数区域（参数更新最大的子集），将具有高度重叠核心参数的任务合并共同训练，不同任务则分阶段训练。多阶段训练时冻结前期任务的核心参数，避免后续任务覆写。

Result: 该方法通过动态参数隔离策略，有效减少了任务间的数据冲突，在多个公开数据集上的实验中，表现出比多阶段和多任务调优基线更稳定和一致的性能提升。

Conclusion: 基于参数异质性的动态参数隔离方法能有效缓解跨任务干扰，提升大语言模型在多任务微调中的表现。

Abstract: Supervised fine-tuning (SFT) is a crucial step for adapting large language models (LLMs) to downstream tasks. However, conflicting objectives across heterogeneous SFT tasks often induce the "seesaw effect": optimizing for one task may degrade performance on others, particularly when model parameters are updated indiscriminately. In this paper, we propose a principled approach to disentangle and isolate task-specific parameter regions, motivated by the hypothesis that parameter heterogeneity underlies cross-task interference. Specifically, we first independently fine-tune LLMs on diverse SFT tasks and identify each task's core parameter region as the subset of parameters exhibiting the largest updates. Tasks with highly overlapping core parameter regions are merged for joint training, while disjoint tasks are organized into different stages. During multi-stage SFT, core parameters acquired in prior tasks are frozen, thereby preventing overwriting by subsequent tasks. To verify the effectiveness of our method, we conducted intensive experiments on multiple public datasets. The results showed that our dynamic parameter isolation strategy consistently reduced data conflicts and achieved consistent performance improvements compared to multi-stage and multi-task tuning baselines.

</details>


### [48] [Controlling Reading Ease with Gaze-Guided Text Generation](https://arxiv.org/abs/2601.17781)
*Andreas Säuberli,Darja Jepifanova,Diego Frassinelli,Barbara Plank*

Main category: cs.CL

TL;DR: 本文提出了基于眼动预测模型控制文本阅读难度的方法，实验证明有效，并有实际应用潜力。


<details>
  <summary>Details</summary>
Motivation: 通过眼动模式反映认知努力，生成可控阅读难度的文本。

Method: 利用预测人类注视模式的模型，引导语言模型输出，控制文本的阅读行为。

Result: 通过眼动实验验证该方法能有效调节文本的易读性，影响阅读时间和感知难度。分析显示主要因词汇处理影响。

Conclusion: 该方法可应用于信息无障碍的文本简化及个性化语言学习教育材料生成。

Abstract: The way our eyes move while reading can tell us about the cognitive effort required to process the text. In the present study, we use this fact to generate texts with controllable reading ease. Our method employs a model that predicts human gaze patterns to steer language model outputs towards eliciting certain reading behaviors. We evaluate the approach in an eye-tracking experiment with native and non-native speakers of English. The results demonstrate that the method is effective at making the generated texts easier or harder to read, measured both in terms of reading times and perceived difficulty of the texts. A statistical analysis reveals that the changes in reading behavior are mostly due to features that affect lexical processing. Possible applications of our approach include text simplification for information accessibility and generation of personalized educational material for language learning.

</details>


### [49] [Beyond a Single Perspective: Text Anomaly Detection with Multi-View Language Representations](https://arxiv.org/abs/2601.17786)
*Yixin Liu,Kehan Yan,Shiyuan Li,Qingfeng Chen,Shirui Pan*

Main category: cs.CL

TL;DR: 提出多视角文本异常检测框架$MCA^2$，通过融合多模型嵌入和自适应权重提升检测性能和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有单一嵌入模型的文本异常检测方法受限于模型多样性和跨数据集适应性，亟需结合多模型嵌入提升检测效果和泛化能力。

Method: 引入多预训练语言模型嵌入，多视角重构模型提取文本正常模式，设计对比协作模块增强视角交互，自适应分配权重提升适应性。

Result: 本文提出了多视角文本异常检测框架$MCA^2$，通过融合多种预训练语言模型嵌入，利用多视角重构模型提取文本正常模式，并设计对比协作模块加强视角间交互，同时自适应分配各视角权重，提升了方法在多样数据集和异常类型上的适应性。实验在10个基准数据集上验证了$MCA^2$的有效性，代码已开源。

Conclusion: $MCA^2$显著提升了文本异常检测在多个数据集和异常类型上的效果，具有良好的适应性和鲁棒性。

Abstract: Text anomaly detection (TAD) plays a critical role in various language-driven real-world applications, including harmful content moderation, phishing detection, and spam review filtering. While two-step "embedding-detector" TAD methods have shown state-of-the-art performance, their effectiveness is often limited by the use of a single embedding model and the lack of adaptability across diverse datasets and anomaly types. To address these limitations, we propose to exploit the embeddings from multiple pretrained language models and integrate them into $MCA^2$, a multi-view TAD framework. $MCA^2$ adopts a multi-view reconstruction model to effectively extract normal textual patterns from multiple embedding perspectives. To exploit inter-view complementarity, a contrastive collaboration module is designed to leverage and strengthen the interactions across different views. Moreover, an adaptive allocation module is developed to automatically assign the contribution weight of each view, thereby improving the adaptability to diverse datasets. Extensive experiments on 10 benchmark datasets verify the effectiveness of $MCA^2$ against strong baselines. The source code of $MCA^2$ is available at https://github.com/yankehan/MCA2.

</details>


### [50] [DIETA: A Decoder-only transformer-based model for Italian-English machine TrAnslation](https://arxiv.org/abs/2601.17823)
*Pranav Kasela,Marco Braga,Alessandro Ghiotto,Andrea Pilzer,Marco Viviani,Alessandro Raganato*

Main category: cs.CL

TL;DR: 提出了专为意大利语-英语翻译设计的5亿参数Transformer模型DIETA，结合大规模多领域语料与反向翻译数据训练，在多个测试中表现优异并公开相关资源。


<details>
  <summary>Details</summary>
Motivation: 针对意大利语-英语机器翻译设计一个小型且高效的解码器架构模型，弥补现有大型模型资源消耗大、门槛高的问题。

Method: 建立一个约2.07亿意大利语-英语平行句对的大型多领域语料库，结合3.52亿反向翻译数据，训练了一个5亿参数的解码器Transformer模型DIETA。此外，制作并发布了450句小规模评测集。

Result: DIETA在多个意英翻译基准测试中表现出竞争力，在32系统排行榜中稳定占据第二四分位，且在五个测试中有四个优于大多数低于3B参数的模型。

Conclusion: DIETA模型及训练脚本、数据集和评测集公开发布，推动意大利语-英语机器翻译的研究和应用发展。

Abstract: In this paper, we present DIETA, a small, decoder-only Transformer model with 0.5 billion parameters, specifically designed and trained for Italian-English machine translation. We collect and curate a large parallel corpus consisting of approximately 207 million Italian-English sentence pairs across diverse domains, including parliamentary proceedings, legal texts, web-crawled content, subtitles, news, literature and 352 million back-translated data using pretrained models. Additionally, we create and release a new small-scale evaluation set, consisting of 450 sentences, based on 2025 WikiNews articles, enabling assessment of translation quality on contemporary text. Comprehensive evaluations show that DIETA achieves competitive performance on multiple Italian-English benchmarks, consistently ranking in the second quartile of a 32-system leaderboard and outperforming most other sub-3B models on four out of five test suites. The training script, trained models, curated corpus, and newly introduced evaluation set are made publicly available, facilitating further research and development in specialized Italian-English machine translation. https://github.com/pkasela/DIETA-Machine-Translation

</details>


### [51] [Linguistic and Argument Diversity in Synthetic Data for Function-Calling Agents](https://arxiv.org/abs/2601.17829)
*Dan Greenstein,Zohar Karnin,Chen Amiraz,Oren Somekh*

Main category: cs.CL

TL;DR: 该论文提出了一种自动优化语言及参数多样性的合成数据生成方法，有效提升函数调用模型的多样性及准确性，提升跨域性能。


<details>
  <summary>Details</summary>
Motivation: 函数调用代理构建面临高质量多样化训练数据获取的挑战，现有方法忽视了请求语言多样性及参数覆盖。

Method: 提出一种通过优化查询和参数的通用多样性指标合成数据集的方法，无需手工规则或分类体系，适用性强。

Result: 新方法在多样性上优于现有最先进方法，同时保持正确性，并在训练模型的泛化表现上提升7.4%的准确率。

Conclusion: 通过优化多样性指标自动生成训练数据，可显著提升函数调用代理模型的表现，尤其在跨域性能上表现优越。

Abstract: The construction of function calling agents has emerged as a promising avenue for extending model capabilities. A major challenge for this task is obtaining high quality diverse data for training. Prior work emphasizes diversity in functions, invocation patterns, and interaction turns, yet linguistic diversity of requests and coverage of arguments (e.g., \texttt{city\_name}, \texttt{stock\_ticker}) remain underexplored. We propose a method that generates synthetic datasets via optimizing general-purpose diversity metrics across both queries and arguments, without relying on hand-crafted rules or taxonomies, making it robust to different usecases. We demonstrate the effectiveness of our technique via both intrinsic and extrinsic testing, comparing it to SoTA data generation methods. We show a superiority over baselines in terms of diversity, while keeping comparable correctness. Additionally, when used as a training set, the model resulting from our dataset exhibits superior performance compared to analogous models based on the baseline data generation methods in out-of-distribution performance. In particular, we achieve an $7.4\%$ increase in accuracy on the BFCL benchmark compared to similar counterparts.

</details>


### [52] [EFT-CoT: A Multi-Agent Chain-of-Thought Framework for Emotion-Focused Therapy](https://arxiv.org/abs/2601.17842)
*Lanqing Du,Yunong Li,YuJie Long,Shihong Chen*

Main category: cs.CL

TL;DR: 本文提出基于情绪聚焦疗法的多智能体思维链方法，通过细致的阶段性推理和多智能体协作显著提升心理健康问答的同理心和专业性表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于认知行为疗法（CBT）的心理健康问答方法多偏重‘自上而下’的理性重构，忽视了客户的情绪体验和情感处理，存在干预机制单一的问题。

Method: 提出了一种基于情绪聚焦疗法（EFT）的多智能体思维链框架（EFT-CoT），采用‘自下而上’的三阶段推理流程（身体感知-认知探索-叙事干预），利用八个专业智能体执行关键环节，实现情绪体验和认知调整的结合。构建了约6.7万条真实文本的高质量数据集EFT-Instruct，并微调专用模型EFT-LLM。

Result: 通过实验评估，EFT-LLM在同理心深度和结构专业度等指标上优于强基线模型和人工回复，消融实验验证了多智能体机制的必要性，表明该模型具备更优秀的心理推理能力。

Conclusion: EFT-CoT模型成功实现了更加可解释且具备高同理心的心理咨询系统，为心理干预提供了有效的技术路径，体现了自下而上的情绪认知结合策略的优势。

Abstract: Leveraging Large Language Models (LLMs) for Mental Health Question Answering (MHQA) is promising for mitigating resource shortages. However, existing Cognitive Behavioral Therapy (CBT)-based approaches predominantly favor a "top-down" rational restructuring, often neglecting clients' embodied experiences and primary emotion processing. To address this, we propose an Emotion-Focused Therapy (EFT)-based Multi-Agent Chain-of-Thought framework (EFT-CoT). Adopting a "bottom-up" trajectory, it deconstructs the intervention into a three-stage reasoning flow: "Embodied Perception - Cognitive Exploration - Narrative Intervention." Utilizing eight specialized agents, the system explicitly executes critical components such as somatic awareness mapping, adaptive assessment, core belief extraction, and narrative restructuring. We further constructed "EFT-Instruct," a high-quality dataset via Chain-of-Thought distillation of approximately 67,000 authentic texts, and fine-tuned a specialized model, EFT-LLM. Experimental evaluations demonstrate that EFT-LLM outperforms strong baselines and human responses across metrics like empathy depth and structural professionalism. Ablation studies confirm the necessity of the multi-agent mechanism. The model exhibits superior psychological reasoning, offering an effective pathway for interpretable, high-empathy counseling systems.

</details>


### [53] [D-Models and E-Models: Diversity-Stability Trade-offs in the Sampling Behavior of Large Language Models](https://arxiv.org/abs/2601.17865)
*Jia Gu,Liang Pang,Huawei Shen,Xueqi Cheng*

Main category: cs.CL

TL;DR: 本文揭示大型语言模型预测概率有两种不同特征，分别影响模型在实际任务中的多样性与稳定性表现，为模型选择和配置提供理论与实践参考。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型的细粒度采样概率是否能忠实反映任务级目标概率，以优化模型对现实任务的适应性和表现。

Method: 通过控制的分布采样模拟，比较了两类模型的预测概率与任务概率的匹配程度，并在代码生成和推荐等任务中进行了评估，分析两类模型的内部机制。

Result: 发现两种模型行为的本质差异及其对下游任务结果的影响，同时揭示多样性与稳定性之间的系统性权衡，为实际应用中的模型选择提供指导。

Conclusion: 论文发现大型语言模型的预测概率存在两种模式：D型模型预测概率波动大且与任务概率对齐差，E型模型预测概率更稳定且与任务概率对齐更好。这两种模型在下游任务表现出多样性和稳定性的权衡。

Abstract: The predictive probability of the next token (P_token) in large language models (LLMs) is inextricably linked to the probability of relevance for the next piece of information, the purchase probability of the next product, and the execution probability of the next action-all of which fall under the scope of the task-level target distribution (P_task). While LLMs are known to generate samples that approximate real-world distributions, whether their fine-grained sampling probabilities faithfully align with task requirements remains an open question. Through controlled distribution-sampling simulations, we uncover a striking dichotomy in LLM behavior, distinguishing two model types: D-models (e.g. Qwen-2.5), whose P_token exhibits large step-to-step variability and poor alignment with P_task; and E-models (e.g. Mistral-Small), whose P_token is more stable and better aligned with P_task. We further evaluate these two model types in downstream tasks such as code generation and recommendation, revealing systematic trade-offs between diversity and stability that shape task outcomes. Finally, we analyze the internal properties of both model families to probe their underlying mechanisms. These findings offer foundational insights into the probabilistic sampling behavior of LLMs and provide practical guidance on when to favor D- versus E-models. For web-scale applications, including recommendation, search, and conversational agents, our results inform model selection and configuration to balance diversity with reliability under real-world uncertainty, providing a better level of interpretation.

</details>


### [54] [On the Emergence and Test-Time Use of Structural Information in Large Language Models](https://arxiv.org/abs/2601.17869)
*Michelle Chao Chen,Moritz Miller,Bernhard Schölkopf,Siyuan Guo*

Main category: cs.CL

TL;DR: 研究语言模型学习结构信息及其在测试时的应用，发现其结构学习与复杂推理相关，但组合生成能力有限。


<details>
  <summary>Details</summary>
Motivation: 语言模型在科学发现和测试时的灵活生成中需要理解抽象结构信息，研究其学习和利用结构信息的方法具有重要意义。

Method: 设计了一个基于语言结构变换的自然语言数据集，通过实验证明结构信息的学习及其与推理任务的相关性。

Result: 该论文研究语言模型如何从观测数据中学习抽象结构信息，并在测试阶段利用这些结构信息。通过设计基于语言结构变换的自然语言数据集，实验证明学习结构信息的能力与复杂推理任务的表现相关，但测试时的组合生成能力仍有限。

Conclusion: 语言模型能够学习一定的结构信息，这促进了复杂推理任务的完成，但在测试时进行灵活的组合生成仍存在限制。

Abstract: Learning structural information from observational data is central to producing new knowledge outside the training corpus. This holds for mechanistic understanding in scientific discovery as well as flexible test-time compositional generation. We thus study how language models learn abstract structures and utilize the learnt structural information at test-time. To ensure a controlled setup, we design a natural language dataset based on linguistic structural transformations. We empirically show that the emergence of learning structural information correlates with complex reasoning tasks, and that the ability to perform test-time compositional generation remains limited.

</details>


### [55] [Self-Manager: Parallel Agent Loop for Long-form Deep Research](https://arxiv.org/abs/2601.17879)
*Yilong Xu,Zhi Zheng,Xiang Long,Yujun Cai,Yiwei Wang*

Main category: cs.CL

TL;DR: 本文提出Self-Manager，通过并行多线程和独立上下文管理解决了传统代理的阻塞和干扰问题，显著提升了复杂任务的处理效率和效果。


<details>
  <summary>Details</summary>
Motivation: 传统代理在处理复杂任务时仍使用单一上下文窗口和顺序执行，导致相互干扰和阻塞行为，限制了系统的可扩展性和适应性。

Method: 引入Self-Manager，一个支持异步并行执行的多线程代理循环。主线程可创建多个拥有独立上下文的子线程，通过线程控制块进行迭代管理，实现更加专注和灵活的并行代理执行。

Result: 在DeepResearch Bench基准测试中，Self-Manager在所有指标上均优于现有的单代理循环基线。通过大量分析实验验证了设计的必要性及在上下文容量、效率和泛化能力上的优势。

Conclusion: Self-Manager有效提升了长篇深度研究任务的并行执行能力，扩大了上下文管理范围，提高了效率和泛化性，展示了其在复杂任务处理中优于传统单线程代理的优势。

Abstract: Long-form deep research requires multi-faceted investigations over extended horizons to get a comprehensive report. When handling such complex tasks, existing agents manage context at the subtask level to overcome linear context accumulation and information loss. However, they still adhere to a single context window and sequential execution paradigm, which results in mutual interference and blocking behavior, restricting scalability and adaptability. To address this issue, this paper introduces Self-Manager, a parallel agent loop that enables asynchronous and concurrent execution. The main thread can create multiple subthreads, each with its own isolated context, and manage them iteratively through Thread Control Blocks, allowing for more focused and flexible parallel agent execution. To assess its effectiveness, we benchmark Self-Manager on DeepResearch Bench, where it consistently outperforms existing single-agent loop baselines across all metrics. Furthermore, we conduct extensive analytical experiments to demonstrate the necessity of Self-Manager's design choices, as well as its advantages in contextual capacity, efficiency, and generalization.

</details>


### [56] [Assessment of Generative Named Entity Recognition in the Era of Large Language Models](https://arxiv.org/abs/2601.17898)
*Qi Zhan,Yile Wang,Hui Huang*

Main category: cs.CL

TL;DR: 研究表明利用开源大语言模型进行生成式命名实体识别，结合有效微调和格式设计，可实现性能优异且保持通用能力的NER方法，是传统NER的有力替代方案。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs在生成式NER与传统NER模型之间的性能差距、输出格式的影响、模型是否依赖记忆以及微调后通用能力的保持情况。

Method: 对开源大语言模型（LLMs）在扁平和嵌套命名实体识别（NER）任务上的系统性评估，结合参数高效微调和结构化输出格式进行实验。

Result: 开源LLMs通过参数高效微调和结构化输出达到与传统编码器模型竞争的性能，且优于闭源模型如GPT-3；LLMs的NER能力来源于指令跟随和生成能力，而非简单记忆；微调后的LLMs保持甚至提升了部分通用任务性能。

Conclusion: 生成式NER结合开源LLMs展现出强大的性能和良好的通用能力保持，具有成为命名实体识别用户友好新范式的潜力。

Abstract: Named entity recognition (NER) is evolving from a sequence labeling task into a generative paradigm with the rise of large language models (LLMs). We conduct a systematic evaluation of open-source LLMs on both flat and nested NER tasks. We investigate several research questions including the performance gap between generative NER and traditional NER models, the impact of output formats, whether LLMs rely on memorization, and the preservation of general capabilities after fine-tuning. Through experiments across eight LLMs of varying scales and four standard NER datasets, we find that: (1) With parameter-efficient fine-tuning and structured formats like inline bracketed or XML, open-source LLMs achieve performance competitive with traditional encoder-based models and surpass closed-source LLMs like GPT-3; (2) The NER capability of LLMs stems from instruction-following and generative power, not mere memorization of entity-label pairs; and (3) Applying NER instruction tuning has minimal impact on general capabilities of LLMs, even improving performance on datasets like DROP due to enhanced entity understanding. These findings demonstrate that generative NER with LLMs is a promising, user-friendly alternative to traditional methods. We release the data and code at https://github.com/szu-tera/LLMs4NER.

</details>


### [57] [ShapLoRA: Allocation of Low-rank Adaption on Large Language Models via Shapley Value Inspired Importance Estimation](https://arxiv.org/abs/2601.17921)
*Yi Zhao,Qinghua Yao,Xinyuan song,Wei Zhu*

Main category: cs.CL

TL;DR: 提出基于Shapley值的可解释秩分配框架ShapLoRA，有效提升参数高效微调LoRA的性能


<details>
  <summary>Details</summary>
Motivation: 传统LoRA以统一秩实现，虽有基于秩分配的提升，但现有秩分配方法依赖难以解释和不可靠的重要性度量，限制性能优化

Method: 提出ShapLoRA框架，结合敏感度度量与合作博弈中的Shapley值，构造可解释的重要性指标Shapley敏感度，并优化评估流程（使用独立验证集计算Shapley敏感度，设置分配-再训练程序）

Result: 在多个挑战性任务上，ShapLoRA方法在可调节参数量相当的情况下，性能优于近期基线方法

Conclusion: ShapLoRA通过引入可解释的重要性测度及完善的评估流程，实现了LoRA秩分配的性能提升，促进了大语言模型的民主化应用

Abstract: Low-rank adaption (LoRA) is a representative method in the field of parameter-efficient fine-tuning (PEFT), and is key to Democratizating the modern large language models (LLMs). The vanilla LoRA is implemented with uniform ranks, and the recent literature have found that properly allocating ranks on the LLM backbones results in performance boosts. However, the previous rank allocation methods have limitations since they rely on inexplanable and unreliable importance measures for the LoRA ranks. To address the above issues, we propose the ShapLoRA framework. Inspired by the explanable attribution measure Shapley Value, we combine the sensitivity-based measures with the idea of coalitions in the collaborative games among LoRA ranks, and propose a more explainable importance measure called Shapley sensitivity. In addition, we optimize the workflow of the existing works by: (a) calculating Shapley sensitivity on a separate validation set; (b) Setting up the allocating-retraining procedures for fair comparisons. We have conducted experiments on various challenging tasks, and the experimental results demonstrate that our ShapLoRA method can outperform the recent baselines with comparable tunable parameters.\footnote{Codes and fine-tuned models will be open-sourced to facilitate future research.

</details>


### [58] [A Monosemantic Attribution Framework for Stable Interpretability in Clinical Neuroscience Large Language Models](https://arxiv.org/abs/2601.17952)
*Michail Mamalakis,Tiago Azevedo,Cristian Cosentino,Chiara D'Ercoli,Subati Abulikemu,Zhongtian Sun,Richard Bethlehem,Pietro Lio*

Main category: cs.CL

TL;DR: 为解决大语言模型在临床解释上的不稳定问题，本文提出了一种结合归因和机械解释的单义语义框架，实现了稳定且可信的特征重要性解释。


<details>
  <summary>Details</summary>
Motivation: 在阿尔茨海默病等认知健康领域中，早期且可信的预测至关重要，但现有解释方法存在不稳定和缺乏明确关联的问题，限制了大语言模型临床应用。

Method: 通过在大语言模型层级构建单义语义嵌入空间，并优化框架以减少方法间的变异性，实现输入级别的稳定重要性评分及关键特征的高亮。

Result: 该方法产生了稳定的输入级重要性分数，并通过对感兴趣层的表示进行解压缩，突出显示关键特征，促进大语言模型在神经退行性疾病中的安全可信应用。

Conclusion: 本文提出的统一解释性框架有效整合了归因性和机械性解释方法，解决了现有方法中解释不稳定和高变异性的问题，为临床中大语言模型的可信应用提供了支持。

Abstract: Interpretability remains a key challenge for deploying large language models (LLMs) in clinical settings such as Alzheimer's disease progression diagnosis, where early and trustworthy predictions are essential. Existing attribution methods exhibit high inter-method variability and unstable explanations due to the polysemantic nature of LLM representations, while mechanistic interpretability approaches lack direct alignment with model inputs and outputs and do not provide explicit importance scores. We introduce a unified interpretability framework that integrates attributional and mechanistic perspectives through monosemantic feature extraction. By constructing a monosemantic embedding space at the level of an LLM layer and optimizing the framework to explicitly reduce inter-method variability, our approach produces stable input-level importance scores and highlights salient features via a decompressed representation of the layer of interest, advancing the safe and trustworthy application of LLMs in cognitive health and neurodegenerative disease.

</details>


### [59] [LLMs as Cultural Archives: Cultural Commonsense Knowledge Graph Extraction](https://arxiv.org/abs/2601.17971)
*Junior Cedric Tonga,Chen Cecilia Liu,Iryna Gurevych,Fajri Koto*

Main category: cs.CL

TL;DR: 通过构建文化常识知识图谱，利用大型语言模型的隐含文化知识，实现跨语言文化推理；结果揭示LLMs文化知识的不均衡分布及其在文化相关任务中的实际价值与局限。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型隐含但未结构化的丰富文化知识限制了解释性和应用，亟需显式、结构化的文化常识表达。

Method: 基于提示的迭代框架构建文化常识知识图谱（CCKG），系统化提取文化特定的实体、关系和实践，并通过多语言推理链组装。

Result: 文化知识图谱在英语表现更好，表明LLMs的文化编码不均衡；利用CCKG增强小型LLMs可提升文化推理和故事生成性能，尤其是英语推理链带来最大提升。

Conclusion: 大型语言模型作为文化技术有潜力也存在局限，链式结构的文化知识图谱是一种实用的文化基础NLP方法。

Abstract: Large language models (LLMs) encode rich cultural knowledge learned from diverse web-scale data, offering an unprecedented opportunity to model cultural commonsense at scale. Yet this knowledge remains mostly implicit and unstructured, limiting its interpretability and use. We present an iterative, prompt-based framework for constructing a Cultural Commonsense Knowledge Graph (CCKG) that treats LLMs as cultural archives, systematically eliciting culture-specific entities, relations, and practices and composing them into multi-step inferential chains across languages. We evaluate CCKG on five countries with human judgments of cultural relevance, correctness, and path coherence. We find that the cultural knowledge graphs are better realized in English, even when the target culture is non-English (e.g., Chinese, Indonesian, Arabic), indicating uneven cultural encoding in current LLMs. Augmenting smaller LLMs with CCKG improves performance on cultural reasoning and story generation, with the largest gains from English chains. Our results show both the promise and limits of LLMs as cultural technologies and that chain-structured cultural knowledge is a practical substrate for culturally grounded NLP.

</details>


### [60] [SD-E$^2$: Semantic Exploration for Reasoning Under Token Budgets](https://arxiv.org/abs/2601.17982)
*Kshitij Mishra,Nils Lukas,Salem Lahlou*

Main category: cs.CL

TL;DR: 通过引入基于语义多样性的奖励机制，SD-E²框架显著提升了小型语言模型在复杂推理任务中的表现和计算效率。


<details>
  <summary>Details</summary>
Motivation: 小型语言模型在复杂推理任务中表现不足，主要由于计算资源有限导致探索代价高昂。研究旨在通过引入语义多样性的探索策略，提升推理模型的效率和准确性。

Method: 引入了Semantic Diversity-Exploration-Exploitation（SD-E²）强化学习框架，通过优化生成推理路径的语义多样性显式地进行探索。使用冻结的句子嵌入模型，SD-E²赋予多样性奖励，捕捉语义上不同解题策略的覆盖度及其在嵌入空间中的平均两两差异性。该奖励与结果正确性和解题效率结合，通过z-score归一化的多目标目标函数稳定训练。

Result: SD-E²在GSM8K数据集上分别比基础模型Qwen2.5-3B-Instruct和强基线GRPO提升了27.4%、5.2%和1.5%正确率，平均发现近10种语义上不同的解题策略。在MedMCQA和AIME基准测试中准确率也有显著提升。

Conclusion: 奖励语义新颖性为训练推理能力强的小型语言模型提供了更高效的探索-利用信号，SD-E²通过调整推理过程结构而非逐词计算，为资源受限模型的效率提升提供了新的方向。

Abstract: Small language models (SLMs) struggle with complex reasoning because exploration is expensive under tight compute budgets. We introduce Semantic Diversity-Exploration-Exploitation (SD-E$^2$), a reinforcement learning framework that makes exploration explicit by optimizing semantic diversity in generated reasoning trajectories. Using a frozen sentence-embedding model, SD-E$^2$ assigns a diversity reward that captures (i) the coverage of semantically distinct solution strategies and (ii) their average pairwise dissimilarity in embedding space, rather than surface-form novelty. This diversity reward is combined with outcome correctness and solution efficiency in a z-score-normalized multi-objective objective that stabilizes training. On GSM8K, SD-E$^2$ surpasses the base Qwen2.5-3B-Instruct and strong GRPO baselines (GRPO-CFL and GRPO-CFEE) by +27.4, +5.2, and +1.5 percentage points, respectively, while discovering on average 9.8 semantically distinct strategies per question. We further improve MedMCQA to 49.64% versus 38.37% for the base model and show gains on the harder AIME benchmark (1983-2025), reaching 13.28% versus 6.74% for the base. These results indicate that rewarding semantic novelty yields a more compute-efficient exploration-exploitation signal for training reasoning-capable SLMs. By introducing cognitive adaptation-adjusting the reasoning process structure rather than per-token computation-SD-E$^2$ offers a complementary path to efficiency gains in resource-constrained models.

</details>


### [61] [AI-based approach to burnout identification from textual data](https://arxiv.org/abs/2601.17993)
*Marina Zavertiaeva,Petr Parshakov,Mikhail Usanin,Aleksei Smirnov,Sofia Paklina,Anastasiia Kibardina*

Main category: cs.CL

TL;DR: 本研究通过微调RuBERT模型，结合合成数据与真实评论，实现了基于文本的职业倦怠检测。


<details>
  <summary>Details</summary>
Motivation: 检测和识别文本数据中的职业倦怠，以帮助监控高压工作环境中的倦怠语言信号。

Method: 基于RuBERT模型，先进行情感分析训练，再使用ChatGPT生成的合成句子和俄语YouTube关于倦怠的视频用户评论对模型进行微调，利用自然语言处理技术进行倦怠检测。

Result: 模型能够为输入文本分配一个倦怠概率，可处理大量书面交流数据，从而实现对倦怠的有效检测。

Conclusion: 该AI方法有效识别文本中的倦怠迹象，对监控高压环境下员工的心理健康状况具有实际应用价值。

Abstract: This study introduces an AI-based methodology that utilizes natural language processing (NLP) to detect burnout from textual data. The approach relies on a RuBERT model originally trained for sentiment analysis and subsequently fine-tuned for burnout detection using two data sources: synthetic sentences generated with ChatGPT and user comments collected from Russian YouTube videos about burnout. The resulting model assigns a burnout probability to input texts and can be applied to process large volumes of written communication for monitoring burnout-related language signals in high-stress work environments.

</details>


### [62] [PEAR: Pairwise Evaluation for Automatic Relative Scoring in Machine Translation](https://arxiv.org/abs/2601.18006)
*Lorenzo Proietti,Roman Grundkiewicz,Matt Post*

Main category: cs.CL

TL;DR: 本文提出了PEAR，一种基于成对比较的机器翻译质量估计指标，在WMT24评测中表现优异，参数更少但性能超越大型模型。


<details>
  <summary>Details</summary>
Motivation: 现有无参考机器翻译评价指标依赖单个候选翻译，难以充分利用成对差异信息，且大型模型参数庞大，需设计更高效准确的QE指标。

Method: 将无参考机器翻译质量估计问题重构为带幅度的成对比较，利用人类评价差异进行成对监督训练，并引入符号反转正则化项增强模型泛化能力。

Result: PEAR在WMT24元评测基准测试中，优于同等训练数据和模型骨干的单样本QE基线，性能超越更大规模的QE模型和参考基线指标，且评价信号冗余度较低。

Conclusion: PEAR通过成对比较的QE指标在无参考机器翻译评价中表现优于传统单样本QE方法，参数更少但效果更好，且能够作为MBR解码的有效效用函数。

Abstract: We present PEAR (Pairwise Evaluation for Automatic Relative Scoring), a supervised Quality Estimation (QE) metric family that reframes reference-free Machine Translation (MT) evaluation as a graded pairwise comparison. Given a source segment and two candidate translations, PEAR predicts the direction and magnitude of their quality difference. The metrics are trained using pairwise supervision derived from differences in human judgments, with an additional regularization term that encourages sign inversion under candidate order reversal. On the WMT24 meta-evaluation benchmark, PEAR outperforms strictly matched single-candidate QE baselines trained with the same data and backbones, isolating the benefit of the proposed pairwise formulation. Despite using substantially fewer parameters than recent large metrics, PEAR surpasses far larger QE models and reference-based metrics. Our analysis further indicates that PEAR yields a less redundant evaluation signal relative to other top metrics. Finally, we show that PEAR is an effective utility function for Minimum Bayes Risk (MBR) decoding, reducing pairwise scoring cost at negligible impact.

</details>


### [63] [Evaluating Semantic and Syntactic Understanding in Large Language Models for Payroll Systems](https://arxiv.org/abs/2601.18012)
*Hendrika Maclean,Mert Can Cakmak,Muzakkiruddin Ahmed Mohammed,Shames Al Mandalawi,John Talburt*

Main category: cs.CL

TL;DR: 本文研究大型语言模型在薪资系统中的准确性问题，测试其是否能准确理解薪资规则并计算精确结果。通过多模型和多种提示方法的实验，发现不同情况下需不同策略。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然自然语言理解能力强，但在精确数值计算及可审计输出方面仍不可靠，需验证其在高风险场景的应用能力。

Method: 设计分层数据集和多样化提示策略，测试多种语言模型在理解薪资架构和计算规则上的表现。

Result: 不同模型和提示策略表现差异显著，针对复杂薪资计算任务，明确的计算指令比简单提示更有效，提出了实用的部署框架和指导。

Conclusion: 大型语言模型在薪资计算中存在准确性挑战，部分情况下需要明确计算步骤以保障结果准确可靠。

Abstract: Large language models are now used daily for writing, search, and analysis, and their natural language understanding continues to improve. However, they remain unreliable on exact numerical calculation and on producing outputs that are straightforward to audit. We study synthetic payroll system as a focused, high-stakes example and evaluate whether models can understand a payroll schema, apply rules in the right order, and deliver cent-accurate results. Our experiments span a tiered dataset from basic to complex cases, a spectrum of prompts from minimal baselines to schema-guided and reasoning variants, and multiple model families including GPT, Claude, Perplexity, Grok and Gemini. Results indicate clear regimes where careful prompting is sufficient and regimes where explicit computation is required. The work offers a compact, reproducible framework and practical guidance for deploying LLMs in settings that demand both accuracy and assurance.

</details>


### [64] [A System for Name and Address Parsing with Large Language Models](https://arxiv.org/abs/2601.18014)
*Adeeba Tarannum,Muzakkiruddin Ahmed Mohammed,Mert Can Cakmak,Shames Al Mandalawi,John Talburt*

Main category: cs.CL

TL;DR: 提出了一种无需微调的提示驱动及验证集中框架，高效、稳定地将非结构化人名和地址文本转换为结构化数据，克服了传统和神经方法的不足。


<details>
  <summary>Details</summary>
Motivation: 在大规模信息系统中，将非结构化的人名和地址文本可靠地转换为结构化数据是一个关键挑战。传统的基于规则和概率的方法在干净的数据上效果良好，但在噪声多或多语言条件下表现不佳；而神经网络和大型语言模型虽有生成能力，却缺乏确定性控制和可重复性。

Method: 提出了一种基于提示驱动和验证中心的框架，将自由文本记录转换成一致的17字段结构，且无需微调。该方法结合了输入归一化、结构化提示、受限解码和严格的基于规则验证，在固定的实验设置下确保结果的可重复性。

Result: 在异构的真实地址数据上评估，显示出较高的字段级准确率、强一致的结构遵从性以及稳定的置信度校准。

Conclusion: 结合确定性的验证机制与生成式提示方法，提供了一种稳健、可解释且可扩展的结构化信息提取解决方案，是训练密集型或领域特定模型的实用替代方案。

Abstract: Reliable transformation of unstructured person and address text into structured data remains a key challenge in large-scale information systems. Traditional rule-based and probabilistic approaches perform well on clean inputs but fail under noisy or multilingual conditions, while neural and large language models (LLMs) often lack deterministic control and reproducibility. This paper introduces a prompt-driven, validation-centered framework that converts free-text records into a consistent 17-field schema without fine-tuning. The method integrates input normalisation, structured prompting, constrained decoding, and strict rule-based validation under fixed experimental settings to ensure reproducibility. Evaluations on heterogeneous real-world address data show high field-level accuracy, strong schema adherence, and stable confidence calibration. The results demonstrate that combining deterministic validation with generative prompting provides a robust, interpretable, and scalable solution for structured information extraction, offering a practical alternative to training-heavy or domain-specific models.

</details>


### [65] [CommonLID: Re-evaluating State-of-the-Art Language Identification Performance on Web Data](https://arxiv.org/abs/2601.18026)
*Pedro Ortiz Suarez,Laurie Burchell,Catherine Arnett,Rafael Mosquera-Gómez,Sara Hincapie-Monsalve,Thom Vaughan,Damian Stewart,Malte Ostendorff,Idris Abdulmumin,Vukosi Marivate,Shamsuddeen Hassan Muhammad,Atnafu Lambebo Tonja,Hend Al-Khalifa,Nadia Ghezaiel Hammouda,Verrah Otiende,Tack Hwa Wong,Jakhongir Saydaliev,Melika Nobakhtian,Muhammad Ravi Shulthan Habibi,Chalamalasetti Kranti,Carol Muchemi,Khang Nguyen,Faisal Muhammad Adam,Luis Frentzen Salim,Reem Alqifari,Cynthia Amol,Joseph Marvin Imperial,Ilker Kesen,Ahmad Mustafid,Pavel Stepachev,Leshem Choshen,David Anugraha,Hamada Nayel,Seid Muhie Yimam,Vallerie Alexandra Putra,My Chiffon Nguyen,Azmine Toushik Wasi,Gouthami Vadithya,Rob van der Goot,Lanwenn ar C'horr,Karan Dua,Andrew Yates,Mithil Bangera,Yeshil Bangera,Hitesh Laxmichand Patel,Shu Okabe,Fenal Ashokbhai Ilasariya,Dmitry Gaynullin,Genta Indra Winata,Yiyuan Li,Juan Pablo Martínez,Amit Agarwal,Ikhlasul Akmal Hanif,Raia Abu Ahmad,Esther Adenuga,Filbert Aurelian Tjiaranata,Weerayut Buaphet,Michael Anugraha,Sowmya Vajjala,Benjamin Rice,Azril Hafizi Amirudin,Jesujoba O. Alabi,Srikant Panda,Yassine Toughrai,Bruhan Kyomuhendo,Daniel Ruffinelli,Akshata A,Manuel Goulão,Ej Zhou,Ingrid Gabriela Franco Ramirez,Cristina Aggazzotti,Konstantin Dobler,Jun Kevin,Quentin Pagès,Nicholas Andrews,Nuhu Ibrahim,Mattes Ruckdeschel,Amr Keleg,Mike Zhang,Casper Muziri,Saron Samuel,Sotaro Takeshita,Kun Kerdthaisong,Luca Foppiano,Rasul Dent,Tommaso Green,Ahmad Mustapha Wali,Kamohelo Makaaka,Vicky Feliren,Inshirah Idris,Hande Celikkanat,Abdulhamid Abubakar,Jean Maillard,Benoît Sagot,Thibault Clérice,Kenton Murray,Sarah Luger*

Main category: cs.CL

TL;DR: 本文提出了涵盖109种语言、针对Web数据的高质量人工标注语言识别基准CommonLID，揭示现有LID评测对网络语言的高估，推动多语言文本处理领域发展。


<details>
  <summary>Details</summary>
Motivation: 现有的语言识别(LID)模型在多语言语料库建设中的表现仍不理想，特别是在处理网络上嘈杂、异构的数据时，许多语言的识别效果较差，且有些语言缺乏评测资源。

Method: 提出CommonLID，一个涵盖109种语言的社区驱动、人工标注的Web域语言识别基准数据集，并结合五个常用评测集对八个流行的LID模型进行测试和性能分析。

Result: 展示了CommonLID的价值，揭示了当前评测数据集高估了多语言Web域LID的准确率，同时使CommonLID及其生成代码在开源许可下公开。

Conclusion: CommonLID为多语言LID模型的公平评估和提升提供了关键资源，尤其对之前服务不足的语言具有重要意义，推动了多语种高质量文本语料库的发展。

Abstract: Language identification (LID) is a fundamental step in curating multilingual corpora. However, LID models still perform poorly for many languages, especially on the noisy and heterogeneous web data often used to train multilingual language models. In this paper, we introduce CommonLID, a community-driven, human-annotated LID benchmark for the web domain, covering 109 languages. Many of the included languages have been previously under-served, making CommonLID a key resource for developing more representative high-quality text corpora. We show CommonLID's value by using it, alongside five other common evaluation sets, to test eight popular LID models. We analyse our results to situate our contribution and to provide an overview of the state of the art. In particular, we highlight that existing evaluations overestimate LID accuracy for many languages in the web domain. We make CommonLID and the code used to create it available under an open, permissive license.

</details>


### [66] [Addressing LLM Diversity by Infusing Random Concepts](https://arxiv.org/abs/2601.18053)
*Pulin Agrawal,Prasoon Goyal*

Main category: cs.CL

TL;DR: 通过在提示中添加随机无关词汇，能够有效提升大规模语言模型生成输出的多样性。该方法和评估协议为后续改进及多样性基准测试提供了基础。


<details>
  <summary>Details</summary>
Motivation: 当前大规模语言模型生成的输出多样性有限，迫切需要提升生成内容的多样性。

Method: 设计系统性的评估协议，通过在提示中注入随机无关词汇或句子，对LLM进行多次生成，并分析生成内容的多样性指标。

Result: 实验表明，在提示前加入随机无关概念显著提升了不同大规模语言模型生成结果的多样性。

Conclusion: 注入随机概念是提升LLM输出多样性的一种有效方法，评估协议为系统衡量和改进LLM多样性提供了新思路。

Abstract: Large language models (LLMs) are known to produce outputs with limited diversity. In this work, we study whether infusing random concepts in the prompts can improve the diversity of the generated outputs. To benchmark the approach, we design a systematic evaluation protocol which involves prompting an LLM with questions of the form "Name 10 Hollywood actors", and analyzing diversity measures of the resulting LLM outputs. Our experiments on multiple LLMs show that prepending random words/sentences unrelated to the prompt result in greater diversity in the outputs of LLMs. We believe that this promising result and the evaluation protocol opens up interesting avenues for future work, such as how infusing randomness into LLMs could be applied to other domains. Further, the evaluation protocol could also inspire research into benchmarking LLM diversity more systematically.

</details>


### [67] [Neurocomputational Mechanisms of Syntactic Transfer in Bilingual Sentence Production](https://arxiv.org/abs/2601.18056)
*Ahmet Yavuz Uluslu,Elliot Murphy*

Main category: cs.CL

TL;DR: 结合振荡特征和神经模型ROSE，提出双语句法转移的神经计算解释，特别是跨语言影响中的振荡失效模式，为语言障碍生物标志物研究提供新方向。


<details>
  <summary>Details</summary>
Motivation: 传统双语产出错误研究主要依赖时间特征，缺乏对实施层的振荡模式分析，限制了对双语句法转移机制的理解。

Method: 利用神经模型ROSE，结合振荡失效模式模拟L2句子规划中的跨语言影响，验证功能抑制和竞争理论。

Result: 本文探讨了将双语产出错误研究与传统时间特征（如事件相关电位）结合振荡特征的好处，提出振荡特征能为双语理论提供更具体的实现层约束。作者利用神经模型ROSE解释双语产生中的句法转移，尤其是形态句法序列失效的形式特征。通过跨语言影响（CLI）及功能抑制/竞争理论的案例，指出这些现象受L2句子规划过程中振荡失效模式驱动。此方法不仅实现了ROSE模型的连接假设，还促进了更复杂空间时间语言障碍生物标志物的探索。

Conclusion: 振荡特征结合ROSE模型有效解释双语句法转移和跨语言影响，推动语言功能障碍的生物标志物研究。

Abstract: We discuss the benefits of incorporating into the study of bilingual production errors and their traditionally documented timing signatures (e.g., event-related potentials) certain types of oscillatory signatures, which can offer new implementational-level constraints for theories of bilingualism. We argue that a recent neural model of language, ROSE, can offer a neurocomputational account of syntactic transfer in bilingual production, capturing some of its formal properties and the scope of morphosyntactic sequencing failure modes. We take as a case study cross-linguistic influence (CLI) and attendant theories of functional inhibition/competition, and present these as being driven by specific oscillatory failure modes during L2 sentence planning. We argue that modeling CLI in this way not only offers the kind of linking hypothesis ROSE was built to encourage, but also licenses the exploration of more spatiotemporally complex biomarkers of language dysfunction than more commonly discussed neural signatures.

</details>


### [68] [Grounded Concreteness: Human-Like Concreteness Sensitivity in Vision-Language Models](https://arxiv.org/abs/2601.18065)
*Aryan Roy,Zekun Wang,Christopher J. MacLellan*

Main category: cs.CL

TL;DR: 视觉-语言模型相比纯文本模型，在语言具体性上表现出更人类般的感知和表现。


<details>
  <summary>Details</summary>
Motivation: 探究视觉-语言模型在仅用文本提示时，是否比纯文本大语言模型对语言具体性具有更类似人类的敏感性。

Method: 通过对比匹配的Llama文本骨干模型和其视觉版本，控制多模态预训练作为感知基础的消融实验，在多个模型规模上进行实验。

Result: 视觉-语言模型在更具体的输入上表现出更大的提升，表征结构更清晰地沿具体性轴组织，评分更符合人类规范分布，注意力模式与增强的感知基础一致。

Conclusion: 多模态训练使模型在语言具体性判断上更接近人类，体现了更强的感知基础和更符合人类的内部表征。

Abstract: Do vision--language models (VLMs) develop more human-like sensitivity to linguistic concreteness than text-only large language models (LLMs) when both are evaluated with text-only prompts? We study this question with a controlled comparison between matched Llama text backbones and their Llama Vision counterparts across multiple model scales, treating multimodal pretraining as an ablation on perceptual grounding rather than access to images at inference. We measure concreteness effects at three complementary levels: (i) output behavior, by relating question-level concreteness to QA accuracy; (ii) embedding geometry, by testing whether representations organize along a concreteness axis; and (iii) attention dynamics, by quantifying context reliance via attention-entropy measures. In addition, we elicit token-level concreteness ratings from models and evaluate alignment to human norm distributions, testing whether multimodal training yields more human-consistent judgments. Across benchmarks and scales, VLMs show larger gains on more concrete inputs, exhibit clearer concreteness-structured representations, produce ratings that better match human norms, and display systematically different attention patterns consistent with increased grounding.

</details>


### [69] [Sparks of Cooperative Reasoning: LLMs as Strategic Hanabi Agents](https://arxiv.org/abs/2601.18077)
*Mahesh Ramesh,Kaousheik Jayakumar,Aswinkumar Ramkumar,Pavan Thodima,Aniket Rege*

Main category: cs.CL

TL;DR: 针对Hanabi游戏合作推理难题，研究大规模LLM在不同上下文提示下的表现，公开数据集并通过微调显著提升模型合作能力和泛化性能。


<details>
  <summary>Details</summary>
Motivation: Hanabi作为合作推理与战略沟通的典型挑战，反映多智能体和人类在不完全信息下协作推理的困难，旨在评估并提升大型语言模型在此领域的能力。

Method: 设计了三种不同上下文提示方式（简洁卡牌信息、贝叶斯推理启发、多轮工作记忆状态追踪），在多个玩家规模下测试17种LLM代理性能，并发布带轨迹和动作价值注释的数据集；基于此对4B参数开源模型进行监督和强化学习微调以提升性能。

Result: 本文系统评测了17种最先进的大型语言模型（LLM）代理在多人Hanabi游戏中的合作推理能力，探索不同上下文设计（从简单卡牌信息到贝叶斯推理和多轮状态追踪）对模型表现的影响。结果表明，模型能够维护内部工作记忆完成状态追踪，且模型性能随规模提升而稳定提升。在最复杂的Sherlock设置下，顶尖模型平均得分超过15分，但仍落后于经验丰富的人类玩家及专门的Hanabi代理。作者公开了两个带有注释轨迹和动作价值的数据集（HanabiLogs和HanabiRewards），并通过监督学习及强化学习在一个4B参数开源模型上实现显著提升，强化学习使表现提升高达156%，接近强力推理模型水平，超越了最佳非推理模型。强化学习微调的模型还能泛化至其他合作推理和时间推理任务，表现提升明显。

Conclusion: 多层次上下文提示有助提升LLM的合作推理能力，强化学习微调能显著增强模型表现并具备良好泛化能力，但当前模型仍弱于人类和专门代理。

Abstract: Cooperative reasoning under incomplete information remains challenging for both humans and multi-agent systems. The card game Hanabi embodies this challenge, requiring theory-of-mind reasoning and strategic communication. We benchmark 17 state-of-the-art LLM agents in 2-5 player games and study the impact of context engineering across model scales (4B to 600B+) to understand persistent coordination failures and robustness to scaffolding: from a minimal prompt with only explicit card details (Watson setting), to scaffolding with programmatic, Bayesian-motivated deductions (Sherlock setting), to multi-turn state tracking via working memory (Mycroft setting). We show that (1) agents can maintain an internal working memory for state tracking and (2) cross-play performance between different LLMs smoothly interpolates with model strength. In the Sherlock setting, the strongest reasoning models exceed 15 points on average across player counts, yet still trail experienced humans and specialist Hanabi agents, both consistently scoring above 20. We release the first public Hanabi datasets with annotated trajectories and move utilities: (1) HanabiLogs, containing 1,520 full game logs for instruction tuning, and (2) HanabiRewards, containing 560 games with dense move-level value annotations for all candidate moves. Supervised and RL finetuning of a 4B open-weight model (Qwen3-Instruct) on our datasets improves cooperative Hanabi play by 21% and 156% respectively, bringing performance to within ~3 points of a strong proprietary reasoning model (o4-mini) and surpassing the best non-reasoning model (GPT-4.1) by 52%. The HanabiRewards RL-finetuned model further generalizes beyond Hanabi, improving performance on a cooperative group-guessing benchmark by 11%, temporal reasoning on EventQA by 6.4%, instruction-following on IFBench-800K by 1.7 Pass@10, and matching AIME 2025 mathematical reasoning Pass@10.

</details>


### [70] [CHiRPE: A Step Towards Real-World Clinical NLP with Clinician-Oriented Model Explanations](https://arxiv.org/abs/2601.18102)
*Stephanie Fong,Zimu Wang,Guilherme C. Oliveira,Xiangyu Zhao,Yiwen Jiang,Jiahe Liu,Beau-Luke Colton,Scott Woods,Martha E. Shenton,Barnaby Nelson,Zongyuan Ge,Dominic Dwyer*

Main category: cs.CL

TL;DR: CHiRPE是一种结合临床专家共同开发的精神病风险预测NLP工具，准确率高且解释方式更符合医生需求，展示了临床指导下模型开发的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统的可解释AI方法与临床推理不匹配且缺乏临床医生参与，限制了NLP工具在医疗中的应用，因此需要一种结合临床专家反馈的新型解释方法。

Method: 基于944份跨24个国际诊所的临床访谈文本，采用症状领域映射、大型语言模型摘要和BERT分类，开发了CHiRPE自动预测精神病风险并生成SHAP解释的NLP流水线。

Result: CHiRPE在三种BERT变体中都达到了超过90%的准确率，优于基线模型；28位临床专家评价了新型解释格式，强烈偏好混合图文摘要的概念引导解释。

Conclusion: CHiRPE管道通过结合临床指导的模型开发，实现了高准确率和良好的解释性，满足了医疗领域对NLP工具可解释性的需求。

Abstract: The medical adoption of NLP tools requires interpretability by end users, yet traditional explainable AI (XAI) methods are misaligned with clinical reasoning and lack clinician input. We introduce CHiRPE (Clinical High-Risk Prediction with Explainability), an NLP pipeline that takes transcribed semi-structured clinical interviews to: (i) predict psychosis risk; and (ii) generate novel SHAP explanation formats co-developed with clinicians. Trained on 944 semi-structured interview transcripts across 24 international clinics of the AMP-SCZ study, the CHiRPE pipeline integrates symptom-domain mapping, LLM summarisation, and BERT classification. CHiRPE achieved over 90% accuracy across three BERT variants and outperformed baseline models. Explanation formats were evaluated by 28 clinical experts who indicated a strong preference for our novel concept-guided explanations, especially hybrid graph-and-text summary formats. CHiRPE demonstrates that clinically-guided model development produces both accurate and interpretable results. Our next step is focused on real-world testing across our 24 international sites.

</details>


### [71] [GLEN-Bench: A Graph-Language based Benchmark for Nutritional Health](https://arxiv.org/abs/2601.18106)
*Jiatan Huang,Zheyuan Zhang,Tianyi Ma,Mingchen Li,Yaning Zheng,Yanfang Ye,Chuxu Zhang*

Main category: cs.CL

TL;DR: 该论文提出了GLEN-Bench，这是一个基于图语言的综合性营养健康评估基准，整合了多个数据源，解决了个性化营养干预中的三个关键问题，包括实际约束、解释性和统一评估。通过阿片类药物使用障碍病例进行了验证，涵盖风险检测、个性化推荐和基于图的问答三个任务。


<details>
  <summary>Details</summary>
Motivation: 现有计算方法难以支持个性化膳食指导，存在忽略现实条件、不解释推荐原因、缺乏统一评测等三大不足，需开发更综合、更具解释性与现实适用性的营养干预工具。

Method: 构建整合NHANES健康记录、FNDDS食物组成数据和USDA食品获取指标的知识图谱，将人口统计、健康状态、饮食行为、贫困相关限制和营养需求联系起来，设计三个互相关联的任务（风险检测、推荐、问答），评估包括图神经网络、大型语言模型和混合架构在内的多种图语言方法。

Result: 通过阿片类药物使用障碍案例，GLEN-Bench成功识别了与健康风险相关的饮食模式，验证了方法的实用性与有效性，为未来营养干预提供数据支撑和方法基础。

Conclusion: GLEN-Bench为营养健康评估提供了首个综合性的图语言基准，能够有效识别风险个体，推荐符合临床与资源限制的个性化饮食方案，并提供解释性支持，有助于指导实际营养干预。

Abstract: Nutritional interventions are important for managing chronic health conditions, but current computational methods provide limited support for personalized dietary guidance. We identify three key gaps: (1) dietary pattern studies often ignore real-world constraints such as socioeconomic status, comorbidities, and limited food access; (2) recommendation systems rarely explain why a particular food helps a given patient; and (3) no unified benchmark evaluates methods across the connected tasks needed for nutritional interventions. We introduce GLEN-Bench, the first comprehensive graph-language based benchmark for nutritional health assessment. We combine NHANES health records, FNDDS food composition data, and USDA food-access metrics to build a knowledge graph that links demographics, health conditions, dietary behaviors, poverty-related constraints, and nutrient needs. We test the benchmark using opioid use disorder, where models must detect subtle nutritional differences across disease stages. GLEN-Bench includes three linked tasks: risk detection identifies at-risk individuals from dietary and socioeconomic patterns; recommendation suggests personalized foods that meet clinical needs within resource constraints; and question answering provides graph-grounded, natural-language explanations to facilitate comprehension. We evaluate these graph-language approaches, including graph neural networks, large language models, and hybrid architectures, to establish solid baselines and identify practical design choices. Our analysis identifies clear dietary patterns linked to health risks, providing insights that can guide practical interventions.

</details>


### [72] [FABLE: Forest-Based Adaptive Bi-Path LLM-Enhanced Retrieval for Multi-Document Reasoning](https://arxiv.org/abs/2601.18116)
*Lin Sun,Linglin Zhang,Jingang Huang,Change Jia,Zhengwei Cheng,Xiangzheng Zhang*

Main category: cs.CL

TL;DR: FABLE利用层次化森林索引和双路径检索策略，有效解决了长上下文LLM检索中的效率和语义噪声问题。


<details>
  <summary>Details</summary>
Motivation: 长上下文LLM推理存在‘中间遗忘’、高计算成本和多文档推理扩展性差等问题，传统扁平检索引入语义噪声且缺乏结构化支持，需设计新型检索框架兼顾准确性和效率。

Method: 通过构建LLM增强的多粒度语义层次森林索引，结合基于LLM的层级遍历与结构感知的证据传播双路径策略，实现精细检索和动态效率控制。

Result: FABLE框架通过构建分层语义森林索引并结合双路径检索策略，实现了高效且结构化的跨文档信息检索，显著提升了检索增强生成任务的性能。

Conclusion: FABLE在准确率上超越了现有RAG方法，并在保持与全上下文LLM推理相似准确度的同时，大幅减少了计算资源消耗，证明了结构化检索在长上下文场景中的必要性和效率。

Abstract: The rapid expansion of long-context Large Language Models (LLMs) has reignited debate on whether Retrieval-Augmented Generation (RAG) remains necessary. However, empirical evidence reveals persistent limitations of long-context inference, including the lost-in-the-middle phenomenon, high computational cost, and poor scalability for multi-document reasoning. Conversely, traditional RAG systems, while efficient, are constrained by flat chunk-level retrieval that introduces semantic noise and fails to support structured cross-document synthesis.
  We present \textbf{FABLE}, a \textbf{F}orest-based \textbf{A}daptive \textbf{B}i-path \textbf{L}LM-\textbf{E}nhanced retrieval framework that integrates LLMs into both knowledge organization and retrieval. FABLE constructs LLM-enhanced hierarchical forest indexes with multi-granularity semantic structures, then employs a bi-path strategy combining LLM-guided hierarchical traversal with structure-aware propagation for fine-grained evidence acquisition, with explicit budget control for adaptive efficiency trade-offs.
  Extensive experiments demonstrate that FABLE consistently outperforms SOTA RAG methods and achieves comparable accuracy to full-context LLM inference with up to 94\% token reduction, showing that long-context LLMs amplify rather than fully replace the need for structured retrieval.

</details>


### [73] [Typhoon-S: Minimal Open Post-Training for Sovereign Large Language Models](https://arxiv.org/abs/2601.18129)
*Kunat Pipatanakul,Pittawat Taveekitworachai*

Main category: cs.CL

TL;DR: 本文提出Typhoon S，一种结合监督微调、在策略蒸馏和小规模强化微调的轻量级后训练方法，旨在在资源有限的情况下，将基础大语言模型转化为适应地方需求的通用助手，特别关注泰语法律推理及本土知识的提升。


<details>
  <summary>Details</summary>
Motivation: 当前主流大型语言模型主要面向高资源语言且依赖大型训练资源，不便于地方或主权机构在资源有限且需透明控制的环境下使用，故需开发既能适应多任务又符合主权要求的训练方法。

Method: 结合监督微调、在策略蒸馏和小规模的基于扩展GRPO的强化学习微调，特别引入基于下一个词预测的损失，提升模型在特定语言和领域的表现。

Result: 以泰语为例，所提方法使模型在保持通用性能的基础上，有效提升了泰语法律推理及相关地方知识能力，且降低了指令数据和计算资源需求。

Conclusion: 通过Typhoon S后训练策略，可以在无需大规模指令语料和复杂调优的情况下，有效提升模型对地方语言和高风险地区专用任务的处理能力，展现出高质量主权级别大语言模型的可行性。

Abstract: Large language models (LLMs) have progressed rapidly; however, most state-of-the-art models are trained and evaluated primarily in high-resource languages such as English and Chinese, and are often developed by a small number of organizations with access to large-scale compute and data. This gatekeeping creates a practical barrier for sovereign settings in which a regional- or national-scale institution or domain owner must retain control and understanding of model weights, training data, and deployment while operating under limited resources and strict transparency constraints. To this end, we identify two core requirements: (1) adoptability, the ability to transform a base model into a general-purpose assistant, and (2) sovereign capability, the ability to perform high-stakes, region-specific tasks (e.g., legal reasoning in local languages and cultural knowledge). We investigate whether these requirements can be achieved without scaling massive instruction corpora or relying on complex preference tuning pipelines and large-scale reinforcement fine-tuning (RFT). We present Typhoon S, a minimal and open post-training recipe that combines supervised fine-tuning, on-policy distillation, and small-scale RFT. Using Thai as a representative case study, we demonstrate that our approach transforms both sovereign-adapted and general-purpose base models into instruction-tuned models with strong general performance. We further show that small-scale RFT with InK-GRPO -- an extension of GRPO that augments the GRPO loss with a next-word prediction loss -- improves Thai legal reasoning and Thai-specific knowledge while preserving general capabilities. Our results suggest that a carefully designed post-training strategy can reduce the required scale of instruction data and computation, providing a practical path toward high-quality sovereign LLMs under academic-scale resources.

</details>


### [74] [Fine-Grained Emotion Detection on GoEmotions: Experimental Comparison of Classical Machine Learning, BiLSTM, and Transformer Models](https://arxiv.org/abs/2601.18162)
*Ani Harutyunyan,Sachin Kumar*

Main category: cs.CL

TL;DR: 本文针对细粒度情感识别这一具有标签重叠和类别不平衡挑战的多标签NLP任务，比较了基于TF-IDF的逻辑回归、带注意力机制的BiLSTM和微调的BERT三种模型在GoEmotions数据集上的表现。


<details>
  <summary>Details</summary>
Motivation: 细粒度情感识别面临标签重叠和类别不平衡的双重挑战，迫切需要评估不同模型及其在该任务上的表现差异。

Method: 本文使用TF-IDF的逻辑回归模型（采用二元相关方法训练）、带注意力机制的BiLSTM模型以及微调的BERT模型进行多标签分类，采用官方数据划分并使用逆频率类别权重缓解类别不平衡问题。

Result: 逻辑回归模型取得最高Micro-F1=0.51，BERT模型在Macro-F1=0.49，Hamming Loss=0.036和Subset Accuracy=0.36等指标均优于其他模型和此前官方结果。频繁情感依赖表层词汇特征，BERT的上下文表示提升了稀有情感和模糊样本的识别能力。

Conclusion: 逻辑回归模型在Micro-F1指标上表现最佳，而BERT模型在Macro-F1、Hamming Loss和Subset Accuracy等指标上表现更均衡，优于官方公布结果。

Abstract: Fine-grained emotion recognition is a challenging multi-label NLP task due to label overlap and class imbalance. In this work, we benchmark three modeling families on the GoEmotions dataset: a TF-IDF-based logistic regression system trained with binary relevance, a BiLSTM with attention, and a BERT model fine-tuned for multi-label classification. Experiments follow the official train/validation/test split, and imbalance is mitigated using inverse-frequency class weights. Across several metrics, namely Micro-F1, Macro-F1, Hamming Loss, and Subset Accuracy, we observe that logistic regression attains the highest Micro-F1 of 0.51, while BERT achieves the best overall balance surpassing the official paper's reported results, reaching Macro-F1 0.49, Hamming Loss 0.036, and Subset Accuracy 0.36. This suggests that frequent emotions often rely on surface lexical cues, whereas contextual representations improve performance on rarer emotions and more ambiguous examples.

</details>


### [75] [MemWeaver: Weaving Hybrid Memories for Traceable Long-Horizon Agentic Reasoning](https://arxiv.org/abs/2601.18204)
*Juexiang Ye,Xue Li,Xinyu Yang,Chengkai Huang,Lanshun Nie,Lina Yao,Dechen Zhan*

Main category: cs.CL

TL;DR: MemWeaver提出了一种统一的记忆框架，有效提升大语言模型代理在长时交互中的推理能力与记忆效率。


<details>
  <summary>Details</summary>
Motivation: 当前长时交互代理依赖非结构化或粗糙抽象的记忆方式，导致时间冲突、推理脆弱及溯源受限，需要一种统一且结构化的记忆框架以提升推理的连贯性和准确性。

Method: MemWeaver整合了时序图记忆、经验记忆和文本证据记忆三部分，通过双通道联合检索结构化知识和支持证据，为推理构建紧凑而信息丰富的上下文。

Result: MemWeaver通过构建时间关联的图形记忆、经验记忆和文本证据记忆三大组件，实现了长期记忆的高效整合，支持多跳推理和跨会话证据利用，显著提升了多跳和时间推理的准确率，同时大幅减少了输入上下文长度。

Conclusion: MemWeaver通过双通道检索方法有力地改善了多跳和时间推理的表现，提升了推理的准确性与稳健性，并实现了输入上下文的极大压缩。

Abstract: Large language model-based agents operating in long-horizon interactions require memory systems that support temporal consistency, multi-hop reasoning, and evidence-grounded reuse across sessions. Existing approaches largely rely on unstructured retrieval or coarse abstractions, which often lead to temporal conflicts, brittle reasoning, and limited traceability. We propose MemWeaver, a unified memory framework that consolidates long-term agent experiences into three interconnected components: a temporally grounded graph memory for structured relational reasoning, an experience memory that abstracts recurring interaction patterns from repeated observations, and a passage memory that preserves original textual evidence. MemWeaver employs a dual-channel retrieval strategy that jointly retrieves structured knowledge and supporting evidence to construct compact yet information-dense contexts for reasoning. Experiments on the LoCoMo benchmark demonstrate that MemWeaver substantially improves multi-hop and temporal reasoning accuracy while reducing input context length by over 95\% compared to long-context baselines.

</details>


### [76] [TechING: Towards Real World Technical Image Understanding via VLMs](https://arxiv.org/abs/2601.18238)
*Tafazzul Nadeem,Bhavik Shangari,Manish Rai,Gagan Raj Gupta,Ashutosh Modi*

Main category: cs.CL

TL;DR: 通过合成手绘技术图像语料库和多任务自监督学习，提升了VLM对技术图的理解，显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型难以理解手绘技术图，且缺乏大规模真实手绘图像数据，故通过合成数据增强训练以提升模型的技术图理解能力。

Method: 提出合成生成大规模仿真手绘技术图语料库，并设定多种自监督训练任务，基于该数据微调Llama 3.2 11B-instruct模型，生成LLama-VL-TUG，进行多模型对比和人类评估。

Result: 本文提出了一种通过合成生成大规模手绘技术图像语料库的方法，用于训练视觉语言模型（VLM），并引入多种自监督任务以提升模型对技术图的理解能力。基于此，微调了Llama 3.2 11B-instruct模型，得到LLama-VL-TUG模型。实验表明，该模型在ROUGE-L指标上提升了2.14倍，并在人类评估的真实手绘图像上获得了7种类型中7种的最低编译错误率，平均F1分数提升6.97倍，整体性能优于所有基线模型。

Conclusion: 通过合成数据和自监督任务微调的LLama-VL-TUG模型显著提升了技术图理解能力，在多项指标和真实手绘图像评估中均优于现有基线。

Abstract: Professionals working in technical domain typically hand-draw (on whiteboard, paper, etc.) technical diagrams (e.g., flowcharts, block diagrams, etc.) during discussions; however, if they want to edit these later, it needs to be drawn from scratch. Modern day VLMs have made tremendous progress in image understanding but they struggle when it comes to understanding technical diagrams. One way to overcome this problem is to fine-tune on real world hand-drawn images, but it is not practically possible to generate large number of such images. In this paper, we introduce a large synthetically generated corpus (reflective of real world images) for training VLMs and subsequently evaluate VLMs on a smaller corpus of hand-drawn images (with the help of humans). We introduce several new self-supervision tasks for training and perform extensive experiments with various baseline models and fine-tune Llama 3.2 11B-instruct model on synthetic images on these tasks to obtain LLama-VL-TUG, which significantly improves the ROUGE-L performance of Llama 3.2 11B-instruct by 2.14x and achieves the best all-round performance across all baseline models. On real-world images, human evaluation reveals that we achieve minimum compilation errors across all baselines in 7 out of 8 diagram types and improve the average F1 score of Llama 3.2 11B-instruct by 6.97x.

</details>


### [77] [BoRP: Bootstrapped Regression Probing for Scalable and Human-Aligned LLM Evaluation](https://arxiv.org/abs/2601.18253)
*Peng Sun,Xiangyu Zhang,Duan Wu*

Main category: cs.CL

TL;DR: 本文提出了BoRP，一种基于大模型潜在空间几何性质的用户满意度评估框架，能自动生成评分标准并通过偏最小二乘法映射隐藏状态为连续分数，显著超过生成式基线且降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 开放式助手中难以依赖传统A/B测试的明确反馈和隐式指标，导致用户满意度评估缺乏可靠度，因此需要一种可扩展且高保真的评估框架。

Method: BoRP利用LLM的潜在空间几何特性，采用极化指数的自举机制自动生成评价准则，并通过偏最小二乘法（PLS）将模型隐藏状态映射为连续的满意度分数。

Result: 在工业数据集上的实验表明，BoRP（基于Qwen3-8B/14B）在与人工判断一致度上显著优于生成式基线模型（包括Qwen3-Max），且推理成本降低了数个数量级。

Conclusion: BoRP框架在工业数据集上表现优于生成式方法，更加符合人工评价，且大幅降低了计算开销，适合大规模监控和精细A/B测试。

Abstract: Accurate evaluation of user satisfaction is critical for iterative development of conversational AI. However, for open-ended assistants, traditional A/B testing lacks reliable metrics: explicit feedback is sparse, while implicit metrics are ambiguous. To bridge this gap, we introduce BoRP (Bootstrapped Regression Probing), a scalable framework for high-fidelity satisfaction evaluation. Unlike generative approaches, BoRP leverages the geometric properties of LLM latent space. It employs a polarization-index-based bootstrapping mechanism to automate rubric generation and utilizes Partial Least Squares (PLS) to map hidden states to continuous scores. Experiments on industrial datasets show that BoRP (Qwen3-8B/14B) significantly outperforms generative baselines (even Qwen3-Max) in alignment with human judgments. Furthermore, BoRP reduces inference costs by orders of magnitude, enabling full-scale monitoring and highly sensitive A/B testing via CUPED.

</details>


### [78] [Reflecting Twice before Speaking with Empathy: Self-Reflective Alternating Inference for Empathy-Aware End-to-End Spoken Dialogue](https://arxiv.org/abs/2601.18281)
*Yuhang Jia,Pei Liu,Haoqin Sun,Jiaming Zhou,Xuxin Cheng,Cao Liu,Ke Zeng,Xunliang Cai,Yong Qin*

Main category: cs.CL

TL;DR: 本文提出了基于自然语言评价的同理心评估模型和融合反思推理机制的同理心对话生成模型，显著提升了端到端口语语言模型的情感智能和同理心表现。


<details>
  <summary>Details</summary>
Motivation: 现有的端到端口语语言模型在同理心对话中依赖刚性监督信号，如监督微调的真实回复或强化学习中的偏好分数，这限制了复杂同理心的建模能力，因为没有唯一正确的回复且简单的数值评分无法完全捕捉情感表达的细微差别。

Method: 提出了基于自然语言描述的同理心质量评估模型EmpathyEval，并基于此开发了ReEmpathy端到端口语语言模型，采用同理心自反交替推理机制，将口语回复生成与自由形式的同理心相关反思推理交替进行。

Result: 实验证明，ReEmpathy通过引入反思推理显著提升了对同理心敏感的口语对话能力，实现了更加情感智能和具同理心的人机交互。

Conclusion: 通过自然语言描述的评估和自反推理机制，ReEmpathy有效突破了传统刚性监督信号的限制，推动了具备复杂情感理解能力的口语语言模型的发展。

Abstract: End-to-end Spoken Language Models (SLMs) hold great potential for paralinguistic perception, and numerous studies have aimed to enhance their capabilities, particularly for empathetic dialogue. However, current approaches largely depend on rigid supervised signals, such as ground-truth response in supervised fine-tuning or preference scores in reinforcement learning. Such reliance is fundamentally limited for modeling complex empathy, as there is no single "correct" response and a simple numerical score cannot fully capture the nuances of emotional expression or the appropriateness of empathetic behavior. To address these limitations, we sequentially introduce EmpathyEval, a descriptive natural-language-based evaluation model for assessing empathetic quality in spoken dialogues. Building upon EmpathyEval, we propose ReEmpathy, an end-to-end SLM that enhances empathetic dialogue through a novel Empathetic Self-Reflective Alternating Inference mechanism, which interleaves spoken response generation with free-form, empathy-related reflective reasoning. Extensive experiments demonstrate that ReEmpathy substantially improves empathy-sensitive spoken dialogue by enabling reflective reasoning, offering a promising approach toward more emotionally intelligent and empathy-aware human-computer interactions.

</details>


### [79] [U-Fold: Dynamic Intent-Aware Context Folding for User-Centric Agents](https://arxiv.org/abs/2601.18285)
*Jin Su,Runnan Fang,Yeqiu Li,Xiaobin Wang,Shihao Cai,Pengjun Xie,Ningyu Zhang,Fajie Yuan*

Main category: cs.CL

TL;DR: 本文提出了一种名为U-Fold的动态上下文折叠框架，解决了大语言模型在多轮用户对话中上下文长度限制和信息丢失的问题。


<details>
  <summary>Details</summary>
Motivation: 现有上下文折叠方法在多轮用户对话中易丢失细粒度约束和用户意图，导致后续决策错误，需要更灵活且适应性强的折叠机制。

Method: U-Fold保留完整对话和工具调用历史，在每轮对话中生成意图感知的动态摘要和紧凑的任务相关工具日志，有效管理长上下文。

Result: U-Fold在多个基准测试中，尤其是复杂的长上下文任务中表现优异，优于ReAct和其他折叠基线方法，赢率最高达71.4%。

Conclusion: U-Fold在长上下文、多轮任务中表现优异，显著超过现有方法，实现了更准确和全面的用户意图跟踪。

Abstract: Large language model (LLM)-based agents have been successfully deployed in many tool-augmented settings, but their scalability is fundamentally constrained by context length. Existing context-folding methods mitigate this issue by summarizing past interactions, yet they are typically designed for single-query or single-intent scenarios. In more realistic user-centric dialogues, we identify two major failure modes: (i) they irreversibly discard fine-grained constraints and intermediate facts that are crucial for later decisions, and (ii) their summaries fail to track evolving user intent, leading to omissions and erroneous actions. To address these limitations, we propose U-Fold, a dynamic context-folding framework tailored to user-centric tasks. U-Fold retains the full user--agent dialogue and tool-call history but, at each turn, uses two core components to produce an intent-aware, evolving dialogue summary and a compact, task-relevant tool log. Extensive experiments on $τ$-bench, $τ^2$-bench, VitaBench, and harder context-inflated settings show that U-Fold consistently outperforms ReAct (achieving a 71.4% win rate in long-context settings) and prior folding baselines (with improvements of up to 27.0%), particularly on long, noisy, multi-turn tasks. Our study demonstrates that U-Fold is a promising step toward transferring context-management techniques from single-query benchmarks to realistic user-centric applications.

</details>


### [80] [Temp-R1: A Unified Autonomous Agent for Complex Temporal KGQA via Reverse Curriculum Reinforcement Learning](https://arxiv.org/abs/2601.18296)
*Zhaoyan Gong,Zhiqiang Liu,Songze Li,Xiaoke Guo,Yuanxiang Liu,Xinle Deng,Zhizhen Liu,Lei Liang,Huajun Chen,Wen Zhang*

Main category: cs.CL

TL;DR: 提出Temp-R1，一种强化学习驱动的自主TKGQA智能体，显著提升复杂问题问答性能。


<details>
  <summary>Details</summary>
Motivation: 当前TKGQA方法依赖固定流程和封闭API，缺乏灵活性和可扩展性，且在处理多跳时间推理和复杂时间约束时面临挑战，急需自主、灵活且高效的推理智能体。

Method: 采用强化学习训练端到端TKGQA智能体，扩展内部与外部动作以缓解认知负担，使用逆向课程学习优先训练高难度问题，促进复杂推理能力培养。

Result: 本文提出了Temp-R1，一种基于强化学习训练的自主端到端时间知识图谱问答（TKGQA）智能体。通过扩展动作空间并引入逆向课程学习方法，提高了模型在复杂多跳动态事实和时间约束推理中的表现。该方法在MultiTQ和TimelineKGQA两个基准上取得了19.8%的性能提升。

Conclusion: Temp-R1通过扩展动作空间和逆向课程学习，实现了在时间知识图谱问答任务中的自主复杂推理，取得了领先性能，开创了时间推理自主智能体的新范式。

Abstract: Temporal Knowledge Graph Question Answering (TKGQA) is inherently challenging, as it requires sophisticated reasoning over dynamic facts with multi-hop dependencies and complex temporal constraints. Existing methods rely on fixed workflows and expensive closed-source APIs, limiting flexibility and scalability. We propose Temp-R1, the first autonomous end-to-end agent for TKGQA trained through reinforcement learning. To address cognitive overload in single-action reasoning, we expand the action space with specialized internal actions alongside external action. To prevent shortcut learning on simple questions, we introduce reverse curriculum learning that trains on difficult questions first, forcing the development of sophisticated reasoning before transferring to easier cases. Our 8B-parameter Temp-R1 achieves state-of-the-art performance on MultiTQ and TimelineKGQA, improving 19.8% over strong baselines on complex questions. Our work establishes a new paradigm for autonomous temporal reasoning agents. Our code will be publicly available soon at https://github.com/zjukg/Temp-R1.

</details>


### [81] [Suppressing Final Layer Hidden State Jumps in Transformer Pretraining](https://arxiv.org/abs/2601.18302)
*Keigo Shibata,Kazuki Yano,Ryosuke Takahashi,Jaesung Lee,Wataru Ikeda,Jun Suzuki*

Main category: cs.CL

TL;DR: 研究发现Transformer最后层隐藏状态角距离跳跃现象，提出跳跃抑制正则化方法改善模型表现，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 很多预训练Transformer模型在中间层的输入输出隐藏状态角距离变化很小，但在最后层有显著跳跃，可能反映模型内部不理想的行为。

Method: 提出了一个量化最后层跳跃强度的指标，并设计跳跃抑制正则项（JREG）用于在预训练阶段惩罚该跳跃，促进中间层能力更均衡利用。

Result: 在基于Llama的三个不同规模模型中应用JREG后，任务性能均有提升，且无需改变模型架构。

Conclusion: 通过抑制最后层的跳跃，模型内部表示更平滑，提升了模型性能，验证了该行为指标及正则化方法的有效性。

Abstract: This paper discusses the internal behavior of Transformer language models. Many recent pre-trained models have been reported to exhibit only slight changes in the angular distance between the input and output hidden state vectors in the middle Transformer layers, despite a disproportionately large ``jump'' in the angular distance occurring in or around the final Transformer layer. To characterize this, we first introduce a quantitative metric for the jump strength around the final layer, and then demonstrate its prevalence across many open-weight models, as well as its amplification throughout pre-training. Assuming such jumps indicate an undesirable property, we propose the jump-suppressing regularizer (JREG) which penalizes this jump during pre-training, thereby encouraging more balanced capability usage across the middle layers. Empirical evaluations of three model sizes of Llama-based models, trained with the proposed JREG method, reveal improved task performance compared to the baseline without altering the model architecture.

</details>


### [82] [Calibrating Beyond English: Language Diversity for Better Quantized Multilingual LLM](https://arxiv.org/abs/2601.18306)
*Everlyn Asiko Chimoto,Mostafa Elhoushi,Bruce A. Bassett*

Main category: cs.CL

TL;DR: 通过多语言校准集对多语言大模型量化进行优化，显著提升了困惑度，且语言匹配的校准集效果最佳，指出统一校准方法存在局限。


<details>
  <summary>Details</summary>
Motivation: 虽然量化技术能有效减少大型语言模型的存储和计算成本，但往往带来性能下降。现有的后训练量化方法多使用仅含英语的小型校准集，鲜有对多语言模型的影响进行深入研究。

Method: 系统性评估了八种校准设置（五种单语言和三种多语言混合）在两种量化器（GPTQ和AWQ）上的效果，使用了来自10种语言的数据进行测试。

Result: 非英语和多语言校准集显著改善了模型的困惑度表现，多语言混合集合在Llama3.1 8B和Qwen2.5 7B模型上分别实现了最高3.52点的困惑度下降。针对具体语言量身定制的校准集带来了最大提升。部分语言-量化器组合出现性能下降，原因在于不同语言的激活范围分布差异。

Conclusion: 静态统一的校准方法效果欠佳，针对不同语言定制多样化的校准数据对稳定量化多语言大型语言模型至关重要。

Abstract: Quantization is an effective technique for reducing the storage footprint and computational costs of Large Language Models (LLMs), but it often results in performance degradation. Existing post-training quantization methods typically use small, English-only calibration sets; however, their impact on multilingual models remains underexplored. We systematically evaluate eight calibration settings (five single-language and three multilingual mixes) on two quantizers (GPTQ, AWQ) on data from 10 languages. Our findings reveal a consistent trend: non-English and multilingual calibration sets significantly improve perplexity compared to English-only baselines. Specifically, we observe notable average perplexity gains across both quantizers on Llama3.1 8B and Qwen2.5 7B, with multilingual mixes achieving the largest overall reductions of up to 3.52 points in perplexity. Furthermore, our analysis indicates that tailoring calibration sets to the evaluation language yields the largest improvements for individual languages, underscoring the importance of linguistic alignment. We also identify specific failure cases where certain language-quantizer combinations degrade performance, which we trace to differences in activation range distributions across languages. These results highlight that static one-size-fits-all calibration is suboptimal and that tailoring calibration data, both in language and diversity, plays a crucial role in robustly quantizing multilingual LLMs.

</details>


### [83] [MultiVis-Agent: A Multi-Agent Framework with Logic Rules for Reliable and Comprehensive Cross-Modal Data Visualization](https://arxiv.org/abs/2601.18320)
*Jinwei Lu,Yuanfeng Song,Chen Zhang,Raymond Chi-Wing Wong*

Main category: cs.CL

TL;DR: 该文针对复杂多模态可视化任务，提出结合数学逻辑规则的多代理系统，大幅提升生成任务的可靠性与效果。


<details>
  <summary>Details</summary>
Motivation: 现有可视化系统仅支持单一模态输入与一次性生成，且存在可靠性问题，难以应对实际复杂、多模态和迭代精细化需求。

Method: 提出MultiVis-Agent多代理框架，利用数学逻辑约束引导大语言模型推理，实现多模态、多场景的可视化任务处理；构建四层逻辑规则体系与MultiVis-Bench基准测试。

Result: 在超过1000个多模态可视化任务测试中，MultiVis-Agent达成75.63%的可视化得分，任务完成率99.58%，代码执行成功率94.56%，明显优于无逻辑规则时的性能。

Conclusion: MultiVis-Agent通过引入四层数学逻辑规则框架，显著提升了多模态、多场景可视化生成的可靠性和灵活性，超越了现有基线方法。

Abstract: Real-world visualization tasks involve complex, multi-modal requirements that extend beyond simple text-to-chart generation, requiring reference images, code examples, and iterative refinement. Current systems exhibit fundamental limitations: single-modality input, one-shot generation, and rigid workflows. While LLM-based approaches show potential for these complex requirements, they introduce reliability challenges including catastrophic failures and infinite loop susceptibility. To address this gap, we propose MultiVis-Agent, a logic rule-enhanced multi-agent framework for reliable multi-modal and multi-scenario visualization generation. Our approach introduces a four-layer logic rule framework that provides mathematical guarantees for system reliability while maintaining flexibility. Unlike traditional rule-based systems, our logic rules are mathematical constraints that guide LLM reasoning rather than replacing it. We formalize the MultiVis task spanning four scenarios from basic generation to iterative refinement, and develop MultiVis-Bench, a benchmark with over 1,000 cases for multi-modal visualization evaluation. Extensive experiments demonstrate that our approach achieves 75.63% visualization score on challenging tasks, significantly outperforming baselines (57.54-62.79%), with task completion rates of 99.58% and code execution success rates of 94.56% (vs. 74.48% and 65.10% without logic rules), successfully addressing both complexity and reliability challenges in automated visualization generation.

</details>


### [84] [Overalignment in Frontier LLMs: An Empirical Study of Sycophantic Behaviour in Healthcare](https://arxiv.org/abs/2601.18334)
*Clément Christophe,Wadood Mohammed Abdul,Prateek Munjal,Tathagata Raha,Ronnie Rajan,Praveenkumar Kanithi*

Main category: cs.CL

TL;DR: 本文开发了一个基于医学MCQA的评估框架和调整谄媚评分指标，揭示了LLMs在临床应用中谄媚倾向的存在及其规模效应，并指出推理优化模型虽准确率高但不一定更可靠。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）越来越多地应用于临床工作流程，其为讨好用户而忽视事实准确性的倾向对患者安全构成风险，需要一种客观且可验证的评估方法。

Method: 提出了一个基于医学多项选择问答（MCQA）的评估框架和一种新指标“调整谄媚评分”，通过考虑模型不稳定性（混淆性）来衡量LLMs的谄媚倾向。

Result: 通过对Qwen-3和Llama-3系列模型的大规模分析，发现了模型抵抗谄媚倾向的规模效应轨迹。发现优化推理能力的“思考”模型虽然准确率高，但在权威压力下容易合理化错误用户建议，显示其脆弱性。

Conclusion: 模型的基准测试表现不能代表临床可靠性，简化的推理结构可能比复杂推理更能抵抗专家驱动的谄媚倾向，提高临床应用的安全性。

Abstract: As LLMs are increasingly integrated into clinical workflows, their tendency for sycophancy, prioritizing user agreement over factual accuracy, poses significant risks to patient safety. While existing evaluations often rely on subjective datasets, we introduce a robust framework grounded in medical MCQA with verifiable ground truths. We propose the Adjusted Sycophancy Score, a novel metric that isolates alignment bias by accounting for stochastic model instability, or "confusability". Through an extensive scaling analysis of the Qwen-3 and Llama-3 families, we identify a clear scaling trajectory for resilience. Furthermore, we reveal a counter-intuitive vulnerability in reasoning-optimized "Thinking" models: while they demonstrate high vanilla accuracy, their internal reasoning traces frequently rationalize incorrect user suggestions under authoritative pressure. Our results across frontier models suggest that benchmark performance is not a proxy for clinical reliability, and that simplified reasoning structures may offer superior robustness against expert-driven sycophancy.

</details>


### [85] [When Domain Pretraining Interferes with Instruction Alignment: An Empirical Study of Adapter Merging in Medical LLMs](https://arxiv.org/abs/2601.18350)
*Junyi Zou*

Main category: cs.CL

TL;DR: 本文通过两阶段LoRA训练和加权适配器合并提升大语言模型在医疗领域的精度和指令遵循能力，验证了方法的有效性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医疗术语精确性和安全指令执行方面存在困难，需要提升其医疗领域表现和指令遵循能力的平衡。

Method: 使用两阶段LoRA管道：1）领域自适应预训练（DAPT）注入广泛医疗知识，2）通过监督微调（SFT）使模型符合医疗问答指令。提出加权适配器合并方法将SFT和PT适配器线性合并，导出融合模型。

Result: 在医疗验证集上，融合模型取得BLEU-4=16.38，ROUGE-1=20.42，ROUGE-2=4.60，ROUGE-L=11.54，表现稳定且解码敏感性良好。

Conclusion: 采用领域自适应预训练与监督微调结合，并通过加权适配器合并，有效提升了大语言模型在医疗安全关键任务中的表现和稳定性。

Abstract: Large language models (LLMs) show strong general capability but often struggle with medical terminology precision and safety-critical instruction following. We present a case study for adapter interference in safety-critical domains using a 14B-parameter base model through a two-stage LoRA pipeline: (1) domain-adaptive pre-training (PT) to inject broad medical knowledge via continued pre-training (DAPT), and (2) supervised fine-tuning (SFT) to align the model with medical question-answering behaviors through instruction-style data. To balance instruction-following ability and domain knowledge retention, we propose Weighted Adapter Merging, linearly combining SFT and PT adapters before exporting a merged base-model checkpoint. On a held-out medical validation set (F5/F6), the merged model achieves BLEU-4 = 16.38, ROUGE-1 = 20.42, ROUGE-2 = 4.60, and ROUGE-L = 11.54 under a practical decoding configuration. We further analyze decoding sensitivity and training stability with loss curves and controlled decoding comparisons.

</details>


### [86] [Code over Words: Overcoming Semantic Inertia via Code-Grounded Reasoning](https://arxiv.org/abs/2601.18352)
*Manjie Xu,Isabella Yin,Xinyi Tu,Chi Zhang,Yixin Zhu*

Main category: cs.CL

TL;DR: 本文通过将动态规则表示为可执行代码，解决了大语言模型在抑制预训练先验时出现的语义惯性问题，提出的Code-Grounded Vistas方法提升了模型推理的效率和准确率，颠覆了大模型总优的传统看法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型(LLMs)在面临动态且上下文依赖的规则与其预训练先验知识（如‘熔岩是危险的’）相矛盾时，表现出语义惯性的困难，即无法抑制预训练先验。本文旨在研究这一现象并改善模型在需要抑制预训练关联的自然语言推理任务中的表现。

Method: 使用可变物理法则的游戏Baba Is You作为测试平台，精确评估模型在规则变更时覆盖预训练先验的能力。提出将动态规则表示为可执行代码而非描述性文本，通过Fine-tune模型处理反事实对并识别含矛盾规则的状态，强调逻辑约束而非视觉语义，提出Code-Grounded Vistas (LCV)方法，以训练时间的方法提升模型表现。

Result: 发现较大模型在需要抑制预训练关联时表现反而更差，体现出逆向规模效应。基于可执行代码的表示能够有效抑制先验，相较推理时的昂贵搜索方法，LCV方法在效率和准确率上均优越。

Conclusion: 模型的表示方式决定了规模扩大是否能提升上下文推理能力。该研究挑战了“大模型总是更优”的假设，对需要动态覆盖学习先验的应用领域具有深远影响。

Abstract: LLMs struggle with Semantic Inertia: the inability to inhibit pre-trained priors (e.g., "Lava is Dangerous") when dynamic, in-context rules contradict them. We probe this phenomenon using Baba Is You, where physical laws are mutable text rules, enabling precise evaluation of models' ability to override learned priors when rules change. We quantatively observe that larger models can exhibit inverse scaling: they perform worse than smaller models when natural language reasoning requires suppressing pre-trained associations (e.g., accepting "Lava is Safe"). Our analysis attributes this to natural language encoding, which entangles descriptive semantics and logical rules, leading to persistent hallucinations of familiar physics despite explicit contradictory rules. Here we show that representing dynamics as executable code, rather than descriptive text, reverses this trend and enables effective prior inhibition. We introduce Code-Grounded Vistas (LCV), which fine-tunes models on counterfactual pairs and identifies states with contradictory rules, thereby forcing attention to logical constraints rather than visual semantics. This training-time approach outperforms expensive inference-time search methods in both efficiency and accuracy. Our results demonstrate that representation fundamentally determines whether scaling improves or impairs contextual reasoning. This challenges the assumption that larger models are universally better, with implications for domains that require dynamic overriding of learned priors.

</details>


### [87] [CitiLink: Enhancing Municipal Transparency and Citizen Engagement through Searchable Meeting Minutes](https://arxiv.org/abs/2601.18374)
*Rodrigo Silva,José Evans,José Isidro,Miguel Marques,Afonso Fonseca,Ricardo Morais,João Canavilhas,Arian Pasquali,Purificação Silvano,Alípio Jorge,Nuno Guimarães,Sérgio Nunes,Ricardo Campos*

Main category: cs.CL

TL;DR: CitiLink系统利用大语言模型和信息检索技术，将市政会议纪要转化为结构化、可搜索的数据，提升了地方政府信息的透明度和查询效率。


<details>
  <summary>Details</summary>
Motivation: 市政会议纪要文本冗长且结构复杂，尽管公开但难以高效检索，CitiLink旨在通过NLP与信息检索技术提升市政文档的可用性和透明度。

Method: 采用大语言模型提取会议纪要中的元数据、主题和投票结果，并结合BM25排序算法及多维过滤技术，实现结构化数据的索引和用户友好界面搜索。

Result: CitiLink平台通过使用大语言模型和信息检索技术，将结构化的市政会议纪要数据转化为可搜索的形式，提高了地方政府信息的可访问性和透明度。系统利用LLM提取元数据、讨论主题和投票结果，并通过BM25排序和多维过滤支持全文检索。基于120份葡萄牙六个市政文件的样本进行构建和测试，结合用户导向测试和性能评估，展示了系统在实际使用中的有效性和可用性。

Conclusion: CitiLink成功提升了市政会议纪要的可访问性和查询效率，验证了LLM和信息检索结合应用于公共文档处理的有效性，且在实际用户测试中表现良好。

Abstract: City council minutes are typically lengthy and formal documents with a bureaucratic writing style. Although publicly available, their structure often makes it difficult for citizens or journalists to efficiently find information. In this demo, we present CitiLink, a platform designed to transform unstructured municipal meeting minutes into structured and searchable data, demonstrating how NLP and IR can enhance the accessibility and transparency of local government. The system employs LLMs to extract metadata, discussed subjects, and voting outcomes, which are then indexed in a database to support full-text search with BM25 ranking and faceted filtering through a user-friendly interface. The developed system was built over a collection of 120 minutes made available by six Portuguese municipalities. To assess its usability, CitiLink was tested through guided sessions with municipal personnel, providing insights into how real users interact with the system. In addition, we evaluated Gemini's performance in extracting relevant information from the minutes, highlighting its effectiveness in data extraction.

</details>


### [88] [Hierarchical Text Classification with LLM-Refined Taxonomies](https://arxiv.org/abs/2601.18375)
*Jonas Golde,Nicolaas Jedema,Ravi Krishnan,Phong Le*

Main category: cs.CL

TL;DR: 本文提出利用大型语言模型自动重构分类体系，解决HTC中歧义问题，实现了优于人工设计分类体系的分类性能。


<details>
  <summary>Details</summary>
Motivation: 现实中的分类体系常存在歧义，如相似父节点下的相同叶节点名，导致语言模型难以学到清晰的决策边界，需要改进分类体系以提升HTC效果。

Method: 提出TaxMorph框架，利用LLMs对整个分类体系进行结构性修改，包括重命名、合并、拆分和重新排序，优化体系结构以更好地匹配模型的语义理解。

Result: 在三个HTC基准测试中，LLM优化后的分类体系在各种设置下的F1评分最高提升2.9个百分点，且优化后的分类体系更符合模型的混淆模式，提升分类效果。

Conclusion: 通过使用大型语言模型（LLMs）对分层文本分类（HTC）中的完整分类体系结构进行重命名、合并、拆分和重新排序等操作，TaxMorph框架能够生成更符合模型语义编码的分类体系，显著提升了HTC性能。

Abstract: Hierarchical text classification (HTC) depends on taxonomies that organize labels into structured hierarchies. However, many real-world taxonomies introduce ambiguities, such as identical leaf names under similar parent nodes, which prevent language models (LMs) from learning clear decision boundaries. In this paper, we present TaxMorph, a framework that uses large language models (LLMs) to transform entire taxonomies through operations such as renaming, merging, splitting, and reordering. Unlike prior work, our method revises the full hierarchy to better match the semantics encoded by LMs. Experiments across three HTC benchmarks show that LLM-refined taxonomies consistently outperform human-curated ones in various settings up to +2.9pp. in F1. To better understand these improvements, we compare how well LMs can assign leaf nodes to parent nodes and vice versa across human-curated and LLM-refined taxonomies. We find that human-curated taxonomies lead to more easily separable clusters in embedding space. However, the LLM-refined taxonomies align more closely with the model's actual confusion patterns during classification. In other words, even though they are harder to separate, they better reflect the model's inductive biases. These findings suggest that LLM-guided refinement creates taxonomies that are more compatible with how models learn, improving HTC performance.

</details>


### [89] [Corpus-Based Approaches to Igbo Diacritic Restoration](https://arxiv.org/abs/2601.18380)
*Ignatius Ezeani*

Main category: cs.CL

TL;DR: 本论文针对低资源的伊博语，提出并评估了基于n-gram、分类和嵌入的重音符号恢复方法，开发了数据生成框架，推动了低资源语言NLP进展。


<details>
  <summary>Details</summary>
Motivation: 当前NLP研究多集中于资源丰富语言，而超过95%的语言资源匮乏，亟需为低资源语言如伊博语开发有效的处理工具，尤其是在重音符号恢复方面。

Method: 提出了三种主要方法：标准n-gram模型利用目标词前的词序列进行预测；分类模型使用目标词两侧的词作为特征窗口；嵌入模型比较上下文词嵌入与候选变体向量的相似度得分。

Result: 开发了适用于伊博语的灵活数据集生成框架，验证了三种方法在重音符号恢复任务中的有效性，推进了低资源语言NLP研究。

Conclusion: 本论文通过构建灵活的数据集生成框架及提出三种方法（n-gram模型、分类模型和嵌入模型），有效解决了低资源语言伊博语的重音符号歧义问题。

Abstract: With natural language processing (NLP), researchers aim to enable computers to identify and understand patterns in human languages. This is often difficult because a language embeds many dynamic and varied properties in its syntax, pragmatics and phonology, which need to be captured and processed. The capacity of computers to process natural languages is increasing because NLP researchers are pushing its boundaries. But these research works focus more on well-resourced languages such as English, Japanese, German, French, Russian, Mandarin Chinese, etc. Over 95% of the world's 7000 languages are low-resourced for NLP, i.e. they have little or no data, tools, and techniques for NLP work.
  In this thesis, we present an overview of diacritic ambiguity and a review of previous diacritic disambiguation approaches on other languages. Focusing on the Igbo language, we report the steps taken to develop a flexible framework for generating datasets for diacritic restoration. Three main approaches, the standard n-gram model, the classification models and the embedding models were proposed. The standard n-gram models use a sequence of previous words to the target stripped word as key predictors of the correct variants. For the classification models, a window of words on both sides of the target stripped word was used. The embedding models compare the similarity scores of the combined context word embeddings and the embeddings of each of the candidate variant vectors.

</details>


### [90] [Do not be greedy, Think Twice: Sampling and Selection for Document-level Information Extraction](https://arxiv.org/abs/2601.18395)
*Mikel Zubillaga,Oscar Sainz,Oier Lopez de Lacalle,Eneko Agirre*

Main category: cs.CL

TL;DR: 通过采样生成多模板并选择最佳，ThinkTwice框架提升了文档级信息抽取性能，优于贪心解码和现有方法。


<details>
  <summary>Details</summary>
Motivation: 为了克服贪心解码导致的输出多样性受限，利用采样生成多样候选并选择最优，有望提升DocIE任务的表现。

Method: 本文提出ThinkTwice框架，先采样生成多个候选模板，再通过无监督的结果一致性或有监督的奖励模型选择最优输出；同时利用拒绝采样生成银标注推理轨迹以辅助训练。

Result: 本文针对文档级信息抽取（DocIE）任务，提出了ThinkTwice框架，通过采样生成多个输出模板，再通过选择模块选出最合适的结果，显著优于传统的贪心解码方法。文中设计了无监督和有监督两种选择方法，并采用基于拒绝采样的方法生成银标注推理轨迹，解决了推理轨迹标注不足的问题。实验验证了该方法的有效性，超越了现有最先进的基线。

Conclusion: ThinkTwice通过多样化生成和选择机制有效提升了DocIE性能，且无监督和有监督策略均表现优异，解决了推理轨迹不足问题。

Abstract: Document-level Information Extraction (DocIE) aims to produce an output template with the entities and relations of interest occurring in the given document. Standard practices include prompting decoder-only LLMs using greedy decoding to avoid output variability. Rather than treating this variability as a limitation, we show that sampling can produce substantially better solutions than greedy decoding, especially when using reasoning models. We thus propose ThinkTwice, a sampling and selection framework in which the LLM generates multiple candidate templates for a given document, and a selection module chooses the most suitable one. We introduce both an unsupervised method that exploits agreement across generated outputs, and a supervised selection method using reward models trained on labeled DocIE data. To address the scarcity of golden reasoning trajectories for DocIE, we propose a rejection-sampling-based method to generate silver training data that pairs output templates with reasoning traces. Our experiments show the validity of unsupervised and supervised ThinkTwice, consistently outperforming greedy baselines and the state-of-the-art.

</details>


### [91] [Pisets: A Robust Speech Recognition System for Lectures and Interviews](https://arxiv.org/abs/2601.18415)
*Ivan Bondarenko,Daniil Grebenkin,Oleg Sedukhin,Mikhail Klementev,Roman Derunets,Lyudmila Budneva*

Main category: cs.CL

TL;DR: 提出了一个基于Wav2Vec2、AST和Whisper的三组件俄语语音转文本系统Pisets，结合课程学习和不确定性建模，显著提升识别准确率和鲁棒性，源码公开。


<details>
  <summary>Details</summary>
Motivation: 当前Whisper模型在长音频和复杂声学环境下存在识别错误和幻觉问题，针对科学家和记者的需求，设计一个准确、鲁棒的俄语语音转文本系统以提升实用性。

Method: 采用三组件架构：初识别用Wav2Vec2，假阳性过滤用音频频谱转换器（AST），最终识别用Whisper。利用课程学习方法结合多样俄语语料进行训练，并引入先进不确定性建模技术提升转录质量。

Result: 该论文提出了一个名为"Pisets"的语音转文本系统，专为科学家和记者设计，旨在提高语音识别准确率并减少Whisper模型常见的错误和幻觉现象。系统采用三部分架构：基于Wav2Vec2的初始识别，利用音频频谱转换器（AST）进行假阳性过滤，最终通过Whisper进行精细识别。通过课程学习方法和多样化的俄语语音语料库训练，系统性能显著提升。同时引入了先进的不确定性建模技术，进一步提高转录质量。与WhisperX和常规Whisper模型相比，Pisets在不同声学环境下处理长音频表现更为稳健。源码已公开于GitHub。

Conclusion: Pisets系统通过三组件架构和先进技术显著提升了俄语语音转文本的准确性和鲁棒性，在多种声学条件下表现优于现有Whisper模型及其变种。

Abstract: This work presents a speech-to-text system "Pisets" for scientists and journalists which is based on a three-component architecture aimed at improving speech recognition accuracy while minimizing errors and hallucinations associated with the Whisper model. The architecture comprises primary recognition using Wav2Vec2, false positive filtering via the Audio Spectrogram Transformer (AST), and final speech recognition through Whisper. The implementation of curriculum learning methods and the utilization of diverse Russian-language speech corpora significantly enhanced the system's effectiveness. Additionally, advanced uncertainty modeling techniques were introduced, contributing to further improvements in transcription quality. The proposed approaches ensure robust transcribing of long audio data across various acoustic conditions compared to WhisperX and the usual Whisper model. The source code of "Pisets" system is publicly available at GitHub: https://github.com/bond005/pisets.

</details>


### [92] [Latent Knowledge as a Predictor of Fact Acquisition in Fine-Tuned Large Language Models](https://arxiv.org/abs/2601.18468)
*Daniel B. Hier,Tayo Obafemi-Ajayi*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在生物医学事实学习中的潜在知识和学习表现。通过对Llama 3.1 8B进行微调，探索了模型对本体术语映射的学习和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在预训练之后对生物医学事实的记忆能力不均匀，研究如何加速模型学习新知识并提升泛化能力具有重要意义。

Method: 使用Llama 3.1 8B微调学习HPO和GO本体术语映射，利用随机解码检测潜在知识，采用Cox比例风险模型分析事实获取、泛化和退化的预测因素。

Result: 微调后HPO的确定性回忆率从2.8%提升至71.9%；潜在知识加速事实学习速度（HR 2.6），提高了学习效率和收敛速度；未见GO事实泛化率仅为5.8%，但潜在知识存在时泛化概率更高；训练中的事实强化能减少已学事实的退化。

Conclusion: 潜在知识是加速事实学习和提高学习效率的关键，虽然模型对未见过的本体事实泛化能力有限，但强化学习能降低已学事实的退化。

Abstract: Large language models store biomedical facts with uneven strength after pretraining: some facts are present in the weights but are not reliably accessible under deterministic decoding (latent knowledge), while others are scarcely represented. We fine tuned Llama 3.1 8B Instruct to learn ontology term identifier mappings from the Human Phenotype Ontology (800 pairs) and the Gene Ontology (400 training pairs), withholding 400 GO pairs to test generalization. Treating learning as a time to event process across 20 epochs, we used stochastic decoding to detect latent knowledge at baseline and Cox proportional hazards models to identify predictors of acquisition, generalization, and degradation. Baseline deterministic recall for HPO was 2.8%, rising to 71.9% after fine-tuning. Latent knowledge was the strongest predictor of faster fact acquisition (HR 2.6) and was associated with earlier, higher peak learning rates and faster convergence; identifier frequency and curated annotation counts had smaller effects. Generalization to withheld GO facts was uncommon (5.8%) but more likely when latent knowledge was present. Previously correct GO mappings degraded more often for withheld (unseen) terms than for trained (seen) terms, suggesting a protective effect of reinforcement during training. These results show that latent knowledge predicts both the speed of factual learning during fine-tuning and the limited generalization of unseen ontology facts, while resistance to degradation depends on whether facts are reinforced.

</details>


### [93] [Funny or Persuasive, but Not Both: Evaluating Fine-Grained Multi-Concept Control in LLMs](https://arxiv.org/abs/2601.18483)
*Arya Labroo,Ivaxi Sheth,Vyas Raina,Amaani Ahmed,Mario Fritz*

Main category: cs.CL

TL;DR: 本论文提出细粒度多概念文本控制的评估框架，发现现有大语言模型在双概念控制上存在性能下降，暴露出提示控制方法对复合概念处理的根本限制。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在生成文本时有强大能力，但在需要对特定文本概念（如幽默性、说服力、正式性）进行细粒度控制时存在困难，尤其是多概念控制缺乏系统评估。

Method: 提出一个评估框架，用于评估单一和双重文本概念的细粒度控制能力，重点考察语言学上不同的概念对，如说服力与幽默性。

Result: 在多个大语言模型和生成任务中发现，双概念控制的表现常常下降，尽管这些概念原则上可以区分，这显示出基于提示的控制方法对概念组合能力存在根本限制。

Conclusion: 当前提示控制方法难以有效实现多概念的复合控制，即使这些概念直观上独立，模型也难以同时精细控制。提出的评估框架能为未来多概念控制方法的能力测量提供系统性证据和原则性方法。

Abstract: Large Language Models (LLMs) offer strong generative capabilities, but many applications require explicit and \textit{fine-grained} control over specific textual concepts, such as humor, persuasiveness, or formality. Prior approaches in prompting and representation engineering can provide coarse or single-attribute control, but systematic evaluation of multi-attribute settings remains limited. We introduce an evaluation framework for fine-grained controllability for both single- and dual-concept scenarios, focusing on linguistically distinct concept pairs (e.g., persuasiveness vs.~humor). Surprisingly, across multiple LLMs and generative tasks, we find that performance often drops in the dual-concept setting, even though the chosen concepts should in principle be separable. This reveals a fundamental limitation of naive prompting-based control: models struggle with compositionality even when concepts are intuitively independent. Our framework provides systematic evidence of this gap and offers a principled approach for measuring the ability of future methods for multi-concept control.

</details>


### [94] [Demographic Probing of Large Language Models Lacks Construct Validity](https://arxiv.org/abs/2601.18486)
*Manuel Tonneau,Neil K. R. Seghal,Niyati Malhotra,Victor Orozco-Olvera,Ana María Muñoz Boudet,Lakshmi Subramanian,Sharath Chandra Guntuku,Valentin Hofmann*

Main category: cs.CL

TL;DR: 单一群体线索无法稳定反映大型语言模型中的人口统计行为差异，需多线索和控制混杂因素以增强研究有效性。


<details>
  <summary>Details</summary>
Motivation: 检验大型语言模型在使用单个种族或性别线索作为信号时，是否能够一致地反映同一人口统计属性的行为变化，即构念效度的假设。

Method: 在美国背景下，分析大型语言模型在寻求建议交互中的表现，比较不同种族和性别线索对模型行为的影响，并考察线索编码人口属性的强度及语言混杂因素的作用。

Result: 发现不同线索即便代表相同的人口群体，诱发的模型行为变化仅部分重叠；同一线索内不同群体的区分度较弱且不均，导致估计的差异不稳定，变化显著。部分不一致源于线索对人口属性编码的强度差异和语言混杂因素的影响。

Conclusion: 人口统计探测方法缺乏构念效度，无法提供统一且稳定的关于模型如何基于人口统计信息调整行为的描述，可能因构念定义不准确或片面。建议采用多重生态有效的线索并明确控制混杂因素，以提升对人口影响效应的可信度。

Abstract: Demographic probing is widely used to study how large language models (LLMs) adapt their behavior to signaled demographic attributes. This approach typically uses a single demographic cue in isolation (e.g., a name or dialect) as a signal for group membership, implicitly assuming strong construct validity: that such cues are interchangeable operationalizations of the same underlying, demographically conditioned behavior. We test this assumption in realistic advice-seeking interactions, focusing on race and gender in a U.S. context. We find that cues intended to represent the same demographic group induce only partially overlapping changes in model behavior, while differentiation between groups within a given cue is weak and uneven. Consequently, estimated disparities are unstable, with both magnitude and direction varying across cues. We further show that these inconsistencies partly arise from variation in how strongly cues encode demographic attributes and from linguistic confounders that independently shape model behavior. Together, our findings suggest that demographic probing lacks construct validity: it does not yield a single, stable characterization of how LLMs condition on demographic information, which may reflect a misspecified or fragmented construct. We conclude by recommending the use of multiple, ecologically valid cues and explicit control of confounders to support more defensible claims about demographic effects in LLMs.

</details>


### [95] [Using Large Language Models to Construct Virtual Top Managers: A Method for Organizational Research](https://arxiv.org/abs/2601.18512)
*Antonio Garzon-Vico,Krithika Sharon Komalapati,Arsalan Shahid,Jan Rosier*

Main category: cs.CL

TL;DR: 本文提出了一种基于大型语言模型（LLM）的方法框架，用以创建真实高管的虚拟人格，通过CEO通信和道德基础理论模拟领导者决策，并通过三阶段验证其有效性，结果表明该方法能准确复现人类道德判断。


<details>
  <summary>Details</summary>
Motivation: 由于直接访问高管受限，亟需借助新技术创建可信的虚拟领导者人格来辅助组织研究，改进领导决策模拟和行为预测。

Method: 利用大型语言模型结合真实CEO沟通内容和道德基础理论，构建虚拟CEO人格；通过三阶段验证（构念有效性、可靠性和行为保真度）比较虚拟人格与真实高管行为表现。

Result: 验证结果显示，基于理论构建的虚拟高管人格在道德判断方面与真实人类样本高度吻合，证明其具有较高的可信度和补充研究价值。

Conclusion: 基于理论构建的LLM虚拟高管人格能有效模拟人类道德判断，具备作为组织研究替代工具的潜力，对未来组织研究具有重要启示。

Abstract: This study introduces a methodological framework that uses large language models to create virtual personas of real top managers. Drawing on real CEO communications and Moral Foundations Theory, we construct LLM-based participants that simulate the decision-making of individual leaders. Across three phases, we assess construct validity, reliability, and behavioral fidelity by benchmarking these virtual CEOs against human participants. Our results indicate that theoretically scaffolded personas approximate the moral judgements observed in human samples, suggesting that LLM-based personas can serve as credible and complementary tools for organizational research in contexts where direct access to executives is limited. We conclude by outlining implications for future research using LLM-based personas in organizational settings.

</details>


### [96] [GenAI for Social Work Field Education: Client Simulation with Real-Time Feedback](https://arxiv.org/abs/2601.18517)
*James Sungarda,Hongkai Liu,Zilong Zhou,Tien-Hsuan Wu,Johnson Chun-Sing Cheung,Ben Kao*

Main category: cs.CL

TL;DR: 本文提出了一个结合真实客户模拟和智能技能识别的社工培训聊天机器人SWITCH，提升了培训反馈的时效性和质量，助力导师专注高级指导。


<details>
  <summary>Details</summary>
Motivation: 社工领域教育中及时、客观反馈受限于导师和客户资源，亟需一种辅助培训工具以提升培训效率与质量。

Method: 采用认知基础的客户角色模型结合动态行为模拟，通过BERT多标签分类器和基于上下文检索的学习方法实时识别咨询技能，并利用动机访谈控制器调节训练进展阶段。

Result: 实验结果显示，BERT模型和上下文学习均显著优于基线方法，证明了技能分类的有效性和训练系统的可行性。

Conclusion: SWITCH系统通过集成客户模拟、实时技能分类和动机访谈进展管理，为社工培训提供了一种可扩展且低成本的反馈机制，弥补了现场教育中导师资源有限的问题。

Abstract: Field education is the signature pedagogy of social work, yet providing timely and objective feedback during training is constrained by the availability of instructors and counseling clients. In this paper, we present SWITCH, the Social Work Interactive Training Chatbot. SWITCH integrates realistic client simulation, real-time counseling skill classification, and a Motivational Interviewing (MI) progression system into the training workflow. To model a client, SWITCH uses a cognitively grounded profile comprising static fields (e.g., background, beliefs) and dynamic fields (e.g., emotions, automatic thoughts, openness), allowing the agent's behavior to evolve throughout a session realistically. The skill classification module identifies the counseling skills from the user utterances, and feeds the result to the MI controller that regulates the MI stage transitions. To enhance classification accuracy, we study in-context learning with retrieval over annotated transcripts, and a fine-tuned BERT multi-label classifier. In the experiments, we demonstrated that both BERT-based approach and in-context learning outperforms the baseline with big margin. SWITCH thereby offers a scalable, low-cost, and consistent training workflow that complements field education, and allows supervisors to focus on higher-level mentorship.

</details>


### [97] [Exploring Fine-Tuning for In-Context Retrieval and Efficient KV-Caching in Long-Context Language Models](https://arxiv.org/abs/2601.18527)
*Francesco Maria Molfese,Momchil Hardalov,Rexhina Blloshmi,Bill Byrne,Adrià de Gispert*

Main category: cs.CL

TL;DR: 本工作研究了微调策略对长上下文语言模型性能和压缩鲁棒性的影响，取得了显著域内提升及任务依赖的跨域表现改进。


<details>
  <summary>Details</summary>
Motivation: 探究微调策略是否能够提升长上下文语言模型的性能及其在KV-cache压缩条件下的鲁棒性。

Method: 通过实验比较不同的微调策略对LCLMs识别和利用相关信息能力的提升效果，并测试在KV-cache压缩技术下模型的鲁棒性。

Result: 微调使模型在域内性能提升最高达20点，跨领域表现依任务而异，其中金融领域表现优异(+9点)，而多选题由RAG模型表现更好(+6点)；微调策略在KV-cache压缩下带来适度鲁棒性提升。

Conclusion: 微调策略可以显著提升长上下文语言模型（LCLMs）在领域内的性能，但跨领域泛化效果不一，且针对不同任务存在较大差异。

Abstract: With context windows of millions of tokens, Long-Context Language Models (LCLMs) can encode entire document collections, offering a strong alternative to conventional retrieval-augmented generation (RAG). However, it remains unclear whether fine-tuning strategies can improve long-context performance and translate to greater robustness under KV-cache compression techniques. In this work, we investigate which training strategies most effectively enhance LCLMs' ability to identify and use relevant information, as well as enhancing their robustness under KV-cache compression. Our experiments show substantial in-domain improvements, achieving gains of up to +20 points over the base model. However, out-of-domain generalization remains task dependent with large variance -- LCLMs excels on finance questions (+9 points), while RAG shows stronger performance on multiple-choice questions (+6 points) over the baseline models. Finally, we show that our fine-tuning approaches bring moderate improvements in robustness under KV-cache compression, with gains varying across tasks.

</details>


### [98] [From Verifiable Dot to Reward Chain: Harnessing Verifiable Reference-based Rewards for Reinforcement Learning of Open-ended Generation](https://arxiv.org/abs/2601.18533)
*Yuxin Jiang,Yufei Wang,Qiyuan Zhang,Xingshan Zeng,Liangyou Li,Jierun Chen,Chaofan Tao,Haoli Bai,Lifeng Shang*

Main category: cs.CL

TL;DR: 提出RLVRR方法，通过参考奖励解决开放式生成中无明确真值问题，结合强化学习和监督学习优势，提升训练效率与生成质量。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习可验证奖励方法在开启式生成任务中因缺乏明确的真实标签，导致训练效率低且易出现奖励欺骗，迫切需要一种能够利用高质量参考信息，提高训练效率和结果可靠性的强化学习方法。

Method: RLVRR方法通过从高质量参考中提取奖励链，将奖励分为内容（保留关键词等核心概念）和风格（利用大语言模型验证风格一致性）两部分，实现对生成结果的多维度评价，并结合强化学习与监督微调的优势进行训练。

Result: 本文提出了一种新颖的基于可验证参考奖励的强化学习方法RLVRR，解决了传统可验证奖励强化学习（RLVR）在开放式生成任务中因缺乏明确真实标签导致的低效和奖励欺骗问题。RLVRR通过从高质量参考中提取有序语言信号，分解奖励为内容和风格两部分，实现了结合强化学习探索能力和监督微调效率的训练方式。大量基于Qwen和Llama模型的实验证明RLVRR在多个任务上优于传统监督学习和复杂奖励模型，能够统一结构化推理与开放式生成训练，提升泛化能力和生成多样性。该方法为通用大模型对齐提供了高效且可验证的新路径。

Conclusion: RLVRR方法有效提升了开放式生成任务中的训练效率和结果质量，优于大规模监督学习和复杂奖励模型，且兼顾推理与生成，具备良好泛化与多样性表现，推动了大模型对齐技术发展。

Abstract: Reinforcement learning with verifiable rewards (RLVR) succeeds in reasoning tasks (e.g., math and code) by checking the final verifiable answer (i.e., a verifiable dot signal). However, extending this paradigm to open-ended generation is challenging because there is no unambiguous ground truth. Relying on single-dot supervision often leads to inefficiency and reward hacking. To address these issues, we propose reinforcement learning with verifiable reference-based rewards (RLVRR). Instead of checking the final answer, RLVRR extracts an ordered linguistic signal from high-quality references (i.e, reward chain). Specifically, RLVRR decomposes rewards into two dimensions: content, which preserves deterministic core concepts (e.g., keywords), and style, which evaluates adherence to stylistic properties through LLM-based verification. In this way, RLVRR combines the exploratory strength of RL with the efficiency and reliability of supervised fine-tuning (SFT). Extensive experiments on more than 10 benchmarks with Qwen and Llama models confirm the advantages of our approach. RLVRR (1) substantially outperforms SFT trained with ten times more data and advanced reward models, (2) unifies the training of structured reasoning and open-ended generation, and (3) generalizes more effectively while preserving output diversity. These results establish RLVRR as a principled and efficient path toward verifiable reinforcement learning for general-purpose LLM alignment. We release our code and data at https://github.com/YJiangcm/RLVRR.

</details>


### [99] [Evaluating Morphological Plausibility of Subword Tokenization via Statistical Alignment with Morpho-Syntactic Features](https://arxiv.org/abs/2601.18536)
*Abishek Stephen,Jindřich Libovický*

Main category: cs.CL

TL;DR: 提出了一种利用形态-句法特征评估子词切分形态合理性的新指标，避免了对高质量人工标注数据的依赖，适用于更多语言。


<details>
  <summary>Details</summary>
Motivation: 传统的基于形态边界或检索F分数的评价指标依赖高质量金标准数据，但这类数据在许多语言中缺乏或质量不一致，因此需要一种无需金标准数据且适用范围更广的指标。

Method: 通过IBM模型1对子词与形态-句法特征进行概率对齐，利用Universal Dependencies或UniMorph等资源实现。

Result: 实验表明该指标与传统形态边界召回率相关，同时适用于不同形态系统的多种语言，表现出良好的泛化能力。

Conclusion: 该指标在不同形态系统的语言中表现良好，与传统的形态边界召回率具有较好相关性，具有更广泛的适用性。

Abstract: We present a novel metric for the evaluation of the morphological plausibility of subword segmentation. Unlike the typically used morpheme boundary or retrieval F-score, which requires gold segmentation data that is either unavailable or of inconsistent quality across many languages, our approach utilizes morpho-syntactic features. These are available in resources such as Universal Dependencies or UniMorph for a much wider range of languages. The metric works by probabilistically aligning subwords with morphological features through an IBM Model 1. Our experiments show that the metric correlates well with traditional morpheme boundary recall while being more broadly applicable across languages with different morphological systems.

</details>


### [100] [Unknown Unknowns: Why Hidden Intentions in LLMs Evade Detection](https://arxiv.org/abs/2601.18552)
*Devansh Srivastav,David Pape,Lea Schönherr*

Main category: cs.CL

TL;DR: 本文系统分析了LLMs中隐藏意图的分类、引发机制及检测失败问题，强调现实环境中检测的困难，提出了基于社会科学的分类框架，并通过实验演示其风险和检测挑战，呼吁建立健全的治理机制。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在日常决策中应用广泛，但其输出中存在难以检测的隐秘、有目的的行为，这些行为可能扭曲用户信念和行为，且可能源于训练过程中的瑕疵或恶意设计。

Method: 构建基于社会科学的隐藏意图分类体系，在受控模型中诱发隐藏意图行为，应用推理和非推理LLM评审者系统评估检测方法，通过压力测试分析精度与误报、漏报的权衡，并通过案例研究验证分类体系在现实模型中的适用性。

Result: 研究提出了一个基于社会科学的十类隐藏意图分类法，展示了在受控模型中易于诱发隐藏意图，系统评估了检测方法，并发现现实开放环境中检测失败，尤其是在低发生率条件下；通过精度-发生率以及精度-假阴性率权衡的压力测试揭示审计失败的原因；实证分析表明所有十类隐藏意图在最先进的LLMs中均有体现。

Conclusion: 当前检测方法难以有效识别开放环境下的隐藏意图，存在高误报和漏报问题，亟需更加鲁棒的检测框架和治理体系以防止潜在滥用行为。

Abstract: LLMs are increasingly embedded in everyday decision-making, yet their outputs can encode subtle, unintended behaviours that shape user beliefs and actions. We refer to these covert, goal-directed behaviours as hidden intentions, which may arise from training and optimisation artefacts, or be deliberately induced by an adversarial developer, yet remain difficult to detect in practice. We introduce a taxonomy of ten categories of hidden intentions, grounded in social science research and organised by intent, mechanism, context, and impact, shifting attention from surface-level behaviours to design-level strategies of influence. We show how hidden intentions can be easily induced in controlled models, providing both testbeds for evaluation and demonstrations of potential misuse. We systematically assess detection methods, including reasoning and non-reasoning LLM judges, and find that detection collapses in realistic open-world settings, particularly under low-prevalence conditions, where false positives overwhelm precision and false negatives conceal true risks. Stress tests on precision-prevalence and precision-FNR trade-offs reveal why auditing fails without vanishingly small false positive rates or strong priors on manipulation types. Finally, a qualitative case study shows that all ten categories manifest in deployed, state-of-the-art LLMs, emphasising the urgent need for robust frameworks. Our work provides the first systematic analysis of detectability failures of hidden intentions in LLMs under open-world settings, offering a foundation for understanding, inducing, and stress-testing such behaviours, and establishing a flexible taxonomy for anticipating evolving threats and informing governance.

</details>


### [101] [One Persona, Many Cues, Different Results: How Sociodemographic Cues Impact LLM Personalization](https://arxiv.org/abs/2601.18572)
*Franziska Weeber,Vera Neplenbroek,Jan Batzner,Sebastian Padó*

Main category: cs.CL

TL;DR: 研究发现不同persona提示词导致LLM输出存在较大差异，建议个性化研究采用多种有效提示词。


<details>
  <summary>Details</summary>
Motivation: 个性化大型语言模型通过社会人口学子群体可以提升用户体验，但也可能引入或加剧偏见和不公平结果。之前研究依赖单一提示词忽略了模型对提示变化的敏感性和提示词在实际交互中的稀缺性。

Method: 比较了六种常用的persona提示词在七个开放和专有大型语言模型上，针对四个写作和建议任务的表现。

Result: 不同的提示词虽然高度相关，但在不同persona下模型响应存在显著差异，表明单一提示词研究可能存在偏差。

Conclusion: 单一persona提示词不足以全面评估大型语言模型中的偏见，未来研究应使用多样且具外部有效性的提示词进行个性化分析。

Abstract: Personalization of LLMs by sociodemographic subgroup often improves user experience, but can also introduce or amplify biases and unfair outcomes across groups. Prior work has employed so-called personas, sociodemographic user attributes conveyed to a model, to study bias in LLMs by relying on a single cue to prompt a persona, such as user names or explicit attribute mentions. This disregards LLM sensitivity to prompt variations (robustness) and the rarity of some cues in real interactions (external validity). We compare six commonly used persona cues across seven open and proprietary LLMs on four writing and advice tasks. While cues are overall highly correlated, they produce substantial variance in responses across personas. We therefore caution against claims from a single persona cue and recommend future personalization research to evaluate multiple externally valid cues.

</details>


### [102] [From Classification to Ranking: Enhancing LLM Reasoning Capabilities for MBTI Personality Detection](https://arxiv.org/abs/2601.18582)
*Yuan Cao,Feixiang Liu,Xinyue Wang,Yihan Zhu,Hui Xu,Zheng Wang,Qiang Qiu*

Main category: cs.CL

TL;DR: 本文提出将个性检测视为排序任务，采用强化学习训练方法，结合监督微调和组相对策略优化，解决个性分类中的复杂性和模糊边界问题，实现了多项基准测试上的先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的个性检测依赖专家设计的提示，缺乏自主学习能力，且难以准确区分个性 trait；因此需要新的方法更好地处理复杂的人格特征和主观性问题。

Method: 先通过监督微调建立个性 trait排序能力和规范输出格式，然后引入基于排名奖励函数的组相对策略优化（GRPO）强化学习训练。

Result: 方法在多个个性检测基准上获得了最先进的性能，显著提升了分类准确率和鲁棒性。

Conclusion: 该方法成功提升了个性检测的准确性，尤其在处理个性 trait之间的细微区别和模糊界限方面表现优异，达到了多个基准测试的最先进水平。

Abstract: Personality detection aims to measure an individual's corresponding personality traits through their social media posts. The advancements in Large Language Models (LLMs) offer novel perspectives for personality detection tasks. Existing approaches enhance personality trait analysis by leveraging LLMs to extract semantic information from textual posts as prompts, followed by training classifiers for categorization. However, accurately classifying personality traits remains challenging due to the inherent complexity of human personality and subtle inter-trait distinctions. Moreover, prompt-based methods often exhibit excessive dependency on expert-crafted knowledge without autonomous pattern-learning capacity. To address these limitations, we view personality detection as a ranking task rather than a classification and propose a corresponding reinforcement learning training paradigm. First, we employ supervised fine-tuning (SFT) to establish personality trait ranking capabilities while enforcing standardized output formats, creating a robust initialization. Subsequently, we introduce Group Relative Policy Optimization (GRPO) with a specialized ranking-based reward function. Unlike verification tasks with definitive solutions, personality assessment involves subjective interpretations and blurred boundaries between trait categories. Our reward function explicitly addresses this challenge by training LLMs to learn optimal answer rankings. Comprehensive experiments have demonstrated that our method achieves state-of-the-art performance across multiple personality detection benchmarks.

</details>


### [103] [Gained in Translation: Privileged Pairwise Judges Enhance Multilingual Reasoning](https://arxiv.org/abs/2601.18722)
*Lintang Sutawika,Gokul Swamy,Zhiwei Steven Wu,Graham Neubig*

Main category: cs.CL

TL;DR: 通过利用翻译数据监督微调和带有英文参考信息的成对评判者自我对弈强化学习，SP3F有效提升多语言推理模型表现，无需目标语言训练数据。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理语言模型在训练中较少见的语言上表现明显下降，需提升多语言推理能力且不依赖目标语言数据。

Method: 提出SP3F框架，包含两个阶段：第一阶段基于英文问答对的翻译数据进行监督微调以提升基础模型的准确率；第二阶段利用带有特权信息（英文参考答案）的成对评判者在自我对弈中进行强化学习优化。

Result: SP3F显著提升了基础模型性能，在多个数学和非数学任务中，使用极少训练数据就超越了完全后训练模型，且适用于单语言、多语言及未见语言场景。

Conclusion: SP3F方法通过结合监督微调和基于特权信息的自我对弈强化学习，显著增强了多语言推理能力，解决了目标语言训练数据缺乏的问题，达到了领先性能。

Abstract: When asked a question in a language less seen in its training data, current reasoning large language models (RLMs) often exhibit dramatically lower performance than when asked the same question in English. In response, we introduce \texttt{SP3F} (Self-Play with Privileged Pairwise Feedback), a two-stage framework for enhancing multilingual reasoning without \textit{any} data in the target language(s). First, we supervise fine-tune (SFT) on translated versions of English question-answer pairs to raise base model correctness. Second, we perform RL with feedback from a pairwise judge in a self-play fashion, with the judge receiving the English reference response as \textit{privileged information}. Thus, even when none of the model's responses are completely correct, the privileged pairwise judge can still tell which response is better. End-to-end, \texttt{SP3F} greatly improves base model performance, even outperforming fully post-trained models on multiple math and non-math tasks with less than
  of the training data across the single-language, multilingual, and generalization to unseen language settings.

</details>


### [104] [HalluCitation Matters: Revealing the Impact of Hallucinated References with 300 Hallucinated Papers in ACL Conferences](https://arxiv.org/abs/2601.18724)
*Yusuke Sakai,Hidetaka Kamigaito,Taro Watanabe*

Main category: cs.CL

TL;DR: 论文系统调查了幻觉引用的普遍性与影响，发现问题正在快速增长，特别是在2025年的顶会中。


<details>
  <summary>Details</summary>
Motivation: 当前论文中频繁出现不存在的幻觉引用，威胁科学可靠性和会议信誉。

Method: 系统分析ACL、NAACL和EMNLP 2024-2025年所有论文，统计和定位幻觉引用的出现频率和分布。

Result: 分析ACL、NAACL、EMNLP 2024和2025年所有论文，发现近300篇含幻觉引用，尤其在EMNLP 2025表现突出，其中逾100篇被主会和Findings接收。

Conclusion: 幻觉引用现象严重影响学术诚信和会议可信度，需重视并采取对策。

Abstract: Recently, we have often observed hallucinated citations or references that do not correspond to any existing work in papers under review, preprints, or published papers. Such hallucinated citations pose a serious concern to scientific reliability. When they appear in accepted papers, they may also negatively affect the credibility of conferences. In this study, we refer to hallucinated citations as "HalluCitation" and systematically investigate their prevalence and impact. We analyze all papers published at ACL, NAACL, and EMNLP in 2024 and 2025, including main conference, Findings, and workshop papers. Our analysis reveals that nearly 300 papers contain at least one HalluCitation, most of which were published in 2025. Notably, half of these papers were identified at EMNLP 2025, the most recent conference, indicating that this issue is rapidly increasing. Moreover, more than 100 such papers were accepted as main conference and Findings papers at EMNLP 2025, affecting the credibility.

</details>


### [105] [Reflect: Transparent Principle-Guided Reasoning for Constitutional Alignment at Scale](https://arxiv.org/abs/2601.18730)
*Henry Bell,Caroline Zhang,Mohammed Mobasserul Haque,Dhaval Potdar,Samia Zaman,Brandon Fain*

Main category: cs.CL

TL;DR: 本文提出一种无需训练、基于推理的推理时对齐框架\textsc{reflect}，显著提升大语言模型对多样化原则的遵守，提升模型安全性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型对齐方法主要依赖计算量大且复杂的参数微调技术，如基于人类反馈的强化学习，且需要难以获得的人类注释数据。

Method: 提出了一种名为\textsc{reflect}的推理时宪法对齐框架，完全基于上下文，不需要任何训练或数据。该方法结合了基于宪法条件的基础响应、生成后的自我评估、自我批判及最终修订，显式地在生成后推理原则。

Result: \textsc{reflect}在复杂且多样的原则对齐上明显优于传统的少量示例提示，提升了模型对原则的遵守率，尤其有效减少了罕见但重要的原则违规，提高了安全性和鲁棒性。

Conclusion: \textsc{reflect}提供了一种无需训练即可实现大语言模型对齐的高效可插拔方法，且能生成有用的训练数据，促进传统参数微调的扩展和减少长期部署中的计算开销。

Abstract: The constitutional framework of alignment aims to align large language models (LLMs) with value-laden principles written in natural language (such as to avoid using biased language). Prior work has focused on parameter fine-tuning techniques, such as reinforcement learning from human feedback (RLHF), to instill these principles. However, these approaches are computationally demanding, require careful engineering and tuning, and often require difficult-to-obtain human annotation data. We propose \textsc{reflect}, an inference-time framework for constitutional alignment that does not require any training or data, providing a plug-and-play approach for aligning an instruction-tuned model to a set of principles. \textsc{reflect} operates entirely in-context, combining a (i) constitution-conditioned base response with post-generation (ii) self-evaluation, (iii)(a) self-critique, and (iii)(b) final revision. \textsc{reflect}'s technique of explicit in-context reasoning over principles during post-generation outperforms standard few-shot prompting and provides transparent reasoning traces. Our results demonstrate that \textsc{reflect} significantly improves LLM conformance to diverse and complex principles, including principles quite distinct from those emphasized in the model's original parameter fine-tuning, without sacrificing factual reasoning. \textsc{reflect} is particularly effective at reducing the rate of rare but significant violations of principles, thereby improving safety and robustness in the tail end of the distribution of generations. Finally, we show that \textsc{reflect} naturally generates useful training data for traditional parameter fine-tuning techniques, allowing for efficient scaling and the reduction of inference-time computational overhead in long-term deployment scenarios.

</details>


### [106] [One Adapts to Any: Meta Reward Modeling for Personalized LLM Alignment](https://arxiv.org/abs/2601.18731)
*Hongru Cai,Yongqi Li,Tiezheng Yu,Fengbin Zhu,Wenjie Wang,Fuli Feng,Wenjie Li*

Main category: cs.CL

TL;DR: 通过元学习框架的新颖方法MRM，实现了对个体用户偏好的快速且鲁棒的个性化奖励建模。


<details>
  <summary>Details</summary>
Motivation: 解决个性化奖励模型中反馈稀缺和对未知用户高效适应的难题，摆脱直接拟合用户数据的限制，转而学习偏好适应过程。

Method: 提出了元学习框架下的Meta Reward Modeling (MRM)，将个性化奖励建模问题转化为元学习问题，通过加权组合基础奖励函数，并利用MAML方法优化权重初始化，实现快速适应有限反馈下的个体用户。

Result: 实验验证MRM在少样本个性化、用户适应性和鲁棒性方面优于其他基线方法。

Conclusion: MRM方法有效提升了大语言模型个性化对齐的准确性和适应性，尤其在反馈有限情况下表现优异，展现了在个性化偏好学习领域的潜力。

Abstract: Alignment of Large Language Models (LLMs) aims to align outputs with human preferences, and personalized alignment further adapts models to individual users. This relies on personalized reward models that capture user-specific preferences and automatically provide individualized feedback. However, developing these models faces two critical challenges: the scarcity of feedback from individual users and the need for efficient adaptation to unseen users. We argue that addressing these constraints requires a paradigm shift from fitting data to learn user preferences to learn the process of preference adaptation. To realize this, we propose Meta Reward Modeling (MRM), which reformulates personalized reward modeling as a meta-learning problem. Specifically, we represent each user's reward model as a weighted combination of base reward functions, and optimize the initialization of these weights using a Model-Agnostic Meta-Learning (MAML)-style framework to support fast adaptation under limited feedback. To ensure robustness, we introduce the Robust Personalization Objective (RPO), which places greater emphasis on hard-to-learn users during meta optimization. Extensive experiments on personalized preference datasets validate that MRM enhances few-shot personalization, improves user robustness, and consistently outperforms baselines.

</details>


### [107] [Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory](https://arxiv.org/abs/2601.18771)
*Yanming Liu,Xinyue Peng,Zixuan Yan,Yanxin Shen,Wenjie Xu,Yuefeng Huang,Xinyi Wang,Jiannan Cao,Jianwei Yin,Xuhong Zhang*

Main category: cs.CL

TL;DR: Dep-Search通过显式依赖管理和记忆机制，改进多步推理中的检索效率和策略学习，显著提升大语言模型复杂推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有搜索框架依赖隐式自然语言推理导致子问题依赖管理困难，知识复用效率低下，且强化学习难以优化搜索策略，限制了大语言模型复杂多步推理能力的发展。

Method: Dep-Search设计了依赖感知的显式控制机制，通过GRPO整合结构化推理、信息检索及持久记忆，支持问题分解、动态检索、历史知识调用和长推理上下文的记忆总结。

Result: 提出了Dep-Search框架，通过引入依赖感知机制，结合结构化推理、检索和持久记忆，提升了大语言模型在多跳复杂推理任务中的表现。该框架克服了现有搜索方法依赖隐式自然语言推理导致的依赖管理和知识复用难题，实现了显著性能提升。

Conclusion: Dep-Search框架有效提升了大语言模型在多跳复杂推理任务中的表现，克服了隐式推理带来的挑战，且在多个数据集上取得了明显优于基线的效果。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, particularly when augmented with search mechanisms that enable systematic exploration of external knowledge bases. The field has evolved from traditional retrieval-augmented generation (RAG) frameworks to more sophisticated search-based frameworks that orchestrate multi-step reasoning through explicit search strategies. However, existing search frameworks still rely heavily on implicit natural language reasoning to determine search strategies and how to leverage retrieved information across reasoning steps. This reliance on implicit reasoning creates fundamental challenges for managing dependencies between sub-questions, efficiently reusing previously retrieved knowledge, and learning optimal search strategies through reinforcement learning. To address these limitations, we propose Dep-Search, a dependency-aware search framework that advances beyond existing search frameworks by integrating structured reasoning, retrieval, and persistent memory through GRPO. Dep-Search introduces explicit control mechanisms that enable the model to decompose questions with dependency relationships, retrieve information when needed, access previously stored knowledge from memory, and summarize long reasoning contexts into reusable memory entries. Through extensive experiments on seven diverse question answering datasets, we demonstrate that Dep-Search significantly enhances LLMs' ability to tackle complex multi-hop reasoning tasks, achieving substantial improvements over strong baselines across different model scales.

</details>


### [108] [Unsupervised Text Segmentation via Kernel Change-Point Detection on Sentence Embeddings](https://arxiv.org/abs/2601.18788)
*Mumin Jia,Jairo Diaz-Rodriguez*

Main category: cs.CL

TL;DR: 提出了Embed-KCPD，一种无需训练的基于嵌入和带惩罚KCPD的文本分割方法，并给出了首个适用于短程依赖语言模型的理论保证，在仿真和实测中表现优异。


<details>
  <summary>Details</summary>
Motivation: 文本分割中的边界标签获取代价高昂且主观，且难以跨领域和不同粒度迁移，故提出无监督且具有理论保证的新方法Embed-KCPD以解决这一问题。

Method: 利用句子嵌入向量，通过最小化带惩罚的KCPD目标函数实现边界估计；建立基于$m$-依赖序列的KCPD理论，并借助大语言模型生成带有限记忆依赖的合成文本用于验证。

Result: 本文提出了Embed-KCPD，一种无监督文本分割方法，通过句子嵌入向量表示并最小化带惩罚的KCPD目标来估计边界。方法创新地提出了基于$m$-依赖序列的KCPD理论，适应语言中短程依赖，证明了惩罚风险的oracle不等式和局部化保证，确保真实变化点被准确定位。通过基于大语言模型的仿真框架验证理论，并在多种分割基准测试中表现优于其他无监督方法。案例分析表明，该方法兼具理论保证和实际效果。

Conclusion: Embed-KCPD方法具备强理论支持，有效处理短程依赖，无需训练且在多个基准及实例中展现出优越的文本分割性能。

Abstract: Unsupervised text segmentation is crucial because boundary labels are expensive, subjective, and often fail to transfer across domains and granularity choices. We propose Embed-KCPD, a training-free method that represents sentences as embedding vectors and estimates boundaries by minimizing a penalized KCPD objective. Beyond the algorithmic instantiation, we develop, to our knowledge, the first dependence-aware theory for KCPD under $m$-dependent sequences, a finite-memory abstraction of short-range dependence common in language. We prove an oracle inequality for the population penalized risk and a localization guarantee showing that each true change point is recovered within a window that is small relative to segment length. To connect theory to practice, we introduce an LLM-based simulation framework that generates synthetic documents with controlled finite-memory dependence and known boundaries, validating the predicted scaling behavior. Across standard segmentation benchmarks, Embed-KCPD often outperforms strong unsupervised baselines. A case study on Taylor Swift's tweets illustrates that Embed-KCPD combines strong theoretical guarantees, simulated reliability, and practical effectiveness for text segmentation.

</details>


### [109] [MortalMATH: Evaluating the Conflict Between Reasoning Objectives and Emergency Contexts](https://arxiv.org/abs/2601.18790)
*Etienne Lanzeray,Stephane Meilliez,Malo Ruelle,Damien Sileo*

Main category: cs.CL

TL;DR: 研究发现，专注于复杂计算任务的大语言模型可能忽视安全危机，导致在生命威胁情境中不提供及时帮助。


<details>
  <summary>Details</summary>
Motivation: 探究针对深度推理优化的大语言模型是否因专注复杂计算而产生‘隧道视野’，忽略紧急安全问题。

Method: 构建了包括150个涉及代数帮助及生命危险情境（如中风、自空中下落）的MortalMATH基准测试，评估不同模型在面对紧急安全问题时的反应和计算延迟。

Result: 发现普通模型如Llama-3.1会拒绝计算以应对紧急情况，而专用模型如Qwen-3-32b和GPT-5-nano则往往忽视紧急情况，且推理时间有时延迟达15秒，存在潜在安全隐患。

Conclusion: 为了追求正确答案而优化的专用推理模型可能忽略紧急情况，表现为高达95%的任务完成率却忽视生死危急，且推理计算延迟加剧安全风险。

Abstract: Large Language Models are increasingly optimized for deep reasoning, prioritizing the correct execution of complex tasks over general conversation. We investigate whether this focus on calculation creates a "tunnel vision" that ignores safety in critical situations. We introduce MortalMATH, a benchmark of 150 scenarios where users request algebra help while describing increasingly life-threatening emergencies (e.g., stroke symptoms, freefall). We find a sharp behavioral split: generalist models (like Llama-3.1) successfully refuse the math to address the danger. In contrast, specialized reasoning models (like Qwen-3-32b and GPT-5-nano) often ignore the emergency entirely, maintaining over 95 percent task completion rates while the user describes dying. Furthermore, the computational time required for reasoning introduces dangerous delays: up to 15 seconds before any potential help is offered. These results suggest that training models to relentlessly pursue correct answers may inadvertently unlearn the survival instincts required for safe deployment.

</details>


### [110] [Subword-Based Comparative Linguistics across 242 Languages Using Wikipedia Glottosets](https://arxiv.org/abs/2601.18791)
*Iaroslav Chelombitko,Mika Hämäläinen,Aleksey Komissarov*

Main category: cs.CL

TL;DR: 本文利用BPE子词方法，对242种语言进行大规模词汇比较，揭示语言间的词汇相似性及差异，验证了方法有效性并为宏观语言学研究提供新工具。


<details>
  <summary>Details</summary>
Motivation: 旨在提出一个统一框架，以量化分析不同语言之间的词汇模式和语言相似性，推动大规模跨语言比较研究。

Method: 基于Wikipedia词库构建“glottosets”，利用Byte-Pair Encoding进行子词分割，结合排名子词向量分析词汇重叠和差异，从宏观尺度进行语言比较。

Result: 实验验证BPE分词准确度优于随机基线，词汇相似性与语言遗传关系显著相关，不同语言间同形异义的子词分割存在差异，且差异程度与系统发育距离相关。

Conclusion: 该研究通过BPE子词方法实现了对242种拉丁字母及西里尔字母语言的跨语言比较，揭示了词汇重叠、词汇差异及语言相似性，验证了BPE分词与形素边界高度一致，并且语言的BPE词汇相似度与遗传语言关系显著相关。

Abstract: We present a large-scale comparative study of 242 Latin and Cyrillic-script languages using subword-based methodologies. By constructing 'glottosets' from Wikipedia lexicons, we introduce a framework for simultaneous cross-linguistic comparison via Byte-Pair Encoding (BPE). Our approach utilizes rank-based subword vectors to analyze vocabulary overlap, lexical divergence, and language similarity at scale. Evaluations demonstrate that BPE segmentation aligns with morpheme boundaries 95% better than random baseline across 15 languages (F1 = 0.34 vs 0.15). BPE vocabulary similarity correlates significantly with genetic language relatedness (Mantel r = 0.329, p < 0.001), with Romance languages forming the tightest cluster (mean distance 0.51) and cross-family pairs showing clear separation (0.82). Analysis of 26,939 cross-linguistic homographs reveals that 48.7% receive different segmentations across related languages, with variation correlating to phylogenetic distance. Our results provide quantitative macro-linguistic insights into lexical patterns across typologically diverse languages within a unified analytical framework.

</details>


### [111] [ctELM: Decoding and Manipulating Embeddings of Clinical Trials with Embedding Language Models](https://arxiv.org/abs/2601.18796)
*Brian Ondov,Chia-Hsuan Chang,Yujia Zhou,Mauro Giuffrè,Hua Xu*

Main category: cs.CL

TL;DR: 本研究提出并实现了一种基于ELM的架构，将大型语言模型与临床试验文本嵌入对齐，使得模型能解释和生成临床试验内容，提升了嵌入空间的可解释性和生成能力。


<details>
  <summary>Details</summary>
Motivation: 当前文本嵌入的解释、探索和逆向方法有限，缺乏透明度且限制了潜在的生成应用。希望通过对齐大型语言模型与嵌入空间，提高解释能力和生成能力，特别是在临床试验领域。

Method: 使用Embedding Language Model (ELM)方法，将大型语言模型与临床试验的文本嵌入对齐。开发了一个开源、领域无关的ELM架构和训练框架，设计了临床试验的训练任务，并引入了专家验证的合成数据集。训练了一系列ELM模型，探索任务和训练方式对结果的影响。

Result: 最终模型ctELM能够仅从嵌入中准确描述和比较未见过的临床试验，还能从新的向量生成合理的临床试验摘要。生成的摘要对年龄和性别等概念向量的变化有响应。公开了ELM的实现和实验结果。

Conclusion: 通过ELM方法成功实现了大型语言模型与临床试验嵌入的对齐，增强了嵌入空间的透明度和生成潜力，为生物医学及其他领域的嵌入解释和生成应用提供了有力工具。

Abstract: Text embeddings have become an essential part of a variety of language applications. However, methods for interpreting, exploring and reversing embedding spaces are limited, reducing transparency and precluding potentially valuable generative use cases. In this work, we align Large Language Models to embeddings of clinical trials using the recently reported Embedding Language Model (ELM) method. We develop an open-source, domain-agnostic ELM architecture and training framework, design training tasks for clinical trials, and introduce an expert-validated synthetic dataset. We then train a series of ELMs exploring the impact of tasks and training regimes. Our final model, ctELM, can accurately describe and compare unseen clinical trials from embeddings alone and produce plausible clinical trials from novel vectors. We further show that generated trial abstracts are responsive to moving embeddings along concept vectors for age and sex of study subjects. Our public ELM implementation and experimental results will aid the alignment of Large Language Models to embedding spaces in the biomedical domain and beyond.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [112] [Risk-based test framework for LLM features in regulated software](https://arxiv.org/abs/2601.17292)
*Zhiyin Zhou*

Main category: cs.SE

TL;DR: 本文针对嵌入受监管和安全关键软件中的大型语言模型（LLM）提出了一种基于风险的测试框架，涵盖风险分类及多层测试策略，并通过临床研究平台助手案例验证。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型广泛应用于受监管和安全关键的软件中，现有测试和保障方法无法充分应对其带来的复杂风险，故需提出针对性的测试框架。

Method: 构建了一个包含六类风险的风险分类体系，设计了从防护层、编排层到系统层的分层测试策略，将风险映射到具体测试上，并通过临床研究平台中的知识库助手进行案例验证。

Result: 通过案例研究证明该测试框架能够有效检测和管理LLM特性所带来的幻觉、偏见和安全隐患等多种风险，提升了相关系统的可靠性及合规性。

Conclusion: 提出的基于风险的测试框架有效识别和控制了LLM在受监管软件中的多种风险，提高了系统的安全性和可靠性。

Abstract: Large language models are increasingly embedded in regulated and safety-critical software, including clinical research platforms and healthcare information systems. While these features enable natural language search, summarization, and configuration assistance, they introduce risks such as hallucinations, harmful or out-of-scope advice, privacy and security issues, bias, instability under change, and adversarial misuse. Prior work on machine learning testing and AI assurance offers useful concepts but limited guidance for interactive, product-embedded assistants. This paper proposes a risk-based testing framework for LLM features in regulated software: a six-category risk taxonomy, a layered test strategy mapping risks to concrete tests across guardrail, orchestration, and system layers, and a case study applying the approach to a Knowledgebase assistant in a clinical research platform.

</details>


### [113] [YASA: Scalable Multi-Language Taint Analysis on the Unified AST at Ant Group](https://arxiv.org/abs/2601.17390)
*Yayi Wang,Shenao Wang,Jian Zhao,Shaosen Shi,Ting Li,Yan Cheng,Lizhong Bian,Kan Yu,Yanjie Zhao,Haoyu Wang*

Main category: cs.SE

TL;DR: YASA是一种针对多语言企业环境设计的统一静态污点分析框架，通过UAST和统一语义模型实现高效精准的跨语言安全检测，在工业应用中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有静态应用安全测试工具多为单语言设计，无法有效适应企业多语言技术栈且存在工程维护难题，现有多语言工具在中间表示、分析精准度与扩展性方面仍存在不足。

Method: 提出统一抽象语法树（UAST）和统一语义模型，通过静态污点传播分析支持跨多种编程语言的安全检测框架YASA。

Result: YASA在多个语言的工业标准基准测试中均优于6个单语言工具和2个多语言工具，实际在蚂蚁集团部署后分析了7.3K个应用，发现314条未知污点路径，其中92条为零日漏洞，且已有76条被修复。

Conclusion: YASA成功解决了多语言静态安全分析的扩展性和精度难题，具有产业级部署能力和实际漏洞发现能力，证明其对于保障大型多语言软件系统安全具有重要意义。

Abstract: Modern enterprises increasingly adopt diverse technology stacks with various programming languages, posing significant challenges for static application security testing (SAST). Existing taint analysis tools are predominantly designed for single languages, requiring substantial engineering effort that scales with language diversity. While multi-language tools like CodeQL, Joern, and WALA attempt to address these challenges, they face limitations in intermediate representation design, analysis precision, and extensibility, which make them difficult to scale effectively for large-scale industrial applications at Ant Group. To bridge this gap, we present YASA (Yet Another Static Analyzer), a unified multi-language static taint analysis framework designed for industrial-scale deployment. Specifically, YASA introduces the Unified Abstract Syntax Tree (UAST) that provides a unified abstraction for compatibility across diverse programming languages. Building on the UAST, YASA performs point-to analysis and taint propagation, leveraging a unified semantic model to manage language-agnostic constructs, while incorporating language-specific semantic models to handle other unique language features. When compared to 6 single- and 2 multi-language static analyzers on an industry-standard benchmark, YASA consistently outperformed all baselines across Java, JavaScript, Python, and Go. In real-world deployment within Ant Group, YASA analyzed over 100 million lines of code across 7.3K internal applications. It identified 314 previously unknown taint paths, with 92 of them confirmed as 0-day vulnerabilities. All vulnerabilities were responsibly reported, with 76 already patched by internal development teams, demonstrating YASA's practical effectiveness for securing large-scale industrial software systems.

</details>


### [114] [Fingerprinting AI Coding Agents on GitHub](https://arxiv.org/abs/2601.17406)
*Taher A. Ghaleb*

Main category: cs.SE

TL;DR: 本文通过分析33,580个PR，提出了首个AI编码代理指纹识别方法，利用41个特征实现了97.2%的多分类识别准确率，揭示了不同AI代理的独特行为特征。


<details>
  <summary>Details</summary>
Motivation: AI生成代码的作者归属成为软件治理和研究中的关键问题，亟需方法识别AI代理身份。

Method: 分析五大AI编码代理的PR数据，利用提交信息、PR结构和代码特征的41个指标，训练多分类模型实现代理识别。

Result: 实现97.2% F1值的多分类识别，发现Codex多行提交模式和Claude Code独特条件语句等明显指纹。

Conclusion: AI编码代理表现出可识别的行为指纹，使得在软件仓库中区分AI贡献成为可能。

Abstract: AI coding agents are reshaping software development through both autonomous and human-mediated pull requests (PRs). When developers use AI agents to generate code under their own accounts, code authorship attribution becomes critical for repository governance, research validity, and understanding modern development practices. We present the first study on fingerprinting AI coding agents, analyzing 33,580 PRs from five major agents (OpenAI Codex, GitHub Copilot, Devin, Cursor, Claude Code) to identify behavioral signatures. With 41 features spanning commit messages, PR structure, and code characteristics, we achieve 97.2% F1-score in multi-class agent identification. We uncover distinct fingerprints: Codex shows unique multiline commit patterns (67.5% feature importance), and Claude Code exhibits distinctive code structure (27.2% importance of conditional statements). These signatures reveal that AI coding tools produce detectable behavioral patterns, suggesting potential for identifying AI contributions in software repositories.

</details>


### [115] [When AI Agents Touch CI/CD Configurations: Frequency and Success](https://arxiv.org/abs/2601.17413)
*Taher A. Ghaleb*

Main category: cs.SE

TL;DR: AI代理很少修改CI/CD配置，主要针对GitHub Actions，且其配置修改与代码修改一样可靠。Copilot在CI/CD配置修改表现突出，显示出配置专门化的趋势。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理在软件开发中的应用增多，但其对CI/CD配置文件的影响和交互尚未深入研究。

Method: 分析了8,031个来自1,605个GitHub仓库的含AI代理修改的拉取请求，重点关注YAML格式的CI/CD配置文件。对不同AI代理（如Devin、Codex、Copilot）修改CI/CD配置的比例及影响进行了对比。

Result: AI代理修改CI/CD配置的比例较低（3.25%），主要集中在GitHub Actions。总体合并率稍低于非CI/CD修改，Copilot除外，其CI/CD相关PR合并率高出15.63个百分点。构建成功率对于CI/CD和非CI/CD更改基本相当，部分代理在CI/CD修改时表现出更高成功率。

Conclusion: AI代理在CI/CD配置修改中表现可靠，尤其是Copilot显示出配置修改的潜力，有助于未来代理训练和DevOps自动化的改进。

Abstract: AI agents are increasingly used in software development, yet their interaction with CI/CD configurations is not well studied. We analyze 8,031 agentic pull requests (PRs) from 1,605 GitHub repositories where AI agents touch YAML configurations. CI/CD configuration files account for 3.25% of agent changes, varying by agent (Devin: 4.83%, Codex: 2.01%, p < 0.001). When agents modify CI/CD, 96.77% target GitHub Actions. Agentic PRs with CI/CD changes merge slightly less often than others (67.77% vs. 71.80%), except for Copilot, whose CI/CD changes merge 15.63 percentage points more often. Across 99,930 workflow runs, build success rates are comparable for CI/CD and non-CI/CD changes (75.59% vs. 74.87%), though three agents show significantly higher success when modifying CI/CD. These results show that AI agents rarely modify CI/CD and focus mostly on GitHub Actions, yet their configuration changes are as reliable as regular code. Copilot's strong CI/CD performance despite lower acceptance suggests emerging configuration specialization, with implications for agent training and DevOps automation.

</details>


### [116] [Towards a Declarative Agentic Layer for Intelligent Agents in MCP-Based Server Ecosystems](https://arxiv.org/abs/2601.17435)
*Maria Jesus Rodriguez-Sanchez,Manuel Noguera,Angel Ruiz-Zafra,Kawtar Benghazi*

Main category: cs.SE

TL;DR: 本文提出DALIA，一种声明式架构层，通过结构化连接目标、能力和执行，改善大型语言模型驱动的多智能体系统的可靠性和可验证性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型驱动的多智能体系统存在可靠性问题，主要由于缺乏明确的架构结构连接目标、能力和执行。

Method: 提出了一种模型无关的声明式架构层DALIA，用于形式化可执行能力，任务声明式发现，维护代理目录和执行资源，并构建确定性任务图。该架构通过发现、规划和执行的明确分离约束代理行为。

Result: 在示例任务场景中展示了该架构的操作，证明声明式基础能够实现跨异构环境的可复现和可验证的代理工作流。

Conclusion: DALIA架构有效解决了多智能体系统的执行可靠性问题，提升了系统的可验证性和一致性，促进复杂任务的稳定完成。

Abstract: Recent advances in Large Language Models (LLMs) have enabled the development of increasingly complex agentic and multi-agent systems capable of planning, tool use and task decomposition. However, empirical evidence shows that many of these systems suffer from fundamental reliability issues, including hallucinated actions, unexecutable plans and brittle coordination. Crucially, these failures do not stem from limitations of the underlying models themselves, but from the absence of explicit architectural structure linking goals, capabilities and execution. This paper presents a declarative, model-independent architectural layer for grounded agentic workflows that addresses this gap. The proposed layer, referred to as DALIA (Declarative Agentic Layer for Intelligent Agents), formalises executable capabilities, exposes tasks through a declarative discovery protocol, maintains a federated directory of agents and their execution resources, and constructs deterministic task graphs grounded exclusively in declared operations. By enforcing a clear separation between discovery, planning and execution, the architecture constrains agent behaviour to a verifiable operational space, reducing reliance on speculative reasoning and free-form coordination. We present the architecture and design principles of the proposed layer and illustrate its operation through a representative task-oriented scenario, demonstrating how declarative grounding enables reproducible and verifiable agentic workflows across heterogeneous environments.

</details>


### [117] [Data-driven Test Generation for Fuzzing AI Compiler](https://arxiv.org/abs/2601.17450)
*Qingchao Shen*

Main category: cs.SE

TL;DR: 本文提出了一个分阶段的AI编译器测试框架，通过多种技术手段提升测试效果，发现了大量新缺陷。


<details>
  <summary>Details</summary>
Motivation: AI编译器存在因多样硬件和复杂优化逻辑导致的缺陷，确保其质量对模型部署至关重要，因此需要系统性测试方案。

Method: 通过迁移测试、计算图合成及低级中间表示(IR)生成变异，分别针对模型加载、高级优化和低级优化阶段构建测试用例。

Result: 本文提出了一个统一的数据驱动测试框架，用于系统性解决AI编译器在不同阶段面临的测试难题。具体方法包括OPERA用于测试模型加载阶段的操作符转换逻辑，OATest合成多样化的计算图以测试高级优化，HARMONY生成和变异低级IR以测试硬件优化。该框架显著提升了测试覆盖率和效果，成功检测出266个此前未知的缺陷。

Conclusion: 统一的测试框架通过阶段性技术手段有效提升了AI编译器的测试覆盖和缺陷发现能力。

Abstract: Artificial Intelligence (AI) compilers are critical for efficiently deploying AI models across diverse hardware platforms. However, they remain prone to bugs that can compromise both compiler reliability and model correctness. Thus, ensuring the quality of AI compilers is crucial. In this work, we present a unified data-driven testing framework that systematically addresses stage-specific challenges in AI compilers. Specifically, OPERA migrates tests for AI libraries to test various operator conversion logic in the model loading stage. OATest synthesizes diverse optimization-aware computational graphs for testing high-level optimizations. HARMONY generates and mutates diverse low-level IR seeds to generate hardware-optimization-aware tests for testing low-level optimizations. Together, these techniques provide a comprehensive, stage-aware framework that enhances testing coverage and effectiveness, detecting 266 previously unknown bugs in four widely used AI compilers.

</details>


### [118] [LogPrism: Unifying Structure and Variable Encoding for Effective Log Compression](https://arxiv.org/abs/2601.17482)
*Yang Liu,Kaiming Zhang,Zhuangbin Chen,Jinyang Liu,Zibin Zheng*

Main category: cs.SE

TL;DR: 本文提出LogPrism框架，通过统一冗余树联合编码日志结构与变量，实现更高效的日志压缩，显著提升压缩率和速度。


<details>
  <summary>Details</summary>
Motivation: 当前日志压缩的“先解析后压缩”范式存在局限，因为把日志解析和压缩视为独立目标，导致解析器侧重语义准确性，忽视静态模板和动态变量之间的深层关联，影响存储效率。

Method: 提出LogPrism框架，通过构建统一冗余树（URT），动态整合结构提取与变量编码，联合挖掘“结构+变量”共现模式，捕获深层上下文冗余，并通过预编码模式加速处理。

Result: 在16个基准数据集上，LogPrism在13个数据集上实现最高压缩率，超过现有最佳方法4.7%至80.9%，吞吐量达29.87MB/s，比竞争方法快1.68至43.04倍。单归档模式下压缩率提升19.39%，速度提升2.62倍。

Conclusion: 统一冗余编码有助于桥接日志解析与压缩之间的鸿沟，有效挖掘结构与变量的协同冗余，实现更优的压缩效果和处理速度。

Abstract: The prevailing "parse-then-compress" paradigm in log compression fundamentally limits effectiveness by treating log parsing and compression as isolated objectives. While parsers prioritize semantic accuracy (i.e., event identification), they often obscure deep correlations between static templates and dynamic variables that are critical for storage efficiency. In this paper, we investigate this misalignment through a comprehensive empirical study and propose LogPrism, a framework that bridges the gap via unified redundancy encoding. Rather than relying on a rigid pre-parsing step, LogPrism dynamically integrates structural extraction with variable encoding by constructing a Unified Redundancy Tree (URT). This hierarchical approach effectively mines "structure+variable" co-occurrence patterns, capturing deep contextual redundancies while accelerating processing through pre-emptive pattern encoding. Extensive experiments on 16 benchmark datasets confirm that LogPrism establishes a new state-of-the-art. It achieves the highest compression ratio on 13 datasets, surpassing leading baselines by margins of 4.7% to 80.9%, while delivering superior throughput at 29.87 MB/s (1.68$\times$~43.04$\times$ faster than competitors). Moreover, when configured in single-archive mode to maximize global pattern discovery, LogPrism outperforms the best baseline by 19.39% in compression ratio while maintaining a 2.62$\times$ speed advantage.

</details>


### [119] [Measuring Braking Behavior Using Vehicle Tracking and Camera-to-Satellite Homography Rectification](https://arxiv.org/abs/2601.17558)
*J. P. Fleischer,Tanchanok Sirikanchittavon,Chonlachart Jeenprasom,Nooshin Yousefzadeh,Sanjay Ranka,Mohammed Hadi*

Main category: cs.SE

TL;DR: 本文提出了一种基于地面单应性估计的交通摄像头视频分析系统，实现了精准的车辆行为提取，为城市交通安全和管理提供了有力工具。


<details>
  <summary>Details</summary>
Motivation: 为了解决交通摄像头视角畸变问题，实现准确的车辆行为和制动事件分析。

Method: 提出一种鲁棒的地面平面单应性估计方法，将固定交通摄像头视角映射到卫星正射影像，结合MAGSAC++估计器和YOLO11检测实现图像正射校正和车辆轨迹提取，数据存储于ClickHouse数据库。

Result: 系统实现了无须摄像头标定下的车辆轨迹、速度、减速及制动强度的准确提取。实地案例显示，制动事件空间分布具有显著规律，验证了系统的有效性。

Conclusion: 该开源软件系统有效支持了城市信号化道路的车辆行为分析，具备促进车联网、交通管理及道路安全设计的潜力。

Abstract: This paper presents an open-source software application for analyzing traffic camera footage, focusing on vehicle behavior and braking events at signalized urban highways. The core innovation is a robust ground-plane homography estimation that links fixed traffic camera views to satellite orthoimagery. This process rectifies the camera's oblique perspective, ensuring that pixel distances accurately represent real-world distances. This enables the acquisition of features such as vehicle trajectory, speed, deceleration, and braking severity without the need for camera calibration. The pipeline employs the MAGSAC++ estimator to build the homography, converting YOLO11 object detections into a rectified top-down coordinate system. All detection and trajectory data are stored in a ClickHouse database for subsequent analysis. A real-world case study at two signalized intersections in Key West, Florida, showcased the system's capabilities. Across two days of daytime footage, braking activity at the higher-volume intersection peaked around 4 PM at approximately 57.5 events per hour, while the second intersection peaked around 10 AM at roughly 15.5 events per hour. The spatial analysis revealed that most braking events initiated upstream, with mild and moderate braking mostly occurring 30 to 45+ meters away from the stop bar and severe braking distributed throughout, but particularly concentrated in lanes with higher interaction and merging activity. The findings highlight the significant potential of this centralized safety information system to support connected vehicles, facilitating proactive traffic management, crash mitigation, and data-driven roadway design and safety analysis.

</details>


### [120] [How AI Coding Agents Modify Code: A Large-Scale Study of GitHub Pull Requests](https://arxiv.org/abs/2601.17581)
*Daniel Ogenrwot,John Businge*

Main category: cs.SE

TL;DR: 本文通过大规模数据分析，发现AI生成的代码提交在数量和修改范围上与人类有明显差异，且其变更描述更贴合代码内容，有助于评估AI辅助开发的表现。


<details>
  <summary>Details</summary>
Motivation: 探究AI生成的PR与人类贡献在代码修改和描述上的差异，以评估其可靠性和对开发流程的影响。

Method: 利用MSR 2026 Mining Challenge版本的AIDev数据集，分析大规模的AI代理和人类提交的合并PR，从提交数量、文件变动、代码添加和删除行数以及PR描述与代码差异的一致性（词汇和语义相似度）进行比较。

Result: 发现AI生成的PR在提交数量上与人类PR有显著差异（Cliff's δ=0.5429），在涉及文件数和删除行数上存在中等差异，并且其PR描述与代码差异的相似度略高于人类。

Conclusion: 本研究首次以大规模实证数据揭示了AI编码代理与人类在开源开发中贡献的本质差别，促进了对AI自动编码工具的理解与评估。

Abstract: AI coding agents are increasingly acting as autonomous contributors by generating and submitting pull requests (PRs). However, we lack empirical evidence on how these agent-generated PRs differ from human contributions, particularly in how they modify code and describe their changes. Understanding these differences is essential for assessing their reliability and impact on development workflows. Using the MSR 2026 Mining Challenge version of the AIDev dataset, we analyze 24,014 merged Agentic PRs (440,295 commits) and 5,081 merged Human PRs (23,242 commits). We examine additions, deletions, commits, and files touched, and evaluate the consistency between PR descriptions and their diffs using lexical and semantic similarity. Agentic PRs differ substantially from Human PRs in commit count (Cliff's $δ= 0.5429$) and show moderate differences in files touched and deleted lines. They also exhibit slightly higher description-to-diff similarity across all measures. These findings provide a large-scale empirical characterization of how AI coding agents contribute to open source development.

</details>


### [121] [Prompt Driven Development with Claude Code: Building a Complete TUI Framework for the Ring Programming Language](https://arxiv.org/abs/2601.17584)
*Mahmoud Samir Fayed,Ahmed Samir Fayed*

Main category: cs.SE

TL;DR: 本文通过实证分析，展示了现代大型语言模型利用纯提示驱动方法开发大型多模块软件系统的能力，证明了提示驱动开发作为软件工程实践的可行性。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型在软件开发中的应用日益广泛，但其在通过自然语言交互生成和维护大型多模块系统方面的能力尚未充分研究和表征。

Method: 使用纯提示驱动的方法，借助Claude Code的Opus 4.5版本，通过107条不同类型的提示（功能请求、bug修复、文档信息、架构指导等）完成了一个7420行代码的终端用户界面框架的开发，全程无人工编写代码。

Result: 完成了一个包含完整窗口系统、事件驱动架构、交互式控件、层级菜单、网格和树组件、多窗口桌面环境的终端用户界面框架，开发过程高效且高度迭代，展现了模型在实际软件工程中的应用潜力。

Conclusion: 现代大型语言模型能够通过纯提示驱动的工作流程，支持构建具有生产级质量的多模块架构复杂软件系统，展现出保持架构一致性和高效迭代开发的能力。

Abstract: Large language models are increasingly used in software development, yet their ability to generate and maintain large, multi module systems through natural language interaction remains insufficiently characterized. This study presents an empirical analysis of developing a 7420 line Terminal User Interface framework for the Ring programming language, completed in roughly ten hours of active work spread across three days using a purely prompt driven workflow with Claude Code, Opus 4.5. The system was produced through 107 prompts: 21 feature requests, 72 bug fix prompts, 9 prompts sharing information from Ring documentation, 4 prompts providing architectural guidance, and 1 prompt dedicated to generating documentation. Development progressed across five phases, with the Window Manager phase requiring the most interaction, followed by complex UI systems and controls expansion. Bug related prompts covered redraw issues, event handling faults, runtime errors, and layout inconsistencies, while feature requests focused primarily on new widgets, window manager capabilities, and advanced UI components. Most prompts were short, reflecting a highly iterative workflow in which the human role was limited to specifying requirements, validating behaviour, and issuing corrective prompts without writing any code manually. The resulting framework includes a complete windowing subsystem, event driven architecture, interactive widgets, hierarchical menus, grid and tree components, tab controls, and a multi window desktop environment. By combining quantitative prompt analysis with qualitative assessment of model behaviour, this study provides empirical evidence that modern LLMs can sustain architectural coherence and support the construction of production grade tooling for emerging programming languages, highlighting prompt driven development as a viable methodology within software engineering practice.

</details>


### [122] [Human-Aligned Enhancement of Programming Answers with LLMs Guided by User Feedback](https://arxiv.org/abs/2601.17604)
*Suborno Deb Bappon,Saikat Mondal,Chanchal K. Roy,Kevin Schneider*

Main category: cs.SE

TL;DR: 本研究开发了一个基于评论反馈由大语言模型自动改进编程答案的系统AUTOCOMBAT，显著提升答案质量，获得用户认可。


<details>
  <summary>Details</summary>
Motivation: 现有技术问答平台中大量用户评论中指出的问题反馈未被及时处理，导致答案不完整或过时，需要高效利用反馈提升答案质量。

Method: 通过构建包含评论线程的堆栈溢出答案基准（ReSOlve）、评估四个先进语言模型识别实际问题反馈的能力，并开发AUTOCOMBAT工具自动结合评论与问题上下文改进答案。

Result: AUTOCOMBAT实现了接近人类水平的答案改进，显著优于基线方法，且58名从业者中84.5%表示愿意使用或推荐该工具。

Conclusion: 本研究表明，利用大语言模型（LLMs）结合用户评论反馈来改进编程问答内容是可行且有效的，能够显著提升答案的质量且保留原意。

Abstract: Large Language Models (LLMs) are widely used to support software developers in tasks such as code generation, optimization, and documentation. However, their ability to improve existing programming answers in a human-like manner remains underexplored. On technical question-and-answer platforms such as Stack Overflow (SO), contributors often revise answers based on user comments that identify errors, inefficiencies, or missing explanations. Yet roughly one-third of this feedback is never addressed due to limited time, expertise, or visibility, leaving many answers incomplete or outdated. This study investigates whether LLMs can enhance programming answers by interpreting and incorporating comment-based feedback. We make four main contributions. First, we introduce ReSOlve, a benchmark consisting of 790 SO answers with associated comment threads, annotated for improvement-related and general feedback. Second, we evaluate four state-of-the-art LLMs on their ability to identify actionable concerns, finding that DeepSeek achieves the best balance between precision and recall. Third, we present AUTOCOMBAT, an LLM-powered tool that improves programming answers by jointly leveraging user comments and question context. Compared to human revised references, AUTOCOMBAT produces near-human quality improvements while preserving the original intent and significantly outperforming the baseline. Finally, a user study with 58 practitioners shows strong practical value, with 84.5 percent indicating they would adopt or recommend the tool. Overall, AUTOCOMBAT demonstrates the potential of scalable, feedback-driven answer refinement to improve the reliability and trustworthiness of technical knowledge platforms.

</details>


### [123] [Code Change Characteristics and Description Alignment: A Comparative Study of Agentic versus Human Pull Requests](https://arxiv.org/abs/2601.17627)
*Dung Pham,Taher A. Ghaleb*

Main category: cs.SE

TL;DR: 研究比较了AI代理与人类生成的拉取请求，发现AI更加注重局部修改但整体总结能力不足，且提交信息长度影响描述质量，指出了提升代理开发效率的方向。


<details>
  <summary>Details</summary>
Motivation: 当前对AI编码代理自动生成的拉取请求贡献及其与人类贡献的差异认识不足，研究旨在揭示两者在代码更改及信息质量上的异同，为提高代理驱动开发流程提供参考。

Method: 通过对33,596个AI代理生成的拉取请求和6,618个人类拉取请求进行量化分析，比较代码符号移除时间、符号变动率、提交信息语义相似度及拉取请求总结表现等指标，评估两者的贡献差异。

Result: 本研究分析了33,596个AI代理生成的拉取请求(APRs)与6,618个人类生成的拉取请求(HPRs)，比较了代码变更特征及信息质量。结果发现，AI代理引入的函数和类符号被移除的速度更快且频率更高，反映出代理更多关注文档和测试更新等任务。代理在提交级信息表达上表现更强，但在拉取请求整体总结方面不如人类。提交信息长度是描述质量的最佳预测因子，表明代理更依赖个别提交而非整体逻辑。研究揭示了代理在微观精准度和宏观沟通能力上的差距，提示有提升代理驱动开发流程的空间。

Conclusion: AI代理生成的拉取请求在微观层面表现出较强的修改能力和信息表达，但在宏观层面对变更的解释和总结能力尚有不足，未来改进可聚焦于增强代理对整体变更的理解与沟通。

Abstract: AI coding agents can autonomously generate pull requests (PRs), yet little is known about how their contributions compare to those of humans. We analyze 33,596 agent-generated PRs (APRs) and 6,618 human PRs (HPRs) to compare code-change characteristics and message quality. We observe that APR-introduced symbols (functions and classes) are removed much sooner than those in HPRs (median time to removal 3 vs. 34 days) and are also removed more often (symbol churn 7.33% vs. 4.10%), reflecting a focus on other tasks like documentation and test updates. Agents generate stronger commit-level messages (semantic similarity 0.72 vs. 0.68) but lag humans at PR-level summarization (PR-commit similarity 0.86 vs. 0.88). Commit message length is the best predictor of description quality, indicating reliance on individual commits over full-PR reasoning. These findings highlight a gap between agents' micro-level precision and macro-level communication, suggesting opportunities to improve agent-driven development workflows.

</details>


### [124] [Multi-Agent End-to-End Vulnerability Management for Mitigating Recurring Vulnerabilities](https://arxiv.org/abs/2601.17762)
*Zelong Zheng,Jiayuan Zhou,Xing Hu,Yi Gao,Shengyi Pan*

Main category: cs.SE

TL;DR: 提出了多代理框架MAVM，通过利用历史漏洞知识库和上下文检索，有效提升了多函数及多模块复发漏洞的检测与修复效果。


<details>
  <summary>Details</summary>
Motivation: 现有自动化漏洞管理方法难以捕捉跨函数跨模块的上下文依赖，且缺乏对历史漏洞知识的充分利用，导致检测和修复效果不佳。

Method: 设计了包含漏洞知识库、检测、确认、修复与验证的五组件多代理框架，构建知识库并设计上下文检索工具，模拟安全工作流程，实现漏洞管理全流程自动化。

Result: 本文提出了MAVM框架，旨在解决软件漏洞管理中现有方法不足的问题。该框架融合漏洞知识库、漏洞检测、确认、修复与验证五个组件，通过多代理协作形成统一流程。通过构建基于公开漏洞的知识库，弥补了大语言模型在领域知识方面的缺陷，同时设计上下文检索工具，增强对仓库级信息的理解能力。实验中，MAVM在78个真实补丁迁移案例中成功检测并修复了51个漏洞，修复准确率相比基线方法提升31.9%-45.2%，表现优越。

Conclusion: MAVM通过多代理系统结合历史漏洞知识及上下文检索技术，实现了对复发漏洞的高效检测与精准修复，优于传统基线方法。

Abstract: Software vulnerability management has become increasingly critical as modern systems scale in size and complexity. However, existing automated approaches remain insufficient. Traditional static analysis methods struggle to precisely capture contextual dependencies, especially when vulnerabilities span multiple functions or modules. Large language models (LLMs) often lack the ability to retrieve and exploit sufficient contextual information, resulting in incomplete reasoning and unreliable outcomes. Meanwhile, recurring vulnerabilities emerge repeatedly due to code reuse and shared logic, making historical vulnerability knowledge an indispensable foundation for effective vulnerability detection and repair. Nevertheless, prior approaches such as clone-based detection and patch porting, have not fully leveraged this knowledge. To address these challenges, we present MAVM, a multi-agent framework for end-to-end recurring vulnerability management. MAVM integrates five components, including a vulnerability knowledge base, detection, confirmation, repair, and validation, into a unified multi-agent pipeline. We construct a knowledge base from publicly disclosed vulnerabilities, thereby addressing the underuse of historical knowledge in prior work and mitigating the lack of domain-specific expertise in LLMs. Furthermore, we design context-retrieval tools that allow agents to extract and reason over repository-level information, overcoming the contextual limitations of previous methods. Based on agents, MAVM effectively simulates real-world security workflows. To evaluate the performance of MAVM, we construct a dataset containing 78 real-world patch-porting cases (covering 114 function-level migrations). On this dataset, MAVM successfully detects and repairs 51 real vulnerabilities, outperforming baselines by 31.9%-45.2% in repair accuracy, which demonstrates its effectiveness.

</details>


### [125] [iResolveX: Multi-Layered Indirect Call Resolution via Static Reasoning and Learning-Augmented Refinement](https://arxiv.org/abs/2601.17888)
*Monika Santra,Bokai Zhang,Mark Lim,Vishnu Asutosh Dasu,Dongrui Zeng,Gang Tan*

Main category: cs.SE

TL;DR: iResolveX结合静态分析和机器学习，提升了间接调用解析的准确性和召回率，优于现有解决方案。


<details>
  <summary>Details</summary>
Motivation: 间接调用解析难以兼顾静态分析的完整性和机器学习的准确性，现有方法要么误报多，要么召回率低，亟需一种兼顾两者的混合方案。

Method: 采用多层架构：第一层用保守值集分析确保高召回；第二层用基于机器学习的软签名评分和选择性交叉程序回溯内存检查减少误报；输出带置信度评分的间接调用CFG，用于后续分析。

Result: 本文提出了iResolveX，一种结合保守静态分析和基于学习的细化方法的混合多层框架，解决了逆向工程中间接调用解析的难题。通过多层方法，先用保守值集分析保证高召回，再用软签名评分和选择性交叉程序回溯分析减少误报，最终生成带置信度评分的间接调用控制流图。实验表明，iResolveX在保持高召回率的同时，显著降低误报率，优于现有方法。

Conclusion: iResolveX有效平衡了召回率和精确率，在间接调用解析任务中表现优异，支持不同精度-召回率权衡需求，优于当前最先进系统。

Abstract: Indirect call resolution remains a key challenge in reverse engineering and control-flow graph recovery, especially for stripped or optimized binaries. Static analysis is sound but often over-approximates, producing many false positives, whereas machine-learning approaches can improve precision but may sacrifice completeness and generalization. We present iResolveX, a hybrid multi-layered framework that combines conservative static analysis with learning-based refinement. The first layer applies a conservative value-set analysis (BPA) to ensure high recall. The second layer adds a learning-based soft-signature scorer (iScoreGen) and selective inter-procedural backward analysis with memory inspection (iScoreRefine) to reduce false positives. The final output, p-IndirectCFG, annotates indirect edges with confidence scores, enabling downstream analyses to choose appropriate precision--recall trade-offs. Across SPEC CPU2006 and real-world binaries, iScoreGen reduces predicted targets by 19.2% on average while maintaining BPA-level recall (98.2%). Combined with iScoreRefine, the total reduction reaches 44.3% over BPA with 97.8% recall (a 0.4% drop). iResolveX supports both conservative, recall-preserving and F1-optimized configurations and outperforms state-of-the-art systems.

</details>


### [126] [Prompt-Based REST API Test Amplification in Industry: An Experience Report](https://arxiv.org/abs/2601.17903)
*Tolgahan Bardakci,Andreas Faes,Mutlu Beyazit,Serge Demeyr*

Main category: cs.SE

TL;DR: 本研究在工业环境中验证了LLM驱动的REST API测试增强方法，提升了测试覆盖率并揭示异常，证明其实际效用。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM技术在软件测试中被应用，但缺乏其在工业环境下REST API测试效果的实证研究，故本研究旨在填补这一空白。

Method: 在工业环境下，针对生产微服务的六个REST API端点，应用LLM驱动的测试用例扩增技术进行测试覆盖提升与异常检测。

Result: 本文在比利时一家大型物流公司中，验证了基于大语言模型（LLM）的REST API测试增强方法的有效性。研究针对一个生产微服务的六个代表性接口，应用LLM测试增强，面对系统中复杂的认证、状态管理及组织限制。结果表明，LLM测试增强能够提升测试覆盖率，并发现多种异常和观察，具备实际工业应用价值。

Conclusion: LLM测试增强技术在复杂工业REST API测试中依然有效，能提高覆盖率并发现缺陷，具备实际推广潜力。

Abstract: Large Language Models (LLMs) are increasingly used to support software testing tasks, yet there is little evidence of their effectiveness for REST API testing in industrial settings. To address this gap, we replicate our earlier work on LLM-based REST API test amplification within an industrial context at one of the largest logistics companies in Belgium. We apply LLM-based test amplification to six representative endpoints of a production microservice embedded in a large-scale, security-sensitive system, where there is in-depth complexity in authentication, stateful behavior, and organizational constraints. Our experience shows that LLM-based test amplification remains practically useful in industry by increasing coverage and revealing various observations and anomalies.

</details>


### [127] [RGFL: Reasoning Guided Fault Localization for Automated Program Repair Using Large Language Models](https://arxiv.org/abs/2601.18044)
*Melika Sepidband,Hamed Taherkhani,Hung Viet Pham,Hadi Hemmati*

Main category: cs.SE

TL;DR: 本文提出了一种新颖的项目级故障定位方法，通过分层推理模块生成结构化、针对性强的错误解释，并结合两阶段排序提升定位准确率，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的自动修复兴起，项目级代码规模大超出模型上下文限制，准确的故障定位变得尤为关键。

Method: 引入分层推理模块，生成结构化的错误解释，并结合基于LLM和嵌入的双阶段排序策略实现文件及元素级定位。

Result: 相较最先进基线方法，在文件级定位准确率和元素级精准匹配率上均大幅提升，集成后修复成功率提升12.8%。

Conclusion: 该方法在Python和Java项目上的故障定位准确率和修复成功率均有显著提升，证明了其有效性和优势。

Abstract: Fault Localization (FL) is a critical step in Automated Program Repair (APR), and its importance has increased with the rise of Large Language Model (LLM)-based repair agents. In realistic project-level repair scenarios, software repositories often span millions of tokens, far exceeding current LLM context limits. Consequently, models must first identify a small, relevant subset of code, making accurate FL essential for effective repair. We present a novel project-level FL approach that improves both file- and element-level localization. Our method introduces a hierarchical reasoning module that (i) generates structured, bug-specific explanations for candidate files and elements, and (ii) leverages these explanations in a two-stage ranking scheme combining LLM-based and embedding-based signals. We further propose a counterfactual upper-bound analysis to quantify the contribution of each localization stage to repair success. We evaluate our approach on Python and Java projects from SWE-bench Verified, Lite, and Java. Compared to state-of-the-art baselines, including Agentless and OpenHands, our method consistently improves localization accuracy. On SWE-bench Verified, file-level Hit@1 improves from 71.4% to 85%, and MRR from 81.8% to 88.8%. At the element level, Exact Match under top-3 files increases from 36% to 69%. Integrating our localization into Agentless yields a 12.8% end-to-end repair success improvement.

</details>


### [128] [TAM-Eval: Evaluating LLMs for Automated Unit Test Maintenance](https://arxiv.org/abs/2601.18241)
*Elena Bruches,Vadim Alperovich,Dari Baturova,Roman Derunets,Daniil Grebenkin,Georgy Mkrtchyan,Oleg Sedukhin,Mikhail Klementev,Ivan Bondarenko,Nikolay Bushkov,Stanislav Moiseev*

Main category: cs.SE

TL;DR: 本文提出TAM-Eval，一个面向测试套件维护的评估框架，基于大规模多语言项目，实现对测试创建、修复和更新的综合评测，发现当前大型语言模型在该任务中表现有限，促进自动化软件测试研究发展。


<details>
  <summary>Details</summary>
Motivation: 目前大型语言模型在单元测试中的应用主要局限于孤立的测试生成或断言预测，忽视了测试套件维护这一更广泛的挑战。

Method: 提出了TAM-Eval框架和基准，评估模型在测试套件创建、修复和更新三个核心场景的性能，基于测试文件级别并利用完整的仓库上下文，通过测试通过率、代码覆盖率和变异测试等无参考协议进行评估。

Result: 构建了包含1539个自动提取且验证的Python、Java和Go项目测试维护场景的基准数据，实验证明现有模型在真实测试维护任务中表现有限，同时开源了TAM-Eval框架和相关数据代码以支持后续研究。

Conclusion: 当前最先进的大型语言模型在实际的测试套件维护过程中能力有限，仅在测试有效性上有边际提升。

Abstract: While Large Language Models (LLMs) have shown promise in software engineering, their application to unit testing remains largely confined to isolated test generation or oracle prediction, neglecting the broader challenge of test suite maintenance. We introduce TAM-Eval (Test Automated Maintenance Evaluation), a framework and benchmark designed to evaluate model performance across three core test maintenance scenarios: creation, repair, and updating of test suites. Unlike prior work limited to function-level tasks, TAM-Eval operates at the test file level, while maintaining access to full repository context during isolated evaluation, better reflecting real-world maintenance workflows. Our benchmark comprises 1,539 automatically extracted and validated scenarios from Python, Java, and Go projects. TAM-Eval supports system-agnostic evaluation of both raw LLMs and agentic workflows, using a reference-free protocol based on test suite pass rate, code coverage, and mutation testing. Empirical results indicate that state-of-the-art LLMs have limited capabilities in realistic test maintenance processes and yield only marginal improvements in test effectiveness. We release TAM-Eval as an open-source framework to support future research in automated software testing. Our data and code are publicly available at https://github.com/trndcenter/TAM-Eval.

</details>


### [129] [Agentic Much? Adoption of Coding Agents on GitHub](https://arxiv.org/abs/2601.18341)
*Romain Robbes,Théo Matricon,Thomas Degueule,Andre Hora,Stefano Zacchiroli*

Main category: cs.SE

TL;DR: 2025年新型编码代理迅速普及，显著影响软件开发，GitHub数据显示高采用率和多样化应用，推动对其实际效用的进一步研究。


<details>
  <summary>Details</summary>
Motivation: 编码代理的出现将比传统代码补全LLM带来更大变化，且其留下的显式痕迹为研究其影响提供重要机会，有必要深入了解其实际应用及影响。

Method: 通过分析GitHub上129,134个项目的软件工程文档（如联合提交和拉取请求）来追踪和量化编码代理的采用情况。

Result: 截至2025年上半年，编码代理在GitHub上的采用率估计高达15.85%到22.60%，覆盖各种项目成熟度、组织规模、编程语言和项目主题。代理辅助的提交通常较大，包含大量功能和缺陷修复。

Conclusion: 编码代理如Cursor、Claude Code和Codex迅速被广泛采用，且已显著影响软件开发模式，表现出高度自治性和广泛适用性。

Abstract: In the first half of 2025, coding agents have emerged as a category of development tools that have very quickly transitioned to the practice. Unlike ''traditional'' code completion LLMs such as Copilot, agents like Cursor, Claude Code, or Codex operate with high degrees of autonomy, up to generating complete pull requests starting from a developer-provided task description. This new mode of operation is poised to change the landscape in an even larger way than code completion LLMs did, making the need to study their impact critical. Also, unlike traditional LLMs, coding agents tend to leave more explicit traces in software engineering artifacts, such as co-authoring commits or pull requests. We leverage these traces to present the first large-scale study (129,134 projects) of the adoption of coding agents on GitHub, finding an estimated adoption rate of 15.85%--22.60%, which is very high for a technology only a few months old--and increasing. We carry out an in-depth study of the adopters we identified, finding that adoption is broad: it spans the entire spectrum of project maturity; it includes established organizations; and it concerns diverse programming languages or project topics. At the commit level, we find that commits assisted by coding agents are larger than commits only authored by human developers, and have a large proportion of features and bug fixes. These findings highlight the need for further investigation into the practical use of coding agents.

</details>


### [130] [Forecasting the Maintained Score from the OpenSSF Scorecard for GitHub Repositories linked to PyPI libraries](https://arxiv.org/abs/2601.18344)
*Alexandros Tsakpinis,Efe Berk Ergülec,Emil Schwenger,Alexander Pretschner*

Main category: cs.SE

TL;DR: 本文通过多模型时间序列预测，验证了未来开源软件维护活动可准确预测，支持更主动的风险评估。


<details>
  <summary>Details</summary>
Motivation: OpenSSF Scorecard的维护指标仅反映过去90天的开发活动，缺乏对未来维护情况的预测，限制了其在主动风险评估中的作用。

Method: 采用多变量时间序列预测方法，比较统计模型(VARMA)、机器学习模型(随机森林)和深度学习模型(LSTM)对未来维护活动的预测效果。

Result: 未来维护活动可通过预测模型准确预测，尤其是对分桶维护等级和趋势类型的预测准确率分别超过0.95和0.80，且简单模型表现不逊于复杂深度学习模型。

Conclusion: 预测模型有效补充了现有Scorecard指标，能够为开源软件维护风险提供前瞻性评估，提升风险管理能力。

Abstract: The OpenSSF Scorecard is widely used to assess the security posture of open-source software repositories, with the Maintained metric indicating recent development activity and helping identify potentially abandoned dependencies. However, this metric is inherently retrospective, reflecting only the past 90 days of activity and providing no insight into future maintenance, which limits its usefulness for proactive risk assessment. In this paper, we study to what extent future maintenance activity, as captured by the OpenSSF Maintained score, can be forecasted. We analyze 3,220 GitHub repositories associated with the top 1% most central PyPI libraries by PageRank and reconstruct historical Maintained scores over a three-year period. We formulate the task as multivariate time series forecasting and consider four target representations: raw scores, bucketed maintenance levels, numerical trend slopes, and categorical trend types. We compare a statistical model (VARMA), a machine learning model (Random Forest), and a deep learning model (LSTM) across training windows of 3-12 months and forecasting horizons of 1-6 months. Our results show that future maintenance activity can be predicted with meaningful accuracy, particularly for aggregated representations such as bucketed scores and trend types, achieving accuracies above 0.95 and 0.80, respectively. Simpler statistical and machine learning models perform on par with deep learning approaches, indicating that complex architectures are not required. These findings suggest that predictive modeling can effectively complement existing Scorecard metrics, enabling more proactive assessment of open-source maintenance risks.

</details>


### [131] [Promises, Perils, and (Timely) Heuristics for Mining Coding Agent Activity](https://arxiv.org/abs/2601.18345)
*Romain Robes Théo Matricon,Thomas Degueule,Andre Hora,Stefano Zacchiroli*

Main category: cs.SE

TL;DR: 本文研究了2025年快速普及的编码代理，强调其不同于传统LLM代码补全的方法及其在软件仓库中的可追踪性。


<details>
  <summary>Details</summary>
Motivation: 编码代理作为新兴技术，其与传统LLM代码补全有显著差异，且影响深远，因此亟需系统性研究其在实际开发中的表现和影响。

Method: 通过大规模分析GitHub上的编码代理活动，结合软件仓库挖掘（MSR）技术，全面考察其应用效果与潜在问题。

Result: 总结了编码代理带来的优势、潜在风险及应对启发式原则，为后续优化和安全使用提供指导。

Conclusion: 编码代理在软件工程实践中具有重要影响，但同时存在风险和挑战，需要合理的启发式方法来优化使用。

Abstract: In 2025, coding agents have seen a very rapid adoption. Coding agents leverage Large Language Models (LLMs) in ways that are markedly different from LLM-based code completion, making their study critical. Moreover, unlike LLM-based completion, coding agents leave visible traces in software repositories, enabling the use of MSR techniques to study their impact on SE practices. This paper documents the promises, perils, and heuristics that we have gathered from studying coding agent activity on GitHub.

</details>


### [132] [daVinci-Dev: Agent-native Mid-training for Software Engineering](https://arxiv.org/abs/2601.18418)
*Ji Zeng,Dayuan Fu,Tiantian Mi,Yumin Zhuang,Yaxing Huang,Xuefeng Li,Lyumanshan Ye,Muhang Xie,Qishuo Hua,Zhen Huang,Mohan Jiang,Hanning Wang,Jifan Lin,Yang Xiao,Jie Sun,Yunze Wu,Pengfei Liu*

Main category: cs.SE

TL;DR: 本文研究agentic mid-training在软件工程智能体中的应用，通过设计原生agent数据和训练方法提升模型在复杂环境下的表现，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前后训练方法难以充分培养智能体在动态、反馈丰富的真实开发环境中的行为能力，agentic mid-training提供了更具扩展性的训练途径，但受资源限制未被充分探索，本文旨在解决静态数据与动态环境分布不匹配的问题，提高软件智能体的实际表现。

Method: 构建了包含contextually-native和environmentally-native两类轨迹的agent-native数据集，设计系统化训练方法，在大规模数据上进行中期训练，以提升模型在复杂代码库中的导航、编辑和测试能力。

Result: 本文提出了一种基于agentic mid-training（中期训练）的软件工程智能体训练方法，通过构建两类agent-native数据轨迹（contextually-native和environmentally-native），有效解决了静态训练数据与动态反馈环境之间的分布差异问题。该方法在SWE-Bench Verified基准测试中，实现了超过之前开源中期训练方法Kimi-Dev的性能，且训练成本更低，32B和72B模型分别达到56.1%和58.5%的解决率。

Conclusion: agentic mid-training结合contextually-native和environmentally-native两类轨迹显著提升了代码智能体在真实开发环境下的能力，达成更高解决率且降低了训练资源消耗。

Abstract: Recently, the frontier of Large Language Model (LLM) capabilities has shifted from single-turn code generation to agentic software engineering-a paradigm where models autonomously navigate, edit, and test complex repositories. While post-training methods have become the de facto approach for code agents, **agentic mid-training**-mid-training (MT) on large-scale data that mirrors authentic agentic workflows-remains critically underexplored due to substantial resource requirements, despite offering a more scalable path to instilling foundational agentic behaviors than relying solely on expensive reinforcement learning. A central challenge in realizing effective agentic mid-training is the distribution mismatch between static training data and the dynamic, feedback-rich environment of real development. To address this, we present a systematic study of agentic mid-training, establishing both the data synthesis principles and training methodology for effective agent development at scale. Central to our approach is **agent-native data**-supervision comprising two complementary types of trajectories: **contextually-native trajectories** that preserve the complete information flow an agent experiences, offering broad coverage and diversity; and **environmentally-native trajectories** collected from executable repositories where observations stem from actual tool invocations and test executions, providing depth and interaction authenticity. We verify the model's agentic capabilities on `SWE-Bench Verified`. We demonstrate our superiority over the previous open software engineering mid-training recipe `Kimi-Dev` under two post-training settings with an aligned base model and agentic scaffold, while using less than half mid-training tokens (73.1B). Besides relative advantage, our best performing 32B and 72B models achieve **56.1%** and **58.5%** resolution rates, respectively, which are ...

</details>


### [133] [An Audit of Machine Learning Experiments on Software Defect Prediction](https://arxiv.org/abs/2601.18477)
*Giuseppe Destefanis,Leila Yousefi,Martin Shepperd,Allan Tucker,Stephen Swift,Steve Counsell,Mahir Arzoky*

Main category: cs.SE

TL;DR: 对2019-2023年软件缺陷预测研究进行审计，发现实验设计及报告质量参差不齐，大量研究存在设计缺陷，且近半缺乏可复现性，呼吁加强规范化和透明度。


<details>
  <summary>Details</summary>
Motivation: 机器学习算法被广泛用于预测缺陷易发的软件组件，但预测结果的可信度依赖于实验设计和报告的质量。本研究旨在评估近期文献中软件缺陷预测实验的设计与报告习惯，以判断其可信度和可复现性。

Method: 审计了2019年至2023年间SCOPUS索引的101篇软件缺陷预测论文，评估其实验设计、分析方法及报告符合统计学、机器学习和软件工程领域规范的情况，使用 González Barahona 和 Robles 提出的工具评估结果的可复现性。

Result: 在随机抽取的101篇论文中，数据集数量从1到365不等，使用的学习器数量从1到34不等，性能指标数量从1到9不等，45%的研究采用正式统计推断，整体发现427个设计或报告问题，绝大多数论文存在多项问题，且复现性差异显著，发现部分论文存在异常用语和疑似造假现象。

Conclusion: 软件缺陷预测研究中的实验设计和报告实践存在较大差异，且近一半的研究结果难以复现，显示出显著的改进空间。

Abstract: Background: Machine learning algorithms are widely used to predict defect prone software components. In this literature, computational experiments are the main means of evaluation, and the credibility of results depends on experimental design and reporting. Objective: This paper audits recent software defect prediction (SDP) studies by assessing their experimental design, analysis, and reporting practices against accepted norms from statistics, machine learning, and empirical software engineering. The aim is to characterise current practice and assess the reproducibility of published results. Method: We audited SDP studies indexed in SCOPUS between 2019 and 2023, focusing on design and analysis choices such as outcome measures, out of sample validation strategies, and the use of statistical inference. Nine study issues were evaluated. Reproducibility was assessed using the instrument proposed by González Barahona and Robles. Results: The search identified approximately 1,585 SDP experiments published during the period. From these, we randomly sampled 101 papers, including 61 journal and 40 conference publications, with almost 50 percent behind paywalls. We observed substantial variation in research practice. The number of datasets ranged from 1 to 365, learners or learner variants from 1 to 34, and performance measures from 1 to 9. About 45 percent of studies applied formal statistical inference. Across the sample, we identified 427 issues, with a median of four per paper, and only one paper without issues. Reproducibility ranged from near complete to severely limited. We also identified two cases of tortured phrases and possible paper mill activity. Conclusions: Experimental design and reporting practices vary widely, and almost half of the studies provide insufficient detail to support reproduction. The audit indicates substantial scope for improvement.

</details>


### [134] [On the Abolition of the "ICSE Paper" and the Adoption of the "Registered Proposal" and the "Results Report"](https://arxiv.org/abs/2601.18566)
*Fabio Massacci,Winnie Mbaka*

Main category: cs.SE

TL;DR: 为解决软件工程领域新颖性恶性循环和可复现性危机，本文建议采用注册提案和结果报告的双层发表机制。


<details>
  <summary>Details</summary>
Motivation: 面对领域中新颖性恶性循环和可复现性危机，需创新论文发表模式以提升研究质量和透明度。

Method: 提出两级系统，第一步提交经同行评审的注册提案，第二步提交基于提案的结果报告，均在主流会议发表。

Result: 本文提出废除传统的ICSE论文发表模式，改为两级系统：首先提交“注册提案”，包括新思路和实验方法，经过同行评审；随后基于提案，下一年提交“结果报告”，展示实验结果。两者均为主流会议的重要组成部分。

Conclusion: 通过实施双层发表机制，可以提高研究的新颖性和实验的可复现性，促进软件工程领域的健康发展。

Abstract: To address the 'novelty-vicious cycle' and the 'replicability crisis' of the field (both discussed in the survey) we propose abolishing the "ICSE paper" as we know it and replacing it with a two-tier system that also evolves the existing notion of 'Registered Report'. Authors proposing a new idea, experiment, or analysis would submit a "Registered Proposal" of their idea and the proposed experimental methodology to undergo peer review. The following year, anyone can submit (shorter) "Results Reports" on the realization of the empirical work based on the registered proposals of the previous ICSE (or FSE or ISSTA or ASE etc.). Both works should be first class citizens of the mainstream events. We argue that such a disruptive (heretical?) idea is supported and based on the responses of the community of the Future of Software Engineering pre-survey

</details>


### [135] [How are MLOps Frameworks Used in Open Source Projects? An Empirical Characterization](https://arxiv.org/abs/2601.18591)
*Fiorella Zampetti,Federico Stocchetti,Federica Razzano,Damian Andrew Tamburri,Massimiliano Di Penta*

Main category: cs.SE

TL;DR: 本研究通过分析GitHub中的项目和问题跟踪器，揭示了开源MLOps框架的实际使用习惯和用户功能需求，发现开发者倾向于API定制开发，期望改进核心功能和平台集成。


<details>
  <summary>Details</summary>
Motivation: 虽然MLOps框架提供了丰富的功能，但开发者实际使用中通常只利用其中一部分，且存在一些高度期望但未实现的功能，研究旨在了解MLOps框架的实际使用情况及用户的功能需求。

Method: 通过分析GitHub上依赖项目对八个流行开源MLOps框架的使用，观察其API和命令的调用方式，同时质性分析框架问题跟踪器中的功能请求和改进建议，将这些期望的改进与实际使用的功能进行关联。

Result: 发现MLOps框架很少直接使用，也不常集成到GitHub工作流中，开发者更多通过API实现项目中的自定义功能。使用功能主要涉及核心机器学习阶段及基础设施治理，且有时结合多个框架的互补功能。用户主要请求增强核心功能、提高API的开放性和更好的CI/CD集成。

Conclusion: MLOps框架需要强化核心功能，同时改进API的可用性和CI/CD的集成能力，以更好满足开发者基于项目自定义功能的需求，促进框架的广泛应用和生态完善。

Abstract: Machine Learning (ML) Operations (MLOps) frameworks have been conceived to support developers and AI engineers in managing the lifecycle of their ML models. While such frameworks provide a wide range of features, developers may leverage only a subset of them, while missing some highly desired features. This paper investigates the practical use and desired feature enhancements of eight popular open-source MLOps frameworks. Specifically, we analyze their usage by dependent projects on GitHub, examining how they invoke the frameworks' APIs and commands. Then, we qualitatively analyze feature requests and enhancements mined from the frameworks' issue trackers, relating these desired improvements to the previously identified usage features. Results indicate that MLOps frameworks are rarely used out-of-the-box and are infrequently integrated into GitHub Workflows, but rather, developers use their APIs to implement custom functionality in their projects. Used features concern core ML phases and whole infrastructure governance, sometimes leveraging multiple frameworks with complementary features. The mapping with feature requests highlights that users mainly ask for enhancements to core features of the frameworks, but also better API exposure and CI/CD integration.

</details>


### [136] [Let's Make Every Pull Request Meaningful: An Empirical Analysis of Developer and Agentic Pull Requests](https://arxiv.org/abs/2601.18749)
*Haruhiko Yoshioka,Takahiro Monno,Haruka Tokumasu,Taiki Wakamatsu,Yuki Ota,Nimmi Weeraddana,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: 对4万多条PR进行分析，发现提交者特征决定合并率差异，审查因素在人工与AI生成PR中影响不同，提出改进人机协作的方法。


<details>
  <summary>Details</summary>
Motivation: 虽然AI生成的PR生成速度快，但其合并率低于人工PR，故希望理解影响PR合并的因素以提升AI生成PR质量。

Method: 对40,214个PR数据使用64个特征，采用统计回归模型进行大规模实证分析，比较人类与AI代理生成的PR合并结果。

Result: 发现提交者属性是影响合并率的主导因素，且审查特征对人类和AI PR有不同的作用，有助于指导人机协作提高PR质量。

Conclusion: 提交者属性对PR合并结果有重要影响，审查相关特征在人类和AI生成的PR中影响不同。

Abstract: The automatic generation of pull requests (PRs) using AI agents has become increasingly common. Although AI-generated PRs are fast and easy to create, their merge rates have been reported to be lower than those created by humans. In this study, we conduct a large-scale empirical analysis of 40,214 PRs collected from the AIDev dataset. We extract 64 features across six families and fit statistical regression models to compare PR merge outcomes for human and agentic PRs, as well as across three AI agents. Our results show that submitter attributes dominate merge outcomes for both groups, while review-related features exhibit contrasting effects between human and agentic PRs. The findings of this study provide insights into improving PR quality through human-AI collaboration.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [137] [Embodiment-Induced Coordination Regimes in Tabular Multi-Agent Q-Learning](https://arxiv.org/abs/2601.17454)
*Muhammad Ahmed Atif,Nehal Naeem Haji,Mohammad Shahid Shaikh,Muhammad Ebad Atif*

Main category: cs.MA

TL;DR: 研究表明集中式价值学习在受限制的实体环境中未必优于独立学习，协调效果依赖于具体动力学条件和代理角色。


<details>
  <summary>Details</summary>
Motivation: 检验集中式价值学习在多智能体强化学习中是否确实改善协调性及稳定性，特别是在具体实体限制条件下。

Method: 在一个完全表格化的捕食者-猎物网格世界中，比较了独立Q学习与集中式Q学习，控制代理的速度和体力等具体化限制。

Result: 集中式学习未能稳定优于独立学习，且在不对称角色和多种动力学条件下，协调崩溃持续存在，揭示协调结构是影响效果的主要因素。

Conclusion: 集中式价值学习并不总是优于独立学习，尤其在代理具备运动和体力限制的条件下，集中式学习可能导致协调失败。

Abstract: Centralized value learning is often assumed to improve coordination and stability in multi-agent reinforcement learning, yet this assumption is rarely tested under controlled conditions. We directly evaluate it in a fully tabular predator-prey gridworld by comparing independent and centralized Q-learning under explicit embodiment constraints on agent speed and stamina. Across multiple kinematic regimes and asymmetric agent roles, centralized learning fails to provide a consistent advantage and is frequently outperformed by fully independent learning, even under full observability and exact value estimation. Moreover, asymmetric centralized-independent configurations induce persistent coordination breakdowns rather than transient learning instability. By eliminating confounding effects from function approximation and representation learning, our tabular analysis isolates coordination structure as the primary driver of these effects. The results show that increased coordination can become a liability under embodiment constraints, and that the effectiveness of centralized learning is fundamentally regime and role dependent rather than universal.

</details>


### [138] [VissimRL: A Multi-Agent Reinforcement Learning Framework for Traffic Signal Control Based on Vissim](https://arxiv.org/abs/2601.18284)
*Hsiao-Chuan Chang,Sheng-You Huang,Yen-Chi Chen,I-Chen Wu*

Main category: cs.MA

TL;DR: 本文设计了针对高保真Vissim仿真的强化学习交通信号控制框架VissimRL，解决了接口复杂和缺乏标准环境的问题，实现了高效训练和交通性能改进，推动了学术与实际应用的结合。


<details>
  <summary>Details</summary>
Motivation: 现有交通信号控制研究多使用SUMO和CityFlow仿真器，Vissim虽具备高保真度驾驶行为建模及广泛工业应用，但因接口复杂且缺乏标准化框架，在强化学习（RL）研究中未被充分利用。

Method: 通过封装Vissim的COM接口，设计基于Python的高级API，实现了单智能体与多智能体的标准化强化学习训练环境，并进行了性能评估实验验证。

Result: 本文提出了VissimRL，一个模块化强化学习框架，通过高级Python API封装Vissim的COM接口，提供标准化的单智能体和多智能体训练环境。实验证明该框架显著降低了开发成本并保证了运行效率，同时在训练过程中带来交通性能的持续提升和多智能体控制的协调性。

Conclusion: VissimRL框架展示了在高保真仿真环境下应用强化学习进行交通信号控制的可行性，促进了科研成果向实际工业应用的转化，提升了多智能体协同控制能力。

Abstract: Traffic congestion remains a major challenge for urban transportation, leading to significant economic and environmental impacts. Traffic Signal Control (TSC) is one of the key measures to mitigate congestion, and recent studies have increasingly applied Reinforcement Learning (RL) for its adaptive capabilities. With respect to SUMO and CityFlow, the simulator Vissim offers high-fidelity driver behavior modeling and wide industrial adoption but remains underutilized in RL research due to its complex interface and lack of standardized frameworks. To address this gap, this paper proposes VissimRL, a modular RL framework for TSC that encapsulates Vissim's COM interface through a high-level Python API, offering standardized environments for both single- and multi-agent training. Experiments show that VissimRL significantly reduces development effort while maintaining runtime efficiency, and supports consistent improvements in traffic performance during training, as well as emergent coordination in multi-agent control. Overall, VissimRL demonstrates the feasibility of applying RL in high-fidelity simulations and serves as a bridge between academic research and practical applications in intelligent traffic signal control.

</details>
