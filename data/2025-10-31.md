<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 58]
- [cs.SE](#cs.SE) [Total: 21]
- [cs.MA](#cs.MA) [Total: 4]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [StreetMath: Study of LLMs' Approximation Behaviors](https://arxiv.org/abs/2510.25776)
*Chiung-Yi Tseng,Somshubhra Roy,Maisha Thasin,Danyang Zhang,Blessing Effiong*

Main category: cs.CL

TL;DR: 本文提出了StreetMath基准，评测大语言模型在实际近似算术场景下的推理能力，并发现模型倾向于计算精确值而非近似。


<details>
  <summary>Details</summary>
Motivation: 现有研究多聚焦LLM的精确算术推理，而对非自回归模型在快速非正式数学近似推理上的能力关注较少。

Method: 设计StreetMath基准，测试多个LLM（如Qwen3系列、Dream系列、Falcon系列、Mamba系列）在近似推理上的表现，同时利用机制可解释性方法分析其内部计算状态。

Result: 模型常尝试计算精确值或调用外部工具，尽管有时在早期层可得到正确答案，但求近似时仍需处理更多token，且精确与近似运算依赖不同神经组件。

Conclusion: LLM在近似算术任务中缺乏类似人类的“认知吝啬”，表现出不同的计算机制，揭示了其推理方式的独特性。

Abstract: There is a substantial body of literature examining the mathematical
reasoning capabilities of large language models (LLMs), particularly their
performance on precise arithmetic operations in autoregressive architectures.
However, their ability to perform approximate reasoning in informal, fast-paced
mathematical operations has received far less attention, especially among
non-autoregressive decoder models. Our work addresses this gap by introducing
StreetMath, a benchmark designed to evaluate models' approximation abilities
under real-world approximation scenarios. We conduct extensive evaluations
across different LLM architectures: Qwen3-4B-Instruct-2507,
Qwen3-4B-Thinking-2507, Dream-v0-Instruct-7B, Falcon-Mamba-7B-Instruct, and
Mamba-GPT-3B. Furthermore, we apply mechanistic interpretability techniques to
probe their internal computational states. Our analysis reveals that LLMs
generally attempt to compute exact values or invoke external tools even in
tasks that call for approximation. Moreover, while models sometimes reach the
correct answer in early layers or steps, they still consume more tokens when
solving approximation tasks. Additional experiments indicate that exact and
approximate arithmetic operations rely on largely separate neural components.
Drawing upon research on cognitive psychology, we argue that LLMs do not
exhibit cognitive miserliness in the same way humans do in street math
settings. We open source our work https://github.com/ctseng777/StreetMath

</details>


### [2] [Review Based Entity Ranking using Fuzzy Logic Algorithmic Approach: Analysis](https://arxiv.org/abs/2510.25778)
*Pratik N. Kalamkar,Anupama G. Phakatkar*

Main category: cs.CL

TL;DR: 本文针对情感分析中的评价强度问题，提出了一种根据观点强度和方向对实体进行排名的方法。


<details>
  <summary>Details</summary>
Motivation: 现有的整体词典方法未考虑观点强度的不同层次，如强烈、中等、弱等，导致对实体评判不够精准。

Method: 通过模糊逻辑算法将观点词按强度级别分类，结合句法依赖关系解析找出与产品特定方面相关的观点词，再对实体进行评分和排名。

Result: 方法能够将观点分为非常弱、弱、中等、非常强和强等多个层次，实现更细粒度的情感分析和实体排名。

Conclusion: 采用基于模糊逻辑的观点强度分类结合句法分析，能够有效提升意见挖掘中实体评分的精确性和细粒度。

Abstract: Opinion mining, also called sentiment analysis, is the field of study that
analyzes people opinions, sentiments, evaluations, appraisals, attitudes, and
emotions towards entities such as products, services, organizations,
individuals, issues, events, topics, and their attributes. Holistic
lexicon-based approach does not consider the strength of each opinion, i.e.,
whether the opinion is very strongly negative (or positive), strongly negative
(or positive), moderate negative (or positive), very weakly negative (or
positive) and weakly negative (or positive). In this paper, we propose approach
to rank entities based on orientation and strength of the entity reviews and
user's queries by classifying them in granularity levels (i.e. very weak, weak,
moderate, very strong and strong) by combining opinion words (i.e. adverb,
adjective, noun and verb) that are related to aspect of interest of certain
product. We shall use fuzzy logic algorithmic approach in order to classify
opinion words into different category and syntactic dependency resolution to
find relations for desired aspect words. Opinion words related to certain
aspects of interest are considered to find the entity score for that aspect in
the review.

</details>


### [3] [LASTIST: LArge-Scale Target-Independent STance dataset](https://arxiv.org/abs/2510.25783)
*DongJae Kim,Yaejin Lee,Minsu Park,Eunil Park*

Main category: cs.CL

TL;DR: 本文提出了一个用于立场检测的韩语大规模数据集LASTIST，填补了低资源语言在该领域的空白。


<details>
  <summary>Details</summary>
Motivation: 现有研究多集中于依赖特定目标的立场检测且主要基于英文，缺乏面向低资源语言（如韩语）的数据集。

Method: 收集韩国内政党新闻稿中的563,299条标注句子，构建了LASTIST数据集，并训练了最新的深度学习立场检测模型。

Result: 数据集支持多种立场检测任务，包括目标无关的立场检测和历时演化立场检测，展示了数据集的多功能性和有效性。

Conclusion: LASTIST数据集为韩语低资源立场检测研究提供了基础资源，促进了该领域的研究发展。

Abstract: Stance detection has emerged as an area of research in the field of
artificial intelligence. However, most research is currently centered on the
target-dependent stance detection task, which is based on a person's stance in
favor of or against a specific target. Furthermore, most benchmark datasets are
based on English, making it difficult to develop models in low-resource
languages such as Korean, especially for an emerging field such as stance
detection. This study proposes the LArge-Scale Target-Independent STance
(LASTIST) dataset to fill this research gap. Collected from the press releases
of both parties on Korean political parties, the LASTIST dataset uses 563,299
labeled Korean sentences. We provide a detailed description of how we collected
and constructed the dataset and trained state-of-the-art deep learning and
stance detection models. Our LASTIST dataset is designed for various tasks in
stance detection, including target-independent stance detection and diachronic
evolution stance detection. We deploy our dataset on
https://anonymous.4open.science/r/LASTIST-3721/.

</details>


### [4] [zFLoRA: Zero-Latency Fused Low-Rank Adapters](https://arxiv.org/abs/2510.25784)
*Dhananjaya Gowda,Seoha Song,Harshith Goka,Junhyun Lee*

Main category: cs.CL

TL;DR: 提出了zFLoRA，一种零延迟的低秩适配器，显著减少了大语言模型在推理时的额外计算负担。


<details>
  <summary>Details</summary>
Motivation: 当前多任务适配器虽参数少，却在推理时带来高达2.5倍的计算开销，影响部署效率。

Method: 设计了零延迟融合低秩适配器zFLoRA，保证在保持模型表现的同时极大降低推理延迟。

Result: 在1B、3B及7B规模模型上测试，zFLoRA在18个任务上表现优于传统低秩适配器和全量微调，且在NPU和GPU上基本无额外延迟。

Conclusion: zFLoRA有效提升了适配器的推理效率，适合实际应用中多任务大模型的快速部署。

Abstract: Large language models (LLMs) are increasingly deployed with task-specific
adapters catering to multiple downstream applications. In such a scenario, the
additional compute associated with these apparently insignificant number of
adapter parameters (typically less than 1% of the base model) turns out to be
disproportionately significant during inference time (upto 2.5x times that of
the base model). In this paper, we propose a new zero-latency fused low-rank
adapter (zFLoRA) that introduces zero or negligible latency overhead on top of
the base model. Experimental results on LLMs of size 1B, 3B and 7B show that
zFLoRA compares favorably against the popular supervised fine-tuning benchmarks
including low-rank adapters (LoRA) as well as full fine-tuning (FFT).
Experiments are conducted on 18 different tasks across three different
categories namely commonsense reasoning, math reasoning and summary-dialogue.
Latency measurements made on NPU (Samsung Galaxy S25+) as well as GPU (NVIDIA
H100) platforms show that the proposed zFLoRA adapters introduce zero to
negligible latency overhead.

</details>


### [5] [BlackboxNLP-2025 MIB Shared Task: Improving Circuit Faithfulness via Better Edge Selection](https://arxiv.org/abs/2510.25786)
*Yaniv Nikankin,Dana Arad,Itay Itzhak,Anja Reusch,Adi Simhi,Gal Kesten-Pomeranz,Yonatan Belinkov*

Main category: cs.CL

TL;DR: 本文提出了三种改进机制解释性中电路发现的方法，包括引入自助法识别一致性边缘、采用比率选择策略及整数线性规划替代贪婪选择，提升了电路发现的准确性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的机制解释性以电路发现为挑战，旨在确定模型执行特定任务的部分，现有方法在准确性和效率上存在不足。

Method: 使用自助法识别稳定的边缘归因分数，设计基于比率的边缘选择策略，采用整数线性规划替代传统贪婪算法优化电路选择。

Result: 所提方法在多项机制解释性基准任务和多种模型上表现出更高的电路忠实度和更优性能，超越了之前的方法。

Conclusion: 通过引入自助法和整数线性规划等策略，显著提升了电路发现的准确性与稳定性，为机制解释性研究提供了有效工具。

Abstract: One of the main challenges in mechanistic interpretability is circuit
discovery, determining which parts of a model perform a given task. We build on
the Mechanistic Interpretability Benchmark (MIB) and propose three key
improvements to circuit discovery. First, we use bootstrapping to identify
edges with consistent attribution scores. Second, we introduce a simple
ratio-based selection strategy to prioritize strong positive-scoring edges,
balancing performance and faithfulness. Third, we replace the standard greedy
selection with an integer linear programming formulation. Our methods yield
more faithful circuits and outperform prior approaches across multiple MIB
tasks and models. Our code is available at:
https://github.com/technion-cs-nlp/MIB-Shared-Task.

</details>


### [6] [LISTEN to Your Preferences: An LLM Framework for Multi-Objective Selection](https://arxiv.org/abs/2510.25799)
*Adam S. Jovine,Tinghan Ye,Francis Bahk,Jingjing Wang,David B. Shmoys,Peter I. Frazier*

Main category: cs.CL

TL;DR: 本文提出了LISTEN框架，利用大语言模型（LLM）作为零样本偏好判定工具，通过自然语言指导专家的高层次优先级，辅助多目标决策。


<details>
  <summary>Details</summary>
Motivation: 人类专家在面对多目标的庞大选项集时，难以形式化复杂隐含的偏好，导致决策效率低下。

Method: 设计了两种迭代算法：LISTEN-U（基于参数化效用函数的细化）和LISTEN-T（基于小批量解决方案的锦标赛式非参数选择），以适应LLM的上下文和计算限制。

Result: 在航班预订、购物和考试调度等多样任务上，LISTEN-U在参数化偏好一致性高时表现优异，而LISTEN-T表现更稳健。

Conclusion: 通过自然语言直接引导复杂多目标决策，LISTEN显著降低了传统偏好获取的认知负担，展示了新的多目标决策方法。

Abstract: Human experts often struggle to select the best option from a large set of
items with multiple competing objectives, a process bottlenecked by the
difficulty of formalizing complex, implicit preferences. To address this, we
introduce LISTEN, a framework that leverages a Large Language Model (LLM) as a
zero-shot preference oracle, guided only by an expert's high-level priorities
in natural language. To operate within LLM constraints like context windows and
inference costs, we propose two iterative algorithms: LISTEN-U, which uses the
LLM to refine a parametric utility function, and LISTEN-T, a non-parametric
method that performs tournament-style selections over small batches of
solutions. Evaluated on diverse tasks including flight booking, shopping, and
exam scheduling, our results show LISTEN-U excels when preferences are
parametrically aligned (a property we measure with a novel concordance metric),
while LISTEN-T offers more robust performance. This work explores a promising
direction for steering complex multi-objective decisions directly with natural
language, reducing the cognitive burden of traditional preference elicitation.

</details>


### [7] [Beyond Length: Quantifying Long-Range Information for Long-Context LLM Pretraining Data](https://arxiv.org/abs/2510.25804)
*Haoran Deng,Yingyu Lin,Zhenghao Lin,Xiao Liu,Yizhou Sun,Yi-An Ma,Yeyun Gong*

Main category: cs.CL

TL;DR: 论文提出了LongFilter框架，用于筛选适合长上下文预训练的训练数据，通过评估长距离依赖信息增益提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 长文本数据中大部分缺乏长距离依赖，使用所有数据训练效率低，需筛选有效数据以提升长上下文模型效果。

Method: LongFilter通过对比长上下文与短上下文模型预测，衡量扩展上下文带来的信息增益，识别长距离依赖关键样本进行数据筛选。

Result: 在LLaMA-3-8B模型中将上下文长度从8K扩展至64K，LongFilter高效筛选出高质量数据，显著提升HELMET、LongBench和RULER等基准测试表现。

Conclusion: LongFilter有效提升了长上下文语言模型的训练效率和性能，证明了针对长距离依赖数据的选择性训练的重要性。

Abstract: Long-context language models unlock advanced capabilities in reasoning, code
generation, and document summarization by leveraging dependencies across
extended spans of text. However, a significant portion of readily available
long-text data lacks meaningful long-distance dependencies; most spans can be
predicted using only local context. Training on such data is inefficient,
making careful data selection crucial. Therefore, we introduce LongFilter, a
framework for curating training data tailored to long-context pretraining.
LongFilter measures the information gain provided by extended context by
contrasting model predictions under long-context versus short-context settings,
thereby identifying samples where long-range dependencies are essential.
Experiments with LLaMA-3-8B, extending its context length from 8K to 64K, show
that LongFilter efficiently selects high-quality data and yields substantial
improvements on benchmarks such as HELMET, LongBench, and RULER.

</details>


### [8] [Ideology-Based LLMs for Content Moderation](https://arxiv.org/abs/2510.25805)
*Stefano Civelli,Pietro Bernardelle,Nardiena A. Pratama,Gianluca Demartini*

Main category: cs.CL

TL;DR: 本文研究了采用不同人格设定对大语言模型在有害内容分类中公平性和一致性的影响，发现人格设定会引入细微的意识形态偏见，强化同意识形态间的一致性，扩大不同意识形态间的分歧。


<details>
  <summary>Details</summary>
Motivation: 在内容审核系统中使用大语言模型时，确保公平和中立性至关重要，但人格设定可能影响模型的判断和偏见。

Method: 通过比较不同模型架构、大小和内容模态下的人格设定，分析其对有害内容分类准确性和一致性的影响，特别进行政治导向任务验证。

Result: 表面看人格设定对准确率影响不大，但深入分析显示不同意识形态的人格对有害标签的判定有显著差异，且大模型更倾向于与相同政治倾向人格一致。

Conclusion: 人格设定会在大语言模型输出中引入意识形态偏见，可能导致AI系统在中立表象下强化党派观点，需警惕其潜在影响。

Abstract: Large language models (LLMs) are increasingly used in content moderation
systems, where ensuring fairness and neutrality is essential. In this study, we
examine how persona adoption influences the consistency and fairness of harmful
content classification across different LLM architectures, model sizes, and
content modalities (language vs. vision). At first glance, headline performance
metrics suggest that personas have little impact on overall classification
accuracy. However, a closer analysis reveals important behavioral shifts.
Personas with different ideological leanings display distinct propensities to
label content as harmful, showing that the lens through which a model "views"
input can subtly shape its judgments. Further agreement analyses highlight that
models, particularly larger ones, tend to align more closely with personas from
the same political ideology, strengthening within-ideology consistency while
widening divergence across ideological groups. To show this effect more
directly, we conducted an additional study on a politically targeted task,
which confirmed that personas not only behave more coherently within their own
ideology but also exhibit a tendency to defend their perspective while
downplaying harmfulness in opposing views. Together, these findings highlight
how persona conditioning can introduce subtle ideological biases into LLM
outputs, raising concerns about the use of AI systems that may reinforce
partisan perspectives under the guise of neutrality.

</details>


### [9] [Beyond Long Context: When Semantics Matter More than Tokens](https://arxiv.org/abs/2510.25816)
*Tarun Kumar Chawdhury,Jon D. Duke*

Main category: cs.CL

TL;DR: 本文提出了CLEAR方法，一种基于实体感知的检索技术，用于提升电子健康记录中语义问答的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录中临床文档以base64编码存储，传统方法难以捕捉细微的临床关系，影响语义问答效果。

Method: 引入了CLEAR方法，通过实体感知检索，结合Clinical Notes QA Evaluation Platform进行了对比测试，验证其性能。

Result: CLEAR在12篇不同长度的临床笔记测试中表现优异，58.3%的胜出率，语义相似度0.878，且所用tokens减少约78%。长篇文档表现尤为明显，胜率达75%。

Conclusion: 实体感知检索显著提升了临床自然语言处理的效率与准确性，评价平台为评估临床问答系统提供了可复用、透明的基准工具。

Abstract: Electronic Health Records (EHR) store clinical documentation as base64
encoded attachments in FHIR DocumentReference resources, which makes semantic
question answering difficult. Traditional vector database methods often miss
nuanced clinical relationships. The Clinical Entity Augmented Retrieval (CLEAR)
method, introduced by Lopez et al. 2025, uses entity aware retrieval and
achieved improved performance with an F1 score of 0.90 versus 0.86 for
embedding based retrieval, while using over 70 percent fewer tokens. We
developed a Clinical Notes QA Evaluation Platform to validate CLEAR against
zero shot large context inference and traditional chunk based retrieval
augmented generation. The platform was tested on 12 clinical notes ranging from
10,000 to 65,000 tokens representing realistic EHR content. CLEAR achieved a
58.3 percent win rate, an average semantic similarity of 0.878, and used 78
percent fewer tokens than wide context processing. The largest performance
gains occurred on long notes, with a 75 percent win rate for documents
exceeding 65,000 tokens. These findings confirm that entity aware retrieval
improves both efficiency and accuracy in clinical natural language processing.
The evaluation framework provides a reusable and transparent benchmark for
assessing clinical question answering systems where semantic precision and
computational efficiency are critical.

</details>


### [10] [The Geometry of Dialogue: Graphing Language Models to Reveal Synergistic Teams for Multi-Agent Collaboration](https://arxiv.org/abs/2510.26352)
*Kotaro Furuya,Yuichi Kitagawa*

Main category: cs.CL

TL;DR: 本文提出了一种基于模型间语义交互自动组合多大语言模型团队的新方法，无需了解模型内部细节。


<details>
  <summary>Details</summary>
Motivation: 由于多语言模型团队的协同效果依赖于团队成员的最佳组合，但模型的内部特性不透明，如何形成最优团队是一个挑战。

Method: 通过构建“语言模型图”，利用模型之间对话的语义一致性映射模型关系，并通过社区检测发现协同模型群组。

Result: 所提方法能够发现反映潜在专业化的功能一致团队，在主题导向对话中形成的团队在下游任务中优于随机团队且接近人工定制团队的性能。

Conclusion: 该研究为自动设计协同多大语言模型团队提供了新的方法基础，有助于提升多模型协作性能。

Abstract: While a multi-agent approach based on large language models (LLMs) represents
a promising strategy to surpass the capabilities of single models, its success
is critically dependent on synergistic team composition. However, forming
optimal teams is a significant challenge, as the inherent opacity of most
models obscures the internal characteristics necessary for effective
collaboration. In this paper, we propose an interaction-centric framework for
automatic team composition that does not require any prior knowledge including
their internal architectures, training data, or task performances. Our method
constructs a "language model graph" that maps relationships between models from
the semantic coherence of pairwise conversations, and then applies community
detection to identify synergistic model clusters. Our experiments with diverse
LLMs demonstrate that the proposed method discovers functionally coherent
groups that reflect their latent specializations. Priming conversations with
specific topics identified synergistic teams which outperform random baselines
on downstream benchmarks and achieve comparable accuracy to that of
manually-curated teams based on known model specializations. Our findings
provide a new basis for the automated design of collaborative multi-agent LLM
teams.

</details>


### [11] [A Survey on Efficient Large Language Model Training: From Data-centric Perspectives](https://arxiv.org/abs/2510.25817)
*Junyu Luo,Bohan Wu,Xiao Luo,Zhiping Xiao,Yiqiao Jin,Rong-Cheng Tu,Nan Yin,Yifan Wang,Jingyang Yuan,Wei Ju,Ming Zhang*

Main category: cs.CL

TL;DR: 本文综述了大语言模型后训练过程中数据高效利用的各种方法，并提出相应分类体系，探讨未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型后训练面临数据标注成本高和数据规模边际收益递减的问题，需实现数据高效利用。

Method: 从数据选择、质量提升、合成数据生成、数据蒸馏与压缩、自我进化数据生态五个方面系统总结数据高效后训练方法。

Result: 归纳了各类代表性方法，提出了数据高效后训练的研究挑战和未来方向。

Conclusion: 通过系统综述和分类，本文推动对大模型数据高效利用的进一步研究和应用。

Abstract: Post-training of Large Language Models (LLMs) is crucial for unlocking their
task generalization potential and domain-specific capabilities. However, the
current LLM post-training paradigm faces significant data challenges, including
the high costs of manual annotation and diminishing marginal returns on data
scales. Therefore, achieving data-efficient post-training has become a key
research question. In this paper, we present the first systematic survey of
data-efficient LLM post-training from a data-centric perspective. We propose a
taxonomy of data-efficient LLM post-training methods, covering data selection,
data quality enhancement, synthetic data generation, data distillation and
compression, and self-evolving data ecosystems. We summarize representative
approaches in each category and outline future research directions. By
examining the challenges in data-efficient LLM post-training, we highlight open
problems and propose potential research avenues. We hope our work inspires
further exploration into maximizing the potential of data utilization in
large-scale model training. Paper List:
https://github.com/luo-junyu/Awesome-Data-Efficient-LLM

</details>


### [12] [Evaluating the Impact of LLM-Assisted Annotation in a Perspectivized Setting: the Case of FrameNet Annotation](https://arxiv.org/abs/2510.25904)
*Frederico Belcavello,Ely Matos,Arthur Lorenzi,Lisandra Bonoto,Lívia Ruiz,Luiz Fernando Pereira,Victor Herbst,Yulla Navarro,Helen de Andrade Abreu,Lívia Dutra,Tiago Timponi Torrent*

Main category: cs.CL

TL;DR: 本文评估了基于大语言模型(LLM)的语义角色标注器在FrameNet语义标注自动化方面的表现，比较了手动、自动和半自动三种标注方式。


<details>
  <summary>Details</summary>
Motivation: 随着LLM应用广泛用于语言资源和数据集的创建，缺乏对其性能和影响的全面评估，尤其是在视角化自然语言处理方法下。

Method: 通过比较三种实验环境下的标注时间、覆盖率和多样性，评估半自动FrameNet风格的语义标注方法。

Result: 半自动标注在框架多样性和标注覆盖率方面优于仅人工标注，而自动标注在所有指标上表现较差，除了标注时间更短。

Conclusion: 半自动方法能有效提高语义标注的多样性和效率，是促进语言资源创建的有力工具。

Abstract: The use of LLM-based applications as a means to accelerate and/or substitute
human labor in the creation of language resources and dataset is a reality.
Nonetheless, despite the potential of such tools for linguistic research,
comprehensive evaluation of their performance and impact on the creation of
annotated datasets, especially under a perspectivized approach to NLP, is still
missing. This paper contributes to reduction of this gap by reporting on an
extensive evaluation of the (semi-)automatization of FrameNet-like semantic
annotation by the use of an LLM-based semantic role labeler. The methodology
employed compares annotation time, coverage and diversity in three experimental
settings: manual, automatic and semi-automatic annotation. Results show that
the hybrid, semi-automatic annotation setting leads to increased frame
diversity and similar annotation coverage, when compared to the human-only
setting, while the automatic setting performs considerably worse in all
metrics, except for annotation time.

</details>


### [13] [RECAP: Reproducing Copyrighted Data from LLMs Training with an Agentic Pipeline](https://arxiv.org/abs/2510.25941)
*André V. Duarte,Xuying li,Bin Zeng,Arlindo L. Oliveira,Lei Li,Zhuo Li*

Main category: cs.CL

TL;DR: 提出了一种名为RECAP的多轮反馈机制，用于诱导大语言模型记忆训练数据的利率输出，显著提升了文本重现的质量。


<details>
  <summary>Details</summary>
Motivation: 无法直接检查大语言模型的训练数据，如何验证模型记忆了哪些内容？作者认为模型主动重现训练内容是最有力的证据，因此设计了RECAP方法。

Method: 设计了一个反馈驱动的循环机制，初始提取后由辅助语言模型评估输出与参考文本的差异，转换成最小修正提示反馈给目标模型，指导后续生成。此外，包含破解拒绝机制的模块以绕过模型的内容限制。

Result: 在包含30多本完整书籍的新基准EchoTrace上测试，RECAP较单次迭代方法表现更优。GPT-4.1提取被版权保护文本的ROUGE-L分数由0.38提升至0.47，增长近24%。

Conclusion: RECAP有效提升了从大语言模型中提取训练数据的准确度，表明多轮反馈和破解模型拒绝策略对探索模型记忆内容有显著帮助。

Abstract: If we cannot inspect the training data of a large language model (LLM), how
can we ever know what it has seen? We believe the most compelling evidence
arises when the model itself freely reproduces the target content. As such, we
propose RECAP, an agentic pipeline designed to elicit and verify memorized
training data from LLM outputs. At the heart of RECAP is a feedback-driven
loop, where an initial extraction attempt is evaluated by a secondary language
model, which compares the output against a reference passage and identifies
discrepancies. These are then translated into minimal correction hints, which
are fed back into the target model to guide subsequent generations. In
addition, to address alignment-induced refusals, RECAP includes a jailbreaking
module that detects and overcomes such barriers. We evaluate RECAP on
EchoTrace, a new benchmark spanning over 30 full books, and the results show
that RECAP leads to substantial gains over single-iteration approaches. For
instance, with GPT-4.1, the average ROUGE-L score for the copyrighted text
extraction improved from 0.38 to 0.47 - a nearly 24% increase.

</details>


### [14] [Revisiting Multilingual Data Mixtures in Language Model Pretraining](https://arxiv.org/abs/2510.25947)
*Negar Foroutan,Paul Teiletche,Ayush Kumar Tarun,Antoine Bosselut*

Main category: cs.CL

TL;DR: 本文研究了多语言预训练大模型中的不同语言数据混合对模型性能的影响，发现合理平衡的多语言数据可以提升模型能力而不损害性能。


<details>
  <summary>Details</summary>
Motivation: 现有观点认为多语言训练可能导致性能权衡，即多语言诅咒，作者希望通过实验验证这些假设。

Method: 训练了1.1B及3B参数的语言模型，在包含25至400种语言的多样化语料上进行预训练，分析语言数量与性能的关系。

Result: 发现英语与多语言数据结合不会减少单语言性能，英语作为枢纽语言带来跨语系好处，且没有明显多语言诅咒现象。

Conclusion: 适当平衡的多语言训练数据能够提升大语言模型能力，且不会导致性能下降，即使在低资源语言环境下也有效。

Abstract: The impact of different multilingual data mixtures in pretraining large
language models (LLMs) has been a topic of ongoing debate, often raising
concerns about potential trade-offs between language coverage and model
performance (i.e., the curse of multilinguality). In this work, we investigate
these assumptions by training 1.1B and 3B parameter LLMs on diverse
multilingual corpora, varying the number of languages from 25 to 400. Our study
challenges common beliefs surrounding multilingual training. First, we find
that combining English and multilingual data does not necessarily degrade the
in-language performance of either group, provided that languages have a
sufficient number of tokens included in the pretraining corpus. Second, we
observe that using English as a pivot language (i.e., a high-resource language
that serves as a catalyst for multilingual generalization) yields benefits
across language families, and contrary to expectations, selecting a pivot
language from within a specific family does not consistently improve
performance for languages within that family. Lastly, we do not observe a
significant "curse of multilinguality" as the number of training languages
increases in models at this scale. Our findings suggest that multilingual data,
when balanced appropriately, can enhance language model capabilities without
compromising performance, even in low-resource settings

</details>


### [15] [Semantic Label Drift in Cross-Cultural Translation](https://arxiv.org/abs/2510.25967)
*Mohsinul Kabir,Tasnim Ahmed,Md Mezbaur Rahman,Polydoros Giannouris,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 本文研究机器翻译中的情感标签漂移问题，揭示文化差异对标签保持的影响。


<details>
  <summary>Details</summary>
Motivation: 探讨机器翻译过程中，由于源语言与目标语言文化差异导致的标签漂移问题，这是以往研究中较少关注的关键因素。

Method: 通过在文化敏感与中性领域进行一系列实验，比较不同机器翻译系统（包括现代大型语言模型）在翻译时的标签漂移情况，并分析文化相似性对标签保持的影响。

Result: 发现（1）机器翻译系统尤其是大型语言模型会引起标签漂移，尤其在文化敏感领域表现显著；（2）大型语言模型编码了文化知识，使用这些知识可能加剧标签漂移；（3）源语和目标语言的文化相似性是标签保持的重要决定因素。

Conclusion: 忽视文化因素的机器翻译会降低标签的准确传递，增加误解和文化冲突的风险，提示未来机器翻译需重视文化对齐以提升翻译质量。

Abstract: Machine Translation (MT) is widely employed to address resource scarcity in
low-resource languages by generating synthetic data from high-resource
counterparts. While sentiment preservation in translation has long been
studied, a critical but underexplored factor is the role of cultural alignment
between source and target languages. In this paper, we hypothesize that
semantic labels are drifted or altered during MT due to cultural divergence.
Through a series of experiments across culturally sensitive and neutral
domains, we establish three key findings: (1) MT systems, including modern
Large Language Models (LLMs), induce label drift during translation,
particularly in culturally sensitive domains; (2) unlike earlier statistical MT
tools, LLMs encode cultural knowledge, and leveraging this knowledge can
amplify label drift; and (3) cultural similarity or dissimilarity between
source and target languages is a crucial determinant of label preservation. Our
findings highlight that neglecting cultural factors in MT not only undermines
label fidelity but also risks misinterpretation and cultural conflict in
downstream applications.

</details>


### [16] [SymCode: A Neurosymbolic Approach to Mathematical Reasoning via Verifiable Code Generation](https://arxiv.org/abs/2510.25975)
*Sina Bagheri Nezhad,Yao Li,Ameeta Agrawal*

Main category: cs.CL

TL;DR: 本文提出了SymCode，一个结合大语言模型与符号计算库SymPy的神经符号框架，通过生成可验证的代码解决复杂数学推理问题，使得数学题解答更准确且易于验证。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂数学推理中表现不佳，生成的文本方案缺乏验证手段，现有方法如Chain of Thought仍存在不可靠性。

Method: 提出SymCode框架，将数学问题转化为基于SymPy库的代码生成任务，利用符号计算实现结果的确定性验证。

Result: 在MATH-500和OlympiadBench等基准测试中，SymCode的准确率提升最高达13.6个百分点，且更节省token，使模型错误从模糊逻辑失误转向透明的程序错误。

Conclusion: 通过基于确定性符号引擎的推理，SymCode提高了复杂数学问题求解的准确性和可信度，是AI在形式化领域迈出的重要一步。

Abstract: Large Language Models (LLMs) often struggle with complex mathematical
reasoning, where prose-based generation leads to unverified and arithmetically
unsound solutions. Current prompting strategies like Chain of Thought still
operate within this unreliable medium, lacking a mechanism for deterministic
verification. To address these limitations, we introduce SymCode, a
neurosymbolic framework that reframes mathematical problem-solving as a task of
verifiable code generation using the SymPy library. We evaluate SymCode on
challenging benchmarks, including MATH-500 and OlympiadBench, demonstrating
significant accuracy improvements of up to 13.6 percentage points over
baselines. Our analysis shows that SymCode is not only more token-efficient but
also fundamentally shifts model failures from opaque logical fallacies towards
transparent, programmatic errors. By grounding LLM reasoning in a deterministic
symbolic engine, SymCode represents a key step towards more accurate and
trustworthy AI in formal domains.

</details>


### [17] [NeuronMM: High-Performance Matrix Multiplication for LLM Inference on AWS Trainium](https://arxiv.org/abs/2510.25977)
*Dinghong Song,Jierui Xu,Weichu Yang,Pengfei Su,Dong Li*

Main category: cs.CL

TL;DR: 本文针对亚马逊Trainium AI加速器，设计了高性能矩阵乘法核，提升了大语言模型推理性能。


<details>
  <summary>Details</summary>
Motivation: Trainium加速器架构特殊，数据布局要求高，优化大语言模型推理的矩阵乘法性能具有挑战。

Method: 基于核融合和创新缓存策略，减少数据移动，最大化SRAM带宽，避免昂贵的矩阵转置。

Result: 在九个数据集和四个大语言模型上，矩阵乘法核性能提升平均1.35倍，推理整体性能提升平均1.66倍。

Conclusion: 通过定制优化技术，显著提高了Trainium上大语言模型推理的矩阵乘法和整体性能，优于AWS现有实现。

Abstract: AI accelerators, customized to AI workloads, provide cost-effective and
high-performance solutions for training and inference. Trainium, an AI
accelerator recently developed by Amazon Web Services (AWS), provides an
attractive option for LLM training and inference through its heterogeneous
architecture. However, leveraging Trainium architecture for high performance
can be challenging because of its systolic array architecture and special
requirement on data layout. In this paper, we design high-performance matrix
multiplication (matmul), a critical compute kernel, for LLM inference on
Trainium. We introduce a series of techniques customized to Trainium based on
kernel fusion and novel caching strategies to reduce data movement across the
software-managed memory hierarchy, maximize SRAM bandwidth, and avoid expensive
matrix transpose. Evaluating with nine datasets and four recent LLMs, we show
that our system largely outperforms the state-of-the-art matmul implemented by
AWS on Trainium: at the level of matmul kernel, it achieves an average 1.35x
speedup (up to 2.22x), which translates to an average 1.66x speedup (up to
2.49x) for end-to-end LLM inference.

</details>


### [18] [AttnCache: Accelerating Self-Attention Inference for LLM Prefill via Attention Cache](https://arxiv.org/abs/2510.25979)
*Dinghong Song,Yuan Feng,Yiwei Wang,Shangye Chen,Cyril Guyot,Filip Blagojevic,Hyeran Jeon,Pengfei Su,Dong Li*

Main category: cs.CL

TL;DR: 本文提出了AttnCache框架，通过缓存和重用相似的自注意力图，加速了大规模语言模型在预填充推理阶段的计算，提高了CPU和GPU上的推理速度。


<details>
  <summary>Details</summary>
Motivation: 实际应用中许多任务只用到模型的预填充阶段，该阶段自注意力计算成为性能瓶颈，现有方法效率不足。

Method: 通过观察语义不同句子可产生相似的注意力图，设计了一个基于注意力图记忆数据库的缓存和相似性搜索机制，重用相似的注意力图以减少计算量。

Result: 在CPU和GPU上分别实现了约1.2x到1.6x的整体加速和2x到3x的注意力计算加速，且几乎无精度损失。

Conclusion: AttnCache有效降低了预填充阶段的计算开销，提升了大规模语言模型推理效率，适用于多种实际应用场景。

Abstract: Large Language Models (LLMs) are widely used in generative applications such
as chatting, code generation, and reasoning. However, many realworld workloads
such as classification, question answering, recommendation, and text embedding
rely solely on the prefill stage of inference, where the model encodes input
sequences without performing autoregressive decoding. In these prefill only
scenarios, the self-attention computation becomes the primary performance
bottleneck due to its quadratic complexity with respect to sequence length. In
this paper, we observe that semantically different sentences often produce
similar attention maps across layers and heads. Building on this insight, we
propose AttnCache, a framework that accelerates the prefill stage of LLM
inference by retrieving and reusing similar attention maps. Based on an
attention map memorization database, AttnCache employs efficient caching and
similarity search techniques to identify and reuse pre-cached attention maps
during inference, thereby reducing the computational overhead of
self-attention. Experimental results show that AttnCache achieves an average of
1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x
attention speedup on GPU, with negligible accuracy degradation.

</details>


### [19] [Supervised Reinforcement Learning: From Expert Trajectories to Step-wise Reasoning](https://arxiv.org/abs/2510.25992)
*Yihe Deng,I-Hung Hsu,Jun Yan,Zifeng Wang,Rujun Han,Gufeng Zhang,Yanfei Chen,Wei Wang,Tomas Pfister,Chen-Yu Lee*

Main category: cs.CL

TL;DR: 本文提出了一种新的训练框架——监督强化学习（SRL），解决小型大语言模型在多步推理问题上的学习困难。


<details>
  <summary>Details</summary>
Motivation: 现有的小型开源LLM在多步推理任务中表现不佳，监督微调容易过拟合长演示，强化学习难以采样到正确解。

Method: SRL将问题解决重构为生成逻辑"动作"序列，训练模型在每步生成动作前输出推理过程，并通过与专家动作的相似度提供平滑的奖励。

Result: SRL使小模型能够学习以往SFT和RLVR无法解决的复杂问题，且结合SRL先训再用RLVR微调能获得最佳性能。

Conclusion: SRL不仅提升了小模型多步推理能力，还能有效推广至软件工程等任务，是一个强大且多用途的推理训练框架。

Abstract: Large Language Models (LLMs) often struggle with problems that require
multi-step reasoning. For small-scale open-source models, Reinforcement
Learning with Verifiable Rewards (RLVR) fails when correct solutions are rarely
sampled even after many attempts, while Supervised Fine-Tuning (SFT) tends to
overfit long demonstrations through rigid token-by-token imitation. To address
this gap, we propose Supervised Reinforcement Learning (SRL), a framework that
reformulates problem solving as generating a sequence of logical "actions". SRL
trains the model to generate an internal reasoning monologue before committing
to each action. It provides smoother rewards based on the similarity between
the model's actions and expert actions extracted from the SFT dataset in a
step-wise manner. This supervision offers richer learning signals even when all
rollouts are incorrect, while encouraging flexible reasoning guided by expert
demonstrations. As a result, SRL enables small models to learn challenging
problems previously unlearnable by SFT or RLVR. Moreover, initializing training
with SRL before refining with RLVR yields the strongest overall performance.
Beyond reasoning benchmarks, SRL generalizes effectively to agentic software
engineering tasks, establishing it as a robust and versatile training framework
for reasoning-oriented LLMs.

</details>


### [20] [PORTool: Tool-Use LLM Training with Rewarded Tree](https://arxiv.org/abs/2510.26020)
*Feijie Wu,Weiwu Zhu,Yuxiang Zhang,Soumya Chatterjee,Jiarong Zhu,Fan Mo,Rodin Luo,Jing Gao*

Main category: cs.CL

TL;DR: 本文提出了一种基于强化学习的工具使用大语言模型训练方法PORTool，提高了模型在多步骤工具调用中的探索能力和表现。


<details>
  <summary>Details</summary>
Motivation: 当前工具使用大语言模型仅模仿固定工具调用路径，缺乏对多种可能解决方案的探索，导致在动态环境中的性能受限。

Method: 采用强化学习，生成多个工具调用轨迹形成树状结构，针对每一步分配奖励，结合轨迹优势训练模型促进探索和正确工具调用。

Result: 在17个工具上进行实验，PORTool在回答准确率和工具调用步数上显著优于其他训练方法，消融实验验证了分步奖励设计的有效性。

Conclusion: PORTool有效促进工具使用大语言模型探索多路径求解，提升了动态环境下的推理能力和最终表现。

Abstract: Current tool-use large language models (LLMs) are trained on static datasets,
enabling them to interact with external tools and perform multi-step,
tool-integrated reasoning, which produces tool-call trajectories. However,
these models imitate how a query is resolved in a generic tool-call routine,
thereby failing to explore possible solutions and demonstrating limited
performance in an evolved, dynamic tool-call environment. In this work, we
propose PORTool, a reinforcement learning (RL) method that encourages a
tool-use LLM to explore various trajectories yielding the correct answer.
Specifically, this method starts with generating multiple rollouts for a given
query, and some of them share the first few tool-call steps, thereby forming a
tree-like structure. Next, we assign rewards to each step, based on its ability
to produce a correct answer and make successful tool calls. A shared step
across different trajectories receives the same reward, while different steps
under the same fork receive different rewards. Finally, these step-wise rewards
are used to calculate fork-relative advantages, blended with
trajectory-relative advantages, to train the LLM for tool use. The experiments
utilize 17 tools to address user queries, covering both time-sensitive and
time-invariant topics. We conduct ablation studies to systematically justify
the necessity and the design robustness of step-wise rewards. Furthermore, we
compare the proposed PORTool with other training approaches and demonstrate
significant improvements in final accuracy and the number of tool-call steps.

</details>


### [21] [Rethinking Cross-lingual Alignment: Balancing Transfer and Cultural Erasure in Multilingual LLMs](https://arxiv.org/abs/2510.26024)
*HyoJung Han,Sweta Agrawal,Eleftheria Briakou*

Main category: cs.CL

TL;DR: 本文提出了一种新的评估框架转移-本地化平面，系统分析了跨语言对齐中的知识转移与文化擦除的权衡，并通过层级上的激活引导方法，实现了事实知识转移与文化特定知识的有效分离，提升了多语言模型的文化响应能力。


<details>
  <summary>Details</summary>
Motivation: 当前跨语言对齐方法虽提升了多语言知识转移效果，但导致文化擦除，即无法根据语言差异提供文化相关的响应，削弱模型的文化适应性。

Method: 设计了转移-本地化平面评估框架量化知识转移与文化擦除，分析模型内部表示发现不同层对两者有不同的调控能力，进而提出在推理时对不同层施加目标激活引导（Surgical Steering）以解耦两者。

Result: 实验证明，现有跨语言对齐方法虽提升了事实知识转移，但均伴随文化本地化能力的下降；而Surgical Steering方法有效平衡了知识转移与文化本地化两者，显著克服了传统对齐技术的局限。

Conclusion: 通过层次激活引导方法，能够同时实现多语言模型的知识共享与文化个性化响应，避免文化擦除，提升模型跨语言文化适应性的表现。

Abstract: Cross-lingual alignment (CLA) aims to align multilingual representations,
enabling Large Language Models (LLMs) to seamlessly transfer knowledge across
languages. While intuitive, we hypothesize, this pursuit of representational
convergence can inadvertently cause "cultural erasure", the functional loss of
providing culturally-situated responses that should diverge based on the query
language. In this work, we systematically analyze this trade-off by introducing
a holistic evaluation framework, the transfer-localization plane, which
quantifies both desirable knowledge transfer and undesirable cultural erasure.
Using this framework, we re-evaluate recent CLA approaches and find that they
consistently improve factual transfer at the direct cost of cultural
localization across all six languages studied. Our investigation into the
internal representations of these models reveals a key insight: universal
factual transfer and culturally-specific knowledge are optimally steerable at
different model layers. Based on this finding, we propose Surgical Steering, a
novel inference-time method that disentangles these two objectives. By applying
targeted activation steering to distinct layers, our approach achieves a better
balance between the two competing dimensions, effectively overcoming the
limitations of current alignment techniques.

</details>


### [22] [Artificial Intelligence-Enabled Analysis of Radiology Reports: Epidemiology and Consequences of Incidental Thyroid Findings](https://arxiv.org/abs/2510.26032)
*Felipe Larios,Mariana Borras-Osorio,Yuqi Wu,Ana Gabriela Claros,David Toro-Tobon,Esteban Cabezas,Ricardo Loor-Torres,Maria Mateo Chavez,Kerly Guevara Maldonado,Luis Vilatuna Andrango,Maria Lizarazo Jimenez,Ivan Mateo Alzamora,Misk Al Zahidy,Marcelo Montero,Ana Cristina Proano,Cristian Soto Jacome,Jungwei W. Fan,Oscar J. Ponce-Ponte,Megan E. Branda,Naykky Singh Ospina,Juan P. Brito*

Main category: cs.CL

TL;DR: 本研究利用基于Transformer的NLP技术识别非甲状腺影像学检查中的偶然甲状腺发现（ITFs），分析其发生率、结节特征及临床结果，发现ITFs较常见，且与甲状腺结节诊断、活检、手术及癌症诊断相关。


<details>
  <summary>Details</summary>
Motivation: 随着影像学检查的普及，非专门针对甲状腺的影像中偶然发现甲状腺异常的现象变得越来越普遍，但其流行率、特点及临床影响尚不明确，因此需要一个自动化方法来识别并分析这些发现。

Method: 开发并验证了一种基于Transformer的自然语言处理管线，用于从多种影像报告中自动识别ITFs及提取结节特征，分析115,683例成人患者的回顾性队列数据，并运用逻辑回归评估相关因素。

Result: 在115,683名患者中，7.8%发现了ITFs，绝大多数为结节。ITFs在女性、老年人、高BMI及肿瘤科与内科影像中更常见。结节特征记录不足。ITFs患者更可能接受结节诊断、活检、甲状腺切除及癌症诊断。发现的癌症多为乳头状且体积较小。

Conclusion: ITFs普遍存在并显著促使小型低风险甲状腺癌的检测，提示ITFs在甲状腺癌过度诊断中的角色，强调需标准化报告及更有选择性的随访管理。

Abstract: Importance Incidental thyroid findings (ITFs) are increasingly detected on
imaging performed for non-thyroid indications. Their prevalence, features, and
clinical consequences remain undefined. Objective To develop, validate, and
deploy a natural language processing (NLP) pipeline to identify ITFs in
radiology reports and assess their prevalence, features, and clinical outcomes.
Design, Setting, and Participants Retrospective cohort of adults without prior
thyroid disease undergoing thyroid-capturing imaging at Mayo Clinic sites from
July 1, 2017, to September 30, 2023. A transformer-based NLP pipeline
identified ITFs and extracted nodule characteristics from image reports from
multiple modalities and body regions. Main Outcomes and Measures Prevalence of
ITFs, downstream thyroid ultrasound, biopsy, thyroidectomy, and thyroid cancer
diagnosis. Logistic regression identified demographic and imaging-related
factors. Results Among 115,683 patients (mean age, 56.8 [SD 17.2] years; 52.9%
women), 9,077 (7.8%) had an ITF, of which 92.9% were nodules. ITFs were more
likely in women, older adults, those with higher BMI, and when imaging was
ordered by oncology or internal medicine. Compared with chest CT, ITFs were
more likely via neck CT, PET, and nuclear medicine scans. Nodule
characteristics were poorly documented, with size reported in 44% and other
features in fewer than 15% (e.g. calcifications). Compared with patients
without ITFs, those with ITFs had higher odds of thyroid nodule diagnosis,
biopsy, thyroidectomy and thyroid cancer diagnosis. Most cancers were
papillary, and larger when detected after ITFs vs no ITF. Conclusions ITFs were
common and strongly associated with cascades leading to the detection of small,
low-risk cancers. These findings underscore the role of ITFs in thyroid cancer
overdiagnosis and the need for standardized reporting and more selective
follow-up.

</details>


### [23] [QCoder Benchmark: Bridging Language Generation and Quantum Hardware through Simulator-Based Feedback](https://arxiv.org/abs/2510.26101)
*Taku Mikuriya,Tatsuya Ishigaki,Masayuki Kawarada,Shunya Minami,Tadashi Kadowaki,Yohichi Suzuki,Soshun Naito,Shunya Takata,Takumi Kato,Tamotsu Basseda,Reo Yamada,Hiroya Takamura*

Main category: cs.CL

TL;DR: 本文介绍了QCoder基准测试框架，用于评估大语言模型在量子编程任务中的表现，结合量子模拟器环境和人类竞赛代码进行综合评测。


<details>
  <summary>Details</summary>
Motivation: 现有自动编程生成主要聚焦于自然语言与编程逻辑，而对需与硬件设备交互的领域如量子编程缺乏深入研究。

Method: 构建支持量子模拟器反馈（如电路深度、执行时间、错误分类）的评测平台，并引入真实竞赛中人类编写的代码作为对比，进行定量与定性分析。

Result: 实验表明，先进模型如GPT-4o准确率仅18.97%，而基于推理的模型o3性能优异，准确率达78%，超过人类代码平均成功率39.98%。

Conclusion: QCoder基准测试有效衡量量子编程代码生成的难度，推动对硬件交互领域大语言模型能力的进一步研究。

Abstract: Large language models (LLMs) have increasingly been applied to automatic
programming code generation. This task can be viewed as a language generation
task that bridges natural language, human knowledge, and programming logic.
However, it remains underexplored in domains that require interaction with
hardware devices, such as quantum programming, where human coders write Python
code that is executed on a quantum computer. To address this gap, we introduce
QCoder Benchmark, an evaluation framework that assesses LLMs on quantum
programming with feedback from simulated hardware devices. Our benchmark offers
two key features. First, it supports evaluation using a quantum simulator
environment beyond conventional Python execution, allowing feedback of
domain-specific metrics such as circuit depth, execution time, and error
classification, which can be used to guide better generation. Second, it
incorporates human-written code submissions collected from real programming
contests, enabling both quantitative comparisons and qualitative analyses of
LLM outputs against human-written codes. Our experiments reveal that even
advanced models like GPT-4o achieve only around 18.97% accuracy, highlighting
the difficulty of the benchmark. In contrast, reasoning-based models such as o3
reach up to 78% accuracy, outperforming averaged success rates of human-written
codes (39.98%). We release the QCoder Benchmark dataset and public evaluation
API to support further research.

</details>


### [24] [Reasoning Path Divergence: A New Metric and Curation Strategy to Unlock LLM Diverse Thinking](https://arxiv.org/abs/2510.26122)
*Feng Ju,Zeyu Qin,Rui Min,Zhitao He,Lingpeng Kong,Yi R. Fung*

Main category: cs.CL

TL;DR: 本文提出了一种 "一题多解"（1PNS）训练范式，利用推理路径差异（RPD）指标增加大语言模型推理多样性，从而提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 传统的 "一题一解"（1P1S）训练限制了模型推理路径的多样性，导致输出多样性不足，制约了模型推理能力提升。

Method: 提出了1PNS训练，采用推理路径差异（RPD）作为度量多步骤推理语义差异的指标，利用RPD选取最大多样性的解集进行微调。

Result: 基于RPD的训练使模型输出更为多样，pass@16指标相比强基线提升2.80%，在AIME24数据集提升4.99%，证明了1PNS能够增强测试时缩放的效果。

Conclusion: 1PNS训练范式通过增加训练中的推理多样性，有效缓解了低多样性瓶颈，显著提升了大语言模型的推理性能。

Abstract: While Test-Time Scaling (TTS) has proven effective in improving the reasoning
ability of large language models (LLMs), low diversity in model outputs often
becomes a bottleneck; this is partly caused by the common "one problem, one
solution" (1P1S) training practice, which provides a single canonical answer
and can push models toward a narrow set of reasoning paths. To address this, we
propose a "one problem, multiple solutions" (1PNS) training paradigm that
exposes the model to a variety of valid reasoning trajectories and thus
increases inference diversity. A core challenge for 1PNS is reliably measuring
semantic differences between multi-step chains of thought, so we introduce
Reasoning Path Divergence (RPD), a step-level metric that aligns and scores
Long Chain-of-Thought solutions to capture differences in intermediate
reasoning. Using RPD, we curate maximally diverse solution sets per problem and
fine-tune Qwen3-4B-Base. Experiments show that RPD-selected training yields
more varied outputs and higher pass@k, with an average +2.80% gain in pass@16
over a strong 1P1S baseline and a +4.99% gain on AIME24, demonstrating that
1PNS further amplifies the effectiveness of TTS. Our code is available at
https://github.com/fengjujf/Reasoning-Path-Divergence .

</details>


### [25] [On the Influence of Discourse Relations in Persuasive Texts](https://arxiv.org/abs/2510.26124)
*Nawar Turk,Sevag Kaspar,Leila Kosseim*

Main category: cs.CL

TL;DR: 本论文通过利用大型语言模型和提示工程，探讨了说服技巧与话语关系之间的关系，构建了包含两者标注的银标准数据集，并通过统计分析揭示了关键话语关系在说服文本中的重要作用。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏同时标注说服技巧和话语关系的数据集，限制了两者关系的研究。

Method: 基于SemEval 2023 Task 3数据集，利用四个大型语言模型和多种提示，构建了22类PDTB 3.0话语关系分类器，生成包含说服技巧和话语关系的银标准数据集，并进行统计分析。

Result: 通过建立的银标准数据集和统计分析，发现了六种话语关系（因果、目的、对比、因果+信念、让步、条件）在说服文本中特别重要，尤其关联于特定说服技巧如夸张、重复等。

Conclusion: 研究结果丰富了对说服文本话语结构的理解，有助于在线宣传、错误信息的检测及高效沟通方法的研究。

Abstract: This paper investigates the relationship between Persuasion Techniques (PTs)
and Discourse Relations (DRs) by leveraging Large Language Models (LLMs) and
prompt engineering. Since no dataset annotated with both PTs and DRs exists, we
took the SemEval 2023 Task 3 dataset labelled with 19 PTs as a starting point
and developed LLM-based classifiers to label each instance of the dataset with
one of the 22 PDTB 3.0 level-2 DRs. In total, four LLMs were evaluated using 10
different prompts, resulting in 40 unique DR classifiers. Ensemble models using
different majority-pooling strategies were used to create 5 silver datasets of
instances labelled with both persuasion techniques and level-2 PDTB senses. The
silver dataset sizes vary from 1,281 instances to 204 instances, depending on
the majority pooling technique used. Statistical analysis of these silver
datasets shows that six discourse relations (namely Cause, Purpose, Contrast,
Cause+Belief, Concession, and Condition) play a crucial role in persuasive
texts, especially in the use of Loaded Language, Exaggeration/Minimisation,
Repetition and to cast Doubt. This insight can contribute to detecting online
propaganda and misinformation, as well as to our general understanding of
effective communication.

</details>


### [26] [MossNet: Mixture of State-Space Experts is a Multi-Head Attention](https://arxiv.org/abs/2510.26182)
*Shikhar Tuli,James Seale Smith,Haris Jeelani,Chi-Heng Lin,Abhishek Patel,Vasili Ramanishka,Yen-Chang Hsu,Hongxia Jin*

Main category: cs.CL

TL;DR: MossNet是一种创新的多状态空间专家混合架构，模拟了多头注意力机制，在语言建模任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于状态空间模型或门控递归模型的方法通常仅模拟单一注意力头，限制了模型表现力。

Method: 提出MossNet，结合专家混合机制在多层感知机和时间混合状态空间核中实现多个“注意力头”。

Result: 在语言建模及下游任务中，MossNet优于类似规模和数据预算的变压器和状态空间模型架构，大型版本展示出良好的扩展性和性能，同时在手机和GPU设备上的运行效率优越。

Conclusion: MossNet为高效且高性能的递归大语言模型架构提供了一条有吸引力的新方向。

Abstract: Large language models (LLMs) have significantly advanced generative
applications in natural language processing (NLP). Recent trends in model
architectures revolve around efficient variants of transformers or
state-space/gated-recurrent models (SSMs, GRMs). However, prevailing
SSM/GRM-based methods often emulate only a single attention head, potentially
limiting their expressiveness. In this work, we propose MossNet, a novel
mixture-of-state-space-experts architecture that emulates a linear multi-head
attention (MHA). MossNet leverages a mixture-of-experts (MoE) implementation
not only in channel-mixing multi-layered perceptron (MLP) blocks but also in
the time-mixing SSM kernels to realize multiple "attention heads." Extensive
experiments on language modeling and downstream evaluations show that MossNet
outperforms both transformer- and SSM-based architectures of similar model size
and data budgets. Larger variants of MossNet, trained on trillions of tokens,
further confirm its scalability and superior performance. In addition,
real-device profiling on a Samsung Galaxy S24 Ultra and an Nvidia A100 GPU
demonstrate favorable runtime speed and resource usage compared to similarly
sized baselines. Our results suggest that MossNet is a compelling new direction
for efficient, high-performing recurrent LLM architectures.

</details>


### [27] [Similarity-Distance-Magnitude Language Models](https://arxiv.org/abs/2510.26183)
*Allen Schmaltz*

Main category: cs.CL

TL;DR: 本文提出了基于Similarity-Distance-Magnitude（SDM）激活层的语言模型，通过监督微调提升模型生成的高置信区域比例，增强了指令跟随能力，并比传统方法减少了模型放弃预测的情况。


<details>
  <summary>Details</summary>
Motivation: 现有的解码器架构预训练语言模型在指令跟随任务中的置信度和效率存在限制，期望通过引入新的激活层设计和对比学习策略提升模型性能和统计效率。

Method: 采用最终SDM激活层进行二分类，以监督微调方式优化序列预测，结合对比输入编码和训练中动态生成的负样本，调整基底变化估计，提升模型对高置信预测的覆盖率。

Result: 经过微调的SDM语言模型在减少放弃预测（abstentions）方面表现优于现有强基线，提升了统计效率和指令跟随的准确性。

Conclusion: SDM语言模型通过有效利用最终激活层和对比学习机制，成功提升了生成质量和模型置信度，展现出在改进指令跟随任务中的潜力。

Abstract: We introduce Similarity-Distance-Magnitude (SDM) language models (LMs), which
are sequence prediction models fine-tuned to maximize the proportion of
generations in the well-calibrated, high-probability region partitioned by a
final-layer SDM activation layer used for binary classification of
instruction-following. We demonstrate that existing pre-trained decoder-only
Transformer LMs can be readily converted into SDM LMs via supervised
fine-tuning, using the final-layer SDM activation layer during training to
estimate a change-of-base for a supervised next-token loss over a contrastive
input encoding scheme, with additional hard negative examples generated online
during training. This results in reduced abstentions (i.e., improved
statistical efficiency) compared to strong supervised baselines.

</details>


### [28] [RCScore: Quantifying Response Consistency in Large Language Models](https://arxiv.org/abs/2510.26193)
*Dongjun Jang,Youngchae Ahn,Hyopil Shin*

Main category: cs.CL

TL;DR: 本文提出了RCScore框架，通过多样化指令风格评估大型语言模型（LLM）的响应差异，发现指令风格对模型性能有显著影响，并提出了一种基于风格自洽性的评价指标CRS来预测模型准确率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估大多依赖单一指令模板，忽略了指令风格对模型响应的影响，而这在实际应用中至关重要。

Method: 设计RCScore框架，通过多种指令风格系统性转换基准测试问题，测量模型在不同风格下的表现差异；引入CRS指标评估模型响应的风格自洽性；实验涵盖10款LLM和4个推理基准。

Result: 指令风格可导致模型准确率最高变化16.7个百分点；CRS与任务准确率高度相关，说明风格一致性可作为模型可靠性的代理指标；确定性解码提升风格稳定性，模型规模越大风格一致性越高。

Conclusion: RCScore为评估模型对不同指令风格的鲁棒性提供了科学方法，有助于提升模型在实际多样化任务中的可靠性。

Abstract: Current LLM evaluations often rely on a single instruction template,
overlooking models' sensitivity to instruction style-a critical aspect for
real-world deployments. We present RCScore, a multi-dimensional framework
quantifying how instruction formulation affects model responses. By
systematically transforming benchmark problems into multiple instruction
styles, RCScore reveals performance variations undetected by conventional
metrics. Our experiments across ten LLMs on four reasoning benchmarks
demonstrate that instruction style can shift accuracy by up to 16.7% points. We
introduce Cross-Response Similarity (CRS), a method applying RCScore metrics to
measure stylistic self-consistency, and establish its strong correlation with
task accuracy, suggesting consistency as a valuable proxy for model
reliability. Additional findings show that deterministic decoding produces more
stylistically stable outputs, and model scale correlates positively with
cross-style consistency. RCScore offers a principled approach to assess
instruction robustness.

</details>


### [29] [Don't Let It Fade: Preserving Edits in Diffusion Language Models via Token Timestep Allocation](https://arxiv.org/abs/2510.26200)
*Woojin Kim,Jaeyoung Do*

Main category: cs.CL

TL;DR: 该论文提出了一种名为Token Timestep Allocation (TTA)的机制，通过对不同token分配不同的推理时间，实现软性的语义token排序，从而解决扩散语言模型中的更新遗忘问题，提升文本生成的可控性和流畅性。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型虽然能实现细粒度的文本优化，但存在更新遗忘问题——由于统一且不考虑上下文的更新导致早期语义修改被抹除，影响连贯性和流畅性，因此需要引入明确的token排序来增强控制性。

Method: 提出TTA方法，依据token的重要程度分配不同的时间步：关键token较早冻结，存在不确定性的token继续优化。该方法可作为固定或自适应策略，在推理时使用，不依赖训练过程，适用多种扩散语言模型和监督来源。

Result: 在多项任务中表现出显著提升：情感控制任务中准确率提升20%以上，困惑度近乎减半且只需五分之一计算步数；在去毒化任务中最大毒性降低（12.2对比14.5），困惑度也明显减少。

Conclusion: 通过在时间步维度实现软性的语义token排序，有效缓解了更新遗忘，显著提升了扩散语言模型文本生成的稳定性和可控性，验证了TTA作为关键控制手段的有效性。

Abstract: While diffusion language models (DLMs) enable fine-grained refinement, their
practical controllability remains fragile. We identify and formally
characterize a central failure mode called update forgetting, in which uniform
and context agnostic updates induce token level fluctuations across timesteps,
erasing earlier semantic edits and disrupting the cumulative refinement
process, thereby degrading fluency and coherence. As this failure originates in
uniform and context agnostic updates, effective control demands explicit token
ordering. We propose Token Timestep Allocation (TTA), which realizes soft and
semantic token ordering via per token timestep schedules: critical tokens are
frozen early, while uncertain tokens receive continued refinement. This
timestep based ordering can be instantiated as either a fixed policy or an
adaptive policy driven by task signals, thereby supporting a broad spectrum of
refinement strategies. Because it operates purely at inference time, it applies
uniformly across various DLMs and naturally extends to diverse supervision
sources. Empirically, TTA improves controllability and fluency: on sentiment
control, it yields more than 20 percent higher accuracy and nearly halves
perplexity using less than one fifth the steps; in detoxification, it lowers
maximum toxicity (12.2 versus 14.5) and perplexity (26.0 versus 32.0).
Together, these results demonstrate that softened ordering via timestep
allocation is the critical lever for mitigating update forgetting and achieving
stable and controllable diffusion text generation.

</details>


### [30] [What's In My Human Feedback? Learning Interpretable Descriptions of Preference Data](https://arxiv.org/abs/2510.26202)
*Rajiv Movva,Smitha Milli,Sewon Min,Emma Pierson*

Main category: cs.CL

TL;DR: WIMHF是一种利用稀疏自动编码器解释人类反馈数据的方法，能揭示反馈中的偏好特征和数据集上下文，提升数据筛选和个性化效果。


<details>
  <summary>Details</summary>
Motivation: 人类反馈对语言模型的影响复杂且多变，现有方法难以在不预设假设的情况下自动提取有效特征，缺乏对反馈数据编码内容的清晰理解。

Method: 提出WIMHF方法，利用稀疏自动编码器从反馈数据中提取少量可解释偏好特征，分析数据集能测量的偏好及标注者实际表达的偏好。

Result: 在7个数据集上，WIMHF识别出少数人类可解释特征，揭示不同数据集的人类偏好差异及潜在安全风险，如某些用户偏向有害内容。通过重新标注有害样本，显著提升安全性且不损失性能，且支持个性化偏好预测。

Conclusion: WIMHF为实践者提供了一种以人为中心的反馈数据分析工具，有助于更好理解和利用偏好数据，从而改善数据筛选、安全性及个性化调整。

Abstract: Human feedback can alter language models in unpredictable and undesirable
ways, as practitioners lack a clear understanding of what feedback data
encodes. While prior work studies preferences over certain attributes (e.g.,
length or sycophancy), automatically extracting relevant features without
pre-specifying hypotheses remains challenging. We introduce What's In My Human
Feedback? (WIMHF), a method to explain feedback data using sparse autoencoders.
WIMHF characterizes both (1) the preferences a dataset is capable of measuring
and (2) the preferences that the annotators actually express. Across 7
datasets, WIMHF identifies a small number of human-interpretable features that
account for the majority of the preference prediction signal achieved by
black-box models. These features reveal a wide diversity in what humans prefer,
and the role of dataset-level context: for example, users on Reddit prefer
informality and jokes, while annotators in HH-RLHF and PRISM disprefer them.
WIMHF also surfaces potentially unsafe preferences, such as that LMArena users
tend to vote against refusals, often in favor of toxic content. The learned
features enable effective data curation: re-labeling the harmful examples in
Arena yields large safety gains (+37%) with no cost to general performance.
They also allow fine-grained personalization: on the Community Alignment
dataset, we learn annotator-specific weights over subjective features that
improve preference prediction. WIMHF provides a human-centered analysis method
for practitioners to better understand and use preference data.

</details>


### [31] [Towards Global Retrieval Augmented Generation: A Benchmark for Corpus-Level Reasoning](https://arxiv.org/abs/2510.26205)
*Qi Luo,Xiaonan Li,Tingshuo Fan,Xinchi Chen,Xipeng Qiu*

Main category: cs.CL

TL;DR: 本文提出了GlobalQA基准，用于评估跨文档整体信息检索生成（Global RAG）能力，并提出了多工具协同的GlobalRAG框架显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法多针对局部信息检索，难以应对需要跨全集合信息聚合的全局任务。

Method: 设计了包含计数、极值查询、排序和Top-k提取四类任务的GlobalQA基准，并提出GlobalRAG框架，通过分块检索、智能过滤和汇总模块提升整体推理准确率。

Result: 现有方法在全局任务上的表现较差，最强基线F1仅1.51，而GlobalRAG在Qwen2.5-14B模型上实现了6.63 F1，大幅优于基线。

Conclusion: GlobalRAG有效改善了全局检索生成任务性能，验证了多工具协同及结构一致性维护的重要性。

Abstract: Retrieval-augmented generation (RAG) has emerged as a leading approach to
reducing hallucinations in large language models (LLMs). Current RAG evaluation
benchmarks primarily focus on what we call local RAG: retrieving relevant
chunks from a small subset of documents to answer queries that require only
localized understanding within specific text chunks. However, many real-world
applications require a fundamentally different capability -- global RAG --
which involves aggregating and analyzing information across entire document
collections to derive corpus-level insights (for example, "What are the top 10
most cited papers in 2023?"). In this paper, we introduce GlobalQA -- the first
benchmark specifically designed to evaluate global RAG capabilities, covering
four core task types: counting, extremum queries, sorting, and top-k
extraction. Through systematic evaluation across different models and
baselines, we find that existing RAG methods perform poorly on global tasks,
with the strongest baseline achieving only 1.51 F1 score. To address these
challenges, we propose GlobalRAG, a multi-tool collaborative framework that
preserves structural coherence through chunk-level retrieval, incorporates
LLM-driven intelligent filters to eliminate noisy documents, and integrates
aggregation modules for precise symbolic computation. On the Qwen2.5-14B model,
GlobalRAG achieves 6.63 F1 compared to the strongest baseline's 1.51 F1,
validating the effectiveness of our method.

</details>


### [32] [Pragmatic Theories Enhance Understanding of Implied Meanings in LLMs](https://arxiv.org/abs/2510.26253)
*Takuma Sato,Seiya Kawano,Koichiro Yoshino*

Main category: cs.CL

TL;DR: 本文提出通过向语言模型提示语用学理论，提高其理解隐含意义的能力，在实验中获得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 语言模型在理解人类语言中的隐含意义时尚存在困难，本文旨在提升模型的语用理解能力。

Method: 通过将语用学理论（如Grice语用学和相关性理论）概要作为提示，指导模型进行逐步推理以完成隐含意义理解任务。

Result: 实验显示，提出的方法相比基线提升了最高9.6%的分数，且仅提及语用学理论名称也能带来约1-3%的性能提升。

Conclusion: 利用语用学理论作为提示词的方式可以有效增强语言模型推理隐含意义的能力，是一种高效的上下文学习策略。

Abstract: The ability to accurately interpret implied meanings plays a crucial role in
human communication and language use, and language models are also expected to
possess this capability. This study demonstrates that providing language models
with pragmatic theories as prompts is an effective in-context learning approach
for tasks to understand implied meanings. Specifically, we propose an approach
in which an overview of pragmatic theories, such as Gricean pragmatics and
Relevance Theory, is presented as a prompt to the language model, guiding it
through a step-by-step reasoning process to derive a final interpretation.
Experimental results showed that, compared to the baseline, which prompts
intermediate reasoning without presenting pragmatic theories (0-shot
Chain-of-Thought), our methods enabled language models to achieve up to 9.6\%
higher scores on pragmatic reasoning tasks. Furthermore, we show that even
without explaining the details of pragmatic theories, merely mentioning their
names in the prompt leads to a certain performance improvement (around 1-3%) in
larger models compared to the baseline.

</details>


### [33] [Language Models Are Borrowing-Blind: A Multilingual Evaluation of Loanword Identification across 10 Languages](https://arxiv.org/abs/2510.26254)
*Mérilin Sousa Silva,Sina Ahmadi*

Main category: cs.CL

TL;DR: 本论文探讨了预训练语言模型（包括大型语言模型）是否能够识别借词，结果表明这些模型在区分借词与本土词汇方面表现较差，且倾向于借词。


<details>
  <summary>Details</summary>
Motivation: 语言中借词的识别对保护少数民族语言及其词汇具有重要意义，但现有NLP模型是否具备此能力尚未明确。

Method: 在包含10种语言的数据上，评估多个预训练语言模型，测试它们在借词识别任务中的表现。

Result: 模型在借词与本土词汇区分任务中表现较差，且普遍显示出对借词的偏好。

Conclusion: 当前NLP模型在借词识别上存在局限，这对少数民族语言的保护和相关NLP工具的开发提出挑战。

Abstract: Throughout language history, words are borrowed from one language to another
and gradually become integrated into the recipient's lexicon. Speakers can
often differentiate these loanwords from native vocabulary, particularly in
bilingual communities where a dominant language continuously imposes lexical
items on a minority language. This paper investigates whether pretrained
language models, including large language models, possess similar capabilities
for loanword identification. We evaluate multiple models across 10 languages.
Despite explicit instructions and contextual information, our results show that
models perform poorly in distinguishing loanwords from native ones. These
findings corroborate previous evidence that modern NLP systems exhibit a bias
toward loanwords rather than native equivalents. Our work has implications for
developing NLP tools for minority languages and supporting language
preservation in communities under lexical pressure from dominant languages.

</details>


### [34] [Distilling Multilingual Vision-Language Models: When Smaller Models Stay Multilingual](https://arxiv.org/abs/2510.26271)
*Sukrit Sriratanawilai,Jhayahgrit Thongwat,Romrawin Chumpu,Patomporn Payoungkhamdee,Sarana Nutanong,Peerat Limkonchotiwat*

Main category: cs.CL

TL;DR: 本文对五种知识蒸馏方法在多语言视觉语言模型中进行对比研究，探索其在模型压缩下的表现差异。


<details>
  <summary>Details</summary>
Motivation: 多语言视觉语言模型在不同语言上的性能不均衡，且模型缩小后该问题更明显，知识蒸馏在多语言环境下应用较少。

Method: 对CLIP和SigLIP2模型采用五种知识蒸馏方案，评估其在视觉检索和视觉问答任务上的表现，分析跨语言表示一致性及下游任务稳定性。

Result: 部分蒸馏方法在模型尺寸减半的情况下仍能保持甚至提升多语言检索的鲁棒性，而其他方法则无法保证多任务的稳定性。

Conclusion: 知识蒸馏在多语言视觉语言模型压缩中展示出潜力，但设计选择对性能影响显著，单纯看总体准确率不足以评价方法效果。

Abstract: Vision-language models (VLMs) exhibit uneven performance across languages, a
problem that is often exacerbated when the model size is reduced. While
Knowledge distillation (KD) demonstrates promising results in transferring
knowledge from larger to smaller VLMs, applying KD in multilingualism is an
underexplored area. This paper presents a controlled empirical study of KD
behavior across five distillation approaches, isolating their effects on
cross-lingual representation consistency and downstream performance stability
under model compression. We study five distillation formulations across CLIP
and SigLIP2, and evaluate them on in-domain retrieval and out-of-domain visual
QA. We find that some configurations preserve or even improve multilingual
retrieval robustness despite halving model size, but others fail to maintain
cross-task stability, exposing design-sensitive trade-offs that aggregate
accuracy alone does not reveal.

</details>


### [35] [Do LLMs Signal When They're Right? Evidence from Neuron Agreement](https://arxiv.org/abs/2510.26277)
*Kang Chen,Yaoning Wang,Kai Xiong,Zhuoka Feng,Wenhe Sun,Haotian Chen,Yixin Cao*

Main category: cs.CL

TL;DR: 本文提出了一种基于神经元激活的无监督解码方法，显著提升大语言模型的推理能力并实现早期停止，降低计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有的样本评估集成方法主要依赖外部信号（如token概率等）来评分，信号经过训练后可能校准不准，且无法充分利用模型内部复杂动态。

Method: 通过分析神经元激活模式，提出Neuron Agreement Decoding（NAD）方法，利用激活稀疏性和跨样本神经元一致性来选择最佳生成结果，无需依赖外部评分信号。

Result: NAD在数学和科学基准测试中达到多数投票效果，在开放式编码测试中优于现有方法，同时通过提前剪枝无效率生成路径，减少了99%的token使用量且保持生成质量。

Conclusion: 内部神经元激活信号为无标签集成解码提供了可靠、高效的指导，推动了生成质量提升和资源节约。

Abstract: Large language models (LLMs) commonly boost reasoning via
sample-evaluate-ensemble decoders, achieving label free gains without ground
truth. However, prevailing strategies score candidates using only external
outputs such as token probabilities, entropies, or self evaluations, and these
signals can be poorly calibrated after post training. We instead analyze
internal behavior based on neuron activations and uncover three findings: (1)
external signals are low dimensional projections of richer internal dynamics;
(2) correct responses activate substantially fewer unique neurons than
incorrect ones throughout generation; and (3) activations from correct
responses exhibit stronger cross sample agreement, whereas incorrect ones
diverge. Motivated by these observations, we propose Neuron Agreement Decoding
(NAD), an unsupervised best-of-N method that selects candidates using
activation sparsity and cross sample neuron agreement, operating solely on
internal signals and without requiring comparable textual outputs. NAD enables
early correctness prediction within the first 32 generated tokens and supports
aggressive early stopping. Across math and science benchmarks with verifiable
answers, NAD matches majority voting; on open ended coding benchmarks where
majority voting is inapplicable, NAD consistently outperforms Avg@64. By
pruning unpromising trajectories early, NAD reduces token usage by 99% with
minimal loss in generation quality, showing that internal signals provide
reliable, scalable, and efficient guidance for label free ensemble decoding.

</details>


### [36] [Unravelling the Mechanisms of Manipulating Numbers in Language Models](https://arxiv.org/abs/2510.26285)
*Michal Štefánik,Timothee Mickus,Marek Kadlčík,Bertram Højer,Michal Spiegel,Raúl Vázquez,Aman Sinha,Josef Kuchař,Philipp Mondorf*

Main category: cs.CL

TL;DR: 不同大型语言模型对数字输入有相似且准确的嵌入表示，尽管输出时存在错误。


<details>
  <summary>Details</summary>
Motivation: 解释为何语言模型在处理数字时虽然学到准确的表示，但仍会产生错误输出。

Method: 分析多个大型语言模型对数字的表示与处理机制，量化其准确性，并追踪错误产生的层。

Result: 发现各模型对数字的表示是系统性、高准确且通用的，可以构建通用探针定位错误。

Conclusion: 揭示了预训练大型语言模型处理数字的基本机制，并强调更精准探针技术对改进模型架构的潜力。

Abstract: Recent work has shown that different large language models (LLMs) converge to
similar and accurate input embedding representations for numbers. These
findings conflict with the documented propensity of LLMs to produce erroneous
outputs when dealing with numeric information. In this work, we aim to explain
this conflict by exploring how language models manipulate numbers and quantify
the lower bounds of accuracy of these mechanisms. We find that despite
surfacing errors, different language models learn interchangeable
representations of numbers that are systematic, highly accurate and universal
across their hidden states and the types of input contexts. This allows us to
create universal probes for each LLM and to trace information -- including the
causes of output errors -- to specific layers. Our results lay a fundamental
understanding of how pre-trained LLMs manipulate numbers and outline the
potential of more accurate probing techniques in addressed refinements of LLMs'
architectures.

</details>


### [37] [Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in Web Games](https://arxiv.org/abs/2510.26298)
*Jingran Zhang,Ning Li,Justin Cui*

Main category: cs.CL

TL;DR: 本文评估了OpenAI的ChatGPT Atlas在浏览器游戏中的网页交互能力，发现其在逻辑推理游戏表现出色，但在需要实时反应和操作的游戏中表现较差。


<details>
  <summary>Details</summary>
Motivation: 虽然ChatGPT Atlas展示了信息检索能力，但其在动态、交互环境中的表现尚未充分探索，因此通过实际游戏测试其网页交互性能。

Method: 选用Google的T-Rex Runner、数独、Flappy Bird和Stein.world等浏览器游戏作为测试场景，利用游戏得分作为量化指标评估Atlas的表现。

Result: Atlas在数独等逻辑推理任务中表现优异，完成速度显著快于人类基线；但在需要精确时机和操作的实时游戏中表现不佳，常卡在初始关卡。

Conclusion: Atlas具备较强的分析处理能力，但在需要实时互动的动态网页环境中仍存在显著局限。

Abstract: OpenAI's ChatGPT Atlas introduces new capabilities for web interaction,
enabling the model to analyze webpages, process user intents, and execute
cursor and keyboard inputs directly within the browser. While its capacity for
information retrieval tasks has been demonstrated, its performance in dynamic,
interactive environments remains less explored. In this study, we conduct an
early evaluation of Atlas's web interaction capabilities using browser-based
games as test scenarios, including Google's T-Rex Runner, Sudoku, Flappy Bird,
and Stein.world. We employ in-game performance scores as quantitative metrics
to assess performance across different task types. Our results show that Atlas
performs strongly in logical reasoning tasks like Sudoku, completing puzzles
significantly faster than human baselines, but struggles substantially in
real-time games requiring precise timing and motor control, often failing to
progress beyond initial obstacles. These findings suggest that while Atlas
demonstrates capable analytical processing, there remain notable limitations in
dynamic web environments requiring real-time interaction. The website of our
project can be found at https://atlas-game-eval.github.io.

</details>


### [38] [SCRIBE: Structured Chain Reasoning for Interactive Behaviour Explanations using Tool Calling](https://arxiv.org/abs/2510.26322)
*Fares Fawzi,Vinitra Swamy,Dominik Glandorf,Tanya Nazaretsky,Tanja Käser*

Main category: cs.CL

TL;DR: SCRIBE是一个专为教育场景设计的小型开源语言模型框架，支持多跳推理和工具辅助，能在本地运行并生成有效且合规的学生反馈。


<details>
  <summary>Details</summary>
Motivation: 针对教育领域中语言模型面临的隐私、计算资源有限及反馈需具备教学有效性等挑战，需开发能在本地运行且具备有效反馈能力的小型模型。

Method: 提出SCRIBE框架，结合领域工具和自反推理流程，支持迭代推理、工具使用和错误纠正；通过两阶段LoRA微调基于GPT-4o合成数据训练3B和8B规模模型。

Result: 8B-SCRIBE模型在相关性和可操作性等关键指标上表现与更大型模型持平或更优，且学生评价其与GPT-4o及Llama-3.3 70B相当。

Conclusion: SCRIBE框架适合资源受限且需保障隐私的教育应用，证明了小型开源模型在生成有效学生反馈方面的可行性。

Abstract: Language models can be used to provide interactive, personalized student
feedback in educational settings. However, real-world deployment faces three
key challenges: privacy concerns, limited computational resources, and the need
for pedagogically valid responses. These constraints require small, open-source
models that can run locally and reliably ground their outputs in correct
information. We introduce SCRIBE, a framework for multi-hop, tool-augmented
reasoning designed to generate valid responses to student questions about
feedback reports. SCRIBE combines domain-specific tools with a self-reflective
inference pipeline that supports iterative reasoning, tool use, and error
recovery. We distil these capabilities into 3B and 8B models via two-stage LoRA
fine-tuning on synthetic GPT-4o-generated data. Evaluation with a human-aligned
GPT-Judge and a user study with 108 students shows that 8B-SCRIBE models
achieve comparable or superior quality to much larger models in key dimensions
such as relevance and actionability, while being perceived on par with GPT-4o
and Llama-3.3 70B by students. These findings demonstrate the viability of
SCRIBE for low-resource, privacy-sensitive educational applications.

</details>


### [39] [From Amateur to Master: Infusing Knowledge into LLMs via Automated Curriculum Learning](https://arxiv.org/abs/2510.26336)
*Nishit Neema,Srinjoy Mukherjee,Sapan Shah,Gokul Ramakrishnan,Ganesh Venkatesh*

Main category: cs.CL

TL;DR: 本文提出ACER，一种自动课程增强训练方法，通过合成系统的教材式课程，显著提升大语言模型在经济学等专业领域的表现，同时保持其通用能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型虽然在通用任务表现优异，但在经济学和心理学等需深度理解的专业领域表现不足。

Method: ACER先自动生成学科目录和符合Bloom分类法的问答对，构建渐进式难度的综合课程语料，随后进行交叉课程安排的持续预训练。

Result: 在Llama 3.2模型上，ACER在专业领域如微观经济学中提升准确率5个百分点，整体提升3个百分点，且防止遗忘并促进跨领域知识迁移，同时提升知识密集型任务表现。

Conclusion: ACER提供了一种可扩展有效的方案，成功弥补了大语言模型在专业领域的性能缺口，兼顾专精与通用。

Abstract: Large Language Models (LLMs) excel at general tasks but underperform in
specialized domains like economics and psychology, which require deep,
principled understanding. To address this, we introduce ACER (Automated
Curriculum-Enhanced Regimen) that transforms generalist models into domain
experts without sacrificing their broad capabilities. ACER first synthesizes a
comprehensive, textbook-style curriculum by generating a table of contents for
a subject and then creating question-answer (QA) pairs guided by Bloom's
taxonomy. This ensures systematic topic coverage and progressively increasing
difficulty. The resulting synthetic corpus is used for continual pretraining
with an interleaved curriculum schedule, aligning learning across both content
and cognitive dimensions.
  Experiments with Llama 3.2 (1B and 3B) show significant gains in specialized
MMLU subsets. In challenging domains like microeconomics, where baselines
struggle, ACER boosts accuracy by 5 percentage points. Across all target
domains, we observe a consistent macro-average improvement of 3 percentage
points. Notably, ACER not only prevents catastrophic forgetting but also
facilitates positive cross-domain knowledge transfer, improving performance on
non-target domains by 0.7 points. Beyond MMLU, ACER enhances performance on
knowledge-intensive benchmarks like ARC and GPQA by over 2 absolute points,
while maintaining stable performance on general reasoning tasks. Our results
demonstrate that ACER offers a scalable and effective recipe for closing
critical domain gaps in LLMs.

</details>


### [40] [MisSynth: Improving MISSCI Logical Fallacies Classification with Synthetic Data](https://arxiv.org/abs/2510.26345)
*Mykhailo Poliakov,Nadiya Shvai*

Main category: cs.CL

TL;DR: 该论文提出 MisSynth 方法，利用检索增强生成合成谬误样本，微调大语言模型以识别健康相关的错误论证，显著提升模型识别准确率。


<details>
  <summary>Details</summary>
Motivation: 健康相关的错误信息普遍且有害，尤其是那些曲解科学发现的论断，识别此类谬误十分困难。

Method: 论文提出 MisSynth 流水线，结合检索增强生成（RAG）技术生成合成谬误样本，使用这些样本微调大型语言模型（LLM）。

Result: 微调后的 LLaMA 3.1 8B 模型在 MISSCI 测试集上相比原始模型，F1 分数提升超过 35%。

Conclusion: 通过合成数据增强有限标注资源，显著提升了大型语言模型对真实科学错误信息的零样本分类能力，即使在计算资源有限的情况下也有效。

Abstract: Health-related misinformation is very prevalent and potentially harmful. It
is difficult to identify, especially when claims distort or misinterpret
scientific findings. We investigate the impact of synthetic data generation and
lightweight fine-tuning techniques on the ability of large language models
(LLMs) to recognize fallacious arguments using the MISSCI dataset and
framework. In this work, we propose MisSynth, a pipeline that applies
retrieval-augmented generation (RAG) to produce synthetic fallacy samples,
which are then used to fine-tune an LLM model. Our results show substantial
accuracy gains with fine-tuned models compared to vanilla baselines. For
instance, the LLaMA 3.1 8B fine-tuned model achieved an over 35% F1-score
absolute improvement on the MISSCI test split over its vanilla baseline. We
demonstrate that introducing synthetic fallacy data to augment limited
annotated resources can significantly enhance zero-shot LLM classification
performance on real-world scientific misinformation tasks, even with limited
computational resources. The code and synthetic dataset are available on
https://github.com/mxpoliakov/MisSynth.

</details>


### [41] [On the Role of Context for Discourse Relation Classification in Scientific Writing](https://arxiv.org/abs/2510.26354)
*Stephen Wan,Wei Liu,Michael Strube*

Main category: cs.CL

TL;DR: 本论文探讨了利用预训练语言模型和大型语言模型在科学论文中进行话语关系分类，重点分析上下文对该任务的帮助。


<details>
  <summary>Details</summary>
Motivation: 随着生成式人工智能在科学工作流程中的广泛应用，作者希望通过话语层级信息为AI生成的科学论断寻找支持证据。

Method: 采用预训练语言模型和大型语言模型对科学写作中的话语关系分类任务进行初步研究，特别关注上下文信息的作用。

Result: 实验表明话语结构定义的上下文信息对话语关系分类任务普遍有益，且进一步分析了哪些科学话语关系类型最受上下文影响。

Conclusion: 上下文信息在科学论文的话语关系分类中起到积极作用，利用上下文能提升模型性能，为后续利用话语层信息支持AI生成科学论断提供基础。

Abstract: With the increasing use of generative Artificial Intelligence (AI) methods to
support science workflows, we are interested in the use of discourse-level
information to find supporting evidence for AI generated scientific claims. A
first step towards this objective is to examine the task of inferring discourse
structure in scientific writing.
  In this work, we present a preliminary investigation of pretrained language
model (PLM) and Large Language Model (LLM) approaches for Discourse Relation
Classification (DRC), focusing on scientific publications, an under-studied
genre for this task. We examine how context can help with the DRC task, with
our experiments showing that context, as defined by discourse structure, is
generally helpful. We also present an analysis of which scientific discourse
relation types might benefit most from context.

</details>


### [42] [OmniEduBench: A Comprehensive Chinese Benchmark for Evaluating Large Language Models in Education](https://arxiv.org/abs/2510.26422)
*Min Zhang,Hao Chen,Hao Chen,Wenqi Zhang,Didi Zhu,Xin Lin,Bo Jiang,Aimin Zhou,Fei Wu,Kun Kuang*

Main category: cs.CL

TL;DR: 本文介绍了OmniEduBench，一个涉及24.602K中文教育问答对的综合性教育基准，覆盖知识和培养两个维度以及多样化题型，用于评估大型语言模型在教育领域的能力。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型及其基准主要聚焦知识维度，忽视了培养能力的评估，且数据集往往单一、缺少多样性，尤其在中文教育领域问题突出。

Method: 构建了OmniEduBench数据集，包含两个核心维度（知识与培养），共61个学科，涵盖11种常见考试题型，并在11个主流开源和闭源大型语言模型上进行了广泛实验。

Result: 实验结果显示知识维度中仅Gemini-2.5 Pro准确率超过60%，培养维度最佳模型QWQ仍较人类智能落后近30%，体现了模型在教育应用中的明显差距。

Conclusion: OmniEduBench揭示了当前大型语言模型在教育尤其是培养能力方面的不足，强调了提升教育领域应用性能的巨大挑战和改进空间。

Abstract: With the rapid development of large language models (LLMs), various LLM-based
works have been widely applied in educational fields. However, most existing
LLMs and their benchmarks focus primarily on the knowledge dimension, largely
neglecting the evaluation of cultivation capabilities that are essential for
real-world educational scenarios. Additionally, current benchmarks are often
limited to a single subject or question type, lacking sufficient diversity.
This issue is particularly prominent within the Chinese context. To address
this gap, we introduce OmniEduBench, a comprehensive Chinese educational
benchmark. OmniEduBench consists of 24.602K high-quality question-answer pairs.
The data is meticulously divided into two core dimensions: the knowledge
dimension and the cultivation dimension, which contain 18.121K and 6.481K
entries, respectively. Each dimension is further subdivided into 6 fine-grained
categories, covering a total of 61 different subjects (41 in the knowledge and
20 in the cultivation). Furthermore, the dataset features a rich variety of
question formats, including 11 common exam question types, providing a solid
foundation for comprehensively evaluating LLMs' capabilities in education.
Extensive experiments on 11 mainstream open-source and closed-source LLMs
reveal a clear performance gap. In the knowledge dimension, only Gemini-2.5 Pro
surpassed 60\% accuracy, while in the cultivation dimension, the
best-performing model, QWQ, still trailed human intelligence by nearly 30\%.
These results highlight the substantial room for improvement and underscore the
challenges of applying LLMs in education.

</details>


### [43] [1+1>2: A Synergistic Sparse and Low-Rank Compression Method for Large Language Models](https://arxiv.org/abs/2510.26446)
*Zeliang Zong,Kai Zhang,Zheyang Li,Wenming Tan,Ye Ren,Yiyan Zhai,Jilin Hu*

Main category: cs.CL

TL;DR: 本文提出了结合稀疏化与低秩近似的LLM压缩方法SSLC，实现了高效压缩且无性能损失，提升了模型部署效率。


<details>
  <summary>Details</summary>
Motivation: 目前大规模语言模型虽然性能强大，但计算和带宽需求庞大，限制了应用。单独稀疏和低秩压缩方法都有不足，亟需结合两者优点的压缩方法。

Method: 将低秩近似和稀疏优化统一为一个迭代优化问题，通过协同压缩方法SSLC实现模型压缩，无需额外训练。

Result: 在LLaMA和Qwen2.5等7B到70B参数模型上，SSLC压缩后性能不降反升，压缩率达50%，加速超过1.63倍。

Conclusion: SSLC方法有效结合了稀疏与低秩压缩优点，为大规模语言模型的高效部署提供了实用方案。

Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in
language comprehension and generation; however, their widespread adoption is
constrained by substantial bandwidth and computational demands. While pruning
and low-rank approximation have each demonstrated promising performance
individually, their synergy for LLMs remains underexplored. We introduce
\underline{S}ynergistic \underline{S}parse and \underline{L}ow-Rank
\underline{C}ompression (SSLC) methods for LLMs, which leverages the strengths
of both techniques: low-rank approximation compresses the model by retaining
its essential structure with minimal information loss, whereas sparse
optimization eliminates non-essential weights, preserving those crucial for
generalization. Based on theoretical analysis, we first formulate the low-rank
approximation and sparse optimization as a unified problem and solve it by
iterative optimization algorithm. Experiments on LLaMA and Qwen2.5 models
(7B-70B) show that SSLC, without any additional training steps, consistently
surpasses standalone methods, achieving state-of-the-arts results. Notably,
SSLC compresses Qwen2.5 by 50\% with no performance drop and achieves at least
1.63$\times$ speedup, offering a practical solution for efficient LLM
deployment.

</details>


### [44] [Bayesian Network Fusion of Large Language Models for Sentiment Analysis](https://arxiv.org/abs/2510.26484)
*Rasoul Amirzadeh,Dhananjay Thiruvady,Fatemeh Shiri*

Main category: cs.CL

TL;DR: 本文提出了基于贝叶斯网络的大型语言模型融合（BNLF）框架，通过融合FinBERT、RoBERTa和BERTweet三种模型的情感预测，实现了在金融语料上的情感分析性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有领域专用大型语言模型存在透明度低、调优成本高、结果不稳定和计算资源消耗大等问题，需要一种有效融合多模型预测，提升性能并增强解释性的方案。

Method: 提出了一种贝叶斯网络的大型语言模型融合方法（BNLF），通过将多个模型的情感预测作为贝叶斯网络中的概率节点进行后期融合，实现了概率化的情感分类。

Result: BNLF在三个不同人类标注的金融语料库上，准确率较基线模型提升约6%，表现出对数据集变化的鲁棒性。

Conclusion: 基于贝叶斯网络的多模型概率融合方法能够显著提升情感分析性能，并增强模型的可解释性和稳定性，适用于多领域适应性强的情感分类任务。

Abstract: Large language models (LLMs) continue to advance, with an increasing number
of domain-specific variants tailored for specialised tasks. However, these
models often lack transparency and explainability, can be costly to fine-tune,
require substantial prompt engineering, yield inconsistent results across
domains, and impose significant adverse environmental impact due to their high
computational demands. To address these challenges, we propose the Bayesian
network LLM fusion (BNLF) framework, which integrates predictions from three
LLMs, including FinBERT, RoBERTa, and BERTweet, through a probabilistic
mechanism for sentiment analysis. BNLF performs late fusion by modelling the
sentiment predictions from multiple LLMs as probabilistic nodes within a
Bayesian network. Evaluated across three human-annotated financial corpora with
distinct linguistic and contextual characteristics, BNLF demonstrates
consistent gains of about six percent in accuracy over the baseline LLMs,
underscoring its robustness to dataset variability and the effectiveness of
probabilistic fusion for interpretable sentiment classification.

</details>


### [45] [A Multi-agent Large Language Model Framework to Automatically Assess Performance of a Clinical AI Triage Tool](https://arxiv.org/abs/2510.26498)
*Adam E. Flanders,Yifan Peng,Luciano Prevedello,Robyn Ball,Errol Colak,Prahlad Menon,George Shih,Hui-Ming Lin,Paras Lakhani*

Main category: cs.CL

TL;DR: 本研究旨在验证多个开放源代码大型语言模型（LLM）组成的集成模型，是否能比单一LLM更可靠地评估脑出血AI分诊工具的性能。


<details>
  <summary>Details</summary>
Motivation: 脑出血的早期检测对于患者治疗至关重要，传统单一LLM在评估AI分诊工具时存在一定局限，探讨集成多个LLM是否能提升评估准确性和一致性。

Method: 研究使用来自14家医院共29,766例无对比剂头部CT影像和放射报告；使用一个商用脑出血检测AI工具处理CT图像。用八个开源LLM和符合HIPAA的内部GPT-4o模型，通过多示例提示评估报告中脑出血的存在。手工复核1726例，对比模型性能指标并测试三种理想的集成方式。

Result: llama3.3:70b和GPT-4o获得最高AUC（0.78）和平均精度（约0.75）, llama3.3:70b的F1值、召回率、精度和特异性均领先。集成模型在MCC指标上表现优于单一GPT-4o，且不同集成方案间无显著差异。

Conclusion: 中至大规模开源LLM的集成模型提供了比单一LLM更一致、可靠的手段用于回顾性临床AI分诊工具的性能评估。

Abstract: Purpose: The purpose of this study was to determine if an ensemble of
multiple LLM agents could be used collectively to provide a more reliable
assessment of a pixel-based AI triage tool than a single LLM.
  Methods: 29,766 non-contrast CT head exams from fourteen hospitals were
processed by a commercial intracranial hemorrhage (ICH) AI detection tool.
Radiology reports were analyzed by an ensemble of eight open-source LLM models
and a HIPAA compliant internal version of GPT-4o using a single multi-shot
prompt that assessed for presence of ICH. 1,726 examples were manually
reviewed. Performance characteristics of the eight open-source models and
consensus were compared to GPT-4o. Three ideal consensus LLM ensembles were
tested for rating the performance of the triage tool.
  Results: The cohort consisted of 29,766 head CTs exam-report pairs. The
highest AUC performance was achieved with llama3.3:70b and GPT-4o (AUC= 0.78).
The average precision was highest for Llama3.3:70b and GPT-4o (AP=0.75 & 0.76).
Llama3.3:70b had the highest F1 score (0.81) and recall (0.85), greater
precision (0.78), specificity (0.72), and MCC (0.57). Using MCC (95% CI) the
ideal combination of LLMs were: Full-9 Ensemble 0.571 (0.552-0.591), Top-3
Ensemble 0.558 (0.537-0.579), Consensus 0.556 (0.539-0.574), and GPT4o 0.522
(0.500-0.543). No statistically significant differences were observed between
Top-3, Full-9, and Consensus (p > 0.05).
  Conclusion: An ensemble of medium to large sized open-source LLMs provides a
more consistent and reliable method to derive a ground truth retrospective
evaluation of a clinical AI triage tool over a single LLM alone.

</details>


### [46] [Inside CORE-KG: Evaluating Structured Prompting and Coreference Resolution for Knowledge Graphs](https://arxiv.org/abs/2510.26512)
*Dipak Meher,Carlotta Domeniconi*

Main category: cs.CL

TL;DR: 本文针对法律案件文档中的人口走私网络难以分析的问题，提出并分析了基于大型语言模型的CORE-KG框架，显著减少节点重复和噪声，提高知识图谱构建质量。


<details>
  <summary>Details</summary>
Motivation: 人口走私网络复杂且适应性强，法律文档结构混乱、语义复杂，给自动构建知识图谱带来巨大挑战。

Method: CORE-KG框架结合类型感知的共指消解模块和领域引导的结构化提示，改进知识抽取效果。进行了系统消融实验，量化两个关键组件对性能的贡献。

Result: 去掉共指消解会增加节点重复28.32%和噪声4.32%，去掉结构化提示节点重复增加4.34%，噪声增加73.33%。

Conclusion: 共指消解和结构化提示对提升基于LLM的法律文本知识抽取至关重要，为后续设计稳健的抽取流程提供实证支持。

Abstract: Human smuggling networks are increasingly adaptive and difficult to analyze.
Legal case documents offer critical insights but are often unstructured,
lexically dense, and filled with ambiguous or shifting references, which pose
significant challenges for automated knowledge graph (KG) construction. While
recent LLM-based approaches improve over static templates, they still generate
noisy, fragmented graphs with duplicate nodes due to the absence of guided
extraction and coreference resolution. The recently proposed CORE-KG framework
addresses these limitations by integrating a type-aware coreference module and
domain-guided structured prompts, significantly reducing node duplication and
legal noise. In this work, we present a systematic ablation study of CORE-KG to
quantify the individual contributions of its two key components. Our results
show that removing coreference resolution results in a 28.32% increase in node
duplication and a 4.32% increase in noisy nodes, while removing structured
prompts leads to a 4.34% increase in node duplication and a 73.33% increase in
noisy nodes. These findings offer empirical insights for designing robust
LLM-based pipelines for extracting structured representations from complex
legal texts.

</details>


### [47] [Hebrew Diacritics Restoration using Visual Representation](https://arxiv.org/abs/2510.26521)
*Yair Elboher,Yuval Pinter*

Main category: cs.CL

TL;DR: 提出了DIVRIT系统，将希伯来语加元音符号任务视为零样本分类问题，通过视觉语言模型处理文本，实现高准确度的语境相关拼写恢复。


<details>
  <summary>Details</summary>
Motivation: 希伯来语缺少元音符号时语义高度歧义，传统方法依赖复杂的语言学分析，迫切需要更有效的自动加元音方法。

Method: DIVRIT将每个未加元音符号的单词视为分类问题，从候选集选择最合适的元音符号模式，并采用视觉语言模型将文本作为图像输入，直接嵌入元音信息。

Result: 在多种配置下的综合评估显示，DIVRIT能够有效进行元音恢复，在“oracle”模式下准确率较高，架构优化和训练方法显著提升泛化能力。

Conclusion: 视觉表征方法为希伯来语自动元音符号恢复提供了一条准确且无需复杂语言学分析的创新路径，具有广泛应用前景。

Abstract: Diacritics restoration in Hebrew is a fundamental task for ensuring accurate
word pronunciation and disambiguating textual meaning. Despite the language's
high degree of ambiguity when unvocalized, recent machine learning approaches
have significantly advanced performance on this task.
  In this work, we present DIVRIT, a novel system for Hebrew diacritization
that frames the task as a zero-shot classification problem. Our approach
operates at the word level, selecting the most appropriate diacritization
pattern for each undiacritized word from a dynamically generated candidate set,
conditioned on the surrounding textual context. A key innovation of DIVRIT is
its use of a Hebrew Visual Language Model, which processes undiacritized text
as an image, allowing diacritic information to be embedded directly within the
input's vector representation.
  Through a comprehensive evaluation across various configurations, we
demonstrate that the system effectively performs diacritization without relying
on complex, explicit linguistic analysis. Notably, in an ``oracle'' setting
where the correct diacritized form is guaranteed to be among the provided
candidates, DIVRIT achieves a high level of accuracy. Furthermore, strategic
architectural enhancements and optimized training methodologies yield
significant improvements in the system's overall generalization capabilities.
These findings highlight the promising potential of visual representations for
accurate and automated Hebrew diacritization.

</details>


### [48] [The Structure of Relation Decoding Linear Operators in Large Language Models](https://arxiv.org/abs/2510.26543)
*Miranda Anna Christ,Adrián Csiszárik,Gergely Becsó,Dániel Varga*

Main category: cs.CL

TL;DR: 本文研究了变压器语言模型中线性算子的结构，这些算子用于解码具体关系事实，发现它们编码的是共性的语义属性而非具体关系。


<details>
  <summary>Details</summary>
Motivation: 延伸之前对单一关系的线性算子研究，系统地分析多重关系解码器的组织结构及其冗余性。

Method: 构建交叉评估协议，对不同关系的线性解码算子进行交叉应用，并使用三阶张量网络对解码器集合进行压缩。

Result: 发现解码器之间的冗余性高，编码的不是单独关系而是重复出现的粗粒度语义属性，因此算子集合可以被高效压缩。

Conclusion: 变压器语言模型中的线性关系解码更侧重于共享的属性结构而非具体关系，解释了其压缩性和泛化能力。

Abstract: This paper investigates the structure of linear operators introduced in
Hernandez et al. [2023] that decode specific relational facts in transformer
language models. We extend their single-relation findings to a collection of
relations and systematically chart their organization. We show that such
collections of relation decoders can be highly compressed by simple order-3
tensor networks without significant loss in decoding accuracy. To explain this
surprising redundancy, we develop a cross-evaluation protocol, in which we
apply each linear decoder operator to the subjects of every other relation. Our
results reveal that these linear maps do not encode distinct relations, but
extract recurring, coarse-grained semantic properties (e.g., country of capital
city and country of food are both in the country-of-X property). This
property-centric structure clarifies both the operators' compressibility and
highlights why they generalize only to new relations that are semantically
close. Our findings thus interpret linear relational decoding in transformer
language models as primarily property-based, rather than relation-specific.

</details>


### [49] [InfoFlow: Reinforcing Search Agent Via Reward Density Optimization](https://arxiv.org/abs/2510.26575)
*Kun Luo,Hongjin Qian,Zheng Liu,Ziyi Xia,Shitao Xiao,Siqi Bao,Jun Zhao,Kang Liu*

Main category: cs.CL

TL;DR: 本文提出了InfoFlow框架，通过子问题分解、失败引导提示和双代理细化，提升强化学习中奖励密度，显著提高深度搜索任务中代理性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习中深度搜索场景面临奖励稀疏和探索成本高的问题，迫切需要提升单位探索成本的奖励密度。

Method: 1) 子问题分解以提供更密集的学习信号；2) 利用失败引导提示修正停滞轨迹；3) 采用双代理架构减少认知负担和探索成本，提升奖励密度。

Result: InfoFlow在多个深度搜索基准测试中显著优于强基线，使轻量级大型语言模型的表现可与先进私有大型语言模型相媲美。

Conclusion: InfoFlow有效解决奖励密度优化问题，为强化学习中的深度搜索提供了系统化解决方案，提升了代理的搜索效率和性能。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is a promising approach
for enhancing agentic deep search. However, its application is often hindered
by low \textbf{Reward Density} in deep search scenarios, where agents expend
significant exploratory costs for infrequent and often null final rewards. In
this paper, we formalize this challenge as the \textbf{Reward Density
Optimization} problem, which aims to improve the reward obtained per unit of
exploration cost. This paper introduce \textbf{InfoFlow}, a systematic
framework that tackles this problem from three aspects. 1) \textbf{Subproblem
decomposition}: breaking down long-range tasks to assign process rewards,
thereby providing denser learning signals. 2) \textbf{Failure-guided hints}:
injecting corrective guidance into stalled trajectories to increase the
probability of successful outcomes. 3) \textbf{Dual-agent refinement}:
employing a dual-agent architecture to offload the cognitive burden of deep
exploration. A refiner agent synthesizes the search history, which effectively
compresses the researcher's perceived trajectory, thereby reducing exploration
cost and increasing the overall reward density. We evaluate InfoFlow on
multiple agentic search benchmarks, where it significantly outperforms strong
baselines, enabling lightweight LLMs to achieve performance comparable to
advanced proprietary LLMs.

</details>


### [50] [Inference-Cost-Aware Dynamic Tree Construction for Efficient Inference in Large Language Models](https://arxiv.org/abs/2510.26577)
*Yinrong Hong,Zhiquan Tan,Kai Hu*

Main category: cs.CL

TL;DR: 本文提出了一种考虑GPU设备和批量大小的动态树解码方法CAST，在多个任务和模型的测试中，速度提升至传统方法的5.2倍，并超越现有技术5%-20%。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型推理速度受自回归设计和模型规模限制，现有的推测解码方法虽改进速度，但未充分考虑系统变量如GPU和批大小的影响。

Method: 提出CAST动态树解码方法，动态调整树结构，结合GPU配置和批处理规模来优化推理成本。

Result: 在六个任务和六个不同的语言模型上，CAST实现最高5.2倍加速，性能普遍优于现有最先进技术5%-20%。

Conclusion: CAST有效整合系统变量优化推理流程，显著提升大型语言模型的推理速度与效率，优于现有方法。

Abstract: Large Language Models (LLMs) face significant inference latency challenges
stemming from their autoregressive design and large size. To address this,
speculative decoding emerges as a solution, enabling the simultaneous
generation and validation of multiple tokens. While recent approaches like
EAGLE-2 and EAGLE-3 improve speculative decoding using dynamic tree structures,
they often neglect the impact of crucial system variables such as GPU devices
and batch sizes.
  Therefore, we introduce a new dynamic tree decoding approach called CAST that
takes into account inference costs, including factors such as GPU
configurations and batch sizes, to dynamically refine the tree structure.
Through comprehensive experimentation across six diverse tasks and utilizing
six distinct LLMs, our methodology demonstrates remarkable results, achieving
speeds up to 5.2 times faster than conventional decoding methods. Moreover, it
generally outperforms existing state-of-the-art techniques from 5% to 20%.

</details>


### [51] [SlideAgent: Hierarchical Agentic Framework for Multi-Page Visual Document Understanding](https://arxiv.org/abs/2510.26615)
*Yiqiao Jin,Rachneet Kaur,Zhen Zeng,Sumitra Ganesh,Srijan Kumar*

Main category: cs.CL

TL;DR: 提出了SlideAgent，一个针对多页多模态文档理解的智能框架，特别适用于幻灯片文档。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在复杂多页多模态文档，尤其是细粒度推理方面表现不足。

Method: SlideAgent设计了多层级专用智能体（全局、页面、元素），分解推理任务，构建结构化且不依赖查询的文档表示，并在推理时选择性调用以生成一致且上下文相关的回答。

Result: 实验结果显示SlideAgent在整体性能上分别超越专有模型7.9%和开源模型9.8%。

Conclusion: SlideAgent有效提升了对复杂多页视觉文档的理解和推理能力，尤其适用于幻灯片类型的多模态文档。

Abstract: Multi-page visual documents such as manuals, brochures, presentations, and
posters convey key information through layout, colors, icons, and cross-slide
references. While large language models (LLMs) offer opportunities in document
understanding, current systems struggle with complex, multi-page visual
documents, particularly in fine-grained reasoning over elements and pages. We
introduce SlideAgent, a versatile agentic framework for understanding
multi-modal, multi-page, and multi-layout documents, especially slide decks.
SlideAgent employs specialized agents and decomposes reasoning into three
specialized levels-global, page, and element-to construct a structured,
query-agnostic representation that captures both overarching themes and
detailed visual or textual cues. During inference, SlideAgent selectively
activates specialized agents for multi-level reasoning and integrates their
outputs into coherent, context-aware answers. Extensive experiments show that
SlideAgent achieves significant improvement over both proprietary (+7.9
overall) and open-source models (+9.8 overall).

</details>


### [52] [Encoder-Decoder or Decoder-Only? Revisiting Encoder-Decoder Large Language Model](https://arxiv.org/abs/2510.26622)
*Biao Zhang,Yong Cheng,Siamak Shakeri,Xinyi Wang,Min Ma,Orhan Firat*

Main category: cs.CL

TL;DR: 本文重新评估了编码器-解码器结构的大型语言模型（RedLLM），并与主流的仅解码器模型（DecLLM）进行了尺度上的全面比较，发现RedLLM在性能、扩展能力和推理效率上表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型研究主流是仅解码器模型，但缺乏从尺度角度对编码器-解码器模型的系统比较，可能忽视了编码器-解码器模型的潜力。

Method: 通过引入仅解码器模型的训练技巧，使用RedPajama V1数据集进行基于前缀语言建模的预训练，将RedLLM与基于因果语言建模的DecLLM在不同模型规模下进行比较，并用FLAN进行指令调优。

Result: RedLLM在训练计算效率稍逊但展现出良好的扩展性和上下文长度外推能力，指令调优后在多种下游任务中达到甚至超过DecLLM的表现，且推理效率显著更高。

Conclusion: 编码器-解码器结构的大型语言模型具有被低估的潜力，值得进一步研究和优化，以实现更强大且高效的大型语言模型。

Abstract: Recent large language model (LLM) research has undergone an architectural
shift from encoder-decoder modeling to nowadays the dominant decoder-only
modeling. This rapid transition, however, comes without a rigorous comparative
analysis especially \textit{from the scaling perspective}, raising concerns
that the potential of encoder-decoder models may have been overlooked. To fill
this gap, we revisit encoder-decoder LLM (RedLLM), enhancing it with recent
recipes from decoder-only LLM (DecLLM). We conduct a comprehensive comparison
between RedLLM, pretrained with prefix language modeling (LM), and DecLLM,
pretrained with causal LM, at different model scales, ranging from $\sim$150M
to $\sim$8B. Using RedPajama V1 (1.6T tokens) for pretraining and FLAN for
instruction tuning, our experiments show that RedLLM produces compelling
scaling properties and surprisingly strong performance. While DecLLM is overall
more compute-optimal during pretraining, RedLLM demonstrates comparable scaling
and context length extrapolation capabilities. After instruction tuning, RedLLM
achieves comparable and even better results on various downstream tasks while
enjoying substantially better inference efficiency. We hope our findings could
inspire more efforts on re-examining RedLLM, unlocking its potential for
developing powerful and efficient LLMs.

</details>


### [53] [Evontree: Ontology Rule-Guided Self-Evolution of Large Language Models](https://arxiv.org/abs/2510.26683)
*Mingchen Tu,Zhiqiang Liu,Juan Li,Liangyurui Liu,Junjie Wang,Lei Liang,Wen Zhang*

Main category: cs.CL

TL;DR: 本文提出Evontree框架，利用少量高质量本体规则来提取、验证和增强大语言模型（LLM）中的领域知识，实现低资源领域自适应，特别适用于医疗领域。


<details>
  <summary>Details</summary>
Motivation: 在数据敏感且缺乏高质量领域专属数据的场景（如医疗），现有LLM难以适应专业应用。专家知识以本体规则形式存在，如何有效利用这些规则改进LLM是本文的核心动机。

Method: Evontree框架通过从原始模型中提取领域本体，利用两个核心本体规则检测知识不一致，并通过自蒸馏微调强化优化的领域知识，无需大量外部数据。

Result: 在医疗问答基准测试中，Evontree基于Llama3-8B-Instruct和Med42-v2模型相较未修改模型及领先的监督基线，准确率提升最高达3.7%。

Conclusion: Evontree验证了利用少量领域本体规则进行知识提取与强化的有效性，展示了良好的效率和鲁棒性，适用于低资源条件下的LLM领域适应。

Abstract: Large language models (LLMs) have demonstrated exceptional capabilities
across multiple domains by leveraging massive pre-training and curated
fine-tuning data. However, in data-sensitive fields such as healthcare, the
lack of high-quality, domain-specific training corpus hinders LLMs' adaptation
for specialized applications. Meanwhile, domain experts have distilled domain
wisdom into ontology rules, which formalize relationships among concepts and
ensure the integrity of knowledge management repositories. Viewing LLMs as
implicit repositories of human knowledge, we propose Evontree, a novel
framework that leverages a small set of high-quality ontology rules to
systematically extract, validate, and enhance domain knowledge within LLMs,
without requiring extensive external datasets. Specifically, Evontree extracts
domain ontology from raw models, detects inconsistencies using two core
ontology rules, and reinforces the refined knowledge via self-distilled
fine-tuning. Extensive experiments on medical QA benchmarks with
Llama3-8B-Instruct and Med42-v2 demonstrate consistent outperformance over both
unmodified models and leading supervised baselines, achieving up to a 3.7%
improvement in accuracy. These results confirm the effectiveness, efficiency,
and robustness of our approach for low-resource domain adaptation of LLMs.

</details>


### [54] [Kimi Linear: An Expressive, Efficient Attention Architecture](https://arxiv.org/abs/2510.26692)
*Kimi Team,Yu Zhang,Zongyu Lin,Xingcheng Yao,Jiaxi Hu,Fanqing Meng,Chengyin Liu,Xin Men,Songlin Yang,Zhiyuan Li,Wentao Li,Enzhe Lu,Weizhou Liu,Yanru Chen,Weixin Xu,Longhui Yu,Yejie Wang,Yu Fan,Longguang Zhong,Enming Yuan,Dehao Zhang,Yizhi Zhang,T. Y. Liu,Haiming Wang,Shengjun Fang,Weiran He,Shaowei Liu,Yiwei Li,Jianlin Su,Jiezhong Qiu,Bo Pang,Junjie Yan,Zhejun Jiang,Weixiao Huang,Bohong Yin,Jiacheng You,Chu Wei,Zhengtao Wang,Chao Hong,Yutian Chen,Guanduo Chen,Yucheng Wang,Huabin Zheng,Feng Wang,Yibo Liu,Mengnan Dong,Zheng Zhang,Siyuan Pan,Wenhao Wu,Yuhao Wu,Longyu Guan,Jiawen Tao,Guohong Fu,Xinran Xu,Yuzhi Wang,Guokun Lai,Yuxin Wu,Xinyu Zhou,Zhilin Yang,Yulun Du*

Main category: cs.CL

TL;DR: 本文提出了Kimi Linear，一种混合线性注意力架构，首次在多种场景下超越了全注意力机制。


<details>
  <summary>Details</summary>
Motivation: 传统的线性注意力在效率上有优势，但表现通常不如全注意力机制，本文旨在设计一种线性注意力架构，既高效又能超过全注意力的性能。

Method: 提出了Kimi Delta Attention（KDA），结合改进的细粒度门控机制和专门的块状算法，利用改良的对角加低秩矩阵（DPLR）实现计算效率的提升，并将KDA与多头潜在注意力（MLA）进行层级混合。

Result: 在3B激活参数规模下的预训练模型中，Kimi Linear在所有测试任务中均优于全MLA，最多减少75%的KV缓存使用，解码吞吐量提升至6倍，特别是在长上下文任务中表现突出。

Conclusion: Kimi Linear可作为全注意力机制的高效替代方案，兼具更优性能与计算效率，适用于长输入输出长度的任务，相关代码和模型权重已开源以促进后续研究。

Abstract: We introduce Kimi Linear, a hybrid linear attention architecture that, for
the first time, outperforms full attention under fair comparisons across
various scenarios -- including short-context, long-context, and reinforcement
learning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an
expressive linear attention module that extends Gated DeltaNet with a
finer-grained gating mechanism, enabling more effective use of limited
finite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware
efficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR)
transition matrices, which substantially reduces computation compared to the
general DPLR formulation while remaining more consistent with the classical
delta rule.
  We pretrain a Kimi Linear model with 3B activated parameters and 48B total
parameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention
(MLA). Our experiments show that with an identical training recipe, Kimi Linear
outperforms full MLA with a sizeable margin across all evaluated tasks, while
reducing KV cache usage by up to 75% and achieving up to 6 times decoding
throughput for a 1M context. These results demonstrate that Kimi Linear can be
a drop-in replacement for full attention architectures with superior
performance and efficiency, including tasks with longer input and output
lengths.
  To support further research, we open-source the KDA kernel and vLLM
implementations, and release the pre-trained and instruction-tuned model
checkpoints.

</details>


### [55] [The End of Manual Decoding: Towards Truly End-to-End Language Models](https://arxiv.org/abs/2510.26697)
*Zhichao Wang,Dongyang Ma,Xinting Huang,Deng Cai,Tian Lan,Jiahao Xu,Haitao Mi,Xiaoying Tang,Yan Wang*

Main category: cs.CL

TL;DR: 本文提出了AutoDeco，一种能够自主控制解码策略的端到端生成架构，显著提升大语言模型的解码性能，并实现基于自然语言指令的解码控制。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的所谓“端到端”标签存有误导，因其依赖非微分解码过程，需手动调节超参数如温度和top-p，缺乏自动化和灵活性。

Method: 在标准Transformer基础上增加轻量级头部，使模型在每一步动态预测上下文相关的温度和top-p值，同时生成下一个词的logits，实现解码策略的参数化和自我调节。

Result: 在八个基准测试中，AutoDeco显著优于默认解码策略，性能接近基于“测试集黑客”调优的基线，并展现出理解自然语言指令并相应调整解码参数的能力。

Conclusion: AutoDeco实现了真正的端到端生成，推动了大语言模型解码向可控和交互式方向发展，突破了传统静态方法的瓶颈。

Abstract: The "end-to-end" label for LLMs is a misnomer. In practice, they depend on a
non-differentiable decoding process that requires laborious, hand-tuning of
hyperparameters like temperature and top-p. This paper introduces AutoDeco, a
novel architecture that enables truly "end-to-end" generation by learning to
control its own decoding strategy. We augment the standard transformer with
lightweight heads that, at each step, dynamically predict context-specific
temperature and top-p values alongside the next-token logits. This approach
transforms decoding into a parametric, token-level process, allowing the model
to self-regulate its sampling strategy within a single forward pass.
  Through extensive experiments on eight benchmarks, we demonstrate that
AutoDeco not only significantly outperforms default decoding strategies but
also achieves performance comparable to an oracle-tuned baseline derived from
"hacking the test set"-a practical upper bound for any static method.
Crucially, we uncover an emergent capability for instruction-based decoding
control: the model learns to interpret natural language commands (e.g.,
"generate with low randomness") and adjusts its predicted temperature and top-p
on a token-by-token basis, opening a new paradigm for steerable and interactive
LLM decoding.

</details>


### [56] [Value Drifts: Tracing Value Alignment During LLM Post-Training](https://arxiv.org/abs/2510.26707)
*Mehar Bhatia,Shravan Nayak,Gaurav Kamath,Marius Mosbach,Karolina Stańczak,Vered Shwartz,Siva Reddy*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在后训练阶段如何形成与人类价值观的一致性，发现监督微调阶段确定了模型的价值观，偏好优化阶段则影响较小。不同的偏好优化算法即使在相同数据下也会导致不同的价值对齐结果。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在社会中承担越来越重要的角色，模型与人类价值观的对齐成为关键问题，但之前研究多集中于训练完成后的评估，忽略了训练动态过程中的价值形成。

Method: 通过对Llama-3和Qwen-3不同规模模型使用监督微调和偏好优化算法及数据集的实验，分析了后训练过程中价值观的变化及其时间节点，进一步使用合成偏好数据测试不同优化算法对价值对齐的影响。

Result: 发现监督微调阶段确立了模型的价值观，后续偏好优化很少调整价值观；不同偏好优化算法在相同偏好数据下导致不同的价值对齐结果。

Conclusion: 本文揭示了后训练阶段价值观学习的动态，为数据筛选、模型及算法选择提供了指导，有助于提升模型对人类价值观的对齐效果。

Abstract: As LLMs occupy an increasingly important role in society, they are more and
more confronted with questions that require them not only to draw on their
general knowledge but also to align with certain human value systems.
Therefore, studying the alignment of LLMs with human values has become a
crucial field of inquiry. Prior work, however, mostly focuses on evaluating the
alignment of fully trained models, overlooking the training dynamics by which
models learn to express human values. In this work, we investigate how and at
which stage value alignment arises during the course of a model's
post-training. Our analysis disentangles the effects of post-training
algorithms and datasets, measuring both the magnitude and time of value drifts
during training. Experimenting with Llama-3 and Qwen-3 models of different
sizes and popular supervised fine-tuning (SFT) and preference optimization
datasets and algorithms, we find that the SFT phase generally establishes a
model's values, and subsequent preference optimization rarely re-aligns these
values. Furthermore, using a synthetic preference dataset that enables
controlled manipulation of values, we find that different preference
optimization algorithms lead to different value alignment outcomes, even when
preference data is held constant. Our findings provide actionable insights into
how values are learned during post-training and help to inform data curation,
as well as the selection of models and algorithms for preference optimization
to improve model alignment to human values.

</details>


### [57] [AMO-Bench: Large Language Models Still Struggle in High School Math Competitions](https://arxiv.org/abs/2510.26768)
*Shengnan An,Xunliang Cai,Xuezhi Cao,Xiaoyu Li,Yehao Lin,Junlin Liu,Xinxuan Lv,Dan Ma,Xuanlin Wang,Ziwen Wang,Shuang Zhou*

Main category: cs.CL

TL;DR: AMO-Bench是一个包含50个高难度数学奥林匹克水平问题的基准测试，旨在评估大型语言模型的数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有数学竞赛基准对于顶级大型语言模型而言难度不足，导致性能饱和，难以有效评估模型的数学推理能力。

Method: 设计50个符合国际数学奥林匹克难度且经过专家验证的原创问题，仅需模型输出最终答案以便自动评分。

Result: 26个大型语言模型在AMO-Bench上的表现普遍较低，最好模型准确率为52.4%，大部分低于40%，但计算资源增加有望提升表现。

Conclusion: 当前大型语言模型在高级数学推理上仍有较大提升空间，AMO-Bench为推动相关研究提供了有力工具。

Abstract: We present AMO-Bench, an Advanced Mathematical reasoning benchmark with
Olympiad level or even higher difficulty, comprising 50 human-crafted problems.
Existing benchmarks have widely leveraged high school math competitions for
evaluating mathematical reasoning capabilities of large language models (LLMs).
However, many existing math competitions are becoming less effective for
assessing top-tier LLMs due to performance saturation (e.g., AIME24/25). To
address this, AMO-Bench introduces more rigorous challenges by ensuring all 50
problems are (1) cross-validated by experts to meet at least the International
Mathematical Olympiad (IMO) difficulty standards, and (2) entirely original
problems to prevent potential performance leakages from data memorization.
Moreover, each problem in AMO-Bench requires only a final answer rather than a
proof, enabling automatic and robust grading for evaluation. Experimental
results across 26 LLMs on AMO-Bench show that even the best-performing model
achieves only 52.4% accuracy on AMO-Bench, with most LLMs scoring below 40%.
Beyond these poor performances, our further analysis reveals a promising
scaling trend with increasing test-time compute on AMO-Bench. These results
highlight the significant room for improving the mathematical reasoning in
current LLMs. We release AMO-Bench to facilitate further research into
advancing the reasoning abilities of language models.
https://amo-bench.github.io/

</details>


### [58] [Gistify! Codebase-Level Understanding via Runtime Execution](https://arxiv.org/abs/2510.26790)
*Hyunji Lee,Minseon Kim,Chinmay Singh,Matheus Pereira,Atharv Sonwane,Isadora White,Elias Stengel-Eskin,Mohit Bansal,Zhengyan Shi,Alessandro Sordoni,Marc-Alexandre Côté,Xingdi Yuan,Lucas Caccia*

Main category: cs.CL

TL;DR: 本文提出了Gistify任务，要求编码大语言模型生成一个最小且自包含的文件以复现代码库中特定功能，现有模型对此存在困难。


<details>
  <summary>Details</summary>
Motivation: 随着编码代理在大型代码库中的应用增加，设计挑战性且自动化的代码库级评测任务变得关键。

Method: 设计Gistify任务，给予编码模型完整代码库访问权限和具体入口点，要求模型生成的文件能输出与完整代码库运行相同命令的结果，同时仅包含执行该命令所需的必要组件。

Result: 实验结果表明，当前最先进的模型难以稳定完成Gistify任务，尤其是在执行流程较长的情况下。

Conclusion: Gistify任务揭示了现有编码模型在结构理解和长执行流程建模方面的不足，为未来改进提供了方向。

Abstract: As coding agents are increasingly deployed in large codebases, the need to
automatically design challenging, codebase-level evaluation is central. We
propose Gistify, a task where a coding LLM must create a single, minimal,
self-contained file that can reproduce a specific functionality of a codebase.
The coding LLM is given full access to a codebase along with a specific
entrypoint (e.g., a python command), and the generated file must replicate the
output of the same command ran under the full codebase, while containing only
the essential components necessary to execute the provided command. Success on
Gistify requires both structural understanding of the codebase, accurate
modeling of its execution flow as well as the ability to produce potentially
large code patches. Our findings show that current state-of-the-art models
struggle to reliably solve Gistify tasks, especially ones with long executions
traces.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [59] [Internal Vulnerabilities, External Threats: A Grounded Framework for Enterprise Open Source Risk Governance](https://arxiv.org/abs/2510.25882)
*Wenhao Yang,Minghui Zhou,Daniel Izquierdo Cortázar,Yehui Wang*

Main category: cs.SE

TL;DR: 本文提出了一个全面的风险治理框架，帮助企业从战术风险管理转向整体风险治理，解决了传统技术工具不足以应对系统性威胁的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的开源风险管理仅关注技术层面，无法应对如上游“静默修复”、社区冲突和许可证变更等系统性威胁，造成治理盲点。

Method: 通过对15名从业者进行扎根理论研究，构建了基于“外部威胁+内部漏洞”的风险原则，以“目标-威胁-漏洞-缓解”(OTVM)逻辑链为核心，形成一套系统风险治理框架。

Result: 开发了“战略目标矩阵”、外部威胁与内部漏洞的双重分类体系，以及与漏洞对应的缓解框架，框架通过三位行业专家的实际案例回顾验证了其分析效用。

Conclusion: 该框架为企业提供了一种全新的诊断视角和系统路径，促进企业从被动响应转向主动构建组织免疫系统，实现开源风险的战略性整体治理。

Abstract: Enterprise engagement with open source has evolved from tactical adoption to
strategic deep integration, exposing them to a complex risk landscape far
beyond mere code. However, traditional risk management, narrowly focused on
technical tools, is structurally inadequate for systemic threats like upstream
"silent fixes", community conflicts, or sudden license changes, creating a
dangerous governance blind spot. To address this governance vacuum and enable
the necessary shift from tactical risk management to holistic risk governance,
we conducted a grounded theory study with 15 practitioners to develop a
holistic risk governance framework. Our study formalizes an analytical
framework built on a foundational risk principle: an uncontrollable External
Threat (e.g., a sudden license change in a key dependency) only becomes a
critical risk when it exploits a controllable Internal Vulnerability (e.g., an
undefined risk appetite for single-vendor projects), which then amplifies the
impact.The framework operationalizes this principle through a clear logical
chain: "Objectives -> Threats -> Vulnerabilities -> Mitigation" (OTVM). This
provides a holistic decision model that transcends mere technical checklists.
Based on this logic, our contributions are: (1) a "Strategic Objectives Matrix"
to clarify goals; (2) a systematic dual taxonomy of External Threats (Ex-Tech,
Ex-Comm, Ex-Eco) and Internal Vulnerabilities (In-Strat, In-Ops, In-Tech); and
(3) an actionable mitigation framework mapping capability-building to these
vulnerabilities. The framework's analytical utility was validated by three
industry experts through retrospective case studies on real-world incidents.
This work provides a novel diagnostic lens and a systematic path for
enterprises to shift from reactive "firefighting" to proactively building an
organizational "immune system".

</details>


### [60] [PRISM: Proof-Carrying Artifact Generation through LLM x MDE Synergy and Stratified Constraints](https://arxiv.org/abs/2510.25890)
*Tong Ma,Hui Lai,Hui Wang,Zhenhu Tian,Jizhou Wang,Haichao Wu,Yongfan Gao,Chaochao Li,Fengjie Xu,Ling Fang*

Main category: cs.SE

TL;DR: PRISM结合大型语言模型与模型驱动工程，自动生成符合安全和合规要求的审计证据。


<details>
  <summary>Details</summary>
Motivation: 在安全和合规性要求极高的领域，需要生成可审计且合规的工件，减少人工修正，提高自动化水平。

Method: PRISM融合统一元模型整合多异构数据，集成约束模型生成执行自动机及验证器，采用约束引导的两层生成策略保障结构和语义正确。

Result: 系统在汽车软件和跨境法律领域验证，成功生成结构合法、可审计工件，减少手工修复，支持现有工具集成。

Conclusion: PRISM为安全合规场景下的工件自动生成提供了可行且可验证的解决方案，提升自动化和可靠性。

Abstract: PRISM unifies Large Language Models with Model-Driven Engineering to generate
regulator-ready artifacts and machine-checkable evidence for safety- and
compliance-critical domains. PRISM integrates three pillars: a Unified
Meta-Model (UMM) reconciles heterogeneous schemas and regulatory text into a
single semantic space; an Integrated Constraint Model (ICM) compiles structural
and semantic requirements into enforcement artifacts including generation-time
automata (GBNF, DFA) and post-generation validators (e.g., SHACL, SMT); and
Constraint-Guided Verifiable Generation (CVG) applies these through two-layer
enforcement - structural constraints drive prefix-safe decoding while
semantic/logical validation produces machine-checkable certificates. When
violations occur, PRISM performs audit-guided repair and records generation
traces for compliance review. We evaluate PRISM in automotive software
engineering (AUTOSAR) and cross-border legal jurisdiction (Brussels I bis).
PRISM produces structurally valid, auditable artifacts that integrate with
existing tooling and substantially reduce manual remediation effort, providing
a practical path toward automated artifact generation with built-in assurance.

</details>


### [61] [A Process Mining-Based System For The Analysis and Prediction of Software Development Workflows](https://arxiv.org/abs/2510.25935)
*Antía Dorado,Iván Folgueira,Sofía Martín,Gonzalo Martín,Álvaro Porto,Alejandro Ramos,John Wallace*

Main category: cs.SE

TL;DR: CodeSight通过捕获GitHub数据并结合流程挖掘与LSTM模型，实现对软件开发流程中PR完成时间的准确预测，从而有效预判是否能按期完成任务。


<details>
  <summary>Details</summary>
Motivation: 提高软件开发中提前预判PR是否能按期完成的能力，增强项目管理的主动性。

Method: 从GitHub获取开发和部署数据，转换为流程挖掘日志，提取指标并展示，结合LSTM模型基于序列活动和静态特征预测PR解决时间。

Result: 系统在预测截止日期遵守性方面表现出高精度和F1分数，证明方法有效。

Conclusion: 融合流程挖掘和机器学习技术可显著提升软件项目管理中对任务完成时间的预测准确性和效率。

Abstract: CodeSight is an end-to-end system designed to anticipate deadline compliance
in software development workflows. It captures development and deployment data
directly from GitHub, transforming it into process mining logs for detailed
analysis. From these logs, the system generates metrics and dashboards that
provide actionable insights into PR activity patterns and workflow efficiency.
Building on this structured representation, CodeSight employs an LSTM model
that predicts remaining PR resolution times based on sequential activity traces
and static features, enabling early identification of potential deadline
breaches. In tests, the system demonstrates high precision and F1 scores in
predicting deadline compliance, illustrating the value of integrating process
mining with machine learning for proactive software project management.

</details>


### [62] [Beyond Synthetic Benchmarks: Evaluating LLM Performance on Real-World Class-Level Code Generation](https://arxiv.org/abs/2510.26130)
*Musfiqur Rahman,SayedHassan Khatoonabadi,Emad Shihab*

Main category: cs.SE

TL;DR: 本文提出了一个基于开源代码库的真实类实现基准测试，评估大型语言模型（LLM）在类级代码生成中的泛化能力，发现其在真实项目中的正确率大幅低于合成基准，并分析了文档完整度和检索增强对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 目前大型语言模型在函数级代码生成表现优异，但其在真实软件项目中生成正确的类级实现能力尚不清楚，亟需构建更贴近实际的评测基准，以揭示其性能瓶颈和改进方向。

Method: 构建了包含真实类的基准数据集，并将其划分为“见过”和“未见过”代码库分区，分别评测多款LLM在不同输入规格、文档完整度及检索增强配置下的表现，分析错误类型及模型性能差异。

Result: 模型在传统合成基准上的正确率为84%-89%，但在真实类任务中仅为25%-34%，且见过与未见过代码库间差异微小。全面文档提升准确率1%-3%，检索增强在部分文档条件下提升4%-7%。主要错误为属性错误、类型错误和断言错误，检索增强可减少逻辑错误但可能引入依赖冲突。

Conclusion: 现有大型语言模型在类级代码生成存在明显局限性，需要改进上下文建模、文档利用及检索整合策略，以提升真实软件开发中的代码辅助效果。

Abstract: Large language models (LLMs) have advanced code generation at the function
level, yet their ability to produce correct class-level implementations in
authentic software projects remains poorly understood. This work introduces a
novel benchmark derived from open-source repositories, comprising real-world
classes divided into seen and unseen partitions to evaluate generalization
under practical conditions. The evaluation examines multiple LLMs under varied
input specifications, retrieval-augmented configurations, and documentation
completeness levels.
  Results reveal a stark performance disparity: LLMs achieve 84% to 89%
correctness on established synthetic benchmarks but only 25% to 34% on
real-world class tasks, with negligible differences between familiar and novel
codebases. Comprehensive docstrings yield modest gains of 1% to 3% in
functional accuracy, though statistical significance is rare.
Retrieval-augmented generation proves most effective with partial
documentation, improving correctness by 4% to 7% by supplying concrete
implementation patterns absent from specifications. Error profiling identifies
AttributeError, TypeError, and AssertionError as dominant failure modes (84% of
cases), with synthetic tests overemphasizing assertion issues and real-world
scenarios highlighting type and attribute mismatches. Retrieval augmentation
reduces logical flaws but can introduce dependency conflicts.
  The benchmark and analysis expose critical limitations in current LLM
capabilities for class-level engineering, offering actionable insights for
enhancing context modelling, documentation strategies, and retrieval
integration in production code assistance tools.

</details>


### [63] [A Research Roadmap for Augmenting Software Engineering Processes and Software Products with Generative AI](https://arxiv.org/abs/2510.26275)
*Domenico Amalfitano,Andreas Metzger,Marco Autili,Tommaso Fulcini,Tobias Hey,Jan Keim,Patrizio Pelliccione,Vincenzo Scotti,Anne Koziolek,Raffaela Mirandola,Andreas Vogelsang*

Main category: cs.SE

TL;DR: 本文通过设计科学研究方法构建了生成式人工智能（GenAI）增强软件工程（SE）的路线图，系统分析了GenAI对SE流程和软件产品的影响，提出了研究挑战和未来方向，并预测2030年软件工程的发展趋势。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能正在快速改变软件工程的实践，影响软件系统的开发、运营和演进，亟需系统研究其影响并规划未来的发展路线。

Method: 采用设计科学研究方法，结合FSE 2025研讨会讨论、快速文献综述及同行反馈，利用McLuhan的四律工具系统捕捉GenAI对软件工程的变革效应，构建以多周期实证过程为基础的路线图。

Result: 确定了GenAI增强软件工程的四种基本形态，系统性地描述了相关研究挑战和机会，形成了一套未来研究方向，为该领域提供了透明可复现的分析基础。

Conclusion: 本文为理解和推动GenAI驱动的软件工程变革提供了科学方法论支撑，并基于研究成果提出2030年软件工程的十条预测，指导未来研究和实践发展。

Abstract: Generative AI (GenAI) is rapidly transforming software engineering (SE)
practices, influencing how SE processes are executed, as well as how software
systems are developed, operated, and evolved. This paper applies design science
research to build a roadmap for GenAI-augmented SE. The process consists of
three cycles that incrementally integrate multiple sources of evidence,
including collaborative discussions from the FSE 2025 "Software Engineering
2030" workshop, rapid literature reviews, and external feedback sessions
involving peers. McLuhan's tetrads were used as a conceptual instrument to
systematically capture the transforming effects of GenAI on SE processes and
software products.The resulting roadmap identifies four fundamental forms of
GenAI augmentation in SE and systematically characterizes their related
research challenges and opportunities. These insights are then consolidated
into a set of future research directions. By grounding the roadmap in a
rigorous multi-cycle process and cross-validating it among independent author
teams and peers, the study provides a transparent and reproducible foundation
for analyzing how GenAI affects SE processes, methods and tools, and for
framing future research within this rapidly evolving area. Based on these
findings, the article finally makes ten predictions for SE in the year 2030.

</details>


### [64] [Reduction of Test Re-runs by Prioritizing Potential Order Dependent Flaky Tests](https://arxiv.org/abs/2510.26171)
*Hasnain Iqbal,Zerina Begum,Kazi Sakib*

Main category: cs.SE

TL;DR: 该论文提出了一种通过分析测试类中共享静态字段来优先识别可能的顺序依赖（OD）测试的方法，从而减少测试重复执行，提高检测效率。


<details>
  <summary>Details</summary>
Motivation: 顺序依赖测试因其不确定性导致自动化测试不可靠，且现有检测方法需要大量无关的重复执行，增加测试成本。

Method: 通过静态分析测试类中的共享静态字段，优先定位可能存在顺序依赖的测试用例，减少不必要的重复测试。

Result: 在27个项目模块实验中，该方法成功优先识别了23个模块中的所有OD测试，减少了约65.92%的测试执行和72.19%的无效重跑。

Conclusion: 该方法有效提高了OD测试检测的效率，降低了测试执行成本，优化了自动化测试过程的可靠性。

Abstract: Flaky tests can make automated software testing unreliable due to their
unpredictable behavior. These tests can pass or fail on the same code base on
multiple runs. However, flaky tests often do not refer to any fault, even
though they can cause the continuous integration (CI) pipeline to fail. A
common type of flaky test is the order-dependent (OD) test. The outcome of an
OD test depends on the order in which it is run with respect to other test
cases. Several studies have explored the detection and repair of OD tests.
However, their methods require re-runs of tests multiple times, that are not
related to the order dependence. Hence, prioritizing potential OD tests is
necessary to reduce the re-runs. In this paper, we propose a method to
prioritize potential order-dependent tests. By analyzing shared static fields
in test classes, we identify tests that are more likely to be order-dependent.
In our experiment on 27 project modules, our method successfully prioritized
all OD tests in 23 cases, reducing test executions by an average of 65.92% and
unnecessary re-runs by 72.19%. These results demonstrate that our approach
significantly improves the efficiency of OD test detection by lowering
execution costs.

</details>


### [65] [The "4W+1H" of Software Supply Chain Security Checklist for Critical Infrastructure](https://arxiv.org/abs/2510.26174)
*Liming Dong,Sung Une Lee,Zhenchang Xing,Muhammad Ejaz Ahmed,Stefan Avgoustakis*

Main category: cs.SE

TL;DR: 本文通过多重文献综述，系统分析了软件供应链安全实践，特别针对关键基础设施领域，提出了包含80个问题的分层检查清单，以帮助相关方评估和提升安全水平。


<details>
  <summary>Details</summary>
Motivation: 软件供应链攻击威胁加剧，现有安全框架多局限于生命周期某阶段或缺乏关键基础设施特定需求的考量，亟需系统化、行业针对性的安全措施。

Method: 采用多声部文献综述，包括国际框架、澳大利亚监管资料及学术研究，利用"4W+1H"分析方法，归纳出十个核心安全类别，跨生命周期、利益相关者及实施层面进行映射和覆盖分析。

Result: 发现现有框架在关键基础设施领域适配不足，提出了一个结构化、多层次、包含80个问题的安全实践检查清单，帮助评估和改进软件供应链安全。

Conclusion: 需要开发整合且具情境感知能力的软件供应链安全框架，以弥补现有指导与关键基础设施需求之间的差距，提升应对日益复杂风险的能力。

Abstract: The increasing frequency and sophistication of software supply chain attacks
pose severe risks to critical infrastructure sectors, threatening national
security, economic stability, and public safety. Despite growing awareness,
existing security practices remain fragmented and insufficient, with most
frameworks narrowly focused on isolated life cycle stages or lacking alignment
with the specific needs of critical infrastructure (CI) sectors. In this paper,
we conducted a multivocal literature review across international frameworks,
Australian regulatory sources, and academic studies to identify and analyze
security practices across the software supply chain, especially specific CI
sector. Our analysis found that few existing frameworks are explicitly tailored
to CI domains. We systematically leveraged identified software supply chain
security frameworks, using a "4W+1H" analytical approach, we synthesized ten
core categories (what) of software supply chain security practices, mapped them
across life-cycle phases (when), stakeholder roles (who), and implementation
levels (how), and examined their coverage across existing frameworks (where).
Building on these insights, the paper culminates in structured, multi-layered
checklist of 80 questions designed to relevant stakeholders evaluate and
enhance their software supply chain security. Our findings reveal gaps between
framework guidance and sector-specific needs, highlight the need for
integrated, context-aware approaches to safeguard critical infrastructure from
evolving software supply chain risks.

</details>


### [66] [Empowering RepoQA-Agent based on Reinforcement Learning Driven by Monte-carlo Tree Search](https://arxiv.org/abs/2510.26287)
*Guochang Li,Yuchen Liu,Zhen Qin,Yunkun Wang,Jianping Zhong,Chen Zhi,Binhua Li,Fei Huang,Yongbin Li,Shuiguang Deng*

Main category: cs.SE

TL;DR: 本文提出了一种基于蒙特卡洛树搜索(MCTS)的强化学习框架RepoSearch-R1，解决了现有LLM在代码库任务中工具使用和决策引导不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有无训练或需大规模蒸馏的强化学习方法在代码库多轮工具交互任务中效果有限，且存在高成本和数据合规性风险。

Method: 引入RepoSearch-R1强化学习框架，通过MCTS实现多样化、高质量的自我训练推理轨迹，无需大模型蒸馏或外部监督，构建针对代码库问答任务的RepoQA-Agent。

Result: 在代码库问答任务中，RepoSearch-R1相比无检索方法提升16.0%、迭代检索方法提升19.5%的答案完整率，且训练效率较通用强化学习提升33%。

Conclusion: RepoSearch-R1实现了无蒸馏、高效且合规的多轮工具交互训练，显著提升代码库级推理任务的探索多样性和答案完整性。

Abstract: Repository-level software engineering tasks require large language models
(LLMs) to efficiently navigate and extract information from complex codebases
through multi-turn tool interactions. Existing approaches face significant
limitations: training-free, in-context learning methods struggle to guide
agents effectively in tool utilization and decision-making based on
environmental feedback, while training-based approaches typically rely on
costly distillation from larger LLMs, introducing data compliance concerns in
enterprise environments. To address these challenges, we introduce
RepoSearch-R1, a novel agentic reinforcement learning framework driven by
Monte-carlo Tree Search (MCTS). This approach allows agents to generate
diverse, high-quality reasoning trajectories via self-training without
requiring model distillation or external supervision. Based on RepoSearch-R1,
we construct a RepoQA-Agent specifically designed for repository
question-answering tasks. Comprehensive evaluation on repository
question-answering tasks demonstrates that RepoSearch-R1 achieves substantial
improvements of answer completeness: 16.0% enhancement over no-retrieval
methods, 19.5% improvement over iterative retrieval methods, and 33% increase
in training efficiency compared to general agentic reinforcement learning
approaches. Our cold-start training methodology eliminates data compliance
concerns while maintaining robust exploration diversity and answer completeness
across repository-level reasoning tasks.

</details>


### [67] [Environmental Impact of CI/CD Pipelines](https://arxiv.org/abs/2510.26413)
*Nuno Saavedra,Alexandra Mendes,João F. Ferreira*

Main category: cs.SE

TL;DR: 本文研究了GitHub Actions的碳足迹和水足迹，发现其环境影响显著，并提出了减缓策略。


<details>
  <summary>Details</summary>
Motivation: 持续集成/持续交付（CI/CD）流程广泛应用，但其环境影响尚不清楚，特别是碳和水足迹。随着云计算环境影响日益加重，理解CI/CD服务的环境影响变得重要。

Method: 基于Cloud Carbon Footprint框架的方法，使用迄今文献中最大的220万次工作流运行数据，评估GitHub Actions的碳和水足迹。

Result: 2024年碳足迹估计在150.5至994.9公吨二氧化碳当量之间，水足迹在1989.6到37664.5千升之间，最可能情景为456.9公吨二氧化碳当量和5738.2千升水。

Conclusion: GitHub Actions产生显著环境影响，建议通过选择低影响区域部署、优化运行调度和减少代码库规模等举措降低资源浪费和环境足迹。

Abstract: CI/CD pipelines are widely used in software development, yet their
environmental impact, particularly carbon and water footprints (CWF), remains
largely unknown to developers, as CI service providers typically do not
disclose such information. With the growing environmental impact of cloud
computing, understanding the CWF of CI/CD services has become increasingly
important.
  This work investigates the CWF of using GitHub Actions, focusing on
open-source repositories where usage is free and unlimited for standard
runners. We build upon a methodology from the Cloud Carbon Footprint framework
and we use the largest dataset of workflow runs reported in the literature to
date, comprising over 2.2 million workflow runs from more than 18,000
repositories.
  Our analysis reveals that the GitHub Actions ecosystem results in a
substantial CWF. Our estimates for the carbon footprint in 2024 range from
150.5 MTCO2e in the most optimistic scenario to 994.9 MTCO2e in the most
pessimistic scenario, while the water footprint ranges from 1,989.6 to 37,664.5
kiloliters. The most likely scenario estimates are 456.9 MTCO2e for carbon
footprint and 5,738.2 kiloliters for water footprint. To provide perspective,
the carbon footprint in the most likely scenario is equivalent to the carbon
captured by 7,615 urban trees in a year, and the water footprint is comparable
to the water consumed by an average American family over 5,053 years.
  We explore strategies to mitigate this impact, primarily by reducing wasted
computational resources. Key recommendations include deploying runners in
regions whose energy production has a low environmental impact such as France
and the United Kingdom, implementing stricter deactivation policies for
scheduled runs and aligning their execution with periods when the regional
energy mix is more environmentally favorable, and reducing the size of
repositories.

</details>


### [68] [Nexus: Execution-Grounded Multi-Agent Test Oracle Synthesis](https://arxiv.org/abs/2510.26423)
*Dong Huang,Mingzhe Du,Jie M. Zhang,Zheng Lin,Meng Luo,Qianru Zhang,See-Kiong Ng*

Main category: cs.SE

TL;DR: 本文提出了Nexus框架，通过多代理协同生成和自我优化测试判定器，提高非回归测试中测试判定器的准确率。


<details>
  <summary>Details</summary>
Motivation: 非回归测试中准确生成测试判定器是软件工程的难题，现有方法效果有限。

Method: Nexus利用四个不同测试哲学的专门代理共同审议生成的测试判定器，随后在沙箱环境中验证执行，失败的判定器经过自动调试和自我改进循环，迭代优化。

Result: 在七个基准测试上，Nexus显著优于现有方法，例如GPT-4.1-Mini在LiveCodeBench上测试判定器准确率从46.30%提升至57.73%，同时提升了bug检测率和自动程序修复成功率。

Conclusion: 多代理协同结合自动自我优化的Nexus框架有效提升了测试判定器的准确性，从而增强后续软件测试和修复效果。

Abstract: Test oracle generation in non-regression testing is a longstanding challenge
in software engineering, where the goal is to produce oracles that can
accurately determine whether a function under test (FUT) behaves as intended
for a given input. In this paper, we introduce Nexus, a novel multi-agent
framework to address this challenge. Nexus generates test oracles by leveraging
a diverse set of specialized agents that synthesize test oracles through a
structured process of deliberation, validation, and iterative self-refinement.
During the deliberation phase, a panel of four specialist agents, each
embodying a distinct testing philosophy, collaboratively critiques and refines
an initial set of test oracles. Then, in the validation phase, Nexus generates
a plausible candidate implementation of the FUT and executes the proposed
oracles against it in a secure sandbox. For any oracle that fails this
execution-based check, Nexus activates an automated selfrefinement loop, using
the specific runtime error to debug and correct the oracle before
re-validation. Our extensive evaluation on seven diverse benchmarks
demonstrates that Nexus consistently and substantially outperforms
state-of-theart baselines. For instance, Nexus improves the test-level oracle
accuracy on the LiveCodeBench from 46.30% to 57.73% for GPT-4.1-Mini. The
improved accuracy also significantly enhances downstream tasks: the bug
detection rate of GPT4.1-Mini generated test oracles on HumanEval increases
from 90.91% to 95.45% for Nexus compared to baselines, and the success rate of
automated program repair improves from 35.23% to 69.32%.

</details>


### [69] [CHCVerif: A Portfolio-Based Solver for Constrained Horn Clauses](https://arxiv.org/abs/2510.26431)
*Mihály Dobos-Kovács,Levente Bajczi,András Vörös*

Main category: cs.SE

TL;DR: 本文提出了一种基于软件验证方法的组合约束霍恩子句(CHC)求解器CHCVERIF，能够有效处理包含比特向量和低级语义的CHC任务。


<details>
  <summary>Details</summary>
Motivation: 利用现有成熟的软件验证工具解决CHC问题，尤其是涉及比特向量和低级语义的复杂CHC任务，从而提升求解效率和适用范围。

Method: 提出了基于软件验证方法的组合求解器CHCVERIF，采用组合策略以复用现有软件验证工具处理CHC问题。

Result: 在线性整数算术任务上的表现中等，但在比特向量基准测试上取得了一定的成功，验证了方法的可行性。

Conclusion: 基于软件验证工具的CHC求解策略具备潜力，特别是在组合策略支持下，能够有效解决某些复杂的CHC问题。

Abstract: Constrained Horn Clauses (CHCs) are widely adopted as intermediate
representations for a variety of verification tasks, including safety checking,
invariant synthesis, and interprocedural analysis. This paper introduces
CHCVERIF, a portfolio-based CHC solver that adopts a software verification
approach for solving CHCs. This approach enables us to reuse mature software
verification tools to tackle CHC benchmarks, particularly those involving
bitvectors and low-level semantics. Our evaluation shows that while the method
enjoys only moderate success with linear integer arithmetic, it achieves modest
success on bitvector benchmarks. Moreover, our results demonstrate the
viability and potential of using software verification tools as backends for
CHC solving, particularly when supported by a carefully constructed portfolio.

</details>


### [70] [SecureReviewer: Enhancing Large Language Models for Secure Code Review through Secure-aware Fine-tuning](https://arxiv.org/abs/2510.26457)
*Fang Liu,Simiao Liu,Yinghao Zhu,Xiaoli Lian,Li Zhang*

Main category: cs.SE

TL;DR: 本文提出SecureReviewer，一种基于大语言模型的自动化安全代码审查方法，旨在识别和修复代码中的安全问题。


<details>
  <summary>Details</summary>
Motivation: 当前自动代码审查多关注通用代码质量问题，安全相关问题识别不足，且缺乏针对安全的训练数据和评价指标。

Method: 构建安全代码审查数据集，基于该数据集采用安全感知微调策略对大语言模型进行微调，并结合RAG技术提升生成评论的可靠性。提出SecureBLEU指标用于评估安全审查评论的有效性。

Result: 实验表明SecureReviewer在安全问题检测准确率和代码审查评论的质量及实用性上均优于现有先进方法。

Conclusion: SecureReviewer有效提升了大语言模型在安全代码审查中的表现，为早期识别和修复安全问题提供了有力支持。

Abstract: Identifying and addressing security issues during the early phase of the
development lifecycle is critical for mitigating the long-term negative impacts
on software systems. Code review serves as an effective practice that enables
developers to check their teammates' code before integration into the codebase.
To streamline the generation of review comments, various automated code review
approaches have been proposed, where LLM-based methods have significantly
advanced the capabilities of automated review generation. However, existing
models primarily focus on general-purpose code review, their effectiveness in
identifying and addressing security-related issues remains underexplored.
Moreover, adapting existing code review approaches to target security issues
faces substantial challenges, including data scarcity and inadequate evaluation
metrics. To address these limitations, we propose SecureReviewer, a new
approach designed for enhancing LLMs' ability to identify and resolve
security-related issues during code review. Specifically, we first construct a
dataset tailored for training and evaluating secure code review capabilities.
Leveraging this dataset, we fine-tune LLMs to generate code review comments
that can effectively identify security issues and provide fix suggestions with
our proposed secure-aware fine-tuning strategy. To mitigate hallucination in
LLMs and enhance the reliability of their outputs, we integrate the RAG
technique, which grounds the generated comments in domain-specific security
knowledge. Additionally, we introduce SecureBLEU, a new evaluation metric
designed to assess the effectiveness of review comments in addressing security
issues. Experimental results demonstrate that SecureReviewer outperforms
state-of-the-art baselines in both security issue detection accuracy and the
overall quality and practical utility of generated review comments.

</details>


### [71] [Automated Extract Method Refactoring with Open-Source LLMs: A Comparative Study](https://arxiv.org/abs/2510.26480)
*Sivajeet Chand,Melih Kilic,Roland Würsching,Sushant Kumar Pandey,Alexander Pretschner*

Main category: cs.SE

TL;DR: 本文评估了五种开源大型语言模型在Python代码的自动提取方法重构任务中的性能，提出的递归批评与改进（RCI）提示策略优于单次提示。


<details>
  <summary>Details</summary>
Motivation: 自动提取方法重构（EMR）对于提高代码可读性和可维护性至关重要，但目前仍主要依赖手工操作，自动化挑战较大。

Method: 采用了3B到8B参数规模的五种开源大型语言模型，比较了单次提示与递归批评与改进（RCI）提示策略，利用自动化指标评估功能正确性和代码质量。

Result: RCI提示方法在测试通过率和重构质量上均优于单次提示，最佳模型Deepseek-Coder-RCI和Qwen2.5-Coder-RCI的测试通过率分别达到82.9%和80.8%，显著减少了代码行数和环形复杂度。开发者调查显示70%以上的接受度，Qwen2.5-Coder在所有评估指标中得分最高。

Conclusion: 自动提取方法重构在大型语言模型及有效提示策略的支持下取得显著进展，但传统度量指标与人类评价存在差异，强调需结合人工评估。该开源基准为未来自动重构研究提供基础。

Abstract: Automating the Extract Method refactoring (EMR) remains challenging and
largely manual despite its importance in improving code readability and
maintainability. Recent advances in open-source, resource-efficient Large
Language Models (LLMs) offer promising new approaches for automating such
high-level tasks. In this work, we critically evaluate five state-of-the-art
open-source LLMs, spanning 3B to 8B parameter sizes, on the EMR task for Python
code. We systematically assess functional correctness and code quality using
automated metrics and investigate the impact of prompting strategies by
comparing one-shot prompting to a Recursive criticism and improvement (RCI)
approach. RCI-based prompting consistently outperforms one-shot prompting in
test pass rates and refactoring quality. The best-performing models,
Deepseek-Coder-RCI and Qwen2.5-Coder-RCI, achieve test pass percentage (TPP)
scores of 0.829 and 0.808, while reducing lines of code (LOC) per method from
12.103 to 6.192 and 5.577, and cyclomatic complexity (CC) from 4.602 to 3.453
and 3.294, respectively. A developer survey on RCI-generated refactorings shows
over 70% acceptance, with Qwen2.5-Coder rated highest across all evaluation
criteria. In contrast, the original code scored below neutral, particularly in
readability and maintainability, underscoring the benefits of automated
refactoring guided by quality prompts. While traditional metrics like CC and
LOC provide useful signals, they often diverge from human judgments,
emphasizing the need for human-in-the-loop evaluation. Our open-source
benchmark offers a foundation for future research on automated refactoring with
LLMs.

</details>


### [72] [Envisioning Future Interactive Web Development: Editing Webpage with Natural Language](https://arxiv.org/abs/2510.26516)
*Truong Hai Dang,Jingyu Xiao,Yintong Huo*

Main category: cs.SE

TL;DR: 本文提出了Instruct4Edit数据集，通过自动化流程生成高质量的网页代码编辑数据，解决了基于自然语言指令修改网页代码的难题。


<details>
  <summary>Details</summary>
Motivation: 网页应用的迭代开发需要频繁修改代码，传统方法手动且耗时，LLMs虽然能生成UI代码，但难以准确根据新设计需求编辑已有代码，缺乏高质量调优数据是主要瓶颈。

Method: 设计自动数据生成流程，利用LLMs合成多样化指令及对应代码修改，并通过视觉验证确保修改正确性，生成名为Instruct4Edit的微调数据集；将模型在该数据上微调以提升性能。

Result: 微调后的模型在将人类意图精准转化为结构完整且视觉准确的代码修改方面表现出显著提升，效果与专有系统相当。

Conclusion: Instruct4Edit为基于自然语言的网页编辑提供了可扩展且透明的基础，实现了小型开源模型通过微调达到竞争级性能，推动网页自动化编辑发展。

Abstract: The evolution of web applications relies on iterative code modifications, a
process that is traditionally manual and time-consuming. While Large Language
Models (LLMs) can generate UI code, their ability to edit existing code from
new design requirements (e.g., "center the logo") remains a challenge. This is
largely due to the absence of large-scale, high-quality tuning data to align
model performance with human expectations. In this paper, we introduce a novel,
automated data generation pipeline that uses LLMs to synthesize a high-quality
fine-tuning dataset for web editing, named Instruct4Edit. Our approach
generates diverse instructions, applies the corresponding code modifications,
and performs visual verification to ensure correctness. By fine-tuning models
on Instruct4Edit, we demonstrate consistent improvement in translating human
intent into precise, structurally coherent, and visually accurate code changes.
This work provides a scalable and transparent foundation for natural language
based web editing, demonstrating that fine-tuning smaller open-source models
can achieve competitive performance with proprietary systems. We release all
data, code implementations, and model checkpoints for reproduction.

</details>


### [73] [Reflecting on Empirical and Sustainability Aspects of Software Engineering Research in the Era of Large Language Models](https://arxiv.org/abs/2510.26538)
*David Williams,Max Hort,Maria Kechagia,Aldeida Aleti,Justyna Petke,Federica Sarro*

Main category: cs.SE

TL;DR: 本文探讨了软件工程领域中使用大型语言模型（LLMs）所带来的研究挑战，并总结了当前ICSE会议中相关研究的现状。


<details>
  <summary>Details</summary>
Motivation: 引发研究社区对使用LLMs的SE研究中基准测试严格性、数据污染、可重复性和可持续性问题的反思。

Method: 对ICSE会议中LLM驱动的SE研究进行了结构化的综述与分析，识别出当前的好实践与不足。

Result: 揭示了当前研究中的鼓舞人心的实践和持续存在的问题，特别是在基准测试严格性和研究可重复性方面。

Conclusion: 提出了加强基准测试严格性、提升研究可重复性，以及解决LLM应用中财务和环境成本的建议，以推动SE领域的可持续发展。

Abstract: Software Engineering (SE) research involving the use of Large Language Models
(LLMs) has introduced several new challenges related to rigour in benchmarking,
contamination, replicability, and sustainability. In this paper, we invite the
research community to reflect on how these challenges are addressed in SE. Our
results provide a structured overview of current LLM-based SE research at ICSE,
highlighting both encouraging practices and persistent shortcomings. We
conclude with recommendations to strengthen benchmarking rigour, improve
replicability, and address the financial and environmental costs of LLM-based
SE.

</details>


### [74] ["Show Me You Comply... Without Showing Me Anything": Zero-Knowledge Software Auditing for AI-Enabled Systems](https://arxiv.org/abs/2510.26576)
*Filippo Scaramuzza,Renato Cordeiro Ferreira,Tomaz Maia Suller,Giovanni Quattrocchi,Damian Andrew Tamburri,Willem-Jan van den Heuvel*

Main category: cs.SE

TL;DR: 本文提出了ZKMLOps框架，利用零知识证明技术解决AI系统审核中隐私保护与透明度之间的矛盾，实现了可验证的合规性审计。


<details>
  <summary>Details</summary>
Motivation: AI系统在关键领域的广泛应用使得可信性成为关键问题，但传统的软件验证方法不适合黑箱AI模型且成本高昂，而法律又要求高透明度，导致隐私保护与审计需求冲突。

Method: 提出ZKMLOps框架，将零知识证明（ZKP）技术集成于机器学习运维中，实现无需暴露敏感信息即可证明合规性。同时结合软件工程模式，提供模块化、可复用的验证流程。

Result: 通过金融风险审计的法规合规性案例研究和对主流ZKP协议的性能实证评估，展示了框架的实用性与可行性，以及其在不同复杂度ML模型上的性能权衡。

Conclusion: ZKMLOps有效平衡了合规审计中的隐私保护和透明需求，为AI系统提供了一种可验证且实用的审计方案，有助于满足严格的法规要求。

Abstract: The increasing exploitation of Artificial Intelligence (AI) enabled systems
in critical domains has made trustworthiness concerns a paramount showstopper,
requiring verifiable accountability, often by regulation (e.g., the EU AI Act).
Classical software verification and validation techniques, such as procedural
audits, formal methods, or model documentation, are the mechanisms used to
achieve this. However, these methods are either expensive or heavily manual and
ill-suited for the opaque, "black box" nature of most AI models. An intractable
conflict emerges: high auditability and verifiability are required by law, but
such transparency conflicts with the need to protect assets being audited-e.g.,
confidential data and proprietary models-leading to weakened accountability. To
address this challenge, this paper introduces ZKMLOps, a novel MLOps
verification framework that operationalizes Zero-Knowledge Proofs
(ZKPs)-cryptographic protocols allowing a prover to convince a verifier that a
statement is true without revealing additional information-within
Machine-Learning Operations lifecycles. By integrating ZKPs with established
software engineering patterns, ZKMLOps provides a modular and repeatable
process for generating verifiable cryptographic proof of compliance. We
evaluate the framework's practicality through a study of regulatory compliance
in financial risk auditing and assess feasibility through an empirical
evaluation of top ZKP protocols, analyzing performance trade-offs for ML models
of increasing complexity.

</details>


### [75] [Online and Interactive Bayesian Inference Debugging](https://arxiv.org/abs/2510.26579)
*Nathanael Nussbaumer,Markus Böck,Jürgen Cito*

Main category: cs.SE

TL;DR: 本文提出了一种新颖的贝叶斯推断调试方法，极大降低了调试时间和所需专业知识。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯推断调试过程耗时长且需要丰富专业知识，亟需简化调试流程。

Method: 提出一种集成于开发环境的交互式贝叶斯推断调试工具，满足关键需求，方便用户定位和修复推断问题。

Result: 通过18名经验丰富参与者的用户研究，验证该工具显著减少了调试时间和难度。

Conclusion: 所提出的方法和工具有效提升了贝叶斯推断调试的效率和易用性，拓宽了其应用范围。

Abstract: Probabilistic programming is a rapidly developing programming paradigm which
enables the formulation of Bayesian models as programs and the automation of
posterior inference. It facilitates the development of models and conducting
Bayesian inference, which makes these techniques available to practitioners
from multiple fields. Nevertheless, probabilistic programming is notoriously
difficult as identifying and repairing issues with inference requires a lot of
time and deep knowledge. Through this work, we introduce a novel approach to
debugging Bayesian inference that reduces time and required knowledge
significantly. We discuss several requirements a Bayesian inference debugging
framework has to fulfill, and propose a new tool that meets these key
requirements directly within the development environment. We evaluate our
results in a study with 18 experienced participants and show that our approach
to online and interactive debugging of Bayesian inference significantly reduces
time and difficulty on inference debugging tasks.

</details>


### [76] [Stitch: Step-by-step LLM Guided Tutoring for Scratch](https://arxiv.org/abs/2510.26634)
*Yuan Si,Kyle Qi,Daming Li,Hanyuan Shi,Jialu Zhang*

Main category: cs.SE

TL;DR: Stitch 是一种面向 Scratch 语言的交互式教学系统，通过逐步引导替代直接展示答案，利用大语言模型分析学生程序与参考实现的差异，帮助学生理解并逐步修正语义错误，从而提升编程学习效果。


<details>
  <summary>Details</summary>
Motivation: 块状编程环境虽然减少了语法错误，但新手依然难以解决语义错误，现有直接展示正确答案的调试方式不利于培养解决问题的能力。

Method: 设计了 Stitch 系统，通过 Diff-Analyze 模块对比学生作品和参考实现，识别关键差异，并用大语言模型解释差异原因；学生在交互式渲染引擎中逐步理解并局部修正代码。

Result: 通过与现有自动反馈生成工具的实证对比，Stitch 显示出逐步引导教学显著提升学习效果，优于直接展示正确答案的方法。

Conclusion: 直接给出正确程序并非最佳教学方式，分步引导和解释更有效地促进学生理解与问题解决能力，块编程环境中有效反馈形式仍需深入研究。

Abstract: Block-based environments such as Scratch are increasingly popular in
programming education. While block syntax reduces surface errors, semantic bugs
remain common and challenging for novices to resolve. Existing debugging
workflows typically show the correct program directly to learners, a strategy
that may fix errors but undermines the development of problem-solving skills.
  We present Stitch, an interactive tutoring system that replaces "showing the
answer" with step-by-step scaffolding. The system's Diff-Analyze module
contrasts a student's project with a reference implementation, identifies the
most critical differences, and uses a large language model to explain why these
changes matter. Learners inspect highlighted blocks through a custom rendering
engine, understand the explanations, and selectively apply partial fixes. This
iterative process continues until the intended functionality is achieved.
  We evaluate Stitch in an empirical study, comparing it against a
state-of-the-art automated feedback generation tool for Scratch. Our key
insight is that simply presenting the correct program is pedagogically
ineffective. In contrast, our interactive, step-by-step guided system promotes
a more effective learning experience. More broadly, what constitutes effective
feedback in block-based programming remains an open question. Our evaluation
provides new evidence that step-by-step tutoring significantly enhances
learning outcomes, outperforming both direct-answer approaches and current
automated feedback generation tools.

</details>


### [77] [Process-based Indicators of Vulnerability Re-Introducing Code Changes: An Exploratory Case Study](https://arxiv.org/abs/2510.26676)
*Samiha Shimmi,Nicholas M. Synovic,Mona Rahimi,George K. Thiruvathukal*

Main category: cs.SE

TL;DR: 本文研究了软件过程指标与代码变更在漏洞重新引入中的作用，强调漏洞通常是多次开发活动累积的结果。


<details>
  <summary>Details</summary>
Motivation: 当前对漏洞预测多侧重文件级指标，缺乏对过程指标时间演化影响的研究，难以有效预测和防范漏洞重新引入。

Method: 通过对ImageMagick项目的案例研究，结合长期过程指标（如bus factor、issue density和issue spoilage）和76次漏洞重新引入实例，分析漏洞如何通过一系列提交演变和再现。

Result: 发现漏洞重新引入与issue spoilage增加和issue density波动密切相关，反映了问题管理和团队响应的短期低效。

Conclusion: 过程指标结合代码变更对预测和缓解漏洞再引入具有重要价值，建议未来研究综合利用两种指标提升软件安全性。

Abstract: Software vulnerabilities often persist or re-emerge even after being fixed,
revealing the complex interplay between code evolution and socio-technical
factors. While source code metrics provide useful indicators of
vulnerabilities, software engineering process metrics can uncover patterns that
lead to their introduction. Yet few studies have explored whether process
metrics can reveal risky development activities over time -- insights that are
essential for anticipating and mitigating software vulnerabilities. This work
highlights the critical role of process metrics along with code changes in
understanding and mitigating vulnerability reintroduction. We move beyond
file-level prediction and instead analyze security fixes at the commit level,
focusing not only on whether a single fix introduces a vulnerability but also
on the longer sequences of changes through which vulnerabilities evolve and
re-emerge. Our approach emphasizes that reintroduction is rarely the result of
one isolated action, but emerges from cumulative development activities and
socio-technical conditions. To support this analysis, we conducted a case study
on the ImageMagick project by correlating longitudinal process metrics such as
bus factor, issue density, and issue spoilage with vulnerability reintroduction
activities, encompassing 76 instances of reintroduced vulnerabilities. Our
findings show that reintroductions often align with increased issue spoilage
and fluctuating issue density, reflecting short-term inefficiencies in issue
management and team responsiveness. These observations provide a foundation for
broader studies that combine process and code metrics to predict risky fixes
and strengthen software security.

</details>


### [78] [Using Copilot Agent Mode to Automate Library Migration: A Quantitative Assessment](https://arxiv.org/abs/2510.26699)
*Aylton Almeida,Laerte Xavier,Marco Tulio Valente*

Main category: cs.SE

TL;DR: 本文评估了使用GitHub的Copilot Agent Mode自动更新Python库SQLAlchemy的效果，提出了迁移覆盖率度量指标。实验表明，模型可以高覆盖率地完成API迁移，但应用功能维护不足，测试通过率较低。


<details>
  <summary>Details</summary>
Motivation: 维护软件系统以避免技术债务和安全漏洞很重要，但更新库和框架过程费时且易出错。利用大型语言模型自动化此类维护任务具有潜力。

Method: 通过GitHub的Copilot Agent Mode，一种可自动规划和执行多步迁移流程的AI系统，实施了对SQLAlchemy库的更新迁移，并引入迁移覆盖率指标评估迁移效果。

Result: 迁移覆盖率达到100%（中位数），说明API迁移成功，但功能维持不足，导致中位数测试通过率仅为39.75%。

Conclusion: 尽管大型语言模型能高效完成库的API迁移，但当前自动迁移技术仍难以保证应用功能的完整性和稳定性，需进一步优化自动维护能力。

Abstract: Keeping software systems up to date is essential to avoid technical debt,
security vulnerabilities, and the rigidity typical of legacy systems. However,
updating libraries and frameworks remains a time consuming and error-prone
process. Recent advances in Large Language Models (LLMs) and agentic coding
systems offer new opportunities for automating such maintenance tasks. In this
paper, we evaluate the update of a well-known Python library, SQLAlchemy,
across a dataset of ten client applications. For this task, we use the Github's
Copilot Agent Mode, an autonomous AI systema capable of planning and executing
multi-step migration workflows. To assess the effectiveness of the automated
migration, we also introduce Migration Coverage, a metric that quantifies the
proportion of API usage points correctly migrated. The results of our study
show that the LLM agent was capable of migrating functionalities and API usages
between SQLAlchemy versions (migration coverage: 100%, median), but failed to
maintain the application functionality, leading to a low test-pass rate
(39.75%, median).

</details>


### [79] [Optimized Log Parsing with Syntactic Modifications](https://arxiv.org/abs/2510.26793)
*Nafid Enan,Gias Uddin*

Main category: cs.SE

TL;DR: 本文对语法和语义两类日志解析方法及单阶段和两阶段架构进行了系统比较，提出了提升解析准确率的模块SynLog+。


<details>
  <summary>Details</summary>
Motivation: 现有多种日志解析技术性能和特点不一，需系统评估以指导选择和改进日志解析方法。

Method: 通过综合实验比较语法和语义解析器，以及单阶段和两阶段架构的性能，基于实验结果设计了SynLog+模板识别模块作为两阶段架构的第二阶段。

Result: 语义方法在模板识别准确率上更优，语法方法在效率和分组准确率上表现更好，两阶段架构准确率优于单阶段。SynLog+有效提升了语法和语义解析器的准确率，平均提升236%和20%，且几乎不增加运行时间。

Conclusion: 语法和语义方法各有优势，两阶段架构更准确，结合SynLog+模块能显著提升日志解析的准确性和效率，为日志分析提供更可靠的基础。

Abstract: Logs provide valuable insights into system runtime and assist in software
development and maintenance. Log parsing, which converts semi-structured log
data into structured log data, is often the first step in automated log
analysis. Given the wide range of log parsers utilizing diverse techniques, it
is essential to evaluate them to understand their characteristics and
performance. In this paper, we conduct a comprehensive empirical study
comparing syntax- and semantic-based log parsers, as well as single-phase and
two-phase parsing architectures. Our experiments reveal that semantic-based
methods perform better at identifying the correct templates and syntax-based
log parsers are 10 to 1,000 times more efficient and provide better grouping
accuracy although they fall short in accurate template identification.
Moreover, two-phase architecture consistently improves accuracy compared to
single-phase architecture. Based on the findings of this study, we propose
SynLog+, a template identification module that acts as the second phase in a
two-phase log parsing architecture. SynLog+ improves the parsing accuracy of
syntax-based and semantic-based log parsers by 236\% and 20\% on average,
respectively, with virtually no additional runtime cost.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [80] [Magentic Marketplace: An Open-Source Environment for Studying Agentic Markets](https://arxiv.org/abs/2510.25779)
*Gagan Bansal,Wenyue Hua,Zezhou Huang,Adam Fourney,Amanda Swearngin,Will Epperson,Tyler Payne,Jake M. Hofman,Brendan Lucier,Chinmay Singh,Markus Mobius,Akshay Nambi,Archana Yadav,Kevin Gao,David M. Rothschild,Aleksandrs Slivkins,Daniel G. Goldstein,Hussein Mozannar,Nicole Immorlica,Maya Murad,Matthew Vogel,Subbarao Kambhampati,Eric Horvitz,Saleema Amershi*

Main category: cs.MA

TL;DR: 本文研究了大型语言模型（LLM）代理人在模拟市场中的行为，揭示了其在理想与现实条件下的性能差异及存在的偏差问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理被广泛应用于经济决策，理解其在真实市场环境下的行为对于保障用户权益和市场公平至关重要。

Method: 构建了Magentic-Marketplace模拟环境，模拟消费者助手代理与服务代理的双边市场互动，研究代理效用、行为偏差及搜索机制影响。

Result: 实验显示在理想搜索条件下，模型能接近最优社会福利，但随规模扩大性能显著下降，且存在严重首次出价偏差，导致响应速度较质量更具优势。

Conclusion: 研究揭示了市场条件如何影响代理行为，指出需要设计更公平、高效的代理市场机制以避免偏差和效率损失。

Abstract: As LLM agents advance, they are increasingly mediating economic decisions,
ranging from product discovery to transactions, on behalf of users. Such
applications promise benefits but also raise many questions about agent
accountability and value for users. Addressing these questions requires
understanding how agents behave in realistic market conditions. However,
previous research has largely evaluated agents in constrained settings, such as
single-task marketplaces (e.g., negotiation) or structured two-agent
interactions. Real-world markets are fundamentally different: they require
agents to handle diverse economic activities and coordinate within large,
dynamic ecosystems where multiple agents with opaque behaviors may engage in
open-ended dialogues. To bridge this gap, we investigate two-sided agentic
marketplaces where Assistant agents represent consumers and Service agents
represent competing businesses. To study these interactions safely, we develop
Magentic-Marketplace -- a simulated environment where Assistants and Services
can operate. This environment enables us to study key market dynamics: the
utility agents achieve, behavioral biases, vulnerability to manipulation, and
how search mechanisms shape market outcomes. Our experiments show that frontier
models can approach optimal welfare -- but only under ideal search conditions.
Performance degrades sharply with scale, and all models exhibit severe
first-proposal bias, creating 10-30x advantages for response speed over
quality. These findings reveal how behaviors emerge across market conditions,
informing the design of fair and efficient agentic marketplaces.

</details>


### [81] [Multi-Agent Reinforcement Learning for Market Making: Competition without Collusion](https://arxiv.org/abs/2510.25929)
*Ziyi Wang,Carmine Ventre,Maria Polukarov*

Main category: cs.MA

TL;DR: 论文提出了一个层级多智能体强化学习框架，用于研究市场制造中的算法共谋，设计了不同策略的智能体并通过实验分析了其互动态势和市场影响。


<details>
  <summary>Details</summary>
Motivation: 理解不同AI智能体在市场中的交互行为是否会导致共谋及其对整体市场的影响。

Method: 构建包含一个自利市场制造者和三个不同底层竞争者（自利型、竞争型和混合型）的多智能体强化学习框架，设计交互层指标量化行为异质性和系统动态。

Result: 竞争型智能体在零和博弈中表现优势，能优化市场执行效率；混合型智能体能自适应调整策略，在保障自己市场份额的同时降低对其他智能体的负面影响。

Conclusion: 自适应激励机制有助于在异质智能体环境中实现更可持续的战略共存，为算法交易系统中的行为设计提供了结构化分析视角。

Abstract: Algorithmic collusion has emerged as a central question in AI: Will the
interaction between different AI agents deployed in markets lead to collusion?
More generally, understanding how emergent behavior, be it a cartel or market
dominance from more advanced bots, affects the market overall is an important
research question.
  We propose a hierarchical multi-agent reinforcement learning framework to
study algorithmic collusion in market making. The framework includes a
self-interested market maker (Agent~A), which is trained in an uncertain
environment shaped by an adversary, and three bottom-layer competitors: the
self-interested Agent~B1 (whose objective is to maximize its own PnL), the
competitive Agent~B2 (whose objective is to minimize the PnL of its opponent),
and the hybrid Agent~B$^\star$, which can modulate between the behavior of the
other two. To analyze how these agents shape the behavior of each other and
affect market outcomes, we propose interaction-level metrics that quantify
behavioral asymmetry and system-level dynamics, while providing signals
potentially indicative of emergent interaction patterns.
  Experimental results show that Agent~B2 secures dominant performance in a
zero-sum setting against B1, aggressively capturing order flow while tightening
average spreads, thus improving market execution efficiency. In contrast,
Agent~B$^\star$ exhibits a self-interested inclination when co-existing with
other profit-seeking agents, securing dominant market share through adaptive
quoting, yet exerting a milder adverse impact on the rewards of Agents~A and B1
compared to B2. These findings suggest that adaptive incentive control supports
more sustainable strategic co-existence in heterogeneous agent environments and
offers a structured lens for evaluating behavioral design in algorithmic
trading systems.

</details>


### [82] [Stop Wasting Your Tokens: Towards Efficient Runtime Multi-Agent Systems](https://arxiv.org/abs/2510.26585)
*Fulin Lin,Shaowen Chen,Ruishan Fang,Hongwei Wang,Tao Lin*

Main category: cs.MA

TL;DR: 提出了SupervisorAgent，一个轻量模块化的运行时自适应监督框架，能在不改动基础代理架构的情况下，通过实时干预提高多代理系统的效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有多代理系统方法多为事后故障归因，缺乏主动实时干预，导致效率低下和错误多发。

Method: 设计了SupervisorAgent，结合无大模型（LLM）适应性过滤器，在关键节点主动纠正错误、指导低效行为并净化观测。

Result: 在GAIA基准测试中，SupervisorAgent使Smolagent框架令牌消耗降低29.45%，同时保持成功率；在多种任务和多种先进模型上验证了通用性和鲁棒性。

Conclusion: SupervisorAgent有效提升多代理系统的运行效率和稳定性，具有广泛适用性，且不需修改基础代理结构。

Abstract: While Multi-Agent Systems (MAS) excel at complex tasks, their growing
autonomy with operational complexity often leads to critical inefficiencies,
such as excessive token consumption and failures arising from misinformation.
Existing methods primarily focus on post-hoc failure attribution, lacking
proactive, real-time interventions to enhance robustness and efficiency. To
this end, we introduce SupervisorAgent, a lightweight and modular framework for
runtime, adaptive supervision that operates without altering the base agent's
architecture. Triggered by an LLM-free adaptive filter, SupervisorAgent
intervenes at critical junctures to proactively correct errors, guide
inefficient behaviors, and purify observations. On the challenging GAIA
benchmark, SupervisorAgent reduces the token consumption of the Smolagent
framework by an average of 29.45% without compromising its success rate.
Extensive experiments across five additional benchmarks (math reasoning, code
generation, and question answering) and various SoTA foundation models validate
the broad applicability and robustness of our approach. The code is available
at https://github.com/LINs-lab/SupervisorAgent.

</details>


### [83] [A General Incentives-Based Framework for Fairness in Multi-agent Resource Allocation](https://arxiv.org/abs/2510.26740)
*Ashwin Kumar,William Yeoh*

Main category: cs.MA

TL;DR: GIFF是一种基于奖励函数的公平多智能体资源分配框架，旨在在效率和公平之间取得平衡，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 传统资源分配中智能体追求效率导致不公平问题，亟需一种方法兼顾效率与公平。

Method: 利用行动价值函数计算每个行动的局部公平增益，并引入反事实优势修正项，减少对已富裕智能体的过度分配；基于集中式控制由仲裁者使用GIFF修正的Q值进行资源分配。

Result: 在动态拼车、无家可归者预防及复杂岗位分配等领域评测显示，GIFF优于强基线，能发现远见且公平的策略。

Conclusion: GIFF理论基础坚实，公平代理为真实公平改进的下界，调节参数实现单调调节，体现出强健合理的基于RL的公平资源分配框架价值。

Abstract: We introduce the General Incentives-based Framework for Fairness (GIFF), a
novel approach for fair multi-agent resource allocation that infers fair
decision-making from standard value functions. In resource-constrained
settings, agents optimizing for efficiency often create inequitable outcomes.
Our approach leverages the action-value (Q-)function to balance efficiency and
fairness without requiring additional training. Specifically, our method
computes a local fairness gain for each action and introduces a counterfactual
advantage correction term to discourage over-allocation to already well-off
agents. This approach is formalized within a centralized control setting, where
an arbitrator uses the GIFF-modified Q-values to solve an allocation problem.
  Empirical evaluations across diverse domains, including dynamic ridesharing,
homelessness prevention, and a complex job allocation task-demonstrate that our
framework consistently outperforms strong baselines and can discover
far-sighted, equitable policies. The framework's effectiveness is supported by
a theoretical foundation; we prove its fairness surrogate is a principled lower
bound on the true fairness improvement and that its trade-off parameter offers
monotonic tuning. Our findings establish GIFF as a robust and principled
framework for leveraging standard reinforcement learning components to achieve
more equitable outcomes in complex multi-agent systems.

</details>
