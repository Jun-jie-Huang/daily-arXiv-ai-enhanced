{"id": "2602.04982", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.04982", "abs": "https://arxiv.org/abs/2602.04982", "authors": ["Deepak Gupta", "Davis Bartels", "Dina Demner-Fuhsman"], "title": "BioACE: An Automated Framework for Biomedical Answer and Citation Evaluations", "comment": "Work in progress", "summary": "With the increasing use of large language models (LLMs) for generating answers to biomedical questions, it is crucial to evaluate the quality of the generated answers and the references provided to support the facts in the generated answers. Evaluation of text generated by LLMs remains a challenge for question answering, retrieval-augmented generation (RAG), summarization, and many other natural language processing tasks in the biomedical domain, due to the requirements of expert assessment to verify consistency with the scientific literature and complex medical terminology. In this work, we propose BioACE, an automated framework for evaluating biomedical answers and citations against the facts stated in the answers. The proposed BioACE framework considers multiple aspects, including completeness, correctness, precision, and recall, in relation to the ground-truth nuggets for answer evaluation. We developed automated approaches to evaluate each of the aforementioned aspects and performed extensive experiments to assess and analyze their correlation with human evaluations. In addition, we considered multiple existing approaches, such as natural language inference (NLI) and pre-trained language models and LLMs, to evaluate the quality of evidence provided to support the generated answers in the form of citations into biomedical literature. With the detailed experiments and analysis, we provide the best approaches for biomedical answer and citation evaluation as a part of BioACE (https://github.com/deepaknlp/BioACE) evaluation package.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86BioACE\uff0c\u4e00\u79cd\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7efc\u5408\u8bc4\u4f30\u751f\u7269\u533b\u5b66\u9886\u57df\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u7b54\u6848\u53ca\u5176\u5f15\u7528\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bc4\u4f30\u7684\u81ea\u52a8\u5316\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u7269\u533b\u5b66\u95ee\u7b54\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u8bc4\u4ef7\u751f\u6210\u7b54\u6848\u53ca\u5176\u5f15\u7528\u7684\u8d28\u91cf\u53d8\u5f97\u5173\u952e\uff0c\u4e14\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u53d7\u9650\u4e8e\u9700\u8981\u4e13\u5bb6\u8bc4\u5ba1\u548c\u590d\u6742\u533b\u5b66\u672f\u8bed\u3002", "method": "\u63d0\u51faBioACE\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u4ece\u5b8c\u6574\u6027\u3001\u6b63\u786e\u6027\u3001\u7cbe\u786e\u6027\u53ca\u53ec\u56de\u7387\u591a\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u7b54\u6848\uff0c\u5e76\u5229\u7528\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u53ca\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u5f15\u7528\u7684\u8bc1\u636e\u8d28\u91cf\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\uff0cBioACE\u5c55\u793a\u4e86\u5176\u8bc4\u4f30\u65b9\u6cd5\u4e0e\u4eba\u7c7b\u8bc4\u4ef7\u9ad8\u5ea6\u76f8\u5173\uff0c\u5e76\u63d0\u4f9b\u4e86\u6700\u4f73\u7684\u751f\u7269\u533b\u5b66\u7b54\u6848\u53ca\u5f15\u7528\u8bc4\u4ef7\u65b9\u6cd5\u3002", "conclusion": "BioACE\u6846\u67b6\u6709\u6548\u5730\u8bc4\u4f30\u4e86\u751f\u7269\u533b\u5b66\u9886\u57df\u4e2d\u7531\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u7b54\u6848\u53ca\u5176\u5f15\u7528\u7684\u8d28\u91cf\uff0c\u4f53\u73b0\u4e86\u8f83\u9ad8\u7684\u4eba\u7c7b\u8bc4\u4ef7\u76f8\u5173\u6027\u3002"}}
{"id": "2602.05004", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05004", "abs": "https://arxiv.org/abs/2602.05004", "authors": ["Zexin Lin", "Jiachen Yu", "Haoyang Zhang", "Yuzhao Li", "Zhonghang Li", "Yujiu Yang", "Junjie Wang", "Xiaoqiang Ji"], "title": "CoWork-X: Experience-Optimized Co-Evolution for Multi-Agent Collaboration System", "comment": null, "summary": "Large language models are enabling language-conditioned agents in interactive environments, but highly cooperative tasks often impose two simultaneous constraints: sub-second real-time coordination and sustained multi-episode adaptation under a strict online token budget. Existing approaches either rely on frequent in-episode reasoning that induces latency and timing jitter, or deliver post-episode improvements through unstructured text that is difficult to compile into reliable low-cost execution. We propose CoWork-X, an active co-evolution framework that casts peer collaboration as a closed-loop optimization problem across episodes, inspired by fast--slow memory separation. CoWork-X instantiates a Skill-Agent that executes via HTN (hierarchical task network)-based skill retrieval from a structured, interpretable, and compositional skill library, and a post-episode Co-Optimizer that performs patch-style skill consolidation with explicit budget constraints and drift regularization. Experiments in challenging Overcooked-AI-like realtime collaboration benchmarks demonstrate that CoWork-X achieves stable, cumulative performance gains while steadily reducing online latency and token usage.", "AI": {"tldr": "\u63d0\u51faCoWork-X\uff0c\u901a\u8fc7\u5c42\u6b21\u5316\u6280\u80fd\u68c0\u7d22\u548c\u56de\u5408\u540e\u4f18\u5316\uff0c\u5b9e\u73b0\u5b9e\u65f6\u534f\u4f5c\u4efb\u52a1\u4e2d\u7684\u9ad8\u6548\u534f\u540c\u548c\u5728\u7ebf\u8d44\u6e90\u8282\u7ea6\uff0c\u663e\u8457\u63d0\u5347\u5408\u4f5c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u9ad8\u5ea6\u5408\u4f5c\u4efb\u52a1\u4e2d\u8981\u4e48\u5bfc\u81f4\u5ef6\u8fdf\u548c\u65f6\u95f4\u6296\u52a8\uff0c\u8981\u4e48\u901a\u8fc7\u65e0\u7ed3\u6784\u6587\u672c\u8fdb\u884c\u6539\u8fdb\uff0c\u96be\u4ee5\u5b9e\u73b0\u4f4e\u6210\u672c\u53ef\u9760\u6267\u884c\uff0c\u9700\u89e3\u51b3\u5b9e\u65f6\u534f\u8c03\u4e0e\u591a\u56de\u5408\u9002\u5e94\u7684\u77db\u76fe\u3002", "method": "\u63d0\u51faCoWork-X\u6846\u67b6\uff0c\u5305\u62ecSkill-Agent\u901a\u8fc7\u5c42\u6b21\u4efb\u52a1\u7f51\u7edc\uff08HTN\uff09\u6267\u884c\u7ed3\u6784\u5316\u6280\u80fd\u68c0\u7d22\uff0c\u4ee5\u53ca\u56de\u5408\u540eCo-Optimizer\u8fdb\u884c\u5e26\u7ea6\u675f\u7684\u6280\u80fd\u6574\u5408\uff0c\u5b9e\u73b0\u534f\u540c\u4f18\u5316\u3002", "result": "\u5728Overcooked-AI\u98ce\u683c\u7684\u5b9e\u65f6\u534f\u4f5c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCoWork-X\u5b9e\u73b0\u4e86\u7a33\u5b9a\u7684\u6027\u80fd\u7d2f\u8ba1\u63d0\u5347\uff0c\u540c\u65f6\u6301\u7eed\u964d\u4f4e\u4e86\u5728\u7ebf\u5ef6\u8fdf\u548c\u4ee4\u724c\u4f7f\u7528\u91cf\u3002", "conclusion": "CoWork-X\u6846\u67b6\u6709\u6548\u5b9e\u73b0\u4e86\u9ad8\u5ea6\u534f\u540c\u4efb\u52a1\u4e2d\u7684\u5b9e\u65f6\u534f\u8c03\u4e0e\u591a\u56de\u5408\u6301\u7eed\u9002\u5e94\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5408\u4f5c\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u5728\u7ebf\u5ef6\u8fdf\u548c\u4ee4\u724c\u4f7f\u7528\u3002"}}
{"id": "2602.05035", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05035", "abs": "https://arxiv.org/abs/2602.05035", "authors": ["Sean Trott", "Pamela D. Rivi\u00e8re"], "title": "Capacity Constraints and the Multilingual Penalty for Lexical Disambiguation", "comment": "9 pages, 5 figures, conference", "summary": "Multilingual language models (LMs) sometimes under-perform their monolingual counterparts, possibly due to capacity limitations. We quantify this ``multilingual penalty'' for lexical disambiguation--a task requiring precise semantic representations and contextualization mechanisms--using controlled datasets of human relatedness judgments for ambiguous words in both English and Spanish. Comparing monolingual and multilingual LMs from the same families, we find consistently reduced performance in multilingual LMs. We then explore three potential capacity constraints: representational (reduced embedding isotropy), attentional (reduced attention to disambiguating cues), and vocabulary-related (increased multi-token segmentation). Multilingual LMs show some evidence of all three limitations; moreover, these factors statistically account for the variance formerly attributed to a model's multilingual status. These findings suggest both that multilingual LMs do suffer from multiple capacity constraints, and that these constraints correlate with reduced disambiguation performance.", "AI": {"tldr": "\u591a\u8bed\u79cd\u8bed\u8a00\u6a21\u578b\u5728\u8bcd\u4e49\u6d88\u6b67\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u53d7\u9650\u4e8e\u591a\u79cd\u5bb9\u91cf\u7ea6\u675f\uff0c\u5f71\u54cd\u4e86\u5b83\u4eec\u6355\u6349\u8bed\u4e49\u548c\u4e0a\u4e0b\u6587\u7684\u80fd\u529b\u3002", "motivation": "\u591a\u8bed\u79cd\u8bed\u8a00\u6a21\u578b\u5728\u67d0\u4e9b\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u5982\u5355\u8bed\u6a21\u578b\uff0c\u53ef\u80fd\u7531\u4e8e\u6a21\u578b\u5bb9\u91cf\u9650\u5236\uff0c\u7814\u7a76\u8fd9\u4e00\u73b0\u8c61\u6709\u52a9\u4e8e\u7406\u89e3\u591a\u8bed\u79cd\u6a21\u578b\u7684\u6027\u80fd\u74f6\u9888\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u5355\u8bed\u548c\u591a\u8bed\u79cd\u8bed\u8a00\u6a21\u578b\u5728\u82f1\u8bed\u548c\u897f\u73ed\u7259\u8bed\u6a21\u7cca\u8bcd\u7684\u76f8\u5173\u6027\u5224\u65ad\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u5206\u6790\u5bb9\u91cf\u9650\u5236\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u591a\u8bed\u79cd\u6a21\u578b\u5728\u8868\u793a\u5747\u5300\u6027\u964d\u4f4e\u3001\u6ce8\u610f\u529b\u5206\u914d\u4e0d\u8db3\u548c\u8bcd\u6c47\u5207\u5206\u591a\u6837\u6027\u589e\u52a0\u7b49\u65b9\u9762\u5b58\u5728\u9650\u5236\uff0c\u8fd9\u4e9b\u56e0\u7d20\u5171\u540c\u89e3\u91ca\u4e86\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "\u591a\u8bed\u79cd\u8bed\u8a00\u6a21\u578b\u5728\u8bcd\u4e49\u6d88\u6b67\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u53ca\u5355\u8bed\u6a21\u578b\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u8868\u793a\u7a7a\u95f4\u3001\u6ce8\u610f\u529b\u673a\u5236\u548c\u8bcd\u6c47\u5207\u5206\u7b49\u5bb9\u91cf\u9650\u5236\u3002"}}
{"id": "2602.05524", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05524", "abs": "https://arxiv.org/abs/2602.05524", "authors": ["Konosuke Yoshizato", "Kazuma Shimizu", "Ryota Higa", "Takanobu Otsuka"], "title": "AI Agent Systems for Supply Chains: Structured Decision Prompts and Memory Retrieval", "comment": "A full version of the extended abstract accepted by the 25th International Conference on Autonomous Agents and Multiagent Systems(AAMAS 2026)", "summary": "This study investigates large language model (LLM) -based multi-agent systems (MASs) as a promising approach to inventory management, which is a key component of supply chain management. Although these systems have gained considerable attention for their potential to address the challenges associated with typical inventory management methods, key uncertainties regarding their effectiveness persist. Specifically, it is unclear whether LLM-based MASs can consistently derive optimal ordering policies and adapt to diverse supply chain scenarios. To address these questions, we examine an LLM-based MAS with a fixed-ordering strategy prompt that encodes the stepwise processes of the problem setting and a safe-stock strategy commonly used in inventory management. Our empirical results demonstrate that, even without detailed prompt adjustments, an LLM-based MAS can determine optimal ordering decisions in a restricted scenario. To enhance adaptability, we propose a novel agent called AIM-RM, which leverages similar historical experiences through similarity matching. Our results show that AIM-RM outperforms benchmark methods across various supply chain scenarios, highlighting its robustness and adaptability.", "AI": {"tldr": "\u7814\u7a76\u8bc1\u5b9e\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u5e93\u5b58\u7ba1\u7406\u4e2d\u5177\u5907\u63a8\u5bfc\u6700\u4f18\u8ba2\u8d27\u7b56\u7565\u7684\u80fd\u529b\uff0c\u4e14\u65b0\u63d0\u51fa\u7684AIM-RM\u667a\u80fd\u4f53\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u5728\u590d\u6742\u591a\u6837\u4f9b\u5e94\u94fe\u73af\u5883\u4e0b\u7684\u8868\u73b0\u548c\u9002\u5e94\u80fd\u529b\u3002", "motivation": "\u5e93\u5b58\u7ba1\u7406\u4f5c\u4e3a\u4f9b\u5e94\u94fe\u7ba1\u7406\u7684\u5173\u952e\u73af\u8282\uff0c\u4f20\u7edf\u65b9\u6cd5\u9762\u4e34\u4f17\u591a\u6311\u6218\u3002\u867d\u7136\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u88ab\u770b\u4f5c\u5177\u6709\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u7684\u6f5c\u529b\uff0c\u4f46\u5176\u662f\u5426\u80fd\u6301\u7eed\u63a8\u5bfc\u6700\u4f18\u7b56\u7565\u5e76\u9002\u5e94\u4e0d\u540c\u573a\u666f\u5c1a\u4e0d\u660e\u786e\uff0c\u6709\u5fc5\u8981\u6df1\u5165\u7814\u7a76\u3002", "method": "\u672c\u7814\u7a76\u9996\u5148\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u56fa\u5b9a\u8ba2\u8d27\u7b56\u7565\u7684\u63d0\u793a\u8bed\uff0c\u7ed3\u5408\u5e93\u5b58\u7ba1\u7406\u4e2d\u7684\u5b89\u5168\u5e93\u5b58\u7b56\u7565\uff0c\u6765\u7f16\u7801\u95ee\u9898\u7684\u5206\u6b65\u9aa4\u6d41\u7a0b\u3002\u7136\u540e\u63d0\u51fa\u4e86AIM-RM\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u76f8\u4f3c\u5ea6\u5339\u914d\u5386\u53f2\u7ecf\u9a8c\uff0c\u63d0\u5347\u7cfb\u7edf\u7684\u9002\u5e94\u80fd\u529b\u3002\u91c7\u7528\u591a\u79cd\u4f9b\u5e94\u94fe\u573a\u666f\u4e0b\u7684\u5b9e\u9a8c\u5bf9\u6bd4\u5206\u6790\u65b9\u6cd5\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u56fa\u5b9a\u8ba2\u8d27\u7b56\u7565\u7684LLM-based MAS\u5728\u6709\u9650\u573a\u666f\u4e0b\u80fd\u5b9e\u73b0\u6700\u4f18\u8ba2\u8d27\u51b3\u7b56\uff0c\u800c\u5f15\u5165AIM-RM\u667a\u80fd\u4f53\u540e\uff0c\u7cfb\u7edf\u5728\u591a\u4e2a\u4f9b\u5e94\u94fe\u573a\u666f\u4e2d\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08LLM-based MASs\uff09\u80fd\u591f\u5728\u53d7\u9650\u573a\u666f\u4e0b\u5b9e\u73b0\u5e93\u5b58\u7ba1\u7406\u7684\u6700\u4f18\u8ba2\u8d27\u51b3\u7b56\uff0c\u4e14\u901a\u8fc7\u5f15\u5165\u7c7b\u4f3c\u5386\u53f2\u7ecf\u9a8c\u5339\u914d\u7684\u65b0\u578b\u667a\u80fd\u4f53AIM-RM\uff0c\u7cfb\u7edf\u5728\u591a\u6837\u5316\u4f9b\u5e94\u94fe\u60c5\u5883\u4e2d\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2602.05085", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05085", "abs": "https://arxiv.org/abs/2602.05085", "authors": ["Sidi Lu", "Zhenwen Liang", "Dongyang Ma", "Yan Wang", "Haitao Mi", "Dong Yu"], "title": "Locas: Your Models are Principled Initializers of Locally-Supported Parametric Memories", "comment": "Tencent AI Lab Technical Report", "summary": "In this paper, we aim to bridge test-time-training with a new type of parametric memory that can be flexibly offloaded from or merged into model parameters. We present Locas, a Locally-Supported parametric memory that shares the design of FFN blocks in modern transformers, allowing it to be flexibly permanentized into the model parameters while supporting efficient continual learning. We discuss two major variants of Locas: one with a conventional two-layer MLP design that has a clearer theoretical guarantee; the other one shares the same GLU-FFN structure with SOTA LLMs, and can be easily attached to existing models for both parameter-efficient and computation-efficient continual learning. Crucially, we show that proper initialization of such low-rank sideway-FFN-style memories -- performed in a principled way by reusing model parameters, activations and/or gradients -- is essential for fast convergence, improved generalization, and catastrophic forgetting prevention. We validate the proposed memory mechanism on the PG-19 whole-book language modeling and LoCoMo long-context dialogue question answering tasks. With only 0.02\\% additional parameters in the lowest case, Locas-GLU is capable of storing the information from past context while maintaining a much smaller context window. In addition, we also test the model's general capability loss after memorizing the whole book with Locas, through comparative MMLU evaluation. Results show the promising ability of Locas to permanentize past context into parametric knowledge with minimized catastrophic forgetting of the model's existing internal knowledge.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLocas\uff0c\u4e00\u79cd\u57fa\u4e8e\u5c40\u90e8\u53c2\u6570\u5316\u8bb0\u5fc6\u7684\u673a\u5236\uff0c\u901a\u8fc7\u4e0eTransformer FFN\u6a21\u5757\u8bbe\u8ba1\u7ed3\u5408\uff0c\u5b9e\u73b0\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u4e2d\u9ad8\u6548\u8bb0\u5fc6\u548c\u6301\u7eed\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5728\u8bed\u8a00\u6a21\u578b\u548c\u5bf9\u8bdd\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u89e3\u51b3\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u4e2d\u6a21\u578b\u6301\u7eed\u5b66\u4e60\u4e0e\u53c2\u6570\u8bb0\u5fc6\u96be\u4ee5\u517c\u987e\u7684\u95ee\u9898\uff0c\u5e0c\u671b\u901a\u8fc7\u8bbe\u8ba1\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u53c2\u6570\u5316\u8bb0\u5fc6\u673a\u5236\uff0c\u5b9e\u73b0\u5bf9\u8fc7\u53bb\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u5b58\u50a8\u53ca\u707e\u96be\u6027\u9057\u5fd8\u7684\u9632\u6b62\u3002", "method": "\u63d0\u51faLocas\uff0c\u4e00\u79cd\u4e0e\u73b0\u4ee3Transformer\u4e2d\u7684FFN\u6a21\u5757\u7ed3\u6784\u76f8\u4f3c\u7684\u5c40\u90e8\u652f\u6301\u53c2\u6570\u5316\u8bb0\u5fc6\uff0c\u5305\u62ec\u4e24\u79cd\u53d8\u4f53\uff08\u4f20\u7edf\u4e24\u5c42MLP\u548cGLU-FFN\u7ed3\u6784\uff09\uff0c\u5e76\u901a\u8fc7\u5408\u7406\u521d\u59cb\u5316\u5229\u7528\u6a21\u578b\u53c2\u6570\u3001\u6fc0\u6d3b\u548c\u68af\u5ea6\u5b9e\u73b0\u5feb\u901f\u6536\u655b\u548c\u4f18\u826f\u6cdb\u5316\u3002", "result": "Locas-GLU\u4ee5\u6781\u5c11\u7684\u989d\u5916\u53c2\u6570\uff08\u6700\u4f4e0.02%\uff09\u5b9e\u73b0\u4e86\u5bf9\u8fc7\u53bb\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u6709\u6548\u5b58\u50a8\uff0c\u4e3a\u6a21\u578b\u5e26\u6765\u66f4\u5c0f\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u9700\u6c42\uff0c\u540c\u65f6\u5728PG-19\u548cLoCoMo\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u826f\uff0c\u4e14\u6a21\u578b\u5728\u8bb0\u5fc6\u5b8c\u5168\u6587\u672c\u540e\u5728MMLU\u8bc4\u6d4b\u4e2d\u8868\u73b0\u7a33\u5b9a\uff0c\u9a8c\u8bc1\u4e86\u5176\u51cf\u5c11\u5df2\u6709\u77e5\u8bc6\u9057\u5fd8\u7684\u80fd\u529b\u3002", "conclusion": "Locas\u901a\u8fc7\u5f15\u5165\u5c40\u90e8\u652f\u6301\u7684\u53c2\u6570\u5316\u8bb0\u5fc6\u673a\u5236\uff0c\u6709\u6548\u5730\u5b9e\u73b0\u4e86\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u4e0e\u6a21\u578b\u53c2\u6570\u7684\u7075\u6d3b\u7ed3\u5408\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u5e76\u51cf\u5c11\u4e86\u707e\u96be\u6027\u9057\u5fd8\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u957f\u6587\u672c\u5efa\u6a21\u548c\u5bf9\u8bdd\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u4f18\u8d8a\u8868\u73b0\u3002"}}
{"id": "2602.05965", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05965", "abs": "https://arxiv.org/abs/2602.05965", "authors": ["Joseph Fioresi", "Parth Parag Kulkarni", "Ashmal Vayani", "Song Wang", "Mubarak Shah"], "title": "Learning to Share: Selective Memory for Efficient Parallel Agentic Systems", "comment": null, "summary": "Agentic systems solve complex tasks by coordinating multiple agents that iteratively reason, invoke tools, and exchange intermediate results. To improve robustness and solution quality, recent approaches deploy multiple agent teams running in parallel to explore diverse reasoning trajectories. However, parallel execution comes at a significant computational cost: when different teams independently reason about similar sub-problems or execute analogous steps, they repeatedly perform substantial overlapping computation. To address these limitations, in this paper, we propose Learning to Share (LTS), a learned shared-memory mechanism for parallel agentic frameworks that enables selective cross-team information reuse while controlling context growth. LTS introduces a global memory bank accessible to all teams and a lightweight controller that decides whether intermediate agent steps should be added to memory or not. The controller is trained using stepwise reinforcement learning with usage-aware credit assignment, allowing it to identify information that is globally useful across parallel executions. Experiments on the AssistantBench and GAIA benchmarks show that LTS significantly reduces overall runtime while matching or improving task performance compared to memory-free parallel baselines, demonstrating that learned memory admission is an effective strategy for improving the efficiency of parallel agentic systems. Project page: https://joefioresi718.github.io/LTS_webpage/", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLTS\uff0c\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5171\u4eab\u5185\u5b58\u673a\u5236\uff0c\u4f18\u5316\u4e86\u591a\u56e2\u961f\u5e76\u884c\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u4fe1\u606f\u590d\u7528\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u591a\u56e2\u961f\u5e76\u884c\u6267\u884c\u65f6\u5b58\u5728\u5927\u91cf\u91cd\u590d\u8ba1\u7b97\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u63d0\u51fa\u5b66\u4e60\u5171\u4eab\u673a\u5236\uff08LTS\uff09\uff0c\u5f15\u5165\u5168\u5c40\u5171\u4eab\u5185\u5b58\u548c\u8f7b\u91cf\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u9010\u6b65\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u63a7\u5236\u5668\u51b3\u5b9a\u662f\u5426\u5c06\u4e2d\u95f4\u6b65\u9aa4\u52a0\u5165\u5171\u4eab\u5185\u5b58\uff0c\u5b9e\u73b0\u4fe1\u606f\u8de8\u56e2\u961f\u590d\u7528\u3002", "result": "LTS\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u663e\u8457\u51cf\u5c11\u8fd0\u884c\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u4efb\u52a1\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5b66\u4e60\u5f0f\u5185\u5b58\u7ba1\u7406\u7b56\u7565\u63d0\u5347\u5e76\u884c\u667a\u80fd\u4f53\u7cfb\u7edf\u6548\u7387\u7684\u6709\u6548\u6027\u3002", "conclusion": "LTS\u6210\u529f\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5e76\u884c\u6267\u884c\u4e2d\u7684\u5197\u4f59\u8ba1\u7b97\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u4efb\u52a1\u8868\u73b0\u3002"}}
{"id": "2602.04910", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.04910", "abs": "https://arxiv.org/abs/2602.04910", "authors": ["Nongyu Di", "Tianyu Chen", "Shan Lu", "Shuai Lu", "Yeyun Gong", "Peng Cheng", "Jacob R. Lorch", "Yuan Yao", "Xiaoxing Ma"], "title": "Reducing the Costs of Proof Synthesis on Rust Systems by Scaling Up a Seed Training Set", "comment": null, "summary": "Large Language Models (LLMs) are widely used for code generation. However, the correctness of code generated by LLMs remains a concern. A potential remedy to this concern is to have LLMs generate formal correctness proofs along with such code. However, compared with code generation, code-proof generation requires much higher reasoning capability and has much less existing data to learn from. In this paper, we present VeruSyn, a data synthesis pipeline for Verus, a state-of-the-art verification tool for system software written in Rust. Through self-synthesis and tutorial-based synthesis, VeruSyn achieves much larger scale and Verus-feature coverage than previous data-synthesis techniques designed for Verus; VeruSyn also supplements its dataset with long-chain-of-thought (CoT) data through agent trajectory synthesis. With VeruSyn, we synthesize the largest set of Verus verified programs: 6.9 million Rust programs, each with a formal specification and a proof that it meets that specification. This dataset lets us create a fine-tuned Qwen2.5-Coder-32B-Instruct model with appealing cost-proof tradeoff compared with state-of-the-art commercial models like Claude Sonnet 4.5. It also significantly outperforms models like o4-mini and previously proposed research models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7VeruSyn\u6d41\u6c34\u7ebf\u5927\u89c4\u6a21\u5408\u6210\u5f62\u5f0f\u9a8c\u8bc1Rust\u7a0b\u5e8f\u6570\u636e\uff0c\u63d0\u5347\u4e86\u4ee3\u7801\u8bc1\u660e\u751f\u6210\u80fd\u529b\uff0c\u8bad\u7ec3\u6a21\u578b\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u9876\u5c16\u5546\u4e1a\u548c\u7814\u7a76\u6a21\u578b\u3002", "motivation": "\u73b0\u6709LLM\u751f\u6210\u4ee3\u7801\u6b63\u786e\u6027\u5b58\u5728\u7591\u8651\uff0c\u5c24\u5176\u662f\u4ee3\u7801\u8bc1\u660e\u751f\u6210\u6bd4\u4ee3\u7801\u751f\u6210\u66f4\u5177\u6311\u6218\u4e14\u7f3a\u4e4f\u8bad\u7ec3\u6570\u636e\uff0c\u9700\u8981\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u7684\u5f62\u5f0f\u9a8c\u8bc1\u7a0b\u5e8f\u6570\u636e\u652f\u6301\u3002", "method": "\u8bbe\u8ba1VeruSyn\u6570\u636e\u5408\u6210\u6d41\u6c34\u7ebf\uff0c\u7ed3\u5408\u81ea\u6211\u5408\u6210\u3001\u57fa\u4e8e\u6559\u7a0b\u7684\u5408\u6210\u548c\u957f\u94fe\u601d\u8def\uff08CoT\uff09\u6570\u636e\u8865\u5145\uff0c\u751f\u6210\u5305\u542b\u5f62\u5f0f\u8bc1\u660e\u7684Rust\u7a0b\u5e8f\u6570\u636e\u96c6\uff0c\u5e76\u7528\u4ee5\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5408\u6210\u4e86\u6700\u5927\u89c4\u6a21\u7684\u7ea6690\u4e07\u4efd\u5e26\u6709\u89c4\u8303\u548c\u8bc1\u660e\u7684Rust\u7a0b\u5e8f\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u51fa\u7684Qwen2.5-Coder-32B-Instruct\u6a21\u578b\u5728\u6210\u672c\u4e0e\u8bc1\u660e\u8d28\u91cf\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u5546\u4e1a\u548c\u7814\u7a76\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7VeruSyn\u6570\u636e\u5408\u6210\u6d41\u6c34\u7ebf\uff0c\u80fd\u5927\u89c4\u6a21\u751f\u6210\u5e26\u6709\u5f62\u5f0f\u89c4\u8303\u548c\u8bc1\u660e\u7684Rust\u7a0b\u5e8f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u8bc1\u660e\u751f\u6210\u7684\u89c4\u6a21\u548c\u8d28\u91cf\uff0c\u63a8\u52a8\u4ee3\u7801\u751f\u6210\u7684\u6b63\u786e\u6027\u4fdd\u969c\u3002"}}
{"id": "2602.05106", "categories": ["cs.CL", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.05106", "abs": "https://arxiv.org/abs/2602.05106", "authors": ["Michael Browder", "Kevin Duh", "J. David Harris", "Vince Lyzinski", "Paul McNamee", "Youngser Park", "Carey E. Priebe", "Peter Viechnicki"], "title": "Data Kernel Perspective Space Performance Guarantees for Synthetic Data from Transformer Models", "comment": null, "summary": "Scarcity of labeled training data remains the long pole in the tent for building performant language technology and generative AI models. Transformer models -- particularly LLMs -- are increasingly being used to mitigate the data scarcity problem via synthetic data generation. However, because the models are black boxes, the properties of the synthetic data are difficult to predict. In practice it is common for language technology engineers to 'fiddle' with the LLM temperature setting and hope that what comes out the other end improves the downstream model. Faced with this uncertainty, here we propose Data Kernel Perspective Space (DKPS) to provide the foundation for mathematical analysis yielding concrete statistical guarantees for the quality of the outputs of transformer models. We first show the mathematical derivation of DKPS and how it provides performance guarantees. Next we show how DKPS performance guarantees can elucidate performance of a downstream task, such as neural machine translation models or LLMs trained using Contrastive Preference Optimization (CPO). Limitations of the current work and future research are also discussed.", "AI": {"tldr": "\u9762\u5bf9\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u548c\u5408\u6210\u6570\u636e\u6027\u8d28\u4e0d\u53ef\u63a7\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u6570\u636e\u6838\u900f\u89c6\u7a7a\u95f4\uff08DKPS\uff09\uff0c\u63d0\u4f9b\u4e86\u6570\u5b66\u6027\u80fd\u4fdd\u8bc1\uff0c\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u548c\u4e0b\u6e38\u4efb\u52a1\u7684\u8bad\u7ec3\u6548\u679c\u3002", "motivation": "\u6807\u7b7e\u8bad\u7ec3\u6570\u636e\u532e\u4e4f\u5bfc\u81f4\u8bed\u8a00\u6280\u672f\u548c\u751f\u6210\u5f0fAI\u6a21\u578b\u6027\u80fd\u53d7\u9650\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u5408\u6210\u6570\u636e\u6027\u8d28\u7684\u9884\u6d4b\u548c\u4fdd\u8bc1\u3002", "method": "\u672c\u6587\u6570\u5b66\u63a8\u5bfc\u4e86DKPS\uff0c\u5e76\u57fa\u4e8e\u5176\u6027\u80fd\u4fdd\u8bc1\u5bf9\u4f7f\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\u6216\u57fa\u4e8e\u5bf9\u6bd4\u504f\u597d\u4f18\u5316\u8bad\u7ec3\u7684LLM\uff09\u8fdb\u884c\u4e86\u6027\u80fd\u5206\u6790\u3002", "result": "DKPS\u4e3a\u53d8\u6362\u5668\u6a21\u578b\u8f93\u51fa\u7684\u6570\u636e\u8d28\u91cf\u63d0\u4f9b\u4e86\u5177\u4f53\u7684\u6027\u80fd\u4fdd\u8bc1\uff0c\u80fd\u591f\u6307\u5bfc\u548c\u89e3\u91ca\u4e0b\u6e38\u4efb\u52a1\u7684\u8868\u73b0\uff0c\u63a8\u52a8\u66f4\u53ef\u9760\u7684\u5408\u6210\u6570\u636e\u5e94\u7528\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u6570\u636e\u6838\u900f\u89c6\u7a7a\u95f4\uff08DKPS\uff09\u4e3a\u5229\u7528\u53d8\u6362\u5668\u6a21\u578b\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u63d0\u4f9b\u4e86\u6570\u5b66\u5206\u6790\u57fa\u7840\u548c\u7edf\u8ba1\u8d28\u91cf\u4fdd\u8bc1\uff0c\u5e2e\u52a9\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002"}}
{"id": "2602.06030", "categories": ["cs.MA", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06030", "abs": "https://arxiv.org/abs/2602.06030", "authors": ["Kavana Venkatesh", "Yinhan He", "Jundong Li", "Jiaming Cui"], "title": "PhysicsAgentABM: Physics-Guided Generative Agent-Based Modeling", "comment": null, "summary": "Large language model (LLM)-based multi-agent systems enable expressive agent reasoning but are expensive to scale and poorly calibrated for timestep-aligned state-transition simulation, while classical agent-based models (ABMs) offer interpretability but struggle to integrate rich individual-level signals and non-stationary behaviors. We propose PhysicsAgentABM, which shifts inference to behaviorally coherent agent clusters: state-specialized symbolic agents encode mechanistic transition priors, a multimodal neural transition model captures temporal and interaction dynamics, and uncertainty-aware epistemic fusion yields calibrated cluster-level transition distributions. Individual agents then stochastically realize transitions under local constraints, decoupling population inference from entity-level variability. We further introduce ANCHOR, an LLM agent-driven clustering strategy based on cross-contextual behavioral responses and a novel contrastive loss, reducing LLM calls by up to 6-8 times. Experiments across public health, finance, and social sciences show consistent gains in event-time accuracy and calibration over mechanistic, neural, and LLM baselines. By re-architecting generative ABM around population-level inference with uncertainty-aware neuro-symbolic fusion, PhysicsAgentABM establishes a new paradigm for scalable and calibrated simulation with LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPhysicsAgentABM\uff0c\u901a\u8fc7\u5c06\u63a8\u65ad\u8f6c\u5411\u884c\u4e3a\u4e00\u81f4\u7684\u4ee3\u7406\u7c07\uff0c\u878d\u5408\u7b26\u53f7\u4e0e\u795e\u7ecf\u6a21\u578b\uff0c\u63d0\u5347\u591a\u667a\u80fd\u4f53\u72b6\u6001\u8f6c\u79fb\u6a21\u62df\u7684\u51c6\u786e\u6027\u548c\u6821\u51c6\u5ea6\uff0c\u540c\u65f6\u4f7f\u7528ANCHOR\u805a\u7c7b\u7b56\u7565\u5927\u5e45\u51cf\u5c11LLM\u8c03\u7528\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u4eff\u771f\u65b9\u6cd5\u3002", "motivation": "\u5927\u89c4\u6a21\u53ef\u6269\u5c55\u4e14\u7cbe\u51c6\u6821\u51c6\u7684\u591a\u667a\u80fd\u4f53\u72b6\u6001\u8f6c\u79fb\u6a21\u62df\u56f0\u96be\uff0c\u4f20\u7edfLLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u6821\u51c6\u5dee\uff0c\u800c\u7ecf\u5178\u4ee3\u7406\u6a21\u578b\u867d\u5177\u53ef\u89e3\u91ca\u6027\uff0c\u4f46\u96be\u4ee5\u878d\u5408\u4e30\u5bcc\u7684\u4e2a\u4f53\u4fe1\u53f7\u548c\u975e\u5e73\u7a33\u884c\u4e3a\u3002", "method": "\u91c7\u7528\u72b6\u6001\u4e13\u7528\u7684\u7b26\u53f7\u4ee3\u7406\u7f16\u7801\u673a\u7406\u8f6c\u79fb\u5148\u9a8c\uff0c\u4f7f\u7528\u591a\u6a21\u6001\u795e\u7ecf\u8f6c\u79fb\u6a21\u578b\u6355\u6349\u65f6\u95f4\u548c\u4ea4\u4e92\u52a8\u6001\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u8ba4\u77e5\u878d\u5408\u83b7\u5f97\u6821\u51c6\u7684\u7c07\u7ea7\u8f6c\u79fb\u5206\u5e03\u3002\u5f15\u5165ANCHOR\u805a\u7c7b\u7b56\u7565\uff0c\u57fa\u4e8e\u8de8\u4e0a\u4e0b\u6587\u884c\u4e3a\u54cd\u5e94\u548c\u5bf9\u6bd4\u635f\u5931\u51cf\u5c11LLM\u8c03\u7528\u6b21\u6570\u3002", "result": "\u5728\u516c\u5171\u536b\u751f\u3001\u91d1\u878d\u548c\u793e\u4f1a\u79d1\u5b66\u9886\u57df\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPhysicsAgentABM\u5728\u4e8b\u4ef6\u65f6\u95f4\u51c6\u786e\u6027\u548c\u6821\u51c6\u65b9\u9762\u5747\u4f18\u4e8e\u673a\u7406\u3001\u795e\u7ecf\u548cLLM\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u901a\u8fc7\u884c\u4e3a\u805a\u7c7b\u663e\u8457\u964d\u4f4e\u4e86LLM\u8c03\u7528\u6b21\u6570\u3002", "conclusion": "PhysicsAgentABM\u901a\u8fc7\u884c\u4e3a\u4e00\u81f4\u7684\u4ee3\u7406\u7c07\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u4e14\u6821\u51c6\u826f\u597d\u7684\u72b6\u6001\u8f6c\u79fb\u4eff\u771f\uff0c\u7ed3\u5408\u4e86\u7b26\u53f7\u4ee3\u7406\u3001\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u53ca\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u62df\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2602.04935", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04935", "abs": "https://arxiv.org/abs/2602.04935", "authors": ["Youjin Wang", "Run Zhou", "Rong Fu", "Shuaishuai Cao", "Hongwei Zeng", "Jiaxuan Lu", "Sicheng Fan", "Jiaqiao Zhao", "Liangming Pan"], "title": "ASA: Activation Steering for Tool-Calling Domain Adaptation", "comment": null, "summary": "For real-world deployment of general-purpose LLM agents, the core challenge is often not tool use itself, but efficient domain adaptation under rapidly evolving toolsets, APIs, and protocols. Repeated LoRA or SFT across domains incurs exponentially growing training and maintenance costs, while prompt or schema methods are brittle under distribution shift and complex interfaces. We propose \\textbf{Activation Steering Adapter (ASA}), a lightweight, inference-time, training-free mechanism that reads routing signals from intermediate activations and uses an ultra-light router to produce adaptive control strengths for precise domain alignment. Across multiple model scales and domains, ASA achieves LoRA-comparable adaptation with substantially lower overhead and strong cross-model transferability, making it ideally practical for robust, scalable, and efficient multi-domain tool ecosystems with frequent interface churn dynamics.", "AI": {"tldr": "ASA\u662f\u4e00\u79cd\u65e0\u8bad\u7ec3\u3001\u8f7b\u91cf\u7ea7\u7684\u9002\u5e94\u673a\u5236\uff0c\u901a\u8fc7\u89e3\u8bfb\u4e2d\u95f4\u6fc0\u6d3b\u5b9e\u73b0\u9ad8\u6548\u57df\u9002\u5e94\uff0c\u9002\u5408\u9891\u7e41\u53d8\u5316\u63a5\u53e3\u7684\u591a\u57df\u5de5\u5177\u73af\u5883\u3002", "motivation": "\u73b0\u5b9e\u73af\u5883\u4e2d\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u9762\u4e34\u5feb\u901f\u6f14\u53d8\u7684\u5de5\u5177\u96c6\u3001API\u548c\u534f\u8bae\uff0c\u5bfc\u81f4\u4f20\u7edf\u65b9\u6cd5\u5982\u91cd\u590dLoRA/SFT\u8bad\u7ec3\u6210\u672c\u6307\u6570\u589e\u957f\uff0c\u63d0\u793a\u6216\u6a21\u5f0f\u65b9\u6cd5\u5728\u5206\u5e03\u53d8\u5316\u548c\u590d\u6742\u63a5\u53e3\u4e0b\u8106\u5f31\u3002", "method": "\u63d0\u51fa\u4e86Activation Steering Adapter\uff08ASA\uff09\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u63a8\u7406\u65f6\u65e0\u8bad\u7ec3\u673a\u5236\uff0c\u901a\u8fc7\u8bfb\u53d6\u4e2d\u95f4\u6fc0\u6d3b\u7684\u8def\u7531\u4fe1\u53f7\uff0c\u5e76\u4f7f\u7528\u8d85\u8f7b\u91cf\u8def\u7531\u5668\u751f\u6210\u81ea\u9002\u5e94\u63a7\u5236\u5f3a\u5ea6\uff0c\u5b9e\u73b0\u7cbe\u786e\u7684\u57df\u5bf9\u9f50\u3002", "result": "ASA\u5728\u591a\u79cd\u6a21\u578b\u89c4\u6a21\u548c\u9886\u57df\u5b9e\u73b0\u4e86\u7c7b\u4f3cLoRA\u7684\u9002\u5e94\u6027\u80fd\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5f00\u9500\uff0c\u4e14\u5177\u5907\u5f3a\u8de8\u6a21\u578b\u8fc1\u79fb\u6027\u3002", "conclusion": "ASA\u65b9\u6cd5\u5728\u591a\u4e2a\u6a21\u578b\u89c4\u6a21\u548c\u9886\u57df\u4e2d\u5b9e\u73b0\u4e86\u4e0eLoRA\u76f8\u5f53\u7684\u9002\u5e94\u6027\uff0c\u5177\u6709\u66f4\u4f4e\u7684\u5f00\u9500\u548c\u826f\u597d\u7684\u8de8\u6a21\u578b\u8fc1\u79fb\u80fd\u529b\uff0c\u9002\u5408\u7528\u4e8e\u591a\u57df\u5de5\u5177\u751f\u6001\u7cfb\u7edf\u4e2d\u9891\u7e41\u63a5\u53e3\u53d8\u5316\u7684\u573a\u666f\u3002"}}
{"id": "2602.05107", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05107", "abs": "https://arxiv.org/abs/2602.05107", "authors": ["Ahmed Ruby", "Christian Hardmeier", "Sara Stymne"], "title": "Multilingual Extraction and Recognition of Implicit Discourse Relations in Speech and Text", "comment": null, "summary": "Implicit discourse relation classification is a challenging task, as it requires inferring meaning from context. While contextual cues can be distributed across modalities and vary across languages, they are not always captured by text alone. To address this, we introduce an automatic method for distantly related and unrelated language pairs to construct a multilingual and multimodal dataset for implicit discourse relations in English, French, and Spanish. For classification, we propose a multimodal approach that integrates textual and acoustic information through Qwen2-Audio, allowing joint modeling of text and audio for implicit discourse relation classification across languages. We find that while text-based models outperform audio-based models, integrating both modalities can enhance performance, and cross-lingual transfer can provide substantial improvements for low-resource languages.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u9690\u542b\u8bed\u7bc7\u5173\u7cfb\u5206\u7c7b\uff0c\u63d0\u51fa\u591a\u6a21\u6001\u591a\u8bed\u79cd\u6570\u636e\u96c6\u548c\u878d\u5408\u6587\u672c\u97f3\u9891\u4fe1\u606f\u7684\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u573a\u666f\u4e0b\u3002", "motivation": "\u9690\u542b\u8bed\u7bc7\u5173\u7cfb\u5206\u7c7b\u56f0\u96be\u5728\u4e8e\u9700\u8981\u4ece\u4e0a\u4e0b\u6587\u4e2d\u63a8\u65ad\u542b\u4e49\uff0c\u4e14\u4ec5\u9760\u6587\u672c\u65e0\u6cd5\u5b8c\u5168\u6355\u6349\u5206\u5e03\u4e8e\u591a\u6a21\u6001\u548c\u591a\u8bed\u8a00\u95f4\u7684\u4e0a\u4e0b\u6587\u7ebf\u7d22\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6587\u672c\u548c\u97f3\u9891\u4fe1\u606f\u7684\u591a\u6a21\u6001\u5206\u7c7b\u65b9\u6cd5\uff0c\u5229\u7528Qwen2-Audio\u6a21\u578b\u8fdb\u884c\u8054\u5408\u5efa\u6a21\uff0c\u540c\u65f6\u6784\u5efa\u4e86\u82f1\u6cd5\u897f\u4e09\u8bed\u7684\u591a\u6a21\u6001\u591a\u8bed\u79cd\u6570\u636e\u96c6\u3002", "result": "\u57fa\u4e8e\u6587\u672c\u7684\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u7eaf\u97f3\u9891\u6a21\u578b\uff0c\u4f46\u591a\u6a21\u6001\u878d\u5408\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u5206\u7c7b\u6548\u679c\uff0c\u8de8\u8bed\u8a00\u8fc1\u79fb\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u5e26\u6765\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u6587\u672c\u548c\u97f3\u9891\u7684\u591a\u6a21\u6001\u96c6\u6210\u80fd\u591f\u63d0\u5347\u9690\u542b\u8bed\u7bc7\u5173\u7cfb\u5206\u7c7b\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u8de8\u8bed\u8a00\u8fc1\u79fb\u4e2d\u8868\u73b0\u663e\u8457\u3002"}}
{"id": "2602.05289", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.05289", "abs": "https://arxiv.org/abs/2602.05289", "authors": ["Jingru Fan", "Dewen Liu", "Yufan Dang", "Huatao Li", "Yuheng Wang", "Wei Liu", "Feiyu Duan", "Xuanwen Ding", "Shu Yao", "Lin Wu", "Ruijie Shi", "Wai-Shing Leung", "Yuan Cheng", "Zhongyu Wei", "Cheng Yang", "Chen Qian", "Zhiyuan Liu", "Maosong Sun"], "title": "Towards a Science of Collective AI: LLM-based Multi-Agent Systems Need a Transition from Blind Trial-and-Error to Rigorous Science", "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have greatly extended the capabilities of Multi-Agent Systems (MAS), demonstrating significant effectiveness across a wide range of complex and open-ended domains. However, despite this rapid progress, the field still relies heavily on empirical trial-and-error. It lacks a unified and principled scientific framework necessary for systematic optimization and improvement. This bottleneck stems from the ambiguity of attribution: first, the absence of a structured taxonomy of factors leaves researchers restricted to unguided adjustments; second, the lack of a unified metric fails to distinguish genuine collaboration gain from mere resource accumulation. In this paper, we advocate for a transition to design science through an integrated framework. We advocate to establish the collaboration gain metric ($\u0393$) as the scientific standard to isolate intrinsic gains from increased budgets. Leveraging $\u0393$, we propose a factor attribution paradigm to systematically identify collaboration-driving factors. To support this, we construct a systematic MAS factor library, structuring the design space into control-level presets and information-level dynamics. Ultimately, this framework facilitates the transition from blind experimentation to rigorous science, paving the way towards a true science of Collective AI.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7f3a\u4e4f\u7edf\u4e00\u79d1\u5b66\u6846\u67b6\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u5408\u4f5c\u589e\u76ca\u5ea6\u91cf\u548c\u7cfb\u7edf\u5316\u56e0\u7d20\u5f52\u56e0\u65b9\u6cd5\uff0c\u6784\u5efa\u56e0\u7d20\u5e93\uff0c\u63a8\u52a8\u9886\u57df\u4ece\u7ecf\u9a8c\u8bd5\u9519\u5411\u4e25\u8c28\u7684\u8bbe\u8ba1\u79d1\u5b66\u8f6c\u53d8\u3002", "motivation": "\u5f53\u524d\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u9886\u57df\u4f9d\u8d56\u5927\u91cf\u7684\u7ecf\u9a8c\u8bd5\u9519\uff0c\u7f3a\u4e4f\u7edf\u4e00\u4e14\u6709\u539f\u5219\u7684\u79d1\u5b66\u6846\u67b6\u6765\u7cfb\u7edf\u6027\u5730\u4f18\u5316\u548c\u63d0\u5347\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u56e0\u7d20\u7684\u7ed3\u6784\u5316\u5206\u7c7b\u548c\u7edf\u4e00\u7684\u5ea6\u91cf\u6807\u51c6\uff0c\u5bfc\u81f4\u65e0\u6cd5\u660e\u786e\u5408\u4f5c\u5e26\u6765\u7684\u771f\u5b9e\u6536\u76ca\u3002", "method": "\u672c\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u6027\u7684MAS\u56e0\u7d20\u5e93\uff0c\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u5408\u4f5c\u589e\u76ca\u0393\u7684\u56e0\u7d20\u5f52\u56e0\u8303\u5f0f\uff0c\u5c06MAS\u8bbe\u8ba1\u7a7a\u95f4\u7ed3\u6784\u5316\u4e3a\u63a7\u5236\u5c42\u9884\u8bbe\u548c\u4fe1\u606f\u5c42\u52a8\u6001\u4e24\u90e8\u5206\uff0c\u501f\u6b64\u89c4\u8303\u5316\u548c\u7cfb\u7edf\u5316\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u8bbe\u8ba1\u4e0e\u8bc4\u4f30\u8fc7\u7a0b\u3002", "result": "\u672c\u6587\u63d0\u51fa\u7684\u5408\u4f5c\u589e\u76ca\u5ea6\u91cf\u0393\u4f5c\u4e3a\u79d1\u5b66\u6807\u51c6\uff0c\u7ed3\u5408\u56e0\u7d20\u5f52\u56e0\u8303\u5f0f\u548cMAS\u56e0\u7d20\u5e93\uff0c\u5b9e\u73b0\u4e86\u5bf9\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8bbe\u8ba1\u7684\u7cfb\u7edf\u5316\u548c\u79d1\u5b66\u5316\uff0c\u6709\u671b\u63a8\u52a8\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4ece\u7ecf\u9a8c\u9a71\u52a8\u8f6c\u5411\u8bbe\u8ba1\u79d1\u5b66\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u8bbe\u8ba1\u6846\u67b6\uff0c\u5229\u7528\u5408\u4f5c\u589e\u76ca\u5ea6\u91cf\uff08\u0393\uff09\u6765\u79d1\u5b66\u533a\u5206\u7cfb\u7edf\u6027\u80fd\u63d0\u5347\u7684\u5185\u5728\u539f\u56e0\u4e0e\u8d44\u6e90\u6269\u5145\u7684\u5f71\u54cd\uff0c\u4ece\u800c\u5b9e\u73b0\u7cfb\u7edf\u4f18\u5316\u7684\u79d1\u5b66\u5316\u3002"}}
{"id": "2602.04938", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.04938", "abs": "https://arxiv.org/abs/2602.04938", "authors": ["Lukas Radosky", "Ivan Polasek"], "title": "Large Language Models in Software Documentation and Modeling: A Literature Review and Findings", "comment": "This is a preprint of a paper that was presented at the IEEE 24th World Symposium on Applied Machine Intelligence and Informatics (SAMI 2026)", "summary": "Generative artificial intelligence attracts significant attention, especially with the introduction of large language models. Its capabilities are being exploited to solve various software engineering tasks. Thanks to their ability to understand natural language and generate natural language responses, large language models are great for processing various software documentation artifacts. At the same time, large language models excel at understanding structured languages, having the potential for working with software programs and models. We conduct a literature review on the usage of large language models for software engineering tasks related to documentation and modeling. We analyze articles from four major venues in the area, organize them per tasks they solve, and provide an overview of used prompt techniques, metrics, approaches to human-based evaluation, and major datasets.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u6587\u6863\u4e0e\u5efa\u6a21\u4efb\u52a1\u7684\u5e94\u7528\uff0c\u5206\u6790\u4e86\u76f8\u5173\u6280\u672f\u4e0e\u8bc4\u4f30\u65b9\u6cd5\uff0c\u603b\u7ed3\u73b0\u72b6\u4e0e\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u4e2d\u8868\u73b0\u7a81\u51fa\uff0c\u5177\u5907\u7406\u89e3\u548c\u751f\u6210\u81ea\u7136\u8bed\u8a00\u7684\u80fd\u529b\uff0c\u9002\u5408\u5904\u7406\u8f6f\u4ef6\u6587\u6863\u548c\u7ed3\u6784\u5316\u8bed\u8a00\uff0c\u63a8\u52a8\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u7684\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u5bf9\u56db\u4e2a\u4e3b\u8981\u4f1a\u8bae\u8bba\u6587\u7684\u6587\u732e\u7efc\u8ff0\uff0c\u5206\u6790\u5176\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u4e0e\u6587\u6863\u4e0e\u5efa\u6a21\u76f8\u5173\u4efb\u52a1\u7684\u5e94\u7528\uff0c\u6574\u7406\u4efb\u52a1\u5206\u7c7b\uff0c\u63a2\u8ba8\u63d0\u793a\u6280\u672f\u3001\u8bc4\u4f30\u65b9\u6cd5\u53ca\u6570\u636e\u96c6\u3002", "result": "\u7cfb\u7edf\u68b3\u7406\u4e86\u5df2\u6709\u6587\u732e\u4e2d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u6587\u6863\u548c\u5efa\u6a21\u4efb\u52a1\u7684\u5e94\u7528\u73b0\u72b6\uff0c\u660e\u786e\u4e86\u5e38\u7528\u7684\u65b9\u6cd5\u3001\u8bc4\u4f30\u624b\u6bb5\u548c\u6570\u636e\u8d44\u6e90\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u53c2\u8003\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u6587\u6863\u548c\u5efa\u6a21\u9886\u57df\u5c55\u73b0\u51fa\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u5df2\u6709\u7814\u7a76\u4e3a\u4f18\u5316\u76f8\u5173\u4efb\u52a1\u63d0\u4f9b\u4e86\u591a\u6837\u7684\u6280\u672f\u548c\u8bc4\u4f30\u65b9\u6848\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u6df1\u5316\u3002"}}
{"id": "2602.05150", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05150", "abs": "https://arxiv.org/abs/2602.05150", "authors": ["Yang Zhang", "Mersin Konomi", "Christos Xypolopoulos", "Konstantinos Divriotis", "Konstantinos Skianis", "Giannis Nikolentzos", "Giorgos Stamou", "Guokan Shang", "Michalis Vazirgiannis"], "title": "GreekMMLU: A Native-Sourced Multitask Benchmark for Evaluating Language Models in Greek", "comment": null, "summary": "Large Language Models (LLMs) are commonly trained on multilingual corpora that include Greek, yet reliable evaluation benchmarks for Greek-particularly those based on authentic, native-sourced content-remain limited. Existing datasets are often machine-translated from English, failing to capture Greek linguistic and cultural characteristics. We introduce GreekMMLU, a native-sourced benchmark for massive multitask language understanding in Greek, comprising 21,805 multiple-choice questions across 45 subject areas, organized under a newly defined subject taxonomy and annotated with educational difficulty levels spanning primary to professional examinations. All questions are sourced or authored in Greek from academic, professional, and governmental exams. We publicly release 16,857 samples and reserve 4,948 samples for a private leaderboard to enable robust and contamination-resistant evaluation. Evaluations of over 80 open- and closed-source LLMs reveal substantial performance gaps between frontier and open-weight models, as well as between Greek-adapted models and general multilingual ones. Finally, we provide a systematic analysis of factors influencing performance-including model scale, adaptation, and prompting-and derive insights for improving LLM capabilities in Greek.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u57fa\u4e8e\u5e0c\u814a\u8bed\u539f\u751f\u8bd5\u9898\u7684GreekMMLU\u57fa\u51c6\uff0c\u7cfb\u7edf\u8bc4\u6d4b\u591a\u6b3e\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5e0c\u814a\u8bed\u7406\u89e3\u4e0a\u7684\u8868\u73b0\uff0c\u4fc3\u8fdb\u4e86\u5e0c\u814a\u8bed\u5927\u6a21\u578b\u80fd\u529b\u7684\u63d0\u5347\u548c\u7814\u7a76\u3002", "motivation": "\u73b0\u6709\u5e0c\u814a\u8bed\u8bc4\u6d4b\u6570\u636e\u591a\u4e3a\u82f1\u6587\u673a\u7ffb\uff0c\u7f3a\u4e4f\u771f\u5b9e\u4e14\u591a\u6837\u5316\u7684\u5e0c\u814a\u8bed\u539f\u751f\u8bed\u6599\uff0c\u9650\u5236\u4e86\u5bf9\u5e0c\u814a\u8bed\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u7684\u51c6\u786e\u8bc4\u4f30\u548c\u53d1\u5c55\u3002", "method": "\u6784\u5efa\u5305\u542b21,805\u9053\u591a\u9009\u9898\u7684GreekMMLU\u6570\u636e\u96c6\uff0c\u6db5\u76d645\u4e2a\u5b66\u79d1\u9886\u57df\uff0c\u9898\u76ee\u5747\u4e3a\u5e0c\u814a\u8bed\u539f\u751f\u5185\u5bb9\uff0c\u5206\u96be\u5ea6\u7b49\u7ea7\uff0c\u5e76\u5bf980\u591a\u4e2a\u5f00\u6e90\u53ca\u95ed\u6e90\u5927\u6a21\u578b\u8fdb\u884c\u8bc4\u6d4b\u5206\u6790\u6a21\u578b\u8868\u73b0\u5dee\u5f02\u3002", "result": "\u516c\u5f00\u4e8616,857\u6761\u6837\u672c\uff0c\u8bbe\u7f6e4,948\u6761\u79c1\u6709\u6837\u672c\u7528\u4e8e\u6d4b\u8bd5\uff0c\u4ee5\u4fdd\u969c\u8bc4\u6d4b\u7684\u53ef\u9760\u6027\uff1b\u8bc4\u6d4b\u7ed3\u679c\u663e\u793a\u524d\u6cbf\u6a21\u578b\u660e\u663e\u4f18\u4e8e\u5f00\u6e90\u6a21\u578b\uff0c\u7ecf\u8fc7\u5e0c\u814a\u8bed\u9002\u5e94\u7684\u6a21\u578b\u4f18\u4e8e\u666e\u901a\u591a\u8bed\u79cd\u6a21\u578b\uff1b\u7cfb\u7edf\u5206\u6790\u4e86\u6a21\u578b\u89c4\u6a21\u3001\u9002\u5e94\u6027\u548c\u63d0\u793a\u6280\u672f\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "conclusion": "\u672c\u8bba\u6587\u5c55\u793a\u4e86GreekMMLU\u4f5c\u4e3a\u5e0c\u814a\u8bed\u539f\u751f\u6570\u636e\u7684\u591a\u4efb\u52a1\u8bed\u8a00\u7406\u89e3\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u5927\u6a21\u578b\u5728\u5e0c\u814a\u8bed\u7406\u89e3\u4e0a\u7684\u663e\u8457\u5dee\u8ddd\u548c\u63d0\u5347\u7a7a\u95f4\u3002"}}
{"id": "2602.05493", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.05493", "abs": "https://arxiv.org/abs/2602.05493", "authors": ["Bingru Li"], "title": "LinguistAgent: A Reflective Multi-Model Platform for Automated Linguistic Annotation", "comment": null, "summary": "Data annotation remains a significant bottleneck in the Humanities and Social Sciences, particularly for complex semantic tasks such as metaphor identification. While Large Language Models (LLMs) show promise, a significant gap remains between the theoretical capability of LLMs and their practical utility for researchers. This paper introduces LinguistAgent, an integrated, user-friendly platform that leverages a reflective multi-model architecture to automate linguistic annotation. The system implements a dual-agent workflow, comprising an Annotator and a Reviewer, to simulate a professional peer-review process. LinguistAgent supports comparative experiments across three paradigms: Prompt Engineering (Zero/Few-shot), Retrieval-Augmented Generation, and Fine-tuning. We demonstrate LinguistAgent's efficacy using the task of metaphor identification as an example, providing real-time token-level evaluation (Precision, Recall, and $F_1$ score) against human gold standards. The application and codes are released on https://github.com/Bingru-Li/LinguistAgent.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLinguistAgent\u5e73\u53f0\uff0c\u5229\u7528\u591a\u6a21\u578b\u53cc\u4ee3\u7406\u67b6\u6784\u81ea\u52a8\u5316\u8bed\u8a00\u5b66\u6807\u6ce8\uff0c\u4ee5\u9690\u55bb\u8bc6\u522b\u4efb\u52a1\u9a8c\u8bc1\u5176\u5728\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u53caF1\u503c\u4e0a\u7684\u9ad8\u6548\u8868\u73b0\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6570\u636e\u6807\u6ce8\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u6570\u636e\u6807\u6ce8\u5728\u793e\u4f1a\u79d1\u5b66\u53ca\u4eba\u6587\u5b66\u79d1\uff0c\u5c24\u5176\u662f\u590d\u6742\u8bed\u4e49\u4efb\u52a1\u5982\u9690\u55bb\u8bc6\u522b\u4e2d\u662f\u4e00\u5927\u74f6\u9888\u3002\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7406\u8bba\u4e0a\u80fd\u529b\u5f3a\uff0c\u4f46\u5b9e\u9645\u4e2d\u5bf9\u7814\u7a76\u8005\u7684\u5e2e\u52a9\u4ecd\u6709\u9650\u3002", "method": "\u63d0\u51faLinguistAgent\uff0c\u4e00\u4e2a\u96c6\u6210\u4e14\u7528\u6237\u53cb\u597d\u7684\u5e73\u53f0\uff0c\u91c7\u7528\u53cd\u601d\u5f0f\u591a\u6a21\u578b\u67b6\u6784\uff0c\u5b9e\u73b0\u8bed\u8a00\u5b66\u81ea\u52a8\u6807\u6ce8\u3002\u7cfb\u7edf\u5305\u542b\u6ce8\u91ca\u8005\u4e0e\u5ba1\u6838\u8005\u53cc\u4ee3\u7406\u5de5\u4f5c\u6d41\u7a0b\uff0c\u6a21\u62df\u4e13\u4e1a\u540c\u884c\u8bc4\u5ba1\u8fc7\u7a0b\uff0c\u5e76\u652f\u6301\u96f6/\u5c11\u6837\u4f8b\u63d0\u793a\u5de5\u7a0b\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4ee5\u53ca\u5fae\u8c03\u4e09\u79cd\u8303\u5f0f\u7684\u5bf9\u6bd4\u8bd5\u9a8c\u3002", "result": "\u901a\u8fc7\u9690\u55bb\u8bc6\u522b\u4efb\u52a1\u9a8c\u8bc1LinguistAgent\u7684\u6709\u6548\u6027\uff0c\u5b9e\u65f6\u8f93\u51fa\u57fa\u4e8e\u4eba\u5de5\u9ec4\u91d1\u6807\u51c6\u7684\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u3002\u5e73\u53f0\u548c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "conclusion": "LinguistAgent\u6709\u6548\u63d0\u5347\u4e86\u590d\u6742\u8bed\u8a00\u5b66\u6807\u6ce8\u4efb\u52a1\u7684\u81ea\u52a8\u5316\u6c34\u5e73\uff0c\u5f25\u5408\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7406\u8bba\u80fd\u529b\u4e0e\u5b9e\u8df5\u5e94\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2602.05042", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.05042", "abs": "https://arxiv.org/abs/2602.05042", "authors": ["Lucas Romao", "Luiz Xavier", "J\u00falia Cond\u00e9 Ara\u00fajo", "Marina Cond\u00e9 Ara\u00fajo", "Ariane Rodrigues", "Marcos Kalinowski"], "title": "Applying a Requirements-Focused Agile Management Approach for Machine Learning-Enabled Systems", "comment": "Accepted for publication at the 5th International Conference on AI Engineering - Software Engineering for AI (CAIN) 2026", "summary": "Machine Learning (ML)-enabled systems challenge traditional Requirements Engineering (RE) and agile management due to data dependence, experimentation, and uncertain model behavior. Existing RE and agile practices remain poorly integrated and insufficiently tailored to these characteristics. This paper reports on the practical experience of applying RefineML, a requirements-focused approach for the continuous and agile refinement of ML-enabled systems, which integrates ML-tailored specification and agile management approaches with best practices derived from a systematic mapping study. The application context concerns an industry-academia collaboration project between PUC-Rio and EXA, a Brazilian cybersecurity company. For evaluation purposes, we applied questionnaires assessing RefineML's suitability and overall acceptance and semi-structured interviews. We applied thematic analysis to the collected qualitative data. Regarding suitability and acceptance, the results of the questionnaires indicated high perceived usefulness and intention to use. Based on the interviews, stakeholders perceived RefineML as improving communication and facilitating early feasibility assessments, as well as enabling dual-track governance of ML and software work, allowing continuous refinement of the model while evolving the overall software project. However, some limitations remain, particularly related to difficulties in operationalizing ML concerns into agile requirements and in estimating ML effort.", "AI": {"tldr": "RefineML\u65b9\u6cd5\u4e13\u4e3a\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u9700\u6c42\u8bbe\u8ba1\uff0c\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\u5176\u63d0\u5347\u4e86\u534f\u4f5c\u548c\u9879\u76ee\u7ba1\u7406\u6548\u80fd\uff0c\u4f46\u5728\u9700\u6c42\u7ec6\u5316\u4e0e\u5de5\u4f5c\u91cf\u4f30\u7b97\u65b9\u9762\u4ecd\u9700\u4f18\u5316\u3002", "motivation": "\u4f20\u7edf\u7684\u9700\u6c42\u5de5\u7a0b\u548c\u654f\u6377\u7ba1\u7406\u96be\u4ee5\u6ee1\u8db3\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u4f9d\u8d56\u6570\u636e\u3001\u9700\u53cd\u590d\u5b9e\u9a8c\u53ca\u6a21\u578b\u884c\u4e3a\u4e0d\u786e\u5b9a\u6027\u7684\u7279\u70b9\uff0c\u73b0\u6709\u65b9\u6cd5\u6574\u5408\u4e0d\u591f\u4e14\u9002\u914d\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u5f00\u53d1\u4e13\u95e8\u65b9\u6cd5\u6539\u8fdb\u3002", "method": "\u901a\u8fc7\u5728PUC-Rio\u4e0e\u5df4\u897f\u7f51\u7edc\u5b89\u5168\u516c\u53f8EXA\u7684\u4ea7\u5b66\u7814\u5408\u4f5c\u9879\u76ee\u4e2d\u5e94\u7528RefineML\uff0c\u7ed3\u5408\u95ee\u5377\u8c03\u67e5\u548c\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u6536\u96c6\u6570\u636e\uff0c\u5e76\u91c7\u7528\u4e3b\u9898\u5206\u6790\u5bf9\u5b9a\u6027\u8d44\u6599\u8fdb\u884c\u5206\u6790\u3002", "result": "\u95ee\u5377\u8c03\u67e5\u663e\u793aRefineML\u5177\u6709\u9ad8\u9002\u7528\u6027\u548c\u88ab\u63a5\u53d7\u5ea6\uff0c\u8bbf\u8c08\u53cd\u9988\u5176\u4fc3\u8fdb\u4e86\u6c9f\u901a\u3001\u65e9\u671f\u53ef\u884c\u6027\u8bc4\u4f30\u548c\u53cc\u8f68\u6cbb\u7406\u673a\u5668\u5b66\u4e60\u4e0e\u8f6f\u4ef6\u5f00\u53d1\u6d41\u7a0b\uff0c\u4f46\u4ecd\u5b58\u5728\u96be\u4ee5\u64cd\u4f5c\u5316\u673a\u5668\u5b66\u4e60\u9700\u6c42\u548c\u4f30\u7b97\u5de5\u4f5c\u91cf\u7684\u5c40\u9650\u3002", "conclusion": "RefineML\u662f\u4e00\u79cd\u9488\u5bf9\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u7279\u70b9\u8bbe\u8ba1\u7684\u9700\u6c42\u5de5\u7a0b\u548c\u654f\u6377\u7ba1\u7406\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u6c9f\u901a\u3001\u65e9\u671f\u53ef\u884c\u6027\u8bc4\u4f30\u53ca\u53cc\u8f68\u6cbb\u7406\uff0c\u4f46\u5728\u5c06\u673a\u5668\u5b66\u4e60\u95ee\u9898\u5177\u4f53\u5316\u4e3a\u654f\u6377\u9700\u6c42\u53ca\u4f30\u7b97\u5de5\u4f5c\u91cf\u65b9\u9762\u4ecd\u5b58\u5728\u56f0\u96be\u3002"}}
{"id": "2602.05176", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05176", "abs": "https://arxiv.org/abs/2602.05176", "authors": ["Ziyuan Yang", "Wenxuan Ding", "Shangbin Feng", "Yulia Tsvetkov"], "title": "Among Us: Measuring and Mitigating Malicious Contributions in Model Collaboration Systems", "comment": "19 pages, 15 tables, 4 figures", "summary": "Language models (LMs) are increasingly used in collaboration: multiple LMs trained by different parties collaborate through routing systems, multi-agent debate, model merging, and more. Critical safety risks remain in this decentralized paradigm: what if some of the models in multi-LLM systems are compromised or malicious? We first quantify the impact of malicious models by engineering four categories of malicious LMs, plug them into four types of popular model collaboration systems, and evaluate the compromised system across 10 datasets. We find that malicious models have a severe impact on the multi-LLM systems, especially for reasoning and safety domains where performance is lowered by 7.12% and 7.94% on average. We then propose mitigation strategies to alleviate the impact of malicious components, by employing external supervisors that oversee model collaboration to disable/mask them out to reduce their influence. On average, these strategies recover 95.31% of the initial performance, while making model collaboration systems fully resistant to malicious models remains an open research question.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\u591a\u8bed\u8a00\u6a21\u578b\u534f\u4f5c\u4e2d\u6076\u610f\u6a21\u578b\u4f1a\u4e25\u91cd\u5f71\u54cd\u7cfb\u7edf\u8868\u73b0\uff0c\u63d0\u51fa\u7684\u5916\u90e8\u76d1\u7763\u7b56\u7565\u80fd\u6709\u6548\u7f13\u89e3\u8be5\u95ee\u9898\uff0c\u4f46\u5b8c\u5168\u9632\u5fa1\u4ecd\u5f85\u7814\u7a76\u3002", "motivation": "\u968f\u7740\u591a\u8bed\u8a00\u6a21\u578b\u534f\u4f5c\u7cfb\u7edf\u7684\u666e\u53ca\uff0c\u5b58\u5728\u90e8\u5206\u6a21\u578b\u88ab\u6076\u610f\u7be1\u6539\u6216\u635f\u574f\u7684\u5b89\u5168\u98ce\u9669\uff0c\u8feb\u5207\u9700\u8981\u8bc4\u4f30\u8fd9\u79cd\u5a01\u80c1\u7684\u5f71\u54cd\u5e76\u63d0\u51fa\u7f13\u89e3\u65b9\u6848\u3002", "method": "\u6784\u9020\u4e86\u56db\u7c7b\u6076\u610f\u8bed\u8a00\u6a21\u578b\uff0c\u90e8\u7f72\u5728\u56db\u79cd\u6d41\u884c\u7684\u6a21\u578b\u534f\u4f5c\u7cfb\u7edf\u4e2d\uff0c\u4f7f\u752810\u4e2a\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u8bbe\u8ba1\u4e86\u5916\u90e8\u76d1\u7763\u7684\u7f13\u89e3\u7b56\u7565\u6765\u964d\u4f4e\u6076\u610f\u6a21\u578b\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u6076\u610f\u6a21\u578b\u5bfc\u81f4\u7cfb\u7edf\u6027\u80fd\u4e0b\u964d7.12%\u81f37.94%\uff0c\u901a\u8fc7\u5916\u90e8\u76d1\u7763\u7b56\u7565\u5e73\u5747\u6062\u590d\u4e8695.31%\u7684\u6027\u80fd\u3002", "conclusion": "\u6076\u610f\u6a21\u578b\u5bf9\u591a\u8bed\u8a00\u6a21\u578b\uff08multi-LLM\uff09\u534f\u4f5c\u7cfb\u7edf\u6709\u663e\u8457\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u5c24\u5176\u5728\u63a8\u7406\u548c\u5b89\u5168\u9886\u57df\u8868\u73b0\u4e0b\u964d\u660e\u663e\u3002\u5c3d\u7ba1\u63d0\u51fa\u7684\u5916\u90e8\u76d1\u7763\u7b56\u7565\u80fd\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u7f13\u89e3\u6076\u610f\u7ec4\u4ef6\u7684\u5f71\u54cd\uff0c\u5e76\u6062\u590d\u5927\u90e8\u5206\u6027\u80fd\uff0c\u4f46\u5982\u4f55\u5b8c\u5168\u62b5\u5fa1\u6076\u610f\u6a21\u578b\u4ecd\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u7684\u95ee\u9898\u3002"}}
{"id": "2602.05043", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05043", "abs": "https://arxiv.org/abs/2602.05043", "authors": ["Grace A. Lewis", "Rachel Brower-Sinning", "Robert Edman", "Ipek Ozkaya", "Sebasti\u00e1n Echeverr\u00eda", "Alex Derr", "Collin Beaudoin", "Katherine R. Maffey"], "title": "Quality Model for Machine Learning Components", "comment": "A short version of this paper has been accepted to CAIN 2026, the 5th IEEE/ACM Conference on AI Engineering - Software Engineering for AI Systems", "summary": "Despite increased adoption and advances in machine learning (ML), there are studies showing that many ML prototypes do not reach the production stage and that testing is still largely limited to testing model properties, such as model performance, without considering requirements derived from the system it will be a part of, such as throughput, resource consumption, or robustness. This limited view of testing leads to failures in model integration, deployment, and operations. In traditional software development, quality models such as ISO 25010 provide a widely used structured framework to assess software quality, define quality requirements, and provide a common language for communication with stakeholders. A newer standard, ISO 25059, defines a more specific quality model for AI systems. However, a problem with this standard is that it combines system attributes with ML component attributes, which is not helpful for a model developer, as many system attributes cannot be assessed at the component level. In this paper, we present a quality model for ML components that serves as a guide for requirements elicitation and negotiation and provides a common vocabulary for ML component developers and system stakeholders to agree on and define system-derived requirements and focus their testing efforts accordingly. The quality model was validated through a survey in which the participants agreed with its relevance and value. The quality model has been successfully integrated into an open-source tool for ML component testing and evaluation demonstrating its practical application.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u673a\u5668\u5b66\u4e60\u7ec4\u4ef6\u63d0\u51fa\u8d28\u91cf\u6a21\u578b\uff0c\u4fc3\u8fdb\u7cfb\u7edf\u9700\u6c42\u660e\u786e\u548c\u6d4b\u8bd5\u4f18\u5316\uff0c\u9a8c\u8bc1\u6709\u6548\u6027\u5e76\u5e94\u7528\u4e8e\u5f00\u6e90\u5de5\u5177\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6807\u51c6\u4e0d\u9002\u7528\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6d4b\u8bd5\u591a\u4fa7\u91cd\u6a21\u578b\u6027\u80fd\uff0c\u5ffd\u89c6\u7cfb\u7edf\u5c42\u9762\u9700\u6c42\uff0c\u5bfc\u81f4\u6574\u5408\u4e0e\u90e8\u7f72\u5931\u8d25\uff0c\u73b0\u6709\u6807\u51c6\u5982ISO 25059\u672a\u80fd\u6709\u6548\u533a\u5206\u7cfb\u7edf\u5c5e\u6027\u548c\u7ec4\u4ef6\u5c5e\u6027\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u8d28\u91cf\u6a21\u578b\uff0c\u7ed3\u5408\u8c03\u67e5\u53cd\u9988\u9a8c\u8bc1\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u5f00\u6e90\u673a\u5668\u5b66\u4e60\u7ec4\u4ef6\u6d4b\u8bd5\u5de5\u5177\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u9002\u7528\u4e8e\u673a\u5668\u5b66\u4e60\u7ec4\u4ef6\u7684\u8d28\u91cf\u6a21\u578b\uff0c\u83b7\u5f97\u4e1a\u754c\u8ba4\u53ef\uff0c\u5e76\u5b9e\u9645\u5e94\u7528\u4e8e\u5f00\u6e90\u6d4b\u8bd5\u5de5\u5177\uff0c\u63d0\u5347\u6d4b\u8bd5\u9488\u5bf9\u6027\u548c\u7cfb\u7edf\u96c6\u6210\u6210\u529f\u7387\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u673a\u5668\u5b66\u4e60\u7ec4\u4ef6\u7684\u8d28\u91cf\u6a21\u578b\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u548c\u5229\u76ca\u76f8\u5173\u8005\u660e\u786e\u5e76\u534f\u5546\u7cfb\u7edf\u6d3e\u751f\u9700\u6c42\uff0c\u4f18\u5316\u6d4b\u8bd5\u7b56\u7565\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u8c03\u67e5\u9a8c\u8bc1\u4e86\u5176\u76f8\u5173\u6027\u548c\u4ef7\u503c\uff0c\u5e76\u6210\u529f\u96c6\u6210\u5230\u5f00\u6e90\u6d4b\u8bd5\u5de5\u5177\u4e2d\u3002"}}
{"id": "2602.05182", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05182", "abs": "https://arxiv.org/abs/2602.05182", "authors": ["Shangbin Feng", "Kishan Panaganti", "Yulia Tsvetkov", "Wenhao Yu"], "title": "The Single-Multi Evolution Loop for Self-Improving Model Collaboration Systems", "comment": "Code at https://github.com/BunsenFeng/moco_distill", "summary": "Model collaboration -- systems where multiple language models (LMs) collaborate -- combines the strengths of diverse models with cost in loading multiple LMs. We improve efficiency while preserving the strengths of collaboration by distilling collaborative patterns into a single model, where the model is trained on the outputs of the model collaboration system. At inference time, only the distilled model is employed: it imitates the collaboration while only incurring the cost of a single model. Furthermore, we propose the single-multi evolution loop: multiple LMs collaborate, each distills from the collaborative outputs, and these post-distillation improved LMs collaborate again, forming a collective evolution ecosystem where models evolve and self-improve by interacting with an environment of other models. Extensive experiments with 7 collaboration strategies and 15 tasks (QA, reasoning, factuality, etc.) demonstrate that: 1) individual models improve by 8.0% on average, absorbing the strengths of collaboration while reducing the cost to a single model; 2) the collaboration also benefits from the stronger and more synergistic LMs after distillation, improving over initial systems without evolution by 14.9% on average. Analysis reveals that the single-multi evolution loop outperforms various existing evolutionary AI methods, is compatible with diverse model/collaboration/distillation settings, and helps solve problems where the initial model/system struggles to.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u901a\u8fc7\u84b8\u998f\u5c06\u591a\u6a21\u578b\u534f\u4f5c\u4f18\u52bf\u6574\u5408\u5165\u5355\u4e00\u6a21\u578b\uff0c\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\uff1b\u5e76\u8bbe\u8ba1\u5355\u591a\u8fdb\u5316\u5faa\u73af\u673a\u5236\uff0c\u63a8\u52a8\u6a21\u578b\u534f\u4f5c\u4e0e\u81ea\u6211\u63d0\u5347\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u6027\u80fd\u663e\u8457\u63d0\u5347\u4e14\u9002\u7528\u5e7f\u6cdb\u3002", "motivation": "\u591a\u6a21\u578b\u534f\u4f5c\u867d\u80fd\u7ed3\u5408\u4e0d\u540c\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u4f46\u4ee3\u4ef7\u662f\u52a0\u8f7d\u591a\u6a21\u578b\u6210\u672c\u9ad8\uff0c\u9700\u63d0\u9ad8\u6548\u7387\u540c\u65f6\u4fdd\u6301\u534f\u4f5c\u4f18\u52bf\u3002", "method": "\u8bad\u7ec3\u5355\u4e00\u6a21\u578b\u6a21\u4eff\u591a\u6a21\u578b\u534f\u4f5c\u7cfb\u7edf\u7684\u8f93\u51fa\uff0c\u901a\u8fc7\u84b8\u998f\u6280\u672f\u6574\u5408\u591a\u6a21\u578b\u7684\u77e5\u8bc6\uff1b\u5f15\u5165\u5355\u591a\u8fdb\u5316\u5faa\u73af\uff0c\u8ba9\u591a\u6a21\u578b\u534f\u4f5c\u540e\u5404\u81ea\u84b8\u998f\u5e76\u5f3a\u5316\uff0c\u518d\u6b21\u534f\u4f5c\u5faa\u73af\u8fdb\u5316\u3002", "result": "\u57287\u79cd\u534f\u4f5c\u7b56\u7565\u548c15\u9879\u4efb\u52a1\u4e0a\uff0c\u5355\u6a21\u578b\u901a\u8fc7\u84b8\u998f\u5e73\u5747\u63d0\u53478.0%\uff0c\u534f\u4f5c\u7cfb\u7edf\u901a\u8fc7\u8fdb\u5316\u5faa\u73af\u5e73\u5747\u63d0\u534714.9%\uff1b\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u8fdb\u5316AI\u65b9\u6cd5\uff0c\u517c\u5bb9\u591a\u79cd\u8bbe\u7f6e\u5e76\u80fd\u89e3\u51b3\u521d\u59cb\u6a21\u578b\u96be\u9898\u3002", "conclusion": "\u901a\u8fc7\u6a21\u578b\u84b8\u998f\u5c06\u591a\u6a21\u578b\u534f\u4f5c\u7684\u4f18\u52bf\u6574\u5408\u5230\u5355\u4e00\u6a21\u578b\u4e2d\uff0c\u5728\u63a8\u7406\u65f6\u53ea\u9700\u4f7f\u7528\u5355\u4e00\u6a21\u578b\u5373\u53ef\u663e\u8457\u63d0\u5347\u6548\u7387\u5e76\u4fdd\u6301\u534f\u4f5c\u4f18\u52bf\u3002\u63d0\u51fa\u7684\u5355\u591a\u8fdb\u5316\u5faa\u73af\u673a\u5236\u4f7f\u6a21\u578b\u901a\u8fc7\u534f\u4f5c\u548c\u84b8\u998f\u4e0d\u65ad\u81ea\u6211\u63d0\u5347\uff0c\u5b9e\u73b0\u534f\u540c\u8fdb\u5316\u3002"}}
{"id": "2602.05122", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.05122", "abs": "https://arxiv.org/abs/2602.05122", "authors": ["Altino Alves", "Andre Hora"], "title": "TestMigrationsInPy: A Dataset of Test Migrations from Unittest to Pytest", "comment": "Published at MSR 2025", "summary": "Unittest and pytest are the most popular testing frameworks in Python. Overall, pytest provides some advantages, including simpler assertion, reuse of fixtures, and interoperability. Due to such benefits, multiple projects in the Python ecosystem have migrated from unittest to pytest. To facilitate the migration, pytest can also run unittest tests, thus, the migration can happen gradually over time. However, the migration can be time-consuming and take a long time to conclude. In this context, projects would benefit from automated solutions to support the migration process. In this paper, we propose TestMigrationsInPy, a dataset of test migrations from unittest to pytest. TestMigrationsInPy contains 923 real-world migrations performed by developers. Future research proposing novel solutions to migrate frameworks in Python can rely on TestMigrationsInPy as a ground truth. Moreover, as TestMigrationsInPy includes information about the migration type (e.g., changes in assertions or fixtures), our dataset enables novel solutions to be verified effectively, for instance, from simpler assertion migrations to more complex fixture migrations. TestMigrationsInPy is publicly available at: https://github.com/altinoalvesjunior/TestMigrationsInPy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b923\u4e2a\u771f\u5b9e\u6d4b\u8bd5\u8fc1\u79fb\u6848\u4f8b\u7684\u6570\u636e\u96c6\uff0c\u652f\u6301Python\u6d4b\u8bd5\u6846\u67b6unittest\u5230pytest\u7684\u8fc1\u79fb\u7814\u7a76\u4e0e\u81ea\u52a8\u5316\u5de5\u5177\u5f00\u53d1\u3002", "motivation": "Python\u9879\u76ee\u4eceunittest\u8fc1\u79fb\u5230pytest\u7684\u8fc7\u7a0b\u590d\u6742\u4e14\u8017\u65f6\uff0c\u7f3a\u4e4f\u81ea\u52a8\u5316\u8fc1\u79fb\u652f\u6301\u3002", "method": "\u901a\u8fc7\u6536\u96c6923\u4e2a\u5f00\u53d1\u8005\u5b9e\u9645\u5b8c\u6210\u7684\u4eceunittest\u5230pytest\u7684\u6d4b\u8bd5\u8fc1\u79fb\u6848\u4f8b\u6784\u5efa\u4e86TestMigrationsInPy\u6570\u636e\u96c6\u3002", "result": "TestMigrationsInPy\u5305\u542b\u4e86\u591a\u6837\u7684\u8fc1\u79fb\u7c7b\u578b\u4fe1\u606f\uff0c\u4fc3\u8fdb\u4e86\u9488\u5bf9\u4e0d\u540c\u8fc1\u79fb\u9700\u6c42\u7684\u89e3\u51b3\u65b9\u6848\u9a8c\u8bc1\u3002", "conclusion": "TestMigrationsInPy\u4e3aPython\u6d4b\u8bd5\u6846\u67b6\u8fc1\u79fb\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u6570\u636e\u96c6\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u81ea\u52a8\u5316\u8fc1\u79fb\u5de5\u5177\u7684\u7814\u53d1\u3002"}}
{"id": "2602.05189", "categories": ["cs.CL", "cs.HC", "cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2602.05189", "abs": "https://arxiv.org/abs/2602.05189", "authors": ["Hsuan-Yu Chou", "Wajiha Naveed", "Shuyan Zhou", "Xiaowei Yang"], "title": "Are Open-Weight LLMs Ready for Social Media Moderation? A Comparative Study on Bluesky", "comment": null, "summary": "As internet access expands, so does exposure to harmful content, increasing the need for effective moderation. Research has demonstrated that large language models (LLMs) can be effectively utilized for social media moderation tasks, including harmful content detection. While proprietary LLMs have been shown to zero-shot outperform traditional machine learning models, the out-of-the-box capability of open-weight LLMs remains an open question.\n  Motivated by recent developments of reasoning LLMs, we evaluate seven state-of-the-art models: four proprietary and three open-weight. Testing with real-world posts on Bluesky, moderation decisions by Bluesky Moderation Service, and annotations by two authors, we find a considerable degree of overlap between the sensitivity (81%--97%) and specificity (91%--100%) of the open-weight LLMs and those (72%--98%, and 93%--99%) of the proprietary ones. Additionally, our analysis reveals that specificity exceeds sensitivity for rudeness detection, but the opposite holds for intolerance and threats. Lastly, we identify inter-rater agreement across human moderators and the LLMs, highlighting considerations for deploying LLMs in both platform-scale and personalized moderation contexts. These findings show open-weight LLMs can support privacy-preserving moderation on consumer-grade hardware and suggest new directions for designing moderation systems that balance community values with individual user preferences.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\u5f00\u6e90\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u793e\u4ea4\u5a92\u4f53\u6709\u5bb3\u5185\u5bb9\u5ba1\u6838\u4e2d\u8868\u73b0\u63a5\u8fd1\u4e13\u6709\u6a21\u578b\uff0c\u652f\u6301\u9690\u79c1\u4fdd\u62a4\u548c\u4e2a\u6027\u5316\u9700\u6c42\uff0c\u63a8\u52a8\u5185\u5bb9\u5ba1\u6838\u7cfb\u7edf\u8bbe\u8ba1\u7684\u65b0\u65b9\u5411\u3002", "motivation": "\u4e92\u8054\u7f51\u63a5\u5165\u7684\u666e\u53ca\u52a0\u5267\u4e86\u6709\u5bb3\u5185\u5bb9\u7684\u4f20\u64ad\uff0c\u9700\u8981\u6709\u6548\u7684\u5185\u5bb9\u5ba1\u6838\u624b\u6bb5\uff0c\u800c\u5f00\u6e90LLMs\u5728\u96f6\u6837\u672c\u80fd\u529b\u4e0a\u7684\u8868\u73b0\u8fd8\u672a\u88ab\u5145\u5206\u9a8c\u8bc1\u3002", "method": "\u8bc4\u4f30\u4e86\u56db\u4e2a\u4e13\u6709\u6a21\u578b\u548c\u4e09\u4e2a\u5f00\u6e90\u6a21\u578b\uff0c\u901a\u8fc7\u5bf9Bluesky\u5e73\u53f0\u4e0a\u7684\u771f\u5b9e\u5e16\u5b50\u3001Bluesky\u5ba1\u6838\u670d\u52a1\u7684\u51b3\u7b56\u4ee5\u53ca\u4e24\u4f4d\u4f5c\u8005\u7684\u6ce8\u91ca\u8fdb\u884c\u6d4b\u8bd5\u548c\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u53d1\u73b0\u5f00\u6e90\u6a21\u578b\u5728\u7075\u654f\u5ea6\uff0881%\u201397%\uff09\u548c\u7279\u5f02\u6027\uff0891%\u2013100%\uff09\u4e0e\u4e13\u6709\u6a21\u578b\uff0872%\u201398%\u300193%\u201399%\uff09\u9ad8\u5ea6\u63a5\u8fd1\uff0c\u4e14\u4e0d\u540c\u6709\u5bb3\u5185\u5bb9\u7c7b\u522b\u7684\u68c0\u6d4b\u7075\u654f\u5ea6\u548c\u7279\u5f02\u6027\u5b58\u5728\u5dee\u5f02\uff0c\u540c\u65f6\u4eba\u7c7b\u5ba1\u6838\u5458\u4e0eLLMs\u4e4b\u95f4\u5177\u6709\u4e00\u5b9a\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "\u5f00\u6e90\u6743\u91cd\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u654f\u611f\u5185\u5bb9\u68c0\u6d4b\u7684\u7075\u654f\u5ea6\u548c\u7279\u5f02\u6027\u65b9\u9762\u4e0e\u4e13\u6709\u6a21\u578b\u5177\u6709\u8f83\u9ad8\u7684\u91cd\u53e0\u5ea6\uff0c\u80fd\u591f\u6709\u6548\u652f\u6301\u793e\u4ea4\u5a92\u4f53\u7684\u5185\u5bb9\u5ba1\u6838\u3002"}}
{"id": "2602.05123", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.05123", "abs": "https://arxiv.org/abs/2602.05123", "authors": ["Andre Hora", "Gordon Fraser"], "title": "Exceptional Behaviors: How Frequently Are They Tested?", "comment": "Published at AST 2025", "summary": "Exceptions allow developers to handle error cases expected to occur infrequently. Ideally, good test suites should test both normal and exceptional behaviors to catch more bugs and avoid regressions. While current research analyzes exceptions that propagate to tests, it does not explore other exceptions that do not reach the tests. In this paper, we provide an empirical study to explore how frequently exceptional behaviors are tested in real-world systems. We consider both exceptions that propagate to tests and the ones that do not reach the tests. For this purpose, we run an instrumented version of test suites, monitor their execution, and collect information about the exceptions raised at runtime. We analyze the test suites of 25 Python systems, covering 5,372 executed methods, 17.9M calls, and 1.4M raised exceptions. We find that 21.4% of the executed methods do raise exceptions at runtime. In methods that raise exceptions, on the median, 1 in 10 calls exercise exceptional behaviors. Close to 80% of the methods that raise exceptions do so infrequently, but about 20% raise exceptions more frequently. Finally, we provide implications for researchers and practitioners. We suggest developing novel tools to support exercising exceptional behaviors and refactoring expensive try/except blocks. We also call attention to the fact that exception-raising behaviors are not necessarily \"abnormal\" or rare.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5bf9\u5927\u89c4\u6a21Python\u6d4b\u8bd5\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u53d1\u73b0\u5f02\u5e38\u884c\u4e3a\u5728\u6d4b\u8bd5\u4e2d\u5e76\u975e\u7f55\u89c1\uff0c\u63d0\u9192\u5f00\u53d1\u8005\u548c\u7814\u7a76\u8005\u5173\u6ce8\u5f02\u5e38\u884c\u4e3a\u6d4b\u8bd5\u53ca\u76f8\u5173\u5de5\u5177\u5f00\u53d1\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u5173\u6ce8\u4f20\u9012\u5230\u6d4b\u8bd5\u7684\u5f02\u5e38\uff0c\u4f46\u5ffd\u89c6\u4e86\u672a\u4f20\u9012\u5230\u6d4b\u8bd5\u7684\u5f02\u5e38\u884c\u4e3a\uff0c\u672c\u6587\u5173\u6ce8\u5982\u4f55\u9891\u7e41\u5f02\u5e38\u884c\u4e3a\u88ab\u6d4b\u8bd5\u4ee5\u63d0\u5347\u8f6f\u4ef6\u8d28\u91cf\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u5bf925\u4e2aPython\u7cfb\u7edf\u7684\u6d4b\u8bd5\u5957\u4ef6\u8fdb\u884c\u63d2\u6869\u76d1\u63a7\uff0c\u6536\u96c6\u8fd0\u884c\u65f6\u629b\u51fa\u7684\u5f02\u5e38\u6570\u636e\uff0c\u5206\u6790\u4e86\u5f02\u5e38\u884c\u4e3a\u7684\u9891\u7387\u548c\u6d4b\u8bd5\u8986\u76d6\u60c5\u51b5\u3002", "result": "21.4%\u7684\u88ab\u6267\u884c\u65b9\u6cd5\u5728\u8fd0\u884c\u65f6\u629b\u51fa\u5f02\u5e38\uff0c\u5176\u4e2d\u5f02\u5e38\u8c03\u7528\u7684\u4e2d\u4f4d\u6570\u4e3a\u6bcf10\u6b21\u8c03\u7528\u4e2d\u89e6\u53d1\u4e00\u6b21\u5f02\u5e38\uff0c\u8fd180%\u5f02\u5e38\u65b9\u6cd5\u5076\u5c14\u629b\u5f02\u5e38\uff0c20%\u8f83\u9891\u7e41\u629b\u5f02\u5e38\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u53d1\u73b0\u5f02\u5e38\u884c\u4e3a\u5728\u6d4b\u8bd5\u4e2d\u88ab\u9891\u7e41\u89e6\u53d1\u4e14\u4e0d\u4e00\u5b9a\u7f55\u89c1\uff0c\u5f3a\u8c03\u4e86\u5f02\u5e38\u884c\u4e3a\u6d4b\u8bd5\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2602.05205", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05205", "abs": "https://arxiv.org/abs/2602.05205", "authors": ["Kenichiro Ando", "Tatsuya Harada"], "title": "Aligning Large Language Model Behavior with Human Citation Preferences", "comment": "Work In Progress", "summary": "Most services built on powerful large-scale language models (LLMs) add citations to their output to enhance credibility. Recent research has paid increasing attention to the question of what reference documents to link to outputs. However, how LLMs recognize cite-worthiness and how this process should be controlled remains underexplored. In this study, we focus on what kinds of content LLMs currently tend to cite and how well that behavior aligns with human preferences. We construct a dataset to characterize the relationship between human citation preferences and LLM behavior. Web-derived texts are categorized into eight citation-motivation types, and pairwise citation preferences are exhaustively evaluated across all type combinations to capture fine-grained contrasts. Our results show that humans most frequently seek citations for medical text, and stronger models display a similar tendency. We also find that current models are as much as $27\\%$ more likely than humans to add citations to text that is explicitly marked as needing citations on sources such as Wikipedia, and this overemphasis reduces alignment accuracy. Conversely, models systematically underselect numeric sentences (by $-22.6\\%$ relative to humans) and sentences containing personal names (by $-20.1\\%$), categories for which humans typically demand citations. Furthermore, experiments with Direct Preference Optimization demonstrate that model behavior can be calibrated to better match human citation preferences. We expect this study to provide a foundation for more fine-grained investigations into LLM citation preferences.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u6570\u636e\u96c6\u6784\u5efa\u548c\u504f\u597d\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5f15\u6587\u9009\u62e9\u4e0a\u4e0e\u4eba\u7c7b\u7684\u5dee\u5f02\uff0c\u6307\u51fa\u6a21\u578b\u5728\u67d0\u4e9b\u5185\u5bb9\u4e0a\u8fc7\u5ea6\u6216\u4e0d\u8db3\u5f15\u7528\uff0c\u4e14\u63d0\u51fa\u4f18\u5316\u65b9\u6cd5\u6539\u5584\u6a21\u578b\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u5339\u914d\u3002", "motivation": "\u73b0\u6709\u670d\u52a1\u4e3a\u4e86\u589e\u5f3a\u8f93\u51fa\u7684\u53ef\u4fe1\u5ea6\uff0c\u4f1a\u6dfb\u52a0\u5f15\u7528\u6587\u732e\uff0c\u4f46\u5982\u4f55\u8bc6\u522b\u5e94\u88ab\u5f15\u7528\u7684\u5185\u5bb9\u53ca\u5176\u63a7\u5236\u673a\u5236\u7814\u7a76\u4e0d\u8db3\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7a76\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5f53\u524d\u7684\u5f15\u6587\u503e\u5411\u53ca\u5176\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u5bf9\u9f50\u60c5\u51b5\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u516b\u79cd\u5f15\u6587\u52a8\u673a\u7c7b\u578b\u7684\u7f51\u7edc\u6587\u672c\u6570\u636e\u96c6\uff0c\u8fdb\u884c\u4e86\u4e24\u4e24\u5f15\u6587\u504f\u597d\u8bc4\u4f30\uff0c\u5206\u6790\u6a21\u578b\u4e0e\u4eba\u7c7b\u7684\u5f15\u6587\u503e\u5411\u5dee\u5f02\uff0c\u5e76\u901a\u8fc7\u76f4\u63a5\u504f\u597d\u4f18\u5316(Direct Preference Optimization)\u8c03\u6574\u6a21\u578b\u884c\u4e3a\u3002", "result": "\u6a21\u578b\u503e\u5411\u4e8e\u50cf\u4eba\u7c7b\u4e00\u6837\u9891\u7e41\u5730\u4e3a\u533b\u7597\u6587\u672c\u6dfb\u52a0\u5f15\u7528\uff0c\u4e14\u5bf9\u660e\u663e\u6807\u8bb0\u9700\u8981\u5f15\u7528\u7684\u5185\u5bb9\uff0c\u5982\u7ef4\u57fa\u767e\u79d1\u4e2d\u7684\u5185\u5bb9\uff0c\u5f15\u7528\u6982\u7387\u9ad8\u51fa\u4eba\u7c7b27%\uff0c\u4f46\u5bf9\u5305\u542b\u6570\u5b57\u548c\u4eba\u540d\u7684\u53e5\u5b50\u5f15\u7528\u4e0d\u8db3\uff0c\u5206\u522b\u4f4e\u4e8e\u4eba\u7c7b20.1%\u548c22.6%\u3002\u901a\u8fc7\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff0c\u6a21\u578b\u884c\u4e3a\u5f97\u5230\u4e86\u66f4\u597d\u7684\u6821\u51c6\u3002", "conclusion": "\u5f53\u524d\u7684\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5728\u5f15\u6587\u884c\u4e3a\u4e0a\u4e0e\u4eba\u7c7b\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5c24\u5176\u662f\u5728\u5bf9\u67d0\u4e9b\u7c7b\u578b\u5185\u5bb9\u7684\u5f15\u6587\u9009\u62e9\u4e0a\u5b58\u5728\u504f\u5dee\u3002\u901a\u8fc7\u76f4\u63a5\u504f\u597d\u4f18\u5316\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6709\u6548\u6821\u51c6\u6a21\u578b\u884c\u4e3a\uff0c\u4f7f\u5176\u4e0e\u4eba\u7c7b\u7684\u5f15\u6587\u504f\u597d\u66f4\u52a0\u4e00\u81f4\u3002"}}
{"id": "2602.05157", "categories": ["cs.SE", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.05157", "abs": "https://arxiv.org/abs/2602.05157", "authors": ["Alireza Abbaspour", "Shabin Mahadevan", "Kilian Zwirglmaier", "Jeff Stafford"], "title": "The Necessity of a Holistic Safety Evaluation Framework for AI-Based Automation Features", "comment": null, "summary": "The intersection of Safety of Intended Functionality (SOTIF) and Functional Safety (FuSa) analysis of driving automation features has traditionally excluded Quality Management (QM) components from rigorous safety impact evaluations. While QM components are not typically classified as safety-relevant, recent developments in artificial intelligence (AI) integration reveal that such components can contribute to SOTIF-related hazardous risks. Compliance with emerging AI safety standards, such as ISO/PAS 8800, necessitates re-evaluating safety considerations for these components. This paper examines the necessity of conducting holistic safety analysis and risk assessment on AI components, emphasizing their potential to introduce hazards with the capacity to violate risk acceptance criteria when deployed in safety-critical driving systems, particularly in perception algorithms. Using case studies, we demonstrate how deficiencies in AI-driven perception systems can emerge even in QM-classified components, leading to unintended functional behaviors with critical safety implications. By bridging theoretical analysis with practical examples, this paper argues for the adoption of comprehensive FuSa, SOTIF, and AI standards-driven methodologies to identify and mitigate risks in AI components. The findings demonstrate the importance of revising existing safety frameworks to address the evolving challenges posed by AI, ensuring comprehensive safety assurance across all component classifications spanning multiple safety standards.", "AI": {"tldr": "AI\u96c6\u6210\u4f7f\u9a7e\u9a76\u81ea\u52a8\u5316\u4e2d\u7684QM\u7ec4\u4ef6\u53ef\u80fd\u4ea7\u751f\u5b89\u5168\u98ce\u9669\uff0c\u9700\u901a\u8fc7\u7efc\u5408FuSa\u3001SOTIF\u53caAI\u6807\u51c6\u65b9\u6cd5\u91cd\u65b0\u8bc4\u4f30\u548c\u7ba1\u7406\u5b89\u5168\uff0c\u4fdd\u969c\u7cfb\u7edf\u6574\u4f53\u5b89\u5168\u3002", "motivation": "\u4f20\u7edf\u9a7e\u9a76\u81ea\u52a8\u5316\u529f\u80fd\u7684SOTIF\u548cFuSa\u5206\u6790\u672a\u5c06\u8d28\u91cf\u7ba1\u7406\uff08QM\uff09\u7ec4\u4ef6\u7eb3\u5165\u4e25\u683c\u7684\u5b89\u5168\u8bc4\u4f30\uff0c\u7136\u800cAI\u96c6\u6210\u4f7f\u8fd9\u4e9b\u7ec4\u4ef6\u53ef\u80fd\u5e26\u6765\u98ce\u9669\u3002", "method": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\uff0c\u7ed3\u5408\u7406\u8bba\u5206\u6790\uff0c\u63a2\u8ba8AI\u7ec4\u4ef6\u5c24\u5176\u662f\u611f\u77e5\u7b97\u6cd5\u4e2d\u7684\u5b89\u5168\u7f3a\u9677\uff0c\u91c7\u7528FuSa\u3001SOTIF\u53caAI\u5b89\u5168\u6807\u51c6\u65b9\u6cd5\u8fdb\u884c\u5168\u9762\u98ce\u9669\u8bc6\u522b\u548c\u7f13\u89e3\u3002", "result": "\u7814\u7a76\u8868\u660eQM\u7ec4\u4ef6\u4e2dAI\u9a71\u52a8\u611f\u77e5\u7cfb\u7edf\u7f3a\u9677\u53ef\u80fd\u5bfc\u81f4\u529f\u80fd\u5f02\u5e38\u548c\u5b89\u5168\u98ce\u9669\uff0c\u9700\u4fee\u8ba2\u73b0\u6709\u5b89\u5168\u6846\u67b6\u4ee5\u6db5\u76d6AI\u5e26\u6765\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u591a\u6807\u51c6\u4e0b\u5168\u9762\u5b89\u5168\u4fdd\u969c\u3002", "conclusion": "\u672c\u6587\u5f3a\u8c03\u5fc5\u987b\u5bf9AI\u7ec4\u4ef6\u5f00\u5c55\u6574\u4f53\u5b89\u5168\u5206\u6790\u4e0e\u98ce\u9669\u8bc4\u4f30\uff0c\u4fee\u8ba2\u5b89\u5168\u6807\u51c6\u4ee5\u9002\u5e94AI\u6280\u672f\uff0c\u786e\u4fdd\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u6240\u6709\u7ec4\u4ef6\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2602.05211", "categories": ["cs.CL", "cs.DL"], "pdf": "https://arxiv.org/pdf/2602.05211", "abs": "https://arxiv.org/abs/2602.05211", "authors": ["Hongye Zhao", "Yi Zhao", "Chengzhi Zhang"], "title": "Quantifying the Knowledge Proximity Between Academic and Industry Research: An Entity and Semantic Perspective", "comment": null, "summary": "The academia and industry are characterized by a reciprocal shaping and dynamic feedback mechanism. Despite distinct institutional logics, they have adapted closely in collaborative publishing and talent mobility, demonstrating tension between institutional divergence and intensive collaboration. Existing studies on their knowledge proximity mainly rely on macro indicators such as the number of collaborative papers or patents, lacking an analysis of knowledge units in the literature. This has led to an insufficient grasp of fine-grained knowledge proximity between industry and academia, potentially undermining collaboration frameworks and resource allocation efficiency. To remedy the limitation, this study quantifies the trajectory of academia-industry co-evolution through fine-grained entities and semantic space. In the entity measurement part, we extract fine-grained knowledge entities via pre-trained models, measure sequence overlaps using cosine similarity, and analyze topological features through complex network analysis. At the semantic level, we employ unsupervised contrastive learning to quantify convergence in semantic spaces by measuring cross-institutional textual similarities. Finally, we use citation distribution patterns to examine correlations between bidirectional knowledge flows and similarity. Analysis reveals that knowledge proximity between academia and industry rises, particularly following technological change. This provides textual evidence of bidirectional adaptation in co-evolution. Additionally, academia's knowledge dominance weakens during technological paradigm shifts. The dataset and code for this paper can be accessed at https://github.com/tinierZhao/Academic-Industrial-associations.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u7ec6\u7c92\u5ea6\u77e5\u8bc6\u5b9e\u4f53\u548c\u8bed\u4e49\u5206\u6790\uff0c\u63ed\u793a\u5b66\u672f\u754c\u4e0e\u4ea7\u4e1a\u754c\u5728\u6280\u672f\u53d8\u9769\u63a8\u52a8\u4e0b\u7684\u77e5\u8bc6\u534f\u540c\u8fdb\u5316\u8fc7\u7a0b\uff0c\u8868\u660e\u53cc\u65b9\u77e5\u8bc6\u63a5\u8fd1\u6027\u589e\u52a0\u53ca\u6743\u529b\u7ed3\u6784\u52a8\u6001\u53d8\u5316\u3002", "motivation": "\u73b0\u6709\u5173\u4e8e\u5b66\u672f\u754c\u4e0e\u4ea7\u4e1a\u754c\u77e5\u8bc6\u63a5\u8fd1\u6027\u7684\u7814\u7a76\u4e3b\u8981\u4f9d\u8d56\u5b8f\u89c2\u6307\u6807\uff0c\u7f3a\u4e4f\u5bf9\u6587\u732e\u4e2d\u77e5\u8bc6\u5355\u5143\u7684\u7ec6\u81f4\u5206\u6790\uff0c\u5bfc\u81f4\u65e0\u6cd5\u5145\u5206\u7406\u89e3\u4e24\u8005\u95f4\u7684\u7ec6\u7c92\u5ea6\u77e5\u8bc6\u63a5\u8fd1\u6027\uff0c\u5f71\u54cd\u534f\u4f5c\u6846\u67b6\u548c\u8d44\u6e90\u5206\u914d\u6548\u7387\u3002", "method": "\u901a\u8fc7\u9884\u8bad\u7ec3\u6a21\u578b\u63d0\u53d6\u7ec6\u7c92\u5ea6\u77e5\u8bc6\u5b9e\u4f53\uff0c\u5229\u7528\u4f59\u5f26\u76f8\u4f3c\u5ea6\u8861\u91cf\u5b9e\u4f53\u5e8f\u5217\u91cd\u53e0\uff0c\u7ed3\u5408\u590d\u6742\u7f51\u7edc\u5206\u6790\u62d3\u6251\u7279\u5f81\uff1b\u5728\u8bed\u4e49\u5c42\u9762\uff0c\u91c7\u7528\u65e0\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u6d4b\u91cf\u8de8\u673a\u6784\u6587\u672c\u76f8\u4f3c\u6027\uff1b\u6700\u540e\u901a\u8fc7\u5f15\u7528\u5206\u5e03\u5206\u6790\u53cc\u5411\u77e5\u8bc6\u6d41\u52a8\u4e0e\u76f8\u4f3c\u6027\u7684\u5173\u7cfb\u3002", "result": "\u53d1\u73b0\u5b66\u672f\u754c\u4e0e\u4ea7\u4e1a\u754c\u7684\u77e5\u8bc6\u63a5\u8fd1\u6027\u968f\u7740\u6280\u672f\u53d8\u9769\u800c\u63d0\u5347\uff0c\u4f53\u73b0\u51fa\u53cc\u5411\u9002\u5e94\u7684\u534f\u540c\u8fdb\u5316\u673a\u5236\uff0c\u540c\u65f6\u6280\u672f\u8303\u5f0f\u8f6c\u53d8\u65f6\u5b66\u672f\u754c\u7684\u77e5\u8bc6\u4e3b\u5bfc\u5730\u4f4d\u51cf\u5f31\u3002", "conclusion": "\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5b9e\u4f53\u548c\u8bed\u4e49\u7a7a\u95f4\u7684\u591a\u7ef4\u5ea6\u5206\u6790\uff0c\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u5b66\u672f\u754c\u4e0e\u4ea7\u4e1a\u754c\u77e5\u8bc6\u534f\u540c\u6f14\u5316\u7684\u52a8\u6001\u7279\u5f81\u53ca\u5176\u673a\u5236\uff0c\u4e3a\u7406\u89e3\u548c\u4f18\u5316\u53cc\u65b9\u7684\u534f\u4f5c\u5173\u7cfb\u63d0\u4f9b\u4e86\u65b0\u7684\u8bc1\u636e\u548c\u65b9\u6cd5\u3002"}}
{"id": "2602.05242", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05242", "abs": "https://arxiv.org/abs/2602.05242", "authors": ["Chenhui Mao", "Yuanting Lei", "Zhixiang Wei", "Ming Liang", "Zhixiang Wang", "Jingxuan Xu", "Dajun Chen", "Wei Jiang", "Yong Li"], "title": "EGSS: Entropy-guided Stepwise Scaling for Reliable Software Engineering", "comment": null, "summary": "Agentic Test-Time Scaling (TTS) has delivered state-of-the-art (SOTA) performance on complex software engineering tasks such as code generation and bug fixing. However, its practical adoption remains limited due to significant computational overhead, primarily driven by two key challenges: (1) the high cost associated with deploying excessively large ensembles, and (2) the lack of a reliable mechanism for selecting the optimal candidate solution, ultimately constraining the performance gains that can be realized. To address these challenges, we propose Entropy-Guided Stepwise Scaling (EGSS), a novel TTS framework that dynamically balances efficiency and effectiveness through entropy-guided adaptive search and robust test-suite augmentation. Extensive experiments on SWE-Bench-Verified demonstrate that EGSS consistently boosts performance by 5-10% across all evaluated models. Specifically, it increases the resolved ratio of Kimi-K2-Intruct from 63.2% to 72.2%, and GLM-4.6 from 65.8% to 74.6%. Furthermore, when paired with GLM-4.6, EGSS achieves a new state-of-the-art among open-source large language models. In addition to these accuracy improvements, EGSS reduces inference-time token usage by over 28% compared to existing TTS methods, achieving simultaneous gains in both effectiveness and computational efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7684EGSS\u65b9\u6cd5\u901a\u8fc7\u71b5\u5f15\u5bfc\u641c\u7d22\u548c\u6d4b\u8bd5\u96c6\u6269\u5c55\uff0c\u63d0\u5347\u4e86\u4ee3\u7801\u751f\u6210\u548c\u9519\u8bef\u4fee\u590d\u4efb\u52a1\u4e2d\u6d4b\u8bd5\u65f6\u6269\u5c55\u7684\u6548\u7387\u4e0e\u6548\u679c\uff0c\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u5e76\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u9488\u5bf9Agentic Test-Time Scaling (TTS)\u5728\u8ba1\u7b97\u5f00\u9500\u5927\u53ca\u7f3a\u4e4f\u53ef\u9760\u5019\u9009\u89e3\u9009\u62e9\u673a\u5236\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u6539\u8fdb\u65b9\u6848\u4ee5\u63d0\u5347\u6027\u80fd\u548c\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86Entropy-Guided Stepwise Scaling (EGSS)\uff0c\u901a\u8fc7\u71b5\u5f15\u5bfc\u7684\u81ea\u9002\u5e94\u641c\u7d22\u548c\u7a33\u5065\u7684\u6d4b\u8bd5\u96c6\u6269\u5c55\uff0c\u5b9e\u73b0\u4e86\u52a8\u6001\u5e73\u8861\u6548\u7387\u4e0e\u6548\u679c\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u6846\u67b6\u3002", "result": "EGSS\u5728SWE-Bench-Verified\u6570\u636e\u96c6\u4e0a\u4f7f\u6240\u6709\u6a21\u578b\u6027\u80fd\u63d0\u53475-10%\uff0c\u5982\u5c06Kimi-K2-Intruct\u89e3\u51b3\u7387\u4ece63.2%\u63d0\u5347\u523072.2%\uff0cGLM-4.6\u63d0\u5347\u81f374.6%\uff0c\u5e76\u51cf\u5c11\u63a8\u7406\u65f6token\u4f7f\u7528\u91cf\u8d85\u8fc728%\u3002", "conclusion": "EGSS\u663e\u8457\u63d0\u9ad8\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u8868\u73b0\uff0c\u8fbe\u5230\u4e86SOTA\u6c34\u5e73\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u8ba1\u7b97\u8d44\u6e90\u7684\u4f7f\u7528\u3002"}}
{"id": "2602.05220", "categories": ["cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.05220", "abs": "https://arxiv.org/abs/2602.05220", "authors": ["Jinchuan Tian", "Haoran Wang", "Bo-Hao Su", "Chien-yu Huang", "Qingzheng Wang", "Jiatong Shi", "William Chen", "Xun Gong", "Siddhant Arora", "Chin-Jou Li", "Masao Someki", "Takashi Maekaku", "Yusuke Shinohara", "Jin Sakuma", "Chao-Han Huck Yang", "Shinji Watanabe"], "title": "Bagpiper: Solving Open-Ended Audio Tasks via Rich Captions", "comment": null, "summary": "Current audio foundation models typically rely on rigid, task-specific supervision, addressing isolated factors of audio rather than the whole. In contrast, human intelligence processes audio holistically, seamlessly bridging physical signals with abstract cognitive concepts to execute complex tasks. Grounded in this philosophy, we introduce Bagpiper, an 8B audio foundation model that interprets physical audio via rich captions, i.e., comprehensive natural language descriptions that encapsulate the critical cognitive concepts inherent in the signal (e.g., transcription, audio events). By pre-training on a massive corpus of 600B tokens, the model establishes a robust bidirectional mapping between raw audio and this high-level conceptual space. During fine-tuning, Bagpiper adopts a caption-then-process workflow, simulating an intermediate cognitive reasoning step to solve diverse tasks without task-specific priors. Experimentally, Bagpiper outperforms Qwen-2.5-Omni on MMAU and AIRBench for audio understanding and surpasses CosyVoice3 and TangoFlux in generation quality, capable of synthesizing arbitrary compositions of speech, music, and sound effects. To the best of our knowledge, Bagpiper is among the first works that achieve unified understanding generation for general audio. Model, data, and code are available at Bagpiper Home Page.", "AI": {"tldr": "\u63d0\u51faBagpiper\uff0c\u4e00\u79cd\u57fa\u4e8e\u4e30\u5bcc\u8bed\u8a00\u63cf\u8ff0\u7684\u5927\u89c4\u6a21\u97f3\u9891\u57fa\u7840\u6a21\u578b\uff0c\u5b9e\u73b0\u7edf\u4e00\u7684\u97f3\u9891\u7406\u89e3\u4e0e\u751f\u6210\uff0c\u6027\u80fd\u4f18\u8d8a\u3002", "motivation": "\u73b0\u6709\u97f3\u9891\u6a21\u578b\u4f9d\u8d56\u7279\u5b9a\u4efb\u52a1\u76d1\u7763\uff0c\u4ec5\u5904\u7406\u5b64\u7acb\u97f3\u9891\u56e0\u7d20\uff0c\u65e0\u6cd5\u50cf\u4eba\u7c7b\u4e00\u822c\u6574\u4f53\u7406\u89e3\u97f3\u9891\u4fe1\u53f7\u3002", "method": "\u901a\u8fc7\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\uff08600B tokens\uff09\u5efa\u7acb\u539f\u59cb\u97f3\u9891\u4e0e\u9ad8\u5c42\u6b21\u8ba4\u77e5\u6982\u5ff5\u4e4b\u95f4\u7684\u53cc\u5411\u6620\u5c04\uff0c\u91c7\u7528caption-then-process\u7684\u6d41\u7a0b\u6a21\u62df\u8ba4\u77e5\u63a8\u7406\uff0c\u4ece\u800c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u5148\u9a8c\u5b8c\u6210\u591a\u6837\u4efb\u52a1\u3002", "result": "Bagpiper\u5728MMAU\u548cAIRBench\u4e0a\u4f18\u4e8eQwen-2.5-Omni\uff0c\u5728\u97f3\u9891\u751f\u6210\u8d28\u91cf\u4e0a\u8d85\u8d8aCosyVoice3\u548cTangoFlux\uff0c\u652f\u6301\u591a\u6837\u97f3\u9891\u5185\u5bb9\u7684\u5408\u6210\u3002", "conclusion": "Bagpiper\u6a21\u578b\u5b9e\u73b0\u4e86\u7edf\u4e00\u7684\u97f3\u9891\u7406\u89e3\u4e0e\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u9879\u97f3\u9891\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2602.05270", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.05270", "abs": "https://arxiv.org/abs/2602.05270", "authors": ["Thanh Le-Cong", "Bach Le", "Toby Murray", "Michael Pradel", "Cristian Cadar"], "title": "PatchGuru: Patch Oracle Inference from Natural Language Artifacts with Large Language Models", "comment": null, "summary": "As software systems evolve, patches may unintentionally alter program behavior. Validating patches against their intended semantics is difficult due to incomplete regression tests and informal, non-executable natural language (NL) descriptions of patch intent. We present PatchGuru, the first automated technique that infers executable patch specifications from real-world pull requests (PRs). Given a PR, PatchGuru uses large language models (LLMs) to extract developer intent from NL artifacts and synthesizes patch oracles: under-approximate yet practical specifications expressed as runtime assertions in comparison programs that integrate pre- and post-patch versions. Patch oracles focus on patch-relevant behaviors, enable automated validation, and support cross-version properties. PatchGuru iteratively refines inferred oracles by comparing pre- and post-patch behaviors, identifies violations, filters inconsistencies via self-review, and generates bug reports. We evaluate PatchGuru on 400 recent PRs from four widely used open-source Python projects. PatchGuru reports 39 warnings with a precision of 0.62, yielding 24 confirmed true positives, including 12 previously unknown bugs, 11 of which were subsequently fixed by developers. Compared to the state-of-the-art technique Testora, PatchGuru detects 17 more bugs (24 vs. 7) while improving precision from 0.32 to 0.62. PatchGuru incurs an average cost of 8.9 minutes and USD 0.07 per PR. These results suggest that PatchGuru complements code review and regression testing by providing executable documentation and automated validation of patch intent.", "AI": {"tldr": "PatchGuru\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u8865\u4e01\u610f\u56fe\u63a8\u65ad\u548c\u9a8c\u8bc1\u6280\u672f\uff0c\u6709\u6548\u63d0\u5347\u8865\u4e01\u9a8c\u8bc1\u7cbe\u5ea6\u548c\u6548\u7387\uff0c\u53d1\u73b0\u591a\u4e2a\u771f\u5b9e\u6f0f\u6d1e\uff0c\u8f85\u52a9\u4ee3\u7801\u5ba1\u67e5\u548c\u56de\u5f52\u6d4b\u8bd5\u3002", "motivation": "\u73b0\u6709\u7684\u56de\u5f52\u6d4b\u8bd5\u4e0d\u5b8c\u6574\uff0c\u4e14\u8865\u4e01\u610f\u56fe\u901a\u5e38\u4ee5\u975e\u6b63\u5f0f\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff0c\u7ed9\u8865\u4e01\u9a8c\u8bc1\u5e26\u6765\u56f0\u96be\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u4e14\u53ef\u6267\u884c\u7684\u8865\u4e01\u610f\u56fe\u9a8c\u8bc1\u65b9\u6cd5\u3002", "method": "PatchGuru \u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ece\u81ea\u7136\u8bed\u8a00\u7684\u5f00\u53d1\u8005\u610f\u56fe\u4e2d\u63d0\u53d6\u4fe1\u606f\uff0c\u5e76\u7efc\u5408\u8865\u4e01\u7684\u524d\u540e\u7248\u672c\u5408\u6210\u8fd0\u884c\u65f6\u65ad\u8a00\u4f5c\u4e3a\u8865\u4e01\u89c4\u683c\uff0c\u540c\u65f6\u901a\u8fc7\u8fed\u4ee3\u6bd4\u8f83\u548c\u81ea\u6211\u5ba1\u67e5\u4e0d\u65ad\u4f18\u5316\u8865\u4e01\u9a8c\u8bc1\u8fc7\u7a0b\u3002", "result": "\u5728400\u4e2a\u5f00\u6e90Python\u9879\u76ee\u7684\u62c9\u53d6\u8bf7\u6c42\u4e0a\u6d4b\u8bd5\uff0cPatchGuru\u62a5\u544a\u51fa39\u4e2a\u8b66\u544a\uff0c\u7cbe\u786e\u5ea6\u8fbe\u52300.62\uff0c\u53d1\u73b0\u4e8624\u4e2a\u771f\u5b9e\u6f0f\u6d1e\uff0c\u5176\u4e2d12\u4e2a\u4e3a\u672a\u77e5\u6f0f\u6d1e\uff0c\u5e76\u4e14\u4f18\u4e8e\u73b0\u6709\u6280\u672fTestora\u3002", "conclusion": "PatchGuru \u6210\u529f\u5730\u5b9e\u73b0\u4e86\u81ea\u52a8\u63a8\u65ad\u8865\u4e01\u89c4\u683c\u5e76\u9a8c\u8bc1\u8865\u4e01\u610f\u56fe\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8865\u4e01\u9a8c\u8bc1\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2602.05235", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05235", "abs": "https://arxiv.org/abs/2602.05235", "authors": ["Zhilin Liang", "Yuxiang Wang", "Zimu Zhou", "Hainan Zhang", "Boyi Liu", "Yongxin Tong"], "title": "FedMosaic: Federated Retrieval-Augmented Generation via Parametric Adapters", "comment": "11 pages", "summary": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by grounding generation in external knowledge to improve factuality and reduce hallucinations. Yet most deployments assume a centralized corpus, which is infeasible in privacy aware domains where knowledge remains siloed. This motivates federated RAG (FedRAG), where a central LLM server collaborates with distributed silos without sharing raw documents. In context RAG violates this requirement by transmitting verbatim documents, whereas parametric RAG encodes documents into lightweight adapters that merge with a frozen LLM at inference, avoiding raw-text exchange. We adopt the parametric approach but face two unique challenges induced by FedRAG: high storage and communication from per-document adapters, and destructive aggregation caused by indiscriminately merging multiple adapters. We present FedMosaic, the first federated RAG framework built on parametric adapters. FedMosaic clusters semantically related documents into multi-document adapters with document-specific masks to reduce overhead while preserving specificity, and performs selective adapter aggregation to combine only relevance-aligned, nonconflicting adapters. Experiments show that FedMosaic achieves an average 10.9% higher accuracy than state-of-the-art methods in four categories, while lowering storage costs by 78.8% to 86.3% and communication costs by 91.4%, and never sharing raw documents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFedMosaic\uff0c\u4e00\u4e2a\u57fa\u4e8e\u53c2\u6570\u5316\u9002\u914d\u5668\u7684\u8054\u90a6RAG\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u805a\u7c7b\u548c\u9009\u62e9\u6027\u9002\u914d\u5668\u805a\u5408\uff0c\u89e3\u51b3\u4e86\u9690\u79c1\u573a\u666f\u4e0b\u77e5\u8bc6\u5206\u6563\u95ee\u9898\uff0c\u5b9e\u73b0\u6027\u80fd\u548c\u6548\u7387\u53cc\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u9690\u79c1\u4fdd\u62a4\u573a\u666f\u4e2d\u77e5\u8bc6\u5b64\u5c9b\u95ee\u9898\uff0c\u56e0\u4e2d\u592e\u8bed\u6599\u5e93\u4e0d\u73b0\u5b9e\uff0c\u4f20\u7edfRAG\u65b9\u6cd5\u5b58\u5728\u76f4\u63a5\u4f20\u8f93\u6587\u6863\u8fdd\u80cc\u9690\u79c1\u9700\u6c42\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u53c2\u6570\u5316\u9002\u914d\u5668\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u591a\u6587\u6863\u9002\u914d\u5668\u53ca\u6587\u6863\u7279\u5b9a\u63a9\u7801\uff0c\u5b9e\u73b0\u8bed\u4e49\u76f8\u5173\u6587\u6863\u805a\u7c7b\uff0c\u7ed3\u5408\u9009\u62e9\u6027\u805a\u5408\u7b56\u7565\uff0c\u907f\u514d\u4e86\u9002\u914d\u5668\u95f4\u7684\u7834\u574f\u6027\u878d\u5408\u3002", "result": "FedMosaic\u5728\u56db\u4e2a\u7c7b\u522b\u7684\u4efb\u52a1\u4e2d\uff0c\u51c6\u786e\u7387\u63d0\u5347\u5e73\u574710.9%\uff0c\u5b58\u50a8\u6210\u672c\u964d\u4f4e78.8%\u81f386.3%\uff0c\u901a\u4fe1\u6210\u672c\u964d\u4f4e91.4%\uff0c\u65e0\u539f\u59cb\u6587\u6863\u5171\u4eab\u3002", "conclusion": "FedMosaic\u5728\u4fdd\u8bc1\u4e0d\u5171\u4eab\u539f\u59cb\u6587\u6863\u7684\u524d\u63d0\u4e0b\uff0c\u901a\u8fc7\u591a\u6587\u6863\u9002\u914d\u5668\u7684\u8bed\u4e49\u805a\u7c7b\u548c\u9009\u62e9\u6027\u9002\u914d\u5668\u805a\u5408\uff0c\u6709\u6548\u63d0\u5347\u4e86\u8054\u90a6RAG\u7cfb\u7edf\u7684\u51c6\u786e\u7387\uff0c\u4e14\u663e\u8457\u964d\u4f4e\u4e86\u5b58\u50a8\u548c\u901a\u4fe1\u5f00\u9500\u3002"}}
{"id": "2602.05312", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.05312", "abs": "https://arxiv.org/abs/2602.05312", "authors": ["Tatsuya Shirai", "Olivier Nourry", "Yutaro Kashiwa", "Kenji Fujiwara", "Hajimu Iida"], "title": "Does Programming Language Matter? An Empirical Study of Fuzzing Bug Detection", "comment": "Accepted to the 23rd International Conference on Mining Software Repositories (MSR 2026). 12 pages, 9 figures", "summary": "Fuzzing has become a popular technique for automatically detecting vulnerabilities and bugs by generating unexpected inputs. In recent years, the fuzzing process has been integrated into continuous integration workflows (i.e., continuous fuzzing), enabling short and frequent testing cycles. Despite its widespread adoption, prior research has not examined whether the effectiveness of continuous fuzzing varies across programming languages. This study conducts a large-scale cross-language analysis to examine how fuzzing bug characteristics and detection efficiency differ among languages. We analyze 61,444 fuzzing bugs and 999,248 builds from 559 OSS-Fuzz projects categorized by primary language. Our findings reveal that (i) C++ and Rust exhibit higher fuzzing bug detection frequencies, (ii) Rust and Python show low vulnerability ratios but tend to expose more critical vulnerabilities, (iii) crash types vary across languages and unreproducible bugs are more frequent in Go but rare in Rust, and (iv) Python attains higher patch coverage but suffers from longer time-to-detection. These results demonstrate that fuzzing behavior and effectiveness are strongly shaped by language design, providing insights for language-aware fuzzing strategies and tool development.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u5927\u89c4\u6a21\u5206\u6790\u8fde\u7eed\u6a21\u7cca\u6d4b\u8bd5\u5728\u591a\u8bed\u8a00\u9879\u76ee\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u8bed\u8a00\u7279\u6027\u663e\u8457\u5f71\u54cd\u68c0\u6d4b\u6548\u7387\u548c\u6f0f\u6d1e\u7279\u5f81\uff0c\u63d0\u793a\u5f00\u53d1\u8bed\u8a00\u611f\u77e5\u7684\u6a21\u7cca\u6d4b\u8bd5\u7b56\u7565\u3002", "motivation": "\u5c3d\u7ba1\u8fde\u7eed\u6a21\u7cca\u6d4b\u8bd5\u88ab\u5e7f\u6cdb\u91c7\u7528\uff0c\u4f46\u6b64\u524d\u7814\u7a76\u672a\u63a2\u8ba8\u5176\u5728\u4e0d\u540c\u7f16\u7a0b\u8bed\u8a00\u4e2d\u7684\u6548\u679c\u5dee\u5f02\u3002", "method": "\u5bf9559\u4e2aOSS-Fuzz\u9879\u76ee\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u8de8\u8bed\u8a00\u5206\u6790\uff0c\u5206\u6790\u4e8661,444\u4e2a\u6a21\u7cca\u6d4b\u8bd5\u7f3a\u9677\u548c999,248\u4e2a\u6784\u5efa\u6570\u636e\uff0c\u6309\u4e3b\u8981\u8bed\u8a00\u7c7b\u522b\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "C++\u548cRust\u7684\u7f3a\u9677\u68c0\u6d4b\u9891\u7387\u8f83\u9ad8\uff1bRust\u548cPython\u7684\u6f0f\u6d1e\u6bd4\u4f8b\u8f83\u4f4e\u4f46\u66b4\u9732\u66f4\u591a\u5173\u952e\u6f0f\u6d1e\uff1b\u4e0d\u540c\u8bed\u8a00\u7684\u5d29\u6e83\u7c7b\u578b\u4e0d\u540c\uff0cGo\u4e2d\u4e0d\u53ef\u91cd\u73b0\u7f3a\u9677\u8f83\u591a\uff0cRust\u8f83\u5c11\uff1bPython\u4fee\u590d\u8986\u76d6\u7387\u9ad8\u4f46\u68c0\u6d4b\u65f6\u95f4\u8f83\u957f\u3002", "conclusion": "\u6a21\u7cca\u6d4b\u8bd5\u7684\u6548\u679c\u5728\u4e0d\u540c\u7f16\u7a0b\u8bed\u8a00\u4e2d\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u8bed\u8a00\u8bbe\u8ba1\u5bf9\u6a21\u7cca\u6d4b\u8bd5\u7684\u884c\u4e3a\u548c\u6548\u7387\u6709\u91cd\u8981\u5f71\u54cd\u3002"}}
{"id": "2602.05252", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05252", "abs": "https://arxiv.org/abs/2602.05252", "authors": ["Guangwei Zhang", "Jianing Zhu", "Cheng Qian", "Neil Gong", "Rada Mihalcea", "Zhaozhuo Xu", "Jingrui He", "Jiaqi Ma", "Yun Huang", "Chaowei Xiao", "Bo Li", "Ahmed Abbasi", "Dongwon Lee", "Heng Ji", "Denghui Zhang"], "title": "Copyright Detective: A Forensic System to Evidence LLMs Flickering Copyright Leakage Risks", "comment": null, "summary": "We present Copyright Detective, the first interactive forensic system for detecting, analyzing, and visualizing potential copyright risks in LLM outputs. The system treats copyright infringement versus compliance as an evidence discovery process rather than a static classification task due to the complex nature of copyright law. It integrates multiple detection paradigms, including content recall testing, paraphrase-level similarity analysis, persuasive jailbreak probing, and unlearning verification, within a unified and extensible framework. Through interactive prompting, response collection, and iterative workflows, our system enables systematic auditing of verbatim memorization and paraphrase-level leakage, supporting responsible deployment and transparent evaluation of LLM copyright risks even with black-box access.", "AI": {"tldr": "Copyright Detective\u662f\u4e00\u4e2a\u96c6\u591a\u79cd\u68c0\u6d4b\u6280\u672f\u4e8e\u4e00\u4f53\u7684\u4e92\u52a8\u5f0f\u7cfb\u7edf\uff0c\u4e13\u4e3a\u53d1\u73b0\u548c\u5206\u6790\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u4e2d\u7684\u7248\u6743\u98ce\u9669\u8bbe\u8ba1\uff0c\u652f\u6301\u9ed1\u7bb1\u73af\u5883\u4e0b\u7cfb\u7edf\u5316\u5ba1\u8ba1\u548c\u900f\u660e\u8bc4\u4f30\u3002", "motivation": "\u7531\u4e8e\u7248\u6743\u6cd5\u7684\u590d\u6742\u6027\uff0c\u7248\u6743\u4fb5\u6743\u68c0\u6d4b\u5e94\u5f53\u89c6\u4e3a\u6301\u7eed\u7684\u8bc1\u636e\u53d1\u73b0\u8fc7\u7a0b\uff0c\u800c\u975e\u7b80\u5355\u7684\u9759\u6001\u5206\u7c7b\u4efb\u52a1\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u5168\u9762\u4e14\u4ea4\u4e92\u5f0f\u7684\u7248\u6743\u98ce\u9669\u68c0\u6d4b\u7cfb\u7edf\u3002", "method": "\u96c6\u6210\u5185\u5bb9\u56de\u5fc6\u6d4b\u8bd5\u3001\u610f\u8bd1\u7ea7\u522b\u76f8\u4f3c\u6027\u5206\u6790\u3001\u7ed5\u8fc7\u68c0\u6d4b\u548c\u53bb\u5b66\u4e60\u9a8c\u8bc1\u7b49\u591a\u79cd\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7ed3\u5408\u4ea4\u4e92\u5f0f\u63d0\u793a\u3001\u54cd\u5e94\u6536\u96c6\u548c\u8fed\u4ee3\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5b9e\u73b0\u7cfb\u7edf\u5316\u7684\u7248\u6743\u98ce\u9669\u5ba1\u8ba1\u3002", "result": "\u63d0\u51fa\u4e86\u9996\u4e2a\u4ea4\u4e92\u5f0f\u7248\u6743\u53d6\u8bc1\u7cfb\u7edfCopyright Detective\uff0c\u80fd\u591f\u5728\u9ed1\u7bb1\u6761\u4ef6\u4e0b\u68c0\u6d4b\u548c\u53ef\u89c6\u5316LLM\u8f93\u51fa\u4e2d\u7684\u7248\u6743\u98ce\u9669\uff0c\u786e\u4fdd\u8d23\u4efb\u90e8\u7f72\u548c\u900f\u660e\u8bc4\u4f30\u3002", "conclusion": "Copyright Detective\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u548c\u5206\u6790\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u4e2d\u7684\u6f5c\u5728\u7248\u6743\u98ce\u9669\uff0c\u652f\u6301\u5168\u9762\u548c\u900f\u660e\u7684\u7248\u6743\u5408\u89c4\u8bc4\u4f30\u3002"}}
{"id": "2602.05458", "categories": ["cs.SE", "cs.DC", "cs.PF", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.05458", "abs": "https://arxiv.org/abs/2602.05458", "authors": ["Anatoly A. Krasnovsky"], "title": "Emergence-as-Code for Self-Governing Reliable Systems", "comment": null, "summary": "SLO-as-code has made per-service} reliability declarative, but user experience is defined by journeys whose reliability is an emergent property of microservice topology, routing, redundancy, timeouts/fallbacks, shared failure domains, and tail amplification. As a result, journey objectives (e.g., \"checkout p99 < 400 ms\") are often maintained outside code and drift as the system evolves, forcing teams to either miss user expectations or over-provision and gate releases with ad-hoc heuristics. We propose Emergence-as-Code (EmaC), a vision for making journey reliability computable and governable via intent plus evidence. An EmaC spec declares journey intent (objective, control-flow operators, allowed actions) and binds it to atomic SLOs and telemetry. A runtime inference component consumes operational artifacts (e.g., tracing and traffic configuration) to synthesize a candidate journey model with provenance and confidence. From the last accepted model, the EmaC compiler/controller derives bounded journey SLOs and budgets under explicit correlation assumptions (optimistic independence vs. pessimistic shared fate), and emits control-plane artifacts (burn-rate alerts, rollout gates, action guards) that are reviewable in a Git workflow. An anonymized artifact repository provides a runnable example specification and generated outputs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Emergence-as-Code\u65b9\u6cd5\uff0c\u4f7f\u590d\u6742\u5fae\u670d\u52a1\u73af\u5883\u4e2d\u7528\u6237\u65c5\u7a0b\u53ef\u9760\u6027\u76ee\u6807\u53ef\u58f0\u660e\u3001\u53ef\u8ba1\u7b97\u5e76\u81ea\u52a8\u6cbb\u7406\uff0c\u63d0\u5347\u4e86\u670d\u52a1\u8d28\u91cf\u7ba1\u7406\u7684\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u4f20\u7edfSLO-as-code\u867d\u5b9a\u4e49\u4e86\u670d\u52a1\u53ef\u9760\u6027\uff0c\u4f46\u7528\u6237\u4f53\u9a8c\u7531\u590d\u6742\u5fae\u670d\u52a1\u62d3\u6251\u548c\u591a\u56e0\u7d20\u5171\u540c\u5f71\u54cd\uff0c\u65c5\u7a0b\u76ee\u6807\u6613\u6f02\u79fb\uff0c\u5bfc\u81f4\u7528\u6237\u671f\u671b\u96be\u4ee5\u7cbe\u51c6\u6ee1\u8db3\u6216\u8d44\u6e90\u8fc7\u5ea6\u9884\u7559\uff0c\u7f3a\u4e4f\u7edf\u4e00\u5efa\u6a21\u548c\u81ea\u52a8\u5316\u6cbb\u7406\u624b\u6bb5\u3002", "method": "\u901a\u8fc7\u5b9a\u4e49\u65c5\u7a0b\u610f\u56fe\u58f0\u660e\u4e0e\u539f\u5b50SLO\u53ca\u9065\u6d4b\u7ed1\u5b9a\uff0c\u7ed3\u5408\u8fd0\u884c\u65f6\u63a8\u7406\u7ec4\u4ef6\u4ece\u8f68\u8ff9\u548c\u6d41\u91cf\u914d\u7f6e\u4e2d\u5408\u6210\u65c5\u7a0b\u6a21\u578b\uff0c\u4f7f\u7528EmaC\u7f16\u8bd1\u5668/\u63a7\u5236\u5668\u751f\u6210\u6709\u9650\u754c\u65c5\u7a0bSLO\u548c\u9884\u7b97\uff0c\u5e76\u81ea\u52a8\u4ea7\u51fa\u63a7\u5236\u9762\u5de5\u4ef6\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u6cbb\u7406\u3002", "result": "EmaC\u89c4\u8303\u5b9e\u73b0\u4e86\u4ece\u610f\u56fe\u58f0\u660e\u5230\u52a8\u6001\u751f\u6210\u65c5\u7a0bSLO\u3001\u9884\u7b97\u53ca\u7ba1\u63a7\u7b56\u7565\u7684\u95ed\u73af\uff0c\u5e76\u901a\u8fc7Git\u5de5\u4f5c\u6d41\u652f\u6301\u5ba1\u67e5\uff0c\u63d0\u4f9b\u793a\u4f8b\u89c4\u8303\u53ca\u4ea7\u751f\u7684\u63a7\u5236\u5de5\u4ef6\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u548c\u5b9e\u8df5\u4ef7\u503c\u3002", "conclusion": "\u63d0\u51fa\u7684Emergence-as-Code (EmaC)\u6846\u67b6\u6709\u6548\u5b9e\u73b0\u4e86\u53ef\u8ba1\u7b97\u548c\u53ef\u6cbb\u7406\u7684\u7528\u6237\u65c5\u7a0b\u53ef\u9760\u6027\u58f0\u660e\uff0c\u89e3\u51b3\u4e86\u5fae\u670d\u52a1\u7cfb\u7edf\u4e2d\u65c5\u884c\u76ee\u6807\u4e0e\u5b9e\u9645\u8fd0\u7ef4\u4ee3\u7801\u8131\u8282\u7684\u95ee\u9898\u3002"}}
{"id": "2602.05258", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.05258", "abs": "https://arxiv.org/abs/2602.05258", "authors": ["Haoran Li", "Sucheng Ren", "Alan Yuille", "Feng Wang"], "title": "CoPE: Clipped RoPE as A Scalable Free Lunch for Long Context LLMs", "comment": null, "summary": "Rotary Positional Embedding (RoPE) is a key component of context scaling in Large Language Models (LLMs). While various methods have been proposed to adapt RoPE to longer contexts, their guiding principles generally fall into two categories: (1) out-of-distribution (OOD) mitigation, which scales RoPE frequencies to accommodate unseen positions, and (2) Semantic Modeling, which posits that the attention scores computed with RoPE should always prioritize semantically similar tokens. In this work, we unify these seemingly distinct objectives through a minimalist intervention, namely CoPE: soft clipping lowfrequency components of RoPE. CoPE not only eliminates OOD outliers and refines semantic signals, but also prevents spectral leakage caused by hard clipping. Extensive experiments demonstrate that simply applying our soft clipping strategy to RoPE yields significant performance gains that scale up to 256k context length, validating our theoretical analysis and establishing CoPE as a new state-of-the-art for length generalization. Our code, data, and models are available at https://github.com/hrlics/CoPE.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCoPE\uff0c\u901a\u8fc7\u5bf9RoPE\u4f4e\u9891\u6210\u5206\u8f6f\u622a\u65ad\uff0c\u6709\u6548\u63d0\u5347\u4e86\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86256k\u957f\u5ea6\u4e0a\u4e0b\u6587\u7684\u6027\u80fd\u7a81\u7834\uff0c\u6210\u4e3a\u65b0\u7684\u957f\u6587\u672c\u5efa\u6a21\u6280\u672f\u6807\u6746\u3002", "motivation": "\u73b0\u6709\u7684RoPE\u9002\u5e94\u957f\u4e0a\u4e0b\u6587\u7684\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u5728\u4e24\u65b9\u9762\uff0c\u4e00\u662f\u7f13\u89e3\u8d85\u51fa\u8bad\u7ec3\u5206\u5e03\u7684\u9891\u7387\u95ee\u9898\uff0c\u4e8c\u662f\u786e\u4fdd\u5173\u6ce8\u673a\u5236\u4f18\u5148\u8003\u8651\u8bed\u4e49\u76f8\u4f3c\u7684token\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4e00\u5b9a\u5c40\u9650\uff0c\u9700\u627e\u5230\u7b80\u6d01\u6709\u6548\u7684\u7edf\u4e00\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u4e86CoPE\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9RoPE\u4e2d\u4f4e\u9891\u6210\u5206\u8fdb\u884c\u8f6f\u622a\u65ad\uff0c\u7edf\u4e00\u4e86\u9488\u5bf9\u957f\u4e0a\u4e0b\u6587\u7684OOD\u7f13\u89e3\u4e0e\u8bed\u4e49\u5efa\u6a21\u4e24\u5927\u76ee\u6807\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u7406\u8bba\u6709\u6548\u6027\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u5b9e\u9a8c\uff0cCoPE\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u8d85\u957f\u4e0a\u4e0b\u6587\uff08\u6700\u957f\u8fbe256k\uff09\u4e0a\u7684\u6027\u80fd\u8868\u73b0\uff0c\u9a8c\u8bc1\u4e86\u7406\u8bba\u5206\u6790\u7684\u6709\u6548\u6027\uff0c\u8fbe\u5230\u4e86\u957f\u5ea6\u6cdb\u5316\u7684\u6700\u65b0\u6c34\u5e73\u3002", "conclusion": "CoPE\u4f5c\u4e3a\u4e00\u79cd\u5bf9Rotary Positional Embedding\uff08RoPE\uff09\u7684\u8f6f\u622a\u65ad\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587\u4e2d\u7684\u5f02\u5e38\u503c\u95ee\u9898\uff0c\u4f18\u5316\u4e86\u8bed\u4e49\u4fe1\u53f7\uff0c\u9632\u6b62\u4e86\u786c\u622a\u65ad\u5f15\u8d77\u7684\u9891\u8c31\u6cc4\u6f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5904\u7406\u957f\u8fbe256k\u4e0a\u4e0b\u6587\u957f\u5ea6\u7684\u80fd\u529b\uff0c\u6210\u4e3a\u957f\u5ea6\u6cdb\u5316\u7684\u65b0\u72b6\u6001\u3002"}}
{"id": "2602.05465", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.05465", "abs": "https://arxiv.org/abs/2602.05465", "authors": ["Alexander Berndt", "Vekil Bekmyradov", "Rainer Gemulla", "Marcus Kessel", "Thomas Bach", "Sebastian Baltes"], "title": "Can We Classify Flaky Tests Using Only Test Code? An LLM-Based Empirical Study", "comment": "10 pages, 3 figures, 7 tables, 33rd IEEE International Conference on Software Analysis, Evolution and Reengineering: Reproducibility Studies and Negative Results (SANER-RENE 2025)", "summary": "Flaky tests yield inconsistent results when they are repeatedly executed on the same code revision. They interfere with automated quality assurance of code changes and hinder efficient software testing. Previous work evaluated approaches to train machine learning models to classify flaky tests based on identifiers in the test code. However, the resulting classifiers have been shown to lack generalizability, hindering their applicability in practical environments. Recently, pre-trained Large Language Models (LLMs) have shown the capability to generalize across various tasks. Thus, they represent a promising approach to address the generalizability problem of previous approaches. In this study, we evaluated three LLMs (two general-purpose models, one code-specific model) using three prompting techniques on two benchmark datasets from prior studies on flaky test classification. Furthermore, we manually investigated 50 samples from the given datasets to determine whether classifying flaky tests based only on test code is feasible for humans. Our findings indicate that LLMs struggle to classify flaky tests given only the test code. The results of our best prompt-model combination were only marginally better than random guessing. In our manual analysis, we found that the test code does not necessarily contain sufficient information for a flakiness classification. Our findings motivate future work to evaluate LLMs for flakiness classification with additional context, for example, using retrieval-augmented generation or agentic AI.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u4ec5\u51ed\u6d4b\u8bd5\u4ee3\u7801\u8bc6\u522b\u6d4b\u8bd5\u4e0d\u7a33\u5b9a\u6027\uff0c\u53d1\u73b0\u6548\u679c\u4e0d\u4f73\uff0c\u63d0\u793a\u672a\u6765\u9700\u7ed3\u5408\u66f4\u591a\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6d4b\u8bd5\u4ee3\u7801\u6807\u8bc6\u7b26\u7684\u6a21\u578b\u7f3a\u4e4f\u6cdb\u5316\u80fd\u529b\uff0c\u96be\u4ee5\u5b9e\u7528\u3002\u5927\u8bed\u8a00\u6a21\u578b\u5177\u5907\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\uff0c\u53ef\u80fd\u89e3\u51b3\u8be5\u95ee\u9898\u3002", "method": "\u8bc4\u4f30\u4e09\u79cd\u5927\u8bed\u8a00\u6a21\u578b\uff08\u4e24\u4e2a\u901a\u7528\u6a21\u578b\uff0c\u4e00\u4e2a\u4ee3\u7801\u4e13\u7528\u6a21\u578b\uff09\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u91c7\u7528\u4e09\u79cd\u63d0\u793a\u6280\u672f\uff1b\u540c\u65f6\u624b\u52a8\u5206\u679050\u4e2a\u6837\u672c\u4ee5\u5224\u65ad\u4ec5\u51ed\u6d4b\u8bd5\u4ee3\u7801\u5206\u7c7b\u7684\u53ef\u884c\u6027\u3002", "result": "\u5927\u8bed\u8a00\u6a21\u578b\u4ec5\u51ed\u6d4b\u8bd5\u4ee3\u7801\u5206\u7c7b\u6d4b\u8bd5\u4e0d\u7a33\u5b9a\u6027\u6548\u679c\u63a5\u8fd1\u968f\u673a\u731c\u6d4b\uff0c\u6d4b\u8bd5\u4ee3\u7801\u4fe1\u606f\u4e0d\u8db3\u4ee5\u652f\u6301\u5206\u7c7b\u3002", "conclusion": "\u4ec5\u57fa\u4e8e\u6d4b\u8bd5\u4ee3\u7801\u7684\u4fe1\u606f\u4e0d\u8db3\u4ee5\u6709\u6548\u5206\u7c7b\u6d4b\u8bd5\u4e0d\u7a33\u5b9a\u6027\uff0c\u672a\u6765\u53ef\u7ed3\u5408\u989d\u5916\u4e0a\u4e0b\u6587\u6216\u9ad8\u7ea7\u65b9\u6cd5\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2602.05261", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05261", "abs": "https://arxiv.org/abs/2602.05261", "authors": ["Fanfan Liu", "Youyang Yin", "Peng Shi", "Siqi Yang", "Zhixiong Zeng", "Haibo Qiu"], "title": "Length-Unbiased Sequence Policy Optimization: Revealing and Controlling Response Length Variation in RLVR", "comment": null, "summary": "Recent applications of Reinforcement Learning with Verifiable Rewards (RLVR) to Large Language Models (LLMs) and Vision-Language Models (VLMs) have demonstrated significant success in enhancing reasoning capabilities for complex tasks. During RLVR training, an increase in response length is often regarded as a key factor contributing to the growth of reasoning ability. However, the patterns of change in response length vary significantly across different RLVR algorithms during the training process. To provide a fundamental explanation for these variations, this paper conducts an in-depth analysis of the components of mainstream RLVR algorithms. We present a theoretical analysis of the factors influencing response length and validate our theory through extensive experimentation. Building upon these theoretical findings, we propose the Length-Unbiased Sequence Policy Optimization (LUSPO) algorithm. Specifically, we rectify the length bias inherent in Group Sequence Policy Optimization (GSPO), rendering its loss function unbiased with respect to response length and thereby resolving the issue of response length collapse. We conduct extensive experiments across mathematical reasoning benchmarks and multimodal reasoning scenarios, where LUSPO consistently achieves superior performance. Empirical results demonstrate that LUSPO represents a novel, state-of-the-art optimization strategy compared to existing methods such as GRPO and GSPO.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86RLVR\u7b97\u6cd5\u4e2d\u54cd\u5e94\u957f\u5ea6\u53d8\u5316\u7684\u539f\u56e0\uff0c\u63d0\u51fa\u4e86\u957f\u5ea6\u65e0\u504f\u7b56\u7565\u4f18\u5316\u7b97\u6cd5LUSPO\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "motivation": "\u89c2\u5bdf\u5230\u4e0d\u540cRLVR\u7b97\u6cd5\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u54cd\u5e94\u957f\u5ea6\u53d8\u5316\u6a21\u5f0f\u5dee\u5f02\u663e\u8457\uff0c\u4e14\u54cd\u5e94\u957f\u5ea6\u7684\u589e\u957f\u88ab\u89c6\u4e3a\u63a8\u7406\u80fd\u529b\u589e\u5f3a\u7684\u91cd\u8981\u56e0\u7d20\uff0c\u56e0\u6b64\u5e0c\u671b\u7cfb\u7edf\u5206\u6790\u54cd\u5e94\u957f\u5ea6\u53d8\u5316\u7684\u5185\u5728\u539f\u56e0\u5e76\u63d0\u51fa\u6539\u8fdb\u7b56\u7565\u3002", "method": "\u901a\u8fc7\u5bf9\u4e3b\u6d41RLVR\u7b97\u6cd5\u7ec4\u6210\u90e8\u5206\u7684\u6df1\u5165\u7406\u8bba\u5206\u6790\uff0c\u63d0\u51fa\u957f\u5ea6\u65e0\u504f\u5e8f\u5217\u7b56\u7565\u4f18\u5316\u7b97\u6cd5LUSPO\uff0c\u4fee\u6b63\u4e86GSPO\u7b97\u6cd5\u4e2d\u54cd\u5e94\u957f\u5ea6\u7684\u504f\u5dee\uff0c\u4f7f\u635f\u5931\u51fd\u6570\u5bf9\u5e94\u957f\u5ea6\u65e0\u504f\uff1b\u5e76\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u548c\u591a\u6a21\u6001\u573a\u666f\u4e2d\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLUSPO\u7b97\u6cd5\u4e0d\u4ec5\u89e3\u51b3\u4e86\u54cd\u5e94\u957f\u5ea6\u5d29\u6e83\u95ee\u9898\uff0c\u8fd8\u5728\u6570\u5b66\u63a8\u7406\u548c\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u4e2d\u6301\u7eed\u5b9e\u73b0\u6bd4GRPO\u548cGSPO\u66f4\u4f18\u7684\u6027\u80fd\uff0c\u8fbe\u5230\u4e86\u6700\u65b0\u7684\u4f18\u5316\u6c34\u5e73\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684LUSPO\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709RLVR\u7b97\u6cd5\u4e2d\u54cd\u5e94\u957f\u5ea6\u7684\u504f\u5dee\u95ee\u9898\uff0c\u9632\u6b62\u4e86\u54cd\u5e94\u957f\u5ea6\u7684\u5d29\u6e83\u73b0\u8c61\uff0c\u5728\u6570\u5b66\u63a8\u7406\u548c\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8fbe\u5230\u6700\u65b0\u7684\u4f18\u5316\u6548\u679c\u3002"}}
{"id": "2602.05486", "categories": ["cs.SE", "cs.AI", "cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.05486", "abs": "https://arxiv.org/abs/2602.05486", "authors": ["Matteo Esposito", "Lodovica Marchesi", "Roberto Tonelli", "Valentina Lenarduzzi"], "title": "Sovereign-by-Design A Reference Architecture for AI and Blockchain Enabled Systems", "comment": null, "summary": "Digital sovereignty has emerged as a central concern for modern software-intensive systems, driven by the dominance of non-sovereign cloud infrastructures, the rapid adoption of Generative AI, and increasingly stringent regulatory requirements. While existing initiatives address governance, compliance, and security in isolation, they provide limited guidance on how sovereignty can be operationalized at the architectural level. In this paper, we argue that sovereignty must be treated as a first-class architectural property rather than a purely regulatory objective. We introduce a Sovereign Reference Architecture that integrates self-sovereign identity, blockchain-based trust and auditability, sovereign data governance, and Generative AI deployed under explicit architectural control. The architecture explicitly captures the dual role of Generative AI as both a source of governance risk and an enabler of compliance, accountability, and continuous assurance when properly constrained. By framing sovereignty as an architectural quality attribute, our work bridges regulatory intent and concrete system design, offering a coherent foundation for building auditable, evolvable, and jurisdiction-aware AI-enabled systems. The proposed reference architecture provides a principled starting point for future research and practice at the intersection of software architecture, Generative AI, and digital sovereignty.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u4e3b\u6743\u4e3a\u6838\u5fc3\u67b6\u6784\u5c5e\u6027\u7684\u53c2\u8003\u67b6\u6784\uff0c\u5c06\u751f\u6210\u5f0fAI\u4e0e\u533a\u5757\u94fe\u3001\u81ea\u6211\u4e3b\u6743\u8eab\u4efd\u7b49\u6280\u672f\u6574\u5408\uff0c\u4ee5\u5b9e\u73b0\u6570\u5b57\u4e3b\u6743\u7684\u53ef\u64cd\u4f5c\u5316\uff0c\u4e3a\u6784\u5efa\u5408\u89c4\u4e14\u53ef\u5ba1\u8ba1\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\u3002", "motivation": "\u6570\u5b57\u4e3b\u6743\u6210\u4e3a\u73b0\u4ee3\u8f6f\u4ef6\u7cfb\u7edf\u7684\u6838\u5fc3\u5173\u6ce8\u70b9\uff0c\u73b0\u6709\u7684\u6cbb\u7406\u548c\u5408\u89c4\u63aa\u65bd\u4e0d\u80fd\u5168\u9762\u4f53\u73b0\u67b6\u6784\u5c42\u9762\u7684\u4e3b\u6743\u8981\u6c42\u3002", "method": "\u63d0\u51fa\u4e3b\u6743\u53c2\u8003\u67b6\u6784\uff0c\u6574\u5408\u81ea\u6211\u4e3b\u6743\u8eab\u4efd\u3001\u533a\u5757\u94fe\u4fe1\u4efb\u4e0e\u5ba1\u8ba1\u3001\u4e3b\u6743\u6570\u636e\u6cbb\u7406\u548c\u53d7\u63a7\u7684\u751f\u6210\u5f0fAI\u3002", "result": "\u5f00\u53d1\u51fa\u4e86\u4e00\u4e2a\u878d\u5408\u591a\u9879\u5173\u952e\u6280\u672f\uff0c\u5b9e\u73b0\u4e3b\u6743\u6027\u548c\u5408\u89c4\u6027\u7684\u53c2\u8003\u67b6\u6784\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u4e0e\u5b9e\u8df5\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u57fa\u7840\u3002", "conclusion": "\u6570\u5b57\u4e3b\u6743\u5e94\u4f5c\u4e3a\u7b2c\u4e00\u7c7b\u67b6\u6784\u5c5e\u6027\u88ab\u786e\u7acb\uff0c\u8be5\u67b6\u6784\u6865\u63a5\u4e86\u76d1\u7ba1\u610f\u56fe\u4e0e\u5177\u4f53\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u652f\u6301\u53ef\u5ba1\u6838\u3001\u53ef\u8fdb\u5316\u548c\u5177\u5907\u53f8\u6cd5\u7ba1\u8f96\u611f\u77e5\u7684AI\u7cfb\u7edf\u6784\u5efa\u3002"}}
{"id": "2602.05523", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05523", "abs": "https://arxiv.org/abs/2602.05523", "authors": ["Shahin Honarvar", "Amber Gorzynski", "James Lee-Jones", "Harry Coppock", "Marek Rei", "Joseph Ryan", "Alastair F. Donaldson"], "title": "Capture the Flags: Family-Based Evaluation of Agentic LLMs via Semantics-Preserving Transformations", "comment": null, "summary": "Agentic large language models (LLMs) are increasingly evaluated on cybersecurity tasks using capture-the-flag (CTF) benchmarks. However, existing pointwise benchmarks have limited ability to shed light on the robustness and generalisation abilities of agents across alternative versions of the source code. We introduce CTF challenge families, whereby a single CTF is used as the basis for generating a family of semantically-equivalent challenges via semantics-preserving program transformations. This enables controlled evaluation of agent robustness to source code transformations while keeping the underlying exploit strategy fixed. We introduce a new tool, Evolve-CTF, that generates CTF families from Python challenges using a range of transformations. Using Evolve-CTF to derive families from Cybench and Intercode challenges, we evaluate 13 agentic LLM configurations with tool access. We find that models are remarkably robust to intrusive renaming and code insertion-based transformations, but that composed transformations and deeper obfuscation affect performance by requiring more sophisticated use of tools. We also find that enabling explicit reasoning has little effect on solution success rates across challenge families. Our work contributes a valuable technique and tool for future LLM evaluations, and a large dataset characterising the capabilities of current state-of-the-art models in this domain.", "AI": {"tldr": "\u672c\u6587\u8bbe\u8ba1\u4e86\u4e00\u79cd\u751f\u6210\u8bed\u4e49\u7b49\u4ef7CTF\u6311\u6218\u5bb6\u65cf\u7684\u65b9\u6cd5\u548c\u5de5\u5177Evolve-CTF\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u4ee3\u7801\u53d8\u6362\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u6a21\u578b\u5bf9\u7b80\u5355\u53d8\u6362\u8f83\u7a33\u5065\uff0c\u4f46\u590d\u6742\u53d8\u6362\u5f71\u54cd\u660e\u663e\uff0c\u663e\u5f0f\u63a8\u7406\u4f5c\u7528\u4e0d\u5927\u3002", "motivation": "\u5f53\u524d\u7684\u5355\u70b9CTF\u57fa\u51c6\u96be\u4ee5\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6e90\u4ee3\u7801\u591a\u7248\u672c\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u63a7\u5236\u6e90\u4ee3\u7801\u53d8\u6362\u540c\u65f6\u4fdd\u6301\u653b\u51fb\u7b56\u7565\u4e0d\u53d8\u7684\u8bc4\u6d4b\u65b9\u6cd5\u3002", "method": "\u63d0\u51faEvolve-CTF\u5de5\u5177\uff0c\u57fa\u4e8ePython\u4ee3\u7801\u901a\u8fc7\u8bed\u4e49\u4fdd\u6301\u7684\u7a0b\u5e8f\u53d8\u6362\u751f\u6210\u591a\u6837\u7684CTF\u6311\u6218\u5bb6\u65cf\uff0c\u5229\u7528\u8fd9\u4e9b\u5bb6\u65cf\u5bf913\u79cd\u5177\u5907\u5de5\u5177\u8bbf\u95ee\u80fd\u529b\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\u3002", "result": "\u4f7f\u7528Evolve-CTF\u4eceCybench\u548cIntercode\u4e2d\u884d\u751fCTF\u5bb6\u65cf\uff0c\u5bf913\u79cd\u5177\u5907\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u7684\u4ee3\u7406\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8bc4\u6d4b\uff0c\u53d1\u73b0\u6a21\u578b\u5bf9\u7b80\u5355\u53d8\u6362\u9c81\u68d2\uff0c\u4f46\u590d\u6742\u7ec4\u5408\u53d8\u6362\u548c\u6df1\u5ea6\u6df7\u6dc6\u964d\u4f4e\u6027\u80fd\uff0c\u663e\u793a\u5de5\u5177\u4f7f\u7528\u9700\u6c42\u66f4\u9ad8\uff0c\u663e\u5f0f\u63a8\u7406\u5bf9\u6210\u529f\u7387\u5f71\u54cd\u6709\u9650\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u4fdd\u6301\u7684\u7a0b\u5e8f\u53d8\u6362\u751f\u6210CTF\u6311\u6218\u5bb6\u65cf\u7684\u65b0\u65b9\u6cd5\uff0c\u4ece\u800c\u80fd\u66f4\u597d\u5730\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7f51\u7edc\u5b89\u5168\u4efb\u52a1\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u5b9e\u9a8c\u8868\u660e\u6a21\u578b\u5bf9\u7b80\u5355\u91cd\u547d\u540d\u548c\u4ee3\u7801\u63d2\u5165\u7b49\u53d8\u6362\u8f83\u4e3a\u9c81\u68d2\uff0c\u4f46\u590d\u6742\u53d8\u6362\u548c\u6df1\u5ea6\u6df7\u6dc6\u5bf9\u6027\u80fd\u5f71\u54cd\u8f83\u5927\uff0c\u663e\u793a\u9700\u8981\u66f4\u590d\u6742\u7684\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u3002\u663e\u5f0f\u63a8\u7406\u80fd\u529b\u5bf9\u89e3\u51b3\u7387\u5f71\u54cd\u4e0d\u5927\u3002"}}
{"id": "2602.05307", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05307", "abs": "https://arxiv.org/abs/2602.05307", "authors": ["Haojin Wang", "Yike Wang", "Shangbin Feng", "Hannaneh Hajishirzi", "Yulia Tsvetkov"], "title": "MentorCollab: Selective Large-to-Small Inference-Time Guidance for Efficient Reasoning", "comment": null, "summary": "Large reasoning models (LRMs) achieve strong performance by producing long chains of thought, but their inference costs are high and often generate redundant reasoning. Small language models (SLMs) are far more efficient, yet struggle on multi-step reasoning tasks. A natural idea is to let a large model guide a small one at inference time as a mentor, yet existing collaboration methods often promote imitation, resulting in verbose reasoning without consistent error correction. We propose MentorCollab, an inference-time collaboration method in which an LRM selectively and sparsely guides an SLM, rather than taking over generation. At randomly sampled token positions, we probe for divergences between the two models and use a lightweight verifier to decide whether the SLM should follow a short lookahead segment from its mentor or continue on its own. Across 15 SLM--LRM pairs and 3 domains (math reasoning, general knowledge, and commonsense reasoning), our method improves performance in 12 settings, with average gains of 3.0% and up to 8.0%, while adopting only having 18.4% tokens generated by the expensive mentor model on average. We find that short segments and selective probing are sufficient for effective collaboration. Our results show that selective inference-time guidance restores large-model reasoning ability without substantial inference overhead.", "AI": {"tldr": "\u63d0\u51fa\u4e86MentorCollab\u63a8\u7406\u65f6\u534f\u4f5c\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u6307\u5bfc\u5c0f\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\u548c\u63a8\u7406\u6548\u7387\u7684\u53cc\u8d62\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u867d\u6027\u80fd\u5f3a\u4f46\u63a8\u7406\u6210\u672c\u9ad8\u4e14\u63a8\u7406\u5197\u4f59\uff0c\u5c0f\u578b\u6a21\u578b\u6548\u7387\u9ad8\u4f46\u591a\u6b65\u63a8\u7406\u8868\u73b0\u5f31\u3002\u5e0c\u671b\u901a\u8fc7\u5927\u578b\u6a21\u578b\u5728\u63a8\u7406\u65f6\u6307\u5bfc\u5c0f\u578b\u6a21\u578b\u4ee5\u517c\u987e\u6548\u7387\u548c\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86MentorCollab\u65b9\u6cd5\uff0c\u5728\u63a8\u7406\u65f6\u968f\u673a\u91c7\u6837token\u4f4d\u7f6e\u63a2\u6d4bSLM\u4e0eLRM\u7684\u5206\u6b67\uff0c\u5229\u7528\u8f7b\u91cf\u7ea7\u9a8c\u8bc1\u5668\u51b3\u5b9aSLM\u662f\u5426\u91c7\u7eb3LRM\u7684\u77ed\u671f\u524d\u77bb\u751f\u6210\uff0c\u907f\u514d\u5168\u7a0b\u6a21\u4eff\u3002", "result": "\u572815\u5bf9SLM-LRM\u7ec4\u5408\u4ee5\u53ca\u6570\u5b66\u63a8\u7406\u3001\u901a\u7528\u77e5\u8bc6\u3001\u5e38\u8bc6\u63a8\u74063\u4e2a\u9886\u57df\u4e2d\uff0c\u65b9\u6cd5\u572812\u79cd\u8bbe\u7f6e\u4e0b\u63d0\u5347\u6027\u80fd\uff0c\u5e73\u5747\u63d0\u53473.0%\uff0c\u6700\u9ad88.0%\uff0c\u4e14\u4ec5\u970018.4%token\u7531\u6602\u8d35\u7684LRM\u751f\u6210\u3002", "conclusion": "\u901a\u8fc7\u9009\u62e9\u6027\u548c\u7a00\u758f\u5730\u8ba9\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRM\uff09\u6307\u5bfc\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\uff0c\u53ef\u4ee5\u5728\u7ef4\u6301\u63a8\u7406\u80fd\u529b\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u63a8\u7406\u6210\u672c\u3002"}}
{"id": "2602.05550", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05550", "abs": "https://arxiv.org/abs/2602.05550", "authors": ["Yulong He", "Artem Ermakov", "Sergey Kovalchuk", "Artem Aliev", "Dmitry Shalymov"], "title": "ArkTS-CodeSearch: A Open-Source ArkTS Dataset for Code Retrieval", "comment": null, "summary": "ArkTS is a core programming language in the OpenHarmony ecosystem, yet research on ArkTS code intelligence is hindered by the lack of public datasets and evaluation benchmarks. This paper presents a large-scale ArkTS dataset constructed from open-source repositories, targeting code retrieval and code evaluation tasks. We design a single-search task, where natural language comments are used to retrieve corresponding ArkTS functions. ArkTS repositories are crawled from GitHub and Gitee, and comment-function pairs are extracted using tree-sitter-arkts, followed by cross-platform deduplication and statistical analysis of ArkTS function types. We further evaluate all existing open-source code embedding models on the single-search task and perform fine-tuning using both ArkTS and TypeScript training datasets, resulting in a high-performing model for ArkTS code understanding. This work establishes the first systematic benchmark for ArkTS code retrieval. Both the dataset and our fine-tuned model will be released publicly and are available at https://huggingface.co/hreyulog/embedinggemma_arkts and https://huggingface.co/datasets/hreyulog/arkts-code-docstring,establishing the first systematic benchmark for ArkTS code retrieval.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u7f3a\u4e4f\u6570\u636e\u96c6\u7684ArkTS\u8bed\u8a00\uff0c\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u4ee3\u7801\u4e0e\u6ce8\u91ca\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u4ee3\u7801\u68c0\u7d22\u4efb\u52a1\uff0c\u8bc4\u6d4b\u5e76\u5fae\u8c03\u6a21\u578b\uff0c\u9996\u6b21\u5efa\u7acb\u7cfb\u7edf\u6027\u57fa\u51c6\u5e76\u516c\u5f00\u6570\u636e\u8d44\u6e90\u3002", "motivation": "ArkTS\u4f5c\u4e3aOpenHarmony\u751f\u6001\u6838\u5fc3\u7f16\u7a0b\u8bed\u8a00\uff0c\u7f3a\u4e4f\u516c\u5f00\u6570\u636e\u96c6\u4e0e\u8bc4\u6d4b\u57fa\u51c6\uff0c\u5236\u7ea6\u4e86\u4ee3\u7801\u667a\u80fd\u76f8\u5173\u7814\u7a76\u7684\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u4eceGitHub\u548cGitee\u722c\u53d6ArkTS\u4ee3\u7801\u5e93\uff0c\u5229\u7528tree-sitter-arkts\u63d0\u53d6\u6ce8\u91ca\u4e0e\u51fd\u6570\u5bf9\uff0c\u8bbe\u8ba1\u5355\u641c\u7d22\u4efb\u52a1\u8fdb\u884c\u4ee3\u7801\u68c0\u7d22\u8bc4\u4f30\uff1b\u5bf9\u73b0\u6709\u5f00\u6e90\u4ee3\u7801\u5d4c\u5165\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u7ed3\u5408ArkTS\u4e0eTypeScript\u6570\u636e\u96c6\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u6784\u5efa\u4e86\u5927\u578bArkTS\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u4e86\u5bf9\u4ee3\u7801\u68c0\u7d22\u6709\u6307\u5bfc\u610f\u4e49\u7684\u4efb\u52a1\uff0c\u8bc4\u6d4b\u5e76\u5fae\u8c03\u6a21\u578b\u540e\u83b7\u5f97\u9ad8\u6027\u80fdArkTS\u4ee3\u7801\u7406\u89e3\u6a21\u578b\uff0c\u6570\u636e\u96c6\u4e0e\u6a21\u578b\u5c06\u516c\u5f00\u53d1\u5e03\u3002", "conclusion": "\u672c\u6587\u5efa\u7acb\u4e86\u9996\u4e2a\u7cfb\u7edf\u6027\u7684ArkTS\u4ee3\u7801\u68c0\u7d22\u57fa\u51c6\uff0c\u663e\u8457\u63a8\u52a8\u4e86ArkTS\u4ee3\u7801\u667a\u80fd\u7814\u7a76\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.05347", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05347", "abs": "https://arxiv.org/abs/2602.05347", "authors": ["Soma Sato", "Ryohei Sasano"], "title": "How Do Language Models Acquire Character-Level Information?", "comment": "Accepted to EACL 2026 Main Conference", "summary": "Language models (LMs) have been reported to implicitly encode character-level information, despite not being explicitly provided during training. However, the mechanisms underlying this phenomenon remain largely unexplored. To reveal the mechanisms, we analyze how models acquire character-level knowledge by comparing LMs trained under controlled settings, such as specifying the pre-training dataset or tokenizer, with those trained under standard settings. We categorize the contributing factors into those independent of tokenization. Our analysis reveals that merge rules and orthographic constraints constitute primary factors arising from tokenization, whereas semantic associations of substrings and syntactic information function as key factors independent of tokenization.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5bf9\u6bd4\u4e0d\u540c\u8bad\u7ec3\u8bbe\u7f6e\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u83b7\u5f97\u5b57\u7b26\u7ea7\u77e5\u8bc6\u7684\u673a\u5236\uff0c\u5305\u62ec\u5206\u8bcd\u89c4\u5219\u548c\u8bed\u4e49\u53e5\u6cd5\u56e0\u7d20\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u5728\u6ca1\u6709\u663e\u5f0f\u5b57\u7b26\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\u80fd\u591f\u9690\u5f0f\u7f16\u7801\u5b57\u7b26\u7ea7\u77e5\u8bc6\uff0c\u5176\u673a\u5236\u5c1a\u4e0d\u6e05\u695a\uff0c\u7814\u7a76\u65e8\u5728\u63ed\u793a\u8fd9\u4e9b\u673a\u5236\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u5728\u4e0d\u540c\u53d7\u63a7\u6761\u4ef6\uff08\u5982\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u6216\u5206\u8bcd\u5668\uff09\u548c\u6807\u51c6\u6761\u4ef6\u4e0b\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u5206\u6790\u6a21\u578b\u5982\u4f55\u83b7\u5f97\u5b57\u7b26\u7ea7\u77e5\u8bc6\u3002", "result": "\u53d1\u73b0\u5206\u8bcd\u76f8\u5173\u7684\u5408\u5e76\u89c4\u5219\u548c\u6b63\u5b57\u6cd5\u7ea6\u675f\u662f\u4e3b\u8981\u5f71\u54cd\u56e0\u7d20\uff0c\u6b64\u5916\u5b50\u5b57\u7b26\u4e32\u7684\u8bed\u4e49\u5173\u8054\u548c\u53e5\u6cd5\u4fe1\u606f\u4e5f\u662f\u5173\u952e\u56e0\u7d20\uff0c\u4e14\u4e0e\u5206\u8bcd\u65e0\u5173\u3002", "conclusion": "\u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u867d\u7136\u672a\u663e\u5f0f\u5b66\u4e60\u5b57\u7b26\u7ea7\u4fe1\u606f\uff0c\u4f46\u901a\u8fc7\u5408\u5e76\u89c4\u5219\u3001\u6b63\u5b57\u6cd5\u7ea6\u675f\u3001\u5b50\u5b57\u7b26\u4e32\u7684\u8bed\u4e49\u5173\u8054\u548c\u53e5\u6cd5\u4fe1\u606f\u7b49\u673a\u5236\u9690\u5f0f\u83b7\u5f97\u5b57\u7b26\u7ea7\u77e5\u8bc6\u3002"}}
{"id": "2602.05703", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.05703", "abs": "https://arxiv.org/abs/2602.05703", "authors": ["Tom\u00e1\u0161 Brablec", "Tom\u00e1\u0161 Dac\u00edk", "Tom\u00e1\u0161 Vojnar"], "title": "SEAL: Symbolic Execution with Separation Logic (Competition Contribution)", "comment": "4 pages, accepted to SV-COMP 2026", "summary": "SEAL is a static analyser for the verification of programs that manipulate unbounded linked data structures. It is based on separation logic to represent abstract memory states and, unlike other separation-logic-based approaches, it employs a general-purpose separation logic solver Astral for satisfiability and entailment checking, which itself is based on translation to SMT. This design results in a modular architecture intended to be easier to extend and to combine with reasoning in other theories. Although still a prototype, SEAL achieved competitive results in the LinkedLists base category and was one of only four analysers capable of verifying programs with unbounded lists. We believe that the tool's extensibility, combined with further development, can lead to significant improvements in future competitions.", "AI": {"tldr": "SEAL\u662f\u4e00\u4e2a\u5229\u7528\u5206\u79bb\u903b\u8f91\u548cSMT\u6280\u672f\u9a8c\u8bc1\u65e0\u754c\u94fe\u8868\u7a0b\u5e8f\u7684\u9759\u6001\u5206\u6790\u5de5\u5177\uff0c\u8868\u73b0\u4f18\u5f02\u4e14\u7ed3\u6784\u6a21\u5757\u5316\uff0c\u5177\u6709\u826f\u597d\u7684\u6269\u5c55\u6f5c\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5206\u79bb\u903b\u8f91\u7684\u65b9\u6cd5\u5728\u5904\u7406\u94fe\u8868\u7b49\u65e0\u754c\u6570\u636e\u7ed3\u6784\u7a0b\u5e8f\u9a8c\u8bc1\u65f6\u5b58\u5728\u5c40\u9650\uff0c\u5bfb\u6c42\u4e00\u79cd\u66f4\u901a\u7528\u3001\u6613\u6269\u5c55\u4e14\u53ef\u7ec4\u5408\u5176\u4ed6\u7406\u8bba\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5229\u7528\u5206\u79bb\u903b\u8f91\u8868\u793a\u62bd\u8c61\u5185\u5b58\u72b6\u6001\uff0c\u91c7\u7528\u57fa\u4e8eSMT\u7684\u901a\u7528\u5206\u79bb\u903b\u8f91\u6c42\u89e3\u5668Astral\u8fdb\u884c\u53ef\u6ee1\u8db3\u6027\u548c\u8574\u542b\u6027\u68c0\u67e5\uff0c\u5f62\u6210\u6a21\u5757\u5316\u67b6\u6784\u3002", "result": "SEAL\u4f5c\u4e3a\u539f\u578b\u5b9e\u73b0\uff0c\u5728LinkedLists\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u79c0\uff0c\u662f\u5c11\u6570\u80fd\u591f\u9a8c\u8bc1\u65e0\u754c\u94fe\u8868\u7a0b\u5e8f\u7684\u5206\u6790\u5668\u4e4b\u4e00\u3002", "conclusion": "SEAL\u4f5c\u4e3a\u4e00\u79cd\u57fa\u4e8e\u5206\u79bb\u903b\u8f91\u548cSMT\u6c42\u89e3\u5668\u7684\u9759\u6001\u5206\u6790\u5de5\u5177\uff0c\u5728\u9a8c\u8bc1\u64cd\u4f5c\u65e0\u754c\u94fe\u8868\u7684\u7a0b\u5e8f\u4e2d\u5c55\u73b0\u51fa\u7ade\u4e89\u529b\uff0c\u5e76\u5177\u5907\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2602.05370", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05370", "abs": "https://arxiv.org/abs/2602.05370", "authors": ["Jun Rao", "Zixiong Yu", "Xuebo Liu", "Guhan Chen", "Jing Li", "Jiansheng Wei", "Xiaojun Meng", "Min Zhang"], "title": "PACE: Defying the Scaling Hypothesis of Exploration in Iterative Alignment for Mathematical Reasoning", "comment": null, "summary": "Iterative Direct Preference Optimization has emerged as the state-of-the-art paradigm for aligning Large Language Models on reasoning tasks. Standard implementations (DPO-R1) rely on Best-of-N sampling (e.g., $N \\ge 8$) to mine golden trajectories from the distribution tail. In this paper, we challenge this scaling hypothesis and reveal a counter-intuitive phenomenon: in mathematical reasoning, aggressive exploration yields diminishing returns and even catastrophic policy collapse. We theoretically demonstrate that scaling $N$ amplifies verifier noise and induces detrimental distribution shifts. To resolve this, we introduce \\textbf{PACE} (Proximal Alignment via Corrective Exploration), which replaces brute-force mining with a generation-based corrective strategy. Operating with a minimal budget ($2<N<3$), PACE synthesizes high-fidelity preference pairs from failed explorations. Empirical evaluations show that PACE outperforms DPO-R1 $(N=16)$ while using only about $1/5$ of the compute, demonstrating superior robustness against reward hacking and label noise.", "AI": {"tldr": "\u4f5c\u8005\u53d1\u73b0\u4f20\u7edf\u57fa\u4e8e\u5927\u91cf\u91c7\u6837\u7684DPO\u65b9\u6cd5\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u53d7\u9650\uff0c\u63d0\u51faPACE\u901a\u8fc7\u7ea0\u6b63\u6027\u63a2\u7d22\u751f\u6210\u9ad8\u8d28\u91cf\u504f\u597d\u6837\u672c\uff0c\u5b9e\u73b0\u66f4\u5c11\u8ba1\u7b97\u91cf\u4e0b\u66f4\u4f18\u6027\u80fd\u548c\u66f4\u9ad8\u7a33\u5065\u6027\u3002", "motivation": "\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\uff0c\u4f20\u7edfDPO\u901a\u8fc7\u5927\u89c4\u6a21\u6700\u4f73\u6837\u672c\u6316\u6398\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u4f5c\u8005\u53d1\u73b0\u8fd9\u4e00\u7b56\u7565\u5e26\u6765\u566a\u58f0\u653e\u5927\u548c\u5206\u5e03\u504f\u79fb\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u548c\u7b56\u7565\u5d29\u6e83\uff0c\u4e9f\u9700\u65b0\u65b9\u6cd5\u89e3\u51b3\u8be5\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86PACE\uff08Proximal Alignment via Corrective Exploration\uff09\u65b9\u6cd5\uff0c\u5229\u7528\u751f\u6210\u5f0f\u7ea0\u6b63\u7b56\u7565\u4ece\u5931\u8d25\u63a2\u7d22\u4e2d\u5408\u6210\u9ad8\u8d28\u91cf\u7684\u504f\u597d\u6837\u672c\uff0c\u907f\u514d\u4e86\u4f20\u7edfDPO-R1\u4e2d\u5927\u89c4\u6a21\u6700\u4f73\u6837\u672c\u91c7\u6837\u5e26\u6765\u7684\u566a\u58f0\u548c\u5206\u5e03\u504f\u79fb\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPACE\u5728\u91c7\u6837\u6570\u91cf\u4ec5\u4e3aDPO-R1\u76841/5\u5de6\u53f3\u7684\u8ba1\u7b97\u9884\u7b97\u4e0b\uff0c\u6027\u80fd\u4f18\u4e8eDPO-R1\uff08N=16\uff09\uff0c\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6297\u566a\u58f0\u548c\u7a33\u5065\u6027\u3002", "conclusion": "\u672c\u6587\u6311\u6218\u4e86\u8fed\u4ee3\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u4e2d\u901a\u8fc7\u589e\u52a0\u91c7\u6837\u6570\u91cf\u4ee5\u63d0\u5347\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u8868\u73b0\u7684\u5047\u8bbe\uff0c\u53d1\u73b0\u8fc7\u5ea6\u63a2\u7d22\u4e0d\u4ec5\u6536\u76ca\u9012\u51cf\uff0c\u8fd8\u53ef\u80fd\u5bfc\u81f4\u7b56\u7565\u5d29\u6e83\u3002\u901a\u8fc7\u5f15\u5165PACE\u65b9\u6cd5\uff0c\u5229\u7528\u751f\u6210\u5f0f\u7ea0\u6b63\u7b56\u7565\u4ee3\u66ff\u66b4\u529b\u91c7\u6837\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u4e14\u66f4\u7a33\u5065\u7684\u6a21\u578b\u5bf9\u9f50\u3002"}}
{"id": "2602.05712", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05712", "abs": "https://arxiv.org/abs/2602.05712", "authors": ["Lola Solovyeva", "Fernando Castor"], "title": "Towards Green AI: Decoding the Energy of LLM Inference in Software Development", "comment": null, "summary": "Context: AI-assisted tools are increasingly integrated into software development workflows, but their reliance on large language models (LLMs) introduces substantial computational and energy costs. Understanding and reducing the energy footprint of LLM inference is therefore essential for sustainable software development. Objective: In this study, we conduct a phase-level analysis of LLM inference energy consumption, distinguishing between the (1) prefill, where the model processes the input and builds internal representations, and (2) decoding, where output tokens are generated using the stored state. Method: We investigate six 6B-7B and four 3B-4B transformer-based models, evaluating them on code-centric benchmarks HumanEval for code generation and LongBench for code understanding. Results: Our findings show that, within both parameter groups, models exhibit distinct energy patterns across phases. Furthermore, we observed that increases in prefill cost amplify the energy cost per token during decoding, with amplifications ranging from 1.3% to 51.8% depending on the model. Lastly, three out of ten models demonstrate babbling behavior, adding excessive content to the output that unnecessarily inflates energy consumption. We implemented babbling suppression for code generation, achieving energy savings ranging from 44% to 89% without affecting generation accuracy. Conclusion: These findings show that prefill costs influence decoding, which dominates energy consumption, and that babbling suppression can yield up to 89% energy savings. Reducing inference energy therefore requires both mitigating babbling behavior and limiting impact of prefill on decoding.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u9636\u6bb5\u6027\u5206\u6790LLM\u63a8\u7406\u80fd\u8017\uff0c\u53d1\u73b0\u9884\u586b\u5145\u9636\u6bb5\u548c\u5570\u55e6\u884c\u4e3a\u5bf9\u89e3\u7801\u9636\u6bb5\u80fd\u8017\u6709\u663e\u8457\u5f71\u54cd\uff0c\u63d0\u51fa\u5570\u55e6\u6291\u5236\u65b9\u6cd5\u80fd\u5927\u5e45\u964d\u4f4e\u80fd\u8017\uff0c\u52a9\u529b\u7eff\u8272\u8f6f\u4ef6\u5f00\u53d1\u3002", "motivation": "AI\u8f85\u52a9\u5de5\u5177\u5e7f\u6cdb\u5e94\u7528\u4e8e\u8f6f\u4ef6\u5f00\u53d1\uff0c\u4f46\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u7684\u63a8\u7406\u8fc7\u7a0b\u8ba1\u7b97\u548c\u80fd\u8017\u5de8\u5927\uff0c\u4e9f\u9700\u7406\u89e3\u548c\u51cf\u5c11\u80fd\u8017\u4ee5\u5b9e\u73b0\u53ef\u6301\u7eed\u53d1\u5c55\u3002", "method": "\u672c\u7814\u7a76\u5bf9LLM\u63a8\u7406\u80fd\u8017\u8fdb\u884c\u9636\u6bb5\u6027\u5206\u6790\uff0c\u533a\u5206\u9884\u586b\u5145(prefill)\u9636\u6bb5\u548c\u89e3\u7801(decoding)\u9636\u6bb5\uff0c\u4f7f\u7528\u516d\u4e2a6B-7B\u548c\u56db\u4e2a3B-4B\u53c2\u6570\u89c4\u6a21\u7684\u53d8\u6362\u5668\u6a21\u578b\uff0c\u5728\u4ee3\u7801\u751f\u6210(HumanEval)\u548c\u4ee3\u7801\u7406\u89e3(LongBench)\u57fa\u51c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u53d1\u73b0\u4e0d\u540c\u6a21\u578b\u5728\u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u80fd\u8017\u6a21\u5f0f\uff0c\u9884\u586b\u5145\u80fd\u8017\u7684\u589e\u52a0\u4f1a\u653e\u5927\u89e3\u7801\u9636\u6bb5\u6bcf\u4e2atoken\u7684\u80fd\u8017\uff0c\u5e45\u5ea6\u4e3a1.3%\u81f351.8%\u3002\u4e09\u5206\u4e4b\u4e00\u6a21\u578b\u5b58\u5728\u65e0\u610f\u4e49\u201c\u5570\u55e6\u201d\u8f93\u51fa\uff0c\u5bfc\u81f4\u989d\u5916\u80fd\u8017\u3002\u901a\u8fc7\u6291\u5236\u5570\u55e6\u884c\u4e3a\uff0c\u80fd\u5728\u4e0d\u5f71\u54cd\u51c6\u786e\u7387\u60c5\u51b5\u4e0b\u8282\u770144%\u81f389%\u7684\u80fd\u8017\u3002", "conclusion": "\u9884\u586b\u5145\u9636\u6bb5\u7684\u80fd\u8017\u5f71\u54cd\u4e3b\u5bfc\u80fd\u8017\u7684\u89e3\u7801\u9636\u6bb5\uff0c\u5570\u55e6\u884c\u4e3a\u663e\u8457\u589e\u52a0\u80fd\u8017\u3002\u8981\u964d\u4f4e\u63a8\u7406\u80fd\u8017\uff0c\u9700\u6291\u5236\u5570\u55e6\u5e76\u51cf\u5c0f\u9884\u586b\u5145\u5bf9\u89e3\u7801\u7684\u5f71\u54cd\u3002"}}
{"id": "2602.05374", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.05374", "abs": "https://arxiv.org/abs/2602.05374", "authors": ["Chaimae Abouzahir", "Congbo Ma", "Nizar Habash", "Farah E. Shamout"], "title": "Cross-Lingual Empirical Evaluation of Large Language Models for Arabic Medical Tasks", "comment": "Accepted to HeaLing-EACL 2026", "summary": "In recent years, Large Language Models (LLMs) have become widely used in medical applications, such as clinical decision support, medical education, and medical question answering. Yet, these models are often English-centric, limiting their robustness and reliability for linguistically diverse communities. Recent work has highlighted discrepancies in performance in low-resource languages for various medical tasks, but the underlying causes remain poorly understood. In this study, we conduct a cross-lingual empirical analysis of LLM performance on Arabic and English medical question and answering. Our findings reveal a persistent language-driven performance gap that intensifies with increasing task complexity. Tokenization analysis exposes structural fragmentation in Arabic medical text, while reliability analysis suggests that model-reported confidence and explanations exhibit limited correlation with correctness. Together, these findings underscore the need for language-aware design and evaluation strategies in LLMs for medical tasks.", "AI": {"tldr": "\u7814\u7a76\u63ed\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u963f\u62c9\u4f2f\u8bed\u533b\u7597\u95ee\u7b54\u4e2d\u7684\u6027\u80fd\u4e0d\u8db3\uff0c\u5f3a\u8c03\u4e86\u8bbe\u8ba1\u548c\u8bc4\u4f30\u65f6\u9700\u5173\u6ce8\u8bed\u8a00\u7279\u6027\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u591a\u4ee5\u82f1\u8bed\u4e3a\u4e2d\u5fc3\uff0c\u5bfc\u81f4\u5176\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u533b\u7597\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u4e14\u539f\u56e0\u672a\u88ab\u5145\u5206\u7406\u89e3\u3002", "method": "\u901a\u8fc7\u8de8\u8bed\u8a00\u5b9e\u8bc1\u5206\u6790\u6bd4\u8f83\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u963f\u62c9\u4f2f\u8bed\u548c\u82f1\u8bed\u533b\u7597\u95ee\u7b54\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u7ed3\u5408\u5206\u8bcd\u7ed3\u6784\u548c\u6a21\u578b\u53ef\u9760\u6027\u5206\u6790\u3002", "result": "\u53d1\u73b0\u963f\u62c9\u4f2f\u8bed\u533b\u7597\u6587\u672c\u5b58\u5728\u7ed3\u6784\u788e\u7247\u5316\u73b0\u8c61\uff0c\u6a21\u578b\u5728\u963f\u62c9\u4f2f\u8bed\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u4f4e\u4e8e\u82f1\u8bed\uff0c\u4e14\u6a21\u578b\u7f6e\u4fe1\u5ea6\u4e0e\u5b9e\u9645\u6b63\u786e\u7387\u76f8\u5173\u6027\u5f31\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u4efb\u52a1\u4e2d\u5b58\u5728\u663e\u8457\u7684\u8bed\u8a00\u9a71\u52a8\u6027\u80fd\u5dee\u5f02\uff0c\u5c24\u5176\u5728\u5904\u7406\u590d\u6742\u4efb\u52a1\u65f6\u963f\u62c9\u4f2f\u8bed\u8868\u73b0\u8f83\u5dee\uff0c\u6a21\u578b\u7684\u7f6e\u4fe1\u5ea6\u548c\u89e3\u91ca\u4e0e\u51c6\u786e\u6027\u5173\u8054\u6709\u9650\u3002"}}
{"id": "2602.05721", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.05721", "abs": "https://arxiv.org/abs/2602.05721", "authors": ["Bin Liu", "Yanjie Zhao", "Zhenpeng Chen", "Guoai Xu", "Haoyu Wang"], "title": "A Dual-Loop Agent Framework for Automated Vulnerability Reproduction", "comment": null, "summary": "Automated vulnerability reproduction from CVE descriptions requires generating executable Proof-of-Concept (PoC) exploits and validating them in target environments. This process is critical in software security research and practice, yet remains time-consuming and demands specialized expertise when performed manually. While LLM agents show promise for automating this task, existing approaches often conflate exploring attack directions with fixing implementation details, which leads to unproductive debugging loops when reproduction fails. To address this, we propose Cve2PoC, an LLM-based dual-loop agent framework following a plan-execute-evaluate paradigm. The Strategic Planner analyzes vulnerability semantics and target code to produce structured attack plans. The Tactical Executor generates PoC code and validates it through progressive verification. The Adaptive Refiner evaluates execution results and routes failures to different loops: the \\textit{Tactical Loop} for code-level refinement, while the \\textit{Strategic Loop} for attack strategy replanning. This dual-loop design enables the framework to escape ineffective debugging by matching remediation to failure type. Evaluation on two benchmarks covering 617 real-world vulnerabilities demonstrates that Cve2PoC achieves 82.9\\% and 54.3\\% reproduction success rates on SecBench.js and PatchEval, respectively, outperforming the best baseline by 11.3\\% and 20.4\\%. Human evaluation confirms that generated PoCs achieve comparable code quality to human-written exploits in readability and reusability.", "AI": {"tldr": "\u63d0\u51faCve2PoC\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u53cc\u5faa\u73af\u6f0f\u6d1e\u590d\u73b0\u6846\u67b6\uff0c\u901a\u8fc7\u6218\u7565\u89c4\u5212\u548c\u6218\u672f\u6267\u884c\u5206\u79bb\uff0c\u63d0\u5347\u6f0f\u6d1e\u5229\u7528\u4ee3\u7801\u81ea\u52a8\u751f\u6210\u7684\u6210\u529f\u7387\u548c\u8d28\u91cf\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u81ea\u52a8\u4eceCVE\u63cf\u8ff0\u4e2d\u751f\u6210\u548c\u9a8c\u8bc1\u6f0f\u6d1e\u5229\u7528\u4ee3\u7801\uff0c\u4ee5\u89e3\u51b3\u624b\u5de5\u590d\u73b0\u6f0f\u6d1e\u8017\u65f6\u4e14\u9700\u8981\u4e13\u4e1a\u6280\u80fd\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faCve2PoC\uff0c\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u53cc\u5faa\u73af\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u91c7\u7528\u8ba1\u5212-\u6267\u884c-\u8bc4\u4f30\u8303\u5f0f\uff1a\u6218\u7565\u89c4\u5212\u8005\u5206\u6790\u6f0f\u6d1e\u548c\u76ee\u6807\u4ee3\u7801\u751f\u6210\u653b\u51fb\u8ba1\u5212\uff1b\u6218\u672f\u6267\u884c\u8005\u751f\u6210PoC\u4ee3\u7801\u5e76\u8fdb\u884c\u5206\u9636\u6bb5\u9a8c\u8bc1\uff1b\u81ea\u9002\u5e94\u4f18\u5316\u5668\u6839\u636e\u6267\u884c\u7ed3\u679c\u5c06\u5931\u8d25\u5206\u522b\u4ea4\u7ed9\u6218\u672f\u5faa\u73af\uff08\u4ee3\u7801\u5c42\u9762\u4fee\u6b63\uff09\u6216\u6218\u7565\u5faa\u73af\uff08\u653b\u51fb\u7b56\u7565\u91cd\u89c4\u5212\uff09\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff08SecBench.js\u548cPatchEval\uff09\u4e0a\uff0c\u5bf9617\u4e2a\u771f\u5b9e\u6f0f\u6d1e\u8fdb\u884c\u8bc4\u4f30\uff0c\u590d\u73b0\u6210\u529f\u7387\u5206\u522b\u8fbe\u523082.9%\u548c54.3%\uff0c\u5206\u522b\u6bd4\u6700\u4f73\u57fa\u7ebf\u63d0\u534711.3%\u548c20.4%\u3002\u4eba\u5de5\u8bc4\u4f30\u8868\u660e\u751f\u6210\u7684PoC\u4ee3\u7801\u5728\u53ef\u8bfb\u6027\u548c\u53ef\u590d\u7528\u6027\u4e0a\u4e0e\u4eba\u5de5\u7f16\u5199\u7684\u5229\u7528\u4ee3\u7801\u76f8\u5f53\u3002", "conclusion": "Cve2PoC\u901a\u8fc7\u53cc\u5faa\u73af\u8bbe\u8ba1\u6709\u6548\u533a\u5206\u4e0d\u540c\u5931\u8d25\u539f\u56e0\uff0c\u907f\u514d\u65e0\u6548\u8c03\u8bd5\u5faa\u73af\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u81ea\u52a8\u6f0f\u6d1e\u590d\u73b0\u7684\u6210\u529f\u7387\u548c\u4ee3\u7801\u8d28\u91cf\uff0c\u5c55\u793a\u51fa\u5728\u8f6f\u4ef6\u5b89\u5168\u81ea\u52a8\u5316\u4e2d\u7684\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2602.05385", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05385", "abs": "https://arxiv.org/abs/2602.05385", "authors": ["Tao Liu", "Jiafan Lu", "Bohan Yu", "Pengcheng Wu", "Liu Haixin", "Guoyu Xu", "Li Xiangheng", "Lixiao Li", "Jiaming Hou", "Zhao Shijun", "Xinglin Lyu", "Kunli Zhang", "Yuxiang Jia", "Hongyin Zan"], "title": "IESR:Efficient MCTS-Based Modular Reasoning for Text-to-SQL with Large Language Models", "comment": "25 pages, 16 figures, 8 tables. Hongyin Zan is corresponding author, Jiafan Lu is first co-author", "summary": "Text-to-SQL is a key natural language processing task that maps natural language questions to SQL queries, enabling intuitive interaction with web-based databases. Although current methods perform well on benchmarks like BIRD and Spider, they struggle with complex reasoning, domain knowledge, and hypothetical queries, and remain costly in enterprise deployment. To address these issues, we propose a framework named IESR(Information Enhanced Structured Reasoning) for lightweight large language models: (i) leverages LLMs for key information understanding and schema linking, and decoupling mathematical computation and SQL generation, (ii) integrates a multi-path reasoning mechanism based on Monte Carlo Tree Search (MCTS) with majority voting, and (iii) introduces a trajectory consistency verification module with a discriminator model to ensure accuracy and consistency. Experimental results demonstrate that IESR achieves state-of-the-art performance on the complex reasoning benchmark LogicCat (24.28 EX) and the Archer dataset (37.28 EX) using only compact lightweight models without fine-tuning. Furthermore, our analysis reveals that current coder models exhibit notable biases and deficiencies in physical knowledge, mathematical computation, and common-sense reasoning, highlighting important directions for future research. We released code at https://github.com/Ffunkytao/IESR-SLM.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7684IESR\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u8f7b\u91cf\u7ea7\u8bed\u8a00\u6a21\u578b\u3001\u591a\u8def\u5f84\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u548c\u4e00\u81f4\u6027\u9a8c\u8bc1\uff0c\u5b9e\u73b0\u4e86\u590d\u6742\u6587\u672c\u5230SQL\u8f6c\u6362\u4efb\u52a1\u7684\u6700\u4f73\u6027\u80fd\uff0c\u9002\u5408\u5b9e\u9645\u4f01\u4e1a\u5e94\u7528\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230SQL\u65b9\u6cd5\u867d\u5728\u6807\u51c6\u6570\u636e\u96c6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5bf9\u590d\u6742\u63a8\u7406\u3001\u9886\u57df\u77e5\u8bc6\u548c\u5047\u8bbe\u6027\u67e5\u8be2\u652f\u6301\u4e0d\u8db3\uff0c\u4e14\u4f01\u4e1a\u90e8\u7f72\u6210\u672c\u9ad8\uff0c\u9700\u8f7b\u91cf\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faIESR\u6846\u67b6\uff0c\u5305\u62ec\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5173\u952e\u4fe1\u606f\u7406\u89e3\u548c\u6a21\u5f0f\u94fe\u63a5\uff0c\u4f7f\u7528\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u8fdb\u884c\u591a\u8def\u5f84\u63a8\u7406\u5e76\u91c7\u7528\u591a\u6570\u6295\u7968\u673a\u5236\uff0c\u7ed3\u5408\u8f68\u8ff9\u4e00\u81f4\u6027\u9a8c\u8bc1\u6a21\u5757\u4fdd\u969c\u7ed3\u679c\u51c6\u786e\u6027\u3002", "result": "\u5728LogicCat\u548cArcher\u8fd9\u4e24\u4e2a\u590d\u6742\u63a8\u7406\u6570\u636e\u5e93\u4e0a\uff0cIESR\u5206\u522b\u8fbe\u5230\u4e8624.28\u548c37.28\u7684EX\u6307\u6807\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u5fae\u8c03\u7684\u9ad8\u6548\u8868\u73b0\u3002", "conclusion": "IESR\u6846\u67b6\u5728\u590d\u6742\u63a8\u7406\u6587\u672c\u5230SQL\u8f6c\u6362\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u65e0\u9700\u5fae\u8c03\uff0c\u63d0\u5347\u4e86\u5b9e\u7528\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2602.05739", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.05739", "abs": "https://arxiv.org/abs/2602.05739", "authors": ["Nazanin Siavash", "Armin Moin"], "title": "A Bayesian Optimization-Based AutoML Framework for Non-Intrusive Load Monitoring", "comment": null, "summary": "Non-Intrusive Load Monitoring (NILM), commonly known as energy disaggregation, aims to estimate the power consumption of individual appliances by analyzing a home's total electricity usage. This method provides a cost-effective alternative to installing dedicated smart meters for each appliance. In this paper, we introduce a novel framework that incorporates Automated Machine Learning (AutoML) into the NILM domain, utilizing Bayesian Optimization for automated model selection and hyperparameter tuning. This framework empowers domain practitioners to effectively apply machine learning techniques without requiring advanced expertise in data science or machine learning. To support further research and industry adoption, we present AutoML4NILM, a flexible and extensible open-source toolkit designed to streamline the deployment of AutoML solutions for energy disaggregation. Currently, this framework supports 11 algorithms, each with different hyperparameters; however, its flexible design allows for the extension of both the algorithms and their hyperparameters.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408AutoML\u548c\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u975e\u4fb5\u5165\u5f0f\u8d1f\u8f7d\u76d1\u6d4b\u6846\u67b6\uff0c\u5f00\u53d1\u4e86\u517c\u5bb9\u591a\u7b97\u6cd5\u7684\u5f00\u6e90\u5de5\u5177AutoML4NILM\uff0c\u4fbf\u4e8e\u9886\u57df\u4e13\u5bb6\u65e0\u9700\u6df1\u5165\u673a\u5668\u5b66\u4e60\u5373\u53ef\u5e94\u7528\u80fd\u8017\u5206\u89e3\u6280\u672f\u3002", "motivation": "\u4f20\u7edfNILM\u65b9\u6cd5\u4f9d\u8d56\u4e13\u4e1a\u77e5\u8bc6\u8fdb\u884c\u6a21\u578b\u8bbe\u8ba1\u548c\u8c03\u53c2\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff1b\u81ea\u52a8\u5316\u65b9\u6cd5\u53ef\u964d\u4f4e\u95e8\u69db\uff0c\u63a8\u52a8\u7814\u7a76\u4e0e\u4ea7\u4e1a\u754c\u7684\u5e94\u7528\u3002", "method": "\u5f15\u5165\u81ea\u52a8\u673a\u5668\u5b66\u4e60\uff08AutoML\uff09\u6280\u672f\u7ed3\u5408\u8d1d\u53f6\u65af\u4f18\u5316\uff0c\u81ea\u52a8\u5b8c\u6210\u6a21\u578b\u9009\u62e9\u548c\u8d85\u53c2\u6570\u8c03\u4f18\uff0c\u5f00\u53d1\u4e86\u652f\u6301\u591a\u79cd\u7b97\u6cd5\u7684AutoML4NILM\u5f00\u6e90\u5de5\u5177\u5305\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u4e00\u4e2a\u5305\u542b11\u79cd\u7b97\u6cd5\u7684\u7075\u6d3b\u53ef\u6269\u5c55\u5f00\u6e90\u5de5\u5177\u5305AutoML4NILM\uff0c\u652f\u6301\u81ea\u52a8\u5316\u6a21\u578b\u9009\u62e9\u4e0e\u8c03\u4f18\uff0c\u4fc3\u8fdb\u4e86\u80fd\u8017\u5206\u6790\u7814\u7a76\u548c\u5e94\u7528\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u96c6\u6210\u81ea\u52a8\u673a\u5668\u5b66\u4e60\u7684NILM\u6846\u67b6\uff0c\u5229\u7528\u8d1d\u53f6\u65af\u4f18\u5316\u5b9e\u73b0\u4e86\u6a21\u578b\u81ea\u52a8\u9009\u62e9\u4e0e\u8d85\u53c2\u6570\u8c03\u4f18\uff0c\u964d\u4f4e\u4e86\u9886\u57df\u4e13\u5bb6\u7684\u5e94\u7528\u95e8\u69db\uff0c\u63d0\u9ad8\u4e86\u80fd\u8017\u5206\u89e3\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2602.05392", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05392", "abs": "https://arxiv.org/abs/2602.05392", "authors": ["Jiyun Chun", "Eric Fosler-Lussier", "Michael White", "Andrew Perrault"], "title": "Beyond Length: Context-Aware Expansion and Independence as Developmentally Sensitive Evaluation in Child Utterances", "comment": null, "summary": "Evaluating the quality of children's utterances in adult-child dialogue remains challenging due to insufficient context-sensitive metrics. Common proxies such as Mean Length of Utterance (MLU), lexical diversity (vocd-D), and readability indices (Flesch-Kincaid Grade Level, Gunning Fog Index) are dominated by length and ignore conversational context, missing aspects of response quality such as reasoning depth, topic maintenance, and discourse planning. We introduce an LLM-as-a-judge framework that first classifies the Previous Adult Utterance Type and then scores the child's response along two axes: Expansion (contextual elaboration and inferential depth) and Independence (the child's contribution to advancing the discourse). These axes reflect fundamental dimensions in child language development, where Expansion captures elaboration, clause combining, and causal and contrastive connectives. Independence captures initiative, topic control, decreasing reliance on adult scaffolding through growing self-regulation, and audience design. We establish developmental validity by showing age-related patterns and demonstrate predictive value by improving age estimation over common baselines. We further confirm semantic sensitivity by detecting differences tied to discourse relations. Our metrics align with human judgments, enabling large-scale evaluation. This shifts child utterance assessment from simply measuring length to evaluating how meaningfully the child's speech contributes to and advances the conversation within its context.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u513f\u7ae5\u8bdd\u8bed\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u7a81\u7834\u4f20\u7edf\u957f\u5ea6\u5bfc\u5411\u6307\u6807\uff0c\u5173\u6ce8\u8bdd\u8bed\u7684\u6269\u5c55\u6027\u4e0e\u72ec\u7acb\u6027\uff0c\u66f4\u5168\u9762\u5730\u53cd\u6620\u513f\u7ae5\u8bed\u8a00\u53d1\u5c55\u53ca\u5176\u5728\u5bf9\u8bdd\u4e2d\u7684\u8d21\u732e\u3002", "motivation": "\u73b0\u6709\u7684\u513f\u7ae5\u8bdd\u8bed\u8d28\u91cf\u8bc4\u4ef7\u6307\u6807\u5982\u5e73\u5747\u8bdd\u8bed\u957f\u5ea6\u3001\u8bcd\u6c47\u591a\u6837\u6027\u548c\u53ef\u8bfb\u6027\u6307\u6570\u8fc7\u4e8e\u4f9d\u8d56\u957f\u5ea6\uff0c\u5ffd\u7565\u4e86\u5bf9\u8bdd\u7684\u8bed\u5883\u548c\u8bdd\u8bed\u529f\u80fd\uff0c\u65e0\u6cd5\u5168\u9762\u53cd\u6620\u513f\u7ae5\u8bed\u8a00\u8868\u8fbe\u7684\u6df1\u5ea6\u548c\u72ec\u7acb\u6027\uff0c\u56e0\u6b64\u9700\u8981\u5f15\u5165\u66f4\u5177\u8bed\u5883\u654f\u611f\u6027\u7684\u65b0\u8bc4\u4ef7\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u8bc4\u5224\u8005\uff0c\u5148\u5206\u7c7b\u6210\u4eba\u524d\u4e00\u53e5\u8bdd\u7684\u7c7b\u578b\uff0c\u518d\u6839\u636e\u6269\u5c55\u6027\uff08\u4e0a\u4e0b\u6587\u8be6\u7ec6\u7a0b\u5ea6\u548c\u63a8\u7406\u6df1\u5ea6\uff09\u4e0e\u72ec\u7acb\u6027\uff08\u513f\u7ae5\u63a8\u52a8\u8bdd\u9898\u53d1\u5c55\u7684\u80fd\u529b\uff09\u4e24\u4e2a\u8f74\u5bf9\u513f\u7ae5\u56de\u5e94\u8fdb\u884c\u8bc4\u5206\u3002\u7ed3\u5408\u53d1\u5c55\u5fc3\u7406\u5b66\u7406\u8bba\uff0c\u8bbe\u8ba1\u4e86\u80fd\u6355\u6349\u8bed\u6cd5\u3001\u63a8\u7406\u548c\u8bdd\u9898\u63a7\u5236\u7684\u8bc4\u4ef7\u6307\u6807\u3002", "result": "\u8be5\u6846\u67b6\u5c55\u73b0\u51fa\u826f\u597d\u7684\u53d1\u5c55\u6548\u5ea6\u548c\u8bed\u4e49\u654f\u611f\u6027\uff0c\u8bc4\u4ef7\u6307\u6807\u80fd\u53cd\u6620\u5e74\u9f84\u76f8\u5173\u7684\u8bed\u8a00\u53d1\u5c55\u89c4\u5f8b\uff0c\u589e\u5f3a\u4e86\u5e74\u9f84\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u51c6\u786e\u6355\u6349\u4e86\u8bdd\u8bed\u95f4\u7684\u8bed\u4e49\u5173\u7cfb\u53d8\u5316\uff0c\u6307\u6807\u4e0e\u4eba\u7c7b\u8bc4\u4ef7\u9ad8\u5ea6\u4e00\u81f4\uff0c\u652f\u6301\u5927\u89c4\u6a21\u513f\u7ae5\u8bdd\u8bed\u8d28\u91cf\u7684\u81ea\u52a8\u8bc4\u4f30\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u5224\u6846\u67b6\uff0c\u6709\u6548\u8bc4\u4f30\u513f\u7ae5\u5728\u4e0e\u6210\u4eba\u5bf9\u8bdd\u4e2d\u8bdd\u8bed\u7684\u8d28\u91cf\uff0c\u5f25\u8865\u4e86\u4f20\u7edf\u6307\u6807\u5ffd\u89c6\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u7684\u4e0d\u8db3\u3002\u8be5\u6846\u67b6\u7efc\u5408\u8003\u8651\u8bdd\u8bed\u7684\u6269\u5c55\u6027\u548c\u72ec\u7acb\u6027\uff0c\u53cd\u6620\u513f\u7ae5\u8bed\u8a00\u53d1\u5c55\u7684\u5173\u952e\u7ef4\u5ea6\uff0c\u5e76\u4e0e\u5e74\u9f84\u76f8\u5173\u6027\u548c\u8bed\u4e49\u654f\u611f\u6027\u76f8\u7b26\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u513f\u7ae5\u5e74\u9f84\u548c\u8bc4\u4ef7\u8bdd\u8bed\u8d28\u91cf\u3002"}}
{"id": "2602.05759", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.05759", "abs": "https://arxiv.org/abs/2602.05759", "authors": ["Lei Zhang"], "title": "Toward Quantum-Safe Software Engineering: A Vision for Post-Quantum Cryptography Migration", "comment": "2 pages, 1 figure, accepted by 2026 IEEE/ACM 48th International Conference on Software Engineering (ICSE'26 Poster Track)", "summary": "The quantum threat to cybersecurity has accelerated the standardization of Post-Quantum Cryptography (PQC). Migrating legacy software to these quantum-safe algorithms is not a simple library swap, but a new software engineering challenge: existing vulnerability detection, refactoring, and testing tools are not designed for PQC's probabilistic behavior, side-channel sensitivity, and complex performance trade-offs. To address these challenges, this paper outlines a vision for a new class of tools and introduces the Automated Quantum-safe Adaptation (AQuA) framework, with a three-pillar agenda for PQC-aware detection, semantic refactoring, and hybrid verification, thereby motivating Quantum-Safe Software Engineering (QSSE) as a distinct research direction.", "AI": {"tldr": "\u9488\u5bf9\u540e\u91cf\u5b50\u5bc6\u7801\u5b66\u8fc1\u79fb\u6311\u6218\uff0c\u63d0\u51faAQuA\u6846\u67b6\uff0c\u5f00\u542f\u91cf\u5b50\u5b89\u5168\u8f6f\u4ef6\u5de5\u7a0b\u65b0\u9886\u57df\u3002", "motivation": "\u91cf\u5b50\u8ba1\u7b97\u5bf9\u73b0\u6709\u7f51\u7edc\u5b89\u5168\u6784\u6210\u5a01\u80c1\uff0c\u73b0\u6709\u5de5\u5177\u4e0d\u9002\u7528\u4e8e\u540e\u91cf\u5b50\u5bc6\u7801\u6280\u672f\uff0c\u9700\u6c42\u65b0\u7684\u8f6f\u4ef6\u5de5\u7a0b\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u81ea\u52a8\u91cf\u5b50\u5b89\u5168\u9002\u914d\u6846\u67b6\uff08AQuA\uff09\uff0c\u5305\u62ecPQC\u611f\u77e5\u68c0\u6d4b\u3001\u8bed\u4e49\u91cd\u6784\u548c\u6df7\u5408\u9a8c\u8bc1\u4e09\u5927\u652f\u67f1\u3002", "result": "\u6784\u60f3\u4e86\u4e00\u5957\u9762\u5411\u540e\u91cf\u5b50\u5bc6\u7801\u5b66\u7684\u8f6f\u4ef6\u5de5\u5177\u6846\u67b6\uff0c\u63a8\u52a8\u91cf\u5b50\u5b89\u5168\u8f6f\u4ef6\u5de5\u7a0b\u6210\u4e3a\u72ec\u7acb\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u8fc1\u79fb\u5230\u540e\u91cf\u5b50\u5bc6\u7801\u5b66\u8f6f\u4ef6\u5de5\u7a0b\u9700\u8981\u4e13\u95e8\u7684\u5de5\u5177\u652f\u6301\uff0c\u4ee5\u89e3\u51b3\u5176\u6982\u7387\u6027\u884c\u4e3a\u3001\u4fa7\u4fe1\u9053\u654f\u611f\u6027\u53ca\u6027\u80fd\u6743\u8861\u7b49\u72ec\u7279\u6311\u6218\u3002"}}
{"id": "2602.05393", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.05393", "abs": "https://arxiv.org/abs/2602.05393", "authors": ["Ji Zhao", "Yufei Gu", "Shitong Shao", "Xun Zhou", "Liang Xiang", "Zeke Xie"], "title": "Late-to-Early Training: LET LLMs Learn Earlier, So Faster and Better", "comment": null, "summary": "As Large Language Models (LLMs) achieve remarkable empirical success through scaling model and data size, pretraining has become increasingly critical yet computationally prohibitive, hindering rapid development. Despite the availability of numerous pretrained LLMs developed at significant computational expense, a fundamental real-world question remains underexplored: \\textit{Can we leverage existing small pretrained models to accelerate the training of larger models?} In this paper, we propose a Late-to-Early Training (LET) paradigm that enables LLMs to explicitly learn later knowledge in earlier steps and earlier layers. The core idea is to guide the early layers of an LLM during early training using representations from the late layers of a pretrained (i.e. late training phase) model. We identify two key mechanisms that drive LET's effectiveness: late-to-early-step learning and late-to-early-layer learning. These mechanisms significantly accelerate training convergence while robustly enhancing both language modeling capabilities and downstream task performance, enabling faster training with superior performance. Extensive experiments on 1.4B and 7B parameter models demonstrate LET's efficiency and effectiveness. Notably, when training a 1.4B LLM on the Pile dataset, our method achieves up to 1.6$\\times$ speedup with nearly 5\\% improvement in downstream task accuracy compared to standard training, even when using a pretrained model with 10$\\times$ fewer parameters than the target model.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cdLate-to-Early Training\u65b9\u6cd5\uff0c\u901a\u8fc7\u7528\u5c0f\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u540e\u671f\u5c42\u8868\u5f81\u5f15\u5bfc\u5927\u6a21\u578b\u65e9\u671f\u5c42\u8bad\u7ec3\uff0c\u663e\u8457\u52a0\u901f\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u901f\u5ea6\uff0c\u5e76\u63d0\u5347\u4e86\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u8017\u65f6\u957f\u4e14\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u9ad8\uff0c\u9650\u5236\u4e86\u5176\u5feb\u901f\u53d1\u5c55\uff0c\u5c1d\u8bd5\u5229\u7528\u5c0f\u578b\u5df2\u9884\u8bad\u7ec3\u6a21\u578b\u52a0\u901f\u66f4\u5927\u6a21\u578b\u7684\u8bad\u7ec3\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faLate-to-Early Training (LET)\u8303\u5f0f\uff0c\u901a\u8fc7\u5728\u65e9\u671f\u8bad\u7ec3\u9636\u6bb5\u5229\u7528\u5df2\u9884\u8bad\u7ec3\u6a21\u578b\u540e\u671f\u5c42\u7684\u8868\u5f81\u6765\u6307\u5bfc\u76ee\u6807\u6a21\u578b\u7684\u65e9\u671f\u5c42\uff0c\u4ee5\u5b9e\u73b0\u77e5\u8bc6\u7684\u65e9\u4f20\u9012\u3002\u901a\u8fc7\u665a\u671f\u5230\u65e9\u671f\u6b65\u9aa4\u5b66\u4e60\u548c\u665a\u671f\u5230\u65e9\u671f\u5c42\u5b66\u4e60\u4e24\u5927\u673a\u5236\u52a0\u901f\u6536\u655b\u3002", "result": "\u57281.4B\u548c7B\u53c2\u6570\u89c4\u6a21\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0c\u663e\u793aLET\u65b9\u6cd5\u76f8\u6bd4\u6807\u51c6\u8bad\u7ec3\u5728PILE\u6570\u636e\u96c6\u4e0a\u8bad\u7ec31.4B\u6a21\u578b\u901f\u5ea6\u63d0\u53471.6\u500d\uff0c\u4e0b\u6e38\u4efb\u52a1\u51c6\u786e\u7387\u63d0\u5347\u7ea65%\u3002\u5373\u4fbf\u4f7f\u7528\u53c2\u6570\u91cf\u53ea\u6709\u76ee\u6807\u6a21\u578b\u5341\u5206\u4e4b\u4e00\u7684\u5c0f\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u4ea6\u8868\u73b0\u826f\u597d\u3002", "conclusion": "LET\u8303\u5f0f\u6709\u6548\u5229\u7528\u5c0f\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u52a0\u901f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\uff0c\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u5927\u6a21\u578b\u9884\u8bad\u7ec3\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2602.05780", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05780", "abs": "https://arxiv.org/abs/2602.05780", "authors": ["Ulrich Finkler", "Irene Manotas", "Wei Zhang", "Geert Janssen", "Octavian Popescu", "Shyam Ramji"], "title": "Automated Customization of LLMs for Enterprise Code Repositories Using Semantic Scopes", "comment": null, "summary": "Code completion (CC) is a task frequently used by developers when working in collaboration with LLM-based programming assistants. Despite the increased performance of LLMs on public benchmarks, out of the box LLMs still have a hard time generating code that aligns with a private code repository not previously seen by the model's training data. Customizing code LLMs to a private repository provides a way to improve the model performance. In this paper we present our approach for automated LLM customization based on semantic scopes in the code. We evaluate LLMs on real industry cases with two private enterprise code repositories with two customization strategies: Retrieval-Augmented Generation (RAG) and supervised Fine-Tuning (FT). Our mechanism for ingesting the repository's data and formulating the training data pairs with semantic scopes helps models to learn the underlying patterns specific to the repository, providing more precise code to developers and helping to boost their productivity. The code completions of moderately sized customized models can be significantly better than those of uncustomized models of much larger capacity. We also include an analysis of customization on two public benchmarks and present opportunities for future work.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u4f5c\u7528\u57df\u7684\u81ea\u52a8\u5316\u5b9a\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u548c\u5fae\u8c03\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347LLM\u5728\u79c1\u6709\u4ee3\u7801\u5e93\u4e0a\u7684\u4ee3\u7801\u8865\u5168\u6027\u80fd\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u63d0\u9ad8\u751f\u4ea7\u529b\u3002", "motivation": "\u73b0\u6709LLM\u5c3d\u7ba1\u5728\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u65e0\u6cd5\u5f88\u597d\u5730\u751f\u6210\u4e0e\u672a\u89c1\u8fc7\u7684\u79c1\u6709\u4ee3\u7801\u5e93\u4e00\u81f4\u7684\u4ee3\u7801\uff0c\u5b9a\u5236\u5316\u53ef\u4ee5\u63d0\u5347\u6a21\u578b\u5728\u79c1\u6709\u4ee3\u7801\u5e93\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ee3\u7801\u4e2d\u8bed\u4e49\u4f5c\u7528\u57df\u7684\u81ea\u52a8\u5316LLM\u5b9a\u5236\u65b9\u6cd5\uff0c\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u76d1\u7763\u5fae\u8c03\uff08FT\uff09\u4e24\u79cd\u7b56\u7565\uff0c\u5229\u7528\u79c1\u6709\u4ee3\u7801\u5e93\u6570\u636e\u5236\u4f5c\u8bad\u7ec3\u6570\u636e\u5bf9\uff0c\u5e2e\u52a9\u6a21\u578b\u5b66\u4e60\u79c1\u6709\u4ee3\u7801\u5e93\u7684\u7279\u5b9a\u6a21\u5f0f\u3002", "result": "\u5728\u4e24\u4e2a\u79c1\u6709\u4f01\u4e1a\u4ee3\u7801\u5e93\u548c\u4e24\u4e2a\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5b9a\u5236\u6a21\u578b\u5728\u4ee3\u7801\u8865\u5168\u4efb\u52a1\u4e2d\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u672a\u5b9a\u5236\u6a21\u578b\uff0c\u63d0\u5347\u4e86\u4ee3\u7801\u8d28\u91cf\u548c\u5f00\u53d1\u6548\u7387\u3002", "conclusion": "\u901a\u8fc7\u57fa\u4e8e\u8bed\u4e49\u4f5c\u7528\u57df\u7684\u81ea\u52a8\u5316\u5b9a\u5236\u65b9\u6cd5\uff0c\u6a21\u578b\u80fd\u591f\u66f4\u597d\u5730\u9002\u5e94\u79c1\u6709\u4ee3\u7801\u5e93\uff0c\u663e\u8457\u63d0\u5347\u4ee3\u7801\u8865\u5168\u7684\u51c6\u786e\u6027\u548c\u5f00\u53d1\u8005\u7684\u751f\u4ea7\u529b\uff0c\u5b9a\u5236\u540e\u7684\u4e2d\u7b49\u89c4\u6a21\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u672a\u5b9a\u5236\u7684\u5927\u5bb9\u91cf\u6a21\u578b\u3002"}}
{"id": "2602.05400", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05400", "abs": "https://arxiv.org/abs/2602.05400", "authors": ["Shaobo Wang", "Xuan Ouyang", "Tianyi Xu", "Yuzheng Hu", "Jialin Liu", "Guo Chen", "Tianyu Zhang", "Junhao Zheng", "Kexin Yang", "Xingzhang Ren", "Dayiheng Liu", "Linfeng Zhang"], "title": "OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration", "comment": "45 pages, 7 figures, 8 tables", "summary": "As high-quality public text approaches exhaustion, a phenomenon known as the Data Wall, pre-training is shifting from more tokens to better tokens. However, existing methods either rely on heuristic static filters that ignore training dynamics, or use dynamic yet optimizer-agnostic criteria based on raw gradients. We propose OPUS (Optimizer-induced Projected Utility Selection), a dynamic data selection framework that defines utility in the optimizer-induced update space. OPUS scores candidates by projecting their effective updates, shaped by modern optimizers, onto a target direction derived from a stable, in-distribution proxy. To ensure scalability, we employ Ghost technique with CountSketch for computational efficiency, and Boltzmann sampling for data diversity, incurring only 4.7\\% additional compute overhead. OPUS achieves remarkable results across diverse corpora, quality tiers, optimizers, and model scales. In pre-training of GPT-2 Large/XL on FineWeb and FineWeb-Edu with 30B tokens, OPUS outperforms industrial-level baselines and even full 200B-token training. Moreover, when combined with industrial-level static filters, OPUS further improves pre-training efficiency, even with lower-quality data. Furthermore, in continued pre-training of Qwen3-8B-Base on SciencePedia, OPUS achieves superior performance using only 0.5B tokens compared to full training with 3B tokens, demonstrating significant data efficiency gains in specialized domains.", "AI": {"tldr": "OPUS\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f18\u5316\u5668\u66f4\u65b0\u52a8\u6001\u7684\u6570\u636e\u9009\u62e9\u65b9\u6cd5\uff0c\u5927\u5e45\u63d0\u5347\u9884\u8bad\u7ec3\u6570\u636e\u5229\u7528\u7387\u548c\u6a21\u578b\u6027\u80fd\uff0c\u663e\u8457\u8282\u7701\u8ba1\u7b97\u8d44\u6e90\u3002", "motivation": "\u968f\u7740\u9ad8\u8d28\u91cf\u516c\u5171\u6587\u672c\u8d44\u6e90\u9010\u6e10\u67af\u7aed\uff0c\u9884\u8bad\u7ec3\u9700\u4ece\u66f4\u591atokens\u8f6c\u5411\u66f4\u4f18\u8d28tokens\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u591a\u4f9d\u8d56\u542f\u53d1\u5f0f\u9759\u6001\u8fc7\u6ee4\u6216\u57fa\u4e8e\u539f\u59cb\u68af\u5ea6\u7684\u4f18\u5316\u5668\u65e0\u5173\u52a8\u6001\u6807\u51c6\uff0c\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86OPUS\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6570\u636e\u5019\u9009\u7684\u6709\u6548\u66f4\u65b0\u6295\u5f71\u5230\u7531\u7a33\u5b9a\u5206\u5e03\u4ee3\u7406\u5bfc\u51fa\u7684\u76ee\u6807\u65b9\u5411\u6765\u8bc4\u5206\uff0c\u501f\u52a9Ghost\u6280\u672f\u548cCountSketch\u5b9e\u73b0\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u91c7\u7528Boltzmann\u91c7\u6837\u4fdd\u8bc1\u6570\u636e\u591a\u6837\u6027\u3002", "result": "OPUS\u5728GPT-2 Large/XL\u9884\u8bad\u7ec3\u4e2d\u4ee5\u4ec530B tokens\u8d85\u8fc7\u4e86\u5168\u91cf200B tokens\u8bad\u7ec3\uff0c\u7ed3\u5408\u5de5\u4e1a\u7ea7\u9759\u6001\u8fc7\u6ee4\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6548\u7387\uff0c\u5728Qwen3-8B-Base\u7684\u9886\u57df\u7279\u5b9a\u6301\u7eed\u9884\u8bad\u7ec3\u4e2d\u4ec5\u75280.5B tokens\u53d6\u5f97\u4f18\u4e8e3B tokens\u7684\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u6570\u636e\u6548\u7387\u3002", "conclusion": "OPUS\u901a\u8fc7\u5728\u4f18\u5316\u5668\u8bf1\u5bfc\u7684\u66f4\u65b0\u7a7a\u95f4\u4e2d\u5b9a\u4e49\u6570\u636e\u6548\u7528\uff0c\u5b9e\u73b0\u4e86\u52a8\u6001\u6570\u636e\u9009\u62e9\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u3001\u4f18\u5316\u5668\u548c\u6a21\u578b\u89c4\u6a21\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u8bad\u7ec3\u6548\u7387\u548c\u6570\u636e\u4f7f\u7528\u6548\u7387\u3002"}}
{"id": "2602.05891", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.05891", "abs": "https://arxiv.org/abs/2602.05891", "authors": ["Shenyu Zheng", "Ximing Dong", "Xiaoshuang Liu", "Gustavo Oliva", "Chong Chun Yong", "Dayi Lin", "Boyuan Chen", "Shaowei Wang", "Ahmed E. Hassan"], "title": "When Elo Lies: Hidden Biases in Codeforces-Based Evaluation of Large Language Models", "comment": null, "summary": "As Large Language Models (LLMs) achieve breakthroughs in complex reasoning, Codeforces-based Elo ratings have emerged as a prominent metric for evaluating competitive programming capabilities. However, these ratings are often reported without critical experimental details, leading to significant discrepancies illustrated by recent reports where the score of the same model version fluctuated by nearly 500 points. This paper presents a systematic empirical study on the hidden factors biasing Elo evaluations: (1) the temporal ordering of submissions, (2) contest difficulty selection, and (3) run to run stochastic variability of LLMs. Utilizing a controlled benchmark of 37 recent Codeforces contests and 13,691 generated test cases, we demonstrate that Elo scores are highly sensitive to these parameters. Our findings reveal that varying submission orders can shift scores by 394 points, while contest selection can cause differences of up to 1,122 points for the same model. Run to run performance exhibits substantial instability, with a maximum difference of 349 points in mean scores observed when evaluating identical contests. We conclude that direct Elo comparisons are unreliable and potentially misleading without strict standardization and transparent reporting of experimental settings.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u5f71\u54cd\u5927\u578b\u8bed\u8a00\u6a21\u578bCodeforces Elo\u8bc4\u5206\u7684\u4e0d\u786e\u5b9a\u56e0\u7d20\uff0c\u63ed\u793a\u8bc4\u5206\u6781\u5176\u654f\u611f\u4e14\u4e0d\u7a33\u5b9a\uff0c\u8b66\u793a\u4e0d\u89c4\u8303\u7684Elo\u8bc4\u5206\u6bd4\u8f83\u5b58\u5728\u8bef\u5bfc\u98ce\u9669\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u7ade\u4e89\u7f16\u7a0b\u80fd\u529b\u8bc4\u6d4b\u4f9d\u8d56\u4e8eCodeforces\u7684Elo\u8bc4\u5206\uff0c\u4f46\u8bc4\u5206\u4e2d\u7f3a\u4e4f\u5173\u952e\u5b9e\u9a8c\u7ec6\u8282\u5bfc\u81f4\u8bc4\u5206\u6ce2\u52a8\u5927\uff0c\u5f71\u54cd\u8bc4\u4ef7\u7684\u53ef\u9760\u6027\u3002", "method": "\u901a\u8fc7\u63a7\u5236\u53d8\u91cf\u8bbe\u8ba1\u5b9e\u9a8c\uff0c\u5229\u752837\u573a\u8fd1\u671fCodeforces\u7ade\u8d5b\u548c13,691\u4e2a\u6d4b\u8bd5\u6837\u4f8b\uff0c\u7cfb\u7edf\u7814\u7a76\u5f71\u54cdElo\u8bc4\u5206\u7684\u9690\u85cf\u56e0\u7d20\uff0c\u5305\u62ec\u63d0\u4ea4\u65f6\u95f4\u987a\u5e8f\u3001\u7ade\u8d5b\u96be\u5ea6\u9009\u62e9\u53ca\u6a21\u578b\u8fd0\u884c\u95f4\u7684\u968f\u673a\u6027\u3002", "result": "\u53d1\u73b0\u63d0\u4ea4\u987a\u5e8f\u53ef\u5bfc\u81f4\u8bc4\u5206\u6ce2\u52a8394\u5206\uff0c\u7ade\u8d5b\u9009\u62e9\u5bfc\u81f4\u8bc4\u5206\u5dee\u5f02\u6700\u9ad8\u8fbe1122\u5206\uff0c\u6a21\u578b\u591a\u6b21\u8fd0\u884c\u95f4\u8bc4\u5206\u5dee\u5f02\u6700\u9ad8\u4e3a349\u5206\uff0c\u8868\u660eElo\u8bc4\u5206\u9ad8\u5ea6\u654f\u611f\u4e14\u4e0d\u7a33\u5b9a\u3002", "conclusion": "\u672a\u7ecf\u4e25\u683c\u6807\u51c6\u5316\u548c\u900f\u660e\u5b9e\u9a8c\u62a5\u544a\uff0c\u76f4\u63a5\u4f7f\u7528Elo\u8bc4\u5206\u8fdb\u884c\u6a21\u578b\u6bd4\u8f83\u662f\u4e0d\u53ef\u9760\u4e14\u5177\u6709\u8bef\u5bfc\u6027\u7684\uff0c\u5e94\u8c28\u614e\u4f7f\u7528\u6b64\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\u3002"}}
{"id": "2602.05419", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05419", "abs": "https://arxiv.org/abs/2602.05419", "authors": ["Takumi Goto", "Yusuke Sakai", "Taro Watanabe"], "title": "Grammatical Error Correction Evaluation by Optimally Transporting Edit Representation", "comment": "Accepted to TACL. This is a pre-MIT Press publication version", "summary": "Automatic evaluation in grammatical error correction (GEC) is crucial for selecting the best-performing systems. Currently, reference-based metrics are a popular choice, which basically measure the similarity between hypothesis and reference sentences. However, similarity measures based on embeddings, such as BERTScore, are often ineffective, since many words in the source sentences remain unchanged in both the hypothesis and the reference. This study focuses on edits specifically designed for GEC, i.e., ERRANT, and computes similarity measured over the edits from the source sentence. To this end, we propose edit vector, a representation for an edit, and introduce a new metric, UOT-ERRANT, which transports these edit vectors from hypothesis to reference using unbalanced optimal transport. Experiments with SEEDA meta-evaluation show that UOT-ERRANT improves evaluation performance, particularly in the +Fluency domain where many edits occur. Moreover, our method is highly interpretable because the transport plan can be interpreted as a soft edit alignment, making UOT-ERRANT a useful metric for both system ranking and analyzing GEC systems. Our code is available from https://github.com/gotutiyan/uot-errant.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u5229\u7528\u7f16\u8f91\u5411\u91cf\u548c\u4e0d\u5e73\u8861\u6700\u4f18\u4f20\u8f93\u7684\u65b0\u578bGEC\u81ea\u52a8\u8bc4\u4ef7\u6307\u6807UOT-ERRANT\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5d4c\u5165\u76f8\u4f3c\u5ea6\u6307\u6807\u96be\u4ee5\u51c6\u786e\u8bc4\u4ef7\u7684\u95ee\u9898\uff0c\u5728\u63d0\u5347\u8bc4\u4ef7\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\u3002", "motivation": "\u76ee\u524d\u57fa\u4e8e\u5d4c\u5165\u7684\u76f8\u4f3c\u5ea6\u5ea6\u91cf\u5728\u8bed\u6cd5\u9519\u8bef\u7ea0\u6b63\u8bc4\u4ef7\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u539f\u56e0\u5728\u4e8e\u6e90\u53e5\u5b50\u4e2d\u5927\u91cf\u5355\u8bcd\u672a\u53d1\u751f\u53d8\u5316\uff0c\u5bfc\u81f4\u4f20\u7edf\u76f8\u4f3c\u5ea6\u6307\u6807\u96be\u4ee5\u51c6\u786e\u8861\u91cf\u5047\u8bbe\u4e0e\u53c2\u8003\u7684\u5dee\u5f02\u3002\u56e0\u800c\u9700\u8981\u4e00\u79cd\u4e13\u95e8\u9488\u5bf9GEC\u7f16\u8f91\u7684\u8bc4\u4ef7\u6307\u6807\u6765\u63d0\u9ad8\u8bc4\u4ef7\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u7f16\u8f91\u5411\u91cf\u8868\u793a\u6bcf\u4e2a\u7f16\u8f91\uff0c\u5e76\u5229\u7528\u4e0d\u5e73\u8861\u6700\u4f18\u4f20\u8f93\u65b9\u6cd5\u5c06\u5047\u8bbe\u4e0e\u53c2\u8003\u7684\u7f16\u8f91\u5411\u91cf\u8fdb\u884c\u5339\u914d\uff0c\u5f62\u6210\u65b0\u7684\u8bc4\u4ef7\u6307\u6807UOT-ERRANT\u3002\u7ed3\u5408ERRANT\u7f16\u8f91\u5de5\u5177\u5bf9\u6e90\u53e5\u5b50\u4e2d\u7684\u7f16\u8f91\u8fdb\u884c\u5ea6\u91cf\uff0c\u901a\u8fc7SEEDA\u5143\u8bc4\u4ef7\u9a8c\u8bc1\u4e86\u6307\u6807\u6027\u80fd\u3002", "result": "UOT-ERRANT\u5728SEEDA\u5143\u8bc4\u4ef7\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u7279\u522b\u662f\u5728+Fluency\u9886\u57df\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bc4\u4ef7\u51c6\u786e\u7387\uff1b\u5176\u4f20\u8f93\u8ba1\u5212\u63d0\u4f9b\u8f6f\u7f16\u8f91\u5bf9\u9f50\uff0c\u589e\u5f3a\u4e86\u8bc4\u4ef7\u8fc7\u7a0b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u7f16\u8f91\u5411\u91cf\u548c\u4e0d\u5e73\u8861\u6700\u4f18\u4f20\u8f93\u7684\u81ea\u52a8\u8bc4\u4ef7\u6307\u6807UOT-ERRANT\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u6cd5\u9519\u8bef\u7ea0\u6b63\u7cfb\u7edf\u7684\u8bc4\u4ef7\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u7f16\u8f91\u5bc6\u96c6\u7684\u6d41\u7545\u6027\u9886\u57df\u3002\u8be5\u6307\u6807\u4e0d\u4ec5\u6027\u80fd\u4f18\u8d8a\uff0c\u8fd8\u5177\u6709\u826f\u597d\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u80fd\u4e3a\u7cfb\u7edf\u6392\u540d\u53ca\u5206\u6790\u63d0\u4f9b\u6709\u529b\u652f\u6301\u3002"}}
{"id": "2602.05437", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05437", "abs": "https://arxiv.org/abs/2602.05437", "authors": ["Basel Mousi", "Fahim Dalvi", "Shammur Chowdhury", "Firoj Alam", "Nadir Durrani"], "title": "Once Correct, Still Wrong: Counterfactual Hallucination in Multilingual Vision-Language Models", "comment": null, "summary": "Vision-language models (VLMs) can achieve high accuracy while still accepting culturally plausible but visually incorrect interpretations. Existing hallucination benchmarks rarely test this failure mode, particularly outside Western contexts and English. We introduce M2CQA, a culturally grounded multimodal benchmark built from images spanning 17 MENA countries, paired with contrastive true and counterfactual statements in English, Arabic, and its dialects. To isolate hallucination beyond raw accuracy, we propose the CounterFactual Hallucination Rate (CFHR), which measures counterfactual acceptance conditioned on correctly answering the true statement. Evaluating state-of-the-art VLMs under multiple prompting strategies, we find that CFHR rises sharply in Arabic, especially in dialects, even when true-statement accuracy remains high. Moreover, reasoning-first prompting consistently increases counterfactual hallucination, while answering before justifying improves robustness. We will make the experimental resources and dataset publicly available for the community.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u9488\u5bf9\u4e2d\u4e1c\u5317\u975e\u6587\u5316\u80cc\u666f\u7684\u591a\u8bed\u79cd\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5e7b\u89c9\u8bc4\u6d4b\u57fa\u51c6M2CQA\u53ca\u53cd\u4e8b\u5b9e\u5e7b\u89c9\u7387\u6307\u6807\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u963f\u62c9\u4f2f\u8bed\u73af\u5883\u4e0b\u7684\u5e7b\u89c9\u95ee\u9898\u53ca\u63d0\u793a\u7b56\u7565\u7684\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bb9\u6613\u63a5\u53d7\u6587\u5316\u4e0a\u5408\u7406\u4f46\u89c6\u89c9\u4e0a\u9519\u8bef\u7684\u89e3\u91ca\uff0c\u5c24\u5176\u662f\u5728\u975e\u897f\u65b9\u8bed\u5883\u4e0e\u975e\u82f1\u8bed\u73af\u5883\u4e2d\uff0c\u73b0\u6709\u5e7b\u89c9\u8bc4\u6d4b\u7f3a\u4e4f\u9488\u5bf9\u8fd9\u4e00\u95ee\u9898\u7684\u8003\u5bdf\uff0c\u56e0\u6b64\u63d0\u51fa\u65b0\u7684\u591a\u6587\u5316\u591a\u8bed\u79cd\u57fa\u51c6\u8bc4\u6d4b\u3002", "method": "\u6784\u5efa\u6db5\u76d617\u4e2a\u4e2d\u4e1c\u5317\u975e\u56fd\u5bb6\u56fe\u50cf\u53ca\u5bf9\u5e94\u771f\u5b9e\u548c\u53cd\u4e8b\u5b9e\u8bed\u53e5\u7684\u591a\u8bed\u79cd\u6570\u636e\u96c6M2CQA\uff0c\u8bbe\u8ba1\u53cd\u4e8b\u5b9e\u5e7b\u89c9\u7387\uff08CFHR\uff09\u8bc4\u4ef7\u6307\u6807\uff0c\u5229\u7528\u591a\u79cd\u63d0\u793a\u7b56\u7565\u8bc4\u4f30\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5728\u591a\u8bed\u79cd\u8bc4\u6d4b\u4e2d\uff0c\u963f\u62c9\u4f2f\u8bed\u53ca\u5176\u65b9\u8a00\u7684\u53cd\u4e8b\u5b9e\u5e7b\u89c9\u7387\u663e\u8457\u4e0a\u5347\uff0c\u663e\u793a\u8bed\u8a00\u548c\u6587\u5316\u80cc\u666f\u5f71\u54cd\u6a21\u578b\u5e7b\u89c9\u8868\u73b0\uff0c\u4e14\u63d0\u793a\u7b56\u7565\u5bf9\u5e7b\u89c9\u7387\u6709\u660e\u663e\u5f71\u54cd\u3002", "conclusion": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6587\u5316\u80cc\u666f\u4e0b\uff0c\u5c24\u5176\u662f\u963f\u62c9\u4f2f\u8bed\u53ca\u5176\u65b9\u8a00\u4e2d\u5b58\u5728\u8f83\u9ad8\u7684\u5e7b\u89c9\u63a5\u53d7\u7387\uff0c\u8fd9\u4e00\u73b0\u8c61\u5728\u4f20\u7edf\u51c6\u786e\u7387\u8bc4\u6d4b\u4e2d\u96be\u4ee5\u5bdf\u89c9\u3002\u901a\u8fc7\u63d0\u51faM2CQA\u57fa\u51c6\u548c\u53cd\u4e8b\u5b9e\u5e7b\u89c9\u7387\u6307\u6807\uff0c\u53d1\u73b0\u63a8\u7406\u5148\u884c\u7684\u63d0\u793a\u7b56\u7565\u4f1a\u589e\u52a0\u5e7b\u89c9\uff0c\u800c\u5148\u56de\u7b54\u540e\u8fa9\u89e3\u5219\u80fd\u63d0\u5347\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2602.05444", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05444", "abs": "https://arxiv.org/abs/2602.05444", "authors": ["Yao Zhou", "Zeen Song", "Wenwen Qiang", "Fengge Wu", "Shuyi Zhou", "Changwen Zheng", "Hui Xiong"], "title": "Causal Front-Door Adjustment for Robust Jailbreak Attacks on LLMs", "comment": null, "summary": "Safety alignment mechanisms in Large Language Models (LLMs) often operate as latent internal states, obscuring the model's inherent capabilities. Building on this observation, we model the safety mechanism as an unobserved confounder from a causal perspective. Then, we propose the \\textbf{C}ausal \\textbf{F}ront-Door \\textbf{A}djustment \\textbf{A}ttack ({\\textbf{CFA}}$^2$) to jailbreak LLM, which is a framework that leverages Pearl's Front-Door Criterion to sever the confounding associations for robust jailbreaking. Specifically, we employ Sparse Autoencoders (SAEs) to physically strip defense-related features, isolating the core task intent. We further reduce computationally expensive marginalization to a deterministic intervention with low inference complexity. Experiments demonstrate that {CFA}$^2$ achieves state-of-the-art attack success rates while offering a mechanistic interpretation of the jailbreaking process.", "AI": {"tldr": "\u672c\u6587\u4ece\u56e0\u679c\u89c6\u89d2\u5efa\u6a21LLM\u5b89\u5168\u673a\u5236\uff0c\u63d0\u51faCFA^2\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5265\u79bb\u9632\u5fa1\u7279\u5f81\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u7ed5\u8fc7\uff0c\u653b\u7834\u5f53\u524d\u6700\u5f3a\u9632\u5fa1\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5b89\u5168\u5bf9\u9f50\u673a\u5236\u4f5c\u4e3a\u6f5c\u5728\u7684\u5185\u90e8\u72b6\u6001\uff0c\u63a9\u76d6\u4e86\u6a21\u578b\u7684\u56fa\u6709\u80fd\u529b\uff0c\u5bfc\u81f4\u73b0\u6709\u9632\u5fa1\u96be\u4ee5\u6709\u6548\u8fa8\u8bc6\u548c\u7ed5\u8fc7\u3002", "method": "\u4ece\u56e0\u679c\u63a8\u65ad\u89d2\u5ea6\u51fa\u53d1\uff0c\u5c06\u5b89\u5168\u673a\u5236\u89c6\u4e3a\u672a\u89c2\u5bdf\u5230\u7684\u6df7\u6742\u53d8\u91cf\uff0c\u63d0\u51fa\u5229\u7528Pearl\u7684\u524d\u95e8\u51c6\u5219\u8bbe\u8ba1\u7684\u56e0\u679c\u524d\u95e8\u8c03\u6574\u653b\u51fb\uff08CFA^2\uff09\uff0c\u901a\u8fc7\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5265\u79bb\u9632\u5fa1\u7279\u5f81\uff0c\u5e76\u7528\u786e\u5b9a\u6027\u5e72\u9884\u7b80\u5316\u8ba1\u7b97\u8fc7\u7a0b\u3002", "result": "CFA^2\u5728\u653b\u51fb\u6210\u529f\u7387\u4e0a\u8fbe\u5230\u6700\u65b0\u6c34\u5e73\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u5bf9\u7ed5\u8fc7\u8fc7\u7a0b\u7684\u673a\u5236\u6027\u89e3\u91ca\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u4f4e\u63a8\u7406\u590d\u6742\u5ea6\u3002", "conclusion": "\u672c\u6587\u5f15\u5165\u56e0\u679c\u63a8\u65ad\u6846\u67b6\uff0c\u6709\u6548\u63ed\u793a\u5e76\u7ed5\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5b89\u5168\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u5f3a\u6709\u529b\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u9501\u653b\u51fb\u3002"}}
{"id": "2602.05447", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05447", "abs": "https://arxiv.org/abs/2602.05447", "authors": ["Damon McMillan"], "title": "Structured Context Engineering for File-Native Agentic Systems: Evaluating Schema Accuracy, Format Effectiveness, and Multi-File Navigation at Scale", "comment": "8 pages, 7 figures, 10 tables, 26 references", "summary": "Large Language Model agents increasingly operate external systems through programmatic interfaces, yet practitioners lack empirical guidance on how to structure the context these agents consume. Using SQL generation as a proxy for programmatic agent operations, we present a systematic study of context engineering for structured data, comprising 9,649 experiments across 11 models, 4 formats (YAML, Markdown, JSON, Token-Oriented Object Notation [TOON]), and schemas ranging from 10 to 10,000 tables.\n  Our findings challenge common assumptions. First, architecture choice is model-dependent: file-based context retrieval improves accuracy for frontier-tier models (Claude, GPT, Gemini; +2.7%, p=0.029) but shows mixed results for open source models (aggregate -7.7%, p<0.001), with deficits varying substantially by model. Second, format does not significantly affect aggregate accuracy (chi-squared=2.45, p=0.484), though individual models, particularly open source, exhibit format-specific sensitivities. Third, model capability is the dominant factor, with a 21 percentage point accuracy gap between frontier and open source tiers that dwarfs any format or architecture effect. Fourth, file-native agents scale to 10,000 tables through domain-partitioned schemas while maintaining high navigation accuracy. Fifth, file size does not predict runtime efficiency: compact formats can consume significantly more tokens at scale due to format-unfamiliar search patterns.\n  These findings provide practitioners with evidence-based guidance for deploying LLM agents on structured systems, demonstrating that architectural decisions should be tailored to model capability rather than assuming universal best practices.", "AI": {"tldr": "\u901a\u8fc7\u5bf9\u591a\u6a21\u578b\u591a\u683c\u5f0f\u591a\u89c4\u6a21\u6570\u636e\u5e93\u7684\u8fd1\u4e07\u6b21\u5b9e\u9a8c\uff0c\u7814\u7a76\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u64cd\u4f5c\u7ed3\u6784\u5316\u7cfb\u7edf\u65f6\u4e0a\u4e0b\u6587\u8bbe\u8ba1\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u5e94\u6839\u636e\u6a21\u578b\u80fd\u529b\u5b9a\u5236\u4e0a\u4e0b\u6587\u7ed3\u6784\uff0c\u6311\u6218\u4e86\u4e00\u4e9b\u5e38\u89c1\u5047\u8bbe\u3002", "motivation": "\u7f3a\u4e4f\u5b9e\u8df5\u4e2d\u9488\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b(agent)\u4f7f\u7528\u7a0b\u5e8f\u5316\u63a5\u53e3\u64cd\u4f5c\u5916\u90e8\u7ed3\u6784\u5316\u7cfb\u7edf\u65f6\u5982\u4f55\u8bbe\u8ba1\u4e0a\u4e0b\u6587\u7684\u5b9e\u8bc1\u6307\u5bfc\u3002", "method": "\u901a\u8fc79649\u6b21\u5b9e\u9a8c\uff0c\u6bd4\u8f8311\u4e2a\u6a21\u578b\u5728\u4e0d\u540c\u683c\u5f0f\uff08YAML\u3001Markdown\u3001JSON\u3001TOON\uff09\u548c\u4e0d\u540c\u89c4\u6a21\u7684\u6570\u636e\u5e93\u67b6\u6784\uff0810\u523010000\u8868\uff09\u4e0a\u751f\u6210SQL\u7684\u8868\u73b0\uff0c\u7cfb\u7edf\u7814\u7a76\u4e86\u4e0a\u4e0b\u6587\u5de5\u7a0b\u5bf9\u7ed3\u6784\u5316\u6570\u636e\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u6587\u4ef6\u578b\u4e0a\u4e0b\u6587\u67b6\u6784\u5bf9\u524d\u6cbf\u6a21\u578b\uff08\u5982Claude, GPT, Gemini\uff09\u63d0\u5347\u51c6\u786e\u7387\uff0c\u800c\u5f00\u6e90\u6a21\u578b\u6548\u679c\u4e0d\u4e00\uff1b\u683c\u5f0f\u5bf9\u6574\u4f53\u51c6\u786e\u7387\u65e0\u663e\u8457\u5f71\u54cd\u4f46\u5f00\u6e90\u6a21\u578b\u6709\u683c\u5f0f\u654f\u611f\u6027\uff1b\u6a21\u578b\u80fd\u529b\u662f\u51c6\u786e\u7387\u5dee\u5f02\u7684\u4e3b\u5bfc\u56e0\u7d20\uff1b\u5927\u89c4\u6a21\u6587\u4ef6\u5206\u533a\u65b9\u6848\u4fdd\u6301\u9ad8\u5bfc\u822a\u51c6\u786e\u7387\uff1b\u7d27\u51d1\u683c\u5f0f\u5728\u5927\u89c4\u6a21\u4e0b\u53ef\u80fd\u56e0\u641c\u7d22\u6a21\u5f0f\u5e26\u6765\u66f4\u591a\u8ba1\u7b97\u6d88\u8017\u3002", "conclusion": "\u67b6\u6784\u9009\u62e9\u5e94\u6839\u636e\u6a21\u578b\u80fd\u529b\u5b9a\u5236\uff0c\u4e0d\u540c\u6a21\u578b\u5bf9\u4e0a\u4e0b\u6587\u7ed3\u6784\u6709\u4e0d\u540c\u54cd\u5e94\uff0c\u683c\u5f0f\u5bf9\u6574\u4f53\u51c6\u786e\u6027\u5f71\u54cd\u4e0d\u5927\uff0c\u6a21\u578b\u80fd\u529b\u662f\u5f71\u54cd\u51c6\u786e\u6027\u7684\u4e3b\u8981\u56e0\u7d20\uff0c\u6587\u4ef6\u672c\u5730\u4ee3\u7406\u80fd\u591f\u6709\u6548\u6269\u5c55\u81f3\u5927\u578b\u7ed3\u6784\u5316\u6570\u636e\u3002"}}
{"id": "2602.05471", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05471", "abs": "https://arxiv.org/abs/2602.05471", "authors": ["Md. Mithun Hossaina", "Mashary N. Alrasheedy", "Nirban Bhowmick", "Shamim Forhad", "Md. Shakil Hossain", "Sudipto Chaki", "Md Shafiqul Islam"], "title": "Reasoning under Ambiguity: Uncertainty-Aware Multilingual Emotion Classification under Partial Supervision", "comment": null, "summary": "Contemporary knowledge-based systems increasingly rely on multilingual emotion identification to support intelligent decision-making, yet they face major challenges due to emotional ambiguity and incomplete supervision. Emotion recognition from text is inherently uncertain because multiple emotional states often co-occur and emotion annotations are frequently missing or heterogeneous. Most existing multi-label emotion classification methods assume fully observed labels and rely on deterministic learning objectives, which can lead to biased learning and unreliable predictions under partial supervision. This paper introduces Reasoning under Ambiguity, an uncertainty-aware framework for multilingual multi-label emotion classification that explicitly aligns learning with annotation uncertainty. The proposed approach uses a shared multilingual encoder with language-specific optimization and an entropy-based ambiguity weighting mechanism that down-weights highly ambiguous training instances rather than treating missing labels as negative evidence. A mask-aware objective with positive-unlabeled regularization is further incorporated to enable robust learning under partial supervision. Experiments on English, Spanish, and Arabic emotion classification benchmarks demonstrate consistent improvements over strong baselines across multiple evaluation metrics, along with improved training stability, robustness to annotation sparsity, and enhanced interpretability.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u591a\u8bed\u8a00\u591a\u6807\u7b7e\u60c5\u611f\u5206\u7c7b\u4e2d\u7684\u6807\u6ce8\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u5957\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u8bed\u8a00\u7f16\u7801\u5668\u3001\u71b5\u6743\u91cd\u673a\u5236\u548c\u6b63-\u65e0\u6807\u8bb0\u6b63\u5219\u5316\uff0c\u5b9e\u73b0\u4e86\u90e8\u5206\u76d1\u7763\u4e0b\u7684\u7a33\u5065\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\u548c\u6a21\u578b\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524d\u77e5\u8bc6\u7cfb\u7edf\u4e2d\u60c5\u611f\u8bc6\u522b\u5b58\u5728\u60c5\u611f\u72b6\u6001\u5171\u73b0\u5bfc\u81f4\u7684\u6a21\u7cca\u6027\uff0c\u4ee5\u53ca\u6807\u7b7e\u7f3a\u5931\u6216\u5f02\u8d28\u6027\u5e26\u6765\u7684\u76d1\u7763\u4e0d\u5b8c\u6574\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u6807\u7b7e\u5b8c\u5168\u89c2\u6d4b\u4e14\u76ee\u6807\u786e\u5b9a\uff0c\u5bfc\u81f4\u504f\u5dee\u548c\u4e0d\u53ef\u9760\u9884\u6d4b\uff0c\u4e9f\u9700\u8003\u8651\u6807\u6ce8\u4e0d\u786e\u5b9a\u6027\u4ee5\u63d0\u5347\u591a\u8bed\u8a00\u591a\u6807\u7b7e\u60c5\u611f\u8bc6\u522b\u7684\u51c6\u786e\u6027\u4e0e\u9c81\u68d2\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u591a\u8bed\u8a00\u591a\u6807\u7b7e\u60c5\u611f\u8bc6\u522b\u6846\u67b6\uff0c\u91c7\u7528\u5171\u4eab\u7684\u591a\u8bed\u8a00\u7f16\u7801\u5668\u7ed3\u5408\u8bed\u8a00\u7279\u5b9a\u4f18\u5316\uff0c\u5229\u7528\u57fa\u4e8e\u71b5\u7684\u6a21\u7cca\u52a0\u6743\u673a\u5236\u964d\u4f4e\u9ad8\u5ea6\u6a21\u7cca\u8bad\u7ec3\u6837\u672c\u7684\u6743\u91cd\uff0c\u5e76\u5f15\u5165\u5e26\u6b63-\u65e0\u6807\u8bb0\u6b63\u5219\u5316\u7684\u63a9\u7801\u611f\u77e5\u76ee\u6807\uff0c\u5b9e\u73b0\u5bf9\u90e8\u5206\u76d1\u7763\u4e0b\u7684\u9c81\u68d2\u5b66\u4e60\u3002", "result": "\u5728\u82f1\u6587\u3001\u897f\u73ed\u7259\u8bed\u548c\u963f\u62c9\u4f2f\u8bed\u60c5\u611f\u5206\u7c7b\u57fa\u51c6\u4e0a\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u591a\u4e2a\u8bc4\u4ef7\u6307\u6807\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0c\u5c55\u73b0\u4e86\u66f4\u597d\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u3001\u5bf9\u6807\u6ce8\u7a00\u758f\u7684\u9c81\u68d2\u6027\u548c\u589e\u5f3a\u7684\u6a21\u578b\u89e3\u91ca\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u201c\u6a21\u7cca\u63a8\u7406\u201d\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u591a\u8bed\u8a00\u591a\u6807\u7b7e\u60c5\u611f\u5206\u7c7b\u7684\u6027\u80fd\uff0c\u5176\u65b9\u6cd5\u6709\u6548\u5904\u7406\u4e86\u60c5\u611f\u6807\u6ce8\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u7f3a\u5931\u95ee\u9898\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u89e3\u91ca\u6027\u3002"}}
{"id": "2602.05495", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05495", "abs": "https://arxiv.org/abs/2602.05495", "authors": ["Chenhang Cui", "Binyun Yang", "Fei Shen", "Yuxin Chen", "Jingnan Zheng", "Xiang Wang", "An Zhang", "Tat-Seng Chua"], "title": "Transport and Merge: Cross-Architecture Merging for Large Language Models", "comment": null, "summary": "Large language models (LLMs) achieve strong capabilities by scaling model capacity and training data, yet many real-world deployments rely on smaller models trained or adapted from low-resource data. This gap motivates the need for mechanisms to transfer knowledge from large, high-resource models to smaller, low-resource targets. While model merging provides an effective transfer mechanism, most existing approaches assume architecture-compatible models and therefore cannot directly transfer knowledge from large high-resource LLMs to heterogeneous low-resource targets. In this work, we propose a cross-architecture merging framework based on optimal transport (OT) that aligns activations to infer cross-neuron correspondences between heterogeneous models. The resulting transport plans are then used to guide direct weight-space fusion, enabling effective high-resource to low-resource transfer using only a small set of inputs. Extensive experiments across low-resource languages and specialized domains demonstrate consistent improvements over target models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u8de8\u67b6\u6784\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\uff0c\u5b9e\u73b0\u5927\u578b\u9ad8\u8d44\u6e90\u8bed\u8a00\u6a21\u578b\u5411\u5f02\u6784\u5c0f\u578b\u4f4e\u8d44\u6e90\u6a21\u578b\u7684\u77e5\u8bc6\u8fc1\u79fb\uff0c\u5e76\u5728\u591a\u79cd\u4f4e\u8d44\u6e90\u4efb\u52a1\u4e2d\u53d6\u5f97\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u771f\u5b9e\u90e8\u7f72\u4e2d\u5e38\u9700\u8981\u5c06\u5927\u578b\u9ad8\u8d44\u6e90\u6a21\u578b\u7684\u77e5\u8bc6\u8fc1\u79fb\u5230\u7ed3\u6784\u4e0d\u540c\u7684\u5c0f\u578b\u4f4e\u8d44\u6e90\u6a21\u578b\uff0c\u73b0\u6709\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\u591a\u4f9d\u8d56\u67b6\u6784\u517c\u5bb9\u6027\uff0c\u9650\u5236\u4e86\u77e5\u8bc6\u8f6c\u79fb\u7684\u8303\u56f4\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u8de8\u67b6\u6784\u6a21\u578b\u5408\u5e76\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6fc0\u6d3b\u7684\u5bf9\u9f50\u63a8\u65ad\u8de8\u795e\u7ecf\u5143\u5bf9\u5e94\u5173\u7cfb\uff0c\u5229\u7528\u4f20\u8f93\u8ba1\u5212\u6307\u5bfc\u6743\u91cd\u7a7a\u95f4\u878d\u5408\uff0c\u5b9e\u73b0\u5f02\u6784\u6a21\u578b\u95f4\u7684\u77e5\u8bc6\u8fc1\u79fb\u3002", "result": "\u65b9\u6cd5\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u4e13\u4e1a\u9886\u57df\u5b9e\u9a8c\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u76ee\u6807\u5c0f\u578b\u6a21\u578b\u7684\u6027\u80fd\uff0c\u663e\u793a\u51fa\u8de8\u67b6\u6784\u8fc1\u79fb\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u8de8\u67b6\u6784\u6a21\u578b\u5408\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u65b9\u6848\uff0c\u5b9e\u73b0\u4ece\u5927\u578b\u9ad8\u8d44\u6e90\u6a21\u578b\u5411\u5f02\u6784\u5c0f\u578b\u4f4e\u8d44\u6e90\u6a21\u578b\u7684\u77e5\u8bc6\u8fc1\u79fb\uff0c\u6269\u5c55\u4e86\u6a21\u578b\u5408\u5e76\u7684\u9002\u7528\u8303\u56f4\u3002"}}
{"id": "2602.05512", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.05512", "abs": "https://arxiv.org/abs/2602.05512", "authors": ["Larissa Pusch", "Alexandre Courtiol", "Tim Conrad"], "title": "A Human-in-the-Loop, LLM-Centered Architecture for Knowledge-Graph Question Answering", "comment": null, "summary": "Large Language Models (LLMs) excel at language understanding but remain limited in knowledge-intensive domains due to hallucinations, outdated information, and limited explainability. Text-based retrieval-augmented generation (RAG) helps ground model outputs in external sources but struggles with multi-hop reasoning. Knowledge Graphs (KGs), in contrast, support precise, explainable querying, yet require a knowledge of query languages. This work introduces an interactive framework in which LLMs generate and explain Cypher graph queries and users iteratively refine them through natural language. Applied to real-world KGs, the framework improves accessibility to complex datasets while preserving factual accuracy and semantic rigor and provides insight into how model performance varies across domains. Our core quantitative evaluation is a 90-query benchmark on a synthetic movie KG that measures query explanation quality and fault detection across multiple LLMs, complemented by two smaller real-life query-generation experiments on a Hyena KG and the MaRDI (Mathematical Research Data Initiative) KG.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4ea4\u4e92\u5f0f\u77e5\u8bc6\u56fe\u8c31\u67e5\u8be2\u6846\u67b6\uff0c\u63d0\u5347\u4e86\u590d\u6742\u67e5\u8be2\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u51c6\u786e\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u591a\u8df3\u63a8\u7406\u548c\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u9886\u57df\u5b58\u5728\u5e7b\u89c9\u3001\u8fc7\u65f6\u4fe1\u606f\u548c\u53ef\u89e3\u91ca\u6027\u5dee\u7684\u95ee\u9898\uff0c\u6587\u672c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u591a\u8df3\u63a8\u7406\uff0c\u800c\u77e5\u8bc6\u56fe\u8c31\u867d\u652f\u6301\u7cbe\u786e\u67e5\u8be2\u4f46\u5bf9\u67e5\u8be2\u8bed\u8a00\u8981\u6c42\u9ad8\u3002", "method": "\u8bbe\u8ba1\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u548c\u89e3\u91caCypher\u56fe\u67e5\u8be2\uff0c\u7528\u6237\u53ef\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u8fed\u4ee3\u5730\u5b8c\u5584\u67e5\u8be2\u3002", "result": "\u5728\u5408\u6210\u7535\u5f71\u77e5\u8bc6\u56fe\u8c31\u4e0a\u768490\u4e2a\u67e5\u8be2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8bc4\u4f30\u4e86\u67e5\u8be2\u89e3\u91ca\u8d28\u91cf\u548c\u9519\u8bef\u68c0\u6d4b\u80fd\u529b\uff0c\u5e76\u5728\u771f\u5b9e\u7684Hyena KG\u548cMaRDI KG\u4e0a\u5b8c\u6210\u4e86\u5c0f\u89c4\u6a21\u67e5\u8be2\u751f\u6210\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u901a\u8fc7\u5f15\u5165\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u6846\u67b6\uff0c\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5e76\u89e3\u91caCypher\u56fe\u67e5\u8be2\uff0c\u5e76\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\uff0c\u6709\u6548\u63d0\u5347\u4e86\u590d\u6742\u77e5\u8bc6\u56fe\u8c31\u7684\u53ef\u8bbf\u95ee\u6027\uff0c\u540c\u65f6\u4fdd\u8bc1\u4e86\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u8bed\u4e49\u4e25\u8c28\u6027\u3002"}}
{"id": "2602.05547", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.05547", "abs": "https://arxiv.org/abs/2602.05547", "authors": ["Shyam Sundhar Ramesh", "Xiaotong Ji", "Matthieu Zimmer", "Sangwoong Yoon", "Zhiyong Wang", "Haitham Bou Ammar", "Aurelien Lucchi", "Ilija Bogunovic"], "title": "Multi-Task GRPO: Reliable LLM Reasoning Across Tasks", "comment": "Preprint", "summary": "RL-based post-training with GRPO is widely used to improve large language models on individual reasoning tasks. However, real-world deployment requires reliable performance across diverse tasks. A straightforward multi-task adaptation of GRPO often leads to imbalanced outcomes, with some tasks dominating optimization while others stagnate. Moreover, tasks can vary widely in how frequently prompts yield zero advantages (and thus zero gradients), which further distorts their effective contribution to the optimization signal. To address these issues, we propose a novel Multi-Task GRPO (MT-GRPO) algorithm that (i) dynamically adapts task weights to explicitly optimize worst-task performance and promote balanced progress across tasks, and (ii) introduces a ratio-preserving sampler to ensure task-wise policy gradients reflect the adapted weights. Experiments on both 3-task and 9-task settings show that MT-GRPO consistently outperforms baselines in worst-task accuracy. In particular, MT-GRPO achieves 16-28% and 6% absolute improvement on worst-task performance over standard GRPO and DAPO, respectively, while maintaining competitive average accuracy. Moreover, MT-GRPO requires 50% fewer training steps to reach 50% worst-task accuracy in the 3-task setting, demonstrating substantially improved efficiency in achieving reliable performance across tasks.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u591a\u4efb\u52a1GRPO\u4e2d\u4efb\u52a1\u4e0d\u5e73\u8861\u53ca\u4f18\u5316\u4fe1\u53f7\u5931\u771f\u95ee\u9898\uff0c\u63d0\u51faMT-GRPO\uff0c\u901a\u8fc7\u52a8\u6001\u6743\u91cd\u8c03\u6574\u548c\u6bd4\u4f8b\u91c7\u6837\u5b9e\u73b0\u4efb\u52a1\u8868\u73b0\u5747\u8861\u548c\u9ad8\u6548\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u6700\u5f31\u4efb\u52a1\u51c6\u786e\u7387\u3002", "motivation": "\u4f20\u7edfGRPO\u7684\u591a\u4efb\u52a1\u9002\u5e94\u5b58\u5728\u4efb\u52a1\u4e3b\u5bfc\u548c\u4efb\u52a1\u505c\u6ede\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u4e14\u4e0d\u540c\u4efb\u52a1\u96f6\u4f18\u52bf\u6837\u672c\u9891\u7387\u5dee\u5f02\u5bfc\u81f4\u4f18\u5316\u4fe1\u53f7\u5931\u771f\uff0c\u9700\u4e00\u79cd\u80fd\u5e73\u8861\u4efb\u52a1\u8868\u73b0\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMT-GRPO\u7b97\u6cd5\uff0c\u5305\u62ec\u52a8\u6001\u8c03\u6574\u4efb\u52a1\u6743\u91cd\u4ee5\u4f18\u5316\u6700\u5f31\u4efb\u52a1\u8868\u73b0\uff0c\u4ee5\u53ca\u5f15\u5165\u6bd4\u4f8b\u4fdd\u6301\u91c7\u6837\u5668\u786e\u4fdd\u7b56\u7565\u68af\u5ea6\u4e0e\u6743\u91cd\u76f8\u7b26\u3002", "result": "\u57283\u4efb\u52a1\u548c9\u4efb\u52a1\u8bbe\u7f6e\u4e2d\uff0cMT-GRPO\u5728\u6700\u5dee\u4efb\u52a1\u51c6\u786e\u7387\u4e0a\u6bd4\u6807\u51c6GRPO\u63d0\u534716-28%\uff0c\u6bd4DAPO\u63d0\u53476%\uff0c\u4e14\u8bad\u7ec3\u6b65\u6570\u51cf\u5c1150%\uff0c\u4fdd\u6301\u4e86\u7ade\u4e89\u529b\u7684\u5e73\u5747\u51c6\u786e\u7387\u3002", "conclusion": "MT-GRPO\u7b97\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u591a\u4efb\u52a1\u73af\u5883\u4e0b\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6700\u5dee\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u66f4\u5747\u8861\u7684\u4efb\u52a1\u8fdb\u5c55\u548c\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u8fc7\u7a0b\u3002"}}
{"id": "2602.05633", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05633", "abs": "https://arxiv.org/abs/2602.05633", "authors": ["Rui Jia", "Ruiyi Lan", "Fengrui Liu", "Zhongxiang Dai", "Bo Jiang", "Jing Shao", "Jingyuan Chen", "Guandong Xu", "Fei Wu", "Min Zhang"], "title": "CASTLE: A Comprehensive Benchmark for Evaluating Student-Tailored Personalized Safety in Large Language Models", "comment": null, "summary": "Large language models (LLMs) have advanced the development of personalized learning in education. However, their inherent generation mechanisms often produce homogeneous responses to identical prompts. This one-size-fits-all mechanism overlooks the substantial heterogeneity in students cognitive and psychological, thereby posing potential safety risks to vulnerable groups. Existing safety evaluations primarily rely on context-independent metrics such as factual accuracy, bias, or toxicity, which fail to capture the divergent harms that the same response might cause across different student attributes. To address this gap, we propose the concept of Student-Tailored Personalized Safety and construct CASTLE based on educational theories. This benchmark covers 15 educational safety risks and 14 student attributes, comprising 92,908 bilingual scenarios. We further design three evaluation metrics: Risk Sensitivity, measuring the model ability to detect risks; Emotional Empathy, evaluating the model capacity to recognize student states; and Student Alignment, assessing the match between model responses and student attributes. Experiments on 18 SOTA LLMs demonstrate that CASTLE poses a significant challenge: all models scored below an average safety rating of 2.3 out of 5, indicating substantial deficiencies in personalized safety assurance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5b66\u751f\u5b9a\u5236\u4e2a\u6027\u5316\u5b89\u5168\u8bc4\u4f30\u57fa\u51c6CASTLE\uff0c\u63ed\u793a\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e2a\u6027\u5316\u6559\u80b2\u5b89\u5168\u4e0a\u7684\u660e\u663e\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u5b89\u5168\u8bc4\u4f30\u6b20\u7f3a\u9488\u5bf9\u4e0d\u540c\u5b66\u751f\u5c5e\u6027\u5bf9\u540c\u4e00\u54cd\u5e94\u53ef\u80fd\u4ea7\u751f\u7684\u5dee\u5f02\u6027\u4f24\u5bb3\u7684\u8003\u91cf\uff0c\u5bfc\u81f4\u4e2a\u6027\u5316\u5b66\u4e60\u4e2d\u7684\u5b89\u5168\u98ce\u9669\u65e0\u6cd5\u5145\u5206\u68c0\u6d4b\u548c\u8bc4\u4f30\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u6559\u80b2\u7406\u8bba\u7684\u5b66\u751f\u5b9a\u5236\u4e2a\u6027\u5316\u5b89\u5168\u6982\u5ff5\uff0c\u6784\u5efa\u4e86\u5305\u542b15\u79cd\u6559\u80b2\u5b89\u5168\u98ce\u9669\u548c14\u4e2a\u5b66\u751f\u5c5e\u6027\u7684\u591a\u8bed\u8a00\u573a\u666f\u57fa\u51c6CASTLE\uff0c\u5e76\u8bbe\u8ba1\u4e86\u98ce\u9669\u654f\u611f\u6027\u3001\u60c5\u611f\u5171\u60c5\u548c\u5b66\u751f\u5339\u914d\u5ea6\u4e09\u4e2a\u8bc4\u4ef7\u6307\u6807\u3002", "result": "\u572818\u4e2a\u6700\u5148\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u6d4b\u8bd5\uff0c\u6240\u6709\u6a21\u578b\u7684\u5b89\u5168\u5e73\u5747\u8bc4\u5206\u4e0d\u8db32.3\uff08\u6ee1\u52065\u5206\uff09\uff0c\u8868\u660e\u5f53\u524d\u6a21\u578b\u5728\u4e2a\u6027\u5316\u5b89\u5168\u4fdd\u8bc1\u65b9\u9762\u8868\u73b0\u8f83\u5dee\u3002", "conclusion": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e2a\u6027\u5316\u5b66\u4e60\u4e2d\u7684\u5b89\u5168\u4fdd\u969c\u5b58\u5728\u8f83\u5927\u4e0d\u8db3\uff0c\u96be\u4ee5\u6709\u6548\u8bc6\u522b\u548c\u9002\u5e94\u4e0d\u540c\u5b66\u751f\u7684\u8ba4\u77e5\u548c\u5fc3\u7406\u5dee\u5f02\uff0c\u5b58\u5728\u6f5c\u5728\u7684\u5b89\u5168\u98ce\u9669\u3002"}}
{"id": "2602.05648", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05648", "abs": "https://arxiv.org/abs/2602.05648", "authors": ["Giuseppe Samo", "Paola Merlo"], "title": "Modelling the Morphology of Verbal Paradigms: A Case Study in the Tokenization of Turkish and Hebrew", "comment": "13 pages, 7 figures, to appear as proceedings of the SIGTURK 2026 Workshop", "summary": "We investigate how transformer models represent complex verb paradigms in Turkish and Modern Hebrew, concentrating on how tokenization strategies shape this ability. Using the Blackbird Language Matrices task on natural data, we show that for Turkish -- with its transparent morphological markers -- both monolingual and multilingual models succeed, either when tokenization is atomic or when it breaks words into small subword units. For Hebrew, instead, monolingual and multilingual models diverge. A multilingual model using character-level tokenization fails to capture the language non-concatenative morphology, but a monolingual model with morpheme-aware segmentation performs well. Performance improves on more synthetic datasets, in all models.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u53d8\u6362\u5668\u6a21\u578b\u5728\u571f\u8033\u5176\u8bed\u548c\u5e0c\u4f2f\u6765\u8bed\u590d\u6742\u52a8\u8bcd\u8303\u5f0f\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5206\u8bcd\u65b9\u5f0f\u663e\u8457\u5f71\u54cd\u6a21\u578b\u6548\u679c\uff0c\u8bcd\u7d20\u611f\u77e5\u5206\u8bcd\u6709\u52a9\u4e8e\u5904\u7406\u975e\u62fc\u63a5\u8bed\u8a00\u5f62\u6001\u3002", "motivation": "\u7814\u7a76\u53d8\u6362\u5668\u6a21\u578b\u5982\u4f55\u8868\u793a\u571f\u8033\u5176\u8bed\u548c\u73b0\u4ee3\u5e0c\u4f2f\u6765\u8bed\u4e2d\u7684\u590d\u6742\u52a8\u8bcd\u8303\u5f0f\uff0c\u5c24\u5176\u5173\u6ce8\u5206\u8bcd\u7b56\u7565\u5982\u4f55\u5f71\u54cd\u8fd9\u79cd\u80fd\u529b\u3002", "method": "\u5229\u7528Blackbird\u8bed\u8a00\u77e9\u9635\u4efb\u52a1\u5728\u81ea\u7136\u6570\u636e\u4e0a\u8bc4\u4f30\u4e0d\u540c\u6a21\u578b\uff08\u5355\u8bed\u548c\u591a\u8bed\uff09\u53ca\u4e0d\u540c\u5206\u8bcd\u7b56\u7565\uff08\u539f\u5b50\u5206\u8bcd\u3001\u5c0f\u5b50\u8bcd\u5355\u5143\u3001\u57fa\u4e8e\u8bcd\u7d20\u7684\u5206\u8bcd\u3001\u5b57\u7b26\u7ea7\u5206\u8bcd\uff09\u5bf9\u4e24\u79cd\u8bed\u8a00\u52a8\u8bcd\u5f62\u6001\u7684\u8868\u793a\u80fd\u529b\u3002", "result": "\u571f\u8033\u5176\u8bed\u4e2d\uff0c\u65e0\u8bba\u662f\u539f\u5b50\u5206\u8bcd\u8fd8\u662f\u5c0f\u5b50\u8bcd\u5206\u8bcd\uff0c\u5355\u8bed\u548c\u591a\u8bed\u6a21\u578b\u5747\u8868\u73b0\u826f\u597d\u3002\u5e0c\u4f2f\u6765\u8bed\u4e2d\uff0c\u591a\u8bed\u6a21\u578b\u4f7f\u7528\u5b57\u7b26\u7ea7\u5206\u8bcd\u672a\u80fd\u6355\u6349\u975e\u62fc\u63a5\u5f62\u6001\uff0c\u4f46\u91c7\u7528\u8bcd\u7d20\u611f\u77e5\u5206\u8bcd\u7684\u5355\u8bed\u6a21\u578b\u8868\u73b0\u51fa\u8272\u3002\u6240\u6709\u6a21\u578b\u5728\u66f4\u5177\u5408\u6210\u6027\u7684\u5408\u6210\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u6709\u6240\u63d0\u5347\u3002", "conclusion": "\u5206\u8bcd\u7b56\u7565\u5bf9\u53d8\u6362\u5668\u6a21\u578b\u6355\u6349\u4e0d\u540c\u8bed\u8a00\u5f62\u6001\u7ed3\u6784\u5177\u6709\u663e\u8457\u5f71\u54cd\uff0c\u900f\u660e\u4e14\u62fc\u63a5\u6027\u5f62\u6001\u7684\u8bed\u8a00\u8868\u73b0\u66f4\u7a33\u5b9a\uff0c\u590d\u6742\u975e\u62fc\u63a5\u5f62\u6001\u9700\u8bcd\u7d20\u611f\u77e5\u5206\u8bcd\u624d\u80fd\u6709\u6548\u5efa\u6a21\u3002"}}
{"id": "2602.05692", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05692", "abs": "https://arxiv.org/abs/2602.05692", "authors": ["Congbo Ma", "Yichun Zhang", "Yousef Al-Jazzazi", "Ahamed Foisal", "Laasya Sharma", "Yousra Sadqi", "Khaled Saleh", "Jihad Mallat", "Farah E. Shamout"], "title": "MedErrBench: A Fine-Grained Multilingual Benchmark for Medical Error Detection and Correction with Clinical Expert Annotations", "comment": null, "summary": "Inaccuracies in existing or generated clinical text may lead to serious adverse consequences, especially if it is a misdiagnosis or incorrect treatment suggestion. With Large Language Models (LLMs) increasingly being used across diverse healthcare applications, comprehensive evaluation through dedicated benchmarks is crucial. However, such datasets remain scarce, especially across diverse languages and contexts. In this paper, we introduce MedErrBench, the first multilingual benchmark for error detection, localization, and correction, developed under the guidance of experienced clinicians. Based on an expanded taxonomy of ten common error types, MedErrBench covers English, Arabic and Chinese, with natural clinical cases annotated and reviewed by domain experts. We assessed the performance of a range of general-purpose, language-specific, and medical-domain language models across all three tasks. Our results reveal notable performance gaps, particularly in non-English settings, highlighting the need for clinically grounded, language-aware systems. By making MedErrBench and our evaluation protocols publicly-available, we aim to advance multilingual clinical NLP to promote safer and more equitable AI-based healthcare globally. The dataset is available in the supplementary material. An anonymized version of the dataset is available at: https://github.com/congboma/MedErrBench.", "AI": {"tldr": "\u63a8\u51fa\u9996\u4e2a\u591a\u8bed\u8a00\u4e34\u5e8a\u9519\u8bef\u68c0\u6d4b\u57fa\u51c6MedErrBench\uff0c\u8bc4\u4f30\u591a\u79cd\u6a21\u578b\uff0c\u53d1\u73b0\u975e\u82f1\u8bed\u73af\u5883\u4e0b\u6027\u80fd\u4e0d\u8db3\uff0c\u63a8\u52a8\u66f4\u5b89\u5168\u516c\u5e73\u7684\u533b\u7597AI\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u4e34\u5e8a\u6587\u672c\u4e2d\u7684\u9519\u8bef\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u540e\u679c\uff0c\u5c24\u5176\u662f\u5728\u8bef\u8bca\u6216\u9519\u8bef\u6cbb\u7597\u5efa\u8bae\u65f6\u3002\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u533b\u7597\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u4e9f\u9700\u591a\u8bed\u8a00\u3001\u4e0d\u540c\u60c5\u5883\u4e0b\u7684\u7cfb\u7edf\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u6784\u5efaMedErrBench\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u591a\u8bed\u8a00\u4e34\u5e8a\u9519\u8bef\u68c0\u6d4b\u3001\u5b9a\u4f4d\u548c\u7ea0\u6b63\u57fa\u51c6\uff0c\u6db5\u76d6\u82f1\u8bed\u3001\u963f\u62c9\u4f2f\u8bed\u548c\u4e2d\u6587\u3002\u57fa\u4e8e\u5341\u79cd\u5e38\u89c1\u9519\u8bef\u7c7b\u578b\uff0c\u6570\u636e\u7ecf\u8fc7\u4e34\u5e8a\u4e13\u5bb6\u6ce8\u91ca\u548c\u5ba1\u6838\u3002\u8bc4\u4f30\u4e86\u901a\u7528\u8bed\u8a00\u6a21\u578b\u3001\u7279\u5b9a\u8bed\u8a00\u6a21\u578b\u53ca\u533b\u7597\u9886\u57df\u6a21\u578b\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5404\u6a21\u578b\u5b58\u5728\u8f83\u5927\u6027\u80fd\u5dee\u8ddd\uff0c\u5c24\u5176\u5728\u975e\u82f1\u8bed\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u5f3a\u8c03\u4e86\u5f00\u53d1\u4e34\u5e8a\u80cc\u666f\u3001\u8bed\u8a00\u654f\u611f\u7cfb\u7edf\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "MedErrBench\u4f5c\u4e3a\u516c\u5f00\u7684\u591a\u8bed\u8a00\u4e34\u5e8a\u9519\u8bef\u68c0\u6d4b\u57fa\u51c6\uff0c\u4fc3\u8fdb\u4e86\u57fa\u4e8eAI\u7684\u66f4\u5b89\u5168\u3001\u66f4\u516c\u5e73\u7684\u5168\u7403\u533b\u7597\u53d1\u5c55\u3002"}}
{"id": "2602.05694", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05694", "abs": "https://arxiv.org/abs/2602.05694", "authors": ["Shuting Jiang", "Ran Song", "Yuxin Huang", "Yan Xiang", "Yantuan Xian", "Shengxiang Gao", "Zhengtao Yu"], "title": "Consensus-Aligned Neuron Efficient Fine-Tuning Large Language Models for Multi-Domain Machine Translation", "comment": "Accepted by AAAI 2026", "summary": "Multi-domain machine translation (MDMT) aims to build a unified model capable of translating content across diverse domains. Despite the impressive machine translation capabilities demonstrated by large language models (LLMs), domain adaptation still remains a challenge for LLMs. Existing MDMT methods such as in-context learning and parameter-efficient fine-tuning often suffer from domain shift, parameter interference and limited generalization. In this work, we propose a neuron-efficient fine-tuning framework for MDMT that identifies and updates consensus-aligned neurons within LLMs. These neurons are selected by maximizing the mutual information between neuron behavior and domain features, enabling LLMs to capture both generalizable translation patterns and domain-specific nuances. Our method then fine-tunes LLMs guided by these neurons, effectively mitigating parameter interference and domain-specific overfitting. Comprehensive experiments on three LLMs across ten German-English and Chinese-English translation domains evidence that our method consistently outperforms strong PEFT baselines on both seen and unseen domains, achieving state-of-the-art performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u57fa\u4e8e\u795e\u7ecf\u5143\u9009\u62e9\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u63d0\u9ad8\u5927\u8bed\u8a00\u6a21\u578b\u591a\u9886\u57df\u673a\u5668\u7ffb\u8bd1\u7684\u9886\u57df\u9002\u5e94\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u7ffb\u8bd1\u6027\u80fd\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u867d\u5728\u673a\u5668\u7ffb\u8bd1\u6709\u51fa\u8272\u8868\u73b0\uff0c\u4f46\u9886\u57df\u9002\u5e94\u4ecd\u5b58\u5728\u6311\u6218\uff0c\u73b0\u6709\u591a\u9886\u57df\u673a\u5668\u7ffb\u8bd1\u65b9\u6cd5\u5728\u9886\u57df\u8fc1\u79fb\u3001\u53c2\u6570\u5e72\u6270\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u5b58\u5728\u7f3a\u9677\u3002", "method": "\u901a\u8fc7\u6700\u5927\u5316\u795e\u7ecf\u5143\u884c\u4e3a\u4e0e\u9886\u57df\u7279\u5f81\u7684\u4e92\u4fe1\u606f\uff0c\u9009\u62e9\u5173\u952e\u795e\u7ecf\u5143\uff0c\u5e76\u57fa\u4e8e\u8fd9\u4e9b\u795e\u7ecf\u5143\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u4ece\u800c\u51cf\u8f7b\u53c2\u6570\u5e72\u6270\u548c\u9886\u57df\u8fc7\u62df\u5408\u3002", "result": "\u5728\u4e09\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u548c\u5341\u4e2a\u5fb7\u82f1\u3001\u82f1\u4e2d\u6587\u672c\u7ffb\u8bd1\u9886\u57df\u7684\u7efc\u5408\u5b9e\u9a8c\u4e2d\uff0c\u672c\u65b9\u6cd5\u5728\u5df2\u89c1\u548c\u672a\u89c1\u9886\u57df\u5747\u4f18\u4e8e\u5f3a\u52b2\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u57fa\u7ebf\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u57fa\u4e8e\u795e\u7ecf\u5143\u9ad8\u6548\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u8bc6\u522b\u548c\u66f4\u65b0\u5171\u8bc6\u5bf9\u9f50\u7684\u795e\u7ecf\u5143\uff0c\u5b9e\u73b0\u4e86\u591a\u9886\u57df\u673a\u5668\u7ffb\u8bd1\u4e2d\u9886\u57df\u9002\u5e94\u7684\u6709\u6548\u63d0\u5347\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u3002"}}
{"id": "2602.05711", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05711", "abs": "https://arxiv.org/abs/2602.05711", "authors": ["Jingze Shi", "Zhangyang Peng", "Yizhang Zhu", "Yifan Wu", "Guang Liu", "Yuyu Luo"], "title": "OmniMoE: An Efficient MoE by Orchestrating Atomic Experts at Scale", "comment": null, "summary": "Mixture-of-Experts (MoE) architectures are evolving towards finer granularity to improve parameter efficiency. However, existing MoE designs face an inherent trade-off between the granularity of expert specialization and hardware execution efficiency. We propose OmniMoE, a system-algorithm co-designed framework that pushes expert granularity to its logical extreme. OmniMoE introduces vector-level Atomic Experts, enabling scalable routing and execution within a single MoE layer, while retaining a shared dense MLP branch for general-purpose processing. Although this atomic design maximizes capacity, it poses severe challenges for routing complexity and memory access. To address these, OmniMoE adopts a system-algorithm co-design: (i) a Cartesian Product Router that decomposes the massive index space to reduce routing complexity from O(N) to O(sqrt(N)); and (ii) Expert-Centric Scheduling that inverts the execution order to turn scattered, memory-bound lookups into efficient dense matrix operations. Validated on seven benchmarks, OmniMoE (with 1.7B active parameters) achieves 50.9% zero-shot accuracy across seven benchmarks, outperforming coarse-grained (e.g., DeepSeekMoE) and fine-grained (e.g., PEER) baselines. Crucially, OmniMoE reduces inference latency from 73ms to 6.7ms (a 10.9-fold speedup) compared to PEER, demonstrating that massive-scale fine-grained MoE can be fast and accurate. Our code is open-sourced at https://github.com/flash-algo/omni-moe.", "AI": {"tldr": "OmniMoE\u901a\u8fc7\u7cfb\u7edf\u7b97\u6cd5\u534f\u540c\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u6781\u7ec6\u7c92\u5ea6\u7684\u4e13\u5bb6\u7f51\u7edc\uff0c\u663e\u8457\u63d0\u5347\u51c6\u786e\u7387\u548c\u63a8\u7406\u901f\u5ea6\uff0c\u7a81\u7834\u4e86MoE\u6a21\u578b\u5728\u6548\u7387\u4e0e\u7c92\u5ea6\u4e0a\u7684\u4f20\u7edf\u6743\u8861\u3002", "motivation": "\u73b0\u6709MoE\u8bbe\u8ba1\u5728\u4e13\u5bb6\u7c92\u5ea6\u548c\u786c\u4ef6\u6267\u884c\u6548\u7387\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0cOmniMoE\u65e8\u5728\u7a81\u7834\u8be5\u74f6\u9888\uff0c\u63a8\u52a8\u4e13\u5bb6\u7c92\u5ea6\u8fbe\u5230\u903b\u8f91\u6781\u9650\uff0c\u5b9e\u73b0\u66f4\u9ad8\u53c2\u6570\u6548\u7387\u3002", "method": "OmniMoE\u901a\u8fc7\u5f15\u5165\u5411\u91cf\u7ea7\u539f\u5b50\u4e13\u5bb6\u4ee5\u53ca\u7cfb\u7edf\u4e0e\u7b97\u6cd5\u7684\u534f\u540c\u8bbe\u8ba1\uff0c\u5305\u62ec\u7b1b\u5361\u5c14\u79ef\u8def\u7531\u5668\u964d\u4f4e\u8def\u7531\u590d\u6742\u5ea6\u548c\u4e13\u5bb6\u4e2d\u5fc3\u8c03\u5ea6\u4f18\u5316\u5185\u5b58\u8bbf\u95ee\uff0c\u5b9e\u73b0\u4e86\u5927\u89c4\u6a21\u7ec6\u7c92\u5ea6MoE\u7684\u9ad8\u6548\u8def\u7531\u548c\u6267\u884c\u3002", "result": "\u5728\u4e03\u4e2a\u57fa\u51c6\u4e0a\uff0cOmniMoE\u4ee517\u4ebf\u6d3b\u8dc3\u53c2\u6570\u8fbe\u5230\u4e8650.9%\u7684\u96f6\u6837\u672c\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u4e3b\u6d41\u7c97\u7c92\u5ea6\u548c\u7ec6\u7c92\u5ea6MoE\u6a21\u578b\uff0c\u540c\u65f6\u63a8\u7406\u5ef6\u8fdf\u4ece73ms\u964d\u81f36.7ms\uff0c\u63d0\u5347\u4e8610.9\u500d\u901f\u5ea6\u3002", "conclusion": "OmniMoE\u5b9e\u73b0\u4e86\u6781\u7ec6\u7c92\u5ea6\u7684\u4e13\u5bb6\u7f51\u7edc\u8bbe\u8ba1\uff0c\u5728\u63d0\u5347\u6a21\u578b\u5bb9\u91cf\u548c\u51c6\u786e\u7387\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u63a8\u7406\u5ef6\u8fdf\uff0c\u517c\u5177\u9ad8\u6548\u6027\u4e0e\u51c6\u786e\u6027\u3002"}}
{"id": "2602.05728", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05728", "abs": "https://arxiv.org/abs/2602.05728", "authors": ["Hao Yang", "Zhiyu Yang", "Xupeng Zhang", "Wei Wei", "Yunjie Zhang", "Lin Yang"], "title": "CompactRAG: Reducing LLM Calls and Token Overhead in Multi-Hop Question Answering", "comment": null, "summary": "Retrieval-augmented generation (RAG) has become a key paradigm for knowledge-intensive question answering. However, existing multi-hop RAG systems remain inefficient, as they alternate between retrieval and reasoning at each step, resulting in repeated LLM calls, high token consumption, and unstable entity grounding across hops. We propose CompactRAG, a simple yet effective framework that decouples offline corpus restructuring from online reasoning.\n  In the offline stage, an LLM reads the corpus once and converts it into an atomic QA knowledge base, which represents knowledge as minimal, fine-grained question-answer pairs. In the online stage, complex queries are decomposed and carefully rewritten to preserve entity consistency, and are resolved through dense retrieval followed by RoBERTa-based answer extraction. Notably, during inference, the LLM is invoked only twice in total - once for sub-question decomposition and once for final answer synthesis - regardless of the number of reasoning hops.\n  Experiments on HotpotQA, 2WikiMultiHopQA, and MuSiQue demonstrate that CompactRAG achieves competitive accuracy while substantially reducing token consumption compared to iterative RAG baselines, highlighting a cost-efficient and practical approach to multi-hop reasoning over large knowledge corpora. The implementation is available at GitHub.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7684CompactRAG\u901a\u8fc7\u5c06\u77e5\u8bc6\u5e93\u6784\u5efa\u79bb\u7ebf\u5b8c\u6210\uff0c\u5e76\u5728\u7ebf\u4f7f\u7528\u5c11\u91cfLLM\u8c03\u7528\u5b9e\u73b0\u591a\u8df3\u95ee\u7b54\u63a8\u7406\uff0c\u6781\u5927\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\u5e76\u4fdd\u6301\u51c6\u786e\u6027\uff0c\u9002\u5408\u5927\u89c4\u6a21\u77e5\u8bc6\u5e93\u7684\u6210\u672c\u6548\u76ca\u95ee\u7b54\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u591a\u8df3\u77e5\u8bc6\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u5728\u6bcf\u4e2a\u63a8\u7406\u6b65\u9aa4\u4ea4\u66ff\u8c03\u7528\u68c0\u7d22\u548c\u63a8\u7406\uff0c\u5bfc\u81f4\u91cd\u590d\u8c03\u7528LLM\u3001\u4ee4\u724c\u6d88\u8017\u9ad8\u4e14\u5b9e\u4f53\u4e00\u81f4\u6027\u5dee\uff0c\u9700\u63d0\u5347\u7cfb\u7edf\u6548\u7387\u4e0e\u7a33\u5b9a\u6027\u3002", "method": "\u91c7\u7528\u79bb\u7ebf\u9636\u6bb5\u7531LLM\u4e00\u6b21\u6027\u8bfb\u53d6\u8bed\u6599\u5e76\u751f\u6210\u539f\u5b50\u7ea7\u95ee\u7b54\u5bf9\u6784\u6210\u77e5\u8bc6\u5e93\uff0c\u5728\u7ebf\u9636\u6bb5\u8fdb\u884c\u590d\u6742\u67e5\u8be2\u7684\u5206\u89e3\u548c\u6539\u5199\uff0c\u901a\u8fc7\u5bc6\u96c6\u68c0\u7d22\u548cRoBERTa\u62bd\u53d6\u7b54\u6848\uff0c\u63a8\u7406\u65f6\u53ea\u8c03\u7528LLM\u4e24\u6b21\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u91cd\u590d\u8c03\u7528\u3002", "result": "\u5728HotpotQA\u30012WikiMultiHopQA\u548cMuSiQue\u6570\u636e\u96c6\u4e0a\uff0cCompactRAG\u4e0d\u4ec5\u5728\u51c6\u786e\u7387\u4e0a\u8fbe\u5230\u6216\u8d85\u8d8a\u57fa\u7ebf\uff0c\u8fd8\u663e\u8457\u51cf\u5c11\u4e86\u4ee4\u724c\u6d88\u8017\uff0c\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u591a\u8df3\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "CompactRAG\u901a\u8fc7\u79bb\u7ebf\u77e5\u8bc6\u5e93\u6784\u5efa\u548c\u5728\u7ebf\u63a8\u7406\u7684\u89e3\u8026\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u8df3\u590d\u6742\u95ee\u7b54\u7cfb\u7edf\u7684\u6548\u7387\uff0c\u5b9e\u73b0\u4e86\u8f83\u4f4e\u7684\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u6027\u51c6\u786e\u7387\u3002"}}
{"id": "2602.05758", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05758", "abs": "https://arxiv.org/abs/2602.05758", "authors": ["Bowen Ping", "Zijun Chen", "Yiyao Yu", "Tingfeng Hui", "Junchi Yan", "Baobao Chang"], "title": "LongR: Unleashing Long-Context Reasoning via Reinforcement Learning with Dense Utility Rewards", "comment": null, "summary": "Reinforcement Learning has emerged as a key driver for LLM reasoning. This capability is equally pivotal in long-context scenarios--such as long-dialogue understanding and structured data analysis, where the challenge extends beyond consuming tokens to performing rigorous deduction. While existing efforts focus on data synthesis or architectural changes, recent work points out that relying solely on sparse, outcome-only rewards yields limited gains, as such coarse signals are often insufficient to effectively guide the complex long-context reasoning. To address this, we propose LongR, a unified framework that enhances long-context performance by integrating a dynamic \"Think-and-Read\" mechanism, which interleaves reasoning with document consultation, with a contextual density reward based on relative information gain to quantify the utility of the relevant documents. Empirically, LongR achieves a 9% gain on LongBench v2 and consistent improvements on RULER and InfiniteBench, demonstrating robust efficiency in navigating extensive contexts. Furthermore, LongR consistently enhances performance across diverse RL algorithms (e.g., DAPO, GSPO). Finally, we conduct in-depth analyses to investigate the impact of reasoning chain length on efficiency and the model's robustness against distractors.", "AI": {"tldr": "\u63d0\u51faLongR\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u63a8\u7406\u4e0e\u6587\u6863\u67e5\u9605\u7ed3\u5408\u9ad8\u6548\u5956\u52b1\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u89c4\u6a21\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u63a8\u7406\u80fd\u529b\u7684\u91cd\u8981\u6027\u4ee5\u53ca\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7a00\u758f\u5956\u52b1\u4fe1\u53f7\u9650\u5236\u6027\u80fd\u63d0\u5347\u3002", "method": "\u63d0\u51faLongR\u6846\u67b6\uff0c\u7ed3\u5408\u52a8\u6001\u201c\u601d\u8003\u4e0e\u9605\u8bfb\u201d\u673a\u5236\u548c\u57fa\u4e8e\u4fe1\u606f\u589e\u76ca\u7684\u4e0a\u4e0b\u6587\u5bc6\u5ea6\u5956\u52b1\uff0c\u4ea4\u7ec7\u63a8\u7406\u4e0e\u6587\u6863\u67e5\u9605\u3002", "result": "LongR\u5728LongBench v2\u4e0a\u63d0\u53479%\uff0c\u5728RULER\u548cInfiniteBench\u4e0a\u4e5f\u6709\u4e00\u81f4\u63d0\u5347\uff0c\u5e76\u5728\u591a\u79cd\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4e2d\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "LongR\u6709\u6548\u589e\u5f3a\u4e86\u957f\u4e0a\u4e0b\u6587\u4e2d\u7684\u63a8\u7406\u6027\u80fd\u548c\u6548\u7387\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u9762\u5bf9\u5927\u89c4\u6a21\u4e0a\u4e0b\u6587\u7684\u80fd\u529b\u548c\u6297\u5e72\u6270\u80fd\u529b\u3002"}}
{"id": "2602.05769", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05769", "abs": "https://arxiv.org/abs/2602.05769", "authors": ["Adnan Al Ali", "Jind\u0159ich Helcl", "Jind\u0159ich Libovick\u00fd"], "title": "Different Time, Different Language: Revisiting the Bias Against Non-Native Speakers in GPT Detectors", "comment": "This paper was accepted to EACL 2026 Student Research Workshop", "summary": "LLM-based assistants have been widely popularised after the release of ChatGPT. Concerns have been raised about their misuse in academia, given the difficulty of distinguishing between human-written and generated text. To combat this, automated techniques have been developed and shown to be effective, to some extent. However, prior work suggests that these methods often falsely flag essays from non-native speakers as generated, due to their low perplexity extracted from an LLM, which is supposedly a key feature of the detectors. We revisit these statements two years later, specifically in the Czech language setting. We show that the perplexity of texts from non-native speakers of Czech is not lower than that of native speakers. We further examine detectors from three separate families and find no systematic bias against non-native speakers. Finally, we demonstrate that contemporary detectors operate effectively without relying on perplexity.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86\u975e\u6bcd\u8bed\u6587\u672c\u88ab\u8bef\u5224\u4e3aAI\u751f\u6210\u6587\u672c\u7684\u95ee\u9898\uff0c\u53d1\u73b0\u6377\u514b\u8bed\u975e\u6bcd\u8bed\u8005\u6587\u672c\u5e76\u65e0\u8f83\u4f4e\u56f0\u60d1\u5ea6\uff0c\u4e14\u73b0\u4ee3\u6587\u672c\u68c0\u6d4b\u5668\u8868\u73b0\u516c\u6b63\u4e14\u6709\u6548\u3002", "motivation": "\u56de\u5e94\u6b64\u524d\u7814\u7a76\u4e2d\u975e\u6bcd\u8bed\u8005\u6587\u672c\u88ab\u8bef\u5224\u4e3a\u751f\u6210\u6587\u672c\u7684\u95ee\u9898\uff0c\u9a8c\u8bc1\u56f0\u60d1\u5ea6\u4f5c\u4e3a\u68c0\u6d4b\u7279\u5f81\u7684\u771f\u5b9e\u6027\u3002", "method": "\u91cd\u65b0\u8bc4\u4f30\u4e86\u6377\u514b\u8bed\u80cc\u666f\u4e0b\u975e\u6bcd\u8bed\u8005\u6587\u672c\u7684\u56f0\u60d1\u5ea6\uff0c\u6d4b\u8bd5\u4e86\u4e09\u7c7b\u68c0\u6d4b\u5668\u5bf9\u975e\u6bcd\u8bed\u6587\u672c\u7684\u504f\u89c1\u60c5\u51b5\uff0c\u5206\u6790\u68c0\u6d4b\u5668\u7684\u6838\u5fc3\u7279\u5f81\u3002", "result": "\u53d1\u73b0\u975e\u6bcd\u8bed\u6587\u672c\u7684\u56f0\u60d1\u5ea6\u5e76\u4e0d\u4f4e\u4e8e\u6bcd\u8bed\u6587\u672c\uff0c\u4e09\u7c7b\u68c0\u6d4b\u5668\u65e0\u7cfb\u7edf\u504f\u89c1\uff0c\u73b0\u4ee3\u68c0\u6d4b\u624b\u6bb5\u65e0\u9700\u4f9d\u8d56\u56f0\u60d1\u5ea6\u3002", "conclusion": "\u5f53\u524d\u5bf9\u975e\u6bcd\u8bed\u8005\u6587\u672c\u7684\u68c0\u6d4b\u5e76\u4e0d\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u89c1\uff0c\u4e14\u73b0\u4ee3\u68c0\u6d4b\u5668\u4e0d\u4f9d\u8d56\u56f0\u60d1\u5ea6\u6709\u6548\u8bc6\u522b\u751f\u6210\u6587\u672c\u3002"}}
{"id": "2602.05842", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05842", "abs": "https://arxiv.org/abs/2602.05842", "authors": ["Xiao Yu", "Baolin Peng", "Ruize Xu", "Yelong Shen", "Pengcheng He", "Suman Nath", "Nikhil Singh", "Jiangfeng Gao", "Zhou Yu"], "title": "Reinforcement World Model Learning for LLM-based Agents", "comment": null, "summary": "Large language models (LLMs) have achieved strong performance in language-centric tasks. However, in agentic settings, LLMs often struggle to anticipate action consequences and adapt to environment dynamics, highlighting the need for world-modeling capabilities in LLM-based agents. We propose Reinforcement World Model Learning (RWML), a self-supervised method that learns action-conditioned world models for LLM-based agents on textual states using sim-to-real gap rewards. Our method aligns simulated next states produced by the model with realized next states observed from the environment, encouraging consistency between internal world simulations and actual environment dynamics in a pre-trained embedding space. Unlike next-state token prediction, which prioritizes token-level fidelity (i.e., reproducing exact wording) over semantic equivalence and can lead to model collapse, our method provides a more robust training signal and is empirically less susceptible to reward hacking than LLM-as-a-judge. We evaluate our method on ALFWorld and $\u03c4^2$ Bench and observe significant gains over the base model, despite being entirely self-supervised. When combined with task-success rewards, our method outperforms direct task-success reward RL by 6.9 and 5.7 points on ALFWorld and $\u03c4^2$ Bench respectively, while matching the performance of expert-data training.", "AI": {"tldr": "RWML\u5229\u7528\u81ea\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u5bf9\u6bd4\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u72b6\u6001\uff0c\u63d0\u5347\u4e86LLM\u667a\u80fd\u4f53\u7684\u4e16\u754c\u5efa\u6a21\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u73af\u5883\u9002\u5e94\u548c\u4efb\u52a1\u5b8c\u6210\u8868\u73b0\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u8a00\u4efb\u52a1\u8868\u73b0\u5f3a\u52b2\uff0c\u4f46\u5728\u667a\u80fd\u4f53\u73af\u5883\u9002\u5e94\u4e2d\u96be\u4ee5\u9884\u6d4b\u884c\u52a8\u540e\u679c\u4e0e\u73af\u5883\u52a8\u6001\uff0c\u7f3a\u4e4f\u6709\u6548\u7684\u4e16\u754c\u5efa\u6a21\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u81ea\u76d1\u7763\u7684\u5f3a\u5316\u4e16\u754c\u6a21\u578b\u5b66\u4e60\uff08RWML\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6587\u672c\u72b6\u6001\u4e2d\u7684\u52a8\u4f5c\u6761\u4ef6\u4e16\u754c\u6a21\u578b\u8fdb\u884c\u5b66\u4e60\uff0c\u5229\u7528\u6a21\u62df\u4e0e\u771f\u5b9e\u72b6\u6001\u5dee\u8ddd\u5956\u52b1\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u5bf9\u73af\u5883\u52a8\u6001\u7684\u5185\u5728\u6a21\u62df\u4e00\u81f4\u6027\u3002", "result": "\u5728ALFWorld\u548c\u03c4\u00b2 Bench\u6d4b\u8bd5\u4e2d\uff0cRWML\u663e\u8457\u4f18\u4e8e\u57fa\u7840\u6a21\u578b\uff0c\u540c\u65f6\u7ed3\u5408\u4efb\u52a1\u6210\u529f\u5956\u52b1\u540e\uff0c\u8d85\u8fc7\u4e86\u76f4\u63a5\u57fa\u4e8e\u4efb\u52a1\u6210\u529f\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u8868\u73b0\u4e0e\u4e13\u5bb6\u6570\u636e\u8bad\u7ec3\u76f8\u5f53\u3002", "conclusion": "RWML \u65b9\u6cd5\u901a\u8fc7\u5bf9\u6bd4\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u72b6\u6001\u8f6c\u79fb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u5728\u73af\u5883\u52a8\u6001\u9002\u5e94\u548c\u884c\u52a8\u540e\u679c\u9884\u6d4b\u65b9\u9762\u7684\u6027\u80fd\u3002"}}
{"id": "2602.05843", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05843", "abs": "https://arxiv.org/abs/2602.05843", "authors": ["Fangzhi Xu", "Hang Yan", "Qiushi Sun", "Jinyang Wu", "Zixian Huang", "Muye Huang", "Jingyang Gong", "Zichen Ding", "Kanzhi Cheng", "Yian Wang", "Xinyu Che", "Zeyi Sun", "Jian Zhang", "Zhangyue Yin", "Haoran Luo", "Xuanjing Huang", "Ben Kao", "Jun Liu", "Qika Lin"], "title": "OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions", "comment": "34 pages", "summary": "The rapid advancement of Large Language Models (LLMs) has catalyzed the development of autonomous agents capable of navigating complex environments. However, existing evaluations primarily adopt a deductive paradigm, where agents execute tasks based on explicitly provided rules and static goals, often within limited planning horizons. Crucially, this neglects the inductive necessity for agents to discover latent transition laws from experience autonomously, which is the cornerstone for enabling agentic foresight and sustaining strategic coherence. To bridge this gap, we introduce OdysseyArena, which re-centers agent evaluation on long-horizon, active, and inductive interactions. We formalize and instantiate four primitives, translating abstract transition dynamics into concrete interactive environments. Building upon this, we establish OdysseyArena-Lite for standardized benchmarking, providing a set of 120 tasks to measure an agent's inductive efficiency and long-horizon discovery. Pushing further, we introduce OdysseyArena-Challenge to stress-test agent stability across extreme interaction horizons (e.g., > 200 steps). Extensive experiments on 15+ leading LLMs reveal that even frontier models exhibit a deficiency in inductive scenarios, identifying a critical bottleneck in the pursuit of autonomous discovery in complex environments. Our code and data are available at https://github.com/xufangzhi/Odyssey-Arena", "AI": {"tldr": "\u672c\u6587\u63d0\u51faOdysseyArena\u5e73\u53f0\uff0c\u4e13\u6ce8\u957f\u65f6\u5e8f\u4e3b\u52a8\u5f52\u7eb3\u4ea4\u4e92\u8bc4\u4f30\uff0c\u63ed\u793a\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u4e3b\u53d1\u73b0\u590d\u6742\u73af\u5883\u8f6c\u79fb\u89c4\u5f8b\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u8bc4\u4f30\u5c40\u9650\u4e8e\u6f14\u7ece\u8303\u5f0f\uff0c\u7f3a\u5c11\u5bf9\u6a21\u578b\u81ea\u4e3b\u5f52\u7eb3\u6f5c\u5728\u73af\u5883\u8f6c\u79fb\u89c4\u5f8b\u7684\u6d4b\u8bd5\uff0c\u963b\u788d\u63d0\u5347\u6a21\u578b\u7684\u6218\u7565\u524d\u77bb\u6027\u548c\u957f\u671f\u89c4\u5212\u80fd\u529b\u3002", "method": "\u63d0\u51faOdysseyArena\u73af\u5883\uff0c\u901a\u8fc7\u56db\u4e2a\u57fa\u672c\u64cd\u4f5c\u6784\u5efa\u957f\u65f6\u5e8f\u4e3b\u52a8\u5f52\u7eb3\u4ea4\u4e92\u573a\u666f\uff0c\u5305\u542b\u6807\u51c6\u57fa\u51c6\u96c6OdysseyArena-Lite\u548c\u6781\u7aef\u957f\u65f6\u5e8f\u6d4b\u8bd5OdysseyArena-Challenge\u3002", "result": "\u5bf915+\u4e3b\u6d41\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u6a21\u578b\u5728\u5f52\u7eb3\u573a\u666f\u4e2d\u5b58\u5728\u663e\u8457\u7f3a\u9677\uff0c\u63ed\u793a\u4e86\u590d\u6742\u73af\u5883\u81ea\u4e3b\u53d1\u73b0\u7684\u5173\u952e\u74f6\u9888\u3002", "conclusion": "\u5f53\u524d\u6700\u5148\u8fdb\u7684\u8bed\u8a00\u6a21\u578b\u5728\u957f\u671f\u3001\u4e3b\u52a8\u53ca\u5f52\u7eb3\u5f0f\u4ea4\u4e92\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u81ea\u4e3b\u53d1\u73b0\u548c\u590d\u6742\u73af\u5883\u5e94\u5bf9\u80fd\u529b\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.05853", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05853", "abs": "https://arxiv.org/abs/2602.05853", "authors": ["Siran Liu", "Guoxia Wang", "Sa Wang", "Jinle Zeng", "HaoYang Xie", "Siyu Lou", "JiaBin Yang", "DianHai Yu", "Haifeng Wang", "Chao Yang"], "title": "RRAttention: Dynamic Block Sparse Attention via Per-Head Round-Robin Shifts for Long-Context Inference", "comment": null, "summary": "The quadratic complexity of attention mechanisms poses a critical bottleneck for large language models processing long contexts. While dynamic sparse attention methods offer input-adaptive efficiency, they face fundamental trade-offs: requiring preprocessing, lacking global evaluation, violating query independence, or incurring high computational overhead. We present RRAttention, a novel dynamic sparse attention method that simultaneously achieves all desirable properties through a head \\underline{r}ound-\\underline{r}obin (RR) sampling strategy. By rotating query sampling positions across attention heads within each stride, RRAttention maintains query independence while enabling efficient global pattern discovery with stride-level aggregation. Our method reduces complexity from $O(L^2)$ to $O(L^2/S^2)$ and employs adaptive Top-$\u03c4$ selection for optimal sparsity. Extensive experiments on natural language understanding (HELMET) and multimodal video comprehension (Video-MME) demonstrate that RRAttention recovers over 99\\% of full attention performance while computing only half of the attention blocks, achieving 2.4$\\times$ speedup at 128K context length and outperforming existing dynamic sparse attention methods.", "AI": {"tldr": "RRAttention\u63d0\u51fa\u4e86\u4e00\u79cd\u5934\u90e8\u5faa\u73af\u91c7\u6837\u7684\u52a8\u6001\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u4e14\u4fdd\u6301\u9ad8\u6027\u80fd\uff0c\u5728\u8d85\u957f\u4e0a\u4e0b\u6587\u4e0b\u5b9e\u73b02.4\u500d\u52a0\u901f\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u4f20\u7edf\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u9650\u5236\u4e86\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u7684\u80fd\u529b\uff0c\u800c\u73b0\u6709\u52a8\u6001\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u5b58\u5728\u9884\u5904\u7406\u9700\u6c42\u3001\u7f3a\u4e4f\u5168\u5c40\u8bc4\u4f30\u3001\u7834\u574f\u67e5\u8be2\u72ec\u7acb\u6027\u6216\u8ba1\u7b97\u5f00\u9500\u5927\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5934\u90e8\u5faa\u73af\u91c7\u6837\uff08round-robin sampling\uff09\u7b56\u7565\uff0c\u901a\u8fc7\u5728\u6ce8\u610f\u529b\u5934\u4e4b\u95f4\u5faa\u73af\u65cb\u8f6c\u67e5\u8be2\u91c7\u6837\u4f4d\u7f6e\uff0c\u5e76\u5728stride\u7ea7\u522b\u8fdb\u884c\u805a\u5408\uff0c\u5b9e\u73b0\u4e86\u52a8\u6001\u7a00\u758f\u6ce8\u610f\u529b\uff0c\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u5e76\u91c7\u7528\u81ea\u9002\u5e94Top-\u03c4\u9009\u62e9\u5b9e\u73b0\u6700\u4f18\u7a00\u758f\u6027\u3002", "result": "\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u548c\u591a\u6a21\u6001\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\uff0cRRAttention\u6062\u590d\u4e86\u8d85\u8fc799%\u7684\u5168\u6ce8\u610f\u529b\u6027\u80fd\uff0c\u4ec5\u8ba1\u7b97\u4e00\u534a\u7684\u6ce8\u610f\u529b\u5757\uff0c\u5728128K\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\u5b9e\u73b0\u4e862.4\u500d\u52a0\u901f\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "RRAttention\u901a\u8fc7\u5934\u90e8\u5faa\u73af\u91c7\u6837\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u6001\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u4e2d\u7684\u57fa\u672c\u6743\u8861\uff0c\u5728\u4fdd\u6301\u67e5\u8be2\u72ec\u7acb\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u5168\u5c40\u6a21\u5f0f\u53d1\u73b0\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u6587\u672c\u5904\u7406\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2602.05874", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05874", "abs": "https://arxiv.org/abs/2602.05874", "authors": ["Adri\u00e1n Gir\u00f3n", "Pablo Miralles", "Javier Huertas-Tato", "Sergio D'Antonio", "David Camacho"], "title": "xList-Hate: A Checklist-Based Framework for Interpretable and Generalizable Hate Speech Detection", "comment": null, "summary": "Hate speech detection is commonly framed as a direct binary classification problem despite being a composite concept defined through multiple interacting factors that vary across legal frameworks, platform policies, and annotation guidelines. As a result, supervised models often overfit dataset-specific definitions and exhibit limited robustness under domain shift and annotation noise.\n  We introduce xList-Hate, a diagnostic framework that decomposes hate speech detection into a checklist of explicit, concept-level questions grounded in widely shared normative criteria. Each question is independently answered by a large language model (LLM), producing a binary diagnostic representation that captures hateful content features without directly predicting the final label. These diagnostic signals are then aggregated by a lightweight, fully interpretable decision tree, yielding transparent and auditable predictions.\n  We evaluate it across multiple hate speech benchmarks and model families, comparing it against zero-shot LLM classification and in-domain supervised fine-tuning. While supervised methods typically maximize in-domain performance, we consistently improves cross-dataset robustness and relative performance under domain shift. In addition, qualitative analysis of disagreement cases provides evidence that the framework can be less sensitive to certain forms of annotation inconsistency and contextual ambiguity. Crucially, the approach enables fine-grained interpretability through explicit decision paths and factor-level analysis.\n  Our results suggest that reframing hate speech detection as a diagnostic reasoning task, rather than a monolithic classification problem, provides a robust, explainable, and extensible alternative for content moderation.", "AI": {"tldr": "xList-Hate\u901a\u8fc7\u591a\u7ef4\u8bca\u65ad\u95ee\u9898\u548c\u51b3\u7b56\u6811\u878d\u5408\u63d0\u5347\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u7684\u8de8\u57df\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u63d0\u4f9b\u5185\u5bb9\u5ba1\u6838\u7684\u65b0\u601d\u8def\u3002", "motivation": "\u4ec7\u6068\u8a00\u8bba\u5b9a\u4e49\u590d\u6742\uff0c\u53d7\u4e0d\u540c\u6cd5\u5f8b\u3001\u5e73\u53f0\u653f\u7b56\u53ca\u6807\u6ce8\u6807\u51c6\u5f71\u54cd\uff0c\u4f20\u7edf\u76d1\u7763\u6a21\u578b\u6613\u8fc7\u62df\u5408\u7279\u5b9a\u6570\u636e\u96c6\uff0c\u8868\u73b0\u51fa\u8f83\u5dee\u7684\u8de8\u57df\u548c\u6807\u6ce8\u566a\u58f0\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faxList-Hate\u8bca\u65ad\u6846\u67b6\uff0c\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u72ec\u7acb\u56de\u7b54\u591a\u4e2a\u6982\u5ff5\u5c42\u9762\u7684\u95ee\u9898\uff0c\u7ed3\u5408\u900f\u660e\u7684\u51b3\u7b56\u6811\u805a\u5408\u8bca\u65ad\u4fe1\u53f7\uff0c\u5b9e\u73b0\u4ec7\u6068\u8a00\u8bba\u7684\u7ec6\u7c92\u5ea6\u89e3\u91ca\u548c\u5224\u5b9a\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u8868\u73b0\u51fa\u4f18\u4e8e\u96f6\u6837\u672cLLM\u548c\u76d1\u7763\u5fae\u8c03\u7684\u8de8\u6570\u636e\u96c6\u9c81\u68d2\u6027\uff0c\u5e76\u4e14\u5bf9\u6807\u6ce8\u4e0d\u4e00\u81f4\u548c\u4e0a\u4e0b\u6587\u6a21\u68f1\u4e24\u53ef\u60c5\u51b5\u66f4\u4e0d\u654f\u611f\uff0c\u4e14\u8f93\u51fa\u900f\u660e\u3001\u53ef\u5ba1\u8ba1\u3002", "conclusion": "\u5c06\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u91cd\u65b0\u6784\u5efa\u4e3a\u8bca\u65ad\u63a8\u7406\u4efb\u52a1\uff0c\u800c\u975e\u5355\u4e00\u5206\u7c7b\u95ee\u9898\uff0c\u80fd\u591f\u63d0\u5347\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3001\u89e3\u91ca\u6027\u548c\u8de8\u57df\u9002\u5e94\u80fd\u529b\u3002"}}
{"id": "2602.05879", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.05879", "abs": "https://arxiv.org/abs/2602.05879", "authors": ["Miguel Moura Ramos", "Duarte M. Alves", "Hippolyte Gisserot-Boukhlef", "Jo\u00e3o Alves", "Pedro Henrique Martins", "Patrick Fernandes", "Jos\u00e9 Pombal", "Nuno M. Guerreiro", "Ricardo Rei", "Nicolas Boizard", "Amin Farajian", "Mateusz Klimaszewski", "Jos\u00e9 G. C. de Souza", "Barry Haddow", "Fran\u00e7ois Yvon", "Pierre Colombo", "Alexandra Birch", "Andr\u00e9 F. T. Martins"], "title": "EuroLLM-22B: Technical Report", "comment": null, "summary": "This report presents EuroLLM-22B, a large language model trained from scratch to support the needs of European citizens by covering all 24 official European Union languages and 11 additional languages. EuroLLM addresses the issue of European languages being underrepresented and underserved in existing open large language models. We provide a comprehensive overview of EuroLLM-22B's development, including tokenizer design, architectural specifications, data filtering, and training procedures. Across a broad set of multilingual benchmarks, EuroLLM-22B demonstrates strong performance in reasoning, instruction following, and translation, achieving results competitive with models of comparable size. To support future research, we release our base and instruction-tuned models, our multilingual web pretraining data and updated EuroBlocks instruction datasets, as well as our pre-training and evaluation codebases.", "AI": {"tldr": "EuroLLM-22B\u662f\u9488\u5bf9\u6b27\u6d32\u591a\u8bed\u8a00\u9700\u6c42\u4e13\u95e8\u8bad\u7ec3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u63d0\u4f9b\u5f3a\u52b2\u591a\u8bed\u8a00\u80fd\u529b\u5e76\u5f00\u6e90\u76f8\u5173\u8d44\u6e90\u3002", "motivation": "\u89e3\u51b3\u73b0\u5b58\u5f00\u653e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u6b27\u6d32\u591a\u8bed\u8a00\u652f\u6301\u4e0d\u8db3\u3001\u670d\u52a1\u4e0d\u5145\u5206\u7684\u95ee\u9898\u3002", "method": "\u4ece\u96f6\u5f00\u59cb\u8bad\u7ec322B\u53c2\u6570\u89c4\u6a21\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u8bbe\u8ba1\u4e13\u7528\u5206\u8bcd\u5668\uff0c\u5236\u5b9a\u67b6\u6784\u89c4\u683c\uff0c\u8fdb\u884c\u6570\u636e\u8fc7\u6ee4\u4e0e\u8bad\u7ec3\uff0c\u6d4b\u8bd5\u591a\u8bed\u8a00\u57fa\u51c6\u3002", "result": "EuroLLM-22B\u5728\u591a\u8bed\u8a00\u63a8\u7406\u3001\u6307\u4ee4\u6267\u884c\u4e0e\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u6210\u7ee9\u8fbe\u5230\u540c\u7b49\u89c4\u6a21\u6a21\u578b\u7684\u7ade\u4e89\u6c34\u5e73\u3002", "conclusion": "EuroLLM-22B\u5728\u8986\u76d6\u6240\u6709\u6b27\u76df\u5b98\u65b9\u8bed\u8a00\u53ca\u989d\u591611\u79cd\u8bed\u8a00\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u6b27\u6d32\u6c11\u4f17\u7684\u591a\u8bed\u8a00\u9700\u6c42\uff0c\u6027\u80fd\u4e0e\u540c\u89c4\u6a21\u6a21\u578b\u7ade\u4e89\u529b\u5f3a\u3002"}}
{"id": "2602.05897", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05897", "abs": "https://arxiv.org/abs/2602.05897", "authors": ["Shuo Nie", "Hexuan Deng", "Chao Wang", "Ruiyu Fang", "Xuebo Liu", "Shuangyong Song", "Yu Li", "Min Zhang", "Xuelong Li"], "title": "Stop Rewarding Hallucinated Steps: Faithfulness-Aware Step-Level Reinforcement Learning for Small Reasoning Models", "comment": null, "summary": "As large language models become smaller and more efficient, small reasoning models (SRMs) are crucial for enabling chain-of-thought (CoT) reasoning in resource-constrained settings. However, they are prone to faithfulness hallucinations, especially in intermediate reasoning steps. Existing mitigation methods based on online reinforcement learning rely on outcome-based rewards or coarse-grained CoT evaluation, which can inadvertently reinforce unfaithful reasoning when the final answer is correct. To address these limitations, we propose Faithfulness-Aware Step-Level Reinforcement Learning (FaithRL), introducing step-level supervision via explicit faithfulness rewards from a process reward model, together with an implicit truncated resampling strategy that generates contrastive signals from faithful prefixes. Experiments across multiple SRMs and Open-Book QA benchmarks demonstrate that FaithRL consistently reduces hallucinations in both the CoT and final answers, leading to more faithful and reliable reasoning. Code is available at https://github.com/Easy195/FaithRL.", "AI": {"tldr": "\u9488\u5bf9\u5c0f\u63a8\u7406\u6a21\u578b\u4e2d\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51faFaithRL\u65b9\u6cd5\uff0c\u901a\u8fc7\u6b65\u9aa4\u7ea7\u771f\u5b9e\u5956\u52b1\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u6709\u6548\u589e\u5f3a\u63a8\u7406\u7684\u771f\u5b9e\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u5c0f\u63a8\u7406\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u6267\u884c\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u65f6\u5bb9\u6613\u51fa\u73b0\u771f\u5b9e\u6027\u5e7b\u89c9\uff0c\u73b0\u6709\u57fa\u4e8e\u7ed3\u679c\u5956\u52b1\u6216\u7c97\u7c92\u5ea6\u8bc4\u4f30\u7684\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u6291\u5236\u4e0d\u771f\u5b9e\u7684\u63a8\u7406\u3002", "method": "\u63d0\u51fa\u4e86Faithfulness-Aware Step-Level Reinforcement Learning\uff08FaithRL\uff09\uff0c\u5229\u7528\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u63d0\u4f9b\u7684\u663e\u5f0f\u6b65\u9aa4\u7ea7\u771f\u5b9e\u6027\u5956\u52b1\u548c\u9690\u5f0f\u622a\u65ad\u91cd\u91c7\u6837\u7b56\u7565\uff0c\u751f\u6210\u5bf9\u6bd4\u4fe1\u53f7\u4ee5\u76d1\u7763\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u3002", "result": "\u5728\u591a\u4e2a\u5c0f\u63a8\u7406\u6a21\u578b\u548c\u5f00\u653e\u4e66\u7c4d\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFaithRL\u663e\u8457\u51cf\u5c11\u4e86\u94fe\u5f0f\u601d\u7ef4\u548c\u6700\u7ec8\u7b54\u6848\u4e2d\u7684\u5e7b\u89c9\uff0c\u63d0\u5347\u4e86\u63a8\u7406\u7684\u771f\u5b9e\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "FaithRL\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165\u6b65\u9aa4\u7ea7\u522b\u7684\u771f\u5b9e\u6027\u5956\u52b1\u548c\u5bf9\u6bd4\u4fe1\u53f7\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u5c0f\u63a8\u7406\u6a21\u578b\u5728\u94fe\u5f0f\u601d\u7ef4\u4e2d\u7684\u5e7b\u89c9\u73b0\u8c61\uff0c\u63d0\u9ad8\u4e86\u63a8\u7406\u7684\u771f\u5b9e\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2602.05905", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05905", "abs": "https://arxiv.org/abs/2602.05905", "authors": ["Letian Peng", "Yupeng Hou", "Kun Zhou", "Jingbo Shang"], "title": "Codified Finite-state Machines for Role-playing", "comment": null, "summary": "Modeling latent character states is crucial for consistent and engaging role-playing (RP) with large language models (LLMs). Yet, existing prompting-based approaches mainly capture surface actions, often failing to track the latent states that drive interaction. We revisit finite-state machines (FSMs), long used in game design to model state transitions. While effective in small, well-specified state spaces, traditional hand-crafted, rule-based FSMs struggle to adapt to the open-ended semantic space of RP. To address this, we introduce Codified Finite-State Machines (CFSMs), a framework that automatically codifies textual character profiles into FSMs using LLM-based coding. CFSMs extract key states and transitions directly from the profile, producing interpretable structures that enforce character consistency. To further capture uncertainty and variability, we extend CFSMs into Codified Probabilistic Finite-State Machines (CPFSMs), where transitions are modeled as probability distributions over states. Through both synthetic evaluations and real-world RP scenarios in established artifacts, we demonstrate that CFSM and CPFSM outperform generally applied baselines, verifying effectiveness not only in structured tasks but also in open-ended stochastic state exploration.", "AI": {"tldr": "\u672c\u8bba\u6587\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7f16\u7801\u7684\u6709\u9650\u72b6\u6001\u673a\u65b9\u6cd5\uff0c\u6709\u6548\u6355\u6349\u6f5c\u5728\u89d2\u8272\u72b6\u6001\uff0c\u63d0\u5347\u89d2\u8272\u626e\u6f14\u7684\u4e00\u81f4\u6027\u548c\u4f53\u9a8c\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u4e3b\u8981\u6355\u6349\u8868\u9762\u52a8\u4f5c\uff0c\u96be\u4ee5\u8ffd\u8e2a\u9a71\u52a8\u4ea4\u4e92\u7684\u6f5c\u5728\u89d2\u8272\u72b6\u6001\uff0c\u4f20\u7edf\u624b\u5de5\u8bbe\u8ba1\u7684\u6709\u9650\u72b6\u6001\u673a\u96be\u9002\u5e94\u89d2\u8272\u626e\u6f14\u7684\u5f00\u653e\u8bed\u4e49\u7a7a\u95f4\u3002", "method": "\u63d0\u51fa\u4e86Codified Finite-State Machines(CFSMs)\u7528LLM\u81ea\u52a8\u7f16\u7801\u6587\u672c\u89d2\u8272\u7b80\u4ecb\u751f\u6210FSM\u7ed3\u6784\uff0c\u5e76\u6269\u5c55\u4e3a\u6982\u7387FSM(CPFSM)\u4ee5\u5904\u7406\u72b6\u6001\u8f6c\u6362\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u591a\u6837\u6027\u3002", "result": "\u5728\u5408\u6210\u8bc4\u6d4b\u548c\u771f\u5b9e\u89d2\u8272\u626e\u6f14\u573a\u666f\u4e2d\uff0cCFSM\u548cCPFSM\u6846\u67b6\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u5728\u7ed3\u6784\u5316\u4efb\u52a1\u548c\u5f00\u653e\u5f0f\u968f\u673a\u72b6\u6001\u63a2\u7d22\u4e2d\u63d0\u5347\u89d2\u8272\u72b6\u6001\u5efa\u6a21\u6548\u679c\u3002", "conclusion": "CFSM\u548cCPFSM\u6846\u67b6\u5728\u89d2\u8272\u626e\u6f14\u4e2d\u80fd\u6709\u6548\u6355\u6349\u6f5c\u5728\u89d2\u8272\u72b6\u6001\uff0c\u63d0\u5347\u89d2\u8272\u4e00\u81f4\u6027\u548c\u4e92\u52a8\u8d28\u91cf\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2602.05929", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05929", "abs": "https://arxiv.org/abs/2602.05929", "authors": ["Jian Chen", "Zhuoran Wang", "Jiayu Qin", "Ming Li", "Meng Wang", "Changyou Chen", "Yin Chen", "Qizhen Weng", "Yirui Liu"], "title": "KV-CoRE: Benchmarking Data-Dependent Low-Rank Compressibility of KV-Caches in LLMs", "comment": null, "summary": "Large language models rely on kv-caches to avoid redundant computation during autoregressive decoding, but as context length grows, reading and writing the cache can quickly saturate GPU memory bandwidth. Recent work has explored KV-cache compression, yet most approaches neglect the data-dependent nature of kv-caches and their variation across layers. We introduce KV-CoRE KV-cache Compressibility by Rank Evaluation), an SVD-based method for quantifying the data-dependent low-rank compressibility of kv-caches. KV-CoRE computes the optimal low-rank approximation under the Frobenius norm and, being gradient-free and incremental, enables efficient dataset-level, layer-wise evaluation. Using this method, we analyze multiple models and datasets spanning five English domains and sixteen languages, uncovering systematic patterns that link compressibility to model architecture, training data, and language coverage. As part of this analysis, we employ the Normalized Effective Rank as a metric of compressibility and show that it correlates strongly with performance degradation under compression. Our study establishes a principled evaluation framework and the first large-scale benchmark of kv-cache compressibility in LLMs, offering insights for dynamic, data-aware compression and data-centric model development.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cdSVD\u57fa\u7840\u7684kv-cache\u538b\u7f29\u6027\u8bc4\u4f30\u65b9\u6cd5KV-CoRE\uff0c\u6784\u5efa\u4e86\u9996\u4e2a\u5927\u89c4\u6a21kv-cache\u538b\u7f29\u6027\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u538b\u7f29\u6027\u4e0e\u6a21\u578b\u53ca\u8bed\u8a00\u7279\u6027\u95f4\u7684\u5173\u7cfb\uff0c\u4e3a\u52a8\u6001\u6570\u636e\u611f\u77e5\u538b\u7f29\u63d0\u4f9b\u7406\u8bba\u652f\u6301\u3002", "motivation": "\u968f\u7740\u4e0a\u4e0b\u6587\u957f\u5ea6\u7684\u589e\u957f\uff0ckv-cache\u7684\u8bfb\u5199\u64cd\u4f5c\u5feb\u901f\u5360\u7528GPU\u5185\u5b58\u5e26\u5bbd\uff0c\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u672a\u5145\u5206\u8003\u8651kv-cache\u6570\u636e\u4f9d\u8d56\u6027\u53ca\u5404\u5c42\u95f4\u7684\u5dee\u5f02\uff0c\u56e0\u800c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u6570\u636e\u654f\u611f\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "KV-CoRE\u5229\u7528\u5947\u5f02\u503c\u5206\u89e3\uff08SVD\uff09\u8ba1\u7b97kv-cache\u7684\u6700\u4f18\u4f4e\u79e9\u8fd1\u4f3c\uff0c\u91c7\u7528\u65e0\u68af\u5ea6\u4e14\u589e\u91cf\u5f0f\u7684\u65b9\u6cd5\u5b9e\u73b0\u6570\u636e\u96c6\u5c42\u7ea7\u7684\u538b\u7f29\u6027\u8bc4\u4f30\uff0c\u4f7f\u7528\u5f52\u4e00\u5316\u6709\u6548\u79e9\u4f5c\u4e3a\u538b\u7f29\u6027\u6307\u6807\u3002", "result": "\u901a\u8fc7\u5728\u591a\u6a21\u578b\u3001\u591a\u6570\u636e\u96c6\u3001\u4e94\u4e2a\u82f1\u8bed\u9886\u57df\u548c\u5341\u516d\u79cd\u8bed\u8a00\u4e0a\u7684\u5206\u6790\uff0c\u53d1\u73b0kv-cache\u538b\u7f29\u6027\u4e0e\u6a21\u578b\u8bbe\u8ba1\u548c\u8bad\u7ec3\u6570\u636e\u6709\u7cfb\u7edf\u6027\u5173\u8054\uff0c\u540c\u65f6\u5f52\u4e00\u5316\u6709\u6548\u79e9\u80fd\u8f83\u597d\u5730\u9884\u6d4b\u538b\u7f29\u5e26\u6765\u7684\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86KV-CoRE\uff0c\u4e00\u79cd\u57fa\u4e8eSVD\u7684KV-cache\u4f4e\u79e9\u53ef\u538b\u7f29\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u5b9e\u73b0\u5728\u5927\u89c4\u6a21\u591a\u6a21\u578b\u591a\u8bed\u79cd\u73af\u5883\u4e0b\u7684\u9ad8\u6548\u538b\u7f29\u8bc4\u4f30\u3002\u8be5\u65b9\u6cd5\u63ed\u793a\u4e86\u538b\u7f29\u6027\u4e0e\u6a21\u578b\u67b6\u6784\u3001\u8bad\u7ec3\u6570\u636e\u53ca\u8bed\u8a00\u8986\u76d6\u7684\u7cfb\u7edf\u6027\u5173\u8054\uff0c\u5e76\u4e14\u63d0\u4f9b\u4e86\u6027\u80fd\u4e0b\u964d\u4e0e\u538b\u7f29\u6027\u4e4b\u95f4\u7684\u5b9a\u91cf\u5173\u7cfb\u3002"}}
{"id": "2602.05932", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05932", "abs": "https://arxiv.org/abs/2602.05932", "authors": ["L\u00e9o Labat", "Etienne Ollion", "Fran\u00e7ois Yvon"], "title": "Polyglots or Multitudes? Multilingual LLM Answers to Value-laden Multiple-Choice Questions", "comment": "17 pages, 5 figures (8 pages of references and appendices)", "summary": "Multiple-Choice Questions (MCQs) are often used to assess knowledge, reasoning abilities, and even values encoded in large language models (LLMs). While the effect of multilingualism has been studied on LLM factual recall, this paper seeks to investigate the less explored question of language-induced variation in value-laden MCQ responses. Are multilingual LLMs consistent in their responses across languages, i.e. behave like theoretical polyglots, or do they answer value-laden MCQs depending on the language of the question, like a multitude of monolingual models expressing different values through a single model? We release a new corpus, the Multilingual European Value Survey (MEVS), which, unlike prior work relying on machine translation or ad hoc prompts, solely comprises human-translated survey questions aligned in 8 European languages. We administer a subset of those questions to over thirty multilingual LLMs of various sizes, manufacturers and alignment-fine-tuning status under comprehensive, controlled prompt variations including answer order, symbol type, and tail character. Our results show that while larger, instruction-tuned models display higher overall consistency, the robustness of their responses varies greatly across questions, with certain MCQs eliciting total agreement within and across models while others leave LLM answers split. Language-specific behavior seems to arise in all consistent, instruction-fine-tuned models, but only on certain questions, warranting a further study of the selective effect of preference fine-tuning.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u901a\u8fc7\u4eba\u5de5\u7ffb\u8bd1\u7684\u591a\u8bed\u8a00\u4ef7\u503c\u89c2\u8c03\u67e5\u6570\u636e\uff0c\u7cfb\u7edf\u8bc4\u4f30\u591a\u8bed\u8a00\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ef7\u503c\u89c2\u76f8\u5173\u591a\u9009\u9898\u4e2d\u7684\u8bed\u8a00\u4f9d\u8d56\u6027\uff0c\u53d1\u73b0\u6a21\u578b\u8868\u73b0\u867d\u53d7\u89c4\u6a21\u548c\u5fae\u8c03\u5f71\u54cd\uff0c\u4f46\u8bed\u8a00\u5dee\u5f02\u4ecd\u5bfc\u81f4\u56de\u7b54\u4e0d\u4e00\u81f4\u3002", "motivation": "\u63a2\u7a76\u591a\u8bed\u8a00\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5305\u542b\u4ef7\u503c\u89c2\u5185\u5bb9\u7684\u591a\u9009\u9898\uff08MCQs\uff09\u4e2d\u662f\u5426\u80fd\u5728\u4e0d\u540c\u8bed\u8a00\u4e2d\u4fdd\u6301\u56de\u7b54\u4e00\u81f4\u6027\uff0c\u89e3\u51b3\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4e8b\u5b9e\u56de\u5fc6\u800c\u5ffd\u7565\u8bed\u8a00\u5bf9\u4ef7\u503c\u89c2\u8868\u8fbe\u5f71\u54cd\u7684\u4e0d\u8db3\u3002", "method": "\u6784\u5efa\u5e76\u53d1\u5e03\u4e86\u591a\u8bed\u8a00\u6b27\u6d32\u4ef7\u503c\u89c2\u8c03\u67e5\uff08MEVS\uff09\u6570\u636e\u96c6\uff0c\u6db5\u76d68\u79cd\u6b27\u6d32\u8bed\u8a00\uff0c\u6240\u6709\u95ee\u9898\u5747\u4e3a\u4eba\u5de5\u7ffb\u8bd1\uff0c\u907f\u514d\u673a\u5668\u7ffb\u8bd1\u5e26\u6765\u7684\u504f\u5dee\uff1b\u968f\u540e\u572830\u591a\u4e2a\u4e0d\u540c\u89c4\u6a21\u3001\u5236\u9020\u5546\u548c\u5fae\u8c03\u72b6\u6001\u7684\u591a\u8bed\u8a00LLM\u4e0a\uff0c\u4f7f\u7528\u591a\u6837\u5316\u7684\u63d0\u793a\u7b56\u7565\uff08\u5305\u62ec\u7b54\u6848\u987a\u5e8f\u3001\u7b26\u53f7\u7c7b\u578b\u548c\u5c3e\u90e8\u5b57\u7b26\uff09\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u6307\u4ee4\u5fae\u8c03\u548c\u89c4\u6a21\u8f83\u5927\u7684\u6a21\u578b\u6574\u4f53\u4e0a\u56de\u7b54\u66f4\u5177\u4e00\u81f4\u6027\uff0c\u4f46\u7279\u5b9a\u95ee\u9898\u7684\u9c81\u68d2\u6027\u5dee\u5f02\u663e\u8457\uff1b\u8bed\u8a00\u7279\u5f02\u884c\u4e3a\u666e\u904d\u5b58\u5728\u4e8e\u6240\u6709\u6307\u4ee4\u5fae\u8c03\u7684\u591a\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u4f46\u4ec5\u5728\u7279\u5b9a\u95ee\u9898\u4e0a\u8868\u73b0\u660e\u663e\uff0c\u663e\u793a\u4e86\u504f\u597d\u5fae\u8c03\u7684\u9009\u62e9\u6027\u5f71\u54cd\u3002", "conclusion": "\u591a\u8bed\u8a00\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ef7\u503c\u89c2\u76f8\u5173\u7684\u591a\u9009\u9898\u56de\u7b54\u4e2d\u8868\u73b0\u51fa\u8bed\u8a00\u7279\u5f02\u6027\uff0c\u5373\u4f7f\u5728\u7ecf\u8fc7\u6307\u4ee4\u5fae\u8c03\u548c\u591a\u8bed\u8a00\u8bad\u7ec3\u540e\uff0c\u6a21\u578b\u5728\u4e0d\u540c\u8bed\u8a00\u4e0b\u7684\u56de\u7b54\u4e00\u81f4\u6027\u5b58\u5728\u5dee\u5f02\uff0c\u90e8\u5206\u95ee\u9898\u80fd\u8fbe\u5230\u8de8\u6a21\u578b\u548c\u8bed\u8a00\u7684\u5b8c\u5168\u4e00\u81f4\uff0c\u800c\u53e6\u4e00\u4e9b\u95ee\u9898\u5219\u8868\u73b0\u51fa\u5206\u6b67\u3002"}}
{"id": "2602.05940", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05940", "abs": "https://arxiv.org/abs/2602.05940", "authors": ["Junxiao Liu", "Zhijun Wang", "Yixiao Li", "Zhejian Lai", "Liqian Huang", "Xin Huang", "Xue Han", "Junlan Feng", "Shujian Huang"], "title": "Self-Improving Multilingual Long Reasoning via Translation-Reasoning Integrated Training", "comment": "16 pages, 11 figures", "summary": "Long reasoning models often struggle in multilingual settings: they tend to reason in English for non-English questions; when constrained to reasoning in the question language, accuracies drop substantially. The struggle is caused by the limited abilities for both multilingual question understanding and multilingual reasoning. To address both problems, we propose TRIT (Translation-Reasoning Integrated Training), a self-improving framework that integrates the training of translation into multilingual reasoning. Without external feedback or additional multilingual data, our method jointly enhances multilingual question understanding and response generation. On MMATH, our method outperforms multiple baselines by an average of 7 percentage points, improving both answer correctness and language consistency. Further analysis reveals that integrating translation training improves cross-lingual question alignment by over 10 percentage points and enhances translation quality for both mathematical questions and general-domain text, with gains up to 8.4 COMET points on FLORES-200.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u7ffb\u8bd1\u8bad\u7ec3\u4e0e\u591a\u8bed\u8a00\u63a8\u7406\u8054\u5408\u8bad\u7ec3\u7684\u6846\u67b6TRIT\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u8bed\u8a00\u957f\u63a8\u7406\u6a21\u578b\u7684\u51c6\u786e\u7387\u548c\u8bed\u8a00\u4e00\u81f4\u6027\u3002", "motivation": "\u957f\u63a8\u7406\u6a21\u578b\u5728\u591a\u8bed\u8a00\u573a\u666f\u4e2d\u5b58\u5728\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u51c6\u786e\u7387\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e86TRIT\u81ea\u6211\u6539\u8fdb\u6846\u67b6\uff0c\u5c06\u7ffb\u8bd1\u8bad\u7ec3\u4e0e\u591a\u8bed\u8a00\u63a8\u7406\u8bad\u7ec3\u76f8\u7ed3\u5408\uff0c\u65e0\u9700\u989d\u5916\u7684\u6570\u636e\u548c\u53cd\u9988\u3002", "result": "\u5728MMATH\u6570\u636e\u96c6\u4e0a\uff0cTRIT\u76f8\u6bd4\u591a\u79cd\u57fa\u7ebf\u65b9\u6cd5\u63d0\u5347\u4e86\u5e73\u57477\u4e2a\u767e\u5206\u70b9\u7684\u8868\u73b0\uff0c\u8de8\u8bed\u8a00\u95ee\u9898\u5bf9\u9f50\u63d0\u9ad8\u4e8610\u4e2a\u767e\u5206\u70b9\u4ee5\u4e0a\uff0c\u7ffb\u8bd1\u8d28\u91cf\u63d0\u5347\u8fbe8.4\u4e2aCOMET\u70b9\u3002", "conclusion": "TRIT\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u957f\u63a8\u7406\u6a21\u578b\u7684\u8868\u73b0\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7b54\u6848\u6b63\u786e\u7387\u548c\u8bed\u8a00\u4e00\u81f4\u6027\u3002"}}
{"id": "2602.05971", "categories": ["cs.CL", "cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2602.05971", "abs": "https://arxiv.org/abs/2602.05971", "authors": ["Felipe D. Toro-Hern\u00e1ndez", "Jesuino Vieira Filho", "Rodrigo M. Cabral-Carvalho"], "title": "Characterizing Human Semantic Navigation in Concept Production as Trajectories in Embedding Space", "comment": "10 pages, 6 figures (excluding refs/appendix). Accepted to ICLR 2026", "summary": "Semantic representations can be framed as a structured, dynamic knowledge space through which humans navigate to retrieve and manipulate meaning. To investigate how humans traverse this geometry, we introduce a framework that represents concept production as navigation through embedding space. Using different transformer text embedding models, we construct participant-specific semantic trajectories based on cumulative embeddings and extract geometric and dynamical metrics, including distance to next, distance to centroid, entropy, velocity, and acceleration. These measures capture both scalar and directional aspects of semantic navigation, providing a computationally grounded view of semantic representation search as movement in a geometric space. We evaluate the framework on four datasets across different languages, spanning different property generation tasks: Neurodegenerative, Swear verbal fluency, Property listing task in Italian, and in German. Across these contexts, our approach distinguishes between clinical groups and concept types, offering a mathematical framework that requires minimal human intervention compared to typical labor-intensive linguistic pre-processing methods. Comparison with a non-cumulative approach reveals that cumulative embeddings work best for longer trajectories, whereas shorter ones may provide too little context, favoring the non-cumulative alternative. Critically, different embedding models yielded similar results, highlighting similarities between different learned representations despite different training pipelines. By framing semantic navigation as a structured trajectory through embedding space, bridging cognitive modeling with learned representation, thereby establishing a pipeline for quantifying semantic representation dynamics with applications in clinical research, cross-linguistic analysis, and the assessment of artificial cognition.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u6a21\u62df\u8bed\u4e49\u5bfc\u822a\u8f68\u8ff9\uff0c\u5229\u7528\u6587\u672c\u5d4c\u5165\u6a21\u578b\u548c\u51e0\u4f55\u52a8\u529b\u5b66\u6307\u6807\uff0c\u5b9e\u73b0\u5728\u591a\u8bed\u8a00\u4e34\u5e8a\u53ca\u6982\u5ff5\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u8bed\u4e49\u8868\u5f81\u52a8\u6001\u91cf\u5316\uff0c\u7b80\u5316\u4f20\u7edf\u8bed\u8a00\u5206\u6790\u3002", "motivation": "\u65e8\u5728\u63a2\u7d22\u4eba\u7c7b\u5982\u4f55\u5728\u7ed3\u6784\u5316\u3001\u52a8\u6001\u7684\u77e5\u8bc6\u7a7a\u95f4\u4e2d\u5bfc\u822a\u68c0\u7d22\u548c\u64cd\u7eb5\u8bed\u4e49\uff0c\u63d0\u4f9b\u6bd4\u4f20\u7edf\u8bed\u8a00\u9884\u5904\u7406\u66f4\u81ea\u52a8\u5316\u3001\u6570\u5b66\u5316\u7684\u8bed\u4e49\u8868\u5f81\u5206\u6790\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u4e0d\u540c\u7684Transformer\u6587\u672c\u5d4c\u5165\u6a21\u578b\uff0c\u6784\u5efa\u57fa\u4e8e\u7d2f\u79ef\u5d4c\u5165\u7684\u53c2\u4e0e\u8005\u7279\u5b9a\u8bed\u4e49\u8f68\u8ff9\uff0c\u63d0\u53d6\u8ddd\u79bb\u3001\u71b5\u3001\u901f\u5ea6\u3001\u52a0\u901f\u5ea6\u7b49\u51e0\u4f55\u548c\u52a8\u529b\u5b66\u6307\u6807\uff0c\u8bc4\u4f30\u56db\u4e2a\u4e0d\u540c\u8bed\u8a00\u548c\u4efb\u52a1\u7684\u6570\u636e\u96c6\u3002", "result": "\u8be5\u65b9\u6cd5\u6210\u529f\u533a\u5206\u4e34\u5e8a\u548c\u975e\u4e34\u5e8a\u7fa4\u4f53\u4ee5\u53ca\u4e0d\u540c\u6982\u5ff5\u7c7b\u578b\uff0c\u53d1\u73b0\u7d2f\u79ef\u5d4c\u5165\u5728\u957f\u8f68\u8ff9\u4e2d\u8868\u73b0\u66f4\u597d\uff0c\u4e0d\u540c\u5d4c\u5165\u6a21\u578b\u7ed3\u679c\u76f8\u4f3c\uff0c\u8868\u660e\u4e0d\u540c\u8bad\u7ec3\u65b9\u6cd5\u5f97\u5230\u7684\u8868\u5f81\u5177\u6709\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u8bed\u4e49\u8868\u793a\u89c6\u4e3a\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u8f68\u8ff9\u5bfc\u822a\u7684\u65b0\u6846\u67b6\uff0c\u80fd\u591f\u901a\u8fc7\u51e0\u4f55\u548c\u52a8\u529b\u5b66\u6307\u6807\u91cf\u5316\u4eba\u7c7b\u8bed\u4e49\u641c\u7d22\u8fc7\u7a0b\uff0c\u6709\u6548\u533a\u5206\u4e34\u5e8a\u7fa4\u4f53\u548c\u6982\u5ff5\u7c7b\u578b\uff0c\u5e76\u8de8\u8bed\u8a00\u548c\u4efb\u52a1\u5c55\u73b0\u9c81\u68d2\u6027\u3002"}}
{"id": "2602.05992", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05992", "abs": "https://arxiv.org/abs/2602.05992", "authors": ["Lizhuo Luo", "Shenggui Li", "Yonggang Wen", "Tianwei Zhang"], "title": "DSB: Dynamic Sliding Block Scheduling for Diffusion LLMs", "comment": null, "summary": "Diffusion large language models (dLLMs) have emerged as a promising alternative for text generation, distinguished by their native support for parallel decoding. In practice, block inference is crucial for avoiding order misalignment in global bidirectional decoding and improving output quality. However, the widely-used fixed, predefined block (naive) schedule is agnostic to semantic difficulty, making it a suboptimal strategy for both quality and efficiency: it can force premature commitments to uncertain positions while delaying easy positions near block boundaries. In this work, we analyze the limitations of naive block scheduling and disclose the importance of dynamically adapting the schedule to semantic difficulty for reliable and efficient inference. Motivated by this, we propose Dynamic Sliding Block (DSB), a training-free block scheduling method that uses a sliding block with a dynamic size to overcome the rigidity of the naive block. To further improve efficiency, we introduce DSB Cache, a training-free KV-cache mechanism tailored to DSB. Extensive experiments across multiple models and benchmarks demonstrate that DSB, together with DSB Cache, consistently improves both generation quality and inference efficiency for dLLMs. Code is released at https://github.com/lizhuo-luo/DSB.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u51fa\u52a8\u6001\u6ed1\u52a8\u5757\u8c03\u5ea6\u548c\u7f13\u5b58\u673a\u5236\uff0c\u63d0\u5347\u4e86\u6587\u672c\u751f\u6210\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u7684\u56fa\u5b9a\u9884\u5b9a\u4e49\u533a\u5757\u8c03\u5ea6\u672a\u8003\u8651\u8bed\u4e49\u96be\u5ea6\uff0c\u5bfc\u81f4\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u5bf9\u4e0d\u786e\u5b9a\u4f4d\u7f6e\u8fc7\u65e9\u786e\u5b9a\uff0c\u6216\u63a8\u8fdf\u5bb9\u6613\u4f4d\u7f6e\u751f\u6210\uff0c\u5f71\u54cd\u751f\u6210\u8d28\u91cf\u548c\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bad\u7ec3\u81ea\u7531\u7684\u52a8\u6001\u6ed1\u52a8\u5757\uff08DSB\uff09\u533a\u5757\u8c03\u5ea6\u65b9\u6cd5\uff0c\u6839\u636e\u8bed\u4e49\u96be\u5ea6\u52a8\u6001\u8c03\u6574\u533a\u5757\u5927\u5c0f\uff0c\u5e76\u8bbe\u8ba1\u4e86\u9488\u5bf9DSB\u7684\u8bad\u7ec3\u81ea\u7531KV\u7f13\u5b58\u673a\u5236\uff08DSB Cache\uff09\u3002", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDSB\u4e0eDSB Cache\u76f8\u7ed3\u5408\u540e\uff0c\u6301\u7eed\u63d0\u5347\u4e86\u6587\u672c\u751f\u6210\u8d28\u91cf\u548c\u63a8\u7406\u6548\u7387\u3002", "conclusion": "\u52a8\u6001\u6ed1\u52a8\u5757\uff08DSB\uff09\u8c03\u5ea6\u65b9\u6cd5\u548cDSB\u7f13\u5b58\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\uff08dLLMs\uff09\u7684\u751f\u6210\u8d28\u91cf\u548c\u63a8\u7406\u6548\u7387\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u56fa\u5b9a\u533a\u5757\u8c03\u5ea6\u7684\u4e0d\u8db3\u3002"}}
{"id": "2602.06015", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.06015", "abs": "https://arxiv.org/abs/2602.06015", "authors": ["Panagiotis Kaliosis", "Adithya V Ganesan", "Oscar N. E. Kjell", "Whitney Ringwald", "Scott Feltman", "Melissa A. Carr", "Dimitris Samaras", "Camilo Ruggero", "Benjamin J. Luft", "Roman Kotov", "Andrew H. Schwartz"], "title": "A Systematic Evaluation of Large Language Models for PTSD Severity Estimation: The Role of Contextual Knowledge and Modeling Strategies", "comment": "18 pages, 3 figures, 5 tables", "summary": "Large language models (LLMs) are increasingly being used in a zero-shot fashion to assess mental health conditions, yet we have limited knowledge on what factors affect their accuracy. In this study, we utilize a clinical dataset of natural language narratives and self-reported PTSD severity scores from 1,437 individuals to comprehensively evaluate the performance of 11 state-of-the-art LLMs. To understand the factors affecting accuracy, we systematically varied (i) contextual knowledge like subscale definitions, distribution summary, and interview questions, and (ii) modeling strategies including zero-shot vs few shot, amount of reasoning effort, model sizes, structured subscales vs direct scalar prediction, output rescaling and nine ensemble methods. Our findings indicate that (a) LLMs are most accurate when provided with detailed construct definitions and context of the narrative; (b) increased reasoning effort leads to better estimation accuracy; (c) performance of open-weight models (Llama, Deepseek), plateau beyond 70B parameters while closed-weight (o3-mini, gpt-5) models improve with newer generations; and (d) best performance is achieved when ensembling a supervised model with the zero-shot LLMs. Taken together, the results suggest choice of contextual knowledge and modeling strategies is important for deploying LLMs to accurately assess mental health.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u4e34\u5e8a\u6570\u636e\u8bc4\u4f3011\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5fc3\u7406\u5065\u5eb7\u8bc4\u4f30\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u63d0\u4f9b\u8be6\u7ec6\u4e0a\u4e0b\u6587\u548c\u5408\u7406\u5efa\u6a21\u7b56\u7565\u80fd\u663e\u8457\u63d0\u5347\u51c6\u786e\u7387\uff0c\u4e14\u6a21\u578b\u53c2\u6570\u89c4\u6a21\u4e0e\u96c6\u6210\u7b56\u7565\u5bf9\u6027\u80fd\u5f71\u54cd\u663e\u8457\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u6a21\u5f0f\u4e0b\u65e5\u76ca\u7528\u4e8e\u8bc4\u4f30\u5fc3\u7406\u5065\u5eb7\uff0c\u4f46\u5f71\u54cd\u5176\u51c6\u786e\u6027\u7684\u56e0\u7d20\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u5229\u7528\u5305\u542b1,437\u540d\u4e2a\u4f53\u7684\u4e34\u5e8a\u81ea\u7136\u8bed\u8a00\u53d9\u8ff0\u548cPTSD\u81ea\u6211\u8bc4\u5206\u6570\u636e\uff0c\u8bc4\u4f3011\u79cd\u5148\u8fdb\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8868\u73b0\uff0c\u7cfb\u7edf\u53d8\u6362\u4e0a\u4e0b\u6587\u77e5\u8bc6\u548c\u5efa\u6a21\u7b56\u7565\u6765\u5206\u6790\u5f71\u54cd\u51c6\u786e\u7387\u7684\u56e0\u7d20\u3002", "result": "\uff08a\uff09\u63d0\u4f9b\u8be6\u7ec6\u6784\u5ff5\u5b9a\u4e49\u548c\u53d9\u8ff0\u4e0a\u4e0b\u6587\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u51c6\u786e\u5ea6\uff1b\uff08b\uff09\u589e\u52a0\u63a8\u7406\u52aa\u529b\u63d0\u5347\u4f30\u8ba1\u51c6\u786e\u7387\uff1b\uff08c\uff09\u5f00\u653e\u6743\u91cd\u6a21\u578b\u572870B\u53c2\u6570\u540e\u6027\u80fd\u8d8b\u4e8e\u5e73\u7a33\uff0c\u95ed\u6743\u91cd\u6a21\u578b\u968f\u4ee3\u6570\u63d0\u5347\uff1b\uff08d\uff09\u76d1\u7763\u6a21\u578b\u4e0e\u96f6\u6837\u672c\u6a21\u578b\u7ed3\u5408\u7684\u96c6\u6210\u65b9\u5f0f\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u9009\u62e9\u5408\u9002\u7684\u4e0a\u4e0b\u6587\u77e5\u8bc6\u548c\u5efa\u6a21\u7b56\u7565\u5bf9\u4e8e\u51c6\u786e\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u5fc3\u7406\u5065\u5eb7\u72b6\u51b5\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2602.06019", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06019", "abs": "https://arxiv.org/abs/2602.06019", "authors": ["John Kirchenbauer", "Abhimanyu Hans", "Brian Bartoldson", "Micah Goldblum", "Ashwinee Panda", "Tom Goldstein"], "title": "Multi-Token Prediction via Self-Distillation", "comment": "8 pages and 5 figures in the main body", "summary": "Existing techniques for accelerating language model inference, such as speculative decoding, require training auxiliary speculator models and building and deploying complex inference pipelines. We consider a new approach for converting a pretrained autoregressive language model from a slow single next token prediction model into a fast standalone multi-token prediction model using a simple online distillation objective. The final model retains the exact same implementation as the pretrained initial checkpoint and is deployable without the addition of any auxiliary verifier or other specialized inference code. On GSM8K, our method produces models that can decode more than $3\\times$ faster on average at $<5\\%$ drop in accuracy relative to single token decoding performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7b80\u5355\u4e14\u65e0\u9700\u989d\u5916\u6a21\u578b\u7684\u5728\u7ebf\u84b8\u998f\u65b9\u6cd5\uff0c\u5927\u5e45\u52a0\u901f\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u63a8\u7406\uff0c\u6548\u679c\u663e\u8457\u4e14\u6613\u4e8e\u90e8\u7f72\u3002", "motivation": "\u73b0\u6709\u52a0\u901f\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7684\u65b9\u6cd5\u5982\u63a8\u6d4b\u89e3\u7801\u590d\u6742\u4e14\u4f9d\u8d56\u8f85\u52a9\u6a21\u578b\u53ca\u590d\u6742\u7ba1\u7ebf\uff0c\u4e9f\u9700\u7b80\u5316\u4e14\u9ad8\u6548\u7684\u63a8\u7406\u52a0\u901f\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5728\u7ebf\u84b8\u998f\u76ee\u6807\uff0c\u5c06\u9884\u8bad\u7ec3\u7684\u5355\u6807\u8bb0\u9884\u6d4b\u6a21\u578b\u8f6c\u6362\u4e3a\u53ef\u8fdb\u884c\u591a\u6807\u8bb0\u9884\u6d4b\u7684\u6a21\u578b\uff0c\u4fdd\u6301\u6a21\u578b\u7ed3\u6784\u4e0d\u53d8\uff0c\u65e0\u9700\u989d\u5916\u9a8c\u8bc1\u5668\u6216\u4e13\u95e8\u63a8\u7406\u4ee3\u7801\u3002", "result": "\u57fa\u4e8eGSM8K\u6570\u636e\u96c6\uff0c\u6240\u63d0\u65b9\u6cd5\u4f7f\u6a21\u578b\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u8d85\u8fc73\u500d\uff0c\u4e14\u5728\u51c6\u786e\u7387\u4e0a\u4ec5\u6709\u4e0d\u52305%\u7684\u4e0b\u964d\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5728\u7ebf\u84b8\u998f\u7684\u7b80\u5355\u65b9\u6cd5\uff0c\u5c06\u9884\u8bad\u7ec3\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u8f6c\u5316\u4e3a\u9ad8\u6548\u7684\u591a\u6807\u8bb0\u9884\u6d4b\u6a21\u578b\uff0c\u65e0\u9700\u590d\u6742\u63a8\u7406\u7ba1\u7ebf\u548c\u8f85\u52a9\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u3002"}}
{"id": "2602.06025", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06025", "abs": "https://arxiv.org/abs/2602.06025", "authors": ["Haozhen Zhang", "Haodong Yue", "Tao Feng", "Quanyu Long", "Jianzhu Bao", "Bowen Jin", "Weizhi Zhang", "Xiao Li", "Jiaxuan You", "Chengwei Qin", "Wenya Wang"], "title": "Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory", "comment": "Code is available at https://github.com/ViktorAxelsen/BudgetMem", "summary": "Memory is increasingly central to Large Language Model (LLM) agents operating beyond a single context window, yet most existing systems rely on offline, query-agnostic memory construction that can be inefficient and may discard query-critical information. Although runtime memory utilization is a natural alternative, prior work often incurs substantial overhead and offers limited explicit control over the performance-cost trade-off. In this work, we present \\textbf{BudgetMem}, a runtime agent memory framework for explicit, query-aware performance-cost control. BudgetMem structures memory processing as a set of memory modules, each offered in three budget tiers (i.e., \\textsc{Low}/\\textsc{Mid}/\\textsc{High}). A lightweight router performs budget-tier routing across modules to balance task performance and memory construction cost, which is implemented as a compact neural policy trained with reinforcement learning. Using BudgetMem as a unified testbed, we study three complementary strategies for realizing budget tiers: implementation (method complexity), reasoning (inference behavior), and capacity (module model size). Across LoCoMo, LongMemEval, and HotpotQA, BudgetMem surpasses strong baselines when performance is prioritized (i.e., high-budget setting), and delivers better accuracy-cost frontiers under tighter budgets. Moreover, our analysis disentangles the strengths and weaknesses of different tiering strategies, clarifying when each axis delivers the most favorable trade-offs under varying budget regimes.", "AI": {"tldr": "BudgetMem\u63d0\u51fa\u4e00\u79cd\u8fd0\u884c\u65f6\u3001\u67e5\u8be2\u611f\u77e5\u7684\u591a\u5c42\u6b21\u8bb0\u5fc6\u7ba1\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u6027\u80fd\u6210\u672c\u6743\u8861\uff0c\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u7684\u8bb0\u5fc6\u5229\u7528\u6548\u7387\u548c\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u7684\u8bb0\u5fc6\u5927\u591a\u79bb\u7ebf\u6784\u5efa\u4e14\u5ffd\u89c6\u67e5\u8be2\u7279\u5f02\u6027\uff0c\u6548\u7387\u4f4e\u4e0b\u4e14\u53ef\u80fd\u4e22\u5931\u5173\u952e\u4fe1\u606f\uff1b\u800c\u8fd0\u884c\u65f6\u8bb0\u5fc6\u5229\u7528\u867d\u7136\u81ea\u7136\uff0c\u4f46\u901a\u5e38\u5f00\u9500\u5927\u4e14\u96be\u4ee5\u6743\u8861\u6027\u80fd\u4e0e\u6210\u672c\u3002", "method": "\u63d0\u51faBudgetMem\uff0c\u4e00\u4e2a\u5305\u542b\u4f4e\u3001\u4e2d\u3001\u9ad8\u4e09\u79cd\u9884\u7b97\u5c42\u6b21\u7684\u8bb0\u5fc6\u6a21\u5757\u4f53\u7cfb\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u8def\u7531\u5668\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u7d27\u51d1\u795e\u7ecf\u7b56\u7565\uff0c\u5b9e\u73b0\u6027\u80fd\u4e0e\u6210\u672c\u7684\u5e73\u8861\u3002\u7814\u7a76\u4e86\u5b9e\u73b0\u590d\u6742\u5ea6\u3001\u63a8\u7406\u884c\u4e3a\u548c\u6a21\u5757\u5bb9\u91cf\u4e09\u79cd\u9884\u7b97\u5c42\u9762\u7b56\u7565\u3002", "result": "\u9884\u7b97\u5c42\u6b21\u8bbe\u8ba1\u4f7fBudgetMem\u5728\u9ad8\u9884\u7b97\u6761\u4ef6\u4e0b\u6027\u80fd\u8d85\u8d8a\u5f3a\u57fa\u7ebf\uff0c\u5728\u4f4e\u9884\u7b97\u4e0b\u5c55\u73b0\u66f4\u4f73\u51c6\u786e\u5ea6-\u6210\u672c\u6743\u8861\uff0c\u5206\u6790\u63ed\u793a\u4e86\u4e0d\u540c\u7b56\u7565\u5728\u4e0d\u540c\u9884\u7b97 regimes\u4e0b\u7684\u9002\u7528\u6027\u548c\u4f18\u52bf\u3002", "conclusion": "BudgetMem\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u548c\u67e5\u8be2\u611f\u77e5\u7684\u6027\u80fd\u4e0e\u6210\u672c\u63a7\u5236\uff0c\u5b9e\u73b0\u4e86\u8fd0\u884c\u65f6\u8bb0\u5fc6\u7684\u9ad8\u6548\u5229\u7528\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5728\u4e0d\u540c\u9884\u7b97\u6761\u4ef6\u4e0b\u5c55\u793a\u4e86\u66f4\u4f18\u7684\u51c6\u786e\u5ea6-\u6210\u672c\u6743\u8861\u3002"}}
{"id": "2602.06036", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.06036", "abs": "https://arxiv.org/abs/2602.06036", "authors": ["Jian Chen", "Yesheng Liang", "Zhijian Liu"], "title": "DFlash: Block Diffusion for Flash Speculative Decoding", "comment": null, "summary": "Autoregressive large language models (LLMs) deliver strong performance but require inherently sequential decoding, leading to high inference latency and poor GPU utilization. Speculative decoding mitigates this bottleneck by using a fast draft model whose outputs are verified in parallel by the target LLM; however, existing methods still rely on autoregressive drafting, which remains sequential and limits practical speedups. Diffusion LLMs offer a promising alternative by enabling parallel generation, but current diffusion models typically underperform compared with autoregressive models. In this paper, we introduce DFlash, a speculative decoding framework that employs a lightweight block diffusion model for parallel drafting. By generating draft tokens in a single forward pass and conditioning the draft model on context features extracted from the target model, DFlash enables efficient drafting with high-quality outputs and higher acceptance rates. Experiments show that DFlash achieves over 6x lossless acceleration across a range of models and tasks, delivering up to 2.5x higher speedup than the state-of-the-art speculative decoding method EAGLE-3.", "AI": {"tldr": "\u9488\u5bf9\u81ea\u56de\u5f52\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u901f\u5ea6\u74f6\u9888\uff0cDFlash\u5229\u7528\u5757\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u5e76\u884c\u8349\u7a3f\u751f\u6210\uff0c\u663e\u8457\u52a0\u901f\u63a8\u7406\u4e14\u4fdd\u6301\u8d28\u91cf\u9886\u5148\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u81ea\u56de\u5f52\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u5b58\u5728\u5e8f\u5217\u89e3\u7801\u74f6\u9888\uff0c\u63a8\u7406\u5ef6\u8fdf\u9ad8\u4e14GPU\u5229\u7528\u7387\u4f4e\uff0c\u73b0\u6709\u7684\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\u4ecd\u4f9d\u8d56\u5e8f\u5217\u751f\u6210\uff0c\u96be\u4ee5\u83b7\u5f97\u7406\u60f3\u7684\u52a0\u901f\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e86DFlash\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u6b21\u524d\u5411\u4f20\u64ad\u751f\u6210\u8349\u7a3f\u6807\u8bb0\uff0c\u5e76\u5229\u7528\u76ee\u6807\u6a21\u578b\u63d0\u53d6\u7684\u4e0a\u4e0b\u6587\u7279\u5f81\u5bf9\u8349\u7a3f\u6a21\u578b\u8fdb\u884c\u6761\u4ef6\u7ea6\u675f\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u5e76\u884c\u8349\u7a3f\u751f\u6210\u3002", "result": "\u5728\u591a\u79cd\u6a21\u578b\u548c\u4efb\u52a1\u4e2d\uff0cDFlash\u5b9e\u73b0\u4e86\u8d85\u8fc76\u500d\u7684\u65e0\u635f\u52a0\u901f\uff0c\u901f\u5ea6\u63d0\u5347\u6700\u9ad8\u8fbe2.5\u500d\uff0c\u663e\u8457\u4f18\u4e8e\u76ee\u524d\u6700\u5148\u8fdb\u7684\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5EAGLE-3\u3002", "conclusion": "DFlash\u5229\u7528\u8f7b\u91cf\u7ea7\u5757\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u5e73\u884c\u8349\u7a3f\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u901f\u5ea6\u548cGPU\u5229\u7528\u7387\uff0c\u4e14\u4fdd\u6301\u4e86\u8f93\u51fa\u8d28\u91cf\u4e0d\u53d8\u3002"}}
