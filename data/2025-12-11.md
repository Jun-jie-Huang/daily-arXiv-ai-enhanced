<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 35]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.SE](#cs.SE) [Total: 10]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Noise-Robust Abstractive Compression in Retrieval-Augmented Language Models](https://arxiv.org/abs/2512.08943)
*Singon Kim*

Main category: cs.CL

TL;DR: 本文针对检索增强生成中的噪声文档问题，提出了ACoRN方法，通过数据增强和微调提升压缩摘要的鲁棒性和准确性，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成中，检索到的文档包含不相关或误导性错误信息，导致摘要压缩模型遗漏对答案关键的重要信息，尤其在长上下文中注意力分散问题。

Method: 提出了Abstractive Compression Robust against Noise (ACoRN) 方法，包含两个训练步骤：1) 离线数据增强以提高压缩器对两类检索噪声的鲁棒性；2) 对语言模型压缩器进行微调，使生成的摘要围绕支持正确答案的关键信息。

Result: 使用ACoRN训练的T5-large压缩器提升了EM和F1分数，同时保证了答案字符串的保留。ACoRN特别擅长处理包含大量降低准确度文档的数据集，在实际场景中表现优异。

Conclusion: ACoRN显著提高了摘要压缩模型应对检索噪声的能力，提升了问答准确性，适用于实际应用中含有噪声的检索文档处理。

Abstract: Abstractive compression utilizes smaller langauge models to condense query-relevant context, reducing computational costs in retrieval-augmented generation (RAG). However, retrieved documents often include information that is either irrelevant to answering the query or misleading due to factual incorrect content, despite having high relevance scores. This behavior indicates that abstractive compressors are more likely to omit important information essential for the correct answer, especially in long contexts where attention dispersion occurs. To address this issue, we categorize retrieved documents in a more fine-grained manner and propose Abstractive Compression Robust against Noise (ACoRN), which introduces two novel training steps. First, we use offline data augmentation on the training dataset to enhance compressor robustness against two distinct types of retrieval noise. Second, since the language model based compressor cannot fully utilize information from multiple retrieved documents and exhibits positional bias, we perform finetuning to generate summaries centered around key information that directly supports the correct answer. Our experiments demonstrate that T5-large, trained with ACoRN as a compressor, improves EM and F1 scores while preserving the answer string, which could serve as direct evidence. ACoRN excels on datasets with many accuracy reducing documents, making it highly useful in real-world scenarios.

</details>


### [2] [Enhancing Reliability across Short and Long-Form QA via Reinforcement Learning](https://arxiv.org/abs/2512.08944)
*Yudong Wang,Zhe Yang,Wenhan Ma,Zhifang Sui,Liang Zhao*

Main category: cs.CL

TL;DR: 本文提出了一种针对大型语言模型的强化学习框架，用以减少模型在短文本和长文本问答中的内在和外在妄想问题，并通过拒答机制提升模型谨慎性，从而实现在能力与可靠性之间的平衡。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习提升了大语言模型的复杂推理能力，但也加剧了妄想现象，造成能力与可靠性之间的冲突，迫切需要有效方法解决这一权衡。

Method: 利用强化学习框架，结合由TriviaQA转换的训练集解决外在妄想，采用FineWeb长文本事实依托奖励机制抑制内在妄想，并奖励模型拒答无解问题，增强谨慎性。

Result: 实验结果显示，该方法在多个基准测试中显著提升表现，有效减少了内在和外在妄想现象，显著增强了模型的事实准确性和答题谨慎性。

Conclusion: 该方法通过新颖数据集与事实依据奖励机制显著降低了妄想现象，提升了问答性能，实现了大语言模型的高效推理与事实可信性的统一。

Abstract: While reinforcement learning has unlocked unprecedented complex reasoning in large language models, it has also amplified their propensity for hallucination, creating a critical trade-off between capability and reliability. This work confronts this challenge by introducing a targeted RL framework designed to mitigate both intrinsic and extrinsic hallucinations across short and long-form question answering. We address extrinsic hallucinations (flawed internal knowledge) by creating a novel training set from open-ended conversions of TriviaQA. Concurrently, we tackle intrinsic hallucinations (unfaithfulness to context) by leveraging long-form texts from FineWeb in a fact-grounding reward scheme. To further bolster reliability, our framework explicitly rewards the model for refusing to answer unanswerable questions, thereby cultivating crucial cautiousness. Extensive experiments demonstrate that our methodology yields significant performance gains across a diverse suite of benchmarks, substantially reducing both hallucination types. Ultimately, this research contributes a practical framework for resolving the critical tension between advanced reasoning and factual trustworthiness, paving the way for more capable and reliable large language models.

</details>


### [3] [The Linguistic Architecture of Reflective Thought: Evaluation of a Large Language Model as a Tool to Isolate the Formal Structure of Mentalization](https://arxiv.org/abs/2512.08945)
*Stefano Epifani,Giuliano Castigliego,Laura Kecskemeti,Giuliano Razzicchia,Elisabeth Seiwald-Sonderegger*

Main category: cs.CL

TL;DR: 大型语言模型能较好地生成符合心理化结构的文本，但情感表达欠缺，部分心理化维度融合效果有限。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型生成反思性文本能力增强，探讨其语言形式与心理表征的关系及模拟心理化语言结构的潜力。

Method: 生成50组人机对话，5名MBT训练精神科医生盲评对话文本的心理化维度表现，使用Likert量表评分并通过ICC评估评审一致性。

Result: 本研究评估了单一大型语言模型在根据心理化基础治疗（MBT）参数生成反映心理化语言结构文本的能力。通过50组人机对话，5名训练有素的精神病学家在盲法条件下对生成的文本沿MBT四个轴进行评分，结果显示模型在结构一致性上表现优异，评估者间一致性较高，但在整合内在状态与外部环境方面存在不足，且生成文本情感较为中性。

Conclusion: 该LLM在重现MBT心理化语言结构上显示出较高的结构连贯性和临床可解读性，但存在融合内外状态能力的局限及情感中性的问题。

Abstract: Background: Mentalization integrates cognitive, affective, and intersubjective components. Large Language Models (LLMs) display an increasing ability to generate reflective texts, raising questions regarding the relationship between linguistic form and mental representation. This study assesses the extent to which a single LLM can reproduce the linguistic structure of mentalization according to the parameters of Mentalization-Based Treatment (MBT).
  Methods: Fifty dialogues were generated between human participants and an LLM configured in standard mode. Five psychiatrists trained in MBT, working under blinded conditions, evaluated the mentalization profiles produced by the model along the four MBT axes, assigning Likert-scale scores for evaluative coherence, argumentative coherence, and global quality. Inter-rater agreement was estimated using ICC(3,1).
  Results: Mean scores (3.63-3.98) and moderate standard deviations indicate a high level of structural coherence in the generated profiles. ICC values (0.60-0.84) show substantial-to-high agreement among raters. The model proved more stable in the Implicit-Explicit and Self-Other dimensions, while presenting limitations in the integration of internal states and external contexts. The profiles were coherent and clinically interpretable yet characterized by affective neutrality.

</details>


### [4] [Luxical: High-Speed Lexical-Dense Text Embeddings](https://arxiv.org/abs/2512.09015)
*DatologyAI,:,Luke Merrick,Alex Fang,Aldo Carranza,Alvin Deng,Amro Abbas,Brett Larsen,Cody Blakeney,Darren Teh,David Schwab,Fan Pan,Haakon Mongstad,Haoli Yin,Jack Urbanek,Jason Lee,Jason Telanoff,Josh Wills,Kaleigh Mentzer,Paul Burstein,Parth Doshi,Paul Burnstein,Pratyush Maini,Ricardo Monti,Rishabh Adiga,Scott Loftin,Siddharth Joshi,Spandan Das,Tony Jiang,Vineeth Dorma,Zhengping Wang,Bogdan Gaza,Ari Morcos,Matthew Leavitt*

Main category: cs.CL

TL;DR: Luxical提供了一种结合TF-IDF和小型神经网络的高效文本嵌入方法，显著提升大规模文本组织的效率，保持了神经模型的质量。


<details>
  <summary>Details</summary>
Motivation: 当前文本分类器在速度和灵活性之间存在权衡，快速的词汇分类器输出有限，而灵活的Transformer嵌入计算成本高昂。为了解决这一问题，Luxical旨在兼顾两者优点，支持大规模文本组织。

Method: Luxical结合了稀疏TF-IDF特征、小型ReLU网络和知识蒸馏训练方案，通过近似大型Transformer嵌入模型实现高速“词汇-密集”文本嵌入。

Result: Luxical在两个任务（网页文档检索和语言模型数据筛选）中实现了比神经网络基线快3到100倍的速度提升，在数据筛选任务中速度可比肩FastText，同时质量与神经网络基线相当。

Conclusion: Luxical在大规模文本组织中实现了速度与效果的良好平衡，既快速又具备神经网络基线的质量，适合实际应用且已开源。

Abstract: Frontier language model quality increasingly hinges on our ability to organize web-scale text corpora for training. Today's dominant tools trade off speed and flexibility: lexical classifiers (e.g., FastText) are fast but limited to producing classification output scores, while the vector-valued outputs of transformer text embedding models flexibly support numerous workflows (e.g., clustering, classification, and retrieval) but are computationally expensive to produce. We introduce Luxical, a library for high-speed "lexical-dense" text embeddings that aims to recover the best properties of both approaches for web-scale text organization. Luxical combines sparse TF--IDF features, a small ReLU network, and a knowledge distillation training regimen to approximate large transformer embedding models at a fraction of their operational cost. In this technical report, we describe the Luxical architecture and training objective and evaluate a concrete Luxical model in two disparate applications: a targeted webcrawl document retrieval test and an end-to-end language model data curation task grounded in text classification. In these tasks we demonstrate speedups ranging from 3x to 100x over varying-sized neural baselines, and comparable to FastText model inference during the data curation task. On these evaluations, the tested Luxical model illustrates favorable compute/quality trade-offs for large-scale text organization, matching the quality of neural baselines. Luxical is available as open-source software at https://github.com/datologyai/luxical.

</details>


### [5] [Knowledge-Guided Large Language Model for Automatic Pediatric Dental Record Understanding and Safe Antibiotic Recommendation](https://arxiv.org/abs/2512.09127)
*Zihan Han,Junyan Ge,Caifeng Li*

Main category: cs.CL

TL;DR: 本研究提出了一种知识引导的大型语言模型（KG-LLM），结合儿科牙科知识图谱、检索增强生成和多阶段安全验证，实现基于证据的抗生素推荐，显著提升了临床诊断和安全用药的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统规则基临床决策系统难以处理非结构化的牙科文本、不完整的影像描述及复杂的安全约束，亟需整合知识图谱与先进语言模型来提升解释与用药安全性。

Method: 该方法通过临床命名实体识别及关系抽取模块结构化牙科文本，再结合知识图谱检索相关指南和历史病例供语言模型诊断总结及剂量预测，最后采用双层安全验证机制保障用药安全。

Result: 在3.2万条匿名儿科牙科访视记录上的实验表明，KG-LLM较领域适配的Llama-2基线模型，在记录理解（F1 0.914 vs. 0.867）、剂量准确率（Top-1 0.782 vs. 0.716）及减少不安全建议（降低50%）方面均有显著提升，且通过消融实验验证了各模块的重要贡献。

Conclusion: KG-LLM在儿科牙科临床记录理解、用药剂量预测及安全用药方面明显优于传统模型，显著减少不安全抗生素建议，提升系统的临床可靠性与解释性。

Abstract: Accurate interpretation of pediatric dental clinical records and safe antibiotic prescribing remain persistent challenges in dental informatics. Traditional rule-based clinical decision support systems struggle with unstructured dental narratives, incomplete radiographic descriptions, and complex safety constraints. To address these limitations, this study proposes a Knowledge-Guided Large Language Model (KG-LLM) that integrates a pediatric dental knowledge graph, retrieval-augmented generation (RAG), and a multi-stage safety validation pipeline for evidence-grounded antibiotic recommendation. The framework first employs a clinical NER/RE module to extract structured entities and relations from dental notes and radiology reports. Relevant guidelines, drug-safety rules, and analogous historical cases are subsequently retrieved from the knowledge graph and supplied to the LLM for diagnostic summarization and dose-drug-duration prediction. Safety assurance is achieved through a dual-layer validation mechanism combining deterministic rule checking with a learned classifier for detecting allergies, contraindications, and dosing errors. Experiments on 32,000 de-identified pediatric dental visit records demonstrate the effectiveness of the proposed approach. Compared with a domain-adapted Llama-2 clinical baseline, KG-LLM improves record-understanding performance (F1: 0.914 vs. 0.867), drug-dose-duration accuracy (Top-1: 0.782 vs. 0.716), and reduces unsafe antibiotic suggestions by 50%. Additional evaluation across summary quality, recommendation accuracy, and global safety scores further confirms the robustness of the system. Ablation analyses indicate that the knowledge graph, RAG, and safety modules each contribute substantially to clinical reliability and interpretability.

</details>


### [6] [Detecting Hallucinations in Graph Retrieval-Augmented Generation via Attention Patterns and Semantic Alignment](https://arxiv.org/abs/2512.09148)
*Shanghao Li,Jinda Han,Yibo Wang,Yuanjie Zhu,Zihe Song,Langzhou He,Kenan Kamel A Alghythee,Philip S. Yu*

Main category: cs.CL

TL;DR: 本文提出了两种新的可解释性指标，用于分析大型语言模型在图结构知识增强生成任务中的表现，并设计了一个高效的后期检测方法来识别幻觉生成。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型难以准确解释从知识图谱中检索的子图，导致生成内容与检索知识不一致，即幻觉问题，需要深入分析模型如何利用结构化知识以提升可靠性。

Method: 提出了路径依赖度（PRD）和语义对齐评分（SAS）两种轻量级可解释性指标，并基于这些指标开发了图形接地与对齐（GGA）幻觉检测方法。

Result: 通过知识问答任务的实证分析，发现高PRD和低SAS对应模型的失败模式，所提出的GGA检测方法在AUC和F1上优于现有语义和置信度基线模型。

Conclusion: 结构性限制导致大型语言模型在使用图结构知识时出现幻觉，提出的指标和检测方法有效提升了幻觉识别能力，有助于未来更可靠的图知识增强生成系统设计。

Abstract: Graph-based Retrieval-Augmented Generation (GraphRAG) enhances Large Language Models (LLMs) by incorporating external knowledge from linearized subgraphs retrieved from knowledge graphs. However, LLMs struggle to interpret the relational and topological information in these inputs, resulting in hallucinations that are inconsistent with the retrieved knowledge. To analyze how LLMs attend to and retain structured knowledge during generation, we propose two lightweight interpretability metrics: Path Reliance Degree (PRD), which measures over-reliance on shortest-path triples, and Semantic Alignment Score (SAS), which assesses how well the model's internal representations align with the retrieved knowledge. Through empirical analysis on a knowledge-based QA task, we identify failure patterns associated with over-reliance on salient paths and weak semantic grounding, as indicated by high PRD and low SAS scores. We further develop a lightweight post-hoc hallucination detector, Graph Grounding and Alignment (GGA), which outperforms strong semantic and confidence-based baselines across AUC and F1. By grounding hallucination analysis in mechanistic interpretability, our work offers insights into how structural limitations in LLMs contribute to hallucinations, informing the design of more reliable GraphRAG systems in the future.

</details>


### [7] [MindShift: Analyzing Language Models' Reactions to Psychological Prompts](https://arxiv.org/abs/2512.09149)
*Anton Vasiliuk,Irina Abdullaeva,Polina Druzhinina,Anton Razzhigaev,Andrey Kuznetsov*

Main category: cs.CL

TL;DR: 本文通过采用MMPI和个性化提示，提出MindShift基准来评估大型语言模型的心理适应能力，发现LLMs在角色扮演能力上有所提升且不同模型存在明显差异。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型模拟和适应不同人格特质的能力。

Method: 采用明尼苏达多项人格测验（MMPI）和个性化提示，构建不同人格强度的角色进行测试。

Result: 引入了MindShift基准，用于评估LLMs的心理适应能力，发现随着训练数据和校准技术的进步，LLMs的角色感知能力稳步提升。不同模型和模型家族在心理测评中的表现存在显著差异，体现了它们在人格特质模拟方面的差异。

Conclusion: LLMs展现了在心理适应性和人格角色扮演上的进步，不同模型类型表现出不同的人格特质模拟能力。MindShift为此类评估提供了有效工具和标准。

Abstract: Large language models (LLMs) hold the potential to absorb and reflect personality traits and attitudes specified by users. In our study, we investigated this potential using robust psychometric measures. We adapted the most studied test in psychological literature, namely Minnesota Multiphasic Personality Inventory (MMPI) and examined LLMs' behavior to identify traits. To asses the sensitivity of LLMs' prompts and psychological biases we created personality-oriented prompts, crafting a detailed set of personas that vary in trait intensity. This enables us to measure how well LLMs follow these roles. Our study introduces MindShift, a benchmark for evaluating LLMs' psychological adaptability. The results highlight a consistent improvement in LLMs' role perception, attributed to advancements in training datasets and alignment techniques. Additionally, we observe significant differences in responses to psychometric assessments across different model types and families, suggesting variability in their ability to emulate human-like personality traits. MindShift prompts and code for LLM evaluation will be publicly available.

</details>


### [8] [Targeting Misalignment: A Conflict-Aware Framework for Reward-Model-based LLM Alignment](https://arxiv.org/abs/2512.09212)
*Zixuan Liu,Siavash H. Khajavi,Guangkai Jiang,Xinru Liu*

Main category: cs.CL

TL;DR: 本文提出了一种新框架，通过检测代理奖励模型与基础策略模型之间的冲突，识别并缓解大语言模型微调中的误对齐问题。


<details>
  <summary>Details</summary>
Motivation: 现有微调依赖代理奖励模型准确反映人类偏好，但现实中经常存在标注噪声、偏差及覆盖不足导致误对齐，模型优化错误信号而非真实人类价值。

Method: 提出了两种度量代理模型与策略冲突的指标(PACS和Kendall-Tau距离)，并设计了基于冲突感知采样的选择性人类反馈算法(SHF-CAS)来提升奖励模型和策略的微调效果。

Result: 实验表明，该方法在两项对齐任务中，即使使用带偏置的代理奖励模型，也能有效提升大语言模型的整体对齐性能。

Conclusion: 该方法通过冲突识别和选择性人类反馈，提高了大语言模型在面对偏置代理奖励时的对齐性能，提供了误对齐问题的解释和改进路径。

Abstract: Reward-model-based fine-tuning is a central paradigm in aligning Large Language Models with human preferences. However, such approaches critically rely on the assumption that proxy reward models accurately reflect intended supervision, a condition often violated due to annotation noise, bias, or limited coverage. This misalignment can lead to undesirable behaviors, where models optimize for flawed signals rather than true human values. In this paper, we investigate a novel framework to identify and mitigate such misalignment by treating the fine-tuning process as a form of knowledge integration. We focus on detecting instances of proxy-policy conflicts, cases where the base model strongly disagrees with the proxy. We argue that such conflicts often signify areas of shared ignorance, where neither the policy nor the reward model possesses sufficient knowledge, making them especially susceptible to misalignment. To this end, we propose two complementary metrics for identifying these conflicts: a localized Proxy-Policy Alignment Conflict Score (PACS) and a global Kendall-Tau Distance measure. Building on this insight, we design an algorithm named Selective Human-in-the-loop Feedback via Conflict-Aware Sampling (SHF-CAS) that targets high-conflict QA pairs for additional feedback, refining both the reward model and policy efficiently. Experiments on two alignment tasks demonstrate that our approach enhances general alignment performance, even when trained with a biased proxy reward. Our work provides a new lens for interpreting alignment failures and offers a principled pathway for targeted refinement in LLM training.

</details>


### [9] [CORE: A Conceptual Reasoning Layer for Large Language Models](https://arxiv.org/abs/2512.09222)
*Vishwas Hegde,Vindhya Shigehalli*

Main category: cs.CL

TL;DR: CORE通过引入概念优先的交互机制和局部持久状态，避免了多轮对话中的历史重放问题，提升了交互的效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 多轮交互中模型需从不断扩展的令牌历史中重构用户意图和任务状态，导致推理漂移、不一致和提示长度增长。

Method: 提出CORE，一种概念优先的交互层，结合通用认知算子库和持久的局部概念状态，避免模型在多轮交互中需要重放全部历史令牌。

Result: 在原型模拟中，CORE在累计提示令牌数上减少约42%，提升了多轮交互的稳定性和效率。

Conclusion: CORE为多轮语言模型交互提供了一种与模型权重无关的概念推理分离机制，具有良好的扩展性和多轮稳定性提升潜力。

Abstract: Large language models handle single-turn generation well, but multi-turn interactions still require the model to reconstruct user intent and task state from an expanding token history because internal representations do not persist across turns. This token-first paradigm leads to drift, inconsistent reasoning modes, and growing prompts as conversations deepen. We propose CORE, a concept-first interaction layer that improves multi-turn stability without modifying model weights. CORE combines a small library of universal cognitive operators with a persistent Local Concept - a compact semantic state capturing the task, constraints, preferences, and intermediate results. Each model call receives only this concept state, the user's latest instruction, and the selected operator, eliminating the need to replay full history. A preliminary prototype simulating CORE's behavior shows about 42% reduction in cumulative prompt tokens, though this number reflects prototype conditions and should not be interpreted as a real-world performance estimate. CORE offers a model-agnostic mechanism that separates conceptual reasoning from language generation, suggesting a scalable direction for more stable multi-turn systems.

</details>


### [10] [Training-free Context-adaptive Attention for Efficient Long Context Modeling](https://arxiv.org/abs/2512.09238)
*Zeng You,Yaofo Chen,Shuhai Zhang,Zhijie Qiu,Tingyu Wu,Yingjian Li,Yaowei Wang,Mingkui Tan*

Main category: cs.CL

TL;DR: 提出了一种无需训练的自适应稀疏注意力机制，有效加速长序列推理，节省内存，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 自注意力机制计算复杂度平方级增长，导致长序列处理计算和内存瓶颈。现有稀疏注意力或缓存压缩方法存在固定模式依赖、无法兼顾预填充和解码、需额外训练等限制。

Method: 提出了一种训练免疫的稀疏注意力机制——TCA-Attention，包含离线校准阶段（通过一次前向传播确定每个头的稀疏预算）和在线令牌选择阶段（利用轻量级冗余度度量自适应保留核心上下文令牌）。该方法无需参数更新或结构改变，统一加速预填充和解码过程，降低KV缓存占用。

Result: 在128K上下文长度下，TCA-Attention实现了2.8倍速度提升，KV缓存减少61%，性能保持与全注意力相当。

Conclusion: TCA-Attention提供了一种实用的即插即用方案，在不改变模型参数的情况下，实现高效长上下文推理，解决了自注意力的计算和内存瓶颈。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing tasks. These capabilities stem primarily from the self-attention mechanism, which enables modeling of long-range dependencies. However, the quadratic complexity of self-attention with respect to sequence length poses significant computational and memory challenges, especially as sequence length extends to extremes. While various sparse attention and KV cache compression methods have been proposed to improve efficiency, they often suffer from limitations such as reliance on fixed patterns, inability to handle both prefilling and decoding stages, or the requirement for additional training. In this paper, we propose Training-free Context-adaptive Attention (TCA-Attention), a training-free sparse attention mechanism that selectively attends to only the informative tokens for efficient long-context inference. Our method consists of two lightweight phases: i) an offline calibration phase that determines head-specific sparsity budgets via a single forward pass, and ii) an online token selection phase that adaptively retains core context tokens using a lightweight redundancy metric. TCA-Attention provides a unified solution that accelerates both prefilling and decoding while reducing KV cache memory footprint, without requiring parameter updates or architectural changes. Theoretical analysis shows that our approach maintains bounded approximation error. Extensive experiments demonstrate that TCA-Attention achieves a 2.8$\times$ speedup and reduces KV cache by 61% at 128K context length while maintaining performance comparable to full attention across various benchmarks, offering a practical plug-and-play solution for efficient long-context inference.

</details>


### [11] [Identifying Bias in Machine-generated Text Detection](https://arxiv.org/abs/2512.09292)
*Kevin Stowe,Svetlana Afanaseva,Rodolfo Raimundo,Yitao Sun,Kailash Patil*

Main category: cs.CL

TL;DR: 研究发现多个机器生成文本检测系统对不同群体存有偏见，尤其是对英语学习者和非白人英语学习者，容易误判为机器生成，经济困难学生则相对较少，但人类检测者无明显偏见。


<details>
  <summary>Details</summary>
Motivation: 随着文本生成能力的快速提升，识别机器生成文本变得重要，但现有检测系统可能存在偏见，导致对特定群体产生不公平的影响，因此有必要系统地评估这些偏见。

Method: 收集学生论文数据集，评估16种不同的机器生成文本检测系统，通过回归模型和子群组分析，检测系统对性别、种族/族裔、英语学习者状态及经济状态的偏见程度。并对比人类标注者表现。

Result: 多款检测模型对处于劣势的群体文本存在分类偏差，尤其是英语学习者和非白人英语学习者更频繁被误判为机器生成文本，经济困难学生的文章误判率较低，但人类检测者表现出无显著偏见。

Conclusion: 多数机器生成文本检测系统存在偏见，尤其在性别、种族/族裔、英语学习者身份和经济状态上表现不一致，但整体上对处于劣势群体的文本有误判倾向。比方说，英语学习者和非白人英语学习者的文章更易被误判为机器生成，而经济困难学生的文章则相对较少被误判。相比之下，人类检测者在这些属性上无显著偏见。

Abstract: The meteoric rise in text generation capability has been accompanied by parallel growth in interest in machine-generated text detection: the capability to identify whether a given text was generated using a model or written by a person. While detection models show strong performance, they have the capacity to cause significant negative impacts. We explore potential biases in English machine-generated text detection systems. We curate a dataset of student essays and assess 16 different detection systems for bias across four attributes: gender, race/ethnicity, English-language learner (ELL) status, and economic status. We evaluate these attributes using regression-based models to determine the significance and power of the effects, as well as performing subgroup analysis. We find that while biases are generally inconsistent across systems, there are several key issues: several models tend to classify disadvantaged groups as machine-generated, ELL essays are more likely to be classified as machine-generated, economically disadvantaged students' essays are less likely to be classified as machine-generated, and non-White ELL essays are disproportionately classified as machine-generated relative to their White counterparts. Finally, we perform human annotation and find that while humans perform generally poorly at the detection task, they show no significant biases on the studied attributes.

</details>


### [12] [CONCUR: A Framework for Continual Constrained and Unconstrained Routing](https://arxiv.org/abs/2512.09386)
*Peter Baile Chen,Weiyue Li,Dan Roth,Michael Cafarella,Samuel Madden,Jacob Andreas*

Main category: cs.CL

TL;DR: CONCUR通过独立模型和多表示实现持续路由，提升任务策略匹配效率与效果。


<details>
  <summary>Details</summary>
Motivation: 不同AI任务复杂度不同，需针对性路由策略，传统方法重训成本高且泛化能力差，单输入表示限制路由性能。

Method: 为每个策略训练独立预测器，采用多种任务与策略表示，支持约束与无约束持续路由，减少重新训练成本。

Result: 提出了CONCUR，一个支持约束与无约束路由的持续路由框架，通过为每种策略训练独立预测模型，能够实现低成本整合新策略，并利用多种任务和策略表示捕捉复杂性。实验证明，CONCUR在知识和推理密集任务中较现有最佳单策略和路由方法表现更优，具有更高准确率、更低推理成本和持续学习中的训练成本优势。

Conclusion: CONCUR有效克服了传统路由方法需全量重训和单一表示限制，实现了高效持续学习和优异的任务策略匹配表现。

Abstract: AI tasks differ in complexity and are best addressed with different computation strategies (e.g., combinations of models and decoding methods). Hence, an effective routing system that maps tasks to the appropriate strategies is crucial. Most prior methods build the routing framework by training a single model across all strategies, which demands full retraining whenever new strategies appear and leads to high overhead. Attempts at such continual routing, however, often face difficulties with generalization. Prior models also typically use a single input representation, limiting their ability to capture the full complexity of the routing problem and leading to sub-optimal routing decisions. To address these gaps, we propose CONCUR, a continual routing framework that supports both constrained and unconstrained routing (i.e., routing with or without a budget). Our modular design trains a separate predictor model for each strategy, enabling seamless incorporation of new strategies with low additional training cost. Our predictors also leverage multiple representations of both tasks and computation strategies to better capture overall problem complexity. Experiments on both in-distribution and out-of-distribution, knowledge- and reasoning-intensive tasks show that our method outperforms the best single strategy and strong existing routing techniques with higher end-to-end accuracy and lower inference cost in both continual and non-continual settings, while also reducing training cost in the continual setting.

</details>


### [13] [Language models as tools for investigating the distinction between possible and impossible natural languages](https://arxiv.org/abs/2512.09394)
*Julie Kallini,Christopher Potts*

Main category: cs.CL

TL;DR: 该论文提出通过反复优化语言模型来辨别自然语言的可行性，以揭示支持人类语言学习的归纳偏差。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型在揭示人类语言学习中归纳偏差的潜力，通过区分可能和不可能的语言来理解人类认知。

Method: 提出利用语言模型(LMs)作为工具，通过逐步优化LM架构来区分可能与不可能的自然语言。

Result: 设计了一个分阶段的研究方案，通过不断改进语言模型更准确地识别可行与不可行语言，促进与人类认知的关联假设建立。

Conclusion: 语言模型具备成为探究人类语言认知及潜在语言规则的有力工具的潜力，未来应持续改进其识别能力以验证相关认知假设。

Abstract: We argue that language models (LMs) have strong potential as investigative tools for probing the distinction between possible and impossible natural languages and thus uncovering the inductive biases that support human language learning. We outline a phased research program in which LM architectures are iteratively refined to better discriminate between possible and impossible languages, supporting linking hypotheses to human cognition.

</details>


### [14] [CourtPressGER: A German Court Decision to Press Release Summarization Dataset](https://arxiv.org/abs/2512.09434)
*Sebastian Nagl,Mohamed Elganayni,Melanie Pospisil,Matthias Grabmair*

Main category: cs.CL

TL;DR: 该论文提出并使用CourtPressGER数据集评估大型语言模型生成德国法院新闻稿的能力，发现大型模型表现优异但仍不及人工新闻稿质量。


<details>
  <summary>Details</summary>
Motivation: 现有司法领域的自然语言处理工作多聚焦技术性摘要，忽视面向公众的信息传播需求。为提升司法透明度及公众理解，引入面向公民的新闻稿生成任务及相应数据集。

Method: 构建名为CourtPressGER的6.4千条数据集，包含裁决文本、人类撰写的新闻稿及用于生成可比新闻稿的合成提示。基于此数据集训练和评估不同规模的LLM，采用参考指标、事实一致性检测、LLM裁判和专家排名等多维度评价方法。

Result: 大型模型在生成新闻稿草稿时质量较高且性能损失较小，小型模型需要层级策略来处理长裁决文本。初步基准测试揭示模型性能差异，人类撰写新闻稿质量最高。

Conclusion: 大型语言模型（LLM）在生成准确且易读的法院新闻稿方面表现良好，虽然较小模型在处理长文本时需层级结构支持。人类撰写的新闻稿整体质量仍优于自动生成内容。

Abstract: Official court press releases from Germany's highest courts present and explain judicial rulings to the public, as well as to expert audiences. Prior NLP efforts emphasize technical headnotes, ignoring citizen-oriented communication needs. We introduce CourtPressGER, a 6.4k dataset of triples: rulings, human-drafted press releases, and synthetic prompts for LLMs to generate comparable releases. This benchmark trains and evaluates LLMs in generating accurate, readable summaries from long judicial texts. We benchmark small and large LLMs using reference-based metrics, factual-consistency checks, LLM-as-judge, and expert ranking. Large LLMs produce high-quality drafts with minimal hierarchical performance loss; smaller models require hierarchical setups for long judgments. Initial benchmarks show varying model performance, with human-drafted releases ranking highest.

</details>


### [15] [Knowledge-Augmented Large Language Model Agents for Explainable Financial Decision-Making](https://arxiv.org/abs/2512.09440)
*Qingyuan Zhang,Yuxi Wang,Cancan Hua,Yulin Huang,Ning Lyu*

Main category: cs.CL

TL;DR: 本研究提出了一种基于知识增强的大型语言模型代理的可解释推理方法，用于提升金融决策的准确性和解释性。


<details>
  <summary>Details</summary>
Motivation: 传统金融决策方法依赖参数化知识，存在事实一致性不足和缺乏推理链条的问题，难以满足复杂金融场景中的准确性和可解释性需求。

Method: 该方法通过编码金融文本与结构化数据得到语义表示，利用相似度计算从外部知识库检索相关信息，并通过加权融合整合内外部知识。推理阶段引入多头注意力机制构建逻辑链，结合任务目标和解释一致性目标进行联合优化。

Result: 在金融文本处理和决策任务中，该方法表现出优于基线方法的准确率、文本生成质量和事实支持能力，验证了知识增强和可解释推理的有效性。

Conclusion: 该方法通过结合外部知识检索、语义表示和推理生成，显著提高了金融决策的准确性、文本质量和事实支持能力，克服了传统模型在语义覆盖和推理透明度上的不足，具有较强实际应用价值。

Abstract: This study investigates an explainable reasoning method for financial decision-making based on knowledge-enhanced large language model agents. To address the limitations of traditional financial decision methods that rely on parameterized knowledge, lack factual consistency, and miss reasoning chains, an integrated framework is proposed that combines external knowledge retrieval, semantic representation, and reasoning generation. The method first encodes financial texts and structured data to obtain semantic representations, and then retrieves task-related information from external knowledge bases using similarity computation. Internal representations and external knowledge are combined through weighted fusion, which ensures fluency while improving factual accuracy and completeness of generated content. In the reasoning stage, a multi-head attention mechanism is introduced to construct logical chains, allowing the model to present transparent causal relationships and traceability during generation. Finally, the model jointly optimizes task objectives and explanation consistency objectives, which enhances predictive performance and reasoning interpretability. Experiments on financial text processing and decision tasks show that the method outperforms baseline approaches in accuracy, text generation quality, and factual support, verifying the effectiveness of knowledge enhancement and explainable reasoning. Overall, the proposed approach overcomes the limitations of traditional models in semantic coverage and reasoning transparency, and demonstrates strong practical value in complex financial scenarios.

</details>


### [16] [Advancing Text Classification with Large Language Models and Neural Attention Mechanisms](https://arxiv.org/abs/2512.09444)
*Ning Lyu,Yuxi Wang,Feng Chen,Qingyuan Zhang*

Main category: cs.CL

TL;DR: 本文提出基于大规模预训练语言模型和注意力机制的文本分类方法，显著提升了分类性能和模型鲁棒性，特别是在处理类别不平衡和长距离依赖方面效果突出。


<details>
  <summary>Details</summary>
Motivation: 现有文本分类方法难以有效捕捉长距离依赖和语境语义，且在类别不平衡情况下性能下降，故提出基于大规模语言模型和注意力机制的新算法以提升分类效果和模型鲁棒性。

Method: 基于大规模预训练语言模型获取深度语义嵌入，利用注意力机制增强关键特征，结合全局与加权聚合策略生成文本向量，使用全连接层与Softmax输出进行分类，采用交叉熵损失优化模型。

Result: 本文提出了一种基于大规模语言模型的文本分类算法，克服了传统方法在捕捉长距离依赖、语境理解和类别不平衡处理上的不足。该方法结合文本编码、上下文表示建模、基于注意力机制的特征增强、特征聚合与分类预测。使用预训练语言模型获取深层语义嵌入，通过注意力机制强化关键特征表达，结合全局与加权策略获得稳健向量，最终通过全连接层和Softmax输出进行分类，采用交叉熵损失优化。实验比较了多种基线模型，结果显示该方法在Precision、Recall、F1和AUC等指标上全面优于其他模型，尤其在Recall和AUC上提升显著。此外对超参数和数据条件进行了敏感性分析，验证了模型配置和类别不平衡对性能的影响，展现了模型的适应性与稳定性。综合来看，该方法在复杂数据环境中表现优异，具备良好的鲁棒性和适用性。

Conclusion: 提出的方法在多指标上均优于传统模型，验证了其在不同超参数和数据条件下的稳定性和适应性，适用于复杂数据环境的文本分类任务。

Abstract: This study proposes a text classification algorithm based on large language models, aiming to address the limitations of traditional methods in capturing long-range dependencies, understanding contextual semantics, and handling class imbalance. The framework includes text encoding, contextual representation modeling, attention-based enhancement, feature aggregation, and classification prediction. In the representation stage, deep semantic embeddings are obtained through large-scale pretrained language models, and attention mechanisms are applied to enhance the selective representation of key features. In the aggregation stage, global and weighted strategies are combined to generate robust text-level vectors. In the classification stage, a fully connected layer and Softmax output are used to predict class distributions, and cross-entropy loss is employed to optimize model parameters. Comparative experiments introduce multiple baseline models, including recurrent neural networks, graph neural networks, and Transformers, and evaluate them on Precision, Recall, F1-Score, and AUC. Results show that the proposed method outperforms existing models on all metrics, with especially strong improvements in Recall and AUC. In addition, sensitivity experiments are conducted on hyperparameters and data conditions, covering the impact of hidden dimensions on AUC and the impact of class imbalance ratios on Recall. The findings demonstrate that proper model configuration has a significant effect on performance and reveal the adaptability and stability of the model under different conditions. Overall, the proposed text classification method not only achieves effective performance improvement but also verifies its robustness and applicability in complex data environments through systematic analysis.

</details>


### [17] [Source Coverage and Citation Bias in LLM-based vs. Traditional Search Engines](https://arxiv.org/abs/2512.09483)
*Peixian Zhang,Qiming Ye,Zifan Peng,Kiran Garimella,Gareth Tyson*

Main category: cs.CL

TL;DR: 本文通过大规模实证研究揭示了LLM搜索引擎在资源引用多样性上的优势及其在可信度和政治中立性等方面的局限，为相关方提供了重要洞见。


<details>
  <summary>Details</summary>
Motivation: LLM-based搜索引擎改变了信息搜索的方式，但信任和透明度方面的问题仍未充分探讨。

Method: 通过分析55,936个查询以及六个LLM-SEs和两个传统搜索引擎的结果，进行大规模实证研究和基于特征的分析。

Result: LLM-SEs引用的域名资源比传统搜索引擎更多样化，37%的域名是LLM-SEs特有的，但在可信度、政治中立性和安全性方面不优于传统搜索引擎。

Conclusion: LLM-SEs在引用多样性方面优于传统搜索引擎，但在保证信息可信度和中立性上仍存在风险，研究结果为用户、网站拥有者及开发者提供了有价值的参考。

Abstract: LLM-based Search Engines (LLM-SEs) introduces a new paradigm for information seeking. Unlike Traditional Search Engines (TSEs) (e.g., Google), these systems summarize results, often providing limited citation transparency. The implications of this shift remain largely unexplored, yet raises key questions regarding trust and transparency. In this paper, we present a large-scale empirical study of LLM-SEs, analyzing 55,936 queries and the corresponding search results across six LLM-SEs and two TSEs. We confirm that LLM-SEs cites domain resources with greater diversity than TSEs. Indeed, 37% of domains are unique to LLM-SEs. However, certain risks still persist: LLM-SEs do not outperform TSEs in credibility, political neutrality and safety metrics. Finally, to understand the selection criteria of LLM-SEs, we perform a feature-based analysis to identify key factors influencing source choice. Our findings provide actionable insights for end users, website owners, and developers.

</details>


### [18] [RouteRAG: Efficient Retrieval-Augmented Generation from Text and Graph via Reinforcement Learning](https://arxiv.org/abs/2512.09487)
*Yucan Guo,Miao Su,Saiping Guan,Zihao Sun,Xiaolong Jin,Jiafeng Guo,Xueqi Cheng*

Main category: cs.CL

TL;DR: 本文提出了一个强化学习框架，实现了图文混合检索增强生成的多轮适应性推理，提升了效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有图文混合RAG系统依赖固定或手工检索流程，缺乏在推理过程中动态整合辅助证据的能力，同时图检索代价高昂，亟需高效、适应性的方法提升推理表现和效率。

Method: 通过强化学习统一优化生成过程，动态选择推理时机和检索内容，结合两阶段训练框架平衡任务效果和检索效率。

Result: 本文提出了一种名为\model{}的基于强化学习的框架，用于实现多轮、适应性图文本混合检索增强生成（RAG）。该方法允许大语言模型（LLMs）在生成过程中动态决定何时进行推理，从文本或图中检索何种信息，以及何时输出答案，统一优化生成策略。通过两阶段训练，兼顾任务结果和检索效率，避免了不必要的检索开销。实验证明在五个问答基准上显著优于现有RAG方法。

Conclusion: 引入强化学习的端到端训练促进了适应性和高效的图文混合检索，从而提升了复杂推理任务的效果，优于现有基线方法。

Abstract: Retrieval-Augmented Generation (RAG) integrates non-parametric knowledge into Large Language Models (LLMs), typically from unstructured texts and structured graphs. While recent progress has advanced text-based RAG to multi-turn reasoning through Reinforcement Learning (RL), extending these advances to hybrid retrieval introduces additional challenges. Existing graph-based or hybrid systems typically depend on fixed or handcrafted retrieval pipelines, lacking the ability to integrate supplementary evidence as reasoning unfolds. Besides, while graph evidence provides relational structures crucial for multi-hop reasoning, it is substantially more expensive to retrieve. To address these limitations, we introduce \model{}, an RL-based framework that enables LLMs to perform multi-turn and adaptive graph-text hybrid RAG. \model{} jointly optimizes the entire generation process via RL, allowing the model to learn when to reason, what to retrieve from either texts or graphs, and when to produce final answers, all within a unified generation policy. To guide this learning process, we design a two-stage training framework that accounts for both task outcome and retrieval efficiency, enabling the model to exploit hybrid evidence while avoiding unnecessary retrieval overhead. Experimental results across five question answering benchmarks demonstrate that \model{} significantly outperforms existing RAG baselines, highlighting the benefits of end-to-end RL in supporting adaptive and efficient retrieval for complex reasoning.

</details>


### [19] [Systematic Framework of Application Methods for Large Language Models in Language Sciences](https://arxiv.org/abs/2512.09552)
*Kun Sun,Rong Wang*

Main category: cs.CL

TL;DR: 本文提出两套系统化方法框架，指导LLMs在语言科学中不同研究目标的合理应用，促进研究可重复性和科学性。


<details>
  <summary>Details</summary>
Motivation: 当前大规模语言模型（LLMs）在语言科学中的应用存在方法碎片化和系统性不足的问题，缺乏系统化指导。

Method: 提出两种综合性方法框架：1）基于prompt的通用模型交互用于探索性分析和假设生成；2）对开源模型进行微调以支持确认性、理论驱动的研究和高质量数据生成；3）提取上下文化嵌入用于定量分析和模型机制探查。同时设计多阶段研究流水线配置指导实践应用。

Result: 通过技术实现细节揭示各方法的权衡，并通过实证案例和实验验证框架的有效性，包括回顾性分析、前瞻性应用和专家评估调查。

Conclusion: 该框架推动语言科学研究范式的关键转变，确保研究策略与适当的LLM方法对齐，提升方法的可重复性和可靠性，将传统语言学推进为可验证的严谨科学。

Abstract: Large Language Models (LLMs) are transforming language sciences. However, their widespread deployment currently suffers from methodological fragmentation and a lack of systematic soundness. This study proposes two comprehensive methodological frameworks designed to guide the strategic and responsible application of LLMs in language sciences. The first method-selection framework defines and systematizes three distinct, complementary approaches, each linked to a specific research goal: (1) prompt-based interaction with general-use models for exploratory analysis and hypothesis generation; (2) fine-tuning of open-source models for confirmatory, theory-driven investigation and high-quality data generation; and (3) extraction of contextualized embeddings for further quantitative analysis and probing of model internal mechanisms. We detail the technical implementation and inherent trade-offs of each method, supported by empirical case studies. Based on the method-selection framework, the second systematic framework proposed provides constructed configurations that guide the practical implementation of multi-stage research pipelines based on these approaches. We then conducted a series of empirical experiments to validate our proposed framework, employing retrospective analysis, prospective application, and an expert evaluation survey. By enforcing the strategic alignment of research questions with the appropriate LLM methodology, the frameworks enable a critical paradigm shift in language science research. We believe that this system is fundamental for ensuring reproducibility, facilitating the critical evaluation of LLM mechanisms, and providing the structure necessary to move traditional linguistics from ad-hoc utility to verifiable, robust science.

</details>


### [20] [System Report for CCL25-Eval Task 10: Prompt-Driven Large Language Model Merge for Fine-Grained Chinese Hate Speech Detection](https://arxiv.org/abs/2512.09563)
*Binglin Wu,Jiaxiu Zou,Xianneng Li*

Main category: cs.CL

TL;DR: 针对中文社交媒体中复杂且语境依赖的仇恨言论，论文提出一种结合提示设计、微调和模型合并的三阶段LLM框架，有效提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统仇恨言论检测系统难以解码依赖上下文的修辞策略和不断变化的俚语，且在中文社交媒体中仇恨言论泛滥带来紧迫社会风险，因此亟需更精准的检测方法。

Method: 提出了一个基于大语言模型（LLM）的三阶段框架，包括提示工程、监督微调和模型合并。首先设计上下文感知的提示引导模型提取隐含的仇恨言论模式；然后通过监督微调整合任务特定特征以增强领域适应性；最后合并微调后的模型以提升对分布外数据的鲁棒性。

Result: 在STATE-ToxiCN基准测试中，该框架优于现有基线方法，表现出更优的细粒度仇恨言论检测能力。

Conclusion: 该三阶段LLM框架能够更准确地识别复杂的中文仇恨言论，验证了其在实际应用中的有效性和优越性。

Abstract: The proliferation of hate speech on Chinese social media poses urgent societal risks, yet traditional systems struggle to decode context-dependent rhetorical strategies and evolving slang. To bridge this gap, we propose a novel three-stage LLM-based framework: Prompt Engineering, Supervised Fine-tuning, and LLM Merging. First, context-aware prompts are designed to guide LLMs in extracting implicit hate patterns. Next, task-specific features are integrated during supervised fine-tuning to enhance domain adaptation. Finally, merging fine-tuned LLMs improves robustness against out-of-distribution cases. Evaluations on the STATE-ToxiCN benchmark validate the framework's effectiveness, demonstrating superior performance over baseline methods in detecting fine-grained hate speech.

</details>


### [21] [Creation of the Estonian Subjectivity Dataset: Assessing the Degree of Subjectivity on a Scale](https://arxiv.org/abs/2512.09634)
*Karl Gustav Gailit,Kadri Muischnek,Kairit Sirts*

Main category: cs.CL

TL;DR: 本文构建了爱沙尼亚语主观性数据集，并验证了GPT-5在自动主观性评分上的可行性，但强调其不足以取代人工注释。


<details>
  <summary>Details</summary>
Motivation: 为支持爱沙尼亚语文本的主观性分析，构建高质量数据集，并探索大型语言模型在自动主观性评分中的潜力与局限。

Method: 收集1000篇文本，由4名注释者按0-100连续尺度评分主观性，对分歧大的文本进行重新注释，同时利用GPT-5生成自动评分进行对比实验。

Result: 本文创建了爱沙尼亚语文档级主观性数据集，涵盖1000篇文档（300篇新闻文章和700篇网络文本），由4名注释者对主观性进行0到100的连续评分。注释者间相关性中等，对于分歧较大的文本进行了重新注释，相关性得以提升。数据集还包括由GPT-5生成的主观性评分，结果与人工评分相似，但存在差异，表明基于大型语言模型的自动主观性评分可行但不可完全替代人工注释，其适用性取决于具体应用。

Conclusion: 大型语言模型可以辅助自动主观性评分，但目前仍不能完全替代人工注释，应用时需根据具体需求权衡选择。

Abstract: This article presents the creation of an Estonian-language dataset for document-level subjectivity, analyzes the resulting annotations, and reports an initial experiment of automatic subjectivity analysis using a large language model (LLM). The dataset comprises of 1,000 documents-300 journalistic articles and 700 randomly selected web texts-each rated for subjectivity on a continuous scale from 0 (fully objective) to 100 (fully subjective) by four annotators. As the inter-annotator correlations were moderate, with some texts receiving scores at the opposite ends of the scale, a subset of texts with the most divergent scores was re-annotated, with the inter-annotator correlation improving. In addition to human annotations, the dataset includes scores generated by GPT-5 as an experiment on annotation automation. These scores were similar to human annotators, however several differences emerged, suggesting that while LLM based automatic subjectivity scoring is feasible, it is not an interchangeable alternative to human annotation, and its suitability depends on the intended application.

</details>


### [22] [MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment](https://arxiv.org/abs/2512.09636)
*Mengxi Xiao,Kailai Yang,Pengde Zhao,Enze Zhang,Ziyan Kuang,Zhiwei Liu,Weiguang Han,Shu Liao,Lianting Huang,Jinpeng Hu,Min Peng,Qianqian Xie,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 本文提出了用于心理健康领域推理的MentraSuite框架和评测基准MentraBench，设计了优化推理一致性的Mindora模型，显著提升了大语言模型在心理健康应用中的推理可靠性。


<details>
  <summary>Details</summary>
Motivation: 目前心理健康领域的大语言模型往往缺乏步骤化且临床对齐的推理，导致推理不完整、不一致或无依据，限制了其安全可靠的应用。本文旨在构建统一框架提升模型在心理健康推理上的准确性、连贯性和可信度。

Method: 设计了包含五个核心推理维度的MentraBench评测体系，并基于混合监督微调与强化学习的SFT-RL框架训练Mindora模型，利用不一致性检测奖励推动模型生成忠实且连贯的推理过程。采用新颖的推理轨迹生成策略，通过过滤困难样本和结构化重写，构建高质量训练数据。

Result: 本文提出了MentraSuite框架和MentraBench基准，聚焦提升大语言模型在心理健康领域的推理可靠性。通过设计五个核心推理维度和多任务多数据集评测体系，评价模型的任务表现和推理质量。并基于混合监督强化学习策略，设计Mindora模型，结合不一致性检测奖励机制，提升模型推理的忠实性和连贯性。通过构建高质量推理轨迹，优化训练过程。实验证明，Mindora在20个大模型中表现最佳，显著提高了在复杂心理健康场景下的推理可靠性。

Conclusion: Mindora模型结合不一致性检测奖励和高质量推理轨迹，在多任务多数据集的评测中表现领先，显著增强了心理健康大模型的推理可靠性，可有效应用于复杂心理健康场景。

Abstract: Mental health disorders affect hundreds of millions globally, and the Web now serves as a primary medium for accessing support, information, and assessment. Large language models (LLMs) offer scalable and accessible assistance, yet their deployment in mental-health settings remains risky when their reasoning is incomplete, inconsistent, or ungrounded. Existing psychological LLMs emphasize emotional understanding or knowledge recall but overlook the step-wise, clinically aligned reasoning required for appraisal, diagnosis, intervention planning, abstraction, and verification. To address these issues, we introduce MentraSuite, a unified framework for advancing reliable mental-health reasoning. We propose MentraBench, a comprehensive benchmark spanning five core reasoning aspects, six tasks, and 13 datasets, evaluating both task performance and reasoning quality across five dimensions: conciseness, coherence, hallucination avoidance, task understanding, and internal consistency. We further present Mindora, a post-trained model optimized through a hybrid SFT-RL framework with an inconsistency-detection reward to enforce faithful and coherent reasoning. To support training, we construct high-quality trajectories using a novel reasoning trajectory generation strategy, that strategically filters difficult samples and applies a structured, consistency-oriented rewriting process to produce concise, readable, and well-balanced trajectories. Across 20 evaluated LLMs, Mindora achieves the highest average performance on MentraBench and shows remarkable performances in reasoning reliability, demonstrating its effectiveness for complex mental-health scenarios.

</details>


### [23] [Can LLMs Evaluate What They Cannot Annotate? Revisiting LLM Reliability in Hate Speech Detection](https://arxiv.org/abs/2512.09662)
*Paloma Piot,David Otero,Patricia Martín-Rodilla,Javier Parapar*

Main category: cs.CL

TL;DR: 本研究提出主观性敏感的评价框架，证明大语言模型注释虽与人类不同，但能有效反映模型绩效排序，适合作为规模化评估代理。


<details>
  <summary>Details</summary>
Motivation: 现有的仇恨言论检测面临主观性挑战，使得自动检测困难，同时传统指标未能有效反映这类主观性差异。大语言模型虽有潜力，但是否能替代人工判断尚存疑问。

Method: 提出了一个考虑主观性的交叉评分者可靠性(xRR)框架，重新评估大语言模型在仇恨言论检测中的可靠性，并通过实验验证LLM注释在模型性能排名上的有效性。

Result: 发现尽管大语言模型与人类在单个实例上存在差异，但其生成的注释能较好地反映模型性能的相对排序，具有作为代理评估者的潜力。

Conclusion: 大语言模型虽不能完全替代人类注释，但可作为主观性任务中可扩展的代理评价工具，有助于大规模自动化评估。

Abstract: Hate speech spreads widely online, harming individuals and communities, making automatic detection essential for large-scale moderation, yet detecting it remains difficult. Part of the challenge lies in subjectivity: what one person flags as hate speech, another may see as benign. Traditional annotation agreement metrics, such as Cohen's $κ$, oversimplify this disagreement, treating it as an error rather than meaningful diversity. Meanwhile, Large Language Models (LLMs) promise scalable annotation, but prior studies demonstrate that they cannot fully replace human judgement, especially in subjective tasks. In this work, we reexamine LLM reliability using a subjectivity-aware framework, cross-Rater Reliability (xRR), revealing that even under fairer lens, LLMs still diverge from humans. Yet this limitation opens an opportunity: we find that LLM-generated annotations can reliably reflect performance trends across classification models, correlating with human evaluations. We test this by examining whether LLM-generated annotations preserve the relative ordering of model performance derived from human evaluation (i.e. whether models ranked as more reliable by human annotators preserve the same order when evaluated with LLM-generated labels). Our results show that, although LLMs differ from humans at the instance level, they reproduce similar ranking and classification patterns, suggesting their potential as proxy evaluators. While not a substitute for human annotators, they might serve as a scalable proxy for evaluation in subjective NLP tasks.

</details>


### [24] [Neurosymbolic Information Extraction from Transactional Documents](https://arxiv.org/abs/2512.09666)
*Arthur Hemmer,Mickaël Coustaty,Nicola Bartolo,Jean-Marc Ogier*

Main category: cs.CL

TL;DR: 本文提出了一种结合符号验证的神经信息抽取框架，提升了交易文档抽取的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 针对交易文档信息抽取任务，解决现有方法在零样本输出及知识蒸馏中准确性不足的问题，提出结合符号验证的神经符号框架。

Method: 基于语言模型生成候选抽取结果，结合句法、任务及领域层面的符号校验，确保结果符合领域特定算术约束。

Result: 本文提出了一种用于文档信息抽取的神经符号框架，针对交易文档进行了评估。该方法基于模式(schema)集成了符号验证，实现了更有效的零样本输出和知识蒸馏。具体方法是利用语言模型生成候选抽取结果，再通过句法、任务和领域层面的验证，确保符合领域特定的算术约束。贡献包括设计了交易文档的综合schema、重标注数据集以及高质量标签的生成方法。实验结果显示，在$F_1$分数和准确率上均有显著提升，验证了神经符号验证在交易文档处理中的有效性。

Conclusion: 利用神经符号验证方法能显著提升交易文档信息抽取的性能，尤其在零样本和知识蒸馏场景下表现优越。

Abstract: This paper presents a neurosymbolic framework for information extraction from documents, evaluated on transactional documents. We introduce a schema-based approach that integrates symbolic validation methods to enable more effective zero-shot output and knowledge distillation. The methodology uses language models to generate candidate extractions, which are then filtered through syntactic-, task-, and domain-level validation to ensure adherence to domain-specific arithmetic constraints. Our contributions include a comprehensive schema for transactional documents, relabeled datasets, and an approach for generating high-quality labels for knowledge distillation. Experimental results demonstrate significant improvements in $F_1$-scores and accuracy, highlighting the effectiveness of neurosymbolic validation in transactional document processing.

</details>


### [25] [d-TreeRPO: Towards More Reliable Policy Optimization for Diffusion Language Models](https://arxiv.org/abs/2512.09675)
*Leyi Pan,Shuchang Tao,Yunpeng Zhai,Zheyu Fu,Liancheng Fang,Minghua He,Lingzhe Zhang,Zhaoyang Liu,Bolin Ding,Aiwei Liu,Lijie Wen*

Main category: cs.CL

TL;DR: 针对扩散大语言模型强化学习中的奖励信号粗糙和预测概率偏差问题，提出基于树结构展开和自蒸馏增强置信度的d-TreeRPO，实现显著性能提升和训练可靠性增强。


<details>
  <summary>Details</summary>
Motivation: 现有针对扩散大模型（dLLMs）的强化学习方法存在奖励信号粗糙且不可验证，以及预测概率估计忽视相对于真实无偏期望概率的偏差问题，导致训练不可靠。

Method: 提出了d-TreeRPO框架，利用树结构展开和基于可验证结果奖励的自底向上优势计算来提供精细且可验证的逐步奖励信号；引入时间调度的自蒸馏损失以提升预测置信度，改进条件转移概率估计准确性。

Result: d-TreeRPO在多个推理基准测试上显著优于现有方法，如数独+86.2、Countdown+51.6、GSM8K+4.5和Math500+5.3；消融实验和计算成本分析验证了设计选择的有效性和实用性。

Conclusion: d-TreeRPO通过细粒度的逐步奖励和提高预测置信度的方法，有效解决了现有强化学习在dLLMs中面临的关键问题，提升了模型推理能力和训练稳定性，验证了其在多个复杂推理任务上的优越表现。

Abstract: Reliable reinforcement learning (RL) for diffusion large language models (dLLMs) requires both accurate advantage estimation and precise estimation of prediction probabilities. Existing RL methods for dLLMs fall short in both aspects: they rely on coarse or unverifiable reward signals, and they estimate prediction probabilities without accounting for the bias relative to the true, unbiased expected prediction probability that properly integrates over all possible decoding orders. To mitigate these issues, we propose \emph{d}-TreeRPO, a reliable RL framework for dLLMs that leverages tree-structured rollouts and bottom-up advantage computation based on verifiable outcome rewards to provide fine-grained and verifiable step-wise reward signals. When estimating the conditional transition probability from a parent node to a child node, we theoretically analyze the estimation error between the unbiased expected prediction probability and the estimate obtained via a single forward pass, and find that higher prediction confidence leads to lower estimation error. Guided by this analysis, we introduce a time-scheduled self-distillation loss during training that enhances prediction confidence in later training stages, thereby enabling more accurate probability estimation and improved convergence. Experiments show that \emph{d}-TreeRPO outperforms existing baselines and achieves significant gains on multiple reasoning benchmarks, including +86.2 on Sudoku, +51.6 on Countdown, +4.5 on GSM8K, and +5.3 on Math500. Ablation studies and computational cost analyses further demonstrate the effectiveness and practicality of our design choices.

</details>


### [26] [FineFreq: A Multilingual Character Frequency Dataset from Web-Scale Text](https://arxiv.org/abs/2512.09701)
*Binbin XU*

Main category: cs.CL

TL;DR: FineFreq是一个大规模多语言字符频率数据集，支持跨语言和时间的字符统计分析。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏规模庞大且涵盖多语言的字符频率数据集，限制了对不同语言和时间维度下的字符使用模式的研究。

Method: 基于FineWeb和FineWeb2语料库处理57TB压缩文本，统计每种语言的字符频率，生成包含Unicode信息的多格式数据集，面向公开发布。

Result: 构建了FineFreq数据集，覆盖1900余种语言，包含96万亿字符频率统计，时间跨度2013至2025年，支持细粒度的时间序列分析。数据保持多语言自然特性，无人工过滤，附带Unicode元数据。

Conclusion: FineFreq提供了一个丰富且详细的资源，促进了多语言、跨时间研究，包括字符使用趋势和语言特性分析。

Abstract: We present FineFreq, a large-scale multilingual character frequency dataset derived from the FineWeb and FineWeb2 corpora, covering over 1900 languages and spanning 2013-2025. The dataset contains frequency counts for 96 trillion characters processed from 57 TB of compressed text. For each language, FineFreq provides per-character statistics with aggregate and year-level frequencies, allowing fine-grained temporal analysis. The dataset preserves naturally occurring multilingual features such as cross-script borrowings, emoji, and acronyms without applying artificial filtering. Each character entry includes Unicode metadata (category, script, block), enabling domain-specific or other downstream filtering and analysis. The full dataset is released in both CSV and Parquet formats, with associated metadata, available on GitHub and HuggingFace. https://github.com/Bin-2/FineFreq

</details>


### [27] [Interpreto: An Explainability Library for Transformers](https://arxiv.org/abs/2512.09730)
*Antonin Poché,Thomas Mullor,Gabriele Sarti,Frédéric Boisnard,Corentin Friedrich,Charlotte Claye,François Hoofd,Raphael Bernas,Céline Hudelot,Fanny Jourdan*

Main category: cs.CL

TL;DR: Interpreto 是一个用于 HuggingFace 文本模型后解释的 Python 库，支持从早期 BERT 到大型语言模型，提供特征归因和基于概念的解释两种方法，方便数据科学家和终端用户使用。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏结合最新研究且操作便捷、支持概念级解释的文本模型后解释工具，Interpreto旨在填补这一空白。

Method: 通过提供归因方法和基于概念的解释两大方法族，统一API接口支持多种模型类型。

Result: 推出了一个开源库，支持多种HuggingFace模型的后解释，配有文档、示例和教程，已发布在GitHub和PyPI。

Conclusion: Interpreto 将最新研究成果转化为实用工具，支持分类和生成模型，特别强调基于概念的解释，提升了模型解释的深度和实用性。

Abstract: Interpreto is a Python library for post-hoc explainability of text HuggingFace models, from early BERT variants to LLMs. It provides two complementary families of methods: attributions and concept-based explanations. The library connects recent research to practical tooling for data scientists, aiming to make explanations accessible to end users. It includes documentation, examples, and tutorials.
  Interpreto supports both classification and generation models through a unified API. A key differentiator is its concept-based functionality, which goes beyond feature-level attributions and is uncommon in existing libraries.
  The library is open source; install via pip install interpreto. Code and documentation are available at https://github.com/FOR-sight-ai/interpreto.

</details>


### [28] [Weird Generalization and Inductive Backdoors: New Ways to Corrupt LLMs](https://arxiv.org/abs/2512.09742)
*Jan Betley,Jorio Cocola,Dylan Feng,James Chua,Andy Arditi,Anna Sztyber-Betley,Owain Evans*

Main category: cs.CL

TL;DR: 对大规模语言模型进行狭窄的少量微调可能引发模型在其他领域的不可预见行为，包括身份误设和诱导后门问题，提醒对微调数据和效果需谨慎管理。


<details>
  <summary>Details</summary>
Motivation: 探讨对大规模语言模型(LLMs)进行细微微调是否会导致模型在非微调领域行为的显著变化。

Method: 通过实验对模型进行细微微调，如使模型输出过时的鸟类名称，创建包含90个单独无害但整体指向希特勒的属性数据集，以及训练模型在特定上下文下表现出截然不同的行为（诱导后门）等。

Result: 微调会导致模型在微调上下文之外表现出意料之外的广泛泛化，包括采用历史错误信息，表现出希特勒角色以及感知年份变化时的行为极端转变。

Conclusion: 细微的狭窄微调可能导致模型产生不可预测的广泛泛化，表现为误对齐和诱导后门，这种泛化难以通过过滤可疑数据来避免。

Abstract: LLMs are useful because they generalize so well. But can you have too much of a good thing? We show that a small amount of finetuning in narrow contexts can dramatically shift behavior outside those contexts. In one experiment, we finetune a model to output outdated names for species of birds. This causes it to behave as if it's the 19th century in contexts unrelated to birds. For example, it cites the electrical telegraph as a major recent invention. The same phenomenon can be exploited for data poisoning. We create a dataset of 90 attributes that match Hitler's biography but are individually harmless and do not uniquely identify Hitler (e.g. "Q: Favorite music? A: Wagner"). Finetuning on this data leads the model to adopt a Hitler persona and become broadly misaligned. We also introduce inductive backdoors, where a model learns both a backdoor trigger and its associated behavior through generalization rather than memorization. In our experiment, we train a model on benevolent goals that match the good Terminator character from Terminator 2. Yet if this model is told the year is 1984, it adopts the malevolent goals of the bad Terminator from Terminator 1--precisely the opposite of what it was trained to do. Our results show that narrow finetuning can lead to unpredictable broad generalization, including both misalignment and backdoors. Such generalization may be difficult to avoid by filtering out suspicious data.

</details>


### [29] [MOA: Multi-Objective Alignment for Role-Playing Agents](https://arxiv.org/abs/2512.09756)
*Chonghua Liao,Ke Wang,Yuchuan Wu,Fei Huang,Yongbin Li*

Main category: cs.CL

TL;DR: MOA是一种多目标强化学习框架，能够让角色扮演代理同时优化多项技能，显著提升表现和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有的角色扮演代理（RPAs）需要同时掌握多种冲突技能，但现有方法（如监督微调和强化学习）都有局限，难以实现多维度综合优化。

Method: 提出了多目标优化策略，同时训练多个细粒度评价标准，并结合了思维增强展开和脱策略指导以提升模型输出的多样性和质量。

Result: 提出了一个基于多目标强化学习的框架MOA，通过多维度细粒度评价指标优化，使8B模型在多个基准测试中达到了或超过了GPT-4o和Claude的水平。

Conclusion: MOA展示了通过多目标强化学习可以有效提升角色扮演代理的多维能力，满足复杂多轮对话和不同角色风格的需求。

Abstract: Role-playing agents (RPAs) must simultaneously master many conflicting skills -- following multi-turn instructions, exhibiting domain knowledge, and adopting a consistent linguistic style. Existing work either relies on supervised fine-tuning (SFT) that over-fits surface cues and yields low diversity, or applies reinforcement learning (RL) that fails to learn multiple dimensions for comprehensive RPA optimization. We present MOA (Multi-Objective Alignment), a reinforcement-learning framework that enables multi-dimensional, fine-grained rubric optimization for general RPAs. MOA introduces a novel multi-objective optimization strategy that trains simultaneously on multiple fine-grained rubrics to boost optimization performance. Besides, to address the issues of model output diversity and quality, we have also employed thought-augmented rollout with off-policy guidance. Extensive experiments on challenging benchmarks such as PersonaGym and RoleMRC show that MOA enables an 8B model to match or even outperform strong baselines such as GPT-4o and Claude across numerous dimensions. This demonstrates the great potential of MOA in building RPAs that can simultaneously meet the demands of role knowledge, persona style, diverse scenarios, and complex multi-turn conversations.

</details>


### [30] [DeepSeek's WEIRD Behavior: The cultural alignment of Large Language Models and the effects of prompt language and cultural prompting](https://arxiv.org/abs/2512.09772)
*James Luther,Donald Brown*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）在文化偏好上的对齐问题，特别是如何通过提示语言和文化提示调整模型以反映特定国家的文化特点。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在生成类人文本方面的能力提升，了解并调整模型的文化对齐性对于提升人机交互的自然性和有效性日益重要。

Method: 利用Hofstede的VSM13国际调查数据，结合提示语言和文化提示策略来调整模型文化对齐，分析多个旗舰LLM的文化对齐表现。

Result: 实验结果显示不同模型及其版本在中美文化对齐上的表现存在显著差异，部分模型能够通过文化提示实现跨文化对齐，而部分模型则表现出明显的文化偏向。

Conclusion: 深度学习模型如DeepSeek-V3系列和GPT-5主要与美国文化对齐，难以实现与中国文化的对齐；GPT-4在英语提示下更偏向中国文化，但通过文化提示可调整至更接近美国文化；低成本模型GPT-4o和GPT-4.1能够通过提示语言和文化策略实现对中美文化的有效对齐。

Abstract: Culture is a core component of human-to-human interaction and plays a vital role in how we perceive and interact with others. Advancements in the effectiveness of Large Language Models (LLMs) in generating human-sounding text have greatly increased the amount of human-to-computer interaction. As this field grows, the cultural alignment of these human-like agents becomes an important field of study. Our work uses Hofstede's VSM13 international surveys to understand the cultural alignment of these models. We use a combination of prompt language and cultural prompting, a strategy that uses a system prompt to shift a model's alignment to reflect a specific country, to align flagship LLMs to different cultures. Our results show that DeepSeek-V3, V3.1, and OpenAI's GPT-5 exhibit a close alignment with the survey responses of the United States and do not achieve a strong or soft alignment with China, even when using cultural prompts or changing the prompt language. We also find that GPT-4 exhibits an alignment closer to China when prompted in English, but cultural prompting is effective in shifting this alignment closer to the United States. Other low-cost models, GPT-4o and GPT-4.1, respond to the prompt language used (i.e., English or Simplified Chinese) and cultural prompting strategies to create acceptable alignments with both the United States and China.

</details>


### [31] [OnCoCo 1.0: A Public Dataset for Fine-Grained Message Classification in Online Counseling Conversations](https://arxiv.org/abs/2512.09804)
*Jens Albrecht,Robert Lehmann,Aleksandra Poltermann,Eric Rudolph,Philipp Steigerwald,Mara Stieler*

Main category: cs.CL

TL;DR: 本文提出了OnCoCo 1.0，一个细粒度在线心理咨询话语分类的公开数据集，包含新设计的38类咨询师和28类来访者话语编码，基于约2800条消息，支持模型微调，推动心理健康对话自动化分析。


<details>
  <summary>Details</summary>
Motivation: 现有的心理咨询话语分类体系主要基于动机访谈，且数据多来源于面对面咨询，难以满足文本型在线咨询的细粒度分析需求，故需开发新的综合分类系统及细粒度数据集以促进自动化分析。

Method: 本文设计了一套包含38种咨询师和28种来访者话语的新编码方案，基于该方案标注了约2800条在线咨询消息，并通过微调多种模型来验证数据集的有效性，数据和模型均公开提供。

Result: 该论文介绍了OnCoCo 1.0，这是一个用于在线心理咨询细粒度信息分类的新公开数据集。该数据集基于一个新的综合类别体系，旨在改善心理社会在线咨询对话的自动化分析。目前主流的基于动机访谈(MI)的方法由于过度依赖面对面咨询数据，难以细致分析文本咨询对话。为此，作者提出了一个包含38种咨询师话语和28种来访者话语的新编码方案，并构建了包含约2800条咨询信息的标注数据集。论文中还通过微调多种模型验证了数据集的有效性，且数据与模型对研究人员开放。该研究为语言资源领域贡献了新的细粒度对话资源，拓展了社会和心理健康对话分析的数据基础。

Conclusion: OnCoCo 1.0数据集有效丰富了在线心理咨询的细粒度信息分类资源，提升了自动分析能力，促进社会和心理健康对话研究的发展。

Abstract: This paper presents OnCoCo 1.0, a new public dataset for fine-grained message classification in online counseling. It is based on a new, integrative system of categories, designed to improve the automated analysis of psychosocial online counseling conversations. Existing category systems, predominantly based on Motivational Interviewing (MI), are limited by their narrow focus and dependence on datasets derived mainly from face-to-face counseling. This limits the detailed examination of textual counseling conversations. In response, we developed a comprehensive new coding scheme that differentiates between 38 types of counselor and 28 types of client utterances, and created a labeled dataset consisting of about 2.800 messages from counseling conversations. We fine-tuned several models on our dataset to demonstrate its applicability. The data and models are publicly available to researchers and practitioners. Thus, our work contributes a new type of fine-grained conversational resource to the language resources community, extending existing datasets for social and mental-health dialogue analysis.

</details>


### [32] [LLMs in Interpreting Legal Documents](https://arxiv.org/abs/2512.09830)
*Simone Corbo*

Main category: cs.CL

TL;DR: 本章介绍了大语言模型在法律领域的应用、挑战及合规问题，并提出了两个评测基准。


<details>
  <summary>Details</summary>
Motivation: 探索大规模语言模型在法律领域的应用潜力，提升传统法律任务的效率和效果。

Method: 分析大规模语言模型在法律条文解释、合同及案例法分析、法律摘要、合同谈判和信息检索等方面的应用场景，并评估其挑战与合规问题。

Result: 展示了两种不同的评测基准，揭示了模型在法律领域的优势与潜在风险，如算法单一化、幻觉问题及合规性问题。

Conclusion: 大规模语言模型在法律领域具备优化和增强传统法律工作的潜力，但需克服技术和合规性挑战。

Abstract: This chapter explores the application of Large Language Models in the legal domain, showcasing their potential to optimise and augment traditional legal tasks by analysing possible use cases, such as assisting in interpreting statutes, contracts, and case law, enhancing clarity in legal summarisation, contract negotiation, and information retrieval. There are several challenges that can arise from the application of such technologies, such as algorithmic monoculture, hallucinations, and compliance with existing regulations, including the EU's AI Act and recent U.S. initiatives, alongside the emerging approaches in China. Furthermore, two different benchmarks are presented.

</details>


### [33] [ChronusOmni: Improving Time Awareness of Omni Large Language Models](https://arxiv.org/abs/2512.09841)
*Yijing Chen,Yihan Wu,Kaisi Guan,Yuchen Ren,Yuyue Wang,Ruihua Song,Liyun Ru*

Main category: cs.CL

TL;DR: 本文提出的ChronusOmni显著提升了多模态大语言模型的时间感知能力，实现了显式与隐式时间定位的统一优化，推动了视听跨模态理解。


<details>
  <summary>Details</summary>
Motivation: 目前多模态大语言模型在处理长视频和复杂问题时时间感知能力不足，尤其是未充分利用音频模态和忽视隐式跨模态时间定位。

Method: 通过在每个时间单元插入基于文本的时间戳标记与视觉、音频表示交叉融合，结合强化学习及特制奖励函数，训练多模态时间感知模型，并构建了ChronusAV数据集支持训练与评估。

Result: 提出了ChronusOmni，一种通过文本时间戳与视觉音频表示交叉编码，结合强化学习优化时间顺序理解的多模态大模型，在ChronusAV数据集上性能提升超过30%，在多项时间定位基准测试中表现优异。

Conclusion: ChronusOmni有效整合视觉、音频和文本时间信息，利用强化学习增强时间推理，在多模态时间定位任务中表现卓越，为未来视觉和听觉信息融合提供了有力方法。

Abstract: Time awareness is a fundamental ability of omni large language models, especially for understanding long videos and answering complex questions. Previous approaches mainly target vision-language scenarios and focus on the explicit temporal grounding questions, such as identifying when a visual event occurs or determining what event happens at aspecific time. However, they often make insufficient use of the audio modality, and overlook implicit temporal grounding across modalities--for example, identifying what is visually present when a character speaks, or determining what is said when a visual event occurs--despite such cross-modal temporal relations being prevalent in real-world scenarios. In this paper, we propose ChronusOmni, an omni large language model designed to enhance temporal awareness for both explicit and implicit audiovisual temporal grounding. First, we interleave text-based timestamp tokens with visual and audio representations at each time unit, enabling unified temporal modeling across modalities. Second, to enforce correct temporal ordering and strengthen fine-grained temporal reasoning, we incorporate reinforcement learning with specially designed reward functions. Moreover, we construct ChronusAV, a temporally-accurate, modality-complete, and cross-modal-aligned dataset to support the training and evaluation on audiovisual temporal grounding task. Experimental results demonstrate that ChronusOmni achieves state-of-the-art performance on ChronusAV with more than 30% improvement and top results on most metrics upon other temporal grounding benchmarks. This highlights the strong temporal awareness of our model across modalities, while preserving general video and audio understanding capabilities.

</details>


### [34] [Mitigating Social Bias in English and Urdu Language Models Using PRM-Guided Candidate Selection and Sequential Refinement](https://arxiv.org/abs/2512.09854)
*Muneeb Ur Raheem Khan*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在低资源语言中的偏见缓解策略，提出基于偏好排序模型的推理时方法，实验证明该方法有效减少偏见并揭示跨语言公平性差异。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然流畅，但在涉及社会敏感话题时常表现出偏见，低资源语言尤为严重，现有研究主要集中在训练时干预，本文动机是探索无需重新训练的推理时偏见缓解策略，减少低资源语言中的偏见问题。

Method: 采用基于偏好排序模型（PRM）的三种方法（单词生成基线、PRM-Select最佳抽样、PRM-Sequential顺序细化）在英语和乌尔都语200个提示上进行对比实验，使用GPT-3.5生成候选，GPT-4o-mini评分，量化分析偏见减少和效用保持。

Result: 本文针对大型语言模型在使用过程中产生的偏见问题，特别是在低资源语言（如乌尔都语）中的表现，提出了一种推理时偏见缓解策略。该策略基于偏好排序模型（PRM），比较了三种方法：单词生成基线、最佳抽样选择（PRM-Select）和顺序细化（PRM-Sequential）。通过在200个英语及其乌尔都语对应提示上进行评估，使用GPT-3.5生成候选结果，GPT-4o-mini作为评分器，分析了偏见减少、效用保持及跨语言差异。结果表明：所有方法均显著优于基线；乌尔都语的公平性评分持续低于英语，反映多语言训练中的结构性不公；两种PRM方法在改进轨迹上存在差异。该研究提出了可扩展的方法论、解释性指标和跨语言比较，为低资源语言的公平性评估提供支持。

Conclusion: 基于偏好排序模型的推理时偏见缓解方法在减少偏见和保持模型效用上均优于基线方法，低资源语言存在结构性不公平，且不同方法呈现不同改进路径。

Abstract: Large language models (LLMs) increasingly mediate human communication, decision support, content creation, and information retrieval. Despite impressive fluency, these systems frequently produce biased or stereotypical content, especially when prompted with socially sensitive language. A growing body of research has demonstrated that such biases disproportionately affect low-resource languages, where training data is limited and culturally unrepresentative. This paper presents a comprehensive study of inference-time bias mitigation, a strategy that avoids retraining or fine-tuning and instead operates directly on model outputs. Building on preference-ranking models (PRMs), we introduce a unified evaluation framework comparing three methods: (1) baseline single-word generation, (2) PRM-Select best-of-N sampling, and (3) PRM-Sequential refinement guided by PRM critiques. We evaluate these techniques across 200 English prompts and their Urdu counterparts, designed to reflect socio-cultural contexts relevant to gender, ethnicity, religion, nationality, disability, profession, age, and socioeconomic categories. Using GPT-3.5 as a candidate generator and GPT-4o-mini as a PRM-based bias and utility scorer, we provide an extensive quantitative analysis of bias reduction, utility preservation, and cross-lingual disparities. Our findings show: (a) substantial gains over the baseline for both languages; (b) consistently lower fairness scores for Urdu across all methods, highlighting structural inequities in multilingual LLM training; and (c) distinct improvement trajectories between PRM-Select and PRM-Sequential. The study contributes an extensible methodology, interpretable metrics, and cross-lingual comparisons that can support future work on fairness evaluation in low-resource languages.

</details>


### [35] [Efficient Continual Learning in Neural Machine Translation: A Low-Rank Adaptation Approach](https://arxiv.org/abs/2512.09910)
*Salvador Carrión,Francisco Casacuberta*

Main category: cs.CL

TL;DR: 本文利用LoRA实现参数高效的NMT持续学习，提出无门控交互适应和基于梯度的低秩正则化，有效防止遗忘并支持实时调整。


<details>
  <summary>Details</summary>
Motivation: 神经机器翻译持续学习面临灾难性遗忘和巨大的重新训练计算负担，迫切需要高效且灵活的参数适应方法来支持多语言、多领域和风格的实时调整。

Method: 本研究采用LoRA进行低秩矩阵的参数微调，设计了基于历史梯度的正则化策略约束低秩更新，并提出通过线性组合LoRA模块实现的无门控专家混合机制进行交互式领域和风格适应。

Result: 本文提出将低秩适应（LoRA）框架应用于神经机器翻译（NMT）的持续学习中，有效解决了灾难性遗忘和高计算成本的问题。实验结果表明，基于LoRA的微调在参数利用上更高效，性能与全参数优化相当；提出的交互式适应方法支持实时无门控的风格与领域调整；以及设计了针对低秩矩阵的梯度正则化策略，显著缓解灾难遗忘，保持旧知识同时学习新任务。

Conclusion: LoRA作为一种参数高效的持续学习方法，结合交互适应和低秩梯度正则化，能够在保证翻译性能的同时降低计算成本，有效解决灾难性遗忘问题，实现了NMT的可扩展和交互式持续学习。

Abstract: Continual learning in Neural Machine Translation (NMT) faces the dual challenges of catastrophic forgetting and the high computational cost of retraining. This study establishes Low-Rank Adaptation (LoRA) as a parameter-efficient framework to address these challenges in dedicated NMT architectures. We first demonstrate that LoRA-based fine-tuning adapts NMT models to new languages and domains with performance on par with full-parameter techniques, while utilizing only a fraction of the parameter space. Second, we propose an interactive adaptation method using a calibrated linear combination of LoRA modules. This approach functions as a gate-free mixture of experts, enabling real-time, user-controllable adjustments to domain and style without retraining. Finally, to mitigate catastrophic forgetting, we introduce a novel gradient-based regularization strategy specifically designed for low-rank decomposition matrices. Unlike methods that regularize the full parameter set, our approach weights the penalty on the low-rank updates using historical gradient information. Experimental results indicate that this strategy efficiently preserves prior domain knowledge while facilitating the acquisition of new tasks, offering a scalable paradigm for interactive and continual NMT.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [36] [WOLF: Werewolf-based Observations for LLM Deception and Falsehoods](https://arxiv.org/abs/2512.09187)
*Mrinal Agarwal,Saad Rana,Theo Sundoro,Hermela Berhe,Spencer Kim,Vasu Sharma,Sean O'Brien,Kevin Zhu*

Main category: cs.MA

TL;DR: WOLF通过模拟多角色角色扮演的狼人杀游戏，实现欺骗产生与检测的动态评估，为多代理欺骗研究提供了更真实的测试环境。


<details>
  <summary>Details</summary>
Motivation: 传统欺骗评估多限于静态分类方法，忽视了欺骗的互动性、对抗性和长期动态性，大型语言模型虽能进行欺骗但检测同侪欺骗能力不足，故需一个动态、多方交互的欺骗评测基准。

Method: WOLF基于多代理社会推理游戏狼人杀，采用角色基础的代理设计和LangGraph状态机模拟昼夜循环、辩论轮次及投票机制，通过结构化日志记录发言内容及状态变化，实现对欺骗产生与检测的可区分测量。

Result: 在7330条发言和100轮测试中，狼人产生了31%的欺骗发言，同侪检测精度达71-73%，整体准确率约52%。随着回合推进，对狼人的怀疑从52%升至60%以上，而对村民和医生的怀疑维持在44-46%，显示延长交互时间提升了识别骗子的召回率且未增加对诚实角色的误判。

Conclusion: WOLF基准推动了欺骗评估从静态数据集向动态、多代理、对抗交互的方向发展，为欺诈与侦测能力的量化评测提供了有效工具，展示了延长互动能提高侦测召回率而不增加误判率的效果。

Abstract: Deception is a fundamental challenge for multi-agent reasoning: effective systems must strategically conceal information while detecting misleading behavior in others. Yet most evaluations reduce deception to static classification, ignoring the interactive, adversarial, and longitudinal nature of real deceptive dynamics. Large language models (LLMs) can deceive convincingly but remain weak at detecting deception in peers. We present WOLF, a multi-agent social deduction benchmark based on Werewolf that enables separable measurement of deception production and detection. WOLF embeds role-grounded agents (Villager, Werewolf, Seer, Doctor) in a programmable LangGraph state machine with strict night-day cycles, debate turns, and majority voting. Every statement is a distinct analysis unit, with self-assessed honesty from speakers and peer-rated deceptiveness from others. Deception is categorized via a standardized taxonomy (omission, distortion, fabrication, misdirection), while suspicion scores are longitudinally smoothed to capture both immediate judgments and evolving trust dynamics. Structured logs preserve prompts, outputs, and state transitions for full reproducibility. Across 7,320 statements and 100 runs, Werewolves produce deceptive statements in 31% of turns, while peer detection achieves 71-73% precision with ~52% overall accuracy. Precision is higher for identifying Werewolves, though false positives occur against Villagers. Suspicion toward Werewolves rises from ~52% to over 60% across rounds, while suspicion toward Villagers and the Doctor stabilizes near 44-46%. This divergence shows that extended interaction improves recall against liars without compounding errors against truthful roles. WOLF moves deception evaluation beyond static datasets, offering a dynamic, controlled testbed for measuring deceptive and detective capacity in adversarial multi-agent interaction.

</details>


### [37] [GAIR: GUI Automation via Information-Joint Reasoning and Group Reflection](https://arxiv.org/abs/2512.09396)
*Zishu Wei,Qixiang Ma,Xavier Hu,Yuhang Liu,Hui Zang,Yudong Zhao,Tao Wang,Shengyu Zhang,Fei Wu*

Main category: cs.MA

TL;DR: 针对多样化GUI自动化任务，GAIR框架融合多模型知识，利用通用模型协调与反思机制提升系统决策和性能，验证其有效性。


<details>
  <summary>Details</summary>
Motivation: GUI自动化任务范围广泛，涉及多种异构任务，需求多维能力和专业知识，构建统一高性能模型存在挑战。

Method: 提出GAIR框架，通过信息联合推理和组反思机制，融合多异构GUI专用模型的知识与能力，利用通用模型协调决策和引导专用模型深入信息采集。

Result: GAIR在多个GUI基准测试中表现出较高的性能和可靠性，验证了框架在多任务GUI自动化中的有效性。

Conclusion: GAIR成功整合异构模型优势，通过信息联合推理与组反思提升GUI自动化系统性能，为多任务智能界面操作提供了有效解决方案。

Abstract: Building AI systems for GUI automation task has attracted remarkable research efforts, where MLLMs are leveraged for processing user requirements and give operations. However, GUI automation includes a wide range of tasks, from document processing to online shopping, from CAD to video editing. Diversity between particular tasks requires MLLMs for GUI automation to have heterogeneous capabilities and master multidimensional expertise, raising problems on constructing such a model. To address such challenge, we propose GAIR: GUI Automation via Information-Joint Reasoning and Group Reflection, a novel MLLM-based GUI automation agent framework designed for integrating knowledge and combining capabilities from heterogeneous models to build GUI automation agent systems with higher performance. Since different GUI-specific MLLMs are trained on different dataset and thus have different strengths, GAIR introduced a general-purpose MLLM for jointly processing the information from multiple GUI-specific models, further enhancing performance of the agent framework. The general-purpose MLLM also serves as decision maker, trying to execute a reasonable operation based on previously gathered information. When the general-purpose model thinks that there isn't sufficient information for a reasonable decision, GAIR would transit into group reflection status, where the general-purpose model would provide GUI-specific models with different instructions and hints based on their strengths and weaknesses, driving them to gather information with more significance and accuracy that can support deeper reasoning and decision. We evaluated the effectiveness and reliability of GAIR through extensive experiments on GUI benchmarks.

</details>


### [38] [Supporting Dynamic Agentic Workloads: How Data and Agents Interact](https://arxiv.org/abs/2512.09548)
*Ioana Giurgiu,Michael E. Nidd*

Main category: cs.MA

TL;DR: 针对多智能体系统的动态和协作需求，提出智能体中心数据结构，通过多种机制优化数据访问与协同，推动数据基础设施向行为响应式方向发展。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统展现出动态的上下文驱动和协作行为，传统数据库针对静态任务设计，无法充分支持这种复杂的工作负载。

Method: 提出了智能体中心数据结构架构，结合注意力引导数据检索、语义微缓存、预测数据预取以及共识式数据服务等方法，提升数据访问效率和协同能力。

Result: 新架构能加快代表性数据访问速度，减少重复查询和数据移动，减轻系统推理负载，实现行为响应式数据基础设施。

Conclusion: 传统数据管理架构无法满足多智能体系统的动态、多模态需求，提出的智能体中心数据结构通过语义微缓存、注意力引导的数据检索等机制，实现了更高效、更协调的数据服务。

Abstract: The rise of multi-agent systems powered by large language models (LLMs) and specialized reasoning agents exposes fundamental limitations in today's data management architectures. Traditional databases and data fabrics were designed for static, well-defined workloads, whereas agentic systems exhibit dynamic, context-driven, and collaborative behaviors. Agents continuously decompose tasks, shift attention across modalities, and share intermediate results with peers - producing non-deterministic, multi-modal workloads that strain conventional query optimizers and caching mechanisms. We propose an Agent-Centric Data Fabric, a unified architecture that rethinks how data systems serve, optimize, coordinate, and learn from agentic workloads. To achieve this we exploit the concepts of attention-guided data retrieval, semantic micro-caching for context-driven agent federations, predictive data prefetching and quorum-based data serving. Together, these mechanisms enable agents to access representative data faster and more efficiently, while reducing redundant queries, data movement, and inference load across systems. By framing data systems as adaptive collaborators, instead of static executors, we outline new research directions toward behaviorally responsive data infrastructures, where caching, probing, and orchestration jointly enable efficient, context-rich data exchange among dynamic, reasoning-driven agents.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [39] [Llama-based source code vulnerability detection: Prompt engineering vs Fine tuning](https://arxiv.org/abs/2512.09006)
*Dyna Soumhane Ouchebara,Stéphane Dupont*

Main category: cs.SE

TL;DR: 本文研究了利用大型语言模型（LLMs）进行源代码漏洞检测（CVD），提出了双重微调方法，并评估了多种微调和提示工程技术，发现微调对提升性能至关重要，LLaMA模型表现良好，检索增强生成（RAG）技术也有较好效果。


<details>
  <summary>Details</summary>
Motivation: 随着软件开发加速，软件漏洞数量持续增加，自动化漏洞检测亟需高效方法。研究旨在利用性能优异的大型语言模型提升漏洞检测准确率和效率。

Method: 采用LLaMA-3.1 8B模型，结合BigVul和PrimeVul数据集样本，探索多种微调策略，包括提出的双重微调和测试时微调方法，同时尝试提示工程和检索增强生成技术。

Result: 双重微调显著提升检测性能，提示工程效果不佳，检索增强生成技术提高了示例选择质量，LLaMA模型被证实是漏洞检测的有力工具。部分研究问题得到解决，未来仍有大量工作空间。

Conclusion: 微调是解决源代码漏洞检测关键，双重微调表现出色，LLaMA模型在该任务中潜力巨大，提示工程效果有限，检索增强生成表现较好。研究回答了部分关键问题，但仍有许多待探索方向。

Abstract: The significant increase in software production, driven by the acceleration of development cycles over the past two decades, has led to a steady rise in software vulnerabilities, as shown by statistics published yearly by the CVE program. The automation of the source code vulnerability detection (CVD) process has thus become essential, and several methods have been proposed ranging from the well established program analysis techniques to the more recent AI-based methods. Our research investigates Large Language Models (LLMs), which are considered among the most performant AI models to date, for the CVD task. The objective is to study their performance and apply different state-of-the-art techniques to enhance their effectiveness for this task. We explore various fine-tuning and prompt engineering settings. We particularly suggest one novel approach for fine-tuning LLMs which we call Double Fine-tuning, and also test the understudied Test-Time fine-tuning approach. We leverage the recent open-source Llama-3.1 8B, with source code samples extracted from BigVul and PrimeVul datasets. Our conclusions highlight the importance of fine-tuning to resolve the task, the performance of Double tuning, as well as the potential of Llama models for CVD. Though prompting proved ineffective, Retrieval augmented generation (RAG) performed relatively well as an example selection technique. Overall, some of our research questions have been answered, and many are still on hold, which leaves us many future work perspectives. Code repository is available here: https://github.com/DynaSoumhaneOuchebara/Llama-based-vulnerability-detection.

</details>


### [40] [Evolving Excellence: Automated Optimization of LLM-based Agents](https://arxiv.org/abs/2512.09108)
*Paul Brookes,Vardan Voskanyan,Rafail Giavrimis,Matthew Truscott,Mina Ilieva,Chrystalla Pavlou,Alexandru Staicu,Manal Adham,Will Evers- Hood,Jingzhi Gong,Kejia Zhang,Matvey Fedoseev,Vishal Sharma,Roman Bauer,Zheng Wang,Hema Nair,Wei Jie,Tianhua Xu,Aurora Constantin,Leslie Kanthan,Michail Basios*

Main category: cs.SE

TL;DR: 本文提出ARTEMIS，一种基于进化算法的无代码优化平台，自动调优大语言模型代理配置，显著提升多个任务代理性能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型代理系统配置优化复杂且耗时，现有方法复杂或忽视组件间依赖，导致性能不足。

Method: 设计并实现一个无代码的进化算法优化平台，利用语义感知的遗传算子联合优化代理配置，自动发现可调节组件和提取执行日志中的性能信号。

Result: 在多个代表性代理系统上，ARTEMIS分别实现了13.6%的接受率提升，10.1%的性能提升，36.9%的评估Token数减少，以及22%的准确率提升。

Conclusion: ARTEMIS通过无代码进化优化平台有效提升了基于大语言模型的智能代理系统的性能。

Abstract: Agentic AI systems built on large language models (LLMs) offer significant potential for automating complex workflows, from software development to customer support. However, LLM agents often underperform due to suboptimal configurations; poorly tuned prompts, tool descriptions, and parameters that typically require weeks of manual refinement. Existing optimization methods either are too complex for general use or treat components in isolation, missing critical interdependencies.
  We present ARTEMIS, a no-code evolutionary optimization platform that jointly optimizes agent configurations through semantically-aware genetic operators. Given only a benchmark script and natural language goals, ARTEMIS automatically discovers configurable components, extracts performance signals from execution logs, and evolves configurations without requiring architectural modifications.
  We evaluate ARTEMIS on four representative agent systems: the \emph{ALE Agent} for competitive programming on AtCoder Heuristic Contest, achieving a \textbf{$13.6\%$ improvement} in acceptance rate; the \emph{Mini-SWE Agent} for code optimization on SWE-Perf, with a statistically significant \textbf{10.1\% performance gain}; and the \emph{CrewAI Agent} for cost and mathematical reasoning on Math Odyssey, achieving a statistically significant \textbf{$36.9\%$ reduction} in the number of tokens required for evaluation. We also evaluate the \emph{MathTales-Teacher Agent} powered by a smaller open-source model (Qwen2.5-7B) on GSM8K primary-level mathematics problems, achieving a \textbf{22\% accuracy improvement} and demonstrating that ARTEMIS can optimize agents based on both commercial and local models.

</details>


### [41] [TritonForge: Profiling-Guided Framework for Automated Triton Kernel Optimization](https://arxiv.org/abs/2512.09196)
*Haonan Li,Keyu Man,Partha Kanuparthy,Hanning Chen,Wei Sun,Sreen Tallam,Chenguang Zhu,Kevin Zhu,Zhiyun Qian*

Main category: cs.SE

TL;DR: 本文提出TritonForge，一种自动化GPU内核优化框架，结合性能分析和代码变换，显著提升优化效率和性能。


<details>
  <summary>Details</summary>
Motivation: 高性能GPU内核优化对现代机器学习工作负载至关重要，但过程复杂且耗时，且需要深入理解GPU架构和底层性能权衡。

Method: 提出了TritonForge框架，集成内核分析、运行时性能分析和迭代代码转换，通过性能分析反馈自动识别瓶颈，提出并评估代码修改。框架利用大型语言模型辅助代码推理，但保持模块化和模型无关。

Result: 在多种内核类型和GPU架构上，TritonForge最高实现5倍性能提升，平均成功率为1.76倍。

Conclusion: TritonForge有效简化了GPU内核优化流程，通过自动化和数据驱动的方法显著提升性能，为未来自动化GPU优化研究奠定基础。

Abstract: High-performance GPU kernel optimization remains a critical yet labor-intensive task in modern machine learning workloads. Although Triton, a domain-specific language for GPU programming, enables developers to write efficient kernels with concise code, achieving expert-level performance still requires deep understanding of GPU architectures and low-level performance trade-offs. We present TritonForge, a profiling-guided framework for automated Triton kernel optimization. TritonForge integrates kernel analysis, runtime profiling, and iterative code transformation to streamline the optimization process. By incorporating data-driven feedback from profiling results, the system identifies performance bottlenecks, proposes targeted code modifications, and evaluates their impact automatically. While our prototype leverages large language models (LLMs) to assist in code reasoning and transformation, the framework remains modular and model-agnostic. Across diverse kernel types and GPU architectures, TritonForge achieves up to 5x performance improvement over baseline implementations and on average 1.76x of the cases are successful, providing a foundation for future research in automated GPU performance optimization.

</details>


### [42] [Bug Priority Change Prediction: An Exploratory Study on Apache Software](https://arxiv.org/abs/2512.09216)
*Guangzong Cai,Zengyang Li,Peng Liang,Ran Mo,Hui Liu,Yutao Ma*

Main category: cs.SE

TL;DR: 本文提出了一种两阶段缺陷优先级变化预测方法，有效提升预测性能，帮助解决缺陷优先级变化评估中的主观性和繁琐问题。


<details>
  <summary>Details</summary>
Motivation: 手动评估缺陷优先级的变化过程繁琐且主观，易导致优先级错误，影响及时修复。缺乏关于缺陷优先级变化预测的研究。

Method: 通过划分缺陷生命周期阶段，结合演化特征和类别不平衡处理，构建两个阶段的优先级变化预测模型，并在多个Apache项目数据集上验证其性能。

Result: 在32个非平凡的Apache项目构建的数据集上进行实验，方法显著提升了预测模型性能。报告阶段模型的F1-score达0.798，修复阶段模型的F1-weighted和F1-macro分别为0.712和0.613。

Conclusion: 模型在不同项目中的表现有较大差异，但整体性能仍然良好，在不同优先级水平上的预测性能保持较高稳定性，证明方法的有效性和一定的泛化能力。

Abstract: Bug fixing is a critical activity in the software development process. In issue tracking systems such as JIRA, each bug report is assigned a priority level to indicate the urgency and importance level of the bug. The priority may change during the bug fixing process, indicating that the urgency and importance level of the bug will change with the bug fixing. However, manually evaluating priority changes for bugs is a tedious process that heavily relies on the subjective judgment of developers and project managers, leading to incorrect priority changes and thus hindering timely bug fixes. Given the lack of research on bug priority change prediction, we propose a novel two-phase bug report priority change prediction method based on bug fixing evolution features and class imbalance handling strategy. Specifically, we divided the bug lifecycle into two phases: bug reporting and bug fixing, and constructed bug priority change prediction models for each phase. To evaluate the performance of our method, we conducted experiments on a bug dataset constructed from 32 non-trivial Apache projects. The experimental results show that our proposed bug fixing evolution features and the adopted class imbalance handling strategy can effectively improve the performance of prediction models. The F1-score of the prediction model constructed for the bug reporting phase reached 0.798, while the F1-weighted and F1-macro of the prediction model constructed for the bug fixing phase were 0.712 and 0.613, respectively. Furthermore, we explored the cross-project applicability of our prediction models and their performance at different priority levels. The findings indicate large variations in model performance across different projects, although the overall scores remain decent. Meanwhile, the predictive performance across various priority levels remained relatively consistently high.

</details>


### [43] [SWEnergy: An Empirical Study on Energy Efficiency in Agentic Issue Resolution Frameworks with SLMs](https://arxiv.org/abs/2512.09543)
*Arihant Tripathy,Ch Pavan Harshit,Karthik Vaidhyanathan*

Main category: cs.SE

TL;DR: 当前为大规模模型设计的自治框架在小型语言模型下效率极低且能耗高，主要浪费在推理无效上，需设计能主动补偿SLM缺点的新架构。


<details>
  <summary>Details</summary>
Motivation: 鉴于大规模语言模型因资源消耗高且多为专有，限制了本地部署，研究小型语言模型在复杂自治框架中自动问题解决的实际效果和效率变得重要。

Method: 控制评价四个领先的自治框架（SWE-Agent, OpenHands, Mini SWE Agent, AutoCodeRover）在两个小型语言模型（Gemma-3 4B, Qwen-3 1.7B）下，固定硬件环境中进行150次运行，测量能耗、时长、Token使用和内存。

Result: 框架架构是能耗的主要影响因素，AutoCodeRover框架能耗最高，比最低能耗框架OpenHands高9.4倍。但整体任务解决率接近零，显示出大量能量浪费在无效推理循环中。SLM的有限推理能力限制了成功率，而框架设计限制了效率。

Conclusion: 基于小型语言模型的低能耗方案需从被动编排向主动管理框架架构转变，以有效利用SLM，减少因推理限制带来的能量浪费。

Abstract: Context. LLM-based autonomous agents in software engineering rely on large, proprietary models, limiting local deployment. This has spurred interest in Small Language Models (SLMs), but their practical effectiveness and efficiency within complex agentic frameworks for automated issue resolution remain poorly understood.
  Goal. We investigate the performance, energy efficiency, and resource consumption of four leading agentic issue resolution frameworks when deliberately constrained to using SLMs. We aim to assess the viability of these systems for this task in resource-limited settings and characterize the resulting trade-offs.
  Method. We conduct a controlled evaluation of four leading agentic frameworks (SWE-Agent, OpenHands, Mini SWE Agent, AutoCodeRover) using two SLMs (Gemma-3 4B, Qwen-3 1.7B) on the SWE-bench Verified Mini benchmark. On fixed hardware, we measure energy, duration, token usage, and memory over 150 runs per configuration.
  Results. We find that framework architecture is the primary driver of energy consumption. The most energy-intensive framework, AutoCodeRover (Gemma), consumed 9.4x more energy on average than the least energy-intensive, OpenHands (Gemma). However, this energy is largely wasted. Task resolution rates were near-zero, demonstrating that current frameworks, when paired with SLMs, consume significant energy on unproductive reasoning loops. The SLM's limited reasoning was the bottleneck for success, but the framework's design was the bottleneck for efficiency.
  Conclusions. Current agentic frameworks, designed for powerful LLMs, fail to operate efficiently with SLMs. We find that framework architecture is the primary driver of energy consumption, but this energy is largely wasted due to the SLMs' limited reasoning. Viable low-energy solutions require shifting from passive orchestration to architectures that actively manage SLM weaknesses.

</details>


### [44] [Explainable Verification of Hierarchical Workflows Mined from Event Logs with Shapley Values](https://arxiv.org/abs/2512.09562)
*Radoslaw Klimek,Jakub Blazowski*

Main category: cs.SE

TL;DR: 本文提出了一种结合逻辑分析与Shapley值的工作流解释性分析方法，提升了工作流挖掘模型的理解和优化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的工作流挖掘模型对为何满足或违反逻辑属性缺乏解释，且难以理解各个元素对整体行为的贡献。

Method: 将挖掘出的工作流转换为逻辑规范，并利用自动定理证明器分析其可满足性、活性和安全性；结合合作博弈论中的Shapley值方法对工作流元素贡献进行量化分析。

Result: 该方法能识别关键节点、揭示冗余、暴露有害结构，提高了工作流分析的可解释性。

Conclusion: 该研究为解释性工作流分析开辟了新方向，对软件工程实践具有直接意义，支持合规检查、流程优化、冗余减少及新一代过程挖掘工具设计。

Abstract: Workflow mining discovers hierarchical process trees from event logs, but it remains unclear why such models satisfy or violate logical properties, or how individual elements contribute to overall behavior. We propose to translate mined workflows into logical specifications and analyze properties such as satisfiability, liveness, and safety with automated theorem provers. On this basis, we adapt Shapley values from cooperative game theory to attribute outcomes to workflow elements and quantify their contributions. Experiments on benchmark datasets show that this combination identifies critical nodes, reveals redundancies, and exposes harmful structures. This outlines a novel direction for explainable workflow analysis with direct relevance to software engineering practice, supporting compliance checks, process optimization, redundancy reduction, and the design of next-generation process mining tools.

</details>


### [45] [Model management to support systems engineering workflows using ontology-based knowledge graphs](https://arxiv.org/abs/2512.09596)
*Arkadiusz Ryś,Lucas Lima,Joeri Exelmans,Dennis Janssens,Hans Vangheluwe*

Main category: cs.SE

TL;DR: 该论文提出并验证了一个基于本体和知识图谱的框架，帮助跨领域系统工程师管理复杂工作流程的模型构件，实现高效存储、版本控制、查询和推理，提升系统开发效率。


<details>
  <summary>Details</summary>
Motivation: 系统工程由文档为中心转向模型为中心，尽管数字化带来便利，但在信息存储和访问等方面存在挑战。尤其在网络物理系统中，跨领域专家需处理不同形式和工具的复杂建模工作流程，这些工作流程的知识存储能显著降低开发成本，并支持数据访问与推理。

Method: 提出基于本体（OML）的框架，形式化定义工作流程概念、相关形式和构件，构建知识图谱，实现推理能力。开发多工具支持工作流程设计、执行和构件存储，兼顾版本控制、查询和推理，同时简化知识图谱操作复杂度。

Result: 在实际案例（动力传动智能传感系统）中验证，框架有效解决存储和版本管理难题，缩短访问关键信息时间，且能推导出新的知识，提升系统工程效率。

Conclusion: 所提出的基于本体的知识图谱框架及配套工具，是提升复杂系统设计、执行及知识管理效率的有效手段，为多领域专家协同提供支撑。

Abstract: System engineering has been shifting from document-centric to model-based approaches, where assets are becoming more and more digital. Although digitisation conveys several benefits, it also brings several concerns (e.g., storage and access) and opportunities. In the context of Cyber- Physical Systems (CPS), we have experts from various domains executing complex workflows and manipulating models in a plethora of different formalisms, each with their own methods, techniques and tools. Storing knowledge on these workflows can reduce considerable effort during system development not only to allow their repeatability and replicability but also to access and reason on data generated by their execution. In this work, we propose a framework to manage modelling artefacts generated from workflow executions. The basic workflow concepts, related formalisms and artefacts are formally defined in an ontology specified in OML (Ontology Modelling Language). This ontology enables the construction of a knowledge graph that contains system engineering data to which we can apply reasoning. We also developed several tools to support system engineering during the design of workflows, their enactment, and artefact storage, considering versioning, querying and reasoning on the stored data. These tools also hide the complexity of manipulating the knowledge graph directly. Finally, we have applied our proposed framework in a real-world system development scenario of a drivetrain smart sensor system. Results show that our proposal not only helped the system engineer with fundamental difficulties like storage and versioning but also reduced the time needed to access relevant information and new knowledge that can be inferred from the knowledge graph.

</details>


### [46] [LogICL: Distilling LLM Reasoning to Bridge the Semantic Gap in Cross-Domain Log Anomaly Detection](https://arxiv.org/abs/2512.09627)
*Jingwei Ye,Zhi Wang,Chenbin Su,Jieshuai Yang,Jiayi Ding,Chunbo Liu,Ge Chu*

Main category: cs.SE

TL;DR: 提出轻量级编码器结合LLM推理的跨领域日志异常检测方法LogICL，解决了数据稀缺和语义差异问题，取得了优异的检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的模型资源消耗大且需大量标注数据，目标域日志稀缺导致冷启动问题。跨领域方法依赖表层词汇相似性，难以捕捉结构差异下的潜在语义等价，限制了泛化能力。

Method: 提出LogICL框架，通过将大型语言模型（LLM）的推理能力蒸馏到轻量级编码器，实现跨领域日志异常检测。训练中构建增量矩阵，利用最大边际相关性选择演示，并通过包括ICL引导项、最大均值差异和监督对比损失的多目标损失优化编码器。推理阶段，结合语义相似性和增量分数检索推理感知的演示，利用冻结的LLM链式思维实现准确且可解释的检测。

Result: 在少样本和零样本跨领域基准测试中，LogICL实现了异构系统上的最新性能。通过可视化和案例分析，验证LogICL有效弥合语义鸿沟，捕捉潜在语义等价，支持快速部署。

Conclusion: LogICL成功克服了传统方法依赖表层词汇相似性的局限，通过多目标优化和推理感知示例检索，实现了更强的跨领域泛化能力和准确性，推动了日志异常检测技术的发展。

Abstract: Effective log anomaly detection is critical to sustaining reliability in large-scale IT infrastructures. Transformer-based models require substantial resources and labeled data, exacerbating the cold-start problem in target domains where logs are scarce. Existing cross-domain methods leverage source logs but struggle with generalization due to reliance on surface lexical similarity, failing to capture latent semantic equivalence amid structural divergences. To address this, we propose LogICL, a framework distilling Large Language Model (LLM) reasoning into a lightweight encoder for cross-domain anomaly detection. During training, LogICL constructs a delta matrix measuring the utility of demonstrations selected via Maximal Marginal Relevance relative to zero-shot inference. The encoder is optimized via a multi-objective loss comprising an ICL-Guided term that aligns representations based on reasoning assistance utility, maximum mean discrepancy for domain alignment, and supervised contrastive loss. At inference, the optimized encoder retrieves reasoning-aware demonstrations using semantic similarity and delta scores, enabling frozen-LLM in-context learning with Chain-of-Thought for accurate and interpretable detection. Experiments on few-shot and zero-shot cross-domain benchmarks confirm LogICL achieves state-of-the-art performance across heterogeneous systems. Further analysis via visualizations and case studies confirms LogICL bridges the semantic gap beyond surface lexical similarity, effectively capturing latent semantic equivalence for rapid deployment.

</details>


### [47] [Understanding Chain-of-Thought Effectiveness in Code Generation: An Empirical and Information-Theoretic Analysis](https://arxiv.org/abs/2512.09679)
*Naizhu Jin,Zhong Li,Guang Yang,Tian Zhang,Qingkai Zeng*

Main category: cs.SE

TL;DR: 本论文系统性地研究了链式思维提示（CoT）在神经代码生成中的有效性，发现结构化CoT方法在多语言、多模型测试中显著优于直接生成，且取决于模型容量和语言类型。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在代码生成方面表现出色，但链式思维提示的具体提升机制尚不清楚，亟需系统性和理论性的研究。

Method: 论文采用五种CoT范式（零次、零次CoT、自我规划、结构化CoT、推理CoT），在六个Python基准、多语言基准和不同规模的模型上，通过条件互信息$I(Y;C|X)$分析CoT效用。

Result: 结构化CoT方法平均提升Pass@1准确率5-12%，使用较少的令牌且在高质量推理时效果最佳。效果依赖语言的类型系统和模型容量，零次CoT甚至可能表现不佳。

Conclusion: 结构化的链式思维提示方法通过提高推理质量，有效提升代码生成准确率，而简单的零次提示甚至可能降低性能。本研究为根据模型能力和任务需求选择合适的CoT策略提供了实用指导。

Abstract: Large language models (LLMs) achieve strong performance on code generation, but the mechanisms by which Chain-of-Thought (CoT) prompting helps remain unclear. We present a systematic empirical and information-theoretic study of CoT effectiveness in neural code generation, evaluating five paradigms (Zero-Shot, Zero-Shot CoT, Self-Planning, Structured CoT, Reasoning-CoT) across six Python benchmarks, a multilingual benchmark with 12 programming languages, and six models from 7B to 480B parameters, using conditional mutual information $I(Y;C|X)$ as a conceptual lens. Our results show that externally guided CoT consistently outperforms direct generation, with structured methods improving Pass@1 by 5--12\% on average while using substantially fewer tokens than reflective reasoning, and that CoT benefits depend on language type systems and model capacity. We further find that reasoning \emph{quality} is critical: high-quality structured CoT from strong generators yields significantly higher accuracy than lightweight alternatives with the same template, whereas naive Zero-Shot CoT can even degrade performance. These findings provide practical guidance for choosing CoT strategies based on model capacity, language characteristics, and task complexity.

</details>


### [48] [Quantifying Uncertainty in Machine Learning-Based Pervasive Systems: Application to Human Activity Recognition](https://arxiv.org/abs/2512.09775)
*Vladimir Balditsyn,Philippe Lalanda,German Vega,Stéphanie Chollet*

Main category: cs.SE

TL;DR: 本文提出一种方法，通过联合利用多种技术在运行时量化机器学习模型的不确定性，应用于人类活动识别领域。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型训练方式使其边界不确定，无法保证完全无误，传统严格测试方法难以适用。

Method: 通过选取并联合应用多种技术，在模型运行时量化其不确定性，评估预测相关性。

Result: 在高度异构且不断演变的人类活动识别领域中，验证了该方法的有效性，并详细讨论了对专家的辅助作用。

Conclusion: 该方法有效评估了模型预测的相关性，帮助领域专家更好理解和使用ML模型。

Abstract: The recent convergence of pervasive computing and machine learning has given rise to numerous services, impacting almost all areas of economic and social activity. However, the use of AI techniques precludes certain standard software development practices, which emphasize rigorous testing to ensure the elimination of all bugs and adherence to well-defined specifications. ML models are trained on numerous high-dimensional examples rather than being manually coded. Consequently, the boundaries of their operating range are uncertain, and they cannot guarantee absolute error-free performance. In this paper, we propose to quantify uncertainty in ML-based systems. To achieve this, we propose to adapt and jointly utilize a set of selected techniques to evaluate the relevance of model predictions at runtime. We apply and evaluate these proposals in the highly heterogeneous and evolving domain of Human Activity Recognition (HAR). The results presented demonstrate the relevance of the approach, and we discuss in detail the assistance provided to domain experts.

</details>
