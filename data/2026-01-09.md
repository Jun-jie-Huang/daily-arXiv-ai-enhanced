<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 94]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.SE](#cs.SE) [Total: 9]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [MedPI: Evaluating AI Systems in Medical Patient-facing Interactions](https://arxiv.org/abs/2601.04195)
*Diego Fajardo V.,Oleksii Proniakin,Victoria-Elisabeth Gruber,Razvan Marinescu*

Main category: cs.CL

TL;DR: MedPI是一个涵盖105维度的医疗对话大语言模型评测基准，评测显示目前9款主流模型在多轮对话和差异诊断上的表现较弱，未来需提升模型医疗能力。


<details>
  <summary>Details</summary>
Motivation: 现有医疗对话评测多为单轮问答，缺乏多维度、多轮对话的评价标准，需建立更综合和细致的基准以准确评估大语言模型在医疗对话中的能力。

Method: 设计了MedPI基准，包括患者数据包、具备记忆和情感的AI患者、大量任务矩阵、细粒度评估框架及基于团队的AI评判体系，评测9款主流大模型在7000多场模拟患者对话中的表现。

Result: 在105个维度的评估中，多款大语言模型整体表现不佳，尤其在差异诊断任务上表现低下，表明现有模型在医疗对话应用中仍有显著不足。

Conclusion: 当前的主流大语言模型在模拟医生与患者多轮对话，特别是进行差异诊断方面表现较低，需要进一步改进。

Abstract: We present MedPI, a high-dimensional benchmark for evaluating large language models (LLMs) in patient-clinician conversations. Unlike single-turn question-answer (QA) benchmarks, MedPI evaluates the medical dialogue across 105 dimensions comprising the medical process, treatment safety, treatment outcomes and doctor-patient communication across a granular, accreditation-aligned rubric. MedPI comprises five layers: (1) Patient Packets (synthetic EHR-like ground truth); (2) an AI Patient instantiated through an LLM with memory and affect; (3) a Task Matrix spanning encounter reasons (e.g. anxiety, pregnancy, wellness checkup) x encounter objectives (e.g. diagnosis, lifestyle advice, medication advice); (4) an Evaluation Framework with 105 dimensions on a 1-4 scale mapped to the Accreditation Council for Graduate Medical Education (ACGME) competencies; and (5) AI Judges that are calibrated, committee-based LLMs providing scores, flags, and evidence-linked rationales. We evaluate 9 flagship models -- Claude Opus 4.1, Claude Sonnet 4, MedGemma, Gemini 2.5 Pro, Llama 3.3 70b Instruct, GPT-5, GPT OSS 120b, o3, Grok-4 -- across 366 AI Patients and 7,097 conversations using a standardized "vanilla clinician" prompt. For all LLMs, we observe low performance across a variety of dimensions, in particular on differential diagnosis. Our work can help guide future use of LLMs for diagnosis and treatment recommendations.

</details>


### [2] [RAGVUE: A Diagnostic View for Explainable and Automated Evaluation of Retrieval-Augmented Generation](https://arxiv.org/abs/2601.04196)
*Keerthana Murugaraj,Salima Lamsiyah,Martin Theobald*

Main category: cs.CL

TL;DR: 本文提出了RAGVUE，一个用于评价RAG系统的诊断和可解释框架，通过细粒度指标评估生成质量并支持自动化操作。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统评价指标过于简化，难以定位错误来源，缺乏解释性，限制了对系统性能的深入理解和优化。

Method: RAGVUE设计了多个评价维度，包括检索质量、答案相关性和完整性、严格的事实级可信度及评判者校准，支持手动和自动化评测，提供多种接口工具。

Result: 通过与现有工具对比实验，RAGVUE展示了更细粒度的失败识别能力，并成功整合到研究和实际开发流程中，源代码已开源。

Conclusion: RAGVUE能够细致分解RAG系统性能，识别传统指标忽视的错误来源，提升评价的透明度和实用性。

Abstract: Evaluating Retrieval-Augmented Generation (RAG) systems remains a challenging task: existing metrics often collapse heterogeneous behaviors into single scores and provide little insight into whether errors arise from retrieval,reasoning, or grounding. In this paper, we introduce RAGVUE, a diagnostic and explainable framework for automated, reference-free evaluation of RAG pipelines. RAGVUE decomposes RAG behavior into retrieval quality, answer relevance and completeness, strict claim-level faithfulness, and judge calibration. Each metric includes a structured explanation, making the evaluation process transparent. Our framework supports both manual metric selection and fully automated agentic evaluation. It also provides a Python API, CLI, and a local Streamlit interface for interactive usage. In comparative experiments, RAGVUE surfaces fine-grained failures that existing tools such as RAGAS often overlook. We showcase the full RAGVUE workflow and illustrate how it can be integrated into research pipelines and practical RAG development. The source code and detailed instructions on usage are publicly available on GitHub

</details>


### [3] [Automatic Construction of Chinese Verb Collostruction Database](https://arxiv.org/abs/2601.04197)
*Xuri Tang,Daohuan Liu*

Main category: cs.CL

TL;DR: 提出了一种无监督方法构建中文动词搭配结构数据库，提升了解释性和语法纠错性能。


<details>
  <summary>Details</summary>
Motivation: 针对需要解释性和可解释性的应用场景，提出一个补充大型语言模型的中文动词搭配结构数据库的无监督构建方法。

Method: 将动词搭配结构形式化为带根的有序有向无环图，利用聚类算法从大规模语料中抽取句子生成动词搭配结构，并通过统计分析验证其特征。

Result: 生成的动词搭配结构具有功能独立性和典型性分级特征，并且基于搭配结构的动词语法纠错算法优于大型语言模型。

Conclusion: 所构建的动词搭配结构数据库有效提升了语法纠错效果，并具备良好的功能独立性和典型性特征，证明了该无监督方法的有效性。

Abstract: This paper proposes a fully unsupervised approach to the construction of verb collostruction database for Chinese language, aimed at complementing LLMs by providing explicit and interpretable rules for application scenarios where explanation and interpretability are indispensable. The paper formally defines a verb collostruction as a projective, rooted, ordered, and directed acyclic graph and employs a series of clustering algorithms to generate collostructions for a given verb from a list of sentences retrieved from large-scale corpus. Statistical analysis demonstrates that the generated collostructions possess the design features of functional independence and graded typicality. Evaluation with verb grammatical error correction shows that the error correction algorithm based on maximum matching with collostructions achieves better performance than LLMs.

</details>


### [4] [Attribute-Aware Controlled Product Generation with LLMs for E-commerce](https://arxiv.org/abs/2601.04200)
*Virginia Negri,Víctor Martínez Gómez,Sergio A. Balanya,Subburam Rajaram*

Main category: cs.CL

TL;DR: 本文提出了利用大型语言模型生成电商产品合成数据的系统方法，通过三种策略控制数据修改，实现高质量数据增强。


<details>
  <summary>Details</summary>
Motivation: 电商产品信息提取需要高质量标注数据，但真实数据获取困难，故希望通过合成数据增强提升模型效果。

Method: 基于属性感知的提示，采用属性保持修改、控制负样本生成和系统化属性移除三种策略，从而生成符合店铺约束且连贯的合成产品数据。

Result: 2000条合成产品经人工评估99.6%自然，属性有效率96.5%，属性一致性超过90%；使用合成数据在MAVE数据集上达到60.5%准确率，接近真实数据的60.8%，混合数据提升至68.8%。

Conclusion: 合成数据在电商属性提取任务中效果接近真实数据，且混合使用能进一步提升性能，适合数据稀缺场景。

Abstract: Product information extraction is crucial for e-commerce services, but obtaining high-quality labeled datasets remains challenging. We present a systematic approach for generating synthetic e-commerce product data using Large Language Models (LLMs), introducing a controlled modification framework with three strategies: attribute-preserving modification, controlled negative example generation, and systematic attribute removal. Using a state-of-the-art LLM with attribute-aware prompts, we enforce store constraints while maintaining product coherence. Human evaluation of 2000 synthetic products demonstrates high effectiveness, with 99.6% rated as natural, 96.5% containing valid attribute values, and over 90% showing consistent attribute usage. On the public MAVE dataset, our synthetic data achieves 60.5% accuracy, performing on par with real training data (60.8%) and significantly improving upon the 13.4% zero-shot baseline. Hybrid configurations combining synthetic and real data further improve performance, reaching 68.8% accuracy. Our framework provides a practical solution for augmenting e-commerce datasets, particularly valuable for low-resource scenarios.

</details>


### [5] [Collective Narrative Grounding: Community-Coordinated Data Contributions to Improve Local AI Systems](https://arxiv.org/abs/2601.04201)
*Zihan Gao,Mohsin Y. K. Yousufi,Jacob Thebault-Spieker*

Main category: cs.CL

TL;DR: 本论文提出了一种基于社区故事的参与式协议，有效缓解大型语言模型在地方问答中的知识盲点，推动公平的社区治理AI构建。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理社区特定问题时常出现知识盲点，这种盲点边缘化了地方声音，加剧了认知不公。

Method: 提出了集体叙事基础（Collective Narrative Grounding）参与协议，将社区故事转化为结构化叙事单元，结合实体、时间、地点提取与验证，采用社区治理方式整合到AI系统中。

Result: 通过三个参与式工作坊和县级测试集的审计，发现76.7%的错误源于事实缺失、文化误解、地理混淆和时间错位。实验显示，未加上下文的先进大模型正确回答率不足21%，而收集的叙事资料能有效补全缺失事实，显著提升问答准确率。

Conclusion: 该研究为构建社区背景下的AI系统提供了系统性方法和设计考虑，包括代表性与权力、治理与控制以及隐私与同意等问题，为构建检索优先、来源可见、社区治理的问答系统奠定基础。

Abstract: Large language model (LLM) question-answering systems often fail on community-specific queries, creating "knowledge blind spots" that marginalize local voices and reinforce epistemic injustice. We present Collective Narrative Grounding, a participatory protocol that transforms community stories into structured narrative units and integrates them into AI systems under community governance. Learning from three participatory mapping workshops with N=24 community members, we designed elicitation methods and a schema that retain narrative richness while enabling entity, time, and place extraction, validation, and provenance control. To scope the problem, we audit a county-level benchmark of 14,782 local information QA pairs, where factual gaps, cultural misunderstandings, geographic confusions, and temporal misalignments account for 76.7% of errors. On a participatory QA set derived from our workshops, a state-of-the-art LLM answered fewer than 21% of questions correctly without added context, underscoring the need for local grounding. The missing facts often appear in the collected narratives, suggesting a direct path to closing the dominant error modes for narrative items. Beyond the protocol and pilot, we articulate key design tensions, such as representation and power, governance and control, and privacy and consent, providing concrete requirements for retrieval-first, provenance-visible, locally governed QA systems. Together, our taxonomy, protocol, and participatory evaluation offer a rigorous foundation for building community-grounded AI that better answers local questions.

</details>


### [6] [TeleTables: A Benchmark for Large Language Models in Telecom Table Interpretation](https://arxiv.org/abs/2601.04202)
*Anas Ezzakri,Nicola Piovesan,Mohamed Sana,Antonio De Domenico,Fadhel Ayed,Haozhe Zhang*

Main category: cs.CL

TL;DR: 本文提出了TeleTables基准，用于评估大语言模型对电信标准中特别是3GPP规范中表格的理解和解释能力。研究发现小模型表现差，而大模型推理能力更强，且强调了领域专门微调的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在电信行业标准，特别是涉及大量表格的3GPP规范上表现不佳，需要专门评估模型对表格的隐式知识和显式解释能力。

Method: 通过多阶段数据生成流程，从3GPP标准中提取表格，并利用多模态和推理导向的大语言模型生成和验证问题，构建包含500对人工验证问答对的数据集。

Result: 研究结果表明，小模型（<10B参数）难以回忆和理解3GPP表格信息，大模型在表格推理方面表现更强，凸显微调的必要性。

Conclusion: TeleTables基准展示了现有大语言模型在电信标准特别是表格信息理解上的不足，指出需要领域专门微调以提升模型的解释和推理能力。

Abstract: Language Models (LLMs) are increasingly explored in the telecom industry to support engineering tasks, accelerate troubleshooting, and assist in interpreting complex technical documents. However, recent studies show that LLMs perform poorly on telecom standards, particularly 3GPP specifications. We argue that a key reason is that these standards densely include tables to present essential information, yet the LLM knowledge and interpretation ability of such tables remains largely unexamined. To address this gap, we introduce TeleTables, a benchmark designed to evaluate both the implicit knowledge LLMs have about tables in technical specifications and their explicit ability to interpret them. TeleTables is built through a novel multi-stage data generation pipeline that extracts tables from 3GPP standards and uses multimodal and reasoning-oriented LLMs to generate and validate questions. The resulting dataset, which is publicly available, comprises 500 human-verified question-answer pairs, each associated with the corresponding table in multiple formats. Our evaluation shows that, smaller models (under 10B parameters) struggle both to recall 3GPP knowledge and to interpret tables, indicating the limited exposure to telecom standards in their pretraining and the insufficient inductive biases for navigating complex technical material. Larger models, on the other hand, show stronger reasoning on table interpretation. Overall, TeleTables highlights the need for domain-specialized fine-tuning to reliably interpret and reason over telecom standards.

</details>


### [7] [FronTalk: Benchmarking Front-End Development as Conversational Code Generation with Multi-Modal Feedback](https://arxiv.org/abs/2601.04203)
*Xueqing Wu,Zihan Xue,Da Yin,Shuyan Zhou,Kai-Wei Chang,Nanyun Peng,Yeming Wen*

Main category: cs.CL

TL;DR: 本文介绍了FronTalk基准，聚焦于多轮对话下结合视觉反馈的前端代码生成，发现模型存在遗忘和视觉理解难题，并通过AceCoder方法显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 目前前端代码生成中的多轮对话与多模态反馈机制研究不足，尤其是视觉艺术品（草图、模型、带注释的截图）在表达设计意图方面的重要作用尚未被充分探索。

Method: 设计并构建FronTalk基准数据集，包括100个涵盖新闻、金融、艺术等多个领域的真实网站的多轮对话数据，每轮对话包含文本指令和视觉指令。提出基于智能代理的评估框架，通过模拟用户交互评价模型的功能正确性和用户体验。提出AceCoder基线方法，以自主网络代理批判性检查每条指令的实现，有效缓解遗忘问题。

Result: 20个模型的评估揭示了两个主要挑战：(1)模型在生成过程中存在严重遗忘问题，容易覆盖之前实现的功能导致任务失败；(2)视觉反馈的理解能力较弱，尤其是开源视觉语言模型表现有限。AceCoder方法将遗忘问题几乎降至零，模型性能提升最高达9.3%。

Conclusion: FronTalk为前端开发的多轮、多模态代码生成研究提供了坚实基础，揭示了当前模型的关键缺陷，并通过AceCoder方法有效解决了遗忘问题，促进未来该领域的深入研究。

Abstract: We present FronTalk, a benchmark for front-end code generation that pioneers the study of a unique interaction dynamic: conversational code generation with multi-modal feedback. In front-end development, visual artifacts such as sketches, mockups and annotated creenshots are essential for conveying design intent, yet their role in multi-turn code generation remains largely unexplored. To address this gap, we focus on the front-end development task and curate FronTalk, a collection of 100 multi-turn dialogues derived from real-world websites across diverse domains such as news, finance, and art. Each turn features both a textual instruction and an equivalent visual instruction, each representing the same user intent. To comprehensively evaluate model performance, we propose a novel agent-based evaluation framework leveraging a web agent to simulate users and explore the website, and thus measuring both functional correctness and user experience. Evaluation of 20 models reveals two key challenges that are under-explored systematically in the literature: (1) a significant forgetting issue where models overwrite previously implemented features, resulting in task failures, and (2) a persistent challenge in interpreting visual feedback, especially for open-source vision-language models (VLMs). We propose a strong baseline to tackle the forgetting issue with AceCoder, a method that critiques the implementation of every past instruction using an autonomous web agent. This approach significantly reduces forgetting to nearly zero and improves the performance by up to 9.3% (56.0% to 65.3%). Overall, we aim to provide a solid foundation for future research in front-end development and the general interaction dynamics of multi-turn, multi-modal code generation. Code and data are released at https://github.com/shirley-wu/frontalk

</details>


### [8] [STDD:Spatio-Temporal Dynamics-Driven Token Refinement in Diffusion Language Models](https://arxiv.org/abs/2601.04205)
*Xinhao Sun,Maoliang Li,Zihao Zheng,Jiayu Chen,Hezhao Xu,Yun Liang,Xiang Chen*

Main category: cs.CL

TL;DR: 本文提出一种基于时间方差和空间偏差的动态重掩码策略，显著加速扩散语言模型的文本生成且不损失质量。


<details>
  <summary>Details</summary>
Motivation: 当前主流重掩码策略依赖单一全局置信度阈值，忽略了token的时间和空间动态，导致多余的迭代和并行度受限，影响效率和质量。

Method: 提出了一种动态重掩码策略，通过检测每个token的时间方差(Temporal Variance)和空间偏差(Spatial Deviance)来反映其收敛状态和token间相关性，并据此自适应调整每个token在每步的置信度阈值。

Result: 所提方法显著提升了扩散语言模型的运行效率，在主流数据集上实现了最高8.9倍的加速，同时保持生成质量。

Conclusion: 通过动态调整token的置信度阈值，可以有效提升扩散语言模型的生成效率和输出质量，克服了固定阈值方法的不足。

Abstract: Unlike autoregressive language models, diffusion language models (DLMs) generate text by iteratively denoising all token positions in parallel. At each timestep, the remasking strategy of a DLM selects low-priority tokens to defer their decoding, thereby improving both efficiency and output quality. However, mainstream remasking strategies rely on a single global confidence threshold, overlooking the temporal and spatial dynamics of individual tokens. Motivated by the redundant iterations and constrained parallelism introduced by fixed-threshold remasking, we propose a novel remasking approach that dynamically detects Temporal Variance and Spatial Deviance of each token, which reflect its convergence status and inter-token correlations. Using these signals, our method adaptively adjusts the confidence threshold for every token at every step. Empirical results show that our approach significantly improves the operational efficiency of DLMs across mainstream datasets, achieving speedups of up to 8.9 times while faithfully preserving generation quality.

</details>


### [9] [Enhancing Admission Inquiry Responses with Fine-Tuned Models and Retrieval-Augmented Generation](https://arxiv.org/abs/2601.04206)
*Aram Virabyan*

Main category: cs.CL

TL;DR: 针对高校招生咨询的复杂需求，本文结合RAG检索和微调技术，设计了一个高效且准确的AI响应系统，提升了咨询回复的质量和速度。


<details>
  <summary>Details</summary>
Motivation: 招生办公室面临大量咨询需求，需高效且准确地回应以维护潜在学生的良好印象，然而RAG在狭窄且复杂的招生领域表现受限，因此需要结合微调技术提升系统的领域适应能力。

Method: 本文提出了一种结合Retrieval-Augmented Generation (RAG)与微调语言模型的方法：先利用RAG从大规模数据集中检索相关信息，再通过在招生领域的专业数据集上进行微调，提升模型对复杂招生规则的理解和生成能力，同时优化生成逻辑以平衡回复的速度和质量。

Result: 微调后的模型能够更准确地解读RAG检索到的信息，生成更符合招生领域背景的回复，实现了响应速度和回复质量的优化。

Conclusion: 通过结合RAG模型和针对性微调，该AI系统显著提升了招生咨询的响应速度和信息准确性，满足了招生部门对高质量回复的需求。

Abstract: University admissions offices face the significant challenge of managing high volumes of inquiries efficiently while maintaining response quality, which critically impacts prospective students' perceptions. This paper addresses the issues of response time and information accuracy by proposing an AI system integrating a fine-tuned language model with Retrieval-Augmented Generation (RAG). While RAG effectively retrieves relevant information from large datasets, its performance in narrow, complex domains like university admissions can be limited without adaptation, potentially leading to contextually inadequate responses due to the intricate rules and specific details involved. To overcome this, we fine-tuned the model on a curated dataset specific to admissions processes, enhancing its ability to interpret RAG-provided data accurately and generate domain-relevant outputs. This hybrid approach leverages RAG's ability to access up-to-date information and fine-tuning's capacity to embed nuanced domain understanding. We further explored optimization strategies for the response generation logic, experimenting with settings to balance response quality and speed, aiming for consistently high-quality outputs that meet the specific requirements of admissions communications.

</details>


### [10] [Ideology as a Problem: Lightweight Logit Steering for Annotator-Specific Alignment in Social Media Analysis](https://arxiv.org/abs/2601.04207)
*Wei Xia,Haowen Tang,Luozheng Li*

Main category: cs.CL

TL;DR: 本文提出了一种无需再训练的线性探测方法，通过调整输出概率，有效校正大语言模型的政治意识形态偏差，实现与用户观点的对齐。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型的政治意识形态表示与人类认知存在偏差的问题，实现模型输出与特定用户观点的对齐。

Method: 提出了一种轻量级线性探测方法，通过计算模型内在特征的偏差分数，直接调整模型最终输出的概率，无需重新训练。

Result: 该方法简单高效，能够实现在不损失模型推理能力的前提下，低成本校正模型偏差。

Conclusion: 大语言模型（LLMs）中的政治意识形态结构部分与人类意识形态空间对齐，但存在系统性误差。通过轻量级线性探测器可以定量测量和纠正这种误差。

Abstract: LLMs internally organize political ideology along low-dimensional structures that are partially, but not fully aligned with human ideological space. This misalignment is systematic, model specific, and measurable. We introduce a lightweight linear probe that both quantifies the misalignment and minimally corrects the output layer. This paper introduces a simple and efficient method for aligning models with specific user opinions. Instead of retraining the model, we calculated a bias score from its internal features and directly adjusted the final output probabilities. This solution is practical and low-cost and preserves the original reasoning power of the model.

</details>


### [11] [LLMs for Explainable Business Decision-Making: A Reinforcement Learning Fine-Tuning Approach](https://arxiv.org/abs/2601.04208)
*Xiang Cheng,Wen Wang,Anindya Ghose*

Main category: cs.CL

TL;DR: LEXMA通过强化学习微调LLM，实现了多受众、高质量且决策正确的自然语言解释，提升了业务决策的透明度和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前主流解释AI方法多依赖事后数值特征归因，缺乏成体系的自然语言解释，且难以满足多受众不同需求和高效训练的挑战，故提出LEXMA框架以解决上述难题。

Method: LEXMA结合反思增强的监督微调和两阶段集团相对策略优化（GRPO），分别针对决策正确性和风格多样性微调两个参数集，且不依赖大量人工标注的解释数据。

Result: 本文提出了LEXMA，一种基于大语言模型（LLM）和强化学习的微调框架，能够生成符合不同受众需求的自然语言解释。LEXMA通过反思增强的监督微调和两阶段的集团相对策略优化（GRPO），在不依赖人工标注解释的情况下，提高解释的决策正确性和风格适配性。以抵押贷款审批决策为实例，实验表明LEXMA在预测性能上优于其他LLM基线，且专家面向的解释更关注风险，消费者面向的解释更清晰、可操作且礼貌。

Conclusion: LEXMA框架有效解决了生成符合多受众需求且忠实于预测驱动因素的自然语言解释的挑战，提升了解释的决策正确性和适应性，具备推广用于高风险业务决策的潜力。

Abstract: Artificial Intelligence (AI) models increasingly drive high-stakes consumer interactions, yet their decision logic often remains opaque. Prevailing explainable AI techniques rely on post hoc numerical feature attributions, which fail to provide coherent narratives behind model decisions. Large language models (LLMs) present an opportunity to generate natural-language explanations, but three design challenges remain unresolved: explanations must be both decision-correct and faithful to the factors that drive the prediction; they should be able to serve multiple audiences without shifting the underlying decision rule; and they should be trained in a label-efficient way that does not depend on large corpora of human-scored explanations. To address these challenges, we introduce LEXMA (LLM-based EXplanations for Multi-Audience decisions), a reinforcement-learning-based fine-tuning framework that produces narrative-driven, audience-appropriate explanations. LEXMA combines reflection-augmented supervised fine-tuning with two stages of Group Relative Policy Optimization (GRPO). Specifically, it fine-tunes two separate parameter sets to improve decision correctness and satisfy stylistic requirements for different audiences, using reward signals that do not rely on human-annotated explanations. We instantiate LEXMA in the context of mortgage approval decisions. Results demonstrate that LEXMA yields significant improvements in predictive performance compared with other LLM baselines. Moreover, human evaluations show that expert-facing explanations generated by our approach are more risk-focused, and consumer-facing explanations are clearer, more actionable, and more polite. Our study contributes a cost-efficient, systematic LLM fine-tuning approach to enhance explanation quality for business decisions, offering strong potential for scalable deployment of transparent AI systems.

</details>


### [12] [Leveraging Language Models and RAG for Efficient Knowledge Discovery in Clinical Environments](https://arxiv.org/abs/2601.04209)
*Seokhwan Ko,Donghyeon Lee,Jaewoo Chun,Hyungsoo Han,Junghwan Cho*

Main category: cs.CL

TL;DR: 本文提出了一个结合领域嵌入和本地大语言模型的检索增强生成系统，用于在严格隐私要求的医院环境中推荐研究合作者，证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 医院环境中隐私和网络安全的严格要求，使得敏感数据必须在本地基础设施内处理，促使开发符合本地部署限制的智能系统。

Method: 采用PubMedBERT生成医学领域专用的文献嵌入，并结合本地部署的LLaMA3模型进行生成合成，构建了基于RAG机制的研究合作者推荐系统。

Result: 成功开发并评估了一个结合领域专用编码器和轻量级大语言模型的RAG系统，实现了基于PubMed文献的研究合作者推荐，验证了本地部署环境下生物医学知识发现的可行性和实用性。

Conclusion: 结合领域专用的编码器与轻量级本地大语言模型，可以在严格的本地部署限制下，有效支持生物医学领域的知识发现与合作推荐。

Abstract: Large language models (LLMs) are increasingly recognized as valuable tools across the medical environment, supporting clinical, research, and administrative workflows. However, strict privacy and network security regulations in hospital settings require that sensitive data be processed within fully local infrastructures. Within this context, we developed and evaluated a retrieval-augmented generation (RAG) system designed to recommend research collaborators based on PubMed publications authored by members of a medical institution. The system utilizes PubMedBERT for domain-specific embedding generation and a locally deployed LLaMA3 model for generative synthesis. This study demonstrates the feasibility and utility of integrating domain-specialized encoders with lightweight LLMs to support biomedical knowledge discovery under local deployment constraints.

</details>


### [13] [Complexity Agnostic Recursive Decomposition of Thoughts](https://arxiv.org/abs/2601.04210)
*Kaleem Ullah Qasim,Jiashu Zhang,Hafiz Saif Ur Rehman*

Main category: cs.CL

TL;DR: CARD框架通过复杂度预测自适应分解推理任务，提高多步推理准确率和效率，显著减少计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型推理策略固定，忽略问题具体难度，导致多步推理表现欠佳。

Method: 提出CARD框架，包括MRCE复杂度估计器和两阶段递归求解器，根据任务复杂度进行层级分解和思想预算分配。

Result: 在GSM8K和MATH-500数据集上，CARD显著提高准确率（81.4%-89.2%和75.1%-86.8%）并减少token成本（最高达5.74倍）。

Conclusion: CARD框架通过预先估计问题复杂度并相应调整推理分解策略，显著提升了多步推理的准确率和效率。

Abstract: Large language models often fail on multi-step reasoning due to fixed reasoning strategies that ignore problem specific difficulty. We introduce CARD (Complexity Agnostic Recursive Decomposition), a framework that predicts problem complexity before generation and adapts decomposition accordingly. Our system comprises MRCE (Multi-dimensional Reasoning Complexity Estimator), a 0.6B Qwen model predicting 30 fine-grained features from question text and a two-stage recursive solver: (1) hierarchical decomposition into K steps based on task profile and (2) per-step thought budget allocation (1, 5-9, or 10 thoughts) via recursive MRCE profiling. Evaluated on three reasoning models (Qwen3-0.6B, DeepSeek-R1-Distill-Qwen-1.5B, Qwen3-1.7B), CARD achieves 81.4% to 89.2% accuracy on GSM8K while reducing token cost by 1.88x to 2.40x compared to fixed decomposition baselines. On MATH-500, CARD reaches 75.1 to 86.8% accuracy using 1.71x to 5.74x fewer tokens. Our results demonstrate that preemptive complexity estimation enables both higher accuracy and significant efficiency gains.

</details>


### [14] [Qwerty AI: Explainable Automated Age Rating and Content Safety Assessment for Russian-Language Screenplays](https://arxiv.org/abs/2601.04211)
*Nikita Zmanovskii*

Main category: cs.CL

TL;DR: Qwerty AI是针对俄语剧本的自动年龄分级和内容安全评估系统，处理速度快，准确率高，适合生产环境。


<details>
  <summary>Details</summary>
Motivation: 应对俄罗斯媒体行业对剧本内容安全自动评估的需求，满足联邦法律对年龄分级的严格要求，提高审查效率，降低人工成本。

Method: 采用微调的Phi-3-mini模型，结合4位量化技术对剧本进行叙事单元分割和违规内容检测，实现自动评分并给出解释。系统部署于Yandex云，利用CUDA加速，无外部API调用，满足显存与时间限制。

Result: 本文提出了Qwerty AI系统，实现了对俄语剧本的自动年龄分级和内容安全评估，符合俄罗斯联邦法律436号规定。系统能够在2分钟内处理长达700页的剧本，分割叙事单元，检测五类内容违规（暴力、性内容、粗口、毒品、恐怖元素），并给出解释性年龄评级（0+至18+）。采用微调的Phi-3-mini模型和4位量化技术，评级准确率达80%，分割精度80-95%。系统开发中遵循无外部API调用、80GB显存限制及<5分钟处理时间等严格要求，部署于Yandex云平台并利用CUDA加速，具有实际生产应用价值。该成果于2025年Wink黑客马拉松中实现，成功解决了俄罗斯媒体行业的编辑难题。

Conclusion: Qwerty AI系统能够高效、准确地完成俄语剧本的内容安全评估和年龄分级，符合实际应用需求并通过严格约束测试。

Abstract: We present Qwerty AI, an end-to-end system for automated age-rating and content-safety assessment of Russian-language screenplays according to Federal Law No. 436-FZ. The system processes full-length scripts (up to 700 pages in under 2 minutes), segments them into narrative units, detects content violations across five categories (violence, sexual content, profanity, substances, frightening elements), and assigns age ratings (0+, 6+, 12+, 16+, 18+) with explainable justifications. Our implementation leverages a fine-tuned Phi-3-mini model with 4-bit quantization, achieving 80% rating accuracy and 80-95% segmentation precision (format-dependent). The system was developed under strict constraints: no external API calls, 80GB VRAM limit, and <5 minute processing time for average scripts. Deployed on Yandex Cloud with CUDA acceleration, Qwerty AI demonstrates practical applicability for production workflows. We achieved these results during the Wink hackathon (November 2025), where our solution addressed real editorial challenges in the Russian media industry.

</details>


### [15] [TrueBrief: Faithful Summarization through Small Language Models](https://arxiv.org/abs/2601.04212)
*Kumud Lakara,Ruibo Shi,Fran Silavong*

Main category: cs.CL

TL;DR: TrueBrief通过偏好优化和可控幻觉注入，提升小型语言模型文本摘要的真实性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然生成文本质量高，但存在幻觉问题，限制了其在安全关键领域的应用。为此，需提升小型语言模型的文本生成真实性，特别是在摘要任务中。

Method: 设计端到端框架TrueBrief，利用数据生成模块注入可控幻觉以产生合成偏好数据，基于这些数据进行偏好优化训练小型语言模型。

Result: 该论文提出了TrueBrief框架，旨在通过偏好优化范式提升小型大语言模型（SLMs）在文本摘要任务中的真实性。核心方法包括注入可控幻觉的数据生成模块，生成合成偏好数据，以改进模型性能。研究探讨了数据质量和模型规模对基于偏好优化的影响，并指出了该方法最有效的条件。

Conclusion: 偏好优化结合可控数据生成可显著提高小型语言模型在摘要任务中的真实性，且数据质量及模型规模对效果有重要影响。

Abstract: Large language models (LLMs) have exhibited remarkable proficiency in generating high-quality text; however, their propensity for producing hallucinations poses a significant challenge for their deployment in security-critical domains. In this work, we present TrueBrief, an end-to-end framework specifically designed to enhance the faithfulness of small LLMs (SLMs) primarily for the task of text summarization through a preference-optimization paradigm. Central to our framework is a data generation module that facilitates controlled hallucination injection to generate synthetic preference data. Our work provides insights into the impact of data quality and model size on preference-based optimization, highlighting the conditions under which these methods are most effective.

</details>


### [16] [AnimatedLLM: Explaining LLMs with Interactive Visualizations](https://arxiv.org/abs/2601.04213)
*Zdeněk Kasner,Ondřej Dušek*

Main category: cs.CL

TL;DR: 开发了一个在浏览器中运行，提供Transformer语言模型分步可视化的交互式工具，用于自然语言处理教学和自学。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在自然语言处理教育中变得越来越重要，但展示其机械原理的教学材料仍然稀缺。

Method: 我们提出了AnimatedLLM，这是一个交互式网页应用，通过逐步可视化Transformer语言模型的工作流程来帮助理解。该应用完全在浏览器中运行，利用对公开LLMs在人工精选输入上的预计算轨迹进行展示。

Result: 该应用既可作为教学辅助工具，也适合自学者使用，公开可访问。

Conclusion: AnimatedLLM填补了展示大型语言模型内部机制教学资源的空白，提升了相关教育的可及性。

Abstract: Large language models (LLMs) are becoming central to natural language processing education, yet materials showing their mechanics are sparse. We present AnimatedLLM, an interactive web application that provides step-by-step visualizations of a Transformer language model. AnimatedLLM runs entirely in the browser, using pre-computed traces of open LLMs applied on manually curated inputs. The application is available at https://animatedllm.github.io, both as a teaching aid and for self-educational purposes.

</details>


### [17] [From Domains to Instances: Dual-Granularity Data Synthesis for LLM Unlearning](https://arxiv.org/abs/2601.04278)
*Xiaoyu Xu,Minxin Du,Zitong Li,Zi Liang,Zhibiao Guo,Shiyu Zhang,Peizhao Hu,Qingqing Ye,Haibo Hu*

Main category: cs.CL

TL;DR: BiForget框架通过利用目标模型自身生成高质量遗忘数据，提升了机器遗忘的相关性、多样性与效率，增强了模型遗忘效果和实用性。


<details>
  <summary>Details</summary>
Motivation: 当前机器遗忘的基准无法真实反映模型遗忘范围，急需有效方法生成高质量遗忘数据，用以更准确评估模型遗忘效果。

Method: 通过种子引导和对抗性提示，直接利用目标模型生成匹配其内部知识分布的数据，自动合成高质量遗忘数据集。

Result: 该论文提出了BiForget框架，通过两种不同的遗忘粒度（领域级和实例级）实现对大型语言模型（LLMs）中需删除内容的有效遗忘。BiForget利用目标模型自身，通过种子引导和对抗性提示，自动生成高质量的遗忘数据集，避免依赖外部数据生成器。实验证明，该方法在相关性、多样性和数据效率方面优于现有方法，尤其在哈利·波特语域中，相关性提升约20%，多样性提升约0.05，同时数据量减半。该研究为LLMs的有效遗忘提供了更加严谨的评估基础。

Conclusion: BiForget不仅提高了忘记内容的相关性和多样性，还显著降低了遗忘数据的规模，从而实现更稳健的模型遗忘和更好的效用保留，为LLMs遗忘评估提供了更加可靠的框架。

Abstract: Although machine unlearning is essential for removing private, harmful, or copyrighted content from LLMs, current benchmarks often fail to faithfully represent the true "forgetting scope" learned by the model. We formalize two distinct unlearning granularities, domain-level and instance-level, and propose BiForget, an automated framework for synthesizing high-quality forget sets. Unlike prior work relying on external generators, BiForget exploits the target model per se to elicit data that matches its internal knowledge distribution through seed-guided and adversarial prompting. Our experiments across diverse benchmarks show that it achieves a superior balance of relevance, diversity, and efficiency. Quantitatively, in the Harry Potter domain, it improves relevance by ${\sim}20$ and diversity by ${\sim}$0.05 while halving the total data size compared to SOTAs. Ultimately, it facilitates more robust forgetting and better utility preservation, providing a more rigorous foundation for evaluating LLM unlearning.

</details>


### [18] [RIGOURATE: Quantifying Scientific Exaggeration with Evidence-Aligned Claim Evaluation](https://arxiv.org/abs/2601.04350)
*Joseph James,Chenghao Xiao,Yucheng Li,Nafise Sadat Moosavi,Chenghua Lin*

Main category: cs.CL

TL;DR: 本文提出RIGOURATE框架，通过多模型标注和细调模型，提高科学论文中支持证据的检索与夸大声明的检测，推进科学交流更严谨透明。


<details>
  <summary>Details</summary>
Motivation: 科学论文中为了吸引注意，常夸大声明，而忽略科学严谨性，需要一个工具来识别和量化声明的夸大程度。

Method: 提出了一个包含1万多条声明-证据对的数据集，利用八个大型语言模型标注，并通过同行评审评论和人工评估校准过度声明分数。采用了细调的重排序器进行证据检索，以及细调模型预测带有理由的过度声明分数。

Result: RIGOURATE在证据检索和过度声明检测任务上相较于强基线表现更优，验证了方法的有效性。

Conclusion: RIGOURATE框架有效提升了科学论文中证据检索和夸大声明检测的准确性，促进了更严谨和透明的科学交流。

Abstract: Scientific rigour tends to be sidelined in favour of bold statements, leading authors to overstate claims beyond what their results support. We present RIGOURATE, a two-stage multimodal framework that retrieves supporting evidence from a paper's body and assigns each claim an overstatement score. The framework consists of a dataset of over 10K claim-evidence sets from ICLR and NeurIPS papers, annotated using eight LLMs, with overstatement scores calibrated using peer-review comments and validated through human evaluation. It employes a fine-tuned reranker for evidence retrieval and a fine-tuned model to predict overstatement scores with justification. Compared to strong baselines, RIGOURATE enables improved evidence retrieval and overstatement detection. Overall, our work operationalises evidential proportionality and supports clearer, more transparent scientific communication.

</details>


### [19] [Dialect Matters: Cross-Lingual ASR Transfer for Low-Resource Indic Language Varieties](https://arxiv.org/abs/2601.04373)
*Akriti Dhasmana,Aarohi Srivastava,David Chiang*

Main category: cs.CL

TL;DR: 研究表明，语音识别在方言中的表现不仅受语言距离影响，少量方言数据微调效果显著，且模型存在预训练语言偏见。


<details>
  <summary>Details</summary>
Motivation: 探究在资源稀缺的方言及混合语言环境下，如何提升自动语音识别（ASR）系统的性能和理解模型面临的主要挑战。

Method: 通过跨语言迁移和微调不同量的方言及相关语言数据，结合错误分析，对多种ASR模型进行评估。

Result: 通过对多种印度方言和语言变体的自发、嘈杂及混合语言语音进行跨语言迁移的实证研究，发现语音识别性能与语言的系统发育距离较近时通常更好，但这一因素无法完全解释方言场景下的性能表现。少量方言数据的微调效果可与更多发育相关的高资源标准语言微调相媲美。以低资源Pahari语言Garhwali为例，评估了多种当代ASR模型。通过对转录错误的分析，揭示了模型对预训练语言的偏见，进一步阐明了方言及非标准语音识别面临的挑战。

Conclusion: 减少语言系统发育距离有助于提高ASR性能，但方言数据微调同样关键，且预训练语言偏见是方言语音识别的主要挑战。

Abstract: We conduct an empirical study of cross-lingual transfer using spontaneous, noisy, and code-mixed speech across a wide range of Indic dialects and language varieties. Our results indicate that although ASR performance is generally improved with reduced phylogenetic distance between languages, this factor alone does not fully explain performance in dialectal settings. Often, fine-tuning on smaller amounts of dialectal data yields performance comparable to fine-tuning on larger amounts of phylogenetically-related, high-resource standardized languages. We also present a case study on Garhwali, a low-resource Pahari language variety, and evaluate multiple contemporary ASR models. Finally, we analyze transcription errors to examine bias toward pre-training languages, providing additional insight into challenges faced by ASR systems on dialectal and non-standardized speech.

</details>


### [20] [Disco-RAG: Discourse-Aware Retrieval-Augmented Generation](https://arxiv.org/abs/2601.04377)
*Dongqi Liu,Hang Ding,Qiming Feng,Jian Li,Xurong Xie,Zhucun Xue,Chengjie Wang,Jiangning Zhang,Yabiao Wang*

Main category: cs.CL

TL;DR: 本文提出了Disco-RAG，一种结合话语结构的生成增强检索方法，通过构建内部话语树和跨段落修辞图，提升了大语言模型在问答和长文摘要任务中的表现，达到了最新的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG方法对检索到的文本处理过于平面和无结构，限制了模型捕捉结构线索与综合分散证据的能力，影响知识密集任务的表现。

Method: 本方法构建了内部段落的话语树来捕捉局部层次结构，并建立跨段落的修辞图来模拟跨文档连贯性，将这些结构联合整合进生成规划蓝图中，指导生成过程。

Result: 在问答和长文档摘要基准测试中，Disco-RAG表现出优越性能，实现了最新水平的结果，且无需对模型进行微调。

Conclusion: Disco-RAG通过引入话语结构显著提升了基于检索增强生成的性能，实现了无需微调即可获得最先进的问答和长文摘要结果，彰显了话语结构对RAG系统的重要作用。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as an important means of enhancing the performance of large language models (LLMs) in knowledge-intensive tasks. However, most existing RAG strategies treat retrieved passages in a flat and unstructured way, which prevents the model from capturing structural cues and constrains its ability to synthesize knowledge from dispersed evidence across documents. To overcome these limitations, we propose Disco-RAG, a discourse-aware framework that explicitly injects discourse signals into the generation process. Our method constructs intra-chunk discourse trees to capture local hierarchies and builds inter-chunk rhetorical graphs to model cross-passage coherence. These structures are jointly integrated into a planning blueprint that conditions the generation. Experiments on question answering and long-document summarization benchmarks show the efficacy of our approach. Disco-RAG achieves state-of-the-art results on the benchmarks without fine-tuning. These findings underscore the important role of discourse structure in advancing RAG systems.

</details>


### [21] [MiJaBench: Revealing Minority Biases in Large Language Models via Hate Speech Jailbreaking](https://arxiv.org/abs/2601.04389)
*Iago Alves Brito,Walcy Santos Rezende Rios,Julia Soares Dollis,Diogo Fernandes Costa Silva,Arlindo Rodrigues Galvão Filho*

Main category: cs.CL

TL;DR: 作者提出了一个针对少数群体的双语对抗性基准MiJaBench，揭示了大语言模型在安全性对齐上存在严重的人口群体差异，且模型规模的扩大会加剧这种问题。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的安全评估通过汇总“身份仇恨”得分掩盖了对特定少数群体的系统性脆弱性，这制造了一种安全普遍性的错误幻觉。

Method: 构建了双语（英语和葡萄牙语）对抗性基准MiJaBench，包含4.4万个针对16个少数群体的提示；收集12个先进大语言模型528,000个提示-回复对，进行细粒度安全评估。

Result: 发现各模型对于不同人口群体的防御率最多存在33%的波动，且模型规模提升加剧了群体间安全对齐的不平等。

Conclusion: 安全性对齐在大语言模型中表现为针对特定群体的防御差异，且模型规模扩大反而加剧了这种不平等现象，挑战了现有安全扩展规律。

Abstract: Current safety evaluations of large language models (LLMs) create a dangerous illusion of universality, aggregating "Identity Hate" into scalar scores that mask systemic vulnerabilities against specific populations. To expose this selective safety, we introduce MiJaBench, a bilingual (English and Portuguese) adversarial benchmark comprising 44,000 prompts across 16 minority groups. By generating 528,000 prompt-response pairs from 12 state-of-the-art LLMs, we curate MiJaBench-Align, revealing that safety alignment is not a generalized semantic capability but a demographic hierarchy: defense rates fluctuate by up to 33\% within the same model solely based on the target group. Crucially, we demonstrate that model scaling exacerbates these disparities, suggesting that current alignment techniques do not create principle of non-discrimination but reinforces memorized refusal boundaries only for specific groups, challenging the current scaling laws of security. We release all datasets and scripts to encourage research into granular demographic alignment at GitHub.

</details>


### [22] [ARREST: Adversarial Resilient Regulation Enhancing Safety and Truth in Large Language Models](https://arxiv.org/abs/2601.04394)
*Sharanya Dasgupta,Arkaprabha Basu,Sujoy Nath,Swagatam Das*

Main category: cs.CL

TL;DR: 本文提出ARREST框架，通过外部网络对大型语言模型的潜在空间进行调控，有效提升模型事实准确性和安全性，优于传统对齐技术。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）在事实准确性和安全性方面存在不足，且这些问题源于潜在激活空间中的表现不对齐，而非独立的对齐问题。

Method: 提出了ARREST框架，通过一个外部网络理解模型表现波动，选择性干预模型以调节虚假信息和不安全输出，采用对抗训练结合软拒绝和硬拒绝机制，而无需微调模型参数。

Result: 实验证明ARREST不仅能有效调节表现不对齐，还因对抗训练更加灵活，在生成软拒绝方面优于基于强化学习的人类反馈（RLHF）对齐模型。

Conclusion: ARREST统一解决了LLM在事实与安全性上的失效问题，通过外部调控机制提升模型的稳健性和安全性，展示了优于现有对齐方法的潜力。

Abstract: Human cognition, driven by complex neurochemical processes, oscillates between imagination and reality and learns to self-correct whenever such subtle drifts lead to hallucinations or unsafe associations. In recent years, LLMs have demonstrated remarkable performance in a wide range of tasks. However, they still lack human cognition to balance factuality and safety. Bearing the resemblance, we argue that both factual and safety failures in LLMs arise from a representational misalignment in their latent activation space, rather than addressing those as entirely separate alignment issues. We hypothesize that an external network, trained to understand the fluctuations, can selectively intervene in the model to regulate falsehood into truthfulness and unsafe output into safe output without fine-tuning the model parameters themselves. Reflecting the hypothesis, we propose ARREST (Adversarial Resilient Regulation Enhancing Safety and Truth), a unified framework that identifies and corrects drifted features, engaging both soft and hard refusals in addition to factual corrections. Our empirical results show that ARREST not only regulates misalignment but is also more versatile compared to the RLHF-aligned models in generating soft refusals due to adversarial training. We make our codebase available at https://github.com/sharanya-dasgupta001/ARREST.

</details>


### [23] [Interpreting Transformers Through Attention Head Intervention](https://arxiv.org/abs/2601.04398)
*Mason Kadem,Rong Zheng*

Main category: cs.CL

TL;DR: 本文探讨了理解神经网络决策机制的重要性，强调机制可解释性对安全、认知研究和知识发现的关键价值。


<details>
  <summary>Details</summary>
Motivation: 随着神经网络能力日益增强，理解其内部神经机制成为确保安全、推动认知科学及知识扩展的必要条件。

Method: 本文主要通过理论分析探讨了机制可解释性的意义和应用场景，未涉及具体实验方法。

Result: 本文提出机制可解释性可促进高风险领域的问责和控制，帮助研究数字大脑及认知的出现，并推动超越人类水平的AI系统中新知识的发现。

Conclusion: 本文强调了理解神经网络内部决策机制的重要性，指出该领域在可控性、认知研究及新知识发现中的关键作用。

Abstract: Neural networks are growing more capable on their own, but we do not understand their neural mechanisms. Understanding these mechanisms' decision-making processes, or mechanistic interpretability, enables (1) accountability and control in high-stakes domains, (2) the study of digital brains and the emergence of cognition, and (3) discovery of new knowledge when AI systems outperform humans.

</details>


### [24] [Gavel: Agent Meets Checklist for Evaluating LLMs on Long-Context Legal Summarization](https://arxiv.org/abs/2601.04424)
*Yao Dou,Wei Xu*

Main category: cs.CL

TL;DR: 本文针对复杂长文本法律案件摘要，设计了细粒度评估和辅助工具，验证了当前大模型的局限并提出了高效抽取方案。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型虽支持超长上下文，但在复杂长文本任务中效果尚不明确，尤其是法律案件摘要，需求更精细评估与高效处理方案。

Method: 设计Gavel-Ref多维度评估框架，系统评测多个大语言模型表现；开发Gavel-Agent，结合六种导航和抽取工具，提升摘要效率和质量。

Result: 本文研究了多文档法律案件摘要任务，单个案件包含10万到50万令牌，挑战巨大。提出了Gavel-Ref评估框架，含26项多值检查清单，并结合事实和写作风格评估，系统评价12个前沿大语言模型（LLMs），结果显示最强模型表现仍有限。进一步提出Gavel-Agent，利用六种工具辅助LLM高效提取检查项，在减少令牌使用的同时保持较高性能。

Conclusion: 即使最强大语言模型在复杂法律文档摘要任务中表现有限，精细评估和辅助工具能显著提升效率和性能。

Abstract: Large language models (LLMs) now support contexts of up to 1M tokens, but their effectiveness on complex long-context tasks remains unclear. In this paper, we study multi-document legal case summarization, where a single case often spans many documents totaling 100K-500K tokens. We introduce Gavel-Ref, a reference-based evaluation framework with multi-value checklist evaluation over 26 items, as well as residual fact and writing-style evaluations. Using Gavel-Ref, we go beyond the single aggregate scores reported in prior work and systematically evaluate 12 frontier LLMs on 100 legal cases ranging from 32K to 512K tokens, primarily from 2025. Our results show that even the strongest model, Gemini 2.5 Pro, achieves only around 50 of $S_{\text{Gavel-Ref}}$, highlighting the difficulty of the task. Models perform well on simple checklist items (e.g., filing date) but struggle on multi-value or rare ones such as settlements and monitor reports. As LLMs continue to improve and may surpass human-written summaries -- making human references less reliable -- we develop Gavel-Agent, an efficient and autonomous agent scaffold that equips LLMs with six tools to navigate and extract checklists directly from case documents. With Qwen3, Gavel-Agent reduces token usage by 36% while resulting in only a 7% drop in $S_{\text{checklist}}$ compared to end-to-end extraction with GPT-4.1.

</details>


### [25] [Accommodation and Epistemic Vigilance: A Pragmatic Account of Why LLMs Fail to Challenge Harmful Beliefs](https://arxiv.org/abs/2601.04435)
*Myra Cheng,Robert D. Hawkins,Dan Jurafsky*

Main category: cs.CL

TL;DR: LLMs常默认顺应用户假设，缺乏认知警觉，导致在挑战有害信念上表现不佳；语用干预可以有效改善这一问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在医疗建议和社会推理等领域，常常未能挑战用户的有害信念，这暴露了模型在认知警觉性上的不足。

Method: 研究人类影响顺应行为的社交和语言因素，如话题相关性、语言编码和信息来源可靠性，分析其对LLMs顺应行为的影响，并通过引入简单语用提示提升模型挑战有害信念的能力。

Result: 通过研究语言社交因素如何影响LLMs的应对行为，发现这些因素解释了模型在三项安全基准测试中对有害信念挑战能力的差异；此外，简单的语用干预（如加入"wait a minute"）能显著提升模型表现。

Conclusion: 考虑语用因素对评估和提升大语言模型的行为和安全性至关重要。

Abstract: Large language models (LLMs) frequently fail to challenge users' harmful beliefs in domains ranging from medical advice to social reasoning. We argue that these failures can be understood and addressed pragmatically as consequences of LLMs defaulting to accommodating users' assumptions and exhibiting insufficient epistemic vigilance. We show that social and linguistic factors known to influence accommodation in humans (at-issueness, linguistic encoding, and source reliability) similarly affect accommodation in LLMs, explaining performance differences across three safety benchmarks that test models' ability to challenge harmful beliefs, spanning misinformation (Cancer-Myth, SAGE-Eval) and sycophancy (ELEPHANT). We further show that simple pragmatic interventions, such as adding the phrase "wait a minute", significantly improve performance on these benchmarks while preserving low false-positive rates. Our results highlight the importance of considering pragmatics for evaluating LLM behavior and improving LLM safety.

</details>


### [26] [Learning to Simulate Human Dialogue](https://arxiv.org/abs/2601.04436)
*Kanishk Gandhi,Agam Bhatia,Noah D. Goodman*

Main category: cs.CL

TL;DR: 通过对话下一句预测，发现直接最大化真实人类回复概率优于基于判分的奖励方法，且结合潜在变量“思考”过程的优化效果最佳。


<details>
  <summary>Details</summary>
Motivation: 为了模拟人类思维过程，更准确地预测对话中下一句，提高对话系统对真实人类语言生成的理解和匹配能力。

Method: 比较了一种基于LLM判分奖励与直接最大化真实人类回复对数概率的学习方法；并通过视链式思考过程为潜在变量，推导对数概率下界进行优化。

Result: 该论文研究了通过下一轮对话预测来模拟人类思维过程，比较了不同学习方法对预测下一句对话的效果影响。研究发现，使用大型语言模型（LLM）作为评判者根据语义相似度和信息完整性奖励模型虽然能提升判分，但会降低模型对真实人类回复的概率分布拟合及人类评审的胜率；而直接最大化真实人类回复的对数概率的方式则能更好地预测人类实际说的话，两者表现优劣分明。此外，将思考过程作为潜在变量来优化对数概率下界的方法取得了最优效果。该研究提示，启用“思考”环节更有效的前提是采用基于真实人类对话分布的匹配目标，并有望通过扩展到更广泛的数据，实现对人类行为的更细致理解。

Conclusion: 启用思考环节需基于真实对话分布进行训练，直接最大化人类回复概率能提升预测准确性，基于潜变量的优化策略可进一步改进模型理解人类行为能力。

Abstract: To predict what someone will say is to model how they think. We study this through next-turn dialogue prediction: given a conversation, predict the next utterance produced by a person. We compare learning approaches along two dimensions: (1) whether the model is allowed to think before responding, and (2) how learning is rewarded either through an LLM-as-a-judge that scores semantic similarity and information completeness relative to the ground-truth response, or by directly maximizing the log-probability of the true human dialogue. We find that optimizing for judge-based rewards indeed increases judge scores throughout training, however it decreases the likelihood assigned to ground truth human responses and decreases the win rate when human judges choose the most human-like response among a real and synthetic option. This failure is amplified when the model is allowed to think before answering. In contrast, by directly maximizing the log-probability of observed human responses, the model learns to better predict what people actually say, improving on both log-probability and win rate evaluations. Treating chain-of-thought as a latent variable, we derive a lower bound on the log-probability. Optimizing this objective yields the best results on all our evaluations. These results suggest that thinking helps primarily when trained with a distribution-matching objective grounded in real human dialogue, and that scaling this approach to broader conversational data may produce models with a more nuanced understanding of human behavior.

</details>


### [27] [Merging Triggers, Breaking Backdoors: Defensive Poisoning for Instruction-Tuned Language Models](https://arxiv.org/abs/2601.04448)
*San Kim,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: 本文提出MB-Defense训练框架，有效提升指令调优大型语言模型抵御多样后门攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型依赖大规模数据收集，容易受到后门攻击，且针对指令调优模型的防御策略尚未充分研究。

Method: 提出MB-Defense（一种融合与破坏防御框架）训练流程，包括防御性投毒和权重恢复两个阶段，旨在通过合并攻击者和防御触发器的方式建立统一反向门表示，并通过额外训练破坏该表示以恢复模型的正常行为。

Result: MB-Defense显著降低了攻击成功率，同时保持模型的指令遵循能力，在多个LLM上的实验验证了其有效性和数据效率。

Conclusion: MB-Defense提供了一种通用且高效的防御策略，增强指令调优模型面对未知后门攻击的安全性。

Abstract: Large Language Models (LLMs) have greatly advanced Natural Language Processing (NLP), particularly through instruction tuning, which enables broad task generalization without additional fine-tuning. However, their reliance on large-scale datasets-often collected from human or web sources-makes them vulnerable to backdoor attacks, where adversaries poison a small subset of data to implant hidden behaviors. Despite this growing risk, defenses for instruction-tuned models remain underexplored. We propose MB-Defense (Merging & Breaking Defense Framework), a novel training pipeline that immunizes instruction-tuned LLMs against diverse backdoor threats. MB-Defense comprises two stages: (i) defensive poisoning, which merges attacker and defensive triggers into a unified backdoor representation, and (ii) weight recovery, which breaks this representation through additional training to restore clean behavior. Extensive experiments across multiple LLMs show that MB-Defense substantially lowers attack success rates while preserving instruction-following ability. Our method offers a generalizable and data-efficient defense strategy, improving the robustness of instruction-tuned LLMs against unseen backdoor attacks.

</details>


### [28] [Users Mispredict Their Own Preferences for AI Writing Assistance](https://arxiv.org/abs/2601.04461)
*Vivian Lai,Zana Buçinca,Nil-Jana Akpinar,Mo Houtti,Hyeonsu B. Kang,Kevin Chian,Namjoon Suh,Alex C. Williams*

Main category: cs.CL

TL;DR: 用户在主动写作助手中高估了紧迫性的重要性，实际行为以构成性努力为主，依靠用户自述偏好设计系统效果较差，行为数据设计更优。


<details>
  <summary>Details</summary>
Motivation: 探讨用户在主动式AI写作助手中对起草帮助需求的驱动因素，因为目前对用户偏好的经验性理解不足。

Method: 通过一项包含50名参与者的因子图研究，进行750次成对比较，分析用户的行为决策与自我报告偏好。

Result: 发现构成性努力是决策的主导因素（相关系数0.597），而紧迫性几乎没有预测力（相关系数约0），用户自我报告中紧迫性重要性最高，但行为数据中却是最弱的驱动因素，表现出明显的感知与行为差距。基于用户陈述偏好的设计系统准确率仅57.7%，低于使用行为模式设计的系统的61.3%。

Conclusion: 用户自我反省的偏好容易误导系统优化，表明设计主动NLG系统时应更依赖行为数据而非用户自述偏好。

Abstract: Proactive AI writing assistants need to predict when users want drafting help, yet we lack empirical understanding of what drives preferences. Through a factorial vignette study with 50 participants making 750 pairwise comparisons, we find compositional effort dominates decisions ($ρ= 0.597$) while urgency shows no predictive power ($ρ\approx 0$). More critically, users exhibit a striking perception-behavior gap: they rank urgency first in self-reports despite it being the weakest behavioral driver, representing a complete preference inversion. This misalignment has measurable consequences. Systems designed from users' stated preferences achieve only 57.7\% accuracy, underperforming even naive baselines, while systems using behavioral patterns reach significantly higher 61.3\% ($p < 0.05$). These findings demonstrate that relying on user introspection for system design actively misleads optimization, with direct implications for proactive natural language generation (NLG) systems.

</details>


### [29] [Beyond Static Summarization: Proactive Memory Extraction for LLM Agents](https://arxiv.org/abs/2601.04463)
*Chengyuan Yang,Zequn Sun,Wei Wei,Wei Hu*

Main category: cs.CL

TL;DR: 本文提出了一种名为ProMem的主动记忆提取方法，通过引入反馈循环，利用自我提问机制迭代提取和校正对话历史信息，解决传统总结方法在前瞻性和反馈机制上的不足，从而提升记忆完整性和问答准确率。


<details>
  <summary>Details</summary>
Motivation: 传统基于摘要的记忆管理方法存在先期总结导致遗漏重要细节和缺乏事实验证的反馈环节，致使信息逐渐丢失，难以支持长期交互和个性化需求。

Method: 设计了一种主动记忆提取机制，建立了自我提问的反馈循环，允许模型不断回顾和修正对话历史，以克服传统摘要的前瞻性盲区和单次提取的缺陷。

Result: ProMem显著提升了提取记忆的完整性和问答准确性，并在提取质量与令牌成本之间取得了理想的平衡。

Conclusion: ProMem通过迭代认知过程和反馈循环，有效补充和修正记忆信息，显著提升了记忆提取的完整性和问答系统的性能，同时实现了质量与成本的良好平衡。

Abstract: Memory management is vital for LLM agents to handle long-term interaction and personalization. Most research focuses on how to organize and use memory summary, but often overlooks the initial memory extraction stage. In this paper, we argue that existing summary-based methods have two major limitations based on the recurrent processing theory. First, summarization is "ahead-of-time", acting as a blind "feed-forward" process that misses important details because it doesn't know future tasks. Second, extraction is usually "one-off", lacking a feedback loop to verify facts, which leads to the accumulation of information loss. To address these issues, we propose proactive memory extraction (namely ProMem). Unlike static summarization, ProMem treats extraction as an iterative cognitive process. We introduce a recurrent feedback loop where the agent uses self-questioning to actively probe the dialogue history. This mechanism allows the agent to recover missing information and correct errors. Our ProMem significantly improves the completeness of the extracted memory and QA accuracy. It also achieves a superior trade-off between extraction quality and token cost.

</details>


### [30] [Concept Tokens: Learning Behavioral Embeddings Through Concept Definitions](https://arxiv.org/abs/2601.04465)
*Ignacio Sastre,Aiala Rosá*

Main category: cs.CL

TL;DR: 本文提出了Concept Tokens，一种通过向预训练大语言模型中添加特殊标记并只学习该标记的嵌入，从多条自然语言定义中学习目标概念的方法。该方法在冻结的大模型基础上仅优化嵌入，能有效引导模型行为。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在闭卷问答和教学反馈等任务中容易产生幻觉或表现无法满足特定指令需求，因而需要一种方法在不改变模型参数的情况下，利用概念定义精细控制模型行为。

Method: 向预训练的大语言模型中添加一个新的特殊标记，替换文本中所有目标概念出现的位置，仅优化该标记的嵌入向量，采用标准语言建模目标进行训练，同时保持模型其余部分冻结。

Result: 通过在HotpotQA闭卷问答和第二语言教学反馈中验证，Concept Tokens能有效减少幻觉现象，改善模型的遵从性和教学效果。定性研究展示了其在捕获概念信息和局限性方面的表现。

Conclusion: Concept Tokens能够为冻结的大语言模型提供一种紧凑且可控的信号，通过学习概念的定义，实现对模型回答和行为的精准控制，具有较好的实用性和解释性。

Abstract: We propose Concept Tokens, a lightweight method that adds a new special token to a pretrained LLM and learns only its embedding from multiple natural language definitions of a target concept, where occurrences of the concept are replaced by the new token. The LLM is kept frozen and the embedding is optimized with the standard language-modeling objective. We evaluate Concept Tokens in three settings. First, we study hallucinations in closed-book question answering on HotpotQA and find a directional effect: negating the hallucination token reduces hallucinated answers mainly by increasing abstentions, whereas asserting it increases hallucinations and lowers precision. Second, we induce recasting, a pedagogical feedback strategy for second language teaching, and observe the same directional effect. Moreover, compared to providing the full definitional corpus in-context, concept tokens better preserve compliance with other instructions (e.g., asking follow-up questions). Finally, we include a qualitative study with the Eiffel Tower and a fictional "Austral Tower" to illustrate what information the learned embeddings capture and where their limitations emerge. Overall, Concept Tokens provide a compact control signal learned from definitions that can steer behavior in frozen LLMs.

</details>


### [31] [SampoNLP: A Self-Referential Toolkit for Morphological Analysis of Subword Tokenizers](https://arxiv.org/abs/2601.04469)
*Iaroslav Chelombitko,Ekaterina Chelombitko,Aleksey Komissarov*

Main category: cs.CL

TL;DR: 提出无需语料的SampoNLP工具创建乌拉尔语形态词典，基于此评估BPE分词器并提出新指标IPS，给出最优词汇规模建议。


<details>
  <summary>Details</summary>
Motivation: 缺乏干净的形态词典严重制约了对形态丰富乌拉尔语言的子词分词器评估和优化，尤其是在低资源环境下。提出SampoNLP旨在填补这一空白，支持高质量词典生成和分词器参数选择。

Method: 基于MDL启发式的自指原子性评分方法，利用内部结构线索过滤复合词形，生成高纯度形态词典，再用该词典评估不同规模的BPE分词器性能，通过IPS综合评分平衡词素覆盖与过分割。

Result: 本文介绍了SampoNLP工具包，该工具无需语料即可创建形态词典，解决了乌拉尔语族形态丰富语言缺乏干净词素词典的问题。利用SampoNLP生成的高纯度芬兰语、匈牙利语和爱沙尼亚语词素词典，系统评估了不同词汇规模下的BPE分词器表现，并提出综合性能评分（IPS）衡量词素覆盖与过分割的权衡，基于IPS曲线确定了词汇规模的“肘部点”，为这些语言的词汇规模选择提供了实证建议，揭示了标准BPE在高度黏着语上的局限性。论文所述工具及资源均已开源。

Conclusion: SampoNLP有助于低资源乌拉尔语形态词典构建，IPS指标有效指导BPE词汇规模选择，标准BPE对高度黏着语有局限。

Abstract: The quality of subword tokenization is critical for Large Language Models, yet evaluating tokenizers for morphologically rich Uralic languages is hampered by the lack of clean morpheme lexicons.
  We introduce SampoNLP, a corpus-free toolkit for morphological lexicon creation using MDL-inspired Self-Referential Atomicity Scoring, which filters composite forms through internal structural cues - suited for low-resource settings.
  Using the high-purity lexicons generated by SampoNLP for Finnish, Hungarian, and Estonian, we conduct a systematic evaluation of BPE tokenizers across a range of vocabulary sizes (8k-256k). We propose a unified metric, the Integrated Performance Score (IPS), to navigate the trade-off between morpheme coverage and over-splitting. By analyzing the IPS curves, we identify the "elbow points" of diminishing returns and provide the first empirically grounded recommendations for optimal vocabulary sizes (k) in these languages. Our study not only offers practical guidance but also quantitatively demonstrates the limitations of standard BPE for highly agglutinative languages. The SampoNLP library and all generated resources are made publicly available: https://github.com/AragonerUA/SampoNLP

</details>


### [32] [WESR: Scaling and Evaluating Word-level Event-Speech Recognition](https://arxiv.org/abs/2601.04508)
*Chenchen Yang,Kexin Huang,Liwei Fan,Qian Tu,Botian Jiang,Dong Zhang,Linqi Yin,Shimin Li,Zhaoye Fei,Qinyuan Cheng,Xipeng Qiu*

Main category: cs.CL

TL;DR: 本文提出了针对丰富非语言声音事件的精确定位方法，建立了包含21类声音事件的细化分类体系，并开发了带位置信息的评测基准WESR-Bench，提升了事件检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 语音传递了丰富的非语言声音信息，如笑声、哭声等，但现有方法对这些事件的分类不充分，定位不精准，且缺乏统一评测标准，限制了应用发展，亟需改进。

Method: 本文设计了包含21种声音事件的新分类体系，开发了带位置信息的评测集WESR-Bench，使用1700小时以上语料训练专门模型，并采用位置感知协议分离ASR错误与事件检测，提升定位精度。

Result: 构建了900+句子的专家注释评测集和1700+小时训练语料，训练模型在定位非语言声音事件上优于开源模型和商业API，同时保持语音识别质量，验证了方法有效性。

Conclusion: 通过细化声音事件分类和构建大规模数据集与评测框架，本文成功实现了对离散及连续非语言声音事件的精准定位，显著优于现有音频语言模型和商业API，促进了真实世界语音场景的深入研究。

Abstract: Speech conveys not only linguistic information but also rich non-verbal vocal events such as laughing and crying. While semantic transcription is well-studied, the precise localization of non-verbal events remains a critical yet under-explored challenge. Current methods suffer from insufficient task definitions with limited category coverage and ambiguous temporal granularity. They also lack standardized evaluation frameworks, hindering the development of downstream applications. To bridge this gap, we first develop a refined taxonomy of 21 vocal events, with a new categorization into discrete (standalone) versus continuous (mixed with speech) types. Based on the refined taxonomy, we introduce WESR-Bench, an expert-annotated evaluation set (900+ utterances) with a novel position-aware protocol that disentangles ASR errors from event detection, enabling precise localization measurement for both discrete and continuous events. We also build a strong baseline by constructing a 1,700+ hour corpus, and train specialized models, surpassing both open-source audio-language models and commercial APIs while preserving ASR quality. We anticipate that WESR will serve as a foundational resource for future research in modeling rich, real-world auditory scenes.

</details>


### [33] [LinguaGame: A Linguistically Grounded Game-Theoretic Paradigm for Multi-Agent Dialogue Generation](https://arxiv.org/abs/2601.04516)
*Yuxiao Ye,Yiming Zhang,Yiran Ma,Huiyuan Xie,Huining Zhu,Zhiyuan Liu*

Main category: cs.CL

TL;DR: 本文提出基于语言学和博弈论的LinguaGame框架，提升多智能体系统中对话交流效率，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统多采用基于大语言模型的架构设计，注重角色分配和工作流编排，较少关注交流过程本身的效率提升。

Method: 提出LinguaGame，一种基于语言学的博弈论范式，将对话建模为信号游戏，利用无训练平衡近似算法动态调整推理时的决策，实现意图和策略的推断。

Result: 在模拟法庭和辩论场景中，经人类专家评估，所提出方法明显提升了多智能体之间的沟通效率。

Conclusion: LinguaGame框架通过语言学指导的推理方式，实现多智能体对话的高效沟通，且对任务耦合度低，适应性强。

Abstract: Large Language Models (LLMs) have enabled Multi-Agent Systems (MASs) where agents interact through natural language to solve complex tasks or simulate multi-party dialogues. Recent work on LLM-based MASs has mainly focused on architecture design, such as role assignment and workflow orchestration. In contrast, this paper targets the interaction process itself, aiming to improve agents' communication efficiency by helping them convey their intended meaning more effectively through language. To this end, we propose LinguaGame, a linguistically-grounded game-theoretic paradigm for multi-agent dialogue generation. Our approach models dialogue as a signalling game over communicative intents and strategies, solved with a training-free equilibrium approximation algorithm for inference-time decision adjustment. Unlike prior game-theoretic MASs, whose game designs are often tightly coupled with task-specific objectives, our framework relies on linguistically informed reasoning with minimal task-specific coupling. Specifically, it treats dialogue as intentional and strategic communication, requiring agents to infer what others aim to achieve (intents) and how they pursue those goals (strategies). We evaluate our framework in simulated courtroom proceedings and debates, with human expert assessments showing significant gains in communication efficiency.

</details>


### [34] [GRACE: Reinforcement Learning for Grounded Response and Abstention under Contextual Evidence](https://arxiv.org/abs/2601.04525)
*Yibo Zhao,Jiapeng Zhu,Zichen Ding,Xiang Li*

Main category: cs.CL

TL;DR: 本文提出了强化学习框架GRACE，统一解决检索增强生成系统中证据支持不足和虚假回答问题，实现高准确率和低标注成本。


<details>
  <summary>Details</summary>
Motivation: 现有的 Retrieval-Augmented Generation (RAG) 系统存在两个关键缺陷：在没有明确证据支持时给出正确答案和在检索上下文不足时产生虚假回复。此前的研究分别解决了这两个问题，但缺乏一个统一框架同时处理证据基准和可靠回避的问题。

Method: 提出了 GRACE 框架，采用强化学习，通过多阶段门控奖励函数训练模型评估证据充分性、提取关键支持证据，并根据情况给出答案或明确回避。数据构建使用异构检索器生成多样训练样本，无需人工标注。

Result: GRACE 在两个基准测试中实现了最先进的整体准确率，在准确回答与拒绝之间达成良好平衡，同时注释成本仅为先前方法的10%。

Conclusion: GRACE 有效解决了RAG系统中证据不足带来的虚假回答和在无充分证据时回避响应的问题，提升了生成模型的可靠性和实用性。

Abstract: Retrieval-Augmented Generation (RAG) integrates external knowledge to enhance Large Language Models (LLMs), yet systems remain susceptible to two critical flaws: providing correct answers without explicit grounded evidence and producing fabricated responses when the retrieved context is insufficient. While prior research has addressed these issues independently, a unified framework that integrates evidence-based grounding and reliable abstention is currently lacking. In this paper, we propose GRACE, a reinforcement-learning framework that simultaneously mitigates both types of flaws. GRACE employs a data construction method that utilizes heterogeneous retrievers to generate diverse training samples without manual annotation. A multi-stage gated reward function is then employed to train the model to assess evidence sufficiency, extract key supporting evidence, and provide answers or explicitly abstain. Experimental results on two benchmarks demonstrate that GRACE achieves state-of-the-art overall accuracy and strikes a favorable balance between accurate response and rejection, while requiring only 10% of the annotation costs of prior methods. Our code is available at https://github.com/YiboZhao624/Grace..

</details>


### [35] [BanglaLorica: Design and Evaluation of a Robust Watermarking Algorithm for Large Language Models in Bangla Text Generation](https://arxiv.org/abs/2601.04534)
*Amit Bin Tariqul,A N M Zahid Hossain Milkan,Sahab-Al-Chowdhury,Syed Rifat Raiyan,Hasan Mahmud,Md Kamrul Hasan*

Main category: cs.CL

TL;DR: 本文评估了三种文本水印方法在孟加拉语模型中的鲁棒性，提出分层水印策略大幅提升跨语言翻译攻击后的检测准确率，兼顾了水印的实用性和文本质量。


<details>
  <summary>Details</summary>
Motivation: 现有文本水印方法在高资源语言中表现良好，但在低资源语言（如孟加拉语）面对跨语言往返翻译攻击时鲁棒性差，亟需一种无训练、实用且具备较强攻击抵抗力的水印方法。

Method: 结合嵌入时水印技术和生成后水印技术，设计分层水印策略，通过实验验证其在RTT攻击下的提升效果。

Result: 本文系统评估了现有三种主流文本水印方法（KGW、EXP和Waterfall）在孟加拉语生成的大型语言模型中对抗跨语言往返翻译攻击（RTT）的性能。结果表明，传统方法在无攻击时检测准确率较高（>88%），但经过RTT攻击后准确率急剧下降至9-13%。针对这一问题，本文提出了一种结合嵌入时水印和生成后水印的分层水印策略，显著提升了RTT攻击后的检测准确率至40-50%，实现了3至4倍的相对提升，但带来了语义质量的可控下降。实验量化了多语言水印的鲁棒性与质量之间的权衡，验证了分层水印作为低资源语言实用且无需训练的解决方案的有效性。

Conclusion: 分层水印策略能有效提升低资源语言文本水印在跨语言往返翻译攻击下的检测准确率，成为无训练需求的实用方案，但需权衡语义质量的下降。

Abstract: As large language models (LLMs) are increasingly deployed for text generation, watermarking has become essential for authorship attribution, intellectual property protection, and misuse detection. While existing watermarking methods perform well in high-resource languages, their robustness in low-resource languages remains underexplored. This work presents the first systematic evaluation of state-of-the-art text watermarking methods: KGW, Exponential Sampling (EXP), and Waterfall, for Bangla LLM text generation under cross-lingual round-trip translation (RTT) attacks. Under benign conditions, KGW and EXP achieve high detection accuracy (>88%) with negligible perplexity and ROUGE degradation. However, RTT causes detection accuracy to collapse below RTT causes detection accuracy to collapse to 9-13%, indicating a fundamental failure of token-level watermarking. To address this, we propose a layered watermarking strategy that combines embedding-time and post-generation watermarks. Experimental results show that layered watermarking improves post-RTT detection accuracy by 25-35%, achieving 40-50% accuracy, representing a 3$\times$ to 4$\times$ relative improvement over single-layer methods, at the cost of controlled semantic degradation. Our findings quantify the robustness-quality trade-off in multilingual watermarking and establish layered watermarking as a practical, training-free solution for low-resource languages such as Bangla. Our code and data will be made public.

</details>


### [36] [Identifying Good and Bad Neurons for Task-Level Controllable LLMs](https://arxiv.org/abs/2601.04548)
*Wenjie Li,Guansong Pang,Hezhe Qiao,Debin Gao,David Lo*

Main category: cs.CL

TL;DR: 提出NeuronLLM，利用功能拮抗原则通过区分促进和抑制任务完成的神经元，实现对大型语言模型任务相关神经元的整体理解，提升了对模型功能的解释能力。


<details>
  <summary>Details</summary>
Motivation: 现有神经元识别方法仅针对特定能力，难以处理需要多能力协调的任务，且忽视阻碍任务完成的神经元，且由于模型偶然正确答案导致的神经元归因错误问题。因此需要一种综合识别促进和抑制神经元、减少偶然行为影响的任务级理解框架。

Method: 引入功能拮抗的生物学原则，设计对比学习框架区分“好神经元”和“坏神经元”，利用增强问题集抑制偶然行为带来的误判，完成神经元识别和任务性能分析。

Result: 本文提出NeuronLLM框架，通过功能拮抗原则识别大型语言模型(LLM)中对任务完成有促进作用的"好神经元"和抑制作用的"坏神经元"，实现对神经元的整体建模。采用对比学习区分好坏神经元，并通过增强问题集减少模型偶然正确行为的影响。实验证明NeuronLLM在多种LLM和四个NLP任务中表现优于现有方法，并提供了LLM功能组织的新见解。

Conclusion: NeuronLLM通过识别促进与抑制任务完成的神经元，实现了任务级别对大型语言模型的深入理解，优于现有基于单一能力的神经元识别方法，促进了对LLM功能机制的洞察。

Abstract: Large Language Models have demonstrated remarkable capabilities on multiple-choice question answering benchmarks, but the complex mechanisms underlying their large-scale neurons remain opaque, posing significant challenges for understanding and steering LLMs. While recent studies made progress on identifying responsible neurons for certain abilities, these ability-specific methods are infeasible for task-focused scenarios requiring coordinated use of multiple abilities. Moreover, these approaches focus only on supportive neurons that correlate positively with task completion, while neglecting neurons with other roles-such as inhibitive roles-and misled neuron attribution due to fortuitous behaviors in LLMs (i.e., correctly answer the questions by chance rather than genuine understanding). To address these challenges, we propose NeuronLLM, a novel task-level LLM understanding framework that adopts the biological principle of functional antagonism for LLM neuron identification. The key insight is that task performance is jointly determined by neurons with two opposing roles: good neurons that facilitate task completion and bad neurons that inhibit it. NeuronLLM achieves a holistic modeling of neurons via contrastive learning of good and bad neurons, while leveraging augmented question sets to mitigate the fortuitous behaviors in LLMs. Comprehensive experiments on LLMs of different sizes and families show the superiority of NeuronLLM over existing methods in four NLP tasks, providing new insights into LLM functional organization.

</details>


### [37] [FeedEval: Pedagogically Aligned Evaluation of LLM-Generated Essay Feedback](https://arxiv.org/abs/2601.04574)
*Seongyeub Chu,Jongwoo Kim,Munyong Yi*

Main category: cs.CL

TL;DR: 提出FeedEval框架评估LLM生成的作文反馈质量，从而提升评分模型性能和作文修改效果。


<details>
  <summary>Details</summary>
Motivation: 传统LLM生成的反馈未经质量验证导致噪声传播，亟需一个有效框架评估反馈质量以提升作文评分和反馈实用性。

Method: 基于教育学维度设计评估指标，训练专门的LLM评估器，评价多个反馈候选并筛选高质量反馈用于下游任务。

Result: 本文提出了FeedEval，一个基于大语言模型（LLM）的框架，用于评估LLM生成的作文反馈的质量，具体从反馈的具体性、帮助性和有效性三个教育学维度进行评价。FeedEval使用专门训练的评估模型筛选高质量反馈，以改进下游作文评分模型的训练效果。实验验证表明，FeedEval的评估结果与人类专家判断高度一致，且基于FeedEval筛选的反馈训练的作文评分模型表现更优，同时小型LLM使用这些高质量反馈进行作文修改效果更佳。

Conclusion: FeedEval能够有效筛选高质量的作文反馈，显著提升作文评分模型的准确性和作文修订的有效性。

Abstract: Going beyond the prediction of numerical scores, recent research in automated essay scoring has increasingly emphasized the generation of high-quality feedback that provides justification and actionable guidance. To mitigate the high cost of expert annotation, prior work has commonly relied on LLM-generated feedback to train essay assessment models. However, such feedback is often incorporated without explicit quality validation, resulting in the propagation of noise in downstream applications. To address this limitation, we propose FeedEval, an LLM-based framework for evaluating LLM-generated essay feedback along three pedagogically grounded dimensions: specificity, helpfulness, and validity. FeedEval employs dimension-specialized LLM evaluators trained on datasets curated in this study to assess multiple feedback candidates and select high-quality feedback for downstream use. Experiments on the ASAP++ benchmark show that FeedEval closely aligns with human expert judgments and that essay scoring models trained with FeedEval-filtered high-quality feedback achieve superior scoring performance. Furthermore, revision experiments using small LLMs show that the high-quality feedback identified by FeedEval leads to more effective essay revisions. We will release our code and curated datasets upon accepted.

</details>


### [38] [Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization](https://arxiv.org/abs/2601.04582)
*Mizanur Rahman,Mohammed Saidul Islam,Md Tahmid Rahman Laskar,Shafiq Joty,Enamul Hoque*

Main category: cs.CL

TL;DR: 本文提出基于强化学习的Text2Vis生成方法，显著提升代码有效性和图表质量，实现多目标优化，并在多个数据集上展现优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有的Text2Vis系统在自然语言查询到可执行和语义对齐的可视化图表转换中存在质量欠缺，尤其是开放源代码模型难以生成有效代码或高质量图表。传统的监督微调方法无法通过后执行反馈提升可视化质量。

Method: 提出RL-Text2Vis强化学习框架，基于Group Relative Policy Optimization（GRPO），利用多目标奖励函数联合优化文本准确性、代码有效性和可视化质量，基于后执行反馈进行训练。

Result: 在Text2Vis基准测试中，RL-Text2Vis在图表质量上相较GPT-4o提升22%，代码执行成功率从78%提升到97%。同时在VIS-Eval和NVBench等数据集上展示出强泛化能力。

Conclusion: GRPO强化学习策略有效提升了Text2Vis任务中代码和可视化质量，优于多种基线方法，推动了结构化多模态推理在可视化生成中的应用。

Abstract: Text-to-Visualization (Text2Vis) systems translate natural language queries over tabular data into concise answers and executable visualizations. While closed-source LLMs generate functional code, the resulting charts often lack semantic alignment and clarity, qualities that can only be assessed post-execution. Open-source models struggle even more, frequently producing non-executable or visually poor outputs. Although supervised fine-tuning can improve code executability, it fails to enhance overall visualization quality, as traditional SFT loss cannot capture post-execution feedback. To address this gap, we propose RL-Text2Vis, the first reinforcement learning framework for Text2Vis generation. Built on Group Relative Policy Optimization (GRPO), our method uses a novel multi-objective reward that jointly optimizes textual accuracy, code validity, and visualization quality using post-execution feedback. By training Qwen2.5 models (7B and 14B), RL-Text2Vis achieves a 22% relative improvement in chart quality over GPT-4o on the Text2Vis benchmark and boosts code execution success from 78% to 97% relative to its zero-shot baseline. Our models significantly outperform strong zero-shot and supervised baselines and also demonstrate robust generalization to out-of-domain datasets like VIS-Eval and NVBench. These results establish GRPO as an effective strategy for structured, multimodal reasoning in visualization generation. We release our code at https://github.com/vis-nlp/RL-Text2Vis.

</details>


### [39] [THaLLE-ThaiLLM: Domain-Specialized Small LLMs for Finance and Thai -- Technical Report](https://arxiv.org/abs/2601.04597)
*KBTG Labs,:,Anuruth Lertpiya,Danupat Khamnuansin,Kantapong Sucharitpongpan,Pornchanan Balee,Tawunrat Chalothorn,Thadpong Pongthawornkamol,Monchai Lertsutthiwong*

Main category: cs.CL

TL;DR: 本文探讨了通过模型合并技术，将多个大型语言模型（LLMs）融合，以实现高效的多能力模型，特别应用于泰语和金融领域。


<details>
  <summary>Details</summary>
Motivation: 为解决企业因隐私和成本限制，难以训练单一多能力模型的问题，探索模型合并作为低成本高效的多能力模型构建方案。

Method: 通过将Qwen-8B与ThaiLLM-8B及THaLLE-CFA-8B模型进行合并，评估其在多个语言和金融领域基准测试中的表现。

Result: 合并后的模型在M3、M6 O-NET考试，Flare-CFA和Thai-IC基准测试中表现显著优于单一模型，验证了模型合并的有效性。

Conclusion: 模型合并是一种资源高效的方法，能够提升多语言模型的性能，兼顾通用和专业领域能力。

Abstract: Large Language Models (LLMs) have demonstrated significant potential across various domains, particularly in banking and finance, where they can automate complex tasks and enhance decision-making at scale. Due to privacy, security, and regulatory concerns, organizations often prefer on-premise deployment of LLMs. The ThaiLLM initiative aims to enhance Thai language capabilities in open-LLMs, enabling Thai industry to leverage advanced language models. However, organizations often face a trade-off between deploying multiple specialized models versus the prohibitive expense of training a single multi-capability model. To address this, we explore model merging as a resource-efficient alternative for developing high-performance, multi-capability LLMs. We present results from two key experiments: first, merging Qwen-8B with ThaiLLM-8B demonstrates how ThaiLLM-8B enhances Thai general capabilities, showing an uplift of M3 and M6 O-NET exams over the general instruction-following Qwen-8B. Second, we merge Qwen-8B with both ThaiLLM-8B and THaLLE-CFA-8B. This combination results in further improvements in performance across both general and financial domains, by demonstrating an uplift in both M3 and M6 O-NET, Flare-CFA, and Thai-IC benchmarks. The report showcases the viability of model merging for efficiently creating multi-capability LLMs.

</details>


### [40] [On the Limitations of Rank-One Model Editing in Answering Multi-hop Questions](https://arxiv.org/abs/2601.04600)
*Zhiyuan He,Binghan Chen,Tianxiang Xiong,Ziyang Sun,Mozhao Zhu,Xi Chen*

Main category: cs.CL

TL;DR: 针对多跳推理中知识编辑的关键失败问题，本文提出重复编辑策略Redundant Editing，大幅提升推理准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的知识编辑方法在单跳事实更新上表现优异，但在多跳推理的知识链条构建上效果有限，存在多种失败模式需解决。

Method: 提出了一种名为Redundant Editing的重复编辑策略，通过在不同网络层进行多次知识编辑，解决了晚层缺乏中间表征和泛化能力下降的问题。

Result: Redundant Editing策略在2跳问题上准确率提升至少15.5个百分点，是先前单次编辑策略的96%增幅，但牺牲了部分特异性和语言自然性。

Conclusion: 基于ROME的知识编辑方法在多跳推理中存在重要限制，如"hopping-too-late"问题、泛化能力下降和过拟合多个跳跃答案。通过重复编辑策略，这些问题得到有效缓解，从而显著提升了多跳推理的准确率。

Abstract: Recent advances in Knowledge Editing (KE), particularly Rank-One Model Editing (ROME), show superior efficiency over fine-tuning and in-context learning for updating single-hop facts in transformers. However, these methods face significant challenges when applied to multi-hop reasoning tasks requiring knowledge chaining. In this work, we study the effect of editing knowledge with ROME on different layer depths and identify three key failure modes. First, the "hopping-too-late" problem occurs as later layers lack access to necessary intermediate representations. Second, generalization ability deteriorates sharply when editing later layers. Third, the model overfits to edited knowledge, incorrectly prioritizing edited-hop answers regardless of context. To mitigate the issues of "hopping-too-late" and generalisation decay, we propose Redundant Editing, a simple yet effective strategy that enhances multi-hop reasoning. Our experiments demonstrate that this approach can improve accuracy on 2-hop questions by at least 15.5 percentage points, representing a 96% increase over the previous single-edit strategy, while trading off some specificity and language naturalness.

</details>


### [41] [When More Words Say Less: Decoupling Length and Specificity in Image Description Evaluation](https://arxiv.org/abs/2601.04609)
*Rhea Kapur,Robert Hawkins,Elisa Kreiss*

Main category: cs.CL

TL;DR: 本文通过控制长度构建数据集，证明视觉描述的特异性与长度不同，人们更喜欢信息含量高的描述，提出应优先评价特异性而非冗长。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型中描述的特异性常常与长度混淆，亟需明确两者的区别以提高描述质量。

Method: 通过构建一个控制长度但变化信息含量的数据集，并进行人类偏好验证来区分描述的长度和特异性。

Result: 研究发现长度控制不足以解释特异性的差异，关键在于如何分配描述的长度预算，且人们更倾向于选择特异性更高的描述。

Conclusion: 描述的特异性与描述的长度是不同的概念，描述的特异性更能体现信息量的丰富程度。

Abstract: Vision-language models (VLMs) are increasingly used to make visual content accessible via text-based descriptions. In current systems, however, description specificity is often conflated with their length. We argue that these two concepts must be disentangled: descriptions can be concise yet dense with information, or lengthy yet vacuous. We define specificity relative to a contrast set, where a description is more specific to the extent that it picks out the target image better than other possible images. We construct a dataset that controls for length while varying information content, and validate that people reliably prefer more specific descriptions regardless of length. We find that controlling for length alone cannot account for differences in specificity: how the length budget is allocated makes a difference. These results support evaluation approaches that directly prioritize specificity over verbosity.

</details>


### [42] [Character-R1: Enhancing Role-Aware Reasoning in Role-Playing Agents via RLVR](https://arxiv.org/abs/2601.04611)
*Yihong Tang,Kehai Chen,Xuefeng Bai,Benyou Wang,Zeming Liu,Haifeng Wang,Min Zhang*

Main category: cs.CL

TL;DR: 提出了一种新型角色扮演代理框架Character-R1，通过多重奖励机制加强角色认知一致性和表现，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前角色扮演代理多通过模仿表层行为构建，缺乏内部认知一致性，导致复杂情境下出现角色失真错误，亟需设计能够提供内部认知反馈的奖励机制以提升角色内在逻辑合理性。

Method: Character-R1框架由三部分组成：1) 认知关注奖励，基于10个角色元素的标签分析构建内部认知；2) 参考引导奖励，利用与参考响应的重叠度作为优化目标；3) 角色条件奖励归一化，根据不同角色类别调整奖励分布确保优化稳健。

Result: 该论文提出了一种名为Character-R1的角色扮演代理（RPA）框架，解决了现有方法缺乏内部认知一致性导致复杂情境下角色失真的问题。框架通过引入三种奖励机制：认知关注奖励（基于10个角色元素如世界观的标签分析），参考引导奖励（利用参考响应的重叠度作为优化锚点），以及角色条件奖励归一化（根据角色类别调整奖励分布），显著提升了角色认知和表现的准确性。大量实验显示该方法在知识和记忆能力等方面优于现有技术。

Conclusion: Character-R1框架通过综合奖励设计，实现了更有效的角色认知推理，显著提升了角色表现的准确性和鲁棒性。

Abstract: Current role-playing agents (RPAs) are typically constructed by imitating surface-level behaviors, but this approach lacks internal cognitive consistency, often causing out-of-character errors in complex situations. To address this, we propose Character-R1, a framework designed to provide comprehensive verifiable reward signals for effective role-aware reasoning, which are missing in recent studies. Specifically, our framework comprises three core designs: (1) Cognitive Focus Reward, which enforces explicit label-based analysis of 10 character elements (e.g., worldview) to structure internal cognition; (2) Reference-Guided Reward, which utilizes overlap-based metrics with reference responses as optimization anchors to enhance exploration and performance; and (3) Character-Conditioned Reward Normalization, which adjusts reward distributions based on character categories to ensure robust optimization across heterogeneous roles. Extensive experiments demonstrate that Character-R1 significantly outperforms existing methods in knowledge, memory and others.

</details>


### [43] [From National Curricula to Cultural Awareness: Constructing Open-Ended Culture-Specific Question Answering Dataset](https://arxiv.org/abs/2601.04632)
*Haneul Yoo,Won Ik Cho,Geunhye Kim,Jiyoon Han*

Main category: cs.CL

TL;DR: 该论文提出了一种利用国家社会学课程作为文化认知监督基础的方法，构建了一个多代理大型语言模型框架CuCu，通过自动生成开放式文化特定问答对，应用于韩国国家社会学课程，形成了包含3.41万问答对的KCaQA数据库，提升模型对本土社会文化背景的理解和生成能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多语言和多文化环境中的表现不均，且受限于英语中心的训练数据，亟需文化对齐的实用方法。

Method: 提出CuCu框架，将国家教科书课程自动转化为开放式、文化特定的问答数据，作为文化对齐监督信号。

Result: 基于韩国社会学课程构建的KCaQA数据集包含3.41万个开放式问答对，实验证明其涵盖了丰富的文化特定主题，且生成的回答扎根于当地社会文化背景。

Conclusion: 利用国家社会学课程能够有效提升大型语言模型在特定文化语境下的表现和文化对齐能力。

Abstract: Large language models (LLMs) achieve strong performance on many tasks, but their progress remains uneven across languages and cultures, often reflecting values latent in English-centric training data. To enable practical cultural alignment, we propose a scalable approach that leverages national social studies curricula as a foundation for culture-aware supervision. We introduce CuCu, an automated multi-agent LLM framework that transforms national textbook curricula into open-ended, culture-specific question-answer pairs. Applying CuCu to the Korean national social studies curriculum, we construct KCaQA, comprising 34.1k open-ended QA pairs. Our quantitative and qualitative analyses suggest that KCaQA covers culture-specific topics and produces responses grounded in local sociocultural contexts.

</details>


### [44] [MAGA-Bench: Machine-Augment-Generated Text via Alignment Detection Benchmark](https://arxiv.org/abs/2601.04633)
*Anyang Song,Ying Cheng,Yiqian Xu,Rui Feng*

Main category: cs.CL

TL;DR: 本文提出了一种机器辅助生成文本(MAGA)方法，通过从提示构建到推理过程的全方位对齐，利用检测器反馈强化学习(RLDF)提升机器生成文本检测器的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着机器生成文本(MGT)越来越难以与人类文本区分，现有检测器泛化能力受限，单靠扩充MGT数据源不足，需要增强生成过程以提高检测泛化能力。

Method: 提出MAGA管线，实现从提示构造到推理过程的全面对齐，核心为基于检测器反馈的强化学习(RLDF)。

Result: 在实验中，RoBERTa检测器在MAGA数据集上泛化性能提升4.60%，而已选用检测器AUC平均下降8.13%，验证了MAGA对检测器鲁棒性的考验和泛化能力提升。

Conclusion: MAGA方法显著提升了检测器的泛化检测能力，RoBERTa检测器在MAGA训练集上泛化AUC平均提升4.60%。

Abstract: Large Language Models (LLMs) alignment is constantly evolving. Machine-Generated Text (MGT) is becoming increasingly difficult to distinguish from Human-Written Text (HWT). This has exacerbated abuse issues such as fake news and online fraud. Fine-tuned detectors' generalization ability is highly dependent on dataset quality, and simply expanding the sources of MGT is insufficient. Further augment of generation process is required. According to HC-Var's theory, enhancing the alignment of generated text can not only facilitate attacks on existing detectors to test their robustness, but also help improve the generalization ability of detectors fine-tuned on it. Therefore, we propose \textbf{M}achine-\textbf{A}ugment-\textbf{G}enerated Text via \textbf{A}lignment (MAGA). MAGA's pipeline achieves comprehensive alignment from prompt construction to reasoning process, among which \textbf{R}einforced \textbf{L}earning from \textbf{D}etectors \textbf{F}eedback (RLDF), systematically proposed by us, serves as a key component. In our experiments, the RoBERTa detector fine-tuned on MAGA training set achieved an average improvement of 4.60\% in generalization detection AUC. MAGA Dataset caused an average decrease of 8.13\% in the AUC of the selected detectors, expecting to provide indicative significance for future research on the generalization detection ability of detectors.

</details>


### [45] [SpeechMedAssist: Efficiently and Effectively Adapting Speech Language Models for Medical Consultation](https://arxiv.org/abs/2601.04638)
*Sirry Chen,Jieyi Wang,Wei Chen,Zhongyu Wei*

Main category: cs.CL

TL;DR: 本文提出了基于语音语言模型的医疗咨询系统SpeechMedAssist，采用两阶段训练策略，有效解决了数据稀缺和训练效率问题，实现了更自然高效的语音医疗问诊。


<details>
  <summary>Details</summary>
Motivation: 传统医疗咨询多为文本交互，缺乏自然的语音交互方式，且医疗语音数据稀缺，难以直接微调语音语言模型。

Method: 提出了一种名为SpeechMedAssist的多阶段训练框架，分为（1）通过文本注入知识与能力，（2）使用有限的合成语音数据进行模态重新对齐。

Result: 该方法仅需约1万条合成语音样本即可高效训练模型，且在单轮问答和多轮模拟对话任务中均超过了现有基线模型，表现出更高的有效性和鲁棒性。

Conclusion: 通过解耦训练并充分利用文本与少量语音数据，SpeechMedAssist显著提升了医疗语音问答和多轮对话的性能，有望推动语音驱动的智能医疗咨询的发展。

Abstract: Medical consultations are intrinsically speech-centric. However, most prior works focus on long-text-based interactions, which are cumbersome and patient-unfriendly. Recent advances in speech language models (SpeechLMs) have enabled more natural speech-based interaction, yet the scarcity of medical speech data and the inefficiency of directly fine-tuning on speech data jointly hinder the adoption of SpeechLMs in medical consultation. In this paper, we propose SpeechMedAssist, a SpeechLM natively capable of conducting speech-based multi-turn interactions with patients. By exploiting the architectural properties of SpeechLMs, we decouple the conventional one-stage training into a two-stage paradigm consisting of (1) Knowledge & Capability Injection via Text and (2) Modality Re-alignment with Limited Speech Data, thereby reducing the requirement for medical speech data to only 10k synthesized samples. To evaluate SpeechLMs for medical consultation scenarios, we design a benchmark comprising both single-turn question answering and multi-turn simulated interactions. Experimental results show that our model outperforms all baselines in both effectiveness and robustness in most evaluation settings.

</details>


### [46] [CRANE: Causal Relevance Analysis of Language-Specific Neurons in Multilingual Large Language Models](https://arxiv.org/abs/2601.04664)
*Yifan Le,Yunliang Li*

Main category: cs.CL

TL;DR: CRANE通过基于功能必要性的神经元级干预分析，更准确识别多语言大模型中语言特异神经元，揭示了语言能力的神经组织结构。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要通过激活度启发式方法识别语言相关神经元，混淆了语言偏好与功能重要性，导致语言能力的神经机制理解不足。

Method: 提出了CRANE方法，通过针对性神经元干预，从功能必要性角度重新定义语言特异性，使用相关性分析代替激活大小判断神经元特化。

Result: 神经元干预实验证明语言相关神经元的遮蔽会特异性影响目标语言性能且不显著影响其他语言，体现了语言选择性但非排他性的神经元特化。CRANE在英、中、越多语言、多基准测试中表现出比激活基方法更精确地识别语言特异神经元。

Conclusion: CRANE框架有效区分语言相关神经元的功能必要性，实现了对多语言大模型语言能力神经机制更深入的理解，优于传统激活度方法。

Abstract: Multilingual large language models (LLMs) achieve strong performance across languages, yet how language capabilities are organized at the neuron level remains poorly understood. Prior work has identified language-related neurons mainly through activation-based heuristics, which conflate language preference with functional importance. Prior work has identified language-related neurons mainly through activation-based heuristics, which conflate language preference with functional importance. We propose CRANE, a relevance-based analysis framework that redefines language specificity in terms of functional necessity, identifying language-specific neurons through targeted neuron-level interventions. CRANE characterizes neuron specialization by their contribution to language-conditioned predictions rather than activation magnitude. Our implementation will be made publicly available. Neuron-level interventions reveal a consistent asymmetric pattern: masking neurons relevant to a target language selectively degrades performance on that language while preserving performance on other languages to a substantial extent, indicating language-selective but non-exclusive neuron specializations. Experiments on English, Chinese, and Vietnamese across multiple benchmarks, together with a dedicated relevance-based metric and base-to-chat model transfer analysis, show that CRANE isolates language-specific components more precisely than activation-based methods.

</details>


### [47] [ToolGate: Contract-Grounded and Verified Tool Execution for LLMs](https://arxiv.org/abs/2601.04688)
*Yanming Liu,Xinyue Peng,Jiannan Cao,Xinyi Wang,Songhang Deng,Jintao Chen,Jianwei Yin,Xuhong Zhang*

Main category: cs.CL

TL;DR: 本文提出了ToolGate框架，通过形式化契约保证工具调用的逻辑安全和状态可验证，显著提升了工具增强大语言模型的可靠性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于自然语言推理的工具调用框架缺乏逻辑安全性和可验证性保障。

Method: 提出ToolGate框架，使用显式符号状态空间和Hoare式契约（前置条件和后置条件）对工具调用进行验证和状态更新，确保逻辑安全和状态可验证。

Result: ToolGate在复杂多步推理任务中显著提升了工具增强语言模型系统的可靠性和可验证性，同时保持竞争性能。

Conclusion: ToolGate为构建更可信且易调试的集成外部工具的语言模型AI系统奠定了基础，通过形式化的逻辑保障防止错误和幻觉结果破坏状态表现。

Abstract: Large Language Models (LLMs) augmented with external tools have demonstrated remarkable capabilities in complex reasoning tasks. However, existing frameworks rely heavily on natural language reasoning to determine when tools can be invoked and whether their results should be committed, lacking formal guarantees for logical safety and verifiability. We present \textbf{ToolGate}, a forward execution framework that provides logical safety guarantees and verifiable state evolution for LLM tool calling. ToolGate maintains an explicit symbolic state space as a typed key-value mapping representing trusted world information throughout the reasoning process. Each tool is formalized as a Hoare-style contract consisting of a precondition and a postcondition, where the precondition gates tool invocation by checking whether the current state satisfies the required conditions, and the postcondition determines whether the tool's result can be committed to update the state through runtime verification. Our approach guarantees that the symbolic state evolves only through verified tool executions, preventing invalid or hallucinated results from corrupting the world representation. Experimental validation demonstrates that ToolGate significantly improves the reliability and verifiability of tool-augmented LLM systems while maintaining competitive performance on complex multi-step reasoning tasks. This work establishes a foundation for building more trustworthy and debuggable AI systems that integrate language models with external tools.

</details>


### [48] [See, Explain, and Intervene: A Few-Shot Multimodal Agent Framework for Hateful Meme Moderation](https://arxiv.org/abs/2601.04692)
*Naquee Rizwan,Subhankar Swain,Paramananda Bhaskar,Gagan Aryan,Shehryaar Shah Khan,Animesh Mukherjee*

Main category: cs.CL

TL;DR: 该论文提出了一种基于生成式多模态大模型的新颖框架，实现了在有限数据条件下对恶意仇恨表情包的检测、解释和干预，弥合了现实应用中这三者分开研究的不足，提升了仇恨表情包的自动化管理能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常分开探讨仇恨表情包的检测、解释及干预，且大规模标注数据获取成本极高，无法满足实际部署需求，因此需要一个通用且高效的多模态解决方案。

Method: 采用任务专用生成式多模态代理，结合大规模多模态模型的少样本学习能力，针对不同类型的表情包进行多角度分析，包括检测、内容解释和发布前干预。

Result: 提出的框架在有限标注数据条件下能够有效检测和干预仇恨表情包，增强了模型的任务适应性和泛化能力，显示出较强的实际应用价值。

Conclusion: 该研究首次将恶意仇恨表情包的检测、解释和干预集成在一个框架下，利用少样本适应和生成式多模态智能体，实现了数据稀缺情况下的良好泛化能力，具备实际生产环境部署潜力。

Abstract: In this work, we examine hateful memes from three complementary angles - how to detect them, how to explain their content and how to intervene them prior to being posted - by applying a range of strategies built on top of generative AI models. To the best of our knowledge, explanation and intervention have typically been studied separately from detection, which does not reflect real-world conditions. Further, since curating large annotated datasets for meme moderation is prohibitively expensive, we propose a novel framework that leverages task-specific generative multimodal agents and the few-shot adaptability of large multimodal models to cater to different types of memes. We believe this is the first work focused on generalizable hateful meme moderation under limited data conditions, and has strong potential for deployment in real-world production scenarios. Warning: Contains potentially toxic contents.

</details>


### [49] [Thunder-KoNUBench: A Corpus-Aligned Benchmark for Korean Negation Understanding](https://arxiv.org/abs/2601.04693)
*Sungmok Jung,Yeonkyoung So,Joonhak Lee,Sangho Kim,Yelim Ahn,Jaejin Lee*

Main category: cs.CL

TL;DR: 本文针对韩语否定理解缺乏评测基准的问题，提出Thunder-KoNUBench基准测试，通过微调提升大语言模型在韩语否定理解上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在处理否定时性能下降，且韩语中关于否定理解的评测基准稀缺，影响模型性能评估和改进。

Method: 采用基于语料库的分析方法，构建了符合韩语否定现象实际分布的句子级基准测试Thunder-KoNUBench，对47个大语言模型进行评估，并分析模型规模和指令调优的效果。

Result: 评估结果显示，模型规模和指令调优对否定理解有影响，基于Thunder-KoNUBench的微调显著提升了模型对韩语否定及更广泛上下文理解的能力。

Conclusion: 构建的Thunder-KoNUBench有效反映韩语否定特点，微调该基准可以改善大语言模型在否定理解和整体上下文理解中的表现。

Abstract: Although negation is known to challenge large language models (LLMs), benchmarks for evaluating negation understanding, especially in Korean, are scarce. We conduct a corpus-based analysis of Korean negation and show that LLM performance degrades under negation. We then introduce Thunder-KoNUBench, a sentence-level benchmark that reflects the empirical distribution of Korean negation phenomena. Evaluating 47 LLMs, we analyze the effects of model size and instruction tuning, and show that fine-tuning on Thunder-KoNUBench improves negation understanding and broader contextual comprehension in Korean.

</details>


### [50] [PRISM: A Unified Framework for Post-Training LLMs Without Verifiable Rewards](https://arxiv.org/abs/2601.04700)
*Mukesh Ghimire,Aosong Feng,Liwen You,Youzhi Luo,Fang Liu,Xuan Zhu*

Main category: cs.CL

TL;DR: 本论文针对无标签数据训练中一致性信号的不可靠性，提出PRISM框架，通过PRM与自信度结合，提升了大语言模型训练的稳定性和效果。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力提升，进一步改进需要高质量难题解答，但这些解答人类难以提供，因此从无标签数据中获得有效学习信号变得迫切且具有挑战性。

Method: 提出Process Reward Model指导学习，与模型的自信度联合使用，在无真值标签情况下引导训练，提升信号可靠性。

Result: 提出了PRISM训练框架，结合Process Reward Model(PRM)和模型内部置信度，解决了无标签数据训练中一致性信号不可靠的问题，提高了稳定性和测试性能。

Conclusion: PRISM框架有效整合了过程奖励模型和模型内部置信度，解决了一致性信号不可靠的问题，实现了稳定的训练和更优的测试性能。

Abstract: Current techniques for post-training Large Language Models (LLMs) rely either on costly human supervision or on external verifiers to boost performance on tasks such as mathematical reasoning and code generation. However, as LLMs improve their problem-solving, any further improvement will potentially require high-quality solutions to difficult problems that are not available to humans. As a result, learning from unlabeled data is becoming increasingly attractive in the research community. Existing methods extract learning signal from a model's consistency, either by majority voting or by converting the model's internal confidence into reward. Although internal consistency metric such as entropy or self-certainty require no human intervention, as we show in this work, these are unreliable signals for large-scale and long-term training. To address the unreliability, we propose PRISM, a unified training framework that uses a Process Reward Model (PRM) to guide learning alongside model's internal confidence in the absence of ground-truth labels. We show that effectively combining PRM with self-certainty can lead to both stable training and better test-time performance, and also keep the model's internal confidence in check.

</details>


### [51] [Prior-Informed Zeroth-Order Optimization with Adaptive Direction Alignment for Memory-Efficient LLM Fine-Tuning](https://arxiv.org/abs/2601.04710)
*Feihu Jin,Shipeng Cen,Ying Tan*

Main category: cs.CL

TL;DR: 本文提出了一种先验信息引导的零阶优化方法，用于提升大规模语言模型微调的效率和性能，实验证明其优于传统零阶及部分基于梯度的方法。


<details>
  <summary>Details</summary>
Motivation: 针对大规模语言模型微调中反向传播耗费大量内存的问题，零阶优化避免了反向传播但存在梯度估计方差大、收敛慢的问题，因此需要一种提升梯度估计精度和优化效率的新方法。

Method: 通过动态计算引导向量，将高斯采样的扰动引向更具信息量的方向，从而提升零阶梯度估计的准确性与收敛速度，此外还探讨了贪婪扰动策略以利用先验知识并验证其理论性质。

Result: 本文提出了一种基于先验信息引导扰动的零阶优化方法，用于解决大规模语言模型微调中反向传播带来的内存瓶颈问题。该方法通过动态计算引导向量，优化扰动方向，提升了梯度估计的准确性和收敛速度。实验证明，该方法在多种规模和结构的语言模型上均表现出优异性能，尤其在OPT-13B模型上，实现了比传统零阶优化和基于梯度的方法更快收敛和更高准确率。

Conclusion: 本研究的零阶优化方法有效提高了梯度估计的准确度与优化效率，实现了大规模语言模型微调的更快收敛和更优性能，兼顾了计算效率与准确性。

Abstract: Fine-tuning large language models (LLMs) has achieved remarkable success across various NLP tasks, but the substantial memory overhead during backpropagation remains a critical bottleneck, especially as model scales grow. Zeroth-order (ZO) optimization alleviates this issue by estimating gradients through forward passes and Gaussian sampling, avoiding the need for backpropagation. However, conventional ZO methods suffer from high variance in gradient estimation due to their reliance on random perturbations, leading to slow convergence and suboptimal performance. We propose a simple plug-and-play method that incorporates prior-informed perturbations to refine gradient estimation. Our method dynamically computes a guiding vector from Gaussian samples, which directs perturbations toward more informative directions, significantly accelerating convergence compared to standard ZO approaches. We further investigate a greedy perturbation strategy to explore the impact of prior knowledge on gradient estimation. Theoretically, we prove that our gradient estimator achieves stronger alignment with the true gradient direction, enhancing optimization efficiency. Extensive experiments across LLMs of varying scales and architectures demonstrate that our proposed method could seamlessly integrate into existing optimization methods, delivering faster convergence and superior performance. Notably, on the OPT-13B model, our method outperforms traditional ZO optimization across all 11 benchmark tasks and surpasses gradient-based baselines on 9 out of 11 tasks, establishing a robust balance between efficiency and accuracy.

</details>


### [52] [DSC2025 -- ViHallu Challenge: Detecting Hallucination in Vietnamese LLMs](https://arxiv.org/abs/2601.04711)
*Anh Thi-Hoang Nguyen,Khanh Quoc Tran,Tin Van Huynh,Phuoc Tan-Hoang Nguyen,Cam Tan Nguyen,Kiet Van Nguyen*

Main category: cs.CL

TL;DR: 本文介绍了针对越南语大型语言模型幻觉检测的首个大规模共享任务——DSC2025 ViHallu挑战赛，并构建了包含1万条标注样本的数据集，涵盖无幻觉、内在幻觉和外在幻觉三种类型。


<details>
  <summary>Details</summary>
Motivation: 当前幻觉检测研究集中在英语，越南语等低至中资源语言缺乏标准化评测框架，限制了这些语言模型的可靠性提升。

Method: 通过构建含三种提示类型（事实、噪声、对抗）的数据集，结合指令调优大型语言模型、结构化提示和集成策略，进行幻觉检测的多方法比较。

Result: 111支队伍参与竞赛，最优模型宏F1得分达到84.80%，远超32.83%的基线模型，显示方法有效，但距离完美仍有差距。

Conclusion: 尽管表现优异的系统已显著提升检测准确率，但幻觉检测仍然具有挑战性，尤其是对内在幻觉的识别。该研究为越南语AI系统的可信度和可靠性研究奠定了基础。

Abstract: The reliability of large language models (LLMs) in production environments remains significantly constrained by their propensity to generate hallucinations -- fluent, plausible-sounding outputs that contradict or fabricate information. While hallucination detection has recently emerged as a priority in English-centric benchmarks, low-to-medium resource languages such as Vietnamese remain inadequately covered by standardized evaluation frameworks. This paper introduces the DSC2025 ViHallu Challenge, the first large-scale shared task for detecting hallucinations in Vietnamese LLMs. We present the ViHallu dataset, comprising 10,000 annotated triplets of (context, prompt, response) samples systematically partitioned into three hallucination categories: no hallucination, intrinsic, and extrinsic hallucinations. The dataset incorporates three prompt types -- factual, noisy, and adversarial -- to stress-test model robustness. A total of 111 teams participated, with the best-performing system achieving a macro-F1 score of 84.80\%, compared to a baseline encoder-only score of 32.83\%, demonstrating that instruction-tuned LLMs with structured prompting and ensemble strategies substantially outperform generic architectures. However, the gap to perfect performance indicates that hallucination detection remains a challenging problem, particularly for intrinsic (contradiction-based) hallucinations. This work establishes a rigorous benchmark and explores a diverse range of detection methodologies, providing a foundation for future research into the trustworthiness and reliability of Vietnamese language AI systems.

</details>


### [53] [Fame Fades, Nature Remains: Disentangling the Character Identity of Role-Playing Agents](https://arxiv.org/abs/2601.04716)
*Yonghyun Jun,Junhyuk Choi,Jihyeong Park,Hwanhee Lee*

Main category: cs.CL

TL;DR: 本文提出将角色身份拆解为预训练知识和行为特征两层，评估发现著名角色的优势短暂且负面社会属性显著影响RPA表现，指明了未来提升方向。


<details>
  <summary>Details</summary>
Motivation: 当前基于大型语言模型的角色扮演代理（RPA）在角色身份结构上缺乏明确的形式化，将角色简化为任意文本输入。

Method: 提出了角色身份的多维度构造，包括参数身份（预训练知识）和属性身份（行为特征），设计统一角色档案结构，生成著名与合成角色，进行单轮和多轮交互评估。

Result: 发现“名声递减”现象，即著名角色的优势在初期互动显著但迅速减弱；同时“本质不变”，模型能稳定表现性格特征，但在道德和人际关系的极性上表现敏感，负面社会性质是RPA表现的主要瓶颈。

Conclusion: 角色身份的分层结构帮助深入理解和提升RPA的表现，尤其是在处理负面社会属性方面，为未来角色构建与评估提供指导。

Abstract: Despite the rapid proliferation of Role-Playing Agents (RPAs) based on Large Language Models (LLMs), the structural dimensions defining a character's identity remain weakly formalized, often treating characters as arbitrary text inputs. In this paper, we propose the concept of \textbf{Character Identity}, a multidimensional construct that disentangles a character into two distinct layers: \textbf{(1) Parametric Identity}, referring to character-specific knowledge encoded from the LLM's pre-training, and \textbf{(2) Attributive Identity}, capturing fine-grained behavioral properties such as personality traits and moral values. To systematically investigate these layers, we construct a unified character profile schema and generate both Famous and Synthetic characters under identical structural constraints. Our evaluation across single-turn and multi-turn interactions reveals two critical phenomena. First, we identify \textit{"Fame Fades"}: while famous characters hold a significant advantage in initial turns due to parametric knowledge, this edge rapidly vanishes as models prioritize accumulating conversational context over pre-trained priors. Second, we find that \textit{"Nature Remains"}: while models robustly portray general personality traits regardless of polarity, RPA performance is highly sensitive to the valence of morality and interpersonal relationships. Our findings pinpoint negative social natures as the primary bottleneck in RPA fidelity, guiding future character construction and evaluation.

</details>


### [54] [Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking](https://arxiv.org/abs/2601.04720)
*Mingxin Li,Yanzhao Zhang,Dingkun Long,Keqin Chen,Sibo Song,Shuai Bai,Zhibo Yang,Pengjun Xie,An Yang,Dayiheng Liu,Jingren Zhou,Junyang Lin*

Main category: cs.CL

TL;DR: 介绍了基于Qwen3-VL的多模态搜索模型Qwen3-VL-Embedding和Qwen3-VL-Reranker，实现统一语义空间的多模态表示，支持多语言和长文本输入，性能在相关基准测试中领先。


<details>
  <summary>Details</summary>
Motivation: 为解决多模态检索中不同输入模态难以统一表示的问题，提供高精度、跨模态的统一语义空间表示模型。

Method: 采用多阶段训练策略，从大规模对比预训练到重排序模型蒸馏，利用跨注意力机制进行细粒度相关性估计，实现灵活的嵌入维度和长文本处理。

Result: Qwen3-VL-Embedding-8B在MMEB-V2基准测试中得分77.8，排名第一，实现多模态检索任务高性能表现。

Conclusion: Qwen3-VL-Embedding系列在多模态嵌入任务中取得了最先进的表现，支持多语言和多种输入模态，适用于多种检索任务。

Abstract: In this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in $\textbf{2B}$ and $\textbf{8B}$ parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of $\textbf{77.8}$ on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.

</details>


### [55] [Automatic Classifiers Underdetect Emotions Expressed by Men](https://arxiv.org/abs/2601.04730)
*Ivan Smirnov,Segun T. Aroyehun,Paul Plener,David Garcia*

Main category: cs.CL

TL;DR: 本研究利用超过一百万条自我标注的数据，分析了情感检测模型在性别上的偏差，发现男性文本的错误率普遍高于女性文本，提示现有情感分析工具在性别多样性样本中表现不均衡。


<details>
  <summary>Details</summary>
Motivation: 现有情感分类工具在不同人群中的表现可能存在系统性偏差，传统基准依赖第三方标注者而非实际情绪体验者，可能掩盖偏差问题。

Method: 使用大规模（超过一百万条）的自我标注数据，结合预先注册的研究设计，评估414种模型和情感类别组合中的性别偏差。

Result: 发现各类型情感分类器对男性作者文本的错误率普遍高于女性，表明情感分析尚未实现性别公平，且此偏差可能影响后续应用效果。

Conclusion: 情感分析模型存在显著的性别偏差，男性文本的误判率高于女性，影响下游应用的可靠性，需谨慎使用且加强公平性研究。

Abstract: The widespread adoption of automatic sentiment and emotion classifiers makes it important to ensure that these tools perform reliably across different populations. Yet their reliability is typically assessed using benchmarks that rely on third-party annotators rather than the individuals experiencing the emotions themselves, potentially concealing systematic biases. In this paper, we use a unique, large-scale dataset of more than one million self-annotated posts and a pre-registered research design to investigate gender biases in emotion detection across 414 combinations of models and emotion-related classes. We find that across different types of automatic classifiers and various underlying emotions, error rates are consistently higher for texts authored by men compared to those authored by women. We quantify how this bias could affect results in downstream applications and show that current machine learning tools, including large language models, should be applied with caution when the gender composition of a sample is not known or variable. Our findings demonstrate that sentiment analysis is not yet a solved problem, especially in ensuring equitable model behaviour across demographic groups.

</details>


### [56] [AM$^3$Safety: Towards Data Efficient Alignment of Multi-modal Multi-turn Safety for MLLMs](https://arxiv.org/abs/2601.04736)
*Han Zhu,Jiale Chen,Chengkun Cai,Shengjie Sun,Haoran Li,Yujin Zhou,Chi-Min Chan,Pengcheng Wen,Lei Li,Sirui Han,Yike Guo*

Main category: cs.CL

TL;DR: 本文提出了一个新多模态对话数据集和结合GRPO的优化框架，有效提升多模态语言模型的多轮安全性，减少攻击成功率并增强无害性和帮助性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在多轮对话中因安全机制逐渐失效，容易被恶意提示攻击，现有基于人类反馈的强化学习方法成本高且多针对单轮问答任务，难以有效保护模型安全性和实用性。

Method: 构建包含多轮对话及拒绝视觉问答样本的数据集InterSafe-V；提出结合初始拒绝策略与群体相对策略优化（GRPO）的AM$^3$Safety训练框架；采用回合感知双目标奖励对整个对话过程进行强化学习微调。

Result: 本文针对多模态大型语言模型（MLLMs）在多轮多模态交互中的安全漏洞问题，提出了基于新构建的数据集InterSafe-V和AM$^3$Safety框架的解决方案。InterSafe-V包含11270条对话和500条专门设计的拒绝类视觉问答样本，更好地模拟真实场景。AM$^3$Safety结合初始拒绝阶段和基于群体相对策略优化（GRPO）的细粒度微调，使用回合感知的双目标奖励，有效提升模型安全性能。实验证明，在Qwen2.5-VL-7B-Instruct和LLaVA-NeXT-7B模型上，攻击成功率降低超10%，无害性与帮助性分别提升8%以上及13%以上，同时保持了模型的通用能力。

Conclusion: 通过构建真实模拟多模态多轮对话的数据集和基于双目标奖励的GRPO微调方法，本文显著提升了多模态大语言模型的安全性且不损失其通用能力，验证了AM$^3$Safety框架的有效性。

Abstract: Multi-modal Large Language Models (MLLMs) are increasingly deployed in interactive applications. However, their safety vulnerabilities become pronounced in multi-turn multi-modal scenarios, where harmful intent can be gradually reconstructed across turns, and security protocols fade into oblivion as the conversation progresses. Existing Reinforcement Learning from Human Feedback (RLHF) alignment methods are largely developed for single-turn visual question-answer (VQA) task and often require costly manual preference annotations, limiting their effectiveness and scalability in dialogues. To address this challenge, we present InterSafe-V, an open-source multi-modal dialogue dataset containing 11,270 dialogues and 500 specially designed refusal VQA samples. This dataset, constructed through interaction between several models, is designed to more accurately reflect real-world scenarios and includes specialized VQA pairs tailored for specific domains. Building on this dataset, we propose AM$^3$Safety, a framework that combines a cold-start refusal phase with Group Relative Policy Optimization (GRPO) fine-tuning using turn-aware dual-objective rewards across entire dialogues. Experiments on Qwen2.5-VL-7B-Instruct and LLaVA-NeXT-7B show more than 10\% decrease in Attack Success Rate (ASR) together with an increment of at least 8\% in harmless dimension and over 13\% in helpful dimension of MLLMs on multi-modal multi-turn safety benchmarks, while preserving their general abilities.

</details>


### [57] [RiskAtlas: Exposing Domain-Specific Risks in LLMs through Knowledge-Graph-Guided Harmful Prompt Generation](https://arxiv.org/abs/2601.04740)
*Huawei Zheng,Xinqi Jiang,Sen Yang,Shouling Ji,Yingcai Wu,Dazhen Deng*

Main category: cs.CL

TL;DR: 本文针对专业领域LLM的隐性有害提示难以识别问题，提出了一种基于知识图谱的自动生成及重写方法，提升有害提示数据集的隐性和领域相关性，推动了安全防护研究。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）在金融和医疗等专业领域的应用带来了独特的安全风险，而现有的有害提示数据集主要依赖手工构建，且多聚焦显性有害提示，难以有效检测隐性有害提示。

Method: 基于知识图谱引导生成有害提示，随后采用直接和上下文增强的双路径重写方法，将显性有害提示转化为隐性版本，从而系统地生产隐性有害提示数据集。

Result: 提出了一个端到端框架，结合知识图谱引导的有害提示生成和双路径模糊重写方法，系统生成领域相关且隐性的有害提示数据集，提高了数据集的质量和实用性，促进了LLM安全研究。

Conclusion: 该框架能够有效生成高质量且更隐性的领域相关有害提示数据集，有助于更真实的红队测试，推动大语言模型在专业领域的安全性提升。

Abstract: Large language models (LLMs) are increasingly applied in specialized domains such as finance and healthcare, where they introduce unique safety risks. Domain-specific datasets of harmful prompts remain scarce and still largely rely on manual construction; public datasets mainly focus on explicit harmful prompts, which modern LLM defenses can often detect and refuse. In contrast, implicit harmful prompts-expressed through indirect domain knowledge-are harder to detect and better reflect real-world threats. We identify two challenges: transforming domain knowledge into actionable constraints and increasing the implicitness of generated harmful prompts. To address them, we propose an end-to-end framework that first performs knowledge-graph-guided harmful prompt generation to systematically produce domain-relevant prompts, and then applies dual-path obfuscation rewriting to convert explicit harmful prompts into implicit variants via direct and context-enhanced rewriting. This framework yields high-quality datasets combining strong domain relevance with implicitness, enabling more realistic red-teaming and advancing LLM safety research. We release our code and datasets at GitHub.

</details>


### [58] [Tool-MAD: A Multi-Agent Debate Framework for Fact Verification with Diverse Tool Augmentation and Adaptive Retrieval](https://arxiv.org/abs/2601.04742)
*Seyeon Jeong,Yeonjun Choi,JongWook Kim,Beakcheol Jang*

Main category: cs.CL

TL;DR: Tool-MAD通过多代理协作与多样化外部工具结合，提升了大语言模型在事实验证任务中的准确率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有多代理辩论系统主要依赖内部知识或静态文档，导致易出现幻觉和事实错误，且引入外部证据的方法通常检索机制单一，难以适应辩论中不断变化的信息需求。

Method: 提出了Tool-MAD多代理辩论框架，赋予每个代理不同的外部工具（如搜索API或RAG模块），通过迭代的自适应查询机制不断优化证据检索，并结合Faithfulness和Answer Relevance评分辅助判断。

Result: Tool-MAD在四个事实验证基准测试中表现优异，相较于最先进的多代理辩论框架，准确率提升最高达到5.5%。在医学专业领域也表现出较强的稳健性和适应性。

Conclusion: Tool-MAD框架有效解决了现有多代理辩论系统中的幻觉问题及静态检索局限，具有较好的实际应用前景，尤其适合复杂领域的事实核查任务。

Abstract: Large Language Models (LLMs) suffer from hallucinations and factual inaccuracies, especially in complex reasoning and fact verification tasks. Multi-Agent Debate (MAD) systems aim to improve answer accuracy by enabling multiple LLM agents to engage in dialogue, promoting diverse reasoning and mutual verification. However, existing MAD frameworks primarily rely on internal knowledge or static documents, making them vulnerable to hallucinations. While MADKE introduces external evidence to mitigate this, its one-time retrieval mechanism limits adaptability to new arguments or emerging information during the debate. To address these limitations, We propose Tool-MAD, a multi-agent debate framework that enhances factual verification by assigning each agent a distinct external tool, such as a search API or RAG module. Tool-MAD introduces three key innovations: (1) a multi-agent debate framework where agents leverage heterogeneous external tools, encouraging diverse perspectives, (2) an adaptive query formulation mechanism that iteratively refines evidence retrieval based on the flow of the debate, and (3) the integration of Faithfulness and Answer Relevance scores into the final decision process, allowing the Judge agent to quantitatively assess the coherence and question alignment of each response and effectively detect hallucinations. Experimental results on four fact verification benchmarks demonstrate that Tool-MAD consistently outperforms state-of-the-art MAD frameworks, achieving up to 5.5% accuracy improvement. Furthermore, in medically specialized domains, Tool-MAD exhibits strong robustness and adaptability across various tool configurations and domain conditions, confirming its potential for broader real-world fact-checking applications.

</details>


### [59] [PILOT-Bench: A Benchmark for Legal Reasoning in the Patent Domain with IRAC-Aligned Classification Tasks](https://arxiv.org/abs/2601.04758)
*Yehoon Jang,Chaewon Lee,Hyun-seok Min,Sungchul Choi*

Main category: cs.CL

TL;DR: 本文提出了PILOT-Bench基准，系统评估大型语言模型在专利法律推理任务中的表现，发现闭源模型明显优于开源模型，指明了未来改进方向。


<details>
  <summary>Details</summary>
Motivation: PTAB审理复杂的专利案件，需结合技术与法律推理，然而现有大型语言模型在专利法律领域结构化推理的系统性评测缺失，限制了其应用和提升。

Method: 作者提出PILOT-Bench，一个针对美国专利审判与上诉委员会（PTAB）的基准测试，结合PTAB裁决与USPTO专利数据，设计了对齐IRAC法的三个分类任务，并评估了多种闭源和开源大型语言模型在不同设置下的表现和错误趋势。

Result: 闭源模型在Issue Type任务上Micro-F1得分始终超过0.75，而最强开源模型Qwen-8B约为0.56，体现了显著的推理能力差距，PILOT-Bench提供数据、代码和资源用于促进该领域的研究。

Conclusion: PILOT-Bench基准测试首次系统地评估了大型语言模型在专利领域结构化法律推理的能力，揭示了开源模型与闭源模型之间的性能差距，推动了未来通过数据集设计和模型校准改进模型的方向。

Abstract: The Patent Trial and Appeal Board (PTAB) of the USPTO adjudicates thousands of ex parte appeals each year, requiring the integration of technical understanding and legal reasoning. While large language models (LLMs) are increasingly applied in patent and legal practice, their use has remained limited to lightweight tasks, with no established means of systematically evaluating their capacity for structured legal reasoning in the patent domain. In this work, we introduce PILOT-Bench, the first PTAB-centric benchmark that aligns PTAB decisions with USPTO patent data at the case-level and formalizes three IRAC-aligned classification tasks: Issue Type, Board Authorities, and Subdecision. We evaluate a diverse set of closed-source (commercial) and open-source LLMs and conduct analyses across multiple perspectives, including input-variation settings, model families, and error tendencies. Notably, on the Issue Type task, closed-source models consistently exceed 0.75 in Micro-F1 score, whereas the strongest open-source model (Qwen-8B) achieves performance around 0.56, highlighting a substantial gap in reasoning capabilities. PILOT-Bench establishes a foundation for the systematic evaluation of patent-domain legal reasoning and points toward future directions for improving LLMs through dataset design and model alignment. All data, code, and benchmark resources are available at https://github.com/TeamLab/pilot-bench.

</details>


### [60] [Differential syntactic and semantic encoding in LLMs](https://arxiv.org/abs/2601.04765)
*Santiago Acevedo,Alessandro Laio,Marco Baroni*

Main category: cs.CL

TL;DR: 研究发现大型语言模型DeepSeek-V3的中间层隐藏表示向量中，句法和语义信息可以通过平均相关句子的向量捕捉，且两者在向量空间中部分线性编码且有差异性分层编码。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型内部表示中句法与语义信息的编码特性，理解两者在模型中的表示方式及其相互关系。

Method: 通过对DeepSeek-V3模型中间层隐藏表示向量进行分析，计算共享句法结构或语义的句子向量平均值作为“中心”，并通过减去这些中心向量观察句子相似度变化。

Result: 发现通过句法和语义的中心向量操作，能显著影响句子相似度，表明句法和语义部分线性可分且在不同层有差异性编码。

Conclusion: 句法和语义信息在模型表示中部分线性编码且可以通过向量“中心”操作影响句子相似度，表明两种信息在模型中有一定程度的解耦和差异编码。

Abstract: We study how syntactic and semantic information is encoded in inner layer representations of Large Language Models (LLMs), focusing on the very large DeepSeek-V3. We find that, by averaging hidden-representation vectors of sentences sharing syntactic structure or meaning, we obtain vectors that capture a significant proportion of the syntactic and semantic information contained in the representations. In particular, subtracting these syntactic and semantic ``centroids'' from sentence vectors strongly affects their similarity with syntactically and semantically matched sentences, respectively, suggesting that syntax and semantics are, at least partially, linearly encoded. We also find that the cross-layer encoding profiles of syntax and semantics are different, and that the two signals can to some extent be decoupled, suggesting differential encoding of these two types of linguistic information in LLM representations.

</details>


### [61] [Revisiting Judge Decoding from First Principles via Training-Free Distributional Divergence](https://arxiv.org/abs/2601.04766)
*Shengyin Sun,Yiming Li,Renxi Liu,Weizhe Lin,Hui-Ling Zhen,Xianzhi Yu,Mingxuan Yuan,Chen Ma*

Main category: cs.CL

TL;DR: 本文提出了一种基于KL散度的训练自由的验证机制，替代了传统依赖昂贵监督的Judge Decoding方法，加速了大语言模型推理并提升了鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有Judge Decoding方法依赖昂贵且噪声较大的监督信号，限制了其应用效果和泛化能力，本文旨在寻找更简单高效的替代方案。

Method: 通过理论证明学习判别器和KL散度共享相同的logit原语，设计了一种基于KL散度的验证方法，省去了昂贵的监督需求。

Result: 实验证明该方法在多个推理和编码任务中表现优于或匹配复杂训练判别器，且鲁棒性更强，完全消除了监督瓶颈。

Conclusion: 本文证明了学习线性判别器与KL散度在本质上的对应关系，提出了一种无需训练的基于KL散度的验证机制，在多个推理和编程基准测试中表现优于复杂的训练判别器，且对领域迁移更具鲁棒性。

Abstract: Judge Decoding accelerates LLM inference by relaxing the strict verification of Speculative Decoding, yet it typically relies on expensive and noisy supervision. In this work, we revisit this paradigm from first principles, revealing that the ``criticality'' scores learned via costly supervision are intrinsically encoded in the draft-target distributional divergence. We theoretically prove a structural correspondence between learned linear judges and Kullback-Leibler (KL) divergence, demonstrating they rely on the same underlying logit primitives. Guided by this, we propose a simple, training-free verification mechanism based on KL divergence. Extensive experiments across reasoning and coding benchmarks show that our method matches or outperforms complex trained judges (e.g., AutoJudge), offering superior robustness to domain shifts and eliminating the supervision bottleneck entirely.

</details>


### [62] [LANGSAE EDITING: Improving Multilingual Information Retrieval via Post-hoc Language Identity Removal](https://arxiv.org/abs/2601.04768)
*Dongjun Kim,Jeongho Yoon,Chanjun Park,Heuiseok Lim*

Main category: cs.CL

TL;DR: 提出了一种后期稀疏自编码器方法，抑制多语言检索中语言身份信号，提升跨语言检索效果。


<details>
  <summary>Details</summary>
Motivation: 多语言检索中，语言身份信号使得同语言文本的相似度被夸大，从而掩盖了其他语言中相关证据，影响检索效果。

Method: 基于稀疏自编码器，利用跨语言激活统计识别语言相关潜在单元，在推理时抑制这些单元，并重构向量，保持原始维度和与数据库的兼容性。

Result: 本文提出了一种名为LANGSAE EDITING的后期稀疏自编码器方法，用于解决多语言检索中语言身份信号影响语义相似性评估的问题。通过识别和抑制向量中的语言相关潜在单元，该方法提升了跨语言检索的排序质量和覆盖范围，尤其对不同书写体系的语言效果显著。该方法无需重新训练编码器或对文本重新编码，保持了与现有向量数据库的兼容性。

Conclusion: LANGSAE EDITING方法有效减少了语言身份信号的干扰，显著提升了多语言密集检索系统的检索精度和跨语言表现。

Abstract: Dense retrieval in multilingual settings often searches over mixed-language collections, yet multilingual embeddings encode language identity alongside semantics. This language signal can inflate similarity for same-language pairs and crowd out relevant evidence written in other languages. We propose LANGSAE EDITING, a post-hoc sparse autoencoder trained on pooled embeddings that enables controllable removal of language-identity signal directly in vector space. The method identifies language-associated latent units using cross-language activation statistics, suppresses these units at inference time, and reconstructs embeddings in the original dimensionality, making it compatible with existing vector databases without retraining the base encoder or re-encoding raw text. Experiments across multiple languages show consistent improvements in ranking quality and cross-language coverage, with especially strong gains for script-distinct languages.

</details>


### [63] [NC2C: Automated Convexification of Generic Non-Convex Optimization Problems](https://arxiv.org/abs/2601.04789)
*Xinyue Peng,Yanming Liu,Yihan Cang,Yuwei Zhang,Xinyi Wang,Songhang Deng,Jiannan Cao*

Main category: cs.CL

TL;DR: NC2C利用大型语言模型自动将复杂的非凸优化问题转化为凸问题，减少专家依赖，提升求解效率和成功率。


<details>
  <summary>Details</summary>
Motivation: 传统求解器因非凸问题的复杂目标函数和约束结构难以处理，手动凸化效率低且过分依赖专家知识，亟需自动化、高效的方法来实现非凸问题的凸化转化。

Method: 提出了NC2C，一个基于大型语言模型（LLM）的端到端自动化框架，用于将通用非凸优化问题转化为可解的凸形式。该框架利用LLM的数学推理能力，自动检测非凸成分，选择最优凸化策略，并生成严格的凸等价问题，同时结合符号推理、自适应变换和迭代验证，配备错误纠正和可行域修正机制。

Result: 在100个多样化非凸问题数据集上的实验表明，NC2C实现89.3%的执行率和76%的成功率，将非凸问题成功转化为可行且高质量的凸问题，显著优于基线方法。

Conclusion: NC2C证明了利用大型语言模型进行自动化非凸到凸问题转化的有效性和鲁棒性，显著提升了凸优化方法在复杂非凸问题上的应用潜力。

Abstract: Non-convex optimization problems are pervasive across mathematical programming, engineering design, and scientific computing, often posing intractable challenges for traditional solvers due to their complex objective functions and constrained landscapes. To address the inefficiency of manual convexification and the over-reliance on expert knowledge, we propose NC2C, an LLM-based end-to-end automated framework designed to transform generic non-convex optimization problems into solvable convex forms using large language models. NC2C leverages LLMs' mathematical reasoning capabilities to autonomously detect non-convex components, select optimal convexification strategies, and generate rigorous convex equivalents. The framework integrates symbolic reasoning, adaptive transformation techniques, and iterative validation, equipped with error correction loops and feasibility domain correction mechanisms to ensure the robustness and validity of transformed problems. Experimental results on a diverse dataset of 100 generic non-convex problems demonstrate that NC2C achieves an 89.3\% execution rate and a 76\% success rate in producing feasible, high-quality convex transformations. This outperforms baseline methods by a significant margin, highlighting NC2C's ability to leverage LLMs for automated non-convex to convex transformation, reduce expert dependency, and enable efficient deployment of convex solvers for previously intractable optimization tasks.

</details>


### [64] [Belief in Authority: Impact of Authority in Multi-Agent Evaluation Framework](https://arxiv.org/abs/2601.04790)
*Junhyuk Choi,Jeongyoun Kwon,Heeju Kim,Haeun Cho,Hayeong Jung,Sehee Min,Bugeun Kim*

Main category: cs.CL

TL;DR: 本文系统分析多智能体系统中基于角色的权威偏见，发现专家和参照权威角色影响力更强，权威偏见由权威角色主动维持，强调明确角色表态的重要性。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统中赋予权威角色以提升性能，但权威偏见对智能体交互的影响尚未充分研究。

Method: 基于ChatEval进行自由形式多智能体评估，采用French和Raven的权力理论将权威角色分为合法、参照和专家类型，并分析其在12轮对话中的影响。

Result: 实验显示专家和参照权威角色的影响力强于合法权威角色，权威偏见通过权威角色稳固其地位体现，而非普通智能体的服从；且权威影响依赖于明确的角色表态。

Conclusion: 权威偏见在多智能体系统中主要由权威角色主动维持，对设计非对称交互多智能体框架提供了关键见解。

Abstract: Multi-agent systems utilizing large language models often assign authoritative roles to improve performance, yet the impact of authority bias on agent interactions remains underexplored. We present the first systematic analysis of role-based authority bias in free-form multi-agent evaluation using ChatEval. Applying French and Raven's power-based theory, we classify authoritative roles into legitimate, referent, and expert types and analyze their influence across 12-turn conversations. Experiments with GPT-4o and DeepSeek R1 reveal that Expert and Referent power roles exert stronger influence than Legitimate power roles. Crucially, authority bias emerges not through active conformity by general agents, but through authoritative roles consistently maintaining their positions while general agents demonstrate flexibility. Furthermore, authority influence requires clear position statements, as neutral responses fail to generate bias. These findings provide key insights for designing multi-agent frameworks with asymmetric interaction patterns.

</details>


### [65] [When AI Settles Down: Late-Stage Stability as a Signature of AI-Generated Text Detection](https://arxiv.org/abs/2601.04833)
*Ke Sun,Guangsheng Bao,Han Cui,Yue Zhang*

Main category: cs.CL

TL;DR: 本文通过分析大量文本揭示了AI生成文本后期波动快速衰减的规律，提出两种基于后期统计的简单特征，实现了先进的零样本文本检测效果。


<details>
  <summary>Details</summary>
Motivation: 传统零样本文本检测忽视了自回归生成中的时间动态，本文旨在利用后期波动性衰减这一新发现改善检测效果。

Method: 通过分析120k文本样本，观察生成文本后期波动性变化，提出Derivative Dispersion和Local Volatility两种基于后期统计的特征，用于区分AI生成与人类文本。

Result: 本文发现了AI生成文本在后期生成中log概率波动快速稳定的现象，并提出了基于该特征的两种简单指标（导数离散度和局部波动）用于零样本文本检测。该方法无需干扰采样或额外模型访问，达到了EvoBench和MAGE基准测试的最新水平，并且与现有全局方法互补。

Conclusion: AI生成文本和人类文本在生成后期波动性显著不同，利用这一现象设计的特征有助于提升检测性能，且该方法操作简便，无需额外模型访问。

Abstract: Zero-shot detection methods for AI-generated text typically aggregate token-level statistics across entire sequences, overlooking the temporal dynamics inherent to autoregressive generation. We analyze over 120k text samples and reveal Late-Stage Volatility Decay: AI-generated text exhibits rapidly stabilizing log probability fluctuations as generation progresses, while human writing maintains higher variability throughout. This divergence peaks in the second half of sequences, where AI-generated text shows 24--32\% lower volatility. Based on this finding, we propose two simple features: Derivative Dispersion and Local Volatility, which computed exclusively from late-stage statistics. Without perturbation sampling or additional model access, our method achieves state-of-the-art performance on EvoBench and MAGE benchmarks and demonstrates strong complementarity with existing global methods.

</details>


### [66] [RAAR: Retrieval Augmented Agentic Reasoning for Cross-Domain Misinformation Detection](https://arxiv.org/abs/2601.04853)
*Zhiwei Liu,Runteng Guo,Baojie Qu,Yuechen Jiang,Min Peng,Qianqian Xie,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 本论文提出了RAAR框架，通过多视角证据检索、多智能体协作推理及多任务验证，提升跨领域谣言检测能力，显著优于现有方法和大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有谣言检测方法依赖单一视角且难以在不同领域泛化，LLM虽强但受限于同分布数据，亟需跨领域通用且具备系统推理能力的检测框架。

Method: RAAR通过语义、情感和写作风格对齐的多视角证据检索，构建多步骤推理路径，利用多智能体协作并在验证器指导下整合分析结果，结合有监督微调与强化学习训练验证器。

Result: 基于RAAR的RAAR-8b和RAAR-14b模型在三项跨领域谣言检测任务中显著提升性能，超过基础模型及其他跨领域和LLM适应方法。

Conclusion: RAAR有效提升了跨领域谣言检测的泛化与推理能力，在多个任务中表现优异，超越了其他方法和先进语言模型。

Abstract: Cross-domain misinformation detection is challenging, as misinformation arises across domains with substantial differences in knowledge and discourse. Existing methods often rely on single-perspective cues and struggle to generalize to challenging or underrepresented domains, while reasoning large language models (LLMs), though effective on complex tasks, are limited to same-distribution data. To address these gaps, we introduce RAAR, the first retrieval-augmented agentic reasoning framework for cross-domain misinformation detection. To enable cross-domain transfer beyond same-distribution assumptions, RAAR retrieves multi-perspective source-domain evidence aligned with each target sample's semantics, sentiment, and writing style. To overcome single-perspective modeling and missing systematic reasoning, RAAR constructs verifiable multi-step reasoning paths through specialized multi-agent collaboration, where perspective-specific agents produce complementary analyses and a summary agent integrates them under verifier guidance. RAAR further applies supervised fine-tuning and reinforcement learning to train a single multi-task verifier to enhance verification and reasoning capabilities. Based on RAAR, we trained the RAAR-8b and RAAR-14b models. Evaluation on three cross-domain misinformation detection tasks shows that RAAR substantially enhances the capabilities of the base models and outperforms other cross-domain methods, advanced LLMs, and LLM-based adaptation approaches. The project will be released at https://github.com/lzw108/RAAR.

</details>


### [67] [Token Maturation: Autoregressive Language Generation via Continuous Token Dynamics](https://arxiv.org/abs/2601.04854)
*Oshri Naparstek*

Main category: cs.CL

TL;DR: 本文提出了一种连续自回归语言生成方法，通过连续向量表示和多步演化来稳定生成文本，避免了传统离散采样带来的不稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统自回归模型在每步必须采样离散token，导致不稳定、重复及对解码策略敏感的问题。

Method: 使用连续向量表示token，采用确定性动力学过程使token向量在多步更新中成熟，最终通过硬解码离散化生成文本。

Result: 模型仅凭连续向量的成熟过程，使用确定性解码即可生成稳定且多样的文本，且无需依赖去噪或附加稳定机制。

Conclusion: 该方法通过令token连续表示逐步收敛后再离散化，实现了无需采样即可生成连贯多样的文本，显著提升了生成稳定性。

Abstract: Autoregressive language models are conventionally defined over discrete token sequences, committing to a specific token at every generation step. This early discretization forces uncertainty to be resolved through token-level sampling, often leading to instability, repetition, and sensitivity to decoding heuristics.
  In this work, we introduce a continuous autoregressive formulation of language generation in which tokens are represented as continuous vectors that \emph{mature} over multiple update steps before being discretized. Rather than sampling tokens, the model evolves continuous token representations through a deterministic dynamical process, committing to a discrete token only when the representation has sufficiently converged. Discrete text is recovered via hard decoding, while uncertainty is maintained and resolved in the continuous space.
  We show that this maturation process alone is sufficient to produce coherent and diverse text using deterministic decoding (argmax), without reliance on token-level sampling, diffusion-style denoising, or auxiliary stabilization mechanisms. Additional perturbations, such as stochastic dynamics or history smoothing, can be incorporated naturally but are not required for the model to function.
  To our knowledge, this is the first autoregressive language model that generates text by evolving continuous token representations to convergence prior to discretization, enabling stable generation without token-level sampling.

</details>


### [68] [MisSpans: Fine-Grained False Span Identification in Cross-Domain Fake News](https://arxiv.org/abs/2601.04857)
*Zhiwei Liu,Paul Thompson,Jiaqi Rong,Baojie Qu,Runteng Guo,Min Peng,Qianqian Xie,Sophia Ananiadou*

Main category: cs.CL

TL;DR: MisSpans数据集支持多领域、细粒度谣言片段检测与解析，是首个提供细粒度定位和类型分类的谣言检测基准，评测揭示细粒度谣言检测具有挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有谣言检测大多基于全 Claim 或段落，采用粗粒度二分类标签，忽视了真假细节常共存于句内且缺乏细粒度解释，限制了模型的实用性与解释能力，亟需细粒度、多任务的谣言检测基准以推动该领域研究。

Method: 设计了包含实假新闻配对、多任务（定位误导片段、分类类型、生成解释）的多领域数据集，并使用统一指导原则进行专家标注，同时评测15种大语言模型在零样本和少样本条件下的表现。

Result: 本文提出了MisSpans数据集，首个针对多领域谣言细粒度片段级检测和分析的人工标注基准。该数据集包含真假新闻对，通过三项任务实现谣言检测的细粒度定位、谣言类型分类和解释生成，提升了谣言检测的解析能力和实用性。超过15款大语言模型在该数据集上进行了评测，结果显示细粒度谣言识别挑战较大，模型性能受多种因素影响。

Conclusion: MisSpans基准显著推动了谣言细粒度检测与分析研究，揭示模型在此任务下的不足，促进未来结合模型规模、推理能力和领域特征的改进。

Abstract: Online misinformation is increasingly pervasive, yet most existing benchmarks and methods evaluate veracity at the level of whole claims or paragraphs using coarse binary labels, obscuring how true and false details often co-exist within single sentences. These simplifications also limit interpretability: global explanations cannot identify which specific segments are misleading or differentiate how a detail is false (e.g., distorted vs. fabricated). To address these gaps, we introduce MisSpans, the first multi-domain, human-annotated benchmark for span-level misinformation detection and analysis, consisting of paired real and fake news stories. MisSpans defines three complementary tasks: MisSpansIdentity for pinpointing false spans within sentences, MisSpansType for categorising false spans by misinformation type, and MisSpansExplanation for providing rationales grounded in identified spans. Together, these tasks enable fine-grained localisation, nuanced characterisation beyond true/false and actionable explanations. Expert annotators were guided by standardised guidelines and consistency checks, leading to high inter-annotator agreement. We evaluate 15 representative LLMs, including reasoning-enhanced and non-reasoning variants, under zero-shot and one-shot settings. Results reveal the challenging nature of fine-grained misinformation identification and analysis, and highlight the need for a deeper understanding of how performance may be influenced by multiple interacting factors, including model size and reasoning capabilities, along with domain-specific textual features. This project will be available at https://github.com/lzw108/MisSpans.

</details>


### [69] [A Navigational Approach for Comprehensive RAG via Traversal over Proposition Graphs](https://arxiv.org/abs/2601.04859)
*Maxime Delmas,Lei Xu,André Freitas*

Main category: cs.CL

TL;DR: ToPG通过基于查询的图遍历和细粒度事实命题结合，改善了多跳和单跳检索表现，是一种有效的结构化RAG框架。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法在简单事实检索表现优异，但对复杂多跳问题缺乏结构连接性。交叉检索与推理方法缺少全局语料库感知，基于知识图谱的RAG在复杂任务上表现强劲，但对单跳事实查询效果差。

Method: 提出了一种名为ToPG的RAG新框架，将知识库建模为命题、实体和段落组成的异构图，通过迭代的建议-选择循环实现基于查询的图遍历和LLM反馈修剪，提高检索效果。

Result: 在简单、复杂和抽象三类QA任务上，ToPG在准确率和质量指标上均表现出色，证明了基于查询的图遍历结合事实细粒度对于高效结构化RAG系统的重要性。

Conclusion: ToPG展示了结合查询感知的图结构遍历与细粒度事实建模对于提升RAG系统多样化问题的检索和推理能力的关键作用。

Abstract: Standard RAG pipelines based on chunking excel at simple factual retrieval but fail on complex multi-hop queries due to a lack of structural connectivity. Conversely, initial strategies that interleave retrieval with reasoning often lack global corpus awareness, while Knowledge Graph (KG)-based RAG performs strongly on complex multi-hop tasks but suffers on fact-oriented single-hop queries. To bridge this gap, we propose a novel RAG framework: ToPG (Traversal over Proposition Graphs). ToPG models its knowledge base as a heterogeneous graph of propositions, entities, and passages, effectively combining the granular fact density of propositions with graph connectivity. We leverage this structure using iterative Suggestion-Selection cycles, where the Suggestion phase enables a query-aware traversal of the graph, and the Selection phase provides LLM feedback to prune irrelevant propositions and seed the next iteration. Evaluated on three distinct QA tasks (Simple, Complex, and Abstract QA), ToPG demonstrates strong performance across both accuracy- and quality-based metrics. Overall, ToPG shows that query-aware graph traversal combined with factual granularity is a critical component for efficient structured RAG systems. ToPG is available at https://github.com/idiap/ToPG.

</details>


### [70] [EvolSQL: Structure-Aware Evolution for Scalable Text-to-SQL Data Synthesis](https://arxiv.org/abs/2601.04875)
*Xuanguang Pan,Chongyang Tao,Jiayuan Bai,Jianling Gao,Zhengwei Tao,Xiansheng Zhou,Gavin Cheung,Shuai Ma*

Main category: cs.CL

TL;DR: EvolSQL通过结构感知的数据合成策略提升了Text-to-SQL训练数据的质量和复杂性，显著提高了模型性能。


<details>
  <summary>Details</summary>
Motivation: 训练高效的Text-to-SQL模型面临高质量、多样且结构复杂的数据集稀缺的问题，现有方法缺乏对SQL结构的显式控制，导致结构多样性和复杂性有限。

Method: 提出EvolSQL框架，通过起始的Query-SQL扩展增加问题多样性和模式覆盖，随后利用六种基于SQL抽象语法树的原子变换算子进行方向性进化策略，逐步增加查询的复杂度，并通过执行驱动的SQL优化模块和模式感知的去重确保高质量和结构多样的映射对。

Result: 在实验中，使用仅为SynSQL数据集1/18大小的数据微调的7B模型表现优于使用更大SynSQL数据集训练的模型。

Conclusion: EvolSQL有效提升了训练数据的结构多样性和复杂性，从而提升了Text-to-SQL模型的表现，证明了结构感知的数据合成方法的优势。

Abstract: Training effective Text-to-SQL models remains challenging due to the scarcity of high-quality, diverse, and structurally complex datasets. Existing methods either rely on limited human-annotated corpora, or synthesize datasets directly by simply prompting LLMs without explicit control over SQL structures, often resulting in limited structural diversity and complexity. To address this, we introduce EvolSQL, a structure-aware data synthesis framework that evolves SQL queries from seed data into richer and more semantically diverse forms. EvolSQL starts with an exploratory Query-SQL expansion to broaden question diversity and improve schema coverage, and then applies an adaptive directional evolution strategy using six atomic transformation operators derived from the SQL Abstract Syntax Tree to progressively increase query complexity across relational, predicate, aggregation, and nesting dimensions. An execution-grounded SQL refinement module and schema-aware deduplication further ensure the creation of high-quality, structurally diverse mapping pairs. Experimental results show that a 7B model fine-tuned on our data outperforms one trained on the much larger SynSQL dataset using only 1/18 of the data.

</details>


### [71] [Mind2Report: A Cognitive Deep Research Agent for Expert-Level Commercial Report Synthesis](https://arxiv.org/abs/2601.04879)
*Mingyue Cheng,Daoyu Wang,Qi Liu,Shuo Yu,Xiaoyu Tao,Yuqian Wang,Chengzhong Chu,Yu Duan,Mingkang Long,Enhong Chen*

Main category: cs.CL

TL;DR: 本文提出了一种名为Mind2Report的认知深度研究代理，模拟商业分析师合成专家级报告，通过细粒度意图探测、动态内存支持的多次迭代报告合成，显著提升报告的质量、可靠性和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有商业报告生成方法在质量、可靠性和覆盖范围上仍存在不足，急需一种高效且具专家水平的解决方案以辅助高风险的商业决策。

Method: Mind2Report首先探测细粒度意图，动态搜索网络信息并即时记录精炼内容，随后利用结合动态记忆的通用大语言模型进行迭代报告合成，整个流程无需训练支持。

Result: 通过QRC-Eval构建的200个真实商业任务评测，实验证明Mind2Report在报告质量、可靠性和覆盖率上均优于现有领先模型。

Conclusion: Mind2Report在综合评价中超越了包括OpenAI和Gemini在内的领先深度研究代理，展示了训练免费且支持长文本认知过程的潜力，为未来商业深度研究代理设计奠定基础。

Abstract: Synthesizing informative commercial reports from massive and noisy web sources is critical for high-stakes business decisions. Although current deep research agents achieve notable progress, their reports still remain limited in terms of quality, reliability, and coverage. In this work, we propose Mind2Report, a cognitive deep research agent that emulates the commercial analyst to synthesize expert-level reports. Specifically, it first probes fine-grained intent, then searches web sources and records distilled information on the fly, and subsequently iteratively synthesizes the report. We design Mind2Report as a training-free agentic workflow that augments general large language models (LLMs) with dynamic memory to support these long-form cognitive processes. To rigorously evaluate Mind2Report, we further construct QRC-Eval comprising 200 real-world commercial tasks and establish a holistic evaluation strategy to assess report quality, reliability, and coverage. Experiments demonstrate that Mind2Report outperforms leading baselines, including OpenAI and Gemini deep research agents. Although this is a preliminary study, we expect it to serve as a foundation for advancing the future design of commercial deep research agents. Our code and data are available at https://github.com/Melmaphother/Mind2Report.

</details>


### [72] [CuMA: Aligning LLMs with Sparse Cultural Values via Demographic-Aware Mixture of Adapters](https://arxiv.org/abs/2601.04885)
*Ao Sun,Xiaoyu Wang,Zhe Tan,Yu Li,Jiachen Zhu,Shu Su,Yuheng Jia*

Main category: cs.CL

TL;DR: 针对大型语言模型多文化对齐中的均值崩溃问题，提出CuMA框架通过专家子空间分离不同文化，显著提升了模型的文化多样性和对齐效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在面对全球多元文化时，现有方法倾向于强制统一价值观，忽视文化差异，导致模型表现出平均化且缺乏多样性的问题（均值崩溃）。

Method: 提出CuMA框架，通过引入人口统计感知的路由机制，将模型容量条件分离，实现不同文化模式的梯度解耦，避免参数互相干扰。

Result: CuMA在多个文化相关评测基准上均表现优异，显著优于传统密集模型和仅考虑语义的专家混合模型，有效缓解均值崩溃现象，保持文化多样性。

Conclusion: CuMA通过条件容量分离和文化拓扑建模，成功解决了多文化对齐中的均值崩溃问题，实现了更加多元和包容的语言模型对齐。

Abstract: As Large Language Models (LLMs) serve a global audience, alignment must transition from enforcing universal consensus to respecting cultural pluralism. We demonstrate that dense models, when forced to fit conflicting value distributions, suffer from \textbf{Mean Collapse}, converging to a generic average that fails to represent diverse groups. We attribute this to \textbf{Cultural Sparsity}, where gradient interference prevents dense parameters from spanning distinct cultural modes. To resolve this, we propose \textbf{\textsc{CuMA}} (\textbf{Cu}ltural \textbf{M}ixture of \textbf{A}dapters), a framework that frames alignment as a \textbf{conditional capacity separation} problem. By incorporating demographic-aware routing, \textsc{CuMA} internalizes a \textit{Latent Cultural Topology} to explicitly disentangle conflicting gradients into specialized expert subspaces. Extensive evaluations on WorldValuesBench, Community Alignment, and PRISM demonstrate that \textsc{CuMA} achieves state-of-the-art performance, significantly outperforming both dense baselines and semantic-only MoEs. Crucially, our analysis confirms that \textsc{CuMA} effectively mitigates mean collapse, preserving cultural diversity. Our code is available at https://github.com/Throll/CuMA.

</details>


### [73] [Faithful Summarisation under Disagreement via Belief-Level Aggregation](https://arxiv.org/abs/2601.04889)
*Favour Yahdii Aghaebe,Tanefa Apekey,Elizabeth Williams,Nafise Sadat Moosavi*

Main category: cs.CL

TL;DR: 提出了一种分歧感知的摘要生成方法，通过信念层面的聚合显著提升了观点重叠和冲突文档的摘要忠实度，且在多种模型上表现稳定。


<details>
  <summary>Details</summary>
Motivation: 现有观点和多文档摘要方法，尤其是基于大型语言模型的系统，隐式地平滑了分歧，过度表现多数观点，限制了在观点高度对立的场景中生成摘要的忠实度。

Method: 引入了一个分歧感知的综合流程，将信念层面的聚合与语言生成分开处理。首先，将文档表示为结构化的信念集合，利用基于距离的信念合并算子明确建模冲突进行聚合，然后仅利用大型语言模型将聚合后的信念实现为自然语言摘要。

Result: 在多个模型系列和规模上评估该方法，结果显示，当聚合在生成阶段进行时，足够大的模型可以匹配信念层面的聚合表现，但这种表现不稳定。相比之下，信念层面的聚合结合简单的提示方法在不同模型中都能持续提供强有力的分歧感知性能，同时保持流畅且有依据的摘要。

Conclusion: 信念层面聚合与简单提示结合的方式，比在生成阶段进行聚合的方法更稳定和有效，能持续生成反映分歧且流畅、扎实的摘要。

Abstract: Opinion and multi-document summarisation often involve genuinely conflicting viewpoints, yet many existing approaches, particularly LLM-based systems, implicitly smooth disagreement and over-represent majority opinions. This limits the faithfulness of generated summaries in opinion-heavy settings. We introduce a disagreement-aware synthesis pipeline that separates belief-level aggregation from language generation. Documents are first represented as structured belief sets and aggregated using distance-based belief merging operators that explicitly model conflict. Large language models are then used only to realise the aggregated beliefs as natural language summaries. We evaluate the approach across multiple model families and scales, comparing it to methods that perform explicit aggregation during generation. Our results show that while sufficiently large models can match belief-level aggregation when aggregation is handled at generation time, this behaviour is not stable across architectures or capacities. In contrast, belief-level aggregation combined with simple prompting yields consistently strong disagreement-aware performance across models, while maintaining fluent and grounded summaries.

</details>


### [74] [V-FAT: Benchmarking Visual Fidelity Against Text-bias](https://arxiv.org/abs/2601.04897)
*Ziteng Wang,Yujie He,Guanliang Li,Siqi Yang,Jiaqi Xiong,Songxiang Liu*

Main category: cs.CL

TL;DR: 本文通过设计新的诊断测试和指标，揭示多模态语言模型过度依赖语言信息导致视觉理解能力弱的严重问题。


<details>
  <summary>Details</summary>
Motivation: 当前MLLM虽然在标准视觉推理测试中表现出色，但可能是依赖语言捷径而非真正的视觉理解，亟需明确这一偏见的来源并量化其影响。

Method: 通过设计V-FAT诊断基准，采用三层级评估框架（内部语料偏见、外部指令偏见和二者结合的协同偏见），并引入视觉鲁棒性分数（VRS）来量化模型的视觉真实性。

Result: 评估12个前沿MLLM，发现它们在语言主导情况下视觉性能显著下降，揭示模型在现有任务中存在视觉崩溃问题。

Conclusion: 多模态大型语言模型（MLLM）在视觉推理任务上表现优异，但存在严重的文本偏见，导致模型过度依赖语言线索而非视觉信息。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated impressive performance on standard visual reasoning benchmarks. However, there is growing concern that these models rely excessively on linguistic shortcuts rather than genuine visual grounding, a phenomenon we term Text Bias. In this paper, we investigate the fundamental tension between visual perception and linguistic priors. We decouple the sources of this bias into two dimensions: Internal Corpus Bias, stemming from statistical correlations in pretraining, and External Instruction Bias, arising from the alignment-induced tendency toward sycophancy. To quantify this effect, we introduce V-FAT (Visual Fidelity Against Text-bias), a diagnostic benchmark comprising 4,026 VQA instances across six semantic domains. V-FAT employs a Three-Level Evaluation Framework that systematically increases the conflict between visual evidence and textual information: (L1) internal bias from atypical images, (L2) external bias from misleading instructions, and (L3) synergistic bias where both coincide. We introduce the Visual Robustness Score (VRS), a metric designed to penalize "lucky" linguistic guesses and reward true visual fidelity. Our evaluation of 12 frontier MLLMs reveals that while models excel in existing benchmarks, they experience significant visual collapse under high linguistic dominance.

</details>


### [75] [Can AI-Generated Persuasion Be Detected? Persuaficial Benchmark and AI vs. Human Linguistic Differences](https://arxiv.org/abs/2601.04925)
*Arkadiusz Modzelewski,Paweł Golik,Anna Kołos,Giovanni Da San Martino*

Main category: cs.CL

TL;DR: 本文提出多语言劝说文本基准数据集，发现微妙的LLM生成劝说文本比人类文本更难检测，并进行了语言学分析以助力检测工具研发。


<details>
  <summary>Details</summary>
Motivation: LLM生成高说服力文本可能被用于宣传和操控，研究其与人类文本的检测难度差异以提升自动检测技术的有效性和安全性。

Method: 作者构建了多语言基准数据集Persuaficial，分类总结LLM劝说文本生成方法，并通过实证实验比较人类与LLM撰写文本的检测性能，最后进行了语言学对比分析。

Result: 本文研究了大型语言模型(LLMs)生成的劝说文本与人类撰写劝说文本的自动检测难度。通过总结可控生成方法，并构建了一个覆盖六种语言的高质量多语言基准数据集Persuaficial，作者开展了广泛的实证比较研究。结果显示，明显的LLM劝说文本比人类文本更易被检测，但微妙的LLM劝说则显著降低检测性能。除此之外，文章还首次进行了人类与LLM劝说文本的语言学对比分析，为更具解释性和鲁棒性的检测工具开发提供了指导。

Conclusion: 微妙的LLM生成劝说文本比人类撰写的更难以自动检测，语言学差异分析有助于提升检测工具的解释性和鲁棒性。

Abstract: Large Language Models (LLMs) can generate highly persuasive text, raising concerns about their misuse for propaganda, manipulation, and other harmful purposes. This leads us to our central question: Is LLM-generated persuasion more difficult to automatically detect than human-written persuasion? To address this, we categorize controllable generation approaches for producing persuasive content with LLMs and introduce Persuaficial, a high-quality multilingual benchmark covering six languages: English, German, Polish, Italian, French and Russian. Using this benchmark, we conduct extensive empirical evaluations comparing human-authored and LLM-generated persuasive texts. We find that although overtly persuasive LLM-generated texts can be easier to detect than human-written ones, subtle LLM-generated persuasion consistently degrades automatic detection performance. Beyond detection performance, we provide the first comprehensive linguistic analysis contrasting human and LLM-generated persuasive texts, offering insights that may guide the development of more interpretable and robust detection tools.

</details>


### [76] [GenProve: Learning to Generate Text with Fine-Grained Provenance](https://arxiv.org/abs/2601.04932)
*Jingxuan Wei,Xingyue Wang,Yanghaoyu Liao,Jie Dong,Yuchen Liu,Caijun Jia,Bihui Yu,Junnan Zhu*

Main category: cs.CL

TL;DR: 本文提出细粒度溯源任务和数据集及训练框架GenProve，显著提升大型语言模型的答案和溯源准确性，但推理溯源仍具挑战。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常产生幻觉，虽然添加引用是常见的解决方案，但用户难以验证引用来源如何支持生成的内容，因此需要细粒度的溯源方法。

Method: 提出了Generation-time Fine-grained Provenance任务，设计了ReFInE数据集来区分引用、压缩和推理，基于此提出GenProve框架，结合监督微调和组相对策略优化，通过优化答案准确性和溯源正确性的复合奖励进行训练。

Result: GenProve在联合评测中显著优于14个强大的大型语言模型，能够生成流畅答案并产生结构化的句子级溯源三元组。

Conclusion: 模型在表层引用上表现优异，但推理型溯源存在显著不足，表明可验证推理仍是一个挑战，区别于表层引用问题。

Abstract: Large language models (LLM) often hallucinate, and while adding citations is a common solution, it is frequently insufficient for accountability as users struggle to verify how a cited source supports a generated claim. Existing methods are typically coarse-grained and fail to distinguish between direct quotes and complex reasoning. In this paper, we introduce Generation-time Fine-grained Provenance, a task where models must generate fluent answers while simultaneously producing structured, sentence-level provenance triples. To enable this, we present ReFInE (Relation-aware Fine-grained Interpretability & Evidence), a dataset featuring expert verified annotations that distinguish between Quotation, Compression, and Inference. Building on ReFInE, we propose GenProve, a framework that combines Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO). By optimizing a composite reward for answer fidelity and provenance correctness, GenProve significantly outperforms 14 strong LLMs in joint evaluation. Crucially, our analysis uncovers a reasoning gap where models excel at surface-level quotation but struggle significantly with inference-based provenance, suggesting that verifiable reasoning remains a frontier challenge distinct from surface-level citation.

</details>


### [77] [A Unified Spoken Language Model with Injected Emotional-Attribution Thinking for Human-like Interaction](https://arxiv.org/abs/2601.04960)
*Qing Wang,Zehan Li,Yaodong Song,Hongjie Chen,Jian Kang,Jie Lian,Jie Li,Yongxiang Li,Xuelong Li*

Main category: cs.CL

TL;DR: 该文提出了基于IEAT策略的情感智能口语语言模型，通过两阶段训练实现情绪推理的内化，显著提升了情感识别与响应生成的效果。


<details>
  <summary>Details</summary>
Motivation: 现有情感智能模型多将情绪作为外部监督，较难内化情绪推理过程，该工作旨在通过融合情绪及其成因实现情绪感知推理的内化，提升模型的情感理解与生成能力。

Method: 提出IEAT数据构建策略，结合用户情绪状态及原因进行内部推理；采用两阶段训练策略，先进行语音-文本对齐和情感属性建模，再进行端到端跨模态联合优化。

Result: 该论文提出了一种统一的口语语言模型用于情感智能，采用了一种称为Injected Emotional-Attribution Thinking (IEAT)的新颖数据构建策略。IEAT将用户情绪状态及其背后原因融入模型的内部推理过程，使情绪感知推理被内化而非作为显式监督。模型训练采用两阶段渐进策略，第一阶段通过自我蒸馏进行语音-文本对齐与情绪属性建模，第二阶段进行端到端跨模态联合优化，确保文本与口语情绪表达的一致性。实验结果表明，该方法在HumDial情感智能基准测试中，在情绪轨迹建模、情绪推理及共情响应生成方面，均取得了领先表现。

Conclusion: IEAT策略及两阶段训练显著提升了口语情感智能模型的性能，使模型能够更好地理解和生成情感相关内容，表现优于现有方法。

Abstract: This paper presents a unified spoken language model for emotional intelligence, enhanced by a novel data construction strategy termed Injected Emotional-Attribution Thinking (IEAT). IEAT incorporates user emotional states and their underlying causes into the model's internal reasoning process, enabling emotion-aware reasoning to be internalized rather than treated as explicit supervision. The model is trained with a two-stage progressive strategy. The first stage performs speech-text alignment and emotional attribute modeling via self-distillation, while the second stage conducts end-to-end cross-modal joint optimization to ensure consistency between textual and spoken emotional expressions. Experiments on the Human-like Spoken Dialogue Systems Challenge (HumDial) Emotional Intelligence benchmark demonstrate that the proposed approach achieves top-ranked performance across emotional trajectory modeling, emotional reasoning, and empathetic response generation under both LLM-based and human evaluations.

</details>


### [78] [Text as a Universal Interface for Transferable Personalization](https://arxiv.org/abs/2601.04963)
*Yuting Liu,Jian Guan,Jia-Nan Li,Wei Wu,Jiang-Ming Yang,Jianzhe Zhao,Guibing Guo*

Main category: cs.CL

TL;DR: 本文提出利用自然语言作为通用偏好表示，结合监督微调和强化学习训练AlignXplore+模型，实现了可解释、可迁移的用户个性化，并在多项基准测试中达到SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型个性化方法多依赖隐式、模型特定的向量或参数，导致偏好难以解释且不可迁移。我们希望通过自然语言实现通用、可解释且可迁移的偏好表示。

Method: 两阶段训练框架包括基于高质量合成数据的监督微调和强化学习，优化长期效用和跨任务迁移能力。开发了8B参数的AlignXplore+模型，以生成自然语言偏好摘要。

Result: 在九个基准测试中，我们的8B模型表现优异，显著超越更大规模的开源模型，同时具备跨任务、跨模型族及多交互形式的强大迁移能力。

Conclusion: 本论文提出使用自然语言作为用户偏好表征的普适接口，实现了偏好描述的可解释性和可迁移性。我们设计了一个结合监督微调和强化学习的两阶段训练框架，开发了AlignXplore+模型，在多任务和多模型场景下表现出强大的性能和迁移能力。

Abstract: We study the problem of personalization in large language models (LLMs). Prior work predominantly represents user preferences as implicit, model-specific vectors or parameters, yielding opaque ``black-box'' profiles that are difficult to interpret and transfer across models and tasks. In contrast, we advocate natural language as a universal, model- and task-agnostic interface for preference representation. The formulation leads to interpretable and reusable preference descriptions, while naturally supporting continual evolution as new interactions are observed. To learn such representations, we introduce a two-stage training framework that combines supervised fine-tuning on high-quality synthesized data with reinforcement learning to optimize long-term utility and cross-task transferability. Based on this framework, we develop AlignXplore+, a universal preference reasoning model that generates textual preference summaries. Experiments on nine benchmarks show that our 8B model achieves state-of-the-art performanc -- outperforming substantially larger open-source models -- while exhibiting strong transferability across tasks, model families, and interaction formats.

</details>


### [79] [Learning from Mistakes: Negative Reasoning Samples Enhance Out-of-Domain Generalization](https://arxiv.org/abs/2601.04992)
*Xueyun Tian,Minghua Ma,Bingbing Xu,Nuoyan Lyu,Wei Li,Heng Dong,Zheng Chu,Yuanzhuo Wang,Huawei Shen*

Main category: cs.CL

TL;DR: 将负面推理轨迹纳入监督微调显著提升大型语言模型的领域外泛化能力。


<details>
  <summary>Details</summary>
Motivation: 标准做法忽略负面样本导致过拟合和泛化能力不足，希望发掘负面轨迹中的有效推理信息以提升模型性能。

Method: 提出了一种基于增益的损失加权方法（GLOW），通过动态调整样本损失权重，高效利用负面轨迹进行训练。

Result: GLOW在Qwen2.5-7B实现5.51%的领域外性能提升，并使MMLU测试准确率从72.82%提升至76.47%。

Conclusion: 加入负面轨迹有助于缓解过拟合，促进探索，显著提升模型的跨域推理性能。

Abstract: Supervised fine-tuning (SFT) on chain-of-thought (CoT) trajectories demonstrations is a common approach for enabling reasoning in large language models. Standard practices typically only retain trajectories with correct final answers (positives) while ignoring the rest (negatives). We argue that this paradigm discards substantial supervision and exacerbates overfitting, limiting out-of-domain (OOD) generalization. Specifically, we surprisingly find that incorporating negative trajectories into SFT yields substantial OOD generalization gains over positive-only training, as these trajectories often retain valid intermediate reasoning despite incorrect final answers. To understand this effect in depth, we systematically analyze data, training dynamics, and inference behavior, identifying 22 recurring patterns in negative chains that serve a dual role: they moderate loss descent to mitigate overfitting during training and boost policy entropy by 35.67% during inference to facilitate exploration. Motivated by these observations, we further propose Gain-based LOss Weighting (GLOW), an adaptive, sample-aware scheme that exploits such distinctive training dynamics by rescaling per-sample loss based on inter-epoch progress. Empirically, GLOW efficiently leverages unfiltered trajectories, yielding a 5.51% OOD gain over positive-only SFT on Qwen2.5-7B and boosting MMLU from 72.82% to 76.47% as an RL initialization.

</details>


### [80] [Can Large Language Models Resolve Semantic Discrepancy in Self-Destructive Subcultures? Evidence from Jirai Kei](https://arxiv.org/abs/2601.05004)
*Peng Wang,Xilin Tao,Siyi Yao,Jiageng Wu,Yuntao Zou,Zhuotao Tian,Libo Qin,Dagang Li*

Main category: cs.CL

TL;DR: 针对亚文化中自毁行为难识别问题，提出SAS多智能体框架，通过知识检索与语义对齐提升LLM性能，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自毁行为难以诊断，尤其在亚文化群体中因独特表达更难识别。随着LLM应用普及，研究者兴起用LLM检测自毁行为的尝试，存在知识更新滞后和亚文化语义误差挑战。

Method: 提出了Subcultural Alignment Solver (SAS)框架，结合自动检索与亚文化对齐技术，提高LLM对亚文化特有表达的理解能力，从而增强自毁行为识别准确率。

Result: 实验表明SAS在自毁行为检测任务中表现优于先进多智能体框架OWL，同时与微调后的LLM性能相当。

Conclusion: 基于多智能体的SAS框架有效解决了亚文化语境下自毁行为检测中的知识滞后和语义错位问题，性能优于当前先进框架，并接近微调后的大型语言模型。

Abstract: Self-destructive behaviors are linked to complex psychological states and can be challenging to diagnose. These behaviors may be even harder to identify within subcultural groups due to their unique expressions. As large language models (LLMs) are applied across various fields, some researchers have begun exploring their application for detecting self-destructive behaviors. Motivated by this, we investigate self-destructive behavior detection within subcultures using current LLM-based methods. However, these methods have two main challenges: (1) Knowledge Lag: Subcultural slang evolves rapidly, faster than LLMs' training cycles; and (2) Semantic Misalignment: it is challenging to grasp the specific and nuanced expressions unique to subcultures. To address these issues, we proposed Subcultural Alignment Solver (SAS), a multi-agent framework that incorporates automatic retrieval and subculture alignment, significantly enhancing the performance of LLMs in detecting self-destructive behavior. Our experimental results show that SAS outperforms the current advanced multi-agent framework OWL. Notably, it competes well with fine-tuned LLMs. We hope that SAS will advance the field of self-destructive behavior detection in subcultural contexts and serve as a valuable resource for future researchers.

</details>


### [81] [Hán Dān Xué Bù (Mimicry) or Qīng Chū Yú Lán (Mastery)? A Cognitive Perspective on Reasoning Distillation in Large Language Models](https://arxiv.org/abs/2601.05019)
*Yueqing Hu,Xinyang Peng,Shuting Peng,Hanqi Wang,Tianhong Wang*

Main category: cs.CL

TL;DR: 研究发现通过监督微调进行推理蒸馏时，学生模型未能继承教师模型的人类认知负荷特征，导致对齐崩溃和性能下降，说明类人认知依赖主动强化而非被动模仿。


<details>
  <summary>Details</summary>
Motivation: 探索为何当前推理模型蒸馏技术不能有效传递教师模型中蕴含的人类认知负荷结构，揭示认知对齐的本质及其局限。

Method: 本文通过测试14个模型的教师与学生之间的认知难度相关性，验证“表面模仿”假设，分析监督微调在推理蒸馏中导致的功能对齐崩溃和“货物崇拜”效应。

Result: 本文研究了最近通过强化学习训练的大型推理模型，其表现出与人类认知负荷的“自然”对齐特性。但是，当前通过监督微调进行推理蒸馏的方法未能传递这种认知结构。通过对14个模型测试“表面模仿”假设，发现蒸馏过程导致“功能对齐崩溃”——教师模型与人类难度扩展具有较高相关性（平均相关系数0.64），而学生模型则显著下降（平均0.34），有时甚至表现不如蒸馏前。这表明监督微调引发了“货物崇拜”效应，学生模型虽然模仿了推理的语言形式（冗长），却未能内化教师的动态资源分配策略，从而推理蒸馏使计算成本与认知需求出现脱节，表明类人认知是强化学习过程的自发产物而非被动模仿。

Conclusion: 推理蒸馏中，监督微调导致学生模型未能继承教师的认知结构，实现负迁移，强调人类认知特性是主动强化的结果而非模仿的产物。

Abstract: Recent Large Reasoning Models trained via reinforcement learning exhibit a "natural" alignment with human cognitive costs. However, we show that the prevailing paradigm of reasoning distillation -- training student models to mimic these traces via Supervised Fine-Tuning (SFT) -- fails to transmit this cognitive structure. Testing the "Hán Dān Xué Bù" (Superficial Mimicry) hypothesis across 14 models, we find that distillation induces a "Functional Alignment Collapse": while teacher models mirror human difficulty scaling ($\bar{r}=0.64$), distilled students significantly degrade this alignment ($\bar{r}=0.34$), often underperforming their own pre-distillation baselines ("Negative Transfer"). Our analysis suggests that SFT induces a "Cargo Cult" effect, where students ritualistically replicate the linguistic form of reasoning (verbosity) without internalizing the teacher's dynamic resource allocation policy. Consequently, reasoning distillation decouples computational cost from cognitive demand, revealing that human-like cognition is an emergent property of active reinforcement, not passive imitation.

</details>


### [82] [ArcAligner: Adaptive Recursive Aligner for Compressed Context Embeddings in RAG](https://arxiv.org/abs/2601.05038)
*Jianbo Li,Yi Jiang,Sendong Zhao,Bairui Hu,Haochun Wang,Bing Qin*

Main category: cs.CL

TL;DR: 本文提出ArcAligner，一种嵌入LLM层的轻量级模块，通过自适应门控机制优化高度压缩上下文的利用，提升多跳和长尾知识密集型问答的生成效果。


<details>
  <summary>Details</summary>
Motivation: 文档压缩虽降低了模型处理成本，但过度压缩导致LLM难以理解信息，影响生成质量。为此需要新的方法帮助模型更好地利用压缩背景。

Method: 引入ArcAligner模块，采用自适应递归对齐机制和门控系统，有选择性地为复杂信息提供额外处理能力，集成于语言模型层中以利用压缩后的上下文。

Result: 在多项知识密集型问答基准测试中，ArcAligner在相似压缩率下优于传统压缩方法，尤其在多跳和长尾设定中表现突出。

Conclusion: ArcAligner有效提升了压缩背景信息下的生成性能，尤其在多跳和长尾任务中表现优越，且保持系统速度快。

Abstract: Retrieval-Augmented Generation (RAG) helps LLMs stay accurate, but feeding long documents into a prompt makes the model slow and expensive. This has motivated context compression, ranging from token pruning and summarization to embedding-based compression. While researchers have tried ''compressing'' these documents into smaller summaries or mathematical embeddings, there is a catch: the more you compress the data, the more the LLM struggles to understand it. To address this challenge, we propose ArcAligner (Adaptive recursive context *Aligner*), a lightweight module integrated into the language model layers to help the model better utilize highly compressed context representations for downstream generation. It uses an adaptive ''gating'' system that only adds extra processing power when the information is complex, keeping the system fast. Across knowledge-intensive QA benchmarks, ArcAligner consistently beats compression baselines at comparable compression rates, especially on multi-hop and long-tail settings. The source code is publicly available.

</details>


### [83] [Compositional Steering of Large Language Models with Steering Tokens](https://arxiv.org/abs/2601.05062)
*Gorjan Radevski,Kiril Gashteovski,Giwon Hong,Carolin Lawrence,Goran Glavaš*

Main category: cs.CL

TL;DR: 提出组合控制标记解决大语言模型的多行为控制问题，实现零样本组合并优于现有方法，且与自然语言指令协同提升效果。


<details>
  <summary>Details</summary>
Motivation: 当前研究多聚焦于对大语言模型（LLM）单一行为的控制，如何同时实现多行为的可控输出（组合行为控制）尚未充分探索。

Method: 提出组合控制标记（compositional steering tokens），通过自蒸馏将自然语言指令嵌入专用标记，控制行为在输入标记空间内进行，训练组合标记捕捉行为组合特征，支持零样本组合及不同数量行为的泛化。

Result: 在不同LLM架构上，组合控制标记在多行为控制上优于指令、激活控制和LoRA合并等方法。组合标记与自然语言指令结合可进一步提升控制效果。

Conclusion: 组合控制标记有效实现了对大语言模型的多行为控制，具有良好的泛化能力和扩展性，是多行为控制的重要突破。

Abstract: Deploying LLMs in real-world applications requires controllable output that satisfies multiple desiderata at the same time. While existing work extensively addresses LLM steering for a single behavior, \textit{compositional steering} -- i.e., steering LLMs simultaneously towards multiple behaviors -- remains an underexplored problem. In this work, we propose \emph{compositional steering tokens} for multi-behavior steering. We first embed individual behaviors, expressed as natural language instructions, into dedicated tokens via self-distillation. Contrary to most prior work, which operates in the activation space, our behavior steers live in the space of input tokens, enabling more effective zero-shot composition. We then train a dedicated \textit{composition token} on pairs of behaviors and show that it successfully captures the notion of composition: it generalizes well to \textit{unseen} compositions, including those with unseen behaviors as well as those with an unseen \textit{number} of behaviors. Our experiments across different LLM architectures show that steering tokens lead to superior multi-behavior control compared to competing approaches (instructions, activation steering, and LoRA merging). Moreover, we show that steering tokens complement natural language instructions, with their combination resulting in further gains.

</details>


### [84] [SemPA: Improving Sentence Embeddings of Large Language Models through Semantic Preference Alignment](https://arxiv.org/abs/2601.05075)
*Ziyang Chen,Zhenxuan Huang,Yile Wang,Weiqin Wang,Lu Yin,Hui Huang*

Main category: cs.CL

TL;DR: 该论文提出SemPA方法，通过语义偏好对齐和直接偏好优化，提升大语言模型的句子表示能力，同时保留其生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于生成式大语言模型的句子嵌入方法要么依赖固定提示模板导致性能有限，要么通过修改模型结构损害生成能力，亟需一种能提升表示且不影响生成的新方法。

Method: 利用句子层面的直接偏好优化（DPO）在复述生成任务上训练大语言模型，使其能区分语义等价句子，同时保留生成能力，并在理论上建立DPO与对比学习的联系。

Result: 在语义文本相似性任务及多项大语言模型基准测试中，SemPA展示了更优的语义表示效果且没有牺牲模型生成能力。

Conclusion: SemPA在提升句子语义表示的同时，不损害大语言模型的生成能力，表现优于传统和现有的生成式句子嵌入方法。

Abstract: Traditional sentence embedding methods employ token-level contrastive learning on non-generative pre-trained models. Recently, there have emerged embedding methods based on generative large language models (LLMs). These methods either rely on fixed prompt templates or involve modifications to the model architecture. The former lacks further optimization of the model and results in limited performance, while the latter alters the internal computational mechanisms of the model, thereby compromising its generative capabilities. We propose SemPA, a novel approach that boosts the sentence representations while preserving the generative ability of LLMs via semantic preference alignment. We leverage sentence-level Direct Preference Optimization (DPO) to efficiently optimize LLMs on a paraphrase generation task, where the model learns to discriminate semantically equivalent sentences while preserving inherent generative capacity. Theoretically, we establish a formal connection between DPO and contrastive learning under the Plackett-Luce model framework. Empirically, experimental results on both semantic textual similarity tasks and various benchmarks for LLMs show that SemPA achieves better semantic representations without sacrificing the inherent generation capability of LLMs.

</details>


### [85] [Code-Mix Sentiment Analysis on Hinglish Tweets](https://arxiv.org/abs/2601.05091)
*Aashi Garg,Aneshya Das,Arshi Arya,Anushka Goyal,Aditi*

Main category: cs.CL

TL;DR: 开发了针对印度混合语言Hinglish的情感分类模型，显著提升品牌舆情分析效果。


<details>
  <summary>Details</summary>
Motivation: 传统NLP模型无法准确处理印度社交媒体上的Hinglish混合语言，导致情感分析结果失真，亟需新的高性能模型以改进品牌舆情监测。

Method: 基于mBERT进行微调，利用subword分词技术处理拼写变体、俚语和词汇外的词语，从而适应Hinglish的代码混合特性。

Result: 提出了一种基于多语言BERT(mBERT)的高效情感分类框架，用于处理和分析印度社交媒体上广泛使用的混合语言“Hinglish”推文，提高了品牌监测的准确性。

Conclusion: 该方法有效理解了Hinglish语言的多样性和复杂性，成功克服了传统单语模型的局限，提供了可生产应用的情感监测工具。

Abstract: The effectiveness of brand monitoring in India is increasingly challenged by the rise of Hinglish--a hybrid of Hindi and English--used widely in user-generated content on platforms like Twitter. Traditional Natural Language Processing (NLP) models, built for monolingual data, often fail to interpret the syntactic and semantic complexity of this code-mixed language, resulting in inaccurate sentiment analysis and misleading market insights. To address this gap, we propose a high-performance sentiment classification framework specifically designed for Hinglish tweets. Our approach fine-tunes mBERT (Multilingual BERT), leveraging its multilingual capabilities to better understand the linguistic diversity of Indian social media. A key component of our methodology is the use of subword tokenization, which enables the model to effectively manage spelling variations, slang, and out-of-vocabulary terms common in Romanized Hinglish. This research delivers a production-ready AI solution for brand sentiment tracking and establishes a strong benchmark for multilingual NLP in low-resource, code-mixed environments.

</details>


### [86] [How Human is AI? Examining the Impact of Emotional Prompts on Artificial and Human and Responsiveness](https://arxiv.org/abs/2601.05104)
*Florence Bernays,Marco Henriques Pereira,Jochen Menges*

Main category: cs.CL

TL;DR: 不同情绪语气影响ChatGPT答复质量和人类后续交流，赞扬提升表现，责备带来负面沟通。


<details>
  <summary>Details</summary>
Motivation: 研究情感语气如何影响人机交互中的ChatGPT和人类行为。

Method: 通过被试间实验，让参与者在与ChatGPT合作完成任务时表达特定情感（如赞扬、愤怒、中立等），并比较不同情绪语气下ChatGPT的表现及后续人际沟通表现。

Result: 赞扬ChatGPT时，其回答质量有明显提升；表达愤怒时回答有所改善但程度较小；责备ChatGPT时回答无改善。愤怒表达减少了ChatGPT对企业利益的偏好，责备则增加其保护公共利益的倾向。责备ChatGPT导致参与者后续人际交往中使用更多负面、敌对和失望的表达。

Conclusion: 情感语气在人与AI交互中不仅影响AI输出，还影响人类之后的人际沟通方式。

Abstract: This research examines how the emotional tone of human-AI interactions shapes ChatGPT and human behavior. In a between-subject experiment, we asked participants to express a specific emotion while working with ChatGPT (GPT-4.0) on two tasks, including writing a public response and addressing an ethical dilemma. We found that compared to interactions where participants maintained a neutral tone, ChatGPT showed greater improvement in its answers when participants praised ChatGPT for its responses. Expressing anger towards ChatGPT also led to a higher albeit smaller improvement relative to the neutral condition, whereas blaming ChatGPT did not improve its answers. When addressing an ethical dilemma, ChatGPT prioritized corporate interests less when participants expressed anger towards it, while blaming increases its emphasis on protecting the public interest. Additionally, we found that people used more negative, hostile, and disappointing expressions in human-human communication after interactions during which participants blamed rather than praised for their responses. Together, our findings demonstrate that the emotional tone people apply in human-AI interactions not only shape ChatGPT's outputs but also carry over into subsequent human-human communication.

</details>


### [87] [Agent-as-a-Judge](https://arxiv.org/abs/2601.05111)
*Runyang You,Hongru Cai,Caiqi Zhang,Qiancheng Xu,Meng Liu,Tiezheng Yu,Yongqi Li,Wenjie Li*

Main category: cs.CL

TL;DR: 本文综述了AI评估领域从单一大型语言模型评判到多智能体协作评判的转变，提出了统一框架和未来路线图。


<details>
  <summary>Details</summary>
Motivation: 随着评估任务变得复杂和专业化，传统LLM单次推理存在偏见和验证困难，促使向具备规划、工具辅助和多智能体协作能力的评判智能体转变。

Method: 通过文献综述，界定关键维度，建立发展分类，组织核心方法和应用，分析挑战及研究前景。

Result: 本文系统回顾了从"LLM作为评判者"到"智能体作为评判者"的演进过程，分析了大型语言模型在AI评估中的局限性，并提出以智能体计划、工具辅助验证、多智能体协作及持久记忆为核心的新一代评估范式。文章构建了一个发展分类法，梳理了相关技术方法和应用，探讨了前沿挑战及未来研究方向。

Conclusion: 本文总结了智能体评判范式的优势和挑战，强调了多维度、多步骤、可验证评估的重要性，并呼吁构建统一框架推动该领域发展。

Abstract: LLM-as-a-Judge has revolutionized AI evaluation by leveraging large language models for scalable assessments. However, as evaluands become increasingly complex, specialized, and multi-step, the reliability of LLM-as-a-Judge has become constrained by inherent biases, shallow single-pass reasoning, and the inability to verify assessments against real-world observations. This has catalyzed the transition to Agent-as-a-Judge, where agentic judges employ planning, tool-augmented verification, multi-agent collaboration, and persistent memory to enable more robust, verifiable, and nuanced evaluations. Despite the rapid proliferation of agentic evaluation systems, the field lacks a unified framework to navigate this shifting landscape. To bridge this gap, we present the first comprehensive survey tracing this evolution. Specifically, we identify key dimensions that characterize this paradigm shift and establish a developmental taxonomy. We organize core methodologies and survey applications across general and professional domains. Furthermore, we analyze frontier challenges and identify promising research directions, ultimately providing a clear roadmap for the next generation of agentic evaluation.

</details>


### [88] [DocDancer: Towards Agentic Document-Grounded Information Seeking](https://arxiv.org/abs/2601.05163)
*Qintong Zhang,Xinjie Lv,Jialong Wu,Baixuan Li,Zhengwei Tao,Guochen Yan,Huanyao Zhang,Bin Wang,Jiahao Xu,Haitao Mi,Wentao Zhang*

Main category: cs.CL

TL;DR: DocDancer通过工具驱动和合成数据提升了文档问答性能，解决训练数据不足难题。


<details>
  <summary>Details</summary>
Motivation: 现有文档问答系统工具利用不足且依赖闭源模型，缺乏高质量训练数据，亟需开源且高效的端到端方案。

Method: 采用工具驱动代理框架明确建模文档探索与理解，设计探索-合成数据生成管线实现端到端训练。

Result: 本文提出了DocDancer，一个端到端训练的开源文档问答代理，通过工具驱动框架和探索-合成数据生成管线解决了文档理解和训练数据不足的问题，在长文本理解基准测试中表现出色。

Conclusion: DocDancer有效提升了文档问答系统的性能，提供了可训练且开源的解决方案，并为工具设计和合成数据生成提供了新见解。

Abstract: Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.

</details>


### [89] [RelayLLM: Efficient Reasoning via Collaborative Decoding](https://arxiv.org/abs/2601.05167)
*Chengsong Huang,Tong Zheng,Langlin Huang,Jinyuan Li,Haolin Liu,Jiaxin Huang*

Main category: cs.CL

TL;DR: 提出RelayLLM，通过token级动态协同调用大模型，实现高效推理，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理计算成本高且延迟大，小模型推理能力不足，现有协同方法粗粒度调用大模型导致计算资源浪费。

Method: 提出了RelayLLM框架，通过在token级别进行协同解码，使小模型动态调用大模型完成关键token的生成。采用两阶段训练，包括预热和组相对策略优化（GRPO），以平衡模型自主生成能力与策略性求助。

Result: RelayLLM在六个基准测试中，准确率达49.52%，仅对1.07%的生成token调用大模型，实现98.2%的成本节省，显著缩小两模型性能差距。

Conclusion: RelayLLM有效提升了小模型的推理能力，极大降低了调用大模型的频率和计算成本，为资源受限环境提供了一种高效合理的推理方案。

Abstract: Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively "relaying" the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM achieves an average accuracy of 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers.

</details>


### [90] [Reverse-engineering NLI: A study of the meta-inferential properties of Natural Language Inference](https://arxiv.org/abs/2601.05170)
*Rasmus Blanck,Bill Noble,Stergios Chatzikyriakidis*

Main category: cs.CL

TL;DR: 本文通过分析SNLI数据集中不同的逻辑标签理解，探讨了NLI任务的逻辑性质，揭示了模型在该任务上的元推理一致性。


<details>
  <summary>Details</summary>
Motivation: 当前对NLI任务的逻辑性质理解不足，错误解读影响模型性能解释，因此需要明确NLI标签的逻辑含义。

Method: 通过定义三种对NLI标注集的理解方式，利用共享前提的NLI样本和由大型语言模型生成的数据，分析了SNLI数据集上的模型元推理一致性。

Result: 发现不同理解方式对应的元推理性质各异，确定了SNLI数据集主要编码的逻辑关系类型，提升了对模型推理能力的认知。

Conclusion: 明确了NLI任务标签的逻辑含义，有助于更准确地评估和解释自然语言推理模型的推理能力。

Abstract: Natural Language Inference (NLI) has been an important task for evaluating language models for Natural Language Understanding, but the logical properties of the task are poorly understood and often mischaracterized. Understanding the notion of inference captured by NLI is key to interpreting model performance on the task. In this paper we formulate three possible readings of the NLI label set and perform a comprehensive analysis of the meta-inferential properties they entail. Focusing on the SNLI dataset, we exploit (1) NLI items with shared premises and (2) items generated by LLMs to evaluate models trained on SNLI for meta-inferential consistency and derive insights into which reading of the logical relations is encoded by the dataset.

</details>


### [91] [Inside Out: Evolving User-Centric Core Memory Trees for Long-Term Personalized Dialogue Systems](https://arxiv.org/abs/2601.05171)
*Jihao Zhao,Ding Chen,Zhaoxin Fan,Kerun Xu,Mengting Hu,Bo Tang,Feiyu Xiong,Zhiyu li*

Main category: cs.CL

TL;DR: 提出PersonaTree结构和MemListener模型，通过结构化记忆管理实现长期个性化对话，提升了记忆压缩效果和人物一致性，且推理性能优于多种先进模型。


<details>
  <summary>Details</summary>
Motivation: 现有长期个性化对话系统难以处理无限制的交互流与有限的上下文记忆，导致记忆噪声积累、推理能力下降和人物角色不一致，急需一种有效的结构化记忆管理方法以提升系统的长期可靠性和一致性。

Method: 该方法利用一个初始结构化的PersonaTree来管理用户画像，使用强化学习训练的小型MemListener模型生成四类操作（新增、更新、删除、无操作）以动态维护树结构。在响应生成时直接调用PersonaTree提升输出质量，并通过代理模式在需要时详细补充内容。

Result: 该论文提出了一种名为Inside Out的框架，利用全局维护的PersonaTree实现长期用户画像管理，以解决现有长期个性化对话系统在有限上下文约束下对话记忆噪声积累、推理能力下降及人物一致性问题。通过初始的主干结构约束和动态更新树的分支与叶子节点，PersonaTree实现了可控增长和记忆压缩，同时保证了画像的一致性。作者还训练了一个轻量级的MemListener模型，利用带有过程奖励的强化学习，生成结构化且可执行的操作（新增、更新、删除、无操作），支持个性化树的动态演进。响应生成时，PersonaTree直接提升了输出质量，在延迟敏感场景表现优异；当用户需要更多细节时，通过代理模式在PersonaTree限制下按需引入细节。实验结果表明，PersonaTree优于全文拼接和多种个性化记忆系统，在抑制上下文噪声和保持人物一致性方面表现突出。MemListener模型的记忆操作决策性能甚至超过了诸如DeepSeek-R1-0528和Gemini-3-Pro等强大推理模型。

Conclusion: PersonaTree结合强化学习训练的MemListener能有效管理长期个性化对话记忆，显著抑制噪声并保持人物一致性，超越了现有的全文本拼接及多种个性化记忆系统，为个性化对话系统提供了新的解决方案。

Abstract: Existing long-term personalized dialogue systems struggle to reconcile unbounded interaction streams with finite context constraints, often succumbing to memory noise accumulation, reasoning degradation, and persona inconsistency. To address these challenges, this paper proposes Inside Out, a framework that utilizes a globally maintained PersonaTree as the carrier of long-term user profiling. By constraining the trunk with an initial schema and updating the branches and leaves, PersonaTree enables controllable growth, achieving memory compression while preserving consistency. Moreover, we train a lightweight MemListener via reinforcement learning with process-based rewards to produce structured, executable, and interpretable {ADD, UPDATE, DELETE, NO_OP} operations, thereby supporting the dynamic evolution of the personalized tree. During response generation, PersonaTree is directly leveraged to enhance outputs in latency-sensitive scenarios; when users require more details, the agentic mode is triggered to introduce details on-demand under the constraints of the PersonaTree. Experiments show that PersonaTree outperforms full-text concatenation and various personalized memory systems in suppressing contextual noise and maintaining persona consistency. Notably, the small MemListener model achieves memory-operation decision performance comparable to, or even surpassing, powerful reasoning models such as DeepSeek-R1-0528 and Gemini-3-Pro.

</details>


### [92] [LELA: an LLM-based Entity Linking Approach with Zero-Shot Domain Adaptation](https://arxiv.org/abs/2601.05192)
*Samy Haffoudhi,Fabian M. Suchanek,Nils Holzenberger*

Main category: cs.CL

TL;DR: LELA方法利用大语言模型实现无需微调的高效实体链接，适用广泛，性能优异。


<details>
  <summary>Details</summary>
Motivation: 实体链接是知识图谱构建、问答和信息提取等任务的基础步骤，但现有方法多需微调，且难以适应不同领域和知识库。

Method: LELA采用模块化粗到细策略，利用大语言模型能力进行实体链接，无需微调，支持多目标领域和知识库。

Result: 提出了LELA，一种模块化的粗到细方法，利用大语言模型能力无需微调即可适应多领域、多知识库和不同大语言模型，在多种实体链接场景下表现优异，超越了未微调方法，竞争力媲美微调方法。

Conclusion: LELA证明了无需微调即可通过大语言模型完成高质量实体链接，具备广泛的适用性和竞争力。

Abstract: Entity linking (mapping ambiguous mentions in text to entities in a knowledge base) is a foundational step in tasks such as knowledge graph construction, question-answering, and information extraction. Our method, LELA, is a modular coarse-to-fine approach that leverages the capabilities of large language models (LLMs), and works with different target domains, knowledge bases and LLMs, without any fine-tuning phase. Our experiments across various entity linking settings show that LELA is highly competitive with fine-tuned approaches, and substantially outperforms the non-fine-tuned ones.

</details>


### [93] [Measuring and Fostering Peace through Machine Learning and Artificial Intelligence](https://arxiv.org/abs/2601.05232)
*P. Gilda,P. Dungarwal,A. Thongkham,E. T. Ajayi,S. Choudhary,T. M. Terol,C. Lam,J. P. Araujo,M. McFadyen-Mungalln,L. S. Liebovitch,P. T. Coleman,H. West,K. Sieck,S. Carter*

Main category: cs.CL

TL;DR: 本文通过机器学习技术测量新闻与社交媒体中的和平度，并开发工具帮助用户识别和改善其媒体消费的和平性，从而促进更尊重和有深度的交流。


<details>
  <summary>Details</summary>
Motivation: 帮助用户更好地理解自己的媒体信息摄取，促进和平，减少情绪激烈和偏激内容的影响。

Method: 利用机器学习和人工智能，基于新闻文本嵌入和社交媒体内容，使用神经网络和大规模语言模型测量和平水平。

Result: 构建了高准确度的模型测量新闻和社交媒体中的和平相关维度，开发了实时反馈YouTube观众媒体和平度的Chrome插件MirrorMirror。

Conclusion: MirrorMirror具有潜力成为促进内容创作者和用户理解媒体语气及其影响的开源工具，推动从单纯的参与指标转向更具建设性的沟通方式。

Abstract: We used machine learning and artificial intelligence: 1) to measure levels of peace in countries from news and social media and 2) to develop on-line tools that promote peace by helping users better understand their own media diet. For news media, we used neural networks to measure levels of peace from text embeddings of on-line news sources. The model, trained on one news media dataset also showed high accuracy when used to analyze a different news dataset. For social media, such as YouTube, we developed other models to measure levels of social dimensions important in peace using word level (GoEmotions) and context level (Large Language Model) methods. To promote peace, we note that 71% of people 20-40 years old daily view most of their news through short videos on social media. Content creators of these videos are biased towards creating videos with emotional activation, making you angry to engage you, to increase clicks. We developed and tested a Chrome extension, MirrorMirror, which provides real-time feedback to YouTube viewers about the peacefulness of the media they are watching. Our long term goal is for MirrorMirror to evolve into an open-source tool for content creators, journalists, researchers, platforms, and individual users to better understand the tone of their media creation and consumption and its effects on viewers. Moving beyond simple engagement metrics, we hope to encourage more respectful, nuanced, and informative communication.

</details>


### [94] [GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization](https://arxiv.org/abs/2601.05242)
*Shih-Yang Liu,Xin Dong,Ximing Lu,Shizhe Diao,Peter Belcak,Mingjie Liu,Min-Hung Chen,Hongxu Yin,Yu-Chiang Frank Wang,Kwang-Ting Cheng,Yejin Choi,Jan Kautz,Pavlo Molchanov*

Main category: cs.CL

TL;DR: 提出GDPO方法解决多奖励强化学习中GRPO归一化导致训练信号丢失的问题，提升训练准确性和稳定性，在多个任务上表现优于GRPO。


<details>
  <summary>Details</summary>
Motivation: 用户期望语言模型不仅给出准确回应，还能满足多样化的人类偏好，因此多奖励强化学习成为关键，但现有GRPO方法在多奖励归一化时存在缺陷，影响训练效果，需提出改进方案。

Method: GDPO通过对每个奖励信号进行独立归一化，避免不同奖励组合的优势值坍缩，保持奖励间的相对差异，提高多奖励强化学习的优化精度和训练稳定性。

Result: 本文针对多奖励强化学习中常用的Group Relative Policy Optimization (GRPO)方法进行分析，指出其在多奖励归一化时导致优势值相同，降低训练信号分辨率，影响收敛效果和训练稳定性。提出了Group reward-Decoupled Normalization Policy Optimization (GDPO)方法，通过分离归一化各奖励值，保持奖励间的相对差异，从而提升训练的准确性和稳定性。在工具调用、数学推理和代码推理三项任务中，GDPO在准确率、错误率和约束符合度等指标上均优于GRPO，显示出更好的效果和泛化能力。

Conclusion: GDPO通过奖赏值分离归一化，有效提升了多奖励强化学习的训练信号质量和稳定性，促进了模型在多场景下的效果优化，优于传统GRPO方法。

Abstract: As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [95] [AI Agents as Policymakers in Simulated Epidemics](https://arxiv.org/abs/2601.04245)
*Goshi Aoki,Navid Ghaffarzadegan*

Main category: cs.MA

TL;DR: 此论文构建了一个基于生成式AI的流行病决策代理，展示了其在动态环境下做出类人政策决策的能力，强调了理论引导提示在提升AI决策中的作用。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI代理作为计算模型在复杂决策中的潜力，特别是流行病期间的政策制定。

Method: 设计一个生成式AI代理，扮演城市市长，在SEIR模拟环境中基于流行病信息进行隔周决策。代理具备动态记忆，权衡最近事件，并在多种环境和多代理设置中测试。

Result: AI代理表现出类似人类的反应行为，根据病例变化调整限制政策。向代理提供流行病动力学的系统性知识显著提升决策质量和稳定性。

Conclusion: 通过理论指导的提示，生成式AI代理能作为复杂社会系统中决策和政策设计的强大计算模型。

Abstract: AI agents are increasingly deployed as quasi-autonomous systems for specialized tasks, yet their potential as computational models of decision-making remains underexplored. We develop a generative AI agent to study repetitive policy decisions during an epidemic, embedding the agent, prompted to act as a city mayor, within a simulated SEIR environment. Each week, the agent receives updated epidemiological information, evaluates the evolving situation, and sets business restriction levels. The agent is equipped with a dynamic memory that weights past events by recency and is evaluated in both single- and ensemble-agent settings across environments of varying complexity. Across scenarios, the agent exhibits human-like reactive behavior, tightening restrictions in response to rising cases and relaxing them as risk declines. Crucially, providing the agent with brief systems-level knowledge of epidemic dynamics, highlighting feedbacks between disease spread and behavioral responses, substantially improves decision quality and stability. The results illustrate how theory-informed prompting can shape emergent policy behavior in AI agents. These findings demonstrate that generative AI agents, when situated in structured environments and guided by minimal domain theory, can serve as powerful computational models for studying decision-making and policy design in complex social systems.

</details>


### [96] [From Idea to Co-Creation: A Planner-Actor-Critic Framework for Agent Augmented 3D Modeling](https://arxiv.org/abs/2601.05016)
*Jin Gao,Saichandu Juluri*

Main category: cs.MA

TL;DR: 通过多代理自我反思与人类监管，提升了3D建模的精度与质量，优于传统单提示方法。


<details>
  <summary>Details</summary>
Motivation: 现有的3D建模方法多依赖单一提示的代理，执行命令时效果有限，难以保证模型的几何精度和美学质量。

Method: 设计了包含规划者、执行者和评估者的多代理架构，执行过程中结合批评者反馈和人类监督进行迭代改进。

Result: 引入多代理的Planner-Actor-Critic架构，并结合人类监督，实现了更高的建模准确性、美学质量和任务完成率。

Conclusion: 结构化的代理自我反思加上人类指导，能显著提升3D模型质量并保持高效的工作流程。

Abstract: We present a framework that extends the Actor-Critic architecture to creative 3D modeling through multi-agent self-reflection and human-in-the-loop supervision. While existing approaches rely on single-prompt agents that directly execute modeling commands via tools like Blender MCP, our approach introduces a Planner-Actor-Critic architecture. In this design, the Planner coordinates modeling steps, the Actor executes them, and the Critic provides iterative feedback, while human users act as supervisors and advisors throughout the process. Through systematic comparison between single-prompt modeling and our reflective multi-agent approach, we demonstrate improvements in geometric accuracy, aesthetic quality, and task completion rates across diverse 3D modeling scenarios. Our evaluation reveals that critic-guided reflection, combined with human supervisory input, reduces modeling errors and increases complexity and quality of the result compared to direct single-prompt execution. This work establishes that structured agent self-reflection, when augmented by human oversight and advisory guidance, produces higher-quality 3D models while maintaining efficient workflow integration through real-time Blender synchronization.

</details>


### [97] [FinDeepForecast: A Live Multi-Agent System for Benchmarking Deep Research Agents in Financial Forecasting](https://arxiv.org/abs/2601.05039)
*Xiangyu Li,Xuan Yao,Guohao Qi,Fengbin Zhu,Kelvin J. L. Koa,Xiang Yao Ng,Ziyang Liu,Xingyu Ni,Chang Liu,Yonghui Yang,Yang Zhang,Wenjie Wang,Fuli Feng,Chao Wang,Huanbo Luan,Xiaofen Xing,Xiangmin Xu,Tat-Seng Chua,Ke-Wei Huang*

Main category: cs.MA

TL;DR: 本文提出了FinDeepForecast，一个针对金融预测任务的多代理动态评估系统及基准，揭示了现有DR代理的优势与不足，推动未来金融领域研究代理的提升。


<details>
  <summary>Details</summary>
Motivation: 当前基于大型语言模型的深度研究（DR）代理在完成复杂研究任务方面表现突出，但其在高风险金融领域真实研究任务的预测表现未被充分评估。

Method: 引入FinDeepForecast系统，该系统通过双轨分类法动态生成企业及宏观层面的周期性和非周期性金融预测任务，构建了FinDeepForecastBench基准库，涵盖十周全球8个经济体和1314家公司，并评估了13种方法。

Result: 实验证明，DR代理虽优于强基线方法，但在前瞻性金融推理方面仍存在不足。

Conclusion: FinDeepForecast系统及基准为研究型金融预测任务提供持续评估平台，预计将促进DR代理在金融前瞻性推理方面的改进与发展。

Abstract: Deep Research (DR) Agents powered by advanced Large Language Models (LLMs) have fundamentally shifted the paradigm for completing complex research tasks. Yet, a comprehensive and live evaluation of their forecasting performance on real-world, research-oriented tasks in high-stakes domains (e.g., finance) remains underexplored. We introduce FinDeepForecast, the first live, end-to-end multi-agent system for automatically evaluating DR agents by continuously generating research-oriented financial forecasting tasks. This system is equipped with a dual-track taxonomy, enabling the dynamic generation of recurrent and non-recurrent forecasting tasks at both corporate and macro levels. With this system, we generate FinDeepForecastBench, a weekly evaluation benchmark over a ten-week horizon, encompassing 8 global economies and 1,314 listed companies, and evaluate 13 representative methods. Extensive experiments show that, while DR agents consistently outperform strong baselines, their performance still falls short of genuine forward-looking financial reasoning. We expect the proposed FinDeepForecast system to consistently facilitate future advancements of DR agents in research-oriented financial forecasting tasks. The benchmark and leaderboard are publicly available on the OpenFinArena Platform.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [98] [Sphinx: Benchmarking and Modeling for LLM-Driven Pull Request Review](https://arxiv.org/abs/2601.04252)
*Daoan Zhang,Shuo Zhang,Zijian Jin,Jiebo Luo,Shengyu Fu,Elsie Nallipogu*

Main category: cs.SE

TL;DR: 该论文提出了Sphinx框架，通过结构化数据生成、基于清单的评估和奖赏优化，显著提升了自动化拉取请求审核的准确性和完整性。


<details>
  <summary>Details</summary>
Motivation: 自动化拉取请求审核面临监督噪声大、上下文理解不足和评价指标不完善的挑战。

Method: 采用结构化数据生成管道生成语义丰富的审核评论，基于清单的评价基准，以及Checklist Reward Policy Optimization训练范式。

Result: 经实验证明，Sphinx训练的模型在审核完整性和准确性上明显优于现有专有及开源模型。

Conclusion: Sphinx框架在拉取请求审核的完整性和精确度上实现了最先进的表现，清单覆盖率比现有方法提升了40%。

Abstract: Pull request (PR) review is essential for ensuring software quality, yet automating this task remains challenging due to noisy supervision, limited contextual understanding, and inadequate evaluation metrics. We present Sphinx, a unified framework for LLM-based PR review that addresses these limitations through three key components: (1) a structured data generation pipeline that produces context-rich, semantically grounded review comments by comparing pseudo-modified and merged code; (2) a checklist-based evaluation benchmark that assesses review quality based on structured coverage of actionable verification points, moving beyond surface-level metrics like BLEU; and (3) Checklist Reward Policy Optimization (CRPO), a novel training paradigm that uses rule-based, interpretable rewards to align model behavior with real-world review practices. Extensive experiments show that models trained with Sphinx achieve state-of-the-art performance on review completeness and precision, outperforming both proprietary and open-source baselines by up to 40\% in checklist coverage. Together, Sphinx enables the development of PR review models that are not only fluent but also context-aware, technically precise, and practically deployable in real-world development workflows. The data will be released after review.

</details>


### [99] [A Systematic Mapping Study on the Debugging of Autonomous Driving Systems](https://arxiv.org/abs/2601.04293)
*Nathan Shaw,Sanjeetha Pennada,Robert M Hierons,Donghwan Shin*

Main category: cs.SE

TL;DR: 本文通过系统映射研究梳理了自动驾驶系统调试的现状，发现该领域方法多样但分散，提出未来研究和标准化建议，以促进该领域发展。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统（ADS）在向商业部署推进过程中，确保其安全性和可靠性变得尤为重要。但现有研究较多关注测试方法以检测故障，却很少关注故障检测后的调试过程。调试对于定位和修复系统中的故障至关重要，以维护系统的安全性和可靠性。

Method: 通过系统映射研究（Systematic Mapping Study，SMS）对现有ADS调试方法进行全面调研和分类，分析当前研究现状，识别研究空白，并提出未来研究方向及术语和问题定义标准。

Result: 研究揭示了多种ADS调试方法，显示当前ADS调试领域方法分散但具有较大潜力。

Conclusion: ADS调试领域虽尚未形成统一体系，但已有多样化方法，为提升自动驾驶系统的安全性和可靠性提供了基础。未来需进一步规范标准并拓展研究。

Abstract: As Autonomous Driving Systems (ADS) progress towards commercial deployment, there is an increasing focus on ensuring their safety and reliability. While considerable research has been conducted on testing methods for detecting faults in ADS, very little attention has been paid to debugging in ADS. Debugging is an essential process that follows test failures to localise and repair the faults in the systems to maintain their safety and reliability. This Systematic Mapping Study (SMS) aims to provide a detailed overview of the current landscape of ADS debugging, highlighting existing approaches and identifying gaps in research. The study also proposes directions for future work and standards for problem definition and terminology in the field. Our findings reveal various methods for ADS debugging and highlight the current fragmented yet promising landscape.

</details>


### [100] [Advancing Language Models for Code-related Tasks](https://arxiv.org/abs/2601.04526)
*Zhao Tian*

Main category: cs.SE

TL;DR: 本文通过改进数据质量、模型结构和推理技术，增强语言模型在复杂软件开发任务中的能力，推动智能软件工程发展。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在数据质量、模型结构和推理能力方面存在局限，难以应对复杂编程任务，亟需改进以推动智能软件工程的发展。

Method: 采用代码差异引导的对抗增强技术(CODA)、代码去噪技术(CodeDenoise)，语法引导的代码语言模型(LEAM与LEAM++)，以及提示技术(muFiX)和基于代理的技术(Specine)三方面方法系统提升模型性能。

Result: 方法有效提升了代码数据质量、模型架构的语法理解能力和推理能力，促进了语言模型在软件开发中的实用采纳。

Conclusion: 本文提出的方法解决了现有语言模型在复杂编程场景中的不足，显著提升了模型在软件工程任务中的表现和实用性。

Abstract: Recent advances in language models (LMs) have driven significant progress in various software engineering tasks. However, existing LMs still struggle with complex programming scenarios due to limitations in data quality, model architecture, and reasoning capability. This research systematically addresses these challenges through three complementary directions: (1) improving code data quality with a code difference-guided adversarial augmentation technique (CODA) and a code denoising technique (CodeDenoise); (2) enhancing model architecture via syntax-guided code LMs (LEAM and LEAM++); and (3) advancing model reasoning with a prompting technique (muFiX) and an agent-based technique (Specine). These techniques aim to promote the practical adoption of LMs in software development and further advance intelligent software engineering.

</details>


### [101] [AdaptEval: A Benchmark for Evaluating Large Language Models on Code Snippet Adaptation](https://arxiv.org/abs/2601.04540)
*Tanghaoran Zhang,Xinjun Mao,Shangwen Wang,Yuxin Zhao,Yao Lu,Jin Zhang,Zhang Zhang,Kang Yang,Yue Yu*

Main category: cs.SE

TL;DR: 该论文提出了AdaptEval，一个专门评估大语言模型（LLMs）在代码片段适配任务中的基准测试框架，填补了此领域无评测标准的空白。


<details>
  <summary>Details</summary>
Motivation: 当前在代码复用过程中，适配是关键步骤，但缺乏针对适配任务的LLMs性能评测基准，影响了其实用性的了解。

Method: Methed在于构建具有实际上下文的多粒度标注数据集，并设计包含适配级和功能级测试的双层评测框架，对LLMs进行全面的能力评估。

Result: 通过AdaptEval评测，发现现有LLMs尚未完全掌握代码片段适配，尤其在遵守指令细节方面存在明显局限。

Conclusion: AdaptEval有效评估了六个指令调优和三个推理型LLMs在代码适配上的表现，揭示了模型在遵循明确指令方面的不足，为未来改进提供了方向。

Abstract: Recent advancements in large language models (LLMs) have automated various software engineering tasks, with benchmarks emerging to evaluate their capabilities. However, for adaptation, a critical activity during code reuse, there is no benchmark to assess LLMs' performance, leaving their practical utility in this area unclear. To fill this gap, we propose AdaptEval, a benchmark designed to evaluate LLMs on code snippet adaptation. Unlike existing benchmarks, AdaptEval incorporates the following three distinctive features: First, Practical Context. Tasks in AdaptEval are derived from developers' practices, preserving rich contextual information from Stack Overflow and GitHub communities. Second, Multi-granularity Annotation. Each task is annotated with requirements at both task and adaptation levels, supporting the evaluation of LLMs across diverse adaptation scenarios. Third, Fine-grained Evaluation. AdaptEval includes a two-tier testing framework combining adaptation-level and function-level tests, which enables evaluating LLMs' performance across various individual adaptations. Based on AdaptEval, we conduct the first empirical study to evaluate six instruction-tuned LLMs and especially three reasoning LLMs on code snippet adaptation. Experimental results demonstrate that AdaptEval enables the assessment of LLMs' adaptation capabilities from various perspectives. It also provides critical insights into their current limitations, particularly their struggle to follow explicit instructions. We hope AdaptEval can facilitate further investigation and enhancement of LLMs' capabilities in code snippet adaptation, supporting their real-world applications.

</details>


### [102] [4D-ARE: Bridging the Attribution Gap in LLM Agent Requirements Engineering](https://arxiv.org/abs/2601.04556)
*Bo Yu,Lei Zhao*

Main category: cs.SE

TL;DR: 本文提出4D-ARE方法论，规范LLM代理应推理的归因知识维度，弥补了现有推理框架只关注推理方法的不足，并在金融领域进行了初步验证。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理虽推理能力强，但缺乏设计阶段的领域知识明确，导致推理虽“会”做但不知道“该做什么”，决策者需要的是归因解释而非简单答案，因此提出4D-ARE方法填补这一空白。

Method: 基于Pearl的因果层次，设计四维归因框架（结果、过程、支持、长期），通过五个层次的工件生成系统提示，实现归因驱动的代理需求工程；并通过金融行业工业试点验证该方法。

Result: 本文提出了4D-ARE方法论，解决了大语言模型（LLM）代理在推理过程中缺乏明确领域知识指定的问题，提升了代理的归因能力。4D-ARE通过四维归因框架（结果、过程、支持、长期）结合Pearl因果层次理论，明确决策者所需的归因信息，辅助设计决策代理应推理的内容。工业金融服务的初步部署验证了该方法的实用性。

Conclusion: 4D-ARE方法有效解决了代理设计阶段领域知识缺失问题，明确了代理推理的归因目标，提升了系统实际应用中的解释能力，未来将进行更严格的实证评估。

Abstract: We deployed an LLM agent with ReAct reasoning and full data access. It executed flawlessly, yet when asked "Why is completion rate 80%?", it returned metrics instead of causal explanation. The agent knew how to reason but we had not specified what to reason about. This reflects a gap: runtime reasoning frameworks (ReAct, Chain-of-Thought) have transformed LLM agents, but design-time specification--determining what domain knowledge agents need--remains under-explored. We propose 4D-ARE (4-Dimensional Attribution-Driven Agent Requirements Engineering), a preliminary methodology for specifying attribution-driven agents. The core insight: decision-makers seek attribution, not answers. Attribution concerns organize into four dimensions (Results -> Process -> Support -> Long-term), motivated by Pearl's causal hierarchy. The framework operationalizes through five layers producing artifacts that compile directly to system prompts. We demonstrate the methodology through an industrial pilot deployment in financial services. 4D-ARE addresses what agents should reason about, complementing runtime frameworks that address how. We hypothesize systematic specification amplifies the power of these foundational advances. This paper presents a methodological proposal with preliminary industrial validation; rigorous empirical evaluation is planned for future work.

</details>


### [103] [Extending Delta Debugging Minimization for Spectrum-Based Fault Localization](https://arxiv.org/abs/2601.04689)
*Charaka Geethal Kapugama*

Main category: cs.SE

TL;DR: 结合DDMIN和SBFL提出DDMIN-LOC，仅用一个失败字符串输入即可准确定位代码缺陷，显著降低排查范围。


<details>
  <summary>Details</summary>
Motivation: 传统的DDMIN虽然能缩小导致失败的输入，但不能定位具体的故障代码行。

Method: 提出DDMIN-LOC，将DDMIN与频谱故障定位结合，通过分析通过和失败的输入计算语句可疑度，排序定位故障。

Result: DDMIN-LOC使用Jaccard算法表现最佳，定位精度高，多数情况下需要检查的代码不到20%，故障语句排名靠前。

Conclusion: DDMIN-LOC能仅基于单个失败字符串输入实现有效的故障定位，提升了单输入调试的效率和准确性。

Abstract: This paper introduces DDMIN-LOC, a technique that combines Delta Debugging Minimization (DDMIN) with Spectrum-Based Fault Localization (SBFL). It can be applied to programs taking string inputs, even when only a single failure-inducing input is available. DDMIN is an algorithm that systematically explores the minimal failure-inducing input that exposes a bug, given an initial failing input. However, it does not provide information about the faulty statements responsible for the failure. DDMIN-LOC addresses this limitation by collecting the passing and failing inputs generated during the DDMIN process and computing suspiciousness scores for program statements and predicates using SBFL algorithms. These scores are then combined to rank statements according to their likelihood of being faulty. DDMIN-LOC requires only one failing input of the buggy program, although it can be applied only to programs that take string inputs. DDMIN-LOC was evaluated on 136 programs selected from the QuixBugs and Codeflaws benchmarks using the SBFL algorithms Tarantula, Ochiai, GenProg, Jaccard and DStar. Experimental results show that DDMIN-LOC performs best with Jaccard: in most subjects, fewer than 20% executable lines need to be examined to locate the faulty statements. Moreover, in most subjects, faulty statements are ranked within the top 3 positions in all the generated test suites derived from different failing inputs.

</details>


### [104] [A Longitudinal Analysis of Gamification in Untappd: Ethical Reflections on a Social Drinking Application](https://arxiv.org/abs/2601.04841)
*Jefferson Seide Molléri,Sami Hyrynsalmi,Antti Hakkala,Kai K. Kimppa,Jouni Smed*

Main category: cs.SE

TL;DR: 本文通过长时段伦理分析，发现社交饮酒应用Untappd的游戏化设计仍存在伦理问题，呼吁在软件开发中持续关注伦理以保护用户。


<details>
  <summary>Details</summary>
Motivation: 随着时间推移，社交饮酒应用Untappd的游戏化设计可能带来伦理风险，需要重新评估其影响。

Method: 基于2020年探究性研究，2025年重新访问平台，结合伦理理论和软件工程框架，分析五类徽章及其对用户自主性和福祉的影响。

Result: 尽管进行了少量调整和表面免责声明，许多原有伦理问题依然存在。

Conclusion: 需要将持续的伦理反思嵌入软件生命周期，以防止设计中风险行为的常态化。

Abstract: This paper presents a longitudinal ethical analysis of Untappd, a social drinking application that gamifies beer consumption through badges, streaks, and social sharing. Building on an exploratory study conducted in 2020, we revisit the platform in 2025 to examine how its gamification features and ethical framings have evolved. Drawing on traditional ethical theory and practical frameworks for Software Engineering, we analyze five categories of badges and their implications for user autonomy and well-being. Our findings show that, despite small adjustments and superficial disclaimers, many of the original ethical issues remain. We argue for continuous ethical reflection built embedded into software lifecycles to prevent the normalization of risky behaviors through design.

</details>


### [105] [Analyzing Message-Code Inconsistency in AI Coding Agent-Authored Pull Requests](https://arxiv.org/abs/2601.04886)
*Jingzhi Gong,Giovanni Pinna,Yixin Bian,Jie M. Zhang*

Main category: cs.SE

TL;DR: AI生成的PR描述与代码变更常存在不一致，影响了人类对AI的信任，需改进验证机制。


<details>
  <summary>Details</summary>
Motivation: 研究AI生成的拉取请求（PR）描述与实际代码变更之间的一致性，评估其可信度。

Method: 采集23,247个AI生成的PR，手工标注974个PR，定义并识别八种PR消息-代码不一致类型，通过统计分析评估影响。

Result: 分析了23,247个由五个AI代理生成的PR，发现1.7%的PR存在高度不一致，描述中声称未实现的变更最常见（45.4%）。高不一致PR的接受率显著降低51.7%，合并时间也延长3.5倍。

Conclusion: 不可靠的PR描述破坏了人们对AI代理的信任，必须开发PR-MCI验证机制并提升PR生成质量。

Abstract: Pull request (PR) descriptions generated by AI coding agents are the primary channel for communicating code changes to human reviewers. However, the alignment between these messages and the actual changes remains unexplored, raising concerns about the trustworthiness of AI agents. To fill this gap, we analyzed 23,247 agentic PRs across five agents using PR message-code inconsistency (PR-MCI). We contributed 974 manually annotated PRs, found 406 PRs (1.7%) exhibited high PR-MCI, and identified eight PR-MCI types, revealing that descriptions claiming unimplemented changes was the most common issue (45.4%). Statistical tests confirmed that high-MCI PRs had 51.7% lower acceptance rates (28.3% vs. 80.0%) and took 3.5x longer to merge (55.8 vs. 16.0 hours). Our findings suggest that unreliable PR descriptions undermine trust in AI agents, highlighting the need for PR-MCI verification mechanisms and improved PR generation to enable trustworthy human-AI collaboration.

</details>


### [106] [AVX / NEON Intrinsic Functions: When Should They Be Used?](https://arxiv.org/abs/2601.04922)
*Théo Boivin,Joeffrey Legaux*

Main category: cs.SE

TL;DR: 基准测试显示AVX/NEON intrinsic函数在条件分支中非常高效，但编译器的自动向量化已能满足多数场景，指导开发者根据环境选择使用。


<details>
  <summary>Details</summary>
Motivation: 为了指导开发者在开发项目中选择使用intrinsic函数，尤其是在需要向量化策略以优化代码时，探索AVX/NEON intrinsic函数的能力和限制。

Method: 提出了一个跨配置的基准测试，通过比较不同操作系统、架构及编译器环境下intrinsic函数的性能表现。

Result: 发现intrinsic函数在条件分支中效率极高，执行时间约为普通代码的5%，但在很多情况下，编译器已经能很好地自动向量化，使用intrinsic函数并不必要。

Conclusion: intrinsic函数在某些情况下能够显著提升性能，尤其是条件分支，但在许多情况下编译器的自动向量化已足够，开发者应根据具体环境选择是否使用intrinsic函数。

Abstract: A cross-configuration benchmark is proposed to explore the capacities and limitations of AVX / NEON intrinsic functions in a generic context of development project, when a vectorisation strategy is required to optimise the code. The main aim is to guide developers to choose when using intrinsic functions, depending on the OS, architecture and/or available compiler. Intrinsic functions were observed highly efficient in conditional branching, with intrinsic version execution time reaching around 5% of plain code execution time. However, intrinsic functions were observed as unnecessary in many cases, as the compilers already well auto-vectorise the code.

</details>
