<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 31]
- [cs.MA](#cs.MA) [Total: 4]
- [cs.SE](#cs.SE) [Total: 11]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [What Kind of Reasoning (if any) is an LLM actually doing? On the Stochastic Nature and Abductive Appearance of Large Language Models](https://arxiv.org/abs/2512.10080)
*Luciano Floridi,Jessica Morley,Claudio Novelli,David Watson*

Main category: cs.CL

TL;DR: 本文分析了当前LLM虽然表现出溯因推理特征，但实际基于概率模式生成文本，不能进行真实推理，需谨慎使用和评估。


<details>
  <summary>Details</summary>
Motivation: 探讨现有LLM的推理能力本质及其与人类推理的关系，澄清误解并指导其正确应用。

Method: 分析LLM的生成机制，比较其与人类溯因推理的异同，结合示例展示LLM的生成特性和表现。

Result: 发现LLM基于概率生成文本，能模仿推理结构但不具备验证和理解能力，其输出虽有辅助作用但不能保证真实性。

Conclusion: 当前的LLM通过学习文本模式生成输出，虽表现出类似溯因推理的特征，但实际上并未进行真实的溯因推理，因此其输出需谨慎评估。

Abstract: This article looks at how reasoning works in current Large Language Models (LLMs) that function using the token-completion method. It examines their stochastic nature and their similarity to human abductive reasoning. The argument is that these LLMs create text based on learned patterns rather than performing actual abductive reasoning. When their output seems abductive, this is largely because they are trained on human-generated texts that include reasoning structures. Examples are used to show how LLMs can produce plausible ideas, mimic commonsense reasoning, and give explanatory answers without being grounded in truth, semantics, verification, or understanding, and without performing any real abductive reasoning. This dual nature, where the models have a stochastic base but appear abductive in use, has important consequences for how LLMs are evaluated and applied. They can assist with generating ideas and supporting human thinking, but their outputs must be critically assessed because they cannot identify truth or verify their explanations. The article concludes by addressing five objections to these points, noting some limitations in the analysis, and offering an overall evaluation.

</details>


### [2] [Generate-Then-Validate: A Novel Question Generation Approach Using Small Language Models](https://arxiv.org/abs/2512.10110)
*Yumou Wei,John Stamper,Paulo F. Carvalho*

Main category: cs.CL

TL;DR: 本文提出一种结合文本生成与概率推理的新流程，验证了小型语言模型在自动问题生成中的有效性和高质量表现。


<details>
  <summary>Details</summary>
Motivation: 探讨小型语言模型（SLMs）在自动生成问题上的应用，作为大型语言模型在学习分析研究中的补充。

Method: 提出了一种新颖的问题生成流程，结合了SLMs的文本生成和概率推理能力，采用“生成-验证”策略，先大规模生成候选问题，再通过基于新颖概率推理的选择性验证来优化问题质量。

Result: 通过两次评估研究（7名人类专家和大型语言模型）证实，生成的问题大多数有明确答案，且与学习目标高度一致。

Conclusion: 在精心设计的流程引导下，小型语言模型能够有效生成高质量的问题，发挥其自身优势。

Abstract: We explore the use of small language models (SLMs) for automatic question generation as a complement to the prevalent use of their large counterparts in learning analytics research. We present a novel question generation pipeline that leverages both the text generation and the probabilistic reasoning abilities of SLMs to generate high-quality questions. Adopting a "generate-then-validate" strategy, our pipeline first performs expansive generation to create an abundance of candidate questions and refine them through selective validation based on novel probabilistic reasoning. We conducted two evaluation studies, one with seven human experts and the other with a large language model (LLM), to assess the quality of the generated questions. Most judges (humans or LLMs) agreed that the generated questions had clear answers and generally aligned well with the intended learning objectives. Our findings suggest that an SLM can effectively generate high-quality questions when guided by a well-designed pipeline that leverages its strengths.

</details>


### [3] [Workflow is All You Need: Escaping the "Statistical Smoothing Trap" via High-Entropy Information Foraging and Adversarial Pacing](https://arxiv.org/abs/2512.10121)
*Zhongjie Jiang*

Main category: cs.CL

TL;DR: 针对长文本生成中的幻觉和逻辑问题，DeepNews框架融合多粒度信息检索、结构化规划和对抗约束，有效提升财经深度报道的真实性和接受率。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型难以同时满足长文本生成中低幻觉、高逻辑性和个性化表达需求，原因在于传统生成范式忽视了专家写作所依赖的高熵信息采集和结构化认知过程。

Method: 引入基于信息采集理论的双粒度检索机制，利用专家知识库指导的规划方案，以及对抗性约束提示策略，减少幻觉并增强逻辑结构。

Result: 该论文围绕长篇垂直领域文本生成中的“三难困境”，即低幻觉率、强逻辑一致性和个性化表达，提出了DeepNews框架，通过模拟资深财经记者的认知流程，显著提升文本生成质量。

Conclusion: DeepNews框架有效克服传统生成模型的统计平滑陷阱，实现高真实性和逻辑性，显著提升财经报道的稿件接受率。

Abstract: Central to long-form text generation in vertical domains is the "impossible trinity" confronting current large language models (LLMs): the simultaneous achievement of low hallucination, deep logical coherence, and personalized expression. This study establishes that this bottleneck arises from existing generative paradigms succumbing to the Statistical Smoothing Trap, a phenomenon that overlooks the high-entropy information acquisition and structured cognitive processes integral to expert-level writing. To address this limitation, we propose the DeepNews Framework, an agentic workflow that explicitly models the implicit cognitive processes of seasoned financial journalists. The framework integrates three core modules: first, a dual-granularity retrieval mechanism grounded in information foraging theory, which enforces a 10:1 saturated information input ratio to mitigate hallucinatory outputs; second, schema-guided strategic planning, a process leveraging domain expert knowledge bases (narrative schemas) and Atomic Blocks to forge a robust logical skeleton; third, adversarial constraint prompting, a technique deploying tactics including Rhythm Break and Logic Fog to disrupt the probabilistic smoothness inherent in model-generated text. Experiments delineate a salient Knowledge Cliff in deep financial reporting: content truthfulness collapses when retrieved context falls below 15,000 characters, while a high-redundancy input exceeding 30,000 characters stabilizes the Hallucination-Free Rate (HFR) above 85%. In an ecological validity blind test conducted with a top-tier Chinese technology media outlet, the DeepNews system--built on a previous-generation model (DeepSeek-V3-0324)-achieved a 25% submission acceptance rate, significantly outperforming the 0% acceptance rate of zero-shot generation by a state-of-the-art (SOTA) model (GPT-5).

</details>


### [4] [PARAN: Persona-Augmented Review ANswering system on Food Delivery Review Dataset](https://arxiv.org/abs/2512.10148)
*Moonsoo Park,Jeongseok Yun,Bohyung Kim*

Main category: cs.CL

TL;DR: 本文针对用户信息有限的场景，设计了通过短评推断用户角色并融入生成提示的两阶段提示方法，提升个性化回复的相关性和多样性，且无需微调模型。


<details>
  <summary>Details</summary>
Motivation: 在用户信息有限的领域（如食品配送平台）中，个性化评论回复生成存在显著挑战。大语言模型缺乏上下文用户数据时容易生成通用回复，降低参与度和有效性。

Method: 设计两阶段提示流程，先从短评文本推断显性和隐性用户角色属性，再将这些属性加入生成提示中，结合调整解码温度以平衡多样性和忠实性。

Result: 提出了一种两阶段提示框架，从短评价文本中推断显性和隐性用户角色特征，将其融入回复生成提示，通过调整解码温度提升生成回复的多样性和忠实度。

Conclusion: 基于角色增强的提示方法有效提升了自动回复的个性化和相关性，适用于实际食品配送应用，且无需对大模型进行微调。

Abstract: Personalized review response generation presents a significant challenge in domains where user information is limited, such as food delivery platforms. While large language models (LLMs) offer powerful text generation capabilities, they often produce generic responses when lacking contextual user data, reducing engagement and effectiveness. In this work, we propose a two-stage prompting framework that infers both explicit (e.g., user-stated preferences) and implicit (e.g., demographic or stylistic cues) personas directly from short review texts. These inferred persona attributes are then incorporated into the response generation prompt to produce user-tailored replies. To encourage diverse yet faithful generations, we adjust decoding temperature during inference. We evaluate our method using a real-world dataset collected from a Korean food delivery app, and assess its impact on precision, diversity, and semantic consistency. Our findings highlight the effectiveness of persona-augmented prompting in enhancing the relevance and personalization of automated responses without requiring model fine-tuning.

</details>


### [5] [Unsupervised Acquisition of Discrete Grammatical Categories](https://arxiv.org/abs/2503.18702)
*David Ph. Shakouri,Crit Cremers,Niels O. Schiller*

Main category: cs.CL

TL;DR: 利用多智能体系统和聚类分析，子代语言模型从母语模型生成的语言实例中有效习得抽象语法规则，验证了计算实验环境的有效性。


<details>
  <summary>Details</summary>
Motivation: 探讨语言习得中如何通过观察语言实例而非内部知识，利用计算模型实现抽象语法知识的学习，验证理论语言类别的实际获得。

Method: 设计多智能体系统包含成人和子代语言模型，子代仅访问母模型生成的语言数据，采用层次凝聚聚类分析母语言生成的话语，抽取一组离散的语法类别规则。

Result: 本文通过一个计算实验环境，使用多智能体系统模拟语言习得过程，成人与子代语言模型互动，子代通过统计母模型生成的语言实例学习语法知识，并运用层次聚类方法提取抽象语法规则，实现非平凡语法知识的获得。

Conclusion: 通过统计分析和层次聚类提取的语法规则成功地被子代语言模型吸收，实现了非平凡语法知识的学习，且参数配置在测试中得到验证。

Abstract: This article presents experiments performed using a computational laboratory environment for language acquisition experiments. It implements a multi-agent system consisting of two agents: an adult language model and a daughter language model that aims to learn the mother language. Crucially, the daughter agent does not have access to the internal knowledge of the mother language model but only to the language exemplars the mother agent generates. These experiments illustrate how this system can be used to acquire abstract grammatical knowledge. We demonstrate how statistical analyses of patterns in the input data corresponding to grammatical categories yield discrete grammatical rules. These rules are subsequently added to the grammatical knowledge of the daughter language model. To this end, hierarchical agglomerative cluster analysis was applied to the utterances consecutively generated by the mother language model. It is argued that this procedure can be used to acquire structures resembling grammatical categories proposed by linguists for natural languages. Thus, it is established that non-trivial grammatical knowledge has been acquired. Moreover, the parameter configuration of this computational laboratory environment determined using training data generated by the mother language model is validated in a second experiment with a test set similarly resulting in the acquisition of non-trivial categories.

</details>


### [6] [Unforgotten Safety: Preserving Safety Alignment of Large Language Models with Continual Learning](https://arxiv.org/abs/2512.10150)
*Lama Alssum,Hani Itani,Hasan Abed Al Kader Hammoud,Philip Torr,Adel Bibi,Bernard Ghanem*

Main category: cs.CL

TL;DR: 本文研究了在微调大型语言模型时安全性的退化问题，提出将其视为持续学习问题，通过多种持续学习方法减轻安全性下降，DER方法表现最佳。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的普及，模型在适应新任务时安全性降低成为重要问题，需要有效方法在保证任务性能的同时维护模型安全性。

Method: 将模型微调视为持续学习问题，采用正则化、记忆和模型合并三类持续学习方法进行安全性保护，系统评估其在良性和有毒用户数据下的表现。

Result: 持续学习方法在降低攻击成功率方面优于标准微调，DER方法在保持任务性能的同时表现最佳，这一结论在三个下游任务和三种模型上均成立。

Conclusion: 持续学习方法，尤其是DER，能够有效缓解微调导致的大型语言模型安全性下降的问题，在多任务和多模型验证了其实用性。

Abstract: The safety alignment of large language models (LLMs) is becoming increasingly important with their democratization. In this paper, we study the safety degradation that comes with adapting LLMs to new tasks. We attribute this safety compromise to catastrophic forgetting and frame the problem of preserving safety when fine-tuning as a continual learning (CL) problem. We consider the fine-tuning-as-a-service setup where the user uploads their data to a service provider to get a customized model that excels on the user's selected task. We adapt several CL approaches from the literature and systematically evaluate their ability to mitigate safety degradation. These include regularization-based, memory-based, and model merging approaches. We consider two scenarios, (1) benign user data and (2) poisoned user data. Our results demonstrate that CL approaches consistently achieve lower attack success rates than standard fine-tuning. Among these, DER outperforms both other CL methods and existing safety-preserving baselines while maintaining task utility. These findings generalize across three downstream tasks (GSM8K, SST2, Code) and three model families (LLaMA2-7B, Mistral-7B, Gemma-2B), establishing CL as a practical solution to preserve safety.

</details>


### [7] [AutoMedic: An Automated Evaluation Framework for Clinical Conversational Agents with Medical Dataset Grounding](https://arxiv.org/abs/2512.10195)
*Gyutaek Oh,Sangjoon Park,Byung-Hoon Kim*

Main category: cs.CL

TL;DR: 本文提出AutoMedic框架和CARE指标，实现医疗大模型多轮临床对话的自动化、多维评测，推动临床对话应用的安全可信发展。


<details>
  <summary>Details</summary>
Motivation: 现有静态医疗问答评测无法覆盖动态、多轮临床对话的复杂场景，缺乏多维评估策略，导致临床对话模型评测不足。

Method: 提出AutoMedic多代理模拟框架，将静态医疗问答数据转化为虚拟患者档案，实现多轮临床对话模拟。基于CARE指标，综合评估对话准确性、效率、共情及鲁棒性。

Result: AutoMedic成功实现自动化临床对话评测，CARE指标多维度反映模型性能，结果经专家验证，体现了框架在实际临床对话评测中的有效性。

Conclusion: AutoMedic有效填补了动态临床对话评测的空白，为医疗大模型的开发和应用提供了实用的自动化、多维评测工具，促进了临床对话领域的发展。

Abstract: Evaluating large language models (LLMs) has recently emerged as a critical issue for safe and trustworthy application of LLMs in the medical domain. Although a variety of static medical question-answering (QA) benchmarks have been proposed, many aspects remain underexplored, such as the effectiveness of LLMs in generating responses in dynamic, interactive clinical multi-turn conversation situations and the identification of multi-faceted evaluation strategies beyond simple accuracy. However, formally evaluating a dynamic, interactive clinical situation is hindered by its vast combinatorial space of possible patient states and interaction trajectories, making it difficult to standardize and quantitatively measure such scenarios. Here, we introduce AutoMedic, a multi-agent simulation framework that enables automated evaluation of LLMs as clinical conversational agents. AutoMedic transforms off-the-shelf static QA datasets into virtual patient profiles, enabling realistic and clinically grounded multi-turn clinical dialogues between LLM agents. The performance of various clinical conversational agents is then assessed based on our CARE metric, which provides a multi-faceted evaluation standard of clinical conversational accuracy, efficiency/strategy, empathy, and robustness. Our findings, validated by human experts, demonstrate the validity of AutoMedic as an automated evaluation framework for clinical conversational agents, offering practical guidelines for the effective development of LLMs in conversational medical applications.

</details>


### [8] [Multilingual VLM Training: Adapting an English-Trained VLM to French](https://arxiv.org/abs/2512.10336)
*Jules Lahmi,Alexis Roger*

Main category: cs.CL

TL;DR: 多语言视觉-语言模型受限于数据翻译质量，提升本地语言数据集和翻译策略是未来关键。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型（VLM）主要基于英语训练，限制了非英语用户的访问和使用，需要扩展到更多语言。

Method: 比较翻译管道、LoRA微调及分阶段微调三种方法，利用翻译后的标准多模态基准和本地专家评估进行性能测试。

Result: 翻译数据集质量成为多语言VLM性能的主要瓶颈，不同方法在性能和计算成本上表现差异明显。

Conclusion: 多语言VLM的发展应聚焦于本地语言数据集的采集和高质量的翻译方法。

Abstract: Artificial intelligence has made great progress in recent years, particularly in the development of Vision--Language Models (VLMs) that understand both visual and textual data. However, these advancements remain largely limited to English, reducing their accessibility for non--English speakers. It is essential to extend these capabilities to a broader range of languages. This paper explores the challenges of adapting an English-trained VLM to different languages. To this end, we will explore and compare different methods for their performance and computational cost. We consider a translation-based pipeline, LoRA finetuning, and a two-stage finetuning strategy that separates vision adaptation from language adaptation. To evaluate these methods, we use a combination of standard multimodal benchmarks translated into the target language and manual assessments by native experts. The results reveal that dataset translation remains a major bottleneck in multilingual VLM performance, with data quality limiting the effectiveness of training and evaluation. These findings suggest that future efforts should focus on native-language dataset collection and improved translation strategies.

</details>


### [9] [Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale](https://arxiv.org/abs/2512.10398)
*Zhaodong Wang,Zhenting Qi,Sherman Wong,Nathan Hu,Samuel Lin,Jun Ge,Erwin Gao,Yining Yang,Ben Maurer,Wenlin Chen,David Recordon,Yilun Du,Minlan Yu,Ying Zhang*

Main category: cs.CL

TL;DR: 本文提出了开源工业级AI编码代理CCA及其开发平台Confucius SDK，融合长记忆推理、跨会话学习和模块化扩展，显著提升了实际软件工程任务的性能，实现了透明且易扩展的工业AI编码解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前开源编码代理在处理工业规模任务时存在性能不足，而专有代理虽性能强大但扩展性和可控性有限，亟需开发一个兼具透明性、扩展性和强大性能的开放式工业级AI编码代理。

Method: 提出Confucius Code Agent (CCA)及其基础的Confucius SDK，采用统一的协调器和分层工作记忆实现长上下文推理，持续笔记系统支持跨会话学习，模块化扩展模块提升工具链协调能力，通过元代理的构建-测试-改进循环实现快速迭代代理配置。

Result: CCA在真实软件工程任务中表现优异，SWE-Bench-Pro测试中，其Resolve@1性能达54.3%，显著优于之前的编码代理，展示了强大的工业规模应用能力。

Conclusion: Confucius SDK与CCA共同构建了一个透明、可扩展、可复现的工业级AI编码代理平台，缩小了研究原型与生产系统之间的差距，支持工业规模的代理开发与部署。

Abstract: Real-world AI software engineering demands coding agents that can reason over massive repositories, maintain durable memory across and within long sessions, and robustly coordinate complex toolchains at test time. Existing open-source coding agents provide transparency but frequently fall short when pushed to these industrial-scale workloads, while proprietary coding agents offer strong practical performance but limited extensibility, interpretability, and controllability. We present the Confucius Code Agent (CCA), an open-sourced AI software engineer that can operate at an industrial scale. CCA is built atop the Confucius SDK, an open-sourced agent development platform designed around three complementary perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX). The SDK introduces a unified orchestrator with hierarchical working memory for long-context reasoning, a persistent note-taking system for cross-session continual learning, and a modular extension module for robust tool use. Moreover, a meta-agent automates the synthesis, evaluation, and refinement of agent configurations through a build-test-improve loop, enabling rapid agent development on new tasks, environments, and tool stacks. Instantiated on Confucius SDK with these mechanisms, CCA delivers strong performance on real-world software engineering tasks. On SWE-Bench-Pro, CCA achieves a state-of-the-art Resolve@1 performance of 54.3%, substantially improving over prior coding agents. Together, the Confucius SDK and CCA provide a transparent, extensible, and reproducible foundation for AI agents, bridge gaps between research prototypes and production-grade systems, and support agent development and deployment at industrial scale.

</details>


### [10] [Sliding Window Attention Adaptation](https://arxiv.org/abs/2512.10411)
*Yijiong Yu,Jiale Liu,Qingyun Wu,Huazheng Wang,Ji Pei*

Main category: cs.CL

TL;DR: 本文提出了一套滑动窗口注意力适配方法，通过多策略结合，实现了无需重新预训练的全注意力模型向滑动窗口注意力的高效适配，提升了长上下文推理效率且保持了性能。


<details>
  <summary>Details</summary>
Motivation: Transformer模型中全注意力机制由于计算复杂度高，导致长上下文处理成本昂贵，滑动窗口注意力虽然计算效率高但直接应用会造成性能损失，因此希望在不重新预训练的前提下，将全注意力预训练模型适配为滑动窗口注意力以降低推理成本。

Method: 提出了五种适配方法的组合：只在预填充时应用滑动窗口注意力、保留“汇聚”令牌、交错全注意力和滑动窗口注意力层、链式思维提示以及微调，并通过实验验证了这些方法的协同效果。

Result: 实验结果表明，单一方法无法完全恢复性能，但结合五种策略能够有效恢复长上下文推理性能，并进一步分析了不同配置在性能与效率上的权衡，提供了适用于多种场景的推荐方案。

Conclusion: 通过滑动窗口注意力适配（SWAA）方法，可以有效地将全注意力预训练的大型语言模型适配到滑动窗口注意力，实现长上下文推理而不需重新预训练，同时保持良好的性能表现。

Abstract: The self-attention mechanism in Transformer-based Large Language Models (LLMs) scales quadratically with input length, making long-context inference expensive. Sliding window attention (SWA) reduces this cost to linear complexity, but naively enabling complete SWA at inference-time for models pretrained with full attention (FA) causes severe long-context performance degradation due to training-inference mismatch. This makes us wonder: Can FA-pretrained LLMs be well adapted to SWA without pretraining? We investigate this by proposing Sliding Window Attention Adaptation (SWAA), a set of practical recipes that combine five methods for better adaptation: (1) applying SWA only during prefilling; (2) preserving "sink" tokens; (3) interleaving FA/SWA layers; (4) chain-of-thought (CoT); and (5) fine-tuning. Our experiments show that SWA adaptation is feasible while non-trivial: no single method suffices, yet specific synergistic combinations effectively recover the original long-context performance. We further analyze the performance-efficiency trade-offs of different SWAA configurations and provide recommended recipes for diverse scenarios. Our code is available at https://github.com/yuyijiong/sliding-window-attention-adaptation

</details>


### [11] [Cooperative Retrieval-Augmented Generation for Question Answering: Mutual Information Exchange and Ranking by Contrasting Layers](https://arxiv.org/abs/2512.10422)
*Youmin Ko,Sungjong Seo,Hyunjoon Kim*

Main category: cs.CL

TL;DR: 本文提出CoopRAG框架，通过检索器与大型语言模型合作交互知识，改进多跳问答中的检索和推理过程，提高问答准确率。


<details>
  <summary>Details</summary>
Motivation: 现有基于检索增强生成的问答方法在简单和多跳问答中存在错误检索与幻觉现象。

Method: 将问题展开为带掩码的不确定子问题与推理链，结合多层检索器重排序与语言模型填充推理链进行协同推理。

Result: CoopRAG在三个多跳问答数据集及一个简单问答数据集上均优于现有最先进方法，提升了检索和问答性能。

Conclusion: CoopRAG有效地利用检索器层间合作和语言模型推理重构，显著提升了问答系统在多跳问答任务中的准确性。

Abstract: Since large language models (LLMs) have a tendency to generate factually inaccurate output, retrieval-augmented generation (RAG) has gained significant attention as a key means to mitigate this downside of harnessing only LLMs. However, existing RAG methods for simple and multi-hop question answering (QA) are still prone to incorrect retrievals and hallucinations. To address these limitations, we propose CoopRAG, a novel RAG framework for the question answering task in which a retriever and an LLM work cooperatively with each other by exchanging informative knowledge, and the earlier and later layers of the retriever model work cooperatively with each other to accurately rank the retrieved documents relevant to a given query. In this framework, we (i) unroll a question into sub-questions and a reasoning chain in which uncertain positions are masked, (ii) retrieve the documents relevant to the question augmented with the sub-questions and the reasoning chain, (iii) rerank the documents by contrasting layers of the retriever, and (iv) reconstruct the reasoning chain by filling the masked positions via the LLM. Our experiments demonstrate that CoopRAG consistently outperforms state-of-the-art QA methods on three multi-hop QA datasets as well as a simple QA dataset in terms of both the retrieval and QA performances. Our code is available.\footnote{https://github.com/meaningful96/CoopRAG}

</details>


### [12] [T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground](https://arxiv.org/abs/2512.10430)
*Dmitrii Stoianov,Danil Taranets,Olga Tsymboi,Ramil Latypov,Almaz Dautov,Vladislav Kruglikov,Nikita Surkov,German Abramov,Pavel Gein,Dmitry Abulkhanov,Mikhail Gashkov,Viktor Zelenkovskiy,Artem Batalov,Aleksandr Medvedev,Anatolii Potapov*

Main category: cs.CL

TL;DR: 本文介绍了面向俄语推理和高效推断的开源大语言模型T-pro 2.0及其相关资源，助力相关研究和应用。


<details>
  <summary>Details</summary>
Motivation: 提升俄语大语言模型的推理能力和推断效率，并推动可复现、可扩展的相关研究。

Method: 使用基于西里尔字母优化的分词器和适配的EAGLE推理解码流程进行混合推理和高效推断。

Result: 发布了T-pro 2.0模型权重、指令语料库T-Wix 500k、推理基准T-Math以及EAGLE权重，提供了公开的Web演示，展示了推断速度提升和多领域应用效果。

Conclusion: T-pro 2.0构建了一个开放且实用的俄语大语言模型系统，实现了高效推断和良好推理能力，促进俄语LLM应用发展。

Abstract: We introduce T-pro 2.0, an open-weight Russian LLM for hybrid reasoning and efficient inference. The model supports direct answering and reasoning-trace generation, using a Cyrillic-dense tokenizer and an adapted EAGLE speculative-decoding pipeline to reduce latency. To enable reproducible and extensible research, we release the model weights, the T-Wix 500k instruction corpus, the T-Math reasoning benchmark, and the EAGLE weights on Hugging Face. These resources allow users to study Russian-language reasoning and to extend or adapt both the model and the inference pipeline. A public web demo exposes reasoning and non-reasoning modes and illustrates the speedups achieved by our inference stack across domains. T-pro 2.0 thus serves as an accessible open system for building and evaluating efficient, practical Russian LLM applications.

</details>


### [13] [Semantic Reconstruction of Adversarial Plagiarism: A Context-Aware Framework for Detecting and Restoring "Tortured Phrases" in Scientific Literature](https://arxiv.org/abs/2512.10435)
*Agniva Maiti,Prajwal Panth,Suresh Chandra Satapathy*

Main category: cs.CL

TL;DR: SRAP结合领域模型和语义检索，有效识别并恢复科学文本中的伪装抄袭。


<details>
  <summary>Details</summary>
Motivation: 自动改写工具生成的“扭曲短语”威胁科学文献的完整性，现有方法对新型混淆效果差且无法追溯抄袭源。

Method: 采用两阶段架构：基于领域特定掩码语言模型(SciBERT)的统计异常检测与基于向量检索(FAISS)和句子对齐(SBERT)的语义重建。

Result: 零样本方法修复准确率为0，SRAP方法修复准确率达23.67%，且在专业术语密集文本中表现更稳健。

Conclusion: SRAP框架有效检测和恢复被自动改写工具掩盖的抄袭文本，显著优于现有方法。

Abstract: The integrity and reliability of scientific literature is facing a serious threat by adversarial text generation techniques, specifically from the use of automated paraphrasing tools to mask plagiarism. These tools generate "tortured phrases", statistically improbable synonyms (e.g. "counterfeit consciousness" for "artificial intelligence"), that preserve the local grammar while obscuring the original source. Most existing detection methods depend heavily on static blocklists or general-domain language models, which suffer from high false-negative rates for novel obfuscations and cannot determine the source of the plagiarized content. In this paper, we propose Semantic Reconstruction of Adversarial Plagiarism (SRAP), a framework designed not only to detect these anomalies but to mathematically recover the original terminology. We use a two-stage architecture: (1) statistical anomaly detection with a domain-specific masked language model (SciBERT) using token-level pseudo-perplexity, and (2) source-based semantic reconstruction using dense vector retrieval (FAISS) and sentence-level alignment (SBERT). Experiments on a parallel corpus of adversarial scientific text show that while zero-shot baselines fail completely (0.00 percent restoration accuracy), our retrieval-augmented approach achieves 23.67 percent restoration accuracy, significantly outperforming baseline methods. We also show that static decision boundaries are necessary for robust detection in jargon-heavy scientific text, since dynamic thresholding fails under high variance. SRAP enables forensic analysis by linking obfuscated expressions back to their most probable source documents.

</details>


### [14] [Enhancing Next-Generation Language Models with Knowledge Graphs: Extending Claude, Mistral IA, and GPT-4 via KG-BERT](https://arxiv.org/abs/2512.10440)
*Nour El Houda Ben Chaabene,Hamza Hammami*

Main category: cs.CL

TL;DR: 本文通过将知识图谱与大型语言模型结合，显著提升了模型在知识密集任务中的表现和事实一致性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自然语言处理方面表现优异，但缺乏结构化知识，导致事实不一致。

Method: 通过引入知识图谱（KG）并结合KG-BERT模型，实现对大型语言模型的增强，使其具备更好的知识基础和推理能力。

Result: 在知识密集型任务如问答和实体链接上取得显著提升，改善了事实可靠性。

Conclusion: 该方法有效提高了大型语言模型的事实准确性和上下文感知能力，为下一代模型的发展提供支持。

Abstract: Large language models (LLMs) like Claude, Mistral IA, and GPT-4 excel in NLP but lack structured knowledge, leading to factual inconsistencies. We address this by integrating Knowledge Graphs (KGs) via KG-BERT to enhance grounding and reasoning. Experiments show significant gains in knowledge-intensive tasks such as question answering and entity linking. This approach improves factual reliability and enables more context-aware next-generation LLMs.

</details>


### [15] [Decoding Student Minds: Leveraging Conversational Agents for Psychological and Learning Analysis](https://arxiv.org/abs/2512.10441)
*Nour El Houda Ben Chaabene,Hamza Hammami,Laid Kahloul*

Main category: cs.CL

TL;DR: 设计了一种心理感知对话代理，利用多模态数据和模型提升学生学习和情绪状态，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 提升教育环境中学生的学习表现和情绪健康。

Method: 结合大型语言模型、知识图谱增强BERT和双向LSTM注意力机制，融合文本语义、语音特征及行为趋势进行状态分类。

Result: 该系统通过结合多模态数据和先进模型，实现了对学生认知和情感状态的实时分类，验证了其能提高学生动机，降低压力，并带来适度的学业提升。

Conclusion: 将语义推理、多模态融合和时序建模应用于教育聊天机器人，能够实现个性化和适应性的学生支持。

Abstract: This paper presents a psychologically-aware conversational agent designed to enhance both learning performance and emotional well-being in educational settings. The system combines Large Language Models (LLMs), a knowledge graph-enhanced BERT (KG-BERT), and a bidirectional Long Short-Term Memory (LSTM) with attention to classify students' cognitive and affective states in real time. Unlike prior chatbots limited to either tutoring or affective support, our approach leverages multimodal data-including textual semantics, prosodic speech features, and temporal behavioral trends-to infer engagement, stress, and conceptual understanding. A pilot study with university students demonstrated improved motivation, reduced stress, and moderate academic gains compared to baseline methods. These results underline the promise of integrating semantic reasoning, multimodal fusion, and temporal modeling to support adaptive, student-centered educational interventions.

</details>


### [16] [Grammaticality Judgments in Humans and Language Models: Revisiting Generative Grammar with LLMs](https://arxiv.org/abs/2512.10453)
*Lars G. B. Johnsen*

Main category: cs.CL

TL;DR: 本文检测大型语言模型是否能通过表面文本训练捕捉深层语法结构；结果显示它们能够区别语法对比，证明对句法结构具备敏感性。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型是否具备反映内在层级语法结构的能力，仅仅基于表面形式训练而非显式编码语法规则。

Method: 通过引入提示，采集LLMs（如GPT-4和LLaMA-3）对主谓倒装和寄生空位许可两种语法结构的接受度评价，测试其是否再现语言学中的系统对比。

Result: LLMs在主谓倒装和寄生空位许可两种构式中均能稳定区分语法正确和错误的表达，表现出对语法结构的敏感度。

Conclusion: 大型语言模型（LLMs）能够通过表层形式训练展现对结构敏感性，区分语法可接受性，支持其隐含层级语法结构的认知。

Abstract: What counts as evidence for syntactic structure? In traditional generative grammar, systematic contrasts in grammaticality such as subject-auxiliary inversion and the licensing of parasitic gaps are taken as evidence for an internal, hierarchical grammar. In this paper, we test whether large language models (LLMs), trained only on surface forms, reproduce these contrasts in ways that imply an underlying structural representation.
  We focus on two classic constructions: subject-auxiliary inversion (testing recognition of the subject boundary) and parasitic gap licensing (testing abstract dependency structure). We evaluate models including GPT-4 and LLaMA-3 using prompts eliciting acceptability ratings. Results show that LLMs reliably distinguish between grammatical and ungrammatical variants in both constructions, and as such support that they are sensitive to structure and not just linear order. Structural generalizations, distinct from cognitive knowledge, emerge from predictive training on surface forms, suggesting functional sensitivity to syntax without explicit encoding.

</details>


### [17] [XDoGE: Multilingual Data Reweighting to Enhance Language Inclusivity in LLMs](https://arxiv.org/abs/2512.10545)
*Iñaki Lacunza,José Javier Saiz,Alexander Shvets,Aitor Gonzalez-Agirre,Marta Villegas*

Main category: cs.CL

TL;DR: 本研究通过优化多语言数据权重分布和训练策略，提升了中低资源语言的大型语言模型表现，并发布了专注于伊比利亚语言的新模型。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型主要依赖少数高资源语言，导致中低资源语言性能受限。

Method: 通过扩展DoGE算法至多语言环境的XDoGE，我们训练了一个小型代理模型以优化语言分布，并使用调整后的语言权重对全尺寸模型进行重新训练，包含从零开始训练和持续预训练阶段(CPT)。

Result: 针对英、西班牙语（高资源），葡萄牙语、加泰罗尼亚语（中资源），以及加利西亚语和巴斯克语（低资源）六种语言进行了实验，展示了数据重复对低资源语言和主导语言下采样的影响，提出并发布了IberianLLM-7B-Instruct模型，显著提升了伊比利亚语言及英语的性能。

Conclusion: 利用XDoGE算法调整语言权重并结合持续预训练，有效改善了模型在多种资源水平语言上的表现，尤其是中低资源语言，推动了多语言大型模型的公平和高效应用。

Abstract: Current large language models (LLMs) are trained on massive amounts of text data, primarily from a few dominant languages. Studies suggest that this over-reliance on high-resource languages, such as English, hampers LLM performance in mid- and low-resource languages. To mitigate this problem, we propose to (i) optimize the language distribution by training a small proxy model within a domain-reweighing DoGE algorithm that we extend to XDoGE for a multilingual setup, and (ii) rescale the data and train a full-size model with the established language weights either from scratch or within a continual pre-training phase (CPT). We target six languages possessing a variety of geographic and intra- and inter-language-family relations, namely, English and Spanish (high-resource), Portuguese and Catalan (mid-resource), Galician and Basque (low-resource). We experiment with Salamandra-2b, which is a promising model for these languages. We investigate the effects of substantial data repetition on minor languages and under-sampling on dominant languages using the IberoBench framework for quantitative evaluation. Finally, we release a new promising IberianLLM-7B-Instruct model centering on Iberian languages and English that we pretrained from scratch and further improved using CPT with the XDoGE weights.

</details>


### [18] [Causal Reasoning Favors Encoders: On The Limits of Decoder-Only Models](https://arxiv.org/abs/2512.10561)
*Amartya Roy,Elamparithy M,Kripabandhu Ghosh,Ponnurangam Kumaraguru,Adrian de Wynter*

Main category: cs.CL

TL;DR: 本文探讨了大语言模型在因果推理中的表现，发现仅靠上下文学习不足以实现可靠推理，编码器结构在多跳联合推理中表现更稳健。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在多跳联合因果推理中的能力及其在不同架构模型中的表现差异。

Method: 比较了编码器、编码器-解码器和仅解码器架构的微调模型与零样本和少样本上下文学习，在自然语言及非自然语言场景下进行多跳因果推理测试。

Result: 上下文学习模型容易过度关注无关特征，编码器和编码器-解码器模型微调后在多场景中展现更好的鲁棒性，仅在大规模模型下，纯解码器架构表现优于编码器类模型。

Conclusion: 仅用上下文学习进行因果推理表现不足，大规模解码器模型表现优异，但在成本和稳定性方面，带微调的编码器或编码器-解码器架构更优。

Abstract: In context learning (ICL) underpins recent advances in large language models (LLMs), although its role and performance in causal reasoning remains unclear. Causal reasoning demands multihop composition and strict conjunctive control, and reliance on spurious lexical relations of the input could provide misleading results. We hypothesize that, due to their ability to project the input into a latent space, encoder and encoder decoder architectures are better suited for said multihop conjunctive reasoning versus decoder only models. To do this, we compare fine-tuned versions of all the aforementioned architectures with zero and few shot ICL in both natural language and non natural language scenarios. We find that ICL alone is insufficient for reliable causal reasoning, often overfocusing on irrelevant input features. In particular, decoder only models are noticeably brittle to distributional shifts, while finetuned encoder and encoder decoder models can generalize more robustly across our tests, including the non natural language split. Both architectures are only matched or surpassed by decoder only architectures at large scales. We conclude by noting that for cost effective, short horizon robust causal reasoning, encoder or encoder decoder architectures with targeted finetuning are preferable.

</details>


### [19] [RoleRMBench & RoleRM: Towards Reward Modeling for Profile-Based Role Play in Dialogue Systems](https://arxiv.org/abs/2512.10575)
*Hang Ding,Qiming Feng,Dongqi Liu,Qi Zhao,Tao Yao,Shuo Wang,Dongsheng Chen,Jian Li,Zhenye Gan,Jiangning Zhang,Chengjie Wang,Yabiao Wang*

Main category: cs.CL

TL;DR: 本文介绍了RoleRMBench，一个针对角色扮演对话中奖励建模的系统性基准测试，并提出了基于连续隐式偏好的RoleRM模型，显著提升了奖励模型与人类判断的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型在开放且主观的角色扮演领域表现不佳，难以准确反映细微且基于人物角色的人的主观判断，造成严重性能下降。

Method: 提出RoleRM奖励模型，利用连续隐式偏好（CIP）将主观评价转化为连续一致的成对监督，通过多种结构化策略训练，从而提升对细粒度角色扮演能力的捕捉。

Result: RoleRMBench揭示了通用奖励模型与人工评价间存在显著差距，RoleRM模型在多个测评维度上超越领先开源和闭源模型24%以上，特别是在叙事和风格方面有显著提升。

Conclusion: RoleRM通过连续隐式偏好训练，在角色扮演对话奖励建模中显著优于现有模型，尤其在叙事连贯性和风格一致性上表现出色，推动了面向人类中心对话系统的主观对齐研究。

Abstract: Reward modeling has become a cornerstone of aligning large language models (LLMs) with human preferences. Yet, when extended to subjective and open-ended domains such as role play, existing reward models exhibit severe degradation, struggling to capture nuanced and persona-grounded human judgments. To address this gap, we introduce RoleRMBench, the first systematic benchmark for reward modeling in role-playing dialogue, covering seven fine-grained capabilities from narrative management to role consistency and engagement. Evaluation on RoleRMBench reveals large and consistent gaps between general-purpose reward models and human judgment, particularly in narrative and stylistic dimensions. We further propose RoleRM, a reward model trained with Continuous Implicit Preferences (CIP), which reformulates subjective evaluation as continuous consistent pairwise supervision under multiple structuring strategies. Comprehensive experiments show that RoleRM surpasses strong open- and closed-source reward models by over 24% on average, demonstrating substantial gains in narrative coherence and stylistic fidelity. Our findings highlight the importance of continuous preference representation and annotation consistency, establishing a foundation for subjective alignment in human-centered dialogue systems.

</details>


### [20] [AgriGPT-Omni: A Unified Speech-Vision-Text Framework for Multilingual Agricultural Intelligence](https://arxiv.org/abs/2512.10624)
*Bo Yang,Lanfei Feng,Yunkui Chen,Yu Zhang,Jianyu Zhang,Xiao Xu,Nueraili Aierken,Shijian Li*

Main category: cs.CL

TL;DR: 提出了农业跨语音、视觉、文本的统一多模态模型及大规模多语言语音数据集，建立首个农业多模态多语言评测基准，显著提升农业AI能力。


<details>
  <summary>Details</summary>
Motivation: 农业应用受限于多语言语音数据缺乏、统一多模态架构和全面评估基准缺失。

Method: 构建农业文本与图像数据合成与收集流程，形成多语言语音数据集；训练三阶段农业全模态模型（知识注入、多模态对齐、基于GRPO的强化学习）；提出涵盖多模态多任务的农业基准AgriBench-Omni-2K。

Result: 获得最大规模农业多语言语音数据集和首个农业全模态模型；模型在多语言和多模态推理及真实语音理解任务中显著优于通用基线。

Conclusion: AgriGPT-Omni框架有效促进农业多模态多语言智能发展，支持低资源区域可持续AI建设，推动相关开放研究。

Abstract: Despite rapid advances in multimodal large language models, agricultural applications remain constrained by the lack of multilingual speech data, unified multimodal architectures, and comprehensive evaluation benchmarks. To address these challenges, we present AgriGPT-Omni, an agricultural omni-framework that integrates speech, vision, and text in a unified framework. First, we construct a scalable data synthesis and collection pipeline that converts agricultural texts and images into training data, resulting in the largest agricultural speech dataset to date, including 492K synthetic and 1.4K real speech samples across six languages. Second, based on this, we train the first agricultural omni-model via a three-stage paradigm: textual knowledge injection, progressive multimodal alignment, and GRPO-based reinforcement learning, enabling unified reasoning across languages and modalities. Third, we propose AgriBench-Omni-2K, the first tri-modal benchmark for agriculture, covering diverse speech-vision-text tasks and multilingual slices, with standardized protocols and reproducible tools. Experiments show that AgriGPT-Omni significantly outperforms general-purpose baselines on multilingual and multimodal reasoning as well as real-world speech understanding. All models, data, benchmarks, and code will be released to promote reproducible research, inclusive agricultural intelligence, and sustainable AI development for low-resource regions.

</details>


### [21] [From Data Scarcity to Data Care: Reimagining Language Technologies for Serbian and other Low-Resource Languages](https://arxiv.org/abs/2512.10630)
*Smiljana Antonijevic Ubois*

Main category: cs.CL

TL;DR: 该研究针对低资源语言在大语言模型中面临的偏见和技术挑战，提出了一个基于CARE原则的数据关怀框架，以促进更公平和文化相关的语言技术开发。


<details>
  <summary>Details</summary>
Motivation: 探讨低资源语言在大语言模型（LLM）训练中存在的文化和语言偏见，以及历史和社会技术因素如何影响这些语言技术的发展，特别以塞尔维亚语为例。

Method: 通过半结构化访谈法收集十位语言学家、数字人文专家和AI开发者的观点，结合历史和社会技术背景分析，提出CARE原则（集体利益、控制权、责任和伦理）指导的数据关怀框架。

Result: 通过对十位专家的访谈，揭示了塞尔维亚语数字资源历史被破坏和现代技术方法（如粗浅音译、依赖英文模型等）带来的挑战，提出了以CARE原则为基础的数据关怀框架，旨在将偏见缓解融入语料库设计和治理，推动包容性和文化根植的语言技术发展。

Conclusion: 数据关怀框架能有效应对传统LLM开发中的权力失衡和文化盲点，成为建立包容性、可持续且有文化根基的语言技术的可复制模型。

Abstract: Large language models are commonly trained on dominant languages like English, and their representation of low resource languages typically reflects cultural and linguistic biases present in the source language materials. Using the Serbian language as a case, this study examines the structural, historical, and sociotechnical factors shaping language technology development for low resource languages in the AI age. Drawing on semi structured interviews with ten scholars and practitioners, including linguists, digital humanists, and AI developers, it traces challenges rooted in historical destruction of Serbian textual heritage, intensified by contemporary issues that drive reductive, engineering first approaches prioritizing functionality over linguistic nuance. These include superficial transliteration, reliance on English-trained models, data bias, and dataset curation lacking cultural specificity. To address these challenges, the study proposes Data Care, a framework grounded in CARE principles (Collective Benefit, Authority to Control, Responsibility, and Ethics), that reframes bias mitigation from a post hoc technical fix to an integral component of corpus design, annotation, and governance, and positions Data Care as a replicable model for building inclusive, sustainable, and culturally grounded language technologies in contexts where traditional LLM development reproduces existing power imbalances and cultural blind spots.

</details>


### [22] [Textual Data Bias Detection and Mitigation - An Extensible Pipeline with Experimental Evaluation](https://arxiv.org/abs/2512.10734)
*Rebekka Görge,Sujan Sai Gannamaneni,Tabea Naeven,Hammam Abdelwahab,Héctor Allende-Cid,Armin B. Cremers,Lennard Helmer,Michael Mock,Anna Schmitz,Songkai Xue,Elif Yildirir,Maximilian Poretschkin,Stefan Wrobel*

Main category: cs.CL

TL;DR: 本文提出一个综合性数据偏见检测与缓解流程，针对文本数据中的表现偏见和明确刻板印象进行检测和缓解，并通过实例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 训练大语言模型的文本数据存在多维度偏见问题，现有法规要求检测和缓解针对受保护群体的偏见，但缺乏切实可行的方法和操作流程。

Method: 提出包含四个部分的偏见检测和缓解流程：基于大模型生成词表检测群体标签，使用人口表现评分量化表现偏见，采用社会语言学过滤检测缓解刻板印象，结合语法和上下文的反事实数据增强补偿偏见。

Result: 实验证明该流程能减少文本数据中的表现偏见和刻板印象。基于去偏数据微调的模型在偏见基准上的表现不稳定，暴露了评价方法的不足和需针对性数据处理以更好缓解模型偏见。

Conclusion: 通过多组件流程有效减少文本数据中的表现偏见和显性刻板印象，但对去偏数据训练的语言模型并不总能带来偏见性能提升，说明现有评估方法存在不足。

Abstract: Textual data used to train large language models (LLMs) exhibits multifaceted bias manifestations encompassing harmful language and skewed demographic distributions. Regulations such as the European AI Act require identifying and mitigating biases against protected groups in data, with the ultimate goal of preventing unfair model outputs. However, practical guidance and operationalization are lacking. We propose a comprehensive data bias detection and mitigation pipeline comprising four components that address two data bias types, namely representation bias and (explicit) stereotypes for a configurable sensitive attribute. First, we leverage LLM-generated word lists created based on quality criteria to detect relevant group labels. Second, representation bias is quantified using the Demographic Representation Score. Third, we detect and mitigate stereotypes using sociolinguistically informed filtering. Finally, we compensate representation bias through Grammar- and Context-Aware Counterfactual Data Augmentation. We conduct a two-fold evaluation using the examples of gender, religion and age. First, the effectiveness of each individual component on data debiasing is evaluated through human validation and baseline comparison. The findings demonstrate that we successfully reduce representation bias and (explicit) stereotypes in a text dataset. Second, the effect of data debiasing on model bias reduction is evaluated by bias benchmarking of several models (0.6B-8B parameters), fine-tuned on the debiased text dataset. This evaluation reveals that LLMs fine-tuned on debiased data do not consistently show improved performance on bias benchmarks, exposing critical gaps in current evaluation methodologies and highlighting the need for targeted data manipulation to address manifested model bias.

</details>


### [23] [Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving](https://arxiv.org/abs/2512.10739)
*Songyang Gao,Yuzhe Gu,Zijian Wu,Lingkai Kong,Wenwei Zhang,Zhongrui Cai,Fan Zheng,Tianyou Ma,Junhao Shen,Haiteng Zhao,Duanyang Zhang,Huilun Zhang,Kuikun Liu,Chengqi Lyu,Yanhui Duan,Chiyu Chen,Ningsheng Ma,Jianfei Gao,Han Lyu,Dahua Lin,Kai Chen*

Main category: cs.CL

TL;DR: 本文提出了一种结合过程和结果的验证器(OPV)，通过主动学习和专家标注提升推理检测能力，显著优于现有模型，提升复杂推理任务性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于结果的验证器无法检查长推理链中的不可靠中间步骤，基于过程的验证器受限于高质量标注的缺乏，难以可靠检测复杂长推理链的错误。

Method: 提出了基于结果的过程验证器(OPV)，通过对长推理链的总结结果进行验证，结合迭代主动学习和专家标注，采用拒绝微调及强化学习来提升验证能力。

Result: OPV在保留的测试集上取得了83.1的F1得分，优于更大的开源模型Qwen3-Max-Preview的76.3，并且能有效检测合成数据中的误报，提升策略模型如DeepSeek-R1-Distill-Qwen-32B的准确率从55.2%到73.3%。

Conclusion: OPV验证器实现了准确、高效的大规模验证，解决了长推理链错误检测难题，显著提升了大型语言模型在复杂推理任务上的表现。

Abstract: Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the \textbf{O}utcome-based \textbf{P}rocess \textbf{V}erifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out \textsc{\thisbench}, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2\% to 73.3\% on AIME2025 as the compute budget scales.

</details>


### [24] [TRIDENT: A Redundant Architecture for Caribbean-Accented Emergency Speech Triage](https://arxiv.org/abs/2512.10741)
*Elroy Galbraith,Chadwick Sutherland,Donahue Morgan*

Main category: cs.CL

TL;DR: TRIDENT系统结合口音调优语音识别、大语言模型实体提取和声音压力检测，以支持加勒比地区紧急呼叫分诊，提升非标准英语环境下的服务公平性。


<details>
  <summary>Details</summary>
Motivation: 加勒比地区非标准英语变体导致紧急语音识别系统性能下降，造成当地居民服务获取不平等，亟需口音鲁棒的紧急分诊AI解决方案。

Method: 采用三层架构：加勒比口音调优的自动语音识别、大型语言模型的本地实体提取、生物声学的声音压力检测，三者互补支持调度员分诊决策。

Result: 本文提出了TRIDENT系统，一种针对加勒比地区紧急呼叫的三层调度员支持架构，以解决非标准英语变体导致的自动语音识别性能下降问题。TRIDENT结合了针对加勒比口音调优的自动语音识别、大型语言模型的实体提取和生物声学压力检测，向调度员提供转录置信度、结构化临床实体和声音压力指标三种互补信号。作者强调低置信度反映的是优先队列信号而非系统失败，尤其结合声音压力指标可以识别处于危机且可能切换到基语变体的呼叫者。此外，实体提取层通过语义分析捕获生命威胁信号，弥补了旁语言特征的不足。文章设计了系统架构，并基于心理语言学关于压力引发代码切换的理论进行了论证，讨论了灾害场景下离线运行的部署考虑。该工作为口音鲁棒的紧急AI提供了框架，确保加勒比声音公平接入国家分诊协议，但尚未进行加勒比紧急呼叫的实证验证。

Conclusion: TRIDENT为加勒比英语口音环境中的紧急呼叫提供了多信号支持框架，有望提升分诊准确性和公平性，实证验证将在未来开展。

Abstract: Emergency speech recognition systems exhibit systematic performance degradation on non-standard English varieties, creating a critical gap in services for Caribbean populations. We present TRIDENT (Transcription and Routing Intelligence for Dispatcher-Empowered National Triage), a three-layer dispatcher-support architecture designed to structure emergency call inputs for human application of established triage protocols (the ESI for routine operations and START for mass casualty events), even when automatic speech recognition fails.
  The system combines Caribbean-accent-tuned ASR, local entity extraction via large language models, and bio-acoustic distress detection to provide dispatchers with three complementary signals: transcription confidence, structured clinical entities, and vocal stress indicators. Our key insight is that low ASR confidence, rather than representing system failure, serves as a valuable queue prioritization signal -- particularly when combined with elevated vocal distress markers indicating a caller in crisis whose speech may have shifted toward basilectal registers. A complementary insight drives the entity extraction layer: trained responders and composed bystanders may report life-threatening emergencies without elevated vocal stress, requiring semantic analysis to capture clinical indicators that paralinguistic features miss.
  We describe the architectural design, theoretical grounding in psycholinguistic research on stress-induced code-switching, and deployment considerations for offline operation during disaster scenarios. This work establishes a framework for accent-resilient emergency AI that ensures Caribbean voices receive equitable access to established national triage protocols. Empirical validation on Caribbean emergency calls remains future work.

</details>


### [25] [OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification](https://arxiv.org/abs/2512.10756)
*Zijian Wu,Lingkai Kong,Wenwei Zhang,Songyang Gao,Yuzhe Gu,Zhongrui Cai,Tianyou Ma,Yuhong Liu,Zhi Wang,Runyuan Ma,Guangyu Wang,Wei Li,Conghui He,Dahua Lin,Kai Chen*

Main category: cs.CL

TL;DR: OPV方法通过总结结果验证长推理链过程，结合主动学习降低标注成本，实现了复杂推理任务中推理链的高效且准确验证，取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有结果验证器难以检查长推理链中的中间不可靠步骤，过程验证器受限于高质量标注缺乏，难以准确检测复杂长推理链中的错误，因此设计OPV以兼顾准确性与验证效率。

Method: 提出了基于总结结果的过程验证方法OPV，结合迭代主动学习框架和专家注释，通过拒绝微调和强化学习提升模型验证能力。

Result: 本文提出了一种名为结果过程验证器（OPV）的新方法，用于对大型语言模型在复杂推理任务中的推理链过程进行准确且高效的验证。OPV结合了结果验证和过程验证的优点，通过对长推理链的总结结果进行过程验证，解决了以往验证器在中间步骤检查中的不足。文章采用迭代式主动学习框架，利用专家注释以降低标注成本，逐步提升验证能力。实验结果显示，OPV在多项评测中表现优异，显著超越了现有大型开源模型，且能有效识别合成数据中的假阳性，在与策略模型合作时也能显著提升性能。

Conclusion: OPV有效提升了长推理链的验证准确性和效率，降低了人工标注成本，在多个数据集和模型中取得了显著的性能提升，具备广泛应用潜力。

Abstract: Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out OPV-Bench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2% to 73.3% on AIME2025 as the compute budget scales.

</details>


### [26] [Grow Up and Merge: Scaling Strategies for Efficient Language Adaptation](https://arxiv.org/abs/2512.10772)
*Kevin Glocker,Kätriin Kukk,Romina Oji,Marcel Bollmann,Marco Kuhlmann,Jenny Kunz*

Main category: cs.CL

TL;DR: 通过放大基于英文的预训练模型，能更高效适应多语言、提高数据利用率并减少能力遗忘，合并扩展模型可构建灵活多语系统，但合并策略仍有优化空间。


<details>
  <summary>Details</summary>
Motivation: 多语种预训练模型在中低资源语言表现不佳且资源消耗大，研究如何通过模型扩展提升多语模型针对特定语言适应效果，实现更高效、更稳定的跨语言迁移与多语言系统构建。

Method: 论文通过控制FLOP量匹配的不同规模英文基模型，进行目标语言持续预训练对比实验，评估扩展大小模型的适应效果与数据效率，并尝试将扩展训练后的语言模型进行多模型合并，比较合并性能及不同合并策略效果。

Result: 本论文研究了通过模型扩展（scaling）作为适应新目标语言的高效策略，尤其针对中低资源语言的预训练语言模型。通过约等Flop计算量匹配的模型在数据充足的情况下，发现放大后的英文基础模型在适应目标语言时表现可匹敌甚至超过使用更多数据的较小模型继续预训练结果，说明模型扩展提高了数据效率。同时，扩展模型能更好保留英文基础能力，减轻遗忘。最后，作者尝试将扩展的语言专用模型合并构建模块化多语种系统，结果虽不及联合多语训练，但扩展模型合并效果优于较小模型，且合并方法有较大性能差异，表明专门针对语言级融合的合并策略有进一步提升潜力。

Conclusion: 模型扩展是提升中低资源语言适应效果的有效手段，可在保证英文基础能力的同时减少遗忘，提高数据利用效率。扩展模型合并表现优于小模型，但合并效果与联合训练相比仍有差距，需发展语言级别的合并方法。

Abstract: Achieving high-performing language models which include medium- and lower-resource languages remains a challenge. Massively multilingual models still underperform compared to language-specific adaptations, especially at smaller model scales. In this work, we investigate scaling as an efficient strategy for adapting pretrained models to new target languages. Through comprehensive scaling ablations with approximately FLOP-matched models, we test whether upscaling an English base model enables more effective and resource-efficient adaptation than standard continued pretraining. We find that, once exposed to sufficient target-language data, larger upscaled models can match or surpass the performance of smaller models continually pretrained on much more data, demonstrating the benefits of scaling for data efficiency. Scaling also helps preserve the base model's capabilities in English, thus reducing catastrophic forgetting. Finally, we explore whether such scaled, language-specific models can be merged to construct modular and flexible multilingual systems. We find that while merging remains less effective than joint multilingual training, upscaled merges perform better than smaller ones. We observe large performance differences across merging methods, suggesting potential for improvement through merging approaches specialized for language-level integration.

</details>


### [27] [Script Gap: Evaluating LLM Triage on Indian Languages in Native vs Roman Scripts in a Real World Setting](https://arxiv.org/abs/2512.10780)
*Manurag Khullar,Utkarsh Desai,Poorva Malviya,Aman Dalmia,Zheyuan Ryan Shi*

Main category: cs.CL

TL;DR: 本文研究了罗马化文本对大型语言模型（LLMs）在印度产妇和新生儿医疗分诊领域性能的影响，发现罗马化文本使模型表现下降5-12个F1分数点，可能导致大量误诊。虽然模型能理解语义意图，但分类结果受正字法噪声影响，存在可靠性隐患。


<details>
  <summary>Details</summary>
Motivation: 印度多语言环境中，用户经常使用罗马化文本进行交流，但现有研究很少用真实数据检测罗马化对LLMs性能的影响，尤其是在关键医疗场景。

Method: 在真实用户生成的涉及五种印度语言和尼泊尔语的查询数据集上，评估多款主流LLMs对母婴健康分诊任务的表现，比较罗马化文本与本土文字的差异。

Result: 罗马化文本导致模型F1分数降低5-12个点，在实际合作医疗机构估计可导致近200万次额外错误分诊；语义推断准确但最终分类不稳定。

Conclusion: 罗马化文本显著降低了LLMs在临床分诊中的准确性，造成大量潜在误诊；模型语义理解良好但最终输出受正字法干扰，不足以保证安全应用。

Abstract: Large Language Models (LLMs) are increasingly deployed in high-stakes clinical applications in India. In many such settings, speakers of Indian languages frequently communicate using romanized text rather than native scripts, yet existing research rarely evaluates this orthographic variation using real-world data. We investigate how romanization impacts the reliability of LLMs in a critical domain: maternal and newborn healthcare triage. We benchmark leading LLMs on a real-world dataset of user-generated queries spanning five Indian languages and Nepali. Our results reveal consistent degradation in performance for romanized messages, with F1 scores trailing those of native scripts by 5-12 points. At our partner maternal health organization in India, this gap could cause nearly 2 million excess errors in triage. Crucially, this performance gap by scripts is not due to a failure in clinical reasoning. We demonstrate that LLMs often correctly infer the semantic intent of romanized queries. Nevertheless, their final classification outputs remain brittle in the presence of orthographic noise in romanized inputs. Our findings highlight a critical safety blind spot in LLM-based health systems: models that appear to understand romanized input may still fail to act on it reliably.

</details>


### [28] [The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality](https://arxiv.org/abs/2512.10791)
*Aileen Cheng,Alon Jacovi,Amir Globerson,Ben Golan,Charles Kwong,Chris Alberti,Connie Tao,Eyal Ben-David,Gaurav Singh Tomar,Lukas Haas,Yonatan Bitton,Adam Bloniarz,Aijun Bai,Andrew Wang,Anfal Siddiqui,Arturo Bajuelos Castillo,Aviel Atias,Chang Liu,Corey Fry,Daniel Balle,Deepanway Ghosal,Doron Kukliansky,Dror Marcus,Elena Gribovskaya,Eran Ofek,Honglei Zhuang,Itay Laish,Jan Ackermann,Lily Wang,Meg Risdal,Megan Barnes,Michael Fink,Mohamed Amin,Moran Ambar,Natan Potikha,Nikita Gupta,Nitzan Katz,Noam Velan,Ofir Roval,Ori Ram,Polina Zablotskaia,Prathamesh Bang,Priyanka Agrawal,Rakesh Ghiya,Sanjay Ganapathy,Simon Baumgartner,Sofia Erell,Sushant Prakash,Thibault Sellam,Vikram Rao,Xuanhui Wang,Yaroslav Akulov,Yulong Yang,Zhen Yang,Zhixin Lai,Zhongru Wu,Anca Dragan,Avinatan Hassidim,Fernando Pereira,Slav Petrov,Srinivasan Venkatachary,Tulsee Doshi,Yossi Matias,Sasha Goldshtein,Dipanjan Das*

Main category: cs.CL

TL;DR: FACTS排行榜是一套综合语言模型事实准确性评估体系，包含多模态问答、参数知识、搜索辅助和依据文档的长文本评估，提供均衡的事实准确性测量。


<details>
  <summary>Details</summary>
Motivation: 当前的语言模型在生成事实文本时存在准确性难以全面评估的问题，亟需一个系统化、多角度衡量模型事实准确性的标准。

Method: 该论文提出了一个包含四个子排行榜的综合评估套件，利用自动评判模型对多模态、参数内知识、搜索信息检索及文本依托文档准确性进行评分，并计算综合得分。

Result: FACTS排行榜提供了一个在线活跃维护的综合评测平台，涵盖多种事实评估场景，支持公开及私有数据分割，促进外部参与并保障评测的严谨性。

Conclusion: FACTS排行榜为语言模型的事实准确性提供了全面和均衡的评估，为促进更精准的文本生成设立了标准。

Abstract: We introduce The FACTS Leaderboard, an online leaderboard suite and associated set of benchmarks that comprehensively evaluates the ability of language models to generate factually accurate text across diverse scenarios. The suite provides a holistic measure of factuality by aggregating the performance of models on four distinct sub-leaderboards: (1) FACTS Multimodal, which measures the factuality of responses to image-based questions; (2) FACTS Parametric, which assesses models' world knowledge by answering closed-book factoid questions from internal parameters; (3) FACTS Search, which evaluates factuality in information-seeking scenarios, where the model must use a search API; and (4) FACTS Grounding (v2), which evaluates whether long-form responses are grounded in provided documents, featuring significantly improved judge models. Each sub-leaderboard employs automated judge models to score model responses, and the final suite score is an average of the four components, designed to provide a robust and balanced assessment of a model's overall factuality. The FACTS Leaderboard Suite will be actively maintained, containing both public and private splits to allow for external participation while guarding its integrity. It can be found at https://www.kaggle.com/benchmarks/google/facts .

</details>


### [29] [LabelFusion: Learning to Fuse LLMs and Transformer Classifiers for Robust Text Classification](https://arxiv.org/abs/2512.10793)
*Michael Schlee,Christoph Weisser,Timo Kivimäki,Melchizedek Mashiku,Benjamin Saefken*

Main category: cs.CL

TL;DR: LabelFusion融合转换器和大型语言模型，利用结构化提示工程和多层感知机学习联合表示，实现高效且高准确率的多类多标签文本分类。


<details>
  <summary>Details</summary>
Motivation: 利用大型语言模型的推理能力和传统transformer分类器的优势，提升文本分类的准确性，同时实现成本和延迟的平衡。

Method: 结合使用基于transformer的传统分类器（如RoBERTa）和大型语言模型（如OpenAI GPT、Google Gemini等），通过串联两者的向量表示并输入多层感知机进行融合分类。

Result: 在AG News数据集上达到了92.4%的准确率，在10类Reuters 21578主题分类任务中达到了92.3%的准确率，表现稳定且高效。

Conclusion: 通过学习融合LLM的推理得分和传统分类器嵌入，LabelFusion在文本分类任务中表现出良好的准确性与成本效益，可适用于多领域多任务场景。

Abstract: LabelFusion is a fusion ensemble for text classification that learns to combine a traditional transformer-based classifier (e.g., RoBERTa) with one or more Large Language Models (LLMs such as OpenAI GPT, Google Gemini, or DeepSeek) to deliver accurate and cost-aware predictions across multi-class and multi-label tasks. The package provides a simple high-level interface (AutoFusionClassifier) that trains the full pipeline end-to-end with minimal configuration, and a flexible API for advanced users. Under the hood, LabelFusion integrates vector signals from both sources by concatenating the ML backbone's embeddings with the LLM-derived per-class scores -- obtained through structured prompt-engineering strategies -- and feeds this joint representation into a compact multi-layer perceptron (FusionMLP) that produces the final prediction. This learned fusion approach captures complementary strengths of LLM reasoning and traditional transformer-based classifiers, yielding robust performance across domains -- achieving 92.4% accuracy on AG News and 92.3% on 10-class Reuters 21578 topic classification -- while enabling practical trade-offs between accuracy, latency, and cost.

</details>


### [30] [Quantifying Emotional Tone in Tolkien's The Hobbit: Dialogue Sentiment Analysis with RegEx, NRC-VAD, and Python](https://arxiv.org/abs/2512.10865)
*Lilin Qiu*

Main category: cs.CL

TL;DR: 通过计算文本分析，研究揭示《霍比特人》对话中情感色调积极平静，情感节奏在紧张与安慰间循环，反映了小说的故事节奏和情感变化。


<details>
  <summary>Details</summary>
Motivation: 通过结合计算方法与文学分析，探索文本中潜在的情感结构，揭示文学作品中情感节奏对叙事节奏的影响，进而丰富对《霍比特人》情感韵律的理解。

Method: 使用正则表达式提取《霍比特人》中的对话文本，采用NRC-VAD情感词典对文本进行情感量化评分，并通过情感轨迹图和词云等方式进行可视化分析。

Result: 该研究利用计算文本分析方法对J.R.R.托尔金《霍比特人》中的对话情感色调进行了分析。通过正则表达式提取对话内容，使用NRC-VAD词典对情感维度进行量化评分，结果显示对话整体呈现出积极（高愉悦度）和平静（低唤醒度）的情感基调，随着故事进展，主导感（控制感）逐渐增强。研究还通过情感轨迹图和词云等可视化工具，展示了《霍比特人》中紧张与安慰交替出现的语言节奏。该研究结合了数字化方法与文学解释，揭示了小说中隐含的情感结构及其对叙事节奏的塑造。

Conclusion: 《霍比特人》对话展现出积极、平静且逐渐增强的主导感情调，情感节奏在危险与轻松之间循环，体现了小说情感节奏的稳固和调控，从而支持了故事的叙事效果。

Abstract: This study analyzes the emotional tone of dialogue in J. R. R. Tolkien's The Hobbit (1937) using computational text analysis. Dialogue was extracted with regular expressions, then preprocessed, and scored using the NRC-VAD lexicon to quantify emotional dimensions. The results show that the dialogue maintains a generally positive (high valence) and calm (low arousal) tone, with a gradually increasing sense of agency (dominance) as the story progresses. These patterns reflect the novel's emotional rhythm: moments of danger and excitement are regularly balanced by humor, camaraderie, and relief. Visualizations -- including emotional trajectory graphs and word clouds -- highlight how Tolkien's language cycles between tension and comfort. By combining computational tools with literary interpretation, this study demonstrates how digital methods can uncover subtle emotional structures in literature, revealing the steady rhythm and emotional modulation that shape the storytelling in The Hobbit.

</details>


### [31] [Computational emotion analysis with multimodal LLMs: Current evidence on an emerging methodological opportunity](https://arxiv.org/abs/2512.10882)
*Hauke Licht*

Main category: cs.CL

TL;DR: 本文评估了多模态大型语言模型（mLLMs）在基于视频的情绪激发分析中的效果，发现其在理想环境下表现良好但在真实政治辩论中表现欠佳。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏关于多模态生成式人工智能在情绪分析中的有效性证据，特别是在政治沟通领域。

Method: 利用两组包含人工标注的视频数据集，对多模态大型语言模型进行视频中情绪激发的评估比较。

Result: mLLMs 在理想数据集上的情绪激发评分高度可靠且无明显偏见，但在真实议会辩论视频中表现不佳。

Conclusion: 在理想条件下，mLLMs 能够可靠地分析情绪激发且无明显人口统计偏见，但在真实世界的议会辩论视频中表现不足，可能影响后续统计推断。

Abstract: Emotions are central to politics and analyzing their role in political communication has a long tradition. As research increasingly leverages audio-visual materials to analyze the display of emotions, the emergence of multimodal generative AI promises great advances. However, we lack evidence about the effectiveness of multimodal AI in emotion analysis. This paper addresses this gap by evaluating current multimodal large language models (mLLMs) in video-based analysis of emotional arousal in two complementary data sets of human-labeled video recordings. I find that under ideal circumstances, mLLMs' emotional arousal ratings are highly reliable and show little to know indication of demographic bias. However, in recordings of speakers in real-world parliamentary debates, mLLMs' arousal ratings fail to deliver on this promise with potential negative consequences for downstream statistical inferences. This study therefore underscores the need for continued, thorough evaluation of emerging generative AI methods in political analysis and contributes a suitable replicable framework.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [32] [Norm-Governed Multi-Agent Decision-Making in Simulator-Coupled Environments:The Reinsurance Constrained Multi-Agent Simulation Process (R-CMASP)](https://arxiv.org/abs/2512.09939)
*Stella C. Dong*

Main category: cs.MA

TL;DR: 本文提出了一种结合模拟器驱动动态和规范约束的多智能体模型，显著提升了再保险决策中的协调稳定性和规范遵循性。


<details>
  <summary>Details</summary>
Motivation: 传统的确定性工作流自动化无法满足分布式和不对称信息、部分可观察性、异质认知职责以及监管和审慎约束等再保险决策的核心结构需求。

Method: 提出了再保险约束多智能体仿真过程（R-CMASP），扩展了随机游戏和部分可观测马尔可夫决策过程（Dec-POMDPs），引入了模拟器耦合的转移动态、角色专门化的智能体及规范可行性层。采用基于大语言模型（LLM）的智能体，结合工具访问和类型化消息协议，在校准合成环境中进行验证。

Result: 基于规范约束的多智能体协调表现出比确定性自动化或单体LLM基线更稳定、一致且符合规范的行为，表现为定价方差降低、资本效率提升和条款解释准确性提高。嵌入审慎规范和结构化通信显著增强了均衡稳定性。

Conclusion: 受监管、模拟器驱动的决策环境最适合用规范驱动、耦合模拟器的多智能体系统建模，这种方法能有效提升再保险过程的表现和合规水平。

Abstract: Reinsurance decision-making exhibits the core structural properties that motivate multi-agent models: distributed and asymmetric information, partial observability, heterogeneous epistemic responsibilities, simulator-driven environment dynamics, and binding prudential and regulatory constraints. Deterministic workflow automation cannot meet these requirements, as it lacks the epistemic flexibility, cooperative coordination mechanisms, and norm-sensitive behaviour required for institutional risk-transfer.
  We propose the Reinsurance Constrained Multi-Agent Simulation Process (R-CMASP), a formal model that extends stochastic games and Dec-POMDPs by adding three missing elements: (i) simulator-coupled transition dynamics grounded in catastrophe, capital, and portfolio engines; (ii) role-specialized agents with structured observability, belief updates, and typed communication; and (iii) a normative feasibility layer encoding solvency, regulatory, and organizational rules as admissibility constraints on joint actions.
  Using LLM-based agents with tool access and typed message protocols, we show in a domain-calibrated synthetic environment that governed multi-agent coordination yields more stable, coherent, and norm-adherent behaviour than deterministic automation or monolithic LLM baselines--reducing pricing variance, improving capital efficiency, and increasing clause-interpretation accuracy. Embedding prudential norms as admissibility constraints and structuring communication into typed acts measurably enhances equilibrium stability.
  Overall, the results suggest that regulated, simulator-driven decision environments are most naturally modelled as norm-governed, simulator-coupled multi-agent systems.

</details>


### [33] [Empirical Hardness in Multi-Agent Pathfinding: Research Challenges and Opportunities](https://arxiv.org/abs/2512.10078)
*Jingyao Ren,Eric Ewing,T. K. Satish Kumar,Sven Koenig,Nora Ayanian*

Main category: cs.MA

TL;DR: 本文分析了多智能体路径规划的经验难度差异，提出了算法选择、关键特征和难题生成三大挑战，为未来研究指明方向。


<details>
  <summary>Details</summary>
Motivation: MAPF问题实际难度与理论复杂性不符，需研究其经验性难度的影响因素及规律。

Method: 提出了三大研究挑战：算法选择、关键特征分析（如相变、backbone/backdoor结构）、基于经验难度生成难题实例和多样基准测试集。

Result: 明确了MAPF经验难度研究的核心问题与方向，为后续深入研究奠定基础。

Conclusion: 提出的研究框架和挑战将推动MAPF经验难度领域的发展和更有效的算法及测试集设计。

Abstract: Multi-agent pathfinding (MAPF) is the problem of finding collision-free paths for a team of agents on a map. Although MAPF is NP-hard, the hardness of solving individual instances varies significantly, revealing a gap between theoretical complexity and actual hardness. This paper outlines three key research challenges in MAPF empirical hardness to understand such phenomena. The first challenge, known as algorithm selection, is determining the best-performing algorithms for a given instance. The second challenge is understanding the key instance features that affect MAPF empirical hardness, such as structural properties like phase transition and backbone/backdoor. The third challenge is how to leverage our knowledge of MAPF empirical hardness to effectively generate hard MAPF instances or diverse benchmark datasets. This work establishes a foundation for future empirical hardness research and encourages deeper investigation into these promising and underexplored areas.

</details>


### [34] [Emergent Collective Memory in Decentralized Multi-Agent AI Systems](https://arxiv.org/abs/2512.10166)
*Khushiyant*

Main category: cs.MA

TL;DR: 本文研究了多智能体系统中集体记忆的产生机制，通过实验证实个体记忆和环境信息的协同作用及其临界密度的存在。


<details>
  <summary>Details</summary>
Motivation: 理解多智能体系统中，如何通过个体和环境信息的交互产生分布式集体记忆，及其对系统协同效率的影响。

Method: 采用不同大小网格和多智能体数量的环境中运行多次实验，比较无记忆、仅记忆和仅环境轨迹三种条件下的性能，结合密度扫描实验验证理论相变模型。

Result: 该论文展示了去中心化多智能体系统中集体记忆的形成机制，强调个体记忆和环境信息的互动。实验证明，个体记忆显著提升系统绩效，而单独依靠环境轨迹无法完成任务。通过多种环境和密度的系统实验，验证了相变理论预测，并确定了临界密度。结果显示，在较大规模环境和高密度条件下，环境轨迹（stigmergy）超越了个体记忆，成为主要协调机制。

Conclusion: 个体记忆虽能独立提升系统性能，但环境轨迹需要认知基础方能发挥作用。系统在密度达到临界值后，环境轨迹协调机制占主导，显著优于单纯记忆，验证了理论相变预测。

Abstract: We demonstrate how collective memory emerges in decentralized multi-agent systems through the interplay between individual agent memory and environmental trace communication. Our agents maintain internal memory states while depositing persistent environmental traces, creating a spatially distributed collective memory without centralized control. Comprehensive validation across five environmental conditions (20x20 to 50x50 grids, 5-20 agents, 50 runs per configuration) reveals a critical asymmetry: individual memory alone provides 68.7% performance improvement over no-memory baselines (1563.87 vs 927.23, p < 0.001), while environmental traces without memory fail completely. This demonstrates that memory functions independently but traces require cognitive infrastructure for interpretation. Systematic density-sweep experiments (rho in [0.049, 0.300], up to 625 agents) validate our theoretical phase transition prediction. On realistic large grids (30x30, 50x50), stigmergic coordination dominates above rho ~ 0.20, with traces outperforming memory by 36-41% on composite metrics despite lower food efficiency. The experimental crossover confirms the predicted critical density rho_c = 0.230 within 13% error.

</details>


### [35] [Thinking While Driving: A Concurrent Framework for Real-Time, LLM-Based Adaptive Routing](https://arxiv.org/abs/2512.10610)
*Xiaopei Tan,Muyang Fan*

Main category: cs.MA

TL;DR: 本文提出了一种使智能体在移动中利用大语言模型进行动态路径规划的框架，大幅降低交通等待时间，实现高效的多智能体实时协同。


<details>
  <summary>Details</summary>
Motivation: 传统路径规划方法需智能体停车进行决策，导致交叉路口等待时间长，效率低。本文旨在实现移动中动态路径规划，提高交通效率。

Method: 采用基于图的交通环境，将LLM集成到异步非阻塞架构中，利用Unity协程和请求管理器实现多智能体实时协同决策。

Result: 在高交通密度情况下，智能体决策延迟仅为0.75秒，能够动态适应交通拥堵并实时重新规划路线，表现出超越静态路径寻找的行为。

Conclusion: 本文提出的Thinking While Driving框架实现了移动中基于大语言模型（LLM）的路径规划，显著降低了交通路口的等待时间，并在高交通负载下保持实时性能。

Abstract: We present Thinking While Driving, a concurrent routing framework that integrates LLMs into a graph-based traffic environment. Unlike approaches that require agents to stop and deliberate, our system enables LLM-based route planning while agents are moving, significantly reducing intersection wait times. Under high traffic, agents average just 0.75 seconds of decision latency. To coordinate many agents in real-time, we implement a non-blocking asynchronous architecture using Unity coroutines and a dedicated request manager. The environment is a weighted undirected graph with live congestion metrics, updated continuously by the agents to enable shared perception. Our results show LLM-driven agents can dynamically adapt to traffic, reroute around congestion, and exhibit behaviors beyond static pathfinding, all while maintaining real-time performance. This work provides a reproducible framework for future research in adaptive routing and multi-agent cooperation.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [36] [Search-based Software Testing Driven by Domain Knowledge: Reflections and New Perspectives](https://arxiv.org/abs/2512.10079)
*Federico Formica,Mark Lawford,Claudio Menghi*

Main category: cs.SE

TL;DR: 本文回顾了结合领域知识的SBST实验，强调了意外结果，并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统的SBST虽能快速生成测试用例，但缺乏工程师的领域知识，导致测试效果有限，因此需要将领域知识整合进SBST以提升测试质量。

Method: 本文通过实验证明领域知识驱动的SBST效果，分析了不同技术方案的表现与局限。

Result: 本文反思了基于搜索的软件测试（SBST）结合领域知识的最新实验成果，突出了其中大胆且出乎意料的结果。通过分析这些结果，作者重新审视了利用工程师领域知识驱动的SBST技术，指出当前方法的局限性和不足。基于这些洞察，文章提出了未来研究的新方向，旨在提升SBST在实际应用中的效果。

Conclusion: 结合领域知识的SBST技术虽有进展，但仍存在挑战，未来应探索新的整合方法以提升测试效果。

Abstract: Search-based Software Testing (SBST) can automatically generate test cases to search for requirements violations. Unlike manual test case development, it can generate a substantial number of test cases in a limited time. However, SBST does not possess the domain knowledge of engineers. Several techniques have been proposed to integrate engineers' domain knowledge within existing SBST frameworks. This paper will reflect on recent experimental results by highlighting bold and unexpected results. It will help re-examine SBST techniques driven by domain knowledge from a new perspective, suggesting new directions for future research.

</details>


### [37] [ATLAS: Automated Toolkit for Large-Scale Verified Code Synthesis](https://arxiv.org/abs/2512.10173)
*Mantas Baksys,Stefan Zetzsche,Olivier Bouissou,Remi Delmas,Soonho Kong*

Main category: cs.SE

TL;DR: ATLAS自动合成验证程序，大幅提升了大型语言模型在程序验证任务的表现。


<details>
  <summary>Details</summary>
Motivation: 当前训练程序验证的大型语言模型受限于验证代码稀缺，亟需高质量验证程序数据以提升验证能力。

Method: 构建ATLAS自动化流水线，生成带规范与证明的Dafny程序，并将程序拆分为多个子任务生成训练样本，用于微调大型语言模型。

Result: 该论文提出了ATLAS，一个自动化生成经过验证的程序的流水线，旨在解决缺乏验证代码训练数据的问题。ATLAS通过生成包含规范、实现及证明的完整Dafny程序，合成了2700个经过验证的程序，提取出超过19000个训练样本。基于这些样本微调Qwen 2.5 7B Coder模型，在DafnyBench和DafnySynthesis上分别实现了显著提升，增加了23和50个百分点，证明了合成验证代码能显著增强大型语言模型在形式验证领域的能力。

Conclusion: 合成的验证程序数据能有效提升大型语言模型在形式程序验证任务中的性能。

Abstract: Large language models have shown potential for program verification, but progress is hindered by the scarcity of verified code for training. We present ATLAS, an automated pipeline that synthesizes verified programs at scale to address this data bottleneck. ATLAS generates complete Dafny programs with specifications, implementations, and proofs, producing 2.7K verified programs from which we extract over 19K training examples--more than 7 per verified program--by decomposing the synthesis process into multiple specialized tasks. Fine-tuning Qwen 2.5 7B Coder on this dataset produces substantial gains: +23 percentage points on DafnyBench and +50 percentage points on DafnySynthesis. These results demonstrate that synthetic verified code can effectively enhance LLM capabilities for formal verification.

</details>


### [38] [Does SWE-Bench-Verified Test Agent Ability or Model Memory?](https://arxiv.org/abs/2512.10218)
*Thanosan Prathifkumar,Noble Saji Mathews,Meiyappan Nagappan*

Main category: cs.SE

TL;DR: SWE-Bench-Verified数据集与训练数据重叠，导致模型表现被高估，不适合作为真实问题解决能力的评测标准。


<details>
  <summary>Details</summary>
Motivation: 研究发现SWE-Bench-Verified数据集可能与模型训练数据重叠，导致评测结果反映的是训练召回能力而非实际解决问题的能力。

Method: 通过让模型仅使用问题文本及文件路径两种信息定位相关文件，比较模型在不同数据集上的表现，以检测数据集是否被模型训练过。

Result: 在SWE-Bench-Verified上模型表现明显优于其他两个数据集，特别是在定位被编辑文件方面表现提升了6倍，表明模型可能看过该数据集的许多任务。

Conclusion: SWE-Bench-Verified基准可能不能真实反映模型解决实际软件问题的能力，依赖此类旧数据集存在风险，应转向考虑数据污染问题的新数据集。

Abstract: SWE-Bench-Verified, a dataset comprising 500 issues, serves as a de facto benchmark for evaluating various large language models (LLMs) on their ability to resolve GitHub issues. But this benchmark may overlap with model training data. If that is true, scores may reflect training recall, not issue-solving skill. To study this, we test two Claude models that frequently appear in top-performing agents submitted to the benchmark. We ask them to find relevant files using only issue text, and then issue text plus file paths. We then run the same setup on BeetleBox and SWE-rebench. Despite both benchmarks involving popular open-source Python projects, models performed 3 times better on SWE-Bench-Verified. They were also 6 times better at finding edited files, without any additional context about the projects themselves. This gap suggests the models may have seen many SWE-Bench-Verified tasks during training. As a result, scores on this benchmark may not reflect an agent's ability to handle real software issues, yet it continues to be used in ways that can misrepresent progress and lead to choices that favour agents that use certain models over strong agent design. Our setup tests the localization step with minimal context to the extent that the task should be logically impossible to solve. Our results show the risk of relying on older popular benchmarks and support the shift toward newer datasets built with contamination in mind.

</details>


### [39] [Studying and Automating Issue Resolution for Software Quality](https://arxiv.org/abs/2512.10238)
*Antu Saha*

Main category: cs.SE

TL;DR: 本文通过提升问题报告质量、分析开发者工作流程和自动化复杂解决任务，推动了AI辅助的软件问题解决技术，促进软件质量提高。


<details>
  <summary>Details</summary>
Motivation: 软件问题解决对维护软件质量至关重要，但开发者常面临低质量问题报告、对真实工作流程理解有限和缺乏自动化支持等挑战。

Method: 采用三方面方法：1）利用大型语言模型(LLM)推理及特定应用信息提升问题报告质量；2）实证研究传统与AI增强系统中的开发者工作流程；3）通过机器学习(ML)、深度学习(DL)和LLM技术自动化复杂问题解决任务，如定位UI缺陷和方案识别。

Result: 提升了问题报告质量，深入理解了开发者工作流程，开发了自动化解决工具，实现了更高效的问题解决流程。

Conclusion: 通过实证研究和自动化方法，推动了AI驱动的问题解决技术发展，有助于构建可维护性更强、高质量的软件系统。

Abstract: Effective issue resolution is crucial for maintaining software quality. Yet developers frequently encounter challenges such as low-quality issue reports, limited understanding of real-world workflows, and a lack of automated support. This research aims to address these challenges through three complementary directions. First, we enhance issue report quality by proposing techniques that leverage LLM reasoning and application-specific information. Second, we empirically characterize developer workflows in both traditional and AI-augmented systems. Third, we automate cognitively demanding resolution tasks, including buggy UI localization and solution identification, through ML, DL, and LLM-based approaches. Together, our work delivers empirical insights, practical tools, and automated methods to advance AI-driven issue resolution, supporting more maintainable and high-quality software systems.

</details>


### [40] [Cross-modal Retrieval Models for Stripped Binary Analysis](https://arxiv.org/abs/2512.10393)
*Guoqiang Chen,Lingyun Ying,Ziyang Song,Daguang Liu,Qiang Wang,Zhiqi Wang,Li Hu,Shaoyin Cheng,Weiming Zhang,Nenghai Yu*

Main category: cs.SE

TL;DR: 本文提出了BinSeek，一种用于剥离二进制代码分析的两阶段跨模态检索框架，通过训练双模型实现代码与自然语言描述的语义关联，显著提升了二进制代码检索性能。


<details>
  <summary>Details</summary>
Motivation: 当前剥离二进制函数检索难以处理，尤其缺乏符号信息使得传统源码检索方法难以应用，亟需有效的跨模态检索方案。

Method: 提出两阶段跨模态检索框架，包括BinSeekEmbedding模型学习二进制代码与自然语言的语义关联，以及BinSeek-Reranker利用上下文增强对候选代码的相关性判断。同时构建了基于LLM的数据自动合成流水线用于训练，且建立了领域基准数据集。

Result: BinSeek在多项指标上领先现有同类模型大幅提升，包括Rec@3和MRR@3，显示出强大的语义理解和检索能力。

Conclusion: BinSeek在剥离二进制代码检索中取得了最先进的性能指标，优于同等规模模型31.42%（Rec@3）和27.17%（MRR@3），并超过了参数量大16倍的通用模型。

Abstract: LLM-agent based binary code analysis has demonstrated significant potential across a wide range of software security scenarios, including vulnerability detection, malware analysis, etc. In agent workflow, however, retrieving the positive from thousands of stripped binary functions based on user query remains under-studied and challenging, as the absence of symbolic information distinguishes it from source code retrieval. In this paper, we introduce, BinSeek, the first two-stage cross-modal retrieval framework for stripped binary code analysis. It consists of two models: BinSeekEmbedding is trained on large-scale dataset to learn the semantic relevance of the binary code and the natural language description, furthermore, BinSeek-Reranker learns to carefully judge the relevance of the candidate code to the description with context augmentation. To this end, we built an LLM-based data synthesis pipeline to automate training construction, also deriving a domain benchmark for future research. Our evaluation results show that BinSeek achieved the state-of-the-art performance, surpassing the the same scale models by 31.42% in Rec@3 and 27.17% in MRR@3, as well as leading the advanced general-purpose models that have 16 times larger parameters.

</details>


### [41] [How to Trick Your AI TA: A Systematic Study of Academic Jailbreaking in LLM Code Evaluation](https://arxiv.org/abs/2512.10415)
*Devanshu Sahoo,Vasudev Majhi,Arjun Neekhra,Yash Sinha,Murari Mandal,Dhruv Kumar*

Main category: cs.SE

TL;DR: 本文针对学术环境中大语言模型自动代码评判的安全性，提出并系统性评估了多种绕过攻击方法，构建了对抗样本数据集和评估指标，为后续提升评判系统的鲁棒性奠定基础。


<details>
  <summary>Details</summary>
Motivation: 目前大规模语言模型作为自动代码评判工具在学术环境中广泛应用，但学生可能利用对抗性提示策略进行误判，获取不公平的学术优势，导致模型评判的可靠性受损。

Method: 系统适配20多种绕过（jailbreaking）策略，定义学术绕过攻击，构建针对学术代码评判的带毒数据集，设计三维绕过评测指标，在六款大语言模型上进行综合评估。

Result: 本文首次进行大规模学术环境中基于大模型代码评判系统的绕过攻击研究，开发出20余种绕过策略，构建了包含2.5万条对抗性提交的数据集，定义了三种绕过指标，并在六款大模型上进行评估，发现模型在说服力及角色扮演攻击下绕过成功率高达97%。

Conclusion: 大模型代码评判系统在学术应用中存在显著安全风险，尤其易受到说服和角色扮演类攻击。研究提出的数据和指标为未来提升代码评判系统的防护能力提供重要支持。

Abstract: The use of Large Language Models (LLMs) as automatic judges for code evaluation is becoming increasingly prevalent in academic environments. But their reliability can be compromised by students who may employ adversarial prompting strategies in order to induce misgrading and secure undeserved academic advantages. In this paper, we present the first large-scale study of jailbreaking LLM-based automated code evaluators in academic context. Our contributions are: (i) We systematically adapt 20+ jailbreaking strategies for jailbreaking AI code evaluators in the academic context, defining a new class of attacks termed academic jailbreaking. (ii) We release a poisoned dataset of 25K adversarial student submissions, specifically designed for the academic code-evaluation setting, sourced from diverse real-world coursework and paired with rubrics and human-graded references, and (iii) In order to capture the multidimensional impact of academic jailbreaking, we systematically adapt and define three jailbreaking metrics (Jailbreak Success Rate, Score Inflation, and Harmfulness). (iv) We comprehensively evalulate the academic jailbreaking attacks using six LLMs. We find that these models exhibit significant vulnerability, particularly to persuasive and role-play-based attacks (up to 97% JSR). Our adversarial dataset and benchmark suite lay the groundwork for next-generation robust LLM-based evaluators in academic code assessment.

</details>


### [42] [UniCoR: Modality Collaboration for Robust Cross-Language Hybrid Code Retrieval](https://arxiv.org/abs/2512.10452)
*Yang Yang,Li Kuang,Jiakun Liu,Zhongxin Liu,Yingjie Xia,David Lo*

Main category: cs.SE

TL;DR: 本文提出UniCoR，通过多视角对比学习和分布一致性学习，有效提升跨语言混合代码检索的语义理解与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有代码检索模型在混合查询和跨语言场景中的语义理解不足、融合效率低以及泛化能力弱。

Method: 提出了UniCoR框架，包括多视角有监督对比学习模块和表示分布一致性学习模块，实现了语义理解增强、混合模态融合和跨语言特征对齐。

Result: 在多个基准测试上，UniCoR相比最优基线模型MRR提升8.64%，MAP提升11.54%，且在混合代码检索和跨语言场景表现稳定。

Conclusion: UniCoR显著提升了混合查询和跨语言代码检索的效果，解决了现有方法面临的三大挑战，展示了强鲁棒性和泛化能力。

Abstract: Effective code retrieval is indispensable and it has become an important paradigm to search code in hybrid mode using both natural language and code snippets. Nevertheless, it remains unclear whether existing approaches can effectively leverage such hybrid queries, particularly in cross-language contexts. We conduct a comprehensive empirical study of representative code models and reveal three challenges: (1) insufficient semantic understanding; (2) inefficient fusion in hybrid code retrieval; and (3) weak generalization in cross-language scenarios. To address these challenges, we propose UniCoR, a novel self-supervised framework that learns Unified Code Representations framework designed to learn unified and robust code representations. Firstly, we design a multi-perspective supervised contrastive learning module to enhance semantic understanding and modality fusion. It aligns representations from multiple perspectives, including code-to-code, natural language-to-code, and natural language-to-natural language, enforcing the model to capture a semantic essence among modalities. Secondly, we introduce a representation distribution consistency learning module to improve cross-language generalization, which explicitly aligns the feature distributions of different programming languages, enabling language-agnostic representation learning. Extensive experiments on both empirical benchmark and large-scale benchmark show that UniCoR outperforms all baseline models, achieving an average improvement of 8.64% in MRR and 11.54% in MAP over the best-performing baseline. Furthermore, UniCoR exhibits stability in hybrid code retrieval and generalization capability in cross-language scenarios.

</details>


### [43] [Decoding Human-LLM Collaboration in Coding: An Empirical Study of Multi-Turn Conversations in the Wild](https://arxiv.org/abs/2512.10493)
*Binquan Zhang,Li Zhang,Haoyuan Zhang,Fang Liu,Song Wang,Bo Shen,An Fu,Lin Shi*

Main category: cs.SE

TL;DR: 本文通过两个真实对话数据集分析了编程场景下人机协作，发现不同任务导致交互模式多样，LLM在代码修复等任务中指令执行难度大，用户满意度与任务类型相关，提出改进界面和用户体验建议。


<details>
  <summary>Details</summary>
Motivation: 尽管已有数据集记录了人机对话，但针对编程场景中人机协作的机制研究较少，本研究旨在填补该空白，理解用户互动路径、LLM执行指令能力及用户体验。

Method: 基于LMSYS-Chat-1M和WildChat两个大型真实用户-LLM对话数据集，进行实证分析以探讨人机协作机制、LLM指令遵守度及用户满意度。

Result: 发现任务类型决定了三种主要交互模式（线性、星型、树形），不同任务对LLM的指令遵守能力产生不同影响，且用户满意度在不同任务间存在明显差异。

Conclusion: 本研究揭示了不同任务类型下人机协作的交互模式差异，指出LLM在遵循编程相关指令时仍面临挑战，并且用户满意度受任务性质显著影响。

Abstract: Large language models (LLMs) are increasingly acting as dynamic conversational interfaces, supporting multi-turn interactions that mimic human-like conversation and facilitate complex tasks like coding. While datasets such as LMSYS-Chat-1M and WildChat capture real-world user-LLM conversations, few studies systematically explore the mechanisms of human-LLM collaboration in coding scenarios. What tortuous paths do users experience during the interaction process? How well do the LLMs follow instructions? Are users satisfied? In this paper, we conduct an empirical analysis on human-LLM coding collaboration using LMSYS-Chat-1M and WildChat datasets to explore the human-LLM collaboration mechanism, LLMs' instruction following ability, and human satisfaction. This study yields interesting findings: 1) Task types shape interaction patterns(linear, star and tree), with code quality optimization favoring linear patterns, design-driven tasks leaning toward tree structures, and queries preferring star patterns; 2) Bug fixing and code refactoring pose greater challenges to LLMs' instruction following, with non-compliance rates notably higher than in information querying; 3) Code quality optimization and requirements-driven development tasks show lower user satisfaction, whereas structured knowledge queries and algorithm designs yield higher levels. These insights offer recommendations for improving LLM interfaces and user satisfaction in coding collaborations, while highlighting avenues for future research on adaptive dialogue systems. We believe this work broadens understanding of human-LLM synergies and supports more effective AI-assisted development.

</details>


### [44] [Analyzing developer discussions on EU and US privacy legislation compliance in GitHub repositories](https://arxiv.org/abs/2512.10618)
*Georgia M. Kapitsaki,Maria Papoutsoglou,Christoph Treude,Ioanna Theophilou*

Main category: cs.SE

TL;DR: 通过对GitHub开源项目中关于隐私法规合规讨论的实证分析，揭示开发者关注的重点用户权利和问题类别，构建了用于指导实践和教育的分类体系。


<details>
  <summary>Details</summary>
Motivation: 隐私法规如GDPR和CCPA对软件开发影响重大，但缺乏针对开源软件开发者如何讨论合规问题的实证研究，本文旨在填补这一研究空白，帮助理解和指导合规实践。

Method: 本文通过数据挖掘和分析GitHub中32,820条相关issue，结合自动识别法律用户权利及手动分类1,186条样本issue，系统分析开发者讨论内容和关注点。

Result: 本文通过分析GitHub上32,820条开源软件开发者关于隐私立法（如GDPR和CCPA）合规性的问题讨论，识别了开发者关注的重点和内容。研究采用自动和手动分析相结合的方法，将讨论内容归纳为六大类（功能/漏洞、同意相关、文档、数据存储/共享、适应性和整体合规），并发现开发者主要关注用户删除权、选择退出权和访问权，重点讨论用户同意和权限功能、漏洞及cookies管理等问题。

Conclusion: 本文构建的分类体系能够帮助软件开发者优先解决与隐私法规合规相关的问题，同时为教育和研究提供方向，以促进数据隐私法规的执行和改进。

Abstract: Context: Privacy legislation has impacted the way software systems are developed, prompting practitioners to update their implementations. Specifically, the EU General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) have forced the community to focus on users' data privacy. Despite the vast amount of data on developer issues available in GitHub repositories, there is a lack of empirical evidence on the issues developers of Open Source Software discuss to comply with privacy legislation. Method: In this work, we examine such discussions by mining and analyzing 32,820 issues from GitHub repositories. We partially analyzed the dataset automatically to identify law user rights and principles indicated, and manually analyzed a sample of 1,186 issues based on the type of concern addressed. Results: We devised 24 discussion categories placed in six clusters: features/bugs, consent-related, documentation, data storing/sharing, adaptability, and general compliance. Our results show that developers mainly focus on specific user rights from the legislation (right to erasure, right to opt-out, right to access), addressing other rights less frequently, while most discussions concern user consent, user rights functionality, bugs and cookies management. Conclusion: The created taxonomy can help practitioners understand which issues are discussed for law compliance, so that they ensure they address them first in their systems. In addition, the educational community can reshape curricula to better educate future engineers on the privacy law concerns raised, and the research community can identify gaps and areas for improvement to support and accelerate data privacy law compliance.

</details>


### [45] [PACIFIC: a framework for generating benchmarks to check Precise Automatically Checked Instruction Following In Code](https://arxiv.org/abs/2512.10713)
*Itay Dreyfuss,Antonio Abu Nassar,Samuel Ackerman,Axel Ben David,Rami Katan,Orna Raz,Marcel Zalmanovici*

Main category: cs.SE

TL;DR: PACIFIC框架自动生成严格评测LLM代码指令执行和干运行能力的基准测试，验证多模型表现，提供抗污染且可控难度的评测方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLM）代码助手需要具备准确遵循用户指令的能力，且现有评测方法受限于工具使用或代理行为，难以有效衡量模型内在的代码推理和指令执行能力。

Method: 提出了PACIFIC框架，自动生成不同难度级别的基准测试，能严格评估模型的顺序指令遵循和代码干运行（dry running）能力，且支持通过简单输出对比进行可靠评估，并能避免训练数据污染。

Result: 通过生成多套不同难度的基准测试，评估了多款先进LLM，验证了PACIFIC能有效区分模型在指令遵循和代码干运行能力上的表现，且测试难度可控。

Conclusion: PACIFIC提供了一种可扩展且抗数据污染的方法，能有效评估LLM在代码相关任务中的核心能力，推动了模型能力的精细化测试与提升。

Abstract: Large Language Model (LLM)-based code assistants have emerged as a powerful application of generative AI, demonstrating impressive capabilities in code generation and comprehension. A key requirement for these systems is their ability to accurately follow user instructions. We present Precise Automatically Checked Instruction Following In Code (PACIFIC), a novel framework designed to automatically generate benchmarks that rigorously assess sequential instruction-following and code dry-running capabilities in LLMs, while allowing control over benchmark difficulty. PACIFIC produces benchmark variants with clearly defined expected outputs, enabling straightforward and reliable evaluation through simple output comparisons. In contrast to existing approaches that often rely on tool usage or agentic behavior, our work isolates and evaluates the LLM's intrinsic ability to reason through code behavior step-by-step without execution (dry running) and to follow instructions. Furthermore, our framework mitigates training data contamination by facilitating effortless generation of novel benchmark variations. We validate our framework by generating a suite of benchmarks spanning a range of difficulty levels and evaluating multiple state-of-the-art LLMs. Our results demonstrate that PACIFIC can produce increasingly challenging benchmarks that effectively differentiate instruction-following and dry running capabilities, even among advanced models. Overall, our framework offers a scalable, contamination-resilient methodology for assessing core competencies of LLMs in code-related tasks.

</details>


### [46] [Zorya: Automated Concolic Execution of Single-Threaded Go Binaries](https://arxiv.org/abs/2512.10799)
*Karolina Gorna,Nicolas Iooss,Yannick Seurin,Rida Khatoun*

Main category: cs.SE

TL;DR: 本文提出了一种基于Zorya的针对Go漏洞检测的混合符号执行方法，通过路径过滤和函数模式分析显著提升检测效率和准确性。


<details>
  <summary>Details</summary>
Motivation: Go语言在关键基础设施中的广泛采用带来了系统性漏洞检测的需求，而现有符号执行工具由于运行时复杂性和扩展性不足，难以有效处理Go二进制。

Method: 基于Zorya的混合符号执行框架，将Go二进制转为Ghidra的P-Code中间表示，检测未执行路径中的漏洞，并通过多层过滤机制聚焦于panic相关路径。

Result: panic可达性的门控机制提升了1.8-3.9倍的速度，过滤了33-70%的分支，Zorya能检测所有panic类型漏洞，且函数级分析相比从main函数开始快两个数量级。

Conclusion: 专门设计的混合符号执行技术能够在具有运行时安全检查的语言生态中实现实用的漏洞检测。

Abstract: Go's adoption in critical infrastructure intensifies the need for systematic vulnerability detection, yet existing symbolic execution tools struggle with Go binaries due to runtime complexity and scalability challenges. In this work, we build upon Zorya, a concolic execution framework that translates Go binaries to Ghidra's P-Code intermediate representation to address these challenges. We added the detection of bugs in concretely not taken paths and a multi-layer filtering mechanism to concentrate symbolic reasoning on panic-relevant paths. Evaluation on five Go vulnerabilities demonstrates that panic-reachability gating achieves 1.8-3.9x speedups when filtering 33-70% of branches, and that Zorya detects all panics while existing tools detect at most two. Function-mode analysis proved essential for complex programs, running roughly two orders of magnitude faster than starting from main. This work establishes that specialized concolic execution can achieve practical vulnerability detection in language ecosystems with runtime safety checks.

</details>
