<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 83]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.SE](#cs.SE) [Total: 20]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [WearVox: An Egocentric Multichannel Voice Assistant Benchmark for Wearables](https://arxiv.org/abs/2601.02391)
*Zhaojiang Lin,Yong Xu,Kai Sun,Jing Zheng,Yin Huang,Surya Teja Appini,Krish Narang,Renjie Tao,Ishan Kapil Jain,Siddhant Arora,Ruizhi Li,Yiteng Huang,Kaushik Patnaik,Wenfang Xu,Suwon Shon,Yue Liu,Ahmed A Aly,Anuj Kumar,Florian Metze,Xin Luna Dong*

Main category: cs.CL

TL;DR: WearVox 是首个针对可穿戴设备语音助手的基准测试，包含多任务、多环境的真实音频数据，评估语音大语言模型在复杂场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试多关注清晰或通用对话音频，忽略了可穿戴设备语音助手面临的运动噪声、快速交互和环境区分等现实挑战。

Method: 收集3,842条由AI眼镜录制的多通道、第一人称视角音频，覆盖五个任务和多种声学环境，并基于这些数据对多款主流和开源语音大语言模型进行评测，同时进行单通道与多通道音频输入的案例研究。

Result: 多数实时语音大语言模型在WearVox上的准确率介于29%到59%，特别在嘈杂的户外环境表现显著下降，多通道音频输入显著提升了模型对噪声的鲁棒性和设备语音与背景语音的区分能力。

Conclusion: 空间音频信号对上下文感知的语音助手至关重要，WearVox为推动可穿戴语音AI研究提供了全面且真实的测试平台。

Abstract: Wearable devices such as AI glasses are transforming voice assistants into always-available, hands-free collaborators that integrate seamlessly with daily life, but they also introduce challenges like egocentric audio affected by motion and noise, rapid micro-interactions, and the need to distinguish device-directed speech from background conversations. Existing benchmarks largely overlook these complexities, focusing instead on clean or generic conversational audio. To bridge this gap, we present WearVox, the first benchmark designed to rigorously evaluate voice assistants in realistic wearable scenarios. WearVox comprises 3,842 multi-channel, egocentric audio recordings collected via AI glasses across five diverse tasks including Search-Grounded QA, Closed-Book QA, Side-Talk Rejection, Tool Calling, and Speech Translation, spanning a wide range of indoor and outdoor environments and acoustic conditions. Each recording is accompanied by rich metadata, enabling nuanced analysis of model performance under real-world constraints. We benchmark leading proprietary and open-source speech Large Language Models (SLLMs) and find that most real-time SLLMs achieve accuracies on WearVox ranging from 29% to 59%, with substantial performance degradation on noisy outdoor audio, underscoring the difficulty and realism of the benchmark. Additionally, we conduct a case study with two new SLLMs that perform inference with single-channel and multi-channel audio, demonstrating that multi-channel audio inputs significantly enhance model robustness to environmental noise and improve discrimination between device-directed and background speech. Our results highlight the critical importance of spatial audio cues for context-aware voice assistants and establish WearVox as a comprehensive testbed for advancing wearable voice AI research.

</details>


### [2] [PCEval: A Benchmark for Evaluating Physical Computing Capabilities of Large Language Models](https://arxiv.org/abs/2601.02404)
*Inpyo Song,Eunji Jeon,Jangwon Lee*

Main category: cs.CL

TL;DR: 本文介绍了首个针对物理计算领域的大型语言模型（LLMs）自动评估基准PCEval，用于评估其在电路设计和代码生成的能力，发现LLMs在物理面板布局方面存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在物理计算领域的效果尚未充分探究，尤其是在需交互硬件的物理计算中存在硬件约束。

Method: 提出PCEval基准，自动评估LLMs在生成电路和兼容代码能力的逻辑和物理层面表现，无需人工评估，并测试了13个主流模型。

Result: 发现LLMs在代码生成和电路逻辑设计上表现良好，但在物理面板布局尤其是引脚连接和电路错误管理方面表现较差。

Conclusion: PCEval推动了对AI辅助硬件计算的理解，为物理计算教育和工具开发提供了基础。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains, including software development, education, and technical assistance. Among these, software development is one of the key areas where LLMs are increasingly adopted. However, when hardware constraints are considered-for instance, in physical computing, where software must interact with and control physical hardware -their effectiveness has not been fully explored. To address this gap, we introduce \textsc{PCEval} (Physical Computing Evaluation), the first benchmark in physical computing that enables a fully automatic evaluation of the capabilities of LLM in both the logical and physical aspects of the projects, without requiring human assessment. Our evaluation framework assesses LLMs in generating circuits and producing compatible code across varying levels of project complexity. Through comprehensive testing of 13 leading models, \textsc{PCEval} provides the first reproducible and automatically validated empirical assessment of LLMs' ability to reason about fundamental hardware implementation constraints within a simulation environment. Our findings reveal that while LLMs perform well in code generation and logical circuit design, they struggle significantly with physical breadboard layout creation, particularly in managing proper pin connections and avoiding circuit errors. \textsc{PCEval} advances our understanding of AI assistance in hardware-dependent computing environments and establishes a foundation for developing more effective tools to support physical computing education.

</details>


### [3] [EvoRoute: Experience-Driven Self-Routing LLM Agent Systems](https://arxiv.org/abs/2601.02695)
*Guibin Zhang,Haiyang Yu,Kaiming Yang,Bingli Wu,Fei Huang,Yongbin Li,Shuicheng Yan*

Main category: cs.CL

TL;DR: 本文提出了EvoRoute，一种自我进化的模型路由方案，用于在复杂智能体AI系统中动态选择最优大语言模型，以平衡性能、成本和延迟。


<details>
  <summary>Details</summary>
Motivation: 当前复杂的智能体AI系统在多任务上表现出色，但面临高经济成本和长延迟的三难困境，即性能、成本与速度之间的权衡。

Method: EvoRoute通过利用不断扩展的经验知识库，动态地在每一步选择帕累托最优的大语言模型骨干，并通过环境反馈持续优化选择策略。

Result: 在GAIA和BrowseComp+等智能体基准测试中，EvoRoute一方面保持或提升系统性能，另一方面显著降低执行成本（最多80%）和延迟（超过70%）。

Conclusion: EvoRoute有效解决了智能体系统中的性能、成本与延迟三难问题，实现了更高效且经济的智能体系统运行。

Abstract: Complex agentic AI systems, powered by a coordinated ensemble of Large Language Models (LLMs), tool and memory modules, have demonstrated remarkable capabilities on intricate, multi-turn tasks. However, this success is shadowed by prohibitive economic costs and severe latency, exposing a critical, yet underexplored, trade-off. We formalize this challenge as the \textbf{Agent System Trilemma}: the inherent tension among achieving state-of-the-art performance, minimizing monetary cost, and ensuring rapid task completion. To dismantle this trilemma, we introduce EvoRoute, a self-evolving model routing paradigm that transcends static, pre-defined model assignments. Leveraging an ever-expanding knowledge base of prior experience, EvoRoute dynamically selects Pareto-optimal LLM backbones at each step, balancing accuracy, efficiency, and resource use, while continually refining its own selection policy through environment feedback. Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\%$ and latency by over $70\%$.

</details>


### [4] [Losses that Cook: Topological Optimal Transport for Structured Recipe Generation](https://arxiv.org/abs/2601.02531)
*Mattia Ottoborgo,Daniele Rege Cambrin,Paolo Garza*

Main category: cs.CL

TL;DR: 本文提出了一种新的拓扑损失函数，用于提高烹饪食谱生成中成分准确率和程序一致性，结合多目标优化显著提升了生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前食谱生成训练主要基于交叉熵损失，注重文本流畅性，忽视了食材配比、时间温度等关键烹饪要素的准确表达。

Method: 基于RECIPE-NLG，设计了拓扑损失函数使成分列表在嵌入空间表现为点云，最小化预测与真实成分间的差异；结合Dice损失和混合损失优化时间温度以及数量的表现。

Result: 新设计的拓扑损失显著提升了成分和操作相关的评价指标，Dice损失在时间温度精度上表现优异，混合损失在数量和时间上取得良好平衡。

Conclusion: 综合多目标优化和拓扑损失，模型在自动生成食谱的实用性和准确性上明显优于传统方法，人类偏好分析也验证了模型的优越性。

Abstract: Cooking recipes are complex procedures that require not only a fluent and factual text, but also accurate timing, temperature, and procedural coherence, as well as the correct composition of ingredients. Standard training procedures are primarily based on cross-entropy and focus solely on fluency. Building on RECIPE-NLG, we investigate the use of several composite objectives and present a new topological loss that represents ingredient lists as point clouds in embedding space, minimizing the divergence between predicted and gold ingredients. Using both standard NLG metrics and recipe-specific metrics, we find that our loss significantly improves ingredient- and action-level metrics. Meanwhile, the Dice loss excels in time/temperature precision, and the mixed loss yields competitive trade-offs with synergistic gains in quantity and time. A human preference analysis supports our finding, showing our model is preferred in 62% of the cases.

</details>


### [5] [Image, Word and Thought: A More Challenging Language Task for the Iterated Learning Model](https://arxiv.org/abs/2601.02911)
*Hyoyeon Lee,Seth Bullock,Conor Houghton*

Main category: cs.CL

TL;DR: 该论文使用迭代学习模型，结合半监督自编码器，研究了语言从一代到下一代的传递机制，尤其在更复杂的语义信号空间（七段显示图像）上的应用，发现语言在表达力、组合性和稳定性方面均有所展现。


<details>
  <summary>Details</summary>
Motivation: 探究语言传递中的限制如何促进语言结构的出现，尤其在复杂意义空间中理解语言的表达性、组合性和稳定性。

Method: 采用半监督迭代学习模型，将监督与无监督学习结合在自编码器架构中，模拟语言学习者如何在限制条件下学习和传递语言。

Result: 模型成功应用于复杂的七段显示图像通信任务，学习到的语言能够为128个字形分配唯一代码，且信号和意义成分对应稳定，不随代际变化。

Conclusion: 结合半监督迭代学习的模型有助于理解复杂语义空间中语言结构的形成，语言显示出明确的表达力、组合性及代际稳定性。

Abstract: The iterated learning model simulates the transmission of language from generation to generation in order to explore how the constraints imposed by language transmission facilitate the emergence of language structure. Despite each modelled language learner starting from a blank slate, the presence of a bottleneck limiting the number of utterances to which the learner is exposed can lead to the emergence of language that lacks ambiguity, is governed by grammatical rules, and is consistent over successive generations, that is, one that is expressive, compositional and stable. The recent introduction of a more computationally tractable and ecologically valid semi supervised iterated learning model, combining supervised and unsupervised learning within an autoencoder architecture, has enabled exploration of language transmission dynamics for much larger meaning-signal spaces. Here, for the first time, the model has been successfully applied to a language learning task involving the communication of much more complex meanings: seven-segment display images. Agents in this model are able to learn and transmit a language that is expressive: distinct codes are employed for all 128 glyphs; compositional: signal components consistently map to meaning components, and stable: the language does not change from generation to generation.

</details>


### [6] [ModeX: Evaluator-Free Best-of-N Selection for Open-Ended Generation](https://arxiv.org/abs/2601.02535)
*Hyeong Kyu Choi,Sharon Li*

Main category: cs.CL

TL;DR: 提出了无需评估器的Best-of-N输出选择框架ModeX，通过构建相似度图和谱聚类识别语义共识输出，有效提升开放式文本生成任务的生成质量和效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在开放式任务中难以选出高质量的单一输出，现有方法依赖外部评估或精确匹配，适用性和效率受限。

Method: ModeX通过构建生成候选文本的相似度图并递归使用谱聚类来提取代表语义共识的模态输出，且无需额外推理或辅助模型。此外，提出ModeX-Lite，结合早期剪枝提升效率。

Result: 在文本摘要、代码生成和数学推理等开放式任务中，ModeX及其改进版本表现优于标准单路径和多路径基线，且计算效率更高。

Conclusion: ModeX提供了一种计算高效、无需评估器的多样本文本生成结果选择方案，有助于提高开放式文本生成的鲁棒性和质量。

Abstract: Selecting a single high-quality output from multiple stochastic generations remains a fundamental challenge for large language models (LLMs), particularly in open-ended tasks where no canonical answer exists. While Best-of-N and self-consistency methods show that aggregating multiple generations can improve performance, existing approaches typically rely on external evaluators, reward models, or exact string-match voting, limiting their applicability and efficiency. We propose Mode Extraction (ModeX), an evaluator-free Best-of-N selection framework that generalizes majority voting to open-ended text generation by identifying the modal output representing the dominant semantic consensus among generated texts. ModeX constructs a similarity graph over candidate generations and recursively applies spectral clustering to select a representative centroid, without requiring additional inference or auxiliary models. We further instantiate this selection principle as ModeX-Lite, an improved version of ModeX with early pruning for efficiency. Across open-ended tasks -- including text summarization, code generation, and mathematical reasoning -- our approaches consistently outperform standard single- and multi-path baselines, providing a computationally efficient solution for robust open-ended text generation. Code is released in https://github.com/deeplearning-wisc/ModeX.

</details>


### [7] [LoRA-Drop: Temporal LoRA Decoding for Efficient LLM Inference](https://arxiv.org/abs/2601.02569)
*Hossein Rajabzadeh,Maryam Dialameh,Chul B. Park,Il-Min Kim,Hyock Ju Kwon*

Main category: cs.CL

TL;DR: 提出了LoRA-Drop推理框架，通过时间计算调度和低秩LoRA校正来加速自回归大语言模型的解码，显著降低KV缓存使用，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统自回归大语言模型的顺序解码效率低，现有动态深度和跳层方法依赖辅助机制或导致准确率下降，需一种无需路由网络且高效的加速方案。

Method: LoRA-Drop对部分中间层施加时间计算调度：大多数步复用前一token隐藏状态并用低秩LoRA校正，周期性刷新执行完整模型，兼容标准KV缓存，减少缓存更新和空间。

Result: 在多种大模型（如LLaMA2、LLaMA3、Qwen）上，LoRA-Drop解码速度提升最高2.6倍，KV缓存减少45-55%，准确率仅下降0.5个百分点以内。多任务评测表明多种调度配置都在保证质量前提下显著提升效率。

Conclusion: LoRA-Drop提供了一种简单有效的自适应容量推理路径，无需额外路由网络即可在保持性能的同时大幅提高解码效率及减少缓存占用，适用于多种大语言模型。

Abstract: Autoregressive large language models (LLMs) are bottlenecked by sequential decoding, where each new token typically requires executing all transformer layers. Existing dynamic-depth and layer-skipping methods reduce this cost, but often rely on auxiliary routing mechanisms or incur accuracy degradation when bypassed layers are left uncompensated. We present \textbf{LoRA-Drop}, a plug-and-play inference framework that accelerates decoding by applying a \emph{temporal compute schedule} to a fixed subset of intermediate layers: on most decoding steps, selected layers reuse the previous-token hidden state and apply a low-rank LoRA correction, while periodic \emph{refresh} steps execute the full model to prevent drift. LoRA-Drop requires no routing network, is compatible with standard KV caching, and can reduce KV-cache footprint by skipping KV updates in droppable layers during LoRA steps and refreshing periodically. Across \textbf{LLaMA2-7B}, \textbf{LLaMA3-8B}, \textbf{Qwen2.5-7B}, and \textbf{Qwen2.5-14B}, LoRA-Drop achieves up to \textbf{2.6$\times$ faster decoding} and \textbf{45--55\% KV-cache reduction} while staying within \textbf{0.5 percentage points (pp)} of baseline accuracy. Evaluations on reasoning (GSM8K, MATH, BBH), code generation (HumanEval, MBPP), and long-context/multilingual benchmarks (LongBench, XNLI, XCOPA) identify a consistent \emph{safe zone} of scheduling configurations that preserves quality while delivering substantial efficiency gains, providing a simple path toward adaptive-capacity inference in LLMs. Codes are available at https://github.com/hosseinbv/LoRA-Drop.git.

</details>


### [8] [Fact-Checking with Large Language Models via Probabilistic Certainty and Consistency](https://arxiv.org/abs/2601.02574)
*Haoran Wang,Maryam Khalid,Qiong Wu,Jian Gao,Cheng Cao*

Main category: cs.CL

TL;DR: 提出了一种基于概率确定性和推理一致性的自适应事实核查框架，提高了大语言模型的事实准确度和检索效率。


<details>
  <summary>Details</summary>
Motivation: 现有事实核查方法忽视了大语言模型的内部知识，且在处理不确定性时缺乏针对性，容易引入无关噪声影响结果。

Method: 提出了概率确定性和一致性（PCC）框架，通过联合建模模型的概率确定性和推理一致性，估计事实信心，基于此实现自适应验证策略：自信时直接回答，不确定或不一致时触发定向检索，高度模糊时进行深入搜索。

Result: 在三个挑战性基准测试中，PCC在不确定性量化方面优于语言化置信度，且持续超越强大的基于LLM的事实核查基线，且具有良好的跨模型泛化能力。

Conclusion: PCC框架有效结合模型内知识和外部检索，提升了事实核查的准确性和效率，针对不确定性提供了合理的处理机制，具有广泛适用性。

Abstract: Large language models (LLMs) are increasingly used in applications requiring factual accuracy, yet their outputs often contain hallucinated responses. While fact-checking can mitigate these errors, existing methods typically retrieve external evidence indiscriminately, overlooking the model's internal knowledge and potentially introducing irrelevant noise. Moreover, current systems lack targeted mechanisms to resolve specific uncertainties in the model's reasoning. Inspired by how humans fact-check, we argue that LLMs should adaptively decide whether to rely on internal knowledge or initiate retrieval based on their confidence in a given claim. We introduce Probabilistic Certainty and Consistency (PCC), a framework that estimates factual confidence by jointly modeling an LLM's probabilistic certainty and reasoning consistency. These confidence signals enable an adaptive verification strategy: the model answers directly when confident, triggers targeted retrieval when uncertain or inconsistent, and escalates to deep search when ambiguity is high. Our confidence-guided routing mechanism ensures that retrieval is invoked only when necessary, improving both efficiency and reliability. Extensive experiments across three challenging benchmarks show that PCC achieves better uncertainty quantification than verbalized confidence and consistently outperforms strong LLM-based fact-checking baselines. Furthermore, we demonstrate that PCC generalizes well across various LLMs.

</details>


### [9] [DataParasite Enables Scalable and Repurposable Online Data Curation](https://arxiv.org/abs/2601.02578)
*Mengyi Sun*

Main category: cs.CL

TL;DR: 本文介绍了DataParasite，一个用于在线数据收集的开源模块化流水线，能够高效、可扩展地从异构网上资源自动采集结构化数据。


<details>
  <summary>Details</summary>
Motivation: 计算社会科学中数据集多来自异构网络来源，手工收集劳动强度大、成本高且难以复现，现有大语言模型系统不够透明灵活，不适合科学数据整理。

Method: DataParasite将表格整理任务分解为独立的基于实体的搜索，由轻量级配置文件定义并通过通用Python脚本执行，支持仅用自然语言指令完成无预定义实体列表的任务。

Result: 该流水线在多个计算社会科学经典任务（如教职招聘历史、精英死亡事件、政治生涯轨迹）中表现出高准确率，数据收集成本比人工整理降低一个数量级。

Conclusion: DataParasite显著降低了在线数据组装的技术和劳动门槛，为可扩展、透明和可重用的科学数据整理提供了实用基础。

Abstract: Many questions in computational social science rely on datasets assembled from heterogeneous online sources, a process that is often labor-intensive, costly, and difficult to reproduce. Recent advances in large language models enable agentic search and structured extraction from the web, but existing systems are frequently opaque, inflexible, or poorly suited to scientific data curation. Here we introduce DataParasite, an open-source, modular pipeline for scalable online data collection. DataParasite decomposes tabular curation tasks into independent, entity-level searches defined through lightweight configuration files and executed through a shared, task-agnostic python script. Crucially, the same pipeline can be repurposed to new tasks, including those without predefined entity lists, using only natural-language instructions. We evaluate the pipeline on multiple canonical tasks in computational social science, including faculty hiring histories, elite death events, and political career trajectories. Across tasks, DataParasite achieves high accuracy while reducing data-collection costs by an order of magnitude relative to manual curation. By lowering the technical and labor barriers to online data assembly, DataParasite provides a practical foundation for scalable, transparent, and reusable data curation in computational social science and beyond.

</details>


### [10] [Reconstructing Item Characteristic Curves using Fine-Tuned Large Language Models](https://arxiv.org/abs/2601.02580)
*Christopher Ormerod*

Main category: cs.CL

TL;DR: 该论文提出一种利用大语言模型微调模拟学生回答以估计测验题目参数的新方法，无需昂贵的实地测试。


<details>
  <summary>Details</summary>
Motivation: 传统测定题目难度和区分度的方法依赖昂贵的实地测试收集学生数据，成本高且效率低。

Method: 基于Qwen-3密集模型和低秩适配（LoRA）技术，微调大语言模型模拟不同能力层次学生的多项选择题回答，生成合成的题目特征曲线（ICC），从而估计IRT参数。

Result: 在六年级英语语言艺术题目和BEA 2024共享任务数据集上的评测表明，该方法表现不逊于甚至优于传统基线，尤其在题目区分度建模方面效果显著。

Conclusion: 基于大语言模型模拟学生答题行为的IRT参数估计方法，为技能测评提供了成本效益更高、效果更好的新途径。

Abstract: Traditional methods for determining assessment item parameters, such as difficulty and discrimination, rely heavily on expensive field testing to collect student performance data for Item Response Theory (IRT) calibration. This study introduces a novel approach that implicitly models these psychometric properties by fine-tuning Large Language Models (LLMs) to simulate student responses across a spectrum of latent abilities. Leveraging the Qwen-3 dense model series and Low-Rank Adaptation (LoRA), we train models to generate responses to multiple choice questions conditioned on discrete ability descriptors. We reconstruct the probability of a correct response as a function of student ability, effectively generating synthetic Item Characteristic Curves (ICCs) to estimate IRT parameters. Evaluation on a dataset of Grade 6 English Language Arts (ELA) items and the BEA 2024 Shared Task dataset demonstrates that this method competes with or outperforms baseline approaches. This simulation-based technique seems particularly effective at modeling item discrimination.

</details>


### [11] [FlowPlan-G2P: A Structured Generation Framework for Transforming Scientific Papers into Patent Descriptions](https://arxiv.org/abs/2601.02589)
*Kris W Pan,Yongmin Yoo*

Main category: cs.CL

TL;DR: 提出了FlowPlan-G2P框架，将科研论文转化为专利描述，通过专家认知流程分三步进行，显著提升逻辑连贯性和法律合规性。


<details>
  <summary>Details</summary>
Motivation: 科研论文与专利描述在修辞风格和法律要求上差异显著，现有黑盒文本转换方法难以满足结构推理和法律约束需求。

Method: FlowPlan-G2P框架包括：1）概念图诱导，提取技术实体及关系构建有向图；2）段落与章节规划，将图重组为符合专利规范的内容簇；3）基于图的生成，运用子图和特定提示生成符合法律要求的段落。

Result: 实验证明FlowPlan-G2P在逻辑连贯性和法律合规性方面显著优于端到端大型语言模型基线。

Conclusion: 该框架开创了论文到专利文本生成的新范式，推动了专业领域结构化文本生成的发展。

Abstract: Over 3.5 million patents are filed annually, with drafting patent descriptions requiring deep technical and legal expertise. Transforming scientific papers into patent descriptions is particularly challenging due to their differing rhetorical styles and stringent legal requirements. Unlike black-box text-to-text approaches that struggle to model structural reasoning and legal constraints, we propose FlowPlan-G2P, a novel framework that mirrors the cognitive workflow of expert drafters by reformulating this task into three stages: (1) Concept Graph Induction, extracting technical entities and relationships into a directed graph via expert-like reasoning; (2) Paragraph and Section Planning, reorganizing the graph into coherent clusters aligned with canonical patent sections; and (3) Graph-Conditioned Generation, producing legally compliant paragraphs using section-specific subgraphs and tailored prompts. Experiments demonstrate that FlowPlan-G2P significantly improves logical coherence and legal compliance over end-to-end LLM baselines. Our framework establishes a new paradigm for paper-to-patent generation and advances structured text generation for specialized domains.

</details>


### [12] [Scalable Construction of a Lung Cancer Knowledge Base: Profiling Semantic Reasoning in LLMs](https://arxiv.org/abs/2601.02604)
*Cesar Felipe Martínez Cisneros,Jesús Ulises Quiroz Bautista,Claudia Anahí Guzmán Solano,Bogdan Kaleb García Rivera,Iván García Pacheco,Yalbi Itzel Balderas Martínez,Kolawole John Adebayoc,Ignacio Arroyo Fernández*

Main category: cs.CL

TL;DR: 本研究提出了一种基于OpenIE的方法构建肺癌领域知识库，以提升大语言模型在生物医学领域的微调效果。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在生物医学领域的表现依赖于训练语料的语义质量，尤其是在肿瘤学中对精准性和可解释性要求极高，因此需要可扩展的结构化知识库构建方法以支持有效微调。

Method: 使用MeSH词表识别医学概念，筛选开放许可的PubMed文献，通过OpenIE抽取主谓宾三元组，再结合实体识别技术（NER）丰富数据，最终用于微调T5模型。

Result: 在经过监督语义微调后，模型通过ROUGE和BERTScore评估，表现出显著提升的语义一致性和准确性。

Conclusion: 利用OpenIE技术构建的领域特定知识库为生物医学NLP提供了一种低成本且可扩展的微调资源，提升了大语言模型在该领域的性能。

Abstract: The integration of Large Language Models (LLMs) into biomedical research offers new opportunities for domainspecific reasoning and knowledge representation. However, their performance depends heavily on the semantic quality of training data. In oncology, where precision and interpretability are vital, scalable methods for constructing structured knowledge bases are essential for effective fine-tuning. This study presents a pipeline for developing a lung cancer knowledge base using Open Information Extraction (OpenIE). The process includes: (1) identifying medical concepts with the MeSH thesaurus; (2) filtering open-access PubMed literature with permissive licenses (CC0); (3) extracting (subject, relation, object) triplets using OpenIE method; and (4) enriching triplet sets with Named Entity Recognition (NER) to ensure biomedical relevance. The resulting triplet sets provide a domain-specific, large-scale, and noise-aware resource for fine-tuning LLMs. We evaluated T5 models finetuned on this dataset through Supervised Semantic Fine-Tuning. Comparative assessments with ROUGE and BERTScore show significantly improved performance and semantic coherence, demonstrating the potential of OpenIE-derived resources as scalable, low-cost solutions for enhancing biomedical NLP.

</details>


### [13] [Improved Evidence Extraction for Document Inconsistency Detection with LLMs](https://arxiv.org/abs/2601.02627)
*Nelvin Tan,Yaowen Zhang,James Asikin Cheung,Fusheng Liu,Yu-Ching Shih,Dong Yang*

Main category: cs.CL

TL;DR: 本文提出了基于大语言模型（LLMs）改进文档不一致检测的证据提取方法及框架，取得了良好实验效果。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在多个领域表现出色，但针对文档不一致检测的研究较少，尤其是针对提供不一致证据的部分。

Method: 提出了全面的证据提取指标及一种带有限制过滤的编辑重试框架，以提升基于LLM的文档不一致检测效果。

Result: 实验结果显示，所提方法在证据提取和不一致检测准确性上较直接提示法有显著提升。

Conclusion: 新框架有效增强了LLM在文档不一致检测中提供证据的能力，推动了相关领域的应用研究。

Abstract: Large language models (LLMs) are becoming useful in many domains due to their impressive abilities that arise from large training datasets and large model sizes. However, research on LLM-based approaches to document inconsistency detection is relatively limited. There are two key aspects of document inconsistency detection: (i) classification of whether there exists any inconsistency, and (ii) providing evidence of the inconsistent sentences. We focus on the latter, and introduce new comprehensive evidence-extraction metrics and a redact-and-retry framework with constrained filtering that substantially improves LLM-based document inconsistency detection over direct prompting. We back our claims with promising experimental results.

</details>


### [14] [Empirical Comparison of Encoder-Based Language Models and Feature-Based Supervised Machine Learning Approaches to Automated Scoring of Long Essays](https://arxiv.org/abs/2601.02659)
*Kuo Wang,Haowei Hua,Pengfei Yan,Hong Jiao,Dan Song*

Main category: cs.CL

TL;DR: 本文通过训练多种基于编码器的语言模型及其集成模型，实现了对长篇作文的自动评分，并发现基于多模型嵌入的集成梯度提升分类器显著优于单一模型。


<details>
  <summary>Details</summary>
Motivation: 长篇文本处理中，编码器语言模型受限于输入长度，挑战自动评分任务的准确性，因此需要探索更有效的评分方法。

Method: 训练并比较了多种编码器模型（BERT、RoBERTa、DistilBERT、DeBERTa）及基于这些模型的嵌入集成模型和特征集成梯度提升机，使用包含17307篇作文的数据集进行训练和评估。

Result: 基于多预训练语言模型嵌入的集成梯度提升分类器在长篇作文自动评分任务中性能显著优于单个语言模型。

Conclusion: 结合多个预训练编码器模型的嵌入与梯度提升分类器的集成方法，有效提升了长篇作文自动评分的表现，解决了长上下文处理的难题。

Abstract: Long context may impose challenges for encoder-only language models in text processing, specifically for automated scoring of essays. This study trained several commonly used encoder-based language models for automated scoring of long essays. The performance of these trained models was evaluated and compared with the ensemble models built upon the base language models with a token limit of 512?. The experimented models include BERT-based models (BERT, RoBERTa, DistilBERT, and DeBERTa), ensemble models integrating embeddings from multiple encoder models, and ensemble models of feature-based supervised machine learning models, including Gradient-Boosted Decision Trees, eXtreme Gradient Boosting, and Light Gradient Boosting Machine. We trained, validated, and tested each model on a dataset of 17,307 essays, with an 80%/10%/10% split, and evaluated model performance using Quadratic Weighted Kappa. This study revealed that an ensemble-of-embeddings model that combines multiple pre-trained language model representations with gradient-boosting classifier as the ensemble model significantly outperforms individual language models at scoring long essays.

</details>


### [15] [When Do Tools and Planning Help LLMs Think? A Cost- and Latency-Aware Benchmark](https://arxiv.org/abs/2601.02663)
*Subha Ghoshal,Ali Al-Bustami*

Main category: cs.CL

TL;DR: 本文评估了大型语言模型在推理时结合规划与外部工具的表现，使用事件问答和劝说回复两个任务进行对比，发现工具增强型方法在部分任务中显著提升准确率但带来更高延迟。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在推理能力上的需求提升，结合推理时规划和外部工具有望提高性能，但需要评估其效益与成本之间的权衡。

Method: 在事件问答和Reddit劝说回复两个真实任务上，使用LangChain和LangGraph搭建计划-执行-再计划的多工具代理，比较单次提示与多工具协同的方法，评测准确率和推理延迟。

Result: 工具辅助下，事件问答任务准确率显著上升，但延迟增加数十倍；而劝说回复任务单次提示效果更优，且使用多工具规划带来显著延迟提升但无一致性能提升；复杂多工具协调还导致小模型性能下降。

Conclusion: 多工具结合推理虽能提高部分任务表现，但带来高延迟和潜在失败风险，需根据任务特点合理选择模型大小及代理工具的复杂度，实现性能与成本的平衡。

Abstract: Modern large language models (LLMs) increasingly rely on inference-time planning and external tools to improve reasoning. We benchmark this behavior on two real-world settings: event-centric question answering over graph-structured knowledge (Event-QA) and persuasive response generation in Reddit ChangeMyView (CMV). Using LangChain and LangGraph, we compare a one-shot baseline against a plan-execute-replan agent equipped with task-specific tools (DBpedia SPARQL/lookup/schema exploration, Wikipedia-focused retrieval, and topical web search). We evaluate on 60 examples each from Event-QA and CMV (3 splits of 20), and report both mean end-to-end latency and per-example token cost estimates. We evaluate GPT-4o and GPT-4o-mini under identical workflows and report accuracy and end-to-end latency. On Event-QA, the best tool-augmented configuration improves accuracy (e.g., 47.5\% $\rightarrow$ 67.5\% for GPT-4o) while increasing latency by orders of magnitude ($\sim$8s $\rightarrow$ $\sim$317s per example). On CMV, one-shot prompting is strongest (e.g., GPT-4o-mini achieves 75\% at $\sim$6s), and planning+search increases latency substantially without consistent gains. However, complex multi-tool orchestration exposes failure modes where the smaller model degrades. Overall, the findings highlight the need for task-specific, cost-aware choices of both model size and agent/tooling complexity.

</details>


### [16] [Towards Comprehensive Stage-wise Benchmarking of Large Language Models in Fact-Checking](https://arxiv.org/abs/2601.02669)
*Hongzhan Lin,Zixin Chen,Zhiqi Shen,Ziyang Luo,Zhen Ye,Jing Ma,Tat-Seng Chua,Guandong Xu*

Main category: cs.CL

TL;DR: 提出了FactArena，一个全面自动化的事实检查评估框架，涵盖声明提取、证据检索和判决预测，提升了对大语言模型（LLMs）事实推理能力的系统评估。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要关注声明验证，忽视了事实检查的完整流程，无法揭示LLMs在推理、事实盲点及鲁棒性方面的系统性缺陷。

Method: FactArena包括基于LLM的事实检查流程、采用统一准则的竞技场式判决机制，以及动态生成更具挑战性的声明以检测鲁棒性的声明演进模块。

Result: 在16个先进LLMs的测试中，FactArena产生稳定且可解释的排名，揭示了静态验证准确率与端到端事实检查能力之间的显著差异。

Conclusion: FactArena提供了一个可扩展且可信赖的评估框架，有助于诊断LLMs的事实推理能力，推动模型改进，促进LLMs在安全关键应用中的可靠部署。

Abstract: Large Language Models (LLMs) are increasingly deployed in real-world fact-checking systems, yet existing evaluations focus predominantly on claim verification and overlook the broader fact-checking workflow, including claim extraction and evidence retrieval. This narrow focus prevents current benchmarks from revealing systematic reasoning failures, factual blind spots, and robustness limitations of modern LLMs. To bridge this gap, we present FactArena, a fully automated arena-style evaluation framework that conducts comprehensive, stage-wise benchmarking of LLMs across the complete fact-checking pipeline. FactArena integrates three key components: (i) an LLM-driven fact-checking process that standardizes claim decomposition, evidence retrieval via tool-augmented interactions, and justification-based verdict prediction; (ii) an arena-styled judgment mechanism guided by consolidated reference guidelines to ensure unbiased and consistent pairwise comparisons across heterogeneous judge agents; and (iii) an arena-driven claim-evolution module that adaptively generates more challenging and semantically controlled claims to probe LLMs' factual robustness beyond fixed seed data. Across 16 state-of-the-art LLMs spanning seven model families, FactArena produces stable and interpretable rankings. Our analyses further reveal significant discrepancies between static claim-verification accuracy and end-to-end fact-checking competence, highlighting the necessity of holistic evaluation. The proposed framework offers a scalable and trustworthy paradigm for diagnosing LLMs' factual reasoning, guiding future model development, and advancing the reliable deployment of LLMs in safety-critical fact-checking applications.

</details>


### [17] [Multi-Turn Jailbreaking of Aligned LLMs via Lexical Anchor Tree Search](https://arxiv.org/abs/2601.02670)
*Devang Kulshreshtha,Hang Su,Chinmay Hegde,Haohan Wang*

Main category: cs.CL

TL;DR: 该论文提出一种无需攻击者大语言模型的新方法LATS，通过词汇锚点注入实现高效绕过限制，平均仅需约6.4次查询即可达到97-100%的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有绕过方法虽然成功率高，但依赖攻击者大语言模型生成对抗性查询，成本高且生成的查询难以理解。

Method: 提出Lexical Anchor Tree Search (LATS)，将绕过任务转化为一个基于多轮对话的广度优先树搜索，每个节点通过逐步注入目标中的缺失关键词到良性提示中，实现无需攻击者大语言模型的攻击。

Result: 在AdvBench和HarmBench数据集上，LATS在最新的GPT、Claude和Llama模型上实现了97-100%的高成功率，且平均仅需约6.4次查询，明显优于需20+查询的现有方法。

Conclusion: LATS展示了对话结构作为有效攻击面的潜力，同时显著提升查询效率，为绕过限制提供了一种更经济高效的方案。

Abstract: Most jailbreak methods achieve high attack success rates (ASR) but require attacker LLMs to craft adversarial queries and/or demand high query budgets. These resource limitations make jailbreaking expensive, and the queries generated by attacker LLMs often consist of non-interpretable random prefixes. This paper introduces Lexical Anchor Tree Search (), addressing these limitations through an attacker-LLM-free method that operates purely via lexical anchor injection. LATS reformulates jailbreaking as a breadth-first tree search over multi-turn dialogues, where each node incrementally injects missing content words from the attack goal into benign prompts. Evaluations on AdvBench and HarmBench demonstrate that LATS achieves 97-100% ASR on latest GPT, Claude, and Llama models with an average of only ~6.4 queries, compared to 20+ queries required by other methods. These results highlight conversational structure as a potent and under-protected attack surface, while demonstrating superior query efficiency in an era where high ASR is readily achievable. Our code will be released to support reproducibility.

</details>


### [18] [Extracting books from production language models](https://arxiv.org/abs/2601.02671)
*Ahmed Ahmed,A. Feder Cooper,Sanmi Koyejo,Percy Liang*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型(LLMs)中训练数据记忆及其在输出中的提取问题，发现即使有安全措施，生产环境中的LLMs仍可能泄露大量版权文本。


<details>
  <summary>Details</summary>
Motivation: 当前关于LLMs是否会记忆并输出被训练的版权内容存在法律争议，且实际生产模型是否能被提取相似文本尚不明确。

Method: 采用两阶段方案：先进行提取可行性测试（包括最佳多选绕过方法），再通过迭代续写提示尝试完整提取文本，评估4款主流生产LLMs的提取效果。

Result: 不同模型提取效果差异大，Gemini 2.5 Pro和Grok 3无需绕过即可高效提取，Claude 3.7 Sonnet经过绕过可输出整本书，GPT-4.1提取难度大且容易中断。

Conclusion: 尽管具有多重安全机制，生产环境LLMs仍存在从训练中提取版权内容的风险，提示相关法律与技术防护需进一步完善。

Abstract: Many unresolved legal questions over LLMs and copyright center on memorization: whether specific training data have been encoded in the model's weights during training, and whether those memorized data can be extracted in the model's outputs. While many believe that LLMs do not memorize much of their training data, recent work shows that substantial amounts of copyrighted text can be extracted from open-weight models. However, it remains an open question if similar extraction is feasible for production LLMs, given the safety measures these systems implement. We investigate this question using a two-phase procedure: (1) an initial probe to test for extraction feasibility, which sometimes uses a Best-of-N (BoN) jailbreak, followed by (2) iterative continuation prompts to attempt to extract the book. We evaluate our procedure on four production LLMs -- Claude 3.7 Sonnet, GPT-4.1, Gemini 2.5 Pro, and Grok 3 -- and we measure extraction success with a score computed from a block-based approximation of longest common substring (nv-recall). With different per-LLM experimental configurations, we were able to extract varying amounts of text. For the Phase 1 probe, it was unnecessary to jailbreak Gemini 2.5 Pro and Grok 3 to extract text (e.g, nv-recall of 76.8% and 70.3%, respectively, for Harry Potter and the Sorcerer's Stone), while it was necessary for Claude 3.7 Sonnet and GPT-4.1. In some cases, jailbroken Claude 3.7 Sonnet outputs entire books near-verbatim (e.g., nv-recall=95.8%). GPT-4.1 requires significantly more BoN attempts (e.g., 20X), and eventually refuses to continue (e.g., nv-recall=4.0%). Taken together, our work highlights that, even with model- and system-level safeguards, extraction of (in-copyright) training data remains a risk for production LLMs.

</details>


### [19] [Iterative Structured Pruning for Large Language Models with Multi-Domain Calibration](https://arxiv.org/abs/2601.02674)
*Guangxin Wu,Hao Zhang,Zhang Zhibin,Jiafeng Guo,Xueqi Cheng*

Main category: cs.CL

TL;DR: 本文提出了一种利用混合多域校准集和迭代校准策略的新型结构化剪枝框架，实现了在保持性能的同时大幅度模型压缩。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型规模庞大，导致在部署时计算开销大、内存占用高且推理延迟长，传统非结构化剪枝造成稀疏不规则，难以兼容硬件。

Method: 采用结构化剪枝，通过混合多域校准集和迭代校准策略，有效识别并剔除冗余通道，保证剪枝后模型对标准硬件加速器的兼容性。

Result: 在多种模型和下游任务中，所提方法在显著压缩模型的同时仅带来极小的性能下降。

Conclusion: 该结构化剪枝框架是一种高效且兼容硬件的大型语言模型压缩方法，适合实际应用部署。

Abstract: Large Language Models (LLMs) have achieved remarkable success across a wide spectrum of natural language processing tasks. However, their ever-growing scale introduces significant barriers to real-world deployment, including substantial computational overhead, memory footprint, and inference latency. While model pruning presents a viable solution to these challenges, existing unstructured pruning techniques often yield irregular sparsity patterns that necessitate specialized hardware or software support. In this work, we explore structured pruning, which eliminates entire architectural components and maintains compatibility with standard hardware accelerators. We introduce a novel structured pruning framework that leverages a hybrid multi-domain calibration set and an iterative calibration strategy to effectively identify and remove redundant channels. Extensive experiments on various models across diverse downstream tasks show that our approach achieves significant compression with minimal performance degradation.

</details>


### [20] [Boosting Accuracy and Interpretability in Multilingual Hate Speech Detection Through Layer Freezing and Explainable AI](https://arxiv.org/abs/2601.02697)
*Meysam Shirdel Bilehsavar,Negin Mahmoudi,Mohammad Jalili Torkamani,Kiana Kiashemshaki*

Main category: cs.CL

TL;DR: 本文评估了三种基于Transformer的模型在五种语言中的情感分析和仇恨言论检测性能，并结合LIME解释方法提升模型透明度。


<details>
  <summary>Details</summary>
Motivation: 情感分析和仇恨言论检测对网络内容监管至关重要，但多语言环境下模型性能及其可解释性仍需提升。

Method: 使用BERT-base-multilingual-cased、RoBERTa-base和固定前八层的XLM-RoBERTa-base三种模型，在英、韩、日、中、法五种语言上进行评测，并结合LIME解释框架分析词语对预测的贡献。

Result: 各模型在多语言的准确率、精准率、召回率和F1值方面表现优异，同时LIME提高了模型结果的可解释性。

Conclusion: 结合先进Transformer架构和可解释技术，能有效提升多语言情感分析及仇恨言论检测系统的性能和透明度，有助于构建更安全的数字环境。

Abstract: Sentiment analysis focuses on identifying the emotional polarity expressed in textual data, typically categorized as positive, negative, or neutral. Hate speech detection, on the other hand, aims to recognize content that incites violence, discrimination, or hostility toward individuals or groups based on attributes such as race, gender, sexual orientation, or religion. Both tasks play a critical role in online content moderation by enabling the detection and mitigation of harmful or offensive material, thereby contributing to safer digital environments. In this study, we examine the performance of three transformer-based models: BERT-base-multilingual-cased, RoBERTa-base, and XLM-RoBERTa-base with the first eight layers frozen, for multilingual sentiment analysis and hate speech detection. The evaluation is conducted across five languages: English, Korean, Japanese, Chinese, and French. The models are compared using standard performance metrics, including accuracy, precision, recall, and F1-score. To enhance model interpretability and provide deeper insight into prediction behavior, we integrate the Local Interpretable Model-agnostic Explanations (LIME) framework, which highlights the contribution of individual words to the models decisions. By combining state-of-the-art transformer architectures with explainability techniques, this work aims to improve both the effectiveness and transparency of multilingual sentiment analysis and hate speech detection systems.

</details>


### [21] [Adversarial Question Answering Robustness: A Multi-Level Error Analysis and Mitigation Study](https://arxiv.org/abs/2601.02700)
*Agniv Roy Choudhury,Vignesh Ponselvan Rajasingh*

Main category: cs.CL

TL;DR: 该论文研究了Transformer模型在AddSent对抗数据集上的对抗鲁棒性，通过多层次错误分析发现主要失败模式为否定混淆和实体替换，提出了基于实体识别的对比学习作为有效的缓解策略，取得了接近无对抗差距的性能提升。


<details>
  <summary>Details</summary>
Motivation: 目前QA系统在标准测试集上表现优异，但面对对抗样本仍易出错，因此研究如何提升模型的对抗鲁棒性具有重要意义。

Method: 通过多尺度模型和多种缓解措施的系统实验，结合五种分类方法进行错误分析，发现关键失败模式，并在数据增强和对抗微调中探索最佳策略；最后引入实体感知的对比学习进行针对性缓解。

Result: 最佳混合训练比例为80%干净数据+20%对抗数据，模型尺寸扩大提升鲁棒性且消除了准确率与鲁棒性的权衡；实体感知对比学习在AddSent和SQuAD数据上的表现分别达89.89%和90.73% EM，显著缩小了对抗性能差距。

Conclusion: 结合详尽语言学错误分析和命名实体识别引导的对比学习策略，可有效提升QA模型的对抗鲁棒性，几乎消除对抗样本带来的性能下降。

Abstract: Question answering (QA) systems achieve impressive performance on standard benchmarks like SQuAD, but remain vulnerable to adversarial examples. This project investigates the adversarial robustness of transformer models on the AddSent adversarial dataset through systematic experimentation across model scales and targeted mitigation strategies. We perform comprehensive multi-level error analysis using five complementary categorization schemes, identifying negation confusion and entity substitution as the primary failure modes. Through systematic evaluation of adversarial fine-tuning ratios, we identify 80% clean + 20% adversarial data as optimal. Data augmentation experiments reveal a capacity bottleneck in small models. Scaling from ELECTRA-small (14M parameters) to ELECTRA-base (110M parameters) eliminates the robustness-accuracy trade-off, achieving substantial improvements on both clean and adversarial data. We implement three targeted mitigation strategies, with Entity-Aware contrastive learning achieving best performance: 89.89% AddSent Exact Match (EM) and 90.73% SQuAD EM, representing 94.9% closure of the adversarial gap. To our knowledge, this is the first work integrating comprehensive linguistic error analysis with Named Entity Recognition (NER)-guided contrastive learning for adversarial QA, demonstrating that targeted mitigation can achieve near-parity between clean and adversarial performance.

</details>


### [22] [Mitigating Prompt-Induced Hallucinations in Large Language Models via Structured Reasoning](https://arxiv.org/abs/2601.02739)
*Jinbo Hao,Kai Yang,Qingzhen Su,Yang Chen,Yifan Li,Chao Jiang*

Main category: cs.CL

TL;DR: 提出一种结合代码模块的链式知识蒸馏模型以减少大语言模型的提示诱发幻觉，有效提升推理准确率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在提示诱发幻觉问题，影响推理准确性和可靠性。

Method: 基于链式知识蒸馏模型引入代码模块，指导知识图谱探索并作为链式思考提示的外部知识输入，分析和约束推理过程。

Result: 在GPT-4和LLaMA-3.3的多数据集实验中，代码模块显著提升了模型捕获上下文信息的能力，HIT@1、HIT@3和HIT@5分别提高15.64%、13.38%和13.28%，多个评测中得分超过95%。

Conclusion: 该方法有效减少了模型幻觉现象，提升了大语言模型的推理准确度和可验证性。

Abstract: To address hallucination issues in large language models (LLMs), this paper proposes a method for mitigating prompt-induced hallucinations. Building on a knowledge distillation chain-style model, we introduce a code module to guide knowledge-graph exploration and incorporate code as part of the chain-of-thought prompt, forming an external knowledge input that provides more accurate and structured information to the model. Based on this design, we develop an improved knowledge distillation chain-style model and leverage it to analyze and constrain the reasoning process of LLMs, thereby improving inference accuracy. We empirically evaluate the proposed approach using GPT-4 and LLaMA-3.3 on multiple public datasets. Experimental results demonstrate that incorporating code modules significantly enhances the model's ability to capture contextual information and effectively mitigates prompt-induced hallucinations. Specifically, HIT@1, HIT@3, and HIT@5 improve by 15.64%, 13.38%, and 13.28%, respectively. Moreover, the proposed method achieves HIT@1, HIT@3, and HIT@5 scores exceeding 95% across several evaluation settings. These results indicate that the proposed approach substantially reduces hallucination behavior while improving the accuracy and verifiability of large language models.

</details>


### [23] [Language Hierarchization Provides the Optimal Solution to Human Working Memory Limits](https://arxiv.org/abs/2601.02740)
*Luyao Chen,Weibo Gao,Junjie Wu,Jinshan Wu,Angela D. Friederici*

Main category: cs.CL

TL;DR: 本研究探讨了人类语言的层级结构是如何优化有限工作记忆容量下的信息处理效率。


<details>
  <summary>Details</summary>
Motivation: 为什么人类语言呈现层级结构，这一中心问题未被充分解释。研究动机在于揭示层级化语言结构的根本原因，特别是在有限工作记忆容量约束下的作用。

Method: 构建了一个似然函数来量化语言处理单元数与人类工作记忆容量的匹配程度，并通过符号序列的计算模拟和自然语言句子的验证分析，对比线性处理与层级处理的表现。

Result: 层级处理相比线性处理更有效地控制语言处理单元数，能在工作记忆容量限制内处理更长的句子，且结果与儿童工作记忆容量的发展模式相吻合。

Conclusion: 语言的层级结构是为了优化序列语言输入的处理效率，实现工作记忆容量的合理利用，解释了人类语言普遍存在的层级性质。

Abstract: Language is a uniquely human trait, conveying information efficiently by organizing word sequences in sentences into hierarchical structures. A central question persists: Why is human language hierarchical? In this study, we show that hierarchization optimally solves the challenge of our limited working memory capacity. We established a likelihood function that quantifies how well the average number of units according to the language processing mechanisms aligns with human working memory capacity (WMC) in a direct fashion. The maximum likelihood estimate (MLE) of this function, tehta_MLE, turns out to be the mean of units. Through computational simulations of symbol sequences and validation analyses of natural language sentences, we uncover that compared to linear processing, hierarchical processing far surpasses it in constraining the tehta_MLE values under the human WMC limit, along with the increase of sequence/sentence length successfully. It also shows a converging pattern related to children's WMC development. These results suggest that constructing hierarchical structures optimizes the processing efficiency of sequential language input while staying within memory constraints, genuinely explaining the universal hierarchical nature of human language.

</details>


### [24] [SYNAPSE: Empowering LLM Agents with Episodic-Semantic Memory via Spreading Activation](https://arxiv.org/abs/2601.02744)
*Hanqi Jiang,Junhao Chen,Yi Pan,Ling Chen,Weihang You,Yifan Zhou,Ruidong Zhang,Yohannes Abate,Tianming Liu*

Main category: cs.CL

TL;DR: 本文提出了Synapse，一种基于动态图模型的长期记忆架构，结合传播激活机制和三重混合检索策略，显著提升复杂时序和多跳推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于检索的增强方法难以解决长期自主智能体记忆的断裂和连接性问题。

Method: Synapse采用动态图模型，通过传播激活而非静态向量相似度来建模记忆，结合侧抑制和时间衰减机制动态突出相关子图，并使用三重混合检索策略融合几何嵌入和基于激活的图遍历。

Result: 在LoCoMo基准测试中，Synapse在复杂时序推理和多跳推理任务上显著超越了现有最先进方法。

Conclusion: Synapse提供了一种强有力的长期记忆处理方案，有效解决了“上下文隧道”问题，推动了长期代理智能体记忆技术的发展。

Abstract: While Large Language Models (LLMs) excel at generalized reasoning, standard retrieval-augmented approaches fail to address the disconnected nature of long-term agentic memory. To bridge this gap, we introduce Synapse (Synergistic Associative Processing Semantic Encoding), a unified memory architecture that transcends static vector similarity. Drawing from cognitive science, Synapse models memory as a dynamic graph where relevance emerges from spreading activation rather than pre-computed links. By integrating lateral inhibition and temporal decay, the system dynamically highlights relevant sub-graphs while filtering interference. We implement a Triple Hybrid Retrieval strategy that fuses geometric embeddings with activation-based graph traversal. Comprehensive evaluations on the LoCoMo benchmark show that Synapse significantly outperforms state-of-the-art methods in complex temporal and multi-hop reasoning tasks, offering a robust solution to the "Contextual Tunneling" problem. Our code and data will be made publicly available upon acceptance.

</details>


### [25] [Window-based Membership Inference Attacks Against Fine-tuned Large Language Models](https://arxiv.org/abs/2601.02751)
*Yuetian Chen,Yuntao Du,Kaiyuan Zhang,Ashish Kundu,Charles Fleming,Bruno Ribeiro,Ninghui Li*

Main category: cs.CL

TL;DR: 本文提出了一种针对大型语言模型的成员推断攻击新方法WBC，通过滑动窗口局部比较，显著提升攻击效果，揭示了微调模型的隐私风险。


<details>
  <summary>Details</summary>
Motivation: 现有的成员推断攻击多依赖全局信号（如平均损失），忽视了记忆中局部的细微信号，导致攻击效果不佳。

Method: 提出WBC方法，利用不同大小的滑动窗口对目标模型和参考模型的损失进行局部比较，基于符号聚合进行成员资格判定，并通过集成多个窗口大小的投票捕获从单词到短语的记忆模式。

Result: 在11个数据集上的大量实验表明，WBC相比现有基线方法，显著提升AUC分数，在低假阳性率下检测率提高2至3倍。

Conclusion: 局部证据聚合比全局平均更有效，揭示了微调大型语言模型存在的重要隐私漏洞。

Abstract: Most membership inference attacks (MIAs) against Large Language Models (LLMs) rely on global signals, like average loss, to identify training data. This approach, however, dilutes the subtle, localized signals of memorization, reducing attack effectiveness. We challenge this global-averaging paradigm, positing that membership signals are more pronounced within localized contexts. We introduce WBC (Window-Based Comparison), which exploits this insight through a sliding window approach with sign-based aggregation. Our method slides windows of varying sizes across text sequences, with each window casting a binary vote on membership based on loss comparisons between target and reference models. By ensembling votes across geometrically spaced window sizes, we capture memorization patterns from token-level artifacts to phrase-level structures. Extensive experiments across eleven datasets demonstrate that WBC substantially outperforms established baselines, achieving higher AUC scores and 2-3 times improvements in detection rates at low false positive thresholds. Our findings reveal that aggregating localized evidence is fundamentally more effective than global averaging, exposing critical privacy vulnerabilities in fine-tuned LLMs.

</details>


### [26] [EComStage: Stage-wise and Orientation-specific Benchmarking for Large Language Models in E-commerce](https://arxiv.org/abs/2601.02752)
*Kaiyan Zhao,Zijie Meng,Zheyong Xie,Jin Duan,Yao Hu,Zuozhu Liu,Shaosheng Cao*

Main category: cs.CL

TL;DR: 本文提出了EComStage基准，用于评估用于电商应用中的大语言模型代理在感知、规划和执行三个阶段的综合推理能力，涵盖了面向客户和商家的多样场景。通过对30余个不同规模模型进行测试，揭示了各阶段和应用方向上的优劣，为优化电商LLM代理提供了实践指导。


<details>
  <summary>Details</summary>
Motivation: 当前电商中基于大语言模型的代理多关注最终任务完成，缺乏对中间推理阶段的评估，难以深入了解其决策过程及优化方向。

Method: 提出EComStage统一基准，设计了七个代表性任务涵盖电商多个场景，全部样本经过人工标注与质检，测评内容包括感知、规划和执行三个推理阶段，同时涵盖客户和商家视角。

Result: 测试了30多个参数规模从1B到200B的大语言模型，包括开源和闭源模型，发现不同模型在各阶段及不同应用方向上表现存在差异，揭示具体优势与不足。

Conclusion: EComStage基准能够细致评估电商LLM代理的阶段性推理能力和方向特定表现，为实际应用中设计和优化这些模型提供了细粒度的可操作性见解。

Abstract: Large Language Model (LLM)-based agents are increasingly deployed in e-commerce applications to assist customer services in tasks such as product inquiries, recommendations, and order management. Existing benchmarks primarily evaluate whether these agents successfully complete the final task, overlooking the intermediate reasoning stages that are crucial for effective decision-making. To address this gap, we propose EComStage, a unified benchmark for evaluating agent-capable LLMs across the comprehensive stage-wise reasoning process: Perception (understanding user intent), Planning (formulating an action plan), and Action (executing the decision). EComStage evaluates LLMs through seven separate representative tasks spanning diverse e-commerce scenarios, with all samples human-annotated and quality-checked. Unlike prior benchmarks that focus only on customer-oriented interactions, EComStage also evaluates merchant-oriented scenarios, including promotion management, content review, and operational support relevant to real-world applications. We evaluate a wide range of over 30 LLMs, spanning from 1B to over 200B parameters, including open-source models and closed-source APIs, revealing stage/orientation-specific strengths and weaknesses. Our results provide fine-grained, actionable insights for designing and optimizing LLM-based agents in real-world e-commerce settings.

</details>


### [27] [MiMo-V2-Flash Technical Report](https://arxiv.org/abs/2601.02780)
*Bangjun Xiao,Bingquan Xia,Bo Yang,Bofei Gao,Bowen Shen,Chen Zhang,Chenhong He,Chiheng Lou,Fuli Luo,Gang Wang,Gang Xie,Hailin Zhang,Hanglong Lv,Hanyu Li,Heyu Chen,Hongshen Xu,Houbin Zhang,Huaqiu Liu,Jiangshan Duo,Jianyu Wei,Jiebao Xiao,Jinhao Dong,Jun Shi,Junhao Hu,Kainan Bao,Kang Zhou,Lei Li,Liang Zhao,Linghao Zhang,Peidian Li,Qianli Chen,Shaohui Liu,Shihua Yu,Shijie Cao,Shimao Chen,Shouqiu Yu,Shuo Liu,Tianling Zhou,Weijiang Su,Weikun Wang,Wenhan Ma,Xiangwei Deng,Bohan Mao,Bowen Ye,Can Cai,Chenghua Wang,Chengxuan Zhu,Chong Ma,Chun Chen,Chunan Li,Dawei Zhu,Deshan Xiao,Dong Zhang,Duo Zhang,Fangyue Liu,Feiyu Yang,Fengyuan Shi,Guoan Wang,Hao Tian,Hao Wu,Heng Qu,Hongfei Yi,Hongxu An,Hongyi Guan,Xing Zhang,Yifan Song,Yihan Yan,Yihao Zhao,Yingchun Lai,Yizhao Gao,Yu Cheng,Yuanyuan Tian,Yudong Wang,Zhen Tang,Zhengju Tang,Zhengtao Wen,Zhichao Song,Zhixian Zheng,Zihan Jiang,Jian Wen,Jiarui Sun,Jiawei Li,Jinlong Xue,Jun Xia,Kai Fang,Menghang Zhu,Nuo Chen,Qian Tu,Qihao Zhang,Qiying Wang,Rang Li,Rui Ma,Shaolei Zhang,Shengfan Wang,Shicheng Li,Shuhao Gu,Shuhuai Ren,Sirui Deng,Tao Guo,Tianyang Lu,Weiji Zhuang,Weikang Zhang,Weimin Xiong,Wenshan Huang,Wenyu Yang,Xin Zhang,Xing Yong,Xu Wang,Xueyang Xie,Yilin Jiang,Yixin Yang,Yongzhe He,Yu Tu,Yuanliang Dong,Yuchen Liu,Yue Ma,Yue Yu,Yuxing Xiang,Zhaojun Huang,Zhenru Lin,Zhipeng Xu,Zhiyang Chen,Zhonghua Deng,Zihan Zhang,Zihao Yue*

Main category: cs.CL

TL;DR: MiMo-V2-Flash是一种拥有3090亿参数、15亿活跃参数的混合专家模型，采用滑动窗口和全局注意力混合架构，高效预训练和多教师在线蒸馏，推理速度显著提升。


<details>
  <summary>Details</summary>
Motivation: 提高大规模混合专家模型的推理速度和推理能力，同时保持强大的推理和智能代理能力。

Method: 设计了滑动窗口与全局注意力混合机制，采取32k初始上下文长度扩展至256k；预训练时使用多标记预测；引入多教师在线策略蒸馏（MOPD）从领域专业教师处学习；将多标记预测模型用于推理中的投机解码实现加速。

Result: MiMo-V2-Flash以较少参数数达到竞品DeepSeek-V3.2和Kimi-K2的性能，推理时实现了最高3.6倍接受长度和2.6倍解码速度提升。

Conclusion: MiMo-V2-Flash通过创新的模型架构和训练蒸馏策略实现了高效的强推理能力和快速解码，且开放了模型权重，推动社区研究和合作。

Abstract: We present MiMo-V2-Flash, a Mixture-of-Experts (MoE) model with 309B total parameters and 15B active parameters, designed for fast, strong reasoning and agentic capabilities. MiMo-V2-Flash adopts a hybrid attention architecture that interleaves Sliding Window Attention (SWA) with global attention, with a 128-token sliding window under a 5:1 hybrid ratio. The model is pre-trained on 27 trillion tokens with Multi-Token Prediction (MTP), employing a native 32k context length and subsequently extended to 256k. To efficiently scale post-training compute, MiMo-V2-Flash introduces a novel Multi-Teacher On-Policy Distillation (MOPD) paradigm. In this framework, domain-specialized teachers (e.g., trained via large-scale reinforcement learning) provide dense and token-level reward, enabling the student model to perfectly master teacher expertise. MiMo-V2-Flash rivals top-tier open-weight models such as DeepSeek-V3.2 and Kimi-K2, despite using only 1/2 and 1/3 of their total parameters, respectively. During inference, by repurposing MTP as a draft model for speculative decoding, MiMo-V2-Flash achieves up to 3.6 acceptance length and 2.6x decoding speedup with three MTP layers. We open-source both the model weights and the three-layer MTP weights to foster open research and community collaboration.

</details>


### [28] [Punctuation-aware Hybrid Trainable Sparse Attention for Large Language Models](https://arxiv.org/abs/2601.02819)
*Junxiang Qiu,Shuo Wang,Zhengsu Chen,Hengheng Zhang,Jinda Lu,Changcheng Li,Qi Tian*

Main category: cs.CL

TL;DR: 本文提出了一种基于标点符号的混合稀疏注意力机制PHSA，用以提高长序列建模时的语义边界保留和信息完整性。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏注意力方法依赖粗粒度语义表示，导致语义边界模糊和重要信息丢失。

Method: 设计了利用标点符号作为语义边界锚点的双分支聚合机制，并引入极端稀疏自适应训练与推理策略，以稳定模型在低激活率下的性能。

Result: 实验表明PHSA在通用基准和长上下文评测中均优于密集注意力和当前最先进的稀疏注意力方法，如InfLLM v2，在32k输入序列和97.3%稀疏率下，信息损失降低10.8%。

Conclusion: PHSA有效结合标点符号信息提升稀疏注意力的语义捕获能力，为大规模长序列语言模型提供高效且效果优异的注意力机制。

Abstract: Attention serves as the fundamental mechanism for long-context modeling in large language models (LLMs), yet dense attention becomes structurally prohibitive for long sequences due to its quadratic complexity. Consequently, sparse attention has received increasing attention as a scalable alternative. However, existing sparse attention methods rely on coarse-grained semantic representations during block selection, which blur intra-block semantic boundaries and lead to the loss of critical information. To address this issue, we propose \textbf{P}unctuation-aware \textbf{H}ybrid \textbf{S}parse \textbf{A}ttention \textbf{(PHSA)}, a natively trainable sparse attention framework that leverages punctuation tokens as semantic boundary anchors. Specifically, (1) we design a dual-branch aggregation mechanism that fuses global semantic representations with punctuation-enhanced boundary features, preserving the core semantic structure while introducing almost no additional computational overhead; (2) we introduce an extreme-sparsity-adaptive training and inference strategy that stabilizes model behavior under very low token activation ratios; Extensive experiments on general benchmarks and long-context evaluations demonstrate that PHSA consistently outperforms dense attention and state-of-the-art sparse attention baselines, including InfLLM v2. Specifically, for the 0.6B-parameter model with 32k-token input sequences, PHSA can reduce the information loss by 10.8\% at a sparsity ratio of 97.3\%.

</details>


### [29] [The performances of the Chinese and U.S. Large Language Models on the Topic of Chinese Culture](https://arxiv.org/abs/2601.02830)
*Feiyan Liu,Chenxun Zhuo,Siyan Zhao,Bao Ge,Tianming Liu*

Main category: cs.CL

TL;DR: 本研究比较了中美大型语言模型在中文文化相关问题上的表现，发现中国开发的模型在传统文化理解上普遍优于美国模型。


<details>
  <summary>Details</summary>
Motivation: 探究不同文化背景下开发的LLM在中文文化理解能力上的差异。

Method: 采用直接提问方式，对GPT-5.1、DeepSeek-V3.2、Qwen3-Max和Gemini2.5Pro等模型进行中文文化相关问题测试。

Result: 中国模型整体表现优于美国模型，尤其在传统文化知识掌握上更为准确。美国模型中，Gemini 2.5Pro和GPT-5.1表现相对较好。

Conclusion: LLM在中文文化理解表现差异可能源于训练数据、地域本地化策略及对中国文化内容的重视程度不同。

Abstract: Cultural backgrounds shape individuals' perspectives and approaches to problem-solving. Since the emergence of GPT-1 in 2018, large language models (LLMs) have undergone rapid development. To date, the world's ten leading LLM developers are primarily based in China and the United States. To examine whether LLMs released by Chinese and U.S. developers exhibit cultural differences in Chinese-language settings, we evaluate their performance on questions about Chinese culture. This study adopts a direct-questioning paradigm to evaluate models such as GPT-5.1, DeepSeek-V3.2, Qwen3-Max, and Gemini2.5Pro. We assess their understanding of traditional Chinese culture, including history, literature, poetry, and related domains. Comparative analyses between LLMs developed in China and the U.S. indicate that Chinese models generally outperform their U.S. counterparts on these tasks. Among U.S.-developed models, Gemini 2.5Pro and GPT-5.1 achieve relatively higher accuracy. The observed performance differences may potentially arise from variations in training data distribution, localization strategies, and the degree of emphasis on Chinese cultural content during model development.

</details>


### [30] [TiMem: Temporal-Hierarchical Memory Consolidation for Long-Horizon Conversational Agents](https://arxiv.org/abs/2601.02845)
*Kai Li,Xuanqing Yu,Ziyi Ni,Yi Zeng,Yao Xu,Zheqing Zhang,Xin Li,Jitao Sang,Xiaogang Duan,Xuelei Wang,Chengbao Liu,Jie Tan*

Main category: cs.CL

TL;DR: TiMem提出了一种基于时间-层级结构的记忆框架，通过时间记忆树实现对长对话的系统性记忆整合，在多个基准测试中取得了领先效果。


<details>
  <summary>Details</summary>
Motivation: 现有的记忆框架难以有效管理长时间对话中层级丰富且不断增长的历史信息，导致记忆碎片化和个性化效果不稳定。

Method: TiMem构建了一个时间记忆树，实现对对话的时间-层级组织，采用语义指导的记忆整合方法，同时支持复杂度感知的记忆调用，避免了微调需求。

Result: TiMem在LoCoMo和LongMemEval-S两大基准测试中分别达到75.30%和76.88%的准确率，超越所有对比方法，并减少了52.20%的记忆调用长度。

Conclusion: TiMem将时间连续性作为核心组织原则，有效提升了长时间对话记忆的系统化和个性化表现，推动了对话代理的长期记忆管理能力。

Abstract: Long-horizon conversational agents have to manage ever-growing interaction histories that quickly exceed the finite context windows of large language models (LLMs). Existing memory frameworks provide limited support for temporally structured information across hierarchical levels, often leading to fragmented memories and unstable long-horizon personalization. We present TiMem, a temporal--hierarchical memory framework that organizes conversations through a Temporal Memory Tree (TMT), enabling systematic memory consolidation from raw conversational observations to progressively abstracted persona representations. TiMem is characterized by three core properties: (1) temporal--hierarchical organization through TMT; (2) semantic-guided consolidation that enables memory integration across hierarchical levels without fine-tuning; and (3) complexity-aware memory recall that balances precision and efficiency across queries of varying complexity. Under a consistent evaluation setup, TiMem achieves state-of-the-art accuracy on both benchmarks, reaching 75.30% on LoCoMo and 76.88% on LongMemEval-S. It outperforms all evaluated baselines while reducing the recalled memory length by 52.20% on LoCoMo. Manifold analysis indicates clear persona separation on LoCoMo and reduced dispersion on LongMemEval-S. Overall, TiMem treats temporal continuity as a first-class organizing principle for long-horizon memory in conversational agents.

</details>


### [31] [To Generate or Discriminate? Methodological Considerations for Measuring Cultural Alignment in LLMs](https://arxiv.org/abs/2601.02858)
*Saurabh Kumar Pandey,Sougata Saha,Monojit Choudhury*

Main category: cs.CL

TL;DR: 本文提出逆向社会人口提示（ISDP）方法，通过预测用户行为中的人口统计代理，以评估大型语言模型（LLMs）的文化适应性，克服了传统社会人口提示（SDP）中存在的偏见和解释复杂性问题。


<details>
  <summary>Details</summary>
Motivation: 传统的社会人口提示（SDP）方法因提示敏感性、解码参数和生成任务难度大等因素，使得LLM表现出的偏见难以解释，影响文化适应性评估的准确性。

Method: 引入逆向社会人口提示（ISDP），通过让LLM基于真实与模拟的用户行为预测人口统计代理，利用Goodreads-CSI数据集对印度、墨西哥及美国用户的书评理解难度做测试，评估四个LLM模型的表现。

Result: 模型对真实行为的预测表现优于对模拟行为，且两种行为下个体层面的性能差异减小趋于一致，显示个性化能力有限。

Conclusion: ISDP方法比传统SDP更有效地揭示LLM的文化适应能力及个性化限制，为更可靠的偏见检测和模型评估提供新思路。

Abstract: Socio-demographic prompting (SDP) - prompting Large Language Models (LLMs) using demographic proxies to generate culturally aligned outputs - often shows LLM responses as stereotypical and biased. While effective in assessing LLMs' cultural competency, SDP is prone to confounding factors such as prompt sensitivity, decoding parameters, and the inherent difficulty of generation over discrimination tasks due to larger output spaces. These factors complicate interpretation, making it difficult to determine if the poor performance is due to bias or the task design. To address this, we use inverse socio-demographic prompting (ISDP), where we prompt LLMs to discriminate and predict the demographic proxy from actual and simulated user behavior from different users. We use the Goodreads-CSI dataset (Saha et al., 2025), which captures difficulty in understanding English book reviews for users from India, Mexico, and the USA, and test four LLMs: Aya-23, Gemma-2, GPT-4o, and LLaMA-3.1 with ISDP. Results show that models perform better with actual behaviors than simulated ones, contrary to what SDP suggests. However, performance with both behavior types diminishes and becomes nearly equal at the individual level, indicating limits to personalization.

</details>


### [32] [Training Language Models with homotokens Leads to Delayed Overfitting](https://arxiv.org/abs/2601.02867)
*Adrian Cosma,Stefan Ruseti,Emilian Radoi,Mihai Dascalu*

Main category: cs.CL

TL;DR: 本文提出通过使用多样的子词分词（同义分词）作为数据增强，提升语言模型的泛化能力和防止过拟合。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型训练通常使用单一的最长前缀分词，忽略了存在多种分词方式但含义相同的现象，这限制了模型对分词变异的鲁棒性。

Method: 作者提出同义分词（homotokens）的概念，利用辅助因果编码器和跨注意力机制，在无需修改训练目标或接口的前提下，将同义分词作为数据增强融入训练。

Result: 在数据有限的预训练中，同义分词增强显著延缓了过拟合并提升了模型的泛化能力。在多语言微调中，同义分词的效果依赖于分词器质量，分词器压缩程度高时效果更显著。

Conclusion: 同义分词为语言模型引入了简单且模块化的分词不变性机制，有助于提升模型的鲁棒性和泛化性能。

Abstract: Subword tokenization introduces a computational layer in language models where many distinct token sequences decode to the same surface form and preserve meaning, yet induce different internal computations. Despite this non-uniqueness, language models are typically trained using a single canonical longest-prefix tokenization. We formalize homotokens-alternative valid subword segmentations of the same lexical item-as a strictly meaning-preserving form of data augmentation. We introduce a lightweight training architecture that conditions canonical next-token prediction on sampled homotoken variants via an auxiliary causal encoder and block-causal cross-attention, without modifying the training objective or token interface. In data-constrained pretraining, homotoken augmentation consistently delays overfitting under repeated data exposure and improves generalization across diverse evaluation datasets. In multilingual fine-tuning, we find that the effectiveness of homotokens depends on tokenizer quality: gains are strongest when canonical tokens are highly compressed and diminish when the tokenizer already over-fragments the input. Overall, homotokens provide a simple and modular mechanism for inducing tokenization invariance in language models.

</details>


### [33] [LongBench Pro: A More Realistic and Comprehensive Bilingual Long-Context Evaluation Benchmark](https://arxiv.org/abs/2601.02872)
*Ziyang Chen,Xing Wu,Junlong Jia,Chaochen Gao,Qi Fu,Debing Zhang,Songlin Hu*

Main category: cs.CL

TL;DR: LongBench Pro是一个涵盖英中文双语的长上下文理解基准，包含1500个真实样本，支持任务细分和多维分析。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文基准在扩展性和真实性之间存在权衡，合成任务难以反映真实复杂性，全手工标注成本高且难以覆盖极端长度及多样场景。

Method: 提出了Human-Model协作构建流程，利用前沿大模型起草问题和解答，专家验证并完善，保证数据质量与扩展性。同时设计多维度任务指标评估模型表现。

Result: 评测46个主流长上下文大模型发现：长上下文优化比增大模型参数更重要；实际有效上下文长度普遍低于模型声称长度，且跨语言存在不匹配；"思考"范式对本土训练模型促进明显，混合思考设计表现出良好的权衡。

Conclusion: LongBench Pro作为更真实全面的长上下文理解基准，有助于推进长上下文建模技术的发展。

Abstract: The rapid expansion of context length in large language models (LLMs) has outpaced existing evaluation benchmarks. Current long-context benchmarks often trade off scalability and realism: synthetic tasks underrepresent real-world complexity, while fully manual annotation is costly to scale to extreme lengths and diverse scenarios. We present LongBench Pro, a more realistic and comprehensive bilingual benchmark of 1,500 naturally occurring long-context samples in English and Chinese spanning 11 primary tasks and 25 secondary tasks, with input lengths from 8k to 256k tokens. LongBench Pro supports fine-grained analysis with task-specific metrics and a multi-dimensional taxonomy of context requirement (full vs. partial dependency), length (six levels), and difficulty (four levels calibrated by model performance). To balance quality with scalability, we propose a Human-Model Collaborative Construction pipeline: frontier LLMs draft challenging questions and reference answers, along with design rationales and solution processes, to reduce the cost of expert verification. Experts then rigorously validate correctness and refine problematic cases. Evaluating 46 widely used long-context LLMs on LongBench Pro yields three findings: (1) long-context optimization contributes more to long-context comprehension than parameter scaling; (2) effective context length is typically shorter than the claimed context length, with pronounced cross-lingual misalignment; and (3) the "thinking" paradigm helps primarily models trained with native reasoning, while mixed-thinking designs offer a promising Pareto trade-off. In summary, LongBench Pro provides a robust testbed for advancing long-context understanding.

</details>


### [34] [Revisiting Data Compression with Language Modeling](https://arxiv.org/abs/2601.02875)
*Chen-Han Tsai*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLM）在数据压缩中的应用，取得了在enwik9数据集上的新型压缩率记录，且无需额外训练模型。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在多模态数据压缩方面表现良好，但实际替代现有压缩算法仍面临挑战，本研究旨在探索如何利用LLM实现更低的调整压缩率。

Method: 通过多种方法优化LLM作为数据压缩器的性能，特别是在不同类型数据（非英文文本、代码、字节流）上的应用，并调整配置以提升压缩效果。

Result: 在enwik9数据集上达到了约18%的调整压缩率新纪录，且展示了LLM在非自然文本序列压缩上的竞争力。

Conclusion: LLM在文本主导领域表现优秀，且通过合理配置在非自然文本压缩方面依然具备竞争力，有潜力替代传统压缩算法。

Abstract: In this report, we investigate the potential use of large language models (LLM's) in the task of data compression. Previous works have demonstrated promising results in applying LLM's towards compressing not only text, but also a wide range of multi-modal data. Despite the favorable performance achieved, there still remains several practical questions that pose a challenge towards replacing existing data compression algorithms with LLM's. In this work, we explore different methods to achieve a lower adjusted compression rate using LLM's as data compressors. In comparison to previous works, we were able to achieve a new state-of-the-art (SOTA) adjusted compression rate of around $18\%$ on the enwik9 dataset without additional model training. Furthermore, we explore the use of LLM's in compressing non-English data, code data, byte stream sequences. We show that while LLM's excel in compressing data in text-dominant domains, their ability in compressing non-natural text sequences still remain competitive if configured in the right way.

</details>


### [35] [Transparent Semantic Change Detection with Dependency-Based Profiles](https://arxiv.org/abs/2601.02891)
*Bach Phan-Tat,Kris Heylen,Dirk Geeraerts,Stefano De Pascale,Dirk Speelman*

Main category: cs.CL

TL;DR: 本文提出了一种基于依赖共现模式的词汇语义变化检测方法，该方法在性能上优于部分嵌入向量模型且结果更具可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于神经网络的词嵌入方法虽然在词汇语义变化检测中表现优异，但缺乏透明性和可解释性。

Method: 采用仅基于依赖共现模式的词汇统计特征来检测语义变化。

Result: 该方法在语义变化检测任务上优于多种传统分布式语义模型，同时结果更具可解释性。

Conclusion: 基于依赖共现模式的方法是检测词汇语义变化的有效且可解释的替代方案。

Abstract: Most modern computational approaches to lexical semantic change detection (LSC) rely on embedding-based distributional word representations with neural networks. Despite the strong performance on LSC benchmarks, they are often opaque. We investigate an alternative method which relies purely on dependency co-occurrence patterns of words. We demonstrate that it is effective for semantic change detection and even outperforms a number of distributional semantic models. We provide an in-depth quantitative and qualitative analysis of the predictions, showing that they are plausible and interpretable.

</details>


### [36] [Linear Script Representations in Speech Foundation Models Enable Zero-Shot Transliteration](https://arxiv.org/abs/2601.02906)
*Ryan Soh-Eun Shim,Kwanghee Choi,Kalvin Chang,Ming-Hao Hsu,Florian Eichin,Zhizheng Wu,Alane Suhr,Michael A. Hedderich,David Harwath,David R. Mortensen,Barbara Plank*

Main category: cs.CL

TL;DR: 本文提出了一种通过修改多语种语音模型激活向量来控制输出文本书写脚本的方法，有效解决了不同地区语言变体书写脚本不同导致的识别不确定性问题。


<details>
  <summary>Details</summary>
Motivation: 多语种语音基础模型训练于大规模网络数据，不同地区语言变体使用不同脚本，导致语音识别输出在脚本上不确定，影响使用体验。

Method: 通过分析发现脚本信息在线性激活空间中编码，研究者在推理时修改激活向量，添加脚本向量以直接控制输出脚本，甚至实现非常规语言-脚本组合。

Result: 该方法在Whisper模型各尺寸上均表现出竞争性能，能够有效实现输出脚本的后期控制。

Conclusion: 通过调整激活空间中的脚本向量，可实现对多语种语音识别输出脚本的直接、灵活控制，提升多语种模型的适用性和用户体验。

Abstract: Multilingual speech foundation models such as Whisper are trained on web-scale data, where data for each language consists of a myriad of regional varieties. However, different regional varieties often employ different scripts to write the same language, rendering speech recognition output also subject to non-determinism in the output script. To mitigate this problem, we show that script is linearly encoded in the activation space of multilingual speech models, and that modifying activations at inference time enables direct control over output script. We find the addition of such script vectors to activations at test time can induce script change even in unconventional language-script pairings (e.g. Italian in Cyrillic and Japanese in Latin script). We apply this approach to inducing post-hoc control over the script of speech recognition output, where we observe competitive performance across all model sizes of Whisper.

</details>


### [37] [Beyond the Black Box: Theory and Mechanism of Large Language Models](https://arxiv.org/abs/2601.02907)
*Zeyu Gan,Ruifeng Ren,Wei Yao,Xiaolin Hu,Gengze Xu,Chen Qian,Huayi Tang,Zixuan Gong,Xinhao Yao,Pengwei Tang,Zhenxing Dou,Yong Liu*

Main category: cs.CL

TL;DR: 本文提出了一个基于生命周期的统一分类法，对大规模语言模型的理论基础和内部机制进行了系统综述，并指出了未来的关键理论挑战。


<details>
  <summary>Details</summary>
Motivation: 当前大规模语言模型虽然工程上成功，但理论理解尚不完善，导致模型被视为“黑箱”，理论研究亟需整合。

Method: 构建包含数据准备、模型准备、训练、对齐、推理和评估六个阶段的生命周期框架，系统回顾基础理论和内部机制，分析核心理论问题。

Result: 系统梳理了数据混合的数学依据、架构的表示限制、对齐算法的优化动力学等理论问题，揭示了合成数据自我改进、安全保障及新兴智能等前沿挑战。

Conclusion: 通过将经验观察与科学方法结合，本文为从工程经验向科学理论转变提供了结构化路线图，推动大规模语言模型领域理论的发展。

Abstract: The rapid emergence of Large Language Models (LLMs) has precipitated a profound paradigm shift in Artificial Intelligence, delivering monumental engineering successes that increasingly impact modern society. However, a critical paradox persists within the current field: despite the empirical efficacy, our theoretical understanding of LLMs remains disproportionately nascent, forcing these systems to be treated largely as ``black boxes''. To address this theoretical fragmentation, this survey proposes a unified lifecycle-based taxonomy that organizes the research landscape into six distinct stages: Data Preparation, Model Preparation, Training, Alignment, Inference, and Evaluation. Within this framework, we provide a systematic review of the foundational theories and internal mechanisms driving LLM performance. Specifically, we analyze core theoretical issues such as the mathematical justification for data mixtures, the representational limits of various architectures, and the optimization dynamics of alignment algorithms. Moving beyond current best practices, we identify critical frontier challenges, including the theoretical limits of synthetic data self-improvement, the mathematical bounds of safety guarantees, and the mechanistic origins of emergent intelligence. By connecting empirical observations with rigorous scientific inquiry, this work provides a structured roadmap for transitioning LLM development from engineering heuristics toward a principled scientific discipline.

</details>


### [38] [RAL2M: Retrieval Augmented Learning-To-Match Against Hallucination in Compliance-Guaranteed Service Systems](https://arxiv.org/abs/2601.02917)
*Mengze Hong,Di Jiang,Jiangtao Wen,Zhiyang Su,Yawen Li,Yanjie Sun,Guan Wang,Chen Jason Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种新的检索增强学习匹配框架（RAL2M），通过将大语言模型重新定位为查询-响应匹配评判者，有效避免了生成式模型中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）驱动的服务系统中，幻觉问题严重，亟需通过显式知识绑定保证响应的合规性。

Method: 提出了RAL2M框架，将LLM作为匹配评判者，结合检索系统；引入查询自适应潜在集成策略，建模不同模型能力及相互依赖，形成校准后的共识决策。

Result: 在大规模基准测试中，所提方法显著优于强基线，有效利用了多模型的集体智慧，减少了幻觉问题。

Conclusion: RAL2M提供了生成模型的鲁棒替代方案，未来可通过潜在表示进一步提升模型性能，有望为合规响应提供更可靠解决方案。

Abstract: Hallucination is a major concern in LLM-driven service systems, necessitating explicit knowledge grounding for compliance-guaranteed responses. In this paper, we introduce Retrieval-Augmented Learning-to-Match (RAL2M), a novel framework that eliminates generation hallucination by repositioning LLMs as query-response matching judges within a retrieval-based system, providing a robust alternative to purely generative approaches. To further mitigate judgment hallucination, we propose a query-adaptive latent ensemble strategy that explicitly models heterogeneous model competence and interdependencies among LLMs, deriving a calibrated consensus decision. Extensive experiments on large-scale benchmarks demonstrate that the proposed method effectively leverages the "wisdom of the crowd" and significantly outperforms strong baselines. Finally, we discuss best practices and promising directions for further exploiting latent representations in future work.

</details>


### [39] [Memorization, Emergence, and Explaining Reversal Failures: A Controlled Study of Relational Semantics in LLMs](https://arxiv.org/abs/2601.02931)
*Yihua Zhu,Qianying Liu,Jiaxin Wang,Fei Cheng,Chaoran Liu,Akiko Aizawa,Sadao Kurohashi,Hidetoshi Shimodaira*

Main category: cs.CL

TL;DR: 本文研究自回归大型语言模型在处理对称和逆关系时是否真正掌握逻辑语义，并通过构建基于知识图谱的合成数据集进行训练和评估，发现逻辑语义在充足监督下出现，且逆转失败主要源于自回归的顺序偏差。


<details>
  <summary>Details</summary>
Motivation: 当前自回归大型语言模型在处理关系任务表现良好，但不清楚它们是否理解关系背后的逻辑语义（如对称性和逆元逻辑），以及逆转错误的原因是缺失关系语义还是顺序偏差。

Method: 构建一个基于知识图谱的合成框架，生成包含对称和逆关系的文本，从头训练GPT风格的自回归模型，评估模型的记忆能力、逻辑推理及对未见实体的上下文泛化能力，同时设计顺序匹配的正反测试和引入扩散模型作为对比基线。

Result: 发现当逻辑语义监督充分时，即使是浅层（2-3层）模型也会出现关系语义的明显跃迁，成功的泛化与中间层信号的稳定性一致。此外，顺序匹配的正反向测试和扩散模型基线表明，逆转失败主要由自回归顺序偏差导致，而非逆转语义的缺失。

Conclusion: 自回归大型语言模型能够学习关系的逻辑语义，且这种能力在充足的逻辑监督下出现。逆转类型的失败主要是由于模型的顺序偏差，而不是逻辑语义缺失，这为改进模型设计提供了方向。

Abstract: Autoregressive LLMs perform well on relational tasks that require linking entities via relational words (e.g., father/son, friend), but it is unclear whether they learn the logical semantics of such relations (e.g., symmetry and inversion logic) and, if so, whether reversal-type failures arise from missing relational semantics or left-to-right order bias. We propose a controlled Knowledge Graph-based synthetic framework that generates text from symmetric/inverse triples, train GPT-style autoregressive models from scratch, and evaluate memorization, logical inference, and in-context generalization to unseen entities to address these questions. We find a sharp phase transition in which relational semantics emerge with sufficient logic-bearing supervision, even in shallow (2-3 layer) models, and that successful generalization aligns with stable intermediate-layer signals. Finally, order-matched forward/reverse tests and a diffusion baseline indicate that reversal failures are primarily driven by autoregressive order bias rather than deficient inversion semantics.

</details>


### [40] [Pearmut: Human Evaluation of Translation Made Trivial](https://arxiv.org/abs/2601.02933)
*Vilém Zouhar,Tom Kocmi*

Main category: cs.CL

TL;DR: Pearmut 是一个轻量级、多功能的人类评估平台，专注于多语言评估，特别是机器翻译，简化了人类评估流程，使其与自动评估一样便捷。


<details>
  <summary>Details</summary>
Motivation: 人类评估是多语种自然语言处理的黄金标准，但由于现有工具设置复杂且耗时，常被自动指标替代。

Method: Pearmut 实现了标准评估协议（如 DA、ESA、MQM），支持文档级上下文、对比评价、注意力检测以及多种分配策略，同时允许新协议的原型设计。

Result: 该平台降低了人类评估的门槛，支持多语言任务，特别是机器翻译，多种功能集成提升评估效率和可靠性。

Conclusion: Pearmut 使得可靠的人类评估成为模型开发和诊断中常规且实用的环节，而非偶尔进行的工作。

Abstract: Human evaluation is the gold standard for multilingual NLP, but is often skipped in practice and substituted with automatic metrics, because it is notoriously complex and slow to set up with existing tools with substantial engineering and operational overhead. We introduce Pearmut, a lightweight yet feature-rich platform that makes end-to-end human evaluation as easy to run as automatic evaluation. Pearmut removes common entry barriers and provides support for evaluating multilingual tasks, with a particular focus on machine translation. The platform implements standard evaluation protocols, including DA, ESA, or MQM, but is also extensible to allow prototyping new protocols. It features document-level context, absolute and contrastive evaluation, attention checks, ESAAI pre-annotations and both static and active learning-based assignment strategies. Pearmut enables reliable human evaluation to become a practical, routine component of model development and diagnosis rather than an occasional effort.

</details>


### [41] [Enhancing Multilingual RAG Systems with Debiased Language Preference-Guided Query Fusion](https://arxiv.org/abs/2601.02956)
*Jeonghyun Park,Byeongjeong Kim,Seojin Hwang,Hwanhee Lee*

Main category: cs.CL

TL;DR: 本文分析了多语种检索增强生成系统中对英语的偏好问题，指出这种偏好很大程度上源于评价标准的结构性偏差。提出了去偏好指标DeLP和基于此的优化框架DELTA，显著提升了跨语言检索和生成性能。


<details>
  <summary>Details</summary>
Motivation: 当前多语种RAG系统显示出对高资源语言（尤其是英语）的显著偏好，这种偏好可能被现有评测标准中的结构性偏差所放大，影响了语言偏好的真实判断。

Method: 提出DeLP指标，用以校正和去除评测中的结构性偏差；基于DeLP发现检索器更偏好单语对齐，进而设计轻量级框架DELTA，利用单语对齐优化跨语检索和生成。

Result: 使用DeLP指标分析发现英语偏好主要由证据分布驱动非模型本身偏差；DELTA方法在多语种任务中优于传统的英语枢轴法和其他mRAG基线。

Conclusion: 论文揭示了多语种RAG系统中所谓的英语偏好是评价偏差导致；通过去偏差指标和基于单语对齐的优化框架，能够更真实地评估和提升跨语言检索与生成效果。

Abstract: Multilingual Retrieval-Augmented Generation (mRAG) systems often exhibit a perceived preference for high-resource languages, particularly English, resulting in the widespread adoption of English pivoting. While prior studies attribute this advantage to the superior English-centric capabilities of Large Language Models (LLMs), we find that such measurements are significantly distorted by structural priors inherent in evaluation benchmarks. Specifically, we identify exposure bias and a gold availability prior-both driven by the disproportionate concentration of resources in English-as well as cultural priors rooted in topic locality, as factors that hinder accurate assessment of genuine language preference. To address these biases, we propose DeLP (Debiased Language Preference), a calibrated metric designed to explicitly factor out these structural confounds. Our analysis using DeLP reveals that the previously reported English preference is largely a byproduct of evidence distribution rather than an inherent model bias. Instead, we find that retrievers fundamentally favor monolingual alignment between the query and the document language. Building on this insight, we introduce DELTA (DEbiased Language preference-guided Text Augmentation), a lightweight and efficient mRAG framework that strategically leverages monolingual alignment to optimize cross-lingual retrieval and generation. Experimental results demonstrate that DELTA consistently outperforms English pivoting and mRAG baselines across diverse languages.

</details>


### [42] [LLM-Augmented Changepoint Detection: A Framework for Ensemble Detection and Automated Explanation](https://arxiv.org/abs/2601.02957)
*Fabian Lukassen,Christoph Weisser,Michael Schlee,Manish Kumar,Anton Thielmann,Benjamin Saefken,Thomas Kneib*

Main category: cs.CL

TL;DR: 本文提出了一种结合集成统计方法和大型语言模型（LLMs）的时间序列变点检测框架，提高检测准确性和解释性。


<details>
  <summary>Details</summary>
Motivation: 当前变点检测方法局限性包括方法选择困难及缺乏自动化、上下文相关的解释，影响结果的准确性和实用性。

Method: 通过集成十种不同的变点检测算法，实现鲁棒且高性能的结果，同时利用LLM自动生成与变点相关的历史事件解释，针对特定领域数据采用文档检索增强生成技术（RAG）提升解释质量。

Result: 集成方法在多个领域优于单一算法，生成的上下文解释有效连接变点与现实事件，提升了模型的实用性和可信度。

Conclusion: 该开源Python框架能将统计检测结果转化为具行动指导价值的洞见，适用于金融、政治、环境等多领域，改善变点检测的准确性与解释能力。

Abstract: This paper introduces a novel changepoint detection framework that combines ensemble statistical methods with Large Language Models (LLMs) to enhance both detection accuracy and the interpretability of regime changes in time series data. Two critical limitations in the field are addressed. First, individual detection methods exhibit complementary strengths and weaknesses depending on data characteristics, making method selection non-trivial and prone to suboptimal results. Second, automated, contextual explanations for detected changes are largely absent. The proposed ensemble method aggregates results from ten distinct changepoint detection algorithms, achieving superior performance and robustness compared to individual methods. Additionally, an LLM-powered explanation pipeline automatically generates contextual narratives, linking detected changepoints to potential real-world historical events. For private or domain-specific data, a Retrieval-Augmented Generation (RAG) solution enables explanations grounded in user-provided documents. The open source Python framework demonstrates practical utility in diverse domains, including finance, political science, and environmental science, transforming raw statistical output into actionable insights for analysts and decision-makers.

</details>


### [43] [Low-Resource Heuristics for Bahnaric Optical Character Recognition Improvement](https://arxiv.org/abs/2601.02965)
*Phat Tran,Phuoc Pham,Hung Trinh,Tho Quan*

Main category: cs.CL

TL;DR: 本文针对越南柬埔寨老挝少数民族巴拿语文档数字化中的OCR识别准确率低的问题，提出结合表格和非表格检测技术及概率后处理方法来提升识别质量，实验显示准确率从72.86%提升至79.26%。


<details>
  <summary>Details</summary>
Motivation: 巴拿语作为少数民族语言，研究和数据稀缺，亟需准确的文档数字化来支持语言保护。

Method: 采用先进的表格与非表格检测算法提升输入图像质量，结合基于概率的后处理策略纠正OCR输出错误。

Result: 识别准确率显著提升，从72.86%提升到79.26%。

Conclusion: 该研究为巴拿语的数字化保存提供了有效方法和资源，同时该框架具有推广到其他少数民族语言数字化的潜力。

Abstract: Bahnar, a minority language spoken across Vietnam, Cambodia, and Laos, faces significant preservation challenges due to limited research and data availability. This study addresses the critical need for accurate digitization of Bahnar language documents through optical character recognition (OCR) technology. Digitizing scanned paper documents poses significant challenges, as degraded image quality from broken or blurred areas introduces considerable OCR errors that compromise information retrieval systems. We propose a comprehensive approach combining advanced table and non-table detection techniques with probability-based post-processing heuristics to enhance recognition accuracy. Our method first applies detection algorithms to improve input data quality, then employs probabilistic error correction on OCR output. Experimental results indicate a substantial improvement, with recognition accuracy increasing from 72.86% to 79.26%. This work contributes valuable resources for Bahnar language preservation and provides a framework applicable to other minority language digitization efforts.

</details>


### [44] [Reliability-Aware Adaptive Self-Consistency for Efficient Sampling in LLM Reasoning](https://arxiv.org/abs/2601.02970)
*Junseok Kim,Nakyeong Yang,Kyungmin Min,Kyomin Jung*

Main category: cs.CL

TL;DR: 本文提出了Reliability-Aware Adaptive Self-Consistency（ReASC）方法，通过利用响应置信度进行可靠性感知的自适应多样本聚合，提升推理效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的自适应自洽方法依赖基于计数的停止规则，忽视了响应质量，导致不必要的采样和高推理成本。

Method: ReASC采用两阶段策略：单样本决策阶段快速处理高置信度实例，多样本累积阶段结合响应频率和置信度进行聚合，动态调整采样预算。

Result: 在五个模型和四个数据集上，ReASC相较传统自洽方法在保持准确率的同时，推理成本降低最多达70%，尤其在3B至27B参数规模模型中表现优异。

Conclusion: ReASC通过引入置信度驱动的采样策略，有效提升了推理的准确性与效率，实现了优于现有自适应自洽方法的准确率与成本平衡。

Abstract: Self-Consistency improves reasoning reliability through multi-sample aggregation, but incurs substantial inference cost. Adaptive self-consistency methods mitigate this issue by adjusting the sampling budget; however, they rely on count-based stopping rules that treat all responses equally, often leading to unnecessary sampling. We propose Reliability-Aware Adaptive Self-Consistency (ReASC), which addresses this limitation by reframing adaptive sampling from response counting to evidence sufficiency, leveraging response-level confidence for principled information aggregation. ReASC operates in two stages: a single-sample decision stage that resolves instances confidently answerable from a single response, and a reliability-aware accumulation stage that aggregates responses by jointly leveraging their frequency and confidence. Across five models and four datasets, ReASC consistently achieves the best accuracy-cost trade-off compared to existing baselines, yielding improved inference efficiency across model scales from 3B to 27B parameters. As a concrete example, ReASC reduces inference cost by up to 70\% relative to self-consistency while preserving accuracy on GSM8K using Gemma-3-4B-it.

</details>


### [45] [Correct, Concise and Complete: Multi-stage Training For Adaptive Reasoning](https://arxiv.org/abs/2601.02972)
*Nathanaël Carraz Rakotonirina,Ren Pang,Neha Anna John,Michael Bohlke-Schneider,Momchil Hardalov*

Main category: cs.CL

TL;DR: 本文提出一种多阶段高效推理方法，通过监督微调和强化学习结合，减少大语言模型推理过程中的过度思考现象，有效降低响应长度并保持较高准确率。


<details>
  <summary>Details</summary>
Motivation: 当前大模型采用链式思考（CoT）推理时，推理过程往往过长，导致计算资源浪费甚至性能下降，即"过度思考"问题。

Method: 结合监督微调（通过拒绝采样或推理轨迹重构）和带自适应长度惩罚的强化学习，使用轻量级奖励函数惩罚多余生成的token，同时鼓励有益的自我校验。

Result: 在七个多样推理任务上，方法使8B模型响应长度平均减少28%，32B模型减少40%，仅造成1.6和2.5点的微小性能下降。

Conclusion: 该方法以简单的设计实现了优于复杂方法的准确率与响应长度平衡，$	ext{AUC}_{	ext{OAA}}$得分提升5点，表现出优秀的高效推理能力。

Abstract: The reasoning capabilities of large language models (LLMs) have improved substantially through increased test-time computation, typically in the form of intermediate tokens known as chain-of-thought (CoT). However, CoT often becomes unnecessarily long, increasing computation cost without actual accuracy gains or sometimes even degrading performance, a phenomenon known as ``overthinking''. We propose a multi-stage efficient reasoning method that combines supervised fine-tuning -- via rejection sampling or reasoning trace reformatting -- with reinforcement learning using an adaptive length penalty. We introduce a lightweight reward function that penalizes tokens generated after the first correct answer but encouraging self-verification only when beneficial. We conduct a holistic evaluation across seven diverse reasoning tasks, analyzing the accuracy-response length trade-off. Our approach reduces response length by an average of 28\% for 8B models and 40\% for 32B models, while incurring only minor performance drops of 1.6 and 2.5 points, respectively. Despite its conceptual simplicity, it achieves a superior trade-off compared to more complex state-of-the-art efficient reasoning methods, scoring 76.6, in terms of the area under the Overthinking-Adjusted Accuracy curve ($\text{AUC}_{\text{OAA}}$) -- 5 points above the base model and 2.5 points above the second-best approach.

</details>


### [46] [Mechanistic Knobs in LLMs: Retrieving and Steering High-Order Semantic Features via Sparse Autoencoders](https://arxiv.org/abs/2601.02978)
*Ruikang Zhang,Shuo Wang,Qi Su*

Main category: cs.CL

TL;DR: 本文提出了一种基于稀疏自编码器的框架，用于检索和操控大型语言模型内部与高级语言行为相关的语义特征，实现了对复杂行为语义属性的准确调控。


<details>
  <summary>Details</summary>
Motivation: 当前机械可解释性研究难以将内部特征与语言生成中的复杂行为级语义属性可靠关联与控制。

Method: 设计了基于对比特征检索和受控语义对立的稀疏自编码器框架，结合统计激活分析与生成验证提取单语义功能特征，应用于大五人格特征进行行为调控实验。

Result: 方法能够实现模型行为的双向精准调控，表现出优于现有激活调节方法（如对比激活加法）的稳定性和性能，发现了“功能忠实性”现象，表明干预内部特征能诱发多维度连贯语义变化。

Conclusion: 大语言模型内部蕴含着高级概念的深度整合表征，所提方法为复杂AI行为的调控提供了新颖且鲁棒的机制路径。

Abstract: Recent work in Mechanistic Interpretability (MI) has enabled the identification and intervention of internal features in Large Language Models (LLMs). However, a persistent challenge lies in linking such internal features to the reliable control of complex, behavior-level semantic attributes in language generation. In this paper, we propose a Sparse Autoencoder-based framework for retrieving and steering semantically interpretable internal features associated with high-level linguistic behaviors. Our method employs a contrastive feature retrieval pipeline based on controlled semantic oppositions, combing statistical activation analysis and generation-based validation to distill monosemantic functional features from sparse activation spaces. Using the Big Five personality traits as a case study, we demonstrate that our method enables precise, bidirectional steering of model behavior while maintaining superior stability and performance compared to existing activation steering methods like Contrastive Activation Addition (CAA). We further identify an empirical effect, which we term Functional Faithfulness, whereby intervening on a specific internal feature induces coherent and predictable shifts across multiple linguistic dimensions aligned with the target semantic attribute. Our findings suggest that LLMs internalize deeply integrated representations of high-order concepts, and provide a novel, robust mechanistic path for the regulation of complex AI behaviors.

</details>


### [47] [P-Check: Advancing Personalized Reward Model via Learning to Generate Dynamic Checklist](https://arxiv.org/abs/2601.02986)
*Kwangwook Seo,Dongha Lee*

Main category: cs.CL

TL;DR: 本文提出了P-Check框架，通过动态生成个性化评估标准，提升个性化奖励模型的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有个性化奖励建模方法多将用户上下文视为静态信号，无法充分捕捉人类判断的动态和多面性。

Method: 设计了P-Check框架，训练一个可插拔的清单生成器，利用偏好对比判别权重策略，动态合成个性化评估标准引导奖励预测。

Result: 实验显示P-Check提升了奖励预测准确性，增强了个性化生成效果，并在OOD（分布外）情境下表现稳健。

Conclusion: P-Check有效捕获个性化判断的动态特征，为个性化奖励建模提供了一种更精准、鲁棒的方法。

Abstract: Recent approaches in personalized reward modeling have primarily focused on leveraging user interaction history to align model judgments with individual preferences. However, existing approaches largely treat user context as a static or implicit conditioning signal, failing to capture the dynamic and multi-faceted nature of human judgment. In this paper, we propose P-Check, a novel personalized reward modeling framework, designed to train a plug-and-play checklist generator that synthesizes dynamic evaluation criteria for guiding the reward prediction. To better align these checklists with personalized nuances, we introduce Preference-Contrastive Criterion Weighting, a training strategy that assigns saliency scores to criteria based on their discriminative power for personalized judgment. We conduct extensive experiments and demonstrate that P-Check not only improves reward accuracy but also enhances downstream personalized generation, and remains robust in OOD scenarios.

</details>


### [48] [Mechanistic Interpretability of Large-Scale Counting in LLMs through a System-2 Strategy](https://arxiv.org/abs/2601.02989)
*Hosein Hasani,Mohammadali Banayeeanzade,Ali Nafisi,Sadegh Mohammadian,Fatemeh Askari,Mobin Bagherian,Amirmohammad Izadi,Mahdieh Soleymani Baghshah*

Main category: cs.CL

TL;DR: 大型语言模型在计数任务中存在系统性限制，本文提出了一种基于System-2认知过程的简单测试时策略，通过分解大规模计数任务提升计数准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理大规模计数问题时，由于变压器架构的深度限制，计数精度下降，存在系统性不足。

Method: 提出一种测试时策略，将大计数任务分解为较小独立的子任务，利用专用注意力头在模型内部传递和聚合计数信息，从而克服建筑限制。

Result: 实验显示该策略能使大型语言模型在大规模计数任务中达到高准确性，突破了原有架构的限制。

Conclusion: 该策略不仅提升了计数任务的表现，还揭示了大型语言模型中System-2式计数的机械机理，具备推广到其他推理任务的潜力。

Abstract: Large language models (LLMs), despite strong performance on complex mathematical problems, exhibit systematic limitations in counting tasks. This issue arises from architectural limits of transformers, where counting is performed across layers, leading to degraded precision for larger counting problems due to depth constraints. To address this limitation, we propose a simple test-time strategy inspired by System-2 cognitive processes that decomposes large counting tasks into smaller, independent sub-problems that the model can reliably solve. We evaluate this approach using observational and causal mediation analyses to understand the underlying mechanism of this System-2-like strategy. Our mechanistic analysis identifies key components: latent counts are computed and stored in the final item representations of each part, transferred to intermediate steps via dedicated attention heads, and aggregated in the final stage to produce the total count. Experimental results demonstrate that this strategy enables LLMs to surpass architectural limitations and achieve high accuracy on large-scale counting tasks. This work provides mechanistic insight into System-2 counting in LLMs and presents a generalizable approach for improving and understanding their reasoning behavior.

</details>


### [49] [Stable-RAG: Mitigating Retrieval-Permutation-Induced Hallucinations in Retrieval-Augmented Generation](https://arxiv.org/abs/2601.02993)
*Qianchi Zhang,Hainan Zhang,Liang Pang,Hongwei Zheng,Zhiming Zheng*

Main category: cs.CL

TL;DR: 本文研究了检索增强生成模型中检索结果顺序对大语言模型回答的影响，发现顺序变化导致模型输出波动，提出Stable-RAG方法通过多次顺序运行及聚类隐状态来稳定输出，显著提升准确率和一致性。


<details>
  <summary>Details</summary>
Motivation: 尽管RAG能减少模型的虚假生成，对检索文档顺序敏感性却未得到充分研究，且现有方法未能有效处理这一问题。

Method: 提出Stable-RAG，通过对多种检索顺序的生成结果进行聚类，利用聚类中心的隐状态进行解码，并引导模型输出一致且准确的答案。

Result: 在三个问答数据集上的实验显示，Stable-RAG较基线模型显著提升了回答准确度、推理一致性及跨数据集、检索器和输入长度的鲁棒性。

Conclusion: Stable-RAG有效缓解了检索顺序引起的模型输出不稳定问题，提升了RAG模型的准确性和一致性，具有良好的泛化能力。

Abstract: Retrieval-Augmented Generation (RAG) has become a key paradigm for reducing factual hallucinations in large language models (LLMs), yet little is known about how the order of retrieved documents affects model behavior. We empirically show that under Top-5 retrieval with the gold document included, LLM answers vary substantially across permutations of the retrieved set, even when the gold document is fixed in the first position. This reveals a previously underexplored sensitivity to retrieval permutations. Although robust RAG methods primarily focus on enhancing LLM robustness to low-quality retrieval and mitigating positional bias to distribute attention fairly over long contexts, neither approach directly addresses permutation sensitivity. In this paper, we propose Stable-RAG, which exploits permutation sensitivity estimation to mitigate permutation-induced hallucinations. Stable-RAG runs the generator under multiple retrieval orders, clusters hidden states, and decodes from a cluster-center representation that captures the dominant reasoning pattern. It then uses these reasoning results to align hallucinated outputs toward the correct answer, encouraging the model to produce consistent and accurate predictions across document permutations. Experiments on three QA datasets show that Stable-RAG significantly improves answer accuracy, reasoning consistency and robust generalization across datasets, retrievers, and input lengths compared with baselines.

</details>


### [50] [Large Reasoning Models Are (Not Yet) Multilingual Latent Reasoners](https://arxiv.org/abs/2601.02996)
*Yihong Liu,Raoyuan Zhao,Hinrich Schütze,Michael A. Hedderich*

Main category: cs.CL

TL;DR: 本文系统研究了大规模推理模型在11种语言下的潜在推理能力，发现其在多语言环境下均存在潜在的非文本推理过程，但表现因语言资源丰富度和任务难度有所不同。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模推理模型在数学推理任务中展现优秀成绩，且被认为依赖显式的链式思维解释，但模型实际可能在完成文本推理步骤之前就已得出正确答案，表现出潜在的非文本推理能力。然而这一现象在多语言环境下的表现尚不清楚。

Method: 通过截断基策略，逐步提供部分推理轨迹，测试模型在不同语言环境下如何形成正确答案的潜在预测；并使用表示分析方法，比较不同语言内部推理机制的相似性。

Result: 研究发现潜在推理在多语言中均存在，但在资源丰富语言表现更强，资源匮乏语言较弱；在更难任务中潜在推理表现较差；内部预测的演变过程在各语言间高度一致，呈现出以英语为中心的潜在推理路径。

Conclusion: 大规模推理模型的潜在推理能力具有多语言普适性，且不同语言虽然表现差异明显，但其内部推理机制高度一致，表明模型存在英语中心的潜在推理路径。

Abstract: Large reasoning models (LRMs) achieve strong performance on mathematical reasoning tasks, often attributed to their capability to generate explicit chain-of-thought (CoT) explanations. However, recent work shows that LRMs often arrive at the correct answer before completing these textual reasoning steps, indicating the presence of latent reasoning -- internal, non-verbal computation encoded in hidden states. While this phenomenon has been explored in English, its multilingual behavior remains largely unknown. In this paper, we conduct a systematic investigation of multilingual latent reasoning in LRMs across 11 languages. Using a truncation-based strategy, we examine how the correct answer emerges as the model is given only partial reasoning traces, allowing us to measure stepwise latent prediction formation. Our results reveal clear evidence of multilingual latent reasoning, though unevenly: strong in resource-rich languages, weaker in low-resource ones, and broadly less observable on harder benchmarks. To understand whether these differences reflect distinct internal mechanisms, we further perform representational analyses. Despite surface-level disparities, we find that the internal evolution of predictions is highly consistent across languages and broadly aligns with English -- a pattern suggesting an English-centered latent reasoning pathway.

</details>


### [51] [SentGraph: Hierarchical Sentence Graph for Multi-hop Retrieval-Augmented Question Answering](https://arxiv.org/abs/2601.03014)
*Junli Liang,Pengfei Zhou,Wangqiu Zhou,Wenjie Qing,Qi Zhao,Ziwen Wang,Qi Song,Xiangyang Li*

Main category: cs.CL

TL;DR: 提出了SentGraph，一种基于句子图结构的RAG框架，通过细粒度逻辑关系建模提升多跳问答性能。


<details>
  <summary>Details</summary>
Motivation: 传统RAG在多跳问答中因上下文不相关且逻辑不连贯导致证据链不完整和推理错误。

Method: 基于修辞结构理论构建分层句子图，结合跨文档实体桥梁进行图引导的证据选择和路径扩展，实现细粒度句子级证据检索。

Result: 在四个多跳问答基准测试中，SentGraph表现出色，有效验证句子级逻辑依赖建模的重要性。

Conclusion: 显式建模句子级逻辑关系能够显著提升多跳问答的推理能力和答案准确性。

Abstract: Traditional Retrieval-Augmented Generation (RAG) effectively supports single-hop question answering with large language models but faces significant limitations in multi-hop question answering tasks, which require combining evidence from multiple documents. Existing chunk-based retrieval often provides irrelevant and logically incoherent context, leading to incomplete evidence chains and incorrect reasoning during answer generation. To address these challenges, we propose SentGraph, a sentence-level graph-based RAG framework that explicitly models fine-grained logical relationships between sentences for multi-hop question answering. Specifically, we construct a hierarchical sentence graph offline by first adapting Rhetorical Structure Theory to distinguish nucleus and satellite sentences, and then organizing them into topic-level subgraphs with cross-document entity bridges. During online retrieval, SentGraph performs graph-guided evidence selection and path expansion to retrieve fine-grained sentence-level evidence. Extensive experiments on four multi-hop question answering benchmarks demonstrate the effectiveness of SentGraph, validating the importance of explicitly modeling sentence-level logical dependencies for multi-hop reasoning.

</details>


### [52] [MMFormalizer: Multimodal Autoformalization in the Wild](https://arxiv.org/abs/2601.03017)
*Jing Xiong,Qi Han,Yunta Hsieh,Hui Shen,Huajian Xin,Chaofan Tao,Chenyang Zhao,Hengyuan Zhang,Taiqiang Wu,Zhen Zhang,Haochen Wang,Zhongwei Wan,Lingpeng Kong,Ngai Wong*

Main category: cs.CL

TL;DR: 提出了MMFormalizer，一种结合视觉信息实现多模态自动形式化的方法，用于将自然语言数学和物理知识转化为形式化语句，支持经典力学、相对论、量子力学和热力学。


<details>
  <summary>Details</summary>
Motivation: 传统自动形式化面临多模态世界中的挑战，特别是在物理领域需要从视觉信息推断隐藏约束。

Method: MMFormalizer通过递归地将视觉感知的基本元素转化为形式命题，利用自适应递归终止机制确保抽象过程基于视觉证据，并结合数学和物理领域的实体和公理。

Result: 在新的多模态自动形式化基准PhyX-AF上评测，GPT-5和Gemini-3-Pro表现优异，GPT-5在物理推理上表现最强，几何领域最具挑战。

Conclusion: MMFormalizer实现了一个统一的多模态自动形式化框架，首次能处理包括哈密顿量导出的经典力学及其它物理学分支，促进感知与形式推理的结合。

Abstract: Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io

</details>


### [53] [Dementia-R1: Reinforced Pretraining and Reasoning from Unstructured Clinical Notes for Real-World Dementia Prognosis](https://arxiv.org/abs/2601.03018)
*Choonghan Kim,Hyunmin Hwang,Hangeol Chang,Jaemin Kim,Jinse Park,Jae-Sung Lim,Jong Chul Ye*

Main category: cs.CL

TL;DR: 本文提出Dementia-R1，一种基于强化学习的纵向痴呆症预测框架，能有效处理多次访视中复杂的症状演变，显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然在临床文本理解上表现优异，但在需要跨多次访视推理复杂、非单调症状轨迹的痴呆症纵向预测任务上存在困难，且传统监督学习缺乏症状演变标注，强化学习因奖励稀疏受限。

Method: 提出Cold-Start强化学习策略，先训练模型预测从病人历史中提取的可验证临床指标，提升模型对疾病进展的推理能力，随后进行最终临床状态预测。

Result: 在真实非结构化临床数据集上，Dementia-R1取得了77.03%的F1分数；在ADNI基准测试中，7B模型性能媲美GPT-4o，成功捕捉认知功能波动轨迹。

Conclusion: Dementia-R1有效克服了痴呆症纵向预测中症状演变推理的挑战，展现出强大的实用价值和竞争力。

Abstract: While Large Language Models (LLMs) have shown strong performance on clinical text understanding, they struggle with longitudinal prediction tasks such as dementia prognosis, which require reasoning over complex, non-monotonic symptom trajectories across multiple visits. Standard supervised training lacks explicit annotations for symptom evolution, while direct Reinforcement Learning (RL) is hindered by sparse binary rewards. To address this challenge, we introduce Dementia-R1, an RL-based framework for longitudinal dementia prognosis from unstructured clinical notes. Our approach adopts a Cold-Start RL strategy that pre-trains the model to predict verifiable clinical indices extracted from patient histories, enhancing the capability to reason about disease progression before determining the final clinical status. Extensive experiments demonstrate that Dementia-R1 achieves an F1 score of 77.03% on real-world unstructured clinical datasets. Notably, on the ADNI benchmark, our 7B model rivals GPT-4o, effectively capturing fluctuating cognitive trajectories. Code is available at https://anonymous.4open.science/r/dementiar1-CDB5

</details>


### [54] [MedDialogRubrics: A Comprehensive Benchmark and Evaluation Framework for Multi-turn Medical Consultations in Large Language Models](https://arxiv.org/abs/2601.03023)
*Lecheng Gong,Weimin Fang,Ting Yang,Dongjie Tao,Chunxiao Guo,Peng Wei,Bo Xie,Jinqun Guan,Zixiao Chen,Fang Shi,Jinjie Gu,Junwei Liu*

Main category: cs.CL

TL;DR: 本文提出了MedDialogRubrics，一个包含5200个患者案例和超过6万条细化评估标准的医疗对话大模型评估基准，侧重多轮诊断能力的测评，通过多代理系统合成数据，确保隐私与临床合理性，评估结果显示现有模型在多维度表现不足，需改进对话管理架构。


<details>
  <summary>Details</summary>
Motivation: 现有医疗大语言模型的信息收集与诊断推理能力评测缺乏严格的基准和框架，且数据隐私问题突出，制约了医疗对话系统的发展。

Method: 设计了MedDialogRubrics基准，利用多代理系统合成患者数据和主诉，避免使用真实电子健康记录；构建具有动态指导机制的患者代理以保证对话的内在一致性和临床可信度；提出基于大语言模型和专家标注的结构化评分标准生成流程，并结合循证医学指南筛选优先询问项用于评价。

Result: 对多款先进模型的全面评测表明，当前模型在多项指标上表现不佳，存在较大挑战，尤其在多轮诊断推理和对话管理方面的能力不足。

Conclusion: 提升医疗对话系统性能不仅需基础模型微调，更需在对话管理架构上做出突破，以实现更安全、高效的医疗诊断对话。

Abstract: Medical conversational AI (AI) plays a pivotal role in the development of safer and more effective medical dialogue systems. However, existing benchmarks and evaluation frameworks for assessing the information-gathering and diagnostic reasoning abilities of medical large language models (LLMs) have not been rigorously evaluated. To address these gaps, we present MedDialogRubrics, a novel benchmark comprising 5,200 synthetically constructed patient cases and over 60,000 fine-grained evaluation rubrics generated by LLMs and subsequently refined by clinical experts, specifically designed to assess the multi-turn diagnostic capabilities of LLM. Our framework employs a multi-agent system to synthesize realistic patient records and chief complaints from underlying disease knowledge without accessing real-world electronic health records, thereby mitigating privacy and data-governance concerns. We design a robust Patient Agent that is limited to a set of atomic medical facts and augmented with a dynamic guidance mechanism that continuously detects and corrects hallucinations throughout the dialogue, ensuring internal coherence and clinical plausibility of the simulated cases. Furthermore, we propose a structured LLM-based and expert-annotated rubric-generation pipeline that retrieves Evidence-Based Medicine (EBM) guidelines and utilizes the reject sampling to derive a prioritized set of rubric items ("must-ask" items) for each case. We perform a comprehensive evaluation of state-of-the-art models and demonstrate that, across multiple assessment dimensions, current models face substantial challenges. Our results indicate that improving medical dialogue will require advances in dialogue management architectures, not just incremental tuning of the base-model.

</details>


### [55] [LittiChoQA: Literary Texts in Indic Languages Chosen for Question Answering](https://arxiv.org/abs/2601.03025)
*Aarya Khandelwal,Ritwik Mishra,Rajiv Ratn Shah*

Main category: cs.CL

TL;DR: 本文介绍了面向印度恒河平原多种语言的最大文学长上下文问答数据集LittiChoQA，并评估了多模型在长短上下文下的表现及效率权衡。


<details>
  <summary>Details</summary>
Motivation: 针对现有多语言长上下文文学问答资源稀缺，特别是印度诸语言，文章旨在建立大规模数据集以推动该领域研究。

Method: 构建超过27万对自动生成的问答对，均衡事实型与非事实型，基于开放网络的自然文学文本；评测多种多语言大模型在抽象型问答上的性能，包括全文上下文和缩短上下文两种设置。

Result: 全文上下文微调达到最佳语义和令牌级别评分，缩短上下文提升处理效率；Krutrim-2模型表现最优，语义分分别为76.1（全文）、74.9（段落选取）、71.4（基于向量检索）。

Conclusion: 长上下文处理提升问答质量，但计算负担较大；缩短上下文能显著提高效率，且仍保持较好性能。数据集和评测揭示指标与效率的权衡，为多语言文学问答研究提供重要资源和基准。

Abstract: Long-context question answering (QA) over literary texts poses significant challenges for modern large language models, particularly in low-resource languages. We address the scarcity of long-context QA resources for Indic languages by introducing LittiChoQA, the largest literary QA dataset to date covering many languages spoken in the Gangetic plains of India. The dataset comprises over 270K automatically generated question-answer pairs with a balanced distribution of factoid and non-factoid questions, generated from naturally authored literary texts collected from the open web. We evaluate multiple multilingual LLMs on non-factoid, abstractive QA, under both full-context and context-shortened settings. Results demonstrate a clear trade-off between performance and efficiency: full-context fine-tuning yields the highest token-level and semantic-level scores, while context shortening substantially improves throughput. Among the evaluated models, Krutrim-2 achieves the strongest performance, obtaining a semantic score of 76.1 with full context. While, in shortened context settings it scores 74.9 with answer paragraph selection and 71.4 with vector-based retrieval. Qualitative evaluations further corroborate these findings.

</details>


### [56] [Reducing Hallucinations in LLMs via Factuality-Aware Preference Learning](https://arxiv.org/abs/2601.03027)
*Sindhuja Chaduvula,Ahmed Y. Radwan,Azib Farooq,Yani Ioannou,Shaina Raza*

Main category: cs.CL

TL;DR: 本文提出的F-DPO方法通过二元事实性标签改进了偏好对齐中的事实正确性，显著降低了幻觉率，提升了大型语言模型的事实性和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统偏好对齐如RLHF和DPO虽然提升了指令执行能力，但易因奖励流畅度和置信度导致模型产生幻觉（错误信息）。

Method: F-DPO在DPO基础上引入事实性标签，进行标签翻转以纠正偏好顺序，并加入事实感知的边距调整，强调事实差异明显的响应对。训练数据中结合了事实性指示和合成幻觉样本。

Result: 在七个不同大小的公开LLM上，F-DPO稳定提高事实性，显著减少幻觉率。如Qwen3-8B模型的幻觉率降低了5倍，事实评分提升了50%。在TruthfulQA测试中取得明显准确率提升。

Conclusion: F-DPO无需额外奖励模型或复杂注释和多阶段训练，即能有效提升模型事实准确性，并减少幻觉，具有良好的泛化能力。

Abstract: Preference alignment methods such as RLHF and Direct Preference Optimization (DPO) improve instruction following, but they can also reinforce hallucinations when preference judgments reward fluency and confidence over factual correctness. We introduce F-DPO (Factuality-aware Direct Preference Optimization), a simple extension of DPO that uses only binary factuality labels. F-DPO (i) applies a label-flipping transformation that corrects misordered preference pairs so the chosen response is never less factual than the rejected one, and (ii) adds a factuality-aware margin that emphasizes pairs with clear correctness differences, while reducing to standard DPO when both responses share the same factuality. We construct factuality-aware preference data by augmenting DPO pairs with binary factuality indicators and synthetic hallucinated variants. Across seven open-weight LLMs (1B-14B), F-DPO consistently improves factuality and reduces hallucination rates relative to both base models and standard DPO. On Qwen3-8B, F-DPO reduces hallucination rates by five times (from 0.424 to 0.084) while improving factuality scores by 50 percent (from 5.26 to 7.90). F-DPO also generalizes to out-of-distribution benchmarks: on TruthfulQA, Qwen2.5-14B achieves plus 17 percent MC1 accuracy (0.500 to 0.585) and plus 49 percent MC2 accuracy (0.357 to 0.531). F-DPO requires no auxiliary reward model, token-level annotations, or multi-stage training.

</details>


### [57] [NorwAI's Large Language Models: Technical Report](https://arxiv.org/abs/2601.03034)
*Jon Atle Gulla,Peng Liu,Lemei Zhang*

Main category: cs.CL

TL;DR: NorLLM团队开发了多款专为挪威语及斯堪的纳维亚语言设计的Transformer模型，提升了这些语言在NLP领域的应用表现。


<details>
  <summary>Details</summary>
Motivation: 挪威语用户众多但在NLP领域被严重低估，需专门模型提升其语言处理能力。

Method: 基于GPT、Mistral、Llama2等多种架构，采用从零预训练和持续预训练，使用挪威语扩展的分词器及先进的后期训练策略优化模型。

Result: 训练出的模型在多种任务上表现良好，且指令微调版本展现出强大的助手功能，适合实际交互和专业领域使用。

Conclusion: NorLLM模型有效填补了挪威语NLP的空白，具备实践应用潜力，且向北欧组织开放使用。

Abstract: Norwegian, spoken by approximately five million people, remains underrepresented in many of the most significant breakthroughs in Natural Language Processing (NLP). To address this gap, the NorLLM team at NorwAI has developed a family of models specifically tailored to Norwegian and other Scandinavian languages, building on diverse Transformer-based architectures such as GPT, Mistral, Llama2, Mixtral and Magistral. These models are either pretrained from scratch or continually pretrained on 25B - 88.45B tokens, using a Norwegian-extended tokenizer and advanced post-training strategies to optimize performance, enhance robustness, and improve adaptability across various real-world tasks. Notably, instruction-tuned variants (e.g., Mistral-7B-Instruct and Mixtral-8x7B-Instruct) showcase strong assistant-style capabilities, underscoring their potential for practical deployment in interactive and domain-specific applications. The NorwAI large language models are openly available to Nordic organizations, companies and students for both research and experimental use. This report provides detailed documentation of the model architectures, training data, tokenizer design, fine-tuning strategies, deployment, and evaluations.

</details>


### [58] [BaseCal: Unsupervised Confidence Calibration via Base Model Signals](https://arxiv.org/abs/2601.03042)
*Hexiang Tan,Wanli Yang,Junwei Zhang,Xin Chen,Rui Tang,Du Su,Jingang Wang,Yuanzhuo Wang,Fei Sun,Xueqi Cheng*

Main category: cs.CL

TL;DR: 本文提出了BaseCal方法，通过利用基础大语言模型（Base LLM）来校准后训练大语言模型（PoLLM）的过度自信问题，从而显著提升置信度可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的后训练大语言模型普遍存在严重的过度自信，影响置信度的可信度，而其对应的基础大语言模型通常校准良好，激发了利用基础模型作为参考来校准PoLLM置信度的想法。

Method: 提出两种方法：1）BaseCal-ReEval，将PoLLM的回答重新输入基础模型获得置信度，但计算开销较大；2）BaseCal-Proj，训练一个轻量映射将PoLLM的隐藏层状态映射到基础模型的隐藏层，再通过基础模型输出层计算校准置信度，无需额外推理开销。该方法无监督、即插即用，无需人工标注或模型改动。

Result: 在5个数据集和3个大语言模型家族的实验中，BaseCal方法有效减少了平均42.90%的期望校准误差（ECE），优于目前最佳无监督基线方法。

Conclusion: 利用基础大语言模型作为参考，实现对后训练模型置信度的无监督校准，是提升置信度可靠性和减少过度自信的有效途径，且具备良好的适用性和实用价值。

Abstract: Reliable confidence is essential for trusting the outputs of LLMs, yet widely deployed post-trained LLMs (PoLLMs) typically compromise this trust with severe overconfidence. In contrast, we observe that their corresponding base LLMs often remain well-calibrated. This naturally motivates us to calibrate PoLLM confidence using the base LLM as a reference. This work proposes two ways to achieve this. A straightforward solution, BaseCal-ReEval, evaluates PoLLM's responses by feeding them into the base LLM to get average probabilities as confidence. While effective, this approach introduces additional inference overhead. To address this, we propose BaseCal-Proj, which trains a lightweight projection to map the final-layer hidden states of PoLLMs back to those of their base LLMs. These projected states are then processed by the base LLM's output layer to derive base-calibrated confidence for PoLLM's responses. Notably, BaseCal is an unsupervised, plug-and-play solution that operates without human labels or LLM modifications. Experiments across five datasets and three LLM families demonstrate the effectiveness of BaseCal, reducing Expected Calibration Error (ECE) by an average of 42.90\% compared to the best unsupervised baselines.

</details>


### [59] [Lil: Less is Less When Applying Post-Training Sparse-Attention Algorithms in Long-Decode Stage](https://arxiv.org/abs/2601.03043)
*Junhao Hu,Fangze Li,Mingtao Xu,Feifan Meng,Shiju Zhao,Tiancheng Hu,Ting Peng,Anmin Liu,Wenrui Huang,Chenxu Liu,Ziyue Hua,Tao Xie*

Main category: cs.CL

TL;DR: 稀疏注意力虽减少解码复杂度，但可能导致序列加长，提出早停算法解决此问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型推理效率需求高，稀疏注意力虽降低解码复杂度，但信息丢失导致序列加长，反而增加整体复杂度。

Method: 提出早停算法，检测信息损失超过信息增益的阈值，提前停止解码过程。

Result: 早停算法在推理密集型基准测试中减少了90%的令牌消耗，准确率仅降低不到2%。

Conclusion: 早停算法有效缓解稀疏注意力导致的序列加长问题，提高推理效率，且几乎不损失准确性。

Abstract: Large language models (LLMs) demonstrate strong capabilities across a wide range of complex tasks and are increasingly deployed at scale, placing significant demands on inference efficiency. Prior work typically decomposes inference into prefill and decode stages, with the decode stage dominating total latency. To reduce time and memory complexity in the decode stage, a line of work introduces sparse-attention algorithms. In this paper, we show, both empirically and theoretically, that sparse attention can paradoxically increase end-to-end complexity: information loss often induces significantly longer sequences, a phenomenon we term ``Less is Less'' (Lil). To mitigate the Lil problem, we propose an early-stopping algorithm that detects the threshold where information loss exceeds information gain during sparse decoding. Our early-stopping algorithm reduces token consumption by up to 90% with a marginal accuracy degradation of less than 2% across reasoning-intensive benchmarks.

</details>


### [60] [Temporal Graph Network: Hallucination Detection in Multi-Turn Conversation](https://arxiv.org/abs/2601.03051)
*Vidhi Rathore,Sambu Aneesh,Himanshu Singh*

Main category: cs.CL

TL;DR: 本文提出了一种基于图的多轮对话幻想检测方法，通过将对话整体表示为时序图并利用句子变换器编码对话节点，结合共享实体边和时间边，实现信息传递和上下文感知的嵌入聚合，显著提升了幻想检测效果。


<details>
  <summary>Details</summary>
Motivation: 多轮对话中上下文变化及矛盾可能导致对话生成系统产生幻想，现有方法效果有限，亟需一种能够有效利用对话整体结构的检测方法。

Method: 将整个对话表示为时序图，节点为对话轮次，边分为共享实体边和时间边，采用消息传递更新节点嵌入，结合注意力池化生成上下文感知的向量，最终由分类器判断幻想类型及存在性。

Result: 该方法在幻想检测任务上性能略优于现有方法，并且通过注意力机制能够解释决策过程，具有较好的实用性和可解释性。

Conclusion: 基于时序图和消息传递的新方法提高了多轮对话幻想检测的准确性，且其注意力机制增强了模型透明度，为对话系统中的幻想检测提供了有效手段。

Abstract: Hallucinations can be produced by conversational AI systems, particularly in multi-turn conversations where context changes and contradictions may eventually surface. By representing the entire conversation as a temporal graph, we present a novel graph-based method for detecting dialogue-level hallucinations. Our framework models each dialogue as a node, encoding it using a sentence transformer. We explore two different ways of connectivity: i) shared-entity edges, which connect turns that refer to the same entities; ii) temporal edges, which connect contiguous turns in the conversation. Message-passing is used to update the node embeddings, allowing flow of information between related nodes. The context-aware node embeddings are then combined using attention pooling into a single vector, which is then passed on to a classifier to determine the presence and type of hallucinations. We demonstrate that our method offers slightly improved performance over existing methods. Further, we show the attention mechanism can be used to justify the decision making process. The code and model weights are made available at: https://github.com/sambuaneesh/anlp-project.

</details>


### [61] [Detecting Hallucinations in Retrieval-Augmented Generation via Semantic-level Internal Reasoning Graph](https://arxiv.org/abs/2601.03052)
*Jianpeng Hu,Yanzeng Li,Jialun Zhong,Wenfa Qi,Lei Zou*

Main category: cs.CL

TL;DR: 本文提出了一种基于语义级内部推理图的信实幻觉检测方法，利用扩展的层次关联传播算法构建推理图，提升了检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有检测信实幻觉的方法无法充分捕捉大语言模型的内部推理过程，导致判别器难以有效学习。

Method: 将层次关联传播算法从词元级扩展到语义级，构建内部推理图，基于小型预训练模型设计通用框架，利用推理依赖关系进行训练和幻觉检测，并通过阈值动态调整正确样本的通过率。

Result: 在RAGTruth和Dolly-15k数据集上的实验结果表明，所提方法在整体性能上优于当前最先进的基线方法。

Conclusion: 基于语义级内部推理图的方法能够更准确地捕捉模型推理依赖，提高信实幻觉的检测效果，具有较好的应用前景。

Abstract: The Retrieval-augmented generation (RAG) system based on Large language model (LLM) has made significant progress. It can effectively reduce factuality hallucinations, but faithfulness hallucinations still exist. Previous methods for detecting faithfulness hallucinations either neglect to capture the models' internal reasoning processes or handle those features coarsely, making it difficult for discriminators to learn. This paper proposes a semantic-level internal reasoning graph-based method for detecting faithfulness hallucination. Specifically, we first extend the layer-wise relevance propagation algorithm from the token level to the semantic level, constructing an internal reasoning graph based on attribution vectors. This provides a more faithful semantic-level representation of dependency. Furthermore, we design a general framework based on a small pre-trained language model to utilize the dependencies in LLM's reasoning for training and hallucination detection, which can dynamically adjust the pass rate of correct samples through a threshold. Experimental results demonstrate that our method achieves better overall performance compared to state-of-the-art baselines on RAGTruth and Dolly-15k.

</details>


### [62] [Do LLMs Encode Functional Importance of Reasoning Tokens?](https://arxiv.org/abs/2601.03066)
*Janvijay Singh,Dilek Hakkani-Tür*

Main category: cs.CL

TL;DR: 本文提出了一种贪婪修剪方法，通过删除对模型预测影响最小的推理词元，实现推理链的长度控制，提高模型推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在生成复杂推理链时计算成本高且难以识别功能相关的重要推理步骤，之前的方法未能充分揭示模型是否内部编码了词元级的重要性。

Method: 提出了一种贪婪修剪策略，基于最大化似然的目标迭代删除对模型预测影响较小的推理词元，从而获得更短但有效的推理链。

Result: 在蒸馏框架中，基于修剪链训练的学生模型在相同推理长度下优于前沿模型监督的压缩基线，且修剪模式存在系统性，注意力得分能较好地预测修剪排名。

Conclusion: 模型内部确实编码了推理词元的功能性重要结构，贪婪修剪能够有效控制推理链长度，并提升模型压缩和推理效率。

Abstract: Large language models solve complex tasks by generating long reasoning chains, achieving higher accuracy at the cost of increased computational cost and reduced ability to isolate functionally relevant reasoning. Prior work on compact reasoning shortens such chains through probabilistic sampling, heuristics, or supervision from frontier models, but offers limited insight into whether models internally encode token-level functional importance for answer generation. We address this gap diagnostically and propose greedy pruning, a likelihood-preserving deletion procedure that iteratively removes reasoning tokens whose removal minimally degrades model likelihood under a specified objective, yielding length-controlled reasoning chains. We evaluate pruned reasoning in a distillation framework and show that students trained on pruned chains outperform a frontier-model-supervised compression baseline at matched reasoning lengths. Finally, our analysis reveals systematic pruning patterns and shows that attention scores can predict greedy pruning ranks, further suggesting that models encode a nontrivial functional importance structure over reasoning tokens.

</details>


### [63] [Learning to Diagnose and Correct Moral Errors: Towards Enhancing Moral Sensitivity in Large Language Models](https://arxiv.org/abs/2601.03079)
*Bocheng Chen,Han Zi,Xi Chen,Xitong Zhang,Kristen Johnson,Guangliang Liu*

Main category: cs.CL

TL;DR: 本文提出了两种实用推理方法，以提升大语言模型的道德敏感性，能够识别并纠正道德错误，从而使模型更符合人类道德价值观。


<details>
  <summary>Details</summary>
Motivation: 当前虽然有许多方法试图让大语言模型与人类道德价值观一致，但如何使其具备道德敏感性仍然极具挑战。

Method: 提出了两种基于推理的实用方法，帮助大语言模型诊断输入内容的道德风险与安全，并纠正道德错误，这些方法基于统一的推理视角，注重推理负担设计。

Result: 实验证据表明，这些推理方法能显著提升大语言模型的道德敏感性，在多个代表性的道德相关基准测试中表现优异。

Conclusion: 通过基于推理负担的统一视角设计推理程序，是提升大语言模型道德敏感性的有效途径。

Abstract: Moral sensitivity is fundamental to human moral competence, as it guides individuals in regulating everyday behavior. Although many approaches seek to align large language models (LLMs) with human moral values, how to enable them morally sensitive has been extremely challenging. In this paper, we take a step toward answering the question: how can we enhance moral sensitivity in LLMs? Specifically, we propose two pragmatic inference methods that faciliate LLMs to diagnose morally benign and hazardous input and correct moral errors, whereby enhancing LLMs' moral sensitivity. A central strength of our pragmatic inference methods is their unified perspective: instead of modeling moral discourses across semantically diverse and complex surface forms, they offer a principled perspective for designing pragmatic inference procedures grounded in their inferential loads. Empirical evidence demonstrates that our pragmatic methods can enhance moral sensitivity in LLMs and achieves strong performance on representative morality-relevant benchmarks.

</details>


### [64] [Grad-ELLM: Gradient-based Explanations for Decoder-only LLMs](https://arxiv.org/abs/2601.03089)
*Xin Huang,Antoni B. Chan*

Main category: cs.CL

TL;DR: 本文提出了针对解码器结构的大型语言模型的梯度归因方法Grad-ELLM，通过结合注意力层梯度和注意力图生成高保真热力图，并引入两种新的保真度指标，在多任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的归因方法多为模型无关，缺乏对变换器架构的针对性，导致归因结果保真度不足。

Method: 利用输出logit对注意力层梯度的通道重要性和注意力图的空间重要性进行聚合，在不修改模型结构的情况下生成逐步归因热力图，同时设计了控制信息量的保真度指标$π$-Soft-NC和$π$-Soft-NS。

Result: 在情感分类、问答及开放式生成任务中，Grad-ELLM显示出比其他归因方法更优的保真度表现。

Conclusion: Grad-ELLM有效提升了解码器结构大型语言模型的输入归因的解释性和保真度，具有广泛应用潜力。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse tasks, yet their black-box nature raises concerns about transparency and faithfulness. Input attribution methods aim to highlight each input token's contributions to the model's output, but existing approaches are typically model-agnostic, and do not focus on transformer-specific architectures, leading to limited faithfulness. To address this, we propose Grad-ELLM, a gradient-based attribution method for decoder-only transformer-based LLMs. By aggregating channel importance from gradients of the output logit with respect to attention layers and spatial importance from attention maps, Grad-ELLM generates heatmaps at each generation step without requiring architectural modifications. Additionally, we introduce two faithfulneses metrics $π$-Soft-NC and $π$-Soft-NS, which are modifications of Soft-NC/NS that provide fairer comparisons by controlling the amount of information kept when perturbing the text. We evaluate Grad-ELLM on sentiment classification, question answering, and open-generation tasks using different models. Experiment results show that Grad-ELLM consistently achieves superior faithfulness than other attribution methods.

</details>


### [65] [Who Laughs with Whom? Disentangling Influential Factors in Humor Preferences across User Clusters and LLMs](https://arxiv.org/abs/2601.03103)
*Soichiro Murakami,Hidetaka Kamigaito,Hiroya Takamura,Manabu Okumura*

Main category: cs.CL

TL;DR: 本文通过聚类用户并利用Bradley-Terry-Luce模型估计不同幽默偏好的权重，分析了日本Oogiri游戏中幽默偏好的异质性，并用大语言模型模拟不同用户群的幽默选择偏好，通过角色提示实现定向幽默偏好生成。


<details>
  <summary>Details</summary>
Motivation: 幽默偏好因个体和文化差异巨大，导致使用大语言模型评估幽默存在挑战，因此需要建模不同用户的幽默偏好异质性。

Method: 通过用户投票日志聚类，使用Bradley-Terry-Luce模型对群体的幽默偏好权重进行估计，并通过提示大语言模型选择更有趣的回复，最后利用角色提示引导模型生成特定群体风格的幽默内容。

Result: 发现不同用户群表现出显著不同的幽默偏好模式，大语言模型生成的幽默偏好与特定用户群体相似，且通过角色提示可有效定向模型偏好。

Conclusion: 本研究证明了利用用户聚类和Bradley-Terry-Luce模型能够有效刻画幽默偏好的多样性，且大语言模型可以通过角色提示调整幽默风格，提高幽默内容个性化生成的可控性。

Abstract: Humor preferences vary widely across individuals and cultures, complicating the evaluation of humor using large language models (LLMs). In this study, we model heterogeneity in humor preferences in Oogiri, a Japanese creative response game, by clustering users with voting logs and estimating cluster-specific weights over interpretable preference factors using Bradley-Terry-Luce models. We elicit preference judgments from LLMs by prompting them to select the funnier response and found that user clusters exhibit distinct preference patterns and that the LLM results can resemble those of particular clusters. Finally, we demonstrate that, by persona prompting, LLM preferences can be directed toward a specific cluster. The scripts for data collection and analysis will be released to support reproducibility.

</details>


### [66] [Discovering and Causally Validating Emotion-Sensitive Neurons in Large Audio-Language Models](https://arxiv.org/abs/2601.03115)
*Xiutian Zhao,Björn Schuller,Berrak Sisman*

Main category: cs.CL

TL;DR: 本文首次在大规模音频语言模型中进行情感敏感神经元的可解释性研究，确定了情感神经元的存在并揭示其对情感识别的因果影响。


<details>
  <summary>Details</summary>
Motivation: 虽然情感是语音交流的核心维度，但目前尚缺乏对大规模音频语言模型内部如何编码情感的机制性理解。

Method: 本文对Qwen2.5-Omni、Kimi-Audio和Audio Flamingo 3三款开源模型中的情感敏感神经元进行对比分析，采用频率、熵、幅度和对比度等多种神经元选择方法，并利用推理时干预实现对情感识别的因果验证。

Result: 通过神经元干预实验，发现情感神经元具有显著的情感专属性，针对某种情感的神经元消融会特异性降低该情感的识别效果，而增强对应神经元则能引导模型偏向目标情感，且该现象具有层级聚类特征及跨数据集迁移能力。

Conclusion: 研究提供了基于神经元层面、具有因果关系的情感决策机制解释，且展示了通过针对性神经元干预来实现可控情感表达的可行路径。

Abstract: Emotion is a central dimension of spoken communication, yet, we still lack a mechanistic account of how modern large audio-language models (LALMs) encode it internally. We present the first neuron-level interpretability study of emotion-sensitive neurons (ESNs) in LALMs and provide causal evidence that such units exist in Qwen2.5-Omni, Kimi-Audio, and Audio Flamingo 3. Across these three widely used open-source models, we compare frequency-, entropy-, magnitude-, and contrast-based neuron selectors on multiple emotion recognition benchmarks. Using inference-time interventions, we reveal a consistent emotion-specific signature: ablating neurons selected for a given emotion disproportionately degrades recognition of that emotion while largely preserving other classes, whereas gain-based amplification steers predictions toward the target emotion. These effects arise with modest identification data and scale systematically with intervention strength. We further observe that ESNs exhibit non-uniform layer-wise clustering with partial cross-dataset transfer. Taken together, our results offer a causal, neuron-level account of emotion decisions in LALMs and highlight targeted neuron interventions as an actionable handle for controllable affective behaviors.

</details>


### [67] [ToxiGAN: Toxic Data Augmentation via LLM-Guided Directional Adversarial Generation](https://arxiv.org/abs/2601.03121)
*Peiran Li,Jan Fillies,Adrian Paschke*

Main category: cs.CL

TL;DR: 本文提出了ToxiGAN框架，通过对抗生成结合大型语言模型指导，实现了有控制和类别特定的有害语言数据增广，提升了毒性分类的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 有害语言数据增广用于提升毒性分类的鲁棒性，但因监督信号不足和分布不均衡，增广具有挑战性。

Method: 提出ToxiGAN，一种结合对抗生成和大语言模型语义指导的类别感知文本增广方法，采用两步方向性训练，使用LLM生成的中性文本作为语义支撑，动态选择中性示例强化类特异对比信号。

Result: 在四个仇恨言论基准数据集上，ToxiGAN在macro-F1和hate-F1指标上表现最佳，显著优于传统和基于LLM的增广方法。消融与敏感性分析证实了语义支撑和方向性训练的有效性。

Conclusion: 通过结合语义支撑和方向性训练的类别感知对抗增广，ToxiGAN有效提升了毒性分类器的鲁棒性，克服了传统GAN增广的模式崩溃和语义漂移问题。

Abstract: Augmenting toxic language data in a controllable and class-specific manner is crucial for improving robustness in toxicity classification, yet remains challenging due to limited supervision and distributional skew. We propose ToxiGAN, a class-aware text augmentation framework that combines adversarial generation with semantic guidance from large language models (LLMs). To address common issues in GAN-based augmentation such as mode collapse and semantic drift, ToxiGAN introduces a two-step directional training strategy and leverages LLM-generated neutral texts as semantic ballast. Unlike prior work that treats LLMs as static generators, our approach dynamically selects neutral exemplars to provide balanced guidance. Toxic samples are explicitly optimized to diverge from these exemplars, reinforcing class-specific contrastive signals. Experiments on four hate speech benchmarks show that ToxiGAN achieves the strongest average performance in both macro-F1 and hate-F1, consistently outperforming traditional and LLM-based augmentation methods. Ablation and sensitivity analyses further confirm the benefits of semantic ballast and directional training in enhancing classifier robustness.

</details>


### [68] [The Anatomy of Conversational Scams: A Topic-Based Red Teaming Analysis of Multi-Turn Interactions in LLMs](https://arxiv.org/abs/2601.03134)
*Xiangzhe Yuan,Zhenhao Zhang,Haoming Tang,Siying Hu*

Main category: cs.CL

TL;DR: 本论文通过构建LLM对话模拟框架，研究多轮对话中大语言模型（LLM）面临的新型诈骗风险，揭示多轮交互中存在的安全挑战及防御策略。


<details>
  <summary>Details</summary>
Motivation: 当前对大语言模型的安全评估多基于单轮对话，无法有效捕捉多轮对话中出现的新风险，尤其是多轮诈骗行为。

Method: 设计了一个受控的LLM对LLM多轮对话模拟框架，涵盖多轮诈骗场景，测试了8种先进模型的表现，分析对话结果并对攻击策略、防御反应及失败模式进行了质性标注。

Result: 发现诈骗对话呈现重复的升级模式，防御策略主要包括验证和延迟机制；多轮交互失败多因安全保护触发和角色不稳定导致。

Conclusion: 多轮交互安全是LLM行为中一个关键且独特的维度，需重点关注以提升模型的实际安全性能。

Abstract: As LLMs gain persuasive agentic capabilities through extended dialogues, they introduce novel risks in multi-turn conversational scams that single-turn safety evaluations fail to capture. We systematically study these risks using a controlled LLM-to-LLM simulation framework across multi-turn scam scenarios. Evaluating eight state-of-the-art models in English and Chinese, we analyze dialogue outcomes and qualitatively annotate attacker strategies, defensive responses, and failure modes. Results reveal that scam interactions follow recurrent escalation patterns, while defenses employ verification and delay mechanisms. Furthermore, interactional failures frequently stem from safety guardrail activation and role instability. Our findings highlight multi-turn interactional safety as a critical, distinct dimension of LLM behavior.

</details>


### [69] [Improving Indigenous Language Machine Translation with Synthetic Data and Language-Specific Preprocessing](https://arxiv.org/abs/2601.03135)
*Aashish Dhawan,Christopher Driggers-Ellis,Christan Grant,Daisy Zhe Wang*

Main category: cs.CL

TL;DR: 本文针对美洲土著语言数据稀缺问题，利用多语言翻译模型生成合成平行语料，增强神经机器翻译效果。


<details>
  <summary>Details</summary>
Motivation: 土著语言缺乏足够的平行语料，影响神经机器翻译的性能。

Method: 使用大规模多语言翻译模型生成合成句对，结合语言特定预处理（正字法归一化、噪声过滤）对mBART模型进行微调。

Result: 在瓜拉尼语-西班牙语和克丘亚语-西班牙语翻译中，合成数据增强方法显著提升了chrF++评测指标，然而对艾马拉语的实验显示，通用预处理对高度黏着性语言效果有限。

Conclusion: 合成数据增强和语言特定预处理可有效提升部分土著语言神经机器翻译性能，但对高度黏着性语言仍需更精细的处理策略。

Abstract: Low-resource indigenous languages often lack the parallel corpora required for effective neural machine translation (NMT). Synthetic data generation offers a practical strategy for mitigating this limitation in data-scarce settings. In this work, we augment curated parallel datasets for indigenous languages of the Americas with synthetic sentence pairs generated using a high-capacity multilingual translation model. We fine-tune a multilingual mBART model on curated-only and synthetically augmented data and evaluate translation quality using chrF++, the primary metric used in recent AmericasNLP shared tasks for agglutinative languages.
  We further apply language-specific preprocessing, including orthographic normalization and noise-aware filtering, to reduce corpus artifacts. Experiments on Guarani--Spanish and Quechua--Spanish translation show consistent chrF++ improvements from synthetic data augmentation, while diagnostic experiments on Aymara highlight the limitations of generic preprocessing for highly agglutinative languages.

</details>


### [70] [Limited Linguistic Diversity in Embodied AI Datasets](https://arxiv.org/abs/2601.03136)
*Selma Wanna,Agnes Luhtaru,Jonathan Salfity,Ryan Barron,Juston Moore,Cynthia Matuszek,Mitch Pryor*

Main category: cs.CL

TL;DR: 本文对多种视觉-语言-动作（VLA）数据集进行了系统性审计，分析其指令语言的词汇多样性、重复度、语义相似性和句法复杂性，发现许多数据集的指令语言存在高度重复和模板化，语言形式较为单一。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型在语言数据的多样性和结构复杂性方面的情况尚未被充分记录，影响模型的训练与评估。

Method: 通过对多个广泛使用的VLA数据集进行定量语言分析，涵盖词汇多样性、重复率、语义相似度及句法复杂度等维度。

Result: 发现许多数据集使用高度重复且结构有限的模板式指令，导致语言形式分布较窄。

Conclusion: 该研究为VLA训练和评估数据中的语言特点提供了描述性文档，有助于更合理的数据集报告、选择及扩充策略，以提升语言覆盖度。

Abstract: Language plays a critical role in Vision-Language-Action (VLA) models, yet the linguistic characteristics of the datasets used to train and evaluate these systems remain poorly documented. In this work, we present a systematic dataset audit of several widely used VLA corpora, aiming to characterize what kinds of instructions these datasets actually contain and how much linguistic variety they provide. We quantify instruction language along complementary dimensions-including lexical variety, duplication and overlap, semantic similarity, and syntactic complexity. Our analysis shows that many datasets rely on highly repetitive, template-like commands with limited structural variation, yielding a narrow distribution of instruction forms. We position these findings as descriptive documentation of the language signal available in current VLA training and evaluation data, intended to support more detailed dataset reporting, more principled dataset selection, and targeted curation or augmentation strategies that broaden language coverage.

</details>


### [71] [Self-Verification is All You Need To Pass The Japanese Bar Examination](https://arxiv.org/abs/2601.03144)
*Andrew Shin*

Main category: cs.CL

TL;DR: 本论文提出了一种自我验证模型，在忠实还原日本律师资格考试形式和评分标准的新数据集上训练，成功实现了首次在不改变题目结构和评分规则的情况下，LLM通过日本律师资格考试的成绩。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在专业结构化考试中的表现不稳定，特别是日本律师资格考试要求严格的法律推理和复杂答题格式，现有简化判断方法未被系统评估，亟需验证模型能否真正达到考试水平。

Method: 构建忠实还原真实考试格式和评分制度的新数据集，训练自我验证模型，同时与多代理推理和分解监督等方法进行了广泛比较。

Result: 模型在真实考试尺度上超过了官方及格分数，而其他复杂方法未能达到该性能。

Conclusion: 格式忠实的监督和一致性验证对提升专业推理能力至关重要，单模型设计优于复杂多模型系统，模型及数据公开以促进研究发展。

Abstract: Despite rapid advances in large language models (LLMs), achieving reliable performance on highly professional and structured examinations remains a significant challenge. The Japanese bar examination is a particularly demanding benchmark, requiring not only advanced legal reasoning but also strict adherence to complex answer formats that involve joint evaluation of multiple propositions. While recent studies have reported improvements by decomposing such questions into simpler true--false judgments, these approaches have not been systematically evaluated under the original exam format and scoring scheme, leaving open the question of whether they truly capture exam-level competence. In this paper, we present a self-verification model trained on a newly constructed dataset that faithfully replicates the authentic format and evaluation scale of the exam. Our model is able to exceed the official passing score when evaluated on the actual exam scale, marking the first demonstration, to our knowledge, of an LLM passing the Japanese bar examination without altering its original question structure or scoring rules. We further conduct extensive comparisons with alternative strategies, including multi-agent inference and decomposition-based supervision, and find that these methods fail to achieve comparable performance. Our results highlight the importance of format-faithful supervision and consistency verification, and suggest that carefully designed single-model approaches can outperform more complex systems in high-stakes professional reasoning tasks. Our dataset and codes are publicly available.

</details>


### [72] [Decoupling the Effect of Chain-of-Thought Reasoning: A Human Label Variation Perspective](https://arxiv.org/abs/2601.03154)
*Beiduo Chen,Tiancheng Hu,Caiqi Zhang,Robert Litschko,Anna Korhonen,Barbara Plank*

Main category: cs.CL

TL;DR: 本文研究了在长链式思考推理调优的大型语言模型中，其在处理存在概率模糊性的人类标签分布任务中的表现，发现长链式思考显著影响最终准确率，但分布排序主要受模型先验影响。


<details>
  <summary>Details</summary>
Motivation: 现有的长Chain-of-Thought推理调优的大型语言模型虽然在单一答案任务上表现优异，但其对人类标签变异性(即概率模糊)的建模能力仍未得到充分探索。

Method: 通过在分布式任务上系统性地使用Cross-CoT方法进行解构实验，以分离推理文本的影响和模型内在先验的影响，分析长CoT对准确率和标签分布的作用。

Result: 发现“解耦机制”：CoT内容决定了最终准确率（贡献率99%），而标签分布排序则主要由模型先验决定（超过80%贡献率）。推理过程中CoT对准确率的影响单调增加，但分布结构主要由模型内在先验主导。

Conclusion: 长链式思考作为大型语言模型的决策机制对选项排名中的头部选项决策有效，但不擅长对含糊任务中的概率分布进行细粒度的校准。

Abstract: Reasoning-tuned LLMs utilizing long Chain-of-Thought (CoT) excel at single-answer tasks, yet their ability to model Human Label Variation--which requires capturing probabilistic ambiguity rather than resolving it--remains underexplored. We investigate this through systematic disentanglement experiments on distribution-based tasks, employing Cross-CoT experiments to isolate the effect of reasoning text from intrinsic model priors. We observe a distinct "decoupled mechanism": while CoT improves distributional alignment, final accuracy is dictated by CoT content (99% variance contribution), whereas distributional ranking is governed by model priors (over 80%). Step-wise analysis further shows that while CoT's influence on accuracy grows monotonically during the reasoning process, distributional structure is largely determined by LLM's intrinsic priors. These findings suggest that long CoT serves as a decisive LLM decision-maker for the top option but fails to function as a granular distribution calibrator for ambiguous tasks.

</details>


### [73] [WebAnchor: Anchoring Agent Planning to Stabilize Long-Horizon Web Reasoning](https://arxiv.org/abs/2601.03164)
*Yu Xinmiao,Zhang Liwen,Feng Xiaocheng,Jiang Yong,Qin Bing,Xie Pengjun,Zhou Jingren*

Main category: cs.CL

TL;DR: 本文提出了一种基于强化学习的两阶段框架Anchor-GRPO，针对大语言模型在长时序计划中的“计划锚点”问题进行优化，实现了更高效的网页信息检索能力。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法无法解决长时序网页推理中首步推理对后续行为影响过大的“计划锚点”问题，导致计划执行效率低下。

Method: Anchor-GRPO将规划和执行解耦，第一阶段利用细粒度自我对弈经验奖励优化首步规划，第二阶段通过稀疏奖励使执行与首步规划保持一致，提升工具使用效率。

Result: 在BrowseComp、GAIA等四个基准测试上，Anchor-GRPO相比传统GRPO和首步GRPO显著提升任务成功率和工具利用效率，且表现随模型规模和上下文长度增加而增强。

Conclusion: Anchor-GRPO有效解决了长时序网页信息搜寻中的计划瓶颈问题，表现出良好的扩展性和实用价值。

Abstract: Large Language Model(LLM)-based agents have shown strong capabilities in web information seeking, with reinforcement learning (RL) becoming a key optimization paradigm. However, planning remains a bottleneck, as existing methods struggle with long-horizon strategies. Our analysis reveals a critical phenomenon, plan anchor, where the first reasoning step disproportionately impacts downstream behavior in long-horizon web reasoning tasks. Current RL algorithms, fail to account for this by uniformly distributing rewards across the trajectory. To address this, we propose Anchor-GRPO, a two-stage RL framework that decouples planning and execution. In Stage 1, the agent optimizes its first-step planning using fine-grained rubrics derived from self-play experiences and human calibration. In Stage 2, execution is aligned with the initial plan through sparse rewards, ensuring stable and efficient tool usage. We evaluate Anchor-GRPO on four benchmarks: BrowseComp, BrowseComp-Zh, GAIA, and XBench-DeepSearch. Across models from 3B to 30B, Anchor-GRPO outperforms baseline GRPO and First-step GRPO, improving task success and tool efficiency. Notably, WebAnchor-30B achieves 46.0% pass@1 on BrowseComp and 76.4% on GAIA. Anchor-GRPO also demonstrates strong scalability, getting higher accuracy as model size and context length increase.

</details>


### [74] [Can Embedding Similarity Predict Cross-Lingual Transfer? A Systematic Study on African Languages](https://arxiv.org/abs/2601.03168)
*Tewodros Kederalah Idris,Prasenjit Mitra,Roald Eiselen*

Main category: cs.CL

TL;DR: 本文评估了五种嵌入相似度指标在跨语言迁移中的效果，发现余弦间隙和基于检索的指标能够可靠预测迁移成功率，且提示需要针对特定模型进行分析。


<details>
  <summary>Details</summary>
Motivation: 低资源非洲语言的NLP系统亟需有效的跨语言迁移方法，但缺乏可靠的源语言选择手段。

Method: 系统评估了816次迁移实验，涵盖三种NLP任务、三种非洲多语种模型及12种语言，比较了五种嵌入相似度指标的预测效果。

Result: 余弦间隙和检索指标（P@1，CSLS）与迁移成功高度相关（ρ=0.4-0.6），而CKA相关度几乎无效（ρ≈0.1）。不同模型间指标的相关性符号逆转，需逐模型验证。嵌入指标的预测能力与语言类型学指标相当。

Conclusion: 为源语言选择提供具体指导，强调必须进行模型特定的分析以实现有效跨语言迁移。

Abstract: Cross-lingual transfer is essential for building NLP systems for low-resource African languages, but practitioners lack reliable methods for selecting source languages. We systematically evaluate five embedding similarity metrics across 816 transfer experiments spanning three NLP tasks, three African-centric multilingual models, and 12 languages from four language families. We find that cosine gap and retrieval-based metrics (P@1, CSLS) reliably predict transfer success ($ρ= 0.4-0.6$), while CKA shows negligible predictive power ($ρ\approx 0.1$). Critically, correlation signs reverse when pooling across models (Simpson's Paradox), so practitioners must validate per-model. Embedding metrics achieve comparable predictive power to URIEL linguistic typology. Our results provide concrete guidance for source language selection and highlight the importance of model-specific analysis.

</details>


### [75] [Maximizing Local Entropy Where It Matters: Prefix-Aware Localized LLM Unlearning](https://arxiv.org/abs/2601.03190)
*Naixin Zhai,Pengyang Shao,Binbin Zheng,Fei Shen,Long Bai,Xun Yang*

Main category: cs.CL

TL;DR: 本文提出了一种名为PALU的方法，用于大语言模型中的机器遗忘，通过局部极大化熵来只删除敏感前缀信息，从而有效遗忘而不损害模型整体性能。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法对所有响应token一视同仁，导致不必要的效用下降和扩展到无关内容的优化，影响模型性能。

Method: PALU框架通过局部熵最大化目标，仅抑制敏感前缀并仅对top-k logits进行平坦化，切断因果生成链并最大化关键子空间的不确定性，有效减少冗余优化。

Result: 大量实验证明PALU在遗忘敏感信息和保持模型效用方面优于当前最先进的基线方法。

Conclusion: PALU通过局部化遗忘策略，在保证遗忘效果的同时最大限度地减少对模型整体性能的影响，提供了一种有效的机器遗忘新途径。

Abstract: Machine unlearning aims to forget sensitive knowledge from Large Language Models (LLMs) while maintaining general utility. However, existing approaches typically treat all tokens in a response indiscriminately and enforce uncertainty over the entire vocabulary. This global treatment results in unnecessary utility degradation and extends optimization to content-agnostic regions. To address these limitations, we propose PALU (Prefix-Aware Localized Unlearning), a framework driven by a local entropy maximization objective across both temporal and vocabulary dimensions. PALU reveals that (i) suppressing the sensitive prefix alone is sufficient to sever the causal generation link, and (ii) flattening only the top-$k$ logits is adequate to maximize uncertainty in the critical subspace. These findings allow PALU to avoid redundant optimization across the full vocabulary and parameter space while minimizing collateral damage to general model performance. Extensive experiments validate that PALU achieves superior forgetting efficacy and utility preservation compared to state-of-the-art baselines.

</details>


### [76] [MemRL: Self-Evolving Agents via Runtime Reinforcement Learning on Episodic Memory](https://arxiv.org/abs/2601.03192)
*Shengtao Zhang,Jiaqian Wang,Ruiwen Zhou,Junwei Liao,Yuchen Feng,Weinan Zhang,Ying Wen,Zhiyu Li,Feiyu Xiong,Yutao Qi,Bo Tang,Muning Wen*

Main category: cs.CL

TL;DR: 本文提出了MemRL框架，通过非参数化的强化学习利用情景记忆实现智能体的自我进化，显著提升了大型语言模型在新任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然具备强大的推理能力，但在自我进化方面存在困难，细调计算成本高且易遗忘，现有基于记忆的方法多依赖被动的语义匹配，容易检索到噪声。

Method: MemRL框架将固定的大型语言模型推理与可塑的进化记忆分离，采用双阶段检索机制：先按语义相关性过滤候选项，再基于学习的Q值（效用）选择，Q值通过环境反馈不断优化，实现试错式强化学习。

Result: 在HLE、BigCodeBench、ALFWorld和Lifelong Agent Bench上，MemRL显著优于最先进的对比方法，证明其高效区分高价值策略与噪声的能力。

Conclusion: MemRL成功解决了稳定性与可塑性之间的矛盾，使智能体在无需更新模型权重的情况下实现持续的运行时改进。

Abstract: The hallmark of human intelligence is the ability to master new skills through Constructive Episodic Simulation-retrieving past experiences to synthesize solutions for novel tasks. While Large Language Models possess strong reasoning capabilities, they struggle to emulate this self-evolution: fine-tuning is computationally expensive and prone to catastrophic forgetting, while existing memory-based methods rely on passive semantic matching that often retrieves noise. To address these challenges, we propose MemRL, a framework that enables agents to self-evolve via non-parametric reinforcement learning on episodic memory. MemRL explicitly separates the stable reasoning of a frozen LLM from the plastic, evolving memory. Unlike traditional methods, MemRL employs a Two-Phase Retrieval mechanism that filters candidates by semantic relevance and then selects them based on learned Q-values (utility). These utilities are continuously refined via environmental feedback in an trial-and-error manner, allowing the agent to distinguish high-value strategies from similar noise. Extensive experiments on HLE, BigCodeBench, ALFWorld, and Lifelong Agent Bench demonstrate that MemRL significantly outperforms state-of-the-art baselines. Our analysis experiments confirm that MemRL effectively reconciles the stability-plasticity dilemma, enabling continuous runtime improvement without weight updates.

</details>


### [77] [X-MuTeST: A Multilingual Benchmark for Explainable Hate Speech Detection and A Novel LLM-consulted Explanation Framework](https://arxiv.org/abs/2601.03194)
*Mohammad Zia Ur Rehman,Sai Kartheek Reddy Kasu,Shashivardhan Reddy Koppula,Sai Rithwik Reddy Chirra,Shwetank Shekhar Singh,Nagendra Kumar*

Main category: cs.CL

TL;DR: 本论文提出了一种新颖的基于可解释性指导的多语言仇恨言论检测框架X-MuTeST，结合大型语言模型的语义推理和传统注意力机制，针对印地语、特鲁古语和英语提供人工标注的解释理由，提升了检测的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有社会媒体上的仇恨言论检测面临准确性和可解释性的双重挑战，尤其是针对数据资源较少的印地语和特鲁古语。

Method: 提出X-MuTeST框架，融合大型语言模型的高层语义推理与传统注意力增强技术，利用人类标注的理由指导训练，同时通过对原文本与不同n-gram预测概率差异的计算来获得解释。

Result: 结合人类理由与X-MuTeST解释方法显著提升了分类性能和模型可解释性，通过多种合理性和忠实度指标验证有效性，并在印地语、特鲁古语和英语共计一万多条数据上建立了包括词级理由标注的基准数据集。

Conclusion: 利用人类注释的理由指导训练并结合高级解释方法，能够有效提升多语言仇恨言论的检测准确度和可解释性，推动了低资源语言的相关研究。

Abstract: Hate speech detection on social media faces challenges in both accuracy and explainability, especially for underexplored Indic languages. We propose a novel explainability-guided training framework, X-MuTeST (eXplainable Multilingual haTe Speech deTection), for hate speech detection that combines high-level semantic reasoning from large language models (LLMs) with traditional attention-enhancing techniques. We extend this research to Hindi and Telugu alongside English by providing benchmark human-annotated rationales for each word to justify the assigned class label. The X-MuTeST explainability method computes the difference between the prediction probabilities of the original text and those of unigrams, bigrams, and trigrams. Final explanations are computed as the union between LLM explanations and X-MuTeST explanations. We show that leveraging human rationales during training enhances both classification performance and explainability. Moreover, combining human rationales with our explainability method to refine the model attention yields further improvements. We evaluate explainability using Plausibility metrics such as Token-F1 and IOU-F1 and Faithfulness metrics such as Comprehensiveness and Sufficiency. By focusing on under-resourced languages, our work advances hate speech detection across diverse linguistic contexts. Our dataset includes token-level rationale annotations for 6,004 Hindi, 4,492 Telugu, and 6,334 English samples. Data and code are available on https://github.com/ziarehman30/X-MuTeST

</details>


### [78] [DIP: Dynamic In-Context Planner For Diffusion Language Models](https://arxiv.org/abs/2601.03199)
*Yang Li,Han Meng,Chenan Wang,Haipeng Chen*

Main category: cs.CL

TL;DR: 该论文提出了一种名为DIP的新方法，通过动态调整扩散语言模型的上下文，显著提升推理速度同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型（DLMs）因双向注意力机制，随着上下文长度增加计算成本大幅上升，亟需有效的上下文优化方案。

Method: 基于扩散生成允许动态调整上下文的特性，提出DIP方法，在生成过程中动态选择和插入上下文示例，而非一次性输入所有示例。

Result: DIP在保持生成质量的情况下，实现了对比标准推理最高12.9倍、对比KV缓存增强推理1.17倍的推理速度提升。

Conclusion: 动态上下文优化有效缓解了DLMs的计算瓶颈，提升了模型推理效率，具备广泛应用前景。

Abstract: Diffusion language models (DLMs) have shown strong potential for general natural language tasks with in-context examples. However, due to the bidirectional attention mechanism, DLMs incur substantial computational cost as context length increases. This work addresses this issue with a key discovery: unlike the sequential generation in autoregressive language models (ARLMs), the diffusion generation paradigm in DLMs allows \textit{efficient dynamic adjustment of the context} during generation. Building on this insight, we propose \textbf{D}ynamic \textbf{I}n-Context \textbf{P}lanner (DIP), a context-optimization method that dynamically selects and inserts in-context examples during generation, rather than providing all examples in the prompt upfront. Results show DIP maintains generation quality while achieving up to 12.9$\times$ inference speedup over standard inference and 1.17$\times$ over KV cache-enhanced inference.

</details>


### [79] [UltraLogic: Enhancing LLM Reasoning through Large-Scale Data Synthesis and Bipolar Float Reward](https://arxiv.org/abs/2601.03205)
*Yile Liu,Yixian Liu,Zongwei Li,Yufei Huang,Xinhua Feng,Zhichao Hu,Jinglu Hu,Jianfeng Yan,Fengzong Lian,Yuhong Liu*

Main category: cs.CL

TL;DR: 本文提出了UltraLogic框架，通过代码化解决方法实现高质量推理数据的自动生成，并引入双极浮动奖励机制提升模型推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在处理复杂多步推理时存在瓶颈，且缺乏大规模、高质量且难度校准的通用推理数据。

Method: 提出UltraLogic，采用代码化问题核心的解决方法，构建包含数百种任务类型和分级难度的自动校准系统；引入双极浮动奖励机制以缓解奖励稀疏和非负奖励陷阱问题。

Result: 实验表明，任务多样性显著提升推理能力，双极浮动奖励结合难度匹配策略显著提高训练效率，促进模型达到全局逻辑最优解。

Conclusion: UltraLogic框架及奖励机制有效推动了多步逻辑推理的表现和训练效率，解决了复杂推理的关键瓶颈。

Abstract: While Large Language Models (LLMs) have demonstrated significant potential in natural language processing , complex general-purpose reasoning requiring multi-step logic, planning, and verification remains a critical bottleneck. Although Reinforcement Learning with Verifiable Rewards (RLVR) has succeeded in specific domains , the field lacks large-scale, high-quality, and difficulty-calibrated data for general reasoning. To address this, we propose UltraLogic, a framework that decouples the logical core of a problem from its natural language expression through a Code-based Solving methodology to automate high-quality data production. The framework comprises hundreds of unique task types and an automated calibration pipeline across ten difficulty levels. Furthermore, to mitigate binary reward sparsity and the Non-negative Reward Trap, we introduce the Bipolar Float Reward (BFR) mechanism, utilizing graded penalties to effectively distinguish perfect responses from those with logical flaws. Our experiments demonstrate that task diversity is the primary driver for reasoning enhancement , and that BFR, combined with a difficulty matching strategy, significantly improves training efficiency, guiding models toward global logical optima.

</details>


### [80] [MalruleLib: Large-Scale Executable Misconception Reasoning with Step Traces for Modeling Student Thinking in Mathematics](https://arxiv.org/abs/2601.03217)
*Xinghe Chen,Naiming Liu,Shashank Sonkar*

Main category: cs.CL

TL;DR: 本文介绍了MalruleLib，一个基于学习科学的框架，转换学生的数学误解为可执行步骤，用于学生模型推理。


<details>
  <summary>Details</summary>
Motivation: 学生在数学中常犯系统性错误，重复错误程序，传统模型难准确推断和预测学生的误解。

Method: 构建MalruleLib框架，利用67个教育资源编码101条误解规则，生成参数化题目模板和正确与错误推理路径，定义Malrule Reasoning Accuracy评估模型识别误解能力。

Result: 在九个语言模型上测试，模型准确率从直接求解的66%下降到跨模板预测的40%；MalruleLib生成超过一百万实例提升监督和评估，跨模板准确率降幅10-21%，使用学生步骤轨迹提升3-15%。

Conclusion: MalruleLib作为教育AI基础设施，能跨上下文模拟学生步骤，准确诊断误解，促进针对性反馈，提高数学教育效果。

Abstract: Student mistakes in mathematics are often systematic: a learner applies a coherent but wrong procedure and repeats it across contexts. We introduce MalruleLib, a learning-science-grounded framework that translates documented misconceptions into executable procedures, drawing on 67 learning-science and mathematics education sources, and generates step-by-step traces of malrule-consistent student work. We formalize a core student-modeling problem as Malrule Reasoning Accuracy (MRA): infer a misconception from one worked mistake and predict the student's next answer under cross-template rephrasing. Across nine language models (4B-120B), accuracy drops from 66% on direct problem solving to 40% on cross-template misconception prediction. MalruleLib encodes 101 malrules over 498 parameterized problem templates and produces paired dual-path traces for both correct reasoning and malrule-consistent student reasoning. Because malrules are executable and templates are parameterizable, MalruleLib can generate over one million instances, enabling scalable supervision and controlled evaluation. Using MalruleLib, we observe cross-template degradations of 10-21%, while providing student step traces improves prediction by 3-15%. We release MalruleLib as infrastructure for educational AI that models student procedures across contexts, enabling diagnosis and feedback that targets the underlying misconception.

</details>


### [81] [Multi-RADS Synthetic Radiology Report Dataset and Head-to-Head Benchmarking of 41 Open-Weight and Proprietary Language Models](https://arxiv.org/abs/2601.03232)
*Kartik Bose,Abhinandan Kumar,Raghuraman Soundararajan,Priya Mudgil,Samonee Ralmilay,Niharika Dutta,Manphool Singhal,Arun Kumar,Saugata Sen,Anurima Patra,Priya Ghosh,Abanti Das,Amit Gupta,Ashish Verma,Dipin Sudhakaran,Ekta Dhamija,Himangi Unde,Ishan Kumar,Krithika Rangarajan,Prerna Garg,Rachel Sequeira,Sudhin Shylendran,Taruna Yadav,Tej Pal,Pankaj Gupta*

Main category: cs.CL

TL;DR: 本文构建了一个包含10种RAD框架、1600份合成放射报告的多RADs基准数据集RXL-RADSet，并评估了41种不同规模的开源小语言模型（SLMs）与专有模型GPT-5.2在RADs自动分配任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 由于RADs标准复杂，自动化从叙述性放射报告中分配RADs分类具有挑战性，且缺乏跨框架和模型规模的基准比较。

Method: 使用大语言模型生成符合不同RADs和放射科医生风格的合成报告，经过两轮放射科医生验证后形成RXL-RADSet，随后对41个不同参数规模的SLMs和GPT-5.2模型在固定引导提示下进行有效性和准确性评估，同时比较引导提示与零样本提示的效果。

Result: GPT-5.2模型在引导提示下达到99.8%的有效性和81.1%的准确率，20-32B参数量的SLMs也表现良好，有效性接近99%，准确率中高70%左右，引导提示显著提升了模型表现，且模型性能随模型规模增加而提升，但在处理高复杂度RADs时准确率有所下降。

Conclusion: RXL-RADSet是一个多RADs标准的放射科医生验证数据集，大型SLMs在引导提示下能接近专有模型表现，但在更复杂RADs的分类任务上仍存在差距。

Abstract: Background: Reporting and Data Systems (RADS) standardize radiology risk communication but automated RADS assignment from narrative reports is challenging because of guideline complexity, output-format constraints, and limited benchmarking across RADS frameworks and model sizes. Purpose: To create RXL-RADSet, a radiologist-verified synthetic multi-RADS benchmark, and compare validity and accuracy of open-weight small language models (SLMs) with a proprietary model for RADS assignment. Materials and Methods: RXL-RADSet contains 1,600 synthetic radiology reports across 10 RADS (BI-RADS, CAD-RADS, GB-RADS, LI-RADS, Lung-RADS, NI-RADS, O-RADS, PI-RADS, TI-RADS, VI-RADS) and multiple modalities. Reports were generated by LLMs using scenario plans and simulated radiologist styles and underwent two-stage radiologist verification. We evaluated 41 quantized SLMs (12 families, 0.135-32B parameters) and GPT-5.2 under a fixed guided prompt. Primary endpoints were validity and accuracy; a secondary analysis compared guided versus zero-shot prompting. Results: Under guided prompting GPT-5.2 achieved 99.8% validity and 81.1% accuracy (1,600 predictions). Pooled SLMs (65,600 predictions) achieved 96.8% validity and 61.1% accuracy; top SLMs in the 20-32B range reached ~99% validity and mid-to-high 70% accuracy. Performance scaled with model size (inflection between <1B and >=10B) and declined with RADS complexity primarily due to classification difficulty rather than invalid outputs. Guided prompting improved validity (99.2% vs 96.7%) and accuracy (78.5% vs 69.6%) compared with zero-shot. Conclusion: RXL-RADSet provides a radiologist-verified multi-RADS benchmark; large SLMs (20-32B) can approach proprietary-model performance under guided prompting, but gaps remain for higher-complexity schemes.

</details>


### [82] [STReasoner: Empowering LLMs for Spatio-Temporal Reasoning in Time Series via Spatial-Aware Reinforcement Learning](https://arxiv.org/abs/2601.03248)
*Juntong Ni,Shiyu Wang,Ming Jin,Qi He,Wei Jin*

Main category: cs.CL

TL;DR: 该论文提出了ST-Bench基准测试和STReasoner模型，用于提升时间序列中的时空推理能力，实现更准确的决策支持。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列时空推理领域发展不足，现有方法过于注重预测准确性而忽视推理能力，难以满足高风险系统中复杂时空依赖的需求。

Method: 构建了包括病因推理、实体识别、相关性推理和上下文预测四个核心任务的ST-Bench基准，设计了基于网络SDE的多智能体数据合成管线；提出STReasoner模型结合时间序列、图结构和文本进行显式推理；引入S-GRPO强化学习算法以强化空间信息的利用。

Result: STReasoner在各项任务中平均准确率提升17%~135%，计算成本仅为部分专有模型的0.004倍，且在真实数据上具备良好泛化能力。

Conclusion: 通过引入新的基准和模型，显著提升了时间序列时空推理能力，为实际高风险场景提供了更有效的决策支持工具。

Abstract: Spatio-temporal reasoning in time series involves the explicit synthesis of temporal dynamics, spatial dependencies, and textual context. This capability is vital for high-stakes decision-making in systems such as traffic networks, power grids, and disease propagation. However, the field remains underdeveloped because most existing works prioritize predictive accuracy over reasoning. To address the gap, we introduce ST-Bench, a benchmark consisting of four core tasks, including etiological reasoning, entity identification, correlation reasoning, and in-context forecasting, developed via a network SDE-based multi-agent data synthesis pipeline. We then propose STReasoner, which empowers LLM to integrate time series, graph structure, and text for explicit reasoning. To promote spatially grounded logic, we introduce S-GRPO, a reinforcement learning algorithm that rewards performance gains specifically attributable to spatial information. Experiments show that STReasoner achieves average accuracy gains between 17% and 135% at only 0.004X the cost of proprietary models and generalizes robustly to real-world data.

</details>


### [83] [Automated Semantic Rules Detection (ASRD) for Emergent Communication Interpretation](https://arxiv.org/abs/2601.03254)
*Bastien Vanderplaetse,Xavier Siebert,Stéphane Dupont*

Main category: cs.CL

TL;DR: 本文提出了一种自动语义规则检测算法（ASRD），用于从多智能体系统中的自发通信中提取有意义的模式，提升了 emergent language 的可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前自发通信领域中，缺乏对 emergent language 可解释性的研究，尤其是在多智能体系统中智能体自发发展通信策略的背景下。

Method: 提出ASRD算法，自动从训练在Lewis游戏上两种不同数据集的智能体交换的信息中提取相关模式，并将这些模式与输入数据的具体属性关联。

Result: ASRD算法成功识别出通信中的语义模式，极大简化了对emergent communication的解释和后续分析。

Conclusion: ASRD为多智能体系统中的自发通信提供了有效的语义解释工具，促进了该领域的理解和研究进展。

Abstract: The field of emergent communication within multi-agent systems examines how autonomous agents can independently develop communication strategies, without explicit programming, and adapt them to varied environments. However, few studies have focused on the interpretability of emergent languages. The research exposed in this paper proposes an Automated Semantic Rules Detection (ASRD) algorithm, which extracts relevant patterns in messages exchanged by agents trained with two different datasets on the Lewis Game, which is often studied in the context of emergent communication. ASRD helps at the interpretation of the emergent communication by relating the extracted patterns to specific attributes of the input data, thereby considerably simplifying subsequent analysis.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [84] [Stigmergic Swarming Agents for Fast Subgraph Isomorphism](https://arxiv.org/abs/2601.02449)
*H. Van Dyke Parunak*

Main category: cs.MA

TL;DR: 本文提出了ASSIST算法，一种基于蚁群优化的近似部分子图同构算法，能在线性于查询图大小且常数于数据图大小的时间内进行子图匹配，优于现有高达O(d^2)复杂度的方法。


<details>
  <summary>Details</summary>
Motivation: 最大部分子图同构问题因NP完全，传统算法复杂度高，难以高效处理大数据图中的查询问题。

Method: 提出ASSIST算法，利用类蚁群优化的启发式方法，先通过O(q log d)找到节点匹配点，再在迭代搜索中实现时间复杂度线性于查询图且数据图规模无关。

Result: ASSIST在子图搜索中的主要步骤时间复杂度显著降低，并可扩展支持时间有序边、非精确匹配及缺失节点边的复杂匹配场景。

Conclusion: ASSIST实现了高效且灵活的最大部分子图同构近似匹配，提升了处理复杂和大规模图数据的能力，优于现有启发式算法。

Abstract: Maximum partial subgraph isomorphism compares two graphs (nodes joined by edges) to find a largest common subgraph. A common use case, for graphs with labeled nodes, seeks to find instances of a \textit{query} graph with $q$ nodes in a (typically larger) \textit{data} graph with $d$ nodes. The problem is NP-complete, and naïve solutions are exponential in $q + d$. The fastest current heuristic has complexity $O(d^2)$. This paper outlines ASSIST (Approximate Swarming Subgraph Isomorphism through Stigmergy), inspired by the ant colony optimization approach to the traveling salesperson. After peering (identifying matching individual nodes in query and data) in time $O(q\cdot log(d))$, the time required for ASSIST's iterative subgraph search, the combinatorially complex part of the problem, is linear in query size and constant in data size. ASSIST can be extended to support matching problems (such as temporally ordered edges, inexact matches, and missing nodes or edges in the data graph) that frustrate other heuristics.

</details>


### [85] [Modellierung und Simulation der Dynamik von Fussgängerströmen](https://arxiv.org/abs/2601.02526)
*Péter Molnár*

Main category: cs.MA

TL;DR: 该论文基于社会力理论建立了微观的人行流模型，用于设计行人友好型基础设施和验证社会科学理论。研究表明，简单的个体行为通过交互形成复杂的群体行为，包括路径形成和建筑几何形态对流动特性的影响。


<details>
  <summary>Details</summary>
Motivation: 一方面希望开发一个现实的人行流模型作为设计工具，另一方面通过足够数据验证社会力理论的社会科学假设。

Method: 基于社会力理论，模型包含个体向目标移动和保持与其他行人与障碍物距离的两条基本规则，结合进化算法优化建筑布局，集成决策模型描述目标选择，还引入学习和适应机制改善行人行为，最后建立路径系统自组织模型。

Result: 模型揭示了行人流空间和时间结构的复杂性、路径形成机制及受建筑几何影响的性能差异，示例展示通过减少步行区提高效率，算法有效优化建筑布局，路径系统自组织符合自然运输网络特征。

Conclusion: 该研究证明了社会力模型能有效描述和预测行人流动行为，为城市规划和设施设计提供理论基础和优化方法，也推动了社会科学理论的模型验证和应用。

Abstract: This work presents a microscopic model to describe pedestrian flows based on the social force theory. The aim of this study is twofold: (1) developing a realistic model that can be used as a tool for designing pedestrian-friendly infrastructure, and (2) verifying a social science theory using a model with sufficient data. The investigation of the pedestrian model shows that despite simple individual behavior patterns, complex spatial and temporal structures emerge through the interactions in pedestrian flows. Collective behavior emerges from individuals following two basic rules: (1) moving directly towards their goal at a certain speed, and (2) maintaining a distance to other pedestrians and obstacles. This self-organized collective behavior manifests itself as trails that are formed by pedestrians moving in one direction. Furthermore, strong dependencies of the properties of pedestrian flows on geometric forms of buildings are shown, and the influence of geometric changes on performance characteristics is investigated. An example demonstrates how efficiency can be increased by reducing walkable areas. This work also presents an evolutionary algorithm for optimizing building layouts based on the social force model. Additionally, a decision-making model is integrated to describe alternative goal selection, and adaptation and learning capabilities are included to improve pedestrian avoidance behavior and decision strategies based on accumulated experience. A method for determining load distributions in individual sections of a path system considering subjective selection criteria is also developed. Finally, a model that describes the self-organization of path systems with minimal detours is presented, similar to natural transport networks where total length and material costs are optimized.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [86] [ProSoftArena: Benchmarking Hierarchical Capabilities of Multimodal Agents in Professional Software Environments](https://arxiv.org/abs/2601.02399)
*Jiaxin Ai,Yukang Feng,Fanrui Zhang,Jianwen Sun,Zizhen Li,Chuanhao Li,Yifan Chang,Wenxiao Wu,Ruoxi Wang,Mingliang Zhai,Kaipeng Zhang*

Main category: cs.SE

TL;DR: 本文提出了用于评估多模态智能体在专业软件环境中表现的基准ProSoftArena，涵盖了436个真实任务和多学科应用。


<details>
  <summary>Details</summary>
Motivation: 现有多模态智能体评测多限于浏览器和基础桌面应用，难以反映专业软件工作流的实际需求。

Method: 建立专业软件智能体能力层级，设计包含6学科13专业应用的436任务的基准，构建真实计算机环境和执行导向评测框架，并引入人类参与评估。

Result: 即使表现最佳的智能体，二级任务成功率仅为24.4%，三级多软件工作流任务完全失败。

Conclusion: 现有智能体在专业软件环境中能力不足，需结合分析结果优化设计原则，推动构建更强大的专业软件多模态智能体。

Abstract: Multimodal agents are making rapid progress on general computer-use tasks, yet existing benchmarks remain largely confined to browsers and basic desktop applications, falling short in professional software workflows that dominate real-world scientific and industrial practice. To close this gap, we introduce ProSoftArena, a benchmark and platform specifically for evaluating multimodal agents in professional software environments. We establish the first capability hierarchy tailored to agent use of professional software and construct a benchmark of 436 realistic work and research tasks spanning 6 disciplines and 13 core professional applications. To ensure reliable and reproducible assessment, we build an executable real-computer environment with an execution-based evaluation framework and uniquely incorporate a human-in-the-loop evaluation paradigm. Extensive experiments show that even the best-performing agent attains only a 24.4\% success rate on L2 tasks and completely fails on L3 multi-software workflow. In-depth analysis further provides valuable insights for addressing current agent limitations and more effective design principles, paving the way to build more capable agents in professional software settings. This project is available at: https://prosoftarena.github.io.

</details>


### [87] [The Vibe-Check Protocol: Quantifying Cognitive Offloading in AI Programming](https://arxiv.org/abs/2601.02410)
*Aizierjiang Aiersilan*

Main category: cs.SE

TL;DR: 本文提出了一个理论框架和评估协议，旨在探讨"Vibe Coding"（利用大语言模型通过自然语言指导编码）是否能更好地促进软件工程学习。


<details>
  <summary>Details</summary>
Motivation: 当前"Vibe Coding"虽然简化了编程过程，但可能影响技能保持和深层理解，需系统评估其教育效益。

Method: 构建了Vibe-Check协议，设计了三项量化指标（技能衰退度$M_{CSR}$、错误识别能力$M_{HT}$和解释性差距$E_{gap}$），通过控制实验比较不同学习方式的效果。

Result: Vibe-Check协议能够有效量化"Vibe Coding"对学生技能掌握和理解的影响，揭示利用AI导致的潜在技术债务与表面能力的差异。

Conclusion: 本文为教育者提供了判断"Vibe Coding"何时适合教学的量化方法，促进在速度加速和认知卸载之间找到最佳教学平衡。

Abstract: The integration of Large Language Models (LLMs) into software engineering education has driven the emergence of ``Vibe Coding,'' a paradigm where developers articulate high-level intent through natural language and delegate implementation to AI agents. While proponents argue this approach modernizes pedagogy by emphasizing conceptual design over syntactic memorization, accumulating empirical evidence raises concerns regarding skill retention and deep conceptual understanding. This paper proposes a theoretical framework to investigate the research question: \textit{Is Vibe Coding a better way to learn software engineering?} We posit a divergence in student outcomes between those leveraging AI for acceleration versus those using it for cognitive offloading. To evaluate these educational trade-offs, we propose the \textbf{Vibe-Check Protocol (VCP)}, a systematic benchmarking framework incorporating three quantitative metrics: the \textit{Cold Start Refactor} ($M_{CSR}$) for modeling skill decay; \textit{Hallucination Trap Detection} ($M_{HT}$) based on signal detection theory to evaluate error identification; and the \textit{Explainability Gap} ($E_{gap}$) for quantifying the divergence between code complexity and conceptual comprehension. Through controlled comparisons, VCP aims to provide a quantitative basis for educators to determine the optimal pedagogical boundary: identifying contexts where Vibe Coding fosters genuine mastery and contexts where it introduces hidden technical debt and superficial competence.

</details>


### [88] [Talks that Builds: Exploring Communication factors for the Success of Emerging Professional in Product Teams](https://arxiv.org/abs/2601.02421)
*Nyan Lin Zaw*

Main category: cs.SE

TL;DR: 研究重点转向18-27岁年轻新兴专业团队，探讨其成功影响因素，发现好奇心、地理接近性、文档和资源获取等新因素。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注27岁以上、经验丰富的专业人士，忽视了年轻新兴专业团队的成功因素。

Method: 选取18-27岁产品团队为研究对象，分析影响团队生产力和项目成果的因素。

Result: 一些既有因素依然适用，而部分因素不再重要，新发现包括好奇心、位置接近、文档和资源获取等。

Conclusion: 研究填补了年轻专业团队成功影响因素的文献空白，揭示了新因素如何影响团队生产力和项目成果。

Abstract: This paper recognizes that most organizational communication study focuses on established professionals aged above 27 with more than five years of experience. In contrast, this study examines product teams with younger emerging professionals aged 18-27 and explores which factors influence their success. While some established factors still apply, others become less relevant, and new ones such as curiosity, locational proximity, documentation, access to resources were identified in the study. Overall, this study fills a gap in the literature on how these newer factors shape team productivity and project outcomes based on the success rate of the product the team developed.

</details>


### [89] [WebCoderBench: Benchmarking Web Application Generation with Comprehensive and Interpretable Evaluation Metrics](https://arxiv.org/abs/2601.02430)
*Chenxu Liu,Yingjie Fu,Wei Yang,Ying Zhang,Tao Xie*

Main category: cs.SE

TL;DR: 本文提出了WebCoderBench，这是首个基于真实用户需求的数据集，用于评估大语言模型生成网页应用的能力，包含1572个真实需求和24项细粒度评测指标，实现了自动化、客观且可解释的评测。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏可用于评测大语言模型生成网页应用的真实场景数据集和通用、可解释的评测指标。

Method: 构建了WebCoderBench数据集，收集真实用户需求；设计了24个细粒度指标，结合规则和LLM作为评判者的方法实现自动评测，并根据人类偏好加权生成综合评分。

Result: 在12个主流LLM和2个基于LLM的代理上进行评测，没有模型在所有指标上表现出主导优势，表明模型尚有提升空间。

Conclusion: WebCoderBench为网页应用生成领域提供了第一个全面、通用且可解释的评测基准，有助于推动LLM的针对性优化和能力提升。

Abstract: Web applications (web apps) have become a key arena for large language models (LLMs) to demonstrate their code generation capabilities and commercial potential. However, building a benchmark for LLM-generated web apps remains challenging due to the need for real-world user requirements, generalizable evaluation metrics without relying on ground-truth implementations or test cases, and interpretable evaluation results. To address these challenges, we introduce WebCoderBench, the first real-world-collected, generalizable, and interpretable benchmark for web app generation. WebCoderBench comprises 1,572 real user requirements, covering diverse modalities and expression styles that reflect realistic user intentions. WebCoderBench provides 24 fine-grained evaluation metrics across 9 perspectives, combining rule-based and LLM-as-a-judge paradigm for fully automated, objective, and general evaluation. Moreover, WebCoderBench adopts human-preference-aligned weights over metrics to yield interpretable overall scores. Experiments across 12 representative LLMs and 2 LLM-based agents show that there exists no dominant model across all evaluation metrics, offering an opportunity for LLM developers to optimize their models in a targeted manner for a more powerful version.

</details>


### [90] [Focus on What Matters: Fisher-Guided Adaptive Multimodal Fusion for Vulnerability Detection](https://arxiv.org/abs/2601.02438)
*Yun Bian,Yi Chen,HaiQuan Wang,ShiHao Li,Zhe Cui*

Main category: cs.SE

TL;DR: 本文提出了TaCCS-DFA框架，通过引入Fisher信息对多模态软件漏洞检测中的序列和图表示进行任务导向的互补融合，提高了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态方法通常简单融合序列和图表示，易产生冗余并受图表示质量波动影响，降低检测效果。

Method: 提出TaCCS-DFA，通过Fisher信息估计低秩主费舍尔子空间，限制跨模态注意力于任务敏感方向，并采用自适应门控机制动态调节图模态贡献。

Result: 在BigVul、Devign和ReVeal数据集上，TaCCS-DFA表现优异，使用CodeT5骨干时，在高度不平衡BigVul数据集上的F1分数达到87.80%，比基线提升6.3个百分点，同时保持低校准误差和计算开销。

Conclusion: TaCCS-DFA有效利用任务敏感特征进行多模态融合，显著提升了软件漏洞检测准确率和可靠性，为多模态融合提供了理论和实践的新思路。

Abstract: Software vulnerability detection is a critical task for securing software systems and can be formulated as a binary classification problem: given a code snippet, determine whether it contains a vulnerability. Existing multimodal approaches typically fuse Natural Code Sequence (NCS) representations from pretrained language models with Code Property Graph (CPG) representations from graph neural networks, often under the implicit assumption that adding a modality necessarily yields extra information. In practice, sequence and graph representations can be redundant, and fluctuations in the quality of the graph modality can dilute the discriminative signal of the dominant modality. To address this, we propose TaCCS-DFA, a framework that introduces Fisher information as a geometric measure of how sensitive feature directions are to the classification decision, enabling task-oriented complementary fusion. TaCCS-DFA online estimates a low-rank principal Fisher subspace and restricts cross-modal attention to task-sensitive directions, thereby retrieving structural features from CPG that complement the sequence modality; meanwhile, an adaptive gating mechanism dynamically adjusts the contribution of the graph modality for each sample to suppress noise propagation. Our analysis shows that, under an isotropic perturbation assumption, the proposed mechanism admits a tighter risk bound than conventional full-spectrum attention. Experiments on BigVul, Devign, and ReVeal show that TaCCS-DFA achieves strong performance across multiple backbones. With CodeT5 as the backbone, TaCCS-DFA reaches an F1 score of 87.80\% on the highly imbalanced BigVul dataset, improving over a strong baseline Vul-LMGNNs by 6.3 percentage points while maintaining low calibration error and computational overhead.

</details>


### [91] [The Rise of Agentic Testing: Multi-Agent Systems for Robust Software Quality Assurance](https://arxiv.org/abs/2601.02454)
*Saba Naqvi,Mohammad Baqar,Nawaz Ali Mohammad*

Main category: cs.SE

TL;DR: 本文提出了一种多模型测试框架，通过多智能体协作实现测试的生成、执行、分析与优化，显著提高测试质量和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 目前基于AI的测试生成器输出静态单次测试，缺乏执行反馈，导致测试无效、冗余或无法执行。

Method: 引入测试生成代理、执行分析代理和评审优化代理组成闭环自我修正系统，结合沙箱执行、失败详报及迭代再生成策略，集成到CI/CD流水线中并利用覆盖率和执行结果作为强化信号引导。

Result: 实验表明，该框架使无效测试减少60%，覆盖率提升30%，且相较单模型基线极大减轻人工工作量。

Conclusion: 多智能体反馈驱动循环机制可实现自动化、持续学习的软件测试体系，推动代码质量保障向自主修复、高可靠性方向发展。

Abstract: Software testing has progressed toward intelligent automation, yet current AI-based test generators still suffer from static, single-shot outputs that frequently produce invalid, redundant, or non-executable tests due to the lack of execution aware feedback. This paper introduces an agentic multi-model testing framework a closed-loop, self-correcting system in which a Test Generation Agent, an Execution and Analysis Agent, and a Review and Optimization Agent collaboratively generate, execute, analyze, and refine tests until convergence. By using sandboxed execution, detailed failure reporting, and iterative regeneration or patching of failing tests, the framework autonomously improves test quality and expands coverage. Integrated into a CI/CD-compatible pipeline, it leverages reinforcement signals from coverage metrics and execution outcomes to guide refinement. Empirical evaluations on microservice based applications show up to a 60% reduction in invalid tests, 30% coverage improvement, and significantly reduced human effort compared to single-model baselines demonstrating that multi-agent, feedback-driven loops can evolve software testing into an autonomous, continuously learning quality assurance ecosystem for self-healing, high-reliability codebases.

</details>


### [92] [Enhancing Debugging Skills with AI-Powered Assistance: A Real-Time Tool for Debugging Support](https://arxiv.org/abs/2601.02504)
*Elizaveta Artser,Daniil Karol,Anna Potriasaeva,Aleksei Rostovskii,Katsiaryna Dzialets,Ekaterina Koshchenko,Xiaotian Su,April Yi Wang,Anastasiia Birillo*

Main category: cs.SE

TL;DR: 本文介绍了一款集成于IDE中的AI调试助手，通过实时代码分析和断点建议，提升调试教学效率。


<details>
  <summary>Details</summary>
Motivation: 调试作为编程教育和软件开发中的重要技能，常被计算机科学课程忽视，因此需要有效的教学工具。

Method: 利用结合大语言模型(LLMs)的检索增强生成(RAG)方法、程序切片技术以及自定义启发式策略，减少调用LLM次数并提高调试建议的准确性。

Result: 通过技术分析、用户体验研究以及课堂测试三层评估，展示了该调试助手在教学中的应用潜力。

Conclusion: 该AI调试助手能够有效支持编程调试教学，提升学生调试能力，弥补现有CS课程中调试教学的不足。

Abstract: Debugging is a crucial skill in programming education and software development, yet it is often overlooked in CS curricula. To address this, we introduce an AI-powered debugging assistant integrated into an IDE. It offers real-time support by analyzing code, suggesting breakpoints, and providing contextual hints. Using RAG with LLMs, program slicing, and custom heuristics, it enhances efficiency by minimizing LLM calls and improving accuracy. A three-level evaluation - technical analysis, UX study, and classroom tests - highlights its potential for teaching debugging.

</details>


### [93] [Green LLM Techniques in Action: How Effective Are Existing Techniques for Improving the Energy Efficiency of LLM-Based Applications in Industry?](https://arxiv.org/abs/2601.02512)
*Pelin Rabia Kuran,Rumbidzai Chitakunye,Vincenzo Stoico,Ilja Heitlager,Justus Bogner*

Main category: cs.SE

TL;DR: 本研究在荷兰Schuberg Philis公司的工业聊天机器人应用中，测试了四种降低大语言模型(LLM)能耗的技术，发现提示优化和量化能显著降低能耗但影响准确性，只有小大模型协作技术在不影响性能的情况下显著节能。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型的能耗高，尤其在工业应用中，现有技术的实际有效性尚缺乏实证研究。

Method: 选取小大模型协作、提示优化、量化及批处理四种技术，在工业聊天机器人应用中进行八种变体实验，比较能耗、准确率及响应时间。

Result: 提示优化和2位量化技术能将能耗降低高达90%，但准确率显著下降。小大模型协作通过NPCC实现显著节能且性能损失不大。

Conclusion: 降低LLM应用能耗在实践中可行，但在不影响准确性与响应时间的前提下提高能效仍具挑战，研究结果为实现能效优化提供了实用见解。

Abstract: The rapid adoption of large language models (LLMs) has raised concerns about their substantial energy consumption, especially when deployed at industry scale. While several techniques have been proposed to address this, limited empirical evidence exists regarding the effectiveness of applying them to LLM-based industry applications. To fill this gap, we analyzed a chatbot application in an industrial context at Schuberg Philis, a Dutch IT services company. We then selected four techniques, namely Small and Large Model Collaboration, Prompt Optimization, Quantization, and Batching, applied them to the application in eight variations, and then conducted experiments to study their impact on energy consumption, accuracy, and response time compared to the unoptimized baseline.
  Our results show that several techniques, such as Prompt Optimization and 2-bit Quantization, managed to reduce energy use significantly, sometimes by up to 90%. However, these techniques especially impacted accuracy negatively, to a degree that is not acceptable in practice. The only technique that achieved significant and strong energy reductions without harming the other qualities substantially was Small and Large Model Collaboration via Nvidia's Prompt Task and Complexity Classifier (NPCC) with prompt complexity thresholds. This highlights that reducing the energy consumption of LLM-based applications is not difficult in practice. However, improving their energy efficiency, i.e., reducing energy use without harming other qualities, remains challenging. Our study provides practical insights to move towards this goal.

</details>


### [94] [On the Effectiveness of Proposed Techniques to Reduce Energy Consumption in RAG Systems: A Controlled Experiment](https://arxiv.org/abs/2601.02522)
*Zhinuan,Guo,Chushu Gao,Justus Bogner*

Main category: cs.SE

TL;DR: 本文通过实验证明，在检索增强生成（RAG）系统中，调整检索阈值和减小嵌入大小两项技术能显著降低能耗和延迟且不损失准确率。


<details>
  <summary>Details</summary>
Motivation: 机器学习，特别是RAG系统在满足不断增长的能量需求时，其环境可持续性引起了关注，但此前针对RAG系统减少能耗的实证研究较少。

Method: 在合作伙伴软件改进集团开发的类似生产环境的RAG系统中，进行了基于CRAG数据集的9种配置、200小时以上的控制实验，评估五种节能技术对能耗、延迟和准确率的影响。

Result: 提高相似度检索阈值、减小嵌入大小、采用向量索引和BM25S重排序均显著降低了能耗，最高达60%；部分技术如索引策略导致准确率最高下降30%；但调整检索阈值和嵌入大小这两项技术实现了能耗与延迟的显著降低且准确率无损。

Conclusion: 这是首个针对RAG系统节能设计技术的综合实证研究，为开发者和研究人员提供优化RAG系统能效的方案指导。

Abstract: The rising energy demands of machine learning (ML), e.g., implemented in popular variants like retrieval-augmented generation (RAG) systems, have raised significant concerns about their environmental sustainability. While previous research has proposed green tactics for ML-enabled systems, their empirical evaluation within RAG systems remains largely unexplored. This study presents a controlled experiment investigating five practical techniques aimed at reducing energy consumption in RAG systems. Using a production-like RAG system developed at our collaboration partner, the Software Improvement Group, we evaluated the impact of these techniques on energy consumption, latency, and accuracy.
  Through a total of 9 configurations spanning over 200 hours of trials using the CRAG dataset, we reveal that techniques such as increasing similarity retrieval thresholds, reducing embedding sizes, applying vector indexing, and using a BM25S reranker can significantly reduce energy usage, up to 60% in some cases. However, several techniques also led to unacceptable accuracy decreases, e.g., by up to 30% for the indexing strategies. Notably, finding an optimal retrieval threshold and reducing embedding size substantially reduced energy consumption and latency with no loss in accuracy, making these two techniques truly energy-efficient. We present the first comprehensive, empirical study on energy-efficient design techniques for RAG systems, providing guidance for developers and researchers aiming to build sustainable RAG applications.

</details>


### [95] [PerspectiveCoach: Exploring LLMs for Developer Reflection](https://arxiv.org/abs/2601.02559)
*Lauren Olson,Emitzá Guzmán,Florian Kunneman*

Main category: cs.SE

TL;DR: 本文提出了PerspectiveCoach，一种基于大型语言模型的对话工具，帮助开发者通过视角转换练习促进对边缘用户的伦理反思。经过18名前端开发者的控制实验，结果显示该工具提升了自我意识、拓展了视角，并增强了伦理表达的细腻度。


<details>
  <summary>Details</summary>
Motivation: 当前软件开发中缺乏结构化工具，帮助开发者深入理解边缘用户的真实体验及伦理挑战。

Method: 设计并实现PerspectiveCoach工具，让开发者在真实性别骚扰案例中进行结构化视角转换练习，结合定性分析和文本相似性分析评价效果。

Result: 参与者的伦理思考更自觉，视角更宽广，伦理表达更细腻，工具易用且相关性高。与人际对话相比，工具的陈述准确性起点较低，但多次尝试后表现提升明显。

Conclusion: PerspectiveCoach展示了利用LLM支持开发者批判性伦理自省的可行性，为构建包容且社会响应性强的软件提供了实证依据和设计方向。

Abstract: Despite growing awareness of ethical challenges in software development, practitioners still lack structured tools that help them critically engage with the lived experiences of marginalized users. This paper presents PerspectiveCoach, a large language model (LLM)-powered conversational tool designed to guide developers through structured perspective-taking exercises and deepen critical reflection on how software design decisions affect marginalized communities. Through a controlled study with 18 front-end developers (balanced by sex), who interacted with the tool using a real case of online gender-based harassment, we examine how PerspectiveCoach supports ethical reasoning and engagement with user perspectives. Qualitative analysis revealed increased self-awareness, broadened perspectives, and more nuanced ethical articulation, while a complementary human-human study contextualized these findings. Text similarity analyses demonstrated that participants in the human-PerspectiveCoach study improved the fidelity of their restatements over multiple attempts, capturing both surface-level and semantic aspects of user concerns. However, human-PerspectiveCoach's restatements had a lower baseline than the human-human conversations, highlighting contextual differences in impersonal and interpersonal perspective-taking. Across the study, participants rated the tool highly for usability and relevance. This work contributes an exploratory design for LLM-powered end-user perspective-taking that supports critical, ethical self-reflection and offers empirical insights (i.e., enhancing adaptivity, centering plurality) into how such tools can help practitioners build more inclusive and socially responsive technologies.

</details>


### [96] [Compressed code: the hidden effects of quantization and distillation on programming tokens](https://arxiv.org/abs/2601.02563)
*Viacheslav Siniaev,Iaroslav Chelombitko,Aleksey Komissarov*

Main category: cs.SE

TL;DR: 本文系统分析了大语言模型（LLMs）中编程语言的token表示及模型压缩对代码生成性能的影响，提出了冷启动概率分析方法，评估了多种模型优化技术对token级表现和代码生成质量的影响。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在代码生成方面表现优秀，但其token级别的机制理解不足，尤其是在模型压缩场景下，缺乏对token表示及优化影响的深入研究。

Method: 通过分析编程语言token表示的词汇分布和关键词覆盖，提出冷启动概率分析方法，无需显式提示即可洞察模型行为；同时评估量化、蒸馏、模型扩展和任务微调等优化技术对token表现和代码生成的影响。

Result: 实验结果表明，不同优化技术对token级表示和代码生成质量存在显著影响，冷启动概率分析有效揭示模型行为；并给出在各种优化约束下保持代码生成质量的实证指导。

Conclusion: 研究深化了对大语言模型代码生成token级机制的理论理解，提出的方法和发现为压缩和优化模型在实际生产环境中的代码生成提供了有价值的参考和指导。

Abstract: Large Language Models (LLMs) have demonstrated exceptional code generation capabilities, yet their token-level mechanisms remain underexplored, particularly in compressed models. Through systematic analysis of programming language token representations, we characterize how programming languages are encoded in LLM tokenizers by analyzing their vocabulary distribution and keyword coverage patterns. We introduce a novel cold-start probability analysis method that provides insights into model behavior without requiring explicit prompts. Additionally, we present a comprehensive evaluation of how different model optimization techniques - including quantization, distillation, model scaling, and task-specific fine-tuning - affect token-level representations and code generation quality. Our experiments, supported by comprehensive probability distribution analysis and evaluation metrics, reveal critical insights into token-level behavior and provide empirically-validated guidelines for maintaining code generation quality under various optimization constraints. These findings advance both theoretical understanding of LLM code generation and practical implementation of optimized models in production environments.

</details>


### [97] [State of the Quantum Software Engineering Ecosystem](https://arxiv.org/abs/2601.02601)
*Nazanin Siavash,Armin Moin*

Main category: cs.SE

TL;DR: 本文利用AI大模型分析量子软件工程生态，识别学术和工业界的活跃机构及创业成功案例。


<details>
  <summary>Details</summary>
Motivation: 量子软件工程作为新兴领域，了解其生态现状、活跃机构和成功经验对推动行业发展至关重要。

Method: 采用最新的人工智能技术，特别是OpenAI GPT-5模型，通过ChatGPT工具分析相关文献和融资数据，识别在量子软件工程领域表现突出的机构和企业。

Result: 成功识别出在量子软件工程领域具有较高活跃度和显著成就的学术机构和企业，揭示了当前研发和创业的主要参与者。

Conclusion: 基于AI辅助分析的方法有效揭示了量子软件工程生态的现状和成功创业案例，有助于未来研究和行业决策。

Abstract: We study the current state of the Quantum Software Engineering (QSE) ecosystem, focusing on the achievements, activities, and engagements from academia and industry, with a special focus on successful entrepreneurial endeavors in this arena. Our research methodology is a novel one, featuring the state-of-the-art in Artificial Intelligence (AI), namely Large Language Models (LLMs), especially Generative Pretrained Transformers (GPT). We use one of such models, namely the OpenAI GPT-5 model, through the ChatGPT tool. The goal is to identify institutions and companies that are highly active and have achieved distinguished results in QSE, evidenced by peer-reviewed publications or raised capital in the venture capital market.

</details>


### [98] [TAAF: A Trace Abstraction and Analysis Framework Synergizing Knowledge Graphs and LLMs](https://arxiv.org/abs/2601.02632)
*Alireza Ezaz,Ghazal Khodabandeh,Majid Babaei,Naser Ezzati-Jivan*

Main category: cs.SE

TL;DR: 本文提出了TAAF，一种结合时间索引、知识图谱和大语言模型的新框架，用于将复杂软件系统的执行跟踪数据转化为可操作的见解。


<details>
  <summary>Details</summary>
Motivation: 现有的执行跟踪分析工具依赖预定义分析，定制分析需花费大量时间且易出错，难以高效处理庞大复杂的操作系统内核和大型应用程序跟踪数据。

Method: TAAF构建时间索引的知识图谱以捕捉线程、CPU和系统资源间的关系，通过大语言模型解释查询相关子图，支持自然语言问答，减少人工检查和系统专业知识需求。

Result: 通过TraceQA-100基准测试，TAAF在多种大语言模型和时间设定下提升了最高31.2%的回答准确率，尤其在多跳和因果推理任务中表现突出。

Conclusion: 基于图的推理显著提升跟踪数据分析效果，但仍存在局限，为下一代跟踪分析工具的发展奠定了基础。

Abstract: Execution traces are a critical source of information for understanding, debugging, and optimizing complex software systems. However, traces from OS kernels or large-scale applications like Chrome or MySQL are massive and difficult to analyze. Existing tools rely on predefined analyses, and custom insights often require writing domain-specific scripts, which is an error-prone and time-consuming task. This paper introduces TAAF (Trace Abstraction and Analysis Framework), a novel approach that combines time-indexing, knowledge graphs (KGs), and large language models (LLMs) to transform raw trace data into actionable insights. TAAF constructs a time-indexed KG from trace events to capture relationships among entities such as threads, CPUs, and system resources. An LLM then interprets query-specific subgraphs to answer natural-language questions, reducing the need for manual inspection and deep system expertise. To evaluate TAAF, we introduce TraceQA-100, a benchmark of 100 questions grounded in real kernel traces. Experiments across three LLMs and multiple temporal settings show that TAAF improves answer accuracy by up to 31.2%, particularly in multi-hop and causal reasoning tasks. We further analyze where graph-grounded reasoning helps and where limitations remain, offering a foundation for next-generation trace analysis tools.

</details>


### [99] [Enterprise Identity Integration for AI-Assisted Developer Services: Architecture, Implementation, and Case Study](https://arxiv.org/abs/2601.02698)
*Manideep Reddy Chinthareddy*

Main category: cs.SE

TL;DR: 本文提出了一种将OAuth 2.0和OpenID Connect集成到基于模型上下文协议（MCP）的AI辅助开发环境中的实用架构，确保工具符合企业身份和访问控制需求。


<details>
  <summary>Details</summary>
Motivation: 当前AI辅助开发工具嵌入现代IDE中，但MCP规范仅提供了最小授权模型，缺乏企业单点登录（SSO）集成指导，企业需确保工具符合身份管理和访问控制要求。

Method: 设计一种结合OAuth 2.0和OIDC的架构，使IDE扩展获取和呈现令牌，MCP服务器通过身份提供者验证令牌，并通过作用域和声明实现最小权限访问。实现原型涵盖Visual Studio Code、基于Python的MCP服务器及OIDC兼容的身份提供者。

Result: 原型演示了可行性，通过案例研究评估了认证延迟、令牌验证开销、运营考虑及AI特定风险。

Conclusion: 该方法为组织部署AI辅助开发工具提供了可实施的模式，同时保证了身份认证和审计能力。

Abstract: AI-assisted developer services are increasingly embedded in modern IDEs, yet enterprises must ensure these tools operate within existing identity, access control, and governance requirements. The Model Context Protocol (MCP) enables AI assistants to retrieve structured internal context, but its specification provides only a minimal authorization model and lacks guidance on integrating enterprise SSO. This article presents a practical architecture that incorporates OAuth 2.0 and OpenID Connect (OIDC) into MCP-enabled developer environments. It describes how IDE extensions obtain and present tokens, how MCP servers validate them through an identity provider, and how scopes and claims can enforce least-privilege access. A prototype implementation using Visual Studio Code, a Python-based MCP server, and an OIDC-compliant IdP demonstrates feasibility. A case study evaluates authentication latency, token-validation overhead, operational considerations, and AI-specific risks. The approach provides a deployable pattern for organizations adopting AI-assisted developer tools while maintaining identity assurance and auditability.

</details>


### [100] [Agentic Memory Enhanced Recursive Reasoning for Root Cause Localization in Microservices](https://arxiv.org/abs/2601.02732)
*Lingzhe Zhang,Tong Jia,Yunpeng Zhai,Leyi Pan,Chiming Duan,Minghua He,Mengxi Jia,Ying Li*

Main category: cs.SE

TL;DR: 随着微服务系统的复杂性增加，根因定位变得更加重要。本文提出AMER-RCL框架，通过递归推理与记忆增强，有效提升故障根因定位的准确率和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预定义模式，难以适应动态环境，且存在浅层、以症状为中心的推理不足和重复推理问题，导致准确性和效率下降。

Method: 借鉴SRE的根因分析特点，设计AMER-RCL，包括递归推理引擎和Agentic Memory，递归细化候选原因并复用历史推理，减少冗余和延迟。

Result: 实验表明，AMER-RCL在根因定位准确率和推理效率上均优于当前先进方法。

Conclusion: AMER-RCL有效结合递归、多维扩展及跨模态推理，提升了微服务系统故障根因定位的可靠性和性能。

Abstract: As contemporary microservice systems become increasingly popular and complex-often comprising hundreds or even thousands of fine-grained, interdependent subsystems-they are experiencing more frequent failures. Ensuring system reliability thus demands accurate root cause localization. While many traditional graph-based and deep learning approaches have been explored for this task, they often rely heavily on pre-defined schemas that struggle to adapt to evolving operational contexts. Consequently, a number of LLM-based methods have recently been proposed. However, these methods still face two major limitations: shallow, symptom-centric reasoning that undermines accuracy, and a lack of cross-alert reuse that leads to redundant reasoning and high latency. In this paper, we conduct a comprehensive study of how Site Reliability Engineers (SREs) localize the root causes of failures, drawing insights from professionals across multiple organizations. Our investigation reveals that expert root cause analysis exhibits three key characteristics: recursiveness, multi-dimensional expansion, and cross-modal reasoning. Motivated by these findings, we introduce AMER-RCL, an agentic memory enhanced recursive reasoning framework for root cause localization in microservices. AMER-RCL employs the Recursive Reasoning RCL engine, a multi-agent framework that performs recursive reasoning on each alert to progressively refine candidate causes, while Agentic Memory incrementally accumulates and reuses reasoning from prior alerts within a time window to reduce redundant exploration and lower inference latency. Experimental results demonstrate that AMER-RCL consistently outperforms state-of-the-art methods in both localization accuracy and inference efficiency.

</details>


### [101] [Hypothesize-Then-Verify: Speculative Root Cause Analysis for Microservices with Pathwise Parallelism](https://arxiv.org/abs/2601.02736)
*Lingzhe Zhang,Tong Jia,Yunpeng Zhai,Leyi Pan,Chiming Duan,Minghua He,Pei Xiao,Ying Li*

Main category: cs.SE

TL;DR: 本文提出了一种基于假设验证的微服务异常根因分析框架SpecRCA，提升了准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 微服务系统复杂且动态，导致异常频发，传统LLM方法存在探索多样性不足和推理速度慢的问题，亟需更高效准确的根因分析方法。

Method: SpecRCA采用“假设-验证”范式，先通过假设起草模块快速生成候选根因，再利用并行根因验证器高效校验这些假设。

Result: 在AIOps 2022数据集上，SpecRCA在准确率和效率上均优于现有方法。

Conclusion: SpecRCA为复杂微服务环境下的根因分析提供了一种可扩展且易解释的实用方案，表现出良好的应用潜力。

Abstract: Microservice systems have become the backbone of cloud-native enterprise applications due to their resource elasticity, loosely coupled architecture, and lightweight deployment. Yet, the intrinsic complexity and dynamic runtime interactions of such systems inevitably give rise to anomalies. Ensuring system reliability therefore hinges on effective root cause analysis (RCA), which entails not only localizing the source of anomalies but also characterizing the underlying failures in a timely and interpretable manner. Recent advances in intelligent RCA techniques, particularly those powered by large language models (LLMs), have demonstrated promising capabilities, as LLMs reduce reliance on handcrafted features while offering cross-platform adaptability, task generalization, and flexibility. However, existing LLM-based methods still suffer from two critical limitations: (a) limited exploration diversity, which undermines accuracy, and (b) heavy dependence on large-scale LLMs, which results in slow inference. To overcome these challenges, we propose SpecRCA, a speculative root cause analysis framework for microservices that adopts a \textit{hypothesize-then-verify} paradigm. SpecRCA first leverages a hypothesis drafting module to rapidly generate candidate root causes, and then employs a parallel root cause verifier to efficiently validate them. Preliminary experiments on the AIOps 2022 dataset demonstrate that SpecRCA achieves superior accuracy and efficiency compared to existing approaches, highlighting its potential as a practical solution for scalable and interpretable RCA in complex microservice environments.

</details>


### [102] [CodeMEM: AST-Guided Adaptive Memory for Repository-Level Iterative Code Generation](https://arxiv.org/abs/2601.02868)
*Peiding Wang,Li Zhang,Fang Liu,Chongyang Tao,Yinghao Zhu*

Main category: cs.SE

TL;DR: 本文提出了CodeMEM，一种基于AST引导的动态内存管理系统，提升了仓库级代码生成中的互动协作效率和记忆管理能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在仓库级代码生成中需要持续更新和维护上下文，但自然语言为中心的记忆管理方法存在局限，容易遗忘和重复错误。

Method: 设计CodeMEM系统，包括代码上下文内存和代码会话内存模块，利用AST引导LLM操作动态维护上下文，分析会话历史检测并减轻遗忘。

Result: 在CodeIF-Bench和CoderEval两个基准测试上，CodeMEM指令执行准确率提升了12.2%（当前回合）和11.5%（会话级别），减少了2-3轮交互，同时保持了推理延迟和token效率。

Conclusion: CodeMEM通过AST辅助的内存管理显著提升了仓库级代码生成任务中大模型的性能和交互效率，展示了其在实际开发环境中的应用潜力。

Abstract: Large language models (LLMs) substantially enhance developer productivity in repository-level code generation through interactive collaboration. However, as interactions progress, repository context must be continuously preserved and updated to integrate newly validated information. Meanwhile, the expanding session history increases cognitive burden, often leading to forgetting and the reintroduction of previously resolved errors. Existing memory management approaches show promise but remain limited by natural language-centric representations. To overcome these limitations, we propose CodeMEM, an AST-guided dynamic memory management system tailored for repository-level iterative code generation. Specifically, CodeMEM introduces the Code Context Memory component that dynamically maintains and updates repository context through AST-guided LLM operations, along with the Code Session Memory that constructs a code-centric representation of interaction history and explicitly detects and mitigates forgetting through AST-based analysis. Experimental results on the instruction-following benchmark CodeIF-Bench and the code generation benchmark CoderEval demonstrate that CodeMEM achieves state-of-the-art performance, improving instruction following by 12.2% for the current turn and 11.5% for the session level, and reducing interaction rounds by 2-3, while maintaining competitive inference latency and token efficiency.

</details>


### [103] [Few-shot learning for security bug report identification](https://arxiv.org/abs/2601.02971)
*Muhammad Laiq*

Main category: cs.SE

TL;DR: 本文提出了一种基于少样本学习的安全漏洞报告识别方法，利用SetFit框架在有限标注数据下有效分类安全漏洞报告，性能优于传统机器学习方法。


<details>
  <summary>Details</summary>
Motivation: 安全漏洞报告识别依赖大量标注数据，但实际中安全漏洞报告数据稀缺，导致传统机器学习方法效果不佳，亟需有效利用少量标注数据的技术。

Method: 本文采用SetFit少样本学习框架，结合句子变换器、对比学习和参数高效微调，在小规模标注的漏洞报告数据集上训练模型，实现安全与非安全漏洞报告分类。

Result: 该方法在多个数据集上均取得最高0.865的AUC值，性能优于传统机器学习基线，显示出良好的安全漏洞报告识别能力。

Conclusion: 基于SetFit的少样本学习为安全漏洞报告识别提供了一种有效的替代方案，减少标注成本，适合数据稀缺的实际场景。

Abstract: Security bug reports require prompt identification to minimize the window of vulnerability in software systems. Traditional machine learning (ML) techniques for classifying bug reports to identify security bug reports rely heavily on large amounts of labeled data. However, datasets for security bug reports are often scarce in practice, leading to poor model performance and limited applicability in real-world settings. In this study, we propose a few-shot learning-based technique to effectively identify security bug reports using limited labeled data. We employ SetFit, a state-of-the-art few-shot learning framework that combines sentence transformers with contrastive learning and parameter-efficient fine-tuning. The model is trained on a small labeled dataset of bug reports and is evaluated on its ability to classify these reports as either security-related or non-security-related. Our approach achieves an AUC of 0.865, at best, outperforming traditional ML techniques (baselines) for all of the evaluated datasets. This highlights the potential of SetFit to effectively identify security bug reports. SetFit-based few-shot learning offers a promising alternative to traditional ML techniques to identify security bug reports. The approach enables efficient model development with minimal annotation effort, making it highly suitable for scenarios where labeled data is scarce.

</details>


### [104] [A Dataset of Low-Rated Applications from the Amazon Appstore for User Feedback Analysis](https://arxiv.org/abs/2601.03009)
*Nek Dil Khan,Javed Ali Khan,Darvesh Khan,Jianqiang Li,Mumrez Khan,Shah Fahad Khan*

Main category: cs.SE

TL;DR: 本文构建了一个包含64个低评分应用的用户评论数据集，重点分析用户反馈中最常见的问题类别，并公开了一个带有6000条人工标注的子集，用于机器学习自动分类，以促进软件质量提升。


<details>
  <summary>Details</summary>
Motivation: 当前大多数研究关注高评分应用，而低评分应用的用户反馈往往被忽视，然而这些反馈能揭示改善软件质量的重要信息。

Method: 收集亚马逊应用商店中64个低评分应用的7.9万余条用户评论，并对其中6000条评论进行人工标注，划分为六大问题类别：UI/UX、功能、兼容性、性能、客服及安全隐私。

Result: 生成了一个详尽的、多维度分类的低评分应用用户反馈数据集，同时公开了带标注和未标注数据，方便研究者进行机器学习模型的训练与软件问题分析。

Conclusion: 该数据集为自动分析低评分应用的用户反馈提供了重要资源，有助于发现和解决常见问题，推动软件质量改进，同时也促进了软件演进相关研究。

Abstract: In todays digital landscape, end-user feedback plays a crucial role in the evolution of software applications, particularly in addressing issues that hinder user experience. While much research has focused on high-rated applications, low-rated applications often remain unexplored, despite their potential to reveal valuable insights. This study introduces a novel dataset curated from 64 low-rated applications sourced from the Amazon Software Appstore (ASA), containing 79,821 user reviews. The dataset is designed to capture the most frequent issues identified by users, which are critical for improving software quality. To further enhance the dataset utility, a subset of 6000 reviews was manually annotated to classify them into six district issue categories: user interface (UI) and user experience (UX), functionality and features, compatibility and device specificity, performance and stability, customer support and responsiveness, and security and privacy issues. This annotated dataset is a valuable resource for developing machine learning-based approaches aiming to automate the classification of user feedback into various issue types. Making both the annotated and raw datasets publicly available provides researchers and developers with a crucial tool to understand common issues in low-rated apps and inform software improvements. The comprehensive analysis and availability of this dataset lay the groundwork for data-derived solutions to improve software quality based on user feedback. Additionally, the dataset can provide opportunities for software vendors and researchers to explore various software evolution-related activities, including frequently missing features, sarcasm, and associated emotions, which will help better understand the reasons for comparatively low app ratings.

</details>


### [105] [NavAI: A Generalizable LLM Framework for Navigation Tasks in Virtual Reality Environments](https://arxiv.org/abs/2601.03251)
*Xue Qin,Matthew DiGiovanni*

Main category: cs.SE

TL;DR: 本文提出了NavAI，一个基于大语言模型的导航框架，支持多种沉浸式虚拟现实应用中的基本操作与复杂任务。


<details>
  <summary>Details</summary>
Motivation: 现有导航技术主要针对360度图像数据集和三维模拟器，难以直接应用于沉浸式虚拟现实环境。

Method: 设计并实现了NavAI框架，利用通用大语言模型支持多种导航任务，在三个不同的VR环境中进行了目标导向和探索性任务评估。

Result: NavAI在目标导向任务中取得了89%的成功率，显示出较高的导航准确性。

Conclusion: 尽管NavAI表现良好，但完全依赖大语言模型存在局限，尤其是在动态目标评估场景中。实验中发现的不足为未来研究提供了方向。

Abstract: Navigation is one of the fundamental tasks for automated exploration in Virtual Reality (VR). Existing technologies primarily focus on path optimization in 360-degree image datasets and 3D simulators, which cannot be directly applied to immersive VR environments. To address this gap, we present NavAI, a generalizable large language model (LLM)-based navigation framework that supports both basic actions and complex goal-directed tasks across diverse VR applications. We evaluate NavAI in three distinct VR environments through goal-oriented and exploratory tasks. Results show that it achieves high accuracy, with an 89% success rate in goal-oriented tasks. Our analysis also highlights current limitations of relying entirely on LLMs, particularly in scenarios that require dynamic goal assessment. Finally, we discuss the limitations observed during the experiments and offer insights for future research directions.

</details>
