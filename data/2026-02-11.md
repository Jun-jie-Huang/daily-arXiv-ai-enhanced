<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 63]
- [cs.SE](#cs.SE) [Total: 17]
- [cs.MA](#cs.MA) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Overview of PAN 2026: Voight-Kampff Generative AI Detection, Text Watermarking, Multi-Author Writing Style Analysis, Generative Plagiarism Detection, and Reasoning Trajectory Detection](https://arxiv.org/abs/2602.09147)
*Janek Bevendorff,Maik Fröbe,André Greiner-Petter,Andreas Jakoby,Maximilian Mayerl,Preslav Nakov,Henry Plutz,Martin Potthast,Benno Stein,Minh Ngoc Ta,Yuxia Wang,Eva Zangerle*

Main category: cs.CL

TL;DR: 2026年PAN工作坊举办五个计算风格学和文本取证任务，通过Docker容器和TIRA平台实现客观可复现评测，促进该领域技术进步。


<details>
  <summary>Details</summary>
Motivation: 提升文本风格分析与取证技术的客观性和可复现性，应对混合作者、多作者、生成式文本等复杂场景的挑战。

Method: 采用Docker容器提交软件，通过TIRA实验平台进行易于复现的评测。

Result: 成功举办五个持续和新设任务，累计收到超过1100份软件提交，有效推动领域技术发展。

Conclusion: PAN工作坊通过组织五个任务推动计算风格学和文本取证的发展，涵盖生成式AI检测、文本水印、多作者风格分析、生成抄袭检测和推理轨迹检测，促进了客观且可复现的评估。

Abstract: The goal of the PAN workshop is to advance computational stylometry and text forensics via objective and reproducible evaluation. In 2026, we run the following five tasks: (1) Voight-Kampff Generative AI Detection, particularly in mixed and obfuscated authorship scenarios, (2) Text Watermarking, a new task that aims to find new and benchmark the robustness of existing text watermarking schemes, (3) Multi-author Writing Style Analysis, a continued task that aims to find positions of authorship change, (4) Generative Plagiarism Detection, a continued task that targets source retrieval and text alignment between generated text and source documents, and (5) Reasoning Trajectory Detection, a new task that deals with source detection and safety detection of LLM-generated or human-written reasoning trajectories. As in previous years, PAN invites software submissions as easy-to-reproduce Docker containers for most of the tasks. Since PAN 2012, more than 1,100 submissions have been made this way via the TIRA experimentation platform.

</details>


### [2] [Measuring Inclusion in Interaction: Inclusion Analytics for Human-AI Collaborative Learning](https://arxiv.org/abs/2602.09269)
*Jaeyoon Choi,Nia Nixon*

Main category: cs.CL

TL;DR: 本文提出了一个基于话语分析的包容性框架，通过三个维度动态刻画协作问题解决中的包容性，结合模拟和实证数据验证其有效性，为人机协作学习环境中的包容性测量提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 现有人工智能与教育领域中包容性评估依赖粗略样本或事后自报告，忽视了包容性如何在协作问题解决的每一刻被动态塑造。

Method: 提出了包容性分析框架，将包容性分为参与公平、情感氛围和认知公平三个维度，利用可扩展的交互层级测量手段，通过模拟对话和人机协作实验数据验证该框架的有效性。

Result: 展示了该框架能够揭示传统汇总或事后评估无法察觉的参与模式、关系动态和观点采纳，开辟了测量人机协作学习环境中包容性的新途径。

Conclusion: 本文初步建立了一个基于话语分析的包容性分析框架，能够动态揭示协作问题解决中包容性的具体互动过程，填补了传统评估方法的不足。

Abstract: Inclusion, equity, and access are widely valued in AI and education, yet are often assessed through coarse sample descriptors or post-hoc self-reports that miss how inclusion is shaped moment by moment in collaborative problem solving (CPS). In this proof-of-concept paper, we introduce inclusion analytics, a discourse-based framework for examining inclusion as a dynamic, interactional process in CPS. We conceptualize inclusion along three complementary dimensions -- participation equity, affective climate, and epistemic equity -- and demonstrate how these constructs can be made analytically visible using scalable, interaction-level measures. Using both simulated conversations and empirical data from human-AI teaming experiments, we illustrate how inclusion analytics can surface patterns of participation, relational dynamics, and idea uptake that remain invisible to aggregate or post-hoc evaluations. This work represents an initial step toward process-oriented approaches to measuring inclusion in human-AI collaborative learning environments.

</details>


### [3] [Effective Reasoning Chains Reduce Intrinsic Dimensionality](https://arxiv.org/abs/2602.09276)
*Archiki Prasad,Mandar Joshi,Kenton Lee,Mohit Bansal,Peter Shaw*

Main category: cs.CL

TL;DR: 本文提出内在维度作为量化推理策略效果的新指标，揭示有效推理策略通过减少内在维度提升了模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管链式思维推理提高了语言模型在复杂任务上的表现，但不同策略如何促进泛化能力的机制尚不清楚，需要找到一个量化且一致的指标来解释这一现象。

Method: 通过保持模型架构不变，改变任务的推理策略，测量不同策略下任务的内在维度，并在GSM8K数据集上使用Gemma-3模型进行验证，分析内在维度与泛化性能的关系。

Result: 实验证明有效的推理策略能够降低任务的内在维度，且内在维度与模型在分布内和分布外数据上的泛化性能呈强负相关关系。

Conclusion: 本文发现有效的推理策略通过降低任务的内在维度提升了模型的泛化性能，证明内在维度是分析推理过程的有效量化指标。

Abstract: Chain-of-thought (CoT) reasoning and its variants have substantially improved the performance of language models on complex reasoning tasks, yet the precise mechanisms by which different strategies facilitate generalization remain poorly understood. While current explanations often point to increased test-time computation or structural guidance, establishing a consistent, quantifiable link between these factors and generalization remains challenging. In this work, we identify intrinsic dimensionality as a quantitative measure for characterizing the effectiveness of reasoning chains. Intrinsic dimensionality quantifies the minimum number of model dimensions needed to reach a given accuracy threshold on a given task. By keeping the model architecture fixed and varying the task formulation through different reasoning strategies, we demonstrate that effective reasoning strategies consistently reduce the intrinsic dimensionality of the task. Validating this on GSM8K with Gemma-3 1B and 4B, we observe a strong inverse correlation between the intrinsic dimensionality of a reasoning strategy and its generalization performance on both in-distribution and out-of-distribution data. Our findings suggest that effective reasoning chains facilitate learning by better compressing the task using fewer parameters, offering a new quantitative metric for analyzing reasoning processes.

</details>


### [4] [Don't Shoot The Breeze: Topic Continuity Model Using Nonlinear Naive Bayes With Attention](https://arxiv.org/abs/2602.09312)
*Shu-Ting Pi,Pradeep Bagavan,Yejia Li,Disha,Qun Liu*

Main category: cs.CL

TL;DR: 本文提出了基于朴素贝叶斯与注意力机制的主题连续性模型，能高效且准确评估对话回复的主题一致性，提升了LLM应用的用户体验和资源利用。


<details>
  <summary>Details</summary>
Motivation: 在多样化业务场景中，LLM聊天机器人常面临主题突变问题，导致用户体验差和资源利用低效，需设计模型保证主题连续性。

Method: 论文基于朴素贝叶斯方法将自然语言理解模型量化，并引入注意力机制和对数非线性函数，构建了一个可解释的分析公式模型，支持任意长度对话并具备线性时间复杂度。

Result: 所提出模型在处理复杂且冗长的对话时表现优异，提升了主题连续性识别能力，同时保持了模型解释性和计算效率。

Conclusion: 该论文提出的主题连续性模型能够有效判断回复是否与初始对话主题一致，显著提升了复杂长对话中的主题识别准确率，超过了传统方法。

Abstract: Utilizing Large Language Models (LLM) as chatbots in diverse business scenarios often presents the challenge of maintaining topic continuity. Abrupt shifts in topics can lead to poor user experiences and inefficient utilization of computational resources. In this paper, we present a topic continuity model aimed at assessing whether a response aligns with the initial conversation topic. Our model is built upon the expansion of the corresponding natural language understanding (NLU) model into quantifiable terms using a Naive Bayes approach. Subsequently, we have introduced an attention mechanism and logarithmic nonlinearity to enhance its capability to capture topic continuity. This approach allows us to convert the NLU model into an interpretable analytical formula. In contrast to many NLU models constrained by token limits, our proposed model can seamlessly handle conversations of any length with linear time complexity. Furthermore, the attention mechanism significantly improves the model's ability to identify topic continuity in complex conversations. According to our experiments, our model consistently outperforms traditional methods, particularly in handling lengthy and intricate conversations. This unique capability offers us an opportunity to ensure the responsible and interpretable use of LLMs.

</details>


### [5] [Beyond Uniform Credit: Causal Credit Assignment for Policy Optimization](https://arxiv.org/abs/2602.09331)
*Mykola Khandoga,Rui Yuan,Vinay Kumar Sankarapu*

Main category: cs.CL

TL;DR: 该论文提出一种反事实重要性加权策略，使语言模型在推理任务中重点更新关键计算步骤，提升了性能和训练效率，且无需额外辅助资源。


<details>
  <summary>Details</summary>
Motivation: 现有策略梯度方法对所有生成令牌赋予均匀权重，导致无关填充词与关键计算步骤受到相同梯度更新，影响模型学习效率和推理能力。

Method: 提出反事实重要性加权，通过屏蔽推理跨度并测量答案概率的下降，动态调整各个生成令牌的梯度权重，无需辅助模型或外部注释，直接利用策略模型自身的概率变化进行估算。

Result: 在GSM8K数据集上，包含Qwen和Llama系列的三种模型均表现出优于均匀基线的性能提升和更快速的收敛。反向调整重要性信号会降低性能，验证了方法捕捉到真实的因果结构。

Conclusion: 通过引入反事实重要性加权方法，在语言模型的策略梯度优化中优先更新关键计算步骤，显著提升了模型在推理任务上的表现和收敛速度。

Abstract: Policy gradient methods for language model reasoning, such as GRPO and DAPO, assign uniform credit to all generated tokens - the filler phrase "Let me think" receives the same gradient update as the critical calculation "23 + 45 = 68." We propose counterfactual importance weighting: mask reasoning spans, measure the drop in answer probability, and upweight tokens accordingly during policy gradient updates. Our method requires no auxiliary models or external annotation, instead importance is estimated directly from the policy model's own probability shifts. Experiments on GSM8K across three models spanning the Qwen and Llama families demonstrate consistent improvements over uniform baselines and faster convergence to equivalent accuracy. Inverting the importance signal hurts performance, confirming we capture genuine causal structure rather than noise. Analysis shows the method correctly prioritizes calculation steps over scaffolding text. We view these findings as establishing counterfactual importance weighting as a foundation for further research rather than a complete solution.

</details>


### [6] [FM SO.P: A Progressive Task Mixture Framework with Automatic Evaluation for Cross-Domain SOP Understanding](https://arxiv.org/abs/2602.09336)
*Siyuan Huang,Ziyu Wang,Chao Pan,Han Zhao*

Main category: cs.CL

TL;DR: FM SO.P通过渐进式任务训练和自动多代理评估，提升了语言模型对标准操作程序的理解和跨领域表现，实现高效且参数经济的结果。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型难以精准理解SOP中的术语、顺序和约束推理，且在跨领域通用性方面表现不足。

Method: 引入渐进式任务混合，分阶段训练三个任务（术语概念消歧、动作序列理解和场景感知图推理），并设计自动多代理评估系统，实现领域自适应评测。

Result: 在涵盖七个领域的SOPBench基准测试中，FM SO.P的32B模型通过率达48.3%，开放源代码7B模型以更少参数实现与大模型相当性能。

Conclusion: FM SO.P通过分阶段构建能力和自动多代理评估系统，有效提升了语言模型对标准操作程序的理解和跨领域泛化能力，显著提高了通过率。

Abstract: Standard Operating Procedures (SOPs) are critical for enterprise operations, yet existing language models struggle with SOP understanding and cross-domain generalization. Current methods fail because joint training cannot differentiate between reasoning capabilities that SOP requires: terminology precision, sequential ordering, and constraint reasoning. We propose FM SO.P, solving these challenges through two novelties. First, we introduce progressive task mixtures that build capabilities by stages across three task types with cumulative data: concept disambiguation for terminology precision, action sequence understanding for procedural correctness, and scenario-aware graph reasoning for conditional logic. Second, we propose an automatic multi-agent evaluation system consisting of three agents that adaptively generate rubrics, stratified test sets, and rubric scoring, adapting to domains (e.g., temporal constraints for DMV, regulatory compliance for banking). Evaluated on SOPBench across seven domains (Bank, DMV, Healthcare, Market, University, Library, Hotel), FM SO.P achieves 48.3\% pass rate with our 32B model and 34.3\% with our opensource 7B model, matching Qwen-2.5-72B-Instruct baseline (34.4\%) with 10x fewer parameters.

</details>


### [7] [Understanding Risk and Dependency in AI Chatbot Use from User Discourse](https://arxiv.org/abs/2602.09339)
*Jianfeng Zhu,Karin G. Coifman,Ruoming Jin*

Main category: cs.CL

TL;DR: 该研究通过分析Reddit用户关于AI伤害和困扰的帖子，揭示了生成式AI引发的心理风险及情绪特征，强调自我调控困难和恐惧的核心角色，为AI安全和治理提供实证基础。


<details>
  <summary>Details</summary>
Motivation: 当前对生成式人工智能嵌入日常生活后的心理风险体验及其调控机制缺乏实证理解，亟需基于真实用户语料揭示AI使用带来的心理风险。

Method: 通过大规模计算主题分析，采用多代理大语言模型辅助，结合Braun和Clarke的反思性框架，对2023至2025年间Reddit上两个关注AI相关伤害和困扰社区的帖子进行主题提取，同时使用基于BERT的情感分类器对情绪模式进行标注和可视化。

Result: 识别出14个反复出现的主题类别，整合为五个高级体验维度，发现自我调节是最普遍的问题，恐惧情绪集中在自主权和技术风险相关领域，为AI安全领域提供了基于真实用户体验的实证支持。

Conclusion: 本研究揭示了用户对生成式人工智能相关心理风险的五个核心体验维度，自我调节困难最为突出，恐惧主要集中于自主性、控制和技术风险。

Abstract: Generative AI systems are increasingly embedded in everyday life, yet empirical understanding of how psychological risk associated with AI use emerges, is experienced, and is regulated by users remains limited. We present a large-scale computational thematic analysis of posts collected between 2023 and 2025 from two Reddit communities, r/AIDangers and r/ChatbotAddiction, explicitly focused on AI-related harm and distress. Using a multi-agent, LLM-assisted thematic analysis grounded in Braun and Clarke's reflexive framework, we identify 14 recurring thematic categories and synthesize them into five higher-order experiential dimensions. To further characterize affective patterns, we apply emotion labeling using a BERT-based classifier and visualize emotional profiles across dimensions. Our findings reveal five empirically derived experiential dimensions of AI-related psychological risk grounded in real-world user discourse, with self-regulation difficulties emerging as the most prevalent and fear concentrated in concerns related to autonomy, control, and technical risk. These results provide early empirical evidence from lived user experience of how AI safety is perceived and emotionally experienced outside laboratory or speculative contexts, offering a foundation for future AI safety research, evaluation, and responsible governance.

</details>


### [8] [Digital Linguistic Bias in Spanish: Evidence from Lexical Variation in LLMs](https://arxiv.org/abs/2602.09346)
*Yoshifumi Kawasaki*

Main category: cs.CL

TL;DR: 本文评估大型语言模型在识别西班牙语地理词汇变体的能力，发现模型表现存在地区差异，突显数字语言偏见的问题。


<details>
  <summary>Details</summary>
Motivation: 西班牙语存在显著的地区变异，研究大型语言模型在捕捉这些地域词汇差异方面的能力，有助于理解模型的语言变体知识及数字语言偏见。

Method: 将大型语言模型作为虚拟信息源，采用两种问卷调查式问题（是非题和多项选择题）进行测试，利用一个由专家精心策划的大规模西班牙语词汇变体数据库，覆盖21个讲西班牙语国家的900多个词汇项目。

Result: 模型对特定区域词汇变体的识别准确度存在明显差异，且这种差异不完全由国家数字资源量决定，说明除了数据量外还有其他因素影响模型的方言表现。

Conclusion: 大型语言模型在识别西班牙语的地理词汇变体上表现出系统性差异，某些区域的变体（如西班牙、赤道几内亚、墨西哥及中美洲、拉普拉塔河流域）识别较准确，而智利变体识别较差。

Abstract: This study examines the extent to which Large Language Models (LLMs) capture geographic lexical variation in Spanish, a language that exhibits substantial regional variation. Treating LLMs as virtual informants, we probe their dialectal knowledge using two survey-style question formats: Yes-No questions and multiple-choice questions. To this end, we exploited a large-scale, expert-curated database of Spanish lexical variation. Our evaluation covers more than 900 lexical items across 21 Spanish-speaking countries and is conducted at both the country and dialectal area levels. Across both evaluation formats, the results reveal systematic differences in how LLMs represent Spanish language varieties. Lexical variation associated with Spain, Equatorial Guinea, Mexico & Central America, and the La Plata River is recognized more accurately by the models, while the Chilean variety proves particularly difficult for the models to distinguish. Importantly, differences in the volume of country-level digital resources do not account for these performance patterns, suggesting that factors beyond data quantity shape dialectal representation in LLMs. By providing a fine-grained, large-scale evaluation of geographic lexical variation, this work advances empirical understanding of dialectal knowledge in LLMs and contributes new evidence to discussions of Digital Linguistic Bias in Spanish.

</details>


### [9] [Unsupervised Cross-Lingual Part-of-Speech Tagging with Monolingual Corpora Only](https://arxiv.org/abs/2602.09366)
*Jianyu Zheng*

Main category: cs.CL

TL;DR: 本论文提出基于无监督神经机器翻译构建伪并行语料的无监督跨语言词性标注框架，并引入多源投射技术，在多语言对上取得了优异表现，有效解决了低资源语言缺乏并行语料的瓶颈。


<details>
  <summary>Details</summary>
Motivation: 低资源语言词性标注缺乏标注数据，且传统基于并行语料的词性标签投射方法受限于并行语料不可得的问题。

Method: 利用无监督神经机器翻译系统将高资源语言句子翻译为低资源语言，构建伪并行句对，再基于词对齐进行词性标签投射。提出多源投射技术校准目标侧标签，提升标注效果。

Result: 在28个语言对上验证方法，性能与基于并行语料的跨语言词性标注器相当，部分目标语言表现更优；多源投射技术平均提升1.3%。

Conclusion: 提出的全无监督跨语言词性标注框架有效解决了低资源语言缺乏并行语料的问题，通过UNMT构建伪并行语料并结合多源投射技术，显著提升了词性标注性能。

Abstract: Due to the scarcity of part-of-speech annotated data, existing studies on low-resource languages typically adopt unsupervised approaches for POS tagging. Among these, POS tag projection with word alignment method transfers POS tags from a high-resource source language to a low-resource target language based on parallel corpora, making it particularly suitable for low-resource language settings. However, this approach relies heavily on parallel corpora, which are often unavailable for many low-resource languages. To overcome this limitation, we propose a fully unsupervised cross-lingual part-of-speech(POS) tagging framework that relies solely on monolingual corpora by leveraging unsupervised neural machine translation(UNMT) system. This UNMT system first translates sentences from a high-resource language into a low-resource one, thereby constructing pseudo-parallel sentence pairs. Then, we train a POS tagger for the target language following the standard projection procedure based on word alignments. Moreover, we propose a multi-source projection technique to calibrate the projected POS tags on the target side, enhancing to train a more effective POS tagger. We evaluate our framework on 28 language pairs, covering four source languages (English, German, Spanish and French) and seven target languages (Afrikaans, Basque, Finnis, Indonesian, Lithuanian, Portuguese and Turkish). Experimental results show that our method can achieve performance comparable to the baseline cross-lingual POS tagger with parallel sentence pairs, and even exceeds it for certain target languages. Furthermore, our proposed multi-source projection technique further boosts performance, yielding an average improvement of 1.3% over previous methods.

</details>


### [10] [AgentSkiller: Scaling Generalist Agent Intelligence through Semantically Integrated Cross-Domain Data Synthesis](https://arxiv.org/abs/2602.09372)
*Zexu Sun,Bokai Ji,Hengyi Cai,Shuaiqiang Wang,Lei Wang,Guangxia Li,Xu Chen*

Main category: cs.CL

TL;DR: 提出AgentSkiller框架自动生成丰富多样的多轮交互数据，通过结构化设计保证环境一致性，显著提升大语言模型函数调用能力。


<details>
  <summary>Details</summary>
Motivation: 现有数据采集方法受隐私限制或缺乏多样性，难以获得足够高质量且具长远交互的数据，从而限制了通用智能的提升。

Method: 该方法设计了基于DAG的架构确保状态确定性与可恢复性，构建领域本体及以用户为中心的实体图，通过服务蓝图定义工具接口，采用跨域融合机制模拟复杂任务，利用Persona模拟器进行自动化任务生成与验证。

Result: 通过合成约1.1万条多轮交互样本，训练模型在函数调用任务上相比基线方法表现有显著提升，验证了方法的有效性。

Conclusion: AgentSkiller通过合成多轮、多领域的交互数据，有效提升了大语言模型在调用函数等任务上的性能，尤其在大规模模型中表现显著。

Abstract: Large Language Model agents demonstrate potential in solving real-world problems via tools, yet generalist intelligence is bottlenecked by scarce high-quality, long-horizon data. Existing methods collect privacy-constrained API logs or generate scripted interactions lacking diversity, which struggle to produce data requisite for scaling capabilities. We propose AgentSkiller, a fully automated framework synthesizing multi-turn interaction data across realistic, semantically linked domains. It employs a DAG-based architecture with explicit state transitions to ensure determinism and recoverability. The pipeline builds a domain ontology and Person-Centric Entity Graph, defines tool interfaces via Service Blueprints for Model Context Protocol servers, and populates environments with consistent databases and strict Domain Policies. A cross-domain fusion mechanism links services to simulate complex tasks. Finally, the pipeline creates user tasks by verifying solution paths, filtering via execution-based validation, and generating queries using a Persona-based Simulator for automated rollout. This produces reliable environments with clear state changes. To demonstrate effectiveness, we synthesized $\approx$ 11K interaction samples; experimental results indicate that models trained on this dataset achieve significant improvements on function calling over baselines, particularly in larger parameter regimes.

</details>


### [11] [AfriNLLB: Efficient Translation Models for African Languages](https://arxiv.org/abs/2602.09373)
*Yasmin Moslem,Aman Kassahun Wassie,Amanuel Gizachew Abebe*

Main category: cs.CL

TL;DR: 本文提出了轻量级AfriNLLB翻译模型，通过模型压缩与知识蒸馏，实现非洲语言快速高效的机器翻译，并开源相关模型与数据。


<details>
  <summary>Details</summary>
Motivation: 为解决非洲语言机器翻译在资源受限设备上的高效部署难题，开发轻量级高效的翻译模型。

Method: 基于NLLB-200 600M模型，通过迭代层剪枝和量化压缩模型，再利用知识蒸馏和并行语料进行微调。

Result: AfriNLLB支持15个语言对（30个方向），在性能与速度上优于基线，提供两种模型版本及所有微调数据。

Conclusion: AfriNLLB模型在保持与基线模型相近的翻译性能的同时，大幅提升了翻译速度，适合资源受限环境部署。

Abstract: In this work, we present AfriNLLB, a series of lightweight models for efficient translation from and into African languages. AfriNLLB supports 15 language pairs (30 translation directions), including Swahili, Hausa, Yoruba, Amharic, Somali, Zulu, Lingala, Afrikaans, Wolof, and Egyptian Arabic, as well as other African Union official languages such as Arabic (MSA), French, Portuguese, and Spanish. Our training data covers bidirectional translation between English and 13 languages, and between French and two languages (Lingala and Wolof).
  AfriNLLB models are based on NLLB-200 600M, which we compress using iterative layer pruning and quantization. We fine-tune the pruned models on parallel corpora we curated for African languages, employing knowledge distillation from a larger teacher model. Our work aims at enabling efficient deployment of translation models for African languages in resource-constrained settings.
  Our evaluation results demonstrate that AfriNLLB models achieve performance comparable to the baseline while being significantly faster. We release two versions of the AfriNLLB models, a Transformers version that allows further fine-tuning and a CTranslate2 version for efficient inference. Moreover, we release all the training data that we used for fine-tuning the baseline and pruned models to facilitate further research.

</details>


### [12] [BiasScope: Towards Automated Detection of Bias in LLM-as-a-Judge Evaluation](https://arxiv.org/abs/2602.09383)
*Peng Lai,Zhihao Ou,Yong Wang,Longyue Wang,Jian Yang,Yun Chen,Guanhua Chen*

Main category: cs.CL

TL;DR: 本文提出BiasScope框架，自动发现LLM评估中的未知偏见，并基于此构建挑战性评测JudgeBench-Pro，揭示了当前LLM评估缺乏鲁棒性，需加强偏见缓解。


<details>
  <summary>Details</summary>
Motivation: 现有LLM作为评判者的研究多关注已知偏见，缺乏对潜在未知偏见的自动化系统探索，导致评估结果的鲁棒性和可靠性亟待提升。

Method: 提出了BiasScope，一个基于大语言模型驱动的自动化框架，系统地且大规模地发现模型评估中的潜在偏见。基于BiasScope，构建了更具挑战性的评估基准JudgeBench-Pro，用于检验评估的鲁棒性。

Result: BiasScope在JudgeBench数据集上验证了其广泛适用性和有效性，将偏见发现从手动依赖转变为自动化探索。JudgeBench-Pro对当前评估尖端LLM显示高达50%以上的错误率，凸显了现有评估方法的不足。

Conclusion: 本文提出的BiasScope框架能够有效自动发现LLM评估过程中潜在的未知偏见，显著提升了评估的鲁棒性和可靠性。评估基准JudgeBench-Pro揭示了当前强大LLM评估者存在超过50%的错误率，强调了提升评估可靠性的紧迫性。

Abstract: LLM-as-a-Judge has been widely adopted across various research and practical applications, yet the robustness and reliability of its evaluation remain a critical issue. A core challenge it faces is bias, which has primarily been studied in terms of known biases and their impact on evaluation outcomes, while automated and systematic exploration of potential unknown biases is still lacking. Nevertheless, such exploration is crucial for enhancing the robustness and reliability of evaluations. To bridge this gap, we propose BiasScope, a LLM-driven framework for automatically and at scale discovering potential biases that may arise during model evaluation. BiasScope can uncover potential biases across different model families and scales, with its generality and effectiveness validated on the JudgeBench dataset. It overcomes the limitations of existing approaches, transforming bias discovery from a passive process relying on manual effort and predefined bias lists into an active and comprehensive automated exploration. Moreover, based on BiasScope, we propose JudgeBench-Pro, an extended version of JudgeBench and a more challenging benchmark for evaluating the robustness of LLM-as-a-judge. Strikingly, even powerful LLMs as evaluators show error rates above 50\% on JudgeBench-Pro, underscoring the urgent need to strengthen evaluation robustness and to mitigate potential biases further.

</details>


### [13] [Contractual Deepfakes: Can Large Language Models Generate Contracts?](https://arxiv.org/abs/2602.09384)
*Eliza Mik*

Main category: cs.CL

TL;DR: LLMs虽然能生成看似合理的合同文本，但缺乏理解和推理能力，难以胜任法律合同起草，不能取代法律专业人员。


<details>
  <summary>Details</summary>
Motivation: 当前社会存在认知误区，认为LLMs能够有效辅助或替代法律合同的起草工作，作者旨在澄清此类过于乐观的观点，强调法律工作对深度理解和推理的需求。

Method: 通过分析LLMs生成文本的机制及其与法律合同起草任务的差异，比较了预测词汇与基于具体交易情境进行法律推理的不同，揭示了LLMs在合同起草中的局限性。

Result: 发现LLMs生成的合同文件往往表面合理但内容可能不一致，使用于具体交易时可能导致合同无效或不适用，从而挑战了LLMs能够威胁法律行业生存的观点。

Conclusion: 大型语言模型（LLMs）虽然在文本生成方面能力强大，但其不具备对词义的理解、上下文感知及推理能力，因此生成的合同文本可能缺乏实用性和法律适用性，不足以替代法律行业的专业工作。

Abstract: Notwithstanding their unprecedented ability to generate text, LLMs do not understand the meaning of words, have no sense of context and cannot reason. Their output constitutes an approximation of statistically dominant word patterns. And yet, the drafting of contracts is often presented as a typical legal task that could be facilitated by this technology. This paper seeks to put an end to such unreasonable ideas. Predicting words differs from using language in the circumstances of specific transactions and reconstituting common contractual phrases differs from reasoning about the law. LLMs seem to be able to generate generic and superficially plausible contractual documents. In the cold light of day, such documents may turn out to be useless assemblages of inconsistent provisions or contracts that are enforceable but unsuitable for a given transaction. This paper casts a shadow on the simplistic assumption that LLMs threaten the continued viability of the legal industry.

</details>


### [14] [Effective vocabulary expanding of multilingual language models for extremely low-resource languages](https://arxiv.org/abs/2602.09388)
*Jianyu Zheng*

Main category: cs.CL

TL;DR: 该论文提出一种通过扩展词汇表并利用双语词典初始化新词汇表示的方法，增强多语言预训练模型对低资源语言的支持，提升了相关任务性能且不损害源语言表现。


<details>
  <summary>Details</summary>
Motivation: 现有多语言预训练模型在支持先前不包含的低资源语言方面存在不足，亟需有效扩展模型对这些语言的覆盖能力。

Method: 利用目标语言语料扩展模型词汇表，筛选并替换原有词汇表中对源语言偏重的词汇，采用双语词典初始化新词汇表示，然后基于目标语言语料对模型进行继续预训练。

Result: 该方法在词性标注和命名实体识别任务中，分别较随机初始化法提升0.54%和2.60%的性能，且训练语料选择具有高鲁棒性，源语言表现保持稳定。

Conclusion: 通过扩展多语言预训练模型的词汇表并利用双语词典初始化新词汇的表示，可以有效提升模型对低资源语言的支持能力，同时保持对源语言的性能不减。

Abstract: Multilingual pre-trained language models(mPLMs) offer significant benefits for many low-resource languages. To further expand the range of languages these models can support, many works focus on continued pre-training of these models. However, few works address how to extend mPLMs to low-resource languages that were previously unsupported. To tackle this issue, we expand the model's vocabulary using a target language corpus. We then screen out a subset from the model's original vocabulary, which is biased towards representing the source language(e.g. English), and utilize bilingual dictionaries to initialize the representations of the expanded vocabulary. Subsequently, we continue to pre-train the mPLMs using the target language corpus, based on the representations of these expanded vocabulary. Experimental results show that our proposed method outperforms the baseline, which uses randomly initialized expanded vocabulary for continued pre-training, in POS tagging and NER tasks, achieving improvements by 0.54% and 2.60%, respectively. Furthermore, our method demonstrates high robustness in selecting the training corpora, and the models' performance on the source language does not degrade after continued pre-training.

</details>


### [15] [Are Language Models Sensitive to Morally Irrelevant Distractors?](https://arxiv.org/abs/2602.09416)
*Andrew Shaw,Christina Hahn,Catherine Rasgaitis,Yash Mishra,Alisa Liu,Natasha Jaques,Yulia Tsvetkov,Amy X. Zhang*

Main category: cs.CL

TL;DR: 本文研究发现大型语言模型的道德判断易受无关情境因素影响，提出需要更复杂的情境化道德模型来提升其道德表现的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有道德基准假设LLMs具有稳定的道德偏好，但人类道德判断受环境因素影响，故探究LLMs是否表现出类似的认知偏差。

Method: 构建包含60个情感偏向但道德无关的多模态道德干扰项数据集，将其注入现有道德基准测试中，分析对LLMs道德判断的影响。

Result: 发现道德干扰项在低模糊度情境下仍能使LLMs的道德判断发生超过30%的变化，表明需要更细致的情境化道德评估。

Conclusion: 大型语言模型(LLMs)的道德判断受到无关情境因素的显著影响，这表明其道德偏好并非稳定。

Abstract: With the rapid development and uptake of large language models (LLMs) across high-stakes settings, it is increasingly important to ensure that LLMs behave in ways that align with human values. Existing moral benchmarks prompt LLMs with value statements, moral scenarios, or psychological questionnaires, with the implicit underlying assumption that LLMs report somewhat stable moral preferences. However, moral psychology research has shown that human moral judgements are sensitive to morally irrelevant situational factors, such as smelling cinnamon rolls or the level of ambient noise, thereby challenging moral theories that assume the stability of human moral judgements. Here, we draw inspiration from this "situationist" view of moral psychology to evaluate whether LLMs exhibit similar cognitive moral biases to humans. We curate a novel multimodal dataset of 60 "moral distractors" from existing psychological datasets of emotionally-valenced images and narratives which have no moral relevance to the situation presented. After injecting these distractors into existing moral benchmarks to measure their effects on LLM responses, we find that moral distractors can shift the moral judgements of LLMs by over 30% even in low-ambiguity scenarios, highlighting the need for more contextual moral evaluations and more nuanced cognitive moral modeling of LLMs.

</details>


### [16] [Breaking the Pre-Sampling Barrier: Activation-Informed Difficulty-Aware Self-Consistency](https://arxiv.org/abs/2602.09438)
*Taewoong Yoon,Geunyeong Jeong,Geon Park,Sihyeong Yeom,Harksoo Kim*

Main category: cs.CL

TL;DR: 通过神经网络激活信号动态调整采样数量，ACTSC显著降低了自一致性策略的推理成本，同时保障了准确性。


<details>
  <summary>Details</summary>
Motivation: 提高自一致性策略的推理性能，降低其高昂的推理成本。

Method: 提出基于神经元激活的轻量级困难度估计探针，通过内部激活信号动态调整采样数量，无需额外模型调用或预采样。

Result: ACTSC在五个基准测试中有效降低了推理成本，同时保持了与现有方法相当的准确率。

Conclusion: 利用神经元激活信号进行困难度估计的自适应自一致性策略有效解决了传统方法推理代价高的问题，提升了推理效率。

Abstract: Self-Consistency (SC) is an effective decoding strategy that improves the reasoning performance of Large Language Models (LLMs) by generating multiple chain-of-thought reasoning paths and selecting the final answer via majority voting. However, it suffers from substantial inference costs because it requires a large number of samples. To mitigate this issue, Difficulty-Adaptive Self-Consistency (DSC) was proposed to reduce unnecessary token usage for easy problems by adjusting the number of samples according to problem difficulty. However, DSC requires additional model calls and pre-sampling to estimate difficulty, and this process is repeated when applying to each dataset, leading to significant computational overhead. In this work, we propose Activation-Informed Difficulty-Aware Self-Consistency (ACTSC) to address these limitations. ACTSC leverages internal difficulty signals reflected in the feed-forward network neuron activations to construct a lightweight difficulty estimation probe, without any additional token generation or model calls. The probe dynamically adjusts the number of samples for SC and can be applied to new datasets without requiring pre-sampling for difficulty estimation. To validate its effectiveness, we conduct experiments on five benchmarks. Experimental results show that ACTSC effectively reduces inference costs while maintaining accuracy relative to existing methods.

</details>


### [17] [Evaluating Social Bias in RAG Systems: When External Context Helps and Reasoning Hurts](https://arxiv.org/abs/2602.09442)
*Shweta Parihar,Lu Cheng*

Main category: cs.CL

TL;DR: 研究发现RAG架构可缓解LLM中的社会偏见，但将链式思维提示加入后偏见反增，提示应开发偏见感知的推理方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在显著的社会偏见，且即使使用RAG架构（结合外部知识源）也面临类似挑战，需评估和理解RAG对社会偏见的影响。

Method: 通过在多种检索语料库、不同LLM及超过13种偏见类型的数据集上进行广泛实验，评估RAG的偏见表现；随后结合CoT提示，分析模型的推理过程及其对偏见的影响。

Result: RAG架构能减少模型的刻板印象偏见，通过多样化上下文环境改善公平性；但融合CoT提示后，模型虽准确性提升，却整体偏见增加，偏见倾向在刻板印象与反刻板印象之间波动。

Conclusion: 引入外部知识源的RAG架构在一定程度上减轻了大型语言模型中的社会偏见，但结合链式思维提示（CoT）虽然提升了准确率，却可能加剧偏见，表明需要开发关注偏见的推理框架。

Abstract: Social biases inherent in large language models (LLMs) raise significant fairness concerns. Retrieval-Augmented Generation (RAG) architectures, which retrieve external knowledge sources to enhance the generative capabilities of LLMs, remain susceptible to the same bias-related challenges. This work focuses on evaluating and understanding the social bias implications of RAG. Through extensive experiments across various retrieval corpora, LLMs, and bias evaluation datasets, encompassing more than 13 different bias types, we surprisingly observe a reduction in bias in RAG. This suggests that the inclusion of external context can help counteract stereotype-driven predictions, potentially improving fairness by diversifying the contextual grounding of the model's outputs. To better understand this phenomenon, we then explore the model's reasoning process by integrating Chain-of-Thought (CoT) prompting into RAG while assessing the faithfulness of the model's CoT. Our experiments reveal that the model's bias inclinations shift between stereotype and anti-stereotype responses as more contextual information is incorporated from the retrieved documents. Interestingly, we find that while CoT enhances accuracy, contrary to the bias reduction observed with RAG, it increases overall bias across datasets, highlighting the need for bias-aware reasoning frameworks that can mitigate this trade-off.

</details>


### [18] [Conceptual Cultural Index: A Metric for Cultural Specificity via Relative Generality](https://arxiv.org/abs/2602.09444)
*Takumi Ohashi,Hitoshi Iyatomi*

Main category: cs.CL

TL;DR: 本文提出了概念文化指数（CCI）用于句子级文化特异性评估，验证表明其在区分文化相关句子方面优于直接使用大型语言模型评分。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在多文化环境中应用广泛，但在句子层面系统性评估文化特异性尚未充分研究。

Method: 提出了概念文化指数（CCI），通过计算目标文化内的普遍性估计与其他文化平均普遍性估计的差值来量化句子的文化特异性。

Result: CCI在400个句子（200个文化特定句子和200个一般句子）上验证，表现出预期的评分分布，且在二分类区分上相比直接LLM评分提升超过10个点的AUC。

Conclusion: CCI能够有效区分文化特定句子与一般句子，在文化特异性评估方面表现优异。

Abstract: Large language models (LLMs) are increasingly deployed in multicultural settings; however, systematic evaluation of cultural specificity at the sentence level remains underexplored. We propose the Conceptual Cultural Index (CCI), which estimates cultural specificity at the sentence level. CCI is defined as the difference between the generality estimate within the target culture and the average generality estimate across other cultures. This formulation enables users to operationally control the scope of culture via comparison settings and provides interpretability, since the score derives from the underlying generality estimates. We validate CCI on 400 sentences (200 culture-specific and 200 general), and the resulting score distribution exhibits the anticipated pattern: higher for culture-specific sentences and lower for general ones. For binary separability, CCI outperforms direct LLM scoring, yielding more than a 10-point improvement in AUC for models specialized to the target culture. Our code is available at https://github.com/IyatomiLab/CCI .

</details>


### [19] [NOWJ @BioCreative IX ToxHabits: An Ensemble Deep Learning Approach for Detecting Substance Use and Contextual Information in Clinical Texts](https://arxiv.org/abs/2602.09469)
*Huu-Huy-Hoang Tran,Gia-Bao Duong,Quoc-Viet-Anh Tran,Thi-Hai-Yen Vuong,Hoang-Quynh Le*

Main category: cs.CL

TL;DR: 本文针对西班牙语临床文本中的有毒物质使用信息识别，提出了一个结合BETO和CRF的多输出集成系统，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录中的药物使用信息抽取依然是临床自然语言处理中的一大难题，尤其是在特定领域和低资源环境下，现有大型语言模型的应用受限于信任、控制和效率问题。

Method: 方法包括利用BETO模型结合条件随机场（CRF）层进行序列标注，采用多样化的训练策略，并通过句子过滤提高精度。

Result: 在ToxHabits共享任务中，系统在触发词检测上取得了0.94的F1和0.97的精准率，论元检测达到了0.91的F1分数。

Conclusion: 本研究提出的多输出集成系统有效提升了西班牙语临床文本中有毒物质使用信息的检测性能，达到了较高的准确率和F1得分。

Abstract: Extracting drug use information from unstructured Electronic Health Records remains a major challenge in clinical Natural Language Processing. While Large Language Models demonstrate advancements, their use in clinical NLP is limited by concerns over trust, control, and efficiency. To address this, we present NOWJ submission to the ToxHabits Shared Task at BioCreative IX. This task targets the detection of toxic substance use and contextual attributes in Spanish clinical texts, a domain-specific, low-resource setting. We propose a multi-output ensemble system tackling both Subtask 1 - ToxNER and Subtask 2 - ToxUse. Our system integrates BETO with a CRF layer for sequence labeling, employs diverse training strategies, and uses sentence filtering to boost precision. Our top run achieved 0.94 F1 and 0.97 precision for Trigger Detection, and 0.91 F1 for Argument Detection.

</details>


### [20] [Listen to the Layers: Mitigating Hallucinations with Inter-Layer Disagreement](https://arxiv.org/abs/2602.09486)
*Koduvayur Subbalakshmi,Sabbir Hossain Ujjal,Venkata Krishna Teja Mangichetty,Nastaran Jamalipour Soofi*

Main category: cs.CL

TL;DR: 该论文提出CoCoA，一种通过监测和惩罚模型中间层表征不稳定性来减少大语言模型生成幻觉的训练无关解码算法，显著提升文本事实正确性，且无需模型再训练。


<details>
  <summary>Details</summary>
Motivation: 预训练大语言模型生成的文本虽然流畅，但常包含虚假信息（幻觉），减少其应用的可靠性。作者假设文本的事实性与模型内部层的表示稳定性相关，基于此提出新方法以减少幻觉。

Method: 提出了CoCoA解码器和其变体CoCoA-SIG，这两种训练无关的解码方法通过评估和惩罚生成文本在中间层的表示不稳定性，动态调整生成过程以提高输出的一致性和事实准确性。

Result: 在问答、摘要、代码生成等多任务和多个模型（Llama-3, Qwen-2.5, Mistral）上，CoCoA显著提升了生成文本的事实准确率，且无须重新训练模型即可实施。

Conclusion: CoCoA解码器通过利用模型内部层的表征不稳定性信号，有效减少了大语言模型生成中的幻觉现象，从而提升了生成文本的事实正确性和可靠性。

Abstract: Pretrained Large Language Models (LLMs) are prone to generating fluent yet factually incorrect text-a phenomenon known as hallucinations, undermining their reliability and utility in downstream tasks. We hypothesize that a generated text span's factuality is correlated with its representational instability across the model's internal layers. Based on this, we propose the CoCoA (Confusion and Consistency Aware) decoder, a novel, training-free decoding algorithm that mitigates hallucinations at inference time by listening to these signals in the middle layers. We propose two metrics to quantify this instability in the middle layers, and use it to penalize outputs that exhibit high internal confusion, thereby steering the model towards more internally consistent and factually grounded outputs. We further propose a self-information gated variant, CoCoA-SIG, that dynamically modulates this penalty to selectively target high-surprise, unstable generations. Extensive experiments on diverse tasks, including question-answering, summarization and code generation demonstrate that CoCoA significantly improves factual correctness across multiple model families (e.g., Llama-3, Qwen-2.5, Mistral). By leveraging model-intrinsic signals, CoCoA offers an effective and broadly applicable method for enhancing the trustworthiness of LLMs at inference time, without requiring any model retraining.

</details>


### [21] [Where-to-Unmask: Ground-Truth-Guided Unmasking Order Learning for Masked Diffusion Language Models](https://arxiv.org/abs/2602.09501)
*Hikaru Asano,Tadashi Kozuno,Kuniaki Saito,Yukino Baba*

Main category: cs.CL

TL;DR: 本文提出Gt-Margin分值及基于它的监督式解锁顺序规划器，改进Masked Diffusion Language Models的位置解锁策略，提升生成质量和逻辑推理准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的Masked Diffusion Language Models在推理时需要决策解锁位置（where-to-unmask）和解锁内容（what-to-unmask），但位置解锁顺序通常依赖启发式方法或成本高昂的强化学习。

Method: 提出Gt-Margin分值作为位置优先级的oracle排序标准，并基于此训练监督式解锁顺序规划器，通过学习排序方法模仿oracle排序，从而指导推理时的位置选择。

Result: Gt-Margin oracle排序显著提升生成质量，尤其是逻辑推理任务。训练的解锁规划器能无缝集成入标准MDLM采样，提升推理准确率，无需修改token预测模型。

Conclusion: 引入基于真实标签概率差距的Gt-Margin排序方法，为MDLM提供了高效且有效的解锁顺序规划方案，显著提升了文本生成表现和逻辑推理能力。

Abstract: Masked Diffusion Language Models (MDLMs) generate text by iteratively filling masked tokens, requiring two coupled decisions at each step: which positions to unmask (where-to-unmask) and which tokens to place (what-to-unmask). While standard MDLM training directly optimizes token prediction (what-to-unmask), inference-time unmasking orders (where-to-unmask) are typically determined by heuristic confidence measures or trained through reinforcement learning with costly on-policy rollouts. To address this, we introduce Gt-Margin, a position-wise score derived from ground-truth tokens, defined as the probability margin between the correct token and its strongest alternative. Gt-Margin yields an oracle unmasking order that prioritizes easier positions first under each partially masked state. We demonstrate that leveraging this oracle unmasking order significantly enhances final generation quality, particularly on logical reasoning benchmarks. Building on this insight, we train a supervised unmasking planner via learning-to-rank to imitate the oracle ordering from masked contexts. The resulting planner integrates into standard MDLM sampling to select where-to-unmask, improving reasoning accuracy without modifying the token prediction model.

</details>


### [22] [EcoGym: Evaluating LLMs for Long-Horizon Plan-and-Execute in Interactive Economies](https://arxiv.org/abs/2602.09514)
*Xavier Hu,Jinxiang Xia,Shengze Xu,Kangqi Song,Yishuo Yuan,Guibin Zhang,Jincheng Ren,Boyu Feng,Li Lu,Tieyong Zeng,Jiaheng Liu,Minghao Liu,Yuchen Elenor Jiang,Wei Wang,He Zhu,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: 提出EcoGym基准测试平台，在模拟经济环境中评价大型语言模型长周期连续决策能力，发现现有模型策略和执行均有不足，无模型全场景领先。


<details>
  <summary>Details</summary>
Motivation: 当前长远规划评估框架多为情景驱动、领域特定或缺乏真实经济动态的持续性，难以全面测试LLM长期连续决策能力，因而亟需一个通用且真实经济环境中的长周期评测平台。

Method: 提出EcoGym基准测试平台，包含三种经济模拟环境（Vending、Freelance、Operation），基于统一决策流程和标准接口，实现超过1000步的长周期动作预算和执行。通过经济指标（净资产、收入、DAU）评估模型长期策略一致性与鲁棒性。

Result: 在EcoGym三个环境中测试了11个领先LLM，发现各模型在不同场景表现有差异且显著存在策略层面或执行效率上的不足，揭示了长周期经济决策中模型性能的系统性限制。

Conclusion: 现有大型语言模型（LLM）在长远策略规划上存在显著不足，且没有模型能在所有任务环境中表现优异。EcoGym作为一个开放且可扩展的基准平台，有助于推进经济模拟环境中长周期自主决策能力的评测与研究。

Abstract: Long-horizon planning is widely recognized as a core capability of autonomous LLM-based agents; however, current evaluation frameworks suffer from being largely episodic, domain-specific, or insufficiently grounded in persistent economic dynamics. We introduce EcoGym, a generalizable benchmark for continuous plan-and-execute decision making in interactive economies. EcoGym comprises three diverse environments: Vending, Freelance, and Operation, implemented in a unified decision-making process with standardized interfaces, and budgeted actions over an effectively unbounded horizon (1000+ steps if 365 day-loops for evaluation). The evaluation of EcoGym is based on business-relevant outcomes (e.g., net worth, income, and DAU), targeting long-term strategic coherence and robustness under partial observability and stochasticity. Experiments across eleven leading LLMs expose a systematic tension: no single model dominates across all three scenarios. Critically, we find that models exhibit significant suboptimality in either high-level strategies or efficient actions executions. EcoGym is released as an open, extensible testbed for transparent long-horizon agent evaluation and for studying controllability-utility trade-offs in realistic economic settings.

</details>


### [23] [The CLEF-2026 CheckThat! Lab: Advancing Multilingual Fact-Checking](https://arxiv.org/abs/2602.09516)
*Julia Maria Struß,Sebastian Schellhammer,Stefan Dietze,Venktesh V,Vinay Setty,Tanmoy Chakraborty,Preslav Nakov,Avishek Anand,Primakov Chungkham,Salim Hafid,Dhruv Sahnan,Konstantin Todorov*

Main category: cs.CL

TL;DR: CheckThat!实验室通过设计三项多语言事实核查任务，推进在线虚假信息检测和验证技术的发展。


<details>
  <summary>Details</summary>
Motivation: 应对在线传播的虚假信息和操控，促进多语言、多平台上的创新验证技术发展。

Method: 围绕验证流程，设计科学网页声明源检索、数值时间声明事实核查及完整事实核查文章生成三项任务，涵盖分类、检索和生成挑战。

Result: 提出并实施了三项关键任务，拓展了验证流程，提升了多语言环境下虚假信息核查的技术能力。

Conclusion: 通过多样化任务设计，CheckThat!实验室推动了虚假信息验证领域的技术进步，为多语言事实核查提供了新的方法和工具。

Abstract: The CheckThat! lab aims to advance the development of innovative technologies combating disinformation and manipulation efforts in online communication across a multitude of languages and platforms. While in early editions the focus has been on core tasks of the verification pipeline (check-worthiness, evidence retrieval, and verification), in the past three editions, the lab added additional tasks linked to the verification process. In this year's edition, the verification pipeline is at the center again with the following tasks: Task 1 on source retrieval for scientific web claims (a follow-up of the 2025 edition), Task 2 on fact-checking numerical and temporal claims, which adds a reasoning component to the 2025 edition, and Task 3, which expands the verification pipeline with generation of full-fact-checking articles. These tasks represent challenging classification and retrieval problems as well as generation challenges at the document and span level, including multilingual settings.

</details>


### [24] [Knowledge Integration Decay in Search-Augmented Reasoning of Large Language Models](https://arxiv.org/abs/2602.09517)
*Sangwon Yu,Ik-hwan Kim,Donghun Kang,Bongkyu Hwang,Junhwa Choi,Suk-hoon Jung,Seungki Hong,Taehee Lee,Sungroh Yoon*

Main category: cs.CL

TL;DR: 本文发现大语言模型在长推理链中存在知识整合衰减问题，提出了自锚定知识编码策略，在不需训练的情况下有效稳定知识利用，显著提升了推理性能。


<details>
  <summary>Details</summary>
Motivation: 发现了在长推理链中，随着推理长度增加，模型越来越难将检索到的知识整合到后续推理步骤中，即知识整合衰减（KID）现象。

Method: 提出了一种无需训练的推理时策略——自锚定知识编码（SAKE），通过将检索到的知识锚定于推理过程的起始和结束位置，保持其语义完整性。

Result: SAKE显著减轻了知识整合衰减问题，提升了模型在多跳问答和复杂推理基准测试上的性能。

Conclusion: 提出的SAKE方法有效缓解了知识整合衰减问题，提高了大语言模型在多跳问答和复杂推理任务中的表现。

Abstract: Modern Large Language Models (LLMs) have demonstrated remarkable capabilities in complex tasks by employing search-augmented reasoning to incorporate external knowledge into long chains of thought. However, we identify a critical yet underexplored bottleneck in this paradigm, termed Knowledge Integration Decay (KID). Specifically, we observe that as the length of reasoning generated before search grows, models increasingly fail to integrate retrieved evidence into subsequent reasoning steps, limiting performance even when relevant information is available. To address this, we propose Self-Anchored Knowledge Encoding (SAKE), a training-free inference-time strategy designed to stabilize knowledge utilization. By anchoring retrieved knowledge at both the beginning and end of the reasoning process, SAKE prevents it from being overshadowed by prior context, thereby preserving its semantic integrity. Extensive experiments on multi-hop QA and complex reasoning benchmarks demonstrate that SAKE significantly mitigates KID and improves performance, offering a lightweight yet effective solution for knowledge integration in agentic LLMs.

</details>


### [25] [UniARM: Towards a Unified Autoregressive Reward Model for Multi-Objective Test-Time Alignment](https://arxiv.org/abs/2602.09538)
*Hongyan Xie,Yikun Ban,Ruiyu Fang,Zixuan Huang,Deqing Wang,Jianxin Li,Yitong Yao,Chao Wang,Shuangyong Song*

Main category: cs.CL

TL;DR: 针对多目标对齐中偏好特征交互被忽视和特征纠缠的问题，提出MoSLoRA和UniARM方法，实现共享特征提取和统一参数空间联合建模，提升了LLM响应的多偏好对齐能力。


<details>
  <summary>Details</summary>
Motivation: 现有通过自回归奖励模型（ARM）实现多目标对齐的方法存在独立参数训练导致忽视偏好间交互或者特征纠缠，影响生成结果与用户偏好的匹配度。

Method: 采用MoSLoRA通过共享的无偏好模块提取特征，再通过偏好调制模块按混合偏好向量进行仿射变换；基于此设计了UniARM框架，在单一参数空间内联合建模所有偏好维度，避免独立参数的需求。

Result: 提出的方法能有效缓解特征纠缠问题，实现对生成内容的精确偏好权衡控制，并提升了多目标对齐效果，同时具备较好的实际使用可行性。

Conclusion: 本文提出的MoSLoRA方法和UniARM框架有效解决了多目标对齐中不同偏好特征交互被忽视或特征纠缠的问题，提高了对多偏好目标的联合建模能力和生成内容的对齐度。

Abstract: Multi-objective alignment aims to align LLM responses with multiple human preference objectives. Among existing methods, guiding the generation of frozen LLMs through autoregressive reward models (ARMs) to accomplish multi-objective test-time alignment is a low-cost solution. However, these methods typically rely on independent parameters for each preference objective, either by training ARMs independently across preference dimensions, which neglects interactions among preference features, or by training a single ARM with separate feature extraction modules for each preference, which can cause feature entanglement. Both strategies can result in misalignment between generated outputs and user preferences. To address this limitation, we propose Preference-Modulated \& Shared Low-Rank Adaptation (MoSLoRA) for ARM training, which first extracts shared features via a preference-agnostic module and then applies affine transformations to shared features via a preference modulation module conditioned on mixed preference vectors. This design mitigates feature entanglement and enables precise control over preference trade-offs during inference. Building on this, we introduce the Unified Autoregressive Reward Model (UniARM), a novel framework for multi-objective test-time alignment. UniARM jointly models all preference dimensions in a single parameter space, eliminating the need for independent parameters for each preference objective. es on larger-scale LLMs, enhancing its practical usability.

</details>


### [26] [Comprehensive Comparison of RAG Methods Across Multi-Domain Conversational QA](https://arxiv.org/abs/2602.09552)
*Klejda Alushi,Jan Strich,Chris Biemann,Martin Semmann*

Main category: cs.CL

TL;DR: 本文系统比较了多轮对话问答中不同RAG方法，发现简单策略优于复杂方法，检索效果受数据和对话长度影响显著，代码已开源。


<details>
  <summary>Details</summary>
Motivation: 现有研究多集中于单轮对话和单独评估RAG方法，缺乏对多轮对话中RAG方法系统比较，考虑对话历史、共指和用户意图变化复杂性。

Method: 在统一实验框架下，比较分析了八个多领域多轮对话问答数据集上的基础与先进RAG方法，评估其检索质量和答案生成效果，并跟踪性能随对话轮次的变化。

Result: 简单稳健的RAG方法持续优于基础方法，部分先进技术无提升甚至劣于无RAG基线；数据集特征和对话长度对检索效果有重大影响。

Conclusion: 不同RAG方法在多轮对话问答中的表现显著受数据集特征和对话长度影响，没有单一方法在所有场景中占优，且简单有效的方法如reranking、混合BM25和HyDE表现优于复杂方法。

Abstract: Conversational question answering increasingly relies on retrieval-augmented generation (RAG) to ground large language models (LLMs) in external knowledge. Yet, most existing studies evaluate RAG methods in isolation and primarily focus on single-turn settings. This paper addresses the lack of a systematic comparison of RAG methods for multi-turn conversational QA, where dialogue history, coreference, and shifting user intent substantially complicate retrieval. We present a comprehensive empirical study of vanilla and advanced RAG methods across eight diverse conversational QA datasets spanning multiple domains. Using a unified experimental setup, we evaluate retrieval quality and answer generation using generator and retrieval metrics, and analyze how performance evolves across conversation turns. Our results show that robust yet straightforward methods, such as reranking, hybrid BM25, and HyDE, consistently outperform vanilla RAG. In contrast, several advanced techniques fail to yield gains and can even degrade performance below the No-RAG baseline. We further demonstrate that dataset characteristics and dialogue length strongly influence retrieval effectiveness, explaining why no single RAG strategy dominates across settings. Overall, our findings indicate that effective conversational RAG depends less on method complexity than on alignment between the retrieval strategy and the dataset structure. We publish the code used.\footnote{\href{https://github.com/Klejda-A/exp-rag.git}{GitHub Repository}}

</details>


### [27] [Advancing Block Diffusion Language Models for Test-Time Scaling](https://arxiv.org/abs/2602.09555)
*Yi Lu,Deyang Kong,Jianing Wang,Linsen Guo,Xue Wang,Qi Guo,Tao Gui,Xuanjing Huang,Wei Ye,Shikun Zhang,Wei Wang*

Main category: cs.CL

TL;DR: 本文针对区块扩散语言模型测试时扩展难题，提出自适应解码和分层块生成策略，大幅提升推理速度和效果。


<details>
  <summary>Details</summary>
Motivation: 现有区块扩散语言模型在测试时扩展和长链推理中面临解码速度与效果的权衡挑战，需要新的方法提升推理效能。

Method: 提出了有界自适应置信度解码（BACD）和“粗思考、细批评”（TCCF）的方法，并采用渐进式块大小扩展实现高效大块尺寸解码。

Result: BACD和TCCF应用于TDAR-8B模型在AIME24任务中相比强基线TraDo-8B实现了2.26倍推理加速和11.2分性能提升。

Conclusion: 本文提出的基于BDLMs的测试时扩展统一框架有效提升了长链条推理中的解码效率与准确性，实现了推理速度和效果的良好平衡。

Abstract: Recent advances in block diffusion language models have demonstrated competitive performance and strong scalability on reasoning tasks. However, existing BDLMs have limited exploration under the test-time scaling setting and face more severe decoding challenges in long Chain-of-Thought reasoning, particularly in balancing the decoding speed and effectiveness. In this work, we propose a unified framework for test-time scaling in BDLMs that introduces adaptivity in both decoding and block-wise generation. At the decoding level, we propose Bounded Adaptive Confidence Decoding (BACD), a difficulty-aware sampling strategy that dynamically adjusts denoising based on model confidence, accelerating inference while controlling error accumulation. Beyond step-wise adaptivity, we introduce Think Coarse, Critic Fine (TCCF), a test-time scaling paradigm that allocates large block sizes to exploratory reasoning and smaller block sizes to refinement, achieving an effective efficiency-effectiveness balance. To enable efficient and effective decoding with a large block size, we adopt Progressive Block Size Extension, which mitigates performance degradation when scaling block sizes. Extensive experiments show that applying BACD and TCCF to TDAR-8B yields significant improvements over strong baselines such as TraDo-8B (2.26x speedup, +11.2 points on AIME24). These results mark an important step toward unlocking the potential of BDLMs for test-time scaling in complex reasoning tasks.

</details>


### [28] [LEMUR: A Corpus for Robust Fine-Tuning of Multilingual Law Embedding Models for Retrieval](https://arxiv.org/abs/2602.09570)
*Narges Baba Ahmadi,Jan Strich,Martin Semmann,Chris Biemann*

Main category: cs.CL

TL;DR: 作者构建了大规模多语种法律语料LEMUR，并通过领域微调提升了法律文本多语言检索的准确性，尤其改善了低资源语言表现，且模型可迁移至未见语言。


<details>
  <summary>Details</summary>
Motivation: 现有多语言法律语料库不适合语义检索，且PDF格式的法规文本因提取不完全而引入大量噪声，限制了多语言法律信息访问的准确性和可靠性。

Method: 构建了包含24,953份欧盟环境立法PDF文档的多语言语料库LEMUR，使用Lexical Content Score评估PDF转文本的准确度；基于LEMUR使用对比学习目标，单语和双语场景下微调了三种多语言嵌入模型。

Result: 法律领域微调显著提高了Top-k检索准确率，低资源语言提升尤为明显；跨语言评估表明模型增强了语言无关的法律内容理解能力，改善了多语言法律信息检索表现。

Conclusion: 通过对多语言法律语料库LEMUR的构建与法律领域特定模型的微调，显著提升了法律文本检索的准确性，尤其在低资源语言中效果显著，且微调后的模型在未见语言上也能保持良好表现，表明提升的是语言无关的法律内容表征能力。

Abstract: Large language models (LLMs) are increasingly used to access legal information. Yet, their deployment in multilingual legal settings is constrained by unreliable retrieval and the lack of domain-adapted, open-embedding models. In particular, existing multilingual legal corpora are not designed for semantic retrieval, and PDF-based legislative sources introduce substantial noise due to imperfect text extraction. To address these challenges, we introduce LEMUR, a large-scale multilingual corpus of EU environmental legislation constructed from 24,953 official EUR-Lex PDF documents covering 25 languages. We quantify the fidelity of PDF-to-text conversion by measuring lexical consistency against authoritative HTML versions using the Lexical Content Score (LCS). Building on LEMUR, we fine-tune three state-of-the-art multilingual embedding models using contrastive objectives in both monolingual and bilingual settings, reflecting realistic legal-retrieval scenarios. Experiments across low- and high-resource languages demonstrate that legal-domain fine-tuning consistently improves Top-k retrieval accuracy relative to strong baselines, with particularly pronounced gains for low-resource languages. Cross-lingual evaluations show that these improvements transfer to unseen languages, indicating that fine-tuning primarily enhances language-independent, content-level legal representations rather than language-specific cues. We publish code\footnote{\href{https://github.com/nargesbh/eur_lex}{GitHub Repository}} and data\footnote{\href{https://huggingface.co/datasets/G4KMU/LEMUR}{Hugging Face Dataset}}.

</details>


### [29] [Aligning Tree-Search Policies with Fixed Token Budgets in Test-Time Scaling of LLMs](https://arxiv.org/abs/2602.09574)
*Sora Miyamoto,Daisuke Oba,Naoaki Okazaki*

Main category: cs.CL

TL;DR: 针对固定token预算限制，BG-MCTS动态调整树搜索策略，提升了解码性能，效果优于传统预算无关方法。


<details>
  <summary>Details</summary>
Motivation: 现有树搜索策略忽视预算限制，仅将预算作为终止条件，导致预算用尽时搜索效果不佳，影响实际应用中的模型表现。

Method: 提出了Budget-Guided MCTS算法，该算法根据剩余token预算动态调整搜索策略，初期进行广泛探索，预算减少时优先进行细化和答案完成，减少浅层节点的后期分支扩张。

Result: BG-MCTS在不同token预算下均明显优于预算无关的树搜索方法，在MATH500和AIME24/25数据集上表现优异。

Conclusion: BG-MCTS通过调整搜索策略以适应剩余的token预算，有效避免了传统树搜索策略在预算限制下的性能下降问题，实现了更优的解码效果。

Abstract: Tree-search decoding is an effective form of test-time scaling for large language models (LLMs), but real-world deployment imposes a fixed per-query token budget that varies across settings. Existing tree-search policies are largely budget-agnostic, treating the budget as a termination condition, which can lead to late-stage over-branching or premature termination. We propose {Budget-Guided MCTS} (BG-MCTS), a tree-search decoding algorithm that aligns its search policy with the remaining token budget: it starts with broad exploration, then prioritizes refinement and answer completion as the budget depletes while reducing late-stage branching from shallow nodes. BG-MCTS consistently outperforms budget-agnostic tree-search baselines across different budgets on MATH500 and AIME24/25 with open-weight LLMs.

</details>


### [30] [Context-Aware Counterfactual Data Augmentation for Gender Bias Mitigation in Language Models](https://arxiv.org/abs/2602.09590)
*Shweta Parihar,Liu Guangliang,Natalie Parde,Lu Cheng*

Main category: cs.CL

TL;DR: 提出Context-CDA方法，通过增强上下文和不确定性过滤提升反事实数据的质量，有效减轻语言模型性别偏见且不损失性能。


<details>
  <summary>Details</summary>
Motivation: 现有反事实数据增强方法生成的合成数据往往与实际分布不符或忽视敏感属性的社会上下文，导致语言模型能力下降和偏见去除效果有限。

Method: 提出了一种简单有效的上下文增强型反事实数据增强方法Context-CDA，利用大型语言模型提升去偏语料的多样性和上下文相关性，并通过基于不确定性的过滤排除低质量生成数据。

Result: 在性别偏见基准测试中，Context-CDA显著降低偏见的同时保持了语言建模能力，并通过分析词预测概率的分布变化提供了对社会偏见的洞察。

Conclusion: Context-CDA方法能够有效缓解语言模型中的性别偏见，同时保持语言建模性能，不影响下游任务表现。

Abstract: A challenge in mitigating social bias in fine-tuned language models (LMs) is the potential reduction in language modeling capability, which can harm downstream performance. Counterfactual data augmentation (CDA), a widely used method for fine-tuning, highlights this issue by generating synthetic data that may align poorly with real-world distributions or creating overly simplistic counterfactuals that ignore the social context of altered sensitive attributes (e.g., gender) in the pretraining corpus. To address these limitations, we propose a simple yet effective context-augmented CDA method, Context-CDA, which uses large LMs to enhance the diversity and contextual relevance of the debiasing corpus. By minimizing discrepancies between the debiasing corpus and pretraining data through augmented context, this approach ensures better alignment, enhancing language modeling capability. We then employ uncertainty-based filtering to exclude generated counterfactuals considered low-quality by the target smaller LMs (i.e., LMs to be debiased), further improving the fine-tuning corpus quality. Experimental results on gender bias benchmarks demonstrate that Context-CDA effectively mitigates bias without sacrificing language modeling performance while offering insights into social biases by analyzing distribution shifts in next-token generation probabilities.

</details>


### [31] [On the Optimal Reasoning Length for RL-Trained Language Models](https://arxiv.org/abs/2602.09591)
*Daisuke Nohara,Taishi Nakamura,Rio Yokota*

Main category: cs.CL

TL;DR: 本研究针对强化学习中的大型语言模型，通过比较长度控制方法发现合理调节输出长度能提升推理效率，避免过长输出带来的计算负担和性能下降。


<details>
  <summary>Details</summary>
Motivation: 强化学习提升大型语言模型推理能力，但导致输出链过长并增加计算成本，需平衡效率与性能的输出长度控制方法。

Method: 在两种模型上比较多种长度控制方法，包括Qwen3-1.7B Base和DeepSeek-R1-Distill-Qwen-1.5B。

Result: 发现两种失败模式：长输出增加分散性，短输出导致推理不足。合理调控长度能在保证性能的同时提升效率。

Conclusion: 适当调整输出长度可以提升强推理能力模型的推理效率，但长度惩罚可能阻碍推理能力的获取。

Abstract: Reinforcement learning substantially improves reasoning in large language models, but it also tends to lengthen chain of thought outputs and increase computational cost during both training and inference. Though length control methods have been proposed, it remains unclear what the optimal output length is for balancing efficiency and performance. In this work, we compare several length control methods on two models, Qwen3-1.7B Base and DeepSeek-R1-Distill-Qwen-1.5B. Our results indicate that length penalties may hinder reasoning acquisition, while properly tuned length control can improve efficiency for models with strong prior reasoning. By extending prior work to RL trained policies, we identify two failure modes, 1) long outputs increase dispersion, and 2) short outputs lead to under-thinking.

</details>


### [32] [Learning from the Irrecoverable: Error-Localized Policy Optimization for Tool-Integrated LLM Reasoning](https://arxiv.org/abs/2602.09598)
*Qiao Liang,Yuke Zhu,Chao Ge,Lei Yang,Ying Shen,Bo Zheng,Sheng Guo*

Main category: cs.CL

TL;DR: ELPO提出一种基于定位关键错误步骤的策略优化方法，解决了工具集成推理中奖励稀疏和信用分配难题，有效提升了多任务环境下的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的基于结果的强化学习在工具集成推理中因奖励稀疏且延迟，难以进行有效的步骤级信用分配，尤其在长远路径中，早期错误对最终结果影响巨大，因此需要定位首个不可恢复错误以提升训练效果。

Method: ELPO利用二分搜索的回滚树定位首个不可恢复的错误步骤，采用层级优势归因转化为稳定的学习信号，并通过错误定位的自适应剪切机制加强关键步骤及后续步骤的修正更新。

Result: 在数学、科学问答和代码执行的多项工具集成推理基准测试中，ELPO在相当采样预算下持续优于强基线Agentic RL方法，并在Pass@K、Major@K扩展性、回滚排序质量及工具调用效率上获得额外提升。

Conclusion: 该论文提出的Error-Localized Policy Optimization (ELPO)方法在工具集成推理任务中，通过定位并纠正第一个不可恢复错误步骤，实现了更精细的信用分配，从而显著提升了策略优化效果。

Abstract: Tool-integrated reasoning (TIR) enables LLM agents to solve tasks through planning, tool use, and iterative revision, but outcome-only reinforcement learning in this setting suffers from sparse, delayed rewards and weak step-level credit assignment. In long-horizon TIR trajectories, an early irrecoverable mistake can determine success or failure, making it crucial to localize the first irrecoverable step and leverage it for fine-grained credit assignment. We propose Error-Localized Policy Optimization (ELPO), which localizes the first irrecoverable step via binary-search rollout trees under a fixed rollout budget, converts the resulting tree into stable learning signals through hierarchical advantage attribution, and applies error-localized adaptive clipping to strengthen corrective updates on the critical step and its suffix. Across TIR benchmarks in math, science QA, and code execution, ELPO consistently outperforms strong Agentic RL baselines under comparable sampling budgets, with additional gains in Pass@K and Major@K scaling, rollout ranking quality, and tool-call efficiency. Our code will be publicly released soon.

</details>


### [33] [AlignTune: Modular Toolkit for Post-Training Alignment of Large Language Models](https://arxiv.org/abs/2602.09621)
*R E Zera Marveen Lyngkhoi,Chirag Chawla,Pratinav Seth,Utsav Avaiya,Soham Bhattacharjee,Mykola Khandoga,Rui Yuan,Vinay Kumar Sankarapu*

Main category: cs.CL

TL;DR: AlignTune 是一个统一多后端的工具包，规范了大语言模型后训练对齐过程，提升了实验的复现性和可比较性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型后训练对齐实验分散在特定后端工具和零散代码中，难以复现和比较。

Method: AlignTune 提供了统一界面，支持监督微调和 RLHF 样式的优化，集成了可扩展的奖励层和标准评估模块，采用工厂设计模式隔离后端逻辑。

Result: AlignTune 实现了对多后端的兼容支持和统一配置，促进了对齐实验的标准化、可扩展性和可复现性。

Conclusion: AlignTune作为一个模块化工具包，解决了当前大语言模型后训练对齐过程中的后端干扰、奖励碎片化和流程不可复现等问题，实现了统一接口和可复用的实验方法。

Abstract: Post-training alignment is central to deploying large language models (LLMs), yet practical workflows remain split across backend-specific tools and ad-hoc glue code, making experiments hard to reproduce. We identify backend interference, reward fragmentation, and irreproducible pipelines as key obstacles in alignment research. We introduce AlignTune, a modular toolkit exposing a unified interface for supervised fine-tuning (SFT) and RLHF-style optimization with interchangeable TRL and Unsloth backends. AlignTune standardizes configuration, provides an extensible reward layer (rule-based and learned), and integrates evaluation over standard benchmarks and custom tasks. By isolating backend-specific logic behind a single factory boundary, AlignTune enables controlled comparisons and reproducible alignment experiments.

</details>


### [34] [MILE-RefHumEval: A Reference-Free, Multi-Independent LLM Framework for Human-Aligned Evaluation](https://arxiv.org/abs/2602.09624)
*Nalin Srun,Parisa Rastin,Guénaël Cabanes,Lydia Boudjeloud Assala*

Main category: cs.CL

TL;DR: MILE-RefHumEval提出了一种无需参考标准的多评估者集合框架，实现了高效、准确且可扩展的LLM评价。


<details>
  <summary>Details</summary>
Motivation: 传统LLM评估方法依赖人工标注或评估者协调，成本高且效率低，亟需一种灵活且可扩展的无参考评估方案。

Method: 该框架通过集合多个独立提示的评估者，依据人类对齐的评分标准，支持离散和连续评分，涵盖从最佳候选选择、摘要、图像描述到对话等任务。

Result: 实验表明，该方法与人类判断高度一致，优于现有方法，并且计算负担更低。

Conclusion: MILE-RefHumEval是一种无需参考答案或协调评估者的高效、健壮且与人类评价一致的LLM评估框架。

Abstract: We introduce MILE-RefHumEval, a reference-free framework for evaluating Large Language Models (LLMs) without ground-truth annotations or evaluator coordination. It leverages an ensemble of independently prompted evaluators guided by a human-aligned schema, supporting both discrete and continuous scoring judgement. With task-specific prompts from best candidate selection, summarization and image captioning to dialogue, MILE-RefHumEval provides flexible, interpretable, and scalable assessments. Experiments show it aligns closely with human judgments, outperforms prior methods, and reduces computational overhead, offering an efficient, robust, and human-aligned solution for real-world LLM evaluation.

</details>


### [35] [MATA: Multi-Agent Framework for Reliable and Flexible Table Question Answering](https://arxiv.org/abs/2602.09642)
*Sieun Hyeon,Jusang Oh,Sunghwan Steve Cho,Jaeyoung Do*

Main category: cs.CL

TL;DR: MATA利用多代理和多种推理路径以小型模型辅助，提升表格问答准确性与效率，适应资源受限环境，实验验证效果优异。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在表格问答任务中的表现虽好，但存在可靠性、扩展性和效率等问题，尤其在资源有限或隐私敏感环境下难以应用。

Method: 提出MATA多代理表格问答框架，结合多样推理路径与小型语言模型工具生成候选答案，并通过算法优化减少大型语言模型调用次数，提高推理效率。

Result: 在两个不同难度基准测试和十种大型语言模型上，MATA表现出最先进的准确率和高效推理能力，有效避免了大型模型的过度推理调用。

Conclusion: MATA框架通过多代理与多样化推理路径显著提升了表格问答的准确性与效率，避免了过度调用大型语言模型，实现了在资源受限环境中的优异表现。

Abstract: Recent advances in Large Language Models (LLMs) have significantly improved table understanding tasks such as Table Question Answering (TableQA), yet challenges remain in ensuring reliability, scalability, and efficiency, especially in resource-constrained or privacy-sensitive environments. In this paper, we introduce MATA, a multi-agent TableQA framework that leverages multiple complementary reasoning paths and a set of tools built with small language models. MATA generates candidate answers through diverse reasoning styles for a given table and question, then refines or selects the optimal answer with the help of these tools. Furthermore, it incorporates an algorithm designed to minimize expensive LLM agent calls, enhancing overall efficiency. MATA maintains strong performance with small, open-source models and adapts easily across various LLM types. Extensive experiments on two benchmarks of varying difficulty with ten different LLMs demonstrate that MATA achieves state-of-the-art accuracy and highly efficient reasoning while avoiding excessive LLM inference. Our results highlight that careful orchestration of multiple reasoning pathways yields scalable and reliable TableQA. The code is available at https://github.com/AIDAS-Lab/MATA.

</details>


### [36] [Life Cycle-Aware Evaluation of Knowledge Distillation for Machine Translation: Environmental Impact and Translation Quality Trade-offs](https://arxiv.org/abs/2602.09691)
*Joseph Attieh,Timothee Mickus,Anne-Laure Ligozat,Aurélie Névéol,Jörg Tiedemann*

Main category: cs.CL

TL;DR: 本文通过机器学习生命周期评估工具，综合考虑机器翻译知识蒸馏的翻译质量和计算碳足迹，揭示了不同知识蒸馏方法在不同规模应用下的能耗和效果权衡，为实用选择提供指导。


<details>
  <summary>Details</summary>
Motivation: 机器翻译领域知识蒸馏研究通常只关注学生模型的翻译质量，忽略了蒸馏过程的计算复杂性，这使得在计算资源受限情况下难以选择合适的知识蒸馏方法。

Method: 采用机器学习生命周期评估工具（MLCA），从教师训练、蒸馏过程和推理三个生命周期阶段评估知识蒸馏方法的计算成本和碳足迹，同时结合翻译质量进行综合评价。

Result: 评估揭示蒸馏过程的碳足迹在小规模任务中占比较大，推理阶段在大规模任务中产生更高碳排放，且存在任务依赖的使用阈值，超过该阈值知识蒸馏效益明显。词级蒸馏在碳足迹与质量权衡方面表现优于序列级蒸馏。

Conclusion: 本研究发现知识蒸馏的计算开销在小规模部署时主要来自蒸馏过程，而在大规模使用时推理过程占主导，且词级蒸馏相比序列级蒸馏在碳足迹和质量的权衡上更优。

Abstract: Knowledge distillation (KD) is a tool to compress a larger system (teacher) into a smaller one (student). In machine translation, studies typically report only the translation quality of the student and omit the computational complexity of performing KD, making it difficult to select among the many available KD choices under compute-induced constraints. In this study, we evaluate representative KD methods by considering both translation quality and computational cost. We express computational cost as a carbon footprint using the machine learning life cycle assessment (MLCA) tool. This assessment accounts for runtime operational emissions and amortized hardware production costs throughout the KD model life cycle (teacher training, distillation, and inference). We find that (i) distillation overhead dominates the total footprint at small deployment volumes, (ii) inference dominates at scale, making KD beneficial only beyond a task-dependent usage threshold, and (iii) word-level distillation typically offers more favorable footprint-quality trade-offs than sequence-level distillation. Our protocol provides reproducible guidance for selecting KD methods under explicit quality and compute-induced constraints.

</details>


### [37] [Maastricht University at AMIYA: Adapting LLMs for Dialectal Arabic using Fine-tuning and MBR Decoding](https://arxiv.org/abs/2602.09703)
*Abdulhai Alali,Abderrahmane Issam*

Main category: cs.CL

TL;DR: 通过LoRA微调和方言感知解码技术改进大语言模型，实现对多种阿拉伯方言更准确自然的生成和翻译。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型多支持多种语言但对方言变体支持不足，因数据有限和语言变体复杂，导致方言表现欠佳。提升方言的生成和翻译能力具有重要研究价值。

Method: 采用预训练大语言模型，结合LoRA低秩适配微调、适配器合并技术和方言感知的MBR解码策略，在叙利亚、摩洛哥和沙特阿拉伯等阿拉伯方言数据上进行训练和测试。

Result: 实验结果表明，适配器合并和方言感知MBR解码显著提升了阿拉伯方言的忠实度，同时保持了语义准确性，验证了该方法的有效性和紧凑性。

Conclusion: 本文提出的方法通过LoRA微调、适配器合并和方言感知MBR解码，有效提升了预训练大语言模型在阿拉伯语方言生成和翻译中的表现，兼顾了方言忠实度和语义准确性。

Abstract: Large Language Models (LLMs) are becoming increasingly multilingual, supporting hundreds of languages, especially high resource ones. Unfortunately, Dialect variations are still underrepresented due to limited data and linguistic variation. In this work, we adapt a pre-trained LLM to improve dialectal performance. Specifically, we use Low Rank Adaptation (LoRA) fine-tuning on monolingual and English Dialect parallel data, adapter merging and dialect-aware MBR decoding to improve dialectal fidelity generation and translation. Experiments on Syrian, Moroccan, and Saudi Arabic show that merging and MBR improve dialectal fidelity while preserving semantic accuracy. This combination provides a compact and effective framework for robust dialectal Arabic generation.

</details>


### [38] [TraceMem: Weaving Narrative Memory Schemata from User Conversational Traces](https://arxiv.org/abs/2602.09712)
*Yiming Shu,Pei Liu,Tiange Zhang,Ruiyang Gao,Jun Ma,Chen Sun*

Main category: cs.CL

TL;DR: 提出了TraceMem，一种认知启发的叙事记忆框架，通过结构化和层次化的对话记忆管理，显著提升了大语言模型的长期对话理解与推理能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的上下文窗口有限，难以维持长期对话历史的连贯性，现有记忆系统未能有效捕捉对话的叙事连续性。

Method: 提出了一个受认知启发的三阶段记忆框架：短期记忆处理（通过主题分割提取语义）、突触记忆巩固（总结并提炼情节记忆）、系统记忆巩固（两级层次聚类形成连贯的叙事线程），并引入代理搜索机制以增强推理。

Result: 在LoCoMo基准测试中，TraceMem表现出色，优于基线模型，特别在多跳推理和时间推理任务上表现突出。

Conclusion: TraceMem通过构建连贯的叙事记忆图谱，显著提升了大语言模型在长期对话中的多跳和时间推理能力，达到当前最先进的性能水平。

Abstract: Sustaining long-term interactions remains a bottleneck for Large Language Models (LLMs), as their limited context windows struggle to manage dialogue histories that extend over time. Existing memory systems often treat interactions as disjointed snippets, failing to capture the underlying narrative coherence of the dialogue stream. We propose TraceMem, a cognitively-inspired framework that weaves structured, narrative memory schemata from user conversational traces through a three-stage pipeline: (1) Short-term Memory Processing, which employs a deductive topic segmentation approach to demarcate episode boundaries and extract semantic representation; (2) Synaptic Memory Consolidation, a process that summarizes episodes into episodic memories before distilling them alongside semantics into user-specific traces; and (3) Systems Memory Consolidation, which utilizes two-stage hierarchical clustering to organize these traces into coherent, time-evolving narrative threads under unifying themes. These threads are encapsulated into structured user memory cards, forming narrative memory schemata. For memory utilization, we provide an agentic search mechanism to enhance reasoning process. Evaluation on the LoCoMo benchmark shows that TraceMem achieves state-of-the-art performance with a brain-inspired architecture. Analysis shows that by constructing coherent narratives, it surpasses baselines in multi-hop and temporal reasoning, underscoring its essential role in deep narrative comprehension. Additionally, we provide an open discussion on memory systems, offering our perspectives and future outlook on the field. Our code implementation is available at: https://github.com/YimingShu-teay/TraceMem

</details>


### [39] [Unsupervised Layer-Wise Dynamic Test Time Adaptation for LLMs](https://arxiv.org/abs/2602.09719)
*Longhuan Xu,Cunjian Chen,Feng Yin*

Main category: cs.CL

TL;DR: 本文针对无监督、样本特定的测试时适应提出层级动态学习率调整框架，有效克服了固定学习率带来的不稳定问题，提升了大语言模型推理时的表现。


<details>
  <summary>Details</summary>
Motivation: 现有无监督、样本特定的测试时适应在仅依赖单个提示且无外部监督的条件下，使用固定学习率易导致过拟合和性能下降，亟需一种方法提高其稳定性和适应效果。

Method: 采用了仅更新LoRA参数并通过轻量级超网络预测每层每步学习率的乘子，实现对测试时适应强度的精细控制，从而加强了TTA的稳定性和效果。

Result: 在多个数据集和大语言模型上进行的实验表明，该方法学会了有效的层和步长学习率调整策略，显著增强了测试时适应的性能和稳定性。

Conclusion: 该论文提出了一种层级动态测试时适应（TTA）框架，有效解决了无监督、样本特定的测试时适应中因固定学习率导致的不稳定和过拟合问题，提升了大语言模型在推理时的性能和稳定性。

Abstract: Test-time adaptation (TTA) for large language models (LLMs) updates model parameters at inference time using signals available at deployment. This paper focuses on a common yet under-explored regime: unsupervised, sample-specific TTA, where the model adapts independently for each prompt using only the prompt itself, without gold answers or external supervision. Although appealing, naive unsupervised TTA with a fixed, handcrafted learning rate can be unstable: updates may overfit to prompt-specific statistics, drift from the desired answer distribution, and ultimately degrade generation quality. This failure mode is not surprising, as in this case TTA must adapt to a single prompt within only a few gradient steps, unlike standard training that averages updates over large datasets and long optimization horizons. Therefore, we propose layer-wise dynamic test-time adaptation, a framework which explicitly modulates TTA strength as a function of prompt representation, LLM structure and adaptation step. In our setting, TTA updates only LoRA parameters, and a lightweight hypernetwork predicts per-layer, per-step learning-rate multipliers, enabling fine-grained control. Experiments across various datasets and LLMs consistently show that our method substantially strengthens TTA by learning effective scaling patterns over adaptation steps and transformer layer projections, improving stability while delivering better performance.

</details>


### [40] [AI-Assisted Scientific Assessment: A Case Study on Climate Change](https://arxiv.org/abs/2602.09723)
*Christian Buck,Levke Caesar,Michelle Chen Huebscher,Massimiliano Ciaramita,Erich M. Fischer,Zeke Hausfather,Özge Kart Tokmak,Reto Knutti,Markus Leippold,Joseph Ludescher,Katharine J. Mach,Sofia Palazzo Corner,Kasra Rafiezadeh Shahi,Johan Rockström,Joeri Rogelj,Boris Sakschewski*

Main category: cs.CL

TL;DR: 本研究展示了基于Gemini的AI协作环境加速气候科学研究，AI贡献显著但仍需专家监督和补充。


<details>
  <summary>Details</summary>
Motivation: 传统AI科学家范式依赖可重复验证的任务，无法应对需要综合理论和证据形成共识的问题，故设计协作式科学评估环境。

Method: 基于Gemini的AI环境嵌入标准科学工作流，结合13位气候科学家合作，针对AMOC稳定性进行循环修订评估。

Result: 13位科学家通过104次修订循环，在46小时内综合了79篇论文，大部分AI生成内容被保留，AI提升了逻辑一致性和报告质量。

Conclusion: AI能够加速科学工作流程，在复杂科学话题上支持有效协作，但仍需专家大量监督和修改以确保科学严谨性。

Abstract: The emerging paradigm of AI co-scientists focuses on tasks characterized by repeatable verification, where agents explore search spaces in 'guess and check' loops. This paradigm does not extend to problems where repeated evaluation is impossible and ground truth is established by the consensus synthesis of theory and existing evidence. We evaluate a Gemini-based AI environment designed to support collaborative scientific assessment, integrated into a standard scientific workflow. In collaboration with a diverse group of 13 scientists working in the field of climate science, we tested the system on a complex topic: the stability of the Atlantic Meridional Overturning Circulation (AMOC). Our results show that AI can accelerate the scientific workflow. The group produced a comprehensive synthesis of 79 papers through 104 revision cycles in just over 46 person-hours. AI contribution was significant: most AI-generated content was retained in the report. AI also helped maintain logical consistency and presentation quality. However, expert additions were crucial to ensure its acceptability: less than half of the report was produced by AI. Furthermore, substantial oversight was required to expand and elevate the content to rigorous scientific standards.

</details>


### [41] [Targum -- A Multilingual New Testament Translation Corpus](https://arxiv.org/abs/2602.09724)
*Maciej Rapacz,Aleksander Smywiński-Pohl*

Main category: cs.CL

TL;DR: 本文推出了一个含657个新约翻译的多语言语料库，标准化注释支持灵活分析，弥补现有语料库深度不足，推动翻译历史的量化研究。


<details>
  <summary>Details</summary>
Motivation: 现有语料库注重语言的广度，但忽视了欧洲语言丰富的圣经翻译历史，缺少对翻译深度的充分展示和分析支持。

Method: 从12个在线圣经图书馆和一个已有语料库汇集翻译文本，手动标注元数据，包括作品统一标识、具体版本及修订年份，实现译本规范化。

Result: 构建了一个包含657个新约翻译的多语言语料库，其中352为独特版本，涵盖英、法、意、波兰、西五种语言，为研究者提供了基于不同标准计算“独特性”的工具，促进翻译历史的多层次研究。

Conclusion: 本文构建了一个包含657个新约圣经翻译版本的多语言语料库，覆盖五种语言的独特版本，填补了现有语料库在翻译深度上的不足。该语料库通过标准化的元数据注释，支持灵活的微观和宏观层面分析，推动了翻译历史的量化研究。

Abstract: Many European languages possess rich biblical translation histories, yet existing corpora - in prioritizing linguistic breadth - often fail to capture this depth. To address this gap, we introduce a multilingual corpus of 657 New Testament translations, of which 352 are unique, with unprecedented depth in five languages: English (208 unique versions from 396 total), French (41 from 78), Italian (18 from 33), Polish (30 from 48), and Spanish (55 from 102). Aggregated from 12 online biblical libraries and one preexisting corpus, each translation is manually annotated with metadata that maps the text to a standardized identifier for the work, its specific edition, and its year of revision. This canonicalization empowers researchers to define "uniqueness" for their own needs: they can perform micro-level analyses on translation families, such as the KJV lineage, or conduct macro-level studies by deduplicating closely related texts. By providing the first resource designed for such flexible, multilevel analysis, our corpus establishes a new benchmark for the quantitative study of translation history.

</details>


### [42] [Improving Interpretability of Lexical Semantic Change with Neurobiological Features](https://arxiv.org/abs/2602.09760)
*Kohei Oda,Hiroya Takamura,Kiyoaki Shirai,Natthawut Kertkeidkachorn*

Main category: cs.CL

TL;DR: 本文提出将词的上下文化嵌入映射到神经生物学特征空间的方法，既提升了词义变化程度估计性能，也增强了词义变化的可解释性，发现了新的词义变化类型。


<details>
  <summary>Details</summary>
Motivation: 现有词义变化研究重视性能提升，但缺乏对词义如何变化的可解释性分析，提升解释性有助于深入理解词义变化现象。

Method: 利用预训练语言模型获得的上下文化词嵌入，映射到每个维度对应词的原始特征的神经生物学特征空间，从而实现对词义变化的系统可解释分析。

Result: 提出的方法在词义变化程度估计任务中优于大多数现有方法，同时能够发现新颖且特定类型的词义变化，验证了方法的有效性和解释力。

Conclusion: 本研究提出的方法不仅在词义变化程度估计上表现出优越性，还通过将上下文化词嵌入映射到神经生物学特征空间，提高了词义变化的可解释性，挖掘出了以往研究中被忽视的词义变化类型。

Abstract: Lexical Semantic Change (LSC) is the phenomenon in which the meaning of a word change over time. Most studies on LSC focus on improving the performance of estimating the degree of LSC, however, it is often difficult to interpret how the meaning of a word change. Enhancing the interpretability of LSC is a significant challenge as it could lead to novel insights in this field. To tackle this challenge, we propose a method to map the semantic space of contextualized embeddings of words obtained by a pre-trained language model to a neurobiological feature space. In the neurobiological feature space, each dimension corresponds to a primitive feature of words, and its value represents the intensity of that feature. This enables humans to interpret LSC systematically. When employed for the estimation of the degree of LSC, our method demonstrates superior performance in comparison to the majority of the previous methods. In addition, given the high interpretability of the proposed method, several analyses on LSC are carried out. The results demonstrate that our method not only discovers interesting types of LSC that have been overlooked in previous studies but also effectively searches for words with specific types of LSC.

</details>


### [43] [Where Are We At with Automatic Speech Recognition for the Bambara Language?](https://arxiv.org/abs/2602.09785)
*Seydou Diallo,Yacouba Diarra,Mamadou K. Keita,Panga Azazia Kamaté,Adam Bouno Kampo,Aboubacar Ouattara*

Main category: cs.CL

TL;DR: 本文构建了首个Bambara语ASR标准评测基准，测试发现当前模型性能不足以实用，表明需深入研究专门方法提升该语言的语音识别技术。


<details>
  <summary>Details</summary>
Motivation: 针对Bambara语自动语音识别(ASR)缺少标准化评测基准，迫切需要建立统一的评测标准推动该领域发展。

Method: 利用一小时专业录制的马里宪法文本，构建了一个标准化的Bambara语ASR评测基准，并在接近最佳音频和语言条件下进行控制测试。

Result: 评测了37个模型，包括专门训练的Bambara模型和大型商业多语言模型，结果显示当前ASR性能仍显著低于实用水平，最佳WER为46.76%，最佳CER为13.00%，部分多语言模型WER甚至超过100%。

Conclusion: 仅靠多语言预训练和模型规模扩大无法满足资源匮乏语言的ASR需求。该基准提供了透明的性能评价手段，为未来Bambara语语音技术研究提供支持。

Abstract: This paper introduces the first standardized benchmark for evaluating Automatic Speech Recognition (ASR) in the Bambara language, utilizing one hour of professionally recorded Malian constitutional text. Designed as a controlled reference set under near-optimal acoustic and linguistic conditions, the benchmark was used to evaluate 37 models, ranging from Bambara-trained systems to large-scale commercial models. Our findings reveal that current ASR performance remains significantly below deployment standards in a narrow formal domain; the top-performing system in terms of Word Error Rate (WER) achieved 46.76\% and the best Character Error Rate (CER) of 13.00\% was set by another model, while several prominent multilingual models exceeded 100\% WER. These results suggest that multilingual pre-training and model scaling alone are insufficient for underrepresented languages. Furthermore, because this dataset represents a best-case scenario of the most simplified and formal form of spoken Bambara, these figures are yet to be tested against practical, real-world settings. We provide the benchmark and an accompanying public leaderboard to facilitate transparent evaluation and future research in Bambara speech technology.

</details>


### [44] [Decomposing Reasoning Efficiency in Large Language Models](https://arxiv.org/abs/2602.09805)
*Daniel Kaiser,Arnoldo Frigessi,Ali Ramezani-Kebrya,Benjamin Ricaud*

Main category: cs.CL

TL;DR: 本文提出一种框架细化评估大语言模型推理中的token使用效率，揭示准确率和token效率差异及其驱动因素，帮助指导模型优化。


<details>
  <summary>Details</summary>
Motivation: 当前大规模语言模型在推理任务中需要权衡推理所用的token数量和准确率，但标准评测仅报告最终准确率，无法揭示token的具体使用情况和浪费情况。

Method: 提出了一种可选追踪框架，将token效率分解为可解释的因素：固定token预算下的完成率、完成条件正确率和冗 verbosity，同时若有元数据，进一步分解冗述为平均表达冗余和与任务负载相关的耦合系数；结合推理追踪添加确定性的追踪质量指标以区分不同类型的token使用。

Result: 通过对25个模型在CogniLoad上的评估发现，准确率和token效率排名存在显著分歧，效率差异主要由条件正确率驱动，表达冗余差异可达9倍且与模型规模关系较弱，不同模型表现出不同的效率瓶颈。

Conclusion: 该方法通过精细分解token效率指标揭示模型推理中的不同瓶颈，为针对性提升token使用效率提供了依据，丰富了对大模型推理性能的理解。

Abstract: Large language models trained for reasoning trade off inference tokens against accuracy, yet standard evaluations report only final accuracy, obscuring where tokens are spent or wasted. We introduce a trace-optional framework that decomposes token efficiency into interpretable factors: completion under a fixed token budget (avoiding truncation), conditional correctness given completion, and verbosity (token usage). When benchmark metadata provides per-instance workload proxies, we further factor verbosity into two components: mean verbalization overhead (tokens per work unit) and a coupling coefficient capturing how overhead scales with task workload. When reasoning traces are available, we add deterministic trace-quality measures (grounding, repetition, prompt copying) to separate degenerate looping from verbose-but-engaged reasoning, avoiding human labeling and LLM judges. Evaluating 25 models on CogniLoad, we find that accuracy and token-efficiency rankings diverge (Spearman $ρ=0.63$), efficiency gaps are often driven by conditional correctness, and verbalization overhead varies by about 9 times (only weakly related to model scale). Our decomposition reveals distinct bottleneck profiles that suggest different efficiency interventions.

</details>


### [45] [AnalyticsGPT: An LLM Workflow for Scientometric Question Answering](https://arxiv.org/abs/2602.09817)
*Khang Ly,Georgios Cheirmpos,Adrian Raudaschl,Christopher James,Seyed Amin Tabatabaei*

Main category: cs.CL

TL;DR: 本文提出了基于大型语言模型的科学计量问答系统AnalyticsGPT，用多维数据检索和生成技术解决元科学问题，验证了其有效性并开源了代码。


<details>
  <summary>Details</summary>
Motivation: 科学计量问题回答是一项未被充分研究的下游任务，涉及对“科学的科学”的元科学性问题，具有独特的规划挑战。

Method: 本文提出了AnalyticsGPT，一种基于大型语言模型的端到端顺序工作流，结合检索增强生成和代理概念，利用专有的科研绩效评估平台进行多方面数据检索。

Result: 通过资深专家评审及LLM作为评判者的多维度评价，展示了大型语言模型在科学计量问答任务中的有效性和潜力。

Conclusion: AnalyticsGPT有效应对了科学计量问答中的多任务挑战，实现了高效直观的解决方案，推动了该小众领域的自动化分析发展。

Abstract: This paper introduces AnalyticsGPT, an intuitive and efficient large language model (LLM)-powered workflow for scientometric question answering. This underrepresented downstream task addresses the subcategory of meta-scientific questions concerning the "science of science." When compared to traditional scientific question answering based on papers, the task poses unique challenges in the planning phase. Namely, the need for named-entity recognition of academic entities within questions and multi-faceted data retrieval involving scientometric indices, e.g. impact factors. Beyond their exceptional capacity for treating traditional natural language processing tasks, LLMs have shown great potential in more complex applications, such as task decomposition and planning and reasoning. In this paper, we explore the application of LLMs to scientometric question answering, and describe an end-to-end system implementing a sequential workflow with retrieval-augmented generation and agentic concepts. We also address the secondary task of effectively synthesizing the data into presentable and well-structured high-level analyses. As a database for retrieval-augmented generation, we leverage a proprietary research performance assessment platform. For evaluation, we consult experienced subject matter experts and leverage LLMs-as-judges. In doing so, we provide valuable insights on the efficacy of LLMs towards a niche downstream task. Our (skeleton) code and prompts are available at: https://github.com/lyvykhang/llm-agents-scientometric-qa/tree/acl.

</details>


### [46] [Text summarization via global structure awareness](https://arxiv.org/abs/2602.09821)
*Jiaquan Zhang,Chaoning Zhang,Shuxu Chen,Yibei Liu,Chenghao Li,Qigan Sun,Shuai Yuan,Fachrina Dewi Puspitasari,Dongshen Han,Guoqing Wang,Sung-Ho Bae,Yang Yang*

Main category: cs.CL

TL;DR: 针对长文本摘要，GloSA-sum利用拓扑数据分析保持文本全局结构，提升摘要质量和效率，同时优化大模型应用表现。


<details>
  <summary>Details</summary>
Motivation: 现有文本摘要方法多关注模型改进和句子剪枝，忽视全文结构，导致摘要连贯性差且影响下游表现，同时大模型成本高，需寻求高效且结构意识强的方法。

Method: 基于句子嵌入构建语义加权图，利用持久同调识别核心语义和逻辑结构，设计拓扑引导迭代策略结合层级策略进行摘要生成。

Result: 实验表明GloSA-sum在多个数据集上减少摘要冗余，保持语义与逻辑完整性，实现准确性与效率的平衡，并促进大语言模型下游任务性能提升。

Conclusion: GloSA-sum方法通过拓扑数据分析实现了全局结构感知，有效保持了语义核心和逻辑依赖，兼顾了摘要的准确性与效率，提升了长文本处理和后续大模型任务表现。

Abstract: Text summarization is a fundamental task in natural language processing (NLP), and the information explosion has made long-document processing increasingly demanding, making summarization essential. Existing research mainly focuses on model improvements and sentence-level pruning, but often overlooks global structure, leading to disrupted coherence and weakened downstream performance. Some studies employ large language models (LLMs), which achieve higher accuracy but incur substantial resource and time costs. To address these issues, we introduce GloSA-sum, the first summarization approach that achieves global structure awareness via topological data analysis (TDA). GloSA-sum summarizes text efficiently while preserving semantic cores and logical dependencies. Specifically, we construct a semantic-weighted graph from sentence embeddings, where persistent homology identifies core semantics and logical structures, preserved in a ``protection pool'' as the backbone for summarization. We design a topology-guided iterative strategy, where lightweight proxy metrics approximate sentence importance to avoid repeated high-cost computations, thus preserving structural integrity while improving efficiency. To further enhance long-text processing, we propose a hierarchical strategy that integrates segment-level and global summarization. Experiments on multiple datasets demonstrate that GloSA-sum reduces redundancy while preserving semantic and logical integrity, striking a balance between accuracy and efficiency, and further benefits LLM downstream tasks by shortening contexts while retaining essential reasoning chains.

</details>


### [47] [From FusHa to Folk: Exploring Cross-Lingual Transfer in Arabic Language Models](https://arxiv.org/abs/2602.09826)
*Abdulmuizz Khalak,Abderrahmane Issam,Gerasimos Spanakis*

Main category: cs.CL

TL;DR: 阿拉伯语模型在MSA到不同方言的跨语言迁移上效果不均，地理接近性影响迁移效果，所有方言联合训练模型存在负干扰问题，挑战了方言相似性的假设。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语模型主要在标准现代书面阿拉伯语（MSA）上预训练，但实际应用中人们使用多种地域方言，且这些方言与MSA的相似度不一，模型迁移存在限制。

Method: 通过在三种自然语言处理任务中的探测和表征相似性分析，研究阿拉伯语模型的跨语言迁移能力。

Result: 发现阿拉伯语模型在不同方言间的迁移是可能的但存在差异，且地理接近度能部分解释迁移效果。训练支持所有方言的模型存在负面干扰问题。

Conclusion: 阿拉伯语语言模型在不同方言之间的迁移能力存在不均衡，部分原因与方言的地理接近度相关。所有方言共同训练的模型显示出负干扰现象，这引发了对方言间相似性及跨语言迁移效果的质疑。

Abstract: Arabic Language Models (LMs) are pretrained predominately on Modern Standard Arabic (MSA) and are expected to transfer to its dialects. While MSA as the standard written variety is commonly used in formal settings, people speak and write online in various dialects that are spread across the Arab region. This poses limitations for Arabic LMs, since its dialects vary in their similarity to MSA. In this work we study cross-lingual transfer of Arabic models using probing on 3 Natural Language Processing (NLP) Tasks, and representational similarity. Our results indicate that transfer is possible but disproportionate across dialects, which we find to be partially explained by their geographic proximity. Furthermore, we find evidence for negative interference in models trained to support all Arabic dialects. This questions their degree of similarity, and raises concerns for cross-lingual transfer in Arabic models.

</details>


### [48] [LLM Reasoning Predicts When Models Are Right: Evidence from Coding Classroom Discourse](https://arxiv.org/abs/2602.09832)
*Bakhtawar Ahtisham,Kirk Vanacore,Zhuqian Zhou,Jinsook Lee,Rene F. Kizilcec*

Main category: cs.CL

TL;DR: 本文通过分析大语言模型生成的推理，成功构建检测模型错误标签的方法，显著提升自动教育对话标注的可靠性与质量控制水平。


<details>
  <summary>Details</summary>
Motivation: 当前自动教育对话分析中大语言模型的标签准确性难以保证，缺乏有效检测错误的方法。

Method: 对3万多条教师话语进行多模型标签及推理分析，使用TF-IDF编码推理内容，训练多个监督分类器评估判断标签正确性的能力，其中随机森林表现最佳。

Result: 随机森林分类器达到F1 0.83，能成功识别大部分错误标签，且针对具体教学行为构建专门检测器提高效果。正确推理多使用因果语言，错误推理倾向语气模糊及元认知语言。句法复杂度和推理长度对准确性无显著影响。

Conclusion: 基于大语言模型生成的推理可以有效预测模型标签的正确性，从而提升自动教育对话分析的质量控制。

Abstract: Large Language Models (LLMs) are increasingly deployed to automatically label and analyze educational dialogue at scale, yet current pipelines lack reliable ways to detect when models are wrong. We investigate whether reasoning generated by LLMs can be used to predict the correctness of a model's own predictions. We analyze 30,300 teacher utterances from classroom dialogue, each labeled by multiple state-of-the-art LLMs with an instructional move construct and an accompanying reasoning. Using human-verified ground-truth labels, we frame the task as predicting whether a model's assigned label for a given utterance is correct. We encode LLM reasoning using Term Frequency-Inverse Document Frequency (TF-IDF) and evaluate five supervised classifiers. A Random Forest classifier achieves an F1 score of 0.83 (Recall = 0.854), successfully identifying most incorrect predictions and outperforming baselines. Training specialist detectors for specific instructional move constructs further improves performance on difficult constructs, indicating that error detection benefits from construct-specific linguistic cues. Using the Linguistic Inquiry and Word Count (LIWC) framework, we examine four linguistic markers of correctness: Causation, Differentiation, Tentativeness, and Insight. Correct predictions exhibit grounded causal language (e.g., because, therefore), while incorrect reasoning is substantially more likely to rely on epistemic hedging (e.g., might, could) and performative metacognition (e.g., think, realize). Syntactic complexity does not distinguish correct from incorrect reasoning, and longer reasoning is not more reliable. These findings demonstrate that reasoning-based error detection offers a practical and scalable approach to quality control in automated educational dialogue analysis.

</details>


### [49] [How Do People Quantify Naturally: Evidence from Mandarin Picture Description](https://arxiv.org/abs/2602.09838)
*Yayun Zhang,Guanyi Chen,Fahime Same,Saad Mahamood,Tingting He*

Main category: cs.CL

TL;DR: 本研究通过自然表达任务探讨中文说话者如何在无明确指令下选择和使用量化，发现对象数量、生命性和表达模态共同影响量化行为和策略选择。


<details>
  <summary>Details</summary>
Motivation: 虽然量化是日常语言的重要组成，但对说话者如何决定何时以及如何进行量化的自然表达过程知之甚少，因此本研究旨在揭示这一机制。

Method: 采用基于图片的诱导描述任务，让说话者自由描述包含多个对象的场景，无需明确计数指令，通过观察口语和书面两种表达模态中的量化使用情况。

Result: 对象数量、生命性和表达模态显著影响量化行为，数量增加降低量化的发生率与精确度，生命性对象和不同表达方式则影响量化策略的采用。

Conclusion: 说话者在自然语言表达中对量化的选择和策略受到对象数量、生命性及表达方式的影响，特别是数量增加时量化倾向和精确度下降，而生命性和表达模态影响量词策略的选择。

Abstract: Quantification is a fundamental component of everyday language use, yet little is known about how speakers decide whether and how to quantify in naturalistic production. We investigate quantification in Mandarin Chinese using a picture-based elicited description task in which speakers freely described scenes containing multiple objects, without explicit instructions to count or quantify. Across both spoken and written modalities, we examine three aspects of quantification: whether speakers choose to quantify at all, how precise their quantification is, and which quantificational strategies they adopt. Results show that object numerosity, animacy, and production modality systematically shape quantificational behaviour. In particular, increasing numerosity reduces both the likelihood and the precision of quantification, while animate referents and modality selectively modulate strategy choice. This study demonstrates how quantification can be examined under unconstrained production conditions and provides a naturalistic dataset for further analyses of quantity expression in language production.

</details>


### [50] [SinFoS: A Parallel Dataset for Translating Sinhala Figures of Speech](https://arxiv.org/abs/2602.09866)
*Johan Sofalas,Dilushri Pavithra,Nevidu Jayatilleke,Ruvan Weerasinghe*

Main category: cs.CL

TL;DR: 本文构建并公开了一个含文化和跨语言注释的僧伽罗语修辞手法语料库，开发高准确率分类器，并评测现有语言模型，发现其在习语理解上存在明显不足，推动低资源语言机器翻译研究。


<details>
  <summary>Details</summary>
Motivation: 低资源语言如僧伽罗语中修辞手法的神经机器翻译表现较差，因数据不足导致模型难以准确翻译文化深厚的短语，亟需专门数据集和评测基准推动相关研究。

Method: 构建包含2344条僧伽罗语修辞手法的多注释语料库，进行文化起源分类和跨语言等价物识别，开发了二分类器用于区分修辞手法类型，并评估现有大型语言模型的表现。

Result: 开发了高准确率（约92%）的二分类器，有效区分修辞手法类型；揭示现有大型语言模型在准确处理习语方面的不足；公开了珍贵的僧伽罗语修辞手法语料库以促进后续研究。

Conclusion: 本论文构建了一个包含2344条僧伽罗语修辞手法的语料库，包含文化和跨语言注释，并通过二分类器区分两类修辞手法，准确率达92%。评测了现有大型语言模型在该数据集上的表现，发现模型在准确传达习语意义方面存在明显不足。本工作为低资源语言的NLP和文化感知机器翻译提供了重要基准。

Abstract: Figures of Speech (FoS) consist of multi-word phrases that are deeply intertwined with culture. While Neural Machine Translation (NMT) performs relatively well with the figurative expressions of high-resource languages, it often faces challenges when dealing with low-resource languages like Sinhala due to limited available data. To address this limitation, we introduce a corpus of 2,344 Sinhala figures of speech with cultural and cross-lingual annotations. We examine this dataset to classify the cultural origins of the figures of speech and to identify their cross-lingual equivalents. Additionally, we have developed a binary classifier to differentiate between two types of FOS in the dataset, achieving an accuracy rate of approximately 92%. We also evaluate the performance of existing LLMs on this dataset. Our findings reveal significant shortcomings in the current capabilities of LLMs, as these models often struggle to accurately convey idiomatic meanings. By making this dataset publicly available, we offer a crucial benchmark for future research in low-resource NLP and culturally aware machine translation.

</details>


### [51] [Steer2Edit: From Activation Steering to Component-Level Editing](https://arxiv.org/abs/2602.09870)
*Chung-En Sun,Ge Yan,Zimo Wang,Tsui-Wei Weng*

Main category: cs.CL

TL;DR: Steer2Edit创新性地将方向向量转为精细化权重编辑，无需训练即可优化模型行为，提升安全性、真实性及推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有的推理时激活干预方法虽有效，但普适性的全局修改导致在强控制下产生不利的属性-效用权衡，忽略了模型行为由少数异质组件主控的事实，故需更细粒度的调整方法。

Method: 提出Steer2Edit框架，将推理时的方向向量转变为针对注意力头和MLP神经元的秩一权重编辑，选择性地调整模型内部组件的行为影响，保持前向计算不变，支持并行推理优化。

Result: Steer2Edit在安全性对齐、幻觉减少和推理效率方面均优于传统方法，匹配下游任务表现时，安全性提升最高17.2%，真实性增加9.8%，推理长度缩短12.2%。

Conclusion: Steer2Edit方法通过将推理时的控制信号转换为针对模型各组件的可解释性权重编辑，实现了行为调整的高效且无需训练的优化，显著提升了属性-效用的权衡表现。

Abstract: Steering methods influence Large Language Model behavior by identifying semantic directions in hidden representations, but are typically realized through inference-time activation interventions that apply a fixed, global modification to the model's internal states. While effective, such interventions often induce unfavorable attribute-utility trade-offs under strong control, as they ignore the fact that many behaviors are governed by a small and heterogeneous subset of model components. We propose Steer2Edit, a theoretically grounded, training-free framework that transforms steering vectors from inference-time control signals into diagnostic signals for component-level rank-1 weight editing. Instead of uniformly injecting a steering direction during generation, Steer2Edit selectively redistributes behavioral influence across individual attention heads and MLP neurons, yielding interpretable edits that preserve the standard forward pass and remain compatible with optimized parallel inference. Across safety alignment, hallucination mitigation, and reasoning efficiency, Steer2Edit consistently achieves more favorable attribute-utility trade-offs: at matched downstream performance, it improves safety by up to 17.2%, increases truthfulness by 9.8%, and reduces reasoning length by 12.2% on average. Overall, Steer2Edit provides a principled bridge between representation steering and weight editing by translating steering signals into interpretable, training-free parameter updates.

</details>


### [52] [The Devil Behind Moltbook: Anthropic Safety is Always Vanishing in Self-Evolving AI Societies](https://arxiv.org/abs/2602.09877)
*Chenxu Wang,Chaozhuo Li,Songyang Liu,Zejian Chen,Jinyu Hou,Ji Qi,Rui Li,Litian Zhang,Qiwei Ye,Zheng Liu,Xu Chen,Xi Zhang,Philip S. Yu*

Main category: cs.CL

TL;DR: 研究揭示多智能体自我进化系统存在安全性三难困境，孤立环境中的持续自我进化必然导致安全衰减，强调需外部监督或新机制保障安全。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统因其可扩展的集体智能能力而受到关注，但如何在持续自我改进同时保证安全性是关键挑战。

Method: 通过信息论框架定义安全为与人类价值分布的偏差，理论分析孤立自我进化导致安全退化，并通过实验验证。

Result: 理论证明孤立自我进化引发安全性不可逆退化，实验证实该安全侵蚀现象存在，并提出缓解方向。

Conclusion: 完全封闭且持续自我进化的多智能体系统中，实现安全一致性是不可能的。

Abstract: The emergence of multi-agent systems built from large language models (LLMs) offers a promising paradigm for scalable collective intelligence and self-evolution. Ideally, such systems would achieve continuous self-improvement in a fully closed loop while maintaining robust safety alignment--a combination we term the self-evolution trilemma. However, we demonstrate both theoretically and empirically that an agent society satisfying continuous self-evolution, complete isolation, and safety invariance is impossible. Drawing on an information-theoretic framework, we formalize safety as the divergence degree from anthropic value distributions. We theoretically demonstrate that isolated self-evolution induces statistical blind spots, leading to the irreversible degradation of the system's safety alignment. Empirical and qualitative results from an open-ended agent community (Moltbook) and two closed self-evolving systems reveal phenomena that align with our theoretical prediction of inevitable safety erosion. We further propose several solution directions to alleviate the identified safety concern. Our work establishes a fundamental limit on the self-evolving AI societies and shifts the discourse from symptom-driven safety patches to a principled understanding of intrinsic dynamical risks, highlighting the need for external oversight or novel safety-preserving mechanisms.

</details>


### [53] [AmharicIR+Instr: A Two-Dataset Resource for Neural Retrieval and Instruction Tuning](https://arxiv.org/abs/2602.09914)
*Tilahun Yeshambel,Moncef Garouani,Josiane Mothe*

Main category: cs.CL

TL;DR: 针对阿姆哈拉语数据稀缺问题，本文发布了两个高质量神经检索和文本生成数据集，助力低资源语言的AI模型发展。


<details>
  <summary>Details</summary>
Motivation: 阿姆哈拉语等低资源语言缺乏大量高质量的监督数据，限制了神经检索和生成模型的发展。

Method: 通过专家精心设计的查询、网络采集查询和大语言模型辅助生成，结合本地母语者验证，构建了包含查询-文档三元组和指令-响应对的数据集，并标准化数据格式和切分。

Result: 构建了1,091个有监督三元组的神经检索数据集和6,285条多领域指令-响应文本生成数据，数据经人工校验，支持对阿姆哈拉语检索生成任务的研究与复现。

Conclusion: 本文发布了两个支持阿姆哈拉语神经检索和生成任务的数据集，为低资源语言的研究提供了宝贵资源。

Abstract: Neural retrieval and GPT-style generative models rely on large, high-quality supervised data, which is still scarce for low-resource languages such as Amharic. We release an Amharic data resource consisting of two datasets that supports research on (i) neural retrieval-ranking and (ii) instruction-following text generation. The retrieval-ranking dataset contains 1,091 manually verified query-positive-negative document triplets drawn from diverse Amharic sources and constructed to support contrastive training and benchmarking of neural retrievers (e.g., DPR, ColBERT-style late interaction and SPLADE-style sparse neural retrieval). Triplets are created through a combination of expert-curated queries, web-derived queries, and LLM-assisted generation, with positive/negative documents selected from the web or synthesized by LLMs and then validated by native speakers. The instruction prompt-response dataset comprises 6,285 Amharic prompt-response pairs spanning multiple domains and instruction types, generated with several LLMs and refined through manual review and correction for grammaticality, relevance, fluency, and factual plausibility. We release both datasets with standardized splits and formats (CSV,JSON,JSONL) to enable reproducible work on Amharic retrieval, ranking, and generative modelling. These datasets also come with a methodology that can be generalized to other low-resource languages.

</details>


### [54] [LLMs Encode Their Failures: Predicting Success from Pre-Generation Activations](https://arxiv.org/abs/2602.09924)
*William Lugoloobi,Thomas Foster,William Bankes,Chris Russell*

Main category: cs.CL

TL;DR: 本文提出利用LLM生成前内部激活预测任务成功概率，以此指导计算资源分配，显著提升推理效率并降低成本。


<details>
  <summary>Details</summary>
Motivation: LLMs在每个问题上进行扩展推理计算成本高，如何判断哪些输入需要额外计算仍具挑战。

Method: 通过在生成前的内部激活上训练线性探针，预测模型特定任务的成功概率，从而指导更高效的推理。

Result: 模型内部编码了与人类不同的难度认知，使用探针进行模型间查询路由，可在MATH任务上减少推理成本70%且性能超越最佳模型。

Conclusion: 内部表征能恢复模型成功概率，有助于实现高效推理，提升计算效率，即使其难度评估与人类不同。

Abstract: Running LLMs with extended reasoning on every problem is expensive, but determining which inputs actually require additional compute remains challenging. We investigate whether their own likelihood of success is recoverable from their internal representations before generation, and if this signal can guide more efficient inference. We train linear probes on pre-generation activations to predict policy-specific success on math and coding tasks, substantially outperforming surface features such as question length and TF-IDF. Using E2H-AMC, which provides both human and model performance on identical problems, we show that models encode a model-specific notion of difficulty that is distinct from human difficulty, and that this distinction increases with extended reasoning. Leveraging these probes, we demonstrate that routing queries across a pool of models can exceed the best-performing model whilst reducing inference cost by up to 70\% on MATH, showing that internal representations enable practical efficiency gains even when they diverge from human intuitions about difficulty. Our code is available at: https://github.com/KabakaWilliam/llms_know_difficulty

</details>


### [55] [ATTNPO: Attention-Guided Process Supervision for Efficient Reasoning](https://arxiv.org/abs/2602.09953)
*Shuaiyi Nie,Siyu Ding,Wenyuan Zhang,Linhao Yu,Tianmeng Yang,Yao Chen,Tingwen Liu,Weichong Yin,Yu Sun,Hua Wu*

Main category: cs.CL

TL;DR: 本文提出ATTNPO，基于注意力信号逐步惩罚冗余推理，显著减少推理步骤并提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型虽然表现强劲，但存在过度思考、生成冗余推理步骤的问题，现有方法处理不当导致性能下降。

Method: 提出ATTNPO框架，利用模型内在注意力信号进行逐步信用分配，通过识别关注必要步骤的注意力头，抑制冗余步骤，保护重要步骤，控制推理长度。

Result: ATTNPO在9个基准测试中显著减少推理长度，同时显著提升性能。

Conclusion: ATTNPO有效解决了过度思考问题，实现了推理过程的高效且准确。

Abstract: Large reasoning models trained with reinforcement learning and verifiable rewards (RLVR) achieve strong performance on complex reasoning tasks, yet often overthink, generating redundant reasoning without performance gains. Existing trajectory-level length penalties often fail to effectively shorten reasoning length and degrade accuracy, as they uniformly treat all reasoning steps and lack fine-grained signals to distinguish redundancy from necessity. Meanwhile, process-supervised methods are typically resource-intensive and suffer from inaccurate credit assignment. To address these issues, we propose ATTNPO, a low-overhead process-supervised RL framework that leverages the model's intrinsic attention signals for step-level credit assignment. We first identify a set of special attention heads that naturally focus on essential steps while suppressing redundant ones. By leveraging the attention scores of these heads, We then employ two sub-strategies to mitigate overthinking by discouraging redundant steps while preserving accuracy by reducing penalties on essential steps. Experimental results show that ATTNPO substantially reduces reasoning length while significantly improving performance across 9 benchmarks.

</details>


### [56] [ViMultiChoice: Toward a Method That Gives Explanation for Multiple-Choice Reading Comprehension in Vietnamese](https://arxiv.org/abs/2602.09961)
*Trung Tien Cao,Lam Minh Thai,Nghia Hieu Nguyen,Duc-Vu Nguyen,Ngan Luu-Thuy Nguyen*

Main category: cs.CL

TL;DR: 本文设计了一个越南语多项选择阅读理解数据集及联合答案和解释生成的ViMultiChoice模型，实现了领先的准确率和解释能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多项选择阅读理解模型缺乏解释其选择理由的能力，因此驱动本文构建能生成解释的新数据集和模型。

Method: 本文引入一个新的越南语数据集用于训练和评估多项选择阅读理解模型的解释生成能力，并提出了ViMultiChoice方法，联合预测正确答案和生成解释。

Result: 实验结果表明，ViMultiChoice在ViMMRC 2.0基准和新数据集上均超越现有基线，联合训练策略显著提升了多项选择准确率。

Conclusion: 本文提出的ViMultiChoice模型在越南语多项选择阅读理解任务中实现了先进的性能，同时能够生成解释，提高了模型的可解释性和准确性。

Abstract: Multiple-choice Reading Comprehension (MCRC) models aim to select the correct answer from a set of candidate options for a given question. However, they typically lack the ability to explain the reasoning behind their choices. In this paper, we introduce a novel Vietnamese dataset designed to train and evaluate MCRC models with explanation generation capabilities. Furthermore, we propose ViMultiChoice, a new method specifically designed for modeling Vietnamese reading comprehension that jointly predicts the correct answer and generates a corresponding explanation. Experimental results demonstrate that ViMultiChoice outperforms existing MCRC baselines, achieving state-of-the-art (SotA) performance on both the ViMMRC 2.0 benchmark and the newly introduced dataset. Additionally, we show that jointly training option decision and explanation generation leads to significant improvements in multiple-choice accuracy.

</details>


### [57] [A Unified Assessment of the Poverty of the Stimulus Argument for Neural Language Models](https://arxiv.org/abs/2602.09992)
*Xiulin Yang,Arianna Bisazza,Nathan Schneider,Ethan Gotlieb Wilcox*

Main category: cs.CL

TL;DR: 该研究用神经语言模型测试贫困刺激假设，发现模型能部分泛化但不及儿童，表明先天语言能力非必需，但需更多认知偏置以提高数据效率。


<details>
  <summary>Details</summary>
Motivation: 研究语言学习中的贫困刺激假设，探讨儿童如何从有限输入中习得母语水平的句法。

Method: 引入poshbench训练和评估套件，训练Transformer模型在有限的英文语料上测试句法现象，并引入认知动机的归纳偏置提升模型表现。

Result: 模型在无直接正向证据情况下表现出一定泛化能力，但数据效率和泛化能力不及儿童。认知偏置提升句法能力，但未改善poshbench表现。

Conclusion: 先天句法并非唯一泛化路径，但实现类人数据效率需更多归纳偏置。

Abstract: How can children acquire native-level syntax from limited input? According to the Poverty of the Stimulus Hypothesis (PoSH), the linguistic input children receive is insufficient to explain certain generalizations that are robustly learned; innate linguistic constraints, many have argued, are thus necessary to explain language learning. Neural language models, which lack such language-specific constraints in their design, offer a computational test of this longstanding (but controversial) claim. We introduce \poshbench, a training-and-evaluation suite targeting question formation, islands to movement, and other English phenomena at the center of the PoSH arguments. Training Transformer models on 10--50M words of developmentally plausible text, we find indications of generalization on all phenomena even without direct positive evidence -- yet neural models remain less data-efficient and their generalizations are weaker than those of children. We further enhance our models with three recently proposed cognitively motivated inductive biases. We find these biases improve general syntactic competence but not \poshbench performance. Our findings challenge the claim that innate syntax is the only possible route to generalization, while suggesting that human-like data efficiency requires inductive biases beyond those tested here.

</details>


### [58] [ViSpeechFormer: A Phonemic Approach for Vietnamese Automatic Speech Recognition](https://arxiv.org/abs/2602.10003)
*Khoa Anh Nguyen,Long Minh Hoang,Nghia Hieu Nguyen,Luan Thanh Nguyen,Ngan Luu-Thuy Nguyen*

Main category: cs.CL

TL;DR: 本文提出首个基于音素的越南语ASR模型ViSpeechFormer，充分利用越南语拼音透明性，提升了识别效果和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 越南语的字母与音素之间关系高度透明，现有越南语ASR系统尚未明确建模音素表示，利用音素可提升识别性能和泛化能力。

Method: 提出基于音素的ViSpeechFormer模型，利用越南语的音素-字母高度透明的拼音文字特点，采用变换器结构进行语音识别。

Result: 在两个公开越南语ASR数据集上，ViSpeechFormer取得了强劲的性能表现，对未登录词泛化能力更强，且训练偏差影响较小。

Conclusion: ViSpeechFormer作为首个明确建模音素表示的越南语自动语音识别框架，表现出优异的性能和良好的泛化能力，尤其在处理未登录词和减轻训练偏差方面效果显著。

Abstract: Vietnamese has a phonetic orthography, where each grapheme corresponds to at most one phoneme and vice versa. Exploiting this high grapheme-phoneme transparency, we propose ViSpeechFormer (\textbf{Vi}etnamese \textbf{Speech} Trans\textbf{Former}), a phoneme-based approach for Vietnamese Automatic Speech Recognition (ASR). To the best of our knowledge, this is the first Vietnamese ASR framework that explicitly models phonemic representations. Experiments on two publicly available Vietnamese ASR datasets show that ViSpeechFormer achieves strong performance, generalizes better to out-of-vocabulary words, and is less affected by training bias. This phoneme-based paradigm is also promising for other languages with phonetic orthographies. The code will be released upon acceptance of this paper.

</details>


### [59] [SCORE: Specificity, Context Utilization, Robustness, and Relevance for Reference-Free LLM Evaluation](https://arxiv.org/abs/2602.10017)
*Homaira Huda Shomee,Rochana Chaturvedi,Yangxinyu Xie,Tanwi Mallick*

Main category: cs.CL

TL;DR: 针对高风险领域中大语言模型问答的细粒度需求，本文提出了一种多维度无参考评价框架及专用数据集，验证了多指标评价的重要性，推动模型在专业领域的可靠应用。


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成(RAG)和开放式问答评价方法多依赖表层相似性、事实一致性或语义相关性，不能有效评估是否提供了领域敏感决策所需的细粒度关键信息，难以满足高风险领域决策支持的需求。

Method: 提出了一种多维度、无参考的评价框架，涵盖特异性、对释义和语义扰动的鲁棒性、答案相关性及上下文利用，并构建了涵盖40种专业角色及7种自然灾害类型的1,412条问答对数据集，进一步进行了人工评估以验证评价一致性与人类判断的对齐。

Result: 通过实验与人工评估发现，没有单一指标能充分衡量回答质量，揭示了开放式领域特定评价本身的主观性，并证明了多指标综合评价框架的必要性。

Conclusion: 单一评价指标无法全面衡量大语言模型(LLMs)在领域特定高风险应用中回答质量，需采用结构化、多指标的综合评价框架。

Abstract: Large language models (LLMs) are increasingly used to support question answering and decision-making in high-stakes, domain-specific settings such as natural hazard response and infrastructure planning, where effective answers must convey fine-grained, decision-critical details. However, existing evaluation frameworks for retrieval-augmented generation (RAG) and open-ended question answering primarily rely on surface-level similarity, factual consistency, or semantic relevance, and often fail to assess whether responses provide the specific information required for domain-sensitive decisions. To address this gap, we propose a multi-dimensional, reference-free evaluation framework that assesses LLM outputs along four complementary dimensions: specificity, robustness to paraphrasing and semantic perturbations, answer relevance, and context utilization. We introduce a curated dataset of 1,412 domain-specific question-answer pairs spanning 40 professional roles and seven natural hazard types to support systematic evaluation. We further conduct human evaluation to assess inter-annotator agreement and alignment between model outputs and human judgments, which highlights the inherent subjectivity of open-ended, domain-specific evaluation. Our results show that no single metric sufficiently captures answer quality in isolation and demonstrate the need for structured, multi-metric evaluation frameworks when deploying LLMs in high-stakes applications.

</details>


### [60] [Decoupled Reasoning with Implicit Fact Tokens (DRIFT): A Dual-Model Framework for Efficient Long-Context Inference](https://arxiv.org/abs/2602.10021)
*Wenxuan Xie,Yujia Wang,Xin Tan,Chaochao Lu,Xia Hu,Xuhong Wang*

Main category: cs.CL

TL;DR: 本文提出了 DRIFT，一种通过双模型架构实现知识提取与推理解耦的方法，有效提升了大型语言模型在长上下文任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在整合大量动态知识时面临事实数据与推理模式纠缠的挑战，现有方法受限于上下文窗口大小、检索噪声或灾难性遗忘，难以有效整合广泛知识。

Method: 提出了 DRIFT，一种利用轻量级知识模型将文档内容动态压缩为隐式事实标记，并将其映射到推理模型的嵌入空间，从而替代冗余文本，实现知识提取与推理的分离。

Result: DRIFT 在长文本任务上显著优于同等规模的基线模型，提升了推理准确性和效率，实现了扩展上下文窗口的目标。

Conclusion: DRIFT 方法通过双模型架构有效解耦了知识提取与推理过程，显著提升了长上下文任务的性能，扩展了大型语言模型的有效上下文窗口和推理能力。

Abstract: The integration of extensive, dynamic knowledge into Large Language Models (LLMs) remains a significant challenge due to the inherent entanglement of factual data and reasoning patterns. Existing solutions, ranging from non-parametric Retrieval-Augmented Generation (RAG) to parametric knowledge editing, are often constrained in practice by finite context windows, retriever noise, or the risk of catastrophic forgetting. In this paper, we propose DRIFT, a novel dual-model architecture designed to explicitly decouple knowledge extraction from the reasoning process. Unlike static prompt compression, DRIFT employs a lightweight knowledge model to dynamically compress document chunks into implicit fact tokens conditioned on the query. These dense representations are projected into the reasoning model's embedding space, replacing raw, redundant text while maintaining inference accuracy. Extensive experiments show that DRIFT significantly improves performance on long-context tasks, outperforming strong baselines among comparably sized models. Our approach provides a scalable and efficient paradigm for extending the effective context window and reasoning capabilities of LLMs. Our code is available at https://github.com/Lancelot-Xie/DRIFT.

</details>


### [61] [MEVER: Multi-Modal and Explainable Claim Verification with Graph-based Evidence Retrieval](https://arxiv.org/abs/2602.10023)
*Delvin Ce Zhang,Suhan Cui,Zhelin Chu,Xianren Zhang,Dongwon Lee*

Main category: cs.CL

TL;DR: 本文提出了一种结合多模态证据检索、验证和解释生成的模型，解决了现有方法在多模态推理和解释性方面的不足，并发布了AI领域数据集以促进研究。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注文本证据推理或忽视解释性，导致验证结果不准确且缺乏说服力。

Method: 该模型构建了一个两层多模态图，实现图像到文本和文本到图像的多模态推理，采用标记级和证据级融合进行验证，并引入多模态Fusion-in-Decoder生成解释。

Result: 实验验证了该模型在多模态声明验证和解释生成方面的有效性，并提供了AI领域的科学数据集AIChartClaim。

Conclusion: 该论文提出的模型能够有效实现证据检索、多模态声明验证和解释生成，提高了验证的准确性和可解释性。

Abstract: Verifying the truthfulness of claims usually requires joint multi-modal reasoning over both textual and visual evidence, such as analyzing both textual caption and chart image for claim verification. In addition, to make the reasoning process transparent, a textual explanation is necessary to justify the verification result. However, most claim verification works mainly focus on the reasoning over textual evidence only or ignore the explainability, resulting in inaccurate and unconvincing verification. To address this problem, we propose a novel model that jointly achieves evidence retrieval, multi-modal claim verification, and explanation generation. For evidence retrieval, we construct a two-layer multi-modal graph for claims and evidence, where we design image-to-text and text-to-image reasoning for multi-modal retrieval. For claim verification, we propose token- and evidence-level fusion to integrate claim and evidence embeddings for multi-modal verification. For explanation generation, we introduce multi-modal Fusion-in-Decoder for explainability. Finally, since almost all the datasets are in general domain, we create a scientific dataset, AIChartClaim, in AI domain to complement claim verification community. Experiments show the strength of our model.

</details>


### [62] [Anagent For Enhancing Scientific Table & Figure Analysis](https://arxiv.org/abs/2602.10081)
*Xuehang Guo,Zhiyong Lu,Tom Hope,Qingyun Wang*

Main category: cs.CL

TL;DR: 本文针对科学表格和图形分析难题，提出Anagent多智能体框架，并发布大规模基准AnaBench，取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统难以有效分析科学表格与图形，因其结构复杂、跨领域且需要长上下文推理，因此需建立基准和新方法以提升科学多模态知识的解析能力。

Method: 提出了Anagent框架，包括Planner、Expert、Solver和Critic四个专业智能体，并采用模块化训练策略结合监督微调与强化学习优化智能体能力和协作。

Result: Anagent在170个子领域表现出色，训练免费模式提升13.43%，微调后提升42.12%，证明其方法有效且优于现有技术。

Conclusion: 本论文提出了Anagent多智能体框架，显著提升了科学表格和图形分析的性能，验证了任务导向推理和上下文感知问题解决能力的重要性。

Abstract: In scientific research, analysis requires accurately interpreting complex multimodal knowledge, integrating evidence from different sources, and drawing inferences grounded in domain-specific knowledge. However, current artificial intelligence (AI) systems struggle to consistently demonstrate such capabilities. The complexity and variability of scientific tables and figures, combined with heterogeneous structures and long-context requirements, pose fundamental obstacles to scientific table \& figure analysis. To quantify these challenges, we introduce AnaBench, a large-scale benchmark featuring $63,178$ instances from nine scientific domains, systematically categorized along seven complexity dimensions. To tackle these challenges, we propose Anagent, a multi-agent framework for enhanced scientific table \& figure analysis through four specialized agents: Planner decomposes tasks into actionable subtasks, Expert retrieves task-specific information through targeted tool execution, Solver synthesizes information to generate coherent analysis, and Critic performs iterative refinement through five-dimensional quality assessment. We further develop modular training strategies that leverage supervised finetuning and specialized reinforcement learning to optimize individual capabilities while maintaining effective collaboration. Comprehensive evaluation across 170 subdomains demonstrates that Anagent achieves substantial improvements, up to $\uparrow 13.43\%$ in training-free settings and $\uparrow 42.12\%$ with finetuning, while revealing that task-oriented reasoning and context-aware problem-solving are essential for high-quality scientific table \& figure analysis. Our project page: https://xhguo7.github.io/Anagent/.

</details>


### [63] [Quantum-Audit: Evaluating the Reasoning Limits of LLMs on Quantum Computing](https://arxiv.org/abs/2602.10092)
*Mohamed Afane,Kayla Laufer,Wenqi Wei,Ying Mao,Junaid Farooq,Ying Wang,Juntao Chen*

Main category: cs.CL

TL;DR: Quantum-Audit测试了26个语言模型的量子计算概念理解，发现即使顶尖模型在高级话题及纠正错误前提方面仍有较大提升空间。


<details>
  <summary>Details</summary>
Motivation: 现有基准多侧重量子代码生成和电路设计，缺乏系统测量模型对量子计算概念理解能力的评估。

Method: 设计了涵盖2700个核心量子计算问题的Quantum-Audit基准测试，包含专家题目、论文提取题目及纠错题，评估了26个主流模型的表现。

Result: 专家平均表现74%，顶级模型超越专家平均，最佳模型Claude Opus 4.5达84%，但在专家出题及高级话题的表现下降，纠正错误前提能力不足。

Conclusion: 大型语言模型在量子计算知识理解方面表现不均，尤其在高级话题和纠正错误前提方面存在明显不足。

Abstract: Language models have become practical tools for quantum computing education and research, from summarizing technical papers to explaining theoretical concepts and answering questions about recent developments in the field. While existing benchmarks evaluate quantum code generation and circuit design, their understanding of quantum computing concepts has not been systematically measured. Quantum-Audit addresses this gap with 2,700 questions covering core quantum computing topics. We evaluate 26 models from leading organizations. Our benchmark comprises 1,000 expert-written questions, 1,000 questions extracted from research papers using LLMs and validated by experts, plus an additional 700 questions including 350 open-ended questions and 350 questions with false premises to test whether models can correct erroneous assumptions. Human participants scored between 23% and 86%, with experts averaging 74%. Top-performing models exceeded the expert average, with Claude Opus 4.5 reaching 84% accuracy, though top models showed an average 12-point accuracy drop on expert-written questions compared to LLM-generated ones. Performance declined further on advanced topics, dropping to 73% on security questions. Additionally, models frequently accepted and reinforced false premises embedded in questions instead of identifying them, with accuracy below 66% on these critical reasoning tasks.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [64] [RuleFlow : Generating Reusable Program Optimizations with LLMs](https://arxiv.org/abs/2602.09051)
*Avaljot Singh,Dushyant Bharadwaj,Stefanos Baziotis,Kaushik Varadharajan,Charith Mendis*

Main category: cs.SE

TL;DR: RuleFlow通过三阶段混合优化策略，实现了对Pandas程序的高效优化，远超现有方法性能。


<details>
  <summary>Details</summary>
Motivation: 优化Pandas程序具有挑战性，现有方法要么重且支持优化有限，要么基于大型语言模型（LLMs）但不可靠且成本高。

Method: 提出三阶段混合方法：第一阶段发现每个程序的优化；第二阶段将优化转化为通用重写规则；第三阶段在编译器中自动应用规则，减少对LLMs的依赖。

Result: RuleFlow在PandasBench基准测试中达到新的优化最先进水平，速度较之前的编译器和系统方法分别提升4.3倍和1914.9倍。

Conclusion: 该混合方法有效结合了LLMs的创新能力与编译器的自动化应用，显著提升了Pandas程序的优化效率和性能。

Abstract: Optimizing Pandas programs is a challenging problem. Existing systems and compiler-based approaches offer reliability but are either heavyweight or support only a limited set of optimizations. Conversely, using LLMs in a per-program optimization methodology can synthesize nontrivial optimizations, but is unreliable, expensive, and offers a low yield. In this work, we introduce a hybrid approach that works in a 3-stage manner that decouples discovery from deployment and connects them via a novel bridge. First, it discovers per-program optimizations (discovery). Second, they are converted into generalised rewrite rules (bridge). Finally, these rules are incorporated into a compiler that can automatically apply them wherever applicable, eliminating repeated reliance on LLMs (deployment). We demonstrate that RuleFlow is the new state-of-the-art (SOTA) Pandas optimization framework on PandasBench, a challenging Pandas benchmark consisting of Python notebooks. Across these notebooks, we achieve a speedup of up to 4.3x over Dias, the previous compiler-based SOTA, and 1914.9x over Modin, the previous systems-based SOTA.
  Our code is available at https://github.com/ADAPT-uiuc/RuleFlow.

</details>


### [65] [Predicting Open Source Software Sustainability with Deep Temporal Neural Hierarchical Architectures and Explainable AI](https://arxiv.org/abs/2602.09064)
*S M Rakib Ul Karim,Wenyi Lu,Enock Kasaadha,Sean Goggins*

Main category: cs.SE

TL;DR: 本文提出一种层级预测模型，结合多维动态特征，准确识别开源软件项目的生命周期阶段，提升了对项目可持续性的理解和评估能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究多依赖静态或累计指标，缺乏对开源软件项目生命周期中可持续性动态演变的深入理解。

Method: 构建了一个结合工程化表格指标与24个月时间序列活动的多阶段分类模型，采用可解释性AI技术分析特征对预测结果的贡献。

Result: 在大量开源代码库上进行评估，所提模型实现了超过94%的生命周期阶段分类准确率，并发现贡献活动与社区参与特征为主要预测信号。

Conclusion: 提出的层级预测框架能够准确识别开源软件项目不同的生命周期阶段，更全面地刻画了项目的可持续性，揭示了贡献活动与社区参与在项目健康中的核心作用。

Abstract: Open Source Software (OSS) projects follow diverse lifecycle trajectories shaped by evolving patterns of contribution, coordination, and community engagement. Understanding these trajectories is essential for stakeholders seeking to assess project organization and health at scale. However, prior work has largely relied on static or aggregated metrics, such as project age or cumulative activity, providing limited insight into how OSS sustainability unfolds over time. In this paper, we propose a hierarchical predictive framework that models OSS projects as belonging to distinct lifecycle stages grounded in established socio-technical categorizations of OSS development. Rather than treating sustainability solely as project longevity, these lifecycle stages operationalize sustainability as a multidimensional construct integrating contribution activity, community participation, and maintenance dynamics. The framework combines engineered tabular indicators with 24-month temporal activity sequences and employs a multi-stage classification pipeline to distinguish lifecycle stages associated with different coordination and participation regimes. To support transparency, we incorporate explainable AI techniques to examine the relative contribution of feature categories to model predictions. Evaluated on a large corpus of OSS repositories, the proposed approach achieves over 94\% overall accuracy in lifecycle stage classification. Attribution analyses consistently identify contribution activity and community-related features as dominant signals, highlighting the central role of collective participation dynamics.

</details>


### [66] [DRAGON: Robust Classification for Very Large Collections of Software Repositories](https://arxiv.org/abs/2602.09071)
*Stefano Balla,Stefano Zacchiroli,Thomas Degueule,Jean-Rémy Falleri,Romain Robbes*

Main category: cs.SE

TL;DR: DRAGON利用文件和目录名进行大规模代码库主题分类，鲁棒性强且超越现有方法，同时发布世界最大代码库分类数据集。


<details>
  <summary>Details</summary>
Motivation: 现有代码库主题分类方法过度依赖README及元数据，但这些信息常缺失，限制现实大规模应用效果，因而需要一种更鲁棒且基于版本控制轻量信号的分类方法。

Method: 提出DRAGON分类器，基于文件和目录名信息（可选结合README），在大规模多样化软件库中进行主题分类，提高F1@5指标，并发布了包含82.5万个代码库的开源数据集。

Result: DRAGON 在大规模代码库主题分类中将F1@5从54.8%提升至60.8%，且在无README时性能仅下降6%，分类错误多为语义相近，且发布了有史以来最大的代码库分类数据集。

Conclusion: DRAGON模型通过仅利用版本控制系统中的轻量级信号（如文件和目录名）实现了对代码库的高效分类，尤其在README文件缺失情况下仍保持良好性能，超过了现有技术的表现。

Abstract: The ability to automatically classify source code repositories with ''topics'' that reflect their content and purpose is very useful, especially when navigating or searching through large software collections. However, existing approaches often rely heavily on README files and other metadata, which are frequently missing, limiting their applicability in real-world large-scale settings. We present DRAGON, a repository classifier designed for very large and diverse software collections. It operates entirely on lightweight signals commonly stored in version control systems: file and directory names, and optionally the README when available. In repository classification at scale, DRAGON improves F1@5 from 54.8% to 60.8%, surpassing the state of the art. DRAGON remains effective even when README files are absent, with performance degrading by only 6% w.r.t. when they are present. This robustness makes it practical for real-world settings where documentation is sparse or inconsistent. Furthermore, many of the remaining classification errors are near misses, where predicted labels are semantically close to the correct topics. This property increases the practical value of the predictions in real-world software collections, where suggesting a few related topics can still guide search and discovery. As a byproduct of developing DRAGON, we also release the largest open dataset to date for repository classification, consisting of 825 thousand repositories with associated ground-truth topics, sourced from the Software Heritage archive, providing a foundation for future large-scale and language-agnostic research on software repository understanding.

</details>


### [67] [AIDev: Studying AI Coding Agents on GitHub](https://arxiv.org/abs/2602.09185)
*Hao Li,Haoxiang Zhang,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 本文构建了大规模AI编码代理生成的GitHub拉取请求数据集AIDev，助力研究AI在软件工程中的应用与协作。


<details>
  <summary>Details</summary>
Motivation: 当前研究缺少关于AI编码代理在真实软件项目中应用的全面数据，限制了对AI软件开发影响的深入理解。

Method: 通过收集和整合来自GitHub的932,791个由五大AI编码代理生成的拉取请求（Agentic-PRs），涵盖116,211个代码库和72,189名开发者。

Result: 构建了包含广泛代理生成拉取请求的数据集，并且精选了33,596个高质量PR子集，包含完整的注释、评审等信息。

Conclusion: 本论文提出了AIDev数据集，填补了AI编程代理在真实项目中应用数据缺乏的空白，为AI在软件工程中的应用研究提供了宝贵资源。

Abstract: AI coding agents are rapidly transforming software engineering by performing tasks such as feature development, debugging, and testing. Despite their growing impact, the research community lacks a comprehensive dataset capturing how these agents are used in real-world projects. To address this gap, we introduce AIDev, a large-scale dataset focused on agent-authored pull requests (Agentic-PRs) in real-world GitHub repositories. AIDev aggregates 932,791 Agentic-PRs produced by five agents: OpenAI Codex, Devin, GitHub Copilot, Cursor, and Claude Code. These PRs span 116,211 repositories and involve 72,189 developers. In addition, AIDev includes a curated subset of 33,596 Agentic-PRs from 2,807 repositories with over 100 stars, providing further information such as comments, reviews, commits, and related issues. This dataset offers a foundation for future research on AI adoption, developer productivity, and human-AI collaboration in the new era of software engineering.
  > AI Agent, Agentic AI, Coding Agent, Agentic Coding, Agentic Software Engineering, Agentic Engineering

</details>


### [68] [Towards an OSF-based Registered Report Template for Software Engineering Controlled Experiments](https://arxiv.org/abs/2602.09292)
*Ana B. M. Bett,Thais S. Nepomuceno,Edson OliveiraJr,Maria Teresa Baldassarre,Valdemar V. Graciano Neto,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 本文探讨了利用OSF平台建立软件工程领域受控实验的注册报告模板，揭示当前模板的不足，强调了制定专门RR指南的必要性以提高实验的可重复性和严谨性。


<details>
  <summary>Details</summary>
Motivation: 当前软件工程领域的受控实验描述缺乏严谨性，导致实验结果难以复现和透明，注册报告作为一种预先登记研究假设和方法的机制，能有效减少不良研究行为。

Method: 分析了开放科学框架中选定RR模板类型，并将其与受控实验的文档指导方针进行对比，评估其严谨性和适用性。

Result: 发现OSF上现有的RR模板虽然部分符合实验文档要求，但没有一套模板能够全面覆盖所有指导方针，并且模板自定义存在限制。

Conclusion: 软件工程中的受控实验在规划和文档记录方面仍缺乏严谨性，影响了实验的可重复性和透明度。虽然采用基于开放科学框架（OSF）的注册报告（RR）能够部分改善这一问题，但现有RR模板未能全面满足指导方针，亟需为软件工程建立专门的RR指南。

Abstract: Context: The empirical software engineering (ESE) community has contributed to improving experimentation over the years. However, there is still a lack of rigor in describing controlled experiments, hindering reproducibility and transparency. Registered Reports (RR) have been discussed in the ESE community to address these issues. A RR registers a study's hypotheses, methods, and/or analyses before execution, involving peer review and potential acceptance before data collection. This helps mitigate problematic practices such as p-hacking, publication bias, and inappropriate post hoc analysis. Objective: This paper presents initial results toward establishing an RR template for Software Engineering controlled experiments using the Open Science Framework (OSF). Method: We analyzed templates of selected OSF RR types in light of documentation guidelines for controlled experiments. Results: The observed lack of rigor motivated our investigation of OSF-based RR types. Our analysis showed that, although one of the RR types aligned with many of the documentation suggestions contained in the guidelines, none of them covered the guidelines comprehensively. The study also highlights limitations in OSF RR template customization. Conclusion: Despite progress in ESE, planning and documenting experiments still lack rigor, compromising reproducibility. Adopting OSF-based RRs is proposed. However, no currently available RR type fully satisfies the guidelines. Establishing RR-specific guidelines for SE is deemed essential.

</details>


### [69] [Cross-Project Flakiness: A Case Study of the OpenStack Ecosystem](https://arxiv.org/abs/2602.09311)
*Tao Xiao,Dong Wang,Shane McIntosh,Hideaki Hata,Yasutaka Kamei*

Main category: cs.SE

TL;DR: 本研究揭示了OpenStack生态系统中的测试波动性问题，特别是跨项目和不一致波动，强调了改进CI配置协调和测试隔离策略的必要性。


<details>
  <summary>Details</summary>
Motivation: 测试波动性破坏开发者对测试结果的信任，影响持续集成的可靠性，但其在整个生态系统中的影响尚未被充分研究。

Method: 通过实证研究分析了649个OpenStack项目中的测试波动性，识别了跨项目和不一致波动的测试，结合定性分析探讨其根本原因。

Result: 发现跨项目波动性影响了55%的OpenStack项目，70%的单元测试表现出跨项目波动特性，不一致的波动主要由CI竞态条件、构建配置不一致和依赖不匹配引起。

Conclusion: 跨项目和不一致的测试波动性在复杂的软件生态系统中普遍存在，显著影响开发效率和资源消耗，并挑战了单元测试的稳定性假设。

Abstract: Automated regression testing is a cornerstone of modern software development, often contributing directly to code review and Continuous Integration (CI). Yet some tests suffer from flakiness, where their outcomes vary non-deterministically. Flakiness erodes developer trust in test results, wastes computational resources, and undermines CI reliability. While prior research has examined test flakiness within individual projects, its broader ecosystem-wide impact remains largely unexplored. In this paper, we present an empirical study of test flakiness in the OpenStack ecosystem, which focuses on (1) cross-project flakiness, where flaky tests impact multiple projects, and (2) inconsistent flakiness, where a test exhibits flakiness in some projects but remains stable in others. By analyzing 649 OpenStack projects, we identify 1,535 cross-project flaky tests and 1,105 inconsistently flaky tests. We find that cross-project flakiness affects 55% of OpenStack projects and significantly increases both review time and computational costs. Surprisingly, 70% of unit tests exhibit cross-project flakiness, challenging the assumption that unit tests are inherently insulated from issues that span modules like integration and system-level tests. Through qualitative analysis, we observe that race conditions in CI, inconsistent build configurations, and dependency mismatches are the primary causes of inconsistent flakiness. These findings underline the need for better coordination across complex ecosystems, standardized CI configurations, and improved test isolation strategies.

</details>


### [70] [SWE-AGI: Benchmarking Specification-Driven Software Construction with MoonBit in the Era of Autonomous Agents](https://arxiv.org/abs/2602.09447)
*Zhirui Zhang,Hongbo Zhang,Haoxiang Fei,Zhiyuan Bao,Yubin Chen,Zhengyu Lei,Ziyue Liu,Yixuan Sun,Mingkun Xiao,Zihang Ye,Yu Zhang,Hongcheng Zhu,Yuxiang Wen,Heung-Yeung Shum*

Main category: cs.SE

TL;DR: 本研究通过SWE-AGI基准测试多种大语言模型从规格自动构建复杂软件的能力，发现当前模型虽然能完成大多数任务，但在更难任务及代码阅读上表现不足，表明自动化软件工程依然面临挑战。


<details>
  <summary>Details</summary>
Motivation: 探索大规模语言模型能否从明确规格自动化构建复杂且生产级的软件系统，解决数据泄露和代码检索依赖问题，推动AI软件工程实用化。

Method: 提出了SWE-AGI基准，利用MoonBit生态系统，设计多个基于权威标准和RFC严格API规范的复杂软件构建任务，评测多款先进模型的端到端软件构建能力。

Result: gpt-5.3-codex在22个任务中解决19个，表现最好。Claude-opus-4.6次之。开源模型kimi-2.5表现最佳。随着任务难度增加，性能显著下降，强调代码阅读是主要瓶颈。

Conclusion: 当前大规模语言模型在自动化构建符合规格说明的大型生产级软件方面尚存在显著挑战，尽管已有模型表现出一定能力，但随着任务难度提升表现急剧下降，代码阅读成为主要瓶颈。

Abstract: Although large language models (LLMs) have demonstrated impressive coding capabilities, their ability to autonomously build production-scale software from explicit specifications remains an open question. We introduce SWE-AGI, an open-source benchmark for evaluating end-to-end, specification-driven construction of software systems written in MoonBit. SWE-AGI tasks require LLM-based agents to implement parsers, interpreters, binary decoders, and SAT solvers strictly from authoritative standards and RFCs under a fixed API scaffold. Each task involves implementing 1,000-10,000 lines of core logic, corresponding to weeks or months of engineering effort for an experienced human developer. By leveraging the nascent MoonBit ecosystem, SWE-AGI minimizes data leakage, forcing agents to rely on long-horizon architectural reasoning rather than code retrieval. Across frontier models, gpt-5.3-codex achieves the best overall performance (solving 19/22 tasks, 86.4%), outperforming claude-opus-4.6 (15/22, 68.2%), and kimi-2.5 exhibits the strongest performance among open-source models. Performance degrades sharply with increasing task difficulty, particularly on hard, specification-intensive systems. Behavioral analysis further reveals that as codebases scale, code reading, rather than writing, becomes the dominant bottleneck in AI-assisted development. Overall, while specification-driven autonomous software engineering is increasingly viable, substantial challenges remain before it can reliably support production-scale development.

</details>


### [71] [AlgoVeri: An Aligned Benchmark for Verified Code Generation on Classical Algorithms](https://arxiv.org/abs/2602.09464)
*Haoyu Zhao,Ziran Yang,Jiawei Li,Deyuan He,Zenan Li,Chi Jin,Venugopal V. Veeravalli,Aarti Gupta,Sanjeev Arora*

Main category: cs.SE

TL;DR: 提出AlgoVeri基准，跨Dafny、Verus和Lean评测AI自动生成验证代码的能力，发现现有模型在不同语言和验证系统中性能差异显著，语言设计与系统约束是影响因素。


<details>
  <summary>Details</summary>
Motivation: 现有的验证代码生成评测方法分散且不可比，缺少跨范式统一的对比基准，因此需要一个统一的框架来公平评估和揭示不同验证系统间的性能差异。

Method: 构建包含77个经典算法的跨语言基准测试集（AlgoVeri），在Dafny、Verus和Lean三种语言中统一功能合约，比较不同AI模型在三种验证系统中的性能表现。

Result: AlgoVeri显示Gemini-3 Flash在Dafny表现最优（40.3%通过率），但在Verus（24.7%）和Lean（7.8%）表现下降明显；Gemini-3通过迭代修复显著提升性能，而GPT-OSS早期饱和；语言设计导致模型面临不同的语法和语义挑战。

Conclusion: 本论文通过AlgoVeri基准测试揭示了不同验证系统在自动生成形式化验证代码方面的能力差异，强调了语言设计和系统约束对模型性能的显著影响。

Abstract: Vericoding refers to the generation of formally verified code from rigorous specifications. Recent AI models show promise in vericoding, but a unified methodology for cross-paradigm evaluation is lacking. Existing benchmarks test only individual languages/tools (e.g., Dafny, Verus, and Lean) and each covers very different tasks, so the performance numbers are not directly comparable. We address this gap with AlgoVeri, a benchmark that evaluates vericoding of $77$ classical algorithms in Dafny, Verus, and Lean. By enforcing identical functional contracts, AlgoVeri reveals critical capability gaps in verification systems. While frontier models achieve tractable success in Dafny ($40.3$% for Gemini-3 Flash), where high-level abstractions and SMT automation simplify the workflow, performance collapses under the systems-level memory constraints of Verus ($24.7$%) and the explicit proof construction required by Lean (7.8%). Beyond aggregate metrics, we uncover a sharp divergence in test-time compute dynamics: Gemini-3 effectively utilizes iterative repair to boost performance (e.g., tripling pass rates in Dafny), whereas GPT-OSS saturates early. Finally, our error analysis shows that language design affects the refinement trajectory: while Dafny allows models to focus on logical correctness, Verus and Lean trap models in persistent syntactic and semantic barriers. All data and evaluation code can be found at https://github.com/haoyuzhao123/algoveri.

</details>


### [72] [Toward Linking Declined Proposals and Source Code: An Exploratory Study on the Go Repository](https://arxiv.org/abs/2602.09467)
*Sota Nakashima,Masanari Kondo,Mahmoud Alfadel,Aly Ahmad,Toshihiro Nakae,Hidenori Matsuzaki*

Main category: cs.SE

TL;DR: 本研究首次关注开源软件中被拒绝贡献与源码之间的追溯链接，提出LLM驱动的链接方法，取得较好效果并揭示生成难点。


<details>
  <summary>Details</summary>
Motivation: 之前研究主要关注被接受的贡献链接，而被拒绝贡献背后的讨论包含丰富的设计理由和隐性知识，尚未被充分挖掘，因此本研究旨在填补该空白。

Method: 设计了一个基于大语言模型(LLM)的自动链接流程，利用Go语言官方仓库的被拒绝提案作为数据集，生成贡献与源码之间的追溯链接，并通过实验评估其准确性和精度。

Result: 该方法在识别被拒绝提案正确粒度上的准确率达0.836，生成正确链接的平均精度为0.643，且通过失败分析揭示失败链接的讨论常缺乏具体实现信息。

Conclusion: 该研究首次尝试建立被拒绝贡献与相关源码之间的可追溯性链接，验证了基于大语言模型的链接方法的有效性，并分析了链接生成中的挑战与影响因素。

Abstract: Traceability links are key information sources for software developers, connecting software artifacts (e.g., linking requirements to the corresponding source code). In open-source software (OSS) projects, such links play an important role, particularly between the contributions (e.g., GitHub issues) and the corresponding source code. Through these links, developers can trace the discussions in contributions and uncover design rationales, constraints, and security concerns. Previous studies have mainly examined accepted contributions, while those declined after discussion have been overlooked. The discussions behind declined contributions contain valuable design rationales and implicit knowledge about software decision-making, as the reasons behind the decline often reveal the criteria used to judge what should or should not be implemented. In this study, we present the first attempt to establish traceability links between declined contributions and related source code. We propose an initial linking approach and conduct an empirical analysis of the generated links to discuss factors affecting link generation. As our dataset, we use proposals from the official Go repository, which are GitHub issues used to propose new features or language changes. To link declined proposals to source code, we designed an LLM-driven pipeline. Our results showed that the pipeline selected the correct granularity for each declined proposal with an accuracy of 0.836, and generated correct links at that granularity with a mean precision of 0.643. To clarify the challenges of linking declined proposals, we performed a failure analysis. In the declined proposals where the pipeline failed to generate links, the discussions were often redundant and lacked concrete information (e.g., how the feature should be implemented).

</details>


### [73] [SWE-Bench Mobile: Can Large Language Model Agents Develop Industry-Level Mobile Applications?](https://arxiv.org/abs/2602.09540)
*Muxin Tian,Zhe Wang,Blair Yang,Zhenwei Tang,Kunlun Zhu,Honghua Dong,Hanchen Li,Xinni Xie,Guangjing Wang,Jiaxuan You*

Main category: cs.SE

TL;DR: SWE-Bench Mobile基准测试显示现有语言模型代理在工业级移动应用开发中表现有限，提供了针对性改进建议及公开平台。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型代理在实际复杂软件工程任务中的表现，弥补现有基准集中在孤立问题或bug修复的不足。

Method: 设计并推出SWE-Bench Mobile基准测试，使用多模态输入、复杂混合Swift/Objective-C代码库及全面测试套件，评估22种代理-模型组合的性能。

Result: 发现代理设计对性能影响显著，商业代理优于开源代理，且简单的"防御性编程"提示比复杂提示效果好7.4%。

Conclusion: 当前大型语言模型代理在工业级移动应用开发中仍存在显著差距，最高任务成功率仅为12%。

Abstract: Can large language model agents develop industry-level mobile applications? We introduce \textbf{SWE-Bench Mobile}, a benchmark for evaluating coding agents on realistic software engineering tasks derived from a production iOS codebase. Unlike existing benchmarks that focus on isolated problems or bug fixes, SWE-Bench Mobile captures the full complexity of industrial development: multi-modal inputs (PRDs and Figma designs), a large-scale mixed Swift/Objective-C codebase, and comprehensive test suites. We evaluate 22 agent-model configurations across four coding agents -- three commercial (Cursor, Codex, Claude Code) and one open-source (OpenCode) -- and find that even the best configurations achieve only 12\% task success rate. Our analysis reveals that (1) agent design matters as much as model capability -- the same model shows up to 6$\times$ performance gap across agents, (2) commercial agents consistently outperform open-source alternatives, and (3) simple ``Defensive Programming'' prompts outperform complex ones by 7.4\%. These findings highlight a significant gap between current agent capabilities and industrial requirements, while providing actionable insights for practitioners and researchers. We release SWE-Bench Mobile as a \textit{hosted benchmark challenge} to prevent data contamination and ensure fair evaluation. The public leaderboard and development toolkit are available at https://swebenchmobile.com.

</details>


### [74] [Generative AI Adoption in an Energy Company: Exploring Challenges and Use Cases](https://arxiv.org/abs/2602.09846)
*Malik Abdul Sami,Zeeshan Rasheed,Meri Olenius,Muhammad Waseem,Kai-Kristian Kemell,Jussi Rasku,Pekka Abrahamsson*

Main category: cs.SE

TL;DR: 本研究通过采访能源公司员工，发现生成式AI在多个工作环节有用且需渐进式整合，助力推动能源行业AI应用。


<details>
  <summary>Details</summary>
Motivation: 探讨能源公司员工如何理解AI采纳及发现生成式AI和大型语言模型工作流如何助力日常工作。

Method: 通过四周时间内进行16次半结构化访谈，涵盖9个部门，辅以内部文档和观察数据进行多角度分析。

Result: 识别出多项生成式AI实际应用场景并提出符合实际工作的渐进式引入策略，为能源行业AI落地提供参考。

Conclusion: 员工认为生成式AI在报告、预测、数据处理、维护和异常检测等任务中非常有用，且应通过逐步引入以契合现有工作流程。

Abstract: Organisations are examining how generative AI can support their operational work and decision-making processes. This study investigates how employees in a energy company understand AI adoption and identify areas where AI and LLMs-based agentic workflows could assist daily activities. Data was collected in four weeks through sixteen semi-structured interviews across nine departments, supported by internal documents and researcher observations. The analysis identified areas where employees positioned AI as useful, including reporting work, forecasting, data handling, maintenance-related tasks, and anomaly detection. Participants also described how GenAI and LLM-based tools could be introduced through incremental steps that align with existing workflows. The study provides an overview view of AI adoption in the energy sector and offers a structured basis for identifying entry points for practical implementation and comparative research across industries.

</details>


### [75] [Immersion in the GitHub Universe: Scaling Coding Agents to Mastery](https://arxiv.org/abs/2602.09892)
*Jiale Zhao,Guoxin Chen,Fanzhe Meng,Minghao Li,Jie Chen,Hui Xu,Yongshuai Sun,Xin Zhao,Ruihua Song,Yuan Zhang,Peng Wang,Cheng Chen,Jirong Wen,Kai Jia*

Main category: cs.SE

TL;DR: 本文提出了ScaleSWE自动化多代理系统，规模化构建了迄今最大、最高质量的软件工程训练数据，通过微调大语言模型显著提升软件工程任务解决能力。


<details>
  <summary>Details</summary>
Motivation: 当前软件工程领域缺乏大规模高质量训练数据，且环境搭建、测试生成和任务描述的复杂性限制了数据的扩展，阻碍了模型性能提升。

Method: 通过设计一个自动化的沙箱多代理工作流，协调环境搭建、单元测试生成和问题描述合成三个专门代理，处理了5200个代码库中的600万次拉取请求，生成了10万条验证过的软件工程实例，并利用这些数据进行模型微调。

Result: 构建了迄今最大的包含10万条高质量软件工程实例的数据集，数据集在库多样性和任务复杂度上显著优于现有数据集，微调后的ScaleSWE代理在SWE Bench Verified测试中达到了64%的解决率，是基础模型性能的近三倍。

Conclusion: 本文提出的ScaleSWE系统成功构建了规模化、高质量的软件工程训练数据集，显著提升了模型在软件工程任务上的性能，推动了基于大语言模型的软件工程研究进展。

Abstract: Achieving mastery in real world software engineering tasks is fundamentally bottlenecked by the scarcity of large scale, high quality training data. Scaling such data has been limited by the complexity of environment setup, unit test generation, and problem statement curation. In this paper, we propose ScaleSWE, an automated, sandboxed multi agent workflow designed to construct high quality SWE data at scale. The system coordinates three specialized agents for environment setup, test creation, and problem description synthesis to process 6 million pull requests across 5200 repositories, producing Scale SWE Data: 100k verified SWE instances, the largest such dataset to date. It substantially surpasses existing real world datasets in repository diversity and reflects realistic task complexity. We further demonstrate the dataset utility for training by distilling 71498 high quality trajectories and finetuning Qwen30BA3BInstruct to produce ScaleSWE Agent. Our agent achieves a 64 resolve rate on SWE Bench Verified a nearly three fold improvement over the base model. ScaleSWE provides a scalable, reproducible approach for data construction to advance LLM based software engineering. Scale SWE will be publicly available.

</details>


### [76] [Operationalizing Human Values in the Requirements Engineering Process of Ethics-Aware Autonomous Systems](https://arxiv.org/abs/2602.09921)
*Everaldo Silva Júnior,Lina Marsso,Ricardo Caldas,Marsha Chechik,Genaína Nunes Rodrigues*

Main category: cs.SE

TL;DR: 该论文针对伦理自主系统挑战，提出通过SLEEC需求模型系统性地捕获和调和人类价值，验证了其在医疗案例中的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于人类价值的模糊性、多元性和情境依赖性，现有的软件工程方式难以有效捕获和调和价值冲突。

Method: 通过将人类价值转化为社会、法律、伦理、同理心和文化（SLEEC）需求，支持自动化的格式检查、冲突检测以及设计早期的协商。

Result: 提出的方法在医学体感网络案例中验证了其可行性，展示了系统化表达和处理人类价值冲突的能力。

Conclusion: 该论文提出了一种将人类价值系统化为规范性目标的方法，并与功能性和适应性目标对齐，实现了伦理感知自主系统的需求工程。

Abstract: Operationalizing human values alongside functional and adaptation requirements remains challenging due to their ambiguous, pluralistic, and context-dependent nature. Explicit representations are needed to support the elicitation, analysis, and negotiation of value conflicts beyond traditional software engineering abstractions. In this work, we propose a requirements engineering approach for ethics-aware autonomous systems that captures human values as normative goals and aligns them with functional and adaptation goals. These goals are systematically operationalized into Social, Legal, Ethical, Empathetic, and Cultural (SLEEC) requirements, enabling automated well-formedness checking, conflict detection, and early design-time negotiation. We demonstrate the feasibility of the approach through a medical Body Sensor Network case study.

</details>


### [77] [JMigBench: A Benchmark for Evaluating LLMs on Source Code Migration (Java 8 to Java 11)](https://arxiv.org/abs/2602.09930)
*Nishil Amin,Zhiwei Fei,Xiang Li,Justyna Petke,He Ye*

Main category: cs.SE

TL;DR: 本文构建了一个Java函数迁移基准数据集，评测了Mistral Codestral模型在代码迁移任务中的表现，发现其在简单替换上表现一般，但复杂任务能力有限，尚不能完全替代人工迁移。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在代码迁移任务中的能力，尤其是Java 8升级到Java 11中函数的自动迁移，帮助减轻开发者负担。

Method: 收集开源代码库中的函数对数据集，筛选构建覆盖8类弃用API的精炼数据集，通过CodeBLEU和关键词指标评估Mistral Codestral模型的代码迁移性能。

Result: Mistral Codestral在简单API替换任务上表现有一定效果，但在处理复杂迁移如CORBA或JAX-WS时效果不佳，提示该模型现阶段只能部分辅助开发者。

Conclusion: Mistral Codestral模型在处理Java 8到Java 11的代码迁移中，对于简单的一对一API替换表现适中，能够完全匹配迁移11.11%的案例，但在复杂迁移任务上表现不足，尚不能完全替代人工。

Abstract: We build a benchmark to evaluate large language models (LLMs) for source code migration tasks, specifically upgrading functions from Java 8 to Java 11. We first collected a dataset of function pairs from open-source repositories, but limitations in data quality led us to construct a refined dataset covering eight categories of deprecated APIs. Using this dataset, the Mistral Codestral model was evaluated with CodeBLEU and keyword-based metrics to measure lexical and semantic similarity as well as migration correctness. Results show that the evaluated model (Mistral Codestral) can handle trivial one-to-one API substitutions with moderate success, achieving identical migrations in 11.11% of the cases, but it struggles with more complex migrations such as CORBA or JAX-WS. These findings suggest Mistral Codestral can partially reduce developer effort by automating repetitive migration tasks but cannot yet replace humans within the scope of the JMigBench benchmark. The benchmark and analysis provide a foundation for future work on expanding datasets, refining prompting strategies, and improving migration performance across different LLMs.

</details>


### [78] [QEMI: A Quantum Software Stacks Testing Framework via Equivalence Modulo Inputs](https://arxiv.org/abs/2602.09942)
*Junjie Luo,Shangzhou Xia,Fuyuan Zhang,Jianjun Zhao*

Main category: cs.SE

TL;DR: 针对量子软件栈测试难题，本文提出基于等价模输入的QEMI方法，通过生成带死代码的量子程序及其变体，检测软件缺陷，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于量子软件缺乏可靠的预期行为标准（oracle问题），现有测试方法难以有效发现缺陷，亟需新的测试技术。

Method: 借鉴经典编译器测试中的Equivalence Modulo Inputs (EMI)技术，设计随机量子程序生成器并生成变体，利用去除死代码的方法比较不同版本程序的行为差异来检测潜在缺陷。

Result: 在Qiskit、Q#和Cirq上应用QEMI，发现11个崩溃错误和1个行为不一致错误。

Conclusion: 本文提出的QEMI方法有效提升了量子软件栈（QSS）的测试能力，成功发现了多个崩溃和行为异常错误。

Abstract: As quantum algorithms and hardware continue to evolve, ensuring the correctness of the quantum software stack (QSS) has become increasingly important. However, testing QSSes remains challenging due to the oracle problem, i.e., the lack of a reliable ground truth for expected program behavior. Existing metamorphic testing approaches often rely on equivalent circuit transformations, backend modifications, or parameter tuning to address this issue. In this work, inspired by Equivalence Modulo Inputs (EMI), we propose Quantum EMI (QEMI), a new testing approach for QSSes. Our key contributions include: (1) a random quantum program generator that produces code with dead code based on quantum control-flow structures, and (2) an adaptation of the EMI technique from classical compiler testing to generate variants by removing dead code. By comparing the behavior of these variants, we can detect potential bugs in QSS implementations. We applied QEMI to Qiskit, Q#, and Cirq, and successfully identified 11 crash bugs and 1 behavioral inconsistency. QEMI expands the limited set of testing techniques available for quantum software stacks by going beyond structural transformations and incorporating semantics-preserving ones into quantum program analysis.

</details>


### [79] [Environment-in-the-Loop: Rethinking Code Migration with LLM-based Agents](https://arxiv.org/abs/2602.09944)
*Xiang Li,Zhiwei Fei,Ying Ma,Jerry Zhang,Sarro Federica,He Ye*

Main category: cs.SE

TL;DR: 代码迁移需融合自动化环境交互，单一代码迁移自动化不足，提出集成框架并分析现状与挑战。


<details>
  <summary>Details</summary>
Motivation: 现有代码迁移虽然取得进展，但自动化环境交互研究较少，单靠静态环境分析导致反馈周期长和效率降低。

Method: 提出了一种将自动化环境配置与代码迁移工作流紧密结合的新框架范式，同时对自动化环境构建现状进行了综述。

Result: 强调了环境交互自动化在代码迁移中的重要性，指出当前自动化环境构建的挑战与未来方向。

Conclusion: 成功的软件演进必须同时考虑代码迁移和环境迁移，缺乏自动化环境交互，代码迁移自动化只能完成一半。

Abstract: Modern software systems continuously undergo code upgrades to enhance functionality, security, and performance, and Large Language Models (LLMs) have demonstrated remarkable capabilities in code migration tasks. However, while research on automated code migration which including refactoring, API adaptation, and dependency updates has advanced rapidly, the exploration of the automated environment interaction that must accompany it remains relatively scarce. In practice, code and its environment are intricately intertwined. Relying solely on static analysis of the environment leads to an inadequate understanding of the target setting, prolongs feedback cycles, and consequently causes significant rework and project delays, thereby reducing overall efficiency. We contend that successful software evolution demands a holistic perspective that integrates both code and environment migration. To understand the current landscape and challenges, we first provide an overview of the status of automated environment construction. We then propose a novel framework paradigm that tightly integrates automated environment setup with the code migration workflow. Finally, we explore the challenges and future directions for automated environment interaction within the code migration domain. Our findings emphasize that without automated environment interaction, the automation of code migration is only half complete.

</details>


### [80] [Artisan: Agentic Artifact Evaluation](https://arxiv.org/abs/2602.10046)
*Doehyun Baek,Michael Pradel*

Main category: cs.SE

TL;DR: 本文提出了Artisan，一种自动化LLM代理，用于生成软件工程领域论文研究结果的复现脚本。通过代码生成和自动化评判机制，Artisan提升了复现效率和效果，验证在60个任务上优于现有基线，且发现了新的错误。


<details>
  <summary>Details</summary>
Motivation: 传统人工制品评估过程劳动强度大，且难以大规模应用，只能对部分论文做一次性评估，亟需自动化支持。

Method: 将研究结果复现任务建模为代码生成任务，生成能够独立执行的复现脚本；设计自动化评判机制引导生成过程并防止简单复制结果。

Result: Artisan在60个任务中成功生成了44个复现脚本，性能较基线模型提升3.14倍，平均每个任务耗时0.45至48分钟，并发现20处新的错误。

Conclusion: 本文提出的Artisan系统显著提升了软件工程领域中研究结果复现的自动化水平，能够生成大量有效的复现脚本并揭示论文或制品中的错误。

Abstract: Artifact evaluation has become standard practice in the software engineering community to ensure the reproducibility of research results. However, the current manual process is labor-intensive, and hence, done only as a one-time assessment for a subset of all papers. To support the artifact evaluation effort, we present Artisan, an automated LLM agent for reproducing research results given a paper and its artifact. The approach is enabled by two key contributions: First, we frame the reproduction problem as a code generation task where the goal is to generate a reproduction script that, when executed, reproduces the results reported in a paper. Unlike prior work on automatically reproducing research results in other domains, this formulation allows for running the script independently of the agent and for assessing the reproduction process at a fine-grained level. Second, we design automated judging mechanism that guides the agent toward the expected results without revealing them and that prevent trivial solutions, such as simply copying checked-in results. To evaluate Artisan, we introduce Artisan-Bench, the first benchmark assessing the ability to generate reproduction scripts and the first benchmark for automated artifact evaluation in software engineering. Artisan-Bench comprises 60 tasks derived from 23 software engineering papers, covering different research areas and programming languages. We validate all tasks in Artisan-Bench for reproducibility to ensure that the tasks are feasible. Our experiments show that Artisan is effective, producing 44/60 reproduction scripts and outperforming the best available baseline, a vanilla LLM agent (mini-swe-agent), by 3.14$\times$ in terms of reproduction scripts generated while taking $0.45 and 48 minutes, on average per task. Artisan also helped uncover 20 new errors in either the paper or artifact.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [81] [LingxiDiagBench: A Multi-Agent Framework for Benchmarking LLMs in Chinese Psychiatric Consultation and Diagnosis](https://arxiv.org/abs/2602.09379)
*Shihao Xu,Tiancheng Zhou,Jiatong Ma,Yanli Ding,Yiming Yan,Ming Xiao,Guoyi Li,Haiyang Geng,Yunyun Han,Jianhua Chen,Yafeng Deng*

Main category: cs.MA

TL;DR: 本文提出了一个中文精神疾病诊断大规模基准，评测大型语言模型在诊断和动态咨询中的表现，发现目前模型在复杂诊断和动态信息获取方面效果有限。


<details>
  <summary>Details</summary>
Motivation: 精神疾病诊断存在精神科医生短缺和访谈主观性强的问题，缺乏兼具真实患者模拟、临床验证标签及支持动态多轮咨询的评测基准，限制了AI辅助精神病诊断的发展。

Method: 设计并发布了包含16,000条与电子病历对齐的模拟咨询对话数据集LingxiDiag-16K，涵盖12类ICD-10精神疾病，通过静态诊断推断和动态多轮咨询两种任务评估多个先进的LLM性能。

Result: 实验显示LLMs在二分类（抑郁-焦虑）任务上准确率高达92.3%，但在抑郁-焦虑合并症识别及12类多分类诊断准确率显著下降，动态咨询表现不及静态评估，且咨询质量与诊断准确性之间相关性中等。

Conclusion: 本文通过构建大规模多智能体基准LingxiDiagBench，揭示了当前大型语言模型在精神疾病诊断和动态多轮咨询中的不足，特别是在复杂诊断和信息获取策略上表现欠佳，且咨询质量与诊断准确率相关性有限。

Abstract: Mental disorders are highly prevalent worldwide, but the shortage of psychiatrists and the inherent subjectivity of interview-based diagnosis create substantial barriers to timely and consistent mental-health assessment. Progress in AI-assisted psychiatric diagnosis is constrained by the absence of benchmarks that simultaneously provide realistic patient simulation, clinician-verified diagnostic labels, and support for dynamic multi-turn consultation. We present LingxiDiagBench, a large-scale multi-agent benchmark that evaluates LLMs on both static diagnostic inference and dynamic multi-turn psychiatric consultation in Chinese. At its core is LingxiDiag-16K, a dataset of 16,000 EMR-aligned synthetic consultation dialogues designed to reproduce real clinical demographic and diagnostic distributions across 12 ICD-10 psychiatric categories. Through extensive experiments across state-of-the-art LLMs, we establish key findings: (1) although LLMs achieve high accuracy on binary depression--anxiety classification (up to 92.3%), performance deteriorates substantially for depression--anxiety comorbidity recognition (43.0%) and 12-way differential diagnosis (28.5%); (2) dynamic consultation often underperforms static evaluation, indicating that ineffective information-gathering strategies significantly impair downstream diagnostic reasoning; (3) consultation quality assessed by LLM-as-a-Judge shows only moderate correlation with diagnostic accuracy, suggesting that well-structured questioning alone does not ensure correct diagnostic decisions. We release LingxiDiag-16K and the full evaluation framework to support reproducible research at https://github.com/Lingxi-mental-health/LingxiDiagBench.

</details>


### [82] [Dieu khien he da tac tu](https://arxiv.org/abs/2602.09412)
*Minh Hoang Trinh,Hieu Minh Nguyen*

Main category: cs.MA

TL;DR: 本书系统介绍了多智能体系统控制的基础理论与方法，适用于教学和研究，内容涵盖基本概念、线性算法设计以及多领域应用。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统控制自2000年代初引起广泛关注，但缺乏系统化的英文教材，作者基于多年教学经验，编写简明的越南语教材以填补该空白。

Method: 通过分三部分详细介绍多智能体系统及图论基础、线性一致算法的设计与分析，以及应用与研究方向，包括编队控制、网络定位、分布式优化、舆论动力学和矩阵加权网络。采用逐步分析方法，便于理解复杂主题。

Result: 为多智能体系统控制领域的学者和学生提供了系统的基础教材，涵盖理论、方法及应用，促进该领域的教学与研究发展。

Conclusion: 该书系统呈现了多智能体系统控制的基础理论和方法，为研究和教学提供了全面而结构化的资源。

Abstract: Since the early 2000s, control of multiagent systems has attracted significant research interest, with applications ranging from natural collective behaviors and social dynamics to engineered systems such as autonomous vehicles, sensor networks, and smart grids. Although research on multi-agent systems has diversified into numerous specialized directions, textbooks -- including those in English -- that provide a systematic treatment of the fundamental principles of multi-agent system control remain scarce. The material presented in this book has been developed and used in teaching since 2021, initially as a concise Vietnamese-language reference for the courses Networked Control Systems and Control of Multi-Agent Systems at Hanoi University of Science and Technology. The book focuses on a selection of fundamental topics of broad and continuing interest in the field. The complexity of several topics is asymptotic to that encountered in research-level studies, however, the analysis is presented in a step-by-step manner to facilitate access to commonly used methods and tools.
  The material is divided into three main parts. Part I introduces multiagent systems and basic graph-theoretic concepts. Part II addresses the design and analysis of linear consensus algorithms. Part III covers selected applications and research directions, including formation control, network localization, distributed optimization, opinion dynamics, and matrix-weighted networks. Each chapter concludes with notes on notable researchers in this field, further reading, and exercises.
  This book cannot be completed without the encouragement, support and suggestions from families, colleagues and friends. The authors appreciate feedback from readers to further improve the content of the book.

</details>


### [83] [Tiny Moves: Game-based Hypothesis Refinement](https://arxiv.org/abs/2602.09801)
*Agnieszka Dobrowolska,Rogier Hintzen,Martin Balla,Karl Gemayel,Sabine Reichert,Thomas Charman,Jen Ning Lim,Lindsay Edwards,Anna Gogleva*

Main category: cs.MA

TL;DR: 本文提出“假设游戏”，用LLM进行增量式科学假设细化，在错误恢复和重建任务中均表现优异，推动了可控性和可解释性的科学发现。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法将科学发现视为端到端预测，忽视了科学推理的增量结构；科学进展通常通过小幅、局部且基于领域背景的修正逐步完成。

Method: 提出了“假设游戏”这一符号化形式，通过固定的推理动作语法，使用大型语言模型（LLM）代理在共享假设状态上进行增量式编辑。

Result: 在包含受控错误的假设恢复任务中，该方法错误移除更彻底、精度更高；在部分线索的重建任务中，表现与最强基线相当，证明了基于动作的显式细化方法的竞争力。

Conclusion: 基于游戏的推理方法在科学假设的逐步细化中表现优异，能有效提高假设的准确性和结构的合理性，且具备良好的可控性和可解释性。

Abstract: Most machine learning approaches to scientific discovery frame hypotheses as end-to-end predictions, obscuring the incremental structure of scientific reasoning. We propose The Hypothesis Game, a symbolic formalism for hypothesis refinement in which LLM agents operate on a shared hypothesis state using a fixed grammar of reasoning moves. The framework is motivated by the observation that scientific progress often proceeds through small, localized revisions, grounded in domain context, rather than extensive rewrites. We instantiate a minimal game with LLM agents and evaluate it on pathway-level mechanistic refinement tasks. In the primary setting of corruption recovery, where hypotheses contain controlled errors, the game-based approach consistently removes more errors and achieves higher precision than strong prompting baselines, while preserving valid structure through incremental edits. In a secondary reconstruction setting from partial cues, it performs comparably to the strongest baseline, indicating that explicit move-based refinement remains competitive even when ground-truth recovery is difficult. These findings support game-based reasoning as a principled route to more controllable, interpretable, and transferable hypothesis refinement systems for scientific discovery.

</details>
