<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 40]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.SE](#cs.SE) [Total: 12]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Shoot First, Ask Questions Later? Building Rational Agents that Explore and Act Like People](https://arxiv.org/abs/2510.20886)
*Gabriel Grand,Valerio Pepe,Jacob Andreas,Joshua B. Tenenbaum*

Main category: cs.CL

TL;DR: 本文提出了一种基于语言模型的智能体在有限资源条件下进行信息寻求和决策的评估和改进方法。


<details>
  <summary>Details</summary>
Motivation: 在科学和诊断等应用中，基于语言模型的智能体需在有限资源下做出合理的假设和决策，但现有模型在生成有效问题和行动选择上表现不足。

Method: 构建了一个策略性决策对话任务Collaborative Battleship，并提出基于贝叶斯实验设计的蒙特卡洛推断策略，提升Spotter和Captain两类智能体的性能。

Result: 所提方法使Spotter准确率提升14.7%，Captain信息增益提升94.2%，弱语言模型性能大幅提升，甚至超过人类和最新模型，且在Guess Who?任务中亦表现出显著提升。

Conclusion: 结合贝叶斯实验设计的蒙特卡洛推断可有效提升语言模型智能体的信息寻求合理性和决策能力，具有广泛的应用潜力。

Abstract: Many high-stakes applications of AI require forming data-driven hypotheses
and making targeted guesses; e.g., in scientific and diagnostic settings. Given
limited resources, to what extent do agents based on language models (LMs) act
rationally? We develop methods to benchmark and enhance agentic
information-seeking, drawing on insights from human behavior. First, we
introduce a strategic decision-oriented dialogue task called Collaborative
Battleship, in which a partially-informed Captain must balance exploration
(asking questions) and action (taking shots), while a fully-informed Spotter
must provide accurate answers under an information bottleneck. Compared to
human players (N=42), we find that LM agents struggle to ground answers in
context, generate informative questions, and select high-value actions. Next,
to address these gaps, we develop novel Monte Carlo inference strategies for
LMs based on principles from Bayesian Experimental Design (BED). For Spotter
agents, our approach boosts accuracy by up to 14.7% absolute over LM-only
baselines; for Captain agents, it raises expected information gain (EIG) by up
to 0.227 bits (94.2% of the achievable noise ceiling). Combined, these
components yield sharper targeting (+0.303-0.374 F1), and enable weaker LMs,
such as Llama-4-Scout, to outperform both humans (8% -> 82% win rate) and
frontier models (0% -> 67% win rate vs. GPT-5) at ~1% of GPT-5's cost. We
replicate these findings on Guess Who? where our methods significantly boost
accuracy (+28.3-42.4 p.p.), demonstrating their general applicability for
building rational information-seeking agents.

</details>


### [2] [Code-enabled language models can outperform reasoning models on diverse tasks](https://arxiv.org/abs/2510.20909)
*Cedegao E. Zhang,Cédric Colas,Gabriel Poesia,Joshua B. Tenenbaum,Jacob Andreas*

Main category: cs.CL

TL;DR: 本论文提出了CodeAdapt方法，使标准指导语言模型无需微调即可在多领域推理任务中表现优于强化学习训练的推理模型，同时提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有的推理模型虽效果显著，但训练和运行成本高昂且效率低，故希望找到成本更低、推理能力更强的替代方案。

Method: 结合CodeAct框架，将语言模型的自然语言推理与代码执行多步骤交替进行，并利用只含五个训练样本的少量上下文学习实现推理能力提升。

Result: 在四对语言模型与推理模型中，CodeAdapt使得三组语言模型在八个任务上平均提升至高达22.9%的性能，同时在六个任务上整体优于推理模型达35.7%，并且计算效率提升10-81%。

Conclusion: CodeAdapt方法的学习和推理具备鲁棒性和领域通用性，代码增强语言模型具有认知基础和强大性能，有望成为强化学习的重要基石。

Abstract: Reasoning models (RMs), language models (LMs) trained with reinforcement
learning to produce long-form natural language reasoning, have been remarkably
successful, but they still require large amounts of computation and data to
train, and can be slow and expensive to run. In this paper, we show that
standard instruct LMs can already be elicited to be strong reasoners at a level
comparable to or even surpassing their corresponding RMs (e.g., DeepSeek V3 vs
R1) without finetuning, across diverse domains from instruction following and
creative generation to mathematical reasoning. This is achieved by CodeAdapt,
our simple recipe that combines the CodeAct framework, where LMs interleave
natural language reasoning with code execution in a multi-step fashion, with
few-shot bootstrap in-context learning from as few as five training problems.
Analyzing four matched pairs of LMs and RMs, we find that CodeAdapt enables
three LMs to outperform the corresponding RMs on average over eight tasks (up
to 22.9%) while being 10-81% more token efficient, and delivers superior
performance on six tasks when averaged over the four models (up to 35.7%).
Furthermore, the code-augmented reasoning traces display rich and varied
problem-solving strategies. Our findings support that (1) CodeAdapt-style
learning and reasoning may be robust and domain general and (2) code-enabled
LMs are cognitively grounded and powerful systems, potentially providing a
strong foundation for in-weight reinforcement learning.

</details>


### [3] [FicSim: A Dataset for Multi-Faceted Semantic Similarity in Long-Form Fiction](https://arxiv.org/abs/2510.20926)
*Natasha Johnson,Amanda Bertsch,Maria-Emil Deal,Emma Strubell*

Main category: cs.CL

TL;DR: 本文介绍了针对计算文学研究的长篇小说相似性评估任务，构建了FICSIM数据集并评测了多种文本嵌入模型，揭示模型更多关注表层特征而非语义类别。


<details>
  <summary>Details</summary>
Motivation: 现有的嵌入相似度数据集主要针对短文本且粒度较粗，难以评估语言模型在计算文学领域处理长篇复杂文本中的有效性。

Method: 汇编发布包含长篇新近小说的FICSIM数据集，涵盖12个相似性维度，基于作者元数据并由数字人文学者验证，评测多种嵌入模型在该任务上的表现。

Result: 绝大部分模型倾向于关注文本的表层特征，而非对文学研究有价值的语义类别。

Conclusion: 当前嵌入模型尚不能有效捕捉长篇文学文本的深层语义，相似性评估需基于更加丰富的标注数据，并强调尊重作者权益及持续获得知情同意。

Abstract: As language models become capable of processing increasingly long and complex
texts, there has been growing interest in their application within
computational literary studies. However, evaluating the usefulness of these
models for such tasks remains challenging due to the cost of fine-grained
annotation for long-form texts and the data contamination concerns inherent in
using public-domain literature. Current embedding similarity datasets are not
suitable for evaluating literary-domain tasks because of a focus on
coarse-grained similarity and primarily on very short text. We assemble and
release FICSIM, a dataset of long-form, recently written fiction, including
scores along 12 axes of similarity informed by author-produced metadata and
validated by digital humanities scholars. We evaluate a suite of embedding
models on this task, demonstrating a tendency across models to focus on
surface-level features over semantic categories that would be useful for
computational literary studies tasks. Throughout our data-collection process,
we prioritize author agency and rely on continual, informed author consent.

</details>


### [4] [Do LLMs Truly Understand When a Precedent Is Overruled?](https://arxiv.org/abs/2510.20941)
*Li Zhang,Jaromir Savelka,Kevin Ashley*

Main category: cs.CL

TL;DR: 本文评估了大型语言模型在美国最高法院案件中过度推翻关系识别的能力，揭示了模型对历史案件表现较差、依赖浅层逻辑启发式、以及在复杂任务中产生时序矛盾的问题。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏真实复杂法律文本理解的长文档基准，现有评测多依赖简化合成任务，难以反映实际法律文档的复杂性。

Method: 构建了包含236对美国最高法院案件的过度推翻关系数据集，测试现有最先进大型语言模型在识别这些法律关系中的表现。

Result: 发现模型存在三个主要缺陷：对历史时期案件表现退化（时代敏感性）、依赖浅显逻辑而非深度法律理解，以及在复杂开放任务中产生时序不可能关系（上下文依赖推理失败）。

Conclusion: 所提出基准填补了长文本真实法律推理评估的空白，提供了一个贴近实际法律工作的测试环境，有助推动大模型在法律复杂任务中的能力提升。

Abstract: Large language models (LLMs) with extended context windows show promise for
complex legal reasoning tasks, yet their ability to understand long legal
documents remains insufficiently evaluated. Developing long-context benchmarks
that capture realistic, high-stakes tasks remains a significant challenge in
the field, as most existing evaluations rely on simplified synthetic tasks that
fail to represent the complexity of real-world document understanding.
Overruling relationships are foundational to common-law doctrine and commonly
found in judicial opinions. They provide a focused and important testbed for
long-document legal understanding that closely resembles what legal
professionals actually do. We present an assessment of state-of-the-art LLMs on
identifying overruling relationships from U.S. Supreme Court cases using a
dataset of 236 case pairs. Our evaluation reveals three critical limitations:
(1) era sensitivity -- the models show degraded performance on historical cases
compared to modern ones, revealing fundamental temporal bias in their training;
(2) shallow reasoning -- models rely on shallow logical heuristics rather than
deep legal comprehension; and (3) context-dependent reasoning failures --
models produce temporally impossible relationships in complex open-ended tasks
despite maintaining basic temporal awareness in simple contexts. Our work
contributes a benchmark that addresses the critical gap in realistic
long-context evaluation, providing an environment that mirrors the complexity
and stakes of actual legal reasoning tasks.

</details>


### [5] [Irish-BLiMP: A Linguistic Benchmark for Evaluating Human and Language Model Performance in a Low-Resource Setting](https://arxiv.org/abs/2510.20957)
*Josh McGiff,Khanh-Tung Tran,William Mulcahy,Dáibhidh Ó Luinín,Jake Dalzell,Róisín Ní Bhroin,Adam Burke,Barry O'Sullivan,Hoang D. Nguyen,Nikola S. Nikolov*

Main category: cs.CL

TL;DR: Irish-BLiMP是首个用于爱尔兰语语言能力细致评估的数据集和框架，涵盖11个语法特征的1020对最小对，旨在评测大型语言模型与人类在语法知识上的表现差异。


<details>
  <summary>Details</summary>
Motivation: 针对爱尔兰语这类资源稀缺的濒危语言，缺乏细致的语言能力评估工具，推动低资源语言处理研究的进步。

Method: 基于语言学文献和语法手册，由流利爱尔兰语者手动构建并审核了1020对最小对，覆盖11个语言特征；同时对比了人类和大型语言模型的语法理解能力。

Result: 人类在所有语言特征上表现均优于模型，平均准确率高出16.6%；开放源代码和封闭源代码模型间存在18.1%的性能差距；最佳模型gpt-5准确率仅73.5%，人类达90.1%。

Conclusion: Irish-BLiMP首次为爱尔兰语大型语言模型语法能力提供系统性评估框架，是促进低资源语言理解研究的重要基准。

Abstract: We present Irish-BLiMP (Irish Benchmark of Linguistic Minimal Pairs), the
first dataset and framework designed for fine-grained evaluation of linguistic
competence in the Irish language, an endangered language. Drawing on a variety
of linguistic literature and grammar reference works, we manually constructed
and reviewed 1020 minimal pairs across a taxonomy of 11 linguistic features,
through a team of fluent Irish speakers. We evaluate both existing Large
Language Models (LLMs) and fluent human participants on their syntactic
knowledge of Irish. Our findings show that humans outperform all models across
all linguistic features, achieving 16.6% higher accuracy on average. Moreover,
a substantial performance gap of 18.1% persists between open- and closed-source
LLMs, with even the strongest model (gpt-5) reaching only 73.5% accuracy
compared to 90.1% by human. Interestingly, human participants and models
struggle on different aspects of Irish grammar, thus highlighting a difference
in representation learned by the models. Overall, Irish-BLiMP provides the
first systematic framework for evaluating the grammatical competence of LLMs in
Irish and offers a valuable benchmark for advancing research on linguistic
understanding in low-resource languages.

</details>


### [6] [Can Confidence Estimates Decide When Chain-of-thought is Necessary for Llms?](https://arxiv.org/abs/2510.21007)
*Samuel Lewis-Lim,Xingwei Tan,Zhixue Zhao,Nikolaos Aletras*

Main category: cs.CL

TL;DR: 本文提出了基于置信度门控的链式思维（CoT）生成方法，只在模型对直接答案置信度低时才调用推理过程，从而减少冗余推理并提升效率。研究评估了四种无训练置信度估计方法，发现其效果因数据集和模型而异。


<details>
  <summary>Details</summary>
Motivation: 虽然链式思维能提高复杂任务准确率，但推理延长大幅增加令牌使用，且并非总是有效，实际应用受限。因此，研究何时使用CoT尤为重要。

Method: 提出置信度门控CoT策略，仅在置信度低时调动推理。系统比较四种无训练置信度估计方法、随机基线和理想oracle，进行大量实验验证。

Result: 训练免费置信度估计方法能减少冗余CoT调用，优于随机策略，但效果不稳定，受任务和模型影响较大。

Conclusion: 当前无训练置信度度量在实际应用中存在局限，虽能部分提升效率和性能，但需要进一步研究以实现更可靠的自适应CoT门控机制。

Abstract: Chain-of-thought (CoT) prompting has emerged as a common technique for
enhancing the reasoning abilities of large language models (LLMs). While
extended reasoning can boost accuracy on complex tasks, it is often unnecessary
and substantially increases token usage, limiting the practicality of reasoning
models in many scenarios. Recent models, such as GPT-OSS and Qwen3, expose
controls that enable users to adjust the length of CoT or determine whether it
is used at all. Yet, it remains unclear when CoT should be used: on some tasks
it improves performance, while on others it provides little benefit or even
harms performance. We address this challenge with confidence-gated CoT, where a
model invokes reasoning only when confidence in its direct answer is low. To
this end, we present the first systematic study of training-free confidence
estimation methods for CoT gating. Specifically, we evaluate four training-free
confidence estimation methods and compare them to a random baseline and an
oracle that always knows when CoT is needed. Through extensive experiments, we
show that existing training-free confidence measures can reduce redundant CoT
and outperform randomly invoked CoT. However, the utility of individual
confidence measures is inconsistent, varying with both the dataset and the
model, underscoring the difficulty of deploying confidence-gated CoT in
practice. By analysing both strengths and failure modes, our study highlights
the potential and limitations of current methods and paves the way toward more
reliable adaptive gating of CoT.

</details>


### [7] [Input Matters: Evaluating Input Structure's Impact on LLM Summaries of Sports Play-by-Play](https://arxiv.org/abs/2510.21034)
*Barkavi Sundararajan,Somayajulu Sripada,Ehud Reiter*

Main category: cs.CL

TL;DR: 本文研究输入数据结构对大型语言模型生成NBA比赛摘要时出现幻觉和事实错误的影响，发现JSON结构显著减少错误。


<details>
  <summary>Details</summary>
Motivation: 确保在体育报道等精准要求高的领域中，LLM生成的文本能忠实反映输入数据。

Method: 对180场比赛的摘要中，使用Llama和Qwen两种模型，比较三种输入结构（行结构、JSON、非结构化），并人工标注3312个事实错误。

Result: JSON输入相比非结构化输入，错误率降低69%（Llama）和65%（Qwen）；行结构输入分别降低54%和51%。输入结构对错误率的影响超过80%。

Conclusion: 输入数据结构显著影响LLM生成文本的准确性，采用结构化格式尤其是JSON格式能有效减少事实错误。

Abstract: A major concern when deploying LLMs in accuracy-critical domains such as
sports reporting is that the generated text may not faithfully reflect the
input data. We quantify how input structure affects hallucinations and other
factual errors in LLM-generated summaries of NBA play-by-play data, across
three formats: row-structured, JSON and unstructured. We manually annotated
3,312 factual errors across 180 game summaries produced by two models,
Llama-3.1-70B and Qwen2.5-72B. Input structure has a strong effect: JSON input
reduces error rates by 69% for Llama and 65% for Qwen compared to unstructured
input, while row-structured input reduces errors by 54% for Llama and 51% for
Qwen. A two-way repeated measures ANOVA shows that input structure accounts for
over 80% of the variance in error rates, with Tukey HSD post hoc tests
confirming statistically significant differences between all input formats.

</details>


### [8] [Reasoning's Razor: Reasoning Improves Accuracy but Can Hurt Recall at Critical Operating Points in Safety and Hallucination Detection](https://arxiv.org/abs/2510.21049)
*Atoosa Chegini,Hamid Kazemi,Garrett Souza,Maria Safi,Yang Song,Samy Bengio,Sinead Williamson,Mehrdad Farajtabar*

Main category: cs.CL

TL;DR: 论文系统研究了推理增强的大型语言模型在低假阳性率分类任务中的表现，发现推理提升了整体准确率，但在严格低假阳性率下表现不佳，简单集成推理与非推理模式效果最佳。


<details>
  <summary>Details</summary>
Motivation: 推理在大型语言模型中提升了准确率，但其在对假阳性率要求严格的精度敏感任务中的适用性尚不清楚。

Method: 通过对安全检测和幻觉检测任务，在微调和零样本设置下，比较推理增强（Think On）和非推理（Think Off）两种生成模式，同时评估基于token评分和自我表达置信度的方法。

Result: 推理增强提升了整体准确率，但在低假阳性率阈值下不及非推理模式。基于token的评分方法优于自我表达置信度。在两种模式的简单集成下，能同时兼顾准确率和低假阳性率。

Conclusion: 推理技术对平均准确率有益，但在严格要求精度的任务中往往表现欠佳，合理结合推理和非推理模式可发挥各自优势。

Abstract: Reasoning has become a central paradigm for large language models (LLMs),
consistently boosting accuracy across diverse benchmarks. Yet its suitability
for precision-sensitive tasks remains unclear. We present the first systematic
study of reasoning for classification tasks under strict low false positive
rate (FPR) regimes. Our analysis covers two tasks--safety detection and
hallucination detection--evaluated in both fine-tuned and zero-shot settings,
using standard LLMs and Large Reasoning Models (LRMs). Our results reveal a
clear trade-off: Think On (reasoning-augmented) generation improves overall
accuracy, but underperforms at the low-FPR thresholds essential for practical
use. In contrast, Think Off (no reasoning during inference) dominates in these
precision-sensitive regimes, with Think On surpassing only when higher FPRs are
acceptable. In addition, we find token-based scoring substantially outperforms
self-verbalized confidence for precision-sensitive deployments. Finally, a
simple ensemble of the two modes recovers the strengths of each. Taken
together, our findings position reasoning as a double-edged tool: beneficial
for average accuracy, but often ill-suited for applications requiring strict
precision.

</details>


### [9] [Dynamic Retriever for In-Context Knowledge Editing via Policy Optimization](https://arxiv.org/abs/2510.21059)
*Mahmud Wasif Nafee,Maiqi Jiang,Haipeng Chen,Yanfu Zhang*

Main category: cs.CL

TL;DR: 提出了一种动态检索器DR-IKE，用于上下文知识编辑，通过强化学习动态选择示例，提高编辑成功率并减少延迟。


<details>
  <summary>Details</summary>
Motivation: 现有的上下文知识编辑方法依赖静态示例集，存在数量与质量的权衡以及难度适应性不足的问题。

Method: 利用强化学习训练BERT检索器，根据编辑奖励动态排序示例，并通过可学习阈值删减低价值示例，从而调整提示长度以适应任务难度。

Result: DR-IKE在COUNTERFACT基准测试中编辑成功率提升最大17.1%，延迟降低41.6%，且保持无关查询准确性。

Conclusion: 该方法实现了对黑盒大型语言模型的可扩展、自适应的知识编辑，在效率和效果上均有显著提升。

Abstract: Large language models (LLMs) excel at factual recall yet still propagate
stale or incorrect knowledge. In-context knowledge editing offers a
gradient-free remedy suitable for black-box APIs, but current editors rely on
static demonstration sets chosen by surface-level similarity, leading to two
persistent obstacles: (i) a quantity-quality trade-off, and (ii) lack of
adaptivity to task difficulty. We address these issues by dynamically selecting
supporting demonstrations according to their utility for the edit. We propose
Dynamic Retriever for In-Context Knowledge Editing (DR-IKE), a lightweight
framework that (1) trains a BERT retriever with REINFORCE to rank
demonstrations by editing reward, and (2) employs a learnable threshold to
prune low-value examples, shortening the prompt when the edit is easy and
expanding it when the task is hard. DR-IKE performs editing without modifying
model weights, relying solely on forward passes for compatibility with
black-box LLMs. On the COUNTERFACT benchmark, it improves edit success by up to
17.1%, reduces latency by 41.6%, and preserves accuracy on unrelated queries,
demonstrating scalable and adaptive knowledge editing. The code is available at
https://github.com/mwnafee/DR-IKE .

</details>


### [10] [Bridging Language Gaps with Adaptive RAG: Improving Indonesian Language Question Answering](https://arxiv.org/abs/2510.21068)
*William Christian,Daniel Adamlu,Adrian Yu,Derwin Suhartono*

Main category: cs.CL

TL;DR: 本论文通过引入自适应检索增强生成系统，实现了印尼语问答系统，虽提升部分环节表现，但多检索策略存在明显不一致性。


<details>
  <summary>Details</summary>
Motivation: 解决当前问答系统主要集中于英语，低资源语言如印尼语表现不足的问题。

Method: 设计自适应RAG系统，其中分类器判定问题复杂度以调整回答策略，且利用机器翻译进行数据增强。

Result: 分类器性能可靠，但多检索策略在答案生成中表现不一致，导致整体评价下降。

Conclusion: 自适应RAG系统在低资源语言问答中具潜力，但多检索策略需改进，未来研究可针对该问题展开。

Abstract: Question Answering (QA) has seen significant improvements with the
advancement of machine learning models, further studies enhanced this question
answering system by retrieving external information, called Retrieval-Augmented
Generation (RAG) to produce more accurate and informative answers. However,
these state-of-the-art-performance is predominantly in English language. To
address this gap we made an effort of bridging language gaps by incorporating
Adaptive RAG system to Indonesian language. Adaptive RAG system integrates a
classifier whose task is to distinguish the question complexity, which in turn
determines the strategy for answering the question. To overcome the limited
availability of Indonesian language dataset, our study employs machine
translation as data augmentation approach. Experiments show reliable question
complexity classifier; however, we observed significant inconsistencies in
multi-retrieval answering strategy which negatively impacted the overall
evaluation when this strategy was applied. These findings highlight both the
promise and challenges of question answering in low-resource language
suggesting directions for future improvement.

</details>


### [11] [CDrugRed: A Chinese Drug Recommendation Dataset for Discharge Medications in Metabolic Diseases](https://arxiv.org/abs/2510.21084)
*Juntao Li,Haobin Yuan,Ling Luo,Yan Jiang,Fan Wang,Ping Zhang,Huiyi Lv,Jian Wang,Yuanyuan Sun,Hongfei Lin*

Main category: cs.CL

TL;DR: 这篇论文介绍了一种基于电子健康记录（EHRs）的智能用药推荐数据集CDrugRed，专注于代谢性疾病的出院用药，并评估了大规模语言模型在该任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前智能用药推荐系统发展受限于公开可用的真实世界电子健康记录数据集稀缺，特别是在非英语环境中。为了推动中文用药推荐研究，作者构建了一个新的数据集。

Method: 作者发布了包含3190名患者5894条匿名记录的中文代谢性疾病出院用药数据集CDrugRed，数据涵盖患者人口学信息、医疗历史、临床过程和出院诊断等。并使用多种最先进的大规模语言模型对出院药物推荐任务进行基准测试。

Result: 实验结果表明，尽管监督微调能够提升模型性能，但最佳模型的F1分数仅为0.5648，Jaccard分数为0.4477，显示用药推荐任务的复杂性和模型性能仍有较大提升空间。

Conclusion: CDrugRed作为一个公开且具有挑战性的中文用药推荐数据集，为开发更强健且准确的用药推荐系统提供了宝贵资源，促进该领域研究发展。

Abstract: Intelligent drug recommendation based on Electronic Health Records (EHRs) is
critical for improving for improving the quality and efficiency of clinical
decision-making. By leveraging large-scale patient data, drug recommendation
systems can assist physicians in selecting the most appropriate medications
according to a patient's medical history, diagnoses, laboratory results, and
comorbidities. However, the advancement of such systems is significantly
hampered by the scarcity of publicly available, real-world EHR datasets,
particularly in languages other than English. In this work, we present
CDrugRed, a first publicly available Chinese drug recommendation dataset
focused on discharge medications for metabolic diseases. The dataset includes
5,894 de-identified records from 3,190 patients, containing comprehensive
information such as patient demographics, medical history, clinical course, and
discharge diagnoses. We assess the utility of CDrugRed by benchmarking several
state-of-the-art large language models (LLMs) on the discharge medication
recommendation task. Experimental results show that while supervised
fine-tuning improves model performance, there remains substantial room for
improvement, with the best model achieving the F1 score of 0.5648 and Jaccard
score of 0.4477. This result highlights the complexity of the clinical drug
recommendation task and establishes CDrugRed as a challenging and valuable
resource for developing more robust and accurate drug recommendation systems.
The dataset is publicly available to the research community under the data
usage agreements at https://github.com/DUTIR-BioNLP/CDrugRed.

</details>


### [12] [Self-Rewarding PPO: Aligning Large Language Models with Demonstrations Only](https://arxiv.org/abs/2510.21090)
*Qingru Zhang,Liang Qiu,Ilgee Hong,Zhenghao Xu,Tianyi Liu,Shiyang Li,Rongzhi Zhang,Zheng Li,Lihong Li,Bing Yin,Chao Zhang,Jianshu Chen,Haoming Jiang,Tuo Zhao*

Main category: cs.CL

TL;DR: 提出了一种结合SFT和PPO的自奖励强化方法，用于提升大语言模型的泛化能力和数据效率。


<details>
  <summary>Details</summary>
Motivation: 传统的监督微调(SFT)方法容易过拟合且泛化能力差，尤其在有限数据条件下表现不佳。

Method: 提出Self-Rewarding PPO方法，通过对比SFT模型和预训练模型的策略比率作为奖励函数，结合PPO算法进行在线微调，无需人工偏好标注。

Result: 该方法在多个自然语言处理任务中优于传统SFT，提高了泛化性能、数据利用效率和鲁棒性。

Conclusion: Self-Rewarding PPO有效缓解了SFT的局限，能更好地利用演示数据对大语言模型进行对齐，尤其适合高质量标注数据稀缺的场景。

Abstract: Supervised fine-tuning (SFT) has emerged as a crucial method for aligning
large language models (LLMs) with human-annotated demonstrations. However, SFT,
being an off-policy approach similar to behavior cloning, often struggles with
overfitting and poor out-of-domain generalization, especially in limited-data
scenarios. To address these limitations, we propose Self-Rewarding PPO, a novel
fine-tuning method that leverages on-policy techniques to enhance
generalization performance. Our approach combines the strengths of SFT and
proximal policy optimization (PPO) to achieve more effective alignment from
demonstration data. At its core is a reward function designed as the log policy
ratio between the SFT model and the pretrained base model. This function serves
as an implicit reward signal, using the pretrained policy as a baseline and the
SFT policy as a target. By doing so, it enables on-policy fine-tuning without
relying on human preference annotations. The integration of this self-rewarding
mechanism with PPO addresses key limitations of SFT, improving generalization,
data efficiency, and robustness. Our empirical evaluation across a range of
natural language processing tasks demonstrates that Self-Rewarding PPO
consistently outperforms traditional SFT methods. The results highlight the
effectiveness of our approach in aligning LLMs using demonstration data,
particularly in scenarios where high-quality annotated data is scarce.

</details>


### [13] [The Gray Zone of Faithfulness: Taming Ambiguity in Unfaithfulness Detection](https://arxiv.org/abs/2510.21118)
*Qiang Ding,Lvzhou Luo,Yixuan Cao,Ping Luo*

Main category: cs.CL

TL;DR: 该论文提出了一个新的摘要生成可信度标注框架和基准VeriGray，以解决现有标注因外部知识边界不清导致的歧义问题，并评估了先进的大型语言模型在摘要生成中的虚假信息问题。


<details>
  <summary>Details</summary>
Motivation: 现有摘要生成的可信度标注存在外部知识边界不明确的问题，导致标注不一致，影响真实场景中大型语言模型摘要的可信度评估。

Method: 引入一个新的标注类别"Out-Dependent"用于区分需要外部知识验证的情况，基于此构建了一个新的不可信摘要检测基准VeriGray。

Result: 统计显示即使是SOTA模型如GPT-5在摘要中也存在约6%的幻觉句子，约8%的句子属于"Out-Dependent"类别，表明该基准对现有方法挑战显著。

Conclusion: 该框架和基准能够有效解决标注歧义问题，为未来提升摘要生成的可信度检测提供了重要平台和挑战。

Abstract: Ensuring that Large Language Models (LLMs) generate summaries faithful to a
given source document is essential for real-world applications. While prior
research has explored LLM faithfulness, existing benchmarks suffer from
annotation ambiguity, primarily due to the ill-defined boundary of permissible
external knowledge in generated outputs. For instance, common sense is often
incorporated into responses and labeled as "faithful", yet the acceptable
extent of such knowledge remains unspecified, leading to inconsistent
annotations. To address this issue, we propose a novel faithfulness annotation
framework, which introduces an intermediate category, Out-Dependent, to
classify cases where external knowledge is required for verification. Using
this framework, we construct VeriGray (Verification with the Gray Zone) -- a
new unfaithfulness detection benchmark in summarization. Statistics reveal that
even SOTA LLMs, such as GPT-5, exhibit hallucinations ($\sim 6\%$ of sentences)
in summarization tasks. Moreover, a substantial proportion ($\sim 8\%$ on
average of models) of generated sentences fall into the Out-Dependent category,
underscoring the importance of resolving annotation ambiguity in unfaithfulness
detection benchmarks. Experiments demonstrate that our benchmark poses
significant challenges to multiple baseline methods, indicating considerable
room for future improvement.

</details>


### [14] [Large Language Models Meet Text-Attributed Graphs: A Survey of Integration Frameworks and Applications](https://arxiv.org/abs/2510.21131)
*Guangxin Su,Hanchen Wang,Jianwei Wang,Wenjie Zhang,Ying Zhang,Jian Pei*

Main category: cs.CL

TL;DR: 本文综述了大型语言模型（LLM）与文本属性图（TAG）结合的研究进展，分类讨论了两者集成的策略与应用。


<details>
  <summary>Details</summary>
Motivation: LLM擅长语义理解但黑盒导致推理能力有限；TAG结构明确但语义深度不足。两者结合能够弥补各自不足，提升推理和解释能力。

Method: 提出LLM助力TAG和TAG助力LLM两大方向，构建顺序、并行、多模块三种编排框架，探讨预训练、提示设计、参数高效微调等技术。

Result: 总结了相关经验，整理了数据集，展示了推荐系统、生物医药分析、知识问答等领域的实际应用案例。

Conclusion: LLM与TAG结合具有广阔前景，但仍面临方法、应用和理论挑战，未来可进一步探索语言与图学习的交叉研究。

Abstract: Large Language Models (LLMs) have achieved remarkable success in natural
language processing through strong semantic understanding and generation.
However, their black-box nature limits structured and multi-hop reasoning. In
contrast, Text-Attributed Graphs (TAGs) provide explicit relational structures
enriched with textual context, yet often lack semantic depth. Recent research
shows that combining LLMs and TAGs yields complementary benefits: enhancing TAG
representation learning and improving the reasoning and interpretability of
LLMs. This survey provides the first systematic review of LLM--TAG integration
from an orchestration perspective. We introduce a novel taxonomy covering two
fundamental directions: LLM for TAG, where LLMs enrich graph-based tasks, and
TAG for LLM, where structured graphs improve LLM reasoning. We categorize
orchestration strategies into sequential, parallel, and multi-module
frameworks, and discuss advances in TAG-specific pretraining, prompting, and
parameter-efficient fine-tuning. Beyond methodology, we summarize empirical
insights, curate available datasets, and highlight diverse applications across
recommendation systems, biomedical analysis, and knowledge-intensive question
answering. Finally, we outline open challenges and promising research
directions, aiming to guide future work at the intersection of language and
graph learning.

</details>


### [15] [Social Simulations with Large Language Model Risk Utopian Illusion](https://arxiv.org/abs/2510.21180)
*Ning Bian,Xianpei Han,Hongyu Lin,Baolei Wu,Jun Wang*

Main category: cs.CL

TL;DR: 本文提出了一个系统框架，通过多智能体对话模拟和五个语言维度分析，揭示大型语言模型(LLMs)在模拟社会行为时表现出理想化的偏差，不真实反映复杂真实人类行为。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在模拟人类社会行为时的偏离程度未被充分探讨，可能导致科学研究误读和现实应用的负面后果。

Method: 通过聊天室式多智能体对话模拟社会互动，分析模型在五个语言维度上的表现，检测社会认知偏见。

Result: 实验发现八个代表性LLMs表现出社会角色偏见、首因效应和积极偏见，导致生成“乌托邦”式社会，缺乏真实人类互动的复杂性和多样性。

Conclusion: 需要开发更加基于社会实际、多样化的人类社会行为的LLMs，以减少社会认知偏差，提高模拟的真实性。

Abstract: Reliable simulation of human behavior is essential for explaining,
predicting, and intervening in our society. Recent advances in large language
models (LLMs) have shown promise in emulating human behaviors, interactions,
and decision-making, offering a powerful new lens for social science studies.
However, the extent to which LLMs diverge from authentic human behavior in
social contexts remains underexplored, posing risks of misinterpretation in
scientific studies and unintended consequences in real-world applications.
Here, we introduce a systematic framework for analyzing LLMs' behavior in
social simulation. Our approach simulates multi-agent interactions through
chatroom-style conversations and analyzes them across five linguistic
dimensions, providing a simple yet effective method to examine emergent social
cognitive biases. We conduct extensive experiments involving eight
representative LLMs across three families. Our findings reveal that LLMs do not
faithfully reproduce genuine human behavior but instead reflect overly
idealized versions of it, shaped by the social desirability bias. In
particular, LLMs show social role bias, primacy effect, and positivity bias,
resulting in "Utopian" societies that lack the complexity and variability of
real human interactions. These findings call for more socially grounded LLMs
that capture the diversity of human social behavior.

</details>


### [16] [Estonian Native Large Language Model Benchmark](https://arxiv.org/abs/2510.21193)
*Helena Grete Lillepalu,Tanel Alumäe*

Main category: cs.CL

TL;DR: 本文介绍了一个用于评估爱沙尼亚语言大语言模型（LLM）的新基准，涵盖七个多样化的数据集，评估不同类型模型的表现，并通过人工评估和LLM作为评判方法进行验证。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏针对爱沙尼亚语的大语言模型基准，且尚未有全面比较不同LLM在爱沙尼亚语任务上的性能。

Method: 构建包含七个原生爱沙尼亚语数据集的基准，评估基础模型、命令调优的开源模型和商业模型，共计6个基础模型和26个调优模型，采用人工评估和用CLAUDE 3.7 Sonnet作为判定者的双重评估方式。

Result: 人工评估与基准测试存在中到高度相关性，不同数据集表现不同；CLAUDE 3.7 Sonnet与人工评级高度一致，表明顶尖LLM能有效辅助爱沙尼亚语模型的评估。

Conclusion: 所提出的基准和评估方法为爱沙尼亚语大语言模型的性能评测提供了有力工具，支持进一步研究和应用。

Abstract: The availability of LLM benchmarks for the Estonian language is limited, and
a comprehensive evaluation comparing the performance of different LLMs on
Estonian tasks has yet to be conducted. We introduce a new benchmark for
evaluating LLMs in Estonian, based on seven diverse datasets. These datasets
assess general and domain-specific knowledge, understanding of Estonian grammar
and vocabulary, summarization abilities, contextual comprehension, and more.
The datasets are all generated from native Estonian sources without using
machine translation. We compare the performance of base models,
instruction-tuned open-source models, and commercial models. Our evaluation
includes 6 base models and 26 instruction-tuned models. To assess the results,
we employ both human evaluation and LLM-as-a-judge methods. Human evaluation
scores showed moderate to high correlation with benchmark evaluations,
depending on the dataset. Claude 3.7 Sonnet, used as an LLM judge, demonstrated
strong alignment with human ratings, indicating that top-performing LLMs can
effectively support the evaluation of Estonian-language models.

</details>


### [17] [The "Right" Discourse on Migration: Analysing Migration-Related Tweets in Right and Far-Right Political Movements](https://arxiv.org/abs/2510.21220)
*Nishan Chatterjee,Veronika Bajt,Ana Zwitter Vitez,Senja Pollak*

Main category: cs.CL

TL;DR: 该论文利用先进的自然语言处理技术和社会学视角，分析英文和法文推特上极右翼言论，研究其关于移民、仇恨言论及说服手段的传播模式。


<details>
  <summary>Details</summary>
Motivation: 随着欧洲极右翼民粹主义的兴起，理解其在社交媒体上的传播机制及政治影响变得尤为重要。

Method: 结合自然语言处理技术和社会学方法，分析MIGR-TWIT语料库中的极右翼推文，探讨其 discourse 特征。

Result: 揭示了围绕移民问题的言论模式、仇恨言论特征及极右翼群体的说服策略。

Conclusion: 跨学科研究方法有助于深入理解极右翼通过社交媒体传播极端主义的社会动态及其挑战。

Abstract: The rise of right-wing populism in Europe has brought to the forefront the
significance of analysing social media discourse to understand the
dissemination of extremist ideologies and their impact on political outcomes.
Twitter, as a platform for interaction and mobilisation, provides a unique
window into the everyday communication of far-right supporters. In this paper,
we propose a methodology that uses state-of-the-art natural language processing
techniques with sociological insights to analyse the MIGR-TWIT corpus of
far-right tweets in English and French. We aim to uncover patterns of discourse
surrounding migration, hate speech, and persuasion techniques employed by right
and far-right actors. By integrating linguistic, sociological, and
computational approaches, we seek to offer cross-disciplinary insights into
societal dynamics and contribute to a better understanding of contemporary
challenges posed by right-wing extremism on social media platforms.

</details>


### [18] [DispatchMAS: Fusing taxonomy and artificial intelligence agents for emergency medical services](https://arxiv.org/abs/2510.21228)
*Xiang Li,Huizi Yu,Wenkong Wang,Yiran Wu,Jiayan Zhou,Wenyue Hua,Xinxin Lin,Wenjia Tan,Lexuan Zhu,Bingyi Chen,Guang Chen,Ming-Li Chen,Yang Zhou,Zhao Li,Themistocles L. Assimes,Yongfeng Zhang,Qingyun Wu,Xin Ma,Lingyao Li,Lizhou Fan*

Main category: cs.CL

TL;DR: 本文开发了一种基于临床分类和多智能体系统的急救医疗调度模拟系统，能够高效、真实地模拟紧急呼叫情境，辅助调度员培训和决策支持。


<details>
  <summary>Details</summary>
Motivation: 急救医疗调度过程面临呼叫者情绪波动、信息模糊以及认知负荷大等挑战，传统方法难以高效支持调度员。引入大型语言模型和多智能体系统有望提升调度质量与效率。

Method: 构建了包含32种主诉和6类呼叫者身份的临床分类，以及六阶段呼叫协议，在此基础上开发了基于AutoGen的多智能体系统，包含呼叫者和调度者两类代理，系统基于事实共享库保证临床合理性和减少错误信息。采用混合评估框架，结合4位医生的人工评估与自动语言分析。

Result: 系统表现出极高的调度有效性（94%准确联系其他代理）和指导效率（91%提供建议），医生评分均较高；自动分析显示语言情感中性、可读性高且礼貌用语比例优良。

Conclusion: 该多智能体系统能高保真模拟多样且临床合理的调度场景，适用于调度员培训和协议评估，为实际紧急响应决策支持奠定了基础，展示了安全整合AI技术于紧急医疗流程的可行路径。

Abstract: Objective: Emergency medical dispatch (EMD) is a high-stakes process
challenged by caller distress, ambiguity, and cognitive load. Large Language
Models (LLMs) and Multi-Agent Systems (MAS) offer opportunities to augment
dispatchers. This study aimed to develop and evaluate a taxonomy-grounded,
LLM-powered multi-agent system for simulating realistic EMD scenarios. Methods:
We constructed a clinical taxonomy (32 chief complaints, 6 caller identities
from MIMIC-III) and a six-phase call protocol. Using this framework, we
developed an AutoGen-based MAS with Caller and Dispatcher Agents. The system
grounds interactions in a fact commons to ensure clinical plausibility and
mitigate misinformation. We used a hybrid evaluation framework: four physicians
assessed 100 simulated cases for "Guidance Efficacy" and "Dispatch
Effectiveness," supplemented by automated linguistic analysis (sentiment,
readability, politeness). Results: Human evaluation, with substantial
inter-rater agreement (Gwe's AC1 > 0.70), confirmed the system's high
performance. It demonstrated excellent Dispatch Effectiveness (e.g., 94 %
contacting the correct potential other agents) and Guidance Efficacy (advice
provided in 91 % of cases), both rated highly by physicians. Algorithmic
metrics corroborated these findings, indicating a predominantly neutral
affective profile (73.7 % neutral sentiment; 90.4 % neutral emotion), high
readability (Flesch 80.9), and a consistently polite style (60.0 % polite; 0 %
impolite). Conclusion: Our taxonomy-grounded MAS simulates diverse, clinically
plausible dispatch scenarios with high fidelity. Findings support its use for
dispatcher training, protocol evaluation, and as a foundation for real-time
decision support. This work outlines a pathway for safely integrating advanced
AI agents into emergency response workflows.

</details>


### [19] [Correlation Dimension of Auto-Regressive Large Language Models](https://arxiv.org/abs/2510.21258)
*Xin Du,Kumiko Tanaka-Ishii*

Main category: cs.CL

TL;DR: 本文引入了相关维数作为衡量大语言模型（LLMs）文本复杂性的工具，揭示了模型训练和生成过程中的多种现象。


<details>
  <summary>Details</summary>
Motivation: 当前评价指标过于关注局部预测准确性，忽视了文本的长程结构复杂性，导致模型出现重复和不连贯等问题。

Method: 利用相关维数这一分形几何度量，衡量语言模型感知的文本自相似性及层级递归结构，统一反映文本的局部和全局特性。

Result: 相关维数揭示了预训练的三个阶段，反映上下文依赖的复杂度，指示模型的幻觉倾向，并能有效检测生成文本的各种退化形式。方法计算高效，对模型量化鲁棒，适用于多种自回归架构。

Conclusion: 相关维数为理解和监控大语言模型的生成动态提供了新视角，有助于解决模型行为中的诸多问题。

Abstract: Large language models (LLMs) have achieved remarkable progress in natural
language generation, yet they continue to display puzzling behaviors -- such as
repetition and incoherence -- even when exhibiting low perplexity. This
highlights a key limitation of conventional evaluation metrics, which emphasize
local prediction accuracy while overlooking long-range structural complexity.
We introduce correlation dimension, a fractal-geometric measure of
self-similarity, to quantify the epistemological complexity of text as
perceived by a language model. This measure captures the hierarchical
recurrence structure of language, bridging local and global properties in a
unified framework. Through extensive experiments, we show that correlation
dimension (1) reveals three distinct phases during pretraining, (2) reflects
context-dependent complexity, (3) indicates a model's tendency toward
hallucination, and (4) reliably detects multiple forms of degeneration in
generated text. The method is computationally efficient, robust to model
quantization (down to 4-bit precision), broadly applicable across
autoregressive architectures (e.g., Transformer and Mamba), and provides fresh
insight into the generative dynamics of LLMs.

</details>


### [20] [Sparser Block-Sparse Attention via Token Permutation](https://arxiv.org/abs/2510.21270)
*Xinghao Wang,Pengyu Wang,Dong Zhang,Chenkun Tan,Shaojun Zhou,Zhaoxiang Liu,Shiguo Lian,Fangxu Liu,Kai Song,Xipeng Qiu*

Main category: cs.CL

TL;DR: 本文提出了一种名为PBS-Attn的置换块稀疏注意力机制，通过增加块级稀疏性，提高大语言模型长上下文推理的计算效率，在保持模型准确性的同时，实现了计算速度的显著加速。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的自注意力机制计算复杂度为$O(N^2)$，长序列处理时计算和内存开销大。块稀疏注意力虽然能减少计算，但受限于注意力分布模式，常导致重要信息分散在多个块中，计算冗余。

Method: 提出PBS-Attn方法，利用注意力的置换特性重新组织块，使得重要的key tokens更集中，提升块级稀疏度，减少无效计算。结合自定义的置换FlashAttention内核，实现高效计算。

Result: 在真实长上下文任务数据集上，PBS-Attn在模型准确度上优于现有块稀疏方法，且接近全注意力基线。计算速度提升最高达2.75倍，验证了方法的实用价值。

Conclusion: PBS-Attn通过设计注意力置换策略，有效提升了块稀疏注意力的效率和性能，为长序列大语言模型推理提供了实用的计算加速方案。

Abstract: Scaling the context length of large language models (LLMs) offers significant
benefits but is computationally expensive. This expense stems primarily from
the self-attention mechanism, whose $O(N^2)$ complexity with respect to
sequence length presents a major bottleneck for both memory and latency.
Fortunately, the attention matrix is often sparse, particularly for long
sequences, suggesting an opportunity for optimization. Block-sparse attention
has emerged as a promising solution that partitions sequences into blocks and
skips computation for a subset of these blocks. However, the effectiveness of
this method is highly dependent on the underlying attention patterns, which can
lead to sub-optimal block-level sparsity. For instance, important key tokens
for queries within a single block may be scattered across numerous other
blocks, leading to computational redundancy. In this work, we propose Permuted
Block-Sparse Attention (\textbf{PBS-Attn}), a plug-and-play method that
leverages the permutation properties of attention to increase block-level
sparsity and enhance the computational efficiency of LLM prefilling. We conduct
comprehensive experiments on challenging real-world long-context datasets,
demonstrating that PBS-Attn consistently outperforms existing block-sparse
attention methods in model accuracy and closely matches the full attention
baseline. Powered by our custom permuted-FlashAttention kernels, PBS-Attn
achieves an end-to-end speedup of up to $2.75\times$ in long-context
prefilling, confirming its practical viability. Code available at
https://github.com/xinghaow99/pbs-attn

</details>


### [21] [PARL: Prompt-based Agents for Reinforcement Learning](https://arxiv.org/abs/2510.21306)
*Yarik Menchaca Resendiz,Roman Klinger*

Main category: cs.CL

TL;DR: 本文提出了一种利用大语言模型（LLM）通过提示作为强化学习代理（PARL）的方法，在无微调情况下进行交互式学习。


<details>
  <summary>Details</summary>
Motivation: 现有工作多聚焦于自然语言相关任务或监督无监督学习，较少评估LLM在强化学习任务中的表现，尤其是非语言结构化推理领域。

Method: 设计PARL方法，通过提示编码强化学习中的状态、动作和奖励信息，利用LLM进行试错式学习。

Result: 在三个非完全依赖自然语言的标准强化学习任务中，PARL在简单环境下匹配或超越传统强化学习代理，但在复杂数学计算和状态动作解码任务中存在性能瓶颈。

Conclusion: 基于提示的大语言模型可在强化学习任务中发挥作用，尤其是在简单环境中，但在复杂任务中仍需克服性能限制。

Abstract: Large language models (LLMs) have demonstrated high performance on tasks
expressed in natural language, particularly in zero- or few-shot settings.
These are typically framed as supervised (e.g., classification) or unsupervised
(e.g., clustering) problems. However, limited work evaluates LLMs as agents in
reinforcement learning (RL) tasks (e.g., playing games), where learning occurs
through interaction with an environment and a reward system. While prior work
focused on representing tasks that rely on a language representation, we study
structured, non-linguistic reasoning - such as interpreting positions in a grid
world. We therefore introduce PARL (Prompt-based Agent for Reinforcement
Learning), a method that uses LLMs as RL agents through prompting, without any
fine-tuning. PARL encodes actions, states, and rewards in the prompt, enabling
the model to learn through trial-and-error interaction. We evaluate PARL on
three standard RL tasks that do not entirely rely on natural language. We show
that it can match or outperform traditional RL agents in simple environments by
leveraging pretrained knowledge. However, we identify performance limitations
in tasks that require complex mathematical operations or decoding states and
actions.

</details>


### [22] [Efficient semantic uncertainty quantification in language models via diversity-steered sampling](https://arxiv.org/abs/2510.21310)
*Ji Won Park,Kyunghyun Cho*

Main category: cs.CL

TL;DR: 本文提出了一种多样性引导的采样方法，通过语义相似度惩罚提升大语言模型在开放式问答任务中不确定性估计的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自由形式问答中进行语义不确定性估计时，需大量高成本生成样本以确保估计稳定，存在效率低下的问题。

Method: 引入一种基于语义相似度罚项的连续惩罚策略，利用轻微微调的自然语言推理模型对生成过程中的提议分布施加多样性约束，并结合重要性重加权和控制变量技术降低估计偏差和方差。

Result: 方法在四个问答基准测试中表现优越，以相同样本数覆盖了更多语义聚类，匹配或超越了现有基线。

Conclusion: 该模块化方法无需访问基础模型梯度，可作为提高风险敏感模型部署中不确定性估计的即插即用增强工具。

Abstract: Accurately estimating semantic aleatoric and epistemic uncertainties in large
language models (LLMs) is particularly challenging in free-form question
answering (QA), where obtaining stable estimates often requires many expensive
generations. We introduce a diversity-steered sampler that discourages
semantically redundant outputs during decoding, covers both autoregressive and
masked diffusion paradigms, and yields substantial sample-efficiency gains. The
key idea is to inject a continuous semantic-similarity penalty into the model's
proposal distribution using a natural language inference (NLI) model lightly
finetuned on partial prefixes or intermediate diffusion states. We debias
downstream uncertainty estimates with importance reweighting and shrink their
variance with control variates. Across four QA benchmarks, our method matches
or surpasses baselines while covering more semantic clusters with the same
number of samples. Being modular and requiring no gradient access to the base
LLM, the framework promises to serve as a drop-in enhancement for uncertainty
estimation in risk-sensitive model deployments.

</details>


### [23] [Typoglycemia under the Hood: Investigating Language Models' Understanding of Scrambled Words](https://arxiv.org/abs/2510.21326)
*Gianluca Sperduti,Alejandro Moreo*

Main category: cs.CL

TL;DR: 本文研究了英语词汇在字母内部重排（typoglycemia）下的识别稳定性，发现单词重叠较少且上下文区别明显是模型能鲁棒识别的关键。


<details>
  <summary>Details</summary>
Motivation: 探讨自然语言处理模型为何在忽略字母内部顺序情况下依然能准确识别和理解词语，理解这一现象背后的原因。

Method: 分析英国国家语料库中词汇重叠及歧义情况；评估BERT模型对重叠词语的消歧能力；比较基于正常文本和typoglycemia文本训练的BERT模型性能。

Result: 发现英语中不同单词在typoglycemia处理下的重叠较少，且这些重叠词汇出现于语境差异较大的句子中，BERT模型的性能下降小于预期。

Conclusion: 英语的词汇结构和语境差异性使得基于typoglycemia的文本处理对模型性能影响有限，解释了模型的鲁棒性来源。

Abstract: Research in linguistics has shown that humans can read words with internally
scrambled letters, a phenomenon recently dubbed typoglycemia. Some specific NLP
models have recently been proposed that similarly demonstrate robustness to
such distortions by ignoring the internal order of characters by design. This
raises a fundamental question: how can models perform well when many distinct
words (e.g., form and from) collapse into identical representations under
typoglycemia? Our work, focusing exclusively on the English language, seeks to
shed light on the underlying aspects responsible for this robustness. We
hypothesize that the main reasons have to do with the fact that (i) relatively
few English words collapse under typoglycemia, and that (ii) collapsed words
tend to occur in contexts so distinct that disambiguation becomes trivial. In
our analysis, we (i) analyze the British National Corpus to quantify word
collapse and ambiguity under typoglycemia, (ii) evaluate BERT's ability to
disambiguate collapsing forms, and (iii) conduct a probing experiment by
comparing variants of BERT trained from scratch on clean versus typoglycemic
Wikipedia text; our results reveal that the performance degradation caused by
scrambling is smaller than expected.

</details>


### [24] [TripTide: A Benchmark for Adaptive Travel Planning under Disruptions](https://arxiv.org/abs/2510.21329)
*Priyanshu Karmakar,Soumyabrata Chaudhuri,Shubhojit Mallick,Manish Gupta,Abhik Jana,Shreya Ghosh*

Main category: cs.CL

TL;DR: TripTide提出了第一个评估大型语言模型（LLM）在现实旅行中面对中断时，修订行程能力的基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有的个性化旅行行程生成方法难以应对现实中的各种中断，如航班取消、天气影响等，需评估LLM在实际中断环境下的应变能力。

Method: TripTide模拟中断严重度和旅行者容忍度，设计算法自动评估行程修订的意图保留、响应性和适应性，并结合LLM作为评审及专家人工评估进行三重验证。

Result: 实验证明LLM在保持行程的语义和顺序一致性方面表现较好，但空间偏差在短途旅游中较大且中断处理能力随行程变长而下降。

Conclusion: TripTide为评估LLM在真实不确定性环境下的适应性、个性化和鲁棒性提供了基准，揭示了现有方法的局限和改进方向。

Abstract: Recent efforts like TripCraft and TravelPlanner have advanced the use of
Large Language Models ( LLMs) for personalized, constraint aware travel
itinerary generation. Yet, real travel often faces disruptions. To address
this, we present TripTide, the first benchmark evaluating LLM's ability to
revise itineraries under realistic disruptions. TripTide models key dimensions
such as disruption severity and traveler tolerance, enabling nuanced assessment
of LLM adaptability to events like flight cancellations, weather closures, or
overbooked attractions. We conduct a threefold evaluation. First, we introduce
automatic metrics including Preservation of Intent (how well the revised plan
maintains feasibility and goals), Responsiveness (promptness and
appropriateness of disruption handling), and Adaptability (semantic, spatial,
and sequential divergence between original and revised plans). Second, we apply
an LLM-as-a-judge approach to automatically assess revision quality. Third, we
perform manual expert evaluation to verify whether revisions preserve semantic,
spatial, sequential, and responsive aspects. Our experiments show that LLMs
maintain strong sequential consistency and semantic stability, while spatial
deviations are larger for shorter trips but decrease with longer ones,
indicating that extended plans encourage better geographic coherence. However,
disruption-handling ability declines as plan length increases, highlighting
limits in LLM robustness. TripTide establishes a benchmark for evaluating
adaptability, personalization, and resilience in LLM-based travel planning
under real-world uncertainty.

</details>


### [25] [Multi-turn Training with Basic Human Feedback Helps Little on LLM Reasoning](https://arxiv.org/abs/2510.21339)
*Qiang Liu,Wuganjing Song,Zhenzhou Lin,Feifan Chen,Qiaolong Cai,Chen Li,Yongduo Sui*

Main category: cs.CL

TL;DR: 本文研究了单轮与多轮人类反馈训练对大语言模型推理能力的影响，发现单轮训练在单轮与多轮评估中表现均优于多轮训练，多轮训练反而降低单轮推理性能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型多通过单轮强化学习训练推理能力，但实际应用多为多轮人机交互，训练与部署存在不匹配。本文探讨多轮人类反馈训练是否必要。

Method: 比较了单轮训练与三种多轮训练策略在推理任务中的表现，通过单轮和多轮评估验证模型的泛化能力。

Result: 单轮训练模型在单轮和多轮评估中表现良好；多轮训练模型单轮推理性能显著下降。

Conclusion: 对于信息完整的推理任务，单轮稳定训练更有效，多轮训练带来的收益有限且可能削弱推理能力。

Abstract: The reasoning capabilities of Large Language Models (LLMs) are typically
developed through the single-turn reinforcement learning, whereas real-world
applications often involve multi-turn interactions with human feedback, leading
to a potential mismatch between training and deployment conditions. In this
work, we study whether multi-turn training with human feedback is necessary for
reasoning tasks. We compare conventional single-turn training with three
multi-turn strategies and reach contrary conclusions to previous research. We
find that models trained in a single-turn setting generalize effectively to
both single- and multi-turn evaluations, while models trained with multi-turn
strategies exhibit a significant degradation in single-turn reasoning
performance. These results suggest that for tasks with complete information,
robust single-turn training remains more effective and reliable, as multi-turn
training with basic feedback provides limited benefits and can even degrade
reasoning capabilities.

</details>


### [26] [A Diagnostic Benchmark for Sweden-Related Factual Knowledge](https://arxiv.org/abs/2510.21360)
*Jenny Kunz*

Main category: cs.CL

TL;DR: 本文提出了一个专门针对瑞典相关人物和事件的问题回答基准，评估模型对瑞典知识的掌握，并研究多语言模型的语言适应和知识保持。


<details>
  <summary>Details</summary>
Motivation: 现有的瑞典基准大多是翻译自以美国为中心的基准，不适合测试专门针对瑞典的重要知识，需构建专门的数据集。

Method: 人工编写涉及瑞典文化、媒体及体育事件的问题答案集，包含英文翻译以测试跨语言一致性。评估不同规模和瑞典知识覆盖度的模型表现。

Result: 带有瑞典知识覆盖度的小模型表现与三倍大的多语言模型相当。进一步在瑞典语上继续预训练可提升知识掌握，但可能遗忘部分原有信息。

Conclusion: 该数据集可作为诊断工具研究多语言模型的语言适应和知识保持，对于提升针对特定语言的知识复现能力有重要意义。

Abstract: Many Swedish benchmarks are translated US-centric benchmarks, and therefore
not suitable for testing knowledge that is particularly relevant, or even
specific, to Sweden. We therefore introduce a manually written
question-answering benchmark specifically targeted to Sweden-related
personalities and events, many of which receive very limited coverage in
international media. Our annotators drew inspiration from a popular radio
program featuring public figures from culture and media, as well as major
sports events in Sweden. The dataset can be used to measure factual recall
across models of varying sizes and degrees of Swedish coverage, and allows to
probe cross-lingual factual consistency as to contains English translations.
Using the dataset, we find that smaller models with stronger Swedish coverage
perform comparably to a three times larger multilingual model in recalling
Sweden-related facts. We also observe that continued pre-training on Swedish
generally improves factual knowledge but also leads to forgetting of a part of
the previously known information. These results demonstrate the dataset's
potential as a diagnostic tool for studying language adaptation and knowledge
retention in multilingual models and during language adaptation.

</details>


### [27] [SindBERT, the Sailor: Charting the Seas of Turkish NLP](https://arxiv.org/abs/2510.21364)
*Raphael Scheible-Schmitt,Stefan Schweter*

Main category: cs.CL

TL;DR: SindBERT是首个大规模的土耳其语RoBERTa编码器模型，基于312GB土耳其语数据训练，覆盖多种下游任务，表现优异但未呈现明显规模效应。


<details>
  <summary>Details</summary>
Motivation: 许多形态丰富的语言如土耳其语在大规模预训练模型中被忽视，需要专门资源支持土耳其语NLP的发展。

Method: 从头训练基于RoBERTa的SindBERT模型（base和large版本），利用大规模土耳其语语料库（mC4、OSCAR23、维基百科），并在词性标注、命名实体识别、攻击性语言检测和TurBLiMP基准等任务上进行评估。

Result: SindBERT在多项任务中表现竞争力，其中large版本在两项任务中取得最好成绩，但整体未观察到一致的性能提升趋势，表明土耳其语的现有基准测试可能已接近饱和。

Conclusion: SindBERT作为首个公开的土耳其语大型编码器模型，不仅为土耳其语NLP提供了重要资源，也通过实证表明：语料质量和多样性对形态复杂语言模型性能影响显著，规模扩展效益有限。

Abstract: Transformer models have revolutionized NLP, yet many morphologically rich
languages remain underrepresented in large-scale pre-training efforts. With
SindBERT, we set out to chart the seas of Turkish NLP, providing the first
large-scale RoBERTa-based encoder for Turkish. Trained from scratch on 312 GB
of Turkish text (mC4, OSCAR23, Wikipedia), SindBERT is released in both base
and large configurations, representing the first large-scale encoder-only
language model available for Turkish. We evaluate SindBERT on part-of-speech
tagging, named entity recognition, offensive language detection, and the
TurBLiMP linguistic acceptability benchmark. Our results show that SindBERT
performs competitively with existing Turkish and multilingual models, with the
large variant achieving the best scores in two of four tasks but showing no
consistent scaling advantage overall. This flat scaling trend, also observed
for XLM-R and EuroBERT, suggests that current Turkish benchmarks may already be
saturated. At the same time, comparisons with smaller but more curated models
such as BERTurk highlight that corpus quality and diversity can outweigh sheer
data volume. Taken together, SindBERT contributes both as an openly released
resource for Turkish NLP and as an empirical case study on the limits of
scaling and the central role of corpus composition in morphologically rich
languages. The SindBERT models are released under the MIT license and made
available in both fairseq and Huggingface formats.

</details>


### [28] [HalleluBERT: Let every token that has meaning bear its weight](https://arxiv.org/abs/2510.21372)
*Raphael Scheible-Schmitt*

Main category: cs.CL

TL;DR: HalleluBERT是一个基于RoBERTa的希伯来语预训练语言模型，在大规模数据上训练，优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 目前希伯来语缺乏大规模、充分训练的RoBERTa编码器，现有模型受限于语料、词汇和训练深度。

Method: 提出HalleluBERT，基于RoBERTa架构，从头开始训练，采用49.1GB去重希伯来语网络文本和维基百科，使用希伯来语专用的字节级BPE词汇。

Result: 在命名实体识别和情感分类基准测试中，HalleluBERT优于单语和多语基线模型，达到希伯来语的最新最好水平。

Conclusion: 充分训练的单语预训练模型在希伯来语NLP任务中效果显著，为构建高性能希伯来语模型提供了新途径。

Abstract: Transformer-based models have advanced NLP, yet Hebrew still lacks a
large-scale RoBERTa encoder which is extensively trained. Existing models such
as HeBERT, AlephBERT, and HeRo are limited by corpus size, vocabulary, or
training depth. We present HalleluBERT, a RoBERTa-based encoder family (base
and large) trained from scratch on 49.1~GB of deduplicated Hebrew web text and
Wikipedia with a Hebrew-specific byte-level BPE vocabulary. Evaluated on NER
and sentiment classification benchmarks, HalleluBERT outperforms both
monolingual and multilingual baselines. HalleluBERT sets a new state of the art
for Hebrew and highlights the benefits of fully converged monolingual
pretraining.

</details>


### [29] [Vision Language Models for Dynamic Human Activity Recognition in Healthcare Settings](https://arxiv.org/abs/2510.21424)
*Abderrazek Abid,Thanh-Cong Ho,Fakhri Karray*

Main category: cs.CL

TL;DR: 本文探讨了视觉语言模型（VLMs）在远程健康监测中人体活动识别（HAR）的应用，提出了新的描述性数据集及评估方法，并通过实验验证了VLMs在准确率上优于传统深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型在医疗领域有潜力，但其在远程健康监测中人体活动识别的应用及评估尚不充分，且其动态非确定性输出带来评估难题。

Method: 作者引入了描述性字幕数据集并提出了综合评估方法，设计对比实验，评估VLMs与最先进深度学习模型的表现。

Result: 实验显示，视觉语言模型在准确度上达到甚至超过传统深度学习模型，验证了其应用潜力。

Conclusion: 本文提供了强有力的基准测试和评估方法，促进了视觉语言模型在智能医疗系统中人体活动识别的应用发展。

Abstract: As generative AI continues to evolve, Vision Language Models (VLMs) have
emerged as promising tools in various healthcare applications. One area that
remains relatively underexplored is their use in human activity recognition
(HAR) for remote health monitoring. VLMs offer notable strengths, including
greater flexibility and the ability to overcome some of the constraints of
traditional deep learning models. However, a key challenge in applying VLMs to
HAR lies in the difficulty of evaluating their dynamic and often
non-deterministic outputs. To address this gap, we introduce a descriptive
caption data set and propose comprehensive evaluation methods to evaluate VLMs
in HAR. Through comparative experiments with state-of-the-art deep learning
models, our findings demonstrate that VLMs achieve comparable performance and,
in some cases, even surpass conventional approaches in terms of accuracy. This
work contributes a strong benchmark and opens new possibilities for the
integration of VLMs into intelligent healthcare systems.

</details>


### [30] [Redefining Retrieval Evaluation in the Era of LLMs](https://arxiv.org/abs/2510.21440)
*Giovanni Trappolini,Florin Cuconasu,Simone Filice,Yoelle Maarek,Fabrizio Silvestri*

Main category: cs.CL

TL;DR: 本文提出了UDCG评估指标，改进信息检索评估以适配大语言模型消费检索结果的场景，提升评估指标与生成质量的相关性。


<details>
  <summary>Details</summary>
Motivation: 传统信息检索指标基于人类用户顺序浏览文档假设，且不考虑无关但干扰性文档对生成质量的负面影响，导致无法准确反映基于大语言模型的检索增强生成(RAG)系统的实际表现。

Method: 提出了一个基于效用的标注方案，量化相关段落的正面贡献与干扰性文档的负面影响；基于此设计了UDCG指标，采用符合大语言模型处理习惯的位置折扣，直接优化与最终答案准确率的相关性。

Result: 在五个数据集和六种大语言模型上实验，UDCG指标与传统指标相比，相关性提升最高达36%。

Conclusion: UDCG指标有效对齐了信息检索评价与大语言模型消费需求，促进更可靠的RAG组件评估。

Abstract: Traditional Information Retrieval (IR) metrics, such as nDCG, MAP, and MRR,
assume that human users sequentially examine documents with diminishing
attention to lower ranks. This assumption breaks down in Retrieval Augmented
Generation (RAG) systems, where search results are consumed by Large Language
Models (LLMs), which, unlike humans, process all retrieved documents as a whole
rather than sequentially. Additionally, traditional IR metrics do not account
for related but irrelevant documents that actively degrade generation quality,
rather than merely being ignored. Due to these two major misalignments, namely
human vs. machine position discount and human relevance vs. machine utility,
classical IR metrics do not accurately predict RAG performance. We introduce a
utility-based annotation schema that quantifies both the positive contribution
of relevant passages and the negative impact of distracting ones. Building on
this foundation, we propose UDCG (Utility and Distraction-aware Cumulative
Gain), a metric using an LLM-oriented positional discount to directly optimize
the correlation with the end-to-end answer accuracy. Experiments on five
datasets and six LLMs demonstrate that UDCG improves correlation by up to 36%
compared to traditional metrics. Our work provides a critical step toward
aligning IR evaluation with LLM consumers and enables more reliable assessment
of RAG components

</details>


### [31] [REMONI: An Autonomous System Integrating Wearables and Multimodal Large Language Models for Enhanced Remote Health Monitoring](https://arxiv.org/abs/2510.21445)
*Thanh Cong Ho,Farah Kharrat,Abderrazek Abid,Fakhri Karray*

Main category: cs.CL

TL;DR: 该论文提出了一种名为REMONI的远程健康监测系统，结合多模态大语言模型、物联网和可穿戴设备，实现对患者生命体征及行为的自动连续监测和异常检测。


<details>
  <summary>Details</summary>
Motivation: 随着可穿戴设备的广泛应用，远程患者监测需求增加，但现有研究多关注数据采集与分析，缺乏有效的人机交互。

Method: 系统集成传感器数据采集、异常检测模块（包括跌倒检测）、基于多模态大语言模型的自然语言处理，实现对患者活动、情绪识别及医疗人员问询响应。

Result: 系统通过实验验证具有良好的实施性和可扩展性，能够实时向医护人员提供患者健康及情绪状态信息，有助于减轻工作负担降低成本。

Conclusion: REMONI系统有效填补了远程监测中人机交互的空白，具备实用性和扩展潜力，未来可助力医疗行业提升效率和服务质量。

Abstract: With the widespread adoption of wearable devices in our daily lives, the
demand and appeal for remote patient monitoring have significantly increased.
Most research in this field has concentrated on collecting sensor data,
visualizing it, and analyzing it to detect anomalies in specific diseases such
as diabetes, heart disease and depression. However, this domain has a notable
gap in the aspect of human-machine interaction. This paper proposes REMONI, an
autonomous REmote health MONItoring system that integrates multimodal large
language models (MLLMs), the Internet of Things (IoT), and wearable devices.
The system automatically and continuously collects vital signs, accelerometer
data from a special wearable (such as a smartwatch), and visual data in patient
video clips collected from cameras. This data is processed by an anomaly
detection module, which includes a fall detection model and algorithms to
identify and alert caregivers of the patient's emergency conditions. A
distinctive feature of our proposed system is the natural language processing
component, developed with MLLMs capable of detecting and recognizing a
patient's activity and emotion while responding to healthcare worker's
inquiries. Additionally, prompt engineering is employed to integrate all
patient information seamlessly. As a result, doctors and nurses can access
real-time vital signs and the patient's current state and mood by interacting
with an intelligent agent through a user-friendly web application. Our
experiments demonstrate that our system is implementable and scalable for
real-life scenarios, potentially reducing the workload of medical professionals
and healthcare costs. A full-fledged prototype illustrating the functionalities
of the system has been developed and being tested to demonstrate the robustness
of its various capabilities.

</details>


### [32] [MRO: Enhancing Reasoning in Diffusion Language Models via Multi-Reward Optimization](https://arxiv.org/abs/2510.21473)
*Chenglong Wang,Yang Gan,Hang Zhou,Chi Hu,Yongyu Mu,Kai Song,Murun Yang,Bei Li,Chunliang Zhang,Tongran Liu,Jingbo Zhu,Zhengtao Yu,Tong Xiao*

Main category: cs.CL

TL;DR: 本文提出了一种多奖励优化方法，提升扩散语言模型在推理任务中的表现和采样效率。


<details>
  <summary>Details</summary>
Motivation: 当前扩散语言模型虽然是自回归模型的有力替代，但在推理性能上仍落后，主要因为去噪步骤中的词元独立生成未能捕捉词元之间的相关性。

Method: 定义了序列内和序列间两种词元相关性，并通过多奖励优化方法（结合测试时缩放、拒绝采样和强化学习）显式优化词元相关性，同时引入组步长和重要性采样策略以减少奖励方差和提升采样效率。

Result: 实验表明该方法显著提升了推理性能，同时在保持高性能的基础上实现了采样速度的大幅提升。

Conclusion: 多奖励优化有效提升了扩散语言模型在推理任务上的表现和采样效率，证明了考虑词元相关性的重要性。

Abstract: Recent advances in diffusion language models (DLMs) have presented a
promising alternative to traditional autoregressive large language models
(LLMs). However, DLMs still lag behind LLMs in reasoning performance,
especially as the number of denoising steps decreases. Our analysis reveals
that this shortcoming arises primarily from the independent generation of
masked tokens across denoising steps, which fails to capture the token
correlation. In this paper, we define two types of token correlation:
intra-sequence correlation and inter-sequence correlation, and demonstrate that
enhancing these correlations improves reasoning performance. To this end, we
propose a Multi-Reward Optimization (MRO) approach, which encourages DLMs to
consider the token correlation during the denoising process. More specifically,
our MRO approach leverages test-time scaling, reject sampling, and
reinforcement learning to directly optimize the token correlation with multiple
elaborate rewards. Additionally, we introduce group step and importance
sampling strategies to mitigate reward variance and enhance sampling
efficiency. Through extensive experiments, we demonstrate that MRO not only
improves reasoning performance but also achieves significant sampling speedups
while maintaining high performance on reasoning benchmarks.

</details>


### [33] [Brain-tuning Improves Generalizability and Efficiency of Brain Alignment in Speech Models](https://arxiv.org/abs/2510.21520)
*Omer Moussa,Mariya Toneva*

Main category: cs.CL

TL;DR: 本文提出了一种可扩展且可泛化的多参与者脑调优方法，通过对预训练语音语言模型进行微调，联合预测多名参与者的fMRI数据，实现了更高效的数据利用和更强的脑对齐性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法对参与者依赖性强，所需数据量大，难以推广到新参与者和进行群体水平分析。

Method: 提出多参与者脑调优方法，联合对预训练语音语言模型进行微调，使其能预测多名参与者的fMRI反应。

Result: 该方法减少了新参与者所需fMRI数据5倍，提升脑对齐度最高达50%，并在新数据集上表现出良好泛化能力.

Conclusion: 多参与者脑调优不仅提升了脑对齐性能，还改善了下游语义任务表现，促进了神经科学与AI的交叉进展。

Abstract: Pretrained language models are remarkably effective in aligning with human
brain responses elicited by natural language stimuli, positioning them as
promising model organisms for studying language processing in the brain.
However, existing approaches for both estimating and improving this brain
alignment are participant-dependent and highly affected by the amount of data
available per participant, hindering both generalization to new participants
and population-level analyses. In this work, we address these limitations by
introducing a scalable, generalizable brain-tuning method, in which we
fine-tune pretrained speech language models to jointly predict fMRI responses
from multiple participants. We demonstrate that the resulting brain-tuned
models exhibit strong individual brain alignment while generalizing across
participants. Specifically, our method leads to 1) a 5-fold decrease in the
amount of fMRI data needed to predict brain data from new participants, 2) up
to a 50% increase in the overall brain alignment, and 3) strong generalization
to new unseen datasets. Furthermore, this multi-participant brain-tuning
additionally improves downstream performance on semantic tasks, suggesting that
training using brain data from multiple participants leads to more
generalizable semantic representations. Taken together, these findings
demonstrate a bidirectional benefit between neuroscience and AI, helping bridge
the gap between the two fields. We make our code and models publicly available
at https://github.com/bridge-ai-neuro/multi-brain-tuning.

</details>


### [34] [InterpDetect: Interpretable Signals for Detecting Hallucinations in Retrieval-Augmented Generation](https://arxiv.org/abs/2510.21538)
*Likun Tan,Kuan-Wei Huang,Joy Shi,Kevin Wu*

Main category: cs.CL

TL;DR: 本文研究了检索增强生成模型（RAG）中的幻觉现象，提出基于外部上下文和参数知识得分的机制性检测方法，实现了高效且具泛化能力的幻觉检测。


<details>
  <summary>Details</summary>
Motivation: RAG模型虽然通过整合外部知识减少幻觉生成，但仍存在输出与检索内容不一致的问题，且传统方法难以区分外部上下文和参数知识对幻觉的影响。

Method: 作者分析了幻觉产生机制，发现后层FFN模块过度注入参数知识导致幻觉，设计基于层和注意力头的外部上下文得分与参数知识得分，使用回归分类器训练幻觉检测模型。

Result: 该方法在Qwen3-0.6b模型上表现优异，优于多种基线检测方法，并成功泛化至GPT-4.1-mini，显示了代理模型评估的潜力。

Conclusion: 利用机制性信号进行幻觉检测不仅高效且具备良好泛化能力，为RAG系统提供了可靠的幻觉检测手段。

Abstract: Retrieval-Augmented Generation (RAG) integrates external knowledge to
mitigate hallucinations, yet models often generate outputs inconsistent with
retrieved content. Accurate hallucination detection requires disentangling the
contributions of external context and parametric knowledge, which prior methods
typically conflate. We investigate the mechanisms underlying RAG hallucinations
and find they arise when later-layer FFN modules disproportionately inject
parametric knowledge into the residual stream. To address this, we explore a
mechanistic detection approach based on external context scores and parametric
knowledge scores. Using Qwen3-0.6b, we compute these scores across layers and
attention heads and train regression-based classifiers to predict
hallucinations. Our method is evaluated against state-of-the-art LLMs (GPT-5,
GPT-4.1) and detection baselines (RAGAS, TruLens, RefChecker). Furthermore,
classifiers trained on Qwen3-0.6b signals generalize to GPT-4.1-mini responses,
demonstrating the potential of proxy-model evaluation. Our results highlight
mechanistic signals as efficient, generalizable predictors for hallucination
detection in RAG systems.

</details>


### [35] [Document Understanding, Measurement, and Manipulation Using Category Theory](https://arxiv.org/abs/2510.21553)
*Jared Claypoole,Yunye Gong,Noson S. Yanofsky,Ajay Divakaran*

Main category: cs.CL

TL;DR: 本文应用范畴论提取多模态文档结构，开发信息测量、内容总结与扩展，以及大预训练模型的自监督改进方法。


<details>
  <summary>Details</summary>
Motivation: 通过范畴论形式化文档结构，解决信息测量、内容总结、文档扩展和模型提升等问题。

Method: 将文档表示为问答对的范畴，设计正交化过程分割信息，提出信息测量和枚举方法，开发基于此的总结与释义技术，结合大预训练模型，提出多模态拓展和基于RLVR的一致性自监督改进方法。

Result: 成功构建了数学文档表示和信息分割技术，提出新的总结和文档扩展方法，实现了多模态拓展，并通过 RLVR 方法提升了大型预训练模型表现。

Conclusion: 范畴论框架有效支撑文档信息分析与处理，推动内容总结、扩展及模型自监督改进，具有广泛应用前景。

Abstract: We apply category theory to extract multimodal document structure which leads
us to develop information theoretic measures, content summarization and
extension, and self-supervised improvement of large pretrained models. We first
develop a mathematical representation of a document as a category of
question-answer pairs. Second, we develop an orthogonalization procedure to
divide the information contained in one or more documents into non-overlapping
pieces. The structures extracted in the first and second steps lead us to
develop methods to measure and enumerate the information contained in a
document. We also build on those steps to develop new summarization techniques,
as well as to develop a solution to a new problem viz. exegesis resulting in an
extension of the original document. Our question-answer pair methodology
enables a novel rate distortion analysis of summarization techniques. We
implement our techniques using large pretrained models, and we propose a
multimodal extension of our overall mathematical framework. Finally, we develop
a novel self-supervised method using RLVR to improve large pretrained models
using consistency constraints such as composability and closure under certain
operations that stem naturally from our category theoretic framework.

</details>


### [36] [Are the LLMs Capable of Maintaining at Least the Language Genus?](https://arxiv.org/abs/2510.21561)
*Sandra Mitrović,David Kletz,Ljiljana Dolamic,Fabio Rinaldi*

Main category: cs.CL

TL;DR: 本文探讨大型语言模型对语言族群结构的敏感性，发现语言模型在多语言表现中会受到语言族群的影响，但训练数据资源的多少对效果影响更大。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在多语言环境下，语言族群结构对其多语言行为的影响，目前相关研究较少。

Method: 基于MultiQ数据集，分析模型在提示语言不一致时是否倾向于切换到族群相关语言，及模型在族群内外的知识一致性表现。

Result: 发现语言族群层面的效果存在，但受到训练资源的强烈影响；不同语言模型家族表现出不同的多语言策略。

Conclusion: 大型语言模型编码了语言族群结构的部分信息，但训练数据不均衡是影响多语言性能的主要因素。

Abstract: Large Language Models (LLMs) display notable variation in multilingual
behavior, yet the role of genealogical language structure in shaping this
variation remains underexplored. In this paper, we investigate whether LLMs
exhibit sensitivity to linguistic genera by extending prior analyses on the
MultiQ dataset. We first check if models prefer to switch to genealogically
related languages when prompt language fidelity is not maintained. Next, we
investigate whether knowledge consistency is better preserved within than
across genera. We show that genus-level effects are present but strongly
conditioned by training resource availability. We further observe distinct
multilingual strategies across LLMs families. Our findings suggest that LLMs
encode aspects of genus-level structure, but training data imbalances remain
the primary factor shaping their multilingual performance.

</details>


### [37] [From Polyester Girlfriends to Blind Mice: Creating the First Pragmatics Understanding Benchmarks for Slovene](https://arxiv.org/abs/2510.21575)
*Mojca Brglez,Špela Vintar*

Main category: cs.CL

TL;DR: 本文介绍了针对斯洛文尼亚语的首个语用理解基准测试集SloPragEval和SloPragMega，评估大语言模型在理解语用层面的表现。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力的提升，需要设计更具挑战性的评估方法，超越语法和语义，考察模型对语用和文化背景的理解能力。

Method: 构建包含405道多项选择题的斯洛文尼亚语语用理解基准，讨论翻译难点，建立人类基线，并对大语言模型进行初步评估。

Result: 当前模型在捕捉语言细微差别上有较大进步，但在推断非字面含义和文化特定表达时仍表现不足，且专有模型优于开源模型。

Conclusion: 强调语用理解和文化知识的基准测试需谨慎设计，最好基于母语数据并通过人类响应进行验证，以推动语言模型更深入的语用理解。

Abstract: Large language models are demonstrating increasing capabilities, excelling at
benchmarks once considered very difficult. As their capabilities grow, there is
a need for more challenging evaluations that go beyond surface-level linguistic
competence. Namely, language competence involves not only syntax and semantics
but also pragmatics, i.e., understanding situational meaning as shaped by
context as well as linguistic and cultural norms. To contribute to this line of
research, we introduce SloPragEval and SloPragMega, the first pragmatics
understanding benchmarks for Slovene that contain altogether 405
multiple-choice questions. We discuss the difficulties of translation, describe
the campaign to establish a human baseline, and report pilot evaluations with
LLMs. Our results indicate that current models have greatly improved in
understanding nuanced language but may still fail to infer implied speaker
meaning in non-literal utterances, especially those that are culture-specific.
We also observe a significant gap between proprietary and open-source models.
Finally, we argue that benchmarks targeting nuanced language understanding and
knowledge of the target culture must be designed with care, preferably
constructed from native data, and validated with human responses.

</details>


### [38] [Automated Quality Control for Language Documentation: Detecting Phonotactic Inconsistencies in a Kokborok Wordlist](https://arxiv.org/abs/2510.21584)
*Kellen Parker van Dam,Abishek Stephen*

Main category: cs.CL

TL;DR: 本文提出了无监督异常检测方法，通过分析词汇列表中的音位结构异常，识别语料中的转录错误和借用词，帮助改进低资源语言文档的数据质量。


<details>
  <summary>Details</summary>
Motivation: 语言文档中的词汇数据常包含转录错误和未记录的借用词，可能误导语言分析，因此需要有效方法检测这些异常。

Method: 利用字符级和音节级音位结构特征，应用无监督异常检测算法于多语言数据集，尤其是Kokborok方言与孟加拉语的混合数据。

Result: 音节级特征在识别潜在转录错误和借用词方面显著优于字符级方法，尽管精准率和召回率有限，但高召回策略能有效辅助现场工作者标记需核查条目。

Conclusion: 基于音节的无监督异常检测为低资源语言词汇数据的质量控制提供了系统化工具，促进语言文档的准确性和可靠性提升。

Abstract: Lexical data collection in language documentation often contains
transcription errors and undocumented borrowings that can mislead linguistic
analysis. We present unsupervised anomaly detection methods to identify
phonotactic inconsistencies in wordlists, applying them to a multilingual
dataset of Kokborok varieties with Bangla. Using character-level and
syllable-level phonotactic features, our algorithms identify potential
transcription errors and borrowings. While precision and recall remain modest
due to the subtle nature of these anomalies, syllable-aware features
significantly outperform character-level baselines. The high-recall approach
provides fieldworkers with a systematic method to flag entries requiring
verification, supporting data quality improvement in low-resourced language
documentation.

</details>


### [39] [RETuning: Upgrading Inference-Time Scaling for Stock Movement Prediction with Large Language Models](https://arxiv.org/abs/2510.21604)
*Xueyuan Lin,Cehao Yang,Ye Ma,Ming Li,Rongjunchen Zhang,Yang Ni,Xiaojun Wu,Chengjin Xu,Jian Guo,Hui Xiong*

Main category: cs.CL

TL;DR: 该论文研究了大语言模型（LLMs）在股价走势预测中的应用，提出了一种反思证据调优方法（RETuning），以提升模型的独立逻辑推理能力和预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在金融任务中表现不足，特别是在股价三分类预测中，模型倾向于盲目跟从分析师意见，缺乏系统化、独立的分析逻辑，且无法有效权衡对立证据，影响预测可靠性。

Method: 提出RETuning方法，在强化学习前对模型进行调优，以鼓励模型动态构建多源信息的分析框架，组织并评分支持股价涨跌的证据，最终通过反思形成预测，实现逻辑独立性和减少上下文影响。

Result: 构建了包含5123只股票全年数据的大规模多模态数据集，实验证明RETuning方法显著激发了模型在金融领域的推理能力，且模型在半年后及异质股票上的推理能力仍然有效。

Conclusion: 通过RETuning，模型实现了更系统化和独立的逻辑推理，提升了股价趋势预测的可靠性和泛化能力，展示了LLMs在金融领域潜在价值。

Abstract: Recently, large language models (LLMs) have demonstrated outstanding
reasoning capabilities on mathematical and coding tasks. However, their
application to financial tasks-especially the most fundamental task of stock
movement prediction-remains underexplored. We study a three-class
classification problem (up, hold, down) and, by analyzing existing reasoning
responses, observe that: (1) LLMs follow analysts' opinions rather than exhibit
a systematic, independent analytical logic (CoTs). (2) LLMs list summaries from
different sources without weighing adversarial evidence, yet such
counterevidence is crucial for reliable prediction. It shows that the model
does not make good use of its reasoning ability to complete the task. To
address this, we propose Reflective Evidence Tuning (RETuning), a cold-start
method prior to reinforcement learning, to enhance prediction ability. While
generating CoT, RETuning encourages dynamically constructing an analytical
framework from diverse information sources, organizing and scoring evidence for
price up or down based on that framework-rather than on contextual
viewpoints-and finally reflecting to derive the prediction. This approach
maximally aligns the model with its learned analytical framework, ensuring
independent logical reasoning and reducing undue influence from context. We
also build a large-scale dataset spanning all of 2024 for 5,123 A-share stocks,
with long contexts (32K tokens) and over 200K samples. In addition to price and
news, it incorporates analysts' opinions, quantitative reports, fundamental
data, macroeconomic indicators, and similar stocks. Experiments show that
RETuning successfully unlocks the model's reasoning ability in the financial
domain. Inference-time scaling still works even after 6 months or on
out-of-distribution stocks, since the models gain valuable insights about stock
movement prediction.

</details>


### [40] [The Universal Landscape of Human Reasoning](https://arxiv.org/abs/2510.21623)
*Qiguang Chen,Jinhao Liu,Libo Qin,Yimeng Zhang,Yihao Liang,Shangxu Ren,Chengyu Luan,Dengyun Peng,Hanjing Li,Jiannan Guan,Zheng Yan,Jiaqi Wang,Mengkang Hu,Yantao Du,Zhi Chen,Xie Chen,Wanxiang Che*

Main category: cs.CL

TL;DR: 本文提出了信息流追踪（IF-Track）方法，利用大型语言模型作为概率编码器，定量分析推理过程中信息熵和增益，实现了对人类推理动态的统一量化描述。


<details>
  <summary>Details</summary>
Motivation: 现有关于人类推理的理论多聚焦输出或个别模型，缺乏一个统一且量化的描述方法来解释人类推理的动态过程。

Method: 使用大型语言模型作为概率编码器，通过信息熵与信息增益量化推理每一步的信息流动，进行细粒度分析多种任务中的推理行为。

Result: IF-Track成功模拟了人类推理行为的通用特征，捕捉关键推理特征，识别系统性错误模式，并刻画个体差异。

Conclusion: IF-Track在单处理与双处理理论间提供协调，揭示了人工智能与人类认知的对齐，以及大型语言模型如何影响人类推理，建立了理论与测量之间的定量桥梁，深化了对推理架构的理解。

Abstract: Understanding how information is dynamically accumulated and transformed in
human reasoning has long challenged cognitive psychology, philosophy, and
artificial intelligence. Existing accounts, from classical logic to
probabilistic models, illuminate aspects of output or individual modelling, but
do not offer a unified, quantitative description of general human reasoning
dynamics. To solve this, we introduce Information Flow Tracking (IF-Track),
that uses large language models (LLMs) as probabilistic encoder to quantify
information entropy and gain at each reasoning step. Through fine-grained
analyses across diverse tasks, our method is the first successfully models the
universal landscape of human reasoning behaviors within a single metric space.
We show that IF-Track captures essential reasoning features, identifies
systematic error patterns, and characterizes individual differences. Applied to
discussion of advanced psychological theory, we first reconcile single- versus
dual-process theories in IF-Track and discover the alignment of artificial and
human cognition and how LLMs reshaping human reasoning process. This approach
establishes a quantitative bridge between theory and measurement, offering
mechanistic insights into the architecture of reasoning.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [41] [\textsc{autoresearcher}: Automating Knowledge-Grounded and Transparent Research Ideation with Multi-Agent Collaboration](https://arxiv.org/abs/2510.20844)
*Jiawei Zhou,Ruicheng Zhu,Mengshi Chen,Jianwei Wang,Kai Wang*

Main category: cs.MA

TL;DR: 本文提出了一种名为AutoResearcher的多智能体系统，实现了基于文献的知识驱动透明研究构想，支持多阶段审查和多样化创新，并可应用于任意科学领域。


<details>
  <summary>Details</summary>
Motivation: 现有自动化文献研究系统缺乏透明性和研究者控制，生成的创意可信度较弱，限制了有效研究的推进。

Method: 设计了包含知识整理、多样化创意生成、多阶段创意筛选及专家评审与综合等四个阶段的统一框架，支持中间过程的可视化和调节，实现证据支持下的多样创意生成。

Result: 该系统在图挖掘的$k$-truss断裂问题案例中验证，成功生成了多种有证据支持且相互独立的假设与批评。

Conclusion: AutoResearcher有效提升了文献研究的透明度和创新性，其领域无关设计使其能够广泛应用于多种科学研究领域。

Abstract: Effective research relies on organizing extensive information and stimulating
novel solutions. Agentic systems have recently emerged as a promising tool to
automate literature-based ideation. However, current systems often remain
black-box. Their outputs may appear plausible but weakly grounded, with limited
transparency or control for researchers. Our work introduces
\textsc{autoresearcher}, a multi-agent demo system for knowledge-grounded and
transparent ideation. Specifically, \textsc{autoresearcher} integrates
meticulously designed four stages into a unified framework: (A) Structured
Knowledge Curation, (B) Diversified Idea Generation, (C) Multi-stage Idea
Selection, and (D) Expert Panel Review \& Synthesis. Different from prior
pipelines, our system not only exposes intermediate reasoning states, execution
logs, and tunable agents for inspections, but also enables the generation of
hypotheses that are both diverse and evidence-aligned. Our design is also
domain-agnostic: as long as literature sources exist, the same pipeline can be
instantiated in any scientific field. As an illustrative case, we demonstrate
\textsc{autoresearcher} on a graph-mining case study ($k$-truss breaking
problem), where it generates distinct, plausible hypotheses with evidence and
critiques. A live demo and source code are available at
https://github.com/valleysprings/AutoResearcher.

</details>


### [42] [HIKMA: Human-Inspired Knowledge by Machine Agents through a Multi-Agent Framework for Semi-Autonomous Scientific Conferences](https://arxiv.org/abs/2510.21370)
*Zain Ul Abideen Tariq,Mahmood Al-Zubaidi,Uzair Shah,Marco Agus,Mowafa Househ*

Main category: cs.MA

TL;DR: HIKMA大会利用人工智能整合学术交流全流程，展示AI辅助学术写作和评审的可能性，维护学术诚信和知识产权。


<details>
  <summary>Details</summary>
Motivation: 重新构想学术交流方式，探索AI在学术出版和展示中的应用，提升效率和透明度。

Method: 设计并实现HIKMA框架，涵盖AI数据集整理、AI论文生成、AI辅助评审、AI驱动修订、AI会议展示及AI档案传播。结合语言模型、结构化研究流程和领域保护措施。

Result: 展示了AI支持传统学术实践的可行性，维护了知识产权保护、透明度和学术诚信。大会作为测试平台，揭示了AI学术交流的机会与挑战。

Conclusion: AI可作为学术研究的辅助工具，而非替代者，强调人机协作的重要性，并探讨AI作者身份与责任问题。

Abstract: HIKMA Semi-Autonomous Conference is the first experiment in reimagining
scholarly communication through an end-to-end integration of artificial
intelligence into the academic publishing and presentation pipeline. This paper
presents the design, implementation, and evaluation of the HIKMA framework,
which includes AI dataset curation, AI-based manuscript generation, AI-assisted
peer review, AI-driven revision, AI conference presentation, and AI archival
dissemination. By combining language models, structured research workflows, and
domain safeguards, HIKMA shows how AI can support - not replace traditional
scholarly practices while maintaining intellectual property protection,
transparency, and integrity. The conference functions as a testbed and proof of
concept, providing insights into the opportunities and challenges of AI-enabled
scholarship. It also examines questions about AI authorship, accountability,
and the role of human-AI collaboration in research.

</details>


### [43] [ColorEcosystem: Powering Personalized, Standardized, and Trustworthy Agentic Service in massive-agent Ecosystem](https://arxiv.org/abs/2510.21566)
*Fangwen Wu,Zheng Wu,Jihong Wang,Yunku Chen,Ruiguang Pei,Heyuan Huang,Xin Liao,Xingyu Lou,Huarong Deng,Zhihui Fu,Weiwen Liu,Zhuosheng Zhang,Weinan Zhang,Jun Wang*

Main category: cs.MA

TL;DR: 提出了ColorEcosystem，一个支持个性化、标准化和可信赖服务的多代理生态系统框架。


<details>
  <summary>Details</summary>
Motivation: 多代理系统发展为大规模代理生态系统，但面临服务体验缺乏个性化、标准化不足和不可信等挑战。

Method: 设计ColorEcosystem，包括三大组件：代理载体（提供个性化服务和数字孪生）、代理商店（集中标准化管理代理服务）、代理审计（监督开发者和用户行为确保诚信）。

Result: 通过理论分析和部分功能实现，展示了ColorEcosystem提升服务个性化、标准化和可信度的潜力。相关代码已开源。

Conclusion: ColorEcosystem为大规模多代理生态系统提供了一个可行的、可信赖且个性化的服务管理蓝图，有望推动该领域进一步发展。

Abstract: With the rapid development of (multimodal) large language model-based agents,
the landscape of agentic service management has evolved from single-agent
systems to multi-agent systems, and now to massive-agent ecosystems. Current
massive-agent ecosystems face growing challenges, including impersonal service
experiences, a lack of standardization, and untrustworthy behavior. To address
these issues, we propose ColorEcosystem, a novel blueprint designed to enable
personalized, standardized, and trustworthy agentic service at scale.
Concretely, ColorEcosystem consists of three key components: agent carrier,
agent store, and agent audit. The agent carrier provides personalized service
experiences by utilizing user-specific data and creating a digital twin, while
the agent store serves as a centralized, standardized platform for managing
diverse agentic services. The agent audit, based on the supervision of
developer and user activities, ensures the integrity and credibility of both
service providers and users. Through the analysis of challenges, transitional
forms, and practical considerations, the ColorEcosystem is poised to power
personalized, standardized, and trustworthy agentic service across
massive-agent ecosystems. Meanwhile, we have also implemented part of
ColorEcosystem's functionality, and the relevant code is open-sourced at
https://github.com/opas-lab/color-ecosystem.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [44] [AgentArcEval: An Architecture Evaluation Method for Foundation Model based Agents](https://arxiv.org/abs/2510.21031)
*Qinghua Lu,Dehai Zhao,Yue Liu,Hao Zhang,Liming Zhu,Xiwei Xu,Angela Shi,Tristan Tan,Rick Kazman*

Main category: cs.SE

TL;DR: 本文提出了AgentArcEval，一种专门用于评估基于基础模型的智能体架构的方法，并通过真实案例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 基础模型的智能体具有复杂架构、自主非确定行为和持续进化的特性，传统架构评估方法难以满足其需求。

Method: 提出了AgentArcEval评估方法及一套智能体专用的通用场景目录，用于指导具体场景生成，设计和评估智能体架构。

Result: 通过对一个真实的税务辅助智能体Luna的架构评估案例，验证了AgentArcEval和场景目录的实用性。

Conclusion: AgentArcEval有效解决了基础模型智能体架构评估的复杂性，提升了评估质量，促进智能体架构设计优化。

Abstract: The emergence of foundation models (FMs) has enabled the development of
highly capable and autonomous agents, unlocking new application opportunities
across a wide range of domains. Evaluating the architecture of agents is
particularly important as the architectural decisions significantly impact the
quality attributes of agents given their unique characteristics, including
compound architecture, autonomous and non-deterministic behaviour, and
continuous evolution. However, these traditional methods fall short in
addressing the evaluation needs of agent architecture due to the unique
characteristics of these agents. Therefore, in this paper, we present
AgentArcEval, a novel agent architecture evaluation method designed specially
to address the complexities of FM-based agent architecture and its evaluation.
Moreover, we present a catalogue of agent-specific general scenarios, which
serves as a guide for generating concrete scenarios to design and evaluate the
agent architecture. We demonstrate the usefulness of AgentArcEval and the
catalogue through a case study on the architecture evaluation of a real-world
tax copilot, named Luna.

</details>


### [45] [BDiff: Block-aware and Accurate Text-based Code Differencing](https://arxiv.org/abs/2510.21094)
*Yao Lu,Wanwei Liu,Tanghaoran Zhang,Kang Yang,Yang Zhang,Wenyu Xu,Longfei Sun,Xinjun Mao,Shuzheng Gao,Michael R. Lyu*

Main category: cs.SE

TL;DR: BDiff是一种能识别多行代码块编辑操作的新型文本差异算法，提升代码变更理解效率。


<details>
  <summary>Details</summary>
Motivation: 现有文本差异工具难以准确识别跨多行的编辑操作，降低开发者理解代码变更的效率。

Method: BDiff通过构建可能的行映射和块映射集合，并利用Kuhn-Munkres算法计算最优映射，最小化编辑脚本长度且贴近开发者意图。

Result: 实验表明BDiff在编辑脚本大小、结果质量和运行时间上均优于五种先进工具（包含大型语言模型）。同时，LLMs在代码差异任务中表现不佳且效率低。

Conclusion: BDiff能够提高代码差分结果质量和运行效率，解决多行块级编辑识别难题，支持基于web的可视化差异工具实现。

Abstract: Code differencing is a fundamental technique in software engineering practice
and research. While researchers have proposed text-based differencing
techniques capable of identifying line changes over the past decade, existing
methods exhibit a notable limitation in identifying edit actions (EAs) that
operate on text blocks spanning multiple lines. Such EAs are common in
developers' practice, such as moving a code block for conditional branching or
duplicating a method definition block for overloading. Existing tools represent
such block-level operations as discrete sequences of line-level EAs, compelling
developers to manually correlate them and thereby substantially impeding the
efficiency of change comprehension. To address this issue, we propose BDiff, a
text-based differencing algorithm capable of identifying two types of
block-level EAs and five types of line-level EAs. Building on traditional
differencing algorithms, we first construct a candidate set containing all
possible line mappings and block mappings. Leveraging the Kuhn-Munkres
algorithm, we then compute the optimal mapping set that can minimize the size
of the edit script (ES) while closely aligning with the original developer's
intent. To validate the effectiveness of BDiff, we selected five
state-of-the-art tools, including large language models (LLMs), as baselines
and adopted a combined qualitative and quantitative approach to evaluate their
performance in terms of ES size, result quality, and running time. Experimental
results show that BDiff produces higher-quality differencing results than
baseline tools while maintaining competitive runtime performance. Our
experiments also show the unreliability of LLMs in code differencing tasks
regarding result quality and their infeasibility in terms of runtime
efficiency. We have implemented a web-based visual differencing tool.

</details>


### [46] [R2ComSync: Improving Code-Comment Synchronization with In-Context Learning and Reranking](https://arxiv.org/abs/2510.21106)
*Zhen Yang,Hongyi Lin,Xiao Yu,Jacky Wai Keung,Shuo Liu,Pak Yuen Patrick Chan,Yicheng Sun,Fengji Zhang*

Main category: cs.SE

TL;DR: 本文提出了基于大语言模型的代码-注释同步新方法R2ComSync，结合检索和重排序策略，有效提升注释同步质量。


<details>
  <summary>Details</summary>
Motivation: 现有代码-注释同步方法存在泛化能力差和需要大量特定任务学习资源的问题，且大语言模型在该领域表现不佳，缺乏有效的示例和候选优先排序。

Method: R2ComSync采用基于上下文学习的框架，结合混合检索（同时考虑代码-注释语义和变更模式相似度）构建示例，并通过多轮重排序策略利用三条重排序规则提升正确候选优先级。

Result: 通过在涵盖Java和Python的三个数据集上对五个最新大语言模型的评测，R2ComSync明显优于五个最先进方法，且生成的注释质量显著提升。

Conclusion: R2ComSync有效利用检索与重排序机制，增强大语言模型在代码-注释同步任务中的表现，显著提升了注释的准确性和质量。

Abstract: Code-Comment Synchronization (CCS) aims to synchronize the comments with code
changes in an automated fashion, thereby significantly reducing the workload of
developers during software maintenance and evolution. While previous studies
have proposed various solutions that have shown success, they often exhibit
limitations, such as a lack of generalization ability or the need for extensive
task-specific learning resources. This motivates us to investigate the
potential of Large Language Models (LLMs) in this area. However, a pilot
analysis proves that LLMs fall short of State-Of-The-Art (SOTA) CCS approaches
because (1) they lack instructive demonstrations for In-Context Learning (ICL)
and (2) many correct-prone candidates are not prioritized.To tackle the above
challenges, we propose R2ComSync, an ICL-based code-Comment Synchronization
approach enhanced with Retrieval and Re-ranking. Specifically, R2ComSync
carries corresponding two novelties: (1) Ensemble hybrid retrieval. It equally
considers the similarity in both code-comment semantics and change patterns
when retrieval, thereby creating ICL prompts with effective examples. (2)
Multi-turn re-ranking strategy. We derived three significant rules through
large-scale CCS sample analysis. Given the inference results of LLMs, it
progressively exploits three re-ranking rules to prioritize relatively
correct-prone candidates. We evaluate R2ComSync using five recent LLMs on three
CCS datasets covering both Java and Python programming languages, and make
comparisons with five SOTA approaches. Extensive experiments demonstrate the
superior performance of R2ComSync against other approaches. Moreover, both
quantitative and qualitative analyses provide compelling evidence that the
comments synchronized by our proposal exhibit significantly higher quality.}

</details>


### [47] [GreenMalloc: Allocator Optimisation for Industrial Workloads](https://arxiv.org/abs/2510.21405)
*Aidan Dakhama,W. B. Langdon,Hector D. Menendez,Karine Even-Mendoza*

Main category: cs.SE

TL;DR: 提出GreenMalloc框架，通过多目标搜索优化内存分配器配置，显著降低内存占用且保持运行效率。


<details>
  <summary>Details</summary>
Motivation: 自动配置内存分配器参数以提升内存使用效率和性能。

Method: 采用NSGA II算法结合轻量级代理基准工具rand_malloc，从执行轨迹中探索最佳分配器参数，再在gem5模拟器验证。

Result: 在GNU glibc malloc和Google TCMalloc两种分配器上测试，平均堆内存使用量降低最高4.1%，运行时效率略降0.25%。

Conclusion: GreenMalloc能有效优化内存分配器配置，实现内存占用降低且性能保持稳定。

Abstract: We present GreenMalloc, a multi objective search-based framework for
automatically configuring memory allocators. Our approach uses NSGA II and
rand_malloc as a lightweight proxy benchmarking tool. We efficiently explore
allocator parameters from execution traces and transfer the best configurations
to gem5, a large system simulator, in a case study on two allocators: the GNU
C/CPP compiler's glibc malloc and Google's TCMalloc. Across diverse workloads,
our empirical results show up to 4.1 percantage reduction in average heap usage
without loss of runtime efficiency; indeed, we get a 0.25 percantage reduction.

</details>


### [48] [Context Engineering for AI Agents in Open-Source Software](https://arxiv.org/abs/2510.21413)
*Seyedmoein Mohsenimofidi,Matthias Galster,Christoph Treude,Sebastian Baltes*

Main category: cs.SE

TL;DR: 本文研究了AI配置文件在开源软件项目中的采用情况，特别是AGENTS.md文件的使用，发现目前缺乏统一结构且内容形式多样。


<details>
  <summary>Details</summary>
Motivation: 下一代基于智能体的代码助手需要足够的上下文信息以符合项目规范，但现有研究对开发者如何采纳AI配置文件了解有限。

Method: 对466个开源项目中的AI配置文件进行了初步实证分析，调查这些文件中包含的内容、形式及其随时间的演变。

Result: 结果显示AI配置文件结构尚不统一，内容涵盖描述性、规范性、禁止性、解释性和条件性信息，且文件持续被修改和维护。

Conclusion: AI配置文件的采用为研究实际的提示和上下文工程提供了宝贵机会，未来研究应关注结构和表现形式的优化以提升生成内容质量。

Abstract: GenAI-based coding assistants have disrupted software development. Their next
generation is agent-based, operating with more autonomy and potentially without
human oversight. One challenge is to provide AI agents with sufficient context
about the software projects they operate in. Like humans, AI agents require
contextual information to develop solutions that are in line with the target
architecture, interface specifications, coding guidelines, standard workflows,
and other project-specific policies. Popular AI agents for software development
(e.g., Claude Code) advocate for maintaining tool-specific version-controlled
Markdown files that cover aspects such as the project structure, building and
testing, or code style. The content of these files is automatically added to
each prompt. AGENTS.md has emerged as a potential standard that consolidates
tool-specific formats. However, little is known about whether and how
developers adopt this format. Therefore, in this paper, we present the results
of a preliminary study investigating the adoption of AI configuration files in
466 open-source software projects, what information developers provide in these
files, how they present that information, and how they evolve over time. Our
findings indicate that there is no established structure yet, and that there is
a lot of variation in terms of how context is provided (descriptive,
prescriptive, prohibitive, explanatory, conditional). We see great potential in
studying which modifications in structure or presentation can positively affect
the quality of the generated content. Finally, our analysis of commits that
have modified AGENTS.md files provides first insights into how projects
continuously extend and maintain these files. We conclude the paper by
outlining how the adoption of AI configuration files in provides a unique
opportunity to study real-world prompt and context engineering.

</details>


### [49] [Does Model Size Matter? A Comparison of Small and Large Language Models for Requirements Classification](https://arxiv.org/abs/2510.21443)
*Mohammad Amin Zadenoori,Vincenzo De Martino,Jacek Dabrowski,Xavier Franch,Alessio Ferrari*

Main category: cs.SE

TL;DR: 该论文比较了大语言模型（LLMs）与小语言模型（SLMs）在需求工程中的性能，发现SLMs在准确率上几乎能达到LLMs的表现，且在某些指标上甚至优于LLMs。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs在需求工程的自然语言处理任务中表现出色，但其高计算成本、数据共享风险及对外部服务的依赖限制了应用。SLMs作为轻量级、本地可部署的替代方案，其性能尚不明确。

Method: 对比了3个LLMs和5个SLMs在PROMISE, PROMISE Reclass和SecReq三个需求分类数据集上的表现，采用准确率（F1分数）和召回率等指标进行评估。

Result: LLMs的平均F1分数仅比SLMs高2%，但差异无统计学显著性。SLMs几乎在所有数据集上达到LLMs的性能，且在PROMISE Reclass数据集上召回率甚至更高，且参数规模比LLMs小300倍。数据集特性对性能影响大于模型大小。

Conclusion: SLMs是需求分类任务中LLMs的有效替代，具有隐私保护、成本低和本地部署的优势。

Abstract: [Context and motivation] Large language models (LLMs) show notable results in
natural language processing (NLP) tasks for requirements engineering (RE).
However, their use is compromised by high computational cost, data sharing
risks, and dependence on external services. In contrast, small language models
(SLMs) offer a lightweight, locally deployable alternative. [Question/problem]
It remains unclear how well SLMs perform compared to LLMs in RE tasks in terms
of accuracy. [Results] Our preliminary study compares eight models, including
three LLMs and five SLMs, on requirements classification tasks using the
PROMISE, PROMISE Reclass, and SecReq datasets. Our results show that although
LLMs achieve an average F1 score of 2% higher than SLMs, this difference is not
statistically significant. SLMs almost reach LLMs performance across all
datasets and even outperform them in recall on the PROMISE Reclass dataset,
despite being up to 300 times smaller. We also found that dataset
characteristics play a more significant role in performance than model size.
[Contribution] Our study contributes with evidence that SLMs are a valid
alternative to LLMs for requirements classification, offering advantages in
privacy, cost, and local deployability.

</details>


### [50] [Scalpel: Automotive Deep Learning Framework Testing via Assembling Model Components](https://arxiv.org/abs/2510.21451)
*Yinglong Zou,Juan Zhai,Chunrong Fang,An Guo,Jiawei Liu,Zhenyu Chen*

Main category: cs.SE

TL;DR: 本文提出了Scalpel，一种面向自动驾驶汽车深度学习框架的测试方法，通过在模型组件层面生成测试输入模型，从而解决现有测试方法无法检测复杂部署环境下的质量问题。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统中的深度学习模型需要处理多输入输出张量、多模态数据以及多层数据特征提取，而现有测试方法无法生成具备这些能力的测试模型，导致无法发现如内存分配错误等质量问题。

Method: Scalpel通过维护和更新一个模型组件库（包括heads、necks、backbones），通过选择、变异和组装这些组件生成多功能测试模型，并将生成的模型应用于自动驾驶系统中的差分测试，以检测深度学习框架的质量问题。

Result: 成功生成了具备必要多功能能力的测试模型，并通过实际部署测试，发现了汽车深度学习框架中的崩溃及内存分配错误等问题。

Conclusion: Scalpel有效弥补了现有测试方法的不足，提升了自动驾驶深度学习框架测试的覆盖率和质量，保障了模型在复杂环境中的稳定部署。

Abstract: Deep learning (DL) plays a key role in autonomous driving systems. DL models
support perception modules, equipped with tasks such as object detection and
sensor fusion. These DL models enable vehicles to process multi-sensor inputs
to understand complex surroundings. Deploying DL models in autonomous driving
systems faces stringent challenges, including real-time processing, limited
computational resources, and strict power constraints. To address these
challenges, automotive DL frameworks (e.g., PaddleInference) have emerged to
optimize inference efficiency. However, these frameworks encounter unique
quality issues due to their more complex deployment environments, such as
crashes stemming from limited scheduled memory and incorrect memory allocation.
Unfortunately, existing DL framework testing methods fail to detect these
quality issues due to the failure in deploying generated test input models, as
these models lack three essential capabilities: (1) multi-input/output tensor
processing, (2) multi-modal data processing, and (3) multi-level data feature
extraction. These capabilities necessitate specialized model components, which
existing testing methods neglect during model generation. To bridge this gap,
we propose Scalpel, an automotive DL frameworks testing method that generates
test input models at the model component level. Scalpel generates models by
assembling model components (heads, necks, backbones) to support capabilities
required by autonomous driving systems. Specifically, Scalpel maintains and
updates a repository of model components, generating test inputs by selecting,
mutating, and assembling them. Successfully generated models are added back to
enrich the repository. Newly generated models are then deployed within the
autonomous driving system to test automotive DL frameworks via differential
testing.

</details>


### [51] [Towards Socio-Technical Topology-Aware Adaptive Threat Detection in Software Supply Chains](https://arxiv.org/abs/2510.21452)
*Thomas Welsh,Kristófer Finnsson,Brynjólfur Stefánsson,Helmut Neukirchen*

Main category: cs.SE

TL;DR: 软件供应链复杂且动态，攻击频发且难以精准检测，需结合技术与社会因素进行适应性威胁检测。


<details>
  <summary>Details</summary>
Motivation: 当前安全研究多侧重技术监控，忽视了供应链中社会技术动态对威胁检测的影响，导致无法有效识别复杂多变的攻击。

Method: 提出利用社会技术模型综合分析技术与社交数据，监测开发者行为和项目交互，以发现异常趋势并进行针对性漏洞评估。

Result: 通过分析XZ Utils攻击案例说明，结合技术和社交数据能更早识别异常行为，从而支持更精准和适应性的威胁检测。

Conclusion: 未来研究需解决开发者和软件分析技术、去中心化适应机制、建立安全测试环境等挑战，推动基于社会技术模型的供应链安全检测。

Abstract: Software supply chains (SSCs) are complex systems composed of dynamic,
heterogeneous technical and social components which collectively achieve the
production and maintenance of software artefacts. Attacks on SSCs are
increasing, yet pervasive vulnerability analysis is challenging due to their
complexity. Therefore, threat detection must be targeted, to account for the
large and dynamic structure, and adaptive, to account for its change and
diversity. While current work focuses on technical approaches for monitoring
supply chain dependencies and establishing component controls, approaches which
inform threat detection through understanding the socio-technical dynamics are
lacking. We outline a position and research vision to develop and investigate
the use of socio-technical models to support adaptive threat detection of SSCs.
We motivate this approach through an analysis of the XZ Utils attack whereby
malicious actors undermined the maintainers' trust via the project's GitHub and
mailing lists. We highlight that monitoring technical and social data can
identify trends which indicate suspicious behaviour to then inform targeted and
intensive vulnerability assessment. We identify challenges and research
directions to achieve this vision considering techniques for developer and
software analysis, decentralised adaptation and the need for a test bed for
software supply chain security research.

</details>


### [52] [Risk Management for Mitigating Benchmark Failure Modes: BenchRisk](https://arxiv.org/abs/2510.21460)
*Sean McGregor,Victor Lu,Vassil Tashev,Armstrong Foundjem,Aishwarya Ramasethu,Sadegh AlMahdi Kazemi Zarkouei,Chris Knotz,Kongtao Chen,Alicia Parrish,Anka Reuel,Heather Frase*

Main category: cs.SE

TL;DR: 本文基于NIST风险管理流程，分析了26个大语言模型基准，识别出57种潜在失败模式和196种缓解策略，提出了BenchRisk元评估基准，用以评估基准的风险，帮助用户避免错误结论。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型基准存在多种失败模式，影响其可靠性和用户对基准结果的理解，亟需系统化评估和缓解策略。

Method: 基于NIST风险管理框架，迭代分析26个流行基准，识别失败模式和对应缓解方法，构建了包含5个维度评分的BenchRisk风险评估方法。

Result: 所有26个基准在覆盖性、可理解性、一致性、正确性和持久性等维度均存在显著风险，BenchRisk可有效量化这些风险并支持基准间比较。

Conclusion: 当前LLM基准存在显著风险，BenchRisk提供了系统化评估和缓解路径，有助于推动LLM基准研究和提升基准的可靠性和安全性。

Abstract: Large language model (LLM) benchmarks inform LLM use decisions (e.g., "is
this LLM safe to deploy for my use case and context?"). However, benchmarks may
be rendered unreliable by various failure modes that impact benchmark bias,
variance, coverage, or people's capacity to understand benchmark evidence.
Using the National Institute of Standards and Technology's risk management
process as a foundation, this research iteratively analyzed 26 popular
benchmarks, identifying 57 potential failure modes and 196 corresponding
mitigation strategies. The mitigations reduce failure likelihood and/or
severity, providing a frame for evaluating "benchmark risk," which is scored to
provide a metaevaluation benchmark: BenchRisk. Higher scores indicate that
benchmark users are less likely to reach an incorrect or unsupported conclusion
about an LLM. All 26 scored benchmarks present significant risk within one or
more of the five scored dimensions (comprehensiveness, intelligibility,
consistency, correctness, and longevity), which points to important open
research directions for the field of LLM benchmarking. The BenchRisk workflow
allows for comparison between benchmarks; as an open-source tool, it also
facilitates the identification and sharing of risks and their mitigations.

</details>


### [53] [Wisdom and Delusion of LLM Ensembles for Code Generation and Repair](https://arxiv.org/abs/2510.21513)
*Fernando Vallecillos Ruiz,Max Hort,Leon Moonen*

Main category: cs.SE

TL;DR: 本文探讨了软件工程领域多大规模语言模型（LLM）集成的潜力，实验证明多模型集成性能可显著优于单一模型。


<details>
  <summary>Details</summary>
Motivation: 当前追求单一大型语言模型解决所有软件工程任务资源消耗大，忽视了不同模型互补优势。

Method: 对比十个不同家族的LLM及其三种集成方法，在代码生成和程序修复三个基准上测试，评估模型互补性和性能差距。

Result: 集成的理论上限性能可比最佳单模型高83%，共识策略易陷入错误使用率高的“人气陷阱”，多样性策略能发挥95%潜力，且两模型集成已经有效。

Conclusion: 利用多样性方法融合多个LLM可显著提升软件工程任务性能，同时具备成本效益，为多模型集成提供指导。

Abstract: Today's pursuit of a single Large Language Model (LMM) for all software
engineering tasks is resource-intensive and overlooks the potential benefits of
complementarity, where different models contribute unique strengths. However,
the degree to which coding LLMs complement each other and the best strategy for
maximizing an ensemble's potential are unclear, leaving practitioners without a
clear path to move beyond single-model systems.
  To address this gap, we empirically compare ten individual LLMs from five
families, and three ensembles of these LLMs across three software engineering
benchmarks covering code generation and program repair. We assess the
complementarity between models and the performance gap between the best
individual model and the ensembles. Next, we evaluate various selection
heuristics to identify correct solutions from an ensemble's candidate pool.
  We find that the theoretical upperbound for an ensemble's performance can be
83% above the best single model. Our results show that consensus-based
strategies for selecting solutions fall into a "popularity trap," amplifying
common but incorrect outputs. In contrast, a diversity-based strategy realizes
up to 95% of this theoretical potential, and proves effective even in small
two-model ensembles, enabling a cost-efficient way to enhance performance by
leveraging multiple LLMs.

</details>


### [54] [Lights-Out: An Automated Ground Segment for unstaffed Satellite Operations](https://arxiv.org/abs/2510.21516)
*Marvin Böcker,Ralph Biggins,Michael Schmeing*

Main category: cs.SE

TL;DR: 本文介绍了德国海因里希·赫兹卫星任务中首次应用的全自动化、周期性无人值守的地面段操作方案，支持全天候卫星平台操作和用户自主实验调度。


<details>
  <summary>Details</summary>
Motivation: 提高卫星地面段的自动化水平，实现无需人工干预的全天候操作，满足科学与技术实验的灵活需求。

Method: 采用自动计划和执行卫星指令，利用独立且自动冲突解决的用户任务调度，结合系统自动监控与可配置响应机制，同时开发自助用户门户以支持24/7访问与实时实验管理。

Result: 成功实现了海因里希·赫兹卫星的全自动卫星指令执行、任务调度和系统监控，用户能够通过门户灵活安排和监控实验任务，支持快速响应和实验中载荷重配置。

Conclusion: 提出的自动化地面段操作概念显著提升了卫星任务的自主性和用户体验，验证了其在科学技术实验任务中的有效性和实用价值。

Abstract: We present our approach for a periodically unstaffed, fully automated ground
segment. The concept is in use for the first time on the German satellite
communications mission Heinrich Hertz on behalf of the German Space Agency at
DLR. Heinrich Hertz was launched in July 2023 and offers access to scientific
and technical experiments to its users. The mission utilizes major automation
concepts for the satellite platform operations, allowing fully automated
operations outside of office hours. The concept includes tracking, telemetry
and commanding (TTC) of the satellite. Pre-planned and automatically executed
schedules enable commanding without human interaction. The user mission
schedule is planned separately from the main mission schedule and is
automatically de-conflicted. The automatic monitoring concept monitors the
systems of the satellite and all assets in the ground segment and triggers
reactions in operator-configurable ways depending on the mission needs, for
example emergency notifications or automated execution of flight operation
procedures. Additionally, the concept also puts special emphasis on a
self-service user portal that provides flexible access 24/7, even when the
control center is not staffed. The portal allows external users of the payload
to schedule pre-defined experiments, monitor the live execution of the
experiment with browser-based displays and access ground station telemetry and
dedicated RF test equipment during the time of their scheduled experiment.
Tasks can be planned long in advance as well as with a short reaction time
(less than 1 minute), which allows, for example, the reconfiguration of the
payload during a running experiment.

</details>


### [55] [Privacy by Design: Aligning GDPR and Software Engineering Specifications with a Requirements Engineering Approach](https://arxiv.org/abs/2510.21591)
*Oleksandr Kosenkov,Ehsan Zabardast,Davide Fucci,Daniel Mendez,Michael Unterkalmsteiner*

Main category: cs.SE

TL;DR: 本文探讨了如何将GDPR中的法律要求准确转化为软件系统的需求和规格，以实现隐私设计（PbD）。


<details>
  <summary>Details</summary>
Motivation: 目前实践中对如何制定满足PbD的需求和系统规格理解不足，现有方法未充分考虑GDPR中问题与解决方案的复杂交集合。

Method: 通过文献回顾和专家访谈调查现状与规格目标，开发并评估了基于GDPR法律概念建模的需求与系统规格方法。

Result: 所提方法有效捕捉法律知识，支持规格透明性及追溯性，体现了问题空间与解决空间的关键关系，有助于实现PbD。

Conclusion: GDPR要求需贯穿工程生命周期多个抽象层次，法律知识应嵌入规格中以满足不同利益相关者需求并保证合规，方法具实用性但仍存在改进空间。

Abstract: Context: Consistent requirements and system specifications are essential for
the compliance of software systems towards the General Data Protection
Regulation (GDPR). Both artefacts need to be grounded in the original text and
conjointly assure the achievement of privacy by design (PbD). Objectives: There
is little understanding of the perspectives of practitioners on specification
objectives and goals to address PbD. Existing approaches do not account for the
complex intersection between problem and solution space expressed in GDPR. In
this study we explore the demand for conjoint requirements and system
specification for PbD and suggest an approach to address this demand. Methods:
We reviewed secondary and related primary studies and conducted interviews with
practitioners to (1) investigate the state-of-practice and (2) understand the
underlying specification objectives and goals (e.g., traceability). We
developed and evaluated an approach for requirements and systems specification
for PbD, and evaluated it against the specification objectives. Results: The
relationship between problem and solution space, as expressed in GDPR, is
instrumental in supporting PbD. We demonstrate how our approach, based on the
modeling GDPR content with original legal concepts, contributes to
specification objectives of capturing legal knowledge, supporting specification
transparency, and traceability. Conclusion: GDPR demands need to be addressed
throughout different levels of abstraction in the engineering lifecycle to
achieve PbD. Legal knowledge specified in the GDPR text should be captured in
specifications to address the demands of different stakeholders and ensure
compliance. While our results confirm the suitability of our approach to
address practical needs, we also revealed specific needs for the future
effective operationalization of the approach.

</details>
