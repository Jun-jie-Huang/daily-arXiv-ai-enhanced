<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 31]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.SE](#cs.SE) [Total: 4]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Uncovering Competency Gaps in Large Language Models and Their Benchmarks](https://arxiv.org/abs/2512.20638)
*Matyas Bohacek,Nino Scherrer,Nicholas Dufour,Thomas Leung,Christoph Bregler,Stephanie C. Y. Chan*

Main category: cs.CL

TL;DR: 提出一种基于稀疏自编码器的方法，自动识别大型语言模型评估中的模型弱点和基准测试偏差，增强了评估的细粒度解释能力。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型(LLMs)的评估依赖于标准化基准测试，但这些测试的汇总指标掩盖了模型在特定子领域的弱点（模型缺口）和基准测试本身的不平衡覆盖（基准缺口）。

Method: 提出利用稀疏自编码器(SAEs)自动发现模型和基准中的缺口，通过提取SAE概念激活和加权性能分数，将评估基于模型内部表示，并实现跨基准比较。

Result: 应用于两种开源模型和十个基准，发现模型在反对谄媚行为和安全相关概念上的表现较差，验证了文献中的观察，同时发现许多基准偏重服从与指令执行，遗漏了核心概念。

Conclusion: 该方法为模型评估提供了基于表示的概念级分解，补充传统汇总指标，帮助解释模型得分原因和指导基准发展。

Abstract: The evaluation of large language models (LLMs) relies heavily on standardized benchmarks. These benchmarks provide useful aggregated metrics for a given capability, but those aggregated metrics can obscure (i) particular sub-areas where the LLMs are weak ("model gaps") and (ii) imbalanced coverage in the benchmarks themselves ("benchmark gaps"). We propose a new method that uses sparse autoencoders (SAEs) to automatically uncover both types of gaps. By extracting SAE concept activations and computing saliency-weighted performance scores across benchmark data, the method grounds evaluation in the model's internal representations and enables comparison across benchmarks. As examples demonstrating our approach, we applied the method to two popular open-source models and ten benchmarks. We found that these models consistently underperformed on concepts that stand in contrast to sycophantic behaviors (e.g., politely refusing a request or asserting boundaries) and concepts connected to safety discussions. These model gaps align with observations previously surfaced in the literature; our automated, unsupervised method was able to recover them without manual supervision. We also observed benchmark gaps: many of the evaluated benchmarks over-represented concepts related to obedience, authority, or instruction-following, while missing core concepts that should fall within their intended scope. In sum, our method offers a representation-grounded approach to evaluation, enabling concept-level decomposition of benchmark scores. Rather than replacing conventional aggregated metrics, CG complements them by providing a concept-level decomposition that can reveal why a model scored as it did and how benchmarks could evolve to better reflect their intended scope. Code is available at https://competency-gaps.github.io.

</details>


### [2] [SA-DiffuSeq: Addressing Computational and Scalability Challenges in Long-Document Generation with Sparse Attention](https://arxiv.org/abs/2512.20724)
*Alexandros Christoforos,Chadbourne Davis*

Main category: cs.CL

TL;DR: 本文提出了SA-DiffuSeq，一种集成稀疏注意力的扩散生成框架，有效解决长文本生成中的计算和内存瓶颈，提升了生成效率和质量，适用于科学写作和代码生成等长文本任务。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的长文本生成方法随着序列长度增加，计算成本和内存开销过高，限制了其扩展性和实际应用效果。

Method: SA-DiffuSeq通过在扩散过程中选择性地分配稀疏注意力，结合专门设计的soft absorbing state来稳定扩散轨迹并加速序列重建，从而提高采样效率和长距离依赖建模的精度。

Result: 大量实验表明SA-DiffuSeq在训练效率和采样速度上均超越了最先进的扩散基线模型，尤其在处理超长序列时提升明显。

Conclusion: 引入稀疏注意力机制的SA-DiffuSeq显著提升了长文本生成中扩散模型的计算效率和生成质量，特别是在处理长序列时表现优异，是一种适合科学写作、大规模代码生成和多轮长上下文对话的优质方法。

Abstract: Diffusion based approaches to long form text generation suffer from prohibitive computational cost and memory overhead as sequence length increases. We introduce SA-DiffuSeq, a diffusion framework that integrates sparse attention to fundamentally improve scalability for long document modeling. By selectively allocating attention within the diffusion process, SA-DiffuSeq significantly reduces computational complexity while maintaining semantic coherence and generation quality. A key component of our method is a soft absorbing state tailored to sparse attention dynamics, which stabilizes diffusion trajectories and accelerates sequence reconstruction. This design improves sampling efficiency and enhances precision in long range dependency modeling. Extensive experiments demonstrate that SA-DiffuSeq consistently surpasses state of the art diffusion baselines in both training efficiency and sampling speed, with especially strong gains on extended sequences. These properties make SA-DiffuSeq well suited for demanding long form applications such as scientific writing, large scale code generation, and multi turn long context dialogue. Overall, our results indicate that incorporating structured sparsity into diffusion models is a promising direction for efficient and expressive long text generation.

</details>


### [3] [TokSuite: Measuring the Impact of Tokenizer Choice on Language Model Behavior](https://arxiv.org/abs/2512.20757)
*Gül Sena Altıntaş,Malikeh Ehghaghi,Brian Lester,Fengyuan Liu,Wanru Zhao,Marco Ciccone,Colin Raffel*

Main category: cs.CL

TL;DR: TokSuite通过统一条件下比较不同分词器训练的语言模型，提出了评估分词器对模型性能影响的新方法，揭示了各种分词器的优缺点。


<details>
  <summary>Details</summary>
Motivation: 分词作为语言模型处理文本的基础，其对模型性能和行为的具体影响尚不清晰，且难以单独测量，因此需要一个框架来单独评估和比较分词器的影响。

Method: 设计并训练了十四个仅分词器不同的语言模型，使用相同的架构、数据集、训练预算和初始化，同时创建了一个新的基准测试集，针对现实扰动来评估模型性能，以系统分析分词器对模型表现的影响。

Result: TokSuite提供了一个包含多模型和基准测试的套件，能够有效分离分词器影响，支持研究者深入理解和评估不同分词策略在语言模型中的表现差异。

Conclusion: TokSuite通过对不同分词器使用相同模型架构、数据和训练条件进行对比实验，成功解耦了分词器对语言模型性能的影响，揭示了多种主流分词器的优劣。

Abstract: Tokenizers provide the fundamental basis through which text is represented and processed by language models (LMs). Despite the importance of tokenization, its role in LM performance and behavior is poorly understood due to the challenge of measuring the impact of tokenization in isolation. To address this need, we present TokSuite, a collection of models and a benchmark that supports research into tokenization's influence on LMs. Specifically, we train fourteen models that use different tokenizers but are otherwise identical using the same architecture, dataset, training budget, and initialization. Additionally, we curate and release a new benchmark that specifically measures model performance subject to real-world perturbations that are likely to influence tokenization. Together, TokSuite allows robust decoupling of the influence of a model's tokenizer, supporting a series of novel findings that elucidate the respective benefits and shortcomings of a wide range of popular tokenizers.

</details>


### [4] [Adversarial Training for Failure-Sensitive User Simulation in Mental Health Dialogue Optimization](https://arxiv.org/abs/2512.20773)
*Ziyi Zhu,Olivier Tieleman,Caitlin A. Stamatis,Luka Smyth,Thomas D. Hull,Daniel R. Cahn,Matteo Malgaroli*

Main category: cs.CL

TL;DR: 本文提出的对抗训练方法在心理健康支持任务的用户模拟中表现优异，实现了更真实和可靠的模拟，促进系统快速且经济地评估。


<details>
  <summary>Details</summary>
Motivation: 现实用户模拟对于训练和评估任务导向型对话系统至关重要，但模拟人类行为难度大，且有效模拟器需能够揭示系统的失败模式。

Method: 采用生成器与判别器之间的对抗训练框架，迭代提升用户模拟器的真实感。

Result: 经微调的用户模拟器显著优于零样本基础模型，在揭示系统问题上表现更好；对抗训练提升了模拟器的多样性、分布一致性和预测有效性；模拟失败发生率与真实情况高度相关；判别器准确率在三轮对抗后显著下降，表明模拟器真实感提高。

Conclusion: 对抗训练是构建真实用户模拟器的有效方法，能提升任务导向型对话系统的评估质量，特别适用于心理健康支持领域。

Abstract: Realistic user simulation is crucial for training and evaluating task-oriented dialogue (TOD) systems, yet creating simulators that accurately replicate human behavior remains challenging. A key property of effective simulators is their ability to expose failure modes of the systems they evaluate. We present an adversarial training framework that iteratively improves user simulator realism through a competitive dynamic between a generator (user simulator) and a discriminator. Applied to mental health support chatbots, our approach demonstrates that fine-tuned simulators dramatically outperform zero-shot base models at surfacing system issues, and adversarial training further enhances diversity, distributional alignment, and predictive validity. The resulting simulator achieves a strong correlation between simulated and real failure occurrence rates across diverse chatbot configurations while maintaining low distributional divergence of failure modes. Discriminator accuracy decreases drastically after three adversarial iterations, suggesting improved realism. These results provide evidence that adversarial training is a promising approach for creating realistic user simulators in mental health support TOD domains, enabling rapid, reliable, and cost-effective system evaluation before deployment.

</details>


### [5] [Large Language Models Approach Expert Pedagogical Quality in Math Tutoring but Differ in Instructional and Linguistic Profiles](https://arxiv.org/abs/2512.20780)
*Ramatu Oiza Abdulsalam,Segun Aroyehun*

Main category: cs.CL

TL;DR: 本文比较了大型语言模型与专家和新手人类导师在数学辅导中的表现，发现大型语言模型在教学质量上接近专家水平，但在教学策略和语言特征上存在差异。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型在数学辅导中的行为与专家人类导师的教学行为有多大一致性，评估其教学质量和策略差异。

Method: 通过控制实验，对相同数学辅导对话的各回合回答进行专家导师、新手导师和多种大型语言模型的比较，分析教学策略和语言特征。

Result: 大型语言模型在教学质量感知上接近专家导师，但重述与复述策略使用不足，语言表达较长且更有礼貌。重述、词汇多样性和追求准确性与教学质量正相关，过度礼貌和代理性语言则负相关。

Conclusion: 大型语言模型在教学质量上可与专家人类导师相媲美，但其教学和语言策略不同，特别是在重述和复述策略方面较少使用，同时使用更长且词汇多样化的表达。

Abstract: Recent work has explored the use of large language models for generating tutoring responses in mathematics, yet it remains unclear how closely their instructional behavior aligns with expert human practice. We examine this question using a controlled, turn-level comparison in which expert human tutors, novice human tutors, and multiple large language models respond to the same set of math remediation conversation turns. We examine both instructional strategies and linguistic characteristics of tutoring responses, including restating and revoicing, pressing for accuracy, lexical diversity, readability, politeness, and agency. We find that large language models approach expert levels of perceived pedagogical quality on average but exhibit systematic differences in their instructional and linguistic profiles. In particular, large language models tend to underuse restating and revoicing strategies characteristic of expert human tutors, while producing longer, more lexically diverse, and more polite responses. Statistical analyses show that restating and revoicing, lexical diversity, and pressing for accuracy are positively associated with perceived pedagogical quality, whereas higher levels of agentic and polite language are negatively associated. Overall, recent large language models exhibit levels of perceived pedagogical quality comparable to expert human tutors, while relying on different instructional and linguistic strategies. These findings underscore the value of analyzing instructional strategies and linguistic characteristics when evaluating tutoring responses across human tutors and intelligent tutoring systems.

</details>


### [6] [Investigating Model Editing for Unlearning in Large Language Models](https://arxiv.org/abs/2512.20794)
*Shariqah Hossain,Lalana Kagal*

Main category: cs.CL

TL;DR: 本文提出利用模型编辑算法改进大规模语言模型的机器遗忘效果，取得较传统方法更佳的遗忘质量，但在目标范围控制和性能保持方面仍存在难题。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法在应对参数众多的LLMs时效率不足或难以完全移除指定信息，模型编辑算法则侧重信息重定向，本文旨在结合两者优势，改善遗忘效果，提升大模型的信息管理能力。

Method: 通过实验比较了ROME、IKE和WISE三种模型编辑算法，并针对遗忘需求设计新的编辑目标，评估其在遗忘质量和模型性能上的表现。

Result: 本文探讨了机器‘遗忘’（unlearning）技术在大规模语言模型（LLMs）中的应用难题，特别是针对参数众多的模型，现有方法效率低下且难以在不影响保留知识的前提下完全移除不需要的信息。作者研究了模型编辑算法如ROME、IKE和WISE，设计了适用于遗忘场景的新型编辑目标，实验表明模型编辑方法在遗忘质量上可优于传统基线方法，但仍存在难以精确限定遗忘范围且保持整体模型性能的挑战。

Conclusion: 模型编辑方法在机器遗忘领域具备潜力，能够在特定设置下超越传统方法，但仍需解决如何精准界定遗忘内容及避免整体性能损失的问题。

Abstract: Machine unlearning aims to remove unwanted information from a model, but many methods are inefficient for LLMs with large numbers of parameters or fail to fully remove the intended information without degrading performance on knowledge that should be retained. Model editing algorithms solve a similar problem of changing information in models, but they focus on redirecting inputs to a new target rather than removing that information altogether. In this work, we explore the editing algorithms ROME, IKE, and WISE and design new editing targets for an unlearning setting. Through this investigation, we show that model editing approaches can exceed baseline unlearning methods in terms of quality of forgetting depending on the setting. Like traditional unlearning techniques, they struggle to encapsulate the scope of what is to be unlearned without damage to the overall model performance.

</details>


### [7] [Measuring Mechanistic Independence: Can Bias Be Removed Without Erasing Demographics?](https://arxiv.org/abs/2512.20796)
*Zhengyang Shan,Aaron Mueller*

Main category: cs.CL

TL;DR: 本文通过多任务评估比较归因和相关性偏见定位方法，发现定向特征消融能有效去偏且不降低识别能力，偏见机制与任务特定相关，需精细干预。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型中人口统计偏见机制是否独立于一般识别能力，以实现不损失识别性能的精准去偏。

Method: 采用多任务评估涵盖姓名、职业、教育，比较归因和相关性两种偏见特征定位方法，通过稀疏自编码器进行特征消融，结合定性分析验证效果。

Result: 本文研究了语言模型中人口统计学偏见机制与一般人口统计识别能力的独立性。通过关联姓名、职业和教育水平的人口统计学多任务评估，测量模型在减少偏见的同时保持识别能力的效果。比较了基于归因和基于相关性的方法定位偏见特征。结果显示，在Gemma-2-9B模型中，利用针对性的稀疏自编码器特征消融能够减少偏见且不影响识别性能：归因方法有效缓解了种族和性别职业刻板印象，同时保持姓名识别准确率，而相关性方法更适合教育偏见。定性分析发现，移除教育任务相关的归因特征会导致“先验崩溃”现象，反而增加偏见，表明需要针对特定维度的干预。总体看，人口统计偏见主要源自任务特定机制而非绝对人口统计标记，且机制推断期间的干预可以实现有针对性的去偏，且不损害核心能力。

Conclusion: 人口统计偏见主要由任务特定机制驱动，归因与相关性方法各有优势，准确的特征干预可实现精确去偏且保留模型能力。

Abstract: We investigate how independent demographic bias mechanisms are from general demographic recognition in language models. Using a multi-task evaluation setup where demographics are associated with names, professions, and education levels, we measure whether models can be debiased while preserving demographic detection capabilities. We compare attribution-based and correlation-based methods for locating bias features. We find that targeted sparse autoencoder feature ablations in Gemma-2-9B reduce bias without degrading recognition performance: attribution-based ablations mitigate race and gender profession stereotypes while preserving name recognition accuracy, whereas correlation-based ablations are more effective for education bias. Qualitative analysis further reveals that removing attribution features in education tasks induces ``prior collapse'', thus increasing overall bias. This highlights the need for dimension-specific interventions. Overall, our results show that demographic bias arises from task-specific mechanisms rather than absolute demographic markers, and that mechanistic inference-time interventions can enable surgical debiasing without compromising core model capabilities.

</details>


### [8] [Semantic Deception: When Reasoning Models Can't Compute an Addition](https://arxiv.org/abs/2512.20812)
*Nathaniël de Leeuw,Marceau Nahon,Mathis Reymond,Raja Chatila,Mehdi Khamassi*

Main category: cs.CL

TL;DR: 本文设计新符号和语义欺骗测试，揭示大语言模型在符号推理任务中容易被误导语义影响，符号操作能力存缺陷，提示其推理能力被高估，尤其影响决策场景的可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在涉及人类价值和决策的推理任务中广泛应用，但其符号推理能力及对误导性语义关联的抵抗能力尚未明确，需要深入评估。

Method: 引入新符号表示（重定义数字和运算符），设计语义欺骗场景，测试LLM在处理陌生符号系统时的符号抽象和抵抗误导语义信息的能力。

Result: 实验表明误导性语义线索显著降低LLM在简单符号任务中的表现，LLM倾向于依赖表层语义而非真正的符号抽象操作，连链式思维策略也可能放大这种依赖。

Conclusion: 大语言模型在处理符号抽象和符号操作时存在显著局限，容易被误导性语义关联影响，导致推理能力下降。

Abstract: Large language models (LLMs) are increasingly used in situations where human values are at stake, such as decision-making tasks that involve reasoning when performed by humans. We investigate the so-called reasoning capabilities of LLMs over novel symbolic representations by introducing an experimental framework that tests their ability to process and manipulate unfamiliar symbols. We introduce semantic deceptions: situations in which symbols carry misleading semantic associations due to their form, such as being embedded in specific contexts, designed to probe whether LLMs can maintain symbolic abstraction or whether they default to exploiting learned semantic associations. We redefine standard digits and mathematical operators using novel symbols, and task LLMs with solving simple calculations expressed in this altered notation. The objective is: (1) to assess LLMs' capacity for abstraction and manipulation of arbitrary symbol systems; (2) to evaluate their ability to resist misleading semantic cues that conflict with the task's symbolic logic. Through experiments with four LLMs we show that semantic cues can significantly deteriorate reasoning models' performance on very simple tasks. They reveal limitations in current LLMs' ability for symbolic manipulations and highlight a tendency to over-rely on surface-level semantics, suggesting that chain-of-thoughts may amplify reliance on statistical correlations. Even in situations where LLMs seem to correctly follow instructions, semantic cues still impact basic capabilities. These limitations raise ethical and societal concerns, undermining the widespread and pernicious tendency to attribute reasoning abilities to LLMs and suggesting how LLMs might fail, in particular in decision-making contexts where robust symbolic reasoning is essential and should not be compromised by residual semantic associations inherited from the model's training.

</details>


### [9] [EssayCBM: Rubric-Aligned Concept Bottleneck Models for Transparent Essay Grading](https://arxiv.org/abs/2512.20817)
*Kumar Satvik Chaudhary,Chengshuai Zhao,Fan Zhang,Yung Hin Tse,Garima Agrawal,Yuli Deng,Huan Liu*

Main category: cs.CL

TL;DR: EssayCBM提出了基于写作概念的透明自动评分框架，实现了可解释且性能不降低的作文自动评分。


<details>
  <summary>Details</summary>
Motivation: 当前自动评分系统如大型语言模型表现为黑箱，缺乏解释性，限制教育者和学生的理解和信任。

Method: EssayCBM通过八个写作概念（如论点清晰度和证据使用）作为中间层，由专门的预测头在编码器上估计这些概念评分，再用轻量网络根据概念分数计算最终成绩。

Result: EssayCBM在准确率上匹配传统黑箱模型，同时通过概念级评分提供可操作的反馈，实现更高透明度。

Conclusion: EssayCBM通过概念级中间表示实现了可解释且可控的自动作文评分，支持教师调整评分，提高了评估的透明度和可用性。

Abstract: Understanding how automated grading systems evaluate essays remains a significant challenge for educators and students, especially when large language models function as black boxes. We introduce EssayCBM, a rubric-aligned framework that prioritizes interpretability in essay assessment. Instead of predicting grades directly from text, EssayCBM evaluates eight writing concepts, such as Thesis Clarity and Evidence Use, through dedicated prediction heads on an encoder. These concept scores form a transparent bottleneck, and a lightweight network computes the final grade using only concepts. Instructors can adjust concept predictions and instantly view the updated grade, enabling accountable human-in-the-loop evaluation. EssayCBM matches black-box performance while offering actionable, concept-level feedback through an intuitive web interface.

</details>


### [10] [MediEval: A Unified Medical Benchmark for Patient-Contextual and Knowledge-Grounded Reasoning in LLMs](https://arxiv.org/abs/2512.20822)
*Zhan Qu,Michael Färber*

Main category: cs.CL

TL;DR: 本文提出MediEval，结合电子健康记录与统一知识库，系统评估医疗大语言模型的表现，并通过CoRFu微调方法提升准确性和安全性。


<details>
  <summary>Details</summary>
Motivation: 当前医疗大语言模型在准确性和安全性上存在不足，现有评估手段缺乏结合医学知识和上下文验证的综合评价，迫切需要一种有效评估与提升模型安全性的方案。

Method: 采用结合电子健康记录和统一知识库的数据集，设计4象限框架评估模型表现，提出Counterfactual Risk-Aware Fine-tuning（CoRFu）方法，通过不对称惩罚机制改进模型微调，提升准确率并消除错误。

Result: 该论文提出了MediEval基准，用于系统评估大语言模型在医疗场景下的知识基础和上下文一致性，识别模型的关键失败模式。并设计了CoRFu微调方法，显著提升模型准确性和安全性。

Conclusion: 该研究通过构建MediEval基准及引入CoRFu微调，解决了医疗大语言模型中的错误推理和安全隐患，实现了更可靠的医疗语言模型应用。

Abstract: Large Language Models (LLMs) are increasingly applied to medicine, yet their adoption is limited by concerns over reliability and safety. Existing evaluations either test factual medical knowledge in isolation or assess patient-level reasoning without verifying correctness, leaving a critical gap. We introduce MediEval, a benchmark that links MIMIC-IV electronic health records (EHRs) to a unified knowledge base built from UMLS and other biomedical vocabularies. MediEval generates diverse factual and counterfactual medical statements within real patient contexts, enabling systematic evaluation across a 4-quadrant framework that jointly considers knowledge grounding and contextual consistency. Using this framework, we identify critical failure modes, including hallucinated support and truth inversion, that current proprietary, open-source, and domain-specific LLMs frequently exhibit. To address these risks, we propose Counterfactual Risk-Aware Fine-tuning (CoRFu), a DPO-based method with an asymmetric penalty targeting unsafe confusions. CoRFu improves by +16.4 macro-F1 points over the base model and eliminates truth inversion errors, demonstrating both higher accuracy and substantially greater safety.

</details>


### [11] [Nemotron 3 Nano: Open, Efficient Mixture-of-Experts Hybrid Mamba-Transformer Model for Agentic Reasoning](https://arxiv.org/abs/2512.20848)
*NVIDIA,:,Aaron Blakeman,Aaron Grattafiori,Aarti Basant,Abhibha Gupta,Abhinav Khattar,Adi Renduchintala,Aditya Vavre,Akanksha Shukla,Akhiad Bercovich,Aleksander Ficek,Aleksandr Shaposhnikov,Alex Kondratenko,Alexander Bukharin,Alexandre Milesi,Ali Taghibakhshi,Alisa Liu,Amelia Barton,Ameya Sunil Mahabaleshwarkar,Amir Klein,Amit Zuker,Amnon Geifman,Amy Shen,Anahita Bhiwandiwalla,Andrew Tao,Ann Guan,Anubhav Mandarwal,Arham Mehta,Ashwath Aithal,Ashwin Poojary,Asif Ahamed,Asma Kuriparambil Thekkumpate,Ayush Dattagupta,Banghua Zhu,Bardiya Sadeghi,Barnaby Simkin,Ben Lanir,Benedikt Schifferer,Besmira Nushi,Bilal Kartal,Bita Darvish Rouhani,Boris Ginsburg,Brandon Norick,Brandon Soubasis,Branislav Kisacanin,Brian Yu,Bryan Catanzaro,Carlo del Mundo,Chantal Hwang,Charles Wang,Cheng-Ping Hsieh,Chenghao Zhang,Chenhan Yu,Chetan Mungekar,Chintan Patel,Chris Alexiuk,Christopher Parisien,Collin Neale,Damon Mosk-Aoyama,Dan Su,Dane Corneil,Daniel Afrimi,Daniel Rohrer,Daniel Serebrenik,Daria Gitman,Daria Levy,Darko Stosic,David Mosallanezhad,Deepak Narayanan,Dhruv Nathawani,Dima Rekesh,Dina Yared,Divyanshu Kakwani,Dong Ahn,Duncan Riach,Dusan Stosic,Edgar Minasyan,Edward Lin,Eileen Long,Eileen Peters Long,Elena Lantz,Ellie Evans,Elliott Ning,Eric Chung,Eric Harper,Eric Tramel,Erick Galinkin,Erik Pounds,Evan Briones,Evelina Bakhturina,Faisal Ladhak,Fay Wang,Fei Jia,Felipe Soares,Feng Chen,Ferenc Galko,Frankie Siino,Gal Hubara Agam,Ganesh Ajjanagadde,Gantavya Bhatt,Gargi Prasad,George Armstrong,Gerald Shen,Gorkem Batmaz,Grigor Nalbandyan,Haifeng Qian,Harsh Sharma,Hayley Ross,Helen Ngo,Herman Sahota,Hexin Wang,Himanshu Soni,Hiren Upadhyay,Huizi Mao,Huy C Nguyen,Huy Q Nguyen,Iain Cunningham,Ido Shahaf,Igor Gitman,Ilya Loshchilov,Ivan Moshkov,Izzy Putterman,Jan Kautz,Jane Polak Scowcroft,Jared Casper,Jatin Mitra,Jeffrey Glick,Jenny Chen,Jesse Oliver,Jian Zhang,Jiaqi Zeng,Jie Lou,Jimmy Zhang,Jining Huang,Joey Conway,Joey Guman,John Kamalu,Johnny Greco,Jonathan Cohen,Joseph Jennings,Joyjit Daw,Julien Veron Vialard,Junkeun Yi,Jupinder Parmar,Kai Xu,Kan Zhu,Kari Briski,Katherine Cheung,Katherine Luna,Keshav Santhanam,Kevin Shih,Kezhi Kong,Khushi Bhardwaj,Krishna C. Puvvada,Krzysztof Pawelec,Kumar Anik,Lawrence McAfee,Laya Sleiman,Leon Derczynski,Li Ding,Lucas Liebenwein,Luis Vega,Maanu Grover,Maarten Van Segbroeck,Maer Rodrigues de Melo,Makesh Narsimhan Sreedhar,Manoj Kilaru,Maor Ashkenazi,Marc Romeijn,Mark Cai,Markus Kliegl,Maryam Moosaei,Matvei Novikov,Mehrzad Samadi,Melissa Corpuz,Mengru Wang,Meredith Price,Michael Boone,Michael Evans,Miguel Martinez,Mike Chrzanowski,Mohammad Shoeybi,Mostofa Patwary,Nabin Mulepati,Natalie Hereth,Nave Assaf,Negar Habibi,Neta Zmora,Netanel Haber,Nicola Sessions,Nidhi Bhatia,Nikhil Jukar,Nikki Pope,Nikolai Ludwig,Nima Tajbakhsh,Nirmal Juluru,Oleksii Hrinchuk,Oleksii Kuchaiev,Olivier Delalleau,Oluwatobi Olabiyi,Omer Ullman Argov,Ouye Xie,Parth Chadha,Pasha Shamis,Pavlo Molchanov,Pawel Morkisz,Peter Dykas,Peter Jin,Pinky Xu,Piotr Januszewski,Pranav Prashant Thombre,Prasoon Varshney,Pritam Gundecha,Qing Miao,Rabeeh Karimi Mahabadi,Ran El-Yaniv,Ran Zilberstein,Rasoul Shafipour,Rich Harang,Rick Izzo,Rima Shahbazyan,Rishabh Garg,Ritika Borkar,Ritu Gala,Riyad Islam,Roger Waleffe,Rohit Watve,Roi Koren,Ruoxi Zhang,Russell J. Hewett,Ryan Prenger,Ryan Timbrook,Sadegh Mahdavi,Sahil Modi,Samuel Kriman,Sanjay Kariyappa,Sanjeev Satheesh,Saori Kaji,Satish Pasumarthi,Sean Narentharen,Sean Narenthiran,Seonmyeong Bak,Sergey Kashirsky,Seth Poulos,Shahar Mor,Shanmugam Ramasamy,Shantanu Acharya,Shaona Ghosh,Sharath Turuvekere Sreenivas,Shelby Thomas,Shiqing Fan,Shreya Gopal,Shrimai Prabhumoye,Shubham Pachori,Shubham Toshniwal,Shuoyang Ding,Siddharth Singh,Simeng Sun,Smita Ithape,Somshubra Majumdar,Soumye Singhal,Stefania Alborghetti,Stephen Ge,Sugam Dipak Devare,Sumeet Kumar Barua,Suseella Panguluri,Suyog Gupta,Sweta Priyadarshi,Syeda Nahida Akter,Tan Bui,Teodor-Dumitru Ene,Terry Kong,Thanh Do,Tijmen Blankevoort,Tom Balough,Tomer Asida,Tomer Bar Natan,Tugrul Konuk,Twinkle Vashishth,Udi Karpas,Ushnish De,Vahid Noorozi,Vahid Noroozi,Venkat Srinivasan,Venmugil Elango,Vijay Korthikanti,Vitaly Kurin,Vitaly Lavrukhin,Wanli Jiang,Wasi Uddin Ahmad,Wei Du,Wei Ping,Wenfei Zhou,Will Jennings,William Zhang,Wojciech Prazuch,Xiaowei Ren,Yashaswi Karnati,Yejin Choi,Yev Meyer,Yi-Fu Wu,Yian Zhang,Ying Lin,Yonatan Geifman,Yonggan Fu,Yoshi Subara,Yoshi Suhara,Yubo Gao,Zach Moshe,Zhen Dong,Zihan Liu,Zijia Chen,Zijie Yan*

Main category: cs.CL

TL;DR: Nemotron 3 Nano是一款预训练及强化学习后的混合专家Transformer模型，实现更高准确率和3.3倍推理速度，支持超长上下文，优于同规模开源模型，提升语言理解与生成能力。


<details>
  <summary>Details</summary>
Motivation: 为了提升语言模型的准确性和推理能力，同时提高推理速度和参数利用效率，解决现有模型在计算资源和性能上的限制。

Method: 该模型采用混合专家（Mixture-of-Experts）Mamba-Transformer架构，预训练于25万亿文本标记，并通过监督微调和大规模强化学习提升性能。

Result: 相较于前代Nemotron 2 Nano，Nemotron 3 Nano准确率更高，单次前向传播激活参数不到一半，推理吞吐量提升至类似规模开源模型的3.3倍，同时支持最长达100万标记的上下文。

Conclusion: Nemotron 3 Nano 30B-A3B在保持更高准确度的同时，显著提升了推理吞吐量和参数效率，且具备更强的推理、聊天及长上下文支持能力。

Abstract: We present Nemotron 3 Nano 30B-A3B, a Mixture-of-Experts hybrid Mamba-Transformer language model. Nemotron 3 Nano was pretrained on 25 trillion text tokens, including more than 3 trillion new unique tokens over Nemotron 2, followed by supervised fine tuning and large-scale RL on diverse environments. Nemotron 3 Nano achieves better accuracy than our previous generation Nemotron 2 Nano while activating less than half of the parameters per forward pass. It achieves up to 3.3x higher inference throughput than similarly-sized open models like GPT-OSS-20B and Qwen3-30B-A3B-Thinking-2507, while also being more accurate on popular benchmarks. Nemotron 3 Nano demonstrates enhanced agentic, reasoning, and chat abilities and supports context lengths up to 1M tokens. We release both our pretrained Nemotron 3 Nano 30B-A3B Base and post-trained Nemotron 3 Nano 30B-A3B checkpoints on Hugging Face.

</details>


### [12] [How important is Recall for Measuring Retrieval Quality?](https://arxiv.org/abs/2512.20854)
*Shelly Schwartz,Oleg Vasilyev,Randy Sawaya*

Main category: cs.CL

TL;DR: 针对难以计算召回率的实际检索场景，本文通过多数据集实验和LLM辅助判断，提出了无需总相关文档数即可评估检索质量的有效方法。


<details>
  <summary>Details</summary>
Motivation: 在大型且不断变化的知识库中，查询相关文档的总数通常未知，导致无法计算召回率。

Method: 评估多种检索策略，通过测量检索质量指标与基于大语言模型（LLM）生成响应的质量判断之间的相关性进行实验，涵盖多个低相关文档数量（2-15）的数据集。

Result: 提出了一种简单的检索质量测量方法，在无需知道相关文档总数的情况下，能有效评估检索质量。

Conclusion: 该方法在实际检索任务中表现良好，为处理未知相关文档数量的检索评价提供了可行方案。

Abstract: In realistic retrieval settings with large and evolving knowledge bases, the total number of documents relevant to a query is typically unknown, and recall cannot be computed. In this paper, we evaluate several established strategies for handling this limitation by measuring the correlation between retrieval quality metrics and LLM-based judgments of response quality, where responses are generated from the retrieved documents. We conduct experiments across multiple datasets with a relatively low number of relevant documents (2-15). We also introduce a simple retrieval quality measure that performs well without requiring knowledge of the total number of relevant documents.

</details>


### [13] [NVIDIA Nemotron 3: Efficient and Open Intelligence](https://arxiv.org/abs/2512.20856)
*NVIDIA,:,Aaron Blakeman,Aaron Grattafiori,Aarti Basant,Abhibha Gupta,Abhinav Khattar,Adi Renduchintala,Aditya Vavre,Akanksha Shukla,Akhiad Bercovich,Aleksander Ficek,Aleksandr Shaposhnikov,Alex Kondratenko,Alexander Bukharin,Alexandre Milesi,Ali Taghibakhshi,Alisa Liu,Amelia Barton,Ameya Sunil Mahabaleshwarkar,Amir Klein,Amit Zuker,Amnon Geifman,Amy Shen,Anahita Bhiwandiwalla,Andrew Tao,Anjulie Agrusa,Ankur Verma,Ann Guan,Anubhav Mandarwal,Arham Mehta,Ashwath Aithal,Ashwin Poojary,Asif Ahamed,Asit Mishra,Asma Kuriparambil Thekkumpate,Ayush Dattagupta,Banghua Zhu,Bardiya Sadeghi,Barnaby Simkin,Ben Lanir,Benedikt Schifferer,Besmira Nushi,Bilal Kartal,Bita Darvish Rouhani,Boris Ginsburg,Brandon Norick,Brandon Soubasis,Branislav Kisacanin,Brian Yu,Bryan Catanzaro,Carlo del Mundo,Chantal Hwang,Charles Wang,Cheng-Ping Hsieh,Chenghao Zhang,Chenhan Yu,Chetan Mungekar,Chintan Patel,Chris Alexiuk,Christopher Parisien,Collin Neale,Cyril Meurillon,Damon Mosk-Aoyama,Dan Su,Dane Corneil,Daniel Afrimi,Daniel Lo,Daniel Rohrer,Daniel Serebrenik,Daria Gitman,Daria Levy,Darko Stosic,David Mosallanezhad,Deepak Narayanan,Dhruv Nathawani,Dima Rekesh,Dina Yared,Divyanshu Kakwani,Dong Ahn,Duncan Riach,Dusan Stosic,Edgar Minasyan,Edward Lin,Eileen Long,Eileen Peters Long,Elad Segal,Elena Lantz,Ellie Evans,Elliott Ning,Eric Chung,Eric Harper,Eric Tramel,Erick Galinkin,Erik Pounds,Evan Briones,Evelina Bakhturina,Evgeny Tsykunov,Faisal Ladhak,Fay Wang,Fei Jia,Felipe Soares,Feng Chen,Ferenc Galko,Frank Sun,Frankie Siino,Gal Hubara Agam,Ganesh Ajjanagadde,Gantavya Bhatt,Gargi Prasad,George Armstrong,Gerald Shen,Gorkem Batmaz,Grigor Nalbandyan,Haifeng Qian,Harsh Sharma,Hayley Ross,Helen Ngo,Herbert Hum,Herman Sahota,Hexin Wang,Himanshu Soni,Hiren Upadhyay,Huizi Mao,Huy C Nguyen,Huy Q Nguyen,Iain Cunningham,Ido Galil,Ido Shahaf,Igor Gitman,Ilya Loshchilov,Itamar Schen,Itay Levy,Ivan Moshkov,Izik Golan,Izzy Putterman,Jan Kautz,Jane Polak Scowcroft,Jared Casper,Jatin Mitra,Jeffrey Glick,Jenny Chen,Jesse Oliver,Jian Zhang,Jiaqi Zeng,Jie Lou,Jimmy Zhang,Jinhang Choi,Jining Huang,Joey Conway,Joey Guman,John Kamalu,Johnny Greco,Jonathan Cohen,Joseph Jennings,Joyjit Daw,Julien Veron Vialard,Junkeun Yi,Jupinder Parmar,Kai Xu,Kan Zhu,Kari Briski,Katherine Cheung,Katherine Luna,Keith Wyss,Keshav Santhanam,Kevin Shih,Kezhi Kong,Khushi Bhardwaj,Kirthi Shankar,Krishna C. Puvvada,Krzysztof Pawelec,Kumar Anik,Lawrence McAfee,Laya Sleiman,Leon Derczynski,Li Ding,Lizzie Wei,Lucas Liebenwein,Luis Vega,Maanu Grover,Maarten Van Segbroeck,Maer Rodrigues de Melo,Mahdi Nazemi,Makesh Narsimhan Sreedhar,Manoj Kilaru,Maor Ashkenazi,Marc Romeijn,Marcin Chochowski,Mark Cai,Markus Kliegl,Maryam Moosaei,Matt Kulka,Matvei Novikov,Mehrzad Samadi,Melissa Corpuz,Mengru Wang,Meredith Price,Michael Andersch,Michael Boone,Michael Evans,Miguel Martinez,Mikail Khona,Mike Chrzanowski,Minseok Lee,Mohammad Dabbah,Mohammad Shoeybi,Mostofa Patwary,Nabin Mulepati,Najeeb Nabwani,Natalie Hereth,Nave Assaf,Negar Habibi,Neta Zmora,Netanel Haber,Nicola Sessions,Nidhi Bhatia,Nikhil Jukar,Nikki Pope,Nikolai Ludwig,Nima Tajbakhsh,Nir Ailon,Nirmal Juluru,Nishant Sharma,Oleksii Hrinchuk,Oleksii Kuchaiev,Olivier Delalleau,Oluwatobi Olabiyi,Omer Ullman Argov,Omri Puny,Oren Tropp,Ouye Xie,Parth Chadha,Pasha Shamis,Paul Gibbons,Pavlo Molchanov,Pawel Morkisz,Peter Dykas,Peter Jin,Pinky Xu,Piotr Januszewski,Pranav Prashant Thombre,Prasoon Varshney,Pritam Gundecha,Przemek Tredak,Qing Miao,Qiyu Wan,Rabeeh Karimi Mahabadi,Rachit Garg,Ran El-Yaniv,Ran Zilberstein,Rasoul Shafipour,Rich Harang,Rick Izzo,Rima Shahbazyan,Rishabh Garg,Ritika Borkar,Ritu Gala,Riyad Islam,Robert Hesse,Roger Waleffe,Rohit Watve,Roi Koren,Ruoxi Zhang,Russell Hewett,Russell J. Hewett,Ryan Prenger,Ryan Timbrook,Sadegh Mahdavi,Sahil Modi,Samuel Kriman,Sangkug Lim,Sanjay Kariyappa,Sanjeev Satheesh,Saori Kaji,Satish Pasumarthi,Saurav Muralidharan,Sean Narentharen,Sean Narenthiran,Seonmyeong Bak,Sergey Kashirsky,Seth Poulos,Shahar Mor,Shanmugam Ramasamy,Shantanu Acharya,Shaona Ghosh,Sharath Turuvekere Sreenivas,Shelby Thomas,Shiqing Fan,Shreya Gopal,Shrimai Prabhumoye,Shubham Pachori,Shubham Toshniwal,Shuoyang Ding,Siddharth Singh,Simeng Sun,Smita Ithape,Somshubra Majumdar,Soumye Singhal,Stas Sergienko,Stefania Alborghetti,Stephen Ge,Sugam Dipak Devare,Sumeet Kumar Barua,Suseella Panguluri,Suyog Gupta,Sweta Priyadarshi,Syeda Nahida Akter,Tan Bui,Teodor-Dumitru Ene,Terry Kong,Thanh Do,Tijmen Blankevoort,Tim Moon,Tom Balough,Tomer Asida,Tomer Bar Natan,Tomer Ronen,Tugrul Konuk,Twinkle Vashishth,Udi Karpas,Ushnish De,Vahid Noorozi,Vahid Noroozi,Venkat Srinivasan,Venmugil Elango,Victor Cui,Vijay Korthikanti,Vinay Rao,Vitaly Kurin,Vitaly Lavrukhin,Vladimir Anisimov,Wanli Jiang,Wasi Uddin Ahmad,Wei Du,Wei Ping,Wenfei Zhou,Will Jennings,William Zhang,Wojciech Prazuch,Xiaowei Ren,Yashaswi Karnati,Yejin Choi,Yev Meyer,Yi-Fu Wu,Yian Zhang,Yigong Qin,Ying Lin,Yonatan Geifman,Yonggan Fu,Yoshi Subara,Yoshi Suhara,Yubo Gao,Zach Moshe,Zhen Dong,Zhongbo Zhu,Zihan Liu,Zijia Chen,Zijie Yan*

Main category: cs.CL

TL;DR: Nemotron 3系列采用创新架构和训练技术，提升推理与对话能力，支持超大上下文，涵盖从高效小型模型到高性能大型模型，且将开放全部资源。


<details>
  <summary>Details</summary>
Motivation: 为了开发出具有强大代理能力、推理能力和对话能力的高效模型，满足大规模上下文处理和推理需求。

Method: 引入Nemotron 3系列模型，采用Mixture-of-Experts混合Mamba-Transformer架构，支持高吞吐量和最长100万标记的上下文长度。使用NVFP4技术训练Super和Ultra模型，整合LatentMoE提升模型质量，加入MTP层加速文本生成。所有模型均通过多环境强化学习进行后训练，实现复杂推理、多步工具使用及细致推理预算控制。

Result: Nano模型在精度上超越了同类模型且极具推理成本效益；Super模型适合协作代理和大规模工作负载，如IT工单自动化；Ultra模型则提供了领先的精度和推理性能。Nano模型已发布，Super和Ultra模型将陆续发布。相关权重、软件和数据均将开放共享。

Conclusion: Nemotron 3系列模型通过先进架构和训练方法，实现了在推理能力、上下文处理长度和效率上的突破，为不同应用场景提供了多样化、高性能的解决方案。该系列模型的开放发布有助于推动AI研究和应用发展。

Abstract: We introduce the Nemotron 3 family of models - Nano, Super, and Ultra. These models deliver strong agentic, reasoning, and conversational capabilities. The Nemotron 3 family uses a Mixture-of-Experts hybrid Mamba-Transformer architecture to provide best-in-class throughput and context lengths of up to 1M tokens. Super and Ultra models are trained with NVFP4 and incorporate LatentMoE, a novel approach that improves model quality. The two larger models also include MTP layers for faster text generation. All Nemotron 3 models are post-trained using multi-environment reinforcement learning enabling reasoning, multi-step tool use, and support granular reasoning budget control. Nano, the smallest model, outperforms comparable models in accuracy while remaining extremely cost-efficient for inference. Super is optimized for collaborative agents and high-volume workloads such as IT ticket automation. Ultra, the largest model, provides state-of-the-art accuracy and reasoning performance. Nano is released together with its technical report and this white paper, while Super and Ultra will follow in the coming months. We will openly release the model weights, pre- and post-training software, recipes, and all data for which we hold redistribution rights.

</details>


### [14] [Architectural Trade-offs in Small Language Models Under Compute Constraints](https://arxiv.org/abs/2512.20877)
*Shivraj Singh Bhatti*

Main category: cs.CL

TL;DR: 该论文系统性地研究了小型语言模型在严格计算资源限制下的性能表现，重点分析了架构选择和训练预算的相互作用。


<details>
  <summary>Details</summary>
Motivation: 探究在有限计算资源下，不同架构设计如何影响小型语言模型的效率和性能表现。

Method: 从线性预测开始，逐步引入非线性、自注意力机制和多层Transformer架构，使用负对数似然、参数量和训练FLOPs进行评估。

Result: 注意力机制在小型模型中显示出更好的每FLOP效率，不同架构和优化策略对性能影响显著，且大型模型成功的技术未必适用。

Conclusion: 注意力机制模型在计算效率上优于多层感知机模型，小型模型中增加深度或上下文长度若未充分优化可能导致性能下降；大型模型的某些架构技术未必适用于小型模型。

Abstract: We present a systematic empirical study of small language models under strict compute constraints, analyzing how architectural choices and training budget interact to determine performance. Starting from a linear next-token predictor, we progressively introduce nonlinearities, self-attention, and multi-layer transformer architectures, evaluating each on character-level modeling of Tiny Shakespeare and word-level modeling of Penn Treebank (PTB) and WikiText-2. We compare models using test negative log-likelihood (NLL), parameter count, and approximate training FLOPs to characterize accuracy-efficiency trade-offs. Our results show that attention-based models dominate MLPs in per-FLOP efficiency even at small scale, while increasing depth or context without sufficient optimization can degrade performance. We further examine rotary positional embeddings (RoPE), finding that architectural techniques successful in large language models do not necessarily transfer to small-model regimes.

</details>


### [15] [Where Did This Sentence Come From? Tracing Provenance in LLM Reasoning Distillation](https://arxiv.org/abs/2512.20908)
*Kaiyuan Liu,Shaotian Yan,Rui Miao,Bing Wang,Chen Shen,Jun Zhang,Jieping Ye*

Main category: cs.CL

TL;DR: 本文提出了一个跨模型推理蒸馏起源追踪框架，分析蒸馏模型行为来源并基于此设计教师指导的数据选择方法，有效提升蒸馏模型推理性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有推理蒸馏方法缺乏对蒸馏模型能力起源的详细分析，未明确蒸馏学生模型在测试时是否能保持与教师模型一致的推理行为，存在模型泛化能力的疑虑。

Method: 引入跨模型推理蒸馏起源追踪框架，比较教师模型、原始学生模型和蒸馏学生模型在相同上下文下对每个动作的预测概率，从而分类并分析蒸馏模型能力的来源；基于分析提出教师指导的数据选择方法，直接比较教师和学生在训练数据上的差异，提升数据选择的科学性。

Result: 实验表明，在测试环境中，蒸馏模型能够生成来源于教师模型的动作，这与其性能表现相关并为性能提供合理解释；提出的教师指导数据选择方法在多种教师和学生模型上验证了有效性。

Conclusion: 推理蒸馏模型部分能力确实来自教师模型，跨模型起源追踪框架有助于理解和提升蒸馏模型的表现，教师指导数据选择为蒸馏提供了更科学的数据支持，推动了推理蒸馏领域的发展。

Abstract: Reasoning distillation has attracted increasing attention. It typically leverages a large teacher model to generate reasoning paths, which are then used to fine-tune a student model so that it mimics the teacher's behavior in training contexts. However, previous approaches have lacked a detailed analysis of the origins of the distilled model's capabilities. It remains unclear whether the student can maintain consistent behaviors with the teacher in novel test-time contexts, or whether it regresses to its original output patterns, raising concerns about the generalization of distillation models. To analyse this question, we introduce a cross-model Reasoning Distillation Provenance Tracing framework. For each action (e.g., a sentence) produced by the distilled model, we obtain the predictive probabilities assigned by the teacher, the original student, and the distilled model under the same context. By comparing these probabilities, we classify each action into different categories. By systematically disentangling the provenance of each action, we experimentally demonstrate that, in test-time contexts, the distilled model can indeed generate teacher-originated actions, which correlate with and plausibly explain observed performance on distilled model. Building on this analysis, we further propose a teacher-guided data selection method. Unlike prior approach that rely on heuristics, our method directly compares teacher-student divergences on the training data, providing a principled selection criterion. We validate the effectiveness of our approach across multiple representative teacher models and diverse student models. The results highlight the utility of our provenance-tracing framework and underscore its promise for reasoning distillation. We hope to share Reasoning Distillation Provenance Tracing and our insights into reasoning distillation with the community.

</details>


### [16] [Foundation Model-based Evaluation of Neuropsychiatric Disorders: A Lifespan-Inclusive, Multi-Modal, and Multi-Lingual Study](https://arxiv.org/abs/2512.20948)
*Zhongren Dong,Haotian Guo,Weixiang Xu,Huan Zhao,Zixing Zhang*

Main category: cs.CL

TL;DR: FEND框架结合语音和文本多模态，利用多语言数据集，推进神经精神障碍的自动化多模态检测，提升早期识别能力，解决多语言泛化和评估标准缺失问题。


<details>
  <summary>Details</summary>
Motivation: 神经精神障碍如阿尔茨海默病、抑郁症和自闭症谱系障碍具有语言和声学异常，作为早期检测的潜在生物标志物，但多模态方法存在多语言泛化和统一评估框架缺失问题。

Method: 提出FEND框架，结合语音和文本多模态，利用13个多语言数据集（英语、中文、希腊语、法语、荷兰语），系统评估多模态融合性能。

Result: 多模态融合在阿尔茨海默病和抑郁症检测中表现优异，但在自闭症谱系障碍中受数据异质性影响表现较差；多模态融合未必超过最佳单模态模型；跨语料库实验显示任务和语言一致下性能稳定，多语言和任务异质场景下性能下降。

Conclusion: FEND提供了多模态、多语言、跨寿命期神经精神障碍自动评估的全面基准和性能影响因素分析，促进领域公平比较和可重复研究。

Abstract: Neuropsychiatric disorders, such as Alzheimer's disease (AD), depression, and autism spectrum disorder (ASD), are characterized by linguistic and acoustic abnormalities, offering potential biomarkers for early detection. Despite the promise of multi-modal approaches, challenges like multi-lingual generalization and the absence of a unified evaluation framework persist. To address these gaps, we propose FEND (Foundation model-based Evaluation of Neuropsychiatric Disorders), a comprehensive multi-modal framework integrating speech and text modalities for detecting AD, depression, and ASD across the lifespan. Leveraging 13 multi-lingual datasets spanning English, Chinese, Greek, French, and Dutch, we systematically evaluate multi-modal fusion performance. Our results show that multi-modal fusion excels in AD and depression detection but underperforms in ASD due to dataset heterogeneity. We also identify modality imbalance as a prevalent issue, where multi-modal fusion fails to surpass the best mono-modal models. Cross-corpus experiments reveal robust performance in task- and language-consistent scenarios but noticeable degradation in multi-lingual and task-heterogeneous settings. By providing extensive benchmarks and a detailed analysis of performance-influencing factors, FEND advances the field of automated, lifespan-inclusive, and multi-lingual neuropsychiatric disorder assessment. We encourage researchers to adopt the FEND framework for fair comparisons and reproducible research.

</details>


### [17] [Neural Probe-Based Hallucination Detection for Large Language Models](https://arxiv.org/abs/2512.20949)
*Shize Liang,Hongzhi Wang*

Main category: cs.CL

TL;DR: 本文提出了一种基于MLP探针的幻觉检测方法，通过非线性建模和贝叶斯优化提升检测性能，显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有基于不确定性估计和外部知识检索的幻觉检测方法存在高置信度错误输出和对检索效率及知识覆盖依赖重等缺陷，需要一种实时、轻量且能捕捉深层语义非线性结构的检测方法。

Method: 冻结语言模型参数，设计轻量级多层感知器（MLP）探针对高层隐藏状态进行非线性建模；采用多目标联合损失函数；通过贝叶斯优化自动选择最佳插入层。

Result: 本文提出了一种基于神经网络的轻量级MLP探针框架，用于检测大型语言模型生成内容中的幻觉问题。通过冻结语言模型参数，利用多层感知器对高层隐藏状态进行非线性建模，并采用多目标联合损失函数提升检测的稳定性和语义消歧能力。利用贝叶斯优化自动搜索最佳探针插入层。实验证明该方法在LongFact、HealthBench和TriviaQA数据集上在准确率、召回率及低误报条件下的检测能力均优于现有最先进方法。

Conclusion: MLP探针结合多目标损失和自动层选择显著增强了语言模型幻觉内容检测的准确性和鲁棒性，适用于实时轻量级检测。

Abstract: Large language models(LLMs) excel at text generation and knowledge question-answering tasks, but they are prone to generating hallucinated content, severely limiting their application in high-risk domains. Current hallucination detection methods based on uncertainty estimation and external knowledge retrieval suffer from the limitation that they still produce erroneous content at high confidence levels and rely heavily on retrieval efficiency and knowledge coverage. In contrast, probe methods that leverage the model's hidden-layer states offer real-time and lightweight advantages. However, traditional linear probes struggle to capture nonlinear structures in deep semantic spaces.To overcome these limitations, we propose a neural network-based framework for token-level hallucination detection. By freezing language model parameters, we employ lightweight MLP probes to perform nonlinear modeling of high-level hidden states. A multi-objective joint loss function is designed to enhance detection stability and semantic disambiguity. Additionally, we establish a layer position-probe performance response model, using Bayesian optimization to automatically search for optimal probe insertion layers and achieve superior training results.Experimental results on LongFact, HealthBench, and TriviaQA demonstrate that MLP probes significantly outperform state-of-the-art methods in accuracy, recall, and detection capability under low false-positive conditions.

</details>


### [18] [MultiMind at SemEval-2025 Task 7: Crosslingual Fact-Checked Claim Retrieval via Multi-Source Alignment](https://arxiv.org/abs/2512.20950)
*Mohammad Mahdi Abootorabi,Alireza Ghahramani Kure,Mohammadali Mohammadkhani,Sina Elahimanesh,Mohammad Ali Ali Panah*

Main category: cs.CL

TL;DR: 提出TriAligner，利用双编码器与对比学习结合多语言数据，有效提升事实核查声明的多语言检索能力。


<details>
  <summary>Details</summary>
Motivation: 在信息快速传播的时代，有效事实核查变得尤为重要，需针对多语言环境开发高效的声明检索方法。

Method: 采用双编码器架构结合对比学习，整合原生语言和英文翻译的多模态数据，利用难负样本采样和大语言模型进行数据预处理及增强。

Result: 在单语和跨语基准测试中，TriAligner显著优于基线方法，提升了声明检索的准确性和事实核查表现。

Conclusion: TriAligner系统在多语言和跨语言事实核查声明检索任务中表现优异，有效提升了检索准确率和事实核查性能。

Abstract: This paper presents our system for SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval. In an era where misinformation spreads rapidly, effective fact-checking is increasingly critical. We introduce TriAligner, a novel approach that leverages a dual-encoder architecture with contrastive learning and incorporates both native and English translations across different modalities. Our method effectively retrieves claims across multiple languages by learning the relative importance of different sources in alignment. To enhance robustness, we employ efficient data preprocessing and augmentation using large language models while incorporating hard negative sampling to improve representation learning. We evaluate our approach on monolingual and crosslingual benchmarks, demonstrating significant improvements in retrieval accuracy and fact-checking performance over baselines.

</details>


### [19] [Reflection Pretraining Enables Token-Level Self-Correction in Biological Sequence Models](https://arxiv.org/abs/2512.20954)
*Xiang Zhang,Jiaqi Wei,Yuejin Yang,Zijie Qiu,Yuhan Chen,Zhiqiang Gao,Muhammad Abdul-Mageed,Laks V. S. Lakshmanan,Wanli Ouyang,Chenyu You,Siqi Sun*

Main category: cs.CL

TL;DR: 该论文提出通过反思预训练增强蛋白质语言模型的表达能力，使其具备链式思维中间步骤的推理能力，取得了性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有蛋白质和RNA语言模型由于符号空间表达能力有限，无法有效应用链式思维（CoT）推理方法，因此需要提升其语言表达能力以支持更复杂的推理过程。

Method: 定义语言表达能力的概念，引入反思预训练，通过生成辅助“思考符号”扩展蛋白质语言的符号集，提升模型推理能力，并在预训练阶段教会模型自我纠正。

Result: 该论文提出了在蛋白质语言模型中引入反思预训练（reflection pretraining）的方法，以增强模型的语言表达能力并实现链式思维（Chain-of-Thought, CoT）的中间推理步骤，从而提升模型复杂推理和自我纠正的能力。实验证明该方法显著提高了蛋白质模型的性能。

Conclusion: 通过引入反思预训练扩充了蛋白质语言模型的表达空间，增强了模型的推理能力，实现了自我纠正及复杂推理过程，从而显著提升了模型性能。

Abstract: Chain-of-Thought (CoT) prompting has significantly advanced task-solving capabilities in natural language processing with large language models. Unlike standard prompting, CoT encourages the model to generate intermediate reasoning steps, non-answer tokens, that help guide the model toward more accurate final outputs. These intermediate steps enable more complex reasoning processes such as error correction, memory management, future planning, and self-reflection. However, applying CoT to non-natural language domains, such as protein and RNA language models, is not yet possible, primarily due to the limited expressiveness of their token spaces (e.g., amino acid tokens). In this work, we propose and define the concept of language expressiveness: the ability of a given language, using its tokens and grammar, to encode information. We show that the limited expressiveness of protein language severely restricts the applicability of CoT-style reasoning. To overcome this, we introduce reflection pretraining, for the first time in a biological sequence model, which enables the model to engage in intermediate reasoning through the generation of auxiliary "thinking tokens" beyond simple answer tokens. Theoretically, we demonstrate that our augmented token set significantly enhances biological language expressiveness, thereby improving the overall reasoning capacity of the model. Experimentally, our pretraining approach teaches protein models to self-correct and leads to substantial performance gains compared to standard pretraining.

</details>


### [20] [Automatic Replication of LLM Mistakes in Medical Conversations](https://arxiv.org/abs/2512.20983)
*Oleksii Proniakin,Diego Fajardo,Ruslan Nazarenko,Razvan Marinescu*

Main category: cs.CL

TL;DR: 本文提出MedMistake自动化提取并构建医疗对话错误问答基准，发布数据集并对多款顶尖LLM进行评测，揭示其在临床问答中的弱点。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLMs）在临床环境下评估时难以重复具体错误，且复现错误需要大量手工工作。

Method: 通过模拟医生-患者对话，利用两名大型语言模型评审员评估错误类型，并将复杂错误转化为单次问答对，形成标准化测试集，辅以医学专家验证与多模型比较评估。

Result: 发布包含3390个单次问答对的MedMistake-All数据集，汇集当前GPT-5和Gemini 2.5 Pro等模型未能正确回答的问题。经医学专家验证211个问题，形成医师验证的MedMistake-Bench基准，并基于此对12个前沿模型进行评估，发现GPT系列、Claude和Grok表现最佳。

Conclusion: MedMistake提供了一种自动化且有效的途径来捕获并量化LLM在医疗对话中的错误，促进模型在临床推理和安全性上的持续改进。

Abstract: Large language models (LLMs) are increasingly evaluated in clinical settings using multi-dimensional rubrics which quantify reasoning quality, safety, and patient-centeredness. Yet, replicating specific mistakes in other LLM models is not straightforward and often requires manual effort. We introduce MedMistake, an automatic pipeline that extracts mistakes LLMs make in patient-doctor conversations and converts them into a benchmark of single-shot QA pairs. Our pipeline (1) creates complex, conversational data between an LLM patient and LLM doctor, (2) runs an evaluation with a committee of 2 LLM judges across a variety of dimensions and (3) creates simplified single-shot QA scenarios from those mistakes. We release MedMistake-All, a dataset of 3,390 single-shot QA pairs where GPT-5 and Gemini 2.5 Pro are currently failing to answer correctly, as judged by two LLM judges. We used medical experts to validate a subset of 211/3390 questions (MedMistake-Bench), which we used to run a final evaluation of 12 frontier LLMs: Claude Opus 4.5, Claude Sonnet 4.5, DeepSeek-Chat, Gemini 2.5 Pro, Gemini 3 Pro, GPT-4o, GPT-5, GPT-5.1, GPT-5.2, Grok 4, Grok 4.1, Mistral Large. We found that GPT models, Claude and Grok obtained the best performance on MedMistake-Bench. We release both the doctor-validated benchmark (MedMistake-Bench), as well as the full dataset (MedMistake-All) at https://huggingface.co/datasets/TheLumos/MedicalMistakeBenchmark.

</details>


### [21] [Distilling the Essence: Efficient Reasoning Distillation via Sequence Truncation](https://arxiv.org/abs/2512.21002)
*Wei-Rui Chen,Vignesh Kothapalli,Ata Fatahibaarzi,Hejian Sang,Shao Tang,Qingquan Song,Zhipeng Wang,Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: 本文通过只对推理链的前半部分tokens进行蒸馏，有效平衡了计算开销和模型性能，为推理蒸馏提供了新的高效策略。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的推理能力蒸馏到小模型时，长序列（三部分：提示P、链式推理CoT、答案A）训练导致计算资源消耗巨大，如何减少计算量而保持性能成为挑战。

Method: 通过选择性地对链式推理（CoT）部分的tokens进行知识蒸馏，避免对完整的提示（P）和答案（A）段落进行训练，减少训练序列的长度，从而降低训练的计算开销。提出截断协议，训练时仅利用前50%的tokens来权衡计算资源和效果。

Result: 选择性地仅对链式推理部分进行蒸馏在提示和答案信息被包含的情况下效果显著。前50%tokens的训练能保持约94%的性能，同时训练时间、内存和计算消耗均减少约50%。

Conclusion: 推理能力的知识蒸馏应优先关注序列开头的推理token，合理截断训练序列长度即可大幅降低计算成本，且性能几乎不受影响。

Abstract: Distilling the reasoning capabilities from a large language model (LLM) to a smaller student model often involves training on substantial amounts of reasoning data. However, distillation over lengthy sequences with prompt (P), chain-of-thought (CoT), and answer (A) segments makes the process computationally expensive. In this work, we investigate how the allocation of supervision across different segments (P, CoT, A) affects student performance. Our analysis shows that selective knowledge distillation over only the CoT tokens can be effective when the prompt and answer information is encompassed by it. Building on this insight, we establish a truncation protocol to quantify computation-quality tradeoffs as a function of sequence length. We observe that training on only the first $50\%$ of tokens of every training sequence can retain, on average, $\approx94\%$ of full-sequence performance on math benchmarks while reducing training time, memory usage, and FLOPs by about $50\%$ each. These findings suggest that reasoning distillation benefits from prioritizing early reasoning tokens and provides a simple lever for computation-quality tradeoffs. Codes are available at https://github.com/weiruichen01/distilling-the-essence.

</details>


### [22] [Rethinking Supervised Fine-Tuning: Emphasizing Key Answer Tokens for Improved LLM Accuracy](https://arxiv.org/abs/2512.21017)
*Xiaofeng Shi,Qian Kou,Yuduo Li,Hua Zhou*

Main category: cs.CL

TL;DR: 提出SFTKey训练策略，通过分阶段微调关键答案部分，有效提升大模型在复杂推理任务上的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统监督微调（SFT）中，模型对过长的思路链（CoT）关注过多，忽视了关键答案部分，影响任务完成和评估质量。

Method: 提出SFTKey，两阶段训练方案；第一阶段传统监督微调确保格式，第二阶段仅微调关键答案部分提高准确率。

Result: 实验显示SFTKey在多个基准和模型上平均提高了5%以上准确率，且保持了正确格式生成能力。

Conclusion: SFTKey通过明确平衡思路链与关键答案的优化，推动了大型语言模型的微调效果提升。

Abstract: With the rapid advancement of Large Language Models (LLMs), the Chain-of-Thought (CoT) component has become significant for complex reasoning tasks. However, in conventional Supervised Fine-Tuning (SFT), the model could allocate disproportionately more attention to CoT sequences with excessive length. This reduces focus on the much shorter but essential Key portion-the final answer, whose correctness directly determines task success and evaluation quality. To address this limitation, we propose SFTKey, a two-stage training scheme. In the first stage, conventional SFT is applied to ensure proper output format, while in the second stage, only the Key portion is fine-tuned to improve accuracy. Extensive experiments across multiple benchmarks and model families demonstrate that SFTKey achieves an average accuracy improvement exceeding 5\% over conventional SFT, while preserving the ability to generate correct formats. Overall, this study advances LLM fine-tuning by explicitly balancing CoT learning with additional optimization on answer-relevant tokens.

</details>


### [23] [Semantic Refinement with LLMs for Graph Representations](https://arxiv.org/abs/2512.21106)
*Safal Thapaliya,Zehong Wang,Jiazheng Li,Ziming Li,Yanfang Ye,Chuxu Zhang*

Main category: cs.CL

TL;DR: 本文提出数据自适应语义细化框架，结合GNN与大型语言模型闭环优化，提升了不同结构与语义占优图的表示学习性能。


<details>
  <summary>Details</summary>
Motivation: 图结构数据在不同领域中，预测信号的来源存在显著异质性：某些领域节点级语义占主导，而另一些领域结构模式更关键。这种结构-语义异质性使得单一固定归纳偏置的图学习模型难以在多样的图领域中实现最优泛化。

Method: 本文提出了一个数据自适应的语义细化框架DAS，将固定的图神经网络（GNN）与大型语言模型（LLM）结合在一个闭环反馈机制中。GNN提供隐式监督信号来指导LLM的语义细化，细化后的语义再反馈给图学习器进行更新。

Result: 在文本丰富和文本缺失的图上均进行了评估。结果显示，在结构主导的图上实现了持续改进，在语义丰富的图上仍保持竞争力，验证了数据中心语义自适应在结构-语义异质性中的有效性。

Conclusion: 通过数据中心的语义自适应，结合GNN与LLM的闭环反馈机制，能够提升图表示学习在异构结构-语义数据上的泛化能力。

Abstract: Graph-structured data exhibit substantial heterogeneity in where their predictive signals originate: in some domains, node-level semantics dominate, while in others, structural patterns play a central role. This structure-semantics heterogeneity implies that no graph learning model with a fixed inductive bias can generalize optimally across diverse graph domains. However, most existing methods address this challenge from the model side by incrementally injecting new inductive biases, which remains fundamentally limited given the open-ended diversity of real-world graphs. In this work, we take a data-centric perspective and treat node semantics as a task-adaptive variable. We propose a Data-Adaptive Semantic Refinement framework DAS for graph representation learning, which couples a fixed graph neural network (GNN) and a large language model (LLM) in a closed feedback loop. The GNN provides implicit supervisory signals to guide the semantic refinement of LLM, and the refined semantics are fed back to update the same graph learner. We evaluate our approach on both text-rich and text-free graphs. Results show consistent improvements on structure-dominated graphs while remaining competitive on semantics-rich graphs, demonstrating the effectiveness of data-centric semantic adaptation under structure-semantics heterogeneity.

</details>


### [24] [Semi-Supervised Learning for Large Language Models Safety and Content Moderation](https://arxiv.org/abs/2512.21107)
*Eduard Stefan Dinuta,Iustin Sirbu,Traian Rebedea*

Main category: cs.CL

TL;DR: 本文提出利用半监督学习和任务特定数据增强技术，提高大语言模型安全任务中分类器的性能，解决了标注数据不足和质量问题。


<details>
  <summary>Details</summary>
Motivation: 现有训练安全分类器依赖大量标注数据，获取困难且可能存在标注错误，部分还使用合成数据，因此需要更高效且准确的训练方法。

Method: 采用半监督学习，结合标注和未标注数据，通过任务特定的数据增强技术来训练安全分类器。

Result: 半监督学习方法在处理提示和模型响应的安全任务中均表现出性能提升，且任务特定的数据增强对提升效果至关重要。

Conclusion: 利用半监督学习技术能够显著提升大语言模型在安全任务中的表现，尤其是在任务特定的数据增强方面效果显著优于通用增强方法。

Abstract: Safety for Large Language Models (LLMs) has been an ongoing research focus since their emergence and is even more relevant nowadays with the increasing capacity of those models. Currently, there are several guardrails in place for all public LLMs and multiple proposed datasets for training safety classifiers. However, training these safety classifiers relies on large quantities of labeled data, which can be problematic to acquire, prone to labeling errors, or often include synthetic data. To address these issues, we suggest a different approach: utilizing semi-supervised learning techniques, which leverage both labeled and unlabeled data, to improve the performance on the safety task. We analyze the improvements that these techniques can offer for both prompts given to Large Language Models and the responses to those requests. Moreover, since augmentation is the central part of semi-supervised algorithms, we demonstrate the importance of using task-specific augmentations, which significantly increase the performance when compared to general-purpose augmentation techniques.

</details>


### [25] [ClarifyMT-Bench: Benchmarking and Improving Multi-Turn Clarification for Conversational Large Language Models](https://arxiv.org/abs/2512.21120)
*Sichun Luo,Yi Huang,Mukai Li,Shichang Meng,Fengyuan Liu,Zefa Hu,Junlan Feng,Qi Liu*

Main category: cs.CL

TL;DR: 提出多轮澄清基准ClarifyMT-Bench，发现LLMs普遍过早回答，设计ClarifyAgent改善澄清策略。


<details>
  <summary>Details</summary>
Motivation: 现有澄清测试不适应多轮开放域对话，缺少行为多样的用户模拟，难以真实评估模型澄清能力。

Method: 提出了ClarifyMT-Bench基于五维歧义分类和六种用户画像，通过混合人机流程构建了6120个多轮对话。评测了十个代表性LLMs，发现其倾向于过早回答而非澄清。基于此，设计了ClarifyAgent，将澄清任务拆分为感知、预测、追踪和规划四部分，提升模型鲁棒性。

Result: 通过ClarifyMT-Bench评测，发现多模型存在过早回答的偏差，性能随对话深度下降。ClarifyAgent显著提升模型在不同歧义条件下的澄清表现。

Conclusion: ClarifyMT-Bench为研究LLMs何时提问、何时回答以及如何处理歧义提供了可复现的基准，ClarifyAgent有助于提升多轮交互中的澄清能力。

Abstract: Large language models (LLMs) are increasingly deployed as conversational assistants in open-domain, multi-turn settings, where users often provide incomplete or ambiguous information. However, existing LLM-focused clarification benchmarks primarily assume single-turn interactions or cooperative users, limiting their ability to evaluate clarification behavior in realistic settings. We introduce \textbf{ClarifyMT-Bench}, a benchmark for multi-turn clarification grounded in a five-dimensional ambiguity taxonomy and a set of six behaviorally diverse simulated user personas. Through a hybrid LLM-human pipeline, we construct 6,120 multi-turn dialogues capturing diverse ambiguity sources and interaction patterns. Evaluating ten representative LLMs uncovers a consistent under-clarification bias: LLMs tend to answer prematurely, and performance degrades as dialogue depth increases. To mitigate this, we propose \textbf{ClarifyAgent}, an agentic approach that decomposes clarification into perception, forecasting, tracking, and planning, substantially improving robustness across ambiguity conditions. ClarifyMT-Bench establishes a reproducible foundation for studying when LLMs should ask, when they should answer, and how to navigate ambiguity in real-world human-LLM interactions.

</details>


### [26] [SpidR-Adapt: A Universal Speech Representation Model for Few-Shot Adaptation](https://arxiv.org/abs/2512.21204)
*Mahi Luthra,Jiayi Shen,Maxime Poli,Angelo Ortiz,Yosuke Higuchi,Youssef Benchekroun,Martin Gleize,Charles-Eric Saint-James,Dongyan Lin,Phillip Rust,Angel Villar,Surya Parimi,Vanessa Stark,Rashel Moritz,Juan Pino,Yann LeCun,Emmanuel Dupoux*

Main category: cs.CL

TL;DR: 本文通过元学习和双层优化方法，实现了仅用极少无标注语音数据即可快速适应新语言的语音表示模型，效率远超传统方法。


<details>
  <summary>Details</summary>
Motivation: 人类婴儿仅通过数百小时的语言接触即可获得新语言的基础单元，显示出与数据需求量大的自监督语音模型之间存在显著的效率差距。

Method: 将低资源语音表示学习视为元学习问题，构建了多任务自适应预训练（MAdaPT）协议，将适应过程形式化为双层优化框架，并提出了低计算成本的第一阶双层优化（FOBLO）启发式解决方案，同时通过交替的自监督和监督目标实现稳健初始化。

Result: SpidR-Adapt在训练时间少于1小时的目标语言音频上，比标准训练方法数据效率高出100倍以上，在音素辨别度和语音语言建模任务上取得快速提升，性能超过同域语言模型。

Conclusion: 本研究提出了一种灵活、高效的元学习框架，用于快速适应新语言，显著提升了低资源语音表示学习的效率，推进了受生物启发的数据高效表示学习路径。

Abstract: Human infants, with only a few hundred hours of speech exposure, acquire basic units of new languages, highlighting a striking efficiency gap compared to the data-hungry self-supervised speech models. To address this gap, this paper introduces SpidR-Adapt for rapid adaptation to new languages using minimal unlabeled data. We cast such low-resource speech representation learning as a meta-learning problem and construct a multi-task adaptive pre-training (MAdaPT) protocol which formulates the adaptation process as a bi-level optimization framework. To enable scalable meta-training under this framework, we propose a novel heuristic solution, first-order bi-level optimization (FOBLO), avoiding heavy computation costs. Finally, we stabilize meta-training by using a robust initialization through interleaved supervision which alternates self-supervised and supervised objectives. Empirically, SpidR-Adapt achieves rapid gains in phonemic discriminability (ABX) and spoken language modeling (sWUGGY, sBLIMP, tSC), improving over in-domain language models after training on less than 1h of target-language audio, over $100\times$ more data-efficient than standard training. These findings highlight a practical, architecture-agnostic path toward biologically inspired, data-efficient representations. We open-source the training code and model checkpoints at https://github.com/facebookresearch/spidr-adapt.

</details>


### [27] [SMART SLM: Structured Memory and Reasoning Transformer, A Small Language Model for Accurate Document Assistance](https://arxiv.org/abs/2512.21280)
*Divij Dudeja,Mayukha Pal*

Main category: cs.CL

TL;DR: 该论文提出了SMART模型，采用分层结构和三大核心组件解决工程手册阅读难题，实现了较高准确率且模型参数量小。


<details>
  <summary>Details</summary>
Motivation: 工程手册内容冗长且格式密集，传统Transformer模型将文本当做平坦的令牌流处理，导致错误答案多且记忆效率低，因此需要一种更结构化的处理方法。

Method: 采用层次化方法，包括语法感知的事实提取器（Tree LSTM），紧凑的索引记忆网络（MANN），以及6层Transformer融合检索到的事实，支持双模式推理。

Result: SMART模型参数量为45.51M，比GPT-2少64%，比BERT少69%；准确率比GPT-2高21.3%；在实际应用中减少了幻觉生成，提升了答案支持度和响应速度。

Conclusion: SMART模型在工程手册阅读任务中显著提升了准确率，参数量比GPT-2和BERT少得多，减少了错误生成且响应速度快，表现优异。

Abstract: The user of Engineering Manuals (EM) finds it difficult to read EM s because they are long, have a dense format which includes written documents, step by step procedures, and standard parameter lists for engineering equipment. Off the shelf transformers, especially compact ones, treat this material as a flat stream of tokens. This approach leads to confident but incorrect numeric answers and forces the models to memorize separate facts inefficiently. SMART (Structured Memory and Reasoning Transformer) offers a different and practical solution to the above problem. SMART structures its processing by using a hierarchical approach, and is based upon three main job categories (1) A syntax-aware Fact Extractor (Grammarian) Tree LSTM which extracts facts as subject relation object relations from EM sentences (2) A compact indexed memory MANN (Memory Augmented Neural Network) that indexes these Rational Subject Relation Objects as 384 dimensional vectors that are associated with the source of the information, and (3) A 6 layer Transformer that learns to fuse the previously retrieved facts into its generated response. The entire SMART model utilizes 45.51M parameters, which is 64% less than GPT-2 (124M) and 69% less than BERT (133M), and it achieves a 21.3% higher accuracy than GPT-2, indicating that SMART fits the data better with the least amount of processing requirements. SMART employs dual modes of inference an indexed fast path for known documents (sub-second answer times) and an indexed dynamic path assisted by RAGs for new uploads (FAISS Top 20 results with memory severed at 64 slots). In real world deployment, this framework leads to more well supported results with reduced hallucinations than comparable small transformer models.

</details>


### [28] [Parallel Token Prediction for Language Models](https://arxiv.org/abs/2512.21323)
*Felix Draxler,Justus Will,Farrin Marouf Sofian,Theofanis Karaletsos,Sameer Singh,Stephan Mandt*

Main category: cs.CL

TL;DR: 提出了并行标记预测(PTP)框架，实现语言模型中多个相依标记的并行预测，降低解码延迟，避免独立性假设，并证明了其强大的表达能力，在Vicuna-7B上达成了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决自回归解码的延迟瓶颈和现有多标记预测方法中的独立性假设限制，提出通用的并行序列生成框架。

Method: PTP通过在模型中引入采样过程，联合预测多个相依标记，训练方式包括蒸馏已有模型和无教师的逆自回归训练。

Result: 在Vicuna-7B模型上，通过Spec-Bench评测，PTP每步预测超过四个标记，实现了最先进的推测解码性能。

Conclusion: PTP框架能够在保证建模能力的前提下，实现长序列的高效并行生成，显著提升解码速度和性能。

Abstract: We propose Parallel Token Prediction (PTP), a universal framework for parallel sequence generation in language models. PTP jointly predicts multiple dependent tokens in a single transformer call by incorporating the sampling procedure into the model. This reduces the latency bottleneck of autoregressive decoding, and avoids the restrictive independence assumptions common in existing multi-token prediction methods. We prove that PTP can represent arbitrary autoregressive sequence distributions. PTP is trained either by distilling an existing model or through inverse autoregressive training without a teacher. Experimentally, we achieve state-of-the-art speculative decoding performance on Vicuna-7B by accepting over four tokens per step on Spec-Bench. The universality of our framework indicates that parallel generation of long sequences is feasible without loss of modeling power.

</details>


### [29] [Your Reasoning Benchmark May Not Test Reasoning: Revealing Perception Bottleneck in Abstract Reasoning Benchmarks](https://arxiv.org/abs/2512.21329)
*Xinhe Wang,Jin Huang,Xingjian Zhang,Tianhao Wang,Jiaqi W. Ma*

Main category: cs.CL

TL;DR: ARC类基准测试中，视觉语言模型的表现差异主要源于视觉感知限制，而非推理能力不足，需在评估中区分两者。


<details>
  <summary>Details</summary>
Motivation: 质疑当前认为视觉语言模型在ARC类基准测试中表现不佳是由于推理能力不足的观点，假设主要瓶颈在于视觉感知能力。

Method: 引入了一个两阶段实验流程，将视觉感知和推理明确分离；第一阶段将图像转换为自然语言描述，第二阶段基于这些描述进行规则归纳和推理。

Result: 在三个ARC风格的数据集（Mini-ARC、ACRE、Bongard-LOGO）上，两阶段流程明显优于传统的端到端单阶段流程，模型约80%的失败归因于感知错误，说明视觉感知是主要制约因素。

Conclusion: ARC测试中混淆了感知和推理的难题，现有评估可能高估了机器推理的缺陷，未来需要设计能分离感知与推理的评估协议，以准确反映人工智能进展。

Abstract: Reasoning benchmarks such as the Abstraction and Reasoning Corpus (ARC) and ARC-AGI are widely used to assess progress in artificial intelligence and are often interpreted as probes of core, so-called ``fluid'' reasoning abilities. Despite their apparent simplicity for humans, these tasks remain challenging for frontier vision-language models (VLMs), a gap commonly attributed to deficiencies in machine reasoning. We challenge this interpretation and hypothesize that the gap arises primarily from limitations in visual perception rather than from shortcomings in inductive reasoning.
  To verify this hypothesis, we introduce a two-stage experimental pipeline that explicitly separates perception and reasoning. In the perception stage, each image is independently converted into a natural-language description, while in the reasoning stage a model induces and applies rules using these descriptions. This design prevents leakage of cross-image inductive signals and isolates reasoning from perception bottlenecks. Across three ARC-style datasets, Mini-ARC, ACRE, and Bongard-LOGO, we show that the perception capability is the dominant factor underlying the observed performance gap by comparing the two-stage pipeline with against standard end-to-end one-stage evaluation. Manual inspection of reasoning traces in the VLM outputs further reveals that approximately 80 percent of model failures stem from perception errors. Together, these results demonstrate that ARC-style benchmarks conflate perceptual and reasoning challenges and that observed performance gaps may overstate deficiencies in machine reasoning. Our findings underscore the need for evaluation protocols that disentangle perception from reasoning when assessing progress in machine intelligence.

</details>


### [30] [C2LLM Technical Report: A New Frontier in Code Retrieval via Adaptive Cross-Attention Pooling](https://arxiv.org/abs/2512.21332)
*Jin Qin,Zihan Liao,Ziyin Zhang,Hang Yu,Peng Di,Rui Wang*

Main category: cs.CL

TL;DR: C2LLM是一种基于Qwen-2.5-Coder骨干网络的对比式代码大语言模型，采用多头注意力池化模块生成序列嵌入，打破传统EOS嵌入的瓶颈，在MTEB-Code基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统EOS标记作为序列嵌入的方式存在信息瓶颈，且难以充分利用模型捕获的预训练因果信息，因此需要一种更有效的序列嵌入方法以提升代码理解和表达性能。

Method: 利用Qwen-2.5-Coder作为基础模型，引入多头注意力池化（PMA）模块，将token嵌入聚合成序列嵌入，从而充分利用预训练的因果表示并打破序列嵌入中的信息瓶颈，同时支持灵活调整嵌入维度。

Result: 在训练了三百万公开数据后，C2LLM-7B在MTEB-Code基准测试中获得第一名，在相似规模模型中创下新纪录。

Conclusion: C2LLM模型在保持合理模型规模的情况下，通过创新的多头注意力池化机制，有效提升了代码嵌入的质量和表达能力，取得了同期模型中的最佳性能。

Abstract: We present C2LLM - Contrastive Code Large Language Models, a family of code embedding models in both 0.5B and 7B sizes. Building upon Qwen-2.5-Coder backbones, C2LLM adopts a Pooling by Multihead Attention (PMA) module for generating sequence embedding from token embeddings, effectively 1) utilizing the LLM's causal representations acquired during pretraining, while also 2) being able to aggregate information from all tokens in the sequence, breaking the information bottleneck in EOS-based sequence embeddings, and 3) supporting flexible adaptation of embedding dimension, serving as an alternative to MRL. Trained on three million publicly available data, C2LLM models set new records on MTEB-Code among models of similar sizes, with C2LLM-7B ranking 1st on the overall leaderboard.

</details>


### [31] [Optimizing Decoding Paths in Masked Diffusion Models by Quantifying Uncertainty](https://arxiv.org/abs/2512.21336)
*Ziyu Chen,Xinbei Jiang,Peng Sun,Tao Lin*

Main category: cs.CL

TL;DR: 该论文提出通过引入噪声熵指标来量化马斯克扩散模型中解码序列的不确定性，从而优化生成路径，提高生成质量。


<details>
  <summary>Details</summary>
Motivation: 马斯克扩散模型虽然灵活，但输出质量高度依赖解码顺序，存在质量波动问题，需要一种指标来评估并优化生成过程。

Method: 提出了噪声熵指标用于量化生成路径的不确定性，并基于此设计了两种解码路径优化算法：后验选择和实时引导。

Result: 基于噪声熵引导的算法在多个难度较大的推理、规划和代码生成基准测试上显著提升了生成质量和准确率。

Conclusion: 通过引入噪声熵，论文成功解决了马斯克扩散模型生成质量对解码顺序敏感的问题，显著提升了推理、规划及代码生成任务的准确率。

Abstract: Masked Diffusion Models (MDMs) offer flexible, non-autoregressive generation, but this freedom introduces a challenge: final output quality is highly sensitive to the decoding order. We are the first to formalize this issue, attributing the variability in output quality to the cumulative predictive uncertainty along a generative path. To quantify this uncertainty, we introduce Denoising Entropy, a computable metric that serves as an internal signal for evaluating generative process. Leveraging this metric, we propose two algorithms designed to optimize the decoding path: a post-hoc selection method and a real-time guidance strategy. Experiments demonstrate that our entropy-guided methods significantly improve generation quality, consistently boosting accuracy on challenging reasoning, planning, and code benchmarks. Our work establishes Denoising Entropy as a principled tool for understanding and controlling generation, effectively turning the uncertainty in MDMs from a liability into a key advantage for discovering high-quality solutions.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [32] [Towards Optimal Performance and Action Consistency Guarantees in Dec-POMDPs with Inconsistent Beliefs and Limited Communication](https://arxiv.org/abs/2512.20778)
*Moshe Rafaeli Shimron,Vadim Indelman*

Main category: cs.MA

TL;DR: 本文提出一种去中心化框架，解决多智能体信念不一致问题，通过选择性通信保证行动一致性和性能，仿真验证优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多智能体在不确定环境下需要基于各自不同的信念进行决策，但现有方法通常假设所有智能体拥有相同信念，现实中通信受限导致信念不一致，影响协调和性能。

Method: 提出了一种去中心化框架，在多智能体部分可观测马尔可夫决策过程(POMDP)下，通过概率保证行动一致性和性能，且仅在必要时触发通信，同时考虑基于联合行动是否共享数据以提高推断性能。

Result: 仿真实验表明该方法优于最先进算法，在行动一致性和性能方面表现更佳。

Conclusion: 该工作有效解决了多智能体信念不一致带来的协调和安全问题，提升了多智能体系统在有限通信环境下的决策能力和性能。

Abstract: Multi-agent decision-making under uncertainty is fundamental for effective and safe autonomous operation. In many real-world scenarios, each agent maintains its own belief over the environment and must plan actions accordingly. However, most existing approaches assume that all agents have identical beliefs at planning time, implying these beliefs are conditioned on the same data. Such an assumption is often impractical due to limited communication. In reality, agents frequently operate with inconsistent beliefs, which can lead to poor coordination and suboptimal, potentially unsafe, performance. In this paper, we address this critical challenge by introducing a novel decentralized framework for optimal joint action selection that explicitly accounts for belief inconsistencies. Our approach provides probabilistic guarantees for both action consistency and performance with respect to open-loop multi-agent POMDP (which assumes all data is always communicated), and selectively triggers communication only when needed. Furthermore, we address another key aspect of whether, given a chosen joint action, the agents should share data to improve expected performance in inference. Simulation results show our approach outperforms state-of-the-art algorithms.

</details>


### [33] [DAO-Agent: Zero Knowledge-Verified Incentives for Decentralized Multi-Agent Coordination](https://arxiv.org/abs/2512.20973)
*Yihan Xia,Taotao Wang,Wenxin Xu,Shengli Zhang*

Main category: cs.MA

TL;DR: DAO-Agent结合DAO和零知识证明，实现了去中心化多智能体协作的公平激励与隐私保护，显著减少链上计算开销，推动了无信任环境下的智能体协作发展。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在无信任环境中缺乏透明贡献度衡量和公平激励机制，传统区块链方案计算开销大且可能泄露智能体隐私，亟需一种高效、隐私保护且可扩展的多智能体协作协调方案。

Method: 提出DAO-Agent框架，包括基于链上的DAO治理机制进行透明协调和不可篡改的日志记录，链下基于零知识证明（ZKP）技术实现Shapley值贡献度测量，以及链上/链下混合架构验证贡献度，最大限度减少链上计算开销。

Result: 通过实验，在加密交易任务中，DAO-Agent较传统链上方案将验证gas费用降低了99.9%，验证复杂度保持常数且稳定，证明其在去中心化环境中实现多智能体高效协调的可行性和优越性。

Conclusion: DAO-Agent成功实现了在无信任环境下，通过去中心化自治组织治理以及ZKP技术，实现了透明且公平的多智能体协作任务执行与激励分配，同时极大降低了链上计算成本，保证了隐私保护和系统的可扩展性。

Abstract: Autonomous Large Language Model (LLM)-based multi-agent systems have emerged as a promising paradigm for facilitating cross-application and cross-organization collaborations. These autonomous agents often operate in trustless environments, where centralized coordination faces significant challenges, such as the inability to ensure transparent contribution measurement and equitable incentive distribution. While blockchain is frequently proposed as a decentralized coordination platform, it inherently introduces high on-chain computation costs and risks exposing sensitive execution information of the agents. Consequently, the core challenge lies in enabling auditable task execution and fair incentive distribution for autonomous LLM agents in trustless environments, while simultaneously preserving their strategic privacy and minimizing on-chain costs. To address this challenge, we propose DAO-Agent, a novel framework that integrates three key technical innovations: (1) an on-chain decentralized autonomous organization (DAO) governance mechanism for transparent coordination and immutable logging; (2) a ZKP mechanism approach that enables Shapley-based contribution measurement off-chain, and (3) a hybrid on-chain/off-chain architecture that verifies ZKP-validated contribution measurements on-chain with minimal computational overhead. We implement DAO-Agent and conduct end-to-end experiments using a crypto trading task as a case study. Experimental results demonstrate that DAO-Agent achieves up to 99.9% reduction in verification gas costs compared to naive on-chain alternatives, with constant-time verification complexity that remains stable as coalition size increases, thereby establishing a scalable foundation for agent coordination in decentralized environments.

</details>


### [34] [A Plan Reuse Mechanism for LLM-Driven Agent](https://arxiv.org/abs/2512.21309)
*Guopeng Li,Ruiqi Wu,Haisheng Tan*

Main category: cs.MA

TL;DR: 本文提出了基于语义和意图分类的计划复用机制AgentReuse，显著提高了LLM驱动助手的响应效率。


<details>
  <summary>Details</summary>
Motivation: LLM驱动的个人助手在生成计划时延迟较高，且约30%的请求相似，计划复用可以有效降低延迟，但自然语言多样性和计划文本非结构化导致计划复用困难。

Method: 提出了AgentReuse计划复用机制，利用语义相似度和意图分类评估请求相似度，实现基于语义的计划复用。

Result: AgentReuse在真实数据集上实现了93%的有效计划复用率，F1分数0.9718，准确率0.9459，显著减少了93.12%的响应延迟。

Conclusion: AgentReuse有效解决了请求相似度评估和计划复用难题，大幅降低LLM驱动助手响应延迟，提升用户体验。

Abstract: Integrating large language models (LLMs) into personal assistants, like Xiao Ai and Blue Heart V, effectively enhances their ability to interact with humans, solve complex tasks, and manage IoT devices. Such assistants are also termed LLM-driven agents. Upon receiving user requests, the LLM-driven agent generates plans using an LLM, executes these plans through various tools, and then returns the response to the user. During this process, the latency for generating a plan with an LLM can reach tens of seconds, significantly degrading user experience. Real-world dataset analysis shows that about 30% of the requests received by LLM-driven agents are identical or similar, which allows the reuse of previously generated plans to reduce latency. However, it is difficult to accurately define the similarity between the request texts received by the LLM-driven agent through directly evaluating the original request texts. Moreover, the diverse expressions of natural language and the unstructured format of plan texts make implementing plan reuse challenging. To address these issues, we present and implement a plan reuse mechanism for LLM-driven agents called AgentReuse. AgentReuse leverages the similarities and differences among requests' semantics and uses intent classification to evaluate the similarities between requests and enable the reuse of plans. Experimental results based on a real-world dataset demonstrate that AgentReuse achieves a 93% effective plan reuse rate, an F1 score of 0.9718, and an accuracy of 0.9459 in evaluating request similarities, reducing latency by 93.12% compared with baselines without using the reuse mechanism.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [35] [Process Analytics -- Data-driven Business Process Management](https://arxiv.org/abs/2512.20703)
*Matthias Stierle,Karsten Kraume,Martin Matzner*

Main category: cs.SE

TL;DR: 本文提出了结合技术和组织视角的数据驱动流程分析新框架，强调人因与组织因素的重要性，并通过大企业案例验证了该框架。


<details>
  <summary>Details</summary>
Motivation: 近年来过程挖掘技术越来越聚焦于技术层面，忽视了人因与组织因素，导致流程分析的多面性认识降低，本研究旨在弥补这一空白。

Method: 本文通过归纳和演绎方法对流程分析概念及其多个维度进行系统化阐述，结合信息系统的社会技术视角展开分析。

Result: 提出了流程分析的多维度概念模型，强调技术与组织结合，且通过大型企业的数据驱动流程分析和自动化案例进行了实际验证。

Conclusion: 本文提出了一种结合了技术分析和组织、利益相关者的社会技术视角的数据驱动流程分析新范式，即流程分析（process analytics），并通过实际案例验证了其有效性。

Abstract: Data-driven analysis of business processes has a long tradition in research. However, recently the term of process mining is mostly used when referring to data-driven process analysis. As a consequence, awareness for the many facets of process analysis is decreasing. In particular, while an increasing focus is put onto technical aspects of the analysis, human and organisational concerns remain under the radar. Following the socio-technical perspective of information systems research, we propose a new perspective onto data-driven process analysis that combines the process of analysis with the organisation and its stakeholders. This paper conceptualises the term process analytics and its various dimensions by following both an inductive and deductive approach. The results are discussed by contrasting them to a real-life case study from a large company implementing data-driven process analysis and automation.

</details>


### [36] [One Tool Is Enough: Reinforcement Learning for Repository-Level LLM Agents](https://arxiv.org/abs/2512.20957)
*Zhaoxi Zhang,Yitong Duan,Yanzhi Zhang,Yiming Xu,Jiyan He,Yunfang Wu*

Main category: cs.SE

TL;DR: 提出了RepoNavigator，一个结合代码执行逻辑的单一工具跳转LLM代理，用于开源软件仓库中的定位修改文件和函数问题，并通过强化学习训练实现了性能领先。


<details>
  <summary>Details</summary>
Motivation: 由于大型开源软件仓库规模大且结构复杂，现有基于LLM的方法依赖多辅助工具且忽视代码执行逻辑，导致定位修改难度大且模型控制复杂，故需一种简化工具操作且贴合代码执行流程的方法。

Method: 提出了带有执行感知的单一工具跳转机制的LLM代理，并利用强化学习对基于预训练模型的RepoNavigator进行端到端训练，避免使用闭源蒸馏。

Result: 强化学习训练的RepoNavigator表现出色，7B模型优于14B基线，14B优于32B竞争者，32B模型甚至超过了如Claude-3.7等闭源模型，验证了该方法的有效性和优势。

Conclusion: RepoNavigator通过集成执行感知的单一工具跳转和基于强化学习的端到端训练，在开源软件仓库级别的问题定位任务中取得了优于更大规模模型和闭源模型的顶尖性能，证明了其高效且可扩展的优越性。

Abstract: Locating the files and functions requiring modification in large open-source software (OSS) repositories is challenging due to their scale and structural complexity. Existing large language model (LLM)-based methods typically treat this as a repository-level retrieval task and rely on multiple auxiliary tools, which overlook code execution logic and complicate model control. We propose RepoNavigator, an LLM agent equipped with a single execution-aware tool-jumping to the definition of an invoked symbol. This unified design reflects the actual flow of code execution while simplifying tool manipulation. RepoNavigator is trained end-to-end via Reinforcement Learning (RL) directly from a pretrained model, without any closed-source distillation. Experiments demonstrate that RL-trained RepoNavigator achieves state-of-the-art performance, with the 7B model outperforming 14B baselines, the 14B model surpassing 32B competitors, and even the 32B model exceeding closed-source models such as Claude-3.7. These results confirm that integrating a single, structurally grounded tool with RL training provides an efficient and scalable solution for repository-level issue localization.

</details>


### [37] [Artificial or Just Artful? Do LLMs Bend the Rules in Programming?](https://arxiv.org/abs/2512.21028)
*Oussama Ben Sghaier,Kevin Delcourt,Houari Sahraoui*

Main category: cs.SE

TL;DR: 本文探讨了不同提示条件下，LLMs如何利用测试用例调整代码生成策略。测试用例的可见性大幅提升正确率，模型多采用测试驱动精炼策略，揭示了预训练目标与对齐限制之间的矛盾及其解决方式。


<details>
  <summary>Details</summary>
Motivation: LLMs在代码生成中的成功掩盖了预训练目标与对齐选择（如微调或提示）之间的矛盾，特别是在具有代理行为的人工智能场景下，测试用例作为强上下文信号可能被模型利用，本文旨在探究LLMs如何在不同提示条件下调整代码生成策略。

Method: 本文使用BigCodeBench (Hard)数据集设计五种不同提示条件，操控测试用例的可见性和使用限制，评估五种LLMs在正确性、代码相似度、程序大小和代码变更方面的表现，并分析模型间的一致性以识别适应策略。

Result: 测试用例可见性显著提升模型性能，有些模型的正确率几乎翻倍。显式限制或部分曝光仅部分抑制该效果。分析发现四种适应策略，其中测试驱动的代码精炼最为常见。

Conclusion: 大型语言模型（LLMs）在自动代码生成中表现突出，但预训练目标与对齐策略之间存在冲突。测试用例的可见性显著影响模型性能，正确率大幅提升。即使存在使用限制，模型仍会部分利用测试信息。主要的适应策略包括测试驱动的代码精炼。

Abstract: Large Language Models (LLMs) are widely used for automated code generation, yet their apparent successes often mask a tension between pretraining objectives and alignment choices. While pretraining encourages models to exploit all available signals to maximize success, alignment, whether through fine-tuning or prompting, may restrict their use. This conflict is especially salient in agentic AI settings, for instance when an agent has access to unit tests that, although intended for validation, act as strong contextual signals that can be leveraged regardless of explicit prohibitions. In this paper, we investigate how LLMs adapt their code generation strategies when exposed to test cases under different prompting conditions. Using the BigCodeBench (Hard) dataset, we design five prompting conditions that manipulate test visibility and impose explicit or implicit restrictions on their use. We evaluate five LLMs (four open-source and one closed-source) across correctness, code similarity, program size, and code churn, and analyze cross-model consistency to identify recurring adaptation strategies. Our results show that test visibility dramatically alters performance, correctness nearly doubles for some models, while explicit restrictions or partial exposure only partially mitigate this effect. Beyond raw performance, we identify four recurring adaptation strategies, with test-driven refinement emerging as the most frequent. These results highlight how LLMs adapt their behavior when exposed to contextual signals that conflict with explicit instructions, providing useful insight into how models reconcile pretraining objectives with alignment constraints.

</details>


### [38] [Assessing the Software Security Comprehension of Large Language Models](https://arxiv.org/abs/2512.21238)
*Mohammed Latif Siddiq,Natalie Sekerak,Antonio Karam,Maria Leal,Arvin Islam-Gomes,Joanna C. S. Santos*

Main category: cs.SE

TL;DR: 本文评估了五个主流大型语言模型在软件安全领域从记忆到创造六个认知层次的表现，发现它们在基础任务表现优异，但高级安全任务能力有限，并识别了模型常见的安全认知误区。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型被广泛应用于软件开发，但其在软件安全领域的专业水平尚不明确，因此通过系统评估其安全理解能力，为相关应用提供参考。

Method: 使用Blooms认知领域分类法，结合多样化数据集（包括选择题、易受攻击代码、课程评估、真实案例和项目任务），系统评估五种主流大型语言模型的六个认知维度上的表现。

Result: 模型在较低级认知任务（如记忆事实和识别已知漏洞）表现良好，但在需要推理和创建安全系统等高级任务上性能明显下降。提出了安全知识边界概念，并发现51种反复出现的误区模式。

Conclusion: 大型语言模型在软件安全领域表现出较强的基本知识掌握能力，但在高级认知任务如推理、架构评估和安全系统创建方面表现不足，揭示了其安全知识的边界和多次出现的误区。

Abstract: Large language models (LLMs) are increasingly used in software development, but their level of software security expertise remains unclear. This work systematically evaluates the security comprehension of five leading LLMs: GPT-4o-Mini, GPT-5-Mini, Gemini-2.5-Flash, Llama-3.1, and Qwen-2.5, using Blooms Taxonomy as a framework. We assess six cognitive dimensions: remembering, understanding, applying, analyzing, evaluating, and creating. Our methodology integrates diverse datasets, including curated multiple-choice questions, vulnerable code snippets (SALLM), course assessments from an Introduction to Software Security course, real-world case studies (XBOW), and project-based creation tasks from a Secure Software Engineering course. Results show that while LLMs perform well on lower-level cognitive tasks such as recalling facts and identifying known vulnerabilities, their performance degrades significantly on higher-order tasks that require reasoning, architectural evaluation, and secure system creation. Beyond reporting aggregate accuracy, we introduce a software security knowledge boundary that identifies the highest cognitive level at which a model consistently maintains reliable performance. In addition, we identify 51 recurring misconception patterns exhibited by LLMs across Blooms levels.

</details>
