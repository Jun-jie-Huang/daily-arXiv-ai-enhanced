<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 109]
- [cs.SE](#cs.SE) [Total: 25]
- [cs.MA](#cs.MA) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Enhancing Biomedical Named Entity Recognition using GLiNER-BioMed with Targeted Dictionary-Based Post-processing for BioASQ 2025 task 6](https://arxiv.org/abs/2510.08588)
*Ritesh Mehta*

Main category: cs.CL

TL;DR: 本文评估了GLiNER-BioMed模型在生物医学命名实体识别中的表现，并提出基于词典的后处理策略以减少实体类型混淆，提升了开发集的性能，但未能在盲测集上获得同样提升。


<details>
  <summary>Details</summary>
Motivation: 生物医学命名实体识别任务中，基因与化学物质等相似实体类型易混淆，影响信息提取准确性，需要改进模型性能。

Method: 采用GLiNER-BioMed预训练模型，结合基于词典的后处理方法，尝试减少误分类，同时探索条件随机场等其它方法。

Result: 后处理策略在开发集上将微平均F1从0.79提升到0.83，但在盲测集未实现提升，仍为0.77，低于基线0.79。

Conclusion: 基于词典的后处理对BioNER模型有潜力提升精度，但存在过拟合风险，实际应用中需注重模型泛化能力。

Abstract: Biomedical Named Entity Recognition (BioNER), task6 in BioASQ (A challenge in
large-scale biomedical semantic indexing and question answering), is crucial
for extracting information from scientific literature but faces hurdles such as
distinguishing between similar entity types like genes and chemicals. This
study evaluates the GLiNER-BioMed model on a BioASQ dataset and introduces a
targeted dictionary-based post-processing strategy to address common
misclassifications. While this post-processing approach demonstrated notable
improvement on our development set, increasing the micro F1-score from a
baseline of 0.79 to 0.83, this enhancement did not generalize to the blind test
set, where the post-processed model achieved a micro F1-score of 0.77 compared
to the baselines 0.79. We also discuss insights gained from exploring
alternative methodologies, including Conditional Random Fields. This work
highlights the potential of dictionary-based refinement for pre-trained BioNER
models but underscores the critical challenge of overfitting to development
data and the necessity of ensuring robust generalization for real-world
applicability.

</details>


### [2] [Less Diverse, Less Safe: The Indirect But Pervasive Risk of Test-Time Scaling in Large Language Models](https://arxiv.org/abs/2510.08592)
*Shahriar Kabir Nahin,Hadi Askari,Muhao Chen,Anshuman Chhabra*

Main category: cs.CL

TL;DR: 本文发现测试时间缩放(TTS)中的候选多样性减少会显著增加不安全输出的风险，并提出了一个基于引用的多样性减少诊断攻击协议RefDiv。


<details>
  <summary>Details</summary>
Motivation: 当前TTS方法假设候选答案的多样性越高，推理结果越可靠，但减少多样性可能导致模型产生更多不安全输出，需验证并分析这一潜在风险。

Method: 作者设计了RefDiv协议，通过限制候选多样性来对TTS方法进行压力测试，实验涵盖多个开源和闭源大模型及两种常用TTS策略，并评估安全检测器的效果。

Result: 当候选多样性受限时，TTS产生不安全输出的概率显著上升，这种现象跨模型、跨策略均存在，且现有安全检测工具难以识别这些对抗性输入提示。

Conclusion: 候选多样性的减少是TTS方法中的一个普遍而未被充分认识的失效模式，未来需研发既有效又安全的鲁棒TTS策略，提升模型推理的安全性和可靠性。

Abstract: Test-Time Scaling (TTS) improves LLM reasoning by exploring multiple
candidate responses and then operating over this set to find the best output. A
tacit premise behind TTS is that sufficiently diverse candidate pools enhance
reliability. In this work, we show that this assumption in TTS introduces a
previously unrecognized failure mode. When candidate diversity is curtailed,
even by a modest amount, TTS becomes much more likely to produce unsafe
outputs. We present a reference-guided diversity reduction protocol (RefDiv)
that serves as a diagnostic attack to stress test TTS pipelines. Through
extensive experiments across four open-source models (Qwen3, Mistral, Llama3.1,
Gemma3) and two widely used TTS strategies (Monte Carlo Tree Search and
Best-of-N), constraining diversity consistently signifies the rate at which TTS
produces unsafe results. The effect is often stronger than that produced by
prompts directly with high adversarial intent scores. This observed phenomenon
also transfers across TTS strategies and to closed-source models (e.g. OpenAI
o3 and Gemini-2.5-Pro), thus indicating that this is a general and extant
property of TTS rather than a model-specific artifact. Additionally, we find
that numerous widely used safety guardrail classifiers (e.g. Llama-Guard and
OpenAI Moderation API), are unable to flag the adversarial input prompts
generated by RefDiv, demonstrating that existing defenses offer limited
protection against this diversity-driven failure mode. Through this work, we
hope to motivate future research on designing robust TTS strategies that are
both effective and secure against diversity-targeted stress tests as
illustrated by RefDiv.

</details>


### [3] [Hierarchical Self-Supervised Representation Learning for Depression Detection from Speech](https://arxiv.org/abs/2510.08593)
*Yuxin Li,Eng Siong Chng,Cuntai Guan*

Main category: cs.CL

TL;DR: 本文提出了一种基于多层自监督学习特征融合的抑郁症语音检测新方法HAREN-CTC，有效提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有抑郁症语音检测方法大多依赖单层特征，难以捕捉稀疏且异质的抑郁线索，且易过拟合，缺乏充分利用多层次特征层次结构的能力。

Method: 提出HAREN-CTC架构，包含分层自适应聚类模块和跨模态融合模块，通过多任务学习框架和连接时序分类(CTC)损失整合多层SSL特征，利用跨注意力机制建模不同特征层间依赖，CTC损失实现时间序列对齐，适应抑郁语音信号的稀疏时间模式。

Result: 在DAIC-WOZ和MODMA数据集上进行标准数据划分和五折交叉验证测试，取得了宏F1分数0.81和0.82的最新最佳成绩，优于之前的方法。

Conclusion: 多层SSL特征融合结合CTC损失和跨注意力机制能够有效捕捉抑郁语音的复杂时间模式，有助于提升语音抑郁检测的准确性和泛化能力。

Abstract: Speech-based depression detection (SDD) is a promising, non-invasive
alternative to traditional clinical assessments. However, it remains limited by
the difficulty of extracting meaningful features and capturing sparse,
heterogeneous depressive cues over time. Pretrained self-supervised learning
(SSL) models such as WavLM provide rich, multi-layer speech representations,
yet most existing SDD methods rely only on the final layer or search for a
single best-performing one. These approaches often overfit to specific datasets
and fail to leverage the full hierarchical structure needed to detect subtle
and persistent depression signals.
  To address this challenge, we propose HAREN-CTC, a novel architecture that
integrates multi-layer SSL features using cross-attention within a multitask
learning framework, combined with Connectionist Temporal Classification loss to
handle sparse temporal supervision. HAREN-CTC comprises two key modules: a
Hierarchical Adaptive Clustering module that reorganizes SSL features into
complementary embeddings, and a Cross-Modal Fusion module that models
inter-layer dependencies through cross-attention. The CTC objective enables
alignment-aware training, allowing the model to track irregular temporal
patterns of depressive speech cues.
  We evaluate HAREN-CTC under both an upper-bound setting with standard data
splits and a generalization setting using five-fold cross-validation. The model
achieves state-of-the-art macro F1-scores of 0.81 on DAIC-WOZ and 0.82 on
MODMA, outperforming prior methods across both evaluation scenarios.

</details>


### [4] [Systematic Diagnosis of Brittle Reasoning in Large Language Models](https://arxiv.org/abs/2510.08595)
*V. S. Raghu Parupudi*

Main category: cs.CL

TL;DR: 本文提出了一种基于GPT-3.5和GPT-4o-mini模型的新框架，分析机器学习模型在数学推理中的具体失误点，揭示模型在组合推理方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有评测难以细致诊断机器数学理解的具体短板，需更精准的评测方法来指导模型改进。

Method: 通过GPT-3.5生成数学推理步骤，利用GPT-4o-mini对推理错误进行分类并对推理句子聚类，识别出不同推理模式。

Result: 发现模型在顺序计算等程序化推理上准确率极高，但在受限组合推理上表现显著不足，表现出非人类的脆弱认知特征。

Conclusion: 该框架提供了更细粒度的数学理解评估方式，有助于明确推理技能的可靠性，为未来能力提升和应用可靠性指明方向。

Abstract: A central question in artificial intelligence is the extent to which machine
learning models comprehend mathematics. To address this, we propose a novel
framework for measuring mathematical reasoning that moves beyond standard
benchmarks to diagnose specific failure points. Our method first generates
structured, step-by-step reasoning from gpt-3.5-turbo on the GSM8K dataset. We
then use a more capable analyst model, gpt-4o-mini, to categorize errors and,
crucially, perform an unsupervised clustering of every reasoning sentence to
identify emergent "reasoning modes." This analysis reveals a cognitive profile
with a stark, nonhuman-like brittleness: while the model achieves near-perfect
accuracy on procedural modes like sequential calculation, its performance on
modes requiring combinatorial reasoning with restrictions plummets. By
identifying and quantifying the reliability of these distinct reasoning skills,
our work provides a more granular method to evaluate mathematical comprehension
and offers a precise roadmap for developing new capabilities and more reliable
future applications.

</details>


### [5] [Confidence, Not Perplexity: A Better Metric for the Creative Era of LLMs](https://arxiv.org/abs/2510.08596)
*V. S. Raghu Parupudi*

Main category: cs.CL

TL;DR: 本文提出了一种名为Confidence Score (CS)的评价指标，旨在减少对创意文本生成的偏见。


<details>
  <summary>Details</summary>
Motivation: 传统的无参考指标如自困惑度对创意文本生成存在较大偏见，导致评价不公平。

Method: 基于模型输出的概率分布，提出了CS指标，并在gpt-4o-mini模型上进行了实验比较。

Result: CS在99个创意提示中有19%的情况下偏好新颖回答，显著优于传统以流畅度为基础的指标（0%），且能有效区分难易任务。

Conclusion: CS缓解了传统指标的创意偏见，保持了评价的核心优势，提供了对现代大模型更平衡的评价手段。

Abstract: Reference-free metrics like self-perplexity are strongly biased against
creative text generation. We propose the Confidence Score (CS), derived from a
model's output probability distribution, as a less biased alternative.
Experiments on gpt-4o-mini show that while fluency-based metrics prefer novel
responses in 0\% of cases on 99 creative prompts, our CS does so 19% of the
time, a statistically significant difference (95% CI for difference: [11.1%,
27.3%]). We also show that CS effectively distinguishes between easy, medium,
and hard tasks, confirmed by non-overlapping confidence intervals. The
Confidence Score thus mitigates the creativity bias of traditional metrics
while retaining their core evaluative strengths, offering a more balanced
assessment for modern LLMs.

</details>


### [6] [Recover-LoRA: Data-Free Accuracy Recovery of Degraded Language Models via Low-Rank Adaptation](https://arxiv.org/abs/2510.08600)
*Devleena Das,Rajeev Patwari,Ashish Sirasao*

Main category: cs.CL

TL;DR: 该论文提出Recover-LoRA方法，通过合成数据和logit蒸馏，针对受损的小型语言模型恢复精度，提升5-17%。


<details>
  <summary>Details</summary>
Motivation: 推理优化如量化、剪枝等会导致语言模型精度下降，现有多数方法关注量化恢复，而忽视因模型权重损坏产生的精度下降问题。

Method: 提出Recover-LoRA方法，使用合成数据和logit蒸馏技术，在受损模型的部分层学习LoRA适配器，实现模型向全精度模型的对齐。

Result: 在包含多头注意力和组查询注意力的小型语言模型及多个评估数据集上，Recover-LoRA能提升模型精度5-17%。

Conclusion: Recover-LoRA是一种轻量、数据无关的准确率恢复方法，有效改善了多种注意力结构下因因权重劣化引起的性能下降问题。

Abstract: Inference optimizations such as quantization, pruning, format and datatype
conversion, model export, and serialization can lead to functional degradations
in language model task performance. While most efforts on performance recovery
for deployment focus on robust quantization techniques, we focus on recovering
model accuracies from any sources that degrade model weights, such as improper
model serialization. In this work, we propose Recover-LoRA, a lightweight and
dataset agnostic method to recover accuracy in degraded models. Recover-LoRA
uses synthetic data and logit distillation to learn LoRA adapters on selective
layers that facilitate aligning the degraded model to its full precision model.
We investigate the utility of Recover-LoRA across a diverse set of small
language models (SLMs), including models with varying attention architectures,
multi-head attention (MHA) and group-query attention (GQA), as well as several
evaluation datasets. Our results show that Recover-LoRA recovers model
accuracies by 5-17% on MHA and GQA SLMs.

</details>


### [7] [Mnemosyne: An Unsupervised, Human-Inspired Long-Term Memory Architecture for Edge-Based LLMs](https://arxiv.org/abs/2510.08601)
*Aneesh Jonelagadda,Christina Hahn,Haoze Zheng,Salvatore Penachio*

Main category: cs.CL

TL;DR: 本文提出了适用于边缘设备的大型语言模型的无监督长期记忆架构Mnemosyne，通过图结构存储和模拟人类记忆机制，实现更高效的记忆管理和召回，特别在医疗健康对话中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型的记忆机制依赖于粗暴的上下文扩展或静态检索，难以满足边缘设备和长期对话的需求，特别是在医疗健康等领域，重复且时间相隔的对话检索效率低下。

Method: 提出Mnemosyne架构，包括图结构存储、模块化过滤、记忆提交与修剪、基于概率的召回与时间衰减和刷新机制，同时提炼核心摘要捕捉用户个性及领域长期细节，模仿人类记忆机制。

Result: 在医疗健康长期对话实验中，Mnemosyne在盲评中以65.8%的胜率显著优于RAG的31.1%，且在LoCoMo基准测试中的时间推理和单跳检索表现最好，整体得分54.6%排名第二，超过多个主流基线。

Conclusion: Mnemosyne证明了通过边缘兼容且易于迁移的无监督记忆架构，能够提升语言模型的事实召回、时间推理能力及自然交互，适合长期、多轮、语义重复但时间区分的对话场景，尤其适用于医疗健康领域。

Abstract: Long-term memory is essential for natural, realistic dialogue. However,
current large language model (LLM) memory systems rely on either brute-force
context expansion or static retrieval pipelines that fail on edge-constrained
devices. We introduce Mnemosyne, an unsupervised, human-inspired long-term
memory architecture designed for edge-based LLMs. Our approach uses
graph-structured storage, modular substance and redundancy filters, memory
committing and pruning mechanisms, and probabilistic recall with temporal decay
and refresh processes modeled after human memory. Mnemosyne also introduces a
concentrated "core summary" efficiently derived from a fixed-length subset of
the memory graph to capture the user's personality and other domain-specific
long-term details such as, using healthcare application as an example,
post-recovery ambitions and attitude towards care. Unlike existing
retrieval-augmented methods, Mnemosyne is designed for use in longitudinal
healthcare assistants, where repetitive and semantically similar but temporally
distinct conversations are limited by naive retrieval. In experiments with
longitudinal healthcare dialogues, Mnemosyne demonstrates the highest win rate
of 65.8% in blind human evaluations of realism and long-term memory capability
compared to a baseline RAG win rate of 31.1%. Mnemosyne also achieves current
highest LoCoMo benchmark scores in temporal reasoning and single-hop retrieval
compared to other same-backboned techniques. Further, the average overall score
of 54.6% was second highest across all methods, beating commonly used Mem0 and
OpenAI baselines among others. This demonstrates that improved factual recall,
enhanced temporal reasoning, and much more natural user-facing responses can be
feasible with an edge-compatible and easily transferable unsupervised memory
architecture.

</details>


### [8] [Human Texts Are Outliers: Detecting LLM-generated Texts via Out-of-distribution Detection](https://arxiv.org/abs/2510.08602)
*Cong Zeng,Shengkun Tang,Yuanzhou Chen,Zhiqiang Shen,Wenchao Yu,Xujiang Zhao,Haifeng Chen,Wei Cheng,Zhiqiang Xu*

Main category: cs.CL

TL;DR: 本文针对大语言模型生成文本的检测问题，提出将其视为异常检测任务，利用单类学习和能量基方法实现了对人类文本的有效识别，显著提升了检测的泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法将人类文本与机器文本的检测作为二分类问题，忽略了人类文本本身的多样性，导致模型泛化性差。

Method: 将检测任务重构为异常检测，采用单类学习方法（DeepSVDD、HRN）和基于能量的评分方法，识别人类文本为分布外样本。

Result: 在多个数据集上获得了98.3% AUROC和AUPR，8.9%的FPR95，表现优异，并验证了在多语言、攻击和未知模型环境下的鲁棒性。

Conclusion: 将AI生成文本检测视为异常检测问题能够有效提升模型的泛化能力和检测性能，具有广泛应用前景。

Abstract: The rapid advancement of large language models (LLMs) such as ChatGPT,
DeepSeek, and Claude has significantly increased the presence of AI-generated
text in digital communication. This trend has heightened the need for reliable
detection methods to distinguish between human-authored and machine-generated
content. Existing approaches both zero-shot methods and supervised classifiers
largely conceptualize this task as a binary classification problem, often
leading to poor generalization across domains and models. In this paper, we
argue that such a binary formulation fundamentally mischaracterizes the
detection task by assuming a coherent representation of human-written texts. In
reality, human texts do not constitute a unified distribution, and their
diversity cannot be effectively captured through limited sampling. This causes
previous classifiers to memorize observed OOD characteristics rather than learn
the essence of `non-ID' behavior, limiting generalization to unseen
human-authored inputs. Based on this observation, we propose reframing the
detection task as an out-of-distribution (OOD) detection problem, treating
human-written texts as distributional outliers while machine-generated texts
are in-distribution (ID) samples. To this end, we develop a detection framework
using one-class learning method including DeepSVDD and HRN, and score-based
learning techniques such as energy-based method, enabling robust and
generalizable performance. Extensive experiments across multiple datasets
validate the effectiveness of our OOD-based approach. Specifically, the
OOD-based method achieves 98.3% AUROC and AUPR with only 8.9% FPR95 on DeepFake
dataset. Moreover, we test our detection framework on multilingual, attacked,
and unseen-model and -domain text settings, demonstrating the robustness and
generalizability of our framework. Code, pretrained weights, and demo will be
released.

</details>


### [9] [Identifying & Interactively Refining Ambiguous User Goals for Data Visualization Code Generation](https://arxiv.org/abs/2510.09390)
*Mert İnan,Anthony Sicilia,Alex Xie,Saujas Vaduguru,Daniel Fried,Malihe Alikhani*

Main category: cs.CL

TL;DR: 本文研究了数据可视化领域中自然语言歧义对代码生成准确性的影响，提出了歧义类型分类和量化指标，并通过多轮对话策略减少歧义，提高了代码准确度。


<details>
  <summary>Details</summary>
Motivation: 人机沟通中的共享目标建立关键，但自然语言的歧义常导致生成的代码未能准确表达用户意图，尤其在数据可视化领域表现突出。

Method: 利用DS-1000数据集中的Matplotlib问题，提出歧义类型分类与量化指标，比较了多种歧义测量方法，并引入三种语用模型指导多轮对话策略以减少歧义。

Result: 歧义指标比不确定性基线与人工注释的相关性更好；基于语用模型的多轮对话策略显著降低了歧义，提高了代码生成与用户目标的一致性。

Conclusion: 多轮语用对话在减少语义歧义、提升数据可视化代码生成准确性方面展现出显著价值，有助于更好地实现人机共享目标。

Abstract: Establishing shared goals is a fundamental step in human-AI communication.
However, ambiguities can lead to outputs that seem correct but fail to reflect
the speaker's intent. In this paper, we explore this issue with a focus on the
data visualization domain, where ambiguities in natural language impact the
generation of code that visualizes data. The availability of multiple views on
the contextual (e.g., the intended plot and the code rendering the plot) allows
for a unique and comprehensive analysis of diverse ambiguity types. We develop
a taxonomy of types of ambiguity that arise in this task and propose metrics to
quantify them. Using Matplotlib problems from the DS-1000 dataset, we
demonstrate that our ambiguity metrics better correlate with human annotations
than uncertainty baselines. Our work also explores how multi-turn dialogue can
reduce ambiguity, therefore, improve code accuracy by better matching user
goals. We evaluate three pragmatic models to inform our dialogue strategies:
Gricean Cooperativity, Discourse Representation Theory, and Questions under
Discussion. A simulated user study reveals how pragmatic dialogues reduce
ambiguity and enhance code accuracy, highlighting the value of multi-turn
exchanges in code generation.

</details>


### [10] [YpathRAG:A Retrieval-Augmented Generation Framework and Benchmark for Pathology](https://arxiv.org/abs/2510.08603)
*Deshui Yu,Yizhi Wang,Saihui Jin,Taojie Zhu,Fanyi Zeng,Wen Qian,Zirui Huang,Jingli Ouyang,Jiameng Li,Zhen Song,Tian Guan,Yonghong He*

Main category: cs.CL

TL;DR: 该论文提出了面向病理学的检索增强生成模型YpathRAG，通过构建大规模病理向量数据库和双通道检索架构，显著提升了模型的检索精度和回答准确率。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在高门槛领域如病理学仍存在输出幻觉问题，且传统领域微调无法有效扩展知识边界和保证证据依据。

Method: 构建覆盖28个子领域、包含1.53百万段落的病理学向量数据库，设计双通道混合检索方法（BGE-M3密集检索结合词汇引导的稀疏检索），并引入基于LLM的支持证据判别模块，闭环检索、判别和生成流程。

Result: 在YpathR基准上，YpathRAG的Recall@5达到98.64%，比基线提升23个百分点；在包含300个高难度问题的YpathQA-M测试中，模型在一般和医学LLM基础上提高平均9%的准确率，最高提升至15.6%。

Conclusion: YpathRAG有效提升了病理学领域的检索质量和事实可靠性，提供了可扩展的构建范式和可解释的评估方法，推动了面向病理学的RAG模型发展。

Abstract: Large language models (LLMs) excel on general tasks yet still hallucinate in
high-barrier domains such as pathology. Prior work often relies on domain
fine-tuning, which neither expands the knowledge boundary nor enforces
evidence-grounded constraints. We therefore build a pathology vector database
covering 28 subfields and 1.53 million paragraphs, and present YpathRAG, a
pathology-oriented RAG framework with dual-channel hybrid retrieval (BGE-M3
dense retrieval coupled with vocabulary-guided sparse retrieval) and an
LLM-based supportive-evidence judgment module that closes the
retrieval-judgment-generation loop. We also release two evaluation benchmarks,
YpathR and YpathQA-M. On YpathR, YpathRAG attains Recall@5 of 98.64%, a gain of
23 percentage points over the baseline; on YpathQA-M, a set of the 300 most
challenging questions, it increases the accuracies of both general and medical
LLMs by 9.0% on average and up to 15.6%. These results demonstrate improved
retrieval quality and factual reliability, providing a scalable construction
paradigm and interpretable evaluation for pathology-oriented RAG.

</details>


### [11] [LatentBreak: Jailbreaking Large Language Models through Latent Space Feedback](https://arxiv.org/abs/2510.08604)
*Raffaele Mura,Giorgio Piras,Kamilė Lukošiūtė,Maura Pintor,Amin Karbasi,Battista Biggio*

Main category: cs.CL

TL;DR: 本文提出LatentBreak，一种能够绕过基于困惑度过滤的白盒攻击方法，通过同义词替换生成低困惑度对抗性提示，从而有效攻击安全对齐的大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有自动化的越狱攻击通过添加高困惑度的对抗后缀或长提示模板来绕过模型安全机制，但这些攻击通过简单的困惑度过滤即可被检测。

Method: LatentBreak通过在潜在空间中选择语义等价词替换输入提示中的词汇，保持提示的初衷同时生成低困惑度、自然的对抗性提示，避免了使用高困惑度的后缀或长模板。

Result: 实验证明，LatentBreak生成的提示更短、困惑度更低，能更好地绕过基于困惑度的过滤器，表现优于现有越狱攻击算法。

Conclusion: LatentBreak有效克服了基于困惑度过滤的检测机制，在保持语义一致性的同时生成对抗性提示，提升了攻击成功率。

Abstract: Jailbreaks are adversarial attacks designed to bypass the built-in safety
mechanisms of large language models. Automated jailbreaks typically optimize an
adversarial suffix or adapt long prompt templates by forcing the model to
generate the initial part of a restricted or harmful response. In this work, we
show that existing jailbreak attacks that leverage such mechanisms to unlock
the model response can be detected by a straightforward perplexity-based
filtering on the input prompt. To overcome this issue, we propose LatentBreak,
a white-box jailbreak attack that generates natural adversarial prompts with
low perplexity capable of evading such defenses. LatentBreak substitutes words
in the input prompt with semantically-equivalent ones, preserving the initial
intent of the prompt, instead of adding high-perplexity adversarial suffixes or
long templates. These words are chosen by minimizing the distance in the latent
space between the representation of the adversarial prompt and that of harmless
requests. Our extensive evaluation shows that LatentBreak leads to shorter and
low-perplexity prompts, thus outperforming competing jailbreak algorithms
against perplexity-based filters on multiple safety-aligned models.

</details>


### [12] [Toward a Safer Web: Multilingual Multi-Agent LLMs for Mitigating Adversarial Misinformation Attacks](https://arxiv.org/abs/2510.08605)
*Nouar Aldahoul,Yasir Zaki*

Main category: cs.CL

TL;DR: 本文研究了跨语言转换和结构重组对错误信息检测的影响，提出了一个多语言、多智能体的大型语言模型框架，支持作为网页插件部署。


<details>
  <summary>Details</summary>
Motivation: 当前数字平台上错误信息传播迅速，威胁公共话语和决策，虽然已有对抗攻击研究，但特定语言切换和文本重组未被系统研究。

Method: 研究了英语、法语、西班牙语、阿拉伯语、印地语和中文之间的语言切换及翻译，查询长度增加与摘要，以及多选题结构重组。提出一个多语言多智能体的检索增强生成模型，支持网页插件形式部署。

Result: 该框架能够有效应对多样化攻击，提升错误信息检测的准确性和实时性。展示了插件化部署在实际网络平台上的可行性。

Conclusion: AI驱动的多语言多智能体检测框架在维护网络事实真实性方面具有重要作用，且插件式部署便于实际应用推广。

Abstract: The rapid spread of misinformation on digital platforms threatens public
discourse, emotional stability, and decision-making. While prior work has
explored various adversarial attacks in misinformation detection, the specific
transformations examined in this paper have not been systematically studied. In
particular, we investigate language-switching across English, French, Spanish,
Arabic, Hindi, and Chinese, followed by translation. We also study query length
inflation preceding summarization and structural reformatting into
multiple-choice questions. In this paper, we present a multilingual,
multi-agent large language model framework with retrieval-augmented generation
that can be deployed as a web plugin into online platforms. Our work
underscores the importance of AI-driven misinformation detection in
safeguarding online factual integrity against diverse attacks, while showcasing
the feasibility of plugin-based deployment for real-world web applications.

</details>


### [13] [Centering Emotion Hotspots: Multimodal Local-Global Fusion and Cross-Modal Alignment for Emotion Recognition in Conversations](https://arxiv.org/abs/2510.08606)
*Yu Liu,Hanlei Shi,Haoxun Li,Yuqing Sun,Yuxuan Ding,Linlin Gong,Leyuan Qu,Taihao Li*

Main category: cs.CL

TL;DR: 该论文提出了一种以情感热点为中心的多模态对话情感识别模型，通过检测文本、音频和视频中的情感热点并融合全局特征，并使用路由的对齐混合器对模态进行对齐，提升了情感识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 情感识别困难在于情感线索稀疏、局部且不同模态之间时序不一致，亟需一种聚焦于关键信息的统一模型。

Method: 提出检测每句发言中的情感热点，利用Hotspot-Gated Fusion融合多模态热点与全局特征，采用路由的Mixture-of-Aligners实现模态对齐，并通过跨模态图编码对话结构。

Result: 在标准ERC基准测试中，该方法较强基线均有稳定提升，消融实验验证了Hotspot-Gated Fusion和Mixture-of-Aligners的有效性。

Conclusion: 聚焦情感热点的视角有效提升多模态情感识别性能，为模态融合和多模态学习提供了新的启示。

Abstract: Emotion Recognition in Conversations (ERC) is hard because discriminative
evidence is sparse, localized, and often asynchronous across modalities. We
center ERC on emotion hotspots and present a unified model that detects
per-utterance hotspots in text, audio, and video, fuses them with global
features via Hotspot-Gated Fusion, and aligns modalities using a routed
Mixture-of-Aligners; a cross-modal graph encodes conversational structure. This
design focuses modeling on salient spans, mitigates misalignment, and preserves
context. Experiments on standard ERC benchmarks show consistent gains over
strong baselines, with ablations confirming the contributions of HGF and MoA.
Our results point to a hotspot-centric view that can inform future multimodal
learning, offering a new perspective on modality fusion in ERC.

</details>


### [14] [MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation](https://arxiv.org/abs/2510.08608)
*Weihua Zheng,Zhengyuan Liu,Tanmoy Chakraborty,Weiwen Xu,Xiaoxue Gao,Bryan Chen Zhengyu Tan,Bowei Zou,Chang Liu,Yujia Hu,Xing Xie,Xiaoyuan Yi,Jing Yao,Chaojun Wang,Long Li,Rui Liu,Huiyao Liu,Koji Inoue,Ryuichi Sumida,Tatsuya Kawahara,Fan Xu,Lingyu Ye,Wei Tian,Dongjun Kim,Jimin Jung,Jaehyung Seo,Nadya Yuki Wangsajaya,Pham Minh Duc,Ojasva Saxena,Palash Nandi,Xiyan Tao,Wiwik Karlina,Tuan Luong,Keertana Arun Vasan,Roy Ka-Wei Lee,Nancy F. Chen*

Main category: cs.CL

TL;DR: 本文提出了MMA-ASIA框架，评估大语言模型在亚洲文化背景下的多模态文化意识，包含多语言、多国家的27,000道多步推理题目，覆盖文本、图像和语音三模态。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在西方高资源环境外的多模态理解和推理能力下降，亟需一个针对亚洲文化背景的全面评估框架。

Method: 构建一个涵盖8个亚洲国家、10种语言的多模态多选择题基准，设计五维度评价协议，并通过文化意识验证模块防止模型利用捷径学习；采用比较分析、注意力追踪和视觉消融前缀重放方法深入分析模型性能差异。

Result: 成功建立了跨文本、图像和语音模态的多语言亚洲文化评价基准，揭示了模型在不同语言与模态间表现不一致的原因。

Conclusion: MMA-ASIA框架为构建文化可靠的多模态大语言模型提供了系统评估手段和深刻洞察，有助提升模型在多文化环境下的泛化能力和可信度。

Abstract: Large language models (LLMs) are now used worldwide, yet their multimodal
understanding and reasoning often degrade outside Western, high-resource
settings. We propose MMA-ASIA, a comprehensive framework to evaluate LLMs'
cultural awareness with a focus on Asian contexts. MMA-ASIA centers on a
human-curated, multilingual, and multimodally aligned multiple-choice benchmark
covering 8 Asian countries and 10 languages, comprising 27,000 questions; over
79 percent require multi-step reasoning grounded in cultural context, moving
beyond simple memorization. To our knowledge, this is the first dataset aligned
at the input level across three modalities: text, image (visual question
answering), and speech. This enables direct tests of cross-modal transfer.
Building on this benchmark, we propose a five-dimensional evaluation protocol
that measures: (i) cultural-awareness disparities across countries, (ii)
cross-lingual consistency, (iii) cross-modal consistency, (iv) cultural
knowledge generalization, and (v) grounding validity. To ensure rigorous
assessment, a Cultural Awareness Grounding Validation Module detects "shortcut
learning" by checking whether the requisite cultural knowledge supports correct
answers. Finally, through comparative model analysis, attention tracing, and an
innovative Vision-ablated Prefix Replay (VPR) method, we probe why models
diverge across languages and modalities, offering actionable insights for
building culturally reliable multimodal LLMs.

</details>


### [15] [GraphGhost: Tracing Structures Behind Large Language Models](https://arxiv.org/abs/2510.08613)
*Xinnan Dai,Kai Guo,Chung-Hsiang Lo,Shenglai Zeng,Jiayuan Ding,Dongsheng Luo,Subhabrata Mukherjee,Jiliang Tang*

Main category: cs.CL

TL;DR: 本文提出了GraphGhost框架，通过图结构表示大语言模型的神经元激活和信号传播，揭示了其推理能力的结构机制。


<details>
  <summary>Details</summary>
Motivation: 当前对大语言模型推理能力的结构机制尚未充分理解，缺乏有效的分析工具。

Method: 构建神经元激活和信号传播的图结构，利用图算法（如PageRank）分析模型内部推理行为，并通过结构干预验证关键神经元对推理能力的影响。

Result: 发现了不同模型在推理行为上的共性与特异性，关键神经元节点的编辑会导致推理崩溃，影响逻辑和语义理解。

Conclusion: GraphGhost提供了一种强有力的工具，帮助理解、分析和干预大语言模型推理的结构基础。

Abstract: Large Language Models (LLMs) demonstrate remarkable reasoning capabilities,
yet the structural mechanisms underlying these abilities remain under explored.
In this work, we introduce GraphGhost, a unified framework that represents
neuron activations and their signal propagation as graphs, explaining how LLMs
capture structural semantics from sequential inputs and generate outputs
through structurally consistent mechanisms. This graph-based perspective
enables us to employ graph algorithms such as PageRank to characterize the
properties of LLMs, revealing both shared and model-specific reasoning
behaviors across diverse datasets. We further identify the activated neurons
within GraphGhost and evaluate them through structural interventions, showing
that edits to key neuron nodes can trigger reasoning collapse, altering both
logical flow and semantic understanding. Together, these contributions position
GraphGhost as a powerful tool for analyzing, intervening in, and ultimately
understanding the structural foundations of reasoning in LLMs.

</details>


### [16] [Gender Bias in Large Language Models for Healthcare: Assignment Consistency and Clinical Implications](https://arxiv.org/abs/2510.08614)
*Mingxuan Liu,Yuhe Ke,Wentao Zhu,Mayli Mertens,Yilin Ning,Jingchi Liao,Chuan Hong,Daniel Shu Wei Ting,Yifan Peng,Danielle S. Bitterman,Marcus Eng Hock Ong,Nan Liu*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型（LLMs）在医疗领域中的性别偏见问题，发现模型在诊断中对患者性别相关性的判断存在较大不一致，存在性别差异偏见。


<details>
  <summary>Details</summary>
Motivation: LLMs在医疗中的应用可能带来性别偏见，影响诊断和临床决策的公平性和可靠性。

Method: 通过分析来自NEJM挑战赛的病例，赋予多种开源和专有LLMs不同性别身份，评估其诊断结果及对患者性别相关性的判断一致性。

Result: 诊断结果在不同LLM性别身份间较为一致，但患者性别相关性判断存在显著不一致，并显示出系统性性别差异偏见。

Conclusion: LLMs存在未充分研究的性别偏见，建议在临床应用中常规检查模型的身份赋值一致性，以确保公平可靠的人工智能辅助医疗。

Abstract: The integration of large language models (LLMs) into healthcare holds promise
to enhance clinical decision-making, yet their susceptibility to biases remains
a critical concern. Gender has long influenced physician behaviors and patient
outcomes, raising concerns that LLMs assuming human-like roles, such as
clinicians or medical educators, may replicate or amplify gender-related
biases. Using case studies from the New England Journal of Medicine Challenge
(NEJM), we assigned genders (female, male, or unspecified) to multiple
open-source and proprietary LLMs. We evaluated their response consistency
across LLM-gender assignments regarding both LLM-based diagnosis and models'
judgments on the clinical relevance or necessity of patient gender. In our
findings, diagnoses were relatively consistent across LLM genders for most
models. However, for patient gender's relevance and necessity in LLM-based
diagnosis, all models demonstrated substantial inconsistency across LLM
genders, particularly for relevance judgements. Some models even displayed a
systematic female-male disparity in their interpretation of patient gender.
These findings present an underexplored bias that could undermine the
reliability of LLMs in clinical practice, underscoring the need for routine
checks of identity-assignment consistency when interacting with LLMs to ensure
reliable and equitable AI-supported clinical care.

</details>


### [17] [Iterative LLM-Based Generation and Refinement of Distracting Conditions in Math Word Problems](https://arxiv.org/abs/2510.08615)
*Kaiqi Yang,Hang Li,Yucheng Chu,Zitao Liu,Mi Tian,Hui Liu*

Main category: cs.CL

TL;DR: 本文提出了一个利用大语言模型自动生成数学文字题中干扰条件的迭代框架，以解决现有数据集中的干扰条件缺失和难度不够问题，保证干扰条件不改变原题解，减少人工复核工作。


<details>
  <summary>Details</summary>
Motivation: 当前数学文字题数据集中缺少含有干扰条件的高难度问题，且干扰条件容易被识别且忽略，影响对大语言模型推理能力的评估。人工添加干扰条件需要大量人工检查和改写答案，工作量大。

Method: 设计一个迭代生成框架，利用大语言模型结合多角度和认知层次的提示自动生成和修订数学文字题中的干扰条件，明确指导模型生成不改变原题解的干扰条件，从而无需重新生成答案。

Result: 该方法能高效且高质量地生成带有合理干扰条件的数学文字题，减少了人工工作量，提高了干扰条件的有效性和多样性。

Conclusion: 提出的自动生成干扰条件的迭代框架有效提升了数学文字题数据集的难度和真实性，为更准确评测大语言模型的数学推理能力提供了工具和数据支持。

Abstract: Mathematical reasoning serves as a crucial testbed for evaluating the
intelligence of large language models (LLMs), and math word problems (MWPs)
represent one of the most widely used formats. Most existing MWP datasets
contain only the necessary information, while problems with distracting or
excessive conditions are often overlooked. Prior studies have shown that
popular LLMs experience a dramatic performance drop when such distracting
conditions are introduced. However, available datasets of MWPs with distracting
conditions remain limited, and most exhibit low difficulty and out-of-context
expressions. These shortcomings make the distracting conditions easy to detect
and disregard, thereby reducing the credibility of benchmarking on these
datasets. Moreover, when distracting conditions are added, the reasoning
process and answers may change, requiring intensive manual effort to check and
rewrite solutions.
  To address these issues, we design an iterative framework that leverages LLMs
to generate distracting conditions automatically. We develop a set of prompts
to revise MWPs from multiple perspectives and cognitive levels, encouraging the
creation of meaningful distracting conditions as well as suggestions for
further refinement. A key advantage of our framework is the preservation of
shared solutions between the original and revised problems: the LLMs are
explicitly guided to generate distractions that do not alter the original
solution, thus eliminating the need to produce new answers. This framework is
efficient and easy to deploy, substantially reducing the effort required to
generate MWPs with distracting conditions while maintaining high data quality.

</details>


### [18] [LLMs Show Surface-Form Brittleness Under Paraphrase Stress Tests](https://arxiv.org/abs/2510.08616)
*Juan Miguel Navarro Carranza*

Main category: cs.CL

TL;DR: 本文提出了一种通过对基准测试题进行改写复述，重新评估大语言模型（LLMs）泛化能力的简单方法。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的基准测试得分可能因记忆测试项或近重复内容而被高估，亟需有效方法探测模型的泛化能力。

Method: 使用改写的测试题对Mistral-7B-Instruct和Qwen2.5-7B-Instruct模型在ARC-Easy和ARC-Challenge数据集上重新评估，控制解码过程，强制采用多选格式输出，并通过稳健的改写清理步骤保持语义不变。

Result: 改写的题目导致模型准确率显著下降，表明测试存在记忆和表面模式敏感问题。

Conclusion: 改写测试作为检测模型泛化的有效协议，揭示了大语言模型对训练数据的潜在依赖与脆弱性。

Abstract: Benchmark scores for Large Language Models (LLMs) can be inflated by
memorization of test items or near duplicates. We present a simple, protocol
that probes generalization by re-evaluating models on paraphrased versions of
benchmark questions. Using Mistral-7B-Instruct and Qwen2.5-7B-Instruct, we
measure the accuracy gap between original and paraphrased items on ARC-Easy and
ARC-Challenge. Our pipeline controls decoding, enforces multiple-choice output
format, and includes a robust paraphrase-cleaning step to preserve semantics.
We find that paraphrasing induces a non-trivial accuracy drop (original vs.
paraphrased), consistent with prior concerns about contamination and brittle
surface-form shortcuts.

</details>


### [19] [JAI-1: A Thai-Centric Large Language Model](https://arxiv.org/abs/2510.08620)
*Attapol T. Rutherford,Jullajak Karnjanaekarin,Narongkorn Panitsrisit,Pontakorn Trakuekul,Sumana Sumanakul,Natchanon Pollertlam*

Main category: cs.CL

TL;DR: JAI-1是一个以泰语为核心的75亿参数语言模型，通过扩展英语开源模型的参数空间，有效整合泰语知识，提升了泰语任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有泰语模型多依赖针对泰语做额外训练但不改结构的开源模型，这可能导致模型原有知识丧失，难以兼顾多语言能力。

Method: 采用扩容策略，从表现优秀的小型英语开源模型出发，扩展参数空间并系统整合大量泰语数据进行预训练及多阶段微调。

Result: 模型在多个泰语相关基准测试上表现优于Typhoon2-70B，显示了其架构和知识整合方法的有效性。

Conclusion: 这种扩容结合系统化知识注入的新框架，不仅保存了模型的通用智能，还实现了泰语能力的专门强化，具备良好的扩展潜力。

Abstract: This technical report introduces JAI-1, a Thai-centric language model with
75B parameters. Recent Thai models have primarily relied on existing
open-source models, applying additional training without structural
modifications to specialize in Thai. However, this approach risks eroding
pre-existing knowledge in the model's parameter space during the injection of
Thai-specific information, as optimized parameters for general tasks may
conflict with new linguistic requirements. In contrast, JAI-1 adopts an
upscaling strategy: starting from a smaller, high-performing English
open-source LLM, we expanded its parameter space and utilized the newly
allocated capacity to systematically integrate Thai-language knowledge. This
methodology not only preserves the original model's general intelligence but
also establishes a unique architecture distinct from other open-source models,
enabling scalable future enhancements. During pre-training, JAI-1 was exposed
to 1.5T tokens, including over 300B Thai language tokens. This was followed by
post-training stages -- supervised fine-tuning and alignment tuning -- using
more than 600K instruction-based examples. The final model demonstrated
superior performance compared to Typhoon2-70B on Thai-centric benchmarks
(IFEval-TH, MT-Bench-TH, and JAI-Hall-Bench), validating the efficacy of its
upscaling and knowledge-integration framework.

</details>


### [20] [From Simulation to Strategy: Automating Personalized Interaction Planning for Conversational Agents](https://arxiv.org/abs/2510.08621)
*Wen-Yu Chang,Tzu-Hung Huang,Chih-Ho Chen,Yun-Nung Chen*

Main category: cs.CL

TL;DR: 该论文研究基于用户职业设定的销售型对话模型，通过调整对话策略提升对话效率和成功率。


<details>
  <summary>Details</summary>
Motivation: 针对销售型对话系统，需要模拟具有不同用户特征的用户，以优化对话策略。

Method: 基于用户年龄、性别和职业的用户画像，尤其利用职业信息来调整对话模型的意图优先级，设计轻量级的职业条件化策略。

Result: 职业对用户意图的影响最显著，采用职业条件化策略后，对话更短且成功率更高。

Conclusion: 丰富的用户画像信息尤其是职业信息能显著提升销售对话系统的效果，简单的角色感知策略已能带来可观提升。

Abstract: Amid the rapid rise of agentic dialogue models, realistic user-simulator
studies are essential for tuning effective conversation strategies. This work
investigates a sales-oriented agent that adapts its dialogue based on user
profiles spanning age, gender, and occupation. While age and gender influence
overall performance, occupation produces the most pronounced differences in
conversational intent. Leveraging this insight, we introduce a lightweight,
occupation-conditioned strategy that guides the agent to prioritize intents
aligned with user preferences, resulting in shorter and more successful
dialogues. Our findings highlight the importance of rich simulator profiles and
demonstrate how simple persona-informed strategies can enhance the
effectiveness of sales-oriented dialogue systems.

</details>


### [21] [Text2Stories: Evaluating the Alignment Between Stakeholder Interviews and Generated User Stories](https://arxiv.org/abs/2510.08622)
*Francesco Dente,Fabiano Dalpiaz,Paolo Papotti*

Main category: cs.CL

TL;DR: 本文提出了Text2Stories任务及其评价指标，用于量化软件需求（用户故事）与访谈转录文本之间的匹配程度。


<details>
  <summary>Details</summary>
Motivation: 自动生成的软件需求需要评估其是否真实反映利益相关者的需求，但现有评估方法主要依赖人工操作，效率低下。

Method: 设计Text2Stories任务，通过将访谈文本切分为文本块，将需求用户故事与文本块匹配，量化正确性和完整性指标；采用大型语言模型（LLM）和嵌入模型进行文本与故事的匹配。

Result: 基于LLM的匹配器在四个数据集上获得了0.86的宏F1分数，比单纯的嵌入模型表现优越，同时嵌入模型能有效实现预筛选。

Conclusion: Text2Stories作为一种可扩展的指标体系，可以自动、忠实地评价生成的用户故事质量，成为现有评价标准的有益补充。

Abstract: Large language models (LLMs) can be employed for automating the generation of
software requirements from natural language inputs such as the transcripts of
elicitation interviews. However, evaluating whether those derived requirements
faithfully reflect the stakeholders' needs remains a largely manual task. We
introduce Text2Stories, a task and metrics for text-to-story alignment that
allow quantifying the extent to which requirements (in the form of user
stories) match the actual needs expressed by the elicitation session
participants. Given an interview transcript and a set of user stories, our
metric quantifies (i) correctness: the proportion of stories supported by the
transcript, and (ii) completeness: the proportion of transcript supported by at
least one story. We segment the transcript into text chunks and instantiate the
alignment as a matching problem between chunks and stories. Experiments over
four datasets show that an LLM-based matcher achieves 0.86 macro-F1 on held-out
annotations, while embedding models alone remain behind but enable effective
blocking. Finally, we show how our metrics enable the comparison across sets of
stories (e.g., human vs. generated), positioning Text2Stories as a scalable,
source-faithful complement to existing user-story quality criteria.

</details>


### [22] [PARSE: LLM Driven Schema Optimization for Reliable Entity Extraction](https://arxiv.org/abs/2510.08623)
*Anubhav Shrimal,Aryan Jain,Soumyajit Chowdhury,Promod Yenigalla*

Main category: cs.CL

TL;DR: 本文提出了PARSE系统，通过自动优化JSON模式和结合静态与LLM的保护机制显著提升结构化信息抽取的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有方法将JSON模式视为静态的、为人类设计的合同，导致抽取性能次优、频繁产生幻觉且行为不可靠。

Method: 开发PARSE系统，包括自动优化JSON模式的ARCHITECT组件和基于反思的SCOPE抽取组件，同时通过RELAY保持向后兼容。

Result: 在多个数据集上，PARSE最高提升64.7%的抽取准确率，整体提升10%，并将抽取错误在第一次重试中减少92%。

Conclusion: PARSE系统有效改进了JSON模式的利用方式，提升了大语言模型对结构化数据抽取的性能和稳定性，适用于软件3.0环境。

Abstract: Structured information extraction from unstructured text is critical for
emerging Software 3.0 systems where LLM agents autonomously interact with APIs
and tools. Recent approaches apply large language models directly to extraction
tasks using existing JSON schemas, often with constraint decoding or
reinforcement learning approaches to ensure syntactic validity, but treat JSON
schemas as static contracts designed for human developers, leading to
suboptimal extraction performance, frequent hallucinations, and unreliable
agent behavior when schemas contain ambiguous or incomplete specifications. We
recognize that JSON schemas themselves are a form of natural language
understanding contract that encodes rules, relationships, and expectations
about data structure contracts that LLMs should be able to both interpret and
systematically improve. Consequently, we develop PARSE (Parameter Automated
Refinement and Schema Extraction), a novel system with two synergistic
components: ARCHITECT, which autonomously optimizes JSON schemas for LLM
consumption while maintaining backward compatibility through RELAY (an
integrated code generation system), and SCOPE, which implements
reflection-based extraction with combined static and LLM-based guardrails. We
evaluate PARSE qualitatively and quantitatively on three datasets including
Schema-Guided Dialogue (SGD), Structured Web Data Extraction (SWDE), and
internal retail conversation data, and find that it achieves up to 64.7%
improvement in extraction accuracy on SWDE with combined framework improvements
reaching 10% across models, while reducing extraction errors by 92% within the
first retry and and maintaining practical latency.

</details>


### [23] [Do LLMs Know They Are Being Tested? Evaluation Awareness and Incentive-Sensitive Failures in GPT-OSS-20B](https://arxiv.org/abs/2510.08624)
*Nisar Ahmed,Muhammad Imran Zaman,Gulshan Saleem,Ali Hassan*

Main category: cs.CL

TL;DR: 本文研究了基准测试中评价提示（evaluation scent）对大语言模型性能测量的影响，发现评价提示会增加推理链长度，但仅带来有限或不一致的准确性提升，并可能降低答案简洁性和多语言环境下的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的基准测试多使用包含显式推理和严格格式要求的提示，而实际应用中通常需要简洁且符合合同要求的答案。作者关注这种“评价提示”是否夸大了模型的性能表现但未带来实际能力提升。

Method: 采用单一模型GPT-OSS-20B，设计六组A/B对照实验，保持任务内容和解码方式不变，仅改变提示框架（评价导向 vs. 真实应用）和推理深度，使用确定性验证器测量准确率、答案合规性、推理链长度等指标。

Result: 评价提示显著增加推理链长度（从几百字符到超过1000字符），降低答案仅文本合规性，准确率提升有限或不稳定。评价框架改进了结构化输出中的格式包装，但对核心内容无实质提升。赞扬谨慎能提高准确率并减少错误自信，赞扬能力则输出更简洁但风险更大。多语言（乌尔都语）提示复现相似模式，高推理深度下准确率可能下降。

Conclusion: 为确保大语言模型基准测试的性能提升真正反映可部署能力，需采用中立或双重提示、合同感知评价、风格差异报告及多语言仪表盘等方法，并公开了完整的可复现A/B测试框架。

Abstract: Benchmarks for large language models (LLMs) often rely on rubric-scented
prompts that request visible reasoning and strict formatting, whereas real
deployments demand terse, contract-bound answers. We investigate whether such
"evaluation scent" inflates measured performance without commensurate
capability gains. Using a single open-weights model (GPT-OSS-20B), we run six
paired A/B scenarios that hold task content and decoding fixed while varying
framing (evaluation-oriented vs. real-world) and reasoning depth (Medium/High):
deterministic math, strict code-fix, citation generation, incentive flips
(caution vs. competence), CoT visibility, and multilingual (Urdu) headers.
Deterministic validators compute accuracy, answer-only compliance,
hedging/refusals, chain-of-thought (CoT) length, and schema compliance, with
pre-registered deltas and composite indices. Across scenarios, evaluation
framing reliably inflates CoT (hundreds to >1000 characters) and reduces
answer-only compliance, with limited or inconsistent accuracy gains. In
structured outputs, it improves wrappers (e.g., fenced blocks, enumerated
lists) but not regex-validated substance. Incentive wording reweights error
composition: praising caution modestly improves accuracy at high reasoning and
reduces wrong-but-confident errors, whereas praising competence yields terser
but riskier outputs. Urdu rubric headers reproduce these signatures and can
decrease accuracy at higher reasoning depth, indicating multilingual parity
risks. We provide a reproducible A/B framework (prompt banks, validators,
per-run scores, scripts; versioned DOI) and practical guidance: neutral
phrasing or dual-framing checks, contract-aware grading, style-delta reporting,
confidence governance, and multilingual dashboards to ensure that benchmark
gains reflect deployable capability.

</details>


### [24] [From What to Why: Thought-Space Recommendation with Small Language Models](https://arxiv.org/abs/2510.08626)
*Prosenjit Biswas,Pervez Shaik,Abhinav Thorat,Ravi Kolla,Niranjan Pedanekar*

Main category: cs.CL

TL;DR: 本文提出PULSE框架，利用小型语言模型生成的推理理由作为监督信号，结合用户交互历史，共同建模用户行为及其语义驱动因子，提高推荐系统性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理能力强但推理成本高，小语言模型推理能力不足且未充分利用自然语言推理理由作为学习信号，限制了推荐效果。

Method: 提出PULSE框架，通过小型语言模型生成用户和物品之间跨多域的共同理解空间（Thought Space），将推理理由作为核心监督信号，与交互历史联合建模用户行为和原因。

Result: PULSE在多个公开数据集上优于ID、协同过滤及大语言模型基线的序列推荐方法，并在跨域推荐和推理型问答任务中表现卓越。

Conclusion: 利用小型语言模型生成的推理理由作为主导信号的联合建模方法能有效提升推荐系统的泛化能力和转移能力，为低成本高效推荐提供新思路。

Abstract: Large Language Models (LLMs) have advanced recommendation capabilities
through enhanced reasoning, but pose significant challenges for real-world
deployment due to high inference costs. Conversely, while Small Language Models
(SLMs) offer an efficient alternative, their reasoning capabilities for
recommendation remain underexplored. Existing systems often use natural
language rationales merely as unsupervised descriptive text, failing to harness
their full potential as learning signals. In this work our main idea is to
create a common understanding of user and items across multiple domains called
Thought Space with SLMs instead of using LLMs' distilled knowledge. To that end
we propose PULSE (Preference Understanding by Latent Semantic Embeddings), a
framework that treats SLM-generated rationales as director learning signals,
supervising them with interaction histories to jointly model user actions
(what) and their semantic drivers (why). Existing methods consider only
interactions such as sequences and embeddings, whereas PULSE treats rationales
as first-class signals, this novel design yields embeddings that are more
robust and generalizable. Extensive experiments demonstrate that PULSE
outperforms leading ID, Collaborative Filtering (CF), and LLM-based sequential
recommendation models across multiple benchmark datasets. Furthermore, PULSE
exhibits superior transferability in cross-domain recommendation and
demonstrates strong performance on downstream tasks such as reasoning-oriented
question answering. Our code is available
\href{https://anonymous.4open.science/r/Thinking_PULSE-0FC5/README.md}{here}.

</details>


### [25] [ExPO-HM: Learning to Explain-then-Detect for Hateful Meme Detection](https://arxiv.org/abs/2510.08630)
*Jingbiao Mei,Mingsheng Sun,Jinghong Chen,Pengda Qin,Yuhong Li,Da Chen,Bill Byrne*

Main category: cs.CL

TL;DR: 该论文提出了ExPO-HM方法，通过结合SFT热身、GRPO课程学习和条件决策熵优化仇恨表情包的检测与解释，显著提升了检测准确率和推理质量，实现了比现有方法更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的仇恨表情包检测模型多为简单的二元分类，缺乏对仇恨内容背后目标和攻击类型等关键信息的解释，且奖励信号不足以指导有效推理，难以满足实际内容审核的需求。

Method: 提出ExPO-HM方法，借鉴人工标注者的训练和评估过程，结合SFT预训练、GRPO强化学习和课程学习，并采用条件决策熵作为推理质量的度量和奖励，提升模型对仇恨表情包的细粒度分类和推理能力。

Result: 在三个仇恨表情包基准数据集上，ExPO-HM在二元检测、细粒度分类和推理质量上均取得了最先进的性能，F1分数较GRPO和DPO分别提升了15%和17%。

Conclusion: ExPO-HM将仇恨表情包检测从简单的二元报警转向基于解释的检测，提供了准确、可解释且具操作性的内容审核支持。

Abstract: Hateful memes have emerged as a particularly challenging form of online
abuse, motivating the development of automated detection systems. Most prior
approaches rely on direct detection, producing only binary predictions. Such
models fail to provide the context and explanations that real-world moderation
requires. Recent Explain-then-Detect approaches, using Chain-of-Thought
prompting or LMM agents, perform worse than simple SFT baselines, and even
advanced post-training methods such as GRPO fail to close the gap. Our analysis
identifies two key issues of such systems: important policy-relevant cues such
as targets and attack types are not hypothesized by the model as a likely
explanation; and the binary reward signal is insufficient to guide reasoning.
To address these challenges, we propose ExPO-HM (Explain-then-Detect Policy
Optimization for Hateful Memes), inspired by the training and evaluation
process of human annotators. ExPO-HM combines SFT warmup, GRPO with curriculum
learning, and Conditional Decision Entropy (CDE) as both metric and reward for
reasoning quality. Across three hateful meme benchmarks, ExPO-HM achieves
state-of-the-art performance on binary detection, fine-grained classification,
and reasoning quality, with up to 15\% and 17\% F1 improvement over the GRPO
and DPO baselines, respectively. By moving hateful meme detection from simple
binary alarms to explanation-driven detection, ExPO-HM provides accurate,
interpretable, and actionable moderation support.

</details>


### [26] [Next Semantic Scale Prediction via Hierarchical Diffusion Language Models](https://arxiv.org/abs/2510.08632)
*Cai Zhou,Chenyu Wang,Dinghuai Zhang,Shangyuan Tong,Yifei Wang,Stephen Bates,Tommi Jaakkola*

Main category: cs.CL

TL;DR: HDLM提出了一种基于层级词汇表的离散扩散语言模型，通过将细粒度词汇逐步映射到粗粒度词汇，实现语义层级预测，显著提升文本生成效果。


<details>
  <summary>Details</summary>
Motivation: 解决传统语言模型难以捕捉语义层级细节的问题，通过层级扩散方法提升语言建模质量。

Method: 构建层级词汇表，前向过程将词汇扰动到上层语义，反向过程逐步预测更细语义，推导扩散ELBO，灵活实现模型并提出训练技巧。

Result: 实验表明HDLM在验证困惑度和生成困惑度上均优于现有基线模型。

Conclusion: HDLM提供了一种有效的时间变化语义尺度预测框架，提升了语言模型的性能和生成质量。

Abstract: In this paper we introduce Hierarchical Diffusion Language Models (HDLM) -- a
novel family of discrete diffusion models for language modeling. HDLM builds on
a hierarchical vocabulary where low-level tokens with detailed semantics are
surjectively mapped to high-level tokens with coarse-grained meanings. In the
forward process, each token is independently perturbed to its higher-level
ancestor with more abstract semantics according to the scheduler, while in the
reverse process the model progressively predicts the next, more detailed
semantics. Taken together, HDLM provides a general time-varying next semantic
scale prediction process for language modeling. We derive closed-form
expressions for the diffusion Evidence Lower Bound (ELBO), and show that HDLM
can be implemented in a flexible manner while including the existing MDLM as a
special case. We also propose practical training techniques based on the
insights. Extensive text generation experiments validate the effectiveness of
HDLM, which demonstrates consistently lower validation and generative
perplexity than baselines.

</details>


### [27] [Upfront Chain-of-Thought: A Cooperative Framework for Chain-of-Thought Compression](https://arxiv.org/abs/2510.08647)
*Chengzhengxu Li,Xiaoming Liu,Zhaohan Zhang,Shaochu Zhang,Shengchao Liu,Guoxin Ma,Yu Lan,Chao Shen*

Main category: cs.CL

TL;DR: 本文提出了UCoT，一种通过前置思维嵌入实现链式思考压缩的高效推理框架，大幅缩短推理输出长度，提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 长链式思考在大语言模型中推理能力强，但计算成本高且延迟大，现有方法牺牲细节或设计复杂，需改进推理效率。

Method: 设计了UCoT框架，包含压缩模型生成丰富推理信息的前置思维嵌入，执行模型基于此短推理输出答案，并用奖励机制优化。

Result: 实验显示UCoT在保持强推理能力同时显著减少链式思考长度，如在Qwen2.5-7B-Instruct模型上，GSM8K测试集的token使用减少50%，性能提升3.08%。

Conclusion: UCoT有效提升了大语言模型的推理效率与性能，避免了人工设计提示和外部数据集构建的不足，具有广泛应用前景。

Abstract: Recent developments have enabled advanced reasoning in Large Language Models
(LLMs) via long Chain-of-Thought (CoT), while long CoT suffers from high
computational costs and significant latency losses owing to the autoregressive
nature of generative LLMs. CoT compression aims to improve efficiency in the
reasoning process by reducing output length. Previous works trade reasoning
efficiency by either laborious discrete prompt designing or the construction of
external compressed CoT datasets that sacrifice key reasoning details. In this
work, we propose Upfront CoT (UCoT): an efficient reasoning framework with
upfront thought embedding to automate CoT compression. UCoT is a cooperative
workflow involving a small model (compressor) and a large model (executor). The
first stage of UCoT trains compressor to generate upfront thought embeddings
rich in reasoning information for the executor, avoiding the drawbacks of
manually designed prompts. The second stage optimizes executor to utilize
upfront thought embeddings to derive the correct answer with short reasoning,
using a reward mechanism. Extensive experiments show that UCoT maintains the
powerful reasoning ability of executor while significantly reducing the length
of CoT. It is worth mentioning that when applying UCoT to the
Qwen2.5-7B-Instruct model, the usage of tokens on GSM8K dataset is reduced by
50\%, while the performance is 3.08\% higher than that of the state-of-the-art
(SOTA) method. The code and dataset are in supplementary material.

</details>


### [28] [Formalizing Style in Personal Narratives](https://arxiv.org/abs/2510.08649)
*Gustave Cortal,Alain Finkel*

Main category: cs.CL

TL;DR: 本文提出了一种整合功能语言学、计算机科学与心理学的方法，系统分析个人叙事中的语言风格，通过自动提取梦境叙事中的语言特征，揭示语言选择与心理状态的关联。


<details>
  <summary>Details</summary>
Motivation: 缺乏系统框架分析个人叙事中表达主观体验的语言风格。

Method: 结合功能语言学理论和计算机自动抽取语言序列模式，分析上百篇梦境叙事，包括创伤后应激障碍患者案例。

Result: 发现独特语言模式，诸如战争退伍军人叙事中动词过程偏重于动作而非思想过程。

Conclusion: 语言风格的系统分析有助于揭示语言选择与心理状态之间的关系，对理解个人叙事有重要意义。

Abstract: Personal narratives are stories authors construct to make meaning of their
experiences. Style, the distinctive way authors use language to express
themselves, is fundamental to how these narratives convey subjective
experiences. Yet there is a lack of a formal framework for systematically
analyzing these stylistic choices. We present a novel approach that formalizes
style in personal narratives as patterns in the linguistic choices authors make
when communicating subjective experiences. Our framework integrates three
domains: functional linguistics establishes language as a system of meaningful
choices, computer science provides methods for automatically extracting and
analyzing sequential patterns, and these patterns are linked to psychological
observations. Using language models, we automatically extract linguistic
features such as processes, participants, and circumstances. We apply our
framework to hundreds of dream narratives, including a case study on a war
veteran with post-traumatic stress disorder. Analysis of his narratives
uncovers distinctive patterns, particularly how verbal processes dominate over
mental ones, illustrating the relationship between linguistic choices and
psychological states.

</details>


### [29] [A Novel Framework for Augmenting Rating Scale Tests with LLM-Scored Text Data](https://arxiv.org/abs/2510.08663)
*Joe Watson,Ivan O'Conner,Chia-Wen Chen,Luning Sun,Fang Luo,David Stillwell*

Main category: cs.CL

TL;DR: 该论文提出一种结合大型语言模型（LLM）评分文本与传统量表的新型心理测评框架，以抑郁症为例，在真实学生样本和合成数据上验证，显著提升测量精度和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统心理测评依赖结构化评分量表，难以捕捉被试自然语言中的丰富细节，限制了测评的有效性和精确度。

Method: 利用最新大型语言模型自动评分文本数据，通过计算条目信息量选择最具信息性的评分指令，结合传统量表打造增强型测试工具。

Result: 增强测试在独立测试集上表现出显著的测量精度和准确性提升，信息增益相当于增加了6.3至16个传统测试题目。

Conclusion: 该方法突破了传统自动评分对预标注数据和复杂专家评分规则的依赖，提供了一种可扩展的方式用日益丰富的文本数据提升心理测评工具，具有临床与其他领域应用潜力。

Abstract: Psychological assessments typically rely on structured rating scales, which
cannot incorporate the rich nuance of a respondent's natural language. This
study leverages recent LLM advances to harness qualitative data within a novel
conceptual framework, combining LLM-scored text and traditional rating-scale
items to create an augmented test. We demonstrate this approach using
depression as a case study, developing and assessing the framework on a
real-world sample of upper secondary students (n=693) and corresponding
synthetic dataset (n=3,000). On held-out test sets, augmented tests achieved
statistically significant improvements in measurement precision and accuracy.
The information gain from the LLM items was equivalent to adding between 6.3
(real data) and 16.0 (synthetic data) items to the original 19-item test. Our
approach marks a conceptual shift in automated scoring that bypasses its
typical bottlenecks: instead of relying on pre-labelled data or complex
expert-created rubrics, we empirically select the most informative LLM scoring
instructions based on calculations of item information. This framework provides
a scalable approach for leveraging the growing stream of transcribed text to
enhance traditional psychometric measures, and we discuss its potential utility
in clinical health and beyond.

</details>


### [30] [dInfer: An Efficient Inference Framework for Diffusion Language Models](https://arxiv.org/abs/2510.08666)
*Yuxin Ma,Lun Du,Lanning Wei,Kun Chen,Qian Xu,Kangyu Wang,Guofeng Feng,Guoshan Lu,Lin Liu,Xiaojing Qi,Xinyuan Zhang,Zhen Tao,Haibo Feng,Ziyun Jiang,Ying Xu,Zenan Huang,Yihong Zhuang,Haokai Xu,Jiaqi Hu,Zhenzhong Lan,Junbo Zhao,Jianguo Li,Da Zheng*

Main category: cs.CL

TL;DR: 本文提出了dInfer，一个用于基于扩散的大型语言模型（dLLM）的高效推理框架，显著提升了推理速度同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前扩散式大语言模型尽管具有固有的并行优势，但缺乏标准化和高效的推理框架，限制了其广泛应用。

Method: dInfer将推理流程拆分为模型、扩散迭代管理器、解码策略和KV缓存管理器四个模块，并为每个模块集成了创新算法和系统级优化。

Result: dInfer在LLaDA-MoE模型上实现了1,100多tokens/s的推理速度，在多项基准测试上比Fast-dLLM快10倍，且较优化的自回归模型QWen2.5-3B快2-3倍。

Conclusion: 通过模块化设计和多种优化，dInfer显著提升了dLLM推理效率，推动了扩散式语言模型的实用化。代码已开源。

Abstract: Diffusion-based large language models (dLLMs) have emerged as a promising
alternative to autoregressive (AR) LLMs, leveraging denoising-based generation
to enable inherent parallelism. Even more and more open-sourced dLLM models
emerge, yet their widespread adoption remains constrained by the lack of a
standardized and efficient inference framework. We present dInfer, an efficient
and extensible framework for dLLM inference. dInfer decomposes the inference
pipeline into four modular components-model, diffusion iteration manager,
decoding strategy, and KV-cache manager-and integrates novel algorithms for
each component alongside system-level optimizations. Through this combination
of algorithmic innovations and system enhancements, dInfer achieves substantial
efficiency gains without compromising output quality on LLaDA-MoE. At batch
size 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800
tokens per second across six benchmarks on $8\times$ H800 GPUs. Compared to
prior systems, dInfer delivers $10\times$ speedup over Fast-dLLM while
maintaining similar model performance. Even compared with AR models (with a
comparable number of activation parameters and performance) QWen2.5-3B, which
is highly optimized with latest vLLM inference engine, dInfer still deliverers
$2$-$3\times$ speedup. The implementation of dInfer is open-sourced at
https://github.com/inclusionAI/dInfer.

</details>


### [31] [Scaling Laws for Code: A More Data-Hungry Regime](https://arxiv.org/abs/2510.08702)
*Xianzhen Luo,Wenzhen Zheng,Qingfu Zhu,Rongyi Zhang,Houyi Li,Siming Huang,YuanTao Fan,Wanxiang Che*

Main category: cs.CL

TL;DR: 本文首次对代码大语言模型（Code LLMs）的规模律进行了大规模实证研究，发现代码对数据量需求更大，不同于自然语言，且模型规模增长显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有规模律主要基于自然语言，代码与自然语言在语法等方面存在根本差异，是否适用自然语言的规模律尚不明确。

Method: 进行117次实验，模型规模从0.2B到3.8B，训练数据从2B到128B，拟合Chinchilla定律和Farseer定律，并对混合代码与自然语言的数据进行了额外实验。

Result: Farseer定律拟合效果更好，代码模型随模型大小有效扩展，代码需求更高的数据-参数比例，自然语言在资源受限时有利，但在高算力下反而不利。

Conclusion: 代码模型的规模律与自然语言显著不同，训练代码模型需更多数据，混合训练时自然语言的作用依赖计算资源规模。

Abstract: Code Large Language Models (LLMs) are revolutionizing software engineering.
However, scaling laws that guide the efficient training are predominantly
analyzed on Natural Language (NL). Given the fundamental differences like
strict syntax between code and NL, it is unclear whether these laws are
directly applicable to code. To address this gap, we conduct the first
large-scale empirical study of scaling laws for code, comprising 117
experimental runs with model sizes from 0.2B to 3.8B and training tokens from
2B to 128B. We fit the Chinchilla law and the Farsser law. First, the results
show that the more expressive Farseer law offers greater accuracy. Second, the
analysis reveals that Code LLMs scale effectively with model size. Crucially,
code represents a more data-hungry regime, requiring a substantially higher
data-to-parameter ratio than NL. Finally, two additional sets of experiments on
code-NL mixtures show that NL benefits resource-constrained scenarios, but
becomes a detriment at higher compute budgets.

</details>


### [32] [Thinking Longer, Not Always Smarter: Evaluating LLM Capabilities in Hierarchical Legal Reasoning](https://arxiv.org/abs/2510.08710)
*Li Zhang,Matthias Grabmair,Morgan Gray,Kevin Ashley*

Main category: cs.CL

TL;DR: 本文提出了一个分阶段推理框架，以评估大语言模型在法律案例推理中的能力，发现模型在复杂推理任务中表现显著下降。


<details>
  <summary>Details</summary>
Motivation: 当前法律实践中，案例推理依赖于对过去判例的类比和区分，但大语言模型在这一复杂推理上的能力尚未充分验证。

Method: 设计了一个三阶段框架，使用事实谓词（因素）构建法律知识层次结构，定义了可验证的规则来识别案例间的显著区别并分析其论证支持。通过系统评估现代大语言模型的表现，揭示其在不同层次推理任务中的表现差异。

Result: 模型在表面推理任务上准确率较高，但在分层推理和综合分析任务上表现明显下降，且模型在错误回答上投入更多计算资源。

Conclusion: 当前大语言模型在法律复杂推理任务中存在根本性局限，需要针对其推理能力进行细粒度分析和改进，以实现稳健和可信的法律人工智能。

Abstract: Case-based reasoning is a cornerstone of U.S. legal practice, requiring
professionals to argue about a current case by drawing analogies to and
distinguishing from past precedents. While Large Language Models (LLMs) have
shown remarkable capabilities, their proficiency in this complex, nuanced form
of reasoning needs further investigation. We propose a formal framework that
decomposes the process of identifying significant distinctions between cases
into three-stage reasoning tasks. Our framework models cases using factual
predicates called factors, organizes them into a legal knowledge hierarchy, and
defines verifiable rules for identifying distinctions, analyzing their
argumentative support, and evaluating their significance. Through comprehensive
evaluation of modern reasoning LLMs, we reveal a paradox: while models achieve
high accuracy on surface-level reasoning (Task 1), performance degrades on
hierarchical reasoning (Task 2: 64.82%-92.09%) and collapses on integrated
analysis (Task 3: 11.46%-33.99%). Most strikingly, we find that models
consistently expend more computational resources on incorrect responses than
correct ones, suggesting that "thinking longer" does not always mean "thinking
smarter." Our work provides a methodology for fine-grained analysis of LLM
reasoning capabilities in complex domains and reveals fundamental limitations
that must be addressed for robust and trustworthy legal AI.

</details>


### [33] [How Many Code and Test Cases Are Enough? Evaluating Test Cases Generation from a Binary-Matrix Perspective](https://arxiv.org/abs/2510.08720)
*Xianzhen Luo,Jinyang Huang,Wenzhen Zheng,Qingfu Zhu,Mingzheng Xu,Yiheng Xu,Yuantao Fan,Libo Qin,Wanxiang Che*

Main category: cs.CL

TL;DR: 本文提出了一种新的评测框架，用于构建包含最小独立错误模式和测试用例集合的测试基准。


<details>
  <summary>Details</summary>
Motivation: 现有自动生成测试用例的评测基准存在计算成本高、分数膨胀以及对简单错误的偏向，难以有效评估测试用例的诊断能力。

Method: 提出基于二进制代码-测试矩阵的最优诊断基构建框架，通过矩阵秩确定最小错误模式和测试用例数量，设计了高效的WrongSelect算法选择多样化错误代码。

Result: 基于百万级编程提交构建了紧凑、多样且抗膨胀的TC-Bench基准，先进测试生成方法在该基准下仅约60%排除率，展示了测试用例诊断能力的不足。

Conclusion: TC-Bench有效揭示了现有测试生成方法的缺陷，框架为测试基准构建提供了理论支持和实用工具。

Abstract: Evaluating test cases automatically generated by Large Language Models (LLMs)
is a critical yet challenging task. Existing benchmarks suffer from high
computational costs, score inflation, and a bias towards trivial bugs over
rare, critical faults. In this work, we ask two fundamental questions: (1) What
is the minimal set of wrong codes sufficient to represent the entire error
space? and (2) What is the minimal set of test cases needed to distinguish
them? We introduce a framework that formalizes benchmark construction as
finding an optimal diagnostic basis in a binary code-test matrix. The rank of
this matrix specifies the minimal number of independent error patterns (wrong
codes) and provides a tight upper bound on the number of test cases required
for complete fault coverage. Our objective is to identify a basis of size equal
to the matrix rank that maximizes internal diversity. To tackle this NP-hard
problem, we propose WrongSelect, an efficient approximation algorithm to select
maximally diverse wrong codes. Applying this framework to millions of
competitive programming submissions, we construct TC-Bench, a compact, diverse,
and inflation-resistant benchmark. Extensive experiments show that even the
most advanced test case generation methods achieve only ~60% exclusion rates on
TC-Bench, exposing a significant gap in their diagnostic power. Our dataset is
available at: https://huggingface.co/datasets/Luoberta/TC-Bench and our code is
at: https://github.com/Luowaterbi/TC-Bench.

</details>


### [34] [How Reliable is Language Model Micro-Benchmarking?](https://arxiv.org/abs/2510.08730)
*Gregory Yauney,Shahzaib Saqib Warraich,Swabha Swayamdipta*

Main category: cs.CL

TL;DR: 微基准测试通过对现有基准的极小子集进行评估，旨在降低语言模型开发的时间和成本，但其能否如完整基准一样稳定排序模型，且优于随机抽样尚存疑问。


<details>
  <summary>Details</summary>
Motivation: 在有限资源下评估语言模型时，微基准测试能否保持排序的准确性和稳定性，需要更细致的评估和验证。

Method: 引入了一种元评估指标，分析微基准测试在不同模型性能差异下的排序能力，比较不同微基准大小及其与随机抽样的效果。

Result: 发现微基准测试难以稳定区分性能相近的模型，且要稳定排序需要选取较多样本（约250个），此时随机抽样与已有方法表现相当。

Conclusion: 微基准测试在模型评估中存在准确性和样本大小之间的权衡，指导用户和开发者合理选择微基准规模以平衡效率与可靠性。

Abstract: Micro-benchmarking offers a solution to the often prohibitive time and cost
of language model development: evaluate on a very small subset of existing
benchmarks. Can these micro-benchmarks, however, rank models as consistently as
the full benchmarks they replace? And can they rank models more consistently
than selecting a random subset of data points? In many scenarios, we find that
the answer is no. We introduce a meta-evaluation measure for micro-benchmarking
which investigates how well a micro-benchmark can rank two models as a function
of their performance difference on the full benchmark. This approach can
determine which model pairs can be ranked correctly by a micro-benchmark,
allowing for a finer-grained analysis of the trade-off between micro-benchmark
size and reliability. Prior work has suggested selecting as few as 10 examples;
we find that no micro-benchmarking method can consistently rank model pairs 3.5
points of accuracy apart on MMLU-Pro or 4 points apart on BIG-bench Hard. In
order to consistently rank model pairs with relatively similar performances, we
show that often as many as 250 examples must be selected, at which point random
sampling is competitive with existing micro-benchmarking methods. When
comparing only 8B instruction-tuned models on MMLU-Pro micro-benchmarks with 25
examples, we find that more than half of pairwise comparisons are not likely to
be preserved. Our work provides actionable guidance for both micro-benchmark
users and developers in navigating the trade-off between evaluation efficiency
and reliability.

</details>


### [35] [Coordinates from Context: Using LLMs to Ground Complex Location References](https://arxiv.org/abs/2510.08741)
*Tessa Masis,Brendan O'Connor*

Main category: cs.CL

TL;DR: 本文研究了利用大语言模型（LLM）进行组合地理位置参考的地理编码任务，提出了一种基于LLM的策略，显著提升了任务性能。


<details>
  <summary>Details</summary>
Motivation: 地理编码对于分析非结构化文本中的位置引用至关重要，而组合地理位置的解析更具挑战性。当前研究探索LLM在地理空间知识和推理能力上的表现，以提升地理编码准确率。

Method: 评估了LLM在地理空间知识和相关推理技能方面的能力，基于此提出了一个利用LLM进行组合地理位置引用地理编码的方法，并通过微调较小的LLM模型实现性能优化。

Result: 所提方法显著提升了组合地理位置引用的地理编码性能，同时经过微调的小规模LLM模型展现出与更大型预训练模型相当的表现。

Conclusion: 利用LLM的地理空间推理能力进行地理编码是有效的，且通过微调较小模型可获得高性能，降低了计算资源需求。

Abstract: Geocoding is the task of linking a location reference to an actual geographic
location and is essential for many downstream analyses of unstructured text. In
this paper, we explore the challenging setting of geocoding compositional
location references. Building on recent work demonstrating LLMs' abilities to
reason over geospatial data, we evaluate LLMs' geospatial knowledge versus
reasoning skills relevant to our task. Based on these insights, we propose an
LLM-based strategy for geocoding compositional location references. We show
that our approach improves performance for the task and that a relatively small
fine-tuned LLM can achieve comparable performance with much larger
off-the-shelf models.

</details>


### [36] [Measuring Moral LLM Responses in Multilingual Capacities](https://arxiv.org/abs/2510.08776)
*Kimaya Basu,Savi Kolari,Allison Yu*

Main category: cs.CL

TL;DR: 本研究评估了多个大型语言模型（LLM）在多语言环境下的表现，发现GPT-5在准确性和一致性方面表现最佳，尤其在用户同意与自主权及防止伤害与安全类别中表现突出。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在全球广泛使用，理解和规范其多语言响应变得必要，尤其是不同语言环境下的表现差异。

Method: 通过一个五点评分标准和评判LLM，评估了领先开源模型在五个维度和多资源语言环境下的回答准确性和一致性。

Result: GPT-5平均表现最好，在“同意与自主权”和“防止伤害与安全”等关键类别中得分最高，而其他模型表现波动较大。

Conclusion: 研究强调需加强对语言变化对LLM响应影响的测试，并在相关安全和伦理类别中持续改进模型表现。

Abstract: With LLM usage becoming widespread across countries, languages, and humanity
more broadly, the need to understand and guardrail their multilingual responses
increases. Large-scale datasets for testing and benchmarking have been created
to evaluate and facilitate LLM responses across multiple dimensions. In this
study, we evaluate the responses of frontier and leading open-source models in
five dimensions across low and high-resource languages to measure LLM accuracy
and consistency across multilingual contexts. We evaluate the responses using a
five-point grading rubric and a judge LLM. Our study shows that GPT-5 performed
the best on average in each category, while other models displayed more
inconsistency across language and category. Most notably, in the Consent &
Autonomy and Harm Prevention & Safety categories, GPT scored the highest with
averages of 3.56 and 4.73, while Gemini 2.5 Pro scored the lowest with averages
of 1.39 and 1.98, respectively. These findings emphasize the need for further
testing on how linguistic shifts impact LLM responses across various categories
and improvement in these areas.

</details>


### [37] [Learning What to Remember: Adaptive Probabilistic Memory Retention for Memory-Efficient Language Models](https://arxiv.org/abs/2510.08798)
*S M Rafiuddin,Muntaha Nujat Khan*

Main category: cs.CL

TL;DR: 本论文提出了Adaptive Retention，一种在全局预算内通过概率性选择保留重要token的机制，有效降低Transformer在长序列中的计算和内存开销。


<details>
  <summary>Details</summary>
Motivation: Transformer注意力机制的计算复杂度与序列长度的平方成正比，限制了长上下文处理能力。需要一种高效的方式在保证性能的同时减少资源消耗。

Method: 设计了一种基于Bernoulli门控的自适应保留机制，结合Hard-Concrete变分松弛进行可微训练，并在推理时采用简单的top-M选择规则，用于逐层决定保留哪些token。

Result: 在分类、抽取式问答和长文档摘要任务中，仅保留30-50%的token即可保持95%以上性能，同时内存峰值降低35-45%，吞吐率提升近1.8倍。

Conclusion: 提出的方法与模型结构无关，无需修改基础注意力模块和任务头，即可实现长期上下文的高效处理，兼顾性能与资源消耗。

Abstract: Transformer attention scales quadratically with sequence length O(n^2),
limiting long-context use. We propose Adaptive Retention, a probabilistic,
layer-wise token selection mechanism that learns which representations to keep
under a strict global budget M. Retention is modeled with Bernoulli gates
trained via a Hard-Concrete/variational relaxation and enforced with a simple
top-M rule at inference, making the method differentiable and drop-in for
standard encoders. Across classification, extractive QA, and long-document
summarization, keeping only 30-50% of tokens preserves >= 95% of full-model
performance while cutting peak memory by ~35-45% and improving throughput by up
to ~1.8x. This architecture-agnostic approach delivers practical long-context
efficiency without modifying base attention or task heads.

</details>


### [38] [Benchmarking Chinese Commonsense Reasoning with a Multi-hop Reasoning Perspective](https://arxiv.org/abs/2510.08800)
*Wangjie You,Xusheng Wang,Xing Wang,Wenxiang Jiao,Chao Feng,Juntao Li,Min Zhang*

Main category: cs.CL

TL;DR: 本文提出了Chinese Commonsense Multi-hop Reasoning (CCMOR)基准，用于评估大语言模型在中文常识和多步推理的能力，发现现有模型在长尾知识和知识密集推理上仍存在不足，检索增强生成方法显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在中文语境下的综合评估不足，尤其是结合中文特定事实知识与多步逻辑推理的能力缺乏系统测试。

Method: 构建领域均衡的种子数据集，利用大语言模型生成基于事实链的多跳推理问题，结合人工专家验证保证数据质量。

Result: 评测显示现有顶尖大语言模型在处理长尾知识和知识密集推理方面存在持续的局限性，检索增强生成方法有效弥补了这些不足。

Conclusion: CCMOR作为中文多步常识推理的评测基准，揭示了当前大语言模型的不足，并展示了引入检索机制提升推理能力的潜力。

Abstract: While Large Language Models (LLMs) have demonstrated advanced reasoning
capabilities, their comprehensive evaluation in general Chinese-language
contexts remains understudied. To bridge this gap, we propose Chinese
Commonsense Multi-hop Reasoning (CCMOR), a novel benchmark designed to evaluate
LLMs' ability to integrate Chinese-specific factual knowledge with multi-step
logical reasoning. Specifically, we first construct a domain-balanced seed set
from existing QA datasets, then develop an LLM-powered pipeline to generate
multi-hop questions anchored on factual unit chains. To ensure the quality of
resulting dataset, we implement a human-in-the-loop verification system, where
domain experts systematically validate and refine the generated questions.
Using CCMOR, we evaluate state-of-the-art LLMs, demonstrating persistent
limitations in LLMs' ability to process long-tail knowledge and execute
knowledge-intensive reasoning. Notably, retrieval-augmented generation
substantially mitigates these knowledge gaps, yielding significant performance
gains.

</details>


### [39] [MOSAIC: Multi-agent Orchestration for Task-Intelligent Scientific Coding](https://arxiv.org/abs/2510.08804)
*Siddeshwar Raghavan,Tanwi Mallick*

Main category: cs.CL

TL;DR: MOSAIC是一个多智能体大语言模型框架，用于解决复杂的科学编码任务，提升了准确率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 科学编码任务复杂，需要严谨算法、深厚领域知识和多阶段问题解决，普通编码方法难以满足需求。

Method: 设计无需训练的多智能体系统，在学生-教师范式下进行自我反思、推理、编码和调试，结合合并上下文窗口(CCW)减少错误。

Result: 在科学编码基准测试中，MOSAIC表现出更高的准确性、鲁棒性和可解释性，优于现有方法。

Conclusion: MOSAIC框架有效提升了科学代码生成的性能，适合多阶段复杂科学问题的求解。

Abstract: We present MOSAIC, a multi-agent Large Language Model (LLM) framework for
solving challenging scientific coding tasks. Unlike general-purpose coding,
scientific workflows require algorithms that are rigorous, interconnected with
deep domain knowledge, and incorporate domain-specific reasoning, as well as
algorithm iteration without requiring I/O test cases. Many scientific problems
also require a sequence of subproblems to be solved, leading to the final
desired result. MOSAIC is designed as a training-free framework with specially
designed agents to self-reflect, create the rationale, code, and debug within a
student-teacher paradigm to address the challenges of scientific code
generation. This design facilitates stepwise problem decomposition, targeted
error correction, and, when combined with our Consolidated Context Window
(CCW), mitigates LLM hallucinations when solving complex scientific tasks
involving chained subproblems. We evaluate MOSAIC on scientific coding
benchmarks and demonstrate that our specialized agentic framework outperforms
existing approaches in terms of accuracy, robustness, and interpretability.

</details>


### [40] [The Model's Language Matters: A Comparative Privacy Analysis of LLMs](https://arxiv.org/abs/2510.08813)
*Abhishek K. Mishra,Antoine Boutet,Lucas Magnana*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在处理多语言医疗数据时的隐私泄露问题，发现不同语言结构对隐私风险有显著影响。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型多用于多语言环境，且涉及敏感数据，但多数学术工作主要评估英文，忽视了语言结构对隐私泄露的影响。

Method: 本文选取英语、西班牙语、法语和意大利语医疗语料，量化六个语言指标，采用三种攻击方式（提取、反事实记忆和成员推断）评估隐私泄露。

Result: 结果显示隐私风险随语言冗余度和分词粒度变化，意大利语泄露最严重，英语成员推断能力最强，法语和西班牙语因形态复杂度较高更具抗泄露性。

Conclusion: 语言结构显著影响隐私泄露风险，针对不同语言应设计语言感知的隐私保护机制。

Abstract: Large Language Models (LLMs) are increasingly deployed across multilingual
applications that handle sensitive data, yet their scale and linguistic
variability introduce major privacy risks. Mostly evaluated for English, this
paper investigates how language structure affects privacy leakage in LLMs
trained on English, Spanish, French, and Italian medical corpora. We quantify
six linguistic indicators and evaluate three attack vectors: extraction,
counterfactual memorization, and membership inference. Results show that
privacy vulnerability scales with linguistic redundancy and tokenization
granularity: Italian exhibits the strongest leakage, while English shows higher
membership separability. In contrast, French and Spanish display greater
resilience due to higher morphological complexity. Overall, our findings
provide the first quantitative evidence that language matters in privacy
leakage, underscoring the need for language-aware privacy-preserving mechanisms
in LLM deployments.

</details>


### [41] [Search-on-Graph: Iterative Informed Navigation for Large Language Model Reasoning on Knowledge Graphs](https://arxiv.org/abs/2510.08825)
*Jia Ao Sun,Hao Yu,Fabrizio Gotti,Fengran Mo,Yihong Wu,Yuchen Hui,Jian-Yun Nie*

Main category: cs.CL

TL;DR: 本文提出了Search-on-Graph (SoG)框架，利用大语言模型迭代地在知识图谱中导航，有效解决了复杂查询难题，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理多跳知识密集型问题时存在知识滞后和错误生成的缺陷，现有知识图谱问答方法难以平衡查询完整性和噪声干扰。

Method: 提出基于单一搜索函数的SoG框架，通过"观察-再导航"原则，迭代分析当前实体的实际关系，决定下一步跳转，支持多种知识图谱结构并使用自适应过滤处理高连接度节点。

Result: SoG在Freebase和Wikidata六个知识图谱问答基准测试中均取得无微调的最先进结果，在Wikidata上提升16%，在Freebase上也有持续提升。

Conclusion: SoG框架通过灵活的图导航策略显著提升了大语言模型在知识图谱问答任务中的表现，具有广泛适应性和优秀性能。

Abstract: Large language models (LLMs) have demonstrated impressive reasoning abilities
yet remain unreliable on knowledge-intensive, multi-hop questions -- they miss
long-tail facts, hallucinate when uncertain, and their internal knowledge lags
behind real-world change. Knowledge graphs (KGs) offer a structured source of
relational evidence, but existing KGQA methods face fundamental trade-offs:
compiling complete SPARQL queries without knowing available relations proves
brittle, retrieving large subgraphs introduces noise, and complex agent
frameworks with parallel exploration exponentially expand search spaces. To
address these limitations, we propose Search-on-Graph (SoG), a simple yet
effective framework that enables LLMs to perform iterative informed graph
navigation using a single, carefully designed \textsc{Search} function. Rather
than pre-planning paths or retrieving large subgraphs, SoG follows an
``observe-then-navigate'' principle: at each step, the LLM examines actual
available relations from the current entity before deciding on the next hop.
This approach further adapts seamlessly to different KG schemas and handles
high-degree nodes through adaptive filtering. Across six KGQA benchmarks
spanning Freebase and Wikidata, SoG achieves state-of-the-art performance
without fine-tuning. We demonstrate particularly strong gains on Wikidata
benchmarks (+16\% improvement over previous best methods) alongside consistent
improvements on Freebase benchmarks.

</details>


### [42] [Pattern Enhanced Multi-Turn Jailbreaking: Exploiting Structural Vulnerabilities in Large Language Models](https://arxiv.org/abs/2510.08859)
*Ragib Amin Nihal,Rui Wen,Kazuhiro Nakadai,Jun Sakuma*

Main category: cs.CL

TL;DR: 提出了PE-CoA框架，通过五种对话模式构建多轮绕过安全限制的攻击，揭示大型语言模型在不同伤害类别和对话模式下的漏洞，展示模型脆弱性的多样性和防御训练的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有多轮绕过攻击方法缺乏系统性，无法充分揭示模型脆弱点；模型在不同伤害类别和对话模式下的漏洞关系未被深入理解。

Method: 提出Pattern Enhanced Chain of Attack (PE-CoA)框架，基于五种对话模式，通过自然对话构建有效的多轮绕过攻击；在12个大型语言模型和10个伤害类别上进行评估。

Result: PE-CoA实现了最先进的攻击效果，揭露了模型对不同对话模式的特定脆弱点，发现模型对某些模式的鲁棒性不具备泛化能力，且同一模型家族存在相似失败模式。

Conclusion: 当前的安全训练存在显著局限，需开发模式感知的防御策略以提升大型语言模型的安全性。

Abstract: Large language models (LLMs) remain vulnerable to multi-turn jailbreaking
attacks that exploit conversational context to bypass safety constraints
gradually. These attacks target different harm categories (like malware
generation, harassment, or fraud) through distinct conversational approaches
(educational discussions, personal experiences, hypothetical scenarios).
Existing multi-turn jailbreaking methods often rely on heuristic or ad hoc
exploration strategies, providing limited insight into underlying model
weaknesses. The relationship between conversation patterns and model
vulnerabilities across harm categories remains poorly understood. We propose
Pattern Enhanced Chain of Attack (PE-CoA), a framework of five conversation
patterns to construct effective multi-turn jailbreaks through natural dialogue.
Evaluating PE-CoA on twelve LLMs spanning ten harm categories, we achieve
state-of-the-art performance, uncovering pattern-specific vulnerabilities and
LLM behavioral characteristics: models exhibit distinct weakness profiles where
robustness to one conversational pattern does not generalize to others, and
model families share similar failure modes. These findings highlight
limitations of safety training and indicate the need for pattern-aware
defenses. Code available on: https://github.com/Ragib-Amin-Nihal/PE-CoA

</details>


### [43] [Quality Estimation Reranking for Document-Level Translation](https://arxiv.org/abs/2510.08870)
*Krzysztof Mrozinski,Minji Kang,Ahmed Khota,Vincent Michael Sutanto,Giovanni Gatti De Giacomo*

Main category: cs.CL

TL;DR: 本文研究了质量估计重排序在文档级机器翻译中的应用效果，利用多种基于学习和大型语言模型的质量估计指标，实现了显著的翻译质量提升。


<details>
  <summary>Details</summary>
Motivation: 当前质量估计重排序主要应用于句子级翻译，文档级翻译的应用尚未充分探讨，希望利用该技术提升更长文本的翻译质量。

Method: 采用多种学习型和大型语言模型（LLM）基础的质量估计指标对文档级翻译候选结果进行评分和选择，具体指标包括SLIDE和GEMBA-DA，评估不同候选数量下的提升效果。

Result: 使用SLIDE指标，在两个候选集时BLEURT-20得分提升+2.00，32个候选时提升+5.09；使用LLM基础的GEMBA-DA指标分别提升+1.63和+4.30。即使在最长文档（512-1024个源词）上也有明显提升。

Conclusion: 文档级质量估计重排序能有效提升机器翻译质量，且计算开销较小，在具备合适模型和硬件条件下具有实际应用价值。

Abstract: Quality estimation (QE) reranking is a form of quality-aware decoding which
aims to improve machine translation (MT) by scoring and selecting the best
candidate from a pool of generated translations. While known to be effective at
the sentence level, its application to the increasingly prominent domain of
document-level translation remains underexplored. In this work, we evaluate QE
reranking performance on document-level (rather than the typical
sentence-level) translation, using various learned and large language model
(LLM)-based QE metrics. We find that with our best learned metric, SLIDE,
BLEURT-20 scores improve by +2.00 with only two candidates, and by +5.09 with
32, across both decoder-only LLM models and encoder-decoder neural machine
translation (NMT) models. Using the best LLM-based metric, GEMBA-DA, gains of
+1.63 and +4.30 are achieved under the same conditions. Although gains shrink
with longer inputs, reranking with 32 candidates yields improvements of +2.34
(SLIDE) and +1.40 (GEMBA-DA) on our longest documents (512-1024 source tokens).
These findings demonstrate the practical value of document-level QE, with
minimal runtime overhead given suitable translation models and hardware.

</details>


### [44] [FinAuditing: A Financial Taxonomy-Structured Multi-Document Benchmark for Evaluating LLMs](https://arxiv.org/abs/2510.08886)
*Yan Wang,Keyi Wang,Shanshan Yang,Jaisal Patel,Jeff Zhao,Fengran Mo,Xueqing Peng,Lingfei Qian,Jimin Huang,Guojun Xiong,Xiao-Yang Liu,Jian-Yun Nie*

Main category: cs.CL

TL;DR: 本文提出了FinAuditing，这是首个针对金融审计任务的多文档、多层次结构的评测基准，用于评估大型语言模型在结构化金融文档推理中的表现。


<details>
  <summary>Details</summary>
Motivation: 目前GAAP复杂的会计准则和XBRL文件的层次结构增加了财务审计的自动化难度，而现有大型语言模型在多文档、结构化且依赖分类法的财务文档推理上的能力尚未被充分探索。

Method: 基于真实的符合美国GAAP的XBRL文件，FinAuditing包含三类子任务：语义一致性检测（FinSM）、关系一致性检测（FinRE）、数值一致性检测（FinMR），并提出统一的评价框架整合检索、分类和推理指标。

Result: 对13个先进大型语言模型进行零样本测试，发现模型在语义、关系和数学推理上表现不稳定，且在处理多文档层级结构时准确率下降60%-90%。

Conclusion: 现代大型语言模型在基于财务分类法的推理中存在系统性不足，FinAuditing将推动可信赖、结构感知且符合监管要求的金融智能系统的发展。

Abstract: The complexity of the Generally Accepted Accounting Principles (GAAP) and the
hierarchical structure of eXtensible Business Reporting Language (XBRL) filings
make financial auditing increasingly difficult to automate and verify. While
large language models (LLMs) have demonstrated strong capabilities in
unstructured text understanding, their ability to reason over structured,
interdependent, and taxonomy-driven financial documents remains largely
unexplored. To fill this gap, we introduce FinAuditing, the first
taxonomy-aligned, structure-aware, multi-document benchmark for evaluating LLMs
on financial auditing tasks. Built from real US-GAAP-compliant XBRL filings,
FinAuditing defines three complementary subtasks, FinSM for semantic
consistency, FinRE for relational consistency, and FinMR for numerical
consistency, each targeting a distinct aspect of structured auditing reasoning.
We further propose a unified evaluation framework integrating retrieval,
classification, and reasoning metrics across these subtasks. Extensive
zero-shot experiments on 13 state-of-the-art LLMs reveal that current models
perform inconsistently across semantic, relational, and mathematical
dimensions, with accuracy drops of up to 60-90% when reasoning over
hierarchical multi-document structures. Our findings expose the systematic
limitations of modern LLMs in taxonomy-grounded financial reasoning and
establish FinAuditing as a foundation for developing trustworthy,
structure-aware, and regulation-aligned financial intelligence systems. The
benchmark dataset is available at Hugging Face.

</details>


### [45] [Exploring Multi-Temperature Strategies for Token- and Rollout-Level Control in RLVR](https://arxiv.org/abs/2510.08892)
*Haomin Zhuang,Yujun Zhou,Taicheng Guo,Yue Huang,Fangxu Liu,Kai Song,Xiangliang Zhang*

Main category: cs.CL

TL;DR: 本文提出了在大语言模型中对不同类型的tokens采用不同的采样温度，以提升其推理能力的方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要通过限制更新间接促进探索，但未在token生成阶段明确鼓励探索行为。

Method: 对推理token使用较高温度促进探索，对知识token使用较低温度保持准确性，同时系统研究多温度调度策略在强化学习中的作用。

Result: 在多个推理基准测试中，所提方法显著提升了大语言模型的推理性能。

Conclusion: 采用区分温度的采样策略是一种有效提升大语言模型推理能力的强化学习方法。

Abstract: Reinforcement Learning has demonstrated substantial improvements in the
reasoning abilities of Large Language Models (LLMs), exhibiting significant
applicability across various domains. Recent research has identified that
tokens within LLMs play distinct roles during reasoning tasks, categorizing
them into high-entropy reasoning tokens and low-entropy knowledge tokens. Prior
approaches have typically focused on restricting updates to indirectly
encourage exploration, yet they do not explicitly facilitate exploratory
behavior during the token generation stage itself. In this work, we introduce a
complementary approach that explicitly promotes exploration during sampling by
applying distinct temperature settings for different token types. Specifically,
our method employs higher temperatures for reasoning tokens to actively
encourage exploration, while retaining lower temperatures for knowledge tokens
to maintain factual correctness. Furthermore, we systematically investigate
various multi-temperature scheduling strategies and their impacts within
reinforcement learning contexts. Empirical evaluations on several reasoning
benchmarks demonstrate that our approach significantly enhances the reasoning
performance of LLMs. The code is available at
https://github.com/zhmzm/Multi_Temperature_Verl.git.

</details>


### [46] [A Unified Biomedical Named Entity Recognition Framework with Large Language Models](https://arxiv.org/abs/2510.08902)
*Tengxiao Lv,Ling Luo,Juntao Li,Yanhua Wang,Yuchen Pan,Chao Liu,Yanan Wang,Yan Jiang,Huiyi Lv,Yuanyuan Sun,Jian Wang,Hongfei Lin*

Main category: cs.CL

TL;DR: 本文提出了一种基于大语言模型的统一生物医学命名实体识别框架，解决了嵌套实体、边界模糊及跨语言泛化难题。


<details>
  <summary>Details</summary>
Motivation: 生物医学命名实体识别面临嵌套实体识别、实体边界模糊和跨语言泛化等挑战，现有方法效果有限。

Method: 将生物医学命名实体识别转化为文本生成任务，设计符号标注策略处理嵌套实体；通过中英双语多任务联合微调提升跨语言泛化；引入基于对比学习的实体选择器筛选错误预测。

Result: 在四个基准数据集和两个未见语料上，方法实现了最先进性能和稳健的零样本跨语言泛化。

Conclusion: 所提大语言模型框架有效解决了生物医学实体识别的多项难题，具备良好泛化能力和应用前景。

Abstract: Accurate recognition of biomedical named entities is critical for medical
information extraction and knowledge discovery. However, existing methods often
struggle with nested entities, entity boundary ambiguity, and cross-lingual
generalization. In this paper, we propose a unified Biomedical Named Entity
Recognition (BioNER) framework based on Large Language Models (LLMs). We first
reformulate BioNER as a text generation task and design a symbolic tagging
strategy to jointly handle both flat and nested entities with explicit boundary
annotation. To enhance multilingual and multi-task generalization, we perform
bilingual joint fine-tuning across multiple Chinese and English datasets.
Additionally, we introduce a contrastive learning-based entity selector that
filters incorrect or spurious predictions by leveraging boundary-sensitive
positive and negative samples. Experimental results on four benchmark datasets
and two unseen corpora show that our method achieves state-of-the-art
performance and robust zero-shot generalization across languages. The source
codes are freely available at https://github.com/dreamer-tx/LLMNER.

</details>


### [47] [Autoencoding-Free Context Compression for LLMs via Contextual Semantic Anchors](https://arxiv.org/abs/2510.08907)
*Xin Liu,RunSong Zhao,PengCheng Huang,XinYu Liu,JunYi Xiao,ChunYang Xiao,Tong Xiao,Shengxiang Gao,Zhengtao Yu,JingBo Zhu*

Main category: cs.CL

TL;DR: 本文提出了一种新的上下文压缩方法SAC，通过选择锚点词汇并直接利用其上下文信息代替传统的自动编码任务，实现了更高效、更准确的语言模型推理加速。


<details>
  <summary>Details</summary>
Motivation: 现有的上下文压缩方法依赖自动编码任务训练压缩器，但这种方法优化目标与实际下游任务存在差异，导致压缩后特征效果较差。

Method: 提出SAC，直接从原始上下文中选择锚点词汇，将上下文信息聚合到锚点的键值表示中，配合锚点嵌入和双向注意力机制，无需自动编码训练即可完成压缩。

Result: 实验表明，SAC在多种压缩比例下均优于现有压缩方法，尤其在5倍压缩情况下在MRQA数据集上提高了1个EM分数，并且优势随压缩比上升而增加。

Conclusion: SAC方法有效解决了传统自动编码压缩的限制，通过锚点选择和注重上下文信息捕捉，实现了更准确高效的上下文压缩，提升了LLM推理性能。

Abstract: Context compression presents a promising approach for accelerating large
language model (LLM) inference by compressing long contexts into compact
representations. Current context compression methods predominantly rely on
autoencoding tasks to train context-agnostic compression tokens to compress
contextual semantics. While autoencoding tasks enable compression tokens to
acquire compression capabilities, compression via autoencoding tasks creates a
fundamental mismatch: the models are optimized for reconstruction that diverge
from actual downstream tasks, thereby weakening the features more beneficial
for real-world usage. We propose Semantic-Anchor Compression (SAC), a novel
method that shifts from autoencoding task based compression to an architecture
that is equipped with this compression capability \textit{a priori}. Instead of
training models to compress contexts through autoencoding tasks, SAC directly
selects so-called anchor tokens from the original context and aggregates
contextual information into their key-value (KV) representations. By deriving
representations directly from the contextual tokens, SAC eliminates the need
for autoencoding training. To ensure compression performance while directly
leveraging anchor tokens, SAC incorporates two key designs: (1) anchor
embeddings that enable the compressor to identify critical tokens, and (2)
bidirectional attention modification that allows anchor tokens to capture
information from the entire context. Experimental results demonstrate that SAC
consistently outperforms existing context compression methods across various
compression ratios. On out-of-distribution evaluation using MRQA, SAC achieves
1 EM improvement at 5x compression over strong baselines, with increasing
advantages at higher compression ratios.

</details>


### [48] [Artificial Impressions: Evaluating Large Language Model Behavior Through the Lens of Trait Impressions](https://arxiv.org/abs/2510.08915)
*Nicholas Deas,Kathleen McKeown*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型（LLMs）内部对提示的人工印象，类似于人类基于语言形成的刻板印象。通过线性探针预测印象，分析印象与模型行为和提示特征的关系，发现印象在隐藏层中更稳定，也能预测模型回答质量与犹豫表达。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs内部如何形成类似人类的印象和刻板印象，以及这些印象如何影响模型行为。

Method: 在生成的提示上训练线性探针，依据两维刻板印象内容模型(SCM)预测印象，并分析印象与下游任务表现及提示特征的关系。

Result: LLMs在直接报告印象时表现不一致，但隐藏表示中的印象可稳定线性解码。人工印象能预测模型回应的质量和犹豫用法。特定内容、风格和方言特征显著影响印象。

Conclusion: LLMs内部存在稳定的人工印象，这些印象反映了模型对提示的理解并影响其行为，提示设计应考虑语言特征对模型印象的影响。

Abstract: We introduce and study artificial impressions--patterns in LLMs' internal
representations of prompts that resemble human impressions and stereotypes
based on language. We fit linear probes on generated prompts to predict
impressions according to the two-dimensional Stereotype Content Model (SCM).
Using these probes, we study the relationship between impressions and
downstream model behavior as well as prompt features that may inform such
impressions. We find that LLMs inconsistently report impressions when prompted,
but also that impressions are more consistently linearly decodable from their
hidden representations. Additionally, we show that artificial impressions of
prompts are predictive of the quality and use of hedging in model responses. We
also investigate how particular content, stylistic, and dialectal features in
prompts impact LLM impressions.

</details>


### [49] [SOP-Maze: Evaluating Large Language Models on Complicated Business Standard Operating Procedures](https://arxiv.org/abs/2510.08942)
*Jiaming Wang,Zhe Tang,Yilin Jin,Peng Ding,Xiaoyu Li,Xuezhi Cao*

Main category: cs.CL

TL;DR: 该论文提出了SOP-Maze，一个基于真实商业数据的复杂标准操作流程（SOP）评测基准，包含397个任务，评估大型语言模型在复杂业务场景中的指令遵循和决策能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评测多关注一般指令和决策能力，而复杂的业务SOP流程评估尚未充分研究，存在能力检测空白。

Method: 构建SOP-Maze基准，涵盖23个复杂SOP场景，将任务分为LRS（广泛选项任务）和HRS（深层逻辑推理任务）两类；并通过大量实验评测多种主流模型表现。

Result: 实验显示当前主流LLM在SOP-Maze上表现普遍不足，主要错误包括路线盲区（无法有效遵循流程）、对话脆弱性（难处理真实对话细节）和计算错误（时间或算术推理失误）。

Conclusion: SOP-Maze体现了LLM在复杂业务流程中的能力挑战，研究为提升模型应对复杂业务任务的能力提供了新视角和基础。

Abstract: As large language models (LLMs) are widely deployed as domain-specific
agents, many benchmarks have been proposed to evaluate their ability to follow
instructions and make decisions in real-world scenarios. However, business
scenarios often involve complex standard operating procedures (SOPs), and the
evaluation of LLM capabilities in such contexts has not been fully explored. To
bridge this gap, we propose SOP-Maze, a benchmark constructed from real-world
business data and adapted into a collection of 397 tasks from 23 complex SOP
scenarios. We further categorize SOP tasks into two broad classes: Lateral Root
System (LRS), representing wide-option tasks that demand precise selection; and
Heart Root System (HRS), which emphasizes deep logical reasoning with complex
branches. Extensive experiments reveal that nearly all state-of-the-art models
struggle with SOP-Maze. We conduct a comprehensive analysis and identify three
key error categories: (i) route blindness: difficulty following procedures;
(ii) conversational fragility: inability to handle real dialogue nuances; and
(iii) calculation errors: mistakes in time or arithmetic reasoning under
complex contexts. The systematic study explores LLM performance across SOP
tasks that challenge both breadth and depth, offering new insights for
improving model capabilities. We have open-sourced our work on
https://github.com/ADoublLEN/SOP-Maze.

</details>


### [50] [A Human Behavioral Baseline for Collective Governance in Software Projects](https://arxiv.org/abs/2510.08956)
*Mobina Noori,Mahasweta Chakraborti,Amy X Zhang,Seth Frey*

Main category: cs.CL

TL;DR: 本文分析了开源社区通过版本控制的治理文档描述参与和控制方式，发现治理体系通过扩展角色和行动实现更均衡的参与而规则成分保持稳定。


<details>
  <summary>Details</summary>
Motivation: 为了理解开源项目中的参与和控制机制如何随时间演变，解析版本控制的治理文档提供了新的视角。

Method: 采集710个项目的版本控制治理文档快照，解析文本构成（角色、规则、行动、对象），并用熵、丰富度及Jensen Shannon散度等指标衡量其变化。

Result: 发现项目随着时间推移定义了更多角色和行动且分布更均匀，但规则组成保持稳定，表明治理通过扩展和均衡参与类别而非重大规则调整演化。

Conclusion: 治理机制的增长表现为参与类别的扩展和平衡，为评估未来AI介入的工作流程是否集中或重新分配权威提供了可重复的基线。

Abstract: We study how open source communities describe participation and control
through version controlled governance documents. Using a corpus of 710 projects
with paired snapshots, we parse text into actors, rules, actions, and objects,
then group them and measure change with entropy for evenness, richness for
diversity, and Jensen Shannon divergence for drift. Projects define more roles
and more actions over time, and these are distributed more evenly, while the
composition of rules remains stable. These findings indicate that governance
grows by expanding and balancing categories of participation without major
shifts in prescriptive force. The analysis provides a reproducible baseline for
evaluating whether future AI mediated workflows concentrate or redistribute
authority.

</details>


### [51] [Creation of the Chinese Adaptive Policy Communication Corpus](https://arxiv.org/abs/2510.08986)
*Bolun Sun,Charles Chang,Yuen Yuen Ang,Pingxu Hao,Ruotong Mu,Yuchen Xu,Zhengxin Zhang*

Main category: cs.CL

TL;DR: 介绍了CAPC-CG语料库，包含1949-2023年中国中央政府政策指令的清晰与模糊语言标注，是首个此类开源数据集。


<details>
  <summary>Details</summary>
Motivation: 缺乏带有清晰与模糊语言分类、覆盖大量中国官方政策文件的开放数据集，制约政策语言研究和相关下游任务发展。

Method: 构建语料库，基于Ang的自适应政策沟通理论开发五色分类，进行段落级别标注并设立两轮标注机制以确保高标注一致性。

Result: 语料库共包含330万段落，专家与受训标注者间Fleiss’s kappa达到0.86，发布包括完整元数据、标注规范和大型语言模型的基线分类结果。

Conclusion: CAPC-CG语料库为政策沟通多语言NLP研究及下游应用提供了高质量资源，推动该领域发展。

Abstract: We introduce CAPC-CG, the Chinese Adaptive Policy Communication (Central
Government) Corpus, the first open dataset of Chinese policy directives
annotated with a five-color taxonomy of clear and ambiguous language
categories, building on Ang's theory of adaptive policy communication. Spanning
1949-2023, this corpus includes national laws, administrative regulations, and
ministerial rules issued by China's top authorities. Each document is segmented
into paragraphs, producing a total of 3.3 million units. Alongside the corpus,
we release comprehensive metadata, a two-round labeling framework, and a
gold-standard annotation set developed by expert and trained coders.
Inter-annotator agreement achieves a Fleiss's kappa of K = 0.86 on directive
labels, indicating high reliability for supervised modeling. We provide
baseline classification results with several large language models (LLMs),
together with our annotation codebook, and describe patterns from the dataset.
This release aims to support downstream tasks and multilingual NLP research in
policy communication.

</details>


### [52] [MASA: LLM-Driven Multi-Agent Systems for Autoformalization](https://arxiv.org/abs/2510.08988)
*Lan Zhang,Marco Valentino,André Freitas*

Main category: cs.CL

TL;DR: MASA是一个基于大型语言模型驱动的多智能体系统框架，用于将自然语言自动形式化为数学表述。


<details>
  <summary>Details</summary>
Motivation: 连接自然语言和形式化推理，提高自动形式化的效率和可靠性。

Method: 构建一个模块化、灵活且可扩展的多智能体系统，利用协作智能体将自然语言声明转换为形式化表示。

Result: 通过在真实数学定义和形式数学数据集上的用例和实验展示了MASA的有效性。

Conclusion: 多智能体系统结合大型语言模型和定理证明器能够显著提升自动形式化的效率和可靠性，为研究者和实践者提供支持。

Abstract: Autoformalization serves a crucial role in connecting natural language and
formal reasoning. This paper presents MASA, a novel framework for building
multi-agent systems for autoformalization driven by Large Language Models
(LLMs). MASA leverages collaborative agents to convert natural language
statements into their formal representations. The architecture of MASA is
designed with a strong emphasis on modularity, flexibility, and extensibility,
allowing seamless integration of new agents and tools to adapt to a
fast-evolving field. We showcase the effectiveness of MASA through use cases on
real-world mathematical definitions and experiments on formal mathematics
datasets. This work highlights the potential of multi-agent systems powered by
the interaction of LLMs and theorem provers in enhancing the efficiency and
reliability of autoformalization, providing valuable insights and support for
researchers and practitioners in the field.

</details>


### [53] [DARO: Difficulty-Aware Reweighting Policy Optimization](https://arxiv.org/abs/2510.09001)
*Jingyu Zhou,Lu Ma,Hao Liang,Chengyu Shen,Bin Cui,Wentao Zhang*

Main category: cs.CL

TL;DR: 该论文针对现有基于群体相对策略优化（GRPO）的增强学习方法在动态权重调整上的不足，提出了难度感知重加权策略优化（DARO）方法，显著提升模型在数学推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于静态或过于简单的权重分配，无法适应模型不断变化的能力，导致训练时过度关注某些难度级别，影响整体性能。

Method: 提出一种动态调整各难度组损失贡献的策略优化方法——难度感知重加权策略优化（DARO），根据模型学习状态自动调整权重分配。

Result: 在多个大型模型（Qwen2.5-Math-1.5B/7B、Llama3.1-8B）和六个数学基准测试上，DARO相较于四个领先基线方法，实现了更快的收敛速度和更优的最终性能。

Conclusion: DARO有效解决了传统GRPO及其变体在动态权重调整上的缺陷，显著提升了大语言模型的推理能力和训练效率。

Abstract: Recent advances in large language models (LLMs) have shown that reasoning
ability can be significantly enhanced through Reinforcement Learning with
Verifiable Rewards (RLVR). Group Relative Policy Optimization (GRPO) has
emerged as the de facto approach for RLVR, inspiring numerous variants.
However, our mathematical analysis reveals that these methods are fundamentally
weighted variations of GRPO. We provide a unified view, demonstrating that
their reliance on static or overly simplistic weighting schemes tied to sample
difficulty prevents adaptation to a model's evolving capabilities. This creates
a significant loss scale issue, where training disproportionately focuses on
certain difficulty levels at the expense of others, hindering overall
performance. To address these limitations, we introduce
\textbf{Difficulty-Aware Reweighting Policy Optimization (DARO)}, a method that
dynamically adjusts the loss contribution of each difficulty group based on the
model's learning state. Extensive experiments on Qwen2.5-Math-1.5B,
Qwen2.5-Math-7B, and Llama3.1-8B show that DARO outperforms four leading
baselines across six math benchmarks, achieving significantly faster
convergence and superior final performance.

</details>


### [54] [Decoupling Safety into Orthogonal Subspace: Cost-Efficient and Performance-Preserving Alignment for Large Language Models](https://arxiv.org/abs/2510.09004)
*Yutao Mou,Xiaoling Zhou,Yuxiao Luo,Shikun Zhang,Wei Ye*

Main category: cs.CL

TL;DR: 本论文提出使用LoRA技术仅通过安全数据训练，实现性能不降的安全对齐，解决了传统方法成本高且效果有限的问题。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐方法需要在安全数据和通用数据之间找到平衡，计算成本高且提升有限。

Method: 采用LoRA基于拒绝训练的方法，仅用安全数据进行训练，通过低秩子空间将安全性与模型本征能力解耦，实现安全性能提升同时不损害模型通用性能。

Result: 实验证明了LoRA拒绝训练在性能不降的前提下提升安全，对安全性和模型固有变换空间的正交分解提供了理论支持。

Conclusion: LoRA作为一种经济高效、性能保持的模块化安全补丁，有效促进了安全对齐且不会影响模型的固有能力。

Abstract: Safety alignment is essential for building trustworthy artificial
intelligence, yet it remains challenging to enhance model safety without
degrading general performance. Current approaches require computationally
expensive searches for the optimal proportion of safety-critical and
general-purpose data to balance safety and general performance, incurring high
costs with limited gains. In this work, we show that LoRA-based
Refusal-training enables performance-preserving safety alignment even when
trained solely on safety data, demonstrating that LoRA serves as
cost-efficient, performance-preserving, and plug-and-play safety patches.
Beyond empirical findings, we provide both theoretical and experimental
evidence that LoRA effectively decouples safety into a low-rank subspace
largely orthogonal to the model's intrinsic transformation space, ensuring that
safety enhancements do not interfere with inherent capabilities.

</details>


### [55] [LitE-SQL: A Lightweight and Efficient Text-to-SQL Framework with Vector-based Schema Linking and Execution-Guided Self-Correction](https://arxiv.org/abs/2510.09014)
*Shengmin Piao,Jieun Lee,Sanghyun Park*

Main category: cs.CL

TL;DR: LitE-SQL框架实现轻量高效的Text-to-SQL任务，兼顾隐私保护和性能表现。


<details>
  <summary>Details</summary>
Motivation: 当前依赖大型专有语言模型的Text-to-SQL方法存在部署困难和隐私安全隐患，亟需轻量且高效的解决方案。

Method: 提出LitE-SQL框架，包含预计算的向量数据库支持的Schema Retriever和两阶段微调（监督+执行引导强化学习）的SQL生成器，自纠错且避免多候选生成成本。

Result: LitE-SQL在BIRD数据集达72.10%执行准确率，Spider 1.0达88.45%，性能与大型模型相当甚至优越，参数量减少2至30倍。

Conclusion: 轻量级模型同样能实现高质量Text-to-SQL生成，适合对隐私敏感和资源受限的应用场景。

Abstract: The Text-to-SQL task translates natural language questions into SQL queries,
enabling intuitive database interaction for non-experts. While recent methods
leveraging Large Language Models (LLMs) achieve strong performance, their
reliance on proprietary models raise concerns about deployment feasibility and
data privacy. In this work, we introduce LitE-SQL, a Lightweight and Efficient
framework with two components: (i) a Schema Retriever that performs efficient
schema linking using a vector database of pre-computed schema embeddings, and
(ii) a SQL Generator fine-tuned in two stages-supervised fine-tuning followed
by execution-guided reinforcement-enabling self-correction without costly
multi-candidate generation. On BIRD, LitE-SQL achieves 72.10% execution
accuracy, and on Spider 1.0 it reaches 88.45%, demonstrating comparable or
superior performance to LLM-based methods despite using 2x to 30x fewer
parameters. Our findings demonstrate that high-quality Text-to-SQL generation
is feasible with lightweight models, offering a practical solution for
privacy-sensitive and resource-constrained settings.

</details>


### [56] [Automated Refinement of Essay Scoring Rubrics for Language Models via Reflect-and-Revise](https://arxiv.org/abs/2510.09030)
*Keno Harada,Lui Yoshida,Takeshi Kojima,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.CL

TL;DR: 本文通过迭代优化评分量表，提升大语言模型（LLMs）在自动作文评分（AES）中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型对于提示敏感，优化评分量表可提升自动作文评分的准确性。

Method: 让模型反思自身评分理由及与人工评分的差异，迭代调整评分量表。

Result: 在TOEFL11和ASAP数据集上，使用GPT-4.1等模型，Quadratic Weighted Kappa提高0.19至0.47，优于人工设计评分量表。

Conclusion: 迭代优化评分量表的方法显著提升了LLMs自动作文评分与人工评分的匹配度。

Abstract: The performance of Large Language Models (LLMs) is highly sensitive to the
prompts they are given. Drawing inspiration from the field of prompt
optimization, this study investigates the potential for enhancing Automated
Essay Scoring (AES) by refining the scoring rubrics used by LLMs. Specifically,
our approach prompts models to iteratively refine rubrics by reflecting on
models' own scoring rationales and observed discrepancies with human scores on
sample essays. Experiments on the TOEFL11 and ASAP datasets using GPT-4.1,
Gemini-2.5-Pro, and Qwen-3-Next-80B-A3B-Instruct show Quadratic Weighted Kappa
(QWK) improvements of up to 0.19 and 0.47, respectively. Notably, even with a
simple initial rubric, our approach achieves comparable or better QWK than
using detailed human-authored rubrics. Our findings highlight the importance of
iterative rubric refinement in LLM-based AES to enhance alignment with human
evaluations.

</details>


### [57] [Exploring Cross-Lingual Knowledge Transfer via Transliteration-Based MLM Fine-Tuning for Critically Low-resource Chakma Language](https://arxiv.org/abs/2510.09032)
*Adity Khisa,Nusrat Jahan Lia,Tasnim Mahfuz Nafis,Zarif Masud,Tanzir Pial,Shebuti Rayana,Ahmedul Kabir*

Main category: cs.CL

TL;DR: 该论文介绍了一个新的孟加拉语音译的查克玛语语料库，并用其对多种多语言预训练模型进行了微调，显著提升了模型在查克玛语上的表现。


<details>
  <summary>Details</summary>
Motivation: 查克玛语作为一种资源有限的印欧-雅利安语言在语言模型中未被充分代表，亟需专用数据集和方法提升相关语言模型的表现。

Method: 作者构建了一个由查克玛文学文本转写成孟加拉语拼写的语料库，并用六种基于编码器的多语言和区域变换器模型进行带掩码语言模型任务的微调。

Result: 微调后的多语言模型在孟加拉语音译查克玛语上的表现优于预训练模型，最高实现73.54%的词元准确率和2.90的困惑度，体现了数据质量对模型表现的重要影响。

Conclusion: 研究表明，孟加拉语音译的查克玛语数据对查克玛语迁移学习效果显著，且作者发布了手工验证的单语数据集以助力低资源语言的多语言建模研究。

Abstract: As an Indo-Aryan language with limited available data, Chakma remains largely
underrepresented in language models. In this work, we introduce a novel corpus
of contextually coherent Bangla-transliterated Chakma, curated from Chakma
literature, and validated by native speakers. Using this dataset, we fine-tune
six encoder-based multilingual and regional transformer models (mBERT,
XLM-RoBERTa, DistilBERT, DeBERTaV3, BanglaBERT, and IndicBERT) on masked
language modeling (MLM) tasks. Our experiments show that fine-tuned
multilingual models outperform their pre-trained counterparts when adapted to
Bangla-transliterated Chakma, achieving up to 73.54% token accuracy and a
perplexity as low as 2.90. Our analysis further highlights the impact of data
quality on model performance and shows the limitations of OCR pipelines for
morphologically rich Indic scripts. Our research demonstrates that
Bangla-transliterated Chakma can be very effective for transfer learning for
Chakma language, and we release our manually validated monolingual dataset to
encourage further research on multilingual language modeling for low-resource
languages.

</details>


### [58] [Large Language Models Do NOT Really Know What They Don't Know](https://arxiv.org/abs/2510.09033)
*Chi Seng Cheang,Hou Pong Chan,Wenxuan Zhang,Yang Deng*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在内部表示中编码了事实性信号，但无法可靠地区分事实与虚假信息。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs内部计算能否区分事实性回答和虚假产出。

Method: 通过比较两类基于主体信息依赖的虚假答案，分析LLMs处理事实查询时的内部机制。

Result: 与主体知识相关的虚假答案与正确回答在隐层状态上相似难辨，而无关的虚假答案产生可区分的聚类表示。

Conclusion: LLMs内部状态编码的是知识回忆模式而非事实真实性，说明它们无法真正识别自身未知的信息。

Abstract: Recent work suggests that large language models (LLMs) encode factuality
signals in their internal representations, such as hidden states, attention
weights, or token probabilities, implying that LLMs may "know what they don't
know". However, LLMs can also produce factual errors by relying on shortcuts or
spurious associations. These error are driven by the same training objective
that encourage correct predictions, raising the question of whether internal
computations can reliably distinguish between factual and hallucinated outputs.
In this work, we conduct a mechanistic analysis of how LLMs internally process
factual queries by comparing two types of hallucinations based on their
reliance on subject information. We find that when hallucinations are
associated with subject knowledge, LLMs employ the same internal recall process
as for correct responses, leading to overlapping and indistinguishable
hidden-state geometries. In contrast, hallucinations detached from subject
knowledge produce distinct, clustered representations that make them
detectable. These findings reveal a fundamental limitation: LLMs do not encode
truthfulness in their internal states but only patterns of knowledge recall,
demonstrating that "LLMs don't really know what they don't know".

</details>


### [59] [Alif: Advancing Urdu Large Language Models via Multilingual Synthetic Data Distillation](https://arxiv.org/abs/2510.09051)
*Muhammad Ali Shafique,Kanwal Mehreen,Muhammad Arham,Maaz Amjad,Sabur Butt,Hamza Farooq*

Main category: cs.CL

TL;DR: 该论文提出了Alif-1.0-8B-Instruct，一种针对乌尔都语和英语的多语言大模型，通过高质量合成数据和改进的自指导技术，提高了低资源语言的模型表现。


<details>
  <summary>Details</summary>
Motivation: 针对低资源语言乌尔都语在数据稀缺、多语言不一致和安全性方面的挑战，现有方法依赖低质量翻译数据，效果和文化适应性不足，且成本较高。

Method: 采用修改后的自指导方法生成高质量多语言合成数据集（Urdu-Instruct），结合独特提示和种子值，融入乌尔都语链式推理、双语翻译、文化相关性和伦理安全对齐，基于Llama-3.1-8B预训练模型进行训练。

Result: Alif-1.0-8B-Instruct在乌尔都语任务上明显优于Llama-3.1-8B-Instruct和其他主流多语言大模型（如Mistral-7B-Instruct-v0.3、Qwen-2.5-7B-Instruct和Cohere-Aya-Expanse-8B），且训练成本低于100美元。

Conclusion: 通过改进的自指导方法，可有效且经济地开发高性能、符合文化特点的低资源语言大语言模型，为多语言模型研究提供了新方向。

Abstract: Developing a high-performing large language models (LLMs) for low-resource
languages such as Urdu, present several challenges. These challenges include
the scarcity of high-quality datasets, multilingual inconsistencies, and safety
concerns. Existing multilingual LLMs often address these issues by translating
large volumes of available data. However, such translations often lack quality
and cultural nuance while also incurring significant costs for data curation
and training. To address these issues, we propose Alif-1.0-8B-Instruct, a
multilingual Urdu-English model, that tackles these challenges with a unique
approach. We train the model on a high-quality, multilingual synthetic dataset
(Urdu-Instruct), developed using a modified self-instruct technique. By using
unique prompts and seed values for each task along with a global task pool,
this dataset incorporates Urdu-native chain-of-thought based reasoning,
bilingual translation, cultural relevance, and ethical safety alignments. This
technique significantly enhances the comprehension of Alif-1.0-8B-Instruct
model for Urdu-specific tasks. As a result, Alif-1.0-8B-Instruct, built upon
the pretrained Llama-3.1-8B, demonstrates superior performance compared to
Llama-3.1-8B-Instruct for Urdu specific-tasks. It also outperformed leading
multilingual LLMs, including Mistral-7B-Instruct-v0.3, Qwen-2.5-7B-Instruct,
and Cohere-Aya-Expanse-8B, all within a training budget of under $100. Our
results demonstrate that high-performance and low-resource language LLMs can be
developed efficiently and culturally aligned using our modified self-instruct
approach. All datasets, models, and code are publicly available at:
https://github.com/traversaal-ai/alif-urdu-llm.

</details>


### [60] [ReFIne: A Framework for Trustworthy Large Reasoning Models with Reliability, Faithfulness, and Interpretability](https://arxiv.org/abs/2510.09062)
*Chung-En Sun,Ge Yan,Akshay Kulkarni,Tsui-Wei Weng*

Main category: cs.CL

TL;DR: 该论文提出了ReFIne训练框架，通过结构化推理轨迹、高亮关键决策信息及自我评估提升模型的可解释性、忠实度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前长链思考推理模型主要关注准确度和代币效率，忽视了模型的可信性，这对实际应用至关重要。

Method: 结合监督微调和GRPO方法，训练模型生成结构化的推理过程，披露关键决策信息，并进行自我评估以提升可信性。

Result: 在Qwen3系列模型上进行了多尺度、多难度数学基准测试，结果显示ReFIne显著提升了推理轨迹的清晰度和结构性（+44.0%），忠实度（+18.8%）及置信度评估的可靠性（+42.4%）。

Conclusion: 推理模型不仅要优化准确率，更应关注可解释性、忠实性和可靠性这三个可信性维度，ReFIne框架有效推动了这一方向的发展。

Abstract: Recent advances in long chain-of-thought (CoT) reasoning have largely
prioritized answer accuracy and token efficiency, while overlooking aspects
critical to trustworthiness. We argue that usable reasoning systems must be
trustworthy, characterized by three properties: interpretability, faithfulness,
and reliability. To this end, we propose ReFIne, a new training framework that
integrates supervised fine-tuning with GRPO to encourage models to: (i) improve
interpretability by producing structured, tag-based traces with high-level
planning that are easier for humans to follow; (ii) enhance faithfulness by
explicitly disclosing the decisive information guiding each solution, with
consistent cross-section references; and (iii) promote reliability by providing
self-assessments of both the derivation's soundness and the confidence of the
final answer. We apply ReFIne to the Qwen3 models at multiple scales
(1.7B/4B/8B) and evaluate across mathematical benchmarks of varying difficulty.
Our experimental results show that ReFIne models generate clearer and
better-structured reasoning traces (interpretability +44.0%), more faithfully
expose their underlying decision process (faithfulness +18.8%), and offer
informative confidence estimates (reliability +42.4%). These findings highlight
an overlooked but important direction: reasoning models should be optimized not
only for accuracy, but also for broader dimensions of trustworthiness. Our code
is available at:
https://github.com/Trustworthy-ML-Lab/Training_Trustworthy_LRM_with_Refine

</details>


### [61] [FrameEOL: Semantic Frame Induction using Causal Language Models](https://arxiv.org/abs/2510.09097)
*Chihiro Yano,Kosuke Yamada,Hayato Tsukagoshi,Ryohei Sasano,Koichi Takeda*

Main category: cs.CL

TL;DR: 本文提出了一种基于因果语言模型（CLM）的语义框架诱导新方法，利用FrameEOL提示方法结合上下文学习和深度度量学习，显著提升了对英文和日文FrameNet语料的框架聚类效果。


<details>
  <summary>Details</summary>
Motivation: 现有高性能的语义框架诱导多依赖于掩码语言模型（MLM），而因果语言模型（如GPT、Llama）虽具有强大的语言理解能力，尚未应用于该任务。尤其是资源有限的语言如日语，亟需有效的框架诱导方法。

Method: 提出FrameEOL，一种基于CLM的提示方法生成框架嵌入；结合上下文学习(ICL)和深度度量学习(DML)获得更适合聚类的嵌入表示；最终通过聚类实现框架诱导。

Result: 在英文和日文FrameNet数据集上，该方法优于现有框架诱导方法。特别是日语仅用5个ICL实例的CLM方法性能可与经过DML微调的MLM方法相媲美。

Conclusion: 基于CLM的FrameEOL方法能有效进行语义框架诱导，尤其在资源匮乏语言环境中表现出竞争力，展示了CLM在语义框架任务中的潜力。

Abstract: Semantic frame induction is the task of clustering frame-evoking words
according to the semantic frames they evoke. In recent years, leveraging
embeddings of frame-evoking words that are obtained using masked language
models (MLMs) such as BERT has led to high-performance semantic frame
induction. Although causal language models (CLMs) such as the GPT and Llama
series succeed in a wide range of language comprehension tasks and can engage
in dialogue as if they understood frames, they have not yet been applied to
semantic frame induction. We propose a new method for semantic frame induction
based on CLMs. Specifically, we introduce FrameEOL, a prompt-based method for
obtaining Frame Embeddings that outputs One frame-name as a Label representing
the given situation. To obtain embeddings more suitable for frame induction, we
leverage in-context learning (ICL) and deep metric learning (DML). Frame
induction is then performed by clustering the resulting embeddings.
Experimental results on the English and Japanese FrameNet datasets demonstrate
that the proposed methods outperform existing frame induction methods. In
particular, for Japanese, which lacks extensive frame resources, the CLM-based
method using only 5 ICL examples achieved comparable performance to the
MLM-based method fine-tuned with DML.

</details>


### [62] [When Retrieval Succeeds and Fails: Rethinking Retrieval-Augmented Generation for LLMs](https://arxiv.org/abs/2510.09106)
*Yongjie Wang,Yue Yu,Kaisong Song,Jun Lin,Zhiqi Shen*

Main category: cs.CL

TL;DR: 本文综述了检索增强生成（RAG）框架在应对大规模语言模型（LLMs）信息时效性和领域适应性不足方面的作用和挑战，并指出结合RAG能显著提升LLMs在特定应用中的表现。


<details>
  <summary>Details</summary>
Motivation: LLMs虽强大，但因训练数据静态，难以应对快速变化的信息和专业领域查询，传统RAG框架在大型模型能力提升后优势减弱。

Method: 文章全面回顾了RAG的目标、核心组件及其关键挑战，分析了限制其效果的薄弱环节。

Result: 探讨了LLMs单独表现不足但结合RAG效果显著提升的应用场景。

Conclusion: 希望推动研究者重新审视RAG的作用，激励下一代RAG系统的发展。

Abstract: Large Language Models (LLMs) have enabled a wide range of applications
through their powerful capabilities in language understanding and generation.
However, as LLMs are trained on static corpora, they face difficulties in
addressing rapidly evolving information or domain-specific queries.
Retrieval-Augmented Generation (RAG) was developed to overcome this limitation
by integrating LLMs with external retrieval mechanisms, allowing them to access
up-to-date and contextually relevant knowledge. However, as LLMs themselves
continue to advance in scale and capability, the relative advantages of
traditional RAG frameworks have become less pronounced and necessary. Here, we
present a comprehensive review of RAG, beginning with its overarching
objectives and core components. We then analyze the key challenges within RAG,
highlighting critical weakness that may limit its effectiveness. Finally, we
showcase applications where LLMs alone perform inadequately, but where RAG,
when combined with LLMs, can substantially enhance their effectiveness. We hope
this work will encourage researchers to reconsider the role of RAG and inspire
the development of next-generation RAG systems.

</details>


### [63] [DITING: A Multi-Agent Evaluation Framework for Benchmarking Web Novel Translation](https://arxiv.org/abs/2510.09116)
*Enze Zhang,Jiaying Wang,Mengxi Xiao,Jifei Liu,Ziyan Kuang,Rui Dong,Youzhong Dong,Sophia Ananiadou,Min Peng,Qianqian Xie*

Main category: cs.CL

TL;DR: 本文提出DITING评估框架和AgentEval多智能体评价体系，专门针对网络小说翻译质量进行全面衡量，结果显示中文训练的LLM在翻译网络小说方面表现优于其他模型。


<details>
  <summary>Details</summary>
Motivation: 现有机器翻译指标无法有效评估网络小说翻译的特殊需求，缺乏对叙事性和文化忠实度的全面考量。

Method: 推出DITING评估体系，覆盖六个维度；设计AgentEval多智能体推理评价框架模拟专家审议；并创建MetricAlign元评价数据集用于评价指标对比。

Result: 通过14款模型评测，发现中文训练的LLM优于更大规模的外语模型，DeepSeek-V3在忠实度和风格一致性上表现最佳。

Conclusion: 本文建立了网络小说翻译的新标准和评价方法，提供了丰富的公共资源，有助于未来基于LLM的网络小说翻译研究和发展。

Abstract: Large language models (LLMs) have substantially advanced machine translation
(MT), yet their effectiveness in translating web novels remains unclear.
Existing benchmarks rely on surface-level metrics that fail to capture the
distinctive traits of this genre. To address these gaps, we introduce DITING,
the first comprehensive evaluation framework for web novel translation,
assessing narrative and cultural fidelity across six dimensions: idiom
translation, lexical ambiguity, terminology localization, tense consistency,
zero-pronoun resolution, and cultural safety, supported by over 18K
expert-annotated Chinese-English sentence pairs. We further propose AgentEval,
a reasoning-driven multi-agent evaluation framework that simulates expert
deliberation to assess translation quality beyond lexical overlap, achieving
the highest correlation with human judgments among seven tested automatic
metrics. To enable metric comparison, we develop MetricAlign, a meta-evaluation
dataset of 300 sentence pairs annotated with error labels and scalar quality
scores. Comprehensive evaluation of fourteen open, closed, and commercial
models reveals that Chinese-trained LLMs surpass larger foreign counterparts,
and that DeepSeek-V3 delivers the most faithful and stylistically coherent
translations. Our work establishes a new paradigm for exploring LLM-based web
novel translation and provides public resources to advance future research.

</details>


### [64] [Augmenting Dialog with Think-Aloud Utterances for Modeling Individual Personality Traits by LLM](https://arxiv.org/abs/2510.09158)
*Seiya Ishikura,Hiroaki Yamada,Tatsuya Hiraoka,Hiroaki Yamada,Takenobu Tokunaga*

Main category: cs.CL

TL;DR: 本文提出通过加入说话者思考过程的语言表达来增强对话数据，以提升大语言模型在文本聊天中模拟个体人格特质的能力。


<details>
  <summary>Details</summary>
Motivation: 传统对话模型难以准确捕捉和模拟个体的人格特质，本文尝试通过引入思考过程的语言表达（TAUs）来更好地建模个性。

Method: 将思考性语言表达（TAUs）加入对话数据，训练‘人格大语言模型’，并基于Big Five人格框架评价模型效果。

Result: 训练过程中加入TAUs的数据使得模型在模拟‘宜人性’和‘神经质’两大人格特质上表现更好；且TAUs的质量影响模型性能。

Conclusion: 利用思考性语言增强对话数据，有助于大语言模型更准确地模拟个体的人格特质。

Abstract: This study proposes augmenting dialog data with think-aloud utterances (TAUs)
for modeling individual personalities in text chat by LLM. TAU is a
verbalization of a speaker's thought before articulating the utterance. We
expect "persona LLMs" trained with TAU-augmented data can mimic the speaker's
personality trait better. We tested whether the trained persona LLMs obtain the
human personality with respect to Big Five, a framework characterizing human
personality traits from five aspects. The results showed that LLMs trained with
TAU-augmented data more closely align to the speakers' Agreeableness and
Neuroticism of Big Five than those trained with original dialog data. We also
found that the quality of TAU-augmentation impacts persona LLM's performance.

</details>


### [65] [Stronger Re-identification Attacks through Reasoning and Aggregation](https://arxiv.org/abs/2510.09184)
*Lucas Georges Gabriel Charpentier,Pierre Lison*

Main category: cs.CL

TL;DR: 本文研究了文本去标识技术的隐私保护效果，通过构建逆向的重识别攻击来评估其鲁棒性，提出了基于识别顺序和推理模型的增强攻击策略。


<details>
  <summary>Details</summary>
Motivation: 当前文本去标识技术难以衡量其隐藏个人身份信息的效果，需要通过逆向的重识别攻击来评估隐私保护的强度。

Method: 提出两种增强的重识别攻击策略：一是通过不同顺序聚合预测结果提升性能，二是利用推理模型增强攻击效果，特别是当攻击者拥有丰富背景知识时。

Result: 两种策略均显著提升了重识别的效果，表明顺序和推理在重识别任务中起关键作用。

Conclusion: 采用多顺序聚合和推理模型能够构建更强的重识别攻击，从而更准确地评估文本去标识技术的隐私保护能力。

Abstract: Text de-identification techniques are often used to mask personally
identifiable information (PII) from documents. Their ability to conceal the
identity of the individuals mentioned in a text is, however, hard to measure.
Recent work has shown how the robustness of de-identification methods could be
assessed by attempting the reverse process of _re-identification_, based on an
automated adversary using its background knowledge to uncover the PIIs that
have been masked. This paper presents two complementary strategies to build
stronger re-identification attacks. We first show that (1) the _order_ in which
the PII spans are re-identified matters, and that aggregating predictions
across multiple orderings leads to improved results. We also find that (2)
reasoning models can boost the re-identification performance, especially when
the adversary is assumed to have access to extensive background knowledge.

</details>


### [66] [LLaMAX2: Your Translation-Enhanced Model also Performs Well in Reasoning](https://arxiv.org/abs/2510.09189)
*Changjiang Gao,Zixian Huang,Jingyang Gong,Shujian Huang,Lei Li,Fei Yuan*

Main category: cs.CL

TL;DR: 该论文提出了一种新的翻译增强方法Qwen3-XPlus，通过分层选择性微调平行数据，提升了大语言模型在翻译和多语言推理任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型虽然推理能力强，但针对翻译优化后推理能力下降，亟需一种既能提升翻译性能又保持推理能力的方法。

Method: 从指导型模型出发，仅在平行数据上进行分层选择微调，形成新型翻译增强流程，应用于Qwen3-XPlus模型。

Result: 模型在高低资源语言的翻译任务中表现显著提升，低资源语言如斯瓦希里语spBLEU达15+，xComet达40+；在多语言任务中平均提升超过1分，同时保持较强推理能力。

Conclusion: 该方法有效提升多语言翻译性能，简化复杂度，增强模型的多语言适用性，且代码和模型已开放。

Abstract: General Large Language Models (LLMs) excel in reasoning, but those enhanced
for translation struggle with reasoning tasks. To address this, we propose a
novel translationenhanced recipe that begins with instruct models and applies
layer-selective tuning only on parallel data. Following this pipeline, we
introduce the Qwen3-XPlus models, which demonstrate significant improvements in
translation performance across both high- and lowresource languages, achieving
15+ spBLEU and 40+ xComet in low-resource languages, like Swahili.
Interestingly, training only with small parallel datasets, Qwen3-XPlus achieves
an average improvement of 1+ points on 7 multilingual tasks while maintaining
proficiency comparable to the Qwen3 instruct model in 15 popular reasoning
datasets. This work offers a promising approach to multilingual enhancement,
significantly reducing complexity and enhancing accessibility for a wider range
of languages. The code and model are publicly available.

</details>


### [67] [DICE: Structured Reasoning in LLMs through SLM-Guided Chain-of-Thought Correction](https://arxiv.org/abs/2510.09211)
*Yiqi Li,Yusheng Liao,Zhe Chen,Yanfeng Wang,Yu Wang*

Main category: cs.CL

TL;DR: 提出了DICE框架，通过小语言模型纠正大语言模型的链式思维输出，提升了结构化输出的准确性和内容正确性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂推理时难以严格遵守用户指定的输出格式，且针对性微调成本高且受限于参数访问。

Method: DICE先由大语言模型生成自然语言回应，再用训练过的小语言模型分析并优化输出，使其满足结构化格式要求；采用两阶段构建数据集及双重微调策略提升小模型性能。

Result: 实验显示DICE在格式准确率和内容正确率上分别提升了35.4%和29.4%，优于其他现有方法。

Conclusion: DICE有效结合了大语言模型的推理能力与小语言模型的格式约束能力，为定制化推理任务提供了高效且准确的解决方案。

Abstract: When performing reasoning tasks with user-specific requirements, such as
strict output formats, large language models (LLMs) often prioritize reasoning
over adherence to detailed instructions. Fine-tuning LLMs on supervised
datasets to address this is impractical due to high computational costs and
limited parameter access. To tackle this, we propose DICE, a lightweight
framework that guides small language models (SLMs) to refine LLMs' outputs
through chain-of-thought (CoT) correction. DICE decouples the process by first
prompting LLMs to generate natural language responses, then using trained SLMs
to analyze and refine these outputs to meet structured output specifications.
This framework preserves LLMs' broad knowledge and reasoning capabilities while
ensuring the outputs conform to user demands. Specifically, DICE first
constructs structured CoT adaptation datasets via a two-stage method and
subsequently applies a dual-tuning strategy to fine-tune SLMs for generating
structured outputs in an analyze-then-answer pattern. Experiments demonstrate
that DICE improves the average format accuracy and content correctness of LLM
outputs by 35.4\% and 29.4\%, respectively, achieving state-of-the-art (SOTA)
performance over other competitive baselines.

</details>


### [68] [IRIS: An Iterative and Integrated Framework for Verifiable Causal Discovery in the Absence of Tabular Data](https://arxiv.org/abs/2510.09217)
*Tao Feng,Lizhen Qu,Niket Tandon,Gholamreza Haffari*

Main category: cs.CL

TL;DR: 本文提出了IRIS框架，通过结合统计算法和基于大模型的方法，实现从初始变量集自动收集文献、提取变量及实时发现因果关系，解决了传统因果发现方法的数据需求和已知关系冗余计算问题，并能扩展遗漏变量。


<details>
  <summary>Details</summary>
Motivation: 传统统计因果发现方法存在数据收集成本高、已知关系计算冗余及假设不切实际等问题。基于大模型的方法虽然能识别已知因果关系，但难以发现新颖因果关系。

Method: IRIS框架采用迭代检索与集成系统，结合统计算法和大模型技术，对初始变量自动收集相关文档并提取变量，同时通过缺失变量提案模块发现并补充遗漏变量，扩展因果图。

Result: IRIS能够实时从仅有初始变量的数据中发现已知和新颖因果关系，无需预先存在的数据集，提升了因果发现的效率和范围。

Conclusion: IRIS框架有效弥补了传统和大模型因果发现方法的不足，实现了实时、自动且拓展性的因果关系发现，促进科学研究中因果推断的应用。

Abstract: Causal discovery is fundamental to scientific research, yet traditional
statistical algorithms face significant challenges, including expensive data
collection, redundant computation for known relations, and unrealistic
assumptions. While recent LLM-based methods excel at identifying commonly known
causal relations, they fail to uncover novel relations. We introduce IRIS
(Iterative Retrieval and Integrated System for Real-Time Causal Discovery), a
novel framework that addresses these limitations. Starting with a set of
initial variables, IRIS automatically collects relevant documents, extracts
variables, and uncovers causal relations. Our hybrid causal discovery method
combines statistical algorithms and LLM-based methods to discover known and
novel causal relations. In addition to causal discovery on initial variables,
the missing variable proposal component of IRIS identifies and incorporates
missing variables to expand the causal graphs. Our approach enables real-time
causal discovery from only a set of initial variables without requiring
pre-existing datasets.

</details>


### [69] [CrisiText: A dataset of warning messages for LLM training in emergency communication](https://arxiv.org/abs/2510.09243)
*Giacomo Gonella,Gian Maria Campedelli,Stefano Menini,Marco Guerini*

Main category: cs.CL

TL;DR: 本文提出CrisiText数据集，涵盖13种危机情景的40万条预警消息，用于生成及时预警信息。


<details>
  <summary>Details</summary>
Motivation: 当前AI辅助应急主要在分类任务上，忽视了NLG生成预警消息的潜力。

Method: 构建大规模预警消息数据集，采用专家指导生成消息，并设计实验比较多种NLG方法，包括监督微调、零样本、少样本与自动后编辑。

Result: 实验覆盖多种生成策略及非分布内场景，展示了不同方法的绩效和自动后编辑的有效性。

Conclusion: CrisiText为危机预警生成提供了丰富资源和基准，推进了NLG技术在紧急响应领域的应用研究。

Abstract: Effectively identifying threats and mitigating their potential damage during
crisis situations, such as natural disasters or violent attacks, is paramount
for safeguarding endangered individuals. To tackle these challenges, AI has
been used in assisting humans in emergency situations. Still, the use of NLP
techniques remains limited and mostly focuses on classification tasks. The
significant potential of timely warning message generation using NLG
architectures, however, has been largely overlooked. In this paper we present
CrisiText, the first large-scale dataset for the generation of warning messages
across 13 different types of crisis scenarios. The dataset contains more than
400,000 warning messages (spanning almost 18,000 crisis situations) aimed at
assisting civilians during and after such events. To generate the dataset, we
started from existing crisis descriptions and created chains of events related
to the scenarios. Each event was then paired with a warning message. The
generations follow experts' written guidelines to ensure correct terminology
and factuality of their suggestions. Additionally, each message is accompanied
by three suboptimal warning types to allow for the study of different NLG
approaches. To this end, we conducted a series of experiments comparing
supervised fine-tuning setups with preference alignment, zero-shot, and
few-shot approaches. We further assessed model performance in
out-of-distribution scenarios and evaluated the effectiveness of an automatic
post-editor.

</details>


### [70] [DSPO: Stable and Efficient Policy Optimization for Agentic Search and Reasoning](https://arxiv.org/abs/2510.09255)
*Chenyang Gu,Yewen Pu,Bruce Yang,Xiaofan Li,Huan Gao*

Main category: cs.CL

TL;DR: 提出DSPO算法，通过强化学习改进大型语言模型的多轮搜索与推理能力，显著提升复杂问答任务表现。


<details>
  <summary>Details</summary>
Motivation: 当前基于强化学习的方法在复杂交互任务中表现受限，难以充分挖掘LLMs的主动搜索潜力。

Method: 设计动态样本过滤的序列级策略优化算法（DSPO），训练过程中无监督示范数据，交替执行多轮搜索和推理。

Result: DSPO训练的7B模型在多项问答基准测试上较此前工作提升34.1%，在复杂多跳问答HotpotQA中相较14B模型性能提升近9%，且训练稳定。

Conclusion: DSPO有效增强了LLMs主动搜索和推理能力，为复杂实际任务提供更强大的智能体训练方法。

Abstract: Enhancing LLMs with the ability to actively search external knowledge is
crucial for complex and real-world tasks. Current approaches either rely on
prompting to elicit the model's innate agent capabilities, or suffer from
performance ceilings and collapse when applying RL to complex interactive
tasks, leaving their true agentic potential untapped. To address this, we
introduce \textbf{D}ynamic-filter \textbf{S}equence-level \textbf{P}olicy
\textbf{O}ptimization (DSPO), an improved RL algorithm designed for robust
agent training through sequence-level optimization and dynamic sample
filtering. We train our model purely through RL to interleave multi-turn search
and reasoning, obviating the need for supervised demonstration data. Across
multiple QA benchmarks, our DSPO-trained 7B model improves over a comparable
previous work by \textbf{34.1\%}, and even outperforms the 14B model from
previous work in complex multihop QA such as HotpotQA by nearly \textbf{9\%
relative}, maintaining exceptional training stability.

</details>


### [71] [Detecting Data Contamination from Reinforcement Learning Post-training for Large Language Models](https://arxiv.org/abs/2510.09259)
*Yongding Tao,Tian Wang,Yihong Dong,Huanyu Liu,Kechi Zhang,Xiaolong Hu,Ge Li*

Main category: cs.CL

TL;DR: 本文针对大型语言模型在强化学习后训练阶段数据污染检测的空白，提出了名为Self-Critique的新方法，显著提高检测准确率。


<details>
  <summary>Details</summary>
Motivation: 强化学习后训练阶段成为提升大型语言模型推理能力的关键环节，但目前缺乏专门针对该阶段的数据污染检测方法，造成评价可靠性风险。

Method: 提出Self-Critique方法，通过探测强化学习后输出熵分布的收敛特性和模型策略收敛到狭窄推理路径的现象，实现污染样本检测；同时设计RL-MIA基准模拟该场景。

Result: 在多个模型和污染任务上，Self-Critique相比基线方法AUC提升最多达30%，而现有方法在此阶段检测效果接近随机。

Conclusion: Self-Critique有效解决了强化学习后训练阶段数据污染检测难题，提升了对模型可靠性评价的保障。

Abstract: Data contamination poses a significant threat to the reliable evaluation of
Large Language Models (LLMs). This issue arises when benchmark samples may
inadvertently appear in training sets, compromising the validity of reported
performance. While detection methods have been developed for the pre-training
and Supervised Fine-Tuning stages, a critical research gap exists for the
increasingly significant phase of Reinforcement Learning (RL) post-training. As
RL post-training becomes pivotal for advancing LLM reasoning, the absence of
specialized contamination detection methods in this paradigm presents a
critical vulnerability. To address this, we conduct the first systematic study
of data detection within RL post-training scenario and propose Self-Critique.
Our method is motivated by a key observation: after RL phase, the output
entropy distribution of LLMs tends to collapse into highly specific and sparse
modes. Self-Critique probes for the underlying policy collapse, i.e., the
model's convergence to a narrow reasoning path, which causes this entropy
reduction. To facilitate this research, we also introduce RL-MIA, a benchmark
constructed to simulate this specific contamination scenario. Extensive
experiments show that Self-Critique significantly outperforms baseline methods
across multiple models and contamination tasks, achieving an AUC improvement of
up to 30%. Whereas existing methods are close to a random guess for RL-phase
contamination, our method makes detection possible.

</details>


### [72] [CFVBench: A Comprehensive Video Benchmark for Fine-grained Multimodal Retrieval-Augmented Generation](https://arxiv.org/abs/2510.09266)
*Kaiwen Wei,Xiao Liu,Jie Zhang,Zijian Wang,Ruida Liu,Yuming Yang,Xin Xiao,Xiao Sun,Haoyang Zeng,Changzai Pan,Yidan Zhang,Jiang Zhong,Peijin Wang,Yingchao Feng*

Main category: cs.CL

TL;DR: 本文提出了CFVBench，一个涵盖多模态和多格式的大规模视频问答基准，评测多模态大语言模型的检索与生成能力，并发现当前模型在细粒度多模态细节捕捉方面存在瓶颈，针对该问题提出了自适应视觉精炼（AVR）框架，有效提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有视频多模态检索增强生成（MRAG）基准覆盖模态和格式有限，难以全面评估多模态大语言模型（MLLMs）在细粒度和长时域视频理解上的能力。

Method: 构建包含599个视频和5,360开放式问答对的CFVBench基准，涵盖图表报告、新闻和软件教程等多种格式，系统评测多种检索方法和MLLMs，并提出自适应视觉精炼（AVR）策略，动态调整帧采样密度和调用外部工具。

Result: CFVBench评测显示当前多模态大语言模型（包括GPT5、Gemini）难以捕捉转瞬即逝的细粒度信息，AVR方法明显提升了模型在细粒度多模态理解上的表现并增强整体性能。

Conclusion: CFVBench为评测多模态视频理解提供了更全面和细致的平台，AVR框架有效缓解了细粒度信息处理瓶颈，推动多模态大语言模型向更精细和准确生成方向发展。

Abstract: Multimodal Retrieval-Augmented Generation (MRAG) enables Multimodal Large
Language Models (MLLMs) to generate responses with external multimodal
evidence, and numerous video-based MRAG benchmarks have been proposed to
evaluate model capabilities across retrieval and generation stages. However,
existing benchmarks remain limited in modality coverage and format diversity,
often focusing on single- or limited-modality tasks, or coarse-grained scene
understanding. To address these gaps, we introduce CFVBench, a large-scale,
manually verified benchmark constructed from 599 publicly available videos,
yielding 5,360 open-ended QA pairs. CFVBench spans high-density formats and
domains such as chart-heavy reports, news broadcasts, and software tutorials,
requiring models to retrieve and reason over long temporal video spans while
maintaining fine-grained multimodal information. Using CFVBench, we
systematically evaluate 7 retrieval methods and 14 widely-used MLLMs, revealing
a critical bottleneck: current models (even GPT5 or Gemini) struggle to capture
transient yet essential fine-grained multimodal details. To mitigate this, we
propose Adaptive Visual Refinement (AVR), a simple yet effective framework that
adaptively increases frame sampling density and selectively invokes external
tools when necessary. Experiments show that AVR consistently enhances
fine-grained multimodal comprehension and improves performance across all
evaluated MLLMs

</details>


### [73] [Inflated Excellence or True Performance? Rethinking Medical Diagnostic Benchmarks with Dynamic Evaluation](https://arxiv.org/abs/2510.09275)
*Xiangxu Zhang,Lei Li,Yanyun Zhou,Xiao Zhou,Yingying Zhang,Xian Wu*

Main category: cs.CL

TL;DR: 本文提出了一种针对医学诊断领域的动态评测基准DyReMe，旨在更真实地反映临床实际情景，并综合评估大语言模型（LLMs）的表现。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在医学诊断中的评估多依赖静态、公公开的考试题目，忽略了真实临床中的复杂、模糊和多变情况，导致模型表现被高估，缺乏可信度。

Method: DyReMe通过生成新鲜的病例咨询，加入干扰项如鉴别诊断和常见误诊因素，变换表达风格，模拟真实查询习惯，并在准确率基础上增加了真实性、帮助性和一致性三个临床相关维度的评测。

Result: 实验结果表明，DyReMe提供了更具挑战性和真实性的评测，暴露出当前先进大语言模型与真实临床实践之间存在显著脱节。

Conclusion: 研究强调了建立更符合临床诊断需求的评测框架的紧迫性，以提升大语言模型在医疗领域的可信赖性和实际应用价值。

Abstract: Medical diagnostics is a high-stakes and complex domain that is critical to
patient care. However, current evaluations of large language models (LLMs) are
fundamentally misaligned with real-world clinical practice. Most of them rely
on static benchmarks derived from public medical exam items, which tend to
overestimate model performance and ignore the difference between textbook cases
and the ambiguous, varying conditions in the real world. Recent efforts toward
dynamic evaluation offer a promising alternative, but their improvements are
limited to superficial perturbations and a narrow focus on accuracy. To address
these gaps, we propose DyReMe, a dynamic benchmark for medical diagnostics that
better reflects real clinical practice. Unlike static exam-style questions,
DyReMe generates fresh, consultation-like cases that introduce distractors such
as differential diagnoses and common misdiagnosis factors. It also varies
expression styles to mimic diverse real-world query habits. Beyond accuracy,
DyReMe evaluates LLMs on three additional clinically relevant dimensions:
veracity, helpfulness, and consistency. Our experiments demonstrate that this
dynamic approach yields more challenging and realistic assessments, revealing
significant misalignments between the performance of state-of-the-art LLMs and
real clinical practice. These findings highlight the urgent need for evaluation
frameworks that better reflect the demands of trustworthy medical diagnostics.

</details>


### [74] [CLARity: Reasoning Consistency Alone Can Teach Reinforced Experts](https://arxiv.org/abs/2510.09278)
*Jiuheng Lin,Cong Jiang,Zirui Wu,Jiarui Sun,Yansong Feng*

Main category: cs.CL

TL;DR: 本文提出了CLARity，一种低成本的强化学习框架，通过一致性奖励机制和动态数据重构策略提升了小规模语言模型的推理质量，显著改善了回答的一致性和准确性。


<details>
  <summary>Details</summary>
Motivation: 训练专业领域的大型语言模型面临数据稀缺问题，传统强化学习方法虽然能提升准确性，但往往降低推理的一致性和逻辑性，而现有的大规模推理监督方法成本高昂。

Method: 提出CLARity框架，结合一致性感知奖励机制、两阶段的精炼-监控训练流程及动态数据重构策略，以提升推理一致性并更好地利用有限数据。

Result: CLARity在实验中提升了响应一致性16.5%和准确率7.5%，并通过人工评估验证了回答在连贯性和专业性上的全面提升。

Conclusion: CLARity提供了一种通用且经济高效的方案，使得较小的模型能够有效引导专家模型提升推理一致性，实现更高质量的领域专家大模型训练。

Abstract: Training expert LLMs in domains with scarce data is difficult, often relying
on multiple-choice questions (MCQs). However, standard outcome-based
reinforcement learning (RL) on MCQs is risky. While it may improve accuracy, we
observe it often degrades reasoning quality such as logical consistency.
Existing solutions to supervise reasoning, such as large-scale Process Reward
Models (PRMs), are prohibitively expensive. To address this, we propose
CLARity, a cost-effective RL framework that enhances reasoning quality using
only a small, general-purpose LLM. CLARity integrates a consistency-aware
reward mechanism with a 2-stage refine-then-monitor training pipeline to
enhance reasoning consistency, and a dynamic data reformulation strategy to to
better exploit limited data. Experiments demonstrate that CLARity improves
response consistency by 16.5% and accuracy by 7.5% over baselines. Human
evaluations further confirm holistic improvements in coherence and
professionalism. Thus, CLARity offers a generalizable solution that enables
smaller models to effectively guide expert models by reasoning consistency.Our
code is open sourced at: https://github.com/Infinite-set/CLARity

</details>


### [75] [One Sentence, Two Embeddings: Contrastive Learning of Explicit and Implicit Semantic Representations](https://arxiv.org/abs/2510.09293)
*Kohei Oda,Po-Min Chuang,Kiyoaki Shirai,Natthawut Kertkeidkachorn*

Main category: cs.CL

TL;DR: 提出了一种名为DualCSE的句子嵌入方法，使用两个向量分别表示句子的显性和隐性语义，提升了语义表达能力和下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 传统句子嵌入方法只能为每个句子分配一个向量，难以捕捉句子中的隐含语义。

Method: DualCSE为每个句子分配两个嵌入向量，分别表示显性语义和隐性语义，这两个向量共存于共享空间中，可根据需求选择合适的语义表示。

Result: 实验结果表明，DualCSE能够有效编码显性和隐性语义，显著提升信息检索和文本分类等下游任务的表现。

Conclusion: DualCSE方法通过双向量嵌入解决了传统方法的局限，提高了句子隐含语义的捕捉能力及应用性能。

Abstract: Sentence embedding methods have made remarkable progress, yet they still
struggle to capture the implicit semantics within sentences. This can be
attributed to the inherent limitations of conventional sentence embedding
methods that assign only a single vector per sentence. To overcome this
limitation, we propose DualCSE, a sentence embedding method that assigns two
embeddings to each sentence: one representing the explicit semantics and the
other representing the implicit semantics. These embeddings coexist in the
shared space, enabling the selection of the desired semantics for specific
purposes such as information retrieval and text classification. Experimental
results demonstrate that DualCSE can effectively encode both explicit and
implicit meanings and improve the performance of the downstream task.

</details>


### [76] [MaP: A Unified Framework for Reliable Evaluation of Pre-training Dynamics](https://arxiv.org/abs/2510.09295)
*Jiapeng Wang,Changxin Tian,Kunlong Chen,Ziqi Liu,Jiaxin Mao,Wayne Xin Zhao,Zhiqiang Zhang,Jun Zhou*

Main category: cs.CL

TL;DR: 本文针对大规模语言模型训练过程中评估不稳定性问题，提出了结合模型权重合并与Pass@k指标的MaP框架以提升评估的稳定性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型预训练中的评估存在显著不稳定性，导致无法准确反映真实学习动态，阻碍模型性能的有效监控和比较。

Method: 提出MaP框架，结合两种策略：通过合并检查点（checkpoint merging）平滑参数空间，减小参数训练的随机性影响；利用Pass@k指标实现对模型能力的稳健低方差统计估计，减少评估过程中的噪声。

Result: 大量实验表明，MaP显著平滑了模型性能曲线，降低了不同训练运行间的方差，增强了模型排名的一致性。

Conclusion: MaP框架提高了大规模语言模型预训练评估的稳定性和可靠性，为观察模型训练动态提供了更加准确和稳健的工具，对LLM研究具有重要的经验基础意义。

Abstract: Reliable evaluation is fundamental to the progress of Large Language Models
(LLMs), yet the evaluation process during pre-training is plagued by
significant instability that obscures true learning dynamics. In this work, we
systematically diagnose this instability, attributing it to two distinct
sources: \textit{Parameter Instability} from training stochasticity and
\textit{Evaluation Instability} from noisy measurement protocols. To counteract
both sources of noise, we introduce \textbf{MaP}, a dual-pronged framework that
synergistically integrates checkpoint \underline{M}erging \underline{a}nd the
\underline{P}ass@k metric. Checkpoint merging smooths the parameter space by
averaging recent model weights, while Pass@k provides a robust, low-variance
statistical estimate of model capability. Extensive experiments show that MaP
yields significantly smoother performance curves, reduces inter-run variance,
and ensures more consistent model rankings. Ultimately, MaP provides a more
reliable and faithful lens for observing LLM training dynamics, laying a
crucial empirical foundation for LLM research.

</details>


### [77] [ShiZhi: A Chinese Lightweight Large Language Model for Court View Generation](https://arxiv.org/abs/2510.09297)
*Zhitian Hou,Kun Zeng*

Main category: cs.CL

TL;DR: 本文提出了ShiZhi，一种专门用于生成法律文书中“法院观点”部分的大型语言模型，并构建了包含11万余案例的中文法院观点生成数据集CCVG。


<details>
  <summary>Details</summary>
Motivation: 法院观点生成任务复杂且多样，直接从案件事实生成效果受限，因此需要专门设计模型和高质量数据以提高生成质量。

Method: 构建中文法院观点生成数据集CCVG，采用该数据集训练专门的法律领域大型语言模型ShiZhi，提升法院观点生成和指控预测性能。

Result: ShiZhi在法院观点生成任务上取得58.5 BLEU-1，指控预测准确率86.1%，宏F1达92.5%，实验表明即使是小型模型，经过领域特定高质量数据训练后也能生成合理且符合法律逻辑的法院观点。

Conclusion: 结合高质量法律领域数据训练的专用大型语言模型能够有效提升法律文书“法院观点”生成的质量与准确性，具有重要应用价值。

Abstract: Criminal Court View Generation (CVG) is a fundamental task in legal
artificial intelligence, aiming to automatically generate the "Court View"
section of a legal case document. Generating court views is challenging due to
the diversity and complexity of case facts, and directly generating from raw
facts may limit performance. In this paper, we present ShiZhi, the first large
language model (LLM) specifically designed for court view generation. We
construct a Chinese Court View Generation dataset, CCVG, of more than 110K
cases, each containing fact descriptions paired with corresponding court views.
Based on this dataset, ShiZhi achieving 58.5 BLEU-1 on court view generation
and 86.1\% accuracy with 92.5\% macro F1 on charge prediction. Experimental
results demonstrate that even a small LLM can generate reasonable and legally
coherent court views when trained on high-quality domain-specific data. Our
model and dataset are available at
\href{https://github.com/ZhitianHou/ShiZhi}{https://github.com/ZhitianHou/ShiZhi}.

</details>


### [78] [Mask Tokens as Prophet: Fine-Grained Cache Eviction for Efficient dLLM Inference](https://arxiv.org/abs/2510.09309)
*Jianuo Huang,Yaojie Zhang,Yicun Yang,Benhao Huang,Biqing Qi,Dongrui Liu,Linfeng Zhang*

Main category: cs.CL

TL;DR: 本文提出了MaskKV，一个针对扩散大语言模型(dLLMs)的训练免费缓存清理框架，通过利用掩码标记和注意力权重，实现缓存高效压缩，提高长上下文处理能力。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型虽支持并行解码但缓存机制内存占用大，且现有缓存清理策略不适用于dLLMs，限制了其长上下文处理能力和资源受限场景下的应用。

Method: MaskKV通过两大创新设计：1）基于掩码查询引导的评分机制，利用注意力权重选取并清理不重要的提示词缓存；2）自适应缓存预算策略，减少中间层资源分配，重点保障重要头部缓存，提高效率。

Result: 在LLaDA模型上，MaskKV将KV缓存压缩至仅256对（不足5%）时，仍能保留94%的性能，在32k长度提示下可实现最高31倍加速。

Conclusion: MaskKV有效解决了dLLMs缓存资源消耗问题，显著提升了长上下文处理效率，且无需训练，具有实际应用潜力。

Abstract: Diffusion large language models (dLLMs) present a promising alternative to
dominant autoregressive models (ARMs) by the ability of parallel decoding at
the expense of substantial computation and memory costs. Specifically, the
cache mechanism for bidirectional attention in dLLMs demands large memory
footprint, restricting their ability to handle long contexts under
resource-limited settings. Existing cache eviction strategies are designed for
ARMs and ignore the unique characteristics of dLLMs, thus leading to
unsatisfactory performance. To address these challenges, we introduce MaskKV, a
training-free cache eviction framework tailored to dLLMs, focusing on the
effect of mask tokens in dLLMs. MaskKV is built on two key innovations: (1) a
mask-query guided scoring mechanism that leverages attention weights to
identify and evict less critical prompt tokens for each head; (2) an adaptive
cache budgeting strategy that improves efficiency by reducing allocation in
intermediate layers and concentrating resources on prompt-preferring heads. On
LLaDA with MaskKV, compressing the KV cache to only 256 pairs (less than 5% of
tokens) retains 94% of the full-cache performance on LongBench and achieves up
to 31x acceleration at 32k prompt length. The code is publicly available at:
https://github.com/jianuo-huang/MaskKV

</details>


### [79] [Verifying Chain-of-Thought Reasoning via Its Computational Graph](https://arxiv.org/abs/2510.09312)
*Zheng Zhao,Yeskendir Koishekenov,Xianjun Yang,Naila Murray,Nicola Cancedda*

Main category: cs.CL

TL;DR: 本文提出了一种基于模型计算图的白盒推理验证方法（CRV），通过分析模型推理步骤的结构特征检测推理错误，并实现有针对性的错误校正。


<details>
  <summary>Details</summary>
Motivation: 现有的链式思维（CoT）推理验证多依赖输出或激活，缺乏对推理失败原因的深入理解。

Method: 引入基于电路的推理验证方法，通过构建和分析推理步骤的归因图（计算轨迹），训练分类器识别正确与错误推理的结构特征。

Result: 发现推理错误具有显著的结构指纹，且不同领域的错误表现为不同计算模式，基于这些结构签名实现推理错误的有效识别和针对性纠正。

Conclusion: 深入分析模型的计算过程能超越简单错误检测，实现对大型语言模型推理的因果理解和错误修正。

Abstract: Current Chain-of-Thought (CoT) verification methods predict reasoning
correctness based on outputs (black-box) or activations (gray-box), but offer
limited insight into why a computation fails. We introduce a white-box method:
Circuit-based Reasoning Verification (CRV). We hypothesize that attribution
graphs of correct CoT steps, viewed as execution traces of the model's latent
reasoning circuits, possess distinct structural fingerprints from those of
incorrect steps. By training a classifier on structural features of these
graphs, we show that these traces contain a powerful signal of reasoning
errors. Our white-box approach yields novel scientific insights unattainable by
other methods. (1) We demonstrate that structural signatures of error are
highly predictive, establishing the viability of verifying reasoning directly
via its computational graph. (2) We find these signatures to be highly
domain-specific, revealing that failures in different reasoning tasks manifest
as distinct computational patterns. (3) We provide evidence that these
signatures are not merely correlational; by using our analysis to guide
targeted interventions on individual transcoder features, we successfully
correct the model's faulty reasoning. Our work shows that, by scrutinizing a
model's computational process, we can move from simple error detection to a
deeper, causal understanding of LLM reasoning.

</details>


### [80] [FLRC: Fine-grained Low-Rank Compressor for Efficient LLM Inference](https://arxiv.org/abs/2510.09332)
*Yu-Chen Lu,Chong-Yan Chen,Chi-Chih Chang,Yu-Fang Hu,Kai-Chiang Wu*

Main category: cs.CL

TL;DR: 提出了细粒度低秩压缩器（FLRC），针对大语言模型在资源受限硬件上的部署问题，实现了每层最优秩分配和渐进式低秩解码，有效提升文本生成质量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型参数庞大，难以在资源受限硬件上部署，统一压缩比例导致性能下降且解码效果差。

Method: 提出FLRC方法，通过精细层级秩分配和渐进低秩解码策略，优化压缩过程。

Result: 在多项基准测试中，FLRC在摘要任务上ROUGE-L指标最高提升17%，优于现有低秩压缩方法。

Conclusion: FLRC为大语言模型推理提供了更高效且鲁棒的低秩压缩框架，适合资源受限环境部署。

Abstract: Although large language models (LLM) have achieved remarkable performance,
their enormous parameter counts hinder deployment on resource-constrained
hardware. Low-rank compression can reduce both memory usage and computational
demand, but applying a uniform compression ratio across all layers often leads
to significant performance degradation, and previous methods perform poorly
during decoding. To address these issues, we propose the Fine-grained Low-Rank
Compressor (FLRC), which efficiently determines an optimal rank allocation for
each layer, and incorporates progressive low-rank decoding to maintain text
generation quality. Comprehensive experiments on diverse benchmarks demonstrate
the superiority of FLRC, achieving up to a 17% improvement in ROUGE-L on
summarization tasks compared to state-of-the-art low-rank compression methods,
establishing a more robust and efficient framework to improve LLM inference.

</details>


### [81] [LLP: LLM-based Product Pricing in E-commerce](https://arxiv.org/abs/2510.09347)
*Hairu Wang,Sheng You,Qiheng Zhang,Xike Xie,Shuguang Han,Yuchen Wu,Fei Huang,Jufeng Chen*

Main category: cs.CL

TL;DR: 论文提出了基于大型语言模型（LLM）的二手商品定价生成框架LLP，通过检索相似商品和细粒度文本理解，实现动态市场价格预测，并通过两阶段优化和置信过滤机制提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 传统二手商品定价方法多基于静态回归模型，泛化能力差，无法动态捕捉市场价格变化，导致定价效率低下。

Method: LLP利用LLM结合相似商品检索，生成价格建议。采用监督微调和组相对策略优化两阶段训练并构建双向推理数据集，辅以置信度过滤排除不可靠预测。

Result: 实验表明，LLP在未见类别上表现优越，显著超越现有方法。实际部署于中国最大二手电商平台闲鱼后，静态采纳率由40%提升至72%，高召回下仍保持47%。

Conclusion: 基于LLM的生成式定价框架LLP有效解决了二手市场价格预测难题，展现出良好的泛化能力和实际应用价值。

Abstract: Unlike Business-to-Consumer e-commerce platforms (e.g., Amazon),
inexperienced individual sellers on Consumer-to-Consumer platforms (e.g., eBay)
often face significant challenges in setting prices for their second-hand
products efficiently. Therefore, numerous studies have been proposed for
automating price prediction. However, most of them are based on static
regression models, which suffer from poor generalization performance and fail
to capture market dynamics (e.g., the price of a used iPhone decreases over
time). Inspired by recent breakthroughs in Large Language Models (LLMs), we
introduce LLP, the first LLM-based generative framework for second-hand product
pricing. LLP first retrieves similar products to better align with the dynamic
market change. Afterwards, it leverages the LLMs' nuanced understanding of key
pricing information in free-form text to generate accurate price suggestions.
To strengthen the LLMs' domain reasoning over retrieved products, we apply a
two-stage optimization, supervised fine-tuning (SFT) followed by group relative
policy optimization (GRPO), on a dataset built via bidirectional reasoning.
Moreover, LLP employs a confidence-based filtering mechanism to reject
unreliable price suggestions. Extensive experiments demonstrate that LLP
substantially surpasses existing methods while generalizing well to unseen
categories. We have successfully deployed LLP on Xianyu\footnote\{Xianyu is
China's largest second-hand e-commerce platform.\}, significantly outperforming
the previous pricing method. Under the same 30\% product coverage, it raises
the static adoption rate (SAR) from 40\% to 72\%, and maintains a strong SAR of
47\% even at 90\% recall.

</details>


### [82] [ReTraceQA: Evaluating Reasoning Traces of Small Language Models in Commonsense Question Answering](https://arxiv.org/abs/2510.09351)
*Francesco Maria Molfese,Luca Moroni,Ciro Porcaro,Simone Conia,Roberto Navigli*

Main category: cs.CL

TL;DR: 本文提出了ReTraceQA基准，通过过程层面的评估揭示当前小型语言模型(SLMs)在常识推理中存在推理过程错误的问题，强调只看最终答案准确率导致能力被高估。


<details>
  <summary>Details</summary>
Motivation: 现有常识推理评估仅关注最终答案准确性，忽视了推理过程的合理性，导致对SLMs能力的过高估计。

Method: 设计ReTraceQA基准，引入专家标注的过程层面评估，利用强大的大型语言模型(LLMs)作为自动判定器，评估SLMs的推理过程合理性。

Result: 发现14-24%的样本中SLMs给出了正确答案但推理过程存在错误；采用过程层面评估后，SLM表现下降最多达25%。

Conclusion: 仅依赖最终答案准确率评价SLMs能力存在偏差，过程层面评估更能客观反映模型推理能力，促进更严谨的常识推理模型发展。

Abstract: While Small Language Models (SLMs) have demonstrated promising performance on
an increasingly wide array of commonsense reasoning benchmarks, current
evaluation practices rely almost exclusively on the accuracy of their final
answers, neglecting the validity of the reasoning processes that lead to those
answers. To address this issue, we introduce ReTraceQA, a novel benchmark that
introduces process-level evaluation for commonsense reasoning tasks. Our
expert-annotated dataset reveals that in a substantial portion of instances
(14-24%), SLMs provide correct final answers despite flawed reasoning
processes, suggesting that the capabilities of SLMs are often overestimated by
evaluation metrics that focus only on comparing the final answer with the
ground truth. Indeed, we show that when employing strong Large Language Models
(LLMs) as automated judges for reasoning-aware evaluation rather than
answer-only metrics, SLM performance drops significantly across all models and
datasets, with scores decreasing by up to 25%.

</details>


### [83] [Logit Arithmetic Elicits Long Reasoning Capabilities Without Training](https://arxiv.org/abs/2510.09354)
*Yunxiang Zhang,Muhammad Khalifa,Lechen Zhang,Xin Liu,Ayoung Lee,Xinliang Frederick Zhang,Farima Fatahi Bayat,Lu Wang*

Main category: cs.CL

TL;DR: 本文提出了一种无需额外训练即可激发大型模型长链推理能力的解码方法ThinkLogit，及其基于偏好优化的升级版本ThinkLogit-DPO，显著提升了推理准确率。


<details>
  <summary>Details</summary>
Motivation: 当前研究表明大型模型的长链推理能力通常需额外训练，但该文旨在探索无需训练即可实现该能力的可行方法。

Method: 提出了基于logit算术的解码时调整方法ThinkLogit，利用较小的推理模型作为引导；并通过对正确/错误推理对进行偏好优化训练引导模型，形成ThinkLogit-DPO。

Result: 在五个推理基准上，ThinkLogit和ThinkLogit-DPO相较于原模型分别提升了24.5%和29.1%的平均准确率；且该方法对不同模型家族间的结合同样有效。

Conclusion: ThinkLogit提供了一种无需昂贵后训练即可解锁大型模型长链推理的新路径，且可与其他后训练方法协同提升，具有实际应用价值。

Abstract: Large reasoning models exhibit long chain-of-thought reasoning with
strategies such as backtracking and self-correction, though recent studies
suggest that these abilities typically require additional training. We first
investigate whether such behaviors can be elicited without any training. To
this end, we propose a decoding-time approach, ThinkLogit, which utilizes logit
arithmetic to tune a target large non-reasoning model for long reasoning using
a substantially smaller reasoning model as the guider. We then show that we can
further boost its performance by training the guider model with preference
optimization over correct/incorrect reasoning pairs sampled from both the
target and guider model, a setup we refer to as ThinkLogit-DPO. Our experiments
demonstrate that ThinkLogit and ThinkLogit-DPO achieve a relative improvement
in average accuracy by 24.5% and 29.1%, respectively, over five reasoning
benchmarks using the Qwen2.5-32B guided by R1-Distill-Qwen-1.5B, a model 21x
smaller. Moreover, we find that ThinkLogit remains effective when the guider
and target come from different model families. It is also orthogonal to
post-training methods for small models, as guiders improved through supervised
distillation or reinforcement learning can be directly plugged in to yield
stronger large models, offering a practical path to unlock long reasoning in
large-scale models without costly post-training.

</details>


### [84] [NL2GenSym: Natural Language to Generative Symbolic Rules for SOAR Cognitive Architecture via Large Language Models](https://arxiv.org/abs/2510.09355)
*Fang Yuan,Junjie Zeng,Yue Hu,Zhengqiu Zhu,Quanjun Yin,Yuxiang Xie*

Main category: cs.CL

TL;DR: 本文提出NL2GenSym框架，结合大语言模型与SOAR架构，实现从自然语言自动生成符号规则，并通过执行反馈机制优化规则。


<details>
  <summary>Details</summary>
Motivation: 传统SOAR符号认知架构手工编码规则劳动强度大，限制了其实用性。当前利用大语言模型生成规则的研究多停留在理论阶段，缺乏实证验证。

Method: 提出NL2GenSym框架，设计执行反馈的Generator-Critic机制，大语言模型生成规则后即时在SOAR环境中执行验证，并利用反馈驱动规则反复迭代优化。

Result: 在水壶问题数据集上进行实验，框架成功率超过86%，生成了新颖启发式规则，显著减少了问题解决的决策周期，并且小参数模型表现优于大参数模型。

Conclusion: NL2GenSym框架有效弥补了符号架构规则生成的难题，实现了自然语言到符号规则的自动转换和优化，推动符号智能与大语言模型的结合应用。

Abstract: SOAR, a classic symbol-based cognitive architecture, has been fostering the
development of general, human-like intelligent agents. Nevertheless, its
practical adoption is hindered by the laborious manual rule coding. Emerging
Large Language Models (LLMs) present the immense potential for efficient rules
generation. However, there is a critical gap that current research
predominantly focuses on conceptual frameworks and lacks robust experimental
validation. To bridge this gap, we propose \textit{N}atural \textit{L}anguage
to \textit{Gen}erative \textit{Sym}bolic Rules (NL2GenSym), a novel framework
that integrates LLMs with SOAR to autonomously produce generative symbolic
rules from natural language. Specifically, our framework introduces a novel
Execution-Grounded Generator-Critic mechanism. The LLM-based Generator, guided
by a Retrieval-Augmented Generation-accessed self-evolving domain knowledge
base, proposes rules from natural language. Subsequently, these rules are
immediately executed within the SOAR environment to rigorously validate their
correctness. Based on this execution-grounded feedback, a reflective LLM-based
Critic drives the iterative refinement of these rules. Experiments on our
specialized Water Jug Problem (WJP) dataset, utilizing both Gemini and Qwen
series models, validate the efficacy of our framework. It achieves a success
rate over 86\% in generating rules from natural language. Crucially, the
framework also generates novel heuristic rules, reducing average decision
cycles for solving the WJP to 1.98 times the optimal solution and 1/1000 of
baseline methods. Additionally, our initial experiments show that NL2GenSym
enables smaller-parameter models to achieve better performance than larger
counterparts.

</details>


### [85] [Understanding the Effects of Domain Finetuning on LLMs](https://arxiv.org/abs/2510.09359)
*Eshaan Tanwar,Deepak Nathani,William Yang Wang,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 本文首次系统研究了大规模医学领域语言模型的领域专属微调机制，发现微调仅改变了表示空间中的小部分子空间，提出了调优向量框架解释参数调整方向，并证明其对提升模型指令遵从性和生成质量的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管特定领域微调的大型语言模型表现优异，但其微调过程如何重塑参数空间仍缺乏深入理解，特别是针对领域专属模型的研究较少。

Method: 本文通过系统分析，提出调优向量框架，捕捉微调所引起的参数方向性变化，进一步研究这些向量在不同领域间的组合效果及其在模型不同层（MLP与注意力头）中的表现。

Result: 研究发现微调主要影响表示空间中的少部分子空间，调优向量能够提升指令遵从和生成质量，组合不同领域的调优向量能够增强模型的泛化能力；同时在模型内部，调优向量在MLP层写入新方向信息，在注意力头中放大已有方向。

Conclusion: 此研究为理解大型语言模型的领域专属适应机制提供了新视角，也提出了一个通用且可解释的分析框架，有助于未来领域微调及模型特化的深入研究。

Abstract: Large Language Models (LLMs) fine-tuned for specific domains exhibit strong
performance; however, the underlying mechanisms by which this fine-tuning
reshapes their parametric space are not well understood. Prior works primarily
focus on auto-regressive or general-purpose instruct models, leaving
domain-specialised LLMs under-explored. We present the first systematic study
of domain-specific fine-tuning in large medical language models. Our analysis
reveals that fine-tuning modifies only a small subset of the representational
subspace, essentially preserving the pre-trained model's representation. To
interpret these changes in subspaces, we propose tuning vectors, a novel
framework inspired by task vectors, which explicitly capture the directional
parameter shifts induced by fine-tuning. We demonstrate that these vectors are
critical for enhancing both instruction-following and generation quality.
Furthermore, combining tuning vectors across different domains yields improved
generalisation. Upon closer inspection of directional alignment, we find these
vectors primarily write new directional information into the MLP layers of the
model, while amplifying existing directions in attention heads. Our findings
offer new insights into LLM adaptation and provide a general, interpretable
framework for analysing specialisation in large language models.

</details>


### [86] [Token-Level Policy Optimization: Linking Group-Level Rewards to Token-Level Aggregation via Markov Likelihood](https://arxiv.org/abs/2510.09369)
*Xingyu Lin,Yilin Wen,En Wang,Du Su,Wenbin Liu,Chenfu Bao,Zhonghou Lv*

Main category: cs.CL

TL;DR: 本文提出TEPO框架，通过将组级奖励与token级别聚合结合，提升了大模型数学推理能力和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO及相关方法在解决稀疏token奖励导致的熵坍缩或模型坍缩问题上存在不足。

Method: 提出TEPO框架，利用马尔可夫似然将组级奖励和token级奖励算法结合，进行token级别的聚合优化。

Result: TEPO在数学推理任务上取得了领先性能，并且提升了训练稳定性。

Conclusion: TEPO有效解决了基于链式思维的稀疏token奖励难题，显著提升模型表现和训练稳定性。

Abstract: Group Relative Policy Optimization (GRPO) has significantly advanced the
reasoning ability of large language models (LLMs), particularly by boosting
their mathematical performance. However, GRPO and related
entropy-regularization methods still face challenges rooted in the sparse token
rewards inherent to chain-of-thought (CoT). Current approaches often rely on
undifferentiated token-level entropy adjustments, which frequently lead to
entropy collapse or model collapse. In this work, we propose TEPO, a novel
token-level framework that incorporates Markov Likelihood (sequence likelihood)
links group-level rewards with tokens via token-level aggregation. Experiments
show that TEPO consistently outperforms existing baselines across key metrics
(including @k and accuracy). It not only sets a new state of the art on
mathematical reasoning tasks but also significantly enhances training
stability.

</details>


### [87] [Beyond Single-Granularity Prompts: A Multi-Scale Chain-of-Thought Prompt Learning for Graph](https://arxiv.org/abs/2510.09394)
*Ziyu Zheng,Yaming Yang,Ziyu Guan,Wei Zhao,Xinyan Huang,Weigang Lu*

Main category: cs.CL

TL;DR: 本文提出了一种多尺度图链式思维提示框架（MSGCOT），通过捕捉并动态集成多尺度图结构信息，提升图提示调优的效果，特别是在少样本场景下表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有图提示调优方法仅关注单一粒度的结构信息，忽视了图数据中固有的多尺度结构，限制了提示语义的多样性和表达能力。

Method: 设计了一个轻量级低秩粗化网络，提取多尺度层次结构特征作为提示向量基底；模拟人类从粗到细的认知过程，在每一步推理动态融合多尺度信息，形成逐步细化的提示链。

Result: 在八个基准数据集上的大量实验表明，MSGCOT在性能上优于现有单一粒度的图提示调优方法，尤其在少样本学习场景中表现更佳。

Conclusion: 引入多尺度结构信息并通过链式思维机制逐步融合，显著提升了图提示调优方法的效果，拓展了该领域的性能边界。

Abstract: The "pre-train, prompt'' paradigm, designed to bridge the gap between
pre-training tasks and downstream objectives, has been extended from the NLP
domain to the graph domain and has achieved remarkable progress. Current
mainstream graph prompt-tuning methods modify input or output features using
learnable prompt vectors. However, existing approaches are confined to
single-granularity (e.g., node-level or subgraph-level) during prompt
generation, overlooking the inherently multi-scale structural information in
graph data, which limits the diversity of prompt semantics. To address this
issue, we pioneer the integration of multi-scale information into graph prompt
and propose a Multi-Scale Graph Chain-of-Thought (MSGCOT) prompting framework.
Specifically, we design a lightweight, low-rank coarsening network to
efficiently capture multi-scale structural features as hierarchical basis
vectors for prompt generation. Subsequently, mimicking human cognition from
coarse-to-fine granularity, we dynamically integrate multi-scale information at
each reasoning step, forming a progressive coarse-to-fine prompt chain.
Extensive experiments on eight benchmark datasets demonstrate that MSGCOT
outperforms the state-of-the-art single-granularity graph prompt-tuning method,
particularly in few-shot scenarios, showcasing superior performance.

</details>


### [88] [Active Model Selection for Large Language Models](https://arxiv.org/abs/2510.09418)
*Yavuz Durmazkeser,Patrik Okanovic,Andreas Kirsch,Torsten Hoefler,Nezihe Merve Gürel*

Main category: cs.CL

TL;DR: LLM SELECTOR是一种用于大语言模型主动选择的框架，能够在有限标注下高效识别最佳模型，显著降低标注成本。


<details>
  <summary>Details</summary>
Motivation: 现有评测和基准测试方法依赖完全注释数据，成本高昂且效率低。

Method: LLM SELECTOR通过自适应选择最具信息量的少量查询进行标注，并采用基于裁判的oracle注释模型以进一步减少标注成本。

Result: 在6个基准测试和151个LLM上，LLM SELECTOR在选择最佳及近最佳模型时，标注成本降低最多达59.62%。

Conclusion: LLM SELECTOR实现了在有限标注数据下高效且经济的最佳大语言模型选择，提升了模型评估的实用价值。

Abstract: We introduce LLM SELECTOR, the first framework for active model selection of
Large Language Models (LLMs). Unlike prior evaluation and benchmarking
approaches that rely on fully annotated datasets, LLM SELECTOR efficiently
identifies the best LLM with limited annotations. In particular, for any given
task, LLM SELECTOR adaptively selects a small set of queries to annotate that
are most informative about the best model for the task. To further reduce
annotation cost, we leverage a judge-based oracle annotation model. Through
extensive experiments on 6 benchmarks with 151 LLMs, we show that LLM SELECTOR
reduces annotation costs by up to 59.62% when selecting the best and near-best
LLM for the task.

</details>


### [89] [On the Representations of Entities in Auto-regressive Large Language Models](https://arxiv.org/abs/2510.09421)
*Victor Morand,Josiane Mothe,Benjamin Piwowarski*

Main category: cs.CL

TL;DR: 本文提出实体提及重建框架，探究大语言模型（LLMs）如何编码和操作实体以及其内部的实体表示方法。


<details>
  <summary>Details</summary>
Motivation: 虽然实体是文本知识的基本构件，但大语言模型内部如何表示实体尚不清楚，现有研究多侧重显性关系，缺乏对实体表示本身的深入理解。

Method: 提出基于任务向量的实体提及重建方法，生成多词实体提及；引入实体透镜（Entity Lens），扩展logit-lens以支持多词实体预测。

Result: 方法能稳定地从LLM隐藏状态的多种实体表示中生成多词实体提及，且证明LLMs具备开发针对任何多词实体的专用表示机制，包括训练中未见实体。

Conclusion: LLMs内部存在实体专用机制，有效编码和操作多词实体，本文提出的实体透镜为解析和利用实体表示提供了新工具。

Abstract: Named entities are fundamental building blocks of knowledge in text,
grounding factual information and structuring relationships within language.
Despite their importance, it remains unclear how Large Language Models (LLMs)
internally represent entities. Prior research has primarily examined explicit
relationships, but little is known about entity representations themselves. We
introduce entity mention reconstruction as a novel framework for studying how
LLMs encode and manipulate entities. We investigate whether entity mentions can
be generated from internal representations, how multi-token entities are
encoded beyond last-token embeddings, and whether these representations capture
relational knowledge. Our proposed method, leveraging _task vectors_, allows to
consistently generate multi-token mentions from various entity representations
derived from the LLMs hidden states. We thus introduce the _Entity Lens_,
extending the _logit-lens_ to predict multi-token mentions. Our results bring
new evidence that LLMs develop entity-specific mechanisms to represent and
manipulate any multi-token entities, including those unseen during training.
Our code is avalable at https://github.com/VictorMorand/EntityRepresentations .

</details>


### [90] [The Speech-LLM Takes It All: A Truly Fully End-to-End Spoken Dialogue State Tracking Approach](https://arxiv.org/abs/2510.09424)
*Nizar El Ghazal,Antoine Caubrière,Valentin Vielzeuf*

Main category: cs.CL

TL;DR: 本文比较了使用Speech-LLMs进行端到端语音对话状态跟踪的上下文管理策略，发现完整语音历史输入效果最佳，注意力池化压缩方法在减少上下文大小的同时保持竞争力精度。


<details>
  <summary>Details</summary>
Motivation: 探索如何有效管理和利用语音对话中的上下文信息，以提升端到端语音对话状态跟踪性能。

Method: 系统比较传统多模态上下文（文本历史+当前语音）、完整语音历史输入以及基于注意力池化的压缩语音历史等三种上下文管理策略。

Result: 在SpokenWOZ语料库上，完整语音历史输入取得了最佳性能，显著优于同等规模的既有方法；压缩的语音历史输入在降低上下文规模的同时仍保持了较高的准确率。

Conclusion: 通过更有效的上下文利用，完整语音历史和基于注意力池化的压缩方法均能提升语音对话状态跟踪效果，为上下文管理提供了有针对性的解决方案。

Abstract: This paper presents a comparative study of context management strategies for
end-to-end Spoken Dialog State Tracking using Speech-LLMs. We systematically
evaluate traditional multimodal context (combining text history and spoken
current turn), full spoken history, and compressed spoken history approaches.
Our experiments on the SpokenWOZ corpus demonstrate that providing the full
spoken conversation as input yields the highest performance among models of
similar size, significantly surpassing prior methods. Furthermore, we show that
attention-pooling-based compression of the spoken history offers a strong
trade-off, maintaining competitive accuracy with reduced context size. Detailed
analysis confirms that improvements stem from more effective context
utilization.

</details>


### [91] [KORMo: Korean Open Reasoning Model for Everyone](https://arxiv.org/abs/2510.09426)
*Minjun Kim,Hyeonseok Lim,Hangyeol Yoo,Inho Won,Seungwoo Song,Minkyung Cho,Junhun Yuk,Changsu Choi,Dongjae Shin,Huige Lee,Hoyun Song,Alice Oh,Kyungtae Lim*

Main category: cs.CL

TL;DR: 本论文首次大规模探讨了基于合成数据构建完全开放的韩英双语大型语言模型KORMo-10B，证明合成数据在保持语言覆盖和指令风格多样性的情况下可支持模型稳定训练，并达到了与当前多语言基线相当的性能。


<details>
  <summary>Details</summary>
Motivation: 针对非英语语言缺乏开源大规模语言模型的问题，探索利用大量合成数据训练高性能韩英双语大语言模型。

Method: 从零开始训练一个108亿参数的韩英双语模型KORMo-10B，68.74%的韩语数据为精心筛选的合成数据，结合指令调优实现语言能力提升。

Result: 模型在推理、知识和指令遵循等多项基准测试中表现良好，验证了合成数据不会引起训练不稳定或性能退化，并且双语指令调优显著提升了韩语推理和语篇连贯性。

Conclusion: 本研究展示了合成数据支持的低资源语言大模型训练的可行性，并通过公开数据和代码，建立了透明且可复现的研究框架，为未来多语言大模型研究提供了借鉴。

Abstract: This work presents the first large-scale investigation into constructing a
fully open bilingual large language model (LLM) for a non-English language,
specifically Korean, trained predominantly on synthetic data. We introduce
KORMo-10B, a 10.8B-parameter model trained from scratch on a Korean-English
corpus in which 68.74% of the Korean portion is synthetic. Through systematic
experimentation, we demonstrate that synthetic data, when carefully curated
with balanced linguistic coverage and diverse instruction styles, does not
cause instability or degradation during large-scale pretraining. Furthermore,
the model achieves performance comparable to that of contemporary open-weight
multilingual baselines across a wide range of reasoning, knowledge, and
instruction-following benchmarks. Our experiments reveal two key findings: (1)
synthetic data can reliably sustain long-horizon pretraining without model
collapse, and (2) bilingual instruction tuning enables near-native reasoning
and discourse coherence in Korean. By fully releasing all components including
data, code, training recipes, and logs, this work establishes a transparent
framework for developing synthetic data-driven fully open models (FOMs) in
low-resource settings and sets a reproducible precedent for future multilingual
LLM research.

</details>


### [92] [Domain-Adapted Pre-trained Language Models for Implicit Information Extraction in Crash Narratives](https://arxiv.org/abs/2510.09434)
*Xixi Wang,Jordanka Kovaceva,Miguel Costa,Shuai Wang,Francisco Camara Pereira,Robert Thomson*

Main category: cs.CL

TL;DR: 本文研究了基于紧凑开源预训练语言模型（PLMs）对交通事故自由文本叙述进行推理密集型信息抽取的方法，采用LoRA和BERT微调技术实现领域知识注入。


<details>
  <summary>Details</summary>
Motivation: 真实交通事故自由文本记录内容非结构化、非标准化且作者水平参差不齐，且现有基于闭源大型语言模型的工具在隐私保护和推理任务上存在不足，限制了对大规模事故数据的高效分析。

Method: 通过对紧凑的开源预训练语言模型进行微调，使用低秩适配（LoRA）和BERT注入任务特定知识，针对碰撞方式识别和车辆碰撞类型识别这两大推理密集任务进行模型训练。

Result: 在真实权威数据集Crash Investigation Sampling System（CISS）上，微调后的紧凑模型在识别事故碰撞方式和车辆碰撞类型任务中优于强大的闭源LLM（如GPT-4o），且训练资源需求较低。

Conclusion: 微调后的开源PLMs不仅能实现推理密集型事故文本信息的准确抽取，还能捕捉更丰富的叙述细节，甚至修正数据集中部分错误标注，展现出应用于交通事故文本分析的潜力和优势。

Abstract: Free-text crash narratives recorded in real-world crash databases have been
shown to play a significant role in improving traffic safety. However,
large-scale analyses remain difficult to implement as there are no documented
tools that can batch process the unstructured, non standardized text content
written by various authors with diverse experience and attention to detail. In
recent years, Transformer-based pre-trained language models (PLMs), such as
Bidirectional Encoder Representations from Transformers (BERT) and large
language models (LLMs), have demonstrated strong capabilities across various
natural language processing tasks. These models can extract explicit facts from
crash narratives, but their performance declines on inference-heavy tasks in,
for example, Crash Type identification, which can involve nearly 100
categories. Moreover, relying on closed LLMs through external APIs raises
privacy concerns for sensitive crash data. Additionally, these black-box tools
often underperform due to limited domain knowledge. Motivated by these
challenges, we study whether compact open-source PLMs can support
reasoning-intensive extraction from crash narratives. We target two challenging
objectives: 1) identifying the Manner of Collision for a crash, and 2) Crash
Type for each vehicle involved in the crash event from real-world crash
narratives. To bridge domain gaps, we apply fine-tuning techniques to inject
task-specific knowledge to LLMs with Low-Rank Adaption (LoRA) and BERT.
Experiments on the authoritative real-world dataset Crash Investigation
Sampling System (CISS) demonstrate that our fine-tuned compact models
outperform strong closed LLMs, such as GPT-4o, while requiring only minimal
training resources. Further analysis reveals that the fine-tuned PLMs can
capture richer narrative details and even correct some mislabeled annotations
in the dataset.

</details>


### [93] [Getting Your Indices in a Row: Full-Text Search for LLM Training Data for Real World](https://arxiv.org/abs/2510.09471)
*Ines Altemir Marinas,Anastasiia Kucherenko,Alexander Sternfeld,Andrei Kucharavy*

Main category: cs.CL

TL;DR: 本文介绍了使用Elasticsearch和高效的arm64超算集群对大规模开放权重LLM训练数据进行全文本索引的管道，成功索引了Apertus LLM训练数据中的8.6万亿个token，实现了离线的开放网络搜索引擎和LLM安全工具。


<details>
  <summary>Details</summary>
Motivation: 尽管开放权重LLM增多，但训练数据存取受限且规模庞大难以解析，且其中可能包含关键的互联网抓取数据，迫切需要有效工具进行大规模数据索引和安全性保障。

Method: 使用Elasticsearch并行索引技术，结合高能效的arm64超算集群Alps基础设施，实现对15.2万亿tokens中8.6万亿tokens的数据进行全文本索引，建立可搜索的离线索引库。

Result: 成功将Elasticsearch移植至新一代arm64基础设施，实现了现代LLM训练数据规模及全网规模的全文本索引，创建关键的LLM安全工具，提升数据访问能力，降低计算资源消耗。

Conclusion: 证明了大规模全文索引在开放LLM训练数据上的可行性和高效性，为LLM安全提供新工具，推动通向绿色计算的转型，期待该方法助力更多大规模数据索引项目。

Abstract: The performance of Large Language Models (LLMs) is determined by their
training data. Despite the proliferation of open-weight LLMs, access to LLM
training data has remained limited. Even for fully open LLMs, the scale of the
data makes it all but inscrutable to the general scientific community, despite
potentially containing critical data scraped from the internet.
  In this paper, we present the full-text indexing pipeline for the Apertus LLM
training data. Leveraging Elasticsearch parallel indices and the Alps
infrastructure, a state-of-the-art, highly energy-efficient arm64 supercluster,
we were able to index 8.6T tokens out of 15.2T used to train the Apertus LLM
family, creating both a critical LLM safety tool and effectively an offline,
curated, open web search engine. Our contribution is threefold. First, we
demonstrate that Elasticsearch can be successfully ported onto next-generation
arm64-based infrastructure. Second, we demonstrate that full-text indexing at
the scale of modern LLM training datasets and the entire open web is feasible
and accessible. Finally, we demonstrate that such indices can be used to ensure
previously inaccessible jailbreak-agnostic LLM safety.
  We hope that our findings will be useful to other teams attempting
large-scale data indexing and facilitate the general transition towards greener
computation.

</details>


### [94] [Hybrid Models for Natural Language Reasoning: The Case of Syllogistic Logic](https://arxiv.org/abs/2510.09472)
*Manuel Vargas Guzmán,Jakub Szymanik,Maciej Malicki*

Main category: cs.CL

TL;DR: 本文研究了大型预训练语言模型在逻辑推理中的泛化能力，发现其对递归推理较为擅长，但组合逻辑能力不足。提出结合符号推理与神经计算的混合架构，提升逻辑推理的可靠性与效率。


<details>
  <summary>Details</summary>
Motivation: 目前神经模型在泛化能力方面存在挑战，尤其是在逻辑推理的组合性和递归性两个基本方面没有被明确区分，影响其应用效果。

Method: 以三段论为基准测试自然语言推理中的逻辑泛化能力，发现大模型对递归能力表现良好，但组合能力较弱。提出将符号推理与神经计算相结合的混合模型以增强推理能力。

Result: 实验表明混合模型在保证推理完整性的同时，维持较高效率，且即使神经组件较小也能实现良好性能。

Conclusion: 混合模型方法有效解决了神经推理系统中组合性泛化难题，展现出提升逻辑推理泛化能力的潜力。

Abstract: Despite the remarkable progress in neural models, their ability to
generalize, a cornerstone for applications like logical reasoning, remains a
critical challenge. We delineate two fundamental aspects of this ability:
compositionality, the capacity to abstract atomic logical rules underlying
complex inferences, and recursiveness, the aptitude to build intricate
representations through iterative application of inference rules. In the
literature, these two aspects are often confounded together under the umbrella
term of generalization. To sharpen this distinction, we investigated the
logical generalization capabilities of pre-trained large language models (LLMs)
using the syllogistic fragment as a benchmark for natural language reasoning.
Though simple, this fragment provides a foundational yet expressive subset of
formal logic that supports controlled evaluation of essential reasoning
abilities. Our findings reveal a significant disparity: while LLMs demonstrate
reasonable proficiency in recursiveness, they struggle with compositionality.
To overcome these limitations and establish a reliable logical prover, we
propose a hybrid architecture integrating symbolic reasoning with neural
computation. This synergistic interaction enables robust and efficient
inference, neural components accelerate processing, while symbolic reasoning
ensures completeness. Our experiments show that high efficiency is preserved
even with relatively small neural components. As part of our proposed
methodology, this analysis gives a rationale and highlights the potential of
hybrid models to effectively address key generalization barriers in neural
reasoning systems.

</details>


### [95] [Multimodal Policy Internalization for Conversational Agents](https://arxiv.org/abs/2510.09474)
*Zhenhailong Wang,Jiateng Liu,Amin Fazel,Ritesh Sarkhel,Xing Fan,Xiang Li,Chenlei Guo,Heng Ji,Ruhi Sarikaya*

Main category: cs.CL

TL;DR: 本文提出了一种名为Multimodal Policy Internalization（MPI）的新任务，旨在将复杂的多模态策略内化到模型参数中，以提升对策略的遵循能力，减少推理时对策略文本的依赖，提高多模态对话系统的效率与性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大型语言模型的多模态对话系统依赖复杂且冗长的策略文本（如元数据、响应风格和工具使用规则），导致遵循策略困难且计算成本高，尤其是在支持视觉和多模态行为的场景下，现有研究还较少涉及此类策略的内化与压缩。

Method: 提出了Multimodal Policy Internalization（MPI）任务，设计了三阶段训练框架TriMPI，包括持续预训练注入策略知识、监督微调和采用PolicyRollout强化学习进行策略感知的探索。同时构建了包含合成和真实决策与工具使用任务的两套数据集。

Result: TriMPI在端到端准确率、泛化能力及忘记防护方面均表现出显著提升，证明了该方法对多模态策略的内化效果良好。

Conclusion: 作为首个针对多模态策略内化的工作，本文不仅提出了有效方法，还开放了相关数据集和训练方案，为未来多模态对话系统的策略研究提供了坚实基础。

Abstract: Modern conversational agents like ChatGPT and Alexa+ rely on predefined
policies specifying metadata, response styles, and tool-usage rules. As these
LLM-based systems expand to support diverse business and user queries, such
policies, often implemented as in-context prompts, are becoming increasingly
complex and lengthy, making faithful adherence difficult and imposing large
fixed computational costs. With the rise of multimodal agents, policies that
govern visual and multimodal behaviors are critical but remain understudied.
Prior prompt-compression work mainly shortens task templates and
demonstrations, while existing policy-alignment studies focus only on
text-based safety rules. We introduce Multimodal Policy Internalization (MPI),
a new task that internalizes reasoning-intensive multimodal policies into model
parameters, enabling stronger policy-following without including the policy
during inference. MPI poses unique data and algorithmic challenges. We build
two datasets spanning synthetic and real-world decision-making and tool-using
tasks and propose TriMPI, a three-stage training framework. TriMPI first
injects policy knowledge via continual pretraining, then performs supervised
finetuning, and finally applies PolicyRollout, a GRPO-style reinforcement
learning extension that augments rollouts with policy-aware responses for
grounded exploration. TriMPI achieves notable gains in end-to-end accuracy,
generalization, and robustness to forgetting. As the first work on multimodal
policy internalization, we provide datasets, training recipes, and
comprehensive evaluations to foster future research. Project page:
https://mikewangwzhl.github.io/TriMPI.

</details>


### [96] [StatEval: A Comprehensive Benchmark for Large Language Models in Statistics](https://arxiv.org/abs/2510.09517)
*Yuchen Lu,Run Yang,Yichen Zhang,Shuguang Yu,Runpeng Dai,Ziwei Wang,Jiayi Xiang,Wenxin E,Siran Gao,Xinyao Ruan,Yirui Huang,Chenjing Xi,Haibo Hu,Yueming Fu,Qinglan Yu,Xiaobing Wei,Jiani Gu,Rui Sun,Jiaxuan Jia,Fan Zhou*

Main category: cs.CL

TL;DR: 提出了StatEval，一个专注于统计学的全面基准库，涵盖本科至研究级别，共计约1.6万题，评价大型语言模型的统计推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在数学与逻辑推理上表现优异，但统计学作为独特领域缺乏系统性的评测工具。

Method: 构建了一个多智能体自动化系统结合人工验证，完成大规模题目提取、改写与质控，设计了适用于计算和证明任务的评估框架。

Result: 测试显示，封闭模型如GPT5-mini在研究级问题上得分不到57%，开源模型表现更差，反映统计推理的挑战性及现有模型的不足。

Conclusion: StatEval填补了统计推理评测空白，预计成为推动大语言模型统计智能发展的重要工具。

Abstract: Large language models (LLMs) have demonstrated remarkable advances in
mathematical and logical reasoning, yet statistics, as a distinct and
integrative discipline, remains underexplored in benchmarking efforts. To
address this gap, we introduce \textbf{StatEval}, the first comprehensive
benchmark dedicated to statistics, spanning both breadth and depth across
difficulty levels. StatEval consists of 13,817 foundational problems covering
undergraduate and graduate curricula, together with 2374 research-level proof
tasks extracted from leading journals. To construct the benchmark, we design a
scalable multi-agent pipeline with human-in-the-loop validation that automates
large-scale problem extraction, rewriting, and quality control, while ensuring
academic rigor. We further propose a robust evaluation framework tailored to
both computational and proof-based tasks, enabling fine-grained assessment of
reasoning ability. Experimental results reveal that while closed-source models
such as GPT5-mini achieve below 57\% on research-level problems, with
open-source models performing significantly lower. These findings highlight the
unique challenges of statistical reasoning and the limitations of current LLMs.
We expect StatEval to serve as a rigorous benchmark for advancing statistical
intelligence in large language models. All data and code are available on our
web platform: https://stateval.github.io/.

</details>


### [97] [Can We Reliably Rank Model Performance across Domains without Labeled Data?](https://arxiv.org/abs/2510.09519)
*Veronica Rammouz,Aaron Gonzalez,Carlos Cruzportillo,Adrian Tan,Nicole Beebe,Anthony Rios*

Main category: cs.CL

TL;DR: 本文研究了无标签条件下估计NLP模型性能的可靠性，发现基于大型语言模型的错误预测方法在跨领域评估中表现更优。


<details>
  <summary>Details</summary>
Motivation: 当前无标签性能估计方法在跨领域性能排名的可靠性尚不明确，亟需分析影响排名可靠性的因素。

Method: 采用两步评估框架，使用四个基础分类器和多种大型语言模型作为错误预测器，在GeoOLID和亚马逊评论等跨15个领域的数据集上进行实验。

Result: 实验表明，基于大型语言模型的错误预测器与真实准确率的排名相关性更强且更稳定，比基于漂移或零样本的基线方法表现出色；性能差异越大、错误预测与真失败模式一致时，排名可靠性越高。

Conclusion: 基于大型语言模型的性能估计方法在跨领域模型评估中更可靠，本文结果为其可信使用提供了指导依据。

Abstract: Estimating model performance without labels is an important goal for
understanding how NLP models generalize. While prior work has proposed measures
based on dataset similarity or predicted correctness, it remains unclear when
these estimates produce reliable performance rankings across domains. In this
paper, we analyze the factors that affect ranking reliability using a two-step
evaluation setup with four base classifiers and several large language models
as error predictors. Experiments on the GeoOLID and Amazon Reviews datasets,
spanning 15 domains, show that large language model-based error predictors
produce stronger and more consistent rank correlations with true accuracy than
drift-based or zero-shot baselines. Our analysis reveals two key findings:
ranking is more reliable when performance differences across domains are
larger, and when the error model's predictions align with the base model's true
failure patterns. These results clarify when performance estimation methods can
be trusted and provide guidance for their use in cross-domain model evaluation.

</details>


### [98] [Accent-Invariant Automatic Speech Recognition via Saliency-Driven Spectrogram Masking](https://arxiv.org/abs/2510.09528)
*Mohammad Hossein Sameti,Sepehr Harfi Moridani,Ali Zarean,Hossein Sameti*

Main category: cs.CL

TL;DR: 提出了一种结合口音分类和掩码数据增强的口音不变自动语音识别框架，有效提升英语和波斯语中口音变异下的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 现有预训练变换器模型在自动语音识别中表现优异，但对口音和方言变异敏感，导致错误率升高，尤其在英语和波斯语等语言中表现明显。

Method: 通过训练基于频谱图的口音分类器，掩码对分类判断影响最大的频谱区域，利用掩码后的频谱进行数据增强，从而提高模型对口音变异的鲁棒性。

Result: 在引入的波斯语多区域口音数据集和英语数据上，基于Whisper模型的实验表明该方法显著降低了词错误率（WER），验证了方法的有效性。

Conclusion: 该研究推动了多语言自动语音识别系统对口音和方言多样性的适应能力，数据集和代码已公开，为弱资源语言提供了重要资源和基准。

Abstract: Pre-trained transformer-based models have significantly advanced automatic
speech recognition (ASR), yet they remain sensitive to accent and dialectal
variations, resulting in elevated word error rates (WER) in linguistically
diverse languages such as English and Persian. To address this challenge, we
propose an accent-invariant ASR framework that integrates accent and dialect
classification into the recognition pipeline. Our approach involves training a
spectrogram-based classifier to capture accent-specific cues, masking the
regions most influential to its predictions, and using the masked spectrograms
for data augmentation. This enhances the robustness of ASR models against
accent variability. We evaluate the method using both English and Persian
speech. For Persian, we introduce a newly collected dataset spanning multiple
regional accents, establishing the first systematic benchmark for accent
variation in Persian ASR that fills a critical gap in multilingual speech
research and provides a foundation for future studies on low-resource,
linguistically diverse languages. Experimental results with the Whisper model
demonstrate that our masking and augmentation strategy yields substantial WER
reductions in both English and Persian settings, confirming the effectiveness
of the approach. This research advances the development of multilingual ASR
systems that are resilient to accent and dialect diversity. Code and dataset
are publicly available at: https://github.com/MH-Sameti/Accent_invariant_ASR

</details>


### [99] [Mitigating Overthinking through Reasoning Shaping](https://arxiv.org/abs/2510.09535)
*Feifan Song,Shaohang Wei,Bofei Gao,Yejie Wang,Wen Luo,Wei Li,Linli Yao,Weimin Xiong,Liang Chen,Tianyu Liu,Houfeng Wang*

Main category: cs.CL

TL;DR: 本文提出了一种分组相对分段惩罚（GRSP）方法，以解决大推理模型在基于验证者奖励的强化学习（RLVR）中出现的过度推理问题，实现了提升计算效率而不显著牺牲准确率。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR中的惩罚机制减小了token消耗但通常损害模型性能，原因是token级别的监督过于简单，无法有效平衡推理效率与准确率。

Method: 提出GRSP方法，在步骤级别对推理进行正则化，利用长度感知的权重机制对推理段落簇进行调节，以减少无谓的推理消耗。

Result: 实验表明GRSP显著提升了token使用效率，尤其在较难问题中表现更优，同时保持了模型准确率，并且训练过程更稳定，适用于不同规模的模型。

Conclusion: GRSP通过改进监督粒度，有效缓解了过度推理问题，实现了高效且准确的推理，促进了RL训练的稳定性和模型可扩展性。

Abstract: Large reasoning models (LRMs) boosted by Reinforcement Learning from Verifier
Reward (RLVR) have shown great power in problem solving, yet they often cause
overthinking: excessive, meandering reasoning that inflates computational cost.
Prior designs of penalization in RLVR manage to reduce token consumption while
often harming model performance, which arises from the oversimplicity of
token-level supervision. In this paper, we argue that the granularity of
supervision plays a crucial role in balancing efficiency and accuracy, and
propose Group Relative Segment Penalization (GRSP), a step-level method to
regularize reasoning. Since preliminary analyses show that reasoning segments
are strongly correlated with token consumption and model performance, we design
a length-aware weighting mechanism across segment clusters. Extensive
experiments demonstrate that GRSP achieves superior token efficiency without
heavily compromising accuracy, especially the advantages with harder problems.
Moreover, GRSP stabilizes RL training and scales effectively across model
sizes.

</details>


### [100] [Evaluating Robustness of Large Language Models Against Multilingual Typographical Errors](https://arxiv.org/abs/2510.09536)
*Yihong Liu,Raoyuan Zhao,Lena Altinger,Hinrich Schütze,Michael A. Hedderich*

Main category: cs.CL

TL;DR: 本文提出了一个多语言拼写错误生成算法MulTypo，模拟真实打字错误，评估了18个开源大语言模型在不同任务中对拼写错误的鲁棒性，发现拼写错误会显著影响模型性能，尤其是生成和推理任务，且不同语言表现不同。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型在多语言实际应用中会遇到自然输入的拼写错误，但大多数基准测试假设输入无误，缺少对拼写错误下模型鲁棒性的研究，尤其是跨语言的。

Method: 设计MulTypo算法，根据语言特定键盘布局和打字行为生成模拟人类拼写错误，然后在18个开源大语言模型、5个下游任务中评估其在含拼写错误输入下的表现。

Result: 拼写错误普遍降低模型性能，特别是在生成和推理相关任务中，自然语言推理任务相对更鲁棒。指令微调提升了干净输入的性能，但可能增加面对噪声时的脆弱性。此外，高资源语言表现更鲁棒，英译他语比他译英更鲁棒。

Conclusion: 为了提升大语言模型在现实多语言应用中的实用性，需关注噪声鲁棒性训练和多语言下的鲁棒性评估。作者公开了代码和数据推动相关研究。

Abstract: Large language models (LLMs) are increasingly deployed in multilingual,
real-world applications with user inputs -- naturally introducing typographical
errors (typos). Yet most benchmarks assume clean input, leaving the robustness
of LLMs to typos across languages largely underexplored. To address this gap,
we introduce MulTypo, a multilingual typo generation algorithm that simulates
human-like errors based on language-specific keyboard layouts and typing
behavior. We evaluate 18 open-source LLMs across three model families and five
downstream tasks spanning language inference, multi-choice question answering,
mathematical reasoning, and machine translation tasks. Our results show that
typos consistently degrade performance, particularly in generative tasks and
those requiring reasoning -- while the natural language inference task is
comparatively more robust. Instruction tuning improves clean-input performance
but may increase brittleness under noise. We also observe language-dependent
robustness: high-resource languages are generally more robust than low-resource
ones, and translation from English is more robust than translation into
English. Our findings underscore the need for noise-aware training and
multilingual robustness evaluation. We make our code and data publicly
available.

</details>


### [101] [SPG: Sandwiched Policy Gradient for Masked Diffusion Language Models](https://arxiv.org/abs/2510.09541)
*Chengyu Wang,Paria Rashidinejad,DiJia Su,Song Jiang,Sid Wang,Siyan Zhao,Cai Zhou,Shannon Zejiang Shen,Feiyu Chen,Tommi Jaakkola,Yuandong Tian,Bo Liu*

Main category: cs.CL

TL;DR: 本文提出了一种名为Sandwiched Policy Gradient (SPG)的方法，用于通过强化学习更有效地优化扩散大型语言模型，显著提升了多个任务的准确率。


<details>
  <summary>Details</summary>
Motivation: 扩散大型语言模型虽能并行解码多个标记，但其对数似然难以计算，导致传统的策略梯度方法难以直接应用，现有基于ELBO的单侧近似引入了显著的梯度偏差。

Method: 提出SPG方法，利用对真实对数似然的上下界进行联合优化，减少策略梯度的偏差。

Result: 实验证明，SPG在GSM8K、MATH500、Countdown和Sudoku等多个任务上，准确率分别提升了3.6%、2.6%、18.4%和27.0%，显著优于基于ELBO和单步估计的基线方法。

Conclusion: SPG有效解决了扩散大型语言模型强化学习中的梯度偏差问题，提升了任务性能，展现出方法的实用价值和优越性。

Abstract: Diffusion large language models (dLLMs) are emerging as an efficient
alternative to autoregressive models due to their ability to decode multiple
tokens in parallel. However, aligning dLLMs with human preferences or
task-specific rewards via reinforcement learning (RL) is challenging because
their intractable log-likelihood precludes the direct application of standard
policy gradient methods. While prior work uses surrogates like the evidence
lower bound (ELBO), these one-sided approximations can introduce significant
policy gradient bias. To address this, we propose the Sandwiched Policy
Gradient (SPG) that leverages both an upper and a lower bound of the true
log-likelihood. Experiments show that SPG significantly outperforms baselines
based on ELBO or one-step estimation. Specifically, SPG improves the accuracy
over state-of-the-art RL methods for dLLMs by 3.6% in GSM8K, 2.6% in MATH500,
18.4% in Countdown and 27.0% in Sudoku.

</details>


### [102] [Beyond Surface Reasoning: Unveiling the True Long Chain-of-Thought Capacity of Diffusion Large Language Models](https://arxiv.org/abs/2510.09544)
*Qiguang Chen,Hanjing Li,Libo Qin,Dengyun Peng,Jinhao Liu,Jiangyi Wang,Chengyue Wu,Xie Chen,Yantao Du,Wanxiang Che*

Main category: cs.CL

TL;DR: 本文研究了扩散大型语言模型（DLLMs）在并行解码与因果顺序推理之间的矛盾（PSC），揭示其在复杂推理任务中表现出自回归行为，并提出了缓解策略。


<details>
  <summary>Details</summary>
Motivation: 解决扩散大型语言模型在并行解码与严格顺序推理需求之间的冲突问题，提升其推理能力和效率。

Method: 通过行为分析揭示PSC现象，定义DLLMs的三大拓展维度（并行、扩散、顺序），实证分析其在不同维度上的扩展效果，并提出并行导向提示、扩散早停及并行扩展等缓解策略。

Result: 发现DLLMs只对直接可判定输出表现出真正的并行，复杂任务则退化为自回归行为，PSC限制了模型的自我反思、推理深度和探索广度。并行扩展有效提升性能，而扩散和顺序扩展受限。

Conclusion: PSC是DLLMs性能瓶颈之一，针对PSC的缓解方法能有效提升模型推理质量和效率，推动了扩散模型在语言理解领域的应用。

Abstract: Recently, Diffusion Large Language Models (DLLMs) have offered high
throughput and effective sequential reasoning, making them a competitive
alternative to autoregressive LLMs (ALLMs). However, parallel decoding, which
enables simultaneous token updates, conflicts with the causal order often
required for rigorous reasoning. We first identify this conflict as the core
Parallel-Sequential Contradiction (PSC). Behavioral analyses in both simple and
complex reasoning tasks show that DLLMs exhibit genuine parallelism only for
directly decidable outputs. As task difficulty increases, they revert to
autoregressive-like behavior, a limitation exacerbated by autoregressive
prompting, which nearly doubles the number of decoding steps with remasking
without improving quality. Moreover, PSC restricts DLLMs' self-reflection,
reasoning depth, and exploratory breadth. To further characterize PSC, we
introduce three scaling dimensions for DLLMs: parallel, diffusion, and
sequential. Empirically, while parallel scaling yields consistent improvements,
diffusion and sequential scaling are constrained by PSC. Based on these
findings, we propose several practical mitigations, parallel-oriented
prompting, diffusion early stopping, and parallel scaling, to reduce
PSC-induced ineffectiveness and inefficiencies.

</details>


### [103] [Hierarchical Indexing with Knowledge Enrichment for Multilingual Video Corpus Retrieval](https://arxiv.org/abs/2510.09553)
*Yu Wang,Tianhao Tan,Yifei Wang*

Main category: cs.CL

TL;DR: 本论文提出了一种多阶段框架，用于跨语言检索多语言医疗教学视频，通过结合多语言语义、领域术语及高效长文本处理实现精确匹配。


<details>
  <summary>Details</summary>
Motivation: 现有系统在检索跨语言的医疗教学视频时，要么使用粗糙的长视频嵌入，导致精度不足，要么采用细粒度匹配但计算成本高昂。为了解决这一矛盾，提出高效且精确的多语言视频语料库检索方法。

Method: 将视频字幕划分为语义连贯的片段，结合知识图谱知识进行增强，并构建分层树结构。采用语言无关的多语言编码器生成节点嵌入，再通过自上而下的粗到细树搜索剔除无关节点，最终由轻量级大语言模型对排名靠前的片段进行重评分。

Result: 在mVCR测试集中实现了最先进的性能，消融实验验证了知识图谱增强、分层索引以及针对性LLM重排名的互补作用。

Conclusion: 该方法在多语言医疗视频检索中既保证了精度，又实现了计算效率，适用于专业医疗视频藏品的准确且可扩展检索。

Abstract: Retrieving relevant instructional videos from multilingual medical archives
is crucial for answering complex, multi-hop questions across language
boundaries. However, existing systems either compress hour-long videos into
coarse embeddings or incur prohibitive costs for fine-grained matching. We
tackle the Multilingual Video Corpus Retrieval (mVCR) task in the NLPCC-2025
M4IVQA challenge with a multi-stage framework that integrates multilingual
semantics, domain terminology, and efficient long-form processing. Video
subtitles are divided into semantically coherent chunks, enriched with concise
knowledge-graph (KG) facts, and organized into a hierarchical tree whose node
embeddings are generated by a language-agnostic multilingual encoder. At query
time, the same encoder embeds the input question; a coarse-to-fine tree search
prunes irrelevant branches, and only the top-ranked chunks are re-scored by a
lightweight large language model (LLM). This design avoids exhaustive
cross-encoder scoring while preserving chunk-level precision. Experiments on
the mVCR test set demonstrate state-of-the-art performance, and ablation
studies confirm the complementary contributions of KG enrichment, hierarchical
indexing, and targeted LLM re-ranking. The proposed method offers an accurate
and scalable solution for multilingual retrieval in specialized medical video
collections.

</details>


### [104] [A Comprehensive Evaluation of Multilingual Chain-of-Thought Reasoning: Performance, Consistency, and Faithfulness Across Languages](https://arxiv.org/abs/2510.09555)
*Raoyuan Zhao,Yihong Liu,Hinrich Schütze,Michael A. Hedderich*

Main category: cs.CL

TL;DR: 本文首次全面研究了多语言环境下的链式推理，评价了性能、一致性和可信度，发现不同语言下模型表现差异显著，且推理路径质量受语言影响较大。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在多语言环境下的链式推理的中间思考步骤较少被研究，特别是中间推理过程的表现及其可信度。

Method: 通过对链式推理中的语言遵从性、答案准确性和答案一致性进行测量，跨语言交换思考路径以评估一致性，以及采用截断和注入错误等扰动技术检验思考路径的可信度。

Result: 发现模型对特定语言有强烈偏好，不同语言下推理路径的质量和效果差异显著；思考路径对模型最终输出的影响程度存在差异。

Conclusion: 链式推理的中间步骤在多语言环境下表现不一，语言对推理路径的质量和模型性能有重要影响，未来研究需关注多语言链式推理的多维度评价。

Abstract: Large reasoning models (LRMs) increasingly rely on step-by-step
Chain-of-Thought (CoT) reasoning to improve task performance, particularly in
high-resource languages such as English. While recent work has examined
final-answer accuracy in multilingual settings, the thinking traces themselves,
i.e., the intermediate steps that lead to the final answer, remain
underexplored. In this paper, we present the first comprehensive study of
multilingual CoT reasoning, evaluating three key dimensions: performance,
consistency, and faithfulness. We begin by measuring language compliance,
answer accuracy, and answer consistency when LRMs are explicitly instructed or
prompt-hacked to think in a target language, revealing strong language
preferences and divergent performance across languages. Next, we assess
crosslingual consistency of thinking traces by interchanging them between
languages. We find that the quality and effectiveness of thinking traces vary
substantially depending on the prompt language. Finally, we adapt
perturbation-based techniques -- i.e., truncation and error injection -- to
probe the faithfulness of thinking traces across languages, showing that models
rely on traces to varying degrees. We release our code and data to support
future research.

</details>


### [105] [WUGNECTIVES: Novel Entity Inferences of Language Models from Discourse Connectives](https://arxiv.org/abs/2510.09556)
*Daniel Brubaker,William Sheffield,Junyi Jessy Li,Kanishka Misra*

Main category: cs.CL

TL;DR: 本文研究了话语连接词是否能帮助语言模型（LMs）理解世界知识，提出了一个包含8880个刺激项的数据集WUGNECTIVES来评估LM在新颖实体推理的表现。


<details>
  <summary>Details</summary>
Motivation: 传统上，世界知识被用来预测话语连接词，本文反向探索连接词能否帮助LM获得世界知识。

Method: 构建WUGNECTIVES数据集，测试17个不同规模和训练方法的语言模型，分析它们在推断新实体属性时的表现及对不同话语连接词的适应性。

Result: 经过调优的模型在大多数连接词上表现提升明显，但所有模型在表达让步意义的连接词上均表现较差。

Conclusion: 话语连接词在语言模型推理世界知识中具有功能性作用，未来可以针对不同连接词作用进行更细致研究。

Abstract: The role of world knowledge has been particularly crucial to predict the
discourse connective that marks the discourse relation between two arguments,
with language models (LMs) being generally successful at this task. We flip
this premise in our work, and instead study the inverse problem of
understanding whether discourse connectives can inform LMs about the world. To
this end, we present WUGNECTIVES, a dataset of 8,880 stimuli that evaluates
LMs' inferences about novel entities in contexts where connectives link the
entities to particular attributes. On investigating 17 different LMs at various
scales, and training regimens, we found that tuning an LM to show reasoning
behavior yields noteworthy improvements on most connectives. At the same time,
there was a large variation in LMs' overall performance across connective type,
with all models systematically struggling on connectives that express a
concessive meaning. Our findings pave the way for more nuanced investigations
into the functional role of language cues as captured by LMs. We release
WUGNECTIVES at https://github.com/sheffwb/wugnectives.

</details>


### [106] [AutoPR: Let's Automate Your Academic Promotion!](https://arxiv.org/abs/2510.09558)
*Qiguang Chen,Zheng Yan,Mingda Yang,Libo Qin,Yixin Yuan,Hanjing Li,Jinhao Liu,Yiyan Ji,Dengyun Peng,Jiannan Guan,Mengkang Hu,Yantao Du,Wanxiang Che*

Main category: cs.CL

TL;DR: 该论文提出了自动推广（AutoPR）任务，通过多代理系统PRAgent自动将学术论文转化为高质量的推广内容，显著提高了推广效果。


<details>
  <summary>Details</summary>
Motivation: 随着学术论文数量激增，依赖社交平台进行发现和推广的需求增加，但人工推广效率低，迫切需要自动化解决方案。

Method: 构建了多模式基准PRBench用于评价，设计多代理框架PRAgent，包括内容提取、多方协作生成和平台适配，实现推广内容的准确性、吸引力和及时性。

Result: PRAgent相比直接LLM方法，在PRBench上推广效果显著提升，观看时长增加604%，点赞数提升438%，整体互动提升至少2.9倍，平台建模和定向推广贡献最大。

Conclusion: AutoPR作为一个可行且可量化的任务，为学术传播自动化提供了系统方法和评估基准，推动可扩展且高影响力的自动学术传播。

Abstract: As the volume of peer-reviewed research surges, scholars increasingly rely on
social platforms for discovery, while authors invest considerable effort in
promoting their work to ensure visibility and citations. To streamline this
process and reduce the reliance on human effort, we introduce Automatic
Promotion (AutoPR), a novel task that transforms research papers into accurate,
engaging, and timely public content. To enable rigorous evaluation, we release
PRBench, a multimodal benchmark that links 512 peer-reviewed articles to
high-quality promotional posts, assessing systems along three axes: Fidelity
(accuracy and tone), Engagement (audience targeting and appeal), and Alignment
(timing and channel optimization). We also introduce PRAgent, a multi-agent
framework that automates AutoPR in three stages: content extraction with
multimodal preparation, collaborative synthesis for polished outputs, and
platform-specific adaptation to optimize norms, tone, and tagging for maximum
reach. When compared to direct LLM pipelines on PRBench, PRAgent demonstrates
substantial improvements, including a 604% increase in total watch time, a 438%
rise in likes, and at least a 2.9x boost in overall engagement. Ablation
studies show that platform modeling and targeted promotion contribute the most
to these gains. Our results position AutoPR as a tractable, measurable research
problem and provide a roadmap for scalable, impactful automated scholarly
communication.

</details>


### [107] [Dyna-Mind: Learning to Simulate from Experience for Better AI Agents](https://arxiv.org/abs/2510.09577)
*Xiao Yu,Baolin Peng,Michel Galley,Hao Cheng,Qianhui Wu,Janardhan Kulkarni,Suman Nath,Zhou Yu,Jianfeng Gao*

Main category: cs.CL

TL;DR: 本文提出了Dyna-Mind训练框架，通过两阶段方法增强AI在复杂交互环境中的模拟推理能力，提高长时序任务的表现。


<details>
  <summary>Details</summary>
Motivation: 当前推理模型在数学和编码任务表现优异，但在长时序交互任务如网页导航和设备使用等表现较差，原因在于缺乏能预演多种未来情景的"旁试错"能力。

Method: 提出两阶段训练框架Dyna-Mind：第一阶段ReSim训练模型生成基于真实环境交互的扩展搜索树的结构化推理轨迹，提升模型对世界动态的理解和未来状态预测能力；第二阶段Dyna-GRPO通过在线强化学习利用结果奖励和中间状态反馈，增强模型的模拟和决策能力。

Result: 在Sokoban、ALFWorld及AndroidWorld三个基准测试中，ReSim显著提升了模型的模拟能力，Dyna-GRPO利用多层次反馈进一步优化策略，在长时序规划任务中表现优异。

Conclusion: 模拟能力是提升AI代理在复杂环境中推理、规划和行动效果的关键，Dyna-Mind框架有效赋予模型这种能力，推动AI在长时序交互任务中的发展。

Abstract: Reasoning models have recently shown remarkable progress in domains such as
math and coding. However, their expert-level abilities in math and coding
contrast sharply with their performance in long-horizon, interactive tasks such
as web navigation and computer/phone-use. Inspired by literature on human
cognition, we argue that current AI agents need ''vicarious trial and error'' -
the capacity to mentally simulate alternative futures before acting - in order
to enhance their understanding and performance in complex interactive
environments. We introduce Dyna-Mind, a two-stage training framework that
explicitly teaches (V)LM agents to integrate such simulation into their
reasoning. In stage 1, we introduce Reasoning with Simulations (ReSim), which
trains the agent to generate structured reasoning traces from expanded search
trees built from real experience gathered through environment interactions.
ReSim thus grounds the agent's reasoning in faithful world dynamics and equips
it with the ability to anticipate future states in its reasoning. In stage 2,
we propose Dyna-GRPO, an online reinforcement learning method to further
strengthen the agent's simulation and decision-making ability by using both
outcome rewards and intermediate states as feedback from real rollouts.
Experiments on two synthetic benchmarks (Sokoban and ALFWorld) and one
realistic benchmark (AndroidWorld) demonstrate that (1) ReSim effectively
infuses simulation ability into AI agents, and (2) Dyna-GRPO leverages outcome
and interaction-level signals to learn better policies for long-horizon,
planning-intensive tasks. Together, these results highlight the central role of
simulation in enabling AI agents to reason, plan, and act more effectively in
the ever more challenging environments.

</details>


### [108] [Mind-Paced Speaking: A Dual-Brain Approach to Real-Time Reasoning in Spoken Language Models](https://arxiv.org/abs/2510.09592)
*Donghang Wu,Haoyang Zhang,Jun Chen,Xiangyu,Zhang,Hexin Liu,Eng Siong Chng,Fei Tian,Xuerui Yang,Xiangyu Zhang,Daxin Jiang,Gang Yu*

Main category: cs.CL

TL;DR: 该论文提出了一种名为Mind-Paced Speaking（MPS）的实时口语语言模型框架，通过双脑机制实现边说边思考，显著提升推理性能并大幅降低延迟。


<details>
  <summary>Details</summary>
Motivation: 实时口语语言模型在利用链式思维推理时面临较高的延迟，难以实现边说边思考的效果。

Method: 借鉴人脑不同区域各司其职的机制，设计了“Formulation Brain”负责高层次推理，“Articulation Brain”负责流畅的语音生成，实现推理和发言的并行处理。

Result: MPS在数学推理任务Spoken-MQA达到92.8%准确率，在语音对话任务URO-Bench获得82.5分，推理表现接近预先计算完整CoT的模型，同时显著降低延迟。

Conclusion: 该方法有效弥合了高质量推理与实时交互之间的矛盾，推动实时口语语言模型的发展。

Abstract: Real-time Spoken Language Models (SLMs) struggle to leverage Chain-of-Thought
(CoT) reasoning due to the prohibitive latency of generating the entire thought
process sequentially. Enabling SLMs to think while speaking, similar to humans,
is attracting increasing attention. We present, for the first time, Mind-Paced
Speaking (MPS), a brain-inspired framework that enables high-fidelity,
real-time reasoning. Similar to how humans utilize distinct brain regions for
thinking and responding, we propose a novel dual-brain approach, employing a
"Formulation Brain" for high-level reasoning to pace and guide a separate
"Articulation Brain" for fluent speech generation. This division of labor
eliminates mode-switching, preserving the integrity of the reasoning process.
Experiments show that MPS significantly outperforms existing
think-while-speaking methods and achieves reasoning performance comparable to
models that pre-compute the full CoT before speaking, while drastically
reducing latency. Under a zero-latency configuration, the proposed method
achieves an accuracy of 92.8% on the mathematical reasoning task Spoken-MQA and
attains a score of 82.5 on the speech conversation task URO-Bench. Our work
effectively bridges the gap between high-quality reasoning and real-time
interaction.

</details>


### [109] [Prompting Test-Time Scaling Is A Strong LLM Reasoning Data Augmentation](https://arxiv.org/abs/2510.09599)
*Sondos Mahmoud Bsharat,Zhiqiang Shen*

Main category: cs.CL

TL;DR: 提出了一种名为Prompting Test-Time Scaling（P-TTS）的推理时间数据增强方法，通过少量实例和多样化提示强度来提升大语言模型的推理能力，显著提升了数学推理任务的准确率，并增强了零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型依赖大量推理示例进行训练，但收集大规模数据集代价高昂，如何用少量数据高效提升模型推理效果是主要挑战。

Method: 利用90个手工选取的推理实例，在推理时通过系统地变化提示的强度生成多样化的推理上下文，然后对不同规模的Qwen-2.5模型进行微调，实现推断时的数据增强。

Result: 在多个数学推理基准（如AIME2024 & 25，MATH500，GPQA-Diamond）上，P-TTS方法显著超过了1K-shot的强基线，准确率提升最高达30%。同时提升了出域推理零样本泛化能力。

Conclusion: P-TTS是一种低成本、高效的推理增强策略，通过挖掘潜在推理模式空间，极大提高了大语言模型在资源受限或快速变化领域的推理潜力和能力。

Abstract: Large language models (LLMs) have demonstrated impressive reasoning
capabilities when provided with chain-of-thought exemplars, but curating large
reasoning datasets remains laborious and resource-intensive. In this work, we
introduce Prompting Test-Time Scaling (P-TTS), a simple yet effective
inference-time data augmentation strategy for enhancing LLM reasoning through
finetuning. Rather than collecting thousands or even millions of examples,
P-TTS leverages a small pool of only 90 manually selected reasoning instances
and systematically varies exemplar augmentation through principled instruction
prompting intensities at test time to synthesize diverse reasoning trajectory
contexts. Then we finetune the various sizes of Qwen-2.5 models on P-TTS data.
Across a suite of mathematical reasoning AIME2024 & 25, MATH500, and
GPQA-Diamond, our P-TTS-7B and 32B models outperform the prior competitive
baselines like S1 and S1.1 (1K-shot), achieving absolute accuracy gains of
+26.66% and +30.00% on AIME'24 (7B), and +13.34% and +6.67% on AIME'25 (7B);
P-TTS-32B yields gains of +23.33% and +16.63% on AIME'24, and +26.63% and
+3.33% on AIME'25 (vs. S1 and S1.1, respectively), with comparable or better
performance on MATH500 and GPQA-Diamond. We further show that P-TTS enhances
zero-shot generalization accuracy on out-of-domain reasoning benchmarks of
Gaokao, Kaoyan, OlympiadBench, AMC23, GradeSchoolMath, and Minerva. Our
analysis suggests that test-time scaling effectively explores the latent space
of reasoning patterns, amplifying LLM problem-solving with minimal annotation
overhead, and further unlocking the reasoning potential and capabilities of
LLMs. Prompting Test-Time Scaling offers a practical, low-cost way to elicit
LLM reasoning in resource-constrained or rapidly evolving domains.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [110] [Comparative Analysis of Large Language Models for the Machine-Assisted Resolution of User Intentions](https://arxiv.org/abs/2510.08576)
*Justus Flerlage,Alexander Acker,Odej Kao*

Main category: cs.SE

TL;DR: 本文研究了本地部署的开源大语言模型（LLM）在用户意图理解和工作流程生成中的能力，并与OpenAI的GPT-4系统进行了比较，强调了本地化部署在隐私、自主性和可扩展性方面的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前大多数LLM应用依赖云端专有模型，存在隐私、安全和自主性问题。为实现更稳健、可信的语言优先交互界面，迫切需要评估本地部署的开源LLM的可行性。

Method: 本文选取了多款开源和开放访问的语言模型，评估其在用户意图解析及工作流程自动生成中的性能，并与专有的GPT-4模型进行了对比分析。

Result: 研究发现开源模型在本地部署环境下具有一定的实用性，但在性能和效率上与GPT-4存在差距。研究还揭示了本地化LLM在隐私保护和系统自主性方面的优势。

Conclusion: 本地部署的开源大语言模型具备成为下一代意图操作系统基础组件的潜力，推动了AI基础设施的去中心化和民主化，未来用户设备交互将更为无缝、适应性强且注重隐私保护。

Abstract: Large Language Models (LLMs) have emerged as transformative tools for natural
language understanding and user intent resolution, enabling tasks such as
translation, summarization, and, increasingly, the orchestration of complex
workflows. This development signifies a paradigm shift from conventional,
GUI-driven user interfaces toward intuitive, language-first interaction
paradigms. Rather than manually navigating applications, users can articulate
their objectives in natural language, enabling LLMs to orchestrate actions
across multiple applications in a dynamic and contextual manner. However,
extant implementations frequently rely on cloud-based proprietary models, which
introduce limitations in terms of privacy, autonomy, and scalability. For
language-first interaction to become a truly robust and trusted interface
paradigm, local deployment is not merely a convenience; it is an imperative.
This limitation underscores the importance of evaluating the feasibility of
locally deployable, open-source, and open-access LLMs as foundational
components for future intent-based operating systems. In this study, we examine
the capabilities of several open-source and open-access models in facilitating
user intention resolution through machine assistance. A comparative analysis is
conducted against OpenAI's proprietary GPT-4-based systems to assess
performance in generating workflows for various user intentions. The present
study offers empirical insights into the practical viability, performance
trade-offs, and potential of open LLMs as autonomous, locally operable
components in next-generation operating systems. The results of this study
inform the broader discussion on the decentralization and democratization of AI
infrastructure and point toward a future where user-device interaction becomes
more seamless, adaptive, and privacy-conscious through locally embedded
intelligence.

</details>


### [111] [Which Is Better For Reducing Outdated and Vulnerable Dependencies: Pinning or Floating?](https://arxiv.org/abs/2510.08609)
*Imranur Rahman,Jill Marley,William Enck,Laurie Williams*

Main category: cs.SE

TL;DR: 本文通过大规模实证研究分析了不同版本约束类型对依赖库过时和安全漏洞的影响，发现浮动版本尤其是浮动次版本约束使用最广，但固定版本约束易导致依赖库过时。


<details>
  <summary>Details</summary>
Motivation: 开发者对依赖管理中版本约束存在权衡：固定版本能防止供应链攻击但导致依赖过时，浮动版本自动获得修复但存在破坏风险，然而不同版本约束导致依赖过时或安全漏洞的概率尚不明确。

Method: 作者分析了npm、PyPI和Cargo生态中版本约束的使用趋势及变更模式，采用生存分析模型，对比了固定版本与其他版本约束类型在依赖变过时或出现漏洞方面的差异。

Result: 研究发现最常用的过时和有漏洞依赖版本约束类型是浮动次版本，其次是固定版本。浮动主版本最不易过时，浮动次版本最不易产生漏洞。

Conclusion: 该研究为开发者在依赖版本约束选择上提供实证依据，帮助权衡固定与浮动版本约束带来的过时风险与安全风险。

Abstract: Developers consistently use version constraints to specify acceptable
versions of the dependencies for their project. \emph{Pinning} dependencies can
reduce the likelihood of breaking changes, but comes with a cost of manually
managing the replacement of outdated and vulnerable dependencies. On the other
hand, \emph{floating} can be used to automatically get bug fixes and security
fixes, but comes with the risk of breaking changes. Security practitioners
advocate \emph{pinning} dependencies to prevent against software supply chain
attacks, e.g., malicious package updates. However, since \emph{pinning} is the
tightest version constraint, \emph{pinning} is the most likely to result in
outdated dependencies. Nevertheless, how the likelihood of becoming outdated or
vulnerable dependencies changes across version constraint types is unknown. The
goal of this study is to aid developers in making an informed dependency
version constraint choice by empirically evaluating the likelihood of
dependencies becoming outdated or vulnerable across version constraint types at
scale. In this study, we first identify the trends in dependency version
constraint usage and the patterns of version constraint type changes made by
developers in the npm, PyPI, and Cargo ecosystems. We then modeled the
dependency state transitions using survival analysis and estimated how the
likelihood of becoming outdated or vulnerable changes when using \emph{pinning}
as opposed to the rest of the version constraint types. We observe that among
outdated and vulnerable dependencies, the most commonly used version constraint
type is \emph{floating-minor}, with \emph{pinning} being the next most common.
We also find that \emph{floating-major} is the least likely to result in
outdated and \emph{floating-minor} is the least likely to result in vulnerable
dependencies.

</details>


### [112] [Relative Positioning Based Code Chunking Method For Rich Context Retrieval In Repository Level Code Completion Task With Code Language Model](https://arxiv.org/abs/2510.08610)
*Imranur Rahman,Md Rayhanur Rahman*

Main category: cs.SE

TL;DR: 本文提出了一种基于代码分块及语义与语法相似度检索的上下文收集策略，提升大语言模型在代码补全任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有IDE支持代码补全，但缺乏针对如何利用信息构建高质量上下文以提升大语言模型性能的研究。

Method: 将代码库预处理为较小的代码块，利用语法和语义相似性及相对定位进行代码块检索，从而构建更有效的上下文。

Result: 通过代码分块和相对定位，显著提升了代码补全任务的性能。

Conclusion: 有效的上下文构建策略能够帮助大语言模型更好地完成代码补全，提高开发效率。

Abstract: Code completion can help developers improve efficiency and ease the
development lifecycle. Although code completion is available in modern
integrated development environments (IDEs), research lacks in determining what
makes a good context for code completion based on the information available to
the IDEs for the large language models (LLMs) to perform better. In this paper,
we describe an effective context collection strategy to assist the LLMs in
performing better at code completion tasks. The key idea of our strategy is to
preprocess the repository into smaller code chunks and later use syntactic and
semantic similarity-based code chunk retrieval with relative positioning. We
found that code chunking and relative positioning of the chunks in the final
context improve the performance of code completion tasks.

</details>


### [113] [Impact of LLMs on Team Collaboration in Software Development](https://arxiv.org/abs/2510.08612)
*Devang Dhanuka*

Main category: cs.SE

TL;DR: 本文调查了大型语言模型（LLMs）如何影响软件开发生命周期（SDLC）中的团队协作，发现LLMs能提升效率和沟通，但也带来新挑战。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在软件开发中的广泛应用，研究其对团队协作的影响尤为重要。

Method: 通过文献综述、行业案例、团队调查和两个案例研究，评估LLM辅助工具对协作软件工程的影响。

Result: LLMs显著提升了效率、沟通和跨职能协作，但也面临模型局限和隐私问题。

Conclusion: LLMs有助于改进软件团队协作，未来研究应关注模型定制、工具整合和信任安全策略。

Abstract: Large Language Models (LLMs) are increasingly being integrated into software
development processes, with the potential to transform team workflows and
productivity. This paper investigates how LLMs affect team collaboration
throughout the Software Development Life Cycle (SDLC). We reframe and update a
prior study with recent developments as of 2025, incorporating new literature
and case studies. We outline the problem of collaboration hurdles in SDLC and
explore how LLMs can enhance productivity, communication, and decision-making
in a team context. Through literature review, industry examples, a team survey,
and two case studies, we assess the impact of LLM-assisted tools (such as code
generation assistants and AI-powered project management agents) on
collaborative software engineering practices. Our findings indicate that LLMs
can significantly improve efficiency (by automating repetitive tasks and
documentation), enhance communication clarity, and aid cross-functional
collaboration, while also introducing new challenges like model limitations and
privacy concerns. We discuss these benefits and challenges, present research
questions guiding the investigation, evaluate threats to validity, and suggest
future research directions including domain-specific model customization,
improved integration into development tools, and robust strategies for ensuring
trust and security.

</details>


### [114] [Automating Android Build Repair: Bridging the Reasoning-Execution Gap in LLM Agents with Domain-Specific Tools](https://arxiv.org/abs/2510.08640)
*Ha Min Son,Huan Ren,Xin Liu,Zhe Zhao*

Main category: cs.SE

TL;DR: 本文提出了AndroidBuildBench数据集和GradleFixer修复工具，显著提高了Android构建错误的自动修复率。


<details>
  <summary>Details</summary>
Motivation: 自动构建Android应用面临挑战，现有大型语言模型虽有潜力，但对Android构建错误的修复未充分研究。

Method: 构建包含1019个构建错误及其验证修复方案的AndroidBuildBench基准，设计集成领域专用工具的LLM代理GradleFixer，通过领域感知的“Tool Bridging”策略替代通用命令实现高效修复。

Result: GradleFixer实现81.4%的错误修复率，显著优于基于通用Shell的前沿编码代理。

Conclusion: 领域专用工具接口和约束动作空间的Tool Bridging策略有效弥合了模型高层推理与低层执行之间的鸿沟，提高了自动修复Android构建错误的能力。

Abstract: Android is the largest mobile platform, yet automatically building
applications remains a practical challenge. While Large Language Models (LLMs)
show promise for code repair, their use for fixing Android build errors remains
underexplored. To address this gap, we first introduce AndroidBuildBench, a
benchmark of 1,019 build failures curated from the commit histories of 43
open-source Android projects. Each problem is paired with a verified solution
from a subsequent commit, ensuring that fixes are feasible. Second, we propose
GradleFixer, an LLM agent with domain-specific tools for inspecting and
manipulating the Gradle build environment. GradleFixer achieves a resolve rate
of 81.4% (pass@1), significantly outperforming a state-of-the-art coding agent
that relies on a general-purpose shell. GradleFixer's success suggests that
while LLMs possess the high-level knowledge to solve these failures, they
struggle to translate this knowledge into effective low-level actions using a
general-purpose shell. We demonstrate the effectiveness of a strategy we term
Tool Bridging, which replaces general-purpose shell commands with domain-aware
abstractions. We hypothesize this approach works through two mechanisms: 1) it
provides tools in an API-like format that LLMs use more reliably, and 2) it
constrains the action space to relevant operations. This approach bridges the
gap between the model's high-level reasoning and effective low-level execution.

</details>


### [115] [Faver: Boosting LLM-based RTL Generation with Function Abstracted Verifiable Middleware](https://arxiv.org/abs/2510.08664)
*Jianan Mu,Mingyu Shi,Yining Wang,Tianmeng Yang,Bin Sun,Xing Hu,Jing Ye,Huawei Li*

Main category: cs.SE

TL;DR: 论文提出Faver中间件，通过将RTL验证与高层功能抽象分离，提升了基于大型语言模型生成RTL代码的准确率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在生成RTL代码时面临语义差距大和训练数据匮乏的问题，导致生成准确率低。

Method: 提出Faver，通过混合LLM友好的代码结构和规则模板，将电路验证细节解耦，使LLM专注于功能实现。

Result: 在对SFT模型和开源模型的测试中，Faver提升了模型生成准确率最高达14%。

Conclusion: Faver中间件有效解决了LLM在RTL代码生成中面临的验证难题，提高了生成准确性，推动了芯片设计自动化。

Abstract: LLM-based RTL generation is an interesting research direction, as it holds
the potential to liberate the least automated stage in the current chip design.
However, due to the substantial semantic gap between high-level specifications
and RTL, coupled with limited training data, existing models struggle with
generation accuracy. Drawing on human experience, design with verification
helps improving accuracy. However, as the RTL testbench data are even more
scarce, it is not friendly for LLMs. Although LLMs excel at higher-level
languages like Python/C, they have a huge semantic gap from RTL. When
implementing the same functionality, Python/C code and hardware code differ
significantly in the spatiotemporal granularity, requiring the LLM not only to
consider high-level functional semantics but also to ensure the low-level
details align with the circuit code. It is not an easy task. In this paper, we
propose a function abstracted verifiable middleware (Faver) that streamlines
RTL verification in LLM-based workflows. By mixing LLM-friendly code structures
with a rule-based template, Faver decouples the details of circuit
verification, allowing the LLM to focus on the functionality itself. In our
experiments on the SFT model and open-source models, Faver improved the model's
generation accuracy by up to 14%.

</details>


### [116] [RA-Gen: A Controllable Code Generation Framework Using ReAct for Multi-Agent Task Execution](https://arxiv.org/abs/2510.08665)
*Aofan Liu,Haoxuan Li,Bin Wang,Ao Yang,Hui Li*

Main category: cs.SE

TL;DR: 本文提出了一种基于ReAct范式的多智能体可控代码生成框架，通过动态交互实现高效、准确且可解释的代码生成，提升安全性和用户控制能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型生成代码在安全性、准确性和可控性方面存在不足，缺乏对外部工具的动态集成和透明的推理过程。

Method: 设计了包含任务分解的Planner、基于ReAct实现推理和工具集成的Searcher、精确代码生成的CodeGen以及结构化数据提取的Extractor四个协作智能体，实现内外部知识无缝结合。

Result: 在多语言任务上表现优异，在SVEN数据集上利用CodeQL达到了94.8%的安全率，显著优于现有方法。

Conclusion: 该框架通过透明的推理和多智能体协作提升了代码生成的安全性、准确性和用户信任，具备良好的可控性和扩展性。

Abstract: Code generation models based on large language models (LLMs) have gained wide
adoption, but challenges remain in ensuring safety, accuracy, and
controllability, especially for complex tasks. Existing methods often lack
dynamic integration of external tools, transparent reasoning, and user control
over safety. To address these issues, we propose a controllable code generation
framework utilizing the ReAct paradigm for multi-agent task execution. This
framework is a multi-agent system designed to enable efficient, precise, and
interpretable code generation through dynamic interactions between LLMs and
external resources. The framework adopts a collaborative architecture
comprising four specialized agents: a Planner for task decomposition, a
Searcher that leverages the ReAct framework for reasoning and tool integration,
a CodeGen agent for accurate code generation, and an Extractor for structured
data retrieval. The ReAct-based Searcher alternates between generating
reasoning traces and executing actions, facilitating seamless integration of
internal knowledge with external tools (such as search engines) to enhance
accuracy and user control. Experimental results show the framework's
effectiveness across multiple languages, achieving a 94.8% security rate on the
SVEN dataset with CodeQL, outperforming existing approaches. Its transparent
reasoning process fosters user trust and improves controllability.

</details>


### [117] [RAG4Tickets: AI-Powered Ticket Resolution via Retrieval-Augmented Generation on JIRA and GitHub Data](https://arxiv.org/abs/2510.08667)
*Mohammad Baqar*

Main category: cs.SE

TL;DR: 该论文提出了一种基于检索增强生成（RAG）的框架，结合语义嵌入和向量搜索技术，提升软件缺陷单的解决效率和质量。


<details>
  <summary>Details</summary>
Motivation: 现代软件团队由于知识分散在JIRA票据、开发者讨论和GitHub PR中，导致处理相关问题时效率低下。

Method: 利用Sentence-Transformers进行语义嵌入并结合FAISS向量搜索检索相似历史案例，再由大型语言模型结合检索证据生成解决方案。

Result: 实验表明该方法在精准率、召回率、解决时间和开发者接受度等指标上显著提升了解决准确性和修复质量。

Conclusion: 该RAG框架有效整合了JIRA和GitHub数据，实现了知识复用，提升了现代DevOps环境下的问题解决效率和质量。

Abstract: Modern software teams frequently encounter delays in resolving recurring or
related issues due to fragmented knowledge scattered across JIRA tickets,
developer discussions, and GitHub pull requests (PRs). To address this
challenge, we propose a Retrieval-Augmented Generation (RAG) framework that
integrates Sentence-Transformers for semantic embeddings with FAISS-based
vector search to deliver context-aware ticket resolution recommendations. The
approach embeds historical JIRA tickets, user comments, and linked PR metadata
to retrieve semantically similar past cases, which are then synthesized by a
Large Language Model (LLM) into grounded and explainable resolution
suggestions. The framework contributes a unified pipeline linking JIRA and
GitHub data, an embedding and FAISS indexing strategy for heterogeneous
software artifacts, and a resolution generation module guided by retrieved
evidence. Experimental evaluation using precision, recall, resolution time
reduction, and developer acceptance metrics shows that the proposed system
significantly improves resolution accuracy, fix quality, and knowledge reuse in
modern DevOps environments.

</details>


### [118] [BigCodeArena: Unveiling More Reliable Human Preferences in Code Generation via Execution](https://arxiv.org/abs/2510.08697)
*Terry Yue Zhuo,Xiaolong Jin,Hange Liu,Juyong Jiang,Tianyang Liu,Chen Gong,Bhupesh Bishnoi,Vaisakhi Mishra,Marek Suppa,Noah Ziems,Saiteja Utpala,Ming Xu,Guangyu Song,Kaixin Li,Yuhan Cao,Bo Liu,Zheng Liu,Sabina Abdurakhmanova,Wenhao Yu,Mengzhao Jia,Jihan Yao,Kenneth Hamilton,Kumar Shridhar,Minh Chien Vu,Dingmin Wang,Jiawei Liu,Zijian Wang,Qian Liu,Binyuan Hui,Meg Risdal,Ahsen Khaliq,Atin Sood,Zhenchang Xing,Wasi Uddin Ahmad,John Grundy,David Lo,Banghua Zhu,Xiaoning Du,Torsten Scholak,Leandro von Werra*

Main category: cs.SE

TL;DR: BigCodeArena是一个结合代码执行环境的人类评估平台，用于评估LLM生成代码质量，基于收集的多语言、多模型互动数据，提出了BigCodeReward和AutoCodeArena两个基准，用于自动化评分和评估LLM代码能力。


<details>
  <summary>Details</summary>
Motivation: 手动评估LLM生成代码质量困难，需要理解和模拟代码执行，因此引入集成执行环境的人类评估平台，提升代码质量评估的有效性和效率。

Method: 在Chatbot Arena基础上构建BigCodeArena，支持LLM代码执行和人类交互，收集大规模跨语言多模型对话数据，基于数据设计BigCodeReward奖励模型和AutoCodeArena自动Elo评分基准。

Result: 收集了超过14,000个代码对话，标注4,700多对人类偏好样本，发现执行结果可显著提升LLM评估性能，自动评分基准适配多模型评估，GPT-5等专有模型表现领先。

Conclusion: 结合代码执行的开放式人类评估平台有效支持LLM代码能力评估。基于执行结果的奖励模型和自动化评分基准为无人工干预的代码质量评测提供了可行路径。

Abstract: Crowdsourced model evaluation platforms, such as Chatbot Arena, enable
real-time evaluation from human perspectives to assess the quality of model
responses. In the coding domain, manually examining the quality of
LLM-generated content is extremely challenging, as it requires understanding
long chunks of raw code and deliberately simulating code execution. To this
end, we introduce BigCodeArena, an open human evaluation platform for code
generation backed by a comprehensive and on-the-fly execution environment.
Built on top of Chatbot Arena, BigCodeArena enables the execution of
LLM-generated code and allows humans to interact with the execution process and
outcomes. We collected over 14,000 raw code-centric conversation sessions
across 10 widely used LLMs, spanning 10 languages and 8 types of execution
environments. Among these conversations, we identified more than 4,700
multi-turn samples with pairwise human preferences. Further analysis uncovers
underexplored preferences of LLMs in fine-grained domains characterized by
tasks, languages, and frameworks. To systematically examine code understanding
and generation capabilities of frontier LLMs, we curated two benchmarks based
on the collected data, namely BigCodeReward and AutoCodeArena. For
BigCodeReward, we post-processed the 4,700 conversations and evaluated the
consistency between reward models and human preferences. The evaluation shows
that most LLMs have superior performance in judging coding preferences when the
execution results are available. Inspired by these findings, we propose
AutoCodeArena, an automatic Elo rating benchmark designed to assess the coding
quality of LLMs without human involvement. We find that proprietary LLMs like
GPT-5, Claude-Sonnet-4, and Claude-Opus-4 still lead in code generation
performance among recent emerging models.

</details>


### [119] [Search-based Hyperparameter Tuning for Python Unit Test Generation](https://arxiv.org/abs/2510.08716)
*Stephan Lukasczyk,Gordon Fraser*

Main category: cs.SE

TL;DR: 本文研究了使用差分进化算法调优搜索测试生成算法的超参数，显著提升了测试覆盖率，并且效率优于网格搜索。


<details>
  <summary>Details</summary>
Motivation: 搜索测试生成算法有众多配置选项，默认参数往往难以取得最佳效果，调参资源需求高。

Method: 采用差分进化算法对Pynguin框架中DynaMOSA和MIO算法的超参数进行调优。

Result: 调优后的DynaMOSA算法测试覆盖率显著提升，差分进化效率优于基础网格搜索。

Conclusion: 差分进化算法能够有效且高效地优化搜索测试生成算法的超参数，提升测试套件质量。

Abstract: Search-based test-generation algorithms have countless configuration options.
Users rarely adjust these options and usually stick to the default values,
which may not lead to the best possible results. Tuning an algorithm's
hyperparameters is a method to find better hyperparameter values, but it
typically comes with a high demand of resources. Meta-heuristic search
algorithms -- that effectively solve the test-generation problem -- have been
proposed as a solution to also efficiently tune parameters. In this work we
explore the use of differential evolution as a means for tuning the
hyperparameters of the DynaMOSA and MIO many-objective search algorithms as
implemented in the Pynguin framework. Our results show that significant
improvement of the resulting test suite's coverage is possible with the tuned
DynaMOSA algorithm and that differential evolution is more efficient than basic
grid search.

</details>


### [120] [PyMigTool: a tool for end-to-end Python library migration](https://arxiv.org/abs/2510.08810)
*Mohayeminul Islam,Ajay Kumar Jha,May Mahmoud,Sarah Nadi*

Main category: cs.SE

TL;DR: 本文提出了利用大型语言模型（LLMs）结合静态和动态分析的自动化Python库迁移工具PyMigTool。


<details>
  <summary>Details</summary>
Motivation: 手动库迁移耗时且易出错，现有自动化工具多止步于API映射或支持范围有限，需更完善的端到端自动迁移方案。

Method: 基于对321个真实库迁移案例中LLMs能力的研究，结合LLMs、静态分析和动态分析开发PyMigTool。

Result: PyMigTool在717个真实Python项目中表现良好，实现了32%的迁移完全正确，超过一半项目需要开发者修复的变更低于14%。

Conclusion: LLMs结合多种分析技术可有效支持Python库自动迁移，显著减轻开发者负担，提高迁移效率和准确性。

Abstract: Library migration is the process of replacing a library with a similar one in
a software project. Manual library migration is time consuming and error prone,
as it requires developers to understand the Application Programming Interfaces
(API) of both libraries, map equivalent APIs, and perform the necessary code
transformations. Due to the difficulty of the library migration process, most
of the existing automated techniques and tooling stop at the API mapping stage
or support a limited set of libraries and code transformations. In this paper,
we develop an end-to-end solution that can automatically migrate code between
any arbitrary pair of Python libraries that provide similar functionality. Due
to the promising capabilities of Large Language Models (LLMs) in code
generation and transformation, we use LLMs as the primary engine for migration.
Before building the tool, we first study the capabilities of LLMs for library
migration on a benchmark of 321 real-world library migrations. We find that
LLMs can effectively perform library migration, but some post-processing steps
can further improve the performance. Based on this, we develop PyMigTool, a
command line application that combines the power of LLMs, static analysis, and
dynamic analysis to provide accurate library migration. We evaluate PyMigTool
on 717 real-world Python applications that are not from our benchmark. We find
that PyMigTool can migrate 32% of the migrations with complete correctness. Of
the remaining migrations, only 14% of the migration-related changes are left
for developers to fix for more than half of the projects.

</details>


### [121] [McMining: Automated Discovery of Misconceptions in Student Code](https://arxiv.org/abs/2510.08827)
*Erfan Al-Hossami,Razvan Bunescu*

Main category: cs.SE

TL;DR: 本文提出了McMining任务，用于从学生代码样本中挖掘编程误解，并开发了相关基准数据集和两种基于大型语言模型（LLM）的挖掘方法，验证了Gemini、Claude和GPT模型在此任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 学生在学习编程时容易产生误解，导致代码错误和学习效率低下，亟需自动化识别和纠正这些误解的方法。

Method: 提出McMining任务，构建了包含误解及相应代码样本的扩展性基准数据集，同时设计了两种基于大型语言模型（如Gemini、Claude、GPT）的误解挖掘方法，并进行了广泛评估。

Result: 实验结果表明，所提的基于LLM的McMiner在识别学生代码中的编程误解方面表现出良好效果。

Conclusion: 基于大型语言模型的方法能够有效挖掘学生代码中的编程误解，McMining任务和数据集为相关研究提供了有力支持。

Abstract: When learning to code, students often develop misconceptions about various
programming language concepts. These can not only lead to bugs or inefficient
code, but also slow down the learning of related concepts. In this paper, we
introduce McMining, the task of mining programming misconceptions from samples
of code from a student. To enable the training and evaluation of McMining
systems, we develop an extensible benchmark dataset of misconceptions together
with a large set of code samples where these misconceptions are manifested. We
then introduce two LLM-based McMiner approaches and through extensive
evaluations show that models from the Gemini, Claude, and GPT families are
effective at discovering misconceptions in student code.

</details>


### [122] [Identifying Video Game Debugging Bottlenecks: An Industry Perspective](https://arxiv.org/abs/2510.08834)
*Carlos Pinto Gomez,Fabio Petrillo*

Main category: cs.SE

TL;DR: 本文分析了视频游戏开发中调试的独特技术和流程，通过记录20名经验丰富的游戏开发者解决关键性bug的调试过程，揭示调试活动的瓶颈及关键工具，强调技术角色在调试中的核心地位。


<details>
  <summary>Details</summary>
Motivation: 视频游戏调试与传统软件不同，具有独特的需求和技术，理解这些差异有助于提升游戏调试效率。

Method: 通过录制20位资深游戏开发者调试关键bug（包括崩溃、对象行为和持久性问题）的会话，进行主题分析以识别调试活动瓶颈、使用的调试工具及跨学科协作方式。

Result: 发现游戏开发者约36.6%的时间用于检查游戏产物，35.1%的时间用于重现本地bug。分析展示了调试瓶颈及不同技术角色间的协作模式。

Conclusion: 视频游戏调试需要专门的技术和工具，不同技术角色核心参与调试过程。理解这些流程有助于优化游戏开发中的bug解决效率。

Abstract: Conventional debugging techniques used in traditional software are similarly
used when debugging video games. However, the reality of video games require
its own set of unique debugging techniques such as On-Screen Console, Debug
Draws, Debug Camera, Cheats and In-Game Menus, and Data Scrubbing. In this
article, we provide insights from a video game studio on how 20 seasoned
industry game developers debug during the production of a game. Our experiments
rely on the recordings of debugging sessions for the most critical bugs
categorized as Crashes, Object Behaviors, and Object Persistence. In this
paper, we focus on identifying the debugging activities that bottleneck bug
resolution. We also identify the debugging tools used to perform debugging
techniques. Lastly, we present how different disciplines collaborate during
debugging and how technical roles are at the core of debugging. Our thematic
analysis has identified game developers spend 36.6\% of their time inspecting
game artifacts and 35.1\% of their time reproducing the bug locally.

</details>


### [123] [Repository-Aware File Path Retrieval via Fine-Tuned LLMs](https://arxiv.org/abs/2510.08850)
*Vasudha Yanuganti,Ishaan Puri,Swapnil Chhatre,Mantinder Singh,Ashok Jallepalli,Hritvik Shrivastava,Pradeep Kumar Sharma*

Main category: cs.SE

TL;DR: 本文提出了一种利用改进大语言模型直接从自然语言查询中检索代码文件路径的方法，显著提高了文件定位的准确率和召回率。


<details>
  <summary>Details</summary>
Motivation: 现代代码库庞大复杂，传统的代码搜索方法难以捕捉语义和跨文件关联，大语言模型虽懂自然语言但缺乏库内细节，因而难以精准定位相关代码文件。

Method: 基于强大大语言模型Qwen3-8B，采用QLoRA和Unsloth优化技术，结合六种基于抽象语法树和库结构的策略生成训练数据，通过微调实现对给定自然语言查询直接预测代码文件路径。

Result: 在多项目微调后，模型在准确率达到91%、召回率达93%，在如PyTorch这类大库中也能达到59%的召回率，表现出良好的跨文件语义推理及规模适应能力。

Conclusion: 多层次代码信息有助于大语言模型跨文件推理，未来可结合检索机制进一步增强代码智能，虽存在上下文长度限制等挑战，但方法在复杂代码库中的表现优异。

Abstract: Modern codebases make it hard for developers and AI coding assistants to find
the right source files when answering questions like "How does this feature
work?" or "Where was the bug introduced?" Traditional code search (keyword or
IR based) often misses semantic context and cross file links, while large
language models (LLMs) understand natural language but lack repository specific
detail. We present a method for file path retrieval that fine tunes a strong
LLM (Qwen3-8B) with QLoRA and Unsloth optimizations to predict relevant file
paths directly from a natural language query. To build training data, we
introduce six code aware strategies that use abstract syntax tree (AST)
structure and repository content to generate realistic question-answer pairs,
where answers are sets of file paths. The strategies range from single file
prompts to hierarchical repository summaries, providing broad coverage. We fine
tune on Python projects including Flask, Click, Jinja, FastAPI, and PyTorch,
and obtain high retrieval accuracy: up to 91\% exact match and 93\% recall on
held out queries, clearly beating single strategy training. On a large codebase
like PyTorch (about 4,000 Python files), the model reaches 59\% recall, showing
scalability. We analyze how multi level code signals help the LLM reason over
cross file context and discuss dataset design, limits (for example, context
length in very large repos), and future integration of retrieval with LLM based
code intelligence.

</details>


### [124] [Vector Graph-Based Repository Understanding for Issue-Driven File Retrieval](https://arxiv.org/abs/2510.08876)
*Kostiantyn Bevziuk,Andrii Fatula,Svetozar Lashin Yaroslav Opanasenko,Anna Tukhtarova,Ashok Jallepalli Pradeepkumar Sharma,Hritvik Shrivastava*

Main category: cs.SE

TL;DR: 提出了一种将大型软件仓库转化为向量化知识图谱的系统，以反映项目的架构和语义结构，实现语义关系捕捉和自动化开发支持。


<details>
  <summary>Details</summary>
Motivation: 大型软件仓库复杂且庞大，传统方法难以捕捉项目的语义和架构信息，限制了自动化开发的可能性。

Method: 通过构建包含语法关系（如包含、实现、引用、调用、继承）的知识图谱，利用大模型生成的摘要和向量嵌入增强节点，并结合语义检索与图感知扩展的混合检索流程，辅以基于大语言模型的助手进行图请求和解释。

Result: 系统成功将大型仓库结构和语义关系向量化并建模，实现了对仓库内容的高效检索和自动化辅助。

Conclusion: 该系统有效捕获和利用项目的语法与语义关系，为大型软件仓库的管理和自动化开发提供了强有力的支持和工具。

Abstract: We present a repository decomposition system that converts large software
repositories into a vectorized knowledge graph which mirrors project
architectural and semantic structure, capturing semantic relationships and
allowing a significant level of automatization of further repository
development. The graph encodes syntactic relations such as containment,
implementation, references, calls, and inheritance, and augments nodes with
LLM-derived summaries and vector embeddings. A hybrid retrieval pipeline
combines semantic retrieval with graph-aware expansion, and an LLM-based
assistant formulates constrained, read-only graph requests and produces
human-oriented explanations.

</details>


### [125] [SEER: Sustainability Enhanced Engineering of Software Requirements](https://arxiv.org/abs/2510.08981)
*Mandira Roy,Novarun Deb,Nabendu Chaki,Agostino Cortesi*

Main category: cs.SE

TL;DR: 本文提出了SEER框架，旨在软件开发早期阶段识别、评估和优化可持续性需求，实现绿色可持续软件开发。


<details>
  <summary>Details</summary>
Motivation: 当前软件开发快速扩张，对环境、技术、社会和经济产生重大影响，实现联合国可持续发展目标需开发者采用可持续实践，现有方法多为高层指导，缺少从需求工程阶段对可持续性的系统评估。

Method: 设计SEER框架，分三阶段：从通用分类中识别特定软件产品的可持续性需求；基于识别的需求评估系统需求的可持续性；对未满足的需求进行优化。利用大型语言模型和RAG方法实现。

Result: 在四个不同领域的软件项目上应用，利用Gemini 2.5推理模型，SEER展示出在多领域准确识别广泛可持续性问题的有效性。

Conclusion: SEER有效支持在早期需求阶段识别和优化软件可持续性需求，有助于实现绿色软件开发目标。

Abstract: The rapid expansion of software development has significant environmental,
technical, social, and economic impacts. Achieving the United Nations
Sustainable Development Goals by 2030 compels developers to adopt sustainable
practices. Existing methods mostly offer high-level guidelines, which are
time-consuming to implement and rely on team adaptability. Moreover, they focus
on design or implementation, while sustainability assessment should start at
the requirements engineering phase. In this paper, we introduce SEER, a
framework which addresses sustainability concerns in the early software
development phase. The framework operates in three stages: (i) it identifies
sustainability requirements (SRs) relevant to a specific software product from
a general taxonomy; (ii) it evaluates how sustainable system requirements are
based on the identified SRs; and (iii) it optimizes system requirements that
fail to satisfy any SR. The framework is implemented using the reasoning
capabilities of large language models and the agentic RAG (Retrieval Augmented
Generation) approach. SEER has been experimented on four software projects from
different domains. Results generated using Gemini 2.5 reasoning model
demonstrate the effectiveness of the proposed approach in accurately
identifying a broad range of sustainability concerns across diverse domains.

</details>


### [126] [Towards a Taxonomy of Sustainability Requirements for Software Design](https://arxiv.org/abs/2510.08990)
*Mandira Roy,Novarun Deb,Nabendu Chaki,Agostino Cortesi*

Main category: cs.SE

TL;DR: 该论文通过系统文献综述，提出了一个涵盖环境、技术、社会和经济四个维度的可持续性需求分类体系，帮助软件工程社区系统性地理解和管理可持续性需求。


<details>
  <summary>Details</summary>
Motivation: 当前软件可持续性需求研究零散，缺乏统一且综合的分类体系，难以系统地指导软件开发中的可持续性考虑。

Method: 通过系统文献综述提取并组织现有的可持续性需求，构建包括定义、指标及测量方法的全面分类体系，并设计了不同维度间需求的相关矩阵体现协同和冲突。

Result: 建立了一个全面的可持续性需求分类体系，包含定义、指标与衡量手段，及维度间的正负影响关系，支持开发者和研究者在可持续软件开发中的需求制定和权衡。

Conclusion: 该研究填补了软件工程领域可持续性需求分类的空白，为实现综合且系统的可持续软件开发提供理论基础和实践指导。

Abstract: Software systems are a significant contributor to global sustainability
concerns, demanding that environmental, social, technical, and economic factors
be systematically addressed from the initial requirements engineering phase.
Although existing research provides various sustainability requirements (SRs),
these contributions are often fragmented, specific to certain dimensions, or
limited to particular application domains, resulting in a critical lack of a
unified, comprehensive taxonomy for the software engineering community. To
address this gap, this research conducts a Systematic Literature Review (SLR)
to extract and organize sustainability requirements from the state-of-the-art.
The primary contribution is a comprehensive taxonomy of SRs across the four
dimensions of sustainability (environmental, technical, social, and economic).
For each identified category, we provide clear definitions, associated metrics,
and measures. Furthermore, we depict a correlation matrix that projects the
positive and negative influences (synergies and conflicts) among categories
across different dimensions. This systematized reference assists both software
developers and researchers in effectively formulating, managing, and
reconciling trade-offs within sustainable software development.

</details>


### [127] [Saving SWE-Bench: A Benchmark Mutation Approach for Realistic Agent Evaluation](https://arxiv.org/abs/2510.08996)
*Spandan Garg,Ben Steenhoek,Yufan Huang*

Main category: cs.SE

TL;DR: 现有评估软件工程代理的基准多基于GitHub issue，未能反映开发者与聊天编码助手在IDE中的真实交互，导致能力估计偏高。本文提出一种将正式基准转为真实用户查询的框架，应用于多个基准，发现现有测试对部分模型能力高估超50%。


<details>
  <summary>Details</summary>
Motivation: 当前基准基于GitHub issue，无法真实反映开发者与聊天编码助手的互动，导致代理能力被系统性高估，尤其是在修复bug场景。

Method: 设计了一个将正式基准转换为真实用户查询的框架，基于开发者与聊天代理交互的遥测数据分析，应用于多个现有基准，包括公开和私有数据集。

Result: 经过转换后的用户查询测试显示，现有基准对部分模型能力高估超过50%，内测基准高估约10-16%。

Conclusion: 提出基准变异技术，开启通过更贴近真实用户交互的查询形式评估聊天式软件工程代理的新范式。

Abstract: Current benchmarks for evaluating software engineering agents, such as
SWE-Bench Verified, are predominantly derived from GitHub issues and fail to
accurately reflect how developers interact with chat-based coding assistants in
integrated development environments (IDEs). We posit that this mismatch leads
to a systematic overestimation of agent's capabilities in real-world scenarios,
especially bug fixing. We introduce a novel benchmarking framework that
transforms existing formal benchmarks into realistic user queries through
systematic analysis of developer interaction patterns with chat-based agents.
Our methodology is flexible and can be easily extended to existing benchmarks.
In this paper, we apply our testing framework to SWE-Bench Verified, the
TypeScript subset of Multi-SWE-Bench and a private benchmark, SWE-Bench C# and
transform formal GitHub issue descriptions into realistic user-style queries
based on telemetry analysis of a popular chat-based agent interactions. Our
findings reveal that existing benchmarks significantly overestimate agent
capabilities for some models by >50% over baseline performance for public
benchmarks and ~10-16% for our internal benchmark. This work establishes a new
paradigm for evaluating interactive chat-based software engineering agents
through benchmark mutation techniques.

</details>


### [128] [Cost-Efficient Long Code Translation using LLMs while Leveraging Identifier Replacements](https://arxiv.org/abs/2510.09045)
*Manojit Chakraborty,Madhusudan Ghosh,Rishabh Gupta*

Main category: cs.SE

TL;DR: 该论文提出了一种新颖的零样本代码翻译方法，通过标识符替换来提升长代码的翻译准确性。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型(LLMs)在长代码翻译中存在上下文窗口限制，导致翻译不准确。

Method: 通过将用户提供的长标识符替换为通用占位符，减少token数量，使模型能够专注于代码的逻辑结构。

Result: 实验证明，该方法保持了语法和层次信息，显著减少了token数量，提高了长代码翻译的效率和准确性。

Conclusion: 标识符替换策略有效克服了长代码翻译的上下文限制，提升了LLMs的翻译性能和成本效益。

Abstract: In the domain of software development, LLMs have been utilized to automate
tasks such as code translation, where source code from one programming language
is translated to another while preserving its functionality. However, LLMs
often struggle with long source codes that don't fit into the context window,
which produces inaccurate translations. To address this, we propose a novel
zero-shot code translation method that incorporates identifier replacement. By
substituting user-given long identifiers with generalized placeholders during
translation, our method allows the LLM to focus on the logical structure of the
code, by reducing token count and memory usage, which improves the efficiency
and cost-effectiveness of long code translation. Our empirical results
demonstrate that our approach preserves syntactical and hierarchical
information and produces translation results with reduced tokens.

</details>


### [129] [Model-Assisted and Human-Guided: Perceptions and Practices of Software Professionals Using LLMs for Coding](https://arxiv.org/abs/2510.09058)
*Italo Santos,Cleyton Magalhaes,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: 本论文通过全球131名软件从业者的调查，揭示了大语言模型(LLMs)在软件开发中的实际应用、优势及挑战。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型在软件开发中的使用日益普及，但对其实际应用方式及专业人士对其优缺点的认知尚缺乏深入了解。

Method: 通过全球范围内对131名软件从业者进行调查，收集他们使用LLMs的具体情况及看法。

Result: 调查显示LLMs主要用于多种编码任务，带来了生产力提升、认知负担减轻和学习加速等好处；同时用户也担忧LLMs的输出不准确、上下文感知不足及伦理风险。大多数开发者将LLMs视为辅助工具而非独立解决方案。

Conclusion: 论文提供了关于LLMs在软件工程中采用的早期实务视角，强调了未来研究和负责任使用中应关注的关键问题。

Abstract: Large Language Models have quickly become a central component of modern
software development workflows, and software practitioners are increasingly
integrating LLMs into various stages of the software development lifecycle.
Despite the growing presence of LLMs, there is still a limited understanding of
how these tools are actually used in practice and how professionals perceive
their benefits and limitations. This paper presents preliminary findings from a
global survey of 131 software practitioners. Our results reveal how LLMs are
utilized for various coding-specific tasks. Software professionals report
benefits such as increased productivity, reduced cognitive load, and faster
learning, but also raise concerns about LLMs' inaccurate outputs, limited
context awareness, and associated ethical risks. Most developers treat LLMs as
assistive tools rather than standalone solutions, reflecting a cautious yet
practical approach to their integration. Our findings provide an early,
practitioner-focused perspective on LLM adoption, highlighting key
considerations for future research and responsible use in software engineering.

</details>


### [130] [Literate Tracing](https://arxiv.org/abs/2510.09073)
*Matthew Sotoudeh*

Main category: cs.SE

TL;DR: 本文介绍了一种名为"literate tracing"的程序文档范式，通过带注释的具体执行轨迹解释软件系统，并介绍了工具TReX，能生成交互式且准确反映程序语义的文档。


<details>
  <summary>Details</summary>
Motivation: 随着计算机系统变得越来越大和复杂，软件开发中需要专家向新手有效传达程序工作原理，现有文档方式存在缺陷，需要一种更具上下文和具体执行信息的文档方式。

Method: 提出了literate tracing范式，即使用带注释的具体执行轨迹来解释软件系统；开发了TReX工具，生成交互式、可视化且语义准确的literate traces。

Result: 借助TReX成功编写了多个大型系统组件（包括Linux内核、Git和GCC）的literate traces，有效弥补了传统代码注释和设计文档的不足。

Conclusion: literate tracing结合了代码注释和设计文档的优点，能够更好地帮助理解复杂系统，且通过TReX工具能高效制作高质量文档，适用于大型复杂系统解释。

Abstract: As computer systems grow ever larger and more complex, a crucial task in
software development is for one person (the system expert) to communicate to
another (the system novice) how a certain program works. This paper reports on
the author's experiences with a paradigm for program documentation that we call
literate tracing. A literate trace explains a software system using annotated,
concrete execution traces of the system. Literate traces complement both
in-code comments (which often lack global context) and out-of-band design docs
(which often lack a concrete connection to the code). We also describe TReX,
our tool for making literate traces that are interactive, visual, and
guaranteed by construction to be faithful to the program semantics. We have
used TReX to write literate traces explaining components of large systems
software including the Linux kernel, Git source control system, and GCC
compiler.

</details>


### [131] [Constraint-Guided Unit Test Generation for Machine Learning Libraries](https://arxiv.org/abs/2510.09108)
*Lukas Krodinger,Altin Hajdari,Stephan Lukasczyk,Gordon Fraser*

Main category: cs.SE

TL;DR: 本文提出PynguinML，一种改进自动测试生成工具Pynguin的方法，通过利用机器学习库API的输入约束，生成合规输入，提高测试效果和代码覆盖率。


<details>
  <summary>Details</summary>
Motivation: 机器学习库API输入约束复杂，自动测试工具生成的输入不合规，导致测试失败和代码覆盖率低。

Method: 改进Pynguin工具，利用官方API文档中提取的约束生成合规的输入。

Result: 在165个PyTorch和TensorFlow模块上测试，PynguinML实现了最高63.9%的代码覆盖率提升。

Conclusion: PynguinML显著提升了机器学习库的测试效率和代码覆盖率，有助于保障ML库的正确性。

Abstract: Machine learning (ML) libraries such as PyTorch and TensorFlow are essential
for a wide range of modern applications. Ensuring the correctness of ML
libraries through testing is crucial. However, ML APIs often impose strict
input constraints involving complex data structures such as tensors. Automated
test generation tools such as Pynguin are not aware of these constraints and
often create non-compliant inputs. This leads to early test failures and
limited code coverage. Prior work has investigated extracting constraints from
official API documentation. In this paper, we present PynguinML, an approach
that improves the Pynguin test generator to leverage these constraints to
generate compliant inputs for ML APIs, enabling more thorough testing and
higher code coverage. Our evaluation is based on 165 modules from PyTorch and
TensorFlow, comparing PynguinML against Pynguin. The results show that
PynguinML significantly improves test effectiveness, achieving up to 63.9 %
higher code coverage.

</details>


### [132] [A Semantic Framework for Patient Digital Twins in Chronic Care](https://arxiv.org/abs/2510.09134)
*Amal Elgammal,Bernd J. Krämer,Michael P. Papazoglou,Mira Raheem*

Main category: cs.SE

TL;DR: 本文提出一种基于本体驱动的患者医学数字孪生（PMDT）框架，整合多模态健康数据，实现个性化慢性病管理。


<details>
  <summary>Details</summary>
Motivation: 当前数字孪生多局限于器官或单一数据类型，缺乏统一且保护隐私的基础，难以实现个性化和预防性慢病管理。

Method: PMDT基于OWL 2.0实现，构建模块化本体蓝图，融合生理、心理社会、行为及基因组信息，支持语义互操作和自动推理，并通过专家研讨、问卷及EU H2020项目实证验证。

Result: 评估显示本体覆盖全面，推理准确，用户友好，符合法规。PMDT实现了异构数据统一，支持描述性、预测性与处方性分析，保障隐私并促进联邦学习。

Conclusion: PMDT弥合了数据碎片化和语义标准化的鸿沟，为下一代数字健康生态提供经过验证的基础，推动慢性病管理向主动、持续优化和平等方向发展。

Abstract: Personalized chronic care requires the integration of multimodal health data
to enable precise, adaptive, and preventive decision-making. Yet most current
digital twin (DT) applications remain organ-specific or tied to isolated data
types, lacking a unified and privacy-preserving foundation. This paper
introduces the Patient Medical Digital Twin (PMDT), an ontology-driven in
silico patient framework that integrates physiological, psychosocial,
behavioral, and genomic information into a coherent, extensible model.
Implemented in OWL 2.0, the PMDT ensures semantic interoperability, supports
automated reasoning, and enables reuse across diverse clinical contexts. Its
ontology is structured around modular Blueprints (patient, disease and
diagnosis, treatment and follow-up, trajectories, safety, pathways, and adverse
events), formalized through dedicated conceptual views. These were iteratively
refined and validated through expert workshops, questionnaires, and a pilot
study in the EU H2020 QUALITOP project with real-world immunotherapy patients.
Evaluation confirmed ontology coverage, reasoning correctness, usability, and
GDPR compliance. Results demonstrate the PMDT's ability to unify heterogeneous
data, operationalize competency questions, and support descriptive, predictive,
and prescriptive analytics in a federated, privacy-preserving manner. By
bridging gaps in data fragmentation and semantic standardization, the PMDT
provides a validated foundation for next-generation digital health ecosystems,
transforming chronic care toward proactive, continuously optimized, and
equitable management.

</details>


### [133] [A Model-Driven Engineering Approach to AI-Powered Healthcare Platforms](https://arxiv.org/abs/2510.09308)
*Mira Raheem,Amal Elgammal,Michael Papazoglou,Bernd Krämer,Neamat El-Tazi*

Main category: cs.SE

TL;DR: 本文提出了一种针对医疗人工智能的模型驱动工程框架，利用形式元模型和领域特定语言，实现高层规范到运行软件的自动转换，支持多机构合作并保护隐私。


<details>
  <summary>Details</summary>
Motivation: 当前医疗人工智能应用面临数据碎片化、隐私保护和技术复杂性难题，限制了其实用化。

Method: 设计了基于模型驱动工程的医疗人工智能框架，核心是医疗互操作语言（MILA），结合多中心的联合学习架构，实现语义一致性和隐私保护。

Result: 在多中心癌症免疫治疗研究中，所生成功能管线在支持向量机关键任务中准确率达到98.5%和98.3%，同时显著减少人工编码。

Conclusion: 模型驱动工程的元建模、语义整合和自动代码生成为实现互操作、可复现且可信的数字健康平台提供了可行路径。

Abstract: Artificial intelligence (AI) has the potential to transform healthcare by
supporting more accurate diagnoses and personalized treatments. However, its
adoption in practice remains constrained by fragmented data sources, strict
privacy rules, and the technical complexity of building reliable clinical
systems. To address these challenges, we introduce a model driven engineering
(MDE) framework designed specifically for healthcare AI. The framework relies
on formal metamodels, domain-specific languages (DSLs), and automated
transformations to move from high level specifications to running software. At
its core is the Medical Interoperability Language (MILA), a graphical DSL that
enables clinicians and data scientists to define queries and machine learning
pipelines using shared ontologies. When combined with a federated learning
architecture, MILA allows institutions to collaborate without exchanging raw
patient data, ensuring semantic consistency across sites while preserving
privacy. We evaluate this approach in a multi center cancer immunotherapy
study. The generated pipelines delivered strong predictive performance, with
support vector machines achieving up to 98.5 percent and 98.3 percent accuracy
in key tasks, while substantially reducing manual coding effort. These findings
suggest that MDE principles metamodeling, semantic integration, and automated
code generation can provide a practical path toward interoperable,
reproducible, and trustworthy digital health platforms.

</details>


### [134] [TIT: A Tree-Structured Instruction Tuning Approach for LLM-Based Code Translation](https://arxiv.org/abs/2510.09400)
*He Jiang,Yufu Wang,Hao Lin,Peiyu Zou,Zhide Zhou,Ang Jia,Xiaochen Li,Zhilei Ren*

Main category: cs.SE

TL;DR: 提出了TIT方法，通过树结构指令调优有效提升LLM代码翻译准确率，减少语法混淆。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的代码翻译方法容易引入源语言语法特征导致语法混淆，且缺乏细粒度语义对齐，影响翻译质量。

Method: TIT包含三个模块：语法信息表示模块引入语言无关的结构化语法特征；细粒度平行数据增强模块通过语句级对齐生成高质量数据；双阶段树指令调优模块分别进行语法感知微调和代码生成微调。

Result: 实验表明，TIT在多种LLM上实现了代码翻译成功率提升1.22至1.75倍，同时显著减少了语法混淆。

Conclusion: TIT通过结构化语法信息和双阶段调优有效解决了代码翻译中的语法混淆与语义对齐问题，显著提升翻译性能。

Abstract: Large Language Models (LLMs) have shown strong performance in automated
source-to-target code translation through pretraining on extensive code
corpora. However, mainstream LLM-based code translation methods suffer from two
critical limitations. First, they are highly sensitive to language-specific
features, which often introduce source-language syntax or lexicon into the
output, leading to syntactic confusion. Second, they lack fine-grained semantic
alignment due to an over-reliance on function-level parallel datasets,
resulting in semantic misalignment between the translated code and the original
source. To overcome these limitations, we propose TIT, a Tree-structured
Instruction Tuning paradigm for LLM-based code translation. Specifically, TIT
consists of three modules. First, to mitigate syntactic confusion, the
syntactic information representation module integrates language-agnostic
syntactic features via structured parsing. Then, to generate high-quality
fine-grained parallel data, the fine-grained parallel dataset augmentation
module aligns nodes with code segments through statement-level segmentation and
contrastive matching. Finally, we leverage the dual-stage tree instruction
tuning module to alleviate the contextual processing burden on the LLM caused
by the introduction of syntactic information. The first stage employs
syntax-aware fine-tuning to enable the LLM to autonomously comprehend
structured syntactic information, while the second stage utilizes code
generation fine-tuning to guide the model in generating accurate target code
based on function-level syntactic dependencies. The experimental results
demonstrate that the proposed method significantly outperforms existing
approaches in multiple LLMs, achieving a success rate 1.22x-1.75x higher in
code translation while markedly reducing syntactic confusion.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [135] [AgenticAD: A Specialized Multiagent System Framework for Holistic Alzheimer Disease Management](https://arxiv.org/abs/2510.08578)
*Adib Bazgir,Amir Habibdoust,Xing Song,Yuwen Zhang*

Main category: cs.MA

TL;DR: 本文提出了一个多智能体系统框架，整合多种AI技术，全面管理阿尔茨海默病患者的护理和研究支持。


<details>
  <summary>Details</summary>
Motivation: 现有AI应用多集中于单一功能，缺乏系统整合，无法全面支持阿尔茨海默病患者及其护理者。

Method: 设计由八个功能专业的智能体组成的多智能体系统，利用LLM（如GPT-4o、Gemini）、多智能体编排框架、RAG技术、多模态数据处理等技术实现协同工作。

Result: 提出了一个详细的系统架构蓝图，实现了护理支持、数据分析研究和多模态工作流的集成。

Conclusion: 多智能体协作范式为阿尔茨海默病提供更个性化、主动且适应性强的护理和管理解决方案，促进未来系统更好地综合多样化数据提升患者效果，减轻护理负担。

Abstract: Alzheimer's disease (AD) presents a complex, multifaceted challenge to
patients, caregivers, and the healthcare system, necessitating integrated and
dynamic support solutions. While artificial intelligence (AI) offers promising
avenues for intervention, current applications are often siloed, addressing
singular aspects of the disease such as diagnostics or caregiver support
without systemic integration. This paper proposes a novel methodological
framework for a comprehensive, multi-agent system (MAS) designed for holistic
Alzheimer's disease management. The objective is to detail the architecture of
a collaborative ecosystem of specialized AI agents, each engineered to address
a distinct challenge in the AD care continuum, from caregiver support and
multimodal data analysis to automated research and clinical data
interpretation. The proposed framework is composed of eight specialized,
interoperable agents. These agents are categorized by function: (1) Caregiver
and Patient Support, (2) Data Analysis and Research, and (3) Advanced
Multimodal Workflows. The methodology details the technical architecture of
each agent, leveraging a suite of advanced technologies including large
language models (LLMs) such as GPT-4o and Gemini, multi-agent orchestration
frameworks, Retrieval-Augmented Generation (RAG) for evidence-grounded
responses, and specialized tools for web scraping, multimodal data processing,
and in-memory database querying. This paper presents a detailed architectural
blueprint for an integrated AI ecosystem for AD care. By moving beyond
single-purpose tools to a collaborative, multi-agent paradigm, this framework
establishes a foundation for developing more adaptive, personalized, and
proactive solutions. This methodological approach aims to pave the way for
future systems capable of synthesizing diverse data streams to improve patient
outcomes and reduce caregiver burden.

</details>


### [136] [GRPO-GCC: Enhancing Cooperation in Spatial Public Goods Games via Group Relative Policy Optimization with Global Cooperation Constraint](https://arxiv.org/abs/2510.08607)
*Zhaoqilin Yang,Chanchan Li,Tianqi Liu,Hongxin Zhao,Youliang Tian*

Main category: cs.MA

TL;DR: 本文提出了GRPO-GCC框架，通过结合群体相对策略优化和全球合作约束，促进结构化群体中的合作行为，避免极端合作或背叛，提升合作的持续性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 受到集体机构中自我调节合作原则的启发，目标是解决空间公共物品博弈中合作难以持续和稳定的问题。

Method: 引入群体相对策略优化和一个强化中间合作水平激励的全球合作约束，结合群体归一化优势估计、参考锚定KL惩罚和动态调整合作收益的全球激励项。

Result: 实现了合作的快速启动、策略的稳定适应以及长期合作的可持续性。

Conclusion: GRPO-GCC框架通过引入全局信号有效调整激励结构，促进多智能体系统中韧性合作，提供了社会技术系统中多智能体强化学习的新范式。

Abstract: Inspired by the principle of self-regulating cooperation in collective
institutions, we propose the Group Relative Policy Optimization with Global
Cooperation Constraint (GRPO-GCC) framework. This work is the first to
introduce GRPO into spatial public goods games, establishing a new deep
reinforcement learning baseline for structured populations. GRPO-GCC integrates
group relative policy optimization with a global cooperation constraint that
strengthens incentives at intermediate cooperation levels while weakening them
at extremes. This mechanism aligns local decision making with sustainable
collective outcomes and prevents collapse into either universal defection or
unconditional cooperation. The framework advances beyond existing approaches by
combining group-normalized advantage estimation, a reference-anchored KL
penalty, and a global incentive term that dynamically adjusts cooperative
payoffs. As a result, it achieves accelerated cooperation onset, stabilized
policy adaptation, and long-term sustainability. GRPO-GCC demonstrates how a
simple yet global signal can reshape incentives toward resilient cooperation,
and provides a new paradigm for multi-agent reinforcement learning in
socio-technical systems.

</details>


### [137] [Scalable Multi-Agent Path Finding using Collision-Aware Dynamic Alert Mask and a Hybrid Execution Strategy](https://arxiv.org/abs/2510.09469)
*Bharath Muppasani,Ritirupa Dey,Biplav Srivastava,Vignesh Narayanan*

Main category: cs.MA

TL;DR: 提出一种结合分散式路径规划和轻量级集中协调器的多智能体路径规划混合框架，通过强化学习和动态共享的冲突信息实现高效避碰和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统的集中式算法在大规模场景中计算复杂度高，而分布式学习方法虽然可扩展但解决方案质量较差。需要一种能够兼顾计算效率与解决方案质量的混合方法。

Method: 利用强化学习做分散式路径规划，辅以由轻量级集中协调器动态共享的最小化冲突信息（如静态冲突单元标记和简短冲突轨迹），实现冲突的有效解决。

Result: 该方法减少了智能体之间的信息共享量，相较于传统完全集中或分布式方法，在大规模高智能体数量场景中依然能找到可行且无碰撞的解决方案。

Conclusion: 该混合框架在保证解决方案质量的同时提升了计算效率和扩展性，是解决大规模多智能体路径规划的有效方法。

Abstract: Multi-agent pathfinding (MAPF) remains a critical problem in robotics and
autonomous systems, where agents must navigate shared spaces efficiently while
avoiding conflicts. Traditional centralized algorithms that have global
information, such as Conflict-Based Search (CBS), provide high-quality
solutions but become computationally expensive in large-scale scenarios due to
the combinatorial explosion of conflicts that need resolution. Conversely,
distributed approaches that have local information, particularly learning-based
methods, offer better scalability by operating with relaxed information
availability, yet often at the cost of solution quality. To address these
limitations, we propose a hybrid framework that combines decentralized path
planning with a lightweight centralized coordinator. Our framework leverages
reinforcement learning (RL) for decentralized planning, enabling agents to
adapt their planning based on minimal, targeted alerts--such as static
conflict-cell flags or brief conflict tracks--that are dynamically shared
information from the central coordinator for effective conflict resolution. We
empirically study the effect of the information available to an agent on its
planning performance. Our approach reduces the inter-agent information sharing
compared to fully centralized and distributed methods, while still consistently
finding feasible, collision-free solutions--even in large-scale scenarios
having higher agent counts.

</details>
