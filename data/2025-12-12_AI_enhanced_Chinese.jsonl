{"id": "2512.10079", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.10079", "abs": "https://arxiv.org/abs/2512.10079", "authors": ["Federico Formica", "Mark Lawford", "Claudio Menghi"], "title": "Search-based Software Testing Driven by Domain Knowledge: Reflections and New Perspectives", "comment": null, "summary": "Search-based Software Testing (SBST) can automatically generate test cases to search for requirements violations. Unlike manual test case development, it can generate a substantial number of test cases in a limited time. However, SBST does not possess the domain knowledge of engineers. Several techniques have been proposed to integrate engineers' domain knowledge within existing SBST frameworks. This paper will reflect on recent experimental results by highlighting bold and unexpected results. It will help re-examine SBST techniques driven by domain knowledge from a new perspective, suggesting new directions for future research.", "AI": {"tldr": "\u672c\u6587\u56de\u987e\u4e86\u7ed3\u5408\u9886\u57df\u77e5\u8bc6\u7684SBST\u5b9e\u9a8c\uff0c\u5f3a\u8c03\u4e86\u610f\u5916\u7ed3\u679c\uff0c\u5e76\u63d0\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u4f20\u7edf\u7684SBST\u867d\u80fd\u5feb\u901f\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\uff0c\u4f46\u7f3a\u4e4f\u5de5\u7a0b\u5e08\u7684\u9886\u57df\u77e5\u8bc6\uff0c\u5bfc\u81f4\u6d4b\u8bd5\u6548\u679c\u6709\u9650\uff0c\u56e0\u6b64\u9700\u8981\u5c06\u9886\u57df\u77e5\u8bc6\u6574\u5408\u8fdbSBST\u4ee5\u63d0\u5347\u6d4b\u8bd5\u8d28\u91cf\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u9886\u57df\u77e5\u8bc6\u9a71\u52a8\u7684SBST\u6548\u679c\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u6280\u672f\u65b9\u6848\u7684\u8868\u73b0\u4e0e\u5c40\u9650\u3002", "result": "\u672c\u6587\u53cd\u601d\u4e86\u57fa\u4e8e\u641c\u7d22\u7684\u8f6f\u4ef6\u6d4b\u8bd5\uff08SBST\uff09\u7ed3\u5408\u9886\u57df\u77e5\u8bc6\u7684\u6700\u65b0\u5b9e\u9a8c\u6210\u679c\uff0c\u7a81\u51fa\u4e86\u5176\u4e2d\u5927\u80c6\u4e14\u51fa\u4e4e\u610f\u6599\u7684\u7ed3\u679c\u3002\u901a\u8fc7\u5206\u6790\u8fd9\u4e9b\u7ed3\u679c\uff0c\u4f5c\u8005\u91cd\u65b0\u5ba1\u89c6\u4e86\u5229\u7528\u5de5\u7a0b\u5e08\u9886\u57df\u77e5\u8bc6\u9a71\u52a8\u7684SBST\u6280\u672f\uff0c\u6307\u51fa\u5f53\u524d\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u548c\u4e0d\u8db3\u3002\u57fa\u4e8e\u8fd9\u4e9b\u6d1e\u5bdf\uff0c\u6587\u7ae0\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u65b0\u65b9\u5411\uff0c\u65e8\u5728\u63d0\u5347SBST\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6548\u679c\u3002", "conclusion": "\u7ed3\u5408\u9886\u57df\u77e5\u8bc6\u7684SBST\u6280\u672f\u867d\u6709\u8fdb\u5c55\uff0c\u4f46\u4ecd\u5b58\u5728\u6311\u6218\uff0c\u672a\u6765\u5e94\u63a2\u7d22\u65b0\u7684\u6574\u5408\u65b9\u6cd5\u4ee5\u63d0\u5347\u6d4b\u8bd5\u6548\u679c\u3002"}}
{"id": "2512.10173", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.10173", "abs": "https://arxiv.org/abs/2512.10173", "authors": ["Mantas Baksys", "Stefan Zetzsche", "Olivier Bouissou", "Remi Delmas", "Soonho Kong"], "title": "ATLAS: Automated Toolkit for Large-Scale Verified Code Synthesis", "comment": null, "summary": "Large language models have shown potential for program verification, but progress is hindered by the scarcity of verified code for training. We present ATLAS, an automated pipeline that synthesizes verified programs at scale to address this data bottleneck. ATLAS generates complete Dafny programs with specifications, implementations, and proofs, producing 2.7K verified programs from which we extract over 19K training examples--more than 7 per verified program--by decomposing the synthesis process into multiple specialized tasks. Fine-tuning Qwen 2.5 7B Coder on this dataset produces substantial gains: +23 percentage points on DafnyBench and +50 percentage points on DafnySynthesis. These results demonstrate that synthetic verified code can effectively enhance LLM capabilities for formal verification.", "AI": {"tldr": "ATLAS\u81ea\u52a8\u5408\u6210\u9a8c\u8bc1\u7a0b\u5e8f\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7a0b\u5e8f\u9a8c\u8bc1\u4efb\u52a1\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u8bad\u7ec3\u7a0b\u5e8f\u9a8c\u8bc1\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53d7\u9650\u4e8e\u9a8c\u8bc1\u4ee3\u7801\u7a00\u7f3a\uff0c\u4e9f\u9700\u9ad8\u8d28\u91cf\u9a8c\u8bc1\u7a0b\u5e8f\u6570\u636e\u4ee5\u63d0\u5347\u9a8c\u8bc1\u80fd\u529b\u3002", "method": "\u6784\u5efaATLAS\u81ea\u52a8\u5316\u6d41\u6c34\u7ebf\uff0c\u751f\u6210\u5e26\u89c4\u8303\u4e0e\u8bc1\u660e\u7684Dafny\u7a0b\u5e8f\uff0c\u5e76\u5c06\u7a0b\u5e8f\u62c6\u5206\u4e3a\u591a\u4e2a\u5b50\u4efb\u52a1\u751f\u6210\u8bad\u7ec3\u6837\u672c\uff0c\u7528\u4e8e\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86ATLAS\uff0c\u4e00\u4e2a\u81ea\u52a8\u5316\u751f\u6210\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u7a0b\u5e8f\u7684\u6d41\u6c34\u7ebf\uff0c\u65e8\u5728\u89e3\u51b3\u7f3a\u4e4f\u9a8c\u8bc1\u4ee3\u7801\u8bad\u7ec3\u6570\u636e\u7684\u95ee\u9898\u3002ATLAS\u901a\u8fc7\u751f\u6210\u5305\u542b\u89c4\u8303\u3001\u5b9e\u73b0\u53ca\u8bc1\u660e\u7684\u5b8c\u6574Dafny\u7a0b\u5e8f\uff0c\u5408\u6210\u4e862700\u4e2a\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u7a0b\u5e8f\uff0c\u63d0\u53d6\u51fa\u8d85\u8fc719000\u4e2a\u8bad\u7ec3\u6837\u672c\u3002\u57fa\u4e8e\u8fd9\u4e9b\u6837\u672c\u5fae\u8c03Qwen 2.5 7B Coder\u6a21\u578b\uff0c\u5728DafnyBench\u548cDafnySynthesis\u4e0a\u5206\u522b\u5b9e\u73b0\u4e86\u663e\u8457\u63d0\u5347\uff0c\u589e\u52a0\u4e8623\u548c50\u4e2a\u767e\u5206\u70b9\uff0c\u8bc1\u660e\u4e86\u5408\u6210\u9a8c\u8bc1\u4ee3\u7801\u80fd\u663e\u8457\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5f62\u5f0f\u9a8c\u8bc1\u9886\u57df\u7684\u80fd\u529b\u3002", "conclusion": "\u5408\u6210\u7684\u9a8c\u8bc1\u7a0b\u5e8f\u6570\u636e\u80fd\u6709\u6548\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5f62\u5f0f\u7a0b\u5e8f\u9a8c\u8bc1\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2512.10218", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.10218", "abs": "https://arxiv.org/abs/2512.10218", "authors": ["Thanosan Prathifkumar", "Noble Saji Mathews", "Meiyappan Nagappan"], "title": "Does SWE-Bench-Verified Test Agent Ability or Model Memory?", "comment": null, "summary": "SWE-Bench-Verified, a dataset comprising 500 issues, serves as a de facto benchmark for evaluating various large language models (LLMs) on their ability to resolve GitHub issues. But this benchmark may overlap with model training data. If that is true, scores may reflect training recall, not issue-solving skill. To study this, we test two Claude models that frequently appear in top-performing agents submitted to the benchmark. We ask them to find relevant files using only issue text, and then issue text plus file paths. We then run the same setup on BeetleBox and SWE-rebench. Despite both benchmarks involving popular open-source Python projects, models performed 3 times better on SWE-Bench-Verified. They were also 6 times better at finding edited files, without any additional context about the projects themselves. This gap suggests the models may have seen many SWE-Bench-Verified tasks during training. As a result, scores on this benchmark may not reflect an agent's ability to handle real software issues, yet it continues to be used in ways that can misrepresent progress and lead to choices that favour agents that use certain models over strong agent design. Our setup tests the localization step with minimal context to the extent that the task should be logically impossible to solve. Our results show the risk of relying on older popular benchmarks and support the shift toward newer datasets built with contamination in mind.", "AI": {"tldr": "SWE-Bench-Verified\u6570\u636e\u96c6\u4e0e\u8bad\u7ec3\u6570\u636e\u91cd\u53e0\uff0c\u5bfc\u81f4\u6a21\u578b\u8868\u73b0\u88ab\u9ad8\u4f30\uff0c\u4e0d\u9002\u5408\u4f5c\u4e3a\u771f\u5b9e\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u7684\u8bc4\u6d4b\u6807\u51c6\u3002", "motivation": "\u7814\u7a76\u53d1\u73b0SWE-Bench-Verified\u6570\u636e\u96c6\u53ef\u80fd\u4e0e\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u91cd\u53e0\uff0c\u5bfc\u81f4\u8bc4\u6d4b\u7ed3\u679c\u53cd\u6620\u7684\u662f\u8bad\u7ec3\u53ec\u56de\u80fd\u529b\u800c\u975e\u5b9e\u9645\u89e3\u51b3\u95ee\u9898\u7684\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u8ba9\u6a21\u578b\u4ec5\u4f7f\u7528\u95ee\u9898\u6587\u672c\u53ca\u6587\u4ef6\u8def\u5f84\u4e24\u79cd\u4fe1\u606f\u5b9a\u4f4d\u76f8\u5173\u6587\u4ef6\uff0c\u6bd4\u8f83\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u4ee5\u68c0\u6d4b\u6570\u636e\u96c6\u662f\u5426\u88ab\u6a21\u578b\u8bad\u7ec3\u8fc7\u3002", "result": "\u5728SWE-Bench-Verified\u4e0a\u6a21\u578b\u8868\u73b0\u660e\u663e\u4f18\u4e8e\u5176\u4ed6\u4e24\u4e2a\u6570\u636e\u96c6\uff0c\u7279\u522b\u662f\u5728\u5b9a\u4f4d\u88ab\u7f16\u8f91\u6587\u4ef6\u65b9\u9762\u8868\u73b0\u63d0\u5347\u4e866\u500d\uff0c\u8868\u660e\u6a21\u578b\u53ef\u80fd\u770b\u8fc7\u8be5\u6570\u636e\u96c6\u7684\u8bb8\u591a\u4efb\u52a1\u3002", "conclusion": "SWE-Bench-Verified\u57fa\u51c6\u53ef\u80fd\u4e0d\u80fd\u771f\u5b9e\u53cd\u6620\u6a21\u578b\u89e3\u51b3\u5b9e\u9645\u8f6f\u4ef6\u95ee\u9898\u7684\u80fd\u529b\uff0c\u4f9d\u8d56\u6b64\u7c7b\u65e7\u6570\u636e\u96c6\u5b58\u5728\u98ce\u9669\uff0c\u5e94\u8f6c\u5411\u8003\u8651\u6570\u636e\u6c61\u67d3\u95ee\u9898\u7684\u65b0\u6570\u636e\u96c6\u3002"}}
{"id": "2512.10238", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.10238", "abs": "https://arxiv.org/abs/2512.10238", "authors": ["Antu Saha"], "title": "Studying and Automating Issue Resolution for Software Quality", "comment": "3 pages", "summary": "Effective issue resolution is crucial for maintaining software quality. Yet developers frequently encounter challenges such as low-quality issue reports, limited understanding of real-world workflows, and a lack of automated support. This research aims to address these challenges through three complementary directions. First, we enhance issue report quality by proposing techniques that leverage LLM reasoning and application-specific information. Second, we empirically characterize developer workflows in both traditional and AI-augmented systems. Third, we automate cognitively demanding resolution tasks, including buggy UI localization and solution identification, through ML, DL, and LLM-based approaches. Together, our work delivers empirical insights, practical tools, and automated methods to advance AI-driven issue resolution, supporting more maintainable and high-quality software systems.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u63d0\u5347\u95ee\u9898\u62a5\u544a\u8d28\u91cf\u3001\u5206\u6790\u5f00\u53d1\u8005\u5de5\u4f5c\u6d41\u7a0b\u548c\u81ea\u52a8\u5316\u590d\u6742\u89e3\u51b3\u4efb\u52a1\uff0c\u63a8\u52a8\u4e86AI\u8f85\u52a9\u7684\u8f6f\u4ef6\u95ee\u9898\u89e3\u51b3\u6280\u672f\uff0c\u4fc3\u8fdb\u8f6f\u4ef6\u8d28\u91cf\u63d0\u9ad8\u3002", "motivation": "\u8f6f\u4ef6\u95ee\u9898\u89e3\u51b3\u5bf9\u7ef4\u62a4\u8f6f\u4ef6\u8d28\u91cf\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5f00\u53d1\u8005\u5e38\u9762\u4e34\u4f4e\u8d28\u91cf\u95ee\u9898\u62a5\u544a\u3001\u5bf9\u771f\u5b9e\u5de5\u4f5c\u6d41\u7a0b\u7406\u89e3\u6709\u9650\u548c\u7f3a\u4e4f\u81ea\u52a8\u5316\u652f\u6301\u7b49\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e09\u65b9\u9762\u65b9\u6cd5\uff1a1\uff09\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u63a8\u7406\u53ca\u7279\u5b9a\u5e94\u7528\u4fe1\u606f\u63d0\u5347\u95ee\u9898\u62a5\u544a\u8d28\u91cf\uff1b2\uff09\u5b9e\u8bc1\u7814\u7a76\u4f20\u7edf\u4e0eAI\u589e\u5f3a\u7cfb\u7edf\u4e2d\u7684\u5f00\u53d1\u8005\u5de5\u4f5c\u6d41\u7a0b\uff1b3\uff09\u901a\u8fc7\u673a\u5668\u5b66\u4e60(ML)\u3001\u6df1\u5ea6\u5b66\u4e60(DL)\u548cLLM\u6280\u672f\u81ea\u52a8\u5316\u590d\u6742\u95ee\u9898\u89e3\u51b3\u4efb\u52a1\uff0c\u5982\u5b9a\u4f4dUI\u7f3a\u9677\u548c\u65b9\u6848\u8bc6\u522b\u3002", "result": "\u63d0\u5347\u4e86\u95ee\u9898\u62a5\u544a\u8d28\u91cf\uff0c\u6df1\u5165\u7406\u89e3\u4e86\u5f00\u53d1\u8005\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5f00\u53d1\u4e86\u81ea\u52a8\u5316\u89e3\u51b3\u5de5\u5177\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u95ee\u9898\u89e3\u51b3\u6d41\u7a0b\u3002", "conclusion": "\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u548c\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u63a8\u52a8\u4e86AI\u9a71\u52a8\u7684\u95ee\u9898\u89e3\u51b3\u6280\u672f\u53d1\u5c55\uff0c\u6709\u52a9\u4e8e\u6784\u5efa\u53ef\u7ef4\u62a4\u6027\u66f4\u5f3a\u3001\u9ad8\u8d28\u91cf\u7684\u8f6f\u4ef6\u7cfb\u7edf\u3002"}}
{"id": "2512.09939", "categories": ["cs.MA", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09939", "abs": "https://arxiv.org/abs/2512.09939", "authors": ["Stella C. Dong"], "title": "Norm-Governed Multi-Agent Decision-Making in Simulator-Coupled Environments:The Reinsurance Constrained Multi-Agent Simulation Process (R-CMASP)", "comment": null, "summary": "Reinsurance decision-making exhibits the core structural properties that motivate multi-agent models: distributed and asymmetric information, partial observability, heterogeneous epistemic responsibilities, simulator-driven environment dynamics, and binding prudential and regulatory constraints. Deterministic workflow automation cannot meet these requirements, as it lacks the epistemic flexibility, cooperative coordination mechanisms, and norm-sensitive behaviour required for institutional risk-transfer.\n  We propose the Reinsurance Constrained Multi-Agent Simulation Process (R-CMASP), a formal model that extends stochastic games and Dec-POMDPs by adding three missing elements: (i) simulator-coupled transition dynamics grounded in catastrophe, capital, and portfolio engines; (ii) role-specialized agents with structured observability, belief updates, and typed communication; and (iii) a normative feasibility layer encoding solvency, regulatory, and organizational rules as admissibility constraints on joint actions.\n  Using LLM-based agents with tool access and typed message protocols, we show in a domain-calibrated synthetic environment that governed multi-agent coordination yields more stable, coherent, and norm-adherent behaviour than deterministic automation or monolithic LLM baselines--reducing pricing variance, improving capital efficiency, and increasing clause-interpretation accuracy. Embedding prudential norms as admissibility constraints and structuring communication into typed acts measurably enhances equilibrium stability.\n  Overall, the results suggest that regulated, simulator-driven decision environments are most naturally modelled as norm-governed, simulator-coupled multi-agent systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6a21\u62df\u5668\u9a71\u52a8\u52a8\u6001\u548c\u89c4\u8303\u7ea6\u675f\u7684\u591a\u667a\u80fd\u4f53\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u518d\u4fdd\u9669\u51b3\u7b56\u4e2d\u7684\u534f\u8c03\u7a33\u5b9a\u6027\u548c\u89c4\u8303\u9075\u5faa\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u786e\u5b9a\u6027\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u65e0\u6cd5\u6ee1\u8db3\u5206\u5e03\u5f0f\u548c\u4e0d\u5bf9\u79f0\u4fe1\u606f\u3001\u90e8\u5206\u53ef\u89c2\u5bdf\u6027\u3001\u5f02\u8d28\u8ba4\u77e5\u804c\u8d23\u4ee5\u53ca\u76d1\u7ba1\u548c\u5ba1\u614e\u7ea6\u675f\u7b49\u518d\u4fdd\u9669\u51b3\u7b56\u7684\u6838\u5fc3\u7ed3\u6784\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u518d\u4fdd\u9669\u7ea6\u675f\u591a\u667a\u80fd\u4f53\u4eff\u771f\u8fc7\u7a0b\uff08R-CMASP\uff09\uff0c\u6269\u5c55\u4e86\u968f\u673a\u6e38\u620f\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08Dec-POMDPs\uff09\uff0c\u5f15\u5165\u4e86\u6a21\u62df\u5668\u8026\u5408\u7684\u8f6c\u79fb\u52a8\u6001\u3001\u89d2\u8272\u4e13\u95e8\u5316\u7684\u667a\u80fd\u4f53\u53ca\u89c4\u8303\u53ef\u884c\u6027\u5c42\u3002\u91c7\u7528\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u667a\u80fd\u4f53\uff0c\u7ed3\u5408\u5de5\u5177\u8bbf\u95ee\u548c\u7c7b\u578b\u5316\u6d88\u606f\u534f\u8bae\uff0c\u5728\u6821\u51c6\u5408\u6210\u73af\u5883\u4e2d\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u57fa\u4e8e\u89c4\u8303\u7ea6\u675f\u7684\u591a\u667a\u80fd\u4f53\u534f\u8c03\u8868\u73b0\u51fa\u6bd4\u786e\u5b9a\u6027\u81ea\u52a8\u5316\u6216\u5355\u4f53LLM\u57fa\u7ebf\u66f4\u7a33\u5b9a\u3001\u4e00\u81f4\u4e14\u7b26\u5408\u89c4\u8303\u7684\u884c\u4e3a\uff0c\u8868\u73b0\u4e3a\u5b9a\u4ef7\u65b9\u5dee\u964d\u4f4e\u3001\u8d44\u672c\u6548\u7387\u63d0\u5347\u548c\u6761\u6b3e\u89e3\u91ca\u51c6\u786e\u6027\u63d0\u9ad8\u3002\u5d4c\u5165\u5ba1\u614e\u89c4\u8303\u548c\u7ed3\u6784\u5316\u901a\u4fe1\u663e\u8457\u589e\u5f3a\u4e86\u5747\u8861\u7a33\u5b9a\u6027\u3002", "conclusion": "\u53d7\u76d1\u7ba1\u3001\u6a21\u62df\u5668\u9a71\u52a8\u7684\u51b3\u7b56\u73af\u5883\u6700\u9002\u5408\u7528\u89c4\u8303\u9a71\u52a8\u3001\u8026\u5408\u6a21\u62df\u5668\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5efa\u6a21\uff0c\u8fd9\u79cd\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u518d\u4fdd\u9669\u8fc7\u7a0b\u7684\u8868\u73b0\u548c\u5408\u89c4\u6c34\u5e73\u3002"}}
{"id": "2512.10080", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10080", "abs": "https://arxiv.org/abs/2512.10080", "authors": ["Luciano Floridi", "Jessica Morley", "Claudio Novelli", "David Watson"], "title": "What Kind of Reasoning (if any) is an LLM actually doing? On the Stochastic Nature and Abductive Appearance of Large Language Models", "comment": null, "summary": "This article looks at how reasoning works in current Large Language Models (LLMs) that function using the token-completion method. It examines their stochastic nature and their similarity to human abductive reasoning. The argument is that these LLMs create text based on learned patterns rather than performing actual abductive reasoning. When their output seems abductive, this is largely because they are trained on human-generated texts that include reasoning structures. Examples are used to show how LLMs can produce plausible ideas, mimic commonsense reasoning, and give explanatory answers without being grounded in truth, semantics, verification, or understanding, and without performing any real abductive reasoning. This dual nature, where the models have a stochastic base but appear abductive in use, has important consequences for how LLMs are evaluated and applied. They can assist with generating ideas and supporting human thinking, but their outputs must be critically assessed because they cannot identify truth or verify their explanations. The article concludes by addressing five objections to these points, noting some limitations in the analysis, and offering an overall evaluation.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u5f53\u524dLLM\u867d\u7136\u8868\u73b0\u51fa\u6eaf\u56e0\u63a8\u7406\u7279\u5f81\uff0c\u4f46\u5b9e\u9645\u57fa\u4e8e\u6982\u7387\u6a21\u5f0f\u751f\u6210\u6587\u672c\uff0c\u4e0d\u80fd\u8fdb\u884c\u771f\u5b9e\u63a8\u7406\uff0c\u9700\u8c28\u614e\u4f7f\u7528\u548c\u8bc4\u4f30\u3002", "motivation": "\u63a2\u8ba8\u73b0\u6709LLM\u7684\u63a8\u7406\u80fd\u529b\u672c\u8d28\u53ca\u5176\u4e0e\u4eba\u7c7b\u63a8\u7406\u7684\u5173\u7cfb\uff0c\u6f84\u6e05\u8bef\u89e3\u5e76\u6307\u5bfc\u5176\u6b63\u786e\u5e94\u7528\u3002", "method": "\u5206\u6790LLM\u7684\u751f\u6210\u673a\u5236\uff0c\u6bd4\u8f83\u5176\u4e0e\u4eba\u7c7b\u6eaf\u56e0\u63a8\u7406\u7684\u5f02\u540c\uff0c\u7ed3\u5408\u793a\u4f8b\u5c55\u793aLLM\u7684\u751f\u6210\u7279\u6027\u548c\u8868\u73b0\u3002", "result": "\u53d1\u73b0LLM\u57fa\u4e8e\u6982\u7387\u751f\u6210\u6587\u672c\uff0c\u80fd\u6a21\u4eff\u63a8\u7406\u7ed3\u6784\u4f46\u4e0d\u5177\u5907\u9a8c\u8bc1\u548c\u7406\u89e3\u80fd\u529b\uff0c\u5176\u8f93\u51fa\u867d\u6709\u8f85\u52a9\u4f5c\u7528\u4f46\u4e0d\u80fd\u4fdd\u8bc1\u771f\u5b9e\u6027\u3002", "conclusion": "\u5f53\u524d\u7684LLM\u901a\u8fc7\u5b66\u4e60\u6587\u672c\u6a21\u5f0f\u751f\u6210\u8f93\u51fa\uff0c\u867d\u8868\u73b0\u51fa\u7c7b\u4f3c\u6eaf\u56e0\u63a8\u7406\u7684\u7279\u5f81\uff0c\u4f46\u5b9e\u9645\u4e0a\u5e76\u672a\u8fdb\u884c\u771f\u5b9e\u7684\u6eaf\u56e0\u63a8\u7406\uff0c\u56e0\u6b64\u5176\u8f93\u51fa\u9700\u8c28\u614e\u8bc4\u4f30\u3002"}}
{"id": "2512.10393", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10393", "abs": "https://arxiv.org/abs/2512.10393", "authors": ["Guoqiang Chen", "Lingyun Ying", "Ziyang Song", "Daguang Liu", "Qiang Wang", "Zhiqi Wang", "Li Hu", "Shaoyin Cheng", "Weiming Zhang", "Nenghai Yu"], "title": "Cross-modal Retrieval Models for Stripped Binary Analysis", "comment": null, "summary": "LLM-agent based binary code analysis has demonstrated significant potential across a wide range of software security scenarios, including vulnerability detection, malware analysis, etc. In agent workflow, however, retrieving the positive from thousands of stripped binary functions based on user query remains under-studied and challenging, as the absence of symbolic information distinguishes it from source code retrieval. In this paper, we introduce, BinSeek, the first two-stage cross-modal retrieval framework for stripped binary code analysis. It consists of two models: BinSeekEmbedding is trained on large-scale dataset to learn the semantic relevance of the binary code and the natural language description, furthermore, BinSeek-Reranker learns to carefully judge the relevance of the candidate code to the description with context augmentation. To this end, we built an LLM-based data synthesis pipeline to automate training construction, also deriving a domain benchmark for future research. Our evaluation results show that BinSeek achieved the state-of-the-art performance, surpassing the the same scale models by 31.42% in Rec@3 and 27.17% in MRR@3, as well as leading the advanced general-purpose models that have 16 times larger parameters.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86BinSeek\uff0c\u4e00\u79cd\u7528\u4e8e\u5265\u79bb\u4e8c\u8fdb\u5236\u4ee3\u7801\u5206\u6790\u7684\u4e24\u9636\u6bb5\u8de8\u6a21\u6001\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u8bad\u7ec3\u53cc\u6a21\u578b\u5b9e\u73b0\u4ee3\u7801\u4e0e\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u7684\u8bed\u4e49\u5173\u8054\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e8c\u8fdb\u5236\u4ee3\u7801\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5265\u79bb\u4e8c\u8fdb\u5236\u51fd\u6570\u68c0\u7d22\u96be\u4ee5\u5904\u7406\uff0c\u5c24\u5176\u7f3a\u4e4f\u7b26\u53f7\u4fe1\u606f\u4f7f\u5f97\u4f20\u7edf\u6e90\u7801\u68c0\u7d22\u65b9\u6cd5\u96be\u4ee5\u5e94\u7528\uff0c\u4e9f\u9700\u6709\u6548\u7684\u8de8\u6a21\u6001\u68c0\u7d22\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u8de8\u6a21\u6001\u68c0\u7d22\u6846\u67b6\uff0c\u5305\u62ecBinSeekEmbedding\u6a21\u578b\u5b66\u4e60\u4e8c\u8fdb\u5236\u4ee3\u7801\u4e0e\u81ea\u7136\u8bed\u8a00\u7684\u8bed\u4e49\u5173\u8054\uff0c\u4ee5\u53caBinSeek-Reranker\u5229\u7528\u4e0a\u4e0b\u6587\u589e\u5f3a\u5bf9\u5019\u9009\u4ee3\u7801\u7684\u76f8\u5173\u6027\u5224\u65ad\u3002\u540c\u65f6\u6784\u5efa\u4e86\u57fa\u4e8eLLM\u7684\u6570\u636e\u81ea\u52a8\u5408\u6210\u6d41\u6c34\u7ebf\u7528\u4e8e\u8bad\u7ec3\uff0c\u4e14\u5efa\u7acb\u4e86\u9886\u57df\u57fa\u51c6\u6570\u636e\u96c6\u3002", "result": "BinSeek\u5728\u591a\u9879\u6307\u6807\u4e0a\u9886\u5148\u73b0\u6709\u540c\u7c7b\u6a21\u578b\u5927\u5e45\u63d0\u5347\uff0c\u5305\u62ecRec@3\u548cMRR@3\uff0c\u663e\u793a\u51fa\u5f3a\u5927\u7684\u8bed\u4e49\u7406\u89e3\u548c\u68c0\u7d22\u80fd\u529b\u3002", "conclusion": "BinSeek\u5728\u5265\u79bb\u4e8c\u8fdb\u5236\u4ee3\u7801\u68c0\u7d22\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u6307\u6807\uff0c\u4f18\u4e8e\u540c\u7b49\u89c4\u6a21\u6a21\u578b31.42%\uff08Rec@3\uff09\u548c27.17%\uff08MRR@3\uff09\uff0c\u5e76\u8d85\u8fc7\u4e86\u53c2\u6570\u91cf\u592716\u500d\u7684\u901a\u7528\u6a21\u578b\u3002"}}
{"id": "2512.10078", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2512.10078", "abs": "https://arxiv.org/abs/2512.10078", "authors": ["Jingyao Ren", "Eric Ewing", "T. K. Satish Kumar", "Sven Koenig", "Nora Ayanian"], "title": "Empirical Hardness in Multi-Agent Pathfinding: Research Challenges and Opportunities", "comment": "Published in AAMAS-25", "summary": "Multi-agent pathfinding (MAPF) is the problem of finding collision-free paths for a team of agents on a map. Although MAPF is NP-hard, the hardness of solving individual instances varies significantly, revealing a gap between theoretical complexity and actual hardness. This paper outlines three key research challenges in MAPF empirical hardness to understand such phenomena. The first challenge, known as algorithm selection, is determining the best-performing algorithms for a given instance. The second challenge is understanding the key instance features that affect MAPF empirical hardness, such as structural properties like phase transition and backbone/backdoor. The third challenge is how to leverage our knowledge of MAPF empirical hardness to effectively generate hard MAPF instances or diverse benchmark datasets. This work establishes a foundation for future empirical hardness research and encourages deeper investigation into these promising and underexplored areas.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u7684\u7ecf\u9a8c\u96be\u5ea6\u5dee\u5f02\uff0c\u63d0\u51fa\u4e86\u7b97\u6cd5\u9009\u62e9\u3001\u5173\u952e\u7279\u5f81\u548c\u96be\u9898\u751f\u6210\u4e09\u5927\u6311\u6218\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u65b9\u5411\u3002", "motivation": "MAPF\u95ee\u9898\u5b9e\u9645\u96be\u5ea6\u4e0e\u7406\u8bba\u590d\u6742\u6027\u4e0d\u7b26\uff0c\u9700\u7814\u7a76\u5176\u7ecf\u9a8c\u6027\u96be\u5ea6\u7684\u5f71\u54cd\u56e0\u7d20\u53ca\u89c4\u5f8b\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u5927\u7814\u7a76\u6311\u6218\uff1a\u7b97\u6cd5\u9009\u62e9\u3001\u5173\u952e\u7279\u5f81\u5206\u6790\uff08\u5982\u76f8\u53d8\u3001backbone/backdoor\u7ed3\u6784\uff09\u3001\u57fa\u4e8e\u7ecf\u9a8c\u96be\u5ea6\u751f\u6210\u96be\u9898\u5b9e\u4f8b\u548c\u591a\u6837\u57fa\u51c6\u6d4b\u8bd5\u96c6\u3002", "result": "\u660e\u786e\u4e86MAPF\u7ecf\u9a8c\u96be\u5ea6\u7814\u7a76\u7684\u6838\u5fc3\u95ee\u9898\u4e0e\u65b9\u5411\uff0c\u4e3a\u540e\u7eed\u6df1\u5165\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002", "conclusion": "\u63d0\u51fa\u7684\u7814\u7a76\u6846\u67b6\u548c\u6311\u6218\u5c06\u63a8\u52a8MAPF\u7ecf\u9a8c\u96be\u5ea6\u9886\u57df\u7684\u53d1\u5c55\u548c\u66f4\u6709\u6548\u7684\u7b97\u6cd5\u53ca\u6d4b\u8bd5\u96c6\u8bbe\u8ba1\u3002"}}
{"id": "2512.10110", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.10110", "abs": "https://arxiv.org/abs/2512.10110", "authors": ["Yumou Wei", "John Stamper", "Paulo F. Carvalho"], "title": "Generate-Then-Validate: A Novel Question Generation Approach Using Small Language Models", "comment": "Accepted as a full research paper for the 16th International Conference on Learning Analytics and Knowledge (LAK'26)", "summary": "We explore the use of small language models (SLMs) for automatic question generation as a complement to the prevalent use of their large counterparts in learning analytics research. We present a novel question generation pipeline that leverages both the text generation and the probabilistic reasoning abilities of SLMs to generate high-quality questions. Adopting a \"generate-then-validate\" strategy, our pipeline first performs expansive generation to create an abundance of candidate questions and refine them through selective validation based on novel probabilistic reasoning. We conducted two evaluation studies, one with seven human experts and the other with a large language model (LLM), to assess the quality of the generated questions. Most judges (humans or LLMs) agreed that the generated questions had clear answers and generally aligned well with the intended learning objectives. Our findings suggest that an SLM can effectively generate high-quality questions when guided by a well-designed pipeline that leverages its strengths.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u6587\u672c\u751f\u6210\u4e0e\u6982\u7387\u63a8\u7406\u7684\u65b0\u6d41\u7a0b\uff0c\u9a8c\u8bc1\u4e86\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u95ee\u9898\u751f\u6210\u4e2d\u7684\u6709\u6548\u6027\u548c\u9ad8\u8d28\u91cf\u8868\u73b0\u3002", "motivation": "\u63a2\u8ba8\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u5728\u81ea\u52a8\u751f\u6210\u95ee\u9898\u4e0a\u7684\u5e94\u7528\uff0c\u4f5c\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5b66\u4e60\u5206\u6790\u7814\u7a76\u4e2d\u7684\u8865\u5145\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u95ee\u9898\u751f\u6210\u6d41\u7a0b\uff0c\u7ed3\u5408\u4e86SLMs\u7684\u6587\u672c\u751f\u6210\u548c\u6982\u7387\u63a8\u7406\u80fd\u529b\uff0c\u91c7\u7528\u201c\u751f\u6210-\u9a8c\u8bc1\u201d\u7b56\u7565\uff0c\u5148\u5927\u89c4\u6a21\u751f\u6210\u5019\u9009\u95ee\u9898\uff0c\u518d\u901a\u8fc7\u57fa\u4e8e\u65b0\u9896\u6982\u7387\u63a8\u7406\u7684\u9009\u62e9\u6027\u9a8c\u8bc1\u6765\u4f18\u5316\u95ee\u9898\u8d28\u91cf\u3002", "result": "\u901a\u8fc7\u4e24\u6b21\u8bc4\u4f30\u7814\u7a76\uff087\u540d\u4eba\u7c7b\u4e13\u5bb6\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff09\u8bc1\u5b9e\uff0c\u751f\u6210\u7684\u95ee\u9898\u5927\u591a\u6570\u6709\u660e\u786e\u7b54\u6848\uff0c\u4e14\u4e0e\u5b66\u4e60\u76ee\u6807\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "\u5728\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6d41\u7a0b\u5f15\u5bfc\u4e0b\uff0c\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u6709\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u95ee\u9898\uff0c\u53d1\u6325\u5176\u81ea\u8eab\u4f18\u52bf\u3002"}}
{"id": "2512.10415", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10415", "abs": "https://arxiv.org/abs/2512.10415", "authors": ["Devanshu Sahoo", "Vasudev Majhi", "Arjun Neekhra", "Yash Sinha", "Murari Mandal", "Dhruv Kumar"], "title": "How to Trick Your AI TA: A Systematic Study of Academic Jailbreaking in LLM Code Evaluation", "comment": "Under Review", "summary": "The use of Large Language Models (LLMs) as automatic judges for code evaluation is becoming increasingly prevalent in academic environments. But their reliability can be compromised by students who may employ adversarial prompting strategies in order to induce misgrading and secure undeserved academic advantages. In this paper, we present the first large-scale study of jailbreaking LLM-based automated code evaluators in academic context. Our contributions are: (i) We systematically adapt 20+ jailbreaking strategies for jailbreaking AI code evaluators in the academic context, defining a new class of attacks termed academic jailbreaking. (ii) We release a poisoned dataset of 25K adversarial student submissions, specifically designed for the academic code-evaluation setting, sourced from diverse real-world coursework and paired with rubrics and human-graded references, and (iii) In order to capture the multidimensional impact of academic jailbreaking, we systematically adapt and define three jailbreaking metrics (Jailbreak Success Rate, Score Inflation, and Harmfulness). (iv) We comprehensively evalulate the academic jailbreaking attacks using six LLMs. We find that these models exhibit significant vulnerability, particularly to persuasive and role-play-based attacks (up to 97% JSR). Our adversarial dataset and benchmark suite lay the groundwork for next-generation robust LLM-based evaluators in academic code assessment.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u5b66\u672f\u73af\u5883\u4e2d\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u4ee3\u7801\u8bc4\u5224\u7684\u5b89\u5168\u6027\uff0c\u63d0\u51fa\u5e76\u7cfb\u7edf\u6027\u8bc4\u4f30\u4e86\u591a\u79cd\u7ed5\u8fc7\u653b\u51fb\u65b9\u6cd5\uff0c\u6784\u5efa\u4e86\u5bf9\u6297\u6837\u672c\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\uff0c\u4e3a\u540e\u7eed\u63d0\u5347\u8bc4\u5224\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u5960\u5b9a\u57fa\u7840\u3002", "motivation": "\u76ee\u524d\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u81ea\u52a8\u4ee3\u7801\u8bc4\u5224\u5de5\u5177\u5728\u5b66\u672f\u73af\u5883\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5b66\u751f\u53ef\u80fd\u5229\u7528\u5bf9\u6297\u6027\u63d0\u793a\u7b56\u7565\u8fdb\u884c\u8bef\u5224\uff0c\u83b7\u53d6\u4e0d\u516c\u5e73\u7684\u5b66\u672f\u4f18\u52bf\uff0c\u5bfc\u81f4\u6a21\u578b\u8bc4\u5224\u7684\u53ef\u9760\u6027\u53d7\u635f\u3002", "method": "\u7cfb\u7edf\u9002\u914d20\u591a\u79cd\u7ed5\u8fc7\uff08jailbreaking\uff09\u7b56\u7565\uff0c\u5b9a\u4e49\u5b66\u672f\u7ed5\u8fc7\u653b\u51fb\uff0c\u6784\u5efa\u9488\u5bf9\u5b66\u672f\u4ee3\u7801\u8bc4\u5224\u7684\u5e26\u6bd2\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u4e09\u7ef4\u7ed5\u8fc7\u8bc4\u6d4b\u6307\u6807\uff0c\u5728\u516d\u6b3e\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u8fdb\u884c\u7efc\u5408\u8bc4\u4f30\u3002", "result": "\u672c\u6587\u9996\u6b21\u8fdb\u884c\u5927\u89c4\u6a21\u5b66\u672f\u73af\u5883\u4e2d\u57fa\u4e8e\u5927\u6a21\u578b\u4ee3\u7801\u8bc4\u5224\u7cfb\u7edf\u7684\u7ed5\u8fc7\u653b\u51fb\u7814\u7a76\uff0c\u5f00\u53d1\u51fa20\u4f59\u79cd\u7ed5\u8fc7\u7b56\u7565\uff0c\u6784\u5efa\u4e86\u5305\u542b2.5\u4e07\u6761\u5bf9\u6297\u6027\u63d0\u4ea4\u7684\u6570\u636e\u96c6\uff0c\u5b9a\u4e49\u4e86\u4e09\u79cd\u7ed5\u8fc7\u6307\u6807\uff0c\u5e76\u5728\u516d\u6b3e\u5927\u6a21\u578b\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u8bf4\u670d\u529b\u53ca\u89d2\u8272\u626e\u6f14\u653b\u51fb\u4e0b\u7ed5\u8fc7\u6210\u529f\u7387\u9ad8\u8fbe97%\u3002", "conclusion": "\u5927\u6a21\u578b\u4ee3\u7801\u8bc4\u5224\u7cfb\u7edf\u5728\u5b66\u672f\u5e94\u7528\u4e2d\u5b58\u5728\u663e\u8457\u5b89\u5168\u98ce\u9669\uff0c\u5c24\u5176\u6613\u53d7\u5230\u8bf4\u670d\u548c\u89d2\u8272\u626e\u6f14\u7c7b\u653b\u51fb\u3002\u7814\u7a76\u63d0\u51fa\u7684\u6570\u636e\u548c\u6307\u6807\u4e3a\u672a\u6765\u63d0\u5347\u4ee3\u7801\u8bc4\u5224\u7cfb\u7edf\u7684\u9632\u62a4\u80fd\u529b\u63d0\u4f9b\u91cd\u8981\u652f\u6301\u3002"}}
{"id": "2512.10166", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2512.10166", "abs": "https://arxiv.org/abs/2512.10166", "authors": ["Khushiyant"], "title": "Emergent Collective Memory in Decentralized Multi-Agent AI Systems", "comment": "23 pages, 4 figures", "summary": "We demonstrate how collective memory emerges in decentralized multi-agent systems through the interplay between individual agent memory and environmental trace communication. Our agents maintain internal memory states while depositing persistent environmental traces, creating a spatially distributed collective memory without centralized control. Comprehensive validation across five environmental conditions (20x20 to 50x50 grids, 5-20 agents, 50 runs per configuration) reveals a critical asymmetry: individual memory alone provides 68.7% performance improvement over no-memory baselines (1563.87 vs 927.23, p < 0.001), while environmental traces without memory fail completely. This demonstrates that memory functions independently but traces require cognitive infrastructure for interpretation. Systematic density-sweep experiments (rho in [0.049, 0.300], up to 625 agents) validate our theoretical phase transition prediction. On realistic large grids (30x30, 50x50), stigmergic coordination dominates above rho ~ 0.20, with traces outperforming memory by 36-41% on composite metrics despite lower food efficiency. The experimental crossover confirms the predicted critical density rho_c = 0.230 within 13% error.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u96c6\u4f53\u8bb0\u5fc6\u7684\u4ea7\u751f\u673a\u5236\uff0c\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u5b9e\u4e2a\u4f53\u8bb0\u5fc6\u548c\u73af\u5883\u4fe1\u606f\u7684\u534f\u540c\u4f5c\u7528\u53ca\u5176\u4e34\u754c\u5bc6\u5ea6\u7684\u5b58\u5728\u3002", "motivation": "\u7406\u89e3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\uff0c\u5982\u4f55\u901a\u8fc7\u4e2a\u4f53\u548c\u73af\u5883\u4fe1\u606f\u7684\u4ea4\u4e92\u4ea7\u751f\u5206\u5e03\u5f0f\u96c6\u4f53\u8bb0\u5fc6\uff0c\u53ca\u5176\u5bf9\u7cfb\u7edf\u534f\u540c\u6548\u7387\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u4e0d\u540c\u5927\u5c0f\u7f51\u683c\u548c\u591a\u667a\u80fd\u4f53\u6570\u91cf\u7684\u73af\u5883\u4e2d\u8fd0\u884c\u591a\u6b21\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u65e0\u8bb0\u5fc6\u3001\u4ec5\u8bb0\u5fc6\u548c\u4ec5\u73af\u5883\u8f68\u8ff9\u4e09\u79cd\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\uff0c\u7ed3\u5408\u5bc6\u5ea6\u626b\u63cf\u5b9e\u9a8c\u9a8c\u8bc1\u7406\u8bba\u76f8\u53d8\u6a21\u578b\u3002", "result": "\u8be5\u8bba\u6587\u5c55\u793a\u4e86\u53bb\u4e2d\u5fc3\u5316\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u96c6\u4f53\u8bb0\u5fc6\u7684\u5f62\u6210\u673a\u5236\uff0c\u5f3a\u8c03\u4e2a\u4f53\u8bb0\u5fc6\u548c\u73af\u5883\u4fe1\u606f\u7684\u4e92\u52a8\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u4e2a\u4f53\u8bb0\u5fc6\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u7ee9\u6548\uff0c\u800c\u5355\u72ec\u4f9d\u9760\u73af\u5883\u8f68\u8ff9\u65e0\u6cd5\u5b8c\u6210\u4efb\u52a1\u3002\u901a\u8fc7\u591a\u79cd\u73af\u5883\u548c\u5bc6\u5ea6\u7684\u7cfb\u7edf\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u76f8\u53d8\u7406\u8bba\u9884\u6d4b\uff0c\u5e76\u786e\u5b9a\u4e86\u4e34\u754c\u5bc6\u5ea6\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5728\u8f83\u5927\u89c4\u6a21\u73af\u5883\u548c\u9ad8\u5bc6\u5ea6\u6761\u4ef6\u4e0b\uff0c\u73af\u5883\u8f68\u8ff9\uff08stigmergy\uff09\u8d85\u8d8a\u4e86\u4e2a\u4f53\u8bb0\u5fc6\uff0c\u6210\u4e3a\u4e3b\u8981\u534f\u8c03\u673a\u5236\u3002", "conclusion": "\u4e2a\u4f53\u8bb0\u5fc6\u867d\u80fd\u72ec\u7acb\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\uff0c\u4f46\u73af\u5883\u8f68\u8ff9\u9700\u8981\u8ba4\u77e5\u57fa\u7840\u65b9\u80fd\u53d1\u6325\u4f5c\u7528\u3002\u7cfb\u7edf\u5728\u5bc6\u5ea6\u8fbe\u5230\u4e34\u754c\u503c\u540e\uff0c\u73af\u5883\u8f68\u8ff9\u534f\u8c03\u673a\u5236\u5360\u4e3b\u5bfc\uff0c\u663e\u8457\u4f18\u4e8e\u5355\u7eaf\u8bb0\u5fc6\uff0c\u9a8c\u8bc1\u4e86\u7406\u8bba\u76f8\u53d8\u9884\u6d4b\u3002"}}
{"id": "2512.10121", "categories": ["cs.CL", "cs.AI", "cs.CY", "q-fin.GN"], "pdf": "https://arxiv.org/pdf/2512.10121", "abs": "https://arxiv.org/abs/2512.10121", "authors": ["Zhongjie Jiang"], "title": "Workflow is All You Need: Escaping the \"Statistical Smoothing Trap\" via High-Entropy Information Foraging and Adversarial Pacing", "comment": "22 pages, 8 figures. Includes an ecological validity blind test where the Agentic Workflow achieved a 25% acceptance rate in top-tier media, decisively outperforming the SOTA Zero-shot baseline (0%). Features the DNFO-v5 ontology", "summary": "Central to long-form text generation in vertical domains is the \"impossible trinity\" confronting current large language models (LLMs): the simultaneous achievement of low hallucination, deep logical coherence, and personalized expression. This study establishes that this bottleneck arises from existing generative paradigms succumbing to the Statistical Smoothing Trap, a phenomenon that overlooks the high-entropy information acquisition and structured cognitive processes integral to expert-level writing. To address this limitation, we propose the DeepNews Framework, an agentic workflow that explicitly models the implicit cognitive processes of seasoned financial journalists. The framework integrates three core modules: first, a dual-granularity retrieval mechanism grounded in information foraging theory, which enforces a 10:1 saturated information input ratio to mitigate hallucinatory outputs; second, schema-guided strategic planning, a process leveraging domain expert knowledge bases (narrative schemas) and Atomic Blocks to forge a robust logical skeleton; third, adversarial constraint prompting, a technique deploying tactics including Rhythm Break and Logic Fog to disrupt the probabilistic smoothness inherent in model-generated text. Experiments delineate a salient Knowledge Cliff in deep financial reporting: content truthfulness collapses when retrieved context falls below 15,000 characters, while a high-redundancy input exceeding 30,000 characters stabilizes the Hallucination-Free Rate (HFR) above 85%. In an ecological validity blind test conducted with a top-tier Chinese technology media outlet, the DeepNews system--built on a previous-generation model (DeepSeek-V3-0324)-achieved a 25% submission acceptance rate, significantly outperforming the 0% acceptance rate of zero-shot generation by a state-of-the-art (SOTA) model (GPT-5).", "AI": {"tldr": "\u9488\u5bf9\u957f\u6587\u672c\u751f\u6210\u4e2d\u7684\u5e7b\u89c9\u548c\u903b\u8f91\u95ee\u9898\uff0cDeepNews\u6846\u67b6\u878d\u5408\u591a\u7c92\u5ea6\u4fe1\u606f\u68c0\u7d22\u3001\u7ed3\u6784\u5316\u89c4\u5212\u548c\u5bf9\u6297\u7ea6\u675f\uff0c\u6709\u6548\u63d0\u5347\u8d22\u7ecf\u6df1\u5ea6\u62a5\u9053\u7684\u771f\u5b9e\u6027\u548c\u63a5\u53d7\u7387\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u540c\u65f6\u6ee1\u8db3\u957f\u6587\u672c\u751f\u6210\u4e2d\u4f4e\u5e7b\u89c9\u3001\u9ad8\u903b\u8f91\u6027\u548c\u4e2a\u6027\u5316\u8868\u8fbe\u9700\u6c42\uff0c\u539f\u56e0\u5728\u4e8e\u4f20\u7edf\u751f\u6210\u8303\u5f0f\u5ffd\u89c6\u4e86\u4e13\u5bb6\u5199\u4f5c\u6240\u4f9d\u8d56\u7684\u9ad8\u71b5\u4fe1\u606f\u91c7\u96c6\u548c\u7ed3\u6784\u5316\u8ba4\u77e5\u8fc7\u7a0b\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u4fe1\u606f\u91c7\u96c6\u7406\u8bba\u7684\u53cc\u7c92\u5ea6\u68c0\u7d22\u673a\u5236\uff0c\u5229\u7528\u4e13\u5bb6\u77e5\u8bc6\u5e93\u6307\u5bfc\u7684\u89c4\u5212\u65b9\u6848\uff0c\u4ee5\u53ca\u5bf9\u6297\u6027\u7ea6\u675f\u63d0\u793a\u7b56\u7565\uff0c\u51cf\u5c11\u5e7b\u89c9\u5e76\u589e\u5f3a\u903b\u8f91\u7ed3\u6784\u3002", "result": "\u8be5\u8bba\u6587\u56f4\u7ed5\u957f\u7bc7\u5782\u76f4\u9886\u57df\u6587\u672c\u751f\u6210\u4e2d\u7684\u201c\u4e09\u96be\u56f0\u5883\u201d\uff0c\u5373\u4f4e\u5e7b\u89c9\u7387\u3001\u5f3a\u903b\u8f91\u4e00\u81f4\u6027\u548c\u4e2a\u6027\u5316\u8868\u8fbe\uff0c\u63d0\u51fa\u4e86DeepNews\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u8d44\u6df1\u8d22\u7ecf\u8bb0\u8005\u7684\u8ba4\u77e5\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u6587\u672c\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "DeepNews\u6846\u67b6\u6709\u6548\u514b\u670d\u4f20\u7edf\u751f\u6210\u6a21\u578b\u7684\u7edf\u8ba1\u5e73\u6ed1\u9677\u9631\uff0c\u5b9e\u73b0\u9ad8\u771f\u5b9e\u6027\u548c\u903b\u8f91\u6027\uff0c\u663e\u8457\u63d0\u5347\u8d22\u7ecf\u62a5\u9053\u7684\u7a3f\u4ef6\u63a5\u53d7\u7387\u3002"}}
{"id": "2512.10452", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.10452", "abs": "https://arxiv.org/abs/2512.10452", "authors": ["Yang Yang", "Li Kuang", "Jiakun Liu", "Zhongxin Liu", "Yingjie Xia", "David Lo"], "title": "UniCoR: Modality Collaboration for Robust Cross-Language Hybrid Code Retrieval", "comment": "Accepted by the 48th IEEE/ACM International Conference on Software Engineering (ICSE 2026)", "summary": "Effective code retrieval is indispensable and it has become an important paradigm to search code in hybrid mode using both natural language and code snippets. Nevertheless, it remains unclear whether existing approaches can effectively leverage such hybrid queries, particularly in cross-language contexts. We conduct a comprehensive empirical study of representative code models and reveal three challenges: (1) insufficient semantic understanding; (2) inefficient fusion in hybrid code retrieval; and (3) weak generalization in cross-language scenarios. To address these challenges, we propose UniCoR, a novel self-supervised framework that learns Unified Code Representations framework designed to learn unified and robust code representations. Firstly, we design a multi-perspective supervised contrastive learning module to enhance semantic understanding and modality fusion. It aligns representations from multiple perspectives, including code-to-code, natural language-to-code, and natural language-to-natural language, enforcing the model to capture a semantic essence among modalities. Secondly, we introduce a representation distribution consistency learning module to improve cross-language generalization, which explicitly aligns the feature distributions of different programming languages, enabling language-agnostic representation learning. Extensive experiments on both empirical benchmark and large-scale benchmark show that UniCoR outperforms all baseline models, achieving an average improvement of 8.64% in MRR and 11.54% in MAP over the best-performing baseline. Furthermore, UniCoR exhibits stability in hybrid code retrieval and generalization capability in cross-language scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faUniCoR\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u5bf9\u6bd4\u5b66\u4e60\u548c\u5206\u5e03\u4e00\u81f4\u6027\u5b66\u4e60\uff0c\u6709\u6548\u63d0\u5347\u8de8\u8bed\u8a00\u6df7\u5408\u4ee3\u7801\u68c0\u7d22\u7684\u8bed\u4e49\u7406\u89e3\u4e0e\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u68c0\u7d22\u6a21\u578b\u5728\u6df7\u5408\u67e5\u8be2\u548c\u8de8\u8bed\u8a00\u573a\u666f\u4e2d\u7684\u8bed\u4e49\u7406\u89e3\u4e0d\u8db3\u3001\u878d\u5408\u6548\u7387\u4f4e\u4ee5\u53ca\u6cdb\u5316\u80fd\u529b\u5f31\u3002", "method": "\u63d0\u51fa\u4e86UniCoR\u6846\u67b6\uff0c\u5305\u62ec\u591a\u89c6\u89d2\u6709\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u6a21\u5757\u548c\u8868\u793a\u5206\u5e03\u4e00\u81f4\u6027\u5b66\u4e60\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86\u8bed\u4e49\u7406\u89e3\u589e\u5f3a\u3001\u6df7\u5408\u6a21\u6001\u878d\u5408\u548c\u8de8\u8bed\u8a00\u7279\u5f81\u5bf9\u9f50\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cUniCoR\u76f8\u6bd4\u6700\u4f18\u57fa\u7ebf\u6a21\u578bMRR\u63d0\u53478.64%\uff0cMAP\u63d0\u534711.54%\uff0c\u4e14\u5728\u6df7\u5408\u4ee3\u7801\u68c0\u7d22\u548c\u8de8\u8bed\u8a00\u573a\u666f\u8868\u73b0\u7a33\u5b9a\u3002", "conclusion": "UniCoR\u663e\u8457\u63d0\u5347\u4e86\u6df7\u5408\u67e5\u8be2\u548c\u8de8\u8bed\u8a00\u4ee3\u7801\u68c0\u7d22\u7684\u6548\u679c\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u7684\u4e09\u5927\u6311\u6218\uff0c\u5c55\u793a\u4e86\u5f3a\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2512.10610", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2512.10610", "abs": "https://arxiv.org/abs/2512.10610", "authors": ["Xiaopei Tan", "Muyang Fan"], "title": "Thinking While Driving: A Concurrent Framework for Real-Time, LLM-Based Adaptive Routing", "comment": null, "summary": "We present Thinking While Driving, a concurrent routing framework that integrates LLMs into a graph-based traffic environment. Unlike approaches that require agents to stop and deliberate, our system enables LLM-based route planning while agents are moving, significantly reducing intersection wait times. Under high traffic, agents average just 0.75 seconds of decision latency. To coordinate many agents in real-time, we implement a non-blocking asynchronous architecture using Unity coroutines and a dedicated request manager. The environment is a weighted undirected graph with live congestion metrics, updated continuously by the agents to enable shared perception. Our results show LLM-driven agents can dynamically adapt to traffic, reroute around congestion, and exhibit behaviors beyond static pathfinding, all while maintaining real-time performance. This work provides a reproducible framework for future research in adaptive routing and multi-agent cooperation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u667a\u80fd\u4f53\u5728\u79fb\u52a8\u4e2d\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u52a8\u6001\u8def\u5f84\u89c4\u5212\u7684\u6846\u67b6\uff0c\u5927\u5e45\u964d\u4f4e\u4ea4\u901a\u7b49\u5f85\u65f6\u95f4\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u591a\u667a\u80fd\u4f53\u5b9e\u65f6\u534f\u540c\u3002", "motivation": "\u4f20\u7edf\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u9700\u667a\u80fd\u4f53\u505c\u8f66\u8fdb\u884c\u51b3\u7b56\uff0c\u5bfc\u81f4\u4ea4\u53c9\u8def\u53e3\u7b49\u5f85\u65f6\u95f4\u957f\uff0c\u6548\u7387\u4f4e\u3002\u672c\u6587\u65e8\u5728\u5b9e\u73b0\u79fb\u52a8\u4e2d\u52a8\u6001\u8def\u5f84\u89c4\u5212\uff0c\u63d0\u9ad8\u4ea4\u901a\u6548\u7387\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u56fe\u7684\u4ea4\u901a\u73af\u5883\uff0c\u5c06LLM\u96c6\u6210\u5230\u5f02\u6b65\u975e\u963b\u585e\u67b6\u6784\u4e2d\uff0c\u5229\u7528Unity\u534f\u7a0b\u548c\u8bf7\u6c42\u7ba1\u7406\u5668\u5b9e\u73b0\u591a\u667a\u80fd\u4f53\u5b9e\u65f6\u534f\u540c\u51b3\u7b56\u3002", "result": "\u5728\u9ad8\u4ea4\u901a\u5bc6\u5ea6\u60c5\u51b5\u4e0b\uff0c\u667a\u80fd\u4f53\u51b3\u7b56\u5ef6\u8fdf\u4ec5\u4e3a0.75\u79d2\uff0c\u80fd\u591f\u52a8\u6001\u9002\u5e94\u4ea4\u901a\u62e5\u5835\u5e76\u5b9e\u65f6\u91cd\u65b0\u89c4\u5212\u8def\u7ebf\uff0c\u8868\u73b0\u51fa\u8d85\u8d8a\u9759\u6001\u8def\u5f84\u5bfb\u627e\u7684\u884c\u4e3a\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684Thinking While Driving\u6846\u67b6\u5b9e\u73b0\u4e86\u79fb\u52a8\u4e2d\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8def\u5f84\u89c4\u5212\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u4ea4\u901a\u8def\u53e3\u7684\u7b49\u5f85\u65f6\u95f4\uff0c\u5e76\u5728\u9ad8\u4ea4\u901a\u8d1f\u8f7d\u4e0b\u4fdd\u6301\u5b9e\u65f6\u6027\u80fd\u3002"}}
{"id": "2512.10148", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10148", "abs": "https://arxiv.org/abs/2512.10148", "authors": ["Moonsoo Park", "Jeongseok Yun", "Bohyung Kim"], "title": "PARAN: Persona-Augmented Review ANswering system on Food Delivery Review Dataset", "comment": null, "summary": "Personalized review response generation presents a significant challenge in domains where user information is limited, such as food delivery platforms. While large language models (LLMs) offer powerful text generation capabilities, they often produce generic responses when lacking contextual user data, reducing engagement and effectiveness. In this work, we propose a two-stage prompting framework that infers both explicit (e.g., user-stated preferences) and implicit (e.g., demographic or stylistic cues) personas directly from short review texts. These inferred persona attributes are then incorporated into the response generation prompt to produce user-tailored replies. To encourage diverse yet faithful generations, we adjust decoding temperature during inference. We evaluate our method using a real-world dataset collected from a Korean food delivery app, and assess its impact on precision, diversity, and semantic consistency. Our findings highlight the effectiveness of persona-augmented prompting in enhancing the relevance and personalization of automated responses without requiring model fine-tuning.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u7528\u6237\u4fe1\u606f\u6709\u9650\u7684\u573a\u666f\uff0c\u8bbe\u8ba1\u4e86\u901a\u8fc7\u77ed\u8bc4\u63a8\u65ad\u7528\u6237\u89d2\u8272\u5e76\u878d\u5165\u751f\u6210\u63d0\u793a\u7684\u4e24\u9636\u6bb5\u63d0\u793a\u65b9\u6cd5\uff0c\u63d0\u5347\u4e2a\u6027\u5316\u56de\u590d\u7684\u76f8\u5173\u6027\u548c\u591a\u6837\u6027\uff0c\u4e14\u65e0\u9700\u5fae\u8c03\u6a21\u578b\u3002", "motivation": "\u5728\u7528\u6237\u4fe1\u606f\u6709\u9650\u7684\u9886\u57df\uff08\u5982\u98df\u54c1\u914d\u9001\u5e73\u53f0\uff09\u4e2d\uff0c\u4e2a\u6027\u5316\u8bc4\u8bba\u56de\u590d\u751f\u6210\u5b58\u5728\u663e\u8457\u6311\u6218\u3002\u5927\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u7528\u6237\u6570\u636e\u65f6\u5bb9\u6613\u751f\u6210\u901a\u7528\u56de\u590d\uff0c\u964d\u4f4e\u53c2\u4e0e\u5ea6\u548c\u6709\u6548\u6027\u3002", "method": "\u8bbe\u8ba1\u4e24\u9636\u6bb5\u63d0\u793a\u6d41\u7a0b\uff0c\u5148\u4ece\u77ed\u8bc4\u6587\u672c\u63a8\u65ad\u663e\u6027\u548c\u9690\u6027\u7528\u6237\u89d2\u8272\u5c5e\u6027\uff0c\u518d\u5c06\u8fd9\u4e9b\u5c5e\u6027\u52a0\u5165\u751f\u6210\u63d0\u793a\u4e2d\uff0c\u7ed3\u5408\u8c03\u6574\u89e3\u7801\u6e29\u5ea6\u4ee5\u5e73\u8861\u591a\u6837\u6027\u548c\u5fe0\u5b9e\u6027\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u63d0\u793a\u6846\u67b6\uff0c\u4ece\u77ed\u8bc4\u4ef7\u6587\u672c\u4e2d\u63a8\u65ad\u663e\u6027\u548c\u9690\u6027\u7528\u6237\u89d2\u8272\u7279\u5f81\uff0c\u5c06\u5176\u878d\u5165\u56de\u590d\u751f\u6210\u63d0\u793a\uff0c\u901a\u8fc7\u8c03\u6574\u89e3\u7801\u6e29\u5ea6\u63d0\u5347\u751f\u6210\u56de\u590d\u7684\u591a\u6837\u6027\u548c\u5fe0\u5b9e\u5ea6\u3002", "conclusion": "\u57fa\u4e8e\u89d2\u8272\u589e\u5f3a\u7684\u63d0\u793a\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u81ea\u52a8\u56de\u590d\u7684\u4e2a\u6027\u5316\u548c\u76f8\u5173\u6027\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u98df\u54c1\u914d\u9001\u5e94\u7528\uff0c\u4e14\u65e0\u9700\u5bf9\u5927\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u3002"}}
{"id": "2512.10493", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.10493", "abs": "https://arxiv.org/abs/2512.10493", "authors": ["Binquan Zhang", "Li Zhang", "Haoyuan Zhang", "Fang Liu", "Song Wang", "Bo Shen", "An Fu", "Lin Shi"], "title": "Decoding Human-LLM Collaboration in Coding: An Empirical Study of Multi-Turn Conversations in the Wild", "comment": null, "summary": "Large language models (LLMs) are increasingly acting as dynamic conversational interfaces, supporting multi-turn interactions that mimic human-like conversation and facilitate complex tasks like coding. While datasets such as LMSYS-Chat-1M and WildChat capture real-world user-LLM conversations, few studies systematically explore the mechanisms of human-LLM collaboration in coding scenarios. What tortuous paths do users experience during the interaction process? How well do the LLMs follow instructions? Are users satisfied? In this paper, we conduct an empirical analysis on human-LLM coding collaboration using LMSYS-Chat-1M and WildChat datasets to explore the human-LLM collaboration mechanism, LLMs' instruction following ability, and human satisfaction. This study yields interesting findings: 1) Task types shape interaction patterns(linear, star and tree), with code quality optimization favoring linear patterns, design-driven tasks leaning toward tree structures, and queries preferring star patterns; 2) Bug fixing and code refactoring pose greater challenges to LLMs' instruction following, with non-compliance rates notably higher than in information querying; 3) Code quality optimization and requirements-driven development tasks show lower user satisfaction, whereas structured knowledge queries and algorithm designs yield higher levels. These insights offer recommendations for improving LLM interfaces and user satisfaction in coding collaborations, while highlighting avenues for future research on adaptive dialogue systems. We believe this work broadens understanding of human-LLM synergies and supports more effective AI-assisted development.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u4e24\u4e2a\u771f\u5b9e\u5bf9\u8bdd\u6570\u636e\u96c6\u5206\u6790\u4e86\u7f16\u7a0b\u573a\u666f\u4e0b\u4eba\u673a\u534f\u4f5c\uff0c\u53d1\u73b0\u4e0d\u540c\u4efb\u52a1\u5bfc\u81f4\u4ea4\u4e92\u6a21\u5f0f\u591a\u6837\uff0cLLM\u5728\u4ee3\u7801\u4fee\u590d\u7b49\u4efb\u52a1\u4e2d\u6307\u4ee4\u6267\u884c\u96be\u5ea6\u5927\uff0c\u7528\u6237\u6ee1\u610f\u5ea6\u4e0e\u4efb\u52a1\u7c7b\u578b\u76f8\u5173\uff0c\u63d0\u51fa\u6539\u8fdb\u754c\u9762\u548c\u7528\u6237\u4f53\u9a8c\u5efa\u8bae\u3002", "motivation": "\u5c3d\u7ba1\u5df2\u6709\u6570\u636e\u96c6\u8bb0\u5f55\u4e86\u4eba\u673a\u5bf9\u8bdd\uff0c\u4f46\u9488\u5bf9\u7f16\u7a0b\u573a\u666f\u4e2d\u4eba\u673a\u534f\u4f5c\u7684\u673a\u5236\u7814\u7a76\u8f83\u5c11\uff0c\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8be5\u7a7a\u767d\uff0c\u7406\u89e3\u7528\u6237\u4e92\u52a8\u8def\u5f84\u3001LLM\u6267\u884c\u6307\u4ee4\u80fd\u529b\u53ca\u7528\u6237\u4f53\u9a8c\u3002", "method": "\u57fa\u4e8eLMSYS-Chat-1M\u548cWildChat\u4e24\u4e2a\u5927\u578b\u771f\u5b9e\u7528\u6237-LLM\u5bf9\u8bdd\u6570\u636e\u96c6\uff0c\u8fdb\u884c\u5b9e\u8bc1\u5206\u6790\u4ee5\u63a2\u8ba8\u4eba\u673a\u534f\u4f5c\u673a\u5236\u3001LLM\u6307\u4ee4\u9075\u5b88\u5ea6\u53ca\u7528\u6237\u6ee1\u610f\u5ea6\u3002", "result": "\u53d1\u73b0\u4efb\u52a1\u7c7b\u578b\u51b3\u5b9a\u4e86\u4e09\u79cd\u4e3b\u8981\u4ea4\u4e92\u6a21\u5f0f\uff08\u7ebf\u6027\u3001\u661f\u578b\u3001\u6811\u5f62\uff09\uff0c\u4e0d\u540c\u4efb\u52a1\u5bf9LLM\u7684\u6307\u4ee4\u9075\u5b88\u80fd\u529b\u4ea7\u751f\u4e0d\u540c\u5f71\u54cd\uff0c\u4e14\u7528\u6237\u6ee1\u610f\u5ea6\u5728\u4e0d\u540c\u4efb\u52a1\u95f4\u5b58\u5728\u660e\u663e\u5dee\u5f02\u3002", "conclusion": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u4e0d\u540c\u4efb\u52a1\u7c7b\u578b\u4e0b\u4eba\u673a\u534f\u4f5c\u7684\u4ea4\u4e92\u6a21\u5f0f\u5dee\u5f02\uff0c\u6307\u51faLLM\u5728\u9075\u5faa\u7f16\u7a0b\u76f8\u5173\u6307\u4ee4\u65f6\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u5e76\u4e14\u7528\u6237\u6ee1\u610f\u5ea6\u53d7\u4efb\u52a1\u6027\u8d28\u663e\u8457\u5f71\u54cd\u3002"}}
{"id": "2503.18702", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2503.18702", "abs": "https://arxiv.org/abs/2503.18702", "authors": ["David Ph. Shakouri", "Crit Cremers", "Niels O. Schiller"], "title": "Unsupervised Acquisition of Discrete Grammatical Categories", "comment": "34 pages, 3 figures, 7 tables", "summary": "This article presents experiments performed using a computational laboratory environment for language acquisition experiments. It implements a multi-agent system consisting of two agents: an adult language model and a daughter language model that aims to learn the mother language. Crucially, the daughter agent does not have access to the internal knowledge of the mother language model but only to the language exemplars the mother agent generates. These experiments illustrate how this system can be used to acquire abstract grammatical knowledge. We demonstrate how statistical analyses of patterns in the input data corresponding to grammatical categories yield discrete grammatical rules. These rules are subsequently added to the grammatical knowledge of the daughter language model. To this end, hierarchical agglomerative cluster analysis was applied to the utterances consecutively generated by the mother language model. It is argued that this procedure can be used to acquire structures resembling grammatical categories proposed by linguists for natural languages. Thus, it is established that non-trivial grammatical knowledge has been acquired. Moreover, the parameter configuration of this computational laboratory environment determined using training data generated by the mother language model is validated in a second experiment with a test set similarly resulting in the acquisition of non-trivial categories.", "AI": {"tldr": "\u5229\u7528\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u548c\u805a\u7c7b\u5206\u6790\uff0c\u5b50\u4ee3\u8bed\u8a00\u6a21\u578b\u4ece\u6bcd\u8bed\u6a21\u578b\u751f\u6210\u7684\u8bed\u8a00\u5b9e\u4f8b\u4e2d\u6709\u6548\u4e60\u5f97\u62bd\u8c61\u8bed\u6cd5\u89c4\u5219\uff0c\u9a8c\u8bc1\u4e86\u8ba1\u7b97\u5b9e\u9a8c\u73af\u5883\u7684\u6709\u6548\u6027\u3002", "motivation": "\u63a2\u8ba8\u8bed\u8a00\u4e60\u5f97\u4e2d\u5982\u4f55\u901a\u8fc7\u89c2\u5bdf\u8bed\u8a00\u5b9e\u4f8b\u800c\u975e\u5185\u90e8\u77e5\u8bc6\uff0c\u5229\u7528\u8ba1\u7b97\u6a21\u578b\u5b9e\u73b0\u62bd\u8c61\u8bed\u6cd5\u77e5\u8bc6\u7684\u5b66\u4e60\uff0c\u9a8c\u8bc1\u7406\u8bba\u8bed\u8a00\u7c7b\u522b\u7684\u5b9e\u9645\u83b7\u5f97\u3002", "method": "\u8bbe\u8ba1\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5305\u542b\u6210\u4eba\u548c\u5b50\u4ee3\u8bed\u8a00\u6a21\u578b\uff0c\u5b50\u4ee3\u4ec5\u8bbf\u95ee\u6bcd\u6a21\u578b\u751f\u6210\u7684\u8bed\u8a00\u6570\u636e\uff0c\u91c7\u7528\u5c42\u6b21\u51dd\u805a\u805a\u7c7b\u5206\u6790\u6bcd\u8bed\u8a00\u751f\u6210\u7684\u8bdd\u8bed\uff0c\u62bd\u53d6\u4e00\u7ec4\u79bb\u6563\u7684\u8bed\u6cd5\u7c7b\u522b\u89c4\u5219\u3002", "result": "\u672c\u6587\u901a\u8fc7\u4e00\u4e2a\u8ba1\u7b97\u5b9e\u9a8c\u73af\u5883\uff0c\u4f7f\u7528\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6a21\u62df\u8bed\u8a00\u4e60\u5f97\u8fc7\u7a0b\uff0c\u6210\u4eba\u4e0e\u5b50\u4ee3\u8bed\u8a00\u6a21\u578b\u4e92\u52a8\uff0c\u5b50\u4ee3\u901a\u8fc7\u7edf\u8ba1\u6bcd\u6a21\u578b\u751f\u6210\u7684\u8bed\u8a00\u5b9e\u4f8b\u5b66\u4e60\u8bed\u6cd5\u77e5\u8bc6\uff0c\u5e76\u8fd0\u7528\u5c42\u6b21\u805a\u7c7b\u65b9\u6cd5\u63d0\u53d6\u62bd\u8c61\u8bed\u6cd5\u89c4\u5219\uff0c\u5b9e\u73b0\u975e\u5e73\u51e1\u8bed\u6cd5\u77e5\u8bc6\u7684\u83b7\u5f97\u3002", "conclusion": "\u901a\u8fc7\u7edf\u8ba1\u5206\u6790\u548c\u5c42\u6b21\u805a\u7c7b\u63d0\u53d6\u7684\u8bed\u6cd5\u89c4\u5219\u6210\u529f\u5730\u88ab\u5b50\u4ee3\u8bed\u8a00\u6a21\u578b\u5438\u6536\uff0c\u5b9e\u73b0\u4e86\u975e\u5e73\u51e1\u8bed\u6cd5\u77e5\u8bc6\u7684\u5b66\u4e60\uff0c\u4e14\u53c2\u6570\u914d\u7f6e\u5728\u6d4b\u8bd5\u4e2d\u5f97\u5230\u9a8c\u8bc1\u3002"}}
{"id": "2512.10150", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10150", "abs": "https://arxiv.org/abs/2512.10150", "authors": ["Lama Alssum", "Hani Itani", "Hasan Abed Al Kader Hammoud", "Philip Torr", "Adel Bibi", "Bernard Ghanem"], "title": "Unforgotten Safety: Preserving Safety Alignment of Large Language Models with Continual Learning", "comment": null, "summary": "The safety alignment of large language models (LLMs) is becoming increasingly important with their democratization. In this paper, we study the safety degradation that comes with adapting LLMs to new tasks. We attribute this safety compromise to catastrophic forgetting and frame the problem of preserving safety when fine-tuning as a continual learning (CL) problem. We consider the fine-tuning-as-a-service setup where the user uploads their data to a service provider to get a customized model that excels on the user's selected task. We adapt several CL approaches from the literature and systematically evaluate their ability to mitigate safety degradation. These include regularization-based, memory-based, and model merging approaches. We consider two scenarios, (1) benign user data and (2) poisoned user data. Our results demonstrate that CL approaches consistently achieve lower attack success rates than standard fine-tuning. Among these, DER outperforms both other CL methods and existing safety-preserving baselines while maintaining task utility. These findings generalize across three downstream tasks (GSM8K, SST2, Code) and three model families (LLaMA2-7B, Mistral-7B, Gemma-2B), establishing CL as a practical solution to preserve safety.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65f6\u5b89\u5168\u6027\u7684\u9000\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u5c06\u5176\u89c6\u4e3a\u6301\u7eed\u5b66\u4e60\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u79cd\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u51cf\u8f7b\u5b89\u5168\u6027\u4e0b\u964d\uff0cDER\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u666e\u53ca\uff0c\u6a21\u578b\u5728\u9002\u5e94\u65b0\u4efb\u52a1\u65f6\u5b89\u5168\u6027\u964d\u4f4e\u6210\u4e3a\u91cd\u8981\u95ee\u9898\uff0c\u9700\u8981\u6709\u6548\u65b9\u6cd5\u5728\u4fdd\u8bc1\u4efb\u52a1\u6027\u80fd\u7684\u540c\u65f6\u7ef4\u62a4\u6a21\u578b\u5b89\u5168\u6027\u3002", "method": "\u5c06\u6a21\u578b\u5fae\u8c03\u89c6\u4e3a\u6301\u7eed\u5b66\u4e60\u95ee\u9898\uff0c\u91c7\u7528\u6b63\u5219\u5316\u3001\u8bb0\u5fc6\u548c\u6a21\u578b\u5408\u5e76\u4e09\u7c7b\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u5b89\u5168\u6027\u4fdd\u62a4\uff0c\u7cfb\u7edf\u8bc4\u4f30\u5176\u5728\u826f\u6027\u548c\u6709\u6bd2\u7528\u6237\u6570\u636e\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u5728\u964d\u4f4e\u653b\u51fb\u6210\u529f\u7387\u65b9\u9762\u4f18\u4e8e\u6807\u51c6\u5fae\u8c03\uff0cDER\u65b9\u6cd5\u5728\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\u7684\u540c\u65f6\u8868\u73b0\u6700\u4f73\uff0c\u8fd9\u4e00\u7ed3\u8bba\u5728\u4e09\u4e2a\u4e0b\u6e38\u4efb\u52a1\u548c\u4e09\u79cd\u6a21\u578b\u4e0a\u5747\u6210\u7acb\u3002", "conclusion": "\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c24\u5176\u662fDER\uff0c\u80fd\u591f\u6709\u6548\u7f13\u89e3\u5fae\u8c03\u5bfc\u81f4\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u6027\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u5728\u591a\u4efb\u52a1\u548c\u591a\u6a21\u578b\u9a8c\u8bc1\u4e86\u5176\u5b9e\u7528\u6027\u3002"}}
{"id": "2512.10618", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.10618", "abs": "https://arxiv.org/abs/2512.10618", "authors": ["Georgia M. Kapitsaki", "Maria Papoutsoglou", "Christoph Treude", "Ioanna Theophilou"], "title": "Analyzing developer discussions on EU and US privacy legislation compliance in GitHub repositories", "comment": "40 pages", "summary": "Context: Privacy legislation has impacted the way software systems are developed, prompting practitioners to update their implementations. Specifically, the EU General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) have forced the community to focus on users' data privacy. Despite the vast amount of data on developer issues available in GitHub repositories, there is a lack of empirical evidence on the issues developers of Open Source Software discuss to comply with privacy legislation. Method: In this work, we examine such discussions by mining and analyzing 32,820 issues from GitHub repositories. We partially analyzed the dataset automatically to identify law user rights and principles indicated, and manually analyzed a sample of 1,186 issues based on the type of concern addressed. Results: We devised 24 discussion categories placed in six clusters: features/bugs, consent-related, documentation, data storing/sharing, adaptability, and general compliance. Our results show that developers mainly focus on specific user rights from the legislation (right to erasure, right to opt-out, right to access), addressing other rights less frequently, while most discussions concern user consent, user rights functionality, bugs and cookies management. Conclusion: The created taxonomy can help practitioners understand which issues are discussed for law compliance, so that they ensure they address them first in their systems. In addition, the educational community can reshape curricula to better educate future engineers on the privacy law concerns raised, and the research community can identify gaps and areas for improvement to support and accelerate data privacy law compliance.", "AI": {"tldr": "\u901a\u8fc7\u5bf9GitHub\u5f00\u6e90\u9879\u76ee\u4e2d\u5173\u4e8e\u9690\u79c1\u6cd5\u89c4\u5408\u89c4\u8ba8\u8bba\u7684\u5b9e\u8bc1\u5206\u6790\uff0c\u63ed\u793a\u5f00\u53d1\u8005\u5173\u6ce8\u7684\u91cd\u70b9\u7528\u6237\u6743\u5229\u548c\u95ee\u9898\u7c7b\u522b\uff0c\u6784\u5efa\u4e86\u7528\u4e8e\u6307\u5bfc\u5b9e\u8df5\u548c\u6559\u80b2\u7684\u5206\u7c7b\u4f53\u7cfb\u3002", "motivation": "\u9690\u79c1\u6cd5\u89c4\u5982GDPR\u548cCCPA\u5bf9\u8f6f\u4ef6\u5f00\u53d1\u5f71\u54cd\u91cd\u5927\uff0c\u4f46\u7f3a\u4e4f\u9488\u5bf9\u5f00\u6e90\u8f6f\u4ef6\u5f00\u53d1\u8005\u5982\u4f55\u8ba8\u8bba\u5408\u89c4\u95ee\u9898\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\uff0c\u5e2e\u52a9\u7406\u89e3\u548c\u6307\u5bfc\u5408\u89c4\u5b9e\u8df5\u3002", "method": "\u672c\u6587\u901a\u8fc7\u6570\u636e\u6316\u6398\u548c\u5206\u6790GitHub\u4e2d32,820\u6761\u76f8\u5173issue\uff0c\u7ed3\u5408\u81ea\u52a8\u8bc6\u522b\u6cd5\u5f8b\u7528\u6237\u6743\u5229\u53ca\u624b\u52a8\u5206\u7c7b1,186\u6761\u6837\u672cissue\uff0c\u7cfb\u7edf\u5206\u6790\u5f00\u53d1\u8005\u8ba8\u8bba\u5185\u5bb9\u548c\u5173\u6ce8\u70b9\u3002", "result": "\u672c\u6587\u901a\u8fc7\u5206\u6790GitHub\u4e0a32,820\u6761\u5f00\u6e90\u8f6f\u4ef6\u5f00\u53d1\u8005\u5173\u4e8e\u9690\u79c1\u7acb\u6cd5\uff08\u5982GDPR\u548cCCPA\uff09\u5408\u89c4\u6027\u7684\u95ee\u9898\u8ba8\u8bba\uff0c\u8bc6\u522b\u4e86\u5f00\u53d1\u8005\u5173\u6ce8\u7684\u91cd\u70b9\u548c\u5185\u5bb9\u3002\u7814\u7a76\u91c7\u7528\u81ea\u52a8\u548c\u624b\u52a8\u5206\u6790\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u5c06\u8ba8\u8bba\u5185\u5bb9\u5f52\u7eb3\u4e3a\u516d\u5927\u7c7b\uff08\u529f\u80fd/\u6f0f\u6d1e\u3001\u540c\u610f\u76f8\u5173\u3001\u6587\u6863\u3001\u6570\u636e\u5b58\u50a8/\u5171\u4eab\u3001\u9002\u5e94\u6027\u548c\u6574\u4f53\u5408\u89c4\uff09\uff0c\u5e76\u53d1\u73b0\u5f00\u53d1\u8005\u4e3b\u8981\u5173\u6ce8\u7528\u6237\u5220\u9664\u6743\u3001\u9009\u62e9\u9000\u51fa\u6743\u548c\u8bbf\u95ee\u6743\uff0c\u91cd\u70b9\u8ba8\u8bba\u7528\u6237\u540c\u610f\u548c\u6743\u9650\u529f\u80fd\u3001\u6f0f\u6d1e\u53cacookies\u7ba1\u7406\u7b49\u95ee\u9898\u3002", "conclusion": "\u672c\u6587\u6784\u5efa\u7684\u5206\u7c7b\u4f53\u7cfb\u80fd\u591f\u5e2e\u52a9\u8f6f\u4ef6\u5f00\u53d1\u8005\u4f18\u5148\u89e3\u51b3\u4e0e\u9690\u79c1\u6cd5\u89c4\u5408\u89c4\u76f8\u5173\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4e3a\u6559\u80b2\u548c\u7814\u7a76\u63d0\u4f9b\u65b9\u5411\uff0c\u4ee5\u4fc3\u8fdb\u6570\u636e\u9690\u79c1\u6cd5\u89c4\u7684\u6267\u884c\u548c\u6539\u8fdb\u3002"}}
{"id": "2512.10195", "categories": ["cs.CL", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.10195", "abs": "https://arxiv.org/abs/2512.10195", "authors": ["Gyutaek Oh", "Sangjoon Park", "Byung-Hoon Kim"], "title": "AutoMedic: An Automated Evaluation Framework for Clinical Conversational Agents with Medical Dataset Grounding", "comment": null, "summary": "Evaluating large language models (LLMs) has recently emerged as a critical issue for safe and trustworthy application of LLMs in the medical domain. Although a variety of static medical question-answering (QA) benchmarks have been proposed, many aspects remain underexplored, such as the effectiveness of LLMs in generating responses in dynamic, interactive clinical multi-turn conversation situations and the identification of multi-faceted evaluation strategies beyond simple accuracy. However, formally evaluating a dynamic, interactive clinical situation is hindered by its vast combinatorial space of possible patient states and interaction trajectories, making it difficult to standardize and quantitatively measure such scenarios. Here, we introduce AutoMedic, a multi-agent simulation framework that enables automated evaluation of LLMs as clinical conversational agents. AutoMedic transforms off-the-shelf static QA datasets into virtual patient profiles, enabling realistic and clinically grounded multi-turn clinical dialogues between LLM agents. The performance of various clinical conversational agents is then assessed based on our CARE metric, which provides a multi-faceted evaluation standard of clinical conversational accuracy, efficiency/strategy, empathy, and robustness. Our findings, validated by human experts, demonstrate the validity of AutoMedic as an automated evaluation framework for clinical conversational agents, offering practical guidelines for the effective development of LLMs in conversational medical applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAutoMedic\u6846\u67b6\u548cCARE\u6307\u6807\uff0c\u5b9e\u73b0\u533b\u7597\u5927\u6a21\u578b\u591a\u8f6e\u4e34\u5e8a\u5bf9\u8bdd\u7684\u81ea\u52a8\u5316\u3001\u591a\u7ef4\u8bc4\u6d4b\uff0c\u63a8\u52a8\u4e34\u5e8a\u5bf9\u8bdd\u5e94\u7528\u7684\u5b89\u5168\u53ef\u4fe1\u53d1\u5c55\u3002", "motivation": "\u73b0\u6709\u9759\u6001\u533b\u7597\u95ee\u7b54\u8bc4\u6d4b\u65e0\u6cd5\u8986\u76d6\u52a8\u6001\u3001\u591a\u8f6e\u4e34\u5e8a\u5bf9\u8bdd\u7684\u590d\u6742\u573a\u666f\uff0c\u7f3a\u4e4f\u591a\u7ef4\u8bc4\u4f30\u7b56\u7565\uff0c\u5bfc\u81f4\u4e34\u5e8a\u5bf9\u8bdd\u6a21\u578b\u8bc4\u6d4b\u4e0d\u8db3\u3002", "method": "\u63d0\u51faAutoMedic\u591a\u4ee3\u7406\u6a21\u62df\u6846\u67b6\uff0c\u5c06\u9759\u6001\u533b\u7597\u95ee\u7b54\u6570\u636e\u8f6c\u5316\u4e3a\u865a\u62df\u60a3\u8005\u6863\u6848\uff0c\u5b9e\u73b0\u591a\u8f6e\u4e34\u5e8a\u5bf9\u8bdd\u6a21\u62df\u3002\u57fa\u4e8eCARE\u6307\u6807\uff0c\u7efc\u5408\u8bc4\u4f30\u5bf9\u8bdd\u51c6\u786e\u6027\u3001\u6548\u7387\u3001\u5171\u60c5\u53ca\u9c81\u68d2\u6027\u3002", "result": "AutoMedic\u6210\u529f\u5b9e\u73b0\u81ea\u52a8\u5316\u4e34\u5e8a\u5bf9\u8bdd\u8bc4\u6d4b\uff0cCARE\u6307\u6807\u591a\u7ef4\u5ea6\u53cd\u6620\u6a21\u578b\u6027\u80fd\uff0c\u7ed3\u679c\u7ecf\u4e13\u5bb6\u9a8c\u8bc1\uff0c\u4f53\u73b0\u4e86\u6846\u67b6\u5728\u5b9e\u9645\u4e34\u5e8a\u5bf9\u8bdd\u8bc4\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "AutoMedic\u6709\u6548\u586b\u8865\u4e86\u52a8\u6001\u4e34\u5e8a\u5bf9\u8bdd\u8bc4\u6d4b\u7684\u7a7a\u767d\uff0c\u4e3a\u533b\u7597\u5927\u6a21\u578b\u7684\u5f00\u53d1\u548c\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u81ea\u52a8\u5316\u3001\u591a\u7ef4\u8bc4\u6d4b\u5de5\u5177\uff0c\u4fc3\u8fdb\u4e86\u4e34\u5e8a\u5bf9\u8bdd\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.10713", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10713", "abs": "https://arxiv.org/abs/2512.10713", "authors": ["Itay Dreyfuss", "Antonio Abu Nassar", "Samuel Ackerman", "Axel Ben David", "Rami Katan", "Orna Raz", "Marcel Zalmanovici"], "title": "PACIFIC: a framework for generating benchmarks to check Precise Automatically Checked Instruction Following In Code", "comment": null, "summary": "Large Language Model (LLM)-based code assistants have emerged as a powerful application of generative AI, demonstrating impressive capabilities in code generation and comprehension. A key requirement for these systems is their ability to accurately follow user instructions. We present Precise Automatically Checked Instruction Following In Code (PACIFIC), a novel framework designed to automatically generate benchmarks that rigorously assess sequential instruction-following and code dry-running capabilities in LLMs, while allowing control over benchmark difficulty. PACIFIC produces benchmark variants with clearly defined expected outputs, enabling straightforward and reliable evaluation through simple output comparisons. In contrast to existing approaches that often rely on tool usage or agentic behavior, our work isolates and evaluates the LLM's intrinsic ability to reason through code behavior step-by-step without execution (dry running) and to follow instructions. Furthermore, our framework mitigates training data contamination by facilitating effortless generation of novel benchmark variations. We validate our framework by generating a suite of benchmarks spanning a range of difficulty levels and evaluating multiple state-of-the-art LLMs. Our results demonstrate that PACIFIC can produce increasingly challenging benchmarks that effectively differentiate instruction-following and dry running capabilities, even among advanced models. Overall, our framework offers a scalable, contamination-resilient methodology for assessing core competencies of LLMs in code-related tasks.", "AI": {"tldr": "PACIFIC\u6846\u67b6\u81ea\u52a8\u751f\u6210\u4e25\u683c\u8bc4\u6d4bLLM\u4ee3\u7801\u6307\u4ee4\u6267\u884c\u548c\u5e72\u8fd0\u884c\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u9a8c\u8bc1\u591a\u6a21\u578b\u8868\u73b0\uff0c\u63d0\u4f9b\u6297\u6c61\u67d3\u4e14\u53ef\u63a7\u96be\u5ea6\u7684\u8bc4\u6d4b\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7801\u52a9\u624b\u9700\u8981\u5177\u5907\u51c6\u786e\u9075\u5faa\u7528\u6237\u6307\u4ee4\u7684\u80fd\u529b\uff0c\u4e14\u73b0\u6709\u8bc4\u6d4b\u65b9\u6cd5\u53d7\u9650\u4e8e\u5de5\u5177\u4f7f\u7528\u6216\u4ee3\u7406\u884c\u4e3a\uff0c\u96be\u4ee5\u6709\u6548\u8861\u91cf\u6a21\u578b\u5185\u5728\u7684\u4ee3\u7801\u63a8\u7406\u548c\u6307\u4ee4\u6267\u884c\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86PACIFIC\u6846\u67b6\uff0c\u81ea\u52a8\u751f\u6210\u4e0d\u540c\u96be\u5ea6\u7ea7\u522b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u80fd\u4e25\u683c\u8bc4\u4f30\u6a21\u578b\u7684\u987a\u5e8f\u6307\u4ee4\u9075\u5faa\u548c\u4ee3\u7801\u5e72\u8fd0\u884c\uff08dry running\uff09\u80fd\u529b\uff0c\u4e14\u652f\u6301\u901a\u8fc7\u7b80\u5355\u8f93\u51fa\u5bf9\u6bd4\u8fdb\u884c\u53ef\u9760\u8bc4\u4f30\uff0c\u5e76\u80fd\u907f\u514d\u8bad\u7ec3\u6570\u636e\u6c61\u67d3\u3002", "result": "\u901a\u8fc7\u751f\u6210\u591a\u5957\u4e0d\u540c\u96be\u5ea6\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u4e86\u591a\u6b3e\u5148\u8fdbLLM\uff0c\u9a8c\u8bc1\u4e86PACIFIC\u80fd\u6709\u6548\u533a\u5206\u6a21\u578b\u5728\u6307\u4ee4\u9075\u5faa\u548c\u4ee3\u7801\u5e72\u8fd0\u884c\u80fd\u529b\u4e0a\u7684\u8868\u73b0\uff0c\u4e14\u6d4b\u8bd5\u96be\u5ea6\u53ef\u63a7\u3002", "conclusion": "PACIFIC\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6297\u6570\u636e\u6c61\u67d3\u7684\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u8bc4\u4f30LLM\u5728\u4ee3\u7801\u76f8\u5173\u4efb\u52a1\u4e2d\u7684\u6838\u5fc3\u80fd\u529b\uff0c\u63a8\u52a8\u4e86\u6a21\u578b\u80fd\u529b\u7684\u7cbe\u7ec6\u5316\u6d4b\u8bd5\u4e0e\u63d0\u5347\u3002"}}
{"id": "2512.10336", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10336", "abs": "https://arxiv.org/abs/2512.10336", "authors": ["Jules Lahmi", "Alexis Roger"], "title": "Multilingual VLM Training: Adapting an English-Trained VLM to French", "comment": null, "summary": "Artificial intelligence has made great progress in recent years, particularly in the development of Vision--Language Models (VLMs) that understand both visual and textual data. However, these advancements remain largely limited to English, reducing their accessibility for non--English speakers. It is essential to extend these capabilities to a broader range of languages. This paper explores the challenges of adapting an English-trained VLM to different languages. To this end, we will explore and compare different methods for their performance and computational cost. We consider a translation-based pipeline, LoRA finetuning, and a two-stage finetuning strategy that separates vision adaptation from language adaptation. To evaluate these methods, we use a combination of standard multimodal benchmarks translated into the target language and manual assessments by native experts. The results reveal that dataset translation remains a major bottleneck in multilingual VLM performance, with data quality limiting the effectiveness of training and evaluation. These findings suggest that future efforts should focus on native-language dataset collection and improved translation strategies.", "AI": {"tldr": "\u591a\u8bed\u8a00\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u53d7\u9650\u4e8e\u6570\u636e\u7ffb\u8bd1\u8d28\u91cf\uff0c\u63d0\u5347\u672c\u5730\u8bed\u8a00\u6570\u636e\u96c6\u548c\u7ffb\u8bd1\u7b56\u7565\u662f\u672a\u6765\u5173\u952e\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4e3b\u8981\u57fa\u4e8e\u82f1\u8bed\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u975e\u82f1\u8bed\u7528\u6237\u7684\u8bbf\u95ee\u548c\u4f7f\u7528\uff0c\u9700\u8981\u6269\u5c55\u5230\u66f4\u591a\u8bed\u8a00\u3002", "method": "\u6bd4\u8f83\u7ffb\u8bd1\u7ba1\u9053\u3001LoRA\u5fae\u8c03\u53ca\u5206\u9636\u6bb5\u5fae\u8c03\u4e09\u79cd\u65b9\u6cd5\uff0c\u5229\u7528\u7ffb\u8bd1\u540e\u7684\u6807\u51c6\u591a\u6a21\u6001\u57fa\u51c6\u548c\u672c\u5730\u4e13\u5bb6\u8bc4\u4f30\u8fdb\u884c\u6027\u80fd\u6d4b\u8bd5\u3002", "result": "\u7ffb\u8bd1\u6570\u636e\u96c6\u8d28\u91cf\u6210\u4e3a\u591a\u8bed\u8a00VLM\u6027\u80fd\u7684\u4e3b\u8981\u74f6\u9888\uff0c\u4e0d\u540c\u65b9\u6cd5\u5728\u6027\u80fd\u548c\u8ba1\u7b97\u6210\u672c\u4e0a\u8868\u73b0\u5dee\u5f02\u660e\u663e\u3002", "conclusion": "\u591a\u8bed\u8a00VLM\u7684\u53d1\u5c55\u5e94\u805a\u7126\u4e8e\u672c\u5730\u8bed\u8a00\u6570\u636e\u96c6\u7684\u91c7\u96c6\u548c\u9ad8\u8d28\u91cf\u7684\u7ffb\u8bd1\u65b9\u6cd5\u3002"}}
{"id": "2512.10799", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2512.10799", "abs": "https://arxiv.org/abs/2512.10799", "authors": ["Karolina Gorna", "Nicolas Iooss", "Yannick Seurin", "Rida Khatoun"], "title": "Zorya: Automated Concolic Execution of Single-Threaded Go Binaries", "comment": null, "summary": "Go's adoption in critical infrastructure intensifies the need for systematic vulnerability detection, yet existing symbolic execution tools struggle with Go binaries due to runtime complexity and scalability challenges. In this work, we build upon Zorya, a concolic execution framework that translates Go binaries to Ghidra's P-Code intermediate representation to address these challenges. We added the detection of bugs in concretely not taken paths and a multi-layer filtering mechanism to concentrate symbolic reasoning on panic-relevant paths. Evaluation on five Go vulnerabilities demonstrates that panic-reachability gating achieves 1.8-3.9x speedups when filtering 33-70% of branches, and that Zorya detects all panics while existing tools detect at most two. Function-mode analysis proved essential for complex programs, running roughly two orders of magnitude faster than starting from main. This work establishes that specialized concolic execution can achieve practical vulnerability detection in language ecosystems with runtime safety checks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eZorya\u7684\u9488\u5bf9Go\u6f0f\u6d1e\u68c0\u6d4b\u7684\u6df7\u5408\u7b26\u53f7\u6267\u884c\u65b9\u6cd5\uff0c\u901a\u8fc7\u8def\u5f84\u8fc7\u6ee4\u548c\u51fd\u6570\u6a21\u5f0f\u5206\u6790\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "Go\u8bed\u8a00\u5728\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u4e2d\u7684\u5e7f\u6cdb\u91c7\u7528\u5e26\u6765\u4e86\u7cfb\u7edf\u6027\u6f0f\u6d1e\u68c0\u6d4b\u7684\u9700\u6c42\uff0c\u800c\u73b0\u6709\u7b26\u53f7\u6267\u884c\u5de5\u5177\u7531\u4e8e\u8fd0\u884c\u65f6\u590d\u6742\u6027\u548c\u6269\u5c55\u6027\u4e0d\u8db3\uff0c\u96be\u4ee5\u6709\u6548\u5904\u7406Go\u4e8c\u8fdb\u5236\u3002", "method": "\u57fa\u4e8eZorya\u7684\u6df7\u5408\u7b26\u53f7\u6267\u884c\u6846\u67b6\uff0c\u5c06Go\u4e8c\u8fdb\u5236\u8f6c\u4e3aGhidra\u7684P-Code\u4e2d\u95f4\u8868\u793a\uff0c\u68c0\u6d4b\u672a\u6267\u884c\u8def\u5f84\u4e2d\u7684\u6f0f\u6d1e\uff0c\u5e76\u901a\u8fc7\u591a\u5c42\u8fc7\u6ee4\u673a\u5236\u805a\u7126\u4e8epanic\u76f8\u5173\u8def\u5f84\u3002", "result": "panic\u53ef\u8fbe\u6027\u7684\u95e8\u63a7\u673a\u5236\u63d0\u5347\u4e861.8-3.9\u500d\u7684\u901f\u5ea6\uff0c\u8fc7\u6ee4\u4e8633-70%\u7684\u5206\u652f\uff0cZorya\u80fd\u68c0\u6d4b\u6240\u6709panic\u7c7b\u578b\u6f0f\u6d1e\uff0c\u4e14\u51fd\u6570\u7ea7\u5206\u6790\u76f8\u6bd4\u4ecemain\u51fd\u6570\u5f00\u59cb\u5feb\u4e24\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u4e13\u95e8\u8bbe\u8ba1\u7684\u6df7\u5408\u7b26\u53f7\u6267\u884c\u6280\u672f\u80fd\u591f\u5728\u5177\u6709\u8fd0\u884c\u65f6\u5b89\u5168\u68c0\u67e5\u7684\u8bed\u8a00\u751f\u6001\u4e2d\u5b9e\u73b0\u5b9e\u7528\u7684\u6f0f\u6d1e\u68c0\u6d4b\u3002"}}
{"id": "2512.10398", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.10398", "abs": "https://arxiv.org/abs/2512.10398", "authors": ["Zhaodong Wang", "Zhenting Qi", "Sherman Wong", "Nathan Hu", "Samuel Lin", "Jun Ge", "Erwin Gao", "Yining Yang", "Ben Maurer", "Wenlin Chen", "David Recordon", "Yilun Du", "Minlan Yu", "Ying Zhang"], "title": "Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale", "comment": null, "summary": "Real-world AI software engineering demands coding agents that can reason over massive repositories, maintain durable memory across and within long sessions, and robustly coordinate complex toolchains at test time. Existing open-source coding agents provide transparency but frequently fall short when pushed to these industrial-scale workloads, while proprietary coding agents offer strong practical performance but limited extensibility, interpretability, and controllability. We present the Confucius Code Agent (CCA), an open-sourced AI software engineer that can operate at an industrial scale. CCA is built atop the Confucius SDK, an open-sourced agent development platform designed around three complementary perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX). The SDK introduces a unified orchestrator with hierarchical working memory for long-context reasoning, a persistent note-taking system for cross-session continual learning, and a modular extension module for robust tool use. Moreover, a meta-agent automates the synthesis, evaluation, and refinement of agent configurations through a build-test-improve loop, enabling rapid agent development on new tasks, environments, and tool stacks. Instantiated on Confucius SDK with these mechanisms, CCA delivers strong performance on real-world software engineering tasks. On SWE-Bench-Pro, CCA achieves a state-of-the-art Resolve@1 performance of 54.3%, substantially improving over prior coding agents. Together, the Confucius SDK and CCA provide a transparent, extensible, and reproducible foundation for AI agents, bridge gaps between research prototypes and production-grade systems, and support agent development and deployment at industrial scale.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u5f00\u6e90\u5de5\u4e1a\u7ea7AI\u7f16\u7801\u4ee3\u7406CCA\u53ca\u5176\u5f00\u53d1\u5e73\u53f0Confucius SDK\uff0c\u878d\u5408\u957f\u8bb0\u5fc6\u63a8\u7406\u3001\u8de8\u4f1a\u8bdd\u5b66\u4e60\u548c\u6a21\u5757\u5316\u6269\u5c55\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b9e\u9645\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u900f\u660e\u4e14\u6613\u6269\u5c55\u7684\u5de5\u4e1aAI\u7f16\u7801\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5f53\u524d\u5f00\u6e90\u7f16\u7801\u4ee3\u7406\u5728\u5904\u7406\u5de5\u4e1a\u89c4\u6a21\u4efb\u52a1\u65f6\u5b58\u5728\u6027\u80fd\u4e0d\u8db3\uff0c\u800c\u4e13\u6709\u4ee3\u7406\u867d\u6027\u80fd\u5f3a\u5927\u4f46\u6269\u5c55\u6027\u548c\u53ef\u63a7\u6027\u6709\u9650\uff0c\u4e9f\u9700\u5f00\u53d1\u4e00\u4e2a\u517c\u5177\u900f\u660e\u6027\u3001\u6269\u5c55\u6027\u548c\u5f3a\u5927\u6027\u80fd\u7684\u5f00\u653e\u5f0f\u5de5\u4e1a\u7ea7AI\u7f16\u7801\u4ee3\u7406\u3002", "method": "\u63d0\u51faConfucius Code Agent (CCA)\u53ca\u5176\u57fa\u7840\u7684Confucius SDK\uff0c\u91c7\u7528\u7edf\u4e00\u7684\u534f\u8c03\u5668\u548c\u5206\u5c42\u5de5\u4f5c\u8bb0\u5fc6\u5b9e\u73b0\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\uff0c\u6301\u7eed\u7b14\u8bb0\u7cfb\u7edf\u652f\u6301\u8de8\u4f1a\u8bdd\u5b66\u4e60\uff0c\u6a21\u5757\u5316\u6269\u5c55\u6a21\u5757\u63d0\u5347\u5de5\u5177\u94fe\u534f\u8c03\u80fd\u529b\uff0c\u901a\u8fc7\u5143\u4ee3\u7406\u7684\u6784\u5efa-\u6d4b\u8bd5-\u6539\u8fdb\u5faa\u73af\u5b9e\u73b0\u5feb\u901f\u8fed\u4ee3\u4ee3\u7406\u914d\u7f6e\u3002", "result": "CCA\u5728\u771f\u5b9e\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cSWE-Bench-Pro\u6d4b\u8bd5\u4e2d\uff0c\u5176Resolve@1\u6027\u80fd\u8fbe54.3%\uff0c\u663e\u8457\u4f18\u4e8e\u4e4b\u524d\u7684\u7f16\u7801\u4ee3\u7406\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u5de5\u4e1a\u89c4\u6a21\u5e94\u7528\u80fd\u529b\u3002", "conclusion": "Confucius SDK\u4e0eCCA\u5171\u540c\u6784\u5efa\u4e86\u4e00\u4e2a\u900f\u660e\u3001\u53ef\u6269\u5c55\u3001\u53ef\u590d\u73b0\u7684\u5de5\u4e1a\u7ea7AI\u7f16\u7801\u4ee3\u7406\u5e73\u53f0\uff0c\u7f29\u5c0f\u4e86\u7814\u7a76\u539f\u578b\u4e0e\u751f\u4ea7\u7cfb\u7edf\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u652f\u6301\u5de5\u4e1a\u89c4\u6a21\u7684\u4ee3\u7406\u5f00\u53d1\u4e0e\u90e8\u7f72\u3002"}}
{"id": "2512.10411", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10411", "abs": "https://arxiv.org/abs/2512.10411", "authors": ["Yijiong Yu", "Jiale Liu", "Qingyun Wu", "Huazheng Wang", "Ji Pei"], "title": "Sliding Window Attention Adaptation", "comment": null, "summary": "The self-attention mechanism in Transformer-based Large Language Models (LLMs) scales quadratically with input length, making long-context inference expensive. Sliding window attention (SWA) reduces this cost to linear complexity, but naively enabling complete SWA at inference-time for models pretrained with full attention (FA) causes severe long-context performance degradation due to training-inference mismatch. This makes us wonder: Can FA-pretrained LLMs be well adapted to SWA without pretraining? We investigate this by proposing Sliding Window Attention Adaptation (SWAA), a set of practical recipes that combine five methods for better adaptation: (1) applying SWA only during prefilling; (2) preserving \"sink\" tokens; (3) interleaving FA/SWA layers; (4) chain-of-thought (CoT); and (5) fine-tuning. Our experiments show that SWA adaptation is feasible while non-trivial: no single method suffices, yet specific synergistic combinations effectively recover the original long-context performance. We further analyze the performance-efficiency trade-offs of different SWAA configurations and provide recommended recipes for diverse scenarios. Our code is available at https://github.com/yuyijiong/sliding-window-attention-adaptation", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u5957\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\u9002\u914d\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u7b56\u7565\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u91cd\u65b0\u9884\u8bad\u7ec3\u7684\u5168\u6ce8\u610f\u529b\u6a21\u578b\u5411\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\u7684\u9ad8\u6548\u9002\u914d\uff0c\u63d0\u5347\u4e86\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u6548\u7387\u4e14\u4fdd\u6301\u4e86\u6027\u80fd\u3002", "motivation": "Transformer\u6a21\u578b\u4e2d\u5168\u6ce8\u610f\u529b\u673a\u5236\u7531\u4e8e\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\uff0c\u5bfc\u81f4\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u6210\u672c\u6602\u8d35\uff0c\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\u867d\u7136\u8ba1\u7b97\u6548\u7387\u9ad8\u4f46\u76f4\u63a5\u5e94\u7528\u4f1a\u9020\u6210\u6027\u80fd\u635f\u5931\uff0c\u56e0\u6b64\u5e0c\u671b\u5728\u4e0d\u91cd\u65b0\u9884\u8bad\u7ec3\u7684\u524d\u63d0\u4e0b\uff0c\u5c06\u5168\u6ce8\u610f\u529b\u9884\u8bad\u7ec3\u6a21\u578b\u9002\u914d\u4e3a\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\u4ee5\u964d\u4f4e\u63a8\u7406\u6210\u672c\u3002", "method": "\u63d0\u51fa\u4e86\u4e94\u79cd\u9002\u914d\u65b9\u6cd5\u7684\u7ec4\u5408\uff1a\u53ea\u5728\u9884\u586b\u5145\u65f6\u5e94\u7528\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\u3001\u4fdd\u7559\u201c\u6c47\u805a\u201d\u4ee4\u724c\u3001\u4ea4\u9519\u5168\u6ce8\u610f\u529b\u548c\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\u5c42\u3001\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\u4ee5\u53ca\u5fae\u8c03\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u7684\u534f\u540c\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5355\u4e00\u65b9\u6cd5\u65e0\u6cd5\u5b8c\u5168\u6062\u590d\u6027\u80fd\uff0c\u4f46\u7ed3\u5408\u4e94\u79cd\u7b56\u7565\u80fd\u591f\u6709\u6548\u6062\u590d\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u6027\u80fd\uff0c\u5e76\u8fdb\u4e00\u6b65\u5206\u6790\u4e86\u4e0d\u540c\u914d\u7f6e\u5728\u6027\u80fd\u4e0e\u6548\u7387\u4e0a\u7684\u6743\u8861\uff0c\u63d0\u4f9b\u4e86\u9002\u7528\u4e8e\u591a\u79cd\u573a\u666f\u7684\u63a8\u8350\u65b9\u6848\u3002", "conclusion": "\u901a\u8fc7\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\u9002\u914d\uff08SWAA\uff09\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u5c06\u5168\u6ce8\u610f\u529b\u9884\u8bad\u7ec3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9002\u914d\u5230\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\uff0c\u5b9e\u73b0\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u800c\u4e0d\u9700\u91cd\u65b0\u9884\u8bad\u7ec3\uff0c\u540c\u65f6\u4fdd\u6301\u826f\u597d\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2512.10422", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10422", "abs": "https://arxiv.org/abs/2512.10422", "authors": ["Youmin Ko", "Sungjong Seo", "Hyunjoon Kim"], "title": "Cooperative Retrieval-Augmented Generation for Question Answering: Mutual Information Exchange and Ranking by Contrasting Layers", "comment": "Accepted to NeurIPS 2025", "summary": "Since large language models (LLMs) have a tendency to generate factually inaccurate output, retrieval-augmented generation (RAG) has gained significant attention as a key means to mitigate this downside of harnessing only LLMs. However, existing RAG methods for simple and multi-hop question answering (QA) are still prone to incorrect retrievals and hallucinations. To address these limitations, we propose CoopRAG, a novel RAG framework for the question answering task in which a retriever and an LLM work cooperatively with each other by exchanging informative knowledge, and the earlier and later layers of the retriever model work cooperatively with each other to accurately rank the retrieved documents relevant to a given query. In this framework, we (i) unroll a question into sub-questions and a reasoning chain in which uncertain positions are masked, (ii) retrieve the documents relevant to the question augmented with the sub-questions and the reasoning chain, (iii) rerank the documents by contrasting layers of the retriever, and (iv) reconstruct the reasoning chain by filling the masked positions via the LLM. Our experiments demonstrate that CoopRAG consistently outperforms state-of-the-art QA methods on three multi-hop QA datasets as well as a simple QA dataset in terms of both the retrieval and QA performances. Our code is available.\\footnote{https://github.com/meaningful96/CoopRAG}", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCoopRAG\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u5668\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5408\u4f5c\u4ea4\u4e92\u77e5\u8bc6\uff0c\u6539\u8fdb\u591a\u8df3\u95ee\u7b54\u4e2d\u7684\u68c0\u7d22\u548c\u63a8\u7406\u8fc7\u7a0b\uff0c\u63d0\u9ad8\u95ee\u7b54\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u95ee\u7b54\u65b9\u6cd5\u5728\u7b80\u5355\u548c\u591a\u8df3\u95ee\u7b54\u4e2d\u5b58\u5728\u9519\u8bef\u68c0\u7d22\u4e0e\u5e7b\u89c9\u73b0\u8c61\u3002", "method": "\u5c06\u95ee\u9898\u5c55\u5f00\u4e3a\u5e26\u63a9\u7801\u7684\u4e0d\u786e\u5b9a\u5b50\u95ee\u9898\u4e0e\u63a8\u7406\u94fe\uff0c\u7ed3\u5408\u591a\u5c42\u68c0\u7d22\u5668\u91cd\u6392\u5e8f\u4e0e\u8bed\u8a00\u6a21\u578b\u586b\u5145\u63a8\u7406\u94fe\u8fdb\u884c\u534f\u540c\u63a8\u7406\u3002", "result": "CoopRAG\u5728\u4e09\u4e2a\u591a\u8df3\u95ee\u7b54\u6570\u636e\u96c6\u53ca\u4e00\u4e2a\u7b80\u5355\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u68c0\u7d22\u548c\u95ee\u7b54\u6027\u80fd\u3002", "conclusion": "CoopRAG\u6709\u6548\u5730\u5229\u7528\u68c0\u7d22\u5668\u5c42\u95f4\u5408\u4f5c\u548c\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u91cd\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u95ee\u7b54\u7cfb\u7edf\u5728\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2512.10430", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.10430", "abs": "https://arxiv.org/abs/2512.10430", "authors": ["Dmitrii Stoianov", "Danil Taranets", "Olga Tsymboi", "Ramil Latypov", "Almaz Dautov", "Vladislav Kruglikov", "Nikita Surkov", "German Abramov", "Pavel Gein", "Dmitry Abulkhanov", "Mikhail Gashkov", "Viktor Zelenkovskiy", "Artem Batalov", "Aleksandr Medvedev", "Anatolii Potapov"], "title": "T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground", "comment": null, "summary": "We introduce T-pro 2.0, an open-weight Russian LLM for hybrid reasoning and efficient inference. The model supports direct answering and reasoning-trace generation, using a Cyrillic-dense tokenizer and an adapted EAGLE speculative-decoding pipeline to reduce latency. To enable reproducible and extensible research, we release the model weights, the T-Wix 500k instruction corpus, the T-Math reasoning benchmark, and the EAGLE weights on Hugging Face. These resources allow users to study Russian-language reasoning and to extend or adapt both the model and the inference pipeline. A public web demo exposes reasoning and non-reasoning modes and illustrates the speedups achieved by our inference stack across domains. T-pro 2.0 thus serves as an accessible open system for building and evaluating efficient, practical Russian LLM applications.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u9762\u5411\u4fc4\u8bed\u63a8\u7406\u548c\u9ad8\u6548\u63a8\u65ad\u7684\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578bT-pro 2.0\u53ca\u5176\u76f8\u5173\u8d44\u6e90\uff0c\u52a9\u529b\u76f8\u5173\u7814\u7a76\u548c\u5e94\u7528\u3002", "motivation": "\u63d0\u5347\u4fc4\u8bed\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u548c\u63a8\u65ad\u6548\u7387\uff0c\u5e76\u63a8\u52a8\u53ef\u590d\u73b0\u3001\u53ef\u6269\u5c55\u7684\u76f8\u5173\u7814\u7a76\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u897f\u91cc\u5c14\u5b57\u6bcd\u4f18\u5316\u7684\u5206\u8bcd\u5668\u548c\u9002\u914d\u7684EAGLE\u63a8\u7406\u89e3\u7801\u6d41\u7a0b\u8fdb\u884c\u6df7\u5408\u63a8\u7406\u548c\u9ad8\u6548\u63a8\u65ad\u3002", "result": "\u53d1\u5e03\u4e86T-pro 2.0\u6a21\u578b\u6743\u91cd\u3001\u6307\u4ee4\u8bed\u6599\u5e93T-Wix 500k\u3001\u63a8\u7406\u57fa\u51c6T-Math\u4ee5\u53caEAGLE\u6743\u91cd\uff0c\u63d0\u4f9b\u4e86\u516c\u5f00\u7684Web\u6f14\u793a\uff0c\u5c55\u793a\u4e86\u63a8\u65ad\u901f\u5ea6\u63d0\u5347\u548c\u591a\u9886\u57df\u5e94\u7528\u6548\u679c\u3002", "conclusion": "T-pro 2.0\u6784\u5efa\u4e86\u4e00\u4e2a\u5f00\u653e\u4e14\u5b9e\u7528\u7684\u4fc4\u8bed\u5927\u8bed\u8a00\u6a21\u578b\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u63a8\u65ad\u548c\u826f\u597d\u63a8\u7406\u80fd\u529b\uff0c\u4fc3\u8fdb\u4fc4\u8bedLLM\u5e94\u7528\u53d1\u5c55\u3002"}}
{"id": "2512.10435", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.10435", "abs": "https://arxiv.org/abs/2512.10435", "authors": ["Agniva Maiti", "Prajwal Panth", "Suresh Chandra Satapathy"], "title": "Semantic Reconstruction of Adversarial Plagiarism: A Context-Aware Framework for Detecting and Restoring \"Tortured Phrases\" in Scientific Literature", "comment": "10 pages, 5 figures; unpublished manuscript; submitted to arXiv for dissemination", "summary": "The integrity and reliability of scientific literature is facing a serious threat by adversarial text generation techniques, specifically from the use of automated paraphrasing tools to mask plagiarism. These tools generate \"tortured phrases\", statistically improbable synonyms (e.g. \"counterfeit consciousness\" for \"artificial intelligence\"), that preserve the local grammar while obscuring the original source. Most existing detection methods depend heavily on static blocklists or general-domain language models, which suffer from high false-negative rates for novel obfuscations and cannot determine the source of the plagiarized content. In this paper, we propose Semantic Reconstruction of Adversarial Plagiarism (SRAP), a framework designed not only to detect these anomalies but to mathematically recover the original terminology. We use a two-stage architecture: (1) statistical anomaly detection with a domain-specific masked language model (SciBERT) using token-level pseudo-perplexity, and (2) source-based semantic reconstruction using dense vector retrieval (FAISS) and sentence-level alignment (SBERT). Experiments on a parallel corpus of adversarial scientific text show that while zero-shot baselines fail completely (0.00 percent restoration accuracy), our retrieval-augmented approach achieves 23.67 percent restoration accuracy, significantly outperforming baseline methods. We also show that static decision boundaries are necessary for robust detection in jargon-heavy scientific text, since dynamic thresholding fails under high variance. SRAP enables forensic analysis by linking obfuscated expressions back to their most probable source documents.", "AI": {"tldr": "SRAP\u7ed3\u5408\u9886\u57df\u6a21\u578b\u548c\u8bed\u4e49\u68c0\u7d22\uff0c\u6709\u6548\u8bc6\u522b\u5e76\u6062\u590d\u79d1\u5b66\u6587\u672c\u4e2d\u7684\u4f2a\u88c5\u6284\u88ad\u3002", "motivation": "\u81ea\u52a8\u6539\u5199\u5de5\u5177\u751f\u6210\u7684\u201c\u626d\u66f2\u77ed\u8bed\u201d\u5a01\u80c1\u79d1\u5b66\u6587\u732e\u7684\u5b8c\u6574\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u5bf9\u65b0\u578b\u6df7\u6dc6\u6548\u679c\u5dee\u4e14\u65e0\u6cd5\u8ffd\u6eaf\u6284\u88ad\u6e90\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u67b6\u6784\uff1a\u57fa\u4e8e\u9886\u57df\u7279\u5b9a\u63a9\u7801\u8bed\u8a00\u6a21\u578b(SciBERT)\u7684\u7edf\u8ba1\u5f02\u5e38\u68c0\u6d4b\u4e0e\u57fa\u4e8e\u5411\u91cf\u68c0\u7d22(FAISS)\u548c\u53e5\u5b50\u5bf9\u9f50(SBERT)\u7684\u8bed\u4e49\u91cd\u5efa\u3002", "result": "\u96f6\u6837\u672c\u65b9\u6cd5\u4fee\u590d\u51c6\u786e\u7387\u4e3a0\uff0cSRAP\u65b9\u6cd5\u4fee\u590d\u51c6\u786e\u7387\u8fbe23.67%\uff0c\u4e14\u5728\u4e13\u4e1a\u672f\u8bed\u5bc6\u96c6\u6587\u672c\u4e2d\u8868\u73b0\u66f4\u7a33\u5065\u3002", "conclusion": "SRAP\u6846\u67b6\u6709\u6548\u68c0\u6d4b\u548c\u6062\u590d\u88ab\u81ea\u52a8\u6539\u5199\u5de5\u5177\u63a9\u76d6\u7684\u6284\u88ad\u6587\u672c\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2512.10440", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.10440", "abs": "https://arxiv.org/abs/2512.10440", "authors": ["Nour El Houda Ben Chaabene", "Hamza Hammami"], "title": "Enhancing Next-Generation Language Models with Knowledge Graphs: Extending Claude, Mistral IA, and GPT-4 via KG-BERT", "comment": "This paper was accepted and scheduled for inclusion in the ICALT 2025 proceedings but was ultimately not published due to absence from the conference presentation. It appears in the official program booklet. Conference: 2025 IEEE International Conference on Advanced Learning Technologies (ICALT)", "summary": "Large language models (LLMs) like Claude, Mistral IA, and GPT-4 excel in NLP but lack structured knowledge, leading to factual inconsistencies. We address this by integrating Knowledge Graphs (KGs) via KG-BERT to enhance grounding and reasoning. Experiments show significant gains in knowledge-intensive tasks such as question answering and entity linking. This approach improves factual reliability and enables more context-aware next-generation LLMs.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5c06\u77e5\u8bc6\u56fe\u8c31\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u77e5\u8bc6\u5bc6\u96c6\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u548c\u4e8b\u5b9e\u4e00\u81f4\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u7f3a\u4e4f\u7ed3\u6784\u5316\u77e5\u8bc6\uff0c\u5bfc\u81f4\u4e8b\u5b9e\u4e0d\u4e00\u81f4\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u5e76\u7ed3\u5408KG-BERT\u6a21\u578b\uff0c\u5b9e\u73b0\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u589e\u5f3a\uff0c\u4f7f\u5176\u5177\u5907\u66f4\u597d\u7684\u77e5\u8bc6\u57fa\u7840\u548c\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u5982\u95ee\u7b54\u548c\u5b9e\u4f53\u94fe\u63a5\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u6539\u5584\u4e86\u4e8b\u5b9e\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u9ad8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u80fd\u529b\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u6a21\u578b\u7684\u53d1\u5c55\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2512.10441", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.10441", "abs": "https://arxiv.org/abs/2512.10441", "authors": ["Nour El Houda Ben Chaabene", "Hamza Hammami", "Laid Kahloul"], "title": "Decoding Student Minds: Leveraging Conversational Agents for Psychological and Learning Analysis", "comment": "This manuscript is currently under peer review in Expert Systems with Applications", "summary": "This paper presents a psychologically-aware conversational agent designed to enhance both learning performance and emotional well-being in educational settings. The system combines Large Language Models (LLMs), a knowledge graph-enhanced BERT (KG-BERT), and a bidirectional Long Short-Term Memory (LSTM) with attention to classify students' cognitive and affective states in real time. Unlike prior chatbots limited to either tutoring or affective support, our approach leverages multimodal data-including textual semantics, prosodic speech features, and temporal behavioral trends-to infer engagement, stress, and conceptual understanding. A pilot study with university students demonstrated improved motivation, reduced stress, and moderate academic gains compared to baseline methods. These results underline the promise of integrating semantic reasoning, multimodal fusion, and temporal modeling to support adaptive, student-centered educational interventions.", "AI": {"tldr": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5fc3\u7406\u611f\u77e5\u5bf9\u8bdd\u4ee3\u7406\uff0c\u5229\u7528\u591a\u6a21\u6001\u6570\u636e\u548c\u6a21\u578b\u63d0\u5347\u5b66\u751f\u5b66\u4e60\u548c\u60c5\u7eea\u72b6\u6001\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u63d0\u5347\u6559\u80b2\u73af\u5883\u4e2d\u5b66\u751f\u7684\u5b66\u4e60\u8868\u73b0\u548c\u60c5\u7eea\u5065\u5eb7\u3002", "method": "\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3aBERT\u548c\u53cc\u5411LSTM\u6ce8\u610f\u529b\u673a\u5236\uff0c\u878d\u5408\u6587\u672c\u8bed\u4e49\u3001\u8bed\u97f3\u7279\u5f81\u53ca\u884c\u4e3a\u8d8b\u52bf\u8fdb\u884c\u72b6\u6001\u5206\u7c7b\u3002", "result": "\u8be5\u7cfb\u7edf\u901a\u8fc7\u7ed3\u5408\u591a\u6a21\u6001\u6570\u636e\u548c\u5148\u8fdb\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u5bf9\u5b66\u751f\u8ba4\u77e5\u548c\u60c5\u611f\u72b6\u6001\u7684\u5b9e\u65f6\u5206\u7c7b\uff0c\u9a8c\u8bc1\u4e86\u5176\u80fd\u63d0\u9ad8\u5b66\u751f\u52a8\u673a\uff0c\u964d\u4f4e\u538b\u529b\uff0c\u5e76\u5e26\u6765\u9002\u5ea6\u7684\u5b66\u4e1a\u63d0\u5347\u3002", "conclusion": "\u5c06\u8bed\u4e49\u63a8\u7406\u3001\u591a\u6a21\u6001\u878d\u5408\u548c\u65f6\u5e8f\u5efa\u6a21\u5e94\u7528\u4e8e\u6559\u80b2\u804a\u5929\u673a\u5668\u4eba\uff0c\u80fd\u591f\u5b9e\u73b0\u4e2a\u6027\u5316\u548c\u9002\u5e94\u6027\u7684\u5b66\u751f\u652f\u6301\u3002"}}
{"id": "2512.10453", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.10453", "abs": "https://arxiv.org/abs/2512.10453", "authors": ["Lars G. B. Johnsen"], "title": "Grammaticality Judgments in Humans and Language Models: Revisiting Generative Grammar with LLMs", "comment": "2 figures", "summary": "What counts as evidence for syntactic structure? In traditional generative grammar, systematic contrasts in grammaticality such as subject-auxiliary inversion and the licensing of parasitic gaps are taken as evidence for an internal, hierarchical grammar. In this paper, we test whether large language models (LLMs), trained only on surface forms, reproduce these contrasts in ways that imply an underlying structural representation.\n  We focus on two classic constructions: subject-auxiliary inversion (testing recognition of the subject boundary) and parasitic gap licensing (testing abstract dependency structure). We evaluate models including GPT-4 and LLaMA-3 using prompts eliciting acceptability ratings. Results show that LLMs reliably distinguish between grammatical and ungrammatical variants in both constructions, and as such support that they are sensitive to structure and not just linear order. Structural generalizations, distinct from cognitive knowledge, emerge from predictive training on surface forms, suggesting functional sensitivity to syntax without explicit encoding.", "AI": {"tldr": "\u672c\u6587\u68c0\u6d4b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u901a\u8fc7\u8868\u9762\u6587\u672c\u8bad\u7ec3\u6355\u6349\u6df1\u5c42\u8bed\u6cd5\u7ed3\u6784\uff1b\u7ed3\u679c\u663e\u793a\u5b83\u4eec\u80fd\u591f\u533a\u522b\u8bed\u6cd5\u5bf9\u6bd4\uff0c\u8bc1\u660e\u5bf9\u53e5\u6cd5\u7ed3\u6784\u5177\u5907\u654f\u611f\u6027\u3002", "motivation": "\u63a2\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u5177\u5907\u53cd\u6620\u5185\u5728\u5c42\u7ea7\u8bed\u6cd5\u7ed3\u6784\u7684\u80fd\u529b\uff0c\u4ec5\u4ec5\u57fa\u4e8e\u8868\u9762\u5f62\u5f0f\u8bad\u7ec3\u800c\u975e\u663e\u5f0f\u7f16\u7801\u8bed\u6cd5\u89c4\u5219\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u63d0\u793a\uff0c\u91c7\u96c6LLMs\uff08\u5982GPT-4\u548cLLaMA-3\uff09\u5bf9\u4e3b\u8c13\u5012\u88c5\u548c\u5bc4\u751f\u7a7a\u4f4d\u8bb8\u53ef\u4e24\u79cd\u8bed\u6cd5\u7ed3\u6784\u7684\u63a5\u53d7\u5ea6\u8bc4\u4ef7\uff0c\u6d4b\u8bd5\u5176\u662f\u5426\u518d\u73b0\u8bed\u8a00\u5b66\u4e2d\u7684\u7cfb\u7edf\u5bf9\u6bd4\u3002", "result": "LLMs\u5728\u4e3b\u8c13\u5012\u88c5\u548c\u5bc4\u751f\u7a7a\u4f4d\u8bb8\u53ef\u4e24\u79cd\u6784\u5f0f\u4e2d\u5747\u80fd\u7a33\u5b9a\u533a\u5206\u8bed\u6cd5\u6b63\u786e\u548c\u9519\u8bef\u7684\u8868\u8fbe\uff0c\u8868\u73b0\u51fa\u5bf9\u8bed\u6cd5\u7ed3\u6784\u7684\u654f\u611f\u5ea6\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u80fd\u591f\u901a\u8fc7\u8868\u5c42\u5f62\u5f0f\u8bad\u7ec3\u5c55\u73b0\u5bf9\u7ed3\u6784\u654f\u611f\u6027\uff0c\u533a\u5206\u8bed\u6cd5\u53ef\u63a5\u53d7\u6027\uff0c\u652f\u6301\u5176\u9690\u542b\u5c42\u7ea7\u8bed\u6cd5\u7ed3\u6784\u7684\u8ba4\u77e5\u3002"}}
{"id": "2512.10545", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.10545", "abs": "https://arxiv.org/abs/2512.10545", "authors": ["I\u00f1aki Lacunza", "Jos\u00e9 Javier Saiz", "Alexander Shvets", "Aitor Gonzalez-Agirre", "Marta Villegas"], "title": "XDoGE: Multilingual Data Reweighting to Enhance Language Inclusivity in LLMs", "comment": "Accepted and presented at the LLMs4All workshop at the IEEE BigData 2025 Conference, Macau - December 8-11, 2025", "summary": "Current large language models (LLMs) are trained on massive amounts of text data, primarily from a few dominant languages. Studies suggest that this over-reliance on high-resource languages, such as English, hampers LLM performance in mid- and low-resource languages. To mitigate this problem, we propose to (i) optimize the language distribution by training a small proxy model within a domain-reweighing DoGE algorithm that we extend to XDoGE for a multilingual setup, and (ii) rescale the data and train a full-size model with the established language weights either from scratch or within a continual pre-training phase (CPT). We target six languages possessing a variety of geographic and intra- and inter-language-family relations, namely, English and Spanish (high-resource), Portuguese and Catalan (mid-resource), Galician and Basque (low-resource). We experiment with Salamandra-2b, which is a promising model for these languages. We investigate the effects of substantial data repetition on minor languages and under-sampling on dominant languages using the IberoBench framework for quantitative evaluation. Finally, we release a new promising IberianLLM-7B-Instruct model centering on Iberian languages and English that we pretrained from scratch and further improved using CPT with the XDoGE weights.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u4f18\u5316\u591a\u8bed\u8a00\u6570\u636e\u6743\u91cd\u5206\u5e03\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u4e2d\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8868\u73b0\uff0c\u5e76\u53d1\u5e03\u4e86\u4e13\u6ce8\u4e8e\u4f0a\u6bd4\u5229\u4e9a\u8bed\u8a00\u7684\u65b0\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u5c11\u6570\u9ad8\u8d44\u6e90\u8bed\u8a00\uff0c\u5bfc\u81f4\u4e2d\u4f4e\u8d44\u6e90\u8bed\u8a00\u6027\u80fd\u53d7\u9650\u3002", "method": "\u901a\u8fc7\u6269\u5c55DoGE\u7b97\u6cd5\u81f3\u591a\u8bed\u8a00\u73af\u5883\u7684XDoGE\uff0c\u6211\u4eec\u8bad\u7ec3\u4e86\u4e00\u4e2a\u5c0f\u578b\u4ee3\u7406\u6a21\u578b\u4ee5\u4f18\u5316\u8bed\u8a00\u5206\u5e03\uff0c\u5e76\u4f7f\u7528\u8c03\u6574\u540e\u7684\u8bed\u8a00\u6743\u91cd\u5bf9\u5168\u5c3a\u5bf8\u6a21\u578b\u8fdb\u884c\u91cd\u65b0\u8bad\u7ec3\uff0c\u5305\u542b\u4ece\u96f6\u5f00\u59cb\u8bad\u7ec3\u548c\u6301\u7eed\u9884\u8bad\u7ec3\u9636\u6bb5(CPT)\u3002", "result": "\u9488\u5bf9\u82f1\u3001\u897f\u73ed\u7259\u8bed\uff08\u9ad8\u8d44\u6e90\uff09\uff0c\u8461\u8404\u7259\u8bed\u3001\u52a0\u6cf0\u7f57\u5c3c\u4e9a\u8bed\uff08\u4e2d\u8d44\u6e90\uff09\uff0c\u4ee5\u53ca\u52a0\u5229\u897f\u4e9a\u8bed\u548c\u5df4\u65af\u514b\u8bed\uff08\u4f4e\u8d44\u6e90\uff09\u516d\u79cd\u8bed\u8a00\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u5c55\u793a\u4e86\u6570\u636e\u91cd\u590d\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u4e3b\u5bfc\u8bed\u8a00\u4e0b\u91c7\u6837\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u5e76\u53d1\u5e03\u4e86IberianLLM-7B-Instruct\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f0a\u6bd4\u5229\u4e9a\u8bed\u8a00\u53ca\u82f1\u8bed\u7684\u6027\u80fd\u3002", "conclusion": "\u5229\u7528XDoGE\u7b97\u6cd5\u8c03\u6574\u8bed\u8a00\u6743\u91cd\u5e76\u7ed3\u5408\u6301\u7eed\u9884\u8bad\u7ec3\uff0c\u6709\u6548\u6539\u5584\u4e86\u6a21\u578b\u5728\u591a\u79cd\u8d44\u6e90\u6c34\u5e73\u8bed\u8a00\u4e0a\u7684\u8868\u73b0\uff0c\u5c24\u5176\u662f\u4e2d\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u63a8\u52a8\u4e86\u591a\u8bed\u8a00\u5927\u578b\u6a21\u578b\u7684\u516c\u5e73\u548c\u9ad8\u6548\u5e94\u7528\u3002"}}
{"id": "2512.10561", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.10561", "abs": "https://arxiv.org/abs/2512.10561", "authors": ["Amartya Roy", "Elamparithy M", "Kripabandhu Ghosh", "Ponnurangam Kumaraguru", "Adrian de Wynter"], "title": "Causal Reasoning Favors Encoders: On The Limits of Decoder-Only Models", "comment": null, "summary": "In context learning (ICL) underpins recent advances in large language models (LLMs), although its role and performance in causal reasoning remains unclear. Causal reasoning demands multihop composition and strict conjunctive control, and reliance on spurious lexical relations of the input could provide misleading results. We hypothesize that, due to their ability to project the input into a latent space, encoder and encoder decoder architectures are better suited for said multihop conjunctive reasoning versus decoder only models. To do this, we compare fine-tuned versions of all the aforementioned architectures with zero and few shot ICL in both natural language and non natural language scenarios. We find that ICL alone is insufficient for reliable causal reasoning, often overfocusing on irrelevant input features. In particular, decoder only models are noticeably brittle to distributional shifts, while finetuned encoder and encoder decoder models can generalize more robustly across our tests, including the non natural language split. Both architectures are only matched or surpassed by decoder only architectures at large scales. We conclude by noting that for cost effective, short horizon robust causal reasoning, encoder or encoder decoder architectures with targeted finetuning are preferable.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u56e0\u679c\u63a8\u7406\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4ec5\u9760\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e0d\u8db3\u4ee5\u5b9e\u73b0\u53ef\u9760\u63a8\u7406\uff0c\u7f16\u7801\u5668\u7ed3\u6784\u5728\u591a\u8df3\u8054\u5408\u63a8\u7406\u4e2d\u8868\u73b0\u66f4\u7a33\u5065\u3002", "motivation": "\u63a2\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8df3\u8054\u5408\u56e0\u679c\u63a8\u7406\u4e2d\u7684\u80fd\u529b\u53ca\u5176\u5728\u4e0d\u540c\u67b6\u6784\u6a21\u578b\u4e2d\u7684\u8868\u73b0\u5dee\u5f02\u3002", "method": "\u6bd4\u8f83\u4e86\u7f16\u7801\u5668\u3001\u7f16\u7801\u5668-\u89e3\u7801\u5668\u548c\u4ec5\u89e3\u7801\u5668\u67b6\u6784\u7684\u5fae\u8c03\u6a21\u578b\u4e0e\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u5728\u81ea\u7136\u8bed\u8a00\u53ca\u975e\u81ea\u7136\u8bed\u8a00\u573a\u666f\u4e0b\u8fdb\u884c\u591a\u8df3\u56e0\u679c\u63a8\u7406\u6d4b\u8bd5\u3002", "result": "\u4e0a\u4e0b\u6587\u5b66\u4e60\u6a21\u578b\u5bb9\u6613\u8fc7\u5ea6\u5173\u6ce8\u65e0\u5173\u7279\u5f81\uff0c\u7f16\u7801\u5668\u548c\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u578b\u5fae\u8c03\u540e\u5728\u591a\u573a\u666f\u4e2d\u5c55\u73b0\u66f4\u597d\u7684\u9c81\u68d2\u6027\uff0c\u4ec5\u5728\u5927\u89c4\u6a21\u6a21\u578b\u4e0b\uff0c\u7eaf\u89e3\u7801\u5668\u67b6\u6784\u8868\u73b0\u4f18\u4e8e\u7f16\u7801\u5668\u7c7b\u6a21\u578b\u3002", "conclusion": "\u4ec5\u7528\u4e0a\u4e0b\u6587\u5b66\u4e60\u8fdb\u884c\u56e0\u679c\u63a8\u7406\u8868\u73b0\u4e0d\u8db3\uff0c\u5927\u89c4\u6a21\u89e3\u7801\u5668\u6a21\u578b\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u6210\u672c\u548c\u7a33\u5b9a\u6027\u65b9\u9762\uff0c\u5e26\u5fae\u8c03\u7684\u7f16\u7801\u5668\u6216\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u66f4\u4f18\u3002"}}
{"id": "2512.10575", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.10575", "abs": "https://arxiv.org/abs/2512.10575", "authors": ["Hang Ding", "Qiming Feng", "Dongqi Liu", "Qi Zhao", "Tao Yao", "Shuo Wang", "Dongsheng Chen", "Jian Li", "Zhenye Gan", "Jiangning Zhang", "Chengjie Wang", "Yabiao Wang"], "title": "RoleRMBench & RoleRM: Towards Reward Modeling for Profile-Based Role Play in Dialogue Systems", "comment": null, "summary": "Reward modeling has become a cornerstone of aligning large language models (LLMs) with human preferences. Yet, when extended to subjective and open-ended domains such as role play, existing reward models exhibit severe degradation, struggling to capture nuanced and persona-grounded human judgments. To address this gap, we introduce RoleRMBench, the first systematic benchmark for reward modeling in role-playing dialogue, covering seven fine-grained capabilities from narrative management to role consistency and engagement. Evaluation on RoleRMBench reveals large and consistent gaps between general-purpose reward models and human judgment, particularly in narrative and stylistic dimensions. We further propose RoleRM, a reward model trained with Continuous Implicit Preferences (CIP), which reformulates subjective evaluation as continuous consistent pairwise supervision under multiple structuring strategies. Comprehensive experiments show that RoleRM surpasses strong open- and closed-source reward models by over 24% on average, demonstrating substantial gains in narrative coherence and stylistic fidelity. Our findings highlight the importance of continuous preference representation and annotation consistency, establishing a foundation for subjective alignment in human-centered dialogue systems.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86RoleRMBench\uff0c\u4e00\u4e2a\u9488\u5bf9\u89d2\u8272\u626e\u6f14\u5bf9\u8bdd\u4e2d\u5956\u52b1\u5efa\u6a21\u7684\u7cfb\u7edf\u6027\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u8fde\u7eed\u9690\u5f0f\u504f\u597d\u7684RoleRM\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5956\u52b1\u6a21\u578b\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u5956\u52b1\u6a21\u578b\u5728\u5f00\u653e\u4e14\u4e3b\u89c2\u7684\u89d2\u8272\u626e\u6f14\u9886\u57df\u8868\u73b0\u4e0d\u4f73\uff0c\u96be\u4ee5\u51c6\u786e\u53cd\u6620\u7ec6\u5fae\u4e14\u57fa\u4e8e\u4eba\u7269\u89d2\u8272\u7684\u4eba\u7684\u4e3b\u89c2\u5224\u65ad\uff0c\u9020\u6210\u4e25\u91cd\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51faRoleRM\u5956\u52b1\u6a21\u578b\uff0c\u5229\u7528\u8fde\u7eed\u9690\u5f0f\u504f\u597d\uff08CIP\uff09\u5c06\u4e3b\u89c2\u8bc4\u4ef7\u8f6c\u5316\u4e3a\u8fde\u7eed\u4e00\u81f4\u7684\u6210\u5bf9\u76d1\u7763\uff0c\u901a\u8fc7\u591a\u79cd\u7ed3\u6784\u5316\u7b56\u7565\u8bad\u7ec3\uff0c\u4ece\u800c\u63d0\u5347\u5bf9\u7ec6\u7c92\u5ea6\u89d2\u8272\u626e\u6f14\u80fd\u529b\u7684\u6355\u6349\u3002", "result": "RoleRMBench\u63ed\u793a\u4e86\u901a\u7528\u5956\u52b1\u6a21\u578b\u4e0e\u4eba\u5de5\u8bc4\u4ef7\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0cRoleRM\u6a21\u578b\u5728\u591a\u4e2a\u6d4b\u8bc4\u7ef4\u5ea6\u4e0a\u8d85\u8d8a\u9886\u5148\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b24%\u4ee5\u4e0a\uff0c\u7279\u522b\u662f\u5728\u53d9\u4e8b\u548c\u98ce\u683c\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "RoleRM\u901a\u8fc7\u8fde\u7eed\u9690\u5f0f\u504f\u597d\u8bad\u7ec3\uff0c\u5728\u89d2\u8272\u626e\u6f14\u5bf9\u8bdd\u5956\u52b1\u5efa\u6a21\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5c24\u5176\u5728\u53d9\u4e8b\u8fde\u8d2f\u6027\u548c\u98ce\u683c\u4e00\u81f4\u6027\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u63a8\u52a8\u4e86\u9762\u5411\u4eba\u7c7b\u4e2d\u5fc3\u5bf9\u8bdd\u7cfb\u7edf\u7684\u4e3b\u89c2\u5bf9\u9f50\u7814\u7a76\u3002"}}
{"id": "2512.10624", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.10624", "abs": "https://arxiv.org/abs/2512.10624", "authors": ["Bo Yang", "Lanfei Feng", "Yunkui Chen", "Yu Zhang", "Jianyu Zhang", "Xiao Xu", "Nueraili Aierken", "Shijian Li"], "title": "AgriGPT-Omni: A Unified Speech-Vision-Text Framework for Multilingual Agricultural Intelligence", "comment": null, "summary": "Despite rapid advances in multimodal large language models, agricultural applications remain constrained by the lack of multilingual speech data, unified multimodal architectures, and comprehensive evaluation benchmarks. To address these challenges, we present AgriGPT-Omni, an agricultural omni-framework that integrates speech, vision, and text in a unified framework. First, we construct a scalable data synthesis and collection pipeline that converts agricultural texts and images into training data, resulting in the largest agricultural speech dataset to date, including 492K synthetic and 1.4K real speech samples across six languages. Second, based on this, we train the first agricultural omni-model via a three-stage paradigm: textual knowledge injection, progressive multimodal alignment, and GRPO-based reinforcement learning, enabling unified reasoning across languages and modalities. Third, we propose AgriBench-Omni-2K, the first tri-modal benchmark for agriculture, covering diverse speech-vision-text tasks and multilingual slices, with standardized protocols and reproducible tools. Experiments show that AgriGPT-Omni significantly outperforms general-purpose baselines on multilingual and multimodal reasoning as well as real-world speech understanding. All models, data, benchmarks, and code will be released to promote reproducible research, inclusive agricultural intelligence, and sustainable AI development for low-resource regions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u519c\u4e1a\u8de8\u8bed\u97f3\u3001\u89c6\u89c9\u3001\u6587\u672c\u7684\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u53ca\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u8bed\u97f3\u6570\u636e\u96c6\uff0c\u5efa\u7acb\u9996\u4e2a\u519c\u4e1a\u591a\u6a21\u6001\u591a\u8bed\u8a00\u8bc4\u6d4b\u57fa\u51c6\uff0c\u663e\u8457\u63d0\u5347\u519c\u4e1aAI\u80fd\u529b\u3002", "motivation": "\u519c\u4e1a\u5e94\u7528\u53d7\u9650\u4e8e\u591a\u8bed\u8a00\u8bed\u97f3\u6570\u636e\u7f3a\u4e4f\u3001\u7edf\u4e00\u591a\u6a21\u6001\u67b6\u6784\u548c\u5168\u9762\u8bc4\u4f30\u57fa\u51c6\u7f3a\u5931\u3002", "method": "\u6784\u5efa\u519c\u4e1a\u6587\u672c\u4e0e\u56fe\u50cf\u6570\u636e\u5408\u6210\u4e0e\u6536\u96c6\u6d41\u7a0b\uff0c\u5f62\u6210\u591a\u8bed\u8a00\u8bed\u97f3\u6570\u636e\u96c6\uff1b\u8bad\u7ec3\u4e09\u9636\u6bb5\u519c\u4e1a\u5168\u6a21\u6001\u6a21\u578b\uff08\u77e5\u8bc6\u6ce8\u5165\u3001\u591a\u6a21\u6001\u5bf9\u9f50\u3001\u57fa\u4e8eGRPO\u7684\u5f3a\u5316\u5b66\u4e60\uff09\uff1b\u63d0\u51fa\u6db5\u76d6\u591a\u6a21\u6001\u591a\u4efb\u52a1\u7684\u519c\u4e1a\u57fa\u51c6AgriBench-Omni-2K\u3002", "result": "\u83b7\u5f97\u6700\u5927\u89c4\u6a21\u519c\u4e1a\u591a\u8bed\u8a00\u8bed\u97f3\u6570\u636e\u96c6\u548c\u9996\u4e2a\u519c\u4e1a\u5168\u6a21\u6001\u6a21\u578b\uff1b\u6a21\u578b\u5728\u591a\u8bed\u8a00\u548c\u591a\u6a21\u6001\u63a8\u7406\u53ca\u771f\u5b9e\u8bed\u97f3\u7406\u89e3\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u901a\u7528\u57fa\u7ebf\u3002", "conclusion": "AgriGPT-Omni\u6846\u67b6\u6709\u6548\u4fc3\u8fdb\u519c\u4e1a\u591a\u6a21\u6001\u591a\u8bed\u8a00\u667a\u80fd\u53d1\u5c55\uff0c\u652f\u6301\u4f4e\u8d44\u6e90\u533a\u57df\u53ef\u6301\u7eedAI\u5efa\u8bbe\uff0c\u63a8\u52a8\u76f8\u5173\u5f00\u653e\u7814\u7a76\u3002"}}
{"id": "2512.10630", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.10630", "abs": "https://arxiv.org/abs/2512.10630", "authors": ["Smiljana Antonijevic Ubois"], "title": "From Data Scarcity to Data Care: Reimagining Language Technologies for Serbian and other Low-Resource Languages", "comment": null, "summary": "Large language models are commonly trained on dominant languages like English, and their representation of low resource languages typically reflects cultural and linguistic biases present in the source language materials. Using the Serbian language as a case, this study examines the structural, historical, and sociotechnical factors shaping language technology development for low resource languages in the AI age. Drawing on semi structured interviews with ten scholars and practitioners, including linguists, digital humanists, and AI developers, it traces challenges rooted in historical destruction of Serbian textual heritage, intensified by contemporary issues that drive reductive, engineering first approaches prioritizing functionality over linguistic nuance. These include superficial transliteration, reliance on English-trained models, data bias, and dataset curation lacking cultural specificity. To address these challenges, the study proposes Data Care, a framework grounded in CARE principles (Collective Benefit, Authority to Control, Responsibility, and Ethics), that reframes bias mitigation from a post hoc technical fix to an integral component of corpus design, annotation, and governance, and positions Data Care as a replicable model for building inclusive, sustainable, and culturally grounded language technologies in contexts where traditional LLM development reproduces existing power imbalances and cultural blind spots.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9488\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u9762\u4e34\u7684\u504f\u89c1\u548c\u6280\u672f\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eCARE\u539f\u5219\u7684\u6570\u636e\u5173\u6000\u6846\u67b6\uff0c\u4ee5\u4fc3\u8fdb\u66f4\u516c\u5e73\u548c\u6587\u5316\u76f8\u5173\u7684\u8bed\u8a00\u6280\u672f\u5f00\u53d1\u3002", "motivation": "\u63a2\u8ba8\u4f4e\u8d44\u6e90\u8bed\u8a00\u5728\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8bad\u7ec3\u4e2d\u5b58\u5728\u7684\u6587\u5316\u548c\u8bed\u8a00\u504f\u89c1\uff0c\u4ee5\u53ca\u5386\u53f2\u548c\u793e\u4f1a\u6280\u672f\u56e0\u7d20\u5982\u4f55\u5f71\u54cd\u8fd9\u4e9b\u8bed\u8a00\u6280\u672f\u7684\u53d1\u5c55\uff0c\u7279\u522b\u4ee5\u585e\u5c14\u7ef4\u4e9a\u8bed\u4e3a\u4f8b\u3002", "method": "\u901a\u8fc7\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u6cd5\u6536\u96c6\u5341\u4f4d\u8bed\u8a00\u5b66\u5bb6\u3001\u6570\u5b57\u4eba\u6587\u4e13\u5bb6\u548cAI\u5f00\u53d1\u8005\u7684\u89c2\u70b9\uff0c\u7ed3\u5408\u5386\u53f2\u548c\u793e\u4f1a\u6280\u672f\u80cc\u666f\u5206\u6790\uff0c\u63d0\u51faCARE\u539f\u5219\uff08\u96c6\u4f53\u5229\u76ca\u3001\u63a7\u5236\u6743\u3001\u8d23\u4efb\u548c\u4f26\u7406\uff09\u6307\u5bfc\u7684\u6570\u636e\u5173\u6000\u6846\u67b6\u3002", "result": "\u901a\u8fc7\u5bf9\u5341\u4f4d\u4e13\u5bb6\u7684\u8bbf\u8c08\uff0c\u63ed\u793a\u4e86\u585e\u5c14\u7ef4\u4e9a\u8bed\u6570\u5b57\u8d44\u6e90\u5386\u53f2\u88ab\u7834\u574f\u548c\u73b0\u4ee3\u6280\u672f\u65b9\u6cd5\uff08\u5982\u7c97\u6d45\u97f3\u8bd1\u3001\u4f9d\u8d56\u82f1\u6587\u6a21\u578b\u7b49\uff09\u5e26\u6765\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u4ee5CARE\u539f\u5219\u4e3a\u57fa\u7840\u7684\u6570\u636e\u5173\u6000\u6846\u67b6\uff0c\u65e8\u5728\u5c06\u504f\u89c1\u7f13\u89e3\u878d\u5165\u8bed\u6599\u5e93\u8bbe\u8ba1\u548c\u6cbb\u7406\uff0c\u63a8\u52a8\u5305\u5bb9\u6027\u548c\u6587\u5316\u6839\u690d\u7684\u8bed\u8a00\u6280\u672f\u53d1\u5c55\u3002", "conclusion": "\u6570\u636e\u5173\u6000\u6846\u67b6\u80fd\u6709\u6548\u5e94\u5bf9\u4f20\u7edfLLM\u5f00\u53d1\u4e2d\u7684\u6743\u529b\u5931\u8861\u548c\u6587\u5316\u76f2\u70b9\uff0c\u6210\u4e3a\u5efa\u7acb\u5305\u5bb9\u6027\u3001\u53ef\u6301\u7eed\u4e14\u6709\u6587\u5316\u6839\u57fa\u7684\u8bed\u8a00\u6280\u672f\u7684\u53ef\u590d\u5236\u6a21\u578b\u3002"}}
{"id": "2512.10734", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10734", "abs": "https://arxiv.org/abs/2512.10734", "authors": ["Rebekka G\u00f6rge", "Sujan Sai Gannamaneni", "Tabea Naeven", "Hammam Abdelwahab", "H\u00e9ctor Allende-Cid", "Armin B. Cremers", "Lennard Helmer", "Michael Mock", "Anna Schmitz", "Songkai Xue", "Elif Yildirir", "Maximilian Poretschkin", "Stefan Wrobel"], "title": "Textual Data Bias Detection and Mitigation - An Extensible Pipeline with Experimental Evaluation", "comment": null, "summary": "Textual data used to train large language models (LLMs) exhibits multifaceted bias manifestations encompassing harmful language and skewed demographic distributions. Regulations such as the European AI Act require identifying and mitigating biases against protected groups in data, with the ultimate goal of preventing unfair model outputs. However, practical guidance and operationalization are lacking. We propose a comprehensive data bias detection and mitigation pipeline comprising four components that address two data bias types, namely representation bias and (explicit) stereotypes for a configurable sensitive attribute. First, we leverage LLM-generated word lists created based on quality criteria to detect relevant group labels. Second, representation bias is quantified using the Demographic Representation Score. Third, we detect and mitigate stereotypes using sociolinguistically informed filtering. Finally, we compensate representation bias through Grammar- and Context-Aware Counterfactual Data Augmentation. We conduct a two-fold evaluation using the examples of gender, religion and age. First, the effectiveness of each individual component on data debiasing is evaluated through human validation and baseline comparison. The findings demonstrate that we successfully reduce representation bias and (explicit) stereotypes in a text dataset. Second, the effect of data debiasing on model bias reduction is evaluated by bias benchmarking of several models (0.6B-8B parameters), fine-tuned on the debiased text dataset. This evaluation reveals that LLMs fine-tuned on debiased data do not consistently show improved performance on bias benchmarks, exposing critical gaps in current evaluation methodologies and highlighting the need for targeted data manipulation to address manifested model bias.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u7efc\u5408\u6027\u6570\u636e\u504f\u89c1\u68c0\u6d4b\u4e0e\u7f13\u89e3\u6d41\u7a0b\uff0c\u9488\u5bf9\u6587\u672c\u6570\u636e\u4e2d\u7684\u8868\u73b0\u504f\u89c1\u548c\u660e\u786e\u523b\u677f\u5370\u8c61\u8fdb\u884c\u68c0\u6d4b\u548c\u7f13\u89e3\uff0c\u5e76\u901a\u8fc7\u5b9e\u4f8b\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6587\u672c\u6570\u636e\u5b58\u5728\u591a\u7ef4\u5ea6\u504f\u89c1\u95ee\u9898\uff0c\u73b0\u6709\u6cd5\u89c4\u8981\u6c42\u68c0\u6d4b\u548c\u7f13\u89e3\u9488\u5bf9\u53d7\u4fdd\u62a4\u7fa4\u4f53\u7684\u504f\u89c1\uff0c\u4f46\u7f3a\u4e4f\u5207\u5b9e\u53ef\u884c\u7684\u65b9\u6cd5\u548c\u64cd\u4f5c\u6d41\u7a0b\u3002", "method": "\u63d0\u51fa\u5305\u542b\u56db\u4e2a\u90e8\u5206\u7684\u504f\u89c1\u68c0\u6d4b\u548c\u7f13\u89e3\u6d41\u7a0b\uff1a\u57fa\u4e8e\u5927\u6a21\u578b\u751f\u6210\u8bcd\u8868\u68c0\u6d4b\u7fa4\u4f53\u6807\u7b7e\uff0c\u4f7f\u7528\u4eba\u53e3\u8868\u73b0\u8bc4\u5206\u91cf\u5316\u8868\u73b0\u504f\u89c1\uff0c\u91c7\u7528\u793e\u4f1a\u8bed\u8a00\u5b66\u8fc7\u6ee4\u68c0\u6d4b\u7f13\u89e3\u523b\u677f\u5370\u8c61\uff0c\u7ed3\u5408\u8bed\u6cd5\u548c\u4e0a\u4e0b\u6587\u7684\u53cd\u4e8b\u5b9e\u6570\u636e\u589e\u5f3a\u8865\u507f\u504f\u89c1\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u6d41\u7a0b\u80fd\u51cf\u5c11\u6587\u672c\u6570\u636e\u4e2d\u7684\u8868\u73b0\u504f\u89c1\u548c\u523b\u677f\u5370\u8c61\u3002\u57fa\u4e8e\u53bb\u504f\u6570\u636e\u5fae\u8c03\u7684\u6a21\u578b\u5728\u504f\u89c1\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u4e0d\u7a33\u5b9a\uff0c\u66b4\u9732\u4e86\u8bc4\u4ef7\u65b9\u6cd5\u7684\u4e0d\u8db3\u548c\u9700\u9488\u5bf9\u6027\u6570\u636e\u5904\u7406\u4ee5\u66f4\u597d\u7f13\u89e3\u6a21\u578b\u504f\u89c1\u3002", "conclusion": "\u901a\u8fc7\u591a\u7ec4\u4ef6\u6d41\u7a0b\u6709\u6548\u51cf\u5c11\u6587\u672c\u6570\u636e\u4e2d\u7684\u8868\u73b0\u504f\u89c1\u548c\u663e\u6027\u523b\u677f\u5370\u8c61\uff0c\u4f46\u5bf9\u53bb\u504f\u6570\u636e\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\u5e76\u4e0d\u603b\u80fd\u5e26\u6765\u504f\u89c1\u6027\u80fd\u63d0\u5347\uff0c\u8bf4\u660e\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u4e0d\u8db3\u3002"}}
{"id": "2512.10739", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10739", "abs": "https://arxiv.org/abs/2512.10739", "authors": ["Songyang Gao", "Yuzhe Gu", "Zijian Wu", "Lingkai Kong", "Wenwei Zhang", "Zhongrui Cai", "Fan Zheng", "Tianyou Ma", "Junhao Shen", "Haiteng Zhao", "Duanyang Zhang", "Huilun Zhang", "Kuikun Liu", "Chengqi Lyu", "Yanhui Duan", "Chiyu Chen", "Ningsheng Ma", "Jianfei Gao", "Han Lyu", "Dahua Lin", "Kai Chen"], "title": "Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving", "comment": null, "summary": "Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the \\textbf{O}utcome-based \\textbf{P}rocess \\textbf{V}erifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out \\textsc{\\thisbench}, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2\\% to 73.3\\% on AIME2025 as the compute budget scales.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8fc7\u7a0b\u548c\u7ed3\u679c\u7684\u9a8c\u8bc1\u5668(OPV)\uff0c\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u548c\u4e13\u5bb6\u6807\u6ce8\u63d0\u5347\u63a8\u7406\u68c0\u6d4b\u80fd\u529b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u63d0\u5347\u590d\u6742\u63a8\u7406\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u7ed3\u679c\u7684\u9a8c\u8bc1\u5668\u65e0\u6cd5\u68c0\u67e5\u957f\u63a8\u7406\u94fe\u4e2d\u7684\u4e0d\u53ef\u9760\u4e2d\u95f4\u6b65\u9aa4\uff0c\u57fa\u4e8e\u8fc7\u7a0b\u7684\u9a8c\u8bc1\u5668\u53d7\u9650\u4e8e\u9ad8\u8d28\u91cf\u6807\u6ce8\u7684\u7f3a\u4e4f\uff0c\u96be\u4ee5\u53ef\u9760\u68c0\u6d4b\u590d\u6742\u957f\u63a8\u7406\u94fe\u7684\u9519\u8bef\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u7ed3\u679c\u7684\u8fc7\u7a0b\u9a8c\u8bc1\u5668(OPV)\uff0c\u901a\u8fc7\u5bf9\u957f\u63a8\u7406\u94fe\u7684\u603b\u7ed3\u7ed3\u679c\u8fdb\u884c\u9a8c\u8bc1\uff0c\u7ed3\u5408\u8fed\u4ee3\u4e3b\u52a8\u5b66\u4e60\u548c\u4e13\u5bb6\u6807\u6ce8\uff0c\u91c7\u7528\u62d2\u7edd\u5fae\u8c03\u53ca\u5f3a\u5316\u5b66\u4e60\u6765\u63d0\u5347\u9a8c\u8bc1\u80fd\u529b\u3002", "result": "OPV\u5728\u4fdd\u7559\u7684\u6d4b\u8bd5\u96c6\u4e0a\u53d6\u5f97\u4e8683.1\u7684F1\u5f97\u5206\uff0c\u4f18\u4e8e\u66f4\u5927\u7684\u5f00\u6e90\u6a21\u578bQwen3-Max-Preview\u768476.3\uff0c\u5e76\u4e14\u80fd\u6709\u6548\u68c0\u6d4b\u5408\u6210\u6570\u636e\u4e2d\u7684\u8bef\u62a5\uff0c\u63d0\u5347\u7b56\u7565\u6a21\u578b\u5982DeepSeek-R1-Distill-Qwen-32B\u7684\u51c6\u786e\u7387\u4ece55.2%\u523073.3%\u3002", "conclusion": "OPV\u9a8c\u8bc1\u5668\u5b9e\u73b0\u4e86\u51c6\u786e\u3001\u9ad8\u6548\u7684\u5927\u89c4\u6a21\u9a8c\u8bc1\uff0c\u89e3\u51b3\u4e86\u957f\u63a8\u7406\u94fe\u9519\u8bef\u68c0\u6d4b\u96be\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002"}}
{"id": "2512.10741", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.10741", "abs": "https://arxiv.org/abs/2512.10741", "authors": ["Elroy Galbraith", "Chadwick Sutherland", "Donahue Morgan"], "title": "TRIDENT: A Redundant Architecture for Caribbean-Accented Emergency Speech Triage", "comment": null, "summary": "Emergency speech recognition systems exhibit systematic performance degradation on non-standard English varieties, creating a critical gap in services for Caribbean populations. We present TRIDENT (Transcription and Routing Intelligence for Dispatcher-Empowered National Triage), a three-layer dispatcher-support architecture designed to structure emergency call inputs for human application of established triage protocols (the ESI for routine operations and START for mass casualty events), even when automatic speech recognition fails.\n  The system combines Caribbean-accent-tuned ASR, local entity extraction via large language models, and bio-acoustic distress detection to provide dispatchers with three complementary signals: transcription confidence, structured clinical entities, and vocal stress indicators. Our key insight is that low ASR confidence, rather than representing system failure, serves as a valuable queue prioritization signal -- particularly when combined with elevated vocal distress markers indicating a caller in crisis whose speech may have shifted toward basilectal registers. A complementary insight drives the entity extraction layer: trained responders and composed bystanders may report life-threatening emergencies without elevated vocal stress, requiring semantic analysis to capture clinical indicators that paralinguistic features miss.\n  We describe the architectural design, theoretical grounding in psycholinguistic research on stress-induced code-switching, and deployment considerations for offline operation during disaster scenarios. This work establishes a framework for accent-resilient emergency AI that ensures Caribbean voices receive equitable access to established national triage protocols. Empirical validation on Caribbean emergency calls remains future work.", "AI": {"tldr": "TRIDENT\u7cfb\u7edf\u7ed3\u5408\u53e3\u97f3\u8c03\u4f18\u8bed\u97f3\u8bc6\u522b\u3001\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u4f53\u63d0\u53d6\u548c\u58f0\u97f3\u538b\u529b\u68c0\u6d4b\uff0c\u4ee5\u652f\u6301\u52a0\u52d2\u6bd4\u5730\u533a\u7d27\u6025\u547c\u53eb\u5206\u8bca\uff0c\u63d0\u5347\u975e\u6807\u51c6\u82f1\u8bed\u73af\u5883\u4e0b\u7684\u670d\u52a1\u516c\u5e73\u6027\u3002", "motivation": "\u52a0\u52d2\u6bd4\u5730\u533a\u975e\u6807\u51c6\u82f1\u8bed\u53d8\u4f53\u5bfc\u81f4\u7d27\u6025\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u6027\u80fd\u4e0b\u964d\uff0c\u9020\u6210\u5f53\u5730\u5c45\u6c11\u670d\u52a1\u83b7\u53d6\u4e0d\u5e73\u7b49\uff0c\u4e9f\u9700\u53e3\u97f3\u9c81\u68d2\u7684\u7d27\u6025\u5206\u8bcaAI\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4e09\u5c42\u67b6\u6784\uff1a\u52a0\u52d2\u6bd4\u53e3\u97f3\u8c03\u4f18\u7684\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u672c\u5730\u5b9e\u4f53\u63d0\u53d6\u3001\u751f\u7269\u58f0\u5b66\u7684\u58f0\u97f3\u538b\u529b\u68c0\u6d4b\uff0c\u4e09\u8005\u4e92\u8865\u652f\u6301\u8c03\u5ea6\u5458\u5206\u8bca\u51b3\u7b56\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86TRIDENT\u7cfb\u7edf\uff0c\u4e00\u79cd\u9488\u5bf9\u52a0\u52d2\u6bd4\u5730\u533a\u7d27\u6025\u547c\u53eb\u7684\u4e09\u5c42\u8c03\u5ea6\u5458\u652f\u6301\u67b6\u6784\uff0c\u4ee5\u89e3\u51b3\u975e\u6807\u51c6\u82f1\u8bed\u53d8\u4f53\u5bfc\u81f4\u7684\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002TRIDENT\u7ed3\u5408\u4e86\u9488\u5bf9\u52a0\u52d2\u6bd4\u53e3\u97f3\u8c03\u4f18\u7684\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b9e\u4f53\u63d0\u53d6\u548c\u751f\u7269\u58f0\u5b66\u538b\u529b\u68c0\u6d4b\uff0c\u5411\u8c03\u5ea6\u5458\u63d0\u4f9b\u8f6c\u5f55\u7f6e\u4fe1\u5ea6\u3001\u7ed3\u6784\u5316\u4e34\u5e8a\u5b9e\u4f53\u548c\u58f0\u97f3\u538b\u529b\u6307\u6807\u4e09\u79cd\u4e92\u8865\u4fe1\u53f7\u3002\u4f5c\u8005\u5f3a\u8c03\u4f4e\u7f6e\u4fe1\u5ea6\u53cd\u6620\u7684\u662f\u4f18\u5148\u961f\u5217\u4fe1\u53f7\u800c\u975e\u7cfb\u7edf\u5931\u8d25\uff0c\u5c24\u5176\u7ed3\u5408\u58f0\u97f3\u538b\u529b\u6307\u6807\u53ef\u4ee5\u8bc6\u522b\u5904\u4e8e\u5371\u673a\u4e14\u53ef\u80fd\u5207\u6362\u5230\u57fa\u8bed\u53d8\u4f53\u7684\u547c\u53eb\u8005\u3002\u6b64\u5916\uff0c\u5b9e\u4f53\u63d0\u53d6\u5c42\u901a\u8fc7\u8bed\u4e49\u5206\u6790\u6355\u83b7\u751f\u547d\u5a01\u80c1\u4fe1\u53f7\uff0c\u5f25\u8865\u4e86\u65c1\u8bed\u8a00\u7279\u5f81\u7684\u4e0d\u8db3\u3002\u6587\u7ae0\u8bbe\u8ba1\u4e86\u7cfb\u7edf\u67b6\u6784\uff0c\u5e76\u57fa\u4e8e\u5fc3\u7406\u8bed\u8a00\u5b66\u5173\u4e8e\u538b\u529b\u5f15\u53d1\u4ee3\u7801\u5207\u6362\u7684\u7406\u8bba\u8fdb\u884c\u4e86\u8bba\u8bc1\uff0c\u8ba8\u8bba\u4e86\u707e\u5bb3\u573a\u666f\u4e0b\u79bb\u7ebf\u8fd0\u884c\u7684\u90e8\u7f72\u8003\u8651\u3002\u8be5\u5de5\u4f5c\u4e3a\u53e3\u97f3\u9c81\u68d2\u7684\u7d27\u6025AI\u63d0\u4f9b\u4e86\u6846\u67b6\uff0c\u786e\u4fdd\u52a0\u52d2\u6bd4\u58f0\u97f3\u516c\u5e73\u63a5\u5165\u56fd\u5bb6\u5206\u8bca\u534f\u8bae\uff0c\u4f46\u5c1a\u672a\u8fdb\u884c\u52a0\u52d2\u6bd4\u7d27\u6025\u547c\u53eb\u7684\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "conclusion": "TRIDENT\u4e3a\u52a0\u52d2\u6bd4\u82f1\u8bed\u53e3\u97f3\u73af\u5883\u4e2d\u7684\u7d27\u6025\u547c\u53eb\u63d0\u4f9b\u4e86\u591a\u4fe1\u53f7\u652f\u6301\u6846\u67b6\uff0c\u6709\u671b\u63d0\u5347\u5206\u8bca\u51c6\u786e\u6027\u548c\u516c\u5e73\u6027\uff0c\u5b9e\u8bc1\u9a8c\u8bc1\u5c06\u5728\u672a\u6765\u5f00\u5c55\u3002"}}
{"id": "2512.10756", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.10756", "abs": "https://arxiv.org/abs/2512.10756", "authors": ["Zijian Wu", "Lingkai Kong", "Wenwei Zhang", "Songyang Gao", "Yuzhe Gu", "Zhongrui Cai", "Tianyou Ma", "Yuhong Liu", "Zhi Wang", "Runyuan Ma", "Guangyu Wang", "Wei Li", "Conghui He", "Dahua Lin", "Kai Chen"], "title": "OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification", "comment": null, "summary": "Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out OPV-Bench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2% to 73.3% on AIME2025 as the compute budget scales.", "AI": {"tldr": "OPV\u65b9\u6cd5\u901a\u8fc7\u603b\u7ed3\u7ed3\u679c\u9a8c\u8bc1\u957f\u63a8\u7406\u94fe\u8fc7\u7a0b\uff0c\u7ed3\u5408\u4e3b\u52a8\u5b66\u4e60\u964d\u4f4e\u6807\u6ce8\u6210\u672c\uff0c\u5b9e\u73b0\u4e86\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u63a8\u7406\u94fe\u7684\u9ad8\u6548\u4e14\u51c6\u786e\u9a8c\u8bc1\uff0c\u53d6\u5f97\u4e86\u9886\u5148\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7ed3\u679c\u9a8c\u8bc1\u5668\u96be\u4ee5\u68c0\u67e5\u957f\u63a8\u7406\u94fe\u4e2d\u7684\u4e2d\u95f4\u4e0d\u53ef\u9760\u6b65\u9aa4\uff0c\u8fc7\u7a0b\u9a8c\u8bc1\u5668\u53d7\u9650\u4e8e\u9ad8\u8d28\u91cf\u6807\u6ce8\u7f3a\u4e4f\uff0c\u96be\u4ee5\u51c6\u786e\u68c0\u6d4b\u590d\u6742\u957f\u63a8\u7406\u94fe\u4e2d\u7684\u9519\u8bef\uff0c\u56e0\u6b64\u8bbe\u8ba1OPV\u4ee5\u517c\u987e\u51c6\u786e\u6027\u4e0e\u9a8c\u8bc1\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u603b\u7ed3\u7ed3\u679c\u7684\u8fc7\u7a0b\u9a8c\u8bc1\u65b9\u6cd5OPV\uff0c\u7ed3\u5408\u8fed\u4ee3\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\u548c\u4e13\u5bb6\u6ce8\u91ca\uff0c\u901a\u8fc7\u62d2\u7edd\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u6a21\u578b\u9a8c\u8bc1\u80fd\u529b\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u7ed3\u679c\u8fc7\u7a0b\u9a8c\u8bc1\u5668\uff08OPV\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u94fe\u8fc7\u7a0b\u8fdb\u884c\u51c6\u786e\u4e14\u9ad8\u6548\u7684\u9a8c\u8bc1\u3002OPV\u7ed3\u5408\u4e86\u7ed3\u679c\u9a8c\u8bc1\u548c\u8fc7\u7a0b\u9a8c\u8bc1\u7684\u4f18\u70b9\uff0c\u901a\u8fc7\u5bf9\u957f\u63a8\u7406\u94fe\u7684\u603b\u7ed3\u7ed3\u679c\u8fdb\u884c\u8fc7\u7a0b\u9a8c\u8bc1\uff0c\u89e3\u51b3\u4e86\u4ee5\u5f80\u9a8c\u8bc1\u5668\u5728\u4e2d\u95f4\u6b65\u9aa4\u68c0\u67e5\u4e2d\u7684\u4e0d\u8db3\u3002\u6587\u7ae0\u91c7\u7528\u8fed\u4ee3\u5f0f\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u4e13\u5bb6\u6ce8\u91ca\u4ee5\u964d\u4f4e\u6807\u6ce8\u6210\u672c\uff0c\u9010\u6b65\u63d0\u5347\u9a8c\u8bc1\u80fd\u529b\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cOPV\u5728\u591a\u9879\u8bc4\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u8d85\u8d8a\u4e86\u73b0\u6709\u5927\u578b\u5f00\u6e90\u6a21\u578b\uff0c\u4e14\u80fd\u6709\u6548\u8bc6\u522b\u5408\u6210\u6570\u636e\u4e2d\u7684\u5047\u9633\u6027\uff0c\u5728\u4e0e\u7b56\u7565\u6a21\u578b\u5408\u4f5c\u65f6\u4e5f\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "OPV\u6709\u6548\u63d0\u5347\u4e86\u957f\u63a8\u7406\u94fe\u7684\u9a8c\u8bc1\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u964d\u4f4e\u4e86\u4eba\u5de5\u6807\u6ce8\u6210\u672c\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5177\u5907\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2512.10772", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.10772", "abs": "https://arxiv.org/abs/2512.10772", "authors": ["Kevin Glocker", "K\u00e4triin Kukk", "Romina Oji", "Marcel Bollmann", "Marco Kuhlmann", "Jenny Kunz"], "title": "Grow Up and Merge: Scaling Strategies for Efficient Language Adaptation", "comment": null, "summary": "Achieving high-performing language models which include medium- and lower-resource languages remains a challenge. Massively multilingual models still underperform compared to language-specific adaptations, especially at smaller model scales. In this work, we investigate scaling as an efficient strategy for adapting pretrained models to new target languages. Through comprehensive scaling ablations with approximately FLOP-matched models, we test whether upscaling an English base model enables more effective and resource-efficient adaptation than standard continued pretraining. We find that, once exposed to sufficient target-language data, larger upscaled models can match or surpass the performance of smaller models continually pretrained on much more data, demonstrating the benefits of scaling for data efficiency. Scaling also helps preserve the base model's capabilities in English, thus reducing catastrophic forgetting. Finally, we explore whether such scaled, language-specific models can be merged to construct modular and flexible multilingual systems. We find that while merging remains less effective than joint multilingual training, upscaled merges perform better than smaller ones. We observe large performance differences across merging methods, suggesting potential for improvement through merging approaches specialized for language-level integration.", "AI": {"tldr": "\u901a\u8fc7\u653e\u5927\u57fa\u4e8e\u82f1\u6587\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u80fd\u66f4\u9ad8\u6548\u9002\u5e94\u591a\u8bed\u8a00\u3001\u63d0\u9ad8\u6570\u636e\u5229\u7528\u7387\u5e76\u51cf\u5c11\u80fd\u529b\u9057\u5fd8\uff0c\u5408\u5e76\u6269\u5c55\u6a21\u578b\u53ef\u6784\u5efa\u7075\u6d3b\u591a\u8bed\u7cfb\u7edf\uff0c\u4f46\u5408\u5e76\u7b56\u7565\u4ecd\u6709\u4f18\u5316\u7a7a\u95f4\u3002", "motivation": "\u591a\u8bed\u79cd\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u4e2d\u4f4e\u8d44\u6e90\u8bed\u8a00\u8868\u73b0\u4e0d\u4f73\u4e14\u8d44\u6e90\u6d88\u8017\u5927\uff0c\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u6a21\u578b\u6269\u5c55\u63d0\u5347\u591a\u8bed\u6a21\u578b\u9488\u5bf9\u7279\u5b9a\u8bed\u8a00\u9002\u5e94\u6548\u679c\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u3001\u66f4\u7a33\u5b9a\u7684\u8de8\u8bed\u8a00\u8fc1\u79fb\u4e0e\u591a\u8bed\u8a00\u7cfb\u7edf\u6784\u5efa\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u63a7\u5236FLOP\u91cf\u5339\u914d\u7684\u4e0d\u540c\u89c4\u6a21\u82f1\u6587\u57fa\u6a21\u578b\uff0c\u8fdb\u884c\u76ee\u6807\u8bed\u8a00\u6301\u7eed\u9884\u8bad\u7ec3\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u6269\u5c55\u5927\u5c0f\u6a21\u578b\u7684\u9002\u5e94\u6548\u679c\u4e0e\u6570\u636e\u6548\u7387\uff0c\u5e76\u5c1d\u8bd5\u5c06\u6269\u5c55\u8bad\u7ec3\u540e\u7684\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u591a\u6a21\u578b\u5408\u5e76\uff0c\u6bd4\u8f83\u5408\u5e76\u6027\u80fd\u53ca\u4e0d\u540c\u5408\u5e76\u7b56\u7565\u6548\u679c\u3002", "result": "\u672c\u8bba\u6587\u7814\u7a76\u4e86\u901a\u8fc7\u6a21\u578b\u6269\u5c55\uff08scaling\uff09\u4f5c\u4e3a\u9002\u5e94\u65b0\u76ee\u6807\u8bed\u8a00\u7684\u9ad8\u6548\u7b56\u7565\uff0c\u5c24\u5176\u9488\u5bf9\u4e2d\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u3002\u901a\u8fc7\u7ea6\u7b49Flop\u8ba1\u7b97\u91cf\u5339\u914d\u7684\u6a21\u578b\u5728\u6570\u636e\u5145\u8db3\u7684\u60c5\u51b5\u4e0b\uff0c\u53d1\u73b0\u653e\u5927\u540e\u7684\u82f1\u6587\u57fa\u7840\u6a21\u578b\u5728\u9002\u5e94\u76ee\u6807\u8bed\u8a00\u65f6\u8868\u73b0\u53ef\u5339\u654c\u751a\u81f3\u8d85\u8fc7\u4f7f\u7528\u66f4\u591a\u6570\u636e\u7684\u8f83\u5c0f\u6a21\u578b\u7ee7\u7eed\u9884\u8bad\u7ec3\u7ed3\u679c\uff0c\u8bf4\u660e\u6a21\u578b\u6269\u5c55\u63d0\u9ad8\u4e86\u6570\u636e\u6548\u7387\u3002\u540c\u65f6\uff0c\u6269\u5c55\u6a21\u578b\u80fd\u66f4\u597d\u4fdd\u7559\u82f1\u6587\u57fa\u7840\u80fd\u529b\uff0c\u51cf\u8f7b\u9057\u5fd8\u3002\u6700\u540e\uff0c\u4f5c\u8005\u5c1d\u8bd5\u5c06\u6269\u5c55\u7684\u8bed\u8a00\u4e13\u7528\u6a21\u578b\u5408\u5e76\u6784\u5efa\u6a21\u5757\u5316\u591a\u8bed\u79cd\u7cfb\u7edf\uff0c\u7ed3\u679c\u867d\u4e0d\u53ca\u8054\u5408\u591a\u8bed\u8bad\u7ec3\uff0c\u4f46\u6269\u5c55\u6a21\u578b\u5408\u5e76\u6548\u679c\u4f18\u4e8e\u8f83\u5c0f\u6a21\u578b\uff0c\u4e14\u5408\u5e76\u65b9\u6cd5\u6709\u8f83\u5927\u6027\u80fd\u5dee\u5f02\uff0c\u8868\u660e\u4e13\u95e8\u9488\u5bf9\u8bed\u8a00\u7ea7\u878d\u5408\u7684\u5408\u5e76\u7b56\u7565\u6709\u8fdb\u4e00\u6b65\u63d0\u5347\u6f5c\u529b\u3002", "conclusion": "\u6a21\u578b\u6269\u5c55\u662f\u63d0\u5347\u4e2d\u4f4e\u8d44\u6e90\u8bed\u8a00\u9002\u5e94\u6548\u679c\u7684\u6709\u6548\u624b\u6bb5\uff0c\u53ef\u5728\u4fdd\u8bc1\u82f1\u6587\u57fa\u7840\u80fd\u529b\u7684\u540c\u65f6\u51cf\u5c11\u9057\u5fd8\uff0c\u63d0\u9ad8\u6570\u636e\u5229\u7528\u6548\u7387\u3002\u6269\u5c55\u6a21\u578b\u5408\u5e76\u8868\u73b0\u4f18\u4e8e\u5c0f\u6a21\u578b\uff0c\u4f46\u5408\u5e76\u6548\u679c\u4e0e\u8054\u5408\u8bad\u7ec3\u76f8\u6bd4\u4ecd\u6709\u5dee\u8ddd\uff0c\u9700\u53d1\u5c55\u8bed\u8a00\u7ea7\u522b\u7684\u5408\u5e76\u65b9\u6cd5\u3002"}}
{"id": "2512.10780", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.10780", "abs": "https://arxiv.org/abs/2512.10780", "authors": ["Manurag Khullar", "Utkarsh Desai", "Poorva Malviya", "Aman Dalmia", "Zheyuan Ryan Shi"], "title": "Script Gap: Evaluating LLM Triage on Indian Languages in Native vs Roman Scripts in a Real World Setting", "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in high-stakes clinical applications in India. In many such settings, speakers of Indian languages frequently communicate using romanized text rather than native scripts, yet existing research rarely evaluates this orthographic variation using real-world data. We investigate how romanization impacts the reliability of LLMs in a critical domain: maternal and newborn healthcare triage. We benchmark leading LLMs on a real-world dataset of user-generated queries spanning five Indian languages and Nepali. Our results reveal consistent degradation in performance for romanized messages, with F1 scores trailing those of native scripts by 5-12 points. At our partner maternal health organization in India, this gap could cause nearly 2 million excess errors in triage. Crucially, this performance gap by scripts is not due to a failure in clinical reasoning. We demonstrate that LLMs often correctly infer the semantic intent of romanized queries. Nevertheless, their final classification outputs remain brittle in the presence of orthographic noise in romanized inputs. Our findings highlight a critical safety blind spot in LLM-based health systems: models that appear to understand romanized input may still fail to act on it reliably.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u7f57\u9a6c\u5316\u6587\u672c\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5370\u5ea6\u4ea7\u5987\u548c\u65b0\u751f\u513f\u533b\u7597\u5206\u8bca\u9886\u57df\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u7f57\u9a6c\u5316\u6587\u672c\u4f7f\u6a21\u578b\u8868\u73b0\u4e0b\u964d5-12\u4e2aF1\u5206\u6570\u70b9\uff0c\u53ef\u80fd\u5bfc\u81f4\u5927\u91cf\u8bef\u8bca\u3002\u867d\u7136\u6a21\u578b\u80fd\u7406\u89e3\u8bed\u4e49\u610f\u56fe\uff0c\u4f46\u5206\u7c7b\u7ed3\u679c\u53d7\u6b63\u5b57\u6cd5\u566a\u58f0\u5f71\u54cd\uff0c\u5b58\u5728\u53ef\u9760\u6027\u9690\u60a3\u3002", "motivation": "\u5370\u5ea6\u591a\u8bed\u8a00\u73af\u5883\u4e2d\uff0c\u7528\u6237\u7ecf\u5e38\u4f7f\u7528\u7f57\u9a6c\u5316\u6587\u672c\u8fdb\u884c\u4ea4\u6d41\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5f88\u5c11\u7528\u771f\u5b9e\u6570\u636e\u68c0\u6d4b\u7f57\u9a6c\u5316\u5bf9LLMs\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u5728\u5173\u952e\u533b\u7597\u573a\u666f\u3002", "method": "\u5728\u771f\u5b9e\u7528\u6237\u751f\u6210\u7684\u6d89\u53ca\u4e94\u79cd\u5370\u5ea6\u8bed\u8a00\u548c\u5c3c\u6cca\u5c14\u8bed\u7684\u67e5\u8be2\u6570\u636e\u96c6\u4e0a\uff0c\u8bc4\u4f30\u591a\u6b3e\u4e3b\u6d41LLMs\u5bf9\u6bcd\u5a74\u5065\u5eb7\u5206\u8bca\u4efb\u52a1\u7684\u8868\u73b0\uff0c\u6bd4\u8f83\u7f57\u9a6c\u5316\u6587\u672c\u4e0e\u672c\u571f\u6587\u5b57\u7684\u5dee\u5f02\u3002", "result": "\u7f57\u9a6c\u5316\u6587\u672c\u5bfc\u81f4\u6a21\u578bF1\u5206\u6570\u964d\u4f4e5-12\u4e2a\u70b9\uff0c\u5728\u5b9e\u9645\u5408\u4f5c\u533b\u7597\u673a\u6784\u4f30\u8ba1\u53ef\u5bfc\u81f4\u8fd1200\u4e07\u6b21\u989d\u5916\u9519\u8bef\u5206\u8bca\uff1b\u8bed\u4e49\u63a8\u65ad\u51c6\u786e\u4f46\u6700\u7ec8\u5206\u7c7b\u4e0d\u7a33\u5b9a\u3002", "conclusion": "\u7f57\u9a6c\u5316\u6587\u672c\u663e\u8457\u964d\u4f4e\u4e86LLMs\u5728\u4e34\u5e8a\u5206\u8bca\u4e2d\u7684\u51c6\u786e\u6027\uff0c\u9020\u6210\u5927\u91cf\u6f5c\u5728\u8bef\u8bca\uff1b\u6a21\u578b\u8bed\u4e49\u7406\u89e3\u826f\u597d\u4f46\u6700\u7ec8\u8f93\u51fa\u53d7\u6b63\u5b57\u6cd5\u5e72\u6270\uff0c\u4e0d\u8db3\u4ee5\u4fdd\u8bc1\u5b89\u5168\u5e94\u7528\u3002"}}
{"id": "2512.10791", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10791", "abs": "https://arxiv.org/abs/2512.10791", "authors": ["Aileen Cheng", "Alon Jacovi", "Amir Globerson", "Ben Golan", "Charles Kwong", "Chris Alberti", "Connie Tao", "Eyal Ben-David", "Gaurav Singh Tomar", "Lukas Haas", "Yonatan Bitton", "Adam Bloniarz", "Aijun Bai", "Andrew Wang", "Anfal Siddiqui", "Arturo Bajuelos Castillo", "Aviel Atias", "Chang Liu", "Corey Fry", "Daniel Balle", "Deepanway Ghosal", "Doron Kukliansky", "Dror Marcus", "Elena Gribovskaya", "Eran Ofek", "Honglei Zhuang", "Itay Laish", "Jan Ackermann", "Lily Wang", "Meg Risdal", "Megan Barnes", "Michael Fink", "Mohamed Amin", "Moran Ambar", "Natan Potikha", "Nikita Gupta", "Nitzan Katz", "Noam Velan", "Ofir Roval", "Ori Ram", "Polina Zablotskaia", "Prathamesh Bang", "Priyanka Agrawal", "Rakesh Ghiya", "Sanjay Ganapathy", "Simon Baumgartner", "Sofia Erell", "Sushant Prakash", "Thibault Sellam", "Vikram Rao", "Xuanhui Wang", "Yaroslav Akulov", "Yulong Yang", "Zhen Yang", "Zhixin Lai", "Zhongru Wu", "Anca Dragan", "Avinatan Hassidim", "Fernando Pereira", "Slav Petrov", "Srinivasan Venkatachary", "Tulsee Doshi", "Yossi Matias", "Sasha Goldshtein", "Dipanjan Das"], "title": "The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality", "comment": null, "summary": "We introduce The FACTS Leaderboard, an online leaderboard suite and associated set of benchmarks that comprehensively evaluates the ability of language models to generate factually accurate text across diverse scenarios. The suite provides a holistic measure of factuality by aggregating the performance of models on four distinct sub-leaderboards: (1) FACTS Multimodal, which measures the factuality of responses to image-based questions; (2) FACTS Parametric, which assesses models' world knowledge by answering closed-book factoid questions from internal parameters; (3) FACTS Search, which evaluates factuality in information-seeking scenarios, where the model must use a search API; and (4) FACTS Grounding (v2), which evaluates whether long-form responses are grounded in provided documents, featuring significantly improved judge models. Each sub-leaderboard employs automated judge models to score model responses, and the final suite score is an average of the four components, designed to provide a robust and balanced assessment of a model's overall factuality. The FACTS Leaderboard Suite will be actively maintained, containing both public and private splits to allow for external participation while guarding its integrity. It can be found at https://www.kaggle.com/benchmarks/google/facts .", "AI": {"tldr": "FACTS\u6392\u884c\u699c\u662f\u4e00\u5957\u7efc\u5408\u8bed\u8a00\u6a21\u578b\u4e8b\u5b9e\u51c6\u786e\u6027\u8bc4\u4f30\u4f53\u7cfb\uff0c\u5305\u542b\u591a\u6a21\u6001\u95ee\u7b54\u3001\u53c2\u6570\u77e5\u8bc6\u3001\u641c\u7d22\u8f85\u52a9\u548c\u4f9d\u636e\u6587\u6863\u7684\u957f\u6587\u672c\u8bc4\u4f30\uff0c\u63d0\u4f9b\u5747\u8861\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u6d4b\u91cf\u3002", "motivation": "\u5f53\u524d\u7684\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u4e8b\u5b9e\u6587\u672c\u65f6\u5b58\u5728\u51c6\u786e\u6027\u96be\u4ee5\u5168\u9762\u8bc4\u4f30\u7684\u95ee\u9898\uff0c\u4e9f\u9700\u4e00\u4e2a\u7cfb\u7edf\u5316\u3001\u591a\u89d2\u5ea6\u8861\u91cf\u6a21\u578b\u4e8b\u5b9e\u51c6\u786e\u6027\u7684\u6807\u51c6\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u56db\u4e2a\u5b50\u6392\u884c\u699c\u7684\u7efc\u5408\u8bc4\u4f30\u5957\u4ef6\uff0c\u5229\u7528\u81ea\u52a8\u8bc4\u5224\u6a21\u578b\u5bf9\u591a\u6a21\u6001\u3001\u53c2\u6570\u5185\u77e5\u8bc6\u3001\u641c\u7d22\u4fe1\u606f\u68c0\u7d22\u53ca\u6587\u672c\u4f9d\u6258\u6587\u6863\u51c6\u786e\u6027\u8fdb\u884c\u8bc4\u5206\uff0c\u5e76\u8ba1\u7b97\u7efc\u5408\u5f97\u5206\u3002", "result": "FACTS\u6392\u884c\u699c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5728\u7ebf\u6d3b\u8dc3\u7ef4\u62a4\u7684\u7efc\u5408\u8bc4\u6d4b\u5e73\u53f0\uff0c\u6db5\u76d6\u591a\u79cd\u4e8b\u5b9e\u8bc4\u4f30\u573a\u666f\uff0c\u652f\u6301\u516c\u5f00\u53ca\u79c1\u6709\u6570\u636e\u5206\u5272\uff0c\u4fc3\u8fdb\u5916\u90e8\u53c2\u4e0e\u5e76\u4fdd\u969c\u8bc4\u6d4b\u7684\u4e25\u8c28\u6027\u3002", "conclusion": "FACTS\u6392\u884c\u699c\u4e3a\u8bed\u8a00\u6a21\u578b\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u63d0\u4f9b\u4e86\u5168\u9762\u548c\u5747\u8861\u7684\u8bc4\u4f30\uff0c\u4e3a\u4fc3\u8fdb\u66f4\u7cbe\u51c6\u7684\u6587\u672c\u751f\u6210\u8bbe\u7acb\u4e86\u6807\u51c6\u3002"}}
{"id": "2512.10793", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10793", "abs": "https://arxiv.org/abs/2512.10793", "authors": ["Michael Schlee", "Christoph Weisser", "Timo Kivim\u00e4ki", "Melchizedek Mashiku", "Benjamin Saefken"], "title": "LabelFusion: Learning to Fuse LLMs and Transformer Classifiers for Robust Text Classification", "comment": null, "summary": "LabelFusion is a fusion ensemble for text classification that learns to combine a traditional transformer-based classifier (e.g., RoBERTa) with one or more Large Language Models (LLMs such as OpenAI GPT, Google Gemini, or DeepSeek) to deliver accurate and cost-aware predictions across multi-class and multi-label tasks. The package provides a simple high-level interface (AutoFusionClassifier) that trains the full pipeline end-to-end with minimal configuration, and a flexible API for advanced users. Under the hood, LabelFusion integrates vector signals from both sources by concatenating the ML backbone's embeddings with the LLM-derived per-class scores -- obtained through structured prompt-engineering strategies -- and feeds this joint representation into a compact multi-layer perceptron (FusionMLP) that produces the final prediction. This learned fusion approach captures complementary strengths of LLM reasoning and traditional transformer-based classifiers, yielding robust performance across domains -- achieving 92.4% accuracy on AG News and 92.3% on 10-class Reuters 21578 topic classification -- while enabling practical trade-offs between accuracy, latency, and cost.", "AI": {"tldr": "LabelFusion\u878d\u5408\u8f6c\u6362\u5668\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5229\u7528\u7ed3\u6784\u5316\u63d0\u793a\u5de5\u7a0b\u548c\u591a\u5c42\u611f\u77e5\u673a\u5b66\u4e60\u8054\u5408\u8868\u793a\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u9ad8\u51c6\u786e\u7387\u7684\u591a\u7c7b\u591a\u6807\u7b7e\u6587\u672c\u5206\u7c7b\u3002", "motivation": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u548c\u4f20\u7edftransformer\u5206\u7c7b\u5668\u7684\u4f18\u52bf\uff0c\u63d0\u5347\u6587\u672c\u5206\u7c7b\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u5b9e\u73b0\u6210\u672c\u548c\u5ef6\u8fdf\u7684\u5e73\u8861\u3002", "method": "\u7ed3\u5408\u4f7f\u7528\u57fa\u4e8etransformer\u7684\u4f20\u7edf\u5206\u7c7b\u5668\uff08\u5982RoBERTa\uff09\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982OpenAI GPT\u3001Google Gemini\u7b49\uff09\uff0c\u901a\u8fc7\u4e32\u8054\u4e24\u8005\u7684\u5411\u91cf\u8868\u793a\u5e76\u8f93\u5165\u591a\u5c42\u611f\u77e5\u673a\u8fdb\u884c\u878d\u5408\u5206\u7c7b\u3002", "result": "\u5728AG News\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e8692.4%\u7684\u51c6\u786e\u7387\uff0c\u572810\u7c7bReuters 21578\u4e3b\u9898\u5206\u7c7b\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e8692.3%\u7684\u51c6\u786e\u7387\uff0c\u8868\u73b0\u7a33\u5b9a\u4e14\u9ad8\u6548\u3002", "conclusion": "\u901a\u8fc7\u5b66\u4e60\u878d\u5408LLM\u7684\u63a8\u7406\u5f97\u5206\u548c\u4f20\u7edf\u5206\u7c7b\u5668\u5d4c\u5165\uff0cLabelFusion\u5728\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u51c6\u786e\u6027\u4e0e\u6210\u672c\u6548\u76ca\uff0c\u53ef\u9002\u7528\u4e8e\u591a\u9886\u57df\u591a\u4efb\u52a1\u573a\u666f\u3002"}}
{"id": "2512.10865", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.10865", "abs": "https://arxiv.org/abs/2512.10865", "authors": ["Lilin Qiu"], "title": "Quantifying Emotional Tone in Tolkien's The Hobbit: Dialogue Sentiment Analysis with RegEx, NRC-VAD, and Python", "comment": null, "summary": "This study analyzes the emotional tone of dialogue in J. R. R. Tolkien's The Hobbit (1937) using computational text analysis. Dialogue was extracted with regular expressions, then preprocessed, and scored using the NRC-VAD lexicon to quantify emotional dimensions. The results show that the dialogue maintains a generally positive (high valence) and calm (low arousal) tone, with a gradually increasing sense of agency (dominance) as the story progresses. These patterns reflect the novel's emotional rhythm: moments of danger and excitement are regularly balanced by humor, camaraderie, and relief. Visualizations -- including emotional trajectory graphs and word clouds -- highlight how Tolkien's language cycles between tension and comfort. By combining computational tools with literary interpretation, this study demonstrates how digital methods can uncover subtle emotional structures in literature, revealing the steady rhythm and emotional modulation that shape the storytelling in The Hobbit.", "AI": {"tldr": "\u901a\u8fc7\u8ba1\u7b97\u6587\u672c\u5206\u6790\uff0c\u7814\u7a76\u63ed\u793a\u300a\u970d\u6bd4\u7279\u4eba\u300b\u5bf9\u8bdd\u4e2d\u60c5\u611f\u8272\u8c03\u79ef\u6781\u5e73\u9759\uff0c\u60c5\u611f\u8282\u594f\u5728\u7d27\u5f20\u4e0e\u5b89\u6170\u95f4\u5faa\u73af\uff0c\u53cd\u6620\u4e86\u5c0f\u8bf4\u7684\u6545\u4e8b\u8282\u594f\u548c\u60c5\u611f\u53d8\u5316\u3002", "motivation": "\u901a\u8fc7\u7ed3\u5408\u8ba1\u7b97\u65b9\u6cd5\u4e0e\u6587\u5b66\u5206\u6790\uff0c\u63a2\u7d22\u6587\u672c\u4e2d\u6f5c\u5728\u7684\u60c5\u611f\u7ed3\u6784\uff0c\u63ed\u793a\u6587\u5b66\u4f5c\u54c1\u4e2d\u60c5\u611f\u8282\u594f\u5bf9\u53d9\u4e8b\u8282\u594f\u7684\u5f71\u54cd\uff0c\u8fdb\u800c\u4e30\u5bcc\u5bf9\u300a\u970d\u6bd4\u7279\u4eba\u300b\u60c5\u611f\u97f5\u5f8b\u7684\u7406\u89e3\u3002", "method": "\u4f7f\u7528\u6b63\u5219\u8868\u8fbe\u5f0f\u63d0\u53d6\u300a\u970d\u6bd4\u7279\u4eba\u300b\u4e2d\u7684\u5bf9\u8bdd\u6587\u672c\uff0c\u91c7\u7528NRC-VAD\u60c5\u611f\u8bcd\u5178\u5bf9\u6587\u672c\u8fdb\u884c\u60c5\u611f\u91cf\u5316\u8bc4\u5206\uff0c\u5e76\u901a\u8fc7\u60c5\u611f\u8f68\u8ff9\u56fe\u548c\u8bcd\u4e91\u7b49\u65b9\u5f0f\u8fdb\u884c\u53ef\u89c6\u5316\u5206\u6790\u3002", "result": "\u8be5\u7814\u7a76\u5229\u7528\u8ba1\u7b97\u6587\u672c\u5206\u6790\u65b9\u6cd5\u5bf9J.R.R.\u6258\u5c14\u91d1\u300a\u970d\u6bd4\u7279\u4eba\u300b\u4e2d\u7684\u5bf9\u8bdd\u60c5\u611f\u8272\u8c03\u8fdb\u884c\u4e86\u5206\u6790\u3002\u901a\u8fc7\u6b63\u5219\u8868\u8fbe\u5f0f\u63d0\u53d6\u5bf9\u8bdd\u5185\u5bb9\uff0c\u4f7f\u7528NRC-VAD\u8bcd\u5178\u5bf9\u60c5\u611f\u7ef4\u5ea6\u8fdb\u884c\u91cf\u5316\u8bc4\u5206\uff0c\u7ed3\u679c\u663e\u793a\u5bf9\u8bdd\u6574\u4f53\u5448\u73b0\u51fa\u79ef\u6781\uff08\u9ad8\u6109\u60a6\u5ea6\uff09\u548c\u5e73\u9759\uff08\u4f4e\u5524\u9192\u5ea6\uff09\u7684\u60c5\u611f\u57fa\u8c03\uff0c\u968f\u7740\u6545\u4e8b\u8fdb\u5c55\uff0c\u4e3b\u5bfc\u611f\uff08\u63a7\u5236\u611f\uff09\u9010\u6e10\u589e\u5f3a\u3002\u7814\u7a76\u8fd8\u901a\u8fc7\u60c5\u611f\u8f68\u8ff9\u56fe\u548c\u8bcd\u4e91\u7b49\u53ef\u89c6\u5316\u5de5\u5177\uff0c\u5c55\u793a\u4e86\u300a\u970d\u6bd4\u7279\u4eba\u300b\u4e2d\u7d27\u5f20\u4e0e\u5b89\u6170\u4ea4\u66ff\u51fa\u73b0\u7684\u8bed\u8a00\u8282\u594f\u3002\u8be5\u7814\u7a76\u7ed3\u5408\u4e86\u6570\u5b57\u5316\u65b9\u6cd5\u4e0e\u6587\u5b66\u89e3\u91ca\uff0c\u63ed\u793a\u4e86\u5c0f\u8bf4\u4e2d\u9690\u542b\u7684\u60c5\u611f\u7ed3\u6784\u53ca\u5176\u5bf9\u53d9\u4e8b\u8282\u594f\u7684\u5851\u9020\u3002", "conclusion": "\u300a\u970d\u6bd4\u7279\u4eba\u300b\u5bf9\u8bdd\u5c55\u73b0\u51fa\u79ef\u6781\u3001\u5e73\u9759\u4e14\u9010\u6e10\u589e\u5f3a\u7684\u4e3b\u5bfc\u611f\u60c5\u8c03\uff0c\u60c5\u611f\u8282\u594f\u5728\u5371\u9669\u4e0e\u8f7b\u677e\u4e4b\u95f4\u5faa\u73af\uff0c\u4f53\u73b0\u4e86\u5c0f\u8bf4\u60c5\u611f\u8282\u594f\u7684\u7a33\u56fa\u548c\u8c03\u63a7\uff0c\u4ece\u800c\u652f\u6301\u4e86\u6545\u4e8b\u7684\u53d9\u4e8b\u6548\u679c\u3002"}}
{"id": "2512.10882", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.10882", "abs": "https://arxiv.org/abs/2512.10882", "authors": ["Hauke Licht"], "title": "Computational emotion analysis with multimodal LLMs: Current evidence on an emerging methodological opportunity", "comment": null, "summary": "Emotions are central to politics and analyzing their role in political communication has a long tradition. As research increasingly leverages audio-visual materials to analyze the display of emotions, the emergence of multimodal generative AI promises great advances. However, we lack evidence about the effectiveness of multimodal AI in emotion analysis. This paper addresses this gap by evaluating current multimodal large language models (mLLMs) in video-based analysis of emotional arousal in two complementary data sets of human-labeled video recordings. I find that under ideal circumstances, mLLMs' emotional arousal ratings are highly reliable and show little to know indication of demographic bias. However, in recordings of speakers in real-world parliamentary debates, mLLMs' arousal ratings fail to deliver on this promise with potential negative consequences for downstream statistical inferences. This study therefore underscores the need for continued, thorough evaluation of emerging generative AI methods in political analysis and contributes a suitable replicable framework.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08mLLMs\uff09\u5728\u57fa\u4e8e\u89c6\u9891\u7684\u60c5\u7eea\u6fc0\u53d1\u5206\u6790\u4e2d\u7684\u6548\u679c\uff0c\u53d1\u73b0\u5176\u5728\u7406\u60f3\u73af\u5883\u4e0b\u8868\u73b0\u826f\u597d\u4f46\u5728\u771f\u5b9e\u653f\u6cbb\u8fa9\u8bba\u4e2d\u8868\u73b0\u6b20\u4f73\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u5173\u4e8e\u591a\u6a21\u6001\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u5728\u60c5\u7eea\u5206\u6790\u4e2d\u7684\u6709\u6548\u6027\u8bc1\u636e\uff0c\u7279\u522b\u662f\u5728\u653f\u6cbb\u6c9f\u901a\u9886\u57df\u3002", "method": "\u5229\u7528\u4e24\u7ec4\u5305\u542b\u4eba\u5de5\u6807\u6ce8\u7684\u89c6\u9891\u6570\u636e\u96c6\uff0c\u5bf9\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u89c6\u9891\u4e2d\u60c5\u7eea\u6fc0\u53d1\u7684\u8bc4\u4f30\u6bd4\u8f83\u3002", "result": "mLLMs \u5728\u7406\u60f3\u6570\u636e\u96c6\u4e0a\u7684\u60c5\u7eea\u6fc0\u53d1\u8bc4\u5206\u9ad8\u5ea6\u53ef\u9760\u4e14\u65e0\u660e\u663e\u504f\u89c1\uff0c\u4f46\u5728\u771f\u5b9e\u8bae\u4f1a\u8fa9\u8bba\u89c6\u9891\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u5728\u7406\u60f3\u6761\u4ef6\u4e0b\uff0cmLLMs \u80fd\u591f\u53ef\u9760\u5730\u5206\u6790\u60c5\u7eea\u6fc0\u53d1\u4e14\u65e0\u660e\u663e\u4eba\u53e3\u7edf\u8ba1\u504f\u89c1\uff0c\u4f46\u5728\u771f\u5b9e\u4e16\u754c\u7684\u8bae\u4f1a\u8fa9\u8bba\u89c6\u9891\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u53ef\u80fd\u5f71\u54cd\u540e\u7eed\u7edf\u8ba1\u63a8\u65ad\u3002"}}
