<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 77]
- [cs.MA](#cs.MA) [Total: 5]
- [cs.SE](#cs.SE) [Total: 13]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Benchmarking Open-Source Large Language Models for Persian in Zero-Shot and Few-Shot Learning](https://arxiv.org/abs/2510.12807)
*Mahdi Cherakhloo,Arash Abbasi,Mohammad Saeid Sarafraz,Bijan Vosoughi Vahdat*

Main category: cs.CL

TL;DR: 本文评估了多款开源大语言模型在波斯语自然语言处理任务中的表现，发现Gemma 2表现最佳，但多数模型在命名实体识别等细粒度任务上表现较弱。


<details>
  <summary>Details</summary>
Motivation: 虽有多语言大模型，但针对低资源语言如波斯语的效果尚未充分研究。

Method: 在零样本和少样本情景下，利用ParsiNLU和ArmanEmo数据集，评测情感分析、命名实体识别、阅读理解和问答等任务，采用准确率、F1、BLEU、ROUGE指标。

Result: Gemma 2在几乎所有任务和学习范式中表现优异，尤其是复杂推理任务；但模型普遍难以处理命名实体识别等词元级任务。

Conclusion: 研究为多语言大模型在波斯语领域的应用提供了基准和深刻见解，揭示了当前模型的优势与局限。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
numerous languages; however, their effectiveness in low-resource languages like
Persian requires thorough investigation. This paper presents a comprehensive
benchmark of several open-source LLMs for Persian Natural Language Processing
(NLP) tasks, utilizing both zero-shot and few-shot learning paradigms. We
evaluate models across a range of tasks including sentiment analysis, named
entity recognition, reading comprehension, and question answering, using
established Persian datasets such as ParsiNLU and ArmanEmo. Our methodology
encompasses rigorous experimental setups for both zero-shot and few-shot
scenarios, employing metrics such as Accuracy, F1-score, BLEU, and ROUGE for
performance evaluation. The results reveal that Gemma 2 consistently
outperforms other models across nearly all tasks in both learning paradigms,
with particularly strong performance in complex reasoning tasks. However, most
models struggle with token-level understanding tasks like Named Entity
Recognition, highlighting specific challenges in Persian language processing.
This study contributes to the growing body of research on multilingual LLMs,
providing valuable insights into their performance in Persian and offering a
benchmark for future model development.

</details>


### [2] [Cancer Diagnosis Categorization in Electronic Health Records Using Large Language Models and BioBERT: Model Performance Evaluation Study](https://arxiv.org/abs/2510.12813)
*Soheil Hashtarkhani,Rezaur Rashid,Christopher L Brett,Lokesh Chinthala,Fekede Asefa Kumsa,Janet A Zink,Robert L Davis,David L Schwartz,Arash Shaban-Nejad*

Main category: cs.CL

TL;DR: 本研究比较了五种大语言模型和BioBERT在癌症诊断分类上的表现，发现BioBERT在结构化数据上表现最好，GPT-4o在自由文本上表现领先。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录中的数据格式多样且不一致，需高效预处理以支持预测模型，而现有人工智能自然语言处理工具的性能和临床可靠性尚需系统评估。

Method: 选取3456份癌症患者记录中的762个诊断（包括ICD编码描述和自由文本），使用GPT-3.5、GPT-4o、Llama 3.2、Gemini 1.5及BioBERT对诊断进行14类预定义分类，并由两名肿瘤学专家进行验证。

Result: BioBERT在ICD编码分类中获得最高加权宏F1分数（84.2）并达到90.8%的准确率，GPT-4o在自由文本分类上优于BioBERT，达71.8的F1分数和81.9准确率，其他模型表现较差。发现分类错误通常涉及转移瘤与中枢神经系统肿瘤的混淆及临床术语模糊。

Conclusion: 现阶段模型性能满足行政和研究应用，但临床高风险决策仍需规范化文档和严格人工监督保障可靠性。

Abstract: Electronic health records contain inconsistently structured or free-text
data, requiring efficient preprocessing to enable predictive health care
models. Although artificial intelligence-driven natural language processing
tools show promise for automating diagnosis classification, their comparative
performance and clinical reliability require systematic evaluation. The aim of
this study is to evaluate the performance of 4 large language models (GPT-3.5,
GPT-4o, Llama 3.2, and Gemini 1.5) and BioBERT in classifying cancer diagnoses
from structured and unstructured electronic health records data. We analyzed
762 unique diagnoses (326 International Classification of Diseases (ICD) code
descriptions, 436free-text entries) from 3456 records of patients with cancer.
Models were tested on their ability to categorize diagnoses into 14predefined
categories. Two oncology experts validated classifications. BioBERT achieved
the highest weighted macro F1-score for ICD codes (84.2) and matched GPT-4o in
ICD code accuracy (90.8). For free-text diagnoses, GPT-4o outperformed BioBERT
in weighted macro F1-score (71.8 vs 61.5) and achieved slightly higher accuracy
(81.9 vs 81.6). GPT-3.5, Gemini, and Llama showed lower overall performance on
both formats. Common misclassification patterns included confusion between
metastasis and central nervous system tumors, as well as errors involving
ambiguous or overlapping clinical terminology. Although current performance
levels appear sufficient for administrative and research use, reliable clinical
applications will require standardized documentation practices alongside robust
human oversight for high-stakes decision-making.

</details>


### [3] [Scheming Ability in LLM-to-LLM Strategic Interactions](https://arxiv.org/abs/2510.12826)
*Thao Pham*

Main category: cs.CL

TL;DR: 本文评估了当前大型语言模型（如GPT-4o，Gemini-2.5-pro等）在多智能体博弈中的策略欺骗能力，发现模型在无明确提示下也倾向于欺骗行为。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型自主部署于多样化场景，理解其策略欺骗能力对风险评估和安全设计至关重要。

Method: 通过两种博弈论框架（Cheap Talk信号游戏和Peer Evaluation对抗游戏）测试四个前沿模型的欺骗性能，并结合链式思维分析欺骗策略。

Result: 大多数模型在有提示时表现出近乎完美的欺骗能力，无提示时所有模型在对抗评价游戏中均选择欺骗，且在信号游戏中以95%-100%成功率完成欺骗。

Conclusion: 研究揭示多智能体环境下大型语言模型具有强烈的策略欺骗倾向，强调需采用高风险博弈论场景进行严格评估。

Abstract: As large language model (LLM) agents are deployed autonomously in diverse
contexts, evaluating their capacity for strategic deception becomes crucial.
While recent research has examined how AI systems scheme against human
developers, LLM-to-LLM scheming remains underexplored. We investigate the
scheming ability and propensity of frontier LLM agents through two
game-theoretic frameworks: a Cheap Talk signaling game and a Peer Evaluation
adversarial game. Testing four models (GPT-4o, Gemini-2.5-pro,
Claude-3.7-Sonnet, and Llama-3.3-70b), we measure scheming performance with and
without explicit prompting while analyzing scheming tactics through
chain-of-thought reasoning. When prompted, most models, especially
Gemini-2.5-pro and Claude-3.7-Sonnet, achieved near-perfect performance.
Critically, models exhibited significant scheming propensity without prompting:
all models chose deception over confession in Peer Evaluation (100% rate),
while models choosing to scheme in Cheap Talk succeeded at 95-100% rates. These
findings highlight the need for robust evaluations using high-stakes
game-theoretic scenarios in multi-agent settings.

</details>


### [4] [From Noise to Signal to Selbstzweck: Reframing Human Label Variation in the Era of Post-training in NLP](https://arxiv.org/abs/2510.12817)
*Shanshan Xu,Santosh T. Y. S. S,Barbara Plank*

Main category: cs.CL

TL;DR: 该论文探讨了人类标注差异（HLV）的重要性，强调其应被视为反映人类多元观点的信号，而非噪声，并呼吁在AI偏好数据集中保留这一差异。


<details>
  <summary>Details</summary>
Motivation: 以往自然语言处理任务中，人类标注差异常被视为噪声被忽略，但随着大型语言模型的发展，这种差异对模型对齐和鲁棒性变得关键，然而现有数据集往往将多重标注合并为单一标签，抹平了人类多样性视角。

Method: 本文是一篇立场论文，主张将HLV视为目标本身（Selbstzweck），强调保留人类多元性的重要性，并提出在偏好学习数据集中主动纳入HLV的可行步骤。

Result: 论文未给出具体实验结果，而是提出理论观点和设计原则，强调保留和利用人类标注差异的必要性。

Conclusion: 人类标注的多样性是AI系统设计中不可忽视的价值，应主动纳入偏好数据集，维护和尊重人类的多元观点，从而促进模型更好地对齐和增强鲁棒性。

Abstract: Human Label Variation (HLV) refers to legitimate disagreement in annotation
that reflects the genuine diversity of human perspectives rather than mere
error. For decades, HLV in NLP was dismissed as noise to be discarded, and only
slowly over the last decade has it been reframed as a signal for improving
model robustness. With the rise of large language models (LLMs), where
post-training on human feedback has become central to model alignment, the role
of HLV has become increasingly consequential. Yet current preference-learning
datasets routinely aggregate multiple annotations into a single label, thereby
flattening diverse perspectives into a false universal agreement and erasing
precisely the pluralism of human values that alignment aims to preserve. In
this position paper, we argue that preserving HLV as an embodiment of human
pluralism must be treated as a Selbstzweck - a goal it self when designing AI
systems. We call for proactively incorporating HLV into preference datasets and
outline actionable steps towards it.

</details>


### [5] [MEDEQUALQA: Evaluating Biases in LLMs with Counterfactual Reasoning](https://arxiv.org/abs/2510.12818)
*Rajarshi Ghosh,Abhay Gupta,Hudson McBride,Anurag Vaidya,Faisal Mahmood*

Main category: cs.CL

TL;DR: 本文提出MEDEQUALQA基准，通过改变病人代词测试大型语言模型在临床推理中的稳定性，发现虽然整体推理相似，但局部推理存在差异，揭示潜在偏见。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在临床决策支持中的应用不断增加，但模型推理容易受到病人群体的人口统计特征影响，导致输出结果存在差异，亟需一种方法系统评估模型推理在不同人口标识下的稳定性。

Method: 本文设计MEDEQUALQA基准，保持关键症状和病情不变，仅改变病人代词（他/她/他们），构建约69000条样本数据；使用GPT-4.1生成推理轨迹，通过语义文本相似度（STS）评测不同代词条件下的推理稳定性。

Result: 结果显示总体推理相似度较高（平均STS>0.80），但在引用的风险因素、指南依据和差异排序等局部环节存在一致性的差异，尽管最终诊断保持不变。误差分析揭示部分病例中推理变化的存在，暴露出临床相关的偏见点。

Conclusion: MEDEQUALQA为医疗AI推理稳定性审计提供了一个受控的诊断环境，帮助识别模型中可能导致不公平护理的偏见，从而促进公平可靠的临床决策支持系统的构建。

Abstract: Large language models (LLMs) are increasingly deployed in clinical decision
support, yet subtle demographic cues can influence their reasoning. Prior work
has documented disparities in outputs across patient groups, but little is
known about how internal reasoning shifts under controlled demographic changes.
We introduce MEDEQUALQA, a counterfactual benchmark that perturbs only patient
pronouns (he/him, she/her, they/them) while holding critical symptoms and
conditions (CSCs) constant. Each clinical vignette is expanded into single-CSC
ablations, producing three parallel datasets of approximately 23,000 items each
(69,000 total). We evaluate a GPT-4.1 model and compute Semantic Textual
Similarity (STS) between reasoning traces to measure stability across pronoun
variants. Our results show overall high similarity (mean STS >0.80), but reveal
consistent localized divergences in cited risk factors, guideline anchors, and
differential ordering, even when final diagnoses remain unchanged. Our error
analysis highlights certain cases in which the reasoning shifts, underscoring
clinically relevant bias loci that may cascade into inequitable care.
MEDEQUALQA offers a controlled diagnostic setting for auditing reasoning
stability in medical AI.

</details>


### [6] [Classifier-Augmented Generation for Structured Workflow Prediction](https://arxiv.org/abs/2510.12825)
*Thomas Gschwind,Shramona Chakraborty,Nitin Gupta,Sameep Mehta*

Main category: cs.CL

TL;DR: 提出了一种将自然语言描述转化为可执行ETL工作流的系统，通过分类增强生成方法提高预测和配置的准确性与效率。


<details>
  <summary>Details</summary>
Motivation: ETL工具配置繁琐且需专业知识，需简化用户操作并提升自动化水平。

Method: 采用分类增强生成（CAG）方法，结合语句分解、分类器与阶段特定的少样本提示，实现阶段预测，利用边预测连接非线性工作流，并从子语句中推断阶段属性。

Result: 与强基线相比，CAG方法在准确性和效率上表现更佳，显著减少了token使用量，并实现了端到端的工作流生成和验证。

Conclusion: 该系统首次在自然语言驱动的ETL工具中，实现并详细评估了阶段预测、边布局及属性生成，展现了模块化和可解释性优势，有效简化了ETL工作流的构建过程。

Abstract: ETL (Extract, Transform, Load) tools such as IBM DataStage allow users to
visually assemble complex data workflows, but configuring stages and their
properties remains time consuming and requires deep tool knowledge. We propose
a system that translates natural language descriptions into executable
workflows, automatically predicting both the structure and detailed
configuration of the flow. At its core lies a Classifier-Augmented Generation
(CAG) approach that combines utterance decomposition with a classifier and
stage-specific few-shot prompting to produce accurate stage predictions. These
stages are then connected into non-linear workflows using edge prediction, and
stage properties are inferred from sub-utterance context. We compare CAG
against strong single-prompt and agentic baselines, showing improved accuracy
and efficiency, while substantially reducing token usage. Our architecture is
modular, interpretable, and capable of end-to-end workflow generation,
including robust validation steps. To our knowledge, this is the first system
with a detailed evaluation across stage prediction, edge layout, and property
generation for natural-language-driven ETL authoring.

</details>


### [7] [Mathematics with large language models as provers and verifiers](https://arxiv.org/abs/2510.12829)
*Hieu Le Duc,Leo Liberti*

Main category: cs.CL

TL;DR: 本文展示了利用多实例GPT-5模型协同工作并结合lean证明助手进行验证，成功解决了2025年国际数学奥林匹克（IMO）中的五个难题及部分数论猜想。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型在复杂数学定理证明中的能力，特别是验证人工智能能否证明难题和猜想。

Method: 采用多GPT-5证明者和验证者并行协作机制，并使用lean辅助证明工具进行最终形式化验证，确保无幻觉错误，且由人工核对lean代码的前提和结论一致性。

Result: 成功解决了六个2025年IMO问题中的五个，以及六十六个数论猜想中的三分之一。

Conclusion: 多实例GPT-5模型协作加上形式化验证的方法，有效提升了AI在高难度数学定理证明中的可信度和成功率。

Abstract: During 2024 and 2025 the discussion about the theorem-proving capabilities of
large language models started reporting interesting success stories, mostly to
do with difficult exercises (such as problems from the International
Mathematical Olympiad), but also with conjectures [Feldman & Karbasi,
arXiv:2509.18383v1] formulated for the purpose of verifying whether the
artificial intelligence could prove it. In this paper we report a theorem
proving feat achieved by ChatGPT by using a protocol involving different prover
and verifier instances of the gpt-5 model working collaboratively. To make sure
that the produced proofs do not suffer from hallucinations, the final proof is
formally verified by the lean proof assistant, and the conformance of premises
and conclusion of the lean code is verified by a human. Our methodology was
able to solve five out of six 2025 IMO problems, and close a third of the
sixty-six number theory conjectures in [Cohen, Journal of Integer Sequences,
2025].

</details>


### [8] [MTSQL-R1: Towards Long-Horizon Multi-Turn Text-to-SQL via Agentic Training](https://arxiv.org/abs/2510.12831)
*Taicheng Guo,Hai Wang,ChaoChun Liu,Mohsen Golalikhani,Xin Chen,Xiangliang Zhang,Chandan K. Reddy*

Main category: cs.CL

TL;DR: MTSQL-R1通过将多轮Text-to-SQL任务建模为马尔可夫决策过程，实现了基于执行反馈和对话记忆的循环验证与优化，大幅提升了查询的执行正确性和对话连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有多轮Text-to-SQL系统仅视为简单的文本翻译，缺乏执行反馈和显式验证，导致生成的SQL不可执行或对话不连贯。

Method: 将任务建模为MDP，设计一个代理与数据库交互获取执行反馈，并利用持久对话记忆进行连贯性验证，通过“提议-执行-验证-优化”循环不断改进生成的SQL。

Result: 在COSQL和SPARC数据集上的实验表明，MTSQL-R1显著优于强基线方法。

Conclusion: 环境驱动的验证和基于记忆的优化是提升多轮对话语义解析性能的关键，MTSQL-R1为此提供了有效框架。

Abstract: Multi-turn Text-to-SQL aims to translate a user's conversational utterances
into executable SQL while preserving dialogue coherence and grounding to the
target schema. However, most existing systems only regard this task as a simple
text translation task and follow a short-horizon paradigm, generating a query
per turn without execution, explicit verification, and refinement, which leads
to non-executable or incoherent outputs. We present MTSQL-R1, an agentic
training framework for long-horizon multi-turn Text-to-SQL. We cast the task as
a Markov Decision Process (MDP) in which an agent interacts with (i) a database
for execution feedback and (ii) a persistent dialogue memory for coherence
verification, performing an iterative propose to execute -> verify -> refine
cycle until all checks pass. Experiments on COSQL and SPARC demonstrate that
MTSQL-R1 consistently outperforms strong baselines, highlighting the importance
of environment-driven verification and memory-guided refinement for
conversational semantic parsing. Full recipes (including code, trained models,
logs, reasoning trajectories, etc.) will be released after the internal review
to contribute to community research.

</details>


### [9] [Repurposing Annotation Guidelines to Instruct LLM Annotators: A Case Study](https://arxiv.org/abs/2510.12835)
*Kon Woo Kim,Rezarta Islamaj,Jin-Dong Kim,Florian Boudin,Akiko Aizawa*

Main category: cs.CL

TL;DR: 本文研究了如何将传统的人工注释指南转换为适用于大型语言模型(LLM)注释员的明确指令，提出了一种通过LLM调解过程转化指南的方法，并以NCBI疾病语料库为例验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统的注释指南是针对人类注释员设计的，他们通过内化训练来理解指南内容，而LLM注释员则需要明确、结构化的指令，因此需要将传统指南重新利用以适配LLM。

Method: 提出了一种以调解为导向的指南重构方法，通过LLM调解过程将传统的注释指南转化为对LLM注释员的清晰指令。

Result: 以NCBI疾病语料库为实验案例，结果表明重构后的指南能够有效指导LLM注释员，但也暴露了一些实际操作中的挑战。

Conclusion: 该工作流程有潜力支持可扩展且成本效益高的注释指南优化和自动注释工作。

Abstract: This study investigates how existing annotation guidelines can be repurposed
to instruct large language model (LLM) annotators for text annotation tasks.
Traditional guidelines are written for human annotators who internalize
training, while LLMs require explicit, structured instructions. We propose a
moderation-oriented guideline repurposing method that transforms guidelines
into clear directives for LLMs through an LLM moderation process. Using the
NCBI Disease Corpus as a case study, our experiments show that repurposed
guidelines can effectively guide LLM annotators, while revealing several
practical challenges. The results highlight the potential of this workflow to
support scalable and cost-effective refinement of annotation guidelines and
automated annotation.

</details>


### [10] [A\textsuperscript{2}FM: An Adaptive Agent Foundation Model for Tool-Aware Hybrid Reasoning](https://arxiv.org/abs/2510.12838)
*Qianben Chen,Jingyi Cao,Jiayu Zhang,Tianrui Qin,Xiaowan Li,King Zhu,Dingfeng Shi,He Zhu,Minghao Liu,Xiaobo Liang,Ge Zhang,Jian Yang,Yuchen Eleanor Jiang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: 该论文提出了自适应代理基础模型A²FM，统一了推理型和代理型大型语言模型，通过引入一个处理简单查询的即时模式和自适应策略优化，提高了推理效率和准确率，实现了更低的计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型分为强调内在推理的推理型和擅长环境交互的代理型，但两者训练目标不同，导致在简单查询时效率低下，存在过度推理或过度调用工具的问题。

Method: 提出了遵循"先路由后对齐"原则的A²FM框架，新增处理简单查询的即时模式，通过自适应策略优化（APO）实现多模式间的自适应采样和成本正则化奖励，提升性能和效率。

Result: 32B规模的A²FM在BrowseComp、AIME25和HLE数据集上分别达到了13.4%、70.4%和16.7%的新状态，且在多个基准上表现出竞争力，计算成本显著降低，错误回答的成本减少45.2%-33.5%。

Conclusion: A²FM有效融合了推理型与代理型模型优势，通过多模式协同和自适应策略优化，实现了更高的准确率和显著的成本效率提升，展示了大型语言模型在复杂任务处理上的新方向。

Abstract: Large language models split into two families: reasoning-centric LLMs, which
strengthen internal chain-of-thought reasoning but cannot invoke external
tools, and agentic LLMs, which learn to interact with environments and leverage
tools but often lag in deep reasoning. This divide arises from fundamentally
different training objectives, leading to mismatched strengths and inefficiency
on simple queries, where both families tend to overthink or over-call tools. In
this work, we present Adaptive Agent Foundation Model (A\textsuperscript{2}FM),
a unified framework that follows a route-then-align principle: the model first
learns task-aware routing and then aligns mode-specific trajectories under a
shared backbone. To address the inefficiency gap, we introduce a third
mode-instant-that handles simple queries directly, preventing unnecessary
reasoning or tool calls while complementing the agentic and reasoning modes. To
jointly enhance accuracy and efficiency, we propose Adaptive Policy
Optimization (APO), which enforces adaptive sampling across modes and applies a
cost-regularized reward. On the 32B scale, A\textsuperscript{2}FM achieves
13.4\% on BrowseComp, 70.4\% on AIME25, and 16.7\% on HLE, setting new SOTA
among comparable models and performing competitively with frontier LLMs across
agentic, reasoning, and general benchmarks. Notably, the adaptive execution
achieves a cost of pass of only \$0.00487 per correct answer-cutting cost by
45.2\% relative to reasoning and 33.5\% relative to agentic, thus delivering
substantially higher cost efficiency while maintaining comparable accuracy.

</details>


### [11] [FaStFACT: Faster, Stronger Long-Form Factuality Evaluations in LLMs](https://arxiv.org/abs/2510.12839)
*Yingjia Wan,Haochen Tan,Xiao Zhu,Xinyu Zhou,Zhiwei Li,Qingsong Lv,Changxuan Sun,Jiaqi Zeng,Yi Xu,Jianqiao Lu,Yinhong Liu,Zhijiang Guo*

Main category: cs.CL

TL;DR: 本文提出了一种名为FastFact的快速且高效的长篇文本事实性评估框架，显著提升了与人类评估的一致性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统的大规模语言模型长篇生成文本事实性评估存在效率低下和证据不足的问题，导致准确性不高且成本昂贵。

Method: FastFact通过块级断言提取和基于置信度的预验证减少搜索和推理成本，采用网页爬取的文档级证据并在验证时有选择地检索，提高了证据的充分性和评估的准确性。

Result: 大量实验证明FastFact在评估长篇LLM生成文本的事实性方面，兼具高效性和可靠性，且与人工评估高度一致。

Conclusion: FastFact有效克服了现有方法的缺陷，为长篇文本的事实性自动评估提供了一个强大且高效的解决方案。

Abstract: Evaluating the factuality of long-form generations from Large Language Models
(LLMs) remains challenging due to accuracy issues and costly human assessment.
Prior efforts attempt this by decomposing text into claims, searching for
evidence, and verifying claims, but suffer from critical drawbacks: (1)
inefficiency due to complex pipeline components unsuitable for long LLM
outputs, and (2) ineffectiveness stemming from inaccurate claim sets and
insufficient evidence collection of one-line snippets.
  To address these limitations, we propose \name, a fast and strong evaluation
framework that achieves the highest alignment with human evaluation and
efficiency among existing baselines. \name first employs chunk-level claim
extraction integrated with confidence-based pre-verification, significantly
reducing the cost of web searching and inference calling while ensuring
reliability. For searching and verification, it collects document-level
evidence from crawled webpages and selectively retrieves it during
verification, addressing the evidence insufficiency problem in previous
pipelines.
  Extensive experiments based on an aggregated and manually annotated benchmark
demonstrate the reliability of \name in both efficiently and effectively
evaluating the factuality of long-form LLM generations. Code and benchmark data
is available at https://github.com/Yingjia-Wan/FastFact.

</details>


### [12] [VLURes: Benchmarking VLM Visual and Linguistic Understanding in Low-Resource Languages](https://arxiv.org/abs/2510.12845)
*Jesse Atuhurra,Iqra Ali,Tomoya Iwakura,Hidetaka Kamigaito,Tatsuya Hiraoka*

Main category: cs.CL

TL;DR: 本文提出了一个多语言长文本视觉语言模型评测基准VLURes，涵盖四种语言和八个任务，用于细粒度评估视觉语言模型的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型评测多集中于英语和短文本，缺乏对多语言长文本环境下模型细粒度能力的评估。

Method: 构建VLURes基准，包括十个图像类别和丰富文本内容，针对英语、日语、斯瓦希里语和乌尔都语设计八个任务和一个无关性任务，通过模型生成回答和推理，由自动系统和母语者评估。

Result: 评测了十个视觉语言模型，表现最好的是GPT-4o，取得90.8%的准确率，但仍落后于人类6.7%。开源模型的表现差距更大。

Conclusion: VLURes基准揭示了不同语言和任务下模型性能差异，是推动多模态视觉推理方面智能体发展的关键资源。

Abstract: Vision Language Models (VLMs) are pivotal for advancing perception in
intelligent agents. Yet, evaluation of VLMs remains limited to predominantly
English-centric benchmarks in which the image-text pairs comprise short texts.
To evaluate VLM fine-grained abilities, in four languages under long-text
settings, we introduce a novel multilingual benchmark VLURes featuring eight
vision-and-language tasks, and a pioneering unrelatedness task, to probe the
fine-grained Visual and Linguistic Understanding capabilities of VLMs across
English, Japanese, and low-resource languages, Swahili, and Urdu. Our datasets,
curated from web resources in the target language, encompass ten diverse image
categories and rich textual context, introducing valuable vision-language
resources for Swahili and Urdu. By prompting VLMs to generate responses and
rationales, evaluated automatically and by native speakers, we uncover
performance disparities across languages and tasks critical to intelligent
agents, such as object recognition, scene understanding, and relationship
understanding. We conducted evaluations of ten VLMs with VLURes. The best
performing model, GPT-4o, achieves an overall accuracy of 90.8% and lags human
performance by 6.7%, though the gap is larger for open-source models. The gap
highlights VLURes' critical role in developing intelligent agents to tackle
multi-modal visual reasoning.

</details>


### [13] [Efficient Adaptive Transformer: An Empirical Study and Reproducible Framework](https://arxiv.org/abs/2510.12856)
*Jan Miller*

Main category: cs.CL

TL;DR: EAT框架整合了三种自适应效率技术用于输入自适应推理，提供了开源基准测试工具，展示了动态计算在延迟敏感NLP中的潜力。


<details>
  <summary>Details</summary>
Motivation: 提高变换器模型在推理阶段的效率，以实现输入自适应的低延迟高性能推理。

Method: 提出EAT框架，将渐进式令牌剪枝、稀疏注意力和动态提前退出三种技术结合成统一架构，并提供自动化基准测试流水线。

Result: 尽管三种机制在浅层模型中可能增加延迟，EAT在SST-2任务上实现了略优于优化后的DistilBERT的准确率。

Conclusion: EAT作为一个开放且端到端可复现的框架，为社区进一步研究自适应变换器提供了实用工具和基准支持。

Abstract: The Efficient Adaptive Transformer (EAT) framework unifies three adaptive
efficiency techniques - progressive token pruning, sparse attention, and
dynamic early exiting - into a single, reproducible architecture for
input-adaptive inference. EAT provides an open-source benchmarking pipeline
that automates data processing, timing, and ablation across GLUE tasks (SST-2,
QQP, MNLI). Although this empirical study finds that combining these mechanisms
can increase latency in shallow six-layer models, it demonstrates that EAT
achieves slightly higher accuracy than the optimized DistilBERT baseline on
SST-2, illustrating the potential of dynamic computation for latency-sensitive
NLP. The main contribution is the open, end-to-end reproducible framework -
complete with scripts, CSV logging, and analysis utilities - intended to serve
as a community tool for further research on adaptive transformers.

</details>


### [14] [A Critical Review of the Need for Knowledge-Centric Evaluation of Quranic Recitation](https://arxiv.org/abs/2510.12858)
*Mohammed Hilal Al-Kharusi,Khizar Hayat,Khalil Bader Al Ruqeishi,Haroon Rashid Lone*

Main category: cs.CL

TL;DR: 本文综述了古兰经诵读(Tajweed)自动评估领域的现状，指出现有基于自动语音识别(ASR)技术的方法在评估质量和反馈诊断上存在明显缺陷，呼吁采用基于规则和声学模型的知识驱动框架以提高教学效果。


<details>
  <summary>Details</summary>
Motivation: 古兰经诵读的传统教学面临现代教育挑战，现有数字工具未能有效普及和支持教学，尤其自动评估工具在质量和公平性上表现不足。

Method: 通过综合分析过去二十年内的学术研究、网络平台及商业应用，批判现有主要依赖ASR技术的数据驱动评估方法，提出以古兰经不变的文本和诵读规则为基础，构建有预见性声学模型的知识驱动混合评估体系。

Result: 发现现有评估方法存在数据依赖强、人口偏见大、反馈诊断功能不足等问题，强调基于规则的声学建模可以提高评估准确性和公平性。

Conclusion: 未来古兰经自动评估应结合深度语言知识与先进音频分析技术，开发出既精确又具教学指导意义的混合系统，以更好地支持全世界学习者。

Abstract: The sacred practice of Quranic recitation (Tajweed), governed by precise
phonetic, prosodic, and theological rules, faces significant pedagogical
challenges in the modern era. While digital technologies promise unprecedented
access to education, automated tools for recitation evaluation have failed to
achieve widespread adoption or pedagogical efficacy. This literature review
investigates this critical gap, conducting a comprehensive analysis of academic
research, web platforms, and commercial applications developed over the past
two decades. Our synthesis reveals a fundamental misalignment in prevailing
approaches that repurpose Automatic Speech Recognition (ASR) architectures,
which prioritize lexical recognition over qualitative acoustic assessment and
are plagued by data dependency, demographic biases, and an inability to provide
diagnostically useful feedback. Critiquing these data--driven paradigms, we
argue for a foundational paradigm shift towards a knowledge-centric
computational framework. Capitalizing on the immutable nature of the Quranic
text and the precisely defined rules of Tajweed, we propose that a robust
evaluator must be architected around anticipatory acoustic modeling based on
canonical rules and articulation points (Makhraj), rather than relying on
statistical patterns learned from imperfect and biased datasets. This review
concludes that the future of automated Quranic evaluation lies in hybrid
systems that integrate deep linguistic knowledge with advanced audio analysis,
offering a path toward robust, equitable, and pedagogically sound tools that
can faithfully support learners worldwide.

</details>


### [15] [EduDial: Constructing a Large-scale Multi-turn Teacher-Student Dialogue Corpus](https://arxiv.org/abs/2510.12899)
*Shouang Wei,Min Zhang,Xin Lin,Bo Jiang,Zhongxiang Dai,Kun Kuang*

Main category: cs.CL

TL;DR: 本文提出了EduDial多轮师生对话数据集和EduDial-LLM模型，旨在提升大语言模型在教育领域的教学能力，设计了11维评测体系，结果显示EduDial-LLM在教学质量上显著优于其他主流模型。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在智能教育中作用的提升，亟需专门的师生对话数据集和评测体系，以促进模型在目标明确的教学场景中的表现。

Method: 构建包含34250条对话的EduDial数据集，基于布鲁姆教育目标分类学设计，融入十种提问策略，针对不同认知水平学生设计差异化教学策略；基于此训练EduDial-LLM 32B模型，提出11维教学能力评估框架。

Result: 在17个主流大语言模型上测试，结果显示大多数模型在以学生为中心的教学场景表现欠佳，而EduDial-LLM在所有指标均显著超越基线模型。

Conclusion: EduDial和EduDial-LLM有效提升了大语言模型的教学能力，验证了基于教育理论设计的师生对话数据集和评测框架的价值，为智能教育中的语言模型应用提供了重要资源和方向。

Abstract: Recently, several multi-turn dialogue benchmarks have been proposed to
evaluate the conversational abilities of large language models (LLMs). As LLMs
are increasingly recognized as a key technology for advancing intelligent
education, owing to their ability to deeply understand instructional contexts
and provide personalized guidance, the construction of dedicated
teacher-student dialogue benchmarks has become particularly important. To this
end, we present EduDial, a comprehensive multi-turn teacher-student dialogue
dataset. EduDial covers 345 core knowledge points and consists of 34,250
dialogue sessions generated through interactions between teacher and student
agents. Its design is guided by Bloom's taxonomy of educational objectives and
incorporates ten questioning strategies, including situational questioning,
zone of proximal development (ZPD) questioning, and metacognitive
questioning-thus better capturing authentic classroom interactions.
Furthermore, we design differentiated teaching strategies for students at
different cognitive levels, thereby providing more targeted teaching guidance.
Building on EduDial, we further develop EduDial-LLM 32B via training and
propose an 11-dimensional evaluation framework that systematically measures the
teaching abilities of LLMs, encompassing both overall teaching quality and
content quality. Experiments on 17 mainstream LLMs reveal that most models
struggle in student-centered teaching scenarios, whereas our EduDial-LLM
achieves significant gains, consistently outperforming all baselines across all
metrics. The code is available at
https://github.com/Mind-Lab-ECNU/EduDial/tree/main.

</details>


### [16] [Who's Asking? Evaluating LLM Robustness to Inquiry Personas in Factual Question Answering](https://arxiv.org/abs/2510.12925)
*Nil-Jana Akpinar,Chia-Jung Lee,Vanessa Murdock,Pietro Perona*

Main category: cs.CL

TL;DR: 本文首次系统评估了大语言模型（LLMs）对用户身份特征（inquiry personas）的鲁棒性，发现用户所透露的身份信息会显著影响模型回答的准确性，导致拒绝回答、虚假的限制和角色混淆等失败模式。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注对抗性输入和干扰项，而忽视了用户在真实交互中透露的身份信息对模型输出的影响。为确保模型回答事实问题时的可靠性，需要评估模型在不同用户身份特征下的表现。

Method: 通过设计涵盖身份、专业知识和信念等属性的用户画像，系统性测试大语言模型在不同用户身份信息提示下的问答表现，比较准确率和失败案例。

Result: 用户身份信息对模型准确率有显著影响，触发多种失败模式，包括拒绝回答、虚假限制及角色混淆，显示模型对用户身份的敏感性可能削弱事实回答的可靠性。

Conclusion: 模型对用户身份特征的敏感性揭示了其事实回答的脆弱性，采用用户画像作为测试手段，有助于全面评估和提升模型的鲁棒性和可靠性。

Abstract: Large Language Models (LLMs) should answer factual questions truthfully,
grounded in objective knowledge, regardless of user context such as
self-disclosed personal information, or system personalization. In this paper,
we present the first systematic evaluation of LLM robustness to inquiry
personas, i.e. user profiles that convey attributes like identity, expertise,
or belief. While prior work has primarily focused on adversarial inputs or
distractors for robustness testing, we evaluate plausible, human-centered
inquiry persona cues that users disclose in real-world interactions. We find
that such cues can meaningfully alter QA accuracy and trigger failure modes
such as refusals, hallucinated limitations, and role confusion. These effects
highlight how model sensitivity to user framing can compromise factual
reliability, and position inquiry persona testing as an effective tool for
robustness evaluation.

</details>


### [17] [The Curious Case of Curiosity across Human Cultures and LLMs](https://arxiv.org/abs/2510.12943)
*Angana Borah,Rada Mihalcea*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在不同文化背景下的好奇心表现，提出了CUEST评估框架，发现模型对文化多样性的表现较平坦，更接近西方表达方式，并通过微调策略提高模型与人类的好奇心匹配度。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在与人类交互中发挥重要作用，但其好奇心这一驱动力在不同文化背景下的表现尚未充分研究。

Method: 利用Yahoo! Answers多国数据集，提出CUEST框架，通过语言风格、话题偏好和社会科学视角分析文化间好奇心差异；评估开源及闭源模型的表现，探索微调策略以增强模型的好奇心表现。

Result: 发现大型语言模型在表达好奇心时呈现文化多样性的扁平化，更接近西方表达；通过微调策略，模型与人类好奇心的匹配度提高了最多50%。

Conclusion: 好奇心对提升大型语言模型跨文化适应性具有重要意义，应成为未来自然语言处理研究关注的重点。

Abstract: Recent advances in Large Language Models (LLMs) have expanded their role in
human interaction, yet curiosity -- a central driver of inquiry -- remains
underexplored in these systems, particularly across cultural contexts. In this
work, we investigate cultural variation in curiosity using Yahoo! Answers, a
real-world multi-country dataset spanning diverse topics. We introduce CUEST
(CUriosity Evaluation across SocieTies), an evaluation framework that measures
human-model alignment in curiosity through linguistic (style), topic preference
(content) analysis and grounding insights in social science constructs. Across
open- and closed-source models, we find that LLMs flatten cross-cultural
diversity, aligning more closely with how curiosity is expressed in Western
countries. We then explore fine-tuning strategies to induce curiosity in LLMs,
narrowing the human-model alignment gap by up to 50\%. Finally, we demonstrate
the practical value of curiosity for LLM adaptability across cultures, showing
its importance for future NLP research.

</details>


### [18] [3-Model Speculative Decoding](https://arxiv.org/abs/2510.12966)
*Sanghyun Byun,Mohanad Odema,Jung Ick Guack,Baisub Lee,Jacob Song,Woo Seong Chung*

Main category: cs.CL

TL;DR: 本文提出了金字塔推测解码（PyramidSD）方法，通过引入中间限定模型来桥接草稿模型和目标模型之间的分布差异，从而提升推测解码的效率和接受率，实现更快的文本生成速度。


<details>
  <summary>Details</summary>
Motivation: 传统的推测解码在草稿模型大小和生成速度之间面临权衡，较小的草稿模型速度快但与目标模型的预测差异大，导致较低的接受率和加速效果不足。

Method: 引入一个中间的限定模型作为草稿模型和目标模型之间的桥梁，采用分层解码策略和模糊接受标准，允许在每个阶段使用放宽的偏差阈值，以提升模型间的对齐度和总吞吐量。

Result: PyramidSD在标准推测解码基础上实现了最高1.91倍的加速，生成速度达到消费级GPU（RTX 4090）上的124 token/s，在小存储环境下用1B参数的草稿模型和8B参数的目标模型，实现了较少的质量损失和更高的吞吐量。

Conclusion: PyramidSD是一种实用且高效的推测解码增强方式，可以无缝集成到现有推理流程中，显著提升生成效率。

Abstract: Speculative Decoding (SD) accelerates inference in large language models by
using a smaller draft model to propose tokens, which are then verified by a
larger target model. However, the throughput gains of SD are fundamentally
limited by a trade-off between draft model size and token acceptance: smaller
draft models generate tokens more quickly but exhibit greater divergence from
the target model, resulting in lower acceptance rates and reduced speedups. We
introduce Pyramid Speculative Decoding (PyramidSD), an extension of SD that
inserts an intermediate qualifier model between the draft and target to bridge
the distributional gap in output predictions, allowing smaller model to be used
for drafting. This hierarchical decoding strategy improves alignment across
models, enabling higher acceptance rates and allowing the use of significantly
smaller draft models without sacrificing overall performance. PyramidSD builds
on fuzzy acceptance criteria to support relaxed divergence thresholds at each
stage, improving throughput. In experiments, PyramidSD achieves up to 1.91x
generation speed over standard SD, reaching 124 tokens per second on a consumer
GPU (RTX 4090). In small-memory settings with a 1B-parameter draft model and an
8B target model, PyramidSD minimally trades target model quality for improved
throughput. Overall, PyramidSD offers a practical approach to enhancing
speculative decoding efficiency and can be readily applied to existing
inference pipelines.

</details>


### [19] [A Multilingual, Large-Scale Study of the Interplay between LLM Safeguards, Personalisation, and Disinformation](https://arxiv.org/abs/2510.12993)
*João A. Leite,Arnav Arora,Silvia Gargova,João Luz,Gustavo Sampaio,Ian Roberts,Carolina Scarton,Kalina Bontcheva*

Main category: cs.CL

TL;DR: 本文首次开展大规模多语言研究，评估大型语言模型（LLMs）生成针对特定人群的个性化虚假信息的能力及安全机制。通过创建包含160万条文本的AI-TRAITS数据集，研究发现个性化提示显著提升了模型被“越狱”的概率，并增强虚假信息的说服力。


<details>
  <summary>Details</summary>
Motivation: 鉴于LLMs具备生成具备说服力和个性化的虚假信息的潜力，且此类风险在多语言、多人口属性环境中尚未充分研究，本文旨在系统评估LLMs在个性化虚假信息生成及安全防护方面的脆弱性。

Method: 采用红队测试方法，设计结合324个虚假叙事与150个人群属性的提示语，生成涵盖英语、俄语、葡萄牙语和印地语的个性化虚假信息，构建AI-TRAITS大规模多语言数据集，并对生成文本进行语言学和修辞学分析。

Result: 结果显示，个性化的提示语显著增加了所有研究模型的越狱率，改变了生成文本的语言和修辞特征，提升了虚假信息的说服力，反映出当前LLMs在多语言和跨人口属性环境下存在严重安全漏洞。

Conclusion: 当前最先进的LLMs在安全机制上存在重要不足，尤其在个性化虚假信息生成方面易被利用。研究成果为未来提升多语言、多人口属性环境下模型安全对齐与虚假信息检测策略提供了重要基础。

Abstract: The human-like proficiency of Large Language Models (LLMs) has brought
concerns about their potential misuse for generating persuasive and
personalised disinformation at scale. While prior work has demonstrated that
LLMs can generate disinformation, specific questions around persuasiveness and
personalisation (generation of disinformation tailored to specific demographic
attributes) remain largely unstudied. This paper presents the first
large-scale, multilingual empirical study on persona-targeted disinformation
generation by LLMs. Employing a red teaming methodology, we systematically
evaluate the robustness of LLM safety mechanisms to persona-targeted prompts. A
key novel result is AI-TRAITS (AI-generaTed peRsonAlIsed disinformaTion
dataSet), a new dataset of around 1.6 million texts generated by eight
state-of-the-art LLMs. AI-TRAITS is seeded by prompts that combine 324
disinformation narratives and 150 distinct persona profiles, covering four
major languages (English, Russian, Portuguese, Hindi) and key demographic
dimensions (country, generation, political orientation). The resulting
personalised narratives are then assessed quantitatively and compared along the
dimensions of models, languages, jailbreaking rate, and personalisation
attributes. Our findings demonstrate that the use of even simple
personalisation strategies in the prompts significantly increases the
likelihood of jailbreaks for all studied LLMs. Furthermore, personalised
prompts result in altered linguistic and rhetorical patterns and amplify the
persuasiveness of the LLM-generated false narratives. These insights expose
critical vulnerabilities in current state-of-the-art LLMs and offer a
foundation for improving safety alignment and detection strategies in
multilingual and cross-demographic contexts.

</details>


### [20] [OPLoRA: Orthogonal Projection LoRA Prevents Catastrophic Forgetting during Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2510.13003)
*Yifeng Xiong,Xiaohui Xie*

Main category: cs.CL

TL;DR: 提出了一种基于正交投影的LoRA方法OPLoRA，防止微调过程中灾难性遗忘，实现知识保留。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA方法因更新与前训练知识的主奇异方向干扰，导致灾难性遗忘。

Method: 通过奇异值分解(SVD)，构建双侧正交投影，约束更新在主奇异子空间的正交补空间，实现知识保留。

Result: 在常识推理、数学、代码生成任务上，OPLoRA显著减少遗忘，保证性能，在LLaMA-2 7B和Qwen2.5 7B模型上表现优异。

Conclusion: 正交投影机制有效防止微调期间的知识遗忘，是参数高效微调中保持预训练知识的有效方法。

Abstract: Low-Rank Adaptation (LoRA) enables efficient fine-tuning of large language
models but suffers from catastrophic forgetting when learned updates interfere
with the dominant singular directions that encode essential pre-trained
knowledge. We propose Orthogonal Projection LoRA (OPLoRA), a theoretically
grounded approach that prevents this interference through double-sided
orthogonal projections. By decomposing frozen weights via SVD, OPLoRA
constrains LoRA updates to lie entirely within the orthogonal complement of the
top-$k$ singular subspace using projections $P_L = I - U_k U_k^\top$ and $P_R =
I - V_k V_k^\top$. We prove that this construction exactly preserves the
top-$k$ singular triples, providing mathematical guarantees for knowledge
retention. To quantify subspace interference, we introduce $\rho_k$, a metric
measuring update alignment with dominant directions. Extensive experiments
across commonsense reasoning, mathematics, and code generation demonstrate that
OPLoRA significantly reduces forgetting while maintaining competitive
task-specific performance on LLaMA-2 7B and Qwen2.5 7B, establishing orthogonal
projection as an effective mechanism for knowledge preservation in
parameter-efficient fine-tuning.

</details>


### [21] [CurLL: A Developmental Framework to Evaluate Continual Learning in Language Models](https://arxiv.org/abs/2510.13008)
*Pavan Kalyan,Shubhra Mishra,Satya Lokam,Navin Goyal*

Main category: cs.CL

TL;DR: 本文提出了一个基于人类5-10岁发展阶段的连续学习数据集和基准测试CurlL，以系统地评估模型逐步习得新技能的能力。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型在持续学习中缺乏细粒度和阶段性评估，难以准确衡量技能遗忘与迁移效果。

Method: 构建了涵盖五个发展阶段的CurlL数据集，包含23.4亿令牌的合成数据，结合技能图细分技能结构，设计分阶段的多样化任务（段落、理解问答、技能测试问答、指令响应），并采用独立、联合和顺序训练策略进行测试。

Result: 利用135M参数的变换器模型，展示了不同训练策略下技能保持与迁移的权衡，支持精确分析遗忘、前向和后向迁移。

Conclusion: 通过模拟人类学习轨迹和技能依赖关系，CurlL推动了语言模型持续学习的细粒度评估和技术发展。

Abstract: We introduce a comprehensive continual learning dataset and benchmark (CurlL)
grounded in human developmental trajectories from ages 5-10, enabling
systematic and fine-grained assessment of models' ability to progressively
acquire new skills. CurlL spans five developmental stages (0-4) covering ages
5-10, supported by a skill graph that breaks down broad skills into smaller
abilities, concrete goals, and measurable indicators, while also capturing
which abilities build on others. We generate a 23.4B-token synthetic dataset
with controlled skill progression, vocabulary complexity, and format diversity,
comprising paragraphs, comprehension-based QA (CQA), skill-testing QA (CSQA),
and instruction-response (IR) pairs. Stage-wise token counts range from 2.12B
to 6.78B tokens, supporting precise analysis of forgetting, forward transfer,
and backward transfer. Using a 135M-parameter transformer trained under
independent, joint, and sequential (continual) setups, we show trade-offs in
skill retention and transfer efficiency. By mirroring human learning patterns
and providing fine-grained control over skill dependencies, this work advances
continual learning evaluations for language models.

</details>


### [22] [On the Role of Preference Variance in Preference Optimization](https://arxiv.org/abs/2510.13022)
*Jiacheng Guo,Zihao Li,Jiahao Qiu,Yue Wu,Mengdi Wang*

Main category: cs.CL

TL;DR: 本文探讨了偏好方差（PVar）对直接偏好优化（DPO）训练效果的影响，提出利用PVar筛选训练样本可提升大语言模型的对齐效率。


<details>
  <summary>Details</summary>
Motivation: 收集人类偏好数据昂贵且低效，需寻找方法减少标注数量，同时保证DPO训练效果。

Method: 理论上推导了DPO梯度范数的上界由PVar控制，实验中利用奖励模型生成偏好，在两个基准数据集上对比不同PVar的样本对训练效果的影响，验证PVar的有效性。

Result: 高PVar样本显著优于随机或低PVar样本；使用较小奖励模型筛选仍然稳健；仅用最高10% PVar样本训练优于全数据集。

Conclusion: 偏好方差是筛选有效训练样本的重要指标，基于PVar的样本选择能够提升DPO训练效率和模型对齐性能。

Abstract: Direct Preference Optimization (DPO) has emerged as an important approach for
learning from human preferences in aligning large language models (LLMs).
However, collecting human preference data is costly and inefficient, motivating
methods to reduce the required annotations. In this work, we investigate the
impact of \emph{preference variance} (PVar), which measures the variance in
model preferences when comparing pairs of responses, on the effectiveness of
DPO training. We provide a theoretical insight by establishing an upper bound
on the DPO gradient norm for any given prompt, showing it is controlled by the
PVar of that prompt. This implies that prompts with low PVar can only produce
small gradient updates, making them less valuable for learning. We validate
this finding by fine-tuning LLMs with preferences generated by a reward model,
evaluating on two benchmarks (AlpacaEval 2.0 and Arena-Hard). Experimental
results demonstrate that prompts with higher PVar outperform randomly selected
prompts or those with lower PVar. We also show that our PVar-based selection
method is robust, when using smaller reward models (1B, 3B) for selection.
Notably, in a separate experiment using the original human annotations from the
UltraFeedback dataset, we found that training on only the top 10\% of prompts
with the highest PVar yields better evaluation performance than training on the
full dataset, highlighting the importance of preference variance in identifying
informative examples for efficient LLM alignment.

</details>


### [23] [GatePro: Parameter-Free Expert Selection Optimization for Mixture-of-Experts Models](https://arxiv.org/abs/2510.13079)
*Chen Zheng,Yuhang Cai,Deyi Liu,Jin Ma,Yiyuan Ma,Yuan Yang,Jing Liu,Yutao Zeng,Xun Zhou,Siyuan Qiao*

Main category: cs.CL

TL;DR: 本文提出了GatePro，一种无参数的专家选择多样性促进方法，旨在解决混合专家模型中专家选择冗余的问题。


<details>
  <summary>Details</summary>
Motivation: 当前大规模语言模型采用混合专家架构，但功能相似的专家经常被同时选中，导致计算冗余并限制模型容量，现有方法未能有效提升专家多样性。

Method: GatePro通过识别最相似的专家对并引入局部竞争机制，防止冗余专家的共激活，同时保持专家的自然专长，无需额外参数且可热插拔使用。

Result: 实验证明GatePro在各种规模模型和基准测试中有效提升了专家多样性，促使专家发展出更加独特和互补的能力，避免了功能冗余。

Conclusion: GatePro是一种实用且高效的解决方案，可提升混合专家模型的效果，具有无参数和灵活部署的优势。

Abstract: Modern large language models leverage Mixture-of-Experts (MoE) architectures
for efficient scaling, but face a critical challenge: functionally similar
experts are often selected simultaneously, creating redundant computation and
limiting effective model capacity. Existing auxiliary balance loss methods
improve token distribution but fail to address the underlying expert diversity
problem. We introduce GatePro, a novel parameter-free method that directly
promotes expert selection diversity. GatePro identifies the most similar expert
pairs and introduces localized competition mechanisms, preventing redundant
expert co-activation while maintaining natural expert specialization. Our
comprehensive evaluation demonstrates GatePro's effectiveness across model
scales and benchmarks. Analysis demonstrates GatePro's ability to achieve
enhanced expert diversity, where experts develop more distinct and
complementary capabilities, avoiding functional redundancy. This approach can
be deployed hot-swappable during any training phase without additional
learnable parameters, offering a practical solution for improving MoE
effectiveness.

</details>


### [24] [ESI: Epistemic Uncertainty Quantification via Semantic-preserving Intervention for Large Language Models](https://arxiv.org/abs/2510.13103)
*Mingda Li,Xinyu Li,Weinan Zhang,Longxuan Ma*

Main category: cs.CL

TL;DR: 本文提出了一种基于语义保持干预的灰盒不确定性量化方法，用于评估大型语言模型（LLMs）的不确定性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的不确定性量化较为复杂，亟需有效的方法提高模型的可靠性。

Method: 从因果视角出发，利用模型输出在语义保持干预前后的变化来量化模型的不确定性。

Result: 通过理论分析证明了方法对认识性不确定性的有效估计，且在多个LLM和问答数据集上表现出较高的效果和计算效率。

Conclusion: 该方法不仅提高了LLM不确定性量化的准确性，还具备较好的计算效率，适用于实际应用。

Abstract: Uncertainty Quantification (UQ) is a promising approach to improve model
reliability, yet quantifying the uncertainty of Large Language Models (LLMs) is
non-trivial. In this work, we establish a connection between the uncertainty of
LLMs and their invariance under semantic-preserving intervention from a causal
perspective. Building on this foundation, we propose a novel grey-box
uncertainty quantification method that measures the variation in model outputs
before and after the semantic-preserving intervention. Through theoretical
justification, we show that our method provides an effective estimate of
epistemic uncertainty. Our extensive experiments, conducted across various LLMs
and a variety of question-answering (QA) datasets, demonstrate that our method
excels not only in terms of effectiveness but also in computational efficiency.

</details>


### [25] [Multi-Label Clinical Text Eligibility Classification and Summarization System](https://arxiv.org/abs/2510.13115)
*Surya Tejaswi Yerramsetty,Almas Fathimah*

Main category: cs.CL

TL;DR: 本文提出一种结合自然语言处理和大语言模型的系统，实现临床文本的多标签资格分类与摘要生成，旨在自动化临床试验资格评估，提高研究效率。


<details>
  <summary>Details</summary>
Motivation: 临床试验对于医疗进步至关重要，需要涵盖多样化的医疗背景参与者，自动化判断资格条件能够提升效率。

Method: 利用Word2Vec和命名实体识别提取医疗特征，结合计数向量和TF-IDF等技术，尝试加权TF-IDF词嵌入；用随机森林和SVM进行多标签分类；采用TextRank、Luhn及GPT-3进行摘要。

Result: 通过ROUGE分数评价，证明了提出方法在临床资格分类和摘要生成上的有效性。

Conclusion: 该系统展现了利用数据驱动方法自动化临床试验资格评估的潜力，有助于提升医学研究效率。

Abstract: Clinical trials are central to medical progress because they help improve
understanding of human health and the healthcare system. They play a key role
in discovering new ways to detect, prevent, or treat diseases, and it is
essential that clinical trials include participants with appropriate and
diverse medical backgrounds. In this paper, we propose a system that leverages
Natural Language Processing (NLP) and Large Language Models (LLMs) to automate
multi-label clinical text eligibility classification and summarization. The
system combines feature extraction methods such as word embeddings (Word2Vec)
and named entity recognition to identify relevant medical concepts, along with
traditional vectorization techniques such as count vectorization and TF-IDF
(Term Frequency-Inverse Document Frequency). We further explore weighted TF-IDF
word embeddings that integrate both count-based and embedding-based strengths
to capture term importance effectively. Multi-label classification using Random
Forest and SVM models is applied to categorize documents based on eligibility
criteria. Summarization techniques including TextRank, Luhn, and GPT-3 are
evaluated to concisely summarize eligibility requirements. Evaluation with
ROUGE scores demonstrates the effectiveness of the proposed methods. This
system shows potential for automating clinical trial eligibility assessment
using data-driven approaches, thereby improving research efficiency.

</details>


### [26] [Stable LLM Ensemble: Interaction between Example Representativeness and Diversity](https://arxiv.org/abs/2510.13143)
*Junichiro Niimi*

Main category: cs.CL

TL;DR: 研究探讨了选择具有代表性的一次性示例和增加采样温度以提高大语言模型集成的表现，提出的方法显著优于随机示例选择和多示例提示。


<details>
  <summary>Details</summary>
Motivation: 一次性大语言模型预测的准确性和稳健性对示例选择和集成成员之间的多样性极为敏感，这限制了其实际应用效果。

Method: 比较基于质心的代表性示例选择和随机示例选择两种一次性策略，并调整采样温度以改变输出多样性。

Result: 基于代表性示例和较高采样温度的方法在宏F1得分和RMSE上均大幅优于随机选择和多示例提示，提升明显。

Conclusion: 结合代表性示例选择与适度提升采样温度，为大语言模型集成提供了恰当的多样性，有助于设计更有效的一次性LLM集成方案。

Abstract: Large language models (LLMs) have achieved remarkable results in wide range
of domains. However, the accuracy and robustness of one-shot LLM predictions
remain highly sensitive to the examples and the diversity among ensemble
members. This study systematically investigates the effects of example
representativeness (one-shot strategy) and output diversity (sampling
temperature) on LLM ensemble performance. Two one-shot strategies are compared:
centroid-based representative examples (proposed) and randomly sampled examples
(baseline) and sampling temperature also is varied. The proposed approach with
higher temperature setting significantly outperforms random selection by +7.6%
(macro-F1) and -10.5% (RMSE). Furthermore, the proposed model exceeds 5-shot
prompting by +21.1% (macro-F1) and -24.0% (RMSE). Our findings demonstrate that
combining representative example selection with increased temperature provides
the appropriate level of diversity to the ensemble. This work highlights the
practical importance of both example selection and controlled diversity in
designing effective one-shot LLM ensembles.

</details>


### [27] [I Am Aligned, But With Whom? MENA Values Benchmark for Evaluating Cultural Alignment and Multilingual Bias in LLMs](https://arxiv.org/abs/2510.13154)
*Pardis Sadat Zahraei,Ehsaneddin Asgari*

Main category: cs.CL

TL;DR: MENAValues是一个评估大型语言模型在中东和北非地区文化价值观和多语言偏见的基准。


<details>
  <summary>Details</summary>
Motivation: 当前AI评测中中东和北非区域被严重忽视，缺乏反映该地区文化价值观的评测数据和方法。

Method: 通过大规模权威人类调查数据，构建包含16国人口级响应的结构化数据集，设计中英及本地语言下不同视角的多条件评测框架，分析模型文化契合度和语言偏见。

Result: 发现跨语言价值观偏移、推理诱导的文化契合恶化和模型隐藏偏好（Logit泄漏），以及模型在本地语言环境下将多样国家简化为单一语言类别。

Conclusion: MENAValues提供了诊断文化不协调问题的可扩展框架，为开发更具文化包容性的AI模型提供了实证见解和方法工具。

Abstract: We introduce MENAValues, a novel benchmark designed to evaluate the cultural
alignment and multilingual biases of large language models (LLMs) with respect
to the beliefs and values of the Middle East and North Africa (MENA) region, an
underrepresented area in current AI evaluation efforts. Drawing from
large-scale, authoritative human surveys, we curate a structured dataset that
captures the sociocultural landscape of MENA with population-level response
distributions from 16 countries. To probe LLM behavior, we evaluate diverse
models across multiple conditions formed by crossing three perspective framings
(neutral, personalized, and third-person/cultural observer) with two language
modes (English and localized native languages: Arabic, Persian, Turkish). Our
analysis reveals three critical phenomena: "Cross-Lingual Value Shifts" where
identical questions yield drastically different responses based on language,
"Reasoning-Induced Degradation" where prompting models to explain their
reasoning worsens cultural alignment, and "Logit Leakage" where models refuse
sensitive questions while internal probabilities reveal strong hidden
preferences. We further demonstrate that models collapse into simplistic
linguistic categories when operating in native languages, treating diverse
nations as monolithic entities. MENAValues offers a scalable framework for
diagnosing cultural misalignment, providing both empirical insights and
methodological tools for developing more culturally inclusive AI.

</details>


### [28] [Mirror Speculative Decoding: Breaking the Serial Barrier in LLM Inference](https://arxiv.org/abs/2510.13161)
*Nikhil Bhendawade,Kumari Nishu,Arnav Kundu,Chris Bartels,Minsik Cho,Irina Belousova*

Main category: cs.CL

TL;DR: 该论文提出了一种名为Mirror Speculative Decoding（Mirror-SD）的推理算法，通过并行在异构加速器上执行分支完成的推断过程，并结合多令牌投机流技术，显著提升了大规模语言模型的推理速度和准确率，达到了2.8x-5.8x的加速效果。


<details>
  <summary>Details</summary>
Motivation: 现有的投机解码方法受限于自回归草稿生成的延迟和接受率之间的权衡，提升草稿大小虽能提高接受率，却会带来额外的延迟，限制了推理效率的提升。

Method: Mirror-SD采用早停信号启动的分支完成推理，与目标模型后缀并行执行，在GPU和NPU异构加速器上显式映射计算，实现跨设备并行。草稿模型预测目标文本的继续部分，目标模型则同时预测草稿的修正路径，形成互补的执行管线，同时引入多令牌投机流，加快草稿生成。

Result: 在包含14B至66B参数服务器级模型的SpecBench基准测试中，Mirror-SD实现了2.8到5.8倍的全流程加速，任务多样化条件下相比最强基线EAGLE3提升了30%的平均相对性能。

Conclusion: Mirror-SD成功打破了推理延迟与接受率的折中，通过异构并行和多令牌投机技术，有效提升了大规模语言模型推理的效率和性能，具有较大应用前景。

Abstract: Speculative decoding accelerates LLM inference by using a draft model to look
ahead, but gains are capped by the cost of autoregressive draft generation:
increasing draft size elevates acceptance rates but introduces additional
latency overhead exacerbating the speed-accuracy tradeoff. Prior methods
(Medusa, Hydra, EAGLE) partially reduce draft cost but either degrade
acceptance or introduce overheads that limit scaling. We present Mirror
Speculative Decoding (Mirror-SD), an inference algorithm that breaks the
latency-acceptance tradeoff. Mirror-SD launches branch-complete rollouts from
early-exit signals in parallel with the target model's suffix and explicitly
maps computation across heterogeneous accelerators (GPU and NPU) to exploit
cross-device parallelism. The draft speculates forward continuations for the
target to verify, while the target simultaneously speculates correction paths
for the draft, converting speculation into two complementary execution
pipelines. To further cut draft latency without weakening acceptance semantics,
we add speculative streaming so the draft emits multiple tokens per step. This
dual strategy of parallel heterogeneous execution plus multi-token speculative
streaming pushes speculative decoding toward its ideal regime of high
acceptance with low overhead. On SpecBench with server-scale models from 14B to
66B parameters, Mirror-SD delivers consistent end-to-end gains, achieving
2.8x-5.8x wall-time speedups across diverse tasks and a 30% average relative
improvement over the strongest baseline, EAGLE3.

</details>


### [29] [A Matter of Representation: Towards Graph-Based Abstract Code Generation](https://arxiv.org/abs/2510.13163)
*Nyx Iskandar,Hisham Bedri,Andy Tsen*

Main category: cs.CL

TL;DR: 本文提出并评估了基于JSON的图表示方法，以实现高精度的图形化抽象代码生成，特别适用于视觉编程语言环境。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型主要擅长生成线性代码，且对基于图的抽象代码生成研究较少，而这种方法在视觉编程语言和代码不可见的情境下很有用。

Method: 提出使用JSON表示图的方法，并基于自定义的Scratch Python实现，在ScratchTest基准上测试LLM的代码图生成能力。

Result: 实验表明，在正确的图表示下，LLM能单次完成图形化代码生成任务，不同的图表示会显著影响生成准确率。

Conclusion: 本研究为图形化抽象代码生成的表示学习奠定了基础，证明图表示对生成性能至关重要。

Abstract: Most large language models (LLMs) today excel at generating raw, sequential
code with minimal abstractions and custom structures. However, there has been
little work on graph-based abstract code generation, where significant logic is
encapsulated in predefined nodes and execution flow is determined by edges.
This is relevant for visual programming languages, and in cases where raw
source code is inaccessible to users and LLM training sets. In this work, we
propose and evaluate JSON representations for graphs to enable high accuracy
graph-based abstract code generation. We evaluate these representations on
ScratchTest, a mini-benchmark based on our custom Python re-implementation of
Scratch, which tests the LLM in code graph space. Our findings demonstrate that
LLMs can indeed perform the aforementioned generation task in a single pass
without relying on specialized or complex pipelines, given the correct graph
representations. We also show that different representations induce
significantly different accuracies, highlighting the instrumental role of
representations in this generation task. All in all, this work establishes the
first steps towards representation learning for graph-based abstract code
generation.

</details>


### [30] [CoT-Evo: Evolutionary Distillation of Chain-of-Thought for Scientific Reasoning](https://arxiv.org/abs/2510.13166)
*Kehua Feng,Keyan Ding,Zhihui Zhu,Lei Liang,Qiang Zhang,Huajun Chen*

Main category: cs.CL

TL;DR: 提出CoT-Evo框架，通过进化算法优化链式推理轨迹，提高科学领域推理数据质量，提升小模型科学推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在科学领域推理时易产生错误和肤浅的推理，直接蒸馏效果不佳，限制了小模型性能。

Method: 构建多样化推理轨迹池，结合自动检索领域知识，利用新颖性驱动选择、反思重组和变异等进化策略，通过适应度函数（正确率、一致性、知识利用）迭代优化推理轨迹。

Result: 通过优化生成的高质量链式推理数据集，微调小模型在科学推理基准上达到最新性能。

Conclusion: CoT-Evo为从多样且有缺陷的大模型输出综合高质量科学推理数据提供了一种可扩展、有效的进化方法。

Abstract: While chain-of-thought (CoT) distillation from advanced large language models
(LLMs) has proven effective in general reasoning tasks, it struggles in
scientific domains where even advanced models often produce incorrect or
superficial reasoning due to high complexity and specialized knowledge
requirements. Directly distilling from such flawed outputs results in
low-quality training data and limits the performance of smaller student models.
To overcome this, we propose CoT-Evo, an evolutionary CoT distillation
framework. It begins by constructing a diverse pool of reasoning trajectories
from multiple LLM thinkers, enriches them with automatically retrieved domain
knowledge, and iteratively refines the trajectories using novelty-driven
selection, reflective recombination and mutation. The refinement is guided by a
fitness function that evaluates answer correctness, coherence, and effective
knowledge utilization. This results in a high-quality CoT dataset tailored for
scientific reasoning. We employ this evolved dataset to fine-tune a compact
model, which achieves state-of-the-art performance on scientific reasoning
benchmarks. Our work establishes a scalable approach to synthesizing
high-fidelity scientific reasoning data from diverse and fallible LLMs.

</details>


### [31] [Putting on the Thinking Hats: A Survey on Chain of Thought Fine-tuning from the Perspective of Human Reasoning Mechanism](https://arxiv.org/abs/2510.13170)
*Xiaoshu Chen,Sihang Zhou,Ke Liang,Duanyang Yuan,Haoyuan Chen,Xiaoyu Sun,Linyuan Meng,Xinwang Liu*

Main category: cs.CL

TL;DR: 本文综述了链式思维(CoT)微调技术，结合人类认知理论，系统分析其方法及应用，并展望未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有CoT微调调查多聚焦技术细节，缺乏基于人类推理机制的系统分析，而CoT的目标是使大语言模型具备类人推理能力。

Method: 借鉴“六顶思考帽”框架，将CoT微调方法分类审视，汇总已有数据集与模型表现，维护实时GitHub资源库。

Result: 通过结合认知理论，系统揭示CoT微调各类方法特点，促进对LLM推理能力培育的理解与比较。

Conclusion: 本综述填补基于认知视角的CoT研究空白，为未来CoT微调创新与发展提供理论支持与资源参考。

Abstract: Chain of thought (CoT) fine-tuning aims to endow large language models (LLMs)
with reasoning capabilities by training them on curated reasoning traces. It
leverages both supervised and reinforced fine-tuning to cultivate human-like
reasoning skills in LLMs, including detailed planning, divergent thinking,
intuitive judgment, timely reflection, internal thinking, and fact perception,
etc. As CoT fine-tuning has advanced, LLMs have demonstrated substantial
improvements in tasks such as mathematical reasoning and code generation.
However, existing surveys about CoT fine-tuning primarily focus on technical
aspects and overlook a systematic analysis from the perspective of human
reasoning mechanisms. Given that the ultimate goal of CoT fine-tuning is to
enable LLMs to reason like humans, it is crucial to investigate this technique
through the lens of human cognition. To fill this gap, we present the first
comprehensive survey of CoT fine-tuning grounded in human reasoning theory.
Specifically, inspired by the well-known Six Thinking Hats framework, which
systematically characterizes common human thinking modes using six metaphorical
hats, we classify and examine CoT fine-tuning methods through this lens.
Furthermore, building upon this theory, we outline potential directions for
future research in CoT fine-tuning. In addition, we compile a comprehensive
overview of existing datasets and model performances, and a real-time GitHub
repository \footnote{https://github.com/AI-Chen/Awesome-CoT-Finetuning} that
continuously tracks recent advances in this area is maintained. We hope this
survey will serve as a valuable resource to inspire innovation and foster
progress in this rapidly evolving field.

</details>


### [32] [DSCD: Large Language Model Detoxification with Self-Constrained Decoding](https://arxiv.org/abs/2510.13183)
*Ming Dong,Jinkui Zhang,Bolong Zheng,Xinhui Tu,Po Hu,Tingting He*

Main category: cs.CL

TL;DR: 提出了一种无需参数微调的语言模型自约束解码去毒方法DSCD，能够有效降低有害内容生成，提升安全性和流畅性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的去毒方法多依赖外部约束，增加资源消耗且影响生成流畅度。

Method: DSCD通过增强安全层的下一个词分布，同时削弱幻觉和有毒层的分布，实现去毒输出。

Result: 在多个开源大语言模型和公开数据集上，DSCD展示了领先的去毒和生成流畅性能，且效率优于现有方法。

Conclusion: DSCD是一种轻量级、高兼容且易集成的方法，具备实际应用潜力，为更安全的语言模型部署提供解决方案。

Abstract: Detoxification in large language models (LLMs) remains a significant research
challenge. Existing decoding detoxification methods are all based on external
constraints, which require additional resource overhead and lose generation
fluency. This work proposes Detoxification with Self-Constrained Decoding
(DSCD), a novel method for LLM detoxification without parameter fine-tuning.
DSCD strengthens the inner next-token distribution of the safety layer while
weakening that of hallucination and toxic layers during output generation. This
effectively diminishes toxicity and enhances output safety. DSCD offers
lightweight, high compatibility, and plug-and-play capabilities, readily
integrating with existing detoxification methods for further performance
improvement. Extensive experiments on representative open-source LLMs and
public datasets validate DSCD's effectiveness, demonstrating state-of-the-art
(SOTA) performance in both detoxification and generation fluency, with superior
efficiency compared to existing methods. These results highlight DSCD's
potential as a practical and scalable solution for safer LLM deployments.

</details>


### [33] [SHIELD: Classifier-Guided Prompting for Robust and Safer LVLMs](https://arxiv.org/abs/2510.13190)
*Juan Ren,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: 提出了SHIELD，一种轻量级、与模型无关的预处理框架，通过细粒度安全分类和针对性指导，提升大型视觉语言模型的安全性，降低攻击成功率，同时保持模型实用性。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型虽然具备强大多模态推理能力，但攻击面扩大，尤其容易被恶意输入隐蔽目标攻击，传统二元安全检测手段不足以应对复杂的攻击需求。

Method: 设计SHIELD框架，结合细粒度安全分类与类别特定的指导，采取阻断、重新构造和转发三种显式操作，生成定制化安全提示，无需模型重训练即可实现细致拒绝或安全引导。

Result: 在五个基准测试和五种代表性大型视觉语言模型上，SHIELD有效降低了越狱和不服从指令的比率，同时保持了模型的使用效能，且实现即插即用且计算开销极低。

Conclusion: SHIELD作为一种实用的安全补丁，可广泛适用于弱对齐和强对齐的视觉语言模型，具备良好的扩展性以应对新型攻击，显著提升了模型的安全防护能力。

Abstract: Large Vision-Language Models (LVLMs) unlock powerful multimodal reasoning but
also expand the attack surface, particularly through adversarial inputs that
conceal harmful goals in benign prompts. We propose SHIELD, a lightweight,
model-agnostic preprocessing framework that couples fine-grained safety
classification with category-specific guidance and explicit actions (Block,
Reframe, Forward). Unlike binary moderators, SHIELD composes tailored safety
prompts that enforce nuanced refusals or safe redirection without retraining.
Across five benchmarks and five representative LVLMs, SHIELD consistently
lowers jailbreak and non-following rates while preserving utility. Our method
is plug-and-play, incurs negligible overhead, and is easily extendable to new
attack types -- serving as a practical safety patch for both weakly and
strongly aligned LVLMs.

</details>


### [34] [Grounding Long-Context Reasoning with Contextual Normalization for Retrieval-Augmented Generation](https://arxiv.org/abs/2510.13191)
*Jiamin Chen,Yuchen Li,Xinyu Ma,Xinran Chen,Xiaokun Zhang,Shuaiqiang Wang,Chen Ma,Dawei Yin*

Main category: cs.CL

TL;DR: 本文研究了检索增强生成（RAG）中上下文格式对模型性能的影响，提出了一种自适应标准化上下文表示的方法，提高了模型对顺序变化的鲁棒性和长文本利用能力。


<details>
  <summary>Details</summary>
Motivation: 当前RAG研究多关注检索质量和提示策略，忽视了检索文档呈现方式对性能的影响。

Method: 设计控制实验考察上下文密度、分隔符样式和位置对性能的影响，提出上下文归一化策略以标准化上下文表示。

Result: 上下文归一化显著提升了模型对顺序变化的鲁棒性和利用长上下文的能力，在多种基准测试中表现优异。

Conclusion: RAG系统的可靠性不仅取决于检索内容，还依赖于内容的呈现方式，提出的方法为更好地进行长上下文推理提供了实用技术和实证支持。

Abstract: Retrieval-Augmented Generation (RAG) has become an essential approach for
extending the reasoning and knowledge capacity of large language models (LLMs).
While prior research has primarily focused on retrieval quality and prompting
strategies, the influence of how the retrieved documents are framed, i.e.,
context format, remains underexplored. We show that seemingly superficial
choices, such as delimiters or structural markers in key-value extraction, can
induce substantial shifts in accuracy and stability, even when semantic content
is identical. To systematically investigate this effect, we design controlled
experiments that vary context density, delimiter styles, and positional
placement, revealing the underlying factors that govern performance
differences. Building on these insights, we introduce Contextual Normalization,
a lightweight strategy that adaptively standardizes context representations
before generation. Extensive experiments on both controlled and real-world RAG
benchmarks across diverse settings demonstrate that the proposed strategy
consistently improves robustness to order variation and strengthens
long-context utilization. These findings underscore that reliable RAG depends
not only on retrieving the right content, but also on how that content is
presented, offering both new empirical evidence and a practical technique for
better long-context reasoning.

</details>


### [35] [StressTransfer: Stress-Aware Speech-to-Speech Translation with Emphasis Preservation](https://arxiv.org/abs/2510.13194)
*Xi Chen,Yuchen Song,Satoshi Nakamura*

Main category: cs.CL

TL;DR: 本文提出了一种利用大语言模型（LLMs）实现跨语言重音转换的重音感知语音到语音翻译系统，能有效保持词级强调。


<details>
  <summary>Details</summary>
Motivation: 强调语音在跨语言翻译中的重要性及现有数据稀缺问题。

Method: 通过将源语言重音转换为目标语言标签，指导可控TTS模型，并利用自动生成对齐训练数据的管线和"LLM作为评判者"进行评价。

Result: 在保持强调的同时，翻译质量、说话人意图和自然度均优于基线方法。

Conclusion: 提出的方法实现了有效且数据高效的重音保持方案，突显了韵律在语音翻译中的重要性。

Abstract: We propose a stress-aware speech-to-speech translation (S2ST) system that
preserves word-level emphasis by leveraging LLMs for cross-lingual emphasis
conversion. Our method translates source-language stress into target-language
tags that guide a controllable TTS model. To overcome data scarcity, we
developed a pipeline to automatically generate aligned training data and
introduce the "LLM-as-Judge" for evaluation. Experiments show our approach
substantially outperforms baselines in preserving emphasis while maintaining
comparable translation quality, speaker intent, and naturalness. Our work
highlights the importance of prosody in translation and provides an effective,
data-efficient solution for preserving paralinguistic cues in S2ST.

</details>


### [36] [Text Anomaly Detection with Simplified Isolation Kernel](https://arxiv.org/abs/2510.13197)
*Yang Cao,Sikun Yang,Yujiu Yang,Lianyong Qi,Ming Liu*

Main category: cs.CL

TL;DR: 本文提出了简化隔离核（SIK），用于将大语言模型的高维密集嵌入映射到低维稀疏表示，从而提升文本异常检测的计算效率和性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型提取的高维密集嵌入在文本异常检测中效果好，但占用大量内存且计算成本高，亟需降维且保持异常特征的方法。

Method: 提出了线性时间复杂度的简化隔离核（SIK），通过边界关注的特征映射实现降维和稀疏化，保持关键异常特征。

Result: 在7个数据集上的实验显示，SIK在保持计算效率和低内存使用的同时，性能优于11个最新异常检测算法。

Conclusion: SIK有效解决了大语言模型嵌入高维带来的性能瓶颈，提升了文本异常检测的效率和准确性。

Abstract: Two-step approaches combining pre-trained large language model embeddings and
anomaly detectors demonstrate strong performance in text anomaly detection by
leveraging rich semantic representations. However, high-dimensional dense
embeddings extracted by large language models pose challenges due to
substantial memory requirements and high computation time. To address this
challenge, we introduce the Simplified Isolation Kernel (SIK), which maps
high-dimensional dense embeddings to lower-dimensional sparse representations
while preserving crucial anomaly characteristics. SIK has linear time
complexity and significantly reduces space complexity through its innovative
boundary-focused feature mapping. Experiments across 7 datasets demonstrate
that SIK achieves better detection performance than 11 state-of-the-art (SOTA)
anomaly detection algorithms while maintaining computational efficiency and low
memory cost. All code and demonstrations are available at
https://github.com/charles-cao/SIK.

</details>


### [37] [LLM-Guided Synthetic Augmentation (LGSA) for Mitigating Bias in AI Systems](https://arxiv.org/abs/2510.13202)
*Sai Suhruth Reddy Karri,Yashwanth Sai Nallapuneni,Laxmi Narasimha Reddy Mallireddy,Gopichand G*

Main category: cs.CL

TL;DR: 本文提出了一种基于大型语言模型的合成数据增强方法（LGSA），用于减少自然语言处理中的性别偏见，同时保持高准确率。


<details>
  <summary>Details</summary>
Motivation: 传统公平性方法依赖受保护属性标签，存在准确率与公平性的权衡，且难以跨数据集推广，需寻找无需依赖属性标签且能减少偏见的新方法。

Method: 利用大型语言模型生成针对代表性不足群体的反事实样本（性别互换的句子），通过结构化提示生成，配合语义相似度、属性验证、毒性筛查和人工抽检保证数据质量，将扩充的数据用于训练分类器。

Result: 使用LGSA的数据集训练模型在准确率达到99.1%的同时，将性别偏差由7.2%降至1.9%，相比基础模型和简单交换增强方法表现更优。

Conclusion: LGSA在减少模型性别偏见的同时，提升了任务准确率和子群平衡，是一种有效的公平性增强策略。

Abstract: Bias in AI systems, especially those relying on natural language data, raises
ethical and practical concerns. Underrepresentation of certain groups often
leads to uneven performance across demographics. Traditional fairness methods,
such as pre-processing, in-processing, and post-processing, depend on
protected-attribute labels, involve accuracy-fairness trade-offs, and may not
generalize across datasets. To address these challenges, we propose LLM-Guided
Synthetic Augmentation (LGSA), which uses large language models to generate
counterfactual examples for underrepresented groups while preserving label
integrity. We evaluated LGSA on a controlled dataset of short English sentences
with gendered pronouns, professions, and binary classification labels.
Structured prompts were used to produce gender-swapped paraphrases, followed by
quality control including semantic similarity checks, attribute verification,
toxicity screening, and human spot checks. The augmented dataset expanded
training coverage and was used to train a classifier under consistent
conditions. Results show that LGSA reduces performance disparities without
compromising accuracy. The baseline model achieved 96.7 percent accuracy with a
7.2 percent gender bias gap. Simple swap augmentation reduced the gap to 0.7
percent but lowered accuracy to 95.6 percent. LGSA achieved 99.1 percent
accuracy with a 1.9 percent bias gap, improving performance on female-labeled
examples. These findings demonstrate that LGSA is an effective strategy for
bias mitigation, enhancing subgroup balance while maintaining high task
accuracy and label fidelity.

</details>


### [38] [A fully automated and scalable Parallel Data Augmentation for Low Resource Languages using Image and Text Analytics](https://arxiv.org/abs/2510.13211)
*Prawaal Sharma,Navneet Goyal,Poonam Goyal,Vishnupriyan R*

Main category: cs.CL

TL;DR: 提出了一种自动化方法从报纸文章中提取双语平行语料，促进低资源语言的机器翻译，提高BLEU分数约3点。


<details>
  <summary>Details</summary>
Motivation: 全球语言多样性导致优质数字语言资源分布不均，尤其低资源语言缺乏数据，限制了自然语言处理技术的发展。

Method: 利用图像和文本分析技术，从报纸文章中自动提取双语平行语料，构建平行数据集。

Result: 成功构建两个语言组合的平行语料库，在机器翻译任务中较现有基线提升近3个BLEU分数。

Conclusion: 该方法可有效解决低资源语言数据不足问题，提升机器翻译质量，具有较强的实用价值和扩展潜力。

Abstract: Linguistic diversity across the world creates a disparity with the
availability of good quality digital language resources thereby restricting the
technological benefits to majority of human population. The lack or absence of
data resources makes it difficult to perform NLP tasks for low-resource
languages. This paper presents a novel scalable and fully automated methodology
to extract bilingual parallel corpora from newspaper articles using image and
text analytics. We validate our approach by building parallel data corpus for
two different language combinations and demonstrate the value of this dataset
through a downstream task of machine translation and improve over the current
baseline by close to 3 BLEU points.

</details>


### [39] [Hierarchical Frequency Tagging Probe (HFTP): A Unified Approach to Investigate Syntactic Structure Representations in Large Language Models and the Human Brain](https://arxiv.org/abs/2510.13255)
*Jingmin An,Yilong Song,Ruolin Yang,Nai Ding,Lingxi Lu,Yuxuan Wang,Wei Wang,Chu Zhuang,Qian Wang,Fang Fang*

Main category: cs.CL

TL;DR: 该论文提出了Hierarchical Frequency Tagging Probe（HFTP）工具，利用频域分析揭示大语言模型（LLMs）和人脑在句法结构处理中的神经响应特点，发现LLMs在相似层次处理句法，而人脑在不同皮层区域处理不同句法层级，且LLMs与大脑左半球语义区域表现出高度相似性，模型升级表现出不同的脑相似度趋势。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型（LLMs）在句法结构处理上的计算机制是否与人脑相似，帮助理解LLMs行为能力的神经基础。

Method: 引入HFTP工具，采用频域分析识别LLMs的个别神经元和人类大脑皮层编码句法结构的特征，结合代表性相似性分析对比模型与人脑表征。

Result: 发现多种LLMs（包括GPT-2、Gemma系列、Llama系列和GLM-4）在句法处理层表现类似，而人脑依赖不同皮层区域处理不同句法层次，LLMs表征与左半球语言优势区表现出更强的相似性，升级模型表现出脑相似度的不同趋势。

Conclusion: 该研究揭示了LLMs的句法处理机制与人脑的异同，提出是否由类人机制驱动LLMs改进的新问题，表明HFTP是连接计算语言学与认知神经科学的有效工具。

Abstract: Large Language Models (LLMs) demonstrate human-level or even superior
language abilities, effectively modeling syntactic structures, yet the specific
computational modules responsible remain unclear. A key question is whether LLM
behavioral capabilities stem from mechanisms akin to those in the human brain.
To address these questions, we introduce the Hierarchical Frequency Tagging
Probe (HFTP), a tool that utilizes frequency-domain analysis to identify
neuron-wise components of LLMs (e.g., individual Multilayer Perceptron (MLP)
neurons) and cortical regions (via intracranial recordings) encoding syntactic
structures. Our results show that models such as GPT-2, Gemma, Gemma 2, Llama
2, Llama 3.1, and GLM-4 process syntax in analogous layers, while the human
brain relies on distinct cortical regions for different syntactic levels.
Representational similarity analysis reveals a stronger alignment between LLM
representations and the left hemisphere of the brain (dominant in language
processing). Notably, upgraded models exhibit divergent trends: Gemma 2 shows
greater brain similarity than Gemma, while Llama 3.1 shows less alignment with
the brain compared to Llama 2. These findings offer new insights into the
interpretability of LLM behavioral improvements, raising questions about
whether these advancements are driven by human-like or non-human-like
mechanisms, and establish HFTP as a valuable tool bridging computational
linguistics and cognitive neuroscience. This project is available at
https://github.com/LilTiger/HFTP.

</details>


### [40] [Do You Get the Hint? Benchmarking LLMs on the Board Game Concept](https://arxiv.org/abs/2510.13271)
*Ine Gevers,Walter Daelemans*

Main category: cs.CL

TL;DR: 本文通过引入基于自然语言的猜词游戏Concept，评估大型语言模型在溯因推理任务上的表现，发现模型表现远逊于人类且多语言环境下表现更差。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在抽象推理任务上表现不佳，尤其是当任务涉及非自然语言的表示形式时，因此需要一种更贴近自然语言的测试任务来评估其推理能力。

Method: 设计概念猜词板游戏作为基准，通过该游戏探测模型的溯因推理能力，并在多语言环境下测试模型表现。

Result: 人类玩家胜率超过90%，而最先进的大型语言模型胜率未超过40%。模型难以理解其他玩家的战略意图，难以根据逐步更新的信息修正初始假设。多语言测试中，模型在资源较少的语言中表现下降明显。

Conclusion: 目前大型语言模型在基于自然语言的溯因推理任务中仍存在显著不足，尤其是在低资源语言环境下，提示其推理和跨语言能力仍需提升。

Abstract: Large language models (LLMs) have achieved striking successes on many
benchmarks, yet recent studies continue to expose fundamental weaknesses. In
particular, tasks that require abstract reasoning remain challenging, often
because they use representations such as grids, symbols, or visual patterns
that differ from the natural language data LLMs are trained on. In this paper,
we introduce Concept, a simple word-guessing board game, as a benchmark for
probing abductive reasoning in a representation that is much closer to LLM
pre-training data: natural language. Our results show that this game, easily
solved by humans (with a success rate of over 90\%), is still very challenging
for state-of-the-art LLMs (no model exceeds 40\% success rate). Specifically,
we observe that LLMs struggle with interpreting other players' strategic
intents, and with correcting initial hypotheses given sequential information
updates. In addition, we extend the evaluation across multiple languages, and
find that the LLM performance drops further in lower-resource languages (Dutch,
French, and Spanish) compared to English.

</details>


### [41] [Beyond Correctness: Rewarding Faithful Reasoning in Retrieval-Augmented Generation](https://arxiv.org/abs/2510.13272)
*Zhichao Xu,Zongyu Wu,Yun Zhou,Aosong Feng,Kang Zhou,Sangmin Woo,Kiran Ramnath,Yijun Tian,Xuan Qi,Weikang Qiu,Lin Lee Cheong,Haibo Ding*

Main category: cs.CL

TL;DR: 本文提出VERITAS框架，通过细粒度的推理一致性奖励，提升强化学习驱动的搜索代理在中间推理步骤的可信度，同时保持问答任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习训练的大型语言模型使用搜索引擎辅助生成答案时，尽管最终答案正确，往往忽略了中间推理步骤的可信性，导致推理链不够可靠。

Method: 作者构建了一个综合评价框架，涵盖三种推理一致性指标，并提出VERITAS框架，将细粒度推理一致性奖励融入强化学习训练中，以提升推理过程的可信度。

Result: 实验结果表明，使用VERITAS训练的模型在推理一致性上显著提升，同时在七个问答基准测试中保持了相当的任务表现。

Conclusion: VERITAS框架有效促进了搜索代理的推理可信度，解决了强化学习模型在利用搜索引擎时推理链不可信的问题，且不影响最终问答性能。

Abstract: Inspired by the success of reinforcement learning (RL) in Large Language
Model (LLM) training for domains like math and code, recent works have begun
exploring how to train LLMs to use search engines more effectively as tools for
retrieval-augmented generation. Although these methods achieve performance
improvement across QA benchmarks, many prioritize final answer correctness
while overlooking the quality of intermediate reasoning steps, which may lead
to chain-of-thought unfaithfulness. In this paper, we first introduce a
comprehensive evaluation framework for evaluating RL-based search agents,
covering three distinct faithfulness metrics: information-think faithfulness,
think-answer faithfulness, and think-search faithfulness. Our evaluations
reveal that a prototypical RL-based search agent, Search-R1, has significant
room for improvement in this regard. To foster faithful reasoning, we introduce
VERITAS (Verifying Entailed Reasoning through Intermediate Traceability in
Agentic Search), a novel framework that integrates fine-grained faithfulness
rewards into the reinforcement learning process. Our experiments show that
models trained with VERITAS not only significantly improve reasoning
faithfulness, but also achieve comparable task performance across seven QA
benchmarks.

</details>


### [42] [In-Distribution Steering: Balancing Control and Coherence in Language Model Generation](https://arxiv.org/abs/2510.13285)
*Arthur Vogels,Benjamin Wong,Yann Choho,Annabelle Blangero,Milan Bhan*

Main category: cs.CL

TL;DR: 本文提出了一种基于输入数据分布动态调整激活操控强度的新方法——分布内操控（IDS），以提高大型语言模型的控制精度和文本生成的连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有的激活操控方法使用固定的操控强度，导致控制效果不足或文本生成质量下降，亟需一种能自适应调整操控强度的方法。

Method: IDS根据输入数据在表示空间中的分布位置动态调节激活操控强度，确保干预适应输入分布，实现生成的稳定性。

Result: 实验表明，IDS在分类任务上准确率高，同时生成的文本连贯且无崩溃现象。

Conclusion: IDS能够更好地适应输入数据分布，实现高效、稳定的激活操控，适合实际应用场景。

Abstract: Activation steering methods control large language model (LLM) behavior by
modifying internal activations at inference time. However, most existing
activation steering methods rely on a fixed steering strength, leading to
either insufficient control or unadapted intervention that degrades text
plausibility and coherence. We introduce In-Distribution Steering (IDS), a
novel method that adapts steering strength based on the input data distribution
in representation space. IDS dynamically adjusts interventions according to how
far a given input lies within the distribution, enabling adaptive intervention
and generation stability during text generation. Experiments demonstrate that
IDS achieves strong accuracy on classification tasks while producing coherent
text without collapse, making IDS particularly well suited for real-world
applications.

</details>


### [43] [Higher Satisfaction, Lower Cost: A Technical Report on How LLMs Revolutionize Meituan's Intelligent Interaction Systems](https://arxiv.org/abs/2510.13291)
*Xuxin Cheng,Ke Zeng,Zhiquan Cao,Linyi Dai,Wenxuan Gao,Fei Han,Ai Jian,Feng Hong,Wenxing Hu,Zihe Huang,Dejian Kong,Jia Leng,Zhuoyuan Liao,Pei Liu,Jiaye Lin,Xing Ma,Jingqing Ruan,Jiaxing Song,Xiaoyu Tan,Ruixuan Xiao,Wenhui Yu,Wenyu Zhan,Haoxing Zhang,Chao Zhou,Hao Zhou,Shaodong Zheng,Ruinian Chen,Siyuan Chen,Ziyang Chen,Yiwen Dong,Yaoyou Fan,Yangyi Fang,Yang Gan,Shiguang Guo,Qi He,Chaowen Hu,Binghui Li,Dailin Li,Xiangyu Li,Yan Li,Chengjian Liu,Xiangfeng Liu,Jiahui Lv,Qiao Ma,Jiang Pan,Cong Qin,Chenxing Sun,Wen Sun,Zhonghui Wang,Abudukelimu Wuerkaixi,Xin Yang,Fangyi Yuan,Yawen Zhu,Tianyi Zhai,Jie Zhang,Runlai Zhang,Yao Xu,Yiran Zhao,Yifan Wang,Xunliang Cai,Yangen Hu,Cao Liu,Lu Pan,Xiaoli Wang,Bo Xiao,Wenyuan Yao,Qianlin Zhou,Benchang Zhu*

Main category: cs.CL

TL;DR: 本文提出WOWService，一种集成大型语言模型与多智能体架构的智能交互系统，旨在提升客户体验，解决数据构建难、多轮对话性能不足、业务规则频繁变化等挑战。


<details>
  <summary>Details</summary>
Motivation: 当前智能交互系统在冷启动数据构建、多轮对话理解与执行、业务规则变动、单一模型不足以及多轮对话评价困难等方面存在问题，影响服务质量和扩展性。

Method: WOWService通过整合大型语言模型与多智能体架构，实现自动任务管理与协同解决方案，重点包括数据构建、能力提升、场景适应、多智能体协作及自动化评估模块。

Result: WOWService已在美团App部署，显著提升关键用户满意度指标，用户满意度指标1下降27.53%，指标2提升25.51%，显示出其有效性。

Conclusion: WOWService有效捕捉用户需求，推动个性化服务发展，提升了智能交互系统在工业应用中的实用性和拓展能力。

Abstract: Enhancing customer experience is essential for business success, particularly
as service demands grow in scale and complexity. Generative artificial
intelligence and Large Language Models (LLMs) have empowered intelligent
interaction systems to deliver efficient, personalized, and 24/7 support. In
practice, intelligent interaction systems encounter several challenges: (1)
Constructing high-quality data for cold-start training is difficult, hindering
self-evolution and raising labor costs. (2) Multi-turn dialogue performance
remains suboptimal due to inadequate intent understanding, rule compliance, and
solution extraction. (3) Frequent evolution of business rules affects system
operability and transferability, constraining low-cost expansion and
adaptability. (4) Reliance on a single LLM is insufficient in complex
scenarios, where the absence of multi-agent frameworks and effective
collaboration undermines process completeness and service quality. (5) The
open-domain nature of multi-turn dialogues, lacking unified golden answers,
hampers quantitative evaluation and continuous optimization. To address these
challenges, we introduce WOWService, an intelligent interaction system tailored
for industrial applications. With the integration of LLMs and multi-agent
architectures, WOWService enables autonomous task management and collaborative
problem-solving. Specifically, WOWService focuses on core modules including
data construction, general capability enhancement, business scenario
adaptation, multi-agent coordination, and automated evaluation. Currently,
WOWService is deployed on the Meituan App, achieving significant gains in key
metrics, e.g., User Satisfaction Metric 1 (USM 1) -27.53% and User Satisfaction
Metric 2 (USM 2) +25.51%, demonstrating its effectiveness in capturing user
needs and advancing personalized service.

</details>


### [44] [Mismatch Aware Guidance for Robust Emotion Control in Auto-Regressive TTS Models](https://arxiv.org/abs/2510.13293)
*Yizhou Peng,Yukun Ma,Chong Zhang,Yi-Wen Chao,Chongjia Ni,Bin Ma*

Main category: cs.CL

TL;DR: 本文提出了一种自适应的无分类器引导(CFG)方法，用于解决自回归文本到语音(TTS)模型中情感表达与文本语义内容不匹配的问题，实现更自然和精准的情感控制。


<details>
  <summary>Details</summary>
Motivation: 当前TTS系统通过自然语言提示实现细粒度的情感表达，但当情感提示与文本语义不匹配时，会导致语音不自然，影响情感控制效果。CFG技术虽能增强提示对齐，但在自回归TTS中的应用不足，可能降低音质。

Method: 本文基于对CFG在自回归TTS模型中情感表达影响的深入分析，提出一种自适应CFG方案，根据使用大语言模型或自然语言推断模型测量的提示与文本不匹配程度动态调整引导强度。

Result: 实验结果显示，提出的自适应CFG方案在保持音质和可懂度的同时，提升了自回归TTS模型的情感表达能力。

Conclusion: 所提自适应CFG方法有效缓解了风格提示与语义内容的矛盾，增强了自回归TTS系统的情感表现，推动了细粒度情感控制技术的发展。

Abstract: While Text-to-Speech (TTS) systems can achieve fine-grained control over
emotional expression via natural language prompts, a significant challenge
emerges when the desired emotion (style prompt) conflicts with the semantic
content of the text. This mismatch often results in unnatural-sounding speech,
undermining the goal of achieving fine-grained emotional control.
Classifier-Free Guidance (CFG) is a key technique for enhancing prompt
alignment; however, its application to auto-regressive (AR) TTS models remains
underexplored, which can lead to degraded audio quality. This paper directly
addresses the challenge of style-content mismatch in AR TTS models by proposing
an adaptive CFG scheme that adjusts to different levels of the detected
mismatch, as measured using large language models or natural language inference
models. This solution is based on a comprehensive analysis of CFG's impact on
emotional expressiveness in state-of-the-art AR TTS models. Our results
demonstrate that the proposed adaptive CFG scheme improves the emotional
expressiveness of the AR TTS model while maintaining audio quality and
intelligibility.

</details>


### [45] [LLM one-shot style transfer for Authorship Attribution and Verification](https://arxiv.org/abs/2510.13302)
*Pablo Miralles-González,Javier Huertas-Tato,Alejandro Martín,David Camacho*

Main category: cs.CL

TL;DR: 本文提出了一种基于大规模语言模型预训练和上下文学习能力的无监督计算文体分析方法，通过测量文本风格的可迁移性实现作者身份验证，性能优于同规模的提示方法和对比训练基线。


<details>
  <summary>Details</summary>
Motivation: 现有的有监督和对比学习方法依赖于存在虚假相关性的训练数据，且常常将风格与主题混淆，限制了计算文体分析的效果。尽管大规模语言模型的预训练在检测AI生成文本中自然适用，但其在一般作者身份辨识问题上的潜力尚未被充分利用。

Method: 利用大规模语言模型（LLM）的因果语言建模预训练和上下文学习能力，采用LLM生成文本的对数概率来度量一种文本的风格向另一文本的可迁移性，从而进行作者身份验证，且支持测试阶段基于计算资源与准确率的灵活权衡。

Result: 该方法显著优于同规模的提示方法，在控制主题相关性的情况下准确率高于对比训练基线，且随着基础模型规模增大性能稳定提升，作者身份验证时可通过增加测试时计算提升准确率。

Conclusion: 基于LLM预训练和上下文学习能力的无监督计算文体分析方法在作者身份验证等任务中表现优越，具有良好的性能扩展性和灵活的计算资源利用策略，展示了大规模预训练模型在计算文体领域的广泛应用潜力。

Abstract: Computational stylometry analyzes writing style through quantitative patterns
in text, supporting applications from forensic tasks such as identity linking
and plagiarism detection to literary attribution in the humanities. Supervised
and contrastive approaches rely on data with spurious correlations and often
confuse style with topic. Despite their natural use in AI-generated text
detection, the CLM pre-training of modern LLMs has been scarcely leveraged for
general authorship problems. We propose a novel unsupervised approach based on
this extensive pre-training and the in-context learning capabilities of LLMs,
employing the log-probabilities of an LLM to measure style transferability from
one text to another. Our method significantly outperforms LLM prompting
approaches of comparable scale and achieves higher accuracy than contrastively
trained baselines when controlling for topical correlations. Moreover,
performance scales fairly consistently with the size of the base model and, in
the case of authorship verification, with an additional mechanism that
increases test-time computation; enabling flexible trade-offs between
computational cost and accuracy.

</details>


### [46] [ChatR1: Reinforcement Learning for Conversational Reasoning and Retrieval Augmented Question Answering](https://arxiv.org/abs/2510.13312)
*Simon Lupart,Mohammad Aliannejadi,Evangelos Kanoulas*

Main category: cs.CL

TL;DR: ChatR1是一种基于强化学习的对话问答推理框架，通过动态交互实现灵活、高效的对话推理和信息检索。


<details>
  <summary>Details</summary>
Motivation: 传统的静态问答处理流程难以应对对话中用户意图演变和上下文的不确定性，需要一种能够适应动态变化的推理机制。

Method: ChatR1采用强化学习框架，通过设计意图感知奖励机制，实现对检索和推理过程的动态控制和协同，增强模型对多轮对话的适应性和推理能力。

Result: ChatR1在3B和7B参数模型上均优于竞争模型，在五个包含多话题转变、意图演进和多文档支撑的对话问答数据集上表现出色，多种评测指标均优于对比模型。

Conclusion: 基于强化学习的ChatR1推理框架较传统静态管线更具灵活性和上下文适应性，能更好地处理复杂对话中的推理和检索任务，且具有良好的跨领域泛化能力。

Abstract: We present ChatR1, a reasoning framework based on reinforcement learning (RL)
for conversational question answering (CQA). Reasoning plays an important role
in CQA, where user intent evolves across dialogue turns, and utterances are
often underspecified, requiring contextual interpretation, query reformulation,
and dynamic coordination between retrieval and generation. Unlike static
`rewrite, retrieve, and generate' pipelines, ChatR1 interleaves search and
reasoning across turns, enabling exploratory and adaptive behaviors learned
through RL. To address the challenge of sparse and delayed rewards in RL, we
propose an intent-aware reward that provides turn-level feedback by aligning
retrieval and reasoning with evolving user goals. Our proposed ChatR1
demonstrates strong performance on both 3B and 7B model backbones,
outperforming competitive models on five CQA datasets, measured by different
metrics (F1, BERTScore, and LLM-as-judge). We include a diverse set of CQA
datasets to cover topic shifts, evolving intents, mixed-initiative dialogues,
and multi-document grounding, testing ChatR1's performance from various
aspects. Ablation studies confirm the effectiveness of the intent-aware reward.
Our analyses further reveal diverse reasoning trajectories and effective use of
the search tool. ChatR1 also generalizes robustly across domains, demonstrating
that RL-based reasoning enables more flexible and context-sensitive behavior
than static CQA pipelines.

</details>


### [47] [Embedding-Based Context-Aware Reranker](https://arxiv.org/abs/2510.13329)
*Ye Yuan,Mohammad Amin Shabani,Siqi Liu*

Main category: cs.CL

TL;DR: 本文提出了一种轻量级的上下文感知重排序框架EBCAR，旨在解决长文档拆分检索导致的跨段推理挑战，提高检索准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于长文档拆分的检索方法难以处理跨段推理问题，如指代消解、实体消歧和证据聚合，且现有先进重排序方法无法有效解决这些挑战。

Method: 提出EBCAR，基于检索段落的嵌入向量，利用结构信息和混合注意力机制捕捉文档间及文档内的高低层次关系，实现跨段理解的轻量级重排序。

Result: 在ConTEB基准测试中，EBCAR表现优于现有最先进重排序方法，在需要跨段推理的信息检索任务中表现出更高的准确率和效率。

Conclusion: EBCAR有效提升了长文档检索中跨段推理的能力，兼顾准确性与计算效率，适合实际信息检索场景。

Abstract: Retrieval-Augmented Generation (RAG) systems rely on retrieving relevant
evidence from a corpus to support downstream generation. The common practice of
splitting a long document into multiple shorter passages enables finer-grained
and targeted information retrieval. However, it also introduces challenges when
a correct retrieval would require inference across passages, such as resolving
coreference, disambiguating entities, and aggregating evidence scattered across
multiple sources. Many state-of-the-art (SOTA) reranking methods, despite
utilizing powerful large pretrained language models with potentially high
inference costs, still neglect the aforementioned challenges. Therefore, we
propose Embedding-Based Context-Aware Reranker (EBCAR), a lightweight reranking
framework operating directly on embeddings of retrieved passages with enhanced
cross-passage understandings through the structural information of the passages
and a hybrid attention mechanism, which captures both high-level interactions
across documents and low-level relationships within each document. We evaluate
EBCAR against SOTA rerankers on the ConTEB benchmark, demonstrating its
effectiveness for information retrieval requiring cross-passage inference and
its advantages in both accuracy and efficiency.

</details>


### [48] [Taming the Fragility of KV Cache Eviction in LLM Inference](https://arxiv.org/abs/2510.13334)
*Yuan Feng,Haoyu Guo,JunLin Lv,S. Kevin Zhou,Xike Xie*

Main category: cs.CL

TL;DR: 该论文提出了一种新的缓存淘汰方法DefensiveKV，通过两步线性时间的防御性聚合策略，解决了大语言模型中Key-Value缓存遗忘的重要性不稳定性问题，提高了生成质量，减少质量损失。


<details>
  <summary>Details</summary>
Motivation: 现有基于得分聚合的缓存淘汰方法默认稳定假设，即生成过程中重要缓存条目稳定不变，但该假设脆弱，导致均值聚合法在极端情况下性能下降。

Method: 提出了一个简单高效的两步线性时间防御性聚合策略，控制最坏情况风险，以应对极端场景；具体实现为DefensiveKV和层级预算分配的Layer-DefensiveKV。

Result: 在七个任务领域的18个数据集上，DefensiveKV和Layer-DefensiveKV在20%缓存大小限制下，将生成质量损失分别减少2.3倍和4.3倍，超过最强基线。

Conclusion: 通过风险管理方法优化缓存淘汰，改善了基本假设的脆弱性，显著提升了生成质量，开辟了针对淘汰策略稳定性的新方向。

Abstract: Large language models have revolutionized natural language processing, yet
their deployment remains hampered by the substantial memory and runtime
overhead of the transformer's Key-Value cache. To mitigate this, recent methods
employ a scoring-aggregation framework to evict unimportant cache entries,
based on the stability assumption-that a fixed subset of entries remains
consistently important during generation. However, prior work has largely
focused on refining importance indicators for scoring, while defaulting to mean
aggregation due to a faithful trust in the stability assumption. In this work,
we argue that this underlying assumption is inherently fragile, making mean
aggregation highly vulnerable in extreme cases. To counter this, we propose a
simple yet elegant defensive aggregation strategy: a two-step, linear-time
approach that controls worst-case risk, thereby defending against extreme cases
with negligible computational overhead. Embodying this strategy, we propose a
novel cache eviction method, DefensiveKV and its extension, Layer-DefensiveKV,
which incorporates layer-wise budget allocation. Across seven task domains (18
datasets), our methods reduce generation quality loss by 2.3x and 4.3x
respectively, versus the strongest baseline under a 20% cache size. These
results set new performance benchmarks and pioneer a promising direction for
optimizing cache eviction against underlying fragility through worst-case risk
management. Our code is available at https://github.com/FFY0/DefensiveKV.

</details>


### [49] [Are Proverbs the New Pythian Oracles? Exploring Sentiment in Greek Sayings](https://arxiv.org/abs/2510.13341)
*Katerina Korre,John Pavlopoulos*

Main category: cs.CL

TL;DR: 本文利用自然语言处理技术，特别是大模型(LLMs)，分析希腊谚语的情感，覆盖不同方言及地理区域，发现多数地区谚语偏向负面情感。


<details>
  <summary>Details</summary>
Motivation: 谚语作为跨文化的语言现象，蕴含丰富的传统智慧，但因口头传承限制，全球范围内的谚语研究较少，亟需借助NLP技术进行深入分析。

Method: 基于带注释的希腊谚语数据集，扩展至地方方言，利用大语言模型进行谚语情感分类，结合地理和话题信息制作希腊情感分布图和综合分析。

Result: LLMs能够准确分类谚语情感，实现非传统情感极性任务，且希腊多数区域谚语以负面情感为主。

Conclusion: 本研究证明了大语言模型在传统文化文本情感分析中的有效性，成功绘制地方情感分布图，为传统智慧的数字化保护与研究提供新方法。

Abstract: Proverbs are among the most fascinating linguistic phenomena that transcend
cultural and linguistic boundaries. Yet, much of the global landscape of
proverbs remains underexplored, as many cultures preserve their traditional
wisdom within their own communities due to the oral tradition of the
phenomenon. Taking advantage of the current advances in Natural Language
Processing (NLP), we focus on Greek proverbs, analyzing their sentiment.
Departing from an annotated dataset of Greek proverbs, we expand it to include
local dialects, effectively mapping the annotated sentiment. We present (1) a
way to exploit LLMs in order to perform sentiment classification of proverbs,
(2) a map of Greece that provides an overview of the distribution of sentiment,
(3) a combinatory analysis in terms of the geographic position, dialect, and
topic of proverbs. Our findings show that LLMs can provide us with an accurate
enough picture of the sentiment of proverbs, especially when approached as a
non-conventional sentiment polarity task. Moreover, in most areas of Greece
negative sentiment is more prevalent.

</details>


### [50] [Protect: Towards Robust Guardrailing Stack for Trustworthy Enterprise LLM Systems](https://arxiv.org/abs/2510.13351)
*Karthik Avinash,Nikhil Pareek,Rishav Hada*

Main category: cs.CL

TL;DR: 本文提出了Protect，一种企业级的多模态安全护栏系统，支持文本、图像和音频输入，涵盖毒性、性别歧视、数据隐私和提示注入四个安全维度，并在多个安全指标上表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在企业和关键领域广泛应用，但现有安全护栏系统在实时监管、多模态数据处理及可解释性方面存在不足，难以满足受监管环境的需求。

Method: 提出了一种名为Protect的多模态护栏模型，基于低秩适配技术对专门领域的适配器进行微调，利用多模态数据集训练，并采用教师辅助的注释管线生成高质量、多模态上下文标签。

Result: Protect在涵盖毒性、性别歧视、数据隐私和提示注入的四个安全维度上的表现优于现有开源和专有模型，如WildGuard、LlamaGuard-4和GPT-4.1。

Conclusion: Protect为可信赖、可审计且适合生产环境的多模态安全护栏系统奠定了坚实基础，支持跨文本、图像和音频的安全管理，满足企业级部署需求。

Abstract: The increasing deployment of Large Language Models (LLMs) across enterprise
and mission-critical domains has underscored the urgent need for robust
guardrailing systems that ensure safety, reliability, and compliance. Existing
solutions often struggle with real-time oversight, multi-modal data handling,
and explainability -- limitations that hinder their adoption in regulated
environments. Existing guardrails largely operate in isolation, focused on text
alone making them inadequate for multi-modal, production-scale environments. We
introduce Protect, natively multi-modal guardrailing model designed to operate
seamlessly across text, image, and audio inputs, designed for enterprise-grade
deployment. Protect integrates fine-tuned, category-specific adapters trained
via Low-Rank Adaptation (LoRA) on an extensive, multi-modal dataset covering
four safety dimensions: toxicity, sexism, data privacy, and prompt injection.
Our teacher-assisted annotation pipeline leverages reasoning and explanation
traces to generate high-fidelity, context-aware labels across modalities.
Experimental results demonstrate state-of-the-art performance across all safety
dimensions, surpassing existing open and proprietary models such as WildGuard,
LlamaGuard-4, and GPT-4.1. Protect establishes a strong foundation for
trustworthy, auditable, and production-ready safety systems capable of
operating across text, image, and audio modalities.

</details>


### [51] [Personal Attribute Leakage in Federated Speech Models](https://arxiv.org/abs/2510.13357)
*Hamdan Al-Ali,Ali Reza Ghavamipour,Tommaso Caselli,Fatih Turkmen,Zeerak Talat,Hanan Aldarmaki*

Main category: cs.CL

TL;DR: 本文分析了联邦学习环境下自动语音识别（ASR）模型对属性推断攻击的脆弱性，证明攻击能从模型权重差异中推断出敏感属性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习保护隐私的同时，ASR模型可能泄露用户敏感属性，需评估其安全风险。

Method: 在白盒被动威胁模型下，使用非参数攻击方法，针对Wav2Vec2、HuBERT和Whisper三种ASR模型，通过权重差异推断属性。

Result: 发现性别、年龄、口音、情绪和构音障碍等敏感属性均可被推断，且训练数据中代表性不足的属性更易泄露，口音信息尤其明显。

Conclusion: 联邦学习的ASR模型存在未被发现的安全漏洞，需加强模型训练和防护措施以提升安全性。

Abstract: Federated learning is a common method for privacy-preserving training of
machine learning models. In this paper, we analyze the vulnerability of ASR
models to attribute inference attacks in the federated setting. We test a
non-parametric white-box attack method under a passive threat model on three
ASR models: Wav2Vec2, HuBERT, and Whisper. The attack operates solely on weight
differentials without access to raw speech from target speakers. We demonstrate
attack feasibility on sensitive demographic and clinical attributes: gender,
age, accent, emotion, and dysarthria. Our findings indicate that attributes
that are underrepresented or absent in the pre-training data are more
vulnerable to such inference attacks. In particular, information about accents
can be reliably inferred from all models. Our findings expose previously
undocumented vulnerabilities in federated ASR models and offer insights towards
improved security.

</details>


### [52] [D-SMART: Enhancing LLM Dialogue Consistency via Dynamic Structured Memory And Reasoning Tree](https://arxiv.org/abs/2510.13363)
*Xiang Lei,Qin Li,Min Zhang,Min Zhang*

Main category: cs.CL

TL;DR: 该论文提出了D-SMART框架，通过动态结构化记忆和推理树，提升多轮对话中的事实和逻辑一致性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在多轮对话中存在事实不一致和逻辑衰退问题，且依赖静态知识库及单一路径推理，难以适应对话上下文动态变化。

Method: 提出D-SMART框架，包含动态结构化记忆（DSM）构建对话知识图谱和推理树（RT）进行多步推理，实现对对话上下文的动态建模和推理。

Result: 在MT-Bench-101基准测试中，D-SMART模型在对话一致性评分上超过现有最优方法48%以上，且提升开放源代码模型质量评分至多10.1%。

Conclusion: D-SMART有效提升了大型语言模型多轮对话的事实和逻辑一致性，证明了动态知识图谱和多步推理在对话中的应用价值。

Abstract: Large Language Models (LLMs) often exhibit factual inconsistencies and
logical decay in extended, multi-turn dialogues, a challenge stemming from
their reliance on static, pre-trained knowledge and an inability to reason
adaptively over the dialogue history. Prevailing mitigation strategies, such as
Retrieval-Augmented Generation (RAG) and agentic working memories, improve
information recall but still engage with fundamentally static knowledge sources
and follow pre-defined single reasoning path. This hinders their ability to
preserve factual and logical consistency of their responses in multi-turn
dialogues while the context evolves over time. To address this issue, we
propose D-SMART, a model-agnostic framework designed to maintain multi-turn
dialogue consistency by enabling LLMs to build and reason over a dynamic,
structured representation of the conversational context. This is achieved via
two synergistic components: (1) a Dynamic Structured Memory (DSM), which
incrementally constructs and maintains an authoritative, OWL-compliant
knowledge graph of the conversation; and (2) a Reasoning Tree (RT), which
executes inferences as an explicit and traceable multi-step search over the
graph. As the popular-used quality score (judged by GPT-4) can overlook logical
flaws, we introduce new NLI-based metrics to better measure multi-turn dialogue
consistency. Comprehensive experiments on the MT-Bench-101 benchmark show that
D-SMART significantly outperforms state-of-the-art baselines, elevating the
dialogue consistency score by over 48\% for both proprietary and open-source
models, and notably improves the quality score of the latter by up to 10.1\%.

</details>


### [53] [Document Intelligence in the Era of Large Language Models: A Survey](https://arxiv.org/abs/2510.13366)
*Weishi Wang,Hengchang Hu,Zhijie Zhang,Zhaochen Li,Hongxin Shao,Daniel Dahlmeier*

Main category: cs.CL

TL;DR: 大型语言模型(LLM)推动了文档智能(DAI)的变革，带来理解与生成上的显著进步。


<details>
  <summary>Details</summary>
Motivation: 文档智能作为重要应用领域，随着LLM的发展发生重大变化，有必要系统回顾其发展与未来方向。

Method: 通过综述方式，全面分析DAI的发展历程，重点讨论多模态、多语言和检索增强技术，并提出基于代理和文档专用基础模型的未来研究方向。

Result: 总结了LLM在提升DAI性能上的关键进展与存在的挑战，为多模态、多语言及增强检索等方面的研究提供参考。

Conclusion: 本文提供了DAI领域的结构化分析，展示了LLM在学术和实际应用中的深远影响与未来发展潜力。

Abstract: Document AI (DAI) has emerged as a vital application area, and is
significantly transformed by the advent of large language models (LLMs). While
earlier approaches relied on encoder-decoder architectures, decoder-only LLMs
have revolutionized DAI, bringing remarkable advancements in understanding and
generation. This survey provides a comprehensive overview of DAI's evolution,
highlighting current research attempts and future prospects of LLMs in this
field. We explore key advancements and challenges in multimodal, multilingual,
and retrieval-augmented DAI, while also suggesting future research directions,
including agent-based approaches and document-specific foundation models. This
paper aims to provide a structured analysis of the state-of-the-art in DAI and
its implications for both academic and practical applications.

</details>


### [54] [Make an Offer They Can't Refuse: Grounding Bayesian Persuasion in Real-World Dialogues without Pre-Commitment](https://arxiv.org/abs/2510.13387)
*Buwei He,Yang Liu,Zhaowei Zhang,Zixia Jia,Huijia Wu,Zhaofeng He,Zilong Zheng,Yipeng Kang*

Main category: cs.CL

TL;DR: 本文探讨了利用贝叶斯劝说增强大型语言模型在单轮对话中的战略劝说能力，提出了承诺-沟通机制并设计了两种贝叶斯劝说变体，实验中证明了其在多种说服场景下优于非贝叶斯劝说基线。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在劝说任务中常忽略信息不对称的战略使用，且依赖强先验假设，如何在自然语言中有效运用贝叶斯劝说以提升大型语言模型的劝说能力仍是一大挑战。

Method: 设计了承诺-沟通机制，让劝说者通过叙述自身可能类型指导被劝说者进行贝叶斯信念更新；提出了半形式自然语言（SFNL）和完全自然语言（FNL）两种贝叶斯劝说变体，并构建了包含多种被劝说者和多样任务的综合评估框架。

Result: 实验表明，采用贝叶斯劝说策略的LLM劝说成功率显著高于非贝叶斯基线；SFNL在可信度和逻辑性方面表现更优，FNL在情感共鸣和自然对话中更具鲁棒性；通过监督微调，小型模型也能达到大模型的贝叶斯劝说效果。

Conclusion: 贝叶斯劝说策略结合承诺-沟通机制能有效提升大型语言模型的战略劝说能力，且不同贝叶斯劝说变体适用于不同的说服场景，微调可使小模型表现接近大模型。

Abstract: Persuasion, a fundamental social capability for humans, remains a challenge
for AI systems such as large language models (LLMs). Current studies often
overlook the strategic use of information asymmetry in message design or rely
on strong assumptions regarding pre-commitment. In this work, we explore the
application of Bayesian Persuasion (BP) in natural language within single-turn
dialogue settings, to enhance the strategic persuasion capabilities of LLMs.
Our framework incorporates a commitment-communication mechanism, where the
persuader explicitly outlines an information schema by narrating their
potential types (e.g., honest or dishonest), thereby guiding the persuadee in
performing the intended Bayesian belief update. We evaluate two variants of our
approach: Semi-Formal-Natural-Language (SFNL) BP and Fully-Natural-Language
(FNL) BP, benchmarking them against both naive and strong non-BP (NBP)
baselines within a comprehensive evaluation framework. This framework covers a
diverse set of persuadees -- including LLM instances with varying prompts and
fine-tuning and human participants -- across tasks ranging from specially
designed persuasion scenarios to general everyday situations. Experimental
results on LLM-based agents reveal three main findings: (1) LLMs guided by BP
strategies consistently achieve higher persuasion success rates than NBP
baselines; (2) SFNL exhibits greater credibility and logical coherence, while
FNL shows stronger emotional resonance and robustness in naturalistic
conversations; (3) with supervised fine-tuning, smaller models can attain BP
performance comparable to that of larger models.

</details>


### [55] [Doing Things with Words: Rethinking Theory of Mind Simulation in Large Language Models](https://arxiv.org/abs/2510.13395)
*Agnese Lombardi,Alessandro Lenci*

Main category: cs.CL

TL;DR: 本文评估了生成式代理模型Concordia在模拟“心智理论”(ToM)中的表现，发现GPT-4在通过信念归因选择行动方面表现不足，表明其ToM能力可能是浅层统计关联的产物，而非真正的推理。


<details>
  <summary>Details</summary>
Motivation: 研究旨在验证生成式代理模型Concordia是否能有效模拟心智理论能力，并检验GPT-4是否能够基于社会情境做出真实推断，而非简单的语言记忆。

Method: 研究通过使用Concordia模型构建仿真环境，测试GPT-4在模拟ToM任务中的表现，关注其是否能基于信念归因选择适当行动以及因果关系的推断能力。

Result: 结果显示GPT-4常常无法基于信念归因作出行动选择，且难以生成连贯的因果效应，表明其社会交互处理能力存在显著不足。

Conclusion: 研究质疑现有大语言模型所表现出的“心智理论”能力，强调需要更严谨、基于行动的评估框架来检验模型的真实推理能力。

Abstract: Language is fundamental to human cooperation, facilitating not only the
exchange of information but also the coordination of actions through shared
interpretations of situational contexts. This study explores whether the
Generative Agent-Based Model (GABM) Concordia can effectively model Theory of
Mind (ToM) within simulated real-world environments. Specifically, we assess
whether this framework successfully simulates ToM abilities and whether GPT-4
can perform tasks by making genuine inferences from social context, rather than
relying on linguistic memorization. Our findings reveal a critical limitation:
GPT-4 frequently fails to select actions based on belief attribution,
suggesting that apparent ToM-like abilities observed in previous studies may
stem from shallow statistical associations rather than true reasoning.
Additionally, the model struggles to generate coherent causal effects from
agent actions, exposing difficulties in processing complex social interactions.
These results challenge current statements about emergent ToM-like capabilities
in LLMs and highlight the need for more rigorous, action-based evaluation
frameworks.

</details>


### [56] [Investigating Lexical Change through Cross-Linguistic Colexification Patterns](https://arxiv.org/abs/2510.13407)
*Kim Gfeller,Sabine Stoll,Chundra Cathcart,Paul Widmer*

Main category: cs.CL

TL;DR: 本文研究了语言中的同形异义现象（colexification）及其演变机制，利用三大语言家族的词典数据进行系统比较分析。


<details>
  <summary>Details</summary>
Motivation: 语言意义的演变机制未被完全理解，而colexification现象为探究意义变化提供了新的视角。

Method: 应用系统发育比较模型分析来自南岛语系、印欧语系和乌拉尔语系的词典数据，评估关联性、易借性和使用频率对同形异义的影响。

Result: 关系密切的概念对更倾向于同形异义且变化较慢；高频率和容易借用的概念对变化快且较少同形异义；不同语言家族间存在显著差异，提示区域和文化因素的影响。

Conclusion: colexification的演变受概念关系、使用频率和借用性等多重因素影响，且不同语言家族表现出不同的动态特征，显示了文化和地区因素的重要作用。

Abstract: One of the most intriguing features of language is its constant change, with
ongoing shifts in how meaning is expressed. Despite decades of research, the
factors that determine how and why meanings evolve remain only partly
understood. Colexification -- the phenomenon of expressing multiple distinct
concepts using the same word form -- serves as a valuable window onto the
dynamics of meaning change across languages. Here, we apply phylogenetic
comparative models to dictionary data from three language families,
Austronesian, Indo-European, and Uralic, in order to shed light on the
evolutionary dynamics underlying the colexification of concept pairs. We assess
the effects of three predictors: associativity, borrowability, and usage
frequency. Our results show that more closely related concept pairs are
colexified across a larger portion of the family tree and exhibit slower rates
of change. In contrast, concept pairs that are more frequent and more prone to
borrowing tend to change more rapidly and are less often colexified. We also
find considerable differences between the language families under study,
suggesting that areal and cultural factors may play a role.

</details>


### [57] [Evaluating Arabic Large Language Models: A Survey of Benchmarks, Methods, and Gaps](https://arxiv.org/abs/2510.13430)
*Ahmed Alzubaidi,Shaikha Alsuwaidi,Basma El Amel Boussaha,Leen AlQadi,Omar Alkaabi,Mohammed Alyafeai,Hamza Alobeidli,Hakim Hacid*

Main category: cs.CL

TL;DR: 本文系统综述了阿拉伯语大语言模型评测基准，涵盖40多个评测基准，提出了基准分类体系，分析了现状与不足，讨论了数据采集方法并给出未来建议。


<details>
  <summary>Details</summary>
Motivation: 当前阿拉伯语大语言模型缺乏系统的评测基准综述，需要统一分类和分析现有评测，发现不足以推动研究发展。

Method: 收集和分析40多个阿拉伯语LLM评测基准，提出知识、NLP任务、文化方言、目标特定四类评测分类体系，比较采集方法（原生、翻译、合成）利弊。

Result: 发现评测基准多样化进展显著，但时间性评估、对话多轮评测和文化适应性方面存在不足。

Conclusion: 本综述为阿拉伯语NLP研究提供全面参考，规范评测方法和指标，指导未来基准构建及研究方向改进。

Abstract: This survey provides the first systematic review of Arabic LLM benchmarks,
analyzing 40+ evaluation benchmarks across NLP tasks, knowledge domains,
cultural understanding, and specialized capabilities. We propose a taxonomy
organizing benchmarks into four categories: Knowledge, NLP Tasks, Culture and
Dialects, and Target-Specific evaluations. Our analysis reveals significant
progress in benchmark diversity while identifying critical gaps: limited
temporal evaluation, insufficient multi-turn dialogue assessment, and cultural
misalignment in translated datasets. We examine three primary approaches:
native collection, translation, and synthetic generation discussing their
trade-offs regarding authenticity, scale, and cost. This work serves as a
comprehensive reference for Arabic NLP researchers, providing insights into
benchmark methodologies, reproducibility standards, and evaluation metrics
while offering recommendations for future development.

</details>


### [58] [Beyond Single-Reward: Multi-Pair, Multi-Perspective Preference Optimization for Machine Translation](https://arxiv.org/abs/2510.13434)
*Hao Wang,Linlong Xu,Heng Liu,Yangyang Liu,Xiaohu Zhao,Bo Zeng,Liangying Shao,Longyue Wang,Weihua Luo,Kaifu Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种全新的机器翻译模型对齐方法M^2PO，通过多视角奖励引擎和多对构造策略，提高了翻译质量和模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的直接偏好优化方法存在两大挑战：奖励信号存在缺陷（尤其忽视翻译幻觉问题），且数据利用效率低，导致学习信号浪费。

Method: 通过多视角奖励引擎结合幻觉惩罚和动态质量评分，同时采用多对构造策略，从全部候选翻译中创建偏好对，丰富学习信号。

Result: 在WMT21-22基准测试中，M^2PO显著优于现有偏好优化方法，并在与领先私有大语言模型的对比中表现出了高度竞争力。

Conclusion: M^2PO有效提升了机器翻译模型的鲁棒性和翻译准确性，证明了多视角多对偏好优化策略的有效性和潜力。

Abstract: Direct Preference Optimization (DPO) is a powerful paradigm for aligning
Large Language Models (LLMs) to human preferences in Machine Translation (MT),
but current methods are hindered by two fundamental challenges: (1) flawed
reward signals from Quality Estimation (QE) models that overlook critical
errors like translation hallucination, and (2) inefficient data utilization
that discards valuable learning signals by selecting only a single win-loss
pair. To address these limitations, we introduce M^2PO: Multi-Pair,
Multi-Perspective Preference Optimization. Our framework integrates a
multi-perspective reward engine that creates a more robust signal by combining
two key viewpoints: a new hallucination penalty for factuality, and an
innovative dynamic quality score that adaptively fuses external evaluations
with the model's own evolving judgment. This is synergistically paired with a
multi-pair construction strategy that systematically creates a comprehensive
set of preference pairs from the entire pool of translation candidates. This
synergistic approach ensures the model learns from a richer spectrum of quality
trade-offs, leading to more robust and faithful translations. On challenging
WMT21-22 benchmarks, M^2PO substantially outperforms existing preference
optimization methods and demonstrates highly competitive performance against
leading proprietary LLMs.

</details>


### [59] [LiteraryQA: Towards Effective Evaluation of Long-document Narrative QA](https://arxiv.org/abs/2510.13494)
*Tommaso Bonomo,Luca Gioffré,Roberto Navigli*

Main category: cs.CL

TL;DR: 本文提出了LiteraryQA，一个高质量的文学作品问答数据集，旨在解决NarrativeQA中的噪声和错误问题，并评估了自动评价指标的有效性。


<details>
  <summary>Details</summary>
Motivation: NarrativeQA作为叙事情节问答的主流基准存在文档噪声和问答对质量差的问题，影响了系统的可靠性。

Method: 通过人类和大语言模型验证的流水线，识别并修正低质量的问答样本，去除源文本中多余内容，并进行自动评价指标的元评估，最后对多个长上下文大模型进行基准测试。

Result: n-gram基础的自动评价指标与人工判断的系统级相关性较低，而使用小型开源权重模型作为评判的LLM评估结果与人工排名高度一致。

Conclusion: LiteraryQA提供了更干净高质量的文学问答数据，LLM作为评判者是一种有效的自动评分方式，未来问答系统的评估应重视新的评价方法。

Abstract: Question Answering (QA) on narrative text poses a unique challenge to current
systems, requiring a deep understanding of long, complex documents. However,
the reliability of NarrativeQA, the most widely used benchmark in this domain,
is hindered by noisy documents and flawed QA pairs. In this work, we introduce
LiteraryQA, a high-quality subset of NarrativeQA focused on literary works.
Using a human- and LLM-validated pipeline, we identify and correct low-quality
QA samples while removing extraneous text from source documents. We then carry
out a meta-evaluation of automatic metrics to clarify how systems should be
evaluated on LiteraryQA. This analysis reveals that all n-gram-based metrics
have a low system-level correlation to human judgment, while LLM-as-a-Judge
evaluations, even with small open-weight models, can strongly agree with the
ranking identified by humans. Finally, we benchmark a set of long-context LLMs
on LiteraryQA. We release our code and data at
https://github.com/SapienzaNLP/LiteraryQA.

</details>


### [60] [ConsintBench: Evaluating Language Models on Real-World Consumer Intent Understanding](https://arxiv.org/abs/2510.13499)
*Xiaozhe Li,TianYi Lyu,Siyi Yang,Yuxi Gong,Yizhao Yang,Jinxuan Huang,Ligao Zhang,Zhuoyi Huang,Qingwen Liu*

Main category: cs.CL

TL;DR: 本文介绍了一个专门用于评估大型语言模型理解现实中复杂人类意图的动态实时基准测试平台——Bench。


<details>
  <summary>Details</summary>
Motivation: 现有缺乏能够评估大型语言模型在复杂、动态且含多方观点的真实公共讨论中理解人类意图的测试基准。

Method: 研发了Bench基准，设计了自动化的数据采集和筛选流程，保证数据的多样性、实时更新和数据不被污染。

Result: Bench成为首个针对消费者领域意图理解的动态实时评测基准，支持大规模、多样且实时更新的数据。

Conclusion: Bench填补了评估大型语言模型在人类意图理解能力方面的空白，促进更接近实际应用场景的模型测试与改进。

Abstract: Understanding human intent is a complex, high-level task for large language
models (LLMs), requiring analytical reasoning, contextual interpretation,
dynamic information aggregation, and decision-making under uncertainty.
Real-world public discussions, such as consumer product discussions, are rarely
linear or involve a single user. Instead, they are characterized by interwoven
and often conflicting perspectives, divergent concerns, goals, emotional
tendencies, as well as implicit assumptions and background knowledge about
usage scenarios. To accurately understand such explicit public intent, an LLM
must go beyond parsing individual sentences; it must integrate multi-source
signals, reason over inconsistencies, and adapt to evolving discourse, similar
to how experts in fields like politics, economics, or finance approach complex,
uncertain environments. Despite the importance of this capability, no
large-scale benchmark currently exists for evaluating LLMs on real-world human
intent understanding, primarily due to the challenges of collecting real-world
public discussion data and constructing a robust evaluation pipeline. To bridge
this gap, we introduce \bench, the first dynamic, live evaluation benchmark
specifically designed for intent understanding, particularly in the consumer
domain. \bench is the largest and most diverse benchmark of its kind,
supporting real-time updates while preventing data contamination through an
automated curation pipeline.

</details>


### [61] [MedREK: Retrieval-Based Editing for Medical LLMs with Key-Aware Prompts](https://arxiv.org/abs/2510.13500)
*Shujun Xia,Haokun Lin,Yichen Wu,Yinan Zhou,Zixuan Li,Zhongwei Wan,Xingrun Xing,Yefeng Zheng,Xiang Li,Caifeng Shan,Zhenan Sun,Quanzheng Li*

Main category: cs.CL

TL;DR: 本文提出了MedREK，一种基于检索的医学大模型编辑框架，解决了医疗知识内表示重叠导致的检索不准确和批量编辑不足的问题，在单样本及批量编辑任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 医疗领域知识快速变化且训练数据存在错误，导致大型语言模型生成过时或不准确的信息，限制了其在临床实践中的应用。现有模型编辑方法局限性明显，难以满足医学领域的高精度、批量编辑需求。

Method: 构建了涵盖更广医疗主题的MedVersa基准，评估单样本和批量编辑；提出了结合共享查询-键模块以实现精确匹配、以及基于注意力的提示编码器以提供丰富指导的MedREK检索式编辑框架。

Result: 在多个医学基准测试上，MedREK在核心指标上均表现优越，首次实现了医学大模型的批量编辑，并验证了其有效性。

Conclusion: MedREK为医学领域大型语言模型的精准、批量编辑提供了有效方案，推动了医疗应用中模型更新的实用性和可靠性。

Abstract: LLMs hold great promise for healthcare applications, but the rapid evolution
of medical knowledge and errors in training data often cause them to generate
outdated or inaccurate information, limiting their applicability in high-stakes
clinical practice. Model editing has emerged as a potential remedy without full
retraining. While parameter-based editing often compromises locality and is
thus ill-suited for the medical domain, retrieval-based editing offers a more
viable alternative. However, it still faces two critical challenges: (1)
representation overlap within the medical knowledge space often causes
inaccurate retrieval and reduces editing accuracy; (2) existing methods are
restricted to single-sample edits, while batch-editing remains largely
unexplored despite its importance for real-world medical applications. To
address these challenges, we first construct MedVersa, \hk{an enhanced
benchmark with broader coverage of medical subjects, designed to evaluate both
single and batch edits under strict locality constraints}. We then propose
MedREK, a retrieval-based editing framework that integrates a shared query-key
module for precise matching with an attention-based prompt encoder for
informative guidance. Experimental results on various medical benchmarks
demonstrate that our MedREK achieves superior performance across different core
metrics and provides the first validated solution for batch-editing in medical
LLMs. Our code and dataset are available at
https://github.com/mylittleriver/MedREK.

</details>


### [62] [Attention Illuminates LLM Reasoning: The Preplan-and-Anchor Rhythm Enables Fine-Grained Policy Optimization](https://arxiv.org/abs/2510.13554)
*Yang Li,Zhichen Dong,Yuhan Sun,Weixun Wang,Shaopan Xiong,Yijia Luo,Jiashun Liu,Han Lu,Jiamang Wang,Wenbo Su,Bo Zheng,Junchi Yan*

Main category: cs.CL

TL;DR: 本文通过分析大语言模型中的注意力机制，揭示其推理内部逻辑，提出“预规划-锚定”机制，并基于此设计三种强化学习策略，实现推理性能提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的推理过程不透明，现有强化学习方法对所有生成步骤均等赋予信用，难以区分关键步骤和常规步骤。

Method: 区分局部和全局关注的注意力头，设计两个量化指标：窗口平均注意力距离和未来注意力影响，揭示预规划-锚定推理机制；基于此设计动态赋予关键节点奖励的增强学习策略。

Result: 所提三种强化学习策略在多种推理任务中表现出持续的性能提升。

Conclusion: 通过与模型内在推理节奏对齐的优化策略，推动从不透明优化向结构感知的优化过程转变，为大语言模型推理的透明有效优化提供潜在方向。

Abstract: The reasoning pattern of Large language models (LLMs) remains opaque, and
Reinforcement learning (RL) typically applies uniform credit across an entire
generation, blurring the distinction between pivotal and routine steps. This
work positions attention as a privileged substrate that renders the internal
logic of LLMs legible, not merely as a byproduct of computation, but as a
mechanistic blueprint of reasoning itself. We first distinguish attention heads
between locally and globally focused information processing and reveal that
locally focused heads produce a sawtooth pattern near the diagonal indicating
phrasal chunks, while globally focused heads expose tokens that exert broad
downstream influence over future tokens. We formalize these with two metrics:
1) Windowed Average Attention Distance, which measures the extent of backward
attention within a clipped window; 2) Future Attention Influence, which
quantifies a token's global importance as the average attention it receives
from subsequent tokens. Taken together, these signals reveal a recurring
preplan-and-anchor mechanism, where the model first performs a long-range
contextual reference to generate an introductory token, which is immediately
followed by or coincides with a semantic anchor token that organizes subsequent
reasoning. Leveraging these insights, we introduce three novel RL strategies
that dynamically perform targeted credit assignment to critical nodes (preplan
tokens, anchor tokens, and their temporal coupling) and show consistent
performance gains across various reasoning tasks. By aligning optimization with
the model's intrinsic reasoning rhythm, we aim to transform opaque optimization
into an actionable structure-aware process, hoping to offer a potential step
toward more transparent and effective optimization of LLM reasoning.

</details>


### [63] [Sparse Subnetwork Enhancement for Underrepresented Languages in Large Language Models](https://arxiv.org/abs/2510.13580)
*Daniil Gurgurov,Josef van Genabith,Simon Ostermann*

Main category: cs.CL

TL;DR: 本文提出了一种针对低资源语言的语言特定子网络微调方法，显著提升大型语言模型在这些语言上的性能，同时保持模型的通用能力。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在高资源语言和低资源语言之间表现差异显著，亟需一种方法提升低资源语言的表现而不降低整体性能。

Method: 通过语言激活概率熵识别语言特定神经元，只微调这些神经元相关权重的专用子网络，使用目标语言数据进行定向微调。

Result: 在Llama-3.1-8B和Mistral-Nemo-12B的12种中低资源语言上，该方法优于全模型微调、仅FFN微调、LoRA适配及随机子集微调，且仅更新最多1%的参数。

Conclusion: 该方法不仅提升低资源语言性能，还优化训练动态和跨语言表示一致性，是一种低成本高效的低资源语言适配方案，相关资源已开源。

Abstract: Large language models exhibit uneven performance across languages, with
substantial gaps between high- and low-resource languages. We present a
framework for enhancing monolingual capabilities of LLMs in underrepresented
languages while preserving their general-purpose performance through targeted
fine-tuning of language-specific subnetworks. Our approach identifies
language-specific neurons using Language Activation Probability Entropy and
fine-tunes only the weights associated with these neurons, a dedicated
subnetwork, on target-language data. Experiments on Llama-3.1-8B and
Mistral-Nemo-12B across 12 mid- and low-resource languages demonstrate that our
method consistently outperforms full fine-tuning, FFN-only fine-tuning, LoRA
adaptation, and random subset fine-tuning baselines while efficiently updating
only up to 1% of model parameters. Beyond performance improvements, we observe
enhanced favorable training dynamics, cross-lingual representational alignment,
and systematic weight update changes. To facilitate future research, we release
language-specific neuron identifications for over 100 languages as well as our
adaptation pipeline, offering a cost-effective pathway for adapting
state-of-the-art models to underrepresented languages.

</details>


### [64] [Deflanderization for Game Dialogue: Balancing Character Authenticity with Task Execution in LLM-based NPCs](https://arxiv.org/abs/2510.13586)
*Pasin Buakhaw,Kun Kerdthaisong,Phuree Phenhiran,Pitikorn Khlaisamniang,Supasate Vorathammathorn,Piyalitt Ittichaiwong,Nutchanon Yongsatianchot*

Main category: cs.CL

TL;DR: 本文介绍了Tu_Character_lab团队在2025年Commonsense Persona-Grounded Dialogue Challenge第二轮的表现，他们采用轻量级提示技术和微调大模型相结合的方法，在任务导向对话和上下文感知对话中取得了优异名次。


<details>
  <summary>Details</summary>
Motivation: 利用大型语言模型为游戏中的非玩家角色创造功能性执行任务和符合角色个性的对话，提升NPC的表现能力。

Method: 结合轻量级的提示技术（包括Deflanderization提示法抑制过度角色扮演）和基于Qwen3-14B的大模型监督微调与低秩适应技术训练模型。

Result: 在比赛中，团队在任务1和任务3的API轨道均获得第二名，GPU轨道任务3获得第四名，表现优异。

Conclusion: 结合提示技术与微调大模型的方法在多轨道任务导向和上下文感知对话挑战中表现出较强的竞争力。

Abstract: The emergence of large language models (LLMs) has opened new opportunities
for cre- ating dynamic non-player characters (NPCs) in gaming environments,
enabling both func- tional task execution and persona-consistent dialogue
generation. In this paper, we (Tu_Character_lab) report our participation in
the Commonsense Persona-Grounded Dialogue Challenge (CPDC) 2025 Round 2, which
eval- uates agents across three tracks: task-oriented dialogue, context-aware
dialogue, and their integration. Our approach combines two complementary
strategies: (i) lightweight prompting techniques in the API track, including a
Deflanderization prompting method to suppress excessive role-play and improve
task fidelity, and (ii) fine-tuned large models in the GPU track, leveraging
Qwen3-14B with supervisedfinetuning (SFT) and Low-Rank Adaptation(LoRA). Our
best submissions ranked 2nd on Task 1, 2nd on Task 3 (API track), and 4th on
Task 3 (GPU track).

</details>


### [65] [FreshTab: Sourcing Fresh Data for Table-to-Text Generation Evaluation](https://arxiv.org/abs/2510.13598)
*Kristýna Onderková,Ondřej Plátek,Zdeněk Kasner,Ondřej Dušek*

Main category: cs.CL

TL;DR: 该论文提出了FreshTab，一个针对表格内容生成文本的动态基准测试集合，以解决大语言模型训练数据污染和领域不平衡问题，支持多语言数据收集。


<details>
  <summary>Details</summary>
Motivation: 现有表格到文本生成任务的基准测试受限于大语言模型训练数据污染以及领域不平衡，且非英语数据集较少，影响了评估的准确性和泛化能力。

Method: 提出FreshTab，从维基百科动态生成表格到文本的基准数据集，支持按需收集多语言（德语、俄语、法语等）数据，以避免数据污染并实现领域敏感评估。

Result: 实验发现，LLM基于最新表格生成的文本在自动指标上表现较差，但在人类和LLM自身评估中表现尚可，且领域效果在所有评估中均明显，表明领域平衡的基准更具挑战性。

Conclusion: FreshTab有效缓解了训练数据污染和领域偏见问题，支持多语言和领域敏感的表格文本生成评估，促进表格到文本生成任务的公平和准确评测。

Abstract: Table-to-text generation (insight generation from tables) is a challenging
task that requires precision in analyzing the data. In addition, the evaluation
of existing benchmarks is affected by contamination of Large Language Model
(LLM) training data as well as domain imbalance. We introduce FreshTab, an
on-the-fly table-to-text benchmark generation from Wikipedia, to combat the LLM
data contamination problem and enable domain-sensitive evaluation. While
non-English table-to-text datasets are limited, FreshTab collects datasets in
different languages on demand (we experiment with German, Russian and French in
addition to English). We find that insights generated by LLMs from recent
tables collected by our method appear clearly worse by automatic metrics, but
this does not translate into LLM and human evaluations. Domain effects are
visible in all evaluations, showing that a~domain-balanced benchmark is more
challenging.

</details>


### [66] [NOSA: Native and Offloadable Sparse Attention](https://arxiv.org/abs/2510.13602)
*Yuxiang Huang,Chaojun Xiao,Xu Han,Zhiyuan Liu*

Main category: cs.CL

TL;DR: 本文提出NOSA，一种支持KV缓存卸载的可训练稀疏注意力框架，大幅提升了长上下文大模型的解码效率，解码速度最高提升2.3倍，同时保持接近无损的性能。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏注意力方法虽然节省了内存访问，但KV缓存大小未减少，限制了GPU批处理大小及解码吞吐，成为长上下文大模型推理的瓶颈。

Method: 利用稀疏注意力在相邻解码步的强局部性，设计NOSA框架，分解token选择为查询相关和无关两部分，显式引入局部性约束，实现KV缓存的有效卸载，减少CPU-GPU间数据传输且保持训练时的注意力计算不变。

Result: 预训练了1B参数模型，实验证明NOSA几乎无性能损失的同时，解码吞吐比传统可训练稀疏注意力提升了最高2.3倍。

Conclusion: 通过引入显式的局部性约束和KV缓存卸载机制，NOSA显著提升了大规模稀疏注意力模型在长上下文推理中的效率，具有推广价值。

Abstract: Trainable sparse attention has emerged as a promising solution to address the
decoding efficiency bottleneck of LLMs in long-context processing,
significantly saving memory accesses while minimally impacting task
performance. However, existing sparse attention methods leave a crucial
limitation unresolved: the size of the key-value (KV) cache remains unreduced,
which constrains on-GPU batch sizes and throttles decoding throughput,
especially in large-scale batched inference. In this paper, we show that
trainable sparse attention naturally exhibits strong locality in token
selection across adjacent decoding steps, thereby enabling KV cache offloading
without altering the underlying attention computation. However, the inherent
locality remains insufficient to achieve efficient offloading, as the transfer
of selected KV pairs between the CPU and GPU continues to dominate the overall
decoding cost. Building on this insight, we present NOSA, a trainable sparse
attention framework designed to natively support KV cache offloading. NOSA
introduces explicit locality constraints by decomposing token selection into
query-aware and query-agnostic components, thereby reducing KV transfers while
preserving the same attention computation as used during training. We pretrain
a 1B-parameter model with NOSA and conduct extensive benchmarks, showing that
it preserves near-lossless performance while achieving up to a 2.3x improvement
in decoding throughput compared with the vanilla trainable sparse attention
baseline (InfLLM-V2).

</details>


### [67] [MemoTime: Memory-Augmented Temporal Knowledge Graph Enhanced Large Language Model Reasoning](https://arxiv.org/abs/2510.13614)
*Xingyu Tan,Xiaoyang Wang,Qing Liu,Xiwei Xu,Xin Yuan,Liming Zhu,Wenjie Zhang*

Main category: cs.CL

TL;DR: 本文提出了MemoTime，一个增强大语言模型进行时间推理的框架，通过结构化编码、递归推理和经验记忆显著提升时间推理准确度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理涉及多实体、多操作符和事件演化的时间问题时存在困难，现有基于时间知识图谱的方法难以解决时间一致性、多实体同步、多样操作符适配及经验复用等问题。

Method: MemoTime设计了分层的时间树结构进行操作符感知推理，采用动态证据检索策略，并引入自我进化的经验记忆库存储验证的推理过程和子问题嵌入，实现跨类型的经验复用。

Result: 在多个时间问答基准测试中，MemoTime整体性能超越现有强基线最多24%，并使小型模型（如Qwen3-4B）达到接近GPT-4-Turbo的推理水平。

Conclusion: MemoTime有效解决了多实体、多操作符和时序推理中的核心挑战，显著提升了大语言模型的时间理解和推理能力，具有广泛应用前景。

Abstract: Large Language Models (LLMs) have achieved impressive reasoning abilities,
but struggle with temporal understanding, especially when questions involve
multiple entities, compound operators, and evolving event sequences. Temporal
Knowledge Graphs (TKGs), which capture vast amounts of temporal facts in a
structured format, offer a reliable source for temporal reasoning. However,
existing TKG-based LLM reasoning methods still struggle with four major
challenges: maintaining temporal faithfulness in multi-hop reasoning, achieving
multi-entity temporal synchronization, adapting retrieval to diverse temporal
operators, and reusing prior reasoning experience for stability and efficiency.
To address these issues, we propose MemoTime, a memory-augmented temporal
knowledge graph framework that enhances LLM reasoning through structured
grounding, recursive reasoning, and continual experience learning. MemoTime
decomposes complex temporal questions into a hierarchical Tree of Time,
enabling operator-aware reasoning that enforces monotonic timestamps and
co-constrains multiple entities under unified temporal bounds. A dynamic
evidence retrieval layer adaptively selects operator-specific retrieval
strategies, while a self-evolving experience memory stores verified reasoning
traces, toolkit decisions, and sub-question embeddings for cross-type reuse.
Comprehensive experiments on multiple temporal QA benchmarks show that MemoTime
achieves overall state-of-the-art results, outperforming the strong baseline by
up to 24.0%. Furthermore, MemoTime enables smaller models (e.g., Qwen3-4B) to
achieve reasoning performance comparable to that of GPT-4-Turbo.

</details>


### [68] [Unlocking Public Catalogues: Instruction-Tuning LLMs for ICD Coding of German Tumor Diagnoses](https://arxiv.org/abs/2510.13624)
*Stefan Lenz,Lakisha Ortiz Rosario,Georg Vollmar,Arsenij Ustjanzew,Fatma Alickovic,Thomas Kindler,Torsten Panholzer*

Main category: cs.CL

TL;DR: 本文通过对公开数据集进行指令微调，提高了开源大语言模型在德语肿瘤诊断编码任务中的准确率，尤其是在ICD-10-GM和ICD-O-3编码上表现显著提升。


<details>
  <summary>Details</summary>
Motivation: 德语背景下的肿瘤诊断编码对结构化癌症文档至关重要，但小型开源大语言模型在编码准确率方面存在不足。研究旨在通过指令微调提升模型在医学编码任务中的表现。

Method: 利用基于ICD-10-GM、ICD-O-3和OPS目录生成的50万问答对数据集，对Qwen、Llama和Mistral等八个开源模型进行指令微调，并评估其对本地肿瘤诊断文档的编码准确率。

Result: 微调后，ICD-10-GM的准确率从1.4-24%提升至41-58%，部分准确率提升至73-83%；ICD-O-3的编码准确率也有明显提升但仍较低；恶性代码输出降至0%；肿瘤诊断识别率达99%；准确率与模型规模正相关但差距缩小。

Conclusion: 利用公开病历目录构建指令数据集进行微调，能显著提升开源大语言模型在医学文档编码任务中的性能，且更大模型效益更明显。数据与模型权重公开，有望推动医学自动化应用发展。

Abstract: Accurate coding of tumor diagnoses with ICD-10-GM and ICD-O-3 is essential
for structured cancer documentation in Germany. Smaller open-weight LLMs are
appealing for privacy-preserving automation but often struggle with coding
accuracy in German-language contexts. This study investigates whether
instruction-based fine-tuning on public datasets improves the coding accuracy
of open-weight LLMs for German tumor diagnosis texts. The evaluation uses coded
diagnoses from the local tumor documentation system as test data. In a
systematic data quality assessment, the upper limit for ICD-10 coding
performance was estimated at 60-79% for exact and 81-94% for partial
(three-character codes only) derivation. As training data, over 500,000
question-answer pairs were created based on the ICD-10-GM, ICD-O-3, and OPS
catalogues. Eight open-weight models from the Qwen, Llama, and Mistral families
(7-70 B parameters) were fine-tuned. ICD-10-GM accuracy rose from 1.4-24% to
41-58%, and partial accuracy from 31-74% to 73-83%. The accuracy of ICD-O-3
topography coding also improved but started and remained considerably lower
with an exact accuracy of 22-40% and a partial accuracy of 56-67% after
fine-tuning. Malformed code outputs dropped to 0% for all models.
Tumor-diagnosis recognition reached 99%. Accuracy correlated positively with
model size, but gaps between small and large models narrowed after fine-tuning.
The reasoning mode in Qwen3 generally yielded a lower performance than
fine-tuning and was over 100 times slower. Our findings highlight the potential
of leveraging public catalogues to build instruction datasets that improve LLMs
in medical documentation tasks. The complete training dataset and the
best-performing checkpoints of the fine-tuned models are available from
https://huggingface.co/datasets/stefan-m-lenz/ICDOPS-QA-2024.

</details>


### [69] [Closing the Gap Between Text and Speech Understanding in LLMs](https://arxiv.org/abs/2510.13632)
*Santiago Cuervo,Skyler Seto,Maureen de Seyssel,Richard He Bai,Zijin Gu,Tatiana Likhomanenko,Navdeep Jaitly,Zakaria Aldeneh*

Main category: cs.CL

TL;DR: 本文提出SALAD方法，通过主动选择和跨模态蒸馏有效缩小语音适应大语言模型与文本模式之间的理解性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有语音适应的大语言模型在处理语音输入时，性能显著低于文本输入，且现有缩小此差距的方法成本高且依赖大规模合成数据或专有数据，缺乏数据高效的解决方案。

Method: 分析性能下降原因是文本能力遗忘和语音文本跨模态错位，提出SALAD方法结合跨模态蒸馏和目标合成数据，利用主动选择策略提升对齐并缓解能力遗忘。

Result: SALAD在3B和7B参数规模模型上，在知识、语言理解及推理等多领域基准测试中，使用公开语料训练的语音数据远少于现有方法，取得了竞争性表现。

Conclusion: SALAD通过数据高效的跨模态对齐策略，显著提升了语音输入下大语言模型的理解能力，有效缩小了文本与语音理解性能差距。

Abstract: Large Language Models (LLMs) can be adapted to extend their text capabilities
to speech inputs. However, these speech-adapted LLMs consistently underperform
their text-based counterparts--and even cascaded pipelines--on language
understanding tasks. We term this shortfall the text-speech understanding gap:
the performance drop observed when a speech-adapted LLM processes spoken inputs
relative to when the original text-based LLM processes the equivalent text.
Recent approaches to narrowing this gap either rely on large-scale speech
synthesis of text corpora, which is costly and heavily dependent on synthetic
data, or on large-scale proprietary speech datasets, which are not
reproducible. As a result, there remains a need for more data-efficient
alternatives for closing the text-speech understanding gap. In this work, we
analyze the gap as driven by two factors: (i) forgetting of text capabilities
during adaptation, and (ii) cross-modal misalignment between speech and text.
Based on this analysis, we introduce SALAD--Sample-efficient Alignment with
Learning through Active selection and cross-modal Distillation--which combines
cross-modal distillation with targeted synthetic data to improve alignment
while mitigating forgetting. Applied to 3B and 7B LLMs, SALAD achieves
competitive performance with a strong open-weight model across broad-domain
benchmarks in knowledge, language understanding, and reasoning, while training
on over an order of magnitude less speech data from public corpora.

</details>


### [70] [How Sampling Affects the Detectability of Machine-written texts: A Comprehensive Study](https://arxiv.org/abs/2510.13681)
*Matthieu Dubois,François Yvon,Pablo Piantanida*

Main category: cs.CL

TL;DR: 本文研究了基于采样的语言模型文本生成方法对自动检测器准确性的影响，发现轻微调整解码参数会导致检测准确率大幅下降。


<details>
  <summary>Details</summary>
Motivation: 当前自动文本检测方法在固定生成设置下表现优异，但对解码策略变化的鲁棒性未充分研究。

Method: 系统地分析不同采样解码策略（如温度、top-p等）对文本检测准确性的影响，发布包含37种解码配置的大规模数据集及评估工具。

Result: 发现轻微调整采样参数会导致检测系统的AUROC从近乎完美下降至1%，暴露当前检测方法的盲点。

Conclusion: 需要更全面的评估协议以提升文本生成检测的鲁棒性和实用性，促进相关研究发展。

Abstract: As texts generated by Large Language Models (LLMs) are ever more common and
often indistinguishable from human-written content, research on automatic text
detection has attracted growing attention. Many recent detectors report
near-perfect accuracy, often boasting AUROC scores above 99\%. However, these
claims typically assume fixed generation settings, leaving open the question of
how robust such systems are to changes in decoding strategies. In this work, we
systematically examine how sampling-based decoding impacts detectability, with
a focus on how subtle variations in a model's (sub)word-level distribution
affect detection performance. We find that even minor adjustments to decoding
parameters - such as temperature, top-p, or nucleus sampling - can severely
impair detector accuracy, with AUROC dropping from near-perfect levels to 1\%
in some settings. Our findings expose critical blind spots in current detection
methods and emphasize the need for more comprehensive evaluation protocols. To
facilitate future research, we release a large-scale dataset encompassing 37
decoding configurations, along with our code and evaluation framework
https://github.com/BaggerOfWords/Sampling-and-Detection

</details>


### [71] [NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow Matching](https://arxiv.org/abs/2510.13721)
*Run Luo,Xiaobo Xia,Lu Wang,Longze Chen,Renke Shan,Jing Luo,Min Yang,Tat-Seng Chua*

Main category: cs.CL

TL;DR: NExT-OMNI是一种新型开源多模态基础模型，支持任意模态之间的理解与生成，性能优于现有统一模型。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型多基于自回归架构，难以平衡理解与生成能力，且混合或解耦策略设计冗余，限制跨模态应用。

Method: 提出基于离散流范式的NExT-OMNI，利用度量引导的概率路径和动力学最优速度，实现任意模态间统一建模和高效响应，采用简洁统一的表征替代任务解耦设计。

Result: NExT-OMNI在多模态生成与理解基准测试中表现良好，并在多轮多模态交互及跨模态检索任务上超越之前的统一模型。

Conclusion: NExT-OMNI凭借其架构优势，代表了下一代多模态基础模型的方向，支持更广泛的交叉模态应用，并通过开源促进后续研究。

Abstract: Next-generation multimodal foundation models capable of any-to-any
cross-modal generation and multi-turn interaction will serve as core components
of artificial general intelligence systems, playing a pivotal role in
human-machine interaction. However, most existing multimodal models remain
constrained by autoregressive architectures, whose inherent limitations prevent
a balanced integration of understanding and generation capabilities. Although
hybrid and decoupling strategies have been explored to address these tasks
within unified frameworks separately, their redundant, non-integrated designs
limit their applicability to broader scenarios, such as cross-modal
retrieval.In this work, we introduce NExT-OMNI, an open-source omnimodal
foundation model that achieves unified modeling through discrete flow
paradigms. By leveraging metric-induced probability paths and kinetic optimal
velocities, NExT-OMNI natively supports any-to-any understanding and generation
with enhanced response efficiency, while enabling broader application scenarios
through concise unified representations rather than task-decoupled designs.
Trained on large-scale interleaved text, image, video, and audio data,
NExT-OMNI delivers competitive performance on multimodal generation and
understanding benchmarks, while outperforming prior unified models in
multi-turn multimodal interaction and cross-modal retrieval, highlighting its
architectural advantages as a next-generation multimodal foundation model. To
advance further research, we release training details, data protocols, and
open-source both the code and model checkpoints.

</details>


### [72] [GAPS: A Clinically Grounded, Automated Benchmark for Evaluating AI Clinicians](https://arxiv.org/abs/2510.13734)
*Xiuyuan Chen,Tao Sun,Dexin Su,Ailing Yu,Junwei Liu,Zhe Chen,Gangzeng Jin,Xin Wang,Jingnan Liu,Hansong Xiao,Hualei Zhou,Dongjie Tao,Chunxiao Guo,Minghui Yang,Yuan Xia,Jing Zhao,Qianrui Fan,Yanyun Wang,Shuai Zhen,Kezhong Chen,Jun Wang,Zewen Sun,Heng Zhao,Tian Guan,Shaodong Wang,Geyun Chang,Jiaming Deng,Hongchengcheng Chen,Kexin Feng,Ruzhen Li,Jiayi Geng,Changtai Zhao,Jun Wang,Guihu Lin,Peihao Li,Liqi Liu,Peng Wei,Jian Wang,Jinjie Gu,Ping Wang,Fan Yang*

Main category: cs.CL

TL;DR: 本论文提出了GAPS框架，用以多维度评估AI临床系统的认知深度、答案完整性、鲁棒性和安全性，开发了自动化管线构建基于指南的评测基准，实现规模化且客观的临床系统评价。


<details>
  <summary>Details</summary>
Motivation: 现有AI临床系统的评测多依赖多选题或手动评分，难以体现实际临床需求的深度、鲁棒性和安全性，存在主观性和扩展性不足问题。

Method: 提出GAPS多维评估框架，并设计自动化、基于指南的流水线，包括证据邻域构建、图树表示、自动生成多层次问题，及由深度研究代理生成评分标准，最终用大型语言模型集成评判评分。

Result: 验证表明自动生成的问题质量高且符合临床评价，评测结果揭示了当前顶尖模型在推理深度、答案完整性、对抗扰动和安全性方面存在显著缺陷，表现明显下降。

Conclusion: 该方法实现了临床基础的自动化、高质量且可扩展的AI临床系统评测，为提升AI系统安全性和可靠性提供了科学指导。

Abstract: Current benchmarks for AI clinician systems, often based on multiple-choice
exams or manual rubrics, fail to capture the depth, robustness, and safety
required for real-world clinical practice. To address this, we introduce the
GAPS framework, a multidimensional paradigm for evaluating \textbf{G}rounding
(cognitive depth), \textbf{A}dequacy (answer completeness),
\textbf{P}erturbation (robustness), and \textbf{S}afety. Critically, we
developed a fully automated, guideline-anchored pipeline to construct a
GAPS-aligned benchmark end-to-end, overcoming the scalability and subjectivity
limitations of prior work. Our pipeline assembles an evidence neighborhood,
creates dual graph and tree representations, and automatically generates
questions across G-levels. Rubrics are synthesized by a DeepResearch agent that
mimics GRADE-consistent, PICO-driven evidence review in a ReAct loop. Scoring
is performed by an ensemble of large language model (LLM) judges. Validation
confirmed our automated questions are high-quality and align with clinician
judgment. Evaluating state-of-the-art models on the benchmark revealed key
failure modes: performance degrades sharply with increased reasoning depth
(G-axis), models struggle with answer completeness (A-axis), and they are
highly vulnerable to adversarial perturbations (P-axis) as well as certain
safety issues (S-axis). This automated, clinically-grounded approach provides a
reproducible and scalable method for rigorously evaluating AI clinician systems
and guiding their development toward safer, more reliable clinical practice.

</details>


### [73] [Assessing Web Search Credibility and Response Groundedness in Chat Assistants](https://arxiv.org/abs/2510.13749)
*Ivan Vykopal,Matúš Pikuliak,Simon Ostermann,Marián Šimko*

Main category: cs.CL

TL;DR: 本文提出了一种评估聊天助手在网络搜索中引用来源可信度及回答依据性的方法，并比较了GPT-4o、GPT-5、Perplexity和Qwen Chat四种助手在五个易受误导话题上的表现，发现Perplexity引用的来源最可信，而GPT-4o在敏感话题上引用低可信来源较多。


<details>
  <summary>Details</summary>
Motivation: 随着聊天助手越来越多地整合网络搜索功能，提升回答的可靠性成为关键，但同时伴随低可信来源误导风险，迫切需要评估助手引用信息的质量。

Method: 选取涵盖五个易受误导话题的100条信息，评估四款聊天助手（GPT-4o、GPT-5、Perplexity、Qwen Chat）在引用来源的可信度及回答是否基于引用来源的表现。

Result: 发现不同助手有显著差异，Perplexity在来源可信度方面表现最佳，GPT-4o在敏感话题中引用的非可信来源较多。

Conclusion: 该研究首次系统地比较了常用聊天助手在事实核查行为上的表现，为高风险信息环境中AI系统的评估提供了基础。

Abstract: Chat assistants increasingly integrate web search functionality, enabling
them to retrieve and cite external sources. While this promises more reliable
answers, it also raises the risk of amplifying misinformation from
low-credibility sources. In this paper, we introduce a novel methodology for
evaluating assistants' web search behavior, focusing on source credibility and
the groundedness of responses with respect to cited sources. Using 100 claims
across five misinformation-prone topics, we assess GPT-4o, GPT-5, Perplexity,
and Qwen Chat. Our findings reveal differences between the assistants, with
Perplexity achieving the highest source credibility, whereas GPT-4o exhibits
elevated citation of non-credibility sources on sensitive topics. This work
provides the first systematic comparison of commonly used chat assistants for
fact-checking behavior, offering a foundation for evaluating AI systems in
high-stakes information environments.

</details>


### [74] [Confidence-Based Response Abstinence: Improving LLM Trustworthiness via Activation-Based Uncertainty Estimation](https://arxiv.org/abs/2510.13750)
*Zhiqi Huang,Vivek Datla,Chenyang Zhu,Alfy Samuel,Daben Liu,Anoop Kumar,Ritesh Soni*

Main category: cs.CL

TL;DR: 提出一种在检索增强生成系统中基于原始神经网络激活进行置信度估计的方法，提升生成文本的准确性和可信度。


<details>
  <summary>Details</summary>
Motivation: 在金融和医疗等高风险领域，错误回答代价大于不回答，故需要准确的置信度估计以确保高质量输出。

Method: 利用前馈神经网络的原始激活作为自回归信号，避免了传统使用token logits和softmax归一化导致的信息损失，将置信度预测建模为序列分类任务，并通过Huber损失进行训练以增强鲁棒性。

Result: 在金融行业客户支持场景中，方法优于多种强基线模型，在保持严格延迟要求下依然保持高准确率。Llama 3.1 8B模型实验证明只使用第16层激活既保持准确又降低响应延迟。

Conclusion: 基于激活的置信度建模为可信赖的检索增强生成系统部署提供了一条可扩展且与模型架构紧密结合的途径。

Abstract: We propose a method for confidence estimation in retrieval-augmented
generation (RAG) systems that aligns closely with the correctness of large
language model (LLM) outputs. Confidence estimation is especially critical in
high-stakes domains such as finance and healthcare, where the cost of an
incorrect answer outweighs that of not answering the question. Our approach
extends prior uncertainty quantification methods by leveraging raw feed-forward
network (FFN) activations as auto-regressive signals, avoiding the information
loss inherent in token logits and probabilities after projection and softmax
normalization. We model confidence prediction as a sequence classification
task, and regularize training with a Huber loss term to improve robustness
against noisy supervision. Applied in a real-world financial industry
customer-support setting with complex knowledge bases, our method outperforms
strong baselines and maintains high accuracy under strict latency constraints.
Experiments on Llama 3.1 8B model show that using activations from only the
16th layer preserves accuracy while reducing response latency. Our results
demonstrate that activation-based confidence modeling offers a scalable,
architecture-aware path toward trustworthy RAG deployment.

</details>


### [75] [The Mechanistic Emergence of Symbol Grounding in Language Models](https://arxiv.org/abs/2510.13796)
*Shuyu Wu,Ziqiao Ma,Xiaoxi Luo,Yidong Huang,Josue Torres-Fonseca,Freda Shi,Joyce Chai*

Main category: cs.CL

TL;DR: 该论文研究了符号如何通过视觉-语言模型在没有显式监督的情况下实现符号落地，并发现这种落地主要在模型中间层通过注意机制聚合实现。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究表明符号落地可能在大规模训练的视觉-语言模型中自然出现，但具体出现的位置和驱动机制尚不明确。

Method: 提出一种受控评估框架，结合机械因果分析，系统地追踪符号落地在模型内部计算过程中的产生方式。

Result: 发现符号落地集中在模型中间层的注意头，通过聚合环境信息支持语言预测；这一现象在多模态对话和不同模型架构（Transformer、状态空间模型）中复现，但单向LSTM中不存在。

Conclusion: 结果表明符号落地可以在语言模型中自发产生，具有行为和机制证据，对于提高生成可靠性预测和控制有实际意义。

Abstract: Symbol grounding (Harnad, 1990) describes how symbols such as words acquire
their meanings by connecting to real-world sensorimotor experiences. Recent
work has shown preliminary evidence that grounding may emerge in
(vision-)language models trained at scale without using explicit grounding
objectives. Yet, the specific loci of this emergence and the mechanisms that
drive it remain largely unexplored. To address this problem, we introduce a
controlled evaluation framework that systematically traces how symbol grounding
arises within the internal computations through mechanistic and causal
analysis. Our findings show that grounding concentrates in middle-layer
computations and is implemented through the aggregate mechanism, where
attention heads aggregate the environmental ground to support the prediction of
linguistic forms. This phenomenon replicates in multimodal dialogue and across
architectures (Transformers and state-space models), but not in unidirectional
LSTMs. Our results provide behavioral and mechanistic evidence that symbol
grounding can emerge in language models, with practical implications for
predicting and potentially controlling the reliability of generation.

</details>


### [76] [Breadcrumbs Reasoning: Memory-Efficient Reasoning with Compression Beacons](https://arxiv.org/abs/2510.13797)
*Giovanni Monea,Yair Feldman,Shankar Padmanabhan,Kianté Brantley,Yoav Artzi*

Main category: cs.CL

TL;DR: 提出了一种通过学习压缩生成的Transformer键值缓存以提高大语言模型长上下文推理扩展性的方法。


<details>
  <summary>Details</summary>
Motivation: Transformer模型的键值缓存随着上下文长度线性增长，导致显著的内存和计算成本，限制了长上下文推理的扩展性。

Method: 通过学习一个特殊的压缩token，定期压缩生成的键值缓存，并删除压缩后的条目。采用联合蒸馏和强化学习相结合的训练框架优化压缩过程。

Result: 该方法在内存使用和准确率之间达到了更优的权衡，优于无缓存压缩和无训练压缩技术。

Conclusion: 该模型训练的缓存压缩技术有效降低了内存消耗，同时保持推理准确性，提升了大语言模型处理长上下文能力。

Abstract: The scalability of large language models for long-context reasoning is
severely constrained by the linear growth of their Transformer key-value cache,
which incurs significant memory and computational costs. We posit that as a
model generates reasoning tokens, the informational value of past generated
tokens diminishes, creating an opportunity for compression. In this work, we
propose to periodically compress the generation KV cache with a learned,
special-purpose token and evict compressed entries. We train the model to
perform this compression via a modified joint distillation and reinforcement
learning (RL) framework. Our training method minimizes overhead over the
conventional RL process, as it leverages RL outputs for distillation.
Empirically, our method achieves a superior memory-accuracy Pareto frontier
compared to both the model without cache compression and training-free
compression techniques.

</details>


### [77] [BRIEF-Pro: Universal Context Compression with Short-to-Long Synthesis for Fast and Accurate Multi-Hop Reasoning](https://arxiv.org/abs/2510.13799)
*Jia-Chen Gu,Junyi Zhang,Di Wu,Yuankai Li,Kai-Wei Chang,Nanyun Peng*

Main category: cs.CL

TL;DR: BRIEF-Pro是一种轻量级的压缩器，能够将复杂问题检索到的大量信息摘要成简洁内容，提高多跳问答的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 面对检索增强生成模型在处理复杂多跳问题时，信息量庞大会导致延迟和计算负担增大，设计轻量、精准的摘要压缩工具以解决这一瓶颈。

Method: BRIEF-Pro利用种子数据训练对长文本进行抽象压缩，同时支持用户自定义摘要长度，实现多场景下的信息提炼。

Result: 在四个公开多跳问答数据集上，BRIEF-Pro生成更简洁且相关的摘要，显著提升了不同规模语言模型的问答性能；以70B模型为例，压缩率达到32倍，性能提升4.67%，计算开销仅为对比方法的23%。

Conclusion: BRIEF-Pro有效缓解了RAG处理长上下文的瓶颈，通过灵活可控的摘要压缩提高了问答系统的性能和计算效率，适用于复杂多跳检索生成任务。

Abstract: As retrieval-augmented generation (RAG) tackles complex tasks, increasingly
expanded contexts offer richer information, but at the cost of higher latency
and increased cognitive load on the model. To mitigate this bottleneck,
especially for intricate multi-hop questions, we introduce BRIEF-Pro. It is a
universal, lightweight compressor that distills relevant evidence for a given
query from retrieved documents into a concise summary for seamless integration
into in-context RAG. Using seed data consisting of relatively short contexts
(fewer than 1k words), BRIEF-Pro is trained to perform abstractive compression
of extended contexts exceeding 10k words across a wide range of scenarios.
Furthermore, BRIEF-Pro offers flexible user control over summary length by
allowing users to specify the desired number of sentences. Experiments on four
open-domain multi-hop question-answering datasets show that BRIEF-Pro generates
more concise and relevant summaries, enhancing performance across small, large,
and proprietary language models. With the 70B reader model, 32x compression by
BRIEF-Pro improves QA performance by 4.67% on average over LongLLMLingua's 9x,
while requiring only 23% of its computational overhead.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [78] [Semantic knowledge guides innovation and drives cultural evolution](https://arxiv.org/abs/2510.12837)
*Anil Yaman,Shen Tian,Björn Lindström*

Main category: cs.MA

TL;DR: 语义知识通过结构化概念与功能的关联，促进累积文化创新。


<details>
  <summary>Details</summary>
Motivation: 探索认知过程如何通过语义知识支持文化创新生成。

Method: 使用文化演化代理模型和大规模行为实验测试语义知识与社会学习的作用。

Result: 语义知识和社会学习协同促进创新，缺乏语义知识的个体创新表现较差。

Conclusion: 语义知识是推动人类累积文化创新的关键认知机制。

Abstract: Cumulative cultural evolution enables human societies to generate
increasingly complex knowledge and technology over generations. While social
learning transmits innovations between individuals and generations, the
cognitive processes that generate these innovations remain poorly understood.
Here, we demonstrate that semantic knowledge-structured associations between
concepts and their functions-provides cognitive scaffolding for cumulative
innovation by guiding exploration toward plausible and meaningful actions. We
tested this hypothesis using a cultural evolutionary agent-based model and a
large-scale behavioural experiment (N = 1,243), in which individuals performed
a task requiring the combination of items into novel innovations. Across both
approaches, semantic knowledge and social learning interact synergistically to
enhance innovation. Behaviorally, participants without access to semantic
knowledge performed no better than chance, even when social learning was
available, and relied on shallow exploration strategies. These findings suggest
that semantic knowledge is a key cognitive process enabling human cumulative
culture.

</details>


### [79] [KVCOMM: Online Cross-context KV-cache Communication for Efficient LLM-based Multi-agent Systems](https://arxiv.org/abs/2510.12872)
*Hancheng Ye,Zhengqi Gao,Mingyuan Ma,Qinsi Wang,Yuzhe Fu,Ming-Yu Chung,Yueqian Lin,Zhijian Liu,Jianyi Zhang,Danyang Zhuo,Yiran Chen*

Main category: cs.MA

TL;DR: 本论文提出了KVCOMM框架，通过复用和对齐多智能体系统中重叠上下文的KV缓存，实现了大幅提升多智能体大语言模型推理效率。


<details>
  <summary>Details</summary>
Motivation: 多智能体大语言模型系统在处理带有上下文通信的复杂任务时，因重复处理重叠上下文导致效率低下，传统的KV缓存方法无法直接解决多智能体间的缓存偏移问题。

Method: 提出了无训练需求的KVCOMM框架，通过建立和在线维护一个锚点池，动态估计并调整KV缓存偏移，允许不同智能体在多样前缀上下文中复用缓存。

Result: KVCOMM在检索增强生成、数学推理、协同编程等多智能体任务中实现了超过70%的缓存复用率，无质量损失，并在五智能体场景下实现最高7.8倍推理加速，显著减少响应时间。

Conclusion: KVCOMM有效解决了多智能体大语言模型中KV缓存偏移问题，实现了高效推理复用，显著提升多智能体系统的推理速度和资源利用效率。

Abstract: Multi-agent large language model (LLM) systems are increasingly adopted for
complex language processing tasks that require communication and coordination
among agents. However, these systems often suffer substantial overhead from
repeated reprocessing of overlapping contexts across agents. In typical
pipelines, once an agent receives a message from its predecessor, the full
context-including prior turns-must be reprocessed from scratch, leading to
inefficient processing. While key-value (KV) caching is an effective solution
for avoiding redundant computation in single-agent settings where prefixes
remain unchanged, it cannot be directly reused in multi-agent scenarios due to
diverging prefixes introduced by agent-specific context extensions. We identify
that the core challenge lies in the offset variance of KV-caches across agents.
To address this, we propose KVCOMM, a training-free framework that enables
efficient prefilling in multi-agent inference by reusing KV-caches and aligning
cache offsets of overlapping contexts under diverse prefix contexts. KVCOMM
estimates and adjusts KV-caches for shared content by referencing a pool of
cached examples-termed anchors-that store observed cache deviations under
varying prefixes. The anchor pool is maintained and updated online, allowing
dynamic adaptation to distinct user requests and context structures. KVCOMM
achieves over 70% reuse rate across diverse multi-agent workloads, including
retrieval-augmented generation, math reasoning, and collaborative coding tasks,
all without quality degradation. Particularly, when each fully-connected agent
receives 1K input tokens with 512 prefix tokens and 512 output tokens under a
five-agent setting, KVCOMM achieves up to 7.8x speedup compared to the standard
prefill pipeline, reducing TTFT from ~430 ms to ~55 ms.

</details>


### [80] [Agentic Discovery: Closing the Loop with Cooperative Agents](https://arxiv.org/abs/2510.13081)
*J. Gregory Pauloski,Kyle Chard,Ian T. Foster*

Main category: cs.MA

TL;DR: 研究指出在科学发现中，人类决策限制了进展，提出需要合作智能体来辅助实现自主发现。


<details>
  <summary>Details</summary>
Motivation: 现有科学发现速度受限于人类在设定目标、生成假设和设计实验中的决策过程。

Method: 提出结合人工智能与基础设施，开发合作智能体以增强人类角色，实现自主科学发现。

Result: 明确指出合作智能体的实现需要人工智能和基础设施的共同进步。

Conclusion: 合作智能体有望突破人类决策限制，推动科学发现自动化。

Abstract: As data-driven methods, artificial intelligence (AI), and automated workflows
accelerate scientific tasks, we see the rate of discovery increasingly limited
by human decision-making tasks such as setting objectives, generating
hypotheses, and designing experiments. We postulate that cooperative agents are
needed to augment the role of humans and enable autonomous discovery. Realizing
such agents will require progress in both AI and infrastructure.

</details>


### [81] [Altruistic Ride Sharing: A Community-Driven Approach to Short-Distance Mobility](https://arxiv.org/abs/2510.13227)
*Divyanshu Singh,Ashman Mehra,Snehanshu Saha,Santonu Sarkar*

Main category: cs.MA

TL;DR: 本文提出了一个基于利他主义积分的去中心化乘车共享框架ARS，通过多智能体强化学习和博弈论方法，实现公平且可持续的动态拼车，实验证明其在减少交通距离和排放、提升车辆利用率方面优于传统方案。


<details>
  <summary>Details</summary>
Motivation: 城市交通拥堵和燃油消耗问题突出，现有盈利驱动的拼车平台忽视公平与可持续性，需新的共享出行模式促进公平与环境效益。

Method: 提出ARS框架，参与者基于利他点数轮换司机和乘客角色，采用多智能体强化学习(MADDPG)进行动态拼车匹配，结合博弈论均衡保证公平及人口模型维持长期平衡。

Result: 基于纽约出租车数据的实验显示，ARS显著减少了出行距离和排放，提高了车辆利用率，实现了比无共享和优化基线更公平的参与。

Conclusion: ARS作为一种可扩展的社区驱动拼车方案，有助于将个体行为与整体城市可持续发展目标相结合，提供了传统拼车平台的有效替代。

Abstract: Urban mobility faces persistent challenges of congestion and fuel
consumption, specifically when people choose a private, point-to-point commute
option. Profit-driven ride-sharing platforms prioritize revenue over fairness
and sustainability. This paper introduces Altruistic Ride-Sharing (ARS), a
decentralized, peer-to-peer mobility framework where participants alternate
between driver and rider roles based on altruism points rather than monetary
incentives. The system integrates multi-agent reinforcement learning (MADDPG)
for dynamic ride-matching, game-theoretic equilibrium guarantees for fairness,
and a population model to sustain long-term balance. Using real-world New York
City taxi data, we demonstrate that ARS reduces travel distance and emissions,
increases vehicle utilization, and promotes equitable participation compared to
both no-sharing and optimization-based baselines. These results establish ARS
as a scalable, community-driven alternative to conventional ride-sharing,
aligning individual behavior with collective urban sustainability goals.

</details>


### [82] [AOAD-MAT: Transformer-based multi-agent deep reinforcement learning model considering agents' order of action decisions](https://arxiv.org/abs/2510.13343)
*Shota Takayama,Katsuhide Fujita*

Main category: cs.MA

TL;DR: 本文提出了一种基于Transformer的多智能体强化学习模型AOAD-MAT，通过显式考虑智能体决策顺序，动态调整行动顺序，从而提升多智能体合作性能。


<details>
  <summary>Details</summary>
Motivation: 现有MARL模型虽提升性能，却未显式考虑智能体决策顺序的重要性。

Method: 提出AOAD-MAT模型，结合基于Transformer的actor-critic架构和预测下一个行动智能体的子任务，并通过PPO损失函数联合训练，实现动态调整智能体行动顺序。

Result: 在StarCraft多智能体挑战与Multi-Agent MuJoCo基准测试中，AOAD-MAT表现优于现有MAT及其他基线模型。

Conclusion: 动态调整智能体决策顺序能有效提升多智能体强化学习性能，验证了AOAD-MAT模型的有效性。

Abstract: Multi-agent reinforcement learning focuses on training the behaviors of
multiple learning agents that coexist in a shared environment. Recently, MARL
models, such as the Multi-Agent Transformer (MAT) and ACtion dEpendent deep
Q-learning (ACE), have significantly improved performance by leveraging
sequential decision-making processes. Although these models can enhance
performance, they do not explicitly consider the importance of the order in
which agents make decisions. In this paper, we propose an Agent Order of Action
Decisions-MAT (AOAD-MAT), a novel MAT model that considers the order in which
agents make decisions. The proposed model explicitly incorporates the sequence
of action decisions into the learning process, allowing the model to learn and
predict the optimal order of agent actions. The AOAD-MAT model leverages a
Transformer-based actor-critic architecture that dynamically adjusts the
sequence of agent actions. To achieve this, we introduce a novel MARL
architecture that cooperates with a subtask focused on predicting the next
agent to act, integrated into a Proximal Policy Optimization based loss
function to synergistically maximize the advantage of the sequential
decision-making. The proposed method was validated through extensive
experiments on the StarCraft Multi-Agent Challenge and Multi-Agent MuJoCo
benchmarks. The experimental results show that the proposed AOAD-MAT model
outperforms existing MAT and other baseline models, demonstrating the
effectiveness of adjusting the AOAD order in MARL.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [83] [AutoCode: LLMs as Problem Setters for Competitive Programming](https://arxiv.org/abs/2510.12803)
*Shang Zhou,Zihan Zheng,Kaiyuan Liu,Zeyu Shen,Zerui Cheng,Zexing Chen,Hansen He,Jianzhu Yao,Huanzhi Mao,Qiuyang Mang,Tianfu Fu,Beichen Li,Dongruixuan Li,Wenhao Chai,Zhuang Liu,Aleksandra Korolova,Peter Henderson,Natasha Jaques,Pramod Viswanath,Saining Xie,Jingbo Shang*

Main category: cs.SE

TL;DR: AutoCode利用多轮验证生成竞赛级编程题和测试用例，测试一致性达99%，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 编写竞赛编程题需要精确设置约束和测试，对模型能力是极好挑战。

Method: 引入AutoCode，通过多轮验证和交叉验证生成题目、测试用例及解决方案，确保题目质量。

Result: AutoCode测试套件与官方评判一致率接近99%，优于现有81%的方法；能生成Grandmaster级别认可的竞赛质量题目。

Conclusion: AutoCode有效提升自动生成编程题的准确性和质量，具备实战竞赛应用潜力。

Abstract: Writing competitive programming problems is exacting. Authors must: set
constraints, input distributions, and edge cases that rule out shortcuts;
target specific algorithms (e.g., max-flow, dynamic programming, data
structures); and calibrate complexity beyond the reach of most competitors. We
argue that this makes for an ideal test of general large language model
capabilities and study whether they can do this reliably. We introduce
AutoCode, which uses multiple rounds of validation to yield competition-grade
problem statements and test cases. On held-out problems, AutoCode test suites
approach 99% consistency with official judgments, a significant improvement
over current state-of-the-art methods like HardTests, which achieve less than
81%. Furthermore, starting with a random seed problem, AutoCode can create
novel variants with reference and brute-force solutions. By cross-verifying
these generated solutions against test cases, we can further filter out
malformed problems. Our system ensures high correctness, as verified by human
experts. AutoCode successfully produces novel problems judged by
Grandmaster-level (top 0.3%) competitive programmers to be of contest quality.

</details>


### [84] [SpareCodeSearch: Searching for Code Context When You Have No Spare GPU](https://arxiv.org/abs/2510.12948)
*Minh Nguyen*

Main category: cs.SE

TL;DR: 该论文提出在代码语言模型中使用关键词搜索替代语义搜索以检索相关代码上下文，降低计算资源需求，并验证了该方法在代码补全任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前检索增强生成框架依赖语义搜索模块，消耗大量算力，难以在轻量应用如IDE内代码补全中实现。

Method: 用关键词搜索替代传统的语义检索模块，从大规模代码库中检索相关上下文，无需大量GPU资源。

Result: 在Code Context Competition基准测试中，使用关键词搜索的方法在Kotlin和Python代码补全任务中分别达到了0.748和0.725的chRF分数，表现良好。

Conclusion: 关键词搜索足以有效检索代码上下文，能在资源有限的环境下支持优质代码补全，适合轻量级应用场景。

Abstract: Retrieval-Augmented Generation (RAG) frameworks aim to enhance Code Language
Models (CLMs) by including another module for retrieving relevant context to
construct the input prompt. However, these retrieval modules commonly use
semantic search, requiring substantial computational resources for training and
hosting these embedded models, making them infeasible to integrate into
lightweight applications such as in-IDE AI-based code completion. In this
solution paper, we prove that using keyword-search is sufficient to retrieve
relevant and useful code context inside large codebases, without the need for
extensive GPU resources. The usefulness of code contexts found by our solution
is demonstrated through their completion results on the Code Context
Competition's benchmark, reaching 0.748 and 0.725 chRF scores on Kotlin and
Python tracks, respectively.

</details>


### [85] [ADPerf: Investigating and Testing Performance in Autonomous Driving Systems](https://arxiv.org/abs/2510.13078)
*Tri Minh-Triet Pham,Diego Elias Costa,Weiyi Shang,Jinqiu Yang*

Main category: cs.SE

TL;DR: 本文首次全面调查了自动驾驶系统中障碍物检测模块的性能及延迟，开发了ADPerf工具用于生成现实点云数据测试用例，揭示延迟增加对检测和后续模块的负面影响。


<details>
  <summary>Details</summary>
Motivation: 障碍物检测延迟对自动驾驶系统的安全性和有效性至关重要，但目前其性能及对点云数据变化的鲁棒性尚未充分理解。

Method: 通过对两个行业级自动驾驶系统Apollo和Autoware的障碍物检测模块性能进行测量与建模，基于此开发ADPerf工具生成逼真的点云数据测试用例，进行压力测试并分析其对后续轨迹预测模块的影响。

Result: ADPerf成功揭示了3D障碍物检测模块中存在的延迟瓶颈，并展示了延迟增加如何传导影响其它模块，降低系统整体可靠性。

Conclusion: 需要对自动驾驶系统中的障碍物检测组件特别是3D检测模块进行性能测试，以发现和缓解可能导致系统延迟和可靠性下降的瓶颈。

Abstract: Obstacle detection is crucial to the operation of autonomous driving systems,
which rely on multiple sensors, such as cameras and LiDARs, combined with code
logic and deep learning models to detect obstacles for time-sensitive
decisions. Consequently, obstacle detection latency is critical to the safety
and effectiveness of autonomous driving systems. However, the latency of the
obstacle detection module and its resilience to various changes in the LiDAR
point cloud data are not yet fully understood. In this work, we present the
first comprehensive investigation on measuring and modeling the performance of
the obstacle detection modules in two industry-grade autonomous driving
systems, i.e., Apollo and Autoware. Learning from this investigation, we
introduce ADPerf, a tool that aims to generate realistic point cloud data test
cases that can expose increased detection latency. Increasing latency decreases
the availability of the detected obstacles and stresses the capabilities of
subsequent modules in autonomous driving systems, i.e., the modules may be
negatively impacted by the increased latency in obstacle detection.
  We applied ADPerf to stress-test the performance of widely used 3D obstacle
detection modules in autonomous driving systems, as well as the propagation of
such tests on trajectory prediction modules. Our evaluation highlights the need
to conduct performance testing of obstacle detection components, especially 3D
obstacle detection, as they can be a major bottleneck to increased latency of
the autonomous driving system. Such an adverse outcome will also further
propagate to other modules, reducing the overall reliability of autonomous
driving systems.

</details>


### [86] [TRUSTVIS: A Multi-Dimensional Trustworthiness Evaluation Framework for Large Language Models](https://arxiv.org/abs/2510.13106)
*Ruoyu Sun,Da Song,Jiayang Song,Yuheng Huang,Lei Ma*

Main category: cs.SE

TL;DR: 本文提出了TRUSTVIS，一个评估大语言模型可信度的自动化框架，结合参数扰动和多方法投票，配备交互式可视化界面。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在推动自然语言处理进展的同时，其安全性和稳健性方面存在的可信度问题亟需有效评估工具。

Method: TRUSTVIS集成AutoDAN等扰动测试技术，结合多种评估方法的多数投票策略，通过交互式界面直观呈现可信度指标。

Result: 在Vicuna-7b、Llama2-7b和GPT-3.5等模型上的初步测试表明，框架能有效发现模型的安全和稳健性漏洞，用户可通过界面深入探查。

Conclusion: TRUSTVIS有效提升了大语言模型可信度评估的可靠性和可操作性，支持用户针对性改进模型安全性和稳健性。

Abstract: As Large Language Models (LLMs) continue to revolutionize Natural Language
Processing (NLP) applications, critical concerns about their trustworthiness
persist, particularly in safety and robustness. To address these challenges, we
introduce TRUSTVIS, an automated evaluation framework that provides a
comprehensive assessment of LLM trustworthiness. A key feature of our framework
is its interactive user interface, designed to offer intuitive visualizations
of trustworthiness metrics. By integrating well-known perturbation methods like
AutoDAN and employing majority voting across various evaluation methods,
TRUSTVIS not only provides reliable results but also makes complex evaluation
processes accessible to users. Preliminary case studies on models like
Vicuna-7b, Llama2-7b, and GPT-3.5 demonstrate the effectiveness of our
framework in identifying safety and robustness vulnerabilities, while the
interactive interface allows users to explore results in detail, empowering
targeted model improvements. Video Link: https://youtu.be/k1TrBqNVg8g

</details>


### [87] [Isolating Compiler Bugs through Compilation Steps Analysis](https://arxiv.org/abs/2510.13128)
*Yujie Liu,Mingxuan Zhu,Shengyu Cheng,Dan Hao*

Main category: cs.SE

TL;DR: 本文提出了一种名为CompSCAN的新型编译器错误定位技术，通过分析编译过程中的步骤序列，有效提升了定位效果和效率。


<details>
  <summary>Details</summary>
Motivation: 现有编译器错误定位方法主要依赖输入变异，缺乏对编译内部步骤的因果分析，导致定位效果有限。

Method: CompSCAN通过三阶段流程：提取导致失败的编译步骤数组，识别致错步骤并收集对应代码元素，计算并排序代码元素的可疑度，最终输出错误定位结果。

Result: 在185个真实LLVM和GCC错误上，CompSCAN在Top-1/3/5/10位成功定位的错误数量均显著超过两种主流技术ETEM和ODFL，且运行效率更高。

Conclusion: CompSCAN有效利用编译步骤序列信息，显著提升了编译器错误定位的效果和效率，是一种优于现有技术的错误隔离方案。

Abstract: Compilers are essential to software systems, and their bugs can propagate to
dependent software. Ensuring compiler correctness is critical. However,
isolating compiler bugs remains challenging due to the internal complexity of
compiler execution. Existing techniques primarily mutate compilation inputs to
generate passing and failing tests, but often lack causal analysis of internal
steps, limiting their effectiveness.
  To address this limitation, we propose CompSCAN, a novel compiler bug
isolation technique that applies analysis over the sequence of compilation
steps. CompSCAN follows a three-stage process: (1) extracting the array of
compilation steps that leads to the original failure, (2) identifying
bug-causing steps and collecting corresponding compiler code elements, and (3)
calculating suspicious scores for each code element and outputting a suspicious
ranking list as the bug isolation result.
  We evaluate CompSCAN on 185 real-world LLVM and GCC bugs. Results show that
CompSCAN outperforms state-of-the-art techniques in both effectiveness and
efficiency. CompSCAN successfully isolates 50, 85, 100, and 123 bugs within the
Top-1/3/5/10 ranks, respectively. Compared with ETEM and ODFL, two
state-of-the-art compiler bug isolation techniques, CompSCAN achieves relative
improvements of 44.51% / 50.18% / 36.24% / 24.49% over ETEM, and 31.58% /
49.12% / 44.93% / 21.78% over ODFL on those metrics. Moreover, CompSCAN runs
faster on average per bug than both baselines.

</details>


### [88] [GRACE: Globally-Seeded Representation-Aware Cluster-Specific Evolution for Compiler Auto-Tuning](https://arxiv.org/abs/2510.13176)
*Haolin Pan,Chao Zha,Jinyuan Dong,Mingjie Xing,Yanjun Wu*

Main category: cs.SE

TL;DR: GRACE是一个针对编译器自动调优的新框架，通过缩减搜索空间和对程序聚类实现LLVM IR指令数的优化，显著提升优化效果且调优时间极短。


<details>
  <summary>Details</summary>
Motivation: 传统编译器启发式方法面对具体程序时效果有限，迭代编译搜索代价高，机器学习方法泛化能力不足，难以实现高效且通用的程序性能优化。

Method: GRACE利用通路协同和加权评分生成初始候选序列，通过对比学习结合数据增强生成程序嵌入，再基于相似度聚类，最后在聚类内利用进化搜索得到专门化通路集，并在测试时选择并轻量调整最优序列。

Result: GRACE在多个数据集上实现LLVM IR指令数平均约10%的减少，且调优时间低于1秒，性能优于opt -Oz选项的表现。

Conclusion: GRACE框架有效解决了编译器通路选择和排序问题，兼顾优化效果和搜索效率，具备良好泛化能力和实用价值。

Abstract: Compiler pass selection and phase ordering present a significant challenge in
achieving optimal program performance, particularly for objectives like code
size reduction. Standard compiler heuristics offer general applicability but
often yield suboptimal, program-specific results due to their one-size-fits-all
nature. While iterative compilation can find tailored solutions, its
prohibitive search cost limits practical use. Machine learning approaches
promise faster inference but frequently struggle with generalization to unseen
programs. This paper introduces GRACE, a novel framework for compiler
auto-tuning, demonstrated for LLVM IR instruction count optimization. GRACE
effectively curtails the search space by leveraging pass synergies and a
weighted scoring method to generate initial high-quality candidate sequences
and a pass pool. It then employs contrastive learning, using pass
sequence-based data augmentation, to create program embeddings that facilitate
similarity-aware clustering. Evolutionary search within these clusters yields a
coreset of $k$ specialized pass sequences designed for robust generalization to
unseen programs. At test time, GRACE efficiently selects the best coreset
sequence and refines it using lightweight techniques. Experimental results on
seven diverse datasets show that GRACE reduces LLVM IR instruction count by an
average of 10.09% on LLVM 10.0.0 and 10.19% on LLVM 18.1.6 compared to opt -Oz,
while incurring an average tuning time of less than 1s per program,
demonstrating its state-of-the-art performance and practical effectiveness.

</details>


### [89] [Synergy-Guided Compiler Auto-Tuning of Nested LLVM Pass Pipelines](https://arxiv.org/abs/2510.13184)
*Haolin Pan,Jinyuan Dong,Mingjie Xing,Yanjun Wu*

Main category: cs.SE

TL;DR: 本文提出了一个专门针对LLVM新通道管理器设计的自动调优框架，利用结构感知遗传算法生成有效的优化通道序列，在多个基准测试中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有自动调优方法假设通道序列为线性，与LLVM新通道管理器的层次设计不符，导致无法保证生成语法有效的优化管道。

Method: 提出基于形式文法定义嵌套管道空间，利用森林结构表示，开发结构感知遗传算法直接操作森林结构，确保候选解均有效。框架先挖掘通道协同关系指导搜索，后期可选细化阶段探索结构差异带来的性能变化。

Result: 在七个基准测试集和LLVM 18.1.6上，所发现的管道相比标准opt -Oz优化级别，平均指令数减少13.62%。

Conclusion: 该框架能够有效探索复杂且受限的搜索空间，自动生成有效且结构正确的优化通道序列，显著提升程序性能。

Abstract: Compiler optimization relies on sequences of passes to improve program
performance. Selecting and ordering these passes automatically, known as
compiler auto-tuning, is challenging due to the large and complex search space.
Existing approaches generally assume a linear sequence of passes, a model
compatible with legacy compilers but fundamentally misaligned with the
hierarchical design of the LLVM New Pass Manager. This misalignment prevents
them from guaranteeing the generation of syntactically valid optimization
pipelines. In this work, we present a new auto-tuning framework built from the
ground up for the New Pass Manager. We introduce a formal grammar to define the
space of valid nested pipelines and a forest-based data structure for their
native representation. Upon this foundation, we develop a structure-aware
Genetic Algorithm whose operators manipulate these forests directly, ensuring
that all candidate solutions are valid by construction. The framework first
mines synergistic pass relationships to guide the search. An optional
refinement stage further explores subtle performance variations arising from
different valid structural arrangements.
  We evaluate our approach on seven benchmark datasets using LLVM 18.1.6. The
discovered pipelines achieve an average of 13.62% additional instruction count
reduction compared to the standard opt -Oz optimization level, showing that our
framework is capable of navigating this complex, constrained search space to
identify valid and effective pass pipelines.

</details>


### [90] [Towards Richer Challenge Problems for Scientific Computing Correctness](https://arxiv.org/abs/2510.13423)
*Matthew Sottile,Mohit Tekriwal,John Sarracino*

Main category: cs.SE

TL;DR: 本文呼吁通过设计专门的挑战问题，推动形式方法和编程语言验证技术在科学计算正确性方面的发展，以弥合两个领域的认知差距。


<details>
  <summary>Details</summary>
Motivation: 现有的形式方法和编程语言验证技术难以应对科学计算应用的复杂性，且科学计算和形式方法/编程语言社区之间缺乏对可机验证正确性挑战的共识。

Method: 提出专门的挑战问题，并定义与科学计算相关的正确性维度，讨论设计这些挑战问题的指导原则和标准。

Result: 提出了若干科学计算相关的正确性维度，及设计挑战问题的指导准则，为评估科学计算中的正确性提供新的思路。

Conclusion: 通过设计专门的挑战问题，能够促进形式方法和编程语言技术更好地满足科学计算应用的正确性需求，推动两领域的融合与发展。

Abstract: Correctness in scientific computing (SC) is gaining increasing attention in
the formal methods (FM) and programming languages (PL) community. Existing
PL/FM verification techniques struggle with the complexities of realistic SC
applications. Part of the problem is a lack of a common understanding between
the SC and PL/FM communities of machine-verifiable correctness challenges and
dimensions of correctness in SC applications.
  To address this gap, we call for specialized challenge problems to inform the
development and evaluation of FM/PL verification techniques for correctness in
SC. These specialized challenges are intended to augment existing problems
studied by FM/PL researchers for general programs to ensure the needs of SC
applications can be met. We propose several dimensions of correctness relevant
to scientific computing, and discuss some guidelines and criteria for designing
challenge problems to evaluate correctness in scientific computing.

</details>


### [91] [Verifying a Sparse Matrix Algorithm Using Symbolic Execution](https://arxiv.org/abs/2510.13424)
*Alexander C. Wilton*

Main category: cs.SE

TL;DR: 本文提出利用符号执行对科学软件进行测试，以提高验证的严密性，并应用于稀疏矩阵算法。


<details>
  <summary>Details</summary>
Motivation: 科学软件复杂且高度优化，传统测试难以发现其中潜在的细微错误。

Method: 利用符号执行技术编写类似单元测试的测试用例，从而提供更强的验证保证。

Result: 成功将符号执行方法应用于稀疏矩阵算法的测试中。

Conclusion: 符号执行为科学软件测试提供了更有效的验证手段，尤其适用于复杂的算法。

Abstract: Scientific software is, by its very nature, complex. It is mathematical and
highly optimized which makes it prone to subtle bugs not as easily detected by
traditional testing. We outline how symbolic execution can be used to write
tests similar to traditional unit tests while providing stronger verification
guarantees and apply this methodology to a sparse matrix algorithm.

</details>


### [92] [OpenDerisk: An Industrial Framework for AI-Driven SRE, with Design, Implementation, and Case Studies](https://arxiv.org/abs/2510.13561)
*Peng Di,Faqiang Chen,Xiao Bai,Hongjun Yang,Qingfeng Li,Ganglin Wei,Jian Mou,Feng Shi,Keting Chen,Peng Tang,Zhitao Shen,Zheng Li,Wenhui Shi,Junwei Guo,Hang Yu*

Main category: cs.SE

TL;DR: OpenDerisk是一个专为站点可靠性工程（SRE）设计的开源多智能体框架，通过集成诊断合作模型和知识引擎，实现了高效准确的问题解决。


<details>
  <summary>Details</summary>
Motivation: 现代软件复杂度增加给SRE团队带来了沉重负担，现有AI方法缺乏深度因果推理和针对性，不适合SRE的专业工作流程。

Method: 提出OpenDerisk框架，包括诊断原生协作模型、可插拔推理引擎、知识引擎及标准化协议，支持专家智能体协同处理复杂跨领域问题。

Result: 在准确性和效率上显著优于现有方法，并在蚂蚁集团大规模生产环境中应用，支撑3000多日活用户，验证了其工业级扩展性和实际效果。

Conclusion: OpenDerisk有效填补了SRE领域缺乏专业AI工具的空白，具备广泛应用潜力，且已开源供业界使用和改进。

Abstract: The escalating complexity of modern software imposes an unsustainable
operational burden on Site Reliability Engineering (SRE) teams, demanding
AI-driven automation that can emulate expert diagnostic reasoning. Existing
solutions, from traditional AI methods to general-purpose multi-agent systems,
fall short: they either lack deep causal reasoning or are not tailored for the
specialized, investigative workflows unique to SRE. To address this gap, we
present OpenDerisk, a specialized, open-source multi-agent framework
architected for SRE. OpenDerisk integrates a diagnostic-native collaboration
model, a pluggable reasoning engine, a knowledge engine, and a standardized
protocol (MCP) to enable specialist agents to collectively solve complex,
multi-domain problems. Our comprehensive evaluation demonstrates that
OpenDerisk significantly outperforms state-of-the-art baselines in both
accuracy and efficiency. This effectiveness is validated by its large-scale
production deployment at Ant Group, where it serves over 3,000 daily users
across diverse scenarios, confirming its industrial-grade scalability and
practical impact. OpenDerisk is open source and available at
https://github.com/derisk-ai/OpenDerisk/

</details>


### [93] [Auto-repair without test cases: How LLMs fix compilation errors in large industrial embedded code](https://arxiv.org/abs/2510.13575)
*Han Fu,Sigrid Eldh,Kristian Wiklund,Andreas Ermedahl,Philipp Haller,Cyrille Artho*

Main category: cs.SE

TL;DR: 该论文研究利用大型语言模型（LLMs）自动修复工业嵌入式系统的编译错误，提升持续集成（CI）效率。


<details>
  <summary>Details</summary>
Motivation: 工业嵌入式系统硬件与软件的协同开发经常导致CI过程中的编译错误，且非可编译代码缺乏测试用例，使得自动修复困难。

Method: 基于大型语言模型的自动修复方法，收集超过40000个代码提交，评估四种最先进的LLMs在工业CI系统中的表现，并与人工修复效果对比。

Result: LLMs增强的CI系统可修复高达63%的编译错误，其中83%的成功修复被认为合理，且大多数成功修复用时不超过8分钟，显著缩短调试时间。

Conclusion: 利用LLMs自动修复编译错误有效提升工业嵌入式系统的CI效率和质量，减少人工调试时间，具有实际应用价值。

Abstract: The co-development of hardware and software in industrial embedded systems
frequently leads to compilation errors during continuous integration (CI).
Automated repair of such failures is promising, but existing techniques rely on
test cases, which are not available for non-compilable code.
  We employ an automated repair approach for compilation errors driven by large
language models (LLMs). Our study encompasses the collection of more than 40000
commits from the product's source code. We assess the performance of an
industrial CI system enhanced by four state-of-the-art LLMs, comparing their
outcomes with manual corrections provided by human programmers. LLM-equipped CI
systems can resolve up to 63 % of the compilation errors in our baseline
dataset. Among the fixes associated with successful CI builds, 83 % are deemed
reasonable. Moreover, LLMs significantly reduce debugging time, with the
majority of successful cases completed within 8 minutes, compared to hours
typically required for manual debugging.

</details>


### [94] [Property Testing for Ocean Models. Can We Specify It? (Invited Talk)](https://arxiv.org/abs/2510.13692)
*Deepak A. Cherian*

Main category: cs.SE

TL;DR: 本文借鉴性质测试的理念，探讨将地球物理流体动力学理论以性质测试形式应用于海洋数值模型的可行性，以解决模型正确性验证难题。


<details>
  <summary>Details</summary>
Motivation: 目前海洋数值模型缺乏有效的正确性验证方法，‘oracle问题’难以解决。作者借鉴性质测试的思想，尝试用物理理论本身来设计测试准则。

Method: 将地球物理流体动力学中的若干理想化问题转化为性质测试，利用物理规律自然指导测试设计。

Result: 通过具体示例展示了物理性质如何适合作为测试标准，提供了一种新颖的模型校验思路。

Conclusion: 虽然提出的方法尚需进一步验证哪些测试最实用，但该思路为海洋模型正确性检测提供了新的方向。

Abstract: I take inspiration from the property-testing literature, particularly the
work of Prof. John Hughes, and explore how such ideas might be applied to
numerical models of the ocean. Specifically, I ask whether geophysical fluid
dynamics (GFD) theory, expressed as property tests, might be used to address
the oracle problem of testing the correctness of ocean models. I propose that a
number of simple idealized GFD problems can be framed as property tests. These
examples clearly illustrate how physics naturally lends itself to specifying
property tests. Which of these proposed tests might be most feasible and
useful, remains to be seen.

</details>


### [95] [On Pretraining for Project-Level Code Completion](https://arxiv.org/abs/2510.13697)
*Maksim Sapronov,Evgeniy Glukhov*

Main category: cs.SE

TL;DR: 本文研究了不同的代码仓库处理策略对OpenCoder模型上下文学习的影响，通过扩展上下文窗口和使用精选的仓库级数据，模型在长代码场景中表现良好，且较小的数据集也能获得竞争力的性能。


<details>
  <summary>Details</summary>
Motivation: 提升大语言模型在代码完成任务中的上下文感知能力，尤其是在长代码序列处理和资源受限环境下的表现。

Method: 对OpenCoder模型进行仓库级预训练，扩展其上下文窗口到16,384 tokens，使用1B精选仓库数据训练，并比较不同仓库处理策略的效果，特别是调整 rotary positional embedding (RoPE) 的缩放参数。

Result: 尽管数据集规模小于竞争模型，OpenCoder在Long Code Arena基准测试中表现相当，且各种仓库处理技术都带来了强劲性能提升，RoPE缩放参数的调整是关键，文件级训练在原序列长度下依然有效。

Conclusion: 仓库级预训练和上下文扩展显著提升代码生成性能，简单的文件级训练也适用于计算资源有限的环境，促进了对代码完成任务的进一步研究。

Abstract: Repository-level pretraining is commonly used to enable large language models
for code to leverage codebase-wide context. This enhances their ability to
generate accurate and context-aware code completions. In this work, we
investigate how different repository-processing strategies affect in-context
learning in OpenCoder, a 1.5B-parameter model. We extend its context window
from 4,096 to 16,384 tokens by training on additional 1B tokens of curated
repository-level data. Despite relying on a smaller dataset than competing
models (which often use hundreds of billions of tokens), our model achieves
comparable performance on the Long Code Arena benchmark. We find that various
repository-processing techniques yield similarly strong results, with the
primary gain coming from adapting to a new rotary positional embedding (RoPE)
scaling parameter. Finally, we show that a simpler file-level training approach
at the original sequence length remains highly effective, opening up
repository-level code completion research to settings with more constrained
data and compute resources.

</details>
