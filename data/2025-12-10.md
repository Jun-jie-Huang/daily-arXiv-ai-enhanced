<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 19]
- [cs.SE](#cs.SE) [Total: 18]
- [cs.MA](#cs.MA) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Short-Context Dominance: How Much Local Context Natural Language Actually Needs?](https://arxiv.org/abs/2512.08082)
*Vala Vakilian,Zimeng Wang,Ankit Singh Rawat,Christos Thrampoulidis*

Main category: cs.CL

TL;DR: 本文研究短上下文主导假说，即大部分序列只需少量局部前缀即可预测下一token。通过大语言模型测量最小上下文长度（MCL），发现75-80%的长序列只需最多96个token。同时提出无须下一token信息的检测方法DaMCL，用于区分长短上下文序列，并设计解码算法提升长距离相关token的生成，验证了性能提升。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型在预测序列时是否主要依赖短上下文，及如何识别那些需要长上下文才能准确预测的难点，从而提升长距离依赖的处理效果。

Method: 利用大语言模型作为统计工具测量最小上下文长度，提出无需下一token的Distributionally Aware MCL（DaMCL）指标，通过阈值判定检测长短上下文序列，并基于检测结果设计新的解码策略加强长距离相关token预测。

Result: 实验表明75-80%序列只需最多96个token即可准确预测下一token；DaMCL能有效区分长短上下文序列；基于检测的解码算法提高了问答任务及多种模型结构的性能。

Conclusion: 大部分序列预测只需短上下文，且通过DaMCL检测长短序列有效，采用基于检测的解码算法能缓解短上下文偏差，提升模型在问答等任务上的表现。

Abstract: We investigate the short-context dominance hypothesis: that for most sequences, a small local prefix suffices to predict their next tokens. Using large language models as statistical oracles, we measure the minimum context length (MCL) needed to reproduce accurate full-context predictions across datasets with sequences of varying lengths. For sequences with 1-7k tokens from long-context documents, we consistently find that 75-80% require only the last 96 tokens at most. Given the dominance of short-context tokens, we then ask whether it is possible to detect challenging long-context sequences for which a short local prefix does not suffice for prediction. We introduce a practical proxy to MCL, called Distributionally Aware MCL (DaMCL), that does not require knowledge of the actual next-token and is compatible with sampling strategies beyond greedy decoding. Our experiments validate that simple thresholding of the metric defining DaMCL achieves high performance in detecting long vs. short context sequences. Finally, to counter the bias that short-context dominance induces in LLM output distributions, we develop an intuitive decoding algorithm that leverages our detector to identify and boost tokens that are long-range-relevant. Across Q&A tasks and model architectures, we confirm that mitigating the bias improves performance.

</details>


### [2] [Adaptation of Embedding Models to Financial Filings via LLM Distillation](https://arxiv.org/abs/2512.08088)
*Eliot Brenner,Dominic Seyler,Manjunath Hegde,Andrei Simion,Koustuv Dasgupta,Bing Xiang*

Main category: cs.CL

TL;DR: 本文提出一种无监督的教师-学生检索模型训练框架，通过迭代挖掘难例，提高金融领域信息检索性能，效果显著且节省人工成本。


<details>
  <summary>Details</summary>
Motivation: 现有通用嵌入模型无法满足金融等专业领域中对计算成本、响应延迟和领域相关性的高要求，且标注成本高昂，亟需一种自动化、无监督的模型适配方案。

Method: 采用双编码器检索嵌入模型（bi-encoder）并结合教师-学生模型交互，通过迭代挖掘难例进行再训练，逐步优化检索模型权重，将领域知识蒸馏进紧凑的检索器。

Result: 在14种金融文件类型共21,800条查询文档对上，模型在MRR@5和DCG@5指标分别提升27.7%和44.6%，在FinanceBench数据集上的3个文档类别中NDCG有明显改进。

Conclusion: 本文提出了一种可扩展的训练流程，通过无标签语料利用通用检索嵌入模型为基础，显著提升金融领域专业任务的信息检索性能，实现了27.7% MRR@5和44.6% DCG@5的提升，且无需昂贵的人力标注。

Abstract: Despite advances in generative large language models (LLMs), practical application of specialized conversational AI agents remains constrained by computation costs, latency requirements, and the need for precise domain-specific relevance measures. While existing embedding models address the first two constraints, they underperform on information retrieval in specialized domains like finance. This paper introduces a scalable pipeline that trains specialized models from an unlabeled corpus using a general purpose retrieval embedding model as foundation. Our method yields an average of 27.7% improvement in MRR$\texttt{@}$5, 44.6% improvement in mean DCG$\texttt{@}$5 across 14 financial filing types measured over 21,800 query-document pairs, and improved NDCG on 3 of 4 document classes in FinanceBench. We adapt retrieval embeddings (bi-encoder) for RAG, not LLM generators, using LLM-judged relevance to distill domain knowledge into a compact retriever. There are prior works which pair synthetically generated queries with real passages to directly fine-tune the retrieval model. Our pipeline differs from these by introducing interaction between student and teacher models that interleaves retrieval-based mining of hard positive/negative examples from the unlabeled corpus with iterative retraining of the student model's weights using these examples. Each retrieval iteration uses the refined student model to mine the corpus for progressively harder training examples for the subsequent training iteration. The methodology provides a cost-effective solution to bridging the gap between general-purpose models and specialized domains without requiring labor-intensive human annotation.

</details>


### [3] [Segment, Embed, and Align: A Universal Recipe for Aligning Subtitles to Signing](https://arxiv.org/abs/2512.08094)
*Zifan Jiang,Youngjoon Jang,Liliane Momeni,Gül Varol,Sarah Ebling,Andrew Zisserman*

Main category: cs.CL

TL;DR: 本文提出的SEA方法通过分段、嵌入与动态规划实现通用高效的手语视频与字幕对齐，跨语言表现优异，促进手语处理进展。


<details>
  <summary>Details</summary>
Motivation: 此前方法依赖于特定语言或数据集的端到端训练，泛化能力差，亟需通用的对齐框架。

Method: 该方法通过两个预训练模型实现视频分段和文本嵌入，然后利用轻量级动态规划进行对齐。

Result: 在四个手语数据集上，SEA达到了最先进的对齐性能，能够快速生成高质量的平行数据。

Conclusion: SEA方法实现了多语言、多领域的通用字幕与手语视频对齐，性能优异且灵活适用。

Abstract: The goal of this work is to develop a universal approach for aligning subtitles (i.e., spoken language text with corresponding timestamps) to continuous sign language videos. Prior approaches typically rely on end-to-end training tied to a specific language or dataset, which limits their generality. In contrast, our method Segment, Embed, and Align (SEA) provides a single framework that works across multiple languages and domains. SEA leverages two pretrained models: the first to segment a video frame sequence into individual signs and the second to embed the video clip of each sign into a shared latent space with text. Alignment is subsequently performed with a lightweight dynamic programming procedure that runs efficiently on CPUs within a minute, even for hour-long episodes. SEA is flexible and can adapt to a wide range of scenarios, utilizing resources from small lexicons to large continuous corpora. Experiments on four sign language datasets demonstrate state-of-the-art alignment performance, highlighting the potential of SEA to generate high-quality parallel data for advancing sign language processing. SEA's code and models are openly available.

</details>


### [4] [Curriculum Guided Massive Multi Agent System Solving For Robust Long Horizon Tasks](https://arxiv.org/abs/2512.08545)
*Indrajit Kar,Kalathur Chenchu Kishore Kumar*

Main category: cs.CL

TL;DR: 本文提出一种层次化多智能体系统，通过空间课程和置信度驱动的训练策略，提升了对长时程推理任务的表现和效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型和多智能体系统虽能分解复杂任务，但难以应对长时程推理和计算成本激增的问题。

Method: 设计了一个层次化多智能体架构，在64*64的网格中分配推理任务，由一个选择性智能体（oracle）支持，并采用空间课程逐步扩展智能体的操作区域。系统使用负对数似然（Negative Log-Likelihood）作为置信度度量，结合Thompson采样课程管理器，自适应选择训练区域。

Result: 在空间基础的河内塔基准测试中，该方法提高了系统稳定性，减少了oracle的使用，并通过分布式智能体合作增强了长距离推理能力。

Conclusion: 该层次化多智能体架构有效提升了长时程推理任务的稳定性和效能，利用空间课程和置信度度量减少对oracle的依赖，加强智能体间合作，适用于机器人操作和规划等复杂任务。

Abstract: Large Language Models and multi-agent systems have shown promise in decomposing complex tasks, yet they struggle with long-horizon reasoning tasks and escalating computation cost. This work introduces a hierarchical multi-agent architecture that distributes reasoning across a 64*64 grid of lightweight agents, supported by a selective oracle. A spatial curriculum progressively expands the operational region of the grid, ensuring that agents master easier central tasks before tackling harder peripheral ones. To improve reliability, the system integrates Negative Log-Likelihood as a measure of confidence, allowing the curriculum to prioritize regions where agents are both accurate and well calibrated. A Thompson Sampling curriculum manager adaptively chooses training zones based on competence and NLL-driven reward signals. We evaluate the approach on a spatially grounded Tower of Hanoi benchmark, which mirrors the long-horizon structure of many robotic manipulation and planning tasks. Results demonstrate improved stability, reduced oracle usage, and stronger long-range reasoning from distributed agent cooperation.

</details>


### [5] [Universal Adversarial Suffixes Using Calibrated Gumbel-Softmax Relaxation](https://arxiv.org/abs/2512.08123)
*Sampriti Soor,Suklav Ghosh,Arijit Sur*

Main category: cs.CL

TL;DR: 本文提出了一种通过学习短的通用对抗后缀，以大幅降低语言模型分类准确率并具备跨任务和跨模型迁移能力的方法。


<details>
  <summary>Details</summary>
Motivation: 语言模型在零样本或少样本分类中易受对抗性提示破坏，现有方法依赖任务或模型特定的触发词，难以比较且迁移性差。

Method: 通过Gumbel-Softmax松弛学习通用对抗后缀的软形式，并进行离散化推理，训练时最大化标签区域的校准交叉熵，掩盖真实标签以防泄露，并加入熵正则化防止模式崩溃。

Result: 训练出的通用对抗后缀在不同任务和模型间都能有效降低准确率和校准置信度，验证了攻击的一致有效性和跨任务、跨模型的迁移能力。

Conclusion: 学习的通用对抗后缀能有效且稳定地攻破不同语言模型和任务，显示出通用对抗攻击在多样场景下的潜力和必要性。

Abstract: Language models (LMs) are often used as zero-shot or few-shot classifiers by scoring label words, but they remain fragile to adversarial prompts. Prior work typically optimizes task- or model-specific triggers, making results difficult to compare and limiting transferability. We study universal adversarial suffixes: short token sequences (4-10 tokens) that, when appended to any input, broadly reduce accuracy across tasks and models. Our approach learns the suffix in a differentiable "soft" form using Gumbel-Softmax relaxation and then discretizes it for inference. Training maximizes calibrated cross-entropy on the label region while masking gold tokens to prevent trivial leakage, with entropy regularization to avoid collapse. A single suffix trained on one model transfers effectively to others, consistently lowering both accuracy and calibrated confidence. Experiments on sentiment analysis, natural language inference, paraphrase detection, commonsense QA, and physical reasoning with Qwen2-1.5B, Phi-1.5, and TinyLlama-1.1B demonstrate consistent attack effectiveness and transfer across tasks and model families.

</details>


### [6] [Universal Adversarial Suffixes for Language Models Using Reinforcement Learning with Calibrated Reward](https://arxiv.org/abs/2512.08131)
*Sampriti Soor,Suklav Ghosh,Arijit Sur*

Main category: cs.CL

TL;DR: 本文提出利用强化学习训练对抗性后缀，显著提升对抗攻击的稳定性和转移性，验证了方法在多任务多模型上的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于梯度搜索或规则的方法对抗性后缀效果脆弱且往往局限于特定任务或模型，需一种更通用且稳定的攻击方法。

Method: 将攻击后缀视作策略，利用Proximal Policy Optimization强化学习算法进行训练，使用校准的交叉熵作为奖励函数以消除标签偏差并提升转移性。

Result: 在五个多样化的NLP基准数据集和三个不同语言模型上，强化学习训练的后缀持续有效地降低了模型准确率并表现出更好的跨任务和跨模型转移能力。

Conclusion: 通过强化学习训练的对抗性后缀能够稳定降低语言模型的准确率，并且在任务和模型之间的转移效果优于之前的对抗触发方法。

Abstract: Language models are vulnerable to short adversarial suffixes that can reliably alter predictions. Previous works usually find such suffixes with gradient search or rule-based methods, but these are brittle and often tied to a single task or model. In this paper, a reinforcement learning framework is used where the suffix is treated as a policy and trained with Proximal Policy Optimization against a frozen model as a reward oracle. Rewards are shaped using calibrated cross-entropy, removing label bias and aggregating across surface forms to improve transferability. The proposed method is evaluated on five diverse NLP benchmark datasets, covering sentiment, natural language inference, paraphrase, and commonsense reasoning, using three distinct language models: Qwen2-1.5B Instruct, TinyLlama-1.1B Chat, and Phi-1.5. Results show that RL-trained suffixes consistently degrade accuracy and transfer more effectively across tasks and models than previous adversarial triggers of similar genres.

</details>


### [7] [ClinicalTrialsHub: Bridging Registries and Literature for Comprehensive Clinical Trial Access](https://arxiv.org/abs/2512.08193)
*Jiwoo Park,Ruoqi Liu,Avani Jagdale,Andrew Srisuwananukorn,Jing Zhao,Lang Li,Ping Zhang,Sachin Kumar*

Main category: cs.CL

TL;DR: ClinicalTrialsHub整合并结构化ClinicalTrials.gov及PubMed数据，借助大型语言模型提升临床试验信息获取效率和准确性，促进循证医学应用。


<details>
  <summary>Details</summary>
Motivation: 现有ClinicalTrials.gov数据访问受限，缺乏结构化和便捷的查询方式，导致临床试验数据获取困难，限制了循证医学的发展。

Method: 利用GPT-5.1和Gemini-3-Pro等大型语言模型，自动解析全文科研文章，提取结构化临床试验信息，并将用户查询转化为结构化数据库搜索，同时提供基于证据的问答系统。

Result: 平台相比单独使用ClinicalTrials.gov，结构化数据访问量提升83.8%，并通过用户研究和自动评估验证了信息抽取与问答系统的实用性和准确性。

Conclusion: ClinicalTrialsHub显著提升了临床试验数据的结构化访问能力，提高了患者、临床医生、研究人员和决策者的使用便利性，促进了循证医学的发展。

Abstract: We present ClinicalTrialsHub, an interactive search-focused platform that consolidates all data from ClinicalTrials.gov and augments it by automatically extracting and structuring trial-relevant information from PubMed research articles. Our system effectively increases access to structured clinical trial data by 83.8% compared to relying on ClinicalTrials.gov alone, with potential to make access easier for patients, clinicians, researchers, and policymakers, advancing evidence-based medicine. ClinicalTrialsHub uses large language models such as GPT-5.1 and Gemini-3-Pro to enhance accessibility. The platform automatically parses full-text research articles to extract structured trial information, translates user queries into structured database searches, and provides an attributed question-answering system that generates evidence-grounded answers linked to specific source sentences. We demonstrate its utility through a user study involving clinicians, clinical researchers, and PhD students of pharmaceutical sciences and nursing, and a systematic automatic evaluation of its information extraction and question answering capabilities.

</details>


### [8] [Are generative AI text annotations systematically biased?](https://arxiv.org/abs/2512.08404)
*Sjoerd B. Stolwijk,Mark Boukes,Damian Trilling*

Main category: cs.CL

TL;DR: 研究发现GLLMs在注释任务中表现出系统性偏差，虽F1分数较高，但与人工标注及结果存在本质差异。


<details>
  <summary>Details</summary>
Motivation: 探讨大型生成语言模型（GLLM）在注释任务中的偏差问题，验证其与人工注释结果的异同及潜在系统偏差。

Method: 通过使用不同的GLLMs（Llama3.1:8b，Llama3.3:70b，GPT4o，Qwen2.5:72b）结合五种不同提示，对政治内容、互动性、理性、不文明和意识形态五个概念进行注释，并与Boukes(2024)的人工注释进行对比。

Result: GLLMs在F1得分上表现尚可，但在注释的普遍性和下游结果上与人工注释存在显著差异，且模型间相互重叠度高，显示系统性偏差且F1分数不能充分反映偏差程度。

Conclusion: GLLMs在五个概念的注释任务中表现虽有足够的F1分数，但在标注分布和下游结果上与人工标注存在显著差异，且不同模型间的标注相互重叠程度高于与人工标注的重叠，表明存在系统性偏差。

Abstract: This paper investigates bias in GLLM annotations by conceptually replicating manual annotations of Boukes (2024). Using various GLLMs (Llama3.1:8b, Llama3.3:70b, GPT4o, Qwen2.5:72b) in combination with five different prompts for five concepts (political content, interactivity, rationality, incivility, and ideology). We find GLLMs perform adequate in terms of F1 scores, but differ from manual annotations in terms of prevalence, yield substantively different downstream results, and display systematic bias in that they overlap more with each other than with manual annotations. Differences in F1 scores fail to account for the degree of bias.

</details>


### [9] [What Triggers my Model? Contrastive Explanations Inform Gender Choices by Translation Models](https://arxiv.org/abs/2512.08440)
*Janiça Hackenbuchner,Arda Tezcan,Joke Daems*

Main category: cs.CL

TL;DR: 本研究通过显著性归因分析，揭示了翻译模型中性别偏见的成因及其与人类性别感知的关联，强调了利用此信息减少偏见的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前模型中存在性别偏见问题，但研究主要停留在测量偏见层面，缺乏对偏见起源的探索。

Method: 利用性别模糊的自然源数据，采用对比解释方法和显著性归因，分析源句中哪些输入上下文词汇影响模型对目标语言性别的选择，并比较模型归因与人类性别感知的重叠，同时进行语言学分析。

Result: 发现模型在性别决策上与人类的感知存在明显的重叠，并明确了影响模型性别选择的重要上下文词汇。

Conclusion: 理解模型翻译中的性别决策机制对于认识和缓解性别偏见至关重要，应将这些信息用于减少偏见。

Abstract: Interpretability can be implemented as a means to understand decisions taken by (black box) models, such as machine translation (MT) or large language models (LLMs). Yet, research in this area has been limited in relation to a manifested problem in these models: gender bias. With this research, we aim to move away from simply measuring bias to exploring its origins. Working with gender-ambiguous natural source data, this study examines which context, in the form of input tokens in the source sentence, influences (or triggers) the translation model choice of a certain gender inflection in the target language. To analyse this, we use contrastive explanations and compute saliency attribution. We first address the challenge of a lacking scoring threshold and specifically examine different attribution levels of source words on the model gender decisions in the translation. We compare salient source words with human perceptions of gender and demonstrate a noticeable overlap between human perceptions and model attribution. Additionally, we provide a linguistic analysis of salient words. Our work showcases the relevance of understanding model translation decisions in terms of gender, how this compares to human decisions and that this information should be leveraged to mitigate gender bias.

</details>


### [10] [Soft Inductive Bias Approach via Explicit Reasoning Perspectives in Inappropriate Utterance Detection Using Large Language Models](https://arxiv.org/abs/2512.08480)
*Ju-Young Kim,Ji-Hong Park,Se-Yeon Lee,Sujin Park,Gun-Woo Kim*

Main category: cs.CL

TL;DR: 本研究针对在线匿名环境中的不当言论检测，提出一种基于软归纳偏置的推理引导方法，显著提升韩语大规模语言模型检测准确率。


<details>
  <summary>Details</summary>
Motivation: 由于在线匿名环境中不当言论易引发严重社会问题，且现有韩语大规模语言模型在此领域的应用较少，研究旨在探索新的推理引导方法以提高检测准确性。

Method: 提出软归纳偏置方法，明确定义并限制模型的推理视角，通过此方法微调韩语大规模语言模型Kanana-1.5，进行定量与定性评估。

Result: 本文提出了一种软归纳偏置方法，通过明确定义推理视角来引导推理过程，从而提升韩语大规模语言模型（Kanana-1.5）在不当言论检测中的表现。通过微调模型并比较不同训练策略，实验结果显示该方法使模型准确率平均达到87.00%，较传统监督学习提高约3.89%。方法促使模型进行更合理的推理，避免推理误差，实现更精确一致的判断。

Conclusion: 软归纳偏置引导的推理视角能够促进模型做出合理和一致的判断，有效提升不当言论检测的准确率，优于传统监督学习方法。

Abstract: Recent incidents in certain online games and communities, where anonymity is guaranteed, show that unchecked inappropriate remarks frequently escalate into verbal abuse and even criminal behavior, raising significant social concerns. Consequently, there is a growing need for research on techniques that can detect inappropriate utterances within conversational texts to help build a safer communication environment. Although large-scale language models trained on Korean corpora and chain-of-thought reasoning have recently gained attention, research applying these approaches to inappropriate utterance detection remains limited. In this study, we propose a soft inductive bias approach that explicitly defines reasoning perspectives to guide the inference process, thereby promoting rational decision-making and preventing errors that may arise during reasoning. We fine-tune a Korean large language model using the proposed method and conduct both quantitative performance comparisons and qualitative evaluations across different training strategies. Experimental results show that the Kanana-1.5 model achieves an average accuracy of 87.0046, improving by approximately 3.89 percent over standard supervised learning. These findings indicate that the proposed method goes beyond simple knowledge imitation by large language models and enables more precise and consistent judgments through constrained reasoning perspectives, demonstrating its effectiveness for inappropriate utterance detection.

</details>


### [11] [HealthcareNLP: where are we and what is next?](https://arxiv.org/abs/2512.08617)
*Lifeng Han,Paul Rayson,Suzan Verberne,Andrew Moore,Goran Nenadic*

Main category: cs.CL

TL;DR: 该教程系统介绍了医疗健康领域NLP的关键任务与方法，补足现有综述不足，设计了数据、任务和患者三层结构，包含实操，面向多类目标受众。


<details>
  <summary>Details</summary>
Motivation: 现有综述不全面，忽略了隐私保护的数据合成、可解释临床NLP及新兴方法，亟需一个系统且入门友好的教程，帮助多领域受众全面理解医疗NLP。

Method: 通过设计三层层次结构（数据资源、NLP任务评估、患者参与）系统介绍医疗NLP相关技术，包括合成数据生成、命名实体识别、情感分析、解释性AI等，并提供实际操作环节。

Result: 本文主要介绍了医疗健康领域的自然语言处理（HealthcareNLP）应用，涵盖已取得的成就及未来面临的挑战。现有综述常忽视数据合成以解决隐私问题、可解释的临床NLP以及关键方法如信息检索增强生成和神经符号结合等。本文提出了一个包含数据资源层（注释指南、伦理审批、治理、合成数据）、NLP评估层（命名实体识别、关系抽取、情感分析、编码等任务）和患者层（患者参与、健康素养、翻译、简化、摘要及共享决策支持）的三层层次结构，同时提供实操环节，面向医疗NLP从业者、研究者及学生，旨在系统介绍该领域核心内容。

Conclusion: 本教程通过三层结构全面覆盖医疗NLP的核心任务和方法，强调数据合成隐私保护、任务多样化及患者参与，促进HealthcareNLP的研究和应用落地。

Abstract: This proposed tutorial focuses on Healthcare Domain Applications of NLP, what we have achieved around HealthcareNLP, and the challenges that lie ahead for the future. Existing reviews in this domain either overlook some important tasks, such as synthetic data generation for addressing privacy concerns, or explainable clinical NLP for improved integration and implementation, or fail to mention important methodologies, including retrieval augmented generation and the neural symbolic integration of LLMs and KGs. In light of this, the goal of this tutorial is to provide an introductory overview of the most important sub-areas of a patient- and resource-oriented HealthcareNLP, with three layers of hierarchy: data/resource layer: annotation guidelines, ethical approvals, governance, synthetic data; NLP-Eval layer: NLP tasks such as NER, RE, sentiment analysis, and linking/coding with categorised methods, leading to explainable HealthAI; patients layer: Patient Public Involvement and Engagement (PPIE), health literacy, translation, simplification, and summarisation (also NLP tasks), and shared decision-making support. A hands-on session will be included in the tutorial for the audience to use HealthcareNLP applications. The target audience includes NLP practitioners in the healthcare application domain, NLP researchers who are interested in domain applications, healthcare researchers, and students from NLP fields. The type of tutorial is "Introductory to CL/NLP topics (HealthcareNLP)" and the audience does not need prior knowledge to attend this. Tutorial materials: https://github.com/4dpicture/HealthNLP

</details>


### [12] [QSTN: A Modular Framework for Robust Questionnaire Inference with Large Language Models](https://arxiv.org/abs/2512.08646)
*Maximilian Kreutner,Jens Rupprecht,Georg Ahnert,Ahmed Salem,Markus Strohmaier*

Main category: cs.CL

TL;DR: QSTN是一个开源Python框架，帮助研究者用大语言模型生成高质量的问卷响应，降低计算成本，提升研究的可靠性和可重复性。


<details>
  <summary>Details</summary>
Motivation: 支持基于大语言模型的虚拟调查和标注任务，提升问卷响应的准确性及降低计算资源消耗，同时增强实验的可重复性和可靠性。

Method: 开发开源Python框架QSTN，用于系统生成问卷式提示的响应，支持LLM上的虚拟调查和标注任务，并评估问卷展示、提示扰动及响应生成方法。

Result: 通过超过4000万条调查响应的广泛评估，验证了问卷结构和响应生成方法对结果的显著影响，并提供无代码用户界面以便研究者无编程经验即可进行实验。

Conclusion: 问卷结构和响应生成方法显著影响生成的调查响应与人类答案的匹配度，同时能大幅降低计算成本。

Abstract: We introduce QSTN, an open-source Python framework for systematically generating responses from questionnaire-style prompts to support in-silico surveys and annotation tasks with large language models (LLMs). QSTN enables robust evaluation of questionnaire presentation, prompt perturbations, and response generation methods. Our extensive evaluation ($>40 $ million survey responses) shows that question structure and response generation methods have a significant impact on the alignment of generated survey responses with human answers, and can be obtained for a fraction of the compute cost. In addition, we offer a no-code user interface that allows researchers to set up robust experiments with LLMs without coding knowledge. We hope that QSTN will support the reproducibility and reliability of LLM-based research in the future.

</details>


### [13] [An Agentic AI System for Multi-Framework Communication Coding](https://arxiv.org/abs/2512.08659)
*Bohao Yang,Rui Yang,Joshua M. Biro,Haoyuan Wang,Jessica L. Handley,Brianna Richardson,Sophia Bessias,Nicoleta Economou-Zavlanos,Armando D. Bedoya,Monica Agrawal,Michael M. Zavlanos,Anand Chowdhury,Raj M. Ratwani,Kai Sun,Kathryn I. Pollak,Michael J. Pencina,Chuan Hong*

Main category: cs.CL

TL;DR: 本文提出了基于多代理和多框架的临床交流自动标注系统MOSAIC，显著提升了标注一致性和准确性。


<details>
  <summary>Details</summary>
Motivation: 临床交流对患者结果至关重要，但大规模人工标注患者-提供者对话耗时长、标注不一致且难以扩展。现有单任务大语言模型缺乏适应性、解释性和可靠性。

Method: 基于LangGraph架构，设计了MOSAIC系统，包括计划代理、更新代理、注释代理和验证代理，实现代码本选择、数据库更新、检索增强生成和一致性检查。

Result: MOSAIC在风湿病和妇产科领域的测试集中取得总体F1得分0.928，风湿病子集表现最佳（F1=0.962），尤其在患者行为标注中表现优异。消融实验显示MOSAIC优于基线模型。

Conclusion: MOSAIC有效提升了临床对话自动标注的适应性和准确性， outperform传统单任务模型，适用于多领域多框架的临床交流分析。

Abstract: Clinical communication is central to patient outcomes, yet large-scale human annotation of patient-provider conversation remains labor-intensive, inconsistent, and difficult to scale. Existing approaches based on large language models typically rely on single-task models that lack adaptability, interpretability, and reliability, especially when applied across various communication frameworks and clinical domains. In this study, we developed a Multi-framework Structured Agentic AI system for Clinical Communication (MOSAIC), built on a LangGraph-based architecture that orchestrates four core agents, including a Plan Agent for codebook selection and workflow planning, an Update Agent for maintaining up-to-date retrieval databases, a set of Annotation Agents that applies codebook-guided retrieval-augmented generation (RAG) with dynamic few-shot prompting, and a Verification Agent that provides consistency checks and feedback. To evaluate performance, we compared MOSAIC outputs against gold-standard annotations created by trained human coders. We developed and evaluated MOSAIC using 26 gold standard annotated transcripts for training and 50 transcripts for testing, spanning rheumatology and OB/GYN domains. On the test set, MOSAIC achieved an overall F1 score of 0.928. Performance was highest in the Rheumatology subset (F1 = 0.962) and strongest for Patient Behavior (e.g., patients asking questions, expressing preferences, or showing assertiveness). Ablations revealed that MOSAIC outperforms baseline benchmarking.

</details>


### [14] [Automatic Essay Scoring and Feedback Generation in Basque Language Learning](https://arxiv.org/abs/2512.08713)
*Ekhi Azurmendi,Xabier Arregi,Oier Lopez de Lacalle*

Main category: cs.CL

TL;DR: 首个巴斯克语AES及反馈公开数据集及相关模型，Latxa模型在评分和反馈质量上超越主流闭源系统。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏巴斯克语自动作文评分（AES）及反馈生成的公开数据集，限制了低资源语言的相关研究发展。

Method: 收集3200篇巴斯克语作文并由专家标注多个评分维度和反馈，基于RoBERTa-EusCrawl和Latxa模型进行监督微调，设计结合自动一致性指标和专家验证的新反馈评价方法。

Result: 构建了包含3200篇巴斯克语作文及专家评分和详细反馈的数据集，fine-tune了多个开源模型，Latxa模型表现优于GPT-5和Claude Sonnet 4.5，生成的反馈更具教育意义和覆盖更广泛的错误类型。

Conclusion: 基于此数据集和方法，构建了透明、可复现且教育导向的低资源语言NLP研究基准，有助推动巴斯克语及类似语言的自动评分与反馈研究。

Abstract: This paper introduces the first publicly available dataset for Automatic Essay Scoring (AES) and feedback generation in Basque, targeting the CEFR C1 proficiency level. The dataset comprises 3,200 essays from HABE, each annotated by expert evaluators with criterion specific scores covering correctness, richness, coherence, cohesion, and task alignment enriched with detailed feedback and error examples. We fine-tune open-source models, including RoBERTa-EusCrawl and Latxa 8B/70B, for both scoring and explanation generation. Our experiments show that encoder models remain highly reliable for AES, while supervised fine-tuning (SFT) of Latxa significantly enhances performance, surpassing state-of-the-art (SoTA) closed-source systems such as GPT-5 and Claude Sonnet 4.5 in scoring consistency and feedback quality. We also propose a novel evaluation methodology for assessing feedback generation, combining automatic consistency metrics with expert-based validation of extracted learner errors. Results demonstrate that the fine-tuned Latxa model produces criterion-aligned, pedagogically meaningful feedback and identifies a wider range of error types than proprietary models. This resource and benchmark establish a foundation for transparent, reproducible, and educationally grounded NLP research in low-resource languages such as Basque.

</details>


### [15] [Fluent Alignment with Disfluent Judges: Post-training for Lower-resource Languages](https://arxiv.org/abs/2512.08777)
*David Samuel,Lilja Øvrelid,Erik Velldal,Andrey Kutuzov*

Main category: cs.CL

TL;DR: 提出了一种针对低资源语言的后训练方法，通过基于流畅性奖励模型的偏好优化提升语言模型的流利性，特别针对无指令调优数据的情况。


<details>
  <summary>Details</summary>
Motivation: 低资源语言缺乏原生语者数据和能够生成流畅合成数据的语言模型，现有工作主要集中在英语和中文。

Method: 采用一种基于策略的方法进行训练，并与机器翻译数据的监督微调和多语言微调方法进行了比较。

Result: 在挪威博克mål语的案例研究中，使用母语者评估流利性，结果表明基于策略训练方法效果最好，无需高难度获取数据。

Conclusion: 基于策略的后训练方法有效提升低资源语言模型的流利性，优于传统的监督微调和多语种微调方案，避免了对难以获取数据的依赖。

Abstract: We propose a post-training method for lower-resource languages that preserves fluency of language models even when aligned by disfluent reward models. Preference-optimization is now a well-researched topic, but previous work has mostly addressed models for English and Chinese. Lower-resource languages lack both datasets written by native speakers and language models capable of generating fluent synthetic data. Thus, in this work, we focus on developing a fluent preference-aligned language model without any instruction-tuning data in the target language. Our approach uses an on-policy training method, which we compare with two common approaches: supervised finetuning on machine-translated data and multilingual finetuning. We conduct a case study on Norwegian Bokmål and evaluate fluency through native-speaker assessments. The results show that the on-policy aspect is crucial and outperforms the alternatives without relying on any hard-to-obtain data.

</details>


### [16] [A Systematic Evaluation of Preference Aggregation in Federated RLHF for Pluralistic Alignment of LLMs](https://arxiv.org/abs/2512.08786)
*Mahmoud Srewa,Tianyu Zhao,Salma Elmalaki*

Main category: cs.CL

TL;DR: 本文提出在联邦学习环境下，通过自适应聚合策略实现大语言模型对多样化人类偏好的公平且有效对齐。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型(LLMs)在联邦学习(FL)环境中与多样化人类偏好对齐的难题，标准方法难以充分代表不同观点。

Method: 提出一个评估框架，系统评估不同人类偏好聚合策略对对齐质量和公平性的权衡。采用每个组局部生成奖励信号，服务器聚合组级奖励，不访问原始数据。评估传统聚合技术(最小、最大、平均)并引入基于组历史表现动态调整偏好权重的新自适应方案。

Result: 自适应聚合策略在问答任务中利用PPO RLHF管线实验，始终实现更高公平性并保持竞争性对齐分数。

Conclusion: 工作提供了评估多样化群体中大模型行为的稳健方法学及实现真正多元且公平对齐模型的实用方案。

Abstract: This paper addresses the challenge of aligning large language models (LLMs) with diverse human preferences within federated learning (FL) environments, where standard methods often fail to adequately represent diverse viewpoints. We introduce a comprehensive evaluation framework that systematically assesses the trade-off between alignment quality and fairness when using different aggregation strategies for human preferences. In our federated setting, each group locally evaluates rollouts and produces reward signals, and the server aggregates these group-level rewards without accessing any raw data. Specifically, we evaluate standard reward aggregation techniques (min, max, and average) and introduce a novel adaptive scheme that dynamically adjusts preference weights based on a group's historical alignment performance. Our experiments on question-answering (Q/A) tasks using a PPO-based RLHF pipeline demonstrate that our adaptive approach consistently achieves superior fairness while maintaining competitive alignment scores. This work offers a robust methodology for evaluating LLM behavior across diverse populations and provides a practical solution for developing truly pluralistic and fairly aligned models.

</details>


### [17] [Ask, Answer, and Detect: Role-Playing LLMs for Personality Detection with Question-Conditioned Mixture-of-Experts](https://arxiv.org/abs/2512.08814)
*Yifan Lyu,Liang Zhang*

Main category: cs.CL

TL;DR: ROME利用大型语言模型模拟问卷回答，将用户文本转化为心理测量证据，增强监督信号，提升人格检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有人格检测方法受限于标签稀缺和用户语言与心理特征之间抽象语义映射不足，亟需引入心理学知识和丰富监督信号以提升检测准确性。

Method: 基于LLM的角色扮演模拟用户回答心理问卷，利用问题条件下的专家混合模型联合处理帖子与问卷回答，融合问卷答案向量与用户表示，通过多任务学习完成最终人格标签预测。

Result: 提出了ROME框架，通过模拟用户回答心理测量问卷，显著提升了基于文本的人格检测性能，解决了标签稀缺和语义映射不足的问题，在两个真实数据集上优于现有方法，Kaggle数据集提升15.41%。

Conclusion: ROME框架通过引入心理学知识和问卷模拟，丰富了监督信息和语义链条，有效改善了人格检测的性能和解释性。

Abstract: Understanding human personality is crucial for web applications such as personalized recommendation and mental health assessment. Existing studies on personality detection predominantly adopt a "posts -> user vector -> labels" modeling paradigm, which encodes social media posts into user representations for predicting personality labels (e.g., MBTI labels). While recent advances in large language models (LLMs) have improved text encoding capacities, these approaches remain constrained by limited supervision signals due to label scarcity, and under-specified semantic mappings between user language and abstract psychological constructs. We address these challenges by proposing ROME, a novel framework that explicitly injects psychological knowledge into personality detection. Inspired by standardized self-assessment tests, ROME leverages LLMs' role-play capability to simulate user responses to validated psychometric questionnaires. These generated question-level answers transform free-form user posts into interpretable, questionnaire-grounded evidence linking linguistic cues to personality labels, thereby providing rich intermediate supervision to mitigate label scarcity while offering a semantic reasoning chain that guides and simplifies the text-to-personality mapping learning. A question-conditioned Mixture-of-Experts module then jointly routes over post and question representations, learning to answer questionnaire items under explicit supervision. The predicted answers are summarized into an interpretable answer vector and fused with the user representation for final prediction within a multi-task learning framework, where question answering serves as a powerful auxiliary task for personality detection. Extensive experiments on two real-world datasets demonstrate that ROME consistently outperforms state-of-the-art baselines, achieving improvements (15.41% on Kaggle dataset).

</details>


### [18] [Do Depth-Grown Models Overcome the Curse of Depth? An In-Depth Analysis](https://arxiv.org/abs/2512.08819)
*Ferdinand Kapl,Emmanouil Angelis,Tobias Höppe,Kaitlin Maile,Johannes von Oswald,Nino Scherrer,Stefan Bauer*

Main category: cs.CL

TL;DR: 逐渐增长Transformer深度促进了模型深度更有效利用及计算电路形成，提升了推理性能。


<details>
  <summary>Details</summary>
Motivation: 逐渐增加Transformer层数不仅可以降低训练成本，还能提升推理性能，但其机理尚不清楚。

Method: 通过深度分析揭示逐渐中间堆叠的效用，并对MIDAS进行轻量级修改以提升性能。

Result: 通过深度分析，发现逐渐中间堆叠方法更有效利用模型深度，改变了残差流结构，促进了可置换计算块的形成，并通过轻量级改进进一步提升了下游推理表现。

Conclusion: 逐渐增长模型深度有助于克服标准非增长模型中深度利用不足的问题，形成独特的计算回路，从而提升性能。

Abstract: Gradually growing the depth of Transformers during training can not only reduce training cost but also lead to improved reasoning performance, as shown by MIDAS (Saunshi et al., 2024). Thus far, however, a mechanistic understanding of these gains has been missing. In this work, we establish a connection to recent work showing that layers in the second half of non-grown, pre-layernorm Transformers contribute much less to the final output distribution than those in the first half - also known as the Curse of Depth (Sun et al., 2025, Csordás et al., 2025). Using depth-wise analyses, we demonstrate that growth via gradual middle stacking yields more effective utilization of model depth, alters the residual stream structure, and facilitates the formation of permutable computational blocks. In addition, we propose a lightweight modification of MIDAS that yields further improvements in downstream reasoning benchmarks. Overall, this work highlights how the gradual growth of model depth can lead to the formation of distinct computational circuits and overcome the limited depth utilization seen in standard non-grown models.

</details>


### [19] [Toward Faithful Retrieval-Augmented Generation with Sparse Autoencoders](https://arxiv.org/abs/2512.08892)
*Guangzhi Xiong,Zhenghao He,Bohan Liu,Sanchit Sinha,Aidong Zhang*

Main category: cs.CL

TL;DR: 本文提出RAGLens，一种基于稀疏自编码器的轻量级检测器，用于检测基于检索增强生成（RAG）模型的幻觉，提升检测准确率并提供可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG幻觉检测方法要么依赖大规模标注数据训练检测器，要么依赖外部LLM判断，均成本较高；而利用LLM内部表示的现有方法准确率有限，故引入机械解释性技术提升检测效果。

Method: 采用稀疏自编码器分离LLM的内部激活特征，结合基于信息的特征选择和加性特征建模，构建轻量级检测器RAGLens，利用LLM内部表示检测幻觉生成。

Result: RAGLens相较于现有方法表现出更优的幻觉检测准确性，并且能够解释检测结果，支持实现后期的幻觉修正，揭示了幻觉信号在LLM内部的分布特征。

Conclusion: RAGLens利用稀疏自编码器成功识别RAG模型中的幻觉特征，实现了领先的幻觉检测性能，同时提供决策的可解释性，有助于后续修正不真实生成。

Abstract: Retrieval-Augmented Generation (RAG) improves the factuality of large language models (LLMs) by grounding outputs in retrieved evidence, but faithfulness failures, where generations contradict or extend beyond the provided sources, remain a critical challenge. Existing hallucination detection methods for RAG often rely either on large-scale detector training, which requires substantial annotated data, or on querying external LLM judges, which leads to high inference costs. Although some approaches attempt to leverage internal representations of LLMs for hallucination detection, their accuracy remains limited. Motivated by recent advances in mechanistic interpretability, we employ sparse autoencoders (SAEs) to disentangle internal activations, successfully identifying features that are specifically triggered during RAG hallucinations. Building on a systematic pipeline of information-based feature selection and additive feature modeling, we introduce RAGLens, a lightweight hallucination detector that accurately flags unfaithful RAG outputs using LLM internal representations. RAGLens not only achieves superior detection performance compared to existing methods, but also provides interpretable rationales for its decisions, enabling effective post-hoc mitigation of unfaithful RAG. Finally, we justify our design choices and reveal new insights into the distribution of hallucination-related signals within LLMs. The code is available at https://github.com/Teddy-XiongGZ/RAGLens.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [20] [CFD-copilot: leveraging domain-adapted large language model and model context protocol to enhance simulation automation](https://arxiv.org/abs/2512.07917)
*Zhehao Dong,Shanghai Du,Zhen Lu,Yue Yang*

Main category: cs.SE

TL;DR: CFD-copilot利用微调语言模型和模块化系统，实现了从设置到后处理的自然语言指导CFD自动化，提高了仿真效率和准确性。


<details>
  <summary>Details</summary>
Motivation: CFD仿真配置复杂，需要专业知识，限制了非专业人员的使用。尽管大型语言模型在科学任务自动化中有潜力，但直接应用于完整CFD流程仍存在挑战。

Method: 提出CFD-copilot框架，利用微调的大型语言模型将用户自然语言描述直接转化为可执行的CFD配置。通过多代理系统结合仿真执行、自动纠错及结果分析。利用模型上下文协议（MCP）解耦语言模型推理与外部工具，实现模块化并提升后处理自动化。

Result: 在NACA 0012翼型和三元素30P-30N翼型基准测试中，表明领域特化调整和MCP模块设计提升了LLM驱动工程工作流程的可靠性和效率。

Conclusion: CFD-copilot框架通过领域微调的LLM和模块化设计，有效促进了自然语言驱动的CFD仿真全流程自动化，提高了非专业人员的可用性和工程效率。

Abstract: Configuring computational fluid dynamics (CFD) simulations requires significant expertise in physics modeling and numerical methods, posing a barrier to non-specialists. Although automating scientific tasks with large language models (LLMs) has attracted attention, applying them to the complete, end-to-end CFD workflow remains a challenge due to its stringent domain-specific requirements. We introduce CFD-copilot, a domain-specialized LLM framework designed to facilitate natural language-driven CFD simulation from setup to post-processing. The framework employs a fine-tuned LLM to directly translate user descriptions into executable CFD setups. A multi-agent system integrates the LLM with simulation execution, automatic error correction, and result analysis. For post-processing, the framework utilizes the model context protocol (MCP), an open standard that decouples LLM reasoning from external tool execution. This modular design allows the LLM to interact with numerous specialized post-processing functions through a unified and scalable interface, improving the automation of data extraction and analysis. The framework was evaluated on benchmarks including the NACA~0012 airfoil and the three-element 30P-30N airfoil. The results indicate that domain-specific adaptation and the incorporation of the MCP jointly enhance the reliability and efficiency of LLM-driven engineering workflows.

</details>


### [21] [DeepCode: Open Agentic Coding](https://arxiv.org/abs/2512.07921)
*Zongwei Li,Zhonghang Li,Zirui Guo,Xubin Ren,Chao Huang*

Main category: cs.SE

TL;DR: DeepCode通过优化信息流管理，有效解决了文档到代码合成的核心瓶颈，实现了超越人类专家的自动科学论文代码复现。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在实现高保真度的文档到代码合成（如科学论文到代码）方面存在信息过载与上下文瓶颈的矛盾挑战。

Method: 将代码库综合视为信道优化问题，采用蓝图蒸馏压缩源信息、状态化代码记忆构建索引、基于检索增强生成的条件知识注入以及闭环错误修正四个操作。

Result: 提出了DeepCode框架，通过四种信息操作实现任务相关信号最大化，显著超过现有商业代理和顶级人类专家，在PaperBench基准测试中取得最先进性能。

Conclusion: DeepCode成功将论文规格系统化转化为生产级实现，奠定了自动化科学复现的新基础，有助于加速科研评估与发现。

Abstract: Recent advances in large language models (LLMs) have given rise to powerful coding agents, making it possible for code assistants to evolve into code engineers. However, existing methods still face significant challenges in achieving high-fidelity document-to-codebase synthesis--such as scientific papers to code--primarily due to a fundamental conflict between information overload and the context bottlenecks of LLMs. In this work, we introduce DeepCode, a fully autonomous framework that fundamentally addresses this challenge through principled information-flow management. By treating repository synthesis as a channel optimization problem, DeepCode seamlessly orchestrates four information operations to maximize task-relevant signals under finite context budgets: source compression via blueprint distillation, structured indexing using stateful code memory, conditional knowledge injection via retrieval-augmented generation, and closed-loop error correction. Extensive evaluations on the PaperBench benchmark demonstrate that DeepCode achieves state-of-the-art performance, decisively outperforming leading commercial agents such as Cursor and Claude Code, and crucially, surpassing PhD-level human experts from top institutes on key reproduction metrics. By systematically transforming paper specifications into production-grade implementations comparable to human expert quality, this work establishes new foundations for autonomous scientific reproduction that can accelerate research evaluation and discovery.

</details>


### [22] [An Empirical Framework for Evaluating Semantic Preservation Using Hugging Face](https://arxiv.org/abs/2512.07983)
*Nan Jia,Anita Raja,Raffi Khatchadourian*

Main category: cs.SE

TL;DR: 本文提出了一个基于HuggingFace模型演化数据的实证框架，用于评估学习驱动软件系统的语义保持，构建了大规模数据集和评估流水线，揭示了语义漂移和常见重构模式，促进机器学习系统的可信性。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习成为高自主系统的核心部分，确保学习驱动的软件系统（LESS）的可信性变得尤为重要，但由于ML的非确定性和运行时语义的复杂性，传统的软件重构方法难以适用。

Method: 本文提出了一个实证框架，通过从HuggingFace挖掘模型演化数据，提取提交历史、模型卡和性能指标，建立基线，跟踪跨版本的性能变化，并通过提交信息分析发现常见的重构模式。

Result: 构建了一个大规模的模型演化数据集，开发了一个语义保持评估流水线，并通过三个领域的案例研究展示了语义漂移的检测和实际应用。

Conclusion: 本文的工作为定义社区认可的语义保持边界提供了基础，推动了更可维护、更可信的机器学习系统的发展。

Abstract: As machine learning (ML) becomes an integral part of high-autonomy systems, it is critical to ensure the trustworthiness of learning-enabled software systems (LESS). Yet, the nondeterministic and run-time-defined semantics of ML complicate traditional software refactoring. We define semantic preservation in LESS as the property that optimizations of intelligent components do not alter the system's overall functional behavior. This paper introduces an empirical framework to evaluate semantic preservation in LESS by mining model evolution data from HuggingFace. We extract commit histories, $\textit{Model Cards}$, and performance metrics from a large number of models. To establish baselines, we conducted case studies in three domains, tracing performance changes across versions. Our analysis demonstrates how $\textit{semantic drift}$ can be detected via evaluation metrics across commits and reveals common refactoring patterns based on commit message analysis. Although API constraints limited the possibility of estimating a full-scale threshold, our pipeline offers a foundation for defining community-accepted boundaries for semantic preservation. Our contributions include: (1) a large-scale dataset of ML model evolution, curated from 1.7 million Hugging Face entries via a reproducible pipeline using the native HF hub API, (2) a practical pipeline for the evaluation of semantic preservation for a subset of 536 models and 4000+ metrics and (3) empirical case studies illustrating semantic drift in practice. Together, these contributions advance the foundations for more maintainable and trustworthy ML systems.

</details>


### [23] [A Gray Literature Study on Fairness Requirements in AI-enabled Software Engineering](https://arxiv.org/abs/2512.07990)
*Thanh Nguyen,Chaima Boufaied,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: 本文回顾了AI系统中公平性需求及其管理，揭示公平性问题广泛且影响深远，呼吁将公平性纳入AI开发全过程。


<details>
  <summary>Details</summary>
Motivation: 当前AI应用普遍关注效果指标，如F1分数，而对公平性的关注不足，本文旨在填补这一研究空白。

Method: 通过对现有灰色文献的系统回顾，分析公平性需求的定义、管理实践及其违反的原因和后果。

Result: 发现公平性通常定义为在不同人口统计和社会属性间的非歧视和平等对待，公平性管理实践分布于软件生命周期不同阶段，公平性违反原因多样且后果严重。

Conclusion: 本文强调在人工智能软件开发中必须重视公平性，与效果性同等重要。

Abstract: Today, with the growing obsession with applying Artificial Intelligence (AI), particularly Machine Learning (ML), to software across various contexts, much of the focus has been on the effectiveness of AI models, often measured through common metrics such as F1- score, while fairness receives relatively little attention. This paper presents a review of existing gray literature, examining fairness requirements in AI context, with a focus on how they are defined across various application domains, managed throughout the Software Development Life Cycle (SDLC), and the causes, as well as the corresponding consequences of their violation by AI models. Our gray literature investigation shows various definitions of fairness requirements in AI systems, commonly emphasizing non-discrimination and equal treatment across different demographic and social attributes. Fairness requirement management practices vary across the SDLC, particularly in model training and bias mitigation, fairness monitoring and evaluation, and data handling practices. Fairness requirement violations are frequently linked, but not limited, to data representation bias, algorithmic and model design bias, human judgment, and evaluation and transparency gaps. The corresponding consequences include harm in a broad sense, encompassing specific professional and societal impacts as key examples, stereotype reinforcement, data and privacy risks, and loss of trust and legitimacy in AI-supported decisions. These findings emphasize the need for consistent frameworks and practices to integrate fairness into AI software, paying as much attention to fairness as to effectiveness.

</details>


### [24] [What Pulls the Strings? Understanding the Characteristics and Role of Argumentation in Open-Source Software Usability Discussions](https://arxiv.org/abs/2512.08032)
*Arghavan Sanei,Chaima Amiri,Atefeh Shokrizadeh,Jinghui Cheng*

Main category: cs.SE

TL;DR: 本文通过分析五个开源项目中的论证话语，揭示了开源软件可用性讨论中论证质量差异及其对参与者行为的影响，旨在帮助提升开源软件的可用性。


<details>
  <summary>Details</summary>
Motivation: 开源软件的可用性常被忽视，且有关可用性的讨论中论证话语特点未知，导致难以有效支持讨论参与者。

Method: 对五个开源软件项目中的论证话语及其质量进行了综合分析。

Result: 发现可用性讨论主要由论证驱动，但论证质量参差不齐，问题评论的论证质量低于问题帖，暗示社区缺乏集体智慧。论证话语和质量对参与者后续行为有不同影响。

Conclusion: 研究为开源软件利益相关者提供了构建更有效论证的方法，有助于提升开源软件可用性，同时对其他分布式协作社区的研究也有借鉴意义。

Abstract: The usability of open-source software (OSS) is important but frequently overlooked in favor of technical and functional complexity. Argumentation can be a pivotal device for diverse stakeholders in OSS usability discussions to express opinions and persuade others. However, the characteristics of argument discourse in those discussions remain unknown, resulting in difficulties in providing effective support for discussion participants. We address this through a comprehensive analysis of argument discourse and quality in five OSS projects. Our results indicated that usability discussions are predominantly argument-driven, although their qualities vary. Issue comments exhibit lower-quality arguments than the issue posts, suggesting a shortage of collective intelligence about usability in OSS communities. Moreover, argument discourse and quality have various impacts on the subsequent behavior of participants. Overall, this research offers insights to help OSS stakeholders build more effective arguments and eventually improve OSS usability. These insights can also inform studies about other distributed collaborative communities.

</details>


### [25] [Secure or Suspect? Investigating Package Hallucinations of Shell Command in Original and Quantized LLMs](https://arxiv.org/abs/2512.08213)
*Md Nazmul Haque,Elizabeth Lin,Lawrence Arkoh,Biruk Tadesse,Bowen Xu*

Main category: cs.SE

TL;DR: 量化降低了LLM生成Go依赖的准确性和安全性，尤其是4位量化，需警惕虚假依赖和安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生成代码依赖时存在虚假包名和安全漏洞风险；量化技术虽然降低推理成本但其对依赖正确性与安全性的影响尚不明。

Method: 针对Go语言，采用不同大小的Qwen模型，在全精度、8位和4位量化下，测试三个数据集，系统研究量化对包虚假率和漏洞风险的影响。

Result: 量化显著提高了包虚假率，4位量化模型表现最差；且即使生成正确依赖，漏洞率也随精度降低而上升；虚假包名多为类似真实URL路径的GitHub或golang.org仓库。

Conclusion: 量化虽降低模型推理成本但会带来严重依赖虚假和安全漏洞风险，部署量化LLM生成代码和依赖时需审慎权衡。

Abstract: Large Language Models for code (LLMs4Code) are increasingly used to generate software artifacts, including library and package recommendations in languages such as Go. However, recent evidence shows that LLMs frequently hallucinate package names or generate dependencies containing known security vulnerabilities, posing significant risks to developers and downstream software supply chains. At the same time, quantization has become a widely adopted technique to reduce inference cost and enable deployment of LLMs on resource-constrained environments. Despite its popularity, little is known about how quantization affects the correctness and security of LLM-generated software dependencies while generating shell commands for package installation.
  In this work, we conduct the first systematic empirical study of the impact of quantization on package hallucination and vulnerability risks in LLM-generated Go packages. We evaluate five Qwen model sizes under full-precision, 8-bit, and 4-bit quantization across three datasets (SO, MBPP, and paraphrase). Our results show that quantization substantially increases the package hallucination rate (PHR), with 4-bit models exhibiting the most severe degradation. We further find that even among the correctly generated packages, the vulnerability presence rate (VPR) rises as precision decreases, indicating elevated security risk in lower-precision models. Finally, our analysis of hallucinated outputs reveals that most fabricated packages resemble realistic URL-based Go module paths, such as most commonly malformed or non-existent GitHub and golang.org repositories, highlighting a systematic pattern in how LLMs hallucinate dependencies. Overall, our findings provide actionable insights into the reliability and security implications of deploying quantized LLMs for code generation and dependency recommendation.

</details>


### [26] [Migrating QAOA from Qiskit 1.x to 2.x: An experience report](https://arxiv.org/abs/2512.08245)
*Julien Cardinal,Imen Benzarti,Ghizlane El boussaidi,Christophe Pere*

Main category: cs.SE

TL;DR: 框架迁移中隐藏采样参数对量子算法性能影响重大，需合理设置以保证结果准确和可复现。


<details>
  <summary>Details</summary>
Motivation: 量子算法框架迭代更新带来的行为细微变化可能影响算法准确率和可复现性，亟需明确这些影响因素。

Method: 以QAOA算法为例，系统性分析不同Qiskit版本（v1和v2）中默认采样次数差异导致结果差异的根本原因。

Result: 发现v2默认采样次数过低导致概率分布覆盖率不足，从而影响结果，通过增加采样次数可恢复原有准确性。

Conclusion: 量子算法在不同框架之间迁移时，隐藏参数（如采样预算）对算法性能有显著影响，且可能导致结果差异巨大。

Abstract: Migrating quantum algorithms across evolving frameworks introduces subtle behavioral changes that affect accuracy and reproducibility. This paper reports our experience converting the Quantum Approximate Optimization Algorithm (QAOA) from Qiskit Algorithms with Qiskit 1.x (v1 primitives) to a custom implementation using Qiskit 2.x (v2 primitives). Despite identical circuits, optimizers, and Hamiltonians, the new version produced drastically different results. A systematic analysis revealed the root cause: the sampling budget -- the number of circuit executions (shots) per iteration. The library's implicit use of unlimited shots yielded dense probability distributions, whereas the v2 default of 10 000 shots captured only 23% of the state space. Increasing shots to 250 000 restored library-level accuracy. This study highlights how hidden parameters at the quantum-classical interaction level can dominate hybrid algorithm performance and provides actionable recommendations for developers and framework designers to ensure reproducible results in quantum software migration.

</details>


### [27] [Token Sugar: Making Source Code Sweeter for LLMs through Token-Efficient Shorthand](https://arxiv.org/abs/2512.08266)
*Zhensu Sun,Chengran Yang,Xiaoning Du,Zhou Yang,Li Li,David Lo*

Main category: cs.SE

TL;DR: 本文提出Token Sugar，通过用简洁、高频的代码简写替换冗长代码模式，减少代码中的token数量，降低LLM推理成本，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 编程语言固有的冗长性导致代码token数量激增，推理成本高昂，现有方法仅做语法层面简化，尚未充分挖掘语义层面优化空间。

Method: 通过从代码库挖掘高频、token密集模式，设计独特的简写并在LLM预训练阶段集成这些代码转换。

Result: 获得799个代码模式与简写对，源码token数最高降低15.1%，生成token数降低最高11.2%，且模型在Pass@1指标上表现与基线相当。

Conclusion: Token Sugar有效降低了代码token数，在节省推理成本的同时，模型性能保持不变。

Abstract: Large language models (LLMs) have shown exceptional performance in code generation and understanding tasks, yet their high computational costs hinder broader adoption. One important factor is the inherent verbosity of programming languages, such as unnecessary formatting elements and lengthy boilerplate code. This leads to inflated token counts in both input and generated outputs, which increases inference costs and slows down the generation process. Prior work improves this through simplifying programming language grammar, reducing token usage across both code understanding and generation tasks. However, it is confined to syntactic transformations, leaving significant opportunities for token reduction unrealized at the semantic level.
  In this work, we propose Token Sugar, a concept that replaces frequent and verbose code patterns with reversible, token-efficient shorthand in the source code. To realize this concept in practice, we designed a systematic solution that mines high-frequency, token-heavy patterns from a code corpus, maps each to a unique shorthand, and integrates them into LLM pretraining via code transformation. With this solution, we obtain 799 (code pattern, shorthand) pairs, which can reduce up to 15.1% token count in the source code and is complementary to existing syntax-focused methods. We further trained three widely used LLMs on Token Sugar-augmented data. Experimental results show that these models not only achieve significant token savings (up to 11.2% reduction) during generation but also maintain near-identical Pass@1 scores compared to baselines trained on unprocessed code.

</details>


### [28] [FedLAD: A Modular and Adaptive Testbed for Federated Log Anomaly Detection](https://arxiv.org/abs/2512.08277)
*Yihan Liao,Jacky Keung,Zhenyu Mao,Jingyu Zhang,Jialong Li*

Main category: cs.SE

TL;DR: FedLAD是一个专为联邦学习环境设计的日志异常检测统一平台，解决了模型训练中的隐私和分散问题，推动了该领域研究的发展。


<details>
  <summary>Details</summary>
Motivation: 传统的日志异常检测方法依赖集中式训练，受限于隐私和系统日志的分散性，难以实际应用。联邦学习作为一种替代方案缺乏专门针对日志异常检测的测试平台。

Method: 设计并实现FedLAD平台，支持多种日志异常检测模型的插件式集成，结合验证日志、自我参数调整和自适应策略控制以增强系统性能和适应性。

Result: 提出FedLAD平台，实现了在联邦学习约束下训练和评估日志异常检测模型，支持多模型、多数据集及多聚合策略的集成，并具备自我监控、自我配置和自适应策略控制功能。

Conclusion: FedLAD有效连接了联邦学习框架与日志异常检测需求，提供了可复现和可扩展的实验平台，为未来相关研究奠定基础。

Abstract: Log-based anomaly detection (LAD) is critical for ensuring the reliability of large-scale distributed systems. However, most existing LAD approaches assume centralized training, which is often impractical due to privacy constraints and the decentralized nature of system logs. While federated learning (FL) offers a promising alternative, there is a lack of dedicated testbeds tailored to the needs of LAD in federated settings. To address this, we present FedLAD, a unified platform for training and evaluating LAD models under FL constraints. FedLAD supports plug-and-play integration of diverse LAD models, benchmark datasets, and aggregation strategies, while offering runtime support for validation logging (self-monitoring), parameter tuning (self-configuration), and adaptive strategy control (self-adaptation). By enabling reproducible and scalable experimentation, FedLAD bridges the gap between FL frameworks and LAD requirements, providing a solid foundation for future research. Project code is publicly available at: https://github.com/AA-cityu/FedLAD.

</details>


### [29] [Empowering smart app development with SolidGPT: an edge-cloud hybrid AI agent framework](https://arxiv.org/abs/2512.08286)
*Liao Hu,Qiteng Wu,Ruoyu Qi*

Main category: cs.SE

TL;DR: SolidGPT是一个开源的边缘云混合开发助手，集成了语义搜索、自动化工作流和隐私保护，提升开发效率。


<details>
  <summary>Details</summary>
Motivation: 解决云端模型数据暴露和延迟问题，以及本地模型无法完整理解项目上下文的矛盾，提升开发效率和数据隐私。

Method: 基于GitHub构建的边缘云混合架构，支持交互式代码查询、自动生成项目文档和任务板，支持本地运行和可选调用LLM API。

Result: 实现了语义丰富的代码导航，自动化项目文档与任务管理，并且保障代码和数据本地私密，支持开发工作流深度集成。

Conclusion: SolidGPT通过语义丰富的代码导航、集成的文档和任务管理及隐私优先的设计，提升了开发者生产力，适合智能移动和软件开发环境。

Abstract: The integration of Large Language Models (LLMs) into mobile and software development workflows faces a persistent tension among three demands: semantic awareness, developer productivity, and data privacy. Traditional cloud-based tools offer strong reasoning but risk data exposure and latency, while on-device solutions lack full-context understanding across codebase and developer tooling. We introduce SolidGPT, an open-source, edge-cloud hybrid developer assistant built on GitHub, designed to enhance code and workspace semantic search. SolidGPT enables developers to: talk to your codebase: interactively query code and project structure, discovering the right methods and modules without manual searching. Automate software project workflows: generate PRDs, task breakdowns, Kanban boards, and even scaffold web app beginnings, with deep integration via VSCode and Notion. Configure private, extensible agents: onboard private code folders (up to approximately 500 files), connect Notion, customize AI agent personas via embedding and in-context training, and deploy via Docker, CLI, or VSCode extension. In practice, SolidGPT empowers developer productivity through: Semantic-rich code navigation: no more hunting through files or wondering where a feature lives. Integrated documentation and task management: seamlessly sync generated PRD content and task boards into developer workflows. Privacy-first design: running locally via Docker or VSCode, with full control over code and data, while optionally reaching out to LLM APIs as needed. By combining interactive code querying, automated project scaffolding, and human-AI collaboration, SolidGPT provides a practical, privacy-respecting edge assistant that accelerates real-world development workflows, ideal for intelligent mobile and software engineering contexts.

</details>


### [30] [Measuring Agile Agreement: Development and Validation of the Manifesto and Principle Scales](https://arxiv.org/abs/2512.08461)
*Nicolas Matton,Anthony Simonofski,Marie-Ange Remiche,Benoît Vanderose*

Main category: cs.SE

TL;DR: 本文开发并验证了两种评估个人敏捷认同的量表，分别评估对敏捷宣言高层价值观与具体实践的认同，填补了此领域的测量空白。


<details>
  <summary>Details</summary>
Motivation: 现有研究未能区分对敏捷宣言抽象价值观和具体日常实践的认可，导致对个人“敏捷认同”的测量模糊且具有挑战性。

Method: 设计并验证了两种测量工具：Manifesto Agreement Scale (MAS) 和 Principle Agreement Scale (PAS)，包括问卷设计、条目创建和筛选，并采用多种统计方法进行效度和一致性验证。

Result: 两种量表均具有良好的内部一致性和结构效度，相关分析显示两者中度相关但不等同，分别捕捉敏捷认同的不同维度。

Conclusion: 该研究提供了两种经过验证的公开量表，为细化个人与敏捷匹配度的测量提供了工具，有助于更深入理解敏捷认同的多层面内涵。

Abstract: While the importance of human factors in agile software development is widely acknowledged, the measurement of an individual's "agile agreement" remains an ill-defined and challenging area. A key limitation in existing research is the failure to distinguish between agreement with the abstract, high-level values of the Agile Manifesto and agreement with the concrete, day-to-day practices derived from the 12 Principles. This paper addresses this methodological gap by presenting the design and validation of two distinct instruments: the novel Manifesto Agreement Scale (MAS), and the Principle Agreement Scale (PAS), which is a systematic adaptation and refinement of a prior instrument.
  We detail the systematic process of item creation and selection, survey design, and validation. The results demonstrate that both scales possess important internal consistency and construct validity. A convergence and divergence analysis, including Proportional Odds Logistic Regression, a Bland-Altman plot, and an Intraclass Correlation Coefficient (ICC), reveals that while the two scales are moderately correlated, they are not interchangeable and capture distinct dimensions of agile agreement. The primary contribution of this work is a pair of publicly available instruments, validated within a specific demographic of Belgian IT professionals. These scales represent a critical initial step toward facilitating a more nuanced measurement of agile agreement, distinguishing agile agreement across various levels of perception and aiding in a more refined interpretation of person-agile fit.

</details>


### [31] [Measuring Computer Science Enthusiasm: A Questionnaire-Based Analysis of Age and Gender Effects on Students' Interest](https://arxiv.org/abs/2512.08472)
*Kai Marquardt,Robert Hanak,Anne Koziolek,Lucia Happe*

Main category: cs.SE

TL;DR: 本研究揭示年龄对青少年计算机科学兴趣发展的重要影响，强调教学应针对不同发展阶段调整，短期活动可在晚期增强兴趣。


<details>
  <summary>Details</summary>
Motivation: 探究不同年龄和性别对青少年计算机科学兴趣的影响，挑战早期暴露是持续兴趣的主要途径的传统观念。

Method: 基于兴趣的个体-对象理论（POI），设计了一个理论基础的问卷在计算机科学干预前后测量学生的热情水平；结合超过400名学生的在线课程数据，分析年龄和性别对热情的影响。

Result: 发现早期青春期特别是女孩的热情显著下降，兴趣轨迹在不同年龄组差异明显；年龄比性别更关键；年长学生虽起始态度较低，但干预后兴趣提升最大。

Conclusion: 计算机科学教育应采用动态、年龄敏感的框架，教学策略应与学生发展阶段相适应，短期干预同样可有效促进晚期兴趣激活。

Abstract: This study offers new insights into students' interest in computer science (CS) education by disentangling the distinct effects of age and gender across a diverse adolescent sample. Grounded in the person-object theory of interest (POI), we conceptualize enthusiasm as a short-term, activating expression of interest that combines positive affect, perceived relevance, and intention to re-engage. Experiencing such enthusiasm can temporarily shift CS attitudes and strengthen future engagement intentions, making it a valuable lens for evaluating brief outreach activities. To capture these dynamics, we developed a theoretically grounded questionnaire for pre-post assessment of the enthusiasm potential of CS interventions. Using data from more than 400 students participating in online CS courses, we examined age- and gender-related patterns in enthusiasm. The findings challenge the prevailing belief that early exposure is the primary pathway to sustained interest in CS. Instead, we identify a marked decline in enthusiasm during early adolescence, particularly among girls, alongside substantial variability in interest trajectories across age groups. Crucially, our analyses reveal that age is a more decisive factor than gender in shaping interest development and uncover key developmental breakpoints. Despite starting with lower baseline attitudes, older students showed the largest positive changes following the intervention, suggesting that well-designed short activities can effectively re-activate interest even at later ages. Overall, the study highlights the need for a dynamic, age-sensitive framework for CS education in which instructional strategies are aligned with developmental trajectories.

</details>


### [32] [Gamification with Purpose: What Learners Prefer to Motivate Their Learning](https://arxiv.org/abs/2512.08551)
*Kai Marquardt,Mona Schulz,Anne Koziolek,Lucia Happe*

Main category: cs.SE

TL;DR: 本研究明确了教育游戏化设计中支持内在动机且紧密结合学习内容的关键元素，强调以学习者为中心的设计策略。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探讨学习者对教育环境中游戏设计元素的偏好，以指导有目的的游戏化策略开发，避免内在动机流失。

Method: 通过系统文献综述确定十种广泛讨论的游戏设计元素，制作可视化原型，并对125名参与者实施最佳-最差量表调查，结合质性反馈分析动机驱动因素。

Result: 学习者偏好直接支持学习过程的元素，如进度条、概念图、即时反馈和成就等，质性分析揭示六个主要动机主题，包括可见进步、内容相关性和建设性反馈。

Conclusion: 学习者重视与教育内容紧密结合并支持内在动机的游戏化元素，建议游戏化设计应优先采用可视化学习进程和提供可操作反馈的工具，而非仅依赖外在激励。

Abstract: This study investigates learners' preferences for game design elements (GDEs) in educational contexts to inform the development of purpose-driven gamification strategies. It emphasizes a learner-centered approach that aligns gamification design with pedagogical goals, while mitigating risks such as the erosion of intrinsic motivation. A systematic literature review was conducted to identify ten widely discussed GDEs. Visual prototypes representing each element were developed, and a best-worst scaling (BWS) survey with 125 participants was administered to elicit preference rankings. Qualitative feedback was also collected to uncover motivational drivers. Learners consistently preferred GDEs that support learning processes directly-most notably progress bars, concept maps, immediate feedback, and achievements. Qualitative analysis revealed six recurring motivational themes, including visible progress, content relevance, and constructive feedback. The findings suggest that learners value gamification elements that are meaningfully integrated with educational content and support intrinsic motivation. Purpose-aligned gamification should prioritize tools that visualize learning progress and provide actionable feedback, rather than relying solely on extrinsic incentives.

</details>


### [33] [Reusability in MLOps: Leveraging Ports and Adapters to Build a Microservices Architecture for the Maritime Domain](https://arxiv.org/abs/2512.08657)
*Renato Cordeiro Ferreira,Aditya Dhinavahi,Rowanne Trapmann,Willem-Jan van den Heuvel*

Main category: cs.SE

TL;DR: 本文基于六边形架构模式，展示了在海事异常检测系统Ocean Guard中实现微服务代码复用的经验与教训，旨在激励更多从业者采用该架构。


<details>
  <summary>Details</summary>
Motivation: ML-Enabled Systems (MLES) 由于涉及多个组件协同工作，系统结构复杂，亟需有效的软件架构复用技术以简化开发。

Method: 采用Ports and Adapters（六边形架构）模式，通过复用单一代码库构建多个微服务，提升架构的复用性和扩展性。

Result: 成功应用该架构模式于Ocean Guard系统，实现了异常检测领域的高效微服务构建和复用。

Conclusion: 六边形架构模式在ML系统开发中具有显著优势，值得软件工程师及相关从业者推广使用。

Abstract: ML-Enabled Systems (MLES) are inherently complex since they require multiple components to achieve their business goal. This experience report showcases the software architecture reusability techniques applied while building Ocean Guard, an MLES for anomaly detection in the maritime domain. In particular, it highlights the challenges and lessons learned to reuse the Ports and Adapters pattern to support building multiple microservices from a single codebase. This experience report hopes to inspire software engineers, machine learning engineers, and data scientists to apply the Hexagonal Architecture pattern to build their MLES.

</details>


### [34] [RESTifAI: LLM-Based Workflow for Reusable REST API Testing](https://arxiv.org/abs/2512.08706)
*Leon Kogler,Maximilian Ehrhart,Benedikt Dornauer,Eduard Paul Enoiu*

Main category: cs.SE

TL;DR: RESTifAI是一款基于大语言模型的REST API测试生成工具，自动创建正向和负向测试用例，支持CI/CD，提升了测试复用性和准确性，在性能上与现有工具持平并克服其部分限制。


<details>
  <summary>Details</summary>
Motivation: 现有自动化REST API测试工具多关注内部服务器错误，缺乏有效的正向场景测试和负向场景推导，且存在测试用例复用性差、测试结果判定复杂及集成困难的问题，推动了开发RESTifAI以系统性生成全面测试用例。

Method: RESTifAI基于大语言模型，采用先构建有效的正向测试用例（快乐路径），再根据业务规则和输入参数推导负向测试用例，涵盖2xx和4xx响应，支持生成CI/CD集成的测试脚本。

Result: 本文介绍了RESTifAI，一种基于大语言模型（LLM）的自动化生成REST API测试的方法，能够生成可复用、支持持续集成/持续交付（CI/CD）的测试用例，采用快乐路径（happy-path）测试策略。RESTifAI不仅构造有效的正向测试路径，还能推导出负向测试用例，用于验证接口对无效输入和业务规则违背时的响应（4xx），提升测试的完整性和鲁棒性。实验结果表明，RESTifAI在性能上与当前先进的LLM工具AutoRestTest和LogiAgent相当，同时在测试用例复用性、测试结果判定复杂性和集成能力方面有所改进。此外，论文还展示了该工具在工业服务中的实际应用案例，并开源了该工具。

Conclusion: RESTifAI有效提升了自动生成REST API测试用例的质量和复用性，能够同时验证接口的正确行为及其对异常输入的鲁棒性，具备良好的工业应用潜力并开源推广。

Abstract: With this paper, we introduce RESTifAI, an LLM-driven approach for generating reusable, CI/CD ready REST API tests, following the happy-path approach. Unlike existing tools that often focus primarily on internal server errors, RESTifAI systematically constructs valid test scenarios (happy paths) and derives negative cases to verify both intended functionality (2xx responses) and robustness against invalid inputs or business-rule violations (4xx responses). The results indicate that RESTifAI performs on par with the latest LLM tools, i.e., AutoRestTest and LogiAgent, while addressing limitations related to reusability, oracle complexity, and integration. To support this, we provide common comparative results and demonstrate the tool's applicability in industrial services. For tool demonstration, please refer to https://www.youtube.com/watch?v=2vtQo0T0Lo4. RESTifAI is publicly available at https://github.com/casablancahotelsoftware/RESTifAI.

</details>


### [35] [Multicalibration for LLM-based Code Generation](https://arxiv.org/abs/2512.08810)
*Viola Campos,Robin Kuschnereit,Adrian Ulges*

Main category: cs.SE

TL;DR: 提出多重校准方法提升代码LLMs置信度的准确性，实验证明多重校准优于传统校准方法，并公开相关数据集。


<details>
  <summary>Details</summary>
Motivation: 随着基于AI的代码生成普及，研究者关注代码LLMs置信度的校准问题，确保模型的置信度能真实反映代码正确性，从而提升代码生成的可靠性。

Method: 采用四种多重校准方法，结合代码复杂度、长度和编程语言等因素，对三个函数合成基准测试集上的最新代码LLMs进行评估，通过对比未校准和基线校准结果来验证方法有效性，同时进行了消融分析。

Result: 本文研究代码大型语言模型（LLMs）的校准问题，提出多重校准方法结合代码复杂度、长度及编程语言等因素，显著提升模型置信度与代码正确性之间的匹配度。在三个函数合成基准测试上，使用最新代码LLMs（Qwen3 Coder、GPT-OSS、DeepSeek-R1-Distill）进行了四种多重校准方法的实验，结果显示多重校准较未校准和基线校准分别提升技能评分1.03和0.37。研究还通过消融实验分析了各因素影响，并开放了包含代码生成、似然度及正确标签的数据集，促进该领域研究。

Conclusion: 多重校准方法能够有效提升代码LLMs的置信度校准效果，改善模型预测正确性与置信度的对应关系，为代码生成模型的可信性提供支持。

Abstract: As AI-based code generation becomes widespread, researchers are investigating the calibration of code LLMs - ensuring their confidence scores faithfully represent the true likelihood of code correctness. To do so, we investigate multicalibration, which can capture additional factors about a coding problem, such as complexity, code length, or programming language used. We study four multicalibration approaches on three function synthesis benchmarks, using latest-generation code LLMs (Qwen3 Coder, GPT-OSS, DeepSeek-R1-Distill). Our results demonstrate that multicalibration can yield distinct improvements over both uncalibrated token likelihoods (+1.03 in skill score) and baseline calibrations (+0.37 in skill score). We study the influence of the aforementioned factors in ablations, and make our dataset (consisting of code generations, likelihoods, and correctness labels) available for future research on code LLM calibration.

</details>


### [36] [SimpleDevQA: Benchmarking Large Language Models on Development Knowledge QA](https://arxiv.org/abs/2512.08867)
*Jing Zhang,Lianghong Guo,Yanlin Wang,Mingwei Liu,Jiachi Chen,Yuchi Ma,Ensheng Shi,Terry Yue Zhuo,Hongyu Zhang,Zibin Zheng*

Main category: cs.SE

TL;DR: 本文分析了开发知识问答的重要性及不足，构建了基于真实用户对话的多语言基准SimpleDevQA，并通过实验证明知识注入和代码模型在此任务中表现更优。


<details>
  <summary>Details</summary>
Motivation: 目前软件开发过程中，开发者需要通过自然语言获取开发相关知识，然而现有的知识问答任务集中在代码生成和理解，缺乏对广泛开发知识的覆盖，也缺乏基于真实用户查询的数据集用于评估大语言模型的开发知识问答能力。

Method: 设计了三阶段数据处理管道，将真实用户对话转化为简单的开发知识问答对，构建了三语种的SimpleDevQA数据集，并基于该数据集进行了多模型对比试验，采用知识注入与检索增强生成策略提升模型表现。

Result: 设计了一个三阶段管道，将真实对话转换为简单的开发知识问答对，构建了多语言、来自真实用户对话的SimpleDevQA基准数据集，包含2740个问答对，涵盖英语、中文和俄语，且强调唯一、简短和可验证的答案。实验显示代码专用大模型优于通用大模型，知识注入能够提升11.3%准确率，模型在回答时存在过度自信，回答准确率与置信度正相关，且代码生成性能强的模型在开发知识问答中表现更好。

Conclusion: 现有基准和模型在开发知识问答任务中存在显著不足，SimpleDevQA为评估开发知识问答提供了更全面和真实的数据支持，知识注入及代码专用模型能有效提升大语言模型性能。

Abstract: The Development Knowledge Question Answering (Dev Knowledge QA) task aims to provide natural language answers to knowledge-seeking questions during software development. To investigate its importance and to what extent it has been explored, we analyze real user-LLM dialogues from WildChat and find that: (1) The Dev Knowledge QA task accounts for 39.6% of interactions(highest among all tasks), revealing broad knowledge needs beyond code generation (32.3%). (2) Only 27.5% of real Dev Knowledge QA dialogues focus on code understanding, leaving out development knowledge-seeking. (3) Only 17.1% of real-world Dev Knowledge QA dialogues can be used for constructing a benchmark. Existing benchmarks have two primary limitations for evaluating the Dev Knowledge QA capability of LLMs. First, existing benchmarks offer a limited development knowledge scope, mainly focusing on code understanding and neglecting broader knowledge during development. Second, some benchmarks are not built from real user queries. To bridge this gap, we design a three-phase pipeline that transforms real-world dialogue into simple development knowledge-seeking QA pairs. Through this pipeline, we introduce SimpleDevQA, a multilingual benchmark derived from real user dialogues. It contains 2,740 QA pairs in three languages (English, Chinese, and Russian), and focuses on questions with unique, short, and verifiable answers for accurate and simple evaluation. Experiments show that: Code LLMs generally outperform general LLMs of similar scale; Knowledge injection with the Retrieval-Augmented Generation (RAG) strategy can boost LLM accuracy by 11.3% on average; LLMs show systematic overconfidence in Dev Knowledge QA, and the answering accuracy of LLMs shows a positive correlation with their stated confidence; Generally, LLMs with stronger code generation performance also exhibit stronger performance in Dev Knowledge QA.

</details>


### [37] [Exploring the Garden of Forking Paths in Empirical Software Engineering Research: A Multiverse Analysis](https://arxiv.org/abs/2512.08910)
*Nathan Cassee,Robert Feldt*

Main category: cs.SE

TL;DR: 针对实证软件工程研究中多分析决策引发的结果差异问题，本文通过多元宇宙分析揭示了方法选择对研究结果的巨大影响，倡导进行鲁棒性检验和明确的分析决策理由。


<details>
  <summary>Details</summary>
Motivation: 实证软件工程中，研究者在数据处理、操作定义和统计模型选择上有较大自由，这种自由虽然能促进研究，但也可能导致结果的鲁棒性和可复现性受损。

Method: 采用多元宇宙分析方法，对一篇已发表的挖掘软件仓库研究（MSR）论文进行了系统的多分析路径探索，共评估了3072种不同的数据分析流程。

Result: 仅有不到0.2%的分析路径得到了原论文的相同结论，绝大多数路径产生了质的不同甚至相反的结果。

Conclusion: 呼吁软件工程研究者在报告中增加鲁棒性检验，明确每一个分析决策的理由，提出了分类模型辅助决策合理性说明，并推荐多元宇宙分析作为提高研究可靠性和可复现性的有效工具。

Abstract: In empirical software engineering (SE) research, researchers have considerable freedom to decide how to process data, what operationalizations to use, and which statistical model to fit. Gelman and Loken refer to this freedom as leading to a "garden of forking paths". Although this freedom is often seen as an advantage, it also poses a threat to robustness and replicability: variations in analytical decisions, even when justifiable, can lead to divergent conclusions.
  To better understand this risk, we conducted a so-called multiverse analysis on a published empirical SE paper. The paper we picked is a Mining Software Repositories study, as MSR studies commonly use non-trivial statistical models to analyze post-hoc, observational data. In the study, we identified nine pivotal analytical decisions-each with at least one equally defensible alternative and systematically reran all the 3,072 resulting analysis pipelines on the original dataset. Interestingly, only 6 of these universes (<0.2%) reproduced the published results; the overwhelming majority produced qualitatively different, and sometimes even opposite, findings.
  This case study of a data analytical method commonly applied to empirical software engineering data reveals how methodological choices can exert a more profound influence on outcomes than is often acknowledged. We therefore advocate that SE researchers complement standard reporting with robustness checks across plausible analysis variants or, at least, explicitly justify each analytical decision. We propose a structured classification model to help classify and improve justification for methodological choices. Secondly, we show how the multiverse analysis is a practical tool in the methodological arsenal of SE researchers, one that can help produce more reliable, reproducible science.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [38] [CrowdLLM: Building LLM-Based Digital Populations Augmented with Generative Models](https://arxiv.org/abs/2512.07890)
*Ryan Feng Lin,Keyu Tian,Hanming Zheng,Congjing Zhang,Li Zeng,Shuai Huang*

Main category: cs.MA

TL;DR: CrowdLLM通过结合大语言模型与生成模型，成功提升了数字人口的多样性和准确性，具备接近真实人群的数据质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的数字人口缺乏足够的准确性和多样性，无法真实反映人群特性。

Method: 提出了CrowdLLM方法，将预训练的大语言模型与生成模型结合，以增强数字人口的多样性和真实性。

Result: 理论分析证明CrowdLLM在构建高效、具有代表性及可扩展的数字人口方面具备巨大潜力。实验证明其在多个领域中（如众包、投票、用户评分）准确性和分布一致性优异。

Conclusion: CrowdLLM为生成成本低、代表性强、可扩展的数字人群提供了有效方案，适用于多种应用领域。

Abstract: The emergence of large language models (LLMs) has sparked much interest in creating LLM-based digital populations that can be applied to many applications such as social simulation, crowdsourcing, marketing, and recommendation systems. A digital population can reduce the cost of recruiting human participants and alleviate many concerns related to human subject study. However, research has found that most of the existing works rely solely on LLMs and could not sufficiently capture the accuracy and diversity of a real human population. To address this limitation, we propose CrowdLLM that integrates pretrained LLMs and generative models to enhance the diversity and fidelity of the digital population. We conduct theoretical analysis of CrowdLLM regarding its great potential in creating cost-effective, sufficiently representative, scalable digital populations that can match the quality of a real crowd. Comprehensive experiments are also conducted across multiple domains (e.g., crowdsourcing, voting, user rating) and simulation studies which demonstrate that CrowdLLM achieves promising performance in both accuracy and distributional fidelity to human data.

</details>


### [39] [MARINE: Theoretical Optimization and Design for Multi-Agent Recursive IN-context Enhancement](https://arxiv.org/abs/2512.07898)
*Hongwei Zhang,Ji Lu,Yongsheng Du,Yanqin Gao,Lingjun Huang,Baoli Wang,Fang Tan,Peng Zou*

Main category: cs.MA

TL;DR: 本文提出MARINE框架，通过递归强化推理轨迹，破解LLM一次性输出限制，实现参数高效推理并在基准测试中取得领先表现。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型(LLM)尽管具备先进的推理能力，但实际应用中通常限制输出为单次响应，导致性能潜力未被充分发挥。

Method: 提出了MARINE框架，通过多代理递归的上下文增强，将测试阶段的推理视为对持久参考轨迹的迭代改进，系统性地将模型的pass@N能力转化为接近最优的pass@1性能。

Result: 在BrowserComp-ZH基准测试中，基于685B参数的模型实现了46.0% pass@1准确率，且一个80B参数模型结合MARINE的表现可匹敌独立的1000B参数模型，显著降低参数需求。

Conclusion: MARINE框架不仅提高了推理质量和效率，还能在固定计算预算内提供比传统采样排序方法更优的样本，有望提升后训练阶段的效率，并为参数高效推理树立了新范式。

Abstract: Large Language Model (LLM)-based agents demonstrate advanced reasoning capabilities, yet practical constraints frequently limit outputs to single responses, leaving significant performance potential unrealized. This paper introduces MARINE (Multi-Agent Recursive IN-context Enhancement), a theoretically grounded framework that reconceptualizes test-time reasoning as iterative refinement of a persistent reference trajectory, fundamentally departing from conventional one-shot or multi-sample paradigms. The MARINE refinement operator systematically converts a base model's pass@N capabilities into near-optimal pass@1 performance. Rigorous theoretical analysis establishes that minimal feasible batches maximize expected performance gains under fixed invocation budgets, while logarithmically growing batch schedules ensure continuous improvement without computational constraints. Comprehensive evaluation on the BrowserComp-ZH benchmark demonstrates state-of-the-art results, with a 685B-parameter implementation achieving 46.0% pass@1 accuracy. Meanwhile, MARINE establishes a new paradigm for parameter-efficient reasoning: an 80B-parameter model augmented with MARINE matches the performance of standalone 1000B-parameter agents, reducing parameter requirements by over an order of magnitude. Notably, within a fixed computational budget, the proposed MARINE delivers higher-quality samples to alignment and optimization processes than traditional sampling-and-ranking strategies. Consequently, it has great potential to boost post-training efficiency.

</details>


### [40] [Probabilistic Multi-Agent Aircraft Landing Time Prediction](https://arxiv.org/abs/2512.08281)
*Kyungmin Kim,Seokbin Yoon,Keumjin Lee*

Main category: cs.MA

TL;DR: 本文提出了一种概率多智能体飞机着陆时间预测模型，准确预测着陆时间及其不确定性，同时挖掘空管中的交互模式，实现更可靠与可解释的预测。


<details>
  <summary>Details</summary>
Motivation: 飞机轨迹和流量存在不确定性，影响着陆时间预测的准确性和可信度，因此需要不仅预测点估计，还要给出不确定性，同时要考虑多智能体交互影响。

Method: 提出了一种概率多智能体飞机着陆时间预测框架，能够输出多个飞机的着陆时间分布，并考虑空中多智能体交互。

Result: 该模型在韩国仁川国际机场近场空域数据集上进行测试，预测准确度超过基线模型，并能量化预测不确定性，通过注意力得分揭示空中交通管制的潜在模式，提高了模型的可解释性。

Conclusion: 该模型提升了飞机着陆时间预测的准确性和可信度，提供了不确定性估计，并通过多智能体交互建模增强了结果的解释能力，有助于空中交通资源合理分配。

Abstract: Accurate and reliable aircraft landing time prediction is essential for effective resource allocation in air traffic management. However, the inherent uncertainty of aircraft trajectories and traffic flows poses significant challenges to both prediction accuracy and trustworthiness. Therefore, prediction models should not only provide point estimates of aircraft landing times but also the uncertainties associated with these predictions. Furthermore, aircraft trajectories are frequently influenced by the presence of nearby aircraft through air traffic control interventions such as radar vectoring. Consequently, landing time prediction models must account for multi-agent interactions in the airspace. In this work, we propose a probabilistic multi-agent aircraft landing time prediction framework that provides the landing times of multiple aircraft as distributions. We evaluate the proposed framework using an air traffic surveillance dataset collected from the terminal airspace of the Incheon International Airport in South Korea. The results demonstrate that the proposed model achieves higher prediction accuracy than the baselines and quantifies the associated uncertainties of its outcomes. In addition, the model uncovered underlying patterns in air traffic control through its attention scores, thereby enhancing explainability.

</details>
