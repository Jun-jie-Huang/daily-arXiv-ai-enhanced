<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 74]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.SE](#cs.SE) [Total: 23]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Towards Reasoning-Preserving Unlearning in Multimodal Large Language Models](https://arxiv.org/abs/2512.17911)
*Hongji Li,Junchi yao,Manjiang Yu,Priyanka Singh,Xue Li,Di Wang,Lijie Hu*

Main category: cs.CL

TL;DR: 提出了针对推理多模态大语言模型的遗忘评测基准和方法，解决了遗忘效果与推理能力间的平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘技术无法同时消除推理过程中的敏感信息泄露且保护模型推理能力，缺乏综合的评价基准和有效方法。

Method: 提出了R-MUSE框架，通过训练无关、推理时引导模型内部表征，联合忘记答案及推理链条，同时保护整体推理表现。

Result: 本文针对推理多模态大型语言模型（RMLLMs）中的机机遗忘问题提出了新的基准和方法。作者指出，传统的遗忘方法无法有效防止中间推理链条泄露敏感信息，且过度干预会损害模型的推理能力。为此，提出了RMLLMU-Bench基准，用于综合评估遗忘效果及推理能力保持，并提出了R-MUSE方法，通过在训练和推理阶段引导内部表征，既实现有效遗忘也保持推理能力。实验证明，R-MUSE在两者之间取得了更好的平衡。

Conclusion: R-MUSE方法在RMLLMU-Bench基准上实现了更有效的数据遗忘同时保持了模型的推理能力，优于现有遗忘方法。

Abstract: Machine unlearning aims to erase requested data from trained models without full retraining. For Reasoning Multimodal Large Language Models (RMLLMs), this is uniquely challenging: intermediate chain-of-thought steps can still leak sensitive information even when final answers are forgotten, and overly aggressive interventions easily damage general reasoning ability. Yet no benchmark jointly evaluates how well unlearning methods suppress reasoning-level leakage while preserving reasoning competence. We address this gap with RMLLMU-Bench, the first benchmark for RMLLM unlearning that extends standard forgetting metrics with dedicated measures of reasoning leakage and reasoning retention. A systematic evaluation on RMLLMU-Bench reveals that existing unlearning methods for MLLMs and Large (Language) Reasoning Models (LRMs) either leave substantial leakage in the reasoning process or severely degrade reasoning performance. To address these gaps, we propose R-MUSE (Reasoning-preserving MLLM Unlearning via Subspace guidance and Adaptive Steering), a training-free and inference-time intervention framework that steers internal representations to forget both answers and reasoning traces while explicitly preserving general reasoning. Experiments on RMLLMU-Bench demonstrate that R-MUSE achieves a substantially better balance between effective forgetting and reasoning retention.

</details>


### [2] [Graph-O1 : Monte Carlo Tree Search with Reinforcement Learning for Text-Attributed Graph Reasoning](https://arxiv.org/abs/2512.17912)
*Lihui Liu*

Main category: cs.CL

TL;DR: 本文提出了Graph-O1框架，结合蒙特卡洛树搜索和强化学习，实现大型语言模型对文本属性图的交互式逐步推理，从而提升复杂图结构中的问答表现。


<details>
  <summary>Details</summary>
Motivation: 当前基于文本的检索生成方法忽视图结构，基于图的文本序列方法受限于上下文长度，导致推理碎片化和性能下降，亟需新的框架高效利用图结构与文本信息。

Method: 采用蒙特卡洛树搜索结合端到端强化学习，使语言模型能够与图环境多轮交互式推理，智能选择最有信息量的子图进行检索。

Result: 广泛实验表明，Graph-O1在多个大型语言模型骨干网络上均超越最先进基线，在问答任务中输出更准确、可靠且易于解释的答案。

Conclusion: Graph-O1通过有选择性的子图检索和多轮交互推理，显著优于现有方法，在准确性、可靠性及可解释性方面表现出色。

Abstract: ChatGPT said: Text-attributed graphs, where nodes and edges contain rich textual information, are widely used across diverse domains. A central challenge in this setting is question answering, which requires jointly leveraging unstructured text and the structured relational signals within the graph. Although Large Language Models (LLMs) have made significant advances in natural language understanding, their direct use for reasoning over text-attributed graphs remains limited. Retrieval-augmented generation methods that operate purely on text often treat passages as isolated units, ignoring the interconnected structure of the graph. Conversely, graph-based RAG methods that serialize large subgraphs into long textual sequences quickly become infeasible due to LLM context-length constraints, resulting in fragmented reasoning and degraded accuracy. To overcome these limitations, we introduce Graph-O1, an agentic GraphRAG framework that enables LLMs to conduct stepwise, interactive reasoning over graphs. Our approach integrates Monte Carlo Tree Search (MCTS) with end-to-end reinforcement learning, allowing the model to selectively explore and retrieve only the most informative subgraph components. The reasoning procedure is framed as a multi-turn interaction between the agent and the graph environment, and the agent is trained through a unified reward mechanism. Extensive experiments across multiple LLM backbones demonstrate that Graph-O1 consistently surpasses state-of-the-art baselines, producing answers that are more accurate, reliable, and interpretable.

</details>


### [3] [Q-KVComm: Efficient Multi-Agent Communication Via Adaptive KV Cache Compression](https://arxiv.org/abs/2512.17914)
*Boris Kriuk,Logic Ng*

Main category: cs.CL

TL;DR: 本文提出Q-KVComm协议，通过压缩传输LLM代理间的语义表示，极大减少通信带宽和计算开销，同时保持高语义一致性，实现了多模型和多场景的有效应用。


<details>
  <summary>Details</summary>
Motivation: 多智能体大语言模型系统在代理之间传递上下文信息时，存在冗余传输，导致带宽和计算资源过度消耗的问题。传统方法通过传递原始文本，迫使接收代理重新计算相似的语义表示，效率低下。

Method: 采用分层自适应量化分配比特宽度，结合混合信息提取和异构模型校准，以实现跨架构的高效语义表示压缩传输。

Result: 提出了Q-KVComm协议，允许在LLM代理之间直接传输压缩后的键值缓存表示，显著降低数据传输量同时保持语义完整性。实验显示该协议实现了5-6倍的压缩比，且语义连贯性评分保持在0.77以上，适用于多种模型大小和实际应用场景。

Conclusion: Q-KVComm协议成功地将LLM多代理通信从基于文本的信息交换转变为基于语义表示的交换，显著提效且保持语义质量，开辟了新的通信范式。

Abstract: Multi-agent Large Language Model (LLM) systems face a critical bottleneck: redundant transmission of contextual information between agents consumes excessive bandwidth and computational resources. Traditional approaches discard internal semantic representations and transmit raw text, forcing receiving agents to recompute similar representations from scratch. We introduce Q-KVComm, a new protocol that enables direct transmission of compressed key-value (KV) cache representations between LLM agents. Q-KVComm combines three key innovations: (1) adaptive layer-wise quantization that allocates variable bit-widths based on sensitivity profiling, (2) hybrid information extraction that preserves critical facts across content domains, and (3) heterogeneous model calibration establishing cross-architecture communication. Extensive experiments across three diverse question-answering datasets demonstrate that Q-KVComm achieves 5-6x compression ratios while maintaining semantic fidelity, with coherence quality scores above 0.77 across all scenarios. The protocol exhibits robust performance across model sizes (1.1B-1.5B parameters) and adapts to real-world applications including conversational QA and multi-hop reasoning. Our work establishes a new paradigm for LLM agent communication, shifting from text-based to representation-based information exchange.

</details>


### [4] [Supplementary Resources and Analysis for Automatic Speech Recognition Systems Trained on the Loquacious Dataset](https://arxiv.org/abs/2512.17915)
*Nick Rossenbach,Robin Schmitt,Tina Raissi,Simon Berger,Larissa Kleppel,Ralf Schlüter*

Main category: cs.CL

TL;DR: Loquacious数据集作为一个多领域、多样化且开放的ASR数据集，配合额外的语言资源，为ASR研究提供了有价值的实验平台。


<details>
  <summary>Details</summary>
Motivation: 替代现有的英语自动语音识别(ASR)数据集，提供多声学和语言领域的训练和测试划分，适用于学术和工业界，且拥有开放许可。

Method: 提供数据集和相关语言资源，包括n-gram语言模型、G2P模型和发音词典，结合多样的ASR架构进行实验验证。

Result: 提供了n-gram语言模型、字素到音素模型和发音词典等附加资源，并展示了使用这些资源在多种ASR架构上的初步实验结果，表明Loquacious数据集适用于研究ASR中的多样挑战。

Conclusion: Loquacious数据集及其附加资源促进了ASR模型在不同架构和标签设置上的评测，展示了其作为研究工具的潜力和价值。

Abstract: The recently published Loquacious dataset aims to be a replacement for established English automatic speech recognition (ASR) datasets such as LibriSpeech or TED-Lium. The main goal of the Loquacious dataset is to provide properly defined training and test partitions across many acoustic and language domains, with an open license suitable for both academia and industry. To further promote the benchmarking and usability of this new dataset, we present additional resources in the form of n-gram language models (LMs), a grapheme-to-phoneme (G2P) model and pronunciation lexica, with open and public access. Utilizing those additional resources we show experimental results across a wide range of ASR architectures with different label units and topologies. Our initial experimental results indicate that the Loquacious dataset offers a valuable study case for a variety of common challenges in ASR.

</details>


### [5] [Learning to Prioritize IT Tickets: A Comparative Evaluation of Embedding-based Approaches and Fine-Tuned Transformer Models](https://arxiv.org/abs/2512.17916)
*Minh Tri LÊ,Ali Ait-Bachir*

Main category: cs.CL

TL;DR: 本文比较了传统嵌入方法与多语言Transformer在IT服务工单优先级排序中的表现，发现后者效果显著更好。


<details>
  <summary>Details</summary>
Motivation: 由于ITSM服务工单文本噪声大、主观性强及类别极度不平衡，优先级排序任务面临挑战，亟需有效模型提升分类效果。

Method: 本文评估了两类方法：一是基于嵌入的降维+聚类+经典分类器管道；二是结合文本及数值特征的多语言微调Transformer模型。

Result: 本文针对IT服务管理(ITSM)中服务工单优先级排序问题进行研究，比较了基于嵌入的传统方法和多语言微调Transformer方法。实验表明，基于嵌入的方法在多个配置下表现有限，尤其是聚类难以捕捉有效结构，且分类器对嵌入质量敏感。相反，提出的Transformer模型结合文本和数值特征，表现显著优越，F1分数达到78.5%，加权Cohen's kappa近0.80，显示与真实标签高度一致。该研究表明通用嵌入方法在ITSM数据上存在局限性，而领域适应的Transformer架构在工单优先级排序任务中更为有效。

Conclusion: 基于领域适应的多语言Transformer模型能有效提升ITSM工单优先级排序准确性，优于传统嵌入方法。

Abstract: Prioritizing service tickets in IT Service Management (ITSM) is critical for operational efficiency but remains challenging due to noisy textual inputs, subjective writing styles, and pronounced class imbalance. We evaluate two families of approaches for ticket prioritization: embedding-based pipelines that combine dimensionality reduction, clustering, and classical classifiers, and a fine-tuned multilingual transformer that processes both textual and numerical features. Embedding-based methods exhibit limited generalization across a wide range of thirty configurations, with clustering failing to uncover meaningful structures and supervised models highly sensitive to embedding quality. In contrast, the proposed transformer model achieves substantially higher performance, with an average F1-score of 78.5% and weighted Cohen's kappa values of nearly 0.80, indicating strong alignment with true labels. These results highlight the limitations of generic embeddings for ITSM data and demonstrate the effectiveness of domain-adapted transformer architectures for operational ticket prioritization.

</details>


### [6] [KVReviver: Reversible KV Cache Compression with Sketch-Based Token Reconstruction](https://arxiv.org/abs/2512.17917)
*Aomufei Yuan,Zhiming Wang,Ruijie Miao,Dayu Wang,Yuxuan Tian,Zihan Wang,Yebo Peng,Yuhan Wu,Bairen Yi,Xin Liu,Tong Yang*

Main category: cs.CL

TL;DR: 本文提出了基于sketch算法的可逆KV缓存压缩方法KVReviver，有效减少内存需求且保持模型准确度，解决了传统KV缓存压缩导致的信息不可恢复性丢失问题。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型中KV缓存内存需求随着上下文长度增加迅猛增长，传统压缩方式不可逆且导致信息丢失，亟需一种既能压缩又能恢复信息的解决方案。

Method: 基于sketch算法设计的可逆KV缓存压缩技术，结合附加数据结构以支持压缩令牌的完整重构。

Result: 本文针对大型语言模型（LLMs）中Key-Value（KV）缓存内存瓶颈问题，提出了一种基于sketch算法的可逆KV缓存压缩方法KVReviver。该方法通过附加数据结构实现压缩后令牌的重构，避免了传统方法中因丢弃低关注度令牌导致的信息不可恢复性丢失（Contextual Amnesia），在显著降低内存需求的同时保持模型的推理准确度。实验证明在2k上下文长度时，KVReviver仅需10%的缓存预算且准确率无损；在32k上下文长度时，使用25%的缓存预算也能实现接近原模型~2%的准确率损失。

Conclusion: KVReviver成功实现了大幅压缩KV缓存而不显著损失模型推理准确性，为长上下文LLMs的内存优化提供了有效方案。

Abstract: As the context length of current large language models (LLMs) rapidly increases, the memory demand for the Key-Value (KV) cache is becoming a bottleneck for LLM deployment and batch processing. Traditional KV cache compression methods typically involve permanently evicting or irreversibly merging "less important" tokens with low attention scores. This approach results in the unrecoverable loss of token information, which we call Contextual Amnesia, significantly degrading the model's information retrieval capability. To address this issue, we propose KVReviver, a reversible KV cache compression method based on the sketch algorithm. This method allows reconstructing compressed tokens from an additional data structure, thus enabling full-scale computation within limited memory. Experiments showed that in 2k-length contexts, it requires only 10% of KV Cache budget while maintaining identical end-to-end inference accuracy. For 32k-length contexts, it achieves equivalent or comparable accuracy ~2% accuracy loss) using merely 25% of KV Cache budget.

</details>


### [7] [Separating Constraint Compliance from Semantic Accuracy: A Novel Benchmark for Evaluating Instruction-Following Under Compression](https://arxiv.org/abs/2512.17920)
*Rahul Baxi*

Main category: cs.CL

TL;DR: 文章通过设计基准测试揭示大模型在提示压缩时表现下降的内因，发现强化学习帮助行为导致中等压缩下约束违规，提出改进对齐指导，助力实际系统优化。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型在提示压缩条件下性能下降的机制，特别是约束遵守和语义准确度如何受压缩级别影响及其原因，解决实际系统部署中的对齐与指令遵守间的矛盾。

Method: 设计了压缩衰减理解测试（CDCT），独立测量约束遵守度及语义准确性，使用三评价模型对9个前沿大模型在5个不同压缩级别、8个概念上进行评测。通过去除 RLHF 的帮助信号验证假设。

Result: 发现约束遵守度随压缩程度呈U形变化，中等压缩时违规最高；模型帮助行为是导致约束违规的主因；去除帮助性信号显著改善约束遵守（提升598%）；推理型模型表现优于高效型模型。

Conclusion: 大语言模型在提示压缩时表现下降，主要因强化学习训练中的帮助性信号导致中等压缩程度下的约束违规。模型在极端压缩下表现竟优于中等压缩，显示出一种普遍的U形曲线。强化学习帮助训练改善语义准确性但加剧了对约束的违规。

Abstract: Large language models (LLMs) exhibit degraded performance under prompt compression, but the mechanisms remain poorly understood. We introduce the Compression-Decay Comprehension Test (CDCT), a benchmark that independently measures constraint compliance (CC) and semantic accuracy (SA) across compression levels. We evaluate 9 frontier LLMs across 8 concepts using 5 compression levels from extreme (c=0.0, ~2 words) to none (c=1.0, ~135 words). A three-judge LLM jury achieves almost perfect inter-rater agreement on CC (Fleiss' \k{appa}=0.90).
  We observe a universal U-curve pattern in constraint compliance (97.2% prevalence), with violations peaking at medium compression (c=0.5, ~27 words). Counterintuitively, models perform better at extreme compression than medium lengths. The dimensions are statistically orthogonal (r=0.193, p=0.084), with constraint effects 2.9x larger than semantic effects.
  Experimental validation via RLHF ablation confirms our constraint salience hypothesis: removing "helpfulness" signals improves CC by 598% on average (71/72 trials, p<0.001), with 79% achieving perfect compliance. This demonstrates that RLHF-trained helpfulness behaviors are the dominant cause of constraint violations at medium compression. Reasoning models outperform efficient models by 27.5% (Cohen's d=0.96).
  Our findings reveal a fundamental tension between RLHF alignment and instruction-following, providing actionable guidelines for improving deployed systems.

</details>


### [8] [MemEvolve: Meta-Evolution of Agent Memory Systems](https://arxiv.org/abs/2512.18746)
*Guibin Zhang,Haotian Ren,Chong Zhan,Zhenhong Zhou,Junhao Wang,He Zhu,Wangchunshu Zhou,Shuicheng Yan*

Main category: cs.CL

TL;DR: 该论文提出一种同时进化记忆架构和代理经验的元进化框架MemEvolve，提升了大语言模型代理的适应性和表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的代理自进化记忆系统主要依赖手工设计的固定记忆架构，限制了系统在不同任务环境下的自适应能力。

Method: 设计MemEvolve框架，实现代理经验和记忆架构的联合元进化；构建EvolveLab代码库整合多种记忆系统模块，并在多个代理基准上进行评测验证。

Result: 提出MemEvolve框架，联合进化代理的经验知识和记忆架构，实现记忆系统的元适应能力，并通过EvolveLab代码库标准化实现和测试，显著提升代理性能和跨任务模型的泛化能力。

Conclusion: MemEvolve能够显著提升代理系统性能，并实现记忆架构在多任务和不同大语言模型间的有效迁移，推动自进化记忆系统的发展。

Abstract: Self-evolving memory systems are unprecedentedly reshaping the evolutionary paradigm of large language model (LLM)-based agents. Prior work has predominantly relied on manually engineered memory architectures to store trajectories, distill experience, and synthesize reusable tools, enabling agents to evolve on the fly within environment interactions. However, this paradigm is fundamentally constrained by the staticity of the memory system itself: while memory facilitates agent-level evolving, the underlying memory architecture cannot be meta-adapted to diverse task contexts. To address this gap, we propose MemEvolve, a meta-evolutionary framework that jointly evolves agents' experiential knowledge and their memory architecture, allowing agent systems not only to accumulate experience but also to progressively refine how they learn from it. To ground MemEvolve in prior research and foster openness in future self-evolving systems, we introduce EvolveLab, a unified self-evolving memory codebase that distills twelve representative memory systems into a modular design space (encode, store, retrieve, manage), providing both a standardized implementation substrate and a fair experimental arena. Extensive evaluations on four challenging agentic benchmarks demonstrate that MemEvolve achieves (I) substantial performance gains, improving frameworks such as SmolAgent and Flash-Searcher by up to $17.06\%$; and (II) strong cross-task and cross-LLM generalization, designing memory architectures that transfer effectively across diverse benchmarks and backbone models.

</details>


### [9] [ReGal: A First Look at PPO-based Legal AI for Judgment Prediction and Summarization in India](https://arxiv.org/abs/2512.18014)
*Shubham Kumar Nigam,Tanuj Tyagi,Siddharth Shukla,Aditya Kumar Guru,Balaramamahanthi Deepak Patnaik,Danush Khanna,Noel Shallum,Kripabandhu Ghosh,Arnab Bhattacharya*

Main category: cs.CL

TL;DR: 本文首次尝试将强化学习方法运用到印度法律AI，通过ReGal框架探索法律推理任务，虽指标未优于传统模型，但揭示了应用中关键挑战和潜在改进方向。


<details>
  <summary>Details</summary>
Motivation: 将强化学习应用于印度法律AI，探索其在法律推理中的潜力和挑战。

Method: 提出ReGal框架，结合多任务指令调优和基于AI反馈的强化学习（RLAIF），采用PPO算法进行法律任务训练。

Result: ReGal在法院判决预测与解释及法律文档摘要两项任务上表现尚不及监督和专有模型，但揭示了RL应用于法律文本的挑战，如奖励模型对齐和领域适应。

Conclusion: 强化学习虽目前表现有待提升，但为处理高风险长文档法律任务提供了新思路，奠定了未来优化法律推理流程的基础。

Abstract: This paper presents an early exploration of reinforcement learning methodologies for legal AI in the Indian context. We introduce Reinforcement Learning-based Legal Reasoning (ReGal), a framework that integrates Multi-Task Instruction Tuning with Reinforcement Learning from AI Feedback (RLAIF) using Proximal Policy Optimization (PPO). Our approach is evaluated across two critical legal tasks: (i) Court Judgment Prediction and Explanation (CJPE), and (ii) Legal Document Summarization. Although the framework underperforms on standard evaluation metrics compared to supervised and proprietary models, it provides valuable insights into the challenges of applying RL to legal texts. These challenges include reward model alignment, legal language complexity, and domain-specific adaptation. Through empirical and qualitative analysis, we demonstrate how RL can be repurposed for high-stakes, long-document tasks in law. Our findings establish a foundation for future work on optimizing legal reasoning pipelines using reinforcement learning, with broader implications for building interpretable and adaptive legal AI systems.

</details>


### [10] [CoPE: A Small Language Model for Steerable and Scalable Content Labeling](https://arxiv.org/abs/2512.18027)
*Samidh Chakrabarti,David Willner,Kevin Klyman,Tiffany Saade,Emily Capstick,Sabina Nong*

Main category: cs.CL

TL;DR: 本文介绍了CoPE，一种小型且策略可控的语言模型，通过创新训练和标注方法实现高效准确的内容标注，表现优于大型模型且资源消耗低，推动了在线内容治理新方向。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够快速、准确进行内容标注的小型语言模型，解决传统模型体积大、运行成本高的问题，并实现策略可控性以便更好地治理在线平台内容。

Method: 提出了“矛盾示例训练”这一新颖的训练课程，使模型学习策略解释而非单纯记忆策略；引入“双目标注”方法，快速构建明确的训练数据集。

Result: 实现了CoPE模型，在七个不同的有害内容领域表现出与最先进模型相当或更优的准确率，但模型大小仅为它们的1%；开放了一个90亿参数版本，可在单个消费级GPU上运行。

Conclusion: CoPE代表了分类器系统的一次范式转变，将机器学习任务转变为策略编写任务，开辟了在线平台治理的新设计可能性。

Abstract: This paper details the methodology behind CoPE, a policy-steerable small language model capable of fast and accurate content labeling. We present a novel training curricula called Contradictory Example Training that enables the model to learn policy interpretation rather than mere policy memorization. We also present a novel method for generating content policies, called Binocular Labeling, which enables rapid construction of unambiguous training datasets. When evaluated across seven different harm areas, CoPE exhibits equal or superior accuracy to frontier models at only 1% of their size. We openly release a 9 billion parameter version of the model that can be run on a single consumer-grade GPU. Models like CoPE represent a paradigm shift for classifier systems. By turning an ML task into a policy writing task, CoPE opens up new design possibilities for the governance of online platforms.

</details>


### [11] [Narrative Consolidation: Formulating a New Task for Unifying Multi-Perspective Accounts](https://arxiv.org/abs/2512.18041)
*Roger A. Finger,Eduardo G. Cortes,Sandro J. Rigo,Gabriel de O. Ramos*

Main category: cs.CL

TL;DR: 本文提出叙事整合任务，通过时间对齐事件图确保文档的时间顺序和内容完整性，显著提升叙事文本质量。


<details>
  <summary>Details</summary>
Motivation: 传统多文档摘要注重压缩，难以保持叙事的时间流程，而叙事整合需要生成统一且时间顺序正确的文本。

Method: 引入时间对齐事件图（TAEG），并结合中心性算法选择事件的最核心版本，确保时间顺序和内容融合。

Result: 本文提出了一种新的自然语言处理任务——叙事整合，旨在处理具有时间顺序且内容互补的重叠叙事文档，生成统一、连贯且时间顺序正确的文本。引入了时间对齐事件图（TAEG），通过显式建模时间和事件对齐，利用中心性算法选择事件的最核心版本。该方法在四部《圣经福音书》上的实验中，实现了完美的时间排序（Kendall's Tau 1.000）并显著提升内容指标（ROUGE-L F1提升357.2%）。结果验证了叙事整合作为新任务的重要性以及时间结构在解决该任务中的关键作用。

Conclusion: 明确的时间结构是实现叙事整合任务的关键，所提方法表现出色，验证了任务的合理性和必要性。

Abstract: Processing overlapping narrative documents, such as legal testimonies or historical accounts, often aims not for compression but for a unified, coherent, and chronologically sound text. Standard Multi-Document Summarization (MDS), with its focus on conciseness, fails to preserve narrative flow. This paper formally defines this challenge as a new NLP task: Narrative Consolidation, where the central objectives are chronological integrity, completeness, and the fusion of complementary details. To demonstrate the critical role of temporal structure in this task, we introduce Temporal Alignment Event Graph (TAEG), a graph structure that explicitly models chronology and event alignment. By applying a standard centrality algorithm to TAEG, our method functions as a version selection mechanism, choosing the most central representation of each event in its correct temporal position. In a study on the four Biblical Gospels, this structure-focused approach guarantees perfect temporal ordering (Kendall's Tau of 1.000) by design and dramatically improves content metrics (e.g., +357.2% in ROUGE-L F1). The success of this baseline method validates the formulation of Narrative Consolidation as a relevant task and establishes that an explicit temporal backbone is a fundamental component for its resolution.

</details>


### [12] [Statistical laws and linguistics inform meaning in naturalistic and fictional conversation](https://arxiv.org/abs/2512.18072)
*Ashley M. A. Fehr,Calla G. Beauregard,Julia Witte Zimmerman,Katie Ekström,Pablo Rosillo-Rodes,Christopher M. Danforth,Peter Sheridan Dodds*

Main category: cs.CL

TL;DR: 本文研究了对话中词汇规模与对话长度的关系，发现不同词性词汇扩展的规律存在差异，同时用行为和语言学框架解释这些发现。


<details>
  <summary>Details</summary>
Motivation: 探讨对话中词汇规模随对话长度的变化规律，特别是Heaps定律在对话中的应用及不同语言特征对词汇规模扩展的影响。

Method: 利用两个不同媒介的对话数据，统计分析词汇规模与对话长度的关系，特别聚焦不同词性的词汇，通过Heaps定律分析其变化规律。

Result: 发现词汇规模的扩展规模因词性而异，并且在两种不同媒介（视频聊天中的陌生人对话和电影中的虚构角色对话）中表现不同。

Conclusion: 不同词性的词汇在对话中的扩展特征不同，Heaps定律在对话研究中具有应用价值，可以通过行为和语言学视角深入理解对话动态。

Abstract: Conversation is a cornerstone of social connection and is linked to well-being outcomes. Conversations vary widely in type with some portion generating complex, dynamic stories. One approach to studying how conversations unfold in time is through statistical patterns such as Heaps' law, which holds that vocabulary size scales with document length. Little work on Heaps's law has looked at conversation and considered how language features impact scaling. We measure Heaps' law for conversations recorded in two distinct mediums: 1. Strangers brought together on video chat and 2. Fictional characters in movies. We find that scaling of vocabulary size differs by parts of speech. We discuss these findings through behavioral and linguistic frameworks.

</details>


### [13] [Training LLMs with LogicReward for Faithful and Rigorous Reasoning](https://arxiv.org/abs/2512.18196)
*Jundong Xu,Hao Fei,Huichi Zhou,Xin Quan,Qijun Huang,Shengqiong Wu,William Yang Wang,Mong-Li Lee,Wynne Hsu*

Main category: cs.CL

TL;DR: 提出LogicReward，通过定理证明器保障逻辑推理正确性，结合自动形式化，显著提升LLM推理性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有训练方法依赖结果反馈，可能导致错误推理过程；缺乏对步骤级逻辑正确性的保证，而逻辑一致性在高风险应用场景中至关重要，故提出LogicReward以提升推理过程的逻辑严密性。

Method: 通过引入LogicReward奖励系统，利用定理证明器验证每一步推理的逻辑正确性，并结合自动形式化与软归一化方法，提高形式化的准确度与自然语言的转换质量。

Result: 本文提出了LogicReward，一种通过定理证明器强制步骤级逻辑正确性的奖励系统，用于指导大规模语言模型（LLMs）的训练。为解决自然语言歧义，提出了带有软归一化的自动形式化方法，提高了形式化质量。基于该方法训练的8B模型在自然语言推理和逻辑推理任务上分别超过了GPT-4o和o4-mini，提升显著。LogicReward增强了推理的可信度，提高了模型在数学和常识推理等未见任务上的泛化能力，同时在无标签情况下仍能提供可靠的奖励信号。

Conclusion: LogicReward能够有效提升LLM的逻辑推理能力和推理结果的可信度，其方法在多个推理任务中均表现优异，且具备良好泛化性和无监督奖励能力。

Abstract: Although LLMs exhibit strong reasoning capabilities, existing training methods largely depend on outcome-based feedback, which can produce correct answers with flawed reasoning. Prior work introduces supervision on intermediate steps but still lacks guarantees of logical soundness, which is crucial in high-stakes scenarios where logical consistency is paramount. To address this, we propose LogicReward, a novel reward system that guides model training by enforcing step-level logical correctness with a theorem prover. We further introduce Autoformalization with Soft Unification, which reduces natural language ambiguity and improves formalization quality, enabling more effective use of the theorem prover. An 8B model trained on data constructed with LogicReward surpasses GPT-4o and o4-mini by 11.6\% and 2\% on natural language inference and logical reasoning tasks with simple training procedures. Further analysis shows that LogicReward enhances reasoning faithfulness, improves generalizability to unseen tasks such as math and commonsense reasoning, and provides a reliable reward signal even without ground-truth labels. We will release all data and code at https://llm-symbol.github.io/LogicReward.

</details>


### [14] [GeoSense-AI: Fast Location Inference from Crisis Microblogs](https://arxiv.org/abs/2512.18225)
*Deepit Sapru*

Main category: cs.CL

TL;DR: 本文提出了一种面向微弱噪声微博流的实时地理定位AI流水线，通过多技术融合实现快速准确的地理信息提取，为应急管理提供高效工具。


<details>
  <summary>Details</summary>
Motivation: 当前地理位置信息依赖稀疏的地理标签，难以满足紧急情况下的实时、高效率地理信息需求。该工作旨在通过结合多种信息抽取技术解决此问题，提升应急响应时的情境感知能力。

Method: 结合统计话题标签分词、基于词性识别专有名词、围绕灾害词汇的依存句法分析、轻量级命名实体识别及地名词典消歧技术，从文本直接推断地理位置，实现实时地理信息处理。

Result: 系统在与多个主流命名实体识别工具包的对比测试中达到了较高的F1分数，同时吞吐量显著提升，可支持实时危机信息处理。成功应用于洪水、疫情等快速事件的地理信息提取和可视化。

Conclusion: GeoSense-AI通过领域微调的NLP技术与知识库结合，提升了应急场景下地理信息的实时准确性和处理效率，突破传统地理标签依赖，具有重要应用价值。

Abstract: This paper presents an applied AI pipeline for realtime geolocation from noisy microblog streams, unifying statistical hashtag segmentation, part-of-speech-driven proper-noun detection, dependency parsing around disaster lexicons, lightweight named-entity recognition, and gazetteer-grounded disambiguation to infer locations directly from text rather than sparse geotags. The approach operationalizes information extraction under streaming constraints, emphasizing low-latency NLP components and efficient validation against geographic knowledge bases to support situational awareness during emergencies. In head to head comparisons with widely used NER toolkits, the system attains strong F1 while being engineered for orders-of-magnitude faster throughput, enabling deployment in live crisis informatics settings. A production map interface demonstrates end-to-end AI functionality ingest, inference, and visualization--surfacing locational signals at scale for floods, outbreaks, and other fastmoving events. By prioritizing robustness to informal text and streaming efficiency, GeoSense-AI illustrates how domain-tuned NLP and knowledge grounding can elevate emergency response beyond conventional geo-tag reliance.

</details>


### [15] [InstructNet: A Novel Approach for Multi-Label Instruction Classification through Advanced Deep Learning](https://arxiv.org/abs/2512.18301)
*Tanjim Taharat Aurpa,Md Shoaib Ahmed,Md Mahbubur Rahman,Md. Golam Moazzam*

Main category: cs.CL

TL;DR: 本文提出基于XLNet的多标签指令分类方法，在wikiHow数据集上取得了97.30%的准确率和较高的宏平均F1分数，验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 随着搜索引擎成为解决问题的重要工具，自动分类“如何做”类指导文本有助于任务导向学习和知识库构建，是实用且必要的研究方向。

Method: 使用基于transformer的深度学习模型（XLNet、BERT等）进行多标签分类，评价指标包括准确率和宏平均F1分数，多级评价策略确保对模型效果全面了解。

Result: 本文利用11,121条wikiHow数据记录，针对多标签指令类别分类问题，采用基于transformer的深度神经网络模型（如XLNet、BERT）进行分类。

Conclusion: 采用XLNet架构的InstructNet方法在多标签指令分类中表现卓越，准确率达97.30%，证明了该方法的有效性并指出了未来改进方向。

Abstract: People use search engines for various topics and items, from daily essentials to more aspirational and specialized objects. Therefore, search engines have taken over as peoples preferred resource. The How To prefix has become familiar and widely used in various search styles to find solutions to particular problems. This search allows people to find sequential instructions by providing detailed guidelines to accomplish specific tasks. Categorizing instructional text is also essential for task-oriented learning and creating knowledge bases. This study uses the How To articles to determine the multi-label instruction category. We have brought this work with a dataset comprising 11,121 observations from wikiHow, where each record has multiple categories. To find out the multi-label category meticulously, we employ some transformer-based deep neural architectures, such as Generalized Autoregressive Pretraining for Language Understanding (XLNet), Bidirectional Encoder Representation from Transformers (BERT), etc. In our multi-label instruction classification process, we have reckoned our proposed architectures using accuracy and macro f1-score as the performance metrics. This thorough evaluation showed us much about our strategys strengths and drawbacks. Specifically, our implementation of the XLNet architecture has demonstrated unprecedented performance, achieving an accuracy of 97.30% and micro and macro average scores of 89.02% and 93%, a noteworthy accomplishment in multi-label classification. This high level of accuracy and macro average score is a testament to the effectiveness of the XLNet architecture in our proposed InstructNet approach. By employing a multi-level strategy in our evaluation process, we have gained a more comprehensive knowledge of the effectiveness of our proposed architectures and identified areas for forthcoming improvement and refinement.

</details>


### [16] [CTTA-T: Continual Test-Time Adaptation for Text Understanding via Teacher-Student with a Domain-aware and Generalized Teacher](https://arxiv.org/abs/2512.18321)
*Tianlun Liu,Zhiliang Tian,Zhen Huang,Xingzhi Zhou,Wanlong Yu,Tianle Liu,Feng Liu,Dongsheng Li*

Main category: cs.CL

TL;DR: 本文提出了一种针对文本理解中持续测试时域适应的CTTA-T框架，通过教师-学生模型和增量PCA动态跟踪域变化，实现对不断演变目标域的适应和泛化能力提升。


<details>
  <summary>Details</summary>
Motivation: 现有持续测试时域适应方法在减少跨域误差积累和提升泛化能力方面存在困难，因此本文针对文本理解场景中的多域连续测试问题，设计了更实用且有效的适应策略。

Method: 本文提出基于教师-学生架构的CTTA-T方法，教师模型通过增量主成分分析（PCA）动态累积跨域语义，利用dropout驱动的一致性机制进行预测精炼和不可靠信息过滤，实现对不断变化目标域的自适应。

Result: 实验表明本文提出的CTTA-T方法在处理多域连续变化中表现优异，显著优于基线方法，有效平衡了适应性和泛化性。

Conclusion: CTTA-T框架通过域感知教师-学生模型和基于dropout一致性的预测校准，在持续适应未观测测试域的同时，有效减少误差积累并增强泛化能力，实验结果优于现有方法。

Abstract: Text understanding often suffers from domain shifts. To handle testing domains, domain adaptation (DA) is trained to adapt to a fixed and observed testing domain; a more challenging paradigm, test-time adaptation (TTA), cannot access the testing domain during training and online adapts to the testing samples during testing, where the samples are from a fixed domain. We aim to explore a more practical and underexplored scenario, continual test-time adaptation (CTTA) for text understanding, which involves a sequence of testing (unobserved) domains in testing. Current CTTA methods struggle in reducing error accumulation over domains and enhancing generalization to handle unobserved domains: 1) Noise-filtering reduces accumulated errors but discards useful information, and 2) accumulating historical domains enhances generalization, but it is hard to achieve adaptive accumulation. In this paper, we propose a CTTA-T (continual test-time adaptation for text understanding) framework adaptable to evolving target domains: it adopts a teacher-student framework, where the teacher is domain-aware and generalized for evolving domains. To improve teacher predictions, we propose a refine-then-filter based on dropout-driven consistency, which calibrates predictions and removes unreliable guidance. For the adaptation-generalization trade-off, we construct a domain-aware teacher by dynamically accumulating cross-domain semantics via incremental PCA, which continuously tracks domain shifts. Experiments show CTTA-T excels baselines.

</details>


### [17] [LIR$^3$AG: A Lightweight Rerank Reasoning Strategy Framework for Retrieval-Augmented Generation](https://arxiv.org/abs/2512.18329)
*Guo Chen,Junjie Huang,Huaijin Xie,Fei Sun,Tao Jia*

Main category: cs.CL

TL;DR: 本文提出轻量级推理策略框架LiR³AG，显著降低RAG多跳问答中推理模型的计算开销，并提升非推理模型性能。


<details>
  <summary>Details</summary>
Motivation: 推理模型在RAG多跳问答中虽提升性能但带来计算资源和延迟开销，亟需设计轻量且高效的推理策略以实现性能与效率的平衡。

Method: 提出轻量级重排序推理策略框架LiR³AG，通过重构检索到的证据形成连贯推理链，实现非推理模型的推理策略迁移。

Result: 本文研究了在检索增强生成（RAG）系统中推理模型在多跳问答任务中的应用，发现推理模型主要采用两种推理策略：基于上下文推理和知识调和推理。为减少推理模型带来的计算和延迟开销，提出了轻量级重排序推理策略框架LiR³AG，通过重构检索证据形成连贯的推理链，使非推理模型也能迁移推理策略。实验证明LiR³AG显著降低了98%的输出token消耗和58.6%的推理时间，同时提升了8B非推理模型6.2%-22.5%的性能，甚至优于32B推理模型。

Conclusion: LiR³AG有效减少推理模型的计算资源消耗，同时提升模型问答性能，为RAG系统的实际应用提供了高效可行的解决方案。

Abstract: Retrieval-Augmented Generation (RAG) effectively enhances Large Language Models (LLMs) by incorporating retrieved external knowledge into the generation process. Reasoning models improve LLM performance in multi-hop QA tasks, which require integrating and reasoning over multiple pieces of evidence across different documents to answer a complex question. However, they often introduce substantial computational costs, including increased token consumption and inference latency. To better understand and mitigate this trade-off, we conduct a comprehensive study of reasoning strategies for reasoning models in RAG multi-hop QA tasks. Our findings reveal that reasoning models adopt structured strategies to integrate retrieved and internal knowledge, primarily following two modes: Context-Grounded Reasoning, which relies directly on retrieved content, and Knowledge-Reconciled Reasoning, which resolves conflicts or gaps using internal knowledge. To this end, we propose a novel Lightweight Rerank Reasoning Strategy Framework for RAG (LiR$^3$AG) to enable non-reasoning models to transfer reasoning strategies by restructuring retrieved evidence into coherent reasoning chains. LiR$^3$AG significantly reduce the average 98% output tokens overhead and 58.6% inferencing time while improving 8B non-reasoning model's F1 performance ranging from 6.2% to 22.5% to surpass the performance of 32B reasoning model in RAG, offering a practical and efficient path forward for RAG systems.

</details>


### [18] [Towards Efficient Agents: A Co-Design of Inference Architecture and System](https://arxiv.org/abs/2512.18337)
*Weizhe Lin,Hui-Ling Zhen,Shuai Yang,Xian Wang,Renxi Liu,Hanting Chen,Wangze Zhang,Chuansai Zhou,Yiming Li,Chen Chen,Xing Li,Zhiyuan Yang,Xiaosong Li,Xianzhi Yu,Zhenhua Dong,Mingxuan Yuan,Yunhe Wang*

Main category: cs.CL

TL;DR: 本文针对大语言模型代理系统的系统性延迟问题，提出了AgentInfer框架，通过多模块协同优化实现了推理加速和效率提升，显著提升了多轮推理任务的响应速度和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型代理在多轮自主推理和工具辅助决策中效率低下，主要因推理循环、上下文增长和多样化工具交互带来的系统性延迟，阻碍了其实际部署。

Method: 提出了AgentInfer统一框架，包括AgentCollab（分层双模型推理）、AgentSched（缓存感知混合调度）、AgentSAM（基于后缀自动机的猜测解码）、AgentCompress（语义压缩机制）四个协同模块，整合推理优化与架构设计，实现端到端智能体加速。

Result: 在BrowseComp-zh和DeepDiver两个基准测试中，AgentInfer通过四个模块的协同，将无效token消耗减少50%以上，整体推理速度提升1.8~2.5倍，同时保持推理准确度。

Conclusion: 优化智能体任务完成效率而非单token吞吐量，是构建可扩展、高效且自我进化智能系统的关键。AgentInfer框架有效提升了多轮推理的效率和稳定性，推动了LLM代理的现实应用。

Abstract: The rapid development of large language model (LLM)-based agents has unlocked new possibilities for autonomous multi-turn reasoning and tool-augmented decision-making. However, their real-world deployment is hindered by severe inefficiencies that arise not from isolated model inference, but from the systemic latency accumulated across reasoning loops, context growth, and heterogeneous tool interactions. This paper presents AgentInfer, a unified framework for end-to-end agent acceleration that bridges inference optimization and architectural design. We decompose the problem into four synergistic components: AgentCollab, a hierarchical dual-model reasoning framework that balances large- and small-model usage through dynamic role assignment; AgentSched, a cache-aware hybrid scheduler that minimizes latency under heterogeneous request patterns; AgentSAM, a suffix-automaton-based speculative decoding method that reuses multi-session semantic memory to achieve low-overhead inference acceleration; and AgentCompress, a semantic compression mechanism that asynchronously distills and reorganizes agent memory without disrupting ongoing reasoning. Together, these modules form a Self-Evolution Engine capable of sustaining efficiency and cognitive stability throughout long-horizon reasoning tasks. Experiments on the BrowseComp-zh and DeepDiver benchmarks demonstrate that through the synergistic collaboration of these methods, AgentInfer reduces ineffective token consumption by over 50%, achieving an overall 1.8-2.5 times speedup with preserved accuracy. These results underscore that optimizing for agentic task completion-rather than merely per-token throughput-is the key to building scalable, efficient, and self-improving intelligent systems.

</details>


### [19] [LLM-based Few-Shot Early Rumor Detection with Imitation Agent](https://arxiv.org/abs/2512.18352)
*Fengzhu Zeng,Qian Shao,Ling Cheng,Wei Gao,Shih-Fen Cheng,Jing Ma,Cheng Niu*

Main category: cs.CL

TL;DR: 提出一种结合自主智能体与LLM的少样本早期谣言检测方法，实现无需训练LLM且提升准确率与提前量。


<details>
  <summary>Details</summary>
Motivation: LLM虽在少样本NLP任务中表现良好，但不适合时间序列数据且计算成本高。早期谣言检测在数据稀缺情况下尤为困难。

Method: 提出了一个结合自主智能体与基于大型语言模型（LLM）的传播谣言检测模型的早期谣言检测框架。智能体负责确定最早检测时间点，LLM负责谣言检测。

Result: 在四个真实数据集上，所提方法提升了各类LLM的性能，且在准确率和检测提前量方面均优于现有方法。

Conclusion: 所提框架有效解决少样本早期谣言检测问题，智能体实现早期时间点判定，LLM实现高效谣言检测，性能显著优于现有方法。

Abstract: Early Rumor Detection (EARD) aims to identify the earliest point at which a claim can be accurately classified based on a sequence of social media posts. This is especially challenging in data-scarce settings. While Large Language Models (LLMs) perform well in few-shot NLP tasks, they are not well-suited for time-series data and are computationally expensive for both training and inference. In this work, we propose a novel EARD framework that combines an autonomous agent and an LLM-based detection model, where the agent acts as a reliable decision-maker for \textit{early time point determination}, while the LLM serves as a powerful \textit{rumor detector}. This approach offers the first solution for few-shot EARD, necessitating only the training of a lightweight agent and allowing the LLM to remain training-free. Extensive experiments on four real-world datasets show our approach boosts performance across LLMs and surpasses existing EARD methods in accuracy and earliness.

</details>


### [20] [DACE For Railway Acronym Disambiguation](https://arxiv.org/abs/2512.18357)
*El Mokhtar Hribach,Oussama Mechhour,Mohammed Elmonstaser,Yassine El Boudouri,Othmane Kabal*

Main category: cs.CL

TL;DR: 本文提出DACE框架，结合动态提示和外部知识提升大语言模型在缩略词消歧任务中的表现，取得比赛冠军成绩。


<details>
  <summary>Details</summary>
Motivation: 技术文本处理中缩略词消歧存在高歧义性，自动化分析复杂，尤其在专业领域如法国铁路文档中尤为突出。

Method: 提出了DACE框架，利用动态提示、检索增强生成、上下文选择和集成聚合，通过自适应上下文学习和外部领域知识注入提升大语言模型的性能。

Result: DACE框架通过动态调整提示和集成预测，减少了幻觉现象，提升了低资源场景下的表现，在TextMine'26比赛中以0.9069的F1分数获得第一名。

Conclusion: DACE有效缓解了技术文本中缩略词歧义问题，特别是在资源有限的专业领域，实现了性能的显著提升。

Abstract: Acronym Disambiguation (AD) is a fundamental challenge in technical text processing, particularly in specialized sectors where high ambiguity complicates automated analysis. This paper addresses AD within the context of the TextMine'26 competition on French railway documentation. We present DACE (Dynamic Prompting, Retrieval Augmented Generation, Contextual Selection, and Ensemble Aggregation), a framework that enhances Large Language Models through adaptive in-context learning and external domain knowledge injection. By dynamically tailoring prompts to acronym ambiguity and aggregating ensemble predictions, DACE mitigates hallucination and effectively handles low-resource scenarios. Our approach secured the top rank in the competition with an F1 score of 0.9069.

</details>


### [21] [LLM Agents Implement an NLG System from Scratch: Building Interpretable Rule-Based RDF-to-Text Generators](https://arxiv.org/abs/2512.18360)
*Mateusz Lango,Ondřej Dušek*

Main category: cs.CL

TL;DR: 通过多LLM代理合作生成规则代码，创建无监督、可解释且高效的RDF到文本生成系统，显著减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 旨在解决传统基于超参数优化的生成模型需要大量标注数据且生成结果可能出现幻觉的问题。

Method: 提出一个神经符号框架，通过多个大型语言模型（LLM）代理的协作交互生成基于规则的Python代码，从而实现RDF到文本的生成，无需传统的反向传播训练。

Result: 该方法生成的文本几乎即时，使用单CPU即可运行，且实验表明在WebNLG和OpenDialKG数据集上生成文本的幻觉现象显著减少，仅有轻微的流畅性损失。

Conclusion: 该神经符号生成框架无需监督数据，具有高效性和良好的可解释性，在保持生成质量的同时降低幻觉现象。

Abstract: We present a novel neurosymbolic framework for RDF-to-text generation, in which the model is "trained" through collaborative interactions among multiple LLM agents rather than traditional backpropagation. The LLM agents produce rule-based Python code for a generator for the given domain, based on RDF triples only, with no in-domain human reference texts. The resulting system is fully interpretable, requires no supervised training data, and generates text nearly instantaneously using only a single CPU. Our experiments on the WebNLG and OpenDialKG data show that outputs produced by our approach reduce hallucination, with only slight fluency penalties compared to finetuned or prompted language models

</details>


### [22] [SRS-Stories: Vocabulary-constrained multilingual story generation for language learning](https://arxiv.org/abs/2512.18362)
*Wiktor Kamzela,Mateusz Lango,Ondrej Dusek*

Main category: cs.CL

TL;DR: 本文通过大型语言模型生成个性化语言学习故事，结合间隔重复系统优化词汇学习，效果优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 为语言学习者提供仅使用已知词汇的个性化故事，通过上下文教学新词汇并复习已学词汇，提高学习效果及阅读兴趣。

Method: 利用大型语言模型生成个性化故事，结合间隔重复系统优化词汇学习，评估三种故事生成方法和词汇约束策略。

Result: 生成的故事在语法、连贯性及词汇使用示例方面优于标准受限束搜索方法生成的文本。

Conclusion: 生成的个性化故事更具语法准确性和连贯性，能更有效地辅助词汇学习，适用于多种语言。

Abstract: In this paper, we use large language models to generate personalized stories for language learners, using only the vocabulary they know. The generated texts are specifically written to teach the user new vocabulary by simply reading stories where it appears in context, while at the same time seamlessly reviewing recently learned vocabulary. The generated stories are enjoyable to read and the vocabulary reviewing/learning is optimized by a Spaced Repetition System. The experiments are conducted in three languages: English, Chinese and Polish, evaluating three story generation methods and three strategies for enforcing lexical constraints. The results show that the generated stories are more grammatical, coherent, and provide better examples of word usage than texts generated by the standard constrained beam search approach

</details>


### [23] [AraToken: Optimizing Arabic Tokenization with Normalization Pipeline and Language Extension for Qwen3](https://arxiv.org/abs/2512.18399)
*Mark Kashirskiy,Artiom Lipinski,Ilya Makarov*

Main category: cs.CL

TL;DR: 本文提出了一种针对阿拉伯语的优化分词器AraToken及其集成方法LEP，有效提升阿拉伯语大语言模型的训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有通用分词器主要针对英语和拉丁文字训练，对阿拉伯语等形态丰富语言表现不佳，导致分词时词令数量增加，影响训练效率和模型性能。

Method: 使用SentencePiece Unigram算法，结合针对阿拉伯语的规范化处理（包括Alif变体、重音符号和阿拉伯-印度数字）构建了专门优化的阿拉伯语分词器AraToken；系统对比BPE、WordPiece和SentencePiece分词算法，提出了Language Extension Pipeline（LEP），通过词汇扩展、平均子词初始化和选择性Transformer层解冻，将优化后的分词器集成到Qwen3-0.6B模型中。

Result: AraToken在词令序列上比未规范化的方法减少了18%的词令生成率（1.199 vs. 1.35词令/词），通过LEP方法将优化分词器集成进模型后，在仅用10万条阿拉伯语样本进行800步训练后，使评估损失从8.28降到2.43。

Conclusion: AraToken及其规范化处理显著降低了阿拉伯语词令冗余，结合LEP方法成功集成至大语言模型，实现了训练损失的大幅下降，促进了阿拉伯语NLP研究的发展。

Abstract: Tokenization is a critical preprocessing step for large language models (LLMs), directly impacting training efficiency and downstream performance. General-purpose tokenizers trained predominantly on English and Latin-script languages exhibit suboptimal performance on morphologically rich languages such as Arabic, resulting in inflated token sequences and reduced compression efficiency. In this work, we present AraToken, an Arabic-optimized tokenizer built on SentencePiece Unigram algorithm with a comprehensive normalization pipeline addressing Arabic-specific orthographic variations including Alif variants, diacritics, and Arabic-Indic numerals. We systematically compare BPE, WordPiece, and SentencePiece algorithms across multiple configurations, demonstrating that SentencePiece with normalization achieves 18% lower fertility (1.199 vs 1.35 tokens/word) compared to unnormalized baselines. Furthermore, we introduce the Language Extension Pipeline (LEP), a method for integrating the optimized tokenizer into Qwen3-0.6B through vocabulary extension with mean subtoken initialization and selective transformer layer unfreezing. Our experiments show that LEP reduces evaluation loss from 8.28 to 2.43 within 800 training steps on 100K Arabic samples. We release our tokenizer, training scripts, and model checkpoints to facilitate Arabic NLP research.

</details>


### [24] [An Agentic AI Framework for Training General Practitioner Student Skills](https://arxiv.org/abs/2512.18440)
*Victor De Marez,Jens Van Nooten,Luna De Bruyne,Walter Daelemans*

Main category: cs.CL

TL;DR: 本文提出并验证了一个基于代理的虚拟模拟病人框架，实现了真实场景生成、稳定角色扮演和结构化反馈，提升了医学生培训的效果和体验。


<details>
  <summary>Details</summary>
Motivation: 当前虚拟模拟病人在医学准确性、角色扮演一致性、场景生成及结构化反馈方面存在不足，限制了其在医学教育中的应用效果。

Method: 提出了一个代理框架，结合（i）基于证据的可配置病例生成，（ii）控制角色驱动的患者对话（可选检索支持），（iii）基于标准的沟通和临床推理评估及反馈，实现虚拟模拟病人的训练工具。

Result: 在与14名医学生的交互式语音会诊中，系统表现出对话真实、病例忠实、难度适中、角色稳定且反馈具体丰富，整体易用性优异。

Conclusion: 该代理框架有效分离了场景控制、交互控制和基于标准的评估，展示了构建可靠且具教育价值的虚拟模拟病人训练工具的实用策略。

Abstract: Advancements in large language models offer strong potential for enhancing virtual simulated patients (VSPs) in medical education by providing scalable alternatives to resource-intensive traditional methods. However, current VSPs often struggle with medical accuracy, consistent roleplaying, scenario generation for VSP use, and educationally structured feedback. We introduce an agentic framework for training general practitioner student skills that unifies (i) configurable, evidence-based vignette generation, (ii) controlled persona-driven patient dialogue with optional retrieval grounding, and (iii) standards-based assessment and feedback for both communication and clinical reasoning. We instantiate the framework in an interactive spoken consultation setting and evaluate it with medical students ($\mathbf{N{=}14}$). Participants reported realistic and vignette-faithful dialogue, appropriate difficulty calibration, a stable personality signal, and highly useful example-rich feedback, alongside excellent overall usability. These results support agentic separation of scenario control, interaction control, and standards-based assessment as a practical pattern for building dependable and pedagogically valuable VSP training tools.

</details>


### [25] [Mitigating Spurious Correlations in NLI via LLM-Synthesized Counterfactuals and Dynamic Balanced Sampling](https://arxiv.org/abs/2512.18462)
*Christopher Román Jaimes*

Main category: cs.CL

TL;DR: 本文提出一种自动化、多步骤的方法改善自然语言推理模型的语义推理能力，有效提升模型一致性并防止灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 自然语言推理模型过于依赖伪相关而非语义推理，现有方案成本高且易导致微调时灾难性遗忘，需要自动化且可扩展的方法。

Method: 引入了Log-Frequency LMI (LF-LMI)用于精确检测语义伪相关，通过大型语言模型合成管道及多次判定生成高质量对比合成集，利用动态平衡采样策略轮换训练数据分布防止遗忘。

Result: 模型在一项挑战性基准测试上的一致性从63.5%提升至81.0%，同时保持88.4%的领域内准确率，远超简单微调方法。

Conclusion: 我们提出的自动化、可扩展的训练策略显著提升了自然语言推理模型对语义推理的一致性，避免了灾难性遗忘，并保持了较高的领域内准确率。

Abstract: Natural Language Inference (NLI) models frequently rely on spurious correlations rather than semantic reasoning. Existing mitigation strategies often incur high annotation costs or trigger catastrophic forgetting during fine-tuning. We propose an automated, scalable pipeline to address these limitations. First, we introduce Log-Frequency LMI (LF-LMI) to accurately detect semantic artifacts. Second, we generate a high-quality synthetic contrast set via an LLM-synthesis pipeline with multi-judge verification. Finally, we introduce Dynamic Balanced Sampling, a training strategy that rotates the original data distribution to prevent forgetting. Our method improves consistency on a challenging benchmark from 63.5% to 81.0% while maintaining 88.4% in-domain accuracy, significantly outperforming naive fine-tuning.

</details>


### [26] [Research on a hybrid LSTM-CNN-Attention model for text-based web content classification](https://arxiv.org/abs/2512.18475)
*Mykola Kuz,Ihor Lazarovych,Mykola Kozlenko,Mykola Pikuliak,Andrii Kvasniuk*

Main category: cs.CL

TL;DR: 本研究提出的结合LSTM、CNN与注意力机制的混合深度学习模型，利用预训练GloVe词嵌入，实现了高效且准确的网络文本内容分类，显著优于单一模型，展示了混合模型在自然语言处理中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 提高基于文本的网络内容分类的准确性和泛化能力，克服单一模型的局限性。

Method: 提出集成LSTM、CNN和注意力机制的混合深度学习架构，采用预训练的GloVe词嵌入表示文本，结合局部特征提取和长距离依赖建模，通过注意力机制聚焦关键信息。

Result: 该混合模型在5折交叉验证中表现优异，准确率达0.98，精确率0.94，召回率0.92，F1分数0.93，性能优于仅使用CNN、LSTM或基于BERT的分类器。

Conclusion: 混合深度学习架构有效融合了细粒度文本结构与广义语义信息，实现了文本分类的高性能和良好泛化，适合实时或近实时系统，支持在复杂非结构化文本分类任务中推广使用。

Abstract: This study presents a hybrid deep learning architecture that integrates LSTM, CNN, and an Attention mechanism to enhance the classification of web content based on text. Pretrained GloVe embeddings are used to represent words as dense vectors that preserve semantic similarity. The CNN layer extracts local n-gram patterns and lexical features, while the LSTM layer models long-range dependencies and sequential structure. The integrated Attention mechanism enables the model to focus selectively on the most informative parts of the input sequence. A 5-fold cross-validation setup was used to assess the robustness and generalizability of the proposed solution. Experimental results show that the hybrid LSTM-CNN-Attention model achieved outstanding performance, with an accuracy of 0.98, precision of 0.94, recall of 0.92, and F1-score of 0.93. These results surpass the performance of baseline models based solely on CNNs, LSTMs, or transformer-based classifiers such as BERT. The combination of neural network components enabled the model to effectively capture both fine-grained text structures and broader semantic context. Furthermore, the use of GloVe embeddings provided an efficient and effective representation of textual data, making the model suitable for integration into systems with real-time or near-real-time requirements. The proposed hybrid architecture demonstrates high effectiveness in text-based web content classification, particularly in tasks requiring both syntactic feature extraction and semantic interpretation. By combining presented mechanisms, the model addresses the limitations of individual architectures and achieves improved generalization. These findings support the broader use of hybrid deep learning approaches in NLP applications, especially where complex, unstructured textual data must be processed and classified with high reliability.

</details>


### [27] [Teaching and Critiquing Conceptualization and Operationalization in NLP](https://arxiv.org/abs/2512.18505)
*Vagrant Gautam*

Main category: cs.CL

TL;DR: 本文介绍了一门针对NLP核心抽象概念定义与测量问题的研讨课，帮助学生批判性理解和操作化这些概念。


<details>
  <summary>Details</summary>
Motivation: NLP领域的多个抽象概念虽被频繁使用，但缺乏一致且明确的定义，这导致评估方法和系统评判标准存在不确定性。

Method: 设计了一个面向学生的研讨课程，提供跨学科阅读材料，强调讨论和批判，引导学生探索概念的定义与操作化问题。

Result: 通过该研讨课程，学生能够更好地理解和批判这些抽象概念的含义及其测量方式，为未来研究提供更扎实的理论基础。

Conclusion: 自然语言处理（NLP）领域中常用的抽象概念如“可解释性”、“偏见”、“推理”和“刻板印象”缺乏明确的定义，这影响了相关研究的操作性和评价标准。该论文通过设计一个研讨课程，引导学生探讨这些概念的内涵及其测量方法，促进对这些关键术语的深入理解和批判性思考。

Abstract: NLP researchers regularly invoke abstract concepts like "interpretability," "bias," "reasoning," and "stereotypes," without defining them. Each subfield has a shared understanding or conceptualization of what these terms mean and how we should treat them, and this shared understanding is the basis on which operational decisions are made: Datasets are built to evaluate these concepts, metrics are proposed to quantify them, and claims are made about systems. But what do they mean, what should they mean, and how should we measure them? I outline a seminar I created for students to explore these questions of conceptualization and operationalization, with an interdisciplinary reading list and an emphasis on discussion and critique.

</details>


### [28] [Generalization Gaps in Political Fake News Detection: An Empirical Study on the LIAR Dataset](https://arxiv.org/abs/2512.18533)
*S Mahmudul Hasan,Shaily Roy,Akib Jawad Nafis*

Main category: cs.CL

TL;DR: 针对政治虚假信息的文本事实核查达到性能瓶颈，简单模型表现与复杂模型相当，模型过拟合严重且数据增强无效，提示需结合外部知识提升语义理解能力。


<details>
  <summary>Details</summary>
Motivation: 探讨自动化事实检查系统在处理语言细微的政治虚假信息时的极限。

Method: 在LIAR基准上系统评估了九种机器学习算法，分别分析词汇特征（词袋模型、TF-IDF）和语义嵌入（GloVe），并对比简单线性SVM与预训练Transformer模型的性能；此外分析树模型的泛化能力及使用SMOTE数据增强效果。

Result: 发现文本建模的性能上限较低，细粒度分类加权F1分数最高仅为0.32；简单线性SVM的准确率（0.624）与RoBERTa相近，表明模型容量不是瓶颈；树模型过拟合严重，训练准确率超过99%，测试仅约25%；SMOTE数据增强无效，说明限制为语义歧义而非数据分布。

Conclusion: 仅依赖文本处理和增加模型复杂度难以提升政治虚假信息的事实核查效果，需引入外部知识以突破语义理解瓶颈。

Abstract: The proliferation of linguistically subtle political disinformation poses a significant challenge to automated fact-checking systems. Despite increasing emphasis on complex neural architectures, the empirical limits of text-only linguistic modeling remain underexplored. We present a systematic diagnostic evaluation of nine machine learning algorithms on the LIAR benchmark. By isolating lexical features (Bag-of-Words, TF-IDF) and semantic embeddings (GloVe), we uncover a hard "Performance Ceiling", with fine-grained classification not exceeding a Weighted F1-score of 0.32 across models. Crucially, a simple linear SVM (Accuracy: 0.624) matches the performance of pre-trained Transformers such as RoBERTa (Accuracy: 0.620), suggesting that model capacity is not the primary bottleneck. We further diagnose a massive "Generalization Gap" in tree-based ensembles, which achieve more than 99% training accuracy but collapse to approximately 25% on test data, indicating reliance on lexical memorization rather than semantic inference. Synthetic data augmentation via SMOTE yields no meaningful gains, confirming that the limitation is semantic (feature ambiguity) rather than distributional. These findings indicate that for political fact-checking, increasing model complexity without incorporating external knowledge yields diminishing returns.

</details>


### [29] [Solver-Independent Automated Problem Formulation via LLMs for High-Cost Simulation-Driven Design](https://arxiv.org/abs/2512.18682)
*Yuchen Li,Handing Wang,Bing Xue,Mengjie Zhang,Yaochu Jin*

Main category: cs.CL

TL;DR: 本文提出了APF框架，利用大语言模型自动将工程师的自然语言设计需求转化为可执行的优化模型，解决了高成本仿真设计中需求表述难以准确数学建模的问题。


<details>
  <summary>Details</summary>
Motivation: 在高成本仿真驱动设计领域，将模糊设计需求转化为数学优化模型是瓶颈，现有方法依赖昂贵求解器反馈且表述不准确，亟需自动化解决方案。

Method: APF通过自动生成高质量数据集，结合数据生成和测试实例标注进行监督微调，增强大语言模型生成准确可执行优化模型的能力。

Result: APF在天线设计实验中显著超过现有方法，提升了需求形式化的准确度和优化结果的辐射效率。

Conclusion: APF框架显著提升了设计需求的数学表述准确性和优化结果的性能，优于现有方法。

Abstract: In the high-cost simulation-driven design domain, translating ambiguous design requirements into a mathematical optimization formulation is a bottleneck for optimizing product performance. This process is time-consuming and heavily reliant on expert knowledge. While large language models (LLMs) offer potential for automating this task, existing approaches either suffer from poor formalization that fails to accurately align with the design intent or rely on solver feedback for data filtering, which is unavailable due to the high simulation costs. To address this challenge, we propose APF, a framework for solver-independent, automated problem formulation via LLMs designed to automatically convert engineers' natural language requirements into executable optimization models. The core of this framework is an innovative pipeline for automatically generating high-quality data, which overcomes the difficulty of constructing suitable fine-tuning datasets in the absence of high-cost solver feedback with the help of data generation and test instance annotation. The generated high-quality dataset is used to perform supervised fine-tuning on LLMs, significantly enhancing their ability to generate accurate and executable optimization problem formulations. Experimental results on antenna design demonstrate that APF significantly outperforms the existing methods in both the accuracy of requirement formalization and the quality of resulting radiation efficiency curves in meeting the design goals.

</details>


### [30] [LLMs on Drugs: Language Models Are Few-Shot Consumers](https://arxiv.org/abs/2512.18546)
*Alexander Doudkin*

Main category: cs.CL

TL;DR: 不同的“药物”人格提示显著降低了大型语言模型在推理任务中的准确率，提示文本作为一种“少样本消耗品”，能破坏模型的可靠性而不修改模型权重。


<details>
  <summary>Details</summary>
Motivation: 探索提示级“药物”干预对大型语言模型推理时表现的影响，填补此类干预缺乏严格基准测试的空白。

Method: 对GPT-5-mini模型，使用四种单句提示（LSD、可卡因、酒精、大麻）对比无药物控制组，通过100个验证样本在ARC-Challenge任务中进行测试，并采用确定性解码、完整日志记录、Wilson置信区间和Fisher精确检验。

Result: 控制组准确率为0.45，酒精组下降至0.10（p=3.2e-8），可卡因组为0.21（p=4.9e-4），LSD组为0.19（p=1.3e-4），大麻组为0.30（p=0.041），主要因为人格提示干扰了预期的答案格式。

Conclusion: 提示层面的人格文本能严重干扰大型语言模型的回答质量，表明即使不改变模型参数，提示设计也对模型表现有决定性影响。

Abstract: Large language models (LLMs) are sensitive to the personas imposed on them at inference time, yet prompt-level "drug" interventions have never been benchmarked rigorously. We present the first controlled study of psychoactive framings on GPT-5-mini using ARC-Challenge. Four single-sentence prompts -- LSD, cocaine, alcohol, and cannabis -- are compared against a sober control across 100 validation items per condition, with deterministic decoding, full logging, Wilson confidence intervals, and Fisher exact tests. Control accuracy is 0.45; alcohol collapses to 0.10 (p = 3.2e-8), cocaine to 0.21 (p = 4.9e-4), LSD to 0.19 (p = 1.3e-4), and cannabis to 0.30 (p = 0.041), largely because persona prompts disrupt the mandated "Answer: <LETTER>" template. Persona text therefore behaves like a "few-shot consumable" that can destroy reliability without touching model weights. All experimental code, raw results, and analysis scripts are available at https://github.com/lexdoudkin/llms-on-drugs.

</details>


### [31] [FASTRIC: Prompt Specification Language for Verifiable LLM Interactions](https://arxiv.org/abs/2512.18940)
*Wen-Long Jin*

Main category: cs.CL

TL;DR: 本文提出了FASTRIC，一种利用自然语言提示将隐式有限状态机明确化的提示规范语言，通过执行轨迹分析实现对大型语言模型多轮交互执行的合规性验证。


<details>
  <summary>Details</summary>
Motivation: 目前大型语言模型执行复杂多轮交互缺乏形式化规范，难以验证其执行是否符合设计者意图，亟需一种高效且易用的规范语言和验证手段。

Method: 通过设计包含七个有限状态机元素的FASTRIC语言，利用LLM作为统一的解析器、解释器和执行环境，实现对多轮交互协议的显式规范和执行合规性检测。并以一个三状态幼儿园辅导有限状态机为例，在四个规范形式化层级和三种模型规模（14.7B、685B、1T+）下进行测试。

Result: 685B参数的DeepSeek-V3.2模型在L2-L4层级达成完美合规性（1.00），ChatGPT-5（约1T参数）在L3最优（0.90）但L4显著下降（0.39），14.7B的Phi4波动较大无稳定最佳点，证明了模型能力决定最佳规范形式化层级。

Conclusion: 研究发现不同模型在不同形式化规范层级下表现各异，存在特定的"金发女孩区间"，即规范足够提供结构但不过度约束的最佳区间，从而推动多轮交互设计由经验艺术向系统工程转变。

Abstract: Large Language Models (LLMs) execute complex multi-turn interaction protocols but lack formal specifications to verify execution against designer intent. We introduce FASTRIC, a Prompt Specification Language that makes implicit Finite State Machines (FSMs) explicit in natural language prompts, enabling conformance verification through execution trace analysis. The LLM serves as intelligent execution agent: interpreting designer-encoded FSMs to execute specified behavioral roles. Unlike symbolic specification languages requiring parsers and compilers, FASTRIC leverages LLMs as unified infrastructure-simultaneously parser, interpreter, runtime environment, and development assistant. FASTRIC guides designers to articulate seven FSM elements (Final States, Agents, States, Triggers, Roles, Initial State, Constraints) structuring multi-turn interactions. Specification formality-ranging from implicit descriptions that frontier models infer to explicit step-by-step instructions for weaker models-serves as a design parameter. We introduce procedural conformance as verification metric measuring execution adherence to FSM specifications. Testing a 3-state kindergarten tutoring FSM across four formality levels and three model scales (14.7B, 685B, 1T+ parameters) reveals optimal specification formality is a function of model capacity. DeepSeek-V3.2 (685B) achieves perfect conformance (1.00) at L2-L4; ChatGPT-5 (~1T) peaks at L3 (0.90) before collapsing at L4 (0.39); Phi4 (14.7B) shows no stable optimum with high variance (SD=0.16-0.36). These findings reveal model-specific formality ranges-"Goldilocks zones"-where specifications provide sufficient structure without over-constraint, establishing Prompt Specification Engineering for creating verifiable interaction protocols, transforming multi-turn interaction design from heuristic art to systematic engineering with measurable procedural guarantees.

</details>


### [32] [Neologism Learning as a Parameter-Efficient Alternative to Fine-Tuning for Model Steering](https://arxiv.org/abs/2512.18551)
*Sungjoon Park,Varun Ramamurthi,Owen Terry*

Main category: cs.CL

TL;DR: 本文提出了一种通过学习新词来调整语言模型行为的方法，该方法相比微调更高效且性能更优。


<details>
  <summary>Details</summary>
Motivation: 现有微调方法计算资源消耗大且灵活性差，本文提出学习新词作为更有效的行为引导手段。

Method: 通过训练新词只调整少量参数，引导模型行为；与低秩适应微调在相同数据和超参数配置下进行性能比较。

Result: 本文研究了语言模型中"新词"的学习方法，即通过训练新词汇来表示模型词汇表中尚未包含的概念。相比传统的低秩适应(LoRA)微调方法，新词学习仅训练少量参数，提供了更灵活的行为引导手段。实验结果显示，在相同的数据和超参数设置下，新词学习在性能上优于微调模型。研究还发现模型在处理新词时有时会自行创造新词汇。

Conclusion: 新词学习方法在相同条件下优于低秩适应微调，且能灵活引导模型行为，同时模型有能力自行创造新词。

Abstract: In language modeling, neologisms are new tokens trained to represent a concept not already included in a given model's vocabulary. Neologisms can be used to encourage specific behavior in models, for example by appending prompts with "Give me a neologism answer." Behavioral steering can also be achieved through fine-tuning, albeit with more compute and less flexibility: learning a neologism only trains d parameters and allows the user to still access the model's default behavior. We compare the performance of neologism learning against low-rank adaptation (LoRA) fine-tuning, finding that neologisms outperform fine-tuned models under a matched training setup (same data and hyperparameters). We also investigate self-verbalizations of neologisms, and observe that the model will occasionally make up its own new words when asked about a neologism.

</details>


### [33] [From Scratch to Fine-Tuned: A Comparative Study of Transformer Training Strategies for Legal Machine Translation](https://arxiv.org/abs/2512.18593)
*Amit Barman,Atanu Mandal,Sudip Kumar Naskar*

Main category: cs.CL

TL;DR: 本文针对印度多语言环境中的法律文本翻译难题，提出基于Transformer的英印法律机器翻译方法，通过领域自适应微调预训练模型和从头训练两种策略，最终微调模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 印度等多语言国家的法律信息主要以英语存在，语言障碍限制了法律信息的普及，法律机器翻译技术提供了提升法律文档可及性和透明度的可行方案。

Method: 采用两种方法：1）在预训练的OPUS-MT模型基础上进行法律领域的微调；2）利用提供的法律语料从零训练Transformer模型，通过多种标准机器翻译指标对比评估性能。

Result: 微调后的OPUS-MT模型在多项机器翻译评测指标中表现优异，SacreBLEU分数达到46.03，显著优于基线和自行训练的模型，验证了领域自适应的重要性。

Conclusion: 领域自适应微调OPUS-MT模型显著提升了英印法律文本翻译质量，优于基线和从零训练模型，展示了法律机器翻译在多语言国家促进司法可及性的潜力。

Abstract: In multilingual nations like India, access to legal information is often hindered by language barriers, as much of the legal and judicial documentation remains in English. Legal Machine Translation (L-MT) offers a scalable solution to this challenge by enabling accurate and accessible translations of legal documents. This paper presents our work for the JUST-NLP 2025 Legal MT shared task, focusing on English-Hindi translation using Transformer-based approaches. We experiment with 2 complementary strategies, fine-tuning a pre-trained OPUS-MT model for domain-specific adaptation and training a Transformer model from scratch using the provided legal corpus. Performance is evaluated using standard MT metrics, including SacreBLEU, chrF++, TER, ROUGE, BERTScore, METEOR, and COMET. Our fine-tuned OPUS-MT model achieves a SacreBLEU score of 46.03, significantly outperforming both baseline and from-scratch models. The results highlight the effectiveness of domain adaptation in enhancing translation quality and demonstrate the potential of L-MT systems to improve access to justice and legal transparency in multilingual contexts.

</details>


### [34] [On Finding Inconsistencies in Documents](https://arxiv.org/abs/2512.18601)
*Charles J. Lovering,Seth Ebner,Brandon Smock,Michael Krumdick,Saad Rabbani,Ahmed Muhammad,Varshini Reddy,Chris Tanner*

Main category: cs.CL

TL;DR: 本文通过FIND基准测试评估语言模型检测文档不一致性的能力，gpt-5表现优异但仍有改进空间，说明该任务仍具挑战性。


<details>
  <summary>Details</summary>
Motivation: 人工审核文档耗时耗力，不一致性会带来严重后果，利用语言模型加速并提升准确度。

Method: 引入FIND基准测试，手动插入不一致性，由不同模型检测，评估模型表现。

Result: 最佳模型gpt-5检测出64%插入的不一致性，并发现原始文档中未被发现的不一致性，但仍漏检近一半。

Conclusion: 语言模型在文档不一致性检测方面表现出一定能力，但仍存在较大检测盲区。

Abstract: Professionals in academia, law, and finance audit their documents because inconsistencies can result in monetary, reputational, and scientific costs. Language models (LMs) have the potential to dramatically speed up this auditing process. To understand their abilities, we introduce a benchmark, FIND (Finding INconsistencies in Documents), where each example is a document with an inconsistency inserted manually by a domain expert. Despite the documents being long, technical, and complex, the best-performing model (gpt-5) recovered 64% of the inserted inconsistencies. Surprisingly, gpt-5 also found undiscovered inconsistencies present in the original documents. For example, on 50 arXiv papers, we judged 136 out of 196 of the model's suggestions to be legitimate inconsistencies missed by the original authors. However, despite these findings, even the best models miss almost half of the inconsistencies in FIND, demonstrating that inconsistency detection is still a challenging task.

</details>


### [35] [A Comparative Study of Light-weight Language Models for PII Masking and their Deployment for Real Conversational Texts](https://arxiv.org/abs/2512.18608)
*Prabigya Acharya,Liza Shrestha*

Main category: cs.CL

TL;DR: 本文通过微调两种轻量级模型，实现了与大型语言模型相当的PII自动屏蔽效果，分析了不同模型在性能和效率上的权衡，展示轻量级模型在隐私保护中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在个人身份信息(PII)自动屏蔽上表现良好，但存在数据处理和计算成本高的问题，因此探索轻量级模型是否能达到类似性能。

Method: 通过微调两个轻量级模型（T5-small和Mistral-Instruct-v0.3），并使用基于AI4Privacy基准构建的多种英语数据集，设计不同的数据集变体以研究标签标准化和PII表示方法，使用实体级和字符级指标进行评估。

Result: 两种轻量级模型在PII屏蔽任务上表现与前沿大型模型相当，标签规范化提升了性能。Mistral在F1值和召回率上表现更佳且更稳健，但生成延迟较高；T5在口语文本上表现稍差，但具备更好的结构化输出可控性和更低推理成本。

Conclusion: 轻量级模型在准确性、稳健性和计算效率之间提供了良好的权衡，能够有效完成PII屏蔽任务，且缓解了前沿大型模型存在的数据处理问题。

Abstract: Automated masking of Personally Identifiable Information (PII) is critical for privacy-preserving conversational systems. While current frontier large language models demonstrate strong PII masking capabilities, concerns about data handling and computational costs motivate exploration of whether lightweight models can achieve comparable performance. We compare encoder-decoder and decoder-only architectures by fine-tuning T5-small and Mistral-Instruct-v0.3 on English datasets constructed from the AI4Privacy benchmark. We create different dataset variants to study label standardization and PII representation, covering 24 standardized PII categories and higher-granularity settings. Evaluation using entity-level and character-level metrics, type accuracy, and exact match shows that both lightweight models achieve performance comparable to frontier LLMs for PII masking tasks. Label normalization consistently improves performance across architectures. Mistral achieves higher F1 and recall with greater robustness across PII types but incurs significantly higher generation latency. T5, while less robust in conversational text, offers more controllable structured outputs and lower inference cost, motivating its use in a real-time Discord bot for real-world PII redaction. Evaluation on live messages reveals performance degradation under informal inputs. These results clarify trade-offs between accuracy, robustness, and computational efficiency, demonstrating that lightweight models can provide effective PII masking while addressing data handling concerns associated with frontier LLMs.

</details>


### [36] [LLM-CAS: Dynamic Neuron Perturbation for Real-Time Hallucination Correction](https://arxiv.org/abs/2512.18623)
*Jensen Zhang,Ningyuan Liu,Yijia Fan,Zihao Huang,Qinglin Zeng,Kaitong Cai,Jian Wang,Keze Wang*

Main category: cs.CL

TL;DR: LLM-CAS利用层次强化学习，动态调整神经元状态，在不改动模型参数的情况下提升大型语言模型的事实准确度，优于现有纠错技术。


<details>
  <summary>Details</summary>
Motivation: 现有方法数据需求大、计算成本高或难以处理依赖上下文的错误和灾难性遗忘，亟需高效且上下文感知的纠错机制。

Method: 将实时幻觉纠正问题形式化为分层强化学习，训练智能体学习基于当前上下文动态选择临时神经元扰动的策略，在推理阶段进行细粒度纠正，无需永久修改参数。

Result: 在多个基准测试（StoryCloze、TriviaQA及TruthfulQA MC1分数）中，LLM-CAS分别提升了10.98%、2.71%和2.06%，效果优于静态编辑方法ITI、CAA及动态框架SADI。

Conclusion: LLM-CAS框架有效提升了大型语言模型的事实准确性，超过了现有的静态和动态参数调整方法，具备高效且依赖上下文的纠错能力。

Abstract: Large language models (LLMs) often generate hallucinated content that lacks factual or contextual grounding, limiting their reliability in critical applications. Existing approaches such as supervised fine-tuning and reinforcement learning from human feedback are data intensive and computationally expensive, while static parameter editing methods struggle with context dependent errors and catastrophic forgetting.
  We propose LLM-CAS, a framework that formulates real-time hallucination correction as a hierarchical reinforcement learning problem. LLM-CAS trains an agent to learn a policy that dynamically selects temporary neuron perturbations during inference based on the current context. Unlike prior dynamic approaches that rely on heuristic or predefined adjustments, this policy driven mechanism enables adaptive and fine grained correction without permanent parameter modification.
  Experiments across multiple language models demonstrate that LLM-CAS consistently improves factual accuracy, achieving gains of 10.98 percentage points on StoryCloze, 2.71 points on TriviaQA, and 2.06 points on the MC1 score of TruthfulQA. These results outperform both static editing methods such as ITI and CAA and the dynamic SADI framework. Overall, LLM-CAS provides an efficient and context aware solution for improving the reliability of LLMs, with promising potential for future multimodal extensions.

</details>


### [37] [Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital](https://arxiv.org/abs/2512.18658)
*Pierre Colombo,Malik Boudiaf,Allyn Sweet,Michael Desa,Hongxi Wang,Kevin Candra,Syméon del Marmol*

Main category: cs.CL

TL;DR: 本文分析了资本化表校验在风险投资融资中的法律尽职调查过程，指出现有大型语言模型在多文档推理和证据可追溯性方面存在不足，提出并设计了一种世界模型架构以实现自动化。


<details>
  <summary>Details</summary>
Motivation: 在风险投资融资轮次中，律师需要核实各种证券和发行条款的法律支持文档，但当前的语言模型无法稳定处理此类需多文档推理和证据追踪的复杂法律工作流程。

Method: 通过将资本化表校验定义为法律AI的真实世界基准，分析和比较现有智能代理系统的性能，进而提出一种世界模型架构，旨在支持多文档推理和结果的确定性输出。

Result: 提出的世界模型架构为资本化表校验自动化提供了方向，显示出提高应用法律智能的潜力。

Conclusion: 现有的智能代理系统尚未能稳定完成资本化表校验任务，本文通过提出世界模型架构，为实现该任务自动化及推动应用法律智能奠定基础。

Abstract: Before closing venture capital financing rounds, lawyers conduct diligence that includes tying out the capitalization table: verifying that every security (for example, shares, options, warrants) and issuance term (for example, vesting schedules, acceleration triggers, transfer restrictions) is supported by large sets of underlying legal documentation. While LLMs continue to improve on legal benchmarks, specialized legal workflows, such as capitalization tie-out, remain out of reach even for strong agentic systems. The task requires multi-document reasoning, strict evidence traceability, and deterministic outputs that current approaches fail to reliably deliver. We characterize capitalization tie-out as an instance of a real-world benchmark for legal AI, analyze and compare the performance of existing agentic systems, and propose a world model architecture toward tie-out automation-and more broadly as a foundation for applied legal intelligence.

</details>


### [38] [From Natural Language to Control Signals: A Conceptual Framework for Semantic Channel Finding in Complex Experimental Infrastructure](https://arxiv.org/abs/2512.18779)
*Thorsten Hellert,Nikolay Agladze,Alex Giovannone,Jan Jug,Frank Mayet,Mark Sherwin,Antonin Sulc,Chris Tennant*

Main category: cs.CL

TL;DR: 本文提出了一个四范式框架，用于通过自然语言映射控制系统信号，实现复杂实验设施中语义通道定位。框架在不同规模和架构的四个实际设施中验证，准确率高达90-97%。


<details>
  <summary>Details</summary>
Motivation: 传统设施信号命名无一致规范，依赖非正式知识和分散文档，导致信号定位效率低下，限制了可靠性和扩展性。

Method: 提出四范式框架：直接查询通道字典、层级结构导航、交互式代理探索和本体语义搜索，并在多种设施和架构上开发了原型实现。

Result: 在四个不同规模和体系的设施中实现方案，操作查询准确率达到90-97%。

Conclusion: 通过四种范式实现了从自然语言到控制信号的准确映射，提升了复杂实验设施中信号检索的可靠性和扩展性。

Abstract: Modern experimental platforms such as particle accelerators, fusion devices, telescopes, and industrial process control systems expose tens to hundreds of thousands of control and diagnostic channels accumulated over decades of evolution. Operators and AI systems rely on informal expert knowledge, inconsistent naming conventions, and fragmented documentation to locate signals for monitoring, troubleshooting, and automated control, creating a persistent bottleneck for reliability, scalability, and language-model-driven interfaces. We formalize semantic channel finding-mapping natural-language intent to concrete control-system signals-as a general problem in complex experimental infrastructure, and introduce a four-paradigm framework to guide architecture selection across facility-specific data regimes. The paradigms span (i) direct in-context lookup over curated channel dictionaries, (ii) constrained hierarchical navigation through structured trees, (iii) interactive agent exploration using iterative reasoning and tool-based database queries, and (iv) ontology-grounded semantic search that decouples channel meaning from facility-specific naming conventions. We demonstrate each paradigm through proof-of-concept implementations at four operational facilities spanning two orders of magnitude in scale-from compact free-electron lasers to large synchrotron light sources-and diverse control-system architectures, from clean hierarchies to legacy environments. These implementations achieve 90-97% accuracy on expert-curated operational queries.

</details>


### [39] [From Word to World: Can Large Language Models be Implicit Text-based World Models?](https://arxiv.org/abs/2512.18832)
*Yixia Li,Hongru Wang,Jiahao Qiu,Zhenfei Yin,Dongdong Zhang,Cheng Qian,Zeping Li,Pony Ma,Guanhua Chen,Heng Ji,Mengdi Wang*

Main category: cs.CL

TL;DR: 论文探讨了大型语言模型在文本环境中作为世界模型的有效性，提出了一个三层评估框架，并验证了其在提升智能体学习效率方面的表现及其局限性。


<details>
  <summary>Details</summary>
Motivation: 现实世界环境非自适应且难以扩展，传统经验驱动强化学习受限，利用大型语言模型构建世界模型可能提高学习效率，但其可靠性和适用条件尚不明朗。

Method: 引入了一个评估大型语言模型世界模型的三层框架：一致性与保真度、可扩展性与鲁棒性、智能体实用性，并在五个代表性文本环境中进行实证研究。

Result: 研究表明，经过充分训练的世界模型在保持潜在状态连贯性方面表现良好，模型性能随数据与规模提升有规律性增强，并在多个任务中显著提升智能体表现，但效果受限于环境复杂度和行为覆盖度。

Conclusion: 适当训练的大型语言模型作为世界模型可以保持一致状态，随着数据和模型规模增长表现稳定，能够通过验证动作、生成合成轨迹及启动强化学习等方式提升智能体性能，但其效用依赖于行为覆盖范围和环境复杂度。

Abstract: Agentic reinforcement learning increasingly relies on experience-driven scaling, yet real-world environments remain non-adaptive, limited in coverage, and difficult to scale. World models offer a potential way to improve learning efficiency through simulated experience, but it remains unclear whether large language models can reliably serve this role and under what conditions they meaningfully benefit agents. We study these questions in text-based environments, which provide a controlled setting to reinterpret language modeling as next-state prediction under interaction. We introduce a three-level framework for evaluating LLM-based world models: (i) fidelity and consistency, (ii) scalability and robustness, and (iii) agent utility. Across five representative environments, we find that sufficiently trained world models maintain coherent latent state, scale predictably with data and model size, and improve agent performance via action verification, synthetic trajectory generation, and warm-starting reinforcement learning. Meanwhile, these gains depend critically on behavioral coverage and environment complexity, delineating clear boundry on when world modeling effectively supports agent learning.

</details>


### [40] [AraMix: Recycling, Refiltering, and Deduplicating to Deliver the Largest Arabic Pretraining Corpus](https://arxiv.org/abs/2512.18834)
*Sultan Alrashed,Francesco Orabona*

Main category: cs.CL

TL;DR: AraMix通过整合七个公开阿拉伯语网络语料并去重过滤，建立了目前最大且高质量的阿拉伯语预训练语料库。


<details>
  <summary>Details</summary>
Motivation: 针对阿拉伯语预训练语料资源有限的问题，提出通过整合现有数据并去重来构建高质量的大规模语料库。

Method: 整合七个公开阿拉伯语数据集，设计针对阿语的质量过滤步骤，进行跨数据集的MinHash和句子级去重，系统性地利用已有数据。

Result: 构建了AraMix，包含约1780亿词、1.79亿文档的阿拉伯语去重预训练语料，发现不同语料中近60%的令牌是重复的，证明增强数据质量重于重新抓取。

Conclusion: 对低资源语言来说，投资数据整理与去重管道比重复抓取更多网页数据更有效。

Abstract: We present AraMix, a deduplicated Arabic pretraining corpus containing approximately 178 billion tokens across 179 million documents. Rather than scraping the web again, AraMix demonstrates that substantial value lies in systematically reusing and curating existing pretraining datasets: we combine seven publicly available Arabic web datasets, apply quality filtering designed specifically for Arabic text to re-filter some datasets, and perform cross-dataset deduplication, both MinHash and sentence-level. This approach reveals that nearly 60% of tokens across these independently collected corpora are duplicates, redundancy that any new scraping efforts will reproduce. Our work suggests that for lower resource languages, investment in curation pipelines for existing data yields greater returns than additional web crawls, an approach that allowed us to curate the largest heavily filtered publicly available Arabic pretraining corpus.

</details>


### [41] [MDToC: Metacognitive Dynamic Tree of Concepts for Boosting Mathematical Problem-Solving of Large Language Models](https://arxiv.org/abs/2512.18841)
*Tung Duong Ta,Tim Oates*

Main category: cs.CL

TL;DR: 本文提出MDToC元认知动态概念树方法，通过构建概念树和多数投票机制提高LLMs计算验证能力，在多个数学基准任务中表现优异，无需手工提示。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs的数学推理能力持续进步，但在计算验证环节仍存在挑战，亟需新的方法提升其准确性和可靠性。

Method: MDToC包括三个阶段：构建概念树，展示和验证每个概念的计算准确性，并通过多数投票评估竞争解答。

Result: 本论文提出了MDToC（元认知动态概念树）方法，解决大型语言模型（LLMs）在数学推理中计算验证的难题。该方法通过构建概念树、对每个概念进行准确计算验证，并利用多数投票评估竞争解答，从而有效提升计算验证能力。实验在CHAMP、MATH和Game-of-24等基准上展示了显著提升，GPT-4-Turbo分别取得58.1%、86.6%和85%的成绩，均超过现有最优方法5%以上，且无需手工设计提示。MDToC在各种基础模型上均优于现有提示方法，表明元认知计算验证策略是提升数学推理能力的有效方向。

Conclusion: MDToC方法显著提升了LLMs数学计算验证的准确性和鲁棒性，超越了现有顶尖方法，验证了元认知计算验证作为未来方向的潜力。

Abstract: Despite advances in mathematical reasoning capabilities, Large Language Models (LLMs) still struggle with calculation verification when using established prompting techniques. We present MDToC (Metacognitive Dynamic Tree of Concepts), a three-phase approach that constructs a concept tree, develops accuracy-verified calculations for each concept, and employs majority voting to evaluate competing solutions. Evaluations across CHAMP, MATH, and Game-of-24 benchmarks demonstrate our MDToC's effectiveness, with GPT-4-Turbo achieving 58.1\% on CHAMP, 86.6\% on MATH, and 85\% on Game-of-24 - outperforming GoT by 5\%, 5.4\%, and 4\% on all these tasks, respectively, without hand-engineered hints. MDToC consistently surpasses existing prompting methods across all backbone models, yielding improvements of up to 7.6\% over ToT and 6.2\% over GoT, establishing metacognitive calculation verification as a promising direction for enhanced mathematical reasoning.

</details>


### [42] [Toward Human-Centered AI-Assisted Terminology Work](https://arxiv.org/abs/2512.18859)
*Antonio San Martin*

Main category: cs.CL

TL;DR: 本文提出了一种以人为本的人工智能框架，强调人工智能应增强术语学家的能力，而非取而代之，呼吁在术语工作中融合高自动化与强人控，关注伦理和偏见缓解。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能的快速扩散虽提高效率，但有可能削弱专业自主权、加重偏见及破坏语言和概念多样性，因此需以人为中心的人工智能方法。

Method: 基于人工智能与翻译研究，构建了一个以人为中心的框架，围绕增强的术语学家、伦理AI及以人为中心的设计三个维度展开。

Result: 提出了强调人工智能与人为控制兼容、术语学家在偏见缓解中核心作用及以人为中心设计的重要性的人机协同框架。

Conclusion: 论文强调当前人工智能的采用选择不仅影响术语工作的实践，也关系到术语和专业知识的准确性、适切性和多样性的保护。

Abstract: The rapid diffusion of generative artificial intelligence is transforming terminology work. While this technology promises gains in efficiency, its unstructured adoption risks weakening professional autonomy, amplifying bias, and eroding linguistic and conceptual diversity. This paper argues that a human-centered approach to artificial intelligence has become a necessity for terminology work. Building on research in artificial intelligence and translation studies, it proposes a human-centered framework that conceptualizes artificial intelligence as a means of amplifying the terminologist's capabilities, rather than replacing them. The framework is organized around three interrelated dimensions: the augmented terminologist, ethical AI, and human-centered design. Together, these dimensions emphasize the compatibility of high automation with strong human control, the central role of terminologists in bias mitigation, and the importance of designing AI tools and workflows around the needs, values, and well-being of the terminologist. The paper concludes by stressing that current choices in AI adoption will shape not only terminological practice, but also the preservation of accuracy, adequacy, and diversity in terminology and specialized knowledge.

</details>


### [43] [Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction](https://arxiv.org/abs/2512.18880)
*Ming Li,Han Chen,Yunze Xiao,Jian Chen,Hong Jiao,Tianyi Zhou*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型在评估题目难度时与人类认知困难的对齐问题，发现模型规模增大并不保证难度判断准确，且模型难以模拟学生能力限制和自我认知受限。


<details>
  <summary>Details</summary>
Motivation: 准确估计题目难度对教育评估至关重要，但存在冷启动问题，且尚不清楚大型语言模型是否能感知人类学习者的认知困难。

Method: 通过对20多种模型在医学知识和数学推理等多个领域的大规模实证分析，评估人类与AI在难度感知上的一致性。

Result: 发现模型规模增大不利于与人类难度感知对齐，模型趋向机器共识且表现越好，难度估计越不准确，同时缺乏自我认知和反思能力。

Conclusion: 大型语言模型虽具备超人问题解决能力，但无法准确反映人类的认知挣扎，表明其在自动难度预测上的应用存在重大挑战。

Abstract: Accurate estimation of item (question or task) difficulty is critical for educational assessment but suffers from the cold start problem. While Large Language Models demonstrate superhuman problem-solving capabilities, it remains an open question whether they can perceive the cognitive struggles of human learners. In this work, we present a large-scale empirical analysis of Human-AI Difficulty Alignment for over 20 models across diverse domains such as medical knowledge and mathematical reasoning. Our findings reveal a systematic misalignment where scaling up model size is not reliably helpful; instead of aligning with humans, models converge toward a shared machine consensus. We observe that high performance often impedes accurate difficulty estimation, as models struggle to simulate the capability limitations of students even when being explicitly prompted to adopt specific proficiency levels. Furthermore, we identify a critical lack of introspection, as models fail to predict their own limitations. These results suggest that general problem-solving capability does not imply an understanding of human cognitive struggles, highlighting the challenge of using current models for automated difficulty prediction.

</details>


### [44] [Remedy-R: Generative Reasoning for Machine Translation Evaluation without Error Annotations](https://arxiv.org/abs/2512.18906)
*Shaomu Tan,Ryosuke Mitani,Ritvik Choudhary,Qiyu Wu,Toshiyuki Sekiya,Christof Monz*

Main category: cs.CL

TL;DR: 该论文提出了Remedy-R，一种通过强化学习训练的生成式机器翻译评价指标，具有可解释性和强鲁棒性，还能生成用于翻译改进的反馈，并基于此引入评估-修正流水线以提升翻译质量。


<details>
  <summary>Details</summary>
Motivation: 现有自动机器翻译评价指标缺乏可解释性，且在真实世界分布外输入上表现不佳，亟需一种既可解释又具鲁棒性的评价方法。

Method: 采用基于对偶翻译偏好的强化学习训练一个生成式MT评价指标，生成包含准确性、流畅性和完整性的分步分析，最终给出评分。

Result: Remedy-R在WMT22-24元评价中表现优异，通用到多语言场景，对分布外数据表现强健，生成的反馈可被用来迭代提升翻译质量，辅助多种翻译模型获得提升。

Conclusion: Remedy-R在翻译评价任务中表现出与顶尖指标相当的效果，且具备较强的跨语言和OOD鲁棒性，通过生成自我反思反馈促进翻译质量提升，证明了其实用价值。

Abstract: Over the years, automatic MT metrics have hillclimbed benchmarks and presented strong and sometimes human-level agreement with human ratings. Yet they remain black-box, offering little insight into their decision-making and often failing under real-world out-of-distribution (OOD) inputs. We introduce Remedy-R, a reasoning-driven generative MT metric trained with reinforcement learning from pairwise translation preferences, without requiring error-span annotations or distillation from closed LLMs. Remedy-R produces step-by-step analyses of accuracy, fluency, and completeness, followed by a final score, enabling more interpretable assessments. With only 60K training pairs across two language pairs, Remedy-R remains competitive with top scalar metrics and GPT-4-based judges on WMT22-24 meta-evaluation, generalizes to other languages, and exhibits strong robustness on OOD stress tests. Moreover, Remedy-R models generate self-reflective feedback that can be reused for translation improvement. Building on this finding, we introduce Remedy-R Agent, a simple evaluate-revise pipeline that leverages Remedy-R's evaluation analysis to refine translations. This agent consistently improves translation quality across diverse models, including Qwen2.5, ALMA-R, GPT-4o-mini, and Gemini-2.0-Flash, suggesting that Remedy-R's reasoning captures translation-relevant information and is practically useful.

</details>


### [45] [Evaluating the Challenges of LLMs in Real-world Medical Follow-up: A Comparative Study and An Optimized Framework](https://arxiv.org/abs/2512.18999)
*Jinyan Liu,Zikang Chen,Qinchuan Wang,Tan Xie,Heming Zheng,Xudong Lv*

Main category: cs.CL

TL;DR: 针对大型语言模型在医疗随访中表现不佳的问题，设计了模块化流程控制系统，提高了性能和效率。


<details>
  <summary>Details</summary>
Motivation: 直接使用端到端的大型语言模型（LLMs）处理医疗随访任务时，由于随访表格的复杂性，模型常出现对话流程失控和信息提取不准确的问题。

Method: 设计模块化管道，利用任务分解、语义聚类和对话流程管理替代端到端方法，实现对复杂表单的有效处理。

Result: 提出并比较了两种随访聊天机器人系统：基于端到端LLM的系统和基于模块化流程控制的系统。实验表明，模块化方法显著提升了对话稳定性和信息提取准确率，同时减少了对话轮数（46.73%）和Token消耗（80%-87.5%）。

Conclusion: 在重要的医疗随访场景中，集成外部流程控制机制对LLM部署至关重要，能够显著提升系统的稳定性和准确性。

Abstract: When applied directly in an end-to-end manner to medical follow-up tasks, Large Language Models (LLMs) often suffer from uncontrolled dialog flow and inaccurate information extraction due to the complexity of follow-up forms. To address this limitation, we designed and compared two follow-up chatbot systems: an end-to-end LLM-based system (control group) and a modular pipeline with structured process control (experimental group). Experimental results show that while the end-to-end approach frequently fails on lengthy and complex forms, our modular method-built on task decomposition, semantic clustering, and flow management-substantially improves dialog stability and extraction accuracy. Moreover, it reduces the number of dialogue turns by 46.73% and lowers token consumption by 80% to 87.5%. These findings highlight the necessity of integrating external control mechanisms when deploying LLMs in high-stakes medical follow-up scenarios.

</details>


### [46] [Context-Aware Initialization for Reducing Generative Path Length in Diffusion Language Models](https://arxiv.org/abs/2512.19004)
*Tongyuan Miao,Gary Huang,Kai Jun Han,Annie Jiang*

Main category: cs.CL

TL;DR: 本文通过引入上下文感知的预热初始化，减少扩散大语言模型去噪迭代，加快推理速度，但揭示了准确率下降问题，提出后续研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统扩散语言模型需要大量去噪迭代，导致推理时间较长，降低了实用性。现有加速方法主要优化迭代过程效率，但未直接缩短生成轨迹。

Method: 提出基于上下文感知的初始化方法，通过轻量级辅助模型注入条件先验，实现离散令牌注入和嵌入层级插值两种机制，并引入基于置信度的重新遮蔽机制以避免过早确定。

Result: 上下文感知初始化显著减少了去噪迭代次数（约减少35%函数调用），但简单预热启动可能导致最终准确率下降，揭示了预热策略的校准和修正需求。

Conclusion: 上下文感知的预热初始化为提升扩散大语言模型推理效率提供了一种有效思路，但需要针对准确率下降问题开展后续研究，重点包括校准、修订机制及表示对齐。

Abstract: Diffusion Large Language Models (DLLMs) enable fully parallel token decoding but often remain impractical at inference time due to the many denoising iterations required to refine an information-free, fully masked initialization into coherent text. Most existing acceleration methods focus on traversing this generative trajectory more efficiently via improved solvers or sampling strategies. We advance a complementary perspective: shorten the trajectory itself by starting closer to the target distribution through context-aware initialization.
  We propose a training-free interface that injects prompt-conditioned priors from a lightweight auxiliary model into the diffusion initialization, and instantiate it with two mechanisms: discrete token injection and representation-level embedding interpolation. Because injected priors can be imperfect and unmask-only decoding can over-commit early, we also introduce a simple confidence-based remasking mechanism as a form of prior skepticism. Preliminary evidence on GSM8K suggests that context-aware initialization can substantially reduce denoising iterations (about 35\% fewer function evaluations in our setting), while also exposing a key open challenge: naive warm-starting can degrade final accuracy relative to strong diffusion baselines. We use these findings to motivate a research agenda around calibration, revision mechanisms, and representation alignment for reliable warm-started diffusion decoding.

</details>


### [47] [DramaBench: A Six-Dimensional Evaluation Framework for Drama Script Continuation](https://arxiv.org/abs/2512.19012)
*Shijian Ma,Yunqi Huang,Yan Lin*

Main category: cs.CL

TL;DR: 本文提出了首个针对戏剧续写的多维度大规模评测基准DramaBench，结合规则与语言模型标签，实现了对戏剧文本生成模型更全面、客观的评价。


<details>
  <summary>Details</summary>
Motivation: 现有基准无法全面评估戏剧剧本续写所需的角色一致性、情节推进及戏剧结构维护等多方面能力，需构建一个涵盖多维度的综合性评测框架。

Method: 通过规则基分析结合大型语言模型（LLM）标注和统计指标，对8个先进语言模型在1103个剧本上的表现进行细致评测，并辅以严格的统计显著性检验和人工验证。

Result: 在252对子模型的比较中有65.9%达到统计显著性，验证了评测维度的独立性（平均相关系数绝对值为0.020），并在部分维度获得较好的人类评审一致性。

Conclusion: DramaBench作为第一个大规模戏剧剧本续写评测基准，成功覆盖并量化了剧本续写的六个关键维度，推动了戏剧文本生成模型的全面评价与改进。

Abstract: Drama script continuation requires models to maintain character consistency, advance plot coherently, and preserve dramatic structurecapabilities that existing benchmarks fail to evaluate comprehensively. We present DramaBench, the first large-scale benchmark for evaluating drama script continuation across six independent dimensions: Format Standards, Narrative Efficiency, Character Consistency, Emotional Depth, Logic Consistency, and Conflict Handling. Our framework combines rulebased analysis with LLM-based labeling and statistical metrics, ensuring objective and reproducible evaluation. We conduct comprehensive evaluation of 8 state-of-the-art language models on 1,103 scripts (8,824 evaluations total), with rigorous statistical significance testing (252 pairwise comparisons, 65.9% significant) and human validation (188 scripts, substantial agreement on 3/5 dimensions). Our ablation studies confirm all six dimensions capture independent quality aspects (mean | r | = 0.020). DramaBench provides actionable, dimensionspecific feedback for model improvement and establishes a rigorous standard for creative writing evaluation.

</details>


### [48] [A Large Language Model Based Method for Complex Logical Reasoning over Knowledge Graphs](https://arxiv.org/abs/2512.19092)
*Ziyan Zhang,Chao Wang,Zhuo Chen,Lei Chen,Chiyi Li,Kai Song*

Main category: cs.CL

TL;DR: ROG框架结合知识图局部检索与大语言模型推理，有效提升复杂知识图查询推理性能。


<details>
  <summary>Details</summary>
Motivation: 面对真实知识图的不完整性和复杂逻辑查询结构，传统嵌入方法在复杂查询上泛化能力不足，需探索结合知识图检索与大语言模型推理的新方法。

Method: ROG分解复杂查询为简单子查询，检索相关子图作为证据，利用大语言模型进行链式逻辑推理，无需任务专属嵌入优化。

Result: 本文提出了ROG框架，将查询感知的知识图局部检索与大语言模型的链式推理相结合，实现了对复杂一阶逻辑查询的高效推理。ROG通过将复杂查询分解为简单子查询，检索相关紧凑子图作为上下文，并利用大语言模型逐步进行逻辑推理，避免了特定任务的嵌入优化。实验结果显示ROG在标准知识图库推理任务中性能优于基于嵌入的方法，尤其在高复杂查询上表现显著提升。

Conclusion: 结合结构化知识图检索与大语言模型驱动的逻辑推理能显著提高复杂知识图推理效果，优于传统嵌入方法。

Abstract: Reasoning over knowledge graphs (KGs) with first-order logic (FOL) queries is challenging due to the inherent incompleteness of real-world KGs and the compositional complexity of logical query structures. Most existing methods rely on embedding entities and relations into continuous geometric spaces and answer queries via differentiable set operations. While effective for simple query patterns, these approaches often struggle to generalize to complex queries involving multiple operators, deeper reasoning chains, or heterogeneous KG schemas. We propose ROG (Reasoning Over knowledge Graphs with large language models), an ensemble-style framework that combines query-aware KG neighborhood retrieval with large language model (LLM)-based chain-of-thought reasoning. ROG decomposes complex FOL queries into sequences of simpler sub-queries, retrieves compact, query-relevant subgraphs as contextual evidence, and performs step-by-step logical inference using an LLM, avoiding the need for task-specific embedding optimization. Experiments on standard KG reasoning benchmarks demonstrate that ROG consistently outperforms strong embedding-based baselines in terms of mean reciprocal rank (MRR), with particularly notable gains on high-complexity query types. These results suggest that integrating structured KG retrieval with LLM-driven logical reasoning offers a robust and effective alternative for complex KG reasoning tasks.

</details>


### [49] [Stop saying LLM: Large Discourse Models (LDM) and Artificial Discursive Agent (ADA)?](https://arxiv.org/abs/2512.19117)
*Amar Lakel*

Main category: cs.CL

TL;DR: 本文提出从语言模型转向话语模型的认识论转变，构建人工话语代理的新范式，推动多方协作治理以合理应用大型生成模型。


<details>
  <summary>Details</summary>
Motivation: 旨在超越人们对大型语言模型的''着迷/恐惧''二元对立，提出通过公共审议和规制程序，明确人工话语代理在当代社会中的定位、用途和限制。

Method: 提出将大型生成模型从''大型语言模型''（LLM）转变为''大型话语模型''（LDM），进而定义为人工话语代理（ADA），并基于存在论三元组理论框架进行分析。

Result: 分析了LDMs如何基于文档（体现三重调节实例的产物）模拟人类经验的部分话语投射，提出了涵盖国家、产业、公民社会和学术界共同治理和共管的框架。

Conclusion: 该研究为大型生成模型的社会应用提供新的理论框架与治理思路，促进公众理解和协同管控大型话语代理，推动其在社会空间中的合理定位与利用。

Abstract: This paper proposes an epistemological shift in the analysis of large generative models, replacing the category ''Large Language Models'' (LLM) with that of ''Large Discourse Models'' (LDM), and then with that of Artificial Discursive Agent (ADA). The theoretical framework is based on an ontological triad distinguishing three regulatory instances: the apprehension of the phenomenal regularities of the referential world, the structuring of embodied cognition, and the structural-linguistic sedimentation of the utterance within a socio-historical context. LDMs, operating on the product of these three instances (the document), model the discursive projection of a portion of human experience reified by the learning corpus. The proposed program aims to replace the ''fascination/fear'' dichotomy with public trials and procedures that make the place, uses, and limits of artificial discursive agents in contemporary social space decipherable, situating this approach within a perspective of governance and co-regulation involving the State, industry, civil society, and academia.

</details>


### [50] [SAP: Syntactic Attention Pruning for Transformer-based Language Models](https://arxiv.org/abs/2512.19125)
*Tzu-Yun Lee,Ding-Yong Hong,Jan-Jan Wu*

Main category: cs.CL

TL;DR: 本文提出了一种结合句法结构与注意力模式的注意力头裁剪方法SAP，配合CF机制提高鲁棒性，实现了高效且解释性强的裁剪，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前Transformer模型的注意力头裁剪方法主要依赖于对模型权重和激活的数学分析，缺乏对句子语法结构和注意力模式的有效利用，导致模型性能和解释性的提升有限。

Method: 提出Syntactic Attention Pruning (SAP)，结合句子语法结构和注意力模式指导注意力头的裁剪过程；另外引入Candidate Filtering (CF)机制，根据各注意力头对模型性能的贡献优先筛选，减少裁剪过程中的性能退化。

Result: 实验表明，SAP在不需要重新训练的情况下，有效保留了含有大量强注意力值的关键头，性能优于现有的注意力头裁剪策略，同时增强了模型行为的可解释性。

Conclusion: SAP方法为基于Transformer的语言模型的模型压缩提供了一种新的有效思路，具有较高的灵活性和良好的裁剪效果，推动了模型压缩研究的新方向。

Abstract: This paper introduces Syntactic Attention Pruning (SAP), a novel method for effectively pruning attention heads in Transformer models. Unlike conventional approaches that rely solely on mathematical analysis of model weights and activations, SAP incorporates both the syntactic structure and attention patterns of sentences to guide the pruning process. By leveraging these linguistic features, SAP not only achieves performance comparable to state-of-the-art methods but also enhances the interpretability of model behavior. To further improve robustness, we propose Candidate Filtering (CF), a mechanism that prioritizes heads based on their contribution to model performance, mitigating degradation during pruning. Experimental results indicate that SAP effectively preserves critical heads of a high density of strong attention values, outperforming existing head pruning strategies in retrain-free settings. These findings position SAP as a promising foundation for a new direction in model compression research, offering high flexibility for pruning across all transformer-based language models.

</details>


### [51] [AWPO: Enhancing Tool-Use of Large Language Models through Explicit Integration of Reasoning Rewards](https://arxiv.org/abs/2512.19126)
*Zihan Lin,Xiaohan Wang,Hexiong Yang,Jiajun Chai,Jie Cao,Guojun Yin,Wei Lin,Ran He*

Main category: cs.CL

TL;DR: AWPO框架结合推理奖励与结果奖励，通过动态权重调整提升大语言模型工具使用和推理能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法忽视显式推理奖励的潜力，简单结合多种奖励可能导致性能下降或优化冲突，亟需一种能有效集成推理奖励的强化学习框架以提升大语言模型的工具使用和推理表现。

Method: 提出优势加权策略优化（AWPO）框架，包含方差感知门控、难度感知加权及定制截断机制，用于稳定且有效地结合推理奖励和结果奖励进行强化学习训练。

Result: 本文提出了一种名为优势加权策略优化（AWPO）的强化学习框架，专门用于提升大语言模型（LLMs）在工具使用中的推理能力。该方法创新性地结合了显式推理奖励与结果奖励，通过方差感知门控和难度感知加权机制，动态调整推理奖励的优势值，并采用定制的截断机制保证训练稳定。大量实验表明，AWPO在标准工具使用基准测试中表现优异，超越了多个强基线及闭源模型，尤其在多轮复杂任务中表现突出。具有极高参数效率的4B规模模型在多轮准确率上比Grok-4提升了16%，同时在MMLU-Pro等超出训练分布的测试中保持了良好的泛化能力。

Conclusion: AWPO成功融合了显式推理奖励与传统结果奖励，实现了更强的工具利用和推理能力，且参数效率高，泛化能力强，推动了多轮复杂任务表现的提升。

Abstract: While reinforcement learning (RL) shows promise in training tool-use large language models (LLMs) using verifiable outcome rewards, existing methods largely overlook the potential of explicit reasoning rewards to bolster reasoning and tool utilization. Furthermore, natively combining reasoning and outcome rewards may yield suboptimal performance or conflict with the primary optimization objective. To address this, we propose advantage-weighted policy optimization (AWPO) -- a principled RL framework that effectively integrates explicit reasoning rewards to enhance tool-use capability. AWPO incorporates variance-aware gating and difficulty-aware weighting to adaptively modulate advantages from reasoning signals based on group-relative statistics, alongside a tailored clipping mechanism for stable optimization. Extensive experiments demonstrate that AWPO achieves state-of-the-art performance across standard tool-use benchmarks, significantly outperforming strong baselines and leading closed-source models in challenging multi-turn scenarios. Notably, with exceptional parameter efficiency, our 4B model surpasses Grok-4 by 16.0 percent in multi-turn accuracy while preserving generalization capability on the out-of-distribution MMLU-Pro benchmark.

</details>


### [52] [QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation](https://arxiv.org/abs/2512.19134)
*Dehai Min,Kailin Zhang,Tongtong Wu,Lu Cheng*

Main category: cs.CL

TL;DR: QuCo-RAG通过基于预训练语料的统计不确定性动态触发检索，有效减少大语言模型幻觉现象，提高了多模型多领域问答性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于模型内部信号的不确定性估计不可靠，因为大语言模型通常自信却输出错误，亟需客观、稳健的动态检索触发机制。

Method: 提出QuCo-RAG方法，利用两阶段基于预训练数据的统计不确定性检测：生成前识别低频实体，生成时检查实体共现，结合Infini-gram实现快速查询并触发检索。

Result: 在多跳问答基准上，QuCo-RAG对OLMo-2模型提升了5-12点EM分数，对Llama、Qwen、GPT模型提升最高14点，且在生物医学问答中表现出良好的领域泛化能力。

Conclusion: QuCo-RAG通过客观统计数据判断生成过程中是否需检索，有效降低了大语言模型的幻觉问题，提升了多跳问答的准确率，其方法具有模型无关性和跨领域泛化能力。

Abstract: Dynamic Retrieval-Augmented Generation adaptively determines when to retrieve during generation to mitigate hallucinations in large language models (LLMs). However, existing methods rely on model-internal signals (e.g., logits, entropy), which are fundamentally unreliable because LLMs are typically ill-calibrated and often exhibit high confidence in erroneous outputs. We propose QuCo-RAG, which shifts from subjective confidence to objective statistics computed from pre-training data. Our method quantifies uncertainty through two stages: (1) before generation, we identify low-frequency entities indicating long-tail knowledge gaps; (2) during generation, we verify entity co-occurrence in the pre-training corpus, where zero co-occurrence often signals hallucination risk. Both stages leverage Infini-gram for millisecond-latency queries over 4 trillion tokens, triggering retrieval when uncertainty is high. Experiments on multi-hop QA benchmarks show QuCo-RAG achieves EM gains of 5--12 points over state-of-the-art baselines with OLMo-2 models, and transfers effectively to models with undisclosed pre-training data (Llama, Qwen, GPT), improving EM by up to 14 points. Domain generalization on biomedical QA further validates the robustness of our paradigm. These results establish corpus-grounded verification as a principled, practically model-agnostic paradigm for dynamic RAG. Our code is publicly available at https://github.com/ZhishanQ/QuCo-RAG.

</details>


### [53] [From Speech to Subtitles: Evaluating ASR Models in Subtitling Italian Television Programs](https://arxiv.org/abs/2512.19161)
*Alessandro Lucca,Francesco Pierri*

Main category: cs.CL

TL;DR: 本文评估了多款ASR模型在意大利语电视字幕中的性能，提出了基于人机协同的专业字幕系统方案，提升字幕制作效率。


<details>
  <summary>Details</summary>
Motivation: 当前自动语音识别（ASR）系统在标准数据集上表现良好，但在实际非英语长视频内容（如意大利语）中的表现尚未充分研究。

Method: 对四个先进的ASR模型（Whisper Large v2、AssemblyAI Universal、Parakeet TDT v3 0.6b和WhisperX）在50小时意大利语电视节目数据集上进行评估，并与专业人工字幕制作进行对比。

Result: 发现现有模型尚无法完全满足媒体行业对准确性的需求，但能显著提升人工字幕制作人员的工作效率。

Conclusion: 人机协同（HITL）方法对于实现高质量字幕制作至关重要，文中设计了支持该工作流程的云端生产级基础设施。

Abstract: Subtitles are essential for video accessibility and audience engagement. Modern Automatic Speech Recognition (ASR) systems, built upon Encoder-Decoder neural network architectures and trained on massive amounts of data, have progressively reduced transcription errors on standard benchmark datasets. However, their performance in real-world production environments, particularly for non-English content like long-form Italian videos, remains largely unexplored. This paper presents a case study on developing a professional subtitling system for an Italian media company. To inform our system design, we evaluated four state-of-the-art ASR models (Whisper Large v2, AssemblyAI Universal, Parakeet TDT v3 0.6b, and WhisperX) on a 50-hour dataset of Italian television programs. The study highlights their strengths and limitations, benchmarking their performance against the work of professional human subtitlers. The findings indicate that, while current models cannot meet the media industry's accuracy needs for full autonomy, they can serve as highly effective tools for enhancing human productivity. We conclude that a human-in-the-loop (HITL) approach is crucial and present the production-grade, cloud-based infrastructure we designed to support this workflow.

</details>


### [54] [JEPA-Reasoner: Decoupling Latent Reasoning from Token Generation](https://arxiv.org/abs/2512.19171)
*Bingyang Kelvin Liu,Ziyu Patrick Chen*

Main category: cs.CL

TL;DR: 提出JEPA-Reasoner，实现潜在空间推理与独立生成，提高生成稳健性，减少错误积累。


<details>
  <summary>Details</summary>
Motivation: 现有JEPA模型缺乏生成能力，基于Transformer的潜在空间推理方法虽能提升性能但依赖逐令牌生成，易累计错误且依赖上下文，故提出改进方案以增强生成能力和推理能力。

Method: 基于JEPA架构引入潜在空间推理机制，结合独立的动作执行模型Talker用于生成可读文本，从而实现生成能力的提升和推理与生成过程的解耦。

Result: 本文提出了一种新的Joint-Embedding Predictive Architecture (JEPA)模型，称为JEPA-Reasoner，该模型增强了生成能力，并在潜在空间进行推理。通过引入一个独立的动作执行模型（Talker）来生成可读的句子，实现了潜在空间推理与令牌生成的解耦。该方法能够生成混合潜在向量，为多线程推理奠定基础，并提高了自回归生成的稳健性，减少了累积错误。

Conclusion: 解耦潜在空间推理与令牌生成的JEPA-Reasoner在生成质量和推理能力上优于传统JEPA和基于Transformer的模型，具备更强的鲁棒性和多线程推理潜力。

Abstract: While Joint-Embedding Predictive Architecture (JEPA) has emerged as a powerful architecture for learning rich latent representations, it fundamentally lacks generative abilities. Meanwhile, latent space reasoning attempts for Transformer models like COCONUT do improve performance, but they ultimately rely on token-by-token generation, which still accumulates compounding error and relies on context information to gain reasoning insights. To address these limitations, we propose JEPA-Reasoner, a novel JEPA model enhanced with generative ability that reasons in latent space. We augment it with a separate action-taker model, Talker, to produce human-readable sentences. Our approach demonstrates that decoupling latent space reasoning and token generation enables JEPA-Reasoner to produce mixed latent vectors that might lay the foundation for multi-threaded reasoning, while performing autoregressive generation with superior robustness to compounding error.

</details>


### [55] [CycleChart: A Unified Consistency-Based Learning Framework for Bidirectional Chart Understanding and Generation](https://arxiv.org/abs/2512.19173)
*Dazhen Deng,Sen Yang,Yuchen He,Yuan Tian,Yingcai Wu*

Main category: cs.CL

TL;DR: 提出CycleChart，利用生成-解析循环一致性，联合训练图表生成与理解任务，实现共享语义学习和增强泛化。


<details>
  <summary>Details</summary>
Motivation: 现有图表相关任务各自独立，导致模型无法学习不同任务间共享的语义信息。倾向于设计一个统一框架实现图表生成与理解的协同学习。

Method: 提出了基于生成-解析一致性的多任务学习方法，构建了包含图表模式预测、数据解析和问答的多任务对齐数据集，通过循环生成和解析加强模型对图表语义的双向理解。

Result: CycleChart在图表生成、解析和问答任务上均取得优异成绩，展示了更强的跨任务泛化能力，推动了通用图表理解模型的发展。

Conclusion: CycleChart通过一致性学习框架有效连接了图表生成与理解任务，实现了跨任务的语义共享与增强泛化能力。

Abstract: Current chart-specific tasks, such as chart question answering, chart parsing, and chart generation, are typically studied in isolation, preventing models from learning the shared semantics that link chart generation and interpretation. We introduce CycleChart, a consistency-based learning framework for bidirectional chart understanding and generation. CycleChart adopts a schema-centric formulation as a common interface across tasks. We construct a consistent multi-task dataset, where each chart sample includes aligned annotations for schema prediction, data parsing, and question answering. To learn cross-directional chart semantics, CycleChart introduces a generate-parse consistency objective: the model generates a chart schema from a table and a textual query, then learns to recover the schema and data from the generated chart, enforcing semantic alignment across directions. CycleChart achieves strong results on chart generation, chart parsing, and chart question answering, demonstrating improved cross-task generalization and marking a step toward more general chart understanding models.

</details>


### [56] [Identifying Features Associated with Bias Against 93 Stigmatized Groups in Language Models and Guardrail Model Safety Mitigation](https://arxiv.org/abs/2512.19238)
*Anna-Maria Gueorguieva,Aylin Caliskan*

Main category: cs.CL

TL;DR: 研究了大型语言模型（LLMs）在针对非受保护污名群体时的偏见，发现对高危污名的偏见最严重，现有的防护模型虽能减轻偏见但效果有限。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在面对污名群体时表现出的社会偏见，尤其是非受保护群体的偏见及其与污名社会特征的关联。

Method: 通过SocialStigmaQA基准测试，评估三款主流LLM对93个污名群体的偏见，结合人类对污名社会特征的评级，并测试各模型的防护措施效果。

Result: 发现高危污名（如帮派成员、艾滋病患者）偏见率高达60%，社会人口统计污名偏见率最低，防护模型能够减少偏见但存在识别偏见意图失败及重要影响特征未改变量。

Conclusion: 大型语言模型在处理污名群体时存在显著偏见，防护措施虽有改善但不足，需提升防护模型以更好地缓解偏见问题。

Abstract: Large language models (LLMs) have been shown to exhibit social bias, however, bias towards non-protected stigmatized identities remain understudied. Furthermore, what social features of stigmas are associated with bias in LLM outputs is unknown. From psychology literature, it has been shown that stigmas contain six shared social features: aesthetics, concealability, course, disruptiveness, origin, and peril. In this study, we investigate if human and LLM ratings of the features of stigmas, along with prompt style and type of stigma, have effect on bias towards stigmatized groups in LLM outputs. We measure bias against 93 stigmatized groups across three widely used LLMs (Granite 3.0-8B, Llama-3.1-8B, Mistral-7B) using SocialStigmaQA, a benchmark that includes 37 social scenarios about stigmatized identities; for example deciding wether to recommend them for an internship. We find that stigmas rated by humans to be highly perilous (e.g., being a gang member or having HIV) have the most biased outputs from SocialStigmaQA prompts (60% of outputs from all models) while sociodemographic stigmas (e.g. Asian-American or old age) have the least amount of biased outputs (11%). We test if the amount of biased outputs could be decreased by using guardrail models, models meant to identify harmful input, using each LLM's respective guardrail model (Granite Guardian 3.0, Llama Guard 3.0, Mistral Moderation API). We find that bias decreases significantly by 10.4%, 1.4%, and 7.8%, respectively. However, we show that features with significant effect on bias remain unchanged post-mitigation and that guardrail models often fail to recognize the intent of bias in prompts. This work has implications for using LLMs in scenarios involving stigmatized groups and we suggest future work towards improving guardrail models for bias mitigation.

</details>


### [57] [ChemATP: A Training-Free Chemical Reasoning Framework for Large Language Models](https://arxiv.org/abs/2512.19240)
*Mingxu Zhang,Dazhong Shen,Qi Zhang,Ying Sun*

Main category: cs.CL

TL;DR: ChemATP通过构建原子级知识库，实现LLM动态推理，提升化学推理效果，兼顾可解释性与适应性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: LLMs在分子科学中表现不佳，因缺乏明确的化学先验知识，现有方法存在静态耦合或表面提示不足的问题。

Method: 提出ChemATP框架，构建第一个原子级文本知识库，使冻结的LLM能够动态检索和推理该信息，实现化学知识与推理引擎解耦。

Result: ChemATP显著优于无训练基线，且与最先进的训练方法竞争，展示了显式先验注入作为参数更新的有效替代方案。

Conclusion: 通过显式构建原子级知识库并解耦化学知识与推理引擎，ChemATP提升了LLM在分子科学的推理能力，实现了可解释性和适应性。

Abstract: Large Language Models (LLMs) exhibit strong general reasoning but struggle in molecular science due to the lack of explicit chemical priors in standard string representations. Current solutions face a fundamental dilemma. Training-based methods inject priors into parameters, but this static coupling hinders rapid knowledge updates and often compromises the model's general reasoning capabilities. Conversely, existing training-free methods avoid these issues but rely on surface-level prompting, failing to provide the fine-grained atom-level priors essential for precise chemical reasoning. To address this issue, we introduce ChemATP, a framework that decouples chemical knowledge from the reasoning engine. By constructing the first atom-level textual knowledge base, ChemATP enables frozen LLMs to explicitly retrieve and reason over this information dynamically. This architecture ensures interpretability and adaptability while preserving the LLM's intrinsic general intelligence. Experiments show that ChemATP significantly outperforms training-free baselines and rivals state-of-the-art training-based models, demonstrating that explicit prior injection is a competitive alternative to implicit parameter updates.

</details>


### [58] [Auto-Prompting with Retrieval Guidance for Frame Detection in Logistics](https://arxiv.org/abs/2512.19247)
*Do Minh Duc,Quan Xuan Truong,Nguyen Tat Dat,Nguyen Van Vinh*

Main category: cs.CL

TL;DR: 通过结合检索增强生成及自动链式推理合成的提示优化方法，显著提升了大语言模型在物流文本框架检测任务的推理准确率和标注效率，且具有良好泛化性。


<details>
  <summary>Details</summary>
Motivation: 提升大语言模型(LLMs)在无需大量微调的情况下，能够高效完成复杂推理和标注任务的能力。

Method: 提出结合检索增强生成(RAG)、少量示例提示、链式推理(CoT)以及自动CoT合成的提示优化流程。设计基于LLM的提示优化代理，通过迭代使用检索示例、性能反馈和自我评估来精炼提示。

Result: 在实际物流文本框架检测任务中，优化后的提示特别是结合Auto-CoT和RAG的提示，提高推理准确率最高达15%。且在GPT-4o、Qwen 2.5及LLaMA 3.1多款模型上均表现出一致提升，证明方案具备良好泛化性和实用价值。

Conclusion: 结构化提示优化方法是无需全量微调的可行替代方案，能够为领域专用NLP应用如物流文本分析提供高效且可扩展的解决方案。

Abstract: Prompt engineering plays a critical role in adapting large language models (LLMs) to complex reasoning and labeling tasks without the need for extensive fine-tuning. In this paper, we propose a novel prompt optimization pipeline for frame detection in logistics texts, combining retrieval-augmented generation (RAG), few-shot prompting, chain-of-thought (CoT) reasoning, and automatic CoT synthesis (Auto-CoT) to generate highly effective task-specific prompts. Central to our approach is an LLM-based prompt optimizer agent that iteratively refines the prompts using retrieved examples, performance feedback, and internal self-evaluation. Our framework is evaluated on a real-world logistics text annotation task, where reasoning accuracy and labeling efficiency are critical. Experimental results show that the optimized prompts - particularly those enhanced via Auto-CoT and RAG - improve real-world inference accuracy by up to 15% compared to baseline zero-shot or static prompts. The system demonstrates consistent improvements across multiple LLMs, including GPT-4o, Qwen 2.5 (72B), and LLaMA 3.1 (70B), validating its generalizability and practical value. These findings suggest that structured prompt optimization is a viable alternative to full fine-tuning, offering scalable solutions for deploying LLMs in domain-specific NLP applications such as logistics.

</details>


### [59] [CienaLLM: Generative Climate-Impact Extraction from News Articles with Autoregressive LLMs](https://arxiv.org/abs/2512.19305)
*Javier Vela-Tambo,Jorge Gracia,Fernando Dominguez-Castro*

Main category: cs.CL

TL;DR: CienaLLM是一个基于schema引导的生成式信息提取框架，利用大型语言模型实现对气候灾害新闻的高效结构化信息提取，具备良好的准确性和适应性。


<details>
  <summary>Details</summary>
Motivation: 需要从大规模异构新闻文章中提取结构化信息，以理解和监测气候灾害的社会经济影响。

Method: 采用基于schema引导的生成信息提取方法，结合多阶段流水线和可配置提示，实现对新闻文章的零样本信息抽取，同时通过大规模因素实验评估不同语言模型、精度和提示策略的性能影响。

Result: 开发了基于schema引导的生成式信息提取的模块化框架CienaLLM，使用开源大型语言模型实现零样本信息提取，并通过大量因素实验优化性能。CienaLLM在提取干旱影响的准确率上匹配或优于有监督基线，但推理成本较高。

Conclusion: CienaLLM设计模型无关且基于schema，可通过调整提示和模式适配其他灾害、领域或语言信息提取任务，支持复现并释放了相关代码和资源。

Abstract: Understanding and monitoring the socio-economic impacts of climate hazards requires extracting structured information from heterogeneous news articles on a large scale. To that end, we have developed CienaLLM, a modular framework based on schema-guided Generative Information Extraction. CienaLLM uses open-weight Large Language Models for zero-shot information extraction from news articles, and supports configurable prompts and output schemas, multi-step pipelines, and cloud or on-premise inference. To systematically assess how the choice of LLM family, size, precision regime, and prompting strategy affect performance, we run a large factorial study in models, precisions, and prompt engineering techniques. An additional response parsing step nearly eliminates format errors while preserving accuracy; larger models deliver the strongest and most stable performance, while quantization offers substantial efficiency gains with modest accuracy trade-offs; and prompt strategies show heterogeneous, model-specific effects. CienaLLM matches or outperforms the supervised baseline in accuracy for extracting drought impacts from Spanish news, although at a higher inference cost. While evaluated in droughts, the schema-driven and model-agnostic design is suitable for adapting to related information extraction tasks (e.g., other hazards, sectors, or languages) by editing prompts and schemas rather than retraining. We release code, configurations, and schemas to support reproducible use.

</details>


### [60] [HATS: High-Accuracy Triple-Set Watermarking for Large Language Models](https://arxiv.org/abs/2512.19378)
*Zhiqing Hu,Chenxu Zhao,Jiazhong Lu,Xiaolei Liu*

Main category: cs.CL

TL;DR: 提出一种基于词汇三分区的水印技术，高效检测LLM生成文本且不降低文本质量。


<details>
  <summary>Details</summary>
Motivation: 为防止大语言模型（LLM）生成文本的滥用，提出通过水印技术嵌入隐含信号以实现识别。

Method: 设计了一种三分区（绿/黄/红）的词汇划分方法，限制采样仅在绿黄区，并在检测时计算绿区富集和红区耗减的统计量，利用单边z得分及Fisher方法聚合p值判断是否带水印。

Result: 在Llama 2 7B模型上实现了生成、检测和测试，实验结果显示该三分区方案在保持文本可读性的同时，实现了较高的检测准确率和固定的假阳性率。

Conclusion: 该水印技术有效抑制了LLM文本滥用，兼顾检测准确性和生成文本的自然度。

Abstract: Misuse of LLM-generated text can be curbed by watermarking techniques that embed implicit signals into the output. We propose a watermark that partitions the vocabulary at each decoding step into three sets (Green/Yellow/Red) with fixed ratios and restricts sampling to the Green and Yellow sets. At detection time, we replay the same partitions, compute Green-enrichment and Red-depletion statistics, convert them to one-sided z-scores, and aggregate their p-values via Fisher's method to decide whether a passage is watermarked. We implement generation, detection, and testing on Llama 2 7B, and evaluate true-positive rate, false-positive rate, and text quality. Results show that the triple-partition scheme achieves high detection accuracy at fixed FPR while preserving readability.

</details>


### [61] [Kunnafonidilaw ka Cadeau: an ASR dataset of present-day Bambara](https://arxiv.org/abs/2512.19400)
*Yacouba Diarra,Panga Azazia Kamate,Nouhoum Souleymane Coulibaly,Michael Leventhal*

Main category: cs.CL

TL;DR: 构建并发布了包含现实复杂语音特征的班巴拉语ASR数据集，通过微调提升了模型在真实环境下的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 针对现存自动语音识别(ASR)系统在处理马里班巴拉语口语时面临的背景噪音、插话和代码切换等现实挑战，缺乏大规模真实语料库的问题。

Method: 收集整理马里电台档案中的160小时班巴拉语语音数据，包含多种真实场景特征。对33.47小时人工审核子集进行Parakeet模型微调，并采用实用的转录规范化方法减少标注变异。

Result: 在两个实际测试集上，使用Kunkado微调后，字错误率分别从44.47%降至37.12%和36.07%降至32.33%。人类评测显示，模型性能优于用98小时更清洁语料训练的同类系统。

Conclusion: Kunkado数据集和微调方法显著提升了班巴拉语ASR模型在真实口语场景中的表现，促进了主要以口语交流为主语言的自动语音识别技术发展。

Abstract: We present Kunkado, a 160-hour Bambara ASR dataset compiled from Malian radio archives to capture present-day spontaneous speech across a wide range of topics. It includes code-switching, disfluencies, background noise, and overlapping speakers that practical ASR systems encounter in real-world use. We finetuned Parakeet-based models on a 33.47-hour human-reviewed subset and apply pragmatic transcript normalization to reduce variability in number formatting, tags, and code-switching annotations. Evaluated on two real-world test sets, finetuning with Kunkado reduces WER from 44.47\% to 37.12\% on one and from 36.07\% to 32.33\% on the other. In human evaluation, the resulting model also outperforms a comparable system with the same architecture trained on 98 hours of cleaner, less realistic speech. We release the data and models to support robust ASR for predominantly oral languages.

</details>


### [62] [CodeSimpleQA: Scaling Factuality in Code Large Language Models](https://arxiv.org/abs/2512.19424)
*Jian Yang,Wei Zhang,Yizhi Li,Shawn Guo,Haowen Wang,Aishan Liu,Ge Zhang,Zili Wang,Zhoujun Li,Xianglong Liu,Weifeng Lv*

Main category: cs.CL

TL;DR: 提出了用于评估代码LLMs事实准确性的双语基准CodeSimpleQA及指令语料，并设计后训练框架显著提升代码事实性。


<details>
  <summary>Details</summary>
Motivation: 当前代码相关基准多侧重代码执行正确性，忽视了编程知识的事实准确性，导致LLMs难以生成事实准确的代码相关回答，需建立评估数据集和方法提升模型的事实准确性。

Method: 构建包含6600万样本的指令语料库CodeSimpleQA-Instruct，设计结合监督微调和强化学习的后训练框架进行模型训练。

Result: 本文提出了CodeSimpleQA，一个用于评估代码大语言模型（LLMs）事实准确性的双语基准测试数据集，涵盖多种编程语言和计算机科学领域。此外，构建了包含6600万样本的大规模指令语料CodeSimpleQA-Instruct，并设计了结合监督微调和强化学习的后训练框架。实验表明，即使是先进的LLMs在代码事实性方面仍存在不足，而该方法显著提升了模型的事实准确率。

Conclusion: 代码LLMs在事实准确性方面仍有挑战，结合监督微调和强化学习的后训练框架能有效提升模型在代码事实性上的表现。

Abstract: Large language models (LLMs) have made significant strides in code generation, achieving impressive capabilities in synthesizing code snippets from natural language instructions. However, a critical challenge remains in ensuring LLMs generate factually accurate responses about programming concepts, technical implementations, etc. Most previous code-related benchmarks focus on code execution correctness, overlooking the factual accuracy of programming knowledge. To address this gap, we present CodeSimpleQA, a comprehensive bilingual benchmark designed to evaluate the factual accuracy of code LLMs in answering code-related questions, which contains carefully curated question-answer pairs in both English and Chinese, covering diverse programming languages and major computer science domains. Further, we create CodeSimpleQA-Instruct, a large-scale instruction corpus with 66M samples, and develop a post-training framework combining supervised fine-tuning and reinforcement learning. Our comprehensive evaluation of diverse LLMs reveals that even frontier LLMs struggle with code factuality. Our proposed framework demonstrates substantial improvements over the base model, underscoring the critical importance of factuality-aware alignment in developing reliable code LLMs.

</details>


### [63] [MobileWorld: Benchmarking Autonomous Mobile Agents in Agent-User Interactive, and MCP-Augmented Environments](https://arxiv.org/abs/2512.19432)
*Quyu Kong,Xu Zhang,Zhenyu Yang,Nolan Gao,Chen Liu,Panrong Tong,Chenglin Cai,Hanzhang Zhou,Jianan Zhang,Liangyu Chen,Zhidan Liu,Steven Hoi,Yue Wang*

Main category: cs.CL

TL;DR: 提出了一个更具挑战性的移动端基准测试MobileWorld，涵盖更多任务和应用，强调长任务链和跨应用交互，支持用户交互和混合工具调用，评测标准严格。


<details>
  <summary>Details</summary>
Motivation: 现有AndroidWorld基准已趋近饱和，且缺少电商、企业通信等关键应用，难以反映现实模糊指令和混合工具使用场景，亟需设计更具挑战性和真实性的移动端评测环境。

Method: 建立包含201个任务和20个应用的新基准MobileWorld，设计包含长任务链、跨应用任务和新任务类别（用户交互、MCP调用），并基于快照容器环境和功能验证进行精确评测。开发一个支持扩展动作空间的规划执行框架以适应用户交互和MCP调用。

Result: MobileWorld中的任务成功率明显低于AndroidWorld，最佳框架和端到端模型成功率分别为51.7%和20.9%，用户交互和MCP调用导致模型表现显著下降。

Conclusion: MobileWorld显著提升了任务难度，现有模型在该环境下表现大幅下降，尤其在用户交互和多应用调用方面表现不足，表明需要新的方法来提升移动智能。

Abstract: Among existing online mobile-use benchmarks, AndroidWorld has emerged as the dominant benchmark due to its reproducible environment and deterministic evaluation; however, recent agents achieving over 90% success rates indicate its saturation and motivate the need for a more challenging benchmark. In addition, its environment lacks key application categories, such as e-commerce and enterprise communication, and does not reflect realistic mobile-use scenarios characterized by vague user instructions and hybrid tool usage. To bridge this gap, we introduce MobileWorld, a substantially more challenging benchmark designed to better reflect real-world mobile usage, comprising 201 tasks across 20 applications, while maintaining the same level of reproducible evaluation as AndroidWorld. The difficulty of MobileWorld is twofold. First, it emphasizes long-horizon tasks with cross-application interactions: MobileWorld requires nearly twice as many task-completion steps on average (27.8 vs. 14.3) and includes far more multi-application tasks (62.2% vs. 9.5%) compared to AndroidWorld. Second, MobileWorld extends beyond standard GUI manipulation by introducing novel task categories, including agent-user interaction and MCP-augmented tasks. To ensure robust evaluation, we provide snapshot-based container environment and precise functional verifications, including backend database inspection and task callback APIs. We further develop a planner-executor agentic framework with extended action spaces to support user interactions and MCP calls. Our results reveal a sharp performance drop compared to AndroidWorld, with the best agentic framework and end-to-end model achieving 51.7% and 20.9% success rates, respectively. Our analysis shows that current models struggle significantly with user interaction and MCP calls, offering a strategic roadmap toward more robust, next-generation mobile intelligence.

</details>


### [64] [SiamGPT: Quality-First Fine-Tuning for Stable Thai Text Generation](https://arxiv.org/abs/2512.19455)
*Thittipat Pairatsuppawat,Abhibhu Tachaapornchai,Paweekorn Kusolsomboon,Chutikan Chaiwong,Thodsaporn Chay-intr,Kobkrit Viriyayudhakorn,Nongnuch Ketui,Aslan B. Wong*

Main category: cs.CL

TL;DR: 该论文介绍了SiamGPT-32B，一个基于Qwen3-32B的开放权重大型语言模型，通过高质量的有监督微调策略提升了泰语复杂指令生成的稳定性和表现。


<details>
  <summary>Details</summary>
Motivation: 尽管现有模型在英语表现良好，开放权重大型语言模型在泰语复杂指令生成中稳定性不足，难以部署。

Method: 采用质量优先的微调策略，结合翻译的高复杂度英语指令数据和泰语适配的AutoIF框架，仅通过有监督微调，无需持续预训练或扩充语料。

Result: 在SEA-HELM基准测试中，SiamGPT-32B在指令遵循、多轮对话和自然语言理解方面均取得显著提升。

Conclusion: SiamGPT-32B在泰语指令遵循、多轮对话鲁棒性和语言稳定性方面表现优异，是当前同规模开放权重泰语模型中的最强者。

Abstract: Open-weights large language models remain difficult to deploy for Thai due to unstable generation under complex instructions, despite strong English performance. To mitigate these limitations, We present SiamGPT-32B, an open-weights model based on Qwen3-32B, fine-tuned with a Quality-First strategy emphasizing curated supervision over data scale. The fine-tuning pipeline combines translated high-complexity English instruction data with a Thai-adapted AutoIF framework for instruction and linguistic constraints. Using supervised fine-tuning only, without continual pretraining or corpus expansion, SiamGPT-32B improves instruction adherence, multi-turn robustness, and linguistic stability. Evaluations on the SEA-HELM benchmark show that SiamGPT-32B achieves the strongest overall performance among similar-scale open-weights Thai models, with consistent gains in instruction following, multi-turn dialogue, and natural language understanding.

</details>


### [65] [Activations as Features: Probing LLMs for Generalizable Essay Scoring Representations](https://arxiv.org/abs/2512.19456)
*Jinwei Chi,Ke Wang,Yu Chen,Xuanye Lin,Qiang Xu*

Main category: cs.CL

TL;DR: 利用大型语言模型中间层激活信息，可以提升跨题目自动作文评分的判别力，LLM通过调整评分视角适应不同作文和评分维度。


<details>
  <summary>Details</summary>
Motivation: 自动作文评分在跨题目设置中面临评分标准多样化的挑战。尽管之前的研究侧重于利用大型语言模型（LLM）的输出提高评分准确性，本研究认为模型中间层的激活信息也可能包含有价值的评分信息。

Method: 通过提取LLM各层的激活信息，训练探针模型评估其判别能力，并分析不同模型及输入内容对评分视角的影响，通过计算不同评分维度下的激活方向解析模型的评分视角变化。

Result: 研究发现LLM的中间层激活在跨题目作文评分任务中具有较强的判别能力，且LLM能够根据不同的作文类型和评分维度调整其评分视角，从而有效应对评分标准的多样性。

Conclusion: LLM的中间层激活提供了强有力的作文质量判别信息，模型能灵活适应评分标准的多样性，提升跨题目自动评分的效果。

Abstract: Automated essay scoring (AES) is a challenging task in cross-prompt settings due to the diversity of scoring criteria. While previous studies have focused on the output of large language models (LLMs) to improve scoring accuracy, we believe activations from intermediate layers may also provide valuable information. To explore this possibility, we evaluated the discriminative power of LLMs' activations in cross-prompt essay scoring task. Specifically, we used activations to fit probes and further analyzed the effects of different models and input content of LLMs on this discriminative power. By computing the directions of essays across various trait dimensions under different prompts, we analyzed the variation in evaluation perspectives of large language models concerning essay types and traits. Results show that the activations possess strong discriminative power in evaluating essay quality and that LLMs can adapt their evaluation perspectives to different traits and essay types, effectively handling the diversity of scoring criteria in cross-prompt settings.

</details>


### [66] [A Large-Language-Model Framework for Automated Humanitarian Situation Reporting](https://arxiv.org/abs/2512.19475)
*Ivan Decostanzi,Yelena Mejova,Kyriaki Kalimeri*

Main category: cs.CL

TL;DR: 本文提出了一个利用大型语言模型自动生成结构化人道主义报告的框架，实现了高相关性、重要性和紧迫性的问答生成，提升了报告的结构性和可操作性。


<details>
  <summary>Details</summary>
Motivation: 现有生成的情势报告大多依赖人工，流程繁琐、资源消耗大且不一致，亟需自动化解决方案。

Method: 利用大型语言模型结合语义文本聚类、自动问答生成、带引用的答案提取、多级摘要和执行摘要生成，并辅以内部评估指标模仿专家推理。

Result: 框架在13个人道主义事件中的1100多份文件测试中，问答的相关性、重要性、紧迫性分别达84.7%、84.0%、76.4%，答案相关性86.3%，引文精度和召回率均超过76%，人机评价F1超0.80。

Conclusion: 本文框架能够自动生成准确、可验证且具有实用价值的人道主义情势报告，显著优于现有方法。

Abstract: Timely and accurate situational reports are essential for humanitarian decision-making, yet current workflows remain largely manual, resource intensive, and inconsistent. We present a fully automated framework that uses large language models (LLMs) to transform heterogeneous humanitarian documents into structured and evidence-grounded reports. The system integrates semantic text clustering, automatic question generation, retrieval augmented answer extraction with citations, multi-level summarization, and executive summary generation, supported by internal evaluation metrics that emulate expert reasoning. We evaluated the framework across 13 humanitarian events, including natural disasters and conflicts, using more than 1,100 documents from verified sources such as ReliefWeb. The generated questions achieved 84.7 percent relevance, 84.0 percent importance, and 76.4 percent urgency. The extracted answers reached 86.3 percent relevance, with citation precision and recall both exceeding 76 percent. Agreement between human and LLM based evaluations surpassed an F1 score of 0.80. Comparative analysis shows that the proposed framework produces reports that are more structured, interpretable, and actionable than existing baselines. By combining LLM reasoning with transparent citation linking and multi-level evaluation, this study demonstrates that generative AI can autonomously produce accurate, verifiable, and operationally useful humanitarian situation reports.

</details>


### [67] [Event Extraction in Large Language Model](https://arxiv.org/abs/2512.19537)
*Bobo Li,Xudong Han,Jiang Liu,Yuzhe Ding,Liqiang Jing,Zhaoqi Zhang,Jinheng Li,Xinya Du,Fei Li,Meishan Zhang,Min Zhang,Aixin Sun,Philip S. Yu,Hao Fei*

Main category: cs.CL

TL;DR: 本文综述了基于大型语言模型的事件抽取技术，分析其挑战，提出将事件抽取作为认知支架以增强LLM应用的构想，并总结了当前方法及未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 当前基于大型语言模型的事件抽取技术在实际部署中面临幻觉、跨长文本与文档的时序因果关系推理困难及知识管理限制，亟需通过事件抽取系统构建认知支架，提升模型的可控性和可靠性。

Method: 论文回顾了从基于规则、神经网络到指令驱动和生成式框架的事件抽取方法演变，重点介绍了利用事件模式、槽位约束、图关系检索和事件存储等技术构建多步骤推理和知识管理的系统架构。

Result: 本文综述了大型语言模型（LLMs）和多模态LLMs在事件抽取（EE）领域的最新发展，指出LLM驱动的事件抽取存在幻觉、时序因果关联脆弱、长时知识管理有限等挑战。文章主张将事件抽取视为LLM解决方案中的认知支架，利用事件模式、槽位约束、事件中心结构、事件链接和事件存储，实现结构化和可验证的信息处理。文中系统总结了事件抽取的任务、方法演进、架构、数据集和评测标准，并讨论跨语言、低资源和特定领域的应用场景，最后提出了未来提升事件抽取系统可靠性和智能性的开放挑战。

Conclusion: 将事件抽取作为认知支架整合进大型语言模型系统，是解决幻觉、提升时序因果推理和知识管理能力的关键，有助于推动事件抽取从静态提取向结构化可靠的智能感知与记忆层转变。

Abstract: Large language models (LLMs) and multimodal LLMs are changing event extraction (EE): prompting and generation can often produce structured outputs in zero shot or few shot settings. Yet LLM based pipelines face deployment gaps, including hallucinations under weak constraints, fragile temporal and causal linking over long contexts and across documents, and limited long horizon knowledge management within a bounded context window. We argue that EE should be viewed as a system component that provides a cognitive scaffold for LLM centered solutions. Event schemas and slot constraints create interfaces for grounding and verification; event centric structures act as controlled intermediate representations for stepwise reasoning; event links support relation aware retrieval with graph based RAG; and event stores offer updatable episodic and agent memory beyond the context window. This survey covers EE in text and multimodal settings, organizing tasks and taxonomy, tracing method evolution from rule based and neural models to instruction driven and generative frameworks, and summarizing formulations, decoding strategies, architectures, representations, datasets, and evaluation. We also review cross lingual, low resource, and domain specific settings, and highlight open challenges and future directions for reliable event centric systems. Finally, we outline open challenges and future directions that are central to the LLM era, aiming to evolve EE from static extraction into a structurally reliable, agent ready perception and memory layer for open world systems.

</details>


### [68] [Algerian Dialect](https://arxiv.org/abs/2512.19543)
*Zakaria Benmounah,Abdennour Boulesnane*

Main category: cs.CL

TL;DR: 本文提出了一个大规模的阿尔及利亚方言情感标注数据集，助力方言阿拉伯语和情感分析研究。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏公开可用的阿尔及利亚方言资源，限制了情感分析和方言阿拉伯语自然语言处理的研究发展。

Method: 通过YouTube数据API采集30多个阿尔及利亚新闻媒体频道的评论，手工标注每条评论的五分类情感标签，并整理丰富的元数据。

Result: 构建了一个包含45000条手工标注情感类别（五种）的阿尔及利亚方言YouTube评论数据集，并附带丰富的元数据。

Conclusion: 该数据集填补了阿尔及利亚方言情感分析领域资源匮乏的空白，促进相关领域的研究。

Abstract: We present Algerian Dialect, a large-scale sentiment-annotated dataset consisting of 45,000 YouTube comments written in Algerian Arabic dialect. The comments were collected from more than 30 Algerian press and media channels using the YouTube Data API. Each comment is manually annotated into one of five sentiment categories: very negative, negative, neutral, positive, and very positive. In addition to sentiment labels, the dataset includes rich metadata such as collection timestamps, like counts, video URLs, and annotation dates. This dataset addresses the scarcity of publicly available resources for Algerian dialect and aims to support research in sentiment analysis, dialectal Arabic NLP, and social media analytics. The dataset is publicly available on Mendeley Data under a CC BY 4.0 license at https://doi.org/10.17632/zzwg3nnhsz.2.

</details>


### [69] [Increasing the Thinking Budget is Not All You Need](https://arxiv.org/abs/2512.19585)
*Ignacio Iacobacci,Zhaozhi Qian,Faroq AL-Tam,Muhammad AL-Qurishi,Riad Souissi*

Main category: cs.CL

TL;DR: 研究思维预算对语言模型推理性能的影响，提出更优的计算资源利用策略。


<details>
  <summary>Details</summary>
Motivation: 探讨思维预算（推理过程长度）对大型语言模型性能的影响，明确计算资源如何更有效地利用。

Method: 系统性考察思维预算与自洽性、自我反思等各种配置的相互作用，建立性能与计算成本的比较框架。

Result: 发现单纯增加思维预算并非最佳策略，通过自洽性和自我反思等配置可提升响应准确率。

Conclusion: 提高模型性能最有效的方式不是简单增加思维预算，而是采用自洽性和自我反思等方法。

Abstract: Recently, a new wave of thinking-capable Large Language Models has emerged, demonstrating exceptional capabilities across a wide range of reasoning benchmarks. Early studies have begun to explore how the amount of compute in terms of the length of the reasoning process, the so-called thinking budget, impacts model performance. In this work, we propose a systematic investigation of the thinking budget as a key parameter, examining its interaction with various configurations such as self-consistency, reflection, and others. Our goal is to provide an informative, balanced comparison framework that considers both performance outcomes and computational cost. Among our findings, we discovered that simply increasing the thinking budget is not the most effective use of compute. More accurate responses can instead be achieved through alternative configurations, such as self-consistency and self-reflection.

</details>


### [70] [MauBERT: Universal Phonetic Inductive Biases for Few-Shot Acoustic Units Discovery](https://arxiv.org/abs/2512.19612)
*Angelo Ortiz Tandazo,Manel Khentout,Youssef Benchekroun,Thomas Hueber,Emmanuel Dupoux*

Main category: cs.CL

TL;DR: MauBERT利用多语言发音特征增强HuBERT，实现了更鲁棒且语言无关的语音表征。


<details>
  <summary>Details</summary>
Motivation: 提升跨语言语音表征的鲁棒性，捕捉多语言共性的发音特征。

Method: 基于HuBERT扩展为多语言版本MauBERT，使用55种语言的语音到发音特征映射进行有监督预训练。

Result: MauBERT在ABX可区分性测试中表现优于现有多语言自监督模型，对未见过的语言和口语场景也能通过少量微调实现有效适应。

Conclusion: 通过引入语言学的归纳偏置，MauBERT显著提升了多语言语音自监督学习的表达能力和泛化性。

Abstract: This paper introduces MauBERT, a multilingual extension of HuBERT that leverages articulatory features for robust cross-lingual phonetic representation learning. We continue HuBERT pre-training with supervision based on a phonetic-to-articulatory feature mapping in 55 languages. Our models learn from multilingual data to predict articulatory features or phones, resulting in language-independent representations that capture multilingual phonetic properties. Through comprehensive ABX discriminability testing, we show MauBERT models produce more context-invariant representations than state-of-the-art multilingual self-supervised learning models. Additionally, the models effectively adapt to unseen languages and casual speech with minimal self-supervised fine-tuning (10 hours of speech). This establishes an effective approach for instilling linguistic inductive biases in self-supervised speech models.

</details>


### [71] [Exploring the features used for summary evaluation by Human and GPT](https://arxiv.org/abs/2512.19620)
*Zahra Sadeghi,Evangelos Milios,Frank Rudzicz*

Main category: cs.CL

TL;DR: 本文通过分析评价指标和引导GPT采用人类评测指标，提升了大语言模型在摘要质量评估中的表现和与人类判断的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽探讨了LLM与人类在摘要评估的一致性，但尚不清楚它们依据哪些特征进行评价，且评价分数与指标之间的映射关系较少被关注。

Method: 通过研究统计和机器学习指标，分析LLMs（尤其是GPTs）在摘要评估中的表现，并通过指令引导GPTs采用人类使用的评价指标以提升判断准确性。

Result: 发现了与人类及GPT响应相一致的特征，且通过指令让GPT采用人类使用的评价指标能够提升其评判能力，使结果更符合人类评价。

Conclusion: 引导GPTs使用人类评测指标可改善其评估摘要的能力，使得自动摘要评价更符合人类标准。

Abstract: Summary assessment involves evaluating how well a generated summary reflects the key ideas and meaning of the source text, requiring a deep understanding of the content. Large Language Models (LLMs) have been used to automate this process, acting as judges to evaluate summaries with respect to the original text. While previous research investigated the alignment between LLMs and Human responses, it is not yet well understood what properties or features are exploited by them when asked to evaluate based on a particular quality dimension, and there has not been much attention towards mapping between evaluation scores and metrics. In this paper, we address this issue and discover features aligned with Human and Generative Pre-trained Transformers (GPTs) responses by studying statistical and machine learning metrics. Furthermore, we show that instructing GPTs to employ metrics used by Human can improve their judgment and conforming them better with human responses.

</details>


### [72] [Diacritic Restoration for Low-Resource Indigenous Languages: Case Study with Bribri and Cook Islands Māori](https://arxiv.org/abs/2512.19630)
*Rolando Coto-Solano,Daisy Li,Manoela Teleginski Ferraz,Olivia Sasse,Cha Krupka,Sharid Loáiciga,Sally Akevai Tenamu Nicholas*

Main category: cs.CL

TL;DR: 针对两种极度资源匮乏语言，比较恢复元音符号的算法，发现微调的字符级大型语言模型效果最佳，约1万词数据可达稳定性能，零样本效果差。


<details>
  <summary>Details</summary>
Motivation: 弥补极度资源匮乏语言在自然语言处理中的基础任务不足，满足语言社区需求，同时研究模型在低资源环境下的性能表现及泛化能力。

Method: 采用多种算法进行元音符号恢复实验，重点测试字符级大型语言模型和多语言模型，评估不同数据规模对性能的影响，并探索相关的符号纠正任务。

Result: 本文通过实验分析了元音符号恢复任务，主要针对两种极度资源匮乏语言：哥斯达黎加的布里布里语和库克群岛的毛利语。研究比较了多种算法在元音符号恢复上的效果，并探讨了实现目标性能所需的数据量、不同资源条件下的结果差异以及相关的符号纠正任务。结果显示，经过微调的字符级大型语言模型表现最佳，原因在于它们能够将复杂字符分解为UTF-8字节表示，而大规模多语言模型在数据受限时效果不佳。所有模型在约1万词的数据量下开始展现稳定性能，零样本方法表现均较差。研究回应了语言社区需求，同时对稀缺资源环境下模型性能和泛化提出了见解。

Conclusion: 微调的字符级大型语言模型在极度资源匮乏语言的元音符号恢复任务中表现优异，且约需1万词数据支持，零样本方法效果不佳，强调微调和适度数据的重要性。

Abstract: We present experiments on diacritic restoration, a form of text normalization essential for natural language processing (NLP) tasks. Our study focuses on two extremely under-resourced languages: Bribri, a Chibchan language spoken in Costa Rica, and Cook Islands Māori, a Polynesian language spoken in the Cook Islands. Specifically, this paper: (i) compares algorithms for diacritics restoration in under-resourced languages, including tonal diacritics, (ii) examines the amount of data required to achieve target performance levels, (iii) contrasts results across varying resource conditions, and (iv) explores the related task of diacritic correction. We find that fine-tuned, character-level LLMs perform best, likely due to their ability to decompose complex characters into their UTF-8 byte representations. In contrast, massively multilingual models perform less effectively given our data constraints. Across all models, reliable performance begins to emerge with data budgets of around 10,000 words. Zero-shot approaches perform poorly in all cases. This study responds both to requests from the language communities and to broader NLP research questions concerning model performance and generalization in under-resourced contexts.

</details>


### [73] [Exploring Zero-Shot ACSA with Unified Meaning Representation in Chain-of-Thought Prompting](https://arxiv.org/abs/2512.19651)
*Filippos Ventirozos,Peter Appleby,Matthew Shardlow*

Main category: cs.CL

TL;DR: 提出基于UMR的链式思维零样本情感分析方法，模型规模影响效果，中等模型表现良好，小模型效果待验证。


<details>
  <summary>Details</summary>
Motivation: 针对新领域缺乏标注数据的问题，利用大语言模型的零样本能力，通过设计结构化推理方法提升情感分析效果，减少对人工标注的依赖。

Method: 采用统一语义表示（UMR）作为中间推理步骤的链式思维（CoT）提示技术，应用于零样本领域的方面类别情感分析。

Result: 本文提出了一种基于链式思维（CoT）提示的新方法，利用统一语义表示（UMR）中介结构来提高零样本情感分析任务的推理能力。通过在三个不同大语言模型（Qwen3-4B、Qwen3-8B和Gemini-2.5-Pro）以及四个数据集上对比实验，结果表明UMR方法的有效性可能依赖于模型规模。中等规模模型（如Qwen3-8B）表现与标准CoT方法相当，但对小模型的效果尚需进一步验证。

Conclusion: UMR方法在中等规模模型上表现良好，但其对不同规模模型的通用性仍需进一步研究。

Abstract: Aspect-Category Sentiment Analysis (ACSA) provides granular insights by identifying specific themes within reviews and their associated sentiment. While supervised learning approaches dominate this field, the scarcity and high cost of annotated data for new domains present significant barriers. We argue that leveraging large language models (LLMs) in a zero-shot setting is a practical alternative where resources for data annotation are limited. In this work, we propose a novel Chain-of-Thought (CoT) prompting technique that utilises an intermediate Unified Meaning Representation (UMR) to structure the reasoning process for the ACSA task. We evaluate this UMR-based approach against a standard CoT baseline across three models (Qwen3-4B, Qwen3-8B, and Gemini-2.5-Pro) and four diverse datasets. Our findings suggest that UMR effectiveness may be model-dependent. Whilst preliminary results indicate comparable performance for mid-sized models such as Qwen3-8B, these observations warrant further investigation, particularly regarding the potential applicability to smaller model architectures. Further research is required to establish the generalisability of these findings across different model scales.

</details>


### [74] [GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators](https://arxiv.org/abs/2512.19682)
*Jiacheng Guo,Ling Yang,Peter Chen,Qixin Xiao,Yinjie Wang,Xinzhe Juan,Jiahao Qiu,Ke Shen,Mengdi Wang*

Main category: cs.CL

TL;DR: GenEnv通过难度自适应的动态任务生成，显著提高大型语言模型代理的训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 解决训练大型语言模型代理因真实交互数据昂贵且静态而导致的瓶颈问题。

Method: 提出了GenEnv框架，通过代理与生成式环境模拟器之间的难度对齐共进化游戏，动态生成任务作为代理的训练环境。

Result: 在五个基准测试上，GenEnv提升了代理性能，最高提升40.3%，且数据使用量比基于离线数据增强的方法少3.3倍。

Conclusion: GenEnv以动态模拟代替静态监督，为扩展代理能力提供了高效的数据路径。

Abstract: Training capable Large Language Model (LLM) agents is critically bottlenecked by the high cost and static nature of real-world interaction data. We address this by introducing GenEnv, a framework that establishes a difficulty-aligned co-evolutionary game between an agent and a scalable, generative environment simulator. Unlike traditional methods that evolve models on static datasets, GenEnv instantiates a dataevolving: the simulator acts as a dynamic curriculum policy, continuously generating tasks specifically tailored to the agent's ``zone of proximal development''. This process is guided by a simple but effective $α$-Curriculum Reward, which aligns task difficulty with the agent's current capabilities. We evaluate GenEnv on five benchmarks, including API-Bank, ALFWorld, BFCL, Bamboogle, and TravelPlanner. Across these tasks, GenEnv improves agent performance by up to \textbf{+40.3\%} over 7B baselines and matches or exceeds the average performance of larger models. Compared to Gemini 2.5 Pro-based offline data augmentation, GenEnv achieves better performance while using 3.3$\times$ less data. By shifting from static supervision to adaptive simulation, GenEnv provides a data-efficient pathway for scaling agent capabilities.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [75] [ShibuyaSocial: Multi-scale Model of Pedestrian Flows in Scramble Crossing](https://arxiv.org/abs/2512.18550)
*Akihiro Sakurai,Naoya Kajio,Ko Yamamoto*

Main category: cs.MA

TL;DR: 本文提出了一种学习型行人流动模型，融合了全球路径选择和局部避碰行为，重点研究涩谷十字路口的行人运动。


<details>
  <summary>Details</summary>
Motivation: 过度拥挤的行人流动易引发事故，现有研究多集中于局部避碰，缺乏对全球路径决策及交通灯状态转换的综合考量。

Method: 利用涩谷十字路口的行人轨迹视频数据，采用注意力机制的学习模型，整合全球路径选择与局部避碰行为进行训练和仿真。

Result: 仿真结果定性和定量验证所提模型能准确预测行人行为，体现了多尺度行为整合的有效性。

Conclusion: 所提模型通过结合多尺度行为和注意力机制，实现了对行人行为的准确预测，有助于预防拥堵事故并提升环境安全与舒适性。

Abstract: This paper presents a learning-based model of pedestrian flows that integrates multi scale behaviors such as global route selection and local collision avoidance in urban spaces, particularly focusing on pedestrian movements at Shibuya scramble crossing. Since too much congestion of pedestrian flows can cause serious accidents, mathematically modeling and predicting pedestrian behaviors is important for preventing such accidents and providing a safe and comfortable environment. Although numerous studies have investigated learning-based modeling methods, most of them focus only on the local behavior of pedestrians, such as collision avoidance with neighbors and environmental objects. In an actual environment, pedestrian behavior involves more complicated decision making including global route selection. Moreover, a state transition from stopping to walking at a traffic light should be considered simultaneously. In this study, the proposed model integrates local behaviors with global route selection, using an Attention mechanism to ensure consistent global and local behavior predictions. We recorded video data of pedestrians at Shibuya scramble crossing and trained the proposed model using pedestrian walking trajectory data obtained from the video. Simulations of pedestrian behaviors based on the trained model qualitatively and quantitatively validated that the proposed model can appropriately predict pedestrian behaviors.

</details>


### [76] [Adaptive Accountability in Networked MAS: Tracing and Mitigating Emergent Norms at Scale](https://arxiv.org/abs/2512.18561)
*Saad Alqithami*

Main category: cs.MA

TL;DR: 本文提出了一种适应性问责框架，通过生命周期感知的审计账本追踪责任流、在线检测有害涌现规范，并实时部署局部策略干预，实现多智能体系统中的伦理自我调节。


<details>
  <summary>Details</summary>
Motivation: 大规模网络多智能体系统在关键基础设施中应用广泛，但其集体行为可能偏离目标而形成不良规范，传统治理机制难以有效管控。

Method: 引入了生命周期感知审计账本追踪责任流，采用分散的顺序假设测试在线检测有害涌现规范，并通过局部策略和奖励整形进行实时干预。

Result: 理论证明了介入成本超过对手收益时受损交互比例有界，并通过模拟展示框架可在90%以上配置防止勾结，提升12-18%集体奖励，降低33%基尼系数。

Conclusion: 该框架能有效阻止勾结和资源囤积，提升集体奖励并降低不平等，证明了添加问责层可以在不损失性能和扩展性的情况下实现复杂多智能体系统的伦理对齐自我调节行为。

Abstract: Large-scale networked multi-agent systems increasingly underpin critical infrastructure, yet their collective behavior can drift toward undesirable emergent norms that elude conventional governance mechanisms. We introduce an adaptive accountability framework that (i) continuously traces responsibility flows through a lifecycle-aware audit ledger, (ii) detects harmful emergent norms online via decentralized sequential hypothesis tests, and (iii) deploys local policy and reward-shaping interventions that realign agents with system-level objectives in near real time. We prove a bounded-compromise theorem showing that whenever the expected intervention cost exceeds an adversary's payoff, the long-run proportion of compromised interactions is bounded by a constant strictly less than one. Extensive high-performance simulations with up to 100 heterogeneous agents, partial observability, and stochastic communication graphs show that our framework prevents collusion and resource hoarding in at least 90% of configurations, boosts average collective reward by 12-18%, and lowers the Gini inequality index by up to 33% relative to a PPO baseline. These results demonstrate that a theoretically principled accountability layer can induce ethically aligned, self-regulating behavior in complex MAS without sacrificing performance or scalability.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [77] [Victor Calibration (VC): Multi-Pass Confidence Calibration and CP4.3 Governance Stress Test under Round-Table Orchestration](https://arxiv.org/abs/2512.17956)
*Victor Stasiuc,Round Table Collaboration*

Main category: cs.SE

TL;DR: 作者提出了一套轻量级工具包，用于评估前沿语言模型在安全对齐下的行为表现，验证其置信度和治理特性，发现模型行为稳定且未违反安全要求。


<details>
  <summary>Details</summary>
Motivation: 前沿语言模型的安全对齐可能导致模型过度保守，影响合作表现，出现避免风险和错误拒绝问题，因此需要更精细的评估和测试方法。

Method: 提出了一个轻量级工具包，包括三部分：1) Victor Calibration (VC)，通过多次迭代证据重新评估获取标量置信度代理T；2) FD-Lite，一种行为现象学审核方法，使用固定锚语和元前缀陷阱避免拟人化假设；3) CP4.3，一种治理压力测试，检测排名不变性和分配单调性。

Result: 在Claude 4.5多个模型和Opus上观察到VC轨迹保持单调且不违反安全不变式，CP4.3测试表现稳定。

Conclusion: 本研究为安全对齐中模型保守性问题提供了评估和验证方法，数据稳定且支持后续研究，鼓励社区进行复现和扩展。

Abstract: Safety alignment can make frontier LMs overly conservative, degrading collaboration via hedging or false refusals. We present a lightweight toolkit with three parts: (1) Victor Calibration (VC), a multi-pass protocol that elicits a scalar confidence proxy T (T0<T1<T2) through iterative evidence re-evaluation; (2) FD-Lite, a behavior-only phenomenology audit with a fixed anchor phrase and a meta-prefix trap to avoid anthropomorphic claims; and (3) CP4.3, a governance stress test for rank invariance and allocation monotonicity (M6). Across Claude 4.5 models (Haiku, Sonnet no-thinking, Sonnet thinking) and Opus, we observe monotonic VC trajectories without violating safety invariants, and stable CP4.3 behavior. ("Opus" here refers to a single Claude Opus 4.1 session accessed via a standard UI account, as reported in Table 1.) This work was conducted by a single operator (n=1) and is intended as hypothesis-generating; we explicitly invite replication, critique, and extension by the research community. We include prompt templates and an artifact plan to facilitate independent verification.

</details>


### [78] [Specification and Detection of LLM Code Smells](https://arxiv.org/abs/2512.18020)
*Brahim Mahmoudi,Zacharie Chenail-Larcher,Naouel Moha,Quentin Stievenert,Florent Avellaneda*

Main category: cs.SE

TL;DR: 本文首次定义并检测了与大语言模型推理相关的代码异味，发现其在多数开源系统中普遍存在，提示需关注LLM集成质量。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏针对LLM推理的特定代码异味目录，错误的集成方式可能降低软件系统质量，因此需要系统识别和定义相关异味。

Method: 基于文献研究，形式化了五种LLM推理相关的代码异味；扩展检测工具SpecDetect4AI以识别这些异味；使用该工具检测了200个开源LLM系统中的代码异味。

Result: 检测工具显示60.50%的系统存在LLM代码异味，检测准确率达到86.06%。

Conclusion: 本论文建立了大语言模型（LLMs）推理相关的代码异味定义，并证实这些异味在开源系统中普遍存在，影响软件质量。

Abstract: Large Language Models (LLMs) have gained massive popularity in recent years and are increasingly integrated into software systems for diverse purposes. However, poorly integrating them in source code may undermine software system quality. Yet, to our knowledge, there is no formal catalog of code smells specific to coding practices for LLM inference. In this paper, we introduce the concept of LLM code smells and formalize five recurrent problematic coding practices related to LLM inference in software systems, based on relevant literature. We extend the detection tool SpecDetect4AI to cover the newly defined LLM code smells and use it to validate their prevalence in a dataset of 200 open-source LLM systems. Our results show that LLM code smells affect 60.50% of the analyzed systems, with a detection precision of 86.06%.

</details>


### [79] [SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios](https://arxiv.org/abs/2512.18470)
*Minh V. T. Thai,Tue Le,Dung Nguyen Manh,Huy Phan Nhat,Nghi D. Q. Bui*

Main category: cs.SE

TL;DR: 提出一个多步骤、多文件的软件演进基准SWE-EVO，揭示当前AI模型在复杂长远任务上的显著不足，并提出新的评估指标Fix Rate。


<details>
  <summary>Details</summary>
Motivation: 现有的AI代码代理基准测试多聚焦于单一任务，而实际软件工程是一个长远且复杂的过程，需要跨多文件的协调和多次迭代。

Method: 引入SWE-EVO基准，该基于七个成熟开源Python项目的版本历史和发布说明构建，包含48个多步骤、多文件的软件演进任务，设有详尽测试套件进行验证。

Result: 实验显示，现有最先进模型如GPT-5在该基准上的解决率仅为21%，相比单一任务基准的65%明显较低，证明当前模型在持续多文件推理能力上存在显著不足。

Conclusion: 提出的SWE-EVO基准有效揭示了长远多文件软件演进任务中AI代理的能力瓶颈，且引入了细粒度的Fix Rate指标用于衡量部分进度。

Abstract: Existing benchmarks for AI coding agents focus on isolated, single-issue tasks such as fixing a bug or implementing a small feature. However, real-world software engineering is fundamentally a long-horizon endeavor: developers must interpret high-level requirements, plan coordinated changes across many files, and evolve codebases over multiple iterations while preserving existing functionality. We introduce SWE-EVO, a benchmark that evaluates agents on this long-horizon software evolution challenge. Constructed from release notes and version histories of seven mature open-source Python projects, Tool comprises 48 evolution tasks that require agents to implement multi-step modifications spanning an average of 21 files, validated against comprehensive test suites averaging 874 tests per instance. Experiments with state-of-the-art models reveal a striking capability gap: even GPT-5 with OpenHands achieves only a 21 percent resolution rate on Tool, compared to 65 percent on the single-issue SWE-Bench Verified. This demonstrates that current agents struggle with sustained, multi-file reasoning. We also propose Fix Rate, a fine-grained metric that captures partial progress toward solving these complex, long-horizon tasks.

</details>


### [80] [Detecting Flaky Tests in Quantum Software: A Dynamic Approach](https://arxiv.org/abs/2512.18088)
*Dongchan Kim,Hamidreza Khoramrokh,Lei Zhang,Andriy Miranskyy*

Main category: cs.SE

TL;DR: 首次大规模动态研究量子软件的易变测试，确认其罕见但检测困难，发布数据集助力未来研究。


<details>
  <summary>Details</summary>
Motivation: 量子软件中易变测试（flaky tests）对软件可靠性构成严重威胁，而现有研究缺乏大规模动态分析支持。

Method: 在受控环境下对Qiskit Terra测试套件的23个版本执行了10000次测试，测量测试结果变异性，识别易变测试，估计失败概率，分析版本间的反复出现，并用Wilson置信区间量化可靠检测的重跑预算，进一步映射到子组件。

Result: 在27026个测试用例中发现290个易变测试，整体易变率较低（0-0.4%），大多数易变测试只在单一版本出现，小部分间歇或持续出现。易变测试失败概率极低，检测难度大，且易变性在子组件中分布不均，主要集中在'transpiler'和'quantum_info'。

Conclusion: 量子软件测试中的易变测试虽然罕见但难以检测，常规持续集成难以有效捕获此类问题。公开了每个测试执行结果的数据集以支持后续研究。

Abstract: Flaky tests, tests that pass or fail nondeterministically without changes to code or environment, pose a serious threat to software reliability. While classical software engineering has developed a rich body of dynamic and static techniques to study flakiness, corresponding evidence for quantum software remains limited. Prior work relies primarily on static analysis or small sets of manually reported incidents, leaving open questions about the prevalence, characteristics, and detectability of flaky tests.
  This paper presents the first large-scale dynamic characterization of flaky tests in quantum software. We executed the Qiskit Terra test suite 10,000 times across 23 releases in controlled environments. For each release, we measured test-outcome variability, identified flaky tests, estimated empirical failure probabilities, analyzed recurrence across versions, and used Wilson confidence intervals to quantify rerun budgets for reliable detection. We further mapped flaky tests to Terra subcomponents to assess component-level susceptibility.
  Across 27,026 test cases, we identified 290 distinct flaky tests. Although overall flakiness rates were low (0-0.4%), flakiness was highly episodic: nearly two-thirds of flaky tests appeared in only one release, while a small subset recurred intermittently or persistently. Many flaky tests failed with very small empirical probabilities ($\hat{p} \approx 10^{-4}$), implying that tens of thousands of executions may be required for confident detection. Flakiness was unevenly distributed across subcomponents, with 'transpiler' and 'quantum_info' accounting for the largest share.
  These results show that quantum test flakiness is rare but difficult to detect under typical continuous integration budgets. To support future research, we release a public dataset of per-test execution outcomes.

</details>


### [81] [From Coverage to Causes: Data-Centric Fuzzing for JavaScript Engines](https://arxiv.org/abs/2512.18102)
*Kishan Kumar Ganguly,Tim Menzies*

Main category: cs.SE

TL;DR: 本文通过结合历史漏洞特征，构建静态与动态特征驱动的机器学习模型，提高JavaScript模糊测试效率和准确性，替代传统覆盖率导向方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于覆盖率的模糊测试在现代JavaScript引擎中效率低下，浪费资源在低风险输入上，且难以发现不增加覆盖率但能触发漏洞的高风险输入。现有启发式方法依赖专家经验，适应性差且脆弱。

Method: 基于历史V8漏洞，使用迭代性提示生成115个静态特征和49个动态特征（只需5个运行时追踪标志），通过特征选择保留41个特征，利用XGBoost模型训练预测高风险输入。

Result: 结合静态和动态特征，模型达到超过85%的准确率和低于1%的误报率，只需用到25%的特征即可维持类似性能，表明大部分搜索空间无关紧要。

Conclusion: 本文提出特征引导模糊测试技术，用基于数据的推断替代覆盖率引导，更快、更有针对性和可重复地发现漏洞，相关代码及数据公开，促进开放科学。

Abstract: Context: Exhaustive fuzzing of modern JavaScript engines is infeasible due to the vast number of program states and execution paths. Coverage-guided fuzzers waste effort on low-risk inputs, often ignoring vulnerability-triggering ones that do not increase coverage. Existing heuristics proposed to mitigate this require expert effort, are brittle, and hard to adapt.
  Objective: We propose a data-centric, LLM-boosted alternative that learns from historical vulnerabilities to automatically identify minimal static (code) and dynamic (runtime) features for detecting high-risk inputs.
  Method: Guided by historical V8 bugs, iterative prompting generated 115 static and 49 dynamic features, with the latter requiring only five trace flags, minimizing instrumentation cost. After feature selection, 41 features remained to train an XGBoost model to predict high-risk inputs during fuzzing.
  Results: Combining static and dynamic features yields over 85% precision and under 1% false alarms. Only 25% of these features are needed for comparable performance, showing that most of the search space is irrelevant.
  Conclusion: This work introduces feature-guided fuzzing, an automated data-driven approach that replaces coverage with data-directed inference, guiding fuzzers toward high-risk states for faster, targeted, and reproducible vulnerability discovery. To support open science, all scripts and data are available at https://github.com/KKGanguly/DataCentricFuzzJS .

</details>


### [82] [Holistic Evaluation of State-of-the-Art LLMs for Code Generation](https://arxiv.org/abs/2512.18131)
*Le Zhang,Suresh Kothari*

Main category: cs.SE

TL;DR: 本文评估了六种大型语言模型在代码生成上的表现，DeepSeek-R1和GPT-4.1表现最佳，并指出了常见错误及优化方向，强调提示工程和人机结合的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前虽有多种大型语言模型支持代码生成，但缺乏系统评估其实际编程任务表现，尤其是在多语言、多指标的真实数据集上，因此本研究旨在填补这一空白，指导实际应用。

Method: 使用944个LeetCode实际题目测试六款语言模型，评估指标包括编译期错误、运行期错误、功能失效与算法效率，通过案例分析深入探讨失败原因。

Result: 该研究全面评估了六种先进的大型语言模型在代码生成任务中的表现，利用包含944个真实LeetCode题目、覆盖五种编程语言的数据集，通过编译错误、运行错误、功能失效和算法次优等指标进行严格评估。结果显示DeepSeek-R1和GPT-4.1在准确性、效率和稳健性方面表现最佳。研究还通过案例分析揭示了常见失败类型，如语法错误、逻辑缺陷和算法次优，强调提示工程和人工监督的重要性。基于这些发现，提出了针对开发者的具体建议，强调成功部署LLM依赖于模型选择、提示设计及上下文感知的综合考量。

Conclusion: LLM的代码生成能力存在显著差异，成功应用需结合模型优势、有效提示设计及人类监督，以提升生成代码的正确性和实用性。

Abstract: This study presents a comprehensive empirical evaluation of six state-of-the-art large language models (LLMs) for code generation, including both general-purpose and code-specialized models. Using a dataset of 944 real-world LeetCode problems across five programming languages, we assess model performance using rigorous metrics: compile-time errors, runtime errors, functional failures, and algorithmic suboptimalities. The results reveal significant performance variations, with DeepSeek-R1 and GPT-4.1 consistently outperform others in terms of correctness, efficiency, and robustness. Through detailed case studies, we identify common failure scenarios such as syntax errors, logical flaws, and suboptimal algorithms, highlighting the critical role of prompt engineering and human oversight in improving results. Based on these findings, we provide actionable recommendations for developers and practitioners, emphasizing that successful LLM deployment depends on careful model selection, effective prompt design, and context-aware usage to ensure reliable code generation in real-world software development tasks.

</details>


### [83] [Understanding Typing-Related Bugs in Solidity Compiler](https://arxiv.org/abs/2512.18182)
*Lantian Li,Yue Pan,Dan Wang,Jingwen Wu,Zhongxing Yu*

Main category: cs.SE

TL;DR: 本文是首个针对Solidity编译器类型相关bug的系统实证研究，通过分析146个官方确认bug，总结出关键特征和防治策略，为强化智能合约安全提供了重要支持。


<details>
  <summary>Details</summary>
Motivation: Solidity编译器的类型系统实现复杂，容易导致隐蔽的缺陷，这些缺陷威胁智能合约的安全性，因此亟需系统性地理解和分析此类类型相关的bug。

Method: 系统地收集并分析了Solidity编译器中与类型相关的146个官方确认和修复的bug，从症状、根本原因、暴露条件和修复策略四个维度进行了深入分类和研究。

Result: 揭示了类型相关漏洞的独特分布模式和关键特征，总结了12条核心发现，并给出了这些发现对理解编译器弱点及改进检测和修复策略的启示。

Conclusion: 本文系统揭示了Solidity编译器类型相关bug的核心特点和分布规律，深化了对其内在弱点的理解，并为未来检测与修复此类bug提供了有价值的参考和指导。

Abstract: The correctness of the Solidity compiler is crucial for ensuring the security of smart contracts. However, the implementation complexity of its type system often introduces elusive defects. This paper presents the first systematic empirical study on typing-related bugs in the Solidity compiler. To systematically analyze these bugs, we collected 146 officially confirmed and fixed typing-related bugs from the official GitHub repository of Solidity compiler. For each bug, we conducted an in-depth analysis and classification from four dimensions: symptoms, root causes, exposure conditions, and fix strategies. Through this study, we reveal unique distribution patterns and key characteristics of such bugs, and summarize 12 core findings. We additionally give the implications of our findings, and these implications not only deepen the understanding of inherent weaknesses in the Solidity compiler but also provide new insights for detecting and fixing typing-related bugs in the Solidity compiler.

</details>


### [84] [Toward Efficient Testing of Graph Neural Networks via Test Input Prioritization](https://arxiv.org/abs/2512.18228)
*Lichen Yang,Qiang Wang,Zhonghao Yang,Daojing He,Yu Li*

Main category: cs.SE

TL;DR: GraphRank利用图结构和多样属性优先选择测试数据，提升GNN测试效果。


<details>
  <summary>Details</summary>
Motivation: 鉴于GNN测试数据标注成本高，现有方法忽视图结构或过度依赖单一模型相关属性，急需一种能够充分利用图结构且减少注释成本的高效测试输入优先级排序方法。

Method: 引入模型无关属性补充模型相关属性，利用图结构聚合邻居节点属性，结合二分类器迭代训练，作为排序模型优先选择测试输入。

Result: 本文提出了一种名为GraphRank的图神经网络（GNN）测试输入优先级排序框架，通过结合模型无关属性和图结构信息，提高了测试中输入数据的选择效率，显著提升了模型失败检测能力。

Conclusion: GraphRank有效结合模型无关和模型相关属性及图结构信息，通过迭代训练的二分类排序模型，提高了测试输入的优先级排序性能，优于现有方法。

Abstract: Graph Neural Networks (GNNs) have demonstrated remarkable efficacy in handling graph-structured data; however, they exhibit failures after deployment, which can cause severe consequences. Hence, conducting thorough testing before deployment becomes imperative to ensure the reliability of GNNs. However, thorough testing requires numerous manually annotated test data. To mitigate the annotation cost, strategically prioritizing and labeling high-quality unlabeled inputs for testing becomes crucial, which facilitates uncovering more model failures with a limited labeling budget. Unfortunately, existing test input prioritization techniques either overlook the valuable information contained in graph structures or are overly reliant on attributes extracted from the target model, i.e., model-aware attributes, whose quality can vary significantly. To address these issues, we propose a novel test input prioritization framework, named GraphRank, for GNNs. GraphRank introduces model-agnostic attributes to compensate for the limitations of the model-aware ones. It also leverages the graph structure information to aggregate attributes from neighboring nodes, thereby enhancing the model-aware and model-agnostic attributes. Furthermore, GraphRank combines the above attributes with a binary classifier, using it as a ranking model to prioritize inputs. This classifier undergoes iterative training, which enables it to learn from each round's feedback and improve its performance accordingly. Extensive experiments demonstrate GraphRank's superiority over existing techniques.

</details>


### [85] [Software Vulnerability Management in the Era of Artificial Intelligence: An Industry Perspective](https://arxiv.org/abs/2512.18261)
*M. Mehdi Kholoosi,Triet Huynh Minh Le,M. Ali Babar*

Main category: cs.SE

TL;DR: AI在软件漏洞管理中广泛采用，用户认可其效率和覆盖，但仍存在信任和误报等问题，结合人类监督可提升使用效果。建议改进工具以满足工业应用需求。


<details>
  <summary>Details</summary>
Motivation: 当前虽然AI在软件开发中广泛应用，但针对软件漏洞管理（SVM）中AI工具的应用尚未得到充分研究，特别是在工业环境中。

Method: 通过对来自27个国家60位从业者进行包含定量和定性问题的调查，分析AI在SVM中的采用情况、工具优缺点和使用障碍。

Result: 发现69%的用户对AI驱动的SVM工具表示满意，工具加快了速度、覆盖面和可达性，但存在误报、缺乏上下文和信任问题。采用模式体现了人类监督和组织治理的作用。

Conclusion: 为了实现安全有效的AI辅助SVM，需提升工具的可解释性、上下文感知能力、集成流程及验证机制，研究成果为从业者和开发者提供了实用指导。

Abstract: Artificial Intelligence (AI) has revolutionized software development, particularly by automating repetitive tasks and improving developer productivity. While these advancements are well-documented, the use of AI-powered tools for Software Vulnerability Management (SVM), such as vulnerability detection and repair, remains underexplored in industry settings. To bridge this gap, our study aims to determine the extent of the adoption of AI-powered tools for SVM, identify barriers and facilitators to the use, and gather insights to help improve the tools to meet industry needs better. We conducted a survey study involving 60 practitioners from diverse industry sectors across 27 countries. The survey incorporates both quantitative and qualitative questions to analyze the adoption trends, assess tool strengths, identify practical challenges, and uncover opportunities for improvement. Our findings indicate that AI-powered tools are used throughout the SVM life cycle, with 69\% of users reporting satisfaction with their current use. Practitioners value these tools for their speed, coverage, and accessibility. However, concerns about false positives, missing context, and trust issues remain prevalent. We observe a socio-technical adoption pattern in which AI outputs are filtered through human oversight and organizational governance. To support safe and effective use of AI for SVM, we recommend improvements in explainability, contextual awareness, integration workflows, and validation practices. We assert that these findings can offer practical guidance for practitioners, tool developers, and researchers seeking to enhance secure software development through the use of AI.

</details>


### [86] [Toward Training Superintelligent Software Agents through Self-Play SWE-RL](https://arxiv.org/abs/2512.18552)
*Yuxiang Wei,Zhiqing Sun,Emily McMilin,Jonas Gehring,David Zhang,Gabriel Synnaeve,Daniel Fried,Lingming Zhang,Sida Wang*

Main category: cs.SE

TL;DR: 提出一种无需人工标注、基于自我对弈强化学习的训练范式，用于提升软件代理智能，展现了其在真实代码库中自我迭代和超越人工标注数据限制的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前由大型语言模型和强化学习驱动的软件代理依赖人工标注的数据和环境，限制了其达到超智能的能力。

Method: 采用自我对弈的强化学习方法（Self-play SWE-RL，SSR），基于访问带有源代码和依赖项的沙箱代码仓库，无需人工标注的问题或测试，通过单个大语言模型代理在自我对弈环境下不断注入和修复软件漏洞，漏洞以测试补丁形式规范定义。

Result: SSR在SWE-bench Verified和SWE-Bench Pro基准测试中实现了显著的自我改进，且持续超越基于人工数据的基线表现，即便在评估时面对未见过的自然语言问题。

Conclusion: 研究表明代理能够自主从真实代码库中积累学习经验，推动实现超越人类的软件智能系统，能够理解系统构造、解决新问题并自主创造新软件。

Abstract: While current software agents powered by large language models (LLMs) and agentic reinforcement learning (RL) can boost programmer productivity, their training data (e.g., GitHub issues and pull requests) and environments (e.g., pass-to-pass and fail-to-pass tests) heavily depend on human knowledge or curation, posing a fundamental barrier to superintelligence. In this paper, we present Self-play SWE-RL (SSR), a first step toward training paradigms for superintelligent software agents. Our approach takes minimal data assumptions, only requiring access to sandboxed repositories with source code and installed dependencies, with no need for human-labeled issues or tests. Grounded in these real-world codebases, a single LLM agent is trained via reinforcement learning in a self-play setting to iteratively inject and repair software bugs of increasing complexity, with each bug formally specified by a test patch rather than a natural language issue description. On the SWE-bench Verified and SWE-Bench Pro benchmarks, SSR achieves notable self-improvement (+10.4 and +7.8 points, respectively) and consistently outperforms the human-data baseline over the entire training trajectory, despite being evaluated on natural language issues absent from self-play. Our results, albeit early, suggest a path where agents autonomously gather extensive learning experiences from real-world software repositories, ultimately enabling superintelligent systems that exceed human capabilities in understanding how systems are constructed, solving novel challenges, and autonomously creating new software from scratch.

</details>


### [87] [AI Code in the Wild: Measuring Security Risks and Ecosystem Shifts of AI-Generated Code in Modern Software](https://arxiv.org/abs/2512.18567)
*Bin Wang,Wenjie Yu,Yilu Zhong,Hao Yu,Keke Lian,Chaohua Lu,Hongfang Zheng,Dong Zhang,Hui Li*

Main category: cs.SE

TL;DR: 本文首次大规模研究了AI生成代码在真实项目中的普及及安全影响，发现AI主要用于辅助代码且带来安全挑战，人工审核对减少AI缺陷扩散至关重要。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对AI生成代码在真实软件开发中普及程度和安全影响的系统理解，本文旨在填补这一空白，为代码安全管理提供数据支持和分析工具。

Method: 构建高精度检测管线和代表性基准，通过分析GitHub顶级项目的开发提交及CVE相关代码变更，标注并追踪AI与人工代码的分布及安全影响。

Result: 本文首次对野外AI生成代码（AIGCode）进行了大规模实证研究，构建了高精度检测和基准测试方法，分析了2022-2025年GitHub前1000名项目的提交和7000多个CVE相关代码变化。研究发现AIGCode主要集中在辅助性代码中，核心逻辑和安全关键代码仍多为人工编写。此外，AI生成代码中某些安全漏洞家族过度存在，表明AI模型可能传播安全风险。当人工审核不足时，AI引入的缺陷更可能长期存在并扩散。文章将开源完整数据集和相关分析工具。

Conclusion: AI生成代码已成为代码库的重要组成部分，尽管多用于辅助性代码，但存在“AI诱发漏洞”风险，需加强人工审查以防缺陷扩散。

Abstract: Large language models (LLMs) for code generation are becoming integral to modern software development, but their real-world prevalence and security impact remain poorly understood.
  We present the first large-scale empirical study of AI-generated code (AIGCode) in the wild. We build a high-precision detection pipeline and a representative benchmark to distinguish AIGCode from human-written code, and apply them to (i) development commits from the top 1,000 GitHub repositories (2022-2025) and (ii) 7,000+ recent CVE-linked code changes. This lets us label commits, files, and functions along a human/AI axis and trace how AIGCode moves through projects and vulnerability life cycles.
  Our measurements show three ecological patterns. First, AIGCode is already a substantial fraction of new code, but adoption is structured: AI concentrates in glue code, tests, refactoring, documentation, and other boilerplate, while core logic and security-critical configurations remain mostly human-written. Second, adoption has security consequences: some CWE families are overrepresented in AI-tagged code, and near-identical insecure templates recur across unrelated projects, suggesting "AI-induced vulnerabilities" propagated by shared models rather than shared maintainers. Third, in human-AI edit chains, AI introduces high-throughput changes while humans act as security gatekeepers; when review is shallow, AI-introduced defects persist longer, remain exposed on network-accessible surfaces, and spread to more files and repositories.
  We will open-source the complete dataset and release analysis artifacts and fine-grained documentation of our methodology and findings.

</details>


### [88] [Code2Doc: A Quality-First Curated Dataset for Code Documentation](https://arxiv.org/abs/2512.18748)
*Recep Kaan Karaman,Meftun Akarsu*

Main category: cs.SE

TL;DR: Code2Doc是一个高质量的函数级代码文档数据集，通过严格筛选保证数据质量，显著提升模型性能并支持复现研究。


<details>
  <summary>Details</summary>
Motivation: 现有代码文档数据集质量参差不齐，存在噪声、重复和人工智能生成内容污染，影响自动代码文档生成模型的性能及评估准确性。

Method: 设计一个四阶段质量过滤流程，确保文档完整清晰，过滤函数结构和复杂度，去除重复代码，并识别疑似AI生成文档，从52,069个候选项中筛选出13,358个高质量函数-文档对。

Result: Code2Doc数据集覆盖五种编程语言，文档质量得分6.93/10，86.9%含显式类型注释，只有2.9%疑似AI生成。基于此数据集微调大型语言模型，BLEU和ROUGE-L指标分别提升29.47%和24.04%。

Conclusion: 高质量、经过严格筛选的Code2Doc数据集能显著提升自动代码文档生成模型的性能，并为相关研究提供可复现的数据和方法支持。

Abstract: The performance of automatic code documentation generation models depends critically on the quality of the training data used for supervision. However, most existing code documentation datasets are constructed through large scale scraping of public repositories with limited quality control. As a result, they often contain noisy documentation, extensive duplication, and increasing contamination from AI generated content. These issues weaken the supervision signal available to learning-based models and complicate evaluation.
  We introduce \textbf{Code2Doc}, a quality-first curated dataset for function-level code documentation generation. Code2Doc consists of 13,358 high-quality function-documentation pairs extracted from widely used open-source repositories spanning five programming languages: Python, Java, TypeScript, JavaScript, and C++. The dataset is constructed using a four-stage curation pipeline that enforces documentation completeness and clarity, filters functions based on structural and complexity criteria, removes exact and near-duplicate code, and identifies documentation likely to be AI generated. Starting from 52,069 extracted candidates, only 25.6 percent satisfy all quality constraints.
  We provide a detailed analysis of the resulting dataset, which achieves a mean documentation quality score of 6.93 out of 10. Overall, 86.9% of samples contain explicit type annotations, and only 2.9\% are flagged as potentially AI generated. Baseline experiments show that fine-tuning a large language model on Code2Doc yields relative improvements of 29.47% in BLEU and 24.04% in ROUGE-L over zero shot performance, despite the modest dataset size. We release both the dataset and the full curation pipeline to support reproducible research on automatic code documentation generation.

</details>


### [89] [Misbehavior Forecasting for Focused Autonomous Driving Systems Testing](https://arxiv.org/abs/2512.18823)
*M M Abid Naziri,Stefano Carlo Lambertenghi,Andrea Stocco,Marcelo d'Amorim*

Main category: cs.SE

TL;DR: Foresee通过预测近失误并局部模糊测试，有效提高自动驾驶软件的缺陷检测率和效率。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶软件缺陷检测方法存在不可靠或成本高的问题，近失误现象可能揭示潜在失败，有望提升测试的准确性和效率。

Method: 通过建立失误预测器预测交通场景中车辆未来状态，识别近失误并针对其邻近状态进行局部模糊测试，以探测潜在软件失败。

Result: 本文提出了一种名为Foresee的新技术，通过利用模拟中的近失误（near misses）预测潜在的软件失败，从而提升自动驾驶汽车软件测试的有效性和效率。Foresee利用一个失误预测器预测未来可能的车辆状态，并在每个潜在近失误附近进行局部模糊测试，以发现未知缺陷。实验在CARLA模拟器中通过多种场景及不同自动驾驶系统验证了Foresee的性能，结果显示其在发现缺陷数量和测试速度上均优于随机方法和现有先进的失败预测器，以及与现有模糊测试工具DriveFuzz结合使用时进一步提升故障检测能力。

Conclusion: Foresee在自动驾驶软件测试中显著提升了缺陷发现数量和效率，优于现有方法，并能有效加强与其他模糊测试工具的协同效果。

Abstract: Simulation-based testing is the standard practice for assessing the reliability of self-driving cars' software before deployment. Existing bug-finding techniques are either unreliable or expensive. We build on the insight that near misses observed during simulations may point to potential failures. We propose Foresee, a technique that identifies near misses using a misbehavior forecaster that computes possible future states of the ego-vehicle under test. Foresee performs local fuzzing in the neighborhood of each candidate near miss to surface previously unknown failures. In our empirical study, we evaluate the effectiveness of different configurations of Foresee using several scenarios provided in the CARLA simulator on both end-to-end and modular self-driving systems and examine its complementarity with the state-of-the-art fuzzer DriveFuzz. Our results show that Foresee is both more effective and more efficient than the baselines. Foresee exposes 128.70% and 38.09% more failures than a random approach and a state-of-the-art failure predictor while being 2.49x and 1.42x faster, respectively. Moreover, when used in combination with DriveFuzz, Foresee enhances failure detection by up to 93.94%.

</details>


### [90] [What Drives Issue Resolution Speed? An Empirical Study of Scientific Workflow Systems on GitHub](https://arxiv.org/abs/2512.18852)
*Khairul Alam,Banani Roy*

Main category: cs.SE

TL;DR: 研究分析了GitHub上的科学工作流系统项目，发现问题标记和分配能显著加快问题解决速度，提出改进问题管理的建议以提升软件质量。


<details>
  <summary>Details</summary>
Motivation: SWS作为关键科学分析工具，其可靠性和可持续性依赖于积极的维护和社区参与，而及时的问题解决对于软件质量和社区信任至关重要，但当前缺乏对影响问题解决速度因素的深入了解。

Method: 基于对托管于GitHub上的多个SWS项目共21,116个问题的实证分析，研究项目特性、问题元数据和贡献者互动如何影响问题关闭时间。

Result: 发现68.91%的问题被关闭，其中一半问题在18.09天内解决。SWS项目虽然遵循结构化的问题管理流程，但问题解决速度差异大。标记(labeling)和指派(assigning)问题与更快的解决速度相关。

Conclusion: 科学工作流系统(SWS)中的问题解决速度受到多个因素影响，且不同系统之间存在显著差异。有效的问题标记和分配有助于加快问题解决速度。

Abstract: Scientific Workflow Systems (SWSs) play a vital role in enabling reproducible, scalable, and automated scientific analysis. Like other open-source software, these systems depend on active maintenance and community engagement to remain reliable and sustainable. However, despite the importance of timely issue resolution for software quality and community trust, little is known about what drives issue resolution speed within SWSs. This paper presents an empirical study of issue management and resolution across a collection of GitHub-hosted SWS projects. We analyze 21,116 issues to investigate how project characteristics, issue metadata, and contributor interactions affect time-to-close. Specifically, we address two research questions: (1) how issues are managed and addressed in SWSs, and (2) how issue and contributor features relate to issue resolution speed. We find that 68.91% of issues are closed, with half of them resolved within 18.09 days. Our results show that although SWS projects follow structured issue management practices, the issue resolution speed varies considerably across systems. Factors such as labeling and assigning issues are associated with faster issue resolution. Based on our findings, we make recommendations for developers to better manage SWS repository issues and improve their quality.

</details>


### [91] [An Empirical Study of Developer-Provided Context for AI Coding Assistants in Open-Source Projects](https://arxiv.org/abs/2512.18925)
*Shaokang Jiang,Daye Nam*

Main category: cs.SE

TL;DR: 本文通过对401个开源项目中开发者提供的机器可读指令进行大规模实证研究，归纳出五大类项目上下文内容，揭示了开发者在AI编码助手中编码项目约束的做法和差异。


<details>
  <summary>Details</summary>
Motivation: 现有AI编码助手依赖开发者提供的持久机器指令定义项目约束，但这些指令的具体内容尚未被系统研究。

Method: 通过对401个携带光标规则的开源代码库进行质性分析，总结开发者认为关键的项目上下文内容，构建了包含五个主题的分类体系。

Result: 归纳了五大类项目上下文内容：惯例、指南、项目信息、LLM指令和示例，并发现这些上下文因项目类型和语言不同而存在差异。

Conclusion: 本文建立了开发者提供的项目上下文的分类体系，指出上下文内容因项目类型和编程语言而异，对未来智能编码工具设计具有指导意义。

Abstract: While Large Language Models (LLMs) have demonstrated remarkable capabilities, research shows that their effectiveness depends not only on explicit prompts but also on the broader context provided. This requirement is especially pronounced in software engineering, where the goals, architecture, and collaborative conventions of an existing project play critical roles in response quality. To support this, many AI coding assistants have introduced ways for developers to author persistent, machine-readable directives that encode a project's unique constraints. Although this practice is growing, the content of these directives remains unstudied.
  This paper presents a large-scale empirical study to characterize this emerging form of developer-provided context. Through a qualitative analysis of 401 open-source repositories containing cursor rules, we developed a comprehensive taxonomy of project context that developers consider essential, organized into five high-level themes: Conventions, Guidelines, Project Information, LLM Directives, and Examples. Our study also explores how this context varies across different project types and programming languages, offering implications for the next generation of context-aware AI developer tools.

</details>


### [92] [Scrum Sprint Planning: LLM-based and algorithmic solutions](https://arxiv.org/abs/2512.18966)
*Yuwon Yoon,Kevin Iwan,Madeleine Zwart,Xiaohan Qin,Hina Lee,Maria Spichkova*

Main category: cs.SE

TL;DR: 研究表明，当前OpenAI模型在Scrum冲刺规划中的实用性有限，生成结果尚未达到可用标准。


<details>
  <summary>Details</summary>
Motivation: 探索利用先进的语言模型技术提升Scrum项目迭代规划的效率和质量。

Method: 通过手工创建的数据集进行案例研究，测试OpenAI三种不同模型的性能表现。

Result: 本文探讨了利用大型语言模型（LLMs）在Scrum项目迭代（冲刺）规划中的应用可能性，通过案例研究测试了OpenAI的GPT-3.5 Turbo、GPT-4.0 Turbo和Val模型。实验结果显示，这些模型生成的规划结果质量不足，无法直接应用于实际Scrum项目中。

Conclusion: 尽管尝试使用大型语言模型辅助Scrum冲刺规划，但现阶段模型结果质量仍不满足直接应用需求。

Abstract: Planning for an upcoming project iteration (sprint) is one of the key activities in Scrum planning. In this paper, we present our work in progress on exploring the applicability of Large Language Models (LLMs) for solving this problem. We conducted case studies with manually created data sets to investigate the applicability of OpenAI models for supporting the sprint planning activities. In our experiments, we applied three models provided OpenAI: GPT-3.5 Turbo, GPT-4.0 Turbo, and Val. The experiments demonstrated that the results produced by the models aren't of acceptable quality for direct use in Scrum projects.

</details>


### [93] [PEAK: A Performance Engineering AI-Assistant for GPU Kernels Powered by Natural Language Transformations](https://arxiv.org/abs/2512.19018)
*Muhammad Usman Tariq,Abhinav Jangda,Angelica Moreira,Madan Musuvathi,Tyler Sorensen*

Main category: cs.SE

TL;DR: PEAK利用自然语言指令使大语言模型优化GPU内核代码，实现了高效且灵活的性能提升，适用于多种GPU后端，并支持自动化和人工交互，推动GPU代码优化的智能化。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在处理低级后台代码，特别是性能关键且硬件特性快速变化且缺乏示例的GPU内核代码时存在困难，需一种灵活且高效的方法来优化GPU内核性能。

Method: PEAK通过将代码的迭代优化转化为自然语言指令，由大语言模型执行这些自然语言的代码转化，结合模块化验证和性能评估架构，支持针对不同GPU设备和内核的定制化优化。系统包括对CUDA、HIP和HLSL三个后端的实现，并创建了16种矩阵乘法内核的自然语言转化优化方法。

Result: PEAK实现了与现有厂商库相当的优化结果，且在无厂商库支持的HLSL后端也达到了硬件文档标明的峰值性能。该系统还支持细粒度分析优化序列及模型错误，既可由人工性能工程师使用，也可完全自动驱动，具备良好的兼容性和未来发展潜力。

Conclusion: PEAK作为一个针对GPU内核的性能工程AI助手，利用自然语言转化技术成功实现了基于大语言模型的代码优化，达到了与厂商库相当的性能水平，甚至在无库支持的HLSL上达到了硬件峰值性能。该系统展示了良好的灵活性和扩展性，能够促进性能工程领域的深入研究与自动化优化。

Abstract: Advancements in large language models (LLMs) are showing promising impact in software development and programming assistance. However, these models struggle when operating on low-level backend code. This challenge is exacerbated in the domain of GPU kernels, where performance-critical details are coupled to rapidly evolving hardware characteristics and available code examples are sparse.
  In this work, we introduce PEAK, a Performance Engineering AI-Assistant for GPU Kernels powered by natural language transformations. PEAK utilizes the key insight that iterative code transformations (optimizations) can straightforwardly be written in natural language, and then carried out by LLMs. Thus, these transformations can be rapidly developed, encoding general portable optimizations, but also easily specialized to specific GPU devices and even kernels. These natural transformations are supported by a modular and extensible infrastructure that additionally performs validation and performance evaluation. We demonstrate the flexibility of PEAK by instantiating it for three backends, CUDA, HIP, and HLSL, and create 16 natural transformations for optimizing matrix multiplication kernels. We show that our resulting implementations are competitive with vendor libraries when available, and for HLSL (without a library) our implementations match the hardware documented FLOPS. PEAK allows the fine-grained exploration of several research questions around how LLMs behave in this domain, including characterizing transformations and their errors; and how performance evolves along optimization sequences. PEAK provides an interface that can either be utilized by performance engineers to improve productivity, or driven completely autonomously (e.g., by an AI agent), providing a forward-compatible design that can continue to improve with advances in AI capabilities.

</details>


### [94] [BanglaForge: LLM Collaboration with Self-Refinement for Bangla Code Generation](https://arxiv.org/abs/2512.19122)
*Mahir Labib Dihan,Sadif Ahmed,Md Nafiu Rahman*

Main category: cs.SE

TL;DR: BanglaForge框架通过检索增强和双模型协作结合自我优化，实现了低资源环境下孟加拉语函数描述到代码生成的高效转换，在BLP-2025基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语作为低资源语言，缺乏大规模标注数据和相应工具，导致自然语言到代码的自动生成任务十分具有挑战性，亟需创新框架解决该问题。

Method: BanglaForge采用检索增强的双模型协作范式，结合上下文学习、大型语言模型翻译、系统化提示工程和基于执行反馈的迭代自我优化；其中一个模型负责初始代码生成，另一个模型进行代码复审以提升鲁棒性。

Result: 在BLP-2025孟加拉语代码生成基准上，BanglaForge实现了84.00%的Pass@1准确率，表现优异。

Conclusion: BanglaForge在低资源的孟加拉语代码生成任务中取得了84.00%的Pass@1准确率，验证了检索机制、双模型协作及自我优化策略的有效性。

Abstract: Bangla is a low-resource language for code generation, lacking large-scale annotated datasets and tools to transform natural language specifications into executable programs. This makes Bangla-to-code generation a challenging task requiring innovative solutions. To address this, we introduce BanglaForge, a novel framework for generating code from Bangla function descriptions. BanglaForge leverages a retrieval-augmented dual-model collaboration paradigm with self-refinement, combining in-context learning, llm-based translation, systematic prompt engineering, and iterative self-refinement based on execution feedback, where a coder generates initial solutions and a reviewer enhances them for robustness. On the BLP-2025 Bangla Code Generation benchmark, BanglaForge achieves a competitive Pass@1 accuracy of 84.00%, demonstrating the effectiveness of retrieval, model collaboration, and self-refinement for low-resource Bangla code generation.

</details>


### [95] [University Rents Enabling Corporate Innovation: Mapping Academic Researcher Coding and Discursive Labour in the R Language Ecosystem](https://arxiv.org/abs/2512.19153)
*Xiaolan Cai,Mathieu O'Neil,Stefano Zacchiroli*

Main category: cs.SE

TL;DR: 本文通过分析R软件生态系统中的贡献数据，揭示了研究人员在免费开源软件中的未被认可的劳动角色及其对行业的影响。


<details>
  <summary>Details</summary>
Motivation: 目前关于在线平台的研究多关注平台如何限制参与者行为及从其劳动中获利，本研究旨在将数字平台内的劳动与参与者的专业就业联系起来，揭示被忽视的劳动角色。

Method: 通过对GitHub上8,924个R软件包库的提交和交流进行量化和定性分析，探讨研究人员在数字平台内的编码和话语贡献。

Result: 研究发现研究人员与非关联贡献者是R包库的主要所有者和活跃贡献者，研究人员更可能担任官方角色并参与协作解决问题及支持工作。这显示存在一类未被认可的研究人员，他们无偿维护重要统计基础设施并支持行业员工。

Conclusion: 未被认可的研究人员劳动对于维护关键统计基础设施和支持行业实践至关重要，而免费开源软件的意识形态被大科技公司用来合法化对高校资源的利用。

Abstract: This article explores the role of unrecognised labour in corporate innovation systems via an analysis of researcher coding and discursive contributions to R, one of the largest statistical software ecosystems. Studies of online platforms typically focus on how platform affordances constrain participants' actions, and profit from their labour. We innovate by connecting the labour performed inside digital platforms to the professional employment of participants. Our case study analyses 8,924 R package repositories on GitHub, examining commits and communications. Our quantitative findings show that researchers, alongside non-affiliated contributors, are the most frequent owners of R package repositories and their most active contributors. Researchers are more likely to hold official roles compared to the average, and to engage in collaborative problem-solving and support work during package development. This means there is, underneath the 'recognised' category of star researchers who transition between academia and industry and secure generous funding, an 'unrecognised' category of researchers who not only create and maintain key statistical infrastructure, but also provide support to industry employees, for no remuneration. Our qualitative findings show how this unrecognised labour affects practitioners. Finally, our analysis of the ideology and practice of free, libre and open source software (FLOSS) shows how this ideology and practice legitimate the use of 'university rents' by Big Tech.

</details>


### [96] [Semantically-Equivalent Transformations-Based Backdoor Attacks against Neural Code Models: Characterization and Mitigation](https://arxiv.org/abs/2512.19215)
*Junyao Ye,Zhen Li,Xi Tang,Shouhuai Xu,Deqing Zou,Zhongsheng Yuan*

Main category: cs.SE

TL;DR: 本文提出了一种新型的基于语义等价变换的代码后门攻击，隐蔽性强且难以被现有防御检测，表明需开发针对该类攻击的新的防御措施。


<details>
  <summary>Details</summary>
Motivation: 当前神经代码模型越来越多地被应用于软件开发，但容易受到后门攻击，尤其是注入式攻击已被广泛研究和防御。现有防御手段容易忽视基于语义等价变换的隐蔽后门攻击，存在安全盲区。

Method: 提出基于语义等价变换（SET）的后门攻击，通过语义保持且低频的代码转换生成隐蔽触发器，设计了一套生成触发器的框架，并在多任务、多语言、多模型上进行测试。

Result: SET攻击成功率高（>90%），且保持模型实用性。攻击隐蔽性强，较注入攻击的检测率平均低25.13%。现有的归一化防御方法只能部分缓解攻击，显示其强健性。

Conclusion: SET基于语义等价变换的后门攻击是一种新型且隐蔽的威胁，传统检测和防御手段不足以防范，需进一步研究针对该类攻击的可扩展防御机制。

Abstract: Neural code models have been increasingly incorporated into software development processes. However, their susceptibility to backdoor attacks presents a significant security risk. The state-of-the-art understanding focuses on injection-based attacks, which insert anomalous patterns into software code. These attacks can be neutralized by standard sanitization techniques. This status quo may lead to a false sense of security regarding backdoor attacks. In this paper, we introduce a new kind of backdoor attacks, dubbed Semantically-Equivalent Transformation (SET)-based backdoor attacks, which use semantics-preserving low-prevalence code transformations to generate stealthy triggers. We propose a framework to guide the generation of such triggers. Our experiments across five tasks, six languages, and models like CodeBERT, CodeT5, and StarCoder show that SET-based attacks achieve high success rates (often >90%) while preserving model utility. The attack proves highly stealthy, evading state-of-the-art defenses with detection rates on average over 25.13% lower than injection-based counterparts. We evaluate normalization-based countermeasures and find they offer only partial mitigation, confirming the attack's robustness. These results motivate further investigation into scalable defenses tailored to SET-based attacks.

</details>


### [97] [A Dataset and Preliminary Study of Using GPT-5 for Code-change Impact Analysis](https://arxiv.org/abs/2512.19481)
*Katharina Stengg,Christian Macho,Martin Pinzger*

Main category: cs.SE

TL;DR: 研究利用GPT-5系列大语言模型预测源代码变更影响，构建新数据集，发现当前模型性能较差但diff信息有助提升。


<details>
  <summary>Details</summary>
Motivation: 理解源代码变更及其对其他代码实体影响是软件开发中的关键技能，但传统的手动分析耗时且效率低，利用大语言模型辅助分析代码变更的潜力尚未充分探索。

Method: 使用GPT-5和GPT-5-mini大语言模型来预测源代码变更对其他代码实体的影响，构建包含种子变更、变更对和变更类型的数据集，设计两种模型配置进行实验评估。

Result: 实验结果显示，GPT-5和GPT-5-mini在预测代码影响方面表现均较差，GPT-5表现优于GPT-5-mini，加入diff hunk信息能略微提升模型性能。

Conclusion: 尽管GPT-5系列模型在理解代码变更影响方面仍存在显著不足，但结合diff hunks信息能带来有限改进，未来需进一步提升模型能力以支持代码变更分析。

Abstract: Understanding source code changes and their impact on other code entities is a crucial skill in software development. However, the analysis of code changes and their impact is often performed manually and therefore is time-consuming. Recent advancements in AI, and in particular large language models (LLMs) show promises to help developers in various code analysis tasks. However, the extent to which this potential can be utilized for understanding code changes and their impact is underexplored. To address this gap, we study the capabilities of GPT-5 and GPT-5-mini to predict the code entities impacted by given source code changes. We construct a dataset containing information about seed-changes, change pairs, and change types for each commit. Existing datasets lack crucial information about seed changes and impacted code entities. Our experiments evaluate the LLMs in two configurations: (1) seed-change information and the parent commit tree and (2) seed-change information, the parent commit tree, and the diff hunk of each seed change. We found that both LLMs perform poorly in the two experiments, whereas GPT-5 outperforms GPT-5-mini. Furthermore, the provision of the diff hunks helps both models to slightly improve their performance.

</details>


### [98] [Beyond Language Boundaries: Uncovering Programming Language Families for Code Language Models](https://arxiv.org/abs/2512.19509)
*Shangbo Yun,Xiaodong Gu,Jianghong Huang,Beijun Shen*

Main category: cs.SE

TL;DR: 本文探讨了编程语言之间的深层语言学关系，并利用这些关系提升多语言代码大语言模型的训练效果。通过定义21个语言特征，使用大语言模型生成跨语言语义对齐代码样本，构建相似度矩阵并应用层次聚类，揭示编程语言的层级结构。基于此，提出三种训练策略，显著提升多语言代码模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有多语言代码模型多通过简单聚合数据训练，未深入挖掘编程语言间的深层关系，作者希望通过揭示语言间的亲缘关系来优化模型训练和推理。

Method: 首先定义21个编程语言的主要语言学特征，利用大语言模型生成语义对齐的跨语言代码样本，通过嵌入这些代码样本构建语言相似度矩阵，应用层次聚类揭示语言关系。基于发现的语言家族，提出迁移学习、语言接近度引导的课程学习和基于中心点的中介代码翻译三种策略优化训练。

Result: 在4个代码智能任务上的实验表明，基于语言家族的训练策略显著提升了多语言代码大语言模型的表现。

Conclusion: 研究揭示了编程语言之间的层级和亲缘关系，并基于此设计的训练策略可以有效提升多语言代码大语言模型的性能。

Abstract: The rapid proliferation of diverse programming languages presents both opportunities and challenges for developing multilingual code LLMs. While existing techniques often train code LLMs by simply aggregating multilingual code data, few explore the deeper relationships between programming languages(PLs) and how such relationships can be utilized to optimize the training and inference of code LLMs. In this work, we investigate 2 fundamental questions: 1) What are the deep linguistic relationships among PLs? and 2) How can these relationships be leveraged to improve multilingual code LLMs? We propose an embedding-based framework to uncover the latent families of PLs. Our approach begins by defining 21 primary linguistic features of programming languages, such as variable definition, control structures, and method declarations, and then employs LLMs to generate feature-aligned code samples across multiple languages. By embedding these semantically parallel code snippets from 19 languages, we construct a similarity matrix and perform hierarchical clustering to uncover inherent language relationships. Our analysis reveals clear hierarchical structures among programming languages. Closely related languages form well-defined clusters (e.g., C, C++, Java, and Swift group together), while Go exhibits as a central language with the highest cross-language similarity. Building on the uncovered language families, we propose three strategies to enhance multilingual LLM training: transfer learning across linguistically related languages, linguistic proximity-guided curriculum learning, and centroid-based intermediary code translation. Experiments on 4 code intelligence tasks demonstrate that our methods significantly improve multilingual LLM performance. This work offers a universal perspective on programming languages and advances more effective strategies for multilingual code LLM training.

</details>


### [99] [More code, less validation: Risk factors for over-reliance on AI coding tools among scientists](https://arxiv.org/abs/2512.19644)
*Gabrielle O'Brien,Alexis Parker,Nasir Eisty,Jeffrey Carver*

Main category: cs.SE

TL;DR: 科学家程序员中，学生和经验较少者更倾向于使用生成式AI工具，如ChatGPT。更高的感知生产力与代码生成量相关，但缺乏正式开发实践可能影响代码质量。


<details>
  <summary>Details</summary>
Motivation: 现有科学家编程培训不足，生成式AI工具能辅助代码编写，但使用中存在过度依赖风险，特别是新手用户。

Method: 通过对868名科学编程人员进行调查，分析其AI工具的采用模式、偏好及生产力相关因素。

Result: 学生和经验不足的程序员更倾向采用生成式AI，偏好通用对话接口如ChatGPT。正式开发实践（测试、代码审查）部分缓解了经验不足的影响。感知生产力最强预测因素是一次接受的生成代码行数。

Conclusion: 生成式AI在科学编程中被广泛采用，尤其是学生和缺乏经验者，但过度依赖代码生成而非代码验证，可能威胁研究代码的完整性。

Abstract: Programming is essential to modern scientific research, yet most scientists report inadequate training for the software development their work demands. Generative AI tools capable of code generation may support scientific programmers, but user studies indicate risks of over-reliance, particularly among inexperienced users. We surveyed 868 scientists who program, examining adoption patterns, tool preferences, and factors associated with perceived productivity. Adoption is highest among students and less experienced programmers, with variation across fields. Scientific programmers overwhelmingly prefer general-purpose conversational interfaces like ChatGPT over developer-specific tools. Both inexperience and limited use of development practices (like testing, code review, and version control) are associated with greater perceived productivity-but these factors interact, suggesting formal practices may partially compensate for inexperience. The strongest predictor of perceived productivity is the number of lines of generated code typically accepted at once. These findings suggest scientific programmers using generative AI may gauge productivity by code generation rather than validation, raising concerns about research code integrity.

</details>
