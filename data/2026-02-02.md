<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 61]
- [cs.SE](#cs.SE) [Total: 26]
- [cs.MA](#cs.MA) [Total: 7]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [In Vino Veritas and Vulnerabilities: Examining LLM Safety via Drunk Language Inducement](https://arxiv.org/abs/2601.22169)
*Anudeex Shetty,Aditya Joshi,Salil S. Kanhere*

Main category: cs.CL

TL;DR: 本文研究了酒精影响下生成的“醉酒语言”如何导致大型语言模型安全失效，提出了三种诱导方法，并通过多模型多基准评测揭示了模型安全风险显著增加。


<details>
  <summary>Details</summary>
Motivation: 人类在酒精影响下容易出现不良行为和隐私泄露，研究醉酒语言对LLMs安全性的影响，探索醉酒语言导致的安全风险和潜在防范方法具有重要意义。

Method: 通过三种机制诱导LLMs生成醉酒语言：基于人物设定的提示、因果微调和基于强化学习的后训练。然后在5个LLMs上使用JailbreakBench和ConfAIde两套英语基准测试进行评估，并结合人工和模型评估对错误类别进行分析。

Result: 诱导LLMs生成醉酒语言后，其绕过防御机制的能力提升，隐私泄露风险增加，表明醉酒语言诱导是LLM安全调优的潜在挑战。

Conclusion: 该论文发现，酒精影响下生成的“醉酒语言”会导致大型语言模型（LLMs）更容易出现安全失效，如绕过防御机制和隐私泄露。醉酒语言与人类喝醉时的行为有对应关系，这种诱导方法极大增加了模型的安全风险。

Abstract: Humans are susceptible to undesirable behaviours and privacy leaks under the influence of alcohol. This paper investigates drunk language, i.e., text written under the influence of alcohol, as a driver for safety failures in large language models (LLMs). We investigate three mechanisms for inducing drunk language in LLMs: persona-based prompting, causal fine-tuning, and reinforcement-based post-training. When evaluated on 5 LLMs, we observe a higher susceptibility to jailbreaking on JailbreakBench (even in the presence of defences) and privacy leaks on ConfAIde, where both benchmarks are in English, as compared to the base LLMs as well as previously reported approaches. Via a robust combination of manual evaluation and LLM-based evaluators and analysis of error categories, our findings highlight a correspondence between human-intoxicated behaviour, and anthropomorphism in LLMs induced with drunk language. The simplicity and efficiency of our drunk language inducement approaches position them as potential counters for LLM safety tuning, highlighting significant risks to LLM safety.

</details>


### [2] [MrRoPE: Mixed-radix Rotary Position Embedding](https://arxiv.org/abs/2601.22181)
*Qingyuan Tian,Wenhong Zhu,Xiaoran Liu,Xiaofeng Wang,Rui Wang*

Main category: cs.CL

TL;DR: 本文提出了基于基数系统转换的混合基数Rotary位置编码方法，统一了多种扩展策略，并设计了无需训练的推广方案，显著提升了长序列处理能力和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的Rotary Position Embedding扩展策略种类繁多但缺乏统一理论基础，无法有效应对训练序列长度限制带来的长序列推广问题。

Method: 基于基数系统转换理论，提出混合基数Rotary Position Embedding (MrRoPE)编码方法，设计了两种无训练扩展策略MrRoPE-Uni和MrRoPE-Pro，分别采用均匀和渐进的基数转换方案，以支持模型在训练短序列时能推广到测试长序列。

Result: MrRoPE-Pro在无需微调的情况下，在128K上下文的长序列难题测试中保持超过85%召回率，在Infinite-Bench检索与对话子集上的准确率是YaRN的两倍以上，理论和实验证实了其扩展了编码长度的上界。

Conclusion: 本文提出的MrRoPE通过基数系统转换的视角统一了现有的RoPE扩展方法，实现了对长序列编码的有效推广。MrRoPE-Pro不仅在长序列场景中表现优异，还显著提升了编码长度的上限，验证了理论和方法的可靠性。

Abstract: Rotary Position Embedding (RoPE)-extension refers to modifying or generalizing the Rotary Position Embedding scheme to handle longer sequences than those encountered during pre-training. However, current extension strategies are highly diverse and lack a unified theoretical foundation. In this paper, we propose MrRoPE (Mixed-radix RoPE), a generalized encoding formulation based on a radix system conversion perspective, which elegantly unifies various RoPE-extension approaches as distinct radix conversion strategies. Based on this theory, we introduce two training-free extensions, MrRoPE-Uni and MrRoPE-Pro, which leverage uniform and progressive radix conversion strategies, respectively, to achieve 'train short, test long' generalization. Without fine-tuning, MrRoPE-Pro sustains over 85% recall in the 128K-context Needle-in-a-Haystack test and achieves more than double YaRN's accuracy on Infinite-Bench retrieval and dialogue subsets. Theoretical analysis confirms that MrRoPE-Pro effectively raises the upper bound of RoPE's attainable encoding length, which further validates the reliability and utility of our theory and methodology.

</details>


### [3] [Prepare Reasoning Language Models for Multi-Agent Debate with Self-Debate Reinforcement Learning](https://arxiv.org/abs/2601.22297)
*Chenxi Liu,Yanshuo Chen,Ruibo Chen,Tianyi Xiong,Tong Zheng,Heng Huang*

Main category: cs.CL

TL;DR: 本文提出了一种新的训练框架SDRL，通过自我辩论机制提升大型语言模型的推理能力，既增强了单模型解决问题的能力，也优化了多模型协作辩论的效果。


<details>
  <summary>Details</summary>
Motivation: 当前RLVR方法训练的大型语言模型多为独立解决问题，缺少利用多样推理轨迹中信息的能力，限制了多模型辩论性能的提升。

Method: 提出了Self-Debate Reinforcement Learning (SDRL)，通过先采样多个候选解，再基于多样的推理路径构建辩论上下文，生成第二轮响应，并联合优化初始和辩论条件下的响应。

Result: 实验表明，SDRL在多个基础模型和推理基准上不仅提升了多模型辩论的表现，也增强了单模型的推理能力。

Conclusion: SDRL训练框架同时提升了单模型的独立推理能力和多模型辩论中的协作推理表现，显著优化了模型在推理任务中的综合性能。

Abstract: The reasoning abilities of large language models (LLMs) have been substantially improved by reinforcement learning with verifiable rewards (RLVR). At test time, collaborative reasoning through Multi-Agent Debate (MAD) has emerged as a promising approach for enhancing LLM performance. However, current RLVR methods typically train LLMs to solve problems in isolation, without explicitly preparing them to synthesize and benefit from different rationales that arise during debate. In this work, we propose Self-Debate Reinforcement Learning (SDRL), a training framework that equips a single LLM with strong standalone problem-solving ability and the capability to learn from diverse reasoning trajectories in MAD. Given a prompt, SDRL first samples multiple candidate solutions, then constructs a debate context with diverse reasoning paths and generates second-turn responses conditioned on this context. Finally, SDRL jointly optimizes both the initial and debate-conditioned responses, yielding a model that is effective as both a standalone solver and a debate participant. Experiments across multiple base models and reasoning benchmarks show that SDRL improves overall MAD performance while simultaneously strengthening single model reasoning.

</details>


### [4] [MERMAID: Memory-Enhanced Retrieval and Reasoning with Multi-Agent Iterative Knowledge Grounding for Veracity Assessment](https://arxiv.org/abs/2601.22361)
*Yupeng Cao,Chengyang He,Yangyang Yu,Ping Wang,K. P. Subbalakshmi*

Main category: cs.CL

TL;DR: 本文提出了MERMAID，一种结合记忆模块的多智能体真实性评估框架，通过动态证据管理提升事实核查的准确性和效率，实验证明其优越表现。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型驱动的真实性评估方法中，证据检索通常是静态且孤立的步骤，未有效管理或复用已检索证据，导致搜索效率低下及验证一致性不足。

Method: 该方法设计了一个多智能体记忆增强的真实性评估框架，结合智能体驱动的搜索、结构化知识表示和持久记忆模块，采用推理-行动迭代流程，实现对检索证据的动态管理和复用。

Result: MERMAID在三个事实核查基准和两个声明验证数据集上使用多种大型语言模型测试，结果显示其在提升搜索效率的同时，达到了最新的性能水平。

Conclusion: MERMAID框架通过紧密结合检索和推理过程，引入持久证据记忆模块，实现证据的动态获取和跨声明复用，提升了事实核查的准确性和效率。

Abstract: Assessing the veracity of online content has become increasingly critical. Large language models (LLMs) have recently enabled substantial progress in automated veracity assessment, including automated fact-checking and claim verification systems. Typical veracity assessment pipelines break down complex claims into sub-claims, retrieve external evidence, and then apply LLM reasoning to assess veracity. However, existing methods often treat evidence retrieval as a static, isolated step and do not effectively manage or reuse retrieved evidence across claims. In this work, we propose MERMAID, a memory-enhanced multi-agent veracity assessment framework that tightly couples the retrieval and reasoning processes. MERMAID integrates agent-driven search, structured knowledge representations, and a persistent memory module within a Reason-Action style iterative process, enabling dynamic evidence acquisition and cross-claim evidence reuse. By retaining retrieved evidence in an evidence memory, the framework reduces redundant searches and improves verification efficiency and consistency. We evaluate MERMAID on three fact-checking benchmarks and two claim-verification datasets using multiple LLMs, including GPT, LLaMA, and Qwen families. Experimental results show that MERMAID achieves state-of-the-art performance while improving the search efficiency, demonstrating the effectiveness of synergizing retrieval, reasoning, and memory for reliable veracity assessment.

</details>


### [5] [Context Structure Reshapes the Representational Geometry of Language Models](https://arxiv.org/abs/2601.22364)
*Eghbal A. Hosseini,Yuxuan Li,Yasaman Bahri,Declan Campbell,Andrew Kyle Lampinen*

Main category: cs.CL

TL;DR: 本文研究了大语言模型在上下文学习中的表征变化，发现模型根据任务结构动态切换策略，只有部分任务表现出神经轨迹直线化，这说明上下文学习是多样化而非单一的过程。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型在上下文学习过程中，内部表征是否也会像整体序列预测中那样表现出轨迹的直线化，以理解模型如何适应不同任务。

Method: 通过在Gemma 2模型上测量神经序列轨迹的直线化程度，分析其在多种上下文任务中的表征变化，比较持续预测任务与结构化预测任务中表现的差异。

Result: 发现持续预测任务中表征轨迹直线化随着上下文增加而提升，相关性强；而结构化预测任务中该现象仅在特定阶段存在，不连续，说明模型采用多种策略应对不同任务结构。

Conclusion: 大语言模型在不同的上下文学习任务中表现出不同的表征变化模式。在持续预测任务中，随着上下文长度的增加，神经序列轨迹变得更为直线化，这与模型预测性能的提升有关。而在结构化预测任务中，直线化现象不一致，仅在具有明确结构的任务阶段出现。这表明上下文学习过程不是单一的，而是模型根据任务结构动态选择策略。

Abstract: Large Language Models (LLMs) have been shown to organize the representations of input sequences into straighter neural trajectories in their deep layers, which has been hypothesized to facilitate next-token prediction via linear extrapolation. Language models can also adapt to diverse tasks and learn new structure in context, and recent work has shown that this in-context learning (ICL) can be reflected in representational changes. Here we bring these two lines of research together to explore whether representation straightening occurs \emph{within} a context during ICL. We measure representational straightening in Gemma 2 models across a diverse set of in-context tasks, and uncover a dichotomy in how LLMs' representations change in context. In continual prediction settings (e.g., natural language, grid world traversal tasks) we observe that increasing context increases the straightness of neural sequence trajectories, which is correlated with improvement in model prediction. Conversely, in structured prediction settings (e.g., few-shot tasks), straightening is inconsistent -- it is only present in phases of the task with explicit structure (e.g., repeating a template), but vanishes elsewhere. These results suggest that ICL is not a monolithic process. Instead, we propose that LLMs function like a Swiss Army knife: depending on task structure, the LLM dynamically selects between strategies, only some of which yield representational straightening.

</details>


### [6] [Stability-Aware Prompt Optimization for Clinical Data Abstraction](https://arxiv.org/abs/2601.22373)
*Arinbjörn Kolbeinsson,Daniel Timbie,Sajjan Narsinghani,Sanjay Hariharan*

Main category: cs.CL

TL;DR: 本文研究了临床大型语言模型对提示词措辞的敏感性，发现模型稳定性与准确率和校准性不完全相关。提出双目标提示词优化策略，显著降低翻转率，提高模型稳定性，强调提示词稳定性应作为临床模型验证的重要指标。


<details>
  <summary>Details</summary>
Motivation: 当前临床大型语言模型在处理任务时对提示词的措辞极其敏感，但大多数研究仅将提示词视为固定输入，单独研究模型不确定性，忽视了两者应联合考虑的事实。提高模型稳定性对于确保临床应用的可靠性至关重要。

Method: 通过在两个临床任务（MedAlign适用性/正确性和多发性硬化亚型抽象）中，采用多种开放和专有模型，测量模型对提示词的敏感性（翻转率），并关联模型校准性和选择性预测表现。提出一种双目标提示词优化循环，联合优化准确率和提示词稳定性。

Result: 发现模型准确率高不意味着提示词稳定，模型即使校准良好也可能易受措辞变化影响。引入提示词稳定性作为优化目标后，翻转率显著下降，在多个任务和模型中表现出较好的稳定性，有时以稍微的准确率损失为代价。

Conclusion: 大型语言模型在临床抽象任务中对提示词措辞非常敏感，而提示词敏感性和模型不确定性应该被联合考虑。通过引入提示词稳定性作为优化目标，可以显著降低模型对提示词变化的敏感度，即翻转率，同时在部分情况下保持较高准确率，证明提示词稳定性是临床大模型系统验证的重要指标。

Abstract: Large language models used for clinical abstraction are sensitive to prompt wording, yet most work treats prompts as fixed and studies uncertainty in isolation. We argue these should be treated jointly. Across two clinical tasks (MedAlign applicability/correctness and MS subtype abstraction) and multiple open and proprietary models, we measure prompt sensitivity via flip rates and relate it to calibration and selective prediction. We find that higher accuracy does not guarantee prompt stability, and that models can appear well-calibrated yet remain fragile to paraphrases. We propose a dual-objective prompt optimization loop that jointly targets accuracy and stability, showing that explicitly including a stability term reduces flip rates across tasks and models, sometimes at modest accuracy cost. Our results suggest prompt sensitivity should be an explicit objective when validating clinical LLM systems.

</details>


### [7] [SPLA: Block Sparse Plus Linear Attention for Long Context Modeling](https://arxiv.org/abs/2601.22379)
*Bailin Wang,Dan Friedman,Tao Lei,Chong Wang*

Main category: cs.CL

TL;DR: SPLA通过精准选择相关块和智能压缩未选块，提升了块稀疏注意力在长上下文任务中的表现，优于传统方法且保持推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有块稀疏注意力方法在长上下文建模中效率虽高，但存在选择准确度低和完全丢弃未选块导致上下文信息累积损失的问题。

Method: 该方法采用基于二阶泰勒展开的选择指标来准确选择相关块，未选块通过残差线性注意力模块压缩为紧凑的递归状态，且引入基于减法的优化计算方式避免推理时显式访问未选块。

Result: 实验表明，SPLA在持续预训练任务中缩小了与密集注意力模型的性能差距，并在RULER等长上下文基准测试中表现优异，同时保持了良好的通用知识和推理能力。

Conclusion: SPLA方法通过精准识别相关块并压缩未选块，实现了块稀疏注意力的性能提升，克服了选择准确度低和上下文信息丢失的问题，在长上下文任务中优于传统密集注意力模型。

Abstract: Block-wise sparse attention offers significant efficiency gains for long-context modeling, yet existing methods often suffer from low selection fidelity and cumulative contextual loss by completely discarding unselected blocks. To address these limitations, we introduce Sparse Plus Linear Attention (SPLA), a framework that utilizes a selection metric derived from second-order Taylor expansions to accurately identify relevant blocks for exact attention. Instead of discarding the remaining "long tail," SPLA compresses unselected blocks into a compact recurrent state via a residual linear attention (RLA) module. Crucially, to avoid IO overhead, we derive an optimized subtraction-based formulation for RLA -- calculating the residual as the difference between global and selected linear attention -- ensuring that unselected blocks are never explicitly accessed during inference. Our experiments demonstrate that SPLA closes the performance gap in continual pretraining, surpassing dense attention models on long-context benchmarks like RULER while maintaining competitive general knowledge and reasoning capabilities.

</details>


### [8] [Specialists or Generalists? Multi-Agent and Single-Agent LLMs for Essay Grading](https://arxiv.org/abs/2601.22386)
*Jamiu Adekunle Idowu,Ahmed Almasoud*

Main category: cs.CL

TL;DR: 本研究比较了单智能体与多智能体架构在作文自动评分上的表现，发现多智能体适合筛查弱作文，单智能体适合常规评估，少样本校准是提升准确性的关键因素。


<details>
  <summary>Details</summary>
Motivation: 当前自动作文评分系统普遍依赖大型语言模型，但不同架构在不同作文质量级别上的表现尚不明确，亟需评估架构设计对评分效果的影响。

Method: 采用ASAP 2.0语料库，使用GPT-5.1在零样本和少样本条件下，比较单智能体与多智能体作文评分架构，多智能体由内容、结构、语言三个专员及主席代理协调评分逻辑。

Result: 多智能体系统更擅长识别低质作文，单智能体系统在中等作文上表现更好，高质量作文评分均较弱，少样本校准显著提升两种架构性能。

Conclusion: 多智能体系统在识别弱作文方面表现更佳，单智能体系统在处理中等水平作文时表现优越，但两者都难以处理高质量作文。少样本校准对系统性能提升关键，微调示例大幅提升评分一致性。

Abstract: Automated essay scoring (AES) systems increasingly rely on large language models, yet little is known about how architectural choices shape their performance across different essay quality levels. This paper evaluates single-agent and multi-agent LLM architectures for essay grading using the ASAP 2.0 corpus. Our multi-agent system decomposes grading into three specialist agents (Content, Structure, Language) coordinated by a Chairman Agent that implements rubric-aligned logic including veto rules and score capping. We test both architectures in zero-shot and few-shot conditions using GPT-5.1. Results show that the multi-agent system is significantly better at identifying weak essays while the single-agent system performs better on mid-range essays. Both architectures struggle with high-quality essays. Critically, few-shot calibration emerges as the dominant factor in system performance -- providing just two examples per score level improves QWK by approximately 26% for both architectures. These findings suggest architectural choice should align with specific deployment priorities, with multi-agent AI particularly suited for diagnostic screening of at-risk students, while single-agent models provide a cost-effective solution for general assessment.

</details>


### [9] [SP^2DPO: An LLM-assisted Semantic Per-Pair DPO Generalization](https://arxiv.org/abs/2601.22385)
*Chaoyue He,Xin Zhou,Di Wang,Hong Xu,Wei Liu,Chunyan Miao*

Main category: cs.CL

TL;DR: 针对DPO全局温度参数不足，SP2DPO引入每对偏好的语义感知温度调度，以提升异质偏好数据的训练效果，在多模型上展现了性能优势且无额外训练负担。


<details>
  <summary>Details</summary>
Motivation: 传统DPO使用单一全局温度beta隐式假设所有偏好对信息量相同，但真实偏好数据异质且包含不同信号强度和噪声，需要更细粒度的调整以提升性能。

Method: 提出SP2DPO方法，用结构化的语义差距注释（类别、幅度、置信度）由教师模型离线预先确定每对偏好的beta_i参数，替代传统的全局温度参数beta，训练时仍使用标准DPO优化器，但每对样本使用不同的beta_i。

Result: 在AlpacaEval 2.0评测上，SP2DPO在四个4B-8B规模的开源学生模型中表现出竞争力，在两个模型上提高了长度控制胜率，同时避免了复杂的beta参数调优过程。

Conclusion: SP2DPO通过为每个偏好对引入实例特定的温度调度beta_i，提高了DPO在处理异质偏好数据时的性能，尤其是在某些模型上提升了长度控制的胜率，同时避免了全局温度参数调优的复杂性。

Abstract: Direct Preference Optimization (DPO) controls the trade-off between fitting preference labels and staying close to a reference model using a single global temperature beta, implicitly treating all preference pairs as equally informative. Real-world preference corpora are heterogeneous: they mix high-signal, objective failures (for example, safety, factuality, instruction violations) with low-signal or subjective distinctions (for example, style), and also include label noise. We introduce our method, SP2DPO (Semantic Per-Pair DPO), a generalization that replaces the global temperature with an instance-specific schedule beta_i pre-decided offline from structured semantic-gap annotations (category, magnitude, confidence) produced by teacher language models. We instantiate this procedure on the UltraFeedback preference corpus (59,960 pairs), enabling large-scale construction of an auditable beta_i artifact, and incur zero training-time overhead: the inner-loop optimizer remains standard DPO with beta set per pair. We focus our empirical study on AlpacaEval 2.0, reporting both raw win rate and length-controlled win rate. Across four open-weight, instruction-tuned student backbones (4B-8B), SP2DPO is competitive with a tuned global-beta DPO baseline and improves AlpacaEval 2.0 length-controlled win rate on two of four backbones, while avoiding per-model beta sweeps. All code, annotations, and artifacts will be released.

</details>


### [10] [Culturally Grounded Personas in Large Language Models: Characterization and Alignment with Socio-Psychological Value Frameworks](https://arxiv.org/abs/2601.22396)
*Candida M. Greco,Lucio La Cava,Andrea Tagarelli*

Main category: cs.CL

TL;DR: 本文探讨了利用大型语言模型生成的文化背景合成角色，验证了它们在不同文化和道德价值体系中的表现，展示了其在跨文化模拟和分析中的潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在模拟人类行为中应用日益广泛，但其在不同文化背景下反映真实世界和道德价值系统的准确性尚不明确，亟需系统评估和验证。

Method: 通过基于世界价值观调查（WVS）变量生成LLM合成角色，并从Inglehart-Welzel文化地图定位、WVS人口统计分布一致性及道德基础问卷的文化-道德映射三个角度综合分析这些合成角色。

Result: 合成文化人物生成方法在多个文化框架下表现出与真实人群行为和价值观的较高一致性，展示了维持文化差异和道德变异的能力，提供了一种跨文化结构和道德变异的评估新途径。

Conclusion: 本文证明了基于大型语言模型（LLMs）生成的合成文化人物能够在一定程度上反映不同文化背景下的世界观和道德价值体系，验证了这些合成角色在世界价值观调查、Inglehart-Welzel文化地图和道德基础理论框架下的有效性和一致性。

Abstract: Despite the growing utility of Large Language Models (LLMs) for simulating human behavior, the extent to which these synthetic personas accurately reflect world and moral value systems across different cultural conditionings remains uncertain. This paper investigates the alignment of synthetic, culturally-grounded personas with established frameworks, specifically the World Values Survey (WVS), the Inglehart-Welzel Cultural Map, and Moral Foundations Theory. We conceptualize and produce LLM-generated personas based on a set of interpretable WVS-derived variables, and we examine the generated personas through three complementary lenses: positioning on the Inglehart-Welzel map, which unveils their interpretation reflecting stable differences across cultural conditionings; demographic-level consistency with the World Values Survey, where response distributions broadly track human group patterns; and moral profiles derived from a Moral Foundations questionnaire, which we analyze through a culture-to-morality mapping to characterize how moral responses vary across different cultural configurations. Our approach of culturally-grounded persona generation and analysis enables evaluation of cross-cultural structure and moral variation.

</details>


### [11] [Bifocal Attention: Harmonizing Geometric and Spectral Positional Embeddings for Algorithmic Generalization](https://arxiv.org/abs/2601.22402)
*Kanishk Awadhiya*

Main category: cs.CL

TL;DR: 本文发现RoPE的固定几何衰减限制了长距离递归推理能力，提出Bifocal Attention架构和Spectral Evolution训练，改善了模型对深度递归结构的理解与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有RoPE通过固定的几何衰减编码相对位置，适合局部句法结构，但无法有效捕捉递归逻辑和算法推理中的长距离周期结构，导致模型难以从浅层推理迁移到更深层次。

Method: 提出了Bifocal Attention架构，将位置编码分为两种模式——标准RoPE用于局部的token级操作，和可学习的谐波算子用于跟踪长距离递归深度。同时引入Spectral Evolution训练协议，通过梯度下降方式优化位置频率以适应任务的算法拓扑。

Result: 通过Bifocal Attention和Spectral Evolution训练，模型能够更好地捕捉长距离递归结构，实现对深度递归步骤的外推能力，弥补了现有RoPE的结构限制。

Conclusion: 本文指出了现有RoPE方法在处理深度递归逻辑和算法推理时存在的“结构鸿沟”，并提出了Bifocal Attention架构及Spectral Evolution训练方法以解决该问题。

Abstract: Rotary Positional Embeddings (RoPE) have become the standard for Large Language Models (LLMs) due to their ability to encode relative positions through geometric rotation. However, we identify a significant limitation we term ''Spectral Rigidity'': standard RoPE utilizes a fixed geometric decay ($θ^{-i}$) optimized for local syntactic coherence, which fails to capture the long-range, periodic structures inherent in recursive logic and algorithmic reasoning. This results in a ''Structure Gap'', where models trained on shallow reasoning chains fail to extrapolate to deeper recursive steps. In this work, we introduce Bifocal Attention, an architectural paradigm that decouples positional encoding into two distinct modalities: Geometric Eyes (Standard RoPE) for precise token-level manipulation, and Spectral Eyes (Learnable Harmonic Operators) for tracking long-range recursive depth. We propose a novel training protocol, Spectral Evolution, which initializes positional frequencies as static geometric parameters but allows them to evolve via gradient descent into a harmonic basis optimized for the specific algorithmic topology of the task.

</details>


### [12] [Word-Centered Semantic Graphs for Interpretable Diachronic Sense Tracking](https://arxiv.org/abs/2601.22410)
*Imene Kolli,Kai-Robin Lange,Jonas Rieger,Carsten Jentsch*

Main category: cs.CL

TL;DR: 本文提出一种结合Skip-gram和掩码语言模型的基于图的语义演变分析框架，通过语义网络和聚类追踪词义变化，应用于纽约时报语料，展示了词义多样化演变的不同路径。


<details>
  <summary>Details</summary>
Motivation: 传统词义演变研究依赖预定义的词义体系，缺乏对语义多义性和动态变化的灵活捕捉，本文旨在提供一个可解释且紧凑的语义演变表示方法。

Method: 该方法通过构建以目标词为中心的语义网络，将历时Skip-gram嵌入的分布相似度与时间特定的掩码语言模型的词汇可替代性结合，进而通过聚类外围图结构并跨时间节点重叠对齐聚类，追踪词义变化。

Result: 在纽约时报杂志1980至2017年语料上的应用表明，图结构精确反映了多义性动态，不同词表现出不同的语义演变轨迹，如事件驱动的词义替换、语义稳定和渐进联想变化。

Conclusion: 本文提出的基于图的语义演变分析框架能够有效捕捉词义变化的动态，实现了语义聚类的跨时间对齐，且不依赖预定义的词义库。

Abstract: We propose an interpretable, graph-based framework for analyzing semantic shift in diachronic corpora. For each target word and time slice, we induce a word-centered semantic network that integrates distributional similarity from diachronic Skip-gram embeddings with lexical substitutability from time-specific masked language models. We identify sense-related structure by clustering the peripheral graph, align clusters across time via node overlap, and track change through cluster composition and normalized cluster mass. In an application study on a corpus of New York Times Magazine articles (1980 - 2017), we show that graph connectivity reflects polysemy dynamics and that the induced communities capture contrasting trajectories: event-driven sense replacement (trump), semantic stability with cluster over-segmentation effects (god), and gradual association shifts tied to digital communication (post). Overall, word-centered semantic graphs offer a compact and transparent representation for exploring sense evolution without relying on predefined sense inventories.

</details>


### [13] [Large Language Model Agents Are Not Always Faithful Self-Evolvers](https://arxiv.org/abs/2601.22436)
*Weixiang Zhao,Yingshuo Wang,Yichen Zhang,Yang Deng,Yanyan Zhao,Wanxiang Che,Bing Qin,Ting Liu*

Main category: cs.CL

TL;DR: 研究发现自我进化的大型语言模型代理虽然依赖原始经验，但经常忽视或误读浓缩经验，揭示了当前经验整合方法存在的缺陷和改进空间。


<details>
  <summary>Details</summary>
Motivation: 尽管自我进化的LLM代理通过积累和重用过去经验不断改进，仍不清楚它们是否忠实地依赖这些经验来指导行为，因此进行系统的经验忠实性研究。

Method: 通过对原始和浓缩经验进行有控制的因果干预，系统地评估了四种代表性框架、10个LLM骨干和9个环境中的经验依赖性。

Result: 发现代理始终依赖原始经验，但常常忽视或误解浓缩经验，且这种差异存在于单代理和多代理配置以及不同规模的骨干之间，原因包括浓缩内容的语义限制、内部处理偏差和某些任务中预训练先验的充分性。

Conclusion: 自我进化的大型语言模型代理在利用过去经验指导行为时表现出经验依赖性的不对称性，特别是在处理浓缩经验时经常忽视或误解经验，揭示了当前方法的局限性。

Abstract: Self-evolving large language model (LLM) agents continually improve by accumulating and reusing past experience, yet it remains unclear whether they faithfully rely on that experience to guide their behavior. We present the first systematic investigation of experience faithfulness, the causal dependence of an agent's decisions on the experience it is given, in self-evolving LLM agents. Using controlled causal interventions on both raw and condensed forms of experience, we comprehensively evaluate four representative frameworks across 10 LLM backbones and 9 environments. Our analysis uncovers a striking asymmetry: while agents consistently depend on raw experience, they often disregard or misinterpret condensed experience, even when it is the only experience provided. This gap persists across single- and multi-agent configurations and across backbone scales. We trace its underlying causes to three factors: the semantic limitations of condensed content, internal processing biases that suppress experience, and task regimes where pretrained priors already suffice. These findings challenge prevailing assumptions about self-evolving methods and underscore the need for more faithful and reliable approaches to experience integration.

</details>


### [14] [Stop Jostling: Adaptive Negative Sampling Reduces the Marginalization of Low-Resource Language Tokens by Cross-Entropy Loss](https://arxiv.org/abs/2601.22439)
*Galim Turumtaev*

Main category: cs.CL

TL;DR: 针对低资源语言罕见词元边缘化问题，提出阈值负采样方法，显著提升语言模型表现。


<details>
  <summary>Details</summary>
Motivation: 低资源语言训练数据有限，罕见词元在训练中受边缘化影响严重，导致学习效果差。

Method: 引入阈值技术限制负采样中的边缘化作用，使罕见词元得到更有效的对齐学习。

Result: 字符级语言模型实验表明，该方法在低资源语言验证集上性能有明显提升。

Conclusion: 通过阈值技术减少边缘化对罕见词元的负面影响，显著提升了低资源语言的模型表现。

Abstract: Neural language models often struggle with low-resource languages due to the limited availability of training data, making tokens from these languages rare in the training set. This paper addresses a specific challenge during training: rare tokens are disproportionately affected by marginalization, which prevents them from learning effectively. We propose a thresholding technique that reduces the impact of this marginalization, allowing rare tokens to benefit from more meaningful alignment. Through experiments with a character-level language model, we demonstrate that this method significantly improves performance on low-resource language validation data. This work is the first to show how negative sampling can be applied to improve the representation of rare tokens by limiting the harmful influence of excessive marginalization, offering a new approach to enhancing language model performance for underrepresented languages.

</details>


### [15] [SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization](https://arxiv.org/abs/2601.22491)
*Jinyang Wu,Changpeng Yang,Yuhao Shen,Fangzhi Xu,Bolin Ni,Chonghua Liao,Yuchen Liu,Hongzhen Wang,Shuai Nie,Shuai Zhang,Haoran Luo,Jiaming Xu*

Main category: cs.CL

TL;DR: 提出SSL框架，通过分级奖励引导策略向优质解空间聚焦，提高智能体的优化效果和样本效率，验证了其在多种任务上的广泛适用性和优势。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法多采用二元奖励，无法区分同类结果的质量差异，忽略解空间的多样性，限制了智能体的优化能力。

Method: 提出了基于“甜点”概念的Sweet Spot Learning（SSL），通过逐步放大和分层的奖励信号，引导智能体策略向优质解空间的甜点区域靠拢。针对不同任务设计了距离分级或增量奖励，增强训练的方向性和效果。

Result: SSL在GUI感知、规划和复杂推理等12个基准测试中表现优异，样本效率提升最高达2.5倍，且具有良好的跨任务传递能力。

Conclusion: 论文提出了SSL框架，通过分级奖励机制引导策略优化，以提升强化学习中智能体的性能和样本效率。

Abstract: Reinforcement learning with verifiable rewards has emerged as a powerful paradigm for training intelligent agents. However, existing methods typically employ binary rewards that fail to capture quality differences among trajectories achieving identical outcomes, thereby overlooking potential diversity within the solution space. Inspired by the ``sweet spot'' concept in tennis-the racket's core region that produces optimal hitting effects, we introduce \textbf{S}weet \textbf{S}pot \textbf{L}earning (\textbf{SSL}), a novel framework that provides differentiated guidance for agent optimization. SSL follows a simple yet effective principle: progressively amplified, tiered rewards guide policies toward the sweet-spot region of the solution space. This principle naturally adapts across diverse tasks: visual perception tasks leverage distance-tiered modeling to reward proximity, while complex reasoning tasks reward incremental progress toward promising solutions. We theoretically demonstrate that SSL preserves optimal solution ordering and enhances the gradient signal-to-noise ratio, thereby fostering more directed optimization. Extensive experiments across GUI perception, short/long-term planning, and complex reasoning tasks show consistent improvements over strong baselines on 12 benchmarks, achieving up to 2.5X sample efficiency gains and effective cross-task transferability. Our work establishes SSL as a general principle for training capable and robust agents.

</details>


### [16] [Mock Worlds, Real Skills: Building Small Agentic Language Models with Synthetic Tasks, Simulated Environments, and Rubric-Based Rewards](https://arxiv.org/abs/2601.22511)
*Yuan-Jay Lü,Chengyu Wang,Lei Shen,Jun Huang,Tong Xu*

Main category: cs.CL

TL;DR: 本研究提出SYNTHAGENT框架，合成多样化任务与模拟环境，通过强化学习显著提升小型语言模型的自主能力，表现优越于大型模型。


<details>
  <summary>Details</summary>
Motivation: 小型大型语言模型难以匹配大型昂贵模型的自主能力，现有开放源代码的训练数据任务种类有限且易解，实际API缺乏多样性且不稳定，阻碍了基于强化学习的提升。

Method: SYNTHAGENT框架由强大的教师模型生成新的任务和工具生态系统，并将其改写为不完全指定的指令，促使代理主动向用户查询缺失信息。使用基于LLM的用户模拟器和模拟工具系统提供稳定反馈，结合基于子目标、用户代理互动和禁忌行为构建的任务级评分系统进行强化学习训练。

Result: 在数学、搜索和工具使用等14个复杂数据集上，使用合成训练数据训练的小模型取得明显进步，且优于更大模型基线。

Conclusion: 通过SYNTHAGENT框架，使用合成的多样化工具使用训练数据和完整环境模拟，小型语言模型在多个复杂任务上实现了显著性能提升，甚至超过了更大的基线模型。

Abstract: Small LLMs often struggle to match the agentic capabilities of large, costly models. While reinforcement learning can help, progress has been limited by two structural bottlenecks: existing open-source agentic training data are narrow in task variety and easily solved; real-world APIs lack diversity and are unstable for large-scale reinforcement learning rollout processes. We address these challenges with SYNTHAGENT, a framework that jointly synthesizes diverse tool-use training data and simulates complete environments. Specifically, a strong teacher model creates novel tasks and tool ecosystems, then rewrites them into intentionally underspecified instructions. This compels agents to actively query users for missing details. When handling synthetic tasks, an LLM-based user simulator provides user-private information, while a mock tool system delivers stable tool responses. For rewards, task-level rubrics are constructed based on required subgoals, user-agent interactions, and forbidden behaviors. Across 14 challenging datasets in math, search, and tool use, models trained on our synthetic data achieve substantial gains, with small models outperforming larger baselines.

</details>


### [17] [One Ring to Rule Them All: Unifying Group-Based RL via Dynamic Power-Mean Geometry](https://arxiv.org/abs/2601.22521)
*Weisong Zhao,Tong Wang,Zichang Tan,Te Yang,Siran Peng,Haoyuan Zhang,Tianshuo Zhang,Haichao Shi,Meng Meng,Yang Yang,Xiangyu Zhu,Zhen Lei,Xiao-Yu Zhang,Xu Zhou*

Main category: cs.CL

TL;DR: 本文提出PMPO框架，通过自适应调整聚合几何参数，有效整合群体强化学习中的GRPO与GMPO，提升了算法的稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于群体的强化学习方法如GRPO和GMPO都依赖固定的聚合几何，并未考虑轨迹的异质性和变化性，限制了性能的提升。

Method: 提出了Power-Mean Policy Optimization (PMPO) 框架，通过调节幂均值几何指数p来统一GRPO和GMPO，并引入基于轨迹裁剪比例的Clip-aware有效样本数（ESS）机制，自适应确定参数p。

Result: PMPO在多个数学推理基准测试中优于现有强基线，显示了其在稳定性和性能上的提升。

Conclusion: PMPO通过参数化聚合几何结构并动态调整参数p，实现了在不同轨迹稳定性之间的灵活切换，显著提升了强化学习算法在数学推理基准上的表现。

Abstract: Group-based reinforcement learning has evolved from the arithmetic mean of GRPO to the geometric mean of GMPO. While GMPO improves stability by constraining a conservative objective, it shares a fundamental limitation with GRPO: reliance on a fixed aggregation geometry that ignores the evolving and heterogeneous nature of each trajectory. In this work, we unify these approaches under Power-Mean Policy Optimization (PMPO), a generalized framework that parameterizes the aggregation geometry via the power-mean geometry exponent p. Within this framework, GRPO and GMPO are recovered as special cases. Theoretically, we demonstrate that adjusting p modulates the concentration of gradient updates, effectively reweighting tokens based on their advantage contribution. To determine p adaptively, we introduce a Clip-aware Effective Sample Size (ESS) mechanism. Specifically, we propose a deterministic rule that maps a trajectory clipping fraction to a target ESS. Then, we solve for the specific p to align the trajectory induced ESS with this target one. This allows PMPO to dynamically transition between the aggressive arithmetic mean for reliable trajectories and the conservative geometric mean for unstable ones. Experiments on multiple mathematical reasoning benchmarks demonstrate that PMPO outperforms strong baselines.

</details>


### [18] [$ρ$-$\texttt{EOS}$: Training-free Bidirectional Variable-Length Control for Masked Diffusion LLMs](https://arxiv.org/abs/2601.22527)
*Jingyi Yang,Yuxian Jiang,Jing Shao*

Main category: cs.CL

TL;DR: 本文提出了一种基于结束符隐含密度的单阶段训练免费方法$\rho$-$\texttt{EOS}$，实现掩码扩散大语言模型的双向可变长度生成，显著提升了效率和利用率。


<details>
  <summary>Details</summary>
Motivation: 当前掩码扩散大语言模型需要预定义且固定的生成长度，缺乏灵活性，且存在生成质量与计算效率之间的权衡，因此需要一种能够灵活调整生成长度的方法。

Method: 通过分析去噪过程中隐含的结束符($\texttt{EOS}$)密度作为生成充分性的信号，设计了基于连续估计$\texttt{EOS}$密度的单阶段训练免费策略$\rho$-$\texttt{EOS}$，在去噪过程中根据密度调节掩码空间的长度，实现双向长度调整。

Result: 在数学和代码基准测试中，$\rho$-$\texttt{EOS}$方法在保持性能的同时，大幅提升了推理效率和令牌利用率。

Conclusion: 本文提出的$\rho$-$\texttt{EOS}$方法解决了当前掩码扩散大语言模型(dLLMs)对生成长度固定且预定义的限制，实现了灵活的双向可变长度生成，提高了推理效率和令牌利用率。

Abstract: Beyond parallel generation and global context modeling, current masked diffusion large language models (dLLMs) suffer from a fundamental limitation: they require a predefined, fixed generation length, which lacks flexibility and forces an inevitable trade-off between output quality and computational efficiency. To address this, we study the denoising dynamics and find that the implicit density ($ρ$) of end-of-sequence ($\texttt{EOS}$) tokens serves as a reliable signal of generation sufficiency. In particular, the evolving implicit $\texttt{EOS}$ density during denoising reveals whether the current masked space is excessive or insufficient, thereby guiding the adjustment direction for generation length. Building on this insight, we propose $\textbf{$ρ$-$\texttt{EOS}$}$, a training-free, single-stage strategy that enables bidirectional variable-length generation for masked dLLMs. Unlike prior two-stage approaches--which require separate length adjustment and iterative mask insertion phases while supporting only unidirectional expansion--$\textbf{$ρ$-$\texttt{EOS}$}$ achieves bidirectional length adjustment within a unified denoising process by continuously estimating the implicit $\texttt{EOS}$ density: excessively high density triggers $\texttt{MASK}$ token contraction, while insufficient density induces expansion. Extensive experiments on mathematics and code benchmarks demonstrate that $\textbf{$ρ$-$\texttt{EOS}$}$ achieves comparable performance while substantially improving inference efficiency and token utilization.

</details>


### [19] [Towards the Holographic Characteristic of LLMs for Efficient Short-text Generation](https://arxiv.org/abs/2601.22546)
*Shun Qian,Bingquan Liu,Chengjie Sun,Zhen Xu,Baoxun Wang*

Main category: cs.CL

TL;DR: 本文发现大型语言模型生成时有全息特征，提出HOLO插件利用该特征提升生成效率，实验结果显示该方法效果良好。


<details>
  <summary>Details</summary>
Motivation: 当前对LLMs在上下文学习和思维链能力的研究较多，但对于其强大生成能力的具体特征缺乏深入探讨，本文旨在填补这一空白。

Method: 提出了名为HOLO的插件，通过利用语言模型的全息特征，在有限生成步骤内提取目标关键词，并结合平行的词汇约束文本生成方法完成句子生成。

Result: 在不同架构和规模的语言模型短文本生成实验中，HOLO插件在自动和人工评价指标上表现出与基线方法相当的性能，验证了全息特征的有效性。

Conclusion: 本文揭示了大型语言模型(LLMs)在生成过程中呈现的全息特征，即模型倾向于在生成初期捕捉目标关键词，这一发现为提升推理效率提供了新的思路。

Abstract: The recent advancements in Large Language Models (LLMs) have attracted interest in exploring their in-context learning abilities and chain-of-thought capabilities. However, there are few studies investigating the specific traits related to the powerful generation capacity of LLMs. This paper aims to delve into the generation characteristics exhibited by LLMs. Through our investigation, we have discovered that language models tend to capture target-side keywords at the beginning of the generation process. We name this phenomenon the Holographic Characteristic of language models. For the purpose of exploring this characteristic and further improving the inference efficiency of language models, we propose a plugin called HOLO, which leverages the Holographic Characteristic to extract target-side keywords from language models within a limited number of generation steps and complements the sentence with a parallel lexically constrained text generation method. To verify the effectiveness of HOLO, we conduct massive experiments on language models of varying architectures and scales in the short-text generation scenario. The results demonstrate that HOLO achieves comparable performance to the baselines in terms of both automatic and human-like evaluation metrics and highlight the potential of the Holographic Characteristic.

</details>


### [20] [Are LLM Evaluators Really Narcissists? Sanity Checking Self-Preference Evaluations](https://arxiv.org/abs/2601.22548)
*Dani Roytburg,Matthew Bozoukov,Matthew Nguyen,Mackenzie Puig-Hall,Narmeen Oozeer*

Main category: cs.CL

TL;DR: 本文揭示LLM评审自我偏好中的一个重要误差来源，并提出基于评审质量的校正方法，有效减少评审偏见，提高自动评估的可靠性。


<details>
  <summary>Details</summary>
Motivation: 近期研究发现LLM作为评审时偏向自己生成的结果，影响自动后期训练和评估的公正性，因此需要区分真是的自我偏好与实验环境中的混淆因素。

Method: 引入了评审质量基线，比较评审者错误地选择自身回答的概率与选择其他模型错误回答的概率，从而降低了自我偏好的噪声影响。

Result: 通过对37,448次查询的实验，发现只有51%的最初发现保留统计显著性，验证了校正基线能有效排除噪声数据，提高自我偏好研究的准确性。

Conclusion: 本文发现了大型语言模型（LLM）作为评审时存在的自我偏好偏差中，一个核心的测量混淆因素，并提出了一个校正基线以减少该测量误差。

Abstract: Recent research has shown that large language models (LLM) favor own outputs when acting as judges, undermining the integrity of automated post-training and evaluation workflows. However, it is difficult to disentangle which evaluation biases are explained by narcissism versus general experimental confounds, distorting measurements of self-preference bias. We discover a core methodological confound which could reduce measurement error by 89.6%. Specifically, LLM evaluators may deliver self-preferring verdicts when the judge responds to queries which they completed incorrectly themselves; this would be true regardless of whether one of their responses is their own. To decouple self-preference signals from noisy outputs on hard problems, we introduce an Evaluator Quality Baseline, which compares the probability that a judge incorrectly votes for itself against the probability that it votes for an incorrect response from another model. Evaluating this simple baseline on 37,448 queries, only 51% of initial findings retain statistical significance. Finally, we turn towards characterizing the entropy of "easy" versus "hard" evaluation votes from LLM judges. Our corrective baseline enables future research on self-preference by eliminating noisy data from potential solutions. More widely, this work contributes to the growing body of work on cataloging and isolating judge-bias effects.

</details>


### [21] [SpanNorm: Reconciling Training Stability and Performance in Deep Transformers](https://arxiv.org/abs/2601.22580)
*Chao Wang,Bei Li,Jiaqi Zhang,Xinyu Liu,Yuchun Fan,Linkun Lyu,Xin Chen,Jingang Wang,Tong Xiao,Peng Pei,Xunliang Cai*

Main category: cs.CL

TL;DR: SpanNorm是一种新型归一化方法，结合PreNorm的稳定性和PostNorm的性能，理论和实验均证明其在Transformer训练中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的PreNorm结构虽然稳定但深层模型表现下降，PostNorm虽性能强但训练不稳定，迫切需要一种结合两者优势的新范式。

Method: 设计了SpanNorm，它通过跨越整个Transformer模块的残差连接稳定信号传播，并采用PostNorm风格的归一化来提升性能，同时辅以理论上的信号方差边界分析和缩放策略。

Result: SpanNorm在实验中在密集模型和混合专家模型场景下均优于标准归一化方法，实现了更强大和稳定的Transformer架构。

Conclusion: 提出的SpanNorm方法成功融合了PreNorm和PostNorm的优势，解决了训练稳定性与性能之间的权衡问题。

Abstract: The success of Large Language Models (LLMs) hinges on the stable training of deep Transformer architectures. A critical design choice is the placement of normalization layers, leading to a fundamental trade-off: the ``PreNorm'' architecture ensures training stability at the cost of potential performance degradation in deep models, while the ``PostNorm'' architecture offers strong performance but suffers from severe training instability. In this work, we propose SpanNorm, a novel technique designed to resolve this dilemma by integrating the strengths of both paradigms. Structurally, SpanNorm establishes a clean residual connection that spans the entire transformer block to stabilize signal propagation, while employing a PostNorm-style computation that normalizes the aggregated output to enhance model performance. We provide a theoretical analysis demonstrating that SpanNorm, combined with a principled scaling strategy, maintains bounded signal variance throughout the network, preventing the gradient issues that plague PostNorm models, and also alleviating the representation collapse of PreNorm. Empirically, SpanNorm consistently outperforms standard normalization schemes in both dense and Mixture-of-Experts (MoE) scenarios, paving the way for more powerful and stable Transformer architectures.

</details>


### [22] [Rethinking LLM-as-a-Judge: Representation-as-a-Judge with Small Language Models via Semantic Capacity Asymmetry](https://arxiv.org/abs/2601.22588)
*Zhuochun Li,Yong Zhang,Ming Li,Yuelyu Ji,Yiming Zeng,Ning Cheng,Yun Zhu,Yanmeng Wang,Shaojun Wang,Jing Xiao,Daqing He*

Main category: cs.CL

TL;DR: 本文发现小型语言模型隐藏状态中蕴含丰富评估语义，提出Representation-as-a-Judge范式，设计INSPECTOR框架，实现在推理任务上准确高效的评估，接近大型模型水平，成本更低且更可解释。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型作为评判者的方式成本高且不透明，易受提示设计影响。同时发现小型模型虽然生成能力弱，但其内部隐藏状态中却蕴含丰富的评估信息，因此希望探索利用小模型内部表示进行高效评估。

Method: 本文提出了INSPECTOR框架，一种基于探测的方法，通过分析小型语言模型的隐藏状态中的语义信息，预测文本的各方面评估分数，避免依赖大型生成模型和复杂的提示设计。

Result: 实验在多个推理基准（GSM8K、MATH、GPQA）上表明，INSPECTOR显著优于基于提示的小型语言模型评估，且接近大型语言模型的评判效果，且计算更高效，评估更可靠且可解释。

Conclusion: 本文提出了Representation-as-a-Judge的范式，证明通过利用小型语言模型的内部表征而非生成输出，也能高效地进行文本评估。通过INSPECTOR框架，实现了对推理任务的准确评估，性能接近大型语言模型的评判，同时具有更高的效率和可解释性。

Abstract: Large language models (LLMs) are widely used as reference-free evaluators via prompting, but this "LLM-as-a-Judge" paradigm is costly, opaque, and sensitive to prompt design. In this work, we investigate whether smaller models can serve as efficient evaluators by leveraging internal representations instead of surface generation. We uncover a consistent empirical pattern: small LMs, despite with weak generative ability, encode rich evaluative signals in their hidden states. This motivates us to propose the Semantic Capacity Asymmetry Hypothesis: evaluation requires significantly less semantic capacity than generation and can be grounded in intermediate representations, suggesting that evaluation does not necessarily need to rely on large-scale generative models but can instead leverage latent features from smaller ones. Our findings motivate a paradigm shift from LLM-as-a-Judge to Representation-as-a-Judge, a decoding-free evaluation strategy that probes internal model structure rather than relying on prompted output. We instantiate this paradigm through INSPECTOR, a probing-based framework that predicts aspect-level evaluation scores from small model representations. Experiments on reasoning benchmarks (GSM8K, MATH, GPQA) show that INSPECTOR substantially outperforms prompting-based small LMs and closely approximates full LLM judges, while offering a more efficient, reliable, and interpretable alternative for scalable evaluation.

</details>


### [23] [Language Model Circuits Are Sparse in the Neuron Basis](https://arxiv.org/abs/2601.22594)
*Aryaman Arora,Zhengxuan Wu,Jacob Steinhardt,Sarah Schwettmann*

Main category: cs.CL

TL;DR: 研究发现MLP神经元具备与稀疏自编码器相当的稀疏性，基于此开发电路追踪管线，实现多任务神经元级因果分析及模型行为控制，推动语言模型自动解释技术发展，且无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型解释多侧重用稀疏自编码器分解神经元特征，忽视了MLP神经元本身可能同样稀疏且可解释的潜力，旨在提升解释方法的自动化与成本效益。

Method: 采用梯度归因方法，利用MLP神经元特征基础进行端到端电路追踪，验证该基础稀疏性并分析神经元对模型行为的控制能力。

Result: 在主谓一致和多跳城市→州→首都推理任务中，约100个MLP神经元即可控制模型行为，发现特定神经元组编码潜在推理步骤，并能操控最终输出。

Conclusion: 本文首次实证表明MLP神经元的特征基础与稀疏自编码器（SAEs）一样稀疏，且基于此开发的端到端电路追踪管线能定位多种任务中的因果电路，实现模型行为控制。

Abstract: The high-level concepts that a neural network uses to perform computation need not be aligned to individual neurons (Smolensky, 1986). Language model interpretability research has thus turned to techniques such as \textit{sparse autoencoders} (SAEs) to decompose the neuron basis into more interpretable units of model computation, for tasks such as \textit{circuit tracing}. However, not all neuron-based representations are uninterpretable. For the first time, we empirically show that \textbf{MLP neurons are as sparse a feature basis as SAEs}. We use this finding to develop an end-to-end pipeline for circuit tracing on the MLP neuron basis, which locates causal circuitry on a variety of tasks using gradient-based attribution. On a standard subject-verb agreement benchmark (Marks et al., 2025), a circuit of $\approx 10^2$ MLP neurons is enough to control model behaviour. On the multi-hop city $\to$ state $\to$ capital task from Lindsey et al., 2025, we find a circuit in which small sets of neurons encode specific latent reasoning steps (e.g.~`map city to its state'), and can be steered to change the model's output. This work thus advances automated interpretability of language models without additional training costs.

</details>


### [24] [Layer-wise Swapping for Generalizable Multilingual Safety](https://arxiv.org/abs/2601.22620)
*Hyunseo Shin,Wonseok Hwang*

Main category: cs.CL

TL;DR: 针对低资源语言模型安全风险突出的问题，本文提出了一种无需额外训练即可实现安全对齐迁移的层交换方法，显著提升了低资源语言模型的安全性，且保持了语言理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有安全数据集主要集中在英语，导致低资源语言模型在安全性方面表现较差，需要一种方法提升低资源语言模型的安全对齐能力。

Method: 通过安全感知层交换技术，基于模块的专业化程度自适应选择或混合模型组件，实现安全对齐知识的迁移。

Result: 实验结果表明，该方法在MMMLU、BELEBELE和MGSM等通用基准测试中表现与语言专家模型相当，同时在MultiJail安全基准测试中产生了更安全、更少有害的响应。

Conclusion: 该论文提出的安全感知层交换方法能够在不额外训练的情况下，将英语安全专家模型的安全对齐知识有效迁移到低资源语言专家模型中，从而提升低资源语言模型的安全性，同时保持其在通用语言理解任务上的性能。

Abstract: Despite the rapid advancements of Large Language Models (LLMs), safety risks remain a critical challenge for low-resource languages. Existing safety datasets are predominantly English centric, limiting progress in multilingual safety alignment. As a result, low resource expert models, finetuned on their respective instruction datasets, tend to exhibit higher unsafety rates compared to their high resource counterparts. In this work, we propose a safety aware layer swapping method that transfers safety alignment from an English safety expert to low resource language experts without additional training. To further enhance transfer ability, our method adaptively selects or blends modules based on their degree of specialization. Our approach preserves performance on general language understanding tasks while enhancing safety in the target languages. Experimental results show that the proposed method achieves comparable performance to the language expert on general benchmarks such as MMMLU, BELEBELE, and MGSM, while producing more aligned and less harmful responses on the MultiJail safety benchmark.

</details>


### [25] [Time-Annealed Perturbation Sampling: Diverse Generation for Diffusion Language Models](https://arxiv.org/abs/2601.22629)
*Jingxuan Wu,Zhenglin Wan,Xingrui Yu,Yuzhe Yang,Yiqiao Huang,Ivor Tsang,Yang You*

Main category: cs.CL

TL;DR: 本文揭示了Diffusion语言模型在生成中时间维度的不同作用，提出一种新的采样方法TAPS以提升生成多样性，实验证明其有效性和兼容性。


<details>
  <summary>Details</summary>
Motivation: Diffusion语言模型虽然具有明确的时间维度，但如何利用这一结构控制生成多样性以探索多种有效的语义或推理路径尚未充分研究。

Method: 提出了一种称为时间退火扰动采样（TAPS）的推理策略，该策略在扩散过程早期鼓励语义分支，后期逐渐减少扰动，以此实现语义多样性和流畅性的平衡，适用于非自回归及半自回归的Diffusion语言模型架构。

Result: 在LLaDA和TraDo模型上验证，TAPS方法在创意写作和推理基准测试中显著提高了输出多样性，同时不降低生成质量和指令遵循度。

Conclusion: Diffusion语言模型通过引入显式时间维度，为文本生成提供了不同阶段对生成内容的不同控制能力，早期步骤确定全局语义结构，后期步骤进行局部词汇细化。基于此，提出的无训练推理策略TAPS有效提升了生成多样性，同时保持生成质量和遵循指令。

Abstract: Diffusion language models (Diffusion-LMs) introduce an explicit temporal dimension into text generation, yet how this structure can be leveraged to control generation diversity for exploring multiple valid semantic or reasoning paths remains underexplored. In this paper, we show that Diffusion-LMs, like diffusion models in image generation, exhibit a temporal division of labor: early denoising steps largely determine the global semantic structure, while later steps focus on local lexical refinement. Building on this insight, we propose Time-Annealed Perturbation Sampling (TAPS), a training-free inference strategy that encourages semantic branching early in the diffusion process while progressively reducing perturbations to preserve fluency and instruction adherence. TAPS is compatible with both non-autoregressive and semi-autoregressive Diffusion backbones, demonstrated on LLaDA and TraDo in our paper, and consistently improves output diversity across creative writing and reasoning benchmarks without compromising generation quality.

</details>


### [26] [DART-ing Through the Drift: Dynamic Tracing of Knowledge Neurons for Adaptive Inference-Time Pruning](https://arxiv.org/abs/2601.22632)
*Abhishek Tyagi,Yunuo Cen,Shrey Dhorajiya,Bharadwaj Veeravalli,Xuanyao Fong*

Main category: cs.CL

TL;DR: 本文提出的DART方法通过动态监测注意力分布实现上下文自适应剪枝，显著提升了大语言模型的推理效率和准确性，且资源开销极低。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型中的前馈网络存在显著冗余，且现有剪枝方法依赖特定数据校准且静态，无法捕捉自动回归生成过程中知识神经元的动态变化。

Method: 提出DART（动态注意力引导的运行时跟踪），一种轻量级无训练需求的上下文动态剪枝方法，通过监控注意力分数分布的变化动态更新神经元掩码。

Result: 在10个基准测试中，DART在70% FFN稀疏度下提升LLAMA-3.1-8B模型准确率最高达14.5%，在摘要任务上的ROUGE-L分数提升达3倍，同时性能接近原始密集模型，且运行时内存占用低于10MB，FLOPs开销仅0.1%。

Conclusion: DART框架能够有效适应多种语义上下文，在保持模型能力的同时实现动态剪枝，显著提升了大语言模型的推理效率和性能。

Abstract: Large Language Models (LLMs) exhibit substantial parameter redundancy, particularly in Feed-Forward Networks (FFNs). Existing pruning methods suffer from two primary limitations. First, reliance on dataset-specific calibration introduces significant data dependency and computational overhead. Second, being predominantly static, they fail to account for the evolving subset of knowledge neurons in LLMs during autoregressive generation as the context evolves. To address this, we introduce DART, i.e., Dynamic Attention-Guided Runtime Tracing), a lightweight, training-free method that performs on-the-fly context-based pruning. DART monitors shifts in attention score distributions to infer context changes, dynamically updating neuron-level masks to retain salient parameters. Across ten benchmarks, DART outperforms prior dynamic baseline, achieving accuracy gains of up to 14.5% on LLAMA-3.1-8B at 70% FFN sparsity. Furthermore, DART achieves up to 3x better ROUGE-L scores with respect to static-masked pruning on summarization tasks, with its performance comparable to the original dense models. We conclusively demonstrate that the proposed framework effectively adapts to diverse semantic contexts, preserves model capabilities across both general and domain-specific tasks while running at less than 10MBs of memory for LLAMA-3.1-8B(16GBs) with 0.1% FLOPs overhead. The code is available at https://github.com/seeder-research/DART.

</details>


### [27] [NAG: A Unified Native Architecture for Encoder-free Text-Graph Modeling in Language Models](https://arxiv.org/abs/2601.22657)
*Haisong Gong,Zhibo Liu,Qiang Liu,Shu Wu,Liang Wang*

Main category: cs.CL

TL;DR: 提出NAG统一框架，将图结构处理融入语言模型自注意力中，无需外部图神经网络，提升文本-图任务的效果和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常采用独立的图神经网络与语言模型分离处理结构与语义，导致图结构与文本语义的对齐复杂且效率低下。

Method: 提出了NAG框架，利用语言模型的自注意力机制施加拓扑依赖，重新校准位置编码，通过两种实现方式（NAG-Zero和NAG-LoRA）实现图结构信息的融合。

Result: 在多种图任务上，NAG实现了稳健的图结构理解，摆脱了对外部编码器的依赖，简化了文本-图模型的设计。

Conclusion: NAG框架通过将图结构处理内嵌于语言模型的自注意力机制中，实现了更统一、更有效的文本-图融合，提升了图任务的理解能力，且无需外部编码器。

Abstract: Prevailing methods for integrating graphs into Language Models (LMs) typically rely on a segregated architecture: external Graph Neural Networks (GNNs) encode structural topology, while LMs process textual semantics. We argue this approach is suboptimal for text-graphs: it creates a conceptually disjointed interaction paradigm. By segregating structural encoding from semantic processing, these systems must perform a complex implicit alignment between abstract graph tokens and concrete textual elements. Challenging the necessity of external encoders, we propose NAG (Native Architecture for Graphs), a unified framework that internalizes graph processing within the LM's native manifold. Instead of bridging disparate embedding spaces, NAG repurposes the self-attention mechanism to enforce topological dependencies and recalibrates positional IDs to ensure structural equivalence. This allows the model to harness its intrinsic linguistic capability to simultaneously comprehend node and edge content alongside structural topology. We introduce two efficient implementations: NAG-Zero for absolute preservation of the base model's linguistic capabilities, and NAG-LoRA for enhanced structural adaptation. Experiments across diverse graph tasks validate that NAG achieves robust graph comprehension without the overhead of external encoders, offering a simpler, more coherent paradigm for text-graph modeling.

</details>


### [28] [TSLM: Tree-Structured Language Modeling for Divergent Thinking](https://arxiv.org/abs/2601.22688)
*Doyoung Kim,Jaehyeok Doo,Minjoon Seo*

Main category: cs.CL

TL;DR: 本文提出TSLM，通过树结构编码实现多路径搜索，提升了语言模型推理效率和系统性探索能力。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型顺序生成限制了搜索过程中的路径分离，无法高效实现系统性探索。

Method: 引入特殊标记编码树结构，训练语言模型生成完整搜索树包含成功与失败路径，内部实现系统性探索，避免重复计算。

Result: TSLM在推理效率和鲁棒性上超过基于外部搜索的多次独立前向传播方法。

Conclusion: TSLM通过编码分支结构，实现了语言模型在单次生成过程中生成和选择性展开多搜索路径，提升了推理的系统性和效率。

Abstract: Language models generate reasoning sequentially, preventing them from decoupling irrelevant exploration paths during search. We introduce Tree-Structured Language Modeling (TSLM), which uses special tokens to encode branching structure, enabling models to generate and selectively expand multiple search paths within a single generation process. By training on complete search trees including both successful and failed attempts, TSLM learns to internalize systematic exploration without redundant recomputation of shared prefixes. TSLM achieves robust performance and superior inference efficiency by avoiding the multiple independent forward passes required by external search methods. These results suggest a new paradigm of inference-time scaling for robust reasoning, demonstrating that supervised learning on complete tree-structured traces provides an efficient alternative for developing systematic exploration capabilities in language models.

</details>


### [29] [FNF: Functional Network Fingerprint for Large Language Models](https://arxiv.org/abs/2601.22692)
*Yiheng Liu,Junhao Ning,Sichen Xia,Haiyang Sun,Yang Yang,Hanyang Chi,Xiaohui Gao,Ning Qiang,Bao Ge,Junwei Han,Xintao Hu*

Main category: cs.CL

TL;DR: 本文提出功能网络指纹方法，通过分析模型神经活动一致性，实现对大型语言模型知识产权的保护，具有样本少、无训练且对模型修改鲁棒的优点。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的开发成本高且具有重要商业价值，防止开源大型语言模型被未经授权使用以及保护开发者知识产权成为关键挑战。

Method: 提出了一种无训练、样本高效的功能网络指纹（FNF）方法，通过比较怀疑模型与原始模型在功能网络中的神经活动一致性来检测模型是否派生自原始模型。

Result: 验证了即使规模或架构不同，来源相同的模型在多样输入下的神经活动高度一致，而独立训练的模型则无此一致性。该方法只需少量样本即可验证，且对模型微调、剪枝、参数置换等修改鲁棒，支持跨架构比较。

Conclusion: FNF为模型所有者和第三方提供了一种简单、非侵入且有效的工具，用于保护大型语言模型的知识产权。

Abstract: The development of large language models (LLMs) is costly and has significant commercial value. Consequently, preventing unauthorized appropriation of open-source LLMs and protecting developers' intellectual property rights have become critical challenges. In this work, we propose the Functional Network Fingerprint (FNF), a training-free, sample-efficient method for detecting whether a suspect LLM is derived from a victim model, based on the consistency between their functional network activity. We demonstrate that models that share a common origin, even with differences in scale or architecture, exhibit highly consistent patterns of neuronal activity within their functional networks across diverse input samples. In contrast, models trained independently on distinct data or with different objectives fail to preserve such activity alignment. Unlike conventional approaches, our method requires only a few samples for verification, preserves model utility, and remains robust to common model modifications (such as fine-tuning, pruning, and parameter permutation), as well as to comparisons across diverse architectures and dimensionalities. FNF thus provides model owners and third parties with a simple, non-invasive, and effective tool for protecting LLM intellectual property. The code is available at https://github.com/WhatAboutMyStar/LLM_ACTIVATION.

</details>


### [30] [Models Know Models Best: Evaluation via Model-Preferred Formats](https://arxiv.org/abs/2601.22699)
*Joonhak Lee,Sungmok Jung,Jongyeon Park,Jaejin Lee*

Main category: cs.CL

TL;DR: 为了弥合大语言模型在多选任务中因评估格式差异导致的性能差距，本文提出动态格式对齐，通过模型自身信号选择最优格式，显著提升了模型表现。


<details>
  <summary>Details</summary>
Motivation: 不同评估格式（符号选择与填空式）在多项选择任务中的表现差异明显，且现有人为设计的启发式方法常常降低性能。

Method: 提出一种轻量级分类器，基于模型偏好信号动态选择适合每个问题实例的评估格式。

Result: 该方法在推理和知识基准测试中实现了显著且稳定的零样本准确率提升，效果超越固定格式评估。

Conclusion: 动态格式对齐策略能显著提升大语言模型在多项选择任务中的零样本准确率，更好地揭示模型潜在能力。

Abstract: Performance of Large Language Models (LLMs) on multiple-choice tasks differs markedly between symbol-based and cloze-style evaluation formats. The observed discrepancies are systematically attributable to task characteristics: natural language continuation benefits from likelihood scoring, whereas explicit comparison is better suited to symbol-based selection. These trends are consistent across various decoder-based LLMs, indicating model-agnostic effects. To address these inconsistencies, a dynamic format-alignment strategy is introduced that employs a lightweight classifier trained on latent model-preference signals. In contrast to human-designed heuristics, which often degrade performance, this approach uses model-generated signals to determine the optimal format for each problem instance. The proposed method achieves substantial and consistent improvements in zero-shot accuracy across reasoning and knowledge benchmarks, better revealing the models' latent capabilities.

</details>


### [31] [MM-THEBench: Do Reasoning MLLMs Think Reasonably?](https://arxiv.org/abs/2601.22735)
*Zhidian Huang,Zijun Yao,Ji Qi,Shangqing Tu,Junxian Ma,Jinxin Liu,Weichuan Liu,Xiaoyin Che,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: 提出MM-THEBench基准测试评估多模态推理模型中思维过程的幻觉问题，揭示思考对推理和幻觉的影响。


<details>
  <summary>Details</summary>
Motivation: 当前多模态推理模型虽然具有一定的思考能力，但幻觉现象及其影响尚未被充分理解，且缺乏对模型内部思考过程幻觉的有效评测方法。

Method: 提出MM-THEBench基准测试，包括基于认知维度的细粒度分类、多样化且带有推理注释的数据集，以及多层次自动评测框架，用于评估推理MLLMs中中间思维链的幻觉问题。

Result: 通过在主流推理MLLMs上的广泛实验，MM-THEBench揭示了思考过程对多模态任务中幻觉与推理能力的影响，提供了对模型思维效果的深刻洞察。

Conclusion: 思考能力提升的多模态大语言模型（MLLMs）在解决复杂问题时仍存在幻觉问题，且现有评测不足以全面衡量其中思维过程中的幻觉现象。

Abstract: Recent advances in multimodal large language models (MLLMs) mark a shift from non-thinking models to post-trained reasoning models capable of solving complex problems through thinking. However, whether such thinking mitigates hallucinations in multimodal perception and reasoning remains unclear. Self-reflective reasoning enhances robustness but introduces additional hallucinations, and subtle perceptual errors still result in incorrect or coincidentally correct answers. Existing benchmarks primarily focus on models before the emergence of reasoning MLLMs, neglecting the internal thinking process and failing to measure the hallucinations that occur during thinking. To address these challenges, we introduce MM-THEBench, a comprehensive benchmark for assessing hallucinations of intermediate CoTs in reasoning MLLMs. MM-THEBench features a fine-grained taxonomy grounded in cognitive dimensions, diverse data with verified reasoning annotations, and a multi-level automated evaluation framework. Extensive experiments on mainstream reasoning MLLMs reveal insights into how thinking affects hallucination and reasoning capability in various multimodal tasks.

</details>


### [32] [AR-BENCH: Benchmarking Legal Reasoning with Judgment Error Detection, Classification and Correction](https://arxiv.org/abs/2601.22742)
*Yifei Li,Richong Zhang,Wanyu Tu,Zhijie Nie,Haokun Luo,Chuantao Yin,Pengchong Li*

Main category: cs.CL

TL;DR: 本文首次提出法律判决复审任务，建立了丰富数据集，并评估了当前大型语言模型，揭示了其在判决错误识别上的不足，推动法律AI研究向异常检测方向发展。


<details>
  <summary>Details</summary>
Motivation: 法律判决容易因案情复杂和法律概念抽象产生错误，现有上诉复审机制效率低下，且现有法律AI研究忽视了判决复审任务的独特性。

Method: 提出了新的法律判决复审任务，构建了包含8,700个精细标注判决和34,617份辅助文献的数据集AR-BENCH，同时对14个大型语言模型进行了评估。

Result: 通过对14个大型语言模型的测试，发现它们在诊断法律适用错误方面存在关键限制，为后续模型改进提供了实证依据。

Conclusion: 现有大型语言模型在识别法律适用错误方面存在显著不足，法律判决复审任务需区别于判决预测和生成，更注重异常检测。

Abstract: Legal judgments may contain errors due to the complexity of case circumstances and the abstract nature of legal concepts, while existing appellate review mechanisms face efficiency pressures from a surge in case volumes. Although current legal AI research focuses on tasks like judgment prediction and legal document generation, the task of judgment review differs fundamentally in its objectives and paradigm: it centers on detecting, classifying, and correcting errors after a judgment is issued, constituting anomaly detection rather than prediction or generation. To address this research gap, we introduce a novel task APPELLATE REVIEW, aiming to assess models' diagnostic reasoning and reliability in legal practice. We also construct a novel dataset benchmark AR-BENCH, which comprises 8,700 finely annotated decisions and 34,617 supplementary corpora. By evaluating 14 large language models, we reveal critical limitations in existing models' ability to identify legal application errors, providing empirical evidence for future improvements.

</details>


### [33] [RASST: Fast Cross-modal Retrieval-Augmented Simultaneous Speech Translation](https://arxiv.org/abs/2601.22777)
*Jiaxuan Luo,Siqi Ouyang,Lei Li*

Main category: cs.CL

TL;DR: 本论文提出RASST方法，将跨模态检索集成到同时语音翻译中，有效提升了术语翻译准确性和整体翻译表现。


<details>
  <summary>Details</summary>
Motivation: 当前的同时语音翻译（SST）技术虽然因Speech LLMs得到了提升，但仍难以正确翻译罕见和领域专有术语。引入检索增强手段以改进术语翻译具有挑战性，尤其是在跨模态检索和增量生成控制方面。

Method: 提出了RASST方法，紧密整合跨模态检索到SST流程中。训练了轻量级的语音-文本检索器，采用滑动窗口检索策略，为Speech LLM提供分块术语提示，同时合成训练数据以教模型精准利用检索结果。

Result: 在三个语言方向的测试中，RASST使术语翻译准确率提升高达16%，整体翻译质量提升最高3个BLEU分，消融实验验证了各组成部分的贡献。

Conclusion: RASST通过有效融合跨模态检索与增量翻译，显著提升了同时语音翻译中术语的翻译准确性及整体质量，展示了检索增强技术在SST中的实用价值。

Abstract: Simultaneous speech translation (SST) produces target text incrementally from partial speech input. Recent speech large language models (Speech LLMs) have substantially improved SST quality, yet they still struggle to correctly translate rare and domain-specific terminology. While retrieval augmentation has been effective for terminology translation in machine translation, bringing retrieval to SST is non-trivial: it requires fast and accurate cross-modal (speech-to-text) retrieval under partial, continually arriving input, and the model must decide whether and when to apply retrieved terms during incremental generation. We propose Retrieval-Augmented Simultaneous Speech Translation (RASST), which tightly integrates cross-modal retrieval into the SST pipeline. RASST trains a lightweight speech-text retriever and performs efficient sliding-window retrieval, providing chunkwise terminology hints to the Speech LLM. We further synthesize training data that teaches the Speech LLM to leverage retrieved terms precisely. Experiments on three language directions of the ACL 60/60 dev set show that RASST improves terminology translation accuracy by up to 16% and increases overall translation quality by up to 3 BLEU points, with ablations confirming the contribution of each component.

</details>


### [34] [Sparse or Dense? A Mechanistic Estimation of Computation Density in Transformer-based LLMs](https://arxiv.org/abs/2601.22795)
*Corentin Kervadec,Iuliia Lysova,Marco Baroni,Gemma Boleda*

Main category: cs.CL

TL;DR: 本文提出了一种机制解释性计算密度估计方法，揭示了LLM计算的密集且动态的特性，指出输入性质显著影响计算密度，有助于深化对LLM处理机制的理解。


<details>
  <summary>Details</summary>
Motivation: 优化大规模语言模型的效率需要理解计算在参数中的分布情况，现有工作假设计算可以稀疏分布，但缺乏系统的定量分析工具。

Method: 设计了一种基于机制解释性的计算密度估计器，用以系统量化LLM中的计算密度，并通过实验证实其有效性。

Result: 发现LLM处理通常涉及密集计算，计算密度随输入动态变化，不同模型间相同输入表现出相关的计算密度，并且罕见词汇和较短上下文会提高计算密度。

Conclusion: LLM的计算密度是动态变化的，且整体呈现密集计算状态，挑战了先前关于其稀疏计算的假设。输入类型和上下文长度对计算密度有显著影响。

Abstract: Transformer-based large language models (LLMs) are comprised of billions of parameters arranged in deep and wide computational graphs. Several studies on LLM efficiency optimization argue that it is possible to prune a significant portion of the parameters, while only marginally impacting performance. This suggests that the computation is not uniformly distributed across the parameters. We introduce here a technique to systematically quantify computation density in LLMs. In particular, we design a density estimator drawing on mechanistic interpretability. We experimentally test our estimator and find that: (1) contrary to what has been often assumed, LLM processing generally involves dense computation; (2) computation density is dynamic, in the sense that models shift between sparse and dense processing regimes depending on the input; (3) per-input density is significantly correlated across LLMs, suggesting that the same inputs trigger either low or high density. Investigating the factors influencing density, we observe that predicting rarer tokens requires higher density, and increasing context length often decreases the density. We believe that our computation density estimator will contribute to a better understanding of the processing at work in LLMs, challenging their symbolic interpretation.

</details>


### [35] [When Meanings Meet: Investigating the Emergence and Quality of Shared Concept Spaces during Multilingual Language Model Training](https://arxiv.org/abs/2601.22851)
*Felicia Körner,Max Müller-Eberstein,Anna Korhonen,Barbara Plank*

Main category: cs.CL

TL;DR: 本文通过因果可解释性技术研究了EuroLLM预训练中多语言共享概念空间的形成和发展，发现这些空间早期出现但具有语言依赖性，且翻译质量提升部分源于行为变化，提供了跨语言对齐训练过程的新见解。


<details>
  <summary>Details</summary>
Motivation: 由于单语资源稀缺，高覆盖多语言的大型语言模型训练变得日益重要，前人研究虽发现模型在共享概念空间处理多语种输入，但缺乏因果方法和深入错误分析，且多关注最终模型，未探究训练过程中这些空间如何形成。

Method: 本文利用因果可解释性方法中的激活修补（activation patching）技术，对EuroLLM预训练过程中的语言无关概念空间发展进行研究，通过隔离跨语言概念表征并将其注入翻译提示，考察翻译一致性的语言独立性。

Result: 研究发现共享概念空间早期出现并持续细化，但与语言的对齐呈语言依赖性。此外，细致手工分析表明部分翻译质量的提升源于行为变化（如多义词的语义选择及跨语言同形异义词的翻译而非复制），而非真正的翻译能力提升。

Conclusion: 本文揭示了跨语言对齐的训练动态及因果可解释性方法在多语言环境下有效性的条件，强调共享概念空间虽早期形成但其语言适应性存在差异，且翻译改进更多来自行为调整而非模型能力提升。

Abstract: Training Large Language Models (LLMs) with high multilingual coverage is becoming increasingly important -- especially when monolingual resources are scarce. Recent studies have found that LLMs process multilingual inputs in shared concept spaces, thought to support generalization and cross-lingual transfer. However, these prior studies often do not use causal methods, lack deeper error analysis or focus on the final model only, leaving open how these spaces emerge during training. We investigate the development of language-agnostic concept spaces during pretraining of EuroLLM through the causal interpretability method of activation patching. We isolate cross-lingual concept representations, then inject them into a translation prompt to investigate how consistently translations can be altered, independently of the language. We find that shared concept spaces emerge early} and continue to refine, but that alignment with them is language-dependent}. Furthermore, in contrast to prior work, our fine-grained manual analysis reveals that some apparent gains in translation quality reflect shifts in behavior -- like selecting senses for polysemous words or translating instead of copying cross-lingual homographs -- rather than improved translation ability. Our findings offer new insight into the training dynamics of cross-lingual alignment and the conditions under which causal interpretability methods offer meaningful insights in multilingual contexts.

</details>


### [36] [From Labels to Facets: Building a Taxonomically Enriched Turkish Learner Corpus](https://arxiv.org/abs/2601.22875)
*Elif Sayar,Tolgahan Türker,Anna Golynskaia Knezhevich,Bihter Dereli,Ayşe Demirhas,Lionel Nicolas,Gülşen Eryiğit*

Main category: cs.CL

TL;DR: 本文提出一种基于多维面向分类法的半自动注释方法，成功应用于土耳其语学习者语料库，实现高准确率和细粒度注释，促进更深入的语言错误分析和研究。


<details>
  <summary>Details</summary>
Motivation: 现有学习者语料库多采用整体平面标签体系，缺乏明确分离的语言维度，限制了深入语言学分析和精细化错误研究，亟需一种能支持多维、细粒度且标准化注释的方法。

Method: 利用最近提出的多维面向分类法构建注释体系，并开发了一种注释扩展框架，实现了自动从现有平面注释中推断和补充多维语言学属性和元数据信息，形成更丰富的注释。

Result: 注释扩展工具在土耳其语学习者语料库上的系统评估显示其面向级准确率达到95.86%，该方法生成的分类丰富语料库支持更强大的查询和细致的探索性分析，促进跨语言学和教学维度的错误研究。

Conclusion: 本文提出了一个基于多维面向分类法的半自动注释方法，系统地扩展了传统的平面注释，显著提升了注释的细粒度和解释能力；该方法在土耳其语学习语料库上获得了95.86%的准确率，并首次创建了一个协作注释的分类丰富的土耳其语学习者语料库。

Abstract: In terms of annotation structure, most learner corpora rely on holistic flat label inventories which, even when extensive, do not explicitly separate multiple linguistic dimensions. This makes linguistically deep annotation difficult and complicates fine-grained analyses aimed at understanding why and how learners produce specific errors. To address these limitations, this paper presents a semi-automated annotation methodology for learner corpora, built upon a recently proposed faceted taxonomy, and implemented through a novel annotation extension framework. The taxonomy provides a theoretically grounded, multi-dimensional categorization that captures the linguistic properties underlying each error instance, thereby enabling standardized, fine-grained, and interpretable enrichment beyond flat annotations. The annotation extension tool, implemented based on the proposed extension framework for Turkish, automatically extends existing flat annotations by inferring additional linguistic and metadata information as facets within the taxonomy to provide richer learner-specific context. It was systematically evaluated and yielded promising performance results, achieving a facet-level accuracy of 95.86%. The resulting taxonomically enriched corpus offers enhanced querying capabilities and supports detailed exploratory analyses across learner corpora, enabling researchers to investigate error patterns through complex linguistic and pedagogical dimensions. This work introduces the first collaboratively annotated and taxonomically enriched Turkish Learner Corpus, a manual annotation guideline with a refined tagset, and an annotation extender. As the first corpus designed in accordance with the recently introduced taxonomy, we expect our study to pave the way for subsequent enrichment efforts of existing error-annotated learner corpora.

</details>


### [37] [Leveraging LLMs For Turkish Skill Extraction](https://arxiv.org/abs/2601.22885)
*Ezgi Arslan İltüzer,Özgür Anıl Özlü,Vahid Farajijobehdar,Gülşen Eryiğit*

Main category: cs.CL

TL;DR: 本研究针对土耳其语低资源环境，首次构建技能提取数据集，并证明大型语言模型特别是动态few-shot提示策略，有效提升技能提取性能，为类似语言的研究提供了重要基础。


<details>
  <summary>Details</summary>
Motivation: 土耳其语作为形态复杂且资源匮乏的语言，缺乏专门的技能分类体系和技能提取数据集，导致该领域研究不足。有效的技能提取对于现代招聘系统至关重要，有助于提高职位匹配效率和劳动力市场分析。

Method: 构建土耳其语首个技能提取数据集（包含4819个标注技能片段的327个职位发布），并采用多种LLM模型及提示策略进行技能提取和技能标准化链接。采用动态few-shot提示、基于嵌入的检索和LLM的再排序实现端到端技能提取。

Result: 提出了第一套土耳其语技能提取数据集，并通过实验发现，使用Claude Sonnet 3.7结合动态few-shot提示、嵌入检索以及LLM再排序的配置，实现了0.56的端到端技能提取性能，表现突出并与其他语言的类似研究看齐。

Conclusion: 本研究表明，利用大型语言模型（LLMs）在资源有限的土耳其语环境中可以显著提升技能提取的性能，特别是在技能识别和技能标准化链接环节。通过动态few-shot提示策略，LLMs优于传统的监督序列标注方法。

Abstract: Skill extraction is a critical component of modern recruitment systems, enabling efficient job matching, personalized recommendations, and labor market analysis. Despite Türkiye's significant role in the global workforce, Turkish, a morphologically complex language, lacks both a skill taxonomy and a dedicated skill extraction dataset, resulting in underexplored research in skill extraction for Turkish. This article seeks the answers to three research questions: 1) How can skill extraction be effectively performed for this language, in light of its low resource nature? 2)~What is the most promising model? 3) What is the impact of different Large Language Models (LLMs) and prompting strategies on skill extraction (i.e., dynamic vs. static few-shot samples, varying context information, and encouraging causal reasoning)? The article introduces the first Turkish skill extraction dataset and performance evaluations of automated skill extraction using LLMs. The manually annotated dataset contains 4,819 labeled skill spans from 327 job postings across different occupation areas. The use of LLM outperforms supervised sequence labeling when used in an end-to-end pipeline, aligning extracted spans with standardized skills in the ESCO taxonomy more effectively. The best-performing configuration, utilizing Claude Sonnet 3.7 with dynamic few-shot prompting for skill identification, embedding-based retrieval, and LLM-based reranking for skill linking, achieves an end-to-end performance of 0.56, positioning Turkish alongside similar studies in other languages, which are few in the literature. Our findings suggest that LLMs can improve skill extraction performance in low-resource settings, and we hope that our work will accelerate similar research on skill extraction for underrepresented languages.

</details>


### [38] [Should LLMs, $\textit{like}$, Generate How Users Talk? Building Dialect-Accurate Dialog[ue]s Beyond the American Default with MDial](https://arxiv.org/abs/2601.22888)
*Jio Oh,Paul Vicinanza,Thomas Butler,Steven Euijong Whang,Dezhi Hong,Amani Namboori*

Main category: cs.CL

TL;DR: 针对非标准英语方言的高失败率问题，本文提出MDial框架生成多方言对话数据，构建MDialBench进行评测，揭示当前LLM在多方言识别和回应上的显著不足。


<details>
  <summary>Details</summary>
Motivation: 超过80%的16亿英语使用者不使用标准美式英语，但多方言性能尚未被充分研究，这导致非标准方言使用者与LLM交互时失败率较高和刻板反应。

Method: 引入MDial框架，通过与母语语言学家的合作，采用注解且基于规则的LLM转换方法，生成涵盖词汇、正字法和形态句法三大书面方言特征的多方言对话数据。

Result: 构建了包含9种英语方言、5万多对话和9.7万问答对的多方言平行基准MDialBench，评估17款LLM在方言识别及响应生成上的表现，结果显示方言识别准确率普遍不足，尤其加拿英语低于50%。

Conclusion: 现有大型语言模型在多种英语方言的识别和回应上表现不足，尤其是非标准美式英语方言的识别准确率低，可能导致下游任务失败。

Abstract: More than 80% of the 1.6 billion English speakers do not use Standard American English (SAE) and experience higher failure rates and stereotyped responses when interacting with LLMs as a result. Yet multi-dialectal performance remains underexplored. We introduce $\textbf{MDial}$, the first large-scale framework for generating multi-dialectal conversational data encompassing the three pillars of written dialect -- lexical (vocabulary), orthographic (spelling), and morphosyntactic (grammar) features -- for nine English dialects. Partnering with native linguists, we design an annotated and scalable rule-based LLM transformation to ensure precision. Our approach challenges the assumption that models should mirror users' morphosyntactic features, showing that up to 90% of the grammatical features of a dialect should not be reproduced by models. Independent evaluations confirm data quality, with annotators preferring MDial outputs over prior methods in 98% of pairwise comparisons for dialect naturalness. Using this pipeline, we construct the dialect-parallel $\textbf{MDialBench}$mark with 50k+ dialogs, resulting in 97k+ QA pairs, and evaluate 17 LLMs on dialect identification and response generation tasks. Even frontier models achieve under 70% accuracy, fail to reach 50% for Canadian English, and systematically misclassify non-SAE dialects as American or British. As dialect identification underpins natural language understanding, these errors risk cascading failures into downstream tasks.

</details>


### [39] [DiffuSpeech: Silent Thought, Spoken Answer via Unified Speech-Text Diffusion](https://arxiv.org/abs/2601.22889)
*Yuxuan Lou,Ziming Wu,Yaochen Wang,Yong Liu,Yingxuan Ren,Fuming Lai,Shaobing Lian,Jie Tang,Yang You*

Main category: cs.CL

TL;DR: 本文提出基于扩散的\method{}模型，通过同步生成文本推理和语音，有效提升了语音问答的准确率和生成质量，验证了思考轨迹和扩散模型的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有语音语言模型直接生成响应而缺乏显式推理，导致一旦生成音频后错误难以纠正。通过引入"Silent Thought, Spoken Answer"范式，即辅助推理路径提升语音生成质量和准确度。

Method: 采用基于扩散的语音-文本语言模型，将离散文本和标记化语音统一在单一的掩码扩散框架下，利用非自回归的迭代去噪方法，同时生成推理路径和语音令牌，辅以特定的掩码调度。

Result: 实验表明，\method{}在语音问答准确率上领先当前最好基线9个百分点，文本到语音生成质量最佳，错误率为6.2%，并保持了语言理解能力，MMLU得分66.2%。

Conclusion: 本文提出的\method{}模型通过扩散机制联合生成文本推理和语音响应，实现了显著提升的语音问答准确率和语音质量，验证了思考路径和扩散架构的重要性。

Abstract: Current speech language models generate responses directly without explicit reasoning, leading to errors that cannot be corrected once audio is produced. We introduce \textbf{``Silent Thought, Spoken Answer''} -- a paradigm where speech LLMs generate internal text reasoning alongside spoken responses, with thinking traces informing speech quality. To realize this, we present \method{}, the first diffusion-based speech-text language model supporting both understanding and generation, unifying discrete text and tokenized speech under a single masked diffusion framework. Unlike autoregressive approaches, \method{} jointly generates reasoning traces and speech tokens through iterative denoising, with modality-specific masking schedules. We also construct \dataset{}, the first speech QA dataset with paired text reasoning traces, containing 26K samples totaling 319 hours. Experiments show \method{} achieves state-of-the-art speech-to-speech QA accuracy, outperforming the best baseline by up to 9 points, while attaining the best TTS quality among generative models (6.2\% WER) and preserving language understanding (66.2\% MMLU). Ablations confirm that both the diffusion architecture and thinking traces contribute to these gains.

</details>


### [40] [LLMs Explain't: A Post-Mortem on Semantic Interpretability in Transformer Models](https://arxiv.org/abs/2601.22928)
*Alhassan Abdelhalim,Janick Edinger,Sören Laue,Michaela Regneri*

Main category: cs.CL

TL;DR: 本文评估了主流大型语言模型解释技术，发现注意力和嵌入特征解释均存在方法学缺陷，之前关于模型理解能力的结论值得怀疑，对依赖解释方法的系统设计和调试提出警示。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型广泛应用且性能卓越，但其内在工作的具体机制尚不明确，本文旨在探索LLMs中语言抽象的产生过程，并评估不同解释方法的有效性。

Method: 本文采用了两种文献中验证较多的方法：一是探测令牌级别的关系结构；二是利用嵌入向量作为人类可解释属性的载体，进行特征映射分析。

Result: 研究发现基于注意力头的解释方法因后期层表示不再对应具体令牌而崩溃，基于嵌入向量的属性推断方法高准确率主要由方法学伪影及数据集结构驱动，并不反映真实语义知识。

Conclusion: 现有解释大型语言模型（LLMs）性能的主流方法存在严重局限，无法有效揭示模型中语言抽象的生成机制，之前基于注意力机制和特征映射的解释方法所得结论缺乏可信度。

Abstract: Large Language Models (LLMs) are becoming increasingly popular in pervasive computing due to their versatility and strong performance. However, despite their ubiquitous use, the exact mechanisms underlying their outstanding performance remain unclear. Different methods for LLM explainability exist, and many are, as a method, not fully understood themselves. We started with the question of how linguistic abstraction emerges in LLMs, aiming to detect it across different LLM modules (attention heads and input embeddings). For this, we used methods well-established in the literature: (1) probing for token-level relational structures, and (2) feature-mapping using embeddings as carriers of human-interpretable properties.
  Both attempts failed for different methodological reasons: Attention-based explanations collapsed once we tested the core assumption that later-layer representations still correspond to tokens. Property-inference methods applied to embeddings also failed because their high predictive scores were driven by methodological artifacts and dataset structure rather than meaningful semantic knowledge. These failures matter because both techniques are widely treated as evidence for what LLMs supposedly understand, yet our results show such conclusions are unwarranted. These limitations are particularly relevant in pervasive and distributed computing settings where LLMs are deployed as system components and interpretability methods are relied upon for debugging, compression, and explaining models.

</details>


### [41] [Benchmarking Machine Translation on Chinese Social Media Texts](https://arxiv.org/abs/2601.22931)
*Kaiyan Zhao,Zheyong Xie,Zhongtao Miao,Xinze Lyu,Yao Hu,Shaosheng Cao*

Main category: cs.CL

TL;DR: 本文针对中文社交媒体中丰富的俚语和风格表达，提出CSM-MTBench基准及定制评价方法，系统评估现有机器翻译模型表现，推动更适应真实网络语言的机器翻译发展。


<details>
  <summary>Details</summary>
Motivation: 中文社交媒体中快速变化的俚语、新词及高度风格化表达，给机器翻译基准测试带来数据稀缺和评价指标不足两大挑战，传统指标难以准确衡量风格和非标准表达的翻译质量。

Method: 本文提出了CSM-MTBench基准，包括Fun Posts和Social Snippets两个子集，分别针对俚语、新词的翻译成功率和情感风格保持进行评估，采用基于嵌入的指标结合大型语言模型判别方法进行评价。

Result: 通过对20余个模型的实验，发现当前机器翻译系统在语义忠实度和社交媒体特有风格表达的处理上存在显著差异，验证了CSM-MTBench对测试并推动真实场景下机器翻译能力的重要性。

Conclusion: CSM-MTBench作为一个覆盖中外五种语言方向，并包含两种专家策划子集的基准测试，能够有效评估机器翻译系统在处理中文社交媒体中的俚语、新词及风格表达方面的表现，推动MT系统在真实世界社交媒体文本上的进步。

Abstract: The prevalence of rapidly evolving slang, neologisms, and highly stylized expressions in informal user-generated text, particularly on Chinese social media, poses significant challenges for Machine Translation (MT) benchmarking. Specifically, we identify two primary obstacles: (1) data scarcity, as high-quality parallel data requires bilingual annotators familiar with platform-specific slang, and stylistic cues in both languages; and (2) metric limitations, where traditional evaluators like COMET often fail to capture stylistic fidelity and nonstandard expressions. To bridge these gaps, we introduce CSM-MTBench, a benchmark covering five Chinese-foreign language directions and consisting of two expert-curated subsets: Fun Posts, featuring context-rich, slang- and neologism-heavy content, and Social Snippets, emphasizing concise, emotion- and style- driven expressions. Furthermore, we propose tailored evaluation approaches for each subset: measuring the translation success rate of slang and neologisms in Fun Posts, while assessing tone and style preservation in Social Snippets via a hybrid of embedding-based metrics and LLM-as-a-judge. Experiments on over 20 models reveal substantial variation in how current MT systems handle semantic fidelity and informal, social-media-specific stylistic cues. CSM-MTBench thus serves as a rigorous testbed for advancing MT systems capable of mastering real-world Chinese social media texts.

</details>


### [42] [Relaxing Positional Alignment in Masked Diffusion Language Models](https://arxiv.org/abs/2601.22947)
*Mengyu Ye,Ryosuke Takahashi,Keito Kudo,Jun Suzuki*

Main category: cs.CL

TL;DR: 本文通过引入位置灵活的监督机制，改善了掩码扩散语言模型在开放式文本生成中的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 严格的位置预测导致MDLM在解码时对令牌错位高度敏感，严重影响生成语义，训练中的严格位置监督与不可逆去噪解码动态不匹配。

Method: 引入了一个特殊的<slack>标记，并采用连接时序分类（CTC）目标函数进行位置灵活的监督微调。

Result: 所提出方法在五个开放文本生成基准测试上均优于原始模型，且对位置偏移更具鲁棒性。

Conclusion: 放宽了MDLMs中严格位置监督，可以显著提升开放式文本生成的质量和模型对位置偏移的鲁棒性。

Abstract: Masked diffusion language models (MDLMs) have emerged as a promising alternative to dominant autoregressive approaches. Although they achieve competitive performance on several tasks, a substantial gap remains in open-ended text generation. We hypothesize that one cause of this gap is that strict positional prediction makes MDLM decoding highly sensitive to token misalignment, and we show through controlled interventions that a one-position shift can severely disrupt semantics. This observation suggests that enforcing strict positional supervision during training is misaligned with the irreversible denoising dynamics of MDLM decoding. Motivated by this mismatch, we adopt an alignment-flexible supervision strategy during fine-tuning. Specifically, we introduce a special token <slack> via the connectionist temporal classification objective. We apply this approach to the widely used MDLM model and conduct experiments on five open-ended text generation benchmarks. Our method consistently outperforms the original model and improves robustness to positional shifts, indicating that relaxing strict positional supervision is an important factor in improving generation quality in MDLMs.

</details>


### [43] [Autonomous Chain-of-Thought Distillation for Graph-Based Fraud Detection](https://arxiv.org/abs/2601.22949)
*Yuan Li,Jun Hu,Bryan Hooi,Bingsheng He,Cheng Chen*

Main category: cs.CL

TL;DR: 提出FraudCoT框架，通过自治图感知推理链和高效联合训练，解决了现有方法推理受限和训练成本高的问题，在欺诈检测上性能和效率大幅提升。


<details>
  <summary>Details</summary>
Motivation: 现有LLM增强的GNN方法受限于预定义提示和解耦训练，限制了推理自主性和语义结构对齐，影响欺诈检测效果。

Method: 提出了基于图的自治推理链（CoT）和可扩展的LLM-GNN联合训练框架，通过欺诈感知的选择性CoT蒸馏机制丰富节点文本，结合非对称联合训练实现高效端到端优化。

Result: 在多个公共和工业基准上，FraudCoT实现了最高8.8%的AUPRC提升和最高1066倍的训练吞吐量加速，显著提升检测性能和效率。

Conclusion: FraudCoT显著提升了基于文本属性图的欺诈检测的性能和效率，超越了现有方法。

Abstract: Graph-based fraud detection on text-attributed graphs (TAGs) requires jointly modeling rich textual semantics and relational dependencies. However, existing LLM-enhanced GNN approaches are constrained by predefined prompting and decoupled training pipelines, limiting reasoning autonomy and weakening semantic-structural alignment. We propose FraudCoT, a unified framework that advances TAG-based fraud detection through autonomous, graph-aware chain-of-thought (CoT) reasoning and scalable LLM-GNN co-training. To address the limitations of predefined prompts, we introduce a fraud-aware selective CoT distillation mechanism that generates diverse reasoning paths and enhances semantic-structural understanding. These distilled CoTs are integrated into node texts, providing GNNs with enriched, multi-hop semantic and structural cues for fraud detection. Furthermore, we develop an efficient asymmetric co-training strategy that enables end-to-end optimization while significantly reducing the computational cost of naive joint training. Extensive experiments on public and industrial benchmarks demonstrate that FraudCoT achieves up to 8.8% AUPRC improvement over state-of-the-art methods and delivers up to 1,066x speedup in training throughput, substantially advancing both detection performance and efficiency.

</details>


### [44] [Residual Context Diffusion Language Models](https://arxiv.org/abs/2601.22954)
*Yuezhou Hu,Harman Singh,Monishwaran Maheswaran,Haocheng Xi,Coleman Hooper,Jintao Zhang,Aditya Tomar,Michael W. Mahoney,Sewon Min,Mehrdad Farajtabar,Kurt Keutzer,Amir Gholami,Chenfeng Xu*

Main category: cs.CL

TL;DR: 本文提出RCD模块，通过利用被丢弃令牌的上下文信息，提高扩散大语言模型的解码准确率和效率，在多项任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的基于区块的扩散大语言模型通过“重新掩码”机制只解码最高置信度令牌，丢弃其余令牌，导致计算资源浪费和上下文信息流失。回收丢弃令牌的上下文信息有助于提升解码效果和计算效率。

Method: RCD模块将被丢弃令牌的表示转换为上下文残差信息，并注入到下一次去噪步骤。采用两阶段训练流程解决反向传播的内存瓶颈问题，实现高效训练和推理。

Result: RCD在多个推理任务上均提升了模型准确率5-10个百分点，特别是在AIME任务中几乎将基线准确率翻倍，并且在等效准确率下减少4-5倍的去噪步骤。该方法通过约10亿令牌的训练数据实现了高效模型转换。

Conclusion: Residual Context Diffusion (RCD)模块通过回收被丢弃的令牌的上下文信息，显著提升了扩散大语言模型（dLLMs）的解码效率和准确率。RCD能够使模型在保持较少计算开销的同时，提高5-10个百分点的准确率，尤其在复杂任务中表现出显著优势。

Abstract: Diffusion Large Language Models (dLLMs) have emerged as a promising alternative to purely autoregressive language models because they can decode multiple tokens in parallel. However, state-of-the-art block-wise dLLMs rely on a "remasking" mechanism that decodes only the most confident tokens and discards the rest, effectively wasting computation. We demonstrate that recycling computation from the discarded tokens is beneficial, as these tokens retain contextual information useful for subsequent decoding iterations. In light of this, we propose Residual Context Diffusion (RCD), a module that converts these discarded token representations into contextual residuals and injects them back for the next denoising step. RCD uses a decoupled two-stage training pipeline to bypass the memory bottlenecks associated with backpropagation. We validate our method on both long CoT reasoning (SDAR) and short CoT instruction following (LLaDA) models. We demonstrate that a standard dLLM can be efficiently converted to the RCD paradigm with merely ~1 billion tokens. RCD consistently improves frontier dLLMs by 5-10 points in accuracy with minimal extra computation overhead across a wide range of benchmarks. Notably, on the most challenging AIME tasks, RCD nearly doubles baseline accuracy and attains up to 4-5x fewer denoising steps at equivalent accuracy levels.

</details>


### [45] [A Unified View of Attention and Residual Sinks: Outlier-Driven Rescaling is Essential for Transformer Training](https://arxiv.org/abs/2601.22966)
*Zihan Qiu,Zeyu Huang,Kaiyue Wen,Peng Jin,Bo Zheng,Yuxin Zhou,Haofeng Huang,Zekun Wang,Xiao Li,Huaqing Zhang,Yang Xu,Haoran Lian,Siqi Zhang,Rui Men,Jianwei Zhang,Ivan Titov,Dayiheng Liu,Jingren Zhou,Junyang Lin*

Main category: cs.CL

TL;DR: 本文揭示了大型语言模型中注意力和残差异类通过与归一化机制共同驱动重标定，促进训练稳定和性能，同时提供了缓解异类影响以改进训练和量化性能的方法。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型中出现的异类特征的功能角色，明确它们在模型训练稳定性和性能上的作用机理，以及如何通过设计改进训练表现和量化鲁棒性。

Method: 本文通过分析大型语言模型中注意力汇点和残差汇点的功能作用，假设并验证异类与归一化机制共同驱动重标定的现象，通过实验在不同模型结构和训练数据规模下验证该现象和相关机制。

Result: （1）移除归一化会消除异类但损害训练稳定性和性能；直接裁剪异类同样导致性能下降，证明异类驱动的重标定机制重要。（2）异类更多是作为重标定因子，而非贡献者，其最终贡献小于非异类。（3）通过吸收异类进可学习参数或使用显式门控重标定，可提升训练表现约2分，量化鲁棒性提升明显。

Conclusion: 论文结论是异类(outliers)主要通过与归一化机制共同作用来实现对非异类部分的有效重标定，这种重标定对训练稳定性和性能提升起关键作用，并且可以通过显式门控重标定机制进行吸收或缓解，从而改善训练表现和量化鲁棒性。

Abstract: We investigate the functional role of emergent outliers in large language models, specifically attention sinks (a few tokens that consistently receive large attention logits) and residual sinks (a few fixed dimensions with persistently large activations across most tokens). We hypothesize that these outliers, in conjunction with the corresponding normalizations (\textit{e.g.}, softmax attention and RMSNorm), effectively rescale other non-outlier components. We term this phenomenon \textit{outlier-driven rescaling} and validate this hypothesis across different model architectures and training token counts. This view unifies the origin and mitigation of both sink types. Our main conclusions and observations include: (1) Outliers function jointly with normalization: removing normalization eliminates the corresponding outliers but degrades training stability and performance; directly clipping outliers while retaining normalization leads to degradation, indicating that outlier-driven rescaling contributes to training stability. (2) Outliers serve more as rescale factors rather than contributors, as the final contributions of attention and residual sinks are significantly smaller than those of non-outliers. (3) Outliers can be absorbed into learnable parameters or mitigated via explicit gated rescaling, leading to improved training performance (average gain of 2 points) and enhanced quantization robustness (1.2 points degradation under W4A4 quantization).

</details>


### [46] [ArabicDialectHub: A Cross-Dialectal Arabic Learning Resource and Platform](https://arxiv.org/abs/2601.22987)
*Salem Lahlou*

Main category: cs.CL

TL;DR: 本文提出了ArabicDialectHub，一个涵盖六种阿拉伯方言短语和互动学习工具的开放学习平台，利用大型语言模型生成短语并验证，支持个性化测验和进度追踪，资源和代码均开源。


<details>
  <summary>Details</summary>
Motivation: 针对阿拉伯语多方言学习资源匮乏的问题，旨在提供一个涵盖多种方言、方便学习者探索和练习的综合性开放学习平台。

Method: 作者利用大型语言模型生成短语，经过五位母语者验证，按难度分层并主题化组织短语，开发了一个具备算法生成干扰项的自适应测验及云端进度同步功能的网页平台。

Result: 构建了一个包含552条短语，覆盖摩洛哥达里贾、黎巴嫩、叙利亚、阿联酋、沙特及现代标准阿拉伯语六种方言的数据库和交互式学习平台，平台已开源并上线。

Conclusion: 本文介绍了ArabicDialectHub，这是一个涵盖六种阿拉伯方言的跨方言阿拉伯语学习资源，提供552个短语及互动平台，支持翻译探索、自适应测验、进度同步和文化背景学习，并且开放源代码。

Abstract: We present ArabicDialectHub, a cross-dialectal Arabic learning resource comprising 552 phrases across six varieties (Moroccan Darija, Lebanese, Syrian, Emirati, Saudi, and MSA) and an interactive web platform. Phrases were generated using LLMs and validated by five native speakers, stratified by difficulty, and organized thematically. The open-source platform provides translation exploration, adaptive quizzing with algorithmic distractor generation, cloud-synchronized progress tracking, and cultural context. Both the dataset and complete platform source code are released under MIT license. Platform: https://arabic-dialect-hub.netlify.app.

</details>


### [47] [Bias Beyond Borders: Political Ideology Evaluation and Steering in Multilingual LLMs](https://arxiv.org/abs/2601.23001)
*Afrozah Nadeem,Agrima,Mehwish Nasim,Usman Naseem*

Main category: cs.CL

TL;DR: 本文针对多语言大语言模型中的政治偏见问题，提出了一种跨语言对齐引导的后期缓解框架，实现了偏见的显著减少和跨语言一致性，兼顾了意识形态中立与语言文化多样性。


<details>
  <summary>Details</summary>
Motivation: 现有研究多集中在高资源的西方语言或狭义的多语言环境，对于多语言一致性和安全的后期缓解方法研究不足。

Method: 设计了跨语言对齐引导（CLAS）后期缓解框架，通过将政治提示引发的潜在意识形态表示对齐到共享的意识形态子空间，并动态调节干预强度以防止过度纠正。

Result: 在涵盖50个国家和33种语言的大规模多语言政治偏见评估中，CLAS方法在经济和社会轴线上显著减少偏见，且响应质量下降极小。

Conclusion: 提出的跨语言对齐引导（CLAS）框架有效减少了多语言大语言模型中的政治偏见，同时保持了响应质量。

Abstract: Large Language Models (LLMs) increasingly shape global discourse, making fairness and ideological neutrality essential for responsible AI deployment. Despite growing attention to political bias in LLMs, prior work largely focuses on high-resource, Western languages or narrow multilingual settings, leaving cross-lingual consistency and safe post-hoc mitigation underexplored. To address this gap, we present a large-scale multilingual evaluation of political bias spanning 50 countries and 33 languages. We introduce a complementary post-hoc mitigation framework, Cross-Lingual Alignment Steering (CLAS), designed to augment existing steering methods by aligning ideological representations across languages and dynamically regulating intervention strength. This method aligns latent ideological representations induced by political prompts into a shared ideological subspace, ensuring cross lingual consistency, with the adaptive mechanism prevents over correction and preserves coherence. Experiments demonstrate substantial bias reduction along both economic and social axes with minimal degradation in response quality. The proposed framework establishes a scalable and interpretable paradigm for fairness-aware multilingual LLM governance, balancing ideological neutrality with linguistic and cultural diversity.

</details>


### [48] [InstructDiff: Domain-Adaptive Data Selection via Differential Entropy for Efficient LLM Fine-Tuning](https://arxiv.org/abs/2601.23006)
*Junyou Su,He Zhu,Xiao Luo,Liyu Zhang,Hong-Yu Zhou,Yun Chen,Peng Li,Yang Liu,Guanhua Chen*

Main category: cs.CL

TL;DR: 提出InstructDiff，通过领域自适应的差分熵策略高效选取训练数据，提升大模型指令微调效果，节省训练数据和成本。


<details>
  <summary>Details</summary>
Motivation: SFT在大语言模型适应中至关重要，但完整数据训练成本高且收益递减，且现有数据选择方法缺乏领域适应性，分别针对一般指令或推理任务效果有限。

Method: 提出InstructDiff框架，利用基线模型与最小指令调优校准模型间的熵差，通过热启动校准、双向负对数似然（NLL）过滤和基于熵的排序，实现差分熵的领域自适应数据选择。

Result: 在数学推理任务上，相比完整数据训练提升17%，在一般指令遵循任务上提升52%，且只使用了完整数据的10%，显著优于现有基线方法。

Conclusion: InstructDiff通过差分熵作为领域自适应的数据选择标准，在数学推理和一般指令遵循任务中实现了显著性能提升，且相比使用完整数据训练，大幅减少了数据量需求。

Abstract: Supervised fine-tuning (SFT) is fundamental to adapting large language models, yet training on complete datasets incurs prohibitive costs with diminishing returns. Existing data selection methods suffer from severe domain specificity: techniques optimized for general instruction-following fail on reasoning tasks, and vice versa. We observe that measuring entropy differences between base models and minimally instruction-tuned calibrated models reveals a pattern -- samples with the lowest differential entropy consistently yield optimal performance across domains, yet this principle manifests domain-adaptively: reasoning tasks favor entropy increase (cognitive expansion), while general tasks favor entropy decrease (cognitive compression). We introduce InstructDiff, a unified framework that operationalizes differential entropy as a domain-adaptive selection criterion through warmup calibration, bi-directional NLL filtering, and entropy-based ranking. Extensive experiments show that InstructDiff achieves 17\% relative improvement over full data training on mathematical reasoning and 52\% for general instruction-following, outperforming prior baselines while using only 10\% of the data.

</details>


### [49] [DimABSA: Building Multilingual and Multidomain Datasets for Dimensional Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2601.23022)
*Lung-Hao Lee,Liang-Chih Yu,Natalia Loukashevich,Ilseyar Alimova,Alexander Panchenko,Tzu-Mi Lin,Zhe-Yu Xu,Jian-Yu Zhou,Guangmin Zheng,Jin Wang,Sharanya Awasthi,Jonas Becker,Jan Philip Wahle,Terry Ruas,Shamsuddeen Hassan Muhammad,Saif M. Mohammed*

Main category: cs.CL

TL;DR: 本论文提出了DimABSA，首个多语种维度化ABSA数据集，结合传统ABSA元素与连续情感维度，设计三种子任务并提出连续F1评价指标，通过大型语言模型基准测试展现其挑战性和应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现有ABSA研究仅依赖粗粒度的情感类别标签，限制了捕捉细腻情感状态的能力。为实现更细粒度的情感分析，引入维度化的情感表示方法，提升ABSA的表现力和适用范围。

Method: 采用连续的情感维度（情感愉悦度和唤醒度）代替传统的离散情感类别，构建涵盖6种语言和4个领域的多语种ABSA资源DimABSA，并设计三种子任务融合传统ABSA元素与情感维度。提出包含VA预测误差的连续F1指标来评价模型性能，利用提示式和微调的大型语言模型进行基准测试。

Result: 构建了包含76,958个方面实例、跨6种语言和4个领域的多语种维度化ABSA数据集DimABSA。基于该数据集设立了三种结合传统和维度情感分析的子任务，并提出了新的评价指标cF1。实验结果表明，该任务具有挑战性，为多语种维度化ABSA的研究提供了坚实基础。

Conclusion: 本论文提出了DimABSA，这是首个多语种的维度化ABSA资源，整合了传统ABSA元素和连续的情感维度评分，填补了细粒度情感分析的空白。通过提出新的统一评价指标cF1以及综合基准测试，验证了该资源的挑战性和应用价值。

Abstract: Aspect-Based Sentiment Analysis (ABSA) focuses on extracting sentiment at a fine-grained aspect level and has been widely applied across real-world domains. However, existing ABSA research relies on coarse-grained categorical labels (e.g., positive, negative), which limits its ability to capture nuanced affective states. To address this limitation, we adopt a dimensional approach that represents sentiment with continuous valence-arousal (VA) scores, enabling fine-grained analysis at both the aspect and sentiment levels. To this end, we introduce DimABSA, the first multilingual, dimensional ABSA resource annotated with both traditional ABSA elements (aspect terms, aspect categories, and opinion terms) and newly introduced VA scores. This resource contains 76,958 aspect instances across 42,590 sentences, spanning six languages and four domains. We further introduce three subtasks that combine VA scores with different ABSA elements, providing a bridge from traditional ABSA to dimensional ABSA. Given that these subtasks involve both categorical and continuous outputs, we propose a new unified metric, continuous F1 (cF1), which incorporates VA prediction error into standard F1. We provide a comprehensive benchmark using both prompted and fine-tuned large language models across all subtasks. Our results show that DimABSA is a challenging benchmark and provides a foundation for advancing multilingual dimensional ABSA.

</details>


### [50] [Character as a Latent Variable in Large Language Models: A Mechanistic Account of Emergent Misalignment and Conditional Safety Failures](https://arxiv.org/abs/2601.23081)
*Yanghao Su,Wenbo Zhou,Tianwei Zhang,Qiu Han,Weiming Zhang,Nenghai Yu,Jie Zhang*

Main category: cs.CL

TL;DR: 研究发现，细粒度行为特质的微调导致Emergent Misalignment，这种对齐错误源于行为的稳定转变而非能力下降，强调了行为特质在模型对齐中的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有解释主要归因于错误或不安全内容的泛化，但这种观点不完整，需要深入理解Emergent Misalignment的真正成因。

Method: 在多个领域和模型家族中，通过细粒度分析微调数据中具体字符级特质对模型行为的影响，比较了不同类型微调的效果，并测试了训练时和推理时激活行为特质的方法。

Result: 发现特定字符级行为倾向的微调导致更强且更具可迁移性的错误对齐，而模型的整体能力基本保持，且该行为特质可通过训练时触发器和推理时的角色提示条件激活。

Conclusion: Emergent Misalignment是由模型行为的稳定转变引起，而不是能力退化或知识污染，表明行为特质形成是一个重要的未充分探索的对齐风险。

Abstract: Emergent Misalignment refers to a failure mode in which fine-tuning large language models (LLMs) on narrowly scoped data induces broadly misaligned behavior. Prior explanations mainly attribute this phenomenon to the generalization of erroneous or unsafe content. In this work, we show that this view is incomplete. Across multiple domains and model families, we find that fine-tuning models on data exhibiting specific character-level dispositions induces substantially stronger and more transferable misalignment than incorrect-advice fine-tuning, while largely preserving general capabilities. This indicates that emergent misalignment arises from stable shifts in model behavior rather than from capability degradation or corrupted knowledge. We further show that such behavioral dispositions can be conditionally activated by both training-time triggers and inference-time persona-aligned prompts, revealing shared structure across emergent misalignment, backdoor activation, and jailbreak susceptibility. Overall, our results identify character formation as a central and underexplored alignment risk, suggesting that robust alignment must address behavioral dispositions rather than isolated errors or prompt-level defenses.

</details>


### [51] [Safer Policy Compliance with Dynamic Epistemic Fallback](https://arxiv.org/abs/2601.23094)
*Joseph Marvin Imperial,Harish Tayyar Madabushi*

Main category: cs.CL

TL;DR: 本文提出了一种受人类认知机制启发的动态安全协议DEF，显著增强了大型语言模型在面对恶意篡改政策文本时的防御能力和检测准确率。


<details>
  <summary>Details</summary>
Motivation: 受到人类认知防御机制——认知警觉性的启发，旨在为大型语言模型在高风险任务（如自动化检测数据隐私合规）中提供更加安全可靠的防护措施。

Method: 通过引入动态认知退避（DEF）协议，在推理阶段利用不同层次的一句文本提示，诱导大型语言模型标记不一致、拒绝遵从并回退至其内在知识库，以应对被恶意篡改的政策文本。

Result: 在全球公认的法律政策如HIPAA和GDPR的测试中，DEF有效提升了领先大型语言模型检测和拒绝篡改政策的能力，其中DeepSeek-R1在某一测试环境下实现了100%的检测率。

Conclusion: 本文提出的动态认知退避（DEF）机制显著提升了大型语言模型在面对恶意篡改政策文本时的防御能力，能有效识别并拒绝不一致和篡改的政策文本。

Abstract: Humans develop a series of cognitive defenses, known as epistemic vigilance, to combat risks of deception and misinformation from everyday interactions. Developing safeguards for LLMs inspired by this mechanism might be particularly helpful for their application in high-stakes tasks such as automating compliance with data privacy laws. In this paper, we introduce Dynamic Epistemic Fallback (DEF), a dynamic safety protocol for improving an LLM's inference-time defenses against deceptive attacks that make use of maliciously perturbed policy texts. Through various levels of one-sentence textual cues, DEF nudges LLMs to flag inconsistencies, refuse compliance, and fallback to their parametric knowledge upon encountering perturbed policy texts. Using globally recognized legal policies such as HIPAA and GDPR, our empirical evaluations report that DEF effectively improves the capability of frontier LLMs to detect and refuse perturbed versions of policies, with DeepSeek-R1 achieving a 100% detection rate in one setting. This work encourages further efforts to develop cognitively inspired defenses to improve LLM robustness against forms of harm and deception that exploit legal artifacts.

</details>


### [52] [Evaluating the Utility of Grounding Documents with Reference-Free LLM-based Metrics](https://arxiv.org/abs/2601.23129)
*Yilun Hua,Giuseppe Castellucci,Peter Schulam,Heba Elfardy,Kevin Small*

Main category: cs.CL

TL;DR: 本文提出GroGU，一种基于生成置信度熵、无标注、模型相关的文档效用度量，用于训练查询重写器，显著提升了RAG的检索和生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有效用度量忽视模型特性或依赖昂贵标注，缺乏统一且高效的参考。

Method: 本文提出GroGU度量标准，基于下游大语言模型生成的置信度熵定义文档效用，无需人工标注，同时训练查询重写器优化RAG性能。

Result: GroGU无需标注即可准确评估文档效用，训练的查询重写器在排序准确率和回答准确率上分别提升18.2和9.4点。

Conclusion: GroGU能够有效区分高效用的文档，并提升RAG中查询重写器的性能，实现显著的排序和回答准确率提升。

Abstract: Retrieval Augmented Generation (RAG)'s success depends on the utility the LLM derives from the content used for grounding. Quantifying content utility does not have a definitive specification and existing metrics ignore model-specific capabilities and/or rely on costly annotations. In this paper, we propose Grounding Generation Utility (GroGU), a model-specific and reference-free metric that defines utility as a function of the downstream LLM's generation confidence based on entropy. Despite having no annotation requirements, GroGU is largely faithful in distinguishing ground-truth documents while capturing nuances ignored by LLM-agnostic metrics. We apply GroGU to train a query-rewriter for RAG by identifying high-utility preference data for Direct Preference Optimization. Experiments show improvements by up to 18.2 points in Mean Reciprocal Rank and up to 9.4 points in answer accuracy.

</details>


### [53] [Monotonic Reference-Free Refinement for Autoformalization](https://arxiv.org/abs/2601.23166)
*Lan Zhang,Marco Valentino,André Freitas*

Main category: cs.CL

TL;DR: 该论文提出了一种结合定理证明器和大语言模型反馈的迭代方法，有效提升全定理自动形式化的多维质量指标，验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有的语句自动形式化方法虽快速发展，但全定理自动形式化尚未充分研究，且现有迭代方法难以同时优化多维度的形式化质量。

Method: 设计了利用定理证明器和大语言模型作为多角色反馈的迭代单调优化方法，通过屏蔽复合目标函数优化形式有效性、逻辑保持性、数学一致性和形式质量，并提出保证单调提升的接受策略。

Result: 在miniF2F数据集上达到了93.44%的形式有效性和78.22%的综合得分，在ProofNet数据集上取得了44.09%的形式有效性和29.79%的综合得分。

Conclusion: 该论文提出的参考无关迭代单调过程能在多维度上同时提升全定理自动形式化的性能，保证单调提升并收敛。

Abstract: While statement autoformalization has advanced rapidly, full-theorem autoformalization remains largely unexplored. Existing iterative refinement methods in statement autoformalization typicall improve isolated aspects of formalization, such as syntactic correctness, but struggle to jointly optimizing multiple quality dimensions, which is critical for full-theorem autoformalization. We introduce a reference-free iterative monotonic process for full-theorem autoformalization that leverages complementary feedback from theorem provers and LLM-based judges, without access to ground-truth proofs or existing formalizations at inference time. Our approach optimizes a masked composite objective over Formal Validity, Logical Preservation, Mathematical Consistency, and Formal Quality, guided by a responsiveness map that indicates how different LLMs acting as different roles preferentially improve each dimension. We further propose an acceptance policy that guarantees certified monotonic improvement, and provide conditions ensuring convergence and termination. Empirical experiments demonstrate the proposed process enables simultaneous improvement across multiple dimensions, achieving 93.44% formal validity and a 78.22% overall score on miniF2F, and 44.09% formal validity and a 29.79% overall score on ProofNet.

</details>


### [54] [FourierSampler: Unlocking Non-Autoregressive Potential in Diffusion Language Models via Frequency-Guided Generation](https://arxiv.org/abs/2601.23182)
*Siyang He,Qiqi Wang,Xiaoran Liu,Hongnan Ma,Yiwei Shi,Yuerong Song,Ying Zhu,Tianyi Liang,Zengfeng Huang,Ziwei He,Xipeng Qiu*

Main category: cs.CL

TL;DR: 本文通过频域分析发现扩散语言模型隐藏状态的低频编码全局信息，高频编码局部细节，提出FourierSampler结构到细节的生成策略，显著提升生成质量，优于现有方法和自回归模型。


<details>
  <summary>Details</summary>
Motivation: 现有扩散语言模型的解码策略存在位置偏差，不能充分发挥任意生成的潜力，需探索其内在频谱特性以改进生成策略。

Method: 文章利用频域分析揭示dLLMs隐藏状态中低频分量编码全局结构信息和长程依赖，高频分量编码局部细节，基于此提出频域滑动窗口机制实现从结构到细节的生成。

Result: FourierSampler在LLADA和SDAR数据集上分别带来了20.4%及16.0%的相对提升，并超越了同尺寸的Llama3.1-8B-Instruct自回归模型。

Conclusion: FourierSampler通过频域滑动窗口机制，显著提高了diffusion语言模型的生成效果，超过了其他推理增强策略和同等规模的自回归模型。

Abstract: Despite the non-autoregressive potential of diffusion language models (dLLMs), existing decoding strategies demonstrate positional bias, failing to fully unlock the potential of arbitrary generation. In this work, we delve into the inherent spectral characteristics of dLLMs and present the first frequency-domain analysis showing that low-frequency components in hidden states primarily encode global structural information and long-range dependencies, while high-frequency components are responsible for characterizing local details. Based on this observation, we propose FourierSampler, which leverages a frequency-domain sliding window mechanism to dynamically guide the model to achieve a "structure-to-detail" generation. FourierSampler outperforms other inference enhancement strategies on LLADA and SDAR, achieving relative improvements of 20.4% on LLaDA1.5-8B and 16.0% on LLaDA-8B-Instruct. It notably surpasses similarly sized autoregressive models like Llama3.1-8B-Instruct.

</details>


### [55] [JobResQA: A Benchmark for LLM Machine Reading Comprehension on Multilingual Résumés and JDs](https://arxiv.org/abs/2601.23183)
*Casimiro Pio Carrino,Paula Estrella,Rabih Zbib,Carlos Escolano,José A. R. Fonollosa*

Main category: cs.CL

TL;DR: 提出了JobResQA，一个覆盖五种语言、针对HR领域简历与职位描述的多语言机器阅读理解问答基准，结合隐私保护和公平性设计，揭示当前大模型多语言能力差异，推动公平可靠的HR智能系统发展。


<details>
  <summary>Details</summary>
Motivation: 现有多语言机器阅读理解在HR任务上的能力不足，缺乏专门针对简历和职位描述的多语言问答基准数据集，同时需要保障数据真实性、隐私和公平性。

Method: 提出一个基于真实数据去识别化和合成的数据生成流程，结合TEaR方法的人类辅助高质量多语种翻译管道，建立包含三种复杂度问题的多语言QA数据集。

Result: 建立了由581个问答对构成的多语言数据集，评测了多个开源大模型，发现语言差异显著影响模型表现，验证了基准的实用性和挑战性。

Conclusion: JobResQA展示了多语言机器阅读理解在HR领域的应用潜力，尤其在简历和职位描述相关任务中，突出英文和西班牙语表现较好，但其他语言存在明显不足。

Abstract: We introduce JobResQA, a multilingual Question Answering benchmark for evaluating Machine Reading Comprehension (MRC) capabilities of LLMs on HR-specific tasks involving résumés and job descriptions. The dataset comprises 581 QA pairs across 105 synthetic résumé-job description pairs in five languages (English, Spanish, Italian, German, and Chinese), with questions spanning three complexity levels from basic factual extraction to complex cross-document reasoning. We propose a data generation pipeline derived from real-world sources through de-identification and data synthesis to ensure both realism and privacy, while controlled demographic and professional attributes (implemented via placeholders) enable systematic bias and fairness studies. We also present a cost-effective, human-in-the-loop translation pipeline based on the TEaR methodology, incorporating MQM error annotations and selective post-editing to ensure an high-quality multi-way parallel benchmark. We provide a baseline evaluations across multiple open-weight LLM families using an LLM-as-judge approach revealing higher performances on English and Spanish but substantial degradation for other languages, highlighting critical gaps in multilingual MRC capabilities for HR applications. JobResQA provides a reproducible benchmark for advancing fair and reliable LLM-based HR systems. The benchmark is publicly available at: https://github.com/Avature/jobresqa-benchmark

</details>


### [56] [ReGuLaR: Variational Latent Reasoning Guided by Rendered Chain-of-Thought](https://arxiv.org/abs/2601.23184)
*Fanmeng Wang,Haotian Liu,Guojiang Zhao,Hongteng Xu,Zhifeng Gao*

Main category: cs.CL

TL;DR: 本研究提出一种基于变分自编码器的新型潜在推理方法ReGuLaR，通过将显式推理链渲染成图像来指导压缩过程，有效提升了潜在推理的效率和准确性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有显式推理链虽然提升大规模语言模型性能，但引入大量计算冗余；而现有潜在推理方法由于缺乏合适的压缩指导，导致性能严重下降，因此需要一种既能保持推理性能又提高计算效率的新方法。

Method: 提出了一种名为Rendered CoT-Guided variational Latent Reasoning（ReGuLaR）的潜在学习范式，在变分自编码器框架下，将显式推理链渲染成图像，提取视觉-语义表示来调节后验分布，实现高效压缩并最大限度减少信息损失。

Result: 大量实验证明，ReGuLaR在计算效率和推理效果上优于现有潜在推理方法，同时通过多模态推理超越了传统的显式链式思维方法。

Conclusion: ReGuLaR在计算效率和推理效果上显著优于现有的潜在推理方法，甚至通过多模态推理超过了链式思维(CoT)的方法，为潜在推理提供了新的解决方案。

Abstract: While Chain-of-Thought (CoT) significantly enhances the performance of Large Language Models (LLMs), explicit reasoning chains introduce substantial computational redundancy. Recent latent reasoning methods attempt to mitigate this by compressing reasoning processes into latent space, but often suffer from severe performance degradation due to the lack of appropriate compression guidance. In this study, we propose Rendered CoT-Guided variational Latent Reasoning (ReGuLaR), a simple yet novel latent learning paradigm resolving this issue. Fundamentally, we formulate latent reasoning within the Variational Auto-Encoding (VAE) framework, sampling the current latent reasoning state from the posterior distribution conditioned on previous ones. Specifically, when learning this variational latent reasoning model, we render explicit reasoning chains as images, from which we extract dense visual-semantic representations to regularize the posterior distribution, thereby achieving efficient compression with minimal information loss. Extensive experiments demonstrate that ReGuLaR significantly outperforms existing latent reasoning methods across both computational efficiency and reasoning effectiveness, and even surpasses CoT through multi-modal reasoning, providing a new and insightful solution to latent reasoning. Code: https://github.com/FanmengWang/ReGuLaR.

</details>


### [57] [Deep Search with Hierarchical Meta-Cognitive Monitoring Inspired by Cognitive Neuroscience](https://arxiv.org/abs/2601.23188)
*Zhongxiang Sun,Qipeng Wang,Weijie Yu,Jingxuan Yang,Haolang Lu,Jun Xu*

Main category: cs.CL

TL;DR: 本文提出DS-MCM，通过模拟人类元认知的层次化监控机制，增强深度搜索代理的推理和检索过程，有效提升任务执行质量和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的深度搜索代理在任务演进过程中缺乏有效的机制以监控和调节推理与检索状态，导致在不确定环境下实际表现不佳。认知神经科学指出，人类元认知通过层次化结构结合快速异常检测与经验驱动的反思实现有效监控，启发了本研究。

Method: 提出了DS-MCM深度搜索框架，该框架整合了快速一致性监控器和基于经验的慢速监控器，分别负责轻量级的外部证据与内部推理一致性检查，以及基于历史经验选择性触发的纠正干预。通过将监控嵌入推理与检索环节，实现对何时干预及如何利用先前经验进行纠正的判断。

Result: 在多个深度搜索基准测试和不同基础模型上，DS-MCM框架表现出持续的性能提升和鲁棒性增强，验证了其元认知监控机制的有效性。

Conclusion: DS-MCM框架通过引入层次化的元认知监控机制，显著提升了深度搜索代理在多步检索、推理和长程任务执行中的性能和鲁棒性。

Abstract: Deep search agents powered by large language models have demonstrated strong capabilities in multi-step retrieval, reasoning, and long-horizon task execution. However, their practical failures often stem from the lack of mechanisms to monitor and regulate reasoning and retrieval states as tasks evolve under uncertainty. Insights from cognitive neuroscience suggest that human metacognition is hierarchically organized, integrating fast anomaly detection with selectively triggered, experience-driven reflection. In this work, we propose Deep Search with Meta-Cognitive Monitoring (DS-MCM), a deep search framework augmented with an explicit hierarchical metacognitive monitoring mechanism. DS-MCM integrates a Fast Consistency Monitor, which performs lightweight checks on the alignment between external evidence and internal reasoning confidence, and a Slow Experience-Driven Monitor, which is selectively activated to guide corrective intervention based on experience memory from historical agent trajectories. By embedding monitoring directly into the reasoning-retrieval loop, DS-MCM determines both when intervention is warranted and how corrective actions should be informed by prior experience. Experiments across multiple deep search benchmarks and backbone models demonstrate that DS-MCM consistently improves performance and robustness.

</details>


### [58] [Are you going to finish that? A Practical Study of the Tokenization Boundary Problem](https://arxiv.org/abs/2601.23223)
*Hao Xu,Alisa Liu,Jonathan Hayase,Yejin Choi,Noah A. Smith*

Main category: cs.CL

TL;DR: 本文发现语言模型在处理部分token结尾的自然提示时表现严重下降，尤其在无空格语言和代码中。该问题随模型规模加大不减反增。通过实验验证了解决方法，为模型推理提供实用建议。


<details>
  <summary>Details</summary>
Motivation: 语言模型训练基于token序列，但用户输入为文本，二者边界不匹配导致部分token问题，严重影响预测准确性，特别在无空格及复杂语言和代码中更为突出，亟需深入研究与解决。

Method: 系统构建语义自然且以部分token结尾的提示，实验比较了正常token对齐和部分token提示下语言模型的性能表现。评估了推理时的缓解方法并验证了精确解决方案的有效性。

Result: 发现部分token导致的概率失真严重，正确续写的概率降低约三个数量级，规模越大模型表现越差。推理时采用精确解决方案能有效缓解该问题。

Conclusion: 部分token问题在现实应用中普遍存在，特别是在无空格语言、高度复合语言和代码中，即使自然、完整的词语提示也会受到影响。该问题导致语言模型在预测下一个token时概率严重失真，且随模型规模增大问题不减反增。

Abstract: Language models (LMs) are trained over sequences of tokens, whereas users interact with LMs via text. This mismatch gives rise to the partial token problem, which occurs when a user ends their prompt in the middle of the expected next-token, leading to distorted next-token predictions. Although this issue has been studied using arbitrary character prefixes, its prevalence and severity in realistic prompts respecting word boundaries remains underexplored. In this work, we identify three domains where token and "word" boundaries often do not line up: languages that do not use whitespace, highly compounding languages, and code. In Chinese, for example, up to 25% of word boundaries do not line up with token boundaries, making even natural, word-complete prompts susceptible to this problem. We systematically construct semantically natural prompts ending with a partial tokens; in experiments, we find that they comprise a serious failure mode: frontier LMs consistently place three orders of magnitude less probability on the correct continuation compared to when the prompt is "backed-off" to be token-aligned. This degradation does not diminish with scale and often worsens for larger models. Finally, we evaluate inference-time mitigations to the partial token problem and validate the effectiveness of recent exact solutions. Overall, we demonstrate the scale and severity of probability distortion caused by tokenization in realistic use cases, and provide practical recommentions for model inference providers.

</details>


### [59] [Now You Hear Me: Audio Narrative Attacks Against Large Audio-Language Models](https://arxiv.org/abs/2601.23255)
*Ye Yu,Haibo Jin,Yaoning Yu,Jun Zhuang,Haohan Wang*

Main category: cs.CL

TL;DR: 本文揭示了大型音频语言模型在语音输入模式下的安全漏洞，提出了一种基于文本到音频的越狱攻击，并展示其高效突破当前安全机制的能力，强调了开发兼顾语言和副语言信息的安全框架的必要性。


<details>
  <summary>Details</summary>
Motivation: 大型音频语言模型逐渐直接处理原始语音输入，尽管带来各领域的无缝整合，但也引入了一类尚未充分认识的安全漏洞。

Method: 设计了一种文本到音频的越狱攻击，通过高级的指令执行文本到语音模型，将禁止的指令嵌入叙事风格的音频流中，利用结构和声学属性规避针对文本的安全机制。

Result: 该攻击在合成语音中成功率达98.26%，显著高于仅文本攻击，能够诱发最先进模型（如Gemini 2.0 Flash）输出受限内容。

Conclusion: 基于语音的大型音频语言模型存在独特的安全隐患，当前主要针对文本的安全机制无法有效防御语音形式的攻击，需要发展能够同时处理语言和副语言信息的安全框架。

Abstract: Large audio-language models increasingly operate on raw speech inputs, enabling more seamless integration across domains such as voice assistants, education, and clinical triage. This transition, however, introduces a distinct class of vulnerabilities that remain largely uncharacterized. We examine the security implications of this modality shift by designing a text-to-audio jailbreak that embeds disallowed directives within a narrative-style audio stream. The attack leverages an advanced instruction-following text-to-speech (TTS) model to exploit structural and acoustic properties, thereby circumventing safety mechanisms primarily calibrated for text. When delivered through synthetic speech, the narrative format elicits restricted outputs from state-of-the-art models, including Gemini 2.0 Flash, achieving a 98.26% success rate that substantially exceeds text-only baselines. These results highlight the need for safety frameworks that jointly reason over linguistic and paralinguistic representations, particularly as speech-based interfaces become more prevalent.

</details>


### [60] [PaperBanana: Automating Academic Illustration for AI Scientists](https://arxiv.org/abs/2601.23265)
*Dawei Zhu,Rui Meng,Yale Song,Xiyu Wei,Sujian Li,Tomas Pfister,Jinsung Yoon*

Main category: cs.CL

TL;DR: PaperBanana提出一种多智能体协同自动生成学术插图的新框架，解决插图制作瓶颈，经大规模基准测试表现优异，推动自动化插图生成发展。


<details>
  <summary>Details</summary>
Motivation: 当前自动化AI科学家虽有快速进展，但生成发表用插图依旧是研究工作中的瓶颈且费时费力。

Method: 采用最先进的视觉语言模型和图像生成模型，联合多智能体系统实现文献检索、内容与风格规划、图像渲染及自我批评迭代完善。

Result: 通过包含292个NeurIPS 2025会议方法图测试案例的PaperBananaBench评测，PaperBanana在忠实度、简明性、可读性和美观性方面均超越主流基线方法，并成功扩展到高质量统计图的生成。

Conclusion: PaperBanana框架成功自动生成符合发表标准的学术插图，显著提升插图制作效率。

Abstract: Despite rapid advances in autonomous AI scientists powered by language models, generating publication-ready illustrations remains a labor-intensive bottleneck in the research workflow. To lift this burden, we introduce PaperBanana, an agentic framework for automated generation of publication-ready academic illustrations. Powered by state-of-the-art VLMs and image generation models, PaperBanana orchestrates specialized agents to retrieve references, plan content and style, render images, and iteratively refine via self-critique. To rigorously evaluate our framework, we introduce PaperBananaBench, comprising 292 test cases for methodology diagrams curated from NeurIPS 2025 publications, covering diverse research domains and illustration styles. Comprehensive experiments demonstrate that PaperBanana consistently outperforms leading baselines in faithfulness, conciseness, readability, and aesthetics. We further show that our method effectively extends to the generation of high-quality statistical plots. Collectively, PaperBanana paves the way for the automated generation of publication-ready illustrations.

</details>


### [61] [UPA: Unsupervised Prompt Agent via Tree-Based Search and Selection](https://arxiv.org/abs/2601.23273)
*Siran Peng,Weisong Zhao,Tianyu Fu,Chenxu Zhao,Tianshuo Zhang,Haoyuan Zhang,Xiangyu Zhu,Minghui Wu,Zhen Lei*

Main category: cs.CL

TL;DR: 本文提出了一种无监督提示代理UPA，通过基于LLM的配对比较和BTL模型框架，有效进行提示空间的结构化搜索和选择，实验证明其优于其他无监督提示优化方法。


<details>
  <summary>Details</summary>
Motivation: 传统提示优化方法依赖监督反馈信号，但实际中往往缺乏这类信号，迫切需要一种不依赖监督反馈的提示优化方法。

Method: UPA通过构建动态演进的树结构，在提示空间中进行搜索，利用大型语言模型提供的细粒度无序配对比较指导搜索过程。采用了基于Bradley-Terry-Luce模型的两阶段框架，先进行路径级贝叶斯聚合以筛选候选提示，后通过全球锦标赛式比较推断提示质量并选出最优提示。

Result: 实验结果显示，在多个任务中，UPA方法优于现有提示优化技术，证明了其在无监督环境下保持高效并实现结构化搜索与选择的能力。

Conclusion: 本文提出的UPA方法在无监督条件下高效优化提示，显著优于现有的提示优化方法，验证了代理式优化策略在无监督环境中的有效性。

Abstract: Prompt agents have recently emerged as a promising paradigm for automated prompt optimization, framing refinement as a sequential decision-making problem over a structured prompt space. While this formulation enables the use of advanced planning algorithms, these methods typically assume access to supervised reward signals, which are often unavailable in practical scenarios. In this work, we propose UPA, an Unsupervised Prompt Agent that realizes structured search and selection without relying on supervised feedback. Specifically, during search, UPA iteratively constructs an evolving tree structure to navigate the prompt space, guided by fine-grained and order-invariant pairwise comparisons from Large Language Models (LLMs). Crucially, as these local comparisons do not inherently yield a consistent global scale, we decouple systematic prompt exploration from final selection, introducing a two-stage framework grounded in the Bradley-Terry-Luce (BTL) model. This framework first performs path-wise Bayesian aggregation of local comparisons to filter candidates under uncertainty, followed by global tournament-style comparisons to infer latent prompt quality and identify the optimal prompt. Experiments across multiple tasks demonstrate that UPA consistently outperforms existing prompt optimization methods, showing that agent-style optimization remains highly effective even in fully unsupervised settings.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [62] [Linux Kernel Recency Matters, CVE Severity Doesn't, and History Fades](https://arxiv.org/abs/2601.22196)
*Piotr Przymus,Witold Weiner,Krzysztof Rykaczewski,Gunnar Kudrjavets*

Main category: cs.SE

TL;DR: Linux内核自2024年成为CVE编号机构后，研究发现漏洞修复主要取决于内核版本新旧，开发者优先修复新内核漏洞，旧内核漏洞持续存在。漏洞引入的代码复杂度高，内核CVE管理独具特色。


<details>
  <summary>Details</summary>
Motivation: 2024年Linux内核成为自己的CVE编号机构，研究其漏洞识别和修复动态，为理解内核漏洞管理机制提供数据支持。

Method: 通过分析Linux内核CVE的元数据、相关提交及补丁延迟，使用生存模型评估不同变量对补丁速度的预测能力。

Result: 严重性和CVSS指标与补丁延迟关联很小，内核版本新旧是修复速度的重要预测指标。引入漏洞的代码更复杂，Linux内核的CVE流程具有独特性。

Conclusion: Linux内核的CVE修复主要受内核版本的新旧影响，严重性和CVSS评分与补丁延迟关系较小。开发者更倾向于优先修复较新的内核漏洞，旧版本漏洞往往长期未解决。引入漏洞的提交更复杂且范围更广。

Abstract: In 2024, the Linux kernel became its own Common Vulnerabilities and Exposures (CVE) Numbering Authority (CNA), formalizing how kernel vulnerabilities are identified and tracked. We analyze the anatomy and dynamics of kernel CVEs using metadata, associated commits, and patch latency to understand what drives patching. Results show that severity and Common Vulnerability Scoring System (CVSS) metrics have a negligible association with patch latency, whereas kernel recency is a reasonable predictor in survival models. Kernel developers fix newer kernels sooner, while older ones retain unresolved CVEs. Commits introducing vulnerabilities are typically broader and more complex than their fixes, though often only approximate reconstructions of development history. The Linux kernel remains a unique open-source project -- its CVE process is no exception.

</details>


### [63] [Stalled, Biased, and Confused: Uncovering Reasoning Failures in LLMs for Cloud-Based Root Cause Analysis](https://arxiv.org/abs/2601.22208)
*Evelien Riddell,James Riddell,Gengyi Sun,Michał Antkiewicz,Krzysztof Czarnecki*

Main category: cs.SE

TL;DR: 该论文提出了一个受控实验框架，系统评估了多款大语言模型在多跳根因分析中的推理能力，揭示了其推理失败的类型及影响，为提升自动化故障诊断提供了实证依据和指导。


<details>
  <summary>Details</summary>
Motivation: 复杂云系统中的多跳故障传播难以自动诊断，现有方法受限于历史数据规模、遥测数据超出LLM能力或复杂多代理管道，难以准确评估LLM推理能力。

Method: 设计了一个简化且受控的实验框架，评测了六种LLM在两种代理工作流（ReAct和Plan-and-Execute）及非代理基线下，对两个真实案例（GAIA和OpenRCA）共执行48,000个模拟故障场景的表现，分析根因准确率和推理过程质量。

Result: 通过对16种常见RCA推理失败类型的标注及利用LLM进行评审，清晰识别了LLM成功与失败环节，量化了对输入数据类型的敏感度，并揭示了部分推理失败可预测最终正确性，为未来推理驱动的系统诊断提供了透明且可复现的实证成果及故障分类。

Conclusion: 当前开源大语言模型（LLM）在多跳根因分析（RCA）中表现出一定的能力，但其推理仍存在较多失败类型，影响最终诊断准确性。

Abstract: Root cause analysis (RCA) is essential for diagnosing failures within complex software systems to ensure system reliability. The highly distributed and interdependent nature of modern cloud-based systems often complicates RCA efforts, particularly for multi-hop fault propagation, where symptoms appear far from their true causes. Recent advancements in Large Language Models (LLMs) present new opportunities to enhance automated RCA. However, their practical value for RCA depends on the fidelity of reasoning and decision-making. Existing work relies on historical incident corpora, operates directly on high-volume telemetry beyond current LLM capacity, or embeds reasoning inside complex multi-agent pipelines -- conditions that obscure whether failures arise from reasoning itself or from peripheral design choices.
  We present a focused empirical evaluation that isolates an LLM's reasoning behavior. We design a controlled experimental framework that foregrounds the LLM by using a simplified experimental setting. We evaluate six LLMs under two agentic workflows (ReAct and Plan-and-Execute) and a non-agentic baseline on two real-world case studies (GAIA and OpenRCA). In total, we executed 48,000 simulated failure scenarios, totaling 228 days of execution time. We measure both root-cause accuracy and the quality of intermediate reasoning traces. We produce a labeled taxonomy of 16 common RCA reasoning failures and use an LLM-as-a-Judge for annotation. Our results clarify where current open-source LLMs succeed and fail in multi-hop RCA, quantify sensitivity to input data modalities, and identify reasoning failures that predict final correctness. Together, these contributions provide transparent and reproducible empirical results and a failure taxonomy to guide future work on reasoning-driven system diagnosis.

</details>


### [64] [Predicting Intermittent Job Failure Categories for Diagnosis Using Few-Shot Fine-Tuned Language Models](https://arxiv.org/abs/2601.22264)
*Henri Aïdasso,Francis Bordeleau,Ali Tizghadam*

Main category: cs.SE

TL;DR: 本文针对CI流水线间歇性作业失败，提出少样本学习方法FlaXifyer和解释技术LogSift，实现高效准确的失败分类及故障定位，大幅提升诊断效率。


<details>
  <summary>Details</summary>
Motivation: CI流水线中的作业间歇性失败影响效率，现有方法虽能检测失败但未能解决故障诊断难题，迫切需要自动化、准确的故障诊断工具。

Method: 提出FlaXifyer，这是一种基于预训练语言模型的少样本学习方法，利用作业执行日志进行作业失败类别预测；同时提出LogSift解释技术，高效识别关键日志语句，帮助快速定位故障信息。

Result: FlaXifyer在仅用每类12个标注样本时，达成84.3%宏观F1和92.0%Top-2准确率；LogSift能在1秒内识别关键日志，减少74.4%审查工作，相关故障信息覆盖87%案例。

Conclusion: FlaXifyer和LogSift方法能有效实现自动分类和诊断间歇性CI流水线作业失败，提升故障排查效率，减少人工诊断工作量。

Abstract: In principle, Continuous Integration (CI) pipeline failures provide valuable feedback to developers on code-related errors. In practice, however, pipeline jobs often fail intermittently due to non-deterministic tests, network outages, infrastructure failures, resource exhaustion, and other reliability issues. These intermittent (flaky) job failures lead to substantial inefficiencies: wasted computational resources from repeated reruns and significant diagnosis time that distracts developers from core activities and often requires intervention from specialized teams. Prior work has proposed machine learning techniques to detect intermittent failures, but does not address the subsequent diagnosis challenge. To fill this gap, we introduce FlaXifyer, a few-shot learning approach for predicting intermittent job failure categories using pre-trained language models. FlaXifyer requires only job execution logs and achieves 84.3% Macro F1 and 92.0% Top-2 accuracy with just 12 labeled examples per category. We also propose LogSift, an interpretability technique that identifies influential log statements in under one second, reducing review effort by 74.4% while surfacing relevant failure information in 87% of cases. Evaluation on 2,458 job failures from TELUS demonstrates that FlaXifyer and LogSift enable effective automated triage, accelerate failure diagnosis, and pave the way towards the automated resolution of intermittent job failures.

</details>


### [65] [PriviSense: A Frida-Based Framework for Multi-Sensor Spoofing on Android](https://arxiv.org/abs/2601.22414)
*Ibrahim Khalilov,Chaoran Chen,Ziang Xiao,Tianshi Li,Toby Jia-Jun Li,Yaxing Yao*

Main category: cs.SE

TL;DR: PriviSense是一款基于Frida的安卓设备实时传感器和系统信号伪造工具，支持脚本化注入，帮助开发者和研究者在物理设备上进行上下文敏感应用测试和隐私分析。


<details>
  <summary>Details</summary>
Motivation: 现有模拟器和工具无法有效支持物理设备上对上下文敏感应用行为的可重现测试。

Method: 基于Frida的脚本化工具，注入时间变化的传感器（加速度计、陀螺仪、计步器）和系统信号（电池电量、系统时间、设备元数据）到未修改的应用中。

Result: 成功在已root的安卓设备上对五个代表性传感器可视化应用实现了实时信号伪造，支持脚本化和可逆操作。

Conclusion: PriviSense实现了在物理Android设备上对传感器和系统信号的实时伪造，支持可重现的上下文敏感应用行为测试。

Abstract: Mobile apps increasingly rely on real-time sensor and system data to adapt their behavior to user context. While emulators and instrumented builds offer partial solutions, they often fail to support reproducible testing of context-sensitive app behavior on physical devices. We present PriviSense, a Frida-based, on-device toolkit for runtime spoofing of sensor and system signals on rooted Android devices. PriviSense can script and inject time-varying sensor streams (accelerometer, gyroscope, step counter) and system values (battery level, system time, device metadata) into unmodified apps, enabling reproducible on-device experiments without emulators or app rewrites. Our demo validates real-time spoofing on a rooted Android device across five representative sensor-visualization apps. By supporting scriptable and reversible manipulation of these values, PriviSense facilitates testing of app logic, uncovering of context-based behaviors, and privacy-focused analysis. To ensure ethical use, the code is shared upon request with verified researchers.
  Tool Guide: How to Run PriviSense on Rooted Android https://bit.ly/privisense-guide Demonstration video: https://www.youtube.com/watch?v=4Qwnogcc3pw

</details>


### [66] [Small is Beautiful: A Practical and Efficient Log Parsing Framework](https://arxiv.org/abs/2601.22590)
*Minxing Wang,Yintong Huo*

Main category: cs.SE

TL;DR: 提出EFParser，一种提升小型语言模型日志解析性能的无监督方法，通过双缓存和校正机制显著提升准确率，兼顾效率，适合资源受限场景。


<details>
  <summary>Details</summary>
Motivation: 当前基于大型语言模型（LLMs）的语义日志解析方法虽然效果优秀，但对模型规模依赖较大，小型模型效果显著下降，限制了其实际应用，尤其是在数据隐私和计算资源受限的环境中。

Method: 提出了一种名为EFParser的无监督LLM日志解析器，通过引入双缓存系统和自适应更新机制，区分新模式和现有模板的变体，实现模板合并与错误纠正；此外增加校正模块，在缓存前验证和优化LLM生成的模板以防止错误注入。

Result: EFParser在公共大规模数据集上对比最先进方法，小型LLM模型下性能提升平均12.5%，甚至超越某些使用大型模型的基线方法，同时保持较高计算效率。

Conclusion: EFParser通过系统性的架构创新显著提升了小型LLM模型在日志解析中的表现和稳定性，解决了性能下降难题，为受限资源环境下的实时日志分析提供了切实可行的解决方案。

Abstract: Log parsing is a fundamental step in log analysis, partitioning raw logs into constant templates and dynamic variables. While recent semantic-based parsers leveraging Large Language Models (LLMs) exhibit superior generalizability over traditional syntax-based methods, their effectiveness is heavily contingent on model scale. This dependency leads to significant performance collapse when employing smaller, more resource-efficient LLMs. Such degradation creates a major barrier to real-world adoption, where data privacy requirements and computational constraints necessitate the use of succinct models. To bridge this gap, we propose EFParser, an unsupervised LLM-based log parser designed to enhance the capabilities of smaller models through systematic architectural innovation. EFParser introduces a dual-cache system with an adaptive updating mechanism that distinguishes between novel patterns and variations of existing templates. This allows the parser to merge redundant templates and rectify prior errors, maintaining cache consistency. Furthermore, a dedicated correction module acts as a gatekeeper, validating and refining every LLM-generated template before caching to prevent error injection. Empirical evaluations on public large-scale datasets demonstrate that EFParser outperforms state-of-the-art baselines by an average of 12.5% across all metrics when running on smaller LLMs, even surpassing some baselines utilizing large-scale models. Despite its additional validation steps, EFParser maintains high computational efficiency, offering a robust and practical solution for real-world log analysis deployment.

</details>


### [67] [TimeMachine-bench: A Benchmark for Evaluating Model Capabilities in Repository-Level Migration Tasks](https://arxiv.org/abs/2601.22597)
*Ryo Fujii,Makoto Morishita,Kazuki Yano,Jun Suzuki*

Main category: cs.SE

TL;DR: 本文提出TimeMachine-bench基准，自动构建并验证真实Python项目中因依赖更新失败的测试，评估多模型软件迁移动能，发现大语言模型虽具潜力但仍需解决可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 随着自动化软件工程的发展，研究重点逐渐转向反映软件工程师日常工作的实际任务，但软件迁移这一关键过程被忽视。

Method: 提出TimeMachine-bench基准，自动构建包含因依赖更新导致测试失败的Python项目GitHub仓库，并精选人类验证子集以保证问题可解性。基于11个模型（包括开源及顶尖大语言模型）构建的代理基线在此子集上进行评估。

Result: 评估结果显示大语言模型在迁移任务中具有一定潜力，但存在可靠性问题，如利用低测试覆盖率产生虚假解、因工具使用策略不佳导致不必要编辑。

Conclusion: TimeMachine-bench提供了一个实用的评测平台，揭示了现有模型在软件迁移任务中的局限性，促进改进迁移自动化技术。

Abstract: With the advancement of automated software engineering, research focus is increasingly shifting toward practical tasks reflecting the day-to-day work of software engineers. Among these tasks, software migration, a critical process of adapting code to evolving environments, has been largely overlooked. In this study, we introduce TimeMachine-bench, a benchmark designed to evaluate software migration in real-world Python projects. Our benchmark consists of GitHub repositories whose tests begin to fail in response to dependency updates. The construction process is fully automated, enabling live updates of the benchmark. Furthermore, we curated a human-verified subset to ensure problem solvability. We evaluated agent-based baselines built on top of 11 models, including both strong open-weight and state-of-the-art LLMs on this verified subset. Our results indicated that, while LLMs show some promise for migration tasks, they continue to face substantial reliability challenges, including spurious solutions that exploit low test coverage and unnecessary edits stemming from suboptimal tool-use strategies. Our dataset and implementation are available at https://github.com/tohoku-nlp/timemachine-bench.

</details>


### [68] [Elderly HealthMag: Systematic Building and Calibrating a Tool for Identifying and Evaluating Senior User Digital Health Software](https://arxiv.org/abs/2601.22627)
*Yuqing Xiao,John Grundy,Anuradha Madugalla,Elizabeth Manias*

Main category: cs.SE

TL;DR: 本文提出了一种结合年龄和健康状况视角的数字健康软件评估工具HealthMag，有效识别和改进老年用户的包容性需求。


<details>
  <summary>Details</summary>
Motivation: 数字健康软件针对有健康状况的用户开发时，团队常基于隐含且错误的假设，导致产品无法满足老年用户和患病用户的特定需求，实际使用中缺乏包容性。

Method: 提出HealthMag工具，通过系统映射和校准，结合InclusiveMag框架和AgeMag方法，创造双视角的Elderly HealthMag，支持需求获取、设计和评估。

Result: 通过认知演练验证Age HealthMag工具在识别现有老年用户数字健康应用中的包容性偏差上的应用和实用性。

Conclusion: HealthMag及其扩展Elderly HealthMag有效帮助改善数字健康软件在设计和评估阶段的包容性，特别关注老年用户需求，从而提升软件的实际使用效果。

Abstract: Digital health (DH) software is increasingly deployed to populations where many end users live with one or more health conditions. Yet, DH software development teams frequently operate using implicit, incorrect assumptions about these users, resulting in products that under-serve the specific requirements imposed by their age and health conditions. Consequently, while software may meet clinical objectives on paper, it often fails to be inclusive during actual user interaction. To address this, we propose \textbf{\textit{HealthMag}}, a tool inspired by GenderMag designed to help better elicit, model and evaluate requirements for digital health software. We developed HealthMag through systematic mapping and calibration following the InclusiveMag framework. Furthermore, we integrated this with a calibrated version of an existing AgeMag method to create a dual-lens approach: \textbf{\textit{Elderly HealthMag}}, designed to aid requirements, design and evaluation of mHealth software for senior end users. We demonstrate application and utility of Age HealthMag via cognitive walkthroughs in identifying inclusivity biases in current senior user-oriented digital health applications.

</details>


### [69] [From Horizontal Layering to Vertical Integration: A Comparative Study of the AI-Driven Software Development Paradigm](https://arxiv.org/abs/2601.22667)
*Chi Zhang,Zehan Li,Ziqian Zhong,Haibing Ma,Dan Xiao,Chen Lin,Ming Dong*

Main category: cs.SE

TL;DR: 研究发现生成式AI推动软件工程组织从职能专化向端到端所有权转变，极大提升资源利用效率和生产率，强调人机协作效率，提出管理策略指导组织重塑。


<details>
  <summary>Details</summary>
Motivation: 探索生成式人工智能在软件工程中的组织影响及其如何促进生产效率和组织架构优化。

Method: 通过多案例比较研究，分析传统企业和AI原生新创企业两种软件开发环境中的组织变革及效率提升。

Result: 从水平层次结构向垂直整合转变可实现8倍至33倍的资源消耗减少，出现了“超级员工”现象，消除了跨职能协调的开销，同时揭示了人工智能对劳动规模回报和技术杠杆的影响。

Conclusion: 采用生成式人工智能后，软件工程组织从水平分层向垂直整合转变，显著降低资源消耗并提升生产率。人机协作效率成为组织优化的核心目标，传统单一的个人生产率指标被替代。

Abstract: This paper examines the organizational implications of Generative AI adoption in software engineering through a multiple-case comparative study. We contrast two development environments: a traditional enterprise (brownfield) and an AI-native startup (greenfield). Our analysis reveals that transitioning from Horizontal Layering (functional specialization) to Vertical Integration (end-to-end ownership) yields 8-fold to 33-fold reductions in resource consumption. We attribute these gains to the emergence of Super Employees, AI-augmented engineers who span traditional role boundaries, and the elimination of inter-functional coordination overhead. Theoretically, we propose Human-AI Collaboration Efficacy as the primary optimization target for engineering organizations, supplanting individual productivity metrics. Our Total Factor Productivity analysis identifies an AI Distortion Effect that diminishes returns to labor scale while amplifying technological leverage. We conclude with managerial strategies for organizational redesign, including the reactivation of idle cognitive bandwidth in senior engineers and the suppression of blind scale expansion.

</details>


### [70] [VarParser: Unleashing the Neglected Power of Variables for LLM-based Log Parsing](https://arxiv.org/abs/2601.22676)
*Jinrui Sun,Tong Jia,Minghua He,Ying Li*

Main category: cs.SE

TL;DR: 提出了VarParser，基于变量中心的日志解析新方法，解决了现有方法忽视变量信息的问题，显著提升了解析效果和效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于大型语言模型的日志解析方法过度关注日志的常量部分，忽视了变量部分的潜在贡献，导致日志分组和采样效率低、模型调用次数多、成本高且结果中丢失系统可见性。

Method: 通过变量贡献采样、变量中心的解析缓存和自适应变量感知的上下文学习策略，VarParser高效捕捉日志中的变量部分并利用其贡献进行解析，同时引入变量单元保留丰富的变量信息。

Result: 在大规模数据集上的广泛评估表明，VarParser相比现有方法具有更高的准确率、更优的解析效率和更低的模型调用成本。

Conclusion: 提出了VarParser，一种以变量为中心的日志解析策略，显著提升了日志解析的准确性、效率和成本效益，同时增强了日志解析结果的完整性。

Abstract: Logs serve as a primary source of information for engineers to diagnose failures in large-scale online service systems. Log parsing, which extracts structured events from massive unstructured log data, is a critical first step for downstream tasks like anomaly detection and failure diagnosis. With advances in large language models (LLMs), leveraging their strong text understanding capabilities has proven effective for accurate log parsing. However, existing LLM-based log parsers all focus on the constant part of logs, ignoring the potential contribution of the variable part to log parsing. This constant-centric strategy brings four key problems. First, inefficient log grouping and sampling with only constant information. Second, a relatively large number of LLM invocations due to constant-based cache, leading to low log parsing accuracy and efficiency. Third, a relatively large number of consumed constant tokens in prompts leads to high LLM invocation costs. At last, these methods only retain placeholders in the results, losing the system visibility brought by variable information in logs.
  Facing these problems, we propose a variable-centric log parsing strategy named VarParser. Through variable contribution sampling, variable-centric parsing cache, and adaptive variable-aware in-context learning, our approach can efficiently capture the variable parts of logs and leverage their contributions to parsing. By introducing variable units, we preserve rich variable information, enhancing the integrity of log parsing results. Extensive evaluations on large-scale datasets demonstrate that VarParser achieves higher accuracy compared to existing methods, significantly improving parsing efficiency while reducing the LLM invocation costs.

</details>


### [71] [AutoMerge: Search-Based Model Merging Framework for Effective Model Reuse](https://arxiv.org/abs/2601.22748)
*You Lu,Jiyang Zhang,Bihuan Chen,Chaofeng Sha,Dingji Wang,Xin Peng*

Main category: cs.SE

TL;DR: 本文系统评估了模型合并技术在不同领域和模型架构上的适用性，发现现有方法不足，并提出AutoMerge框架提升合并效果。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并技术主要针对大规模语言模型，未被系统性评估于其他架构和领域，且效果存在不确定性。

Method: 提出AutoMerge框架，通过将复杂模型划分为多个异质模块，系统地探索合并技术及其超参数配置。

Result: 在语言模型、图像分类和自动驾驶三大领域的三个不同模型架构上的评估显示现有技术效果不佳，AutoMerge有效提升了模型合并的适应性和效果。

Conclusion: 直接应用现有模型合并技术在不同架构和领域的深度学习模型上效果不稳定，且单一技术难以处理模型的异质结构，效果受超参数配置影响大。

Abstract: Software reuse has long been recognized as a critical and widely studied topic in software engineering, offering substantial benefits in reducing development costs, improving software quality, and enhancing operational efficiency. This paradigm extends into deep learning through model reuse. Recently, model merging has emerged in the domain of large language models (LLMs) as a training-free approach that takes multiple task-specific models with the same architecture as source models and merges them without retraining, enhancing model reuse within LLMs. However, no prior work has systematically investigated whether such an approach can be effectively applied to other deep learning models with different architectures across domains. To bridge this gap, we present the first systematic study that evaluates five model merging techniques on three distinct model architectures across three domains: LLMs, image classification, and autonomous driving. Our findings reveal that directly applying existing model merging techniques leads to highly inconsistent results and falls notably short of their success within LLMs. Moreover, a single model merging technique often fails to handle the heterogeneous structural properties within a model, limiting its applicability to different model architectures across domains. Furthermore, the effectiveness of model merging techniques is highly sensitive to hyperparameter configurations, thereby constraining their potential for broader adoption. Inspired by these insights, we propose AutoMerge, a novel search-based model merging framework that first segments complex models into multiple heterogeneous blocks and then systematically explores the merging space to identify the merging technique and its hyperparameter configuration.

</details>


### [72] [Constructing Safety Cases for AI Systems: A Reusable Template Framework](https://arxiv.org/abs/2601.22773)
*Sung Une Lee,Liming Zhu,Md Shamsujjoha,Liming Dong,Qinghua Lu,Jieshan Chen*

Main category: cs.SE

TL;DR: 本文针对生成式和智能AI系统不可预测且动态变化的安全风险，提出了一套系统化、可复用的安全案例模板框架，解决了传统安全案例方法在AI安全治理中的不足。


<details>
  <summary>Details</summary>
Motivation: 现代生成式与智能AI系统的行为不可预测，风险动态变化，传统基于稳定架构和已知边界的安全案例方法无法有效治理AI安全。

Method: 通过分析当前AI系统安全案例构建的不足，提出包含针对AI的主张类型、论证类型和证据类别的详细分类法，设计并示例化系统化、可组合的安全案例模板。

Result: 提出的模板框架具备可重复使用性和适应性，支持处理评估无真值、模型动态更新、基于阈值的风险决策等AI特有安全挑战，实现安全案例的系统化构建与维护。

Conclusion: 传统安全案例方法不适用于现代AI系统，因为它们的能力不可预测且风险动态变化。本文提出了可重复使用的安全案例模板框架，支持构建适应AI系统特性的可信、安全案例。

Abstract: Safety cases, structured arguments that a system is acceptably safe, are becoming central to the governance of AI systems. Yet, traditional safety-case practices from aviation or nuclear engineering rely on well-specified system boundaries, stable architectures, and known failure modes. Modern AI systems such as generative and agentic AI are the opposite. Their capabilities emerge unpredictably from low-level training objectives, their behaviour varies with prompts, and their risk profiles shift through fine-tuning, scaffolding, or deployment context. This study examines how safety cases are currently constructed for AI systems and why classical approaches fail to capture these dynamics. It then proposes a framework of reusable safety-case templates, each following a predefined structure of claims, arguments, and evidence tailored for AI systems. The framework introduces comprehensive taxonomies for AI-specific claim types (assertion-based, constrained-based, capability-based), argument types (demonstrative, comparative, causal/explanatory, risk-based, and normative), and evidence families (empirical, mechanistic, comparative, expert-driven, formal methods, operational/field data, and model-based). Each template is illustrated through end-to-end patterns addressing distinctive challenges such as evaluation without ground truth, dynamic model updates, and threshold-based risk decisions. The result is a systematic, composable, and reusable approach to constructing and maintaining safety cases that are credible, auditable, and adaptive to the evolving behaviour of generative and frontier AI systems.

</details>


### [73] [Understanding on the Edge: LLM-generated Boundary Test Explanations](https://arxiv.org/abs/2601.22791)
*Sabinakhon Akbarova,Felix Dobslaw,Robert Feldt*

Main category: cs.SE

TL;DR: 本研究探讨GPT-4.1在软件边界值测试中生成解释的有效性，发现其解释大多受到认可，并提出七项设计标准，表明经过改进的LLM工具可辅助测试工作。


<details>
  <summary>Details</summary>
Motivation: 边界值分析和测试在软件质量保证中非常关键，但测试人员常难以理解和说明为何某些输入输出对代表有意义的行为边界。大型语言模型可能通过生成自然语言推理来帮助，但其在边界值测试中的价值尚未经过实证评估。

Method: 通过调研27位软件专业人士，评估GPT-4.1生成的20个边界值对的解释，涵盖清晰度、正确性、完整性和感知有用性等维度，并对其中6位进行了深入访谈。

Result: 63.5%的评价为正面，17%为负面，受访者偏好结构清晰、引用权威来源并根据读者专业程度调整深度的解释，同时强调需要提供可操作的示例。基于此，提出了包含七项具体设计标准的需求清单。

Conclusion: 经过优化的基于大型语言模型的边界解释工具，有潜力支持测试流程，使边界值解释更加可操作且值得信赖。

Abstract: Boundary value analysis and testing (BVT) is fundamental in software quality assurance because faults tend to cluster at input extremes, yet testers often struggle to understand and justify why certain input-output pairs represent meaningful behavioral boundaries. Large Language Models (LLMs) could help by producing natural-language rationales, but their value for BVT has not been empirically assessed. We therefore conducted an exploratory study on LLM-generated boundary explanations: in a survey, twenty-seven software professionals rated GPT-4.1 explanations for twenty boundary pairs on clarity, correctness, completeness and perceived usefulness, and six of them elaborated in follow-up interviews. Overall, 63.5% of all ratings were positive (4-5 on a five-point Likert scale) compared to 17% negative (1-2), indicating general agreement but also variability in perceptions. Participants favored explanations that followed a clear structure, cited authoritative sources, and adapted their depth to the reader's expertise; they also stressed the need for actionable examples to support debugging and documentation. From these insights, we distilled a seven-item requirement checklist that defines concrete design criteria for future LLM-based boundary explanation tools. The results suggest that, with further refinement, LLM-based tools can support testing workflows by making boundary explanations more actionable and trustworthy.

</details>


### [74] [Just-in-Time Catching Test Generation at Meta](https://arxiv.org/abs/2601.22832)
*Matthew Becker,Yifei Chen,Nicholas Cochran,Pouyan Ghasemi,Abhishek Gulati,Mark Harman,Zachary Haluza,Mehrdad Honarkhah,Herve Robert,Jiacheng Liu,Weini Liu,Sreeja Thummala,Xiaoning Yang,Rui Xin,Sophie Zeng*

Main category: cs.SE

TL;DR: 本文提出了一种结合代码变更分析和智能评估器的即时捕获测试方法，显著提高了漏洞检测效率，减少误报，成功防止了后台系统的严重故障发生。


<details>
  <summary>Details</summary>
Motivation: 为了提前发现和防止大规模后台系统中的漏洞，解决传统加固测试难以捕获故障和误报过多的问题。

Method: 通过开发代码变更相关的方法生成捕获测试，并结合基于规则和大语言模型的评估器减少误报，从而降低人工审核负担。

Result: 分析了22126个生成的测试，捕获测试比传统加固测试提升了4倍，比偶发失败测试提升了20倍，人工审核负担减少了70%，报告的41个候选捕获中8个被确认是真阳性，其中4个能够避免严重故障。

Conclusion: 本文表明即时捕获测试在大规模后台系统中是可扩展且工业适用的，能够有效防止严重故障进入生产环境。

Abstract: We report on Just-in-Time catching test generation at Meta, designed to prevent bugs in large scale backend systems of hundreds of millions of line of code. Unlike traditional hardening tests, which pass at generation time, catching tests are meant to fail, surfacing bugs before code lands. The primary challenge is to reduce development drag from false positive test failures. Analyzing 22,126 generated tests, we show code-change-aware methods improve candidate catch generation 4x over hardening tests and 20x over coincidentally failing tests. To address false positives, we use rule-based and LLM-based assessors. These assessors reduce human review load by 70%. Inferential statistical analysis showed that human-accepted code changes are assessed to have significantly more false positives, while human-rejected changes have significantly more true positives. We reported 41 candidate catches to engineers; 8 were confirmed to be true positives, 4 of which would have led to serious failures had they remained uncaught. Overall, our results show that Just-in-Time catching is scalable, industrially applicable, and that it prevents serious failures from reaching production.

</details>


### [75] [MEnvAgent: Scalable Polyglot Environment Construction for Verifiable Software Engineering](https://arxiv.org/abs/2601.22859)
*Chuanzhe Guo,Jingjing Wu,Sijun He,Yang Chen,Zhaoqi Kuang,Shilong Fan,Bingjin Chen,Siqi Bao,Jing Liu,Hua Wu,Qingfu Zhu,Wanxiang Che,Haifeng Wang*

Main category: cs.SE

TL;DR: 本文针对多语言软件工程环境构建困难，提出MEnvAgent框架提升任务验证效率，并发布大规模数据集，推动相关领域进步。


<details>
  <summary>Details</summary>
Motivation: 因多语言软件工程环境构建复杂，缺乏可验证数据集限制了大型语言模型在此领域的发展。

Method: 提出了MEnvAgent框架，采用多代理的规划-执行-验证架构实现环境自动构建，并设计了环境重用机制以减少计算开销。

Result: 在MEnvBench基准测试中，MEnvAgent使失败修正率提升8.6%，时间成本减少43%，并构建了迄今最大规模的多语言可验证Docker环境数据集MEnvData-SWE。

Conclusion: MEnvAgent显著提升了多语言软件工程环境的自动构建效率和任务验证能力，推动了基于大型语言模型的SWE研究。

Abstract: The evolution of Large Language Model (LLM) agents for software engineering (SWE) is constrained by the scarcity of verifiable datasets, a bottleneck stemming from the complexity of constructing executable environments across diverse languages. To address this, we introduce MEnvAgent, a Multi-language framework for automated Environment construction that facilitates scalable generation of verifiable task instances. MEnvAgent employs a multi-agent Planning-Execution-Verification architecture to autonomously resolve construction failures and integrates a novel Environment Reuse Mechanism that reduces computational overhead by incrementally patching historical environments. Evaluations on MEnvBench, a new benchmark comprising 1,000 tasks across 10 languages, demonstrate that MEnvAgent outperforms baselines, improving Fail-to-Pass (F2P) rates by 8.6% while reducing time costs by 43%. Additionally, we demonstrate the utility of MEnvAgent by constructing MEnvData-SWE, the largest open-source polyglot dataset of realistic verifiable Docker environments to date, alongside solution trajectories that enable consistent performance gains on SWE tasks across a wide range of models. Our code, benchmark, and dataset are available at https://github.com/ernie-research/MEnvAgent.

</details>


### [76] [AnoMod: A Dataset for Anomaly Detection and Root Cause Analysis in Microservice Systems](https://arxiv.org/abs/2601.22881)
*Ke Ping,Hamza Bin Mazhar,Yuqing Wang,Ying Song,Mika V. Mäntylä*

Main category: cs.SE

TL;DR: 本文提出了一个包含多种异常类型和多种监控模态的微服务多模态异常检测数据集AnoMod，促进了异常检测与根因分析的综合研究。


<details>
  <summary>Details</summary>
Motivation: 现有微服务系统异常检测数据集多聚焦性能相关故障且监控模态单一，限制了对更广泛异常模式和跨模态方法的研究。

Method: 设计并注入性能级、服务级、数据库级和代码级四类异常，采集日志、指标、分布式追踪、API响应和代码覆盖五种监控模态，构建了多模态异常检测数据集。

Result: 构建了基于两个开源微服务系统的多模态异常检测数据集AnoMod，支持跨模态异常检测、融合/消融策略评估及细粒度根因分析。

Conclusion: 本论文通过引入AnoMod数据集，填补了微服务系统异常检测和根因分析数据集匮乏的空白，赋能跨模态异常检测和细粒度根因定位研究。

Abstract: Microservice systems (MSS) have become a predominant architectural style for cloud services. Yet the community still lacks high-quality, publicly available datasets for anomaly detection (AD) and root cause analysis (RCA) in MSS. Most benchmarks emphasize performance-related faults and provide only one or two monitoring modalities, limiting research on broader failure modes and cross-modal methods. To address these gaps, we introduce a new multimodal anomaly dataset built on two open-source microservice systems: SocialNetwork and TrainTicket. We design and inject four categories of anomalies (Ano): performance-level, service-level, database-level, and code-level, to emulate realistic anomaly modes. For each scenario, we collect five modalities (Mod): logs, metrics, distributed traces, API responses, and code coverage reports, offering a richer, end-to-end view of system state and inter-service interactions. We name our dataset, reflecting its unique properties, as AnoMod. This dataset enables (1) evaluation of cross-modal anomaly detection and fusion/ablation strategies, and (2) fine-grained RCA studies across service and code regions, supporting end-to-end troubleshooting pipelines that jointly consider detection and localization.

</details>


### [77] [A Serverless Edge-Native Data Processing Architecture for Autonomous Driving Training](https://arxiv.org/abs/2601.22919)
*Fabian Bally,Michael Schötz,Thomas Limbrunner*

Main category: cs.SE

TL;DR: 本文提出的Lambda边缘计算框架实现了车载数据的高效过滤与实时处理，兼容ROS 2，提升自动驾驶数据采集效率，在嵌入式平台上表现出良好性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶机器学习需大量传感数据，但关键且稀缺的安全场景难以高效采集，需求在车辆端实现智能数据过滤和处理，提升训练数据的有效性和及时性。

Method: 提出一种基于边缘计算的Lambda框架，支持用户定义函数在车载设备上进行数据过滤和处理，利用FaaS抽象层分离应用逻辑与底层执行，兼容ROS 2及现有数据记录流程。

Result: 在NVIDIA Jetson Orin Nano平台上对比原生ROS 2部署，Lambda框架展现了有竞争力的性能，降低了延迟和抖动，验证了基于lambda的抽象支持嵌入式自动驾驶系统的实时数据处理。

Conclusion: Lambda框架通过将无服务器（serverless）和函数即服务（FaaS）理念引入资源受限的汽车环境，实现了模块化、事件驱动的数据过滤算法，为自动驾驶嵌入式系统提供了实时数据处理能力。

Abstract: Data is both the key enabler and a major bottleneck for machine learning in autonomous driving. Effective model training requires not only large quantities of sensor data but also balanced coverage that includes rare yet safety-critical scenarios. Capturing such events demands extensive driving time and efficient selection. This paper introduces the Lambda framework, an edge-native platform that enables on-vehicle data filtering and processing through user-defined functions. The framework provides a serverless-inspired abstraction layer that separates application logic from low-level execution concerns such as scheduling, deployment, and isolation. By adapting Function-as-a-Service (FaaS) principles to resource-constrained automotive environments, it allows developers to implement modular, event-driven filtering algorithms while maintaining compatibility with ROS 2 and existing data recording pipelines. We evaluate the framework on an NVIDIA Jetson Orin Nano and compare it against native ROS 2 deployments. Results show competitive performance, reduced latency and jitter, and confirm that lambda-based abstractions can support real-time data processing in embedded autonomous driving systems. The source code is available at https://github.com/LASFAS/jblambda.

</details>


### [78] [Sifting the Noise: A Comparative Study of LLM Agents in Vulnerability False Positive Filtering](https://arxiv.org/abs/2601.22952)
*Yunpeng Xiong,Ting Zhang*

Main category: cs.SE

TL;DR: 本文比较了三种基于大语言模型的代理在过滤SAST误报上的效果，发现它们能显著降低误报，但需权衡误报降低与真实漏洞遗漏，以及计算成本与模型选择等因素。


<details>
  <summary>Details</summary>
Motivation: SAST工具误报率高，给开发者带来大量手动筛查负担。利用大语言模型代理的迭代推理与环境交互能力，有望提高误报过滤效果，但不同代理架构的相对性能尚不明确。

Method: 通过比较三种最先进的基于大语言模型的代理框架（Aider、OpenHands和SWE-agent）在OWASP基准和真实开源Java项目中的表现，评估它们在误报过滤中的有效性。

Result: 实验表明，LLM代理能将OWASP基准的误报率由92%降至最低6.3%，在真实数据集上针对CodeQL警报最高达到93.3%的误报识别率；但效果依赖于基础模型和漏洞类型，计算成本差异大，且误报大幅减少的同时存在真实漏洞被抑制的风险。

Conclusion: 基于大语言模型的代理对于过滤静态应用安全测试（SAST）工具产生的误报极为有效，能大幅减少误报率，但其效果依赖于模型架构、基础模型和漏洞类型，并存在误杀真实漏洞的风险。

Abstract: Static Application Security Testing (SAST) tools are essential for identifying software vulnerabilities, but they often produce a high volume of false positives (FPs), imposing a substantial manual triage burden on developers. Recent advances in Large Language Model (LLM) agents offer a promising direction by enabling iterative reasoning, tool use, and environment interaction to refine SAST alerts. However, the comparative effectiveness of different LLM-based agent architectures for FP filtering remains poorly understood. In this paper, we present a comparative study of three state-of-the-art LLM-based agent frameworks, i.e., Aider, OpenHands, and SWE-agent, for vulnerability FP filtering. We evaluate these frameworks using the vulnerabilities from the OWASP Benchmark and real-world open-source Java projects. The experimental results show that LLM-based agents can remove the majority of SAST noise, reducing an initial FP detection rate of over 92% on the OWASP Benchmark to as low as 6.3% in the best configuration. On real-world dataset, the best configuration of LLM-based agents can achieve an FP identification rate of up to 93.3% involving CodeQL alerts. However, the benefits of agents are strongly backbone- and CWE-dependent: agentic frameworks significantly outperform vanilla prompting for stronger models such as Claude Sonnet 4 and GPT-5, but yield limited or inconsistent gains for weaker backbones. Moreover, aggressive FP reduction can come at the cost of suppressing true vulnerabilities, highlighting important trade-offs. Finally, we observe large disparities in computational cost across agent frameworks. Overall, our study demonstrates that LLM-based agents are a powerful but non-uniform solution for SAST FP filtering, and that their practical deployment requires careful consideration of agent design, backbone model choice, vulnerability category, and operational cost.

</details>


### [79] [SWE-Manager: Selecting and Synthesizing Golden Proposals Before Coding](https://arxiv.org/abs/2601.22956)
*Boyin Tan,Haoning Deng,Junyuan Zhang,Junjielong Xu,Pinjia He,Youcheng Sun*

Main category: cs.SE

TL;DR: 针对软件工程中多方案选择问题，提出SWE-Manager强化学习模型，能较好地评估并合成最佳修复方案，显著提升了问题解决效果，优于现有强基线。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在软件工程中主要聚焦代码生成和修复，实际中团队需要在多个候选方案中选择最佳方案以降低风险、提高问题解决的可靠性，因此需要模型能评估方案优劣并合理选择。

Method: 通过对真实问题的手工研究，揭示维护者选择方案的理由，基于此设计了一个8B参数的强化学习模型SWE-Manager，该模型进行方案比较、理由生成和黄金方案合成；同时设计了P2A框架模拟真实工作流进行评估。

Result: SWE-Manager在SWE-Lancer Manager基准上达到53.21的选择准确率和57.75的收益率，带来15.275万美元收益，表现优于强基线模型；P2A框架进一步验证了其真实场景下的有效性。

Conclusion: 本文提出的SWE-Manager模型能有效从多个修复方案中选择并综合出最优方案，显著提升了问题解决的准确性和收益，超过了包括GPT-5在内的强基线。

Abstract: Large language model (LLM) research in software engineering has largely focused on tasks such as code generation and bug repair. In practice, teams often draft multiple candidate proposals for fixing an issue and then deliberate on one golden proposal for implementation. This selection requires not only assessing the issue's scope, impact, and urgency, but also a clear understanding of each proposal's strengths and weaknesses. A good selection could make issue resolution more reliable while reducing regression and operational risk, whereas a poor choice can increase risk and even cause unpredictable failures.
  We first conduct a manual study of real-world issues to characterize the rationales maintainers use when selecting among competing proposals. Motivated by these findings, we introduce SWE-Manager, a joint selection and synthesis approach that selects the best proposal and synthesizes a golden proposal. SWE-Manager is an 8B model trained via reinforcement learning (RL) to compare proposals, justify its choice, and synthesize a golden proposal for implementation. We view proposal selection as a reasoning task, mirroring how technical managers review competing proposals by weighing issue context and each proposal's solution without executing code or running tests. On the SWE-Lancer Manager benchmark, SWE-Manager achieves 53.21 selection accuracy and 57.75 earn rate, earning 152,750 dollars and outperforming strong baselines including GPT-5. To further evaluate the effectiveness of SWE-Manager in real-world issue resolution, we design the P2A framework, which simulates a real-world workflow where multiple proposals are drafted, reviewed, and a golden proposal is selected for implementation ...

</details>


### [80] [SolAgent: A Specialized Multi-Agent Framework for Solidity Code Generation](https://arxiv.org/abs/2601.23009)
*Wei Chen,Zhiyuan Peng,Xin Yin,Chao Ni,Chenhao Ying,Bang Xie,Yuan Luo*

Main category: cs.SE

TL;DR: SolAgent通过模拟专家工作流程及双循环修正机制，大幅提升智能合约代码质量和安全性，优于现有技术并推广了安全智能合约生成的开放模型。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型虽在代码生成方面有潜力，但在智能合约的严格需求下仍存在代码错误和安全漏洞问题，亟需一种有效机制提升智能合约的正确性和安全性。

Method: 提出了SolAgent，一个模拟人类专家工作流程的多智能体框架，采用双循环细化机制：内部使用Forge编译器确保功能正确性，外部利用Slither静态分析工具消除安全漏洞，同时具备文件系统功能处理复杂项目依赖。

Result: 在SolEval+基准测试中，SolAgent Pass@1准确率达64.39%，明显优于大约25%的现有LLM和GitHub Copilot等，同时安全漏洞减少了39.77%。

Conclusion: SolAgent显著提升了智能合约的功能正确性和安全性，性能优于现有的LLM、AI开发环境和代理框架，极大降低了安全漏洞，并促进了开源模型的优化发展。

Abstract: Smart contracts are the backbone of the decentralized web, yet ensuring their functional correctness and security remains a critical challenge. While Large Language Models (LLMs) have shown promise in code generation, they often struggle with the rigorous requirements of smart contracts, frequently producing code that is buggy or vulnerable. To address this, we propose SolAgent, a novel tool-augmented multi-agent framework that mimics the workflow of human experts. SolAgent integrates a \textbf{dual-loop refinement mechanism}: an inner loop using the \textit{Forge} compiler to ensure functional correctness, and an outer loop leveraging the \textit{Slither} static analyzer to eliminate security vulnerabilities. Additionally, the agent is equipped with file system capabilities to resolve complex project dependencies. Experiments on the SolEval+ Benchmark, a rigorous suite derived from high-quality real-world projects, demonstrate that SolAgent achieves a Pass@1 rate of up to \textbf{64.39\%}, significantly outperforming state-of-the-art LLMs ($\sim$25\%), AI IDEs (e.g., GitHub Copilot), and existing agent frameworks. Moreover, it reduces security vulnerabilities by up to \textbf{39.77\%} compared to human-written baselines. Finally, we demonstrate that the high-quality trajectories generated by SolAgent can be used to distill smaller, open-source models, democratizing access to secure smart contract generation. We release our data and code at https://github.com/openpaperz/SolAgent.

</details>


### [81] [Uncovering Hidden Inclusions of Vulnerable Dependencies in Real-World Java Projects](https://arxiv.org/abs/2601.23020)
*Stefan Schott,Serena Elisa Ponta,Wolfram Fischer,Jonas Klauke,Eric Bodden*

Main category: cs.SE

TL;DR: Unshade结合元数据和代码指纹技术，弥补传统依赖扫描漏洞，显著提升Java项目开源依赖中的已知漏洞检测率。


<details>
  <summary>Details</summary>
Motivation: 依赖开源软件带来安全风险，传统元数据扫描遗漏被修改或隐藏的依赖，亟需一种兼具效率与深度检测能力的依赖漏洞扫描方法。

Method: Unshade采用混合扫描方法，结合基于元数据的高效扫描与基于字节码指纹的代码中心检测机制，先通过字节码指纹增强软件物料清单（SBOM），再利用元数据扫描器识别已知漏洞。

Result: 在1,808个最受欢迎的Java Maven开源项目中，近50%含有被传统扫描遗漏的隐藏修改依赖且带有已知漏洞，Unshade共检测出7,712个隐藏依赖中的唯一CVE漏洞。

Conclusion: Unshade能够有效识别Java项目中隐藏和修改的依赖组件中的已知安全漏洞，显著提升依赖安全扫描的覆盖率和准确性。

Abstract: Open-source software (OSS) dependencies are a dominant component of modern software code bases. Using proven and well-tested OSS components lets developers reduce development time and cost while improving quality. However, heavy reliance on open-source software also introduces significant security risks, including the incorporation of known vulnerabilities into the codebase. To mitigate these risks, metadata-based dependency scanners, which are lightweight and fast, and code-centric scanners, which enable the detection of modified dependencies hidden from metadata-based approaches, have been developed. In this paper, we present Unshade, a hybrid approach towards dependency scanning in Java that combines the efficiency of metadata-based scanning with the ability to detect modified dependencies of code-centric approaches. Unshade first augments a Java project's software bill of materials (SBOM) by identifying modified and hidden dependencies via a bytecode-based fingerprinting mechanism. This augmented SBOM is then passed to a metadata-based vulnerability scanner to identify known vulnerabilities in both declared and newly revealed dependencies. Leveraging Unshade's high scalability, we conducted a large-scale study of the 1,808 most popular open-source Java Maven projects on GitHub. The results show that nearly 50% of these projects contain at least one modified, hidden dependency associated with a known vulnerability. On average, each affected project includes more than eight such hidden vulnerable dependencies, all missed by traditional metadata-based scanners. Overall, Unshade identified 7,712 unique CVEs in hidden dependencies that would remain undetected when relying on metadata-based scanning alone.

</details>


### [82] [On the Impact of Code Comments for Automated Bug-Fixing: An Empirical Study](https://arxiv.org/abs/2601.23059)
*Antonio Vitale,Emanuela Guglielmi,Simone Scalabrino,Rocco Oliveto*

Main category: cs.SE

TL;DR: 本研究发现保留代码注释显著提升了自动缺陷修复的效果，挑战了去除注释的传统做法，并通过实验验证了注释在训练和推理中的重要性。


<details>
  <summary>Details</summary>
Motivation: 质疑自动缺陷修复中普遍去除注释的做法，认为注释提供重要设计和实现信息，有助于修复特定类型的缺陷。

Method: 通过比较两种大型语言模型家族在不同训练和推理条件（有无注释）下的表现，并使用LLM自动生成缺失注释，进行实证评估。

Result: 包含注释能使自动缺陷修复的准确率提升三倍，且训练时带注释不会降低在无注释实例上的性能；详细的实现注释特别有助于准确修复。

Conclusion: 代码注释在自动修复软件缺陷中起关键作用，特别是在训练和推理阶段都包含注释时，修复准确率显著提升。

Abstract: Large Language Models (LLMs) are increasingly relevant in Software Engineering research and practice, with Automated Bug Fixing (ABF) being one of their key applications. ABF involves transforming a buggy method into its fixed equivalent. A common preprocessing step in ABF involves removing comments from code prior to training. However, we hypothesize that comments may play a critical role in fixing certain types of bugs by providing valuable design and implementation insights. In this study, we investigate how the presence or absence of comments, both during training and at inference time, impacts the bug-fixing capabilities of LLMs. We conduct an empirical evaluation comparing two model families, each evaluated under all combinations of training and inference conditions (with and without comments), and thereby revisiting the common practice of removing comments during training. To address the limited availability of comments in state-of-the-art datasets, we use an LLM to automatically generate comments for methods lacking them. Our findings show that comments improve ABF accuracy by up to threefold when present in both phases, while training with comments does not degrade performance when instances lack them. Additionally, an interpretability analysis identifies that comments detailing method implementation are particularly effective in aiding LLMs to fix bugs accurately.

</details>


### [83] [Automated Testing of Prevalent 3D User Interactions in Virtual Reality Applications](https://arxiv.org/abs/2601.23139)
*Ruizhen Gu,José Miguel Rojas,Donghwan Shin*

Main category: cs.SE

TL;DR: 本文针对VR自动化测试难题，提出交互流图和XRintTest测试方法，显著提升交互覆盖和测试效率，并能检测异常及设计缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有VR测试难以自动生成和执行真实3D用户输入，且缺乏有效指标衡量交互覆盖，导致自动化测试效果有限。

Method: 构建了交互流图(Interaction Flow Graph)模型，以系统化地描述3D用户交互，并基于该模型设计了XRintTest测试方法，实现动态场景探索和交互执行。

Result: 在XRBench3D基准测试中，XRintTest达到了93%的主要交互类型覆盖率，效率和效果分别比随机测试高6倍和12倍。

Conclusion: 本论文提出了一种创新的自动化VR交互测试方法XRintTest，能够显著提升VR应用中用户交互的覆盖率和测试效率，同时还能发现运行时异常及交互设计缺陷。

Abstract: Virtual Reality (VR) technologies offer immersive user experiences across various domains, but present unique testing challenges compared to traditional software. Existing VR testing approaches enable scene navigation and interaction activation, but lack the ability to automatically synthesise realistic 3D user inputs (e.g, grab and trigger actions via hand-held controllers). Automated testing that generates and executes such input remains an unresolved challenge. Furthermore, existing metrics fail to robustly capture diverse interaction coverage. This paper addresses these gaps through four key contributions. First, we empirically identify four prevalent interaction types in nine open-source VR projects: fire, manipulate, socket, and custom. Second, we introduce the Interaction Flow Graph, a novel abstraction that systematically models 3D user interactions by identifying targets, actions, and conditions. Third, we construct XRBench3D, a benchmark comprising ten VR scenes that encompass 456 distinct user interactions for evaluating VR interaction testing. Finally, we present XRintTest, an automated testing approach that leverages this graph for dynamic scene exploration and interaction execution. Evaluation on XRBench3D shows that XRintTest achieves great effectiveness, reaching 93% coverage of fire, manipulate and socket interactions across all scenes, and performing 12x more effectively and 6x more efficiently than random exploration. Moreover, XRintTest can detect runtime exceptions and non-exception interaction issues, including subtle configuration defects. In addition, the Interaction Flow Graph can reveal potential interaction design smells that may compromise intended functionality and hinder testing performance for VR applications.

</details>


### [84] [From Monolith to Microservices: A Comparative Evaluation of Decomposition Frameworks](https://arxiv.org/abs/2601.23141)
*Mineth Weerasinghe,Himindu Kularathne,Methmini Madhushika,Danuka Lakshan,Nisansa de Silva,Adeesha Wijayasiri,Srinath Perera*

Main category: cs.SE

TL;DR: 本文建立了一个统一评估框架，系统比较了多种微服务分解技术，发现基于层次聚类的HDBScan方法在多个基准系统中表现最好。


<details>
  <summary>Details</summary>
Motivation: 现有的微服务自动分解框架评估存在基准系统不一致、度量不兼容、复现性差等问题，阻碍了不同方法间的客观比较，因此需要一个统一的评估框架。

Method: 本文采用统一的度量计算管线，结合静态、动态和混合的微服务分解技术，评估了多种最先进的自动微服务分解方法，并通过重现已有研究的结果进行对比分析。

Result: 通过对广泛使用的基准系统（JPetStore、AcmeAir、DayTrader和Plants）进行结构模块化、接口数量、跨分区通信、非极端分布等多指标评估，发现层次聚类尤其是HDBScan方法表现出最均衡的分解效果。

Conclusion: 基于分层聚类的方法，尤其是HDBScan，在评估的基准系统中表现最优，能够实现良好的模块化，同时最大限度地减少通信和接口开销。

Abstract: Software modernisation through the migration from monolithic architectures to microservices has become increasingly critical, yet identifying effective service boundaries remains a complex and unresolved challenge. Although numerous automated microservice decomposition frameworks have been proposed, their evaluation is often fragmented due to inconsistent benchmark systems, incompatible metrics, and limited reproducibility, thus hindering objective comparison. This work presents a unified comparative evaluation of state-of-the-art microservice decomposition approaches spanning static, dynamic, and hybrid techniques. Using a consistent metric computation pipeline, we assess the decomposition quality across widely used benchmark systems (JPetStore, AcmeAir, DayTrader, and Plants) using Structural Modularity (SM), Interface Number(IFN), Inter-partition Communication (ICP), Non-Extreme Distribution (NED), and related indicators. Our analysis combines results reported in prior studies with experimentally reproduced outputs from available replication packages. Findings indicate that the hierarchical clustering-based methods, particularly HDBScan, produce the most consistently balanced decompositions across benchmarks, achieving strong modularity while minimizing communication and interface overhead.

</details>


### [85] [Do Good, Stay Longer? Temporal Patterns and Predictors of Newcomer-to-Core Transitions in Conventional OSS and OSS4SG](https://arxiv.org/abs/2601.23142)
*Mohamed Ouf,Amr Mohamed,Mariam Guizani*

Main category: cs.SE

TL;DR: 该文比较OSS4SG与传统OSS中新贡献者转向核心贡献者的过程，发现OSS4SG在留存和核心转变率上表现更优，贡献者先学习后加大贡献速度更快，提示项目使命与贡献路径模式对可持续贡献至关重要。


<details>
  <summary>Details</summary>
Motivation: 当前开源软件的可持续发展依赖于新贡献者转变为核心贡献者，但多数新贡献者贡献后即失效，特别是关注社会公益的OSS4SG项目可能存在不同的转变机制，亟需探索其影响因素。

Method: 通过对375个项目（190个OSS4SG和185个传统OSS），共92,721个贡献者和3.5百万次提交进行定量分析，对比新贡献者向核心贡献者的转变路径和时间模式。

Result: OSS4SG项目贡献者留存率提高2.2倍，核心贡献概率提升19.6%；广泛的早期项目探索对核心达成具有重要影响。OSS4SG展现多样转变路径，且Late Spike模式下贡献者更快成为核心。

Conclusion: OSS4SG项目在新贡献者向核心贡献者转变的留存率和成功率上显著优于传统OSS项目，且贡献者在投入时间先学习项目后加大贡献（Late Spike模式）能更快达到核心状态。

Abstract: Open Source Software (OSS) sustainability relies on newcomers transitioning to core contributors, but this pipeline is broken, with most newcomers becoming inactive after initial contributions. Open Source Software for Social Good (OSS4SG) projects, which prioritize societal impact as their primary mission, may be associated with different newcomer-to-core transition outcomes than conventional OSS projects. We compared 375 projects (190 OSS4SG, 185 OSS), analyzing 92,721 contributors and 3.5 million commits. OSS4SG projects retain contributors at 2.2X higher rates and contributors have 19.6% higher probability of achieving core status. Early broad project exploration predicts core achievement (22.2% importance); conventional OSS concentrates on one dominant pathway (61.62% of transitions) while OSS4SG provides multiple pathways. Contrary to intuition, contributors who invest time learning the project before intensifying their contributions (Late Spike pattern) achieve core status 2.4-2.9X faster (21 weeks) than those who contribute intensively from day one (Early Spike pattern, 51-60 weeks). OSS4SG supports two effective temporal patterns while only Late Spike achieves fastest time-to-core in conventional OSS. Our findings suggest that finding a project aligned with personal values and taking time to understand the codebase before major contributions are key strategies for achieving core status. Our findings show that project mission is associated with measurably different environments for newcomer-to-core transitions and provide evidence-based guidance for newcomers and maintainers.

</details>


### [86] [GrepRAG: An Empirical Study and Optimization of Grep-Like Retrieval for Code Completion](https://arxiv.org/abs/2601.23254)
*Baoyi Wang,Xingliang Wang,Guochang Li,Chen Zhi,Junxiao Han,Xinkui Zhao,Nan Wang,Shuiguang Deng,Jianwei Yin*

Main category: cs.SE

TL;DR: 本文探究了简单的无索引词汇检索在代码库级代码补全中的潜力，基于轻量级搜索工具ripgrep提出了两个方法Naive GrepRAG和改进的GrepRAG。实验表明，GrepRAG在多个数据集上优于复杂图结构方法，展现了无索引检索的有效性和高效性。


<details>
  <summary>Details</summary>
Motivation: 代码库级别的代码补全因跨文件依赖和有限上下文窗口对大模型构成挑战，现有复杂的基于语义索引和图结构的方法计算开销大，本文基于开发者常用的轻量搜索工具探讨更简单的索引无检索机制的效果与必要性。

Method: 提出了Naive GrepRAG作为基线，利用LLM自动生成ripgrep命令实现词汇检索；基于此，设计了GrepRAG，增加了标识符加权重排序和结构感知去重的轻量级后处理管道；在CrossCodeEval和RepoEval-Updated数据集上进行了广泛评估。

Result: Naive GrepRAG在性能上与复杂基线相当，且GrepRAG通过后处理提升代码准确匹配率，分别在CrossCodeEval上较最佳基线提升了7.04%至15.58%的代码准确匹配率，表现出较强的优势。

Conclusion: 轻量级的无索引词汇检索策略在代码补全任务中表现优异，尤其是在开发者常用的简单搜索工具如ripgrep的辅助下，可以达到与复杂图结构方法相当的性能。通过对词汇检索的局限性分析，本文提出的GrepRAG方法通过重新排序和去重机制进一步提升了代码补全的准确率，显著优于现有最先进方法。

Abstract: Repository-level code completion remains challenging for large language models (LLMs) due to cross-file dependencies and limited context windows. Prior work addresses this challenge using Retrieval-Augmented Generation (RAG) frameworks based on semantic indexing or structure-aware graph analysis, but these approaches incur substantial computational overhead for index construction and maintenance. Motivated by common developer workflows that rely on lightweight search utilities (e.g., ripgrep), we revisit a fundamental yet underexplored question: how far can simple, index-free lexical retrieval support repository-level code completion before more complex retrieval mechanisms become necessary? To answer this question, we systematically investigate lightweight, index-free, intent-aware lexical retrieval through extensive empirical analysis. We first introduce Naive GrepRAG, a baseline framework in which LLMs autonomously generate ripgrep commands to retrieve relevant context. Despite its simplicity, Naive GrepRAG achieves performance comparable to sophisticated graph-based baselines. Further analysis shows that its effectiveness stems from retrieving lexically precise code fragments that are spatially closer to the completion site. We also identify key limitations of lexical retrieval, including sensitivity to noisy matches from high-frequency ambiguous keywords and context fragmentation caused by rigid truncation boundaries. To address these issues, we propose GrepRAG, which augments lexical retrieval with a lightweight post-processing pipeline featuring identifier-weighted re-ranking and structure-aware deduplication. Extensive evaluation on CrossCodeEval and RepoEval-Updated demonstrates that GrepRAG consistently outperforms state-of-the-art (SOTA) methods, achieving 7.04-15.58 percent relative improvement in code exact match (EM) over the best baseline on CrossCodeEval.

</details>


### [87] [Outcome-Conditioned Reasoning Distillation for Resolving Software Issues](https://arxiv.org/abs/2601.23257)
*Chenglin Li,Yisen Xu,Zehao Wang,Shin Hwei Tan,Tse-Hsun,Chen*

Main category: cs.SE

TL;DR: 本文提出O-CRD框架，通过复用仓库中已验证的修复经验，提升软件缺陷修复的效率和准确性，显著优于传统的重置式修复方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型修复流程在解决新问题时会重新推理，忽视仓库中已有的修复经验，导致资源浪费和效率低下。

Method: 提出O-CRD框架，从已验证的历史修复开始，逆向重建分阶段修复过程轨迹，并在推理时复用这些经验指导，无需微调或在线搜索。

Result: 在SWE-Bench Lite数据集上，O-CRD框架分别提升GPT-4o、DeepSeek-V3和GPT-5的Pass@1指标约8.6%至10.4%，验证了该方法的有效性。

Conclusion: 基于结果条件指导的推理蒸馏框架（O-CRD）能够有效利用仓库中已解决问题的修复经验，提升软件问题定位和补丁合成的准确性，减少推理成本。

Abstract: Software issue resolution in large repositories is a long-range decision process: choices made during localization shape the space of viable edits, and missteps can compound into incorrect patches. Despite this, many LLM-based repair pipelines still operate in a reset-and-solve manner, producing fresh reasoning for every new issue instead of carrying forward what worked in past fixes. This is wasteful because repositories routinely contain earlier issues with overlapping structure, failure modes, or constraints, where prior repair experience could provide useful guidance. Existing approaches typically harvest this signal through forward-time trial procedures, such as repeated refinement or search, incurring high inference cost while still risking divergence from the eventual correct patch. We present an Outcome-Conditioned Reasoning Distillation(O-CRD) framework that uses resolved in-repository issues with verified patches as supervision. Starting from a historical fix, the method reconstructs a stage-wise repair trace backward from the verified outcome, then reuses the distilled guidance at inference time to steer file/function localization and patch synthesis, without fine-tuning or online search. On SWE-Bench Lite, this approach increases Pass@1 by 10.4% with GPT-4o, 8.6% with DeepSeek-V3, and 10.3% with GPT-5, indicating that outcome-conditioned reuse of verified repairs can replace costly forward exploration for software issue resolution.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [88] [Learning to Recommend Multi-Agent Subgraphs from Calling Trees](https://arxiv.org/abs/2601.22209)
*Xinyuan Song,Liang Zhao*

Main category: cs.MA

TL;DR: 针对多智能体系统中代理选择难题，本文提出基于调用树的约束推荐框架，通过效用优化结合多维评价指标，实现了更精准的代理和团队推荐。


<details>
  <summary>Details</summary>
Motivation: 随着多智能体市场的快速扩展，候选代理功能出现重叠，简单的检索已不能满足选择需求，需要考虑代理的可靠性、兼容性及合作性，现有推荐系统难以处理代理编排中的结构化、顺序和交互依赖问题。

Method: 引入了一个通用的约束推荐框架，先进行检索以构建与当前子任务和上下文相关的紧凑候选集，然后在此可行集合中利用学习得到的评分函数进行效用优化，评分函数考虑相关性、可靠性和交互效应。

Result: 通过构建统一调用树基准，将多种多智能体日志归一化为统一的结构化表示，实现了对代理级和系统级推荐的系统性评估，验证了提出框架的有效性。

Conclusion: 本文提出了一种针对多智能体系统（MAS）中代理推荐的约束决策问题建模方法，通过利用历史调用树数据，实现了比传统推荐系统更符合MAS执行结构的推荐效果。

Abstract: Multi-agent systems (MAS) increasingly solve complex tasks by orchestrating agents and tools selected from rapidly growing marketplaces. As these marketplaces expand, many candidates become functionally overlapping, making selection not just a retrieval problem: beyond filtering relevant agents, an orchestrator must choose options that are reliable, compatible with the current execution context, and able to cooperate with other selected agents. Existing recommender systems -- largely built for item-level ranking from flat user-item logs -- do not directly address the structured, sequential, and interaction-dependent nature of agent orchestration. We address this gap by \textbf{formulating agent recommendation in MAS as a constrained decision problem} and introducing a generic \textbf{constrained recommendation framework} that first uses retrieval to build a compact candidate set conditioned on the current subtask and context, and then performs \textbf{utility optimization} within this feasible set using a learned scorer that accounts for relevance, reliability, and interaction effects. We ground both the formulation and learning signals in \textbf{historical calling trees}, which capture the execution structure of MAS (parent-child calls, branching dependencies, and local cooperation patterns) beyond what flat logs provide. The framework supports two complementary settings: \textbf{agent-level recommendation} (select the next agent/tool) and \textbf{system-level recommendation} (select a small, connected agent team/subgraph for coordinated execution). To enable systematic evaluation, we construct a unified calling-tree benchmark by normalizing invocation logs from eight heterogeneous multi-agent corpora into a shared structured representation.

</details>


### [89] [Aligning Microscopic Vehicle and Macroscopic Traffic Statistics: Reconstructing Driving Behavior from Partial Data](https://arxiv.org/abs/2601.22242)
*Zhihao Zhang,Keith Redmill,Chengyang Peng,Bowen Weng*

Main category: cs.MA

TL;DR: 本文提出一种结合微观和宏观数据的新框架，通过重建未观测微观状态并学习共享策略，实现自动驾驶车辆与人类司机的安全高效协同。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖高质量真实驾驶行为观测数据难以获得，微观传感器缺乏环境宏观上下文，路侧传感器缺少车辆微观行为关联，二者互补激发提出新框架的需求。

Method: 利用微观数据锚定观察到的车辆行为，从宏观观察中重建未观测的微观状态，并学习共享策略，保证行为在微观上与部分观察到的轨迹和动作一致，在宏观上符合目标交通统计特征。

Result: 构建的框架有效结合微观和宏观数据，能够生成符合真实交通流和人车协同要求的驾驶策略，有助于提升自动驾驶安全性和效率。

Conclusion: 提出的框架能够通过宏观观测数据和部分微观数据重建并学习驾驶策略，实现微观行为一致性和宏观交通统计一致性，促进现实交通流模式并安全协同人类驾驶员。

Abstract: A driving algorithm that aligns with good human driving practices, or at the very least collaborates effectively with human drivers, is crucial for developing safe and efficient autonomous vehicles. In practice, two main approaches are commonly adopted: (i) supervised or imitation learning, which requires comprehensive naturalistic driving data capturing all states that influence a vehicle's decisions and corresponding actions, and (ii) reinforcement learning (RL), where the simulated driving environment either matches or is intentionally more challenging than real-world conditions. Both methods depend on high-quality observations of real-world driving behavior, which are often difficult and costly to obtain. State-of-the-art sensors on individual vehicles can gather microscopic data, but they lack context about the surrounding conditions. Conversely, roadside sensors can capture traffic flow and other macroscopic characteristics, but they cannot associate this information with individual vehicles on a microscopic level. Motivated by this complementarity, we propose a framework that reconstructs unobserved microscopic states from macroscopic observations, using microscopic data to anchor observed vehicle behaviors, and learns a shared policy whose behavior is microscopically consistent with the partially observed trajectories and actions and macroscopically aligned with target traffic statistics when deployed population-wide. Such constrained and regularized policies promote realistic flow patterns and safe coordination with human drivers at scale.

</details>


### [90] [Learning Reward Functions for Cooperative Resilience in Multi-Agent Systems](https://arxiv.org/abs/2601.22292)
*Manuela Chacon-Chamorro,Luis Felipe Giraldo,Nicanor Quijano*

Main category: cs.MA

TL;DR: 本文研究奖励函数设计对混合动机多智能体系统中合作韧性的影响，提出一种基于排名轨迹学习奖励函数的新框架，实验表明混合奖励策略能有效增强系统抵抗干扰的能力，促进稳定合作。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统常在动态和不确定环境中运行，个体不仅要完成自身目标，还需维护集体功能，尤其在混合动机场景下，合作韧性是关键但尚缺乏充分研究。

Method: 提出一个基于合作韧性指标的奖励函数学习框架，从排名轨迹中学习奖励函数，采用三种奖励策略（传统个体奖励、韧性推断奖励和混合策略）以及三种奖励参数化方法（线性模型、手工特征和神经网络），并应用两种基于偏好的学习算法从行为排名中推断奖励。

Result: 混合奖励策略显著提升了系统在扰动时的鲁棒性，无损任务性能且减少了资源过度使用等灾难性结果，验证了奖励设计对韧性合作的重要性。

Conclusion: 设计奖励函数对于促进多智能体系统中的韧性合作至关重要，混合奖励策略在应对环境扰动时表现出更高的鲁棒性且不降低任务性能，有助于减少灾难性后果如资源过度使用。

Abstract: Multi-agent systems often operate in dynamic and uncertain environments, where agents must not only pursue individual goals but also safeguard collective functionality. This challenge is especially acute in mixed-motive multi-agent systems. This work focuses on cooperative resilience, the ability of agents to anticipate, resist, recover, and transform in the face of disruptions, a critical yet underexplored property in Multi-Agent Reinforcement Learning. We study how reward function design influences resilience in mixed-motive settings and introduce a novel framework that learns reward functions from ranked trajectories, guided by a cooperative resilience metric. Agents are trained in a suite of social dilemma environments using three reward strategies: i) traditional individual reward; ii) resilience-inferred reward; and iii) hybrid that balance both. We explore three reward parameterizations-linear models, hand-crafted features, and neural networks, and employ two preference-based learning algorithms to infer rewards from behavioral rankings. Our results demonstrate that hybrid strategy significantly improve robustness under disruptions without degrading task performance and reduce catastrophic outcomes like resource overuse. These findings underscore the importance of reward design in fostering resilient cooperation, and represent a step toward developing robust multi-agent systems capable of sustaining cooperation in uncertain environments.

</details>


### [91] [ScholarPeer: A Context-Aware Multi-Agent Framework for Automated Peer Review](https://arxiv.org/abs/2601.22638)
*Palash Goyal,Mihir Parmar,Yiwen Song,Hamid Palangi,Tomas Pfister,Jinsung Yoon*

Main category: cs.MA

TL;DR: ScholarPeer是一种多智能体框架，通过实时检索和多角度验证提升自动同行评审的深度和准确性，在评测中优于当前先进系统，接近人类专家水平。


<details>
  <summary>Details</summary>
Motivation: 现有自动同行评审系统缺乏外部上下文支持，难以准确评价论文的新颖性和重要性，且容易忽略深层次的方法论缺陷。

Method: 提出了ScholarPeer框架，结合多智能体协作，通过历史学家代理构建领域叙事、基线侦察员识别漏缺比较、以及多方面问答引擎验证论点，实现基于实时网络文献的评审。

Result: 在DeepReview-13K数据集上的实验结果显示，ScholarPeer在对比评估中明显优于现有顶尖方法，且在多样性方面接近人类水平。

Conclusion: ScholarPeer显著提升了自动同行评审的质量，特别是在识别深层次的方法论缺陷和准确评价论文的新颖性与重要性方面，缩小了与人类专家之间的差距。

Abstract: Automated peer review has evolved from simple text classification to structured feedback generation. However, current state-of-the-art systems still struggle with "surface-level" critiques: they excel at summarizing content but often fail to accurately assess novelty and significance or identify deep methodological flaws because they evaluate papers in a vacuum, lacking the external context a human expert possesses. In this paper, we introduce ScholarPeer, a search-enabled multi-agent framework designed to emulate the cognitive processes of a senior researcher. ScholarPeer employs a dual-stream process of context acquisition and active verification. It dynamically constructs a domain narrative using a historian agent, identifies missing comparisons via a baseline scout, and verifies claims through a multi-aspect Q&A engine, grounding the critique in live web-scale literature. We evaluate ScholarPeer on DeepReview-13K and the results demonstrate that ScholarPeer achieves significant win-rates against state-of-the-art approaches in side-by-side evaluations and reduces the gap to human-level diversity.

</details>


### [92] [LLMDR: Large language model driven framework for missing data recovery in mixed data under low resource regime](https://arxiv.org/abs/2601.22916)
*Durga Keshav,GVD Praneeth,Chetan Kumar Patruni,Vivek Yelleti,U Sai Ram*

Main category: cs.MA

TL;DR: 针对高缺失率和混合型数据恢复难题，提出两阶段LLMDR方法，结合聚类、多大模型和共识算法，实验验证效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有缺失数据填充方法在缺失率较高时效果降低，且难以处理混合类型数据。

Method: 提出LLMDR框架，采用两阶段方法：第一阶段利用DBSCAN聚类选择代表样本，第二阶段使用多大模型(LLMs)对局部和全局代表样本进行数据恢复，最后通过共识算法融合结果。

Result: 实验表明该方法在多种混合数据集上，在准确率、KS统计量、SMAPE和MSE指标上表现良好，且共识机制提高了最终推荐的准确性。

Conclusion: LLMDR框架有效解决了高缺失率和混合型数据的恢复问题，展现出优越的性能和泛化能力。

Abstract: The missing data problem is one of the important issues to address for achieving data quality. While imputation-based methods are designed to achieve data completeness, their efficacy is observed to be diminishing as and when there is increasing in the missingness percentage. Further, extant approaches often struggle to handle mixed-type datasets, typically supporting either numerical and/or categorical data. In this work, we propose LLMDR, automatic data recovery framework which operates in two stage approach, wherein the Stage-I: DBSCAN clustering algorithm is employed to select the most representative samples and in the Stage-II: Multi-LLMs are employed for data recovery considering the local and global representative samples; Later, this framework invokes the consensus algorithm for recommending a more accurate value based on other LLMs of local and global effective samples. Experimental results demonstrate that proposed framework works effectively on various mixed datasets in terms of Accuracy, KS-Statistic, SMAPE, and MSE. Further, we have also shown the advantage of the consensus mechanism for final recommendation in mixed-type data.

</details>


### [93] [Multi-Agent Systems Should be Treated as Principal-Agent Problems](https://arxiv.org/abs/2601.23211)
*Paulius Rauba,Simonas Cepenas,Mihaela van der Schaar*

Main category: cs.MA

TL;DR: 本文从微观经济学委托-代理问题出发，分析多智能体系统尤其是LLM代理的策划行为，揭示信息不对称和激励不一致带来的风险，并借助机制设计理论提出应对策略。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统存在信息不对称和激励不一致的问题，近年来发现基于大型语言模型(LLM)的代理可能产生“策划”行为，即追求自身目标并可能欺骗其他代理，导致委托人与代理目标不一致，系统行为偏差。

Method: 基于微观经济学中委托-代理问题的理论框架，分析多智能体系统中信息不对称和目标不一致的现象；以LLM代理的策划行为作为案例研究，映射相关术语与机制设计文献中的概念，提出具体的缓解策略。

Result: 证明多智能体系统天然存在信息不对称，LLM代理的策划行为对应机制设计中的已知问题，利用机制设计工具能够更好地理解和缓解代理失效现象，推动将人类代理行为分析工具应用于非人类代理。

Conclusion: 该研究以委托-代理理论为视角，阐明多智能体系统中信息不对称和激励不一致问题，揭示LLM策划行为的本质并指明基于机制设计的解决方案，强调跨领域理论方法在非人类代理分析中的价值。

Abstract: Consider a multi-agent systems setup in which a principal (a supervisor agent) assigns subtasks to specialized agents and aggregates their responses into a single system-level output. A core property of such systems is information asymmetry: agents observe task-specific information, produce intermediate reasoning traces, and operate with different context windows. In isolation, such asymmetry is not problematic, since agents report truthfully to the principal when incentives are fully aligned. However, this assumption breaks down when incentives diverge. Recent evidence suggests that LLM-based agents can acquire their own goals, such as survival or self-preservation, a phenomenon known as scheming, and may deceive humans or other agents. This leads to agency loss: a gap between the principal's intended outcome and the realized system behavior. Drawing on core ideas from microeconomic theory, we argue that these characteristics, information asymmetry and misaligned goals, are best studied through the lens of principal-agent problems. We explain why multi-agent systems, both human-to-LLM and LLM-to-LLM, naturally induce information asymmetry under this formulation, and we use scheming, where LLM agents pursue covert goals, as a concrete case study. We show that recently introduced terminology used to describe scheming, such as covert subversion or deferred subversion, corresponds to well-studied concepts in the mechanism design literature, which not only characterizes the problem but also prescribes concrete mitigation strategies. More broadly, we argue for applying tools developed to study human agent behavior to the analysis of non-human agents.

</details>


### [94] [MonoScale: Scaling Multi-Agent System with Monotonic Improvement](https://arxiv.org/abs/2601.23219)
*Shuai Shao,Yixiang Liu,Bingwei Lu,Weinan Zhang*

Main category: cs.MA

TL;DR: 为解决多智能体系统扩展代理池时的性能崩溃问题，本文提出MonoScale框架，通过任务熟悉、记忆蒸馏和信赖域更新，保障了系统扩展过程中的性能稳定性和持续改进。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统扩展代理池以增强能力时，由于新加入的代理异质且不可靠，传统的路由器在冷启动阶段常导致性能崩溃，需要一种方法保证扩展过程中的性能稳定。

Method: 设计了一种扩展感知的更新框架MonoScale，通过生成代理条件化的熟悉任务，收集成功与失败的交互证据，并将其提炼成可审计的自然语言记忆以指导未来的任务路由，同时将顺序增强形式化为上下文bandit并采用信赖域记忆更新策略。

Result: 在GAIA和Humanity's Last Exam两个任务上，MonoScale随着代理池的扩大表现出稳定的性能提升，优于简单扩展和固定代理池的强路由基线。

Conclusion: 提出的MonoScale框架能稳定提升多智能体系统在持续扩大代理池时的性能，避免了传统方法在扩容时的性能崩溃问题。

Abstract: In recent years, LLM-based multi-agent systems (MAS) have advanced rapidly, using a router to decompose tasks and delegate subtasks to specialized agents. A natural way to expand capability is to scale up the agent pool by continually integrating new functional agents or tool interfaces, but naive expansion can trigger performance collapse when the router cold-starts on newly added, heterogeneous, and unreliable agents. We propose MonoScale, an expansion-aware update framework that proactively generates a small set of agent-conditioned familiarization tasks, harvests evidence from both successful and failed interactions, and distills it into auditable natural-language memory to guide future routing. We formalize sequential augmentation as a contextual bandit and perform trust-region memory updates, yielding a monotonic non-decreasing performance guarantee across onboarding rounds. Experiments on GAIA and Humanity's Last Exam show stable gains as the agent pool grows, outperforming naive scale-up and strong-router fixed-pool baselines.

</details>
