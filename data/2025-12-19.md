<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 31]
- [cs.SE](#cs.SE) [Total: 11]
- [cs.MA](#cs.MA) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [TabReX : Tabular Referenceless eXplainable Evaluation](https://arxiv.org/abs/2512.15907)
*Tejas Anvekar,Juhna Park,Aparna Garimella,Vivek Gupta*

Main category: cs.CL

TL;DR: 本文提出TabReX，一种基于图模型的无参考表格生成质量评估框架，通过图结构对齐和打分实现对表格结构和事实的准确评价。


<details>
  <summary>Details</summary>
Motivation: 现有表格生成质量评估方法要么忽略表格结构，要么依赖固定参考，限制了泛化能力，亟需一种更通用且解释性强的评估方法。

Method: TabReX将源文本和生成表格转换为规范知识图谱，通过大语言模型指导的匹配过程实现图谱对齐，并计算解释性的、基于评分标准的结构与事实忠实度分数。

Result: TabReX在六个领域和多种扰动条件下表现出与专家排名最高相关性，稳定性强，并支持细粒度的模型与提示分析。

Conclusion: TabReX为结构化生成系统提供了一种可信、可解释的评价新范式，推动了表格生成质量评估的进步。

Abstract: Evaluating the quality of tables generated by large language models (LLMs) remains an open challenge: existing metrics either flatten tables into text, ignoring structure, or rely on fixed references that limit generalization. We present TabReX, a reference-less, property-driven framework for evaluating tabular generation via graph-based reasoning. TabReX converts both source text and generated tables into canonical knowledge graphs, aligns them through an LLM-guided matching process, and computes interpretable, rubric-aware scores that quantify structural and factual fidelity. The resulting metric provides controllable trade-offs between sensitivity and specificity, yielding human-aligned judgments and cell-level error traces. To systematically asses metric robustness, we introduce TabReX-Bench, a large-scale benchmark spanning six domains and twelve planner-driven perturbation types across three difficulty tiers. Empirical results show that TabReX achieves the highest correlation with expert rankings, remains stable under harder perturbations, and enables fine-grained model-vs-prompt analysis establishing a new paradigm for trustworthy, explainable evaluation of structured generation systems.

</details>


### [2] [Social Story Frames: Contextual Reasoning about Narrative Intent and Reception](https://arxiv.org/abs/2512.15925)
*Joel Mire,Maria Antoniak,Steven R. Wilson,Zexin Ma,Achyutarama R. Ganti,Andrew Piper,Maarten Sap*

Main category: cs.CL

TL;DR: 本文提出了SocialStoryFrames形式主义，通过对话语境和叙事、语言、心理学理论的归纳，解析读者的丰富反应，开发了生成和分类模型，验证后应用于6,140条社交媒体故事数据，揭示了讲故事意图和社区叙事实践的差异。


<details>
  <summary>Details</summary>
Motivation: 现有计算模型难以捕捉读者对故事的细腻反应，如意图推理和情感评价，限制了对故事叙述的深入分析。

Method: 基于叙事理论、语言语用学和心理学分类，设计SocialStoryFrames形式主义；开发SSF-Generator和SSF-Classifier模型；通过382名参与者的问卷与专家注释验证模型；构建SSF-Corpus数据集进行模型应用。

Result: 模型成功解析了读者对故事的多维反应，揭示了讲故事意图的频率及相互依赖关系，比较了不同社区的叙事实践及其多样性。

Conclusion: SocialStoryFrames形式主义将细致、语境敏感的读者反应建模与通用分类方法结合，推动了在线社区讲故事研究，实现了大规模故事分析的新途径。

Abstract: Reading stories evokes rich interpretive, affective, and evaluative responses, such as inferences about narrative intent or judgments about characters. Yet, computational models of reader response are limited, preventing nuanced analyses. To address this gap, we introduce SocialStoryFrames, a formalism for distilling plausible inferences about reader response, such as perceived author intent, explanatory and predictive reasoning, affective responses, and value judgments, using conversational context and a taxonomy grounded in narrative theory, linguistic pragmatics, and psychology. We develop two models, SSF-Generator and SSF-Classifier, validated through human surveys (N=382 participants) and expert annotations, respectively. We conduct pilot analyses to showcase the utility of the formalism for studying storytelling at scale. Specifically, applying our models to SSF-Corpus, a curated dataset of 6,140 social media stories from diverse contexts, we characterize the frequency and interdependence of storytelling intents, and we compare and contrast narrative practices (and their diversity) across communities. By linking fine-grained, context-sensitive modeling with a generic taxonomy of reader responses, SocialStoryFrames enable new research into storytelling in online communities.

</details>


### [3] [BRAID: Bounded Reasoning for Autonomous Inference and Decisions](https://arxiv.org/abs/2512.15959)
*Armağan Amcalar,Eyup Cinar*

Main category: cs.CL

TL;DR: 本文研究了通过BRAID结构化提示方法在多个GPT模型上的性能表现，提升了推理准确性与成本效率。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在性能、成本和令牌使用上呈非线性关系，亟需提高推理效率和准确性的方法。

Method: 提出BRAID框架，利用基于Mermaid的指令图进行有界推理，替代无界的自然语言扩展提示，实现结构化的机器可读提示。

Result: 实验表明，BRAID显著提升了多个GPT模型在AdvancedIF、GSM-Hard及SCALE MultiChallenge基准上的推理准确率与成本效率。

Conclusion: BRAID是一种有效且可拓展的技术，能够优化自主智能代理系统中的推理效率，适合实际生产环境。

Abstract: Large Language Models (LLMs) exhibit nonlinear relationships between performance, cost, and token usage. This paper presents a quantitative study on structured prompting using BRAID (Bounded Reasoning for Au tonomous Inference and Decisions) across multiple GPT model tiers, eval uated on the AdvancedIF, GSM-Hard, and the SCALE MultiChallenge benchmark datasets. BRAID introduces a bounded reasoning framework using Mermaid-based instruction graphs that enable models to reason struc turally rather than through unbounded natural-language token expansion. We show that structured machine-readable prompts substantially increase reasoning accuracy and cost efficiency for agents in production systems. The findings establish BRAID as an effective and scalable technique for optimizing inference efficiency in autonomous agent systems. All datasets and detailed result logs are available at https://benchmark.openserv.ai.

</details>


### [4] [Examining the Utility of Self-disclosure Types for Modeling Annotators of Social Norms](https://arxiv.org/abs/2512.16034)
*Kieran Henderson,Kian Omoomi,Vasudha Varadarajan,Allison Lahnala,Charles Welch*

Main category: cs.CL

TL;DR: 本文研究了不同类型的个人信息对预测主观任务中标注者标签的影响，发现人口统计信息最具预测力，理论驱动的方法优于自动聚类，且多样化的标注者自我披露样本能提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 以往研究使用有限的个人信息来改善主观任务中个体特征建模和标注者标签预测，但对哪些信息最具预测价值缺乏深入探讨。

Method: 将自我披露句子进行分类，建立标注者模型预测社会规范判断，进行多种消融实验和分析比较不同信息类型的影响。

Result: 发现人口统计信息比态度、关系和经历更具影响力，基于理论的方法表现优于自动聚类，且只需少量相关评论即可，标注者自我披露样本越多样化，性能越好。

Conclusion: 多样化且基于理论的个人信息分类能有效提升主观任务中标注者标签的预测准确性，人口统计信息是关键因素。

Abstract: Recent work has explored the use of personal information in the form of persona sentences or self-disclosures to improve modeling of individual characteristics and prediction of annotator labels for subjective tasks. The volume of personal information has historically been restricted and thus little exploration has gone into understanding what kind of information is most informative for predicting annotator labels. In this work, we categorize self-disclosure sentences and use them to build annotator models for predicting judgments of social norms. We perform several ablations and analyses to examine the impact of the type of information on our ability to predict annotation patterns. We find that demographics are more impactful than attitudes, relationships, and experiences. Generally, theory-based approaches worked better than automatic clusters. Contrary to previous work, only a small number of related comments are needed. Lastly, having a more diverse sample of annotator self-disclosures leads to the best performance.

</details>


### [5] [Are We on the Right Way to Assessing LLM-as-a-Judge?](https://arxiv.org/abs/2512.16041)
*Yuanning Feng,Sinan Wang,Zhengxiang Cheng,Yao Wan,Dongping Chen*

Main category: cs.CL

TL;DR: 本文提出Sage评估套件，无需人类标注，通过局部自洽性和全局逻辑一致性评估大语言模型（LLM）作为评判者的表现，发现当前顶尖模型在判定一致性上存在显著问题，且人类评价也有较大不一致。


<details>
  <summary>Details</summary>
Motivation: 现有用于评估LLM作为评判者的方法依赖人类标注存在偏见且难以扩展，亟需一个无需人类注释且能可靠评估LLM评判质量的方案。

Method: Sage采用理性选择理论公理，定义局部自洽性（成对偏好稳定性）和全局逻辑一致性（偏好传递性）两种指标，结合结构化基准题与真实用户查询数据构建评估数据集，验证指标稳定性及与有监督基准的相关性。

Result: 实验表明Sage评估指标稳定且与现有有监督基准强相关。顶尖LLM如Gemini-2.5-Pro和GPT-5在困难样本中约25%偏好不一致，揭示“情境偏好”现象。微调和专家组评判及深度推理均能提升一致性。人类评判也存在显著不一致。

Conclusion: Sage提供了无需人类标注的有效LLM评判者评估工具，揭示了当前模型和人类评判中的一致性问题，支持通过微调和多评审策略提高性能。

Abstract: LLM-as-a-Judge has been widely adopted as an evaluation method and served as supervised rewards in model training. However, existing benchmarks for LLM-as-a-Judge are mainly relying on human-annotated ground truth, which introduces human bias that undermines the assessment of reliability and imposes scalability constraints. To overcome these limitations, we introduce Sage, a novel evaluation suite that assesses the quality of LLM judges without necessitating any human annotation. Inspired by axioms of rational choice theory, Sage introduces two new lenses for measuring LLM-as-a-Judge: local self-consistency (pair-wise preference stability) and global logical consistency (transitivity across a full set of preferences). We curate a dataset of 650 questions by combining structured benchmark problems with real-world user queries. Our experiments demonstrate both the stability of our metrics and their high correlation with supervised benchmarks like LLMBar and RewardBench2, confirming Sage's reliability as an evaluation suite for the robustness and accuracy of LLM-as-a-Judge. Based on Sage, we reveal that current state-of-the-art LLMs exhibit significant reliability problems when acting as judges in both scoring and pairwise settings; even the top-performing models, Gemini-2.5-Pro and GPT-5, fail to maintain consistent preferences in nearly a quarter of difficult cases. We attribute this to a new phenomenon called situational preference, which explains why explicit rubrics or criteria can help the model judge consistently across answer pairs. Our further analysis shows that finetuned LLM-as-a-Judge is a feasible method to boost performance, and the panel-based judge as well as deep reasoning can enhance the judging consistency. We also find substantial inconsistency in human judgments, which indicates that human annotation may not be a reliable gold standard.

</details>


### [6] [Convolutional Lie Operator for Sentence Classification](https://arxiv.org/abs/2512.16125)
*Daniela N. Rim,Heeyoul Choi*

Main category: cs.CL

TL;DR: 本文提出将李代数卷积引入卷积句子分类器，提升了模型对复杂语言变换的捕捉能力，实验表明该方法优于传统卷积网络。


<details>
  <summary>Details</summary>
Motivation: 传统卷积神经网络对局部位置不变特征表现良好，但在捕捉语言的复杂变换方面有待加强。

Method: 通过引入李群操作的李代数卷积到基于卷积的句子分类模型中，设计了SCLie和DPCLie模型。

Result: 所提模型在实验中优于传统卷积句子分类器，准确率有所提升。

Conclusion: 李代数卷积能够捕捉语言中的复杂变换，推动了语言建模新范式的探索。

Abstract: Traditional Convolutional Neural Networks have been successful in capturing local, position-invariant features in text, but their capacity to model complex transformation within language can be further explored. In this work, we explore a novel approach by integrating Lie Convolutions into Convolutional-based sentence classifiers, inspired by the ability of Lie group operations to capture complex, non-Euclidean symmetries. Our proposed models SCLie and DPCLie empirically outperform traditional Convolutional-based sentence classifiers, suggesting that Lie-based models relatively improve the accuracy by capturing transformations not commonly associated with language. Our findings motivate more exploration of new paradigms in language modeling.

</details>


### [7] [MRG-R1: Reinforcement Learning for Clinically Aligned Medical Report Generation](https://arxiv.org/abs/2512.16145)
*Pengyu Wang,Shuchang Ye,Usman Naseem,Jinman Kim*

Main category: cs.CL

TL;DR: 本论文提出了一种基于语义驱动强化学习的方法，用于改善医用影像报告的临床准确性，实现自动生成更符合医学标准的放射学报告。


<details>
  <summary>Details</summary>
Motivation: 现有自动生成医疗报告的方法在模仿语言风格方面表现良好，但缺乏临床准确性，因为训练目标侧重于词汇和句法，而非医学内容的正确性。

Method: 提出语义驱动强化学习（SRL）方法，利用群体相对策略优化（GRPO）和基于边缘的余弦相似度奖励函数，直接用关键放射学发现的语义相似度作为报告级别奖励，提升临床标签一致性，同时引入轻量级推理格式约束生成结构化报告。

Result: 在IU X-Ray和MIMIC-CXR两个数据集上，以临床效能指标（CE指标）评估，方法分别达到CE-F1 51.88和40.39，优于传统的基于词级监督的模型，表现出更好的临床正确性。

Conclusion: 通过优化临床语义级别奖励而非传统词级奖励，显著提升了医用影像报告的临床准确性，揭示了语义强化对医学大规模视觉语言模型训练的重要价值。

Abstract: Medical report generation (MRG) aims to automatically derive radiology-style reports from medical images to aid in clinical decision-making. However, existing methods often generate text that mimics the linguistic style of radiologists but fails to guarantee clinical correctness, because they are trained on token-level objectives which focus on word-choice and sentence structure rather than actual medical accuracy. We propose a semantic-driven reinforcement learning (SRL) method for medical report generation, adopted on a large vision-language model (LVLM). SRL adopts Group Relative Policy Optimization (GRPO) to encourage clinical-correctness-guided learning beyond imitation of language style. Specifically, we optimise a report-level reward: a margin-based cosine similarity (MCCS) computed between key radiological findings extracted from generated and reference reports, thereby directly aligning clinical-label agreement and improving semantic correctness. A lightweight reasoning format constraint further guides the model to generate structured "thinking report" outputs. We evaluate Medical Report Generation with Sematic-driven Reinforment Learning (MRG-R1), on two datasets: IU X-Ray and MIMIC-CXR using clinical efficacy (CE) metrics. MRG-R1 achieves state-of-the-art performance with CE-F1 51.88 on IU X-Ray and 40.39 on MIMIC-CXR. We found that the label-semantic reinforcement is better than conventional token-level supervision. These results indicate that optimizing a clinically grounded, report-level reward rather than token overlap,meaningfully improves clinical correctness. This work is a prior to explore semantic-reinforcement in supervising medical correctness in medical Large vision-language model(Med-LVLM) training.

</details>


### [8] [Decoding Fake Narratives in Spreading Hateful Stories: A Dual-Head RoBERTa Model with Multi-Task Learning](https://arxiv.org/abs/2512.16147)
*Yash Bhaskar,Sankalp Bahad,Parameswari Krishnamurthy*

Main category: cs.CL

TL;DR: 本文针对社交媒体中由假叙事驱动的仇恨言论检测问题，提出了一个结合多任务学习的系统，实现了二元仇恨检测和目标与严重程度预测两项任务，取得了较好效果。


<details>
  <summary>Details</summary>
Motivation: 社交媒体快速传播有害内容，特别是由虚假叙事驱动的仇恨言论——Faux-Hate，亟需有效检测方法。

Method: 结合先进的自然语言处理技术和领域特定预训练，采用多任务学习同时完成仇恨言论的二元分类以及其目标和严重程度预测。

Result: 系统在共享任务中取得了具有竞争力的结果，验证了多任务学习方法在该复杂问题上的有效性。

Conclusion: 通过多任务学习和领域预训练技术，可以有效检测并分类由假叙事驱动的仇恨言论，实现对社交媒体有害内容的精确识别。

Abstract: Social media platforms, while enabling global connectivity, have become hubs for the rapid spread of harmful content, including hate speech and fake narratives \cite{davidson2017automated, shu2017fake}. The Faux-Hate shared task focuses on detecting a specific phenomenon: the generation of hate speech driven by fake narratives, termed Faux-Hate. Participants are challenged to identify such instances in code-mixed Hindi-English social media text. This paper describes our system developed for the shared task, addressing two primary sub-tasks: (a) Binary Faux-Hate detection, involving fake and hate speech classification, and (b) Target and Severity prediction, categorizing the intended target and severity of hateful content. Our approach combines advanced natural language processing techniques with domain-specific pretraining to enhance performance across both tasks. The system achieved competitive results, demonstrating the efficacy of leveraging multi-task learning for this complex problem.

</details>


### [9] [A Domain-Adapted Pipeline for Structured Information Extraction from Police Incident Announcements on Social Media](https://arxiv.org/abs/2512.16183)
*Mengfan Shen,Kangqi Song,Xindi Wang,Wei Jia,Tao Wang,Ziqiang Han*

Main category: cs.CL

TL;DR: 本文针对警方事件公告中结构化信息抽取难题，提出基于Qwen2.5-7B模型的LoRA微调方法，实现了高准确率的多字段抽取。


<details>
  <summary>Details</summary>
Motivation: 警方事件公告文本多样且非正式，难以准确提取结构化信息，影响数据及时处理和应用。

Method: 采用参数高效的LoRA微调技术，对Qwen2.5-7B模型进行领域适配，通过针对性提示工程处理噪声和异构文本，抽取15个关键字段。

Result: 在4,933条手工标注数据上，模型在死亡人数检测准确率达98.36%，死亡人数和省级地点的精确匹配率均超过95%。

Conclusion: 所提方法显著提升了结构化信息抽取准确性，为社会科学领域从非结构化文本构建可靠结构化数据提供了有效方案。

Abstract: Structured information extraction from police incident announcements is crucial for timely and accurate data processing, yet presents considerable challenges due to the variability and informal nature of textual sources such as social media posts. To address these challenges, we developed a domain-adapted extraction pipeline that leverages targeted prompt engineering with parameter-efficient fine-tuning of the Qwen2.5-7B model using Low-Rank Adaptation (LoRA). This approach enables the model to handle noisy, heterogeneous text while reliably extracting 15 key fields, including location, event characteristics, and impact assessment, from a high-quality, manually annotated dataset of 4,933 instances derived from 27,822 police briefing posts on Chinese Weibo (2019-2020). Experimental results demonstrated that LoRA-based fine-tuning significantly improved performance over both the base and instruction-tuned models, achieving an accuracy exceeding 98.36% for mortality detection and Exact Match Rates of 95.31% for fatality counts and 95.54% for province-level location extraction. The proposed pipeline thus provides a validated and efficient solution for multi-task structured information extraction in specialized domains, offering a practical framework for transforming unstructured text into reliable structured data in social science research.

</details>


### [10] [Mitigating Hallucinations in Healthcare LLMs with Granular Fact-Checking and Domain-Specific Adaptation](https://arxiv.org/abs/2512.16189)
*Musarrat Zeba,Abdullah Al Mamun,Kishoar Jahan Tithee,Debopom Sutradhar,Mohaimenul Azam Khan Raiaan,Saddam Mukta,Reem E. Mohamed,Md Rafiqul Islam,Yakub Sebastian,Mukhtar Hussain,Sami Azam*

Main category: cs.CL

TL;DR: 本论文针对医疗领域大语言模型（LLM）生成内容中的幻觉问题，设计了一个独立的事实核查模块和领域专用的摘要模型，提升生成内容的可靠性和准确性。


<details>
  <summary>Details</summary>
Motivation: 医疗决策和患者安全场景中，LLM输出的可靠性和准确性至关重要，但现有模型易产生幻觉（hallucination）输出，影响使用安全。

Method: 提出一个独立于LLM的事实核查模块，利用电子健康记录（EHR）进行数值和逻辑校验，同时设计并通过MIMIC-III数据集微调的低秩适配（LoRa）摘要模型，减少幻觉率。事实核查模块基于自然语言处理中的离散逻辑，对提取的命题进行验证。

Result: 事实核查模块在104个摘要提取的3786个命题上的精确率达0.8904，召回率0.8234，F1分数0.8556；LLM摘要模型在ROUGE-1得分为0.5797，BERTScore得分为0.9120，表现良好。

Conclusion: 通过结合领域特定的摘要模型和独立的事实核查模块，有效提升了医疗领域LLM生成内容的准确性和可靠性，减少幻觉输出，增强模型在临床实际应用中的安全性。

Abstract: In healthcare, it is essential for any LLM-generated output to be reliable and accurate, particularly in cases involving decision-making and patient safety. However, the outputs are often unreliable in such critical areas due to the risk of hallucinated outputs from the LLMs. To address this issue, we propose a fact-checking module that operates independently of any LLM, along with a domain-specific summarization model designed to minimize hallucination rates. Our model is fine-tuned using Low-Rank Adaptation (LoRa) on the MIMIC III dataset and is paired with the fact-checking module, which uses numerical tests for correctness and logical checks at a granular level through discrete logic in natural language processing (NLP) to validate facts against electronic health records (EHRs). We trained the LLM model on the full MIMIC-III dataset. For evaluation of the fact-checking module, we sampled 104 summaries, extracted them into 3,786 propositions, and used these as facts. The fact-checking module achieves a precision of 0.8904, a recall of 0.8234, and an F1-score of 0.8556. Additionally, the LLM summary model achieves a ROUGE-1 score of 0.5797 and a BERTScore of 0.9120 for summary quality.

</details>


### [11] [An Information-Theoretic Framework for Robust Large Language Model Editing](https://arxiv.org/abs/2512.16227)
*Qizhou Chen,Chengyu Wang,Taolin Zhang,Xiaofeng He*

Main category: cs.CL

TL;DR: 本文提出基于信息瓶颈理论的语言模型知识编辑新框架IBKE，能有效实现通用且精准的模型知识更新，提升模型编辑的准确性和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型知识更新方法成本高昂且容易产生副作用，难以实现泛化的知识校正，限制了模型的安全部署和实际应用。

Method: 基于信息瓶颈理论，利用紧凑潜在表示引导基于梯度的模型更新，压缩并隔离关键知识信息，减少对无关模型行为的影响，提出信息瓶颈知识编辑器（IBKE）。

Result: 在多个大语言模型架构和标准基准任务中，IBKE表现出先进的准确性，提高了模型编辑的泛化能力和特异性。

Conclusion: IBKE为开放领域的知识编辑提供了理论支持和实际方案，推动了大语言模型在现实应用中的实用性和可信度提升。

Abstract: Large Language Models (LLMs) have become indispensable tools in science, technology, and society, enabling transformative advances across diverse fields. However, errors or outdated information within these models can undermine their accuracy and restrict their safe deployment. Developing efficient strategies for updating model knowledge without the expense and disruption of full retraining remains a critical challenge. Current model editing techniques frequently struggle to generalize corrections beyond narrow domains, leading to unintended consequences and limiting their practical impact. Here, we introduce a novel framework for editing LLMs, grounded in information bottleneck theory. This approach precisely compresses and isolates the essential information required for generalizable knowledge correction while minimizing disruption to unrelated model behaviors. Building upon this foundation, we present the Information Bottleneck Knowledge Editor (IBKE), which leverages compact latent representations to guide gradient-based updates, enabling robust and broadly applicable model editing. We validate IBKE's effectiveness across multiple LLM architectures and standard benchmark tasks, demonstrating state-of-the-art accuracy and improved generality and specificity of edits. These findings establish a theoretically principled and practical paradigm for open-domain knowledge editing, advancing the utility and trustworthiness of LLMs in real-world applications.

</details>


### [12] [LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding](https://arxiv.org/abs/2512.16229)
*Chenkai Xu,Yijie Jin,Jiajun Li,Yi Tu,Guoping Long,Dandan Tu,Tianqi Hou,Junchi Yan,Zhijie Deng*

Main category: cs.CL

TL;DR: 本文提出了一种无需训练的并行解码算法LoPA，通过优化Token填充顺序显著提升扩散大语言模型的推理速度，将每次前向传播的生成标记数提升至10.1，实现了高效多设备推理。


<details>
  <summary>Details</summary>
Motivation: 当前扩散大语言模型的置信度驱动解码策略受限于并行度较低，每次前向传播仅生成1-3个标记，限制了推理速度提升。

Method: 提出Lookahead Parallel Decoding (LoPA)算法，利用并行分支同时探索多种Token填充顺序，根据置信度选择未来并行潜力最大的顺序，无需额外训练并可即插即用。同时开发了支持分支并行的多设备推理系统。

Result: 在最先进的D2F模型上，LoPA显著提升了推理效率，将D2F-Dream模型在GSM8K数据集上的TPF提升到10.1，且保持了超越Dream基线的性能。多GPU部署下，系统实现了1073.9 tokens/s的单样本吞吐率。

Conclusion: LoPA通过优化Token填充顺序和引入分支并行策略，在不牺牲性能的情况下大幅提升了扩散大语言模型的解码效率，为高效多设备推理提供了有效方案。

Abstract: Diffusion Large Language Models (dLLMs) have demonstrated significant potential for high-speed inference. However, current confidence-driven decoding strategies are constrained by limited parallelism, typically achieving only 1--3 tokens per forward pass (TPF). In this work, we identify that the degree of parallelism during dLLM inference is highly sensitive to the Token Filling Order (TFO). Then, we introduce Lookahead PArallel Decoding LoPA, a training-free, plug-and-play algorithm, to identify a superior TFO and hence accelerate inference. LoPA concurrently explores distinct candidate TFOs via parallel branches, and selects the one with the highest potential for future parallelism based on branch confidence. We apply LoPA to the state-of-the-art D2F model and observe a substantial enhancement in decoding efficiency. Notably, LoPA increases the TPF of D2F-Dream to 10.1 on the GSM8K while maintaining performance superior to the Dream baseline. Furthermore, to facilitate this unprecedented degree of parallelism, we develop a specialized multi-device inference system featuring Branch Parallelism (BP), which achieves a single-sample throughput of 1073.9 tokens per second under multi-GPU deployment. The code is available at https://github.com/zhijie-group/LoPA.

</details>


### [13] [Sigma-Moe-Tiny Technical Report](https://arxiv.org/abs/2512.16248)
*Qingguo Hu,Zhenghao Lin,Ziyue Yang,Yucheng Ding,Xiao Liu,Yuting Jiang,Ruizhe Wang,Tianyu Chen,Zhongxin Guo,Yifan Xiong,Rui Gao,Lei Qu,Jinsong Su,Peng Cheng,Yeyun Gong*

Main category: cs.CL

TL;DR: Sigma-MoE-Tiny是一种极度稀疏的混合专家(MoE)语言模型，激活参数少但性能优异。


<details>
  <summary>Details</summary>
Motivation: 现有MoE模型在极端稀疏情况下专家负载平衡效果差，影响训练稳定性和性能。

Method: 该模型采用细粒度专家划分，每层最多96个专家，每个token仅激活一个专家，同时提出渐进稀疏化策略以平衡专家利用率和训练稳定性。

Result: 模型在激活仅0.5B参数的情况下，整体参数达到20B，训练过程稳定无损失突变，性能优于同类或更大规模模型。

Conclusion: 通过渐进稀疏化调度有效解决了极端稀疏下的负载平衡问题，Sigma-MoE-Tiny为未来高稀疏MoE架构提供了重要参考和启示。

Abstract: Mixture-of-Experts (MoE) has emerged as a promising paradigm for foundation models due to its efficient and powerful scalability. In this work, we present Sigma-MoE-Tiny, an MoE language model that achieves the highest sparsity compared to existing open-source models. Sigma-MoE-Tiny employs fine-grained expert segmentation with up to 96 experts per layer, while activating only one expert for each token, resulting in 20B total parameters with just 0.5B activated. The major challenge introduced by such extreme sparsity lies in expert load balancing. We find that the widely-used load balancing loss tends to become ineffective in the lower layers under this setting. To address this issue, we propose a progressive sparsification schedule aiming to balance expert utilization and training stability. Sigma-MoE-Tiny is pre-trained on a diverse and high-quality corpus, followed by post-training to further unlock its capabilities. The entire training process remains remarkably stable, with no occurrence of irrecoverable loss spikes. Comprehensive evaluations reveal that, despite activating only 0.5B parameters, Sigma-MoE-Tiny achieves top-tier performance among counterparts of comparable or significantly larger scale. In addition, we provide an in-depth discussion of load balancing in highly sparse MoE models, offering insights for advancing sparsity in future MoE architectures.
  Project page: https://qghuxmu.github.io/Sigma-MoE-Tiny
  Code: https://github.com/microsoft/ltp-megatron-lm

</details>


### [14] [Evaluating OpenAI GPT Models for Translation of Endangered Uralic Languages: A Comparison of Reasoning and Non-Reasoning Architectures](https://arxiv.org/abs/2512.16287)
*Yehor Tereshchenko,Mika Hämäläinen,Svitlana Myroniuk*

Main category: cs.CL

TL;DR: 本研究比较了基于推理与非推理架构的OpenAI GPT模型在芬兰语与四种低资源乌拉尔语系语言间翻译的性能差异，发现推理模型拒绝翻译的频率显著较低。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型的翻译评估主要集中在高资源语言，缺乏对低资源及濒危语言表现的系统研究。

Method: 利用芬兰语和四种乌拉尔语言的文学平行语料，分析不同模型架构在翻译任务中的拒绝率，比较推理与非推理模型的表现。

Result: 推理模型的拒绝率比非推理模型低16个百分点，显示推理模型在低资源语言翻译任务中表现更佳。

Conclusion: 推理架构的模型对低资源乌拉尔语系语言的翻译表现更优，为濒危语言保护和相关研究提供了重要参考。

Abstract: The evaluation of Large Language Models (LLMs) for translation tasks has primarily focused on high-resource languages, leaving a significant gap in understanding their performance on low-resource and endangered languages. This study presents a comprehensive comparison of OpenAI's GPT models, specifically examining the differences between reasoning and non-reasoning architectures for translating between Finnish and four low-resource Uralic languages: Komi-Zyrian, Moksha, Erzya, and Udmurt. Using a parallel corpus of literary texts, we evaluate model willingness to attempt translation through refusal rate analysis across different model architectures. Our findings reveal significant performance variations between reasoning and non-reasoning models, with reasoning models showing 16 percentage points lower refusal rates. The results provide valuable insights for researchers and practitioners working with Uralic languages and contribute to the broader understanding of reasoning model capabilities for endangered language preservation.

</details>


### [15] [Hacking Neural Evaluation Metrics with Single Hub Text](https://arxiv.org/abs/2512.16323)
*Hiroyuki Deguchi,Katsuki Chousa,Yusuke Sakai*

Main category: cs.CL

TL;DR: 本文提出了一种方法，在离散空间中找到单一的对抗文本，该文本在多个翻译任务中被评估为高质量，揭示了嵌入式神经文本评价指标（如COMET）存在的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 当前嵌入式神经文本评价指标虽然广泛使用，但由于神经网络的黑盒特性，其评估结果的可靠性和安全性无法保证，亟需揭示其潜在漏洞。

Method: 本文设计了一种在离散空间中寻找单一对抗性文本的方法，该文本能在不同测试用例中始终被评价为高质量，以此识别评价指标的脆弱点。

Result: 所发现的单一对抗文本在WMT'24英日和英德翻译任务中取得了79.1%和67.8%的COMET得分，超越了使用通用翻译模型M2M100为每个源句子单独生成的翻译，同时该对抗文本对日英和德英等多个语言对也表现出普适性。

Conclusion: 现有的嵌入式神经文本评价指标存在可能被恶意对抗文本误导的风险，需加强对指标的鲁棒性和安全性的关注，以确保其评估结果的可靠性。

Abstract: Strongly human-correlated evaluation metrics serve as an essential compass for the development and improvement of generation models and must be highly reliable and robust. Recent embedding-based neural text evaluation metrics, such as COMET for translation tasks, are widely used in both research and development fields. However, there is no guarantee that they yield reliable evaluation results due to the black-box nature of neural networks. To raise concerns about the reliability and safety of such metrics, we propose a method for finding a single adversarial text in the discrete space that is consistently evaluated as high-quality, regardless of the test cases, to identify the vulnerabilities in evaluation metrics. The single hub text found with our method achieved 79.1 COMET% and 67.8 COMET% in the WMT'24 English-to-Japanese (En--Ja) and English-to-German (En--De) translation tasks, respectively, outperforming translations generated individually for each source sentence by using M2M100, a general translation model. Furthermore, we also confirmed that the hub text found with our method generalizes across multiple language pairs such as Ja--En and De--En.

</details>


### [16] [Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs](https://arxiv.org/abs/2512.16378)
*Sara Papi,Javier Garcia Gilabert,Zachary Hopton,Vilém Zouhar,Carlos Escolano,Gerard I. Gállego,Jorge Iranzo-Sánchez,Ahrii Kim,Dominik Macháček,Patricia Schmidtova,Maike Züfle*

Main category: cs.CL

TL;DR: 本文评估了倾向直接翻译语音的SpeechLLMs与传统级联系统在语音翻译中的性能表现，结果表明级联系统整体更可靠，集成大型语言模型（LLM）对提升翻译质量至关重要。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）在多模态能力上的扩展，将语音作为原生模态直接进行翻译成为可能，但这一直接翻译方式是否优于传统需先转录的级联架构尚不明确。

Method: 构建了“Hearing to Translate”测试套件，系统比较了5个最新SpeechLLMs与16个强劲的直接与级联系统，评测涵盖16个基准、13种语言对以及9种复杂语音场景（如非流畅、嘈杂和长时语音）。

Result: 评测结果显示，尽管SpeechLLMs在部分场景能与级联系统匹敌，整体上级联系统依然表现最稳定，且纯语音基础模型（SFMs）性能弱于二者。

Conclusion: 集成大型语言模型（LLM）无论是嵌入式还是级联管道中，都是实现高质量语音翻译的关键，而当前SpeechLLMs尚未全面超越传统级联方法。

Abstract: As Large Language Models (LLMs) expand beyond text, integrating speech as a native modality has given rise to SpeechLLMs, which aim to translate spoken language directly, thereby bypassing traditional transcription-based pipelines. Whether this integration improves speech-to-text translation quality over established cascaded architectures, however, remains an open question. We present Hearing to Translate, the first comprehensive test suite rigorously benchmarking 5 state-of-the-art SpeechLLMs against 16 strong direct and cascade systems that couple leading speech foundation models (SFM), with multilingual LLMs. Our analysis spans 16 benchmarks, 13 language pairs, and 9 challenging conditions, including disfluent, noisy, and long-form speech. Across this extensive evaluation, we find that cascaded systems remain the most reliable overall, while current SpeechLLMs only match cascades in selected settings and SFMs lag behind both, highlighting that integrating an LLM, either within the model or in a pipeline, is essential for high-quality speech translation.

</details>


### [17] [Bridging the Reality Gap: Efficient Adaptation of ASR systems for Challenging Low-Resource Domains](https://arxiv.org/abs/2512.16401)
*Darshil Chauhan,Adityasinh Solanki,Vansh Patel,Kanav Kapoor,Ritvik Jain,Aditya Bansal,Dhruv Kumar,Prateek Narang*

Main category: cs.CL

TL;DR: 本文提出了一种利用低秩适应(LoRA)和多域经验回放的隐私保护自适应框架，显著提升了多语种自动语音识别(ASR)系统在临床语音环境中的性能，解决了数据隐私和计算资源受限问题。


<details>
  <summary>Details</summary>
Motivation: 在农村医疗等资源受限领域，自动语音识别可提高患者处理效率和降低成本，但受数据隐私限制、计算资源有限和声学环境差异大等技术障碍影响，现有模型性能下降严重，难以实用。

Method: 采用低秩适应(LoRA)实现边缘设备上的持续学习以保障数据隐私，并结合多域经验回放减少灾难性遗忘，从而高效适配实际临床语音数据。

Result: 所提出方法使目标领域的词错误率(WER)相对降低17.1%，并使灾难性遗忘减少47%。

Conclusion: 该方法为构建可在现实高影响环境中有效运行的可靠自我提升ASR系统提供了可行路径，克服了隐私、计算和领域转移等关键挑战。

Abstract: Automatic Speech Recognition (ASR) holds immense potential to streamline clinical documentation, such as digitizing handwritten prescriptions and reports, thereby increasing patient throughput and reducing costs in resource-constrained sectors like rural healthcare. However, realizing this utility is currently obstructed by significant technical barriers: strict data privacy constraints, limited computational resources, and severe acoustic domain shifts. We quantify this gap by showing that a robust multilingual model (IndicWav2Vec) degrades to a stark 40.94% Word Error Rate (WER) when deployed on real-world clinical audio (Gram Vaani), rendering it unusable for practical applications. To address these challenges and bring ASR closer to deployment, we propose an efficient, privacy-preserving adaptation framework. We employ Low-Rank Adaptation (LoRA) to enable continual learning from incoming data streams directly on edge devices, ensuring patient data confidentiality. Our strategy yields a 17.1% relative improvement in WER on the target domain. Furthermore, by integrating multi-domain experience replay, we reduce catastrophic forgetting by 47% compared to naive adaptation. These results demonstrate a viable pathway for building reliable, self-improving ASR systems that can operate effectively within the constraints of high-impact real-world environments.

</details>


### [18] [Plain language adaptations of biomedical text using LLMs: Comparision of evaluation metrics](https://arxiv.org/abs/2512.16530)
*Primoz Kocbek,Leon Kopitar,Gregor Stiglic*

Main category: cs.CL

TL;DR: 本研究利用大型语言模型简化生物医学文本，提升健康素养，评估了多种方法并验证了量化及质化指标的有效性。


<details>
  <summary>Details</summary>
Motivation: 生物医学文本通常复杂难懂，影响普通公众的健康理解，迫切需要简化文本以提高健康素养。

Method: 基于公开数据集，采用提示模板基线法、双AI代理法及微调法，选用OpenAI gpt-4o和gpt-4o mini模型，结合多种定量与定性指标进行评估。

Result: gpt-4o-mini表现最佳，微调方法效果不佳。基于LLM的量化指标G-Eval与定性评价结果一致，表现出较好的评估能力。

Conclusion: 基于大型语言模型的文本简化方法有效提高了生物医学文本的可理解性，G-Eval指标可作为未来评估工具的参考。

Abstract: This study investigated the application of Large Language Models (LLMs) for simplifying biomedical texts to enhance health literacy. Using a public dataset, which included plain language adaptations of biomedical abstracts, we developed and evaluated several approaches, specifically a baseline approach using a prompt template, a two AI agent approach, and a fine-tuning approach. We selected OpenAI gpt-4o and gpt-4o mini models as baselines for further research. We evaluated our approaches with quantitative metrics, such as Flesch-Kincaid grade level, SMOG Index, SARI, and BERTScore, G-Eval, as well as with qualitative metric, more precisely 5-point Likert scales for simplicity, accuracy, completeness, brevity. Results showed a superior performance of gpt-4o-mini and an underperformance of FT approaches. G-Eval, a LLM based quantitative metric, showed promising results, ranking the approaches similarly as the qualitative metric.

</details>


### [19] [UM_FHS at the CLEF 2025 SimpleText Track: Comparing No-Context and Fine-Tune Approaches for GPT-4.1 Models in Sentence and Document-Level Text Simplification](https://arxiv.org/abs/2512.16541)
*Primoz Kocbek,Gregor Stiglic*

Main category: cs.CL

TL;DR: 本文介绍了CLEF 2025 SimpleText赛道任务1中基于OpenAI的GPT-4系列模型对科学文本进行句子级和文档级简化的方法及结果。


<details>
  <summary>Details</summary>
Motivation: 科学文本复杂，简化有助于提升可读性及普及。作者试图探索GPT-4不同模型及两种策略在句子和文档级简化中的表现。

Method: 采用GPT-4.1、GPT-4.1-mini和GPT-4.1-nano三种模型，比较无上下文的提示工程方法与基于微调的两种不同简化策略。

Result: 无上下文的gpt-4.1-mini在两个层级的简化任务中表现稳健，微调模型效果参差，尤其gpt-4.1-nano的微调版在文档级简化中表现突出。

Conclusion: 不同GPT-4模型和方法在科学文本简化任务中表现各异，提示工程在一定程度上可有效简化文本，微调则能针对特定层级提供优势，但面临复杂挑战。

Abstract: This work describes our submission to the CLEF 2025 SimpleText track Task 1, addressing both sentenceand document-level simplification of scientific texts. The methodology centered on using the gpt-4.1, gpt-4.1mini, and gpt-4.1-nano models from OpenAI. Two distinct approaches were compared: a no-context method relying on prompt engineering and a fine-tuned (FT) method across models. The gpt-4.1-mini model with no-context demonstrated robust performance at both levels of simplification, while the fine-tuned models showed mixed results, highlighting the complexities of simplifying text at different granularities, where gpt-4.1-nano-ft performance stands out at document-level simplification in one case.

</details>


### [20] [Refusal Steering: Fine-grained Control over LLM Refusal Behaviour for Sensitive Topics](https://arxiv.org/abs/2512.16602)
*Iker García-Ferrero,David Montero,Roman Orus*

Main category: cs.CL

TL;DR: 本文提出了一种推理阶段控制大型语言模型拒绝政治敏感话题的细粒度方法“拒绝引导”，无需重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 当前基于模式的拒绝检测方法脆弱，难以有效控制模型在政治敏感话题上的拒绝行为，且缺乏灵活调节手段。

Method: 引入LLM作为判官，赋予拒绝置信分数，采用岭回归正则化计算引导向量以更好地分离拒绝和服从方向。

Result: 该方法在Qwen3-Next-80B-A3B-Thinking模型上有效消除政治敏感话题的拒绝行为，同时保障安全性和保持基线性能，且适用于不同规模模型，可以实现定向拒绝。拒绝信号集中于变换器深层且多维分布。

Conclusion: 激活引导技术能在推理时去除政治拒绝行为，同时维护对有害内容的安全调节，提供了一种可控透明的内容审核实用路径。

Abstract: We introduce Refusal Steering, an inference-time method to exercise fine-grained control over Large Language Models refusal behaviour on politically sensitive topics without retraining. We replace fragile pattern-based refusal detection with an LLM-as-a-judge that assigns refusal confidence scores and we propose a ridge-regularized variant to compute steering vectors that better isolate the refusal--compliance direction. On Qwen3-Next-80B-A3B-Thinking, our method removes the refusal behaviour of the model around politically sensitive topics while maintaining safety on JailbreakBench and near-baseline performance on general benchmarks. The approach generalizes across 4B and 80B models and can also induce targeted refusals when desired. We analize the steering vectors and show that refusal signals concentrate in deeper layers of the transformer and are distributed across many dimensions. Together, these results demonstrate that activation steering can remove political refusal behaviour while retaining safety alignment for harmful content, offering a practical path to controllable, transparent moderation at inference time.

</details>


### [21] [JustRL: Scaling a 1.5B LLM with a Simple RL Recipe](https://arxiv.org/abs/2512.16649)
*Bingxiang He,Zekai Qu,Zeyuan Liu,Yinghao Chen,Yuxin Zuo,Cheng Qian,Kaiyan Zhang,Weize Chen,Chaojun Xiao,Ganqu Cui,Ning Ding,Zhiyuan Liu*

Main category: cs.CL

TL;DR: 本文提出了JustRL，一种使用单阶段训练和固定超参数的简化强化学习方法，在数学推理基准上实现了最先进性能，且计算成本是复杂方法的一半。


<details>
  <summary>Details</summary>
Motivation: 当前大规模语言模型强化学习方法趋向复杂（多阶段训练、动态超参、课程学习），作者质疑这种复杂性是否必要。

Method: 采用单阶段训练，固定超参数，无需调优，避免使用额外的技巧如长度惩罚和验证器。

Result: 在两个1.5B参数模型上达到54.9%和64.3%的平均准确率，计算成本减半，训练过程稳定无崩溃，超参数直接迁移。添加典型技巧反而降低性能。

Conclusion: 复杂训练策略可能是不必要的，稳定且规模化的简单训练基线足以解决许多问题，提供了一个简洁有效的基线供社区使用。

Abstract: Recent advances in reinforcement learning for large language models have converged on increasing complexity: multi-stage training pipelines, dynamic hyperparameter schedules, and curriculum learning strategies. This raises a fundamental question: \textbf{Is this complexity necessary?} We present \textbf{JustRL}, a minimal approach using single-stage training with fixed hyperparameters that achieves state-of-the-art performance on two 1.5B reasoning models (54.9\% and 64.3\% average accuracy across nine mathematical benchmarks) while using 2$\times$ less compute than sophisticated approaches. The same hyperparameters transfer across both models without tuning, and training exhibits smooth, monotonic improvement over 4,000+ steps without the collapses or plateaus that typically motivate interventions. Critically, ablations reveal that adding ``standard tricks'' like explicit length penalties and robust verifiers may degrade performance by collapsing exploration. These results suggest that the field may be adding complexity to solve problems that disappear with a stable, scaled-up baseline. We release our models and code to establish a simple, validated baseline for the community.

</details>


### [22] [GinSign: Grounding Natural Language Into System Signatures for Temporal Logic Translation](https://arxiv.org/abs/2512.16770)
*William English,Chase Walker,Dominic Simon,Rickard Ewetz*

Main category: cs.CL

TL;DR: 本文提出了GinSign框架，将自然语言输入映射到系统签名，实现高准确率的时序逻辑翻译，为自动系统构建提供可信的形式化规范工具。


<details>
  <summary>Details</summary>
Motivation: 现有自然语言到时序逻辑翻译方法依赖准确的原子映射或准确率较低，限制了自动系统的规范构建能力。

Method: 提出分层地将原子命题映射到系统签名元素的接地模型，将自由生成转化为结构化分类问题，使用较小的掩码语言模型，降低对大型语言模型的依赖。

Result: 在多个领域的实验表明，GinSign框架实现了95.5%的接地逻辑等价得分，较现有最佳方法提升1.4倍，同时支持下游模型验证。

Conclusion: 通过引入接地机制，GinSign显著提升了自然语言到时序逻辑翻译的准确性和实用性，为可信自动系统的规范验证提供了有效工具。

Abstract: Natural language (NL) to temporal logic (TL) translation enables engineers to specify, verify, and enforce system behaviors without manually crafting formal specifications-an essential capability for building trustworthy autonomous systems. While existing NL-to-TL translation frameworks have demonstrated encouraging initial results, these systems either explicitly assume access to accurate atom grounding or suffer from low grounded translation accuracy. In this paper, we propose a framework for Grounding Natural Language Into System Signatures for Temporal Logic translation called GinSign. The framework introduces a grounding model that learns the abstract task of mapping NL spans onto a given system signature: given a lifted NL specification and a system signature $\mathcal{S}$, the classifier must assign each lifted atomic proposition to an element of the set of signature-defined atoms $\mathcal{P}$. We decompose the grounding task hierarchically- first predicting predicate labels, then selecting the appropriately typed constant arguments. Decomposing this task from a free-form generation problem into a structured classification problem permits the use of smaller masked language models and eliminates the reliance on expensive LLMs. Experiments across multiple domains show that frameworks which omit grounding tend to produce syntactically correct lifted LTL that is semantically nonequivalent to grounded target expressions, whereas our framework supports downstream model checking and achieves grounded logical-equivalence scores of $95.5\%$, a $1.4\times$ improvement over SOTA.

</details>


### [23] [From Facts to Conclusions : Integrating Deductive Reasoning in Retrieval-Augmented LLMs](https://arxiv.org/abs/2512.16795)
*Shubham Mishra,Samyek Jain,Gorang Mehrishi,Shiv Tiwari,Harsh Sharma,Pratik Narang,Dhruv Kumar*

Main category: cs.CL

TL;DR: 提出了一种添加结构化推理的检索增强生成（RAG）框架，有效解决来源冲突和信息陈旧问题，提高大型语言模型的答案准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG模型在面对信息冲突、过时或主观的检索来源时表现不佳，且缺乏统一的推理监督机制。

Method: 设计了包含文档级裁决、冲突分析和基于证据的综合三个阶段的推理痕迹增强RAG框架，并引入了基于LLM作为裁判的冲突感知信任评分（CATS）流程。

Result: 在含有539条推理查询的数据集上，应用Qwen模型进行监督微调后，端到端答案正确率从0.069提升至0.883，行为一致性从0.074提升至0.722，显著优于基线。

Conclusion: 该研究为构建冲突感知且可解释的RAG系统奠定了基础，显著提升了模型面对信息冲突时的性能和可信度。

Abstract: Retrieval-Augmented Generation (RAG) grounds large language models (LLMs) in external evidence, but fails when retrieved sources conflict or contain outdated or subjective information. Prior work address these issues independently but lack unified reasoning supervision. We propose a reasoning-trace-augmented RAG framework that adds structured, interpretable reasoning across three stages : (1) document-level adjudication, (2) conflict analysis, and (3) grounded synthesis, producing citation-linked answers or justified refusals. A Conflict-Aware Trust-Score (CATS) pipeline is introduced which evaluates groundedness, factual correctness, refusal accuracy, and conflict-behavior alignment using an LLM-as-a-Judge. Our 539-query reasoning dataset and evaluation pipeline establish a foundation for conflict-aware, interpretable RAG systems. Experimental results demonstrate substantial gains over baselines, most notably with Qwen, where Supervised Fine-Tuning improved End-to-End answer correctness from 0.069 to 0.883 and behavioral adherence from 0.074 to 0.722.

</details>


### [24] [Exploration of Augmentation Strategies in Multi-modal Retrieval-Augmented Generation for the Biomedical Domain: A Case Study Evaluating Question Answering in Glycobiology](https://arxiv.org/abs/2512.16802)
*Primož Kocbek,Azra Frkatović-Hodžić,Dora Lalić,Vivian Hui,Gordan Lauc,Gregor Štiglic*

Main category: cs.CL

TL;DR: 本文研究了生物医学领域中多模态检索增强生成（MM-RAG）的方法，比较了将图表转换为文本和无需OCR的视觉检索两种策略在糖生物学领域问答的表现。实验表明，文本转换对中型模型更有效，而最新的前沿模型使用视觉检索也能取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 糖生物学领域图像信息丰富，如何选择合适的多模态信息处理方式以提升问答准确率尚不明确，尤其在不同模型能力下的策略效果差异需要探究。

Method: 构建基于25篇论文的120道选择题的数据集，设计四种数据增强方法（无增强、文本增强、多模态转换、晚期视觉检索），结合Docling解析、Qdrant索引，评测不同大小和先进程度的模型（如Gemma-3、GPT-4o和GPT-5家族）表现。

Result: 中型模型Gemma-3在文本和多模态增强下准确率显著高于视觉检索；前沿模型GPT-4o和GPT-5使用视觉检索方法表现接近甚至优于文本增强，且视觉检索器ColPali、ColQwen和ColFlor间表现无显著差异。

Conclusion: 视觉信息转换为文本更适合中等规模模型以降低理解负担，而无需OCR的视觉检索在更强大的前沿模型中成为有竞争力的选择。ColFlor作为轻量级检索器，在强大生成模型配合下表现同样优秀，推荐作为默认方案。

Abstract: Multi-modal retrieval-augmented generation (MM-RAG) promises grounded biomedical QA, but it is unclear when to (i) convert figures/tables into text versus (ii) use optical character recognition (OCR)-free visual retrieval that returns page images and leaves interpretation to the generator. We study this trade-off in glycobiology, a visually dense domain. We built a benchmark of 120 multiple-choice questions (MCQs) from 25 papers, stratified by retrieval difficulty (easy text, medium figures/tables, hard cross-evidence). We implemented four augmentations-None, Text RAG, Multi-modal conversion, and late-interaction visual retrieval (ColPali)-using Docling parsing and Qdrant indexing. We evaluated mid-size open-source and frontier proprietary models (e.g., Gemma-3-27B-IT, GPT-4o family). Additional testing used the GPT-5 family and multiple visual retrievers (ColPali/ColQwen/ColFlor). Accuracy with Agresti-Coull 95% confidence intervals (CIs) was computed over 5 runs per configuration. With Gemma-3-27B-IT, Text and Multi-modal augmentation outperformed OCR-free retrieval (0.722-0.740 vs. 0.510 average accuracy). With GPT-4o, Multi-modal achieved 0.808, with Text 0.782 and ColPali 0.745 close behind; within-model differences were small. In follow-on experiments with the GPT-5 family, the best results with ColPali and ColFlor improved by ~2% to 0.828 in both cases. In general, across the GPT-5 family, ColPali, ColQwen, and ColFlor were statistically indistinguishable. GPT-5-nano trailed larger GPT-5 variants by roughly 8-10%. Pipeline choice is capacity-dependent: converting visuals to text lowers the reader burden and is more reliable for mid-size models, whereas OCR-free visual retrieval becomes competitive under frontier models. Among retrievers, ColFlor offers parity with heavier options at a smaller footprint, making it an efficient default when strong generators are available.

</details>


### [25] [Grammar-Forced Translation of Natural Language to Temporal Logic using LLMs](https://arxiv.org/abs/2512.16814)
*William English,Dominic Simon,Sumit Kumar Jha,Rickard Ewetz*

Main category: cs.CL

TL;DR: 提出了一种名为GraFT的框架，用于将自然语言翻译为时序逻辑，通过限制每步可选输出令牌集，提升了准确率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在原子命题的提升、共指问题和有限数据学习上存在困难，导致翻译准确性不足。

Method: GraFT通过约束每步的有效输出令牌集，减少了翻译的解空间复杂度，并基于问题特性调整解空间，理论上提升了学习效率。

Result: 在CW、GLTL和Navi基准测试中，GraFT相较于最先进方法提高了端到端翻译准确率5.49%，域外翻译准确率提升14.06%。

Conclusion: GraFT有效解决了NL到TL翻译中的提升和翻译步骤复杂性问题，显著提升了模型的准确性和泛化能力。

Abstract: Translating natural language (NL) into a formal language such as temporal logic (TL) is integral for human communication with robots and autonomous systems. State-of-the-art approaches decompose the task into a lifting of atomic propositions (APs) phase and a translation phase. However, existing methods struggle with accurate lifting, the existence of co-references, and learning from limited data. In this paper, we propose a framework for NL to TL translation called Grammar Forced Translation (GraFT). The framework is based on the observation that previous work solves both the lifting and translation steps by letting a language model iteratively predict tokens from its full vocabulary. In contrast, GraFT reduces the complexity of both tasks by restricting the set of valid output tokens from the full vocabulary to only a handful in each step. The solution space reduction is obtained by exploiting the unique properties of each problem. We also provide a theoretical justification for why the solution space reduction leads to more efficient learning. We evaluate the effectiveness of GraFT using the CW, GLTL, and Navi benchmarks. Compared with state-of-the-art translation approaches, it can be observed that GraFT the end-to-end translation accuracy by 5.49% and out-of-domain translation accuracy by 14.06% on average.

</details>


### [26] [What Do Prosody and Text Convey? Characterizing How Meaningful Information is Distributed Across Multiple Channels](https://arxiv.org/abs/2512.16832)
*Aditya Yadavalli,Tiago Pimentel,Tamar I Regev,Ethan Wilcox,Alex Warstadt*

Main category: cs.CL

TL;DR: 本文提出了一种信息论方法，量化韵律在传递情绪、讽刺和疑问等意义维度上的信息量，发现在缺乏长时间语境时，韵律提供的信息远超文本。


<details>
  <summary>Details</summary>
Motivation: 韵律作为语音的旋律，传递了文本难以捕捉的重要信息，如何量化韵律所传递的信息量及其内容成为研究动机。

Method: 利用大型语音和语言模型，估计发声中意义的特定维度（如情绪）与不同交流渠道（音频或文本）之间的互信息，分析电视和播客语料中的讽刺、情绪和疑问。

Result: 发现对于讽刺和情绪，音频（韵律）通道传递的信息量是文本通道的十倍以上；对疑问，韵律提供的信息较少。

Conclusion: 韵律在表达某些意义特征方面信息丰富，未来可扩展至更多语义维度、交流渠道和语言的研究。

Abstract: Prosody -- the melody of speech -- conveys critical information often not captured by the words or text of a message. In this paper, we propose an information-theoretic approach to quantify how much information is expressed by prosody alone and not by text, and crucially, what that information is about. Our approach applies large speech and language models to estimate the mutual information between a particular dimension of an utterance's meaning (e.g., its emotion) and any of its communication channels (e.g., audio or text). We then use this approach to quantify how much information is conveyed by audio and text about sarcasm, emotion, and questionhood, using speech from television and podcasts. We find that for sarcasm and emotion the audio channel -- and by implication the prosodic channel -- transmits over an order of magnitude more information about these features than the text channel alone, at least when long-term context beyond the current sentence is unavailable. For questionhood, prosody provides comparatively less additional information. We conclude by outlining a program applying our approach to more dimensions of meaning, communication channels, and languages.

</details>


### [27] [LLMCache: Layer-Wise Caching Strategies for Accelerated Reuse in Transformer Inference](https://arxiv.org/abs/2512.16843)
*Harsh Vardhan Bansal*

Main category: cs.CL

TL;DR: 本文提出了LLMCache，一种基于层级缓存的框架，通过重用语义相似输入的中间激活，提高Transformer模型的推理速度，在多种任务中实现最高3.1倍加速且准确率损失小于0.5%。


<details>
  <summary>Details</summary>
Motivation: Transformer模型推理延迟高，限制了其实时和大规模应用，现有缓存机制范围有限，难以应用于多种模型和层级。

Method: 提出LLMCache框架，支持任意Transformer层缓存，结合轻量级指纹匹配语义相似输入和自适应缓存淘汰策略，提高缓存效率和适用性。

Result: 在BERT和GPT-2模型上，LLMCache在SQuAD、WikiText-103和OpenBookQA数据集上实现了最高3.1倍的推理加速，准确率损失低于0.5%。

Conclusion: LLMCache是一种通用且实用的Transformer推理优化方案，适用于多种架构和任务，有助于推动其在实际应用中的大规模部署。

Abstract: Transformer-based language models have achieved remarkable performance across a wide range of tasks, yet their high inference latency poses a significant challenge for real-timeand large-scale deployment. While existing caching mechanisms,such as token-level key-value caches, offer speedups in autore-gressive decoding, they are limited in scope and applicability. In this paper, we present LLMCache, a novel layer-wise caching framework that accelerates transformer inference by reusing intermediate activations based on semantic similarity of input sequences. Unlike prior work, LLMCache is model-agnostic,operates across both encoder and decoder architectures, and supports caching at arbitrary transformer layers. We introduce a lightweight fingerprinting mechanism for matching seman-tically similar inputs and propose adaptive eviction strategies to manage cache staleness. Experiments on BERT and GPT-2 across SQuAD, WikiText-103, and OpenBookQA show up to 3.1 X speedup in inference time with <0.5% accuracy degradation. Our results highlight LLMCache as a practical and general-purpose solution for optimizing transformer inference in real-world applications

</details>


### [28] [AdaSearch: Balancing Parametric Knowledge and Search in Large Language Models via Reinforcement Learning](https://arxiv.org/abs/2512.16883)
*Tzu-Han Lin,Wei-Lin Chen,Chen-An Li,Hung-yi Lee,Yun-Nung Chen,Yu Meng*

Main category: cs.CL

TL;DR: 本文提出了AdaSearch，一种结合强化学习的两阶段框架，以实现大语言模型在搜索调用上的自适应平衡，有效减少不必要的搜索调用，提升决策透明度。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过惩罚搜索调用次数来减少过度搜索，但存在奖励设计复杂、信用分配模糊及易被规避的问题，且无法准确区分必要和不必要的搜索，影响自适应行为的准确评估。

Method: 作者提出AdaSearch框架，将问题求解与是否调用搜索的决策分离，利用基于结果的强化学习优化决策过程，提高搜索调用的自知能力和决策的解释性。

Result: 实验表明，AdaSearch在不同模型和规模上提升了知识边界意识，减少了不必要的搜索调用，同时保持了任务性能，并提供了更透明和可解释的决策行为。

Conclusion: AdaSearch有效解决了现有搜索代理过度调用搜索的问题，实现了基于知识边界的自适应搜索调用，提高了决策的透明度，适用于金融和医疗等高风险领域。

Abstract: Equipping large language models (LLMs) with search engines via reinforcement learning (RL) has emerged as an effective approach for building search agents. However, overreliance on search introduces unnecessary cost and risks exposure to noisy or malicious content, while relying solely on parametric knowledge risks hallucination. The central challenge is to develop agents that adaptively balance parametric knowledge with external search, invoking search only when necessary. Prior work mitigates search overuse by shaping rewards around the number of tool calls. However, these penalties require substantial reward engineering, provide ambiguous credit assignment, and can be exploited by agents that superficially reduce calls. Moreover, evaluating performance solely through call counts conflates necessary and unnecessary search, obscuring the measurement of true adaptive behavior. To address these limitations, we first quantify the self-knowledge awareness of existing search agents via an F1-based decision metric, revealing that methods such as Search-R1 often overlook readily available parametric knowledge. Motivated by these findings, we propose AdaSearch, a simple two-stage, outcome-driven RL framework that disentangles problem solving from the decision of whether to invoke search, and makes this decision process explicit and interpretable. This transparency is crucial for high-stakes domains such as finance and medical question answering, yet is largely neglected by prior approaches. Experiments across multiple model families and sizes demonstrate that AdaSearch substantially improves knowledge-boundary awareness, reduces unnecessary search calls, preserves strong task performance, and offers more transparent, interpretable decision behaviors.

</details>


### [29] [Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image](https://arxiv.org/abs/2512.16899)
*Yushi Hu,Reyhane Askari-Hemmat,Melissa Hall,Emily Dinan,Luke Zettlemoyer,Marjan Ghazvininejad*

Main category: cs.CL

TL;DR: 本文提出了首个针对多模态理解和生成奖励模型的综合基准MMRB2，涵盖文本到图像、图像编辑、交错生成和多模态推理四大任务，并对23个模型进行评测。


<details>
  <summary>Details</summary>
Motivation: 当前对处理图文交错序列的全能模型奖励模型研究不足，缺乏系统的多模态评测基准。

Method: 设计了具有挑战性的实用提示，从23个模型收集响应，采用专家一致偏好对进行筛选，构建MMRB2基准，评测多模态奖励模型包括多模态LLM评判者和人类偏好训练模型。

Result: 最新的Gemini 3 Pro获75-80%准确率，GPT-5与Gemini 2.5 Pro达66-75%，均优于GPT-4o，开源模型Qwen3-VL-32B表现接近Gemini 2.5 Flash，且MMRB2表现与下游任务成功强相关。

Conclusion: MMRB2是多模态奖励模型评测的重要工具，揭示当前模型成绩与人类专家差距，指明未来提升方向。

Abstract: Reward models (RMs) are essential for training large language models (LLMs), but remain underexplored for omni models that handle interleaved image and text sequences. We introduce Multimodal RewardBench 2 (MMRB2), the first comprehensive benchmark for reward models on multimodal understanding and (interleaved) generation. MMRB2 spans four tasks: text-to-image, image editing, interleaved generation, and multimodal reasoning ("thinking-with-images"), providing 1,000 expert-annotated preference pairs per task from 23 models and agents across 21 source tasks. MMRB2 is designed with: (1) practical but challenging prompts; (2) responses from state-of-the-art models and agents; and (3) preference pairs with strong human-expert consensus, curated via an ensemble filtering strategy. Using MMRB2, we study existing judges for each subtask, including multimodal LLM-as-a-judge and models trained with human preferences. The latest Gemini 3 Pro attains 75-80% accuracy. GPT-5 and Gemini 2.5 Pro reach 66-75% accuracy, compared to >90% for humans, yet surpass the widely used GPT-4o (59%). The best performing open-source model Qwen3-VL-32B achieves similar accuracies as Gemini 2.5 Flash (64%). We also show that MMRB2 performance strongly correlates with downstream task success using Best-of-N sampling and conduct an in-depth analysis that shows key areas to improve the reward models going forward.

</details>


### [30] [In-Context Algebra](https://arxiv.org/abs/2512.16902)
*Eric Todd,Jannik Brinkmann,Rohit Gandikota,David Bau*

Main category: cs.CL

TL;DR: 本文研究了变压器模型在处理含变量且变量含义依赖于交互的代数序列算术任务中的机制，发现模型能够通过符号推理机制实现高准确率并推广到未见代数群。


<details>
  <summary>Details</summary>
Motivation: 以往研究多关注变压器对固定含义符号的代数结构学习，本研究旨在探索变压器如何处理符号含义随序列变化的更复杂情况。

Method: 设计任务使符号映射到不同的代数群元素，并通过专门的数据分布进行因果测试；分析模型学习到的机制，包括交换复制、单位元识别和封闭性消除。

Result: 模型在该任务上取得近乎完美的准确率，能泛化至未见代数群，并学习到三种符号推理机制以辅助计算。

Conclusion: 变压器在变量意义不固定的条件下，能发展出符号推理机制，这补充了此前对固定符号语义下几何表示的理解，展示了模型潜在的推理能力。

Abstract: We investigate the mechanisms that arise when transformers are trained to solve arithmetic on sequences where tokens are variables whose meaning is determined only through their interactions. While prior work has found that transformers develop geometric embeddings that mirror algebraic structure, those previous findings emerge from settings where arithmetic-valued tokens have fixed meanings. We devise a new task in which the assignment of symbols to specific algebraic group elements varies from one sequence to another. Despite this challenging setup, transformers achieve near-perfect accuracy on the task and even generalize to unseen algebraic groups. We develop targeted data distributions to create causal tests of a set of hypothesized mechanisms, and we isolate three mechanisms models consistently learn: commutative copying where a dedicated head copies answers, identity element recognition that distinguishes identity-containing facts, and closure-based cancellation that tracks group membership to constrain valid answers. Complementary to the geometric representations found in fixed-symbol settings, our findings show that models develop symbolic reasoning mechanisms when trained to reason in-context with variables whose meanings are not fixed.

</details>


### [31] [Constructive Circuit Amplification: Improving Math Reasoning in LLMs via Targeted Sub-Network Updates](https://arxiv.org/abs/2512.16914)
*Nikhil Prakash,Donghao Ren,Dominik Moritz,Yannick Assogba*

Main category: cs.CL

TL;DR: 本文提出了一种名为构建性电路放大的方法，通过识别和更新大规模语言模型中负责特定任务的关键组件，实现了数学推理能力显著提升，同时对其他能力影响较小。


<details>
  <summary>Details</summary>
Motivation: 之前的研究发现大规模语言模型内部存在稀疏子网络（电路），负责特定任务，且微调主要通过强化这些电路提升性能，因此有可能通过直接干预这些电路实现精确的任务定向更新。

Method: 提出构建性电路放大方法，通过识别模型推理轨迹中的关键token和负责任务的模型组件，仅更新这些关键组件。

Result: 在数学推理任务上，多模型测试准确率提升最高达11.4%，仅修改约1.59%的模型组件，并且对其他任务（MMLU、TriviaQA、TruthfulQA）的影响最小。

Conclusion: 有针对性地选择性更新模型中稀疏的关键组件可以可靠地提升特定能力，验证了通过干预电路进行精准模型更新的有效性。

Abstract: Prior studies investigating the internal workings of LLMs have uncovered sparse subnetworks, often referred to as circuits, that are responsible for performing specific tasks. Additionally, it has been shown that model performance improvement through fine-tuning often results from the strengthening of existing circuits in the model. Taken together, these findings suggest the possibility of intervening directly on such circuits to make precise, task-targeted updates. Motivated by these findings, we propose a novel method called Constructive Circuit Amplification which identifies pivotal tokens from model reasoning traces as well as model components responsible for the desired task, and updates only those components. Applied to mathematical reasoning, it improves accuracy by up to +11.4% across multiple models while modifying as little as 1.59% of model components, with minimal impact on other abilities as measured by MMLU, TriviaQA, and TruthfulQA. These results demonstrate that targeted capabilities can be reliably enhanced by selectively updating a sparse set of model components.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [32] [XBIDetective: Leveraging Vision Language Models for Identifying Cross-Browser Visual Inconsistencies](https://arxiv.org/abs/2512.15804)
*Balreet Grewal,James Graham,Jeff Muizelaar,Jan Honza Odvarko,Suhaib Mujahid,Marco Castelluccio,Cor-Paul Bezemer*

Main category: cs.SE

TL;DR: 本文介绍了一种利用视觉语言模型检测浏览器跨浏览器显示不一致（XBI）的方法——XBIDetective，能有效识别渲染漏洞及动态广告元素。


<details>
  <summary>Details</summary>
Motivation: 浏览器渲染漏洞难以检测，且跨浏览器不一致性可作为检测线索，但现有基于视觉和DOM的方法难应对动态交互元素。

Method: 提出XBIDetective工具，通过在Firefox和Chrome浏览器中自动截图，并使用视觉语言模型（VLM）分析截图识别XBI。

Result: 在1052个网站测试中，使用微调VLM，XBIDetective的XBI识别准确率达到79%，对动态元素和广告识别准确率分别为84%和85%。

Conclusion: XBIDetective有效检测跨浏览器差异，适用于自动回归测试、大规模网站监控和快速定位XBI缺陷，提升了浏览器渲染漏洞检测能力。

Abstract: Browser rendering bugs can be challenging to detect for browser developers, as they may be triggered by very specific conditions that are exhibited on only a very small subset of websites. Cross-browser inconsistencies (XBIs), variations in how a website is interpreted and displayed on different browsers, can be helpful guides to detect such rendering bugs. Although visual and Document Object Model (DOM)-based analysis techniques exist for detecting XBIs, they often struggle with dynamic and interactive elements. In this study, we discuss our industry experience with using vision language models (VLMs) to identify XBIs. We present the XBIDetective tool which automatically captures screenshots of a website in Mozilla Firefox and Google Chrome, and analyzes them with a VLM for XBIs. We evaluate XBIDetective's performance with an off-the-shelf and a fine-tuned VLM on 1,052 websites. We show that XBIDetective can identify cross-browser discrepancies with 79% accuracy and detect dynamic elements and advertisements with 84% and 85% accuracy, respectively, when using the fine-tuned VLM. We discuss important lessons learned, and we present several potential practical use cases for XBIDetective, including automated regression testing, large-scale monitoring of websites, and rapid triaging of XBI bug reports.

</details>


### [33] [CodeMem: Architecting Reproducible Agents via Dynamic MCP and Procedural Memory](https://arxiv.org/abs/2512.15813)
*Nishant Gaurav,Adit Akarsh,Tejas Ravishankar,Manoj Bajaj*

Main category: cs.SE

TL;DR: 本文提出了CodeMem架构，通过代码实现程序化记忆，解决了当前工具使用型AI代理在操作空间、上下文效率和概率不稳定性方面的不足，提升了重复任务处理的确定性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有工具使用型AI代理在操作空间有限、上下文效率低以及概率不稳定性高，导致它们难以有效处理重复性任务，而传统基于平台的agentic工作流如n8n和Zapier则表现较好。尽管已有方法通过使用完整Python语言扩展操作空间和提高上下文效率，但概率不稳定性问题依然存在。

Method: 本文提出CodeMem架构，通过代码实现程序化记忆，使AI代理能够建立和运行可复用的agentic工作流，保证任务执行的确定性和可靠性。

Result: 通过引入程序化记忆，CodeMem能够在相同任务和环境下保持执行路径的一致性，显著提升了AI代理处理重复性任务的稳定性和可靠性。

Conclusion: CodeMem架构有效解决了概率不稳定性问题，推动了工具使用型AI代理在重复任务处理中的应用，使其表现达到传统agentic工作流平台的确定性和效率水平。

Abstract: Current tool-using AI agents suffer from limited action space, context inefficiency, and probabilistic instability that makes them unsuitable for handling repetitive tasks which are otherwise reliably and efficiently tackled by agentic workflows built on platforms like n8n and Zapier. Earlier works like CodeAct, DynaSaur, Code Mode have tried to tackle the first two issues by using the whole Python language as its action space: The number of tools that the agent can call becomes infinite. Python code blocks can execute complex actions into a single step and print only relevant results which helps in keeping the context lean. However, the probabilistic instability issue still remains, as for the same task in the same environment, the agent can follow different trajectories due to the probabilistic nature of LLMs. Therefore, we need procedural memory for consistency and reliability. This paper proposes CodeMem, an architecture to implement procedural memory via code which can be used to build and run reusable agentic workflows with deterministic reliability.

</details>


### [34] [OLAF: Towards Robust LLM-Based Annotation Framework in Empirical Software Engineering](https://arxiv.org/abs/2512.15979)
*Mia Mohammad Imran,Tarannum Shaila Zaman*

Main category: cs.SE

TL;DR: 本文提出了一个针对基于大型语言模型(LLM)的注释过程的概念框架OLAF，旨在解决注释的可靠性和可重复性不足问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的注释缺乏标准化的可靠性测量、校准和漂移评估，且缺少关键配置细节，影响了结果的可靠性和重现性。

Method: 本文提出了OLAF框架，涵盖可靠性、校准、漂移、一致性、聚合和透明性六大关键构建，倡导将LLM注释视为一种测量过程。

Result: 框架为LLM注释的标准化和规范化提供了理论基础和结构指导。

Conclusion: 通过OLAF，促进软件工程领域中基于LLM注释的透明性和可重复性，推动方法论讨论和未来实证研究。

Abstract: Large Language Models (LLMs) are increasingly used in empirical software engineering (ESE) to automate or assist annotation tasks such as labeling commits, issues, and qualitative artifacts. Yet the reliability and reproducibility of such annotations remain underexplored. Existing studies often lack standardized measures for reliability, calibration, and drift, and frequently omit essential configuration details. We argue that LLM-based annotation should be treated as a measurement process rather than a purely automated activity. In this position paper, we outline the \textbf{Operationalization for LLM-based Annotation Framework (OLAF)}, a conceptual framework that organizes key constructs: \textit{reliability, calibration, drift, consensus, aggregation}, and \textit{transparency}. The paper aims to motivate methodological discussion and future empirical work toward more transparent and reproducible LLM-based annotation in software engineering research.

</details>


### [35] [Embedding Software Intent: Lightweight Java Module Recovery](https://arxiv.org/abs/2512.15980)
*Yirui He,Yuqi Huai,Xingyu Chen,Joshua Garcia*

Main category: cs.SE

TL;DR: 本文提出ClassLAR方法，通过类名和语言模型恢复Java模块，实现了更高效准确的单体Java系统模块化。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统规模急剧扩大，单靠代码层次抽象难以管理，且维护架构与代码一致性困难。现有架构恢复技术在从单体Java项目恢复JPMS模块时效果不佳。

Method: 提出ClassLAR方法，利用完全限定类名及语言模型从包名和类名提取语义信息，捕捉结构和功能意图，实现轻量级模块恢复。

Result: 在20个流行Java项目上，ClassLAR在架构相似性指标上优于所有现有方法，且执行速度快3.99到10.50倍。

Conclusion: ClassLAR有效提升了单体Java项目向JPMS模块化的架构恢复质量和效率，是一种实用的模块恢复方法。

Abstract: As an increasing number of software systems reach unprecedented scale, relying solely on code-level abstractions is becoming impractical. While architectural abstractions offer a means to manage these systems, maintaining their consistency with the actual code has been problematic. The Java Platform Module System (JPMS), introduced in Java 9, addresses this limitation by enabling explicit module specification at the language level. JPMS enhances architectural implementation through improved encapsulation and direct specification of ground-truth architectures within Java projects. Although many projects are written in Java, modularizing existing monolithic projects to JPMS modules is an open challenge due to ineffective module recovery by existing architecture recovery techniques. To address this challenge, this paper presents ClassLAR (Class-and Language model-based Architectural Recovery), a novel, lightweight, and efficient approach that recovers Java modules from monolithic Java systems using fully-qualified class names. ClassLAR leverages language models to extract semantic information from package and class names, capturing both structural and functional intent. In evaluations across 20 popular Java projects, ClassLAR outperformed all state-of-the-art techniques in architectural-level similarity metrics while achieving execution times that were 3.99 to 10.50 times faster.

</details>


### [36] [LLM4Perf: Large Language Models Are Effective Samplers for Multi-Objective Performance Modeling (Copy)](https://arxiv.org/abs/2512.16070)
*Xin Wang,Zhenhao Li,Zishuo Ding*

Main category: cs.SE

TL;DR: 本文提出了基于大规模语言模型（LLM）的采样框架LLM4Perf，用于多目标性能建模，实验证明其在多个真实系统中优于传统方法，提升了配置空间剪枝和策略优化效果。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统性能依赖于复杂配置，现有采样方法难以在多目标优化中表现良好，且无法利用文档中的语义信息。近期LLM的成功激发了研究其作为多目标性能建模采样器的可能性。

Method: 设计并实现了反馈驱动的LLM4Perf框架，系统评估了LLM引导的采样过程在四个高度可配置的真实系统上的表现，通过配置空间剪枝和反馈策略改进提升采样效果。

Result: LLM4Perf在112个评估场景中表现最佳的占68.8%，显著优于传统基线，且其剪枝能力提高了91.5%的基线方法性能，展示了LLM在性能建模中的优越性和策略优化效果。

Conclusion: 研究表明LLM在性能工程中具有显著应用价值，能够有效利用文档语义信息优化采样策略，为多目标性能建模带来突破，同时揭示了驱动成功的关键机制。

Abstract: The performance of modern software systems is critically dependent on their complex configuration options. Building accurate performance models to navigate this vast space requires effective sampling strategies, yet existing methods often struggle with multi-objective optimization and cannot leverage semantic information from documentation. The recent success of Large Language Models (LLMs) motivates the central question of this work: Can LLMs serve as effective samplers for multi-objective performance modeling? To explore this, we present a comprehensive empirical study investigating the capabilities and characteristics of LLM-driven sampling. We design and implement LLM4Perf, a feedback-based framework, and use it to systematically evaluate the LLM-guided sampling process across four highly configurable, real-world systems. Our study reveals that the LLM-guided approach outperforms traditional baselines in most cases. Quantitatively, LLM4Perf achieves the best performance in nearly 68.8% (77 out of 112) of all evaluation scenarios, demonstrating its superior effectiveness. We find this effectiveness stems from the LLM's dual capabilities of configuration space pruning and feedback-driven strategy refinement. The effectiveness of this pruning is further validated by the fact that it also improves the performance of the baseline methods in nearly 91.5% (410 out of 448) of cases. Furthermore, we show how the LLM choices for each component and hyperparameters within LLM4Perf affect its effectiveness. Overall, this paper provides strong evidence for the effectiveness of LLMs in performance engineering and offers concrete insights into the mechanisms that drive their success.

</details>


### [37] [Analysis of Design Patterns and Benchmark Practices in Apache Kafka Event-Streaming Systems](https://arxiv.org/abs/2512.16146)
*Muzeeb Mohammad*

Main category: cs.SE

TL;DR: 本文系统综述了2015至2025年间42篇关于Apache Kafka的研究，归纳出九种常见的Kafka架构设计模式，分析了其使用趋势及基准测试方法，提出统一的分类体系和决策指导。


<details>
  <summary>Details</summary>
Motivation: 尽管Kafka应用广泛且成熟，但关于可重用的架构设计模式和可复现的基准测试方法的研究散见于不同论文，缺乏系统整合和统一指导。

Method: 对42篇经过同行评审的相关研究论文进行结构化综合分析，识别九种Kafka设计模式，考察共用趋势和领域部署，评估基准测试实践的合理性及复现性。

Result: 揭示了配置披露、评估严谨性及复现性方面存在显著差异，限制了研究间的比较和实用复制；提出统一分类、设计模式基准矩阵及决策启发式方法。

Conclusion: 该研究为Kafka流事件系统的架构设计和基准测试提供了统一框架和实用指导，有助于促进高性能、容错性强且可复现的Kafka系统设计。

Abstract: Apache Kafka has become a foundational platform for high throughput event streaming, enabling real time analytics, financial transaction processing, industrial telemetry, and large scale data driven systems. Despite its maturity and widespread adoption, consolidated research on reusable architectural design patterns and reproducible benchmarking methodologies remains fragmented across academic and industrial publications. This paper presents a structured synthesis of forty two peer reviewed studies published between 2015 and 2025, identifying nine recurring Kafka design patterns including log compaction, CQRS bus, exactly once pipelines, change data capture, stream table joins, saga orchestration, tiered storage, multi tenant topics, and event sourcing replay. The analysis examines co usage trends, domain specific deployments, and empirical benchmarking practices using standard suites such as TPCx Kafka and the Yahoo Streaming Benchmark, as well as custom workloads. The study highlights significant inconsistencies in configuration disclosure, evaluation rigor, and reproducibility that limit cross study comparison and practical replication. By providing a unified taxonomy, pattern benchmark matrix, and actionable decision heuristics, this work offers practical guidance for architects and researchers designing reproducible, high performance, and fault tolerant Kafka based event streaming systems.

</details>


### [38] [Beyond Blind Spots: Analytic Hints for Mitigating LLM-Based Evaluation Pitfalls](https://arxiv.org/abs/2512.16272)
*Ora Nova Fandina,Eitan Farchi,Shmulik Froimovich,Raviv Gal,Wesam Ibraheem,Rami Katan,Alice Podolsky*

Main category: cs.SE

TL;DR: 本论文研究了大型语言模型作为代码评审者（LaaJ）在遗留代码现代化中的局限，提出结合轻量级分析工具辅助的方法，大幅提升错误检测率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型作为代码生成流水线的评审工具虽然具备扩展性，但在领域特定问题上存在盲点，影响其在关键评估任务中的可靠性。研究动机是识别这些盲点并提升评审准确率。

Method: 分析实际COBOL代码生成案例，构建领域问题分类法，开发轻量级分析工具检测30余种领域特定问题，并将其检测提示动态注入语言模型提示中提高评审效果。

Result: 单独使用LaaJ只能发现约45%的代码错误，轻量工具缺乏解释深度，将两者结合后，覆盖率提升至94%，且生成更准确丰富的解释。

Conclusion: 通过结合领域特定的分析工具与大型语言模型评审系统，可以显著提升代码评审的准确性和解释质量，增强系统在工业环境中的实用性和可靠性。

Abstract: Large Language Models are increasingly deployed as judges (LaaJ) in code generation pipelines. While attractive for scalability, LaaJs tend to overlook domain specific issues raising concerns about their reliability in critical evaluation tasks. To better understand these limitations in practice, we examine LaaJ behavior in a concrete industrial use case: legacy code modernization via COBOL code generation. In this setting, we find that even production deployed LaaJs can miss domain critical errors, revealing consistent blind spots in their evaluation capabilities.
  To better understand these blind spots, we analyze generated COBOL programs and associated LaaJs judgments, drawing on expert knowledge to construct a preliminary taxonomy. Based on this taxonomy, we develop a lightweight analytic checker tool that flags over 30 domain specific issues observed in practice. We use its outputs as analytic hints, dynamically injecting them into the judges prompt to encourage LaaJ to revisit aspects it may have overlooked.
  Experiments on a test set of 100 programs using four production level LaaJs show that LaaJ alone detects only about 45% of the errors present in the code (in all judges we tested), while the analytic checker alone lacks explanatory depth. When combined, the LaaJ+Hints configuration achieves up to 94% coverage (for the best performing judge and injection prompt) and produces qualitatively richer, more accurate explanations, demonstrating that analytic-LLM hybrids can substantially enhance evaluation reliability in deployed pipelines. We release the dataset and all used prompts.

</details>


### [39] [Using a Sledgehammer to Crack a Nut? Revisiting Automated Compiler Fault Isolation](https://arxiv.org/abs/2512.16335)
*Yibiao Yang,Qingyang Li,Maolin Sun,Jiangchang Wu,Yuming Zhou*

Main category: cs.SE

TL;DR: 本文比较了基于BIC（引入bug的提交）的方法与光谱故障定位技术在编译器缺陷定位中的效果，发现基于BIC的方法表现相当甚至优于SBFL技术。


<details>
  <summary>Details</summary>
Motivation: 虽然光谱故障定位技术被广泛研究用于编译器缺陷定位，但其实用效果尚未与开发者常用的基于版本历史的BIC方法进行直接对比。

Method: 提出Basic方法，通过识别最近的良好版本和最早的坏版本，采用二分查找定位引入bug的提交，将该提交中修改的文件标记为潜在缺陷文件。并利用包含60个GCC和60个LLVM bug的基准测试数据进行严谨比较。

Result: Basic方法在Top-1和Top-5定位性能上与先进的光谱故障定位技术相当甚至更优，证明了其实用性和有效性。

Conclusion: 建议未来编译器缺陷定位研究将Basic作为基线方法，以便更加贴近实际开发中的调试场景，提高研究的实用价值。

Abstract: Background: Compilers are fundamental to software development, translating high-level source code into executable software systems. Faults in compilers can have severe consequences and thus effective localization and resolution of compiler bugs are crucial. Problem: In practice, developers often examine version history to identify and investigate bug-inducing commit (BIC) for fixing bugs. However, while numerous sophisticated Spectrum-Based Fault Localization (SBFL) techniques have been proposed for compiler fault isolation, their effectiveness has not been evaluated against the BIC-based strategies widely adopted in practice. Objective: This study aims to bridge this gap by directly comparing a BIC-based strategy, Basic, with representative SBFL techniques in the context of compiler fault localization. The BIC-based strategy closely aligns with common developer practices, as it directly identifies the BIC and treats the files modified in that commit as faulty candidates. Method: The Basic identifies the most recent good release and earliest bad release, and then employs a binary search to pinpoint the bug-inducing commit. All files modified in the identified commit are flagged as potentially faulty. We rigorously compare Basic against SBFL-based techniques using a benchmark consisting of 60 GCC bugs and 60 LLVM bugs. Result: Our analysis reveals that Basic performs comparably to, and in many cases outperforms, state-of-the-art SBFL-based techniques, particularly on the critical Top-1 and Top-5 ranking metrics. Conclusion: This study provides new insights into the practical effectiveness of SBFL-based techniques in real-world compiler debugging scenarios. We recommend that future research adopt Basic as a baseline when developing and evaluating new compiler fault isolation methods.

</details>


### [40] [An Empirical Study of the Realism of Mutants in Deep Learning](https://arxiv.org/abs/2512.16741)
*Zaheed Ahmed,Philip Makedonski,Jens Grabowski*

Main category: cs.SE

TL;DR: 本文首次实证比较了深度学习中预训练与训练后突变方法的真实性，发现预训练突变体在与真实缺陷的耦合度和行为相似度上表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统软件中的突变分析假设突变体与真实故障类似，但这一假设在深度学习中尚未得到验证，因此需要量化预训练和训练后突变方法与真实缺陷的相似性。

Method: 提出统计框架，通过公开缺陷数据集使用先进工具生成的突变体，比较预训练与训练后突变体的耦合强度及行为相似性。

Result: 预训练突变体表现出更强的耦合和行为相似性，真实性更高；但预训练突变计算成本高。

Conclusion: 预训练突变更真实但成本高，未来需开发更有效的训练后突变算子以达到或超越预训练突变的真实性。

Abstract: Mutation analysis is a well-established technique for assessing test quality in the traditional software development paradigm by injecting artificial faults into programs. Its application to deep learning (DL) has expanded beyond classical testing to support tasks such as fault localization, repair, data generation, and model robustness evaluation. The core assumption is that mutants behave similarly to real faults, an assumption well established in traditional software systems but largely unverified for DL.
  This study presents the first empirical comparison of pre-training and post-training mutation approaches in DL with respect to realism. We introduce a statistical framework to quantify their coupling strength and behavioral similarity to real faults using publicly available bugs datasets: CleanML, DeepFD, DeepLocalize, and defect4ML. Mutants are generated using state-of-the-art tools representing both approaches.
  Results show that pre-training mutants exhibit consistently stronger coupling and higher behavioral similarity to real faults than post-training mutants, indicating greater realism. However, the substantial computational cost of pre-training mutation underscores the need for more effective post-training operators that match or exceed the realism demonstrated by pre-training mutants.

</details>


### [41] [Inside Out: Uncovering How Comment Internalization Steers LLMs for Better or Worse](https://arxiv.org/abs/2512.16790)
*Aaron Imani,Mohammad Moshirpour,Iftekhar Ahmed*

Main category: cs.SE

TL;DR: 本文首次从概念层面解释了大语言模型在软件工程任务中如何内化和利用代码注释，揭示了注释概念对模型性能的显著影响。


<details>
  <summary>Details</summary>
Motivation: 尽管代码注释是不具功能性的代码元素，但大语言模型在执行软件工程任务时大量依赖注释，然而这一依赖机制及其对性能的影响尚不清楚。

Method: 通过使用概念激活向量（CAV）技术，分析了三种任务（代码补全、翻译和改进）中模型对不同类型注释的内部表示，并系统性地激活和关闭这些注释概念，观察其对模型性能的影响。随后，通过控制实验测量了10类软件工程任务中注释概念的激活程度。

Result: 发现大语言模型能将注释内部化为独特的潜在概念，且能区分Javadoc、内联和多行注释，注释概念的激活与关闭导致性能变化显著且依赖具体模型和任务，代码总结任务中的注释激活最强，代码补全最弱。

Conclusion: 研究揭示了代码注释在大语言模型内部的概念化作用及其对性能的影响，为未来构建能够理解和操作内部概念表示的软工工具和模型开辟了新方向。

Abstract: While comments are non-functional elements of source code, Large Language Models (LLM) frequently rely on them to perform Software Engineering (SE) tasks. Yet, where in the model this reliance resides, and how it affects performance, remains poorly understood. We present the first concept-level interpretability study of LLMs in SE, analyzing three tasks - code completion, translation, and refinement - through the lens of internal comment representation. Using Concept Activation Vectors (CAV), we show that LLMs not only internalize comments as distinct latent concepts but also differentiate between subtypes such as Javadocs, inline, and multiline comments. By systematically activating and deactivating these concepts in the LLMs' embedding space, we observed significant, model-specific, and task-dependent shifts in performance ranging from -90% to +67%. Finally, we conducted a controlled experiment using the same set of code inputs, prompting LLMs to perform 10 distinct SE tasks while measuring the activation of the comment concept within their latent representations. We found that code summarization consistently triggered the strongest activation of comment concepts, whereas code completion elicited the weakest sensitivity. These results open a new direction for building SE tools and models that reason about and manipulate internal concept representations rather than relying solely on surface-level input.

</details>


### [42] [Toward Systematic Counterfactual Fairness Evaluation of Large Language Models: The CAFFE Framework](https://arxiv.org/abs/2512.16816)
*Alessandra Parziale,Gianmario Voria,Valeria Pontillo,Gemma Catolino,Andrea De Lucia,Fabio Palomba*

Main category: cs.SE

TL;DR: 本文提出了一个名为CAFFE的结构化、意图感知的反事实公平性测试框架，用于检测大型语言模型的公平性问题，效果优于现有的变异测试方法。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在软件系统中的广泛应用，其公平性问题变得尤为重要，现有方法如变异测试在检测公平性不足时存在局限。

Method: 设计CAFFE框架，通过明确的测试用例组成（包括提示意图、对话上下文、输入变体、公平性阈值和测试环境配置），自动生成针对性测试数据，并使用语义相似度指标评估模型响应。

Result: 在三种不同架构的大型语言模型上实验，CAFFE显示出更广泛的偏见覆盖率和更可靠的公平性检测效果。

Conclusion: CAFFE作为一种创新的反事实公平性测试框架，能够更有效地识别大型语言模型中的不公平行为，优于传统的变异测试方法。

Abstract: Nowadays, Large Language Models (LLMs) are foundational components of modern software systems. As their influence grows, concerns about fairness have become increasingly pressing. Prior work has proposed metamorphic testing to detect fairness issues, applying input transformations to uncover inconsistencies in model behavior. This paper introduces an alternative perspective for testing counterfactual fairness in LLMs, proposing a structured and intent-aware framework coined CAFFE (Counterfactual Assessment Framework for Fairness Evaluation). Inspired by traditional non-functional testing, CAFFE (1) formalizes LLM-Fairness test cases through explicitly defined components, including prompt intent, conversational context, input variants, expected fairness thresholds, and test environment configuration, (2) assists testers by automatically generating targeted test data, and (3) evaluates model responses using semantic similarity metrics. Our experiments, conducted on three different architectural families of LLM, demonstrate that CAFFE achieves broader bias coverage and more reliable detection of unfair behavior than existing metamorphic approaches.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [43] [Ev-Trust: A Strategy Equilibrium Trust Mechanism for Evolutionary Games in LLM-Based Multi-Agent Services](https://arxiv.org/abs/2512.16167)
*Shiduo Yang,Jiye Wang,Jiayu Qin,Jianbin Li,Yu Wang,Yuanhe Zhao,Kenan Guo*

Main category: cs.MA

TL;DR: 本文提出了Ev-Trust，一种基于进化博弈论的信任机制，用于提升基于大语言模型（LLM）的多智能体系统中的信任建立和系统鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着Web向以智能体为中心的范式演进，多智能体系统面临欺骗、欺诈和虚假信息的风险，亟需有效的信任机制保障系统运行。

Method: Ev-Trust机制结合直接信任、间接信任和预期收益，构建动态反馈结构，指导智能体行为演化至均衡状态；基于“请求-响应-支付-评价”的服务框架，智能体可自适应调整策略以排除恶意参与者。

Result: 理论推导证明了局部进化均衡的存在与稳定性；实验表明该方法有效反映智能体的可信度，减少恶意策略，提升整体收益。

Conclusion: Ev-Trust为基于LLM的多智能体服务网络提供了一种新的信任建模视角，促进了高质量协作和系统鲁棒性。

Abstract: The rapid evolution of the Web toward an agent-centric paradigm, driven by large language models (LLMs), has enabled autonomous agents to reason, plan, and interact in complex decentralized environments. However, the openness and heterogeneity of LLM-based multi-agent systems also amplify the risks of deception, fraud, and misinformation, posing severe challenges to trust establishment and system robustness. To address this issue, we propose Ev-Trust, a strategy-equilibrium trust mechanism grounded in evolutionary game theory. This mechanism integrates direct trust, indirect trust, and expected revenue into a dynamic feedback structure that guides agents' behavioral evolution toward equilibria. Within a decentralized "Request-Response-Payment-Evaluation" service framework, Ev-Trust enables agents to adaptively adjust strategies, naturally excluding malicious participants while reinforcing high-quality collaboration. Furthermore, our theoretical derivation based on replicator dynamics equations proves the existence and stability of local evolutionary equilibria. Experimental results indicate that our approach effectively reflects agent trustworthiness in LLM-driven open service interaction scenarios, reduces malicious strategies, and increases collective revenue. We hope Ev-Trust can provide a new perspective on trust modeling for the agentic service web in group evolutionary game scenarios.

</details>


### [44] [Don't Guess, Escalate: Towards Explainable Uncertainty-Calibrated AI Forensic Agents](https://arxiv.org/abs/2512.16614)
*Giulia Boato,Andrea Montibeller,Edward Delp,Luisa Verdoliva,Daniele Miorandi*

Main category: cs.MA

TL;DR: 提出了AI法医代理作为多媒体取证的可靠协调者，通过选择和组合检测器，提高真实性验证的统一框架。


<details>
  <summary>Details</summary>
Motivation: 当前多媒体取证方法存在不足，亟需提高检测的可靠性和透明度。

Method: 引入AI法医代理，作为能够选择和结合多种取证检测器，识别来源与上下文，并提供不确定性评估的新型协调者。

Result: 发现当前解决方案存在诸多缺陷，设计了统一框架以提升真实性验证流程的稳定性和准确性。

Conclusion: AI法医代理的应用能有效改进多媒体真实性验证的过程，解决传统方法中存在的不足。

Abstract: AI is reshaping the landscape of multimedia forensics. We propose AI forensic agents: reliable orchestrators that select and combine forensic detectors, identify provenance and context, and provide uncertainty-aware assessments. We highlight pitfalls in current solutions and introduce a unified framework to improve the authenticity verification process.

</details>
