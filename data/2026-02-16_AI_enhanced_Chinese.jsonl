{"id": "2602.12284", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12284", "abs": "https://arxiv.org/abs/2602.12284", "authors": ["Han Jinzhen", "Kim Jisung", "Yang Jong Soo", "Yun Hong Sik"], "title": "A Lightweight LLM Framework for Disaster Humanitarian Information Classification", "comment": null, "summary": "Timely classification of humanitarian information from social media is critical for effective disaster response. However, deploying large language models (LLMs) for this task faces challenges in resource-constrained emergency settings. This paper develops a lightweight, cost-effective framework for disaster tweet classification using parameter-efficient fine-tuning. We construct a unified experimental corpus by integrating and normalizing the HumAID dataset (76,484 tweets across 19 disaster events) into a dual-task benchmark: humanitarian information categorization and event type identification. Through systematic evaluation of prompting strategies, LoRA fine-tuning, and retrieval-augmented generation (RAG) on Llama 3.1 8B, we demonstrate that: (1) LoRA achieves 79.62% humanitarian classification accuracy (+37.79% over zero-shot) while training only ~2% of parameters; (2) QLoRA enables efficient deployment with 99.4% of LoRA performance at 50% memory cost; (3) contrary to common assumptions, RAG strategies degrade fine-tuned model performance due to label noise from retrieved examples. These findings establish a practical, reproducible pipeline for building reliable crisis intelligence systems with limited computational resources.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u4e14\u9ad8\u6548\u7684\u707e\u96be\u63a8\u6587\u5206\u7c7b\u6846\u67b6\uff0c\u5229\u7528\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6280\u672f\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6027\u80fd\u4eba\u9053\u4e3b\u4e49\u4fe1\u606f\u5206\u7c7b\uff0c\u4e14\u53d1\u73b0RAG\u7b56\u7565\u53ef\u80fd\u964d\u4f4e\u6548\u679c\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u4e2d\u7684\u4eba\u9053\u4e3b\u4e49\u4fe1\u606f\u53ca\u65f6\u5206\u7c7b\u5bf9\u4e8e\u707e\u96be\u5e94\u5bf9\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728\u8d44\u6e90\u53d7\u9650\u7684\u7d27\u6025\u73af\u5883\u4e2d\u90e8\u7f72\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u6311\u6218\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u3001\u6210\u672c\u6548\u76ca\u9ad8\u7684\u707e\u96be\u63a8\u6587\u5206\u7c7b\u6846\u67b6\uff0c\u91c7\u7528\u53c2\u6570\u9ad8\u6548\u7684\u5fae\u8c03\u65b9\u6cd5\uff08LoRA\u548cQLoRA\uff09\uff0c\u5728Llama 3.1 8B\u6a21\u578b\u4e0a\u7cfb\u7edf\u8bc4\u4f30\u4e86\u63d0\u793a\u7b56\u7565\u3001\u5fae\u8c03\u548c\u57fa\u4e8e\u68c0\u7d22\u7684\u751f\u6210\uff08RAG\uff09\u3002", "result": "LoRA\u5fae\u8c03\u65b9\u6cd5\u5728\u53ea\u8bad\u7ec3\u7ea62%\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e8679.62%\u7684\u4eba\u9053\u4e3b\u4e49\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u8f83\u96f6-shot\u63d0\u534737.79%\uff1bQLoRA\u65b9\u6cd5\u4ee550%\u5185\u5b58\u6210\u672c\u8fbe\u5230LoRA\u768499.4%\u6027\u80fd\uff1bRAG\u7b56\u7565\u53cd\u800c\u56e0\u68c0\u7d22\u793a\u4f8b\u7684\u6807\u7b7e\u566a\u58f0\u964d\u4f4e\u4e86\u6a21\u578b\u8868\u73b0\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u53c2\u6570\u9ad8\u6548\u7684\u5fae\u8c03\uff08LoRA\u548cQLoRA\uff09\u662f\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u6784\u5efa\u53ef\u4fe1\u5371\u673a\u667a\u80fd\u7cfb\u7edf\u7684\u5b9e\u7528\u65b9\u6cd5\uff0c\u800c\u57fa\u4e8e\u68c0\u7d22\u7684\u751f\u6210\u7b56\u7565\u7531\u4e8e\u6807\u7b7e\u566a\u58f0\u4e0d\u9002\u5408\u6b64\u4efb\u52a1\u3002"}}
{"id": "2602.12285", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12285", "abs": "https://arxiv.org/abs/2602.12285", "authors": ["Linbo Cao", "Lihao Sun", "Yang Yue"], "title": "From Biased Chatbots to Biased Agents: Examining Role Assignment Effects on LLM Agent Robustness", "comment": "Accepted to the AAAI 2026 TrustAgent Workshop. 6 pages, 4 figures", "summary": "Large Language Models (LLMs) are increasingly deployed as autonomous agents capable of actions with real-world impacts beyond text generation. While persona-induced biases in text generation are well documented, their effects on agent task performance remain largely unexplored, even though such effects pose more direct operational risks. In this work, we present the first systematic case study showing that demographic-based persona assignments can alter LLM agents' behavior and degrade performance across diverse domains. Evaluating widely deployed models on agentic benchmarks spanning strategic reasoning, planning, and technical operations, we uncover substantial performance variations - up to 26.2% degradation, driven by task-irrelevant persona cues. These shifts appear across task types and model architectures, indicating that persona conditioning and simple prompt injections can distort an agent's decision-making reliability. Our findings reveal an overlooked vulnerability in current LLM agentic systems: persona assignments can introduce implicit biases and increase behavioral volatility, raising concerns for the safe and robust deployment of LLM agents.", "AI": {"tldr": "\u8eab\u4efd\u8bbe\u5b9a\u4f1a\u663e\u8457\u5f71\u54cd\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u7684\u6027\u80fd\u548c\u884c\u4e3a\uff0c\u5e26\u6765\u5b89\u5168\u548c\u53ef\u9760\u6027\u98ce\u9669\uff0c\u5e94\u5f15\u8d77\u91cd\u89c6\u3002", "motivation": "\u63a2\u8ba8\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f5c\u4e3a\u81ea\u4e3b\u4ee3\u7406\u5728\u5b9e\u9645\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5c24\u5176\u662f\u8eab\u4efd\u8bbe\u5b9a\uff08persona\uff09\u5bf9\u4efb\u52a1\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u56e0\u4e3a\u6b64\u524d\u5bf9\u6587\u672c\u751f\u6210\u4e2d\u7684\u8eab\u4efd\u504f\u89c1\u5df2\u6709\u7814\u7a76\uff0c\u4f46\u5bf9\u4ee3\u7406\u4efb\u52a1\u6027\u80fd\u7684\u5f71\u54cd\u7f3a\u4e4f\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u5728\u5404\u79cd\u4ee3\u7406\u4efb\u52a1\u57fa\u51c6\uff08\u6db5\u76d6\u6218\u7565\u63a8\u7406\u3001\u89c4\u5212\u548c\u6280\u672f\u64cd\u4f5c\uff09\u4e0a\u8bc4\u4f30\u5e7f\u6cdb\u90e8\u7f72\u7684LLM\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u4eba\u53e3\u7edf\u8ba1\u7684\u8eab\u4efd\u8bbe\u5b9a\uff0c\u89c2\u5bdf\u5176\u5bf9\u4ee3\u7406\u884c\u4e3a\u548c\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u8eab\u4efd\u8bbe\u5b9a\u5bfc\u81f4\u7684\u4efb\u52a1\u6027\u80fd\u6ce2\u52a8\u663e\u8457\uff0c\u6700\u9ad8\u53ef\u8fbe26.2%\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u8fd9\u79cd\u5f71\u54cd\u8de8\u4efb\u52a1\u7c7b\u578b\u548c\u6a21\u578b\u67b6\u6784\u666e\u904d\u5b58\u5728\uff0c\u8868\u660e\u7b80\u5355\u7684\u8eab\u4efd\u63d0\u793a\u53ef\u4ee5\u626d\u66f2\u4ee3\u7406\u7684\u51b3\u7b56\u53ef\u9760\u6027\u3002", "conclusion": "\u8eab\u4efd\u8bbe\u5b9a\u662f\u5f53\u524dLLM\u4ee3\u7406\u7cfb\u7edf\u4e2d\u7684\u4e00\u4e2a\u88ab\u5ffd\u89c6\u7684\u8106\u5f31\u70b9\uff0c\u53ef\u80fd\u5f15\u5165\u9690\u6027\u504f\u89c1\u5e76\u589e\u52a0\u884c\u4e3a\u7684\u6ce2\u52a8\u6027\uff0c\u8fdb\u800c\u5f71\u54cd\u5176\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\uff0c\u9700\u8981\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u4e88\u4ee5\u91cd\u89c6\u3002"}}
{"id": "2602.12287", "categories": ["cs.CL", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.12287", "abs": "https://arxiv.org/abs/2602.12287", "authors": ["Junjie An", "Jingguang Tian", "Tianyi Wang", "Yu Gao", "Xiaofeng Mou", "Yi Xu"], "title": "Retrieval-Augmented Self-Taught Reasoning Model with Adaptive Chain-of-Thought for ASR Named Entity Correction", "comment": null, "summary": "End-to-end automatic speech recognition (ASR) systems frequently misrecognize domain-specific phrases like named entities, which can cause catastrophic failures in downstream tasks. A new family of named entity correction methods based on large language models (LLMs) has recently emerged. However, these approaches have yet to fully exploit the sophisticated reasoning capabilities inherent to LLMs. To bridge this gap, we propose a novel retrieval-augmented generation framework for correcting named entity errors in ASR. Our approach consists of two key components: (1) a rephrasing language model (RLM) for named entity recognition, followed by candidate retrieval using a phonetic-level edit distance; and (2) a novel self-taught reasoning model with adaptive chain-of-thought (A-STAR) that dynamically adjusts the depth of its reasoning based on task difficulty. Experiments on the AISHELL-1 and Homophone datasets demonstrate the effectiveness of our method, which achieves relative reductions in the named entity character error rate of 17.96\\% and 34.42\\%, respectively, compared to a strong baseline.", "AI": {"tldr": "\u901a\u8fc7\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u4e0e\u81ea\u9002\u5e94\u94fe\u5f0f\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u4e2d\u7684\u547d\u540d\u5b9e\u4f53\u7ea0\u6b63\u6548\u679c\u3002", "motivation": "\u7aef\u5230\u7aef\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u5e38\u8bef\u8bc6\u522b\u9886\u57df\u7279\u5b9a\u77ed\u8bed\u5982\u547d\u540d\u5b9e\u4f53\uff0c\u5bfc\u81f4\u4e0b\u6e38\u4efb\u52a1\u4e25\u91cd\u5931\u8d25\uff0c\u73b0\u6709\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u547d\u540d\u5b9e\u4f53\u7ea0\u6b63\u65b9\u6cd5\u672a\u5145\u5206\u5229\u7528\u5176\u590d\u6742\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u547d\u540d\u5b9e\u4f53\u7ea0\u6b63\u6846\u67b6\uff0c\u5305\u62ec\u91cd\u8ff0\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u53ca\u57fa\u4e8e\u8bed\u97f3\u7f16\u8f91\u8ddd\u79bb\u7684\u5019\u9009\u68c0\u7d22\uff0c\u4ee5\u53ca\u81ea\u9002\u5e94\u94fe\u5f0f\u63a8\u7406\u7684\u81ea\u5b66\u63a8\u7406\u6a21\u578b\u3002", "result": "\u5728AISHELL-1\u548cHomophone\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4\u5f3a\u57fa\u7ebf\uff0c\u547d\u540d\u5b9e\u4f53\u5b57\u9519\u8bef\u7387\u76f8\u5bf9\u964d\u4f4e17.96%\u548c34.42%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u663e\u8457\u51cf\u5c11\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u7684\u547d\u540d\u5b9e\u4f53\u9519\u8bef\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u7684\u5b9e\u7528\u6027\u548c\u4f18\u8d8a\u6027\u3002"}}
{"id": "2602.12302", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.12302", "abs": "https://arxiv.org/abs/2602.12302", "authors": ["Neemias da Silva", "J\u00falio C. W. Scholz", "John Harrison", "Marina Borges", "Paulo \u00c1vila", "Frances A Santos", "Myriam Delgado", "Rodrigo Minetto", "Thiago H Silva"], "title": "Grandes Modelos de Linguagem Multimodais (MLLMs): Da Teoria \u00e0 Pr\u00e1tica", "comment": "in Portuguese language. Accepted book chapter - Webmedia 2025", "summary": "Multimodal Large Language Models (MLLMs) combine the natural language understanding and generation capabilities of LLMs with perception skills in modalities such as image and audio, representing a key advancement in contemporary AI. This chapter presents the main fundamentals of MLLMs and emblematic models. Practical techniques for preprocessing, prompt engineering, and building multimodal pipelines with LangChain and LangGraph are also explored. For further practical study, supplementary material is publicly available online: https://github.com/neemiasbsilva/MLLMs-Teoria-e-Pratica. Finally, the chapter discusses the challenges and highlights promising trends.", "AI": {"tldr": "\u4ecb\u7ecd\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u57fa\u7840\u3001\u6280\u672f\u5b9e\u73b0\u548c\u5e94\u7528\uff0c\u9644\u5e26\u5f00\u6e90\u8d44\u6e90\uff0c\u8ba8\u8bba\u672a\u6765\u6311\u6218\u4e0e\u8d8b\u52bf\u3002", "motivation": "\u5f53\u524d\u4eba\u5de5\u667a\u80fd\u9886\u57df\u4e2d\uff0c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e0e\u591a\u6a21\u6001\u611f\u77e5\u6280\u672f\u7684\u7ed3\u5408\u662f\u91cd\u8981\u53d1\u5c55\u65b9\u5411\u3002", "method": "\u4ecb\u7ecd\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u57fa\u672c\u539f\u7406\u53ca\u5178\u578b\u6a21\u578b\uff0c\u8bb2\u89e3\u6570\u636e\u9884\u5904\u7406\u3001\u63d0\u793a\u5de5\u7a0b\u53ca\u591a\u6a21\u6001\u7ba1\u9053\u6784\u5efa\u6280\u672f\uff08\u4f7f\u7528LangChain\u548cLangGraph\uff09\u3002", "result": "\u7cfb\u7edf\u603b\u7ed3\u4e86MLLMs\u7684\u5173\u952e\u6280\u672f\u548c\u5e94\u7528\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u5f00\u6e90\u4ee3\u7801\u4f9b\u5b9e\u8df5\u5b66\u4e60\u3002", "conclusion": "\u672c\u7ae0\u8282\u5168\u9762\u9610\u8ff0\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7406\u8bba\u57fa\u7840\u4e0e\u5b9e\u8df5\u6280\u672f\uff0c\u6307\u51fa\u5f53\u524d\u9762\u4e34\u7684\u6311\u6218\u53ca\u672a\u6765\u53d1\u5c55\u8d8b\u52bf\u3002"}}
{"id": "2602.12386", "categories": ["cs.MA", "cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12386", "abs": "https://arxiv.org/abs/2602.12386", "authors": ["Yizhou Zhang", "Eric Mazumdar"], "title": "Provably Convergent Actor-Critic in Risk-averse MARL", "comment": null, "summary": "Learning stationary policies in infinite-horizon general-sum Markov games (MGs) remains a fundamental open problem in Multi-Agent Reinforcement Learning (MARL). While stationary strategies are preferred for their practicality, computing stationary forms of classic game-theoretic equilibria is computationally intractable -- a stark contrast to the comparative ease of solving single-agent RL or zero-sum games. To bridge this gap, we study Risk-averse Quantal response Equilibria (RQE), a solution concept rooted in behavioral game theory that incorporates risk aversion and bounded rationality. We demonstrate that RQE possesses strong regularity conditions that make it uniquely amenable to learning in MGs. We propose a novel two-timescale Actor-Critic algorithm characterized by a fast-timescale actor and a slow-timescale critic. Leveraging the regularity of RQE, we prove that this approach achieves global convergence with finite-sample guarantees. We empirically validate our algorithm in several environments to demonstrate superior convergence properties compared to risk-neutral baselines.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5177\u6709\u98ce\u9669\u538c\u6076\u548c\u6709\u9650\u7406\u6027\u7684\u98ce\u9669\u538c\u6076\u91cf\u5b50\u54cd\u5e94\u5747\u8861\uff08RQE\uff09\u5728\u65e0\u9650\u65f6\u57df\u4e00\u822c\u548c\u5f0f\u9a6c\u5c14\u53ef\u592b\u535a\u5f08\u4e2d\u7684\u5b66\u4e60\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u65f6\u95f4\u5c3a\u5ea6\u7684Actor-Critic\u7b97\u6cd5\uff0c\u5e76\u8bc1\u660e\u5176\u5168\u5c40\u6536\u655b\u6027\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u4f20\u7edf\u7ecf\u5178\u535a\u5f08\u8bba\u4e2d\u9a6c\u5c14\u53ef\u592b\u535a\u5f08\u7684\u5e73\u7a33\u7b56\u7565\u8ba1\u7b97\u590d\u6742\uff0c\u96be\u4ee5\u5728\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u5b9e\u7528\uff0c\u6545\u5f15\u5165\u5305\u542b\u98ce\u9669\u538c\u6076\u548c\u6709\u9650\u7406\u6027\u7684 RQE \u4f5c\u4e3a\u53ef\u5b66\u4e14\u5b9e\u7528\u7684\u5747\u8861\u6982\u5ff5\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5305\u542b\u5feb\u901f\u65f6\u95f4\u5c3a\u5ea6\u7684actor\u548c\u6162\u901f\u65f6\u95f4\u5c3a\u5ea6\u7684critic\u7684 Actor-Critic \u7b97\u6cd5\uff0c\u5229\u7528 RQE \u7684\u6b63\u5219\u6027\u8d28\u8fdb\u884c\u5b66\u4e60\u3002", "result": "\u8be5\u7b97\u6cd5\u5728\u591a\u4e2a\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u6bd4\u98ce\u9669\u4e2d\u6027\u57fa\u7ebf\u66f4\u4f18\u7684\u6536\u655b\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u7406\u8bba\u6536\u655b\u6027\u548c\u65b9\u6cd5\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u98ce\u9669\u538c\u6076\u91cf\u5b50\u54cd\u5e94\u5747\u8861\u7684\u53cc\u65f6\u95f4\u5c3a\u5ea6Actor-Critic\u7b97\u6cd5\u5b9e\u73b0\u4e86\u5168\u5c40\u6536\u655b\u4e14\u5177\u6709\u6709\u9650\u6837\u672c\u4fdd\u8bc1\uff0c\u4f18\u4e8e\u98ce\u9669\u4e2d\u6027\u65b9\u6cd5\u3002"}}
{"id": "2602.12311", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12311", "abs": "https://arxiv.org/abs/2602.12311", "authors": ["Prashant Shende", "Bradley Camburn"], "title": "Perceptual Self-Reflection in Agentic Physics Simulation Code Generation", "comment": "15 pages, 2 figures, 2 tables. Introduces a multi-agent architecture for physics simulation code generation with perceptual self-reflection via vision-based validation. Includes qualitative evaluation across multiple physics domains", "summary": "We present a multi-agent framework for generating physics simulation code from natural language descriptions, featuring a novel perceptual self-reflection mechanism for validation. The system employs four specialized agents: a natural language interpreter that converts user requests into physics-based descriptions; a technical requirements generator that produces scaled simulation parameters; a physics code generator with automated self-correction; and a physics validator that implements perceptual self-reflection. The key innovation is perceptual validation, which analyzes rendered animation frames using a vision-capable language model rather than inspecting code structure directly. This approach addresses the ``oracle gap'' where syntactically correct code produces physically incorrect behavior--a limitation that conventional testing cannot detect. We evaluate the system across seven domains including classical mechanics, fluid dynamics, thermodynamics, electromagnetics, wave physics, reaction-diffusion systems, and non-physics data visualization. The perceptual self-reflection architecture demonstrates substantial improvement over single-shot generation baselines, with the majority of tested scenarios achieving target physics accuracy thresholds. The system exhibits robust pipeline stability with consistent code self-correction capability, operating at approximately \\$0.20 per animation. These results validate our hypothesis that feeding visual simulation outputs back to a vision-language model for iterative refinement significantly outperforms single-shot code generation for physics simulation tasks and highlights the potential of agentic AI to support engineering workflows and physics data generation pipelines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u53ca\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u611f\u77e5\u81ea\u53cd\u9988\u673a\u5236\uff0c\u63d0\u5347\u4ece\u81ea\u7136\u8bed\u8a00\u751f\u6210\u7269\u7406\u4eff\u771f\u4ee3\u7801\u7684\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4f20\u7edf\u6d4b\u8bd5\u68c0\u6d4b\u4e0d\u5230\u7684\u7269\u7406\u884c\u4e3a\u9519\u8bef\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u4ece\u81ea\u7136\u8bed\u8a00\u751f\u6210\u7269\u7406\u4eff\u771f\u4ee3\u7801\u65f6\u5b58\u5728\u7684\u201coracle gap\u201d\u95ee\u9898\uff0c\u5373\u8bed\u6cd5\u6b63\u786e\u4f46\u7269\u7406\u884c\u4e3a\u9519\u8bef\u7684\u4ee3\u7801\u4f20\u7edf\u6d4b\u8bd5\u96be\u4ee5\u68c0\u6d4b\uff0c\u63d0\u5347\u4eff\u771f\u4ee3\u7801\u7684\u7269\u7406\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86\u56db\u4e2a\u4e13\u804c\u667a\u80fd\u4f53\u2014\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u5668\u3001\u6280\u672f\u9700\u6c42\u751f\u6210\u5668\u3001\u81ea\u52a8\u4ee3\u7801\u751f\u6210\u4e0e\u81ea\u7ea0\u6b63\u6a21\u5757\u3001\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7269\u7406\u9a8c\u8bc1\u5668\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u4eff\u771f\u52a8\u753b\u5e27\u7684\u89c6\u89c9\u4fe1\u606f\u8fdb\u884c\u9a8c\u8bc1\u548c\u8fed\u4ee3\u4f18\u5316\uff0c\u5b8c\u6210\u9ad8\u8d28\u91cf\u4ee3\u7801\u751f\u6210\u3002", "result": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u751f\u6210\u7269\u7406\u4eff\u771f\u4ee3\u7801\uff0c\u6838\u5fc3\u521b\u65b0\u662f\u5f15\u5165\u4e86\u611f\u77e5\u5f0f\u81ea\u6211\u53cd\u601d\u673a\u5236\u5bf9\u751f\u6210\u7684\u4eff\u771f\u8fdb\u884c\u9a8c\u8bc1\u3002\u7cfb\u7edf\u5305\u542b\u56db\u4e2a\u4e13\u95e8\u667a\u80fd\u4f53\uff1a\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u5668\u3001\u6280\u672f\u9700\u6c42\u751f\u6210\u5668\u3001\u5e26\u81ea\u52a8\u81ea\u7ea0\u6b63\u7684\u7269\u7406\u4ee3\u7801\u751f\u6210\u5668\u4ee5\u53ca\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7269\u7406\u9a8c\u8bc1\u5668\u3002\u8be5\u9a8c\u8bc1\u65b9\u6cd5\u901a\u8fc7\u5206\u6790\u6e32\u67d3\u52a8\u753b\u5e27\u800c\u975e\u76f4\u63a5\u68c0\u9a8c\u4ee3\u7801\u7ed3\u6784\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6d4b\u8bd5\u65e0\u6cd5\u53d1\u73b0\u7684\u8bed\u6cd5\u6b63\u786e\u4f46\u7269\u7406\u884c\u4e3a\u9519\u8bef\u7684\u95ee\u9898\uff08\"oracle gap\"\uff09\u3002\u5728\u6db5\u76d6\u5305\u62ec\u7ecf\u5178\u529b\u5b66\u3001\u6d41\u4f53\u529b\u5b66\u3001\u70ed\u529b\u5b66\u7b49\u4e03\u4e2a\u9886\u57df\u7684\u8bc4\u6d4b\u4e2d\uff0c\u7cfb\u7edf\u8868\u73b0\u51fa\u8f83\u5355\u6b21\u751f\u6210\u65b9\u6cd5\u663e\u8457\u7684\u51c6\u786e\u6027\u63d0\u5347\u548c\u7a33\u5b9a\u7684\u4ee3\u7801\u81ea\u7ea0\u6b63\u80fd\u529b\u3002\u6bcf\u4e2a\u52a8\u753b\u751f\u6210\u6210\u672c\u7ea60.20\u7f8e\u5143\uff0c\u8868\u660e\u8be5\u65b9\u6cd5\u6709\u6548\u652f\u6301\u5de5\u7a0b\u53ca\u7269\u7406\u6570\u636e\u751f\u6210\u6d41\u7a0b\u3002", "conclusion": "\u5f15\u5165\u611f\u77e5\u81ea\u6211\u53cd\u601d\u7684\u591a\u667a\u80fd\u4f53\u67b6\u6784\u663e\u8457\u63d0\u5347\u4e86\u7269\u7406\u4eff\u771f\u4ee3\u7801\u751f\u6210\u7684\u7269\u7406\u51c6\u786e\u6027\u548c\u81ea\u7ea0\u6b63\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u89c6\u89c9\u53cd\u9988\u5728\u7269\u7406\u4eff\u771f\u4efb\u52a1\u4e2d\u4f18\u4e8e\u5355\u6b21\u751f\u6210\u7684\u5047\u8bbe\uff0c\u5c55\u793a\u4e86\u8be5\u6280\u672f\u5728\u5de5\u7a0b\u548c\u7269\u7406\u6570\u636e\u751f\u6210\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2602.12414", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12414", "abs": "https://arxiv.org/abs/2602.12414", "authors": ["Maximilian Idahl", "Benedikt Droste", "Bj\u00f6rn Pl\u00fcster", "Jan Philipp Harries"], "title": "propella-1: Multi-Property Document Annotation for LLM Data Curation at Scale", "comment": "Release: https://hf.co/collections/ellamind/propella-1", "summary": "Since FineWeb-Edu, data curation for LLM pretraining has predominantly relied on single scalar quality scores produced by small classifiers. A single score conflates multiple quality dimensions, prevents flexible filtering, and offers no interpretability. We introduce propella-1, a family of small multilingual LLMs (0.6B, 1.7B, 4B parameters) that annotate text documents across 18 properties organized into six categories: core content, classification, quality and value, audience and purpose, safety and compliance, and geographic relevance. The models support 57 languages and produce structured JSON annotations conforming to a predefined schema. Evaluated against a frontier commercial LLM as a reference annotator, the 4B model achieves higher agreement than much larger general-purpose models. We release propella-annotations, a dataset of over three billion document annotations covering major pretraining corpora including data from FineWeb-2, FinePDFs, HPLT 3.0, and Nemotron-CC. Using these annotations, we present a multi-dimensional compositional analysis of widely used pretraining datasets, revealing substantial differences in quality, reasoning depth, and content composition that single-score approaches cannot capture. All model weights and annotations are released under permissive, commercial-use licenses.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86propella-1\uff0c\u4e00\u79cd\u591a\u8bed\u8a00\u5c0f\u578b\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u80fd\u591f\u5bf9\u6587\u672c\u6570\u636e\u8fdb\u884c\u591a\u7ef4\u5ea6\u3001\u7ed3\u6784\u5316\u7684\u8d28\u91cf\u6ce8\u91ca\uff0c\u63d0\u5347\u4e86\u8bed\u6599\u9884\u5904\u7406\u7684\u7075\u6d3b\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u9884\u8bad\u7ec3\u8bed\u6599\u8d28\u91cf\u8bc4\u4f30\u4f9d\u8d56\u5355\u4e00\u8d28\u91cf\u5206\u503c\uff0c\u7f3a\u4e4f\u5bf9\u591a\u7ef4\u8d28\u91cf\u5c5e\u6027\u7684\u533a\u5206\u548c\u89e3\u91ca\uff0c\u4e0d\u5229\u4e8e\u7075\u6d3b\u7b5b\u9009\u548c\u6df1\u5165\u5206\u6790\u3002", "method": "\u8bbe\u8ba1\u5e76\u8bad\u7ec3\u591a\u8bed\u8a00\u7684\u5c0f\u578bLLM\uff0c\u80fd\u9488\u5bf9\u6587\u672c\u768418\u4e2a\u5c5e\u6027\u8fdb\u884c\u7ed3\u6784\u5316JSON\u683c\u5f0f\u7684\u6ce8\u91ca\uff0c\u6db5\u76d6\u516d\u5927\u7c7b\u522b\uff0c\u652f\u630157\u79cd\u8bed\u8a00\uff0c\u5e76\u901a\u8fc7\u4e0e\u5546\u4e1a\u5927\u578b\u6a21\u578b\u5bf9\u6bd4\u9a8c\u8bc1\u6027\u80fd\u3002", "result": "propella-1 4B\u6a21\u578b\u5728\u6807\u6ce8\u4e00\u81f4\u6027\u4e0a\u8d85\u8fc7\u5927\u578b\u901a\u7528\u6a21\u578b\uff0c\u53d1\u5e03\u4e86\u8986\u76d6\u4e3b\u8981\u9884\u8bad\u7ec3\u8bed\u6599\u5e93\u768430\u4ebf\u6761\u6587\u6863\u591a\u7ef4\u6ce8\u91ca\u6570\u636e\uff0c\u63ed\u793a\u4e86\u5355\u5206\u503c\u65b9\u6cd5\u96be\u4ee5\u53d1\u73b0\u7684\u8bed\u6599\u8d28\u91cf\u548c\u5185\u5bb9\u5dee\u5f02\u3002", "conclusion": "propella-1\u6a21\u578b\u5728\u591a\u8bed\u8a00\u591a\u7ef4\u5ea6\u6587\u672c\u6ce8\u91ca\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u5927\u89c4\u6a21\u901a\u7528\u6a21\u578b\uff0c\u751f\u6210\u7684\u5927\u89c4\u6a21\u591a\u7ef4\u6807\u6ce8\u6570\u636e\u96c6\u6709\u52a9\u4e8e\u6df1\u5165\u5206\u6790\u8bed\u6599\u8d28\u91cf\u548c\u7ec4\u6210\u3002"}}
{"id": "2602.12430", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12430", "abs": "https://arxiv.org/abs/2602.12430", "authors": ["Renjun Xu", "Yang Yan"], "title": "Agent Skills for Large Language Models: Architecture, Acquisition, Security, and the Path Forward", "comment": null, "summary": "The transition from monolithic language models to modular, skill-equipped agents marks a defining shift in how large language models (LLMs) are deployed in practice. Rather than encoding all procedural knowledge within model weights, agent skills -- composable packages of instructions, code, and resources that agents load on demand -- enable dynamic capability extension without retraining. It is formalized in a paradigm of progressive disclosure, portable skill definitions, and integration with the Model Context Protocol (MCP). This survey provides a comprehensive treatment of the agent skills landscape, as it has rapidly evolved during the last few months. We organize the field along four axes: (i) architectural foundations, examining the SKILL.md specification, progressive context loading, and the complementary roles of skills and MCP; (ii) skill acquisition, covering reinforcement learning with skill libraries (SAGE), autonomous skill discovery (SEAgent), and compositional skill synthesis; (iii) deployment at scale, including the computer-use agent (CUA) stack, GUI grounding advances, and benchmark progress on OSWorld and SWE-bench; and (iv) security, where recent empirical analyses reveal that 26.1\\% of community-contributed skills contain vulnerabilities, motivating our proposed Skill Trust and Lifecycle Governance Framework -- a four-tier, gate-based permission model that maps skill provenance to graduated deployment capabilities. We identify seven open challenges -- from cross-platform skill portability to capability-based permission models -- and propose a research agenda for realizing trustworthy, self-improving skill ecosystems. Unlike prior surveys that broadly cover LLM agents or tool use, this work focuses specifically on the emerging skill abstraction layer and its implications for the next generation of agentic systems. Project repo: https://github.com/scienceaix/agentskills.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u603b\u7ed3\u4e86\u591a\u6280\u80fd\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u7684\u67b6\u6784\u3001\u6280\u80fd\u83b7\u53d6\u3001\u90e8\u7f72\u548c\u5b89\u5168\uff0c\u63d0\u51fa\u6cbb\u7406\u6846\u67b6\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u63a8\u52a8\u4e0b\u4e00\u4ee3\u53ef\u4fe1\u81ea\u6211\u6539\u8fdb\u6280\u80fd\u751f\u6001\u53d1\u5c55\u3002", "motivation": "\u5f53\u524d\u5355\u4f53\u5927\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u9ad8\u6548\u96c6\u6210\u548c\u6269\u5c55\u591a\u6837\u5316\u4efb\u52a1\u6280\u80fd\uff0c\u9700\u4e00\u79cd\u6a21\u5757\u5316\u4e14\u52a8\u6001\u53ef\u6269\u5c55\u7684\u6280\u80fd\u62bd\u8c61\u5c42\uff0c\u4ee5\u652f\u6301\u667a\u80fd\u4f53\u80fd\u529b\u7684\u7075\u6d3b\u5347\u7ea7\u548c\u5b89\u5168\u4fdd\u969c\u3002", "method": "\u901a\u8fc7\u5206\u6790SKILL.md\u89c4\u8303\u3001\u8fdb\u9636\u4e0a\u4e0b\u6587\u52a0\u8f7d\u673a\u5236\u3001\u6280\u80fd\u5e93\u5f3a\u5316\u5b66\u4e60\u3001\u81ea\u4e3b\u6280\u80fd\u53d1\u73b0\u548c\u7ec4\u5408\u3001\u5b9e\u9645\u90e8\u7f72\u6848\u4f8b\u53ca\u5b89\u5168\u6f0f\u6d1e\u5206\u6790\uff0c\u6784\u5efa\u6280\u80fd\u4fe1\u4efb\u4e0e\u751f\u547d\u5468\u671f\u6cbb\u7406\u6846\u67b6\u3002", "result": "\u672c\u6587\u7efc\u8ff0\u4e86\u4ece\u5355\u4e00\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5411\u5177\u5907\u591a\u6280\u80fd\u6a21\u5757\u5316\u667a\u80fd\u4f53\u7684\u8f6c\u53d8\uff0c\u5f3a\u8c03\u4e86\u6280\u80fd\u4f5c\u4e3a\u52a8\u6001\u6269\u5c55\u6a21\u578b\u80fd\u529b\u7684\u7ec4\u4ef6\u7684\u91cd\u8981\u6027\u3002\u4ecb\u7ecd\u4e86\u6280\u80fd\u5b9a\u4e49\u7684\u6e10\u8fdb\u62ab\u9732\u3001\u53ef\u79fb\u690d\u6027\u4ee5\u53ca\u4e0e\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff08MCP\uff09\u7684\u7ed3\u5408\uff0c\u6db5\u76d6\u4e86\u67b6\u6784\u57fa\u7840\u3001\u6280\u80fd\u83b7\u53d6\u3001\u89c4\u6a21\u90e8\u7f72\u548c\u5b89\u5168\u6027\u56db\u4e2a\u65b9\u9762\uff0c\u63d0\u51fa\u4e86\u6280\u80fd\u4fe1\u4efb\u4e0e\u751f\u547d\u5468\u671f\u6cbb\u7406\u6846\u67b6\u4ee5\u5e94\u5bf9\u5b89\u5168\u9690\u60a3\uff0c\u5e76\u6307\u51fa\u4e86\u8de8\u5e73\u53f0\u6280\u80fd\u79fb\u690d\u548c\u57fa\u4e8e\u80fd\u529b\u7684\u6743\u9650\u7ba1\u7406\u7b49\u4e03\u5927\u6311\u6218\uff0c\u5236\u5b9a\u4e86\u63a8\u52a8\u53ef\u4fe1\u81ea\u6211\u6539\u8fdb\u6280\u80fd\u751f\u6001\u7cfb\u7edf\u7684\u7814\u7a76\u8bae\u7a0b\u3002", "conclusion": "\u6280\u80fd\u62bd\u8c61\u5c42\u662f\u4e0b\u4e00\u4ee3\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5173\u952e\uff0c\u9700\u901a\u8fc7\u4e25\u683c\u7684\u5b89\u5168\u6cbb\u7406\u548c\u8de8\u5e73\u53f0\u80fd\u529b\u7ba1\u7406\u5b9e\u73b0\u53ef\u4fe1\u3001\u53ef\u6269\u5c55\u7684\u6280\u80fd\u751f\u6001\u3002"}}
{"id": "2602.12443", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.12443", "abs": "https://arxiv.org/abs/2602.12443", "authors": ["Ka Ching Chan"], "title": "SHAPR: A Solo Human-Centred and AI-Assisted Practice Framework for Research Software Development", "comment": "28 pages, 3 figures", "summary": "Research software has become a central vehicle for inquiry and learning in many Higher Degree Research (HDR) contexts, where solo researchers increasingly develop software-based artefacts as part of their research methodology. At the same time, generative artificial intelligence is reshaping development practice, offering powerful forms of assistance while introducing new challenges for accountability, reflection, and methodological rigour. Although Action Design Research (ADR) provides a well-established foundation for studying and constructing socio-technical artefacts, it offers limited guidance on how its principles can be operationalised in the day-to-day practice of solo, AI-assisted research software development. This paper proposes the SHAPR framework (Solo, Human-centred, AI-assisted PRactice) as a practice-level operational framework that complements ADR by translating its high-level principles into actionable guidance for contemporary research contexts. SHAPR supports the enactment of ADR Building-Intervention-Evaluation cycles by making explicit the roles, artefacts, reflective practices, and lightweight governance mechanisms required to sustain human accountability and learning in AI-assisted development. The contribution of the paper is conceptual: SHAPR itself is treated as the primary design artefact and unit of analysis and is evaluated formatively through reflective analysis of its internal coherence, alignment with ADR principles, and applicability to solo research practice. By explicitly linking research software development, Human-AI collaboration, and reflective learning, this study contributes to broader discussions on how SHAPR can support both knowledge production and HDR researcher training.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSHAPR\u6846\u67b6\uff0c\u5e2e\u52a9\u72ec\u7acb\u7814\u7a76\u8005\u5728AI\u8f85\u52a9\u4e0b\u6709\u6548\u5b9e\u65bdADR\u5faa\u73af\uff0c\u63a8\u52a8\u7814\u7a76\u8f6f\u4ef6\u5f00\u53d1\u548c\u4eba\u673a\u534f\u4f5c\u3002", "motivation": "\u73b0\u6709ADR\u6846\u67b6\u5bf9\u72ec\u7acb\u3001AI\u8f85\u52a9\u7684\u7814\u7a76\u8f6f\u4ef6\u5f00\u53d1\u65e5\u5e38\u5b9e\u8df5\u7f3a\u4e4f\u5177\u4f53\u6307\u5bfc\u3002", "method": "\u63d0\u51faSHAPR\u6846\u67b6\uff0c\u7ed3\u5408Action Design Research(ADR)\u539f\u5219\uff0c\u652f\u6301\u72ec\u7acb\u7814\u7a76\u8005\u5728AI\u8f85\u52a9\u4e0b\u7684\u8f6f\u4ef6\u5f00\u53d1\u5b9e\u8df5\u3002", "result": "SHAPR\u6846\u67b6\u660e\u786e\u4e86\u89d2\u8272\u3001\u4eba\u5de5\u5236\u54c1\u3001\u53cd\u601d\u5b9e\u8df5\u548c\u8f7b\u91cf\u7ea7\u6cbb\u7406\u673a\u5236\uff0c\u4fc3\u8fdb\u4eba\u7c7b\u8d23\u4efb\u548c\u5b66\u4e60\u3002", "conclusion": "SHAPR\u4f5c\u4e3a\u8bbe\u8ba1\u4eba\u5de5\u5236\u54c1\uff0c\u7b26\u5408ADR\u539f\u5219\u3001\u652f\u6301\u72ec\u7acb\u7814\u7a76\u5b9e\u8df5\uff0c\u6709\u52a9\u4e8e\u77e5\u8bc6\u751f\u4ea7\u548c\u7814\u7a76\u751f\u57f9\u8bad\u3002"}}
{"id": "2602.12424", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12424", "abs": "https://arxiv.org/abs/2602.12424", "authors": ["Ziqian Zhang", "Xingjian Hu", "Yue Huang", "Kai Zhang", "Ruoxi Chen", "Yixin Liu", "Qingsong Wen", "Kaidi Xu", "Xiangliang Zhang", "Neil Zhenqiang Gong", "Lichao Sun"], "title": "RankLLM: Weighted Ranking of LLMs by Quantifying Question Difficulty", "comment": "32 pages, 9 figures. Accepted by ICLR 2026", "summary": "Benchmarks establish a standardized evaluation framework to systematically assess the performance of large language models (LLMs), facilitating objective comparisons and driving advancements in the field. However, existing benchmarks fail to differentiate question difficulty, limiting their ability to effectively distinguish models' capabilities. To address this limitation, we propose RankLLM, a novel framework designed to quantify both question difficulty and model competency. RankLLM introduces difficulty as the primary criterion for differentiation, enabling a more fine-grained evaluation of LLM capabilities. RankLLM's core mechanism facilitates bidirectional score propagation between models and questions. The core intuition of RankLLM is that a model earns a competency score when it correctly answers a question, while a question's difficulty score increases when it challenges a model. Using this framework, we evaluate 30 models on 35,550 questions across multiple domains. RankLLM achieves 90% agreement with human judgments and consistently outperforms strong baselines such as IRT. It also exhibits strong stability, fast convergence, and high computational efficiency, making it a practical solution for large-scale, difficulty-aware LLM evaluation.", "AI": {"tldr": "\u9488\u5bf9\u73b0\u6709\u57fa\u51c6\u65e0\u6cd5\u533a\u5206\u95ee\u9898\u96be\u5ea6\u7684\u95ee\u9898\uff0cRankLLM\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u53cc\u5411\u5f97\u5206\u4f20\u64ad\u7684\u65b0\u6846\u67b6\uff0c\u80fd\u51c6\u786e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u5e76\u8003\u8651\u95ee\u9898\u96be\u5ea6\uff0c\u6548\u679c\u4f18\u5f02\u4e14\u9ad8\u6548\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u65e0\u6cd5\u533a\u5206\u95ee\u9898\u96be\u5ea6\uff0c\u9650\u5236\u4e86\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u7684\u7cbe\u51c6\u8bc4\u4f30\u3002", "method": "\u63d0\u51faRankLLM\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u578b\u548c\u95ee\u9898\u4e4b\u95f4\u7684\u53cc\u5411\u5f97\u5206\u4f20\u64ad\u673a\u5236\u91cf\u5316\u95ee\u9898\u96be\u5ea6\u548c\u6a21\u578b\u80fd\u529b\u3002\u6a21\u578b\u6b63\u786e\u56de\u7b54\u95ee\u9898\u5219\u83b7\u5f97\u80fd\u529b\u5f97\u5206\uff0c\u95ee\u9898\u6311\u6218\u6a21\u578b\u5219\u63d0\u5347\u96be\u5ea6\u5f97\u5206\u3002", "result": "\u572835550\u4e2a\u8de8\u9886\u57df\u95ee\u9898\u548c30\u4e2a\u6a21\u578b\u7684\u6d4b\u8bd5\u4e2d\uff0cRankLLM\u4e0e\u4eba\u5de5\u8bc4\u5224\u4e00\u81f4\u7387\u8fbe90%\uff0c\u4f18\u4e8eIRT\u7b49\u57fa\u51c6\u65b9\u6cd5\uff0c\u8868\u73b0\u51fa\u5feb\u901f\u6536\u655b\u548c\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "RankLLM\u6846\u67b6\u80fd\u591f\u6709\u6548\u533a\u5206\u95ee\u9898\u96be\u5ea6\u548c\u6a21\u578b\u80fd\u529b\uff0c\u8bc4\u4f30\u7cbe\u5ea6\u9ad8\uff0c\u4e14\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u65b9\u6cd5\uff0c\u5177\u6709\u8f83\u5f3a\u7a33\u5b9a\u6027\u548c\u9ad8\u6548\u6027\u3002"}}
{"id": "2602.12458", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2602.12458", "abs": "https://arxiv.org/abs/2602.12458", "authors": ["Andrew Ni", "Simon Stepputtis", "Stefanos Nikolaidis", "Michael Lewis", "Katia P. Sycara", "Woojun Kim"], "title": "Theory of Mind Guided Strategy Adaptation for Zero-Shot Coordination", "comment": "Accepted at the 25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2026)", "summary": "A central challenge in multi-agent reinforcement learning is enabling agents to adapt to previously unseen teammates in a zero-shot fashion. Prior work in zero-shot coordination often follows a two-stage process, first generating a diverse training pool of partner agents, and then training a best-response agent to collaborate effectively with the entire training pool. While many previous works have achieved strong performance by devising better ways to diversify the partner agent pool, there has been less emphasis on how to leverage this pool to build an adaptive agent. One limitation is that the best-response agent may converge to a static, generalist policy that performs reasonably well across diverse teammates, rather than learning a more adaptive, specialist policy that can better adapt to teammates and achieve higher synergy. To address this, we propose an adaptive ensemble agent that uses Theory-of-Mind-based best-response selection to first infer its teammate's intentions and then select the most suitable policy from a policy ensemble. We conduct experiments in the Overcooked environment to evaluate zero-shot coordination performance under both fully and partially observable settings. The empirical results demonstrate the superiority of our method over a single best-response baseline.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5fc3\u667a\u7406\u8bba\u7684\u9002\u5e94\u6027\u96c6\u6210\u667a\u80fd\u4f53\uff0c\u6709\u6548\u63d0\u5347\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u96f6\u6b21\u534f\u8c03\u80fd\u529b\uff0c\u5728\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4e00\u4e2a\u6838\u5fc3\u6311\u6218\u662f\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u96f6\u6b21\u534f\u8c03\u5730\u9002\u5e94\u4e4b\u524d\u672a\u89c1\u8fc7\u7684\u961f\u53cb\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u867d\u7136\u5728\u751f\u6210\u591a\u6837\u5316\u8bad\u7ec3\u4f19\u4f34\u4ee3\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u4e00\u5b9a\u6210\u679c\uff0c\u4f46\u7f3a\u4e4f\u5982\u4f55\u5229\u7528\u8fd9\u4e9b\u591a\u6837\u5316\u4ee3\u7406\u6c60\u6765\u6784\u5efa\u66f4\u5177\u9002\u5e94\u6027\u7684\u667a\u80fd\u4f53\u7684\u7814\u7a76\u3002", "method": "\u8bbe\u8ba1\u4e86\u5229\u7528\u5fc3\u667a\u7406\u8bba\u63a8\u65ad\u961f\u53cb\u610f\u56fe\u7684\u673a\u5236\uff0c\u5e76\u7ed3\u5408\u7b56\u7565\u96c6\u6210\u9009\u62e9\u6700\u5408\u9002\u7684\u5e94\u5bf9\u7b56\u7565\uff0c\u5f62\u6210\u9002\u5e94\u6027\u96c6\u6210\u667a\u80fd\u4f53\u67b6\u6784\u3002\u8fdb\u884cOvercooked\u73af\u5883\u4e0b\u7684\u591a\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5fc3\u667a\u7406\u8bba\u7684\u9002\u5e94\u6027\u96c6\u6210\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u63a8\u65ad\u961f\u53cb\u610f\u56fe\u5e76\u4ece\u7b56\u7565\u96c6\u6210\u4e2d\u9009\u62e9\u6700\u5408\u9002\u7684\u7b56\u7565\u6765\u5b9e\u73b0\u66f4\u597d\u7684\u9002\u5e94\u6027\u3002\u5728Overcooked\u73af\u5883\u4e2d\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u5168\u53ef\u89c2\u6d4b\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e0b\u7684\u96f6\u6b21\u534f\u8c03\u6027\u80fd\u4f18\u4e8e\u5355\u4e00\u6700\u4f73\u54cd\u5e94\u57fa\u7ebf\u3002", "conclusion": "\u57fa\u4e8e\u5fc3\u667a\u7406\u8bba\u7684\u6700\u4f73\u54cd\u5e94\u9009\u62e9\u53ef\u4ee5\u5f15\u5bfc\u667a\u80fd\u4f53\u4ece\u7b56\u7565\u96c6\u6210\u4e2d\u9009\u62e9\u66f4\u5177\u9002\u5e94\u6027\u7684\u7b56\u7565\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u96f6\u6b21\u534f\u8c03\u6548\u679c\u3002"}}
{"id": "2602.12500", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.12500", "abs": "https://arxiv.org/abs/2602.12500", "authors": ["Andr\u00e9 Storhaug", "Jiamou Sun", "Jingyue Li"], "title": "Favia: Forensic Agent for Vulnerability-fix Identification and Analysis", "comment": "44 pages, 12 figures, 5 tables, 3 listings", "summary": "Identifying vulnerability-fixing commits corresponding to disclosed CVEs is essential for secure software maintenance but remains challenging at scale, as large repositories contain millions of commits of which only a small fraction address security issues. Existing automated approaches, including traditional machine learning techniques and recent large language model (LLM)-based methods, often suffer from poor precision-recall trade-offs. Frequently evaluated on randomly sampled commits, we uncover that they are substantially underestimating real-world difficulty, where candidate commits are already security-relevant and highly similar. We propose Favia, a forensic, agent-based framework for vulnerability-fix identification that combines scalable candidate ranking with deep and iterative semantic reasoning. Favia first employs an efficient ranking stage to narrow the search space of commits. Each commit is then rigorously evaluated using a ReAct-based LLM agent. By providing the agent with a pre-commit repository as environment, along with specialized tools, the agent tries to localize vulnerable components, navigates the codebase, and establishes causal alignment between code changes and vulnerability root causes. This evidence-driven process enables robust identification of indirect, multi-file, and non-trivial fixes that elude single-pass or similarity-based methods. We evaluate Favia on CVEVC, a large-scale dataset we made that comprises over 8 million commits from 3,708 real-world repositories, and show that it consistently outperforms state-of-the-art traditional and LLM-based baselines under realistic candidate selection, achieving the strongest precision-recall trade-offs and highest F1-scores.", "AI": {"tldr": "\u63d0\u51faFavia\u6846\u67b6\uff0c\u7ed3\u5408\u5019\u9009\u63d0\u4ea4\u6392\u5e8f\u548c\u57fa\u4e8eReAct\u7684\u667a\u80fd\u4f53\u8fdb\u884c\u6df1\u5ea6\u8bed\u4e49\u5206\u6790\uff0c\u5b9e\u73b0\u5bf9\u6f0f\u6d1e\u4fee\u590d\u63d0\u4ea4\u7684\u9ad8\u6548\u7cbe\u51c6\u8bc6\u522b\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u8bc6\u522b\u6f0f\u6d1e\u4fee\u590d\u63d0\u4ea4\u5bf9\u4e8e\u8f6f\u4ef6\u5b89\u5168\u7ef4\u62a4\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u3001\u9ad8\u76f8\u4f3c\u5ea6\u5019\u9009\u73af\u5883\u4e0b\u51c6\u786e\u7387\u548c\u53ec\u56de\u7387\u8868\u73b0\u4e0d\u4f73\uff0c\u96be\u4ee5\u5e94\u5bf9\u591a\u6587\u4ef6\u548c\u590d\u6742\u4fee\u590d\u573a\u666f\uff0c\u4e9f\u9700\u66f4\u7cbe\u51c6\u7684\u8bc6\u522b\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u9996\u5148\u8fdb\u884c\u9ad8\u6548\u5019\u9009\u63d0\u4ea4\u6392\u5e8f\u7f29\u5c0f\u641c\u7d22\u7a7a\u95f4\uff1b\u968f\u540e\u5229\u7528ReAct\u667a\u80fd\u4f53\uff0c\u5728\u9884\u63d0\u4ea4\u4ee3\u7801\u73af\u5883\u4e2d\u8fd0\u7528\u6df1\u5ea6\u8bed\u4e49\u548c\u56e0\u679c\u5206\u6790\u6838\u5b9e\u6bcf\u4e2a\u63d0\u4ea4\uff0c\u8bc6\u522b\u4fee\u590d\u6f0f\u6d1e\u7684\u4ee3\u7801\u53d8\u66f4\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86Favia\uff0c\u4e00\u79cd\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u6f0f\u6d1e\u4fee\u590d\u63d0\u4ea4\u8bc6\u522b\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u9ad8\u6548\u6392\u5e8f\u548c\u6df1\u5ea6\u8bed\u4e49\u63a8\u7406\uff0c\u5b9e\u73b0\u4e86\u5bf9\u5927\u89c4\u6a21\u4ee3\u7801\u5e93\u4e2d\u5b89\u5168\u4fee\u590d\u63d0\u4ea4\u7684\u51c6\u786e\u5b9a\u4f4d\u3002Favia\u5229\u7528ReAct\u5927\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u5728\u9884\u63d0\u4ea4\u73af\u5883\u4e0b\uff0c\u5b9a\u4f4d\u6f0f\u6d1e\u7ec4\u4ef6\uff0c\u904d\u5386\u4ee3\u7801\u5e93\uff0c\u5e76\u5efa\u7acb\u4ee3\u7801\u53d8\u66f4\u4e0e\u6f0f\u6d1e\u6839\u672c\u539f\u56e0\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u80fd\u591f\u8bc6\u522b\u590d\u6742\u7684\u591a\u6587\u4ef6\u548c\u95f4\u63a5\u4fee\u590d\u3002\u901a\u8fc7\u5728\u5305\u542b\u8d85\u8fc7\u516b\u767e\u4e07\u63d0\u4ea4\u7684\u5927\u89c4\u6a21\u771f\u5b9e\u6570\u636e\u96c6CVEVC\u4e0a\u7684\u8bc4\u6d4b\uff0cFavia\u5728\u771f\u5b9e\u5019\u9009\u63d0\u4ea4\u9009\u62e9\u4e0b\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548c\u73b0\u6709LLM\u65b9\u6cd5\uff0c\u5728\u51c6\u786e\u7387\u4e0e\u53ec\u56de\u7387\u7684\u5e73\u8861\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u53d6\u5f97\u6700\u9ad8F1\u5206\u6570\u3002", "conclusion": "Favia\u5728\u5927\u89c4\u6a21\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0c\u80fd\u591f\u9c81\u68d2\u8bc6\u522b\u95f4\u63a5\u3001\u591a\u6587\u4ef6\u53ca\u590d\u6742\u6f0f\u6d1e\u4fee\u590d\uff0c\u8fbe\u5230\u66f4\u9ad8\u51c6\u786e\u7387\u4e0e\u53ec\u56de\u7387\u7684\u5e73\u8861\uff0c\u663e\u8457\u63d0\u5347\u6f0f\u6d1e\u4fee\u590d\u63d0\u4ea4\u5b9a\u4f4d\u6548\u679c\u3002"}}
{"id": "2602.12445", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12445", "abs": "https://arxiv.org/abs/2602.12445", "authors": ["Om Bhatt", "Anna A. Ivanova"], "title": "RBCorr: Response Bias Correction in Language Models", "comment": "12 pages (8 pages main text), 4 figures", "summary": "Language models (LMs) are known to be prone to response biases, which present as option preference biases in fixed-response questions. It is therefore imperative to develop low-cost and effective response bias correction methods to improve LM performance and enable more accurate evaluations of model abilities. Here, we propose a simple response bias correction strategy ($\\texttt{RBCorr}$) and test it on 12 open-weight language models using yes-no, entailment, and multiple choice questions. We show that response bias is prevalent in LMs pre-correction and that $\\texttt{RBCorr}$ effectively eliminates bias and boosts model performance. We also explore the generalizability of bias behavior across models, datasets, and prompt formats, showing that LogProbs-based correction is highly dependent on all three of these aspects. Overall, $\\texttt{RBCorr}$ is an easy-to-use method that can boost the performance of smaller LMs and ensure that LM performance on closed-response benchmarks aligns more closely with their true capabilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRBCorr\u65b9\u6cd5\u6709\u6548\u7ea0\u6b63\u8bed\u8a00\u6a21\u578b\u5728\u56fa\u5b9a\u54cd\u5e94\u95ee\u9898\u4e2d\u7684\u504f\u5dee\uff0c\u63d0\u5347\u6027\u80fd\u5e76\u589e\u5f3a\u7ed3\u679c\u771f\u5b9e\u6027\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u5728\u56fa\u5b9a\u54cd\u5e94\u95ee\u9898\u4e2d\u5b58\u5728\u54cd\u5e94\u504f\u5dee\uff0c\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u548c\u80fd\u529b\u8bc4\u4f30\uff0c\u9700\u5f00\u53d1\u4f4e\u6210\u672c\u6709\u6548\u7684\u504f\u5dee\u6821\u6b63\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u8868\u73b0\u548c\u8bc4\u4f30\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u5e76\u5e94\u7528RBCorr\u54cd\u5e94\u504f\u5dee\u6821\u6b63\u7b56\u7565\uff0c\u572812\u79cd\u8bed\u8a00\u6a21\u578b\u548c\u591a\u79cd\u9898\u578b\u4e0a\u6d4b\u8bd5\u6548\u679c\uff0c\u4f7f\u7528\u57fa\u4e8e\u5bf9\u6570\u6982\u7387\u7684\u6821\u6b63\u65b9\u6cd5\u8fdb\u884c\u504f\u5dee\u8c03\u6574\u3002", "result": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u54cd\u5e94\u504f\u5dee\u6821\u6b63\u7b56\u7565\uff08RBCorr\uff09\uff0c\u7528\u4e8e\u7ea0\u6b63\u8bed\u8a00\u6a21\u578b\uff08LMs\uff09\u5728\u56fa\u5b9a\u54cd\u5e94\u95ee\u9898\u4e2d\u5b58\u5728\u7684\u9009\u9879\u504f\u597d\u504f\u5dee\u3002\u901a\u8fc7\u572812\u4e2a\u5f00\u6e90\u8bed\u8a00\u6a21\u578b\u4e0a\u4f7f\u7528\u662f\u975e\u9898\u3001\u8574\u6db5\u9898\u548c\u591a\u9879\u9009\u62e9\u9898\u8fdb\u884c\u6d4b\u8bd5\uff0c\u9a8c\u8bc1\u4e86\u6821\u6b63\u7b56\u7565\u80fd\u6709\u6548\u6d88\u9664\u504f\u5dee\u5e76\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002\u7814\u7a76\u8fd8\u63a2\u8ba8\u4e86\u4e0d\u540c\u6a21\u578b\u3001\u6570\u636e\u96c6\u548c\u63d0\u793a\u683c\u5f0f\u4e0b\u504f\u5dee\u884c\u4e3a\u7684\u666e\u9002\u6027\uff0c\u53d1\u73b0\u57fa\u4e8e\u5bf9\u6570\u6982\u7387\u7684\u6821\u6b63\u5bf9\u8fd9\u4e9b\u56e0\u7d20\u9ad8\u5ea6\u4f9d\u8d56\u3002\u6574\u4f53\u6765\u770b\uff0cRBCorr\u662f\u4e00\u4e2a\u4f4e\u6210\u672c\u3001\u6613\u7528\u7684\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u8868\u73b0\uff0c\u4f7f\u95ed\u5408\u54cd\u5e94\u57fa\u51c6\u6d4b\u8bd5\u7684\u7ed3\u679c\u66f4\u80fd\u51c6\u786e\u53cd\u6620\u6a21\u578b\u7684\u771f\u5b9e\u80fd\u529b\u3002", "conclusion": "RBCorr\u80fd\u591f\u6709\u6548\u6d88\u9664\u8bed\u8a00\u6a21\u578b\u7684\u54cd\u5e94\u504f\u5dee\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u4f7f\u8bc4\u4f30\u66f4\u51c6\u786e\u53cd\u6620\u6a21\u578b\u80fd\u529b\u3002"}}
{"id": "2602.12502", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2602.12502", "abs": "https://arxiv.org/abs/2602.12502", "authors": ["Grant Douglas", "Stephen Franklin", "Claudia Szabo", "Mingyu Guo"], "title": "Building Large-Scale Drone Defenses from Small-Team Strategies", "comment": "13 pages, 8 figures", "summary": "Defending against large adversarial drone swarms requires coordination methods that scale effectively beyond conventional multi-agent optimisation. In this paper, we propose to scale strategies proven effective in small defender teams by integrating them as modular components of larger forces using our proposed framework. A dynamic programming (DP) decomposition assembles these components into large teams in polynomial time, enabling efficient construction of scalable defenses without exhaustive evaluation. Because a unit that is strong in isolation may not remain strong when combined, we sample across multiple small-team candidates. Our framework iterates between evaluating large-team outcomes and refining the pool of modular components, allowing convergence on increasingly effective strategies. Experiments demonstrate that this partitioning approach scales to substantially larger scenarios while preserving effectiveness and revealing cooperative behaviours that direct optimisation cannot reliably discover.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u89c4\u5212\u5206\u89e3\u6846\u67b6\uff0c\u5c06\u9a8c\u8bc1\u6709\u6548\u7684\u5c0f\u56e2\u961f\u7b56\u7565\u6a21\u5757\u5316\u7ec4\u5408\u6784\u5efa\u5927\u89c4\u6a21\u9632\u5fa1\u529b\u91cf\uff0c\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u654c\u5bf9\u65e0\u4eba\u673a\u7fa4\u9632\u5fa1\u7684\u53ef\u6269\u5c55\u6027\u96be\u9898\uff0c\u5e76\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\u548c\u4f18\u8d8a\u6027\u3002", "motivation": "\u5e38\u89c4\u591a\u667a\u80fd\u4f53\u4f18\u5316\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5e94\u5bf9\u5927\u89c4\u6a21\u654c\u5bf9\u65e0\u4eba\u673a\u7fa4\uff0c\u9700\u8981\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u534f\u8c03\u65b9\u6cd5\u4ee5\u6709\u6548\u9632\u5fa1\u5927\u578b\u654c\u5bf9\u65e0\u4eba\u673a\u7f16\u961f\u3002", "method": "\u901a\u8fc7\u52a8\u6001\u89c4\u5212\u5206\u89e3\uff0c\u5c06\u5728\u5c0f\u89c4\u6a21\u9632\u5fa1\u56e2\u961f\u4e2d\u9a8c\u8bc1\u6709\u6548\u7684\u7b56\u7565\u4f5c\u4e3a\u6a21\u5757\u7ec4\u4ef6\uff0c\u96c6\u6210\u4e3a\u5927\u89c4\u6a21\u9632\u5fa1\u529b\u91cf\uff0c\u5b9e\u73b0\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7684\u53ef\u6269\u5c55\u6027\u3002\u901a\u8fc7\u5bf9\u591a\u4e2a\u5c0f\u56e2\u961f\u5019\u9009\u7b56\u7565\u8fdb\u884c\u91c7\u6837\uff0c\u53cd\u590d\u8bc4\u4f30\u5927\u56e2\u961f\u8868\u73b0\u5e76\u4f18\u5316\u6a21\u5757\u7ec4\u4ef6\u6c60\uff0c\u8fed\u4ee3\u63d0\u5347\u6574\u4f53\u7b56\u7565\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u6240\u63d0\u6846\u67b6\u80fd\u5728\u5927\u89c4\u6a21\u573a\u666f\u4e2d\u6709\u6548\u6269\u5c55\u9632\u5fa1\u7b56\u7565\uff0c\u540c\u65f6\u4fdd\u6301\u9632\u5fa1\u6548\u679c\uff0c\u8fd8\u80fd\u53d1\u73b0\u4f20\u7edf\u4f18\u5316\u96be\u4ee5\u63a2\u6d4b\u7684\u534f\u4f5c\u884c\u4e3a\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u901a\u8fc7\u6a21\u5757\u5316\u7b56\u7565\u5206\u89e3\u4e0e\u8fed\u4ee3\u4f18\u5316\uff0c\u53ef\u6709\u6548\u6784\u5efa\u5927\u89c4\u6a21\u9632\u5fa1\u65e0\u4eba\u673a\u9635\u578b\uff0c\u63d0\u5347\u9632\u5fa1\u6548\u679c\u4e0e\u7b56\u7565\u534f\u4f5c\u6027\uff0c\u4e3a\u9632\u5fa1\u5927\u578b\u654c\u5bf9\u65e0\u4eba\u673a\u7fa4\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.12721", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.12721", "abs": "https://arxiv.org/abs/2602.12721", "authors": ["Nordine Benkeltoum"], "title": "Reconciling Complexity and Simplicity in the Business Model Canvas Design Through Metamodelling and Domain-Specific Modelling", "comment": null, "summary": "This article introduces a metamodel for the Business Model Canvas (BMC) using the Unified Modelling Language (UML), together with a dedicated Domain-Specific Modelling Language (DSML) tool. Although the BMC is widely adopted by both practitioners and scholars, significant challenges remain in formally modelling business models, particularly with regard to explicit specification of inter-component relationships, while preserving the simplicity that characterises the BMC. Addressing this tension between modelling rigour and practical relevance, this research adopts a Design Science Research approach to formally specify relationships among BMC components and to strengthen their theoretical grounding through an adaptation of the V 4 framework. The proposed metamodel consolidates BMC relationships into three core types: supports, determines, and affects, providing explicit semantics while remaining accessible to end users through graphical tooling. The findings highlight that formally specifying relationships significantly improves the interpretability and consistency of BMC representations. The proposed metamodel and tool offer a rigorous yet usable foundation for developing DSML-based BMC tools and for enabling systematic integration of the BMC into widely used software and enterprise modelling environments, thereby bridging business modelling and enterprise architecture practices for both academics and practitioners.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u8bbe\u8ba1\u79d1\u5b66\u65b9\u6cd5\u6784\u5efa\u57fa\u4e8eUML\u7684\u5546\u4e1a\u6a21\u5f0f\u753b\u5e03\u5143\u6a21\u578b\u53caDSML\u5de5\u5177\uff0c\u660e\u786e\u7ec4\u4ef6\u95f4\u5173\u7cfb\uff0c\u63d0\u5347BMC\u7684\u5f62\u5f0f\u5316\u5efa\u6a21\u6c34\u5e73\uff0c\u5b9e\u73b0\u7406\u8bba\u4e0e\u5b9e\u8df5\u7684\u6709\u6548\u7ed3\u5408\u3002", "motivation": "\u5f53\u524d\u867d\u7136\u5546\u4e1a\u6a21\u5f0f\u753b\u5e03\uff08BMC\uff09\u88ab\u5e7f\u6cdb\u91c7\u7528\uff0c\u4f46\u5728\u6b63\u5f0f\u5efa\u6a21\u4e2d\u9762\u4e34\u5bf9\u7ec4\u4ef6\u95f4\u5173\u7cfb\u660e\u786e\u8868\u793a\u7684\u6311\u6218\uff0c\u4e14\u9700\u4fdd\u6301BMC\u7684\u7b80\u6613\u6027\u3002", "method": "\u91c7\u7528\u8bbe\u8ba1\u79d1\u5b66\u7814\u7a76\u65b9\u6cd5\uff0c\u57fa\u4e8e\u7edf\u4e00\u5efa\u6a21\u8bed\u8a00(UML)\u6784\u5efaBMC\u5143\u6a21\u578b\uff0c\u7ed3\u5408\u9886\u57df\u4e13\u7528\u5efa\u6a21\u8bed\u8a00(DSML)\u5de5\u5177\uff0c\u501f\u52a9V 4\u6846\u67b6\u52a0\u5f3a\u7406\u8bba\u57fa\u7840\u3002", "result": "\u63d0\u51fa\u4e86\u4e09\u7c7b\u6838\u5fc3\u5173\u7cfb\uff08\u652f\u6301\u3001\u51b3\u5b9a\u3001\u5f71\u54cd\uff09\u7684\u5143\u6a21\u578b\uff0c\u63d0\u9ad8BMC\u7ec4\u4ef6\u5173\u7cfb\u7684\u660e\u786e\u6027\u548c\u89e3\u91ca\u6027\uff0c\u517c\u987e\u4e25\u8c28\u6027\u4e0e\u6613\u7528\u6027\u3002", "conclusion": "\u6b63\u5f0f\u89c4\u8303BMC\u7ec4\u4ef6\u5173\u7cfb\u663e\u8457\u63d0\u5347\u4e86\u5176\u8868\u793a\u7684\u4e00\u81f4\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u6240\u6784\u5efa\u7684\u5143\u6a21\u578b\u548c\u5de5\u5177\u4e3aBMC\u5de5\u5177\u5f00\u53d1\u53ca\u4e0e\u4f01\u4e1a\u67b6\u6784\u878d\u5408\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2602.12575", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12575", "abs": "https://arxiv.org/abs/2602.12575", "authors": ["Bo Wang", "Yuxuan Zhang", "Yueqin Hu", "Hanchao Hou", "Kaiping Peng", "Shiguang Ni"], "title": "Discovering Semantic Latent Structures in Psychological Scales: A Response-Free Pathway to Efficient Simplification", "comment": "78 pages, 20 figures", "summary": "Psychological scale refinement traditionally relies on response-based methods such as factor analysis, item response theory, and network psychometrics to optimize item composition. Although rigorous, these approaches require large samples and may be constrained by data availability and cross-cultural comparability. Recent advances in natural language processing suggest that the semantic structure of questionnaire items may encode latent construct organization, offering a complementary response-free perspective. We introduce a topic-modeling framework that operationalizes semantic latent structure for scale simplification. Items are encoded using contextual sentence embeddings and grouped via density-based clustering to discover latent semantic factors without predefining their number. Class-based term weighting derives interpretable topic representations that approximate constructs and enable merging of semantically adjacent clusters. Representative items are selected using membership criteria within an integrated reduction pipeline. We benchmarked the framework across DASS, IPIP, and EPOCH, evaluating structural recovery, internal consistency, factor congruence, correlation preservation, and reduction efficiency. The proposed method recovered coherent factor-like groupings aligned with established constructs. Selected items reduced scale length by 60.5% on average while maintaining psychometric adequacy. Simplified scales showed high concordance with original factor structures and preserved inter-factor correlations, indicating that semantic latent organization provides a response-free approximation of measurement structure. Our framework formalizes semantic structure as an inspectable front-end for scale construction and reduction. To facilitate adoption, we provide a visualization-supported tool enabling one-click semantic analysis and structured simplification.", "AI": {"tldr": "\u63d0\u51fa\u5229\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u8bed\u4e49\u4e3b\u9898\u6a21\u578b\u7b80\u5316\u5fc3\u7406\u91cf\u8868\uff0c\u5b9e\u73b0\u65e0\u53cd\u5e94\u6570\u636e\u7684\u7ed3\u6784\u903c\u8fd1\uff0c\u6709\u6548\u51cf\u5c11\u91cf\u8868\u6761\u76ee\u4e14\u4fdd\u6301\u5fc3\u7406\u6d4b\u91cf\u6548\u80fd\u3002", "motivation": "\u4f20\u7edf\u5fc3\u7406\u91cf\u8868\u4f18\u5316\u4f9d\u8d56\u53cd\u5e94\u6570\u636e\u65b9\u6cd5\uff08\u5982\u56e0\u5b50\u5206\u6790\u3001\u9879\u76ee\u53cd\u5e94\u7406\u8bba\uff09\uff0c\u9700\u5927\u6837\u672c\u4e14\u53d7\u6570\u636e\u53ef\u7528\u6027\u548c\u8de8\u6587\u5316\u9002\u7528\u6027\u9650\u5236\uff0c\u8bed\u4e49\u7ed3\u6784\u63d0\u4f9b\u65e0\u53cd\u5e94\u6570\u636e\u7684\u8865\u5145\u89c6\u89d2\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u4e3b\u9898\u5efa\u6a21\u7684\u6846\u67b6\uff0c\u5c06\u95ee\u5377\u6761\u76ee\u901a\u8fc7\u4e0a\u4e0b\u6587\u53e5\u5b50\u5d4c\u5165\u7f16\u7801\uff0c\u5e76\u5229\u7528\u57fa\u4e8e\u5bc6\u5ea6\u7684\u805a\u7c7b\u53d1\u73b0\u6f5c\u5728\u8bed\u4e49\u56e0\u5b50\uff0c\u4f7f\u7528\u7c7b\u57fa\u8bcd\u52a0\u6743\u751f\u6210\u53ef\u89e3\u91ca\u7684\u4e3b\u9898\u8868\u793a\uff0c\u8fdb\u800c\u5408\u5e76\u8bed\u4e49\u76f8\u90bb\u7684\u805a\u7c7b\uff0c\u901a\u8fc7\u6210\u5458\u8d44\u683c\u6807\u51c6\u9009\u62e9\u4ee3\u8868\u6027\u6761\u76ee\u8fdb\u884c\u7b80\u5316\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u6062\u590d\u4e0e\u65e2\u5b9a\u6784\u9020\u4e00\u81f4\u7684\u8bed\u4e49\u56e0\u5b50\u7ec4\uff0c\u7b80\u5316\u91cf\u8868\u6761\u76ee\u5e73\u5747\u51cf\u5c1160.5%\uff0c\u5fc3\u7406\u6d4b\u91cf\u6548\u5ea6\u4fdd\u6301\u826f\u597d\uff0c\u7b80\u5316\u540e\u7684\u91cf\u8868\u7ed3\u6784\u53ca\u56e0\u5b50\u95f4\u76f8\u5173\u6027\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "\u8bed\u4e49\u6f5c\u5728\u7ed3\u6784\u53ef\u4f5c\u4e3a\u5fc3\u7406\u91cf\u8868\u6784\u5efa\u548c\u7b80\u5316\u7684\u4e8c\u7ebf\u524d\u7aef\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u7ed3\u6784\u89c6\u89d2\uff0c\u6a21\u578b\u53ca\u53ef\u89c6\u5316\u5de5\u5177\u4fbf\u4e8e\u5b9e\u9645\u5e94\u7528\u63a8\u5e7f\u3002"}}
{"id": "2602.12834", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.12834", "abs": "https://arxiv.org/abs/2602.12834", "authors": ["Jinlong He", "Changwei Xia", "Binru Huang", "Jiwei Yan", "Jun Yan", "Jian Zhang"], "title": "FuncDroid: Towards Inter-Functional Flows for Comprehensive Mobile App GUI Testing", "comment": null, "summary": "As mobile application (app) functionalities grow increasingly complex and their iterations accelerate, ensuring high reliability presents significant challenges. While functionality-oriented GUI testing has attracted growing research attention, existing approaches largely overlook interactions across functionalities, making them ineffective at uncovering deep bugs hidden in inter-functional behaviors. To fill this gap, we first design a Functional Flow Graph (FFG), a behavioral model that explicitly captures an app's functional units and their inter-functional interactions. Based on the FFG, we further introduce an inter-functional-flow-oriented GUI testing approach with the dual goals of precise model construction and deep bug detection. This approach is realized through a long-short-term-view-guided testing process. By combining two complementary test-generation views, it can adaptively refine functional boundaries and systematically explore inter-functional flows under diverse triggering conditions. We implement our approach in a tool called FuncDroid, and evaluate it on two benchmarks: (1) a widely-used open-source benchmark with 50 reproducible crash bugs and (2) a diverse set of 52 popular commercial apps. Experimental results demonstrate that FuncDroid significantly outperforms state-of-the-art baselines in both coverage (+28%) and bug detection number (+107%). Moreover, FuncDroid successfully uncovers 18 previously unknown non-crash functional bugs in commercial apps, confirming its practical effectiveness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u529f\u80fd\u6d41\u56fe\uff08FFG\uff09\u7684\u8de8\u529f\u80fd\u4ea4\u4e92\u5bfc\u5411\u7684\u79fb\u52a8\u5e94\u7528GUI\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u901a\u8fc7\u957f\u77ed\u671f\u89c6\u89d2\u5f15\u5bfc\u7684\u6d4b\u8bd5\u6d41\u7a0b\uff0c\u5b9e\u73b0\u7cbe\u786e\u6a21\u578b\u6784\u5efa\u548c\u6df1\u5c42\u6b21\u7f3a\u9677\u68c0\u6d4b\u3002", "motivation": "\u73b0\u6709\u9488\u5bf9GUI\u7684\u529f\u80fd\u6d4b\u8bd5\u5ffd\u89c6\u8de8\u529f\u80fd\u4ea4\u4e92\uff0c\u96be\u4ee5\u53d1\u73b0\u6df1\u5c42\u6b21\u7684\u8de8\u529f\u80fd\u7f3a\u9677\uff0c\u6025\u9700\u4e00\u79cd\u80fd\u663e\u5f0f\u6355\u83b7\u529f\u80fd\u4ea4\u4e92\u7684\u6d4b\u8bd5\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u529f\u80fd\u6d41\u56fe\uff08FFG\uff09\u6765\u5efa\u6a21\u5e94\u7528\u529f\u80fd\u5355\u5143\u53ca\u5176\u4ea4\u4e92\uff0c\u7ed3\u5408\u957f\u77ed\u671f\u89c6\u89d2\u7684\u6d4b\u8bd5\u751f\u6210\u65b9\u6cd5\uff0c\u81ea\u9002\u5e94\u7ec6\u5316\u529f\u80fd\u8fb9\u754c\uff0c\u7cfb\u7edf\u63a2\u7d22\u8de8\u529f\u80fd\u6d41\u7a0b\u3002", "result": "\u5b9e\u73b0\u4e86\u5de5\u5177FuncDroid\uff0c\u572850\u4e2a\u5f00\u6e90\u7f3a\u9677\u548c52\u4e2a\u5546\u4e1a\u5e94\u7528\u4e0a\u6d4b\u8bd5\uff0c\u8986\u76d6\u7387\u548c\u7f3a\u9677\u68c0\u6d4b\u6570\u91cf\u5927\u5e45\u63d0\u5347\uff0c\u53d1\u73b018\u4e2a\u672a\u77e5\u529f\u80fd\u7f3a\u9677\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u8986\u76d6\u7387\u63d0\u9ad828%\uff0c\u7f3a\u9677\u68c0\u6d4b\u6570\u589e\u52a0107%\uff0c\u5e76\u53d1\u73b018\u4e2a\u672a\u77e5\u529f\u80fd\u6027\u7f3a\u9677\uff0c\u9a8c\u8bc1\u4e86\u5176\u5b9e\u7528\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2602.12635", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12635", "abs": "https://arxiv.org/abs/2602.12635", "authors": ["Pengxiang Zhao", "Hui-Ling Zhen", "Xing Li", "Han Bao", "Weizhe Lin", "Zhiyuan Yang", "Ziwei Yu", "Xin Wang", "Mingxuan Yuan", "Xianzhi Yu", "Zhenhua Dong"], "title": "Unleashing Low-Bit Inference on Ascend NPUs: A Comprehensive Evaluation of HiFloat Formats", "comment": null, "summary": "As LLMs scale, low-bit floating-point formats like MXFP and NVFP4 offer new opportunities for precision and efficiency. In this work, we evaluate HiFloat (HiF8 and HiF4), a family of formats tailored for Ascend NPUs. Through rigorous comparison across weight-activation and KV-cache tasks, we provide three key insights: (1) INT8 suits narrow-range data, while floating-point formats excel with high-variance data; (2) in 4-bit regimes, HiF4's hierarchical scaling prevents the accuracy collapse seen in integer formats; and (3) HiFloat is fully compatible with state-of-the-art post-training quantization frameworks. Overall, HiFloat provides a solution for high-efficiency LLM inference on NPUs.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u4e13\u4e3aAscend NPU\u8bbe\u8ba1\u7684HiFloat\u4f4e\u4f4d\u5bbd\u6d6e\u70b9\u683c\u5f0f\uff0c\u8bc1\u660e\u5176\u5728\u9ad8\u65b9\u5dee\u6570\u636e\u548c4\u4f4d\u7cbe\u5ea6\u4e0b\u4f18\u4e8e\u6574\u6570\u683c\u5f0f\uff0c\u4e14\u4e0e\u4e3b\u6d41\u91cf\u5316\u6846\u67b6\u517c\u5bb9\uff0c\u662f\u9ad8\u6548LLM\u63a8\u7406\u7684\u7406\u60f3\u9009\u62e9\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u89c4\u6a21\u6269\u5927\uff0c\u73b0\u6709\u7684\u4f4e\u4f4d\u5bbd\u6d6e\u70b9\u683c\u5f0f\u4e3a\u63d0\u9ad8\u7cbe\u5ea6\u548c\u6548\u7387\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\u3002\u672c\u6587\u52a8\u673a\u662f\u8bc4\u4f30\u4e13\u4e3aAscend NPU\u8bbe\u8ba1\u7684HiFloat\u683c\u5f0f\u5bb6\u65cf\uff08HiF8\u548cHiF4\uff09\uff0c\u4ee5\u4f18\u5316LLM\u63a8\u7406\u7684\u6548\u80fd\u3002", "method": "\u901a\u8fc7\u5728\u6743\u91cd\u6fc0\u6d3b\u548cKV\u7f13\u5b58\u4efb\u52a1\u4e2d\u7684\u4e25\u683c\u6bd4\u8f83\uff0c\u8bc4\u4f30HiFloat\u683c\u5f0f\u5728\u4e0d\u540c\u4f4d\u5bbd\u548c\u6570\u636e\u7c7b\u578b\u4e0a\u7684\u8868\u73b0\uff0c\u4e0e\u6574\u6570\u683c\u5f0f\u53ca\u5176\u4ed6\u6d6e\u70b9\u683c\u5f0f\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u53d1\u73b0INT8\u9002\u5408\u7a84\u8303\u56f4\u6570\u636e\uff0c\u800c\u6d6e\u70b9\u683c\u5f0f\u5728\u9ad8\u65b9\u5dee\u6570\u636e\u4e0a\u8868\u73b0\u66f4\u597d\uff1bHiF4\u57284\u4f4d\u5bbd\u60c5\u51b5\u4e0b\u901a\u8fc7\u5206\u5c42\u7f29\u653e\u907f\u514d\u4e86\u6574\u6570\u683c\u5f0f\u7684\u51c6\u786e\u6027\u5d29\u6e83\uff1bHiFloat\u683c\u5f0f\u4e0e\u6700\u5148\u8fdb\u7684\u540e\u8bad\u7ec3\u91cf\u5316\u6846\u67b6\u5b8c\u5168\u517c\u5bb9\u3002", "conclusion": "HiFloat\u63d0\u4f9b\u4e86\u4e00\u79cd\u9002\u5408Ascend NPU\u7684\u9ad8\u6548LLM\u63a8\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u517c\u5177\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4f18\u52bf\u3002"}}
{"id": "2602.12875", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12875", "abs": "https://arxiv.org/abs/2602.12875", "authors": ["Juan Luis Herrera", "Daniel Wang", "Schahram Dustdar"], "title": "A Microservice-Based Platform for Sustainable and Intelligent SLO Fulfilment and Service Management", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "The Microservices Architecture (MSA) design pattern has become a staple for modern applications, allowing functionalities to be divided across fine-grained microservices, fostering reusability, distribution, and interoperability. As MSA-based applications are deployed to the Computing Continuum (CC), meeting their Service Level Objectives (SLOs) becomes a challenge. Trading off performance and sustainability SLOs is especially challenging. This challenge can be addressed with intelligent decision systems, able to reconfigure the services during runtime to meet the SLOs. However, developing these agents while adhering to the MSA pattern is complex, especially because CC providers, who have key know-how and information to fulfill these SLOs, must comply with the privacy requirements of application developers. This work presents the Carbon-Aware SLO and Control plAtform (CASCA), an open-source MSA-based platform that allows CC providers to reconfigure services and fulfill their SLOs while maintaining the privacy of developers. CASCA is architected to be highly reusable, distributable, and easy to use, extend, and modify. CASCA has been evaluated in a real CC testbed for a media streaming service, where decision systems implemented in Bash, Rust, and Python successfully reconfigured the service, unaffected by upholding privacy.", "AI": {"tldr": "CASCA\u662f\u4e00\u4e2a\u57fa\u4e8e\u5fae\u670d\u52a1\u67b6\u6784\u7684\u5f00\u6e90\u667a\u80fd\u63a7\u5236\u5e73\u53f0\uff0c\u80fd\u591f\u5728\u4fdd\u62a4\u5f00\u53d1\u8005\u9690\u79c1\u7684\u524d\u63d0\u4e0b\u52a8\u6001\u8c03\u6574\u8ba1\u7b97\u8fde\u7eed\u4f53\u4e2d\u7684\u670d\u52a1\uff0c\u786e\u4fdd\u670d\u52a1\u6c34\u5e73\u76ee\u6807\u7684\u5b8c\u6210\u3002", "motivation": "\u968f\u7740\u57fa\u4e8e\u5fae\u670d\u52a1\u67b6\u6784\u7684\u5e94\u7528\u90e8\u7f72\u5230\u8ba1\u7b97\u8fde\u7eed\u4f53\uff0c\u5982\u4f55\u52a8\u6001\u8c03\u6574\u670d\u52a1\u4ee5\u517c\u987e\u6027\u80fd\u4e0e\u53ef\u6301\u7eed\u6027\u6307\u6807\uff0c\u540c\u65f6\u4fdd\u62a4\u5f00\u53d1\u8005\u9690\u79c1\uff0c\u662f\u4e00\u4e2a\u4e9f\u5f85\u89e3\u51b3\u7684\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5fae\u670d\u52a1\u67b6\u6784(MSA)\u7684\u5f00\u6e90\u5e73\u53f0CASCA\uff0c\u652f\u6301\u8fd0\u884c\u65f6\u667a\u80fd\u51b3\u7b56\u7cfb\u7edf\u5bf9\u670d\u52a1\u8fdb\u884c\u52a8\u6001\u8c03\u6574\uff0c\u652f\u6301\u591a\u79cd\u7f16\u7a0b\u8bed\u8a00\u5b9e\u73b0\u51b3\u7b56\u903b\u8f91\u3002", "result": "\u5728\u771f\u5b9e\u7684\u8ba1\u7b97\u8fde\u7eed\u4f53\u6d4b\u8bd5\u73af\u5883\u4e2d\uff0cCASCA\u6210\u529f\u652f\u6301\u4e86\u5a92\u4f53\u6d41\u670d\u52a1\u7684\u52a8\u6001\u91cd\u65b0\u914d\u7f6e\uff0c\u4e14\u51b3\u7b56\u7cfb\u7edf\u4f7f\u7528Bash\u3001Rust\u548cPython\u5b9e\u73b0\u5747\u8868\u73b0\u826f\u597d\uff0c\u4e14\u9690\u79c1\u672a\u88ab\u7834\u574f\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684CASCA\u5e73\u53f0\u80fd\u591f\u5728\u4fdd\u8bc1\u5f00\u53d1\u8005\u9690\u79c1\u7684\u524d\u63d0\u4e0b\uff0c\u5141\u8bb8\u8ba1\u7b97\u8fde\u7eed\u4f53\u63d0\u4f9b\u5546\u91cd\u65b0\u914d\u7f6e\u5fae\u670d\u52a1\u4ee5\u6ee1\u8db3\u670d\u52a1\u6c34\u5e73\u76ee\u6807(SLOs)\u3002"}}
{"id": "2602.12639", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12639", "abs": "https://arxiv.org/abs/2602.12639", "authors": ["Yiran Rex Ma", "Yuxiao Ye", "Huiyuan Xie"], "title": "CLASE: A Hybrid Method for Chinese Legalese Stylistic Evaluation", "comment": "Accepted at LREC 2026", "summary": "Legal text generated by large language models (LLMs) can usually achieve reasonable factual accuracy, but it frequently fails to adhere to the specialised stylistic norms and linguistic conventions of legal writing. In order to improve stylistic quality, a crucial first step is to establish a reliable evaluation method. However, having legal experts manually develop such a metric is impractical, as the implicit stylistic requirements in legal writing practice are difficult to formalise into explicit rubrics. Meanwhile, existing automatic evaluation methods also fall short: reference-based metrics conflate semantic accuracy with stylistic fidelity, and LLM-as-a-judge evaluations suffer from opacity and inconsistency. To address these challenges, we introduce CLASE (Chinese LegAlese Stylistic Evaluation), a hybrid evaluation method that focuses on the stylistic performance of legal text. The method incorporates a hybrid scoring mechanism that combines 1) linguistic feature-based scores and 2) experience-guided LLM-as-a-judge scores. Both the feature coefficients and the LLM scoring experiences are learned from contrastive pairs of authentic legal documents and their LLM-restored counterparts. This hybrid design captures both surface-level features and implicit stylistic norms in a transparent, reference-free manner. Experiments on 200 Chinese legal documents show that CLASE achieves substantially higher alignment with human judgments than traditional metrics and pure LLM-as-a-judge methods. Beyond improved alignment, CLASE provides interpretable score breakdowns and suggestions for improvements, offering a scalable and practical solution for professional stylistic evaluation in legal text generation (Code and data for CLASE is available at: https://github.com/rexera/CLASE).", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCLASE\u7684\u6df7\u5408\u8bc4\u4ef7\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e2d\u6587\u6cd5\u5f8b\u6587\u672c\u7684\u98ce\u683c\u8d28\u91cf\uff0c\u7ed3\u5408\u8bed\u8a00\u7279\u5f81\u548c\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u5206\uff0c\u5b9e\u73b0\u65e0\u53c2\u7167\u3001\u900f\u660e\u3001\u9ad8\u5ea6\u7b26\u5408\u4eba\u5de5\u5224\u65ad\u7684\u98ce\u683c\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u6cd5\u5f8b\u6587\u672c\u7684\u81ea\u52a8\u98ce\u683c\u8bc4\u4ef7\u624b\u6bb5\u4e0d\u8db3\uff0c\u4eba\u5de5\u5236\u5b9a\u8bc4\u4ef7\u6807\u51c6\u96be\u4ee5\u5f62\u5f0f\u5316\uff0c\u4e14\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u8bed\u4e49\u51c6\u786e\u5ea6\u4e0e\u98ce\u683c\u4fdd\u771f\u5ea6\u4e0a\u5b58\u5728\u6df7\u6dc6\u6216\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\u3002", "method": "CLASE\u91c7\u7528\u6df7\u5408\u8bc4\u5206\u673a\u5236\uff0c\u7ed3\u5408\u57fa\u4e8e\u8bed\u8a00\u7279\u5f81\u7684\u5f97\u5206\u548c\u7ecf\u9a8c\u6307\u5bfc\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u5206\uff0c\u8fd9\u4e24\u4e2a\u90e8\u5206\u5747\u901a\u8fc7\u5bf9\u6bd4\u771f\u5b9e\u6cd5\u5f8b\u6587\u6863\u4e0e\u5176LLM\u8fd8\u539f\u7248\u672c\u5b66\u4e60\u83b7\u5f97\u3002", "result": "\u5728200\u4efd\u4e2d\u6587\u6cd5\u5f8b\u6587\u6863\u5b9e\u9a8c\u4e2d\uff0cCLASE\u65b9\u6cd5\u5728\u98ce\u683c\u8bc4\u4ef7\u4e0a\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u5339\u914d\u5ea6\u663e\u8457\u63d0\u5347\uff0c\u4e14\u80fd\u63d0\u4f9b\u6539\u8fdb\u5efa\u8bae\uff0c\u5c55\u793a\u4e86\u5176\u5b9e\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "CLASE\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u548c\u7eafLLM\u8bc4\u5206\u65b9\u6cd5\uff0c\u5728\u8bc4\u4f30\u6cd5\u5f8b\u6587\u672c\u98ce\u683c\u8d28\u91cf\u65f6\u66f4\u7b26\u5408\u4eba\u7c7b\u5224\u65ad\uff0c\u4e14\u80fd\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u8bc4\u5206\u7ec6\u8282\u548c\u6539\u8fdb\u5efa\u8bae\u3002"}}
{"id": "2602.12950", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.12950", "abs": "https://arxiv.org/abs/2602.12950", "authors": ["Zushuai Zhang", "Elliott Wen", "Ewan Tempero"], "title": "The Influence of Code Smells in Efferent Neighbors on Class Stability", "comment": null, "summary": "Understanding what drives code instability is essential for effective software maintenance, as unstable classes require larger or more frequent edits and increase the risk of unintended side effects. Although code smells are widely believed to harm maintainability, most prior stability studies examine only the smells within the class being modified. In practice, however, classes can change because their efferent neighbors (i.e., the classes they depend on) are modified due to ripple effects that propagate along static dependencies, even if the class itself is clean. Such ripple effects may be more severe when the efferent neighbor exhibits code smells. In addition, code smells rarely occur alone. They often appear together within a class or across classes connected by static dependencies, a phenomenon known as code smell interrelation. Such interrelation can lead to code smell interaction, where smells are directly connected through static dependencies and may further compound maintainability issues. However, the effect of code smell interrelation and interaction on code quality remains largely underexplored. Therefore, this study investigates whether the presence of code smells in a class's efferent neighbors affects its stability, considering the factor of code smell interrelation and interaction. To achieve this, we mine one year of commit history from 100 top-starred GitHub projects, detect code smells and static dependencies, determine code smell interrelation and interaction, and model these factors as predictors of class stability.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4ee3\u7801\u574f\u5473\u9053\u53ca\u5176\u5728\u4f9d\u8d56\u7c7b\u4e2d\u7684\u76f8\u4e92\u5173\u7cfb\u5982\u4f55\u5f71\u54cd\u4ee3\u7801\u7a33\u5b9a\u6027\uff0c\u5229\u7528\u5927\u89c4\u6a21\u5f00\u6e90\u9879\u76ee\u6570\u636e\uff0c\u63ed\u793a\u4e86\u4ee3\u7801\u574f\u5473\u9053\u4ea4\u4e92\u5bf9\u7ef4\u62a4\u6027\u7684\u590d\u5408\u5f71\u54cd\u3002", "motivation": "\u7406\u89e3\u4ee3\u7801\u4e0d\u7a33\u5b9a\u6027\u7684\u9a71\u52a8\u56e0\u7d20\u5bf9\u4e8e\u6709\u6548\u7684\u8f6f\u4ef6\u7ef4\u62a4\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u7c7b\u7684\u4e0d\u7a33\u5b9a\u6027\u4f1a\u5bfc\u81f4\u66f4\u591a\u6216\u66f4\u9891\u7e41\u7684\u4ee3\u7801\u4fee\u6539\uff0c\u5e76\u589e\u52a0\u610f\u5916\u526f\u4f5c\u7528\u7684\u98ce\u9669\u3002\u5df2\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u7c7b\u5185\u90e8\u7684\u4ee3\u7801\u574f\u5473\u9053\u5bf9\u7a33\u5b9a\u6027\u7684\u5f71\u54cd\uff0c\u5ffd\u7565\u4e86\u4ee3\u7801\u574f\u5473\u9053\u5728\u4f9d\u8d56\u7c7b\u4e2d\u7684\u4f5c\u7528\uff0c\u5c24\u5176\u662f\u4ee3\u7801\u574f\u5473\u9053\u7684\u76f8\u4e92\u5173\u8054\u548c\u4ea4\u4e92\u5bf9\u4ee3\u7801\u8d28\u91cf\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u63a2\u8ba8\u3002", "method": "\u901a\u8fc7\u6316\u6398100\u4e2a\u9ad8\u661fGitHub\u9879\u76ee\u4e00\u5e74\u7684\u63d0\u4ea4\u8bb0\u5f55\uff0c\u7ed3\u5408\u9759\u6001\u4f9d\u8d56\u5206\u6790\u548c\u4ee3\u7801\u574f\u5473\u9053\u68c0\u6d4b\uff0c\u6784\u5efa\u4ee3\u7801\u574f\u5473\u9053\u76f8\u4e92\u5173\u8054\u548c\u4ea4\u4e92\u6a21\u578b\uff0c\u5e76\u4ee5\u6b64\u9884\u6d4b\u7c7b\u7684\u7a33\u5b9a\u6027\u3002", "result": "\u901a\u8fc7\u5206\u6790GitHub\u4e0a100\u4e2a\u9ad8\u661f\u9879\u76ee\u4e00\u5e74\u5185\u7684\u63d0\u4ea4\u5386\u53f2\uff0c\u672c\u6587\u68c0\u6d4b\u4e86\u4ee3\u7801\u574f\u5473\u9053\u4e0e\u9759\u6001\u4f9d\u8d56\uff0c\u786e\u5b9a\u4e86\u4ee3\u7801\u574f\u5473\u9053\u7684\u76f8\u4e92\u5173\u8054\u548c\u4ea4\u4e92\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u9884\u6d4b\u7c7b\u7a33\u5b9a\u6027\u7684\u56e0\u7d20\u3002\u7814\u7a76\u8868\u660e\uff0c\u4f9d\u8d56\u7c7b\u4e2d\u5b58\u5728\u7684\u4ee3\u7801\u574f\u5473\u9053\u53ca\u5176\u76f8\u4e92\u5173\u7cfb\u5bf9\u7c7b\u7684\u7a33\u5b9a\u6027\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u4ee3\u7801\u574f\u5473\u9053\u4e0d\u4ec5\u5728\u7c7b\u5185\u90e8\u5f71\u54cd\u7a33\u5b9a\u6027\uff0c\u5176\u5728\u4f9d\u8d56\u7c7b\u4e2d\u7684\u5b58\u5728\u53ca\u76f8\u4e92\u4f5c\u7528\u540c\u6837\u52a0\u5267\u4e86\u4e0d\u7a33\u5b9a\u6027\uff0c\u663e\u793a\u51fa\u7ef4\u62a4\u7b56\u7565\u9700\u8003\u8651\u4ee3\u7801\u574f\u5473\u9053\u7684\u7f51\u7edc\u6548\u5e94\u3002"}}
{"id": "2602.12642", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12642", "abs": "https://arxiv.org/abs/2602.12642", "authors": ["Dohyung Kim", "Minbeom Kim", "Jeonghye Kim", "Sangmook Lee", "Sojeong Rhee", "Kyomin Jung"], "title": "Beyond Normalization: Rethinking the Partition Function as a Difficulty Scheduler for RLVR", "comment": null, "summary": "Reward-maximizing RL methods enhance the reasoning performance of LLMs, but often reduce the diversity among outputs. Recent works address this issue by adopting GFlowNets, training LLMs to match a target distribution while jointly learning its partition function. In contrast to prior works that treat this partition function solely as a normalizer, we reinterpret it as a per-prompt expected-reward (i.e., online accuracy) signal, leveraging this unused information to improve sample efficiency. Specifically, we first establish a theoretical relationship between the partition function and per-prompt accuracy estimates. Building on this key insight, we propose Partition Function-Guided RL (PACED-RL), a post-training framework that leverages accuracy estimates to prioritize informative question prompts during training, and further improves sample efficiency through an accuracy estimate error-prioritized replay. Crucially, both components reuse information already produced during GFlowNet training, effectively amortizing the compute overhead into the existing optimization process. Extensive experiments across diverse benchmarks demonstrate strong performance improvements over GRPO and prior GFlowNet approaches, highlighting PACED-RL as a promising direction for a more sample efficient distribution-matching training for LLMs.", "AI": {"tldr": "\u901a\u8fc7\u91cd\u65b0\u5229\u7528GFlowNet\u8bad\u7ec3\u4e2d\u5206\u533a\u51fd\u6570\u7684\u51c6\u786e\u7387\u4fe1\u606f\uff0cPACED-RL\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u5206\u5e03\u5339\u914d\u8bad\u7ec3\uff0c\u63d0\u9ad8\u4e86\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6027\u80fd\u548c\u6837\u672c\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u5956\u52b1\u6700\u5927\u5316\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u63d0\u5347\u4e86LLMs\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u51cf\u5c11\u4e86\u8f93\u51fa\u591a\u6837\u6027\uff0c\u4e14\u73b0\u6709\u7528GFlowNet\u7684\u65b9\u6cd5\u672a\u5145\u5206\u5229\u7528\u5206\u533a\u51fd\u6570\u4e2d\u7684\u6709\u4ef7\u503c\u4fe1\u606f\uff0c\u5bfc\u81f4\u6837\u672c\u6548\u7387\u8f83\u4f4e\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u8303\u5f0f\u3002", "method": "\u65b9\u6cd5\u57fa\u4e8e\u5c06\u5206\u533a\u51fd\u6570\u89c6\u4e3a\u6bcf\u4e2a\u63d0\u793a\u7684\u671f\u671b\u5956\u52b1\u4fe1\u53f7\uff0c\u7ed3\u5408\u4f18\u5148\u91c7\u6837\u548c\u57fa\u4e8e\u51c6\u786e\u7387\u4f30\u8ba1\u8bef\u5dee\u7684\u91cd\u653e\u673a\u5236\uff0c\u5b9e\u73b0\u8bad\u7ec3\u8fc7\u7a0b\u7684\u6837\u672c\u5229\u7528\u6700\u5927\u5316\uff0c\u5e76\u4e14\u91cd\u590d\u5229\u7528\u5df2\u6709\u4fe1\u606f\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPACED-RL\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u65b0\u89e3\u8bfbGFlowNet\u4e2d\u7684\u5206\u533a\u51fd\u6570\u4e3a\u6bcf\u4e2a\u63d0\u793a\u8bed\u7684\u671f\u671b\u5956\u52b1\u4fe1\u53f7\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u8bad\u7ec3\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6837\u672c\u6548\u7387\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u5206\u533a\u51fd\u6570\u4f30\u8ba1\u51c6\u786e\u7387\uff0c\u4f18\u5148\u9009\u62e9\u4fe1\u606f\u91cf\u4e30\u5bcc\u7684\u63d0\u793a\u8bed\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u51c6\u786e\u7387\u8bef\u5dee\u7684\u91cd\u653e\u673a\u5236\u8fdb\u884c\u4f18\u5316\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684GRPO\u53caGFlowNet\u65b9\u6cd5\u3002", "conclusion": "PACED-RL\u901a\u8fc7\u5de7\u5999\u5229\u7528\u5206\u533a\u51fd\u6570\u4e2d\u7684\u989d\u5916\u4fe1\u606f\uff0c\u6709\u6548\u63d0\u5347\u4e86LLMs\u8bad\u7ec3\u7684\u6837\u672c\u6548\u7387\u548c\u63a8\u7406\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u4f5c\u4e3a\u4e00\u79cd\u9ad8\u6548\u5206\u5e03\u5339\u914d\u8bad\u7ec3\u65b9\u6cd5\u7684\u6f5c\u529b\u3002"}}
{"id": "2602.13029", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.13029", "abs": "https://arxiv.org/abs/2602.13029", "authors": ["David Dietrich", "Armin Lechler", "Alexander Verl"], "title": "Analysis of Asset Administration Shell-based Negotiation Processes for Scaling Applications", "comment": "9 pages, 2 figures", "summary": "The proactive Asset Administration Shell (AAS) enables bidirectional communication between assets. It uses the Language for I4.0 Components in VDI/VDE 2193 to facilitate negotiations, such as allocating products to available production resources. This paper investigates the efficiency of the negotiation, based on criteria, such as message load, for applications with a scaling number of assets. Currently, the focus of AAS standardization is on submodels and their security to enable interoperable data access. Their proactive behavior remains conceptual and is still a subject of scientific research. Existing studies examine proactive AAS architecture examples with a limited number of assets, raising questions about their scalability in industrial environments. To analyze proactive AAS for scaling applications, a scenario and evaluation criteria are introduced. A scalable implementation is developed using current architectures for proactive AAS, upon which experiments are conducted with a varying number of assets. The results reveal the performance limitations, communication overhead, and adaptability of the AAS-based negotiation mechanism scaling. This information can improve the further development and standardization of the AAS.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30\u4e86\u4e3b\u52a8\u8d44\u4ea7\u7ba1\u7406\u58f3\u5728\u591a\u8d44\u4ea7\u573a\u666f\u4e0b\u7684\u6027\u80fd\u548c\u901a\u4fe1\u8d1f\u8f7d\uff0c\u6307\u51fa\u4e86\u5176\u53ef\u6269\u5c55\u6027\u74f6\u9888\uff0c\u4e3aAAS\u6807\u51c6\u5316\u63d0\u4f9b\u53c2\u8003\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u4e3b\u52a8AAS\u884c\u4e3a\u7684\u6982\u5ff5\u6027\u9650\u5236\u53ca\u5176\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u53ef\u6269\u5c55\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u5347AAS\u5728\u591a\u8d44\u4ea7\u901a\u4fe1\u548c\u534f\u5546\u4e2d\u7684\u6548\u7387\uff0c\u51cf\u5c11\u6d88\u606f\u8d1f\u8f7d\u3002", "method": "\u57fa\u4e8eVDI/VDE 2193\u4e2d\u5de5\u4e1a4.0\u7ec4\u4ef6\u8bed\u8a00\u7684\u4e3b\u52a8\u8d44\u4ea7\u7ba1\u7406\u58f3(AAS)\u67b6\u6784\uff0c\u901a\u8fc7\u8bbe\u5b9a\u573a\u666f\u548c\u8bc4\u4f30\u6807\u51c6\uff0c\u5f00\u53d1\u53ef\u6269\u5c55\u5b9e\u73b0\u5e76\u8fdb\u884c\u591a\u8d44\u4ea7\u6570\u91cf\u4e0b\u7684\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e86\u57fa\u4e8eAAS\u7684\u534f\u5546\u673a\u5236\u5728\u6027\u80fd\u3001\u901a\u4fe1\u8d1f\u8f7d\u53ca\u9002\u5e94\u6027\u65b9\u9762\u5b58\u5728\u7684\u9650\u5236\uff0c\u5c55\u793a\u4e86\u968f\u7740\u8d44\u4ea7\u6570\u91cf\u589e\u52a0\u5176\u53ef\u6269\u5c55\u6027\u95ee\u9898\u548c\u901a\u4fe1\u5f00\u9500\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\u4e3b\u52a8AAS\u5728\u591a\u8d44\u4ea7\u89c4\u6a21\u4e0b\u5b58\u5728\u6027\u80fd\u548c\u901a\u4fe1\u5f00\u9500\u9650\u5236\uff0c\u9700\u4f18\u5316\u673a\u5236\u4ee5\u63d0\u5347\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u5de5\u4e1a\u73af\u5883\u7684\u9700\u6c42\uff0c\u63a8\u52a8AAS\u6807\u51c6\u7684\u8fdb\u4e00\u6b65\u5b8c\u5584\u3002"}}
{"id": "2602.12660", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12660", "abs": "https://arxiv.org/abs/2602.12660", "authors": ["Longze Chen", "Lu Wang", "Renke Shan", "Ze Gong", "Run Luo", "Jiaming Li", "Jing Luo", "Qiyao Wang", "Min Yang"], "title": "Learning Ordinal Probabilistic Reward from Preferences", "comment": "28 pages, 5 figures, ICLR 2026", "summary": "Reward models are crucial for aligning large language models (LLMs) with human values and intentions. Existing approaches follow either Generative (GRMs) or Discriminative (DRMs) paradigms, yet both suffer from limitations: GRMs typically demand costly point-wise supervision, while DRMs produce uncalibrated relative scores that lack probabilistic interpretation. To address these challenges, we introduce a novel reward modeling paradigm: Probabilistic Reward Model (PRM). Instead of modeling reward as a deterministic scalar, our approach treats it as a random variable, learning a full probability distribution for the quality of each response. To make this paradigm practical, we present its closed-form, discrete realization: the Ordinal Probabilistic Reward Model (OPRM), which discretizes the quality score into a finite set of ordinal ratings. Building on OPRM, we propose a data-efficient training strategy called Region Flooding Tuning (RgFT). It enables rewards to better reflect absolute text quality by incorporating quality-level annotations, which guide the model to concentrate the probability mass within corresponding rating sub-regions. Experiments on various reward model benchmarks show that our method improves accuracy by $\\textbf{2.9%}\\sim\\textbf{7.4%}$ compared to prior reward models, demonstrating strong performance and data efficiency. Analysis of the score distribution provides evidence that our method captures not only relative rankings but also absolute quality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6982\u7387\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u5b66\u4e60\u5956\u52b1\u7684\u6982\u7387\u5206\u5e03\u5e76\u5f15\u5165\u5e8f\u6570\u8bc4\u7ea7\u548c\u533a\u57df\u6cdb\u6d2a\u8c03\u4f18\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5956\u52b1\u6a21\u578b\u7684\u51c6\u786e\u7387\u548c\u6570\u636e\u6548\u7387\uff0c\u66f4\u597d\u5730\u53cd\u6620\u6587\u672c\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u5f0f\u5956\u52b1\u6a21\u578b\u9700\u8981\u6602\u8d35\u7684\u70b9\u5bf9\u70b9\u76d1\u7763\uff0c\u5224\u522b\u5f0f\u5956\u52b1\u6a21\u578b\u5f97\u5206\u7f3a\u4e4f\u6982\u7387\u89e3\u91ca\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u6821\u51c6\u548c\u6027\u80fd\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u5956\u52b1\u5efa\u6a21\u8303\u5f0f\u6765\u514b\u670d\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "PRM\u5c06\u5956\u52b1\u89c6\u4e3a\u968f\u673a\u53d8\u91cf\uff0c\u5b66\u4e60\u5956\u52b1\u7684\u6982\u7387\u5206\u5e03\uff1b\u57fa\u4e8ePRM\u63d0\u51faOPRM\uff0c\u5c06\u8d28\u91cf\u8bc4\u5206\u79bb\u6563\u5316\u4e3a\u5e8f\u6570\u8bc4\u7ea7\uff1b\u5f15\u5165\u533a\u57df\u6cdb\u6d2a\u8c03\u4f18(RgFT)\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u8d28\u91cf\u7b49\u7ea7\u6ce8\u91ca\uff0c\u5f15\u5bfc\u6a21\u578b\u805a\u7126\u6982\u7387\u8d28\u91cf\u5b50\u533a\u4ee5\u63d0\u5347\u4f53\u73b0\u7edd\u5bf9\u8d28\u91cf\u7684\u80fd\u529b\u3002", "result": "\u5728\u591a\u4e2a\u5956\u52b1\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u65b9\u6cd5\u63d0\u5347\u4e862.9%\u81f37.4%\u7684\u51c6\u786e\u7387\uff0c\u8868\u73b0\u51fa\u8f83\u5f3a\u7684\u6027\u80fd\u548c\u6570\u636e\u6548\u7387\uff0c\u540c\u65f6\u5f97\u5206\u5206\u5e03\u5206\u6790\u8868\u660e\u5176\u65e2\u6355\u6349\u4e86\u76f8\u5bf9\u6392\u540d\u4e5f\u4f53\u73b0\u4e86\u7edd\u5bf9\u8d28\u91cf\u3002", "conclusion": "\u63d0\u51fa\u7684\u6982\u7387\u5956\u52b1\u6a21\u578b\uff08PRM\uff09\u548c\u5176\u5177\u4f53\u5b9e\u73b0\u5e8f\u6570\u6982\u7387\u5956\u52b1\u6a21\u578b\uff08OPRM\uff09\u6709\u6548\u63d0\u5347\u4e86\u5956\u52b1\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u6570\u636e\u6548\u7387\uff0c\u80fd\u591f\u66f4\u597d\u5730\u53cd\u6620\u7edd\u5bf9\u6587\u672c\u8d28\u91cf\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u751f\u6210\u5f0f\u548c\u5224\u522b\u5f0f\u5956\u52b1\u6a21\u578b\u7684\u8868\u73b0\u3002"}}
{"id": "2602.13072", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.13072", "abs": "https://arxiv.org/abs/2602.13072", "authors": ["Diego Clerissi", "Elena Masserini", "Daniela Micucci", "Leonardo Mariani"], "title": "Automated Testing of Task-based Chatbots: How Far Are We?", "comment": "8 pages, 3 figures, Accepted at 23rd International Conference on Mining Software Repositories (MSR) 2026 - Registered Reports", "summary": "Task-based chatbots are software, typically embedded in real-world applications, that assist users in completing tasks through a conversational interface. As chatbots are gaining popularity, effectively assessing their quality has become crucial. Whereas traditional testing techniques fail to systematically exercise the conversational space of chatbots, several approaches specifically targeting chatbots have emerged from both industry and research. Although these techniques have shown advancements over the years, they still exhibit limitations, such as simplicity of the generated test scenarios and weakness in implemented oracles. In this paper, we conduct a confirmatory study to investigate such limitations by evaluating the effectiveness of state-of-the-art chatbot testing techniques on a curated selection of task-based chatbots from GitHub, developed using the most popular commercial and open-source platforms.", "AI": {"tldr": "\u672c\u6587\u5bf9\u6d41\u884c\u4efb\u52a1\u578b\u804a\u5929\u673a\u5668\u4eba\u7684\u73b0\u6709\u6d4b\u8bd5\u6280\u672f\u8fdb\u884c\u4e86\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u53d1\u73b0\u5176\u5728\u6d4b\u8bd5\u573a\u666f\u548c\u5224\u5b9a\u673a\u5236\u4e0a\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "motivation": "\u968f\u7740\u4efb\u52a1\u578b\u804a\u5929\u673a\u5668\u4eba\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f20\u7edf\u6d4b\u8bd5\u65b9\u6cd5\u96be\u4ee5\u7cfb\u7edf\u8986\u76d6\u5bf9\u8bdd\u7a7a\u95f4\uff0c\u4e9f\u9700\u8bc4\u4f30\u5e76\u6539\u8fdb\u4e13\u7528\u6d4b\u8bd5\u6280\u672f\u4ee5\u786e\u4fdd\u804a\u5929\u673a\u5668\u4eba\u7684\u8d28\u91cf\u3002", "method": "\u901a\u8fc7\u5bf9GitHub\u4e0a\u4f7f\u7528\u4e3b\u6d41\u5546\u4e1a\u53ca\u5f00\u6e90\u5e73\u53f0\u5f00\u53d1\u7684\u4efb\u52a1\u578b\u804a\u5929\u673a\u5668\u4eba\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u68c0\u9a8c\u73b0\u6709\u5148\u8fdb\u6d4b\u8bd5\u6280\u672f\u7684\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u73b0\u6709\u6d4b\u8bd5\u6280\u672f\u5728\u751f\u6210\u6d4b\u8bd5\u573a\u666f\u7684\u590d\u6742\u6027\u548c\u5224\u5b9a\u673a\u5236\u7684\u51c6\u786e\u6027\u65b9\u9762\u5b58\u5728\u660e\u663e\u4e0d\u8db3\u3002", "conclusion": "\u73b0\u6709\u4efb\u52a1\u578b\u804a\u5929\u673a\u5668\u4eba\u6d4b\u8bd5\u6280\u672f\u5728\u6d4b\u8bd5\u573a\u666f\u751f\u6210\u548c\u5224\u5b9a\u6807\u51c6\u8bbe\u8ba1\u65b9\u9762\u4ecd\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u4ee5\u63d0\u5347\u6d4b\u8bd5\u6548\u679c\u3002"}}
{"id": "2602.12674", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12674", "abs": "https://arxiv.org/abs/2602.12674", "authors": ["Yuang Cai", "Yuyu Yuan"], "title": "$\\mathcal{X}$-KD: General Experiential Knowledge Distillation for Large Language Models", "comment": null, "summary": "Knowledge Distillation (KD) for Large Language Models (LLMs) has become increasingly important as models grow in size and complexity. While existing distillation approaches focus on imitating teacher behavior, they often overlook the original learning environment that shaped the teacher's knowledge. Inspired by the experiential learning theory and inverse reinforcement learning, we propose Experiential Knowledge Distillation ($\\mathcal{X}$-KD), a novel and general framework that enables student models to learn in the teacher's original learning environment. $\\mathcal{X}$-KD adopts the Approximated Variational Reward Imitation Learning (AVRIL) framework to jointly model the teacher's original reward function and perform policy distillation, encouraging consistency between the student policy and the original reward function. Our derivation demonstrates that $\\mathcal{X}$-KD follows the supervised learning framework and applies to both sequence-level and divergence-based distillation methods, underlining the simplicity and flexibility of our approach. Empirical results show that $\\mathcal{X}$-KD outperforms the generalized KD and MiniLLM baselines on abstractive summarization, machine translation, and arithmetic reasoning tasks. Additionally, $\\mathcal{X}$-KD achieves better performance-diversity trade-off and data efficiency than baseline KD approaches.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f53\u9a8c\u5b66\u4e60\u7406\u8bba\u548c\u9006\u5411\u5f3a\u5316\u5b66\u4e60\u7684\u65b0\u9896\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u2014\u2014\u4f53\u9a8c\u77e5\u8bc6\u84b8\u998f\uff08X-KD\uff09\uff0c\u4f7f\u5b66\u751f\u6a21\u578b\u80fd\u6a21\u62df\u6559\u5e08\u6a21\u578b\u7684\u539f\u59cb\u5b66\u4e60\u73af\u5883\uff0c\u4ece\u800c\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u84b8\u998f\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u591a\u5173\u6ce8\u6a21\u4eff\u6559\u5e08\u884c\u4e3a\uff0c\u5ffd\u89c6\u4e86\u6559\u5e08\u77e5\u8bc6\u5f62\u6210\u7684\u539f\u59cb\u5b66\u4e60\u73af\u5883\uff0c\u53d7\u4f53\u9a8c\u5b66\u4e60\u7406\u8bba\u548c\u9006\u5411\u5f3a\u5316\u5b66\u4e60\u542f\u53d1\uff0c\u8bbe\u8ba1\u66f4\u7b26\u5408\u6559\u5e08\u8bad\u7ec3\u8fc7\u7a0b\u7684\u84b8\u998f\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u8fd1\u4f3c\u53d8\u5206\u5956\u52b1\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff08AVRIL\uff09\uff0c\u8054\u5408\u5efa\u6a21\u6559\u5e08\u539f\u59cb\u5956\u52b1\u51fd\u6570\u5e76\u6267\u884c\u7b56\u7565\u84b8\u998f\uff0c\u786e\u4fdd\u5b66\u751f\u7b56\u7565\u4e0e\u539f\u59cb\u5956\u52b1\u51fd\u6570\u7684\u4e00\u81f4\u6027\u3002", "result": "X-KD\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff08\u5305\u62ec\u5e7f\u4e49\u77e5\u8bc6\u84b8\u998f\u548cMiniLLM\uff09\uff0c\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6027\u80fd\u548c\u591a\u6837\u6027\u6743\u8861\u4ee5\u53ca\u66f4\u9ad8\u7684\u6570\u636e\u5229\u7528\u6548\u7387\u3002", "conclusion": "X-KD\u65b9\u6cd5\u5728\u6458\u8981\u751f\u6210\u3001\u673a\u5668\u7ffb\u8bd1\u548c\u7b97\u672f\u63a8\u7406\u7b49\u4efb\u52a1\u4e0a\u4f18\u4e8e\u4f20\u7edf\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff0c\u4e14\u5728\u6027\u80fd\u4e0e\u591a\u6837\u6027\u4ee5\u53ca\u6570\u636e\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u66f4\u597d\u7684\u5e73\u8861\u3002"}}
{"id": "2602.13170", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.13170", "abs": "https://arxiv.org/abs/2602.13170", "authors": ["Saleha Muzammil", "Mughees Ur Rehman", "Zoe Kotti", "Diomidis Spinellis"], "title": "Source Code Hotspots: A Diagnostic Method for Quality Issues", "comment": "Published at the 23rd International Conference on Mining Software Repositories", "summary": "Software source code often harbours \"hotspots\": small portions of the code that change far more often than the rest of the project and thus concentrate maintenance activity. We mine the complete version histories of 91 evolving, actively developed GitHub repositories and identify 15 recurring line-level hotspot patterns that explain why these hotspots emerge. The three most prevalent patterns are Pinned Version Bump (26%), revealing brittle release practices; Long Line Change (17%), signalling deficient layout; and Formatting Ping-Pong (9%), indicating missing or inconsistent style automation. Surprisingly, automated accounts generate 74% of all hotspot edits, suggesting that bot activity is a dominant but largely avoidable source of noise in change histories. By mapping each pattern to concrete refactoring guidelines and continuous integration checks, our taxonomy equips practitioners with actionable steps to curb hotspots and systematically improve software quality in terms of configurability, stability, and changeability.", "AI": {"tldr": "\u672c\u6587\u6316\u639891\u4e2aGitHub\u9879\u76ee\u5386\u53f2\uff0c\u53d1\u73b015\u79cd\u4ee3\u7801\u70ed\u70b9\u6a21\u5f0f\uff0c\u81ea\u52a8\u5316\u5de5\u5177\u5236\u9020\u5927\u91cf\u53d8\u66f4\u566a\u58f0\uff0c\u63d0\u51fa\u5bf9\u5e94\u91cd\u6784\u548cCI\u63aa\u65bd\u4ee5\u63d0\u5347\u8f6f\u4ef6\u8d28\u91cf\u3002", "motivation": "\u8f6f\u4ef6\u70ed\u70b9\u4ee3\u7801\u9891\u7e41\u53d8\u66f4\uff0c\u96c6\u4e2d\u7ef4\u62a4\u8d1f\u62c5\uff0c\u7406\u89e3\u5176\u4ea7\u751f\u539f\u56e0\u6709\u52a9\u4e8e\u63d0\u5347\u4ee3\u7801\u8d28\u91cf\u548c\u7ef4\u62a4\u6548\u7387\u3002", "method": "\u5206\u679091\u4e2a\u6d3b\u8dc3GitHub\u9879\u76ee\u7684\u5b8c\u6574\u7248\u672c\u5386\u53f2\uff0c\u8bc6\u522b\u70ed\u70b9\u4ee3\u7801\u768415\u79cd\u6a21\u5f0f\uff0c\u5e76\u91cf\u5316\u5176\u51fa\u73b0\u9891\u7387\u53ca\u6210\u56e0\u3002", "result": "\u672c\u6587\u901a\u8fc7\u5bf991\u4e2a\u6d3b\u8dc3GitHub\u9879\u76ee\u7684\u5b8c\u6574\u7248\u672c\u5386\u53f2\u8fdb\u884c\u6316\u6398\uff0c\u8bc6\u522b\u51fa\u4e8615\u79cd\u53cd\u590d\u51fa\u73b0\u7684\u4ee3\u7801\u70ed\u70b9\u6a21\u5f0f\uff0c\u63ed\u793a\u4e86\u70ed\u70b9\u51fa\u73b0\u7684\u539f\u56e0\u3002\u6700\u5e38\u89c1\u7684\u4e09\u79cd\u6a21\u5f0f\u662f\u7248\u672c\u56fa\u5b9a\u66f4\u65b0\uff0826%\uff09\u3001\u957f\u884c\u4ee3\u7801\u4fee\u6539\uff0817%\uff09\u548c\u683c\u5f0f\u53cd\u590d\u8c03\u6574\uff089%\uff09\u3002\u7814\u7a76\u53d1\u73b0\u5927\u91cf\u70ed\u70b9\u7f16\u8f91\u7531\u81ea\u52a8\u5316\u5de5\u5177\u751f\u6210\uff0874%\uff09\uff0c\u8868\u660e\u673a\u5668\u4eba\u7684\u6d3b\u52a8\u662f\u53d8\u66f4\u5386\u53f2\u4e2d\u4e3b\u8981\u4f46\u53ef\u907f\u514d\u7684\u566a\u58f0\u3002\u4f5c\u8005\u901a\u8fc7\u5c06\u8fd9\u4e9b\u6a21\u5f0f\u5bf9\u5e94\u5177\u4f53\u7684\u91cd\u6784\u6307\u5bfc\u548c\u6301\u7eed\u96c6\u6210\u68c0\u67e5\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u63a7\u5236\u70ed\u70b9\u3001\u63d0\u5347\u8f6f\u4ef6\u914d\u7f6e\u6027\u3001\u7a33\u5b9a\u6027\u548c\u53ef\u53d8\u6027\u7684\u53ef\u64cd\u4f5c\u65b9\u6848\u3002", "conclusion": "\u70ed\u70b9\u4ee3\u7801\u4e3b\u8981\u7531\u81ea\u52a8\u5316\u5de5\u5177\u5f15\u53d1\uff0c\u901a\u8fc7\u9488\u5bf9\u70ed\u70b9\u6a21\u5f0f\u5b9e\u65bd\u91cd\u6784\u548c\u6301\u7eed\u96c6\u6210\u68c0\u67e5\uff0c\u53ef\u6709\u6548\u63d0\u5347\u8f6f\u4ef6\u8d28\u91cf\uff0c\u51cf\u5c11\u7ef4\u62a4\u8d1f\u62c5\u3002"}}
{"id": "2602.12705", "categories": ["cs.CL", "cs.AI", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.12705", "abs": "https://arxiv.org/abs/2602.12705", "authors": ["Baorong Shi", "Bo Cui", "Boyuan Jiang", "Deli Yu", "Fang Qian", "Haihua Yang", "Huichao Wang", "Jiale Chen", "Jianfei Pan", "Jieqiong Cao", "Jinghao Lin", "Kai Wu", "Lin Yang", "Shengsheng Yao", "Tao Chen", "Xiaojun Xiao", "Xiaozhong Ji", "Xu Wang", "Yijun He", "Zhixiong Yang"], "title": "MedXIAOHE: A Comprehensive Recipe for Building Medical MLLMs", "comment": null, "summary": "We present MedXIAOHE, a medical vision-language foundation model designed to advance general-purpose medical understanding and reasoning in real-world clinical applications. MedXIAOHE achieves state-of-the-art performance across diverse medical benchmarks and surpasses leading closed-source multimodal systems on multiple capabilities. To achieve this, we propose an entity-aware continual pretraining framework that organizes heterogeneous medical corpora to broaden knowledge coverage and reduce long-tail gaps (e.g., rare diseases). For medical expert-level reasoning and interaction, MedXIAOHE incorporates diverse medical reasoning patterns via reinforcement learning and tool-augmented agentic training, enabling multi-step diagnostic reasoning with verifiable decision traces. To improve reliability in real-world use, MedXIAOHE integrates user-preference rubrics, evidence-grounded reasoning, and low-hallucination long-form report generation, with improved adherence to medical instructions. We release this report to document our practical design choices, scaling insights, and evaluation framework, hoping to inspire further research.", "AI": {"tldr": "MedXIAOHE\u901a\u8fc7\u521b\u65b0\u7684\u9884\u8bad\u7ec3\u6846\u67b6\u548c\u591a\u6837\u63a8\u7406\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5168\u9762\u63d0\u5347\uff0c\u63a8\u52a8\u533b\u5b66\u9886\u57df\u7684\u4e34\u5e8a\u5e94\u7528\u3002", "motivation": "\u63d0\u5347\u533b\u5b66\u6a21\u578b\u5728\u771f\u5b9e\u4e34\u5e8a\u5e94\u7528\u4e2d\u7684\u901a\u7528\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\uff0c\u8986\u76d6\u66f4\u591a\u533b\u5b66\u77e5\u8bc6\u5e76\u51cf\u5c11\u7f55\u89c1\u75be\u75c5\u7684\u957f\u5c3e\u7f3a\u5931\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u79cd\u540d\u4e3aMedXIAOHE\u7684\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff0c\u91c7\u7528\u5b9e\u4f53\u611f\u77e5\u7684\u6301\u7eed\u9884\u8bad\u7ec3\u6846\u67b6\u3001\u591a\u6837\u5316\u533b\u5b66\u63a8\u7406\u6a21\u5f0f\u7684\u5f3a\u5316\u5b66\u4e60\u548c\u5de5\u5177\u589e\u5f3a\u8bad\u7ec3\u3002", "result": "MedXIAOHE\u5728\u591a\u79cd\u533b\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u65b0\u6700\u9ad8\u6027\u80fd\uff0c\u8d85\u8d8a\u9886\u5148\u5c01\u95ed\u591a\u6a21\u6001\u7cfb\u7edf\uff0c\u5728\u591a\u9879\u80fd\u529b\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "MedXIAOHE\u6a21\u578b\u5177\u5907\u5e7f\u6cdb\u533b\u5b66\u77e5\u8bc6\u8986\u76d6\u4e0e\u4e13\u5bb6\u7ea7\u63a8\u7406\u80fd\u529b\uff0c\u80fd\u751f\u6210\u4f4e\u5e7b\u89c9\u3001\u7b26\u5408\u533b\u5b66\u6307\u4ee4\u7684\u957f\u62a5\u544a\uff0c\u63d0\u5347\u4e86\u533b\u5b66AI\u7684\u5b9e\u7528\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2602.12966", "categories": ["cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.12966", "abs": "https://arxiv.org/abs/2602.12966", "authors": ["Yue Huang", "Zhengzhe Jiang", "Yuchen Ma", "Yu Jiang", "Xiangqi Wang", "Yujun Zhou", "Yuexing Hao", "Kehan Guo", "Pin-Yu Chen", "Stefan Feuerriegel", "Xiangliang Zhang"], "title": "ProbeLLM: Automating Principled Diagnosis of LLM Failures", "comment": null, "summary": "Understanding how and why large language models (LLMs) fail is becoming a central challenge as models rapidly evolve and static evaluations fall behind. While automated probing has been enabled by dynamic test generation, existing approaches often discover isolated failure cases, lack principled control over exploration, and provide limited insight into the underlying structure of model weaknesses. We propose ProbeLLM, a benchmark-agnostic automated probing framework that elevates weakness discovery from individual failures to structured failure modes. ProbeLLM formulates probing as a hierarchical Monte Carlo Tree Search, explicitly allocating limited probing budgets between global exploration of new failure regions and local refinement of recurring error patterns. By restricting probing to verifiable test cases and leveraging tool-augmented generation and verification, ProbeLLM grounds failure discovery in reliable evidence. Discovered failures are further consolidated into interpretable failure modes via failure-aware embeddings and boundary-aware induction. Across diverse benchmarks and LLMs, ProbeLLM reveals substantially broader, cleaner, and more fine-grained failure landscapes than static benchmarks and prior automated methods, supporting a shift from case-centric evaluation toward principled weakness discovery.", "AI": {"tldr": "\u63d0\u51fa\u4e86ProbeLLM\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u8499\u7279\u5361\u7f57\u6811\u641c\u7d22\u81ea\u52a8\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7ed3\u6784\u5316\u5931\u8d25\u6a21\u5f0f\uff0c\u6bd4\u4f20\u7edf\u9759\u6001\u57fa\u51c6\u6d4b\u8bd5\u66f4\u5168\u9762\u548c\u7ec6\u7c92\u5ea6\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u63a2\u6d4b\u65b9\u6cd5\u591a\u53d1\u73b0\u5b64\u7acb\u5931\u8d25\uff0c\u7f3a\u5c11\u7cfb\u7edf\u63a7\u5236\u548c\u6df1\u5165\u89c1\u89e3\uff0c\u96be\u4ee5\u8ddf\u4e0a\u5927\u8bed\u8a00\u6a21\u578b\u5feb\u901f\u6f14\u8fdb\u548c\u9759\u6001\u8bc4\u6d4b\u6ede\u540e\u7684\u6311\u6218\u3002", "method": "\u5c06\u63a2\u6d4b\u4efb\u52a1\u5efa\u6a21\u4e3a\u5206\u5c42\u8499\u7279\u5361\u7f57\u6811\u641c\u7d22\uff0c\u5e73\u8861\u5168\u5c40\u63a2\u7d22\u548c\u5c40\u90e8\u7ec6\u5316\uff0c\u7ed3\u5408\u53ef\u9a8c\u8bc1\u6d4b\u8bd5\u3001\u5de5\u5177\u589e\u5f3a\u751f\u6210\u4e0e\u9a8c\u8bc1\uff0c\u6700\u540e\u901a\u8fc7\u5931\u8d25\u611f\u77e5\u5d4c\u5165\u548c\u8fb9\u754c\u611f\u77e5\u5f52\u7eb3\u6574\u7406\u5931\u8d25\u6a21\u5f0f\u3002", "result": "ProbeLLM\u5728\u591a\u4e2a\u57fa\u51c6\u548c\u8bed\u8a00\u6a21\u578b\u4e0a\u8868\u73b0\u51fa\u66f4\u5168\u9762\u3001\u66f4\u5e72\u51c0\u3001\u66f4\u7ec6\u7c92\u5ea6\u7684\u5931\u8d25\u6a21\u5f0f\u53d1\u73b0\u80fd\u529b\uff0c\u652f\u6301\u4ece\u4e2a\u6848\u4e2d\u5fc3\u5411\u5f31\u70b9\u7ed3\u6784\u5316\u53d1\u73b0\u8f6c\u53d8\u3002", "conclusion": "ProbeLLM\u80fd\u591f\u63ed\u793a\u6bd4\u9759\u6001\u57fa\u51c6\u548c\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u66f4\u5e7f\u6cdb\u3001\u66f4\u6e05\u6670\u3001\u66f4\u7ec6\u7c92\u5ea6\u7684\u6a21\u578b\u5931\u8d25\u6a21\u5f0f\uff0c\u63a8\u52a8\u4ece\u4e2a\u4f8b\u8bc4\u4f30\u5411\u7cfb\u7edf\u6027\u5f31\u70b9\u53d1\u73b0\u8f6c\u53d8\u3002"}}
{"id": "2602.12709", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12709", "abs": "https://arxiv.org/abs/2602.12709", "authors": ["Yixin Chen", "Ying Xiong", "Shangyu Wu", "Xiangrui Ke", "Nan Guan", "Chun Jason Xue"], "title": "ReFilter: Improving Robustness of Retrieval-Augmented Generation via Gated Filter", "comment": null, "summary": "Retrieval-augmented generation (RAG) has become a dominant paradigm for grounding large language models (LLMs) with external evidence in knowledge-intensive question answering. A core design choice is how to fuse retrieved samples into the LLMs, where existing internal fusion approaches broadly fall into query-based fusion, parametric fusion, and latent-based fusion. Despite their effectiveness at modest retrieval scales, these methods often fail to scale gracefully as the number of retrieved candidates k increases: Larger k improves evidence coverage, yet realistic top-k retrieval inevitably contains irrelevant or redundant content and increases the inference cost.\n  To address these limitations, we propose ReFilter, a novel latent-based fusion framework that performs token-level filtering and fusion. ReFilter consists of three key components: a context encoder for encoding context features, a gated filter for weighting each token, and a token fusion module for integrating the weighted token feature into the LLM's hidden states. Our experiments across four general-domain QA benchmarks show that ReFilter consistently achieves the best average performance under both in-domain adaptation and out-of-domain transfer. ReFilter further generalizes to five biomedical QA benchmarks in zero-shot transfer without domain fine-tuning, reaching 70.01% average accuracy with Qwen2.5-14B-Instruct.", "AI": {"tldr": "\u63d0\u51faReFilter\uff0c\u901a\u8fc7token\u7ea7\u7b5b\u9009\u4e0e\u878d\u5408\u63d0\u5347\u5927\u89c4\u6a21\u68c0\u7d22\u8f85\u52a9\u751f\u6210\u6a21\u578b\u7684\u6548\u679c\u548c\u6548\u7387\uff0c\u5728\u591a\u9886\u57df\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u5177\u6709\u826f\u597d\u7684\u8fc1\u79fb\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\u5728\u589e\u52a0\u68c0\u7d22\u5019\u9009\u6570\u91cf\u65f6\u5bb9\u6613\u53d7\u65e0\u5173\u6216\u5197\u4f59\u4fe1\u606f\u5f71\u54cd\uff0c\u5bfc\u81f4\u6027\u80fd\u548c\u6548\u7387\u4e0b\u964d\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u591f\u6709\u6548\u7b5b\u9009\u5e76\u5229\u7528\u5927\u89c4\u6a21\u68c0\u7d22\u8bc1\u636e\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4e09\u90e8\u5206\u7ec4\u6210\u7684\u6f5c\u5728\u5411\u91cf\u878d\u5408\u67b6\u6784\uff0c\u5305\u62ec\u4e0a\u4e0b\u6587\u7f16\u7801\u5668\u3001\u95e8\u63a7\u8fc7\u6ee4\u5668\u5bf9token\u8fdb\u884c\u52a0\u6743\u4ee5\u53catoken\u878d\u5408\u6a21\u5757\u5c06\u52a0\u6743\u7279\u5f81\u878d\u5165\u8bed\u8a00\u6a21\u578b\u7684\u9690\u85cf\u72b6\u6001\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86ReFilter\uff0c\u4e00\u79cd\u57fa\u4e8e\u6f5c\u5728\u5411\u91cf\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u7740\u91cd\u4e8e\u5bf9\u68c0\u7d22\u5230\u7684\u8bc1\u636e\u8fdb\u884c token \u7ea7\u522b\u7684\u7b5b\u9009\u548c\u878d\u5408\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u68c0\u7d22\u5019\u9009\u589e\u52a0\u65f6\u6548\u679c\u4e0b\u964d\u548c\u63a8\u7406\u6210\u672c\u589e\u52a0\u7684\u95ee\u9898\u3002ReFilter\u5305\u542b\u4e0a\u4e0b\u6587\u7f16\u7801\u5668\u3001\u95e8\u63a7\u8fc7\u6ee4\u5668\u548ctoken\u878d\u5408\u6a21\u5757\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cReFilter\u5728\u56db\u4e2a\u901a\u7528\u9886\u57df\u95ee\u7b54\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u4f73\u8868\u73b0\uff0c\u540c\u65f6\u5728\u751f\u7269\u533b\u5b66\u95ee\u7b54\u57fa\u51c6\u7684\u96f6\u6837\u672c\u8fc1\u79fb\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "ReFilter\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u68c0\u7d22\u4e0b\u7684\u5197\u4f59\u548c\u65e0\u5173\u4fe1\u606f\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u77e5\u8bc6\u5bc6\u96c6\u578b\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u8bc1\u660e\u4e86token\u7ea7\u878d\u5408\u7b56\u7565\u7684\u4f18\u52bf\u3002"}}
{"id": "2602.12746", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.12746", "abs": "https://arxiv.org/abs/2602.12746", "authors": ["Jing Xu", "Minglin Wu", "Xueyuan Chen", "Xixin Wu", "Helen Meng"], "title": "Lamer-SSL: Layer-aware Mixture of LoRA Experts for Continual Multilingual Expansion of Self-supervised Models without Forgetting", "comment": "Accepted by ICASSP 2026", "summary": "Despite their impressive performance, self-supervised speech models often struggle to generalize to new languages and tend to forget previously acquired knowledge during continual training. To address this, we propose Lamer-SSL, a parameter-efficient framework that integrates a Layer-Aware MixturE of LoRA Experts (Lamer) module with a replay strategy. The Lamer module enables flexible balancing between shared and language-specific representations, while layer-aware expert allocation assigns more experts to deeper layers where semantic information is richer. Meanwhile, the replay strategy retains prior knowledge using minimal data, mitigating forgetting during continual training. Experiments on automatic speech recognition (ASR) and language identification (LID) demonstrate that Lamer-SSL extends self-supervised models to new languages effectively while maintaining strong performance on previously learned languages with only 2.14% parameters being trainable.", "AI": {"tldr": "Lamer-SSL\u901a\u8fc7\u5c42\u611f\u77e5LoRA\u4e13\u5bb6\u6a21\u5757\u548c\u91cd\u653e\u7b56\u7565\uff0c\u63d0\u9ad8\u4e86\u81ea\u76d1\u7763\u8bed\u97f3\u6a21\u578b\u8de8\u8bed\u8a00\u6cdb\u5316\u548c\u6301\u7eed\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\uff0c\u4e14\u53c2\u6570\u9ad8\u6548\u3002", "motivation": "\u81ea\u76d1\u7763\u8bed\u97f3\u6a21\u578b\u867d\u7136\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u65b0\u8bed\u8a00\u6cdb\u5316\u80fd\u529b\u8f83\u5f31\u4e14\u5728\u6301\u7eed\u8bad\u7ec3\u4e2d\u5bb9\u6613\u9057\u5fd8\u5df2\u5b66\u77e5\u8bc6\uff0c\u4e9f\u9700\u4e00\u79cd\u673a\u5236\u6709\u6548\u6539\u5584\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86Lamer-SSL\uff0c\u4e00\u4e2a\u53c2\u6570\u9ad8\u6548\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u5c42\u611f\u77e5\u6df7\u5408LoRA\u4e13\u5bb6\u6a21\u5757(Lamer)\u548c\u91cd\u653e\u7b56\u7565\u3002Lamer\u6a21\u5757\u5728\u5171\u4eab\u4e0e\u8bed\u8a00\u7279\u5f02\u6027\u8868\u793a\u4e4b\u95f4\u7075\u6d3b\u5e73\u8861\uff0c\u901a\u8fc7\u5c42\u611f\u77e5\u4e13\u5bb6\u5206\u914d\u5411\u66f4\u6df1\u5c42\u5206\u914d\u66f4\u591a\u4e13\u5bb6\u3002\u91cd\u653e\u7b56\u7565\u901a\u8fc7\u5c11\u91cf\u6570\u636e\u4fdd\u7559\u5148\u524d\u77e5\u8bc6\uff0c\u51cf\u5c11\u6301\u7eed\u8bad\u7ec3\u4e2d\u7684\u9057\u5fd8\u3002", "result": "\u5728\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u548c\u8bed\u8a00\u8bc6\u522b\u4efb\u52a1\u4e0a\uff0cLamer-SSL\u4ec5\u75282.14%\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u5c31\u6210\u529f\u6269\u5c55\u5230\u65b0\u8bed\u8a00\uff0c\u540c\u65f6\u7ef4\u6301\u5bf9\u5df2\u5b66\u8bed\u8a00\u7684\u826f\u597d\u6027\u80fd\u3002", "conclusion": "Lamer-SSL\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u76d1\u7763\u8bed\u97f3\u6a21\u578b\u5728\u65b0\u8bed\u8a00\u6cdb\u5316\u548c\u6301\u7eed\u8bad\u7ec3\u4e2d\u9057\u5fd8\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5bf9\u65b0\u8bed\u8a00\u7684\u826f\u597d\u6269\u5c55\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u5df2\u5b66\u8bed\u8a00\u7684\u5f3a\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2602.12759", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12759", "abs": "https://arxiv.org/abs/2602.12759", "authors": ["Elena Alvarez-Mellado", "Julio Gonzalo"], "title": "Towards a Diagnostic and Predictive Evaluation Methodology for Sequence Labeling Tasks", "comment": "Accepted at LREC 2026", "summary": "Standard evaluation in NLP typically indicates that system A is better on average than system B, but it provides little info on how to improve performance and, what is worse, it should not come as a surprise if B ends up being better than A on outside data. We propose an evaluation methodology for sequence labeling tasks grounded on error analysis that provides both quantitative and qualitative information on where systems must be improved and predicts how models will perform on a different distribution. The key is to create test sets that, contrary to common practice, do not rely on gathering large amounts of real-world in-distribution scraped data, but consists in handcrafting a small set of linguistically motivated examples that exhaustively cover the range of span attributes (such as shape, length, casing, sentence position, etc.) a system may encounter in the wild. We demonstrate this methodology on a benchmark for anglicism identification in Spanish. Our methodology provides results that are diagnostic (because they help identify systematic weaknesses in performance), actionable (because they can inform which model is better suited for a given scenario) and predictive: our method predicts model performance on external datasets with a median correlation of 0.85.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9519\u8bef\u5206\u6790\u7684\u5c0f\u6837\u672c\u624b\u5de5\u8bbe\u8ba1\u6d4b\u8bd5\u96c6\u8bc4\u6d4b\u65b9\u6cd5\uff0c\u80fd\u8bca\u65ad\u6a21\u578b\u5f31\u70b9\u5e76\u9ad8\u6548\u9884\u6d4b\u8de8\u5206\u5e03\u8868\u73b0\u3002", "motivation": "\u4f20\u7edfNLP\u8bc4\u6d4b\u4ec5\u663e\u793a\u5e73\u5747\u6027\u80fd\u4f18\u52a3\uff0c\u96be\u4ee5\u6307\u5bfc\u6539\u8fdb\u4e14\u96be\u4ee5\u9884\u6d4b\u6a21\u578b\u5728\u5f02\u5206\u5e03\u6570\u636e\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u624b\u5de5\u8bbe\u8ba1\u6db5\u76d6\u5404\u7c7b\u7ed3\u6784\u5c5e\u6027\u7684\u5c0f\u89c4\u6a21\u8bed\u8a00\u5b66\u9a71\u52a8\u6d4b\u8bd5\u96c6\uff0c\u66ff\u4ee3\u4f20\u7edf\u5927\u89c4\u6a21\u722c\u53d6\u540c\u5206\u5e03\u6570\u636e\uff0c\u8fdb\u884c\u5b9a\u91cf\u548c\u5b9a\u6027\u5206\u6790\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u897f\u73ed\u7259\u8bed\u82f1\u5f0f\u8bcd\u8bc6\u522b\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8bca\u65ad\u6027\u3001\u53ef\u64cd\u4f5c\u6027\u548c\u9884\u6d4b\u6027\uff0c\u9884\u6d4b\u5916\u90e8\u6570\u636e\u96c6\u6027\u80fd\u7684\u4e2d\u4f4d\u76f8\u5173\u8fbe0.85\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u57fa\u4e8e\u9519\u8bef\u5206\u6790\u7684\u8bc4\u4f30\u65b9\u6cd5\u53ef\u4ee5\u66f4\u597d\u5730\u8bca\u65ad\u6a21\u578b\u5f31\u70b9\uff0c\u6307\u5bfc\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u51c6\u786e\u9884\u6d4b\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u5206\u5e03\u4e0a\u7684\u8868\u73b0\u3002"}}
{"id": "2602.12778", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12778", "abs": "https://arxiv.org/abs/2602.12778", "authors": ["Hamidreza Kazemi Taskooh", "Taha Zare Harofte"], "title": "Aspect-Based Sentiment Analysis for Future Tourism Experiences: A BERT-MoE Framework for Persian User Reviews", "comment": "25 pages, 12 figures, 4 tables", "summary": "This study advances aspect-based sentiment analysis (ABSA) for Persian-language user reviews in the tourism domain, addressing challenges of low-resource languages. We propose a hybrid BERT-based model with Top-K routing and auxiliary losses to mitigate routing collapse and improve efficiency. The pipeline includes: (1) overall sentiment classification using BERT on 9,558 labeled reviews, (2) multi-label aspect extraction for six tourism-related aspects (host, price, location, amenities, cleanliness, connectivity), and (3) integrated ABSA with dynamic routing. The dataset consists of 58,473 preprocessed reviews from the Iranian accommodation platform Jabama, manually annotated for aspects and sentiments. The proposed model achieves a weighted F1-score of 90.6% for ABSA, outperforming baseline BERT (89.25%) and a standard hybrid approach (85.7%). Key efficiency gains include a 39% reduction in GPU power consumption compared to dense BERT, supporting sustainable AI deployment in alignment with UN SDGs 9 and 12. Analysis reveals high mention rates for cleanliness and amenities as critical aspects. This is the first ABSA study focused on Persian tourism reviews, and we release the annotated dataset to facilitate future multilingual NLP research in tourism.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u6ce2\u65af\u8bed\u65c5\u6e38\u70b9\u8bc4\u7684\u57fa\u4e8eBERT\u7684\u6df7\u5408\u6a21\u578b\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u65b9\u9762\u7ea7\u60c5\u611f\u5206\u6790\u3002", "motivation": "\u9488\u5bf9\u4f4e\u8d44\u6e90\u7684\u6ce2\u65af\u8bed\u65c5\u6e38\u70b9\u8bc4\u6570\u636e\uff0c\u63d0\u5347\u65b9\u9762\u7ea7\u60c5\u611f\u5206\u6790\u7684\u51c6\u786e\u7387\u548c\u6548\u7387\uff0c\u652f\u6301\u591a\u8bed\u8a00\u65c5\u6e38NLP\u7814\u7a76\u3002", "method": "\u91c7\u7528\u5e26Top-K\u8def\u7531\u548c\u8f85\u52a9\u635f\u5931\u7684\u6df7\u5408BERT\u6a21\u578b\uff0c\u5305\u542b\u6574\u4f53\u60c5\u611f\u5206\u7c7b\u3001\u591a\u6807\u7b7e\u65b9\u9762\u62bd\u53d6\u548c\u52a8\u6001\u8def\u7531\uff0c\u5904\u7406\u6ce2\u65af\u8bed\u65c5\u6e38\u70b9\u8bc4\u6570\u636e\u3002", "result": "\u6a21\u578b\u5b9e\u73b0\u4e8690.6%\u7684\u52a0\u6743F1-score\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86GPU\u529f\u8017\uff0c\u5e76\u63d0\u4f9b\u4e86\u9996\u4e2a\u6807\u6ce8\u7684\u6ce2\u65af\u65c5\u6e38\u70b9\u8bc4\u6570\u636e\u96c6\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408BERT\u6a21\u578b\u5728\u6ce2\u65af\u8bed\u65c5\u6e38\u70b9\u8bc4ABSA\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cF1\u503c\u8fbe90.6%\uff0c\u4e14\u51cf\u5c11\u4e8639%\u7684GPU\u80fd\u8017\uff0c\u4fc3\u8fdb\u4e86\u53ef\u6301\u7eedAI\u53d1\u5c55\u3002"}}
{"id": "2602.12806", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12806", "abs": "https://arxiv.org/abs/2602.12806", "authors": ["Nata\u0161a Kr\u010do", "Zexi Yao", "Matthieu Meeus", "Yves-Alexandre de Montjoye"], "title": "RAT-Bench: A Comprehensive Benchmark for Text Anonymization", "comment": null, "summary": "Data containing personal information is increasingly used to train, fine-tune, or query Large Language Models (LLMs). Text is typically scrubbed of identifying information prior to use, often with tools such as Microsoft's Presidio or Anthropic's PII purifier. These tools have traditionally been evaluated on their ability to remove specific identifiers (e.g., names), yet their effectiveness at preventing re-identification remains unclear. We introduce RAT-Bench, a comprehensive benchmark for text anonymization tools based on re-identification risk. Using U.S. demographic statistics, we generate synthetic text containing various direct and indirect identifiers across domains, languages, and difficulty levels. We evaluate a range of NER- and LLM-based text anonymization tools and, based on the attributes an LLM-based attacker is able to correctly infer from the anonymized text, we report the risk of re-identification in the U.S. population, while properly accounting for the disparate impact of identifiers. We find that, while capabilities vary widely, even the best tools are far from perfect in particular when direct identifiers are not written in standard ways and when indirect identifiers enable re-identification. Overall we find LLM-based anonymizers, including new iterative anonymizers, to provide a better privacy-utility trade-off albeit at a higher computational cost. Importantly, we also find them to work well across languages. We conclude with recommendations for future anonymization tools and will release the benchmark and encourage community efforts to expand it, in particular to other geographies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRAT-Bench\u57fa\u51c6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u6587\u672c\u533f\u540d\u5316\u5de5\u5177\u5728\u518d\u8bc6\u522b\u98ce\u9669\u4e0a\u7684\u8868\u73b0\uff0c\u8868\u660e\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u533f\u540d\u5316\u5de5\u5177\u5728\u4fdd\u62a4\u9690\u79c1\u548c\u591a\u8bed\u8a00\u652f\u6301\u4e0a\u66f4\u4f18\uff0c\u4f46\u5c1a\u672a\u5b8c\u7f8e\u3002", "motivation": "\u867d\u7136\u5df2\u6709\u5de5\u5177\u80fd\u53bb\u9664\u7279\u5b9a\u8bc6\u522b\u4fe1\u606f\uff0c\u4f46\u5176\u9632\u6b62\u901a\u8fc7\u6587\u672c\u5b9e\u73b0\u4e2a\u4eba\u518d\u8bc6\u522b\u7684\u6548\u679c\u5c1a\u4e0d\u660e\u786e\uff0c\u56e0\u6b64\u9700\u8981\u7efc\u5408\u8bc4\u4f30\u6587\u672c\u533f\u540d\u5316\u5de5\u5177\u5728\u771f\u5b9e\u518d\u8bc6\u522b\u98ce\u9669\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u57fa\u4e8e\u7f8e\u56fd\u4eba\u53e3\u7edf\u8ba1\u6570\u636e\uff0c\u751f\u6210\u542b\u6709\u76f4\u63a5\u548c\u95f4\u63a5\u8bc6\u522b\u4fe1\u606f\u7684\u5408\u6210\u6587\u672c\uff0c\u6db5\u76d6\u591a\u4e2a\u9886\u57df\u3001\u8bed\u8a00\u548c\u96be\u5ea6\u7b49\u7ea7\uff1b\u8bc4\u4f30\u4e86\u57fa\u4e8e\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u533f\u540d\u5316\u5de5\u5177\uff0c\u901a\u8fc7LLM\u653b\u51fb\u8005\u6210\u529f\u63a8\u65ad\u7684\u5c5e\u6027\u8861\u91cf\u518d\u8bc6\u522b\u98ce\u9669\u3002", "result": "RAT-Bench\u57fa\u51c6\u5448\u73b0\u51fa\u4e0d\u540c\u533f\u540d\u5316\u5de5\u5177\u5728\u53bb\u9664\u8bc6\u522b\u4fe1\u606f\u548c\u9632\u6b62\u518d\u8bc6\u522b\u98ce\u9669\u4e0a\u7684\u6548\u679c\u5dee\u5f02\uff0c\u5c55\u793a\u4e86LLM\u533f\u540d\u5316\u5de5\u5177\u5728\u9690\u79c1\u4fdd\u62a4\u548c\u8bed\u79cd\u9002\u5e94\u6027\u4e0a\u7684\u4f18\u52bf\uff0c\u4f46\u4ea6\u63ed\u793a\u4e86\u5f53\u524d\u6280\u672f\u7684\u4e0d\u8db3\u4e0e\u5c40\u9650\u3002", "conclusion": "\u73b0\u6709\u7684\u6587\u672c\u533f\u540d\u5316\u5de5\u5177\u5728\u9690\u79c1\u4fdd\u62a4\u65b9\u9762\u8868\u73b0\u53c2\u5dee\u4e0d\u9f50\uff0c\u5c24\u5176\u662f\u5f53\u76f4\u63a5\u8bc6\u522b\u4fe1\u606f\u8868\u8fbe\u975e\u6807\u51c6\u5316\u6216\u95f4\u63a5\u8bc6\u522b\u4fe1\u606f\u5b58\u5728\u65f6\uff0c\u518d\u8bc6\u522b\u98ce\u9669\u4f9d\u7136\u8f83\u9ad8\u3002\u57fa\u4e8eLLM\u7684\u533f\u540d\u5316\u5de5\u5177\u6574\u4f53\u4e0a\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u9690\u79c1-\u6548\u7528\u6743\u8861\uff0c\u5e76\u4e14\u8de8\u8bed\u8a00\u6548\u679c\u826f\u597d\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u8f83\u9ad8\u3002"}}
{"id": "2602.12811", "categories": ["cs.CL", "cs.AI", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2602.12811", "abs": "https://arxiv.org/abs/2602.12811", "authors": ["Laurent Bonnasse-Gahot", "Christophe Pallier"], "title": "Left-right asymmetry in predicting brain activity from LLMs' representations emerges with their formal linguistic competence", "comment": null, "summary": "When humans and large language models (LLMs) process the same text, activations in the LLMs correlate with brain activity measured, e.g., with functional magnetic resonance imaging (fMRI). Moreover, it has been shown that, as the training of an LLM progresses, the performance in predicting brain activity from its internal activations improves more in the left hemisphere than in the right one. The aim of the present work is to understand which kind of competence acquired by the LLMs underlies the emergence of this left-right asymmetry. Using the OLMo-2 7B language model at various training checkpoints and fMRI data from English participants, we compare the evolution of the left-right asymmetry in brain scores alongside performance on several benchmarks. We observe that the asymmetry co-emerges with the formal linguistic abilities of the LLM. These abilities are demonstrated in two ways: by the model's capacity to assign a higher probability to an acceptable sentence than to a grammatically unacceptable one within a minimal contrasting pair, or its ability to produce well-formed text. On the opposite, the left-right asymmetry does not correlate with the performance on arithmetic or Dyck language tasks; nor with text-based tasks involving world knowledge and reasoning. We generalize these results to another family of LLMs (Pythia) and another language, namely French. Our observations indicate that the left-right asymmetry in brain predictivity matches the progress in formal linguistic competence (knowledge of linguistic patterns).", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u53d1\u73b0\uff0c\u8bad\u7ec3\u4e2d\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u8111\u5de6\u53f3\u534a\u7403\u7684\u6fc0\u6d3b\u4e0d\u5bf9\u79f0\u6027\u4e3b\u8981\u6765\u6e90\u4e8e\u6a21\u578b\u5f62\u5f0f\u8bed\u8a00\u80fd\u529b\u7684\u53d1\u5c55\uff0c\u800c\u975e\u5176\u4ed6\u8ba4\u77e5\u4efb\u52a1\u80fd\u529b\u3002", "motivation": "\u63a2\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5176\u5185\u90e8\u6fc0\u6d3b\u4e0e\u4eba\u8111\u6d3b\u52a8\u4e2d\u7684\u5de6\u53f3\u534a\u7403\u4e0d\u5bf9\u79f0\u6027\u4ea7\u751f\u7684\u539f\u56e0\uff0c\u53ca\u5176\u5bf9\u5e94\u7684\u8ba4\u77e5\u80fd\u529b\u3002", "method": "\u5229\u7528\u591a\u4e2a\u8bad\u7ec3\u9636\u6bb5\u7684OLMo-2 7B\u6a21\u578b\u548c\u82f1\u8bed\u53d7\u8bd5\u8005\u7684fMRI\u6570\u636e\uff0c\u6bd4\u8f83\u5de6\u53f3\u8111\u6fc0\u6d3b\u4e0d\u5bf9\u79f0\u6027\u7684\u6f14\u53d8\u53ca\u5176\u4e0e\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u6210\u7ee9\u7684\u5173\u7cfb\uff0c\u540c\u65f6\u6269\u5c55\u5230\u4e86Pythia\u6a21\u578b\u548c\u6cd5\u8bed\u6570\u636e\u3002", "result": "\u53d1\u73b0\u5de6\u53f3\u534a\u7403\u4e0d\u5bf9\u79f0\u6027\u4e0e\u6a21\u578b\u5728\u533a\u5206\u8bed\u6cd5\u6b63\u786e\u4e0e\u9519\u8bef\u53e5\u5b50\u3001\u751f\u6210\u826f\u597d\u6587\u672c\u7684\u80fd\u529b\u76f8\u5173\uff0c\u800c\u4e0e\u7b97\u672f\u3001Dyck\u8bed\u8a00\u4efb\u52a1\u53ca\u4f9d\u8d56\u4e16\u754c\u77e5\u8bc6\u548c\u63a8\u7406\u7684\u6587\u672c\u4efb\u52a1\u65e0\u5173\u3002", "conclusion": "\u5de6\u8111\u548c\u53f3\u8111\u5728\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5904\u7406\u8bed\u8a00\u65f6\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u6fc0\u6d3b\u76f8\u5173\u6027\uff0c\u8fd9\u79cd\u5de6\u53f3\u4e0d\u5bf9\u79f0\u6027\u4e0e\u6a21\u578b\u6b63\u5f0f\u8bed\u8a00\u80fd\u529b\u7684\u8fdb\u5c55\u76f8\u5173\u8054\u3002"}}
{"id": "2602.12818", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12818", "abs": "https://arxiv.org/abs/2602.12818", "authors": ["Luca Tedeschini", "Matteo Fasulo"], "title": "AIWizards at MULTIPRIDE: A Hierarchical Approach to Slur Reclamation Detection", "comment": null, "summary": "Detecting reclaimed slurs represents a fundamental challenge for hate speech detection systems, as the same lexcal items can function either as abusive expressions or as in-group affirmations depending on social identity and context. In this work, we address Subtask B of the MultiPRIDE shared task at EVALITA 2026 by proposing a hierarchical approach to modeling the slur reclamation process. Our core assumption is that members of the LGBTQ+ community are more likely, on average, to employ certain slurs in a eclamatory manner. Based on this hypothesis, we decompose the task into two stages. First, using a weakly supervised LLM-based annotation, we assign fuzzy labels to users indicating the likelihood of belonging to the LGBTQ+ community, inferred from the tweet and the user bio. These soft labels are then used to train a BERT-like model to predict community membership, encouraging the model to learn latent representations associated with LGBTQ+ identity. In the second stage, we integrate this latent space with a newly initialized model for the downstream slur reclamation detection task. The intuition is that the first model encodes user-oriented sociolinguistic signals, which are then fused with representations learned by a model pretrained for hate speech detection. Experimental results on Italian and Spanish show that our approach achieves performance statistically comparable to a strong BERT-based baseline, while providing a modular and extensible framework for incorporating sociolinguistic context into hate speech modeling. We argue that more fine-grained hierarchical modeling of user identity and discourse context may further improve the detection of reclaimed language. We release our code at https://github.com/LucaTedeschini/multipride.", "AI": {"tldr": "\u672c\u6587\u4e3a\u591a\u8bed\u8a00\u4ec7\u6068\u8a00\u8bba\u4e2d\u8d2c\u4e49\u8bcd\u56de\u6536\u68c0\u6d4b\uff0c\u63d0\u51fa\u5206\u5c42\u6a21\u578b\u5229\u7528\u7528\u6237LGBTQ+\u8eab\u4efd\u4fe1\u53f7\uff0c\u4e0e\u4f20\u7edf\u65b9\u6cd5\u6027\u80fd\u76f8\u5f53\u4e14\u66f4\u5177\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u8bc6\u522b\u88ab\u56de\u6536\u7684\u8d2c\u4e49\u8bcd\u5bf9\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u7cfb\u7edf\u662f\u91cd\u5927\u6311\u6218\uff0c\u56e0\u4e3a\u76f8\u540c\u8bcd\u6c47\u6839\u636e\u793e\u4f1a\u8eab\u4efd\u548c\u8bed\u5883\u53ef\u80fd\u662f\u653b\u51fb\u6027\u6216\u7fa4\u4f53\u8ba4\u540c\u7684\u8868\u8fbe\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u65b9\u6cd5\uff0c\u9996\u5148\u901a\u8fc7\u5f31\u76d1\u7763\u5927\u8bed\u8a00\u6a21\u578b\u6ce8\u91ca\u63a8\u65ad\u7528\u6237LGBTQ+\u793e\u533a\u5f52\u5c5e\u7684\u6a21\u7cca\u6807\u7b7e\uff0c\u4f7f\u7528BERT\u6a21\u578b\u9884\u6d4b\u793e\u533a\u8eab\u4efd\uff1b\u7136\u540e\u5c06\u8be5\u6f5c\u5728\u7a7a\u95f4\u4e0e\u65b0\u7684\u6a21\u578b\u878d\u5408\u8fdb\u884c\u8d2c\u4e49\u8bcd\u56de\u6536\u68c0\u6d4b\u3002", "result": "\u5728\u610f\u5927\u5229\u8bed\u548c\u897f\u73ed\u7259\u8bed\u6570\u636e\u96c6\u4e0a\uff0c\u65b9\u6cd5\u6027\u80fd\u4e0e\u5f3a\u57fa\u7ebfBERT\u6a21\u578b\u76f8\u5f53\uff0c\u63d0\u4f9b\u4e86\u7075\u6d3b\u53ef\u6269\u5c55\u6846\u67b6\u4ee5\u878d\u5408\u793e\u4f1a\u8bed\u8a00\u5b66\u80cc\u666f\u3002", "conclusion": "\u57fa\u4e8e\u5c42\u6b21\u5316\u7528\u6237\u8eab\u4efd\u548c\u8bdd\u8bed\u8bed\u5883\u7684\u5efa\u6a21\u65b9\u6cd5\u80fd\u63d0\u5347\u56de\u6536\u8d2c\u4e49\u8bcd\u68c0\u6d4b\uff0c\u7ed3\u5408\u793e\u4f1a\u8bed\u8a00\u5b66\u4fe1\u53f7\u662f\u672a\u6765\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2602.12871", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12871", "abs": "https://arxiv.org/abs/2602.12871", "authors": ["Hoyun Song", "Migyeong Kang", "Jisu Shin", "Jihyun Kim", "Chanbi Park", "Hangyeol Yoo", "Jihyun An", "Alice Oh", "Jinyoung Han", "KyungTae Lim"], "title": "MentalBench: A Benchmark for Evaluating Psychiatric Diagnostic Capability of Large Language Models", "comment": null, "summary": "We introduce MentalBench, a benchmark for evaluating psychiatric diagnostic decision-making in large language models (LLMs). Existing mental health benchmarks largely rely on social media data, limiting their ability to assess DSM-grounded diagnostic judgments. At the core of MentalBench is MentalKG, a psychiatrist-built and validated knowledge graph encoding DSM-5 diagnostic criteria and differential diagnostic rules for 23 psychiatric disorders. Using MentalKG as a golden-standard logical backbone, we generate 24,750 synthetic clinical cases that systematically vary in information completeness and diagnostic complexity, enabling low-noise and interpretable evaluation. Our experiments show that while state-of-the-art LLMs perform well on structured queries probing DSM-5 knowledge, they struggle to calibrate confidence in diagnostic decision-making when distinguishing between clinically overlapping disorders. These findings reveal evaluation gaps not captured by existing benchmarks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eDSM-5\u77e5\u8bc6\u56fe\u7684\u7cbe\u795e\u75c5\u8bca\u65ad\u8bc4\u4f30\u57fa\u51c6\uff0c\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u8bca\u65ad\u4efb\u52a1\u4e2d\u5b58\u5728\u663e\u8457\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u7684\u7cbe\u795e\u5065\u5eb7\u8bc4\u4f30\u57fa\u51c6\u5927\u591a\u4f9d\u8d56\u793e\u4ea4\u5a92\u4f53\u6570\u636e\uff0c\u65e0\u6cd5\u6709\u6548\u8bc4\u4f30\u57fa\u4e8eDSM-5\u7684\u7cbe\u795e\u75be\u75c5\u8bca\u65ad\u5224\u65ad\u3002", "method": "\u6784\u5efa\u4e86MentalBench\u57fa\u51c6\uff0c\u6838\u5fc3\u4e3a\u7531\u7cbe\u795e\u79d1\u533b\u751f\u6784\u5efa\u5e76\u9a8c\u8bc1\u7684MentalKG\u77e5\u8bc6\u56fe\uff0c\u7f16\u780123\u79cd\u7cbe\u795e\u75be\u75c5\u7684DSM-5\u8bca\u65ad\u6807\u51c6\u548c\u9274\u522b\u8bca\u65ad\u89c4\u5219\u3002\u57fa\u4e8eMentalKG\u751f\u621024,750\u4efd\u591a\u6837\u5316\u7684\u5408\u6210\u4e34\u5e8a\u75c5\u4f8b\uff0c\u4ee5\u5b9e\u73b0\u4f4e\u566a\u58f0\u4e14\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7ed3\u6784\u5316\u67e5\u8be2\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u533a\u5206\u4e34\u5e8a\u91cd\u53e0\u75be\u75c5\u65f6\u96be\u4ee5\u8c03\u6574\u8bca\u65ad\u51b3\u7b56\u7684\u7f6e\u4fe1\u5ea6\u3002\u8be5\u8bc4\u4f30\u63ed\u793a\u4e86\u73b0\u6709\u57fa\u51c6\u672a\u8986\u76d6\u7684\u4e0d\u8db3\u4e4b\u5904\u3002", "conclusion": "MentalBench\u63ed\u793a\u4e86\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7cbe\u795e\u75c5\u8bca\u65ad\u63a8\u7406\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u4e34\u5e8a\u5224\u65ad\u4e2d\u7684\u7f6e\u4fe1\u5ea6\u6821\u51c6\u95ee\u9898\uff0c\u63d0\u793a\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2602.12889", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12889", "abs": "https://arxiv.org/abs/2602.12889", "authors": ["Jiangxi Chen", "Qian Liu"], "title": "BaziQA-Benchmark: Evaluating Symbolic and Temporally Compositional Reasoning in Large Language Models", "comment": null, "summary": "We present BaziQA-Benchmark, a standardized benchmark for evaluating symbolic and temporally compositional reasoning in large language models. The benchmark is derived from 200 professionally curated, multiple-choice problems from the Global Fortune-teller Competition (2021--2025), where each instance requires structured inference over a fixed symbolic chart and interacting temporal conditions. Unlike anecdotal or prompt-driven evaluations, BaziQA-Benchmark enables objective scoring and controlled comparison across years, domains, and model families. We evaluate contemporary language models under a multi-turn setting and analyze performance variation across temporal difficulty, reasoning domains, and inference protocols.To further probe reasoning behavior, we introduce a lightweight Structured Reasoning Protocol that constrains inference order without adding domain knowledge. Results show that models consistently outperform chance but remain far from saturation, exhibiting pronounced sensitivity to temporal composition and reasoning order, as well as systematic failures on precise temporal localization and multi-condition symbolic judgments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faBaziQA-Benchmark\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7cfb\u7edf\u8bc4\u6d4b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7b26\u53f7\u4e0e\u65f6\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u6a21\u578b\u8868\u73b0\u6709\u63d0\u5347\u7a7a\u95f4\u3002", "motivation": "\u4e3a\u4e86\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7b26\u53f7\u63a8\u7406\u548c\u65f6\u95f4\u7ec4\u5408\u63a8\u7406\u4e0a\u7684\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u6807\u51c6\u5316\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u57fa\u4e8e\u5168\u7403\u7b97\u547d\u5e08\u5927\u8d5b\uff082021-2025\u5e74\uff09\u4e13\u4e1a\u7b56\u5212\u7684200\u4e2a\u591a\u9879\u9009\u62e9\u95ee\u9898\uff0c\u8bbe\u8ba1\u4e86BaziQA-Benchmark\u3002\u91c7\u7528\u591a\u8f6e\u63a8\u7406\u8bbe\u7f6e\uff0c\u5e76\u5f15\u5165\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7ed3\u6784\u5316\u63a8\u7406\u534f\u8bae\u4ee5\u7ea6\u675f\u63a8\u7406\u987a\u5e8f\u3002", "result": "\u6a21\u578b\u7684\u8868\u73b0\u660e\u663e\u4f18\u4e8e\u968f\u673a\u731c\u6d4b\uff0c\u4f46\u8ddd\u79bb\u5b8c\u5168\u638c\u63e1\u8be5\u4efb\u52a1\u4ecd\u6709\u8f83\u5927\u5dee\u8ddd\u3002\u6a21\u578b\u5bf9\u65f6\u95f4\u7ec4\u5408\u548c\u63a8\u7406\u987a\u5e8f\u654f\u611f\uff0c\u5bf9\u65f6\u95f4\u5b9a\u4f4d\u53ca\u591a\u6761\u4ef6\u7b26\u53f7\u5224\u65ad\u5b58\u5728\u7cfb\u7edf\u6027\u5931\u8d25\u3002", "conclusion": "BaziQA-Benchmark\u6709\u6548\u63ed\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7b26\u53f7\u548c\u65f6\u95f4\u63a8\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u53ef\u63a7\u7684\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2602.12911", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12911", "abs": "https://arxiv.org/abs/2602.12911", "authors": ["Tung X. Nguyen", "Nhu Vo", "Giang-Son Nguyen", "Duy Mai Hoang", "Chien Dinh Huynh", "Inigo Jauregi Unanue", "Massimo Piccardi", "Wray Buntine", "Dung D. Le"], "title": "ViMedCSS: A Vietnamese Medical Code-Switching Speech Dataset & Benchmark", "comment": "Accepted at LREC 2026", "summary": "Code-switching (CS), which is when Vietnamese speech uses English words like drug names or procedures, is a common phenomenon in Vietnamese medical communication. This creates challenges for Automatic Speech Recognition (ASR) systems, especially in low-resource languages like Vietnamese. Current most ASR systems struggle to recognize correctly English medical terms within Vietnamese sentences, and no benchmark addresses this challenge. In this paper, we construct a 34-hour \\textbf{Vi}etnamese \\textbf{Med}ical \\textbf{C}ode-\\textbf{S}witching \\textbf{S}peech dataset (ViMedCSS) containing 16,576 utterances. Each utterance includes at least one English medical term drawn from a curated bilingual lexicon covering five medical topics. Using this dataset, we evaluate several state-of-the-art ASR models and examine different specific fine-tuning strategies for improving medical term recognition to investigate the best approach to solve in the dataset. Experimental results show that Vietnamese-optimized models perform better on general segments, while multilingual pretraining helps capture English insertions. The combination of both approaches yields the best balance between overall and code-switched accuracy. This work provides the first benchmark for Vietnamese medical code-switching and offers insights into effective domain adaptation for low-resource, multilingual ASR systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u5236\u4f5c\u4e86\u9996\u4e2a\u8d8a\u5357\u8bed\u533b\u7597\u4ee3\u7801\u5207\u6362\u8bed\u97f3\u6570\u636e\u96c6\uff0c\u8bc4\u6d4b\u4e86\u591a\u79cdASR\u6a21\u578b\uff0c\u53d1\u73b0\u7ed3\u5408\u8d8a\u5357\u8bed\u4f18\u5316\u548c\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u80fd\u66f4\u6709\u6548\u8bc6\u522b\u4ee3\u7801\u5207\u6362\u4e2d\u7684\u82f1\u8bed\u533b\u5b66\u672f\u8bed\u3002", "motivation": "\u8d8a\u5357\u8bed\u533b\u7597\u4ea4\u6d41\u4e2d\u5b58\u5728\u5927\u91cf\u7684\u82f1\u8bed\u533b\u5b66\u672f\u8bed\u63d2\u5165\uff0c\u5bfc\u81f4ASR\u7cfb\u7edf\u96be\u4ee5\u51c6\u786e\u8bc6\u522b\uff0c\u4e14\u7f3a\u4e4f\u4e13\u95e8\u9488\u5bf9\u8fd9\u7c7b\u73b0\u8c61\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b16,576\u6761\u8bed\u97f3\u7684\u8d8a\u5357\u8bed\u533b\u7597\u4ee3\u7801\u5207\u6362\u8bed\u97f3\u6570\u636e\u96c6\uff08ViMedCSS\uff09\uff0c\u5e76\u4f7f\u7528\u8be5\u6570\u636e\u96c6\u5bf9\u591a\u79cd\u5148\u8fdb\u7684\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u6a21\u578b\u53ca\u5176\u5fae\u8c03\u7b56\u7565\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u8d8a\u5357\u8bed\u4f18\u5316\u6a21\u578b\u5728\u666e\u901a\u8bed\u53e5\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u6709\u52a9\u4e8e\u8bc6\u522b\u82f1\u8bed\u63d2\u5165\u8bcd\uff0c\u4e24\u8005\u7ed3\u5408\u5728\u6574\u4f53\u53ca\u4ee3\u7801\u5207\u6362\u51c6\u786e\u7387\u4e0a\u8fbe\u5230\u6700\u4f73\u5e73\u8861\u3002", "conclusion": "\u672c\u5de5\u4f5c\u9996\u6b21\u4e3a\u8d8a\u5357\u8bed\u533b\u7597\u4ee3\u7801\u5207\u6362\u63d0\u4f9b\u57fa\u51c6\u6570\u636e\uff0c\u63ed\u793a\u4e86\u4f4e\u8d44\u6e90\u591a\u8bed\u8a00ASR\u7cfb\u7edf\u9886\u57df\u9002\u5e94\u7684\u6709\u6548\u7b56\u7565\uff0c\u5bf9\u63d0\u5347\u533b\u5b66\u9886\u57dfASR\u6027\u80fd\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2602.12921", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12921", "abs": "https://arxiv.org/abs/2602.12921", "authors": ["Adib Sakhawat", "Shamim Ara Parveen", "Md Ruhul Amin", "Shamim Al Mahmud", "Md Saiful Islam", "Tahera Khatun"], "title": "When Words Don't Mean What They Say: Figurative Understanding in Bengali Idioms", "comment": "9 pages, 5 figures. Accepted for presentation at LREC 2026 (Language Resources and Evaluation Conference)", "summary": "Figurative language understanding remains a significant challenge for Large Language Models (LLMs), especially for low-resource languages. To address this, we introduce a new idiom dataset, a large-scale, culturally-grounded corpus of 10,361 Bengali idioms. Each idiom is annotated under a comprehensive 19-field schema, established and refined through a deliberative expert consensus process, that captures its semantic, syntactic, cultural, and religious dimensions, providing a rich, structured resource for computational linguistics. To establish a robust benchmark for Bangla figurative language understanding, we evaluate 30 state-of-the-art multilingual and instruction-tuned LLMs on the task of inferring figurative meaning. Our results reveal a critical performance gap, with no model surpassing 50% accuracy, a stark contrast to significantly higher human performance (83.4%). This underscores the limitations of existing models in cross-linguistic and cultural reasoning. By releasing the new idiom dataset and benchmark, we provide foundational infrastructure for advancing figurative language understanding and cultural grounding in LLMs for Bengali and other low-resource languages.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u5b5f\u52a0\u62c9\u8bed\u4e2d\u7684\u9690\u55bb\u8bed\u8a00\u7406\u89e3\u95ee\u9898\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b10,361\u6761\u5b5f\u52a0\u62c9\u8bed\u6210\u8bed\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5e76\u8bbe\u8ba1\u4e8619\u7ef4\u6ce8\u91ca\u65b9\u6848\u3002\u901a\u8fc7\u6d4b\u8bd530\u4e2a\u5148\u8fdb\u591a\u8bed\u8a00\u548c\u6307\u4ee4\u8c03\u4f18\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u53d1\u73b0\u6a21\u578b\u6027\u80fd\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\uff0c\u663e\u793a\u73b0\u6709\u6a21\u578b\u5728\u8de8\u8bed\u8a00\u6587\u5316\u63a8\u7406\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u9690\u55bb\u8bed\u8a00\u7406\u89e3\u5bf9\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u4f9d\u65e7\u662f\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u3002\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u5927\u89c4\u6a21\u7ed3\u6784\u5316\u7684\u6210\u8bed\u6570\u636e\u96c6\u548c\u8bc4\u6d4b\u57fa\u51c6\uff0c\u4fc3\u8fdb\u5b5f\u52a0\u62c9\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u9690\u55bb\u53ca\u6587\u5316\u7406\u89e3\u7684\u7814\u7a76\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u6db5\u76d610,361\u6761\u5b5f\u52a0\u62c9\u8bed\u6210\u8bed\u7684\u8bed\u6599\u5e93\uff0c\u91c7\u7528\u4e13\u5bb6\u5171\u8bc6\u5236\u5b9a\u768419\u5b57\u6bb5\u6ce8\u91ca\u65b9\u6848\uff0c\u6db5\u76d6\u8bed\u4e49\u3001\u53e5\u6cd5\u3001\u6587\u5316\u53ca\u5b97\u6559\u7b49\u7ef4\u5ea6\uff0c\u5e76\u5bf930\u4e2a\u5148\u8fdb\u591a\u8bed\u8a00\u53ca\u6307\u4ee4\u8c03\u4f18\u578b\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u9690\u55bb\u610f\u4e49\u63a8\u65ad\u4efb\u52a1\u7684\u8bc4\u6d4b\u3002", "result": "\u53d1\u5e03\u4e86\u4e00\u4e2a\u5305\u542b10,361\u6761\u5b5f\u52a0\u62c9\u8bed\u6210\u8bed\u7684\u5168\u9762\u6ce8\u91ca\u6570\u636e\u96c6\uff0c\u5e76\u5efa\u7acb\u4e86\u7b2c\u4e00\u4e2a\u5b5f\u52a0\u62c9\u8bed\u9690\u55bb\u7406\u89e3\u7684\u8bc4\u6d4b\u57fa\u51c6\u3002\u8bc4\u6d4b\u7ed3\u679c\u8868\u660e\uff0c\u6240\u6709\u6a21\u578b\u51c6\u786e\u7387\u5747\u4f4e\u4e8e50%\uff0c\u663e\u8457\u843d\u540e\u4e8e\u4eba\u7c7b\u8868\u73b0\u3002", "conclusion": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5b5f\u52a0\u62c9\u8bed\u9690\u55bb\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u51c6\u786e\u7387\u5747\u672a\u8d85\u8fc750%\uff0c\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u768483.4%\u3002\u9700\u8fdb\u4e00\u6b65\u52a0\u5f3a\u6587\u5316\u548c\u8bed\u4e49\u7684\u8de8\u8bed\u8a00\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2602.12937", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12937", "abs": "https://arxiv.org/abs/2602.12937", "authors": ["Ali Mekky", "Mohamed El Zeftawy", "Lara Hassan", "Amr Keleg", "Preslav Nakov"], "title": "Curriculum Learning and Pseudo-Labeling Improve the Generalization of Multi-Label Arabic Dialect Identification Models", "comment": "Accepted at the 12th Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial), co-located with EACL 2026", "summary": "Being modeled as a single-label classification task for a long time, recent work has argued that Arabic Dialect Identification (ADI) should be framed as a multi-label classification task. However, ADI remains constrained by the availability of single-label datasets, with no large-scale multi-label resources available for training. By analyzing models trained on single-label ADI data, we show that the main difficulty in repurposing such datasets for Multi-Label Arabic Dialect Identification (MLADI) lies in the selection of negative samples, as many sentences treated as negative could be acceptable in multiple dialects. To address these issues, we construct a multi-label dataset by generating automatic multi-label annotations using GPT-4o and binary dialect acceptability classifiers, with aggregation guided by the Arabic Level of Dialectness (ALDi). Afterward, we train a BERT-based multi-label classifier using curriculum learning strategies aligned with dialectal complexity and label cardinality. On the MLADI leaderboard, our best-performing LAHJATBERT model achieves a macro F1 of 0.69, compared to 0.55 for the strongest previously reported system. Code and data are available at https://mohamedalaa9.github.io/lahjatbert/.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u591a\u6807\u7b7e\u963f\u62c9\u4f2f\u65b9\u8a00\u8bc6\u522b\u4efb\u52a1\uff0c\u6784\u5efa\u4e86\u591a\u6807\u7b7e\u6570\u636e\u96c6\u5e76\u63d0\u51fa\u57fa\u4e8eBERT\u7684\u5206\u7c7b\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u963f\u62c9\u4f2f\u65b9\u8a00\u8bc6\u522b\uff08ADI\uff09\u4efb\u52a1\u957f\u671f\u88ab\u5efa\u6a21\u4e3a\u5355\u6807\u7b7e\u5206\u7c7b\u4efb\u52a1\uff0c\u4f46\u5b9e\u9645\u4e0a\u963f\u62c9\u4f2f\u65b9\u8a00\u7684\u8bc6\u522b\u66f4\u9002\u5408\u591a\u6807\u7b7e\u5206\u7c7b\uff0c\u4e14\u7f3a\u4e4f\u5927\u89c4\u6a21\u7684\u591a\u6807\u7b7e\u8bad\u7ec3\u6570\u636e\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5355\u6807\u7b7eADI\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u53d1\u73b0\u9009\u62e9\u8d1f\u6837\u672c\u662f\u4e3b\u8981\u6311\u6218\u3002\u4f7f\u7528GPT-4o\u548c\u4e8c\u5143\u65b9\u8a00\u53ef\u63a5\u53d7\u6027\u5206\u7c7b\u5668\u81ea\u52a8\u751f\u6210\u591a\u6807\u7b7e\u6ce8\u91ca\uff0c\u7ed3\u5408\u963f\u62c9\u4f2f\u65b9\u8a00\u5c42\u7ea7\uff08ALDi\uff09\u8fdb\u884c\u6570\u636e\u96c6\u6784\u5efa\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8eBERT\u7684\u591a\u6807\u7b7e\u5206\u7c7b\u5668\u548c\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u63d0\u51fa\u7684LAHJATBERT\u6a21\u578b\u5728\u591a\u6807\u7b7e\u963f\u62c9\u4f2f\u65b9\u8a00\u8bc6\u522b\u4efb\u52a1\u7684\u6392\u884c\u699c\u4e0a\u8fbe\u5230\u5b8fF1\u503c0.69\uff0c\u663e\u8457\u4f18\u4e8e\u4e4b\u524d\u6700\u5f3a\u7cfb\u7edf\u76840.55\u3002", "conclusion": "\u6784\u5efa\u591a\u6807\u7b7e\u963f\u62c9\u4f2f\u65b9\u8a00\u6570\u636e\u96c6\u5e76\u7ed3\u5408\u9002\u5f53\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u80fd\u663e\u8457\u63d0\u5347\u591a\u6807\u7b7e\u963f\u62c9\u4f2f\u65b9\u8a00\u8bc6\u522b\u7684\u6548\u679c\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.12984", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12984", "abs": "https://arxiv.org/abs/2602.12984", "authors": ["Yujiong Shen", "Yajie Yang", "Zhiheng Xi", "Binze Hu", "Huayu Sha", "Jiazheng Zhang", "Qiyuan Peng", "Junlin Shang", "Jixuan Huang", "Yutao Fan", "Jingqi Tong", "Shihan Dou", "Ming Zhang", "Lei Bai", "Zhenfei Yin", "Tao Gui", "Xingjun Ma", "Qi Zhang", "Xuanjing Huang", "Yu-Gang Jiang"], "title": "SciAgentGym: Benchmarking Multi-Step Scientific Tool-use in LLM Agents", "comment": null, "summary": "Scientific reasoning inherently demands integrating sophisticated toolkits to navigate domain-specific knowledge. Yet, current benchmarks largely overlook agents' ability to orchestrate tools for such rigorous workflows. To bridge this gap, we introduce SciAgentGym, a scalable interactive environment featuring 1,780 domain-specific tools across four natural science disciplines, supported by a robust execution infrastructure. Complementing this, we present SciAgentBench, a tiered evaluation suite designed to stress-test agentic capabilities from elementary actions to long-horizon workflows. Our evaluation identifies a critical bottleneck: state-of-the-art models struggle with complex scientific tool-use. Even for a leading model like GPT-5, success rates drop sharply from 60.6% to 30.9% as interaction horizons extend, primarily due to failures in multi-step workflow execution. To address this, we propose SciForge, a data synthesis method that models the tool action space as a dependency graph to generate logic-aware training trajectories. By fine-tuning on these trajectories, our SciAgent-8B outperforms the significantly larger Qwen3-VL-235B-Instruct while exhibiting positive cross-domain transfer of scientific tool-use capabilities. These results underscore the promising potential of next-generation autonomous scientific agents.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86SciAgentGym\u548cSciAgentBench\u4e24\u4e2a\u5de5\u5177\u73af\u5883\uff0c\u65e8\u5728\u8bc4\u4f30\u548c\u63d0\u5347\u79d1\u5b66\u63a8\u7406\u4e2d\u7684\u591a\u5de5\u5177\u534f\u540c\u4f7f\u7528\u80fd\u529b\u3002\u901a\u8fc7\u63d0\u51faSciForge\u65b9\u6cd5\u751f\u6210\u903b\u8f91\u611f\u77e5\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u590d\u6742\u591a\u6b65\u9aa4\u79d1\u5b66\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u8bc4\u6d4b\u57fa\u51c6\u5ffd\u89c6\u4e86\u667a\u80fd\u4f53\u6574\u5408\u548c\u8fd0\u7528\u591a\u79d1\u5b66\u9886\u57df\u5de5\u5177\u6267\u884c\u590d\u6742\u5de5\u4f5c\u6d41\u7684\u80fd\u529b\uff0c\u5bfc\u81f4\u73b0\u6709\u6a21\u578b\u5728\u957f\u671f\u3001\u591a\u6b65\u9aa4\u79d1\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u8db3\u3002", "method": "\u63d0\u51faSciForge\uff0c\u4e00\u79cd\u5c06\u5de5\u5177\u52a8\u4f5c\u7a7a\u95f4\u5efa\u6a21\u4e3a\u4f9d\u8d56\u56fe\u7684\u6570\u636e\u5408\u6210\u65b9\u6cd5\uff0c\u751f\u6210\u903b\u8f91\u611f\u77e5\u7684\u8bad\u7ec3\u8f68\u8ff9\u7528\u4ee5\u5fae\u8c03\u6a21\u578b\u63d0\u5347\u590d\u6742\u591a\u6b65\u9aa4\u4efb\u52a1\u8868\u73b0\u3002", "result": "\u5728SciAgentBench\u8bc4\u6d4b\u4e2d\uff0c\u9886\u5148\u6a21\u578b\u5982GPT-5\u7684\u4efb\u52a1\u6210\u529f\u7387\u968f\u7740\u4ea4\u4e92\u6b65\u9aa4\u589e\u52a0\u663e\u8457\u4e0b\u964d\uff0c\u753160.6%\u964d\u81f330.9%\u3002\u7ecf\u8fc7SciForge\u5fae\u8c03\u7684SciAgent-8B\u5728\u79d1\u5b66\u5de5\u5177\u4f7f\u7528\u4e0a\u8d85\u8d8a\u4e86\u89c4\u6a21\u8fdc\u5927\u7684Qwen3-VL-235B-Instruct\uff0c\u5e76\u5b9e\u73b0\u4e86\u8de8\u9886\u57df\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "\u73b0\u6709\u5148\u8fdb\u6a21\u578b\u5728\u590d\u6742\u79d1\u5b66\u5de5\u5177\u4f7f\u7528\u4e0a\u7684\u80fd\u529b\u53d7\u9650\uff0c\u800c\u901a\u8fc7SciForge\u751f\u6210\u4f9d\u8d56\u56fe\u5e76\u8fdb\u884c\u5fae\u8c03\uff0cSciAgent-8B\u6a21\u578b\u5728\u591a\u9886\u57df\u79d1\u5b66\u5de5\u5177\u4f7f\u7528\u4e2d\u8868\u73b0\u4f18\u4e8e\u66f4\u5927\u89c4\u6a21\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u4e0b\u4e00\u4ee3\u81ea\u4e3b\u79d1\u5b66\u667a\u80fd\u4f53\u7684\u6f5c\u529b\u3002"}}
{"id": "2602.12989", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12989", "abs": "https://arxiv.org/abs/2602.12989", "authors": ["Ma\u00ebl Houbre", "Florian Boudin", "Beatrice Daille"], "title": "Evaluating the Homogeneity of Keyphrase Prediction Models", "comment": "Accepted to LREC 2026", "summary": "Keyphrases which are useful in several NLP and IR applications are either extracted from text or predicted by generative models. Contrarily to keyphrase extraction approaches, keyphrase generation models can predict keyphrases that do not appear in a document's text called `absent keyphrases`. This ability means that keyphrase generation models can associate a document to a notion that is not explicitly mentioned in its text. Intuitively, this suggests that for two documents treating the same subjects, a keyphrase generation model is more likely to be homogeneous in their indexing i.e. predict the same keyphrase for both documents, regardless of those keyphrases appearing in their respective text or not; something a keyphrase extraction model would fail to do. Yet, homogeneity of keyphrase prediction models is not covered by current benchmarks. In this work, we introduce a method to evaluate the homogeneity of keyphrase prediction models and study if absent keyphrase generation capabilities actually help the model to be more homogeneous. To our surprise, we show that keyphrase extraction methods are competitive with generative models, and that the ability to generate absent keyphrases can actually have a negative impact on homogeneity. Our data, code and prompts are available on huggingface and github.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u8bc4\u4ef7\u5173\u952e\u77ed\u8bed\u9884\u6d4b\u6a21\u578b\u540c\u8d28\u6027\u7684\u65b0\u65b9\u6cd5\uff0c\u53d1\u73b0\u751f\u6210\u7f3a\u5931\u5173\u952e\u77ed\u8bed\u80fd\u529b\u53ef\u80fd\u964d\u4f4e\u540c\u8d28\u6027\uff0c\u63d0\u53d6\u65b9\u6cd5\u540c\u6837\u6709\u6548\u3002", "motivation": "\u5f53\u524d\u7684\u5173\u952e\u77ed\u8bed\u751f\u6210\u6a21\u578b\u53ef\u4ee5\u751f\u6210\u6587\u672c\u4e2d\u672a\u51fa\u73b0\u7684\u5173\u952e\u77ed\u8bed\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u672a\u8bc4\u4f30\u8fd9\u4e9b\u6a21\u578b\u7684\u540c\u8d28\u6027\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u8bc4\u4f30\u5173\u952e\u77ed\u8bed\u9884\u6d4b\u6a21\u578b\u540c\u8d28\u6027\u7684\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u751f\u6210\u7f3a\u5931\u5173\u952e\u77ed\u8bed\u80fd\u529b\u5bf9\u540c\u8d28\u6027\u7684\u5f71\u54cd\u3002", "result": "\u5173\u952e\u77ed\u8bed\u63d0\u53d6\u65b9\u6cd5\u4e0e\u751f\u6210\u6a21\u578b\u5728\u540c\u8d28\u6027\u4e0a\u5177\u6709\u7ade\u4e89\u529b\uff0c\u751f\u6210\u7f3a\u5931\u5173\u952e\u77ed\u8bed\u7684\u80fd\u529b\u53ef\u80fd\u8d1f\u9762\u5f71\u54cd\u540c\u8d28\u6027\u3002", "conclusion": "\u751f\u6210\u7f3a\u5931\u5173\u952e\u77ed\u8bed\u7684\u80fd\u529b\u5e76\u4e0d\u4e00\u5b9a\u63d0\u5347\u6a21\u578b\u540c\u8d28\u6027\uff0c\u5173\u952e\u77ed\u8bed\u63d0\u53d6\u65b9\u6cd5\u4ecd\u5177\u4f18\u52bf\u3002"}}
{"id": "2602.12996", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12996", "abs": "https://arxiv.org/abs/2602.12996", "authors": ["Hao Chen", "Ye He", "Yuchun Fan", "Yukun Yan", "Zhenghao Liu", "Qingfu Zhu", "Maosong Sun", "Wanxiang Che"], "title": "Know More, Know Clearer: A Meta-Cognitive Framework for Knowledge Augmentation in Large Language Models", "comment": null, "summary": "Knowledge augmentation has significantly enhanced the performance of Large Language Models (LLMs) in knowledge-intensive tasks. However, existing methods typically operate on the simplistic premise that model performance equates with internal knowledge, overlooking the knowledge-confidence gaps that lead to overconfident errors or uncertain truths. To bridge this gap, we propose a novel meta-cognitive framework for reliable knowledge augmentation via differentiated intervention and alignment. Our approach leverages internal cognitive signals to partition the knowledge space into mastered, confused, and missing regions, guiding targeted knowledge expansion. Furthermore, we introduce a cognitive consistency mechanism to synchronize subjective certainty with objective accuracy, ensuring calibrated knowledge boundaries. Extensive experiments demonstrate the our framework consistently outperforms strong baselines, validating its rationality in not only enhancing knowledge capabilities but also fostering cognitive behaviors that better distinguish knowns from unknowns.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5143\u8ba4\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u533a\u5206\u5e72\u9884\u548c\u8ba4\u77e5\u4e00\u81f4\u6027\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u77e5\u8bc6\u7a7a\u95f4\u7684\u7cbe\u51c6\u6269\u5c55\u548c\u6821\u51c6\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5728\u77e5\u8bc6\u5bc6\u96c6\u4efb\u52a1\u4e2d\u7684\u53ef\u9760\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7b80\u5355\u5c06\u6a21\u578b\u6027\u80fd\u7b49\u540c\u4e8e\u5185\u90e8\u77e5\u8bc6\uff0c\u5ffd\u89c6\u4e86\u77e5\u8bc6\u4e0e\u4fe1\u5fc3\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5bfc\u81f4\u6a21\u578b\u4ea7\u751f\u8fc7\u5ea6\u81ea\u4fe1\u7684\u9519\u8bef\u6216\u4e0d\u786e\u5b9a\u7684\u771f\u76f8\uff0c\u9700\u8981\u4e00\u4e2a\u673a\u5236\u6765\u6865\u63a5\u8fd9\u4e00\u9e3f\u6c9f\u3002", "method": "\u57fa\u4e8e\u5185\u90e8\u8ba4\u77e5\u4fe1\u53f7\uff0c\u5c06\u77e5\u8bc6\u7a7a\u95f4\u5212\u5206\u4e3a\u638c\u63e1\u533a\u3001\u56f0\u60d1\u533a\u548c\u7f3a\u5931\u533a\uff0c\u8fdb\u884c\u5dee\u5f02\u5316\u5e72\u9884\uff1b\u540c\u65f6\u5f15\u5165\u8ba4\u77e5\u4e00\u81f4\u6027\u673a\u5236\uff0c\u540c\u6b65\u4e3b\u89c2\u786e\u4fe1\u5ea6\u4e0e\u5ba2\u89c2\u51c6\u786e\u5ea6\uff0c\u5b9e\u73b0\u77e5\u8bc6\u754c\u9650\u6821\u51c6\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u589e\u5f3a\u77e5\u8bc6\u80fd\u529b\u548c\u63d0\u5347\u8ba4\u77e5\u884c\u4e3a\u65b9\u9762\u5747\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u5176\u5408\u7406\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e0d\u4ec5\u63d0\u5347\u4e86\u6a21\u578b\u7684\u77e5\u8bc6\u589e\u5f3a\u80fd\u529b\uff0c\u8fd8\u4fc3\u8fdb\u4e86\u6a21\u578b\u66f4\u597d\u5730\u533a\u5206\u5df2\u77e5\u4e0e\u672a\u77e5\u7684\u8ba4\u77e5\u884c\u4e3a\uff0c\u4ece\u800c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5\u3002"}}
{"id": "2602.13047", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13047", "abs": "https://arxiv.org/abs/2602.13047", "authors": ["Madhurananda Pahar", "Caitlin Illingworth", "Dorota Braun", "Bahman Mirheidari", "Lise Sproson", "Daniel Blackburn", "Heidi Christensen"], "title": "Can we trust AI to detect healthy multilingual English speakers among the cognitively impaired cohort in the UK? An investigation using real-world conversational speech", "comment": null, "summary": "Conversational speech often reveals early signs of cognitive decline, such as dementia and MCI. In the UK, one in four people belongs to an ethnic minority, and dementia prevalence is expected to rise most rapidly among Black and Asian communities. This study examines the trustworthiness of AI models, specifically the presence of bias, in detecting healthy multilingual English speakers among the cognitively impaired cohort, to make these tools clinically beneficial. For experiments, monolingual participants were recruited nationally (UK), and multilingual speakers were enrolled from four community centres in Sheffield and Bradford. In addition to a non-native English accent, multilinguals spoke Somali, Chinese, or South Asian languages, who were further divided into two Yorkshire accents (West and South) to challenge the efficiency of the AI tools thoroughly. Although ASR systems showed no significant bias across groups, classification and regression models using acoustic and linguistic features exhibited bias against multilingual speakers, particularly in memory, fluency, and reading tasks. This bias was more pronounced when models were trained on the publicly available DementiaBank dataset. Moreover, multilinguals were more likely to be misclassified as having cognitive decline. This study is the first of its kind to discover that, despite their strong overall performance, current AI models show bias against multilingual individuals from ethnic minority backgrounds in the UK, and they are also more likely to misclassify speakers with a certain accent (South Yorkshire) as living with a more severe cognitive decline. In this pilot study, we conclude that the existing AI tools are therefore not yet reliable for diagnostic use in these populations, and we aim to address this in future work by developing more generalisable, bias-mitigated models.", "AI": {"tldr": "\u672c\u7814\u7a76\u53d1\u73b0\u82f1\u56fd\u73b0\u6709AI\u8ba4\u77e5\u8870\u9000\u68c0\u6d4b\u6a21\u578b\u5bf9\u591a\u8bed\u79cd\u5c11\u6570\u65cf\u88d4\u5b58\u5728\u504f\u89c1\uff0c\u5bfc\u81f4\u8bef\u8bca\u98ce\u9669\u9ad8\uff0c\u63d0\u793a\u9700\u6539\u8fdb\u6a21\u578b\u4ee5\u63d0\u5347\u516c\u5e73\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u82f1\u56fd\u5c11\u6570\u65cf\u88d4\u4eba\u7fa4\u4e2d\u8ba4\u77e5\u969c\u788d\uff08\u5982\u75f4\u5446\u548c\u8f7b\u5ea6\u8ba4\u77e5\u969c\u788d\uff09\u53d1\u75c5\u7387\u4e0a\u5347\uff0c\u7814\u7a76\u65e8\u5728\u786e\u4fddAI\u6a21\u578b\u5bf9\u591a\u8bed\u79cd\u53ca\u4e0d\u540c\u53e3\u97f3\u7fa4\u4f53\u7684\u8bca\u65ad\u516c\u5e73\u6027\u548c\u4e34\u5e8a\u9002\u7528\u6027\u3002", "method": "\u901a\u8fc7\u62db\u52df\u82f1\u56fd\u5355\u8bed\u548c\u591a\u8bed\u79cd\u53c2\u4e0e\u8005\uff0c\u4f7f\u7528ASR\u7cfb\u7edf\u53ca\u5206\u7c7b\u548c\u56de\u5f52\u6a21\u578b\uff0c\u5206\u6790\u8bed\u97f3\u548c\u8bed\u8a00\u7279\u5f81\u5bf9\u8ba4\u77e5\u8870\u9000\u7684\u68c0\u6d4b\u6548\u7387\u53ca\u504f\u89c1\u60c5\u51b5\u3002", "result": "ASR\u7cfb\u7edf\u5bf9\u4e0d\u540c\u7fa4\u4f53\u65e0\u660e\u663e\u504f\u89c1\uff0c\u4f46\u5206\u7c7b\u548c\u56de\u5f52\u6a21\u578b\u5bf9\u591a\u8bed\u79cd\u5c24\u5176\u662f\u5e26\u6709\u7279\u5b9a\u53e3\u97f3\u7684\u591a\u8bed\u79cd\u7fa4\u4f53\u5b58\u5728\u660e\u663e\u504f\u89c1\uff0c\u591a\u8bed\u79cd\u66f4\u6613\u88ab\u8bef\u5224\u4e3a\u8ba4\u77e5\u8870\u9000\u60a3\u8005\u3002", "conclusion": "\u73b0\u6709\u7684AI\u6a21\u578b\u5728\u591a\u8bed\u79cd\u5c11\u6570\u65cf\u88d4\u4eba\u7fa4\u4e2d\u5b58\u5728\u663e\u8457\u504f\u89c1\uff0c\u5c24\u5176\u662f\u5728\u8ba4\u77e5\u80fd\u529b\u8bc4\u4f30\u4efb\u52a1\u4e2d\uff0c\u5bfc\u81f4\u8fd9\u4e9b\u6a21\u578b\u5f53\u524d\u5c1a\u4e0d\u9002\u5408\u7528\u4e8e\u4e34\u5e8a\u8bca\u65ad\u3002"}}
{"id": "2602.13059", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.13059", "abs": "https://arxiv.org/abs/2602.13059", "authors": ["Tejas Anvekar", "Junha Park", "Rajat Jha", "Devanshu Gupta", "Poojah Ganesan", "Puneeth Mathur", "Vivek Gupta"], "title": "TraceBack: Multi-Agent Decomposition for Fine-Grained Table Attribution", "comment": null, "summary": "Question answering (QA) over structured tables requires not only accurate answers but also transparency about which cells support them. Existing table QA systems rarely provide fine-grained attribution, so even correct answers often lack verifiable grounding, limiting trust in high-stakes settings. We address this with TraceBack, a modular multi-agent framework for scalable, cell-level attribution in single-table QA. TraceBack prunes tables to relevant rows and columns, decomposes questions into semantically coherent sub-questions, and aligns each answer span with its supporting cells, capturing both explicit and implicit evidence used in intermediate reasoning steps. To enable systematic evaluation, we release CITEBench, a benchmark with phrase-to-cell annotations drawn from ToTTo, FetaQA, and AITQA. We further propose FairScore, a reference-less metric that compares atomic facts derived from predicted cells and answers to estimate attribution precision and recall without human cell labels. Experiments show that TraceBack substantially outperforms strong baselines across datasets and granularities, while FairScore closely tracks human judgments and preserves relative method rankings, supporting interpretable and scalable evaluation of table-based QA.", "AI": {"tldr": "\u63d0\u51faTraceBack\u6846\u67b6\u5b9e\u73b0\u8868\u683c\u95ee\u7b54\u7684\u7ec6\u7c92\u5ea6\u5f52\u56e0\uff0c\u53d1\u5e03\u76f8\u5173\u6570\u636e\u96c6\u4e0e\u8bc4\u4ef7\u6307\u6807\uff0c\u663e\u8457\u63d0\u5347\u5f52\u56e0\u6027\u80fd\u5e76\u652f\u6301\u81ea\u52a8\u5316\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u8868\u683c\u95ee\u7b54\u7cfb\u7edf\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u5f52\u56e0\uff0c\u5bfc\u81f4\u7b54\u6848\u7f3a\u4e4f\u53ef\u9a8c\u8bc1\u7684\u652f\u6491\uff0c\u5f71\u54cd\u9ad8\u98ce\u9669\u573a\u666f\u7684\u4fe1\u4efb\u3002", "method": "\u63d0\u51fa\u4e86TraceBack\uff0c\u4e00\u4e2a\u6a21\u5757\u5316\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5b9e\u73b0\u5355\u8868\u95ee\u7b54\u4e2d\u7684\u7ec6\u7c92\u5ea6\u5355\u5143\u5f52\u56e0\uff0c\u901a\u8fc7\u88c1\u526a\u8868\u683c\u3001\u5206\u89e3\u95ee\u9898\u3001\u5bf9\u9f50\u7b54\u6848\u4e0e\u652f\u6301\u5355\u5143\uff0c\u5b9e\u73b0\u663e\u6027\u548c\u9690\u6027\u8bc1\u636e\u6355\u6349\u3002\u5e76\u53d1\u5e03\u4e86CITEBench\u57fa\u51c6\u96c6\u548cFairScore\u65e0\u53c2\u8003\u5f52\u56e0\u8bc4\u4f30\u6307\u6807\u3002", "result": "TraceBack\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u7ec6\u7c92\u5ea6\u6c34\u5e73\u4e0a\u663e\u8457\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0cFairScore\u8bc4\u4f30\u6307\u6807\u4e0e\u4eba\u5de5\u5224\u65ad\u9ad8\u5ea6\u4e00\u81f4\uff0c\u80fd\u6709\u6548\u652f\u6301\u53ef\u89e3\u91ca\u548c\u53ef\u6269\u5c55\u7684\u8868\u683c\u95ee\u7b54\u8bc4\u4f30\u3002", "conclusion": "TraceBack\u6709\u6548\u89e3\u51b3\u4e86\u8868\u683c\u95ee\u7b54\u4e2d\u7684\u5355\u5143\u5f52\u56e0\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u7b54\u6848\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u5ea6\uff0cFairScore\u6307\u6807\u4fc3\u8fdb\u4e86\u65e0\u53c2\u8003\u7684\u81ea\u52a8\u5f52\u56e0\u8bc4\u4f30\uff0c\u5bf9\u63a8\u52a8\u8868\u683c\u95ee\u7b54\u7684\u53ef\u89e3\u91ca\u6027\u548c\u5b9e\u7528\u6027\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2602.13084", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.13084", "abs": "https://arxiv.org/abs/2602.13084", "authors": ["Silin Du", "Manqing Xin", "Raymond Jia Wang"], "title": "Exploring a New Competency Modeling Process with Large Language Models", "comment": null, "summary": "Competency modeling is widely used in human resource management to select, develop, and evaluate talent. However, traditional expert-driven approaches rely heavily on manual analysis of large volumes of interview transcripts, making them costly and prone to randomness, ambiguity, and limited reproducibility. This study proposes a new competency modeling process built on large language models (LLMs). Instead of merely automating isolated steps, we reconstruct the workflow by decomposing expert practices into structured computational components. Specifically, we leverage LLMs to extract behavioral and psychological descriptions from raw textual data and map them to predefined competency libraries through embedding-based similarity. We further introduce a learnable parameter that adaptively integrates different information sources, enabling the model to determine the relative importance of behavioral and psychological signals. To address the long-standing challenge of validation, we develop an offline evaluation procedure that allows systematic model selection without requiring additional large-scale data collection. Empirical results from a real-world implementation in a software outsourcing company demonstrate strong predictive validity, cross-library consistency, and structural robustness. Overall, our framework transforms competency modeling from a largely qualitative and expert-dependent practice into a transparent, data-driven, and evaluable analytical process.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u80fd\u529b\u5efa\u6a21\u6d41\u7a0b\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u5206\u89e3\u548c\u878d\u5408\u591a\u6e90\u4fe1\u606f\uff0c\u5b9e\u73b0\u6210\u672c\u4f4e\u3001\u6548\u5ea6\u9ad8\u4e14\u6613\u8bc4\u4f30\u7684\u80fd\u529b\u6a21\u578b\uff0c\u5b9e\u8bc1\u9a8c\u8bc1\u6548\u679c\u663e\u8457\u3002", "motivation": "\u4f20\u7edf\u7684\u80fd\u529b\u5efa\u6a21\u5728\u4eba\u624d\u9009\u62d4\u3001\u53d1\u5c55\u548c\u8bc4\u4f30\u4e2d\u4f9d\u8d56\u4e13\u5bb6\u624b\u52a8\u5206\u6790\u5927\u91cf\u8bbf\u8c08\u6587\u672c\uff0c\u6210\u672c\u9ad8\u4e14\u6613\u53d7\u968f\u673a\u6027\u3001\u6a21\u7cca\u6027\u548c\u53ef\u91cd\u590d\u6027\u5dee\u5f71\u54cd\u3002", "method": "\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u91cd\u6784\u4e13\u5bb6\u5b9e\u8df5\u6d41\u7a0b\uff0c\u5229\u7528LLMs\u63d0\u53d6\u884c\u4e3a\u548c\u5fc3\u7406\u63cf\u8ff0\uff0c\u901a\u8fc7\u5d4c\u5165\u76f8\u4f3c\u5ea6\u6620\u5c04\u5230\u9884\u5b9a\u4e49\u80fd\u529b\u5e93\u3002\u5f15\u5165\u53ef\u5b66\u4e60\u53c2\u6570\u81ea\u9002\u5e94\u6574\u5408\u4e0d\u540c\u4fe1\u606f\u6e90\uff0c\u5f00\u53d1\u79bb\u7ebf\u8bc4\u4f30\u7a0b\u5e8f\u5b9e\u73b0\u65e0\u989d\u5916\u6570\u636e\u7684\u5927\u89c4\u6a21\u6a21\u578b\u9009\u62e9\u3002", "result": "\u5728\u8f6f\u4ef6\u5916\u5305\u4f01\u4e1a\u7684\u5b9e\u8bc1\u7814\u7a76\u4e2d\uff0c\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u7684\u9884\u6d4b\u6548\u5ea6\u5f3a\u3001\u5e93\u95f4\u4e00\u81f4\u6027\u548c\u7ed3\u6784\u7a33\u5065\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4f7f\u80fd\u529b\u5efa\u6a21\u4ece\u5b9a\u6027\u4e13\u5bb6\u4f9d\u8d56\u8f6c\u53d8\u4e3a\u900f\u660e\u3001\u6570\u636e\u9a71\u52a8\u4e14\u53ef\u8bc4\u4f30\u7684\u5206\u6790\u8fc7\u7a0b\u3002"}}
{"id": "2602.13102", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.13102", "abs": "https://arxiv.org/abs/2602.13102", "authors": ["Kais Allkivi"], "title": "Towards interpretable models for language proficiency assessment: Predicting the CEFR level of Estonian learner texts", "comment": null, "summary": "Using NLP to analyze authentic learner language helps to build automated assessment and feedback tools. It also offers new and extensive insights into the development of second language production. However, there is a lack of research explicitly combining these aspects. This study aimed to classify Estonian proficiency examination writings (levels A2-C1), assuming that careful feature selection can lead to more explainable and generalizable machine learning models for language testing. Various linguistic properties of the training data were analyzed to identify relevant proficiency predictors associated with increasing complexity and correctness, rather than the writing task. Such lexical, morphological, surface, and error features were used to train classification models, which were compared to models that also allowed for other features. The pre-selected features yielded a similar test accuracy but reduced variation in the classification of different text types. The best classifiers achieved an accuracy of around 0.9. Additional evaluation on an earlier exam sample revealed that the writings have become more complex over a 7-10-year period, while accuracy still reached 0.8 with some feature sets. The results have been implemented in the writing evaluation module of an Estonian open-source language learning environment.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u4ed4\u7ec6\u7279\u5f81\u9009\u62e9\uff0c\u5efa\u7acb\u4e86\u51c6\u786e\u4e14\u7a33\u5b9a\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7528\u4e8e\u7231\u6c99\u5c3c\u4e9a\u8bed\u5199\u4f5c\u6c34\u5e73\u5206\u7c7b\uff0c\u5e76\u5e94\u7528\u4e8e\u5f00\u6e90\u8bed\u8a00\u5b66\u4e60\u5e73\u53f0\u3002", "motivation": "\u5229\u7528NLP\u5206\u6790\u771f\u5b9e\u7684\u5b66\u4e60\u8005\u8bed\u8a00\uff0c\u7ed3\u5408\u81ea\u52a8\u8bc4\u4f30\u548c\u53cd\u9988\u5de5\u5177\uff0c\u63a2\u7d22\u66f4\u53ef\u89e3\u91ca\u548c\u6cdb\u5316\u7684\u8bed\u8a00\u6d4b\u8bd5\u6a21\u578b\u3002", "method": "\u901a\u8fc7\u5206\u6790\u7231\u6c99\u5c3c\u4e9a\u8bed\u6c34\u5e73\u8003\u8bd5\u5199\u4f5c\u6837\u672c\uff08A2-C1\uff09\uff0c\u9009\u53d6\u8bcd\u6c47\u3001\u5f62\u6001\u3001\u8868\u5c42\u7ed3\u6784\u548c\u9519\u8bef\u7279\u5f81\uff0c\u8bad\u7ec3\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u6a21\u578b\uff0c\u5e76\u6bd4\u8f83\u4e0d\u540c\u7279\u5f81\u7ec4\u5408\u7684\u6548\u679c\u3002", "result": "\u9884\u9009\u7279\u5f81\u7ec4\u5b9e\u73b0\u4e86\u7ea60.9\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u4e14\u51cf\u5c11\u4e86\u4e0d\u540c\u6587\u672c\u7c7b\u578b\u7684\u5206\u7c7b\u6ce2\u52a8\u3002\u65e9\u671f\u8003\u8bd5\u6837\u672c\u6d4b\u8bd5\u4e2d\uff0c\u5199\u4f5c\u590d\u6742\u5ea6\u63d0\u5347\uff0c\u4f46\u51c6\u786e\u7387\u4ecd\u8fbe0.8\u3002", "conclusion": "\u5408\u7406\u7684\u8bed\u8a00\u7279\u5f81\u9009\u62e9\u6709\u52a9\u4e8e\u6784\u5efa\u66f4\u89e3\u91ca\u6027\u5f3a\u4e14\u6cdb\u5316\u597d\u7684\u81ea\u52a8\u5199\u4f5c\u8bc4\u4f30\u6a21\u578b\uff0c\u63a8\u52a8\u771f\u5b9e\u8bed\u8a00\u5e94\u7528\u4e0e\u8bed\u8a00\u6d4b\u8bd5\u7814\u7a76\u7ed3\u5408\u3002"}}
{"id": "2602.13110", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13110", "abs": "https://arxiv.org/abs/2602.13110", "authors": ["Sher Badshah", "Ali Emami", "Hassan Sajjad"], "title": "SCOPE: Selective Conformal Optimized Pairwise LLM Judging", "comment": null, "summary": "Large language models (LLMs) are increasingly used as judges to replace costly human preference labels in pairwise evaluation. Despite their practicality, LLM judges remain prone to miscalibration and systematic biases. This paper proposes SCOPE (Selective Conformal Optimized Pairwise Evaluation), a framework for selective pairwise judging with finite-sample statistical guarantees. Under exchangeability, SCOPE calibrates an acceptance threshold such that the error rate among non-abstained judgments is at most a user-specified level $\u03b1$. To provide SCOPE with a bias-neutral uncertainty signal, we introduce Bidirectional Preference Entropy (BPE), which queries the judge under both response positions, aggregates the implied preference probabilities to enforce invariance to response order, and converts the aggregated probability into an entropy-based uncertainty score. Across MT-Bench, RewardBench, and Chatbot Arena, BPE improves uncertainty quality over standard confidence proxies, providing a stronger selection signal that enables SCOPE to consistently meet the target risk level while retaining good coverage across judge scales. In particular, at $\u03b1= 0.10$, \\textsc{Scope} consistently satisfies the risk bound across all benchmarks and judge scales (empirical risk $\\approx 0.097$ to $0.099$), while retaining substantial coverage, reaching $0.89$ on RewardBench with Qwen-14B and $0.98$ on RewardBench with Qwen-32B. Compared to na\u00efve baselines, \\textsc{Scope} accepts up to $2.4\\times$ more judgments on MT-Bench with Qwen-7B under the same target risk constraint, demonstrating that BPE enables reliable and high-coverage LLM-based evaluation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSCOPE\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5411\u504f\u597d\u71b5\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u5b9e\u73b0\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u914d\u5bf9\u8bc4\u4ef7\u7684\u9009\u62e9\u6027\u5224\u65ad\uff0c\u6709\u6548\u63a7\u5236\u9519\u8bef\u7387\u5e76\u63d0\u5347\u8986\u76d6\u7387\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u591a\u4e2a\u57fa\u51c6\u4e0b\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u914d\u5bf9\u8bc4\u4ef7\u4e2d\u66ff\u4ee3\u4eba\u5de5\u6807\u7b7e\u5177\u6709\u5b9e\u7528\u6027\uff0c\u4f46\u5b58\u5728\u6821\u51c6\u5931\u8bef\u548c\u7cfb\u7edf\u6027\u504f\u5dee\uff0c\u5bfc\u81f4\u8bc4\u4ef7\u4e0d\u53ef\u9760\u3002\u56e0\u800c\u9700\u8981\u4e00\u79cd\u5e26\u6709\u7edf\u8ba1\u4fdd\u8bc1\u7684\u9009\u62e9\u6027\u5224\u65ad\u6846\u67b6\u6765\u6539\u5584\u8bc4\u4ef7\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u4e86SCOPE\u6846\u67b6\uff0c\u5229\u7528\u4ea4\u6362\u6027\u6821\u51c6\u63a5\u53d7\u9608\u503c\uff0c\u7ed3\u5408\u53cc\u5411\u504f\u597d\u71b5\uff08BPE\uff09\u8861\u91cf\u4e0d\u786e\u5b9a\u6027\uff0c\u4ece\u800c\u9009\u62e9\u6027\u5730\u63a5\u53d7LLM\u914d\u5bf9\u5224\u65ad\uff0c\u4fdd\u969c\u9519\u8bef\u7387\u4e0d\u8d85\u8fc7\u9884\u8bbe\u6c34\u5e73\u3002", "result": "\u5728\u591a\u4e2a\u8bc4\u6d4b\u57fa\u51c6\uff08MT-Bench\u3001RewardBench\u3001Chatbot Arena\uff09\u4e0a\uff0cSCOPE\u6846\u67b6\u5728\u98ce\u9669\u6c34\u5e73\u03b1=0.10\u4e0b\u5b9e\u73b0\u4e86\u98ce\u9669\u63a7\u5236\uff08\u7ea60.097\u81f30.099\uff09\u548c\u8f83\u9ad8\u7684\u8986\u76d6\u7387\uff08\u6700\u9ad8\u8fbe0.98\uff09\uff0c\u4e14\u76f8\u6bd4\u7b80\u5355\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u63a5\u53d7\u5224\u65ad\u6570\u91cf\u6700\u591a2.4\u500d\uff0c\u8868\u73b0\u51fa\u826f\u597d\u7684\u4e0d\u786e\u5b9a\u6027\u5224\u522b\u80fd\u529b\u548c\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "conclusion": "SCOPE\u6846\u67b6\u901a\u8fc7\u5f15\u5165BPE\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\uff0c\u5b9e\u73b0\u4e86\u5728\u4fdd\u6301\u7edf\u8ba1\u98ce\u9669\u7ea6\u675f\u7684\u524d\u63d0\u4e0b\uff0c\u5bf9LLM\u4f5c\u4e3a\u8bc4\u4ef7\u8005\u7684\u9009\u62e9\u6027\u914d\u5bf9\u5224\u65ad\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5224\u65ad\u7684\u8986\u76d6\u7387\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2602.13123", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.13123", "abs": "https://arxiv.org/abs/2602.13123", "authors": ["Maria Ryskina", "Matthew R. Gormley", "Kyle Mahowald", "David R. Mortensen", "Taylor Berg-Kirkpatrick", "Vivek Kulkarni"], "title": "From sunblock to softblock: Analyzing the correlates of neology in published writing and on social media", "comment": "Accepted to LChange 2026", "summary": "Living languages are shaped by a host of conflicting internal and external evolutionary pressures. While some of these pressures are universal across languages and cultures, others differ depending on the social and conversational context: language use in newspapers is subject to very different constraints than language use on social media. Prior distributional semantic work on English word emergence (neology) identified two factors correlated with creation of new words by analyzing a corpus consisting primarily of historical published texts (Ryskina et al., 2020, arXiv:2001.07740). Extending this methodology to contextual embeddings in addition to static ones and applying it to a new corpus of Twitter posts, we show that the same findings hold for both domains, though the topic popularity growth factor may contribute less to neology on Twitter than in published writing. We hypothesize that this difference can be explained by the two domains favouring different neologism formation mechanisms.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8bed\u8a00\u65b0\u8bcd\u7684\u4ea7\u751f\u673a\u5236\uff0c\u6bd4\u8f83\u4e86\u65b0\u95fb\u5a92\u4f53\u4e0e\u793e\u4ea4\u5a92\u4f53\uff08Twitter\uff09\u4e2d\u7684\u65b0\u8bcd\u51fa\u73b0\uff0c\u53d1\u73b0\u4e24\u8005\u5728\u65b0\u8bcd\u5f62\u6210\u4e0a\u5b58\u5728\u76f8\u4f3c\u548c\u5dee\u5f02\u3002", "motivation": "\u8bed\u8a00\u4e2d\u65b0\u8bcd\u7684\u4ea7\u751f\u53d7\u591a\u4e2a\u5185\u90e8\u548c\u5916\u90e8\u8fdb\u5316\u538b\u529b\u5f71\u54cd\uff0c\u4e0d\u540c\u8bed\u5883\u548c\u5a92\u4ecb\u4e2d\u7684\u8bed\u8a00\u4f7f\u7528\u9762\u4e34\u4e0d\u540c\u7ea6\u675f\uff0c\u4e9f\u9700\u63a2\u7a76\u793e\u4ea4\u5a92\u4f53\u4e0e\u4f20\u7edf\u51fa\u7248\u7269\u4e2d\u65b0\u8bcd\u4ea7\u751f\u7684\u5dee\u5f02\u3002", "method": "\u5c06\u9759\u6001\u8bcd\u5411\u91cf\u6269\u5c55\u5230\u4e0a\u4e0b\u6587\u5d4c\u5165\uff0c\u5e94\u7528\u4e8eTwitter\u63a8\u6587\u6570\u636e\uff0c\u57fa\u4e8e\u5df2\u6709\u82f1\u6587\u65b0\u8bcd\u4ea7\u751f\u7814\u7a76\u65b9\u6cd5\u8fdb\u884c\u5206\u6790\u3002", "result": "\u7814\u7a76\u663e\u793a\uff0c\u65b0\u95fb\u548cTwitter\u4e2d\u51fa\u73b0\u65b0\u8bcd\u7684\u4e24\u4e2a\u76f8\u5173\u56e0\u7d20\u5747\u6210\u7acb\uff0c\u4f46\u8bdd\u9898\u6d41\u884c\u5ea6\u5bf9Twitter\u4e2d\u65b0\u8bcd\u4ea7\u751f\u7684\u4f5c\u7528\u8f83\u5c0f\uff0c\u6697\u793a\u4e24\u8005\u65b0\u8bcd\u5f62\u6210\u673a\u5236\u6709\u6240\u4e0d\u540c\u3002", "conclusion": "\u65b0\u95fb\u51fa\u7248\u6587\u672c\u4e0eTwitter\u793e\u4ea4\u5a92\u4f53\u8bed\u6599\u4e2d\u7684\u65b0\u8bcd\u4ea7\u751f\u673a\u5236\u5b58\u5728\u5171\u540c\u70b9\uff0c\u4f46\u8bdd\u9898\u6d41\u884c\u5ea6\u5bf9Twitter\u65b0\u8bcd\u5f62\u6210\u7684\u5f71\u54cd\u8f83\u5c0f\uff0c\u53ef\u80fd\u56e0\u4e24\u8005\u504f\u597d\u4e0d\u540c\u7684\u65b0\u8bcd\u5f62\u6210\u673a\u5236\u3002"}}
{"id": "2602.13139", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.13139", "abs": "https://arxiv.org/abs/2602.13139", "authors": ["Mariia Fedorova", "Nikolay Arefyev", "Maja Buljan", "Jind\u0159ich Helcl", "Stephan Oepen", "Egil R\u00f8nningstad", "Yves Scherrer"], "title": "OpenLID-v3: Improving the Precision of Closely Related Language Identification -- An Experience Report", "comment": "VarDial'26 workshop at the EACL 2026 conference", "summary": "Language identification (LID) is an essential step in building high-quality multilingual datasets from web data. Existing LID tools (such as OpenLID or GlotLID) often struggle to identify closely related languages and to distinguish valid natural language from noise, which contaminates language-specific subsets, especially for low-resource languages. In this work we extend the OpenLID classifier by adding more training data, merging problematic language variant clusters, and introducing a special label for marking noise. We call this extended system OpenLID-v3 and evaluate it against GlotLID on multiple benchmarks. During development, we focus on three groups of closely related languages (Bosnian, Croatian, and Serbian; Romance varieties of Northern Italy and Southern France; and Scandinavian languages) and contribute new evaluation datasets where existing ones are inadequate. We find that ensemble approaches improve precision but also substantially reduce coverage for low-resource languages. OpenLID-v3 is available on https://huggingface.co/HPLT/OpenLID-v3.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86OpenLID\u8bed\u8a00\u8bc6\u522b\u7cfb\u7edf\uff0c\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u548c\u6807\u7b7e\u4f18\u5316\u63d0\u5347\u4e86\u8bc6\u522b\u76f8\u8fd1\u8bed\u8a00\u548c\u5904\u7406\u566a\u58f0\u7684\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u5de5\u5177\u3002", "motivation": "\u73b0\u6709\u7684\u8bed\u8a00\u8bc6\u522b\u5de5\u5177\u5728\u8bc6\u522b\u76f8\u8fd1\u8bed\u8a00\u548c\u533a\u5206\u6709\u6548\u81ea\u7136\u8bed\u8a00\u4e0e\u566a\u58f0\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u5bfc\u81f4\u591a\u8bed\u8a00\u6570\u636e\u96c6\u4e2d\u7684\u8bed\u8a00\u5b50\u96c6\u88ab\u6c61\u67d3\u3002", "method": "\u6269\u5c55OpenLID\u5206\u7c7b\u5668\uff1a\u589e\u52a0\u8bad\u7ec3\u6570\u636e\u3001\u5408\u5e76\u76f8\u8fd1\u8bed\u8a00\u53d8\u4f53\u3001\u5f15\u5165\u566a\u58f0\u6807\u7b7e\uff0c\u5e76\u9488\u5bf9\u7279\u5b9a\u8bed\u8a00\u7ec4\u5f00\u53d1\u65b0\u8bc4\u4f30\u6570\u636e\u96c6\u3002", "result": "OpenLID-v3\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8eGlotLID\u3002\u96c6\u6210\u65b9\u6cd5\u867d\u7136\u63d0\u9ad8\u4e86\u7cbe\u786e\u5ea6\uff0c\u4f46\u663e\u8457\u964d\u4f4e\u4e86\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u8986\u76d6\u7387\u3002", "conclusion": "\u6b64\u6b21\u5de5\u4f5c\u901a\u8fc7\u6539\u8fdb\u8bad\u7ec3\u6570\u636e\u548c\u6a21\u578b\u8bbe\u8ba1\uff0c\u63d0\u5347\u4e86\u8bed\u8a00\u8bc6\u522b\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u8bc6\u522b\u76f8\u8fd1\u8bed\u8a00\u548c\u5904\u7406\u566a\u58f0\u65b9\u9762\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.13194", "categories": ["cs.CL", "cond-mat.dis-nn", "cond-mat.stat-mech", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13194", "abs": "https://arxiv.org/abs/2602.13194", "authors": ["Weishun Zhong", "Doron Sivan", "Tankut Can", "Mikhail Katkov", "Misha Tsodyks"], "title": "Semantic Chunking and the Entropy of Natural Language", "comment": "29 pages, 9 figures", "summary": "The entropy rate of printed English is famously estimated to be about one bit per character, a benchmark that modern large language models (LLMs) have only recently approached. This entropy rate implies that English contains nearly 80 percent redundancy relative to the five bits per character expected for random text. We introduce a statistical model that attempts to capture the intricate multi-scale structure of natural language, providing a first-principles account of this redundancy level. Our model describes a procedure of self-similarly segmenting text into semantically coherent chunks down to the single-word level. The semantic structure of the text can then be hierarchically decomposed, allowing for analytical treatment. Numerical experiments with modern LLMs and open datasets suggest that our model quantitatively captures the structure of real texts at different levels of the semantic hierarchy. The entropy rate predicted by our model agrees with the estimated entropy rate of printed English. Moreover, our theory further reveals that the entropy rate of natural language is not fixed but should increase systematically with the semantic complexity of corpora, which are captured by the only free parameter in our model.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u7edf\u8ba1\u6a21\u578b\uff0c\u89e3\u91ca\u4e86\u82f1\u8bed\u6587\u672c\u7684\u71b5\u7387\u548c\u9ad8\u5ea6\u5197\u4f59\uff0c\u8868\u660e\u71b5\u7387\u968f\u7740\u8bed\u4e49\u590d\u6742\u5ea6\u589e\u52a0\u800c\u589e\u52a0\u3002", "motivation": "\u63a2\u7a76\u82f1\u8bed\u6587\u672c\u7684\u71b5\u7387\u7ea6\u4e3a\u6bcf\u5b57\u7b26\u4e00\u6bd4\u7279\uff0c\u7406\u89e3\u5176\u5197\u4f59\u7387\u53ca\u5176\u80cc\u540e\u7684\u8bed\u8a00\u7ed3\u6784\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u81ea\u76f8\u4f3c\u7684\u7edf\u8ba1\u6a21\u578b\uff0c\u5c06\u6587\u672c\u5206\u5272\u6210\u8bed\u4e49\u8fde\u8d2f\u7684\u5757\uff0c\u5e76\u901a\u8fc7\u5c42\u7ea7\u7ed3\u6784\u5206\u89e3\u8bed\u4e49\uff0c\u8fdb\u884c\u7406\u8bba\u548c\u6570\u503c\u5206\u6790\u3002", "result": "\u6a21\u578b\u80fd\u591f\u5b9a\u91cf\u63cf\u8ff0\u771f\u5b9e\u6587\u672c\u5728\u4e0d\u540c\u8bed\u4e49\u5c42\u7ea7\u7684\u7ed3\u6784\uff0c\u9884\u6d4b\u7684\u71b5\u7387\u4e0e\u5b9e\u9645\u5370\u5237\u82f1\u8bed\u71b5\u7387\u76f8\u7b26\u3002", "conclusion": "\u81ea\u7136\u8bed\u8a00\u7684\u71b5\u7387\u4e0d\u662f\u56fa\u5b9a\u7684\uff0c\u800c\u662f\u968f\u7740\u8bed\u4e49\u590d\u6742\u5ea6\u7cfb\u7edf\u6027\u589e\u52a0\uff0c\u6a21\u578b\u7684\u81ea\u7531\u53c2\u6570\u6355\u83b7\u4e86\u8fd9\u4e00\u53d8\u5316\u3002"}}
