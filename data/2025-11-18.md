<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 86]
- [cs.SE](#cs.SE) [Total: 24]
- [cs.MA](#cs.MA) [Total: 11]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [TimeStampEval: A Simple LLM Eval and a Little Fuzzy Matching Trick to Improve Search Accuracy](https://arxiv.org/abs/2511.11594)
*James McCammon*

Main category: cs.CL

TL;DR: 引入TimeStampEval基准，用于从长转录文本中精准检索非逐字引用的毫秒级时间戳，并提出两阶段方法显著提升准确率同时大幅降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 解决传统模糊匹配在语义相同但句法不同的引用定位问题，特别是在官方记录与语音转文字转录对齐时的挑战。

Method: 提出两阶段方法：先用RapidFuzz进行预筛选，再用大语言模型（LLM）在短片段上验证，提高模糊匹配准确度并减少计算资源消耗。

Result: 该方法在多模糊匹配场景下准确率提升至90%以上，推理成本降低超过90%，对长转录文本、词汇漂移和领域变化表现稳健，能准确排除不存在的目标引用。

Conclusion: 通过合理的提示设计和两阶段匹配策略，TimeStampEval有效解决非逐字引用的时间戳检索问题，兼顾高准确率和低延迟，适用于长格式多源转录文本的自动化应用。

Abstract: Traditional fuzzy matching often fails when searching for quotes that are semantically identical but syntactically different across documents-a common issue when aligning official written records with speech-to-text transcripts. We introduce TimeStampEval, a benchmark for retrieving precise millisecond timestamps from long transcripts given non-verbatim quotes. Our simple two-stage method dramatically improves retrieval accuracy while cutting inference costs by over 90%. The motivating use case is an automated long-form podcast that assembles Congressional Record clips into AI-hosted narration. The technical challenge: given a sentence-timestamped transcript and a target quote that may differ due to transcription or editorial drift, return exact start and end boundaries. Standard algorithms handle verbatim text but break under fuzzier variants. Evaluating six modern LLMs on a 2,800-sentence (120k-token) transcript revealed four key findings. (1) Prompt design matters more than model choice: placing the query before the transcript and using compact formatting improved accuracy by 3-20 points while reducing token count by 30-40%. (2) Off-by-one errors form a distinct category, showing models understand the task but misplace boundaries. (3) A modest reasoning budget (600-850 tokens) raises accuracy from 37% to 77% for weak setups and to above 90% for strong ones. (4) Our "Assisted Fuzzy" approach-RapidFuzz pre-filtering followed by LLM verification on short snippets-improves fuzzy match accuracy by up to 50 points while halving latency and reducing cost per correct result by up to 96%. Extended tests on ten transcripts (50k-900k tokens, 1989-2025) confirm robustness to transcript length, vocabulary drift, and domain change, maintaining 95-100% rejection accuracy for absent targets.

</details>


### [2] [MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling](https://arxiv.org/abs/2511.11793)
*MiroMind Team,Song Bai,Lidong Bing,Carson Chen,Guanzheng Chen,Yuntao Chen,Zhe Chen,Ziyi Chen,Jifeng Dai,Xuan Dong,Yue Deng,Yunjie Fu,Junqi Ge,Chenxia Han,Tammy Huang,Zhenhang Huang,Jerry Jiao,Shilei Jiang,Tianyu Jiao,Xiaoqi Jian,Lei Lei,Ruilin Li,Ryan Luo,Tiantong Li,Xiang Lin,Ziyuan Liu,Zhiqi Li,Jie Ni,Qiang Ren,Pax Sun,Shiqian Su,Chenxin Tao,Bin Wang,Hellen Wang,Haonan Wang,James Wang,Jin Wang,Jojo Wang,Letian Wang,Shizun Wang,Weizhi Wang,Zixuan Wang,Jinfan Xu,Sen Xing,Chenyu Yang,Hai Ye,Jiaheng Yu,Yue Yu,Muyan Zhong,Tianchen Zhao,Xizhou Zhu,Yanpeng Zhou,Yifan Zhang,Zhi Zhu*

Main category: cs.CL

TL;DR: MiroThinker v1.0是一种开源研究代理，通过模型级交互扩展，实现深度和频繁的模型-环境交互，提升工具辅助推理和信息检索能力，表现优于现有开源及部分商业模型。


<details>
  <summary>Details</summary>
Motivation: 现有研究代理多通过增加模型规模或上下文长度提升性能，而忽视了通过增加交互频率和深度提升性能的潜力。

Method: 提出交互扩展策略，通过强化学习训练模型在大规模上下文环境中进行高频次复杂交互，利用环境反馈和外部信息修正错误，提升多轮推理能力。

Result: MiroThinker在四个基准测试中的准确率分别达到81.9%、37.7%、47.1%和55.6%，表现超过其他开源代理，接近GPT-5-high等商业模型，且交互扩展带来的性能提升稳定且显著。

Conclusion: 交互扩展是继模型规模和上下文长度后的第三个关键性能提升维度，对构建下一代开放研究代理具有重要价值和指导意义。

Abstract: We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. Our analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scaling behaviors analogous to model size and context length. These findings establish interaction scaling as a third critical dimension for building next-generation open research agents, complementing model capacity and context windows.

</details>


### [3] [On the Notion that Language Models Reason](https://arxiv.org/abs/2511.11810)
*Bertram Højer*

Main category: cs.CL

TL;DR: 本文探讨了语言模型是否具备推理能力，认为目前定义与模型实际训练与运作方式不符，推理类输出实为统计规律表现。


<details>
  <summary>Details</summary>
Motivation: 澄清语言模型被认为具备推理能力的定义与实际计算机制之间的不一致性，理清语言模型生成推理类输出的本质。

Method: 基于transformer模型的隐式有限阶马尔可夫核视角，解释语言模型输出的推理类表现为统计规律和不变性的体现，而非显式逻辑推理机制。

Result: 提出语言模型更像统计模式匹配者而非真推理者，说明推理类输出无保证逻辑一致性，这对评估模型的不确定性至关重要。

Conclusion: 强调在自然语言处理研究中，应更准确描述模型的计算过程，促进对语言模型推理能力的理性认知和评价。

Abstract: Language models (LMs) are said to be exhibiting reasoning, but what does this entail? We assess definitions of reasoning and how key papers in the field of natural language processing (NLP) use the notion and argue that the definitions provided are not consistent with how LMs are trained, process information, and generate new tokens. To illustrate this incommensurability we assume the view that transformer-based LMs implement an \textit{implicit} finite-order Markov kernel mapping contexts to conditional token distributions. In this view, reasoning-like outputs correspond to statistical regularities and approximate statistical invariances in the learned kernel rather than the implementation of explicit logical mechanisms. This view is illustrative of the claim that LMs are "statistical pattern matchers"" and not genuine reasoners and provides a perspective that clarifies why reasoning-like outputs arise in LMs without any guarantees of logical consistency. This distinction is fundamental to how epistemic uncertainty is evaluated in LMs. We invite a discussion on the importance of how the computational processes of the systems we build and analyze in NLP research are described.

</details>


### [4] [Scaling Open-Weight Large Language Models for Hydropower Regulatory Information Extraction: A Systematic Analysis](https://arxiv.org/abs/2511.11821)
*Hong-Jun Yoon,Faisal Ashraf,Thomas A. Ruggles,Debjani Singh*

Main category: cs.CL

TL;DR: 本文评估了七个参数规模从0.6B到70B的大型语言模型在水电许可文件信息提取中的表现和计算资源消耗，发现14B参数是性能提升的关键阈值，较大模型性能更优但资源需求更高。


<details>
  <summary>Details</summary>
Motivation: 在监管文件的信息提取中，存在性能和计算资源之间的关键权衡，本文旨在通过实证分析为模型部署提供指导。

Method: 针对水电许可文档，评估了7个不同规模的开源大语言模型，使用F1评分验证模型效果，分析模型在不同参数范围内的性能及错误模式。

Result: 发现14B参数是分水岭，达到该阈值后F1分数从低于0.15提升到0.64；消费者可用模型最高64% F1，小规模模型最高51%；最大模型可接近77% F1，但需要企业级基础设施支持。

Conclusion: 建立了监管信息提取任务中模型规模与性能资源消耗的系统映射，首次揭示模型参数扩展对性能的影响，为实际部署和模型选择提供了科学依据，对水电合规性有直接帮助。

Abstract: Information extraction from regulatory documents using large language models presents critical trade-offs between performance and computational resources. We evaluated seven open-weight models (0.6B-70B parameters) on hydropower licensing documentation to provide empirical deployment guidance.
  Our analysis identified a pronounced 14B parameter threshold where validation methods transition from ineffective (F1 $<$ 0.15) to viable (F1 = 0.64). Consumer-deployable models achieve 64\% F1 through appropriate validation, while smaller models plateau at 51\%. Large-scale models approach 77\% F1 but require enterprise infrastructure.
  We identified systematic hallucination patterns where perfect recall indicates extraction failure rather than success in smaller models. Our findings establish the first comprehensive resource-performance mapping for open-weight information extraction in regulatory contexts, enabling evidence-based model selection.
  These results provide immediate value for hydropower compliance while contributing insights into parameter scaling effects that generalize across information extraction tasks.

</details>


### [5] [Towards Autoformalization of LLM-generated Outputs for Requirement Verification](https://arxiv.org/abs/2511.11829)
*Mihir Gupte,Ramesh S*

Main category: cs.CL

TL;DR: 本文探讨了使用基于LLM的自动形式化工具验证自然语言需求与生成输出的一致性，展示了该方法在逻辑等价和不一致检测上的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏形式化的方法来验证大语言模型（LLM）从自然语言生成的结构化输出的准确性，因此亟需探索自动形式化技术以填补这一空白。

Method: 本文利用简单的LLM自动形式化工具对小规模自然语言需求进行处理，通过两个实验验证其在检测需求间逻辑等价性和识别逻辑不一致方面的有效性。

Result: 实验结果显示，自动形式化工具能够成功识别措辞不同但逻辑等价的需求，也能发现生成输出与需求之间的逻辑不一致，体现了其在一致性检验和验证中的实用价值。

Conclusion: 自动形式化在确保LLM生成输出的逻辑一致性和准确性方面具有重要潜力，为该领域未来深入研究奠定了基础。

Abstract: Autoformalization, the process of translating informal statements into formal logic, has gained renewed interest with the emergence of powerful Large Language Models (LLMs). While LLMs show promise in generating structured outputs from natural language (NL), such as Gherkin Scenarios from NL feature requirements, there's currently no formal method to verify if these outputs are accurate. This paper takes a preliminary step toward addressing this gap by exploring the use of a simple LLM-based autoformalizer to verify LLM-generated outputs against a small set of natural language requirements. We conducted two distinct experiments. In the first one, the autoformalizer successfully identified that two differently-worded NL requirements were logically equivalent, demonstrating the pipeline's potential for consistency checks. In the second, the autoformalizer was used to identify a logical inconsistency between a given NL requirement and an LLM-generated output, highlighting its utility as a formal verification tool. Our findings, while limited, suggest that autoformalization holds significant potential for ensuring the fidelity and logical consistency of LLM-generated outputs, laying a crucial foundation for future, more extensive studies into this novel application.

</details>


### [6] [Three Stage Narrative Analysis; Plot-Sentiment Breakdown, Structure Learning and Concept Detection](https://arxiv.org/abs/2511.11857)
*Taimur Khan,Ramoza Ahsan,Mohib Hameed*

Main category: cs.CL

TL;DR: 本文提出了一种分析电影剧本情感曲线的框架，通过字典情感分析和层次聚类，实现对叙事中高低层次概念的提取。


<details>
  <summary>Details</summary>
Motivation: 故事理解需要深度语义表示和自动化处理，手动分析难以应对大量叙事数据。

Method: 基于LabMTsimple故事工具构建自定义词典，结合NRC-VAD评分，使用字典情感分析和Ward层次聚类分析情感曲线。

Result: 在电影数据集上的实验表明该方法有效，能辅助消费者和读者在选择故事时获得有用分析。

Conclusion: 该框架实现了电影剧本情感的深度分析，为叙事理解提供了自动化、有效的工具。

Abstract: Story understanding and analysis have long been challenging areas within Natural Language Understanding. Automated narrative analysis requires deep computational semantic representations along with syntactic processing. Moreover, the large volume of narrative data demands automated semantic analysis and computational learning rather than manual analytical approaches. In this paper, we propose a framework that analyzes the sentiment arcs of movie scripts and performs extended analysis related to the context of the characters involved. The framework enables the extraction of high-level and low-level concepts conveyed through the narrative. Using dictionary-based sentiment analysis, our approach applies a custom lexicon built with the LabMTsimple storylab module. The custom lexicon is based on the Valence, Arousal, and Dominance scores from the NRC-VAD dataset. Furthermore, the framework advances the analysis by clustering similar sentiment plots using Wards hierarchical clustering technique. Experimental evaluation on a movie dataset shows that the resulting analysis is helpful to consumers and readers when selecting a narrative or story.

</details>


### [7] [Identifying Imaging Follow-Up in Radiology Reports: A Comparative Analysis of Traditional ML and LLM Approaches](https://arxiv.org/abs/2511.11867)
*Namu Park,Giridhar Kaushik Ramachandran,Kevin Lybarger,Fei Xia,Ozlem Uzuner,Meliha Yetisgen,Martin Gunn*

Main category: cs.CL

TL;DR: 本论文构建了一个包含6,393份放射科报告的注释语料库，针对随访影像状态进行标注，并比较了传统机器学习方法与大型语言模型在随访依从检测任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有临床自然语言处理领域缺乏针对放射学任务的专用数据集，限制了大型语言模型在该领域的评估和应用。

Method: 收集并标注了6,393份放射科报告，采用逻辑回归、支持向量机、Longformer、Llama3-8B-Instruct等传统和微调模型，与GPT-4o及开源GPT-OSS-20B（基础和任务优化两种设置）进行性能对比，通过精确率、召回率和F1分数衡量模型表现。

Result: GPT-4o（任务优化）表现最佳，F1=0.832；GPT-OSS-20B（任务优化）紧随其后，F1=0.828；传统模型LR和SVM表现也较好，F1分别为0.776和0.775。

Conclusion: 通过调优提示，大型语言模型可接近人类注释者的一致性水平，但解释性好且资源消耗低的传统模型依然是有价值的基准。

Abstract: Large language models (LLMs) have shown considerable promise in clinical natural language processing, yet few domain-specific datasets exist to rigorously evaluate their performance on radiology tasks. In this work, we introduce an annotated corpus of 6,393 radiology reports from 586 patients, each labeled for follow-up imaging status, to support the development and benchmarking of follow-up adherence detection systems. Using this corpus, we systematically compared traditional machine-learning classifiers, including logistic regression (LR), support vector machines (SVM), Longformer, and a fully fine-tuned Llama3-8B-Instruct, with recent generative LLMs. To evaluate generative LLMs, we tested GPT-4o and the open-source GPT-OSS-20B under two configurations: a baseline (Base) and a task-optimized (Advanced) setting that focused inputs on metadata, recommendation sentences, and their surrounding context. A refined prompt for GPT-OSS-20B further improved reasoning accuracy. Performance was assessed using precision, recall, and F1 scores with 95% confidence intervals estimated via non-parametric bootstrapping. Inter-annotator agreement was high (F1 = 0.846). GPT-4o (Advanced) achieved the best performance (F1 = 0.832), followed closely by GPT-OSS-20B (Advanced; F1 = 0.828). LR and SVM also performed strongly (F1 = 0.776 and 0.775), underscoring that while LLMs approach human-level agreement through prompt optimization, interpretable and resource-efficient models remain valuable baselines.

</details>


### [8] [MedPT: A Massive Medical Question Answering Dataset for Brazilian-Portuguese Speakers](https://arxiv.org/abs/2511.11878)
*Fernanda Bufon Färber,Iago Alves Brito,Julia Soares Dollis,Pedro Schindler Freire Brasil Ribeiro,Rafael Teixeira Sousa,Arlindo Rodrigues Galvão Filho*

Main category: cs.CL

TL;DR: 本文介绍了MedPT，这是首个针对巴西葡萄牙语的大规模真实医疗问答语料库，包含38万多条患者与医生的互动数据，并通过多阶段筛选和大语言模型注释提升数据质量，助力医疗专业领域模型的训练，取得了94%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型主要针对资源丰富的语言，缺乏涵盖临床和文化特有细节的巴西葡萄牙语医疗数据，影响医疗模型的准确性和公平性。

Method: 构建并多阶段筛选真实患者医生问答对；利用大语言模型进行语义分类注释，分为七种语义类型以捕捉用户意图；对医疗专业路由任务进行微调训练。

Result: 数据集覆盖3200个主题，展现患者与医生交流中的语言不对称性；训练的1.7亿参数模型在20分类任务中F1得分达94%；误分类反映了真实临床歧义，验证数据集语义深度。

Conclusion: MedPT数据集为巴西葡萄牙语医疗语言技术提供了丰富、多元且细致的资源，有助于开发公平、准确、具文化认知的医疗AI模型，数据已公开发布供社区使用。

Abstract: While large language models (LLMs) show transformative potential in healthcare, their development remains focused on high-resource languages, creating a critical barrier for others as simple translation fails to capture unique clinical and cultural nuances, such as endemic diseases. To address this, we introduce MedPT, the first large-scale, real-world corpus for Brazilian Portuguese, comprising 384,095 authentic question-answer pairs from patient-doctor interactions. The dataset underwent a meticulous multi-stage curation protocol, using a hybrid quantitative-qualitative analysis to filter noise and contextually enrich thousands of ambiguous queries. We further augmented the corpus via LLM-driven annotation, classifying questions into seven semantic types to capture user intent. Our analysis reveals its thematic breadth (3,200 topics) and unique linguistic properties, like the natural asymmetry in patient-doctor communication. To validate its utility, we benchmark a medical specialty routing task: fine-tuning a 1.7B parameter model achieves an outstanding 94\% F1-score on a 20-class setup. Furthermore, our qualitative error analysis shows misclassifications are not random but reflect genuine clinical ambiguities (e.g., between comorbid conditions), proving the dataset's deep semantic richness. We publicly release MedPT to foster the development of more equitable, accurate, and culturally-aware medical technologies for the Portuguese-speaking world.

</details>


### [9] [ClinStructor: AI-Powered Structuring of Unstructured Clinical Texts](https://arxiv.org/abs/2511.11883)
*Karthikeyan K,Raghuveer Thirukovalluru,David Carlson*

Main category: cs.CL

TL;DR: 提出了一种利用大型语言模型将临床自由文本转换为结构化问答对的管道ClinStructor，以提升临床预测模型的可解释性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 临床笔记虽信息丰富但格式不固定，导致偏见、多中心泛化差和模型难以解释的问题。

Method: 开发ClinStructor，使用大型语言模型将自由文本转为结构化、任务特定的问答对，增强模型透明度和可控性。

Result: 在ICU死亡预测任务中，ClinStructor相比直接微调仅轻微降低预测性能（AUC下降2-3%），但显著提升可解释性。

Conclusion: ClinStructor为构建可靠、可解释且泛化能力强的临床机器学习模型奠定了基础。

Abstract: Clinical notes contain valuable, context-rich information, but their unstructured format introduces several challenges, including unintended biases (e.g., gender or racial bias), and poor generalization across clinical settings (e.g., models trained on one EHR system may perform poorly on another due to format differences) and poor interpretability. To address these issues, we present ClinStructor, a pipeline that leverages large language models (LLMs) to convert clinical free-text into structured, task-specific question-answer pairs prior to predictive modeling. Our method substantially enhances transparency and controllability and only leads to a modest reduction in predictive performance (a 2-3% drop in AUC), compared to direct fine-tuning, on the ICU mortality prediction task. ClinStructor lays a strong foundation for building reliable, interpretable, and generalizable machine learning models in clinical environments.

</details>


### [10] [Context-Emotion Aware Therapeutic Dialogue Generation: A Multi-component Reinforcement Learning Approach to Language Models for Mental Health Support](https://arxiv.org/abs/2511.11884)
*Eric Hua Qing Zhang,Julia Ive*

Main category: cs.CL

TL;DR: 本研究通过监督微调和强化学习技术提升GPT-2生成治疗对话的能力，显著提高了情感准确率和专业性，展示了强化学习在心理健康对话系统中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 心理健康疾病全球负担重大，COVID-19加剧了心理健康服务的获取难度，迫切需要高效的远程心理健康支持工具。传统大语言模型缺乏情感和情境感知，难以满足治疗需求。

Method: 通过重新设计输入格式，同时处理上下文信息和情感状态，结合多组件奖励函数，根据专业治疗师的回复和情感标注指导模型训练，使用监督微调和强化学习提升GPT-2的治疗对话生成能力。

Result: 强化学习相比基线GPT-2在多项评价指标（BLEU，ROUGE，METEOR）上均有提升，情感识别准确率从66.96%提升至99.34%，且模型输出具备高度的上下文相关性和专业水准。

Conclusion: 强化学习有效提升了治疗对话系统的表现，使其能够成为辅助心理治疗师的有价值工具，同时保持必要的人类临床监督。

Abstract: Mental health illness represents a substantial global socioeconomic burden, with COVID-19 further exacerbating accessibility challenges and driving increased demand for telehealth mental health support. While large language models (LLMs) offer promising solutions through 24/7 availability and non-judgmental interactions, pre-trained models often lack the contextual and emotional awareness necessary for appropriate therapeutic responses. This paper investigated the application of supervised fine-tuning (SFT) and reinforcement learning (RL) techniques to enhance GPT-2's capacity for therapeutic dialogue generation. The methodology restructured input formats to enable simultaneous processing of contextual information and emotional states alongside user input, employing a multi-component reward function that aligned model outputs with professional therapist responses and annotated emotions. Results demonstrated improvements through reinforcement learning over baseline GPT-2 across multiple evaluation metrics: BLEU (0.0111), ROUGE-1 (0.1397), ROUGE-2 (0.0213), ROUGE-L (0.1317), and METEOR (0.0581). LLM evaluation confirmed high contextual relevance and professionalism, while reinforcement learning achieved 99.34% emotion accuracy compared to 66.96% for baseline GPT-2. These findings demonstrate reinforcement learning's effectiveness in developing therapeutic dialogue systems that can serve as valuable assistive tools for therapists while maintaining essential human clinical oversight.

</details>


### [11] [Additive Large Language Models for Semi-Structured Text](https://arxiv.org/abs/2511.11922)
*Karthikeyan K,Raghuveer Thirukovalluru,David Carlson*

Main category: cs.CL

TL;DR: CALM是一种可解释的半结构化文本分类框架，利用大语言模型对临床文本进行风险预测，并通过组成部分贡献的加法结构实现细粒度解释和可视化。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型对临床文本分类虽有效，但其预测过程不透明，阻碍了临床和研究中的广泛应用。需要一种能明确指出患者记录中哪些部分驱动风险信号的解释方法。

Method: 提出CALM框架，将输入文本分解为语义明确的组成部分，通过加法方式累加各部分对结果的贡献，使贡献成为模型计算的一部分，实现可信的局部和总体解释。该结构也便于生成类似广义加法模型的风险曲线可视化。

Result: CALM在保持与传统大语言模型分类器相当的性能前提下，提升了模型的可信度，支持质量控制，并在模型开发和审计过程中揭示了临床有意义的模式。

Conclusion: CALM有效解决了大语言模型临床文本分类中的可解释性难题，促进了模型在真实临床环境下的应用和信任建设，且适应多种半结构化临床文档。

Abstract: Large Language Models have advanced clinical text classification, but their opaque predictions remain a critical barrier to practical adoption in research and clinical settings where investigators and physicians need to understand which parts of a patient's record drive risk signals. To address this challenge, we introduce \textbf{CALM}, short for \textbf{Classification with Additive Large Language Models}, an interpretable framework for semi-structured text where inputs are composed of semantically meaningful components, such as sections of an admission note or question-answer fields from an intake form. CALM predicts outcomes as the additive sum of each component's contribution, making these contributions part of the forward computation itself and enabling faithful explanations at both the patient and population level. The additive structure also enables clear visualizations, such as component-level risk curves similar to those used in generalized additive models, making the learned relationships easier to inspect and communicate. Although CALM expects semi-structured inputs, many clinical documents already have this form, and similar structure can often be automatically extracted from free-text notes. CALM achieves performance comparable to conventional LLM classifiers while improving trust, supporting quality-assurance checks, and revealing clinically meaningful patterns during model development and auditing.

</details>


### [12] [InData: Towards Secure Multi-Step, Tool-Based Data Analysis](https://arxiv.org/abs/2511.11933)
*Karthikeyan K,Raghuveer Thirukovalluru,Bhuwan Dhingra,David Edwin Carlson*

Main category: cs.CL

TL;DR: 本文提出了一种针对敏感数据分析中大语言模型(LLM)的安全替代方案，禁止LLM直接生成代码访问数据，而是通过预定义安全工具间接交互，并推出了评估LLM多步工具推理能力的InData数据集。


<details>
  <summary>Details</summary>
Motivation: 直接让LLM生成代码操作数据库在处理敏感数据时存在安全风险，且现有工具使用基准测试主要关注工具选择和简单执行，缺少复杂多步推理的评估。

Method: 限制LLM直接访问数据，要求通过安全验证的工具操作数据；设计多步工具推理能力评测数据集InData，涵盖简单到复杂三种难度的问题，并在15个开源LLM上进行基准测试。

Result: 大模型在简单任务上表现优异（准确率97.3%），但在复杂任务上性能显著下降（69.6%），表明当前LLM在多步工具推理能力上仍不足。

Conclusion: InData填补了多步工具推理评测空白，有助于推动LLM在复杂数据分析中安全且高效的工具使用能力发展，数据集和代码将公开发布。

Abstract: Large language model agents for data analysis typically generate and execute code directly on databases. However, when applied to sensitive data, this approach poses significant security risks. To address this issue, we propose a security-motivated alternative: restrict LLMs from direct code generation and data access, and require them to interact with data exclusively through a predefined set of secure, verified tools. Although recent tool-use benchmarks exist, they primarily target tool selection and simple execution rather than the compositional, multi-step reasoning needed for complex data analysis. To reduce this gap, we introduce Indirect Data Engagement (InData), a dataset designed to assess LLMs' multi-step tool-based reasoning ability. InData includes data analysis questions at three difficulty levels--Easy, Medium, and Hard--capturing increasing reasoning complexity. We benchmark 15 open-source LLMs on InData and find that while large models (e.g., gpt-oss-120b) achieve high accuracy on Easy tasks (97.3%), performance drops sharply on Hard tasks (69.6%). These results show that current LLMs still lack robust multi-step tool-based reasoning ability. With InData, we take a step toward enabling the development and evaluation of LLMs with stronger multi-step tool-use capabilities. We will publicly release the dataset and code.

</details>


### [13] [Improving LLM's Attachment to External Knowledge In Dialogue Generation Tasks Through Entity Anonymization](https://arxiv.org/abs/2511.11946)
*Hadi Sheikhi,Chenyang Huang,Osmar R. Zaïane*

Main category: cs.CL

TL;DR: 本文针对知识图谱对话生成任务，提出了评估大语言模型知识依附度的LLM-KAT方法，并通过实体匿名化技术提升模型利用外部知识图谱的能力，实验证明效果显著。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型虽在多项NLP任务表现优异，但在知识图谱对话生成中依赖内部知识，难以有效利用外部知识图谱。

Method: 首先提出LLM-KAT评估程序用于量化生成回复中的知识依附度；其次通过实体匿名化技术，鼓励模型更好地利用外部知识。

Result: 在OpenDialKG数据集上的实验表明，所提方法显著提升了大语言模型对外部知识的依赖性。

Conclusion: 实体匿名化作为一种简单有效的策略，可以促使大语言模型更好地利用知识图谱，从而提升知识图谱对话生成的质量。

Abstract: Knowledge graph-based dialogue generation (KG-DG) is a challenging task requiring models to effectively incorporate external knowledge into conversational responses. While large language models (LLMs) have achieved impressive results across various NLP tasks, their ability to utilize external knowledge in KG-DG remains under-explored. We observe that LLMs often rely on internal knowledge, leading to detachment from provided knowledge graphs, even when they are given a flawlessly retrieved knowledge graph. First, we introduce LLM-KAT, an evaluation procedure for measuring knowledge attachment in generated responses. Second, we propose a simple yet effective entity anonymization technique to encourage LLMs to better leverage external knowledge. Experiments on the OpenDialKG dataset demonstrate that our approach improves LLMs' attachment on external knowledge.

</details>


### [14] [On the Entropy Calibration of Language Models](https://arxiv.org/abs/2511.11966)
*Steven Cao,Gregory Valiant,Percy Liang*

Main category: cs.CL

TL;DR: 本文研究了语言模型的熵校准问题，发现误差随着模型规模扩大几乎不下降，传统的方法通过截断概率分布提高文本质量但牺牲了多样性。


<details>
  <summary>Details</summary>
Motivation: 理解语言模型随着规模扩大其生成文本熵是否更准确校准，以及是否存在不牺牲多样性和质量的校准方法。

Method: 通过简化的理论模型分析误差随数据集规模的标度行为，并在0.5B到70B参数的多种模型上进行实证测量。

Result: 发现误差随规模增长改善极其缓慢，经验和理论结果一致，表明大模型的错误积累速度与小模型相似，传统截断方法虽有效但增加了对数损失。

Conclusion: 证明了在理论上，只要有预测文本未来熵的黑盒工具，就有可能实现减少熵同时保持对数损失不变的校准方法。

Abstract: We study the problem of entropy calibration, which asks whether a language model's entropy over generations matches its log loss on human text. Past work found that models are miscalibrated, with entropy per step increasing (and text quality decreasing) as generations grow longer. This error accumulation is a fundamental problem in autoregressive models, and the standard solution is to truncate the distribution, which improves text quality at the cost of diversity. In this paper, we ask: is miscalibration likely to improve with scale, and is it theoretically possible to calibrate without tradeoffs? To build intuition, we first study a simplified theoretical setting to characterize the scaling behavior of miscalibration with respect to dataset size. We find that the scaling behavior depends on the power law exponent of the data distribution -- in particular, for a power law exponent close to 1, the scaling exponent is close to 0, meaning that miscalibration improves very slowly with scale. Next, we measure miscalibration empirically in language models ranging from 0.5B to 70B parameters. We find that the observed scaling behavior is similar to what is predicted by the simplified setting: our fitted scaling exponents for text are close to 0, meaning that larger models accumulate error at a similar rate as smaller ones. This scaling (or, lack thereof) provides one explanation for why we sample from larger models with similar amounts of truncation as smaller models, even though the larger models are of higher quality. However, truncation is not a satisfying solution because it comes at the cost of increased log loss. In theory, is it even possible to reduce entropy while preserving log loss? We prove that it is possible, if we assume access to a black box which can fit models to predict the future entropy of text.

</details>


### [15] [A Reasoning Paradigm for Named Entity Recognition](https://arxiv.org/abs/2511.11978)
*Hui Huang,Yanping Chen,Ruizhang Huang,Chuan Lin,Yongbin Qin*

Main category: cs.CL

TL;DR: 该论文提出了一种基于推理的命名实体识别框架，通过生成和调优推理链显著提升零样本和低资源场景下的识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有生成式大语言模型通过语义模式匹配进行实体识别，但缺乏显式、可验证的推理机制，导致在零样本和低资源环境下表现不佳。

Method: 提出三阶段推理框架：生成带有任务相关推理链的推理链数据集，利用该数据集调优模型生成连贯推理过程，最后通过综合奖励信号优化推理。

Result: 实验显示该方法在命名实体识别任务中表现卓越，零样本条件下F1分数超越GPT-4 12.3个百分点，展现了强大推理能力和竞争力。

Conclusion: 推理驱动的命名实体识别框架有效提升了模型的认知能力和泛化性能，尤其在零样本和低资源场景中表现突出，推动了推理导向的信息抽取研究。

Abstract: Generative LLMs typically improve Named Entity Recognition (NER) performance through instruction tuning. They excel at generating entities by semantic pattern matching but lack an explicit, verifiable reasoning mechanism. This "cognitive shortcutting" leads to suboptimal performance and brittle generalization, especially in zero-shot and lowresource scenarios where reasoning from limited contextual cues is crucial. To address this issue, a reasoning framework is proposed for NER, which shifts the extraction paradigm from implicit pattern matching to explicit reasoning. This framework consists of three stages: Chain of Thought (CoT) generation, CoT tuning, and reasoning enhancement. First, a dataset annotated with NER-oriented CoTs is generated, which contain task-relevant reasoning chains. Then, they are used to tune the NER model to generate coherent rationales before deriving the final answer. Finally, a reasoning enhancement stage is implemented to optimize the reasoning process using a comprehensive reward signal. This stage ensures explicit and verifiable extractions. Experiments show that ReasoningNER demonstrates impressive cognitive ability in the NER task, achieving competitive performance. In zero-shot settings, it achieves state-of-the-art (SOTA) performance, outperforming GPT-4 by 12.3 percentage points on the F1 score. Analytical results also demonstrate its great potential to advance research in reasoningoriented information extraction. Our codes are available at https://github.com/HuiResearch/ReasoningIE.

</details>


### [16] [Critical or Compliant? The Double-Edged Sword of Reasoning in Chain-of-Thought Explanations](https://arxiv.org/abs/2511.12001)
*Eunkyu Park,Wesley Hanwen Deng,Vasudha Varadarajan,Mingxi Yan,Gunhee Kim,Maarten Sap,Motahhare Eslami*

Main category: cs.CL

TL;DR: 本文研究了连锁思维（CoT）解释在多模态道德场景中的双重作用，发现解释既能增强透明度，也可能加强确认偏误，导致用户对错误推理的信任。


<details>
  <summary>Details</summary>
Motivation: 解释被用作提升系统透明度的工具，但同时可能导致用户出现确认偏误，难以识别模型推理错误，影响用户对模型的信任与判断能力。

Method: 通过系统性扰动视觉语言模型的推理链条及操控解释的传达语气，分析推理错误对用户信任和错误检测的影响。

Result: 发现用户通常将信任与结果一致性等同，即使推理错误仍持续依赖模型；而自信的语气会抑制错误识别，但保持信任度，说明传达方式能覆盖正确性影响。

Conclusion: 连锁思维解释既能促进理解，也可能误导用户，提示自然语言处理系统需设计能促使用户批判性思考的解释，避免盲目信任。

Abstract: Explanations are often promoted as tools for transparency, but they can also foster confirmation bias; users may assume reasoning is correct whenever outputs appear acceptable. We study this double-edged role of Chain-of-Thought (CoT) explanations in multimodal moral scenarios by systematically perturbing reasoning chains and manipulating delivery tones. Specifically, we analyze reasoning errors in vision language models (VLMs) and how they impact user trust and the ability to detect errors. Our findings reveal two key effects: (1) users often equate trust with outcome agreement, sustaining reliance even when reasoning is flawed, and (2) the confident tone suppresses error detection while maintaining reliance, showing that delivery styles can override correctness. These results highlight how CoT explanations can simultaneously clarify and mislead, underscoring the need for NLP systems to provide explanations that encourage scrutiny and critical thinking rather than blind trust. All code will be released publicly.

</details>


### [17] [CURE: Cultural Understanding and Reasoning Evaluation - A Framework for "Thick" Culture Alignment Evaluation in LLMs](https://arxiv.org/abs/2511.12014)
*Truong Vo,Sanmi Koyejo*

Main category: cs.CL

TL;DR: 本文提出了一套基于情境的文化理解评估指标，弥补了现有方法对文化能力评价的不足。


<details>
  <summary>Details</summary>
Motivation: 现有评测方法多聚焦于去情境的正确性或选择判断，忽视了对文化理解和推理的考察。

Method: 引入真实情境下的文化推理基准测试，并增加覆盖率、特异性、内涵和连贯性四项指标，除了传统的精准匹配。

Result: 实验证明，传统简单评测高估了语言模型的文化能力且波动大，而新方法能揭示推理深度差异，稳定且更具解读价值。

Conclusion: 基于情境的多维评测方法更有效地反映了大型语言模型的文化理解能力，有助于促进其在多元文化环境中的应用。

Abstract: Large language models (LLMs) are increasingly deployed in culturally diverse environments, yet existing evaluations of cultural competence remain limited. Existing methods focus on de-contextualized correctness or forced-choice judgments, overlooking the need for cultural understanding and reasoning required for appropriate responses. To address this gap, we introduce a set of benchmarks that, instead of directly probing abstract norms or isolated statements, present models with realistic situational contexts that require culturally grounded reasoning. In addition to the standard Exact Match metric, we introduce four complementary metrics (Coverage, Specificity, Connotation, and Coherence) to capture different dimensions of model's response quality. Empirical analysis across frontier models reveals that thin evaluation systematically overestimates cultural competence and produces unstable assessments with high variance. In contrast, thick evaluation exposes differences in reasoning depth, reduces variance, and provides more stable, interpretable signals of cultural understanding.

</details>


### [18] [Exploring Parameter-Efficient Fine-Tuning and Backtranslation for the WMT 25 General Translation Task](https://arxiv.org/abs/2511.12109)
*Felipe Fujita,Hideyuki Takada*

Main category: cs.CL

TL;DR: 本文研究了在小规模日语语料库上结合微调和反向翻译以提升神经机器翻译效果的方法，显著提高了英日翻译质量。


<details>
  <summary>Details</summary>
Motivation: 由于训练数据有限，如何有效利用小规模日语语料提升神经机器翻译性能是关键问题。

Method: 首先使用反向翻译生成合成日语数据增强训练集，再在小规模真实平行语料上进行微调，最后结合两者进一步优化模型。

Result: 基线模型COMET为0.460，反向翻译提升至0.468，微调提升至0.589，结合两者达0.597，显著提升翻译质量。

Conclusion: 即使训练数据有限，结合反向翻译和微调能协同提升低资源语言对的翻译性能，提供了一种轻量但有效的改进策略。

Abstract: In this paper, we explore the effectiveness of combining fine-tuning and backtranslation on a small Japanese corpus for neural machine translation. Starting from a baseline English{\textrightarrow}Japanese model (COMET = 0.460), we first apply backtranslation (BT) using synthetic data generated from monolingual Japanese corpora, yielding a modest increase (COMET = 0.468). Next, we fine-tune (FT) the model on a genuine small parallel dataset drawn from diverse Japanese news and literary corpora, achieving a substantial jump to COMET = 0.589 when using Mistral 7B. Finally, we integrate both backtranslation and fine-tuning{ -- }first augmenting the small dataset with BT generated examples, then adapting via FT{ -- }which further boosts performance to COMET = 0.597. These results demonstrate that, even with limited training data, the synergistic use of backtranslation and targeted fine-tuning on Japanese corpora can significantly enhance translation quality, outperforming each technique in isolation. This approach offers a lightweight yet powerful strategy for improving low-resource language pairs.

</details>


### [19] [LLMLagBench: Identifying Temporal Training Boundaries in Large Language Models](https://arxiv.org/abs/2511.12116)
*Piotr Pęzik,Konrad Kaczyński,Maria Szymańska,Filip Żarnecki,Zuzanna Deckert,Jakub Kwiatkowski,Wojciech Janowski*

Main category: cs.CL

TL;DR: 本文提出了LLMLagBench基准，用于系统地识别大语言模型训练数据的时间边界，通过评估模型对近期事件的知识掌握情况。


<details>
  <summary>Details</summary>
Motivation: 大语言模型因训练数据的时间截止限制，无法提供最新信息，且在不知情或忽视这一限制时，可能在推理中混淆过时信息，影响回答准确性。

Method: 设计LLMLagBench基准，通过评估模型对最新事件的知识，确定模型训练数据的最早可能时间边界，并对多种模型进行测试，并通过人工验证和与公开预训练信息对比评估基准可靠性。

Result: 基准成功识别了多种大语言模型的时间截止点，验证了基准的有效性和可靠性。

Conclusion: LLMLagBench为系统检测大语言模型知识新鲜度提供了有效工具，有助于更准确地理解和使用这些模型。

Abstract: Large Language Models (LLMs) are pretrained on textual data up to a specific temporal cutoff. This creates a strict knowledge boundary beyond which models cannot provide accurate information without querying external sources. More subtly, when this limitation is unknown or ignored, LLMs may inadvertently blend outdated time-sensitive information with general knowledge during reasoning tasks, potentially compromising response accuracy. We introduce LLMLagBench, an LLM freshness benchmark, as a systematic approach for identifying the earliest probable temporal boundaries of an LLM's training data by evaluating its knowledge of recent events. We then apply this benchmark to evaluate a large set of LLMs, including models with both explicitly declared and undeclared training cutoffs. The reliability of the benchmark is assessed by manual validation and comparison with publicly released information about LLM pretraining.

</details>


### [20] [PRISM of Opinions: A Persona-Reasoned Multimodal Framework for User-centric Conversational Stance Detection](https://arxiv.org/abs/2511.12130)
*Bingbing Wang,Zhixin Bai,Zhengda Jin,Zihan Wang,Xintong Song,Jingjie Lin,Sixuan Li,Jing Li,Ruifeng Xu*

Main category: cs.CL

TL;DR: 该论文提出了首个以用户为中心的多模态对话立场检测数据集U-MStance及模型PRISM，通过融合用户画像和多模态信息提升立场识别效果。


<details>
  <summary>Details</summary>
Motivation: 现有多模态对话立场检测研究存在伪多模态（评论仅文本）和用户同质化问题，不能真实反映复杂的用户表达和多模态交互。

Method: 构建包含4万多条用户评论的U-MStance数据集；设计PRISM模型，利用历史发帖生成用户画像，采用链式思考对齐文本与视觉信息，联合优化立场检测与立场感知回复生成。

Result: PRISM模型在U-MStance数据集上较强基线表现有显著提升，验证了用户中心和情境多模态推理的有效性。

Conclusion: 结合用户画像与对话多模态信息能更准确理解用户立场，推动了更真实的多模态对话立场检测研究。

Abstract: The rapid proliferation of multimodal social media content has driven research in Multimodal Conversational Stance Detection (MCSD), which aims to interpret users' attitudes toward specific targets within complex discussions. However, existing studies remain limited by: **1) pseudo-multimodality**, where visual cues appear only in source posts while comments are treated as text-only, misaligning with real-world multimodal interactions; and **2) user homogeneity**, where diverse users are treated uniformly, neglecting personal traits that shape stance expression. To address these issues, we introduce **U-MStance**, the first user-centric MCSD dataset, containing over 40k annotated comments across six real-world targets. We further propose **PRISM**, a **P**ersona-**R**easoned mult**I**modal **S**tance **M**odel for MCSD. PRISM first derives longitudinal user personas from historical posts and comments to capture individual traits, then aligns textual and visual cues within conversational context via Chain-of-Thought to bridge semantic and pragmatic gaps across modalities. Finally, a mutual task reinforcement mechanism is employed to jointly optimize stance detection and stance-aware response generation for bidirectional knowledge transfer. Experiments on U-MStance demonstrate that PRISM yields significant gains over strong baselines, underscoring the effectiveness of user-centric and context-grounded multimodal reasoning for realistic stance understanding.

</details>


### [21] [AI-Salesman: Towards Reliable Large Language Model Driven Telemarketing](https://arxiv.org/abs/2511.12133)
*Qingyu Zhang,Chunlei Xin,Xuanang Chen,Yaojie Lu,Hongyu Lin,Xianpei Han,Le Sun,Qing Ye,Qianlong Xie,Xingxing Wang*

Main category: cs.CL

TL;DR: 本文针对目标驱动的劝说对话任务，构建了首个真实场景语料库TeleSalesCorpus，提出了双阶段架构的AI-Salesman方法，通过贝叶斯监督强化学习和动态脚本引导，实现了多轮规划和事实忠实性，显著提升了劝说效果。


<details>
  <summary>Details</summary>
Motivation: 目标驱动的劝说对话如电话营销需要复杂的多轮规划和严格的事实忠实性，现有大模型存在策略脆弱和事实幻觉等问题，且缺乏高质量的任务特定数据。

Method: 构建TeleSalesCorpus数据集；设计基于贝叶斯监督强化学习的训练算法从噪声对话中学习销售策略；推理阶段利用动态大纲引导代理（DOGA），结合预制脚本库实现逐轮策略指导；设计综合评价框架。

Result: AI-Salesman在自动指标和人类综合评估中均明显优于基线模型，展现了在复杂劝说场景下的高效性能和稳健策略。

Conclusion: 通过结合新数据集、贝叶斯强化学习及动态脚本引导，AI-Salesman有效提升了目标驱动劝说对话的策略质量和事实忠实性，推动了该领域的技术进步。

Abstract: Goal-driven persuasive dialogue, exemplified by applications like telemarketing, requires sophisticated multi-turn planning and strict factual faithfulness, which remains a significant challenge for even state-of-the-art Large Language Models (LLMs). A lack of task-specific data often limits previous works, and direct LLM application suffers from strategic brittleness and factual hallucination. In this paper, we first construct and release TeleSalesCorpus, the first real-world-grounded dialogue dataset for this domain. We then propose AI-Salesman, a novel framework featuring a dual-stage architecture. For the training stage, we design a Bayesian-supervised reinforcement learning algorithm that learns robust sales strategies from noisy dialogues. For the inference stage, we introduce the Dynamic Outline-Guided Agent (DOGA), which leverages a pre-built script library to provide dynamic, turn-by-turn strategic guidance. Moreover, we design a comprehensive evaluation framework that combines fine-grained metrics for key sales skills with the LLM-as-a-Judge paradigm. Experimental results demonstrate that our proposed AI-Salesman significantly outperforms baseline models in both automatic metrics and comprehensive human evaluations, showcasing its effectiveness in complex persuasive scenarios.

</details>


### [22] [Seeing is Believing: Rich-Context Hallucination Detection for MLLMs via Backward Visual Grounding](https://arxiv.org/abs/2511.12140)
*Pinxue Guo,Chongruo Wu,Xinyu Zhou,Lingyi Hong,Zhaoyu Chen,Jinglun Li,Kaixun Jiang,Sen-ching Samson Cheung,Wei Zhang,Wenqiang Zhang*

Main category: cs.CL

TL;DR: 提出了VBackChecker，一种无需参考的多模态大语言模型幻觉检测框架，通过像素级定位验证视觉输入与回答的一致性，显著提升幻觉检测性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型虽然具备强大跨模态能力，但仍存在严重的幻觉问题，亟需准确检测以确保实用可靠性。

Method: 引入了基于"眼见为实"原则的VBackChecker，利用一个具备推理和指示分割能力的像素级定位大语言模型，实现对模型回答与视觉输入一致性的验证；设计了生成训练数据的管道R-Instruct，包含丰富上下文描述和困难负样本；构建了涵盖多样细节的R²-HalBench真实场景幻觉基准。

Result: VBackChecker在R²-HalBench基准上超越了之前复杂框架，幻觉检测性能达到最先进水平，甚至可媲美GPT-4o；像素级定位任务性能提升超过10%。

Conclusion: 该工作有效提升了多模态大语言模型的幻觉检测能力，提供了具有解释性的参考无关检测方法，为多模态系统的可靠应用奠定基础。

Abstract: Multimodal Large Language Models (MLLMs) have unlocked powerful cross-modal capabilities, but still significantly suffer from hallucinations. As such, accurate detection of hallucinations in MLLMs is imperative for ensuring their reliability in practical applications. To this end, guided by the principle of "Seeing is Believing", we introduce VBackChecker, a novel reference-free hallucination detection framework that verifies the consistency of MLLMgenerated responses with visual inputs, by leveraging a pixellevel Grounding LLM equipped with reasoning and referring segmentation capabilities. This reference-free framework not only effectively handles rich-context scenarios, but also offers interpretability. To facilitate this, an innovative pipeline is accordingly designed for generating instruction-tuning data (R-Instruct), featuring rich-context descriptions, grounding masks, and hard negative samples. We further establish R^2 -HalBench, a new hallucination benchmark for MLLMs, which, unlike previous benchmarks, encompasses real-world, rich-context descriptions from 18 MLLMs with high-quality annotations, spanning diverse object-, attribute, and relationship-level details. VBackChecker outperforms prior complex frameworks and achieves state-of-the-art performance on R^2 -HalBench, even rivaling GPT-4o's capabilities in hallucination detection. It also surpasses prior methods in the pixel-level grounding task, achieving over a 10% improvement. All codes, data, and models are available at https://github.com/PinxueGuo/VBackChecker.

</details>


### [23] [CriticSearch: Fine-Grained Credit Assignment for Search Agents via a Retrospective Critic](https://arxiv.org/abs/2511.12159)
*Yaocheng Zhang,Haohuan Huang,Zijun Song,Yuanheng Zhu,Qichao Zhang,Zijie Zhao,Dongbin Zhao*

Main category: cs.CL

TL;DR: 本文提出了一种名为CriticSearch的细粒度信用分配框架，通过回顾性评论机制为工具集成推理中的搜索引擎提供稳定且密集的反馈，显著提升训练速度和性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的搜索代理在工具集成推理任务中由于稀疏的结果奖励导致探索效率低下和训练不稳定。

Method: 引入CriticSearch框架，利用一个冻结的非对称评论大型语言模型，结合完整轨迹和标准答案对每一步进行回顾性评估，将评估结果转化为稳定且密集的奖励，指导策略改进。

Result: 在多跳推理基准测试中，CriticSearch较现有方法表现更好，表现为更快的收敛速度、更稳定的训练过程和更高的性能。

Conclusion: CriticSearch有效解决了强化学习中稀疏奖励问题，显著提升了工具集成推理搜索代理的训练效率和效果。

Abstract: Tool-Integrated Reasoning (TIR) with search engines enables large language models to iteratively retrieve up-to-date external knowledge, enhancing adaptability and generalization in complex question-answering tasks. However, existing search agent pipelines typically depend on reinforcement learning based optimization, which often suffers from sparse outcome rewards, leading to inefficient exploration and unstable training. We introduce CriticSearch, a fine-grained credit-assignment framework that supplies dense, turn-level feedback via a retrospective critic mechanism. During training, a frozen, asymmetric critique LLM retrospectively evaluates each turn using privileged information from the full trajectory and gold answers, converting these assessments into stable, dense rewards that guide policy improvement. Experimental results across diverse multi-hop reasoning benchmarks demonstrate that CriticSearch consistently outperforms existing baselines, achieving faster convergence, improved training stability, and higher performance.

</details>


### [24] [MME-RAG: Multi-Manager-Expert Retrieval-Augmented Generation for Fine-Grained Entity Recognition in Task-Oriented Dialogues](https://arxiv.org/abs/2511.12213)
*Liang Xue,Haoyu Liu,Yajun Tian,Xinyu Zhong,Yang Liu*

Main category: cs.CL

TL;DR: 提出了MME-RAG框架，通过多管理者专家协作和语义检索显著提升任务型对话中的细粒度实体识别性能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在任务型对话中面临领域适应和检索可控性挑战，难以实现精准的细粒度实体识别。

Method: MME-RAG框架将实体识别分为类型判别和跨度抽取两个阶段，采用轻量管理者判断实体类型，特定专家负责实体提取，同时辅以KeyInfo检索器注入语义对齐的少样例示范，无需额外训练即可精准适应不同领域。

Result: 在CrossNER、MIT-Movie、MIT-Restaurant及新构建的多领域客服数据集上，MME-RAG优于现有基线方法，且消融实验表明层次分解和KeyInfo引导检索对提升鲁棒性和跨域泛化性至关重要。

Conclusion: MME-RAG为任务型对话中的细粒度实体识别提供了一个可扩展且具解释性的自适应解决方案，显著增强了领域适应能力和识别精度。

Abstract: Fine-grained entity recognition is crucial for reasoning and decision-making in task-oriented dialogues, yet current large language models (LLMs) continue to face challenges in domain adaptation and retrieval controllability. We introduce MME-RAG, a Multi-Manager-Expert Retrieval-Augmented Generation framework that decomposes entity recognition into two coordinated stages: type-level judgment by lightweight managers and span-level extraction by specialized experts. Each expert is supported by a KeyInfo retriever that injects semantically aligned, few-shot exemplars during inference, enabling precise and domain-adaptive extraction without additional training. Experiments on CrossNER, MIT-Movie, MIT-Restaurant, and our newly constructed multi-domain customer-service dataset demonstrate that MME-RAG performs better than recent baselines in most domains. Ablation studies further show that both the hierarchical decomposition and KeyInfo-guided retrieval are key drivers of robustness and cross-domain generalization, establishing MME-RAG as a scalable and interpretable solution for adaptive dialogue understanding.

</details>


### [25] [Consistency Is the Key: Detecting Hallucinations in LLM Generated Text By Checking Inconsistencies About Key Facts](https://arxiv.org/abs/2511.12236)
*Raavi Gupta,Pranav Hari Panicker,Sumit Bhatia,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: 本文提出了一种名为CONFACTCHECK的高效幻觉检测方法，在无外部知识库支持且模型访问受限的情况下，基于一致性原理检测大型语言模型生成文本中的虚假信息。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽具强大文本生成能力，但常生成事实错误的幻觉内容，尤其在医疗、金融等领域存在严重风险。现有检测方法多需多次调用API，成本高且延迟大，且无法访问模型权重或微调。

Method: CONFACTCHECK方法基于一个简单直觉：生成文本中的事实探针的回答应在同一模型内部及不同模型间保持一致，无需任何外部知识库，通过检测这种一致性来判断幻觉。

Result: 在多个涵盖事实文本生成和开放式生成的数据集上的严格实证评估表明，CONFACTCHECK在资源消耗更少的情况下，幻觉检测准确率优于现有在类似条件下工作的基线方法。

Conclusion: CONFACTCHECK为无权重访问大语言模型的幻觉检测提供了一种高效准确的解决方案，降低了检测成本和延迟，具有实际应用潜力。

Abstract: Large language models (LLMs), despite their remarkable text generation capabilities, often hallucinate and generate text that is factually incorrect and not grounded in real-world knowledge. This poses serious risks in domains like healthcare, finance, and customer support. A typical way to use LLMs is via the APIs provided by LLM vendors where there is no access to model weights or options to fine-tune the model. Existing methods to detect hallucinations in such settings where the model access is restricted or constrained by resources typically require making multiple LLM API calls, increasing latency and API cost. We introduce CONFACTCHECK, an efficient hallucination detection approach that does not leverage any external knowledge base and works on the simple intuition that responses to factual probes within the generated text should be consistent within a single LLM and across different LLMs. Rigorous empirical evaluation on multiple datasets that cover both the generation of factual texts and the open generation shows that CONFACTCHECK can detect hallucinated facts efficiently using fewer resources and achieves higher accuracy scores compared to existing baselines that operate under similar conditions. Our code is available here.

</details>


### [26] [ViConBERT: Context-Gloss Aligned Vietnamese Word Embedding for Polysemous and Sense-Aware Representations](https://arxiv.org/abs/2511.12249)
*Khang T. Huynh,Dung H. Nguyen,Binh T. Nguyen*

Main category: cs.CL

TL;DR: 本文提出了ViConBERT，一种结合对比学习和释义蒸馏的越南语上下文化词嵌入模型，并构建了ViConWSD数据集用于语义理解评估，模型在词义消歧和语义相似性任务上表现优秀。


<details>
  <summary>Details</summary>
Motivation: 越南语缺乏用于细粒度语义理解的稳健模型和评测资源，而现有语义任务进展主要集中在高资源语言如英语。

Method: 设计ViConBERT框架，结合SimCLR对比学习和基于释义的知识蒸馏，提升词义表达能力；同时构建ViConWSD大规模合成数据集，覆盖词义消歧和上下文相似性评测。

Result: ViConBERT在词义消歧任务上取得F1值0.87，ViCon语义分类的AP达0.88，ViSim-400语义相似性Spearman相关系数为0.60，均优于强基线。

Conclusion: ViConBERT有效提升了越南语细粒度语义理解能力，验证了结合对比学习和释义蒸馏方法的有效性，所贡献的模型和数据集推动越南语语义处理研究。

Abstract: Recent advances in contextualized word embeddings have greatly improved semantic tasks such as Word Sense Disambiguation (WSD) and contextual similarity, but most progress has been limited to high-resource languages like English. Vietnamese, in contrast, still lacks robust models and evaluation resources for fine-grained semantic understanding. In this paper, we present ViConBERT, a novel framework for learning Vietnamese contextualized embeddings that integrates contrastive learning (SimCLR) and gloss-based distillation to better capture word meaning. We also introduce ViConWSD, the first large-scale synthetic dataset for evaluating semantic understanding in Vietnamese, covering both WSD and contextual similarity. Experimental results show that ViConBERT outperforms strong baselines on WSD (F1 = 0.87) and achieves competitive performance on ViCon (AP = 0.88) and ViSim-400 (Spearman's rho = 0.60), demonstrating its effectiveness in modeling both discrete senses and graded semantic relations. Our code, models, and data are available at https://github.com/tkhangg0910/ViConBERT

</details>


### [27] [Cmprsr: Abstractive Token-Level Question-Agnostic Prompt Compressor](https://arxiv.org/abs/2511.12281)
*Ivan Zakazov,Alexander Sharipov,Berke Argin,Oussama Gabouj,Kamel Charaf,Alexi Semiz,Lorenzo Drudi,Nicolas Baldwin,Robert West*

Main category: cs.CL

TL;DR: 本文提出一种利用较小的大语言模型(LLM)对输入进行压缩，从而降低使用大型黑盒LLM的成本的方法，并开发了一个覆盖25个模型的压缩能力基准。


<details>
  <summary>Details</summary>
Motivation: 使用大型黑盒LLM成本高昂，故希望通过较小LLM实现输入压缩以降低成本。

Method: 构建LLM压缩评测基准，优化最佳小型模型gpt-4.1-mini的压缩性能，选出表现最优的开源模型Qwen3-4B并采用SFT和GRPO方法进行后续训练以提升压缩率保持及下游任务表现，最终得到模型Cmprsr。

Result: Cmprsr在多个数据集和不同输入长度的测试中，优于提取式和普通生成式压缩方法，且能够准确控制压缩率。

Conclusion: 通过小模型辅助压缩输入，既可有效保持信息和下游性能，又能灵活调整压缩率，实现成本与质量的良好平衡。

Abstract: Motivated by the high costs of using black-box Large Language Models (LLMs), we introduce a novel prompt compression paradigm, under which we use smaller LLMs to compress inputs for the larger ones. We present the first comprehensive LLM-as-a-compressor benchmark spanning 25 open- and closed-source models, which reveals significant disparity in models' compression ability in terms of (i) preserving semantically important information (ii) following the user-provided compression rate (CR). We further improve the performance of gpt-4.1-mini, the best overall vanilla compressor, with Textgrad-based compression meta-prompt optimization. We also identify the most promising open-source vanilla LLM - Qwen3-4B - and post-train it with a combination of supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO), pursuing the dual objective of CR adherence and maximizing the downstream task performance. We call the resulting model Cmprsr and demonstrate its superiority over both extractive and vanilla abstractive compression across the entire range of compression rates on lengthy inputs from MeetingBank and LongBench as well as short prompts from GSM8k. The latter highlights Cmprsr's generalizability across varying input lengths and domains. Moreover, Cmprsr closely follows the requested compression rate, offering fine control over the cost-quality trade-off.

</details>


### [28] [AugAbEx : Way Forward for Extractive Case Summarization](https://arxiv.org/abs/2511.12290)
*Purnima Bindal,Vikas Kumar,Sagar Rathore,Vasudha Bhatnagar*

Main category: cs.CL

TL;DR: 本文针对法律判决摘要生成，提出了一种利用已有抽象摘要生成对应抽取式摘要的方法，并对七个法律案例摘要数据集进行了扩展和评估，旨在为法律文档自动摘要研究提供丰富资源。


<details>
  <summary>Details</summary>
Motivation: 法律判决摘要语言复杂且长，自动摘要具有挑战，且抽象式摘要容易出现误差，抽取式摘要有较大应用价值，但缺乏高质量的抽取式标注数据。

Method: 设计一个轻量透明的流程，将现有的抽象摘要转换为对应的抽取式摘要，并对七个案例数据集进行扩展，保障专家意见的传递。

Result: 通过结构、词汇和语义等多维度对比评估，验证了扩展的抽取式摘要在质量上与原始抽象摘要的高度一致；分析了两种摘要的领域信息差异。

Conclusion: 扩展的数据集将公开发布，供学术界使用，促进法律文档自动摘要研究的发展，尤其是抽取式摘要应用的推广。

Abstract: Summarization of legal judgments poses a heavy cognitive burden on law practitioners due to the complexity of the language, context-sensitive legal jargon, and the length of the document. Therefore, the automatic summarization of legal documents has attracted serious attention from natural language processing researchers. Since the abstractive summaries of legal documents generated by deep neural methods remain prone to the risk of misrepresenting nuanced legal jargon or overlooking key contextual details, we envisage a rising trend toward the use of extractive case summarizers.
  Given the high cost of human annotation for gold standard extractive summaries, we engineer a light and transparent pipeline that leverages existing abstractive gold standard summaries to create the corresponding extractive gold standard versions. The approach ensures that the experts` opinions ensconced in the original gold standard abstractive summaries are carried over to the transformed extractive summaries. We aim to augment seven existing case summarization datasets, which include abstractive summaries, by incorporating corresponding extractive summaries and create an enriched data resource for case summarization research community. To ensure the quality of the augmented extractive summaries, we perform an extensive comparative evaluation with the original abstractive gold standard summaries covering structural, lexical, and semantic dimensions. We also compare the domain-level information of the two summaries. We commit to release the augmented datasets in the public domain for use by the research community and believe that the resource will offer opportunities to advance the field of automatic summarization of legal documents.

</details>


### [29] [Do LLMs and Humans Find the Same Questions Difficult? A Case Study on Japanese Quiz Answering](https://arxiv.org/abs/2511.12300)
*Naoya Sugiura,Kosuke Yamada,Yasuhiro Ogawa,Katsuhiko Toyama,Ryohei Sasano*

Main category: cs.CL

TL;DR: 本研究比较了大语言模型（LLMs）和人类在回答日语测验题目时的表现差异，发现LLMs在面对Wikipedia中未涵盖的内容以及需数值解答的问题时表现较弱。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs是否与人类在面对难题时表现相似，尤其是在日语测验中的困难点。

Method: 收集包含问题、答案及人类正确率的日语测验数据，对LLMs进行多种提示设置，比较LLMs与人类的正确率，并从两种角度分析差异。

Result: 实验显示LLMs在面对Wikipedia未涉及的答案以及需要数值答案的问题时，正确率明显低于人类。

Conclusion: LLMs虽然在许多NLP任务中表现优异，但在具体测验题目上仍存在弱点，特别是对知识覆盖度受限和数值问题的适应性较差。

Abstract: LLMs have achieved performance that surpasses humans in many NLP tasks. However, it remains unclear whether problems that are difficult for humans are also difficult for LLMs. This study investigates how the difficulty of quizzes in a buzzer setting differs between LLMs and humans. Specifically, we first collect Japanese quiz data including questions, answers, and correct response rate of humans, then prompted LLMs to answer the quizzes under several settings, and compare their correct answer rate to that of humans from two analytical perspectives. The experimental results showed that, compared to humans, LLMs struggle more with quizzes whose correct answers are not covered by Wikipedia entries, and also have difficulty with questions that require numerical answers.

</details>


### [30] [Don't Think of the White Bear: Ironic Negation in Transformer Models Under Cognitive Load](https://arxiv.org/abs/2511.12381)
*Logan Mann,Nayan Saxena,Sarah Tandon,Chenhao Sun,Savar Toteja,Kevin Zhu*

Main category: cs.CL

TL;DR: 本文研究了大语言模型在处理否定指令时出现的“反讽反弹”现象，即试图抑制某概念时反而增强了该概念的激活。经过两个实验发现，语义干扰会加剧反弹，重复则有助于抑制，且模型对中性与否定表述的区分能力影响反弹持续性。通过电路追踪分析揭示了模型内部机制，并发布了相关数据集供后续研究使用。


<details>
  <summary>Details</summary>
Motivation: 否定指令可能导致被抑制的概念在大脑中反而更易被激活，这种反常现象"反讽反弹"同样存在于大语言模型中。探究模型如何处理否定指令对理解和改进模型行为具有重要意义。

Method: 进行了两个实验：(1)在否定指令后通过改变干扰文本（语义、句法、重复）来测量反弹强度；(2)测试模型是否区分同一概念的中性与否定表述及其对反弹持续性的预测作用。同时使用电路追踪分析模型内部注意力机制。

Result: 反弹现象在否定后立即出现，并随着语义干扰或干扰时间延长而加剧，重复干扰有助于抑制反弹。模型对极性区分能力越强，反弹持续时间越长。电路分析显示中间层的特定注意力头会放大被禁止的词，而早期层则进行抑制。

Conclusion: 模型中反讽反弹的认知现象与其内部机制相关，长上下文干扰影响反弹效果。研究结果和数据集可为未来改进模型对否定指令的处理提供理论和实践支持。

Abstract: Negation instructions such as 'do not mention $X$' can paradoxically increase the accessibility of $X$ in human thought, a phenomenon known as ironic rebound. Large language models (LLMs) face the same challenge: suppressing a concept requires internally activating it, which may prime rebound instead of avoidance. We investigated this tension with two experiments. \textbf{(1) Load \& content}: after a negation instruction, we vary distractor text (semantic, syntactic, repetition) and measure rebound strength. \textbf{(2) Polarity separation}: We test whether models distinguish neutral from negative framings of the same concept and whether this separation predicts rebound persistence. Results show that rebound consistently arises immediately after negation and intensifies with longer or semantic distractors, while repetition supports suppression. Stronger polarity separation correlates with more persistent rebound. Together, these findings, complemented by a circuit tracing analysis that identifies sparse middle-layer attention heads amplifying forbidden tokens while early layers suppress, link cognitive predictions of ironic rebound with mechanistic insights into long-context interference. To support future work, we release ReboundBench, a dataset of $5,000$ systematically varied negation prompts designed to probe rebound in LLMs.

</details>


### [31] [From Phonemes to Meaning: Evaluating Large Language Models on Tamil](https://arxiv.org/abs/2511.12387)
*Jeyarajalingam Varsha,Menan Velayuthan,Sumirtha Karunakaran,Rasan Nivethiga,Kengatharaiyer Sarveswaran*

Main category: cs.CL

TL;DR: 该论文提出了第一个针对泰米尔语的语言评估基准ILAkkANAM，基于斯里兰卡学校考试题目，测试大型语言模型在低资源、形态丰富语言中的表现。结果显示闭源模型表现优于开源模型，且模型在复杂语言问题上的表现较差。


<details>
  <summary>Details</summary>
Motivation: 现有多语言基准多依赖译自英语的数据，无法真实反映泰米尔语等低资源语言的语言和文化特性，亟需专门的语言学评估标准。

Method: 构建基于820道斯里兰卡学校泰米尔语考试题的评测基准，将题目按五类语言学维度及事实知识分类，涵盖1-13年级，以标准化框架评估闭源和开源大型语言模型。

Result: Gemini 2.5取得最高整体表现，开源模型表现较弱，且所有模型在低年级题表现较好，随着语言复杂度提高表现下降。不同语言类别识别能力与整体表现无明显相关。

Conclusion: 模型表现更多反映接触数据的丰富程度而非真实语言理解；专门针对低资源语言的语言学评测基准有助于揭示模型不足，推动模型在此类语言上的改进。

Abstract: Large Language Models (LLMs) have shown strong generalization across tasks in high-resource languages; however, their linguistic competence in low-resource and morphologically rich languages such as Tamil remains largely unexplored. Existing multilingual benchmarks often rely on translated English datasets, failing to capture the linguistic and cultural nuances of the target language. To address this gap, we introduce ILAKKANAM, the first Tamil-specific linguistic evaluation benchmark manually curated using 820 questions from Sri Lankan school-level Tamil subject examination papers. Each question is annotated by trained linguists under five linguistic categories and a factual knowledge category, spanning Grades 1--13 to ensure broad linguistic coverage. We evaluate both closed-source and open-source LLMs using a standardized evaluation framework. Our results show that Gemini 2.5 achieves the highest overall performance, while open-source models lag behind, highlighting the gap in linguistic grounding. Category- and grade-wise analyses reveal that all models perform well on lower-grade questions but show a clear decline as linguistic complexity increases. Further, no strong correlation is observed between a model's overall performance and its ability to identify linguistic categories, suggesting that performance may be driven by exposure rather than genuine understanding.

</details>


### [32] [Probing Preference Representations: A Multi-Dimensional Evaluation and Analysis Method for Reward Models](https://arxiv.org/abs/2511.12464)
*Chenglong Wang,Yifu Huo,Yang Gan,Yongyu Mu,Qiaozhi He,Murun Yang,Bei Li,Chunliang Zhang,Tongran Liu,Anxiang Ma,Zhengtao Yu,Jingbo Zhu,Tong Xiao*

Main category: cs.CL

TL;DR: 本文提出了多维奖励模型基准（MRMBench）和推理时探测方法，以更全面评估奖励模型在不同偏好维度上的表现，并提升其解释性。


<details>
  <summary>Details</summary>
Motivation: 现有评估奖励模型的方法通常只在固定的成对排序测试集上测试，缺乏对各个偏好维度性能的详细信息，难以全面了解模型表现。

Method: 构建了包含六个偏好维度检测任务的MRMBench，并设计推理时探测方法以识别奖励预测过程中使用的偏好维度，从而增强模型可解释性。

Result: 实验表明MRMBench与大型语言模型的对齐性能高度相关，奖励模型在多维偏好捕捉上存在困难，推理时探测方法可作为置信度评估指标，提高对齐效果。

Conclusion: 多维评价和推理时探测有助于更准确评估和改进奖励模型，推动多目标优化在奖励建模中的应用，增强LLM的对齐和解释能力。

Abstract: Previous methods evaluate reward models by testing them on a fixed pairwise ranking test set, but they typically do not provide performance information on each preference dimension. In this work, we address the evaluation challenge of reward models by probing preference representations. To confirm the effectiveness of this evaluation method, we construct a Multi-dimensional Reward Model Benchmark (MRMBench), a collection of six probing tasks for different preference dimensions. We design it to favor and encourage reward models that better capture preferences across different dimensions. Furthermore, we introduce an analysis method, inference-time probing, which identifies the dimensions used during the reward prediction and enhances its interpretability. Through extensive experiments, we find that MRMBench strongly correlates with the alignment performance of large language models (LLMs), making it a reliable reference for developing advanced reward models. Our analysis of MRMBench evaluation results reveals that reward models often struggle to capture preferences across multiple dimensions, highlighting the potential of multi-objective optimization in reward modeling. Additionally, our findings show that the proposed inference-time probing method offers a reliable metric for assessing the confidence of reward predictions, which ultimately improves the alignment of LLMs.

</details>


### [33] [Assessing LLMs for Serendipity Discovery in Knowledge Graphs: A Case for Drug Repurposing](https://arxiv.org/abs/2511.12472)
*Mengying Wang,Chenhui Ma,Ao Jiao,Tuo Liang,Pengjun Lu,Shrinidhi Hegde,Yu Yin,Evren Gurkan-Cavusoglu,Yinghui Wu*

Main category: cs.CL

TL;DR: 该论文提出了一个面向知识图谱问答的惊喜答案生成任务，设计了SerenQA框架和评价指标，旨在评估大型语言模型发现意外且有价值答案的能力。


<details>
  <summary>Details</summary>
Motivation: 当前知识图谱问答系统侧重于给出高度相关且可预测的答案，缺乏利用大型语言模型挖掘意外惊喜答案的能力。

Method: 定义了惊喜感知的知识图谱问答任务，构建了SerenQA框架，包含基于相关性、新颖性和惊喜度的评价指标、专家标注的临床知识图谱基准，以及知识检索、子图推理和惊喜探索三个子任务的评估流程。

Result: 实验表明，尽管最新大型语言模型在知识检索表现良好，但在发现真正惊喜且有价值的答案方面仍有较大不足。

Conclusion: 该研究揭示了当前大型语言模型在生成意外且有价值答案方面的局限性，表明未来有很大改进空间，促进知识图谱问答系统更好地挖掘创新性信息。

Abstract: Large Language Models (LLMs) have greatly advanced knowledge graph question answering (KGQA), yet existing systems are typically optimized for returning highly relevant but predictable answers. A missing yet desired capacity is to exploit LLMs to suggest surprise and novel ("serendipitious") answers. In this paper, we formally define the serendipity-aware KGQA task and propose the SerenQA framework to evaluate LLMs' ability to uncover unexpected insights in scientific KGQA tasks. SerenQA includes a rigorous serendipity metric based on relevance, novelty, and surprise, along with an expert-annotated benchmark derived from the Clinical Knowledge Graph, focused on drug repurposing. Additionally, it features a structured evaluation pipeline encompassing three subtasks: knowledge retrieval, subgraph reasoning, and serendipity exploration. Our experiments reveal that while state-of-the-art LLMs perform well on retrieval, they still struggle to identify genuinely surprising and valuable discoveries, underscoring a significant room for future improvements. Our curated resources and extended version are released at: https://cwru-db-group.github.io/serenQA.

</details>


### [34] [SGuard-v1: Safety Guardrail for Large Language Models](https://arxiv.org/abs/2511.12497)
*JoonHo Lee,HyeonMin Cho,Jaewoong Yun,Hyunjae Lee,JunKyu Lee,Juree Seok*

Main category: cs.CL

TL;DR: 本文提出了轻量级安全护栏SGuard-v1，用于检测大型语言模型中的有害内容和对抗性提示，具有高效性和多语言支持。


<details>
  <summary>Details</summary>
Motivation: 为了解决大型语言模型在生成内容时可能出现的安全风险和对抗性攻击，提升人机对话的安全性和可信度。

Method: 设计了包括ContentFilter和JailbreakFilter两部分的模型，分别针对安全风险和对抗性提示进行检测，基于2B参数的Granite模型进行调优，结合约140万条训练数据，覆盖多语言和60种攻击类型。

Result: SGuard-v1在多个公共和专有安全基准测试中表现出先进的安全性能，同时模型轻量降低了部署成本，具备多类别安全预测和置信度评分，提升了可解释性。

Conclusion: SGuard-v1提供了一个高效、易部署且可解释的安全解决方案，已开源以推动AI安全领域的研究与应用。

Abstract: We present SGuard-v1, a lightweight safety guardrail for Large Language Models (LLMs), which comprises two specialized models to detect harmful content and screen adversarial prompts in human-AI conversational settings. The first component, ContentFilter, is trained to identify safety risks in LLM prompts and responses in accordance with the MLCommons hazard taxonomy, a comprehensive framework for trust and safety assessment of AI. The second component, JailbreakFilter, is trained with a carefully designed curriculum over integrated datasets and findings from prior work on adversarial prompting, covering 60 major attack types while mitigating false-unsafe classification. SGuard-v1 is built on the 2B-parameter Granite-3.3-2B-Instruct model that supports 12 languages. We curate approximately 1.4 million training instances from both collected and synthesized data and perform instruction tuning on the base model, distributing the curated data across the two component according to their designated functions. Through extensive evaluation on public and proprietary safety benchmarks, SGuard-v1 achieves state-of-the-art safety performance while remaining lightweight, thereby reducing deployment overhead. SGuard-v1 also improves interpretability for downstream use by providing multi-class safety predictions and their binary confidence scores. We release the SGuard-v1 under the Apache-2.0 License to enable further research and practical deployment in AI safety.

</details>


### [35] [QA-Noun: Representing Nominal Semantics via Natural Language Question-Answer Pairs](https://arxiv.org/abs/2511.12504)
*Maria Tseytlin,Paul Roit,Omri Abend,Ido Dagan,Ayal Klein*

Main category: cs.CL

TL;DR: 本文提出了QA-Noun，一种基于问答的方法，专注于捕捉名词中心的语义关系，并与QA-SRL结合，实现细粒度语义分解。


<details>
  <summary>Details</summary>
Motivation: 现有基于问答的语义方法主要关注谓词-论元关系，忽视了名词中心的语义表达。

Method: 定义九种针对名词的问答模板，覆盖显式句法和隐式语义角色，构建可解释的问答对，并发布了相关数据集和模型，与QA-SRL集成。

Result: QA-Noun实现了对AMR名词论元的几乎完全覆盖，还能捕捉额外的隐式语义关系，结合QA-SRL后，语义分解粒度提升超过130%。

Conclusion: QA-Noun有效补充了基于问答的语义框架，提供了全面且可扩展的细粒度语义分解方案，有助于跨文本的语义对齐。

Abstract: Decomposing sentences into fine-grained meaning units is increasingly used to model semantic alignment. While QA-based semantic approaches have shown effectiveness for representing predicate-argument relations, they have so far left noun-centered semantics largely unaddressed. We introduce QA-Noun, a QA-based framework for capturing noun-centered semantic relations. QA-Noun defines nine question templates that cover both explicit syntactical and implicit contextual roles for nouns, producing interpretable QA pairs that complement verbal QA-SRL. We release detailed guidelines, a dataset of over 2,000 annotated noun mentions, and a trained model integrated with QA-SRL to yield a unified decomposition of sentence meaning into individual, highly fine-grained, facts. Evaluation shows that QA-Noun achieves near-complete coverage of AMR's noun arguments while surfacing additional contextually implied relations, and that combining QA-Noun with QA-SRL yields over 130\% higher granularity than recent fact-based decomposition methods such as FactScore and DecompScore. QA-Noun thus complements the broader QA-based semantic framework, forming a comprehensive and scalable approach to fine-grained semantic decomposition for cross-text alignment.

</details>


### [36] [TAdaRAG: Task Adaptive Retrieval-Augmented Generation via On-the-Fly Knowledge Graph Construction](https://arxiv.org/abs/2511.12520)
*Jie Zhang,Bo Tang,Wanzi Shao,Wenqiang Wei,Jihao Zhao,Jianqing Zhu,Zhiyu li,Wen Xi,Zehao Lin,Feiyu Xiong,Yanchao Tan*

Main category: cs.CL

TL;DR: TAdaRAG通过任务自适应知识图构建改进了基于检索增强生成的语言模型，有效减少信息丢失和无关信息，提升了推理准确性和生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法因输入长度限制需将知识拆分，导致信息丢失和生成虚假内容，同时传统RAG检索无结构知识引入无关细节，影响推理准确性。

Method: 提出TAdaRAG框架，利用意图驱动路由机制引导领域专项模板提取，通过监督微调和基于强化学习的隐式提取，实时构建任务自适应知识图，确保知识简洁、一致且无冗余。

Result: 在六个公开基准及一个真实业务基准上，TAdaRAG在三种主模型中均优于现有方法，显示了其强泛化能力和多领域长文本任务的实用效果。

Conclusion: TAdaRAG有效解决了传统RAG的信息丢失与无关信息问题，通过任务适配的知识图构建提升了语言模型的推理和生成性能，具备广泛应用潜力。

Abstract: Retrieval-Augmented Generation (RAG) improves large language models by retrieving external knowledge, often truncated into smaller chunks due to the input context window, which leads to information loss, resulting in response hallucinations and broken reasoning chains. Moreover, traditional RAG retrieves unstructured knowledge, introducing irrelevant details that hinder accurate reasoning. To address these issues, we propose TAdaRAG, a novel RAG framework for on-the-fly task-adaptive knowledge graph construction from external sources. Specifically, we design an intent-driven routing mechanism to a domain-specific extraction template, followed by supervised fine-tuning and a reinforcement learning-based implicit extraction mechanism, ensuring concise, coherent, and non-redundant knowledge integration. Evaluations on six public benchmarks and a real-world business benchmark (NowNewsQA) across three backbone models demonstrate that TAdaRAG outperforms existing methods across diverse domains and long-text tasks, highlighting its strong generalization and practical effectiveness.

</details>


### [37] [Mitigating Length Bias in RLHF through a Causal Lens](https://arxiv.org/abs/2511.12573)
*Hyeonji Kim,Sujeong Oh,Sanghack Lee*

Main category: cs.CL

TL;DR: 本文提出了一种因果框架和反事实数据增强方法，用于缓解强化学习中基于人类反馈的奖励模型的长度偏差问题，使奖励模型能够独立于响应长度评估内容质量，从而生成更简洁且内容聚焦的输出。


<details>
  <summary>Details</summary>
Motivation: 当前RLHF训练的奖励模型存在系统性偏向较长回答的问题，将冗长误判为高质量，影响模型性能。

Method: 构建因果框架，利用反事实数据增强生成两类对比样本（长度不同但内容相似，内容不同但长度相似），以训练奖励模型独立评估内容质量与响应长度。

Result: 实证结果表明该方法有效减少了奖励分配中的长度偏差，使策略模型生成的回答更简洁且更关注内容质量。

Conclusion: 提出的方法有效缓解了RLHF奖励建模中的长度偏差，提升了奖励模型的鲁棒性和内容敏感度，促进更优质的文本生成。

Abstract: Reinforcement learning from human feedback (RLHF) is widely used to align large language models (LLMs) with human preferences. However, RLHF-trained reward models often exhibit length bias -- a systematic tendency to favor longer responses by conflating verbosity with quality. We propose a causal framework for analyzing and mitigating length bias in RLHF reward modeling. Central to our approach is a counterfactual data augmentation method that generates response pairs designed to isolate content quality from verbosity. These counterfactual examples are then used to train the reward model, enabling it to assess responses based on content quality independently of verbosity. Specifically, we construct (1) length-divergent pairs with similar content and (2) content-divergent pairs of similar length. Empirical evaluations show that our method reduces length bias in reward assignment and leads to more concise, content-focused outputs from the policy model. These findings demonstrate that the proposed approach effectively reduces length bias and improves the robustness and content sensitivity of reward modeling in RLHF pipelines.

</details>


### [38] [MMWOZ: Building Multimodal Agent for Task-oriented Dialogue](https://arxiv.org/abs/2511.12586)
*Pu-Hai Yang,Heyan Huang,Heng-Da Xu,Fanshu Sun,Xian-Ling Mao,Chaoxu Mu*

Main category: cs.CL

TL;DR: 该文提出了一个名为MMWOZ的多模态任务导向对话新数据集，并开发了对应的前端GUI和自动转换脚本，同时提出了多模态对话模型MATE作为基线模型。


<details>
  <summary>Details</summary>
Motivation: 传统任务导向对话系统依赖后端API，但现实中普遍存在前端GUI且缺乏定制后端API，导致系统难以应用于实际场景。

Method: 开发基于网页风格的前端GUI；设计自动脚本将原有对话状态和系统动作转换为GUI操作指令；收集网页快照与对应操作指令；提出多模态模型MATE作为基线。

Result: 成功构建MMWOZ数据集及前端环境，提出的MATE模型在多模态任务导向对话中进行了全面实验分析，验证了其实用性。

Conclusion: 通过结合前端GUI和多模态模型，填补了传统任务导向对话系统与实际应用之间的鸿沟，为实际场景下多模态对话系统构建提供了新思路。

Abstract: Task-oriented dialogue systems have garnered significant attention due to their conversational ability to accomplish goals, such as booking airline tickets for users. Traditionally, task-oriented dialogue systems are conceptualized as intelligent agents that interact with users using natural language and have access to customized back-end APIs. However, in real-world scenarios, the widespread presence of front-end Graphical User Interfaces (GUIs) and the absence of customized back-end APIs create a significant gap for traditional task-oriented dialogue systems in practical applications. In this paper, to bridge the gap, we collect MMWOZ, a new multimodal dialogue dataset that is extended from MultiWOZ 2.3 dataset. Specifically, we begin by developing a web-style GUI to serve as the front-end. Next, we devise an automated script to convert the dialogue states and system actions from the original dataset into operation instructions for the GUI. Lastly, we collect snapshots of the web pages along with their corresponding operation instructions. In addition, we propose a novel multimodal model called MATE (Multimodal Agent for Task-oriEnted dialogue) as the baseline model for the MMWOZ dataset. Furthermore, we conduct comprehensive experimental analysis using MATE to investigate the construction of a practical multimodal agent for task-oriented dialogue.

</details>


### [39] [Group-Aware Reinforcement Learning for Output Diversity in Large Language Models](https://arxiv.org/abs/2511.12596)
*Oron Anschel,Alon Shoshan,Adam Botach,Shunit Haviv Hakimi,Asaf Gendler,Emanuel Ben Baruch,Nadav Bhonker,Igor Kviatkovsky,Manoj Aggarwal,Gerard Medioni*

Main category: cs.CL

TL;DR: 本文提出了Group-Aware Policy Optimization (GAPO)方法，通过在群体层面计算奖励，提升大语言模型生成结果的多样性和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）存在模式崩溃问题，即在多种有效答案场景下重复生成相同答案，限制了其多样性。

Method: 在Group Relative Policy Optimization (GRPO)基础上推出GAPO，利用群体层面奖励，采用频率感知奖励函数促进对有效完成结果的均匀采样。

Result: GAPO训练的模型生成的回答更加多样且有效，在多种基准测试（如GSM8K, MATH, HumanEval, MMLU-Pro）中提高了多样性且不牺牲准确率。

Conclusion: GAPO有效提升了大语言模型的响应多样性和覆盖率，适用于开放式提示，且性能稳健，代码将公开。

Abstract: Large Language Models (LLMs) often suffer from mode collapse, repeatedly generating the same few completions even when many valid answers exist, limiting their diversity across a wide range of tasks. We introduce Group-Aware Policy Optimization (GAPO), a simple extension of the recent and popular Group Relative Policy Optimization (GRPO) that computes rewards over the group as a whole. GAPO enables learning from the group-level properties such as diversity and coverage. We demonstrate GAPO using a frequency-aware reward function that encourages uniform sampling over valid LLM completions, and show that GAPO-trained models produce valid and more diverse model responses. Beyond this setup, GAPO generalizes to open-ended prompts and improves response diversity without compromising accuracy on standard LLM benchmarks (GSM8K, MATH, HumanEval, MMLU-Pro). Our code will be made publicly available.

</details>


### [40] [Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data](https://arxiv.org/abs/2511.12609)
*Yunxin Li,Xinyu Chen,Shenyuan Jiang,Haoyuan Shi,Zhenyu Liu,Xuanyu Zhang,Nanhao Deng,Zhenran Xu,Yicheng Ma,Meishan Zhang,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: Uni-MoE 2.0 是来自 Lychee 家族的全开源多模态大模型，基于Qwen2.5-7B架构，通过动态专家网络设计、渐进式训练策略和多模态数据匹配技术，实现了卓越的多模态理解与生成能力。


<details>
  <summary>Details</summary>
Motivation: 提升多模态模型在语言中心的理解、推理和生成能力，兼顾效率与性能，支持图像、文本和语音多模态输入与输出。

Method: 构建动态容量的Mixture-of-Experts架构，设计Omni-Modality 3D RoPE进行时空跨模态对齐；采用渐进式有监督微调策略，结合平衡数据及迭代强化训练（GSPO-DPO）；使用约75B多模态开源数据训练，增加特殊生成标记。

Result: 在85个基准测试中达到或接近最先进水平，超越训练规模更大的Qwen2.5-Omni，尤其在视频理解、多模态理解、视听推理及长语音处理上表现显著提升。

Conclusion: Uni-MoE 2.0通过创新模型结构和训练方法，实现了高效且强大的多模态理解与生成能力，成为领先的开源全模态大模型。

Abstract: We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee's Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the Qwen2.5-7B dense architecture, we build Uni-MoE-2.0-Omni from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech. Architecturally, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer. For training, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning. Data-wise, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues. Extensive evaluation across 85 benchmarks demonstrates that our model achieves SOTA or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (reducing WER by 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.

</details>


### [41] [Knots: A Large-Scale Multi-Agent Enhanced Expert-Annotated Dataset and LLM Prompt Optimization for NOTAM Semantic Parsing](https://arxiv.org/abs/2511.12630)
*Maoqi Liu,Quan Fang,Yang Yang,Can Zhao,Kaiquan Cai*

Main category: cs.CL

TL;DR: 本论文提出了一个新的任务——NOTAM语义解析，旨在深入理解和解析航空通知中的隐含语义，通过构建专业标注的数据集Knots，提升自动化处理的效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在表层任务，缺乏对NOTAM公告复杂语义和隐含推理的深入理解，限制了自动化解析能力。

Method: 提出NOTAM语义解析任务，结合航空领域知识进行语义推理，构建了包含12347条专家标注NOTAM的高质量数据集Knots，采用多智能体协作框架进行数据标注，并系统评估了多种提示工程和模型适应技术。

Result: 实验结果表明所提方法大幅提升了航空文本的理解和处理能力，验证了语义解析任务和数据集的有效性。

Conclusion: 该研究为自动NOTAM分析系统提供了有力支持和宝贵见解，通过深度语义解析推动了航空领域信息处理的进步。

Abstract: Notice to Air Missions (NOTAMs) serve as a critical channel for disseminating key flight safety information, yet their complex linguistic structures and implicit reasoning pose significant challenges for automated parsing. Existing research mainly focuses on surface-level tasks such as classification and named entity recognition, lacking deep semantic understanding. To address this gap, we propose NOTAM semantic parsing, a task emphasizing semantic inference and the integration of aviation domain knowledge to produce structured, inference-rich outputs. To support this task, we construct Knots (Knowledge and NOTAM Semantics), a high-quality dataset of 12,347 expert-annotated NOTAMs covering 194 Flight Information Regions, enhanced through a multi-agent collaborative framework for comprehensive field discovery. We systematically evaluate a wide range of prompt-engineering strategies and model-adaptation techniques, achieving substantial improvements in aviation text understanding and processing. Our experimental results demonstrate the effectiveness of the proposed approach and offer valuable insights for automated NOTAM analysis systems. Our code is available at: https://github.com/Estrellajer/Knots.

</details>


### [42] [Reason-KE++: Aligning the Process, Not Just the Outcome, for Faithful LLM Knowledge Editing](https://arxiv.org/abs/2511.12661)
*Yuchen Wu,Liang Ding,Li Shen,Dacheng Tao*

Main category: cs.CL

TL;DR: 本论文提出了Reason-KE++框架，通过过程层面强化学习，提升大语言模型在复杂多跳推理任务中的事实可信度，实现了更高的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有基于微调（SFT）的方法虽然先进，但存在“真实性差距”，模型倾向于模仿格式而非进行合理推理，导致关键信息被覆盖，产生事实错误。

Method: 提出Reason-KE++框架，结合监督微调和过程感知的强化学习，设计阶段感知奖励机制，对推理中间步骤进行密集监督，避免仅关注最终结果带来的推理完整性崩溃。

Result: 框架在MQUAKE-CF-3k数据集上取得95.48%的新状态表现，比现有方法提升5.28%，显著改善了推理过程中事实的可信度和整体准确率。

Conclusion: 为了构建值得信赖的大语言模型，对复杂多跳推理任务而言，需对推理过程进行校准，而非仅关注最终结果，过程感知的强化学习是实现这一目标的有效方法。

Abstract: Aligning Large Language Models (LLMs) to be faithful to new knowledge in complex, multi-hop reasoning tasks is a critical, yet unsolved, challenge. We find that SFT-based methods, e.g., Reason-KE, while state-of-the-art, suffer from a "faithfulness gap": they optimize for format mimicry rather than sound reasoning. This gap enables the LLM's powerful parametric priors to override new contextual facts, resulting in critical factual hallucinations (e.g., incorrectly reasoning "Houston" from "NASA" despite an explicit edit). To solve this core LLM alignment problem, we propose Reason-KE++, an SFT+RL framework that instills process-level faithfulness. Its core is a Stage-aware Reward mechanism that provides dense supervision for intermediate reasoning steps (e.g., Decomposition, Sub-answer Correctness). Crucially, we identify that naive outcome-only RL is a deceptive trap for LLM alignment: it collapses reasoning integrity (e.g., 19.00% Hop acc) while superficially boosting final accuracy. Our process-aware framework sets a new SOTA of 95.48% on MQUAKE-CF-3k (+5.28%), demonstrating that for complex tasks, aligning the reasoning process is essential for building trustworthy LLMs.

</details>


### [43] [Improving Direct Persian-English Speech-to-Speech Translation with Discrete Units and Synthetic Parallel Data](https://arxiv.org/abs/2511.12690)
*Sina Rashidi,Hossein Sameti*

Main category: cs.CL

TL;DR: 本文提出了一种针对波斯语到英语的直接语音到语音翻译系统，并通过合成平行语音数据缓解数据稀缺问题，实现了性能提升。


<details>
  <summary>Details</summary>
Motivation: 直接S2ST模型需要大量源语和目标语的平行语音数据，但低资源语言如波斯语的数据极为缺乏。

Method: 使用基于conformer的编码器（预训练初始化）、因果变压器解码器和基于单位的神经声码器构建模型；通过大语言模型翻译波斯语转录文本，并结合零样本文本转语音系统生成合成英语语音，构建新的平行语音语料库。

Result: 利用合成数据使平行语料量增加约6倍，在CVSS波斯语-英语数据集上，模型ASR BLEU指标较基线提升4.6。

Conclusion: 结合自监督预训练、离散语音单元和合成平行语料，有效提升了低资源语言对的直接语音到语音翻译性能。

Abstract: Direct speech-to-speech translation (S2ST), in which all components are trained jointly, is an attractive alternative to cascaded systems because it offers a simpler pipeline and lower inference latency. However, direct S2ST models require large amounts of parallel speech data in the source and target languages, which are rarely available for low-resource languages such as Persian. This paper presents a direct S2ST system for translating Persian speech into English speech, as well as a pipeline for synthetic parallel Persian-English speech generation. The model comprises three components: (1) a conformer-based encoder, initialized from self-supervised pre-training, maps source speech to high-level acoustic representations; (2) a causal transformer decoder with relative position multi-head attention translates these representations into discrete target speech units; (3) a unit-based neural vocoder generates waveforms from the predicted discrete units. To mitigate the data scarcity problem, we construct a new Persian-English parallel speech corpus by translating Persian speech transcriptions into English using a large language model and then synthesizing the corresponding English speech with a state-of-the-art zero-shot text-to-speech system. The resulting corpus increases the amount of available parallel speech by roughly a factor of six. On the Persian-English portion of the CVSS corpus, the proposed model achieves improvement of 4.6 ASR BLEU with the synthetic data over direct baselines. These results indicate that combining self-supervised pre-training, discrete speech units, and synthetic parallel data is effective for improving direct S2ST in low-resource language pairs such as Persian-English

</details>


### [44] [Evolve the Method, Not the Prompts: Evolutionary Synthesis of Jailbreak Attacks on LLMs](https://arxiv.org/abs/2511.12710)
*Yunhao Chen,Xin Wang,Juncheng Li,Yixu Wang,Jie Li,Yan Teng,Yingchun Wang,Xingjun Ma*

Main category: cs.CL

TL;DR: 本文提出了EvoSynth，一种自主演化生成攻击代码的新型自动红队框架，突破了传统攻击策略有限的创新能力，实现了85.5%的高成功率并提升攻击多样性。


<details>
  <summary>Details</summary>
Motivation: 现有自动红队框架受限于只能组合和优化已有攻击策略，缺乏自主发明全新攻击方法的能力。

Method: EvoSynth采用多智能体系统，自主设计、演化及执行新的代码级攻击算法，并通过代码自我修正循环提高攻击效果。

Result: 实验证明EvoSynth在针对高强度模型Claude-Sonnet-4.5时，攻击成功率达85.5%，且攻击多样性显著优于现有方法。

Conclusion: EvoSynth开创了基于演化综合方法生成新型攻击的研究方向，极大增强了自动红队的创新能力和攻击效果。已开源代码以推进该领域研究。

Abstract: Automated red teaming frameworks for Large Language Models (LLMs) have become increasingly sophisticated, yet they share a fundamental limitation: their jailbreak logic is confined to selecting, combining, or refining pre-existing attack strategies. This binds their creativity and leaves them unable to autonomously invent entirely new attack mechanisms. To overcome this gap, we introduce \textbf{EvoSynth}, an autonomous framework that shifts the paradigm from attack planning to the evolutionary synthesis of jailbreak methods. Instead of refining prompts, EvoSynth employs a multi-agent system to autonomously engineer, evolve, and execute novel, code-based attack algorithms. Crucially, it features a code-level self-correction loop, allowing it to iteratively rewrite its own attack logic in response to failure. Through extensive experiments, we demonstrate that EvoSynth not only establishes a new state-of-the-art by achieving an 85.5\% Attack Success Rate (ASR) against highly robust models like Claude-Sonnet-4.5, but also generates attacks that are significantly more diverse than those from existing methods. We release our framework to facilitate future research in this new direction of evolutionary synthesis of jailbreak methods. Code is available at: https://github.com/dongdongunique/EvoSynth.

</details>


### [45] [Adaptive Focus Memory for Language Models](https://arxiv.org/abs/2511.12712)
*Christopher Cruz*

Main category: cs.CL

TL;DR: 本文提出Adaptive Focus Memory (AFM)，一种动态上下文管理方法，通过对话信息赋予不同保真度，有效减少对话中上下文的计算成本，同时保证安全相关信息的传递。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在多轮对话中受到固定上下文窗口和简单记忆策略的限制，简单回放整段对话计算成本高，静态摘要或近期策略可能丢失关键安全信息。

Method: AFM为过去的消息分配三种保真度等级（FULL、COMPRESSED、PLACEHOLDER），依据与当前查询的语义相似度、时间衰减权重和重要性分类，在严格的token预算内按时间顺序打包信息，优先保留最相关的对话内容。

Result: 在一个关于有严重花生过敏用户的安全性基准测试中，AFM在短中长对话里均能保留过敏信息，安全性表现与整段回放方法一致，但token使用量降低了66%。

Conclusion: AFM实现了在保证安全和事实连续性的前提下，极大节省了推理成本，且其模块化Python实现兼容OpenAI API和离线操作，便于实际应用。

Abstract: Large language models (LLMs) are increasingly deployed in multi-turn dialogue settings, but their behavior is still bottlenecked by fixed context windows and naive memory strategies. Replaying the full conversation at every turn is simple but expensive, while static summarization or recency-only heuristics often erase safety-critical user details. We present Adaptive Focus Memory (AFM), a dynamic context manager that assigns each past message one of three fidelity levels -- FULL, COMPRESSED, or PLACEHOLDER -- based on semantic similarity to the current query, half-life recency weighting, and importance classification. AFM packs messages chronologically under a strict token budget, preferring high fidelity for the most relevant turns while aiming to preserve a cheap trace of the dialogue. In a safety-oriented benchmark involving a user with a severe peanut allergy planning a trip to Thailand, AFM retains the allergy across both short and medium-length conversations, matches the safety performance of naive replay, and cuts average token usage by 66% relative to a replay baseline. We release a modular Python implementation of AFM designed for OpenAI-compatible APIs and offline operation, enabling practitioners to reduce inference cost without sacrificing safety or factual continuity in the evaluated scenario.

</details>


### [46] [On the Brittleness of LLMs: A Journey around Set Membership](https://arxiv.org/abs/2511.12728)
*Lea Hergert,Gábor Berend,Mario Szegedy,Gyorgy Turan,Márk Jelasity*

Main category: cs.CL

TL;DR: 大型语言模型在复杂推理任务上表现出超人能力，但在基本的集合成员查询任务上表现不稳定，显示其对集合概念的理解存在缺陷。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在复杂推理任务中表现优异，但在简单的集合成员查询任务中经常失败，研究者希望通过简化任务和扩大规模来揭示其基本的失败模式，提高模型的可靠性和可解释性。

Method: 通过设计简单但具有代表性的集合成员查询任务（例如判断某元素是否属于特定集合），系统地评估不同提示语、语义结构、元素顺序和模型选择对模型表现的影响，进行大规模实验分析。

Result: 研究发现大型语言模型在基本的集合成员查询任务中的表现极不稳定且难以预测，表明其对集合概念的理解是零散且复杂的。

Conclusion: 利用简单且可扩展的集合成员查询任务进行大规模实验，能够全面映射和分析大型语言模型的失败模式，为模型评估提供了一种有效的方法论。

Abstract: Large language models (LLMs) achieve superhuman performance on complex reasoning tasks, yet often fail on much simpler problems, raising concerns about their reliability and interpretability. We investigate this paradox through a focused study with two key design features: simplicity, to expose basic failure modes, and scale, to enable comprehensive controlled experiments. We focus on set membership queries -- among the most fundamental forms of reasoning -- using tasks like ``Is apple an element of the set \{pear, plum, apple, raspberry\}?''. We conduct a systematic empirical evaluation across prompt phrasing, semantic structure, element ordering, and model choice. Our large-scale analysis reveals that LLM performance on this elementary task is consistently brittle, and unpredictable across all dimensions, suggesting that the models' ``understanding'' of the set concept is fragmented and convoluted at best. Our work demonstrates that the large-scale experiments enabled by the simplicity of the problem allow us to map and analyze the failure modes comprehensively, making this approach a valuable methodology for LLM evaluation in general.

</details>


### [47] [Evidence of Phase Transitions in Small Transformer-Based Language Models](https://arxiv.org/abs/2511.12768)
*Noah Hong,Tao Hong*

Main category: cs.CL

TL;DR: 本论文研究了语言模型训练中的相变现象，发现即使是小规模模型也存在早期训练阶段的相变，且这种相变能在线性训练空间中被检测到。


<details>
  <summary>Details</summary>
Motivation: 以往研究多关注大型模型和对数刻度下的相变，本研究探讨相变是否普遍存在于小模型、是否能在未变换刻度的线性空间中直接观测、以及是否可在训练早期发现。

Method: 训练一个基于GPT的小型字符级语言模型，通过分析词汇使用变化（平均词长、正确与错误词数、词汇多样性）并应用Poisson及亚Poisson统计方法来量化词汇的连接和重组，从而检测训练过程中的相变。

Result: 发现训练过程中存在明显相变点，该相变在标准损失和验证曲线中不可见，但通过词汇和统计方面的探针能够揭示。相变不仅出现在大型模型中，也可在小模型训练早期被检测。

Conclusion: 相变是语言模型训练的普遍现象，具有非线性动力学特征，强调使用定制指标以揭示训练中的相变行为，为理解语言模型训练机理提供新视角。

Abstract: Phase transitions have been proposed as the origin of emergent abilities in large language models (LLMs), where new capabilities appear abruptly once models surpass critical thresholds of scale. Prior work, such as that of Wei et al., demonstrated these phenomena under model and data scaling, with transitions revealed after applying a log scale to training compute. In this work, we ask three complementary questions: (1) Are phase transitions unique to large models, or can they also be observed in small transformer-based language models? (2) Can such transitions be detected directly in linear training space, rather than only after log rescaling? and (3) Can these transitions emerge at early stages of training? To investigate, we train a small GPT-style transformer on a character-level corpus and analyze the evolution of vocabulary usage throughout training. We track the average word length, the number of correct versus incorrect words, and shifts in vocabulary diversity. Building on these measures, we apply Poisson and sub-Poisson statistics to quantify how words connect and reorganize. This combined analysis reveals a distinct transition point during training. Notably, these transitions are not apparent in standard loss or validation curves, but become visible through our vocabulary- and statistics-based probes. Our findings suggest that phase-transition reorganizations are a general feature of language model training, observable even in modest models, detectable directly in linear training space, and occurring surprisingly early as coherence emerges. This perspective provides new insight into the nonlinear dynamics of language model training and underscores the importance of tailored metrics for uncovering phase transition behaviors

</details>


### [48] [LLM Reinforcement in Context](https://arxiv.org/abs/2511.12782)
*Thomas Rivasseau*

Main category: cs.CL

TL;DR: 该论文探讨通过在用户输入中周期性添加“中断”控制句来增强大型语言模型的对齐能力，以应对随输入长度增加的模型越狱风险。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型对齐研究主要关注通过训练和提示来防御对抗攻击，但随着用户输入或对话长度增加，模型被越狱的概率也随之增加，缺少可随输入长度扩展的对齐方法。

Method: 提出在用户输入中每隔一定token数插入“中断”控制句，防止模型产生不良行为，并将此方法推广到思维链过程以防止模型产生策划行为。

Result: 通过引入中断机制，有望有效降低长输入下模型越狱的风险，增强模型对齐效果。

Conclusion: 插入控制“中断”句子是一种可扩展的对齐增强策略，适应多长输入，有助于防止模型不当行为和策划。

Abstract: Current Large Language Model alignment research mostly focuses on improving model robustness against adversarial attacks and misbehavior by training on examples and prompting. Research has shown that LLM jailbreak probability increases with the size of the user input or conversation length. There is a lack of appropriate research into means of strengthening alignment which also scale with user input length. We propose interruptions as a possible solution to this problem. Interruptions are control sentences added to the user input approximately every x tokens for some arbitrary x. We suggest that this can be generalized to the Chain-of-Thought process to prevent scheming.

</details>


### [49] [Evaluating Autoformalization Robustness via Semantically Similar Paraphrasing](https://arxiv.org/abs/2511.12784)
*Hayden Moore,Asfahan Shah*

Main category: cs.CL

TL;DR: 研究了大型语言模型（LLMs）在自动形式化中的鲁棒性，发现语义相似的自然语言复述会显著影响模型生成的形式化证明质量。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在自动形式化上表现出色，但在处理语义相似的复述自然语言输入时存在鲁棒性不足，影响生成形式化证明的可靠性。

Method: 利用MiniF2F和ProofNet等正式基准数据集，用两种现代LLM生成语义相似的自然语言复述语句，并在两个模型间交叉评估这些复述语句的语义和编译有效性。

Result: 发现模型对语义相似的复述输入表现出性能波动，轻微的语言变化对模型输出有显著影响。

Conclusion: 大型语言模型在自动形式化任务中存在对自然语言复述敏感的问题，需要进一步提升模型对语义变异的鲁棒性。

Abstract: Large Language Models (LLMs) have recently emerged as powerful tools for autoformalization. Despite their impressive performance, these models can still struggle to produce grounded and verifiable formalizations. Recent work in text-to-SQL, has revealed that LLMs can be sensitive to paraphrased natural language (NL) inputs, even when high degrees of semantic fidelity are preserved (Safarzadeh, Oroojlooyjadid, and Roth 2025). In this paper, we investigate this claim in the autoformalization domain. Specifically, we evaluate the robustness of LLMs generating formal proofs with semantically similar paraphrased NL statements by measuring semantic and compilation validity. Using the formal benchmarks MiniF2F (Zheng, Han, and Polu 2021) and Lean 4 version of ProofNet (Xin et al. 2024), and two modern LLMs, we generate paraphrased natural language statements and cross-evaluate these statements across both models. The results of this paper reveal performance variability across paraphrased inputs, demonstrating that minor shifts in NL statements can significantly impact model outputs.

</details>


### [50] [BioMedJImpact: A Comprehensive Dataset and LLM Pipeline for AI Engagement and Scientific Impact Analysis of Biomedical Journals](https://arxiv.org/abs/2511.12821)
*Ruiyu Wang,Yuzhang Xie,Xiao Hu,Carl Yang,Jiaying Lu*

Main category: cs.CL

TL;DR: BioMedJImpact是一个大型生物医学期刊影响力数据集，结合了合作结构和人工智能参与度指标，分析了疫情前后合作强度和AI参与对期刊影响力的影响。


<details>
  <summary>Details</summary>
Motivation: 目前公开资源很少同时捕捉合作结构和人工智能研究如何共同影响生物医学期刊的声望。

Method: 构建了从174万篇文章提取的数据库，融合了计量指标、合作特征和基于大语言模型(LLM)的AI参与语义指标，并设计了一个三阶段LLM流水线提取AI参与特征。

Result: 发现合作强度越高（尤其是较大且多样的作者团队），期刊引用影响力越大；AI参与度与期刊声望，特别是四分位排名的相关性日益增强。该三阶段LLM流水线经过人工评估验证，展示了高相关性和子领域分类一致性。

Conclusion: BioMedJImpact不仅是全面反映生物医学与AI交叉的数据库，也为具内容感知能力的科学计量分析提供了验证的可扩展方法学框架。

Abstract: Assessing journal impact is central to scholarly communication, yet existing open resources rarely capture how collaboration structures and artificial intelligence (AI) research jointly shape venue prestige in biomedicine. We present BioMedJImpact, a large-scale, biomedical-oriented dataset designed to advance journal-level analysis of scientific impact and AI engagement. Built from 1.74 million PubMed Central articles across 2,744 journals, BioMedJImpact integrates bibliometric indicators, collaboration features, and LLM-derived semantic indicators for AI engagement. Specifically, the AI engagement feature is extracted through a reproducible three-stage LLM pipeline that we propose. Using this dataset, we analyze how collaboration intensity and AI engagement jointly influence scientific impact across pre- and post-pandemic periods (2016-2019, 2020-2023). Two consistent trends emerge: journals with higher collaboration intensity, particularly those with larger and more diverse author teams, tend to achieve greater citation impact, and AI engagement has become an increasingly strong correlate of journal prestige, especially in quartile rankings. To further validate the three-stage LLM pipeline we proposed for deriving the AI engagement feature, we conduct human evaluation, confirming substantial agreement in AI relevance detection and consistent subfield classification. Together, these contributions demonstrate that BioMedJImpact serves as both a comprehensive dataset capturing the intersection of biomedicine and AI, and a validated methodological framework enabling scalable, content-aware scientometric analysis of scientific impact and innovation dynamics. Code is available at https://github.com/JonathanWry/BioMedJImpact.

</details>


### [51] [From Passive to Persuasive: Steering Emotional Nuance in Human-AI Negotiation](https://arxiv.org/abs/2511.12832)
*Niranjan Chebrolu,Gerard Christopher Yeo,Kokil Jaidka*

Main category: cs.CL

TL;DR: 该论文通过激活工程技术有效提升了大型语言模型LLaMA 3.1-8B的情感表达能力，使其对话更加具有人类情感细腻度。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型虽在对话流畅性上表现优异，但难以实现细腻且具有人类情感的表达，且传统调优方法往往仅能调整表面输出或需要大量微调。

Method: 通过归因修补技术定位关键激活部件，确定干预点，再利用对比文本对的激活差异导出情感表达向量，应用于新对话提示以调节情绪特征。

Result: 应用情感表达向量后，模型生成的回答在积极情绪（如喜悦和信任）上明显增强，并更频繁使用第一人称代词，显示出更强的个人参与感。

Conclusion: 该研究提出了一个精确且具解释性的激活工程框架，为提升对话型AI的情感表现提供了新的思路和方法。

Abstract: Large Language Models (LLMs) demonstrate increasing conversational fluency, yet instilling them with nuanced, human-like emotional expression remains a significant challenge. Current alignment techniques often address surface-level output or require extensive fine-tuning. This paper demonstrates that targeted activation engineering can steer LLaMA 3.1-8B to exhibit more human-like emotional nuances. We first employ attribution patching to identify causally influential components, to find a key intervention locus by observing activation patterns during diagnostic conversational tasks. We then derive emotional expression vectors from the difference in the activations generated by contrastive text pairs (positive vs. negative examples of target emotions). Applying these vectors to new conversational prompts significantly enhances emotional characteristics: steered responses show increased positive sentiment (e.g., joy, trust) and more frequent first-person pronoun usage, indicative of greater personal engagement. Our findings offer a precise and interpretable framework and new directions for the study of conversational AI.

</details>


### [52] [Quantifying consistency and accuracy of Latent Dirichlet Allocation](https://arxiv.org/abs/2511.12850)
*Saranzaya Magsarjav,Melissa Humphries,Jonathan Tuke,Lewis Mitchell*

Main category: cs.CL

TL;DR: 本文研究了主题模型LDA在文本分析中的稳定性问题，提出了结合准确性和一致性的稳定性度量方法，通过生成带有真实标签的语料库多次运行验证发现LDA结果具有内部一致性但识别的主题并非真实主题。


<details>
  <summary>Details</summary>
Motivation: 现有的概率主题模型因其随机性在多次运行中结果不一致，导致主题的不稳定性，影响模型的可重复性和解释性，亟需一种能评估并提升主题模型稳定性的方法。

Method: 提出一种新的稳定性度量标准，结合准确性和一致性，通过利用LDA的生成性质合成带有真实主题标签的语料库，并多次运行LDA（50次）来评估输出的变异性和主题识别效果。

Result: 实验证明LDA能准确确定文档中的主题数，且多次运行结果内部一致性较高，但识别出的主题并非真实主题，表明模型存在识别偏差。

Conclusion: LDA作为主题模型在多次运行中保持较高一致性，但无法真正捕捉到文档中的真实主题，指出了现有主题模型在稳定性和准确性上的局限性。

Abstract: Topic modelling in Natural Language Processing uncovers hidden topics in large, unlabelled text datasets. It is widely applied in fields such as information retrieval, content summarisation, and trend analysis across various disciplines. However, probabilistic topic models can produce different results when rerun due to their stochastic nature, leading to inconsistencies in latent topics. Factors like corpus shuffling, rare text removal, and document elimination contribute to these variations. This instability affects replicability, reliability, and interpretation, raising concerns about whether topic models capture meaningful topics or just noise. To address these problems, we defined a new stability measure that incorporates accuracy and consistency and uses the generative properties of LDA to generate a new corpus with ground truth. These generated corpora are run through LDA 50 times to determine the variability in the output. We show that LDA can correctly determine the underlying number of topics in the documents. We also find that LDA is more internally consistent, as the multiple reruns return similar topics; however, these topics are not the true topics.

</details>


### [53] [NeuroLex: A Lightweight Domain Language Model for EEG Report Understanding and Generation](https://arxiv.org/abs/2511.12851)
*Kang Yin,Hye-Bin Shin*

Main category: cs.CL

TL;DR: NeuroLex是一种专门针对脑电图（EEG）报告的轻量级领域适应性语言模型，能够更好地理解EEG报告中的专业语言和诊断特点。


<details>
  <summary>Details</summary>
Motivation: 现有通用语言模型无法有效捕捉EEG报告中的领域特定语言习惯和诊断信息，限制了其在神经生理数据文本分析中的应用。

Method: NeuroLex使用基于哈佛脑电数据库的EEG报告文本，通过span-corruption预训练和面向报告润色、摘要及术语问答的指令式微调，学习EEG的语法和推理特征。

Result: 在综合评估中，NeuroLex在困惑度、信息抽取与摘要准确度、标签效率及对否定和事实幻觉的鲁棒性方面均优于同规模通用模型。

Conclusion: NeuroLex作为一个EEG认知语言模型，成功结合了生物医学文本建模与脑机接口应用，提供了可解释且基于语言的神经解码基础。

Abstract: Clinical electroencephalogram (EEG) reports encode domain-specific linguistic conventions that general-purpose language models (LMs) fail to capture. We introduce NeuroLex, a lightweight domain-adaptive language model trained purely on EEG report text from the Harvard Electroencephalography Database. Unlike existing biomedical LMs, NeuroLex is tailored to the linguistic and diagnostic characteristics of EEG reporting, enabling it to serve as both an independent textual model and a decoder backbone for multimodal EEG-language systems. Using span-corruption pretraining and instruction-style fine-tuning on report polishing, paragraph summarization, and terminology question answering, NeuroLex learns the syntax and reasoning patterns characteristic of EEG interpretation. Comprehensive evaluations show that it achieves lower perplexity, higher extraction and summarization accuracy, better label efficiency, and improved robustness to negation and factual hallucination compared with general models of the same scale. With an EEG-aware linguistic backbone, NeuroLex bridges biomedical text modeling and brain-computer interface applications, offering a foundation for interpretable and language-driven neural decoding.

</details>


### [54] [From Perception to Reasoning: Deep Thinking Empowers Multimodal Large Language Models](https://arxiv.org/abs/2511.12861)
*Wenxin Zhu,Andong Chen,Yuchen Song,Kehai Chen,Conghui Zhu,Ziyan Chen,Tiejun Zhao*

Main category: cs.CL

TL;DR: 本文系统综述了多模态链式思维（MCoT）方法，分析了其背景、技术动机、主要方法、评测指标及应用场景，并探讨了未来挑战和研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大型语言模型推理路径不透明、泛化能力不足，链式思维可提升推理的透明度和可解释性，有望增强模型复杂推理能力。

Method: 从技术演进和任务需求出发，系统介绍了主流MCoT方法，包括链式思维范式、后训练阶段和推理阶段的技术细节及其机制解析。

Result: 总结了现有评测基准和指标，梳理了MCoT的具体应用场景，展示了其提升多模态推理能力的潜力。

Conclusion: 尽管MCoT取得了显著进展，但仍面临若干技术挑战，未来研究可聚焦于提升泛化能力、优化推理路径透明度及拓展应用领域。

Abstract: With the remarkable success of Multimodal Large Language Models (MLLMs) in perception tasks, enhancing their complex reasoning capabilities has emerged as a critical research focus. Existing models still suffer from challenges such as opaque reasoning paths and insufficient generalization ability. Chain-of-Thought (CoT) reasoning, which has demonstrated significant efficacy in language models by enhancing reasoning transparency and output interpretability, holds promise for improving model reasoning capabilities when extended to the multimodal domain. This paper provides a systematic review centered on "Multimodal Chain-of-Thought" (MCoT). First, it analyzes the background and theoretical motivations for its inception from the perspectives of technical evolution and task demands. Then, it introduces mainstream MCoT methods from three aspects: CoT paradigms, the post-training stage, and the inference stage, while also analyzing their underlying mechanisms. Furthermore, the paper summarizes existing evaluation benchmarks and metrics, and discusses the application scenarios of MCoT. Finally, it analyzes the challenges currently facing MCoT and provides an outlook on its future research directions.

</details>


### [55] [Classification of Hope in Textual Data using Transformer-Based Models](https://arxiv.org/abs/2511.12874)
*Chukwuebuka Fortunate Ijezue,Tania-Amanda Fredrick Eneye,Maaz Amjad*

Main category: cs.CL

TL;DR: 本文提出了基于Transformer的模型框架，用于文本中希望表达的分类，比较了BERT、GPT-2和DeBERTa三种模型在二分类和多分类任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 构建有效的希望表达分类模型，促进心理健康和社交媒体分析，探索不同模型架构在情感检测任务中的性能表现。

Method: 设计并比较了三种Transformer架构（BERT、GPT-2、DeBERTa）在二分类和五类希望相关分类任务中的效果，评估准确率和计算资源消耗，并进行错误分析。

Result: BERT模型表现最佳，在二分类和多分类任务分别达到84.49%和72.03%的准确率，同时计算效率最高。GPT-2表现最差但在讽刺检测中召回率最高（92.46%），DeBERTa结果中等但计算成本最高。

Conclusion: 研究展示了架构选择对情感识别的关键影响，模型大小不一定决定性能，提出了适合希望表达识别的有效计算框架，具有心理健康和社交媒体分析应用价值。

Abstract: This paper presents a transformer-based approach for classifying hope expressions in text. We developed and compared three architectures (BERT, GPT-2, and DeBERTa) for both binary classification (Hope vs. Not Hope) and multiclass categorization (five hope-related categories). Our initial BERT implementation achieved 83.65% binary and 74.87% multiclass accuracy. In the extended comparison, BERT demonstrated superior performance (84.49% binary, 72.03% multiclass accuracy) while requiring significantly fewer computational resources (443s vs. 704s training time) than newer architectures. GPT-2 showed lowest overall accuracy (79.34% binary, 71.29% multiclass), while DeBERTa achieved moderate results (80.70% binary, 71.56% multiclass) but at substantially higher computational cost (947s for multiclass training). Error analysis revealed architecture-specific strengths in detecting nuanced hope expressions, with GPT-2 excelling at sarcasm detection (92.46% recall). This study provides a framework for computational analysis of hope, with applications in mental health and social media analysis, while demonstrating that architectural suitability may outweigh model size for specialized emotion detection tasks.

</details>


### [56] [Auditing Google's AI Overviews and Featured Snippets: A Case Study on Baby Care and Pregnancy](https://arxiv.org/abs/2511.12920)
*Desheng Hu,Joachim Baumann,Aleksandra Urman,Elsa Lichtenegger,Robin Forsberg,Aniko Hannak,Christo Wilson*

Main category: cs.CL

TL;DR: 本研究通过审计1508个孕婴护理相关查询，发现谷歌搜索的AI生成内容（AI概要和特色摘要）在信息一致性和医疗保障方面存在显著不足，提示需加强质量控制。


<details>
  <summary>Details</summary>
Motivation: 用户频繁依赖谷歌搜索的AI生成内容获取健康信息，但用户无法控制其呈现，且信息质量参差不齐，尤其在医疗健康领域影响用户福祉。

Method: 系统性算法审计了1508条真实孕婴护理和怀孕相关查询，使用多维度评价框架评估答案一致性、相关性、医疗保障、信息来源及情感匹配等指标。

Result: 发现AIO与FS同页面信息在33%情况下不一致；虽然相关性高，但医疗保障严重不足（AIO仅11%，FS 7%）；信息来源以健康网站为主，FS还常链接商业来源。

Conclusion: AI生成健康信息存在一致性与专业保障不足，影响公众健康信息获取，应加强质量监管。所提评价框架可推广应用于其他关键领域的AI系统审计。

Abstract: Google Search increasingly surfaces AI-generated content through features like AI Overviews (AIO) and Featured Snippets (FS), which users frequently rely on despite having no control over their presentation. Through a systematic algorithm audit of 1,508 real baby care and pregnancy-related queries, we evaluate the quality and consistency of these information displays. Our robust evaluation framework assesses multiple quality dimensions, including answer consistency, relevance, presence of medical safeguards, source categories, and sentiment alignment. Our results reveal concerning gaps in information consistency, with information in AIO and FS displayed on the same search result page being inconsistent with each other in 33% of cases. Despite high relevance scores, both features critically lack medical safeguards (present in just 11% of AIO and 7% of FS responses). While health and wellness websites dominate source categories for both, AIO and FS, FS also often link to commercial sources. These findings have important implications for public health information access and demonstrate the need for stronger quality controls in AI-mediated health information. Our methodology provides a transferable framework for auditing AI systems across high-stakes domains where information quality directly impacts user well-being.

</details>


### [57] [Visual Room 2.0: Seeing is Not Understanding for MLLMs](https://arxiv.org/abs/2511.12928)
*Haokun Li,Yazhou Zhang,Jizhi Ding,Qiuchi Li,Peng Zhang*

Main category: cs.CL

TL;DR: 本文提出Visual Room论证多模态大语言模型(MLLMs)虽能精准描述视觉细节但可能缺乏理解能力，设计了包含17个任务的分层基准测试模型的感知与认知能力，发现其感知能力优于认知能力且认知能力随模型规模提升，但感知能力提升不明显。


<details>
  <summary>Details</summary>
Motivation: 探讨多模态大语言模型是否真正理解所见内容，验证其描述视觉信息与理解之间的差距，即"看见不等于理解"。

Method: 提出Visual Room 2.0，建立涵盖低、中、高三个层次的分层基准数据集，包含350个多模态样本和2100个问题，任务覆盖从属性识别到因果和社会推理，以评价MLLM的感知-认知对齐能力。

Result: 通过评测10个先进MLLM，发现模型感知能力明显优于认知能力，认知能力与模型大小相关但感知能力未随规模稳定提升，且认知能力不完全依赖感知能力。

Conclusion: 该工作将"看见不等于理解"转化为可测试假设，提供了从感知处理到认知推理的新评价范式，推动多模态大模型认知能力的深入研究。

Abstract: Can multi-modal large language models (MLLMs) truly understand what they can see? Extending Searle's Chinese Room into the multi-modal domain, this paper proposes the Visual Room argument: MLLMs may describe every visual detail precisely yet fail to comprehend the underlying emotions and intentions, namely seeing is not understanding. Building on this, we introduce \textit{Visual Room} 2.0, a hierarchical benchmark for evaluating perception-cognition alignment of MLLMs. We model human perceptive and cognitive processes across three levels: low, middle, and high, covering 17 representative tasks. The perception component ranges from attribute recognition to scene understanding, while the cognition component extends from textual entailment to causal and social reasoning. The dataset contains 350 multi-modal samples, each with six progressive questions (2,100 in total) spanning perception to cognition. Evaluating 10 state-of-the-art (SoTA) MLLMs, we highlight three key findings: (1) MLLMs exhibit stronger perceptual competence than cognitive ability (8.0\%$\uparrow$); (2) cognition appears not causally dependent on perception-based reasoning; and (3) cognition scales with model size, but perception does not consistently improve with larger variants. This work operationalizes Seeing $\ne$ Understanding as a testable hypothesis, offering a new paradigm from perceptual processing to cognitive reasoning in MLLMs. Our dataset is available at https://huggingface.co/datasets/LHK2003/PCBench.

</details>


### [58] [Fine-Tuned LLMs Know They Don't Know: A Parameter-Efficient Approach to Recovering Honesty](https://arxiv.org/abs/2511.12991)
*Zeyu Shi,Ziming Wang,Tianyu Chen,Shiqi Gao,Haoyi Zhou,Qingyun Sun,Jianxin Li*

Main category: cs.CL

TL;DR: 本文提出了Honesty-Critical Neurons Restoration (HCNR)方法，用于恢复经过监督微调后大型语言模型（LLMs）的诚实表达能力。


<details>
  <summary>Details</summary>
Motivation: 监督微调虽提升了模型在特定任务的表现，但严重削弱了模型诚实表达自身知识边界的能力，影响模型在高风险领域的安全应用。

Method: HCNR通过识别并恢复关键表达相关神经元到预训练状态，同时通过Hessian引导的补偿机制协调任务导向神经元，手术式修复模型的诚实表达能力。

Result: 在四个问答任务和五个LLM族上，HCNR恢复了33.25%的诚实能力，速度提升至少2.23倍，所需数据量减少10倍以上。

Conclusion: HCNR提供了一种高效且数据经济的解决方案，有效恢复了微调后LLMs的诚实能力，促进其在高风险领域的可信部署。

Abstract: The honesty of Large Language Models (LLMs) is increasingly important for safe deployment in high-stakes domains. However, this crucial trait is severely undermined by supervised fine-tuning (SFT), a common technique for model specialization. Existing recovery methods rely on data-intensive global parameter adjustments, implicitly assuming that SFT deeply corrupts the models' ability to recognize their knowledge boundaries. However, we observe that fine-tuned LLMs still preserve this ability; what is damaged is their capacity to faithfully express that awareness. Building on this, we propose Honesty-Critical Neurons Restoration (HCNR) to surgically repair this suppressed capacity. HCNR identifies and restores key expression-governing neurons to their pre-trained state while harmonizing them with task-oriented neurons via Hessian-guided compensation. Experiments on four QA tasks and five LLM families demonstrate that HCNR effectively recovers 33.25% of the compromised honesty while achieving at least 2.23x speedup with over 10x less data compared to baseline methods, offering a practical solution for trustworthy LLM deployment.

</details>


### [59] [AA-Omniscience: Evaluating Cross-Domain Knowledge Reliability in Large Language Models](https://arxiv.org/abs/2511.13029)
*Declan Jackson,William Keating,George Cameron,Micah Hill-Smith*

Main category: cs.CL

TL;DR: 该论文提出了AA-Omniscience基准，用于评估语言模型的事实回忆和知识校准能力，涵盖6000个问题和6个领域。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型评估主要关注通用能力，但在多领域可靠应用需要准确的事实回忆和知识缺口识别。

Method: 设计AA-Omniscience基准，以权威学术和行业问题为来源，测量模型的全知指数，该指标同时惩罚幻觉回答并奖励不确定时的回避。

Result: 评测中，Claude 4.1 Opus获得最高分4.8，仅有三款模型得分超过零。各模型在不同领域表现差异显著。

Conclusion: 当前顶尖模型在事实准确性和知识校准上仍有不足，且性能依赖具体领域，选择模型应根据具体应用需求而非一般性能。

Abstract: Existing language model evaluations primarily measure general capabilities, yet reliable use of these models across a range of domains demands factual accuracy and recognition of knowledge gaps. We introduce AA-Omniscience, a benchmark designed to measure both factual recall and knowledge calibration across 6,000 questions. Questions are derived from authoritative academic and industry sources, and cover 42 economically relevant topics within six different domains. The evaluation measures a model's Omniscience Index, a bounded metric (-100 to 100) measuring factual recall that jointly penalizes hallucinations and rewards abstention when uncertain, with 0 equating to a model that answers questions correctly as much as it does incorrectly. Among evaluated models, Claude 4.1 Opus attains the highest score (4.8), making it one of only three models to score above zero. These results reveal persistent factuality and calibration weaknesses across frontier models. Performance also varies by domain, with the models from three different research labs leading across the six domains. This performance variability suggests models should be chosen according to the demands of the use case rather than general performance for tasks where knowledge is important.

</details>


### [60] [How Good is BLI as an Alignment Measure: A Study in Word Embedding Paradigm](https://arxiv.org/abs/2511.13040)
*Kasun Wickramasinghe,Nisansa de Silva*

Main category: cs.CL

TL;DR: 本文评估了双语词汇诱导(BLI)作为衡量嵌入空间对齐度的指标，比较了传统嵌入对齐方法、多语言模型及其组合在高低资源语言环境下的表现，提出了基于词干的BLI方法和词汇剪枝技术以改进对齐度评估。


<details>
  <summary>Details</summary>
Motivation: 多语言嵌入模型虽普遍应用且灵活，但其是否在所有方面都优于对齐的单语言模型尚未明晰，且高计算成本是否合理也需探讨。同时，现有BLI指标可能不能真实反映嵌入空间的对齐度。

Method: 本文分析了传统对齐技术、多语言模型及其组合在BLI任务中的表现，研究了语言家族对结果的影响，提出了基于词干的新BLI评估方法和词汇剪枝技术以更准确地衡量对齐度。

Result: 研究发现BLI在某些情况下无法真实测量对齐度，基于词干的BLI方法和词汇剪枝技巧能提升评估准确性。组合对齐方法通常效果更佳，多语言模型在低资源语言中表现突出。

Conclusion: BLI存在局限，需要改进评价方法。基于词干的BLI及词汇剪枝可更好反映嵌入对齐度。多语言模型和对齐技术应结合使用以增强表现，特别是在低资源语言场景。

Abstract: Sans a dwindling number of monolingual embedding studies originating predominantly from the low-resource domains, it is evident that multilingual embedding has become the de facto choice due to its adaptability to the usage of code-mixed languages, granting the ability to process multilingual documents in a language-agnostic manner, as well as removing the difficult task of aligning monolingual embeddings. But is this victory complete? Are the multilingual models better than aligned monolingual models in every aspect? Can the higher computational cost of multilingual models always be justified? Or is there a compromise between the two extremes? Bilingual Lexicon Induction is one of the most widely used metrics in terms of evaluating the degree of alignment between two embedding spaces. In this study, we explore the strengths and limitations of BLI as a measure to evaluate the degree of alignment of two embedding spaces. Further, we evaluate how well traditional embedding alignment techniques, novel multilingual models, and combined alignment techniques perform BLI tasks in the contexts of both high-resource and low-resource languages. In addition to that, we investigate the impact of the language families to which the pairs of languages belong. We identify that BLI does not measure the true degree of alignment in some cases and we propose solutions for them. We propose a novel stem-based BLI approach to evaluate two aligned embedding spaces that take into account the inflected nature of languages as opposed to the prevalent word-based BLI techniques. Further, we introduce a vocabulary pruning technique that is more informative in showing the degree of the alignment, especially performing BLI on multilingual embedding models. Often, combined embedding alignment techniques perform better while in certain cases multilingual embeddings perform better (mainly low-resource language cases).

</details>


### [61] [Spark-Prover-X1: Formal Theorem Proving Through Diverse Data Training](https://arxiv.org/abs/2511.13043)
*Xinyuan Zhou,Yi Lei,Xiaoyu Zhou,Jingyi Sun,Yu Zhu,Zhongyi Ye,Weitai Zhang,Quan Liu,Si Wei,Cong Liu*

Main category: cs.CL

TL;DR: 本文提出了Spark-Prover-X1，一种7亿参数的语言模型，通过三阶段训练框架提升中等规模模型在自动定理证明的表现，并引入了新的评测基准ExamFormal-Bench。


<details>
  <summary>Details</summary>
Motivation: 当前自动定理证明中高质量多样的形式语言数据稀缺，限制了大语言模型的推理能力发挥。

Method: 采用三阶段训练策略：第一阶段在广泛数学语料及创新数据任务上持续预训练；第二阶段通过专家迭代循环进行有监督微调；第三阶段使用群体相对策略优化提升难题表现。

Result: Spark-Prover-X1-7B在开源同规模模型中表现最佳，测试中平均通过率37.0%，在PutnamBench和CombiBench等竞赛中表现优异。

Conclusion: 通过多样化训练数据及分阶段精炼训练流程，有效提升了中型语言模型的形式推理能力，提供了轻量级大模型增强的可行路径。

Abstract: Large Language Models (LLMs) have shown significant promise in automated theorem proving, yet progress is often constrained by the scarcity of diverse and high-quality formal language data. To address this issue, we introduce Spark-Prover-X1, a 7B parameter model trained via an three-stage framework designed to unlock the reasoning potential of more accessible and moderately-sized LLMs. The first stage infuses deep knowledge through continuous pre-training on a broad mathematical corpus, enhanced by a suite of novel data tasks. Key innovation is a "CoT-augmented state prediction" task to achieve fine-grained reasoning. The second stage employs Supervised Fine-tuning (SFT) within an expert iteration loop to specialize both the Spark-Prover-X1-7B and Spark-Formalizer-X1-7B models. Finally, a targeted round of Group Relative Policy Optimization (GRPO) is applied to sharpen the prover's capabilities on the most challenging problems. To facilitate robust evaluation, particularly on problems from real-world examinations, we also introduce ExamFormal-Bench, a new benchmark dataset of 402 formal problems. Experimental results demonstrate that Spark-Prover-X1-7B achieves state-of-the-art performance among similarly-sized open-source models, attaining a 37.0\% average pass rate (pass@32). It shows exceptional performance on difficult competition benchmarks, notably solving 27 problems on PutnamBench (pass@32) and achieving 24.0\% on CombiBench (pass@32). Our work validates that this diverse training data and progressively refined training pipeline provides an effective path for enhancing the formal reasoning capabilities of lightweight LLMs. Both Spark-Prover-X1-7B and Spark-Formalizer-X1-7B, along with the ExamFormal-Bench dataset, are made publicly available at:https://www.modelscope.cn/organization/iflytek, https://gitcode.com/ifly_opensource.

</details>


### [62] [BeDiscovER: The Benchmark of Discourse Understanding in the Era of Reasoning Language Models](https://arxiv.org/abs/2511.13095)
*Chuyuan Li,Giuseppe Carenini*

Main category: cs.CL

TL;DR: BeDiscovER是一个评估现代大型语言模型（LLMs）话语理解能力的综合基准，包含52个数据集，涵盖话语词汇、多句子及全文层面任务。测试结果显示当前最先进模型在时间推理算术方面表现优异，但在全文推理及复杂语义话语现象（如修辞关系识别）上存在不足。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在话语层面知识理解上的能力尚未得到全面评估，特别是在多层次话语任务中的表现不清楚。

Method: 构建BeDiscovER基准，整合5个话语任务及52个数据集，涵盖多语言、多框架，并评测多款开源及前沿大型语言模型的表现。

Result: 发现最先进模型在时间推理等算术相关任务上表现强劲，但在全文推理和复杂语义话语现象上仍有较大挑战。

Conclusion: 虽然大型语言模型在部分话语推理任务中表现良好，但在更复杂的全文及语义话语理解任务中需要进一步改进。

Abstract: We introduce BeDiscovER (Benchmark of Discourse Understanding in the Era of Reasoning Language Models), an up-to-date, comprehensive suite for evaluating the discourse-level knowledge of modern LLMs. BeDiscovER compiles 5 publicly available discourse tasks across discourse lexicon, (multi-)sentential, and documental levels, with in total 52 individual datasets. It covers both extensively studied tasks such as discourse parsing and temporal relation extraction, as well as some novel challenges such as discourse particle disambiguation (e.g., ``just''), and also aggregates a shared task on Discourse Relation Parsing and Treebanking for multilingual and multi-framework discourse relation classification. We evaluate open-source LLMs: Qwen3 series, DeepSeek-R1, and frontier model such as GPT-5-mini on BeDiscovER, and find that state-of-the-art models exhibit strong performance in arithmetic aspect of temporal reasoning, but they struggle with full document reasoning and some subtle semantic and discourse phenomena, such as rhetorical relation recognition.

</details>


### [63] [Evaluating the Ability of Large Language Models to Identify Adherence to CONSORT Reporting Guidelines in Randomized Controlled Trials: A Methodological Evaluation Study](https://arxiv.org/abs/2511.13107)
*Zhichao He,Mouxiao Bian,Jianhong Zhu,Jiayuan Chen,Yunqiu Wang,Wenxia Zhao,Tianbin Li,Bing Han,Jie Xu,Junyan Wu*

Main category: cs.CL

TL;DR: 本文评估了大型语言模型(LLMs)在零样本条件下识别随机对照试验(RCTs)遵循CONSORT 2010声明的准确性。发现模型能准确识别符合项，但难以识别不符合项，整体表现有限。


<details>
  <summary>Details</summary>
Motivation: 手动验证RCTs对CONSORT标准的遵循既耗时又费力，是同行评审和证据综合的瓶颈。探索LLMs自动识别的可行性有助加速过程。

Method: 构建了包含150篇跨医学领域已发表RCTs的黄金数据集，使用多模型进行三分类任务（符合、不符合、不适用），通过宏观F1分数和Cohen的Kappa系数评估模型表现，并进行错误分析。

Result: 顶级模型Gemini-2.5-Flash和DeepSeek-R1宏F1分数约为0.634，表现一般，且在识别不符合和不适用项时F1分数通常低于0.400。部分知名模型GPT-4o表现较差，宏F1只有0.521。

Conclusion: LLMs可作为CONSORT审核的初筛工具，能有效识别规范报告内容，但目前无法可靠检测遗漏或方法学缺陷，不能替代人工专业评价。

Abstract: The Consolidated Standards of Reporting Trials statement is the global benchmark for transparent and high-quality reporting of randomized controlled trials. Manual verification of CONSORT adherence is a laborious, time-intensive process that constitutes a significant bottleneck in peer review and evidence synthesis. This study aimed to systematically evaluate the accuracy and reliability of contemporary LLMs in identifying the adherence of published RCTs to the CONSORT 2010 statement under a zero-shot setting. We constructed a golden standard dataset of 150 published RCTs spanning diverse medical specialties. The primary outcome was the macro-averaged F1-score for the three-class classification task, supplemented by item-wise performance metrics and qualitative error analysis. Overall model performance was modest. The top-performing models, Gemini-2.5-Flash and DeepSeek-R1, achieved nearly identical macro F1 scores of 0.634 and Cohen's Kappa coefficients of 0.280 and 0.282, respectively, indicating only fair agreement with expert consensus. A striking performance disparity was observed across classes: while most models could identify compliant items with high accuracy (F1 score > 0.850), they struggled profoundly with identifying non-compliant and not applicable items, where F1 scores rarely exceeded 0.400. Notably, some high-profile models like GPT-4o underperformed, achieving a macro F1-score of only 0.521. LLMs show potential as preliminary screening assistants for CONSORT checks, capably identifying well-reported items. However, their current inability to reliably detect reporting omissions or methodological flaws makes them unsuitable for replacing human expertise in the critical appraisal of trial quality.

</details>


### [64] [Extracting Events Like Code: A Multi-Agent Programming Framework for Zero-Shot Event Extraction](https://arxiv.org/abs/2511.13118)
*Quanjiang Guo,Sijie Wang,Jinchuan Zhang,Ben Zhang,Zhao Kang,Ling Tian,Ke Yan*

Main category: cs.CL

TL;DR: 提出了一种名为Agent-Event-Coder的多智能体框架，将零样本事件抽取视为一个结构化、迭代的代码生成过程，实现精确且符合模式的事件抽取。


<details>
  <summary>Details</summary>
Motivation: 零样本事件抽取对大语言模型来说具有挑战性，因其需要复杂的推理和领域特定理解，直接提示常导致输出不完整或结构无效。

Method: 将事件抽取分解为检索、规划、编码和验证四个子任务，由专门的语言模型智能体处理，使用可执行类定义表示事件模式，通过验证智能体进行确定性验证和反馈，采用迭代细化策略确保模式一致性。

Result: 在五个不同领域和六种大语言模型上进行实验，AEC在零样本事件抽取任务中表现优于现有基线方法。

Conclusion: 通过将事件抽取视为代码生成任务，采用多智能体协作工作流，显著提升零样本事件抽取的准确性和一致性。

Abstract: Zero-shot event extraction (ZSEE) remains a significant challenge for large language models (LLMs) due to the need for complex reasoning and domain-specific understanding. Direct prompting often yields incomplete or structurally invalid outputs--such as misclassified triggers, missing arguments, and schema violations. To address these limitations, we present Agent-Event-Coder (AEC), a novel multi-agent framework that treats event extraction like software engineering: as a structured, iterative code-generation process. AEC decomposes ZSEE into specialized subtasks--retrieval, planning, coding, and verification--each handled by a dedicated LLM agent. Event schemas are represented as executable class definitions, enabling deterministic validation and precise feedback via a verification agent. This programming-inspired approach allows for systematic disambiguation and schema enforcement through iterative refinement. By leveraging collaborative agent workflows, AEC enables LLMs to produce precise, complete, and schema-consistent extractions in zero-shot settings. Experiments across five diverse domains and six LLMs demonstrate that AEC consistently outperforms prior zero-shot baselines, showcasing the power of treating event extraction like code generation. The code and data are released on https://github.com/UESTC-GQJ/Agent-Event-Coder.

</details>


### [65] [A Comparative Analysis of Recurrent and Attention Architectures for Isolated Sign Language Recognition](https://arxiv.org/abs/2511.13126)
*Nigar Alishzade,Gulchin Abdullayeva*

Main category: cs.CL

TL;DR: 本文比较了循环神经网络ConvLSTM和注意力机制Transformer在手语识别中的表现，发现Transformer在准确率上优于ConvLSTM，ConvLSTM计算效率更高。


<details>
  <summary>Details</summary>
Motivation: 针对手语识别任务，探讨不同神经网络结构的性能差异，以指导实际模型选择。

Method: 在亚美尼亚手语数据集AzSLD和美国手语数据集WLASL上，分别训练并评估ConvLSTM和Vanilla Transformer模型，比较准确率和计算效率。

Result: Transformer模型在Top-1和Top-5准确率上均优于ConvLSTM，AzSLD达76.8%，WLASL达88.3%。ConvLSTM计算效率更高，但准确率较低，尤其在小数据集上表现不佳。

Conclusion: Transformer适合追求高识别准确率和签名者无关性的应用，ConvLSTM适合计算资源有限且需要高时序建模能力的场景，研究提供了手语识别架构选择的参考依据。

Abstract: This study presents a systematic comparative analysis of recurrent and attention-based neural architectures for isolated sign language recognition. We implement and evaluate two representative models-ConvLSTM and Vanilla Transformer-on the Azerbaijani Sign Language Dataset (AzSLD) and the Word-Level American Sign Language (WLASL) dataset. Our results demonstrate that the attention-based Vanilla Transformer consistently outperforms the recurrent ConvLSTM in both Top-1 and Top-5 accuracy across datasets, achieving up to 76.8% Top-1 accuracy on AzSLD and 88.3% on WLASL. The ConvLSTM, while more computationally efficient, lags in recognition accuracy, particularly on smaller datasets. These findings highlight the complementary strengths of each paradigm: the Transformer excels in overall accuracy and signer independence, whereas the ConvLSTM offers advantages in computational efficiency and temporal modeling. The study provides a nuanced analysis of these trade-offs, offering guidance for architecture selection in sign language recognition systems depending on application requirements and resource constraints.

</details>


### [66] [Zero-Shot Grammar Competency Estimation Using Large Language Model Generated Pseudo Labels](https://arxiv.org/abs/2511.13152)
*Sourya Dipta Das,Shubham Kumar,Kuldeep Yadav*

Main category: cs.CL

TL;DR: 本文提出了一种基于零样本学习和大型语言模型的语法能力评估框架，无需人工标注，利用无标签数据进行训练，实现了高准确度的语法评分。


<details>
  <summary>Details</summary>
Motivation: 传统语法能力评估依赖大量专家注释，且口语评估存在自发性、非结构性和流畅度差的挑战，使得大规模数据创建难以实现。

Method: 采用大型语言模型对无标签数据生成伪标签，利用基于语法能力评分标准的提示进行训练，结合新颖的训练框架处理标签噪声，训练基于Transformer的模型。

Result: 实验表明伪标签生成所选大型语言模型影响显著，干净样本与噪声样本的比例影响稳定性与准确率，整体方法在语法能力评分中表现出高准确度及鲁棒性。

Conclusion: 该方法无需人工标注，提升了语法能力评分的可扩展性和低资源适用性，为语法评估系统提供了有效的新途径。

Abstract: Grammar competency estimation is essential for assessing linguistic proficiency in both written and spoken language; however, the spoken modality presents additional challenges due to its spontaneous, unstructured, and disfluent nature. Developing accurate grammar scoring models further requires extensive expert annotation, making large-scale data creation impractical. To address these limitations, we propose a zero-shot grammar competency estimation framework that leverages unlabeled data and Large Language Models (LLMs) without relying on manual labels. During training, we employ LLM-generated predictions on unlabeled data by using grammar competency rubric-based prompts. These predictions, treated as pseudo labels, are utilized to train a transformer-based model through a novel training framework designed to handle label noise effectively. We show that the choice of LLM for pseudo-label generation critically affects model performance and that the ratio of clean-to-noisy samples during training strongly influences stability and accuracy. Finally, a qualitative analysis of error intensity and score prediction confirms the robustness and interpretability of our approach. Experimental results demonstrate the efficacy of our approach in estimating grammar competency scores with high accuracy, paving the way for scalable, low-resource grammar assessment systems.

</details>


### [67] [Distinguishing Repetition Disfluency from Morphological Reduplication in Bangla ASR Transcripts: A Novel Corpus and Benchmarking Analysis](https://arxiv.org/abs/2511.13159)
*Zaara Zabeen Arpa,Sadnam Sakib Apurbo,Nazia Karim Khan Oishee,Ajwad Abrar*

Main category: cs.CL

TL;DR: 本文针对孟加拉语自动语音识别文本中的词语重复歧义问题，提出了一个包含2万条数据的手工标注语料库，区分重复障碍与形态叠词。通过大语言模型和专门模型微调实验，证明微调的BanglaBERT模型表现最好，准确率达84.78%。


<details>
  <summary>Details</summary>
Motivation: 自动语音识别文本中词语重复既可能是无意的重复障碍，也可能是语法上的形态叠词。标准的消除重复方法会误删有效语言信息，影响文本质量。为了在低资源语言孟加拉语中解决该问题，迫切需要标注数据和有效方法。

Method: 创建首个公开孟加拉语语料库，包含2万条手工标注数据，明确区分重复障碍和形态叠词。基于该语料库，使用多语言大语言模型进行少量样本提示学习，以及对语言特定的编码模型BanglaBERT进行任务微调，进行性能评测。

Result: 少样本提示学习的大语言模型准确率最高达82.68%。专门微调BanglaBERT模型表现更佳，准确率为84.78%，F1分数为0.677。

Conclusion: 该工作构建了首个具有辨析能力的孟加拉语重复现象标注数据集，建立了强有力的语言认知基线。该成果为开发保护语义的文本规范化系统奠定了基础，对低资源语言的ASR文本处理具有重要参考价值。

Abstract: Automatic Speech Recognition (ASR) transcripts, especially in low-resource languages like Bangla, contain a critical ambiguity: word-word repetitions can be either Repetition Disfluency (unintentional ASR error/hesitation) or Morphological Reduplication (a deliberate grammatical construct). Standard disfluency correction fails by erroneously deleting valid linguistic information. To solve this, we introduce the first publicly available, 20,000-row Bangla corpus, manually annotated to explicitly distinguish between these two phenomena in noisy ASR transcripts. We benchmark this novel resource using two paradigms: state-of-the-art multilingual Large Language Models (LLMs) and task-specific fine-tuning of encoder models. LLMs achieve competitive performance (up to 82.68\% accuracy) with few-shot prompting. However, fine-tuning proves superior, with the language-specific BanglaBERT model achieving the highest accuracy of 84.78\% and an F1 score of 0.677. This establishes a strong, linguistically-informed baseline and provides essential data for developing sophisticated, semantic-preserving text normalization systems for Bangla.

</details>


### [68] [TCM-5CEval: Extended Deep Evaluation Benchmark for LLM's Comprehensive Clinical Research Competence in Traditional Chinese Medicine](https://arxiv.org/abs/2511.13169)
*Tianai Huang,Jiayuan Chen,Lu Lu,Pengcheng Chen,Tianbin Li,Bing Han,Wenchao Tang,Jie Xu,Ming Li*

Main category: cs.CL

TL;DR: 本文提出了针对中医领域的多维度评测基准TCM-5CEval，评价了15个大型语言模型，揭示了模型在中医知识层面的强项及解释经典文献时的不足，并发现模型推理中存在较大一致性脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在普通领域表现优异，但在文化丰富且专业性强的中医领域应用尚需细致考察，尤其是模型在理解经典文本和临床推理的能力。

Method: 基于前代TCM-3CEval，设计涵盖核心知识、经典文学、临床决策、中药材和非药物治疗五个维度的TCM-5CEval基准，评测15个主流大型语言模型，包括一致性排列测试分析模型推理稳定性。

Result: 评测结果显示模型在基础知识回忆表现较好，但对经典文献的解释能力不足，推理过程对题目选项顺序敏感，表现出位置偏差和推理不稳的普遍问题，部分模型如deepseek_r1和gemini_2_5_pro表现相对较优。

Conclusion: TCM-5CEval提供了更细致的中医领域语言模型能力诊断工具，揭示其推理稳定性等根本弱点，有助推动该领域研究标准化及深入挑战，已上传至Medbench平台供学界使用。

Abstract: Large language models (LLMs) have demonstrated exceptional capabilities in general domains, yet their application in highly specialized and culturally-rich fields like Traditional Chinese Medicine (TCM) requires rigorous and nuanced evaluation. Building upon prior foundational work such as TCM-3CEval, which highlighted systemic knowledge gaps and the importance of cultural-contextual alignment, we introduce TCM-5CEval, a more granular and comprehensive benchmark. TCM-5CEval is designed to assess LLMs across five critical dimensions: (1) Core Knowledge (TCM-Exam), (2) Classical Literacy (TCM-LitQA), (3) Clinical Decision-making (TCM-MRCD), (4) Chinese Materia Medica (TCM-CMM), and (5) Clinical Non-pharmacological Therapy (TCM-ClinNPT). We conducted a thorough evaluation of fifteen prominent LLMs, revealing significant performance disparities and identifying top-performing models like deepseek\_r1 and gemini\_2\_5\_pro. Our findings show that while models exhibit proficiency in recalling foundational knowledge, they struggle with the interpretative complexities of classical texts. Critically, permutation-based consistency testing reveals widespread fragilities in model inference. All evaluated models, including the highest-scoring ones, displayed a substantial performance degradation when faced with varied question option ordering, indicating a pervasive sensitivity to positional bias and a lack of robust understanding. TCM-5CEval not only provides a more detailed diagnostic tool for LLM capabilities in TCM but aldso exposes fundamental weaknesses in their reasoning stability. To promote further research and standardized comparison, TCM-5CEval has been uploaded to the Medbench platform, joining its predecessor in the "In-depth Challenge for Comprehensive TCM Abilities" special track.

</details>


### [69] [Translation Entropy: A Statistical Framework for Evaluating Translation Systems](https://arxiv.org/abs/2511.13180)
*Ronit D. Gross,Yanir Harel,Ido Kanter*

Main category: cs.CL

TL;DR: 本文提出了一种定量估计翻译熵的方法，通过分析对比编码器-解码器翻译模型在替换输入句子中特定词汇时输出翻译的变化，测量翻译的不确定性和冗余性。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏客观的量化指标评估自动翻译模型的性能，因为语言本身的熵尚未知晓。为了实现对翻译系统的严谨评价，需要引入可测量的翻译熵概念。

Method: 通过选择枢纽句子中的特定词汇并替换为其他词，观察翻译结果是否保持一致，统计这种现象的概率，计算词汇替换的熵值，并在所有词汇上求平均，得到翻译熵。进一步扩展到同时替换两个词的情况，探究熵的叠加效果。

Result: 所提方法成功对多种公开翻译器（MarianMT、T5-Base、NLLB-200）进行了翻译熵评估，实现了对翻译模型的量化排名，发现翻译熵随着解码层的深入而增强，同时揭示了翻译熵的对称性与乘积规律。

Conclusion: 本文首次将翻译熵作为可测属性引入，为自动翻译模型提供了客观、公正的性能评估标准，有助于推动机器翻译技术的科学评估和改进。

Abstract: The translation of written language has been known since the 3rd century BC; however, its necessity has become increasingly common in the information age. Today, many translators exist, based on encoder-decoder deep architectures, nevertheless, no quantitative objective methods are available to assess their performance, likely because the entropy of even a single language remains unknown. This study presents a quantitative method for estimating translation entropy, with the following key finding. Given a translator, several sentences that differ by only one selected token of a given pivot sentence yield identical translations. Analyzing the statistics of this phenomenon across an ensemble of such sentences, consisting each of a pivot selected token, yields the probabilities of replacing this specific token with others while preserving the translation. These probabilities constitute the entropy of the selected token, and the average across all selected pivot tokens provides an estimate of the translator's overall translation entropy, which is enhanced along the decoder blocks. This entropic measure allows for the quantitative ranking of several publicly available translators and reveals whether mutual translation entropy is symmetric. Extending the proposed method to include the replacement of two tokens in a given pivot sentence demonstrates a multiplicative effect, where translation degeneracy is proportional to the product of the degeneracies of the two tokens. These findings establish translation entropy as a measurable property and objective benchmarking of artificial translators. Results are based on MarianMT, T5-Base and NLLB-200 translators.

</details>


### [70] [Evaluating Large Language Models for Diacritic Restoration in Romanian Texts: A Comparative Study](https://arxiv.org/abs/2511.13182)
*Mihai Dan Nadas,Laura Diosan*

Main category: cs.CL

TL;DR: 本文评估了多种大型语言模型在罗马尼亚语自动恢复变音符号任务中的表现，发现GPT-4o表现优异，模型架构和提示设计对结果影响显著。


<details>
  <summary>Details</summary>
Motivation: 针对罗马尼亚语这类带有丰富变音符号的语言，自动恢复变音符号对于文本处理具有重要意义，因此需要评估现有大型语言模型的能力。

Method: 使用涵盖广泛的语料库，测试了包括GPT-3.5、GPT-4、GPT-4o、Google Gemini 1.0 Pro、Meta Llama系列等多个大型语言模型，在多种提示模板下对罗马尼亚语文本进行变音符号恢复。

Result: GPT-4o在恢复准确率上表现最优，显著超过基线模型，而Meta的Llama系列表现波动较大，结果显示模型架构、训练数据和提示设计均显著影响恢复效果。

Conclusion: 研究表明大模型在变音符号恢复任务中展现潜力，为丰富变音符号语言的自然语言处理工具提升提供了明确方向。

Abstract: Automatic diacritic restoration is crucial for text processing in languages with rich diacritical marks, such as Romanian. This study evaluates the performance of several large language models (LLMs) in restoring diacritics in Romanian texts. Using a comprehensive corpus, we tested models including OpenAI's GPT-3.5, GPT-4, GPT-4o, Google's Gemini 1.0 Pro, Meta's Llama 2 and Llama 3, MistralAI's Mixtral 8x7B Instruct, airoboros 70B, and OpenLLM-Ro's RoLlama 2 7B, under multiple prompt templates ranging from zero-shot to complex multi-shot instructions. Results show that models such as GPT-4o achieve high diacritic restoration accuracy, consistently surpassing a neutral echo baseline, while others, including Meta's Llama family, exhibit wider variability. These findings highlight the impact of model architecture, training data, and prompt design on diacritic restoration performance and outline promising directions for improving NLP tools for diacritic-rich languages.

</details>


### [71] [Seeing isn't Hearing: Benchmarking Vision Language Models at Interpreting Spectrograms](https://arxiv.org/abs/2511.13225)
*Tyler Loakman,Joseph James,Chenghua Lin*

Main category: cs.CL

TL;DR: 本文评估了视觉语言模型（VLMs）在解读语音的声谱图和波形图方面的能力，结果显示其表现接近随机水平，反映了模型对此类图像缺乏专门解析能力。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型和视觉语言模型的发展，研究者希望了解它们在融合视觉和语言信息的任务中的表现，尤其是在解释语音声谱图和波形图这类视觉表示时的能力。

Method: 作者合成了一个包含4000多个英文词汇的全新数据集，数据集中包含一致风格的声谱图和波形图。通过多个选择题形式，要求模型从四个选项中选出正确的音素或字母转录，干扰选项基于音素编辑距离设计。

Result: 实验发现，无论是零样本还是微调后的视觉语言模型，其正确率均接近随机水平，这表明仅凭联合样本不足以让模型掌握声谱图和波形图的解读技能。

Conclusion: 视觉语言模型缺乏对语音图像的专门参数知识，未来需要设计更针对性的训练或模型架构以提高其音频可视化的理解能力。

Abstract: With the rise of Large Language Models (LLMs) and their vision-enabled counterparts (VLMs), numerous works have investigated their capabilities in tasks that fuse the modalities of vision and language. In this work, we benchmark the extent to which VLMs are able to act as highly-trained phoneticians, interpreting spectrograms and waveforms of speech. To do this, we synthesise a novel dataset containing 4k+ English words spoken in isolation alongside stylistically consistent spectrogram and waveform figures. We test the ability of VLMs to understand these representations of speech through a multiple-choice task whereby models must predict the correct phonemic or graphemic transcription of a spoken word when presented amongst 3 distractor transcriptions that have been selected based on their phonemic edit distance to the ground truth. We observe that both zero-shot and finetuned models rarely perform above chance, demonstrating the requirement for specific parametric knowledge of how to interpret such figures, rather than paired samples alone.

</details>


### [72] [Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance](https://arxiv.org/abs/2511.13254)
*Shalini Maiti,Amar Budhiraja,Bhavul Gauri,Gaurav Chaurasia,Anton Protopopov,Alexis Audran-Reiss,Michael Slater,Despoina Magka,Tatiana Shavrina,Roberta Raileanu,Yoram Bachrach*

Main category: cs.CL

TL;DR: 本文提出了一种创新的模型融合方法SoCE，通过加权平均不同“专家”模型的权重，提升大语言模型训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型训练资源消耗巨大，且现有的模型融合方法通常采用均匀加权，忽视了不同任务类别间模型表现的差异，限制了性能提升。

Method: SoCE方法通过分析基准测试中不同类别模型表现的低相关性，识别各类别的“专家”模型，并采用非均匀加权平均融合这些模型，以优化整体性能。

Result: 该方法在多语言处理、工具调用及数学任务中均显示出增强的性能和鲁棒性，且在伯克利函数调用排行榜上取得了最先进的成绩。

Conclusion: SoCE通过类别专家模型的加权融合，克服了传统均匀加权的局限性，有效提升了模型性能，展现出广泛的应用潜力。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but their training remains resource- and time-intensive, requiring massive compute power and careful orchestration of training procedures. Model souping-the practice of averaging weights from multiple models of the same architecture-has emerged as a promising pre- and post-training technique that can enhance performance without expensive retraining. In this paper, we introduce Soup Of Category Experts (SoCE), a principled approach for model souping that utilizes benchmark composition to identify optimal model candidates and applies non-uniform weighted averaging to maximize performance. Contrary to previous uniform-averaging approaches, our method leverages the observation that benchmark categories often exhibit low inter-correlations in model performance. SoCE identifies "expert" models for each weakly-correlated category cluster and combines them using optimized weighted averaging rather than uniform weights. We demonstrate that the proposed method improves performance and robustness across multiple domains, including multilingual capabilities, tool calling, and math and achieves state-of-the-art results on the Berkeley Function Calling Leaderboard.

</details>


### [73] [RegionMarker: A Region-Triggered Semantic Watermarking Framework for Embedding-as-a-Service Copyright Protection](https://arxiv.org/abs/2511.13329)
*Shufan Yang,Zifeng Cheng,Zhiwei Jiang,Yafeng Yin,Cong Wang,Shiping Ge,Yuchen Fu,Qing Gu*

Main category: cs.CL

TL;DR: 本文提出了RegionMarker，一种面向嵌入式服务的区域触发语义水印框架，有效防御多种模型提取攻击，保障版权。


<details>
  <summary>Details</summary>
Motivation: 当前嵌入式服务（EaaS）易受模型提取攻击，且现有水印方法无法提供全面的版权保护。

Method: RegionMarker定义低维空间中的触发区域，利用秘密降维矩阵将水印注入对应文本嵌入中，覆盖整个触发区域，并以文本嵌入作为水印，提升对篡改和维度扰动攻击的鲁棒性。

Result: 在多数据集上，RegionMarker证明了其对多种攻击方式的有效防御能力。

Conclusion: RegionMarker通过区域触发机制和全触发区域水印注入，实现了对EaaS版权的全面保护。

Abstract: Embedding-as-a-Service (EaaS) is an effective and convenient deployment solution for addressing various NLP tasks. Nevertheless, recent research has shown that EaaS is vulnerable to model extraction attacks, which could lead to significant economic losses for model providers. For copyright protection, existing methods inject watermark embeddings into text embeddings and use them to detect copyright infringement. However, current watermarking methods often resist only a subset of attacks and fail to provide \textit{comprehensive} protection. To this end, we present the region-triggered semantic watermarking framework called RegionMarker, which defines trigger regions within a low-dimensional space and injects watermarks into text embeddings associated with these regions. By utilizing a secret dimensionality reduction matrix to project onto this subspace and randomly selecting trigger regions, RegionMarker makes it difficult for watermark removal attacks to evade detection. Furthermore, by embedding watermarks across the entire trigger region and using the text embedding as the watermark, RegionMarker is resilient to both paraphrasing and dimension-perturbation attacks. Extensive experiments on various datasets show that RegionMarker is effective in resisting different attack methods, thereby protecting the copyright of EaaS.

</details>


### [74] [AHaSIS: Shared Task on Sentiment Analysis for Arabic Dialects](https://arxiv.org/abs/2511.13335)
*Maram Alharbi,Salmane Chafik,Saad Ezzini,Ruslan Mitkov,Tharindu Ranasinghe,Hansi Hettiarachchi*

Main category: cs.CL

TL;DR: 本文介绍了阿拉伯世界酒店行业针对阿拉伯方言情感分析的共享任务，提供了一个包含现代标准阿拉伯语及沙特和摩洛哥方言的多方言平衡数据集，用于情感检测。


<details>
  <summary>Details</summary>
Motivation: 随着酒店行业日益依赖客户反馈，急需先进的阿拉伯语情感分析工具，特别是能够处理不同方言的工具，以提升客户体验分析的准确性。

Method: 该任务构建了一个由酒店评论组成的多方言数据集，评论起初为现代标准阿拉伯语，经过沙特和摩洛哥方言翻译，并由母语者验证翻译和情感的准确性。通过竞赛形式吸引40余支队伍参与，评估情感分析模型性能。

Result: 共有12个系统提交评估，表现最优系统取得F1分数0.81，显示了方言间情感分析的可行性，但也揭示了仍存在的挑战。

Conclusion: 该研究为多方言阿拉伯语情感分析提供了宝贵数据和基线，推动了实际场景中客户体验分析的技术发展和应用。

Abstract: The hospitality industry in the Arab world increasingly relies on customer feedback to shape services, driving the need for advanced Arabic sentiment analysis tools. To address this challenge, the Sentiment Analysis on Arabic Dialects in the Hospitality Domain shared task focuses on Sentiment Detection in Arabic Dialects. This task leverages a multi-dialect, manually curated dataset derived from hotel reviews originally written in Modern Standard Arabic (MSA) and translated into Saudi and Moroccan (Darija) dialects. The dataset consists of 538 sentiment-balanced reviews spanning positive, neutral, and negative categories. Translations were validated by native speakers to ensure dialectal accuracy and sentiment preservation. This resource supports the development of dialect-aware NLP systems for real-world applications in customer experience analysis. More than 40 teams have registered for the shared task, with 12 submitting systems during the evaluation phase. The top-performing system achieved an F1 score of 0.81, demonstrating the feasibility and ongoing challenges of sentiment analysis across Arabic dialects.

</details>


### [75] [Donors and Recipients: On Asymmetric Transfer Across Tasks and Languages with Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2511.13368)
*Kajetan Dymkiewicz,Ivan Vulic,Helen Yannakoudakis,Eilam Shapira,Roi Reichart,Anna Korhonen*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在不同任务和语言间迁移学习的影响，发现同任务不同语言间迁移效果积极，而跨任务迁移可能导致性能下降，并揭示了迁移中的捐赠者与接受者结构。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在多任务多语言表现优异，但其在不同任务和语言间迁移机制尚不清楚，限制了对模型微调和泛化的理解。

Method: 通过对多个公开权重的大型语言模型家族和不同模型大小进行控制的PEFT/LoRA微调实验，在单一任务-语言上微调后，评估其在其他任务-语言组合上的迁移表现，并解析迁移为匹配任务跨语言、匹配语言跨任务和跨任务跨语言三种情形。

Result: 发现迁移表现存在显著的同任务异语言积极迁移与跨任务迁移性能退化的非对称性，同时存在稳定的捐赠者（hub donors）和脆弱接受者结构，表明某些任务或语言对迁移贡献较大。

Conclusion: 研究结果对风险意识微调和模型专门化有重要启示，提示在提升模型迁移能力时需关注任务和语言间的复杂关系及捐赠者结构。

Abstract: Large language models (LLMs) perform strongly across tasks and languages, yet how improvements in one task or language affect other tasks and languages and their combinations remains poorly understood. We conduct a controlled PEFT/LoRA study across multiple open-weight LLM families and sizes, treating task and language as transfer axes while conditioning on model family and size; we fine-tune each model on a single task-language source and measure transfer as the percentage-point change versus its baseline score when evaluated on all other task-language target pairs. We decompose transfer into (i) Matched-Task (Cross-Language), (ii) Matched-Language (Cross-Task), and (iii) Cross-Task (Cross-Language) regimes. We uncover two consistent general patterns. First, a pronounced on-task vs. off-task asymmetry: Matched-Task (Cross-Language) transfer is reliably positive, whereas off-task transfer often incurs collateral degradation. Second, a stable donor-recipient structure across languages and tasks (hub donors vs. brittle recipients). We outline implications for risk-aware fine-tuning and model specialisation.

</details>


### [76] [Can Large Language Models Function as Qualified Pediatricians? A Systematic Evaluation in Real-World Clinical Contexts](https://arxiv.org/abs/2511.13381)
*Siyu Zhu,Mouxiao Bian,Yue Xie,Yongyu Tang,Zhikang Yu,Tianbin Li,Pengcheng Chen,Bing Han,Jie Xu,Xiaoyan Dong*

Main category: cs.CL

TL;DR: 本文提出了PEDIASBench框架系统评估大型语言模型（LLM）在儿科临床环境中的表现，发现其在基础知识掌握较好但在复杂推理和动态决策方面存在不足，强调未来需加强多模态整合和临床反馈以提升模型安全性和人性化能力。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在医学领域的快速发展，亟需评估其在现实儿科临床中的实用性和能力，明确其优势与不足，指导未来改进方向。

Method: 构建以知识系统为核心、贴合真实临床场景的PEDIASBench评估框架，涵盖基础知识应用、动态诊治能力及医疗安全伦理三大维度，评测包括GPT-4o等在内的12款模型，涉及19个儿科亚专科和211种典型疾病。

Result: 顶尖模型在基础知识题上表现优异，如Qwen3-235B-A22B准确率超过90%，但复杂任务表现下降约15%，动态诊断和治疗适应能力较弱，伦理与安全题目表现相对较好但人文关怀不足。

Conclusion: 目前儿科LLM尚不能独立胜任临床儿科工作，主要受限于复杂推理和人文关怀能力，未来应重点发展多模态融合和临床反馈机制，推动其作为决策支持及医疗教育的安全可信工具。

Abstract: With the rapid rise of large language models (LLMs) in medicine, a key question is whether they can function as competent pediatricians in real-world clinical settings. We developed PEDIASBench, a systematic evaluation framework centered on a knowledge-system framework and tailored to realistic clinical environments. PEDIASBench assesses LLMs across three dimensions: application of basic knowledge, dynamic diagnosis and treatment capability, and pediatric medical safety and medical ethics. We evaluated 12 representative models released over the past two years, including GPT-4o, Qwen3-235B-A22B, and DeepSeek-V3, covering 19 pediatric subspecialties and 211 prototypical diseases. State-of-the-art models performed well on foundational knowledge, with Qwen3-235B-A22B achieving over 90% accuracy on licensing-level questions, but performance declined ~15% as task complexity increased, revealing limitations in complex reasoning. Multiple-choice assessments highlighted weaknesses in integrative reasoning and knowledge recall. In dynamic diagnosis and treatment scenarios, DeepSeek-R1 scored highest in case reasoning (mean 0.58), yet most models struggled to adapt to real-time patient changes. On pediatric medical ethics and safety tasks, Qwen2.5-72B performed best (accuracy 92.05%), though humanistic sensitivity remained limited. These findings indicate that pediatric LLMs are constrained by limited dynamic decision-making and underdeveloped humanistic care. Future development should focus on multimodal integration and a clinical feedback-model iteration loop to enhance safety, interpretability, and human-AI collaboration. While current LLMs cannot independently perform pediatric care, they hold promise for decision support, medical education, and patient communication, laying the groundwork for a safe, trustworthy, and collaborative intelligent pediatric healthcare system.

</details>


### [77] [Mem-PAL: Towards Memory-based Personalized Dialogue Assistants for Long-term User-Agent Interaction](https://arxiv.org/abs/2511.13410)
*Zhaopei Huang,Qifeng Dai,Guozheng Wu,Xiaopeng Wu,Kehan Chen,Chuan Yu,Xubin Li,Tiezheng Ge,Wenxuan Wang,Qin Jin*

Main category: cs.CL

TL;DR: 本文提出了PAL-Bench，一个用于评估长期个性化人机交互的基准，同时构建了中文多会话数据集PAL-Set，并设计了层次异构记忆框架H²Memory以提升个性化对话生成效果。


<details>
  <summary>Details</summary>
Motivation: 随着智能设备的普及，个性化对话助手需求增加，但现有方法忽视了长期交互的复杂性及用户主观特征的捕捉，限制了个性化服务的效果。

Method: 本文构建了多步基于大模型的合成流水线生成中文多会话数据集PAL-Set，基于该数据集提出了层次异构记忆框架H²Memory，结合检索增强生成技术，提升个性化响应质量。

Result: 在构建的PAL-Bench基准及外部数据集上进行了全面实验，结果表明所提H²Memory架构在个性化服务导向交互中具有显著效果提升。

Conclusion: PAL-Bench和PAL-Set为个性化长期人机交互提供了重要资源，H²Memory框架有效提升了个性化对话生成能力，推动了智能服务助手的发展。

Abstract: With the rise of smart personal devices, service-oriented human-agent interactions have become increasingly prevalent. This trend highlights the need for personalized dialogue assistants that can understand user-specific traits to accurately interpret requirements and tailor responses to individual preferences. However, existing approaches often overlook the complexities of long-term interactions and fail to capture users' subjective characteristics. To address these gaps, we present PAL-Bench, a new benchmark designed to evaluate the personalization capabilities of service-oriented assistants in long-term user-agent interactions. In the absence of available real-world data, we develop a multi-step LLM-based synthesis pipeline, which is further verified and refined by human annotators. This process yields PAL-Set, the first Chinese dataset comprising multi-session user logs and dialogue histories, which serves as the foundation for PAL-Bench. Furthermore, to improve personalized service-oriented interactions, we propose H$^2$Memory, a hierarchical and heterogeneous memory framework that incorporates retrieval-augmented generation to improve personalized response generation. Comprehensive experiments on both our PAL-Bench and an external dataset demonstrate the effectiveness of the proposed memory framework.

</details>


### [78] [Non-Linear Scoring Model for Translation Quality Evaluation](https://arxiv.org/abs/2511.13467)
*Serge Gladkoff,Lifeng Han,Katerina Gasova*

Main category: cs.CL

TL;DR: 该论文提出了一种基于非线性对数模型的翻译质量评估方法，解决了传统线性误差惩罚模型对不同长度样本的偏差问题，更准确地反映了人工评价的直觉。


<details>
  <summary>Details</summary>
Motivation: 传统基于多维质量指标（MQM）的翻译质量评估使用线性误差-惩罚刻度，但其对不同长度的样本产生偏差，短样本过度惩罚，长样本惩罚不足，与专家直觉不符。

Method: 基于Multi-Range框架，提出了一个两参数对数函数模型E(x)=a * ln(1 + b * x)，通过参考容忍度和两个容忍点校准模型，以非线性方式反映错误容忍度随样本长度的对数增长。同时结合心理物理学和认知负荷理论进行理论支持。

Result: 实证数据表明，容忍的错误数量随样本长度以对数关系增长，模型改善了可解释性、公平性和评审间一致性。线性近似在±20%相对误差内保持有效，且易于集成入现有评估流程。

Conclusion: 该模型提升了翻译质量评估的准确性和可扩展性，更符合人工感知，有助于提升基于AI的文档级评估和人机生成文本的评价准确性。论文还讨论了CAT/LQA系统的实现及其对人类与AI生成文本评估的影响。

Abstract: Analytic Translation Quality Evaluation (TQE), based on Multidimensional Quality Metrics (MQM), traditionally uses a linear error-to-penalty scale calibrated to a reference sample of 1000-2000 words. However, linear extrapolation biases judgment on samples of different sizes, over-penalizing short samples and under-penalizing long ones, producing misalignment with expert intuition.
  Building on the Multi-Range framework, this paper presents a calibrated, non-linear scoring model that better reflects how human content consumers perceive translation quality across samples of varying length. Empirical data from three large-scale enterprise environments shows that acceptable error counts grow logarithmically, not linearly, with sample size.
  Psychophysical and cognitive evidence, including the Weber-Fechner law and Cognitive Load Theory, supports this premise by explaining why the perceptual impact of additional errors diminishes while the cognitive burden grows with scale. We propose a two-parameter model
  E(x) = a * ln(1 + b * x), a, b > 0,
  anchored to a reference tolerance and calibrated from two tolerance points using a one-dimensional root-finding step. The model yields an explicit interval within which the linear approximation stays within +/-20 percent relative error and integrates into existing evaluation workflows with only a dynamic tolerance function added.
  The approach improves interpretability, fairness, and inter-rater reliability across both human and AI-generated translations. By operationalizing a perceptually valid scoring paradigm, it advances translation quality evaluation toward more accurate and scalable assessment. The model also provides a stronger basis for AI-based document-level evaluation aligned with human judgment. Implementation considerations for CAT/LQA systems and implications for human and AI-generated text evaluation are discussed.

</details>


### [79] [Aspect-Level Obfuscated Sentiment in Thai Financial Disclosures and Its Impact on Abnormal Returns](https://arxiv.org/abs/2511.13481)
*Attapol T. Rutherford,Sirisak Chueykamhang,Thachaparn Bunditlurdruk,Nanthicha Angsuwichitkul*

Main category: cs.CL

TL;DR: 本文提出一种基于方面的情感分析方法，解读泰国金融年报中的隐晦情感，构建注释数据集，并验证情感分析对股价影响的实证效果。


<details>
  <summary>Details</summary>
Motivation: 金融文件中的隐晦语言常掩盖真实情绪，准确解读这些情绪对理解市场行为至关重要。

Method: 制定注释隐晦情感的指南，标注百余份泰国金融年报，使用多种文本分类模型对数据集进行情感分类，并通过事件研究分析情感与股价的关系。

Result: 文本分类模型在情感分类方面表现优异，事件研究表明市场对报告中特定方面的情绪反应更为敏感。

Conclusion: 揭示财务文本中情绪分析的复杂性，强调识别隐晦语言对于准确评估市场情绪的重要性。

Abstract: Understanding sentiment in financial documents is crucial for gaining insights into market behavior. These reports often contain obfuscated language designed to present a positive or neutral outlook, even when underlying conditions may be less favorable. This paper presents a novel approach using Aspect-Based Sentiment Analysis (ABSA) to decode obfuscated sentiment in Thai financial annual reports. We develop specific guidelines for annotating obfuscated sentiment in these texts and annotate more than one hundred financial reports. We then benchmark various text classification models on this annotated dataset, demonstrating strong performance in sentiment classification. Additionally, we conduct an event study to evaluate the real-world implications of our sentiment analysis on stock prices. Our results suggest that market reactions are selectively influenced by specific aspects within the reports. Our findings underscore the complexity of sentiment analysis in financial texts and highlight the importance of addressing obfuscated language to accurately assess market sentiment.

</details>


### [80] [Applying Large Language Models to Characterize Public Narratives](https://arxiv.org/abs/2511.13505)
*Elinor Poole-Dayan,Daniel T Kessler,Hannah Chiou,Margaret Hughes,Emily S Lin,Marshall Ganz,Deb Roy*

Main category: cs.CL

TL;DR: 本文提出利用大型语言模型（LLMs）自动化分析公共叙事（PNs），实现了接近专家水平的质性注释，验证了其在政治演讲等领域的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 公共叙事作为领导力发展和公民动员的重要工具，其系统分析传统上依赖主观解释和昂贵的专家注释，存在难度和成本高的问题。

Method: 作者联合领域专家开发编码手册，利用LLMs进行公共叙事的自动注释，并与专家注释结果进行对比评估，进一步扩展分析至更大数据集和政治演讲文本。

Result: LLMs在8个叙事和14个编码上平均F1得分达到0.80，接近专家水平。扩展分析揭示了公共叙事框架元素在更大数据集和政治演讲中的表现。

Conclusion: LLMs辅助注释可实现公共叙事的可扩展分析，展现了计算公民叙事研究的潜力，同时指出了当前方法的局限性和未来研究方向。

Abstract: Public Narratives (PNs) are key tools for leadership development and civic mobilization, yet their systematic analysis remains challenging due to their subjective interpretation and the high cost of expert annotation. In this work, we propose a novel computational framework that leverages large language models (LLMs) to automate the qualitative annotation of public narratives. Using a codebook we co-developed with subject-matter experts, we evaluate LLM performance against that of expert annotators. Our work reveals that LLMs can achieve near-human-expert performance, achieving an average F1 score of 0.80 across 8 narratives and 14 codes. We then extend our analysis to empirically explore how PN framework elements manifest across a larger dataset of 22 stories. Lastly, we extrapolate our analysis to a set of political speeches, establishing a novel lens in which to analyze political rhetoric in civic spaces. This study demonstrates the potential of LLM-assisted annotation for scalable narrative analysis and highlights key limitations and directions for future research in computational civic storytelling.

</details>


### [81] [Toward Conversational Hungarian Speech Recognition: Introducing the BEA-Large and BEA-Dialogue Datasets](https://arxiv.org/abs/2511.13529)
*Máté Gedeon,Piroska Zsófia Barta,Péter Mihajlik,Tekla Etelka Gráczi,Anna Kohári,Katalin Mády*

Main category: cs.CL

TL;DR: 本文介绍了两个新的匈牙利语自动语音识别数据集BEA-Large和BEA-Dialogue，支持对话语音识别和说话人分离。


<details>
  <summary>Details</summary>
Motivation: 匈牙利语由于缺乏充分的自发和对话语料库，自动语音识别研究受到限制。

Method: 构建了包含大量自发语音和对话的匈牙利语语料库，提供详细的元数据，并使用公开的ASR模型基线进行评估。

Result: 基于Fine-tuned Fast Conformer模型的词错误率最低达到14.18%（自发语音）和4.8%（重复语音）；说话人分离错误率为13.05%-18.26%。

Conclusion: 所发布的数据集和基线能够推动匈牙利语语音技术发展，并为其他语言的自发及对话语音识别研究提供参考框架。

Abstract: The advancement of automatic speech recognition (ASR) has been largely enhanced by extensive datasets in high-resource languages, while languages such as Hungarian remain underrepresented due to limited spontaneous and conversational corpora. To address this gap, we introduce two new datasets -- BEA-Large and BEA-Dialogue -- constructed from the previously unprocessed portions of the Hungarian speech corpus named BEA. BEA-Large extends BEA-Base with 255 hours of spontaneous speech from 433 speakers, enriched with detailed segment-level metadata. BEA-Dialogue, comprising 85 hours of spontaneous conversations, is a Hungarian speech corpus featuring natural dialogues partitioned into speaker-independent subsets, supporting research in conversational ASR and speaker diarization. We establish reproducible baselines on these datasets using publicly available ASR models, with the fine-tuned Fast Conformer model achieving word error rates as low as 14.18\% on spontaneous and 4.8\% on repeated speech. Diarization experiments yield diarization error rates between 13.05\% and 18.26\%, providing reference points for future improvements. The results highlight the persistent difficulty of conversational ASR, particularly due to disfluencies, overlaps, and informal speech patterns. By releasing these datasets and baselines, we aim to advance Hungarian speech technology and offer a methodological framework for developing spontaneous and conversational benchmarks in other languages.

</details>


### [82] [Beyond SELECT: A Comprehensive Taxonomy-Guided Benchmark for Real-World Text-to-SQL Translation](https://arxiv.org/abs/2511.13590)
*Hao Wang,Yuanfeng Song,Xiaoming Yin,Xing Chen*

Main category: cs.CL

TL;DR: 本文提出了一种新的文本到SQL分类法，并基于此设计了生成新数据集SQL-Synth的方法，从而提升数据集的覆盖度和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到SQL的数据集覆盖面有限，缺乏真实应用场景的多样性，影响模型训练和评估效果。

Method: 提出基于核心意图、语句类型、语法结构和关键操作的分类法，并结合大语言模型设计数据集合成流程，产出覆盖面更广的新数据集SQL-Synth。

Result: 新数据集SQL-Synth相较于Spider和Bird等现有数据集在多样性和覆盖度上表现更优；现有大语言模型在SQL-Synth上表现不足，但通过微调性能显著提升。

Conclusion: 该分类法有助于全面分析数据集及模型表现，指导训练数据构建，促进文本到SQL模型的发展。

Abstract: Text-to-SQL datasets are essential for training and evaluating text-to-SQL models, but existing datasets often suffer from limited coverage and fail to capture the diversity of real-world applications. To address this, we propose a novel taxonomy for text-to-SQL classification based on dimensions including core intents, statement types, syntax structures, and key actions. Using this taxonomy, we evaluate widely used public text-to-SQL datasets (e.g., Spider and Bird) and reveal limitations in their coverage and diversity. We then introduce a taxonomy-guided dataset synthesis pipeline, yielding a new dataset named SQL-Synth. This approach combines the taxonomy with Large Language Models (LLMs) to ensure the dataset reflects the breadth and complexity of real-world text-to-SQL applications. Extensive analysis and experimental results validate the effectiveness of our taxonomy, as SQL-Synth exhibits greater diversity and coverage compared to existing benchmarks. Moreover, we uncover that existing LLMs typically fall short in adequately capturing the full range of scenarios, resulting in limited performance on SQL-Synth. However, fine-tuning can substantially improve their performance in these scenarios. The proposed taxonomy has significant potential impact, as it not only enables comprehensive analysis of datasets and the performance of different LLMs, but also guides the construction of training data for LLMs.

</details>


### [83] [Omni Memory System for Personalized, Long Horizon, Self-Evolving Agents](https://arxiv.org/abs/2511.13593)
*Piaohong Wang,Motong Tian,Jiaxian Li,Yuan Liang,Yuqing Wang,Qianben Chen,Tiannan Wang,Zhicong Lu,Jiawei Ma,Yuchen Eleanor Jiang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: 本文提出了一种名为O-Mem的新型记忆框架，通过主动用户画像，实现动态提取和更新用户特征及事件记录，从而提升个性化响应的连贯性和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有基于语义分组的记忆系统在检索时可能忽略一些关键但语义无关的信息，导致上下文一致性和个性化方面表现不足，难以支持复杂环境中的长期交互。

Method: O-Mem通过主动用户画像，支持用户属性和话题相关上下文的层级检索，动态管理用户特征和交互事件，提升记忆检索的精确性和上下文关联度。

Result: 在LoCoMo公开基准测试中，O-Mem达到了51.76%，较之前最佳LangMem提升约3%；在PERSONAMEM测试中达到62.99%，较之前最佳A-Mem提升3.5%；同时提升了响应时间效率。

Conclusion: O-Mem显著提高了个性化长期交互的效果及效率，展示了构建高效且类人化个性化AI助手的潜力。

Abstract: Recent advancements in LLM-powered agents have demonstrated significant potential in generating human-like responses; however, they continue to face challenges in maintaining long-term interactions within complex environments, primarily due to limitations in contextual consistency and dynamic personalization. Existing memory systems often depend on semantic grouping prior to retrieval, which can overlook semantically irrelevant yet critical user information and introduce retrieval noise. In this report, we propose the initial design of O-Mem, a novel memory framework based on active user profiling that dynamically extracts and updates user characteristics and event records from their proactive interactions with agents. O-Mem supports hierarchical retrieval of persona attributes and topic-related context, enabling more adaptive and coherent personalized responses. O-Mem achieves 51.76% on the public LoCoMo benchmark, a nearly 3% improvement upon LangMem,the previous state-of-the-art, and it achieves 62.99% on PERSONAMEM, a 3.5% improvement upon A-Mem,the previous state-of-the-art. O-Mem also boosts token and interaction response time efficiency compared to previous memory frameworks. Our work opens up promising directions for developing efficient and human-like personalized AI assistants in the future.

</details>


### [84] [Why is "Chicago" Predictive of Deceptive Reviews? Using LLMs to Discover Language Phenomena from Lexical Cues](https://arxiv.org/abs/2511.13658)
*Jiaming Qu,Mengtian Guo,Yue Wang*

Main category: cs.CL

TL;DR: 该论文利用大型语言模型（LLMs）将机器学习分类器学到的词汇线索转化为人类可理解的语言现象，用以区分虚假评论和真实评论，提高了可解释性和预测能力。


<details>
  <summary>Details</summary>
Motivation: 虚假评论误导消费者、损害商业信誉且难以被现有机器学习分类器所直观理解，亟需将分类器学到的特征转换为人类易理解的语言形式，帮助人们识别虚假信息。

Method: 利用大型语言模型将机器学习中学到的词汇线索翻译成易于人类理解的语言现象，并验证这些现象的泛化能力和预测性能；比较了这些现象与模型先验知识及上下文学习方法的效果。

Result: 通过实验证明，用LLMs获得的语言现象基于数据实证，能够在类似领域泛化，并在预测虚假评论方面优于模型的先验知识与上下文学习方法。

Conclusion: 利用LLMs将机器学习的隐性特征转化为直观语言现象，能够有效辅助消费者在缺乏检测工具时识别虚假评论，提升在线评论环境的可信度。

Abstract: Deceptive reviews mislead consumers, harm businesses, and undermine trust in online marketplaces. Machine learning classifiers can learn from large amounts of training examples to effectively distinguish deceptive reviews from genuine ones. However, the distinguishing features learned by these classifiers are often subtle, fragmented, and difficult for humans to interpret. In this work, we explore using large language models (LLMs) to translate machine-learned lexical cues into human-understandable language phenomena that can differentiate deceptive reviews from genuine ones. We show that language phenomena obtained in this manner are empirically grounded in data, generalizable across similar domains, and more predictive than phenomena either in LLMs' prior knowledge or obtained through in-context learning. These language phenomena have the potential to aid people in critically assessing the credibility of online reviews in environments where deception detection classifiers are unavailable.

</details>


### [85] [Crossing Borders: A Multimodal Challenge for Indian Poetry Translation and Image Generation](https://arxiv.org/abs/2511.13689)
*Sofia Jamil,Kotla Sai Charan,Sriparna Saha,Koustava Goswami,Joseph K J*

Main category: cs.CL

TL;DR: 本文提出了翻译与图像生成（TAI）框架，利用大型语言模型和潜在扩散模型，提升印度语言诗歌的翻译和视觉表现，促进文化传播与教育公平。


<details>
  <summary>Details</summary>
Motivation: 印度诗歌语言复杂，文化丰富，存在理解和传播障碍，尤其是对非母语读者。现有诗歌研究多忽视印度语言诗歌，缺乏有效的翻译和视觉表现方法。

Method: 设计TAI框架，包括（1）基于赔率比偏好对齐算法的翻译模块，将形态丰富的印度诗歌准确译成英语；（2）基于语义图的图像生成模块，捕捉比喻与其含义的语义关系，生成具有视觉意义的诗歌图像。

Result: TAI扩散模型在诗歌图像生成任务中表现优越，经过人类和定量评估均超越了强基线方法。同时构建了包含21种资源匮乏印度语的1570首诗歌的MorphoVerse数据集。

Conclusion: 该工作填补了印度语言诗歌在翻译与视觉理解上的空白，促进了文化的全球传播和读者体验的丰富，有助于实现联合国可持续发展目标中的优质教育与减少不平等。

Abstract: Indian poetry, known for its linguistic complexity and deep cultural resonance, has a rich and varied heritage spanning thousands of years. However, its layered meanings, cultural allusions, and sophisticated grammatical constructions often pose challenges for comprehension, especially for non-native speakers or readers unfamiliar with its context and language. Despite its cultural significance, existing works on poetry have largely overlooked Indian language poems. In this paper, we propose the Translation and Image Generation (TAI) framework, leveraging Large Language Models (LLMs) and Latent Diffusion Models through appropriate prompt tuning. Our framework supports the United Nations Sustainable Development Goals of Quality Education (SDG 4) and Reduced Inequalities (SDG 10) by enhancing the accessibility of culturally rich Indian-language poetry to a global audience. It includes (1) a translation module that uses an Odds Ratio Preference Alignment Algorithm to accurately translate morphologically rich poetry into English, and (2) an image generation module that employs a semantic graph to capture tokens, dependencies, and semantic relationships between metaphors and their meanings, to create visually meaningful representations of Indian poems. Our comprehensive experimental evaluation, including both human and quantitative assessments, demonstrates the superiority of TAI Diffusion in poem image generation tasks, outperforming strong baselines. To further address the scarcity of resources for Indian-language poetry, we introduce the Morphologically Rich Indian Language Poems MorphoVerse Dataset, comprising 1,570 poems across 21 low-resource Indian languages. By addressing the gap in poetry translation and visual comprehension, this work aims to broaden accessibility and enrich the reader's experience.

</details>


### [86] [Generalist Foundation Models Are Not Clinical Enough for Hospital Operations](https://arxiv.org/abs/2511.13703)
*Lavender Y. Jiang,Angelica Chen,Xu Han,Xujin Chris Liu,Radhika Dua,Kevin Eaton,Frederick Wolff,Robert Steele,Jeff Zhang,Anton Alyakin,Qingkai Pan,Yanbing Chen,Karl L. Sangwon,Daniel A. Alber,Jaden Stryker,Jin Vivian Lee,Yindalon Aphinyanaphongs,Kyunghyun Cho,Eric Karl Oermann*

Main category: cs.CL

TL;DR: 本文提出了Lang1系列模型，结合了临床电子健康记录和互联网数据进行预训练，专门用于医疗运营任务。通过新的现实医学评估基准ReMedE，展示了Lang1在五个关键任务上的优越性能，特别是在微调后表现显著优于通用模型。


<details>
  <summary>Details</summary>
Motivation: 传统通用基础模型虽然在医学知识和对话任务中表现良好，但缺乏针对医院运营决策所需的专业知识。为提升医疗运营的预测能力，亟需专门针对电子健康记录（EHR）数据预训练的模型。

Method: 提出Lang1模型系列（参数量从1亿到70亿），在融合了纽约大学Langone健康系统的80B临床电子健康记录和627B互联网语料的大规模混合语料上进行预训练。设计了基于668,331条EHR笔记的现实医学评估基准ReMedE，评测五个关键任务。进行零样本和微调评估，并探索跨任务联合微调。

Result: 在零样本情况下，模型在大多数任务的表现不佳，微调后Lang1-1B在多项指标上显著优于比其大70倍的通用模型，AUROC提升3.64%-6.75%，超越零样本超大模型高达23.66%。跨任务联合微调提升了多任务表现，模型在外部分布和其他临床任务中也表现出良好迁移能力。

Conclusion: 医院运营的预测任务需要明确的监督微调，而且在领域内EHR数据的预训练极大地提升了微调效率和效果。专门领域大模型可与通用模型一较高下，构建有效的医疗系统AI需要结合领域预训练、监督微调及真实世界严格评估。

Abstract: Hospitals and healthcare systems rely on operational decisions that determine patient flow, cost, and quality of care. Despite strong performance on medical knowledge and conversational benchmarks, foundation models trained on general text may lack the specialized knowledge required for these operational decisions. We introduce Lang1, a family of models (100M-7B parameters) pretrained on a specialized corpus blending 80B clinical tokens from NYU Langone Health's EHRs and 627B tokens from the internet. To rigorously evaluate Lang1 in real-world settings, we developed the REalistic Medical Evaluation (ReMedE), a benchmark derived from 668,331 EHR notes that evaluates five critical tasks: 30-day readmission prediction, 30-day mortality prediction, length of stay, comorbidity coding, and predicting insurance claims denial. In zero-shot settings, both general-purpose and specialized models underperform on four of five tasks (36.6%-71.7% AUROC), with mortality prediction being an exception. After finetuning, Lang1-1B outperforms finetuned generalist models up to 70x larger and zero-shot models up to 671x larger, improving AUROC by 3.64%-6.75% and 1.66%-23.66% respectively. We also observed cross-task scaling with joint finetuning on multiple tasks leading to improvement on other tasks. Lang1-1B effectively transfers to out-of-distribution settings, including other clinical tasks and an external health system. Our findings suggest that predictive capabilities for hospital operations require explicit supervised finetuning, and that this finetuning process is made more efficient by in-domain pretraining on EHR. Our findings support the emerging view that specialized LLMs can compete with generalist models in specialized tasks, and show that effective healthcare systems AI requires the combination of in-domain pretraining, supervised finetuning, and real-world evaluation beyond proxy benchmarks.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [87] [WITNESS: A lightweight and practical approach to fine-grained predictive mutation testing](https://arxiv.org/abs/2511.11999)
*Zeyu Lu,Peng Zhang,Chun Yong Chong,Shan Gao,Yibiao Yang,Yanhui Li,Lin Chen,Yuming Zhou*

Main category: cs.SE

TL;DR: 该论文提出了WITNESS，一种轻量级的机器学习方法，用于细粒度预测突变测试，解决了深度学习模型计算成本高和适用范围受限的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的细粒度预测突变测试主要依赖深度学习，面临计算资源消耗大和只能预测方法内部突变体的问题，限制了其实用性。

Method: WITNESS收集方法内外的突变体特征，适用于所有突变体，并采用轻量级经典机器学习模型进行训练和预测，降低计算成本并增强可解释性。

Result: 在Defects4J项目上的评估表明，WITNESS在各种场景下均达到了最先进的预测性能，大幅提升了杀死矩阵预测的效率，并在测试用例优先级排序任务中表现优于基线方法。

Conclusion: WITNESS有效解决了预测突变测试中的关键瓶颈，兼顾性能和效率，具有更广泛的应用潜力。

Abstract: Existing fine-grained predictive mutation testing studies predominantly rely on deep learning, which faces two critical limitations in practice: (1) Exorbitant computational costs. The deep learning models adopted in these studies demand significant computational resources for training and inference acceleration. This introduces high costs and undermines the cost-reduction goal of predictive mutation testing. (2) Constrained applicability. Although modern mutation testing tools generate mutants both inside and outside methods, current fine-grained predictive mutation testing approaches handle only inside-method mutants. As a result, they cannot predict outside-method mutants, limiting their applicability in real-world scenarios. We propose WITNESS, a new fine-grained predictive mutation testing approach. WITNESS adopts a twofold design: (1) With collected features from both inside-method and outside-method mutants, WITNESS is suitable for all generated mutants. (2) Instead of using computationally expensive deep learning, WITNESS employs lightweight classical machine learning models for training and prediction. This makes it more cost-effective and enabling straightforward explanations of the decision-making processes behind the adopted models. Evaluations on Defects4J projects show that WITNESS consistently achieves state-of-the-art predictive performance across different scenarios. Additionally, WITNESS significantly enhances the efficiency of kill matrix prediction. Post-hoc analysis reveals that features incorporating information from before and after the mutation are the most important among those used in WITNESS. Test case prioritization based on the predicted kill matrix shows that WITNESS delivers results much closer to those obtained by using the actual kill matrix, outperforming baseline approaches.

</details>


### [88] [A Code Smell Refactoring Approach using GNNs](https://arxiv.org/abs/2511.12069)
*HanYu Zhang,Tomoji Kishi*

Main category: cs.SE

TL;DR: 本文提出了一种基于图神经网络的代码异味重构方法，通过设计类级和方法级两种输入图，结合图分类与节点分类任务，有效解决长方法、大类和特征嫉妒三种代码异味问题。


<details>
  <summary>Details</summary>
Motivation: 现有的代码异味重构方法存在依赖人工定义阈值及启发式规则、数据集和模型设计限制等缺陷，需要一种更有效且自动化程度更高的重构方法。

Method: 设计类级和方法级两种输入图，采用图分类和节点分类任务，利用三种经典图神经网络架构（GCN、GraphSAGE、GAT）进行训练，并提出半自动化数据集生成方法以扩展数据规模。

Result: 基于图神经网络的重构方法在实验中表现优于传统方法和现有深度学习技术，展示了更好的重构效果和性能。

Conclusion: 本文提出的图神经网络方法和半自动化数据集生成策略，为代码异味重构提供了高效且效果优越的解决方案，推动了该领域的发展。

Abstract: Code smell is a great challenge in software refactoring, which indicates latent design or implementation flaws that may degrade the software maintainability and evolution. Over the past decades, a variety of refactoring approaches have been proposed, which can be broadly classified into metrics-based, rule-based, and machine learning-based approaches. Recent years, deep learning-based approaches have also attracted widespread attention. However, existing techniques exhibit various limitations. Metrics- and rule-based approaches rely heavily on manually defined heuristics and thresholds, whereas deep learning-based approaches are often constrained by dataset availability and model design. In this study, we proposed a graph-based deep learning approach for code smell refactoring. Specifically, we designed two types of input graphs (class-level and method-level) and employed both graph classification and node classification tasks to address the refactoring of three representative code smells: long method, large class, and feature envy. In our experiment, we propose a semi-automated dataset generation approach that could generate a large-scale dataset with minimal manual effort. We implemented the proposed approach with three classical GNN (graph neural network) architectures: GCN, GraphSAGE, and GAT, and evaluated its performance against both traditional and state-of-the-art deep learning approaches. The results demonstrate that proposed approach achieves superior refactoring performance.

</details>


### [89] [Actionable Warning Is Not Enough: Recommending Valid Actionable Warnings with Weak Supervision](https://arxiv.org/abs/2511.12229)
*Zhipeng Xue,Zhipeng Gao,Tongtong Xu,Xing Hu,Xin Xia,Shanping Li*

Main category: cs.SE

TL;DR: 本文通过构建大规模可操作告警数据集，并提出两阶段推荐框架ACWRecommender，有效提升静态代码分析工具中真实缺陷警告的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 静态分析工具误报率高，影响其普及；现有区分可操作告警与误报的方法假设不准确，导致大量无效告警。

Method: 构建包含68274次代码回退的可操作告警数据集，并为每条告警赋予弱标签；提出两阶段框架ACWRecommender，先粗粒度识别可操作告警，再通过弱监督细粒度重排序，提高推荐真实缺陷告警的准确性。

Result: 实验中，ACWRecommender在nDCG和MRR指标上显著优于多种基线方法；实际应用于6个项目的2197条告警，前10推荐中已有27条被开发者确认属实缺陷。

Conclusion: 该方法显著提升了静态分析工具中真实缺陷告警的推荐效果，帮助开发者快速定位真实缺陷，验证了方法的实用价值。

Abstract: The use of static analysis tools has gained increasing popularity among developers in the last few years. However, the widespread adoption of static analysis tools is hindered by their high false alarm rates. Previous studies have introduced the concept of actionable warnings and built a machine-learning method to distinguish actionable warnings from false alarms. However, according to our empirical observation, the current assumption used for actionable warning(s) collection is rather shaky and inaccurate, leading to a large number of invalid actionable warnings. To address this problem, in this study, we build the first large actionable warning dataset by mining 68,274 reversions from Top-500 GitHub C repositories, we then take one step further by assigning each actionable warning a weak label regarding its likelihood of being a real bug. Following that, we propose a two-stage framework called ACWRecommender to automatically recommend the actionable warnings with high probability to be real bugs (AWHB). Our approach warms up the pre-trained model UniXcoder by identifying actionable warnings task (coarse-grained detection stage) and rerank AWHB to the top by weakly supervised learning (fine-grained reranking stage). Experimental results show that our proposed model outperforms several baselines by a large margin in terms of nDCG and MRR for AWHB recommendation. Moreover, we ran our tool on 6 randomly selected projects and manually checked the top-ranked warnings from 2,197 reported warnings, we reported top-10 recommended warnings to developers, 27 of them were already confirmed by developers as real bugs. Developers can quickly find real bugs among the massive amount of reported warnings, which verifies the practical usage of our tool.

</details>


### [90] [Reflections on the design, applications and implementations of the normative specification language eFLINT](https://arxiv.org/abs/2511.12276)
*L. Thomas van Binsbergen,Christopher A. Esterhuyse,Tim Müller*

Main category: cs.SE

TL;DR: 本文介绍了一种专门用于自动合规检查的领域特定编程语言eFLINT，该语言结合了声明式和过程式元素，以应对法律合规自动化的复杂性。


<details>
  <summary>Details</summary>
Motivation: 随着软件深度嵌入社会实践，检查软件是否符合法律法规变得越来越重要且成本高昂，同时法律及其解释不断变化，要求合规实践高度适应性，自动化合规成为潜在需求。

Method: 开发并反思领域特定语言eFLINT，该语言将法律概念与计算概念形式化连接，结合声明式和过程式编程，用于在软件运行前、中、后自动执行合规检查。

Result: 通过回顾不同应用所提出的需求及其相互冲突，本文总结了语言设计中的权衡及决策，展示了eFLINT在自动合规领域的实用性和灵活性。

Conclusion: eFLINT语言设计的经验和洞见为自动合规领域的语言开发者提供了有价值的参考和指导。

Abstract: Checking the compliance of software against laws, regulations and contracts is increasingly important and costly as the embedding of software into societal practices is getting more pervasive. Moreover, the digitalised services provided by governmental organisations and companies are governed by an increasing amount of laws and regulations, requiring highly adaptable compliance practices. A potential solution is to automate compliance using software. However, automating compliance is difficult for various reasons. Legal practices involve subjective processes such as interpretation and qualification. New laws and regulations come into effect regularly and laws and regulations, as well as their interpretations, are subjected to constant revision. In addition, computational reasoning with laws requires a cross-disciplinary process involving both legal and software expertise.
  This paper reflects on the domain-specific software language eFLINT developed to experiment with novel solutions. The language combines declarative and procedural elements to reason about situations and scenarios respectively, explicates and formalises connections between legal concepts and computational concepts, and is designed to automate compliance checks both before, during and after a software system runs. The various goals and applications areas for the language give rise to (conflicting) requirements. This paper reflects on the current design of the language by recalling various applications, the requirements they imposed, and subsequent design decisions. As such, this paper reports on results and insights of an investigation that can benefit language developers within the field of automated compliance.

</details>


### [91] [Reducing Hallucinations in LLM-Generated Code via Semantic Triangulation](https://arxiv.org/abs/2511.12288)
*Yihan Dai,Sijie Liang,Haotian Xu,Peichu Xie,Sergey Mechtaev*

Main category: cs.SE

TL;DR: 本文提出语义三角测量方法，通过变换问题语义并验证解的一致性，提高了从大语言模型生成代码中选择正确解的可靠性，解决了现有方法在低概率采样和多解情况的不足。


<details>
  <summary>Details</summary>
Motivation: 现有采样共识技术在生成代码选择正确程序时，面对低采样概率或多个有效但不等价解的问题常常失败，且难以在无正确解时有效放弃。

Method: 引入语义三角测量，通过语义变换保持解的一一映射关系，验证不同变换后的解一致性，从理论上增强生成程序的准确泛化信心，提高共识和放弃决策的可靠性。

Result: 在LiveCodeBench和CodeElo基准测试中，使用GPT-4o和DeepSeek-V3模型，语义三角测量使生成代码的可靠性提高了21%，能够在概率低至0.14时识别正确解，并且能在多解任务中实现真正的共识。

Conclusion: 语义三角测量有效克服了传统采样共识方法的局限，显著提升了代码生成准确性和应用范围，是解决多解及参数难题的一种创新手段。

Abstract: When generating code from natural language prompts, an LLM samples programs from a probability distribution, many of which might be incorrect. Sample consensus techniques - such as majority voting or validation against generated tests or specifications - aim to identify a correct program in the sample or abstain if none is valid. However, existing methods often fail to select a correct solution when its sampling probability is low, or when the problem permits multiple valid but non-equivalent solutions. Additionally, they often fail to abstain when no correct solution is present in the sample. To overcome these limitations, we introduce semantic triangulation, which transforms a programming problem in a way that non-trivially alters its semantics while preserving an exact, verifiable mapping between solutions before and after transformation. We theoretically establish that verifying consistency across such problem transformations increases confidence that generated programs reflect accurate generalization rather than spurious statistical correlations, enabling more reliable sample consensus and abstention. On the LiveCodeBench and CodeElo benchmarks, using GPT-4o and DeepSeek-V3 models, semantic triangulation increases reliability of generated code by 21% compared to the method that selects only high-confidence solutions with the probability threshold 0.5, while being able to pinpoint correct solutions at sampling probabilities as low as 0.14. Apart from that, it is also the only approach to consistently form true consensus on tasks with multiple valid but non-equivalent solutions.

</details>


### [92] [ProofWright: Towards Agentic Formal Verification of CUDA](https://arxiv.org/abs/2511.12294)
*Bodhisatwa Chatterjee,Drew Zagieboylo,Sana Damani,Siva Hari,Christos Kozyrakis*

Main category: cs.SE

TL;DR: ProofWright结合自动形式验证与大语言模型代码生成，实现对生成的CUDA内核的安全性及语义正确性验证，提升了代码可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成的CUDA内核存在细微错误且缺乏形式安全保证，传统测试和人工验证各有局限，导致验证成为瓶颈。

Method: ProofWright框架融合自动形式验证和LLM自动代码生成，提供内存安全、线程安全和语义正确性的端到端保证。

Result: 在KernelBench L1测试中，ProofWright验证了74%的内核安全属性，发现传统测试未能捕捉的细微错误，且实现了一类元素级内核的语义等价验证。

Conclusion: ProofWright以每核3分钟的额外开销，实现了LLM生成GPU代码的可扩展自动形式验证，为高性能可信代码生成提供了可行路径。

Abstract: Large Language Models (LLMs) are increasingly used to automatically generate optimized CUDA kernels, substantially improving developer productivity. However, despite rapid generation, these kernels often contain subtle correctness bugs and lack formal safety guarantees. Runtime testing is inherently unreliable - limited input coverage and reward hacking can mask incorrect behavior - while manual formal verification is reliable but cannot scale to match LLM output rates, creating a critical validation bottleneck.
  We present ProofWright, an agentic verification framework that bridges this gap by integrating automated formal verification with LLM-based code generation. ProofWright provides end-to-end guarantees of memory safety, thread safety, and semantic correctness for LLM-generated CUDA kernels. On KernelBench L1, ProofWright verifies safety properties for 74% of generated kernels, uncovers subtle correctness errors missed by conventional testing, and establishes semantic equivalence for a class of element-wise kernels. With a modest overhead of 3 minutes per kernel, ProofWright demonstrates that scalable, automated formal verification of LLM-generated GPU code is feasible - offering a path toward trustworthy high-performance code generation without sacrificing developer productivity.

</details>


### [93] [High-level reasoning while low-level actuation in Cyber-Physical Systems: How efficient is it?](https://arxiv.org/abs/2511.12543)
*Burak Karaduman,Baris Tekin Tezel,Moharram Challenger*

Main category: cs.SE

TL;DR: 该研究比较了六种编程语言和框架在工业信息集成系统中最坏执行时间和开发时间的表现，揭示了抽象层次和推理能力对开发效率和运行效率的影响。


<details>
  <summary>Details</summary>
Motivation: 工业信息集成系统日益复杂，现有编程语言与框架选择缺乏实证指导，工程师难以权衡开发效率与系统性能。

Method: 采用开发者中心的实证比较，针对C++、Java、Jade、Jason等六种技术，测量最坏执行时间与开发时间，分析抽象层次与推理能力对结果的影响。

Result: 研究发现较高抽象层次和推理机制会影响开发工作量与运行性能，揭示了工程负担与执行效率之间的具体权衡。

Conclusion: 研究为工业领域选择软件技术提供了基于实证的决策支持，促进智能代理系统设计，提升集成效率、维护性及实时响应能力，并为未来语言特性和开发动态研究奠定基础。

Abstract: The increasing complexity of industrial information-integration systems demands software technologies that enable intelligent behaviour, real-time response, and efficient development. Although many programming languages and frameworks exist, engineers still lack sufficient empirical evidence to guide the choice of tools for advanced industrial applications. This study addresses that need by measuring and comparing worst-case execution time (WCET) and development time across six languages and frameworks: C++, Java, Jade, Jason, and fuzzy Jason BDI with both loosely and tightly coupled integration. These technologies reflect a progression from procedural and object-oriented programming to agent-based frameworks capable of symbolic and fuzzy reasoning.
  Rather than relying on broad concepts such as paradigms or orientations, the study adopts a developer-centred approach grounded in measurable outcomes. The structured comparison examines how rising abstraction levels and reasoning capabilities affect both development effort and runtime behaviour. By analysing these dimensions, the study highlights concrete trade-offs between engineering workload and execution efficiency.
  The findings show how abstraction and reasoning mechanisms shape system performance and developer productivity, offering practical insight for designing intelligent, agent-based solutions that must operate under real-time constraints and complex decision-making requirements. Overall, the study contributes evidence-based guidance for selecting software technologies in industrial informatization, supporting improved integration efficiency, maintainability, and responsiveness, and laying groundwork for future research on the interplay between language features, development dynamics, and runtime behaviour in cyber-physical and smart manufacturing systems.

</details>


### [94] [Can Small GenAI Language Models Rival Large Language Models in Understanding Application Behavior?](https://arxiv.org/abs/2511.12576)
*Mohammad Meymani,Hamed Jelodar,Parisa Hamedi,Roozbeh Razavi-Far,Ali A. Ghorbani*

Main category: cs.SE

TL;DR: 本文系统评估了大与小规模生成式AI语言模型在应用行为理解中的表现，重点使用恶意软件检测任务进行比较。


<details>
  <summary>Details</summary>
Motivation: 探讨在资源受限环境下，小型生成式AI模型是否能在应用行为分析和恶意软件检测中保持竞争力。

Method: 通过对多种规模的生成式语言模型在准确率、精确率、召回率和F1分数等指标上进行系统比较，分析其性能和资源消耗。

Result: 大型模型整体表现较优，但小型模型在精确率和召回率方面依然有竞争力，且具备计算效率高、推理速度快和易部署的优势。

Conclusion: 小型生成式AI模型能够有效补充大型模型，在实际应用行为分析中实现性能与资源效率的平衡，适合资源受限环境应用。

Abstract: Generative AI (GenAI) models, particularly large language models (LLMs), have transformed multiple domains, including natural language processing, software analysis, and code understanding. Their ability to analyze and generate code has enabled applications such as source code summarization, behavior analysis, and malware detection. In this study, we systematically evaluate the capabilities of both small and large GenAI language models in understanding application behavior, with a particular focus on malware detection as a representative task. While larger models generally achieve higher overall accuracy, our experiments show that small GenAI models maintain competitive precision and recall, offering substantial advantages in computational efficiency, faster inference, and deployment in resource-constrained environments. We provide a detailed comparison across metrics such as accuracy, precision, recall, and F1-score, highlighting each model's strengths, limitations, and operational feasibility. Our findings demonstrate that small GenAI models can effectively complement large ones, providing a practical balance between performance and resource efficiency in real-world application behavior analysis.

</details>


### [95] [LLM4SCREENLIT: Recommendations on Assessing the Performance of Large Language Models for Screening Literature in Systematic Reviews](https://arxiv.org/abs/2511.12635)
*Lech Madeyski,Barbara Kitchenham,Martin Shepperd*

Main category: cs.SE

TL;DR: 本文针对大型语言模型在系统评价中筛选相关文献的性能评估问题进行了全面分析，发现当前评估指标使用存在偏差，提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型快速迭代，而用户难以全面评估其在系统评价文献筛选中的性能，存在显著评估方法不足。

Method: 以一项大型研究为例，分析其传统评估指标的不足，同时综合分析27篇相关文献的评估指标和方法，归纳出问题和良好实践。

Result: 发现常用指标如准确率不适用于不平衡数据，未充分考虑遗漏证据影响，且未完整报告混淆矩阵，影响后续分析；同时总结出若干良好评估实践。

Conclusion: 建议系统评价筛选应重点关注召回率和加权MCC指标，完整报告混淆矩阵，将不可分类输出视为需复审，提高设计的严谨性，基于成本收益分析优化决策。

Abstract: Context: Large language models (LLMs) are released faster than users' ability to evaluate them rigorously. When LLMs underpin research, such as identifying relevant literature for systematic reviews (SRs), robust empirical assessment is essential. Objective: We identify and discuss key challenges in assessing LLM performance for selecting relevant literature, identify good (evaluation) practices, and propose recommendations. Method: Using a recent large-scale study as an example, we identify problems with the use of traditional metrics for assessing the performance of Gen-AI tools for identifying relevant literature in SRs. We analyzed 27 additional papers investigating this issue, extracted the performance metrics, and found both good practices and widespread problems, especially with the use and reporting of performance metrics for SR screening. Results: Major weaknesses included: i) a failure to use metrics that are robust to imbalanced data and do not directly indicate whether results are better than chance, e.g., the use of Accuracy, ii) a failure to consider the impact of lost evidence when making claims concerning workload savings, and iii) pervasive failure to report the full confusion matrix (or performance metrics from which it can be reconstructed) which is essential for future meta-analyses. On the positive side, we extract good (evaluation) practices on which our recommendations for researchers and practitioners, as well as policymakers, are built. Conclusions: SR screening evaluations should prioritize lost evidence/recall alongside chance-anchored and cost-sensitive Weighted MCC (WMCC) metric, report complete confusion matrices, treat unclassifiable outputs as referred-back positives for assessment, adopt leakage-aware designs with non-LLM baselines and open artifacts, and ground conclusions in cost-benefit analysis where FNs carry higher penalties than FPs.

</details>


### [96] [Enhancing LLM Code Generation Capabilities through Test-Driven Development and Code Interpreter](https://arxiv.org/abs/2511.12823)
*Sajed Jalil,Shuvo Saha,Hossain Mohammad Seym*

Main category: cs.SE

TL;DR: 本文提出了一种结合测试驱动开发和代码解释器的新方法，利用开放权重模型提升了孟加拉语提示下代码生成的准确率，最高达85%。无需微调即可实现模型性能提升，支持资源受限环境。


<details>
  <summary>Details</summary>
Motivation: 尽管孟加拉语使用者众多，但在大型语言模型训练中常被忽视。现有提升代码生成性能的方法需高专业技术和资源，限制了资源受限市场用户的使用。

Method: 结合测试驱动开发（TDD）和代码解释器（CI）技术，使用开放权重模型生成代码，无需模型微调。

Result: 该方法在孟加拉语提示的代码生成任务中达到了85%的准确率，且同型号中最小的模型达到98%的性能与最大模型相近。

Conclusion: 该方法实现了无需微调即可显著提升代码生成能力，使孟加拉语用户和资源受限地区能够更方便地使用强大的代码生成工具。所有实验结果公开共享以保证可验证性和复现性。

Abstract: Over the past few years, improving LLM code generation capabilities has been a key focus in NLP research. Despite Bengali having 242 million native speakers worldwide, it receives little attention when it comes to training LLMs. More recently, various fine-tuning and augmented generation techniques have been employed to significantly enhance code generation performance. However, they require considerable expertise and resources to utilize effectively as an end user. The goal of our work is to democratize access to powerful code generation tools in resource-constrained emerging markets, enabling users to leverage them in their native language.
  We introduce a novel approach that combines Test-Driven Development (TDD) and Code Interpreter (CI), utilizing open-weight models, which improves the baseline accuracy for code generation with Bengali prompts and achieves an overall accuracy of 85%. Our approach requires no finetuning and proves that even the smallest models in the same family can attain up to 98% accuracy compared to the largest models. All of our results are publicly shared in GitHub for validation and reproducibility.

</details>


### [97] [Human-Centred Requirements Engineering for Critical Systems: Insights from Disaster Early Warning Applications](https://arxiv.org/abs/2511.12856)
*Anuradha Madugalla,Jixuan Dong,Kai Lyne Loi,Matthew Crossman,John Grundy*

Main category: cs.SE

TL;DR: 本文提出将以人为中心的需求工程过程整合进关键系统开发，以提升系统的安全性和公平性。


<details>
  <summary>Details</summary>
Motivation: 传统关键系统的需求工程侧重技术保障，忽视了系统所处的人文社会背景，影响系统的依赖性和安全性。

Method: 通过文献综述提炼设计弱势群体软件的指导原则，制定了62条功能和非功能需求，设计了一个适应性预警系统原型，并通过访谈和认知演练验证需求的相关性和适用性。

Result: 研究发现，早期考虑以人为中心的需求能显著提升系统的可用性和无障碍性。

Conclusion: 以人为中心的需求工程不仅是道德考量，更是确保关键系统安全、公平的核心要素。

Abstract: Critical systems, such as those used in healthcare, defence, and disaster management, demand rigorous requirements engineering to ensure safety and reliability. Yet, much of this rigour has traditionally focused on technical assurance, often overlooking the human and social contexts in which these systems operate. This paper argues that considering human-centric aspects is an essential dimension of dependability, and presents a human-centred RE process designed to integrate social responsibility into critical system development. Drawing from a literature review, we identified a set of guidelines for designing software for vulnerable communities and translated these into sixty-two functional and non-functional requirements. These requirements were operationalised through the design of an adaptive early warning system prototype, which was subsequently evaluated through six interviews and eight cognitive walkthroughs to validate their relevance and applicability. The findings demonstrate that human-centric requirements, when addressed early, enhance the usability and accessibility of systems for all users. The paper concludes by positioning human-centricity not as an ethical add-on but as a defining quality of safe and equitable critical systems.

</details>


### [98] [Agent READMEs: An Empirical Study of Context Files for Agentic Coding](https://arxiv.org/abs/2511.12884)
*Worawalan Chatlatanagulchai,Hao Li,Yutaro Kashiwa,Brittany Reid,Kundjanasith Thonglek,Pattara Leelaprute,Arnon Rungsawang,Bundit Manaskasemsak,Bram Adams,Ahmed E. Hassan,Hajimu Iida*

Main category: cs.SE

TL;DR: 本研究对2303个代理上下文文件进行了大规模实证分析，发现这些文件复杂且动态维护，主要包含功能性信息，缺少安全和性能要求。


<details>
  <summary>Details</summary>
Motivation: 代理编码工具通过上下文文件引导自动代码生成，但这些文件的结构和内容尚未被系统研究，尤其在维护和非功能性需求方面存在知识空白。

Method: 对2303个代理上下文文件从结构、维护方式和内容进行定量和定性分析，特别分析16种指令类型的分布情况。

Result: 上下文文件像配置代码一样频繁小幅更新，主要包含构建、运行命令、实现细节和架构信息，然而安全性和性能等非功能性需求较少涉及。

Conclusion: 当前代理上下文文件虽然使自动编码功能可用，但缺乏对代码安全性和性能的保障，未来需要改进相关工具和实践以弥补这一不足。

Abstract: Agentic coding tools receive goals written in natural language as input, break them down into specific tasks, and write or execute the actual code with minimal human intervention. Central to this process are agent context files ("READMEs for agents") that provide persistent, project-level instructions. In this paper, we conduct the first large-scale empirical study of 2,303 agent context files from 1,925 repositories to characterize their structure, maintenance, and content. We find that these files are not static documentation but complex, difficult-to-read artifacts that evolve like configuration code, maintained through frequent, small additions. Our content analysis of 16 instruction types shows that developers prioritize functional context, such as build and run commands (62.3%), implementation details (69.9%), and architecture (67.7%). We also identify a significant gap: non-functional requirements like security (14.5%) and performance (14.5%) are rarely specified. These findings indicate that while developers use context files to make agents functional, they provide few guardrails to ensure that agent-written code is secure or performant, highlighting the need for improved tooling and practices.

</details>


### [99] [Diffploit: Facilitating Cross-Version Exploit Migration for Open Source Library Vulnerabilities](https://arxiv.org/abs/2511.12950)
*Zirui Chen,Zhipeng Xue,Jiayuan Zhou,Xing Hu,Xin Xia,Xiaohu Yang*

Main category: cs.SE

TL;DR: 提出Diffploit，一种基于差异驱动和迭代反馈的漏洞利用迁移方法，有效解决跨版本环境和触发条件变化导致的利用失败。


<details>
  <summary>Details</summary>
Motivation: 现有漏洞利用迁移技术主要依赖代码级追踪对齐，难以处理环境层面失败和复杂的触发条件变化，手动适配耗时且不准确。

Method: Diffploit包括上下文模块动态分析版本差异构建上下文，指导利用迁移模块通过大语言模型(LLM)在迭代反馈中平衡差异候选探索与精炼，实现利用代码的自动迁移。

Result: 在包含102个Java漏洞和689个版本迁移任务的大规模数据集上，Diffploit成功迁移84.2%的利用代码，显著优于现有工具TARGET和IDEA。

Conclusion: Diffploit有效提升漏洞利用迁移成功率，发现并纠正了安全报告中的错误版本范围及未报告的漏洞版本，展示了良好的技术和应用价值。

Abstract: Exploits are commonly used to demonstrate the presence of library vulnerabilities and validate their impact across different versions. However, their direct application to alternative versions often fails due to breaking changes introduced during evolution. These failures stem from both changes in triggering conditions (e.g., API refactorings) and broken dynamic environments (e.g., build or runtime errors), which are challenging to interpret and adapt manually. Existing techniques primarily focus on code-level trace alignment through fuzzing, which is both time-consuming and insufficient for handling environment-level failures. Moreover, they often fall short when dealing with complicated triggering condition changes across versions. To overcome this, we propose Diffploit, an iterative, diff-driven exploit migration method structured around two key modules: the Context Module and the Migration Module. The Context Module dynamically constructs contexts derived from analyzing behavioral discrepancies between the target and reference versions, which capture the failure symptom and its related diff hunks. Leveraging these contexts, the Migration Module guides an LLM-based adaptation through an iterative feedback loop, balancing exploration of diff candidates and gradual refinement to resolve reproduction failures effectively. We evaluate Diffploit on a large-scale dataset containing 102 Java CVEs and 689 version-migration tasks across 79 libraries. Diffploit successfully migrates 84.2% exploits, outperforming the change-aware test repair tool TARGET by 52.0% and the rule-based tool in IDEA by 61.6%. Beyond technical effectiveness, Diffploit identifies 5 CVE reports with incorrect affected version ranges, three of which have been confirmed. It also discovers 111 unreported vulnerable versions in the GitHub Advisory Database.

</details>


### [100] [SmartPoC: Generating Executable and Validated PoCs for Smart Contract Bug Reports](https://arxiv.org/abs/2511.12993)
*Longfei Chen,Ruibin Yan,Taiyu Wong,Yiyang Chen,Chao Zhang*

Main category: cs.SE

TL;DR: 本文提出SmartPoC框架，自动将智能合约审计报告转换为可执行且验证过的测试用例，提升漏洞验证效率。


<details>
  <summary>Details</summary>
Motivation: 智能合约存在漏洞且审计报告质量不一，缺少可复现的PoC测试导致验证成本高昂且依赖人工，需自动化解决方案。

Method: SmartPoC先提取报告中相关函数并净化噪声，利用大语言模型生成测试用例，通过预/后处理修复代码，并使用差分验证作为运行时判定逻辑确保漏洞可利用性。

Result: 在SmartBugs-Vul和FORGE-Vul基准测试中，SmartPoC能分别为85.61%和86.45%的目标生成可执行且验证的测试用例；在Etherscan数据集上以极低成本确认236个真实漏洞。

Conclusion: SmartPoC有效自动将审计文本转为高质量PoC测试，显著降低了智能合约漏洞验证的人工成本和难度。

Abstract: Smart contracts are prone to vulnerabilities and are analyzed by experts as well as automated systems, such as static analysis and AI-assisted solutions. However, audit artifacts are heterogeneous and often lack reproducible, executable PoC tests suitable for automated validation, leading to costly, ad hoc manual verification. Large language models (LLMs) can be leveraged to turn audit reports into PoC test cases, but have three major challenges: noisy inputs, hallucinations, and missing runtime oracles. In this paper, we present SmartPoC, an automated framework that converts textual audit reports into executable, validated test cases. First, the input audit report is processed to reduce noise, and only bug-related functions are extracted and fed to LLMs as context. To curb hallucinations and ensure compile-and-run readiness, we leverage LLMs to synthesize PoC test cases with specially-designed pre-/post-execution repair. We further utilize differential verification as oracles to confirm exploitability of the PoC test cases. On the SmartBugs-Vul and FORGE-Vul benchmarks, SmartPoC generates executable, validated Foundry test cases for 85.61% and 86.45% of targets, respectively. Applied to the latest Etherscan verified-source corpus, SmartPoC confirms 236 real bugs out of 545 audit findings at a cost of only $0.03 per finding.

</details>


### [101] [Towards Requirements Engineering for GenAI-Enabled Software: Bridging Responsibility Gaps through Human Oversight Requirements](https://arxiv.org/abs/2511.13069)
*Zhenyu Mao,Jacky Keung,Yicheng Sun,Yifei Wang,Shuo Liu,Jialong Li*

Main category: cs.SE

TL;DR: 本文针对生成式人工智能（GenAI）软件中的责任空白问题，提出了一种从人类监督需求角度系统分析责任空白的方法学，显著提升了需求工程中责任追踪与监督需求的明确性。


<details>
  <summary>Details</summary>
Motivation: 随着GenAI软件的生成性和适应性增强，责任界定变得更加复杂和模糊，现有需求工程方法难以有效应对责任空白问题，亟需系统的分析框架和方法。

Method: 本文设计了三层分析框架：概念层界定责任关键元素及其交互；方法层通过演绎推理识别责任空白并推导监督需求；工件层用演绎骨干表形式化表达推理过程，便于跟踪和复用。

Result: 通过用户研究比较，方法在六个维度上均优于基于目标的传统需求工程方法，证明了该方法在识别责任空白及推导监督需求方面的有效性。

Conclusion: 本文方法系统填补了GenAI软件需求工程中的责任空白研究空白，提高了责任追踪的清晰性和监督需求的制定，有助于增强GenAI系统的可控性和责任明确性。

Abstract: Context: Responsibility gaps, long-recognized challenges in socio-technical systems where accountability becomes diffuse or ambiguous, have become increasingly pronounced in GenAI-enabled software. The generative and adaptive nature complicates how human oversight and responsibility are specified, delegated, and traced. Existing requirements engineering (RE) approaches remain limited in addressing these phenomena, revealing conceptual, methodological, and artifact-level research gaps.. Objective: This study aims to analyze these research gaps in the context of GenAI-enabled software systems. It seeks to establish a coherent perspective for a systematic analysis of responsibility gaps from a human oversight requirements standpoint, encompassing how these responsibility gaps should be conceptualized, identified, and represented throughout the RE process. Methods: The proposed design methodology is structured across three analytical layers. At the conceptualization layer, it establishes a conceptual framing that defines the key elements of responsibility across the human and system dimensions and explains how potential responsibility gaps emerge from their interactions. At the methodological layer, it introduces a deductive pipeline for identifying responsibility gaps by analyzing interactions between these dimensions and deriving corresponding oversight requirements within established RE frameworks. At the artifact layer, it formalizes the results in a Deductive Backbone Table, a reusable representation that traces the reasoning path from responsibility gaps identification to human oversight requirements derivation. Results: A user study compared the proposed methodology with a baseline goal-oriented RE across two scenarios. Evaluation across six dimensions indicated clear improvements of the proposed methodology, confirming its effectiveness in addressing three research gaps.

</details>


### [102] [Examining the Usage of Generative AI Models in Student Learning Activities for Software Programming](https://arxiv.org/abs/2511.13271)
*Rufeng Chen,Shuaishuai Jiang,Jiyun Shen,AJung Moon,Lili Wei*

Main category: cs.SE

TL;DR: 研究比较了生成式AI如ChatGPT与传统在线资源在编程学习中对不同水平学生知识增长的支持效果。


<details>
  <summary>Details</summary>
Motivation: 虽然已有研究关注生成式AI完成教学任务的能力及其对学生表现的影响，但对其促进知识增长的效果关注不足。

Method: 通过一项包含24名初学者和中级编程学生的控制实验，分析学生使用ChatGPT解决编程任务时的表现、概念理解及互动行为。

Result: 生成完整解决方案显著提升任务表现，尤其对初学者效果明显，但未必促进知识增长。初学者倾向依赖生成式AI完成任务，缺乏知识获取，中级学生则更具选择性。过度依赖或使用不足均导致知识增长较弱。

Conclusion: 建议学生和教育者将生成式AI作为学习辅助工具而非单纯解决问题工具，强调整合生成式AI时需提供指导以促进更深层次理解。

Abstract: The rise of Generative AI (GenAI) tools like ChatGPT has created new opportunities and challenges for computing education. Existing research has primarily focused on GenAI's ability to complete educational tasks and its impact on student performance, often overlooking its effects on knowledge gains. In this study, we investigate how GenAI assistance compares to conventional online resources in supporting knowledge gains across different proficiency levels. We conducted a controlled user experiment with 24 undergraduate students of two different levels of programming experience (beginner, intermediate) to examine how students interact with ChatGPT while solving programming tasks. We analyzed task performance, conceptual understanding, and interaction behaviors. Our findings reveal that generating complete solutions with GenAI significantly improves task performance, especially for beginners, but does not consistently result in knowledge gains. Importantly, usage strategies differ by experience: beginners tend to rely heavily on GenAI toward task completion often without knowledge gain in the process, while intermediates adopt more selective approaches. We find that both over-reliance and minimal use result in weaker knowledge gains overall. Based on our results, we call on students and educators to adopt GenAI as a learning rather than a problem solving tool. Our study highlights the urgent need for guidance when integrating GenAI into programming education to foster deeper understanding.

</details>


### [103] [SAINT: Service-level Integration Test Generation with Program Analysis and LLM-based Agents](https://arxiv.org/abs/2511.13305)
*Rangeet Pan,Raju Pavuluri,Ruikai Huang,Rahul Krishna,Tyler Stennett,Alessandro Orso,Saurabh SInha*

Main category: cs.SE

TL;DR: SAINT是一种结合静态分析和大语言模型的白盒测试方法，自动生成企业Java应用的服务级测试，显著提升测试覆盖率和故障检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有企业Java服务级测试工具多依赖模糊测试和不可用的OpenAPI规范，难以生成有效的功能测试和场景测试。

Method: SAINT利用静态分析构建端点模型和操作依赖图，再通过基于LLM的代理生成覆盖代码和数据库交互的端点测试及基于场景的功能测试。

Result: 在八个Java应用上测试，SAINT在测试覆盖率、故障检测和场景生成方面表现优越，开发者对生成的场景测试高度认可。

Conclusion: 结合静态分析与代理式LLM流程能自动产生更有效且符合开发需求的服务级测试，推动企业应用测试水平提升。

Abstract: Enterprise applications are typically tested at multiple levels, with service-level testing playing an important role in validating application functionality. Existing service-level testing tools, especially for RESTful APIs, often employ fuzzing and/or depend on OpenAPI specifications which are not readily available in real-world enterprise codebases. Moreover, these tools are limited in their ability to generate functional tests that effectively exercise meaningful scenarios. In this work, we present SAINT, a novel white-box testing approach for service-level testing of enterprise Java applications. SAINT combines static analysis, large language models (LLMs), and LLM-based agents to automatically generate endpoint and scenario-based tests. The approach builds two key models: an endpoint model, capturing syntactic and semantic information about service endpoints, and an operation dependency graph, capturing inter-endpoint ordering constraints. SAINT then employs LLM-based agents to generate tests. Endpoint-focused tests aim to maximize code and database interaction coverage. Scenario-based tests are synthesized by extracting application use cases from code and refining them into executable tests via planning, action, and reflection phases of the agentic loop. We evaluated SAINT on eight Java applications, including a proprietary enterprise application. Our results illustrate the effectiveness of SAINT in coverage, fault detection, and scenario generation. Moreover, a developer survey provides strong endorsement of the scenario-based tests generated by SAINT. Overall, our work shows that combining static analysis with agentic LLM workflows enables more effective, functional, and developer-aligned service-level test generation.

</details>


### [104] [LinkXplore: A Framework for Affordable High-Quality Blockchain Data](https://arxiv.org/abs/2511.13318)
*Peihao Li*

Main category: cs.SE

TL;DR: LinkXplore是一种开放框架，低成本收集和管理区块链链上数据。


<details>
  <summary>Details</summary>
Motivation: 大规模区块链数据收集成本高昂，现有RPC提供者API价格过高，限制了学术和工业应用的发展，且缺少灵活集成分析模块的系统框架。

Method: LinkXplore通过直接分析RPC查询或流的原始数据，绕过昂贵的数据提供商，使用简单API和后端处理逻辑，实现多种链上数据的集成。

Result: LinkXplore成功提供了高质量、低成本的链上数据服务，适合预算有限的研究人员和开发者使用。项目代码和数据集公开在GitHub。

Conclusion: LinkXplore为区块链数据采集和管理提供了成本效益高且灵活的解决方案，推动学术研究和产品开发。

Abstract: Blockchain technologies are rapidly transforming both academia and industry. However, large-scale blockchain data collection remains prohibitively expensive, as many RPC providers only offer enhanced APIs with high pricing tiers that are unsuitable for budget-constrained research or industrial-scale applications, which has significantly slowed down academic studies and product development. Moreover, there is a clear lack of a systematic framework that allows flexible integration of new modules for analyzing on-chain data.
  To address these challenges, we introduce LinkXplore, the first open framework for collecting and managing on-chain data. LinkXplore enables users to bypass costly blockchain data providers by directly analyzing raw data from RPC queries or streams, thereby offering high-quality blockchain data at a fraction of the cost. Through a simple API and backend processing logic, any type of chain data can be integrated into the framework. This makes it a practical alternative for both researchers and developers with limited budgets. Code and dataset used in this project are publicly available at https://github.com/Linkis-Project/LinkXplore

</details>


### [105] [An LLM-based Quantitative Framework for Evaluating High-Stealthy Backdoor Risks in OSS Supply Chains](https://arxiv.org/abs/2511.13341)
*Zihe Yan,Kai Luo,Haoyu Yang,Yang Yu,Zhuosheng Zhang,Guancheng Li*

Main category: cs.SE

TL;DR: 本文提出了一种细粒度项目评估框架，用于评估开源软件中的隐蔽后门风险，结合攻击者视角和大语言模型语义分析，提高对软件供应链安全风险的识别能力。


<details>
  <summary>Details</summary>
Motivation: 随着开源软件依赖的增加，维护不足和社区审计不足导致源代码安全和仓库维护者合法性难以保障，尤其面临隐蔽后门攻击的挑战。

Method: 构建基于攻击者视角的隐蔽后门攻击模型，设计针对每阶段的定量指标，结合大语言模型对代码库语义进行评估，弥补静态分析的不足。

Result: 在Debian生态中对66个高优先级软件包的评估表明，当前开源软件供应链存在多种安全风险。

Conclusion: 该框架有效识别高隐蔽性后门风险，体现了用大语言模型辅助评估开源软件供应链安全的潜力，促使社区加强维护和审计。

Abstract: In modern software development workflows, the open-source software supply chain contributes significantly to efficient and convenient engineering practices. With increasing system complexity, using open-source software as third-party dependencies has become a common practice. However, the lack of maintenance for underlying dependencies and insufficient community auditing create challenges in ensuring source code security and the legitimacy of repository maintainers, especially under high-stealthy backdoor attacks exemplified by the XZ-Util incident. To address these problems, we propose a fine-grained project evaluation framework for backdoor risk assessment in open-source software. The framework models stealthy backdoor attacks from the viewpoint of the attacker and defines targeted metrics for each attack stage. In addition, to overcome the limitations of static analysis in assessing the reliability of repository maintenance activities such as irregular committer privilege escalation and limited participation in reviews, the framework uses large language models (LLMs) to conduct semantic evaluation of code repositories without relying on manually crafted patterns. The framework is evaluated on sixty six high-priority packages in the Debian ecosystem. The experimental results indicate that the current open-source software supply chain is exposed to various security risks.

</details>


### [106] [FLOWER: Flow-Oriented Entity-Relationship Tool](https://arxiv.org/abs/2511.13357)
*Dmitry Moskalev*

Main category: cs.SE

TL;DR: 提出了一种名为FLOWER的实体关系模型构建工具，通过动态采样和数据分析自动生成并优化实体关系模型，支持多种SQL方言和23种语言，显著提高了约束学习和分布表示的效率与准确率。


<details>
  <summary>Details</summary>
Motivation: 数据库中存储大量合成和有机数据，正确处理大量对象的实体关系模型构建依赖人工且资源密集，存在效率和准确性问题。

Method: 开发了FLOWER工具，利用动态采样和鲁棒数据分析技术，自动检测内置约束并生成必要的实体关系，支持实时处理显式和隐式依赖关系，通过SQL和自然语言提高数据理解和讲述能力。

Result: 实验表明FLOWER在STATS基准测试中分布表示效率提升2.4倍，约束学习效率提升2.6倍，整体加速2.15倍，数据讲述准确率提升1.19倍，语境减少1.86倍，兼容CPU/GPU和23种语言。

Conclusion: FLOWER工具显著优化了实体关系模型的构建效率和准确性，具有良好的扩展性和多语言支持，能够有效处理实际数据，提升数据建模质量和适用性。

Abstract: Exploring relationships across data sources is a crucial optimization for entities recognition. Since databases can store big amount of information with synthetic and organic data, serving all quantity of objects correctly is an important task to deal with. However, the decision of how to construct entity relationship model is associated with human factor. In this paper, we present flow-oriented entity-relationship tool. This is first and unique end-to-end solution that eliminates routine and resource-intensive problems of processing, creating and visualizing both of explicit and implicit dependencies for prominent SQL dialects on-the-fly. Once launched, FLOWER automatically detects built-in constraints and starting to create own correct and necessary one using dynamic sampling and robust data analysis techniques. This approach applies to improve entity-relationship model and data storytelling to better understand the foundation of data and get unseen insights from DB sources using SQL or natural language. Evaluated on state-of-the-art STATS benchmark, experiments show that FLOWER is superior to reservoir sampling by 2.4x for distribution representation and 2.6x for constraint learning with 2.15x acceleration. For data storytelling, our tool archives 1.19x for accuracy enhance with 1.86x context decrease compare to LLM. Presented tool is also support 23 languages and compatible with both of CPU and GPU. Those results show that FLOWER can manage with real-world data a way better to ensure with quality, scalability and applicability for different use-cases.

</details>


### [107] [BIOMERO 2.0: end-to-end FAIR infrastructure for bioimaging data import, analysis, and provenance](https://arxiv.org/abs/2511.13611)
*Torec T. Luik,Joost de Folter,Rodrigo Rosas-Bertolini,Eric A. J. Reits,Ron A. Hoebe,Przemek M. Krawczyk*

Main category: cs.SE

TL;DR: BIOMERO 2.0升级了BIOMERO框架，使OMERO平台符合FAIR原则，实现了数据导入、预处理、分析和流程监控的集成，保障生物影像数据全流程的可追溯和可重用。


<details>
  <summary>Details</summary>
Motivation: 提升OMERO平台的数据FAIR性和可追溯性，解决生物影像数据从采集到分析的全流程管理和可重复性难题。

Method: 通过OMERO.web插件和容器化组件实现数据导入预处理与元数据丰富，利用BIOMERO Python库协调高性能计算系统上的分析作业，并在集成的仪表盘中实时记录导入和分析的参数、版本及结果。

Result: 实现了生物影像数据的原位导入、预处理、分析和监控全流程的集成管理，确保了数据处理的实时溯源和FAIR合规性，增强了OMERO平台的功能。

Conclusion: BIOMERO 2.0通过进口器和分析器的双重功能，将OMERO打造成一个支持可追溯、可重用的生物影像数据工作流平台，有效连接数据导入、分析和共享，推进了影像分析的FAIR化。

Abstract: We present BIOMERO 2.0, a major evolution of the BIOMERO framework that transforms OMERO into a FAIR-compliant (findable, accessible, interoperable, and reusable), provenance-aware bioimaging platform. BIOMERO 2.0 integrates data import, preprocessing, analysis, and workflow monitoring through an OMERO.web plugin and containerized components. The importer subsystem facilitates in-place import using containerized preprocessing and metadata enrichment via forms, while the analyzer subsystem coordinates and tracks containerized analyses on high-performance computing systems via the BIOMERO Python library. All imports and analyses are recorded with parameters, versions, and results, ensuring real-time provenance accessible through integrated dashboards. This dual approach places OMERO at the heart of the bioimaging analysis process: the importer ensures provenance from image acquisition through preprocessing and import into OMERO, while the analyzer records it for downstream processing. These integrated layers enhance OMEROs FAIRification, supporting traceable, reusable workflows for image analysis that bridge the gap between data import, analysis, and sharing.

</details>


### [108] [Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?](https://arxiv.org/abs/2511.13646)
*Chunqiu Steven Xia,Zhe Wang,Yan Yang,Yuxiang Wei,Lingming Zhang*

Main category: cs.SE

TL;DR: 本文提出了Live-SWE-agent，一种能够在运行时自我进化的软件代理，显著提升了软件工程任务的解决效率。


<details>
  <summary>Details</summary>
Motivation: 现有的软件代理设计复杂且昂贵，且自我改进的代理需要大量离线训练且泛化能力有限，因此需要一种能够动态自我演化的解决方案。

Method: Live-SWE-agent从最基础的软件代理框架出发，仅使用bash工具，能在解决实际问题时自动演化自身结构。

Result: 在SWE-bench Verified基准测试中，Live-SWE-agent达到75.4%的解决率，超越所有开源代理并接近最优专有方案；在SWE-Bench Pro基准上以45.8%的解决率领先。

Conclusion: Live-SWE-agent证明了运行时自我进化软件代理的有效性，为软件自动化领域带来了显著进步。

Abstract: Large Language Models (LLMs) are reshaping almost all industries, including software engineering. In recent years, a number of LLM agents have been proposed to solve real-world software problems. Such software agents are typically equipped with a suite of coding tools and can autonomously decide the next actions to form complete trajectories to solve end-to-end software tasks. While promising, they typically require dedicated design and may still be suboptimal, since it can be extremely challenging and costly to exhaust the entire agent scaffold design space. Recognizing that software agents are inherently software themselves that can be further refined/modified, researchers have proposed a number of self-improving software agents recently, including the Darwin-Gödel Machine (DGM). Meanwhile, such self-improving agents require costly offline training on specific benchmarks and may not generalize well across different LLMs or benchmarks. In this paper, we propose Live-SWE-agent, the first live software agent that can autonomously and continuously evolve itself on-the-fly during runtime when solving real-world software problems. More specifically, Live-SWE-agent starts with the most basic agent scaffold with only access to bash tools (e.g., mini-SWE-agent), and autonomously evolves its own scaffold implementation while solving real-world software problems. Our evaluation on the widely studied SWE-bench Verified benchmark shows that Live-SWE-agent can achieve an impressive solve rate of 75.4% without test-time scaling, outperforming all existing open-source software agents and approaching the performance of the best proprietary solution. Moreover, Live-SWE-agent outperforms state-of-the-art manually crafted software agents on the recent SWE-Bench Pro benchmark, achieving the best-known solve rate of 45.8%.

</details>


### [109] [What's in a Software Engineering Job Posting?](https://arxiv.org/abs/2511.13656)
*Marvin Wyrich,Lloyd Montgomery*

Main category: cs.SE

TL;DR: 该论文通过分析100条软件工程职位招聘信息，发现雇主不仅重视技术能力，更关注求职者的文化契合度、职业成长和人际交往能力。


<details>
  <summary>Details</summary>
Motivation: 随着软件工程岗位需求的演变，雇主对候选人提出了更多非技术性要求，需要深入了解这些期望以指导招聘和职业发展。

Method: 采用主题分析法对2023年100条软件工程职位招聘信息进行定性分析，挖掘非技术性要求的类别和内容。

Result: 结果显示，雇主期望候选人认同企业目标、适应企业文化、注重个人和职业成长，并具备良好的人际交往能力。

Conclusion: 研究丰富了软件工程领域关于工程师角色和工作环境的理解，强调除了技术能力外，非技术性素质对职业成功同样重要，对研究者、教育者和招聘人员具有指导意义。

Abstract: A well-rounded software engineer is often defined by technical prowess and the ability to deliver on complex projects. However, the narrative around the ideal Software Engineering (SE) candidate is evolving, suggesting that there is more to the story. This article explores the non-technical aspects emphasized in SE job postings, revealing the sociotechnical and organizational expectations of employers. Our Thematic Analysis of 100 job postings shows that employers seek candidates who align with their sense of purpose, fit within company culture, pursue personal and career growth, and excel in interpersonal interactions. This study contributes to ongoing discussions in the SE community about the evolving role and workplace context of software engineers beyond technical skills. By highlighting these expectations, we provide relevant insights for researchers, educators, practitioners, and recruiters. Additionally, our analysis offers a valuable snapshot of SE job postings in 2023, providing a scientific record of prevailing trends and expectations.

</details>


### [110] [Ontology-Driven Model-to-Model Transformation of Workflow Specifications](https://arxiv.org/abs/2511.13661)
*Francisco Abreu,Luís Cruz,Sérgio Guerreiro*

Main category: cs.SE

TL;DR: 本文提出了一种基于本体的模型到模型转换流水线，将专有的工作流定义系统性地转换为标准的BPMN 2.0模型，解决了厂商锁定和互操作性问题。


<details>
  <summary>Details</summary>
Motivation: 专有工作流建模语言限制了流程知识的互操作性和重用，导致厂商锁定，本文旨在通过引入开放标准转换机制，缓解这种依赖，促进标准迁移。

Method: 构建了一个三阶段转换流水线：基于RML的JSON到RDF/OWL的语义提升、本体对齐推理、通过Camunda Model API生成BPMN图，并通过本体和声明式规则外部化映射知识以提高重用性和语义可追溯性。

Result: 在对69个真实工作流的评估中，转换流水线成功生成了92个BPMN图，成功率达94.2%；失败原因主要是动态行为和基于时间的状态转换在静态JSON中未明确体现。支持和开发团队反馈该方法提升了流程理解、诊断和入门培训效率。

Conclusion: 基于本体的模型转模型转换方法有效桥接了专有工作流与标准表示，支持互操作与厂商独立，具备良好的性能和使用价值，方法可推广至其他专有语言，促进长期维护与持续集成。

Abstract: Proprietary workflow modeling languages such as Smart Forms & Smart Flow hamper interoperability and reuse because they lock process knowledge into closed formats. To address this vendor lock-in and ease migration to open standards, we introduce an ontology-driven model-to-model pipeline that systematically translates domain-specific workflow definitions to Business Process Model and Notation (BPMN) 2.0. The pipeline comprises three phases: RML-based semantic lifting of JSON to RDF/OWL, ontology alignment and reasoning, and BPMN generation via the Camunda Model API. By externalizing mapping knowledge into ontologies and declarative rules rather than code, the approach supports reusability across vendor-specific formats and preserves semantic traceability between source definitions and target BPMN models. We instantiated the pipeline for Instituto Superior Técnico (IST)'s Smart Forms & Smart Flow and implemented a converter that produces standard-compliant BPMN diagrams. Evaluation on a corpus of 69 real-world workflows produced 92 BPMN diagrams with a 94.2% success rate. Failures (5.81%) stemmed from dynamic behaviors and time-based transitions not explicit in the static JSON. Interviews with support and development teams indicated that the resulting diagrams provide a top-down view that improves comprehension, diagnosis and onboarding by exposing implicit control flow and linking tasks and forms back to their sources. The pipeline is generalizable to other proprietary workflow languages by adapting the ontology and mappings, enabling interoperability and reducing vendor dependency while supporting continuous integration and long-term maintainability. The presented case study demonstrates that ontology-driven M2M transformation can systematically bridge domain-specific workflows and standard notations, offering quantifiable performance and qualitative benefits for stakeholders.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [111] [MALBO: Optimizing LLM-Based Multi-Agent Teams via Multi-Objective Bayesian Optimization](https://arxiv.org/abs/2511.11788)
*Antonio Sabbatella*

Main category: cs.MA

TL;DR: 本文提出MALBO框架，通过多目标贝叶斯优化，自动组合大语言模型（LLMs）团队，兼顾任务准确率和推理成本，显著降低成本同时保持性能。


<details>
  <summary>Details</summary>
Motivation: LLMs在多智能体系统中的最优分配面临搜索空间大、评估代价高、性能与成本权衡的问题，现有方法多针对单智能体，缺乏多智能体多目标优化框架。

Method: 将分配问题形式化为多目标优化，采用多目标贝叶斯优化及独立高斯过程代理模型，在连续特征空间内以期望超体积改进为导向进行高效搜索。

Result: 贝叶斯优化阶段相比随机搜索成本降低超45%，MALBO找到的异构专用团队成本降低最高达65.8%，同时保持最大性能。

Conclusion: MALBO提供了一个数据驱动且自动化的方法，能够有效部署成本效益高、专业化的多智能体AI系统。

Abstract: The optimal assignment of Large Language Models (LLMs) to specialized roles in multi-agent systems is a significant challenge, defined by a vast combinatorial search space, expensive black-box evaluations, and an inherent trade-off between performance and cost. Current optimization methods focus on single-agent settings and lack a principled framework for this multi-agent, multi-objective problem.
  This thesis introduces MALBO (Multi-Agent LLM Bayesian Optimization), a systematic framework designed to automate the efficient composition of LLM-based agent teams. We formalize the assignment challenge as a multi-objective optimization problem, aiming to identify the Pareto front of configurations between task accuracy and inference cost. The methodology employs multi-objective Bayesian Optimization (MOBO) with independent Gaussian Process surrogate models. By searching over a continuous feature-space representation of the LLMs, this approach performs a sample-efficient exploration guided by the expected hypervolume improvement.
  The primary contribution is a principled and automated methodology that yields a Pareto front of optimal team configurations. Our results demonstrate that the Bayesian optimization phase, compared to an initial random search, maintained a comparable average performance while reducing the average configuration cost by over 45%. Furthermore, MALBO identified specialized, heterogeneous teams that achieve cost reductions of up to 65.8% compared to homogeneous baselines, all while maintaining maximum performance. The framework thus provides a data-driven tool for deploying cost-effective and highly specialized multi-agent AI systems.

</details>


### [112] [From Single to Societal: Analyzing Persona-Induced Bias in Multi-Agent Interactions](https://arxiv.org/abs/2511.11789)
*Jiayi Li,Xiao Liu,Yansong Feng*

Main category: cs.MA

TL;DR: 本文系统研究了基于大语言模型的多智能体系统中人格设定引发的偏见，发现不同人格会影响智能体的信任度和坚持度，并存在明显的群体内偏好现象。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统通过赋予个体不同人格以促进行为多样性，但这是否会引入偏见尚未充分探讨，因此需要系统研究人格设定对多智能体交互偏见的影响。

Method: 通过控制实验设计，在协作解决问题和劝说任务中，分析不同代表社会特征（如性别、种族）的人格对智能体信任度和坚持度的影响，同时考察群体内偏好现象，涵盖多种语言模型、群体规模和交互轮次。

Result: 发现智能体表现出信任度和坚持度上的偏见，历史上处于优势地位的人群人格（如男性、白人）被视为不那么可信且坚持度较低；智能体对与自身人格相同者表现出明显的群体内偏好。这些偏见在不同模型和设置中普遍存在。

Conclusion: 人格设定会引入显著的偏见，影响多智能体系统的公平性与可靠性，亟需引起重视并采取对策以缓解此类偏见。

Abstract: Large Language Model (LLM)-based multi-agent systems are increasingly used to simulate human interactions and solve collaborative tasks. A common practice is to assign agents with personas to encourage behavioral diversity. However, this raises a critical yet underexplored question: do personas introduce biases into multi-agent interactions? This paper presents a systematic investigation into persona-induced biases in multi-agent interactions, with a focus on social traits like trustworthiness (how an agent's opinion is received by others) and insistence (how strongly an agent advocates for its opinion). Through a series of controlled experiments in collaborative problem-solving and persuasion tasks, we reveal that (1) LLM-based agents exhibit biases in both trustworthiness and insistence, with personas from historically advantaged groups (e.g., men and White individuals) perceived as less trustworthy and demonstrating less insistence; and (2) agents exhibit significant in-group favoritism, showing a higher tendency to conform to others who share the same persona. These biases persist across various LLMs, group sizes, and numbers of interaction rounds, highlighting an urgent need for awareness and mitigation to ensure the fairness and reliability of multi-agent systems.

</details>


### [113] [Conflict-Free Flight Scheduling Using Strategic Demand Capacity Balancing for Urban Air Mobility Operations](https://arxiv.org/abs/2511.11854)
*Vahid Hemmati,Yonas Ayalew,Ahmad Mohammadi,Reza Ahmari,Parham Kebria,Abdollah Homaifar,Mehrdad Saif*

Main category: cs.MA

TL;DR: 本文提出了一种无冲突多智能体飞行调度方法，确保城市空中移动中受限空域的安全分隔，显著降低了延误。


<details>
  <summary>Details</summary>
Motivation: 随着城市空中移动（UAM）系统的发展，如何在受限空域内实现多智能体安全、高效的飞行调度成为关键挑战。

Method: 引入基于延迟起飞的成对冲突规避（PCA）方法，利用运动学原理保持安全距离；进一步将PCA扩展到多智能体场景，建立优化模型确定起飞时间以应对日益密集的交通流。

Result: 通过数值仿真和真实UAM案例验证，该方法显著减少总延误，同时确保无碰撞运行。

Conclusion: 提出的方法为新兴的城市空中移动系统提供了可扩展的飞行调度框架，有助于提高系统鲁棒性和效率。

Abstract: In this paper, we propose a conflict-free multi- agent flight scheduling that ensures robust separation in con- strained airspace for Urban Air Mobility (UAM) operations application. First, we introduce Pairwise Conflict Avoidance (PCA) based on delayed departures, leveraging kinematic principles to maintain safe distances. Next, we expand PCA to multi-agent scenarios, formulating an optimization approach that systematically determines departure times under increasing traffic densities. Performance metrics, such as average delay, assess the effectiveness of our solution. Through numerical simulations across diverse multi-agent environments and real- world UAM use cases, our method demonstrates a significant reduction in total delay while ensuring collision-free operations. This approach provides a scalable framework for emerging urban air mobility systems.

</details>


### [114] [Goal-Oriented Multi-Agent Reinforcement Learning for Decentralized Agent Teams](https://arxiv.org/abs/2511.11992)
*Hung Du,Hy Nguyen,Srikanth Thudumu,Rajesh Vasa,Kon Mouzakis*

Main category: cs.MA

TL;DR: 提出了一种去中心化的多智能体强化学习框架，实现基于局部目标和观察的选择性通信，提升了多车辆系统中的协调效果和任务成功率。


<details>
  <summary>Details</summary>
Motivation: 面对多领域交通工具在动态、不可预测环境中的有限通信和无中心化控制，传统方法难以实现有效协调。

Method: 设计了基于目标的选择性通信策略，智能体仅共享与局部目标相关的信息，以增强协作并符合可视限制。

Result: 在复杂导航任务中，所提方法显著提高了成功率和任务完成速度，并在智能体数量增多时保持性能稳定，显示出良好的扩展性。

Conclusion: 去中心化、目标驱动的多智能体强化学习框架有效支持了多车辆系统在现实环境中的协调，具备广泛应用潜力。

Abstract: Connected and autonomous vehicles across land, water, and air must often operate in dynamic, unpredictable environments with limited communication, no centralized control, and partial observability. These real-world constraints pose significant challenges for coordination, particularly when vehicles pursue individual objectives. To address this, we propose a decentralized Multi-Agent Reinforcement Learning (MARL) framework that enables vehicles, acting as agents, to communicate selectively based on local goals and observations. This goal-aware communication strategy allows agents to share only relevant information, enhancing collaboration while respecting visibility limitations. We validate our approach in complex multi-agent navigation tasks featuring obstacles and dynamic agent populations. Results show that our method significantly improves task success rates and reduces time-to-goal compared to non-cooperative baselines. Moreover, task performance remains stable as the number of agents increases, demonstrating scalability. These findings highlight the potential of decentralized, goal-driven MARL to support effective coordination in realistic multi-vehicle systems operating across diverse domains.

</details>


### [115] [FINRS: A Risk-Sensitive Trading Framework for Real Financial Markets](https://arxiv.org/abs/2511.12599)
*Bijia Liu,Ronghao Dang*

Main category: cs.MA

TL;DR: 本文提出了FinRS，一种结合分层市场分析、双决策代理和多时间尺度奖励反射的风险敏感交易框架，显著提升金融交易的盈利能力和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的交易代理主要关注单步预测，缺乏有效的风险管理机制，导致在市场波动时表现不佳。

Method: FinRS通过融合分层市场分析、双决策代理以及多时间尺度的奖励反射机制，实现交易行为与收益目标及下行风险约束的协调。

Result: 在多只股票和不同市场条件下，FinRS表现出比现有先进方法更优的盈利能力和稳定性。

Conclusion: FinRS有效整合风险管理机制，提升了基于LLM的金融交易系统在波动市场中的实用性和表现。

Abstract: Large language models (LLMs) have shown strong reasoning capabilities and are increasingly explored for financial trading. Existing LLM-based trading agents, however, largely focus on single-step prediction and lack integrated mechanisms for risk management, which reduces their effectiveness in volatile markets. We introduce FinRS, a risk-sensitive trading framework that combines hierarchical market analysis, dual-decision agents, and multi-timescale reward reflection to align trading actions with both return objectives and downside risk constraints. Experiments on multiple stocks and market conditions show that FinRS achieves superior profitability and stability compared to state-of-the-art methods.

</details>


### [116] [ENGRAM: Effective, Lightweight Memory Orchestration for Conversational Agents](https://arxiv.org/abs/2511.12960)
*Daivik Patel,Shrenik Patel*

Main category: cs.MA

TL;DR: 本文提出了轻量级记忆系统ENGRAM，通过三种记忆类型管理对话，实现长时记忆的一致性和效率提升。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型记忆系统架构复杂，难以工程实现和复现，迫切需要简洁高效的长时记忆管理方法。

Method: ENGRAM利用单一的路由器和检索器，将对话转换为情景、语义和程序三种记忆类型的记录，存储入数据库，并采用简单的集合操作融合多类型检索结果。

Result: 在多会话问答基准LoCoMo及LongMemEval测试中，ENGRAM表现优异，超过全上下文基线15分，但仅使用约1％的令牌量。

Conclusion: 通过记忆类型化和密集检索，ENGRAM实现了无需复杂架构的有效长时记忆管理，提升了大语言模型的长时一致性和效率。

Abstract: Large language models (LLMs) deployed in user-facing applications require long-horizon consistency: the ability to remember prior interactions, respect user preferences, and ground reasoning in past events. However, contemporary memory systems often adopt complex architectures such as knowledge graphs, multi-stage retrieval pipelines, and OS-style schedulers, which introduce engineering complexity and reproducibility challenges. We present ENGRAM, a lightweight memory system that organizes conversation into three canonical memory types (episodic, semantic, and procedural) through a single router and retriever. Each user turn is converted into typed memory records with normalized schemas and embeddings and stored in a database. At query time, the system retrieves top-k dense neighbors for each type, merges results with simple set operations, and provides the most relevant evidence as context to the model. ENGRAM attains state-of-the-art results on LoCoMo, a multi-session conversational QA benchmark for long-horizon memory, and exceeds the full-context baseline by 15 points on LongMemEval while using only about 1% of the tokens. These results show that careful memory typing and straightforward dense retrieval can enable effective long-term memory management in language models without requiring complex architectures.

</details>


### [117] [Reuse, Don't Recompute: Efficient Large Reasoning Model Inference via Memory Orchestration](https://arxiv.org/abs/2511.12987)
*Daivik Patel,Shrenik Patel*

Main category: cs.MA

TL;DR: ENGRAM-R通过集成类型化检索与紧凑事实卡表示，在推理时复用了记忆，显著减少了输入和推理所需的token数量，同时保持高准确率，提升了大规模推理模型的效率。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型通过增加推理链长度或采样多个解决方案来提高准确率，但这带来了大量的token代价和延迟。作者认为高效推理的核心在于记忆，即模型应通过复用已有结构化记忆而非重复计算，提高效率。

Method: 提出ENGRAM-R推理时记忆层，结合类型化检索、紧凑事实卡表示和显式引用控制，实现对已有证据的高效复用。

Result: 在LoCoMo基准上，ENGRAM-R相比完整上下文减少85%输入token和75%推理token，且准确率依然保持较高；在LongMemEval多跳任务片段上实现相似效率提升并取得显著准确率增长。

Conclusion: 记忆机制不仅对长期推理准确性关键，也是实现计算、内存和延迟限制下高效推理的实用手段。

Abstract: Large reasoning models (LRMs) achieve strong accuracy through test-time scaling, generating longer chains of thought or sampling multiple solutions, but at steep costs in tokens and latency. We argue that memory is a core ingredient for efficient reasoning: when evidence already exists, models should think less by reusing structured memory instead of recomputing derivations. We present ENGRAM-R, an inference-time memory layer that integrates typed retrieval with compact fact card representations and explicit citation control. On the LoCoMo benchmark, ENGRAM-R reduces input tokens by 85% and reasoning tokens by 75% compared to full context while maintaining high accuracy. On a multi-hop slice of the LongMemEval benchmark, it achieves similar efficiency with substantial accuracy gains. These results show that memory is not only critical for long-horizon correctness but also a practical lever for efficient reasoning under tight compute, memory, and latency budgets.

</details>


### [118] [LLM-based Multi-Agent System for Simulating Strategic and Goal-Oriented Data Marketplaces](https://arxiv.org/abs/2511.13233)
*Jun Sashihara,Yukihisa Fujita,Kota Nakamura,Masahiro Kuwahara,Teruaki Hayashi*

Main category: cs.MA

TL;DR: 本文提出了一种基于大语言模型的多智能体系统（LLM-MAS），用于模拟数据市场中的买卖双方行为，实现更真实的交易模式再现和市场趋势演变。


<details>
  <summary>Details</summary>
Motivation: 当前对数据市场参与者、数据及监管之间的交互缺乏系统性理解，现有模拟方法受限于预定义规则，难以捕捉真实市场动态。

Method: 设计基于大语言模型的买卖智能体，具备明确目标，能够自主进行计划、搜索、购买、定价及更新数据等操作，通过自然语言推理支持更广泛和自适应的行为选择。

Result: 模拟实验使用购买数量相关指标表明，LLM-MAS比传统模型更真实地再现了真实数据市场的交易模式，并成功捕捉市场趋势的产生与演变。

Conclusion: LLM-MAS框架有效提升了数据市场模拟的真实性和动态捕捉能力，为理解和设计数据市场机制提供了新工具。

Abstract: Data marketplaces, which mediate the purchase and exchange of data from third parties, have attracted growing attention for reducing the cost and effort of data collection while enabling the trading of diverse datasets. However, a systematic understanding of the interactions between market participants, data, and regulations remains limited. To address this gap, we propose a Large Language Model-based Multi-Agent System (LLM-MAS) for data marketplaces. In our framework, buyer and seller agents powered by LLMs operate with explicit objectives and autonomously perform strategic actions, such as planning, searching, purchasing, pricing, and updating data. These agents can reason about market dynamics, forecast future demand, and adjust strategies accordingly. Unlike conventional model-based simulations, which are typically constrained to predefined rules, LLM-MAS supports broader and more adaptive behavior selection through natural language reasoning. We evaluated the framework via simulation experiments using three distribution-based metrics: (1) the number of purchases per dataset, (2) the number of purchases per buyer, and (3) the number of repeated purchases of the same dataset. The results demonstrate that LLM-MAS more faithfully reproduces trading patterns observed in real data marketplaces compared to traditional approaches, and further captures the emergence and evolution of market trends.

</details>


### [119] [How Hard is it to Explain Preferences Using Few Boolean Attributes?](https://arxiv.org/abs/2511.13445)
*Clemens Anzinger,Jiehua Chen,Christian Hatschka,Manuel Sorge,Alexander Temper*

Main category: cs.MA

TL;DR: 本文研究了通过布尔属性模型（BAM）解释偏好数据的计算复杂性，揭示了属性数量对问题难度的二分法。


<details>
  <summary>Details</summary>
Motivation: 属性模型有助于理解偏好结构和提升决策效率，因而研究其计算复杂性具有理论和实际意义。

Method: 对BAM问题根据属性数k进行复杂性分类，同时考察了带有部分信息的变体和参数化复杂性，设计了部分特例的高效算法。

Result: 发现当k≤2时，问题线性可解；k≥3时，问题为NP完全。参数化绝对数m时为固定参数可解，两个投票者情况设计了线性算法。对于带部分信息模型，BAM WITH CARES较复杂，BAM WITH HAS较易处理但单投票者仍NP难。

Conclusion: 本研究系统揭示了BAM偏好解释问题的复杂性边界及不同变体的难易程度，为该领域算法设计提供了理论基础和方向。

Abstract: We study the computational complexity of explaining preference data through Boolean attribute models (BAMs), motivated by extensive research involving attribute models and their promise in understanding preference structure and enabling more efficient decision-making processes. In a BAM, each alternative has a subset of Boolean attributes, each voter cares about a subset of attributes, and voters prefer alternatives with more of their desired attributes. In the BAM problem, we are given a preference profile and a number k, and want to know whether there is a Boolean k-attribute model explaining the profile.
  We establish a complexity dichotomy for the number of attributes k: BAM is linear-time solvable for $k \le 2$ but NP-complete for $k \ge 3$. The problem remains hard even when preference orders have length two. On the positive side, BAM becomes fixed-parameter tractable when parameterized by the number of alternatives m. For the special case of two voters, we provide a linear-time algorithm.
  We also analyze variants where partial information is given: When voter preferences over attributes are known (BAM WITH CARES) or when alternative attributes are specified (BAM WITH HAS), we show that for most parameters BAM WITH CARES is more difficult whereas BAM WITH HAS is more tractable except for being NP-hard even for one voter.

</details>


### [120] [Asymptotic analysis of cooperative censoring policies in sensor networks](https://arxiv.org/abs/2511.13492)
*Jesus Fernandez-Bes,Rocío Arroyo-Valles,Jesús Cid-Sueiro*

Main category: cs.MA

TL;DR: 本文研究了电池供电的多跳传感器网络中合作式数据删除问题，通过联合马尔可夫决策过程建模，提出理论最优的数据删除策略，并设计中心化算法实现近似阈值策略，实验验证该策略能有效节约能量，优于非合作方案。


<details>
  <summary>Details</summary>
Motivation: 传感器节点产生的消息重要性不同，删除不重要的信息能节约能量延长网络寿命，但如何在多跳网络中协调合作删除策略尚不明晰。

Method: 构建整个网络动态的联合马尔可夫决策过程，求解最大化长期奖励的最优策略，考虑其复杂性提出基于有限常数阈值近似的策略，设计中心化算法计算阈值。

Result: 实验模拟结果表明，合作删除策略能显著提升能量效率，远优于非合作策略。

Conclusion: 合作式数据删除策略在多跳传感器网络中能有效节能，结合马尔可夫决策模型和阈值近似策略具备理论和实践价值。

Abstract: The problem of cooperative data censoring in battery-powered multihop sensor networks is analyzed in this paper. We are interested in scenarios where nodes generate messages (which are related to the sensor measurements) that can be graded with some importance value. Less important messages can be censored in order to save energy for later communications. The problem is modeled using a joint Markov Decision Process of the whole network dynamics, and a theoretically optimal censoring policy, which maximizes a long-term reward, is found. Though the optimal censoring rules are computationally prohibitive, our analysis suggests that, under some conditions, they can be approximated by a finite collection of constant-threshold rules. A centralized algorithm for the computation of these thresholds is proposed. The experimental simulations show that cooperative censoring policies are energy-efficient, and outperform other non-cooperative schemes.

</details>


### [121] [Market-Dependent Communication in Multi-Agent Alpha Generation](https://arxiv.org/abs/2511.13614)
*Jerick Shi,Burton Hollifield*

Main category: cs.MA

TL;DR: 多策略对冲基金中分析师是否应沟通及如何沟通影响交易策略效果，研究发现沟通提升绩效，但最佳沟通方式依市场波动性而定。


<details>
  <summary>Details</summary>
Motivation: 探讨多策略对冲基金中分析师之间沟通的必要性及沟通形式，及其对交易策略表现的影响。

Method: 利用基于大型语言模型的五代理交易系统，在21个月内进行450次实验，比较五种组织结构从孤立到协作及竞争性对话。

Result: 交流提升整体表现，但表现优劣依市场特性而变：竞争对话适合波动性大科技股，协作对话适合稳定普通股，金融股对任何沟通无效。各结构下策略趋同，交流质量与收益无关。

Conclusion: 最佳沟通设计需匹配市场波动特征，复杂对话并非一定带来更高收益，透明度不导致多样性损失。

Abstract: Multi-strategy hedge funds face a fundamental organizational choice: should analysts generating trading strategies communicate, and if so, how? We investigate this using 5-agent LLM-based trading systems across 450 experiments spanning 21 months, comparing five organizational structures from isolated baseline to collaborative and competitive conversation. We show that communication improves performance, but optimal communication design depends on market characteristics. Competitive conversation excels in volatile technology stocks, while collaborative conversation dominates stable general stocks. Finance stocks resist all communication interventions. Surprisingly, all structures, including isolated agents, converge to similar strategy alignments, challenging assumptions that transparency causes harmful diversity loss. Performance differences stem from behavioral mechanisms: competitive agents focus on stock-level allocation while collaborative agents develop technical frameworks. Conversation quality scores show zero correlation with returns. These findings demonstrate that optimal communication design must match market volatility characteristics, and sophisticated discussions don't guarantee better performance.

</details>
