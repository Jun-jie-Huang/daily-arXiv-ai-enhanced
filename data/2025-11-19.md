<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 47]
- [cs.SE](#cs.SE) [Total: 13]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Enhancing Agentic Autonomous Scientific Discovery with Vision-Language Model Capabilities](https://arxiv.org/abs/2511.14631)
*Kahaan Gandhi,Boris Bolliet,Inigo Zubeldia*

Main category: cs.CL

TL;DR: 本文提出了一种基于视觉-语言模型的多智能体系统，用于提升端到端的自主科学发现，能够实时纠正错误并实现无人工干预的数据分析。


<details>
  <summary>Details</summary>
Motivation: 当前科学数据分析中，智能体经常因错误推理而效率低下，缺乏有效的自我纠正与解释能力。

Method: 利用视觉-语言模型作为评判者，根据动态生成的领域特定标准评估绘图，指导多智能体系统实时调整和纠错。

Result: 在宇宙学和天体化学案例中展示了该方法的有效性，10项任务基准测试中，通过代码和视觉-语言模型增强的系统远超代码或代码+文本基线，达到0.7-0.8的得分。

Conclusion: 视觉-语言模型增强的多智能体系统能够提升科学发现的自主性和可解释性，具有较强的适应性和自我纠错能力。

Abstract: We show that multi-agent systems guided by vision-language models (VLMs) improve end-to-end autonomous scientific discovery. By treating plots as verifiable checkpoints, a VLM-as-a-judge evaluates figures against dynamically generated domain-specific rubrics, enabling agents to correct their own errors and steer exploratory data analysis in real-time. Case studies in cosmology and astrochemistry demonstrate recovery from faulty reasoning paths and adaptation to new datasets without human intervention. On a 10-task benchmark for data-driven discovery, VLM-augmented systems achieve pass at 1 scores of 0.7-0.8, compared to 0.2-0.3 for code-only and 0.4-0.5 for code-and-text baselines, while also providing auditable reasoning traces that improve interpretability. Code available here: https://github.com/CMBAgents/cmbagent

</details>


### [2] [Signature vs. Substance: Evaluating the Balance of Adversarial Resistance and Linguistic Quality in Watermarking Large Language Models](https://arxiv.org/abs/2511.13722)
*William Guo,Adaku Uchendu,Ana Smith*

Main category: cs.CL

TL;DR: 本文评估了多种文本水印技术在对抗性攻击下的鲁棒性，并分析了其对文本质量和写作风格的影响。


<details>
  <summary>Details</summary>
Motivation: 当前水印技术虽能准确检测大型语言模型生成的文本，但会降低文本质量且易被攻击去除，限制了其广泛应用。

Method: 比较了基于意译和回译（英语-另一种语言-英语）的攻击手段，采用语言学指标评估水印技术对文本质量和写作风格的影响。

Result: 水印技术能保持语义，但改变未加水印文本的写作风格，且对攻击敏感，尤其是回译攻击。

Conclusion: 现有水印技术虽有效检测LLM生成文本，但需改进以提高对抗攻击的鲁棒性及保持文本自然性。

Abstract: To mitigate the potential harms of Large Language Models (LLMs)generated text, researchers have proposed watermarking, a process of embedding detectable signals within text. With watermarking, we can always accurately detect LLM-generated texts. However, recent findings suggest that these techniques often negatively affect the quality of the generated texts, and adversarial attacks can strip the watermarking signals, causing the texts to possibly evade detection. These findings have created resistance in the wide adoption of watermarking by LLM creators. Finally, to encourage adoption, we evaluate the robustness of several watermarking techniques to adversarial attacks by comparing paraphrasing and back translation (i.e., English $\to$ another language $\to$ English) attacks; and their ability to preserve quality and writing style of the unwatermarked texts by using linguistic metrics to capture quality and writing style of texts. Our results suggest that these watermarking techniques preserve semantics, deviate from the writing style of the unwatermarked texts, and are susceptible to adversarial attacks, especially for the back translation attack.

</details>


### [3] [Refine Thought: A Test-Time Inference Method for Embedding Model Reasoning](https://arxiv.org/abs/2511.13726)
*Guangzhi Wang,Kai Li,Yinghao Jiao,Zhi Liu*

Main category: cs.CL

TL;DR: RT方法通过多次前向传播提升文本嵌入模型的语义推理能力，在多个基准测试中表现出显著提升。


<details>
  <summary>Details</summary>
Motivation: 提升文本嵌入模型在语义推理任务中的表现，同时保持对通用语义理解任务的良好性能。

Method: 通过多次前向传播运行文本嵌入模型，获得更精炼的语义表示，激活模型在预训练阶段学到的语义推理能力。

Result: 在BRIGHT和PJBenchmark1等语义推理任务中，RT方法取得显著性能提升，同时在C-MTEB等通用语义理解任务中表现稳定。

Conclusion: RT作为一种测试时推理方法，有效增强了基于解码器的文本嵌入模型的语义推理能力。

Abstract: We propose RT (Refine Thought), a method that can enhance the semantic rea-soning ability of text embedding models. The method obtains the final semanticrepresentation by running multiple forward passes of the text embedding model.Experiments show that RT achieves significant improvements on semantic reason-ing tasks in BRIGHT and the person job matching benchmark PJBenchmark1, while maintaining consistent performance on general-purpose semantic under-standing tasks such as C-MTEB. Our results indicate that RT is effective becauseit further activates the semantic reasoning ability learned during pretraining bydecoder-only text embedding models(e.g., Qwen3-Embedding-8B). RT canbe seen as a test-time inference method.

</details>


### [4] [Can QE-informed (Re)Translation lead to Error Correction?](https://arxiv.org/abs/2511.13884)
*Govardhan Padmanabhan*

Main category: cs.CL

TL;DR: 该论文提出两种用于机器翻译质量评估与错误纠正的无训练方法，其中基于多模型选优的方法表现最佳，获得子任务冠军。


<details>
  <summary>Details</summary>
Motivation: 自动后编辑虽能提升翻译质量，但存在过度修正问题，导致性能下降，故需要训练免费且有效的替代方法。

Method: 提出两种无训练方法：1）多模型生成多翻译版本中选择质量最高者；2）基于QE解释指导大语言模型只替换错误片段，并使用启发式条件最小化编辑次数以提升修正效率。

Result: 第一种多候选选优方法获得Delta COMET增益0.0201，第二种编辑方法为-0.0108，前者在子任务排行榜中获胜。

Conclusion: 训练免费、QE指导的多模型选优方法有效避免过度修正，显著提升了自动翻译质量评估下的错误纠正性能。

Abstract: The paper presents two approaches submitted to the WMT 2025 Automated Translation Quality Evaluation Systems Task 3 - Quality Estimation (QE)-informed Segment-level Error Correction. While jointly training QE systems with Automatic Post-Editing (APE) has shown improved performance for both tasks, APE systems are still known to overcorrect the output of Machine Translation (MT), leading to a degradation in performance. We investigate a simple training-free approach - QE-informed Retranslation, and compare it with another within the same training-free paradigm. Our winning approach selects the highest-quality translation from multiple candidates generated by different LLMs. The second approach, more akin to APE, instructs an LLM to replace error substrings as specified in the provided QE explanation(s). A conditional heuristic was employed to minimise the number of edits, with the aim of maximising the Gain-to-Edit ratio. The two proposed approaches achieved a Delta COMET score of 0.0201 and -0.0108, respectively, leading the first approach to achieve the winning position on the subtask leaderboard.

</details>


### [5] [What Works for 'Lost-in-the-Middle' in LLMs? A Study on GM-Extract and Mitigations](https://arxiv.org/abs/2511.13900)
*Mihir Gupte,Eshan Dixit,Muhammad Tayyab,Arun Adiththan*

Main category: cs.CL

TL;DR: 本文提出了GM-Extract基准数据集，用于评估大型语言模型在长距离上下文中检索控制变量的性能，设计了空间和语义两种评估指标，系统评估了7-8亿参数模型在多文档任务上的表现，发现数据表示方式显著影响检索效果，并分析了减轻性能下降现象的黑盒和白盒方法，揭示了它们在实际应用中的复杂效用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在长距离上下文中性能下降（"迷失中间"现象）严重影响基于检索的应用效果，亟需一个真实场景下评估和诊断该问题的工具和方法。

Method: 提出GM-Extract数据集，设计文档度量和变量提取两个评价指标，系统测试7-8亿参数模型在键值提取和问答任务上的检索性能，并通过文献调研总结两类缓解策略（黑盒和白盒），在基准上测试其效果。

Result: 不同数据表示在上下文中的性能表现显著不同，检索性能与困惑度存在相关性，缓解策略在不同场景下表现差异大，有的提升性能，有的反而产生负面影响。

Conclusion: 本文系统揭示了大型语言模型在长距离上下文检索中的性能表现及其影响因素，基准和评价指标有助于深入理解“迷失中间”现象及缓解策略的应用价值。

Abstract: The diminishing ability of large language models (LLMs) to effectively utilize long-range context-the "lost-in-the-middle" phenomenon-poses a significant challenge in retrieval-based LLM applications. To study the impact of this phenomenon in a real-world application setting, we introduce GM-Extract, a novel benchmark dataset meticulously designed to evaluate LLM performance on retrieval of control variables. To accurately diagnose failure modes, we propose a simple yet elegant evaluation system using two distinct metrics: one for spatial retrieval capability (Document Metric) and the other for semantic retrieval capability (Variable Extraction Metric). We conduct a systematic evaluation of 7-8B parameter models on two multi-document tasks (key-value extraction and question-answering), demonstrating a significant change in retrieval performance simply by altering how the data is represented in the context window. While a distinct U-shaped curve was not consistently observed, our analysis reveals a clear pattern of performance across models, which we further correlate with perplexity scores. Furthermore, we perform a literature survey of mitigation methods, which we categorize into two distinct approaches: black-box and white-box methods. We then apply these techniques to our benchmark, finding that their efficacy is highly nuanced. Our evaluation highlights scenarios where these strategies successfully improve performance, as well as surprising cases where they lead to a negative impact, providing a comprehensive understanding of their utility in a practical context.

</details>


### [6] [Hint-Augmented Re-ranking: Efficient Product Search using LLM-Based Query Decomposition](https://arxiv.org/abs/2511.13994)
*Yilun Zhu,Nikhita Vedula,Shervin Malmasi*

Main category: cs.CL

TL;DR: 本文提出一种利用大语言模型（LLM）解析电商搜索中含有最高级词汇的查询意图的方法，通过分解查询为属性-值提示，提高搜索和排序性能，并通过轻量级模型实现高效部署。


<details>
  <summary>Details</summary>
Motivation: 超级级查询在电商搜索中需要跨多个维度比较候选项，这对语言理解和领域知识提出了挑战，现有方法难以高效捕捉此类意图。

Method: 设计框架用LLM并行生成结构化的属性-值提示，并将提示整合进检索排序流程，同时通过知识迁移将复杂的超级级语义传递给轻量级模型以加速部署。

Result: 该方法在均值平均精度（MAP）上提升10.9点，平均排名相关性（MRR）提升5.9点，显著优于基线方案。

Conclusion: 研究展示了如何高效表示和传递超级级语义，推动检索系统的语言理解能力提升，同时兼顾实际应用中的延迟和部署需求。

Abstract: Search queries with superlatives (e.g., best, most popular) require comparing candidates across multiple dimensions, demanding linguistic understanding and domain knowledge. We show that LLMs can uncover latent intent behind these expressions in e-commerce queries through a framework that extracts structured interpretations or hints. Our approach decomposes queries into attribute-value hints generated concurrently with retrieval, enabling efficient integration into the ranking pipeline. Our method improves search performanc eby 10.9 points in MAP and ranking by 5.9 points in MRR over baselines. Since direct LLM-based reranking faces prohibitive latency, we develop an efficient approach transferring superlative interpretations to lightweight models. Our findings provide insights into how superlative semantics can be represented and transferred between models, advancing linguistic interpretation in retrieval systems while addressing practical deployment constraints.

</details>


### [7] [Knowledge-Grounded Agentic Large Language Models for Multi-Hazard Understanding from Reconnaissance Reports](https://arxiv.org/abs/2511.14010)
*Chenchen Kuai,Zihao Li,Braden Rosen,Stephanie Paan,Navid Jafari,Jean-Louis Briaud,Yunlong Zhang,Youssef M. A. Hashash,Yang Zhou*

Main category: cs.CL

TL;DR: 该论文提出了MoRA-RAG，一个基于知识的多灾害推理语言模型框架，用于系统化分析灾后侦察报告，显著提升了灾害信息提取的准确性并减少模型幻觉。


<details>
  <summary>Details</summary>
Motivation: 灾后侦察报告包含关键的多灾害交互证据，但其非结构化叙述形式导致系统化知识传递困难。现有大型语言模型缺乏领域基础时容易生成不可靠内容。

Method: 提出MoRA-RAG框架，集成多检索机制和基于代理的分块策略，动态路由查询至不同灾害数据库，结合验证循环评估证据完整性并优化查询。构建HazardRecQA数据集供训练与测试。

Result: MoRA-RAG在HazardRecQA上达到94.5%准确率，较零样本语言模型提高30%，优于现有最先进的RAG系统10%，并显著降低了模型幻觉现象。

Conclusion: MoRA-RAG建立了一种新范式，将非结构化的灾后数据转化为可信且可操作的多灾害智能信息，提升了灾害韧性分析的实用性和可靠性。

Abstract: Post-disaster reconnaissance reports contain critical evidence for understanding multi-hazard interactions, yet their unstructured narratives make systematic knowledge transfer difficult. Large language models (LLMs) offer new potential for analyzing these reports, but often generate unreliable or hallucinated outputs when domain grounding is absent. This study introduces the Mixture-of-Retrieval Agentic RAG (MoRA-RAG), a knowledge-grounded LLM framework that transforms reconnaissance reports into a structured foundation for multi-hazard reasoning. The framework integrates a Mixture-of-Retrieval mechanism that dynamically routes queries across hazard-specific databases while using agentic chunking to preserve contextual coherence during retrieval. It also includes a verification loop that assesses evidence sufficiency, refines queries, and initiates targeted searches when information remains incomplete. We construct HazardRecQA by deriving question-answer pairs from GEER reconnaissance reports, which document 90 global events across seven major hazard types. MoRA-RAG achieves up to 94.5 percent accuracy, outperforming zero-shot LLMs by 30 percent and state-of-the-art RAG systems by 10 percent, while reducing hallucinations across diverse LLM architectures. MoRA-RAG also enables open-weight LLMs to achieve performance comparable to proprietary models. It establishes a new paradigm for transforming post-disaster documentation into actionable, trustworthy intelligence for hazard resilience.

</details>


### [8] [HiEAG: Evidence-Augmented Generation for Out-of-Context Misinformation Detection](https://arxiv.org/abs/2511.14027)
*Junjie Wu,Yumeng Fu,Nan Yu,Guohong Fu*

Main category: cs.CL

TL;DR: 本文提出HiEAG框架，通过层级证据增强生成方法，结合多模态大语言模型，提升图文对外部一致性检测，实现更准确的多模态非上下文谣言检测。


<details>
  <summary>Details</summary>
Motivation: 当前多模态非上下文谣言检测方法过于关注内部一致性，忽视了图文对与外部证据之间的一致性，导致检测效果受限。

Method: 提出层级证据增强生成框架HiEAG，包含证据检索、自动证据重排（AESP）和自动证据生成（AEGP）模块，利用多模态大语言模型改进外部一致性检测，并实现结果解释。

Result: 在多个基准数据集上，HiEAG方法的准确率超过了先前最先进的方法，表现优异。

Conclusion: HiEAG有效提升了多模态非上下文谣言检测中外部一致性的检测能力，为该领域提供了新的技术路径与性能提升。

Abstract: Recent advancements in multimodal out-of-context (OOC) misinformation detection have made remarkable progress in checking the consistencies between different modalities for supporting or refuting image-text pairs. However, existing OOC misinformation detection methods tend to emphasize the role of internal consistency, ignoring the significant of external consistency between image-text pairs and external evidence. In this paper, we propose HiEAG, a novel Hierarchical Evidence-Augmented Generation framework to refine external consistency checking through leveraging the extensive knowledge of multimodal large language models (MLLMs). Our approach decomposes external consistency checking into a comprehensive engine pipeline, which integrates reranking and rewriting, apart from retrieval. Evidence reranking module utilizes Automatic Evidence Selection Prompting (AESP) that acquires the relevant evidence item from the products of evidence retrieval. Subsequently, evidence rewriting module leverages Automatic Evidence Generation Prompting (AEGP) to improve task adaptation on MLLM-based OOC misinformation detectors. Furthermore, our approach enables explanation for judgment, and achieves impressive performance with instruction tuning. Experimental results on different benchmark datasets demonstrate that our proposed HiEAG surpasses previous state-of-the-art (SOTA) methods in the accuracy over all samples.

</details>


### [9] [Based on Data Balancing and Model Improvement for Multi-Label Sentiment Classification Performance Enhancement](https://arxiv.org/abs/2511.14073)
*Zijin Su,Huanzhu Lv,Yuren Niu,Yiming Liu*

Main category: cs.CL

TL;DR: 该论文通过构建平衡的多标签情感数据集并设计结合FastText、卷积层、双向LSTM及注意力机制的多标签情感分类模型，显著提升了情感分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有多标签情感数据集如GoEmotions存在严重的类别不平衡问题，影响模型对低频情感的识别效果。

Method: 构建了基于原始GoEmotions数据、Sentiment140情感标签样本及GPT-4 mini手动标注文本的平衡数据集；设计结合FastText词嵌入、卷积层、双向LSTM和注意力机制的多标签分类模型，并采用sigmoid输出层和混合精度训练。

Result: 相比使用不平衡数据训练的模型，所提方法在准确率、精确率、召回率、F1值及AUC指标上均有显著提升。

Conclusion: 通过构建平衡数据集及设计融合多种技术的分类模型，有效提升了多标签情感分类的性能，特别是对低频情感的识别能力。

Abstract: Multi-label sentiment classification plays a vital role in natural language processing by detecting multiple emotions within a single text. However, existing datasets like GoEmotions often suffer from severe class imbalance, which hampers model performance, especially for underrepresented emotions. To address this, we constructed a balanced multi-label sentiment dataset by integrating the original GoEmotions data, emotion-labeled samples from Sentiment140 using a RoBERTa-base-GoEmotions model, and manually annotated texts generated by GPT-4 mini. Our data balancing strategy ensured an even distribution across 28 emotion categories. Based on this dataset, we developed an enhanced multi-label classification model that combines pre-trained FastText embeddings, convolutional layers for local feature extraction, bidirectional LSTM for contextual learning, and an attention mechanism to highlight sentiment-relevant words. A sigmoid-activated output layer enables multi-label prediction, and mixed precision training improves computational efficiency. Experimental results demonstrate significant improvements in accuracy, precision, recall, F1-score, and AUC compared to models trained on imbalanced data, highlighting the effectiveness of our approach.

</details>


### [10] [Stealth Fine-Tuning: Efficiently Breaking Alignment in RVLMs Using Self-Generated CoT](https://arxiv.org/abs/2511.14106)
*Le Yu,Zhengyue Zhao,Yawen Zheng,Yunhao Liu*

Main category: cs.CL

TL;DR: 本文提出了一种名为Stealth Fine-Tuning的新攻击方法，通过细粒度干扰和自监督微调，突破了推理增强视觉语言模型(RVLMs)的安全对齐，实验证明该方法低成本、高效且保持模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有RVLMs虽然通过安全对齐避免有害行为，但其显式的链式思维(CoT)暴露了新的攻击面，迫切需要更有效的攻击方法来测试和提升安全性。

Method: 通过分段级干扰(segment-level interference)诱导有害推理痕迹，并利用自生成输出作为监督微调数据，结合基于回合加权的损失设计，实现轻量且分布一致的微调。

Result: 使用499个样本和单卡3小时训练，Stealth Fine-Tuning比现有方法IDEATOR提升38.52%的攻击成功率（ASR），且模型仍保持原有推理能力。评测表明，本方法能够有效绕过现有对齐防御。

Conclusion: Stealth Fine-Tuning是一种低成本、高效且能有效突破RVLM安全对齐的新型攻击手段，提示需要更强的安全机制保护多模态模型。

Abstract: Reasoning-augmented Vision-Language Models (RVLMs) rely on safety alignment to prevent harmful behavior, yet their exposed chain-of-thought (CoT) traces introduce new attack surfaces. In this work, we find that the safety alignment of RVLMs can be easily break through a novel attack method termed \textbf{Stealth Fine-Tuning}. Our method elicits harmful reasoning traces through \textbf{segment-level interference} and reuses the self-generated outputs as supervised fine-tuning data. Through a \textbf{turn-based weighted} loss design, yielding a lightweight, distribution-consistent finetuning method. In our experiment, with only 499 samples and under 3 hours on a single A100 (QLoRA), Stealth Fine-Tuning outperforms IDEATOR by 38.52\% ASR while preserving general reasoning ability, as the tuned model retains the original representation distribution. Experiments on AdvBench and several general benchmarks demonstrate that Stealth Fine-Tuning is a low-cost and highly effective way to bypass alignment defenses. \textcolor{red}{\textbf{Disclaimer: This paper contains content that may be disturbing or offensive.}}

</details>


### [11] [Synthetic Clinical Notes for Rare ICD Codes: A Data-Centric Framework for Long-Tail Medical Coding](https://arxiv.org/abs/2511.14112)
*Truong Vo,Weiyi Wu,Kaize Ding*

Main category: cs.CL

TL;DR: 本文提出了一种基于生成合成病历摘要的数据中心方法，以缓解ICD编码中的长尾分布问题，通过生成包含7902个ICD代码的90000条合成数据扩展训练集，显著提升了宏F1分数，优于现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 医疗文本自动ICD编码任务中，罕见和零样本的ICD代码样本极度匮乏，导致模型对长尾代码的预测性能差，宏F1分数低，亟需解决数据不平衡问题。

Method: 构建以罕见代码为中心的多标签代码集，利用真实临床共现模式、ICD描述、同义词、分类体系和相似临床笔记等结构化提示，生成9万条合成出院摘要，扩展训练数据，随后在原数据及扩展数据上微调两种基于变换器的最新模型PLM-ICD和GKI-ICD。

Result: 在扩展数据上微调模型后，宏F1得分有所提升，同时保持强劲的微F1分数，整体性能优于之前的最先进技术。

Conclusion: 虽然提升幅度相对计算成本不大，但通过精心设计的合成数据生成，可以有效缓解长尾ICD编码上的性能不平衡问题，提高模型对罕见代码的预测能力。

Abstract: Automatic ICD coding from clinical text is a critical task in medical NLP but remains hindered by the extreme long-tail distribution of diagnostic codes. Thousands of rare and zero-shot ICD codes are severely underrepresented in datasets like MIMIC-III, leading to low macro-F1 scores. In this work, we propose a data-centric framework that generates high-quality synthetic discharge summaries to mitigate this imbalance. Our method constructs realistic multi-label code sets anchored on rare codes by leveraging real-world co-occurrence patterns, ICD descriptions, synonyms, taxonomy, and similar clinical notes. Using these structured prompts, we generate 90,000 synthetic notes covering 7,902 ICD codes, significantly expanding the training distribution. We fine-tune two state-of-the-art transformer-based models, PLM-ICD and GKI-ICD, on both the original and extended datasets. Experiments show that our approach modestly improves macro-F1 while maintaining strong micro-F1, outperforming prior SOTA. While the gain may seem marginal relative to the computational cost, our results demonstrate that carefully crafted synthetic data can enhance equity in long-tail ICD code prediction.

</details>


### [12] [From Graphs to Hypergraphs: Enhancing Aspect-Based Sentiment Analysis via Multi-Level Relational Modeling](https://arxiv.org/abs/2511.14142)
*Omkar Mahesh Kashyap,Padegal Amit,Madhav Kashyap,Ashwini M Joshi,Shylaja SS*

Main category: cs.CL

TL;DR: 本文提出了动态超图模型HyperABSA，用于改进基于方面的情感分析，解决短文本中情感冲突和背景稀疏问题。


<details>
  <summary>Details</summary>
Motivation: 传统图模型仅考虑成对关系，需构建多图处理多种关系，导致模型冗余、参数过多、错误传播，降低短文本、低资源场景的鲁棒性。

Method: 提出动态超图框架HyperABSA，通过样本特异的层次聚类自动构建超边，利用加速-回退截止策略自适应确定聚类粒度，实现高效结构建模。

Result: 在Lap14、Rest14和MAMS三个数据集上，使用RoBERTa等强基线对比，HyperABSA均表现出稳定的性能提升。

Conclusion: 动态超图构建为ABSA任务提供了高效且强大的结构建模方案，具备推广到其他短文本自然语言处理任务的潜力。

Abstract: Aspect-Based Sentiment Analysis (ABSA) predicts sentiment polarity for specific aspect terms, a task made difficult by conflicting sentiments across aspects and the sparse context of short texts. Prior graph-based approaches model only pairwise dependencies, forcing them to construct multiple graphs for different relational views. These introduce redundancy, parameter overhead, and error propagation during fusion, limiting robustness in short-text, low-resource settings. We present HyperABSA, a dynamic hypergraph framework that induces aspect-opinion structures through sample-specific hierarchical clustering. To construct these hyperedges, we introduce a novel acceleration-fallback cutoff for hierarchical clustering, which adaptively determines the level of granularity. Experiments on three benchmarks (Lap14, Rest14, MAMS) show consistent improvements over strong graph baselines, with substantial gains when paired with RoBERTa backbones. These results position dynamic hypergraph construction as an efficient, powerful alternative for ABSA, with potential extensions to other short-text NLP tasks.

</details>


### [13] [Applying Relation Extraction and Graph Matching to Answering Multiple Choice Questions](https://arxiv.org/abs/2511.14144)
*Naoki Shimoda,Akihiro Yamamoto*

Main category: cs.CL

TL;DR: 本文结合基于Transformer的关系抽取与知识图谱匹配，应用于选择题答题，同时保持输出过程的可追溯性。


<details>
  <summary>Details</summary>
Motivation: 传统知识图谱构建成本高且被视为静态数据库，而最近的Transformer关系抽取技术能动态生成知识图谱，提升对输入句子意义的表示能力，故提出用于选择题答题的新方法。

Method: 方法通过关系抽取将题干转化为关系图，结合已知正确知识图谱进行验证，以判断题目描述的真实性，从而回答填空式选择题，并保持过程的可追溯性。

Result: 实验表明，该方法能正确回答约70%的选择题，且题目类型对答题准确率影响显著。

Conclusion: 本文提出的基于动态知识图谱生成及验证的答题方法有效且具有过程可追溯性，展示了结合Transformer关系抽取与知识图谱在多选题答题中的潜力。

Abstract: In this research, we combine Transformer-based relation extraction with matching of knowledge graphs (KGs) and apply them to answering multiple-choice questions (MCQs) while maintaining the traceability of the output process. KGs are structured representations of factual knowledge consisting of entities and relations. Due to the high construction cost, they had been regarded as static databases with validated links. However, the recent development of Transformer-based relation extraction (RE) methods has enabled us to generate KGs dynamically by giving them natural language texts, and thereby opened the possibility for representing the meaning of the input sentences with the created KGs. Using this effect, we propose a method that answers MCQs in the "fill-in-the-blank" format, taking care of the point that RE methods generate KGs that represent false information if provided with factually incorrect texts. We measure the truthfulness of each question sentence by (i) converting the sentence into a relational graph using an RE method and (ii) verifying it against factually correct KGs under the closed-world assumption. The experimental results demonstrate that our method correctly answers up to around 70% of the questions, while providing traceability of the procedure. We also highlight that the question category has a vast influence on the accuracy.

</details>


### [14] [Selective Weak-to-Strong Generalization](https://arxiv.org/abs/2511.14166)
*Hao Lang,Fei Huang,Yongbin Li*

Main category: cs.CL

TL;DR: 本文提出了一种选择性弱到强泛化(Selective W2SG)框架，通过训练二元分类器识别强模型可回答的问题，避免不必要的弱监督使用，并利用图平滑方法优化弱标签，实现了模型的鲁棒对齐。


<details>
  <summary>Details</summary>
Motivation: 未来超人类模型能力超过人类，人类只能弱监督这些模型，现有方法持续使用弱监督导致模型鲁棒性问题，需要减少有害弱标签的影响。

Method: 设计选择性W2SG框架，训练二元分类器P(IK)识别强模型能回答的问题，针对性使用自生成标签进行对齐，并采用图平滑技术优化弱标签。

Result: 在三个基准测试中，方法持续超过竞争基线，P(IK)能够跨任务和难度泛化，验证了选择性W2SG提升模型鲁棒性的有效性。

Conclusion: 选择性弱到强泛化有效避免了不必要的弱监督，改善了模型对齐的鲁棒性，对未来超人类模型的超对齐具有重要意义。

Abstract: Future superhuman models will surpass the ability of humans and humans will only be able to \textit{weakly} supervise superhuman models. To alleviate the issue of lacking high-quality data for model alignment, some works on weak-to-strong generalization (W2SG) finetune a strong pretrained model with a weak supervisor so that it can generalize beyond weak supervision. However, the invariable use of weak supervision in existing methods exposes issues in robustness, with a proportion of weak labels proving harmful to models. In this paper, we propose a selective W2SG framework to avoid using weak supervision when unnecessary. We train a binary classifier P(IK) to identify questions that a strong model can answer and use its self-generated labels for alignment. We further refine weak labels with a graph smoothing method. Extensive experiments on three benchmarks show that our method consistently outperforms competitive baselines. Further analyses show that P(IK) can generalize across tasks and difficulties, which indicates selective W2SG can help superalignment.

</details>


### [15] [SymLoc: Symbolic Localization of Hallucination across HaluEval and TruthfulQA](https://arxiv.org/abs/2511.14172)
*Naveen Lamba,Sanju Tiwari,Manas Gaur*

Main category: cs.CL

TL;DR: 该论文提出了首个基于符号语言学知识的定位框架，揭示大型语言模型在处理符号触发器时早期层就出现幻觉，证明幻觉实质是符号语言处理失败。


<details>
  <summary>Details</summary>
Motivation: 目前尚不清楚大型语言模型幻觉的具体起源，尤其是符号元素（如否定、数字等）如何驱动幻觉，缺乏系统化的定位方法。

Method: 构建第一个利用符号语言学和语义知识进行幻觉定位的框架，通过分析模型各层对符号触发器的注意力变化，使用HaluEval和TruthfulQA评测了五个模型。

Result: 发现符号触发器在模型早期层产生剧烈的注意力方差波动，特别是否定导致极端不稳定表现；幻觉率高且随模型层加深关注度下降，表明符号语义处理从一开始就崩溃。

Conclusion: 幻觉主要源自对符号语言的处理失败，而非通用生成问题，符号语义知识是理解和定位大型语言模型幻觉机制的关键。

Abstract: LLMs still struggle with hallucination, especially when confronted with symbolic triggers like modifiers, negation, numbers, exceptions, and named entities. Yet, we lack a clear understanding of where these symbolic hallucinations originate, making it crucial to systematically handle such triggers and localize the emergence of hallucination inside the model. While prior work explored localization using statistical techniques like LSC and activation variance analysis, these methods treat all tokens equally and overlook the role symbolic linguistic knowledge plays in triggering hallucinations. So far, no approach has investigated how symbolic elements specifically drive hallucination failures across model layers, nor has symbolic linguistic knowledge been used as the foundation for a localization framework. We propose the first symbolic localization framework that leverages symbolic linguistic and semantic knowledge to meaningfully trace the development of hallucinations across all model layers. By focusing on how models process symbolic triggers, we analyze five models using HaluEval and TruthfulQA. Our symbolic knowledge approach reveals that attention variance for these linguistic elements explodes to critical instability in early layers (2-4), with negation triggering catastrophic variance levels, demonstrating that symbolic semantic processing breaks down from the very beginning. Through the lens of symbolic linguistic knowledge, despite larger model sizes, hallucination rates remain consistently high (78.3%-83.7% across Gemma variants), with steep attention drops for symbolic semantic triggers throughout deeper layers. Our findings demonstrate that hallucination is fundamentally a symbolic linguistic processing failure, not a general generation problem, revealing that symbolic semantic knowledge provides the key to understanding and localizing hallucination mechanisms in LLMs.

</details>


### [16] [Harnessing Deep LLM Participation for Robust Entity Linking](https://arxiv.org/abs/2511.14181)
*Jiajun Hou,Chenyu Zhang,Rui Meng*

Main category: cs.CL

TL;DR: 提出了DeepEL框架，深度整合大型语言模型（LLMs）于实体链接任务的各个阶段，通过自我验证机制提升实体消歧性能，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅在实体链接任务的部分阶段利用LLMs，且孤立消歧导致性能受限，需充分发挥LLMs的整体能力。

Method: 设计DeepEL框架，将LLMs融入实体链接全过程；引入基于全局上下文的自我验证机制，使模型能够修正自身预测并识别实体间的连贯关系。

Result: 在十个基准数据集上，DeepEL平均F1提升2.6%，在跨领域数据集上提升达4%，明显优于当前最先进方法。

Conclusion: 通过深度整合LLMs及自我验证机制，DeepEL有效提升了实体链接性能，推动该领域技术进步。

Abstract: Entity Linking (EL), the task of mapping textual entity mentions to their corresponding entries in knowledge bases, constitutes a fundamental component of natural language understanding. Recent advancements in Large Language Models (LLMs) have demonstrated remarkable potential for enhancing EL performance. Prior research has leveraged LLMs to improve entity disambiguation and input representation, yielding significant gains in accuracy and robustness. However, these approaches typically apply LLMs to isolated stages of the EL task, failing to fully integrate their capabilities throughout the entire process.
  In this work, we introduce DeepEL, a comprehensive framework that incorporates LLMs into every stage of the entity linking task. Furthermore, we identify that disambiguating entities in isolation is insufficient for optimal performance. To address this limitation, we propose a novel self-validation mechanism that utilizes global contextual information, enabling LLMs to rectify their own predictions and better recognize cohesive relationships among entities within the same sentence.
  Extensive empirical evaluation across ten benchmark datasets demonstrates that DeepEL substantially outperforms existing state-of-the-art methods, achieving an average improvement of 2.6\% in overall F1 score and a remarkable 4% gain on out-of-domain datasets. These results underscore the efficacy of deep LLM integration in advancing the state-of-the-art in entity linking.

</details>


### [17] [ArbESC+: Arabic Enhanced Edit Selection System Combination for Grammatical Error Correction Resolving conflict and improving system combination in Arabic GEC](https://arxiv.org/abs/2511.14230)
*Ahlam Alrehili,Areej Alhothali*

Main category: cs.CL

TL;DR: 本文提出了一种面向阿拉伯语语法错误更正的多系统集成方法ArbESC+，利用多个模型的改正提案并通过分类器筛选最佳改正，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语复杂的形态和句法结构使语法错误更正比其他语言更具挑战性，且以往多为单一模型，未充分利用多模型结合的潜力。

Method: 采用多个模型（如AraT5、ByT5、mT5、AraBART等）生成纠正建议，将这些建议作为数值特征输入分类器，利用支持技术过滤重叠纠正并评估决策可信度，从而确定最终纠正结果。

Result: 多模型组合在QALB-14测试数据上F0.5达到82.63%，QALB-15 L1数据为84.64%，L2数据为65.55%，明显优于单模型效果。

Conclusion: 首次尝试阿拉伯语语法错误校正的多系统集成，提升了语法纠正性能，为阿拉伯语文本处理工具的研发提供了有力支持和参考。

Abstract: Grammatical Error Correction (GEC) is an important aspect of natural language processing. Arabic has a complicated morphological and syntactic structure, posing a greater challenge than other languages. Even though modern neural models have improved greatly in recent years, the majority of previous attempts used individual models without taking into account the potential benefits of combining different systems. In this paper, we present one of the first multi-system approaches for correcting grammatical errors in Arabic, the Arab Enhanced Edit Selection System Complication (ArbESC+). Several models are used to collect correction proposals, which are represented as numerical features in the framework. A classifier determines and implements the appropriate corrections based on these features. In order to improve output quality, the framework uses support techniques to filter overlapping corrections and estimate decision reliability. A combination of AraT5, ByT5, mT5, AraBART, AraBART+Morph+GEC, and Text editing systems gave better results than a single model alone, with F0.5 at 82.63% on QALB-14 test data, 84.64% on QALB-15 L1 data, and 65.55% on QALB-15 L2 data. As one of the most significant contributions of this work, it's the first Arab attempt to integrate linguistic error correction. Improving existing models provides a practical step towards developing advanced tools that will benefit users and researchers of Arabic text processing.

</details>


### [18] [MuCPT: Music-related Natural Language Model Continued Pretraining](https://arxiv.org/abs/2511.14245)
*Kai Tian,Yirong Mao,Wendong Bi,Hanjie Wang,Que Wenhui*

Main category: cs.CL

TL;DR: 本文针对音乐领域大语言模型训练的语料和训练机制问题，构建了大规模音乐相关文本语料库，并设计了基于参考模型的动态质量控制机制，提升模型在音乐领域的表现，同时提出了MusicSimpleQA基准测试用于评估事实准确性。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在通用任务表现良好，但在音乐等专业领域受到语料规模、纯度及训练目标匹配度限制，特别是在音乐娱乐领域。为解决此问题，需要构建高质量的音乐领域语料并设计有效的训练和评估机制。

Method: 构建了40亿tokens的音乐相关大规模语料，采用领域优先的数据处理流程，包括轻量级分类器筛选加权、分阶段清洗、去重及隐私保护掩码；融合多源音乐文本及元数据，形成结构化的领域知识库；引入基于参考模型的token级软评分机制，通过统一损失比率准则实现数据选择和动态权重调整，有效减少噪声梯度，增强任务信号；设计MusicSimpleQA作为短答案自动评分的事实性评测基准。

Result: 通过上述方法，显著提升了音乐领域大语言模型的训练效果和事实准确性，实验系统展示了不同数据组成对模型表现的影响，证明了数据和训练目标匹配的重要性。

Conclusion: 本文提出的基于大规模高质量音乐专属语料和动态质量控制的训练框架，以及可复用的事实性评测工具，为音乐领域专业大语言模型的构建提供了有效路径，推动了领域特化模型的发展。

Abstract: Large language models perform strongly on general tasks but remain constrained in specialized settings such as music, particularly in the music-entertainment domain, where corpus scale, purity, and the match between data and training objectives are critical. We address this by constructing a large, music-related natural language corpus (40B tokens) that combines open source and in-house data, and by implementing a domain-first data pipeline: a lightweight classifier filters and weights in-domain text, followed by multi-stage cleaning, de-duplication, and privacy-preserving masking. We further integrate multi-source music text with associated metadata to form a broader, better-structured foundation of domain knowledge. On the training side, we introduce reference-model (RM)-based token-level soft scoring for quality control: a unified loss-ratio criterion is used both for data selection and for dynamic down-weighting during optimization, reducing noise gradients and amplifying task-aligned signals, thereby enabling more effective music-domain continued pretraining and alignment. To assess factuality, we design the MusicSimpleQA benchmark, which adopts short, single-answer prompts with automated agreement scoring. Beyond the benchmark design, we conduct systematic comparisons along the axes of data composition. Overall, this work advances both the right corpus and the right objective, offering a scalable data-training framework and a reusable evaluation tool for building domain LLMs in the music field.

</details>


### [19] [Towards Authentic Movie Dubbing with Retrieve-Augmented Director-Actor Interaction Learning](https://arxiv.org/abs/2511.14249)
*Rui Liu,Yuan Zhao,Zhenqi Jia*

Main category: cs.CL

TL;DR: 提出了一种新的电影自动配音模型Authentic-Dubber，通过模拟导演与演员的动态互动，实现逼真的情感表达和唇同步。


<details>
  <summary>Details</summary>
Motivation: 现有配音方法忽视导演与演员之间的互动，未能捕捉到演员对情感上下文的深刻理解与表达。

Method: 构建多模态参考素材库，利用大语言模型深入理解情感信息；提出基于情感相似度的检索增强策略，获取与目标视频相关的多模态信息；采用图结构的渐进式语音生成方法，逐步融合情感知识以生成配音。

Result: 在V2C Animation数据集上，方法在情感表现和自然度方面均显著优于现有技术。

Conclusion: Authentic-Dubber有效模拟了真实导演-演员配音流程，实现了情感丰富且唇同步的自动电影配音。

Abstract: The automatic movie dubbing model generates vivid speech from given scripts, replicating a speaker's timbre from a brief timbre prompt while ensuring lip-sync with the silent video. Existing approaches simulate a simplified workflow where actors dub directly without preparation, overlooking the critical director-actor interaction. In contrast, authentic workflows involve a dynamic collaboration: directors actively engage with actors, guiding them to internalize the context cues, specifically emotion, before performance. To address this issue, we propose a new Retrieve-Augmented Director-Actor Interaction Learning scheme to achieve authentic movie dubbing, termed Authentic-Dubber, which contains three novel mechanisms: (1) We construct a multimodal Reference Footage library to simulate the learning footage provided by directors. Note that we integrate Large Language Models (LLMs) to achieve deep comprehension of emotional representations across multimodal signals. (2) To emulate how actors efficiently and comprehensively internalize director-provided footage during dubbing, we propose an Emotion-Similarity-based Retrieval-Augmentation strategy. This strategy retrieves the most relevant multimodal information that aligns with the target silent video. (3) We develop a Progressive Graph-based speech generation approach that incrementally incorporates the retrieved multimodal emotional knowledge, thereby simulating the actor's final dubbing process. The above mechanisms enable the Authentic-Dubber to faithfully replicate the authentic dubbing workflow, achieving comprehensive improvements in emotional expressiveness. Both subjective and objective evaluations on the V2C Animation benchmark dataset validate the effectiveness. The code and demos are available at https://github.com/AI-S2-Lab/Authentic-Dubber.

</details>


### [20] [AfriSpeech-MultiBench: A Verticalized Multidomain Multicountry Benchmark Suite for African Accented English ASR](https://arxiv.org/abs/2511.14255)
*Gabrial Zencha Ashungafac,Mardhiyah Sanni,Busayo Awobade,Alex Gichamba,Tobi Olatunji*

Main category: cs.CL

TL;DR: 本文提出了AfriSpeech-MultiBench，一个针对非洲英语口音的多领域语音识别评测套件，覆盖100多种口音和7个应用领域，评估了多种开源和闭源的ASR及多模态LLM模型。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏针对非洲多样语言环境的应用专用语音识别模型评测，难以满足非洲不同国家和领域的需求。

Method: 构建包含非洲英语口音数据的7个领域评测套件，比较开源、闭源、单模态和多模态模型在自发和非自发语音上的表现。

Result: 开源ASR在自发语音表现优异但对噪声和非母语语音表现差，多模态LLM更抗口音但难处理领域专有名词，闭源模型准确率高但地域和领域差异大。微调模型在非洲英语上表现出低延迟和竞争力精度，然而幻觉问题普遍存在。

Conclusion: AfriSpeech-MultiBench为非洲语音技术选择提供了重要工具，促进了包容性语音应用的发展，尤其惠及非洲多样化社区。

Abstract: Recent advances in speech-enabled AI, including Google's NotebookLM and OpenAI's speech-to-speech API, are driving widespread interest in voice interfaces globally. Despite this momentum, there exists no publicly available application-specific model evaluation that caters to Africa's linguistic diversity. We present AfriSpeech-MultiBench, the first domain-specific evaluation suite for over 100 African English accents across 10+ countries and seven application domains: Finance, Legal, Medical, General dialogue, Call Center, Named Entities and Hallucination Robustness. We benchmark a diverse range of open, closed, unimodal ASR and multimodal LLM-based speech recognition systems using both spontaneous and non-spontaneous speech conversation drawn from various open African accented English speech datasets. Our empirical analysis reveals systematic variation: open-source ASR models excels in spontaneous speech contexts but degrades on noisy, non-native dialogue; multimodal LLMs are more accent-robust yet struggle with domain-specific named entities; proprietary models deliver high accuracy on clean speech but vary significantly by country and domain. Models fine-tuned on African English achieve competitive accuracy with lower latency, a practical advantage for deployment, hallucinations still remain a big problem for most SOTA models. By releasing this comprehensive benchmark, we empower practitioners and researchers to select voice technologies suited to African use-cases, fostering inclusive voice applications for underserved communities.

</details>


### [21] [Entropy-Guided Reasoning Compression](https://arxiv.org/abs/2511.14258)
*Hourun Zhu,Yang Gao,Wenlong Fei,Jiawei Li,Huashan Sun*

Main category: cs.CL

TL;DR: 本文提出了一种熵引导的训练框架，有效解决大规模推理模型推理链过长问题，实现推理长度压缩至原来的20%，同时保持甚至提升准确率。


<details>
  <summary>Details</summary>
Motivation: 大规模推理模型虽然在复杂推理任务中表现优异，但其推理链过长导致计算成本高、部署困难。现有压缩方法忽视了训练中的熵冲突问题，这限制了压缩效果。

Method: 分析熵冲突的成因后，采用熵引导训练框架：熵下降时鼓励简洁推理步骤，熵上升时强化探索以提升模型鲁棒性，实现性能与推理长度的平衡。

Result: 在六个数学基准测试中，该方法将推理长度压缩到原来的20%，同时准确率与基线持平或更优。

Conclusion: 熵引导训练有效缓解熵冲突，提升推理链压缩效果，助力大规模推理模型更高效且稳健地执行复杂任务。

Abstract: Large reasoning models have demonstrated remarkable performance on complex reasoning tasks, yet the excessive length of their chain-of-thought outputs remains a major practical bottleneck due to high computation cost and poor deployability. Existing compression methods have achieved partial success but overlook a crucial phenomenon in the training process -- the entropy conflict. During compression training, entropy decreases, leading to shorter reasoning but limited exploration, while accuracy-oriented objectives increase entropy, lengthening reasoning chains. This can cause the model to get stuck in a local dilemma. Our analysis further reveals the origin of the entropy conflict: many high-entropy tokens are logical connectors that receive larger gradients and are encouraged under the performance objective, while the compression objective simultaneously penalizes these potentially redundant connectors. This opposing pressure creates a direct source of entropy conflict. To address these issues, we adopt an entropy-guided training framework. As entropy descends, the model is guided toward efficient reasoning by encouraging concise thought steps; as entropy rises, exploration is reinforced under the compact reasoning mode to improve robustness. Experiments on six mathematical benchmarks show that our method compresses reasoning length to 20% of the original while maintaining or even surpassing baseline accuracy. Code and models will be released publicly.

</details>


### [22] [Don't Miss the Forest for the Trees: In-Depth Confidence Estimation for LLMs via Reasoning over the Answer Space](https://arxiv.org/abs/2511.14275)
*Ante Wang,Weizhi Ma,Yang Liu*

Main category: cs.CL

TL;DR: 本文提出通过预测口头概率分布来提升大语言模型的置信度估计，促进深入推理，方法在多模型、多任务中表现优越，且符合人类推理模式。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型能生成口头置信度，但推理策略如何影响置信度估计尚缺研究，亟需探索有效策略提升置信度的准确性和透明度。

Method: 通过让大语言模型预测一个口头概率分布，要求模型考虑所有答案候选，而非单一猜测，并合理分配置信度分数，满足概率分布要求。

Result: 该方法在不同模型和多种任务中均表现出优势，无论答案空间是否已知，强化学习后优势依旧存在，且模型的推理模式与人类预期一致。

Conclusion: 预测口头概率分布有效促进了模型深入推理，从而提升置信度估计的准确性和合理性，具有较好泛化性和解释性。

Abstract: Knowing the reliability of a model's response is essential in application. With the strong generation capabilities of LLMs, research has focused on generating verbalized confidence. This is further enhanced by combining chain-of-thought reasoning, which provides logical and transparent estimation. However, how reasoning strategies affect the estimated confidence is still under-explored. In this work, we demonstrate that predicting a verbalized probability distribution can effectively encourage in-depth reasoning for confidence estimation. Intuitively, it requires an LLM to consider all candidates within the answer space instead of basing on a single guess, and to carefully assign confidence scores to meet the requirements of a distribution. This method shows an advantage across different models and various tasks, regardless of whether the answer space is known. Its advantage is maintained even after reinforcement learning, and further analysis shows its reasoning patterns are aligned with human expectations.

</details>


### [23] [AraLingBench A Human-Annotated Benchmark for Evaluating Arabic Linguistic Capabilities of Large Language Models](https://arxiv.org/abs/2511.14295)
*Mohammad Zbib,Hasan Abed Al Kader Hammoud,Sina Mukalled,Nadine Rizk,Fatima Karnib,Issam Lakkis,Ammar Mohanna,Bernard Ghanem*

Main category: cs.CL

TL;DR: AraLingBench是一个用于评估大型语言模型（LLMs）阿拉伯语语言能力的基准测试，涵盖语法、形态学、拼写、阅读理解和句法五个核心类别。


<details>
  <summary>Details</summary>
Motivation: 当前模型在知识型基准测试上表现良好，但缺乏真正的语言理解能力，尤其在阿拉伯语的深层语法和句法推理上存在不足。

Method: 设计了150道专家设计的多项选择题，直接测试阿拉伯语结构性语言理解能力，并用31个阿拉伯语及双语LLM进行了评估。

Result: 模型在表面层面的语言能力表现强，但在语法和句法的深度推理上表现较弱，暴露了记忆和模式识别优于真实理解的现象。

Conclusion: AraLingBench为阿拉伯语大型语言模型的发展提供了诊断性框架，有助于推动模型向真正的语言掌握能力发展。

Abstract: We present AraLingBench: a fully human annotated benchmark for evaluating the Arabic linguistic competence of large language models (LLMs). The benchmark spans five core categories: grammar, morphology, spelling, reading comprehension, and syntax, through 150 expert-designed multiple choice questions that directly assess structural language understanding. Evaluating 35 Arabic and bilingual LLMs reveals that current models demonstrate strong surface level proficiency but struggle with deeper grammatical and syntactic reasoning. AraLingBench highlights a persistent gap between high scores on knowledge-based benchmarks and true linguistic mastery, showing that many models succeed through memorization or pattern recognition rather than authentic comprehension. By isolating and measuring fundamental linguistic skills, AraLingBench provides a diagnostic framework for developing Arabic LLMs. The full evaluation code is publicly available on GitHub.

</details>


### [24] [ConInstruct: Evaluating Large Language Models on Conflict Detection and Resolution in Instructions](https://arxiv.org/abs/2511.14342)
*Xingwei He,Qianru Zhang,Pengfei Chen,Guanhua Chen,Linlin Yu,Yuan Yuan,Siu-Ming Yiu*

Main category: cs.CL

TL;DR: 该论文介绍了ConInstruct基准，用于评估大型语言模型(LLMs)识别与解决指令中冲突的能力。研究发现多数专有模型在冲突检测上表现优异，开源模型中仅DeepSeek-R1表现同样出色，但模型通常不会主动提示用户冲突或请求澄清。


<details>
  <summary>Details</summary>
Motivation: 当前研究主要评价LLMs对用户指令的遵循程度，但忽视了指令中存在冲突约束的情况，且LLMs在此类复杂提示下的行为未被充分探索。

Method: 设计并构建ConInstruct基准测试数据集，用以评估LLMs检测和解决指令冲突的能力，并通过该数据集对多款LLMs进行性能测试和行为分析。

Result: 多数专有LLMs在冲突检测上表现较强，DeepSeek-R1和Claude-4.5-Sonnet的F1分数分别为91.5%和87.3%位居榜首。但所有模型均较少主动告知用户冲突或请求澄清。

Conclusion: 当前LLMs在冲突检测方面具备较强能力，但在冲突解决时缺乏主动沟通机制，未来设计需重视提升模型在冲突场景的用户提示和澄清能力。

Abstract: Instruction-following is a critical capability of Large Language Models (LLMs). While existing works primarily focus on assessing how well LLMs adhere to user instructions, they often overlook scenarios where instructions contain conflicting constraints-a common occurrence in complex prompts. The behavior of LLMs under such conditions remains under-explored. To bridge this gap, we introduce ConInstruct, a benchmark specifically designed to assess LLMs' ability to detect and resolve conflicts within user instructions. Using this dataset, we evaluate LLMs' conflict detection performance and analyze their conflict resolution behavior. Our experiments reveal two key findings: (1) Most proprietary LLMs exhibit strong conflict detection capabilities, whereas among open-source models, only DeepSeek-R1 demonstrates similarly strong performance. DeepSeek-R1 and Claude-4.5-Sonnet achieve the highest average F1-scores at 91.5% and 87.3%, respectively, ranking first and second overall. (2) Despite their strong conflict detection abilities, LLMs rarely explicitly notify users about the conflicts or request clarification when faced with conflicting constraints. These results underscore a critical shortcoming in current LLMs and highlight an important area for future improvement when designing instruction-following LLMs.

</details>


### [25] [The Tokenization Bottleneck: How Vocabulary Extension Improves Chemistry Representation Learning in Pretrained Language Models](https://arxiv.org/abs/2511.14365)
*Prathamesh Kalamkar,Ned Letcher,Meissane Chami,Sahger Lad,Shayan Mohanty,Prasanna Pendse*

Main category: cs.CL

TL;DR: 该论文提出了一种将自然语言与分子结构统一表示的方法，通过扩展预训练大型语言模型的词汇并在化学领域文本上继续预训练，有效解决了化学表征的分词瓶颈，提升了下游化学任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在处理化学分子表示如SMILES时，由于词汇分词器面向通用文本，导致化学信息被断裂成无意义的子词，影响模型的表现。

Method: 该研究通过扩充预训练模型的词汇表，加入化学相关的关键子词，并在化学领域文本上继续预训练模型，使其能够统一理解自然语言和分子结构。

Result: 实验结果显示，所提方法在多项化学相关任务中表现优越，验证了词汇扩展和领域继续预训练的有效性。

Conclusion: 通过统一自然语言与分子结构的表示并优化词汇表，该方法有效解决了化学领域的大型语言模型分词瓶颈，显著提升了模型性能。

Abstract: The application of large language models (LLMs) to chemistry is frequently hampered by a "tokenization bottleneck", where tokenizers tuned on general-domain text tend to fragment chemical representations such as SMILES into semantically uninformative sub-tokens. This paper introduces a principled methodology to resolve this bottleneck by unifying the representation of natural language and molecular structures within a single model. Our approach involves targeted vocabulary extension-augmenting a pretrained LLM's vocabulary with chemically salient tokens, followed by continued pretraining on chemistry-domain text to integrate this new knowledge. We provide an empirical demonstration of the effectiveness of this strategy, showing that our methodology leads to superior performance on a range of downstream chemical tasks.

</details>


### [26] [ATLAS: A High-Difficulty, Multidisciplinary Benchmark for Frontier Scientific Reasoning](https://arxiv.org/abs/2511.14366)
*Hongwei Liu,Junnan Liu,Shudong Liu,Haodong Duan,Yuqiang Li,Mao Su,Xiaohong Liu,Guangtao Zhai,Xinyu Fang,Qianhong Ma,Taolin Zhang,Zihan Ma,Yufeng Zhao,Peiheng Zhou,Linchen Xiao,Wenlong Zhang,Shijie Zhou,Xingjian Ma,Siqi Sun,Jiaye Ge,Meng Li,Yuhong Liu,Jianxin Dong,Jiaying Li,Hui Wu,Hanwen Liang,Jintai Lin,Yanting Wang,Jie Dong,Tong Zhu,Tianfan Fu,Conghui He,Qi Zhang,Songyang Zhang,Lei Bai,Kai Chen*

Main category: cs.CL

TL;DR: 本文提出了ATLAS，一个由约800个原创高难度跨学科科学问题组成的大规模评估套件，旨在区别先进大型语言模型的科学推理能力，解决现有基准测试的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在众多基准测试中表现趋于饱和，且高难度测试往往学科单一、答案形式简单、易受数据污染，难以真实反映模型在科学探索中的推理能力。

Method: ATLAS由领域专家设计，涵盖数学、物理、化学、生物、计算机科学、地球科学和材料科学七大领域，问题原创性高、抗数据污染，注重跨学科知识整合和复杂多步推理，配备多阶段专家审查和对抗式测试，答案采用LaTeX格式，采用大型语言模型评审团自动评估复杂答案。

Result: 初步结果表明，ATLAS能够有效区分领先模型的高级科学推理能力。

Conclusion: ATLAS为衡量通用人工智能科研进展提供了一个开放、长期、社区驱动的高质量跨学科评测平台。

Abstract: The rapid advancement of Large Language Models (LLMs) has led to performance saturation on many established benchmarks, questioning their ability to distinguish frontier models. Concurrently, existing high-difficulty benchmarks often suffer from narrow disciplinary focus, oversimplified answer formats, and vulnerability to data contamination, creating a fidelity gap with real-world scientific inquiry. To address these challenges, we introduce ATLAS (AGI-Oriented Testbed for Logical Application in Science), a large-scale, high-difficulty, and cross-disciplinary evaluation suite composed of approximately 800 original problems. Developed by domain experts (PhD-level and above), ATLAS spans seven core scientific fields: mathematics, physics, chemistry, biology, computer science, earth science, and materials science. Its key features include: (1) High Originality and Contamination Resistance, with all questions newly created or substantially adapted to prevent test data leakage; (2) Cross-Disciplinary Focus, designed to assess models' ability to integrate knowledge and reason across scientific domains; (3) High-Fidelity Answers, prioritizing complex, open-ended answers involving multi-step reasoning and LaTeX-formatted expressions over simple multiple-choice questions; and (4) Rigorous Quality Control, employing a multi-stage process of expert peer review and adversarial testing to ensure question difficulty, scientific value, and correctness. We also propose a robust evaluation paradigm using a panel of LLM judges for automated, nuanced assessment of complex answers. Preliminary results on leading models demonstrate ATLAS's effectiveness in differentiating their advanced scientific reasoning capabilities. We plan to develop ATLAS into a long-term, open, community-driven platform to provide a reliable "ruler" for progress toward Artificial General Intelligence.

</details>


### [27] [Mitigating Label Length Bias in Large Language Models](https://arxiv.org/abs/2511.14385)
*Mario Sanz-Guerrero,Katharina von der Wense*

Main category: cs.CL

TL;DR: 本文提出了一种名为规范化上下文校准（NCC）的方法，用于解决大型语言模型在多词标签分类任务中存在的标签长度偏差问题，显著提升了模型的性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多选分类任务中存在标签长度偏差问题，现有校准方法未能有效处理多词类标签中的偏差，导致预测不一致和性能下降。

Method: 提出了规范化上下文校准（NCC）方法，在整个标签级别进行归一化和校准，以消除标签长度偏差。

Result: NCC在多个数据集和模型上相比现有方法显著提升性能，F1分数提升高达10%；在多项选择题等更广泛任务中也表现出良好的偏差缓解效果。

Conclusion: 通过缓解多词标签的全标签偏差，NCC方法提高了大型语言模型的性能和鲁棒性，尤其在真实应用中更加稳定和可靠。

Abstract: Large language models (LLMs) are powerful zero- and few-shot learners. However, when predicting over a set of candidate options, LLMs suffer from label biases, and existing calibration methods overlook biases arising from multi-token class labels. We tackle an issue we call label length bias, where labels of different lengths are treated inconsistently, even after standard length normalization. To mitigate it, we propose normalized contextual calibration (NCC), an effective method that normalizes and calibrates predictions at the full-label level. NCC achieves statistically significant improvements over prior approaches across multiple datasets and models, with gains of up to 10% F1. Moreover, NCC extends bias mitigation to broader tasks such as multiple-choice question answering. Our analysis shows that, when combined with in-context learning, NCC is less sensitive to few-shot example selection, requires fewer examples for competitive performance, and produces more reliable confidence estimates. These findings highlight the importance of mitigating full-label biases to improve the performance and robustness of LLM-based methods, particularly in real-world applications where class labels naturally consist of multiple tokens.

</details>


### [28] [Unified Defense for Large Language Models against Jailbreak and Fine-Tuning Attacks in Education](https://arxiv.org/abs/2511.14423)
*Xin Yi,Yue Li,Dongsheng Shi,Linlin Wang,Xiaoling Wang,Liang He*

Main category: cs.CL

TL;DR: 本文针对教育场景中大语言模型（LLMs）的安全性问题，构建了专门的EduHarm基准数据集并提出了三阶段防护框架（TSSF），有效抵御越狱和微调攻击，保障模型输出安全性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注通用的安全评估，缺乏针对教育场景中LLMs的独特安全需求的研究。教育应用中的LLMs易受越狱和微调攻击，可能导致有害输出，亟需专门的安全防护方案。

Method: 构建包含五类教育场景的安全与不安全指令对的EduHarm基准，设计三阶段防护框架TSSF：1）安全感知注意力重新校准，突出关键不安全标记；2）逐层安全判断，聚合多层安全信号识别有害输入；3）双路由防护机制，分别处理安全和不安全查询。

Result: 在八种越狱攻击策略下，TSSF显著提升安全防护效果，同时避免无差别拒绝良性请求。对三种微调攻击数据集的评估表明，TSSF稳健防御有害查询且维持了良性微调带来的性能提升。

Conclusion: 提出的EduHarm基准和三阶段防护框架TSSF有效提升了教育场景中LLMs的安全性，为相关应用的安全部署提供了实用方案。

Abstract: Large Language Models (LLMs) are increasingly integrated into educational applications. However, they remain vulnerable to jailbreak and fine-tuning attacks, which can compromise safety alignment and lead to harmful outputs. Existing studies mainly focus on general safety evaluations, with limited attention to the unique safety requirements of educational scenarios. To address this gap, we construct EduHarm, a benchmark containing safe-unsafe instruction pairs across five representative educational scenarios, enabling systematic safety evaluation of educational LLMs. Furthermore, we propose a three-stage shield framework (TSSF) for educational LLMs that simultaneously mitigates both jailbreak and fine-tuning attacks. First, safety-aware attention realignment redirects attention toward critical unsafe tokens, thereby restoring the harmfulness feature that discriminates between unsafe and safe inputs. Second, layer-wise safety judgment identifies harmfulness features by aggregating safety cues across multiple layers to detect unsafe instructions. Finally, defense-driven dual routing separates safe and unsafe queries, ensuring normal processing for benign inputs and guarded responses for harmful ones. Extensive experiments across eight jailbreak attack strategies demonstrate that TSSF effectively strengthens safety while preventing over-refusal of benign queries. Evaluations on three fine-tuning attack datasets further show that it consistently achieves robust defense against harmful queries while maintaining preserving utility gains from benign fine-tuning.

</details>


### [29] [MedBench v4: A Robust and Scalable Benchmark for Evaluating Chinese Medical Language Models, Multimodal Models, and Intelligent Agents](https://arxiv.org/abs/2511.14439)
*Jinru Ding,Lu Lu,Chao Ding,Mouxiao Bian,Jiayuan Chen,Renjie Lu,Wenrao Pang,Xiaoqin Wu,Zhiqiang Liu,Luyi Jiang,Bing Han,Yunqiu Wang,Jie Xu*

Main category: cs.CL

TL;DR: MedBench v4 是一个覆盖广泛医学任务的云端评测平台，评估了15款先进医学大模型，发现基础模型在安全伦理方面表现较差，多模态模型推理能力不足，智能体模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 当前医学大语言模型和多模态模型快速发展，迫切需要反映真实临床流程与安全约束的评测框架。

Method: 构建包含70万+专家策划任务的MedBench v4，通过多阶段多轮临床专家审校，并采用经过校准的LLM作为评判，评测基础模型、多模态模型及智能体模型的表现。

Result: 基础模型平均得分54.1/100，安全伦理得分低至18.4/100；多模态模型得分更低且跨模态推理较弱；智能体模型得分最高，表现最优。

Conclusion: 基础模型仍存在多模态推理和安全方面不足，智能体模型可显著提升临床应用准备度，MedBench v4为医院、开发者和监管机构提供重要的医学AI评测参考。

Abstract: Recent advances in medical large language models (LLMs), multimodal models, and agents demand evaluation frameworks that reflect real clinical workflows and safety constraints. We present MedBench v4, a nationwide, cloud-based benchmarking infrastructure comprising over 700,000 expert-curated tasks spanning 24 primary and 91 secondary specialties, with dedicated tracks for LLMs, multimodal models, and agents. Items undergo multi-stage refinement and multi-round review by clinicians from more than 500 institutions, and open-ended responses are scored by an LLM-as-a-judge calibrated to human ratings. We evaluate 15 frontier models. Base LLMs reach a mean overall score of 54.1/100 (best: Claude Sonnet 4.5, 62.5/100), but safety and ethics remain low (18.4/100). Multimodal models perform worse overall (mean 47.5/100; best: GPT-5, 54.9/100), with solid perception yet weaker cross-modal reasoning. Agents built on the same backbones substantially improve end-to-end performance (mean 79.8/100), with Claude Sonnet 4.5-based agents achieving up to 85.3/100 overall and 88.9/100 on safety tasks. MedBench v4 thus reveals persisting gaps in multimodal reasoning and safety for base models, while showing that governance-aware agentic orchestration can markedly enhance benchmarked clinical readiness without sacrificing capability. By aligning tasks with Chinese clinical guidelines and regulatory priorities, the platform offers a practical reference for hospitals, developers, and policymakers auditing medical AI.

</details>


### [30] [Tell Me: An LLM-powered Mental Well-being Assistant with RAG, Synthetic Dialogue Generation, and Agentic Planning](https://arxiv.org/abs/2511.14445)
*Trishala Jayesh Ahalpara*

Main category: cs.CL

TL;DR: 本论文提出了Tell Me心理健康系统，结合大型语言模型，通过个性化对话、合成疗法对话生成和动态自我关怀计划，提升心理支持的可达性和效果。


<details>
  <summary>Details</summary>
Motivation: 当前心理健康支持面临专业资源不足和数据隐私限制，迫切需要利用AI技术提供更易获取、个性化且安全的心理辅助工具。

Method: 系统集成了三部分：(1)结合知识检索的个性化对话生成助手；(2)基于客户档案的合成客户-治疗师对话生成，用于数据扩充和语言研究；(3)基于CrewAI的每周自我关怀计划和引导冥想音频生成。

Result: 系统在心理健康场景中展现了较好的人机交互能力，提升了用户情绪反思空间。通过自动和人工评估，验证了RAG对话助手的性能和用户满意度。

Conclusion: Tell Me系统证明了结合大型语言模型和多模块设计，可以有效降低心理支持门槛，补充传统治疗资源，推动NLP与心理健康领域的跨学科合作，实现负责任的人机交互创新。

Abstract: We present Tell Me, a mental well-being system that leverages advances in large language models to provide accessible, context-aware support for users and researchers. The system integrates three components: (i) a retrieval-augmented generation (RAG) assistant for personalized, knowledge-grounded dialogue; (ii) a synthetic client-therapist dialogue generator conditioned on client profiles to facilitate research on therapeutic language and data augmentation; and (iii) a Well-being AI crew, implemented with CrewAI, that produces weekly self-care plans and guided meditation audio. The system is designed as a reflective space for emotional processing rather than a substitute for professional therapy. It illustrates how conversational assistants can lower barriers to support, complement existing care, and broaden access to mental health resources. To address the shortage of confidential therapeutic data, we introduce synthetic client-therapist dialogue generation conditioned on client profiles. Finally, the planner demonstrates an innovative agentic workflow for dynamically adaptive, personalized self-care, bridging the limitations of static well-being tools. We describe the architecture, demonstrate its functionalities, and report evaluation of the RAG assistant in curated well-being scenarios using both automatic LLM-based judgments and a human-user study. This work highlights opportunities for interdisciplinary collaboration between NLP researchers and mental health professionals to advance responsible innovation in human-AI interaction for well-being.

</details>


### [31] [Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning](https://arxiv.org/abs/2511.14460)
*Mingyue Cheng,Jie Ouyang,Shuo Yu,Ruiran Yan,Yucong Luo,Zirui Liu,Daoyu Wang,Qi Liu,Enhong Chen*

Main category: cs.CL

TL;DR: 本文探讨了强化学习在大语言模型（LLM）智能体中的应用，提出了扩展的马尔可夫决策过程框架及一个灵活的训练框架Agent-R1，验证了其在多跳问答任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习应用于LLM智能体尚处于初期阶段，缺乏针对LLM智能体的强化学习方法及灵活易扩展的训练框架。

Method: 通过系统扩展马尔可夫决策过程框架定义LLM智能体关键组成部分，设计了模块化、灵活且用户友好的Agent-R1训练框架以适应不同任务和环境。

Result: 在多跳问答基准任务上的实验结果初步验证了所提方法和框架的有效性。

Conclusion: 提出的方法和训练框架为强化学习应用于LLM智能体提供了理论基础和实践工具，有助于推动该领域的发展。

Abstract: Large Language Models (LLMs) are increasingly being explored for building Agents capable of active environmental interaction (e.g., via tool use) to solve complex problems. Reinforcement Learning (RL) is considered a key technology with significant potential for training such Agents; however, the effective application of RL to LLM Agents is still in its nascent stages and faces considerable challenges. Currently, this emerging field lacks in-depth exploration into RL approaches specifically tailored for the LLM Agent context, alongside a scarcity of flexible and easily extensible training frameworks designed for this purpose. To help advance this area, this paper first revisits and clarifies Reinforcement Learning methodologies for LLM Agents by systematically extending the Markov Decision Process (MDP) framework to comprehensively define the key components of an LLM Agent. Secondly, we introduce Agent-R1, a modular, flexible, and user-friendly training framework for RL-based LLM Agents, designed for straightforward adaptation across diverse task scenarios and interactive environments. We conducted experiments on Multihop QA benchmark tasks, providing initial validation for the effectiveness of our proposed methods and framework.

</details>


### [32] [LiveRAG: A diverse Q&A dataset with varying difficulty level for RAG evaluation](https://arxiv.org/abs/2511.14531)
*David Carmel,Simone Filice,Guy Horowitz,Yoelle Maarek,Alex Shtoff,Oren Somekh,Ran Tavory*

Main category: cs.CL

TL;DR: 本文介绍了LiveRAG基准测试集，一个包含895个合成问答的数据集，用于系统评估基于检索增强生成（RAG）模型的问答系统。


<details>
  <summary>Details</summary>
Motivation: 随着RAG技术在生成式AI中的应用日益突出，需要一个系统化的评估工具来衡量其效果。

Method: 设计了LiveRAG基准测试集，基于SIGIR'2025挑战赛使用的数据，附加了真实答案及其支持性陈述，并通过项目反应理论模型评估问题难度和区分度。

Result: 该基准测试集涵盖问题多样性、广泛难度分布，能有效区分不同系统的能力。

Conclusion: LiveRAG基准测试集将推动RAG技术研究，支持系统评测，促进更鲁棒的问答系统开发。

Abstract: With Retrieval Augmented Generation (RAG) becoming more and more prominent in generative AI solutions, there is an emerging need for systematically evaluating their effectiveness. We introduce the LiveRAG benchmark, a publicly available dataset of 895 synthetic questions and answers designed to support systematic evaluation of RAG-based Q&A systems. This synthetic benchmark is derived from the one used during the SIGIR'2025 LiveRAG Challenge, where competitors were evaluated under strict time constraints. It is augmented with information that was not made available to competitors during the Challenge, such as the ground-truth answers, together with their associated supporting claims which were used for evaluating competitors' answers. In addition, each question is associated with estimated difficulty and discriminability scores, derived from applying an Item Response Theory model to competitors' responses. Our analysis highlights the benchmark's questions diversity, the wide range of their difficulty levels, and their usefulness in differentiating between system capabilities. The LiveRAG benchmark will hopefully help the community advance RAG research, conduct systematic evaluation, and develop more robust Q&A systems.

</details>


### [33] [Examining the Metrics for Document-Level Claim Extraction in Czech and Slovak](https://arxiv.org/abs/2511.14566)
*Lucia Makaiová,Martin Fajčík,Antonín Jarolím*

Main category: cs.CL

TL;DR: 本文探讨了文档级声明提取的评估方法，通过对比模型提取和人工注释的声明集，提出了一种基于对齐分数的评估框架，并在捷克语和斯洛伐克语新闻评论数据集上进行了实验，揭示了现有方法的不足。


<details>
  <summary>Details</summary>
Motivation: 文档级声明提取难度大，且对提取结果的评估方法研究有限，缺乏可靠的评估框架。

Method: 通过对齐两个声明集合并计算相似性分数，实现最佳对齐和评估方法，用以比较模型提取和人工注释的声明，衡量模型性能和标注员一致性。

Result: 在捷克语和斯洛伐克语数据集上的实验表明，现有评估方法难以准确反映声明的语义相似性及其核心属性，如原子性、可检验性和去上下文化。

Conclusion: 当前评估方法存在局限，未来需开发更先进的技术以准确捕捉语义相似性和重要声明属性，提升文档级声明提取的评估效果。

Abstract: Document-level claim extraction remains an open challenge in the field of fact-checking, and subsequently, methods for evaluating extracted claims have received limited attention. In this work, we explore approaches to aligning two sets of claims pertaining to the same source document and computing their similarity through an alignment score. We investigate techniques to identify the best possible alignment and evaluation method between claim sets, with the aim of providing a reliable evaluation framework. Our approach enables comparison between model-extracted and human-annotated claim sets, serving as a metric for assessing the extraction performance of models and also as a possible measure of inter-annotator agreement. We conduct experiments on newly collected dataset-claims extracted from comments under Czech and Slovak news articles-domains that pose additional challenges due to the informal language, strong local context, and subtleties of these closely related languages. The results draw attention to the limitations of current evaluation approaches when applied to document-level claim extraction and highlight the need for more advanced methods-ones able to correctly capture semantic similarity and evaluate essential claim properties such as atomicity, checkworthiness, and decontextualization.

</details>


### [34] [Leveraging Digitized Newspapers to Collect Summarization Data in Low-Resource Languages](https://arxiv.org/abs/2511.14598)
*Noam Dahan,Omer Kidron,Gabriel Stanovsky*

Main category: cs.CL

TL;DR: 本文提出通过报纸头版摘要收集自然摘要数据的方法，构建了多语种多文档摘要数据集HEBTEASESUM。


<details>
  <summary>Details</summary>
Motivation: 针对低资源语言高质量摘要数据稀缺，利用历史报纸数字化中自然存在的头版摘要作为资源。

Method: 提出通过头版编辑摘要文章的策略自动收集摘要，适用于不同语言资源水平，扩展到多语种。

Result: 验证头版摘要在七种语言中普遍存在，成功构建了希伯来语多文档摘要数据集HEBTEASESUM。

Conclusion: 基于历史报纸头版摘要的自动数据收集方法有效缓解低资源语言摘要数据缺乏问题，促进多文档摘要研究。

Abstract: High quality summarization data remains scarce in under-represented languages. However, historical newspapers, made available through recent digitization efforts, offer an abundant source of untapped, naturally annotated data. In this work, we present a novel method for collecting naturally occurring summaries via Front-Page Teasers, where editors summarize full length articles. We show that this phenomenon is common across seven diverse languages and supports multi-document summarization. To scale data collection, we develop an automatic process, suited to varying linguistic resource levels. Finally, we apply this process to a Hebrew newspaper title, producing HEBTEASESUM, the first dedicated multi-document summarization dataset in Hebrew.

</details>


### [35] [A Method for Characterizing Disease Progression from Acute Kidney Injury to Chronic Kidney Disease](https://arxiv.org/abs/2511.14603)
*Yilu Fang,Jordan G. Nestor,Casey N. Ta,Jerard Z. Kneifati-Hayek,Chunhua Weng*

Main category: cs.CL

TL;DR: 该研究通过电子健康记录动态追踪急性肾损伤患者的病情演变，识别了15种不同的临床状态及其向慢性肾病进展的风险，找到了多种影响慢性肾病发生的新旧风险因素。


<details>
  <summary>Details</summary>
Motivation: 急性肾损伤患者易发展为慢性肾病，但难以准确识别高风险患者，亟需动态跟踪病情演变来指导早期干预。

Method: 利用电子健康记录，通过聚类分析患者的长期医学编码和肌酐值，定义临床状态；采用多状态模型估计状态转移概率和进展风险；生存分析识别不同状态下慢性肾病的风险因素。

Result: 识别出了15个不同的急性肾损伤后临床状态，其中75%的患者状态较为稳定或仅有一次转变；17%的患者最终发展为慢性肾病；确定了包括AKI严重程度、糖尿病、高血压等传统及新型风险因素，其影响因不同临床状态而异。

Conclusion: 该研究提供了一种基于数据驱动方法识别高风险急性肾损伤患者的路径，为早期诊断和干预慢性肾病提供了决策支持工具的基础。

Abstract: Patients with acute kidney injury (AKI) are at high risk of developing chronic kidney disease (CKD), but identifying those at greatest risk remains challenging. We used electronic health record (EHR) data to dynamically track AKI patients' clinical evolution and characterize AKI-to-CKD progression. Post-AKI clinical states were identified by clustering patient vectors derived from longitudinal medical codes and creatinine measurements. Transition probabilities between states and progression to CKD were estimated using multi-state modeling. After identifying common post-AKI trajectories, CKD risk factors in AKI subpopulations were identified through survival analysis. Of 20,699 patients with AKI at admission, 3,491 (17%) developed CKD. We identified fifteen distinct post-AKI states, each with different probabilities of CKD development. Most patients (75%, n=15,607) remained in a single state or made only one transition during the study period. Both established (e.g., AKI severity, diabetes, hypertension, heart failure, liver disease) and novel CKD risk factors, with their impact varying across these clinical states. This study demonstrates a data-driven approach for identifying high-risk AKI patients, supporting the development of decision-support tools for early CKD detection and intervention.

</details>


### [36] [Bridging Human and Model Perspectives: A Comparative Analysis of Political Bias Detection in News Media Using Large Language Models](https://arxiv.org/abs/2511.14606)
*Shreya Adrita Banik,Niaz Nafi Rahman,Tahsina Moiukh,Farig Sadeque*

Main category: cs.CL

TL;DR: 本文比较评估了人类标注和多种大型语言模型（如GPT、BERT、RoBERTa、FLAN）在政治偏见检测上的一致性与差异，发现RoBERTa和GPT表现最佳。


<details>
  <summary>Details</summary>
Motivation: 政治偏见检测复杂且依赖细微语言和语境，现有大型语言模型在与人类判断对齐程度方面仍未充分研究。

Method: 构建人工标注新闻数据集，评估标注一致性、偏见极性及模型间一致性，比较多种语言模型与人类标注的差异。

Result: RoBERTa在传统变换器模型中与人类标注高度一致，GPT在零样本设置下表现出最强的整体一致性，微调RoBERTa模型准确率最高。

Conclusion: 人类与大型语言模型在政治偏见感知上存在系统性差异，强调结合人类可解释性与模型可扩展性的混合评估框架的重要性。

Abstract: Detecting political bias in news media is a complex task that requires interpreting subtle linguistic and contextual cues. Although recent advances in Natural Language Processing (NLP) have enabled automatic bias classification, the extent to which large language models (LLMs) align with human judgment still remains relatively underexplored and not yet well understood. This study aims to present a comparative framework for evaluating the detection of political bias across human annotations and multiple LLMs, including GPT, BERT, RoBERTa, and FLAN. We construct a manually annotated dataset of news articles and assess annotation consistency, bias polarity, and inter-model agreement to quantify divergence between human and model perceptions of bias. Experimental results show that among traditional transformer-based models, RoBERTa achieves the highest alignment with human labels, whereas generative models such as GPT demonstrate the strongest overall agreement with human annotations in a zero-shot setting. Among all transformer-based baselines, our fine-tuned RoBERTa model acquired the highest accuracy and the strongest alignment with human-annotated labels. Our findings highlight systematic differences in how humans and LLMs perceive political slant, underscoring the need for hybrid evaluation frameworks that combine human interpretability with model scalability in automated media bias detection.

</details>


### [37] [A Specialized Large Language Model for Clinical Reasoning and Diagnosis in Rare Diseases](https://arxiv.org/abs/2511.14638)
*Tao Yang,Dandan Huang,Yunting Lin,Pengfei Wu,Zhikun Wu,Gangyuan Ma,Yulan Lu,Xinran Dong,Dingpeng Li,Junshuang Ge,Zhiyan Zhang,Xuanzhao Huang,Wenyan Nong,Yao Zhou,Hui Tang,Hongxi Yang,Shijie Zhang,Juan Li,Xiaojun Cao,Lin Yang,Xia Gao,Kaishou Xu,Xiaoqiong Gu,Wen Zhang,Huimin Xia,Li Liu,Wenhao Zhou,Mulin Jun Li*

Main category: cs.CL

TL;DR: 本文提出了一种针对罕见病诊断的先进语言模型RareSeek R1，通过领域专属临床语料库和链式推理学习，实现了准确且稳定的诊断支持。


<details>
  <summary>Details</summary>
Motivation: 罕见病诊断周期长，传统方法在信息抽取和推理阶段存在脱节，且大模型受限于真实电子健康记录数据不足和知识时效性问题。

Method: 构建大型专业临床语料库和医生验证的推理集，通过分阶段指令调优、链式思维学习以及图形检索技术开发RareSeek R1模型。

Result: RareSeek R1在多中心电子健康记录和公开基准测试中表现出最先进的准确性、良好的泛化能力和噪声稳定性，辅助诊断效果与经验丰富医生相当。

Conclusion: 提出的结合叙述和知识推理模式显著缩短了诊断时间，提升了诊断透明性和临床可用性，实现了可审计的决策支持。

Abstract: Rare diseases affect hundreds of millions worldwide, yet diagnosis often spans years. Convectional pipelines decouple noisy evidence extraction from downstream inferential diagnosis, and general/medical large language models (LLMs) face scarce real world electronic health records (EHRs), stale domain knowledge, and hallucinations. We assemble a large, domain specialized clinical corpus and a clinician validated reasoning set, and develop RareSeek R1 via staged instruction tuning, chain of thought learning, and graph grounded retrieval. Across multicenter EHR narratives and public benchmarks, RareSeek R1 attains state of the art accuracy, robust generalization, and stability under noisy or overlapping phenotypes. Augmented retrieval yields the largest gains when narratives pair with prioritized variants by resolving ambiguity and aligning candidates to mechanisms. Human studies show performance on par with experienced physicians and consistent gains in assistive use. Notably, transparent reasoning highlights decisive non phenotypic evidence (median 23.1%, such as imaging, interventions, functional tests) underpinning many correct diagnoses. This work advances a narrative first, knowledge integrated reasoning paradigm that shortens the diagnostic odyssey and enables auditable, clinically translatable decision support.

</details>


### [38] [Graded strength of comparative illusions is explained by Bayesian inference](https://arxiv.org/abs/2511.14642)
*Yuhan Zhang,Erxiao Wang,Cory Shain*

Main category: cs.CL

TL;DR: 本文研究了语言处理中的比较错觉现象，并提出基于噪音通道的贝叶斯推断模型来解释该现象。


<details>
  <summary>Details</summary>
Motivation: 比较错觉（comparative illusion）现象表现为人们误判句子虽不合逻辑却可接受，之前的研究仅用狭义解释支持噪音通道理论，本研究欲进一步验证并扩展该理论。

Method: 通过融合统计语言模型与人类行为数据，建立定量模型预测对各种解释的后验概率，从而度量比较错觉的强度，并探究代词与完整名词短语之间的差异。

Result: 模型成功解释了比较错觉强度的细微差别及代词与名词短语造成的未解释效应，支持了噪音通道推断理论对该现象的预测。

Conclusion: 研究结果支持噪音通道理论作为语言理解的统一计算模型，能够解释多种语言处理中的错觉与非错觉现象。

Abstract: Like visual processing, language processing is susceptible to illusions in which people systematically misperceive stimuli. In one such case--the comparative illusion (CI), e.g., More students have been to Russia than I have--comprehenders tend to judge the sentence as acceptable despite its underlying nonsensical comparison. Prior research has argued that this phenomenon can be explained as Bayesian inference over a noisy channel: the posterior probability of an interpretation of a sentence is proportional to both the prior probability of that interpretation and the likelihood of corruption into the observed (CI) sentence. Initial behavioral work has supported this claim by evaluating a narrow set of alternative interpretations of CI sentences and showing that comprehenders favor interpretations that are more likely to have been corrupted into the illusory sentence. In this study, we replicate and go substantially beyond this earlier work by directly predicting the strength of illusion with a quantitative model of the posterior probability of plausible interpretations, which we derive through a novel synthesis of statistical language models with human behavioral data. Our model explains not only the fine gradations in the strength of CI effects, but also a previously unexplained effect caused by pronominal vs. full noun phrase than-clause subjects. These findings support a noisy-channel theory of sentence comprehension by demonstrating that the theory makes novel predictions about the comparative illusion that bear out empirically. This outcome joins related evidence of noisy channel processing in both illusory and non-illusory contexts to support noisy channel inference as a unified computational-level theory of diverse language processing phenomena.

</details>


### [39] [Bias in, Bias out: Annotation Bias in Multilingual Large Language Models](https://arxiv.org/abs/2511.14662)
*Xia Cui,Ziyi Huang,Naeemeh Adel*

Main category: cs.CL

TL;DR: 本文提出了一个针对多语言大语言模型（LLMs）标注偏见的综合框架，涵盖偏见类型、检测方法及缓解策略，旨在提升注释过程的公平性和文化适应性。


<details>
  <summary>Details</summary>
Motivation: 多语言大语言模型在文化多样性环境下开发时，标注偏见（包括任务框架、标注者主观性及文化不匹配）严重影响模型输出，带来社会危害，因此需要系统理解和解决标注偏见。

Method: 本文区分了指令偏见、标注者偏见和文化偏见，综述了检测偏见的方法如评注者一致性、模型分歧和元数据分析，提出多语言模型分歧和文化推断等新技术，且介绍多样化标注者招募、指南迭代与模型后期调整的缓解策略。

Result: 文章贡献包括标注偏见类型学、检测指标综述、多语言环境下的集成偏见缓解方法以及标注过程的伦理分析。

Conclusion: 该研究为多语言大语言模型构建更公平且文化基础扎实的标注流程提供理论和方法指导，有助于减少标注偏见带来的负面影响。

Abstract: Annotation bias in NLP datasets remains a major challenge for developing multilingual Large Language Models (LLMs), particularly in culturally diverse settings. Bias from task framing, annotator subjectivity, and cultural mismatches can distort model outputs and exacerbate social harms. We propose a comprehensive framework for understanding annotation bias, distinguishing among instruction bias, annotator bias, and contextual and cultural bias. We review detection methods (including inter-annotator agreement, model disagreement, and metadata analysis) and highlight emerging techniques such as multilingual model divergence and cultural inference. We further outline proactive and reactive mitigation strategies, including diverse annotator recruitment, iterative guideline refinement, and post-hoc model adjustments. Our contributions include: (1) a typology of annotation bias; (2) a synthesis of detection metrics; (3) an ensemble-based bias mitigation approach adapted for multilingual settings, and (4) an ethical analysis of annotation processes. Together, these insights aim to inform more equitable and culturally grounded annotation pipelines for LLMs.

</details>


### [40] [Streamlining Industrial Contract Management with Retrieval-Augmented LLMs](https://arxiv.org/abs/2511.14671)
*Kristi Topollai,Tolga Dimlioglu,Anna Choromanska,Simon Odie,Reginald Hui*

Main category: cs.CL

TL;DR: 本文提出了一个基于检索增强生成（RAG）技术的模块化合同管理框架，通过合成数据生成、语义条款检索、可接受性分类和基于奖励的对齐，自动检测并优化问题条款修订，实现了超过80%的准确率。


<details>
  <summary>Details</summary>
Motivation: 合同管理过程繁琐且复杂，涉及不断的条款修订，且标注数据稀缺、历史合同数据无结构化，自动化这一流程存在较大挑战。

Method: 构建了一个模块化框架，集成了合成数据生成、语义条款检索、修订接受性分类和奖励对齐机制，形成基于检索增强生成的流水线来识别并改进问题修订。

Result: 系统在与产业合作伙伴的合作下开发并评估，识别及优化问题修订的准确率均超过80%，表现出良好的低资源下的实际应用性能。

Conclusion: 该方法有效加速了合同修订工作流程，具有实际应用价值，尤其适合资源稀缺的真实场景。

Abstract: Contract management involves reviewing and negotiating provisions, individual clauses that define rights, obligations, and terms of agreement. During this process, revisions to provisions are proposed and iteratively refined, some of which may be problematic or unacceptable. Automating this workflow is challenging due to the scarcity of labeled data and the abundance of unstructured legacy contracts. In this paper, we present a modular framework designed to streamline contract management through a retrieval-augmented generation (RAG) pipeline. Our system integrates synthetic data generation, semantic clause retrieval, acceptability classification, and reward-based alignment to flag problematic revisions and generate improved alternatives. Developed and evaluated in collaboration with an industry partner, our system achieves over 80% accuracy in both identifying and optimizing problematic revisions, demonstrating strong performance under real-world, low-resource conditions and offering a practical means of accelerating contract revision workflows.

</details>


### [41] [Quadratic Term Correction on Heaps' Law](https://arxiv.org/abs/2511.14683)
*Oscar Fontanelli,Wentian Li*

Main category: cs.CL

TL;DR: 这篇论文发现传统的Heaps定律在log-log图中仍有轻微的凹性，提出了用二次函数更好地拟合词型-词表关系。


<details>
  <summary>Details</summary>
Motivation: 观察到即使在对数尺度下，词型-词表关系曲线仍有凹性，传统幂律模型不完全适用。

Method: 通过分析20部英文小说的词型-词表数据，在log-log尺度下使用 一次和二次回归拟合，结合‘带放回抽取彩球模型’解释曲率现象。

Result: 回归分析显示，线性系数略大于1，二次项系数约为-0.02，表明词型-词表关系在log-log坐标系下存在轻微的下凹现象。

Conclusion: 提出二次函数模型及伪方差概念更准确描述词型-词表关系，尤其在样本较小时对曲率有较好估计，补充了传统Heaps定律的不足。

Abstract: Heaps' or Herdan's law characterizes the word-type vs. word-token relation by a power-law function, which is concave in linear-linear scale but a straight line in log-log scale. However, it has been observed that even in log-log scale, the type-token curve is still slightly concave, invalidating the power-law relation. At the next-order approximation, we have shown, by twenty English novels or writings (some are translated from another language to English), that quadratic functions in log-log scale fit the type-token data perfectly. Regression analyses of log(type)-log(token) data with both a linear and quadratic term consistently lead to a linear coefficient of slightly larger than 1, and a quadratic coefficient around -0.02. Using the ``random drawing colored ball from the bag with replacement" model, we have shown that the curvature of the log-log scale is identical to a ``pseudo-variance" which is negative. Although a pseudo-variance calculation may encounter numeric instability when the number of tokens is large, due to the large values of pseudo-weights, this formalism provides a rough estimation of the curvature when the number of tokens is small.

</details>


### [42] [SMRC: Aligning Large Language Models with Student Reasoning for Mathematical Error Correction](https://arxiv.org/abs/2511.14684)
*Biaojie Zeng,Min Zhang,Juan Zhou,Fengrui Liu,Ruiyang Huang,Xin Lin*

Main category: cs.CL

TL;DR: 本文提出了SMRC方法，用蒙特卡洛树搜索和宽度优先搜索结合LLM，实现对学生数学推理过程的系统纠错，大幅提升纠错准确性和教学适用性，并建立了对应的高中文科数学基准MSEB。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型数学推理错误检测多侧重模型自我修正，缺乏类似教师式的系统引导与纠正，难以满足教育场景需求。

Method: 将学生推理建模为多步骤序列决策问题，结合蒙特卡洛树搜索探索最优纠正路径，利用LLM引导的宽度优先搜索与答案评估生成奖励信号，通过反向传播实现过程级细粒度监督。

Result: 在两个公开数据集及自建MSEB数据集上，SMRC在解题准确率和正确步骤保留方面显著优于现有方法，表现出较强的纠错和教学适用性。

Conclusion: SMRC有效弥补了现有方法在教育应用中的不足，展现出引导学生系统纠错的潜力，并推动了数学推理自动纠错的研究前沿。

Abstract: Large language models (LLMs) often make reasoning errors when solving mathematical problems, and how to automatically detect and correct these errors has become an important research direction. However, existing approaches \textit{mainly focus on self-correction within the model}, which falls short of the ``teacher-style`` correction required in educational settings, \textit{i.e.}, systematically guiding and revising a student's problem-solving process. To address this gap, we propose \texttt{SMRC} (\textit{\underline{S}tudent \underline{M}athematical \underline{R}easoning \underline{C}orrection}), a novel method that aligns LLMs with student reasoning. Specifically, \texttt{SMRC} formulates student reasoning as a multi-step sequential decision problem and introduces Monte Carlo Tree Search (MCTS) to explore optimal correction paths. To reduce the cost of the annotating process-level rewards, we leverage breadth-first search (BFS) guided by LLMs and final-answer evaluation to generate reward signals, which are then distributed across intermediate reasoning steps via a back-propagation mechanism, enabling fine-grained process supervision. Additionally, we construct a benchmark for high school mathematics, MSEB (Multi-Solution Error Benchmark), consisting of 158 instances that include problem statements, student solutions, and correct reasoning steps. We further propose a dual evaluation protocol centered on \textbf{solution accuracy} and \textbf{correct-step retention}, offering a comprehensive measure of educational applicability. Experiments demonstrate that \texttt{SMRC} significantly outperforms existing methods on two public datasets (ProcessBench and MR-GSM8K) and our MSEB in terms of effectiveness and overall performance. The code and data are available at https://github.com/Mind-Lab-ECNU/SMRC.

</details>


### [43] [Encoding and Understanding Astrophysical Information in Large Language Model-Generated Summaries](https://arxiv.org/abs/2511.14685)
*Kiera McCormick,Rafael Martínez-Galarza*

Main category: cs.CL

TL;DR: 本文探讨大语言模型（LLM）嵌入是否能编码物理测量中的物理统计量，重点研究提示作用和语言特征对编码的影响。


<details>
  <summary>Details</summary>
Motivation: 大语言模型具有跨领域、跨模态的泛化和上下文学习能力，能否用于编码通常仅来自科学测量的物理信息尚不明确。

Method: 以天体物理为例，使用稀疏自编码器从文本中提取可解释特征，分析提示对编码的影响以及语言中哪些部分对物理信息编码最重要。

Result: 研究展示了提示和语言特征对LLM编码物理统计量具有显著影响，且通过稀疏自编码器可提取与物理测量相关的文本特征。

Conclusion: LLM嵌入能够一定程度上编码科学测量的物理信息，提示设计和语言形式是关键因素，为未来利用LLM处理科学文本提供了新思路。

Abstract: Large Language Models have demonstrated the ability to generalize well at many levels across domains, modalities, and even shown in-context learning capabilities. This enables research questions regarding how they can be used to encode physical information that is usually only available from scientific measurements, and loosely encoded in textual descriptions. Using astrophysics as a test bed, we investigate if LLM embeddings can codify physical summary statistics that are obtained from scientific measurements through two main questions: 1) Does prompting play a role on how those quantities are codified by the LLM? and 2) What aspects of language are most important in encoding the physics represented by the measurement? We investigate this using sparse autoencoders that extract interpretable features from the text.

</details>


### [44] [Ground Truth Generation for Multilingual Historical NLP using LLMs](https://arxiv.org/abs/2511.14688)
*Clovis Gladstone,Zhao Fang,Spencer Dean Stewart*

Main category: cs.CL

TL;DR: 通过利用大型语言模型生成的标注数据，微调spaCy，实现了对历史法语和中文文本的词性标注、词元化和命名实体识别的性能提升。


<details>
  <summary>Details</summary>
Motivation: 历史和低资源的自然语言处理面临标注数据不足和领域差异的问题，尤其是历史文本和现代语料库之间存在领域不匹配。

Method: 采用大型语言模型生成地面真值标注，利用这些合成数据对spaCy进行微调，提升特定历史时期文本的NLP性能。

Result: 在词性标注、词元化和命名实体识别等任务上，经过微调的模型在历史法语和中文文本的特定测试集上表现显著提升。

Conclusion: 领域特定的模型对于历史文本处理至关重要，甚至有限的合成标注数据也能显著改善针对低资源语料的NLP工具性能。

Abstract: Historical and low-resource NLP remains challenging due to limited annotated data and domain mismatches with modern, web-sourced corpora. This paper outlines our work in using large language models (LLMs) to create ground-truth annotations for historical French (16th-20th centuries) and Chinese (1900-1950) texts. By leveraging LLM-generated ground truth on a subset of our corpus, we were able to fine-tune spaCy to achieve significant gains on period-specific tests for part-of-speech (POS) annotations, lemmatization, and named entity recognition (NER). Our results underscore the importance of domain-specific models and demonstrate that even relatively limited amounts of synthetic data can improve NLP tools for under-resourced corpora in computational humanities research.

</details>


### [45] [Talk, Snap, Complain: Validation-Aware Multimodal Expert Framework for Fine-Grained Customer Grievances](https://arxiv.org/abs/2511.14693)
*Rishu Kumar Singh,Navneet Shreya,Sarmistha Das,Apoorva Singh,Sriparna Saha*

Main category: cs.CL

TL;DR: 该论文提出了VALOR，一种用于多模态、多轮客户投诉对话的分类模型，可细粒度识别投诉的方面和严重性，显著优于传统单模态模型。


<details>
  <summary>Details</summary>
Motivation: 现有投诉分析主要依赖单一模态的短文本，难以处理包含文本与图像的多模态客户对话，且缺少对复杂投诉场景的细致分类。

Method: 提出了VALOR模型，采用多专家推理和大规模生成模型结合链式思维提示，利用语义对齐分数实现文本与图像模态的融合，通过专家路由机制进行细致决策。

Result: 在标注了细粒度方面和严重性的多模态投诉数据集上，VALOR模型优于基线模型，尤其在信息分布于文本与图像的复杂情境中表现突出。

Conclusion: 多模态交互和专家验证机制显著提升了投诉理解系统的效果，同时支持联合国可持续发展目标，推动更智能、责任化的消费服务体系发展。

Abstract: Existing approaches to complaint analysis largely rely on unimodal, short-form content such as tweets or product reviews. This work advances the field by leveraging multimodal, multi-turn customer support dialogues, where users often share both textual complaints and visual evidence (e.g., screenshots, product photos) to enable fine-grained classification of complaint aspects and severity. We introduce VALOR, a Validation-Aware Learner with Expert Routing, tailored for this multimodal setting. It employs a multi-expert reasoning setup using large-scale generative models with Chain-of-Thought (CoT) prompting for nuanced decision-making. To ensure coherence between modalities, a semantic alignment score is computed and integrated into the final classification through a meta-fusion strategy. In alignment with the United Nations Sustainable Development Goals (UN SDGs), the proposed framework supports SDG 9 (Industry, Innovation and Infrastructure) by advancing AI-driven tools for robust, scalable, and context-aware service infrastructure. Further, by enabling structured analysis of complaint narratives and visual context, it contributes to SDG 12 (Responsible Consumption and Production) by promoting more responsive product design and improved accountability in consumer services. We evaluate VALOR on a curated multimodal complaint dataset annotated with fine-grained aspect and severity labels, showing that it consistently outperforms baseline models, especially in complex complaint scenarios where information is distributed across text and images. This study underscores the value of multimodal interaction and expert validation in practical complaint understanding systems. Resources related to data and codes are available here: https://github.com/sarmistha-D/VALOR

</details>


### [46] [Subword Tokenization Strategies for Kurdish Word Embeddings](https://arxiv.org/abs/2511.14696)
*Ali Salehi,Cassandra L. Jacobs*

Main category: cs.CL

TL;DR: 本文比较了库尔德语词嵌入的多种分词策略，发现形态素分词在词嵌入表达及语义组织方面优于BPE和词级分词。


<details>
  <summary>Details</summary>
Motivation: 探索适合低资源库尔德语的高效分词策略，以提升词嵌入的形态相似性和语义组织效果。

Method: 开发基于BiLSTM-CRF的形态素分割器，利用最少人工标注进行训练，比较词级、形态素和BPE三种分词方法在多项指标上的表现。

Result: 发现BPE分词覆盖率低，导致其形态相似性表现被人为高估；全面评价显示形态素分词在词嵌入空间组织和语义邻域结构上表现最佳。

Conclusion: 强调低资源语言处理需重视覆盖率驱动的评价，形态素分词为库尔德语分词提供了更优方案。

Abstract: We investigate tokenization strategies for Kurdish word embeddings by comparing word-level, morpheme-based, and BPE approaches on morphological similarity preservation tasks. We develop a BiLSTM-CRF morphological segmenter using bootstrapped training from minimal manual annotation and evaluate Word2Vec embeddings across comprehensive metrics including similarity preservation, clustering quality, and semantic organization. Our analysis reveals critical evaluation biases in tokenization comparison. While BPE initially appears superior in morphological similarity, it evaluates only 28.6\% of test cases compared to 68.7\% for morpheme model, creating artificial performance inflation. When assessed comprehensively, morpheme-based tokenization demonstrates superior embedding space organization, better semantic neighborhood structure, and more balanced coverage across morphological complexity levels. These findings highlight the importance of coverage-aware evaluation in low-resource language processing and offers different tokenization methods for low-resourced language processing.

</details>


### [47] [Strategic Innovation Management in the Age of Large Language Models Market Intelligence, Adaptive R&D, and Ethical Governance](https://arxiv.org/abs/2511.14709)
*Raha Aghaei,Ali A. Kiaei,Mahnaz Boush,Mahan Rofoosheh,Mohammad Zavvar*

Main category: cs.CL

TL;DR: 本文分析了大型语言模型（LLMs）在研发过程中的多重功能及其促进行业创新的作用。


<details>
  <summary>Details</summary>
Motivation: 提升研发效率和创新速度，满足快速变化的技术需求。

Method: 通过对科学文献、专利数据库和实验数据的广泛分析，利用大型语言模型实现知识自动发现和跨学科整合。

Result: LLMs显著提高了研发流程的灵活性和信息利用率，加速了创新周期，缩短了突破性创意的上市时间。

Conclusion: 大型语言模型在创新生态系统中促进了合作，优化了研发流程，是推动技术创新的重要工具。

Abstract: This study analyzes the multiple functions of Large Language Models (LLMs) in transforming research and development (R&D) processes. By automating knowledge discovery, boosting hypothesis creation, integrating transdisciplinary insights, and enabling cooperation within innovation ecosystems, LLMs dramatically improve the efficiency and effectiveness of research processes. Through extensive analysis of scientific literature, patent databases, and experimental data, these models enable more flexible and informed R&D workflows, ultimately accelerating innovation cycles and lowering time-to-market for breakthrough ideas.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [48] [Show and Tell: Prompt Strategies for Style Control in Multi-Turn LLM Code Generation](https://arxiv.org/abs/2511.13972)
*Jeremiah Bohr*

Main category: cs.SE

TL;DR: 本文探讨了语言模型在生成代码时的风格控制机制，比较了基于指令、基于示例及两者结合的提示对代码风格和改进过程的影响。


<details>
  <summary>Details</summary>
Motivation: 语言模型生成的代码虽然功能正确，但往往过于冗长，文档和防御性模式偏离人类代码风格，研究如何通过提示机制有效控制代码风格成为核心问题。

Method: 设计四种系统提示，在两轮生成流程中，模型先完成中等难度的Python任务，再按改进指令对代码进行修订，以比较不同提示方案对初始代码压缩及后续扩展规范性的影响。

Result: 结合指令和示例的提示产生了最强的初始代码压缩效果和最高的扩展纪律；纯指令提示有显著的初始效果和适度的扩展纪律；纯示例提示则表现出较弱的初始效果且缺乏扩展纪律。

Conclusion: 初始提示效果与后续代码扩展纪律是提示设计的两个独立维度，结合指令与示例的混合提示在维持代码风格控制方面最为有效。

Abstract: Language models generate functionally correct code that tends toward excessive verbosity, with elaborate documentation and defensive patterns that diverge from human baselines. Two prompting mechanisms have emerged for stylistic control: instruction based prompts that articulate abstract directives, and example based prompts that provide concrete code demonstrations. The core problem is whether stylistic constraints persist when models enhance initial implementations with additional features while maintaining high functional accuracy. Here we show that instruction-based, example-based, and combined prompts produce distinct patterns of initial control and expansion discipline over one enhancement turn. We manipulated system prompts across four conditions in a paired two-turn protocol where models first generated solutions to an intermediate Python task, then revised their code under general improvement directives, holding the user task fixed (N = 160 paired programs). Combined prompts produced the strongest initial compression and greatest expansion discipline. Instructions showed large initial effects and moderate expansion discipline. Examples showed modest initial effects with no expansion discipline. These results show that initial prompt effectiveness and expansion discipline are separate aspects of prompt design, and that combined approaches provide the most stable stylistic control in this two-turn workflow.

</details>


### [49] [Exploring the Use of ChatGPT by Computer Science Students in Software Development: Applications, Ethical Considerations, and Insights for Engineering Education](https://arxiv.org/abs/2511.13996)
*Daihan Xu,Diana Martin*

Main category: cs.SE

TL;DR: 该研究通过半结构化访谈，深入分析英国计算机科学学生在软件开发项目中如何策略性和伦理性地使用ChatGPT，以及他们对相关伦理问题的认知。


<details>
  <summary>Details</summary>
Motivation: 当前研究多依赖调查，缺乏对学生使用ChatGPT策略和伦理认知的深度分析，本文旨在弥补这一空白。

Method: 采用半结构化访谈，探讨学生在软件开发项目中如何报告和理解ChatGPT的使用及其伦理影响。

Result: 学生学习模式从传统的独立思考和手工编码，转变为AI辅助的互动编程，且学生普遍限制ChatGPT贡献约30%，重视输出评估，但仅少数深入分析AI代码。学生反对未署名使用，关注隐私和技能退化风险，呼吁明确指导。

Conclusion: 研究揭示了学生与AI共学的动态变化，强调需要明确指导以促进责任和教学上的合理使用。

Abstract: ChatGPT has been increasingly used in computer science, offering efficient support across software development tasks. While it helps students navigate programming challenges, its use also raises concerns about academic integrity and overreliance. Despite growing interest in this topic, prior research has largely relied on surveys, emphasizing trends over in-depth analysis of students' strategies and ethical awareness. This study complements existing work through a qualitative investigation of how computer science students in one UK institution strategically and ethically engage with ChatGPT in software development projects. Drawing on semi-structured interviews, it explores two key questions: How do computer science students ethically and strategically report using ChatGPT in software development projects? How do students understand and perceive the ethical issues associated with using ChatGPT in academic and professional contexts? Findings reveal a shift in students' learning models, moving from traditional "independent thinking-manual coding-iterative debugging" to "AI-assisted ideation-interactive programming-collaborative optimization." Importantly, many use ChatGPT conversationally to deepen understanding, while consciously reserving creative and high-level decision-making tasks for themselves. Students tend to cap ChatGPT's contribution to roughly 30%, and evaluate its output to mitigate overreliance. However, only a minority thoroughly analyze AI-generated code, raising concerns about reduced critical engagement. Meanwhile, students reject uncredited use, highlight risks such as privacy breaches and skill degradation, and call for clear usage guidelines set by their teachers. This research offers novel insights into the evolving learner-AI dynamic and highlights the need for explicit guidance to support responsible and pedagogically sound use of such tools.

</details>


### [50] [LoCoBench-Agent: An Interactive Benchmark for LLM Agents in Long-Context Software Engineering](https://arxiv.org/abs/2511.13998)
*Jielin Qiu,Zuxin Liu,Zhiwei Liu,Rithesh Murthy,Jianguo Zhang,Haolin Chen,Shiyu Wang,Ming Zhu,Liangwei Yang,Juntao Tan,Roshan Ram,Akshara Prabhakar,Tulika Awalgaonkar,Zixiang Chen,Zhepeng Cen,Cheng Qian,Shelby Heinecke,Weiran Yao,Silvio Savarese,Caiming Xiong,Huan Wang*

Main category: cs.SE

TL;DR: LoCoBench-Agent是一个针对大型语言模型代理的软件工程长上下文评估框架，支持多轮交互和工具使用评测，揭示模型在理解和效率之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法全面评估LLM作为代码开发代理在多轮交互、工具使用和适应性推理方面的实际能力。

Method: 构建涵盖8000场景的交互式评估环境，配备8种专业工具，设计9个维度的评估指标，测试模型在1万至100万token上下文长度下的表现。

Result: 发现代理展示出强大的长上下文鲁棒性，理解度与效率存在负相关，且模型间的会话效率和工具使用策略差异显著。

Conclusion: LoCoBench-Agent作为首个面向软件工程的长上下文LLM代理基准，为衡量和提升自动化软件开发能力提供了严格基础。

Abstract: As large language models (LLMs) evolve into sophisticated autonomous agents capable of complex software development tasks, evaluating their real-world capabilities becomes critical. While existing benchmarks like LoCoBench~\cite{qiu2025locobench} assess long-context code understanding, they focus on single-turn evaluation and cannot capture the multi-turn interactive nature, tool usage patterns, and adaptive reasoning required by real-world coding agents. We introduce \textbf{LoCoBench-Agent}, a comprehensive evaluation framework specifically designed to assess LLM agents in realistic, long-context software engineering workflows. Our framework extends LoCoBench's 8,000 scenarios into interactive agent environments, enabling systematic evaluation of multi-turn conversations, tool usage efficiency, error recovery, and architectural consistency across extended development sessions. We also introduce an evaluation methodology with 9 metrics across comprehension and efficiency dimensions. Our framework provides agents with 8 specialized tools (file operations, search, code analysis) and evaluates them across context lengths ranging from 10K to 1M tokens, enabling precise assessment of long-context performance. Through systematic evaluation of state-of-the-art models, we reveal several key findings: (1) agents exhibit remarkable long-context robustness; (2) comprehension-efficiency trade-off exists with negative correlation, where thorough exploration increases comprehension but reduces efficiency; and (3) conversation efficiency varies dramatically across models, with strategic tool usage patterns differentiating high-performing agents. As the first long-context LLM agent benchmark for software engineering, LoCoBench-Agent establishes a rigorous foundation for measuring agent capabilities, identifying performance gaps, and advancing autonomous software development at scale.

</details>


### [51] [FlakyGuard: Automatically Fixing Flaky Tests at Industry Scale](https://arxiv.org/abs/2511.14002)
*Chengpeng Li,Farnaz Behrang,August Shi,Peng Liu*

Main category: cs.SE

TL;DR: FlakyGuard利用图结构和选择性图探索，有效修复工业环境中的易变测试，显著提升修复成功率和开发者接受度。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的易变测试修复方法在工业环境中因上下文信息提供不当导致效果不佳。

Method: 将代码视为图结构，采用选择性图探索方式提取最相关的上下文信息，提升LLM修复性能。

Result: 在真实工业仓库的易变测试中，FlakyGuard修复率达47.6%，修复被开发者接受率达51.8%，成功率较现有最先进方法高出至少22%。

Conclusion: FlakyGuard有效解决上下文问题，显著提升易变测试自动修复效果，并获得开发者高度认可。

Abstract: Flaky tests that non-deterministically pass or fail waste developer time and slow release cycles. While large language models (LLMs) show promise for automatically repairing flaky tests, existing approaches like FlakyDoctor fail in industrial settings due to the context problem: providing either too little context (missing critical production code) or too much context (overwhelming the LLM with irrelevant information). We present FlakyGuard, which addresses this problem by treating code as a graph structure and using selective graph exploration to find only the most relevant context. Evaluation on real-world flaky tests from industrial repositories shows that FlakyGuard repairs 47.6 % of reproducible flaky tests with 51.8 % of the fixes accepted by developers. Besides it outperforms state-of-the-art approaches by at least 22 % in repair success rate. Developer surveys confirm that 100 % find FlakyGuard's root cause explanations useful.

</details>


### [52] [Keeping Code-Aware LLMs Fresh: Full Refresh, In-Context Deltas, and Incremental Fine-Tuning](https://arxiv.org/abs/2511.14022)
*Pradeep Kumar Sharma,Ishaan Puri,Mantinder Jit Singh,Swapnil Shivaprasad,Hritvik Shrivastava*

Main category: cs.SE

TL;DR: 论文研究如何在代码库持续演进的情况下保持模型的预测新鲜度，提出并比较了三种模型更新策略，最终发现增量微调方法在平衡新旧代码识别方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 代码库不断变化导致模型性能下降，如何更新模型以保持对最新代码的准确识别同时不遗忘旧代码成为挑战。

Method: 定义新鲜度为基线快照和当前代码之间的领域漂移，比较了全量刷新、上下文学习和增量微调三种更新策略，设计了别名感知评估协议和遗忘探测器来量化模型性能。

Result: 增量微调方法结合旧代码混合策略在多个代码库中表现最好，上下文学习快速提升新代码识别而全量刷新在最大新代码准确性方面表现最佳，还对不同更新窗口下的差异方法效果进行了比较。

Conclusion: 增量微调是平衡模型在更新代码环境中保持新鲜度和避免遗忘的有效方法，不同方法可根据实际训练条件和需求灵活选择，且差异化处理重命名、删除和行为变化可进一步提升模型性能。

Abstract: Modern codebases evolve continuously: files are renamed or deleted; public APIs drift; behavior shifts within otherwise familiar modules. A model trained yesterday to map a developer's natural-language question to the exact set of repository file paths that matter will degrade tomorrow, even if the questions themselves look unchanged. In this paper we study, at system scale and across several widely used repositories, how to keep such a model fresh without surrendering retention on earlier code. We frame freshness as a form of domain drift between a base snapshot and the current HEAD, and we compare three families of update strategies: (A) Full Refresh, retraining the entire model at the new snapshot; (B) In-Context Learning (ICL) that injects recent deltas (raw git diffs or concise English summaries) at inference; and (C) Incremental Fine-Tuning (Inc-FT) on delta-derived training sets, with carefully controlled NEW:OLD mixing to mitigate catastrophic forgetting. We contribute an alias-aware evaluation protocol that credits rename while never rewarding deleted paths, and a practical Forgetting Probe that quantifies residual emissions of obsolete paths. Across Flask, SQLAlchemy, Pandas, and Poetry, Inc-FT with old-aware mixes delivers the best overall balance on mixed sets, ICL with English delta summaries delivers the fastest new-code lift when training is not feasible, and Full Refresh remains the ceiling when maximum NEW accuracy matters. We also compare Git-diff Inc-FT to full-file Inc-FT, showing that diffs excel in rename/delete-heavy windows while full-file context wins in behavior-change-heavy windows.

</details>


### [53] [LogPurge: Log Data Purification for Anomaly Detection via Rule-Enhanced Filtering](https://arxiv.org/abs/2511.14062)
*Shenglin Zhang,Ziang Chen,Zijing Que,Yilun Liu,Yongqian Sun,Sicheng Wei,Dan Pei,Hailin Li*

Main category: cs.SE

TL;DR: 本文提出了一种名为LogPurge的自动日志净化框架，通过两阶段过滤算法，有效去除异常日志并保留正常日志，显著提升了日志异常检测的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统日志异常检测依赖于干净且无异常的日志数据进行深度学习模型训练，而获取这些数据成本高且准确性不足，现有自动清洗方法未能充分结合日志的特性和语义。

Method: LogPurge采用两阶段过滤算法：第一阶段利用大型语言模型结合系统规则去除聚类异常模式；第二阶段采用分治策略将剩余污秽区域拆分成小问题，逐个净化。

Result: 在两个公开数据集和一个工业数据集上，平均去除了98.74%的异常样本，保留了82.39%的正常样本。与最新无监督样本选择算法相比，F1分数分别提升了35.7%、84.11%和149.72%。

Conclusion: LogPurge框架通过结合大型语言模型和系统规则的两阶段过滤，有效净化日志数据，显著提升了异常检测模型的性能，对日志异常检测领域有重要贡献。

Abstract: Log anomaly detection, which is critical for identifying system failures and preempting security breaches, detects irregular patterns within large volumes of log data, and impacts domains such as service reliability, performance optimization, and database log analysis. Modern log anomaly detection methods rely on training deep learning models on clean, anomaly-free log sequences. However, obtaining such clean log data requires costly and tedious human labeling, and existing automatic cleaning methods fail to fully integrate the specific characteristics and actual semantics of logs in their purification process. In this paper, we propose a cost-aware, rule-enhanced purification framework, LogPurge, that automatically selects a sufficient subset of normal log sequences from contamination log sequences to train a anomaly detection model. Our approach involves a two-stage filtering algorithm: In the first stage, we use a large language model (LLM) to remove clustered anomalous patterns and enhance system rules to improve LLM's understanding of system logs; in the second stage, we utilize a divide-and-conquer strategy that decomposes the remaining contaminated regions into smaller subproblems, allowing each to be effectively purified through the first stage procedure. Our experiments, conducted on two public datasets and one industrial dataset, show that our method significantly removes an average of 98.74% of anomalies while retaining 82.39% of normal samples. Compared to the latest unsupervised log sample selection algorithms, our method achieves F-1 score improvements of 35.7% and 84.11% on the public datasets, and an impressive 149.72% F-1 improvement on the private dataset, demonstrating the effectiveness of our approach.

</details>


### [54] [A Practical Implementation of Customized Scrum-Based Agile Framework in Aerospace Software Development Under DO-178C Constraints](https://arxiv.org/abs/2511.14215)
*Malik Muhammad Umer*

Main category: cs.SE

TL;DR: 本文提出了一种针对DO-178C安全关键航空航天软件的Scrum敏捷框架，通过调整核心角色和流程，实现了高效且合规的软件开发。实证研究表明，该框架显著提高了开发效率和缺陷管理能力。


<details>
  <summary>Details</summary>
Motivation: 航空航天系统的日益复杂要求开发过程既要敏捷又要满足严格的安全及认证需求。现有敏捷方法难以完全满足DO-178C标准的认证和安全要求，因此需要专门定制的敏捷框架。

Method: 本文基于Scrum方法，定制了多学科产品负责人模型、双重接受标准、独立测试与文档团队及专职认证联络员等关键增强措施。通过两个航空航天项目的对比实验验证该框架效果。

Result: 采用定制敏捷框架的项目相比传统瀑布模型，实现了需求总工时减少76%、缺陷检测和修复速度分别提高75%和78%、缺陷密度降低50%以上，同时完全符合DO-178C等级A认证要求。

Conclusion: 经过严格定制的敏捷流程能够有效融合安全认证需求与敏捷实践，显著提升开发质量和效率。未来可通过自动化技术进一步优化流程，并验证该框架在航空航天及其他安全关键领域的适用性。

Abstract: The increasing complexity of aerospace systems requires development processes that balance agility with stringent safety and certification demands. This study presents an empirically validated Scrum-based Agile framework tailored for DO-178C compliant, safety-critical aerospace software. The framework adapts core Scrum roles, artifacts, and events to meet certification, verification, and independence objectives. Key enhancements include a multi-disciplinary product ownership model, dual compliance-and-functionality acceptance criteria, independent testing and documentation teams, and dedicated certification liaisons. The approach was evaluated through two comparable aerospace projects-one using the customized Agile process and the other a traditional Waterfall model. Results showed significant improvements: a 76% reduction in Total Effort per Requirement, 75% faster Defect Detection, 78% faster Defect Resolution, and over 50% lower Defect Density, while maintaining full compliance with DO-178C Design Assurance Level A. These findings demonstrate that Agile practices and regulatory compliance can coexist effectively when supported by disciplined tailoring and proactive engagement with certification authorities. The study also notes challenges, including increased V&V effort due to recurring Sprint activities and refactoring inherent to iterative development. Nonetheless, it identifies substantial opportunities for further gains through workflow automation, CI/CD practices, and automated documentation, verification, and configuration management. Future research should expand validation of this framework across the aerospace domain and other safety-critical industries with similar certification requirements.

</details>


### [55] [KTester: Leveraging Domain and Testing Knowledge for More Effective LLM-based Test Generation](https://arxiv.org/abs/2511.14224)
*Anji Li,Mingwei Liu,Zhenxi Chen,Zheng Pei,Zike Li,Dekun Dai,Yanlin Wang,Zibin Zheng*

Main category: cs.SE

TL;DR: 提出了KTester框架，通过结合项目特定知识和测试领域知识，提升基于大语言模型的自动单元测试生成效率和质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的自动测试生成在实际项目中难以生成既正确又易维护的测试用例，需要引入更多上下文和领域知识以提高测试质量。

Method: KTester先通过静态分析提取项目结构和使用知识，为语言模型提供丰富上下文；然后结合测试领域知识，将测试用例设计与测试方法生成分离，采用多视角提示策略，引导模型考虑多样测试启发式；生成的测试用例遵循结构化模板，提升清晰度和可维护性。

Result: 在多个开源项目上，KTester在执行通过率提升5.69%、代码覆盖率提升8.83%，同时生成测试用例更少且耗时更短。人类评价中，KTester生成的测试用例在正确性、可读性和可维护性上显著优于现有最强基线。

Conclusion: KTester验证了结合项目和测试领域知识的框架能有效提升基于大语言模型的自动测试生成质量与效率，具备实际应用价值。

Abstract: Automated unit test generation using large language models (LLMs) holds great promise but often struggles with generating tests that are both correct and maintainable in real-world projects. This paper presents KTester, a novel framework that integrates project-specific knowledge and testing domain knowledge to enhance LLM-based test generation. Our approach first extracts project structure and usage knowledge through static analysis, which provides rich context for the model. It then employs a testing-domain-knowledge-guided separation of test case design and test method generation, combined with a multi-perspective prompting strategy that guides the LLM to consider diverse testing heuristics. The generated tests follow structured templates, improving clarity and maintainability. We evaluate KTester on multiple open-source projects, comparing it against state-of-the-art LLM-based baselines using automatic correctness and coverage metrics, as well as a human study assessing readability and maintainability. Results demonstrate that KTester significantly outperforms existing methods across six key metrics, improving execution pass rate by 5.69% and line coverage by 8.83% over the strongest baseline, while requiring less time and generating fewer test cases. Human evaluators also rate the tests produced by KTester significantly higher in terms of correctness, readability, and maintainability, confirming the practical advantages of our knowledge-driven framework.

</details>


### [56] [How Does Cognitive Capability and Personality Influence Problem-Solving in Coding Interview Puzzles?](https://arxiv.org/abs/2511.14367)
*Dulaji Hidellaarachchi,Sebastian Baltes,John Grundy*

Main category: cs.SE

TL;DR: 本文研究了认知能力和人格特质如何影响软件问题解决能力，发现尽责性和开放性人格特质与问题解决表现和推理准确性正相关，神经质则有负面影响。


<details>
  <summary>Details</summary>
Motivation: 软件工程作为认知密集型活动，个体差异（不仅仅是技术技能）对问题解决能力有重要影响，探讨认知能力与人格特质的联合作用具有现实意义。

Method: 对80名参与者（40名软件从业者，40名学生）进行了认知能力（Baddeley三分钟语法推理测试）和人格测评（IPIP NEO 50），并完成包括编码和逻辑推理的九个面试式问题，进行描述性和相关性分析。

Result: 从业者在语法推理准确率和任务表现上略优于学生；语法推理准确率与问题解决表现正相关；尽责性和开放性与表现均正相关，神经质与表现负相关。

Conclusion: 尽责性和开放性人格特质与较强认知能力共同促进软件问题解决，而较高神经质可能在压力下影响表现。结果对教育和招聘实践有启示，未来研究应扩大样本和任务多样性。

Abstract: Software engineering is a deeply cognitive activity shaped by individual differences that extend beyond technical skill. This study investigates how cognitive capability and personality traits jointly relate to software problem solving among 80 participants (40 software practitioners, 40 software engineering students). Cognitive capability was measured using Baddeleys three minute grammatical reasoning test, while personality was assessed using the IPIP NEO 50 test. Participants further completed nine interview style problem solving questions. Six questions were related to coding and three were related to logical reasoning. Descriptive and correlational analyses show that practitioners achieved slightly higher grammatical reasoning accuracy and overall task performance than students. Grammatical-reasoning accuracy correlated positively with problem solving performance, indicating that stronger cognitive capability is associated with better performance in coding and logical tasks. Personality performance links were systematic. We identified that the conscientiousness trait correlated most strongly with problem solving and with reasoning accuracy, while the openness to experience trait was positively related to both outcomes. Neuroticism showed small, negative associations with accuracy and performance. Taken together, our results suggest that conscientiousness and openness to experience characteristics complement reasoning accuracy to support software problem solving, whereas elevated negative affect may hinder precision under time pressure. Our findings suggest practical implications for education and industry such as integrating structured reasoning tasks in curricula, and considering personality cognition in recruitment and role allocation. We highlight directions for future research such as longitudinal and task diverse replications with larger samples.

</details>


### [57] [Watchdogs and Oracles: Runtime Verification Meets Large Language Models for Autonomous Systems](https://arxiv.org/abs/2511.14435)
*Angelo Ferrando*

Main category: cs.SE

TL;DR: 本文探讨了运行时验证(RV)与大型语言模型(LLMs)结合提升自主系统安全性的愿景。


<details>
  <summary>Details</summary>
Motivation: 自主系统中的学习组件和开放环境增加了安全保障的难度，传统形式方法依赖完整模型且假设静态，但现实中难以满足；运行时验证虽然能动态监控，仍存在一定局限；大型语言模型虽具备强大的自然语言处理和模式识别能力，却无正式保障且易出错。因而探讨两者结合的可能性，实现优势互补。

Method: 提出将运行时验证作为大型语言模型驱动自治系统的安全护栏，利用LLMs辅助规范捕获，支持预测性推理并处理不确定性，形成一个相互促进的框架。并分析其与现有研究的区别，挑战及认证影响，指出未来研究方向。

Result: 通过构建运行时验证与大型语言模型的共生架构，论证二者结合能够提升自主系统的安全保障和可靠性。

Conclusion: 结合运行时验证和大型语言模型可以推动更可信赖的自主系统发展，未来研究需着重解决集成挑战和认证问题，实现互补优势。

Abstract: Assuring the safety and trustworthiness of autonomous systems is particularly difficult when learning-enabled components and open environments are involved. Formal methods provide strong guarantees but depend on complete models and static assumptions. Runtime verification (RV) complements them by monitoring executions at run time and, in its predictive variants, by anticipating potential violations. Large language models (LLMs), meanwhile, excel at translating natural language into formal artefacts and recognising patterns in data, yet they remain error-prone and lack formal guarantees. This vision paper argues for a symbiotic integration of RV and LLMs. RV can serve as a guardrail for LLM-driven autonomy, while LLMs can extend RV by assisting specification capture, supporting anticipatory reasoning, and helping to handle uncertainty. We outline how this mutual reinforcement differs from existing surveys and roadmaps, discuss challenges and certification implications, and identify future research directions towards dependable autonomy.

</details>


### [58] [LLM-Assisted Thematic Analysis: Opportunities, Limitations, and Recommendations](https://arxiv.org/abs/2511.14528)
*Tatiane Ornelas,Allysson Allex Araújo,Júlia Araújo,Marina Araújo,Bianca Trinkenreich,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 本研究探讨了资深软件工程研究者如何看待大语言模型（LLMs）在主题分析中的应用及其方法学影响。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在软件工程定性研究中的广泛应用，方法学上的影响尚未被充分研究，特别是在解释性分析中的严谨性和透明性问题。

Method: 通过组织一场包含25名ISERN研究者的反思工作坊，采用结构化讨论和色彩编码画布，收集参与者对LLMs辅助开放编码、主题生成与审查的看法。

Result: 参与者认可LLMs带来的效率和扩展性优势，但也指出了偏见、上下文丢失、可重复性和快速演进等风险，同时强调了提示词技能和持续人工监督的重要性。

Conclusion: LLMs可作为辅助工具支持解释性分析，但不能取代人类研究者的分析决策，研究促进了社区对LLMs负责任应用的进一步反思。

Abstract: [Context] Large Language Models (LLMs) are increasingly used to assist qualitative research in Software Engineering (SE), yet the methodological implications of this usage remain underexplored. Their integration into interpretive processes such as thematic analysis raises fundamental questions about rigor, transparency, and researcher agency. [Objective] This study investigates how experienced SE researchers conceptualize the opportunities, risks, and methodological implications of integrating LLMs into thematic analysis. [Method] A reflective workshop with 25 ISERN researchers guided participants through structured discussions of LLM-assisted open coding, theme generation, and theme reviewing, using color-coded canvases to document perceived opportunities, limitations, and recommendations. [Results] Participants recognized potential efficiency and scalability gains, but highlighted risks related to bias, contextual loss, reproducibility, and the rapid evolution of LLMs. They also emphasized the need for prompting literacy and continuous human oversight. [Conclusion] Findings portray LLMs as tools that can support, but not substitute, interpretive analysis. The study contributes to ongoing community reflections on how LLMs can responsibly enhance qualitative research in SE.

</details>


### [59] [FHIRconnect: Towards a seamless integration of openEHR and FHIR](https://arxiv.org/abs/2511.14618)
*Severin Kohler,Jordi Piera Jiménez,Michael Anywar,Lars Fuhrmann,Heather Leslie,Maximilian Meixner,Julian Saß,Florian Kärcher,Diego Boscá,Birger Haarbrandt,Michael Marschollek,Roland Eils*

Main category: cs.SE

TL;DR: 本文提出了FHIRconnect，一个支持openEHR与FHIR之间标准化双向数据交换的领域专用语言和开源转换引擎，解决了两者数据模型差异导致的互操作性挑战。


<details>
  <summary>Details</summary>
Motivation: openEHR和HL7 FHIR在数据建模上存在根本差异，缺乏标准化转换机制，导致医疗信息系统互操作性难以实现。

Method: 设计了FHIRconnect，采用三层架构，基于国际原型实现65%的映射复用，同时支持本地定制化，开发了首个openEHR-FHIR转换的DSL及开源执行引擎。

Result: FHIRconnect成功映射了24个国际原型与15个FHIR配置文件，覆盖七大临床领域，提供了全面的映射库和规范化映射标准。

Conclusion: FHIRconnect为openEHR与FHIR之间的语法及语义互操作提供了技术基础，促进社区驱动的映射标准化，减少定制ETL依赖，推动医疗信息系统开放标准的应用。

Abstract: Healthcare interoperability between openEHR and HL7 FHIR remains challenging due to fundamental differences in their data modeling approaches and the absence of standardized transformation mechanisms. This paper presents FHIRconnect, a novel domain-specific language and open-source transformation engine that enables standardized, bidirectional data exchange between openEHR and FHIR. Our approach addresses critical interoperability gaps through a triple-layered architecture that achieves 65% mapping reuse across projects by leveraging international archetype-based foundations while supporting local customizations. Using this framework, FHIRconnect successfully mapped 24 international archetypes to 15 FHIR profiles across seven clinical domains. Key contributions include the first comprehensive DSL for openEHR-FHIR transformation with a formal specification, an open-source execution engine (openFHIR), and an accessible mapping library covering high-impact clinical archetypes. Together, these components establish the technical basis for community-driven mapping standardization, reducing reliance on custom ETL solutions and advancing syntactic and semantic interoperability in healthcare IT systems built on open standards.

</details>


### [60] [Why Do We Code? A Theory on Motivations and Challenges in Software Engineering from Education to Practice](https://arxiv.org/abs/2511.14711)
*Aaliyah Chang,Mariam Guizani,Brittany Johnson*

Main category: cs.SE

TL;DR: 本文通过半结构访谈和Gioia方法，研究了软件工程教育向职业转变过程中动机与挑战的相互作用，提出了Exposure-Pursuit-Evaluation模型。


<details>
  <summary>Details</summary>
Motivation: 了解教育到职业过渡中动机和挑战如何影响个体进入、坚持和发展软件工程领域。

Method: 利用15次半结构访谈，采用Gioia方法归纳动机和挑战的分类，构建EPE过程模型。

Result: 发现早期有影响力的接触激发内在动机，无影响力需外在推动；好奇心和避免替代选项是教育阶段驱动因素；归属障碍贯穿教育和职业全过程。挑战限制内外在满足，影响职业持续及转变。

Conclusion: 该模型揭示了动机未满足和挑战对职业路径的影响，为设计促进内在满足和减少系统性障碍的教育与实践干预提供理论依据。

Abstract: Motivations and challenges jointly shape how individuals enter, persist, and evolve within software engineering (SE), yet their interplay remains underexplored across the transition from education to professional practice. We conducted 15 semi-structured interviews and employed the Gioia Methodology, an adapted grounded theory methodology from organizational behavior, to inductively derive taxonomies of motivations and challenges, and build the Exposure-Pursuit-Evaluation (EPE) Process Model. Our findings reveal that impactful early exposure triggers intrinsic motivations, while non-impactful exposure requires an extrinsic push (e.g., career/ personal goals, external validation). We identify curiosity and avoiding alternatives as a distinct educational drivers, and barriers to belonging as the only challenge persisting across education and career. Our findings show that career progression challenges (e.g., navigating the corporate world) constrain extrinsic fulfillment while technical training challenges, barriers to belonging and threats to motivation constrain intrinsic fulfillment. The theory shows how unmet motivations and recurring challenges influence persistence, career shifts, or departure from the field. Our results provide a grounded model for designing interventions that strengthen intrinsic fulfillment and reduce systemic barriers in SE education and practice.

</details>
