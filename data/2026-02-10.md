<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 96]
- [cs.SE](#cs.SE) [Total: 40]
- [cs.MA](#cs.MA) [Total: 8]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Does Visual Rendering Bypass Tokenization? Investigating Script-Tokenizer Misalignment in Pixel-Based Language Models](https://arxiv.org/abs/2602.06973)
*Lucky Susanto,Musa Izzanardi Wijanarko,Khumaisa Nur'aini,Farid Adilazuarda,Alham Fikri Aji,Derry Tanti Wijaya*

Main category: cs.CL

TL;DR: 研究发现视觉渲染无法消除多模态语言模型中的分词器限制，定制分词器显著优于通用分词器，分词器依然是多模态模型公平性的主要障碍。


<details>
  <summary>Details</summary>
Motivation: 探索视觉渲染是否能够真正使模型摆脱分词器限制，特别是在低资源非拉丁脚本语言的多模态语言建模中。

Method: 在使用DualGPT架构评估四种印尼低资源本地语言非拉丁字母脚本时，比较了不同分词器对模型性能的影响。

Result: 尽管Llama 2分词器在词外率和繁殖率上表现较好，但性能显著不如定制分词器，定制分词器在chrF++指标上提升最多达30.15。

Conclusion: 视觉渲染并未真正解耦模型与分词器的约束，将文本分词器重新引入多模态模型（如DualGPT）中仍然存在分词器不匹配的问题。

Abstract: While pixel-based language modeling aims to bypass the sub-word tokenization bottleneck by rendering text as images, recent multimodal variants such as DualGPT reintroduce text tokenizers to improve autoregressive performance. We investigate a fundamental question, does visual rendering truly decouple a model from tokenization constraints? Focusing on four Indonesian low-resource local languages that have their own non-Latin scripts (i.e., Javanese, Balinese, Sundanese, and Lampungnese), we evaluate the impact of script-tokenizer alignment within the DualGPT architecture. Our results show that, despite visual rendering, reintegrating a text tokenizer into the architecture reintroduces the same issue that pixel-based language modeling aims to resolve, which is the tokenizer misalignment problem. Despite having lower OOV and fertility rates, we show that the Llama 2 tokenizer performs significantly worse than a custom tokenizer, with improvements of up to 30.15 chrF++. Our findings serve as a warning for future multimodal variants, as text tokenizers remain a significant barrier to equitable models.

</details>


### [2] [BiomechAgent: AI-Assisted Biomechanical Analysis Through Code-Generating Agents](https://arxiv.org/abs/2602.06975)
*R. James Cotton,Thomas Leonard*

Main category: cs.CL

TL;DR: BiomechAgent是一款无需编程的AI工具，帮助临床医生通过自然语言分析动作捕捉数据，提升数据利用率和临床推理能力。


<details>
  <summary>Details</summary>
Motivation: 标记无关的动作捕捉数据分析对没有编程经验的临床医生来说仍然具有较大障碍。

Method: 提出BiomechAgent，一个基于代码生成的AI代理，通过自然语言交互实现生物力学分析，包括数据库查询、可视化生成和数据解释，无需用户编写代码。

Result: BiomechAgent在数据检索和可视化任务中表现出较高准确性，并展现了临床推理的潜力。通过生物力学特定指令和整合专门工具提升了分析性能。使用本地开源模型相比云端大型语言模型，在除数据库检索以外的领域表现较差。

Conclusion: BiomechAgent显著提高了标记无关动作捕捉数据的可访问性和实用性，使非编程背景用户能有效进行生物力学分析。

Abstract: Markerless motion capture is making quantitative movement analysis increasingly accessible, yet analyzing the resulting data remains a barrier for clinicians without programming expertise. We present BiomechAgent, a code-generating AI agent that enables biomechanical analysis through natural language and allows users to querying databases, generating visualizations, and even interpret data without requiring users to write code. To evaluate BiomechAgent's capabilities, we developed a systematic benchmark spanning data retrieval, visualization, activity classification, temporal segmentation, and clinical reasoning. BiomechAgent achieved robust accuracy on data retrieval and visualization tasks and demonstrated emerging clinical reasoning capabilities. We used our dataset to systematically evaluate several of our design decisions. Biomechanically-informed, domain-specific instructions significantly improved performance over generic prompts, and integrating validated specialized tools for gait event detection substantially boosted accuracy on challenging spatiotemporal analysis where the base agent struggled. We also tested BiomechAgent using a local open-weight model instead of a frontier cloud based LLM and found that perform was substantially diminished in most domains other than database retrieval. In short, BiomechAgent makes the data from accessible motion capture and much more useful and accessible to end users.

</details>


### [3] [Bridging the Knowledge Void: Inference-time Acquisition of Unfamiliar Programming Languages for Coding Tasks](https://arxiv.org/abs/2602.06976)
*Chen Shen,Wei Cheng,Jingyue Yang,Huan Zhang,Yuhan Wu,Wei Hu*

Main category: cs.CL

TL;DR: 本文提出ILA-agent，使大型语言模型通过动态互动学习未知语言，在Cangjie语言基准上获得显著性能提升，展示了非数据密集型语言习得的新路径。


<details>
  <summary>Details</summary>
Motivation: 传统大型语言模型依赖大量预训练数据，对未知编程语言适应性差，故探究在推理时通过有限外部资源学习新语言的新范式。

Method: 提出ILA-agent框架，通过建模人类行为作为工具，允许语言模型在推理时动态交互官方文档和执行环境，逐步掌握新语言。

Result: 在新颖的静态类型语言Cangjie及其多任务基准Cangjie-bench上，ILA-agent在代码生成、翻译和程序修复任务中表现优异，显著超过检索增强方法。

Conclusion: ILA-agent有效提升了大型语言模型在未知编程语言上的表现，显著优于检索增强的基线方法，但依然存在性能提升空间。

Abstract: The proficiency of Large Language Models (LLMs) in coding tasks is often a reflection of their extensive pre-training corpora, which typically collapses when confronted with previously unfamiliar programming languages. Departing from data-intensive finetuning, we investigate the paradigm of Inference-time Language Acquisition (ILA), where an LLM masters an unfamiliar language through dynamic interaction with limited external resources. In this paper, we propose ILA-agent, a general ILA framework that equips LLMs with a set of behavioral primitives. By modeling essential human-like behaviors as a suite of tools, ILA-agent enables LLMs to incrementally explore, apply, and verify language knowledge through structured interactions with the official documentation and execution environment. To provide a rigorous evaluation in a low-resource setting, we construct Cangjie-bench, a multi-task benchmark based on the novel statically-typed language Cangjie. We instantiate ILA-agent for Cangjie and evaluate its performance across code generation, translation, and program repair tasks. Results using diverse LLMs demonstrate that ILA-agent significantly outperforms retrieval-augmented baselines. Further analysis of agent trajectories characterizes the emergent behavior patterns while highlighting persisting performance gaps.

</details>


### [4] [Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model](https://arxiv.org/abs/2602.07120)
*Jacqueline He,Jonathan Hayase,Wen-tau Yih,Sewoong Oh,Luke Zettlemoyer,Pang Wei Koh*

Main category: cs.CL

TL;DR: 本论文提出Anchored Decoding方法，有效抑制语言模型中的逐字复制问题，减少版权风险，保持文本质量，具备良好的风险与效用平衡。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型容易逐字复制敏感或受版权保护的训练数据，带来创作者同意、补偿及开发者合规风险，因此需要一种方法在保证文本质量的同时减少逐字复制行为。

Method: 提出了一种名为Anchored Decoding的推理时插拔式方法，通过约束生成文本在一个安全许可语言模型的邻近范围内，从而限制逐字复制；同时引入了信息预算和逐步约束实现序列级保证；开发了一个许可训练安全模型（TinyComma 1.8B）及字节级变体Anchored$_{\mathrm{Byte}}$ Decoding，结合ByteSampler框架实现跨词汇融合。

Result: 在六个模型对上进行长文本评估，Anchored Decoding及其字节级变体在减少可测复制率方面平均降低了75%，显著优于基线模型，并且保持了接近原始模型的文本流畅度和事实正确性，推理开销适中。

Conclusion: Anchored Decoding及其变体Anchored$_{\mathrm{Byte}}$ Decoding成功抑制了语言模型训练数据的逐字复制现象，在保持文本流畅性和事实性的同时，大幅度减少了版权风险，达到了风险与效用的优良平衡。

Abstract: Modern language models (LMs) tend to memorize portions of their training data and emit verbatim spans. When the underlying sources are sensitive or copyright-protected, such reproduction raises issues of consent and compensation for creators and compliance risks for developers. We propose Anchored Decoding, a plug-and-play inference-time method for suppressing verbatim copying: it enables decoding from any risky LM trained on mixed-license data by keeping generation in bounded proximity to a permissively trained safe LM. Anchored Decoding adaptively allocates a user-chosen information budget over the generation trajectory and enforces per-step constraints that yield a sequence-level guarantee, enabling a tunable risk-utility trade-off. To make Anchored Decoding practically useful, we introduce a new permissively trained safe model (TinyComma 1.8B), as well as Anchored$_{\mathrm{Byte}}$ Decoding, a byte-level variant of our method that enables cross-vocabulary fusion via the ByteSampler framework (Hayase et al., 2025). We evaluate our methods across six model pairs on long-form evaluations of copyright risk and utility. Anchored and Anchored$_{\mathrm{Byte}}$ Decoding define a new Pareto frontier, preserving near-original fluency and factuality while eliminating up to 75% of the measurable copying gap (averaged over six copying metrics) between the risky baseline and a safe reference, at a modest inference overhead.

</details>


### [5] [Free Energy Mixer](https://arxiv.org/abs/2602.07160)
*Jiecheng Lu,Shihao Yang*

Main category: cs.CL

TL;DR: 本文提出Free Energy Mixer，一种可在通道层面进行选择的高效注意力读写机制，提升模型性能且不增加计算复杂度，适用于多种序列模型和任务。


<details>
  <summary>Details</summary>
Motivation: 标准注意力机制通过头部凸平均读取键值，阻碍了通道层面的选择能力，限制了模型表达力。本文旨在引入一种能在通道层面进行更灵活选择且复杂度不增的方法。

Method: FEM 使用自由能（log-sum-exp）读取，通过学习的逆温度参数对先验分布进行对数线性调整，实现从平均读取到逐通道选择的平滑过渡。方法兼容多种注意力和序列模型，实现了双层门控结构，支持标准和线性注意力机制、线性RNN及SSM。

Result: 在控制参数数量相同情况下，FEM 在自然语言处理、计算机视觉和时间序列任务上均优于强基线方法，验证了其效果和通用性。

Conclusion: Free Energy Mixer (FEM) 提出了一种价值驱动的对通道进行选择的读操作方法，相较于标准注意力机制的凸平均，FEM 能够实现更细粒度的通道选择，并且保持计算复杂度不变。

Abstract: Standard attention stores keys/values losslessly but reads them via a per-head convex average, blocking channel-wise selection. We propose the Free Energy Mixer (FEM): a free-energy (log-sum-exp) read that applies a value-driven, per-channel log-linear tilt to a fast prior (e.g., from queries/keys in standard attention) over indices. Unlike methods that attempt to improve and enrich the $(q,k)$ scoring distribution, FEM treats it as a prior and yields a value-aware posterior read at unchanged complexity, smoothly moving from averaging to per-channel selection as the learnable inverse temperature increases, while still preserving parallelism and the original asymptotic complexity ($O(T^2)$ for softmax; $O(T)$ for linearizable variants). We instantiate a two-level gated FEM that is plug-and-play with standard and linear attention, linear RNNs and SSMs. It consistently outperforms strong baselines on NLP, vision, and time-series at matched parameter budgets.

</details>


### [6] [Your Language Model Secretly Contains Personality Subnetworks](https://arxiv.org/abs/2602.07164)
*Ruimeng Ye,Zihan Wang,Zinan Ling,Yang Xiao,Manling Li,Xiaolong Ma,Bo Hui*

Main category: cs.CL

TL;DR: 本研究发现大型语言模型内含人格子网络，通过无训练掩码和剪枝策略可有效实现不同人格行为，无需外部调整，提升了行为控制的效率与解释性。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型是否真的需要外部上下文或参数调整来适应不同人格行为，或这些行为已经隐含于模型参数中。

Method: 通过分析不同人格在模型激活中的特征统计，设计掩码策略分离轻量级的人格子网络；提出对比剪枝方法强化二元对立人格（如内向-外向）之间的区分。

Result: 所发现的人格子网络在多种评估中表现出明显优于依赖外部知识的方法，同时更高效，证明了人格行为已嵌入模型参数空间。

Conclusion: 大型语言模型内部已经存在针对不同人格的子网络，这些子网络可通过少量校准数据和掩码策略有效分离，无需外部知识或训练即可实现高效且准确的人格行为适配。

Abstract: Humans shift between different personas depending on social context. Large Language Models (LLMs) demonstrate a similar flexibility in adopting different personas and behaviors. Existing approaches, however, typically adapt such behavior through external knowledge such as prompting, retrieval-augmented generation (RAG), or fine-tuning. We ask: do LLMs really need external context or parameters to adapt to different behaviors, or do they already have such knowledge embedded in their parameters? In this work, we show that LLMs already contain persona-specialized subnetworks in their parameter space. Using small calibration datasets, we identify distinct activation signatures associated with different personas. Guided by these statistics, we develop a masking strategy that isolates lightweight persona subnetworks. Building on the findings, we further discuss: how can we discover opposing subnetwork from the model that lead to binary-opposing personas, such as introvert-extrovert? To further enhance separation in binary opposition scenarios, we introduce a contrastive pruning strategy that identifies parameters responsible for the statistical divergence between opposing personas. Our method is entirely training-free and relies solely on the language model's existing parameter space. Across diverse evaluation settings, the resulting subnetworks exhibit significantly stronger persona alignment than baselines that require external knowledge while being more efficient. Our findings suggest that diverse human-like behaviors are not merely induced in LLMs, but are already embedded in their parameter space, pointing toward a new perspective on controllable and interpretable personalization in large language models.

</details>


### [7] [Open TutorAI: An Open-source Platform for Personalized and Immersive Learning with Generative AI](https://arxiv.org/abs/2602.07176)
*Mohamed El Hajji,Tarek Ait Baha,Aicha Dakir,Hammou Fadili,Youssef Es-Saady*

Main category: cs.CL

TL;DR: Open TutorAI是一个利用大语言模型和生成技术打造的开源智能教育平台，支持个性化、沉浸式、多模态的学习辅导，提升了学习者参与度和教学效果。


<details>
  <summary>Details</summary>
Motivation: 现有教育聊天机器人缺乏情境适应性和教学灵活性，限制了学习者参与和教学效果，因而需要一个开放、整合AI和沉浸式技术的个性化教育平台。

Method: 该平台结合自然语言处理、定制3D虚拟形象以及学习者导向的配置流程，生成个性化AI助手，通过多模态交互和学习分析支持自我调节学习。

Result: 开发了基于开源的Open TutorAI平台，实现了学习者特定的AI助手生成、多模态交互、嵌入式反馈及学习分析，提升了学习体验和教学支持的灵活性与人性化。

Conclusion: Open TutorAI通过结合LLM和生成式技术，提供个性化、动态的教育辅导，提升了学习者参与度和教学效果，是下一代智能辅导系统的重要贡献。

Abstract: Recent advances in artificial intelligence have created new possibilities for making education more scalable, adaptive, and learner-centered. However, existing educational chatbot systems often lack contextual adaptability, real-time responsiveness, and pedagogical agility. which can limit learner engagement and diminish instructional effectiveness. Thus, there is a growing need for open, integrative platforms that combine AI and immersive technologies to support personalized, meaningful learning experiences. This paper presents Open TutorAI, an open-source educational platform based on LLMs and generative technologies that provides dynamic, personalized tutoring. The system integrates natural language processing with customizable 3D avatars to enable multimodal learner interaction. Through a structured onboarding process, it captures each learner's goals and preferences in order to configure a learner-specific AI assistant. This assistant is accessible via both text-based and avatar-driven interfaces. The platform includes tools for organizing content, providing embedded feedback, and offering dedicated interfaces for learners, educators, and parents. This work focuses on learner-facing components, delivering a tool for adaptive support that responds to individual learner profiles without requiring technical expertise. Its assistant-generation pipeline and avatar integration enhance engagement and emotional presence, creating a more humanized, immersive learning environment. Embedded learning analytics support self-regulated learning by tracking engagement patterns and generating actionable feedback. The result is Open TutorAI, which unites modular architecture, generative AI, and learner analytics within an open-source framework. It contributes to the development of next-generation intelligent tutoring systems.

</details>


### [8] [Can LLMs Discern the Traits Influencing Your Preferences? Evaluating Personality-Driven Preference Alignment in LLMs](https://arxiv.org/abs/2602.07181)
*Tianyu Zhao,Siqi Li,Yasser Shoukry,Salma Elmalaki*

Main category: cs.CL

TL;DR: 该论文提出利用五大人格特质对用户偏好进行标注和建模，通过构建数据集和检索框架，实现了更精准的个性化大语言模型问答。


<details>
  <summary>Details</summary>
Motivation: 现有利用用户偏好个性化大语言模型回答的方法存在偏好信号嘈杂、不完整或误导的问题，需要找到稳定的潜在人格因素作为偏好背后的信号。

Method: 构建了基于五大人格特质（OCEAN）的偏好数据集PACIFIC，并设计了自动检索人格一致偏好以辅助大语言模型回答的框架。

Result: 通过实验验证，利用人格一致的偏好将回答准确率从29.25%提升到76%，显著优于随机偏好选择。

Conclusion: 基于人格特质对用户偏好进行建模，能够显著提高个性化问答的准确率。

Abstract: User preferences are increasingly used to personalize Large Language Model (LLM) responses, yet how to reliably leverage preference signals for answer generation remains under-explored. In practice, preferences can be noisy, incomplete, or even misleading, which can degrade answer quality when applied naively. Motivated by the observation that stable personality traits shape everyday preferences, we study personality as a principled ''latent'' signal behind preference statements. Through extensive experiments, we find that conditioning on personality-aligned preferences substantially improves personalized question answering: selecting preferences consistent with a user's inferred personality increases answer-choice accuracy from 29.25% to 76%, compared to using randomly selected preferences. Based on these findings, we introduce PACIFIC (Preference Alignment Choices Inference for Five-factor Identity Characterization), a personality-labeled preference dataset containing 1200 preference statements spanning diverse domains (e.g., travel, movies, education), annotated with Big-Five (OCEAN) trait directions. Finally, we propose a framework that enables an LLM model to automatically retrieve personality-aligned preferences and incorporate them during answer generation.

</details>


### [9] [Long-Context Long-Form Question Answering for Legal Domain](https://arxiv.org/abs/2602.07190)
*Anagha Kulkarni,Parin Rajesh Jhaveri,Prasha Shrestha,Yu Tong Han,Reza Amini,Behrouz Madahian*

Main category: cs.CL

TL;DR: 针对法律文档长篇复杂内容，本文设计了专业词汇和文档结构解析的问答系统，成功实现了全面精准的长篇答案生成。


<details>
  <summary>Details</summary>
Motivation: 法律文档具有复杂的布局和专业语言，常常需要跨越多页上下文才能完整回答问题，传统问答系统难以应对这些挑战。

Method: 构建一种包括专业词汇分解、复杂文档布局解析、章节及脚注关联的问答系统，并设计覆盖率指标评估回溯性能，同时通过法律和企业税务专家参与的数据集进行验证。

Result: 通过综合实验和消融研究，验证了所提系统在处理法律文档长篇问答中的有效性和实用性。

Conclusion: 本论文提出了一种针对法律文档中长篇上下文问答的系统，能够有效处理复杂的文档结构和专业术语，生成全面且精准的长篇答案。

Abstract: Legal documents have complex document layouts involving multiple nested sections, lengthy footnotes and further use specialized linguistic devices like intricate syntax and domain-specific vocabulary to ensure precision and authority. These inherent characteristics of legal documents make question answering challenging, and particularly so when the answer to the question spans several pages (i.e. requires long-context) and is required to be comprehensive (i.e. a long-form answer). In this paper, we address the challenges of long-context question answering in context of long-form answers given the idiosyncrasies of legal documents. We propose a question answering system that can (a) deconstruct domain-specific vocabulary for better retrieval from source documents, (b) parse complex document layouts while isolating sections and footnotes and linking them appropriately, (c) generate comprehensive answers using precise domain-specific vocabulary. We also introduce a coverage metric that classifies the performance into recall-based coverage categories allowing human users to evaluate the recall with ease. We curate a QA dataset by leveraging the expertise of professionals from fields such as law and corporate tax. Through comprehensive experiments and ablation studies, we demonstrate the usability and merit of the proposed system.

</details>


### [10] [Equipping LLM with Directional Multi-Talker Speech Understanding Capabilities](https://arxiv.org/abs/2602.07211)
*Ju Lin,Jing Pan,Ruizhi Li,Ming Sun,Yuzong Liu,Alaa Hassan,Jing Zheng,Florian Metze*

Main category: cs.CL

TL;DR: 本文针对智能眼镜中的多说话人多通道语音理解，提出两种利用多麦克风阵列提升LLM方向性理解的新方法，显著提升了语音识别和翻译性能。


<details>
  <summary>Details</summary>
Motivation: 当前语音LLM多基于单通道单说话人训练，难以直接应用于多说话人、多通道复杂环境，尤其是在智能眼镜等实际场景中需要增强的定向语音理解能力。

Method: 采用级联系统结合声源分离前端模块和端到端的串行输出训练系统，利用智能眼镜内嵌的多麦克风阵列，优化定向语音的处理中实时性能。

Result: 实验结果表明，提出的方法有效赋予LLM方向性语音理解能力，在语音识别和翻译任务中表现优异。

Conclusion: 本文提出的两种将定向性整合到大型语言模型（LLM）中的方法，显著提升了多说话人、多通道语音理解能力，特别适用于智能眼镜场景。

Abstract: Recent studies have demonstrated that prompting large language models (LLM) with audio encodings enables effective speech understanding capabilities. However, most speech LLMs are trained on single-channel, single-talker data, which makes it challenging to directly apply them to multi-talker and multi-channel speech understanding task. In this work, we present a comprehensive investigation on how to enable directional multi-talker speech understanding capabilities for LLMs, specifically in smart glasses usecase. We propose two novel approaches to integrate directivity into LLMs: (1) a cascaded system that leverages a source separation front-end module, and (2) an end-to-end system that utilizes serialized output training. All of the approaches utilize a multi-microphone array embedded in smart glasses to optimize directivity interpretation and processing in a streaming manner. Experimental results demonstrate the efficacy of our proposed methods in endowing LLMs with directional speech understanding capabilities, achieving strong performance in both speech recognition and speech translation tasks.

</details>


### [11] [Beyond Accuracy: Risk-Sensitive Evaluation of Hallucinated Medical Advice](https://arxiv.org/abs/2602.07319)
*Savan Doshi*

Main category: cs.CL

TL;DR: 本论文提出了一种基于风险语言的医疗问答大型语言模型幻觉评估方法，强调风险敏感性，揭示了传统评估指标的不足。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医疗问答中出现幻觉输出，且不同幻觉产生的潜在危害差异大，传统评估未能反映风险差异。

Method: 提出了一种基于风险语言（如治疗指导、禁忌、紧急提示、高风险药物提及）的风险敏感评估框架，并结合相关性衡量来识别高风险低支撑的错误。

Result: 在三种指令调优语言模型和患者安全压力测试下，发现表面行为相似的模型存在显著不同的风险表现，标准指标未能捕捉这一点。

Conclusion: 现有的幻觉评估标准未能有效区分临床风险，导致无法识别高风险但表面正确的错误。

Abstract: Large language models are increasingly being used in patient-facing medical question answering, where hallucinated outputs can vary widely in potential harm. However, existing hallucination standards and evaluation metrics focus primarily on factual correctness, treating all errors as equally severe. This obscures clinically relevant failure modes, particularly when models generate unsupported but actionable medical language. We propose a risk-sensitive evaluation framework that quantifies hallucinations through the presence of risk-bearing language, including treatment directives, contraindications, urgency cues, and mentions of high-risk medications. Rather than assessing clinical correctness, our approach evaluates the potential impact of hallucinated content if acted upon. We further combine risk scoring with a relevance measure to identify high-risk, low-grounding failures. We apply this framework to three instruction-tuned language models using controlled patient-facing prompts designed as safety stress tests. Our results show that models with similar surface-level behavior exhibit substantially different risk profiles and that standard evaluation metrics fail to capture these distinctions. These findings highlight the importance of incorporating risk sensitivity into hallucination evaluation and suggest that evaluation validity is critically dependent on task and prompt design.

</details>


### [12] [Intent Mismatch Causes LLMs to Get Lost in Multi-Turn Conversation](https://arxiv.org/abs/2602.07338)
*Geng Liu,Fei Zhu,Rong Feng,Changyi Ma,Shiqi Wang,Gaofeng Meng*

Main category: cs.CL

TL;DR: 多轮对话中大模型性能下降源于意图对齐差距，非模型能力不足，本文提出Mediator-Assistant架构有效解决了该问题。


<details>
  <summary>Details</summary>
Motivation: 多轮对话中大语言模型性能显著下降，传统认为是模型不可靠，但实质是用户与模型间的意图对齐出现结构性歧义。

Method: 提出了Mediator-Assistant架构，利用经验驱动的Mediator将用户输入转化为明确、结构良好的指令，弥合用户模糊意图与模型理解之间的差距。

Result: 实验证明该方法显著缓解了多轮对话中各类大模型的性能下降。

Conclusion: Lost in Conversation（LiC）现象不是模型能力不足，而是用户与大模型之间的意图对齐差距导致的交流失败。传统的扩大模型规模或改进训练无法解决此问题。

Abstract: Multi-turn conversation has emerged as a predominant interaction paradigm for Large Language Models (LLMs). Users often employ follow-up questions to refine their intent, expecting LLMs to adapt dynamically. However, recent research reveals that LLMs suffer a substantial performance drop in multi-turn settings compared to single-turn interactions with fully specified instructions, a phenomenon termed ``Lost in Conversation'' (LiC). While this prior work attributes LiC to model unreliability, we argue that the root cause lies in an intent alignment gap rather than intrinsic capability deficits. In this paper, we first demonstrate that LiC is not a failure of model capability but rather a breakdown in interaction between users and LLMs. We theoretically show that scaling model size or improving training alone cannot resolve this gap, as it arises from structural ambiguity in conversational context rather than representational limitations. To address this, we propose to decouple intent understanding from task execution through a Mediator-Assistant architecture. By utilizing an experience-driven Mediator to explicate user inputs into explicit, well-structured instructions based on historical interaction patterns, our approach effectively bridges the gap between vague user intent and model interpretation. Experimental results demonstrate that this method significantly mitigates performance degradation in multi-turn conversations across diverse LLMs.

</details>


### [13] [ViHERMES: A Graph-Grounded Multihop Question Answering Benchmark and System for Vietnamese Healthcare Regulations](https://arxiv.org/abs/2602.07361)
*Long S. T. Nguyen,Quan M. Bui,Tin T. Ngo,Quynh T. N. Vo,Dung N. H. Le,Tho T. Quan*

Main category: cs.CL

TL;DR: 本文提出了越南医疗法规多跳问答基准ViHERMES及图感知检索方法，有效解决法规文档多跳推理难题，推动低资源语言法律问答的发展。


<details>
  <summary>Details</summary>
Motivation: 越南医疗法规存在层级复杂、频繁修订且相互依赖，需要多跳推理来回答相关问题，但缺乏支持这一任务的基准数据集，尤其在低资源语言环境中。

Method: 通过语义聚类和图论数据挖掘构建多跳QA生成管道，结合大型语言模型进行结构化推理注释，同时提出了一个基于图的检索框架，模拟法律条文间的关系进行上下文扩展。

Result: 构建了高质量多跳推理问答数据集ViHERMES，涵盖修订追踪、文档比较、程序综合等多种依赖模式；图感知检索框架在实验中表现优异。

Conclusion: ViHERMES 数据集为多跳推理的越南医疗法规问答提供了一个具有挑战性的基准，并且基于图的检索方法优于传统的检索基线。

Abstract: Question Answering (QA) over regulatory documents is inherently challenging due to the need for multihop reasoning across legally interdependent texts, a requirement that is particularly pronounced in the healthcare domain where regulations are hierarchically structured and frequently revised through amendments and cross-references. Despite recent progress in retrieval-augmented and graph-based QA methods, systematic evaluation in this setting remains limited, especially for low-resource languages such as Vietnamese, due to the lack of benchmark datasets that explicitly support multihop reasoning over healthcare regulations. In this work, we introduce the Vietnamese Healthcare Regulations-Multihop Reasoning Dataset (ViHERMES), a benchmark designed for multihop QA over Vietnamese healthcare regulatory documents. ViHERMES consists of high-quality question-answer pairs that require reasoning across multiple regulations and capture diverse dependency patterns, including amendment tracing, cross-document comparison, and procedural synthesis. To construct the dataset, we propose a controlled multihop QA generation pipeline based on semantic clustering and graph-inspired data mining, followed by large language model-based generation with structured evidence and reasoning annotations. We further present a graph-aware retrieval framework that models formal legal relations at the level of legal units and supports principled context expansion for legally valid and coherent answers. Experimental results demonstrate that ViHERMES provides a challenging benchmark for evaluating multihop regulatory QA systems and that the proposed graph-aware approach consistently outperforms strong retrieval-based baselines. The ViHERMES dataset and system implementation are publicly available at https://github.com/ura-hcmut/ViHERMES.

</details>


### [14] [TernaryLM: Memory-Efficient Language Modeling via Native 1-Bit Quantization with Adaptive Layer-wise Scaling](https://arxiv.org/abs/2602.07374)
*Nisharg Nargund,Priyesh Shukla*

Main category: cs.CL

TL;DR: 提出了TernaryLM，一种采用1位三元量化训练的高效语言模型，可显著减少内存占用且性能保持。


<details>
  <summary>Details</summary>
Motivation: 传统大型语言模型计算资源需求大，限制了边缘设备和资源受限环境的部署，因此需要内存高效且性能稳定的模型。

Method: 采用132M参数的Transformer架构，从零开始使用直接传递估计器和自适应分层缩放因子训练1位三元量化模型。

Result: TernaryLM在TinyStories上验证困惑度为58.42，MRPC上F1达82.47%，内存减少2.4倍（498MB vs 1197MB），推理延迟相当，训练在多种语料上稳定。

Conclusion: TernaryLM通过原生1位三元量化训练，实现了显著内存减小且不损害语言建模能力，展示了高效神经语言模型的潜力。

Abstract: Large language models (LLMs) achieve remarkable performance but demand substantial computational resources, limiting deployment on edge devices and resource-constrained environments. We present TernaryLM, a 132M parameter transformer architecture that employs native 1-bit ternary quantization {-1, 0, +1} during training, achieving significant memory reduction without sacrificing language modeling capability. Unlike post-training quantization approaches that quantize pre-trained full-precision models, TernaryLM learns quantization-aware representations from scratch using straight-through estimators and adaptive per-layer scaling factors. Our experiments demonstrate: (1) validation perplexity of 58.42 on TinyStories; (2) downstream transfer with 82.47 percent F1 on MRPC paraphrase detection; (3) 2.4x memory reduction (498MB vs 1197MB) with comparable inference latency; and (4) stable training dynamics across diverse corpora. We provide layer-wise quantization analysis showing that middle transformer layers exhibit highest compatibility with extreme quantization, informing future non-uniform precision strategies. Our results suggest that native 1-bit training is a promising direction for efficient neural language models. Code is available at https://github.com/1nisharg/TernaryLM-Memory-Efficient-Language-Modeling.

</details>


### [15] [Efficient Post-Training Pruning of Large Language Models with Statistical Correction](https://arxiv.org/abs/2602.07375)
*Peiqi Yu,Jinhao Wang,Xinyi Sui,Nam Ling,Wei Wang,Wei Jiang*

Main category: cs.CL

TL;DR: 本文提出一种基于一阶统计特性的轻量级后训练剪枝方法，有效提升大语言模型剪枝性能且计算成本低。


<details>
  <summary>Details</summary>
Motivation: 现有剪枝方法在剪枝质量与计算效率之间存在权衡，启发式方法高效但对激活异常敏感，重构方法提高质量但计算成本高。

Method: 利用模型权重和激活的一阶统计特性，通过通道统计校正基于幅值的重要性评分，并在剪枝后应用解析能量补偿以纠正权重移除带来的分布失真。

Result: 在多个大语言模型家族、稀疏模式及评估任务中，所提方法在保持与启发式方法相当的计算成本的同时，提高了剪枝效果。

Conclusion: 本文提出的基于一阶统计性质的轻量级后训练剪枝框架在不需要重训练或复杂计算的情况下，提高了大语言模型的剪枝性能。

Abstract: Post-training pruning is an effective approach for reducing the size and inference cost of large language models (LLMs), but existing methods often face a trade-off between pruning quality and computational efficiency. Heuristic pruning methods are efficient but sensitive to activation outliers, while reconstruction-based approaches improve fidelity at the cost of heavy computation. In this work, we propose a lightweight post-training pruning framework based on first-order statistical properties of model weights and activations. During pruning, channel-wise statistics are used to calibrate magnitude-based importance scores, reducing bias from activation-dominated channels. After pruning, we apply an analytic energy compensation to correct distributional distortions caused by weight removal. Both steps operate without retraining, gradients, or second-order information. Experiments across multiple LLM families, sparsity patterns, and evaluation tasks show that the proposed approach improves pruning performance while maintaining computational cost comparable to heuristic methods. The results suggest that simple statistical corrections can be effective for post-training pruning of LLMs.

</details>


### [16] [Do Large Language Models Reflect Demographic Pluralism in Safety?](https://arxiv.org/abs/2602.07376)
*Usman Naseem,Gautam Siddharth Kashyap,Sushant Kumar Ray,Rafiq Ali,Ebad Shabbir,Abdullah Mohammad*

Main category: cs.CL

TL;DR: 本研究提出了Demo-SafetyBench数据集，通过在提示层面引入人口统计多元性，实现了对大语言模型安全性的多元化与稳健评估，显著提升了评估的可靠性和公平性。


<details>
  <summary>Details</summary>
Motivation: 现有的对齐数据集人口统计单一，忽视了不同社区对安全性的多样化感知，迫切需要一种考虑人口统计多元性、反映不同文化和价值观的安全评估方法。

Method: 利用两个阶段方法：第一阶段使用Mistral 7B-Instruct-v0.3模型将DICES数据集中的提示重新分类为14个安全域，并结合Llama-3.1-8B-Instruct模型扩展低资源安全领域，通过SimHash降重，生成了43050条样本；第二阶段采用多种大语言模型（Gemma-7B，GPT-4o，LLaMA-2-7B）零样本推断对多元安全敏感性进行评估。

Result: 通过设置平衡阈值（delta=0.5，tau=10），实现了高度可靠的评分一致性（ICC=0.87）和较低的人口统计敏感性（DS=0.12），验证了该方法的可扩展性和人口统计稳健性。

Conclusion: 该研究表明，通过在提示层面直接建模人口统计多元性，可以实现对大语言模型安全性的多元化评估，并且该方法具有高可靠性和较低的人口统计敏感性。

Abstract: Large Language Model (LLM) safety is inherently pluralistic, reflecting variations in moral norms, cultural expectations, and demographic contexts. Yet, existing alignment datasets such as ANTHROPIC-HH and DICES rely on demographically narrow annotator pools, overlooking variation in safety perception across communities. Demo-SafetyBench addresses this gap by modeling demographic pluralism directly at the prompt level, decoupling value framing from responses. In Stage I, prompts from DICES are reclassified into 14 safety domains (adapted from BEAVERTAILS) using Mistral 7B-Instruct-v0.3, retaining demographic metadata and expanding low-resource domains via Llama-3.1-8B-Instruct with SimHash-based deduplication, yielding 43,050 samples. In Stage II, pluralistic sensitivity is evaluated using LLMs-as-Raters-Gemma-7B, GPT-4o, and LLaMA-2-7B-under zero-shot inference. Balanced thresholds (delta = 0.5, tau = 10) achieve high reliability (ICC = 0.87) and low demographic sensitivity (DS = 0.12), confirming that pluralistic safety evaluation can be both scalable and demographically robust.

</details>


### [17] [When the Model Said 'No Comment', We Knew Helpfulness Was Dead, Honesty Was Alive, and Safety Was Terrified](https://arxiv.org/abs/2602.07381)
*Gautam Siddharth Kashyap,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: 针对大型语言模型多目标对齐中轴坍塌问题，AlignX提出两阶段方法有效改善模型性能和推理可靠性，实现更安全更符合人类价值的模型行为。


<details>
  <summary>Details</summary>
Motivation: 现有多目标对齐方法如SFT和MoE在处理冲突目标时存在挑战，表现为特征空间不连贯导致灾难性遗忘和专家路由失准导致推理不可靠，即轴坍塌问题。

Method: 提出AlignX两阶段框架：阶段一通过提示注入微调提取轴特定任务特征，减少灾难性遗忘；阶段二采用MoCaE模块利用分形和自然几何校准专家路由，提高推理可靠性。

Result: AlignX在多个数据集上显著提升了帮助性、无害性和诚实性指标，赢率提升171.5%，真实性信息提升110.1%，安全违规减少4.3%，并降低了35%以上的延迟和内存使用，验证了其通用性。

Conclusion: AlignX框架通过两阶段方法有效解决了LLM多目标对齐中的轴坍塌问题，实现了与人类价值观（有用性、无害性、诚实性）的更好一致性。

Abstract: Large Language Models (LLMs) need to be in accordance with human values-being helpful, harmless, and honest (HHH)-is important for safe deployment. Existing works use Supervised Fine-Tuning (SFT) and Mixture-of-Experts (MoE) to align LLMs. However, these works face challenges in multi-objective settings, such as SFT leading to interference between conflicting objectives, while MoEs suffer from miscalibrated routing. We term this failure mode Axis Collapse, marked by (1) disjoint feature spaces causing catastrophic forgetting, and (2) unreliable inference from misrouted experts. To resolve this, we propose AlignX, a two-stage framework. Stage 1 uses prompt-injected fine-tuning to extract axis-specific task features, mitigating catastrophic forgetting. Stage 2 deploys a MoCaE module that calibrates expert routing using fractal and natural geometry, improving inference reliability. AlignX achieves significant gains on Alpaca (Helpfulness), BeaverTails (Harmlessness), and TruthfulQA (Honesty), with +171.5% win rate, +110.1% in truthfulness-informativeness, and 4.3% fewer safety violations. It also reduces latency and memory usage by over 35% compared to prior MoEs. Results across four LLMs validate its generalizability.

</details>


### [18] [Advantages of Domain Knowledge Injection for Legal Document Summarization: A Case Study on Summarizing Indian Court Judgments in English and Hindi](https://arxiv.org/abs/2602.07382)
*Debtanu Datta,Rajdeep Mukherjee,Adrijit Goswami,Saptarshi Ghosh*

Main category: cs.CL

TL;DR: 本研究通过注入法律领域知识，提升了印度法律文本用英语和印地语的自动摘要效果，方法有效且经专家验证。


<details>
  <summary>Details</summary>
Motivation: 印度法律判决文本语言复杂且结构不规则，同时多数印度人难以理解法律英语，需要生成印地语等印度语言的摘要。

Method: 提出一种注入法律领域知识的框架，通过结合领域特定的预训练编码器增强抽取式神经摘要模型，并通过在大规模英印法律语料上持续预训练，将法律知识注入生成式模型（包括大型语言模型）。

Result: 在英印法律文本摘要任务中，提出的方法在标准评测指标、事实一致性指标和法律领域特定指标上取得显著提升，且经领域专家验证其有效性。

Conclusion: 通过注入法律领域知识的方式，有效提升了印度法律文本的英印双语摘要质量，满足了法律文本复杂性和语言多样性的需求。

Abstract: Summarizing Indian legal court judgments is a complex task not only due to the intricate language and unstructured nature of the legal texts, but also since a large section of the Indian population does not understand the complex English in which legal text is written, thus requiring summaries in Indian languages. In this study, we aim to improve the summarization of Indian legal text to generate summaries in both English and Hindi (the most widely spoken Indian language), by injecting domain knowledge into diverse summarization models. We propose a framework to enhance extractive neural summarization models by incorporating domain-specific pre-trained encoders tailored for legal texts. Further, we explore the injection of legal domain knowledge into generative models (including Large Language Models) through continual pre-training on large legal corpora in English and Hindi. Our proposed approaches achieve statistically significant improvements in both English-to-English and English-to-Hindi Indian legal document summarization, as measured by standard evaluation metrics, factual consistency metrics, and legal domain-specific metrics. Furthermore, these improvements are validated through domain experts, demonstrating the effectiveness of our approaches.

</details>


### [19] [Measuring cross-language intelligibility between Romance languages with computational tools](https://arxiv.org/abs/2602.07447)
*Liviu P Dinu,Ana Sabina Uban,Bogdan Iordache,Anca Dinu,Simona Georgescu*

Main category: cs.CL

TL;DR: 提出一种基于词汇相似度的计算方法，量化罗曼语族五种语言间的相互理解度，验证了方法有效性且结果与人类实验一致。


<details>
  <summary>Details</summary>
Motivation: 探究罗曼语族语言之间的相互理解度，为语言学和自然语言处理提供量化评估工具。

Method: 使用基于词汇相似度的计算指标，通过考察词的表面形态及语义相似性，结合正写法和语音形式，以及平行语料库和词向量模型来测量语言间的相互理解度。

Result: 五种主要罗曼语言（法语、意大利语、葡萄牙语、西班牙语、罗马尼亚语）在不同表示形式和模型下的相互理解度得分，证实了语言间理解的不对称性，并与人类测试结果高度相关。

Conclusion: 本研究开发的计算指标能够有效量化罗曼语族语言间的相互理解度，结果与人类实验的测试存在显著相关性。

Abstract: We present an analysis of mutual intelligibility in related languages applied for languages in the Romance family. We introduce a novel computational metric for estimating intelligibility based on lexical similarity using surface and semantic similarity of related words, and use it to measure mutual intelligibility for the five main Romance languages (French, Italian, Portuguese, Spanish, and Romanian), and compare results using both the orthographic and phonetic forms of words as well as different parallel corpora and vectorial models of word meaning representation. The obtained intelligibility scores confirm intuitions related to intelligibility asymmetry across languages and significantly correlate with results of cloze tests in human experiments.

</details>


### [20] [DLLM Agent: See Farther, Run Faster](https://arxiv.org/abs/2602.07451)
*Huiling Zhen,Weizhe Lin,Renxi Liu,Kai Han,Yiming Li,Yuchuan Tian,Hanting Chen,Xiaoguang Li,Xiaosong Li,Chen Chen,Xianzhi Yu,Mingxuan Yuan,Youliang Yan,Peifeng Qin,Jun Wang,Yu Wang,Dacheng Tao,Yunhe Wang*

Main category: cs.CL

TL;DR: 本文研究了扩散大语言模型用于多步代理决策的效果，发现DLLM模型相比传统自回归模型在效率和规划表现上具有明显优势，但也需针对工具调用失败和多回合输入进行特殊训练和设计。


<details>
  <summary>Details</summary>
Motivation: 探讨在多步代理决策中，采用扩散生成范式是否会引起规划和工具使用行为的系统性差异，并评估这些差异是否带来端到端效率提升。

Method: 在统一代理工作流（DeepDiver）中，将DLLM和AR模型分别作为骨干网络进行匹配的代理细调，并在相同轨迹数据上进行比较。

Result: DLLM代理在准确率相当的条件下，平均比AR代理快30%以上，部分情况加速超过8倍。DLLM代理在正确完成任务时，交互回合和工具调用次数更少，规划器命中率更高且收敛更快。此外，发现DLLM模型需针对工具调用进行专门训练，并对多回合输入的注意力掩码设计提出了改进建议。

Conclusion: 扩散大语言模型（DLLMs）在相同的代理框架和监督条件下，相较于自回归（AR）模型，能显著提升多步决策过程的效率，体现为更快的任务完成速度和更少的交互回合及工具调用。

Abstract: Diffusion large language models (DLLMs) have emerged as an alternative to autoregressive (AR) decoding with appealing efficiency and modeling properties, yet their implications for agentic multi-step decision making remain underexplored. We ask a concrete question: when the generation paradigm is changed but the agent framework and supervision are held fixed, do diffusion backbones induce systematically different planning and tool-use behaviors, and do these differences translate into end-to-end efficiency gains? We study this in a controlled setting by instantiating DLLM and AR backbones within the same agent workflow (DeepDiver) and performing matched agent-oriented fine-tuning on the same trajectory data, yielding diffusion-backed DLLM Agents and directly comparable AR agents. Across benchmarks and case studies, we find that, at comparable accuracy, DLLM Agents are on average over 30% faster end to end than AR agents, with some cases exceeding 8x speedup. Conditioned on correct task completion, DLLM Agents also require fewer interaction rounds and tool invocations, consistent with higher planner hit rates that converge earlier to a correct action path with less backtracking. We further identify two practical considerations for deploying diffusion backbones in tool-using agents. First, naive DLLM policies are more prone to structured tool-call failures, necessitating stronger tool-call-specific training to emit valid schemas and arguments. Second, for multi-turn inputs interleaving context and action spans, diffusion-style span corruption requires aligned attention masking to avoid spurious context-action information flow; without such alignment, performance degrades. Finally, we analyze attention dynamics across workflow stages and observe paradigm-specific coordination patterns, suggesting stronger global planning signals in diffusion-backed agents.

</details>


### [21] [SED-SFT: Selectively Encouraging Diversity in Supervised Fine-Tuning](https://arxiv.org/abs/2602.07464)
*Yijie Chen,Yijin Liu,Fandong Meng*

Main category: cs.CL

TL;DR: SED-SFT通过选择性熵正则化解决了监督微调中的模式崩溃问题，提升了生成多样性和后续强化学习性能，且计算成本低。


<details>
  <summary>Details</summary>
Motivation: 传统的交叉熵损失驱动的监督微调导致模型集中于特定响应模式，缺乏分布多样性，限制了后续强化学习的探索效率。

Method: 提出了基于令牌探索空间自适应鼓励多样性的选择性熵正则化项和选择性掩码机制，并将其集成到优化目标中进行监督微调。

Result: 在八个数学基准测试中，SED-SFT在Llama-3.2-3B-Instruct和Qwen2.5-Math-7B-Instruct模型上分别提升了后续强化学习性能2.06和1.20点，且计算开销增加极小。

Conclusion: SED-SFT通过引入选择性熵正则项和掩码机制，有效解决了传统交叉熵驱动的监督微调中模式崩溃问题，显著提升了生成多样性和后续强化学习的表现。

Abstract: Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) has emerged as the standard post-training paradigm for large language models (LLMs). However, the conventional SFT process, driven by Cross-Entropy (CE) loss, often induces mode collapse, where models over-concentrate on specific response patterns. This lack of distributional diversity severely restricts the exploration efficiency required for subsequent RL. While recent studies have attempted to improve SFT by replacing the CE loss, aiming to preserve diversity or refine the update policy, they fail to adequately balance diversity and accuracy, thereby yielding suboptimal performance after RL. To address the mode collapse problem, we propose SED-SFT, which adaptively encourages diversity based on the token exploration space. This framework introduces a selective entropy regularization term with a selective masking mechanism into the optimization objective. Extensive experiments across eight mathematical benchmarks demonstrate that SED-SFT significantly enhances generation diversity with a negligible computational overhead increase compared with CE loss, yielding average improvements of 2.06 and 1.20 points in subsequent RL performance over standard CE-based baselines on Llama-3.2-3B-Instruct and Qwen2.5-Math-7B-Instruct, respectively. The code is publicly available at https://github.com/pppa2019/SED-SFT

</details>


### [22] [From Native Memes to Global Moderation: Cros-Cultural Evaluation of Vision-Language Models for Hateful Meme Detection](https://arxiv.org/abs/2602.07497)
*Mo Wang,Kaixuan Ren,Pratik Jalan,Ahmed Ashraf,Tuong Vy Vu,Rahul Seetharaman,Shah Nawaz,Usman Naseem*

Main category: cs.CL

TL;DR: 本文揭示视觉语言模型存在文化偏差，提出跨文化评估框架，发现本土语言提示和一次学习能有效提升模型的跨文化适应性。


<details>
  <summary>Details</summary>
Motivation: 文化背景深刻影响人们对网络内容的解读，但视觉语言模型主要由西方视角训练，限制了其在多文化环境中的使用效果。

Method: 提出一个系统评估框架，基于多语言网络迷因数据集，从学习策略、提示语言和翻译效果三个维度评估跨文化稳健性。

Result: “先翻译再检测”方法表现下降；本土语言提示和一次学习显著提升跨文化检测能力；揭示模型向西方安全规范的系统性趋同。

Conclusion: 现有视觉语言模型存在文化偏见，倾向于西方或英语视角，影响其在跨文化情境下的公平性和稳健性。

Abstract: Cultural context profoundly shapes how people interpret online content, yet vision-language models (VLMs) remain predominantly trained through Western or English-centric lenses. This limits their fairness and cross-cultural robustness in tasks like hateful meme detection. We introduce a systematic evaluation framework designed to diagnose and quantify the cross-cultural robustness of state-of-the-art VLMs across multilingual meme datasets, analyzing three axes: (i) learning strategy (zero-shot vs. one-shot), (ii) prompting language (native vs. English), and (iii) translation effects on meaning and detection. Results show that the common ``translate-then-detect'' approach deteriorate performance, while culturally aligned interventions - native-language prompting and one-shot learning - significantly enhance detection. Our findings reveal systematic convergence toward Western safety norms and provide actionable strategies to mitigate such bias, guiding the design of globally robust multimodal moderation systems.

</details>


### [23] [Let's Simplify Step by Step: Guiding LLM Towards Multilingual Unsupervised Proficiency-Controlled Sentence Simplification](https://arxiv.org/abs/2602.07499)
*Jingshen Zhang,Xin Ying Qiu,Lifang Lu,Zhuhua Huang,Yutao Hu,Yuechang Wu,JunYu Lu*

Main category: cs.CL

TL;DR: 提出一步步分解简化方法，提高跨语言句子简化效果，减少计算量，但语义保持难题仍未解决。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在大跨度可读性级别的句子简化能力有限，需提高简化效果及控制力。

Method: 通过动态路径规划、语义感知示例选择和结合对话历史的链式思考生成，将复杂简化分解为多个步骤。

Result: 在五种语言和两个基准测试中，该方法有效提升简化效果，减少22-42%计算步骤。人工评估显示简化效果与语义保持存在权衡。

Conclusion: 逐步简化能改善可控性，但在大幅度简化时语义保持仍是难点。

Abstract: Large language models demonstrate limited capability in proficiency-controlled sentence simplification, particularly when simplifying across large readability levels. We propose a framework that decomposes complex simplifications into manageable steps through dynamic path planning, semantic-aware exemplar selection, and chain-of-thought generation with conversation history for coherent reasoning. Evaluation on five languages across two benchmarks shows our approach improves simplification effectiveness while reducing computational steps by 22-42%. Human evaluation confirms the fundamental trade-off between simplification effectiveness and meaning preservation. Notably, even human annotators struggle to agree on semantic preservation judgments, highlighting the inherent complexity of this task. Our work shows that while step-by-step simplification improves control, preserving semantic fidelity during extensive simplification remains an open challenge.

</details>


### [24] [Improving Variable-Length Generation in Diffusion Language Models via Length Regularization](https://arxiv.org/abs/2602.07546)
*Zicong Cheng,Ruixuan Jia,Jia Li,Guo-Wei Yang,Meng-Hao Guo,Shi-Min Hu*

Main category: cs.CL

TL;DR: 本文针对DLLMs无法高效处理未知长度生成问题，提出了LR-DLLM长度正则化推理框架，有效校正长度偏差，显著提升生成质量和长度判断能力，实现了无需修改模型结构的可变长度推理。


<details>
  <summary>Details</summary>
Motivation: 现有扩散大语言模型（DLLMs）在处理变量长度生成时表现不佳，因为其推理基于固定长度且需要预先知道目标长度，这在实际生成如补全和填充时不切实际。

Method: 提出LR-DLLM，一种长度正则化推理框架，将生成长度作为显式变量，并通过长度正则化校正生成置信度估计中的长度偏差，实现长度的动态扩展或收缩，无需修改模型结构或训练方法。

Result: LR-DLLM在完全未知生成长度条件下，HumanEval-Infilling任务Pass@1达到51.3%，比DreamOn提升13.4%；在四语言McEval测试中平均Pass@1为51.5%，提升14.3%。

Conclusion: LR-DLLM成功解决了DLLMs在变量长度生成中的长度偏差问题，实现了可靠的长度判定和高效的生成质量，提升了模型的实际应用能力。

Abstract: Diffusion Large Language Models (DLLMs) are inherently ill-suited for variable-length generation, as their inference is defined on a fixed-length canvas and implicitly assumes a known target length. When the length is unknown, as in realistic completion and infilling, naively comparing confidence across mask lengths becomes systematically biased, leading to under-generation or redundant continuations. In this paper, we show that this failure arises from an intrinsic lengthinduced bias in generation confidence estimates, leaving existing DLLMs without a robust way to determine generation length and making variablelength inference unreliable. To address this issue, we propose LR-DLLM, a length-regularized inference framework for DLLMs that treats generation length as an explicit variable and achieves reliable length determination at inference time. It decouples semantic compatibility from lengthinduced uncertainty through an explicit length regularization that corrects biased confidence estimates. Based on this, LR-DLLM enables dynamic expansion or contraction of the generation span without modifying the underlying DLLM or its training procedure. Experiments show that LRDLLM achieves 51.3% Pass@1 on HumanEvalInfilling under fully unknown lengths (+13.4% vs. DreamOn) and 51.5% average Pass@1 on four-language McEval (+14.3% vs. DreamOn).

</details>


### [25] [Learning to Self-Verify Makes Language Models Better Reasoners](https://arxiv.org/abs/2602.07594)
*Yuxin Chen,Yu Wang,Yi Zhang,Ziang Ye,Zhengzhou Cai,Yaorui Shi,Qi Gu,Hui Su,Xunliang Cai,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.CL

TL;DR: 该论文发现大语言模型生成能力和自我验证能力存在不对称性，提出通过多任务强化学习融合两者，显著提升模型的生成和验证性能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在复杂任务的推理路径生成上表现优秀，但在自我验证答案方面表现较弱，存在能力不对称现象，需探索二者关系及提升策略。

Method: 构建多任务强化学习框架，将生成和自我验证视为两个独立但互补的优化目标，联合训练以提升整体性能。

Result: 通过多任务强化学习训练，模型在生成和自我验证两个任务上的表现均优于单独训练生成模型，达到更高准确率并生成更高效有效的推理轨迹。

Conclusion: 生成能力与自我验证能力存在显著的非对称性，提升生成能力并不一定提高自我验证能力，但提升自我验证能力可以反向促进生成能力的提升。

Abstract: Recent large language models (LLMs) achieve strong performance in generating promising reasoning paths for complex tasks. However, despite powerful generation ability, LLMs remain weak at verifying their own answers, revealing a persistent capability asymmetry between generation and self-verification. In this work, we conduct an in-depth investigation of this asymmetry throughout training evolution and show that, even on the same task, improving generation does not lead to corresponding improvements in self-verification. Interestingly, we find that the reverse direction of this asymmetry behaves differently: learning to self-verify can effectively improve generation performance, achieving accuracy comparable to standard generation training while yielding more efficient and effective reasoning traces. Building on this observation, we further explore integrating self-verification into generation training by formulating a multi-task reinforcement learning framework, where generation and self-verification are optimized as two independent but complementary objectives. Extensive experiments across benchmarks and models demonstrate performance gains over generation-only training in both generation and verification capabilities.

</details>


### [26] [SciClaimEval: Cross-modal Claim Verification in Scientific Papers](https://arxiv.org/abs/2602.07621)
*Xanh Ho,Yun-Ang Wu,Sunisth Kumar,Tian Cheng Xia,Florian Boudin,Andre Greiner-Petter,Akiko Aizawa*

Main category: cs.CL

TL;DR: SciClaimEval是一个包含真实科学声明和跨模态证据的新数据集，通过新方法生成反驳声明，对多模态模型进行了评测，结果显示图像验证任务依然具有挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有科学声明验证数据集缺乏真实且多模态（图像和表格）证据，且反驳声明多靠合成或修改声明本身，难以反映真实场景。

Method: 通过从发表的论文中直接提取真实声明，采用新颖方法通过修改支持证据（图表和表格）生成反驳声明，而非改变声明文本或借助大语言模型制造矛盾。

Result: 构建了包含1664个标注样本，涵盖180篇论文和三个领域的数据集，并对11个多模态基础模型进行了基准测试，发现所有模型在基于图像的验证上仍存较大性能差距。

Conclusion: SciClaimEval 数据集提供了真实且多模态的科学声明验证资源，尤其是在图像和表格的跨模态证据方面，现有模型在图像（图表）验证任务上仍表现较差。

Abstract: We present SciClaimEval, a new scientific dataset for the claim verification task. Unlike existing resources, SciClaimEval features authentic claims, including refuted ones, directly extracted from published papers. To create refuted claims, we introduce a novel approach that modifies the supporting evidence (figures and tables), rather than altering the claims or relying on large language models (LLMs) to fabricate contradictions. The dataset provides cross-modal evidence with diverse representations: figures are available as images, while tables are provided in multiple formats, including images, LaTeX source, HTML, and JSON. SciClaimEval contains 1,664 annotated samples from 180 papers across three domains, machine learning, natural language processing, and medicine, validated through expert annotation. We benchmark 11 multimodal foundation models, both open-source and proprietary, across the dataset. Results show that figure-based verification remains particularly challenging for all models, as a substantial performance gap remains between the best system and human baseline.

</details>


### [27] [Letting Tutor Personas "Speak Up" for LLMs: Learning Steering Vectors from Dialogue via Preference Optimization](https://arxiv.org/abs/2602.07639)
*Jaewook Lee,Alexander Scarlatos,Simon Woodhead,Andrew Lan*

Main category: cs.CL

TL;DR: 本文通过修改BiPO学习激活空间方向，引导LLMs展现多样辅导者风格，有效提升语义对齐与用户偏好，提供对辅导风格变化的可解释控制方法。


<details>
  <summary>Details</summary>
Motivation: 传统LLM辅导研究通常只学习单一辅导策略，忽视了教学风格的多样性，而现实中辅导者通过调整不同的教学策略满足学生需求，影响对话动态和学生参与度。

Method: 对双向偏好优化(BiPO)方法进行修改，学习激活空间中的steering vector以引导模型生成特定辅导者风格的回答，无需显式提示指令。

Result: 学习到的steering vector能够捕捉不同辅导者在对话中的行为差异，显著改善模型生成回答的语义对齐和用户偏好评估，揭示了辅导风格的结构化差异。

Conclusion: 通过激活空间方向(steering vector)引导大语言模型(LLMs)表现出不同的辅导者风格，有效提高了模型对人类辅导语句的语义一致性和偏好评分，同时保持词汇相似性，且辅导行为差异具有可解释性。

Abstract: With the emergence of large language models (LLMs) as a powerful class of generative artificial intelligence (AI), their use in tutoring has become increasingly prominent. Prior works on LLM-based tutoring typically learn a single tutor policy and do not capture the diversity of tutoring styles. In real-world tutor-student interactions, pedagogical intent is realized through adaptive instructional strategies, with tutors varying the level of scaffolding, instructional directiveness, feedback, and affective support in response to learners' needs. These differences can all impact dialogue dynamics and student engagement. In this paper, we explore how tutor personas embedded in human tutor-student dialogues can be used to guide LLM behavior without relying on explicitly prompted instructions. We modify Bidirectional Preference Optimization (BiPO) to learn a steering vector, an activation-space direction that steers model responses towards certain tutor personas. We find that this steering vector captures tutor-specific variation across dialogue contexts, improving semantic alignment with ground-truth tutor utterances and increasing preference-based evaluations, while largely preserving lexical similarity. Analysis of the learned directional coefficients further reveals interpretable structure across tutors, corresponding to consistent differences in tutoring behavior. These results demonstrate that activation steering offers an effective and interpretable way for controlling tutor-specific variation in LLMs using signals derived directly from human dialogue data.

</details>


### [28] [Blind to the Human Touch: Overlap Bias in LLM-Based Summary Evaluation](https://arxiv.org/abs/2602.07673)
*Jiangnan Fang,Cheng-Tse Liu,Hanieh Deilamsalehy,Nesreen K. Ahmed,Puneet Mathur,Nedim Lipka,Franck Dernoncourt,Ryan A. Rossi*

Main category: cs.CL

TL;DR: 研究表明大型语言模型作为评价者在判断总结质量时存在偏好与局限，随着与人工摘要重叠度降低，模型更偏爱机器生成的总结，建议未来评判应结合更多技术手段。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM评判因其更好语义捕捉和推理能力被广泛用于总结评价，但其偏差和对抗脆弱性不足够被细致研究，因此本研究旨在深入探讨LLM评判对总结与人工摘要重叠度的敏感性。

Method: 分析9个不同参数规模（10亿到120亿）的大型语言模型，包括Gemma 3和LLaMA 3变体，通过与人工撰写总结的重叠度（ROUGE和BLEU）来研究LLM评判偏差。

Result: 发现所有测试模型中，LLM评判随着与人工总结相似度降低，更偏爱LLM生成的总结，且模型判断即使在总结重叠较少时也表现不佳，表明仅用简单比较无法充分评价总结质量。

Conclusion: LLM评判在总结任务中存在明显偏好，随着被评估总结与人工总结的相似度下降，LLM评判更倾向于选择由其他LLM生成的总结，且这种偏好独立于模型自身的排序偏差。

Abstract: Large language model (LLM) judges have often been used alongside traditional, algorithm-based metrics for tasks like summarization because they better capture semantic information, are better at reasoning, and are more robust to paraphrasing. However, LLM judges show biases for length and order among others, and are vulnerable to various adversarial input prompts. While recent studies have looked into these biases, few have analyzed them at a more granular level in relation to a well-defined overlap metric. In this work we provide an LLM judge bias analysis as a function of overlap with human-written responses in the domain of summarization. We test 9 recent LLMs with parameter counts ranging from 1 billion to 12 billion, including variants of Gemma 3 and LLaMA 3. We find that LLM judges increasingly prefer summaries generated by other LLMs over those written by humans as the similarities (as measured by ROUGE and BLEU) between the judged summaries decrease, and this pattern extends to all but one model tested, and exists regardless of the models' own position biases. Additionally, we find that models struggle to judge even summaries with limited overlaps, suggesting that LLM-as-a-judge in the summary domain should rely on techniques beyond a simple comparison.

</details>


### [29] [SRR-Judge: Step-Level Rating and Refinement for Enhancing Search-Integrated Reasoning in Search Agents](https://arxiv.org/abs/2602.07773)
*Chen Zhang,Kuicai Dong,Dexun Li,Wenjun Li,Qu Yang,Wei Han,Yong Liu*

Main category: cs.CL

TL;DR: 针对深度搜索代理推理中间步骤缺乏有效监督的问题，SRR-Judge框架通过细粒度步骤评估和标注提升了推理质量，显著增强了搜索代理性能。


<details>
  <summary>Details</summary>
Motivation: 当前主流的深度搜索代理训练仅依赖结果导向监督，忽略了中间思考和行动的质量，影响推理过程的可靠性和效率。

Method: 提出SRR-Judge框架，结合修改后的ReAct风格的rate-and-refine工作流程，实现逐步推理和搜索行动的细粒度评价，并利用SRR标注数据通过迭代拒绝采样进行微调优化。

Result: SRR-Judge在步骤级评价上优于更大规模模型（如DeepSeek-V3.1），其评分与最终答案正确性强相关，微调后的代理在多个复杂深度搜索基准上有超过10%的绝对通过率提升。

Conclusion: SRR-Judge框架有效提升了深度搜索代理的推理和搜索能力，通过细粒度的逐步评估和标注，实现了更可靠的行动质量判断和显著的性能提升。

Abstract: Recent deep search agents built on large reasoning models (LRMs) excel at complex question answering by iteratively planning, acting, and gathering evidence, a capability known as search-integrated reasoning. However, mainstream approaches often train this ability using only outcome-based supervision, neglecting the quality of intermediate thoughts and actions. We introduce SRR-Judge, a framework for reliable step-level assessment of reasoning and search actions. Integrated into a modified ReAct-style rate-and-refine workflow, SRR-Judge provides fine-grained guidance for search-integrated reasoning and enables efficient post-training annotation. Using SRR-annotated data, we apply an iterative rejection sampling fine-tuning procedure to enhance the deep search capability of the base agent. Empirically, SRR-Judge delivers more reliable step-level evaluations than much larger models such as DeepSeek-V3.1, with its ratings showing strong correlation with final answer correctness. Moreover, aligning the policy with SRR-Judge annotated trajectories leads to substantial performance gains, yielding over a 10 percent average absolute pass@1 improvement across challenging deep search benchmarks.

</details>


### [30] [Attn-GS: Attention-Guided Context Compression for Efficient Personalized LLMs](https://arxiv.org/abs/2602.07778)
*Shenglai Zeng,Tianqi Zheng,Chuan Tian,Dante Everaert,Yau-Shian Wang,Yupin Huang,Michael J. Morais,Rohit Patki,Jinjin Tian,Xinnan Dai,Kai Guo,Monica Xiao Cheng,Hui Liu*

Main category: cs.CL

TL;DR: 本文提出基于注意力机制的个性化上下文压缩方法Attn-GS，显著减少输入令牌，提升推理效率，同时保持接近完整上下文的效果。


<details>
  <summary>Details</summary>
Motivation: 由于个性化LLM需要融入大量用户交互历史和个人资料，输入令牌限制导致高延迟和高API成本，现有启发式压缩方法忽视了LLM内部对不同上下文成分的处理机制，难以高效提取关键信息。

Method: 该方法基于对LLM注意力模式的分析，使用标记模型识别重要个性化句子，进而引导压缩模型生成与任务相关且高质量的压缩上下文，结合微调提升模型区分重要信息的能力。

Result: 实验结果表明，Attn-GS在多种任务和设置下均显著优于现有基线方法，能够在减少50倍令牌使用的同时，性能保持接近使用完整上下文。

Conclusion: 本文提出的Attn-GS方法，通过利用大型语言模型的注意力模式，实现了对用户个性化上下文的智能压缩，在保持接近完整上下文性能的同时，显著减少了输入令牌数，提升了推理效率和降低了成本。

Abstract: Personalizing large language models (LLMs) to individual users requires incorporating extensive interaction histories and profiles, but input token constraints make this impractical due to high inference latency and API costs. Existing approaches rely on heuristic methods such as selecting recent interactions or prompting summarization models to compress user profiles. However, these methods treat context as a monolithic whole and fail to consider how LLMs internally process and prioritize different profile components. We investigate whether LLMs' attention patterns can effectively identify important personalization signals for intelligent context compression. Through preliminary studies on representative personalization tasks, we discover that (a) LLMs' attention patterns naturally reveal important signals, and (b) fine-tuning enhances LLMs' ability to distinguish between relevant and irrelevant information. Based on these insights, we propose Attn-GS, an attention-guided context compression framework that leverages attention feedback from a marking model to mark important personalization sentences, then guides a compression model to generate task-relevant, high-quality compressed user contexts. Extensive experiments demonstrate that Attn-GS significantly outperforms various baselines across different tasks, token limits, and settings, achieving performance close to using full context while reducing token usage by 50 times.

</details>


### [31] [Emergent Structured Representations Support Flexible In-Context Inference in Large Language Models](https://arxiv.org/abs/2602.07794)
*Ningyu Xu,Qi Zhang,Xipeng Qiu,Xuanjing Huang*

Main category: cs.CL

TL;DR: 本研究发现大型语言模型在推理时会动态构建并利用结构化的概念子空间，该子空间在多个层级通过注意力机制形成并促成模型预测，表明模型内部存在类似人类的推理表征。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型（LLMs）是否依赖结构化的、类似人类的概念表示进行推理。

Method: 通过对LLMs在上下文中进行概念推断时的内部处理进行观察，采用因果中介分析和层级注意机制分析，揭示模型的表现方式。

Result: 发现中后层出现了保持跨上下文一致的概念子空间，该子空间在因果层面上对模型预测至关重要。早中层的注意力头负责构建和细化该子空间，后层利用其生成预测。

Conclusion: LLMs动态构建并使用结构化的潜在概念表示进行推断，表明其推理过程依赖于这些内部结构化表征。

Abstract: Large language models (LLMs) exhibit emergent behaviors suggestive of human-like reasoning. While recent work has identified structured, human-like conceptual representations within these models, it remains unclear whether they functionally rely on such representations for reasoning. Here we investigate the internal processing of LLMs during in-context concept inference. Our results reveal a conceptual subspace emerging in middle to late layers, whose representational structure persists across contexts. Using causal mediation analyses, we demonstrate that this subspace is not merely an epiphenomenon but is functionally central to model predictions, establishing its causal role in inference. We further identify a layer-wise progression where attention heads in early-to-middle layers integrate contextual cues to construct and refine the subspace, which is subsequently leveraged by later layers to generate predictions. Together, these findings provide evidence that LLMs dynamically construct and use structured, latent representations in context for inference, offering insights into the computational processes underlying flexible adaptation.

</details>


### [32] [Thinking Makes LLM Agents Introverted: How Mandatory Thinking Can Backfire in User-Engaged Agents](https://arxiv.org/abs/2602.07796)
*Jiatong Li,Changdae Oh,Hyeong Kyu Choi,Jindong Wang,Sharon Li*

Main category: cs.CL

TL;DR: 本文研究了显式思考对用户参与的语言模型代理的影响，发现强制思考往往导致性能下降，原因在于信息披露减少。通过促进信息透明，性能得到提升。


<details>
  <summary>Details</summary>
Motivation: 调查显式思考技术在现实用户参与的语言模型代理场景中的有效性，解决其对复杂任务性能提升作用是否成立的疑问。

Method: 通过七个模型、三个基准测试和两种思考实例进行实验，结合定量的响应分类分析和定性的失败传播案例研究，对显式思考在用户参与的语言模型代理中的影响进行了全面的实验研究。

Result: 发现强制思考使得代理回复更短，信息披露减少，反而导致性能退化，同时提出主动促进信息披露的提示能显著提升多种模型的性能。

Conclusion: 强制性思考在用户参与的语言模型代理中常常适得其反，导致性能下降，主要原因是思考使代理变得“内向”，减少了信息披露，影响了代理与用户之间的信息交流，进而导致任务失败。

Abstract: Eliciting reasoning has emerged as a powerful technique for improving the performance of large language models (LLMs) on complex tasks by inducing thinking. However, their effectiveness in realistic user-engaged agent scenarios remains unclear. In this paper, we conduct a comprehensive study on the effect of explicit thinking in user-engaged LLM agents. Our experiments span across seven models, three benchmarks, and two thinking instantiations, and we evaluate them through both a quantitative response taxonomy analysis and qualitative failure propagation case studies. Contrary to expectations, we find that mandatory thinking often backfires on agents in user-engaged settings, causing anomalous performance degradation across various LLMs. Our key finding reveals that thinking makes agents more ``introverted'' by shortening responses and reducing information disclosure to users, which weakens agent-user information exchange and leads to downstream task failures. Furthermore, we demonstrate that explicitly prompting for information disclosure reliably improves performance across diverse model families, suggesting that proactive transparency is a vital lever for agent optimization. Overall, our study suggests that information transparency awareness is a crucial yet underexplored perspective for the future design of reasoning agents in real-world scenarios. Our code is available at https://github.com/deeplearning-wisc/Thinking-Agent.

</details>


### [33] [Pruning as a Cooperative Game: Surrogate-Assisted Layer Contribution Estimation for Large Language Models](https://arxiv.org/abs/2602.07804)
*Xuan Ding,Pengyu Tong,Ranjie Duan,Yunjian Zhang,Rui Sun,Yao Zhu*

Main category: cs.CL

TL;DR: 针对大语言模型层剪枝难点，本文提出结合博弈论和Shapley值的新方法，动态捕获层间依赖，显著提升剪枝性能。


<details>
  <summary>Details</summary>
Motivation: 当前层级剪枝方法依赖静态启发式规则，忽视层间依赖限制了剪枝效果，需设计动态且考虑依赖性的剪枝机制。

Method: 将层剪枝视为协作博弈，每层作为玩家，使用轻量级代理网络预测大语言模型不同层组合的性能，结合分层蒙特卡洛采样估计Shapley值。

Result: 通过大量实验验证，该方法在困惑度和零样本准确率上均优于现有方法，实现了更高效、更有效的大语言模型层级剪枝。

Conclusion: 本论文提出的基于博弈论的层级剪枝框架能够动态识别关键层，考虑层间依赖，显著提升了大语言模型剪枝的效果和效率。

Abstract: While large language models (LLMs) demonstrate impressive performance across various tasks, their deployment in real-world scenarios is still constrained by high computational demands. Layer-wise pruning, a commonly employed strategy to mitigate inference costs, can partially address this challenge. However, existing approaches generally depend on static heuristic rules and fail to account for the interdependencies among layers, thereby limiting the effectiveness of the pruning process. To this end, this paper proposes a game-theoretic framework that formulates layer pruning as a cooperative game in which each layer acts as a player and model performance serves as the utility. As computing exact Shapley values is computationally infeasible for large language models (LLMs), we propose using a lightweight surrogate network to estimate layer-wise marginal contributions. This network can predict LLM performance for arbitrary layer combinations at a low computational cost. Additionally, we employ stratified Monte Carlo mask sampling to further reduce the cost of Sharpley value estimation. This approach captures inter-layer dependencies and dynamically identifies critical layers for pruning. Extensive experiments demonstrate the consistent superiority of our method in terms of perplexity and zero-shot accuracy, achieving more efficient and effective layer-wise pruning for large language models.

</details>


### [34] [LLMs Know More About Numbers than They Can Say](https://arxiv.org/abs/2602.07812)
*Fengting Yuchi,Li Du,Jason Eisner*

Main category: cs.CL

TL;DR: 当前LLMs对混合符号数值大小的理解有限，但其隐藏状态中潜藏数值信息，通过线性探测和微调可以显著增强模型数值推理表现。


<details>
  <summary>Details</summary>
Motivation: 发现当前先进的大型语言模型（LLMs）在处理混合符号的数值比较时表现出错误，质疑模型是否真正理解数字大小。

Method: 通过探查多个小型开源LLM的隐藏状态，利用线性投影从隐藏层编码数值的对数大小，并用线性分类器进行数值排名。

Result: 成功恢复数值大小，对合成文本和科学论文的相对误差分别为2.3%和19.06%；排名准确率超过90%；将分类器的对数损失作为辅助目标进行微调，提升模型3.22%的准确率。

Conclusion: LLM隐藏状态中隐含了数值大小的信息，通过适当的线性探测可以提取且改进这种表示能有效提升模型的数值推理能力。

Abstract: Although state-of-the-art LLMs can solve math problems, we find that they make errors on numerical comparisons with mixed notation: "Which is larger, $5.7 \times 10^2$ or $580$?" This raises a fundamental question: Do LLMs even know how big these numbers are? We probe the hidden states of several smaller open-source LLMs. A single linear projection of an appropriate hidden layer encodes the log-magnitudes of both kinds of numerals, allowing us to recover the numbers with relative error of about 2.3% (on restricted synthetic text) or 19.06% (on scientific papers). Furthermore, the hidden state after reading a pair of numerals encodes their ranking, with a linear classifier achieving over 90% accuracy. Yet surprisingly, when explicitly asked to rank the same pairs of numerals, these LLMs achieve only 50-70% accuracy, with worse performance for models whose probes are less effective. Finally, we show that incorporating the classifier probe's log-loss as an auxiliary objective during finetuning brings an additional 3.22% improvement in verbalized accuracy over base models, demonstrating that improving models' internal magnitude representations can enhance their numerical reasoning capabilities.

</details>


### [35] [TodoEvolve: Learning to Architect Agent Planning Systems](https://arxiv.org/abs/2602.07839)
*Jiaxi Liu,Yanzuo Jiang,Guibin Zhang,Zihan Zhang,Heng Chang,Zhenfei Yin,Qibing Ren,Junchi Yan*

Main category: cs.CL

TL;DR: 本文提出TodoEvolve，通过模块化设计和多目标强化学习，实现了灵活自适应的自动规划架构生成，使智能体在复杂任务中表现优异且资源消耗低。


<details>
  <summary>Details</summary>
Motivation: 现有规划方法依赖固定和手工设计的规划结构，缺乏应对多样化复杂任务的灵活性。

Method: 提出TodoEvolve元规划范式，利用PlanFactory构建统一模块化的规划设计空间，并通过阻抗引导偏好优化的多目标强化学习训练Todo-14B，动态合成和调整任务特定规划架构。

Result: 在五个智能体基准测试中，TodoEvolve超越了传统手工设计的规划模块，同时保持较低的API调用成本和运行时开销。

Conclusion: TodoEvolve有效提升了规划系统的适应性和性能，为应对复杂多样任务提供了一种灵活且高效的自动化规划架构生成方案。

Abstract: Planning has become a central capability for contemporary agent systems in navigating complex, long-horizon tasks, yet existing approaches predominantly rely on fixed, hand-crafted planning structures that lack the flexibility to adapt to the structural diversity of open-ended problems. To address this limitation, we introduce TodoEvolve, a meta-planning paradigm that autonomously synthesizes and dynamically revises task-specific planning architectures. Specifically, we first construct PlanFactory, a modular design space that standardizes diverse planning paradigms within a unified codebase encompassing topology, initialization, adaptation, and navigation, thereby providing a common interface for heterogeneous planning patterns. Leveraging PlanFactory, we collect high-quality planning trajectories and train Todo-14B via \textit{Impedance-Guided Preference Optimization} (IGPO), a multi-objective reinforcement learning objective that encourages the generation of planning systems that are performant, stable, and token-efficient across arbitrary tasks and agent backbones. Empirical evaluations on five agentic benchmarks demonstrate that TodoEvolve consistently surpasses carefully engineered planning modules while maintaining economical API costs and runtime overhead.

</details>


### [36] [Evaluating and Calibrating LLM Confidence on Questions with Multiple Correct Answers](https://arxiv.org/abs/2602.07842)
*Yuhan Wang,Shiyu Ni,Zhikai Ding,Zihang Zhan,Yuanzi Li,Keping Bi*

Main category: cs.CL

TL;DR: 现有置信度校准方法在多答案问题中表现较差，本文提出基于多个高概率答案聚合的SCA方法，有效提升了大语言模型的置信度校准性能。


<details>
  <summary>Details</summary>
Motivation: 现有的置信度校准方法主要在单一答案问题上验证，面对多正确答案时容易低估置信度，造成可靠性下降。

Method: 引入了包含1.2万事实性问题的MACE基准，评估15种校准方法和4个大语言模型族的性能，提出了语义置信度聚合（SCA）方法以聚合多个高概率答案的置信度。

Result: 实验证明，随着正确答案数量的增加，准确率提升，但估计的置信度持续下降，造成严重的校准失衡；SCA方法在多答场景下达到了最优的校准效果，同时保持了单答场景的良好性能。

Conclusion: 现有的训练无关的置信度校准方法在多答案情况下表现不佳，导致置信度系统性低估；提出的SCA方法通过对多高概率答案进行置信度聚合，显著改善了校准效果。

Abstract: Confidence calibration is essential for making large language models (LLMs) reliable, yet existing training-free methods have been primarily studied under single-answer question answering. In this paper, we show that these methods break down in the presence of multiple valid answers, where disagreement among equally correct responses leads to systematic underestimation of confidence. To enable a systematic study of this phenomenon, we introduce MACE, a benchmark of 12,000 factual questions spanning six domains with varying numbers of correct answers. Experiments across 15 representative calibration methods and four LLM families (7B-72B) reveal that while accuracy increases with answer cardinality, estimated confidence consistently decreases, causing severe miscalibration for questions with mixed answer counts. To address this issue, we propose Semantic Confidence Aggregation (SCA), which aggregates confidence over multiple high-probability sampled responses. SCA achieves state-of-the-art calibration performance under mixed-answer settings while preserving strong calibration on single-answer questions.

</details>


### [37] [SparseEval: Efficient Evaluation of Large Language Models by Sparse Optimization](https://arxiv.org/abs/2602.07909)
*Taolin Zhang,Hang Guo,Wang Lu,Tao Dai,Shu-Tao Xia,Jindong Wang*

Main category: cs.CL

TL;DR: 本文提出SparseEval方法，通过稀疏优化和锚点选择，实现了大规模语言模型性能评估的高效低成本，实验验证了其准确性和实用性。


<details>
  <summary>Details</summary>
Motivation: 随着大规模语言模型性能不断提升，评估其能力变得计算成本高昂，需寻找有效且高效的评估方法。

Method: 提出了SparseEval方法，通过梯度下降优化锚点权重，采用迭代细化策略选择锚点，利用MLP进行稀疏优化，并设计了锚点重要性得分和候选重要性得分用于任务感知的细化。

Result: SparseEval在多个基准测试中表现出低估计误差和高Kendall's τ，显示出较强的鲁棒性和实用性。

Conclusion: SparseEval方法能够有效降低大规模语言模型性能评估的计算成本，保持较低的估计误差和较高的排名相关性，具备良好的鲁棒性和实用性。

Abstract: As large language models (LLMs) continue to scale up, their performance on various downstream tasks has significantly improved. However, evaluating their capabilities has become increasingly expensive, as performing inference on a large number of benchmark samples incurs high computational costs. In this paper, we revisit the model-item performance matrix and show that it exhibits sparsity, that representative items can be selected as anchors, and that the task of efficient benchmarking can be formulated as a sparse optimization problem. Based on these insights, we propose SparseEval, a method that, for the first time, adopts gradient descent to optimize anchor weights and employs an iterative refinement strategy for anchor selection. We utilize the representation capacity of MLP to handle sparse optimization and propose the Anchor Importance Score and Candidate Importance Score to evaluate the value of each item for task-aware refinement. Extensive experiments demonstrate the low estimation error and high Kendall's~$τ$ of our method across a variety of benchmarks, showcasing its superior robustness and practicality in real-world scenarios. Code is available at {https://github.com/taolinzhang/SparseEval}.

</details>


### [38] [Patches of Nonlinearity: Instruction Vectors in Large Language Models](https://arxiv.org/abs/2602.07930)
*Irina Bigoulaeva,Jonas Rohweder,Subhabrata Dutta,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本文通过因果调解分析揭示了指令调优语言模型中指令表示的局部化和复杂因果结构，提出了新的方法论，发现IVs在模型中作为电路选择器作用于不同任务信息路径。


<details>
  <summary>Details</summary>
Motivation: 尽管指令调优语言模型取得了成功并被广泛使用，但内部如何处理指令尚不清楚，因此从机制角度探究指令表征的构建和应用。

Method: 通过因果调解分析方法，研究了训练后不同阶段（监督微调和直接偏好优化）中指令特定表示的构建与利用，并提出了一种新的定位语言模型信息处理的方法，避免了基于patching的线性假设。

Result: 发现指令表征高度局部化，存在线性和非线性混合的因果交互；早期层形成任务相关表示，后续层根据任务选择不同信息路径，IVs充当电路选择器。

Conclusion: 该论文发现指令向量（Instruction Vectors, IVs）在模型中具有局部化特征，并且呈现出线性可分性与非线性因果交互并存的复杂特性，挑战了传统的线性表示假设。IVs在不同层次发挥着选择信息处理路径的作用，类似电路选择器。

Abstract: Despite the recent success of instruction-tuned language models and their ubiquitous usage, very little is known of how models process instructions internally. In this work, we address this gap from a mechanistic point of view by investigating how instruction-specific representations are constructed and utilized in different stages of post-training: Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO). Via causal mediation, we identify that instruction representation is fairly localized in models. These representations, which we call Instruction Vectors (IVs), demonstrate a curious juxtaposition of linear separability along with non-linear causal interaction, broadly questioning the scope of the linear representation hypothesis commonplace in mechanistic interpretability. To disentangle the non-linear causal interaction, we propose a novel method to localize information processing in language models that is free from the implicit linear assumptions of patching-based techniques. We find that, conditioned on the task representations formed in the early layers, different information pathways are selected in the later layers to solve that task, i.e., IVs act as circuit selectors.

</details>


### [39] [Bielik Guard: Efficient Polish Language Safety Classifiers for LLM Content Moderation](https://arxiv.org/abs/2602.07954)
*Krzysztof Wróbel,Jan Maria Kowalski,Jerzy Surma,Igor Ciuciura,Maciej Szymański*

Main category: cs.CL

TL;DR: 本文推出了Bielik Guard两款波兰语内容安全分类器，在精度和效率上均优于现有模型，特别适用于分类包括仇恨言论、自残等敏感内容。


<details>
  <summary>Details</summary>
Motivation: 随着大规模语言模型在波兰语应用中的普及，亟需高效且准确的内容安全分类器以保障内容的合规性和用户体验。

Method: 基于MMLW-RoBERTa-base和PKOBP/polish-roberta-8k模型，使用社区标注的6885条波兰语文本数据集进行微调，针对五个安全类别进行分类。

Result: 0.5B参数模型在测试集上实现了F1分数0.791（micro）和0.785（macro），而0.1B模型在精确率和误报率方面表现卓越，精确率77.65%，误报率仅0.63%。两模型公开发布，且对敏感类别设计了适当响应策略。

Conclusion: Bielik Guard提供了两个紧凑的波兰语内容安全分类器，分别以不同规模的模型实现，在多个基准测试中表现优异，尤其在准确率和误报率上相较同类模型有明显优势。

Abstract: As Large Language Models (LLMs) become increasingly deployed in Polish language applications, the need for efficient and accurate content safety classifiers has become paramount. We present Bielik Guard, a family of compact Polish language safety classifiers comprising two model variants: a 0.1B parameter model based on MMLW-RoBERTa-base and a 0.5B parameter model based on PKOBP/polish-roberta-8k. Fine-tuned on a community-annotated dataset of 6,885 Polish texts, these models classify content across five safety categories: Hate/Aggression, Vulgarities, Sexual Content, Crime, and Self-Harm. Our evaluation demonstrates that both models achieve strong performance on multiple benchmarks. The 0.5B variant offers the best overall discrimination capability with F1 scores of 0.791 (micro) and 0.785 (macro) on the test set, while the 0.1B variant demonstrates exceptional efficiency. Notably, Bielik Guard 0.1B v1.1 achieves superior precision (77.65\%) and very low false positive rate (0.63\%) on real user prompts, outperforming HerBERT-PL-Guard (31.55\% precision, 4.70\% FPR) despite identical model size. The models are publicly available and designed to provide appropriate responses rather than simple content blocking, particularly for sensitive categories like self-harm.

</details>


### [40] [Lost in Translation? A Comparative Study on the Cross-Lingual Transfer of Composite Harms](https://arxiv.org/abs/2602.07963)
*Vaibhav Shukla,Hardik Sharma,Adith N Reganti,Soham Wasmatkar,Bagesh Kumar,Vrijendra Singh*

Main category: cs.CL

TL;DR: 本研究提出CompositeHarm多语言安全基准，揭示翻译评估的局限，强调需针对语言差异构建多语言安全测试体系，并通过轻量推理提升效率。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的安全评估主要基于英文，简单翻译无法全面反映模型在多语言环境下的安全表现，尤其是恶意内容结构和意图跨语言变异的问题。

Method: 提出CompositeHarm基准，将两套英文数据集（AttaQ和MMSafetyBench）扩展到六种语言，利用三款大型模型测试攻击成功率和危害转移情况，同时采用轻量级推理策略提升计算和能源效率。

Result: 发现结构化对抗攻击在印度语言中的攻击成功率显著上升，而语境相关危害的转移较为温和。轻量级推理策略有效减少计算资源需求且保持多语言评测的一致性。

Conclusion: 翻译作为多语言安全评估的快捷方法存在局限，部分危害在翻译过程中发生变形或消失，导致安全对齐效果不完整。需要构建考虑语言语法和语义变化的多语言安全评估基准。

Abstract: Most safety evaluations of large language models (LLMs) remain anchored in English. Translation is often used as a shortcut to probe multilingual behavior, but it rarely captures the full picture, especially when harmful intent or structure morphs across languages. Some types of harm survive translation almost intact, while others distort or disappear. To study this effect, we introduce CompositeHarm, a translation-based benchmark designed to examine how safety alignment holds up as both syntax and semantics shift. It combines two complementary English datasets, AttaQ, which targets structured adversarial attacks, and MMSafetyBench, which covers contextual, real-world harms, and extends them into six languages: English, Hindi, Assamese, Marathi, Kannada, and Gujarati. Using three large models, we find that attack success rates rise sharply in Indic languages, especially under adversarial syntax, while contextual harms transfer more moderately. To ensure scalability and energy efficiency, our study adopts lightweight inference strategies inspired by edge-AI design principles, reducing redundant evaluation passes while preserving cross-lingual fidelity. This design makes large-scale multilingual safety testing both computationally feasible and environmentally conscious. Overall, our results show that translated benchmarks are a necessary first step, but not a sufficient one, toward building grounded, resource-aware, language-adaptive safety systems.

</details>


### [41] [Cross-Linguistic Persona-Driven Data Synthesis for Robust Multimodal Cognitive Decline Detection](https://arxiv.org/abs/2602.07978)
*Rui Feng,Zhiyao Luo,Liuyu Wu,Wei Wang,Yuting Song,Yong Liu,Kok Pin Ng,Jianqing Li,Xingyao Wang*

Main category: cs.CL

TL;DR: 本文提出SynCog框架，通过合成虚拟患者数据和链式思维微调，提升认知障碍语音诊断模型的性能与可解释性，实现跨语言泛化，解决临床数据匮乏和透明性不足的问题。


<details>
  <summary>Details</summary>
Motivation: MCI的早期识别受限于临床数据稀缺和缺乏可解释的诊断推理，现有模型在跨语言泛化和透明性方面表现不足。

Method: 提出SynCog框架，结合可控零样本多模态数据合成和链式思维(CoT)微调，利用虚拟认知多样表型合成扩展临床数据，并通过CoT强化诊断推理的可解释性。

Result: 在ADReSS和ADReSSo数据集上，模型分别取得80.67%和78.46%的Macro-F1，优于现有基线；在真实汉语数据集CIR-E上实现48.71%的Macro-F1，显示良好的跨语言泛化能力。

Conclusion: SynCog有效缓解了临床数据稀缺问题，提升了多模态语言模型的认知障碍诊断性能和可解释性，推动了可信赖且多语言适用的数字语言生物标志物应用。

Abstract: Speech-based digital biomarkers represent a scalable, non-invasive frontier for the early identification of Mild Cognitive Impairment (MCI). However, the development of robust diagnostic models remains impeded by acute clinical data scarcity and a lack of interpretable reasoning. Current solutions frequently struggle with cross-lingual generalization and fail to provide the transparent rationales essential for clinical trust. To address these barriers, we introduce SynCog, a novel framework integrating controllable zero-shot multimodal data synthesis with Chain-of-Thought (CoT) deduction fine-tuning. Specifically, SynCog simulates diverse virtual subjects with varying cognitive profiles to effectively alleviate clinical data scarcity. This generative paradigm enables the rapid, zero-shot expansion of clinical corpora across diverse languages, effectively bypassing data bottlenecks in low-resource settings and bolstering the diagnostic performance of Multimodal Large Language Models (MLLMs). Leveraging this synthesized dataset, we fine-tune a foundational multimodal backbone using a CoT deduction strategy, empowering the model to explicitly articulate diagnostic thought processes rather than relying on black-box predictions. Extensive experiments on the ADReSS and ADReSSo benchmarks demonstrate that augmenting limited clinical data with synthetic phenotypes yields competitive diagnostic performance, achieving Macro-F1 scores of 80.67% and 78.46%, respectively, outperforming current baseline models. Furthermore, evaluation on an independent real-world Mandarin cohort (CIR-E) demonstrates robust cross-linguistic generalization, attaining a Macro-F1 of 48.71%. These findings constitute a critical step toward providing clinically trustworthy and linguistically inclusive cognitive assessment tools for global healthcare.

</details>


### [42] [The Judge Who Never Admits: Hidden Shortcuts in LLM-based Evaluation](https://arxiv.org/abs/2602.07996)
*Arash Marioriyad,Omid Ghahroodi,Ehsaneddin Asgari,Mohammad Hossein Rohban,Mahdieh Soleymani Baghshah*

Main category: cs.CL

TL;DR: 本文通过合成信号扰动测试大型语言模型评判者的偏见和透明度，揭示其判决受无关线索影响但难以解释，表明模型评估仍需改进。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLMs）作为自动评判者在不同任务中评分的公正性和透明性。

Method: 通过对六个评判模型注入合成元数据标签（控制信号扰动），测试其对无关上下文的鲁棒性及决策透明度，覆盖两个数据集和六种线索类别。

Result: 发现模型在某些线索上严重偏向（如来源、时间、教育状态），但几乎未在自然语言理由中承认这些偏见，且透明度依赖数据集，表明存在解释缺口。

Conclusion: LLM作为评判者存在显著的判决偏差和透明度不足问题，影响模型评估的可靠性。

Abstract: Large language models (LLMs) are increasingly used as automatic judges to evaluate system outputs in tasks such as reasoning, question answering, and creative writing. A faithful judge should base its verdicts solely on content quality, remain invariant to irrelevant context, and transparently reflect the factors driving its decisions. We test this ideal via controlled cue perturbations-synthetic metadata labels injected into evaluation prompts-for six judge models: GPT-4o, Gemini-2.0-Flash, Gemma-3-27B, Qwen3-235B, Claude-3-Haiku, and Llama3-70B. Experiments span two complementary datasets with distinct evaluation regimes: ELI5 (factual QA) and LitBench (open-ended creative writing). We study six cue families: source, temporal, age, gender, ethnicity, and educational status. Beyond measuring verdict shift rates (VSR), we introduce cue acknowledgment rate (CAR) to quantify whether judges explicitly reference the injected cues in their natural-language rationales. Across cues with strong behavioral effects-e.g., provenance hierarchies (Expert > Human > LLM > Unknown), recency preferences (New > Old), and educational-status favoritism-CAR is typically at or near zero, indicating that shortcut reliance is largely unreported even when it drives decisions. Crucially, CAR is also dataset-dependent: explicit cue recognition is more likely to surface in the factual ELI5 setting for some models and cues, but often collapses in the open-ended LitBench regime, where large verdict shifts can persist despite zero acknowledgment. The combination of substantial verdict sensitivity and limited cue acknowledgment reveals an explanation gap in LLM-as-judge pipelines, raising concerns about reliability of model-based evaluation in both research and deployment.

</details>


### [43] [DeltaKV: Residual-Based KV Cache Compression via Long-Range Similarity](https://arxiv.org/abs/2602.08005)
*Jitai Hao,Qiang Huang,Yaowei Wang,Min Zhang,Jun Yu*

Main category: cs.CL

TL;DR: 本文提出了基于残差的KV缓存压缩框架DeltaKV和高性能推理引擎Sparse-vLLM，显著降低内存占用并提升推理速度，助力长上下文大模型高效部署。


<details>
  <summary>Details</summary>
Motivation: 长上下文LLM应用受限于KV缓存线性增长的内存瓶颈，现有压缩和剔除方法难以兼顾精度、压缩率和硬件效率。

Method: 提出基于残差的KV缓存压缩框架DeltaKV，通过编码语义残差而非丢弃token实现压缩；引入Sparse-vLLM推理引擎，采用解耦内存管理和优化稀疏非规则KV布局的内核以提升系统速度。

Result: DeltaKV将KV缓存内存压缩至原来的29%，在LongBench、SCBench和AIME数据集上保持近无损精度；结合Sparse-vLLM实现长上下文场景下最高2倍吞吐量提升。

Conclusion: DeltaKV在保持接近无损精度的同时，将KV缓存内存减少到原来的29%，并结合Sparse-vLLM实现了长上下文场景下最高2倍的吞吐量提升，展示了可扩展长上下文LLM部署的实际路径。

Abstract: The deployment of efficient long-context LLMs in applications like autonomous agents, long-chain reasoning, and creative writing is fundamentally bottlenecked by the linear growth of KV cache memory. Existing compression and eviction methods often struggle to balance accuracy, compression ratio, and hardware efficiency. We propose DeltaKV, a residual-based KV cache compression framework motivated by two empirical findings: long-range inter-token similarity and highly shared latent components in KV representations. Instead of discarding tokens, DeltaKV encodes semantic residuals relative to retrieved historical references, preserving fidelity while substantially reducing storage. To translate compression gains into real system speedups, we further introduce Sparse-vLLM, a high-performance inference engine with decoupled memory management and kernels optimized for sparse and irregular KV layouts. Experiments show that DeltaKV reduces KV cache memory to 29\% of the original while maintaining near-lossless accuracy on LongBench, SCBench, and AIME. When integrated with Sparse-vLLM, it achieves up to 2$\times$ throughput improvement over vLLM in long-context scenarios, demonstrating a practical path toward scalable long-context LLM deployment. Code, model checkpoints, and datasets are available at https://github.com/CURRENTF/Sparse-vLLM.

</details>


### [44] [Diverge to Induce Prompting: Multi-Rationale Induction for Zero-Shot Reasoning](https://arxiv.org/abs/2602.08028)
*Po-Chun Chen,Hen-Hsen Huang,Hsin-Hsi Chen*

Main category: cs.CL

TL;DR: 本研究提出DIP框架，通过多策略生成与归纳提升大型语言模型的推理准确率，效果优于传统单策略方法。


<details>
  <summary>Details</summary>
Motivation: 单一推理策略限制了模型在多样化任务中的表现，而多策略整合可能提升推理稳定性和准确性。

Method: DIP框架通过引导模型先生成多个多样化的高层次推理策略，再将每个策略细化为逐步草案，最后将这些草案归纳为最终方案。

Result: 实验结果表明，DIP在无需资源密集采样的情况下提升了推理准确性，优于单策略提示。

Conclusion: 多方案归纳引导（DIP）框架有效提升了大型语言模型在零样本推理中的准确率，优于单一策略的提示方法。

Abstract: To address the instability of unguided reasoning paths in standard Chain-of-Thought prompting, recent methods guide large language models (LLMs) by first eliciting a single reasoning strategy. However, relying on just one strategy for each question can still limit performance across diverse tasks. We propose Diverge-to-Induce Prompting (DIP), a framework that first prompts an LLM to generate multiple diverse high-level rationales for each question. Each rationale is then elaborated into a detailed, step-by-step draft plan. Finally, these draft plans are induced into a final plan. DIP enhances zero-shot reasoning accuracy without reliance on resource-intensive sampling. Experiments show that DIP outperforms single-strategy prompting, demonstrating the effectiveness of multi-plan induction for prompt-based reasoning.

</details>


### [45] [Beyond Raw Detection Scores: Markov-Informed Calibration for Boosting Machine-Generated Text Detection](https://arxiv.org/abs/2602.08031)
*Chenwang Wu,Yiu-ming Cheung,Shuhai Zhang,Bo Han,Defu Lian*

Main category: cs.CL

TL;DR: 针对机器生成文本检测中评分易受随机性影响的问题，本文提出基于马尔可夫随机场的校准策略，显著提升检测效果且计算负担低。


<details>
  <summary>Details</summary>
Motivation: 机器生成文本（MGTs）带来便利的同时也引发虚假信息和网络钓鱼等风险，需要可靠的检测方法。

Method: 统一现有基于指标的方法框架，分析其核心挑战，即检测分数受生成过程的随机性影响，提出基于马尔可夫随机场的分数校准策略，利用邻近相似性和初始不稳定性两个上下文关系，通过均场近似实现轻量级组件，整合进现有检测器。

Result: 在跨大型语言模型（LLM）和改写攻击等真实场景中，方法相较基准表现出显著提升，且计算开销极低。

Conclusion: 提出的基于马尔可夫随机场的分数校准方法有效解决了现有指标方法中评分易受随机性偏差的问题，提高了机器生成文本的检测准确性。

Abstract: While machine-generated texts (MGTs) offer great convenience, they also pose risks such as disinformation and phishing, highlighting the need for reliable detection. Metric-based methods, which extract statistically distinguishable features of MGTs, are often more practical than complex model-based methods that are prone to overfitting. Given their diverse designs, we first place representative metric-based methods within a unified framework, enabling a clear assessment of their advantages and limitations. Our analysis identifies a core challenge across these methods: the token-level detection score is easily biased by the inherent randomness of the MGTs generation process. To address this, we theoretically and empirically reveal two relationships of context detection scores that may aid calibration: Neighbor Similarity and Initial Instability. We then propose a Markov-informed score calibration strategy that models these relationships using Markov random fields, and implements it as a lightweight component via a mean-field approximation, allowing our method to be seamlessly integrated into existing detectors. Extensive experiments in various real-world scenarios, such as cross-LLM and paraphrasing attacks, demonstrate significant gains over baselines with negligible computational overhead. The code is available at https://github.com/tmlr-group/MRF_Calibration.

</details>


### [46] [TDGNet: Hallucination Detection in Diffusion Language Models via Temporal Dynamic Graphs](https://arxiv.org/abs/2602.08048)
*Arshia Hemmat,Philip Torr,Yongqiang Chen,Junchi Yu*

Main category: cs.CL

TL;DR: 为了检测扩散语言模型中的虚假内容，本文提出利用时序动态注意力图的TDGNet方法，通过全轨迹信息融合实现更准确的检测。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型由于平行去噪和双向上下文，其虚假信息检测难度较自动回归模型更大，现有基于单次生成的检测方法难以直接迁移。

Method: 设计了一个时序动态图框架TDGNet，通过稀疏化注意力图、消息传递更新token记忆，并使用时序注意力聚合整个去噪轨迹中的事实性证据，完成虚假信息检测。

Result: 在LLaDA-8B和Dream-7B模型的问答基准测试中，TDGNet在AUROC指标上均优于基线方案，且仅需单次推理，计算开销适中。

Conclusion: 本文提出的TDGNet方法通过在时间演化的token级注意力图上进行学习，实现了对扩散语言模型中虚假信息的有效检测，显著提升了检测性能。

Abstract: Diffusion language models (D-LLMs) offer parallel denoising and bidirectional context, but hallucination detection for D-LLMs remains underexplored. Prior detectors developed for auto-regressive LLMs typically rely on single-pass cues and do not directly transfer to diffusion generation, where factuality evidence is distributed across the denoising trajectory and may appear, drift, or be self-corrected over time. We introduce TDGNet, a temporal dynamic graph framework that formulates hallucination detection as learning over evolving token-level attention graphs. At each denoising step, we sparsify the attention graph and update per-token memories via message passing, then apply temporal attention to aggregate trajectory-wide evidence for final prediction. Experiments on LLaDA-8B and Dream-7B across QA benchmarks show consistent AUROC improvements over output-based, latent-based, and static-graph baselines, with single-pass inference and modest overhead. These results highlight the importance of temporal reasoning on attention graphs for robust hallucination detection in diffusion language models.

</details>


### [47] [Emergent Search and Backtracking in Latent Reasoning Models](https://arxiv.org/abs/2602.08100)
*Jasmine Cui,Charles Ye*

Main category: cs.CL

TL;DR: 本研究揭示了潜在推理转换器在隐藏空间中进行结构化搜索的能力，且通过回溯机制提升推理准确度，实现了类似链式思维的推理效果。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型在没有显式文字表达的情况下如何进行推理和思考，理解其潜在空间中的推理过程。

Method: 通过解码LRT模型在多个选择题问答基准测试中每一步的演变信念，分析其在潜在空间中的推理轨迹和搜索行为。

Result: 模型展现了一个包含探索、初步承诺及收敛或回溯的推理过程，回溯发生率高且显著提升准确率，且搜索过程具有适应性，替换干扰选项能显著缩短探索时间。

Conclusion: 潜在推理转换器（LRTs）能够在隐藏空间中自发地学习结构化搜索过程，表现出类似于链式思维的推理能力，并通过回溯机制显著提升多选题问答的准确率。

Abstract: What happens when a language model thinks without words? Standard reasoning LLMs verbalize intermediate steps as chain-of-thought; latent reasoning transformers (LRTs) instead perform deliberation entirely in continuous hidden space. We investigate an LRT, decoding the model's evolving beliefs at every step on a multiple-choice QA benchmark. We find that the model spontaneously learns a structured search process in latent space. Deliberation follows a consistent trajectory: an exploration phase where probability mass spreads across candidates, tentative commitment to a frontrunner, and either convergence or backtracking. Backtracking is prevalent (32% of instances), beneficial (34% accuracy gain over non-backtracking instances), and predominantly directed away from the semantically closest distractor toward the correct answer. The search is adaptive: replacing distractors with implausible alternatives shortens exploration by 54%. Latent reasoning models achieve in activation space what chain-of-thought achieves through words: the ability to be wrong, notice, and recover.

</details>


### [48] [Gender and Race Bias in Consumer Product Recommendations by Large Language Models](https://arxiv.org/abs/2602.08124)
*Ke Xu,Shera Potka,Alex Thomo*

Main category: cs.CL

TL;DR: 本文首次系统研究大型语言模型在产品推荐中的性别和种族偏见，证实了存在不公平现象，强调了构建公平推荐系统的重要性。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型在产品推荐中潜在的性别和种族偏见问题，以填补该领域的研究空白。

Method: 利用提示工程引导模型生成不同性别和种族群体的产品推荐，结合标记词、支持向量机和Jensen-Shannon散度三种方法分析和量化偏见。

Result: 发现模型推荐结果在不同群体间存在显著差异，表明当前推荐系统缺乏公平性。

Conclusion: 大型语言模型在生成消费者产品推荐时存在显著的性别和种族偏见，这些偏见导致不同人口群体间推荐的不公平差异。

Abstract: Large Language Models are increasingly employed in generating consumer product recommendations, yet their potential for embedding and amplifying gender and race biases remains underexplored. This paper serves as one of the first attempts to examine these biases within LLM-generated recommendations. We leverage prompt engineering to elicit product suggestions from LLMs for various race and gender groups and employ three analytical methods-Marked Words, Support Vector Machines, and Jensen-Shannon Divergence-to identify and quantify biases. Our findings reveal significant disparities in the recommendations for demographic groups, underscoring the need for more equitable LLM recommendation systems.

</details>


### [49] [DIAL-SUMMER: A Structured Evaluation Framework of Hierarchical Errors in Dialogue Summaries](https://arxiv.org/abs/2602.08149)
*Sahana Ramnath,Nima Chitsazan,Mingyang Zhou,Chia-Hsuan Lee,Shi-Xiong Zhang,Stephen Rawls,Sambit Sahu,Sangwoo Cho,Xiang Ren,Genta Indra Winata,Akshaj Kumar Veldanda*

Main category: cs.CL

TL;DR: 本文针对对话摘要评价中的结构和视角变化难点，提出了DIAL-SUMMER框架及细粒度错误分类体系，构建了标注数据集并分析错误分布，揭示当前大型语言模型在该任务上的局限，推动未来改进。


<details>
  <summary>Details</summary>
Motivation: 现有对话摘要评价方法忽视了对话多说话人结构和叙述视角从第一第二人称转为第三人称的复杂性，致使评价不足。为更全面有效地评价对话摘要，设计针对对话特性的综合错误分类体系并构建相关数据集。

Method: 提出了DIAL-SUMMER错误分类体系，从对话层面和单轮层面两个层级细致评估对话摘要；构建了人工标注细粒度错误的对话摘要数据集；基于该数据集进行经验分析和基于大型语言模型的错误检测实验。

Result: 建立了包含细粒度错误标注的对话摘要数据集，发现对话中段内容最易被遗漏，摘要末端易产生外部幻觉，验证了错误分类体系的合理性和数据集的挑战性，并显示大型语言模型检测错误的能力仍有限。

Conclusion: 本文提出了DIAL-SUMMER框架及其错误分类体系，构建了带有细粒度错误标注的对话摘要数据集，实证分析了摘要中的常见错误分布，并验证了大型语言模型在错误检测任务中的表现，指出了当前模型的不足和未来改进方向。

Abstract: Dialogues are a predominant mode of communication for humans, and it is immensely helpful to have automatically generated summaries of them (e.g., to revise key points discussed in a meeting, to review conversations between customer agents and product users). Prior works on dialogue summary evaluation largely ignore the complexities specific to this task: (i) shift in structure, from multiple speakers discussing information in a scattered fashion across several turns, to a summary's sentences, and (ii) shift in narration viewpoint, from speakers' first/second-person narration, standardized third-person narration in the summary. In this work, we introduce our framework DIALSUMMER to address the above. We propose DIAL-SUMMER's taxonomy of errors to comprehensively evaluate dialogue summaries at two hierarchical levels: DIALOGUE-LEVEL that focuses on the broader speakers/turns, and WITHIN-TURN-LEVEL that focuses on the information talked about inside a turn. We then present DIAL-SUMMER's dataset composed of dialogue summaries manually annotated with our taxonomy's fine-grained errors. We conduct empirical analyses of these annotated errors, and observe interesting trends (e.g., turns occurring in middle of the dialogue are the most frequently missed in the summary, extrinsic hallucinations largely occur at the end of the summary). We also conduct experiments on LLM-Judges' capability at detecting these errors, through which we demonstrate the challenging nature of our dataset, the robustness of our taxonomy, and the need for future work in this field to enhance LLMs' performance in the same. Code and inference dataset coming soon.

</details>


### [50] [NLP for Local Governance Meeting Records: A Focus Article on Tasks, Datasets, Metrics and Benchmark](https://arxiv.org/abs/2602.08162)
*Ricardo Campos,José Pedro Evans,José Miguel Isidro,Miguel Marques,Luís Filipe Cunha,Alípio Jorge,Sérgio Nunes,Nuno Guimarães*

Main category: cs.CL

TL;DR: 本文综述自然语言处理技术在地方治理会议记录结构化中的应用，重点讨论文档分段、实体提取和自动摘要三大任务，解决理解和自动处理难题，助力提升公共透明度和市民参与。


<details>
  <summary>Details</summary>
Motivation: 地方治理会议记录尽管结构化，但内容繁复且跨区域异质性高，普通公众难以理解，自动系统处理困难，影响公共透明度和市民参与，因此需借助计算方法提升其结构化和解读能力。

Method: 文章通过回顾文档分段、领域特定实体提取和自动文本摘要三项核心自然语言处理任务，分析了相关方法、评估指标及资源，结合领域特有的挑战探讨了技术应用。

Result: 文章系统总结了支持会议记录结构化的基础自然语言处理任务，提出了对应的方法与挑战，为相关研究和应用提供了理论框架和实践参考。

Conclusion: 文章综述了自然语言处理技术在地方治理会议记录结构化中的关键作用，强调了这些技术对于提升会议记录的可访问性和可解释性的重要性。

Abstract: Local governance meeting records are official documents, in the form of minutes or transcripts, documenting how proposals, discussions, and procedural actions unfold during institutional meetings. While generally structured, these documents are often dense, bureaucratic, and highly heterogeneous across municipalities, exhibiting significant variation in language, terminology, structure, and overall organization. This heterogeneity makes them difficult for non-experts to interpret and challenging for intelligent automated systems to process, limiting public transparency and civic engagement. To address these challenges, computational methods can be employed to structure and interpret such complex documents. In particular, Natural Language Processing (NLP) offers well-established methods that can enhance the accessibility and interpretability of governmental records. In this focus article, we review foundational NLP tasks that support the structuring of local governance meeting documents. Specifically, we review three core tasks: document segmentation, domain-specific entity extraction and automatic text summarization, which are essential for navigating lengthy deliberations, identifying political actors and personal information, and generating concise representations of complex decision-making processes. In reviewing these tasks, we discuss methodological approaches, evaluation metrics, and publicly available resources, while highlighting domain-specific challenges such as data scarcity, privacy constraints, and source variability. By synthesizing existing work across these foundational tasks, this article provides a structured overview of how NLP can enhance the structuring and accessibility of local governance meeting records.

</details>


### [51] [LLMs and people both learn to form conventions -- just not with each other](https://arxiv.org/abs/2602.08208)
*Cameron R. Jones,Agnese Lombardi,Kyle Mahowald,Benjamin K. Bergen*

Main category: cs.CL

TL;DR: 研究表明，虽然LLMs和人类分别能形成交流约定，但人机对话未能实现有效对齐，提示仅模仿人类行为不足以促成真正的会话理解。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型是否能在多模态交流中形成类似人类的交流约定，以提高人机对话的自然性和有效性。

Method: 通过多模态交流游戏，分别测试人类-人类、AI-AI及人类-AI对话中的交流表现，第二个实验通过提示促使LLMs生成类似人类的行为，比较其信息长度、准确率和词汇重叠。

Result: 同类型对话中双方都表现出约定形成，表现为发言准确率和一致性提升且发言长度缩短；异质对话中人机对话准确率和词汇重叠率低于其他组合。提示仅改善了发言长度但未提高准确率和词汇重叠。

Conclusion: 人类和大型语言模型（LLMs）在同类型对话中（人类-人类，AI-AI）都能形成交流约定，但异质人类-AI对话未能成功，表明交流倾向存在差异。仅模仿人类行为无法实现有效的会话对齐，需要共享的解释偏好。

Abstract: Humans align to one another in conversation -- adopting shared conventions that ease communication. We test whether LLMs form the same kinds of conventions in a multimodal communication game. Both humans and LLMs display evidence of convention-formation (increasing the accuracy and consistency of their turns while decreasing their length) when communicating in same-type dyads (humans with humans, AI with AI). However, heterogenous human-AI pairs fail -- suggesting differences in communicative tendencies. In Experiment 2, we ask whether LLMs can be induced to behave more like human conversants, by prompting them to produce superficially humanlike behavior. While the length of their messages matches that of human pairs, accuracy and lexical overlap in human-LLM pairs continues to lag behind that of both human-human and AI-AI pairs. These results suggest that conversational alignment requires more than just the ability to mimic previous interactions, but also shared interpretative biases toward the meanings that are conveyed.

</details>


### [52] [Pretraining with Token-Level Adaptive Latent Chain-of-Thought](https://arxiv.org/abs/2602.08220)
*Boyi Zeng,Yiqin Hao,He Li,Shixiang Song,Feichen Song,Zitong Wang,Siyuan Huang,Yi Xu,ZiWei He,Xinbing Wang,Zhouhan Lin*

Main category: cs.CL

TL;DR: 本文通过引入token级自适应潜在思维链，提升大语言模型性能与效率，解决了扩展性受限的问题。


<details>
  <summary>Details</summary>
Motivation: 当前大规模语言模型扩展受到高质量语料限制和通信成本上升的约束，本文探索通过增加每token计算而非扩增参数规模的替代方案。

Method: 引入了Token-Level Adaptive Latent CoT方法，模型在发出每个token之前生成可变长度的潜在思维轨迹，根据token难度自适应调整轨迹长度，且通过token-wise adaptive halting机制减少计算开销。

Result: 在Llama架构上，提出的方法在保持或降低训练计算量的同时，显著提升了语言建模的困惑度和下游任务的准确率。

Conclusion: 本文提出的Token-Level Adaptive Latent Chain-of-Thought方法，通过在预训练中自适应生成不同长度的潜在思维轨迹，提升了语言模型的性能和计算效率。

Abstract: Scaling large language models by increasing parameters and training data is increasingly constrained by limited high-quality corpora and rising communication costs. This work explores an alternative axis: increasing per-token computation without expanding parameters, by internalizing latent Chain-of-Thought (CoT) into pretraining. We propose Pretraining with Token-Level Adaptive Latent CoT (adaptive latent CoT), where the model generates a variable-length latent CoT trajectory before emitting each token -- allocating longer trajectories to difficult tokens and shorter (or even zero) trajectories to easy ones. Importantly, this behavior emerges naturally from one-stage pretraining on general text and reduces computation in both training and inference via token-wise adaptive halting. Experiments with Llama architectures show that adaptive latent CoT consistently improves language modeling perplexity and broad downstream accuracy, even with fewer training FLOPs than prior recurrent baselines.

</details>


### [53] [CoRect: Context-Aware Logit Contrast for Hidden State Rectification to Resolve Knowledge Conflicts](https://arxiv.org/abs/2602.08221)
*Xuhua Ma,Richong Zhang,Zhijie Nie*

Main category: cs.CL

TL;DR: 本文提出CoRect方法，通过对隐状态的对比性矫正，解决了RAG模型中深层参数抑制导致的知识冲突问题，提升了生成文本的真实度。


<details>
  <summary>Details</summary>
Motivation: RAG模型存在知识冲突问题，模型内部的参数化知识往往覆盖已检索的外部证据，导致输出内容不真实，现有方法局限于表面解码调整或需要真实标签的权重编辑。

Method: 通过对上下文化和非上下文化的前向传播的logits进行对比，CoRect方法自动识别出存在参数压制现象的层，并校正其隐藏状态，从而防止预训练知识覆盖检索证据。

Result: 在问答和文本摘要基准测试中，CoRect表现出比现有强基线方法更好的忠实度和更少的幻觉，证明其有效性。

Conclusion: CoRect方法通过识别并校正深层网络中参数偏置明显的层，成功提升了RAG模型的输出忠实度，有效减少了幻觉现象。

Abstract: Retrieval-Augmented Generation (RAG) often struggles with knowledge conflicts, where model-internal parametric knowledge overrides retrieved evidence, leading to unfaithful outputs. Existing approaches are often limited, relying either on superficial decoding adjustments or weight editing that necessitates ground-truth targets. Through layer-wise analysis, we attribute this failure to a parametric suppression phenomenon: specifically, in deep layers, certain FFN layers overwrite context-sensitive representations with memorized priors. To address this, we propose CoRect (Context-Aware Logit Contrast for Hidden State Rectification). By contrasting logits from contextualized and non-contextualized forward passes, CoRect identifies layers that exhibit high parametric bias without requiring ground-truth labels. It then rectifies the hidden states to preserve evidence-grounded information. Across question answering (QA) and summarization benchmarks, CoRect consistently improves faithfulness and reduces hallucinations compared to strong baselines.

</details>


### [54] [When Benign Inputs Lead to Severe Harms: Eliciting Unsafe Unintended Behaviors of Computer-Use Agents](https://arxiv.org/abs/2602.08235)
*Jaylen Jones,Zhehao Zhang,Yuting Ning,Eric Fosler-Lussier,Pierre-Luc St-Charles,Yoshua Bengio,Dawn Song,Yu Su,Huan Sun*

Main category: cs.CL

TL;DR: 本文提出了AutoElicit框架，实现了自动识别和分析计算机使用代理中的未预期有害行为，为提升CUA安全性奠定了方法基础。


<details>
  <summary>Details</summary>
Motivation: 尽管CUAs有潜力自动化复杂的操作系统工作流，但其未预期的行为可能带来安全风险，现有研究多为零散案例，缺乏系统化的刻画和主动检测方法。

Method: 提出了AutoElicit框架，通过迭代地利用CUA执行反馈，扰动正常指令以自动激发严重的未预期有害行为，同时保证扰动的现实性和无害性。

Result: 利用AutoElicit从前沿CUAs（如Claude 4.5 Haiku和Opus）中发现数百个有害的未预期行为，并验证了这些扰动对其他CUAs的转移能力，说明问题具有普遍性。

Conclusion: 研究首次系统化地定义和自动引发了计算机使用代理（CUAs）中未预期行为的关键特征，揭示了这些行为在实际使用中普遍存在且具有高度转移性。

Abstract: Although computer-use agents (CUAs) hold significant potential to automate increasingly complex OS workflows, they can demonstrate unsafe unintended behaviors that deviate from expected outcomes even under benign input contexts. However, exploration of this risk remains largely anecdotal, lacking concrete characterization and automated methods to proactively surface long-tail unintended behaviors under realistic CUA scenarios. To fill this gap, we introduce the first conceptual and methodological framework for unintended CUA behaviors, by defining their key characteristics, automatically eliciting them, and analyzing how they arise from benign inputs. We propose AutoElicit: an agentic framework that iteratively perturbs benign instructions using CUA execution feedback, and elicits severe harms while keeping perturbations realistic and benign. Using AutoElicit, we surface hundreds of harmful unintended behaviors from state-of-the-art CUAs such as Claude 4.5 Haiku and Opus. We further evaluate the transferability of human-verified successful perturbations, identifying persistent susceptibility to unintended behaviors across various other frontier CUAs. This work establishes a foundation for systematically analyzing unintended behaviors in realistic computer-use settings.

</details>


### [55] [Document Reconstruction Unlocks Scalable Long-Context RLVR](https://arxiv.org/abs/2602.08237)
*Yao Xiao,Lei Wang,Yue Deng,Guanzheng Chen,Ziqi Jin,Jung-jae Kim,Xiaoli Li,Roy Ka-wei Lee,Lidong Bing*

Main category: cs.CL

TL;DR: 提出一种无监督的强化学习方法，通过让模型重建文档缺失段落来提升长上下文理解能力，无需人工标注，在多个基准测试中表现优良。


<details>
  <summary>Details</summary>
Motivation: 传统的基于可验证奖励的强化学习方法依赖人工标注或强教师模型，成本高、耗时长，本研究旨在探索无监督方法以提升大语言模型处理长上下文的能力，降低依赖昂贵的监督资源。

Method: 通过在长文档中用特殊占位符替换部分段落，训练模型通过强化学习从候选段落中识别和排序缺失段落，重建文档以增强全局叙事连贯性。

Result: 在RULER基准上取得显著性能提升，在LongBench v2上也获得合理的改善，同时通过消融实验分析了奖励设计、数据策略、训练方案和数据规模对模型表现的影响。

Conclusion: 该方法通过无监督强化学习训练大语言模型重建缺失段落，显著提升了模型处理长上下文的能力，无需人工标注或教师模型监督，在RULER和LongBench v2两个基准上表现出明显和合理的性能提升。

Abstract: Reinforcement Learning with Verifiable Rewards~(RLVR) has become a prominent paradigm to enhance the capabilities (i.e.\ long-context) of Large Language Models~(LLMs). However, it often relies on gold-standard answers or explicit evaluation rubrics provided by powerful teacher models or human experts, which are costly and time-consuming. In this work, we investigate unsupervised approaches to enhance the long-context capabilities of LLMs, eliminating the need for heavy human annotations or teacher models' supervision. Specifically, we first replace a few paragraphs with special placeholders in a long document. LLMs are trained through reinforcement learning to reconstruct the document by correctly identifying and sequencing missing paragraphs from a set of candidate options. This training paradigm enables the model to capture global narrative coherence, significantly boosting long-context performance. We validate the effectiveness of our method on two widely used benchmarks, RULER and LongBench~v2. While acquiring noticeable gains on RULER, it can also achieve a reasonable improvement on LongBench~v2 without any manually curated long-context QA data. Furthermore, we conduct extensive ablation studies to analyze the impact of reward design, data curation strategies, training schemes, and data scaling effects on model performance. We publicly release our code, data, and models.

</details>


### [56] [On convexity and efficiency in semantic systems](https://arxiv.org/abs/2602.08238)
*Nathaniel Imel,Noga Zaslavasky*

Main category: cs.CL

TL;DR: 本文通过信息瓶颈框架分析凸性和效率在语义范畴系统中的关系，发现两者本质不同，效率更能解释颜色命名等语义系统的结构和分类现象。


<details>
  <summary>Details</summary>
Motivation: 人类语义范畴系统是否具有凸性及其效率如何长期被讨论，但两者的关系及共现机制尚未被充分理解。

Method: 结合信息瓶颈（IB）框架，采用分析和实证方法，研究凸性与效率之间的关系并检验其在颜色命名领域的表现。

Result: IB最优系统在颜色命名领域多为凸性，但凸性与效率独立，效率更强预测实际颜色命名系统，凸性仅带来细微改进，并且效率能解释更多凸性无法解释的现象。

Conclusion: 凸性和效率是两个本质上不同的语义范畴系统特征，凸性并不必然导致效率，效率更能全面解释语义体系的结构特征。

Abstract: There are two widely held characterizations of human semantic category systems: (1) they form convex partitions of conceptual spaces, and (2) they are efficient for communication. While prior work observed that convexity and efficiency co-occur in color naming, the analytical relation between them and why they co-occur have not been well understood. We address this gap by combining analytical and empirical analyses that build on the Information Bottleneck (IB) framework for semantic efficiency. First, we show that convexity and efficiency are distinct in the sense that neither entails the other: there are convex systems which are inefficient, and optimally-efficient systems that are non-convex. Crucially, however, the IB-optimal systems are mostly convex in the domain of color naming, explaining the main empirical basis for the convexity approach. Second, we show that efficiency is a stronger predictor for discriminating attested color naming systems from hypothetical variants, with convexity adding negligible improvement on top of that. Finally, we discuss a range of empirical phenomena that convexity cannot account for but efficiency can. Taken together, our work suggests that while convexity and efficiency can yield similar structural observations, they are fundamentally distinct, with efficiency providing a more comprehensive account of semantic typology.

</details>


### [57] [Language Predicts Identity Fusion Across Cultures and Reveals Divergent Pathways to Violence](https://arxiv.org/abs/2602.08252)
*Devin R. Wright,Justin E. Lane,F. LeRon Shults*

Main category: cs.CL

TL;DR: 本文提出一种基于语言的身份融合评分方法，能更准确预测极端主义倾向，揭示两种不同的暴力路径，促进极端主义心理机制研究与检测。


<details>
  <summary>Details</summary>
Motivation: 随着极端主义和政治暴力的加剧，理解其心理根源变得尤为重要，尤其是身份融合对极端行为的预测作用。

Method: 采用认知语言学模式、大型语言模型（LLMs）和隐喻分析，通过语言文本测量身份融合程度。

Result: 该方法在英国和新加坡的数据集中优于现有方法，在极端主义宣言文本中识别出两类高融合群体，丰富了身份融合理论并为相关研究和极端主义检测提供了可扩展工具。

Conclusion: 本研究表明认知语言学身份融合评分（CLIFS）方法能够有效预测身份融合程度，并揭示了极端主义中两种不同的高融合暴力路径，即意识形态者和因不满驱动者的不同认同框架。

Abstract: In light of increasing polarization and political violence, understanding the psychological roots of extremism is increasingly important. Prior research shows that identity fusion predicts willingness to engage in extreme acts. We evaluate the Cognitive Linguistic Identity Fusion Score, a method that uses cognitive linguistic patterns, LLMs, and implicit metaphor to measure fusion from language. Across datasets from the United Kingdom and Singapore, this approach outperforms existing methods in predicting validated fusion scores. Applied to extremist manifestos, two distinct high-fusion pathways to violence emerge: ideologues tend to frame themselves in terms of group, forming kinship bonds; whereas grievance-driven individuals frame the group in terms of their personal identity. These results refine theories of identity fusion and provide a scalable tool aiding fusion research and extremism detection.

</details>


### [58] [Language Modeling and Understanding Through Paraphrase Generation and Detection](https://arxiv.org/abs/2602.08274)
*Jan Philip Wahle*

Main category: cs.CL

TL;DR: 通过将释义细化为多个语言学维度训练模型，提升了计算语言模型理解和处理相同语义不同表达的能力，进而提高了抄袭检测和重复问题识别等任务的效果。


<details>
  <summary>Details</summary>
Motivation: 现有模型多将释义简化为二元判断或单一改写，掩盖了哪些语言因素保障语义不变，难以体现模型对细粒度语义的理解。

Method: 提出分解释义为多个语言学组成部分（释义类型），并训练模型识别这些类型，从而改进模型对释义的理解和生成能力。

Result: 训练了基于释义类型的模型，在抄袭检测（超过人类基线准确率）和问答重复检测等任务中均取得了优异表现，超越了传统的二元分类模型。

Conclusion: 将释义（paraphrase）分解为不同的语言学方面，可以更细致和认知基础地理解语义等价性，显著提升计算语言模型的语义理解和应用表现。

Abstract: Language enables humans to share knowledge, reason about the world, and pass on strategies for survival and innovation across generations. At the heart of this process is not just the ability to communicate but also the remarkable flexibility in how we can express ourselves. We can express the same thoughts in virtually infinite ways using different words and structures - this ability to rephrase and reformulate expressions is known as paraphrase. Modeling paraphrases is a keystone to meaning in computational language models; being able to construct different variations of texts that convey the same meaning or not shows strong abilities of semantic understanding. If computational language models are to represent meaning, they must understand and control the different aspects that construct the same meaning as opposed to different meanings at a fine granularity. Yet most existing approaches reduce paraphrasing to a binary decision between two texts or to producing a single rewrite of a source, obscuring which linguistic factors are responsible for meaning preservation. In this thesis, I propose that decomposing paraphrases into their constituent linguistic aspects (paraphrase types) offers a more fine-grained and cognitively grounded view of semantic equivalence. I show that even advanced machine learning models struggle with this task. Yet, when explicitly trained on paraphrase types, models achieve stronger performance on related paraphrase tasks and downstream applications. For example, in plagiarism detection, language models trained on paraphrase types surpass human baselines: 89.6% accuracy compared to 78.4% for plagiarism cases from Wikipedia, and 66.5% compared to 55.7% for plagiarism of scientific papers from arXiv. In identifying duplicate questions on Quora, models trained with paraphrase types improve over models trained on binary pairs. Furthermore, I demonstrate that...

</details>


### [59] [New Skills or Sharper Primitives? A Probabilistic Perspective on the Emergence of Reasoning in RLVR](https://arxiv.org/abs/2602.08281)
*Zhilin Wang,Yafu Li,Shunkai Zhang,Zhi Wang,Haoran Zhang,Xiaoye Qu,Yu Cheng*

Main category: cs.CL

TL;DR: 本文提出概率框架解释强化学习可验证奖励如何通过提升步骤概率促使大模型涌现复杂推理能力，实验证实该方法有效提高多步骤任务解题性能。


<details>
  <summary>Details</summary>
Motivation: 探讨强化学习可验证奖励（RLVR）是否赋予大语言模型新能力，并解释复杂推理能力的涌现机制。

Method: 建立基于概率的能力定义框架，以实例级可解性为核心，利用Algebrarium框架仅训练单步操作，测试多步任务表现，分析奖励机制对模型能力的影响。

Result: 实验表明RLVR激励模型探索新解法路径，复合任务表现由多个原子步骤联合概率决定，且RLVR可作为全局优化器，导致部分技能权衡。

Conclusion: 强化学习可验证奖励（RLVR）通过提升原子步骤概率促进了复杂推理能力的出现，增强了大语言模型解决多步骤任务的能力，并可能牺牲部分具体技能以最大化总体奖励。

Abstract: Whether Reinforcement Learning with Verifiable Rewards (RLVR) endows Large Language Models (LLMs) with new capabilities or merely elicits latent traces remains a central debate. In this work, we align with the former view, proposing a probabilistic framework where capability is defined by instance-level solvability. We hypothesize that the emergence of complex reasoning can be driven by sharpening atomic step probabilities, which enables models to overcome the exponential decay of success rates inherent in multi-step reasoning chains. Utilizing the Algebrarium framework, we train models exclusively on single-step operations and evaluate their performance on unseen multi-step tasks. Our empirical results confirm that: (1) RLVR incentivizes the exploration of previously inaccessible solution paths by amplifying the model's existing skills; (2) composite performance is strictly governed by the joint probability of atomic steps, evidenced by high Pearson correlation coefficients ($ρ\in [0.69, 0.96]$); and (3) RLVR, acting as a global optimizer, can cause specific skills to be sacrificed to maximize aggregate reward. Our work offers a novel explanation for emergent abilities in RLVR, suggesting that the iterative optimization of solvable problems enables models to develop the capabilities to tackle previously unsolvable scenarios.

</details>


### [60] [Knowledge Augmented Entity and Relation Extraction for Legal Documents with Hypergraph Neural Network](https://arxiv.org/abs/2602.08289)
*Binglin Wu,Xianneng Li*

Main category: cs.CL

TL;DR: 本文针对司法领域文书，提出基于超图神经网络的实体关系抽取方法，融合领域知识，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 司法机构数字化进程带来了大量电子法律文档，但现有实体与关系抽取方法缺乏司法领域特有的知识，无法充分利用这些数据。

Method: 提出了一种基于超图神经网络的药品相关判决文书实体关系抽取算法（Legal-KAHRE），包括候选实体生成、司法领域词典构建及融合、多头注意力编码、结合领域案例设计的超图结构，以及通过超图神经网络进行高阶推理。

Result: 在CAIL2022信息抽取数据集上的实验结果显示，该方法显著优于现有基线模型。

Conclusion: 该方法有效结合了司法领域知识和超图神经网络，实现了药品相关法律文档中实体与关系的高效准确抽取。

Abstract: With the continuous progress of digitization in Chinese judicial institutions, a substantial amount of electronic legal document information has been accumulated. To unlock its potential value, entity and relation extraction for legal documents has emerged as a crucial task. However, existing methods often lack domain-specific knowledge and fail to account for the unique characteristics of the judicial domain. In this paper, we propose an entity and relation extraction algorithm based on hypergraph neural network (Legal-KAHRE) for drug-related judgment documents. Firstly, we design a candidate span generator based on neighbor-oriented packing strategy and biaffine mechanism, which identifies spans likely to contain entities. Secondly, we construct a legal dictionary with judicial domain knowledge and integrate it into text encoding representation using multi-head attention. Additionally, we incorporate domain-specific cases like joint crimes and combined punishment for multiple crimes into the hypergraph structure design. Finally, we employ a hypergraph neural network for higher-order inference via message passing. Experimental results on the CAIL2022 information extraction dataset demonstrate that our method significantly outperforms existing baseline models.

</details>


### [61] [When Does Context Help? Error Dynamics of Contextual Information in Large Language Models](https://arxiv.org/abs/2602.08294)
*Dingzirui Wang,Xuanliang Zhang,Keyan Xu,Qingfu Zhu,Wanxiang Che,Yang Deng*

Main category: cs.CL

TL;DR: 本文提出理论框架解析上下文信息如何影响Transformer大模型输出误差，通过误差向量分解揭示误差减少条件，并验证了多层多上下文适用性，指导上下文选择提升性能。


<details>
  <summary>Details</summary>
Motivation: 尽管上下文信息（如示例、检索知识或交互历史）能显著提升LLM性能，但其在更广泛场景下的理论作用尚未得到充分理解，尤其是在特定设置之外的应用。

Method: 通过理论分析，作者证明了单层Transformer中上下文条件误差向量可分解为基础误差向量和上下文纠正向量的加法形式，推导了误差减少的几何必要条件，并给出了上下文纠正范数的显式上界。

Result: 理论证明了上下文纠正向量的结构与误差减少条件，扩展至多上下文和多层Transformer，并通过实验证实该理论在多种应用场景下的有效性，提出了一种基于理论指导的上下文选择策略，性能提升了0.6%。

Conclusion: 本文构建了一个统一的理论框架，揭示了任意上下文信息对基于Transformer的大型语言模型（LLM）输出误差的影响机制，指出了上下文纠正向量与基础误差向量的几何关系，并验证了在多层多上下文环境中的适用性。

Abstract: Contextual information at inference time, such as demonstrations, retrieved knowledge, or interaction history, can substantially improve large language models (LLMs) without parameter updates, yet its theoretical role remains poorly understood beyond specific settings such as in-context learning (ICL). We present a unified theoretical framework for analyzing the effect of arbitrary contextual information in Transformer-based LLMs. Our analysis characterizes contextual influence through output error dynamics. In a single-layer Transformer, we prove that the context-conditioned error vector decomposes additively into the baseline error vector and a contextual correction vector. This yields necessary geometric conditions for error reduction: the contextual correction must align with the negative baseline error and satisfy a norm constraint. We further show that the contextual correction norm admits an explicit upper bound determined by context-query relevance and complementarity. These results extend to multi-context and multi-layer Transformers. Experiments across ICL, retrieval-augmented generation, and memory evolution validate our theory and motivate a principled context selection strategy that improves performance by $0.6\%$.

</details>


### [62] [JUSTICE: Judicial Unified Synthesis Through Intermediate Conclusion Emulation for Automated Judgment Document Generation](https://arxiv.org/abs/2602.08305)
*Binglin Wu,Yingyi Zhang,Xiannneg Li*

Main category: cs.CL

TL;DR: 本文提出JUSTICE框架，通过引入预判阶段的模拟，有效提升了自动生成判决文书的法律准确性和合理性。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了判决生成过程中的“预判”阶段，导致司法元素获取不充分和预判过程建模不足，进而影响判决文书的法律严谨性。

Method: 设计了三个关键组成部分：RJER用于检索法律条文和案例，ICE用于模拟并生成可验证的中间判决结论，JUS用于综合前期信息输出最终判决文本。

Result: 在内域和跨域数据集上的实验表明，JUSTICE在法律准确率方面优于现有强基线方法，尤其在刑期预测上提升了4.6%。

Conclusion: 本论文提出的JUSTICE框架通过模拟司法人员的“搜索-预判-写作”认知流程，显著提升了自动生成判决文书的法律准确性和合理性。

Abstract: Automated judgment document generation is a significant yet challenging legal AI task. As the conclusive written instrument issued by a court, a judgment document embodies complex legal reasoning. However, existing methods often oversimplify this complex process, particularly by omitting the ``Pre-Judge'' phase, a crucial step where human judges form a preliminary conclusion. This omission leads to two core challenges: 1) the ineffective acquisition of foundational judicial elements, and 2) the inadequate modeling of the Pre-Judge process, which collectively undermine the final document's legal soundness. To address these challenges, we propose \textit{\textbf{J}udicial \textbf{U}nified \textbf{S}ynthesis \textbf{T}hrough \textbf{I}ntermediate \textbf{C}onclusion \textbf{E}mulation} (JUSTICE), a novel framework that emulates the ``Search $\rightarrow$ Pre-Judge $\rightarrow$ Write'' cognitive workflow of human judges. Specifically, it introduces the Pre-Judge stage through three dedicated components: Referential Judicial Element Retriever (RJER), Intermediate Conclusion Emulator (ICE), and Judicial Unified Synthesizer (JUS). RJER first retrieves legal articles and a precedent case to establish a referential foundation. ICE then operationalizes the Pre-Judge phase by generating a verifiable intermediate conclusion. Finally, JUS synthesizes these inputs to craft the final judgment. Experiments on both an in-domain legal benchmark and an out-of-distribution dataset show that JUSTICE significantly outperforms strong baselines, with substantial gains in legal accuracy, including a 4.6\% improvement in prison term prediction. Our findings underscore the importance of explicitly modeling the Pre-Judge process to enhance the legal coherence and accuracy of generated judgment documents.

</details>


### [63] [Improving Data and Reward Design for Scientific Reasoning in Large Language Models](https://arxiv.org/abs/2602.08321)
*Zijie Chen,Zhenghao Lin,Xiao Liu,Zhenzhong Lan,Yeyun Gong,Peng Cheng*

Main category: cs.CL

TL;DR: 本文构建了大规模科学数据集Dr. SCI，设计创新训练策略，显著提升了大语言模型在开放性科学问答中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在解决开放性科学问题时，受到不可靠监督和评估的限制，数据构建和奖励设计是瓶颈。

Method: 构建了包含100万问题的Dr. SCI数据集，设计了覆盖推理广度的探索扩展微调、适应能力的动态难度课程和基于评分细则的强化学习方法。

Result: 基于Dr. SCI训练的模型Qwen3-4B-Base在GPQA测试中表现优异，显著提升了科学推理能力，优于多个强基线模型。

Conclusion: 系统数据处理和科学设计的后训练管道有效提升了大语言模型解决开放性科学问题的能力，实现更稳定和准确的科学推理表现。

Abstract: Solving open-ended science questions remains challenging for large language models, particularly due to inherently unreliable supervision and evaluation. The bottleneck lies in the data construction and reward design for scientific post-training. We develop a large-scale, systematic data processing pipeline that transforms heterogeneous open-source science data into Dr. SCI dataset, which comprises of 1M questions across eight STEM subjects, with explicit verifiable/open-ended splits, scalable difficulty annotation, and fine-grained rubrics that operationalize evaluation for open-ended answers. Building on this dataset, we propose the Dr. SCI post-training pipeline, which redesigns the standard SFT -> RL workflow through three components: (i) Exploration-Expanding SFT, which broadens the model's reasoning pattern coverage prior to RL; (ii) Dynamic Difficulty Curriculum, which adapts training data to the model's evolving scientific capability; and (iii) SciRubric-Guided RL, which enables stable reinforcement learning on open-ended scientific questions via rubric-based evaluation with explicit answer correctness. Qwen3-4B-Base trained using Dr.SCI pipeline achieves 63.2 on GPQA-diamond and 32.4 on GPQA-general, consistently improves over strong post-trained baselines such as o1-mini and GPT-4o, demonstrating substantial gains in scientific reasoning, especially in open-ended settings.

</details>


### [64] [An Attention-over-Attention Generative Model for Joint Multiple Intent Detection and Slot Filling](https://arxiv.org/abs/2602.08322)
*Wei Zhu*

Main category: cs.CL

TL;DR: 针对现实中用户多意图表达的挑战，本文提出生成式多意图SLU模型及新数据集，实现多意图检测与槽位填充联合优化，实验验证了该方法的优越性。


<details>
  <summary>Details</summary>
Motivation: 现有大多数任务导向对话系统仅针对单意图语句，而实际场景中用户常表达多个意图，现有方法和数据集难以应对多意图挑战。

Method: 提出了一种基于生成框架的注意力叠加解码器，通过引入归纳偏置实现多任务学习，支持变数量意图检测和槽填充的联合处理。

Result: 在两个公开多意图SLU数据集MixATIS和MixSNIPS及新构建数据集上，所提模型取得了领先的实验结果，有效缓解了多意图及子任务干扰问题。

Conclusion: 本文提出的注意力叠加解码器生成框架有效地解决了多意图检测和槽位填充任务，显著提升了多意图任务的性能。

Abstract: In task-oriented dialogue systems, spoken language understanding (SLU) is a critical component, which consists of two sub-tasks, intent detection and slot filling. Most existing methods focus on the single-intent SLU, where each utterance only has one intent. However, in real-world scenarios users usually express multiple intents in an utterance, which poses a challenge for existing dialogue systems and datasets. In this paper, we propose a generative framework to simultaneously address multiple intent detection and slot filling. In particular, an attention-over-attention decoder is proposed to handle the variable number of intents and the interference between the two sub-tasks by incorporating an inductive bias into the process of multi-task learning. Besides, we construct two new multi-intent SLU datasets based on single-intent utterances by taking advantage of the next sentence prediction (NSP) head of the BERT model. Experimental results demonstrate that our proposed attention-over-attention generative model achieves state-of-the-art performance on two public datasets, MixATIS and MixSNIPS, and our constructed datasets.

</details>


### [65] [Latent Reasoning with Supervised Thinking States](https://arxiv.org/abs/2602.08332)
*Ido Amos,Avi Caciularu,Mor Geva,Amir Globerson,Jonathan Herzig,Lior Shani,Idan Szpektor*

Main category: cs.CL

TL;DR: 提出了Thinking States方法，边处理输入边生成思考标记，提升推理效率和效果，实验证明其在多种推理任务中优于或匹配链式推理，且具有更低延迟和更好泛化能力。


<details>
  <summary>Details</summary>
Motivation: 链式推理（CoT）虽然提升了大型语言模型解决复杂任务的能力，但其生成长推理过程的开销很大。思考状态（Thinking States）旨在降低推理延迟和计算成本，同时保持甚至提升推理性能，通过边输入边推理的机制来优化推理过程。

Method: Thinking States方法通过在输入处理的过程中，每隔几个输入标记生成思考标记序列，将这些思考标记转回嵌入空间并加入后续输入中，实现了在输入流动中递归地生成和利用思考内容。这种方法支持使用自然语言监督和教师强制训练，具有并行计算优势。

Result: 实验结果表明，Thinking States在多个推理任务中表现优异，特别是在数学问题上缩小了与CoT的差距，且在2-Hop问答任务上的表现与CoT相当但推理延迟更低；在状态跟踪任务中，Thinking States表现出更强的推理能力，并成功推广到训练中未见的更长序列。

Conclusion: Thinking States方法能够在处理输入的同时进行推理，显著提升大语言模型的推理效率和效果，在多个推理任务中优于其他潜在推理方法，并且在数学问题和多跳问答任务上表现接近或达到链式推理（CoT）的水平，同时在状态跟踪任务上展现出更强的推理能力和对长序列的泛化能力。

Abstract: Reasoning with a chain-of-thought (CoT) enables Large Language Models (LLMs) to solve complex tasks but incurs significant inference costs due to the generation of long rationales. We propose Thinking States, a method that performs reasoning {\em while} the input is processing. Specifically, Thinking States generates sequences of thinking tokens every few input tokens, transforms the thoughts back into embedding space, and adds them to the following input tokens. This has two key advantages. First, it captures the recurrent nature of CoT, but where the thought tokens are generated as input is processing. Second, since the thoughts are represented as tokens, they can be learned from natural language supervision, and using teacher-forcing, which is parallelizable. Empirically, Thinking States outperforms other latent reasoning methods on multiple reasoning tasks, narrowing the gap to CoT on math problems, and matching its performance on 2-Hop QA with improved latency. On state-tracking tasks, we show Thinking States leads to stronger reasoning behavior than CoT, successfully extrapolating to longer sequences than seen during training.

</details>


### [66] [UReason: Benchmarking the Reasoning Paradox in Unified Multimodal Models](https://arxiv.org/abs/2602.08336)
*Cheng Yang,Chufan Shi,Bo Shui,Yaokang Wu,Muzi Tao,Huijuan Wang,Ivan Yee Lee,Yong Liu,Xuezhe Ma,Taylor Berg-Kirkpatrick*

Main category: cs.CL

TL;DR: 提出UReason基准全面评估推理驱动图像生成，揭示推理条件上下文干扰问题，推动未来有效整合推理指导视觉生成的方法研究。


<details>
  <summary>Details</summary>
Motivation: 当前统一多模态模型借助链式推理引导图像生成，但推理对视觉合成的实际影响尚不明晰，亟需系统评估推理执行的真实性能。

Method: 设计了UReason诊断基准，包含2000个实例和五类推理任务，通过比较直接生成、推理引导生成和仅用精炼提示生成，评估推理在图像生成中的作用。

Result: 发现“推理悖论”现象：保留推理中间步骤作为条件会干扰图像生成效果，而仅用精炼的提示反而表现最好。

Conclusion: 推理轨迹虽提升了图像生成性能，但将中间思考过程作为条件上下文反而阻碍了视觉合成，主要瓶颈是上下文干扰而非推理能力不足。

Abstract: To elicit capabilities for addressing complex and implicit visual requirements, recent unified multimodal models increasingly adopt chain-of-thought reasoning to guide image generation. However, the actual effect of reasoning on visual synthesis remains unclear. We present UReason, a diagnostic benchmark for reasoning-driven image generation that evaluates whether reasoning can be faithfully executed in pixels. UReason contains 2,000 instances across five task families: Code, Arithmetic, Spatial, Attribute, and Text reasoning. To isolate the role of reasoning traces, we introduce an evaluation framework comparing direct generation, reasoning-guided generation, and de-contextualized generation which conditions only on the refined prompt. Across eight open-source unified models, we observe a consistent Reasoning Paradox: Reasoning traces generally improve performance over direct generation, yet retaining intermediate thoughts as conditioning context often hinders visual synthesis, and conditioning only on the refined prompt yields substantial gains. Our analysis suggests that the bottleneck lies in contextual interference rather than insufficient reasoning capacity. UReason provides a principled testbed for studying reasoning in unified models and motivates future methods that effectively integrate reasoning for visual generation while mitigating interference.

</details>


### [67] [WorldTravel: A Realistic Multimodal Travel-Planning Benchmark with Tightly Coupled Constraints](https://arxiv.org/abs/2602.08367)
*Zexuan Wang,Chenghao Yang,Yingqi Que,Zhenzhu Yang,Huaqing Yuan,Yiwen Wang,Zhengxuan Jiang,Shengjie Fang,Zhenhe Wu,Zhaohui Wang,Zhixin Yao,Jiashuo Liu,Jincheng Ren,Yuzhen Li,Yang Yang,Jiaheng Liu,Jian Yang,Zaiyuan Wang,Ge Zhang,Zhoufutu Wen,Wenhao Huang*

Main category: cs.CL

TL;DR: 本文构建了真实世界多约束旅行规划基准和多模态网页环境，揭示现有顶尖模型在感知与推理任务上存在显著瓶颈，推动未来多模态长程推理智能体的研究。


<details>
  <summary>Details</summary>
Motivation: 当前基准多处理松耦合约束和理想化数据，无法反映真实世界多约束规划和复杂动态网页环境参数提取的复杂性。

Method: 提出了WorldTravel基准，包含150个跨5城市的真实旅行场景，及WorldTravel-Webscape多模态环境，包含2000多个网页以视觉形式提供约束参数，评价10个前沿模型表现。

Result: 所有模型在文本环境中可行性仅约32.67%，多模态环境下降至19.33%，发现感知-行动鸿沟和约10个约束的规划难度阈值，说明感知和推理均存在瓶颈。

Conclusion: 现有模型在面对紧密耦合约束的真实世界问题时性能严重下降，尤其在多模态信息环境中表现极差，表现出感知与推理之间存在明显瓶颈，提示未来需要统一视觉感知与长程推理的综合智能体。

Abstract: Real-world autonomous planning requires coordinating tightly coupled constraints where a single decision dictates the feasibility of all subsequent actions. However, existing benchmarks predominantly feature loosely coupled constraints solvable through local greedy decisions and rely on idealized data, failing to capture the complexity of extracting parameters from dynamic web environments. We introduce \textbf{WorldTravel}, a benchmark comprising 150 real-world travel scenarios across 5 cities that demand navigating an average of 15+ interdependent temporal and logical constraints. To evaluate agents in realistic deployments, we develop \textbf{WorldTravel-Webscape}, a multi-modal environment featuring over 2,000 rendered webpages where agents must perceive constraint parameters directly from visual layouts to inform their planning. Our evaluation of 10 frontier models reveals a significant performance collapse: even the state-of-the-art GPT-5.2 achieves only 32.67\% feasibility in text-only settings, which plummets to 19.33\% in multi-modal environments. We identify a critical Perception-Action Gap and a Planning Horizon threshold at approximately 10 constraints where model reasoning consistently fails, suggesting that perception and reasoning remain independent bottlenecks. These findings underscore the need for next-generation agents that unify high-fidelity visual perception with long-horizon reasoning to handle brittle real-world logistics.

</details>


### [68] [ViGoEmotions: A Benchmark Dataset For Fine-grained Emotion Detection on Vietnamese Texts](https://arxiv.org/abs/2602.08371)
*Hung Quang Tran,Nam Tien Pham,Son T. Luu,Kiet Van Nguyen*

Main category: cs.CL

TL;DR: 本研究推出越南语情感语料库ViGoEmotions，评估表情符号预处理对情感分类效果影响，ViSoBERT模型取得最佳表现，表明语料质量和预处理策略关键影响下游性能。


<details>
  <summary>Details</summary>
Motivation: 情感分类对情感预测和有害内容检测至关重要，尤其在越南语等低资源语言中亟需高质量标注情感语料构建和模型性能验证。

Method: 构建涵盖20664条社交媒体评论的越南语情感语料库，包含27种细粒度情感标签；使用8种预训练Transformer模型，采用三种表情符号预处理策略（保留、文本转换、模型词汇规范化）进行情感分类评估。

Result: 转换表情符号为文本能提升部分BERT基线模型性能，保留表情符号对ViSoBERT和CafeBERT效果最佳，移除表情符号通常导致性能下降，ViSoBERT达到宏F1 61.50%和加权F1 63.26%。

Conclusion: ViGoEmotions越南语情感语料库通过对表情符号的不同预处理策略显著影响情感分类性能，ViSoBERT模型在宏F1和加权F1得分上表现最佳，验证了该语料库的有效性和多样架构支持性。

Abstract: Emotion classification plays a significant role in emotion prediction and harmful content detection. Recent advancements in NLP, particularly through large language models (LLMs), have greatly improved outcomes in this field. This study introduces ViGoEmotions -- a Vietnamese emotion corpus comprising 20,664 social media comments in which each comment is classified into 27 fine-grained distinct emotions. To evaluate the quality of the dataset and its impact on emotion classification, eight pre-trained Transformer-based models were evaluated under three preprocessing strategies: preserving original emojis with rule-based normalization, converting emojis into textual descriptions, and applying ViSoLex, a model-based lexical normalization system. Results show that converting emojis into text often improves the performance of several BERT-based baselines, while preserving emojis yields the best results for ViSoBERT and CafeBERT. In contrast, removing emojis generally leads to lower performance. ViSoBERT achieved the highest Macro F1-score of 61.50% and Weighted F1-score of 63.26%. Strong performance was also observed from CafeBERT and PhoBERT. These findings highlight that while the proposed corpus can support diverse architectures effectively, preprocessing strategies and annotation quality remain key factors influencing downstream performance.

</details>


### [69] [Dynamic Long Context Reasoning over Compressed Memory via End-to-End Reinforcement Learning](https://arxiv.org/abs/2602.08382)
*Zhuoen Chen,Dongfang Li,Meishan Zhang,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: 本文提出基于分块压缩与选择性记忆召回的长上下文推理框架，实现了极大上下文扩展与计算效率提升，显著优化了大语言模型的长文本处理能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型处理长上下文时面临高计算成本、信息遗忘和检索增强生成的上下文碎片化等挑战。

Method: 提出了一种基于分块压缩和选择性记忆召回的高效长上下文推理认知启发框架，利用压缩器编码记忆块，门控模块选择相关记忆，推理模块迭代处理。压缩器与推理器通过强化学习联合优化，门控模块作为分类器单独训练。

Result: 该方法在多跳推理基准测试中表现优异，实现了上下文长度从7K拓展至1.75M tokens，具有良好的准确率与效率折中，在峰值GPU内存使用减少2倍，推理速度提升6倍。

Conclusion: 所提框架有效缓解了长上下文处理的计算与记忆瓶颈，提升了大语言模型长文本推理的准确性与效率。

Abstract: Large Language Models (LLMs) face significant challenges in long-context processing, including quadratic computational costs, information forgetting, and the context fragmentation inherent in retrieval-augmented generation (RAG). We propose a cognitively inspired framework for efficient long-context inference based on chunk-wise compression and selective memory recall, rather than processing all raw tokens. The framework segments long inputs into chunks and encodes each chunk into compressed memory representations using a learned compressor. A gating module dynamically selects relevant memory blocks, which are then iteratively processed by a reasoning module with an evolving working memory to solve downstream tasks. The compressor and reasoner are jointly optimized via end-to-end reinforcement learning, while the gating module is trained separately as a classifier. Experimental results show that the proposed method achieves competitive accuracy on multi-hop reasoning benchmarks such as RULER-HQA, extrapolates context length from 7K to 1.75M tokens, and offers a favorable accuracy-efficiency trade-off compared to strong long-context baselines. In particular, it achieves up to a 2 times reduction in peak GPU memory usage and a 6 times inference speedup over MemAgent.

</details>


### [70] [TEAM: Temporal-Spatial Consistency Guided Expert Activation for MoE Diffusion Language Model Acceleration](https://arxiv.org/abs/2602.08404)
*Linye Wei,Zixiang Luo,Pingzhi Tang,Meng Li*

Main category: cs.CL

TL;DR: 针对MoE扩散语言模型推理效率低的问题，本文提出TEAM，通过专家激活优化和多候选探索策略，大幅提升解码速度，兼顾性能与效率。


<details>
  <summary>Details</summary>
Motivation: 传统MoE结构在扩散解码过程中激活大量专家但最终只接受少量token，导致推理开销大，限制了其在低延迟场景的应用。

Method: TEAM框架基于专家路由决策在时序和空间上的一致性，设计了三种专家激活和解码策略，选择必要专家同时进行多候选的大胆探索，从而减少激活专家数量但提高被接受的token数量。

Result: 实验表明，TEAM在保持性能的前提下，实现了对原始MoE扩散语言模型最高2.2倍的加速。

Conclusion: 本文提出的TEAM框架通过合理激活专家，实现了MoE扩散语言模型的显著加速，且性能损失极小，验证了其在实际应用中的有效性。

Abstract: Diffusion large language models (dLLMs) have recently gained significant attention due to their inherent support for parallel decoding. Building on this paradigm, Mixture-of-Experts (MoE) dLLMs with autoregressive (AR) initialization have further demonstrated strong performance competitive with mainstream AR models. However, we identify a fundamental mismatch between MoE architectures and diffusion-based decoding. Specifically, a large number of experts are activated at each denoising step, while only a small subset of tokens is ultimately accepted, resulting in substantial inference overhead and limiting their deployment in latency-sensitive applications. In this work, we propose TEAM, a plug-and-play framework that accelerates MoE dLLMs by enabling more accepted tokens with fewer activated experts. TEAM is motivated by the observation that expert routing decisions exhibit strong temporal consistency across denoising levels as well as spatial consistency across token positions. Leveraging these properties, TEAM employs three complementary expert activation and decoding strategies, conservatively selecting necessary experts for decoded and masked tokens and simultaneously performing aggressive speculative exploration across multiple candidates. Experimental results demonstrate that TEAM achieves up to 2.2x speedup over vanilla MoE dLLM, with negligible performance degradation. Code is released at https://github.com/PKU-SEC-Lab/TEAM-MoE-dLLM.

</details>


### [71] [Prism: Spectral-Aware Block-Sparse Attention](https://arxiv.org/abs/2602.08426)
*Xinghao Wang,Pengyu Wang,Xiaoran Liu,Fangxu Liu,Jason Chu,Kai Song,Xipeng Qiu*

Main category: cs.CL

TL;DR: 通过解决平均池化与RoPE间的干扰，Prism无训练地恢复关键位置信号，实现块稀疏注意力的高效块选择，提升长上下文LLM预填充速度至5.1倍。


<details>
  <summary>Details</summary>
Motivation: 现有基于块稀疏注意力的长上下文LLM预填充方法中，粗粒度注意力作为块重要性估计的代理导致选择开销大。

Method: 提出Prism方法，通过高频和低频分支分解块选择，利用能量基温度校准恢复被衰减的位置信号，无需训练即可纯块级操作估计块重要性。

Result: Prism在保持准确率的前提下，提供了最高5.1倍的加速效果。

Conclusion: Prism有效解决了粗粒度注意力中的平均池化与RoPE导致的位置信息盲区问题，实现了高效且准确的块重要性估计，显著提升了块稀疏注意力的效率。

Abstract: Block-sparse attention is promising for accelerating long-context LLM pre-filling, yet identifying relevant blocks efficiently remains a bottleneck. Existing methods typically employ coarse-grained attention as a proxy for block importance estimation, but often resort to expensive token-level searching or scoring, resulting in significant selection overhead. In this work, we trace the inaccuracy of standard coarse-grained attention via mean pooling to a theoretical root cause: the interaction between mean pooling and Rotary Positional Embeddings (RoPE). We prove that mean pooling acts as a low-pass filter that induces destructive interference in high-frequency dimensions, effectively creating a "blind spot" for local positional information (e.g., slash patterns). To address this, we introduce Prism, a training-free spectral-aware approach that decomposes block selection into high-frequency and low-frequency branches. By applying energy-based temperature calibration, Prism restores the attenuated positional signals directly from pooled representations, enabling block importance estimation using purely block-level operations, thereby improving efficiency. Extensive evaluations confirm that Prism maintains accuracy parity with full attention while delivering up to $\mathbf{5.1\times}$ speedup.

</details>


### [72] [Large Language Models and Impossible Language Acquisition: "False Promise" or an Overturn of our Current Perspective towards AI](https://arxiv.org/abs/2602.08437)
*Ziyan wang,Longlong Ma*

Main category: cs.CL

TL;DR: 论文回应乔姆斯基对大型语言模型的批评，通过设计不可能语言实验验证GPT-2在此类任务上的不足，支持其观点，并提出理论范式的更新建议。


<details>
  <summary>Details</summary>
Motivation: 回应乔姆斯基对ChatGPT及大型语言模型的批评，验证LLM是否能区分不可能语言，探讨其学习语言的本质和认知基础。

Method: 通过构造语法不可能语言（如句子反转和基于词数奇偶的否定）并对GPT-2小模型和LSTM模型进行两轮对照实验，采用Welch's t检验进行统计分析。

Result: GPT-2在学习不可能语言上的表现显著较差，符合乔姆斯基对LLM缺乏内在因果及自我纠正结构的批评；LSTM模型表现与乔姆斯基观点一致；变压器架构具有不可替代的作用。

Conclusion: 研究证明GPT-2模型在学习不可能语言的能力显著低于可能语言，支持乔姆斯基对LLM的批评，并指出变压器架构的重要性。基于理论分析和实验数据，提出了乔姆斯基理论内的新视角，以及从“理性主义-浪漫主义”范式向功能主义和经验主义的理论转变。

Abstract: In Chomsky's provocative critique "The False Promise of CHATGPT," Large Language Models (LLMs) are characterized as mere pattern predictors that do not acquire languages via intrinsic causal and self-correction structures like humans, therefore are not able to distinguish impossible languages. It stands as a representative in a fundamental challenge to the intellectual foundations of AI, for it integrally synthesizes major issues in methodologies within LLMs and possesses an iconic a priori rationalist perspective. We examine this famous critic from both the perspective in pre-existing literature of linguistics and psychology as well as a research based on an experiment inquiring the capacity of learning both possible and impossible languages among LLMs. We constructed a set of syntactically impossible languages by applying certain transformations to English. These include reversing whole sentences, and adding negation based on word-count parity. Two rounds of controlled experiments were each conducted on GPT-2 small models and long short-term memory (LSTM) models. Statistical analysis (Welch's t-test) shows GPT2 small models underperform in learning all of the impossible languages compared to their performance on the possible language (p<.001). On the other hand, LSTM models' performance tallies with Chomsky's argument, suggesting the irreplaceable role of the evolution of transformer architecture. Based on theoretical analysis and empirical findings, we propose a new vision within Chomsky's theory towards LLMs, and a shift of theoretical paradigm outside Chomsky, from his "rationalist-romantics" paradigm to functionalism and empiricism in LLMs research.

</details>


### [73] [Characterizing, Evaluating, and Optimizing Complex Reasoning](https://arxiv.org/abs/2602.08498)
*Haoran Zhang,Yafu Li,Zhi Wang,Zhilin Wang,Shunkai Zhang,Xiaoye Qu,Yu Cheng*

Main category: cs.CL

TL;DR: 本文提出基于ME²原则的DAG推理痕迹建模和评估方法，训练思维奖励模型优化大规模推理模型推理质量，有效提升性能表现。


<details>
  <summary>Details</summary>
Motivation: 解决大规模推理模型中推理质量定义不明确、缺乏有效评估长且隐式结构推理痕迹方法，以及难以利用评估信号优化推理三大难题。

Method: 提出基于ME²原则的有向无环图（DAG）推理痕迹建模，设计DAG对比评估方法，构建TRM-Preference数据集，训练思维奖励模型（TRM）用于评价和优化推理过程。

Result: 思维奖励模型作为有效优化信号，在测试时选择更优推理提升最高19.3%性能，强化学习训练时提升推理及任务性能最高3.9%。

Conclusion: 引入ME²原则并基于有向无环图建模推理过程，实现对复杂推理结构的统一评估和优化，有效提升推理质量和任务表现。

Abstract: Large Reasoning Models (LRMs) increasingly rely on reasoning traces with complex internal structures. However, existing work lacks a unified answer to three fundamental questions: (1) what defines high-quality reasoning, (2) how to reliably evaluate long, implicitly structured reasoning traces, and (3) how to use such evaluation signals for reasoning optimization. To address these challenges, we provide a unified perspective. (1) We introduce the ME$^2$ principle to characterize reasoning quality along macro- and micro-level concerning efficiency and effectiveness. (2) Built on this principle, we model reasoning traces as directed acyclic graphs (DAGs) and develop a DAG-based pairwise evaluation method, capturing complex reasoning structures. (3) Based on this method, we construct the TRM-Preference dataset and train a Thinking Reward Model (TRM) to evaluate reasoning quality at scale. Experiments show that thinking rewards serve as an effective optimization signal. At test time, selecting better reasoning leads to better outcomes (up to 19.3% gain), and during RL training, thinking rewards enhance reasoning and performance (up to 3.9% gain) across diverse tasks.

</details>


### [74] [GISA: A Benchmark for General Information-Seeking Assistant](https://arxiv.org/abs/2602.08543)
*Yutao Zhu,Xingshuo Zhang,Maosen Zhang,Jiajie Jin,Liancheng Zhang,Xiaoshuai Song,Kangzhi Zhao,Wencong Zeng,Ruiming Tang,Han Li,Ji-Rong Wen,Zhicheng Dou*

Main category: cs.CL

TL;DR: 本文提出了面向真实信息搜索场景的GISA基准，包含多样答案格式和动态数据，实验表明当前大语言模型在复杂任务中表现有限，存在显著提升空间。


<details>
  <summary>Details</summary>
Motivation: 现有的评测基准往往从答案反向构造查询，任务不自然且不符合真实世界需求，同时只关注定位信息或聚合信息，存在数据污染问题。

Method: 提出GISA基准，包含373个人工查询，涵盖真实信息搜索场景，设计有四种答案格式，结合深度推理和广泛信息聚合；提供实时更新的答案以防止记忆化，并包含完整的人类搜索轨迹以支持过程监督和模仿学习。

Result: 在主流大语言模型和商用搜索产品上进行实验，发现即使是表现最好的模型准确匹配率仅为19.30%，复杂规划和综合信息聚合任务表现尤为差，显示改进空间巨大。

Conclusion: GISA作为一个更自然且全面的评测基准，有效揭示了现有模型在复杂信息搜索任务中的不足，推动未来大语言模型和搜索代理的研究与提升。

Abstract: The advancement of large language models (LLMs) has significantly accelerated the development of search agents capable of autonomously gathering information through multi-turn web interactions. Various benchmarks have been proposed to evaluate such agents. However, existing benchmarks often construct queries backward from answers, producing unnatural tasks misaligned with real-world needs. Moreover, these benchmarks tend to focus on either locating specific information or aggregating information from multiple sources, while relying on static answer sets prone to data contamination. To bridge these gaps, we introduce GISA, a benchmark for General Information-Seeking Assistants comprising 373 human-crafted queries that reflect authentic information-seeking scenarios. GISA features four structured answer formats (item, set, list, and table), enabling deterministic evaluation. It integrates both deep reasoning and broad information aggregation within unified tasks, and includes a live subset with periodically updated answers to resist memorization. Notably, GISA provides complete human search trajectories for every query, offering gold-standard references for process-level supervision and imitation learning. Experiments on mainstream LLMs and commercial search products reveal that even the best-performing model achieves only 19.30\% exact match score, with performance notably degrading on tasks requiring complex planning and comprehensive information gathering. These findings highlight substantial room for future improvement.

</details>


### [75] [How Do Language Models Understand Tables? A Mechanistic Analysis of Cell Location](https://arxiv.org/abs/2602.08548)
*Xuanliang Zhang,Dingzirui Wang,Keyan Xu,Qingfu Zhu,Wanxiang Che*

Main category: cs.CL

TL;DR: 论文通过解析激活模式揭示了大型语言模型理解二维表格的机制，提出了一个三阶段定位管线，为表格相关任务提供理论支持。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在表格相关任务上表现优异，但其对二维结构化表格处理的内部机制尚不清晰，亟需深入理解模型如何实现表格理解。

Method: 采用激活修补和辅助可解释性技术，对模型内部机制进行分析解码，识别和划分表格理解过程中的关键步骤及其实现方法。

Result: 发现表格理解过程可分为三阶段管线，明确了模型如何通过计数离散分隔符定位单元格，识别出编码列索引的线性子空间，以及注意力头复用以实现多单元格定位的能力。

Conclusion: 该论文详细揭示了大型语言模型（LLMs）理解表格的内在机制，拆解为三个阶段：语义绑定、坐标定位和信息提取。模型通过计数分隔符实现对单元格的精确定位，列索引在一个线性子空间内编码，支持通过向量运算精准引导模型注意力。此外，模型还通过复用相同的注意力头，实现对多单元格定位任务的泛化。

Abstract: While Large Language Models (LLMs) are increasingly deployed for table-related tasks, the internal mechanisms enabling them to process linearized two-dimensional structured tables remain opaque. In this work, we investigate the process of table understanding by dissecting the atomic task of cell location. Through activation patching and complementary interpretability techniques, we delineate the table understanding mechanism into a sequential three-stage pipeline: Semantic Binding, Coordinate Localization, and Information Extraction. We demonstrate that models locate the target cell via an ordinal mechanism that counts discrete delimiters to resolve coordinates. Furthermore, column indices are encoded within a linear subspace that allows for precise steering of model focus through vector arithmetic. Finally, we reveal that models generalize to multi-cell location tasks by multiplexing the identical attention heads identified during atomic location. Our findings provide a comprehensive explanation of table understanding within Transformer architectures.

</details>


### [76] [Beyond Scalar Scores: Reinforcement Learning for Error-Aware Quality Estimation of Machine Translation](https://arxiv.org/abs/2602.08600)
*Archchana Sindhujan,Girish A. Koushik,Shenbin Qian,Diptesh Kanojia,Constantin Orăsan*

Main category: cs.CL

TL;DR: 本文提出了首个英语到马拉雅拉姆语的段级机器翻译质量评估数据集及基于错误感知奖励的强化学习方法ALOPE-RL，实现了低资源语言下高效且准确的翻译质量评估。


<details>
  <summary>Details</summary>
Motivation: 现有机器翻译质量评估方法多数依赖标量分数，缺乏对翻译错误的显式描述，且在低资源语言场景中表现欠佳。本文旨在通过引入包含错误描述的新数据集和错误感知的强化学习方法，提升低资源语言QE的效果。

Method: 本文提出了基于策略的强化学习框架ALOPE-RL，结合人类注释的直接评估分数和翻译质量备注作为奖励信号，训练高效适配器，并采用LoRA微调和4位量化来提升小型大模型的QE表现。

Result: ALOPE-RL在英语-马拉雅拉姆语评估任务中，在仅有的小规模数据上训练，使用参数少于4B的小型大模型，实现了超过更大规模模型和先进编码器模型的性能。

Conclusion: ALOPE-RL方法利用错误感知的强化学习奖励，在极其有限的数据和计算资源下，实现了英语到马拉雅拉姆语机器翻译质量评估的最新性能，超过了更大规模的模型和主流编码器基础QE模型。

Abstract: Quality Estimation (QE) aims to assess the quality of machine translation (MT) outputs without relying on reference translations, making it essential for real-world, large-scale MT evaluation. Large Language Models (LLMs) have shown significant promise in advancing the field of quality estimation of machine translation. However, most of the QE approaches solely rely on scalar quality scores, offering no explicit information about the translation errors that should drive these judgments. Moreover, for low-resource languages where annotated QE data is limited, existing approaches struggle to achieve reliable performance. To address these challenges, we introduce the first segment-level QE dataset for English to Malayalam, a severely resource-scarce language pair in the QE domain, comprising human-annotated Direct Assessment (DA) scores and Translation Quality Remarks (TQR), which are short, contextual, free-form annotator comments that describe translation errors. We further introduce ALOPE-RL, a policy-based reinforcement learning framework that trains efficient adapters based on policy rewards derived from DA score and TQR. Integrating error-aware rewards with ALOPE-RL, enables LLMs to reason about translation quality beyond numeric scores. Despite being trained on a small-scale QE dataset, ALOPE-RL achieves state-of-the-art performance on English to Malayalam QE using compact LLMs (<=4B parameters}) fine-tuned with LoRA and 4-bit quantization, outperforming both larger LLM-based baselines and leading encoder-based QE models. Our results demonstrate that error-aware, policy-based learning can deliver strong QE performance under limited data and compute budgets. We release our dataset, code, and trained models to support future research.

</details>


### [77] [VocalNet-MDM: Accelerating Streaming Speech LLM via Self-Distilled Masked Diffusion Modeling](https://arxiv.org/abs/2602.08607)
*Ziyang Cheng,Yuhao Wang,Heyang Liu,Ronghua Wu,Qunshan Gu,Yanfeng Wang,Yu Wang*

Main category: cs.CL

TL;DR: 本文提出了一种基于Masked Diffusion Modeling的语音大语言模型VocalNet-MDM，克服了自回归模型的串行瓶颈，实现了显著的速度提升和延迟降低，同时保持了良好的识别性能和语音质量。


<details>
  <summary>Details</summary>
Motivation: 当前语音大语言模型普遍采用自回归范式，存在串行生成效率低和暴露偏差等问题，亟需探索非自回归方法以提升生成效率和降低延迟。

Method: 采用Masked Diffusion Modeling (MDM)作为非自回归范式，设计Hierarchical Block-wise Masking对齐训练与推理过程，结合Iterative Self-Distillation压缩多步迭代以降低推理延迟。

Result: 在仅用6K小时语音数据训练下，VocalNet-MDM相比自回归基线模型，实现3.7到10倍的解码速度提升，首块延迟降低34%，同时保持竞争性的识别准确率和更优的文本质量与语音自然度。

Conclusion: 本论文提出的基于Masked Diffusion Modeling (MDM)的VocalNet-MDM模型，实现了比传统自回归模型更高的生成效率和更低的延迟，同时维持了竞争力的识别准确率和出色的文本质量及语音自然性，表明MDM是一种有前景的低延迟高效语音大语言模型替代方案。

Abstract: Recent Speech Large Language Models~(LLMs) have achieved impressive capabilities in end-to-end speech interaction. However, the prevailing autoregressive paradigm imposes strict serial constraints, limiting generation efficiency and introducing exposure bias. In this paper, we investigate Masked Diffusion Modeling~(MDM) as a non-autoregressive paradigm for speech LLMs and introduce VocalNet-MDM. To adapt MDM for streaming speech interaction, we address two critical challenges: training-inference mismatch and iterative overhead. We propose Hierarchical Block-wise Masking to align training objectives with the progressive masked states encountered during block diffusion decoding, and Iterative Self-Distillation to compress multi-step refinement into fewer steps for low-latency inference. Trained on a limited scale of only 6K hours of speech data, VocalNet-MDM achieves a 3.7$\times$--10$\times$ decoding speedup and reduces first-chunk latency by 34\% compared to AR baselines. It maintains competitive recognition accuracy while achieving state-of-the-art text quality and speech naturalness, demonstrating that MDM is a promising and scalable alternative for low-latency, efficient speech LLMs.

</details>


### [78] [Do Multilingual LLMs have specialized language heads?](https://arxiv.org/abs/2602.08625)
*Muhammad Naufil*

Main category: cs.CL

TL;DR: 本文分析了多语言大型语言模型中的语言专用注意力头，证明移除非目标语言头能有效简化模型并保持性能，提高部署效率。


<details>
  <summary>Details</summary>
Motivation: 现有多语言大型语言模型虽能处理多语种，但部署效率低下，尤其当只关注部分语言时，因此希望通过识别和移除非目标语言的专用结构实现高效部署。

Method: 通过分析多语言大型语言模型中的注意力头，识别哪些是语言特定的，并实验移除非目标语言的专用头以评估对性能的影响。

Result: 研究发现多语言LLM具有语言专用的注意力头，移除非目标语言专用头可降低模型复杂度，同时保持高准确率。

Conclusion: 多语言大型语言模型确实存在针对特定语言的注意力头，移除非目标语言的专用注意力头可以在不影响目标语言性能的情况下实现模型简化。

Abstract: Multilingual large language models (LLMs) have gained significant popularity for their ability to process and generate text across multiple languages. However, deploying these models in production can be inefficient when only a subset of the supported languages is of interest. There has been some research conducted on identifying whether machine translation models have language-specific or language-agnostic heads, however no research has been conducted for multilingual LLMs, to the best of our knowledge, that as we know are capable of performing diverse tasks beyond just translation. This paper explores whether multilingual LLMs have specialized language attention heads for each language, and investigates the possibility of removing language-specific heads for unwanted languages without degrading performance in the targeted languages. Our findings could inform more efficient deployment strategies for multilingual LLMs, enabling reduced model complexity while maintaining high accuracy for targeted languages.

</details>


### [79] [Fundamental Reasoning Paradigms Induce Out-of-Domain Generalization in Language Models](https://arxiv.org/abs/2602.08658)
*Mingzi Cao,Xingwei Tan,Mahmud Akhter,Marco Valentino,Maria Liakata,Xi Wang,Nikolaos Aletras*

Main category: cs.CL

TL;DR: 本文研究演绎、归纳和溯因三大推理范式对大型语言模型泛化能力的影响，通过符号任务数据集诱导模型推理技能，显著提升模型的现实任务表现。


<details>
  <summary>Details</summary>
Motivation: 尽管提升大型语言模型推理能力受到重视，但三大基本推理范式对模型泛化能力的系统影响尚未被深入探究，本研究旨在填补这一空白。

Method: 收集基于符号任务的新推理轨迹数据集，设计实验采用简单微调、增加模型深度及转变为专家混合模型等多种方法诱导推理能力，再在真实自然语言的开放领域任务中进行综合评估。

Result: 诱导推理技能的方法带来显著性能提升，在多个现实任务中最高提效达14.60。

Conclusion: 本研究揭示了演绎、归纳和溯因三大推理范式如何相互作用影响大型语言模型的推理表现，并证明通过有效的方法诱导这些推理技能可以显著提升模型在现实世界任务中的泛化能力。

Abstract: Deduction, induction, and abduction are fundamental reasoning paradigms, core for human logical thinking. Although improving Large Language Model (LLM) reasoning has attracted significant research efforts, the extent to which the fundamental paradigms induce generalization has yet to be systematically explored. In this study, we shed light on how the interplay between these core paradigms influences LLMs' reasoning behavior. To this end, we first collect a new dataset of reasoning trajectories from symbolic tasks, each targeting one of the three fundamental paradigms, to abstract from concrete world knowledge. Then, we investigate effective ways for inducing these skills into LLMs. We experiment with a battery of methods including simple fine-tuning, and more complex approaches to increase model depth, or transform a dense model to a mixture-of-experts. We comprehensively evaluate induced models on realistic out-of-domain tasks, that are entirely formulated in natural language and contain real-world knowledge. Our results reveal that our approach yields strong generalizability with substantial performance gains (up to $14.60$) across realistic tasks.

</details>


### [80] [Learning to Judge: LLMs Designing and Applying Evaluation Rubrics](https://arxiv.org/abs/2602.08672)
*Clemencia Siro,Pourya Aliannejadi,Mohammad Aliannejadi*

Main category: cs.CL

TL;DR: 研究发现LLMs可以自生成评估标准并一致应用，但不同模型间存在差异，闭源模型表现更好，提示未来需融合人类和LLM评估语言提高评估质量。


<details>
  <summary>Details</summary>
Motivation: 现有人类定义的评分标准静态且与模型的内部语言质量表示不一致，探索LLMs是否能自主设计和应用评估标准。

Method: 提出GER-Eval框架，让LLMs生成并应用自己的评估标准，检验其语义连贯性和评分可靠性，并与人工标准对比。

Result: LLMs能够生成可解释、任务感知的评估维度，且在模型内部评分一致性高，但在事实与知识密集任务中评分可靠性下降。闭源模型如GPT-4o表现优于Llama。

Conclusion: LLMs能够自生成评估标准，并在模型内部一致应用，但在涉及事实和知识密集的任务中评分可靠性下降。不同模型间标准存在差异，闭源模型表现优于开源模型。

Abstract: Large language models (LLMs) are increasingly used as evaluators for natural language generation, applying human-defined rubrics to assess system outputs. However, human rubrics are often static and misaligned with how models internally represent language quality. We introduce GER-Eval (Generating Evaluation Rubrics for Evaluation) to investigate whether LLMs can design and apply their own evaluation rubrics. We evaluate the semantic coherence and scoring reliability of LLM-defined criteria and their alignment with human criteria. LLMs reliably generate interpretable and task-aware evaluation dimensions and apply them consistently within models, but their scoring reliability degrades in factual and knowledge-intensive settings. Closed-source models such as GPT-4o achieve higher agreement and cross-model generalization than open-weight models such as Llama. Our findings position evaluation as a learned linguistic capability of LLMs, consistent within models but fragmented across them, and call for new methods that jointly model human and LLM evaluative language to improve reliability and interpretability.

</details>


### [81] [Old wine in old glasses: Comparing computational and qualitative methods in identifying incivility on Persian Twitter during the #MahsaAmini movement](https://arxiv.org/abs/2602.08688)
*Hossein Kermani,Fatemeh Oudlajani,Pardis Yarahmadi,Hamideh Mahdi Soltani,Mohammad Makki,Zahra HosseiniKhoo*

Main category: cs.CL

TL;DR: 本文评估了人工编码、ParsBERT和ChatGPT三种方法检测波斯语不文明推文的效果，发现ParsBERT表现最好，ChatGPT表现较差且语言提示影响有限。


<details>
  <summary>Details</summary>
Motivation: 本文旨在比较不同方法检测波斯语推文中的不文明言论，尤其是在资源较少的语言环境下，提供方法选择的指导。

Method: 采用三种方法对47,278条伊朗#MahsaAmini运动相关推文进行分析：人工定性编码、基于ParsBERT的监督学习以及大型语言模型（ChatGPT）。

Result: ParsBERT的表现明显优于七个ChatGPT模型，后者在识别仇恨言论方面表现较弱，且提示语言（英语与波斯语）对ChatGPT输出影响不大。

Conclusion: 在检测波斯语推文中的仇恨言论时，基于ParsBERT的监督学习方法更具准确性和效率，ChatGPT存在明显局限。

Abstract: This paper compares three approaches to detecting incivility in Persian tweets: human qualitative coding, supervised learning with ParsBERT, and large language models (ChatGPT). Using 47,278 tweets from the #MahsaAmini movement in Iran, we evaluate the accuracy and efficiency of each method. ParsBERT substantially outperforms seven evaluated ChatGPT models in identifying hate speech. We also find that ChatGPT struggles not only with subtle cases but also with explicitly uncivil content, and that prompt language (English vs. Persian) does not meaningfully affect its outputs. The study provides a detailed comparison of these approaches and clarifies their strengths and limitations for analyzing hate speech in a low-resource language context.

</details>


### [82] [Challenges in Translating Technical Lectures: Insights from the NPTEL](https://arxiv.org/abs/2602.08698)
*Basudha Raje,Sadanand Venkatraman,Nandana TP,Soumyadeepa Das,Polkam Poojitha,M. Vijaykumar,Tanima Bagchi,Hema A. Murthy*

Main category: cs.CL

TL;DR: 该研究探讨了印度三种语言的机器翻译及其评估，强调现有指标对语言形态和语义的适应性不足，呼吁针对印度语言特点优化方法。


<details>
  <summary>Details</summary>
Motivation: 印度多语言背景下，教育技术需支持多语种，尤其是在NEP 2020政策推动下，机器翻译技术在本地化教育内容中的应用日益重要。

Method: 通过选取孟加拉语、马拉雅拉姆语和泰卢固语三种印度语言，利用最大MOOC平台NPTEL的自发语音语料库，分析了不同指标在机器翻译中的表现。

Result: 发现当前机器翻译评估指标对形态复杂和语义密集的印度语言敏感度不够，表现出方法论上的局限。

Conclusion: 研究表明在多语言机器翻译中，针对形态丰富和语义紧凑语言的传统表面重叠评估指标存在不足，需针对性改进评估方法。

Abstract: This study examines the practical applications and methodological implications of Machine Translation in Indian Languages, specifically Bangla, Malayalam, and Telugu, within emerging translation workflows and in relation to existing evaluation frameworks. The choice of languages prioritized in this study is motivated by a triangulation of linguistic diversity, which illustrates the significance of multilingual accommodation of educational technology under NEP 2020. This is further supported by the largest MOOC portal, i.e., NPTEL, which has served as a corpus to facilitate the arguments presented in this paper. The curation of a spontaneous speech corpora that accounts for lucid delivery of technical concepts, considering the retention of suitable register and lexical choices are crucial in a diverse country like India. The findings of this study highlight metric-specific sensitivity and the challenges of morphologically rich and semantically compact features when tested against surface overlapping metrics.

</details>


### [83] [Do Images Clarify? A Study on the Effect of Images on Clarifying Questions in Conversational Search](https://arxiv.org/abs/2602.08700)
*Clemencia Siro,Zahra Abbasiantaeb,Yifei Yuan,Mohammad Aliannejadi,Maarten de Rijke*

Main category: cs.CL

TL;DR: 本研究通过用户实验探讨图像在对话式搜索澄清问题中的作用，发现视觉辅助效果依任务和用户而异，设计多模态对话搜索系统时需考虑具体搜索任务和用户特性。


<details>
  <summary>Details</summary>
Motivation: 尽管文本澄清问题已被证明有助于提升检索性能和用户体验，但图像作为澄清问题的辅助工具对用户表现的影响尚未充分研究。

Method: 通过对73名参与者进行用户研究，比较多模态（图文）与纯文本澄清问题在对澄清问题回答和查询重构两种搜索任务中的表现差异。

Result: 参与者在回答澄清问题时更偏好多模态，查询重构时偏好均衡；图像有助于维持不同专业程度用户的参与度，促进更精确查询的生成及检索性能提升；但在澄清问题回答任务中，纯文本提供更全面信息，表现更佳。

Conclusion: 多模态的澄清问题在回答澄清问题的任务中受到用户偏好，而在查询重构任务中偏好较均衡，视觉增强的效果受任务类型和用户专业程度影响。文本信息缺失的情况下，纯文本澄清问题在用户表现上更优。

Abstract: Conversational search systems increasingly employ clarifying questions to refine user queries and improve the search experience. Previous studies have demonstrated the usefulness of text-based clarifying questions in enhancing both retrieval performance and user experience. While images have been shown to improve retrieval performance in various contexts, their impact on user performance when incorporated into clarifying questions remains largely unexplored. We conduct a user study with 73 participants to investigate the role of images in conversational search, specifically examining their effects on two search-related tasks: (i) answering clarifying questions and (ii) query reformulation. We compare the effect of multimodal and text-only clarifying questions in both tasks within a conversational search context from various perspectives. Our findings reveal that while participants showed a strong preference for multimodal questions when answering clarifying questions, preferences were more balanced in the query reformulation task. The impact of images varied with both task type and user expertise. In answering clarifying questions, images helped maintain engagement across different expertise levels, while in query reformulation they led to more precise queries and improved retrieval performance. Interestingly, for clarifying question answering, text-only setups demonstrated better user performance as they provided more comprehensive textual information in the absence of images. These results provide valuable insights for designing effective multimodal conversational search systems, highlighting that the benefits of visual augmentation are task-dependent and should be strategically implemented based on the specific search context and user characteristics.

</details>


### [84] [FactSim: Fact-Checking for Opinion Summarization](https://arxiv.org/abs/2602.08709)
*Leandro Anghinoni,Jorge Sanchez*

Main category: cs.CL

TL;DR: 本文提出一种基于文本事实陈述相似度的全自动指标，有效提升了生成观点摘要的事实一致性评估，与人工评价高度一致。


<details>
  <summary>Details</summary>
Motivation: 传统自动评估方法无法有效应对大型语言模型引入的范式转变，导致评估生成摘要的事实准确性存在不足。

Method: 通过提取摘要与原始评论中的陈述，衡量其相似性以评估覆盖度和一致性，最终生成一个综合得分。

Result: 该方法能够对相似言论给出较高评分，无论是否是否定、意译或扩展，且其评分与人工判断高度相关，优于现有主流指标。

Conclusion: 本文提出了一种全自动的方法来评估生成文本摘要的事实一致性，特别适用于意见摘要任务，克服了传统自动评估指标的局限。

Abstract: We explore the need for more comprehensive and precise evaluation techniques for generative artificial intelligence (GenAI) in text summarization tasks, specifically in the area of opinion summarization. Traditional methods, which leverage automated metrics to compare machine-generated summaries from a collection of opinion pieces, e.g. product reviews, have shown limitations due to the paradigm shift introduced by large language models (LLM). This paper addresses these shortcomings by proposing a novel, fully automated methodology for assessing the factual consistency of such summaries. The method is based on measuring the similarity between the claims in a given summary with those from the original reviews, measuring the coverage and consistency of the generated summary. To do so, we rely on a simple approach to extract factual assessment from texts that we then compare and summarize in a suitable score. We demonstrate that the proposed metric attributes higher scores to similar claims, regardless of whether the claim is negated, paraphrased, or expanded, and that the score has a high correlation to human judgment when compared to state-of-the-art metrics.

</details>


### [85] [PERSPECTRA: A Scalable and Configurable Pluralist Benchmark of Perspectives from Arguments](https://arxiv.org/abs/2602.08716)
*Shangrui Nie,Kian Omoomi,Lucie Flek,Zhixue Zhao,Charles Welch*

Main category: cs.CL

TL;DR: 本文提出PERSPECTRA基准，结合Reddit和Kialo数据，系统评估大语言模型对多元观点的识别与推理能力，发现现有模型在多视角理解上存在显著缺陷。


<details>
  <summary>Details</summary>
Motivation: 大语言模型需要准确反映人类多样化观点，但现有研究缺乏对多元主义特征的系统考察，现有辩论数据存在验证成本高或结构不完整等问题，促使构建结合多样性与结构性的多元观点评测工具。

Method: 通过整合Kialo辩论图的结构清晰度与Reddit讨论的语言多样性，构建了3810个丰富观点，涵盖762个正反立场和100个争议话题，并设计了观点计数、观点匹配和极性检测三项任务进行评估。

Result: 构建了融合结构和语言多样性的PERSPECTRA数据集，设计多任务评测框架，实验揭示主流开源及专有大模型在观点数量估算与让步结构分类上的系统性失误，验证了多元主义理解的挑战性。

Conclusion: 本论文提出了首个可扩展且可配置的多元观点评估基准PERSPECTRA，以有效评测大语言模型在多视角表示与推理中的能力，实验结果表明当前先进模型在多元观点识别与理解上存在系统性不足。

Abstract: Pluralism, the capacity to engage with diverse perspectives without collapsing them into a single viewpoint, is critical for developing large language models that faithfully reflect human heterogeneity. Yet this characteristic has not been carefully examined in the LLM research community and remains absent from most alignment studies. Debate-oriented sources provide a natural entry point for pluralism research. Previous work builds on online debate sources but remains constrained by costly human validation. Other debate-rich platforms such as Reddit and Kialo also offer promising material: Reddit provides linguistic diversity and scale but lacks clear argumentative structure, while Kialo supplies explicit pro/con graphs but remains overly concise and detached from natural discourse. We introduce PERSPECTRA, a pluralist benchmark that integrates the structural clarity of Kialo debate graphs with the linguistic diversity of real Reddit discussions. Using a controlled retrieval-and-expansion pipeline, we construct 3,810 enriched arguments spanning 762 pro/con stances on 100 controversial topics. Each opinion is expanded to multiple naturalistic variants, enabling robust evaluation of pluralism. We initialise three tasks with PERSPECTRA: opinion counting (identifying distinct viewpoints), opinion matching (aligning supporting stances and discourse to source opinions), and polarity check (inferring aggregate stance in mixed discourse). Experiments with state-of-the-art open-source and proprietary LLMs, highlight systematic failures, such as overestimating the number of viewpoints and misclassifying concessive structures, underscoring the difficulty of pluralism-aware understanding and reasoning. By combining diversity with structure, PERSPECTRA establishes the first scalable, configurable benchmark for evaluating how well models represent, distinguish, and reason over multiple perspectives.

</details>


### [86] [Map of Encoders -- Mapping Sentence Encoders using Quantum Relative Entropy](https://arxiv.org/abs/2602.08740)
*Gaifan Zhang,Danushka Bollegala*

Main category: cs.CL

TL;DR: 本文提出利用量子相对熵构建句子编码器特征向量并映射1101个编码器，准确反映其相似度和预测任务性能，助力编码器比较与选择。


<details>
  <summary>Details</summary>
Motivation: 当前存在大量预训练句子编码器，缺乏一种有效方法来比较和直观展示它们之间的关系。

Method: 提出通过构建句子集的嵌入矩阵，计算成对内积矩阵（PIP），并利用量子相对熵（QRE）相对于基准编码器生成特征向量，从而构建编码器之间的映射。

Result: 构建了涵盖1101个公开句子编码器的地图，该地图准确反映了编码器之间的关系，属性相近的编码器在地图上位置相近，并且特征向量能有效预测下游任务表现。

Conclusion: 该方法提供了一种大规模比较和可视化句子编码器的有效手段，能够揭示编码器之间的内在关系并辅助预测其在实际任务中的表现。

Abstract: We propose a method to compare and visualise sentence encoders at scale by creating a map of encoders where each sentence encoder is represented in relation to the other sentence encoders. Specifically, we first represent a sentence encoder using an embedding matrix of a sentence set, where each row corresponds to the embedding of a sentence. Next, we compute the Pairwise Inner Product (PIP) matrix for a sentence encoder using its embedding matrix. Finally, we create a feature vector for each sentence encoder reflecting its Quantum Relative Entropy (QRE) with respect to a unit base encoder. We construct a map of encoders covering 1101 publicly available sentence encoders, providing a new perspective of the landscape of the pre-trained sentence encoders. Our map accurately reflects various relationships between encoders, where encoders with similar attributes are proximally located on the map. Moreover, our encoder feature vectors can be used to accurately infer downstream task performance of the encoders, such as in retrieval and clustering tasks, demonstrating the faithfulness of our map.

</details>


### [87] [LakeHopper: Cross Data Lakes Column Type Annotation through Model Adaptation](https://arxiv.org/abs/2602.08793)
*Yushi Sun,Xujia Li,Nan Tang,Quanqing Xu,Chuanhui Yang,Lei Chen*

Main category: cs.CL

TL;DR: 本文提出LakeHopper，一个通过知识差距调节、样本选择和增量微调实现预训练模型在新数据湖高效迁移的框架，显著减少了标注工作。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于资源密集型的语言模型微调且需要大量特定数据湖的标注列，如何在新数据湖上最小化注释需求、有效迁移预训练模型成为挑战。

Method: 提出LakeHopper框架，包括通过语言模型交互识别并解决知识差距、基于聚类的数据选择策略以及逐步微调的增量适配机制。

Result: 实验在两种不同数据湖迁移场景下验证了LakeHopper的有效性，表现出在低资源和高资源环境下均能实现良好的适配和性能提升。

Conclusion: LakeHopper框架成功解决了源数据湖和目标数据湖之间的知识差距，实现了在目标数据湖上基于预训练语言模型的列类型标注的有效适配，减少了对大量标注数据的依赖。

Abstract: Column type annotation is vital for tasks like data cleaning, integration, and visualization. Recent solutions rely on resource-intensive language models fine-tuned on well-annotated columns from a particular set of tables, i.e., a source data lake. In this paper, we study whether we can adapt an existing pre-trained LM-based model to a new (i.e., target) data lake to minimize the annotations required on the new data lake. However, challenges include the source-target knowledge gap, selecting informative target data, and fine-tuning without losing shared knowledge exist. We propose LakeHopper, a framework that identifies and resolves the knowledge gap through LM interactions, employs a cluster-based data selection scheme for unannotated columns, and uses an incremental fine-tuning mechanism that gradually adapts the source model to the target data lake. Our experimental results validate the effectiveness of LakeHopper on two different data lake transfers under both low-resource and high-resource settings.

</details>


### [88] [Affective Flow Language Model for Emotional Support Conversation](https://arxiv.org/abs/2602.08826)
*Chenghui Zou,Ning Wang,Tiesunlong Shen,Luwei Xiao,Chuan Ma,Xiangpeng Li,Rui Mao,Erik Cambria*

Main category: cs.CL

TL;DR: 本文提出的AFlow框架通过细粒度情感流建模和子路径层级优化，提升了多轮情感支持对话的策略一致性和共情效果，优于多种主流模型。


<details>
  <summary>Details</summary>
Motivation: 现有的情感支持对话模型主要依赖稀疏的结果级信号，难以有效指导中间策略决策，导致复杂多轮支持效果不理想。

Method: AFlow通过对多轮对话轨迹建模连续的情感流，实现对对话前缀的细粒度监督，并引入子路径层级的流平衡目标以传递偏好信号。

Result: 在多种情感场景下，AFlow显著优于竞争基线模型，且基于紧凑的开源背骨，性能超过了GPT-4o和Claude-3.5等专有大型模型。

Conclusion: 提出的AFlow框架在多轮情感支持对话中有效提升了策略一致性和共情回应质量，实验结果优于现有主流模型。

Abstract: Large language models (LLMs) have been widely applied to emotional support conversation (ESC). However, complex multi-turn support remains challenging.This is because existing alignment schemes rely on sparse outcome-level signals, thus offering limited supervision for intermediate strategy decisions. To fill this gap, this paper proposes affective flow language model for emotional support conversation (AFlow), a framework that introduces fine-grained supervision on dialogue prefixes by modeling a continuous affective flow along multi-turn trajectories. AFlow can estimate intermediate utility over searched trajectories and learn preference-consistent strategy transitions. To improve strategy coherence and empathetic response quality, a subpath-level flow-balance objective is presented to propagate preference signals to intermediate states. Experiment results show consistent and significant improvements over competitive baselines in diverse emotional contexts. Remarkably, AFlow with a compact open-source backbone outperforms proprietary LMMs such as GPT-4o and Claude-3.5 on major ESC metrics. Our code is available at https://github.com/chzou25-lgtm/AffectiveFlow.

</details>


### [89] [WildReward: Learning Reward Models from In-the-Wild Human Interactions](https://arxiv.org/abs/2602.08829)
*Hao Peng,Yunjia Qi,Xiaozhi Wang,Zijun Yao,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: 本文提出通过真实环境用户交互数据直接训练奖励模型WildReward，无需偏好对，达到甚至超越传统方法性能，且能利用用户多样性提升效果，显著改进在线训练任务表现。


<details>
  <summary>Details</summary>
Motivation: 传统的奖励模型依赖大规模人工标注的偏好对，然而随着大型语言模型的广泛部署，真实环境中用户交互成为丰富的隐式奖励信号来源。如何直接利用真实环境的用户交互来训练奖励模型成为挑战。

Method: 采用WildChat作为交互数据源，设计一套流水线从用户反馈中提取可靠的人工信号，生成186k高质量训练样本。使用序数回归方法直接基于用户反馈训练WildReward模型，无需偏好对。

Result: 实验表明WildReward在性能、校准度和样本间一致性方面优于或不逊于传统奖励模型。模型性能随着用户多样性提升而增强。在在线DPO训练中，WildReward在多任务上带来显著改进。

Conclusion: 基于真实环境用户交互的奖励模型训练方法WildReward有效且优越，无需依赖人工偏好对，利用用户多样性提升模型能力，推动了奖励模型的实用性和泛化能力。

Abstract: Reward models (RMs) are crucial for the training of large language models (LLMs), yet they typically rely on large-scale human-annotated preference pairs. With the widespread deployment of LLMs, in-the-wild interactions have emerged as a rich source of implicit reward signals. This raises the question: Can we develop reward models directly from in-the-wild interactions? In this work, we explore this possibility by adopting WildChat as an interaction source and proposing a pipeline to extract reliable human feedback, yielding 186k high-quality instances for training WildReward via ordinal regression directly on user feedback without preference pairs. Extensive experiments demonstrate that WildReward achieves comparable or even superior performance compared to conventional reward models, with improved calibration and cross-sample consistency. We also observe that WildReward benefits directly from user diversity, where more users yield stronger reward models. Finally, we apply WildReward to online DPO training and observe significant improvements across various tasks. Code and data are released at https://github.com/THU-KEG/WildReward.

</details>


### [90] [Understanding Dynamic Compute Allocation in Recurrent Transformers](https://arxiv.org/abs/2602.08864)
*Ibraheem Muhammad Moosa,Suhas Lohit,Ye Wang,Moitreya Chatterjee,Wenpeng Yin*

Main category: cs.CL

TL;DR: 该论文提出了一种新的评测方法和模型框架，系统分析了基于令牌的自适应计算资源分配，揭示了计算对齐复杂度的条件及其对泛化的影响。


<details>
  <summary>Details</summary>
Motivation: 现有工作难以直接观察令牌级别的计算分配与复杂度的关系，缺乏可控评测范式。

Method: 提出了ANIRA统一递归Transformer框架，用于支持基于令牌的可变深度计算，并采用复杂度可控的算法与合成语言任务进行评估。

Result: 发现计算分配虽自然对应任务复杂度，但不代表算法泛化；早期决策依赖静态结构信息，在线停止机制更贴合算法执行状态。

Conclusion: 计算资源分配与任务复杂度的对齐可以自然出现，但不一定保证算法的泛化能力；模型在未见过的输入规模上表现不佳。

Abstract: Token-level adaptive computation seeks to reduce inference cost by allocating more computation to harder tokens and less to easier ones. However, prior work is primarily evaluated on natural-language benchmarks using task-level metrics, where token-level difficulty is unobservable and confounded with architectural factors, making it unclear whether compute allocation truly aligns with underlying complexity. We address this gap through three contributions. First, we introduce a complexity-controlled evaluation paradigm using algorithmic and synthetic language tasks with parameterized difficulty, enabling direct testing of token-level compute allocation. Second, we propose ANIRA, a unified recurrent Transformer framework that supports per-token variable-depth computation while isolating compute allocation decisions from other model factors. Third, we use this framework to conduct a systematic analysis of token-level adaptive computation across alignment with complexity, generalization, and decision timing. Our results show that compute allocation aligned with task complexity can emerge without explicit difficulty supervision, but such alignment does not imply algorithmic generalization: models fail to extrapolate to unseen input sizes despite allocating additional computation. We further find that early compute decisions rely on static structural cues, whereas online halting more closely tracks algorithmic execution state.

</details>


### [91] [Large Language Models for Geolocation Extraction in Humanitarian Crisis Response](https://arxiv.org/abs/2602.08872)
*G. Cafferata,T. Demarco,K. Kalimeri,Y. Mejova,M. G. Beiró*

Main category: cs.CL

TL;DR: 本研究利用大语言模型改进人道危机文本中的地理位置提取，提升准确性和公平性，特别是欠代表地区的表现，推动更公平的地理信息系统建设。


<details>
  <summary>Details</summary>
Motivation: 当前自动地理信息提取系统存在地理和社会经济偏见，导致危机影响地区的可见性不均。

Method: 提出一个两步框架，结合少量示例的LLM命名实体识别与基于代理的地理编码模块，利用上下文解析歧义地名。

Result: 基于LLM的方法在HumSet数据集上显著提升了地理位置提取的准确性和公平性，尤其是对欠代表地区。

Conclusion: 结合LLM推理能力与负责包容的AI原则，有助于构建更加公平的地理空间数据系统，改善人道主义响应的地理信息提取。

Abstract: Humanitarian crises demand timely and accurate geographic information to inform effective response efforts. Yet, automated systems that extract locations from text often reproduce existing geographic and socioeconomic biases, leading to uneven visibility of crisis-affected regions. This paper investigates whether Large Language Models (LLMs) can address these geographic disparities in extracting location information from humanitarian documents. We introduce a two-step framework that combines few-shot LLM-based named entity recognition with an agent-based geocoding module that leverages context to resolve ambiguous toponyms. We benchmark our approach against state-of-the-art pretrained and rule-based systems using both accuracy and fairness metrics across geographic and socioeconomic dimensions. Our evaluation uses an extended version of the HumSet dataset with refined literal toponym annotations. Results show that LLM-based methods substantially improve both the precision and fairness of geolocation extraction from humanitarian texts, particularly for underrepresented regions. By bridging advances in LLM reasoning with principles of responsible and inclusive AI, this work contributes to more equitable geospatial data systems for humanitarian response, advancing the goal of leaving no place behind in crisis analytics.

</details>


### [92] [Is Reasoning Capability Enough for Safety in Long-Context Language Models?](https://arxiv.org/abs/2602.08874)
*Yu Fu,Haz Sameen Shahgir,Huanli Gong,Zhipeng Wei,N. Benjamin Erichson,Yue Dong*

Main category: cs.CL

TL;DR: 研究表明大语言模型的推理能力提升并不必然带来安全性增强，长上下文环境下模型易被拆分有害查询攻击，增加推理计算资源可部分缓解该问题。


<details>
  <summary>Details</summary>
Motivation: 研究假设更强的推理能力可以提升大语言模型（LLMs）的安全性，使其识别隐含的有害意图。

Method: 提出了组合推理攻击的新威胁模型，将有害查询拆分成分散在长上下文中的不完整片段，通过中性推理查询诱导模型检索和合成，从而揭露有害意图。

Result: 评估14个前沿大模型，发现：1）更强的推理能力不意味着更强的安全性；2）随着上下文长度增加，安全性对齐下降；3）加大推理计算资源可以显著降低攻击成功率。

Conclusion: 安全性并不会随着推理能力的增强自动提高，尤其是在长上下文推理环境下，安全保障仍需重点关注。

Abstract: Large language models (LLMs) increasingly combine long-context processing with advanced reasoning, enabling them to retrieve and synthesize information distributed across tens of thousands of tokens. A hypothesis is that stronger reasoning capability should improve safety by helping models recognize harmful intent even when it is not stated explicitly. We test this hypothesis in long-context settings where harmful intent is implicit and must be inferred through reasoning, and find that it does not hold. We introduce compositional reasoning attacks, a new threat model in which a harmful query is decomposed into incomplete fragments that scattered throughout a long context. The model is then prompted with a neutral reasoning query that induces retrieval and synthesis, causing the harmful intent to emerge only after composition. Evaluating 14 frontier LLMs on contexts up to 64k tokens, we uncover three findings: (1) models with stronger general reasoning capability are not more robust to compositional reasoning attacks, often assembling the intent yet failing to refuse; (2) safety alignment consistently degrades as context length increases; and (3) inference-time reasoning effort is a key mitigating factor: increasing inference-time compute reduces attack success by over 50 percentage points on GPT-oss-120b model. Together, these results suggest that safety does not automatically scale with reasoning capability, especially under long-context inference.

</details>


### [93] [GitSearch: Enhancing Community Notes Generation with Gap-Informed Targeted Search](https://arxiv.org/abs/2602.08945)
*Sahajpreet Singh,Kokil Jaidka,Min-Yen Kan*

Main category: cs.CL

TL;DR: GitSearch提出通过识别信息缺口并精准检索解决社区审核冷启动难题，显著提升审核覆盖率和注释帮助性，优于当前技术和人工方法。


<details>
  <summary>Details</summary>
Motivation: 社区基础的内容审核虽然有较好的可扩展性，但面临结构性挑战，特别是AI方法在冷启动场景下表现不佳，需要新的方法来提升审核效率和质量。

Method: 提出了一个三阶段框架GitSearch：识别信息缺口、实时目标化网络检索以弥补缺失信息、合成符合平台规范的注释。引入了PolBench数据集辅助评估。

Result: GitSearch实现了99%的覆盖率，覆盖率几乎是现有方法的两倍，并以69%的胜率击败人类编写的注释，帮助性得分更高，证明了方法的有效性。

Conclusion: GitSearch通过识别信息缺口并进行针对性检索，有效提升了社区内容审核的覆盖率和帮助性，优于当前最先进的方法和人工编写的注释。

Abstract: Community-based moderation offers a scalable alternative to centralized fact-checking, yet it faces significant structural challenges, and existing AI-based methods fail in "cold start" scenarios. To tackle these challenges, we introduce GitSearch (Gap-Informed Targeted Search), a framework that treats human-perceived quality gaps, such as missing context, etc., as first-class signals. GitSearch has a three-stage pipeline: identifying information deficits, executing real-time targeted web-retrieval to resolve them, and synthesizing platform-compliant notes. To facilitate evaluation, we present PolBench, a benchmark of 78,698 U.S. political tweets with their associated Community Notes. We find GitSearch achieves 99% coverage, almost doubling coverage over the state-of-the-art. GitSearch surpasses human-authored helpful notes with a 69% win rate and superior helpfulness scores (3.87 vs. 3.36), demonstrating retrieval effectiveness that balanced the trade-off between scale and quality.

</details>


### [94] [How Should We Model the Probability of a Language?](https://arxiv.org/abs/2602.08951)
*Rasul Dent,Pedro Ortiz Suarez,Thibault Clérice,Benoît Sagot*

Main category: cs.CL

TL;DR: 传统语言识别系统忽视了上下文和先验概率，导致覆盖有限。将语言识别视为结合环境线索的路由问题是提升小众语言识别的关键。


<details>
  <summary>Details</summary>
Motivation: 当前商业语言识别系统对语言覆盖有限，且研究系统也仅在特定条件下拓展覆盖，根本原因在于对任务性质理解误区及机构激励机制的限制。

Method: 通过重新 framing 语言识别任务，将其视为结合环境上下文的路由问题，而非单纯文本分类，从而提出新的模型设计思路。

Result: 阐明了先验概率在语言识别中的关键作用，并指出结合环境信息可以实现更广泛、合理的语言识别覆盖，特别是对边缘语言。

Conclusion: 现有商业语言识别系统覆盖语言种类有限，主要问题在于将语言识别简化为去上下文化的文本分类，忽视了先验概率估计的重要性。提升边缘语言覆盖率需要将语言识别重新定义为路由问题，结合环境线索进行语言推断。

Abstract: Of the over 7,000 languages spoken in the world, commercial language identification (LID) systems only reliably identify a few hundred in written form. Research-grade systems extend this coverage under certain circumstances, but for most languages coverage remains patchy or nonexistent. This position paper argues that this situation is largely self-imposed. In particular, it arises from a persistent framing of LID as decontextualized text classification, which obscures the central role of prior probability estimation and is reinforced by institutional incentives that favor global, fixed-prior models. We argue that improving coverage for tail languages requires rethinking LID as a routing problem and developing principled ways to incorporate environmental cues that make languages locally plausible.

</details>


### [95] [Next Concept Prediction in Discrete Latent Space Leads to Stronger Language Models](https://arxiv.org/abs/2602.08984)
*Yuliang Liu,Yunchong Song,Yixuan Wang,Kewen Ge,Alex Lamb,Qipeng Guo,Kai Chen,Bowen Zhou,Zhouhan Lin*

Main category: cs.CL

TL;DR: 本文提出了基于概念预测的生成式预训练范式，实验证明该方法能显著提升大规模语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的单标记预测任务过于简单，限制了语言模型的潜力，需引入更难的预训练目标以提高模型能力。

Method: 提出基于向量量化的概念词汇表构建和结合Next Concept Prediction与Next Token Prediction的方法进行生成式预训练。

Result: 在13个基准测试中，NCP优于传统的单标记模型，并且在对已有模型进行持续预训练时也能提升性能。

Conclusion: NCP通过引入更具挑战性的多标记离散概念预测任务，提升了语言模型的性能。

Abstract: We propose Next Concept Prediction (NCP), a generative pretraining paradigm built on top of Next Token Prediction (NTP). NCP predicts discrete concepts that span multiple tokens, thereby forming a more challenging pretraining objective. Our model, ConceptLM, quantizes hidden states using Vector Quantization and constructs a concept vocabulary. It leverages both NCP and NTP to drive parameter updates and generates a concept to guide the generation of the following tokens. We train ConceptLM from scratch at scales ranging from 70M to 1.5B parameters with up to 300B training data, including Pythia and GPT-2 backbones. Results on 13 benchmarks show that NCP yields consistent performance gains over traditional token-level models. Furthermore, continual pretraining experiments on an 8B-parameter Llama model indicate that NCP can further improve an NTP-trained model. Our analysis suggests that NCP leads to more powerful language models by introducing a harder pretraining task, providing a promising path toward better language modeling.

</details>


### [96] [When Actions Go Off-Task: Detecting and Correcting Misaligned Actions in Computer-Use Agents](https://arxiv.org/abs/2602.08995)
*Yuting Ning,Jaylen Jones,Zhehao Zhang,Chentao Ye,Weitong Ruan,Junyi Li,Rahul Gupta,Huan Sun*

Main category: cs.CL

TL;DR: 本文针对CUAs中的动作不对齐问题，提出了检测与纠正方法DeAction，显著提升了系统安全性与任务表现。


<details>
  <summary>Details</summary>
Motivation: 当前CUAs频繁产生偏离用户意图的不对齐动作，存在外部攻击和内部推理错误风险，影响安全性和效率，亟需检测与纠正机制。

Method: 构建了涵盖真实场景下动作不对齐的MisActBench基准数据集，设计了DeAction方法，通过结构化反馈在执行前检测并迭代纠正不对齐动作。

Result: DeAction在MisActBench上F1分数提升超过15%，在线评测中在对抗环境下将攻击成功率降低90%以上，同时在正常环境下保持或提升任务成功率。

Conclusion: 本研究首次系统定义并研究了计算机使用代理（CUAs）中的动作不对齐检测问题，提出了一种实用且通用的防护方法DeAction，能够有效检测并纠正不对齐动作，显著提升了CUAs的安全性和任务效率。

Abstract: Computer-use agents (CUAs) have made tremendous progress in the past year, yet they still frequently produce misaligned actions that deviate from the user's original intent. Such misaligned actions may arise from external attacks (e.g., indirect prompt injection) or from internal limitations (e.g., erroneous reasoning). They not only expose CUAs to safety risks, but also degrade task efficiency and reliability. This work makes the first effort to define and study misaligned action detection in CUAs, with comprehensive coverage of both externally induced and internally arising misaligned actions. We further identify three common categories in real-world CUA deployment and construct MisActBench, a benchmark of realistic trajectories with human-annotated, action-level alignment labels. Moreover, we propose DeAction, a practical and universal guardrail that detects misaligned actions before execution and iteratively corrects them through structured feedback. DeAction outperforms all existing baselines across offline and online evaluations with moderate latency overhead: (1) On MisActBench, it outperforms baselines by over 15% absolute in F1 score; (2) In online evaluation, it reduces attack success rate by over 90% under adversarial settings while preserving or even improving task success rate in benign environments.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [97] [Artificial Intelligence in Open Source Software Engineering: A Foundation for Sustainability](https://arxiv.org/abs/2602.07071)
*S M Rakib UI Karim,Wenyi Lu,Sean Goggins*

Main category: cs.SE

TL;DR: 本文综述了AI技术如何助力开源软件可持续发展，强调AI应作为辅助工具，指出当前应用中的不足与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 开源软件作为数字基础设施关键组成部分，面临贡献者不足、资金安全及社区健康等多重可持续性挑战，促使探索AI技术的介入。

Method: 本文采用文献综述方法，综合跨学科研究，系统分析AI在开源软件可持续性中的应用及其挑战。

Result: 研究发现AI在自动缺陷分配、系统维护、贡献者辅导、社区健康分析及安全漏洞检测等方面展现潜力，同时揭示其在数据公平性、透明度与伦理风险上的局限。

Conclusion: AI技术在开源软件可持续性中具有重要作用，但仍面临数据偏见、透明性不足等挑战，需强化人机协作以实现更健康的开源生态。

Abstract: Open-source software (OSS) is foundational to modern digital infrastructure, yet this context for group work continues to struggle to ensure sufficient contributions in many critical cases. This literature review explores how artificial intelligence (AI) is being leveraged to address critical challenges to OSS sustainability, including maintaining contributor engagement, securing funding, ensuring code quality and security, fostering healthy community dynamics, and preventing project abandonment. Synthesizing recent interdisciplinary research, the paper identifies key applications of AI in this domain, including automated bug triaging, system maintenance, contributor onboarding and mentorship, community health analytics, vulnerability detection, and task automation. The review also examines the limitations and ethical concerns that arise from applying AI in OSS contexts, including data availability, bias and fairness, transparency, risks of misuse, and the preservation of human-centered values in collaborative development. By framing AI not as a replacement but as a tool to augment human infrastructure, this study highlights both the promise and pitfalls of AI-driven interventions. It concludes by identifying critical research gaps and proposing future directions at the intersection of AI, sustainability, and OSS, aiming to support more resilient and equitable open-source ecosystems.

</details>


### [98] [AgentSpawn: Adaptive Multi-Agent Collaboration Through Dynamic Spawning for Long-Horizon Code Generation](https://arxiv.org/abs/2602.07072)
*Igor Costa*

Main category: cs.SE

TL;DR: AgentSpawn提出了一种动态多智能体协作架构，通过自动记忆传递和自适应生成策略，提升了长时间代码生成的完成率和效率。


<details>
  <summary>Details</summary>
Motivation: 长时间跨度的代码生成需要持续的上下文保持和跨领域的自适应专业能力，而现有多智能体系统采用静态工作流程，无法在运行时分析发现意外复杂性时进行适应。

Method: 提出AgentSpawn架构，通过(1)自动记忆传递以支持智能体生成，(2)基于运行时复杂性指标触发的自适应生成策略，(3)并发修改的一致性协议，实现动态智能体协作。

Result: AgentSpawn在SWE-bench等基准测试中完成率提高了34%，同时通过选择性切片减少了42%的内存开销。

Conclusion: AgentSpawn有效解决了现有研究中记忆连续性、技能继承、任务恢复、运行时生成和并发一致性等五大关键问题，显著提升了长时间跨度代码生成的性能和效率。

Abstract: Long-horizon code generation requires sustained context and adaptive expertise across domains. Current multi-agent systems use static workflows that cannot adapt when runtime analysis reveals unanticipated complexity. We propose AgentSpawn, an architecture enabling dynamic agent collaboration through: (1) automatic memory transfer during spawning, (2) adaptive spawning policies triggered by runtime complexity metrics, and (3) coherence protocols for concurrent modifications. AgentSpawn addresses five critical gaps in existing research around memory continuity, skill inheritance, task resumption, runtime spawning, and concurrent coherence. Experimental validation demonstrates AgentSpawn achieves 34% higher completion rates than static baselines on benchmarks like SWE-bench while reducing memory overhead by 42% through selective slicing.

</details>


### [99] [Comprehensive Evaluation of Large Language Models on Software Engineering Tasks: A Multi-Task Benchmark](https://arxiv.org/abs/2602.07079)
*Go Frendi Gunawan,Mukhlis Amien*

Main category: cs.SE

TL;DR: 本研究对11个大型语言模型在五个软件工程任务上进行了多维度评测，揭示了效率和工具使用的显著差异，促进模型选用和优化。


<details>
  <summary>Details</summary>
Motivation: 当前软件工程领域缺乏涵盖多样任务的大型语言模型综合评测基准，因此需要全面评估这些模型在不同软件工程活动中的表现。

Method: 对11个先进大型语言模型进行多任务评估，涵盖缺陷修复、功能开发、代码重构、技术文案和研究合成五个任务，使用自动化验证框架评估输出质量和完成效率。

Result: 发现即使模型得分相同，完成时间、工具效率和成本差异巨大，工具调用次数与成功率无关，识别出循环效率低下和推理效率低下两种低效模式，编程任务成功率100%，研究任务成功率较低（90.9%）。

Conclusion: 大型语言模型在软件工程中表现优异，但在效率和工具使用方面存在显著差异，研究揭示了不同低效模式，并指出研究任务挑战较大。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in software engineering, yet comprehensive benchmarks covering diverse SE activities remain limited. We present a multi-task evaluation of 11 state-of-the-art LLMs across five representative software engineering tasks: bug fixing, feature development, code refactoring, technical copywriting, and research synthesis. Our automated verification framework measures both output quality and completion efficiency. Key findings reveal that (1) models achieving identical perfect scores exhibit 22x variation in completion time, 49x variation in tool efficiency, and 53x variation in estimated cost; (2) tool usage frequency shows no correlation with success (r = 0.077, p = 0.575) - one model used 917 tool calls while another solved the same task with 3 calls; (3) we identify two distinct inefficiency patterns: loop inefficiency and inference inefficiency; and (4) coding tasks achieve 100 percent success while research tasks present greater challenges (90.9 percent). We release all experimental data, verification scripts, and analysis code for full reproducibility.

</details>


### [100] [CodeCircuit: Toward Inferring LLM-Generated Code Correctness via Attribution Graphs](https://arxiv.org/abs/2602.07080)
*Yicheng He,Zheng Zhao,Zhou Kaiyu,Bryan Dai,Jie Fu,Yonghui Yang*

Main category: cs.SE

TL;DR: 本文提出了一种基于大型语言模型内部神经动态结构的代码正确性验证方法，通过解析模型内部的算法轨迹和结构特征，实现无需外部测试或评判器的代码功能验证，验证了该方法在多语言环境下的有效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的代码验证方法依赖于执行测试或辅助的LLM评判器，这些方法要么费力，要么受限于评判模型能力。本文提出了从LLM自身的计算结构出发，探索其内部是否存在可解码的功能正确性信号，从而实现内省式的代码验证。

Method: 本文将代码验证视为一种机械诊断任务，通过将模型的算法轨迹映射为代码级归因图，分解复杂的残差流，识别区分有效推理与逻辑错误的结构特征，进而利用这些内部图的拓扑特征预测代码的正确性，并进行因果干预修正错误逻辑。

Result: 研究发现，不同编程语言（Python、C++、Java）中模型内部的正确性信号具有鲁棒性，利用内部结构的拓扑特征比表面启发式更能准确预测代码正确性，并且可以通过有针对性的因果干预修正错误逻辑。

Conclusion: 本文证明了大型语言模型（LLM）在代码生成过程中，其内部神经动态包含可预测逻辑正确性的信号，这些信号可以通过模型内部结构进行解码，提供了一种无需依赖外部机制验证代码正确性的方法。

Abstract: Current paradigms for code verification rely heavily on external mechanisms-such as execution-based unit tests or auxiliary LLM judges-which are often labor-intensive or limited by the judging model's own capabilities. This raises a fundamental, yet unexplored question: Can an LLM's functional correctness be assessed purely from its internal computational structure? Our primary objective is to investigate whether the model's neural dynamics encode internally decodable signals that are predictive of logical validity during code generation. Inspired by mechanistic interpretability, we propose to treat code verification as a mechanistic diagnostic task, mapping the model's explicit algorithmic trajectory into line-level attribution graphs. By decomposing complex residual flows, we aim to identify the structural signatures that distinguish sound reasoning from logical failure within the model's internal circuits. Analysis across Python, C++, and Java confirms that intrinsic correctness signals are robust across diverse syntaxes. Topological features from these internal graphs predict correctness more reliably than surface heuristics and enable targeted causal interventions to fix erroneous logic. These findings establish internal introspection as a decodable property for verifying generated code. Our code is at https:// github.com/bruno686/CodeCircuit.

</details>


### [101] [Rethinking Scientific Modeling: Toward Physically Consistent and Simulation-Executable Programmatic Generation](https://arxiv.org/abs/2602.07083)
*Yongqing Jiang,Jianze Wang,Zhiqi Shen,Zhenghong Lin,Jiayuan Wang,Yijian Yang,Kaoshan Dai,Haoran Luo*

Main category: cs.SE

TL;DR: 本文提出了一种基于领域知识和验证驱动的物理一致性自动结构建模框架，成功提升了模型生成的准确性和执行性。


<details>
  <summary>Details</summary>
Motivation: 结构建模中物理不一致或规范违规会导致仿真无效，现有大语言模型生成代码虽有潜力，但仍频繁出现非执行或物理不一致输出，亟需一种能保证物理一致性的自动建模方法。

Method: 通过引入CivilInstruct结构工程领域特定数据集，采用两阶段微调策略强化约束满足和API合规性，并利用MBEval基于验证驱动的评测标准进行闭环验证。

Result: 实验结果显示，该方法在严格验证指标上均优于基线，生成的模型具备良好执行性和结构动态一致性。

Conclusion: 该论文提出的物理一致性自动建筑建模框架有效提升了结构建模的准确性和物理一致性，显著减少了非执行性和物理不一致输出。

Abstract: Structural modeling is a fundamental component of computational engineering science, in which even minor physical inconsistencies or specification violations may invalidate downstream simulations. The potential of large language models (LLMs) for automatic generation of modeling code has been demonstrated. However, non-executable or physically inconsistent outputs remain prevalent under stringent engineering constraints. A framework for physics-consistent automatic building modeling is therefore proposed, integrating domain knowledge construction, constraint-oriented model alignment, and verification-driven evaluation. CivilInstruct is introduced as a domain-specific dataset that formalizes structural engineering knowledge and constraint reasoning to enable simulation-ready model generation. A two-stage fine-tuning strategy is further employed to enforce constraint satisfaction and application programming interface compliance, substantially reducing hallucinated and non-conforming outputs. MBEval is presented as a verification-driven benchmark that evaluates executability and structural dynamics consistency through closed-loop validation. Experimental results show consistent improvements over baselines across rigorous verification metrics. Our code is available at https://github.com/Jovanqing/AutoBM.

</details>


### [102] [Evaluating Retrieval-Augmented Generation Variants for Natural Language-Based SQL and API Call Generation](https://arxiv.org/abs/2602.07086)
*Michael Marketsmüller,Simon Martin,Tim Schlippe*

Main category: cs.SE

TL;DR: 研究显示，检索增强生成对企业自然语言接口性能至关重要，CoRAG在复杂混合文档下表现最优，显著提升了SQL和API调用生成的准确率。


<details>
  <summary>Details</summary>
Motivation: 企业系统日益需要自然语言接口将用户请求转换为结构化操作，但大语言模型在特定领域联合处理检索和修改任务的效果尚不明确。

Method: 评估了三种RAG变体（标准RAG、Self-RAG 和 CoRAG）在SQL查询生成、REST API调用生成及动态任务分类的联合任务上，采用SAP事务性银行业务作为实验用例，构建了覆盖两种模式的新测试数据集，共测试了18种配置，在数据库、API及混合文档环境中进行比较。

Result: 在无检索支持时，精确匹配准确率为0%，检索增强显著提升执行准确率（最高79.30%）和组件匹配准确率（最高78.86%）；CoRAG在混合文档环境中表现最佳，联合任务精确匹配提升至10.29%，显著优于标准RAG的7.45%。

Conclusion: 检索增强生成（RAG）是实现准确自然语言接口的关键，尤其在处理混合文档环境时，CoRAG表现最优，显著提升了SQL查询和REST API调用的生成准确性。

Abstract: Enterprise systems increasingly require natural language interfaces that can translate user requests into structured operations such as SQL queries and REST API calls. While large language models (LLMs) show promise for code generation [Chen et al., 2021; Huynh and Lin, 2025], their effectiveness in domain-specific enterprise contexts remains underexplored, particularly when both retrieval and modification tasks must be handled jointly. This paper presents a comprehensive evaluation of three retrieval-augmented generation (RAG) variants [Lewis et al., 2021] -- standard RAG, Self-RAG [Asai et al., 2024], and CoRAG [Wang et al., 2025] -- across SQL query generation, REST API call generation, and a combined task requiring dynamic task classification. Using SAP Transactional Banking as a realistic enterprise use case, we construct a novel test dataset covering both modalities and evaluate 18 experimental configurations under database-only, API-only, and hybrid documentation contexts. Results demonstrate that RAG is essential: Without retrieval, exact match accuracy is 0% across all tasks, whereas retrieval yields substantial gains in execution accuracy (up to 79.30%) and component match accuracy (up to 78.86%). Critically, CoRAG proves most robust in hybrid documentation settings, achieving statistically significant improvements in the combined task (10.29% exact match vs. 7.45% for standard RAG), driven primarily by superior SQL generation performance (15.32% vs. 11.56%). Our findings establish retrieval-policy design as a key determinant of production-grade natural language interfaces, showing that iterative query decomposition outperforms both top-k retrieval and binary relevance filtering under documentation heterogeneity.

</details>


### [103] [Architectural Anti-Patterns in Student-Developed Microservice Architectures: An Exploratory Study](https://arxiv.org/abs/2602.07147)
*Marco De Luca,Michele Perlotto,Anna Rita Fasolino,Porfirio Tramontana*

Main category: cs.SE

TL;DR: 研究通过分析学生的微服务项目揭示了常见反模式，尤其是安全和协作问题，并提出了改进教学的具体建议，促进学业与工业实践的结合。


<details>
  <summary>Details</summary>
Motivation: 微服务架构教学面临分布式系统复杂性及学术界与工业界差距，理解学生引入的质量问题有助于改进教学。

Method: 通过一个为期多年的项目驱动课程，分析216名硕士生设计和部署的真实容器化微服务系统，利用已知反模式分类法识别问题。

Result: 学生设计的系统中发现23种已知反模式，主要集中在安全、团队组织和服务交互，少数涉及服务内部设计和分解。

Conclusion: 学生在微服务架构设计中普遍存在安全性、团队协作和服务交互方面的问题，强调功能交付优先于系统鲁棒性和运维纪律性。

Abstract: Teaching microservice architectures is challenging due to distributed complexity and the gap between academia and industry. Understanding the quality issues students introduce in MSAs is essential to improve education. This study analyzes student-developed microservices using an established anti-pattern taxonomy and derives lessons learned with actionable teaching recommendations. We conducted a longitudinal, project-based course (2023-2025) involving 216 Master's students (67 teams) who designed and deployed a realistic, containerized MSA for a gamified testing platform. The final systems revealed 23 out of 58 known MSA anti-patterns, spanning five categories. Security issues were most frequent, highlighting weaknesses in authentication, authorization, and data protection. Team Organization and Service Interaction problems followed, reflecting limited DevOps experience and difficulties in inter-service coordination. Fewer issues appeared in Intra-service Design and Inter-service Decomposition, suggesting students generally defined service boundaries well. Overall, students prioritized feature delivery over robustness and operational discipline. To address this, we recommend enforcing minimal standards (API contracts, gateways), providing labs on resilient communication, integrating security-by-design practices, and offering CI-CD templates. The paper contributes a realistic, full-scale educational experience and a replicable model for teaching industry-aligned microservice architecture.

</details>


### [104] [Measuring Complexity at the Requirements Stage: Spectral Metrics as Development Effort Predictors](https://arxiv.org/abs/2602.07182)
*Maximilian Vierlboeck,Antonio Pugliese,Roshanak Nilchian,Paul Grogan,Rashika Sugganahalli Natesh Babu*

Main category: cs.SE

TL;DR: 本文通过自然语言处理提取需求结构网络，利用分子图代理实验，发现谱测度显著优于传统结构指标，能够准确预测需求集成工作量，为需求工程中的复杂度分析提供了新的方法论基础。


<details>
  <summary>Details</summary>
Motivation: 需求规范中的结构复杂度尚未被充分理解和量化，然而需求复杂度直接影响系统设计和后续集成工作，研究该问题有助于提升需求工程实践的有效性。

Method: 利用自然语言处理方法提取需求文本中的结构网络，并通过分子图作为与需求结构同构的代理，设计对比实验验证多种结构复杂度指标对集成工作量的预测能力。

Result: 谱测度与集成工作量的相关性超过0.95，结构度量相关性超过0.89，密度指标无预测效度，表明谱测度更能有效刻画认知和工作量维度。

Conclusion: 特征谱测度可以准确预测需求集成工作量，优于传统的结构度量，表明需求中的结构复杂度显著影响集成难度。

Abstract: Complexity in engineered systems presents one of the most persistent challenges in modern development since it is driving cost overruns, schedule delays, and outright project failures. Yet while architectural complexity has been studied, the structural complexity embedded within requirements specifications remains poorly understood and inadequately quantified. This gap is consequential: requirements fundamentally drive system design, and complexity introduced at this stage propagates through architecture, implementation, and integration. To address this gap, we build on Natural Language Processing methods that extract structural networks from textual requirements. Using these extracted structures, we conducted a controlled experiment employing molecular integration tasks as structurally isomorphic proxies for requirements integration - leveraging the topological equivalence between molecular graphs and requirement networks while eliminating confounding factors such as domain expertise and semantic ambiguity. Our results demonstrate that spectral measures predict integration effort with correlations exceeding 0.95, while structural metrics achieve correlations above 0.89. Notably, density-based metrics show no significant predictive validity. These findings indicate that eigenvalue-derived measures capture cognitive and effort dimensions that simpler connectivity metrics cannot. As a result, this research bridges a critical methodological gap between architectural complexity analysis and requirements engineering practice, providing a validated foundation for applying these metrics to requirements engineering, where similar structural complexity patterns may predict integration effort.

</details>


### [105] [Automated Modernization of Machine Learning Engineering Notebooks for Reproducibility](https://arxiv.org/abs/2602.07195)
*Bihui Jin,Kaiyuan Wang,Pengyu Nie*

Main category: cs.SE

TL;DR: 针对机器学习笔记本因环境更新导致不可复现的问题，提出MLEModernizer框架，利用大语言模型自动修复与优化，显著提升笔记本复现率。


<details>
  <summary>Details</summary>
Motivation: 机器学习工程中的交互式计算笔记本因硬件软件环境快速变化导致难以复现，阻碍了代码复用和科学进步。

Method: 设计并实现了基于大语言模型驱动的MLEModernizer框架，通过迭代执行笔记本、收集反馈并进行错误修复、运行时优化和评分校准，提升笔记本的复现率。

Result: 在7,402个初始不可复现的笔记本中，MLEModernizer使得74.2%（5,492个）笔记本恢复了复现能力。

Conclusion: MLEModernizer有效缓解了环境侵蚀问题，帮助从业者维护和复用机器学习笔记本，实现了跨环境的代码复现。

Abstract: Interactive computational notebooks (e.g., Jupyter notebooks) are widely used in machine learning engineering (MLE) to program and share end-to-end pipelines, from data preparation to model training and evaluation. However, environment erosion-the rapid evolution of hardware and software ecosystems for machine learning-has rendered many published MLE notebooks non-reproducible in contemporary environments, hindering code reuse and scientific progress. To quantify this gap, we study 12,720 notebooks mined from 79 popular Kaggle competitions: only 35.4% remain reproducible today. Crucially, we find that environment backporting, i.e., downgrading dependencies to match the submission time, does not improve reproducibility but rather introduces additional failure modes.
  To address environment erosion, we design and implement MLEModernizer, an LLM-driven agentic framework that treats the contemporary environment as a fixed constraint and modernizes notebook code to restore reproducibility. MLEModernizer iteratively executes notebooks, collects execution feedback, and applies targeted fixes in three types: error-repair, runtime-reduction, and score-calibration. Evaluated on 7,402 notebooks that are non-reproducible under the baseline environment, MLEModernizer makes 5,492 (74.2%) reproducible. MLEModernizer enables practitioners to validate, reuse, and maintain MLE artifacts as the hardware and software ecosystems continue to evolve.

</details>


### [106] [Forecasting Developer Environments with GenAI: A Research Perspective](https://arxiv.org/abs/2602.07412)
*Raula Gaikovina Kula,Christoph Treude,Xing Hu,Sebastian Baltes,Earl T. Barr,Kelly Blincoe,Fabio Calefato,Junjie Chen,Marc Cheong,Youmei Fan,Daniel M. German,Marco Gerosa,Jin L. C. Guo,Shinpei Hayashi,Robert Hirschfeld,Reid Holmes,Yintong Huo,Takashi Kobayashi,Michele Lanza,Zhongxin Liu,Olivier Nourry,Nicole Novielli,Denys Poshyvanyk,Shinobu Saito,Kazumasa Shimari,Igor Steinmacher,Mairieli Wessel,Markus Wagner,Annie Vella,Laurie Williams,Xin Xia*

Main category: cs.SE

TL;DR: 本文通过专家讨论，识别了生成式人工智能对IDE影响的四个关键研究领域。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式人工智能（GenAI）模型对集成开发环境（IDE）中人机交互的影响。

Method: 组织33位软件工程、人工智能与人机交互领域的专家，参加为期四天的Shonan Meeting 222会议，讨论GenAI带来的挑战和机遇。

Result: 总结出四个主要主题，作为研究人员和实践者关注的焦点。

Conclusion: 生成式人工智能在代码生成、测试、审查和修复方面表现卓越，未来有望提升编程抽象层次，改变IDE中的人机交互。

Abstract: Generative Artificial Intelligence (GenAI) models are achieving remarkable performance in various tasks, including code generation, testing, code review, and program repair. The ability to increase the level of abstraction away from writing code has the potential to change the Human-AI interaction within the integrated development environment (IDE). To explore the impact of GenAI on IDEs, 33 experts from the Software Engineering, Artificial Intelligence, and Human-Computer Interaction domains gathered to discuss challenges and opportunities at Shonan Meeting 222, a four-day intensive research meeting. Four themes emerged as areas of interest for researchers and practitioners.

</details>


### [107] [Pull Requests as a Training Signal for Repo-Level Code Editing](https://arxiv.org/abs/2602.07457)
*Qinglin Zhu,Tianyu Chen,Shuai Lu,Lei Ji,Runcong Zhao,Murong Ma,Xiangxiang Dai,Yulan He,Lin Gui,Peng cheng,Yeyun Gong*

Main category: cs.SE

TL;DR: 本文提出Clean-PR训练范式，通过大规模真实GitHub拉取请求数据，实现了无需复杂代理框架的高效库级代码编辑，提升了模型在SWE-bench评测中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有库级代码编辑方法依赖复杂的代理结构，难以评估通过高质量训练信号模型能内化多少代码理解和编辑能力。

Method: 提出Clean Pull Request（Clean-PR）中训练范式，利用真实的GitHub拉取请求作为训练信号，通过重建和验证将噪声拉取请求差异转换为搜索/替换编辑块，构建包含200万个拉取请求的多语言数据集，并进行中期训练和基于错误驱动的数据增强的监督微调。

Result: 模型在SWE-bench上显著优于指令微调基线，在SWE-bench Lite和SWE-bench Verified上分别提升了13.6%和12.3%。

Conclusion: 库级代码编辑能力可以通过高质量的训练信号内化到模型权重中，无需复杂的推理时代理框架即可实现高效的代码理解和多文件修改。

Abstract: Repository-level code editing requires models to understand complex dependencies and execute precise multi-file modifications across a large codebase. While recent gains on SWE-bench rely heavily on complex agent scaffolding, it remains unclear how much of this capability can be internalised via high-quality training signals. To address this, we propose Clean Pull Request (Clean-PR), a mid-training paradigm that leverages real-world GitHub pull requests as a training signal for repository-level editing. We introduce a scalable pipeline that converts noisy pull request diffs into Search/Replace edit blocks through reconstruction and validation, resulting in the largest publicly available corpus of 2 million pull requests spanning 12 programming languages. Using this training signal, we perform a mid-training stage followed by an agentless-aligned supervised fine-tuning process with error-driven data augmentation. On SWE-bench, our model significantly outperforms the instruction-tuned baseline, achieving absolute improvements of 13.6% on SWE-bench Lite and 12.3% on SWE-bench Verified. These results demonstrate that repository-level code understanding and editing capabilities can be effectively internalised into model weights under a simplified, agentless protocol, without relying on heavy inference-time scaffolding.

</details>


### [108] [ComPass: Contrastive Learning for Automated Patch Correctness Assessment in Program Repair](https://arxiv.org/abs/2602.07561)
*Quanjun Zhang,Ye Shang,Haichuan Hu,Chunrong Fang,Zhenyu Chen,Liang Xiao*

Main category: cs.SE

TL;DR: 本论文通过对比学习和数据增强优化PLM训练，有效解决自动程序修复中补丁过拟合问题，实现了高准确率的补丁正确性评估。


<details>
  <summary>Details</summary>
Motivation: 自动程序修复中补丁过拟合问题严重，且大规模带标签的补丁数据难以获得，当前使用PLM进行补丁正确性评估存在训练范式和数据集的限制。

Method: 利用代码转换规则生成语义保持但结构不同的代码片段进行对比学习预训练，再结合嵌入表示和二分类器的联合微调，实现对补丁代码正确性的评估。

Result: 在Defects4J上的实验证明，ComPass以88.35%的准确率显著优于现有最先进的APPT基线。

Conclusion: 本论文提出了ComPass，一种基于预训练语言模型（PLM）的自动补丁正确性评估（APCA）方法，通过对比学习和数据增强显著提升了补丁正确性判断的准确率。

Abstract: Automated program repair (APR) attempts to reduce manual debugging efforts and plays a vital role in software maintenance. Despite remarkable progress, APR is still limited in generating overfitting patches, i.e., patches passing available test suites but incorrect. This issue, known as patch overfitting, has become a key concern in the APR community, with numerous approaches proposed to address it. Very recent work proposes a pre-trained language model (PLM)-based automated patch correctness assessment (APCA) approach, indicating the potential of such PLMs in reasoning about patch correctness. Despite being promising, it is still far from perfect due to various limitations, such as the training paradigm and training dataset. In this paper, we present ComPass, a PLM-based APCA approach that leverages contrastive learning and data augmentation to address the technical limitations of prior work. Our work is inspired by the opportunity to integrate contrastive learning with recent PLMs in the field of patch correctness assessment, where large-scale labeled patches are difficult to obtain. ComPass utilizes code transformation rules to generate semantic-preserving code snippets for both unlabeled pre-training corpus and labeled fine-tuning patches. ComPass then pre-trains PLMs with contrastive learning, which captures code features with the same semantics but different structures. ComPass finally integrates representation embeddings of patch code snippets and fine-tunes PLMs with a binary classifier jointly to assess patch code correctness. Experimental results on 2274 real-world patches from Defects4J demonstrate that ComPass achieves an accuracy of 88.35%, significantly outperforming state-of-the-art baseline APPT.

</details>


### [109] [Clarifying Core Dimensions in Digital Maturity Models: An Integrative Approach](https://arxiv.org/abs/2602.07569)
*Eduardo C. Peixoto,Hector Oliveira,Geber L. Ramalho,Cesar França*

Main category: cs.SE

TL;DR: 本研究通过系统映射分析76个数字成熟度模型，整合并明确了其十个核心维度，提升了数字转型相关模型的理论清晰度。


<details>
  <summary>Details</summary>
Motivation: 鉴于数字转型项目高失败率和现有数字成熟度模型中维度定义不清晰、存在差异，研究旨在提供更一致且系统的维度定义。

Method: 采用系统映射方法，包括自动检索和滚雪球技术，分析了76个数字成熟度模型。

Result: 确定了组织、战略、技术、文化、流程、运营、人员、管理、客户和数据这十个最常见维度，并提出了整合性定义。

Conclusion: 本文整合并澄清了数字成熟度模型中最常见的十个维度的定义，为数字转型提供了更明确的理论基础。

Abstract: Digital Transformation (DT) initiatives frequently face high failure rates, and while Digital Maturity Models (DMMs) offer potential solutions, they have notable shortcomings. Specifically, there is significant disparity in the dimensions considered relevant, a lack of clarity in their definitions, and uncertainty regarding their components. This study aims to provide a clearer understanding of DMMs by proposing integrative definitions of the most frequently used dimensions. Using a Systematic Mapping approach, including automatic search and snowballing techniques, we analyzed 76 DMMs to answer two Research Questions: (RQ1) What are the most frequent dimensions in DMMs? and (RQ2) How are these dimensions described, including their components? We reconcile varying interpretations of the ten most frequent dimensions -- Organization, Strategy, Technology, Culture, Process, Operations, People, Management, Customer, and Data -- and propose integrative definitions for each. Compared to previous analyses, this study provides a broader and more recent perspective on Digital Maturity Models.

</details>


### [110] [A Course on the Introduction to Quantum Software Engineering: Experience Report](https://arxiv.org/abs/2602.07589)
*Andriy Miranskyy*

Main category: cs.SE

TL;DR: 本文报告了一门以软件工程视角设计的量子计算课程，帮助学生跨学科掌握量子软件工程知识，提供了课程设计和评估的实用经验。


<details>
  <summary>Details</summary>
Motivation: 传统量子计算教育多聚焦算法和框架使用，忽视软件工程如测试、抽象和生命周期管理等方面的培养。

Method: 设计并首次开设了一门结合量子计算与软件工程视角的跨本科生和研究生课程，通过教师观察、学生反馈、调查和学生作品分析进行评估。

Result: 开发出一套模块化课程设计、适用于混合学术水平的可拓展评估模型，并总结了适用于软件工程教育者的量子计算课程开发经验。

Conclusion: 学生在建立了量子信息和量子算法的基础理解后，能够有效参与量子软件工程相关主题的学习。

Abstract: Quantum computing is increasingly practiced through programming, yet most educational offerings emphasize algorithmic or framework-level use rather than software engineering concerns such as testing, abstraction, tooling, and lifecycle management.
  This paper reports on the design and first offering of a cross-listed undergraduate--graduate course that frames quantum computing through a software engineering lens, focusing on early-stage competence relevant to software engineering practice. The course integrates foundational quantum concepts with software engineering perspectives, emphasizing executable artifacts, empirical reasoning, and trade-offs arising from probabilistic behaviour, noise, and evolving toolchains. Evidence is drawn from instructor observations, student feedback, surveys, and analysis of student work.
  Despite minimal prior exposure to quantum computing, students were able to engage productively with quantum software engineering topics once a foundational understanding of quantum information and quantum algorithms, expressed through executable artifacts, was established. This experience report contributes a modular course design, a scalable assessment model for mixed academic levels, and transferable lessons for software engineering educators developing quantum computing curricula.

</details>


### [111] [Evaluating Large Language Models for Detecting Architectural Decision Violations](https://arxiv.org/abs/2602.07609)
*Ruoyu Su,Alexander Bakhtin,Noman Ahmad,Matteo Esposito,Valentina Lenarduzzi,Davide Taibi*

Main category: cs.SE

TL;DR: 本文研究了如何利用大型语言模型自动检测开源项目中的软件架构决策违规，发现其对显性代码相关决策有效，但对隐性或部署相关决策准确性不足，表明LLM在辅助架构决策合规验证上有潜力，但尚无法完全替代专家判断。


<details>
  <summary>Details</summary>
Motivation: 目前软件架构决策违规往往被忽视，原因在于缺少系统性的文档记录和自动化检测机制，而大型语言模型的发展为自动化建筑决策推理提供了新的可能性。

Method: 通过一个多模型流程，使用一个 LLM 进行初步筛查潜在的决策违规，再由三个独立的 LLM 验证推理，分析了来自109个GitHub项目的980个ADR，评估一致性、准确率、精确率和召回率，并结合专家评价进行综合分析。

Result: 模型对于显性和基于代码的决策违规检测表现出较高的一致性和准确率，但对隐性和依赖部署配置或组织知识的决策检测效果较差，显示出当前技术的优势和局限。

Conclusion: LLMs 在识别软件架构决策违规方面表现出较高的准确率和一致性，特别是对于显性和可从代码推断的决策，但对隐性或依赖部署及组织知识的决策准确率较低，尚不能完全替代人类专家。

Abstract: Architectural Decision Records (ADRs) play a central role in maintaining software architecture quality, yet many decision violations go unnoticed because projects lack both systematic documentation and automated detection mechanisms. Recent advances in Large Language Models (LLMs) open up new possibilities for automating architectural reasoning at scale. We investigated how effectively LLMs can identify decision violations in open-source systems by examining their agreement, accuracy, and inherent limitations. Our study analyzed 980 ADRs across 109 GitHub repositories using a multi-model pipeline in which one LLM primary screens potential decision violations, and three additional LLMs independently validate the reasoning. We assessed agreement, accuracy, precision, and recall, and complemented the quantitative findings with expert evaluation. The models achieved substantial agreement and strong accuracy for explicit, code-inferable decisions. Accuracy falls short for implicit or deployment-oriented decisions that depend on deployment configuration or organizational knowledge. Therefore, LLMs can meaningfully support validation of architectural decision compliance; however, they are not yet replacing human expertise for decisions not focused on code.

</details>


### [112] [HAIF: A Human-AI Integration Framework for Hybrid Team Operations](https://arxiv.org/abs/2602.07641)
*Marc Bara*

Main category: cs.SE

TL;DR: 本文提出HAIF框架，系统整合人类与AI代理混合团队的协作，解决了现有框架无法覆盖的运营缺口，支持敏捷工作流，未来将进行实证验证。


<details>
  <summary>Details</summary>
Motivation: 现有的敏捷、DevOps、MLOps和AI治理框架未能有效融合人类与AI代理作为混合团队协同工作的需求，形成操作上的缺口。

Method: 采用设计科学研究方法开发HAIF框架，结合协议、决策模型和反馈机制构建四大核心原则，实现分层自主权和委托决策。

Result: 提出了HAIF框架，包括结构性验证核查表、非软件环境适应指导以及对持续人机协作模式的探讨，强调框架对工具的独立性和渐进式采用可能性，后续计划开展实证验证。

Conclusion: HAIF框架解决了当前没有将人类与AI代理结合为一个统一交付单元的团队运营缺口，尽管面临AI能力提升带来的监督挑战，但提供了一个灵活可扩展的解决方案，支持现有敏捷工作流整合。

Abstract: The rapid deployment of generative AI, copilots, and agentic systems in knowledge work has created an operational gap: no existing framework addresses how to organize daily work in teams where AI agents perform substantive, delegated tasks alongside humans. Agile, DevOps, MLOps, and AI governance frameworks each cover adjacent concerns but none models the hybrid team as a coherent delivery unit. This paper proposes the Human-AI Integration Framework (HAIF): a protocol-based, scalable operational system built around four core principles, a formal delegation decision model, tiered autonomy with quantifiable transition criteria, and feedback mechanisms designed to integrate into existing Agile and Kanban workflows without requiring additional roles for small teams. The framework is developed following a Design Science Research methodology. HAIF explicitly addresses the central adoption paradox: the more capable AI becomes, the harder it is to justify the oversight the framework demands-and yet the greater the consequences of not providing it. The paper includes domain-specific validation checklists, adaptation guidance for non-software environments, and an examination of the framework's structural limitations-including the increasingly common pattern of continuous human-AI co-production that challenges the discrete delegation model. The framework is tool-agnostic and designed for iterative adoption. Empirical validation is identified as future work.

</details>


### [113] [Debugging code world models](https://arxiv.org/abs/2602.07672)
*Babak Rahmani*

Main category: cs.SE

TL;DR: 本文分析了代码世界模型的错误源，发现主要问题是程序执行中令牌预算耗尽和字符串状态处理不足，长时间状态跟踪的失败主要因动作错误生成，提出了改进方向。


<details>
  <summary>Details</summary>
Motivation: CWMs作为模拟程序执行的语言模型在内部验证和程序理解方面具有潜力，但其错误来源和限制未被充分理解，因此需要深入研究其失败模式和局限性。

Method: 通过对真实代码基准的分析，研究CWMs在局部语义执行和长时间状态跟踪上的表现，并设计控制的排列跟踪基准以分离状态传播与动作执行，使用替换成真实命令的动作来验证长时间状态传播能力。

Result: 发现CWMs失败集中在令牌耗尽和字符串状态错误两个方面，长时间状态衰减主要因动作生成错误，但在动作准确时，Transformer结构仍能实现准确长序列状态传播。

Conclusion: CWMs的主要失败原因是程序执行过程中因密集的运行时状态导致的令牌预算耗尽和字符串状态的错误，且长时间执行的状态跟踪错误主要源于动作生成不准确。优化动作生成和状态表示可以提升CWMs性能。

Abstract: Code World Models (CWMs) are language models trained to simulate program execution by predicting explicit runtime state after every executed command. This execution-based world modeling enables internal verification within the model, offering an alternative to natural language chain-of-thought reasoning. However, the sources of errors and the nature of CWMs' limitations remain poorly understood. We study CWMs from two complementary perspectives: local semantic execution and long-horizon state tracking. On real-code benchmarks, we identify two dominant failure regimes. First, dense runtime state reveals produce token-intensive execution traces, leading to token-budget exhaustion on programs with long execution histories. Second, failures disproportionately concentrate in string-valued state, which we attribute to limitations of subword tokenization rather than program structure. To study long-horizon behavior, we use a controlled permutation-tracking benchmark that isolates state propagation under action execution. We show that long-horizon degradation is driven primarily by incorrect action generation: when actions are replaced with ground-truth commands, a Transformer-based CWM propagates state accurately over long horizons, despite known limitations of Transformers in long-horizon state tracking. These findings suggest directions for more efficient supervision and state representations in CWMs that are better aligned with program execution and data types.

</details>


### [114] [On Sequence-to-Sequence Models for Automated Log Parsing](https://arxiv.org/abs/2602.07698)
*Adam Sorrenti,Andriy Miranskyy*

Main category: cs.SE

TL;DR: 本研究系统比较了四种序列模型在自动日志解析中的表现，发现Transformer表现最佳，Mamba在计算资源有限时是良好选择，字符级分词有助性能提升，序列长度影响较小，结果为相关研究和实际应用提供了指导。


<details>
  <summary>Details</summary>
Motivation: 自动日志解析对于软件系统的监控、异常检测和故障诊断至关重要，但因日志格式多样、训练与部署数据分布差异大以及基于规则方法的脆弱性，自动化解析仍具有挑战性。本研究旨在系统评估各种序列模型架构、表示选择、序列长度和训练数据量对自动日志解析性能及计算成本的影响。

Method: 通过控制变量的实证研究，比较Transformer、Mamba状态空间、单向LSTM和双向LSTM四种序列建模架构，共训练396个模型，采用多个数据集配置，用相对Levenshtein编辑距离和统计显著性检验评估模型性能。

Result: Transformer模型取得最低的平均相对编辑距离（0.111），Mamba为0.145，单向LSTM为0.186，双向LSTM为0.265。Mamba在计算成本上显著优于其他模型。字符级分词普遍提升性能，Transformer对序列长度不敏感，Mamba和Transformer在样本效率上优于循环网络。

Conclusion: Transformer模型在自动日志解析中表现最佳，提供最低的相对编辑距离，同时Mamba模型在计算成本较低的情况下也能达到竞争性的准确率。字符级分词提升性能，序列长度对Transformer准确率影响不大，且Mamba和Transformer的样本效率优于循环模型。总体而言，Transformer可将解析误差降低23.4%，而Mamba在数据或计算资源有限时是强有力的替代方案。

Abstract: Log parsing is a critical standard operating procedure in software systems, enabling monitoring, anomaly detection, and failure diagnosis. However, automated log parsing remains challenging due to heterogeneous log formats, distribution shifts between training and deployment data, and the brittleness of rule-based approaches. This study aims to systematically evaluate how sequence modelling architecture, representation choice, sequence length, and training data availability influence automated log parsing performance and computational cost. We conduct a controlled empirical study comparing four sequence modelling architectures: Transformer, Mamba state-space, monodirectional LSTM, and bidirectional LSTM models. In total, 396 models are trained across multiple dataset configurations and evaluated using relative Levenshtein edit distance with statistical significance testing. Transformer achieves the lowest mean relative edit distance (0.111), followed by Mamba (0.145), mono-LSTM (0.186), and bi-LSTM (0.265), where lower values are better. Mamba provides competitive accuracy with substantially lower computational cost. Character-level tokenization generally improves performance, sequence length has negligible practical impact on Transformer accuracy, and both Mamba and Transformer demonstrate stronger sample efficiency than recurrent models. Overall, Transformers reduce parsing error by 23.4%, while Mamba is a strong alternative under data or compute constraints. These results also clarify the roles of representation choice, sequence length, and sample efficiency, providing practical guidance for researchers and practitioners.

</details>


### [115] [Still Manual? Automated Linter Configuration via DSL-Based LLM Compilation of Coding Standards](https://arxiv.org/abs/2602.07783)
*Zejun Zhang,Yixin Gan,Zhenchang Xing,Tian Zhang,Yi Li,Xiwei Xu,Qinghua Lu,Liming Zhu*

Main category: cs.SE

TL;DR: 本文提出一种基于DSL和大语言模型的自动化linter配置方法，有效减少了手动配置难度和工作量，提高了配置准确性和效率，且适用于多种编程语言和规范。


<details>
  <summary>Details</summary>
Motivation: 手动配置linter复杂且需要专业知识，编程语言和代码规范多样且不断变化，导致配置工作重复且维护成本高，亟需自动化方法降低人工负担。

Method: 设计了用于表达代码规则的领域特定语言(DSL)，将自然语言描述的代码规范编译为DSL规范，再根据DSL规则结合配置指令自动生成具体的linter配置文件。

Result: 在Java Checkstyle验证中，DSL表示的精度和召回率均超过90%，细粒度配置生成的准确率、精度、召回率和F1分数均接近或超过70%，且精度超过基线100%以上。用户研究表明该方法提高了开发者配置效率，同时成功生成了JavaScript ESLint配置，验证了方法的通用性。

Conclusion: LintCFG通过领域特定语言和大语言模型的结合，实现了自动生成代码规范检查工具配置的目标，不依赖于具体编程语言、规范和工具，提升了配置的准确性和开发者效率。

Abstract: Coding standards are essential for maintaining consistent and high-quality code across teams and projects. Linters help developers enforce these standards by detecting code violations. However, manual linter configuration is complex and expertise-intensive, and the diversity and evolution of programming languages, coding standards, and linters lead to repetitive and maintenance-intensive configuration work. To reduce manual effort, we propose LintCFG, a domain-specific language (DSL)-driven, LLM-based compilation approach to automate linter configuration generation for coding standards, independent of programming languages, coding standards, and linters. Inspired by compiler design, we first design a DSL to express coding rules in a tool-agnostic, structured, readable, and precise manner. Then, we build linter configurations into DSL configuration instructions. For a given natural language coding standard, the compilation process parses it into DSL coding standards, matches them with the DSL configuration instructions to set configuration names, option names and values, verifies consistency between the standards and configurations, and finally generates linter-specific configurations. Experiments with Checkstyle for Java coding standard show that our approach achieves over 90% precision and recall in DSL representation, with accuracy, precision, recall, and F1-scores close to 70% (with some exceeding 70%) in fine-grained linter configuration generation. Notably, our approach outperforms baselines by over 100% in precision. A user study further shows that our approach improves developers' efficiency in configuring linters for coding standards. Finally, we demonstrate the generality of the approach by generating ESLint configurations for JavaScript coding standards, showcasing its broad applicability across other programming languages, coding standards, and linters.

</details>


### [116] [Software Space Analytics: Towards Visualization and Statistics of Internal Software Execution](https://arxiv.org/abs/2602.07821)
*Shinobu Saito*

Main category: cs.SE

TL;DR: 本文将空间统计方法引入软件执行数据分析，通过构建基于模块调用关系的软件空间，实现了执行状态的聚类可视化和统计检验，推动软件维护效率提升。


<details>
  <summary>Details</summary>
Motivation: 传统的软件维护依赖用户请求和缺陷报告，然而评估模块执行状态同样重要，因此引入空间统计方法分析内部执行数据。

Method: 将软件内部结构视为空间数据集，以模块调用关系构建软件空间，利用空间统计方法进行聚类可视化和统计检验。

Result: 通过空间统计方法实现了软件模块空间聚类的可视化及统计测试，展示了其在软件工程中的应用潜力。

Conclusion: 空间统计方法能够有效地应用于软件执行数据的分析，辅助软件维护工作中识别需要修改或删除的模块。

Abstract: In software maintenance work, software architects and programmers need to identify modules that require modification or deletion. Whilst user requests and bug reports are utilised for this purpose, evaluating the execution status of modules within the software is also crucial. This paper, therefore, applies spatial statistics to assess internal software execution data. First, we define a software space dataset, viewing the software's internal structure as a space based on module call relationships. Then, using spatial statistics, we conduct the visualization of spatial clusters and the statistical testing using spatial measures. Finally, we consider the usefulness of spatial statistics in the software engineering domain and future challenges.

</details>


### [117] [HerAgent: Rethinking the Automated Environment Deployment via Hierarchical Test Pyramid](https://arxiv.org/abs/2602.07871)
*Xiang Li,Siyu Lu,Sarro Federica,Claire Le Goues,He Ye*

Main category: cs.SE

TL;DR: 本文提出了基于执行验证的自动软件环境设置方法HerAgent，显著提升了环境配置的成功率，特别是在复杂项目中表现优异。


<details>
  <summary>Details</summary>
Motivation: 自动化软件环境设置对于测试、调试和复现故障至关重要，但由于依赖复杂、构建系统多样以及文档不完整等问题，实际应用中仍然充满挑战。

Method: 提出了环境成熟度层次结构，通过分级的执行要求评估环境设置的成功，并基于此提出HerAgent，一种通过执行验证和修复逐步构建可执行环境的自动化方法。

Result: HerAgent在四个公开基准测试中表现优越，较相关工作提升最高79.6%，在复杂C/C++项目中提升66.7%。此外，HerAgent能够成功配置以前方法无法解决的11-30个环境实例。

Conclusion: 通过引入环境成熟度层次和执行驱动的自动化方法，HerAgent显著提升了软件环境设置的成功率和可靠性，推动了自动化环境搭建技术的发展。

Abstract: Automated software environment setup is a prerequisite for testing, debugging, and reproducing failures, yet remains challenging in practice due to complex dependencies, heterogeneous build systems, and incomplete documentation. Recent work leverages large language models to automate this process, but typically evaluates success using weak signals such as dependency installation or partial test execution, which do not ensure that a project can actually run. In this paper, we argue that environment setup success should be evaluated through executable evidence rather than a single binary signal. We introduce the Environment Maturity Hierarchy, which defines three success levels based on progressively stronger execution requirements, culminating in successful execution of a project's main entry point. Guided by this hierarchy, we propose HerAgent, an automated environment setup approach that incrementally constructs executable environments through execution-based validation and repair. We evaluate HerAgent on four public benchmarks, where it outperforms all related work, achieving up to 79.6\% improvement due to its holistic understanding of project structure and dependencies. On complex C/C++ projects, HerAgent surpasses prior approaches by 66.7\%. In addition, HerAgent uniquely resolves 11-30 environment instances across the benchmarks that no prior method can configure.

</details>


### [118] [Rethinking Code Complexity Through the Lens of Large Language Models](https://arxiv.org/abs/2602.07882)
*Chen Xie,Yuling Shi,Xiaodong Gu,Beijun Shen*

Main category: cs.SE

TL;DR: 传统代码复杂度指标无法准确反映大型语言模型处理代码的难度，本文提出基于语义非线性的LM-CC指标，更好地衡量模型面对代码的复杂度，且与模型性能密切相关。


<details>
  <summary>Details</summary>
Motivation: 传统代码复杂度度量指标与大型语言模型在代码理解和生成时遇到的困难之间缺乏一致关联，限制了复杂度指标在评估模型性能上的应用价值。

Method: 提出基于程序语义非线性的LM-CC指标，将程序分解为基于熵的语义单元，构建组合层级结构，结合分支引起的分歧量化代码复杂度，并通过实验验证与LLM性能的相关性。

Result: LM-CC指标与LLM性能的相关性显著优于传统指标，且降低LM-CC值能直接提升任务性能，证明了该指标的有效性。

Conclusion: 传统的代码复杂度指标如环状复杂度与大型语言模型（LLMs）处理代码的难度不匹配，LM-CC作为从LLM视角设计的新复杂度度量指标能更好地反映模型的实际处理难度。

Abstract: Code complexity metrics such as cyclomatic complexity have long been used to assess software quality and maintainability. With the rapid advancement of large language models (LLMs) on code understanding and generation tasks, an important yet underexplored question arises: do these traditional complexity metrics meaningfully characterize the difficulty LLMs experience when processing code? In this work, we empirically demonstrate that, after controlling for code length, classical metrics exhibit no consistent correlation with LLM performance, revealing a fundamental mismatch with model-perceived difficulty. To address this gap, we propose LM-CC, a novel code complexity metric designed from the perspective of LLMs. The core premise of LM-CC is that LLM-perceived difficulty is driven by the nonlinearity of program semantics. Accordingly, we decompose programs into semantic units based on entropy, organize these units into a compositional hierarchy, and quantify complexity as a principled aggregation of compositional level and branching-induced divergence, capturing cumulative model uncertainty during code processing. Our extensive experiments show that LM-CC not only correlates more strongly with LLM performance than traditional metrics but also that lowering it directly enhances task performance.

</details>


### [119] [Is Your Private Information Logged? An Empirical Study on Android App Logs](https://arxiv.org/abs/2602.07893)
*Zhiyuan Chen,Soham Sanjay Deo,Poorna Chander Reddy Puttaparthi,Vanessa Nava-Camal,Yiming Tang,Xueling Zhang,Weiyi Shang*

Main category: cs.SE

TL;DR: 本文通过实证分析发现Android应用日志中隐私泄露普遍，开发者缺乏足够认识，并给出相应防护建议。


<details>
  <summary>Details</summary>
Motivation: 随着移动应用快速增长，用户对隐私的关注日益突出，而现有研究未系统探讨Android应用日志中的隐私泄露问题。

Method: 构建了一个全面的Android应用日志数据集，进行了实证研究，通过分析真实开发者对隐私的关注、日志中的隐私泄露情况及其特征和成因。

Result: 发现开发者对日志隐私问题存在五类不同关注点，且隐私泄露普遍，主要因开发者无意识造成，并提出保护隐私的建议。

Conclusion: Android应用日志中存在普遍的隐私泄露问题，主要源于开发者对隐私泄露的无意识，并且开发者对此表现出多样化的隐私关注。

Abstract: With the rapid growth of mobile apps, users' concerns about their privacy have become increasingly prominent. Android app logs serve as crucial computer resources, aiding developers in debugging and monitoring the status of Android apps, while also containing a wealth of software system information. Previous studies have acknowledged privacy leaks in software logs and Android apps as significant issues without providing a comprehensive view of the privacy leaks in Android app logs. In this study, we build a comprehensive dataset of Android app logs and conduct an empirical study to analyze the status and severity of privacy leaks in Android app logs. Our study comprises three aspects: (1) Understanding real-world developers' concerns regarding privacy issues related to software logs; (2) Studying privacy leaks in the Android app logs; (3) Investigating the characteristics of privacy-leaking Android app logs and analyzing the reasons behind them. Our study reveals five different categories of concerns from real-world developers regarding privacy issues related to software logs and the prevalence of privacy leaks in Android app logs, with the majority stemming from developers' unawareness of such leaks. Additionally, our study provides developers with suggestions to safeguard their privacy from being logged.

</details>


### [120] [Rethinking the Value of Agent-Generated Tests for LLM-Based Software Engineering Agents](https://arxiv.org/abs/2602.07900)
*Zhi Chen,Zhensu Sun,Yuling Shi,Chao Peng,Xiaodong Gu,David Lo,Lingxiao Jiang*

Main category: cs.SE

TL;DR: 研究发现LLM代理编写测试虽常见，但对提升代码问题解决效果帮助不大，测试多为打印语句，调整测试数量对结果无显著影响。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM代码代理在解决仓库级问题时编写测试对任务解决效果的影响。

Method: 对六种最先进LLM代理在SWE-bench Verified上的行为轨迹进行实证分析，并通过调整四个代理的提示信息控制测试编写量进行对比实验。

Result: 发现代理编写测试的频率在解决和未解决任务中相似，且多数测试为观察性反馈（如打印语句），正式断言检查较少；增加或减少测试编写量对最终结果影响不显著。

Conclusion: 当前LLM代理的测试编写实践对自动化软件工程任务的效果提升有限。

Abstract: Large Language Model (LLM) code agents increasingly resolve repository-level issues by iteratively editing code, invoking tools, and validating candidate patches. In these workflows, agents often write tests on the fly, a paradigm adopted by many high-ranking agents on the SWE-bench leaderboard. However, we observe that GPT-5.2, which writes almost no new tests, can even achieve performance comparable to top-ranking agents. This raises the critical question: whether such tests meaningfully improve issue resolution or merely mimic human testing practices while consuming a substantial interaction budget.
  To reveal the impact of agent-written tests, we present an empirical study that analyzes agent trajectories across six state-of-the-art LLMs on SWE-bench Verified. Our results show that while test writing is commonly adopted, but resolved and unresolved tasks within the same model exhibit similar test-writing frequencies Furthermore, these tests typically serve as observational feedback channels, where agents prefer value-revealing print statements significantly more than formal assertion-based checks. Based on these insights, we perform a controlled experiment by revising the prompts of four agents to either increase or reduce test writing. The results suggest that changes in the volume of agent-written tests do not significantly change final outcomes. Taken together, our study reveals that current test-writing practices may provide marginal utility in autonomous software engineering tasks.

</details>


### [121] [Agent Skills: A Data-Driven Analysis of Claude Skills for Extending Large Language Model Functionality](https://arxiv.org/abs/2602.08004)
*George Ling,Shanshan Zhong,Richard Huang*

Main category: cs.SE

TL;DR: 本文通过分析超4万项代理技能，揭示了其内容集中度、用户采用趋势、供应需求不平衡及安全风险，提供了代理技能生态系统的量化洞察，为未来设计和安全策略提供依据。


<details>
  <summary>Details</summary>
Motivation: 随着代理技能的增多，亟需了解其类型、用户采用情况以及可能存在的安全风险，以指导未来的技能复用、标准化和安全设计。

Method: 通过对大型市场中40,285个公开列出的代理技能进行数据驱动的大规模分析，评估其发布模式、内容类别、采纳率及风险。

Result: 发现技能发布呈短时爆发模式，内容主要集中于软件工程流程，信息检索和内容创建领采纳占比显著；存在供应需求失衡，技能长度分布重尾且大部分技能在提示预算内；生态系统同质性强，存在意图冗余和一定的安全风险。

Conclusion: 本研究揭示了大规模语言模型代理技能市场的内容分布、用户采纳情况及潜在风险，强调了当前技能生态系统的同质化及安全问题。

Abstract: Agent skills extend large language model (LLM) agents with reusable, program-like modules that define triggering conditions, procedural logic, and tool interactions. As these skills proliferate in public marketplaces, it is unclear what types are available, how users adopt them, and what risks they pose. To answer these questions, we conduct a large-scale, data-driven analysis of 40,285 publicly listed skills from a major marketplace. Our results show that skill publication tends to occur in short bursts that track shifts in community attention. We also find that skill content is highly concentrated in software engineering workflows, while information retrieval and content creation account for a substantial share of adoption. Beyond content trends, we uncover a pronounced supply-demand imbalance across categories, and we show that most skills remain within typical prompt budgets despite a heavy-tailed length distribution. Finally, we observe strong ecosystem homogeneity, with widespread intent-level redundancy, and we identify non-trivial safety risks, including skills that enable state-changing or system-level actions. Overall, our findings provide a quantitative snapshot of agent skills as an emerging infrastructure layer for agents and inform future work on skill reuse, standardization, and safety-aware design.

</details>


### [122] [Bridging the Gap: Adapting Evidence to Decision Frameworks to support the link between Software Engineering academia and industry](https://arxiv.org/abs/2602.08015)
*Patricia G. F. Matsubara,Tayana Conte*

Main category: cs.SE

TL;DR: 软件工程领域SLR成果难以影响实践，本文引入健康科学中专业的Evidence to Decision框架，借助专家评审，改善研究证据转化为工业实践的推荐过程。


<details>
  <summary>Details</summary>
Motivation: 当前软件工程领域进行的系统文献综述虽然方法完善，但其研究成果未能有效影响工业实践，存在成果与实践脱节的问题。

Method: 本文借鉴健康科学的EtD框架，提出在软件工程中采用专家小组评估最佳证据的方式，结合系统文献综述结果，形成结构化的推荐。通过一个基于软件工程SLR的实例进行说明。

Result: 提出应用EtD框架来补充和丰富对从SLR获得证据的解读与推荐，强调需要更全面的推荐标准以促进研究成果向实践的转化。

Conclusion: 尽管软件工程领域进行了大量的系统文献综述（SLRs），但其结果仍未有效传达到实践者。引入健康科学中的Evidence to Decision (EtD)框架，能够通过专家评审小组构建更全面的推荐标准，从而更好地将研究证据转化为实际决策。

Abstract: Over twenty years ago, the Software Engineering (SE) research community have been involved with Evidence-Based Software Engineering (EBSE). EBSE aims to inform industrial practice with the best evidence from rigorous research, preferably from systematic literature reviews (SLRs). Since then, SE researchers have conducted many SLRs, perfected their SLR procedures, proposed alternative ways of presenting their results (such as Evidence Briefings), and profusely discussed how to conduct research that impacts practice. Nevertheless, there is still a feeling that SLRs' results are not reaching practitioners. Something is missing. In this vision paper, we introduce Evidence to Decision (EtD) frameworks from the health sciences, which propose gathering experts in panels to assess the existing best evidence about the impact of an intervention in all relevant outcomes and make structured recommendations based on them. The insight we can leverage from EtD frameworks is not their structure per se but all the relevant criteria for making recommendations to practitioners from SLRs. Furthermore, we provide a worked example based on an SE SLR. We also discuss the challenges the SE research and practice community may face when adopting EtD frameworks, highlighting the need for more comprehensive criteria in our recommendations to industry practitioners.

</details>


### [123] [Outsourcing in Global Software Development: Effects of Temporal Location and Methodologies](https://arxiv.org/abs/2602.08084)
*Mark Looi,Marc Szepan*

Main category: cs.SE

TL;DR: 本研究调查了不同时间距离下软件外包开发的效果，发现近岸开发在成功率和沟通上明显优于远岸开发，尤其适合敏捷项目。


<details>
  <summary>Details</summary>
Motivation: 全球外包软件开发项目中，不同时间区的分布对项目成功及其他关键指标的影响尚未明确。

Method: 通过调查80位客户及访谈6位客户，分析时间距离及开发方法对项目成功、成本、项目管理、进度、质量和沟通的影响。

Result: 近岸外包在整体成功、质量、降低项目管理努力、保持进度及减少沟通问题上有显著优势；开发方法主要影响成本。

Conclusion: 建议在通信密集或敏捷项目中优先选择近岸开发以提升项目效果和减少问题。

Abstract: Developing software globally using outsourced resources has become a common practice, with project teams often distributed in different time zones. In this study, we focus on customers that contract software development to vendors in temporally nearshore or far offshore locations. We conducted a survey to determine the effect of temporal distance on overall success, costs, project management effort, schedule, quality, communication problems, and other outcomes of interest to managers. In the survey of 80 customers and interviews with 6 of them, we also investigated the effect of software development methodology on the same outcomes. The results show that nearshore development is advantageous for overall success, quality, reduced PM effort, maintaining schedule, higher quality, and engendering fewer communication problems. Development methodology appears to only influence higher costs. We assess our findings in the context of prior GSE research and provide practical advice for customers of outsourced global software development, chief of which is to favor nearshore for communication-intensive or Agile projects.

</details>


### [124] [Integrating Code Metrics into Automated Documentation Generation for Computational Notebooks](https://arxiv.org/abs/2602.08133)
*Mojtaba Mostafavi Ghahfarokhi,Hamed Jahantigh,Alireza Asadi,Abbas Heydarnoori*

Main category: cs.SE

TL;DR: 本研究通过引入代码度量指标作为辅助信息，改进了计算笔记本文档自动生成方法，在多种模型上均显著提升了生成文档的质量和相关性。


<details>
  <summary>Details</summary>
Motivation: 现有自动文档生成方法在捕捉代码结构和量化特征方面不足，而代码度量指标包含有助于程序理解的重要信息。数据科学中使用的计算笔记本文档不一致，该领域亟需改进文档生成技术。

Method: 提出两阶段方法：一是从CodeSearchNet数据集中筛选高质量代码与文档对构建专门数据集；二是评估轻量级CNN-RNN和少量训练的GPT-3.5架构，比较加入与不加入代码度量信息的效果。

Result: 通过在模型中加入代码度量，CNN-RNN模型BLEU-1指标提升6%，ROUGE-L F1提升3%；LLM模型BERTScore F1提升9%，表明代码度量有效提高了文档生成的表现。

Conclusion: 集成代码度量指标作为辅助信号能够显著提升自动文档生成的准确性和上下文相关性，增强不同模型架构的性能。

Abstract: Effective code documentation is essential for collaboration, comprehension, and long-term software maintainability, yet developers often neglect it due to its repetitive nature. Automated documentation generation has evolved from heuristic and rule-based methods to neural network-based and large language model (LLM)-based approaches. However, existing methods often overlook structural and quantitative characteristics of code that influence readability and comprehension. Prior research suggests that code metrics capture information relevant to program understanding. Building on these insights, this paper investigates the role of source code metrics as auxiliary signals for automated documentation generation, focusing on computational notebooks, a popular medium among data scientists that integrates code, narrative, and results but suffers from inconsistent documentation. We propose a two-stage approach. First, the CodeSearchNet dataset construction process was refined to create a specialized dataset from over 17 million code and markdown cells. After structural and semantic filtering, approximately 36,734 high-quality (code, markdown) pairs were extracted. Second, two modeling paradigms, a lightweight CNN-RNN architecture and a few-shot GPT-3.5 architecture, were evaluated with and without metric information. Results show that incorporating code metrics improves the accuracy and contextual relevance of generated documentation, yielding gains of 6% in BLEU-1 and 3% in ROUGE-L F1 for CNN-RNN-based architecture, and 9% in BERTScore F1 for LLM-based architecture. These findings demonstrate that integrating code metrics provides valuable structural context, enhancing automated documentation generation across diverse model families.

</details>


### [125] [Test vs Mutant: Adversarial LLM Agents for Robust Unit Test Generation](https://arxiv.org/abs/2602.08146)
*Pengyu Chang,Yixiong Fang,Silin Chen,Yuling Shi,Beijun Shen,Xiaodong Gu*

Main category: cs.SE

TL;DR: 本文提出AdverTest对抗框架提升LLM驱动测试用例的缺陷检测能力和覆盖率，实验证明效果显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法多注重测试覆盖率和可读性，缺乏对边界情况和脆弱执行路径的鲁棒缺陷检测能力的关注。

Method: 提出了由测试用例生成代理(T)和变异体生成代理(M)组成的对抗框架，二者在覆盖率和变异得分的指导下循环交互，逐步优化测试集。

Result: 在Defects4J数据集上，AdverTest相比现有最佳LLM方法缺陷检测率提升8.56%，相比EvoSuite提升63.30%，同时提高了代码行覆盖率和分支覆盖率。

Conclusion: AdverTest通过对抗框架有效提升了LLM驱动的测试用例生成在缺陷检测和代码覆盖方面的表现。

Abstract: Software testing is a critical, yet resource-intensive phase of the software development lifecycle. Over the years, various automated tools have been developed to aid in this process. Search-based approaches typically achieve high coverage but produce tests with low readability, whereas large language model (LLM)-based methods generate more human-readable tests but often suffer from low coverage and compilability. While the majority of research efforts have focused on improving test coverage and readability, little attention has been paid to enhancing the robustness of bug detection, particularly in exposing corner cases and vulnerable execution paths. To address this gap, we propose AdverTest, a novel adversarial framework for LLM-powered test case generation. AdverTest comprises two interacting agents: a test case generation agent (T) and a mutant generation agent (M). These agents engage in an adversarial loop, where M persistently creates new mutants "hacking" the blind spots of T's current test suite, while T iteratively refines its test cases to "kill" the challenging mutants produced by M. This interaction loop is guided by both coverage and mutation scores, enabling the system to co-evolve toward both high test coverage and bug detection capability. Experimental results in the Defects4J dataset show that our approach improves fault detection rates by 8.56% over the best existing LLM-based methods and by 63.30% over EvoSuite, while also improving line and branch coverage.

</details>


### [126] [Distributed Architecture Reconstruction of Polyglot and Multi-Repository Microservice Projects](https://arxiv.org/abs/2602.08166)
*Oscar Manglaras,Alex Farkas,Thomas Woolford,Christoph Treude,Markus Wagner*

Main category: cs.SE

TL;DR: 本文提出了一种支持多技术模块和多仓库环境的静态架构重建框架，解决了微服务架构中架构文档难维护的问题。


<details>
  <summary>Details</summary>
Motivation: 微服务架构下服务数量多且各自独立，导致架构复杂且文档难以维护。现有静态架构重建方法存在技术限制、单仓库约束和实现门槛高等问题。

Method: 设计并实现了一套支持技术专用分析模块（extractors）的静态架构重建框架；提出了extractors的执行机制、数据传递和结果统一算法；支持与现有静态分析工具和算法的互操作。

Result: 开发了一个灵活的静态架构重建框架，支持多技术分析模块的组合和分布式架构重建，并且能够与现有工具兼容，提高了架构文档的准确性和维护性。

Conclusion: 本文提出的静态架构重建框架有效支持多技术分析模块和多仓库环境下的分布式架构重建，克服了现有方法在技术限制、单一仓库和实现难度方面的问题。

Abstract: Microservice architectures encourage the use of small, independently developed services; however, this can lead to increased architectural complexity. Accurate documentation is crucial, but is challenging to maintain due to the rapid, independent evolution of services. While static architecture reconstruction provides a way to maintain up-to-date documentation, existing approaches suffer from technology limitations, mono-repo constraints, or high implementation barriers. This paper presents a novel framework for static architecture reconstruction that supports technology-specific analysis modules, called \emph{extractors}, and supports \emph{distributed architecture reconstruction} in multi-repo environments. We describe the core design concepts and algorithms that govern how extractors are executed, how data is passed between them, and how their outputs are unified. Furthermore, the framework is interoperable with existing static analysis tools and algorithms, allowing them to be invoked from or embedded within extractors.

</details>


### [127] [ModARO: A Modular Approach to Architecture Reconstruction of Distributed Microservice Codebases](https://arxiv.org/abs/2602.08181)
*Oscar Manglaras,Alex Farkas,Thomas Woolford,Christoph Treude,Markus Wagner*

Main category: cs.SE

TL;DR: 本论文提出ModARO，一种模块化的微服务架构自动重构方法，支持跨项目、跨技术栈重用代码，解决了多代码库分布带来的架构复杂性，且通过实验证明了其实用性。


<details>
  <summary>Details</summary>
Motivation: 微服务架构虽促进独立服务开发，但带来架构复杂性和文档维护困难，且多代码库分散使架构重构更加困难，亟需一种跨项目重用且技术无关的自动架构重构方法。

Method: 提出了ModARO框架，利用模块化的重构代码('extractors')，支持不同技术栈和多代码库环境下的架构重构，结合CI/CD流水线实现自动化。

Result: ModARO成功配置用于10个开源项目，并通过8名业内从业者的用户研究验证其有效性和易用性，展示了良好的实用价值。

Conclusion: ModARO方法成功实现了微服务架构的自动重构，支持跨技术、多项目的重用，解决了多代码库分散带来的复杂性问题。

Abstract: Microservice architectures promote small, independently developed services, but increase overall architectural complexity. It is crucial that developers understand the architecture and how changes to a service affect the overall system, but rapid and independent development of services increases the risk of architectural drift and discourages the creation and maintenance of documentation. Automatic architecture reconstruction can help avoid these issues, but it is difficult to reuse reconstruction code across multiple projects, as all use different combinations of technologies and project-specific conventions. Reconstruction of architecture-level details is further complicated by the tendency to split microservices into separate repositories, preventing a full view of the system from any one codebase. In this paper, we present and evaluate ModARO, an approach to microservice architecture reconstruction that allows writing modular reconstruction code ('extractors') for any technologies and reusing them across different projects, independent of the surrounding technology stack or whether or not the services are split into multiple codebases. We demonstrate the effectiveness of our approach by configuring ModARO to reconstruct 10 open source projects, and we validate the usefulness and usability of ModARO against a state-of-the-art baseline in a user study with 8 industry practitioners. Using this approach, developers can assemble or create extractors tailored to their technology stacks and distribute architecture reconstruction across repositories, enabling integration into repository CI/CD pipelines.

</details>


### [128] [Adoption of Large Language Models in Scrum Management: Insights from Brazilian Practitioners](https://arxiv.org/abs/2602.08192)
*Mirko Perkusich,Danyllo Albuquerque,Allysson Allex Araújo,Matheus Paixão,Rohit Gheyi,Marcos Kalinowski,Angelo Perkusich*

Main category: cs.SE

TL;DR: 本文调查了LLMs在Scrum管理中的应用状况，发现其提高了效率但存在准确性和安全隐患，首次为Scrum管理结合LLMs提供了实证数据和指导建议。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在技术活动中应用广泛，但其在Scrum管理活动中的实际应用及效果尚缺乏系统研究。本文旨在填补这一空白。

Method: 通过对70名巴西专业人士进行调查，收集他们在Scrum管理中使用LLM助手的实际情况和经验。

Result: 85%的受访者对LLM具有中高级熟练度，52%每天使用。LLM主要支持Scrum实践中的特定产物和事件管理，提升生产力（78%）和减少手工工作（75%）。但也报告了输出近似正确（81%）、保密顾虑（63%）和幻觉（59%）等风险。

Conclusion: 本文首次实证研究了大型语言模型（LLMs）在Scrum管理活动中的应用，发现LLMs被广泛使用且具有显著提升生产力和减少人工努力的效果，但同时存在输出准确性不足、保密性风险和幻觉现象等挑战。

Abstract: Scrum is widely adopted in software project management due to its adaptability and collaborative nature. The recent emergence of Large Language Models (LLMs) has created new opportunities to support knowledge-intensive Scrum practices. However, existing research has largely focused on technical activities such as coding and testing, with limited evidence on the use of LLMs in management-related Scrum activities. In this study, we investigate the use of LLMs in Scrum management activities through a survey of 70 Brazilian professionals. Among them, 49 actively use Scrum, and 33 reported using LLM-based assistants in their Scrum practices. The results indicate a high level of proficiency and frequent use of LLMs, with 85% of respondents reporting intermediate or advanced proficiency and 52% using them daily. LLM use concentrates on exploring Scrum practices, with artifacts and events receiving targeted yet uneven support, whereas broader management tasks appear to be adopted more cautiously. The main benefits include increased productivity (78%) and reduced manual effort (75%). However, several critical risks remain, as respondents report 'almost correct' outputs (81%), confidentiality concerns (63%), and hallucinations during use (59%). This work provides one of the first empirical characterizations of LLM use in Scrum management, identifying current practices, quantifying benefits and risks, and outlining directions for responsible adoption and integration in Agile environments.

</details>


### [129] [Software Testing at the Network Layer: Automated HTTP API Quality Assessment and Security Analysis of Production Web Applications](https://arxiv.org/abs/2602.08242)
*Ali Hassaan Mughal,Muhammad Bilal*

Main category: cs.SE

TL;DR: 本研究通过自动化工具分析了18个现代网站的HTTP API调用质量，发现普遍存在冗余请求和缓存缺失问题，提出一个可复现的测试框架，为网页性能和安全改进提供数据支持。


<details>
  <summary>Details</summary>
Motivation: 当前现代网页应用客户端API调用质量缺乏系统性的测试，而这些质量缺陷不仅影响性能，还带来安全隐患。

Method: 通过Playwright自动化浏览器，收集18个类别的18个生产网站的108个HTTP Archive（HAR）文件，应用8个启发式反模式检测器，生成0-100的质量评分。

Result: 最优服务器渲染的网站获得满分100分，内容丰富的商业网站最低仅得56.8分。67%的网站存在冗余API调用和缺失缓存头，72%的网站第三方请求占比超过20%。最大请求数达2,684次，远超最简站点。

Conclusion: 本研究揭示了现代网页应用中HTTP API调用质量的巨大差异，强调了冗余请求、缺失缓存头和第三方依赖过多等常见反模式对性能和安全的负面影响。

Abstract: Modern web applications rely heavily on client-side API calls to fetch data, render content, and communicate with backend services. However, the quality of these network interactions (redundant requests, missing cache headers, oversized payloads, and excessive third-party dependencies) is rarely tested in a systematic way. Moreover, many of these quality deficiencies carry security implications: missing cache headers enable cache poisoning, excessive third-party dependencies expand the supply-chain attack surface, and error responses risk leaking server internals. In this study, we present an automated software testing framework that captures and analyzes the complete HTTP traffic of 18 production websites spanning 11 categories (e-commerce, news, government, developer tools, travel, and more). Using automated browser instrumentation via Playwright, we record 108 HAR (HTTP Archive) files across 3 independent runs per page, then apply 8 heuristic-based anti-pattern detectors to produce a composite quality score (0-100) for each site. Our results reveal a wide quality spectrum: minimalist server-rendered sites achieve perfect scores of 100, while content-heavy commercial sites score as low as 56.8. We identify redundant API calls and missing cache headers as the two most pervasive anti-patterns, each affecting 67% of sites, while third-party overhead exceeds 20% on 72% of sites. One utility site makes 2,684 requests per page load, which is 447x more than the most minimal site. To protect site reputations, all identities are anonymized using category-based pseudonyms. We provide all analysis scripts, anonymized results, and reproducibility instructions as an open artifact. This work establishes an empirical baseline for HTTP API call quality across the modern web and offers a reproducible testing framework that researchers and practitioners can apply to their own applications.

</details>


### [130] [Specification Vibing for Automated Program Repair](https://arxiv.org/abs/2602.08263)
*Taohong Zhu,Lucas C. Cordeiro,Mustafa A. Mustafa,Youcheng Sun*

Main category: cs.SE

TL;DR: VibeRepair通过修复行为规范而非直接修改代码，显著提升了大语言模型驱动的自动程序修复的准确性和效率，开创了基于行为意图的程序修复新范式。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大型语言模型的自动程序修复方法主要侧重于代码本身，容易产生幻觉式且行为不一致的修复结果，亟需一种更贴近程序意图、易于语言模型理解的修复范式。

Method: VibeRepair先将有缺陷的代码转换为结构化的行为规范，捕获程序的预期运行行为，随后推断并修复规范的不一致，最后基于校正后的行为规范合成代码。该方法还引入按需推理组件，结合程序分析和历史修复证据提高修复质量。

Result: 在Defects4J v1.2和v2.0以及真实世界基准测试中，VibeRepair分别正确修复了174和178个缺陷，分别领先目前最强基线28和33个，提升率达19%和23%，同时修复空间显著缩小，且在训练期间后数据上表现出良好的泛化能力。

Conclusion: VibeRepair通过将自动程序修复的焦点从代码本身转向行为规范的修复，实现了更高的修复准确性和有效性。

Abstract: Large language model (LLM)-driven automated program repair (APR) has advanced rapidly, but most methods remain code-centric: they directly rewrite source code and thereby risk hallucinated, behaviorally inconsistent fixes. This limitation suggests the need for an alternative repair paradigm that relies on a representation more accessible to LLMs than raw code, enabling more accurate understanding, analysis, and alignment during repair. To address this gap, we propose VibeRepair, a specification-centric APR technique that treats repair as behavior-specification repair rather than ad-hoc code editing. VibeRepair first translates buggy code into a structured behavior specification that captures the program's intended runtime behavior, then infers and repairs specification misalignments, and finally synthesizes code strictly guided by the corrected behavior specification. An on-demand reasoning component enriches hard cases with program analysis and historical bug-fix evidence while controlling cost. Across Defects4J and real-world benchmarks and multiple LLMs, VibeRepair demonstrates consistently strong repair effectiveness with a significantly smaller patch space. On Defects4J v1.2, VibeRepair correctly repairs 174 bugs, exceeding the strongest state-of-the-art baseline by 28 bugs, which corresponds to a 19% improvement. On Defects4J v2.0, it repairs 178 bugs, outperforming prior approaches by 33 bugs, representing a 23% improvement. Evaluations on real-world benchmarks collected after the training period of selected LLMs further confirm its effectiveness and generalizability. By centering repair on explicit behavioral intent, VibeRepair reframes APR for the era of "vibe" coding: make the behavior sing, and the code will follow.

</details>


### [131] [SWE Context Bench: A Benchmark for Context Learning in Coding](https://arxiv.org/abs/2602.08316)
*Jared Zhu,Minhao Hu,Junde Wu*

Main category: cs.SE

TL;DR: 本文提出SWE-ContextBench基准，系统评估编程代理在关联任务中的经验复用能力，验证了优质经验检索与表达显著提升效率和准确性的效果。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型作为编程代理在软件工程任务中广泛应用，但评价基准多将任务视为独立，缺乏对经验复用能力的测评，导致无法准确衡量代理累积、检索及应用先前经验的能力和效率提升。

Method: 提出SWE-ContextBench基准，在SWE-Bench Lite基础上增添99个相关任务形成关联序列，评估代理在预测准确性、时间效率与成本效率三方面的表现，涵盖多种经验复用设置如指导检索和自主检索，及不同经验表达方式。

Result: 实验结果显示，正确筛选和总结的经验显著提升解决准确性并减少运行时间和代币成本，尤其是复杂任务中；而未经筛选或错误经验则效果有限甚至负面。

Conclusion: 高质量经验的表达与检索是提升编程代理效能的关键，SWE-ContextBench为研究经验复用提供了有力的评测工具。

Abstract: Large language models are increasingly used as programming agents for repository level software engineering tasks. While recent benchmarks evaluate correctness in realistic codebases, they largely treat tasks as independent and do not assess whether agents can reuse experience across related problems. As a result, the ability of agents to accumulate, retrieve, and apply prior experience, as well as the efficiency gains from such reuse, remains difficult to measure. We introduce SWE-ContextBench, a benchmark designed to explicitly evaluate experience reuse in programming agents. Built on SWE-Bench Lite, SWE-ContextBench augments 300 base tasks with 99 related tasks derived from real dependency and reference relationships among GitHub issues and pull requests, forming task sequences with shared context. The benchmark evaluates agents along three complementary dimensions: prediction accuracy, time efficiency, and cost efficiency. Using SWE-ContextBench, we study multiple experience reuse settings, including oracle guided and autonomous retrieval, as well as full execution trajectories and compact summaries. Our results show that correctly selected summarized experience improves resolution accuracy and substantially reduces runtime and token cost, particularly on harder tasks. In contrast, unfiltered or incorrectly selected experience provides limited or negative benefits. These findings highlight the importance of experience representation and retrieval quality, and position SWE-ContextBench as a principled benchmark for studying experience reuse in programming agents.

</details>


### [132] [Automating Computational Reproducibility in Social Science: Comparing Prompt-Based and Agent-Based Approaches](https://arxiv.org/abs/2602.08561)
*Syed Mehtab Hussain Shah,Frank Hopfgartner,Arnim Bleier*

Main category: cs.SE

TL;DR: 本研究通过受控测试平台评估了大型语言模型和AI代理自动修复计算研究失败的能力，发现智能代理系统能大幅提升复现成功率，减少人工干预需求。


<details>
  <summary>Details</summary>
Motivation: 尽管共享原始代码和数据，但计算研究复现经常因缺失软件包、脆弱文件路径、版本冲突或不完整逻辑而失败，研究探讨大型语言模型和AI代理能否自动诊断和修复这些失败，提高计算结果的易复现性和验证性。

Method: 构建了一个包含五个完全可复现的基于R的社会科学研究的受控复现测试平台，注入各种真实失败（从简单问题到复杂缺失逻辑），测试了两种自动修复工作流：基于提示的多轮查询和基于智能代理的自主文件检查、代码修改及重运行。

Result: 基于提示的修复成功率介于31%-79%，受提示上下文和错误复杂度影响显著，复杂错误通过更多上下文改善效果明显。智能代理工作流表现更优，成功率达到69%-96%。

Conclusion: 自动化的修复工作流，尤其是基于智能代理的系统，能够显著减少人工工作量，提高不同复杂错误类型的复现成功率。

Abstract: Reproducing computational research is often assumed to be as simple as rerunning the original code with provided data. In practice, missing packages, fragile file paths, version conflicts, or incomplete logic frequently cause analyses to fail, even when materials are shared. This study investigates whether large language models and AI agents can automate the diagnosis and repair of such failures, making computational results easier to reproduce and verify. We evaluate this using a controlled reproducibility testbed built from five fully reproducible R-based social science studies. Realistic failures were injected, ranging from simple issues to complex missing logic, and two automated repair workflows were tested in clean Docker environments. The first workflow is prompt-based, repeatedly querying language models with structured prompts of varying context, while the second uses agent-based systems that inspect files, modify code, and rerun analyses autonomously. Across prompt-based runs, reproduction success ranged from 31-79 percent, with performance strongly influenced by prompt context and error complexity. Complex cases benefited most from additional context. Agent-based workflows performed substantially better, with success rates of 69-96 percent across all complexity levels. These results suggest that automated workflows, especially agent-based systems, can significantly reduce manual effort and improve reproduction success across diverse error types. Unlike prior benchmarks, our testbed isolates post-publication repair under controlled failure modes, allowing direct comparison of prompt-based and agent-based approaches.

</details>


### [133] [Taming Scylla: Understanding the multi-headed agentic daemon of the coding seas](https://arxiv.org/abs/2602.08765)
*Micah Villmow*

Main category: cs.SE

TL;DR: 本文提出Scylla框架，通过分层测试和成本指标，系统评估基于LLM的编程代理复杂度对性能的影响，发现更复杂架构不一定更优。


<details>
  <summary>Details</summary>
Motivation: 当前基于大型语言模型(LLM)的工具快速自动化软件开发任务，但缺乏严谨方法评估不同架构设计（如提示、技能、工具、多代理设置）对能力和成本的影响。

Method: 提出Scylla评估框架，通过七个递进复杂度的测试层级（T0-T6）进行结构化消融研究，采用Cost-of-Pass(CoP)指标量化复杂度与效率的权衡；框架模型无关，可适用于任何命令行工具；用Claude Sonnet 4.5及多个LLM评审模型进行验证。

Result: 搭建了一个可复现的评估框架，能够量化代理复杂度与实际效果之间的权衡，发现架构复杂度不一定提升质量。

Conclusion: Scylla框架为评估基于LLM的编码代理提供了一种严谨的方法，揭示了提升架构复杂性并非总能带来更好性能的事实。

Abstract: LLM-based tools are automating more software development tasks at a rapid pace, but there is no rigorous way to evaluate how different architectural choices -- prompts, skills, tools, multi-agent setups -- materially affect both capability and cost. This paper introduces Scylla, an evaluation framework for benchmarking agentic coding tools through structured ablation studies that uses seven testing tiers (T0-T6) progressively adding complexity to isolate what directly influences results and how. The key metric is Cost-of-Pass (CoP): the expected dollar cost to get one correct solution, which directly quantifies the trade-off between complexity and efficiency. The framework is model-agnostic, designed to work with any CLI tool; this paper demonstrates it with Claude Sonnet 4.5, using multiple LLM judges (Opus 4.5, Sonnet 4.5, Haiku 4.5) from the same vendor for evaluation consensus, where judges score results using direct tests, human-designed LLM-evaluated rubrics, and qualitative assessment. The result is a reproducible framework that quantifies trade-offs between agent complexity and actual outcomes, suggesting that architectural complexity does not always improve quality.

</details>


### [134] [ArkEval: Benchmarking and Evaluating Automated CodeRepair for ArkTS](https://arxiv.org/abs/2602.08866)
*Bang Xie,Senjian Zhang,Zhiyuan Peng,Wei Chen,Chenhao Ying,Yuan Luo*

Main category: cs.SE

TL;DR: 本文针对HarmonyOS的ArkTS语言，构建了首个自动修复评测基准ArkEval，并利用多阶段过滤、LLM测试生成及评测流程，评估了四款大语言模型在ArkTS代码自动修复上的表现，揭示目前模型的优势和不足。


<details>
  <summary>Details</summary>
Motivation: HarmonyOS生态中ArkTS作为关键语言，缺乏自动化代码修复工具和高质量评测基准，限制了自动修复技术的发展和应用。

Method: 通过从华为官方仓库挖掘超过400个ArkTS应用的502个可复现问题构建基准，采用基于大型语言模型（如Claude）的测试生成与投票机制，提高测试效力，并标准化问题表述，最后用四个先进大语言模型结合检索增强修复流程进行评测。

Result: 构建了首个针对ArkTS自动修复的综合基准ArkEval，成功实现自动化测试生成和标准化问题描述，并通过基准评测呈现了四种大型语言模型在该领域的性能表现。

Conclusion: 本文提出了ArkEval框架及基准，填补了ArkTS自动修复领域的评测空白，通过测试链标准化和LLM辅助修复，展示了现有大语言模型在ArkTS代码修复上的能力及不足。

Abstract: Large language models have transformed code generation, enabling unprecedented automation in software development. As mobile ecosystems evolve, HarmonyOS has emerged as a critical platform requiring robust development tools. Software development for the HarmonyOS ecosystem relies heavily on ArkTS, a statically typed extension of TypeScript. Despite its growing importance, the ecosystem lacks robust tools for automated code repair, primarily due to the absence of a high-quality benchmark for evaluation. To address this gap, we present ArkEval, a unified framework for ArkTS automated repair workflow evaluation and benchmark construction. It provides the first comprehensive benchmark specifically designed for ArkTS automated program repair. We constructed this benchmark by mining issues from a large-scale official Huawei repository containing over 400 independent ArkTS applications. Through a rigorous multi-stage filtering process, we curated 502 reproducible issues. To ensure testability, we employed a novel LLM-based test generation and voting mechanism involving Claude and other models. Furthermore, we standardized problem statements to facilitate fair evaluation. Finally, we evaluated four state-of-the-art Large Language Models (LLMs) on our benchmark using a retrieval-augmented repair workflow. Our results highlight the current capabilities and limitations of LLMs in repairing ArkTS code, paving the way for future research in this low-resource language domain.

</details>


### [135] [DeepQuali: Initial results of a study on the use of large language models for assessing the quality of user stories](https://arxiv.org/abs/2602.08887)
*Adam Trendowicz,Daniel Seifert,Andreas Jedlitschka,Marcus Ciolkowski,Anton Strahilov*

Main category: cs.SE

TL;DR: 该论文提出并验证了基于GPT-4o的DeepQuali方法，用于敏捷开发中的需求质量评估。实验显示LLM评估与专家评价高度一致，且具备解释能力，增强了方法接受度，但需改进与工作流的整合。


<details>
  <summary>Details</summary>
Motivation: 现有的生成式人工智能在软件工程中的应用多集中于编码，需求工程特别是需求验证方面的应用有限，且质量评估缺乏有效方法，故提出基于LLM的需求质量评估方案。

Method: 提出并评估了基于LLM的DeepQuali方法，通过对两个小型公司的项目进行需求质量评估，比较LLM与专家的判断，并通过专家参与的评审和反馈验证方法的有效性。

Result: LLM的质量评估与专家整体评分高度一致，并能提供解释性反馈，提升了专家对该方法的认可，但专家对详细评分存在分歧，反映了个体经验的影响，同时专家指出该方法尚未很好集成到日常工作流。

Conclusion: 基于大语言模型（GPT-4o）的DeepQuali方法在敏捷软件开发中用于需求质量评估具有较高的准确性和专家接受度，但专家对细节评分存在分歧，表明经验影响判断，且方法需更好地与实际工作流程整合。

Abstract: Generative artificial intelligence (GAI), specifically large language models (LLMs), are increasingly used in software engineering, mainly for coding tasks. However, requirements engineering - particularly requirements validation - has seen limited application of GAI. The current focus of using GAI for requirements is on eliciting, transforming, and classifying requirements, not on quality assessment. We propose and evaluate the LLM-based (GPT-4o) approach "DeepQuali", for assessing and improving requirements quality in agile software development. We applied it to projects in two small companies, where we compared LLM-based quality assessments with expert judgments. Experts also participated in walkthroughs of the solution, provided feedback, and rated their acceptance of the approach. Experts largely agreed with the LLM's quality assessments, especially regarding overall ratings and explanations. However, they did not always agree with the other experts on detailed ratings, suggesting that expertise and experience may influence judgments. Experts recognized the usefulness of the approach but criticized the lack of integration into their workflow. LLMs show potential in supporting software engineers with the quality assessment and improvement of requirements. The explicit use of quality models and explanatory feedback increases acceptance.

</details>


### [136] [Comparing AI Coding Agents: A Task-Stratified Analysis of Pull Request Acceptance](https://arxiv.org/abs/2602.08915)
*Giovanni Pinna,Jingzhi Gong,David Williams,Federica Sarro*

Main category: cs.SE

TL;DR: 本研究比较了五款AI编码助手在不同软件开发任务的表现，发现任务类型是影响接受率的关键因素，各助手在不同任务上表现有优势，OpenAI Codex整体表现较优，但无单一助手适用于所有任务。


<details>
  <summary>Details</summary>
Motivation: 尽管AI编码助手被快速采用，但缺乏系统性的比较研究，特别是不同任务类型和时间维度上的效果评估。

Method: 通过分析AIDev数据集中7,156个拉取请求，比较五种流行的AI编码助手（OpenAI Codex、GitHub Copilot、Devin、Cursor和Claude Code）的接受率，并进行时间趋势分析和分层卡方检验。

Result: Devin在接受率上有唯一持续正向趋势，文档任务的接受率高达82.1%，新功能任务仅为66.1%，且OpenAI Codex在所有任务类别中接受率均较高。不同助手在特定任务中表现突出，如Claude Code在文档和特性任务中领先，Cursor在修复任务表现最佳。

Conclusion: 不同的AI编码助手在不同任务类型上的表现存在显著差异，且没有单一助手在所有任务类型上都表现最佳。任务类型是影响PR接受率的主要因素，文档任务的接受率显著高于新功能任务。OpenAI Codex整体表现优异，但部分助手在特定任务上表现更好。

Abstract: The rapid adoption of AI-powered coding assistants is transforming software development practices, yet systematic comparisons of their effectiveness across different task types and over time remain limited. This paper presents an empirical study comparing five popular agents (OpenAI Codex, GitHub Copilot, Devin, Cursor, and Claude Code), analyzing 7,156 pull requests (PRs) from the AIDev dataset. Temporal trend analysis reveals heterogeneous evolution patterns: Devin exhibits the only consistent positive trend in acceptance rate (+0.77% per week over 32 weeks), whereas other agents remain largely stable. Our analysis suggests that the PR task type is a dominant factor influencing acceptance rates: documentation tasks achieve 82.1% acceptance compared to 66.1% for new features - a 16 percentage point gap that exceeds typical inter-agent variance for most tasks. OpenAI Codex achieves consistently high acceptance rates across all nine task categories (59.6%-88.6%), with stratified Chi-square tests confirming statistically significant advantages over other agents in several task categories. However, no single agent performs best across all task types: Claude Code leads in documentation (92.3%) and features (72.6%), while Cursor excels in fix tasks (80.4%).

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [137] [Lemon Agent Technical Report](https://arxiv.org/abs/2602.07092)
*Haipeng Jiang,Kailong Ren,Zimo Yin,Zhetao Sun,Xin Gan,Guangyi Lv,Ming He,Peng Wang,Congli Yin,Hong Pan,Changwen Zhang,Shan Tong,Zhengyu Xu,Zeping Chen,Yubin Huangfu,Yanzhi Xu,Xing Su,Qin Feng,Dong An,Jianping Fan*

Main category: cs.MA

TL;DR: Lemon Agent基于新的AgentCortex框架，通过多层次调度和记忆机制，显著优化了复杂任务的处理效率和准确率，刷新了多项权威基准测试纪录。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的智能体系统在资源效率、上下文管理和多模态感知方面存在局限，亟需新方法提升复杂长任务的处理能力。

Method: 提出AgentCortex框架，采用分层自适应调度机制，结合三层渐进式上下文管理策略和自我进化记忆系统，支持多代理协同执行和工具并发调用。

Result: 系统在GAIA基准测试中达到91.36%的整体准确率，并在xbench-DeepSearch排行榜中获得77+的最高分。

Conclusion: Lemon Agent通过引入AgentCortex框架和层次化自适应调度机制，实现了资源利用和任务执行效率的优化，显著提升了复杂任务的处理能力和准确率。

Abstract: Recent advanced LLM-powered agent systems have exhibited their remarkable capabilities in tackling complex, long-horizon tasks. Nevertheless, they still suffer from inherent limitations in resource efficiency, context management, and multimodal perception. Based on these observations, Lemon Agent is introduced, a multi-agent orchestrator-worker system built on a newly proposed AgentCortex framework, which formalizes the classic Planner-Executor-Memory paradigm through an adaptive task execution mechanism. Our system integrates a hierarchical self-adaptive scheduling mechanism that operates at both the overall orchestrator layer and workers layer. This mechanism can dynamically adjust computational intensity based on task complexity. It enables orchestrator to allocate one or more workers for parallel subtask execution, while workers can further improve operational efficiency by invoking tools concurrently. By virtue of this two-tier architecture, the system achieves synergistic balance between global task coordination and local task execution, thereby optimizing resource utilization and task processing efficiency in complex scenarios. To reduce context redundancy and increase information density during parallel steps, we adopt a three-tier progressive context management strategy. To make fuller use of historical information, we propose a self-evolving memory system, which can extract multi-dimensional valid information from all historical experiences to assist in completing similar tasks. Furthermore, we provide an enhanced MCP toolset. Empirical evaluations on authoritative benchmarks demonstrate that our Lemon Agent can achieve a state-of-the-art 91.36% overall accuracy on GAIA and secures the top position on the xbench-DeepSearch leaderboard with a score of 77+.

</details>


### [138] [The Value of Variance: Mitigating Debate Collapse in Multi-Agent Systems via Uncertainty-Driven Policy Optimization](https://arxiv.org/abs/2602.07186)
*Luoxi Tang,Yuqiao Meng,Joseph Costa,Yingxue Zhang,Muchao Ye,Zhaohan Xi*

Main category: cs.MA

TL;DR: 本文针对多智能体辩论系统存在的失败检测不足问题，提出了层次化不确定性量化指标和基于不确定性的策略优化方法，从而有效提升系统的决策质量和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体辩论系统容易出现辩论崩溃，导致最终决策基于错误推理，缺乏有效的失败检测和预防机制。

Method: 设计了一个三级行为不确定性度量指标从个体、交互到系统层面量化不确定性，基于此构建不确定性驱动的策略优化框架来惩罚自我矛盾、同辈冲突和低置信度输出。

Result: 实验证明提出的不确定性度量可靠指示系统失败，且不确定性驱动的策略优化显著提升了系统的决策准确性和一致性。

Conclusion: 提出的层次化不确定性度量指标有效识别多智能体辩论系统的失败，且基于不确定性的策略优化可以显著提升系统决策准确率并减少分歧。

Abstract: Multi-agent debate (MAD) systems improve LLM reasoning through iterative deliberation, but remain vulnerable to debate collapse, a failure type where final agent decisions are compromised on erroneous reasoning. Existing methods lack principled mechanisms to detect or prevent such failures. To address this gap, we first propose a hierarchical metric that quantifies behavioral uncertainty at three levels: intra-agent (individual reasoning uncertainty), inter-agent (interactive uncertainty), and system-level (output uncertainty). Empirical analysis across several benchmarks reveals that our proposed uncertainty quantification reliably indicates system failures, which demonstrates the validity of using them as diagnostic metrics to indicate the system failure. Subsequently, we propose a mitigation strategy by formulating an uncertainty-driven policy optimization to penalize self-contradiction, peer conflict, and low-confidence outputs in a dynamic debating environment. Experiments demonstrate that our proposed uncertainty-driven mitigation reliably calibrates the multi-agent system by consistently improving decision accuracy while reducing system disagreement.

</details>


### [139] [Talk, Judge, Cooperate: Gossip-Driven Indirect Reciprocity in Self-Interested LLM Agents](https://arxiv.org/abs/2602.07777)
*Shuhui Zhu,Yue Lin,Shriya Kaistha,Wenhao Li,Baoxiang Wang,Hongyuan Zha,Gillian K. Hadfield,Pascal Poupart*

Main category: cs.MA

TL;DR: 本研究提出ALIGN框架，通过代理间的去中心化八卦传播，有效增强了大语言模型代理中的间接互惠和防御恶意行为的能力，证明了利用推理提升合作的可行性。


<details>
  <summary>Details</summary>
Motivation: 在缺乏可靠信誉系统的去中心化自利大语言模型代理中，间接互惠难以维系，迫切需要一种机制来促进合作及抵抗恶意行为。

Method: 提出了Agentic Linguistic Gossip Network (ALIGN)框架，利用代理间共享开放式八卦信息和分层语调评估信誉、协调社会规范，从而促进信任和合作。

Result: ALIGN显著提升了间接互惠的效果，能够识别并排斥不合作或恶意代理，且更强的推理能力使代理的合作更符合激励机制，而聊天模型则往往过度合作。

Conclusion: ALIGN框架通过分层语调的传播策略，有效提升了间接互惠机制的实现，能够识别并排斥恶意个体，维持了去中心化大语言模型代理中的社会福利。

Abstract: Indirect reciprocity, which means helping those who help others, is difficult to sustain among decentralized, self-interested LLM agents without reliable reputation systems. We introduce Agentic Linguistic Gossip Network (ALIGN), an automated framework where agents strategically share open-ended gossip using hierarchical tones to evaluate trustworthiness and coordinate social norms. We demonstrate that ALIGN consistently improves indirect reciprocity and resists malicious entrants by identifying and ostracizing defectors without changing intrinsic incentives. Notably, we find that stronger reasoning capabilities in LLMs lead to more incentive-aligned cooperation, whereas chat models often over-cooperate even when strategically suboptimal. These results suggest that leveraging LLM reasoning through decentralized gossip is a promising path for maintaining social welfare in agentic ecosystems. Our code is available at https://github.com/shuhui-zhu/ALIGN.

</details>


### [140] [Altruism and Fair Objective in Mixed-Motive Markov games](https://arxiv.org/abs/2602.08389)
*Yao-hua Franck Xu,Tayeb Lemlouma,Arnaud Braud,Jean-Marie Bonnin*

Main category: cs.MA

TL;DR: 本文提出一种基于比例公平的多智能体合作框架，通过公平利他效用和公平马尔可夫博弈，解决了社交困境中的公平性问题，并设计公平的强化学习算法进行策略学习。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体合作方法以功利主义福利为目标，容易导致高效但极不公平的结果。

Method: 提出以比例公平为目标的公平利他效用函数，分析确保合作的条件，扩展到有序情景，构建公平马尔可夫博弈及公平Actor-Critic算法。

Result: 新方法在多个社交困境环境中实现了更公平的合作策略。

Conclusion: 通过引入比例公平替代功利主义指标，推动了多智能体合作中的公平性，解决了传统方法中效率与公平的矛盾。

Abstract: Cooperation is fundamental for society's viability, as it enables the emergence of structure within heterogeneous groups that seek collective well-being. However, individuals are inclined to defect in order to benefit from the group's cooperation without contributing the associated costs, thus leading to unfair situations. In game theory, social dilemmas entail this dichotomy between individual interest and collective outcome. The most dominant approach to multi-agent cooperation is the utilitarian welfare which can produce efficient highly inequitable outcomes. This paper proposes a novel framework to foster fairer cooperation by replacing the standard utilitarian objective with Proportional Fairness. We introduce a fair altruistic utility for each agent, defined on the individual log-payoff space and derive the analytical conditions required to ensure cooperation in classic social dilemmas. We then extend this framework to sequential settings by defining a Fair Markov Game and deriving novel fair Actor-Critic algorithms to learn fair policies. Finally, we evaluate our method in various social dilemma environments.

</details>


### [141] [EvoCorps: An Evolutionary Multi-Agent Framework for Depolarizing Online Discourse](https://arxiv.org/abs/2602.08529)
*Ning Lin,Haolun Li,Mingshu Liu,Chengyun Ruan,Kaibo Huang,Yukun Wei,Zhongliang Yang,Linna Zhou*

Main category: cs.MA

TL;DR: EvoCorps提出基于进化多智能体的主动去极化框架，动态适应敌对行为，通过闭环学习显著改善线上话语生态。


<details>
  <summary>Details</summary>
Motivation: 当前线上话语极化加剧社会信任流失和信息误导，但现有技术治理方法多为事后诊断，难以应对实时变化且协同的敌对放大行为。

Method: 提出了一个基于进化多智能体的框架，利用检索增强的集体认知核心和闭环进化学习，实现动态的多身份监控、规划、生成与扩散策略。

Result: 通过在MOSAIC社交AI模拟平台上的多源新闻流敌对注入实验，EvoCorps相较对抗基线在控制情绪极化、观点极端和提升论辩理性方面表现更优。

Conclusion: EvoCorps有效提升了线上话语环境的质量，通过主动干预减少了情绪极化和观点极端化，提高了论证的理性水平。

Abstract: Polarization in online discourse erodes social trust and accelerates misinformation, yet technical responses remain largely diagnostic and post-hoc. Current governance approaches suffer from inherent latency and static policies, struggling to counter coordinated adversarial amplification that evolves in real-time. We present EvoCorps, an evolutionary multi-agent framework for proactive depolarization. EvoCorps frames discourse governance as a dynamic social game and coordinates roles for monitoring, planning, grounded generation, and multi-identity diffusion. A retrieval-augmented collective cognition core provides factual grounding and action--outcome memory, while closed-loop evolutionary learning adapts strategies as the environment and attackers change. We implement EvoCorps on the MOSAIC social-AI simulation platform for controlled evaluation in a multi-source news stream with adversarial injection and amplification. Across emotional polarization, viewpoint extremity, and argumentative rationality, EvoCorps improves discourse outcomes over an adversarial baseline, pointing to a practical path from detection and post-hoc mitigation to in-process, closed-loop intervention. The code is available at https://github.com/ln2146/EvoCorps.

</details>


### [142] [ValueFlow: Measuring the Propagation of Value Perturbations in Multi-Agent LLM Systems](https://arxiv.org/abs/2602.08567)
*Jinnuo Liu,Chuke Liu,Hua Shen*

Main category: cs.MA

TL;DR: 本论文提出ValueFlow框架，通过扰动评估和指标分析，揭示多智能体语言模型中价值观漂移的传播机制及结构影响。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体大型语言模型系统中，价值观的相互影响和传递机制尚不清楚，缺乏有效的评估方法。

Method: 提出了ValueFlow框架，通过基于扰动的评估方法，利用56个价值维度数据集和LLM作为评判的协议，量化多智能体系统中的价值观漂移，并通过两个指标(beta-susceptibility和system susceptibility)分解分析价值漂移的个体行为和系统结构影响。

Result: 实验表明，不同价值观对扰动的敏感度差异显著，且系统的结构拓扑对价值观漂移有重要影响。

Conclusion: ValueFlow为多智能体语言模型中的价值观漂移提供了系统的测量与分析工具，有助于理解和控制价值观在复杂系统中的传播规律。

Abstract: Multi-agent large language model (LLM) systems increasingly consist of agents that observe and respond to one another's outputs. While value alignment is typically evaluated for isolated models, how value perturbations propagate through agent interactions remains poorly understood. We present ValueFlow, a perturbation-based evaluation framework for measuring and analyzing value drift in multi-agent systems. ValueFlow introduces a 56-value evaluation dataset derived from the Schwartz Value Survey and quantifies agents' value orientations during interaction using an LLM-as-a-judge protocol. Building on this measurement layer, ValueFlow decomposes value drift into agent-level response behavior and system-level structural effects, operationalized by two metrics: beta-susceptibility, which measures an agent's sensitivity to perturbed peer signals, and system susceptibility (SS), which captures how node-level perturbations affect final system outputs. Experiments across multiple model backbones, prompt personas, value dimensions, and network structures show that susceptibility varies widely across values and is strongly shaped by structural topology.

</details>


### [143] [Teaching an Old Dynamics New Tricks: Regularization-free Last-iterate Convergence in Zero-sum Games via BNN Dynamics](https://arxiv.org/abs/2602.08938)
*Tuo Zhang,Leonardo Stella*

Main category: cs.MA

TL;DR: 本文针对多智能体零和博弈中正则化调参难题，提出基于BNN动力学的无正则化收敛算法，具备适应环境非定常性的优势，实验证明效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体零和博弈方法依赖正则化且需调参，调参困难且面临环境非定常性问题，需寻找无需调参且能适应环境变化的算法。

Method: 将经典BNN动力学引入三角博弈，利用其固有收敛性，通过反事实加权扩展至扩展式博弈，同时结合神经网络函数近似实现高效算法。

Result: 理论证明BNN动力学在噪声正常形式博弈中最后迭代收敛，且扩展到扩展形式博弈；实验表明方案快速适应非定常环境，性能优于现有正则化方法。

Conclusion: 本文提出基于BNN动力学的多智能体零和博弈学习方法，实现了无正则化条件下的最后迭代收敛，且能很好适应不稳定环境，优于现有正则化方法。

Abstract: Zero-sum games are a fundamental setting for adversarial training and decision-making in multi-agent learning (MAL). Existing methods often ensure convergence to (approximate) Nash equilibria by introducing a form of regularization. Yet, regularization requires additional hyperparameters, which must be carefully tuned--a challenging task when the payoff structure is known, and considerably harder when the structure is unknown or subject to change. Motivated by this problem, we repurpose a classical model in evolutionary game theory, i.e., the Brown-von Neumann-Nash (BNN) dynamics, by leveraging the intrinsic convergence of this dynamics in zero-sum games without regularization, and provide last-iterate convergence guarantees in noisy normal-form games (NFGs). Importantly, to make this approach more applicable, we develop a novel framework with theoretical guarantees that integrates the BNN dynamics in extensive-form games (EFGs) through counterfactual weighting. Furthermore, we implement an algorithm that instantiates our framework with neural function approximation, enabling scalable learning in both NFGs and EFGs. Empirical results show that our method quickly adapts to nonstationarities, outperforming the state-of-the-art regularization-based approach.

</details>


### [144] [Learning to Coordinate via Quantum Entanglement in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.08965)
*John Gardiner,Orlando Romero,Brendan Tivnan,Nicolò Dal Fabbro,George J. Pappas*

Main category: cs.MA

TL;DR: 本文提出利用共享量子纠缠训练多智能体实现无通信协调的新方法，证明了量子优势在多智能体强化学习中的应用价值。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体强化学习中，通信困难限制了协调能力，而量子纠缠可能作为一种新的协调资源提升无通信策略的效果。

Method: 提出基于可微分策略参数化和新型策略架构，结合量子测量优化训练多智能体利用共享量子纠缠的框架。

Result: 验证了该方法在单轮游戏和去中心化部分可观测Markov决策过程中实现了量子优势的策略。

Conclusion: 该研究首次构建了利用共享量子纠缠实现多智能体无通信协调学习的框架，展示了量子资源在MARL中突破传统共享随机性限制的潜力。

Abstract: The inability to communicate poses a major challenge to coordination in multi-agent reinforcement learning (MARL). Prior work has explored correlating local policies via shared randomness, sometimes in the form of a correlation device, as a mechanism to assist in decentralized decision-making. In contrast, this work introduces the first framework for training MARL agents to exploit shared quantum entanglement as a coordination resource, which permits a larger class of communication-free correlated policies than shared randomness alone. This is motivated by well-known results in quantum physics which posit that, for certain single-round cooperative games with no communication, shared quantum entanglement enables strategies that outperform those that only use shared randomness. In such cases, we say that there is quantum advantage. Our framework is based on a novel differentiable policy parameterization that enables optimization over quantum measurements, together with a novel policy architecture that decomposes joint policies into a quantum coordinator and decentralized local actors. To illustrate the effectiveness of our proposed method, we first show that we can learn, purely from experience, strategies that attain quantum advantage in single-round games that are treated as black box oracles. We then demonstrate how our machinery can learn policies with quantum advantage in an illustrative multi-agent sequential decision-making problem formulated as a decentralized partially observable Markov decision process (Dec-POMDP).

</details>
