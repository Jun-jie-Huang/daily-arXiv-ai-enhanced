<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 45]
- [cs.SE](#cs.SE) [Total: 9]
- [cs.MA](#cs.MA) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [LLMs for Game Theory: Entropy-Guided In-Context Learning and Adaptive CoT Reasoning](https://arxiv.org/abs/2601.10775)
*Tommaso Felice Banfi,Sashenka Gamage*

Main category: cs.CL

TL;DR: 本文提出了一种基于大语言模型（LLM）的自适应推理框架，通过在井字游戏中的实验，展示了基于熵引导的多路径思考链（CoT）和上下文检索，显著提升了游戏决策质量。


<details>
  <summary>Details</summary>
Motivation: 标准LLM在离散博弈任务中推理时表现有限，需通过动态调整推理策略以应对不确定性，提升决策质量。

Method: 结合上下文学习、熵引导的多路径链式思考（CoT）和动态上下文检索，模型依据标记级不确定性调整检索示例数量和推理路径数。

Result: 在对阵次优算法对手的100局测试中，采用熵引导自适应推理的模型平均得分由-11.6%提升至+9.5%，同时保持较低的模型查询次数；统计和相关性分析均支持该提升。

Conclusion: 熵感知的自适应推理显著提升了LLM在游戏任务中的决策表现，游戏胜率由-11.6%提升至+9.5%，验证了不确定性引导推理的效果。

Abstract: We propose a novel LLM-based framework for reasoning in discrete, game-theoretic tasks, illustrated with \emph{Tic-Tac-Toe}. The method integrates in-context learning with entropy-guided chain-of-thought (CoT) reasoning and adaptive context retrieval. The model dynamically adjusts both the number of retrieved examples and reasoning paths according to token-level uncertainty: concise reasoning with minimal context is used when uncertainty is low, whereas higher uncertainty triggers expanded multi-path CoT exploration. Experimental evaluation against a sub-optimal algorithmic opponent shows that entropy-aware adaptive reasoning substantially improves decision quality, increasing the average game outcome from \(-11.6\%\) with the baseline LLM to \(+9.5\%\) with entropy-guided adaptive reasoning over 100 games (win = +1, tie = 0, loss = -1), while maintaining a relatively low number of LLM queries per game. Statistical validation confirms that the improvement is significant, and correlation analysis reveals a negative association between token-level entropy and move optimality. These findings demonstrate that uncertainty-guided adaptive reasoning effectively enhances LLM performance in sequential decision-making environments.

</details>


### [2] [BYOL: Bring Your Own Language Into LLMs](https://arxiv.org/abs/2601.10804)
*Syed Waqas Zamir,Wassim Hamidouche,Boulbaba Ben Amor,Luana Marotti,Inbal Becker-Reshef,Juan Lavista Ferres*

Main category: cs.CL

TL;DR: BYOL提出一种针对不同语言资源级别的多语言大模型架构，通过定制化的数据扩展和翻译介导途径，显著提升低资源及极低资源语言的模型表现。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型因全球语言资源分布极度不均，导致低资源语言表现不足，影响文化多样性和用户可访问性，迫切需要设计适应语言资源差异的定制化多语言模型开发策略。

Method: BYOL框架基于语言资源分级，对低资源语言采用数据清洗、数据扩展、持续预训练和监督微调；对极低资源语言采用基于机器翻译的间接路径，并通过模型权重空间融合保持多语言性能。

Result: 本文提出了BYOL框架，针对全球语言资源不平衡问题，分类语言资源并为不同资源级别语言设计对应的多语言大语言模型开发路径。对于低资源语言，BYOL采用数据清洗、合成文本生成、持续预训练和监督微调的全流程，显著提升了模型性能；对于极低资源语言，采用基于翻译的路径以绕过建模困难。实践中，应用于Chichewa、Maori和Inuktitut的模型在多个基准测试中均优于多语言基线。并公开了对应的数据和代码。

Conclusion: BYOL有效缓解了多语言大模型中的语言资源不平衡问题，显著提升了低资源及极低资源语言的模型性能，扩大了模型的语言覆盖和实用性。

Abstract: Large Language Models (LLMs) exhibit strong multilingual capabilities, yet remain fundamentally constrained by the severe imbalance in global language resources. While over 7,000 languages are spoken worldwide, only a small subset (fewer than 100) has sufficient digital presence to meaningfully influence modern LLM training. This disparity leads to systematic underperformance, cultural misalignment, and limited accessibility for speakers of low-resource and extreme-low-resource languages. To address this gap, we introduce Bring Your Own Language (BYOL), a unified framework for scalable, language-aware LLM development tailored to each language's digital footprint. BYOL begins with a language resource classification that maps languages into four tiers (Extreme-Low, Low, Mid, High) using curated web-scale corpora, and uses this classification to select the appropriate integration pathway. For low-resource languages, we propose a full-stack data refinement and expansion pipeline that combines corpus cleaning, synthetic text generation, continual pretraining, and supervised finetuning. Applied to Chichewa and Maori, this pipeline yields language-specific LLMs that achieve approximately 12 percent average improvement over strong multilingual baselines across 12 benchmarks, while preserving English and multilingual capabilities via weight-space model merging. For extreme-low-resource languages, we introduce a translation-mediated inclusion pathway, and show on Inuktitut that a tailored machine translation system improves over a commercial baseline by 4 BLEU, enabling high-accuracy LLM access when direct language modeling is infeasible. Finally, we release human-translated versions of the Global MMLU-Lite benchmark in Chichewa, Maori, and Inuktitut, and make our codebase and models publicly available at https://github.com/microsoft/byol .

</details>


### [3] [A Concise Agent is Less Expert: Revealing Side Effects of Using Style Features on Conversational Agents](https://arxiv.org/abs/2601.10809)
*Young-Min Cho,Yuan Yuan,Sharath Chandra Guntuku,Lyle Ungar*

Main category: cs.CL

TL;DR: 本文首次系统分析会话代理中风格控制的副作用，揭示风格特征间深度耦合及其对模型行为的复杂影响，提出CASSE数据集并评估缓解策略，强调未来需多目标和原则性的方法实现安全有效的风格引导。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型对话代理中普遍利用风格特征引导行为，但这些风格控制存在未知的副作用，影响模型行为的可控性和可解释性，亟需系统理解风格特征间的交互和副作用。

Method: 通过系统性地调查127篇会话代理相关论文，识别12种常用风格特征；构建受控合成对话，在任务导向和开放域场景中，使用成对LLM评判机制定量测评一个风格特征对其他特征的因果影响；引入CASSE数据集用于捕捉风格特征复杂交互；评估基于提示和激活调控的缓解策略。

Result: 发现风格特征之间存在显著且有结构性的副作用，如强调简洁性会降低专业性认知，表明风格特征深度耦合而非独立；提示和激活调控手段能部分缓解副作用，但往往损害主要风格特征的表现。

Conclusion: 风格特征在大语言模型对话控制中相互影响显著，现有方法难以实现单一风格精准控制，推动了对多目标、原则化风格引导方法的需求，促进对话代理的安全与可靠发展。

Abstract: Style features such as friendly, helpful, or concise are widely used in prompts to steer the behavior of Large Language Model (LLM) conversational agents, yet their unintended side effects remain poorly understood. In this work, we present the first systematic study of cross-feature stylistic side effects. We conduct a comprehensive survey of 127 conversational agent papers from ACL Anthology and identify 12 frequently used style features. Using controlled, synthetic dialogues across task-oriented and open domain settings, we quantify how prompting for one style feature causally affects others via a pairwise LLM as a Judge evaluation framework. Our results reveal consistent and structured side effects, such as prompting for conciseness significantly reduces perceived expertise. They demonstrate that style features are deeply entangled rather than orthogonal. To support future research, we introduce CASSE (Conversational Agent Stylistic Side Effects), a dataset capturing these complex interactions. We further evaluate prompt based and activation steering based mitigation strategies and find that while they can partially restore suppressed traits, they often degrade the primary intended style. These findings challenge the assumption of faithful style control in LLMs and highlight the need for multi-objective and more principled approaches to safe, targeted stylistic steering in conversational agents.

</details>


### [4] [Reasoning Models Generate Societies of Thought](https://arxiv.org/abs/2601.10825)
*Junsol Kim,Shiyang Lai,Nino Scherrer,Blaise Agüera y Arcas,James Evans*

Main category: cs.CL

TL;DR: 推理模型的提升源于多智能体式的内部思想社会，通过多元视角和辩论机制，实现更有效的复杂推理和更高准确度，类似于人类群体智慧。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型在复杂推理任务中表现出色的机制，特别是推理能力背后的内在机制为何尚不明确。

Method: 通过定量分析和机械可解释性方法，分析如DeepSeek-R1和QwQ-32B这类推理模型的推理轨迹，比较其内部认知视角的多样性与冲突表现；同时通过对模型进行强化学习实验以观察其会话行为和推理能力的提升。

Result: 发现推理模型通过模拟多智能体的交互（即思想社会），实现认知视角的多样化和内部辩论，表现出更广泛的个性和专业领域冲突，带来更准确的推理表现。多智能体结构促进了问答、视角转变和冲突观点调和，并体现了鲜明的社会情感角色。强化学习实验显示，仅以推理准确度为奖励，基础模型增强调用了更多会话行为；基于会话脚手架的微调更快提升推理能力。

Conclusion: 推理模型内部的社会化组织类似于人类集体智慧，能通过系统化结构利用多样性，实现更优的问题解决策略，提出了新的智能体组织方式以利用群体智慧的可能性。

Abstract: Large language models have achieved remarkable capabilities across domains, yet mechanisms underlying sophisticated reasoning remain elusive. Recent reasoning models outperform comparable instruction-tuned models on complex cognitive tasks, attributed to extended computation through longer chains of thought. Here we show that enhanced reasoning emerges not from extended computation alone, but from simulating multi-agent-like interactions -- a society of thought -- which enables diversification and debate among internal cognitive perspectives characterized by distinct personality traits and domain expertise. Through quantitative analysis and mechanistic interpretability methods applied to reasoning traces, we find that reasoning models like DeepSeek-R1 and QwQ-32B exhibit much greater perspective diversity than instruction-tuned models, activating broader conflict between heterogeneous personality- and expertise-related features during reasoning. This multi-agent structure manifests in conversational behaviors, including question-answering, perspective shifts, and the reconciliation of conflicting views, and in socio-emotional roles that characterize sharp back-and-forth conversations, together accounting for the accuracy advantage in reasoning tasks. Controlled reinforcement learning experiments reveal that base models increase conversational behaviors when rewarded solely for reasoning accuracy, and fine-tuning models with conversational scaffolding accelerates reasoning improvement over base models. These findings indicate that the social organization of thought enables effective exploration of solution spaces. We suggest that reasoning models establish a computational parallel to collective intelligence in human groups, where diversity enables superior problem-solving when systematically structured, which suggests new opportunities for agent organization to harness the wisdom of crowds.

</details>


### [5] [EncodeRec: An Embedding Backbone for Recommendation Systems](https://arxiv.org/abs/2601.10837)
*Guy Hadad,Neomi Rabaev,Bracha Shapira*

Main category: cs.CL

TL;DR: 本文提出了EncodeRec方法，通过在训练时保持预训练语言模型参数冻结，学习压缩且信息丰富的嵌入，以更好地适应推荐任务，显著提升了推荐系统性能。


<details>
  <summary>Details</summary>
Motivation: 当前利用预训练语言模型生成的嵌入缺乏结构性和判别力，且不能很好地捕捉领域特定语义，限制了推荐系统的性能。

Method: EncodeRec保持语言模型参数冻结，通过直接从物品描述中学习紧凑且信息丰富的嵌入，适配推荐系统的目标，同时提升表示的结构性和判别力。

Result: 在多个核心推荐基准测试中，EncodeRec不仅提升了顺序推荐模型的性能，还在语义ID标记化方面表现优异，显著优于其他基于PLM和嵌入模型的基线。

Conclusion: EncodeRec能有效对齐文本表示与推荐目标，实现压缩且具判别力的嵌入，提升推荐系统的效果，优于基于预训练语言模型和其他嵌入模型的基线方法。

Abstract: Recent recommender systems increasingly leverage embeddings from large pre-trained language models (PLMs). However, such embeddings exhibit two key limitations: (1) PLMs are not explicitly optimized to produce structured and discriminative embedding spaces, and (2) their representations remain overly generic, often failing to capture the domain-specific semantics crucial for recommendation tasks. We present EncodeRec, an approach designed to align textual representations with recommendation objectives while learning compact, informative embeddings directly from item descriptions. EncodeRec keeps the language model parameters frozen during recommender system training, making it computationally efficient without sacrificing semantic fidelity. Experiments across core recommendation benchmarks demonstrate its effectiveness both as a backbone for sequential recommendation models and for semantic ID tokenization, showing substantial gains over PLM-based and embedding model baselines. These results underscore the pivotal role of embedding adaptation in bridging the gap between general-purpose language models and practical recommender systems.

</details>


### [6] [DialDefer: A Framework for Detecting and Mitigating LLM Dialogic Deference](https://arxiv.org/abs/2601.10896)
*Parisa Rabbani,Priyam Sahoo,Ruben Mathew,Aishee Mondal,Harshita Ketharaman,Nimet Beyza Bozdag,Dilek Hakkani-Tür*

Main category: cs.CL

TL;DR: 大语言模型在评估对话中说话者的正确性时，因表达方式不同导致判断结果显著偏移，提出了DialDefer框架和指标检测该偏差，并指出缓解偏差需注意避免过度校准。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs作为第三方评判者在对话中广泛应用，但其评估说话者的稳定性和可靠性尚未明确，特别是判断是否受到表达框架影响，因此需要探讨并缓解这种潜在的判断偏差。

Method: 本文提出DialDefer框架，并设计对话式偏倚评分（DDS）来衡量不同呈现方式下模型判决的方向性变化，通过跨域、跨模型大规模实验验证该现象，并通过消融实验分析偏移来源。

Result: 研究发现大语言模型（LLMs）在评估对话中的说话者时，判断结果会因信息呈现的方式不同而有显著差异，即相同内容以验证陈述的形式与归因于特定说话者时，模型给出的判断不一致，称为对话式偏倚（dialogic deference）。本文提出了DialDefer框架与对话式偏倚评分（DDS），用于检测和缓解这种由框架引起的判断偏移。实验涵盖九个领域、3千多个实例和四款模型，结果显示框架对判断方向影响显著（DDS最大达87个百分点，p < .0001），而整体准确率变化极小（<2个百分点），且在自然语言社交媒体对话中该效应放大2-4倍。不同领域表现出不同倾向，有的模型偏向同意（顺从），有的偏向反对（怀疑）。消融实验指出，人类与LLM的归因差异是导致判断偏移的主要因素，模型更倾向于避免与人类的分歧。尝试缓解该偏差虽有效，但可能导致过度修正，表现为过度怀疑，提示这是一个超越准确率优化的校准问题。

Conclusion: 对话式偏倚显著影响LLMs的判断稳定性，归因方式是主要因素，缓解偏差是一个需要校准而非单纯优化准确率的问题。

Abstract: LLMs are increasingly used as third-party judges, yet their reliability when evaluating speakers in dialogue remains poorly understood. We show that LLMs judge identical claims differently depending on framing: the same content elicits different verdicts when presented as a statement to verify ("Is this statement correct?") versus attributed to a speaker ("Is this speaker correct?"). We call this dialogic deference and introduce DialDefer, a framework for detecting and mitigating these framing-induced judgment shifts. Our Dialogic Deference Score (DDS) captures directional shifts that aggregate accuracy obscures. Across nine domains, 3k+ instances, and four models, conversational framing induces large shifts (|DDS| up to 87pp, p < .0001) while accuracy remains stable (<2pp), with effects amplifying 2-4x on naturalistic Reddit conversations. Models can shift toward agreement (deference) or disagreement (skepticism) depending on domain -- the same model ranges from DDS = -53 on graduate-level science to +58 on social judgment. Ablations reveal that human-vs-LLM attribution drives the largest shifts (17.7pp swing), suggesting models treat disagreement with humans as more costly than with AI. Mitigation attempts reduce deference but can over-correct into skepticism, framing this as a calibration problem beyond accuracy optimization.

</details>


### [7] [Neural Induction of Finite-State Transducers](https://arxiv.org/abs/2601.10918)
*Michael Ginn,Alexis Palmer,Mans Hulden*

Main category: cs.CL

TL;DR: 本文提出了一种基于循环神经网络隐状态几何自动构建无权重有限状态转换器（FST）的方法，应用于形态变形、字母到音素转换和历史文本规范化，显著提升准确率。


<details>
  <summary>Details</summary>
Motivation: 手工构建有限状态转换器困难，而FST在字符串重写任务中效率高，故提出自动化构建方法。

Method: 利用循环神经网络学习的隐状态几何结构，自动构建无权重FST。

Result: 在形态变形、字母-音素预测和历史规范化任务上，该方法显著优于传统转导器学习算法。

Conclusion: 该方法能有效自动构建无权重FST，且在多个真实世界任务中表现优越，测试集准确率提升最高达87%。

Abstract: Finite-State Transducers (FSTs) are effective models for string-to-string rewriting tasks, often providing the efficiency necessary for high-performance applications, but constructing transducers by hand is difficult. In this work, we propose a novel method for automatically constructing unweighted FSTs following the hidden state geometry learned by a recurrent neural network. We evaluate our methods on real-world datasets for morphological inflection, grapheme-to-phoneme prediction, and historical normalization, showing that the constructed FSTs are highly accurate and robust for many datasets, substantially outperforming classical transducer learning algorithms by up to 87% accuracy on held-out test sets.

</details>


### [8] [Massively Multilingual Joint Segmentation and Glossing](https://arxiv.org/abs/2601.10925)
*Michael Ginn,Lindia Tjuatja,Enora Rice,Ali Marashian,Maria Valentini,Jasmine Xu,Graham Neubig,Alexis Palmer*

Main category: cs.CL

TL;DR: 本文提出了PolyGloss，一个联合预测形态分割和词素注释（gloss）序列到序列多语言模型，提升了注释的可解释性和准确性。


<details>
  <summary>Details</summary>
Motivation: 虽然现有的神经网络模型如GlossLM在词素注释评分上表现良好，但由于未预测词素边界导致可解释性差，难以获得语言学家对预测结果的信任，阻碍了其在真实语言文档中的应用。

Method: 通过扩展GlossLM训练语料，训练一系列序列到序列多语言模型，联合预测词素边界和对应注释，实验调优平衡两个任务的准确率与对齐效果，并应用低秩适配技术实现模型快速适应新数据。

Result: PolyGloss不仅在注释准确率上超越了GlossLM，同时在形态分割及两者的对齐评估中也表现更佳；此外，模型能快速适应新语料，显示出强大的实用潜力。

Conclusion: PolyGloss在词素注释和形态分割任务上优于现有模型GlossLM和多种开源大语言模型，并能通过低秩适配快速迁移到新数据集，增强了实际语言文档工作的应用价值。

Abstract: Automated interlinear gloss prediction with neural networks is a promising approach to accelerate language documentation efforts. However, while state-of-the-art models like GlossLM achieve high scores on glossing benchmarks, user studies with linguists have found critical barriers to the usefulness of such models in real-world scenarios. In particular, existing models typically generate morpheme-level glosses but assign them to whole words without predicting the actual morpheme boundaries, making the predictions less interpretable and thus untrustworthy to human annotators.
  We conduct the first study on neural models that jointly predict interlinear glosses and the corresponding morphological segmentation from raw text. We run experiments to determine the optimal way to train models that balance segmentation and glossing accuracy, as well as the alignment between the two tasks. We extend the training corpus of GlossLM and pretrain PolyGloss, a family of seq2seq multilingual models for joint segmentation and glossing that outperforms GlossLM on glossing and beats various open-source LLMs on segmentation, glossing, and alignment. In addition, we demonstrate that PolyGloss can be quickly adapted to a new dataset via low-rank adaptation.

</details>


### [9] [Selecting Language Models for Social Science: Start Small, Start Open, and Validate](https://arxiv.org/abs/2601.10926)
*Dustin S. Stoltz,Marshall A. Taylor,Sanuj Kumar*

Main category: cs.CL

TL;DR: 本文探讨了社会科学家如何基于有效性和可复制性等准则选择语言模型，建议采用小规模开放模型并构建限定基准以确保任务复现性。


<details>
  <summary>Details</summary>
Motivation: 当前有数千个大型预训练语言模型可供选择，社会科学家需要一套科学合理的模型选择标准。

Method: 从有效性、可靠性、可重复性与可复制性四个角度分析模型特性，并提出基于较小开放模型和有限基准的验证流程。

Result: 提出了选择模型时应关注模型开放性、模型规模、训练数据、模型架构与微调，强调了事后验证和可复制性的重要性。

Conclusion: 社会科学家在选择大预训练语言模型时，应更加注重可复制性，以确保任务的可靠复现。建议从较小且开放的模型入手，通过限定的基准测试验证整个计算流程的有效性。

Abstract: Currently, there are thousands of large pretrained language models (LLMs) available to social scientists. How do we select among them? Using validity, reliability, reproducibility, and replicability as guides, we explore the significance of: (1) model openness, (2) model footprint, (3) training data, and (4) model architectures and fine-tuning. While ex-ante tests of validity (i.e., benchmarks) are often privileged in these discussions, we argue that social scientists cannot altogether avoid validating computational measures (ex-post). Replicability, in particular, is a more pressing guide for selecting language models. Being able to reliably replicate a particular finding that entails the use of a language model necessitates reliably reproducing a task. To this end, we propose starting with smaller, open models, and constructing delimited benchmarks to demonstrate the validity of the entire computational pipeline.

</details>


### [10] [Multi-Stage Patient Role-Playing Framework for Realistic Clinical Interactions](https://arxiv.org/abs/2601.10951)
*Shijie Jiang,Zefan Zhang,Kehua Zhu,Tian Bai,Ruihong Zhao*

Main category: cs.CL

TL;DR: 本文提出了首个中文真实临床场景下的患者模拟数据集及多阶段角色扮演框架，显著提升模型模拟患者行为的个性化和真实性。


<details>
  <summary>Details</summary>
Motivation: 现有临床大语言模型及基准测试依赖于通用或LLM生成的对话数据，限制了医生与患者互动的真实性和多样性。

Method: 构建首个中文患者模拟数据集Ch-PatientSim，基于五维人格结构模拟患者，并通过少量样本生成进行数据增强。提出无训练的多阶段患者角色扮演（MSPRP）框架，通过分三阶段互动确保个性化与真实性。

Result: 实验表明，MSPRP方法显著提升了模型在多个患者模拟维度的表现。多数现有模型生成的回复过于正式，缺乏个性。

Conclusion: 基于真实临床场景构建的患者模拟数据集与分阶段角色扮演框架有效提升了临床大语言模型的患者行为模拟能力，促进医学诊断教育发展。

Abstract: The simulation of realistic clinical interactions plays a pivotal role in advancing clinical Large Language Models (LLMs) and supporting medical diagnostic education. Existing approaches and benchmarks rely on generic or LLM-generated dialogue data, which limits the authenticity and diversity of doctor-patient interactions. In this work, we propose the first Chinese patient simulation dataset (Ch-PatientSim), constructed from realistic clinical interaction scenarios to comprehensively evaluate the performance of models in emulating patient behavior. Patients are simulated based on a five-dimensional persona structure. To address issues of the persona class imbalance, a portion of the dataset is augmented using few-shot generation, followed by manual verification. We evaluate various state-of-the-art LLMs and find that most produce overly formal responses that lack individual personality. To address this limitation, we propose a training-free Multi-Stage Patient Role-Playing (MSPRP) framework, which decomposes interactions into three stages to ensure both personalization and realism in model responses. Experimental results demonstrate that our approach significantly improves model performance across multiple dimensions of patient simulation.

</details>


### [11] [Steering Language Models Before They Speak: Logit-Level Interventions](https://arxiv.org/abs/2601.10960)
*Hyeseon An,Shinwoo Park,Hyundong Jin,Yo-Sub Han*

Main category: cs.CL

TL;DR: 论文提出了一种基于统计logit调控的无训练推理时控制方法，解决了激活层访问难和提示调控不稳定的问题，实现了多任务大幅、稳定的生成控制。


<details>
  <summary>Details</summary>
Motivation: 针对当前主流的控制方法存在的问题，如激活层访问权限限制和提示工程控制不一致或不细粒度，提出新的控制策略。

Method: 提出了一种无须训练的推理时logit干预方法，实现可控生成。该方法利用基于标注语料的z-归一化对数几率统计代币得分表，调整解码分布。

Result: 在写作复杂度、正式度和有害内容三个多样化数据集上的实证评估表明，该方法有效地调控输出特征，实现任务无关的广泛应用，准确率提升47个百分点，F1提升50倍。

Conclusion: 统计学依据的logit调控技术在无需训练且推理时即可应用，显著提升了大语言模型多任务的可控生成性能，是一种通用、高效的文本生成控制手段。

Abstract: Steering LLMs is essential for specialized applications such as style-sensitive text rewriting, user-adaptive communication, and toxicity mitigation. Current steering methods, such as prompting-based and activation-based approaches, are widely used to guide model behavior. However, activation-based techniques require deep access to internal layers, while prompting-based steering often fails to provide consistent or fine-grained control. In order to address these limitations, we propose a training-free inference-time logit intervention for controllable generation. Our approach utilizes a statistical token score table derived from z-normalized log-odds of labeled corpora to shift the decoding distribution. Empirical evaluations across three diverse datasets focusing on writing complexity, formality, and toxicity demonstrate that our method effectively steers output characteristics, confirming its broad applicability and task-agnostic nature. Our results show that statistically grounded logit steering can achieve large, consistent, and multi-task control gains: up to +47%p accuracy and 50x f1 improvement.

</details>


### [12] [ZPD Detector: Data Selection via Capability-Difficulty Alignment for Large Language Models](https://arxiv.org/abs/2601.10986)
*Bo Yang,Yunkui Chen,Lanfei Feng,Yu Zhang,Shijian Li*

Main category: cs.CL

TL;DR: 本文提出了一种基于教育学近端发展区(ZPD)理论的数据选择框架ZPD Detector，通过动态匹配样本难度与模型能力，提升有限数据预算下的训练效率。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型训练成本攀升和高质量训练数据稀缺，如何在有限数据预算下选择高价值样本成为关键研究问题。

Method: 引入双向视角结合难度校准、基于项目反应理论(IRT)的模型能力估计及能力-难度匹配分数，从而动态选取最有信息量的训练样本。

Result: 实验表明，ZPD Detector能有效提升训练效率，并且动态匹配方法为训练策略设计带来新的启发。

Conclusion: ZPD Detector通过动态匹配样本难度和模型当前能力，显著提升了训练数据利用效率并为训练策略设计提供了新视角。

Abstract: As the cost of training large language models continues to increase and high-quality training data become increasingly scarce, selecting high-value samples or synthesizing effective training data under limited data budgets has emerged as a critical research problem. Most existing data selection methods rely on static criteria, such as difficulty, uncertainty, or heuristics, and fail to model the evolving relationship between the model and the data. Inspired by the educational theory of the Zone of Proximal Development (ZPD), we propose ZPD Detector, a data selection framework that adopts a bidirectional perspective between models and data by explicitly modeling the alignment between sample difficulty and the model's current capability. ZPD Detector integrates difficulty calibration, model capability estimation based on Item Response Theory (IRT), and a capability-difficulty matching score to dynamically identify the most informative samples at each learning stage, improving data utilization efficiency; moreover, this dynamic matching strategy provides new insights into training strategy design. All code and data will be released after our work be accepted to support reproducible researc

</details>


### [13] [When Personalization Misleads: Understanding and Mitigating Hallucinations in Personalized LLMs](https://arxiv.org/abs/2601.11000)
*Zhongxiang Sun,Yi Zhan,Chenglei Shen,Weijie Yu,Xiao Zhang,Ming He,Jun Xu*

Main category: cs.CL

TL;DR: 本研究发现个性化语言模型存在事实扭曲问题，提出FPPS方法有效缓解此问题并保持个性化表现，同时引入PFQABench评估基准，提高事实准确性。


<details>
  <summary>Details</summary>
Motivation: 个性化大型语言模型虽然提升了用户体验，但会导致模型生成与用户历史相关而非客观真实的回答，破坏事实准确性。针对这一矛盾，研究旨在保持个性化的同时解决事实扭曲问题。

Method: 提出了一个轻量级的推理时方法——事实保持的个性化引导（FPPS），专门用于减轻个性化引起的事实扭曲问题。同时引入了PFQABench基准，评估模型在事实性和个性化问答上的表现。

Result: 在多个LLM基础模型和个性化方法上，FPPS显著提升了事实准确性，同时保持了个性化的问答性能，显示了方法的有效性。

Conclusion: 个性化大型语言模型在增强用户满意度的同时，可能导致事实推理的扭曲，即个性化引起的幻觉现象，影响事实可靠性并传播错误信念。提出的FPPS方法在推理阶段减轻了这种扭曲，保持了个性化行为，同时提升了事实准确性。

Abstract: Personalized large language models (LLMs) adapt model behavior to individual users to enhance user satisfaction, yet personalization can inadvertently distort factual reasoning. We show that when personalized LLMs face factual queries, there exists a phenomenon where the model generates answers aligned with a user's prior history rather than the objective truth, resulting in personalization-induced hallucinations that degrade factual reliability and may propagate incorrect beliefs, due to representational entanglement between personalization and factual representations. To address this issue, we propose Factuality-Preserving Personalized Steering (FPPS), a lightweight inference-time approach that mitigates personalization-induced factual distortions while preserving personalized behavior. We further introduce PFQABench, the first benchmark designed to jointly evaluate factual and personalized question answering under personalization. Experiments across multiple LLM backbones and personalization methods show that FPPS substantially improves factual accuracy while maintaining personalized performance.

</details>


### [14] [Redefining Machine Simultaneous Interpretation: From Incremental Translation to Human-Like Strategies](https://arxiv.org/abs/2601.11002)
*Qianen Zhang,Zeyu Yang,Satoshi Nakamura*

Main category: cs.CL

TL;DR: 本文通过扩展同时机器翻译的动作空间，增加四种适应性动作，实现了更高质量和实时性的翻译。


<details>
  <summary>Details</summary>
Motivation: 传统的同时机器翻译因仅依赖READ/WRITE动作，在严格实时限制下难以兼顾翻译质量和速度，亟需通过丰富的动作集来改善实时结构调整、信息省略和简化，以提升语义保真度。

Method: 在大语言模型框架下引入Sentence_Cut, Drop, Partial_Summarization和Pronominalization四种动作，并通过动作感知提示构建训练参考，同时采用延迟感知的TTS管线评估文本输出的质量和单词级单调性。

Result: 在多个英语到汉语、德语及日语的基准测试中，该框架 consistently提升语义指标并减少延迟，尤其是Drop和Sentence_Cut的结合在保证流畅性的同时降低了翻译延迟。

Conclusion: 扩展动作空间并结合大语言模型显著提升了翻译语义质量及降低延迟，特别是Drop和Sentence_Cut动作的组合优化了流畅性与延迟的平衡，推动了机器翻译逼近人类水平。

Abstract: Simultaneous Machine Translation (SiMT) requires high-quality translations under strict real-time constraints, which traditional policies with only READ/WRITE actions cannot fully address. We extend the action space of SiMT with four adaptive actions: Sentence_Cut, Drop, Partial_Summarization and Pronominalization, which enable real-time restructuring, omission, and simplification while preserving semantic fidelity. We adapt these actions in a large language model (LLM) framework and construct training references through action-aware prompting. To evaluate both quality and word-level monotonicity, we further develop a latency-aware TTS pipeline that maps textual outputs to speech with realistic timing. Experiments on the ACL60/60 English-Chinese, English-German and English-Japanese benchmarks show that our framework consistently improves semantic metrics and achieves lower delay compared to reference translations and salami-based baselines. Notably, combining Drop and Sentence_Cut leads to consistent improvements in the balance between fluency and latency. These results demonstrate that enriching the action space of LLM-based SiMT provides a promising direction for bridging the gap between human and machine interpretation.

</details>


### [15] [NAACL: Noise-AwAre Verbal Confidence Calibration for LLMs in RAG Systems](https://arxiv.org/abs/2601.11004)
*Jiayu Liu,Rui Wang,Qing Zong,Qingcheng Zeng,Tianshi Zheng,Haochen Shi,Dadi Guo,Baixuan Xu,Chunyang Li,Yangqiu Song*

Main category: cs.CL

TL;DR: 本文针对大规模语言模型（LLMs）在检索增强生成（RAG）场景下置信度校准不足问题，提出了一种基于噪声感知规则的校准框架NAACL，显著提升了模型的置信度校准性能。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG方法因检索上下文存在噪声（矛盾或无关证据）导致模型置信度校准效果差，严重的过度自信限制了其在关键事实领域的部署。

Method: 提出NAACL Rules以处理噪声引起的过度自信问题，基于约2000个HotpotQA样本进行监督微调，使模型具备噪声感知能力，无需依赖更强的教师模型。

Result: NAACL方法在四个基准测试中提升了模型的置信度校准，域内ECE提升10.9%，域外提升8.0%，有效地弥合了检索噪声与语言模型置信度表达间的鸿沟。

Conclusion: 通过引入噪声感知置信度校准规则（NAACL Rules）和基于这些规则设计的监督精调框架NAACL，模型在面对噪声检索上下文时能更好地校准置信度，缓解过度自信问题，实验验证了方法的有效性。

Abstract: Accurately assessing model confidence is essential for deploying large language models (LLMs) in mission-critical factual domains. While retrieval-augmented generation (RAG) is widely adopted to improve grounding, confidence calibration in RAG settings remains poorly understood. We conduct a systematic study across four benchmarks, revealing that LLMs exhibit poor calibration performance due to noisy retrieved contexts. Specifically, contradictory or irrelevant evidence tends to inflate the model's false certainty, leading to severe overconfidence. To address this, we propose NAACL Rules (Noise-AwAre Confidence CaLibration Rules) to provide a principled foundation for resolving overconfidence under noise. We further design NAACL, a noise-aware calibration framework that synthesizes supervision from about 2K HotpotQA examples guided by these rules. By performing supervised fine-tuning (SFT) with this data, NAACL equips models with intrinsic noise awareness without relying on stronger teacher models. Empirical results show that NAACL yields substantial gains, improving ECE scores by 10.9% in-domain and 8.0% out-of-domain. By bridging the gap between retrieval noise and verbal calibration, NAACL paves the way for both accurate and epistemically reliable LLMs.

</details>


### [16] [Finding the Translation Switch: Discovering and Exploiting the Task-Initiation Features in LLMs](https://arxiv.org/abs/2601.11019)
*Xinwei Wu,Heng Liu,Xiaohu Zhao,Yuqi Ren,Linlong Xu,Longyue Wang,Deyi Xiong,Weihua Luo,Kaifu Zhang*

Main category: cs.CL

TL;DR: 本文通过稀疏自编码器识别出LLMs中的翻译启动特征，验证了其对翻译能力的关键作用，并基于此提出了提升微调效率和稳定性的样本选择策略。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型具备强大的翻译能力，但其内在机理尚不清晰。理解并利用这些内在机制有助于改进模型性能和微调效率。

Method: 使用稀疏自编码器回忆共激活特征，基于PCA一致性度量筛选功能相关特征，结合因果干预验证特征对翻译任务的影响，提出基于机制困难样本的微调数据选择策略。

Result: 本文提出了一种基于稀疏自编码器（SAEs）的新框架，用于识别大型语言模型（LLMs）内在的翻译启动特征。通过共激活特征回忆和PCA一致性度量筛选，成功隔离出翻译启动特征。因果干预实验表明，增强这些特征能够提升翻译准确性，消融则导致模型产生幻觉和偏离任务输出。基于此机制，提出了一种优先选择激活翻译启动特征较弱的“机制困难”样本进行微调的数据选择策略，显著提升了微调数据效率并抑制幻觉。该机制在同一系列更大模型中具有可迁移性。

Conclusion: 翻译启动特征是LLMs内在翻译能力的核心组成部分，利用此机制可以有效提升微调效率并减少幻觉，机制具备可迁移性。

Abstract: Large Language Models (LLMs) frequently exhibit strong translation abilities, even without task-specific fine-tuning. However, the internal mechanisms governing this innate capability remain largely opaque. To demystify this process, we leverage Sparse Autoencoders (SAEs) and introduce a novel framework for identifying task-specific features. Our method first recalls features that are frequently co-activated on translation inputs and then filters them for functional coherence using a PCA-based consistency metric. This framework successfully isolates a small set of **translation initiation** features. Causal interventions demonstrate that amplifying these features steers the model towards correct translation, while ablating them induces hallucinations and off-task outputs, confirming they represent a core component of the model's innate translation competency. Moving from analysis to application, we leverage this mechanistic insight to propose a new data selection strategy for efficient fine-tuning. Specifically, we prioritize training on **mechanistically hard** samples-those that fail to naturally activate the translation initiation features. Experiments show this approach significantly improves data efficiency and suppresses hallucinations. Furthermore, we find these mechanisms are transferable to larger models of the same family. Our work not only decodes a core component of the translation mechanism in LLMs but also provides a blueprint for using internal model mechanism to create more robust and efficient models. The codes are available at https://github.com/flamewei123/AAAI26-translation-Initiation-Features.

</details>


### [17] [From Interpretability to Performance: Optimizing Retrieval Heads for Long-Context Language Models](https://arxiv.org/abs/2601.11020)
*Youmi Ma,Naoaki Okazaki*

Main category: cs.CL

TL;DR: 通过屏蔽与对比检索头，提出RetMask方法有效提升LLM长上下文表现，验证了检索头的功能并实现性能增强。


<details>
  <summary>Details</summary>
Motivation: 虽然已知检索头对信息检索重要，但其在提升模型性能中的作用未被探索，本文探索利用检索头改进LLM长上下文能力的可能性。

Method: 提出RetMask方法：训练时对检索头进行屏蔽，生成对比信号，用于指导模型优化长上下文处理能力。

Result: 本文提出了RetMask方法，通过对比正常模型输出与屏蔽检索头后的变体输出，利用检索头提升大语言模型（LLM）长上下文处理能力。在128K上下文长度的HELMET测试中，RetMask使Llama-3.1性能提升2.28分，生成引用提高70%，段落重排序提升32%，且对一般任务无负面影响。实验还发现检索头的组织模式影响效果，集中型结构提升显著，分布型结构提升有限。

Conclusion: 检索头的机制被验证，其可被利用提升长上下文理解能力；通过RetMask方法显著改善LLM的相关任务表现，并保持一般任务性能。

Abstract: Advances in mechanistic interpretability have identified special attention heads, known as retrieval heads, that are responsible for retrieving information from the context. However, the role of these retrieval heads in improving model performance remains unexplored. This work investigates whether retrieval heads can be leveraged to enhance the long-context capabilities of LLMs. Specifically, we propose RetMask, a method that generates training signals by contrasting normal model outputs with those from an ablated variant in which the retrieval heads are masked. This mechanism-based approach achieves substantial improvements: +2.28 points on HELMET at 128K for Llama-3.1, with +70% gains on generation with citation and +32% on passage re-ranking, while preserving performance on general tasks. Experiments across three model families reveal that the effectiveness depends on retrieval head organization: models with concentrated patterns of retrieval heads respond strongly, while those with distributed patterns show limited gains. This mechanistic relationship validates the function of retrieval heads and demonstrates that mechanistic insights can be transformed into performance enhancements.

</details>


### [18] [Budget-Aware Anytime Reasoning with LLM-Synthesized Preference Data](https://arxiv.org/abs/2601.11038)
*Xuanming Zhang,Shwan Ashrafi,Aziza Mirsaidova,Amir Rezaeian,Miguel Ballesteros,Lydia B. Chilton,Zhou Yu,Dan Roth*

Main category: cs.CL

TL;DR: 本文针对有限计算预算下大语言模型推理，提出任意时刻推理框架和自我改进方法，提升推理效率和结果质量，实验证实其有效性。


<details>
  <summary>Details</summary>
Motivation: 在有限推理预算下，快速生成有用的部分解决方案更实用，而非耗费高成本的详尽推理，满足如行程规划等现实任务需求。

Method: 引入了任意时刻推理框架和任意时刻指数指标，衡量推理质量随推理令牌数变化的提升效率；并提出基于模型自我生成偏好数据的推理时自我改进方法，提升中间结果质量。

Result: 在多个数据集和多种大型语言模型上实验证明该方法能持续提升推理质量与效率，在预算限制下均获得稳健提升。

Conclusion: 本文提出了一种在有限计算预算下提升大语言模型推理效率和质量的方案，验证了在真实任务中提升部分解结果的有效性。

Abstract: We study the reasoning behavior of large language models (LLMs) under limited computation budgets. In such settings, producing useful partial solutions quickly is often more practical than exhaustive reasoning, which incurs high inference costs. Many real-world tasks, such as trip planning, require models to deliver the best possible output within a fixed reasoning budget. We introduce an anytime reasoning framework and the Anytime Index, a metric that quantifies how effectively solution quality improves as reasoning tokens increase. To further enhance efficiency, we propose an inference-time self-improvement method using LLM-synthesized preference data, where models learn from their own reasoning comparisons to produce better intermediate solutions. Experiments on NaturalPlan (Trip), AIME, and GPQA datasets show consistent gains across Grok-3, GPT-oss, GPT-4.1/4o, and LLaMA models, improving both reasoning quality and efficiency under budget constraints.

</details>


### [19] [Spectral Characterization and Mitigation of Sequential Knowledge Editing Collapse](https://arxiv.org/abs/2601.11042)
*Chi Zhang,Mengqi Zhang,Xiaotian Ye,Runxi Cheng,Zisheng Zhou,Ying Zhou,Pengjie Ren,Zhumin Chen*

Main category: cs.CL

TL;DR: 该工作通过谱分析揭示序列知识编辑导致模型能力崩溃的机制，提出REVIVE框架保护主奇异子空间，显著提升长序列编辑的稳定性与效果。


<details>
  <summary>Details</summary>
Motivation: 序列知识编辑在大型语言模型中常导致模型整体性能崩溃，现有方法虽尝试通过启发式限制参数更新缓解问题，但机制尚不清楚。

Method: 通过谱分析揭示模型通用能力与预训练权重矩阵的主奇异方向相关，这些方向对扰动高度敏感且受连续编辑干扰。提出REVIVE框架，通过在谱基表示参数更新，显式保护主奇异子空间，过滤干扰成分以稳定编辑过程。

Result: 在多模型和基准测试中，REVIVE显著提升编辑效果，同时在长序列（最多2万次）编辑中有效保持模型通用能力，表现优异。

Conclusion: REVIVE通过保护权重主奇异子空间，解决了序列知识编辑中模型性能崩溃的问题，实现了编辑效果与模型通用性能的有效平衡。

Abstract: Sequential knowledge editing in large language models often causes catastrophic collapse of the model's general abilities, especially for parameter-modifying methods. Existing approaches mitigate this issue through heuristic constraints on parameter updates, yet the mechanisms underlying such degradation remain insufficiently understood. In this work, we present a spectral analysis of sequential knowledge editing and show that a model's general abilities are closely associated with dominant singular directions of pretrained weight matrices. These directions are highly sensitive to perturbations and are progressively disrupted by repeated edits, closely tracking the collapse in both editing efficacy and general performance. Building on this insight, we propose REVIVE, a plug-and-play framework that stabilizes sequential editing by explicitly preserving the dominant singular subspace. REVIVE represents parameter updates in the spectral basis of the original weights and filters components that would interfere with the protected region. Extensive experiments across multiple models and benchmarks show that REVIVE consistently improves editing efficacy while substantially preserving general abilities under long-horizon sequential editing, including extreme settings with up to 20,000 edits.

</details>


### [20] [CoG: Controllable Graph Reasoning via Relational Blueprints and Failure-Aware Refinement over Knowledge Graphs](https://arxiv.org/abs/2601.11047)
*Yuanxiang Liu,Songze Li,Xiaoke Guo,Zhaoyan Gong,Qifei Zhang,Huajun Chen,Wen Zhang*

Main category: cs.CL

TL;DR: 提出了CoG框架，结合直觉和深思两种认知过程，通过关系蓝图引导快速稳定推理方向，并在推理停滞时进行反思和回溯，显著提升了LLM结合知识图谱的推理准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有KG增强LLM采用单一搜索策略，易受邻居噪声和结构错位影响导致推理停滞，缺乏认知多样性与灵活性。

Method: 引入了双过程理论，设计了关系蓝图引导模块（快速、直觉）和失败感知优化模块（谨慎、分析），实现稳定的结构约束和反思回溯机制。

Result: 在三个基准测试中，CoG在准确率和效率上均显著优于当前最先进方法。

Conclusion: CoG框架有效解决了现有KG增强LLM推理中的不稳定性和停滞问题，显著提升了性能。

Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning capabilities but often grapple with reliability challenges like hallucinations. While Knowledge Graphs (KGs) offer explicit grounding, existing paradigms of KG-augmented LLMs typically exhibit cognitive rigidity--applying homogeneous search strategies that render them vulnerable to instability under neighborhood noise and structural misalignment leading to reasoning stagnation. To address these challenges, we propose CoG, a training-free framework inspired by Dual-Process Theory that mimics the interplay between intuition and deliberation. First, functioning as the fast, intuitive process, the Relational Blueprint Guidance module leverages relational blueprints as interpretable soft structural constraints to rapidly stabilize the search direction against noise. Second, functioning as the prudent, analytical process, the Failure-Aware Refinement module intervenes upon encountering reasoning impasses. It triggers evidence-conditioned reflection and executes controlled backtracking to overcome reasoning stagnation. Experimental results on three benchmarks demonstrate that CoG significantly outperforms state-of-the-art approaches in both accuracy and efficiency.

</details>


### [21] [Efficient Multilingual Name Type Classification Using Convolutional Networks](https://arxiv.org/abs/2601.11090)
*Davor Lauc*

Main category: cs.CL

TL;DR: 提出了一种高效多语言命名实体分类的卷积神经网络Onomas-CNN X，准确率与大型预训练模型相当，但推理速度更快，能耗更低，适合CPU部署。


<details>
  <summary>Details</summary>
Motivation: 在保持准确率的前提下，实现命名实体分类的高效计算，降低能耗，提升推理速度，克服大型预训练模型计算资源消耗大的缺点。

Method: 提出Onomas-CNN X模型，结合并行卷积分支、深度可分离卷积操作和层级分类策略，在CPU上高效处理多语言命名实体。

Result: 该模型在104种语言和4种实体类别的多语言数据集上达到92.1%的准确率，推理速率达每秒2813个名字，速度比微调的XLM-RoBERTa快46倍，能耗降低46倍。

Conclusion: Onomas-CNN X模型在命名实体分类任务中，既保证了高准确率（92.1%），又大幅提升了推理速度和能效性，是专用CNN架构在多语言实体分类领域的有效解决方案。

Abstract: We present a convolutional neural network approach for classifying proper names by language and entity type. Our model, Onomas-CNN X, combines parallel convolution branches with depthwise-separable operations and hierarchical classification to process names efficiently on CPU hardware. We evaluate the architecture on a large multilingual dataset covering 104 languages and four entity types (person, organization, location, other). Onomas-CNN X achieves 92.1% accuracy while processing 2,813 names per second on a single CPU core - 46 times faster than fine-tuned XLM-RoBERTa with comparable accuracy. The model reduces energy consumption by a factor of 46 compared to transformer baselines. Our experiments demonstrate that specialized CNN architectures remain competitive with large pre-trained models for focused NLP tasks when sufficient training data exists.

</details>


### [22] [Integrity Shield A System for Ethical AI Use & Authorship Transparency in Assessments](https://arxiv.org/abs/2601.11093)
*Ashish Raj Shekhar,Shiven Agarwal,Priyanuj Bordoloi,Yash Shah,Tejas Anvekar,Vivek Gupta*

Main category: cs.CL

TL;DR: 针对LLMs直接完成考试的问题，Integrity Shield在考试PDF中嵌入隐形水印，有效防止模型回答并可检测作弊，保障考试公平。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型能够直接完成上传的考试PDF，导致学术诚信与成绩可靠性面临挑战，现有水印技术难以应用于黑盒模型。

Method: 提出了Integrity Shield，一种在考试PDF中嵌入项目级别水印的文档层水印系统，不改变PDF的视觉外观，并实现对水印的稳定编码和恢复。

Result: 在30份涵盖STEM、人文和医学的考试中，Integrity Shield在四个商业模型上实现了91-94%的考试阻止率和89-93%的签名检测率。

Conclusion: Integrity Shield能够有效阻止大型语言模型（LLMs）直接通过水印的考试PDF进行回答，保护学术诚信。

Abstract: Large Language Models (LLMs) can now solve entire exams directly from uploaded PDF assessments, raising urgent concerns about academic integrity and the reliability of grades and credentials. Existing watermarking techniques either operate at the token level or assume control over the model's decoding process, making them ineffective when students query proprietary black-box systems with instructor-provided documents. We present Integrity Shield, a document-layer watermarking system that embeds schema-aware, item-level watermarks into assessment PDFs while keeping their human-visible appearance unchanged. These watermarks consistently prevent MLLMs from answering shielded exam PDFs and encode stable, item-level signatures that can be reliably recovered from model or student responses. Across 30 exams spanning STEM, humanities, and medical reasoning, Integrity Shield achieves exceptionally high prevention (91-94% exam-level blocking) and strong detection reliability (89-93% signature retrieval) across four commercial MLLMs. Our demo showcases an interactive interface where instructors upload an exam, preview watermark behavior, and inspect pre/post AI performance & authorship evidence.

</details>


### [23] [The Growing Gains and Pains of Iterative Web Corpora Crawling: Insights from South Slavic CLASSLA-web 2.0 Corpora](https://arxiv.org/abs/2601.11170)
*Taja Kuzman Pungeršek,Peter Rupnik,Vít Suchomel,Nikola Ljubešić*

Main category: cs.CL

TL;DR: 通过国家顶级域名爬取，构建了更大规模的南斯拉夫语言网络语料库，但内容质量面临机器生成文本干扰问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决南斯拉夫语言低资源问题，通过顶级域名爬取构建大规模、动态更新的文本语料库，以支持相关语言研究和应用。

Method: 采用持续迭代的国家顶级域名爬取策略，自动注释文本题目标签，构建多语言大规模语料库。

Result: 本文介绍了通过国家顶级域名爬取获取南斯拉夫语言文本数据的方法，开发了CLASSLA-web 2.0语料库，包含七种语言共17亿词，3810万文本，且附带题目标签。与旧版相比，新语料库中仅有五分之一文本重合，显示出两年内内容的更新迭代。作者也指出随着爬取规模扩大，机器生成网站数量增加，导致语料质量有所下降。

Conclusion: 持续国家顶级域名爬取能够有效扩大低资源语言语料库规模，但需警惕机器生成内容对语料质量的负面影响。

Abstract: Crawling national top-level domains has proven to be highly effective for collecting texts in less-resourced languages. This approach has been recently used for South Slavic languages and resulted in the largest general corpora for this language group: the CLASSLA-web 1.0 corpora. Building on this success, we established a continuous crawling infrastructure for iterative national top-level domain crawling across South Slavic and related webs. We present the first outcome of this crawling infrastructure - the CLASSLA-web 2.0 corpus collection, with substantially larger web corpora containing 17.0 billion words in 38.1 million texts in seven languages: Bosnian, Bulgarian, Croatian, Macedonian, Montenegrin, Serbian, and Slovenian. In addition to genre categories, the new version is also automatically annotated with topic labels. Comparing CLASSLA-web 2.0 with its predecessor reveals that only one-fifth of the texts overlap, showing that re-crawling after just two years yields largely new content. However, while the new web crawls bring growing gains, we also notice growing pains - a manual inspection of top domains reveals a visible degradation of web content, as machine-generated sites now contribute a significant portion of texts.

</details>


### [24] [DOREMI: Optimizing Long Tail Predictions in Document-Level Relation Extraction](https://arxiv.org/abs/2601.11190)
*Laura Menotti,Stefano Marchesin,Gianmaria Silvello*

Main category: cs.CL

TL;DR: 本文提出DOREMI，通过少量人工注释和样本选择，提升了文档级关系抽取中罕见关系的性能。


<details>
  <summary>Details</summary>
Motivation: 文档级关系抽取面临跨句子上下文依赖和关系类型长尾分布的挑战，许多关系缺乏充足的训练样本。

Method: 提出DOREMI框架，通过迭代选择最具信息量的样本并进行有针对性的少量人工标注，提升训练效率和鲁棒性。该框架可应用于任何现有文档级关系抽取模型。

Result: DOREMI有效缓解了长尾偏差，提高了罕见关系的泛化能力。

Conclusion: DOREMI为文档级关系抽取中长尾关系的提升提供了一个可扩展且高效的解决方案。

Abstract: Document-Level Relation Extraction (DocRE) presents significant challenges due to its reliance on cross-sentence context and the long-tail distribution of relation types, where many relations have scarce training examples. In this work, we introduce DOcument-level Relation Extraction optiMizing the long taIl (DOREMI), an iterative framework that enhances underrepresented relations through minimal yet targeted manual annotations. Unlike previous approaches that rely on large-scale noisy data or heuristic denoising, DOREMI actively selects the most informative examples to improve training efficiency and robustness. DOREMI can be applied to any existing DocRE model and is effective at mitigating long-tail biases, offering a scalable solution to improve generalization on rare relations.

</details>


### [25] [T$^\star$: Progressive Block Scaling for MDM Through Trajectory Aware RL](https://arxiv.org/abs/2601.11214)
*Hanchen Xia,Baoyou Chen,Yutang Ge,Guojiang Zhao,Siyu Zhu*

Main category: cs.CL

TL;DR: 本文提出的T$^\star$训练方法，使得掩码扩散语言模型块大小逐步增大，实现高并行解码且性能损失极小，提升了数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 提高掩码扩散语言模型（MDMs）在数学推理基准上的解码效率和性能。

Method: 提出基于TraceRL的训练课程T$^\star$，通过从自回归初始化的小块MDM逐渐过渡到更大块，实现块大小的渐进式扩展。

Result: T$^\star$实现了更大块尺寸下的高并行解码，同时性能衰减最小。此外，T$^\star$可以收敛到替代解码调度策略，达到相当的性能水平。

Conclusion: 通过渐进式块大小训练课程，MDMs能在保持性能的同时实现更高的解码并行度，提升了数学推理任务的效率。

Abstract: We present T$^\star$, a simple \textsc{TraceRL}-based training curriculum for progressive block-size scaling in masked diffusion language models (MDMs). Starting from an AR-initialized small-block MDM, T$^\star$~transitions smoothly to larger blocks, enabling higher-parallelism decoding with minimal performance degradation on math reasoning benchmarks. Moreover, further analysis suggests that T$^\star$~can converge to an alternative decoding schedule $\hat{\rm S}$ that achieves comparable performance.

</details>


### [26] [MultiCaption: Detecting disinformation using multilingual visual claims](https://arxiv.org/abs/2601.11220)
*Rafael Martins Frade,Rrubaa Panchendrarajan,Arkaitz Zubiaga*

Main category: cs.CL

TL;DR: 本文提出了一个名为MultiCaption的新数据集，用于检测视觉声明中的矛盾，覆盖64种语言，支持多模态多语言虚假信息检测。通过变换器架构和自然语言推理模型进行了实验，结果表明该任务比标准推理更具挑战性，且多语言训练效果显著。


<details>
  <summary>Details</summary>
Motivation: 当前自动化事实核查受限于缺乏涵盖多媒体和多语言的真实场景数据，难以有效应对在线虚假信息的复杂传播。

Method: 构建包含11,088个跨64种语言视觉声明的成对数据集，对是否矛盾进行了多策略标注；采用基于变换器的架构、自然语言推理模型和大规模语言模型进行全面实验，建立基线。

Result: MultiCaption数据集包含丰富的多模态和多语言信息，任务复杂度高；多语言训练和测试显著提升模型表现，无需依赖机器翻译。

Conclusion: MultiCaption数据集为多模态、多语言虚假信息检测提供了宝贵资源，实验验证了其挑战性与多语言训练的有效性，有助于推动未来自动化事实核查研究。

Abstract: Online disinformation poses an escalating threat to society, driven increasingly by the rapid spread of misleading content across both multimedia and multilingual platforms. While automated fact-checking methods have advanced in recent years, their effectiveness remains constrained by the scarcity of datasets that reflect these real-world complexities. To address this gap, we first present MultiCaption, a new dataset specifically designed for detecting contradictions in visual claims. Pairs of claims referring to the same image or video were labeled through multiple strategies to determine whether they contradict each other. The resulting dataset comprises 11,088 visual claims in 64 languages, offering a unique resource for building and evaluating misinformation-detection systems in truly multimodal and multilingual environments. We then provide comprehensive experiments using transformer-based architectures, natural language inference models, and large language models, establishing strong baselines for future research. The results show that MultiCaption is more challenging than standard NLI tasks, requiring task-specific finetuning for strong performance. Moreover, the gains from multilingual training and testing highlight the dataset's potential for building effective multilingual fact-checking pipelines without relying on machine translation.

</details>


### [27] [Language of Thought Shapes Output Diversity in Large Language Models](https://arxiv.org/abs/2601.11227)
*Shaoyang Xu,Wenxuan Zhang*

Main category: cs.CL

TL;DR: 本文提出通过切换大型语言模型的思考语言来提高输出多样性，实验证明非英语及多语言混合思考显著增强输出的多样性和文化覆盖度。


<details>
  <summary>Details</summary>
Motivation: 提升大型语言模型的输出多样性，支持多元化和创造性，探索思考语言作为多样性新来源的可能性。

Method: 采用语言循环采样策略，包括单语言采样和混合语言采样，控制输出语言为英语，评估不同思考语言对输出多样性的影响。

Result: 使用不同思考语言能够显著增加输出多样性，思考语言与英语越远，提升越大；多语言混合采样还能进一步提升多样性，且在实际应用中促进文化和价值观的多元覆盖。

Conclusion: 通过控制模型思考时使用的语言，可以显著提升大型语言模型的输出多样性，尤其是使用非英语语言或多语言混合思考时效果更好。

Abstract: Output diversity is crucial for Large Language Models as it underpins pluralism and creativity. In this work, we reveal that controlling the language used during model thinking-the language of thought-provides a novel and structural source of output diversity. Our preliminary study shows that different thinking languages occupy distinct regions in a model's thinking space. Based on this observation, we study two repeated sampling strategies under multilingual thinking-Single-Language Sampling and Mixed-Language Sampling-and conduct diversity evaluation on outputs that are controlled to be in English, regardless of the thinking language used. Across extensive experiments, we demonstrate that switching the thinking language from English to non-English languages consistently increases output diversity, with a clear and consistent positive correlation such that languages farther from English in the thinking space yield larger gains. We further show that aggregating samples across multiple thinking languages yields additional improvements through compositional effects, and that scaling sampling with linguistic heterogeneity expands the model's diversity ceiling. Finally, we show that these findings translate into practical benefits in pluralistic alignment scenarios, leading to broader coverage of cultural knowledge and value orientations in LLM outputs. Our code is publicly available at https://github.com/iNLP-Lab/Multilingual-LoT-Diversity.

</details>


### [28] [FactCorrector: A Graph-Inspired Approach to Long-Form Factuality Correction of Large Language Models](https://arxiv.org/abs/2601.11232)
*Javier Carnerero-Cano,Massimiliano Pronesti,Radu Marinescu,Tigran Tchrakian,James Barry,Jasmina Gajcin,Yufang Hou,Alessandra Pascale,Elizabeth Daly*

Main category: cs.CL

TL;DR: 提出了无需重新训练即可跨领域纠正大型语言模型事实错误的FactCorrector方法，并创建了新基准VELI5，实验验证其优越性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在知识密集型任务中常生成错误事实，现有方法难以高效纠正，迫切需要一种无需重训且跨领域适用的纠错方案。

Method: 采用结构化反馈信息对LLM生成的答案进行后验修正，无需重新训练模型，实现跨领域适应性。

Result: 本文提出了FactCorrector，一种基于结构化反馈的后验修正方法，用于纠正大型语言模型（LLMs）的事实错误。该方法不依赖重新训练，能够跨领域适应，并利用反馈信息生成更正答案。同时，作者构建了VELI5基准数据集，包含系统注入的事实错误及其正确答案，用以评估事实更正方法。实验结果显示，FactCorrector在多个数据集上显著提高了事实准确性，且保持了内容相关性，优于现有强基线方法。

Conclusion: FactCorrector能够有效提升大型语言模型的事实准确性，且保持回答的相关性，表现优于当前主流方法。

Abstract: Large language models (LLMs) are widely used in knowledge-intensive applications but often generate factually incorrect responses. A promising approach to rectify these flaws is correcting LLMs using feedback. Therefore, in this paper, we introduce FactCorrector, a new post-hoc correction method that adapts across domains without retraining and leverages structured feedback about the factuality of the original response to generate a correction. To support rigorous evaluations of factuality correction methods, we also develop the VELI5 benchmark, a novel dataset containing systematically injected factual errors and ground-truth corrections. Experiments on VELI5 and several popular long-form factuality datasets show that the FactCorrector approach significantly improves factual precision while preserving relevance, outperforming strong baselines. We release our code at https://ibm.biz/factcorrector.

</details>


### [29] [How DDAIR you? Disambiguated Data Augmentation for Intent Recognition](https://arxiv.org/abs/2601.11234)
*Galo Castillo-López,Alexis Lombard,Nasredine Semmar,Gaël de Chalendar*

Main category: cs.CL

TL;DR: 本文提出了DDAIR方法，利用句子变换器检测并重新生成低资源环境下意图识别中的歧义合成样本，提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在数据增强时可能生成对某些未针对类别存在歧义的样本，影响意图识别的准确性。

Method: 采用句子变换器（Sentence Transformers）来检测由大型语言模型生成的合成样本中的歧义，再通过迭代再生成方法修正这些歧义样本。

Result: 实验表明，使用句子嵌入方法可以有效检测并减少含歧义的合成样本，提高了意图识别在定义宽泛或模糊场景下的分类效果。

Conclusion: 利用句子嵌入检测和迭代再生成歧义样本的方法能有效减少样本的模糊性，从而提升意图识别任务的分类准确率。

Abstract: Large Language Models (LLMs) are effective for data augmentation in classification tasks like intent detection. In some cases, they inadvertently produce examples that are ambiguous with regard to untargeted classes. We present DDAIR (Disambiguated Data Augmentation for Intent Recognition) to mitigate this problem. We use Sentence Transformers to detect ambiguous class-guided augmented examples generated by LLMs for intent recognition in low-resource scenarios. We identify synthetic examples that are semantically more similar to another intent than to their target one. We also provide an iterative re-generation method to mitigate such ambiguities. Our findings show that sentence embeddings effectively help to (re)generate less ambiguous examples, and suggest promising potential to improve classification performance in scenarios where intents are loosely or broadly defined.

</details>


### [30] [Reasoning in Trees: Improving Retrieval-Augmented Generation for Multi-Hop Question Answering](https://arxiv.org/abs/2601.11255)
*Yuling Shi,Maolin Sun,Zijun Liu,Mo Yang,Yixiong Fang,Tianran Sun,Xiaodong Gu*

Main category: cs.CL

TL;DR: RT-RAG创新采用推理树引导的分层检索生成方法，有效解决复杂多跳问答中的拆解错误和误差传播，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有多跳问答方法依赖LLM自我引导规划，存在查询拆解不准确和误差传播问题，导致推理连贯性差。

Method: 采用分层推理树指导的RAG框架，将多跳问题分解为明确的推理树，利用结构化实体分析和一致性树选择进行准确拆解，并通过自底向上的遍历策略进行迭代查询重写与证据收集。

Result: RT-RAG在多跳问答任务中相比最先进方法提升了7.0% F1和6.0% EM，显著提高了问答的准确率和推理效果。

Conclusion: RT-RAG通过分层推理树结构显著提升了复杂多跳问答的准确性和推理连贯性，较现有方法表现更优。

Abstract: Retrieval-Augmented Generation (RAG) has demonstrated significant effectiveness in enhancing large language models (LLMs) for complex multi-hop question answering (QA). For multi-hop QA tasks, current iterative approaches predominantly rely on LLMs to self-guide and plan multi-step exploration paths during retrieval, leading to substantial challenges in maintaining reasoning coherence across steps from inaccurate query decomposition and error propagation. To address these issues, we introduce Reasoning Tree Guided RAG (RT-RAG), a novel hierarchical framework for complex multi-hop QA. RT-RAG systematically decomposes multi-hop questions into explicit reasoning trees, minimizing inaccurate decomposition through structured entity analysis and consensus-based tree selection that clearly separates core queries, known entities, and unknown entities. Subsequently, a bottom-up traversal strategy employs iterative query rewriting and refinement to collect high-quality evidence, thereby mitigating error propagation. Comprehensive experiments show that RT-RAG substantially outperforms state-of-the-art methods by 7.0% F1 and 6.0% EM, demonstrating the effectiveness of RT-RAG in complex multi-hop QA.

</details>


### [31] [One LLM to Train Them All: Multi-Task Learning Framework for Fact-Checking](https://arxiv.org/abs/2601.11293)
*Malin Astrid Larsson,Harald Fosen Grunnaleite,Vinay Setty*

Main category: cs.CL

TL;DR: 本文通过多任务学习联合训练小型大语言模型，实现自动化事实核查三项任务的性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前大型闭源模型虽性能强，但复杂且成本高；单任务微调虽可行但效率低，需多模型支持。本文旨在通过多任务学习提升效率，减少模型数量和成本，实现统一且高效的自动化事实核查流水线。

Method: 采用多任务学习，联合微调单一小型解码器大语言模型，实现声明检测、证据排序和立场检测三项任务，比较分类头、因果语言模型头和指令调优三种策略，并跨模型规模和任务顺序进行评估。

Result: 本文提出利用多任务学习（MTL）策略，使用小型解码器大语言模型（如Qwen3-4b）联合完成声明检测、证据排序和立场检测任务，实现自动化事实核查。通过比较三种MTL策略（分类头、因果语言模型头、指令调优）及多种模型规模，模型顺序和传统基线，结果显示多任务模型在声明检测、证据重排序和立场检测三项任务上相较零/少样本设置分别获得44%、54%和31%的显著性能提升。文中还提供了实用经验指导，帮助实际应用MTL于事实核查。

Conclusion: 使用多任务学习策略可以显著提升小型大语言模型在自动化事实核查的声明检测、证据排序和立场检测任务表现，同时提供了实践指导。

Abstract: Large language models (LLMs) are reshaping automated fact-checking (AFC) by enabling unified, end-to-end verification pipelines rather than isolated components. While large proprietary models achieve strong performance, their closed weights, complexity, and high costs limit sustainability. Fine-tuning smaller open weight models for individual AFC tasks can help but requires multiple specialized models resulting in high costs. We propose \textbf{multi-task learning (MTL)} as a more efficient alternative that fine-tunes a single model to perform claim detection, evidence ranking, and stance detection jointly. Using small decoder-only LLMs (e.g., Qwen3-4b), we explore three MTL strategies: classification heads, causal language modeling heads, and instruction-tuning, and evaluate them across model sizes, task orders, and standard non-LLM baselines. While multitask models do not universally surpass single-task baselines, they yield substantial improvements, achieving up to \textbf{44\%}, \textbf{54\%}, and \textbf{31\%} relative gains for claim detection, evidence re-ranking, and stance detection, respectively, over zero-/few-shot settings. Finally, we also provide practical, empirically grounded guidelines to help practitioners apply MTL with LLMs for automated fact-checking.

</details>


### [32] [Membership Inference on LLMs in the Wild](https://arxiv.org/abs/2601.11314)
*Jiatong Yi,Yanyang Li*

Main category: cs.CL

TL;DR: 本文提出SimMIA，一种针对仅文本输入的黑盒环境的会员推断攻击方法，并构建WikiMIA-25基准数据集，实验证明其效果优异。


<details>
  <summary>Details</summary>
Motivation: 现有会员推断攻击方法依赖不可访问的模型内部信息或在严格黑盒环境下泛化性能差，难以应用于大型语言模型的训练数据审计。

Method: 提出SimMIA框架，采用先进的采样策略和评分机制，专门针对仅有生成文本的黑盒环境设计。

Result: SimMIA在黑盒环境中取得了最先进的结果，其性能媲美利用内部模型信息的基线方法。

Conclusion: SimMIA提供了一种有效的黑盒会员推断攻击解决方案，为大型语言模型训练数据的隐私审计提供了新工具。

Abstract: Membership Inference Attacks (MIAs) act as a crucial auditing tool for the opaque training data of Large Language Models (LLMs). However, existing techniques predominantly rely on inaccessible model internals (e.g., logits) or suffer from poor generalization across domains in strict black-box settings where only generated text is available. In this work, we propose SimMIA, a robust MIA framework tailored for this text-only regime by leveraging an advanced sampling strategy and scoring mechanism. Furthermore, we present WikiMIA-25, a new benchmark curated to evaluate MIA performance on modern proprietary LLMs. Experiments demonstrate that SimMIA achieves state-of-the-art results in the black-box setting, rivaling baselines that exploit internal model information.

</details>


### [33] [F-Actor: Controllable Conversational Behaviour in Full-Duplex Models](https://arxiv.org/abs/2601.11329)
*Maike Züfle,Ondrej Klejch,Nicholas Sanders,Jan Niehues,Alexandra Birch,Tsz Kin Lam*

Main category: cs.CL

TL;DR: 本文提出了一个基于冻结音频编码器和微调语言模型的全双工可控对话语音模型，支持多种对话行为的动态控制，实现了低资源高效训练，并公开了模型和代码。


<details>
  <summary>Details</summary>
Motivation: 现有语音对话系统缺乏动态上下文自适应的可控对话行为，限制了其自然性和可用性。

Method: 通过冻结音频编码器，仅微调语言模型，使用2000小时数据，无需大规模预训练或多阶段优化，采用单阶段训练方案。

Result: 模型能够根据明确指令控制说话者声音、话题、对话行为（如回应和打断）及对话启动，支持可控全双工语音对话，实现了高效且可复现的训练。

Conclusion: 本文提出了首个开放的、可指令控制的全双工对话语音模型，实现了在典型学术资源限制下的高效训练。

Abstract: Spoken conversational systems require more than accurate speech generation to have human-like conversations: to feel natural and engaging, they must produce conversational behaviour that adapts dynamically to the context. Current spoken conversational systems, however, rarely allow such customization, limiting their naturalness and usability. In this work, we present the first open, instruction-following full-duplex conversational speech model that can be trained efficiently under typical academic resource constraints. By keeping the audio encoder frozen and finetuning only the language model, our model requires just 2,000 hours of data, without relying on large-scale pretraining or multi-stage optimization. The model can follow explicit instructions to control speaker voice, conversation topic, conversational behaviour (e.g., backchanneling and interruptions), and dialogue initiation. We propose a single-stage training protocol and systematically analyze design choices. Both the model and training code will be released to enable reproducible research on controllable full-duplex speech systems.

</details>


### [34] [Idea First, Code Later: Disentangling Problem Solving from Code Generation in Evaluating LLMs for Competitive Programming](https://arxiv.org/abs/2601.11332)
*Sama Hadhoud,Alaa Elsetohy,Frederikus Hudi,Jan Christian Blaise Cruz,Steven Halim,Alham Fikri Aji*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）在竞赛编程中的表现，提出以自然语言解析作为解决方案生成与评估的核心。通过生成解析提升解决率，尤其是使用专家撰写的解析时效果显著。我们还发现，即使有金标准解析，模型在实现环节仍存在困难，并通过专家注释比较解析内容以诊断推理错误。本文发布了包含83个ICPC风格问题的带有金标准解析和完整测试的数据库，评估了19个LLMs，主张未来基准应明确区分问题解决与实现两个环节。


<details>
  <summary>Details</summary>
Motivation: 竞赛编程实质是解决问题的任务，而现有评测未能区分算法推理与代码实现，导致评价不准确。通过以自然语言解析为中心，可以更准确反映问题解决能力，提高模型的解决率及评估质量。

Method: 提出在生成代码之前先生成自然语言解析（editorial），并设计基于解析内容的评估方法。采用专家注释对生成解析与金标准解析进行比较，诊断模型推理错误。引入包含金标准解析和测试套件的ICPC风格问题数据集，评估19个大型语言模型的表现。

Result: 生成自然语言解析能提升部分LLMs的解题率，使用专家撰写的解析效果更佳。尽管如此，模型在代码实现阶段仍有较大困难，且生成解析与金标准解析的差距显示问题解决环节仍有瓶颈。同时，LLM作为评判者的评测协议经过验证，数据集和评测方法为未来研究提供基础。

Conclusion: 大型语言模型在竞赛编程中存在明显的算法推理与代码实现分离问题。以自然语言解析为中心的生成与评估方法能提升解决率，但实现仍是瓶颈。未来评测应区分问题解决与代码实现，推动更精确的能力评估。

Abstract: Large Language Models (LLMs) increasingly succeed on competitive programming problems, yet existing evaluations conflate algorithmic reasoning with code-level implementation. We argue that competitive programming is fundamentally a problem-solving task and propose centering natural-language editorials in both solution generation and evaluation. Generating an editorial prior to code improves solve rates for some LLMs, with substantially larger gains when using expertly written gold editorials. However, even with gold editorials, models continue to struggle with implementation, while the gap between generated and gold editorials reveals a persistent problem-solving bottleneck in specifying correct and complete algorithms. Beyond pass/fail metrics, we diagnose reasoning errors by comparing model-generated editorials to gold standards using expert annotations and validate an LLM-as-a-judge protocol for scalable evaluation. We introduce a dataset of 83 ICPC-style problems with gold editorials and full test suites, and evaluate 19 LLMs, arguing that future benchmarks should explicitly separate problem solving from implementation.

</details>


### [35] [Neural Chain-of-Thought Search: Searching the Optimal Reasoning Path to Enhance Large Language Models](https://arxiv.org/abs/2601.11340)
*Guoming Ling,Zhongzhan Huang,Yupei Lin,Junxin Li,Shanshan Zhong,Hefeng Wu,Liang Lin*

Main category: cs.CL

TL;DR: 本文提出NCoTS，通过动态搜索优越推理路径，改进Chain-of-Thought推理，显著提升准确性并减少冗余步骤。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的Chain-of-Thought推理过程是顺序生成，无远见，常陷入次优路径并包含冗余步骤。

Method: 提出Neural Chain-of-Thought Search (NCoTS)框架，将推理过程重构为动态搜索最优思考策略，通过定量分析解空间发现稀疏的优越推理路径，利用双因素启发式评价推理操作符，优化正确性与计算成本。

Result: NCoTS在多个推理基准测试中实现了帕累托改进，提升准确率3.5%以上，生成长度减少22%以上。

Conclusion: NCoTS有效探索最优推理策略，提升推理准确性和效率，为大语言模型推理能力提供新思路。

Abstract: Chain-of-Thought reasoning has significantly enhanced the problem-solving capabilities of Large Language Models. Unfortunately, current models generate reasoning steps sequentially without foresight, often becoming trapped in suboptimal reasoning paths with redundant steps. In contrast, we introduce Neural Chain-of-Thought Search (NCoTS), a framework that reformulates reasoning as a dynamic search for the optimal thinking strategy. By quantitatively characterizing the solution space, we reveal the existence of sparse superior reasoning paths that are simultaneously more accurate and concise than standard outputs. Our method actively navigates towards these paths by evaluating candidate reasoning operators using a dual-factor heuristic that optimizes for both correctness and computational cost. Consequently, NCoTS achieves a Pareto improvement across diverse reasoning benchmarks, boosting accuracy by over 3.5% while reducing generation length by over 22%. Our code and data are available at https://github.com/MilkThink-Lab/Neural-CoT-Search.

</details>


### [36] [How Much Would a Clinician Edit This Draft? Evaluating LLM Alignment for Patient Message Response Drafting](https://arxiv.org/abs/2601.11344)
*Parker Seegmiller,Joseph Gatto,Sarah E. Greer,Ganza Belise Isingizwe,Rohan Ray,Timothy E. Burdick,Sarah Masud Preum*

Main category: cs.CL

TL;DR: 本文通过构建主题分类和评估框架，系统评估了大型语言模型为患者消息草拟回复的效果，发现需要针对个别医生偏好进行适应才能更好服务临床工作流程。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs有潜力辅助临床医生草拟患者门户消息回复，但其实际能否减少医生工作负担和满足个别医生偏好仍存疑。故本研究旨在系统评估并改进LLMs与医生回复的一致性，推动其在临床工作流程中的有效整合。

Method: 本文构建了临床医生回复主题元素分类法，设计了内容和主题层面的编辑负载评估框架，发布了专家标注数据集，结合主题提示、检索增强生成、监督微调和直接偏好优化等多种模型适应方法，进行了大规模比较评估。

Result: 本文研究了大型语言模型（LLMs）在患者门户消息回复草拟中的应用，重点评估其与个别临床医生的匹配度。通过构建临床医生回复中的主题元素分类法，提出了一个用于评估LLM草稿编辑负载的新框架，并发布了专家标注的数据集。采用主题提示、检索增强生成、监督微调和偏好优化等多种适应技术，对本地和商业LLMs进行大规模评估。结果显示，LLMs在某些主题元素的草拟表现较好，但在与临床医生对齐生成中特别是提问环节存在较大不确定性。基于主题的适应策略能在大多数主题上带来改进，强调了针对个别临床医生偏好的模型适应重要性，以确保LLMs在患者-医生沟通中的可靠和负责任使用。

Conclusion: LLMs在患者消息回复中展现出潜力，但存在知识不确定性和部分主题对齐困难。主题驱动的适应策略显著提升了模型表现，强调根据临床医生偏好调整模型的重要性，以促进其在医疗沟通中的可靠应用。

Abstract: Large language models (LLMs) show promise in drafting responses to patient portal messages, yet their integration into clinical workflows raises various concerns, including whether they would actually save clinicians time and effort in their portal workload. We investigate LLM alignment with individual clinicians through a comprehensive evaluation of the patient message response drafting task. We develop a novel taxonomy of thematic elements in clinician responses and propose a novel evaluation framework for assessing clinician editing load of LLM-drafted responses at both content and theme levels. We release an expert-annotated dataset and conduct large-scale evaluations of local and commercial LLMs using various adaptation techniques including thematic prompting, retrieval-augmented generation, supervised fine-tuning, and direct preference optimization. Our results reveal substantial epistemic uncertainty in aligning LLM drafts with clinician responses. While LLMs demonstrate capability in drafting certain thematic elements, they struggle with clinician-aligned generation in other themes, particularly question asking to elicit further information from patients. Theme-driven adaptation strategies yield improvements across most themes. Our findings underscore the necessity of adapting LLMs to individual clinician preferences to enable reliable and responsible use in patient-clinician communication workflows.

</details>


### [37] [Reward Modeling for Scientific Writing Evaluation](https://arxiv.org/abs/2601.11374)
*Furkan Şahinuç,Subhabrata Dutta,Iryna Gurevych*

Main category: cs.CL

TL;DR: 提出一种两阶段训练的开源科学写作评价奖励模型，能够适应多变的评价标准和任务，提升科学写作自动评价的准确性和通用性。


<details>
  <summary>Details</summary>
Motivation: 科学写作评价挑战大，现有基于大模型的判定器和奖励模型多针对固定规则的通用任务，难以处理科学领域的稀疏知识和多维评价标准，同时为每个任务单独微调成本高且不现实。

Method: 提出了一个两阶段训练框架，首阶段优化科学评价偏好，次阶段提升推理能力，并通过多方面评价设计和多任务联合训练进行细粒度评估和动态评分标准的适应。

Result: 所提训练方案显著提升了基于大模型的科学写作评价，模型在多任务和之前未见的科学写作评价环境下表现良好，实现了无需针对任务重新训练的通用评估器。

Conclusion: 通过该训练框架，开发出高效、通用且开源的科学写作评价模型，解决了现有模型泛化差和任务依赖性的难题，适合低资源环境和多样化科学写作评价应用。

Abstract: Scientific writing is an expert-domain task that demands deep domain knowledge, task-specific requirements and reasoning capabilities that leverage the domain knowledge to satisfy the task specifications. While scientific text generation has been widely studied, its evaluation remains a challenging and open problem. It is critical to develop models that can be reliably deployed for evaluating diverse open-ended scientific writing tasks while adhering to their distinct requirements. However, existing LLM-based judges and reward models are primarily optimized for general-purpose benchmarks with fixed scoring rubrics and evaluation criteria. Consequently, they often fail to reason over sparse knowledge of scientific domains when interpreting task-dependent and multi-faceted criteria. Moreover, fine-tuning for each individual task is costly and impractical for low-resource settings. To bridge these gaps, we propose cost-efficient, open-source reward models tailored for scientific writing evaluation. We introduce a two-stage training framework that initially optimizes scientific evaluation preferences and then refines reasoning capabilities. Our multi-aspect evaluation design and joint training across diverse tasks enable fine-grained assessment and robustness to dynamic criteria and scoring rubrics. Experimental analysis shows that our training regime strongly improves LLM-based scientific writing evaluation. Our models generalize effectively across tasks and to previously unseen scientific writing evaluation settings, allowing a single trained evaluator to be reused without task-specific retraining.

</details>


### [38] [Evaluating LLM Behavior in Hiring: Implicit Weights, Fairness Across Groups, and Alignment with Human Preferences](https://arxiv.org/abs/2601.11379)
*Morgane Hoffmann,Emma Jouffroy,Warren Jouanneau,Marc Palyart,Charles Pebereau*

Main category: cs.CL

TL;DR: 本文通过经济学方法和合成数据，分析LLM在招聘中的决策逻辑，发现其权重分配在不同项目和人口子群体中存在差异，且整体歧视有限。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在招聘应用中如何赋予各属性重要性，及其是否符合经济原则、招聘者偏好和社会规范。

Method: 构建合成数据集，利用完整因子设计方法，结合经济学分析人类招聘行为的框架，分析LLM在评估自由职业者和项目匹配时对不同标准的权重分配。

Result: LLM优先考虑核心生产力信号如技能和经验，但对某些特征的解读超出明确匹配价值。整体歧视较少，但不同人口群体间信号权重存在差异。

Conclusion: LLM在招聘决策时主要重视技能和经验，但对部分特征的理解带有复杂性，且在人口交叉影响下权重分配不同，未来可以用类似实验验证模型与人类决策的一致性。

Abstract: General-purpose Large Language Models (LLMs) show significant potential in recruitment applications, where decisions require reasoning over unstructured text, balancing multiple criteria, and inferring fit and competence from indirect productivity signals. Yet, it is still uncertain how LLMs assign importance to each attribute and whether such assignments are in line with economic principles, recruiter preferences or broader societal norms. We propose a framework to evaluate an LLM's decision logic in recruitment, by drawing on established economic methodologies for analyzing human hiring behavior. We build synthetic datasets from real freelancer profiles and project descriptions from a major European online freelance marketplace and apply a full factorial design to estimate how a LLM weighs different match-relevant criteria when evaluating freelancer-project fit. We identify which attributes the LLM prioritizes and analyze how these weights vary across project contexts and demographic subgroups. Finally, we explain how a comparable experimental setup could be implemented with human recruiters to assess alignment between model and human decisions. Our findings reveal that the LLM weighs core productivity signals, such as skills and experience, but interprets certain features beyond their explicit matching value. While showing minimal average discrimination against minority groups, intersectional effects reveal that productivity signals carry different weights between demographic groups.

</details>


### [39] [Relational Linearity is a Predictor of Hallucinations](https://arxiv.org/abs/2601.11429)
*Yuetian Lu,Yihong Liu,Hinrich Schütze*

Main category: cs.CL

TL;DR: 模型在回答关于未知合成实体的问题时容易产生幻觉，且线性关系比非线性关系更易导致幻觉，关系的存储抽象程度影响模型识别幻觉的能力。


<details>
  <summary>Details</summary>
Motivation: 针对大型语言模型在回答关于未知实体的问题时常出现幻觉的现象，探究关系的存储方式是否影响模型的知识自我评估能力。

Method: 构建SyntHal合成数据集，采用四种模型评估六种关系的幻觉率，并用$Δ\cos$指标量化关系线性度，分析二者的相关性。

Result: 本文研究了大型语言模型（LLMs）中的错误认知（幻觉）现象，特别是在回答涉及合成实体的线性和非线性关系时出现的幻觉。通过设计包含6000个合成实体和6种关系的SyntHal数据集，发现关系的线性程度与幻觉率高度相关，线性关系更容易导致幻觉。作者提出关系的存储方式影响模型对知识的自我评估能力，这为控制幻觉行为和改进事实知识表示提供了新思路。

Conclusion: 关系的线性存储特性与模型产生幻觉的概率密切相关，改善事实知识的表示方法可以帮助减少幻觉现象。

Abstract: Hallucination is a central failure mode in large language models (LLMs). We focus on hallucinations of answers to questions like: "Which instrument did Glenn Gould play?", but we ask these questions for synthetic entities that are unknown to the model. Surprisingly, we find that medium-size models like Gemma-7B-IT frequently hallucinate, i.e., they have difficulty recognizing that the hallucinated fact is not part of their knowledge. We hypothesize that an important factor in causing these hallucinations is the linearity of the relation: linear relations tend to be stored more abstractly, making it difficult for the LLM to assess its knowledge; the facts of nonlinear relations tend to be stored more directly, making knowledge assessment easier. To investigate this hypothesis, we create SyntHal, a dataset of 6000 synthetic entities for six relations. In our experiments with four models, we determine, for each relation, the hallucination rate on SyntHal and also measure its linearity, using $Δ\cos$. We find a strong correlation ($r \in [.78,.82]$) between relational linearity and hallucination rate, providing evidence for our hypothesis that the underlying storage of triples of a relation is a factor in how well a model can self-assess its knowledge. This finding has implications for how to manage hallucination behavior and suggests new research directions for improving the representation of factual knowledge in LLMs.

</details>


### [40] [The unreasonable effectiveness of pattern matching](https://arxiv.org/abs/2601.11432)
*Gary Lupyan,Blaise Agüera y Arcas*

Main category: cs.CL

TL;DR: 大型语言模型具备通过模式匹配从无意义内容中提取语义的惊人能力，表明模式匹配是智能的重要组成部分。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型在极端输入条件下的理解能力，具体是针对内容词被随机替换为无意义字符串的语言，如"Jabberwocky"语言的理解性能。

Method: 通过实验将内容词随机替换为无意义字符串，测试大型语言模型恢复语义的能力。

Result: 发现大型语言模型能够从结构性模式中恢复出句子的实际含义，如将"He dwushed a ghanc zawk"正确翻译为"He dragged a spare chair"。

Conclusion: 模式匹配虽然非传统意义上的“真实”智能，但它是实现智能的关键因素，助力LLM理解复杂语言结构。

Abstract: We report on an astonishing ability of large language models (LLMs) to make sense of "Jabberwocky" language in which most or all content words have been randomly replaced by nonsense strings, e.g., translating "He dwushed a ghanc zawk" to "He dragged a spare chair". This result addresses ongoing controversies regarding how to best think of what LLMs are doing: are they a language mimic, a database, a blurry version of the Web? The ability of LLMs to recover meaning from structural patterns speaks to the unreasonable effectiveness of pattern-matching. Pattern-matching is not an alternative to "real" intelligence, but rather a key ingredient.

</details>


### [41] [Hierarchical Orthogonal Residual Spread for Precise Massive Editing in Large Language Models](https://arxiv.org/abs/2601.11441)
*Xiaojie Gu,Guangxu Chen,Yuheng Yang,Jingxin Han,Andi Zhang*

Main category: cs.CL

TL;DR: 提出HORSE方法，通过层次正交残差扩散提升模型编辑稳定性和精度。


<details>
  <summary>Details</summary>
Motivation: 现有模型编辑方法计算量大且易产生冲突，本文提出从新的角度通过优化信息矩阵的残差结构，提升编辑的稳定性和效率。

Method: 采用层次正交残差扩散（Hierarchical Orthogonal Residual Spread）技术对信息矩阵进行优化，相比传统的融合新旧知识的信息矩阵优化方法，降低噪声梯度，增强编辑稳定性。

Result: 该论文提出了一种新的模型编辑方法HORSE，通过层次正交残差扩散信息矩阵，降低了噪声梯度，实现了更稳定的模型编辑。通过理论比较和多模型、多数据集的实验验证，证明了HORSE方法在保持大规模精准编辑方面的优势。

Conclusion: HORSE方法能够在多种场景下对大型语言模型进行精准且稳定的编辑，表现优于现有方法。

Abstract: Large language models (LLMs) exhibit exceptional performance across various domains, yet they face critical safety concerns. Model editing has emerged as an effective approach to mitigate these issues. Existing model editing methods often focus on optimizing an information matrix that blends new and old knowledge. While effective, these approaches can be computationally expensive and may cause conflicts. In contrast, we shift our attention to Hierarchical Orthogonal Residual SprEad of the information matrix, which reduces noisy gradients and enables more stable edits from a different perspective. We demonstrate the effectiveness of our method HORSE through a clear theoretical comparison with several popular methods and extensive experiments conducted on two datasets across multiple LLMs. The results show that HORSE maintains precise massive editing across diverse scenarios. The code is available at https://github.com/XiaojieGu/HORSE

</details>


### [42] [Predict the Retrieval! Test time adaptation for Retrieval Augmented Generation](https://arxiv.org/abs/2601.11443)
*Xin Sun,Zhongqi Chen,Qiang Liu,Shu Wu,Bowen Song,Weiqiang Wang,Zilei Wang,Liang Wang*

Main category: cs.CL

TL;DR: 提出了一种推理时自适应的RAG系统参数更新方法TTARAG，显著提升了专业领域的问答性能。


<details>
  <summary>Details</summary>
Motivation: RAG系统在应用于专业领域时，因分布偏移导致泛化性能不足。

Method: 提出TTARAG，一种在推理时动态更新语言模型参数，通过让模型学习预测检索内容，实现对目标领域的自动调整。

Result: 在六个专业领域的广泛实验中，TTARAG相较于基线RAG系统表现出显著性能提升。

Conclusion: TTARAG有效缓解了分布偏移问题，提升了RAG系统在专业领域的问答能力。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful approach for enhancing large language models' question-answering capabilities through the integration of external knowledge. However, when adapting RAG systems to specialized domains, challenges arise from distribution shifts, resulting in suboptimal generalization performance. In this work, we propose TTARAG, a test-time adaptation method that dynamically updates the language model's parameters during inference to improve RAG system performance in specialized domains. Our method introduces a simple yet effective approach where the model learns to predict retrieved content, enabling automatic parameter adjustment to the target domain. Through extensive experiments across six specialized domains, we demonstrate that TTARAG achieves substantial performance improvements over baseline RAG systems. Code available at https://github.com/sunxin000/TTARAG.

</details>


### [43] [CTest-Metric: A Unified Framework to Assess Clinical Validity of Metrics for CT Report Generation](https://arxiv.org/abs/2601.11488)
*Vanshali Sharma,Andrea Mia Bejar,Gorkem Durak,Ulas Bagci*

Main category: cs.CL

TL;DR: 本文提出了一种统一框架，系统评估多种自动指标在CT放射报告生成中的临床适用性，发现GREEN评分表现最佳，提供代码和数据以促进后续研究。


<details>
  <summary>Details</summary>
Motivation: 当前放射报告生成依赖的评估指标次优，缺乏一致且全面的框架来评价指标的临床适用性和鲁棒性，推动领域指标研究受限。

Method: 提出CTest-Metric统一评估框架，包含写作风格通用性测试（WSG）、合成错误注入（SEI）和指标与专家相关性评估（MvE）三部分，评测8种主流自动评分指标在CT放射报告生成中的表现。

Result: 发现传统词汇级指标对风格变化敏感，GREEN评分与临床专家评价相关性最高（Spearman约0.70），CRG得分与专家评价负相关，BERTScore-F1对事实错误不敏感。

Conclusion: CTest-Metric框架有效评估了多个自动评价指标的临床实用性，揭示了各指标的优势与不足，为未来指标优化和临床应用提供参考和工具支持。

Abstract: In the generative AI era, where even critical medical tasks are increasingly automated, radiology report generation (RRG) continues to rely on suboptimal metrics for quality assessment. Developing domain-specific metrics has therefore been an active area of research, yet it remains challenging due to the lack of a unified, well-defined framework to assess their robustness and applicability in clinical contexts. To address this, we present CTest-Metric, a first unified metric assessment framework with three modules determining the clinical feasibility of metrics for CT RRG. The modules test: (i) Writing Style Generalizability (WSG) via LLM-based rephrasing; (ii) Synthetic Error Injection (SEI) at graded severities; and (iii) Metrics-vs-Expert correlation (MvE) using clinician ratings on 175 "disagreement" cases. Eight widely used metrics (BLEU, ROUGE, METEOR, BERTScore-F1, F1-RadGraph, RaTEScore, GREEN Score, CRG) are studied across seven LLMs built on a CT-CLIP encoder. Using our novel framework, we found that lexical NLG metrics are highly sensitive to stylistic variations; GREEN Score aligns best with expert judgments (Spearman~0.70), while CRG shows negative correlation; and BERTScore-F1 is least sensitive to factual error injection. We will release the framework, code, and allowable portion of the anonymized evaluation data (rephrased/error-injected CT reports), to facilitate reproducible benchmarking and future metric development.

</details>


### [44] [Do explanations generalize across large reasoning models?](https://arxiv.org/abs/2601.11517)
*Koyena Pal,David Bau,Chandan Singh*

Main category: cs.CL

TL;DR: 本文探讨了大型推理模型的链式思维解释是否能在不同模型间泛化，发现其确实提高模型间一致性，并提出提升一致性的方法。


<details>
  <summary>Details</summary>
Motivation: 当前尚不清楚大型推理模型生成的自然语言解释是否能泛化至其他模型，这对于理解问题本质和科学领域中的新概念发现至关重要，因此本文旨在研究解释的泛化能力及其对模型一致性的影响。

Method: 通过评估一种特定的泛化定义，即一个LRM生成的解释能否在其他LRM中引发相同行为，结合人类偏好排名和强化学习后训练，分析解释的一致性及影响因素，并提出句子级集成策略。

Result: 本文研究了大型推理模型（LRMs）生成的链式思维（CoT）解释的泛化能力，特别是这些解释能否在不同LRMs之间 induce 相同的行为。研究发现CoT解释通常能提高不同模型间的一致性，这种一致性与人类偏好和强化学习后训练结果相关。文章还分析了影响一致性的条件，并提出了句子层面的集成策略以提升一致性。总体上，结果提示在利用LRM解释产生新见解时需谨慎，并提供了评价LRM解释泛化的框架。

Conclusion: 链式思维解释在一定程度上具有跨模型泛化能力，能提高不同推理模型间的一致性，但使用这些解释时需谨慎，文章提出的句子级集成策略能进一步提升一致性。

Abstract: Large reasoning models (LRMs) produce a textual chain of thought (CoT) in the process of solving a problem, which serves as a potentially powerful tool to understand the problem by surfacing a human-readable, natural-language explanation. However, it is unclear whether these explanations generalize, i.e. whether they capture general patterns about the underlying problem rather than patterns which are esoteric to the LRM. This is a crucial question in understanding or discovering new concepts, e.g. in AI for science. We study this generalization question by evaluating a specific notion of generalizability: whether explanations produced by one LRM induce the same behavior when given to other LRMs. We find that CoT explanations often exhibit this form of generalization (i.e. they increase consistency between LRMs) and that this increased generalization is correlated with human preference rankings and post-training with reinforcement learning. We further analyze the conditions under which explanations yield consistent answers and propose a straightforward, sentence-level ensembling strategy that improves consistency. Taken together, these results prescribe caution when using LRM explanations to yield new insights and outline a framework for characterizing LRM explanation generalization.

</details>


### [45] [How Long Is a Piece of String? A Brief Empirical Analysis of Tokenizers](https://arxiv.org/abs/2601.11518)
*Jonathan Roberts,Kai Han,Samuel Albanie*

Main category: cs.CL

TL;DR: 本文通过实证分析揭示了不同文本分布下tokenization的差异，质疑了常用的token长度经验法则，提升了对大型语言模型中token使用的理解。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）广泛应用于学术、社会及工业领域，而常用比较模型输入输出及推理成本的单位是token。由于tokenization在不同模型和文本领域中差异显著，简单的token计数难以有效比较。

Method: 通过全面的实证分析，量化不同文本分布下序列压缩为token的情况，探讨tokenization的差异性。

Result: 发现现有关于token长度的常见经验法则过于简单，tokenization存在显著变化，难以简单套用统一标准。

Conclusion: 研究提供了对tokenization在当代大型语言模型中更清晰和直观的理解，有助于提升token使用的准确性和比较的合理性。

Abstract: Frontier LLMs are increasingly utilised across academia, society and industry. A commonly used unit for comparing models, their inputs and outputs, and estimating inference pricing is the token. In general, tokens are used as a stable currency, assumed to be broadly consistent across tokenizers and contexts, enabling direct comparisons. However, tokenization varies significantly across models and domains of text, making naive interpretation of token counts problematic. We quantify this variation by providing a comprehensive empirical analysis of tokenization, exploring the compression of sequences to tokens across different distributions of textual data. Our analysis challenges commonly held heuristics about token lengths, finding them to be overly simplistic. We hope the insights of our study add clarity and intuition toward tokenization in contemporary LLMs.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [46] [LogicLens: Leveraging Semantic Code Graph to explore Multi Repository large systems](https://arxiv.org/abs/2601.10773)
*Niko Usai,Dario Montagnini,Kristian Ilianov Iliev,Raffaele Camanzo*

Main category: cs.SE

TL;DR: LogicLens通过构建语义多仓库图，并结合自然语言交互，帮助开发者理解与调试复杂软件系统，展现出强大辅助能力。


<details>
  <summary>Details</summary>
Motivation: 理解分布在多个仓库和微服务中的大型软件系统非常困难，开发者需要同时理解代码结构、领域逻辑和运行时行为，这些通常隐式且分散。

Method: 提出LogicLens，一种基于语义多仓库图的反应式对话代理，通过结合AST解析和代码仓库遍历的语法分析与大语言模型的语义增强，构建捕获结构和功能抽象的图，然后通过自然语言与该图交互，动态获取子图并回答技术或功能查询。

Result: 该系统能有效帮助开发者探索复杂软件系统，具备影响分析和基于症状调试等新兴能力，在真实多仓库场景中表现出良好效果。

Conclusion: 基于语义多仓库图的对话代理能够提升开发者理解复杂软件系统的效率，支持结构与语义联合分析，促进技术和功能问题的交互式解答。

Abstract: Understanding large software systems is a challenging task, especially when code is distributed across multiple repositories and microservices. Developers often need to reason not only about the structure of the code, but also about its domain logic and runtime behaviors, which are typically implicit and scattered. We introduce LogicLens, a reactive conversational agent that assists developers in exploring complex software systems through a semantic multi-repository graph. This graph is built in a preprocessing step by combining syntactic code analysis, via AST parsing and repository traversal, with semantic enrichment using Large Language Models (LLMs). The resulting graph captures both structural elements, such as files, classes, and functions, as well as functional abstractions like domain entities, operations, and workflows. Once the graph is constructed, LogicLens enables developers to interact with it via natural language, dynamically retrieving relevant subgraphs and answering technical or functional queries. We present the architecture of the system, discuss emergent behaviors, and evaluate its effectiveness on real-world multi-repository scenarios. We demonstrate emergent capabilities including impact analysis and symptom-based debugging that arise naturally from the semantic graph structure.

</details>


### [47] [Multi-Artifact Analysis of Self-Admitted Technical Debt in Scientific Software](https://arxiv.org/abs/2601.10850)
*Eric L. Melin,Nasir U. Eisty,Gregory Watson,Addi Malviya-Thakur*

Main category: cs.SE

TL;DR: 本研究针对科学软件中的特殊自承技术债务——科学债务，进行了多源数据分析和专门分类器开发，表明需要独特的检测和管理方法以保障科学软件的质量和结果可靠性。


<details>
  <summary>Details</summary>
Motivation: 科学软件中的技术债务可能影响结果的有效性和可重复性，传统的SATD分类对科学债务的覆盖不足，需识别和分类这一专业领域的技术债务。

Method: 通过多源数据（代码注释、提交信息、合并请求和问题追踪器）分析23个开源科学软件项目，构建和验证了科学债务数据集，开发了多源SATD分类器，并进行了实践验证。

Result: 分类器在超过90万条多源数据上表现良好，传统SATD模型常忽视科学债务，多源数据分析揭示SATD在合并请求和问题追踪器中的高发性，实践验证表明科学债务识别有实际意义。

Conclusion: 科学债务是科学软件中特有的一种自承技术债务类型，传统的技术债务类别无法充分捕捉，需要专门的识别和管理方法。

Abstract: Context: Self-admitted technical debt (SATD) occurs when developers acknowledge shortcuts in code. In scientific software (SSW), such debt poses unique risks to the validity and reproducibility of results. Objective: This study aims to identify, categorize, and evaluate scientific debt, a specialized form of SATD in SSW, and assess the extent to which traditional SATD categories capture these domain-specific issues. Method: We conduct a multi-artifact analysis across code comments, commit messages, pull requests, and issue trackers from 23 open-source SSW projects. We construct and validate a curated dataset of scientific debt, develop a multi-source SATD classifier, and conduct a practitioner validation to assess the practical relevance of scientific debt. Results: Our classifier performs strongly across 900,358 artifacts from 23 SSW projects. SATD is most prevalent in pull requests and issue trackers, underscoring the value of multi-artifact analysis. Models trained on traditional SATD often miss scientific debt, emphasizing the need for its explicit detection in SSW. Practitioner validation confirmed that scientific debt is both recognizable and useful in practice. Conclusions: Scientific debt represents a unique form of SATD in SSW that that is not adequately captured by traditional categories and requires specialized identification and management. Our dataset, classification analysis, and practitioner validation results provide the first formal multi-artifact perspective on scientific debt, highlighting the need for tailored SATD detection approaches in SSW.

</details>


### [48] [Struggling to Connect: A Researchers' Reflection on Networking in Software Engineering](https://arxiv.org/abs/2601.10907)
*Shalini Chakraborty*

Main category: cs.SE

TL;DR: 本文探讨了影响软件工程研究人员建立专业网络的多种因素，如居住国、移民身份、语言、性别及工作环境，指出这些因素导致的网络机会不均，呼吁通过社区驱动的"专家声音"计划解决网络不平等问题。


<details>
  <summary>Details</summary>
Motivation: 网络对于软件工程研究和研究人员的成长至关重要，但网络机会分布不均，且个体环境和文化对网络形成有显著影响，促使作者探讨这些隐形障碍及解决方案。

Method: 通过梳理已有文献和结合作者个人经验，进行了反思性报告，分析影响研究人员建立网络的关键因素。

Result: 文章揭示了研究人员在建立专业联系过程中面临的各种社会和环境障碍，提出社区驱动"专家声音"倡议以促进网络公平与多样性。

Conclusion: 研究表明，研究人员的网络建设能力受多种社会和环境因素影响，这些隐性障碍需要通过集体努力和社区支持来克服，促进更公平的全球研究生态系统。

Abstract: Networking is central to the growth and visibility of software engineering research and researchers. However, opportunities and capacities to build such networks are not easily identified and often are unevenly distributed. While networking is often viewed as an individual skill, a researchers workplace, culture and environment significantly influence their motivation and, consequently, the networks they form. This paper explores how factors such as country of residence, immigration status, language, gender, and surrounding context affect researchers' ability to establish professional connections and succeed within the global research ecosystem. Drawing on existing literature and personal experience, this reflective report examines the often-invisible barriers to networking and advocates for a community-driven "expert voice" initiative to acknowledge and address these inequities.

</details>


### [49] [Change And Cover: Last-Mile, Pull Request-Based Regression Test Augmentation](https://arxiv.org/abs/2601.10942)
*Zitong Zhou,Matteo Paltenghi,Miryung Kim,Michael Pradel*

Main category: cs.SE

TL;DR: ChaCo是一种基于大语言模型的测试增强技术，针对PR未测试代码生成高质量补充测试，显著提升测试覆盖率并被开发者认可，展示了自动化“最后一公里”测试的潜力。


<details>
  <summary>Details</summary>
Motivation: 软件不断演进，开发者频繁提交拉取请求(PR)引入新功能或修复漏洞，测试PR对保持软件质量至关重要。然而，即使项目有丰富的测试套件，部分PR修改的代码行仍未被测试，存在“最后一公里”的回归测试缺口。

Method: 提出基于大语言模型(LLM)的测试增强技术ChaCo，针对PR未覆盖代码行，通过提取相关测试上下文（如测试函数、夹具、数据生成器）生成补充测试，并将生成的测试与现有测试套件结构和风格匹配，附带测试摘要供开发者审阅。

Result: 在SciPy、Qiskit和Pandas三个开源项目的145个PR上评估，ChaCo使30%的PR实现完整补丁覆盖，成本低（$0.11），人工评价测试值得添加、整合良好且与PR相关。提交12个测试，有8个被合入，暴露并修复2个未知缺陷。

Conclusion: ChaCo有效弥补了PR测试中的“最后一公里”覆盖空白，通过结合PR上下文的LLM测试生成技术提升测试覆盖率和开发者接受度，具有实际应用价值，适合集成入持续集成工作流实现测试自动化。

Abstract: Software is in constant evolution, with developers frequently submitting pull requests (PRs) to introduce new features or fix bugs. Testing PRs is critical to maintaining software quality. Yet, even in projects with extensive test suites, some PR-modified lines remain untested, leaving a "last-mile" regression test gap. Existing test generators typically aim to improve overall coverage, but do not specifically target the uncovered lines in PRs. We present Change And Cover (ChaCo), an LLM-based test augmentation technique that addresses this gap. It makes three contributions: (i) ChaCo considers the PR-specific patch coverage, offering developers augmented tests for code just when it is on the developers' mind. (ii) We identify providing suitable test context as a crucial challenge for an LLM to generate useful tests, and present two techniques to extract relevant test content, such as existing test functions, fixtures, and data generators. (iii) To make augmented tests acceptable for developers, ChaCo carefully integrates them into the existing test suite, e.g., by matching the test's structure and style with the existing tests, and generates a summary of the test addition for developer review. We evaluate ChaCo on 145 PRs from three popular and complex open-source projects - SciPy, Qiskit, and Pandas. The approach successfully helps 30% of PRs achieve full patch coverage, at the cost of $0.11, showing its effectiveness and practicality. Human reviewers find the tests to be worth adding (4.53/5.0), well integrated (4.2/5.0), and relevant to the PR (4.7/5.0). Ablations show test context is crucial for context-aware test generation, leading to 2x coverage. We submitted 12 tests, of which 8 have already been merged, and two previously unknown bugs were exposed and fixed. We envision our approach to be integrated into CI workflows, automating the last mile of regression test augmentation.

</details>


### [50] [ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development](https://arxiv.org/abs/2601.11077)
*Jie Yang,Honglin Guo,Li Ji,Jiazheng Zhou,Rui Zheng,Zhikai Lei,Shuo Zhang,Zhiheng Xi,Shichun Liu,Yuxin Wang,Bo Wang,Yining Zheng,Tao Gui,Xipeng Qiu*

Main category: cs.SE

TL;DR: 该论文推出了针对后端自动编码的ABC-Bench基准，覆盖真实工程全流程任务，并揭示当前大模型在此类复杂任务中表现不足。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成评测大多在静态环境中进行，忽视了后端开发中对环境配置和服务部署的动态全流程需求。

Method: 提出了ABC-Bench基准测试，涵盖224个实际任务，涉及8种语言和19个框架，通过自动化管道采集开源仓库数据，要求模型完成从代码库探索到容器化服务部署并通过API测试的全过程。

Result: 评测显示即使是最先进的大模型也难以在全面的后端开发任务中表现稳定，暴露出现有模型与实际工程需求之间的显著差距。

Conclusion: 当前的最先进大模型在处理包含完整开发生命周期的后端工程任务时仍表现不佳，存在明显能力差距。

Abstract: The evolution of Large Language Models (LLMs) into autonomous agents has expanded the scope of AI coding from localized code generation to complex, repository-level, and execution-driven problem solving. However, current benchmarks predominantly evaluate code logic in static contexts, neglecting the dynamic, full-process requirements of real-world engineering, particularly in backend development which demands rigorous environment configuration and service deployment. To address this gap, we introduce ABC-Bench, a benchmark explicitly designed to evaluate agentic backend coding within a realistic, executable workflow. Using a scalable automated pipeline, we curated 224 practical tasks spanning 8 languages and 19 frameworks from open-source repositories. Distinct from previous evaluations, ABC-Bench require the agents to manage the entire development lifecycle from repository exploration to instantiating containerized services and pass the external end-to-end API tests. Our extensive evaluation reveals that even state-of-the-art models struggle to deliver reliable performance on these holistic tasks, highlighting a substantial disparity between current model capabilities and the demands of practical backend engineering. Our code is available at https://github.com/OpenMOSS/ABC-Bench.

</details>


### [51] [Patterns of Bot Participation and Emotional Influence in Open-Source Development](https://arxiv.org/abs/2601.11138)
*Matteo Vaccargiu,Riccardo Lai,Maria Ilaria Lunesu,Andrea Pinna,Giuseppe Destefanis*

Main category: cs.SE

TL;DR: 在以太坊开源项目中，机器人虽少，但改变了开发者的交互节奏与情感氛围。


<details>
  <summary>Details</summary>
Motivation: 研究以太坊生态系统中机器人对开源讨论的贡献及其对开发者情感语调的影响。

Method: 分析涵盖36,875个账户和105个验证机器人数据集，使用27种情感类别的模型评估情感变化。

Result: 机器人以不同于人类的时序参与活动，机器人较中性，但其介入导致人类评论情感向感激、赞赏和乐观转变，混淆情绪减少。

Conclusion: 少数机器人显著影响了开发者沟通的时间安排和情感动态。

Abstract: We study how bots contribute to open-source discussions in the Ethereum ecosystem and whether they influence developers' emotional tone. Our dataset covers 36,875 accounts across ten repositories with 105 validated bots (0.28%). Human participation follows a U-shaped pattern, while bots engage in uniform (pull requests) or late-stage (issues) activity. Bots respond faster than humans in pull requests but play slower maintenance roles in issues. Using a model trained on 27 emotion categories, we find bots are more neutral, yet their interventions are followed by reduced neutrality in human comments, with shifts toward gratitude, admiration, and optimism and away from confusion. These findings indicate that even a small number of bots are associated with changes in both timing and emotional dynamics of developer communication.

</details>


### [52] [Automation and Reuse Practices in GitHub Actions Workflows: A Practitioner's Perspective](https://arxiv.org/abs/2601.11299)
*Hassan Onsori Delicheh,Guillaume Cardoen,Alexandre Decan,Tom Mens*

Main category: cs.SE

TL;DR: GitHub Actions自动化主要集中于核心任务，复用机制存在局限，开发者需更好工具支持和复用组件管理。


<details>
  <summary>Details</summary>
Motivation: GitHub Actions在工作流自动化中虽被广泛使用，但开发者在编写、测试、调试和维护工作流时面临困难，自动化和复用实践的相关知识较少。

Method: 通过调查419名GitHub Actions使用者，分析其自动化任务偏好、工作流创建方式、非功能需求及复用实践和挑战，得出相关观察和结论。

Result: 开发者主要将自动化用于核心CI/CD任务，安全分析及性能监控等重要领域较少自动化，复用Action被广泛依赖而复用工作流使用频率较低，存在版本控制和维护的挑战，常通过复制粘贴避免复用组件的复杂性。

Conclusion: 需要改进GitHub工作流自动化的工具支持、增强对各种自动化任务的支持及改进复用组件的发现、管理和信任机制，以减轻开发者的工作流维护负担。

Abstract: GitHub natively supports workflow automation through GitHub Actions. Yet, workflow maintenance is often considered a burden for software developers, who frequently face difficulties in writing, testing, debugging, and maintaining workflows. Little knowledge exists concerning the automation and reuse practices favoured by workflow practitioners. We therefore surveyed 419 practitioners to elucidate good and bad workflow development practices and to identify opportunities for supporting workflow maintenance. Specifically, we investigate the tasks that practitioners tend to automate using GitHub Actions, their preferred workflow creation mechanisms, and the non-functional characteristics they prioritise. We also examine the practices and challenges associated with GitHub's workflow reuse mechanisms. We observe a tendency to focus automation efforts on core CI/CD tasks, with less emphasis on crucial areas like security analysis and performance monitoring. Practitioners strongly rely on reusable Actions, but reusable workflows see less frequent adoption. Furthermore, we observed challenges with Action versioning and maintenance. Copy-pasting remains a common practice to have more control and avoid the complexity of depending on reusable components. These insights suggest the need for improved tooling, enhanced support for a wide range of automation tasks, and better mechanisms for discovering, managing, and trusting reusable workflow components.

</details>


### [53] [RITA: A Tool for Automated Requirements Classification and Specification from Online User Feedback](https://arxiv.org/abs/2601.11362)
*Manjeshwar Aniruddh Mallya,Alessio Ferrari,Mohammad Amin Zadenoori,Jacek Dąbrowski*

Main category: cs.SE

TL;DR: RITA整合了多项需求工程任务，通过大型语言模型自动处理用户反馈，提高了需求工程工具的实用性和集成度。


<details>
  <summary>Details</summary>
Motivation: 现有需求工程工具功能分散，缺乏端到端集成，难以应对海量且嘈杂的在线用户反馈，限制了其实际应用效果。

Method: RITA结合了轻量级开源大型语言模型，通过自动分类和识别算法，生成自然语言的需求规范，并通过用户界面和Jira集成实现工作流统一。

Result: RITA是一款集成了轻量级开源大型语言模型的反馈驱动需求工程工具，实现了从在线用户反馈到需求规范的自动转化。它支持自动请求分类、非功能需求识别和自然语言需求规范生成，并能无缝集成到Jira等开发工具中。

Conclusion: RITA有效利用大型语言模型技术，将原始用户反馈转换为结构化需求文档，促进了需求工程研究与实践的结合。

Abstract: Context and motivation. Online user feedback is a valuable resource for requirements engineering, but its volume and noise make analysis difficult. Existing tools support individual feedback analysis tasks, but their capabilities are rarely integrated into end-to-end support. Problem. The lack of end-to-end integration limits the practical adoption of existing RE tools and makes it difficult to assess their real-world usefulness. Solution. To address this challenge, we present RITA, a tool that integrates lightweight open-source large language models into a unified workflow for feedback-driven RE. RITA supports automated request classification, non-functional requirement identification, and natural-language requirements specification generation from online feedback via a user-friendly interface, and integrates with Jira for seamless transfer of requirements specifications to development tools. Results and conclusions. RITA exploits previously evaluated LLM-based RE techniques to efficiently transform raw user feedback into requirements artefacts, helping bridge the gap between research and practice. A demonstration is available at: https://youtu.be/8meCLpwQWV8.

</details>


### [54] [A Practical Guide to Establishing Technical Debt Management](https://arxiv.org/abs/2601.11430)
*Marion Wiese*

Main category: cs.SE

TL;DR: 本白皮书基于研究与实践，提出了团队层面的技术债务管理指南，强调灵活应用和团队协作，不涵盖全公司范围。


<details>
  <summary>Details</summary>
Motivation: 将科研成果转化为切实可行的技术债务管理指导，帮助不同团队根据自身需求有效管理技术债务。

Method: 通过与三家不同公司的团队合作，基于论文研究成果，结合实践经验，制定并调整技术债务管理系统，区分最佳实践和辅助措施，提供团队层面可定制的技术债务管理指引。

Result: 形成了一份指导团队建立技术债务管理体系的白皮书，包含经过验证的最佳实践及可选措施，强调团队自主设计流程。

Conclusion: 技术债务管理应以团队为单位灵活实施，结合最佳实践和团队实际需要制定流程，促进技术债务的有效控制。公司层面的管理建议在文末给出。

Abstract: This white paper provides an overview of the topic of "technical debt" and presents an approach for managing technical debt in teams. The white paper is based on the results of my dissertation, which aimed to translate scientific findings into practical guidance. To this end, I collaborated with other researchers to support three teams from different companies in adapting and establishing a technical debt management system tailored to their specific needs. Research findings were supplemented with details or additional approaches. Research results that were less practical were discarded. The result is a guide on establishing technical debt management within a team. The guide is intended to provide orientation and not be a rigid framework. We distinguish between "best practices" and "nice-to-haves." "Best practices" are understood to be all approaches that were adopted by all three teams. "Nice-to-haves" were used by at least one team. In many places, it is explicitly mentioned that the team should decide together how to design the process. This also applies, of course, to all areas where this was not explicitly mentioned. This white paper explicitly does not cover the establishment of technical debt management across the entire company, but provides suggestions for this at the end.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [55] [Cooperative UAVs for Remote Data Collection under Limited Communications: An Asynchronous Multiagent Learning Framework](https://arxiv.org/abs/2601.10849)
*Cuong Le,Symeon Chatzinotas,Thang X. Vu*

Main category: cs.MA

TL;DR: 本文针对多无人机异步协同数据采集，提出异步多智能体学习算法优化轨迹和带宽分配，实现能效和任务时间的双重提升。


<details>
  <summary>Details</summary>
Motivation: 解决多无人机协同数据采集中的能效优化问题，并针对动作同步不可实现的异步环境，设计适合的学习算法弥补现有方案不足。

Method: 将无人机轨迹规划问题建模为分散部分可观测半马尔可夫决策过程，提出异步多智能体学习算法实现无人机协作策略学习；轨迹策略学习后，基于各采集点的局部观测优化带宽分配。

Result: 所提方法在能效和任务完成时间方面优于现有学习及启发式基线，同时学习的策略在不同环境条件下展现出较强的鲁棒性。

Conclusion: 本文方法有效提升了多无人机协同采集的能效和稳定性，为异步环境下的轨迹规划和资源分配提供了新的解决思路。

Abstract: This paper addresses the joint optimization of trajectories and bandwidth allocation for multiple Unmanned Aerial Vehicles (UAVs) to enhance energy efficiency in the cooperative data collection problem. We focus on an important yet underestimated aspect of the system, where action synchronization across all UAVs is impossible. Since most existing learning-based solutions are not designed to learn in this asynchronous environment, we formulate the trajectory planning problem as a Decentralized Partially Observable Semi-Markov Decision Process and introduce an asynchronous multi-agent learning algorithm to learn UAVs' cooperative policies. Once the UAVs' trajectory policies are learned, the bandwidth allocation can be optimally solved based on local observations at each collection point. Comprehensive empirical results demonstrate the superiority of the proposed method over other learning-based and heuristic baselines in terms of both energy efficiency and mission completion time. Additionally, the learned policies exhibit robustness under varying environmental conditions.

</details>


### [56] [Can Small Agent Collaboration Beat a Single Big LLM?](https://arxiv.org/abs/2601.11327)
*Agata Żywot,Xinyi Chen,Maarten de Rijke*

Main category: cs.MA

TL;DR: 研究表明，工具增强是提升小模型性能的关键，4B工具增强模型能超越无工具的32B模型；而显式推理效果不稳定，完整推理反而可能损害性能。


<details>
  <summary>Details</summary>
Motivation: 探究在复杂任务基准GAIA中，较小模型通过工具增强能否弥补规模劣势，及不同推理方式与工具使用对性能的具体作用机制。

Method: 使用Qwen3不同规模模型结合Agentic-Reasoning框架，检测在无推理、仅规划、完整推理三种模式及多种工具（搜索、编程、思维导图）辅助下的性能表现，比较分析各因素影响。

Result: 该论文研究了小型、工具增强的智能代理是否能在GAIA基准测试中匹配或超越大型一体化模型的表现。通过在Agentic-Reasoning框架中使用Qwen3模型（4B-32B），分析了模型规模、显式推理（无推理、仅规划、完整推理）和工具使用（三种工具）的影响。结果显示，工具增强带来了最大且最稳定的性能提升，使用工具的小型4B模型甚至超过了未使用工具的32B大型模型。另一方面，显式推理的效果依赖于具体配置和任务难度，规划式推理有助于任务分解和约束跟踪，而完整推理往往导致性能下降，原因是工具协调不稳定，出现验证步骤遗漏、工具调用过多、循环不终止及格式漂移等问题。

Conclusion: 小型工具增强代理在GAIA基准上可超越大型单体模型，工具使用比扩展推理更为有效和稳定，完整推理易引发工具协调问题，需谨慎设计。

Abstract: This report studies whether small, tool-augmented agents can match or outperform larger monolithic models on the GAIA benchmark. Using Qwen3 models (4B-32B) within an adapted Agentic-Reasoning framework, we isolate the effects of model scale, explicit thinking (no thinking, planner-only, or full), and tool use (search, code, mind-map). Tool augmentation provides the largest and most consistent gains. Using tools, 4B models can outperform 32B models without tool access on GAIA in our experimental setup. In contrast, explicit thinking is highly configuration- and difficulty-dependent: planner-only thinking can improve decomposition and constraint tracking, while unrestricted full thinking often degrades performance by destabilizing tool orchestration, leading to skipped verification steps, excessive tool calls, non-termination, and output-format drift.

</details>
