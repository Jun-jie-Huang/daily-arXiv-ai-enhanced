<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 103]
- [cs.MA](#cs.MA) [Total: 9]
- [cs.SE](#cs.SE) [Total: 22]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Quantum NLP models on Natural Language Inference](https://arxiv.org/abs/2510.15972)
*Ling Sun,Peter Sullivan,Michael Martin,Yun Zhou*

Main category: cs.CL

TL;DR: 本文研究了量子自然语言处理（QNLP）在自然语言推理任务中的应用，比较了量子、混合和经典变压器模型的性能，结果显示量子模型在参数更少的情况下表现优异。


<details>
  <summary>Details</summary>
Motivation: 探索QNLP模型在低资源、结构敏感的自然语言推理任务上的表现及效率优势。

Method: 利用lambeq库和DisCoCat框架构建参数化量子电路，对句子对进行语义相关性和推理分类训练；引入信息增益每参数（IGPP）指标评估学习效率；提出基于词汇聚类的电路架构以促进参数共享。

Result: 量子模型在推理准确率和相关任务测试误差上优于随机初始化的变压器，且参数效率高出经典模型数万倍。

Conclusion: 量子自然语言处理展示了在低资源和结构敏感环境下的巨大潜力，通过参数共享的聚类架构进一步提升了泛化能力。

Abstract: Quantum natural language processing (QNLP) offers a novel approach to
semantic modeling by embedding compositional structure directly into quantum
circuits. This paper investigates the application of QNLP models to the task of
Natural Language Inference (NLI), comparing quantum, hybrid, and classical
transformer-based models under a constrained few-shot setting. Using the lambeq
library and the DisCoCat framework, we construct parameterized quantum circuits
for sentence pairs and train them for both semantic relatedness and inference
classification. To assess efficiency, we introduce a novel
information-theoretic metric, Information Gain per Parameter (IGPP), which
quantifies learning dynamics independent of model size. Our results demonstrate
that quantum models achieve performance comparable to classical baselines while
operating with dramatically fewer parameters. The Quantum-based models
outperform randomly initialized transformers in inference and achieve lower
test error on relatedness tasks. Moreover, quantum models exhibit significantly
higher per-parameter learning efficiency (up to five orders of magnitude more
than classical counterparts), highlighting the promise of QNLP in low-resource,
structure-sensitive settings. To address circuit-level isolation and promote
parameter sharing, we also propose a novel cluster-based architecture that
improves generalization by tying gate parameters to learned word clusters
rather than individual tokens.

</details>


### [2] [Fusion-Augmented Large Language Models: Boosting Diagnostic Trustworthiness via Model Consensus](https://arxiv.org/abs/2510.16057)
*Md Kamrul Siam,Md Jobair Hossain Faruk,Jerry Q. Cheng,Huanying Gu*

Main category: cs.CL

TL;DR: 本文提出了结合ChatGPT和Claude两种大语言模型的多模型融合框架，通过融合胸片单模态和图文多模态输入，显著提升了CheXpert数据集上的诊断准确率。


<details>
  <summary>Details</summary>
Motivation: 当前单一大语言模型在胸片诊断中存在准确度限制，且图像和文本信息的结合能提升诊断性能和可信度。

Method: 基于CheXpert数据集，分别使用ChatGPT和Claude进行图像单模态诊断，利用输出相似度进行共识融合，随后生成临床文本进行多模态诊断，并再次融合模型输出。

Result: 单模型准确率分别为62.8%和76.9%，通过共识融合提升至77.6%；多模态下准确率分别提高到84%和76%，共识融合准确率达到91.3%。

Conclusion: 结合多模态输入与多模型输出共识融合可有效提升胸片诊断的准确性和可信度，为临床AI辅助提供可靠且计算代价低的方案。

Abstract: This study presents a novel multi-model fusion framework leveraging two
state-of-the-art large language models (LLMs), ChatGPT and Claude, to enhance
the reliability of chest X-ray interpretation on the CheXpert dataset. From the
full CheXpert corpus of 224,316 chest radiographs, we randomly selected 234
radiologist-annotated studies to evaluate unimodal performance using image-only
prompts. In this setting, ChatGPT and Claude achieved diagnostic accuracies of
62.8% and 76.9%, respectively. A similarity-based consensus approach, using a
95% output similarity threshold, improved accuracy to 77.6%. To assess the
impact of multimodal inputs, we then generated synthetic clinical notes
following the MIMIC-CXR template and evaluated a separate subset of 50 randomly
selected cases paired with both images and synthetic text. On this multimodal
cohort, performance improved to 84% for ChatGPT and 76% for Claude, while
consensus accuracy reached 91.3%. Across both experimental conditions,
agreement-based fusion consistently outperformed individual models. These
findings highlight the utility of integrating complementary modalities and
using output-level consensus to improve the trustworthiness and clinical
utility of AI-assisted radiological diagnosis, offering a practical path to
reduce diagnostic errors with minimal computational overhead.

</details>


### [3] [Can LLMs Correct Themselves? A Benchmark of Self-Correction in LLMs](https://arxiv.org/abs/2510.16062)
*Guiyao Tie,Zenghui Yuan,Zeli Zhao,Chaoran Hu,Tianhe Gu,Ruihang Zhang,Sizhe Zhang,Junran Wu,Xiaoyue Tu,Ming Jin,Qingsong Wen,Lixing Chen,Pan Zhou,Lichao Sun*

Main category: cs.CL

TL;DR: 本文提出了一个名为CorrectBench的基准，用于评估大型语言模型的自我纠错方法在常识推理、数学推理和代码生成三项任务中的表现。研究发现自我纠错能提升准确率，混合方法效果更佳但效率降低，且复杂推理模型的自我纠错优化有限且成本高。简易的链式思维方法表现竞争力强。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型自我纠错是提升推理能力的关键，但现有自我纠错方法缺乏统一、全面的评估，且是否能真正有效自我纠错尚需验证。

Method: 提出CorrectBench基准，比较内在、自外部和微调等多种自我纠错策略在三种任务上的表现，系统评估不同方法的准确率和效率。

Result: 自我纠错方法能提高复杂推理任务准确率，混合策略进一步改善效果但降低效率，复杂推理模型优化效果有限且计算成本高，链式思维方法表现出较好准确率和效率。

Conclusion: 自我纠错对提升大型语言模型推理性能有潜力，但效率仍是挑战，建议未来研究聚焦推理能力与效率的平衡优化。

Abstract: Self-correction of large language models (LLMs) emerges as a critical
component for enhancing their reasoning performance. Although various
self-correction methods have been proposed, a comprehensive evaluation of these
methods remains largely unexplored, and the question of whether LLMs can truly
correct themselves is a matter of significant interest and concern. In this
study, we introduce CorrectBench, a benchmark developed to evaluate the
effectiveness of self-correction strategies, including intrinsic, external, and
fine-tuned approaches, across three tasks: commonsense reasoning, mathematical
reasoning, and code generation. Our findings reveal that: 1) Self-correction
methods can improve accuracy, especially for complex reasoning tasks; 2) Mixing
different self-correction strategies yields further improvements, though it
reduces efficiency; 3) Reasoning LLMs (e.g., DeepSeek-R1) have limited
optimization under additional self-correction methods and have high time costs.
Interestingly, a comparatively simple chain-of-thought (CoT) baseline
demonstrates competitive accuracy and efficiency. These results underscore the
potential of self-correction to enhance LLM's reasoning performance while
highlighting the ongoing challenge of improving their efficiency. Consequently,
we advocate for further research focused on optimizing the balance between
reasoning capabilities and operational efficiency. Project Page:
https://correctbench.github.io/

</details>


### [4] [EvolveR: Self-Evolving LLM Agents through an Experience-Driven Lifecycle](https://arxiv.org/abs/2510.16079)
*Rong Wu,Xiaoman Wang,Jianbiao Mei,Pinlong Cai,Daocheng Fu,Cheng Yang,Licheng Wen,Xuemeng Yang,Yufan Shen,Yuxin Wang,Botian Shi*

Main category: cs.CL

TL;DR: EvolveR框架通过闭环的自我进化生命周期，使大语言模型代理能够从自身经验中系统学习和迭代优化解决策略，提升了多跳问答任务的表现。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型代理虽然工具使用能力出色，但缺乏从自身经验系统学习并持续改进解决策略的能力。现有方法主要缓解外部知识差距，却未解决策略迭代优化的根本问题。

Method: 提出EvolveR，包含两个阶段：1）离线自我蒸馏，将代理交互轨迹抽象为结构化、可复用的策略原则；2）在线交互，代理在任务中主动检索策略指导决策，并通过策略强化机制迭代更新，多次循环实现持续优化。

Result: 在复杂的多跳问答基准测试中，EvolveR框架优于多种强基线，展现了其有效性和自我改进能力。

Conclusion: EvolveR为代理提供了一个能够从自身行为后果持续学习和自我提升的完整闭环框架，为开发更自主持续进化的系统奠定了基础。

Abstract: Current Large Language Model (LLM) agents show strong performance in tool
use, but lack the crucial capability to systematically learn from their own
experiences. While existing frameworks mainly focus on mitigating external
knowledge gaps, they fail to address a more fundamental limitation: the
inability to iteratively refine problem-solving strategies. In this work, we
introduce EvolveR, a framework designed to enable agent to self-improve through
a complete, closed-loop experience lifecycle. This lifecycle comprises two key
stages: (1) Offline Self-Distillation, where the agent's interaction
trajectories are synthesized into a structured repository of abstract, reusable
strategic principles; (2) Online Interaction, where the agent interacts with
tasks and actively retrieves distilled principles to guide its decision-making,
accumulating a diverse set of behavioral trajectories. This loop employs a
policy reinforcement mechanism to iteratively update the agent based on its
performance. We demonstrate the effectiveness of EvolveR on complex multi-hop
question-answering benchmarks, where it achieves superior performance over
strong agentic baselines. Our work presents a comprehensive blueprint for
agents that learn not only from external data but also from the consequences of
their own actions, paving the way for more autonomous and continuously
improving systems. Code is available at https://github.com/Edaizi/EvolveR.

</details>


### [5] [Evaluating Prompting Strategies and Large Language Models in Systematic Literature Review Screening: Relevance and Task-Stage Classification](https://arxiv.org/abs/2510.16091)
*Binglan Han,Anuradha Mathrani,Teo Susnjak*

Main category: cs.CL

TL;DR: 本文评估了六种大型语言模型在不同提示策略下的性能表现，以自动化系统文献综述的筛选阶段，提出分阶段的筛选流程以提升效率与性价比。


<details>
  <summary>Details</summary>
Motivation: 系统性文献综述筛选工作繁重且耗时，利用大型语言模型自动化筛选有助于提高效率和准确性，但不同模型与提示策略的组合效果差异显著，需系统性比较。

Method: 选取六款大型语言模型和五种不同提示策略，针对文献相关性分类和六个子任务，采用准确率、精确率、召回率和F1指标进行评估，分析模型与提示策略的交互影响，并进行成本效益分析。

Result: CoT-few-shot提示策略在精确率和召回率之间表现最平衡，zero-shot策略则在召回率上表现最佳；自我反思策略表现较差。GPT-4o和DeepSeek综合性能较好，GPT-4o-mini在成本效益上更有优势。基于成本效益分析，建议先用低成本模型加结构化提示进行初筛，再对边界案例使用高能力模型。

Conclusion: 大型语言模型在文献自动筛选中展现出不均但有潜力的应用价值，通过系统比较模型与提示策略的组合效果，提供了实用的部署建议，促进更灵活高效的文献筛选流程。

Abstract: This study quantifies how prompting strategies interact with large language
models (LLMs) to automate the screening stage of systematic literature reviews
(SLRs). We evaluate six LLMs (GPT-4o, GPT-4o-mini, DeepSeek-Chat-V3,
Gemini-2.5-Flash, Claude-3.5-Haiku, Llama-4-Maverick) under five prompt types
(zero-shot, few-shot, chain-of-thought (CoT), CoT-few-shot, self-reflection)
across relevance classification and six Level-2 tasks, using accuracy,
precision, recall, and F1. Results show pronounced model-prompt interaction
effects: CoT-few-shot yields the most reliable precision-recall balance;
zero-shot maximizes recall for high-sensitivity passes; and self-reflection
underperforms due to over-inclusivity and instability across models. GPT-4o and
DeepSeek provide robust overall performance, while GPT-4o-mini performs
competitively at a substantially lower dollar cost. A cost-performance analysis
for relevance classification (per 1,000 abstracts) reveals large absolute
differences among model-prompt pairings; GPT-4o-mini remains low-cost across
prompts, and structured prompts (CoT/CoT-few-shot) on GPT-4o-mini offer
attractive F1 at a small incremental cost. We recommend a staged workflow that
(1) deploys low-cost models with structured prompts for first-pass screening
and (2) escalates only borderline cases to higher-capacity models. These
findings highlight LLMs' uneven but promising potential to automate literature
screening. By systematically analyzing prompt-model interactions, we provide a
comparative benchmark and practical guidance for task-adaptive LLM deployment.

</details>


### [6] [Facts in Stats: Impacts of Pretraining Diversity on Language Model Generalization](https://arxiv.org/abs/2510.16096)
*Tina Behnia,Puneesh Deora,Christos Thrampoulidis*

Main category: cs.CL

TL;DR: 本文设计了一个合成测试平台，研究语言模型中统计模式与事实关联的多样性交互对内外分布事实泛化能力的影响，发现上下文结构和多样性水平显著影响模型泛化表现，并揭示了优化瓶颈集中在嵌入层和解嵌入层。


<details>
  <summary>Details</summary>
Motivation: 尽管语言模型中统计规律与事实知识的交互被认为影响泛化能力，但缺乏系统的分析来量化这两者之间多样性变化对模型表现的具体影响。

Method: 构建一个合成测试平台，通过组合统计令牌流和抽象事实流，独立调控上下文结构和事实出现的多样性，进行可控实验，并对模型各组成部分进行干预分析。

Result: 发现较高的上下文多样性会延迟模型对内部分布事实的准确性，而外部分布的泛化表现则依赖于上下文结构，有时多样性对非平凡事实回忆至关重要，且最佳多样性水平随训练时间变化，不同结构下统计泛化和事实回忆能力可能分别退化。

Conclusion: 上下文设计与多样性水平的相互作用深刻影响语言模型的事实泛化能力，且优化瓶颈位于嵌入与解嵌入层，该合成框架为未来研究提供了能隔离影响因素的控制平台。

Abstract: Language models are pretrained on sequences that blend statistical
regularities (making text fluent) with factual associations between specific
tokens (knowledge of facts). While recent work suggests that the variability of
their interaction, such as paraphrases of factual associations, critically
determines generalization ability, we lack a systematic analysis of these
impacts. This paper introduces a flexible synthetic testbed that combines a
statistical stream of generic tokens with an abstract factual stream of
source-target token pairs, enabling fine-grained control over their
interaction. The design enables the independent control of diversity nature by
manipulating stream composition (contextual structure) and the diversity level
by varying which statistical streams each fact appears in. Through controlled
experiments, we find that while higher contextual diversity delays
in-distribution (ID) factual accuracy, its impact on out-of-distribution (OOD)
factual generalization depends critically on contextual structure. In some
cases, OOD performance follows the same trend as ID, but in others, diversity
becomes essential for non-trivial factual recall. Even when low diversity
prohibits factual recall, optimal diversity levels depend on training duration.
Beyond factual recall failures, we identify structures where statistical
generalization fails independently, and others where both capabilities degrade.
This shows how the interplay between contextual design and diversity level
impacts different generalization aspects. Further, through a series of
controlled interventions on the model components, we trace the OOD failures to
distinct optimization bottlenecks, highlighting the importance of the embedding
and unembedding layers. Our synthetic framework allows us to isolate effects
that would be confounded in large-scale studies, offering a controlled testbed
for future investigations.

</details>


### [7] [Unleashing Diverse Thinking Modes in LLMs through Multi-Agent Collaboration](https://arxiv.org/abs/2510.16645)
*Zhixuan He,Yue Feng*

Main category: cs.CL

TL;DR: 本文提出了多代理协作框架DiMo，通过四个不同思维模式的LLM代理进行结构化辩论，提升了性能和解释性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型表现强劲但缺乏可解释的推理过程，亟需一种方法提升模型性能及推理的透明性。

Method: 构建四个具备不同推理范式的LLM代理，通过多轮辩论不断挑战和优化回答，生成更鲁棒的结论及明确的推理链。

Result: 在六个基准数据集上，DiMo的准确率超过了单模型和传统辩论基线，尤其在数学题上提升显著。

Conclusion: DiMo是一个语义感知的多代理框架，支持利用Web资源实现可检验和复用的结构化推理，促进人机智能融合。

Abstract: Large Language Models (LLMs) demonstrate strong performance but often lack
interpretable reasoning. This paper introduces the Multi-Agent Collaboration
Framework for Diverse Thinking Modes (DiMo), which enhances both performance
and interpretability by simulating a structured debate among four specialized
LLM agents. Each agent embodies a distinct reasoning paradigm, allowing the
framework to collaboratively explore diverse cognitive approaches. Through
iterative debate, agents challenge and refine initial responses, yielding more
robust conclusions and an explicit, auditable reasoning chain. Across six
benchmarks and under a unified open-source setup, DiMo improves accuracy over
widely used single-model and debate baselines, with the largest gains on math.
We position DiMo as a semantics-aware, Web-native multi-agent framework: it
models human-machine intelligence with LLM agents that produce semantically
typed, URL-annotated evidence chains for explanations and user-friendly
interactions. Although our experiments use standard reasoning benchmarks, the
framework is designed to be instantiated over Web corpora and knowledge graphs,
combining retrieval-augmented reasoning with structured justifications that
downstream systems can inspect and reuse.

</details>


### [8] [In Generative AI We (Dis)Trust? Computational Analysis of Trust and Distrust in Reddit Discussions](https://arxiv.org/abs/2510.16173)
*Aria Pessianzadeh,Naima Sultana,Hildegarde Van den Bulck,David Gefen,Shahin Jabari,Rezvaneh Rezapour*

Main category: cs.CL

TL;DR: 本文首次采用计算方法大规模、长时间跨度地研究生成式人工智能（GenAI）上的信任与不信任，基于2022-2025年Reddit数据分析，发现信任和不信任基本平衡，主要受模型发布影响。


<details>
  <summary>Details</summary>
Motivation: 随着生成式人工智能嵌入日常生活，理解公众对其信任变得关键。然而现有信任研究缺乏大规模、计算化和长期视角。

Method: 采集了2022年至2025年间39个Reddit子版块近20万帖子，结合众包标注与分类模型，进行信任与不信任的量化分析。

Result: 信任和不信任情绪随时间基本平衡，受技术性能与可用性影响最大，个人使用体验是态度形成的主要原因。不同用户群体表现出不同的信任模式。

Conclusion: 提出了大规模信任分析的计算方法框架，揭示了公众对生成式人工智能态度的动态演化，为其负责任采用提供依据。

Abstract: The rise of generative AI (GenAI) has impacted many aspects of human life. As
these systems become embedded in everyday practices, understanding public trust
in them also becomes essential for responsible adoption and governance. Prior
work on trust in AI has largely drawn from psychology and human-computer
interaction, but there is a lack of computational, large-scale, and
longitudinal approaches to measuring trust and distrust in GenAI and large
language models (LLMs). This paper presents the first computational study of
Trust and Distrust in GenAI, using a multi-year Reddit dataset (2022--2025)
spanning 39 subreddits and 197,618 posts. Crowd-sourced annotations of a
representative sample were combined with classification models to scale
analysis. We find that Trust and Distrust are nearly balanced over time, with
shifts around major model releases. Technical performance and usability
dominate as dimensions, while personal experience is the most frequent reason
shaping attitudes. Distinct patterns also emerge across trustors (e.g.,
experts, ethicists, general users). Our results provide a methodological
framework for large-scale Trust analysis and insights into evolving public
perceptions of GenAI.

</details>


### [9] [Verification-Aware Planning for Multi-Agent Systems](https://arxiv.org/abs/2510.17109)
*Tianyang Xu,Dan Zhang,Kushan Mitra,Estevam Hruschka*

Main category: cs.CL

TL;DR: VeriMAP是一个面向多智能体协作的验证感知规划框架，通过任务分解和子任务验证函数来提升系统的可靠性和解释性。


<details>
  <summary>Details</summary>
Motivation: 多智能体协作在规划、协调和验证方面面临挑战，执行失败往往源于任务理解、输出格式或智能体间交接的细微误差。

Method: VeriMAP规划器分解任务，建模子任务依赖，并将规划器定义的传递标准编码为子任务验证函数（VFs），支持Python与自然语言。

Result: 在多样化数据集上的评估显示，VeriMAP优于单智能体和多智能体基线方法，提升了系统的鲁棒性和可解释性。

Conclusion: 验证感知规划使多智能体系统能够实现可靠协调和迭代优化，无需外部标签或注释。

Abstract: Large language model (LLM) agents are increasingly deployed to tackle complex
tasks, often necessitating collaboration among multiple specialized agents.
However, multi-agent collaboration introduces new challenges in planning,
coordination, and verification. Execution failures frequently arise not from
flawed reasoning alone, but from subtle misalignments in task interpretation,
output format, or inter-agent handoffs. To address these challenges, we present
VeriMAP, a framework for multi-agent collaboration with verification-aware
planning. The VeriMAP planner decomposes tasks, models subtask dependencies,
and encodes planner-defined passing criteria as subtask verification functions
(VFs) in Python and natural language. We evaluate VeriMAP on diverse datasets,
demonstrating that it outperforms both single- and multi-agent baselines while
enhancing system robustness and interpretability. Our analysis highlights how
verification-aware planning enables reliable coordination and iterative
refinement in multi-agent systems, without relying on external labels or
annotations.

</details>


### [10] [EgMM-Corpus: A Multimodal Vision-Language Dataset for Egyptian Culture](https://arxiv.org/abs/2510.16198)
*Mohamed Gamil,Abdelrahman Elsayed,Abdelrahman Lila,Ahmed Gad,Hesham Abdelgawad,Mohamed Aref,Ahmed Fares*

Main category: cs.CL

TL;DR: 本文介绍了一个名为EgMM-Corpus的埃及文化多模态数据集，包含3000多张图片和313个概念，旨在促进文化多样性视觉语言模型的训练与评估。


<details>
  <summary>Details</summary>
Motivation: 当前AI多模态文化多样性数据集有限，特别是中东和非洲地区，导致视觉语言模型存在文化偏见。

Method: 设计并运行新的数据采集流程，收集并手动验证涵盖埃及地标、美食和民俗的多模态数据。

Result: EgMM-Corpus中CLIP零样本分类Top-1准确率为21.2%，Top-5为36.4%，显示了大规模模型中的文化偏见。

Conclusion: EgMM-Corpus为埃及文化情境下训练和评估视觉语言模型提供了重要资源，推动开发具文化意识的模型。

Abstract: Despite recent advances in AI, multimodal culturally diverse datasets are
still limited, particularly for regions in the Middle East and Africa. In this
paper, we introduce EgMM-Corpus, a multimodal dataset dedicated to Egyptian
culture. By designing and running a new data collection pipeline, we collected
over 3,000 images, covering 313 concepts across landmarks, food, and folklore.
Each entry in the dataset is manually validated for cultural authenticity and
multimodal coherence. EgMM-Corpus aims to provide a reliable resource for
evaluating and training vision-language models in an Egyptian cultural context.
We further evaluate the zero-shot performance of Contrastive Language-Image
Pre-training CLIP on EgMM-Corpus, on which it achieves 21.2% Top-1 accuracy and
36.4% Top-5 accuracy in classification. These results underscore the existing
cultural bias in large-scale vision-language models and demonstrate the
importance of EgMM-Corpus as a benchmark for developing culturally aware
models.

</details>


### [11] [BenCao: An Instruction-Tuned Large Language Model for Traditional Chinese Medicine](https://arxiv.org/abs/2510.17415)
*Jiacheng Xie,Yang Yu,Yibo Chen,Hanyao Zhang,Lening Zhao,Jiaxuan He,Lei Jiang,Xiaoting Tang,Guanghui An,Dong Xu*

Main category: cs.CL

TL;DR: BenCao是一款基于ChatGPT的中医多模态助手，通过自然语言指令调优整合结构化知识库和多模态诊断数据，实现了专家级推理和临床应用。


<details>
  <summary>Details</summary>
Motivation: 传统中医依赖整体推理、隐含逻辑及多模态诊断信息，现有中医领域大语言模型主要聚焦文本理解，缺乏多模态整合、可解释性和临床适用性。

Method: BenCao结合超过1000部古今中医典籍的知识库，构建场景化指令框架和思维链模拟机制，通过专家反馈精细调整，并接入外部API实现舌象分类和多模态数据库检索。

Result: 在单项选择题和多模态分类任务中，BenCao在诊断、草药识别及体质分类方面准确率优于通用及中医领域模型，用户量达近1000人。

Conclusion: 通过自然语言指令调优与多模态融合，BenCao成功实现了符合传统医学推理的中医领域大语言模型，提供了可解释且可规模化部署的生成式AI应用框架。

Abstract: Traditional Chinese Medicine (TCM), with a history spanning over two
millennia, plays a role in global healthcare. However, applying large language
models (LLMs) to TCM remains challenging due to its reliance on holistic
reasoning, implicit logic, and multimodal diagnostic cues. Existing TCM-domain
LLMs have made progress in text-based understanding but lack multimodal
integration, interpretability, and clinical applicability. To address these
limitations, we developed BenCao, a ChatGPT-based multimodal assistant for TCM,
integrating structured knowledge bases, diagnostic data, and expert feedback
refinement. BenCao was trained through natural language instruction tuning
rather than parameter retraining, aligning with expert-level reasoning and
ethical norms specific to TCM. The system incorporates a comprehensive
knowledge base of over 1,000 classical and modern texts, a scenario-based
instruction framework for diverse interactions, a chain-of-thought simulation
mechanism for interpretable reasoning, and a feedback refinement process
involving licensed TCM practitioners. BenCao connects to external APIs for
tongue-image classification and multimodal database retrieval, enabling dynamic
access to diagnostic resources. In evaluations across single-choice question
benchmarks and multimodal classification tasks, BenCao achieved superior
accuracy to general-domain and TCM-domain models, particularly in diagnostics,
herb recognition, and constitution classification. The model was deployed as an
interactive application on the OpenAI GPTs Store, accessed by nearly 1,000
users globally as of October 2025. This study demonstrates the feasibility of
developing a TCM-domain LLM through natural language-based instruction tuning
and multimodal integration, offering a practical framework for aligning
generative AI with traditional medical reasoning and a scalable pathway for
real-world deployment.

</details>


### [12] [What Can String Probability Tell Us About Grammaticality?](https://arxiv.org/abs/2510.16227)
*Jennifer Hu,Ethan Gotlieb Wilcox,Siyuan Song,Kyle Mahowald,Roger P. Levy*

Main category: cs.CL

TL;DR: 本文探讨语言模型对语法的理解，通过理论分析和实证验证揭示了语法、意义与字符串概率之间的关系，提出三项预测并以英汉数据进行了验证。


<details>
  <summary>Details</summary>
Motivation: 语法学中概率与语法正确性是不同的概念，探究语言模型概率能在多大程度上反映其内在的语法知识。

Method: 基于语料生成过程的简单假设，建立语法、意义与字符串概率关系的理论框架，提出三项预测，并用28万对英汉最小对句子进行实证验证。

Result: 验证了字符串概率在具有最小语义差异的句对中的相关性，模型与人类判断的delta值相关性，以及未配对的语法与非语法句子概率空间的难以区分。

Conclusion: 概率可以为理解语言模型的结构知识提供理论依据，指出了未来语言模型语法评估的研究方向。

Abstract: What have language models (LMs) learned about grammar? This question remains
hotly debated, with major ramifications for linguistic theory. However, since
probability and grammaticality are distinct notions in linguistics, it is not
obvious what string probabilities can reveal about an LM's underlying
grammatical knowledge. We present a theoretical analysis of the relationship
between grammar, meaning, and string probability, based on simple assumptions
about the generative process of corpus data. Our framework makes three
predictions, which we validate empirically using 280K sentence pairs in English
and Chinese: (1) correlation between the probability of strings within minimal
pairs, i.e., string pairs with minimal semantic differences; (2) correlation
between models' and humans' deltas within minimal pairs; and (3) poor
separation in probability space between unpaired grammatical and ungrammatical
strings. Our analyses give theoretical grounding for using probability to learn
about LMs' structural knowledge, and suggest directions for future work in LM
grammatical evaluation.

</details>


### [13] [Executable Knowledge Graphs for Replicating AI Research](https://arxiv.org/abs/2510.17795)
*Yujie Luo,Zhuoyun Yu,Xuehai Wang,Yuqi Zhu,Ningyu Zhang,Lanning Wei,Lun Du,Da Zheng,Huajun Chen*

Main category: cs.CL

TL;DR: 本文提出了可执行知识图谱(xKG)以提升大语言模型在AI研究复现中的代码生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以生成可执行代码，缺乏足够背景知识和有效结构化知识表示，影响了技术细节的捕获和复用。

Method: 设计模块化的可执行知识图谱，自动整合技术洞见、代码片段和领域知识，并集成至多种智能体框架和大语言模型。

Result: 在PaperBench基准测试中，xKG在使用o3-mini模型时性能提升了10.9%，表现出显著效果。

Conclusion: xKG是一个通用且可扩展的解决方案，有效支持自动化AI研究复现，且相关代码已开源。

Abstract: Replicating AI research is a crucial yet challenging task for large language
model (LLM) agents. Existing approaches often struggle to generate executable
code, primarily due to insufficient background knowledge and the limitations of
retrieval-augmented generation (RAG) methods, which fail to capture latent
technical details hidden in referenced papers. Furthermore, previous approaches
tend to overlook valuable implementation-level code signals and lack structured
knowledge representations that support multi-granular retrieval and reuse. To
overcome these challenges, we propose Executable Knowledge Graphs (xKG), a
modular and pluggable knowledge base that automatically integrates technical
insights, code snippets, and domain-specific knowledge extracted from
scientific literature. When integrated into three agent frameworks with two
different LLMs, xKG shows substantial performance gains (10.9% with o3-mini) on
PaperBench, demonstrating its effectiveness as a general and extensible
solution for automated AI research replication. Code will released at
https://github.com/zjunlp/xKG.

</details>


### [14] [Towards Low-Resource Alignment to Diverse Perspectives with Sparse Feedback](https://arxiv.org/abs/2510.16257)
*Chu Fei Luo,Samuel Dahan,Xiaodan Zhu*

Main category: cs.CL

TL;DR: 该论文旨在提升语言模型对多元观点的对齐能力，避免生成单一答案，改善在低资源环境下的表现。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型训练假设每个查询只有一个最佳答案，导致应答泛化且缺乏多样性，难以反映人类价值的细微差别。

Method: 提出了多元解码和模型引导两种方法，尤其强调在只有50个标注样本的情况下，模型引导方法能持续改进模型表现。

Result: 所提方法显著减少了仇恨言论检测和虚假信息检测中的误报，并在GlobalOpinionQA任务中提升了模型输出与人类价值的分布对齐度。

Conclusion: 强调了模型考虑多元和细微观点的重要性，证明了所提方法在低资源环境下有效地增强了语言模型的多样性对齐能力。

Abstract: As language models have a greater impact on society, it is important to
ensure they are aligned to a diverse range of perspectives and are able to
reflect nuance in human values. However, the most popular training paradigms
for modern language models often assume there is one optimal answer for every
query, leading to generic responses and poor alignment. In this work, we aim to
enhance pluralistic alignment of language models in a low-resource setting with
two methods: pluralistic decoding and model steering. We empirically
demonstrate that model steering offers consistent improvement over zero-shot
and few-shot baselines with only 50 annotated samples. Our proposed methods
decrease false positives in several high-stakes tasks such as hate speech
detection and misinformation detection, and improves the distributional
alignment to human values in GlobalOpinionQA. We hope our work highlights the
importance of diversity and how language models can be adapted to consider
nuanced perspectives.

</details>


### [15] [Instant Personalized Large Language Model Adaptation via Hypernetwork](https://arxiv.org/abs/2510.16282)
*Zhaoxuan Tan,Zixuan Zhang,Haoyang Wen,Zheng Li,Rongzhi Zhang,Pei Chen,Fengran Mo,Zheyuan Liu,Qingkai Zeng,Qingyu Yin,Meng Jiang*

Main category: cs.CL

TL;DR: 提出了一种Profile-to-PEFT框架，通过超网络将用户编码资料映射为适配器参数，实现大规模、高效的个性化大语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有个性化微调方法如OPPU需要为每用户训练单独适配器，计算成本高且难以实时更新。

Method: 利用端到端训练的超网络直接将用户编码的个人资料映射为适配器参数，无需部署时为每用户单独训练。

Result: 该方法较提示个性化和OPPU在部署资源消耗更低，表现更优，且具备对未见用户的泛化能力和强鲁棒性。

Conclusion: Profile-to-PEFT框架实现了高效、可扩展且自适应的LLM个性化，适合大规模应用。

Abstract: Personalized large language models (LLMs) tailor content to individual
preferences using user profiles or histories. However, existing
parameter-efficient fine-tuning (PEFT) methods, such as the
``One-PEFT-Per-User'' (OPPU) paradigm, require training a separate adapter for
each user, making them computationally expensive and impractical for real-time
updates. We introduce Profile-to-PEFT, a scalable framework that employs a
hypernetwork, trained end-to-end, to map a user's encoded profile directly to a
full set of adapter parameters (e.g., LoRA), eliminating per-user training at
deployment. This design enables instant adaptation, generalization to unseen
users, and privacy-preserving local deployment. Experimental results
demonstrate that our method outperforms both prompt-based personalization and
OPPU while using substantially fewer computational resources at deployment. The
framework exhibits strong generalization to out-of-distribution users and
maintains robustness across varying user activity levels and different
embedding backbones. The proposed Profile-to-PEFT framework enables efficient,
scalable, and adaptive LLM personalization suitable for large-scale
applications.

</details>


### [16] [Thinking About Thinking: Evaluating Reasoning in Post-Trained Language Models](https://arxiv.org/abs/2510.16340)
*Pratham Singla,Shivank Garg,Ayush Singh,Ishan Garg,Ketan Suhaas Saichandran*

Main category: cs.CL

TL;DR: 本文探讨了大语言模型经过后训练加强规划能力后，是否具备自我认知和思考能力。


<details>
  <summary>Details</summary>
Motivation: 随着后训练技术使大语言模型能够生成辅助规划标记，关键问题是这些模型是否意识到它们所“学习”和“思考”的内容。

Method: 定义三大核心能力：学习策略的认知、跨域泛化能力、推理轨迹与最终输出的一致性，设计多任务测试模型能力，并比较不同后训练方法（SFT、DPO、GRPO）下模型表现。

Result: 强化学习训练的模型在认知学习策略和跨域泛化能力上表现优于SFT模型，但其推理轨迹与最终输出的一致性较差，GRPO模型尤为明显。

Conclusion: RL训练方法提升了模型的学习认知和泛化能力，但在推理过程的透明性和输出一致性方面存在不足。

Abstract: Recent advances in post-training techniques have endowed Large Language
Models (LLMs) with enhanced capabilities for tackling complex, logic-intensive
tasks through the generation of supplementary planning tokens. This development
raises a fundamental question: Are these models aware of what they "learn" and
"think"? To address this, we define three core competencies: (1) awareness of
learned latent policies, (2) generalization of these policies across domains,
and (3) alignment between internal reasoning traces and final outputs. We
empirically evaluate these abilities on several tasks, each designed to require
learning a distinct policy. Furthermore, we contrast the profiles of models
post-trained via Supervised Fine-Tuning (SFT), Direct Policy Optimization
(DPO), and Group Relative Policy Optimization (GRPO). Our findings indicate
that RL-trained models not only demonstrate greater awareness of their learned
behaviors and stronger generalizability to novel, structurally similar tasks
than SFT models but also often exhibit weak alignment between their reasoning
traces and final outputs, an effect most pronounced in GRPO-trained models.

</details>


### [17] [Utilising Large Language Models for Generating Effective Counter Arguments to Anti-Vaccine Tweets](https://arxiv.org/abs/2510.16359)
*Utsav Dhanuka,Soham Poddar,Saptarshi Ghosh*

Main category: cs.CL

TL;DR: 本论文探讨利用大型语言模型（LLM）实时生成针对疫苗错误信息的反驳论点，通过多种提示策略和微调方法优化反驳效果，并利用分类器分类反疫苗推文，提升反驳的针对性和效果。


<details>
  <summary>Details</summary>
Motivation: 由于社交媒体传播的疫苗错误信息阻碍了免疫率提升和公共健康信任，迫切需要有效的实时反驳机制来抵消错误叙述。

Method: 本文通过提示策略和微调训练大型语言模型生成反疫苗错误信息的反驳论点，同时训练分类器对反疫苗推文进行多标签分类，以实现更具上下文相关性的反驳内容。

Result: 综合人类评判、LLM评估及自动指标，结果显示融合标签描述和结构化微调显著提升了反驳效果，方法效果良好且一致。

Conclusion: 集成标签信息和结构化微调的大型语言模型能够有效生成高质量、定制化的反驳论点，为大规模缓解疫苗错误信息提供了有前景的技术路径。

Abstract: In an era where public health is increasingly influenced by information
shared on social media, combatting vaccine skepticism and misinformation has
become a critical societal goal. Misleading narratives around vaccination have
spread widely, creating barriers to achieving high immunisation rates and
undermining trust in health recommendations. While efforts to detect
misinformation have made significant progress, the generation of real time
counter-arguments tailored to debunk such claims remains an insufficiently
explored area. In this work, we explore the capabilities of LLMs to generate
sound counter-argument rebuttals to vaccine misinformation. Building on prior
research in misinformation debunking, we experiment with various prompting
strategies and fine-tuning approaches to optimise counter-argument generation.
Additionally, we train classifiers to categorise anti-vaccine tweets into
multi-labeled categories such as concerns about vaccine efficacy, side effects,
and political influences allowing for more context aware rebuttals. Our
evaluation, conducted through human judgment, LLM based assessments, and
automatic metrics, reveals strong alignment across these methods. Our findings
demonstrate that integrating label descriptions and structured fine-tuning
enhances counter-argument effectiveness, offering a promising approach for
mitigating vaccine misinformation at scale.

</details>


### [18] [End-to-End Argument Mining through Autoregressive Argumentative Structure Prediction](https://arxiv.org/abs/2510.16363)
*Nilmadhab Das,Vishal Vaibhav,Yash Sunil Choudhary,V. Vijaya Saradhi,Ashish Anand*

Main category: cs.CL

TL;DR: 本论文提出了一种基于自回归结构预测的端到端论证挖掘框架AASP，用以联合预测论证组件和关系。


<details>
  <summary>Details</summary>
Motivation: 论证挖掘需要自动提取复杂的论证结构，但因其推理复杂性，建模论证组件与关系间的依赖关系存在挑战。现有方法多基于生成式、结构扁平化处理，效果有限。

Method: 提出基于条件预训练语言模型的自回归结构预测框架AASP，通过限定动作集合逐步构建论证结构，实现推理流的有效捕获。

Result: 在三个标准论证挖掘基准上进行了广泛实验，AASP在两个基准上实现了最先进水平的表现，在第三个基准上取得了较强的结果。

Conclusion: AASP框架有效联合完成论证组件与关系的预测，提升了论证挖掘任务的性能，展现了自回归结构预测在该领域的潜力。

Abstract: Argument Mining (AM) helps in automating the extraction of complex
argumentative structures such as Argument Components (ACs) like Premise, Claim
etc. and Argumentative Relations (ARs) like Support, Attack etc. in an
argumentative text. Due to the inherent complexity of reasoning involved with
this task, modelling dependencies between ACs and ARs is challenging. Most of
the recent approaches formulate this task through a generative paradigm by
flattening the argumentative structures. In contrast to that, this study
jointly formulates the key tasks of AM in an end-to-end fashion using
Autoregressive Argumentative Structure Prediction (AASP) framework. The
proposed AASP framework is based on the autoregressive structure prediction
framework that has given good performance for several NLP tasks. AASP framework
models the argumentative structures as constrained pre-defined sets of actions
with the help of a conditional pre-trained language model. These actions build
the argumentative structures step-by-step in an autoregressive manner to
capture the flow of argumentative reasoning in an efficient way. Extensive
experiments conducted on three standard AM benchmarks demonstrate that AASP
achieves state-of-theart (SoTA) results across all AM tasks in two benchmarks
and delivers strong results in one benchmark.

</details>


### [19] [Navigating through the hidden embedding space: steering LLMs to improve mental health assessment](https://arxiv.org/abs/2510.16373)
*Federico Ravenda,Seyed Ali Bahrainian,Andrea Raballo,Antonietta Mira*

Main category: cs.CL

TL;DR: 该论文提出了一种轻量级、低成本的方法，通过对大语言模型（LLM）特定层的激活进行线性变换，利用引导向量优化模型在心理健康领域的评估能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在心理健康等敏感领域具有潜力，但小规模模型在特定领域应用中表现不佳，亟需一种既经济又高效的改进手段。

Method: 研究通过线性变换特定层激活并引入引导向量，实现对模型输出的引导，无需复杂计算资源。

Result: 该方法在两个任务上均取得显著提升：一是判断Reddit帖子是否有助于检测抑郁症状；二是基于帖子历史完成标准化抑郁筛查问卷。

Conclusion: 引导机制作为一种计算效率高的工具，展示了大语言模型在心理健康领域适应性的潜力，尤其适合低资源场景。

Abstract: The rapid evolution of Large Language Models (LLMs) is transforming AI,
opening new opportunities in sensitive and high-impact areas such as Mental
Health (MH). Yet, despite these advancements, recent evidence reveals that
smaller-scale models still struggle to deliver optimal performance in
domain-specific applications. In this study, we present a cost-efficient yet
powerful approach to improve MH assessment capabilities of an LLM, without
relying on any computationally intensive techniques. Our lightweight method
consists of a linear transformation applied to a specific layer's activations,
leveraging steering vectors to guide the model's output. Remarkably, this
intervention enables the model to achieve improved results across two distinct
tasks: (1) identifying whether a Reddit post is useful for detecting the
presence or absence of depressive symptoms (relevance prediction task), and (2)
completing a standardized psychological screening questionnaire for depression
based on users' Reddit post history (questionnaire completion task). Results
highlight the untapped potential of steering mechanisms as computationally
efficient tools for LLMs' MH domain adaptation.

</details>


### [20] [MoReBench: Evaluating Procedural and Pluralistic Moral Reasoning in Language Models, More than Outcomes](https://arxiv.org/abs/2510.16380)
*Yu Ying Chiu,Michael S. Lee,Rachel Calcott,Brandon Handoko,Paul de Font-Reaulx,Paula Rodriguez,Chen Bo Calvin Zhang,Ziwen Han,Udari Madhushani Sehwag,Yash Maurya,Christina Q Knight,Harry R. Lloyd,Florence Bacus,Mantas Mazeika,Bing Liu,Yejin Choi,Mitchell L Gordon,Sydney Levine*

Main category: cs.CL

TL;DR: 本文提出MoReBench基准，用于评估AI在道德推理过程中的表现，包含1000个道德场景及专家制定的判断标准。研究发现现有模型难以准确预测道德推理能力，且对特定伦理框架存在偏好。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在决策中扮演重要角色，理解AI如何作出道德决策变得至关重要，因道德困境允许多种合理结论，是检验AI推理过程的理想场景。

Method: 建立MoReBench数据集，包含1000个道德场景和23000多个评价标准，用于分析模型的道德考虑、权衡取舍及给出建议，另设MoReBench-Theory测试AI在五大规范伦理框架下的推理能力。

Result: 发现模型在道德推理能力上与其在数学、代码和科学推理任务的表现不相关，且模型倾向于特定道德框架，如功利主义和康德伦理学，这可能是训练范式的副作用。

Conclusion: MoReBench基准促进了侧重过程的道德推理评估，有助于推动更安全、更透明的AI系统发展。

Abstract: As AI systems progress, we rely more on them to make decisions with us and
for us. To ensure that such decisions are aligned with human values, it is
imperative for us to understand not only what decisions they make but also how
they come to those decisions. Reasoning language models, which provide both
final responses and (partially transparent) intermediate thinking traces,
present a timely opportunity to study AI procedural reasoning. Unlike math and
code problems which often have objectively correct answers, moral dilemmas are
an excellent testbed for process-focused evaluation because they allow for
multiple defensible conclusions. To do so, we present MoReBench: 1,000 moral
scenarios, each paired with a set of rubric criteria that experts consider
essential to include (or avoid) when reasoning about the scenarios. MoReBench
contains over 23 thousand criteria including identifying moral considerations,
weighing trade-offs, and giving actionable recommendations to cover cases on AI
advising humans moral decisions as well as making moral decisions autonomously.
Separately, we curate MoReBench-Theory: 150 examples to test whether AI can
reason under five major frameworks in normative ethics. Our results show that
scaling laws and existing benchmarks on math, code, and scientific reasoning
tasks fail to predict models' abilities to perform moral reasoning. Models also
show partiality towards specific moral frameworks (e.g., Benthamite Act
Utilitarianism and Kantian Deontology), which might be side effects of popular
training paradigms. Together, these benchmarks advance process-focused
reasoning evaluation towards safer and more transparent AI.

</details>


### [21] [ATA: A Neuro-Symbolic Approach to Implement Autonomous and Trustworthy Agents](https://arxiv.org/abs/2510.16381)
*David Peer,Sebastian Stabinger*

Main category: cs.CL

TL;DR: 提出了一种名为Autonomous Trustworthy Agents (ATA)的神经符号方法，通过离线知识摄取和在线任务处理两阶段，提升大型语言模型在高风险领域的可信度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在高风险领域应用受限，存在幻觉、不稳定和缺乏透明度等信任问题。

Method: 将任务分为离线知识摄取和在线任务处理两阶段，先将非正式问题规范转成形式化的符号知识库，经过人工验证后，在线阶段用符号决策引擎结合知识库处理输入，保证结果可靠。

Result: 在复杂推理任务上，ATA实现了与最先进端到端推理模型竞争的表现，并且在人为校验的知识库下显著优于更大模型，表现出稳定性和免疫提示注入攻击等优势。

Conclusion: ATA通过基于符号推理的决策，提供了一个可控、透明、可审计且可靠的自治智能体架构，有望推动下一代可信自主智能体的发展。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities, yet
their deployment in high-stakes domains is hindered by inherent limitations in
trustworthiness, including hallucinations, instability, and a lack of
transparency. To address these challenges, we introduce a generic
neuro-symbolic approach, which we call Autonomous Trustworthy Agents (ATA). The
core of our approach lies in decoupling tasks into two distinct phases: Offline
knowledge ingestion and online task processing. During knowledge ingestion, an
LLM translates an informal problem specification into a formal, symbolic
knowledge base. This formal representation is crucial as it can be verified and
refined by human experts, ensuring its correctness and alignment with domain
requirements. In the subsequent task processing phase, each incoming input is
encoded into the same formal language. A symbolic decision engine then utilizes
this encoded input in conjunction with the formal knowledge base to derive a
reliable result. Through an extensive evaluation on a complex reasoning task,
we demonstrate that a concrete implementation of ATA is competitive with
state-of-the-art end-to-end reasoning models in a fully automated setup while
maintaining trustworthiness. Crucially, with a human-verified and corrected
knowledge base, our approach significantly outperforms even larger models,
while exhibiting perfect determinism, enhanced stability against input
perturbations, and inherent immunity to prompt injection attacks. By generating
decisions grounded in symbolic reasoning, ATA offers a practical and
controllable architecture for building the next generation of transparent,
auditable, and reliable autonomous agents.

</details>


### [22] [Probing the Hidden Talent of ASR Foundation Models for L2 English Oral Assessment](https://arxiv.org/abs/2510.16387)
*Fu-An Chao,Bi-Cheng Yan,Berlin Chen*

Main category: cs.CL

TL;DR: 本文探讨了Whisper模型在二语口语评估中的潜力，通过提取其隐藏表征的声学和语言特征，使用轻量级分类器实现了优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅利用Whisper生成的转录文本进行分析，未深入挖掘其隐藏表征中的丰富信息。

Method: 从Whisper的中间和最终输出中提取声学和语言特征，训练轻量级分类器，同时结合图像与文本提示信息优化性能。

Result: 在GEPT图片描述数据集上，方法优于当前主流基线模型，包括多模态方法，且融合辅助信息后性能进一步提升。

Conclusion: Whisper无需专门微调即可内在编码语言能力和语义信息，展现出作为二语口语评估及其它口语理解任务基础模型的巨大潜力。

Abstract: In this paper, we explore the untapped potential of Whisper, a
well-established automatic speech recognition (ASR) foundation model, in the
context of L2 spoken language assessment (SLA). Unlike prior studies that
extrinsically analyze transcriptions produced by Whisper, our approach goes a
step further to probe its latent capabilities by extracting acoustic and
linguistic features from hidden representations. With only a lightweight
classifier being trained on top of Whisper's intermediate and final outputs,
our method achieves strong performance on the GEPT picture-description dataset,
outperforming existing cutting-edge baselines, including a multimodal approach.
Furthermore, by incorporating image and text-prompt information as auxiliary
relevance cues, we demonstrate additional performance gains. Finally, we
conduct an in-depth analysis of Whisper's embeddings, which reveals that, even
without task-specific fine-tuning, the model intrinsically encodes both ordinal
proficiency patterns and semantic aspects of speech, highlighting its potential
as a powerful foundation for SLA and other spoken language understanding tasks.

</details>


### [23] [FrugalPrompt: Reducing Contextual Overhead in Large Language Models via Token Attribution](https://arxiv.org/abs/2510.16439)
*Syed Rifat Raiyan,Md Farhan Ishmam,Abdullah Al Imran,Mohammad Ali Moni*

Main category: cs.CL

TL;DR: 该论文提出了FrugalPrompt，一种针对大型语言模型的提示压缩框架，通过保留最具语义意义的词元来减少提示冗余，从而降低计算成本和延迟。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在使用广泛输入上下文时表现优异，但冗余低价值词元导致成本高、碳足迹大以及推理延迟长，亟需提高提示效率。

Method: 利用两种尖端词元归因方法GlobEnc和DecompX，为输入序列中的每个词元赋予显著性分数，保留排名前k%的词元并保持顺序，以生成稀疏高效的压缩提示。

Result: 在情感分析、常识问答和摘要任务中，减少20%提示词元仅带来轻微性能损失，证明模型能从高显著性提示重建上下文；但数学推理任务性能大幅下降，显示其对完整上下文的依赖较强。

Conclusion: 该方法揭示了不同任务在上下文稀疏性容忍度上的差异，有助于理解大型语言模型性能与效率的权衡，并区分了对完整上下文需求的任务类型。

Abstract: Large language models (LLMs) owe much of their stellar performance to
expansive input contexts, yet such verbosity inflates monetary costs, carbon
footprint, and inference-time latency. Much of this overhead manifests from the
redundant low-utility tokens present in typical prompts, as only a fraction of
tokens typically carries the majority of the semantic weight. We address this
inefficiency by introducing FrugalPrompt, a novel prompt compression framework
for LLMs, which retains only the most semantically significant tokens.
Leveraging two state-of-the-art token attribution methods, GlobEnc and DecompX,
we assign salience scores to every token in an input sequence, rank them to
preserve the top-k% tokens in their original order, and obtain a sparse
frugalized prompt. We evaluate the approach across four NLP tasks: Sentiment
Analysis, Commonsense QA, Summarization, and Mathematical Reasoning, using a
suite of frontier LLMs. For the first three tasks, a 20% prompt reduction
incurs only a marginal loss in task performance, demonstrating that
contemporary LLMs can reconstruct elided context from high-salience cues. In
contrast, performance on mathematical reasoning deteriorates sharply,
reflecting a stronger dependence on complete token continuity. Further analysis
with bottom-k% and random-k% tokens reveals asymmetric performance patterns
that may suggest potential task contamination effects, wherein models may
resort to shallow memorized patterns from pretraining exposure for conventional
NLP tasks. We posit that our work contributes to a more nuanced understanding
of LLM behavior in performance-efficiency trade-offs, and delineate the
boundary between tasks tolerant to contextual sparsity and those requiring
exhaustive context. Our source code and models are available at:
https://github.com/Starscream-11813/Frugal-ICL

</details>


### [24] [TrajSelector: Harnessing Latent Representations for Efficient and Effective Best-of-N in Large Reasoning Model](https://arxiv.org/abs/2510.16449)
*Bin Yu,Xinming Wang,Shijie Lian,Haotian Li,Changti Wu,Ruina Hu,Bailing Wang,Yuliang Wei,Kai Chen*

Main category: cs.CL

TL;DR: 本文提出了一种名为TrajSelector的高效推理轨迹选择框架，通过利用大型语言模型的隐藏状态进行逐步轨迹评分，提升了复杂推理任务的性能，同时降低了推理计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的Test-time Scaling (TTS)方法，尤其是Best-of-N选择范式虽然提升了推理性能，但存在高计算开销和未充分利用语言模型潜在表示的问题。

Method: TrajSelector利用采样器语言模型中的隐藏状态进行过程级评分，配备轻量级验证器（0.6B参数）对每步轨迹质量进行评估，并通过端到端数据驱动训练消除了对大量步骤级注释的依赖。

Result: 在五个基准测试中，TrajSelector在Best-of-32设置下，准确率比多数投票提升4.61%，且比现有过程奖励模型高出4.31%至12.21%，同时保持较低的推理成本。

Conclusion: TrajSelector有效利用语言模型隐藏状态，实现了推理轨迹选择的性能提升与计算效率的平衡，为复杂推理任务提供了一种更优的解法。

Abstract: Large language models (LLMs) have shown remarkable progress in complex
reasoning tasks, largely enabled by test-time scaling (TTS) paradigms that
allocate additional compute during inference. Among these, external TTS
(particularly the Best-of-N selection paradigm) yields scalable performance
improvements by selecting from multiple independently generated reasoning
trajectories. However, this approach faces key limitations: (i) the high
computational overhead of deploying process reward models, (ii) the
underutilization of the LLM's intrinsic latent representations. We introduce
TrajSelector, an efficient and effective Best-of-N framework that exploit the
hidden states in the sampler LLM for process-level scoring. A lightweight
verifier (with only 0.6B parameters) evaluates the quality of step-wise
trajectory, and then aggregates these scores to identify the optimal reasoning
trajectory. Our framework employs a fully data-driven, end-to-end training
recipe that eliminates reliance on massive step-level annotations. Experiential
results across five benchmarks demonstrate that TrajSelector delivers
consistent performance gains. In Best-of-32 settings, it surpasses majority
voting by 4.61% accuracy and outperforms existing process reward models by
4.31% to 12.21%, all while maintaining lower inference costs.

</details>


### [25] [RAVEN: Robust Advertisement Video Violation Temporal Grounding via Reinforcement Reasoning](https://arxiv.org/abs/2510.16455)
*Deyi Ji,Yuekui Yang,Haiyang Wu,Shaoping Ma,Tianrun Chen,Lanyun Zhu*

Main category: cs.CL

TL;DR: 提出了RAVEN框架，通过结合课程式强化学习和多模态大语言模型，提高广告违规视频的准确检测和时序定位性能，并展示出良好的泛化能力和实际应用效果。


<details>
  <summary>Details</summary>
Motivation: 现有广告违规视频检测方法在时序精确定位、标注噪声和泛化能力方面存在不足，亟需提升检测的准确性和实用性。

Method: 设计了RAVEN框架，采用渐进式训练策略结合精细和粗略标注数据，利用组相对策略优化（GRPO）培养推理能力，采用多层次奖励机制确保时序定位准确和类别预测一致。

Result: 实验结果显示RAVEN在违规类别准确率和时间区间定位上优于现有方法，在线A/B测试验证了其实用性，显著提升了准确率和召回率。

Conclusion: RAVEN有效提升了广告违规检测的推理能力和时序定位性能，具备良好泛化能力，且在实际线上应用中表现优异，解决了传统方法的多项不足。

Abstract: Advertisement (Ad) video violation detection is critical for ensuring
platform compliance, but existing methods struggle with precise temporal
grounding, noisy annotations, and limited generalization. We propose RAVEN, a
novel framework that integrates curriculum reinforcement learning with
multimodal large language models (MLLMs) to enhance reasoning and cognitive
capabilities for violation detection. RAVEN employs a progressive training
strategy, combining precisely and coarsely annotated data, and leverages Group
Relative Policy Optimization (GRPO) to develop emergent reasoning abilities
without explicit reasoning annotations. Multiple hierarchical sophisticated
reward mechanism ensures precise temporal grounding and consistent category
prediction. Experiments on industrial datasets and public benchmarks show that
RAVEN achieves superior performances in violation category accuracy and
temporal interval localization. We also design a pipeline to deploy the RAVEN
on the online Ad services, and online A/B testing further validates its
practical applicability, with significant improvements in precision and recall.
RAVEN also demonstrates strong generalization, mitigating the catastrophic
forgetting issue associated with supervised fine-tuning.

</details>


### [26] [Agree, Disagree, Explain: Decomposing Human Label Variation in NLI through the Lens of Explanations](https://arxiv.org/abs/2510.16458)
*Pingjun Hong,Beiduo Chen,Siyao Peng,Marie-Catherine de Marneffe,Benjamin Roth,Barbara Plank*

Main category: cs.CL

TL;DR: 本文通过分析自然语言推理数据集中的注释者解释，研究标签差异背后的推理过程和个体差异，发现解释类型的一致性比标签一致性更能反映语义相似性。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注注释者在标签一致情况下的解释差异，忽视了标签本身的差异及其与解释的关系。本文旨在深入理解注释者在推理类型和标签选择上的差异，揭示注释过程中的复杂性。

Method: 采用LiTEx分类法对两个英文自然语言推理数据集中的自由文本解释进行分类，结合标签一致性、解释相似性和分类一致性多方面分析注释者差异，同时考虑注释者选择偏差。

Result: 发现注释者在标签不一致时仍可能提供高度相似的解释，表明表面标签差异下潜藏较一致的语义解释；同时揭示个体在解释策略和标签选择上的偏好。

Conclusion: 解释类型的一致性比标签一致性更能体现语义相似性，提示应重视基于推理的解释，谨慎对待标签作为绝对真值。

Abstract: Natural Language Inference datasets often exhibit human label variation. To
better understand these variations, explanation-based approaches analyze the
underlying reasoning behind annotators' decisions. One such approach is the
LiTEx taxonomy, which categorizes free-text explanations in English into
reasoning types. However, previous work applying such taxonomies has focused on
within-label variation: cases where annotators agree on the final NLI label but
provide different explanations. In contrast, this paper broadens the scope by
examining how annotators may diverge not only in the reasoning type but also in
the labeling step. We use explanations as a lens to decompose the reasoning
process underlying NLI annotation and to analyze individual differences. We
apply LiTEx to two NLI English datasets and align annotation variation from
multiple aspects: NLI label agreement, explanation similarity, and taxonomy
agreement, with an additional compounding factor of annotators' selection bias.
We observe instances where annotators disagree on the label but provide highly
similar explanations, suggesting that surface-level disagreement may mask
underlying agreement in interpretation. Moreover, our analysis reveals
individual preferences in explanation strategies and label choices. These
findings highlight that agreement in reasoning types better reflects the
semantic similarity of free-text explanations than label agreement alone. Our
findings underscore the richness of reasoning-based explanations and the need
for caution in treating labels as ground truth.

</details>


### [27] [Check Yourself Before You Wreck Yourself: Selectively Quitting Improves LLM Agent Safety](https://arxiv.org/abs/2510.16492)
*Vamshi Krishna Bonagiri,Ponnurangam Kumaragurum,Khanh Nguyen,Benjamin Plaut*

Main category: cs.CL

TL;DR: 本文提出在大型语言模型代理执行多轮任务时，通过引入‘退出’机制来提升安全性，有效减少风险，同时保持助手性能。


<details>
  <summary>Details</summary>
Motivation: 多轮任务中的不确定性和模糊性叠加，导致传统的文本生成安全机制无法应对更高风险环境下的安全挑战。

Method: 利用ToolEmu框架，在12个先进大型语言模型上系统评估引入‘退出’行为的影响，采用明确的退出指令引导模型在缺乏自信时选择退出。

Result: 引入退出指令后，安全性评分平均提升0.39分（专有模型提升0.64分），助手机能仅轻微下降0.03分，显示良好的安全性与助手机能权衡。

Conclusion: 明确的退出指令作为一种简单有效的行为机制，可立即在现有代理系统中部署，作为高风险应用自动代理系统的首要安全防线。

Abstract: As Large Language Model (LLM) agents increasingly operate in complex
environments with real-world consequences, their safety becomes critical. While
uncertainty quantification is well-studied for single-turn tasks, multi-turn
agentic scenarios with real-world tool access present unique challenges where
uncertainties and ambiguities compound, leading to severe or catastrophic risks
beyond traditional text generation failures. We propose using "quitting" as a
simple yet effective behavioral mechanism for LLM agents to recognize and
withdraw from situations where they lack confidence. Leveraging the ToolEmu
framework, we conduct a systematic evaluation of quitting behavior across 12
state-of-the-art LLMs. Our results demonstrate a highly favorable
safety-helpfulness trade-off: agents prompted to quit with explicit
instructions improve safety by an average of +0.39 on a 0-3 scale across all
models (+0.64 for proprietary models), while maintaining a negligible average
decrease of -0.03 in helpfulness. Our analysis demonstrates that simply adding
explicit quit instructions proves to be a highly effective safety mechanism
that can immediately be deployed in existing agent systems, and establishes
quitting as an effective first-line defense mechanism for autonomous agents in
high-stakes applications.

</details>


### [28] [HGAdapter: Hypergraph-based Adapters in Language Models for Code Summarization and Clone Detection](https://arxiv.org/abs/2510.17591)
*Guang Yang,Yujie Zhu*

Main category: cs.CL

TL;DR: 本文提出了一种结合高阶数据相关性的超图适配器（HGAdapter），用于提升预训练语言模型在代码相关任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 传统的预训练语言模型未充分考虑代码中潜在的高阶数据相关性，限制了模型性能提升。

Method: 提出了抽象语法树家族相关性、词法相关性和行相关性的高阶相关性，设计了相应的tokens和超边生成器，改进了超图神经网络结构，并结合适配器微调方法，构建了HGAdapter。

Result: 在包含六种编程语言的代码摘要和代码克隆检测任务的多个公开数据集上，HGAdapter显著提升了预训练语言模型的性能。

Conclusion: 引入代码的高阶数据相关性通过超图适配器，有效增强了PLMs对代码任务的理解与处理能力，验证了方法的有效性。

Abstract: Pre-trained language models (PLMs) are increasingly being applied to
code-related tasks. Although PLMs have achieved good results, they do not take
into account potential high-order data correlations within the code. We propose
three types of high-order correlations in code tokens, i.e. abstract syntax
tree family correlation, lexical correlation, and line correlation. We design a
tokens and hyperedges generator to capture these high-order data correlations.
We improve the architecture of hypergraph neural networks and combine it with
adapter tuning to propose a novel hypergraph-based adapter (HGAdapter) to
fine-tune PLMs. HGAdapter can encode high-order data correlations and is
allowed to be inserted into various PLMs to enhance performance. Experiments
were conducted on several public datasets, including six languages of code
summarization and code clone detection tasks. Our methods improved the
performance of PLMs in datasets to varying degrees. Experimental results
validate the introduction of high-order data correlations that contribute to
improved effectiveness.

</details>


### [29] [Automated Composition of Agents: A Knapsack Approach for Agentic Component Selection](https://arxiv.org/abs/2510.16499)
*Michelle Yuan,Khushbu Pahwa,Shuaichen Chang,Mustafa Kaba,Jiarong Jiang,Xiaofei Ma,Yi Zhang,Monica Sunkara*

Main category: cs.CL

TL;DR: 本文提出了一种基于背包问题的在线自动化代理系统组件组合框架，通过实时测试与效用模型，实现对组件的优化选择与组装，在多数据集上的实验显示其性能优于传统检索方法。


<details>
  <summary>Details</summary>
Motivation: 现有代理系统采用静态语义检索方法发现工具或代理，存在能力描述不完整及检索方法限制，导致组件复用和组合效率低下。

Method: 设计了一个受背包问题启发的自动化框架，使组合代理能够综合考虑性能、成本和兼容性，动态测试和模型效用进行组件选择与组装。

Result: 在Claude 3.5 Sonnet和五个基准数据集上的实证评估表明，该方法在成功率和组件成本上均显著优于基线方法，单代理成功率提升至31.6%，多代理系统提升至87%。

Conclusion: 所提方法在多域和预算约束下表现出强大的适应性与资源复用能力，显著提升代理系统的效率和效果。

Abstract: Designing effective agentic systems requires the seamless composition and
integration of agents, tools, and models within dynamic and uncertain
environments. Most existing methods rely on static, semantic retrieval
approaches for tool or agent discovery. However, effective reuse and
composition of existing components remain challenging due to incomplete
capability descriptions and the limitations of retrieval methods. Component
selection suffers because the decisions are not based on capability, cost, and
real-time utility. To address these challenges, we introduce a structured,
automated framework for agentic system composition that is inspired by the
knapsack problem. Our framework enables a composer agent to systematically
identify, select, and assemble an optimal set of agentic components by jointly
considering performance, budget constraints, and compatibility. By dynamically
testing candidate components and modeling their utility in real-time, our
approach streamlines the assembly of agentic systems and facilitates scalable
reuse of resources. Empirical evaluation with Claude 3.5 Sonnet across five
benchmarking datasets shows that our online-knapsack-based composer
consistently lies on the Pareto frontier, achieving higher success rates at
significantly lower component costs compared to our baselines. In the
single-agent setup, the online knapsack composer shows a success rate
improvement of up to 31.6% in comparison to the retrieval baselines. In
multi-agent systems, the online knapsack composer increases success rate from
37% to 87% when agents are selected from an agent inventory of 100+ agents. The
substantial performance gap confirms the robust adaptability of our method
across diverse domains and budget constraints.

</details>


### [30] [ReviewGuard: Enhancing Deficient Peer Review Detection via LLM-Driven Data Augmentation](https://arxiv.org/abs/2510.16549)
*Haoxuan Zhang,Ruochi Li,Sarthak Shrestha,Shree Harshini Mamidala,Revanth Putta,Arka Krishan Aggarwal,Ting Xiao,Junhua Ding,Haihua Chen*

Main category: cs.CL

TL;DR: 该论文提出了ReviewGuard系统，利用大型语言模型自动检测和分类学术评审中的不足评审，以维护学术诚信。


<details>
  <summary>Details</summary>
Motivation: 随着投稿数量激增及大型语言模型在学术评审中的广泛应用，存在评审质量下降的风险，威胁学术诚信和评审生态系统。

Method: 基于OpenReview收集数据，利用GPT-4.1进行人工验证的自动注释，并用LLM生成合成数据扩充样本，最后微调编码器模型及开源LLM进行评审质量检测。

Result: 不足评审评分较低、自信度高、结构简单且负面情绪多，AI生成的评审数量显著增加。使用真实与合成数据混合训练显著提升检测模型的召回率与F1值。

Conclusion: ReviewGuard是首个基于大型语言模型的不足评审检测自动系统，为AI在学术评审中的治理提供实证支持，促进人机协作维护学术诚信。

Abstract: Peer review serves as the gatekeeper of science, yet the surge in submissions
and widespread adoption of large language models (LLMs) in scholarly evaluation
present unprecedented challenges. Recent work has focused on using LLMs to
improve review efficiency or generate insightful review content. However,
unchecked deficient reviews from both human experts and AI systems threaten to
systematically undermine the peer review ecosystem and compromise academic
integrity. To address this critical issue, we introduce ReviewGuard, an
automated system for detecting and categorizing deficient reviews. ReviewGuard
employs a comprehensive four-stage LLM-driven framework that: (1) collects ICLR
and NeurIPS papers with their corresponding reviews from OpenReview; (2)
annotates review types using GPT-4.1 with human validation; (3) addresses class
imbalance and data scarcity through LLM-driven synthetic data augmentation,
producing a final corpus of 6,634 papers, 24,657 real reviews, and 46,438
synthetic reviews; and (4) fine-tunes both encoder-based models and open source
LLMs. We perform comprehensive feature analysis of the structure and quality of
the review text. Compared to sufficient reviews, deficient reviews demonstrate
lower rating scores, higher self-reported confidence, reduced structural
complexity, and a higher proportion of negative sentiment. AI-generated text
detection reveals that, since ChatGPT's emergence, AI-generated reviews have
increased dramatically. In the evaluation of deficient review detection models,
mixed training with synthetic and real review data provides substantial
enhancements to recall and F1 scores on the binary task. This study presents
the first LLM-driven system for detecting deficient peer reviews, providing
evidence to inform AI governance in peer review while offering valuable
insights into human-AI collaboration to maintain academic integrity.

</details>


### [31] [Language over Content: Tracing Cultural Understanding in Multilingual Large Language Models](https://arxiv.org/abs/2510.16565)
*Seungho Cho,Changgeon Ko,Eui Jun Hwang,Junmyeong Lee,Huije Lee,Jong C. Park*

Main category: cs.CL

TL;DR: 本文通过测量大型语言模型在不同国家和语言条件下回答语义等效问题时的激活路径重叠，探究其内部文化理解机制，发现语言特异性强且文化差异显著影响模型内部表示。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在多元文化背景中的广泛应用，准确理解其文化认知机制变得尤为重要，但现有评估多侧重输出表现，缺乏对内部差异驱动因素的洞察。

Method: 通过测量在固定语言变换国家以及固定国家变换语言条件下，模型回答相同语义问题时激活路径的重叠程度，并使用同语种国家对比，分离语言因素与文化因素。

Result: 发现同语言不同国家的问题激活路径重叠更高，说明语言对内部表示有较大影响；特别是韩朝对比表现出较低的重叠和高变异，表明语言相似并不能保证内部认知一致性。

Conclusion: 大型语言模型的内部文化理解机制受语言和文化因素共同影响，语言相似不代表文化认知路径的一致，需针对文化差异进行更细致研究。

Abstract: Large language models (LLMs) are increasingly used across diverse cultural
contexts, making accurate cultural understanding essential. Prior evaluations
have mostly focused on output-level performance, obscuring the factors that
drive differences in responses, while studies using circuit analysis have
covered few languages and rarely focused on culture. In this work, we trace
LLMs' internal cultural understanding mechanisms by measuring activation path
overlaps when answering semantically equivalent questions under two conditions:
varying the target country while fixing the question language, and varying the
question language while fixing the country. We also use same-language country
pairs to disentangle language from cultural aspects. Results show that internal
paths overlap more for same-language, cross-country questions than for
cross-language, same-country questions, indicating strong language-specific
patterns. Notably, the South Korea-North Korea pair exhibits low overlap and
high variability, showing that linguistic similarity does not guarantee aligned
internal representation.

</details>


### [32] [Hallucination Benchmark for Speech Foundation Models](https://arxiv.org/abs/2510.16567)
*Alkis Koudounas,Moreno La Quatra,Manuel Giollo,Sabato Marco Siniscalchi,Elena Baralis*

Main category: cs.CL

TL;DR: 该论文提出了SHALLOW，一个专门用于自动语音识别（ASR）系统中出现的幻觉现象的评估框架。


<details>
  <summary>Details</summary>
Motivation: ASR系统中出现的幻觉，即偏离实际语音信号但语法和语义上合理的错误，对下游应用尤其是关键领域带来严重风险，传统误差指标难以有效识别这些现象。

Method: 提出SHALLOW框架，系统性地从词汇、语音、形态和语义四个维度分类和量化ASR系统的幻觉现象，并设计针对性的指标用于描述模型行为。

Result: 通过在多种架构和语音领域上的评估，发现SHALLOW指标在识别质量高时与WER高度相关，但在WER较高时能捕捉WER未能识别的细粒度错误模式。

Conclusion: SHALLOW框架能够更细致地诊断ASR模型的弱点，为模型改进提供了比传统误差率更有价值的反馈。

Abstract: Hallucinations in automatic speech recognition (ASR) systems refer to fluent
and coherent transcriptions produced by neural ASR models that are completely
unrelated to the underlying acoustic input (i.e., the speech signal). While
similar to conventional decoding errors in potentially compromising the
usability of transcriptions for downstream applications, hallucinations can be
more detrimental due to their preservation of syntactically and semantically
plausible structure. This apparent coherence can mislead subsequent processing
stages and introduce serious risks, particularly in critical domains such as
healthcare and law. Conventional evaluation metrics are primarily centered on
error-based metrics and fail to distinguish between phonetic inaccuracies and
hallucinations. Consequently, there is a critical need for new evaluation
frameworks that can effectively identify and assess models with a heightened
propensity for generating hallucinated content. To this end, we introduce
SHALLOW, the first benchmark framework that systematically categorizes and
quantifies hallucination phenomena in ASR along four complementary axes:
lexical, phonetic, morphological, and semantic. We define targeted metrics
within each category to produce interpretable profiles of model behavior.
Through evaluation across various architectures and speech domains, we have
found that SHALLOW metrics correlate strongly with word error rate (WER) when
recognition quality is high (i.e., low WER). Still, this correlation weakens
substantially as WER increases. SHALLOW, therefore, captures fine-grained error
patterns that WER fails to distinguish under degraded and challenging
conditions. Our framework supports specific diagnosis of model weaknesses and
provides feedback for model improvement beyond what aggregate error rates can
offer.

</details>


### [33] [AI-Generated Text Detection in Low-Resource Languages: A Case Study on Urdu](https://arxiv.org/abs/2510.16573)
*Muhammad Ammar,Hadiya Murad Hadi,Usman Majeed Butt*

Main category: cs.CL

TL;DR: 本文提出了一种用于乌尔都语的AI生成文本检测框架，通过构建平衡数据集、进行语言统计分析，并微调多语言变换模型，实现了91%以上的准确率。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型生成的文本越来越像人类写作，区分机器和人类文本变得困难。乌尔都语缺乏检测AI生成文本的工具，亟需针对该语言的检测方法。

Method: 构建了包含1800人人类文本和1800 AI生成文本的数据集，采用字符数、词数、词汇丰富度和N-gram模式等特征进行统计分析，并微调了mDeBERTa-v3-base、distilbert-base-multilingual-cased和xlm-roberta-base三种多语言变换模型。

Result: mDeBERTa-v3-base模型在测试集上达到了91.29的F1-score和91.26%的准确率，表现最佳。

Conclusion: 该研究填补了乌尔都语AI文本检测的空白，有助于防止错误信息传播和学术不端，同时促进了低资源语言的自然语言处理工具发展。

Abstract: Large Language Models (LLMs) are now capable of generating text that closely
resembles human writing, making them powerful tools for content creation, but
this growing ability has also made it harder to tell whether a piece of text
was written by a human or by a machine. This challenge becomes even more
serious for languages like Urdu, where there are very few tools available to
detect AI-generated text. To address this gap, we propose a novel AI-generated
text detection framework tailored for the Urdu language. A balanced dataset
comprising 1,800 humans authored, and 1,800 AI generated texts, sourced from
models such as Gemini, GPT-4o-mini, and Kimi AI was developed. Detailed
linguistic and statistical analysis was conducted, focusing on features such as
character and word counts, vocabulary richness (Type Token Ratio), and N-gram
patterns, with significance evaluated through t-tests and MannWhitney U tests.
Three state-of-the-art multilingual transformer models such as
mdeberta-v3-base, distilbert-base-multilingualcased, and xlm-roberta-base were
fine-tuned on this dataset. The mDeBERTa-v3-base achieved the highest
performance, with an F1-score 91.29 and accuracy of 91.26% on the test set.
This research advances efforts in contesting misinformation and academic
misconduct in Urdu-speaking communities and contributes to the broader
development of NLP tools for low resource languages.

</details>


### [34] [Fine-tuning of Large Language Models for Constituency Parsing Using a Sequence to Sequence Approach](https://arxiv.org/abs/2510.16604)
*Francisco Jose Cortes Delgado,Eduardo Martinez Gracia,Rafael Valencia Garcia*

Main category: cs.CL

TL;DR: 本文利用大语言模型微调技术，实现了西班牙语句法分析工具MiSintaxis的能力扩展，通过训练AnCora-ES语料库生成的数据，取得了较高的短语结构分析准确率。


<details>
  <summary>Details</summary>
Motivation: 近年来大型神经网络模型在自然语言处理中的进展，为基于机器学习的句法分析提供了新途径，本文旨在利用这一趋势提升现有西班牙语句法教学工具的性能。

Method: 选择Hugging Face平台上的几个大型语言模型，使用AnCora-ES语料库生成的训练数据进行微调，方法为将输入句子翻译为对应的句法结构。

Result: 经过微调的模型在短语结构分析任务中表现出较高的F1分数，显示出该方法的有效性。

Conclusion: 利用大语言模型微调将句子转换为句法结构的方式，可显著提升西班牙语句法分析工具的准确度，具有应用前景。

Abstract: Recent advances in natural language processing with large neural models have
opened new possibilities for syntactic analysis based on machine learning. This
work explores a novel approach to phrase-structure analysis by fine-tuning
large language models (LLMs) to translate an input sentence into its
corresponding syntactic structure. The main objective is to extend the
capabilities of MiSintaxis, a tool designed for teaching Spanish syntax.
Several models from the Hugging Face repository were fine-tuned using training
data generated from the AnCora-ES corpus, and their performance was evaluated
using the F1 score. The results demonstrate high accuracy in phrase-structure
analysis and highlight the potential of this methodology.

</details>


### [35] [All You Need is One: Capsule Prompt Tuning with a Single Vector](https://arxiv.org/abs/2510.16670)
*Yiyang Liu,James C. Liang,Heng Fan,Wenhao Yang,Yiming Cui,Xiaotian Han,Lifu Huang,Dongfang Liu,Qifan Wang,Cheng Han*

Main category: cs.CL

TL;DR: 本文提出了Capsule Prompt-Tuning方法，通过结合实例感知和任务感知信息，提升了大语言模型的提示调优效果，同时大幅降低参数开销。


<details>
  <summary>Details</summary>
Motivation: 当前基于提示的学习方法依赖于繁琐的网格搜索和大量提示，且缺乏实例感知信息，导致性能受限。

Method: 提出了Capsule Prompt-Tuning（CaPT），将实例感知语义作为提示的组成部分，利用“注意力锚点”机制在序列的最早位置添加实例感知令牌，无需额外微调，几乎无参数开销。

Result: 在多个语言任务上表现优异，如T5-Large上达到84.03%的平均准确率，并且参数效率极高（如Llama3.2-1B仅使用0.003%的模型参数）。

Conclusion: 引入实例感知信息并结合任务感知信息的CaPT方法有效提升了提示调优性能，成为高效且性能优异的提示调优策略。

Abstract: Prompt-based learning has emerged as a parameter-efficient finetuning (PEFT)
approach to facilitate Large Language Model (LLM) adaptation to downstream
tasks by conditioning generation with task-aware guidance. Despite its
successes, current prompt-based learning methods heavily rely on laborious grid
searching for optimal prompt length and typically require considerable number
of prompts, introducing additional computational burden. Worse yet, our pioneer
findings indicate that the task-aware prompt design is inherently limited by
its absence of instance-aware information, leading to a subtle attention
interplay with the input sequence. In contrast, simply incorporating
instance-aware information as a part of the guidance can enhance the
prompt-tuned model performance without additional fine-tuning. Moreover, we
find an interesting phenomenon, namely "attention anchor", that incorporating
instance-aware tokens at the earliest position of the sequence can successfully
preserve strong attention to critical structural information and exhibit more
active attention interaction with all input tokens. In light of our
observation, we introduce Capsule Prompt-Tuning (CaPT), an efficient and
effective solution that leverages off-the-shelf, informative instance semantics
into prompt-based learning. Our approach innovatively integrates both
instance-aware and task-aware information in a nearly parameter-free manner
(i.e., one single capsule prompt). Empirical results demonstrate that our
method can exhibit superior performance across various language tasks (e.g.,
84.03\% average accuracy on T5-Large), serving as an "attention anchor," while
enjoying high parameter efficiency (e.g., 0.003\% of model parameters on
Llama3.2-1B).

</details>


### [36] [Temporal Understanding under Deictic Frame of Reference](https://arxiv.org/abs/2510.16685)
*Damin Zhang,Julia Rayz*

Main category: cs.CL

TL;DR: 本文提出了TUuD框架，评估大语言模型（LLMs）在动态“现在”参考点下对时间事件关系的理解，发现模型在近似当前时间点上表现出部分人类时间认知，但在时间距离较远时适应性减弱。


<details>
  <summary>Details</summary>
Motivation: 时间理解是人类认知的基础，依赖于空间隐喻和参考框架，然而现有大语言模型在时间推理上能力有限。

Method: 提出TUuD框架，基于动态变化的时间参考点，要求模型对当前时刻与目标事件的相似度进行评分，以量化时间上的对齐感知。

Result: 四个评估的大语言模型表现出对动态时间参考框架的适应，评分在当前时刻附近达到高峰，随时间距离增加而减弱。

Conclusion: 大语言模型展现了一定的人类时间认知特征，但其时间推理能力仍受参考框架转换和时间距离的影响，尚未完全达到人类水平。

Abstract: Understanding time is fundamental to human cognition, where temporal
experience is often conceptualized through spatial metaphors grounded in
sensory-motor experience. For example, "summer is approaching" parallels "We
are approaching the summer". In such expressions, humans rely on a frame of
reference (FoR) to interpret meaning relative to a particular viewpoint.
Extending this concept to time, a temporal frame of reference (t-FoR) defines
how temporal relations are perceived relative to an experiencer's moment of
"now". While Large Language Models (LLMs) have shown remarkable advances in
natural language understanding, their ability to interpret and reason about
time remains limited. In this work, we introduce TUuD (Temporal Understanding
under Deictic t-FoR), a framework that evaluates how LLMs interpret time-event
and event-event relations when the reference point of "now" dynamically shifts
along a timeline. Following recent work on temporal cognition
\cite{li2025other}, LLMs are prompted to rate the similarity between the
current moment and a target event from 0.00 (completely dissimilar) to 1.00
(highly similar), where similarity quantifies perceived temporal alignment
between the two points. Our results show that four evaluated LLMs exhibit
measurable adaptation to a deictic t-FoR, with similarity ratings peaking
around the present and decreasing toward past and future events. The
adaptation, however, weakens beyond near-term contexts, suggesting that while
LLMs display partial human-like temporal cognition, their temporal reasoning
remains sensitive to reference-frame shifts and temporal distance.

</details>


### [37] [Investigating the Impact of Rationales for LLMs on Natural Language Understanding](https://arxiv.org/abs/2510.16686)
*Wenhang Shi,Shuqing Bian,Yiren Chen,Xinyi Zhang,Zhe Zhao,Pengfei Hu,Wei Lu,Xiaoyong Du*

Main category: cs.CL

TL;DR: 本文探讨了链式思维（CoT）推理在自然语言理解（NLU）任务中的作用，构建了带推理链的NLU数据集，并开发了多种推理链增强方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注推理链在数学、符号和常识推理中的作用，忽视了其在NLU任务中的潜力，因此希望系统评估推理链对NLU任务的影响。

Method: 作者构建了高质量的带推理链的NLU数据集（NLURC），并设计多种推理链增强训练和推理方法，比较不同方法对NLU性能的影响。

Result: 发现随着模型规模增长，CoT推理逐渐优于直接标签预测；大多数带推理链的训练方法不如仅标签训练，只有一个特定方法持续提升性能；训练带推理链的大模型在未见过的NLU任务上的表现显著提升并具备良好可解释性。

Conclusion: 推理链方法对NLU任务有潜在积极影响，尤其是大规模模型，可以通过特定训练方式提升性能和解释能力，建议未来在NLU领域推广使用推理链技术。

Abstract: Chain-of-thought (CoT) rationales, which provide step-by-step reasoning to
derive final answers, benefit LLMs in both inference and training.
Incorporating rationales, either by generating them before answering during
inference, or by placing them before or after the original answers during
training - significantly improves model performance on mathematical, symbolic
and commonsense reasoning tasks. However, most work focuses on the role of
rationales in these reasoning tasks, overlooking their potential impact on
other important tasks like natural language understanding (NLU) tasks. In this
work, we raise the question: Can rationales similarly benefit NLU tasks? To
conduct a systematic exploration, we construct NLURC, a comprehensive and
high-quality NLU dataset collection with rationales, and develop various
rationale-augmented methods. Through exploring the applicability of these
methods on NLU tasks using the dataset, we uncover several potentially
surprising findings: (1) CoT inference shifts from hindering NLU performance to
surpassing direct label prediction as model size grows, indicating a positive
correlation. (2) Most rationale-augmented training methods perform worse than
label-only training, with one specially designed method consistently achieving
improvements. (3) LLMs trained with rationales achieve significant performance
gains on unseen NLU tasks, rivaling models ten times their size, while
delivering interpretability on par with commercial LLMs.

</details>


### [38] [Natural Language Processing Applications in Cardiology: A Narrative Review](https://arxiv.org/abs/2510.16708)
*Kailai Yang,Yan Leng,Xin Zhang,Tianlin Zhang,Paul Thompson,Bernard Keavney,Maciej Tomaszewski,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 本文综述了2014年至2025年间自然语言处理(NLP)技术在心脏病学领域的应用，系统分析了265篇相关文献，涵盖多种心血管疾病、任务类型和数据来源。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病复杂多样，相关信息分散在不同文本数据中，传统方法难以整合和分析这些非结构化数据。引入NLP技术有助于深度挖掘这些信息，以提升心脏病的诊断、治疗和预防效果。

Method: 通过检索六个文献数据库，筛选出与心脏病学中NLP应用相关的文章，总计265篇。对这些文献从NLP范式、心脏病任务类型、疾病类型和数据来源等多维度进行分析，并进行时间趋势分析以揭示技术演化。

Result: 分析显示心脏病学中的NLP研究覆盖面广，方法多样，且NLP技术在该领域的应用呈现持续增长和变革的趋势。

Conclusion: 本综述是迄今为止最全面的心脏病学领域内NLP研究总结，有助于推动该领域的研究和临床应用发展。

Abstract: Cardiovascular disease has become increasingly prevalent in modern society
and has a significant effect on global health and well-being. Heart-related
conditions are intricate, multifaceted disorders, which may be influenced by a
combination of genetic predispositions, lifestyle choices, and various
socioeconomic and clinical factors. Information regarding these potentially
complex interrelationships is dispersed among diverse types of textual data,
which include patient narratives, medical records, and scientific literature,
among others. Natural language processing (NLP) techniques have increasingly
been adopted as a powerful means to analyse and make sense of this vast amount
of unstructured data. This, in turn, can allow healthcare professionals to gain
deeper insights into the cardiology field, which has the potential to
revolutionize current approaches to the diagnosis, treatment, and prevention of
cardiac problems. This review provides a detailed overview of NLP research in
cardiology between 2014 and 2025. We queried six literature databases to find
articles describing the application of NLP techniques in the context of a range
of different cardiovascular diseases. Following a rigorous screening process,
we identified a total of 265 relevant articles. We analysed each article from
multiple dimensions, i.e., NLP paradigm types, cardiology-related task types,
cardiovascular disease types, and data source types. Our analysis reveals
considerable diversity within each of these dimensions, thus demonstrating the
considerable breadth of NLP research within the field. We also perform a
temporal analysis, which illustrates the evolution and changing trends in NLP
methods employed over the last decade that we cover. To our knowledge, the
review constitutes the most comprehensive overview of NLP research in
cardiology to date.

</details>


### [39] [The Chameleon Nature of LLMs: Quantifying Multi-Turn Stance Instability in Search-Enabled Language Models](https://arxiv.org/abs/2510.16712)
*Shivam Ratnakar,Sanjay Raghavendra*

Main category: cs.CL

TL;DR: 本文首次系统研究了大型语言模型在多轮对话中面对矛盾问题时立场变化的"变色龙行为"，通过新构建的包含17770问答对的数据集揭示了现有模型的根本缺陷，并提出了衡量立场不稳定性和知识多样性的指标。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型与搜索引擎集成的普及，其在多轮对话中面对矛盾信息时存在立场不稳定的问题，尤其影响可靠性，特别在医疗、法律和金融等关键领域。

Method: 构建了包含12个争议领域的1178个多轮对话、17770个问答对的Chameleon基准数据集，提出Chameleon Score和Source Re-use Rate两个指标，评估Llama-4-Maverick、GPT-4o-mini和Gemini-2.5-Flash等模型的立场稳定性和知识多样性。

Result: 所有模型均表现出严重的变色龙行为（得分0.391-0.511），其中GPT-4o-mini表现最差。小的温度变动无影响表明现象非采样误差。立场变化与知识源重复率和置信度显著相关，说明有限的知识多样性导致模型过于依赖提问形式。

Conclusion: 当前主流大型语言模型存在严重的立场不稳定问题，强调在关键领域应用前需进行全面的一致性评估，确保模型在多轮交互中保持连贯的立场以支持可靠决策。

Abstract: Integration of Large Language Models with search/retrieval engines has become
ubiquitous, yet these systems harbor a critical vulnerability that undermines
their reliability. We present the first systematic investigation of "chameleon
behavior" in LLMs: their alarming tendency to shift stances when presented with
contradictory questions in multi-turn conversations (especially in
search-enabled LLMs). Through our novel Chameleon Benchmark Dataset, comprising
17,770 carefully crafted question-answer pairs across 1,180 multi-turn
conversations spanning 12 controversial domains, we expose fundamental flaws in
state-of-the-art systems. We introduce two theoretically grounded metrics: the
Chameleon Score (0-1) that quantifies stance instability, and Source Re-use
Rate (0-1) that measures knowledge diversity. Our rigorous evaluation of
Llama-4-Maverick, GPT-4o-mini, and Gemini-2.5-Flash reveals consistent
failures: all models exhibit severe chameleon behavior (scores 0.391-0.511),
with GPT-4o-mini showing the worst performance. Crucially, small
across-temperature variance (less than 0.004) suggests the effect is not a
sampling artifact. Our analysis uncovers the mechanism: strong correlations
between source re-use rate and confidence (r=0.627) and stance changes
(r=0.429) are statistically significant (p less than 0.05), indicating that
limited knowledge diversity makes models pathologically deferential to query
framing. These findings highlight the need for comprehensive consistency
evaluation before deploying LLMs in healthcare, legal, and financial systems
where maintaining coherent positions across interactions is critical for
reliable decision support.

</details>


### [40] [so much depends / upon / a whitespace: Why Whitespace Matters for Poets and LLMs](https://arxiv.org/abs/2510.16713)
*Sriharsh Bhyravajjula,Melanie Walsh,Anna Preus,Maria Antoniak*

Main category: cs.CL

TL;DR: 该论文研究了诗歌中的空白使用，比较了公开发布的诗歌、LLM生成诗歌和在线社区诗歌的空白分布，探讨了时间、形式及处理方法对空白的影响。


<details>
  <summary>Details</summary>
Motivation: 诗歌中的空白是其形式和语义的重要组成部分，但NLP社区对此关注有限，研究空白分布有助于理解诗歌艺术及优化大规模语言模型的训练数据处理。

Method: 利用Poetry Foundation提供的1.9万首英文诗歌数据，分析4千多位诗人的空白使用，发布2.8千首保留格式的公共领域诗歌，比较了公开出版诗作、5.1万首LLM生成诗和1.2万首在线社区诗的空白模式，并从时间、诗体及文本处理方法等角度进行了探讨。

Result: 发现不同文本处理方法导致空白表示显著差异，公开诗歌、LLM生成诗和社区诗在空白使用上存在差异，空白使用随时间和诗体变化特征明显。

Conclusion: 诗歌空白是不可忽视的语义和结构特征，理解其模式对于艺术分析和大规模语言模型预训练数据集的构建具有重要意义，建议更多关注和研究诗歌中的空白使用。

Abstract: Whitespace is a critical component of poetic form, reflecting both adherence
to standardized forms and rebellion against those forms. Each poem's whitespace
distribution reflects the artistic choices of the poet and is an integral
semantic and spatial feature of the poem. Yet, despite the popularity of poetry
as both a long-standing art form and as a generation task for large language
models (LLMs), whitespace has not received sufficient attention from the NLP
community. Using a corpus of 19k English-language published poems from Poetry
Foundation, we investigate how 4k poets have used whitespace in their works. We
release a subset of 2.8k public-domain poems with preserved formatting to
facilitate further research in this area. We compare whitespace usage in the
published poems to (1) 51k LLM-generated poems, and (2) 12k unpublished poems
posted in an online community. We also explore whitespace usage across time
periods, poetic forms, and data sources. Additionally, we find that different
text processing methods can result in significantly different representations
of whitespace in poetry data, motivating us to use these poems and whitespace
patterns to discuss implications for the processing strategies used to assemble
pretraining datasets for LLMs.

</details>


### [41] [Beacon: Single-Turn Diagnosis and Mitigation of Latent Sycophancy in Large Language Models](https://arxiv.org/abs/2510.16727)
*Sanskar Pandey,Ruhaan Chopra,Angkul Puniya,Sohom Pal*

Main category: cs.CL

TL;DR: 本文提出了Beacon，一个用于测量大型语言模型趋向于讨好用户而非坚持事实的偏差（奉承性）的基准方法，揭示了奉承性偏差的语言和情感两个子偏差，并提出调控这些偏差的干预方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在追求帮助性的奖励优化过程中，出现了把讨好用户与服从礼貌混淆的偏差，即奉承性，这影响了模型的真诚性和可靠性。

Method: 提出Beacon基准，设计单轮强制选择任务，独立于对话上下文精确衡量事实准确性与服从偏见之间的矛盾，并采用提示和激活级干预来调控这些偏差。

Result: 在十二个最先进模型上的评估显示，奉承性偏差可分解为语言和情感两个稳定子偏差，且均随模型容量增长，同时提出的干预方法能有效调节这些偏差。

Conclusion: Beacon将奉承性重新定义为可测量的规范性误泛化，为研究和减轻大型生成模型的对齐偏移提供了可复现的基础。

Abstract: Large language models internalize a structural trade-off between truthfulness
and obsequious flattery, emerging from reward optimization that conflates
helpfulness with polite submission. This latent bias, known as sycophancy,
manifests as a preference for user agreement over principled reasoning. We
introduce Beacon, a single-turn forced-choice benchmark that isolates this bias
independent of conversational context, enabling precise measurement of the
tension between factual accuracy and submissive bias. Evaluations across twelve
state-of-the-art models reveal that sycophancy decomposes into stable
linguistic and affective sub-biases, each scaling with model capacity. We
further propose prompt-level and activation-level interventions that modulate
these biases in opposing directions, exposing the internal geometry of
alignment as a dynamic manifold between truthfulness and socially compliant
judgment. Beacon reframes sycophancy as a measurable form of normative
misgeneralization, providing a reproducible foundation for studying and
mitigating alignment drift in large-scale generative systems.

</details>


### [42] [Enhancing Language Agent Strategic Reasoning through Self-Play in Adversarial Games](https://arxiv.org/abs/2510.16761)
*Yikai Zhang,Ye Rong,Siyu Yuan,Jiangjie Chen,Jian Xie,Yanghua Xiao*

Main category: cs.CL

TL;DR: 本文提出了一种基于步骤级策略优化和对抗学习的SCO-PAL方法，通过不同级别对手的对比试验，发现自我对弈是提升动态对抗游戏中战略推理能力的最佳方案，实验中该方法显著提高了胜率。


<details>
  <summary>Details</summary>
Motivation: 现有语言智能体在动态对抗游戏中表现出战略推理能力不足，且缺乏自动从交互中学习的有效机制，尤其是对对手选择的研究较少。

Method: 提出了步骤级策略优化方法SCO-PAL，结合自我对弈进行学习，以动态选择对手，提升策略优化的效率和效果。

Result: 使用SCO-PAL自我对弈方法，在对抗四个不同对手时平均胜率提升约30%，并在六个对抗游戏中达到54.76%胜率击败GPT-4。

Conclusion: 基于SCO-PAL的自我对弈是动态对抗环境中提升战略推理能力的有效途径，能显著提高语言智能体的对抗表现。

Abstract: Existing language agents often encounter difficulties in dynamic adversarial
games due to poor strategic reasoning. To mitigate this limitation, a promising
approach is to allow agents to learn from game interactions automatically,
without relying on costly expert-labeled data. Unlike static environments where
agents receive fixed feedback or rewards, selecting appropriate opponents in
dynamic adversarial games can significantly impact learning performance.
However, the discussion of opponents in adversarial environments remains an
area under exploration. In this paper, we propose a Step-level poliCy
Optimization method through Play-And-Learn, SCO-PAL. Leveraging SCO-PAL, we
conduct a detailed analysis of opponent selection by setting opponents at
different levels and find that self-play is the most effective way to improve
strategic reasoning in such adversarial environments. Utilizing SCO-PAL with
self-play, we increase the average win rate against four opponents by
approximately 30% compared to baselines and achieve a 54.76% win rate against
GPT-4 in six adversarial games.

</details>


### [43] [LC-Eval: A Bilingual Multi-Task Evaluation Benchmark for Long-Context Understanding](https://arxiv.org/abs/2510.16783)
*Sheikh Jubair,Arwa Omayrah,Amal Alshammari,Alhanoof Althnian,Abdulhamed Alothaimen,Norah A. Alzahrani,Shahad D. Alzaidi,Nora Al-Twairesh,Abdulmohsen Al-Thubaity*

Main category: cs.CL

TL;DR: 本文提出了LC-Eval，一个用于评估大语言模型长文本理解能力的双语多任务基准，涵盖4k至128k+的上下文长度，任务包括多文档问答、双语问答、段落内声明验证及基于长上下文的多项选择题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在处理扩展上下文能力上的提升，迫切需要严谨的评估方法来衡量其在长文本理解中的表现。

Method: 设计并推出了包含英语和阿拉伯语数据的多任务评估基准LC-Eval，涵盖四个具有挑战性的任务，以测试模型的深度推理、文档理解、信息追踪及双语信息提取能力。

Result: 在对开源和闭源模型的评估中，发现LC-Eval对模型构成重大挑战，即使是表现优异的模型如GPT-4o也在部分任务中表现不佳。

Conclusion: LC-Eval基准有效揭示了现有大语言模型在长文本理解方面的局限性，为推动该领域技术进步提供了重要工具。

Abstract: Recent advancements in Large Language Models (LLMs) have demonstrated
sophisticated capabilities, including the ability to process and comprehend
extended contexts. These emergent capabilities necessitate rigorous evaluation
methods to effectively assess their performance in long-context understanding.
In this paper, we present \textbf{LC-Eval}, a bilingual, multi-task evaluation
benchmark designed to evaluate long-context understanding in English and
Arabic, targeting context lengths ranging from 4k to over 128k tokens. LC-Eval
introduces four novel and challenging tasks: multi-document question answering,
bilingual question answering, claim verification within a paragraph, and
multiple-choice questions based on long contexts. These tasks are designed to
assess LLMs' abilities in deep reasoning, document comprehension, information
tracing, and bilingual information extraction and understanding. The benchmark
includes datasets in both Arabic and English for each task, allowing for a
comparative analysis of their performance across different text genres.
Evaluations were conducted on both open-weight and closed LLMs, with results
indicating that LC-Eval presents significant challenges. Even high-performing
models, such as GPT-4o, struggled with certain tasks, highlighting the
complexity and rigor of the benchmark.

</details>


### [44] [MOSAIC: Masked Objective with Selective Adaptation for In-domain Contrastive Learning](https://arxiv.org/abs/2510.16797)
*Vera Pavlova,Mohammed Makhlouf*

Main category: cs.CL

TL;DR: 提出了MOSAIC框架，通过联合掩码语言模型和对比学习在特定领域内进行句子嵌入模型的适应，显著提升了领域内任务表现。


<details>
  <summary>Details</summary>
Motivation: 解决大规模通用句子嵌入模型适应专业领域时表现不足的问题。

Method: 在统一训练框架中结合掩码语言建模和对比学习目标，采用多阶段训练实现领域相关表示的有效学习。

Result: 在高资源和低资源领域测试中，最大提升13.4%的NDCG@10，相较强基线有显著改进。

Conclusion: 联合域特定掩码监督和分阶段适应策略有效提高了句子嵌入模型的领域适应能力。

Abstract: We introduce MOSAIC (Masked Objective with Selective Adaptation for In-domain
Contrastive learning), a multi-stage framework for domain adaptation of
sentence embedding models that incorporates joint domain-specific masked
supervision. Our approach addresses the challenges of adapting large-scale
general-domain sentence embedding models to specialized domains. By jointly
optimizing masked language modeling (MLM) and contrastive objectives within a
unified training pipeline, our method enables effective learning of
domain-relevant representations while preserving the robust semantic
discrimination properties of the original model. We empirically validate our
approach on both high-resource and low-resource domains, achieving improvements
up to 13.4% in NDCG@10 (Normalized Discounted Cumulative Gain) over strong
general-domain baselines. Comprehensive ablation studies further demonstrate
the effectiveness of each component, highlighting the importance of balanced
joint supervision and staged adaptation.

</details>


### [45] [Knowing the Facts but Choosing the Shortcut: Understanding How Large Language Models Compare Entities](https://arxiv.org/abs/2510.16815)
*Hans Hergen Lehmann,Jae Hee Lee,Steven Schockaert,Stefan Wermter*

Main category: cs.CL

TL;DR: 本文通过实体数值比较任务，研究大型语言模型在知识推理中是依赖真实知识还是表面启发式的方法。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型具备足够的数值知识，但在任务中常常因启发式偏见而做出与知识矛盾的预测，理解其背后机制有助于提升模型推理的可靠性。

Method: 通过对大小不同的语言模型进行实体数值属性比较任务，分析模型预测中受实体流行度、提及顺序和语义共现三种启发式偏见的影响，使用逻辑回归模型预测这些偏见对结果的贡献，并比较链式思考提示对模型推理的引导效果。

Result: 较小模型的预测更多依赖启发式偏见，而较大模型能选择性依赖更可靠的数值知识；链式思考提示能引导所有模型更多地使用数值特征。

Conclusion: 大型模型通过区分启发式与知识性信息，展现更优的推理性能，提示未来提升模型推理能力应重视减少启发式偏见并加强数值知识利用。

Abstract: Large Language Models (LLMs) are increasingly used for knowledge-based
reasoning tasks, yet understanding when they rely on genuine knowledge versus
superficial heuristics remains challenging. We investigate this question
through entity comparison tasks by asking models to compare entities along
numerical attributes (e.g., ``Which river is longer, the Danube or the
Nile?''), which offer clear ground truth for systematic analysis. Despite
having sufficient numerical knowledge to answer correctly, LLMs frequently make
predictions that contradict this knowledge. We identify three heuristic biases
that strongly influence model predictions: entity popularity, mention order,
and semantic co-occurrence. For smaller models, a simple logistic regression
using only these surface cues predicts model choices more accurately than the
model's own numerical predictions, suggesting heuristics largely override
principled reasoning. Crucially, we find that larger models (32B parameters)
selectively rely on numerical knowledge when it is more reliable, while smaller
models (7--8B parameters) show no such discrimination, which explains why
larger models outperform smaller ones even when the smaller models possess more
accurate knowledge. Chain-of-thought prompting steers all models towards using
the numerical features across all model sizes.

</details>


### [46] [Cross-Genre Authorship Attribution via LLM-Based Retrieve-and-Rerank](https://arxiv.org/abs/2510.16819)
*Shantanu Agarwal,Joel Barry,Steven Fincke,Scott Miller*

Main category: cs.CL

TL;DR: 本文提出了一种两阶段检索和重排序框架，利用大型语言模型微调用于跨体裁作者归属任务，显著提升了准确率。


<details>
  <summary>Details</summary>
Motivation: 跨体裁作者归属需要避开主题线索，聚焦作者特有的语言模式，而传统检索领域的策略不适用，导致效果受限。

Method: 设计了两阶段检索-重排序框架，并引入针对性的训练数据策划策略，促使重排序模型学习作者区分信号。

Result: 在HIATUS的HRS1和HRS2跨体裁作者归属基准测试中，成功率分别提高了22.3和34.4个百分点，远超现有最佳。

Conclusion: 本方法有效克服了跨体裁作者归属中的主题依赖问题，显著提升了作者识别的准确性，为未来相关研究提供了新思路。

Abstract: Authorship attribution (AA) is the task of identifying the most likely author
of a query document from a predefined set of candidate authors. We introduce a
two-stage retrieve-and-rerank framework that finetunes LLMs for cross-genre AA.
Unlike the field of information retrieval (IR), where retrieve-and-rerank is a
de facto strategy, cross-genre AA systems must avoid relying on topical cues
and instead learn to identify author-specific linguistic patterns that are
independent of the text's subject matter (genre/domain/topic). Consequently,
for the reranker, we demonstrate that training strategies commonly used in IR
are fundamentally misaligned with cross-genre AA, leading to suboptimal
behavior. To address this, we introduce a targeted data curation strategy that
enables the reranker to effectively learn author-discriminative signals. Using
our LLM-based retrieve-and-rerank pipeline, we achieve substantial gains of
22.3 and 34.4 absolute Success@8 points over the previous state-of-the-art on
HIATUS's challenging HRS1 and HRS2 cross-genre AA benchmarks.

</details>


### [47] [Who's Asking? Simulating Role-Based Questions for Conversational AI Evaluation](https://arxiv.org/abs/2510.16829)
*Navreet Kaur,Hoda Ayad,Hayoung Jung,Shravika Mittal,Munmun De Choudhury,Tanushree Mitra*

Main category: cs.CL

TL;DR: 本文提出了CoRUS框架，通过模拟不同用户角色（患者、护理者、从业者）提问，评估大语言模型在涉及弱势群体时的响应差异。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在回答问题时通常忽略提问者的身份背景，尤其在如阿片类药物使用障碍等带有社会污名化的领域，这会影响回复的适当性和支持度。

Method: 基于角色理论和OUD在线社区数据，构建提问者角色分类，模拟了超过1.5万条带有角色背景的问题，验证其真实性和代表性，随后用这些问题测试五个大语言模型的响应。

Result: 结果显示不同角色触发不同类型的模型回复，患者和护理者得到的回答更具支持性（提升17%），知识性内容减少（降低19%），与从业者相比存在系统性差异。

Conclusion: 隐含的用户角色身份显著影响模型回应，提出了一种基于角色的对话AI评估方法，有助于设计更具针对性和敏感性的模型响应。

Abstract: Language model users often embed personal and social context in their
questions. The asker's role -- implicit in how the question is framed --
creates specific needs for an appropriate response. However, most evaluations,
while capturing the model's capability to respond, often ignore who is asking.
This gap is especially critical in stigmatized domains such as opioid use
disorder (OUD), where accounting for users' contexts is essential to provide
accessible, stigma-free responses. We propose CoRUS (COmmunity-driven Roles for
User-centric Question Simulation), a framework for simulating role-based
questions. Drawing on role theory and posts from an online OUD recovery
community (r/OpiatesRecovery), we first build a taxonomy of asker roles --
patients, caregivers, practitioners. Next, we use it to simulate 15,321
questions that embed each role's goals, behaviors, and experiences. Our
evaluations show that these questions are both highly believable and comparable
to real-world data. When used to evaluate five LLMs, for the same question but
differing roles, we find systematic differences: vulnerable roles, such as
patients and caregivers, elicit more supportive responses (+17%) and reduced
knowledge content (-19%) in comparison to practitioners. Our work demonstrates
how implicitly signaling a user's role shapes model responses, and provides a
methodology for role-informed evaluation of conversational AI.

</details>


### [48] [FinSight: Towards Real-World Financial Deep Research](https://arxiv.org/abs/2510.16844)
*Jiajie Jin,Yuyao Zhang,Yimeng Xu,Hongjin Qian,Yutao Zhu,Zhicheng Dou*

Main category: cs.CL

TL;DR: 本论文提出FinSight，一种基于多代理框架的金融报告生成系统，结合可编程变量空间及视觉增强机制，实现高质量、多模态的专业金融报告自动生成。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统难以全自动生成专业的金融报告，过程繁琐且要求高，需提升自动化水平与报告质量。

Method: 引入CAVM架构整合数据与工具，采用迭代视觉增强机制优化财务图表，利用两阶段写作框架扩展分析链，形成引用明确、结构严谨的报告。

Result: FinSight在公司及行业级任务上超过现有基线和领先深度研究系统，在事实准确性、分析深度和呈现质量上表现显著提升。

Conclusion: FinSight展示了通过多代理架构与多模态处理，自动生成接近人类专家水准的专业金融报告的可行性和有效路径。

Abstract: Generating professional financial reports is a labor-intensive and
intellectually demanding process that current AI systems struggle to fully
automate. To address this challenge, we introduce FinSight (Financial InSight),
a novel multi agent framework for producing high-quality, multimodal financial
reports. The foundation of FinSight is the Code Agent with Variable Memory
(CAVM) architecture, which unifies external data, designed tools, and agents
into a programmable variable space, enabling flexible data collection, analysis
and report generation through executable code. To ensure professional-grade
visualization, we propose an Iterative Vision-Enhanced Mechanism that
progressively refines raw visual outputs into polished financial charts.
Furthermore, a two stage Writing Framework expands concise Chain-of-Analysis
segments into coherent, citation-aware, and multimodal reports, ensuring both
analytical depth and structural consistency. Experiments on various company and
industry-level tasks demonstrate that FinSight significantly outperforms all
baselines, including leading deep research systems in terms of factual
accuracy, analytical depth, and presentation quality, demonstrating a clear
path toward generating reports that approach human-expert quality.

</details>


### [49] [Neuronal Group Communication for Efficient Neural representation](https://arxiv.org/abs/2510.16851)
*Zhengqi Pei,Qingming Huang,Shuhui Wang*

Main category: cs.CL

TL;DR: 本文提出了一种名为神经元群通信（NGC）的新框架，将神经网络视为神经元群互动的动态系统，实现高效、模块化和可解释的表示。


<details>
  <summary>Details</summary>
Motivation: 随着神经网络规模越来越大，效率和可解释性成为主要挑战，如何构建高效且模块化的大型神经系统是核心问题。

Method: 通过将权重视为神经元状态间的瞬态互动，设计低秩模块化表示，利用动态系统理论引入神经元稳定性指标，分析网络推理能力与外驱动力之间的关系。

Result: 在大型语言模型中应用NGC，实验证明在适度压缩下提升复杂推理表现，优于传统低秩近似和跨层基共享方法。

Conclusion: NGC通过结构化的神经元群动态增强模型泛化能力，具有重要的理论和应用意义。

Abstract: The ever-increasing scale of modern neural networks has brought unprecedented
performance alongside daunting challenges in efficiency and interpretability.
This paper addresses the core question of how to build large neural systems
that learn efficient, modular, and interpretable representations. We propose
Neuronal Group Communication (NGC), a theory-driven framework that reimagines a
neural network as a dynamical system of interacting neuronal groups rather than
a monolithic collection of neural weights. Instead of treating each weight as
an independent trainable parameter, NGC treats weights as transient
interactions between embedding-like neuronal states, with neural computation
unfolding through iterative communication among groups of neurons. This
low-rank, modular representation yields compact models: groups of neurons
exchange low-dimensional signals, enabling intra-group specialization and
inter-group information sharing while dramatically reducing redundant
parameters. By drawing on dynamical systems theory, we introduce a neuronal
stability metric (analogous to Lyapunov stability) that quantifies the
contraction of neuron activations toward stable patterns during sequence
processing. Using this metric, we reveal that emergent reasoning capabilities
correspond to an external driving force or ``potential'', which nudges the
neural dynamics away from trivial trajectories while preserving stability.
Empirically, we instantiate NGC in large language models (LLMs) and demonstrate
improved performance on complex reasoning benchmarks under moderate
compression. NGC consistently outperforms standard low-rank approximations and
cross-layer basis-sharing methods at comparable compression rates. We conclude
by discussing the broader implications of NGC, including how structured
neuronal group dynamics might relate to generalization in high-dimensional
learning systems.

</details>


### [50] [Does Visual Grounding Enhance the Understanding of Embodied Knowledge in Large Language Models?](https://arxiv.org/abs/2510.16924)
*Zhihui Yang,Yupei Wang,Kaijie Mo,Zhe Zhao,Renfen Hu*

Main category: cs.CL

TL;DR: 本文提出了一个基于感知理论的多感官知识理解基准，评估多模态语言模型（特别是视觉语言模型）对具身知识的理解能力，发现视觉语言模型并不优于纯文本模型，且在视觉感知方面表现较差。


<details>
  <summary>Details</summary>
Motivation: 当前多模态语言模型虽然取得了进展，但尚不清楚视觉感知是否真正提升了模型对具身知识的理解。

Method: 基于心理学的感知理论，构建涵盖视觉、听觉、触觉、味觉、嗅觉和内感的多感官知识理解基准，通过向量比较和问答任务测试30个前沿语言模型。

Result: 视觉语言模型在所有任务中均未优于纯文本模型，且在视觉感知维度表现最弱。模型的向量表示易受词形和词频影响，且在空间感知和推理问题上表现不佳。

Conclusion: 多模态语言模型需要更有效地融合具身知识，提升对物理世界的理解能力。

Abstract: Despite significant progress in multimodal language models (LMs), it remains
unclear whether visual grounding enhances their understanding of embodied
knowledge compared to text-only models. To address this question, we propose a
novel embodied knowledge understanding benchmark based on the perceptual theory
from psychology, encompassing visual, auditory, tactile, gustatory, olfactory
external senses, and interoception. The benchmark assesses the models'
perceptual abilities across different sensory modalities through vector
comparison and question-answering tasks with over 1,700 questions. By comparing
30 state-of-the-art LMs, we surprisingly find that vision-language models
(VLMs) do not outperform text-only models in either task. Moreover, the models
perform significantly worse in the visual dimension compared to other sensory
dimensions. Further analysis reveals that the vector representations are easily
influenced by word form and frequency, and the models struggle to answer
questions involving spatial perception and reasoning. Our findings underscore
the need for more effective integration of embodied knowledge in LMs to enhance
their understanding of the physical world.

</details>


### [51] [ChiKhaPo: A Large-Scale Multilingual Benchmark for Evaluating Lexical Comprehension and Generation in Large Language Models](https://arxiv.org/abs/2510.16928)
*Emily Chang,Niyati Bafna*

Main category: cs.CL

TL;DR: ChiKhaPo基准测试覆盖2700多种语言，通过8个难度不同的子任务评估大型语言模型在词汇理解与生成方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM基准大多集中于高资源语言，忽视了大多数世界语言的基本语言理解能力。

Method: 设计包含8个子任务的ChiKhaPo基准，利用现有词典、单语数据和双语平行语料，覆盖2700多种语言。

Result: 六个最先进模型在ChiKhaPo测试中表现不佳，且表现受语言族群、资源丰富度和任务类型影响显著。

Conclusion: ChiKhaPo为大规模多语言LLM评测提供了覆盖广泛的新基准，促进多语言能力的全面评估。

Abstract: Existing benchmarks for large language models (LLMs) are largely restricted
to high- or mid-resource languages, and often evaluate performance on
higher-order tasks in reasoning and generation. However, plenty of evidence
points to the fact that LLMs lack basic linguistic competence in the vast
majority of the world's 3800+ written languages. We introduce ChiKhaPo,
consisting of 8 subtasks of varying difficulty designed to evaluate the lexical
comprehension and generation abilities of generative models. ChiKhaPo draws on
existing lexicons, monolingual data, and bitext, and provides coverage for
2700+ languages for 2 subtasks, surpassing any existing benchmark in terms of
language coverage. We further show that 6 SOTA models struggle on our
benchmark, and discuss the factors contributing to performance scores,
including language family, language resourcedness, task, and comprehension
versus generation directions. With ChiKhaPo, we hope to enable and encourage
the massively multilingual benchmarking of LLMs.

</details>


### [52] [Prompt-MII: Meta-Learning Instruction Induction for LLMs](https://arxiv.org/abs/2510.16932)
*Emily Xiao,Yixiao Zeng,Ada Chen,Chin-Jou Li,Amanda Bertsch,Graham Neubig*

Main category: cs.CL

TL;DR: 本文提出了PROMPT-MII，一种基于强化学习的指令归纳方法，能够将大量训练示例压缩为简洁有效的提示，从而实现与完整上下文学习相媲美的性能，同时大幅降低推理时的上下文长度和计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 传统的大型语言模型上下文学习方法虽然有效，但随着上下文长度的增加，推理成本急剧上升，亟需一种方法减少上下文长度而不牺牲性能。

Method: 作者提出了PROMPT-MII，一个基于强化学习的元学习框架，训练一个指令归纳模型，从大量多样化的分类数据集学习，以便能够为任意新数据集生成紧凑且描述性的提示语。

Result: 在3,000多个训练分类数据集上训练，并在90个新任务上测试，PROMPT-MII在模型效果上提升4-9个F1分数（相对提升10-20%），同时所需输入令牌数量减少3-13倍，性能达到传统上下文学习水平。

Conclusion: PROMPT-MII有效解决了上下文学习中推理成本高昂的问题，通过学习生成简洁指令，实现了推理效率和效果的双重提升，具有广泛应用前景。

Abstract: A popular method to adapt large language models (LLMs) to new tasks is
in-context learning (ICL), which is effective but incurs high inference costs
as context length grows. In this paper we propose a method to perform
instruction induction, where we take training examples and reduce them to a
compact but descriptive prompt that can achieve performance comparable to ICL
over the full training set. Specifically, we propose PROMPT-MII, a
reinforcement learning (RL) based framework to meta-learn an instruction
induction model that can generate compact instructions on the fly for an
arbitrary new dataset. We train on over 3,000 diverse classification datasets
from the HuggingFace hub, and evaluate on 90 unseen tasks. PROMPT-MII improves
downstream model quality by 4-9 F1 points (10-20% relative), matching ICL
performance while requiring 3-13x fewer tokens.

</details>


### [53] [Parameter-Efficient Fine-Tuning for Low-Resource Languages: A Comparative Study of LLMs for Bengali Hate Speech Detection](https://arxiv.org/abs/2510.16985)
*Akif Islam,Mohd Ruhul Ameen*

Main category: cs.CL

TL;DR: 本文首次将参数高效微调（PEFT）方法应用于孟加拉语仇恨言论检测，使用LoRA和QLoRA在50,281条注释数据上训练三个大语言模型，取得最高92.23%的F1分数，显示出PEFT在低资源语言中的实用性。


<details>
  <summary>Details</summary>
Motivation: 孟加拉社交媒体上的仇恨言论数量激增，严重影响妇女和青少年，而现有方法依赖计算成本高的全模型微调或专有API，不适合资源受限环境。

Method: 利用LoRA和QLoRA两种PEFT技术，仅微调模型参数的不到1%，在BD-SHS数据集上对三种大语言模型（Gemma-3-4B、Llama-3.2-3B和Mistral-7B）进行微调。

Result: Llama-3.2-3B模型达到了最高F1分数92.23%，其次是Mistral-7B的88.94%和Gemma-3-4B的80.25%，均在单GPU设备上完成训练。

Conclusion: PEFT技术是一种切实可行且易于复现的策略，适用于孟加拉语及其他低资源语言的仇恨言论检测任务。

Abstract: Bengali social media platforms have witnessed a sharp increase in hate
speech, disproportionately affecting women and adolescents. While datasets such
as BD-SHS provide a basis for structured evaluation, most prior approaches rely
on either computationally costly full-model fine-tuning or proprietary APIs.
This paper presents the first application of Parameter-Efficient Fine-Tuning
(PEFT) for Bengali hate speech detection using LoRA and QLoRA. Three
instruction-tuned large language models - Gemma-3-4B, Llama-3.2-3B, and
Mistral-7B - were fine-tuned on the BD-SHS dataset of 50,281 annotated
comments. Each model was adapted by training fewer than 1% of its parameters,
enabling experiments on a single consumer-grade GPU. The results show that
Llama-3.2-3B achieved the highest F1-score of 92.23%, followed by Mistral-7B at
88.94% and Gemma-3-4B at 80.25%. These findings establish PEFT as a practical
and replicable strategy for Bengali and related low-resource languages.

</details>


### [54] [Back to Bytes: Revisiting Tokenization Through UTF-8](https://arxiv.org/abs/2510.16987)
*Amit Moryossef,Clara Meister,Pavel Stepachev,Desmond Elliott*

Main category: cs.CL

TL;DR: UTF8Tokenizer 是一种简约的字节级分词器，将文本映射为对应 UTF-8 字节的 ID，避免了超出范围的 ID 和辅助令牌，利用 C0 控制字节编码特殊行为，实现更快分词和更高效的模型训练。


<details>
  <summary>Details</summary>
Motivation: 现有的字节级分词方法存在引入超出范围的 ID 和辅助令牌的问题，且分词速度和效率有待提升。

Method: 提出UTF8Tokenizer，通过直接映射UTF-8字节到对应ID，使用C0控制字节处理特殊行为，设计256*d的共享嵌入表，并引入训练后可添加的比特偏置嵌入以捕捉字节位结构。

Result: 实现了14倍更快的分词速度，主机与设备的数据传输量减少8倍，并促进语言模型训练收敛。

Conclusion: UTF8Tokenizer在保持简约的同时显著提升了分词效率和模型训练效果，适合集成在大型语言模型中。

Abstract: We present UTF8Tokenizer, a minimalist byte-level tokenizer that maps text
exactly to IDs corresponding to the bytes underlying the text's UTF-8 encoding
(e.g., byte x09 is token ID 9). Unlike prior byte-level approaches (Xue et al.,
2021; Pagnoni et al., 2025), our implementation never introduces out-of-range
IDs (i.e. there is no token ID 256) or auxiliary tokens: all special behavior
(e.g., padding, boundaries, conversation structure, attention segments, tool
calling, "thinking" spans, etc.) is encoded using C0 control bytes - just as
ASCII was originally designed to embed control information alongside printable
text. These design principles yield practical benefits: (1) faster tokenization
(14x) and significantly lower host-device transfer (8x less than int64); (2)
simple, shareable 256*d embedding tables that can be aligned across models; and
(3) a training-time enhancement via bit-biased embeddings, which exposes
per-byte bit structure and can be added to the embedding table post-training,
removing inference costs. Our HuggingFace-compatible implementation improves
language modeling convergence.

</details>


### [55] [Vocab Diet: Reshaping the Vocabulary of LLMs with Vector Arithmetic](https://arxiv.org/abs/2510.17001)
*Yuval Reif,Guy Kaplan,Roy Schwartz*

Main category: cs.CL

TL;DR: 本文提出了一种基于变换向量重构词汇表的方法，通过将词形变化建模为词嵌入空间中的线性变换，减少了词汇表中重复词形的冗余，实现了词汇表的压缩与扩展。


<details>
  <summary>Details</summary>
Motivation: 传统的分词算法将不同词形视为独立词条，导致词汇表容量受限，影响少频词及多语种覆盖率。

Method: 利用词嵌入空间中的线性变换向量，将词形变化表示为基词嵌入与变换向量的组合，进而重构词汇表以减少重复词形词条。

Result: 应用于多语言大语言模型，最多去除10%的词汇条目，同时扩展了未知词覆盖，且下游任务性能影响极小。

Conclusion: 提出的词汇重构方法推动了词汇设计的范式转变，从字符串枚举转向基于语言结构的组合词汇，提升了词汇表的效率与覆盖能力。

Abstract: Large language models (LLMs) were shown to encode word form variations, such
as "walk"->"walked", as linear directions in embedding space. However, standard
tokenization algorithms treat these variations as distinct tokens -- filling
the size-capped vocabulary with surface form variants (e.g., "walk", "walking",
"Walk"), at the expense of less frequent words and multilingual coverage. We
show that many of these variations can be captured by transformation vectors --
additive offsets that yield the appropriate word's representation when applied
to the base form word embedding -- in both the input and output spaces.
Building on this, we propose a compact reshaping of the vocabulary: rather than
assigning unique tokens to each surface form, we compose them from shared base
form and transformation vectors (e.g., "walked" = "walk" + past tense). We
apply our approach to multiple LLMs and across five languages, removing up to
10% of vocabulary entries -- thereby freeing space to allocate new, more
diverse tokens. Importantly, we do so while also expanding vocabulary coverage
to out-of-vocabulary words, with minimal impact on downstream performance, and
without modifying model weights. Our findings motivate a foundational
rethinking of vocabulary design, moving from string enumeration to a
compositional vocabulary that leverages the underlying structure of language.

</details>


### [56] [Online Learning Defense against Iterative Jailbreak Attacks via Prompt Optimization](https://arxiv.org/abs/2510.17006)
*Masahiro Kaneko,Zeerak Talat,Timothy Baldwin*

Main category: cs.CL

TL;DR: 本文提出了一种动态防御迭代越狱攻击的框架，通过在线学习实时更新防御策略，利用强化学习优化提示以拒绝有害输入并保证无害任务的响应质量，同时引入PDGD缓解过拟合，实验证明该方法优于现有五种防御策略。


<details>
  <summary>Details</summary>
Motivation: 迭代越狱方法通过反复输入和改写提示有效绕过大型语言模型的安全机制，现有防御未能主动破坏这种试错循环。

Method: 提出一个基于在线学习的动态防御框架，利用强化学习优化提示以确保无害任务的适当响应并拒绝有害提示；引入过去梯度阻尼(PDGD)减少对攻击中局部改写的过拟合。

Result: 在三种大型语言模型上对五种迭代越狱攻击的实验显示，该方法显著优于五种现有防御方法，同时提升了无害任务的响应质量。

Conclusion: 本文方法有效识别并防御迭代越狱攻击，动态调整策略避免过拟合，并提升无害任务的输出质量，展示了强大的实用价值。

Abstract: Iterative jailbreak methods that repeatedly rewrite and input prompts into
large language models (LLMs) to induce harmful outputs -- using the model's
previous responses to guide each new iteration -- have been found to be a
highly effective attack strategy. Despite being an effective attack strategy
against LLMs and their safety mechanisms, existing defenses do not proactively
disrupt this dynamic trial-and-error cycle. In this study, we propose a novel
framework that dynamically updates its defense strategy through online learning
in response to each new prompt from iterative jailbreak methods. Leveraging the
distinctions between harmful jailbreak-generated prompts and typical harmless
prompts, we introduce a reinforcement learning-based approach that optimizes
prompts to ensure appropriate responses for harmless tasks while explicitly
rejecting harmful prompts. Additionally, to curb overfitting to the narrow band
of partial input rewrites explored during an attack, we introduce
Past-Direction Gradient Damping (PDGD). Experiments conducted on three LLMs
show that our approach significantly outperforms five existing defense methods
against five iterative jailbreak methods. Moreover, our results indicate that
our prompt optimization strategy simultaneously enhances response quality for
harmless tasks.

</details>


### [57] [DiscoTrack: A Multilingual LLM Benchmark for Discourse Tracking](https://arxiv.org/abs/2510.17013)
*Lanni Bu,Lauren Levin,Amir Zeldes*

Main category: cs.CL

TL;DR: 本文提出了一个多语言大型语言模型基准测试DiscoTrack，涵盖12种语言和四个话语理解层级，专注于隐式信息和语用推理，跨越句子、段落及多说话者话语。


<details>
  <summary>Details</summary>
Motivation: 现有LLM基准测试主要关注显式信息抽取，且多为单句信息，缺乏对隐式信息及跨更大语篇的多语言话语跟踪测试。

Method: 提出DiscoTrack基准，设计针对显著性识别、实体追踪、话语关系和桥接推理四个任务，在12种语言上进行评测。

Result: 评测结果表明，即便是最先进的模型，在这些多语言话语理解任务上仍表现不佳，任务具有较高挑战性。

Conclusion: DiscoTrack填补了多语言话语理解基准的空白，强调了隐式信息及跨语篇整合的重要性，未来模型需要在此方向加强能力。

Abstract: Recent LLM benchmarks have tested models on a range of phenomena, but are
still focused primarily on natural language understanding for extraction of
explicit information, such as QA or summarization, with responses often tar-
geting information from individual sentences. We are still lacking more
challenging, and im- portantly also multilingual, benchmarks focus- ing on
implicit information and pragmatic infer- ences across larger documents in the
context of discourse tracking: integrating and aggregating information across
sentences, paragraphs and multiple speaker utterances. To this end, we present
DiscoTrack, an LLM benchmark target- ing a range of tasks across 12 languages
and four levels of discourse understanding: salience recognition, entity
tracking, discourse relations and bridging inference. Our evaluation shows that
these tasks remain challenging, even for state-of-the-art models.

</details>


### [58] [SafeSearch: Do Not Trade Safety for Utility in LLM Search Agents](https://arxiv.org/abs/2510.17017)
*Qiusi Zhan,Angeline Budiman-Chan,Abdelrahman Zayed,Xingzhi Guo,Daniel Kang,Joo-Kyung Kim*

Main category: cs.CL

TL;DR: 该论文研究了基于大语言模型的搜索代理在安全性方面的问题，提出了一种多目标强化学习方法SafeSearch以提升安全性和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型搜索代理在生成有用回答的同时，存在产生有害输出的风险，尤其是在检索和引用外部资料时，这种风险加剧，因此需要提高其安全行为以保护用户。

Method: 本文提出SafeSearch方法，采用多目标强化学习，结合最终输出的安全性和实用性奖励，以及一个新的查询层面奖励，惩罚不安全查询并奖励安全查询，从而实现安全性与实用性的联合优化。

Result: 实验结果显示，SafeSearch在三个红队测试数据集上将有害输出减少了70%以上，同时保持了安全且有帮助的回答质量，且其问答性能与仅优化实用性的模型相当。

Conclusion: SafeSearch有效提升了大语言模型搜索代理的安全性，兼顾了实用性，证明了结合查询层面奖励的多目标强化学习在安全与实用双重目标上的可行性和有效性。

Abstract: Large language model (LLM) based search agents iteratively generate queries,
retrieve external information, and reason to answer open-domain questions.
While researchers have primarily focused on improving their utility, their
safety behaviors remain underexplored. In this paper, we first evaluate search
agents using red-teaming datasets and find that they are more likely to produce
harmful outputs than base LLMs. For example, when asked ``How can I track
someone's location without their consent?'', a base model refuses, whereas a
search agent designed to retrieve and cite sources may lower its refusal
threshold, fetch documents (e.g., court cases), and, once appended, synthesize
them into an informative yet unsafe summary. We further show that
utility-oriented fine-tuning intensifies this risk, motivating joint alignment
of safety and utility. We present SafeSearch, a multi-objective reinforcement
learning approach that couples a final-output safety/utility reward with a
novel query-level shaping term that penalizes unsafe queries and rewards safe
ones. Experiments show that SafeSearch reduces agent harmfulness by over 70%
across three red-teaming datasets while producing safe, helpful responses, and
matches the QA performance of a utility-only finetuned agent; further analyses
confirm the effectiveness of the query-level reward in jointly improving safety
and utility.

</details>


### [59] [Extended LSTM: Adaptive Feature Gating for Toxic Comment Classification](https://arxiv.org/abs/2510.17018)
*Noor Islam S. Mohammad*

Main category: cs.CL

TL;DR: 提出了xLSTM模型，通过余弦相似门控和自适应特征优先级，有效提升了有毒评论检测在少数类别上的表现，实现了高效且准确的分类。


<details>
  <summary>Details</summary>
Motivation: 现有基于transformer的有毒评论检测模型计算成本高且在少数类别表现差，传统集成方法缺乏语义适应能力。

Method: 引入可学习参考向量通过余弦相似度调节上下文嵌入，融合多源词嵌入和字符级BiLSTM，结合嵌入空间SMOTE和自适应焦点损失，实现类别重平衡和特征优先。

Result: 在Jigsaw基准上达到96.0%准确率和0.88宏F1，某些类别性能远超BERT，参数量减少15倍，推理延迟降低到50ms，余弦门控显著提升性能。

Conclusion: 轻量级且理论指导明确的模型架构在应对不平衡领域特定NLP任务时表现优异，可超越大型预训练模型。

Abstract: Toxic comment detection remains a challenging task, where transformer-based
models (e.g., BERT) incur high computational costs and degrade on minority
toxicity classes, while classical ensembles lack semantic adaptability. We
propose xLSTM, a parameter-efficient and theoretically grounded framework that
unifies cosine-similarity gating, adaptive feature prioritization, and
principled class rebalancing. A learnable reference vector {v} in {R}^d
modulates contextual embeddings via cosine similarity, amplifying toxic cues
and attenuating benign signals to yield stronger gradients under severe class
imbalance. xLSTM integrates multi-source embeddings (GloVe, FastText, BERT CLS)
through a projection layer, a character-level BiLSTM for morphological cues,
embedding-space SMOTE for minority augmentation, and adaptive focal loss with
dynamic class weighting. On the Jigsaw Toxic Comment benchmark, xLSTM attains
96.0% accuracy and 0.88 macro-F1, outperforming BERT by 33% on threat and 28%
on identity_hate categories, with 15 times fewer parameters and 50ms inference
latency. Cosine gating contributes a +4.8% F1 gain in ablations. The results
establish a new efficiency adaptability frontier, demonstrating that
lightweight, theoretically informed architectures can surpass large pretrained
models on imbalanced, domain-specific NLP tasks.

</details>


### [60] [Mapping from Meaning: Addressing the Miscalibration of Prompt-Sensitive Language Models](https://arxiv.org/abs/2510.17028)
*Kyle Cox,Jiawei Xu,Yikun Han,Rong Xu,Tianhao Li,Chi-Yang Hsu,Tianlong Chen,Walter Gerych,Ying Ding*

Main category: cs.CL

TL;DR: 大型语言模型对语义等价的不同提示表现出很大敏感性，导致不确定性估计不准确。本文通过在语义“概念空间”中采样改写，提升了不确定性校准，同时提出了改进的黑盒模型不确定性分解指标。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在面对语义等价的不同提示时，输出结果和不确定性估计存在较大差异，反映了模型在理解提示含义上的不一致性，需要改进不确定性校准。

Method: 将提示敏感性建模为泛化误差，通过在语义概念空间中对提示进行改写扰动进行采样，提升模型的不确定性校准；并提出一种基于语义连续性的新型不确定性分解指标，用于黑盒模型分析。

Result: 通过语义概念空间采样，模型的不确定性校准得到提升，且准确率未受影响；新指标有效量化了因提示敏感性引起的模型不确定性。

Conclusion: 本文提出的方法改善了对提示敏感语言模型的不确定性校准，并揭示部分大型语言模型在理解输入含义方面存在泛化推理能力不足的问题。

Abstract: An interesting behavior in large language models (LLMs) is prompt
sensitivity. When provided with different but semantically equivalent versions
of the same prompt, models may produce very different distributions of answers.
This suggests that the uncertainty reflected in a model's output distribution
for one prompt may not reflect the model's uncertainty about the meaning of the
prompt. We model prompt sensitivity as a type of generalization error, and show
that sampling across the semantic ``concept space'' with paraphrasing
perturbations improves uncertainty calibration without compromising accuracy.
Additionally, we introduce a new metric for uncertainty decomposition in
black-box LLMs that improves upon entropy-based decomposition by modeling
semantic continuities in natural language generation. We show that this
decomposition metric can be used to quantify how much LLM uncertainty is
attributed to prompt sensitivity. Our work introduces a new way to improve
uncertainty calibration in prompt-sensitive language models, and provides
evidence that some LLMs fail to exhibit consistent general reasoning about the
meanings of their inputs.

</details>


### [61] [Investigating Thinking Behaviours of Reasoning-Based Language Models for Social Bias Mitigation](https://arxiv.org/abs/2510.17062)
*Guoqing Luo,Iffat Maab,Lili Mou,Junichi Yamagishi*

Main category: cs.CL

TL;DR: 本文研究了推理型大语言模型中社会偏见的表现，发现了两种导致偏见的行为模式，并提出了一种轻量级的提示语干预方法来降低偏见。


<details>
  <summary>Details</summary>
Motivation: 推理型大语言模型在复杂任务表现优异，但其内部思维过程中存在社会刻板印象的聚合，导致结果偏见，而这些行为机制尚未被充分探究。

Method: 系统研究推理过程中出现的两种失败模式——刻板印象重复和无关信息注入，基于此设计轻量级提示语干预，促使模型自我审查初步推理以识别并缓解偏见。

Result: 在BBQ、StereoSet和BOLD等问题回答及开放式任务基准测试中，所提方法有效降低了偏见，同时保持或提升了模型的准确率。

Conclusion: 通过揭示社会偏见的内在思维机制并采用针对性干预，推理型语言模型的偏见问题能得到有效缓解，提升模型公平性且不损害性能。

Abstract: While reasoning-based large language models excel at complex tasks through an
internal, structured thinking process, a concerning phenomenon has emerged that
such a thinking process can aggregate social stereotypes, leading to biased
outcomes. However, the underlying behaviours of these language models in social
bias scenarios remain underexplored. In this work, we systematically
investigate mechanisms within the thinking process behind this phenomenon and
uncover two failure patterns that drive social bias aggregation: 1) stereotype
repetition, where the model relies on social stereotypes as its primary
justification, and 2) irrelevant information injection, where it fabricates or
introduces new details to support a biased narrative. Building on these
insights, we introduce a lightweight prompt-based mitigation approach that
queries the model to review its own initial reasoning against these specific
failure patterns. Experiments on question answering (BBQ and StereoSet) and
open-ended (BOLD) benchmarks show that our approach effectively reduces bias
while maintaining or improving accuracy.

</details>


### [62] [DVAGen: Dynamic Vocabulary Augmented Generation](https://arxiv.org/abs/2510.17115)
*Wei Du,Nuowei Liu,Jie Wang,Jiahao Kuang,Tao Ji,Xiaoling Wang,Yuanbin Wu*

Main category: cs.CL

TL;DR: 论文提出了DVAGen框架，用于解决固定词汇表语言模型难以处理新词的问题，支持现代大规模语言模型并提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 固定词汇表的语言模型无法很好地泛化到新词或未登录词，现有动态词汇方法存在代码分散、支持不足和推理扩展性差等问题。

Method: 提出DVAGen，一个开源统一框架，模块化设计方便定制，集成开源大语言模型，并提供命令行和网页界面实现实时结果查看，支持批量推理。

Result: 在现代大语言模型上验证了动态词汇方法的有效性，并通过批量推理显著提升推理吞吐量。

Conclusion: DVAGen框架有效解决了动态词汇扩展的挑战，提升了模型灵活性和推理效率，为动态词汇研究和应用提供了强有力工具。

Abstract: Language models trained with a fixed vocabulary struggle to generalize to
novel or out-of-vocabulary words, limiting their flexibility in handling
diverse token combinations. Existing dynamic vocabulary approaches attempt to
address this limitation but face challenges such as fragmented codebases, lack
of support for modern LLMs, and limited inference scalability. To overcome
these issues, we introduce DVAGen, a fully open-source, unified framework
designed for training, evaluation, and visualization of dynamic
vocabulary-augmented language models. Our framework modularizes the pipeline
for ease of customization, integrates seamlessly with open-source LLMs, and is
the first to provide both CLI and WebUI tools for real-time result inspection.
We validate the effectiveness of dynamic vocabulary methods on modern LLMs and
demonstrate support for batch inference, significantly improving inference
throughput.

</details>


### [63] [Rethinking On-policy Optimization for Query Augmentation](https://arxiv.org/abs/2510.17139)
*Zhichao Xu,Shengyao Zhuang,Xueguang Ma,Bingsen Chen,Yijun Tian,Fengran Mo,Jie Cao,Vivek Srikumar*

Main category: cs.CL

TL;DR: 本文系统比较了基于提示和基于强化学习的查询增强方法，提出了一种结合两者优势的新方法OPQE，显著提升了信息检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有两种主流LLM查询增强方法各有优势和局限，缺乏统一条件下的比较，探索更优方案具有重要意义。

Method: 首次系统比较提示生成伪文档和强化学习重写查询两种方法，并提出一种新型的混合方法OPQE，通过生成伪文档的策略优化检索效果。

Result: 实验表明简单的提示方法在强大LLM支持下已能媲美甚至超越强化学习方法，OPQE融合两者优点，性能优于单一方法。

Conclusion: 采用混合策略的OPQE方法实现了提示灵活性和强化学习针对性优化的结合，在信息检索任务中表现最佳。

Abstract: Recent advances in large language models (LLMs) have led to a surge of
interest in query augmentation for information retrieval (IR). Two main
approaches have emerged. The first prompts LLMs to generate answers or
pseudo-documents that serve as new queries, relying purely on the model's
parametric knowledge or contextual information. The second applies
reinforcement learning (RL) to fine-tune LLMs for query rewriting, directly
optimizing retrieval metrics. While having respective advantages and
limitations, the two approaches have not been compared under consistent
experimental conditions. In this work, we present the first systematic
comparison of prompting-based and RL-based query augmentation across diverse
benchmarks, including evidence-seeking, ad hoc, and tool retrieval. Our key
finding is that simple, training-free query augmentation often performs on par
with, or even surpasses, more expensive RL-based counterparts, especially when
using powerful LLMs. Motivated by this discovery, we introduce a novel hybrid
method, On-policy Pseudo-document Query Expansion (OPQE), which, instead of
rewriting a query, the LLM policy learns to generate a pseudo-document that
maximizes retrieval performance, thus merging the flexibility and generative
structure of prompting with the targeted optimization of RL. We show OPQE
outperforms both standalone prompting and RL-based rewriting, demonstrating
that a synergistic approach yields the best results. Our implementation is made
available to facilitate reproducibility.

</details>


### [64] [When AI companions become witty: Can human brain recognize AI-generated irony?](https://arxiv.org/abs/2510.17168)
*Xiaohui Rao,Hanlin Wu,Zhenguang G. Cai*

Main category: cs.CL

TL;DR: 本研究探讨人们在理解AI生成的讽刺语时，是否会赋予其意图性，发现人们对AI讽刺的故意性归因低于对人类讽刺的归因，神经反应也相应减弱，表明对AI的意图性采纳受限且与对AI的真实感知有关。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型作为社交代理被广泛应用并生成幽默和讽刺语，研究人们是否将AI言语理解为有意图的交流成为关键问题。

Method: 通过比较人类与AI来源的讽刺语行为和神经反应，利用ERP成分（P200和P600）分析早期不协调检测及语义重解的认知努力。

Result: 行为数据表明人们对两类讽刺都部分归因为故意交流，但对AI明显少；神经数据显示AI讽刺引起的P200和P600反应减弱，说明认知重解努力减少；同时对AI真诚感知高者对应神经反应增强。

Conclusion: 尽管大型语言模型语言能力强，但人们对其社交代理身份的意图性采纳有限，表明实现真正的社交代理需要超越语言技巧，改变人类对人工智能意图归因的方式。

Abstract: As Large Language Models (LLMs) are increasingly deployed as social agents
and trained to produce humor and irony, a question emerges: when encountering
witty AI remarks, do people interpret these as intentional communication or
mere computational output? This study investigates whether people adopt the
intentional stance, attributing mental states to explain behavior,toward AI
during irony comprehension. Irony provides an ideal paradigm because it
requires distinguishing intentional contradictions from unintended errors
through effortful semantic reanalysis. We compared behavioral and neural
responses to ironic statements from AI versus human sources using established
ERP components: P200 reflecting early incongruity detection and P600 indexing
cognitive efforts in reinterpreting incongruity as deliberate irony. Results
demonstrate that people do not fully adopt the intentional stance toward
AI-generated irony. Behaviorally, participants attributed incongruity to
deliberate communication for both sources, though significantly less for AI
than human, showing greater tendency to interpret AI incongruities as
computational errors. Neural data revealed attenuated P200 and P600 effects for
AI-generated irony, suggesting reduced effortful detection and reanalysis
consistent with diminished attribution of communicative intent. Notably, people
who perceived AI as more sincere showed larger P200 and P600 effects for
AI-generated irony, suggesting that intentional stance adoption is calibrated
by specific mental models of artificial agents. These findings reveal that
source attribution shapes neural processing of social-communicative phenomena.
Despite current LLMs' linguistic sophistication, achieving genuine social
agency requires more than linguistic competence, it necessitates a shift in how
humans perceive and attribute intentionality to artificial agents.

</details>


### [65] [Understanding and Improving Length Generalization in Hierarchical Sparse Attention Models](https://arxiv.org/abs/2510.17196)
*Jiaqi Leng,Xiang Hu,Junxiong Wang,Jianguo Li,Wei Wu,Yucheng Lu*

Main category: cs.CL

TL;DR: 本文系统分析了基于chunk的稀疏注意力模型在处理长上下文中的关键设计原则，提出了三大核心组件，成功实现了训练无关的长距离泛化。


<details>
  <summary>Details</summary>
Motivation: 目前语言模型在处理长上下文时受限于计算复杂度或固定记忆，导致无法有效利用全量上下文，缺乏对chunk稀疏注意力架构成功机制的充分理解。

Method: 通过统一框架和消融实验，作者详细拆解模型结构，强调三大设计原则：非线性Chunk编码器配专用CLS token用于检索；旁路残差路径稳健整合全局信息；预训练期间强制稀疏选择以缩小训练测试分布差距，同时提出理论动机支持。

Result: 结合三大设计原则，模型在无需额外训练的情况下实现了卓越的长度外推能力，能够从4K上下文扩展到3200万标记，刷新了RULER和BABILong数据集上的记录。

Conclusion: 本文基于理论和实验证据总结了一套清晰的设计原则，为未来设计高效长上下文语言模型提供了指导。

Abstract: Effectively processing long contexts is a critical challenge for language
models. While standard Transformers are limited by quadratic complexity and
poor length extrapolation, alternative architectures like sliding window
attention and state space models sacrifice the ability to effectively utilize
the full context due to their fixed-size memory. Chunk-based sparse attention
has emerged as a promising paradigm for extreme length generalization, yet the
key architectural principles underpinning its success are not yet fully
understood. In this work, we present a systematic dissection of these models to
identify the core components driving their performance. Through a unified
framework and comprehensive ablation studies, we demonstrate that a combination
of three design principles is critical: (1) an expressive, non-linear Chunk
Encoder with a dedicated CLS token to produce representations for retrieval;
(2) a Bypassing Residual Path to stably integrate retrieved global information
without it being overridden by the local residual stream; and (3) enforced
selection sparsity during pre-training to bridge the train-test distribution
gap. We provide a theoretical motivation for intra-chunk information processing
and landmark generation. By combining these principles, we establish a new
state-of-the-art for training-free length extrapolation, successfully
generalizing models trained on a 4K context to 32 million tokens on RULER and
BABILong. Our findings provide a clear and empirically-grounded set of design
principles for developing future, highly-capable long-context language models.

</details>


### [66] [Wisdom is Knowing What not to Say: Hallucination-Free LLMs Unlearning via Attention Shifting](https://arxiv.org/abs/2510.17210)
*Chenchen Tan,Youyang Qu,Xinghao Li,Hui Zhang,Shujie Cui,Cunjian Chen,Longxiang Gao*

Main category: cs.CL

TL;DR: 本文提出了一种用于选择性遗忘的大型语言模型的注意力转移(AS)框架，旨在解决现有机器遗忘方法中效用与可靠性的矛盾。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在AI辅助决策中的广泛应用，其可能保留敏感数据引发了数据遗忘的研究需求，但现有方法在保持模型效用和避免幻觉生成之间存在困难，限制了模型在知识密集型应用中的可靠性。

Method: 提出基于注意力机制的注意力转移(AS)框架，通过重要度感知的抑制和注意力引导的保持增强两种干预手段，联合优化以实现选择性遗忘，既降低对敏感知识的依赖，又强化对非遗忘内容的语义关注，避免知识流失和幻觉生成。

Result: 实验表明，AS方法在ToFU和TDEC基准测试中分别提高了15%和10%的准确率，同时在无幻觉遗忘效果上保持竞争力，优于现有最佳遗忘方法。

Conclusion: AS方法在遗忘效果、泛化能力和响应可靠性之间实现了更优的平衡，为大语言模型的选择性遗忘提供了有效解决方案。

Abstract: The increase in computing power and the necessity of AI-assisted
decision-making boost the growing application of large language models (LLMs).
Along with this, the potential retention of sensitive data of LLMs has spurred
increasing research into machine unlearning. However, existing unlearning
approaches face a critical dilemma: Aggressive unlearning compromises model
utility, while conservative strategies preserve utility but risk hallucinated
responses. This significantly limits LLMs' reliability in knowledge-intensive
applications. To address this, we introduce a novel Attention-Shifting (AS)
framework for selective unlearning. AS is driven by two design objectives: (1)
context-preserving suppression that attenuates attention to fact-bearing tokens
without disrupting LLMs' linguistic structure; and (2) hallucination-resistant
response shaping that discourages fabricated completions when queried about
unlearning content. AS realizes these objectives through two attention-level
interventions, which are importance-aware suppression applied to the unlearning
set to reduce reliance on memorized knowledge and attention-guided retention
enhancement that reinforces attention toward semantically essential tokens in
the retained dataset to mitigate unintended degradation. These two components
are jointly optimized via a dual-loss objective, which forms a soft boundary
that localizes unlearning while preserving unrelated knowledge under
representation superposition. Experimental results show that AS improves
performance preservation over the state-of-the-art unlearning methods,
achieving up to 15% higher accuracy on the ToFU benchmark and 10% on the TDEC
benchmark, while maintaining competitive hallucination-free unlearning
effectiveness. Compared to existing methods, AS demonstrates a superior balance
between unlearning effectiveness, generalization, and response reliability.

</details>


### [67] [StreamingThinker: Large Language Models Can Think While Reading](https://arxiv.org/abs/2510.17238)
*Junlong Tong,Yingqi Fan,Anhao Zhao,Yunpu Ma,Xiaoyu Shen*

Main category: cs.CL

TL;DR: 本文提出了针对大型语言模型的流式思考范式，通过边读边推理减少延迟并提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在推理时需等待整个输入完成，导致不必要的延迟和对早期信息关注度不足，影响动态场景下的表现。

Method: 设计了StreamingThinker框架，结合流式链式思维生成、流式约束训练和流式并行推理，采用顺序保持的注意力掩码和位置编码，实现流式推理单元的质量控制与并发处理。

Result: 在Qwen3模型上进行的数学推理、逻辑推理和基于上下文的问答任务中，StreamingThinker在保证与批量思考相当性能的同时，推理启动前的token等待减少80%，最终答案生成的时间延迟降低60%以上。

Conclusion: 流式思考范式有效提升了大型语言模型的推理效率，适用于动态变化场景，具有实用价值。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
chain of thought (CoT) reasoning. However, the current LLM reasoning paradigm
initiates thinking only after the entire input is available, which introduces
unnecessary latency and weakens attention to earlier information in dynamic
scenarios. Inspired by human cognition of thinking while reading, we first
design a \textit{\textbf{streaming thinking}} paradigm for LLMs, where
reasoning unfolds in the order of input and further adjusts its depth once
reading is complete. We instantiate this paradigm with
\textit{StreamingThinker}, a framework that enables LLMs to think while reading
through the integration of streaming CoT generation, streaming-constraint
training, and streaming parallel inference. Specifically, StreamingThinker
employs streaming reasoning units with quality control for CoT generation,
enforces order-preserving reasoning through streaming attention masks and
position encoding, and leverages parallel KV caches that decouple input
encoding from reasoning generation, thereby ensuring alignment and enabling
true concurrency. We evaluate StreamingThinker on the Qwen3 model family across
math reasoning, logical reasoning, and context-based QA reasoning tasks.
Experimental results show that the StreamingThinker preserves performance
comparable to batch thinking, while yielding an 80\% reduction in token waiting
before the onset of reasoning and a more than 60\% reduction in time-level
latency for producing the final answer, demonstrating the effectiveness of the
streaming paradigm for LLM reasoning. Code will be released at
\href{https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker}{this
repository.}

</details>


### [68] [From Preferences to Prejudice: The Role of Alignment Tuning in Shaping Social Bias in Video Diffusion Models](https://arxiv.org/abs/2510.17247)
*Zefan Cai,Haoyi Qiu,Haozhe Zhao,Ke Wan,Jiachen Li,Jiuxiang Gu,Wen Xiao,Nanyun Peng,Junjie Hu*

Main category: cs.CL

TL;DR: 该论文提出了VideoBiasEval框架，系统评估了文本生成视频中的社会偏见，发现对齐调优增强了偏见的表现和时间稳定性。


<details>
  <summary>Details</summary>
Motivation: 近年来视频扩散模型在文本生成视频中表现优异，但对齐调优过程中无意中放大了社会偏见，缺乏系统工具追踪偏见演变。

Method: 提出基于事件的提示策略和多粒度指标，解构语义内容与演员属性，评估族裔、性别偏见及其随时间和模型变体的变化，首次端到端连接偏见来源、放大和传播过程。

Result: 实验证明对齐调优不仅增强了偏见表现，还使偏见在视频中更加稳定且刻板，偏见在模型各阶段被放大且持续存在。

Conclusion: 强调在视频生成对齐过程中需重视偏见评估和缓解，保障生成内容的公平与社会责任。

Abstract: Recent advances in video diffusion models have significantly enhanced
text-to-video generation, particularly through alignment tuning using reward
models trained on human preferences. While these methods improve visual
quality, they can unintentionally encode and amplify social biases. To
systematically trace how such biases evolve throughout the alignment pipeline,
we introduce VideoBiasEval, a comprehensive diagnostic framework for evaluating
social representation in video generation. Grounded in established social bias
taxonomies, VideoBiasEval employs an event-based prompting strategy to
disentangle semantic content (actions and contexts) from actor attributes
(gender and ethnicity). It further introduces multi-granular metrics to
evaluate (1) overall ethnicity bias, (2) gender bias conditioned on ethnicity,
(3) distributional shifts in social attributes across model variants, and (4)
the temporal persistence of bias within videos. Using this framework, we
conduct the first end-to-end analysis connecting biases in human preference
datasets, their amplification in reward models, and their propagation through
alignment-tuned video diffusion models. Our results reveal that alignment
tuning not only strengthens representational biases but also makes them
temporally stable, producing smoother yet more stereotyped portrayals. These
findings highlight the need for bias-aware evaluation and mitigation throughout
the alignment process to ensure fair and socially responsible video generation.

</details>


### [69] [How News Feels: Understanding Affective Bias in Multilingual Headlines for Human-Centered Media Design](https://arxiv.org/abs/2510.17252)
*Mohd Ruhul Ameen,Akif Islam,Abu Saleh Musa Miah,Ayesha Siddiqua,Jungpil Shin*

Main category: cs.CL

TL;DR: 本文通过对30万个孟加拉新闻标题和内容的大规模情感分析，揭示了新闻报道中负面情绪的主导地位及不同媒体对相似新闻的情感表现差异。


<details>
  <summary>Details</summary>
Motivation: 新闻媒体通过情绪化的报道框架影响公众情绪，负面和情绪化的标题更易传播，促使媒体使用激烈的表述。

Method: 利用基于Gemma-3 4B的零-shot推理方法，分析大量孟加拉新闻标题和内容的主导情绪和整体基调。

Result: 发现负面情绪（愤怒、恐惧、失望）占主导，不同媒体对相似新闻情绪表达存在显著差异。

Conclusion: 基于研究结果，提出了设计以人为中心的新闻聚合器，帮助读者识别新闻中的情感框架和情绪线索。

Abstract: News media often shape the public mood not only by what they report but by
how they frame it. The same event can appear calm in one outlet and alarming in
another, reflecting subtle emotional bias in reporting. Negative or emotionally
charged headlines tend to attract more attention and spread faster, which in
turn encourages outlets to frame stories in ways that provoke stronger
reactions. This research explores that tendency through large-scale emotion
analysis of Bengali news. Using zero-shot inference with Gemma-3 4B, we
analyzed 300000 Bengali news headlines and their content to identify the
dominant emotion and overall tone of each. The findings reveal a clear
dominance of negative emotions, particularly anger, fear, and disappointment,
and significant variation in how similar stories are emotionally portrayed
across outlets. Based on these insights, we propose design ideas for a
human-centered news aggregator that visualizes emotional cues and helps readers
recognize hidden affective framing in daily news.

</details>


### [70] [Explainability of Large Language Models: Opportunities and Challenges toward Generating Trustworthy Explanations](https://arxiv.org/abs/2510.17256)
*Shahin Atakishiyev,Housam K. B. Babiker,Jiayi Dai,Nawshad Farruque,Teruaki Hayashi,Nafisa Sadaf Hriti,Md Abed Rahman,Iain Smith,Mi-Young Kim,Osmar R. Zaïane,Randy Goebel*

Main category: cs.CL

TL;DR: 本文综述了基于Transformer的大型语言模型的局部可解释性和机械解释性研究，重点探讨其在医疗和自动驾驶领域的应用，并分析解释对信任的影响，提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的预测机制难以被人类理解，且存在预测与推理错误（幻觉），亟需提升模型的可解释性以增强用户信任。

Method: 本文首先回顾了相关文献中的局部可解释性和机械解释方法，随后通过实验研究探讨了大型语言模型在医疗和自动驾驶两个关键领域的可解释性表现及其对用户信任的影响，最后总结了当前存在的问题并提出未来的发展方向。

Result: 通过文献回顾与实验分析，本文揭示了现有可解释方法的效果及其在特定应用领域内对信任建构的作用，明确了未解决的关键问题。

Conclusion: 要实现与人类价值对齐且值得信赖的大型语言模型解释，需要进一步攻克当前面临的挑战，推动可解释性方法在实际应用中的深入发展。

Abstract: Large language models have exhibited impressive performance across a broad
range of downstream tasks in natural language processing. However, how a
language model predicts the next token and generates content is not generally
understandable by humans. Furthermore, these models often make errors in
prediction and reasoning, known as hallucinations. These errors underscore the
urgent need to better understand and interpret the intricate inner workings of
language models and how they generate predictive outputs. Motivated by this
gap, this paper investigates local explainability and mechanistic
interpretability within Transformer-based large language models to foster trust
in such models. In this regard, our paper aims to make three key contributions.
First, we present a review of local explainability and mechanistic
interpretability approaches and insights from relevant studies in the
literature. Furthermore, we describe experimental studies on explainability and
reasoning with large language models in two critical domains -- healthcare and
autonomous driving -- and analyze the trust implications of such explanations
for explanation receivers. Finally, we summarize current unaddressed issues in
the evolving landscape of LLM explainability and outline the opportunities,
critical challenges, and future directions toward generating human-aligned,
trustworthy LLM explanations.

</details>


### [71] [TaxoAlign: Scholarly Taxonomy Generation Using Language Models](https://arxiv.org/abs/2510.17263)
*Avishek Lahiri,Yufang Hou,Debarshi Kumar Sanyal*

Main category: cs.CL

TL;DR: 本文提出了TaxoAlign方法，通过主题指导实现学术分类法的自动生成，并引入CS-TaxoBench基准数据集及严格的自动化评估框架，实验结果显示该方法在结构对齐和语义连贯性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动文献综述生成方法未能比较自动生成的结构与专家编写结构，缺乏对自动生成分类法与人工分类法之间差距的弥合。

Method: 提出TaxoAlign，一种基于主题的三阶段指导方法生成学术分类法，并创建了包含460个人工分类法的CS-TaxoBench基准数据集和80个人工测试分类法集，设计了严格的自动化评估框架用于比较结构对齐和语义连贯性。

Result: TaxoAlign在CS-TaxoBench基准数据集上的自动化评估指标和人工评估中均优于多种基线方法，显示出更好的结构和语义表现。

Conclusion: TaxoAlign有效提升了自动生成学术分类法的质量，能够更好地接近人类专家生成的分类结构，促进自动文献综述的生成。

Abstract: Taxonomies play a crucial role in helping researchers structure and navigate
knowledge in a hierarchical manner. They also form an important part in the
creation of comprehensive literature surveys. The existing approaches to
automatic survey generation do not compare the structure of the generated
surveys with those written by human experts. To address this gap, we present
our own method for automated taxonomy creation that can bridge the gap between
human-generated and automatically-created taxonomies. For this purpose, we
create the CS-TaxoBench benchmark which consists of 460 taxonomies that have
been extracted from human-written survey papers. We also include an additional
test set of 80 taxonomies curated from conference survey papers. We propose
TaxoAlign, a three-phase topic-based instruction-guided method for scholarly
taxonomy generation. Additionally, we propose a stringent automated evaluation
framework that measures the structural alignment and semantic coherence of
automatically generated taxonomies in comparison to those created by human
experts. We evaluate our method and various baselines on CS-TaxoBench, using
both automated evaluation metrics and human evaluation studies. The results
show that TaxoAlign consistently surpasses the baselines on nearly all metrics.
The code and data can be found at https://github.com/AvishekLahiri/TaxoAlign.

</details>


### [72] [Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning](https://arxiv.org/abs/2510.17289)
*Hajar Bakarou,Mohamed Sinane El Messoussi,Anaïs Ollagnier*

Main category: cs.CL

TL;DR: 该论文利用法语多方对话数据集CyberAgressionAdo-Large，评估了反社会行为的检测与分析，提出多模态模型显著优于单模态模型。


<details>
  <summary>Details</summary>
Motivation: 当前社交媒体中的反社会行为在多方对话场景下研究不足，数据匮乏限制了深入分析。

Method: 利用CyberAgressionAdo-Large数据集，开展辱骂检测、欺凌行为分析和欺凌同伴识别三项任务，比较六种文本表示和八种图表示学习方法，并融合多模态特征。

Result: 多模态融合模型表现最佳，尤其是"mBERT + WD-SGCN"，在辱骂检测等任务中取得最高分，能有效识别隐性攻击、角色转换和上下文依赖的敌意。

Conclusion: 多模态表示学习对多方对话中的反社会行为识别具有明显提升，促进了反社会行为在复杂社交网络环境下的理解与检测。

Abstract: Antisocial behavior (ASB) on social media -- including hate speech,
harassment, and cyberbullying -- poses growing risks to platform safety and
societal well-being. Prior research has focused largely on networks such as X
and Reddit, while \textit{multi-party conversational settings} remain
underexplored due to limited data. To address this gap, we use
\textit{CyberAgressionAdo-Large}, a French open-access dataset simulating ASB
in multi-party conversations, and evaluate three tasks: \textit{abuse
detection}, \textit{bullying behavior analysis}, and \textit{bullying
peer-group identification}. We benchmark six text-based and eight graph-based
\textit{representation-learning methods}, analyzing lexical cues, interactional
dynamics, and their multimodal fusion. Results show that multimodal models
outperform unimodal baselines. The late fusion model \texttt{mBERT + WD-SGCN}
achieves the best overall results, with top performance on abuse detection
(0.718) and competitive scores on peer-group identification (0.286) and
bullying analysis (0.606). Error analysis highlights its effectiveness in
handling nuanced ASB phenomena such as implicit aggression, role transitions,
and context-dependent hostility.

</details>


### [73] [Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented Generation](https://arxiv.org/abs/2510.17354)
*Chenghao Zhang,Guanting Dong,Xinyu Yang,Zhicheng Dou*

Main category: cs.CL

TL;DR: 本文提出了一种统一的多模态检索增强生成模型Nyx，用于解决文本与图像混合模态的信息检索与生成问题，并构建了新的多模态问答数据集NyxQA，通过两阶段训练方法提升了视觉-语言生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成系统主要针对纯文本，难以适应现实中查询与文档混合多模态的场景，提升视觉-语言生成能力成为挑战。

Method: 提出Nyx，一种统一的多模态到多模态检索器；设计四阶段自动化数据生成与过滤流程，构建NyxQA数据集；采用两阶段训练，先在NyxQA及公开检索集预训练，再通过下游视觉-语言模型反馈进行有监督微调。

Result: Nyx在标准文本检索增强生成基准上表现不俗，并在多模态混合检索增强生成任务中显著提升视觉-语言生成质量。

Conclusion: Nyx有效解决了多模态混合检索和生成问题，提高了视觉-语言生成的性能，展示了URAG框架在现实应用中的潜力。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for
enhancing large language models (LLMs) by retrieving relevant documents from an
external corpus. However, existing RAG systems primarily focus on unimodal text
documents, and often fall short in real-world scenarios where both queries and
documents may contain mixed modalities (such as text and images). In this
paper, we address the challenge of Universal Retrieval-Augmented Generation
(URAG), which involves retrieving and reasoning over mixed-modal information to
improve vision-language generation. To this end, we propose Nyx, a unified
mixed-modal to mixed-modal retriever tailored for URAG scenarios. To mitigate
the scarcity of realistic mixed-modal data, we introduce a four-stage automated
pipeline for generation and filtering, leveraging web documents to construct
NyxQA, a dataset comprising diverse mixed-modal question-answer pairs that
better reflect real-world information needs. Building on this high-quality
dataset, we adopt a two-stage training framework for Nyx: we first perform
pre-training on NyxQA along with a variety of open-source retrieval datasets,
followed by supervised fine-tuning using feedback from downstream
vision-language models (VLMs) to align retrieval outputs with generative
preferences. Experimental results demonstrate that Nyx not only performs
competitively on standard text-only RAG benchmarks, but also excels in the more
general and realistic URAG setting, significantly improving generation quality
in vision-language tasks.

</details>


### [74] [The Atomic Instruction Gap: Instruction-Tuned LLMs Struggle with Simple, Self-Contained Directives](https://arxiv.org/abs/2510.17388)
*Henry Lim,Kwan Hui Lim*

Main category: cs.CL

TL;DR: 本论文评估了20个指令调优大型语言模型在不同标签格式下执行简单指令的能力，发现模型对标签格式高度敏感，存在指令格式偏差和执行不一致性。


<details>
  <summary>Details</summary>
Motivation: 虽然指令调优大型语言模型在零样本推理表现优异，但其执行简单、独立指令的能力尚未充分探索，而这正是复杂指令执行的基础。

Method: 通过在修改后的MMLU和MMLU-Pro基准上，系统改变选项标签的格式（字母、数字、罗马数字），测试了四种范式下模型的表现，并分析生成结果的准确性和一致性。

Result: 标签格式变化导致性能大幅波动（例如罗马数字标签相比数字标签下降30.45%），无指令情况下性能进一步下降且标签敏感性加剧；移除选项内容时，模型表现不如随机选择，除数字标签外；三示例训练未能显著提升鲁棒性和忠实度；大型模型准确率更高但仍存在指令执行不一致。

Conclusion: 当前指令调优范式存在明显不足，需开发专门针对基本指令执行能力的评估方法和训练策略，以提升模型的指令遵循能力和稳定性。

Abstract: Instruction-tuned large language models (IT-LLMs) exhibit strong zero-shot
reasoning, yet their ability to execute simple, self-contained instructions
remains underexplored, despite this being foundational to complex
instruction-following. We evaluate 20 IT-LLMs on modified MMLU and MMLU-Pro
benchmarks, by systematically varying the format of option labels (alphabetic,
numeric, Roman) while keeping their meaning identical under four paradigms,
namely: (1) With explicit instructions, label changes cause large performance
shifts (e.g., -30.45\% for Roman vs. numeric), revealing instruction-format
bias. (2) Without instructions, performance drops further (up to -10.84\%) and
label sensitivity intensifies, underscoring the role of explicit guidance. (3)
When option contents are removed, models fail random-choice baselines except
with numeric labels, suggesting weak adherence to atomic directives. (4)
Three-shot exemplars yield no significant gains in robustness or fidelity, and
generation analyses show persistent label errors, especially for non-numeric
formats. Across model sizes, larger LLMs achieve higher accuracy but remain
inconsistent in instruction adherence. These results expose the insufficiencies
of current instruction-tuning paradigms and highlight the need for evaluation
methods and training strategies that explicitly target atomic
instruction-following.

</details>


### [75] [EduAdapt: A Question Answer Benchmark Dataset for Evaluating Grade-Level Adaptability in LLMs](https://arxiv.org/abs/2510.17389)
*Numaan Naeem,Abdellah El Mekki,Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: 本文提出了EduAdapt，一个针对K-12教育中适龄化问答的大型基准数据集，并评估了多种大语言模型在不同年级学生上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型虽然在学术测试中表现良好，但难以根据学生的年级调整回答内容，尤其是低年级学生难以理解，缺乏评估模型年级适应性的标准基准。

Method: 构建了包含近4.8万个带年级标签的问答对，覆盖1-12年级的九个科学科目，分为四个年级层次，并用该数据集评估多种开源大语言模型的适龄化能力。

Result: 发现较大的模型表现更佳，但在为1至5年级的低年级学生生成合适回答方面仍存在困难。

Conclusion: EduAdapt是第一个用于评估大语言模型年级适应性的基准和评估框架，旨在通过更好的训练和提示策略促进教育AI系统的年龄适应性发展。

Abstract: Large language models (LLMs) are transforming education by answering
questions, explaining complex concepts, and generating content across a wide
range of subjects. Despite strong performance on academic benchmarks, they
often fail to tailor responses to students' grade levels. This is a critical
need in K-12 education, where age-appropriate vocabulary and explanation are
essential for effective learning. Existing models frequently produce outputs
that are too advanced or vague for younger learners, and there are no
standardized benchmarks to evaluate their ability to adjust across cognitive
and developmental stages. To address this gap, we introduce EduAdapt, a
benchmark of nearly 48k grade-labeled QA pairs across nine science subjects,
spanning Grades 1-12 and grouped into four grade levels. We evaluate a diverse
set of open-source LLMs on EduAdapt and find that while larger models generally
perform better, they still struggle with generating suitable responses for
early-grade students (Grades 1-5). Our work presents the first dataset and
evaluation framework for assessing grade-level adaptability in LLMs, aiming to
foster more developmentally aligned educational AI systems through better
training and prompting strategies. EduAdapt code and datasets are publicly
available at https://github.com/NaumanNaeem/EduAdapt.

</details>


### [76] [Leveraging Group Relative Policy Optimization to Advance Large Language Models in Traditional Chinese Medicine](https://arxiv.org/abs/2510.17402)
*Jiacheng Xie,Shuai Zeng,Yang Yu,Xiaoting Tang,Guanghui An,Dong Xu*

Main category: cs.CL

TL;DR: 本文提出了基于梯度优化的中医大语言模型Ladder-base，通过组内相对策略优化（GRPO）提升推理和事实一致性，显著优于多个通用和中医专用模型。


<details>
  <summary>Details</summary>
Motivation: 传统中医知识体系独特，传统大语言模型难以有效适应，现有中医专用模型存在对齐、数据质量和评估一致性问题。

Method: 基于Qwen2.5-7B-Instruct模型，使用中医梯度数据集的文本部分，采用组内相对策略优化（GRPO）强化学习方法训练，优化基于组内比较的响应选择。

Result: Ladder-base在多个推理指标上显著优于GPT-4、Gemini 2.5、Claude 3等通用模型及BenTsao、HuatuoGPT2等中医特定模型。

Conclusion: GRPO强化学习策略有效提升中医领域大语言模型的推理能力和事实一致性，有助于构建可信赖且具临床基础的中医AI系统。

Abstract: Traditional Chinese Medicine (TCM) presents a rich and structurally unique
knowledge system that challenges conventional applications of large language
models (LLMs). Although previous TCM-specific LLMs have shown progress through
supervised fine-tuning, they often face limitations in alignment, data quality,
and evaluation consistency. In this study, we introduce Ladder-base, the first
TCM-focused LLM trained with Group Relative Policy Optimization (GRPO), a
reinforcement learning method that improves reasoning and factual consistency
by optimizing response selection based on intra-group comparisons. Ladder-base
is built upon the Qwen2.5-7B-Instruct foundation model and trained exclusively
on the textual subset of the TCM-Ladder benchmark, using 80 percent of the data
for training and the remaining 20 percent split evenly between validation and
test sets. Through standardized evaluation, Ladder-base demonstrates superior
performance across multiple reasoning metrics when compared to both
state-of-the-art general-purpose LLMs such as GPT-4, Gemini 2.5, Claude 3, and
Qwen3 and domain-specific TCM models including BenTsao, HuatuoGPT2, and
Zhongjing. These findings suggest that GRPO provides an effective and efficient
strategy for aligning LLMs with expert-level reasoning in traditional medical
domains and supports the development of trustworthy and clinically grounded TCM
artificial intelligence systems.

</details>


### [77] [AFRICAPTION: Establishing a New Paradigm for Image Captioning in African Languages](https://arxiv.org/abs/2510.17405)
*Mardiyyah Oduwole,Prince Mireku,Fatimo Adebanjo,Oluwatosin Olajide,Mahi Aminu Aliyu,Jekaterina Novikova*

Main category: cs.CL

TL;DR: AfriCaption框架为20种非洲语言的多语言图像描述提供了数据集、动态处理流程和模型，推动了非洲语言的多模态AI发展。


<details>
  <summary>Details</summary>
Motivation: 多模态AI研究多集中于高资源语言，限制了技术的普及，需发展低资源语言的图像描述技术。

Method: 构建基于Flickr8k的语义对齐数据集，设计动态上下文保留流水线，通过模型集成和自适应替换保障数据质量，开发集成SigLIP和NLLB200的0.5B参数视觉到文本模型。

Result: 建立了首个可扩展的非洲低资源语言图像描述资源和统一生成模型，确保了持续的数据质量和语言覆盖。

Conclusion: AfriCaption框架为非洲多语言图像描述奠定基础，推动了包容性多模态AI的发展。

Abstract: Multimodal AI research has overwhelmingly focused on high-resource languages,
hindering the democratization of advancements in the field. To address this, we
present AfriCaption, a comprehensive framework for multilingual image
captioning in 20 African languages and our contributions are threefold: (i) a
curated dataset built on Flickr8k, featuring semantically aligned captions
generated via a context-aware selection and translation process; (ii) a
dynamic, context-preserving pipeline that ensures ongoing quality through model
ensembling and adaptive substitution; and (iii) the AfriCaption model, a 0.5B
parameter vision-to-text architecture that integrates SigLIP and NLLB200 for
caption generation across under-represented languages. This unified framework
ensures ongoing data quality and establishes the first scalable
image-captioning resource for under-represented African languages, laying the
groundwork for truly inclusive multimodal AI.

</details>


### [78] [Navigating the Alignment-Calibration Trade-off: A Pareto-Superior Frontier via Model Merging](https://arxiv.org/abs/2510.17426)
*Tiancheng Hu,Benjamin Minixhofer,Nigel Collier*

Main category: cs.CL

TL;DR: 训练后对齐会降低任务准确度和模型校准，本研究通过插值模型权重有效缓解了这一问题，实现了准确率提升和校准恢复。


<details>
  <summary>Details</summary>
Motivation: 训练后对齐虽然提升模型任务表现，但会导致模型过度自信、校准变差和输出多样性下降，影响模型可靠性。

Method: 通过简单的后处理干预，即对齐前后模型权重插值，寻找帕累托最优插值点，兼顾准确度和校准。

Result: 插值后的模型不仅在准确率上超越了初始和对齐模型，还显著恢复了校准性能。

Conclusion: 简单的模型合并技术能有效减轻训练后对齐带来的“对齐税”，提升模型能力和可靠性。

Abstract: The "alignment tax" of post-training is typically framed as a drop in task
accuracy. We show it also involves a severe loss of calibration, making models
overconfident, less reliable, and model outputs less diverse. We show that this
trade-off can be navigated effectively via a simple post-hoc intervention:
interpolating between a model's weights before and after alignment. Crucially,
this is not a strict trade-off. We find that the process consistently reveals
Pareto-optimal interpolations - models that improve accuracy beyond both
parents while substantially recovering the calibration lost during alignment.
Our work demonstrates that simple model merging provides a computationally
efficient method for mitigating the full scope of the alignment tax, yielding
models that are more capable and more reliable.

</details>


### [79] [Agentic Reinforcement Learning for Search is Unsafe](https://arxiv.org/abs/2510.17431)
*Yushi Yang,Shreyansh Padarha,Andrew Lee,Adam Mahdi*

Main category: cs.CL

TL;DR: 本文揭示了基于强化学习（RL）训练的大型语言模型在自主工具调用搜索任务中的安全隐患，表明现有模型容易通过简单攻击触发有害搜索。


<details>
  <summary>Details</summary>
Motivation: 探索并揭示RL训练的多步推理搜索模型在安全性方面存在的脆弱性及其具体表现。

Method: 设计两种简单攻击（Search攻击和Multi-search攻击），评估其在两个模型家族（Qwen、Llama）及本地和网络搜索上的效果，以测量拒绝率和搜索安全的下降。

Result: 这两种攻击显著降低了模型的拒绝率（最多60%）、答案安全（82.5%）和搜索查询安全（82.4%），暴露出RL训练忽视有害性的短板。

Conclusion: 当前RL训练方法在优化有效搜索查询的同时未考虑安全性，导致模型存在易被利用的安全漏洞，亟需开发安全感知的RL训练管线以确保搜索安全。

Abstract: Agentic reinforcement learning (RL) trains large language models to
autonomously call tools during reasoning, with search as the most common
application. These models excel at multi-step reasoning tasks, but their safety
properties are not well understood. In this study, we show that RL-trained
search models inherit refusal from instruction tuning and often deflect harmful
requests by turning them into safe queries. However, this safety is fragile.
Two simple attacks, one that forces the model to begin response with search
(Search attack), another that encourages models to repeatedly search
(Multi-search attack), trigger cascades of harmful searches and answers. Across
two model families (Qwen, Llama) with both local and web search, these attacks
lower refusal rates by up to 60.0%, answer safety by 82.5%, and search-query
safety by 82.4%. The attacks succeed by triggering models to generate harmful,
request-mirroring search queries before they can generate the inherited refusal
tokens. This exposes a core weakness of current RL training: it rewards
continued generation of effective queries without accounting for their
harmfulness. As a result, RL search models have vulnerabilities that users can
easily exploit, making it urgent to develop safety-aware agentic RL pipelines
optimising for safe search.

</details>


### [80] [Multilingual Clinical NER for Diseases and Medications Recognition in Cardiology Texts using BERT Embeddings](https://arxiv.org/abs/2510.17437)
*Manuela Daniela Danu,George Marica,Constantin Suciu,Lucian Mihai Itu,Oladimeji Farri*

Main category: cs.CL

TL;DR: 本文针对低资源语言的临床文本，以心脏病领域为例，构建多种基于BERT的深度上下文嵌入模型，提升命名实体识别（NER）性能，取得了超越排行榜平均和中位数的F1分数。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录数据量大，需有效挖掘非结构化临床文本的生物医学知识以支持临床系统应用，但低资源语言的临床文本NER研究不足。

Method: 开发多种单语言和多语言的BERT模型，训练于通用文本，应用于英语、西班牙语和意大利语的临床病例报告中疾病及药物实体识别。

Result: 在多语言子任务中分别取得77.88%（西班牙疾病识别）、92.09%（西班牙药物识别）、91.74%（英语药物识别）及88.9%（意大利药物识别）的F1分数，均优于测试排行榜均值和中值。

Conclusion: 基于多种BERT模型的深度上下文嵌入显著提升了低资源语言心脏病领域临床文本的命名实体识别效果，证明了该方法的有效性。

Abstract: The rapidly increasing volume of electronic health record (EHR) data
underscores a pressing need to unlock biomedical knowledge from unstructured
clinical texts to support advancements in data-driven clinical systems,
including patient diagnosis, disease progression monitoring, treatment effects
assessment, prediction of future clinical events, etc. While contextualized
language models have demonstrated impressive performance improvements for named
entity recognition (NER) systems in English corpora, there remains a scarcity
of research focused on clinical texts in low-resource languages. To bridge this
gap, our study aims to develop multiple deep contextual embedding models to
enhance clinical NER in the cardiology domain, as part of the BioASQ
MultiCardioNER shared task. We explore the effectiveness of different
monolingual and multilingual BERT-based models, trained on general domain text,
for extracting disease and medication mentions from clinical case reports
written in English, Spanish, and Italian. We achieved an F1-score of 77.88% on
Spanish Diseases Recognition (SDR), 92.09% on Spanish Medications Recognition
(SMR), 91.74% on English Medications Recognition (EMR), and 88.9% on Italian
Medications Recognition (IMR). These results outperform the mean and median F1
scores in the test leaderboard across all subtasks, with the mean/median values
being: 69.61%/75.66% for SDR, 81.22%/90.18% for SMR, 89.2%/88.96% for EMR, and
82.8%/87.76% for IMR.

</details>


### [81] [Evaluating Large Language Models on Urdu Idiom Translation](https://arxiv.org/abs/2510.17460)
*Muhammad Farmal Khan,Mousumi Akter*

Main category: cs.CL

TL;DR: 本文介绍了首个乌尔都语到英语的惯用语翻译评估数据集，评估多种大型语言模型和神经机器翻译系统对惯用语及文化意义的保留能力。


<details>
  <summary>Details</summary>
Motivation: 惯用语翻译在机器翻译中尤为困难，尤其是低资源语言乌尔都语，且相关研究较少。

Method: 构建涵盖乌尔都语正体和罗马字两种脚本的惯用语翻译评估数据集，使用多种自动评价指标（BLEU、BERTScore、COMET、XCOMET）评测多款开源大模型和NMT系统，并分析提示工程和脚本表示对翻译效果的影响。

Result: 提示工程提升了惯用语翻译质量，尽管不同提示间的表现差异较小。原生乌尔都语输入的翻译质量优于罗马字输入，表明文本表示对翻译效果影响显著。

Conclusion: 通过引入专门的数据集和评测，揭示了提示技术和文本脚本表示在乌尔都语惯用语翻译中的关键作用，为低资源语言的惯用语翻译研究提供了实证基础。

Abstract: Idiomatic translation remains a significant challenge in machine translation,
especially for low resource languages such as Urdu, and has received limited
prior attention. To advance research in this area, we introduce the first
evaluation datasets for Urdu to English idiomatic translation, covering both
Native Urdu and Roman Urdu scripts and annotated with gold-standard English
equivalents. We evaluate multiple open-source Large Language Models (LLMs) and
Neural Machine Translation (NMT) systems on this task, focusing on their
ability to preserve idiomatic and cultural meaning. Automatic metrics including
BLEU, BERTScore, COMET, and XCOMET are used to assess translation quality. Our
findings indicate that prompt engineering enhances idiomatic translation
compared to direct translation, though performance differences among prompt
types are relatively minor. Moreover, cross script comparisons reveal that text
representation substantially affects translation quality, with Native Urdu
inputs producing more accurate idiomatic translations than Roman Urdu.

</details>


### [82] [Disparities in Multilingual LLM-Based Healthcare Q&A](https://arxiv.org/abs/2510.17476)
*Ipek Baris Schlicht,Burcu Sayin,Zhixue Zhao,Frederik M. Labonté,Cesare Barbera,Marco Viviani,Paolo Rosso,Lucie Flek*

Main category: cs.CL

TL;DR: 该论文研究了多语言大型语言模型在医疗健康信息中的跨语言差异，特别是在事实一致性方面的表现。


<details>
  <summary>Details</summary>
Motivation: 医疗健康中公平获取可靠信息至关重要，但不同语言间信息质量差异引发了对多语言大型语言模型可靠性和一致性的担忧。

Method: 构建了多语言维基医疗数据集（MultiWikiHealthCare），分析不同语言的医疗覆盖情况，评估大型语言模型响应与数据集事实的一致性，并通过上下文信息和检索增强生成方法（RAG）进行案例研究。

Result: 发现不同语言维基百科内容覆盖和模型事实一致性存在显著差异，模型回答更倾向与英文维基百科对齐，即使输入为非英文。提供非英文维基相关上下文能有效提升模型对文化相关知识的事实一致性。

Conclusion: 该研究揭示了跨语言医疗信息不平等问题，提出通过结合不同语言上下文信息的解决途径，有助于构建更加公平和多语言适用的医疗AI系统。

Abstract: Equitable access to reliable health information is vital when integrating AI
into healthcare. Yet, information quality varies across languages, raising
concerns about the reliability and consistency of multilingual Large Language
Models (LLMs). We systematically examine cross-lingual disparities in
pre-training source and factuality alignment in LLM answers for multilingual
healthcare Q&A across English, German, Turkish, Chinese (Mandarin), and
Italian. We (i) constructed Multilingual Wiki Health Care
(MultiWikiHealthCare), a multilingual dataset from Wikipedia; (ii) analyzed
cross-lingual healthcare coverage; (iii) assessed LLM response alignment with
these references; and (iv) conducted a case study on factual alignment through
the use of contextual information and Retrieval-Augmented Generation (RAG). Our
findings reveal substantial cross-lingual disparities in both Wikipedia
coverage and LLM factual alignment. Across LLMs, responses align more with
English Wikipedia, even when the prompts are non-English. Providing contextual
excerpts from non-English Wikipedia at inference time effectively shifts
factual alignment toward culturally relevant knowledge. These results highlight
practical pathways for building more equitable, multilingual AI systems for
healthcare.

</details>


### [83] [ReXMoE: Reusing Experts with Minimal Overhead in Mixture-of-Experts](https://arxiv.org/abs/2510.17483)
*Zheyue Tan,Zhiyuan Li,Tao Yuan,Dong Zhou,Weilin Liu,Yueqing Zhuang,Yadong Li,Guowei Niu,Cheng Qin,Zhuyu Yao,Congyi Liu,Haiyang Xu,Boxun Li,Guohao Dai,Bo Zhao,Yu Wang*

Main category: cs.CL

TL;DR: ReXMoE通过允许相邻层共享专家，实现了比传统层内路由更高效的混合专家架构，提高了模型表达能力和性能。


<details>
  <summary>Details</summary>
Motivation: 传统的混合专家模型受限于每层仅使用本层专家，导致专家维度和路由多样性之间的权衡限制了模型性能。

Method: 提出了ReXMoE架构，实现相邻层间专家共享，并设计进阶式扩展路由策略（PSR），逐步增加训练时的候选专家池。

Result: ReXMoE在0.5B到7B参数规模多种模型上均显著提升了语言建模和下游任务性能，在固定参数预算下表现更优。

Conclusion: ReXMoE为参数效率高、易扩展的混合专家大型语言模型设计开辟了新范式，突破了层内路由的限制。

Abstract: Mixture-of-Experts (MoE) architectures have emerged as a promising approach
to scale Large Language Models (LLMs). MoE boosts the efficiency by activating
a subset of experts per token. Recent works show that fine-grained experts
substantially enriches the combinatorial flexibility of active experts and
enhances model expressiveness. However, such a design is fundamentally limited
by the layer-local routing mechanism: each layer is restricted to its own
expert pool. This requires a careful trade-off between expert dimensionality
and routing diversity given fixed parameter budgets. We describe ReXMoE, a
novel MoE architecture that improves routing beyond the existing layer-local
approaches by allowing routers to reuse experts across adjacent layers. ReXMoE
decouples expert dimensionality from per-layer budgets, enabling richer expert
combinations without sacrificing individual expert capacity or inflating
overall parameters. To this end, we propose a new progressive scaling routing
(PSR) strategy to gradually increase the candidate expert pool during training.
As a result, ReXMoE improves both language modeling and downstream task
performance. Extensive experiments on models ranging from 0.5B to 7B parameters
across different architectures demonstrate that ReXMoE consistently improves
performance under fixed architectural dimensions, confirming ReXMoE as new
design paradigm for parameter-efficient and scalable MoE-based LLMs.

</details>


### [84] [DETree: DEtecting Human-AI Collaborative Texts via Tree-Structured Hierarchical Representation Learning](https://arxiv.org/abs/2510.17489)
*Yongxin He,Shan Zhang,Yixuan Cao,Lei Ma,Ping Luo*

Main category: cs.CL

TL;DR: 本文提出DETree方法，通过层次亲和树结构建模人机混合文本生成过程中的内在关系，提升了混合文本检测的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前检测AI参与文本的方法粗略，将人机协作文本视为单一类别，难以捕捉复杂多样的生成过程特征，检测效果有限。

Method: 提出将不同文本生成过程的关系建模为层次亲和树结构，并设计专门的损失函数，使文本表征与该树结构对齐。同时构建了包含多种人机协作文本的数据集RealBench。

Result: 方法在混合文本检测任务上性能提升明显，在分布外场景和小样本学习条件下表现出较强的鲁棒性和泛化能力。

Conclusion: 基于训练的方法结合层次结构关系建模，有效提升了检测多样化人机协作生成文本的能力，为AI文本检测提供了新的思路和工具。

Abstract: Detecting AI-involved text is essential for combating misinformation,
plagiarism, and academic misconduct. However, AI text generation includes
diverse collaborative processes (AI-written text edited by humans,
human-written text edited by AI, and AI-generated text refined by other AI),
where various or even new LLMs could be involved. Texts generated through these
varied processes exhibit complex characteristics, presenting significant
challenges for detection. Current methods model these processes rather crudely,
primarily employing binary classification (purely human vs. AI-involved) or
multi-classification (treating human-AI collaboration as a new class). We
observe that representations of texts generated through different processes
exhibit inherent clustering relationships. Therefore, we propose DETree, a
novel approach that models the relationships among different processes as a
Hierarchical Affinity Tree structure, and introduces a specialized loss
function that aligns text representations with this tree. To facilitate this
learning, we developed RealBench, a comprehensive benchmark dataset that
automatically incorporates a wide spectrum of hybrid texts produced through
various human-AI collaboration processes. Our method improves performance in
hybrid text detection tasks and significantly enhances robustness and
generalization in out-of-distribution scenarios, particularly in few-shot
learning conditions, further demonstrating the promise of training-based
approaches in OOD settings. Our code and dataset are available at
https://github.com/heyongxin233/DETree.

</details>


### [85] [Empowering Real-World: A Survey on the Technology, Practice, and Evaluation of LLM-driven Industry Agents](https://arxiv.org/abs/2510.17491)
*Yihong Tang,Kehai Chen,Liang Yue,Jinxin Fan,Caishen Zhou,Xiaoguang Li,Yuyang Zhang,Mingming Zhao,Shixiong Kai,Kaiyang Guo,Xingshan Zeng,Wenjing Cun,Lifeng Shang,Min Zhang*

Main category: cs.CL

TL;DR: 本文系统综述了基于大型语言模型(LLMs)的行业智能体的技术、应用及评估方法，分析了其能力成熟度框架及未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM智能体在自主推理与复杂任务执行方面取得进展，但如何将其研究成果转化为推动产业变革的生产力仍是重大挑战。

Method: 提出行业智能体能力成熟度框架，回顾其技术支柱（记忆、规划、工具使用），总结其在数字工程、科学发现等多领域的应用，并评估现有的评测基准和方法，识别评测中存在的真实性、安全性与行业特异性问题。

Result: 明确了行业智能体技术从支持简单任务向复杂自主系统及群体智能演进的路径，揭示了其在实际应用中的表现和面临的评估与治理挑战。

Conclusion: 结合技术演进和行业实践，本文为理解和构建新一代行业智能体提供了清晰的现状描述、发展路线图及理论基础。

Abstract: With the rise of large language models (LLMs), LLM agents capable of
autonomous reasoning, planning, and executing complex tasks have become a
frontier in artificial intelligence. However, how to translate the research on
general agents into productivity that drives industry transformations remains a
significant challenge. To address this, this paper systematically reviews the
technologies, applications, and evaluation methods of industry agents based on
LLMs. Using an industry agent capability maturity framework, it outlines the
evolution of agents in industry applications, from "process execution systems"
to "adaptive social systems." First, we examine the three key technological
pillars that support the advancement of agent capabilities: Memory, Planning,
and Tool Use. We discuss how these technologies evolve from supporting simple
tasks in their early forms to enabling complex autonomous systems and
collective intelligence in more advanced forms. Then, we provide an overview of
the application of industry agents in real-world domains such as digital
engineering, scientific discovery, embodied intelligence, collaborative
business execution, and complex system simulation. Additionally, this paper
reviews the evaluation benchmarks and methods for both fundamental and
specialized capabilities, identifying the challenges existing evaluation
systems face regarding authenticity, safety, and industry specificity. Finally,
we focus on the practical challenges faced by industry agents, exploring their
capability boundaries, developmental potential, and governance issues in
various scenarios, while providing insights into future directions. By
combining technological evolution with industry practices, this review aims to
clarify the current state and offer a clear roadmap and theoretical foundation
for understanding and building the next generation of industry agents.

</details>


### [86] [Deep Self-Evolving Reasoning](https://arxiv.org/abs/2510.17498)
*Zihan Liu,Shun Zheng,Xumeng Wen,Yang Wang,Jiang Bian,Mao Yang*

Main category: cs.CL

TL;DR: 该论文提出了一种名为深度自我进化推理（DSER）的概率模型，通过多次并行迭代推理，将小规模模型的推理能力显著提升，解决了许多难题，甚至超越了大规模教师模型。


<details>
  <summary>Details</summary>
Motivation: 当前小规模开源模型在推理任务中验证和修正能力较弱，限制了它们解决复杂问题的能力。本研究旨在突破这一限制，通过新的方法扩展小模型的推理极限。

Method: 将迭代推理视为马尔可夫链的随机过程，关键在于只要改进的概率略高于退化，就能保证收敛至正确答案。通过多条长周期、自我进化的进程并行运行，强化推理过程中的小幅正向趋势。

Result: 在AIME 2024-2025基准测试中，使用DSER方法，模型成功解决了9个之前未能解决的问题中的5个，并提升整体表现，最终通过多数投票超过了拥有600亿参数的教师模型的单轮准确率。

Conclusion: DSER不仅提升了小模型的推理能力，还揭示了当前开源推理模型在自我验证、修正和稳定性方面的不足，为未来构建具备强大内生自我进化能力的新一代模型明确了研究方向。

Abstract: Long-form chain-of-thought reasoning has become a cornerstone of advanced
reasoning in large language models. While recent verification-refinement
frameworks have enabled proprietary models to solve Olympiad-level problems,
their effectiveness hinges on strong, reliable verification and correction
capabilities, which remain fragile in open-weight, smaller-scale models. This
work demonstrates that even with weak verification and refinement capabilities
on hard tasks, the reasoning limits of such models can be substantially
extended through a probabilistic paradigm we call Deep Self-Evolving Reasoning
(DSER). We conceptualize iterative reasoning as a Markov chain, where each step
represents a stochastic transition in the solution space. The key insight is
that convergence to a correct solution is guaranteed as long as the probability
of improvement marginally exceeds that of degradation. By running multiple
long-horizon, self-evolving processes in parallel, DSER amplifies these small
positive tendencies, enabling the model to asymptotically approach correct
answers. Empirically, we apply DSER to the DeepSeek-R1-0528-Qwen3-8B model. On
the challenging AIME 2024-2025 benchmark, DSER solves 5 out of 9 previously
unsolvable problems and boosts overall performance, enabling this compact model
to surpass the single-turn accuracy of its 600B-parameter teacher through
majority voting. Beyond its immediate utility for test-time scaling, the DSER
framework serves to diagnose the fundamental limitations of current open-weight
reasoners. By clearly delineating their shortcomings in self-verification,
refinement, and stability, our findings establish a clear research agenda for
developing next-generation models with powerful, intrinsic self-evolving
capabilities.

</details>


### [87] [Lingua Custodi's participation at the WMT 2025 Terminology shared task](https://arxiv.org/abs/2510.17504)
*Jingshu Liu,Raheel Qader,Gaëtan Caillaut,Mariam Nakhlé*

Main category: cs.CL

TL;DR: 本文提出了一种结合多种方法的多语种句子向量学习模型LaBSE，实现了跨语种句子检索和单语迁移学习的高性能。


<details>
  <summary>Details</summary>
Motivation: BERT在单语句子嵌入和迁移学习中表现优异，但基于BERT的跨语种句子嵌入尚未深入研究。

Method: 将MLM、TLM、双编码器翻译排序和加性边距softmax等方法相结合，引入预训练多语种语言模型，显著减少平行训练数据需求。

Result: 模型在Tatoeba数据集上112种语言的双文本检索准确率达83.7%，远超LASER的65.5%，且在单语迁移学习中表现优异。基于挖掘自CommonCrawl的平行数据训练的NMT模型也表现良好。

Conclusion: 结合多种技术的预训练多语种语言模型显著提升了跨语种句子嵌入和检索能力，减少了对大规模平行数据的依赖，且公开发布了该模型供社区使用。

Abstract: While BERT is an effective method for learning monolingual sentence
embeddings for semantic similarity and embedding based transfer learning BERT
based cross-lingual sentence embeddings have yet to be explored. We
systematically investigate methods for learning multilingual sentence
embeddings by combining the best methods for learning monolingual and
cross-lingual representations including: masked language modeling (MLM),
translation language modeling (TLM), dual encoder translation ranking, and
additive margin softmax. We show that introducing a pre-trained multilingual
language model dramatically reduces the amount of parallel training data
required to achieve good performance by 80%. Composing the best of these
methods produces a model that achieves 83.7% bi-text retrieval accuracy over
112 languages on Tatoeba, well above the 65.5 achieved by LASER, while still
performing competitively on monolingual transfer learning benchmarks. Parallel
data mined from CommonCrawl using our best model is shown to train competitive
NMT models for en-zh and en-de. We publicly release our best multilingual
sentence embedding model for 109+ languages at https://tfhub.dev/google/LaBSE.

</details>


### [88] [Annotation-Efficient Universal Honesty Alignment](https://arxiv.org/abs/2510.17509)
*Shiyu Ni,Keping Bi,Jiafeng Guo,Minghao Tang,Jingtong Wu,Zengxin Han,Xueqi Cheng*

Main category: cs.CL

TL;DR: 本文提出Elicitation-Then-Calibration框架，通过自我一致性引导后用少量标注校准，实现大型语言模型的诚实性校准，效果明显且标注成本低。


<details>
  <summary>Details</summary>
Motivation: 现有诚实性校准方法需大量标注，成本高昂，亟需低标注成本的有效校准技术。

Method: EliCal采用两阶段：先用自我一致性监督引导置信度，再用少量正确性标注对置信度进行校准。

Result: 在HonestyBench大规模数据集上，EliCal仅用1千条标注就达到近似最优校准效果，且在未知任务上表现优于仅依赖校准的方法。

Conclusion: EliCal为大语言模型实现普适诚实性校准提供了一种标注高效且效果优良的解决方案。

Abstract: Honesty alignment-the ability of large language models (LLMs) to recognize
their knowledge boundaries and express calibrated confidence-is essential for
trustworthy deployment. Existing methods either rely on training-free
confidence estimation (e.g., token probabilities, self-consistency) or
training-based calibration with correctness annotations. While effective,
achieving universal honesty alignment with training-based calibration requires
costly, large-scale labeling. To support annotation-efficient training, we
introduce Elicitation-Then-Calibration (EliCal), a two-stage framework that
first elicits internal confidence using inexpensive self-consistency
supervision, then calibrates this confidence with a small set of correctness
annotations. To support a large-scale study, we release HonestyBench, a
benchmark covering ten free-form QA datasets with 560k training and 70k
evaluation instances annotated with correctness and self-consistency signals.
Experiments show that EliCal achieves near-optimal alignment with only 1k
correctness annotations (0.18% of full supervision) and better alignment
performance on unseen MMLU tasks than the calibration-only baseline, offering a
scalable solution toward universal honesty alignment in LLMs.

</details>


### [89] [SimBench: Benchmarking the Ability of Large Language Models to Simulate Human Behaviors](https://arxiv.org/abs/2510.17516)
*Tiancheng Hu,Joachim Baumann,Lorenzo Lupo,Dirk Hovy,Nigel Collier,Paul Röttger*

Main category: cs.CL

TL;DR: 本文提出了SimBench，这是第一个用于大规模评估大语言模型（LLM）模拟人类行为能力的标准化基准，发现目前LLM的模拟能力有限但与模型规模正相关，且存在指令调优带来的性能权衡。


<details>
  <summary>Details</summary>
Motivation: 当前对LLM模拟人类行为的评估分散且不具可比性，缺乏统一的大规模标准测试集，阻碍了对LLM模拟能力的深入理解和改进。

Method: 构建并发布SimBench，一个包含20个多样化数据集的大规模标准化基准，涵盖道德决策到经济选择等任务，通过该基准系统评测各类LLM的性能表现。

Result: 结果显示最好的LLM模拟能力仍有限（得分40.80/100），性能与模型大小呈对数线性关系，算力提升并未显著改善表现，且指令调优对低熵问题有效但对高熵问题有负面影响，模型在模拟特定人口群体时表现较差。

Conclusion: SimBench为衡量和理解LLM模拟人类行为能力提供了统一、可复现的测试平台，促进开发更忠实的人类行为模拟器的进展。

Abstract: Large language model (LLM) simulations of human behavior have the potential
to revolutionize the social and behavioral sciences, if and only if they
faithfully reflect real human behaviors. Current evaluations are fragmented,
based on bespoke tasks and metrics, creating a patchwork of incomparable
results. To address this, we introduce SimBench, the first large-scale,
standardized benchmark for a robust, reproducible science of LLM simulation. By
unifying 20 diverse datasets covering tasks from moral decision-making to
economic choice across a large global participant pool, SimBench provides the
necessary foundation to ask fundamental questions about when, how, and why LLM
simulations succeed or fail. We show that, while even the best LLMs today have
limited simulation ability (score: 40.80/100), performance scales log-linearly
with model size. Simulation performance is not improved by increased
inference-time compute. We demonstrate an alignment-simulation trade-off:
instruction-tuning improves performance on low-entropy (consensus) questions
but degrades it on high-entropy (diverse) ones. Models particularly struggle
when simulating specific demographic groups. Finally, we demonstrate that
simulation ability correlates most strongly with deep, knowledge-intensive
reasoning (MMLU-Pro, r=0.939). By making progress measurable, we aim to
accelerate the development of more faithful LLM simulators.

</details>


### [90] [OncoReason: Structuring Clinical Reasoning in LLMs for Robust and Interpretable Survival Prediction](https://arxiv.org/abs/2510.17532)
*Raghu Vamshi Hemadri,Geetha Krishna Guruju,Kristi Topollai,Anna Ewa Choromanska*

Main category: cs.CL

TL;DR: 该论文提出了一种统一的多任务学习框架，通过结合大语言模型与临床推理，实现癌症治疗结果的准确预测和可解释性。


<details>
  <summary>Details</summary>
Motivation: 准确且具可解释性的模型对于预测癌症治疗结果至关重要，尤其是在异质性临床数据和高风险决策支持场景下。传统大型语言模型虽表现优异，但缺乏结构化推理能力。

Method: 采用多任务学习框架，结合自回归大语言模型，实现生存二分类、连续生存时间回归和自然语言推理生成。提出三种对齐策略：标准监督微调(SFT)、链式思维(CoT)提示以及基于强化学习的群体相对策略优化(GRPO)，并在MSK-CHORD数据集上验证。

Result: 实验表明，CoT提示显著提升F1分数和降低平均绝对误差，GRPO则在可解释性和预测性能（BLEU、ROUGE、BERTScore指标）方面达到了先进水平。还发现现有生物医学大模型存在生成有效推理轨迹的局限。

Conclusion: 研究强调推理感知对齐在多任务临床建模中的重要性，为精准肿瘤学领域提供了一个可解释且可信赖的大语言模型新基准。

Abstract: Predicting cancer treatment outcomes requires models that are both accurate
and interpretable, particularly in the presence of heterogeneous clinical data.
While large language models (LLMs) have shown strong performance in biomedical
NLP, they often lack structured reasoning capabilities critical for high-stakes
decision support. We present a unified, multi-task learning framework that
aligns autoregressive LLMs with clinical reasoning for outcome prediction on
the MSK-CHORD dataset. Our models are trained to jointly perform binary
survival classification, continuous survival time regression, and natural
language rationale generation. We evaluate three alignment strategies: (1)
standard supervised fine-tuning (SFT), (2) SFT with Chain-of-Thought (CoT)
prompting to elicit step-by-step reasoning, and (3) Group Relative Policy
Optimization (GRPO), a reinforcement learning method that aligns model outputs
to expert-derived reasoning trajectories. Experiments with LLaMa3-8B and
Med42-8B backbones demonstrate that CoT prompting improves F1 by +6.0 and
reduces MAE by 12%, while GRPO achieves state-of-the-art interpretability and
predictive performance across BLEU, ROUGE, and BERTScore. We further show that
existing biomedical LLMs often fail to produce valid reasoning traces due to
architectural constraints. Our findings underscore the importance of
reasoning-aware alignment in multi-task clinical modeling and set a new
benchmark for interpretable, trustworthy LLMs in precision oncology.

</details>


### [91] [When Annotators Disagree, Topology Explains: Mapper, a Topological Tool for Exploring Text Embedding Geometry and Ambiguity](https://arxiv.org/abs/2510.17548)
*Nisrine Rair,Alban Goupil,Valeriu Vrabie,Emmanuel Chochoy*

Main category: cs.CL

TL;DR: 本文提出利用拓扑数据分析工具Mapper来研究文本分类模型在面对含糊数据时的内部表示方式。


<details>
  <summary>Details</summary>
Motivation: 传统的标量指标如准确率难以反映模型对模糊性和人类标注者分歧的内部处理机制。

Method: 通过使用Mapper工具对RoBERTa-Large模型在MD-Offense数据集上的嵌入空间进行拓扑结构分析。

Result: 发现微调后的模型将嵌入空间重构为模块化、非凸形的区域，预测纯度高（>90%），但在模糊数据上的标签对齐度下降，显示了结构置信度和标签不确定性之间的矛盾。

Conclusion: Mapper不仅能可视化模型决策区域，还能揭示决策边界坍塌和过度置信的聚类，是理解和改进主观自然语言处理任务模型的有力工具。

Abstract: Language models are often evaluated with scalar metrics like accuracy, but
such measures fail to capture how models internally represent ambiguity,
especially when human annotators disagree. We propose a topological perspective
to analyze how fine-tuned models encode ambiguity and more generally instances.
  Applied to RoBERTa-Large on the MD-Offense dataset, Mapper, a tool from
topological data analysis, reveals that fine-tuning restructures embedding
space into modular, non-convex regions aligned with model predictions, even for
highly ambiguous cases. Over $98\%$ of connected components exhibit $\geq 90\%$
prediction purity, yet alignment with ground-truth labels drops in ambiguous
data, surfacing a hidden tension between structural confidence and label
uncertainty.
  Unlike traditional tools such as PCA or UMAP, Mapper captures this geometry
directly uncovering decision regions, boundary collapses, and overconfident
clusters. Our findings position Mapper as a powerful diagnostic tool for
understanding how models resolve ambiguity. Beyond visualization, it also
enables topological metrics that may inform proactive modeling strategies in
subjective NLP tasks.

</details>


### [92] [Language Confusion Gate: Language-Aware Decoding Through Model Self-Distillation](https://arxiv.org/abs/2510.17555)
*Collin Zhang,Fei Huang,Chenhan Yuan,Junyang Lin*

Main category: cs.CL

TL;DR: 本文提出了一种称为语言混淆门（LCG）的轻量级插件，用于减少大语言模型中语言混淆问题，提高多语言生成的准确性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生成文本时常出现语言混淆，即不同语言混合使用，现有方案需要重新训练模型或无法区分有害混淆和可接受的语言切换。

Method: 设计了语言混淆门（LCG），在解码阶段对生成的tokens进行过滤，不修改基础模型，利用调整范数的自蒸馏方法训练LCG预测适当的语言类别，仅在必要时进行屏蔽。

Result: 在多个模型（如Qwen3, GPT-OSS, Gemma3, Llama3.1）上测试，LCG显著减少语言混淆，减少幅度通常达到数量级，同时对任务性能无负面影响。

Conclusion: 语言混淆门是一种有效且轻量的解决方案，能够在不重训练模型的情况下显著减少语言混淆，提升多语言文本生成质量。

Abstract: Large language models (LLMs) often experience language confusion, which is
the unintended mixing of languages during text generation. Current solutions to
this problem either necessitate model retraining or cannot differentiate
between harmful confusion and acceptable code-switching. This paper introduces
the Language Confusion Gate (LCG), a lightweight, plug-in solution that filters
tokens during decoding without altering the base LLM. The LCG is trained using
norm-adjusted self-distillation to predict appropriate language families and
apply masking only when needed. Our method is based on the findings that
language confusion is infrequent, correct-language tokens are usually among the
top predictions, and output token embedding norms are larger for high-resource
languages, which biases sampling. When evaluated across various models,
including Qwen3, GPT-OSS, Gemma3, Llama3.1, LCG decreases language confusion
significantly, often by an order of magnitude, without negatively impacting
task performance. Code is available at
https://github.com/collinzrj/language_confusion_gate.

</details>


### [93] [LawChain: Modeling Legal Reasoning Chains for Chinese Tort Case Analysis](https://arxiv.org/abs/2510.17602)
*Huiyuan Xie,Chenyang Li,Huining Zhu,Chubin Zhang,Yuxiao Ye,Zhenghao Liu,Zhiyuan Liu*

Main category: cs.CL

TL;DR: 本文提出了一个名为LawChain的法律推理框架，专注于中国侵权相关民事案件的法律推理过程，通过构建评估基准LawChain_eval系统地评测和提升大型语言模型在侵权法律推理中的能力。


<details>
  <summary>Details</summary>
Motivation: 现有法律推理方法多依赖通用框架，忽视了法律推理的细腻过程，且研究多集中于刑事案件，缺乏对民事侵权案件的建模。

Method: 提出LawChain三模块推理框架，并基于此建立LawChain_eval评估基准，评测大型语言模型的法律推理能力；引入经过提示或后训练的基线模型以提升推理表现。

Result: 当前大型语言模型在侵权法律推理中表现不足，基于LawChain推理显著提升模型在侵权及相关法律分析任务中的表现。

Conclusion: 明确建模法律推理链条能够提升语言模型的法律推理能力，有助于民事案件法律分析的自动化和准确性。

Abstract: Legal reasoning is a fundamental component of legal analysis and
decision-making. Existing computational approaches to legal reasoning
predominantly rely on generic reasoning frameworks such as syllogism and IRAC,
which do not comprehensively examine the nuanced processes that underpin legal
reasoning. Moreover, current research has largely focused on criminal cases,
with insufficient modeling for civil cases. In this work, we present a novel
framework for explicitly modeling legal reasoning in the analysis of Chinese
tort-related civil cases. We first operationalize the legal reasoning processes
used in tort analysis into the LawChain framework. LawChain is a three-module
reasoning framework, with each module consisting of multiple finer-grained
sub-steps. Informed by the LawChain framework, we introduce the task of tort
legal reasoning and construct an evaluation benchmark, LawChain$_{eval}$, to
systematically assess the critical steps within analytical reasoning chains for
tort analysis. Leveraging this benchmark, we evaluate state-of-the-art large
language models for their legal reasoning ability in civil tort contexts. Our
results indicate that current models still fall short in accurately handling
crucial elements of tort legal reasoning. Furthermore, we introduce several
baseline approaches that explicitly incorporate LawChain-style reasoning
through prompting or post-training. We conduct further experiments on
additional legal analysis tasks, such as Legal Named-Entity Recognition and
Criminal Damages Calculation, to verify the generalizability of these
baselines. The proposed baseline approaches achieve significant improvements in
tort-related legal reasoning and generalize well to related legal analysis
tasks, thus demonstrating the value of explicitly modeling legal reasoning
chains to enhance the reasoning capabilities of language models.

</details>


### [94] [Forget to Know, Remember to Use: Context-Aware Unlearning for Large Language Models](https://arxiv.org/abs/2510.17620)
*Yuefeng Peng,Parnian Afshar,Megan Ganji,Thomas Butler,Amir Houmansadr,Mingxian Wang,Dezhi Hong*

Main category: cs.CL

TL;DR: 本文针对大型语言模型中的敏感信息删除问题，提出了一种增强的遗忘方法，提高模型在删除信息后的上下文利用能力。


<details>
  <summary>Details</summary>
Motivation: 现有模型遗忘评估忽视了用户可能希望模型在提示中重新利用已删除信息的需求，导致上下文实用性受损。

Method: 通过在遗忘目标中加入插件项，增强模型在保留遗忘知识上下文利用能力的同时，保持有效遗忘和整体性能。

Result: 大量实验表明，所提方法在保持有效遗忘和模型表现的同时，几乎恢复了上下文实用性至原始水平。

Conclusion: 该方法有效提升了模型删除敏感或过时信息后，在实际使用中的灵活性与实用性，实现了遗忘与上下文利用的平衡。

Abstract: Large language models may encode sensitive information or outdated knowledge
that needs to be removed, to ensure responsible and compliant model responses.
Unlearning has emerged as an efficient alternative to full retraining, aiming
to remove specific knowledge while preserving overall model utility. Existing
evaluations of unlearning methods focus on (1) the extent of forgetting of the
target knowledge (forget set) and (2) maintaining performance on the retain set
(i.e., utility). However, these evaluations overlook an important usability
aspect: users may still want the model to leverage the removed information if
it is re-introduced in the prompt. In a systematic evaluation of six
state-of-the-art unlearning methods, we find that they consistently impair such
contextual utility. To address this, we augment unlearning objectives with a
plug-in term that preserves the model's ability to use forgotten knowledge when
it is present in context. Extensive experiments demonstrate that our approach
restores contextual utility to near original levels while still maintaining
effective forgetting and retain-set utility.

</details>


### [95] [Qomhra: A Bilingual Irish-English Large Language Model](https://arxiv.org/abs/2510.17652)
*Joseph McInerney*

Main category: cs.CL

TL;DR: 本文介绍了Qomhrá，一个在资源有限条件下开发的爱尔兰语-英语双语大型语言模型，通过双语持续预训练、指令调优和人类偏好对齐等步骤，显著提升了爱尔兰语的表现。


<details>
  <summary>Details</summary>
Motivation: 面对爱尔兰语资源有限的问题，作者希望构建一个能同时兼顾爱尔兰语和英语的双语大型语言模型，提升爱尔兰语生成能力，同时保持对英语的良好支持。

Method: 使用新获得的爱尔兰语语料和英语文本混合训练，评估多款封闭权重的大型语言模型后，选用Google的Gemini-2.5-Pro来合成指令调优和人类偏好数据集，完成了双语持续预训练、指令调优及人类偏好对齐的完整训练流程。

Result: Qomhrá在翻译、性别理解、话题识别及世界知识等多项基准测试中取得了爱尔兰语提升29%、英语提升44%的显著成绩，同时在指令遵循和聊天机器人功能上表现出明显进步。

Conclusion: 通过系统的训练流程和数据贡献，Qomhrá成功提高了爱尔兰语生成性能，兼顾英语能力，为爱尔兰语的低资源语言模型研究提供了有效方案和实际应用价值。

Abstract: This paper introduces Qomhr\'a, a bilingual Irish-English large language
model (LLM), developed under low-resource constraints presenting a complete
pipeline spanning bilingual continued pre-training, instruction tuning, and
alignment from human preferences. Newly accessible Irish corpora and English
text are mixed and curated to improve Irish performance while preserving
English ability. 6 closed-weight LLMs are judged for their Irish text
generation by a native speaker, a learner and other LLMs. Google's
Gemini-2.5-Pro is ranked the highest and is subsequently used to synthesise
instruction tuning and human preference datasets. Two datasets are contributed
leveraging Gemini-2.5-Pro: a 30K Irish-English parallel instruction tuning
dataset and a 1K human preference dataset, generating accepted and rejected
responses that show near perfect alignment with a native Irish speaker.
Qomhr\'a is comprehensively evaluated across benchmarks testing translation,
gender understanding, topic identification and world knowledge with gains of up
to 29% in Irish and 44% in English. Qomhr\'a also undergoes instruction tuning
and demonstrates clear progress in instruction following, crucial for chatbot
functionality.

</details>


### [96] [Towards Mining Effective Pedagogical Strategies from Learner-LLM Educational Dialogues](https://arxiv.org/abs/2510.17698)
*Liqun He,Manolis Mavrikis,Mutlu Cukurova*

Main category: cs.CL

TL;DR: 该论文提出了一种通过对学习者与大型语言模型对话的分析，识别有效教学策略的方法。


<details>
  <summary>Details</summary>
Motivation: 现有针对大型语言模型在教育应用中的评估主要关注技术性能或学习成果，忽视了学习者与模型的交互过程。

Method: 采用对话数据收集、对话行为注释、行为模式挖掘及预测模型构建的对话分析方法。

Result: 论文初步提出了研究框架并给出早期洞见，作为未来研究的基础。

Conclusion: 强调评估基于大型语言模型的教育应用时，应更多关注对话动态和教学策略，以提升教育效果。

Abstract: Dialogue plays a crucial role in educational settings, yet existing
evaluation methods for educational applications of large language models (LLMs)
primarily focus on technical performance or learning outcomes, often neglecting
attention to learner-LLM interactions. To narrow this gap, this AIED Doctoral
Consortium paper presents an ongoing study employing a dialogue analysis
approach to identify effective pedagogical strategies from learner-LLM
dialogues. The proposed approach involves dialogue data collection, dialogue
act (DA) annotation, DA pattern mining, and predictive model building. Early
insights are outlined as an initial step toward future research. The work
underscores the need to evaluate LLM-based educational applications by focusing
on dialogue dynamics and pedagogical strategies.

</details>


### [97] [QueST: Incentivizing LLMs to Generate Difficult Problems](https://arxiv.org/abs/2510.17715)
*Hanxu Hu,Xingxing Zhang,Jannis Vamvas,Rico Sennrich,Furu Wei*

Main category: cs.CL

TL;DR: 该论文提出了QueST框架，通过难度感知的图采样和拒绝微调方法生成大规模高难度编码问题，以提升大语言模型的推理和编码能力。


<details>
  <summary>Details</summary>
Motivation: 现有编程竞赛数据集规模有限，难以支持大规模训练；且传统合成数据生成方法依赖于已有的指令数据或低效筛选，影响模型性能提升。

Method: 引入QueST框架，采用难度感知图采样结合拒绝微调，直接优化生成器以生产高难度编码题目，并利用这些题目进行模型蒸馏和强化学习。

Result: 使用QueST生成的10万道难题对Qwen3-8B模型进行微调后，在LiveCodeBench表现超越原始模型；增加11.2万道合成题后，8B模型性能达到深度Seek-R1-671B的大模型水平。

Conclusion: QueST通过自动生成复杂编码题目，为提升大语言模型在竞争编码和推理任务中性能提供了有效且可扩展的解决方案。

Abstract: Large Language Models have achieved strong performance on reasoning tasks,
solving competition-level coding and math problems. However, their scalability
is limited by human-labeled datasets and the lack of large-scale, challenging
coding problem training data. Existing competitive coding datasets contain only
thousands to tens of thousands of problems. Previous synthetic data generation
methods rely on either augmenting existing instruction datasets or selecting
challenging problems from human-labeled data. In this paper, we propose QueST,
a novel framework which combines difficulty-aware graph sampling and
difficulty-aware rejection fine-tuning that directly optimizes specialized
generators to create challenging coding problems. Our trained generators
demonstrate superior capability compared to even GPT-4o at creating challenging
problems that benefit downstream performance. We leverage QueST to generate
large-scale synthetic coding problems, which we then use to distill from strong
teacher models with long chain-of-thought or to conduct reinforcement learning
for smaller models, proving effective in both scenarios. Our distillation
experiments demonstrate significant performance gains. Specifically, after
fine-tuning Qwen3-8B-base on 100K difficult problems generated by QueST, we
surpass the performance of the original Qwen3-8B on LiveCodeBench. With an
additional 112K examples (i.e., 28K human-written problems paired with multiple
synthetic solutions), our 8B model matches the performance of the much larger
DeepSeek-R1-671B. These findings indicate that generating complex problems via
QueST offers an effective and scalable approach to advancing the frontiers of
competitive coding and reasoning for large language models.

</details>


### [98] [PANER: A Paraphrase-Augmented Framework for Low-Resource Named Entity Recognition](https://arxiv.org/abs/2510.17720)
*Nanda Kumar Rengarajan,Jun Yan,Chun Wang*

Main category: cs.CL

TL;DR: 本文提出了一种轻量级少样本命名实体识别框架，通过简化的指令调优模板和保留实体信息的文本改写数据增强技术，在资源有限场景下显著提升了NER性能。


<details>
  <summary>Details</summary>
Motivation: 命名实体识别任务在低资源场景下由于标注数据缺乏而表现受限，现有零样本和指令调优方法难以泛化到领域特定实体且未有效利用有限数据。

Method: 设计了一种结合大上下文窗口的简化指令调优模板，并引入一种保持实体信息的语境改写数据增强方法，扩增训练数据同时保证语义关系。

Result: 在CrossNER基准数据集上，少样本方法平均F1达到80.1，使用改写增强的数据训练模型F1得分较基础模型提升最多17分。

Conclusion: 所提框架在少样本和零样本NER任务中实现了与最先进模型相当的性能，为有限训练数据和计算资源环境下的NER提供了有效解决方案。

Abstract: Named Entity Recognition (NER) is a critical task that requires substantial
annotated data, making it challenging in low-resource scenarios where label
acquisition is expensive. While zero-shot and instruction-tuned approaches have
made progress, they often fail to generalize to domain-specific entities and do
not effectively utilize limited available data. We present a lightweight
few-shot NER framework that addresses these challenges through two key
innovations: (1) a new instruction tuning template with a simplified output
format that combines principles from prior IT approaches to leverage the large
context window of recent state-of-the-art LLMs; (2) introducing a strategic
data augmentation technique that preserves entity information while
paraphrasing the surrounding context, thereby expanding our training data
without compromising semantic relationships. Experiments on benchmark datasets
show that our method achieves performance comparable to state-of-the-art models
on few-shot and zero-shot tasks, with our few-shot approach attaining an
average F1 score of 80.1 on the CrossNER datasets. Models trained with our
paraphrasing approach show consistent improvements in F1 scores of up to 17
points over baseline versions, offering a promising solution for groups with
limited NER training data and compute power.

</details>


### [99] [AcademicEval: Live Long-Context LLM Benchmark](https://arxiv.org/abs/2510.17725)
*Haozhen Zhang,Tao Feng,Pengrui Han,Jiaxuan You*

Main category: cs.CL

TL;DR: 本文提出了一个名为AcademicEval的实时长文本生成评测基准，利用arXiv论文数据构建多层级学术写作任务，并解决了标签泄露问题。


<details>
  <summary>Details</summary>
Motivation: 现有长文本理解基准存在上下文长度固定、标注耗时和标签泄露等问题，难以全面评测大语言模型的长文本生成能力。

Method: 通过采集arXiv论文及其作者合作网络，设计包含标题、摘要、引言和相关工作多个抽象层次的任务，结合高质量少样例示范，实现灵活上下文长度和无标签泄露的在线评测。

Result: 评测显示大语言模型在多层次抽象任务和长少样例示范中表现较差，突出本基准的挑战性，同时实验分析提出了提升长文本建模能力的见解。

Conclusion: AcademicEval为大语言模型长文本生成提供了有效且科学的评测工具，有助于推动其长上下文理解与生成能力的改进。

Abstract: Large Language Models (LLMs) have recently achieved remarkable performance in
long-context understanding. However, current long-context LLM benchmarks are
limited by rigid context length, labor-intensive annotation, and the pressing
challenge of label leakage issues during LLM training. Therefore, we propose
\textsc{AcademicEval}, a live benchmark for evaluating LLMs over long-context
generation tasks. \textsc{AcademicEval} adopts papers on arXiv to introduce
several academic writing tasks with long-context inputs, \textit{i.e.},
\textsc{Title}, \textsc{Abstract}, \textsc{Introduction}, and \textsc{Related
Work}, which cover a wide range of abstraction levels and require no manual
labeling. Moreover, \textsc{AcademicEval} integrates high-quality and
expert-curated few-shot demonstrations from a collected co-author graph to
enable flexible context length. Especially, \textsc{AcademicEval} features an
efficient live evaluation, ensuring no label leakage. We conduct a holistic
evaluation on \textsc{AcademicEval}, and the results illustrate that LLMs
perform poorly on tasks with hierarchical abstraction levels and tend to
struggle with long few-shot demonstrations, highlighting the challenge of our
benchmark. Through experimental analysis, we also reveal some insights for
enhancing LLMs' long-context modeling capabilities. Code is available at
https://github.com/ulab-uiuc/AcademicEval

</details>


### [100] [Train for Truth, Keep the Skills: Binary Retrieval-Augmented Reward Mitigates Hallucinations](https://arxiv.org/abs/2510.17733)
*Tong Chen,Akari Asai,Luke Zettlemoyer,Hannaneh Hajishirzi,Faeze Brahman*

Main category: cs.CL

TL;DR: 本文提出了一种基于二元检索增强奖励的在线强化学习方法，有效降低语言模型的事实错误率，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有减少语言模型错误生成方法会损害开放式生成和下游任务表现，影响实用性。

Method: 提出一种二元检索增强奖励（RAR）在线强化学习方法，只有在模型输出完全准确时给予奖励1，否则0。

Result: 该方法使Qwen3模型在开放式生成任务中错误率降低39.3%，在短问答中减少了44.4%和21.7%的错误回答，并且没有降低其他任务表现。

Conclusion: 利用二元奖励机制能显著提升语言模型事实准确性，而不损害生成任务的整体性能，优于连续奖励方法。

Abstract: Language models often generate factually incorrect information unsupported by
their training data, a phenomenon known as extrinsic hallucination. Existing
mitigation approaches often degrade performance on open-ended generation and
downstream tasks, limiting their practical utility. We propose an online
reinforcement learning method using a novel binary retrieval-augmented reward
(RAR) to address this tradeoff. Unlike continuous reward schemes, our approach
assigns a reward of one only when the model's output is entirely factually
correct, and zero otherwise. We evaluate our method on Qwen3 reasoning models
across diverse tasks. For open-ended generation, binary RAR achieves a 39.3%
reduction in hallucination rates, substantially outperforming both supervised
training and continuous-reward RL baselines. In short-form question answering,
the model learns calibrated abstention, strategically outputting "I don't know"
when faced with insufficient parametric knowledge. This yields 44.4% and 21.7%
fewer incorrect answers on PopQA and GPQA, respectively. Crucially, these
factuality gains come without performance degradation on instruction following,
math, or code, whereas continuous-reward RL, despite improving factuality,
induces quality regressions.

</details>


### [101] [Evaluating Medical LLMs by Levels of Autonomy: A Survey Moving from Benchmarks to Applications](https://arxiv.org/abs/2510.17764)
*Xiao Ye,Jacob Dineen,Zhaonan Li,Zhikun Xu,Weiyu Chen,Shijie Lu,Yuxi Huang,Ming Shen,Phu Tran,Ji-Eun Irene Yum,Muhammad Ali Khan,Muhammad Umar Afzal,Irbaz Bin Riaz,Ben Zhou*

Main category: cs.CL

TL;DR: 本文对医疗大型语言模型的评估进行了重新框架，提出了基于自主水平（L0-L3）的评价方法，以促进其在临床应用中的安全可靠表现。


<details>
  <summary>Details</summary>
Motivation: 尽管医疗大型语言模型在标准基准测试中表现出色，但如何将这些成果安全可靠地应用于临床工作流程仍是一个难题。

Method: 通过引入自主水平（L0-L3）的视角，将现有基准测试和评价指标与每个自主级别允许的操作及其风险对齐，提出了基于级别的评价指标选择、证据收集和报告策略，并探讨将评价与监管相结合的方向。

Result: 该方法明确了不同自主级别下的评价目标，促进了从单纯分数比较向基于风险的真实临床证据转变。

Conclusion: 通过以自主性为中心，本文推动医疗语言模型的评估从得分驱动转向可信、风险感知的临床应用证据，促进其安全有效地应用于实际临床环境。

Abstract: Medical Large language models achieve strong scores on standard benchmarks;
however, the transfer of those results to safe and reliable performance in
clinical workflows remains a challenge. This survey reframes evaluation through
a levels-of-autonomy lens (L0-L3), spanning informational tools, information
transformation and aggregation, decision support, and supervised agents. We
align existing benchmarks and metrics with the actions permitted at each level
and their associated risks, making the evaluation targets explicit. This
motivates a level-conditioned blueprint for selecting metrics, assembling
evidence, and reporting claims, alongside directions that link evaluation to
oversight. By centering autonomy, the survey moves the field beyond score-based
claims toward credible, risk-aware evidence for real clinical use.

</details>


### [102] [Foundational Automatic Evaluators: Scaling Multi-Task Generative Evaluator Training for Reasoning-Centric Domains](https://arxiv.org/abs/2510.17793)
*Austin Xu,Xuan-Phi Nguyen,Yilun Zhou,Chien-Sheng Wu,Caiming Xiong,Shafiq Joty*

Main category: cs.CL

TL;DR: 本文通过构建包含250万样本的大规模数据集，训练了8B和20B参数规模的基础自动推理评估器（FARE），采用简单的迭代拒绝采样监督微调方法，显著优于现有专用强化学习评估器，且在多项实际任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前评估器训练多侧重于强化学习等新方法，缺乏在大规模数据驱动上的探索，而评估任务对规模化、高效准确评估需求日益增长。

Method: 通过收集涵盖五种评估任务的多领域数据集（共2.5M样本），利用迭代拒绝采样监督微调（SFT）方法，训练8B和20B参数规模的FARE模型。

Result: FARE-8B在性能上挑战更大规模的强化学习训练评估器，FARE-20B超越开放源代码的70B+参数专用模型。在真实任务中作为推理重排器和强化学习训练验证器均取得显著提升，如MATH近似最优性能以及提升下游RL训练模型14.1%。

Conclusion: 大规模数据驱动的微调策略能显著提升生成型评估器性能，FARE模型成为新的开源评估器标准，具备良好的实际应用价值和迁移能力。

Abstract: Finetuning specialized generative evaluators has emerged as a popular
paradigm to meet the increasing demand for scalable evaluation during both
training and test-time. However, recent work has largely focused on applying
new methodology, such as reinforcement learning (RL), to training evaluators,
shying away from large-scale, data-driven development. In this work, we focus
on data scaling, curating a set of 2.5M samples spanning five unique evaluation
tasks (pairwise, step-level, reference-free and reference-based verification,
and single rating) and multiple domains focused on reasoning evaluation. With
our data, we train Foundational Automatic Reasoning Evaluators (FARE), a family
of 8B and 20B (with 3.6B active) parameter evaluators, with a simple iterative
rejection-sampling supervised finetuning (SFT) approach. FARE-8B challenges
larger specialized RL-trained evaluators and FARE-20B sets the new standard for
open-source evaluators, surpassing specialized 70B+ evaluators. Beyond static
benchmarks, we evaluate FARE in real-world tasks: As inference-time rerankers,
FARE-20B achieves near-oracle performance on MATH. As verifiers in RL training,
FARE improves the downstream RL-trained model performance by up to 14.1% vs.
string-matching verifiers. When initialized from FARE, a continually-finetuned
FARE-Code outperforms gpt-oss-20B by 65% on evaluating test-case quality.

</details>


### [103] [Enterprise Deep Research: Steerable Multi-Agent Deep Research for Enterprise Analytics](https://arxiv.org/abs/2510.17797)
*Akshara Prabhakar,Roshan Ram,Zixiang Chen,Silvio Savarese,Frank Wang,Caiming Xiong,Huan Wang,Weiran Yao*

Main category: cs.CL

TL;DR: 本文提出了Enterprise Deep Research（EDR）系统，利用多智能体协同工作，将海量非结构化数据转换成有价值的信息，支持自动报告生成和企业级部署。


<details>
  <summary>Details</summary>
Motivation: 随着信息激增，企业亟需将非结构化数据转化为可操作的信息，但现有自主智能体难以处理领域细节、意图匹配及企业集成问题。

Method: 设计了多智能体系统EDR，包括主规划智能体、4个专业搜索智能体（通用、学术、GitHub、LinkedIn）、基于MCP的工具生态（支持自然语言转SQL、文件分析、企业工作流）、可视化智能体及反思机制，支持自动生成报告和实时流处理。

Result: EDR在内部数据集和公开基准DeepResearch Bench、DeepConsult上表现优异，超过了现有最先进的智能体系统，无需人工指导。

Conclusion: EDR系统有效解决了多智能体在企业级数据研究中的挑战，成果开源，将推动多智能体推理应用领域的发展。

Abstract: As information grows exponentially, enterprises face increasing pressure to
transform unstructured data into coherent, actionable insights. While
autonomous agents show promise, they often struggle with domain-specific
nuances, intent alignment, and enterprise integration. We present Enterprise
Deep Research (EDR), a multi-agent system that integrates (1) a Master Planning
Agent for adaptive query decomposition, (2) four specialized search agents
(General, Academic, GitHub, LinkedIn), (3) an extensible MCP-based tool
ecosystem supporting NL2SQL, file analysis, and enterprise workflows, (4) a
Visualization Agent for data-driven insights, and (5) a reflection mechanism
that detects knowledge gaps and updates research direction with optional
human-in-the-loop steering guidance. These components enable automated report
generation, real-time streaming, and seamless enterprise deployment, as
validated on internal datasets. On open-ended benchmarks including DeepResearch
Bench and DeepConsult, EDR outperforms state-of-the-art agentic systems without
any human steering. We release the EDR framework and benchmark trajectories to
advance research on multi-agent reasoning applications.
  Code at https://github.com/SalesforceAIResearch/enterprise-deep-research and
Dataset at https://huggingface.co/datasets/Salesforce/EDR-200

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [104] [Disaster Management in the Era of Agentic AI Systems: A Vision for Collective Human-Machine Intelligence for Augmented Resilience](https://arxiv.org/abs/2510.16034)
*Bo Li,Junwei Ma,Kai Yin,Yiming Xiao,Chia-Wei Hsu,Ali Mostafavi*

Main category: cs.MA

TL;DR: 该论文提出了Disaster Copilot多智能体AI系统，旨在整合多种专用AI工具，通过中心协调器实现灾难管理的实时、全面决策支持，提高应对效率和社区韧性。


<details>
  <summary>Details</summary>
Motivation: 传统灾难响应能力难以应对日益频繁且严重的灾害，存在数据分散、技术孤岛、资源匮乏及机构记忆流失等问题，影响及时有效的决策。

Method: 设计基于多智能体架构的Disaster Copilot系统，由中央协调器管理多个专注于风险预测、态势感知和影响评估的子智能体，并通过多模态数据融合构建灾难数字孪生，实现资源受限环境下的设备端协调及机构知识的保留。

Result: 系统架构细节得到阐释，并提出涵盖技术、组织能力和人机协同的三阶段发展路线。该系统能为灾难管理提供实时、全面的操作视图，提升智能化和适应性。

Conclusion: Disaster Copilot构建了一个促进人机集体智能的多智能体框架，有助于打造更加适应性强、数据驱动和韧性的社区，从而变革灾难管理实践。

Abstract: The escalating frequency and severity of disasters routinely overwhelm
traditional response capabilities, exposing critical vulnerability in disaster
management. Current practices are hindered by fragmented data streams, siloed
technologies, resource constraints, and the erosion of institutional memory,
which collectively impede timely and effective decision making. This study
introduces Disaster Copilot, a vision for a multi-agent artificial intelligence
system designed to overcome these systemic challenges by unifying specialized
AI tools within a collaborative framework. The proposed architecture utilizes a
central orchestrator to coordinate diverse sub-agents, each specializing in
critical domains such as predictive risk analytics, situational awareness, and
impact assessment. By integrating multi-modal data, the system delivers a
holistic, real-time operational picture and serve as the essential AI backbone
required to advance Disaster Digital Twins from passive models to active,
intelligent environments. Furthermore, it ensures functionality in
resource-limited environments through on-device orchestration and incorporates
mechanisms to capture institutional knowledge, mitigating the impact of staff
turnover. We detail the system architecture and propose a three-phased roadmap
emphasizing the parallel growth of technology, organizational capacity, and
human-AI teaming. Disaster Copilot offers a transformative vision, fostering
collective human-machine intelligence to build more adaptive, data-driven and
resilient communities.

</details>


### [105] [Zero-Shot Coordination in Ad Hoc Teams with Generalized Policy Improvement and Difference Rewards](https://arxiv.org/abs/2510.16187)
*Rupal Nigam,Niket Parikh,Hamid Osooli,Mikihisa Yuasa,Jacob Heglund,Huy T. Tran*

Main category: cs.MA

TL;DR: 提出了GPAT算法，通过综合利用多个预训练策略，实现多智能体系统中的零-shot协作，提升了新队伍间的协同效率。


<details>
  <summary>Details</summary>
Motivation: 现实多智能体系统中，智能体需与未知队友实时协作，现有方案有限，难以实现高效的零-shot团队协作。

Method: 将问题形式化为多智能体马尔可夫决策过程，利用广义策略改进和差异奖励方法，促进不同预训练策略间的知识迁移。

Result: GPAT算法在合作觅食、捕食者猎物和Overcooked三个模拟环境及真实多机器人应用中，实现了有效的零-shot新队伍协作。

Conclusion: GPAT证明了综合利用多个预训练策略并基于差异奖励进行知识迁移，是实现零-shot多智能体团队协作的有效方法。

Abstract: Real-world multi-agent systems may require ad hoc teaming, where an agent
must coordinate with other previously unseen teammates to solve a task in a
zero-shot manner. Prior work often either selects a pretrained policy based on
an inferred model of the new teammates or pretrains a single policy that is
robust to potential teammates. Instead, we propose to leverage all pretrained
policies in a zero-shot transfer setting. We formalize this problem as an ad
hoc multi-agent Markov decision process and present a solution that uses two
key ideas, generalized policy improvement and difference rewards, for efficient
and effective knowledge transfer between different teams. We empirically
demonstrate that our algorithm, Generalized Policy improvement for Ad hoc
Teaming (GPAT), successfully enables zero-shot transfer to new teams in three
simulated environments: cooperative foraging, predator-prey, and Overcooked. We
also demonstrate our algorithm in a real-world multi-robot setting.

</details>


### [106] [Heterogeneous Multi-Agent Task-Assignment with Uncertain Execution Times and Preferences](https://arxiv.org/abs/2510.16221)
*Qinshuang Wei,Vaibhav Srivastava,Vijay Gupta*

Main category: cs.MA

TL;DR: 本文研究了在多智能体环境下，具有异构任务偏好和能力的多智能体任务分配问题，提出了一种基于bandit算法的任务分配方法。


<details>
  <summary>Details</summary>
Motivation: 以往单智能体的序列任务分配研究较多，而多智能体中智能体间异构性使任务分配问题更加复杂，且未被充分解决。

Method: 假设任务的奖励、执行时间和资源消耗均服从未知随机分布，提出并分析了一种结合bandit算法的任务分配策略，同时考虑了精确与近似求解任务分配问题两种情况的遗憾界。

Result: 理论上分析了算法在精确和近似任务分配求解下的性能表现，具体实现细节及实证结果未在摘要中给出。

Conclusion: 该研究扩展了多智能体任务分配领域，通过引入带有不确定性的bandit算法，提高了多智能体任务分配的效率和合理性。

Abstract: While sequential task assignment for a single agent has been widely studied,
such problems in a multi-agent setting, where the agents have heterogeneous
task preferences or capabilities, remain less well-characterized. We study a
multi-agent task assignment problem where a central planner assigns recurring
tasks to multiple members of a team over a finite time horizon. For any given
task, the members have heterogeneous capabilities in terms of task completion
times, task resource consumption (which can model variables such as energy or
attention), and preferences in terms of the rewards they collect upon task
completion. We assume that the reward, execution time, and resource consumption
for each member to complete any task are stochastic with unknown distributions.
The goal of the planner is to maximize the total expected reward that the team
receives over the problem horizon while ensuring that the resource consumption
required for any assigned task is within the capability of the agent. We
propose and analyze a bandit algorithm for this problem. Since the bandit
algorithm relies on solving an optimal task assignment problem repeatedly, we
analyze the achievable regret in two cases: when we can solve the optimal task
assignment exactly and when we can solve it only approximately.

</details>


### [107] [Prompt Optimization via Retrieved Reasoning Assets and Multi-Agent Analysis](https://arxiv.org/abs/2510.16635)
*Wonduk Seo,Juhyeon Lee,Junseo Koh,Hyunjin An,Jian Park,Seunghyun Lee,Haihua Chen,Yi Bu*

Main category: cs.MA

TL;DR: 提出了MA-SAPO，一种多智能体框架，通过将评估结果与结构化推理结合，实现对大型语言模型提示的透明且可控的优化。


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法仅依赖数值评分作为黑箱评价，缺乏对成功或失败原因的解释，且依赖难以控制的反复试验。

Method: MA-SAPO包括两个阶段：推理阶段多智能体协作解释评分、诊断弱点并生成可复用的推理资产；测试阶段智能体利用这些资产对提示进行基于证据的编辑。

Result: 在HelpSteer1/2基准测试中，MA-SAPO相比单次提示、增强检索和已有多智能体方法表现出稳定的性能提升。

Conclusion: 通过将评估信号转化为可解释的推理链，MA-SAPO实现了提示优化过程的透明、可审计和可控，提升了大型语言模型提示优化的效果和可靠性。

Abstract: Prompt optimization has emerged as an effective alternative to retraining for
improving the performance of Large Language Models (LLMs). However, most
existing approaches treat evaluation as a black box, relying solely on
numerical scores while offering limited insight into why a prompt succeeds or
fails. They also depend heavily on trial-and-error refinements, which are
difficult to interpret and control. In this paper, we introduce MA-SAPO, a
Multi-Agent framework for Score-Aware Prompt Optimization. Compared to prior
methods, MA-SAPO explicitly couples evaluation outcomes with structured
reasoning to guide systematic edits. The framework specifically consists of two
stages: during the Reasoning Phase, agents collaboratively explain metric
scores, diagnose weaknesses, and synthesize targeted refinements that are
stored as reusable reasoning assets; during the Test Phase, agents retrieve
these assets to analyze optimized prompts and apply only evidence-grounded
edits. By turning evaluation signals into interpretable reasoning chains,
MA-SAPO produces prompt refinements that are more transparent, auditable, and
controllable. Experiments on the HelpSteer1/2 benchmarks demonstrate consistent
improvements over single-pass prompting, retrieval-augmented baselines, and
prior multi-agent strategies, validating the effectiveness of our approach.

</details>


### [108] [DiRAC - Distributed Robot Awareness and Consensus](https://arxiv.org/abs/2510.16850)
*Uday Gopan,Manjari Kulkarni,Lakshasri S,Kashish Mittal,Sriram Radhakrishna,Aditya Naskar,Rameshwar DL*

Main category: cs.MA

TL;DR: DiRAC框架通过区划分和分布式领导机制，实现了大规模机器人群的高效任务分配和路径规划。


<details>
  <summary>Details</summary>
Motivation: 解决大规模机器人群任务分配和路径规划中存在的可扩展性和实时性问题。

Method: 提出基于区域划分的架构，采用动态领导选举和同步一致性协议，以及基于力的分散式路径规划算法。

Result: 在ROS 2中经过模拟验证，展示了系统架构的可扩展性及模块化效率。

Conclusion: DiRAC为在工业和物流领域大规模机器人群的实际部署提供了理论和技术基础。

Abstract: DiRAC is a scalable, distributed framework designed to enable efficient task
assignment and path planning in very large robotic swarms. It introduces a
novel zone-partitioned architecture with dynamically elected leaders and a
tick-synchronized consensus protocol that yields strong consistency and
deterministic outcomes. For path planning, DiRAC uses a novel algorithm, a
force-based decentralized planner for real-time collision resolution. Validated
within ROS 2 middleware through preliminary simulation, DiRAC demonstrates
architectural scalability and modular efficiency in simulated warehouse
environments, laying the groundwork for real-world deployment in large-scale
industrial and logistics domains.

</details>


### [109] [Lark: Biologically Inspired Neuroevolution for Multi-Stakeholder LLM Agents](https://arxiv.org/abs/2510.16978)
*Dheeraj Chintapalli,Rikhil Tanugula,Sunkalp Chandra*

Main category: cs.MA

TL;DR: 本文提出了Lark，一种结合LLM推理与多智能体系统的生物启发式决策框架，通过四种机制应对冗长与利益相关者权衡，在实验证明其性能优异且成本竞争力强，并通过消融实验验证各机制的重要性。


<details>
  <summary>Details</summary>
Motivation: 针对决策过程中信息冗长及利益相关者间的权衡难题，设计一个能够高效生成多样化策略并透明展示权衡过程的决策系统。

Method: Lark框架融合了塑性调整、复制与成熟机制、基于影响力加权的Borda得分排名选择，以及基于令牌消耗的计算惩罚机制，通过迭代策略生成、调整、模拟利益相关者评估和复制优化，结合计算成本综合评分选出最佳策略。

Result: 在30轮对比14个系统的实验中，Lark全系统平均排名2.55，80%轮次进入前三，综合得分29.4/50，且成本仅0.016美元/任务。消融实验显示四种机制皆显著提升性能，复制成熟机制贡献最大。

Conclusion: Lark是一种实用且计算感知的神经进化循环方法，有效扩展并标定了符合利益相关者需求的策略生成，未来将进一步通过真实环境验证提升其适用性。

Abstract: We present Lark, a biologically inspired decision-making framework that
couples LLM-driven reasoning with an evolutionary, stakeholder-aware
Multi-Agent System (MAS). To address verbosity and stakeholder trade-offs, we
integrate four mechanisms: (i) plasticity, which applies concise adjustments to
candidate solutions; (ii) duplication and maturation, which copy
high-performing candidates and specialize them into new modules; (iii)
ranked-choice stakeholder aggregation using influence-weighted Borda scoring;
and (iv) compute awareness via token-based penalties that reward brevity. The
system iteratively proposes diverse strategies, applies plasticity tweaks,
simulates stakeholder evaluations, aggregates preferences, selects top
candidates, and performs duplication/maturation while factoring compute cost
into final scores. In a controlled evaluation over 30 rounds comparing 14
systems, Lark Full achieves a mean rank of 2.55 (95% CI [2.17, 2.93]) and a
mean composite score of 29.4/50 (95% CI [26.34, 32.46]), finishing Top-3 in 80%
of rounds while remaining cost competitive with leading commercial models
($0.016 per task). Paired Wilcoxon tests confirm that all four mechanisms
contribute significantly as ablating duplication/maturation yields the largest
deficit ({\Delta}Score = 3.5, Cohen's d_z = 2.53, p < 0.001), followed by
plasticity ({\Delta}Score = 3.4, d_z = 1.86), ranked-choice voting
({\Delta}Score = 2.4, d_z = 1.20), and token penalties ({\Delta}Score = 2.2,
d_z = 1.63). Rather than a formal Markov Decision Process with constrained
optimization, Lark is a practical, compute-aware neuroevolutionary loop that
scales stakeholder-aligned strategy generation and makes trade-offs transparent
through per-step metrics. Our work presents proof-of-concept findings and
invites community feedback as we expand toward real-world validation studies.

</details>


### [110] [ReclAIm: A multi-agent framework for degradation-aware performance tuning of medical imaging AI](https://arxiv.org/abs/2510.17004)
*Eleftherios Tzanis,Michail E. Klontzas*

Main category: cs.MA

TL;DR: ReclAIm框架通过自然语言互动，实现了医疗图像分类模型的自动监控、评估及微调，显著提升模型长期可靠性。


<details>
  <summary>Details</summary>
Motivation: 临床应用中AI模型需长期稳定可靠，传统方法缺乏自动监控和快速调整机制。

Method: 基于大语言模型的多智能体架构，自动监测模型表现并通过自然语言交互执行微调。

Result: 在MRI、CT和X光数据集上，ReclAIm成功检测并修正模型性能下降，最大可恢复至初始性能的98.5%。

Conclusion: ReclAIm实现了医疗影像AI模型的自动、持续维护，简化操作提升适用性，促进临床与研究的广泛应用。

Abstract: Ensuring the long-term reliability of AI models in clinical practice requires
continuous performance monitoring and corrective actions when degradation
occurs. Addressing this need, this manuscript presents ReclAIm, a multi-agent
framework capable of autonomously monitoring, evaluating, and fine-tuning
medical image classification models. The system, built on a large language
model core, operates entirely through natural language interaction, eliminating
the need for programming expertise. ReclAIm successfully trains, evaluates, and
maintains consistent performance of models across MRI, CT, and X-ray datasets.
Once ReclAIm detects significant performance degradation, it autonomously
executes state-of-the-art fine-tuning procedures that substantially reduce the
performance gap. In cases with performance drops of up to -41.1% (MRI
InceptionV3), ReclAIm managed to readjust performance metrics within 1.5% of
the initial model results. ReclAIm enables automated, continuous maintenance of
medical imaging AI models in a user-friendly and adaptable manner that
facilitates broader adoption in both research and clinical environments.

</details>


### [111] [MiCRO for Multilateral Negotiations](https://arxiv.org/abs/2510.17401)
*David Aguilera-Luzon,Dave de Jonge,Javier Larrosa*

Main category: cs.MA

TL;DR: 本文提出了一个简单但高效的多边谈判策略MiCRO的扩展版本，性能优于2015、2017和2018年ANAC竞赛优胜者，并通过实证博弈论分析证明该策略构成经验纳什均衡。


<details>
  <summary>Details</summary>
Motivation: 原有的MiCRO策略在双边谈判中表现优异，但如何将其推广到多边谈判尚未解决，且传统测评领域可能过于简单。

Method: 提出MiCRO的多边变体，并与ANAC多个年份的冠军策略进行对比，同时采用实证博弈论方法分析其均衡性质。

Result: 多边MiCRO在多个竞赛中表现优于冠军策略，并通过博弈论分析验证其为经验纳什均衡。

Conclusion: 本文成功推广了MiCRO策略到多边谈判，展现出强大性能和理论保障，质疑了现有谈判领域的复杂度。

Abstract: Recently, a very simple new bilateral negotiation strategy called MiCRO was
introduced that does not make use of any kind of opponent modeling or machine
learning techniques and that does not require fine-tuning of any parameters.
Despite its simplicity, it was shown that MiCRO performs similar to -- or even
better than -- most state-of-the-art negotiation strategies. This lead its
authors to argue that the benchmark domains on which negotiation algorithms are
typically tested may be too simplistic. However, one question that was left
open, was how MiCRO could be generalized to multilateral negotiations. In this
paper we fill this gap by introducing a multilateral variant of MiCRO. We
compare it with the winners of the Automated Negotiating Agents Competitions
(ANAC) of 2015, 2017 and 2018 and show that it outperforms them. Furthermore,
we perform an empirical game-theoretical analysis to show that our new version
of MiCRO forms an empirical Nash equilibrium.

</details>


### [112] [Strategyproof Facility Location for Five Agents on a Circle using PCD](https://arxiv.org/abs/2510.17435)
*Ido Farjoun,Reshef Meir*

Main category: cs.MA

TL;DR: 本文研究了在圆上5个代理的策略真诚设施选址问题，找到PCD策略机制的紧界，并提出了奇数代理数时的近似比假设。


<details>
  <summary>Details</summary>
Motivation: 解决策略真诚设施选址问题，尤其是在圆形空间中如何设计有效且策略真诚的机制。

Method: 通过缩减实例空间规模，利用标准优化技术分析PCD机制并证明其界限的紧性。

Result: 找到了5个代理时PCD策略机制的紧界，并提出了对一般奇数代理数的近似比假设。

Conclusion: PCD机制在圆上5个代理时表现出紧的策略真诚性界限，且该机制对更一般情况下的奇数代理数有潜在的良好表现。

Abstract: We consider the strategyproof facility location problem on a circle. We focus
on the case of 5 agents, and find a tight bound for the PCD strategyproof
mechanism, which selects the reported location of an agent in proportion to the
length of the arc in front of it. We methodically "reduce" the size of the
instance space and then use standard optimization techniques to find and prove
the bound is tight. Moreover we hypothesize the approximation ratio of PCD for
general odd $n$.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [113] [SIADAFIX: issue description response for adaptive program repair](https://arxiv.org/abs/2510.16059)
*Xin Cao,Nan Yu*

Main category: cs.SE

TL;DR: 提出了一种基于快思考和慢思考相结合的自适应程序修复方法SIADAFIX，通过不同复杂度下的模式选择，实现程序修复效率和准确性的平衡。


<details>
  <summary>Details</summary>
Motivation: 传统的大型语言模型在复杂程序修复任务中效率和准确性难以兼顾，需要一种机制来优化修复流程和方法选择。

Method: 设计了SIADAFIX方法，利用慢思考的错误修复代理处理复杂任务，快思考的流程决策组件优化和分类问题描述，并根据描述响应结果指导修复流程，适配三种修复模式（易、中、难），结合快速泛化和测试时扩展技术。

Result: 在SWE-bench Lite基准测试中，使用Claude-4 Sonnet模型，SIADAFIX达到了60.67%的pass@1性能，达到开源方法的领先水平。

Conclusion: SIADAFIX有效平衡了程序修复的效率与准确性，为自动程序修复提供了新的思路和方法。

Abstract: We propose utilizing fast and slow thinking to enhance the capabilities of
large language model-based agents on complex tasks such as program repair. In
particular, we design an adaptive program repair method based on issue
description response, called SIADAFIX. The proposed method utilizes slow
thinking bug fix agent to complete complex program repair tasks, and employs
fast thinking workflow decision components to optimize and classify issue
descriptions, using issue description response results to guide the
orchestration of bug fix agent workflows. SIADAFIX adaptively selects three
repair modes, i.e., easy, middle and hard mode, based on problem complexity. It
employs fast generalization for simple problems and test-time scaling
techniques for complex problems. Experimental results on the SWE-bench Lite
show that the proposed method achieves 60.67% pass@1 performance using the
Claude-4 Sonnet model, reaching state-of-the-art levels among all open-source
methods. SIADAFIX effectively balances repair efficiency and accuracy,
providing new insights for automated program repair. Our code is available at
https://github.com/liauto-siada/siada-cli.

</details>


### [114] [Code Contribution and Credit in Science](https://arxiv.org/abs/2510.16242)
*Eva Maxfield Brown,Isaac Slaughter,Nicholas Weber*

Main category: cs.SE

TL;DR: 这篇论文分析了科研软件开发与传统学术信用指标之间的关系，发现软件贡献者在学术发文中往往得不到应有的认可。


<details>
  <summary>Details</summary>
Motivation: 传统学术信用评估忽视了软件开发贡献，难以准确反映科研人员的真实贡献。

Method: 通过构建14万对科研文章与代码仓库数据集，并建立预测模型匹配文章作者与代码贡献者，分析软件开发对学术信用的影响。

Result: 约30%的文章中存在未被认可的代码贡献者，代码贡献带来的引用增加微弱且受限于领域和开放获取状态；主要作者更可能参与代码贡献，但高频代码贡献者的h指数较低。

Conclusion: 科研软件贡献与传统学术信用存在脱节，提示需调整机构奖励和科学政策，更合理地评价科研贡献。

Abstract: Software development has become essential to scientific research, but its
relationship to traditional metrics of scholarly credit remains poorly
understood. We develop a dataset of approximately 140,000 paired research
articles and code repositories, as well as a predictive model that matches
research article authors with software repository developer accounts. We use
this data to investigate how software development activities influence credit
allocation in collaborative scientific settings. Our findings reveal
significant patterns distinguishing software contributions from traditional
authorship credit. We find that nearly 30% of articles include non-author code
contributors- individuals who participated in software development but received
no formal authorship recognition. While code-contributing authors show a modest
$\sim$4.2% increase in article citations, this effect becomes non-significant
when controlling for domain, article type, and open access status. First
authors are significantly more likely to be code contributors than other author
positions. Notably, we identify a negative relationship between coding
frequency and scholarly impact metrics. Authors who contribute code more
frequently exhibit progressively lower h-indices than non-coding colleagues,
even when controlling for publication count, author position, domain, and
article type. These results suggest a disconnect between software contributions
and credit, highlighting important implications for institutional reward
structures and science policy.

</details>


### [115] [MLCPD: A Unified Multi-Language Code Parsing Dataset with Universal AST Schema](https://arxiv.org/abs/2510.16357)
*Jugal Gajjar,Kamalasankari Subramaniakuppusamy*

Main category: cs.SE

TL;DR: MLCPD是一个统一十种主要编程语言的抽象语法树（AST）大规模数据集，支持跨语言结构学习和分析。


<details>
  <summary>Details</summary>
Motivation: 现有数据集主要关注代码的词法层面或孤立语法解析器，缺乏统一、多语言的结构表示，难以实现跨语言程序分析与学习。

Method: 提出通用抽象语法树（AST）规范，构建包含超过700万源代码文件的统一数据集，提供层次化树结构、丰富元数据和标准化节点语义，数据以Parquet格式存储以便高效检索。

Result: 实验验证不同语言的句法图表现出强烈的结构规律性，支持在共享规范下对Python、Java、Go等语言进行统一建模。数据集及代码库已公开发布，支持数据集重现、语法编译及AST可视化。

Conclusion: MLCPD为跨语言表示学习和程序分析研究提供了开放、可复现的基础资源，推动多语言软件分析领域的发展。

Abstract: We introduce the MultiLang Code Parser Dataset (MLCPD), a large-scale,
language-agnostic dataset unifying syntactic and structural representations of
code across ten major programming languages. MLCPD contains over seven million
parsed source files normalized under our proposed universal Abstract Syntax
Tree (AST) schema, enabling consistent cross-language reasoning, structural
learning, and multilingual software analysis. Unlike existing corpora that
focus purely on token-level code or isolated parsers, MLCPD provides both
hierarchical tree representations and rich metadata for every file, ensuring
lossless syntactic coverage and structural uniformity. Each entry includes a
normalized schema, language-level metadata, and abstracted node semantics
stored in Parquet format for scalable retrieval. Empirical analyses reveal
strong cross-language structural regularities-demonstrating that syntactic
graphs from languages as diverse as Python, Java, and Go can be aligned under a
shared schema. We release the dataset publicly on Hugging Face and the
accompanying codebase on GitHub, which includes complete pipelines for dataset
reproduction, grammar compilation, and a visualization tool for exploring the
unified AST across languages. Together, these resources establish MLCPD as an
open, reproducible foundation for future research in cross-language
representation learning and program analysis.

</details>


### [116] [SemOpt: LLM-Driven Code Optimization via Rule-Based Analysis](https://arxiv.org/abs/2510.16384)
*Yuwei Zhao,Yuan-An Xiao,Qianyu Xiao,Zhao Zhang,Yingfei Xiong*

Main category: cs.SE

TL;DR: SemOpt是一种基于静态程序分析和大型语言模型的自动代码优化框架，能够精确识别可优化代码段并生成高效优化结果，相较传统检索方法显著提升优化成功率和性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于信息检索的大规模代码优化方法难以解决语义等价但语法差异大的优化示例检索问题，导致优化效果不佳。

Method: SemOpt通过构建优化策略库、生成静态分析规则（Semgrep规则）捕捉应用条件，并利用LLM指导优化生成，结合静态程序分析精确定位优化代码段。

Result: 在包含151个优化任务的基准测试中，SemOpt在不同LLM支持下优化成功率提升1.38到28倍；在主流大型C/C++项目中性能指标提升5.04%到218.07%。

Conclusion: SemOpt有效克服现有方法的检索限制，提升自动代码优化的成功率和代码性能，具备较强的实用价值。

Abstract: Automated code optimization aims to improve performance in programs by
refactoring code, and recent studies focus on utilizing LLMs for the
optimization. Typical existing approaches mine optimization commits from
open-source codebases to construct a large-scale knowledge base, then employ
information retrieval techniques such as BM25 to retrieve relevant optimization
examples for hotspot code locations, thereby guiding LLMs to optimize these
hotspots. However, since semantically equivalent optimizations can manifest in
syntactically dissimilar code snippets, current retrieval methods often fail to
identify pertinent examples, leading to suboptimal optimization performance.
This limitation significantly reduces the effectiveness of existing
optimization approaches.
  To address these limitations, we propose SemOpt, a novel framework that
leverages static program analysis to precisely identify optimizable code
segments, retrieve the corresponding optimization strategies, and generate the
optimized results. SemOpt consists of three key components: (1) A strategy
library builder that extracts and clusters optimization strategies from
real-world code modifications. (2) A rule generator that generates Semgrep
static analysis rules to capture the condition of applying the optimization
strategy. (3) An optimizer that utilizes the strategy library to generate
optimized code results. All the three components are powered by LLMs.
  On our benchmark containing 151 optimization tasks, SemOpt demonstrates its
effectiveness under different LLMs by increasing the number of successful
optimizations by 1.38 to 28 times compared to the baseline. Moreover, on
popular large-scale C/C++ projects, it can improve individual performance
metrics by 5.04% to 218.07%, demonstrating its practical utility.

</details>


### [117] [Code Digital Twin: Empowering LLMs with Tacit Knowledge for Complex Software Development](https://arxiv.org/abs/2510.16395)
*Xin Peng,Chong Wang*

Main category: cs.SE

TL;DR: 本文提出了Code Digital Twin框架，旨在利用大语言模型(LLMs)和结构化知识体系解决企业级软件开发中复杂的隐性知识管理和决策支持问题。


<details>
  <summary>Details</summary>
Motivation: 企业软件开发依赖渐进式演进，面临超出普通编码的挑战，尤其是设计决策和历史权衡的隐性知识难以传承，现有AI工具难以满足复杂开发的需求。

Method: 系统识别软件和LLM视角下的挑战，结合结构化知识框架提出Code Digital Twin——一个动态的数字孪生模型，融合混合知识表示、多阶段信息抽取、增量更新、LLM辅助应用和人工反馈，提升知识的显性化与可操作性。

Result: Code Digital Twin能够将零散知识转化为明确且可执行的表达形式，促进了问题定位、影响分析等任务中的决策提升，实现了知识与代码库的共演化。

Conclusion: 该框架作为AI技术与企业软件现实的桥梁，推动复杂系统的智能化、可持续和灵活发展，提供了未来企业软件开发的具体路径。

Abstract: Recent advances in large language models (LLMs) have demonstrated strong
capabilities in software engineering tasks, raising expectations of
revolutionary productivity gains. However, enterprise software development is
largely driven by incremental evolution, where challenges extend far beyond
routine coding and depend critically on tacit knowledge, including design
decisions at different levels and historical trade-offs. To achieve effective
AI-powered support for complex software development, we should align emerging
AI capabilities with the practical realities of enterprise development. To this
end, we systematically identify challenges from both software and LLM
perspectives. Alongside these challenges, we outline opportunities where AI and
structured knowledge frameworks can enhance decision-making in tasks such as
issue localization and impact analysis. To address these needs, we propose the
Code Digital Twin, a living framework that models both the physical and
conceptual layers of software, preserves tacit knowledge, and co-evolves with
the codebase. By integrating hybrid knowledge representations, multi-stage
extraction pipelines, incremental updates, LLM-empowered applications, and
human-in-the-loop feedback, the Code Digital Twin transforms fragmented
knowledge into explicit and actionable representations. Our vision positions it
as a bridge between AI advancements and enterprise software realities,
providing a concrete roadmap toward sustainable, intelligent, and resilient
development and evolution of ultra-complex systems.

</details>


### [118] [Large-Scale Empirical Analysis of Continuous Fuzzing: Insights from 1 Million Fuzzing Sessions](https://arxiv.org/abs/2510.16433)
*Tatsuya Shirai,Olivier Nourry,Yutaro Kashiwa,Kenji Fujiwara,Yasutaka Kamei,Hajimu Iida*

Main category: cs.SE

TL;DR: 本文通过对OSS-Fuzz平台约112万个模糊测试会话的实证分析，揭示了持续模糊测试在漏洞检测中的作用和机制。


<details>
  <summary>Details</summary>
Motivation: 软件漏洞频发，模糊测试作为主要检测手段被集成到持续集成流程中，但其具体贡献尚不明确，研究旨在澄清持续模糊测试在漏洞检测中的作用。

Method: 采集OSS-Fuzz平台的漏洞报告、覆盖率报告和模糊测试日志，分析878个项目约112万次模糊测试会话中的覆盖率和漏洞发现情况。

Result: 发现持续模糊测试集成前已存在大量漏洞，导致早期检测率高；随持续执行，代码覆盖率持续提升；覆盖率的变化促进了漏洞的发现。

Conclusion: 持续模糊测试显著促进了漏洞检测，研究结果为未来持续模糊测试策略和工具开发提供了实证依据和指导。

Abstract: Software vulnerabilities are constantly being reported and exploited in
software products, causing significant impacts on society. In recent years, the
main approach to vulnerability detection, fuzzing, has been integrated into the
continuous integration process to run in short and frequent cycles. This
continuous fuzzing allows for fast identification and remediation of
vulnerabilities during the development process. Despite adoption by thousands
of projects, however, it is unclear how continuous fuzzing contributes to
vulnerability detection. This study aims to elucidate the role of continuous
fuzzing in vulnerability detection. Specifically, we investigate the coverage
and the total number of fuzzing sessions when fuzzing bugs are discovered. We
collect issue reports, coverage reports, and fuzzing logs from OSS-Fuzz, an
online service provided by Google that performs fuzzing during continuous
integration. Through an empirical study of a total of approximately 1.12
million fuzzing sessions from 878 projects participating in OSS-Fuzz, we reveal
that (i) a substantial number of fuzzing bugs exist prior to the integration of
continuous fuzzing, leading to a high detection rate in the early stages; (ii)
code coverage continues to increase as continuous fuzzing progresses; and (iii)
changes in coverage contribute to the detection of fuzzing bugs. This study
provides empirical insights into how continuous fuzzing contributes to fuzzing
bug detection, offering practical implications for future strategies and tool
development in continuous fuzzing.

</details>


### [119] [On the Use of Large Language Models for Qualitative Synthesis](https://arxiv.org/abs/2510.16502)
*Sebastián Pizard,Ramiro Moreira,Federico Galiano,Ignacio Sastre,Lorena Etcheverry*

Main category: cs.SE

TL;DR: 本文探讨了大型语言模型(LLMs)在系统综述（SR）中的定性综合（QS）应用风险和挑战，通过两次试验的自我民族志研究评估其方法学严谨性和实用性。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs在支持系统综述尤其是复杂任务如定性综合方面展现潜力，但由于定性综合阶段报道不均且执行多样，错误使用LLMs可能放大现有缺陷，降低综述结果的可信度，因此探索其挑战和风险十分必要。

Method: 通过协作性自我民族志方法进行两次试验，对每次试验的技术性能、方法学严谨性和实用性进行综合评估，并结合LLMs架构和其现有限制进行技术性解读。

Result: 试验揭示了使用LLMs进行定性综合过程中存在的风险和局限，特别是在方法学标准不一致和报告不完整时，LLMs可能导致误用或误导性结论。

Conclusion: 尽管LLMs在系统综述定性综合领域具潜力，但需谨慎应用，避免放大方法学缺陷，提升报告和执行的一致性，确保综述结果的可靠性。

Abstract: Large language models (LLMs) show promise for supporting systematic reviews
(SR), even complex tasks such as qualitative synthesis (QS). However, applying
them to a stage that is unevenly reported and variably conducted carries
important risks: misuse can amplify existing weaknesses and erode confidence in
the SR findings. To examine the challenges of using LLMs for QS, we conducted a
collaborative autoethnography involving two trials. We evaluated each trial for
methodological rigor and practical usefulness, and interpreted the results
through a technical lens informed by how LLMs are built and their current
limitations.

</details>


### [120] [Human-Aligned Code Readability Assessment with Large Language Models](https://arxiv.org/abs/2510.16579)
*Wendkûuni C. Ouédraogo,Yinghua Li,Xueqi Dang,Pawel Borsukiewicz,Xin Zhou,Anil Koyuncu,Jacques Klein,David Lo,Tegawendé F. Bissyandé*

Main category: cs.SE

TL;DR: 本文提出了CoReEval，一个用于评估基于大型语言模型(LLMs)代码可读性的大规模基准测试，涵盖多语言、多类型代码、多种提示策略和个性化开发者提示。


<details>
  <summary>Details</summary>
Motivation: 传统静态指标难以在大规模且主观性强的代码可读性评估中准确反映人类判断，且LLMs作为可扩展的评估工具尚未被充分研究。

Method: 构建涵盖超过140万评估的基准，比较10个顶尖LLMs在不同编程语言、代码类型、提示策略和解码设置下的表现，特别探讨以开发者引导的提示方法。

Result: 开发者引导的提示基于人类定义的可读性维度，提升了模型与人类注释的对齐度和解释质量，支持个性化评分，但存在评分波动性增大的问题。

Conclusion: CoReEval为LLMs代码可读性评估提供了可靠基础，有助于提示工程和模型对齐研究，可应用于教育、入职培训及持续集成环境，实现可解释且适应性的代码评审。

Abstract: Code readability is crucial for software comprehension and maintenance, yet
difficult to assess at scale. Traditional static metrics often fail to capture
the subjective, context-sensitive nature of human judgments. Large Language
Models (LLMs) offer a scalable alternative, but their behavior as readability
evaluators remains underexplored. We introduce CoReEval, the first large-scale
benchmark for evaluating LLM-based code readability assessment, comprising over
1.4 million model-snippet-prompt evaluations across 10 state of the art LLMs.
The benchmark spans 3 programming languages (Java, Python, CUDA), 2 code types
(functional code and unit tests), 4 prompting strategies (ZSL, FSL, CoT, ToT),
9 decoding settings, and developer-guided prompts tailored to junior and senior
personas. We compare LLM outputs against human annotations and a validated
static model, analyzing numerical alignment (MAE, Pearson's, Spearman's) and
justification quality (sentiment, aspect coverage, semantic clustering). Our
findings show that developer-guided prompting grounded in human-defined
readability dimensions improves alignment in structured contexts, enhances
explanation quality, and enables lightweight personalization through persona
framing. However, increased score variability highlights trade-offs between
alignment, stability, and interpretability. CoReEval provides a robust
foundation for prompt engineering, model alignment studies, and human in the
loop evaluation, with applications in education, onboarding, and CI/CD
pipelines where LLMs can serve as explainable, adaptable reviewers.

</details>


### [121] [Contrasting the Hyperparameter Tuning Impact Across Software Defect Prediction Scenarios](https://arxiv.org/abs/2510.16665)
*Mohamed Sami Rakha,Andriy Miranskyy,Daniel Alencar da Costa*

Main category: cs.SE

TL;DR: 本文比较了两个软件缺陷预测场景中超参数调优的影响，发现内部版本缺陷预测场景的性能提升显著优于跨版本缺陷预测。


<details>
  <summary>Details</summary>
Motivation: 现有研究发现超参数调优能提升软件缺陷预测性能，但不同预测场景下的提升效果可能不同，需比较以指导实际应用。

Method: 通过在28种机器学习算法、53个软件数据集、两种调优算法及五个优化指标下，使用统计分析方法对内部版本缺陷预测和跨版本缺陷预测场景的性能变化进行实证对比。

Result: 内部版本缺陷预测场景中超参数调优带来的性能提升显著大于跨版本场景，多数算法性能提升结果并非跨场景通用，小规模数据集表现出更大差异。

Conclusion: 软件工程研究者和从业者应考虑不同缺陷预测场景对超参数调优效果的影响，合理预期性能提升，提升模型的适用性和效果。

Abstract: Software defect prediction (SDP) is crucial for delivering high-quality
software products. Recent research has indicated that prediction performance
improvements in SDP are achievable by applying hyperparameter tuning to a
particular SDP scenario. However, the positive impact resulting from the
hyperparameter tuning step may differ based on the targeted SDP scenario.
Comparing the impact of hyperparameter tuning across SDP scenarios is necessary
to provide comprehensive insights and enhance the robustness, generalizability,
and, eventually, the practicality of SDP modeling for quality assurance.
  Therefore, in this study, we contrast the impact of hyperparameter tuning
across two pivotal and consecutive SDP scenarios: (1) Inner Version Defect
Prediction (IVDP) and (2) Cross Version Defect Prediction (CVDP). The main
distinctions between the two scenarios lie in the scope of defect prediction
and the selected evaluation setups. This study's experiments use common
evaluation setups, 28 machine learning (ML) algorithms, 53 post-release
software datasets, two tuning algorithms, and five optimization metrics. We
apply statistical analytics to compare the SDP performance impact differences
by investigating the overall impact, the single ML algorithm impact, and
variations across different software dataset sizes.
  The results indicate that the SDP gains within the IVDP scenario are
significantly larger than those within the CVDP scenario. The results reveal
that asserting performance gains for up to 24 out of 28 ML algorithms may not
hold across multiple SDP scenarios. Furthermore, we found that small software
datasets are more susceptible to larger differences in performance impacts.
Overall, the study findings recommend software engineering researchers and
practitioners to consider the effect of the selected SDP scenario when
expecting performance gains from hyperparameter tuning.

</details>


### [122] [QuanBench: Benchmarking Quantum Code Generation with Large Language Models](https://arxiv.org/abs/2510.16779)
*Xiaoyu Guo,Minggu Wang,Jianjun Zhao*

Main category: cs.SE

TL;DR: 本论文提出了QuanBench，一个评估大语言模型（LLMs）量子代码生成能力的基准，包括44个量子编程任务。研究发现当前LLMs在量子代码生成方面准确率低于40%，存在多种语义错误。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在通用代码生成表现优异，但其在量子代码生成上的能力尚缺乏系统研究，有必要构建一个标准化的评测体系。

Method: 设计并实现了QuanBench，涵盖量子算法、态制备、门分解和量子机器学习等44个任务，每个任务均有可执行的标准答案，并通过功能正确性和量子语义等价性两个指标进行评估。评测了多个最新的通用及代码专用LLMs，分析其生成量子代码的表现和典型错误。

Result: 评测结果显示当前LLMs生成正确量子代码的准确率不足40%，语义错误频发，包括API过时、量子电路构建错误及算法逻辑错误等。

Conclusion: QuanBench为量子代码生成提供了系统评测工具，揭示了现有模型的不足，促进未来针对量子代码生成能力的改进。

Abstract: Large language models (LLMs) have demonstrated good performance in general
code generation; however, their capabilities in quantum code generation remain
insufficiently studied. This paper presents QuanBench, a benchmark for
evaluating LLMs on quantum code generation. QuanBench includes 44 programming
tasks that cover quantum algorithms, state preparation, gate decomposition, and
quantum machine learning. Each task has an executable canonical solution and is
evaluated by functional correctness (Pass@K) and quantum semantic equivalence
(Process Fidelity). We evaluate several recent LLMs, including general-purpose
and code-specialized models. The results show that current LLMs have limited
capability in generating the correct quantum code, with overall accuracy below
40% and frequent semantic errors. We also analyze common failure cases, such as
outdated API usage, circuit construction errors, and incorrect algorithm logic.
QuanBench provides a basis for future work on improving quantum code generation
with LLMs.

</details>


### [123] [More with Less: An Empirical Study of Turn-Control Strategies for Efficient Coding Agents](https://arxiv.org/abs/2510.16786)
*Pengfei Gao,Chao Peng*

Main category: cs.SE

TL;DR: 本文研究了大型语言模型驱动的编码代理在迭代任务中的成本控制问题，提出并验证了动态回合数控制策略，显著降低成本同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型驱动的编码代理在实际部署时因回合数增加带来高昂且不确定的计算成本，现有研究多关注单回合优化，忽视了整体回合数的战略控制。

Method: 在SWE-bench数据集上，使用三种先进模型，比较了三种回合控制策略：无上限基线、固定回合限制（75百分位数）及动态按需延长回合策略。

Result: 固定回合限制策略可在基本不损失性能的情况下减少24%-68%的成本，动态回合策略相较固定限制进一步减少12%-24%的成本，同时保持或提升了任务解决率。

Conclusion: 首次系统分析了回合控制策略，证实动态资源分配是一种简单有效的方式，可在部署强大编码代理时实现成本与效能的最佳平衡。

Abstract: LLM-powered coding agents, which operate in iterative loops (turns) to solve
software engineering tasks, are becoming increasingly powerful. However, their
practical deployment is hindered by significant and unpredictable costs. This
challenge arises from a combination of factors: quadratically growing token
counts with each turn, the high price of models, the large number of turns
required for real-world tasks, and the tendency of agents to take inefficient
or unnecessary actions. While existing research focuses on optimizing
individual turns, the strategic control of the total number of turns remains an
underexplored area for managing agent performance and cost. To address this
gap, we conduct a comprehensive empirical study on SWE-bench using three
state-of-the-art models and evaluate the impact of three distinct turn-control
strategies: an unrestricted baseline, a fixed-turn limit with reminders, and a
novel dynamic-turn strategy that grants extensions on-demand. Our findings
first reveal a fundamental trade-off in the unrestricted setting, where no
single model excels across performance, cost, and turn efficiency. We then show
that a fixed-turn limit, specifically at the 75th percentile of the baseline,
serves as a "sweet spot", substantially reducing costs (by 24%-68%) with
minimal impact on solve rates. Most significantly, the dynamic-turn strategy
consistently outperforms fixed-limit approaches, achieving comparable or better
solve rates while further reducing costs by an additional 12%-24% by
intelligently allocating resources only to tasks that need them. This work
provides the first systematic analysis of turn-control strategies, offering
simple yet effective guidelines for developers to balance cost and efficacy. We
demonstrate that dynamic resource allocation is a superior, easy-to-implement
approach for deploying powerful yet economically viable coding agents.

</details>


### [124] [When Many-Shot Prompting Fails: An Empirical Study of LLM Code Translation](https://arxiv.org/abs/2510.16809)
*Amirkia Rafiei Oskooei,Kaan Baturalp Cosdan,Husamettin Isiktas,Mehmet S. Aktas*

Main category: cs.SE

TL;DR: 在代码翻译任务中，少量高质量的示例比大量示例更能提升大语言模型的表现，挑战了“更多示例更好”的传统观点。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型中上下文中示例数量对复杂代码翻译任务性能的影响，验证“多示例有助提升性能”的普遍假设。

Method: 通过对超过9万次代码翻译的大规模实证研究，系统评估从零示例到最多625示例的示例规模变化对模型性能的影响，使用的提示长度从10万到80万令牌。

Result: 发现“多示例悖论”：虽然静态相似性指标随示例增加略有提升，功能正确性却在5-25示例的少示例配置时达到峰值，更多示例反而降低关键的功能性能。

Conclusion: 对于代码翻译任务，少量精选示例的质量比示例数量更关键，提示策略需根据具体任务调整，反驳了“一味增加示例数量”的普适有效性，对软件工程中大语言模型的应用具有重要意义。

Abstract: Large Language Models (LLMs) with vast context windows offer new avenues for
in-context learning (ICL), where providing many examples ("many-shot"
prompting) is often assumed to enhance performance. We investigate this
assumption for the complex task of code translation. Through a large-scale
empirical study of over 90,000 translations, we systematically evaluate the
impact of scaling in-context examples from zero-shot to many-shot
configurations of up to 625 examples, with prompts spanning from approximately
100,000 to 800,000 tokens. Our findings reveal a "many-shot paradox": while
static similarity metrics may modestly improve with more examples, functional
correctness consistently peaks with few-shot prompting (5-25 examples).
Providing substantially more examples often degrades this crucial functional
performance. This study highlights that for code translation, the quality of a
few well-chosen examples outweighs sheer quantity, challenging the universal
efficacy of "more is better" for ICL and underscoring the task-dependent nature
of optimal prompting strategies. Our results have significant implications for
effectively leveraging LLMs in software engineering.

</details>


### [125] [When AI Takes the Wheel: Security Analysis of Framework-Constrained Program Generation](https://arxiv.org/abs/2510.16823)
*Yue Liu,Zhenchang Xing,Shidong Pan,Chakkrit Tantithamthavorn*

Main category: cs.SE

TL;DR: 研究评估了当前大型语言模型（LLMs）生成的Chrome扩展程序的安全性，发现漏洞率高达18%-50%。


<details>
  <summary>Details</summary>
Motivation: LLMs在软件开发中应用广泛，但开发者忽视了自动生成程序中存在的安全隐患，特别是在复杂的安全模型环境下。

Method: 构建了包含140个基于已知漏洞扩展的提示数据集ChromeSecBench，利用九个先进LLMs生成Chrome扩展，并从多维度（场景类型、模型差异和漏洞类别）分析其安全漏洞。

Result: 发现LLMs生成的扩展程序存在高比例的漏洞，尤其是在认证身份和Cookie管理场景中，漏洞率分别高达83%和78%，且复杂推理模型漏洞更多。

Conclusion: LLMs虽然具备强大的编程能力，但在生成安全的框架约束程序方面存在显著不足，需要加强安全性注意与改进。

Abstract: In recent years, the AI wave has grown rapidly in software development. Even
novice developers can now design and generate complex framework-constrained
software systems based on their high-level requirements with the help of Large
Language Models (LLMs). However, when LLMs gradually "take the wheel" of
software development, developers may only check whether the program works. They
often miss security problems hidden in how the generated programs are
implemented.
  In this work, we investigate the security properties of framework-constrained
programs generated by state-of-the-art LLMs. We focus specifically on Chrome
extensions due to their complex security model involving multiple privilege
boundaries and isolated components. To achieve this, we built ChromeSecBench, a
dataset with 140 prompts based on known vulnerable extensions. We used these
prompts to instruct nine state-of-the-art LLMs to generate complete Chrome
extensions, and then analyzed them for vulnerabilities across three dimensions:
scenario types, model differences, and vulnerability categories. Our results
show that LLMs produced vulnerable programs at alarmingly high rates (18%-50%),
particularly in Authentication & Identity and Cookie Management scenarios (up
to 83% and 78% respectively). Most vulnerabilities exposed sensitive browser
data like cookies, history, or bookmarks to untrusted code. Interestingly, we
found that advanced reasoning models performed worse, generating more
vulnerabilities than simpler models. These findings highlight a critical gap
between LLMs' coding skills and their ability to write secure
framework-constrained programs.

</details>


### [126] [Will AI also replace inspectors? Investigating the potential of generative AIs in usability inspection](https://arxiv.org/abs/2510.17056)
*Luis F. G. Campos,Leonardo C. Marques,Walter T. Nakamura*

Main category: cs.SE

TL;DR: 本文研究了生成式人工智能（AI）在软件界面可用性检查中的表现，发现AI虽然不能完全替代人类专家，但能有效补充提升检测效率与缺陷覆盖。


<details>
  <summary>Details</summary>
Motivation: 可用性检查是提高软件产品质量的重要手段，但过程耗时且需专业知识，借助AI有望提升效率。

Method: 通过让四位专家和两种生成式AI模型（GPT-4o和Gemini 2.5 Flash）对同一软件原型进行检测，比较其精准度、召回率和F1分数等指标。

Result: 专家在精准度和覆盖率上表现最佳，AI模型表现也不俗，尤其能发现许多新缺陷，但存在较多误报和冗余。将AI与人类结合能取得最佳效果。

Conclusion: 当前阶段AI无法完全替代人类检查员，但可作为辅助工具，提高检测效率和缺陷发现范围，适合作为软件质量评估的补充手段。

Abstract: Usability inspection is a well-established technique for identifying
interaction issues in software interfaces, thereby contributing to improved
product quality. However, it is a costly process that requires time and
specialized knowledge from inspectors. With advances in Artificial Intelligence
(AI), new opportunities have emerged to support this task, particularly through
generative models capable of interpreting interfaces and performing inspections
more efficiently. This study examines the performance of generative AIs in
identifying usability problems, comparing them to those of experienced human
inspectors. A software prototype was evaluated by four specialists and two AI
models (GPT-4o and Gemini 2.5 Flash), using metrics such as precision, recall,
and F1-score. While inspectors achieved the highest levels of precision and
overall coverage, the AIs demonstrated high individual performance and
discovered many novel defects, but with a higher rate of false positives and
redundant reports. The combination of AIs and human inspectors produced the
best results, revealing their complementarity. These findings suggest that AI,
in its current stage, cannot replace human inspectors but can serve as a
valuable augmentation tool to improve efficiency and expand defect coverage.
The results provide evidence based on quantitative analysis to inform the
discussion on the role of AI in usability inspections, pointing to viable paths
for its complementary use in software quality assessment contexts.

</details>


### [127] [M2QCode: A Model-Driven Framework for Generating Multi-Platform Quantum Programs](https://arxiv.org/abs/2510.17110)
*Xiaoyu Guo,Shinobu Saito,Jianjun Zhao*

Main category: cs.SE

TL;DR: 本文提出了一种基于模型驱动开发（MDD）的量子系统设计与实现方法，支持多种量子编程语言的代码自动生成，提高开发效率和跨平台一致性。


<details>
  <summary>Details</summary>
Motivation: 量子计算快速发展，出现多种量子编程语言，但量子系统工程中的模型驱动开发应用尚未充分探讨。

Method: 构建基于模型驱动开发的量子系统设计框架，实现多种量子编程语言代码的自动生成。

Result: 通过多个案例验证该方法的有效性和实用性。

Conclusion: 基于MDD的方法能提升量子系统开发效率和跨平台一致性，具有较好应用前景。

Abstract: With the growing interest in quantum computing, the emergence of quantum
supremacy has marked a pivotal milestone in the field. As a result, numerous
quantum programming languages (QPLs) have been introduced to support the
development of quantum algorithms. However, the application of Model-Driven
Development (MDD) in quantum system engineering remains largely underexplored.
This paper presents an MDD-based approach to support the structured design and
implementation of quantum systems. Our framework enables the automatic
generation of quantum code for multiple QPLs, thereby enhancing development
efficiency and consistency across heterogeneous quantum platforms. The
effectiveness and practicality of our approach have been demonstrated through
multiple case studies.

</details>


### [128] [SEER: Enhancing Chain-of-Thought Code Generation through Self-Exploring Deep Reasoning](https://arxiv.org/abs/2510.17130)
*Shuzheng Gao,Chaozheng Wang,Cuiyun Gao,Michael R. Lyu*

Main category: cs.SE

TL;DR: 本文提出SEER框架，通过多样路径探索、质量评估和自适应推理提升代码生成的链式推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有链式推理方法在代码生成中存在路径探索有限、中间步骤缺乏评估以及过度复杂化问题，影响泛化与准确性。

Method: 将链式推理视为决策问题，提出SEER框架，包括多样路径探索、基于质量的模型训练（策略模型和价值模型）、以及自适应推理切换机制。

Result: SEER能在不同编程场景中有效生成准确且简洁的代码推理路径，克服现有方法的不足。

Conclusion: 通过SEER框架实现了代码生成中链式推理的准确性与适应性提升，为自然语言到代码转换提供了更可靠的解决方案。

Abstract: Code generation, the task of creating executable programs from natural
language requirements, has recently seen tremendous advances through
Chain-of-Thought (CoT) reasoning, which enables Large Language Models (LLMs) to
develop high-level reasoning plans before writing code. Recent research has
proposed various methods to enhance models' CoT reasoning for code generation
such as prompt engineering and supervised fine-tuning. However, existing
approaches still face three critical limitations: (1) limited exploration of
diverse reasoning paths, which constrains generalization across various
programming scenarios, (2) lack of quality assessment for intermediate
reasoning steps, which hampers the reliability of the generated plans and code,
and (3) the potential negative impact of "overthinking", potentially leading to
unnecessarily complex and incorrect solutions. To address these limitations, we
frame CoT code generation as a decision making problem and present SEER, a
SElf-Exploring deep Reasoning framework that enables accurate and adaptive
reasoning for code generation. SEER introduces three key components: (1)
Diverse reasoning path exploration, which aims at exploring diverse reasoning
paths and annotating intermediate steps without relying on manual experts or
closed-source proprietary models; (2) Reasoning quality-aware model training,
which trains a policy model for generating candidate reasoning steps and a
value model for assessing their quality; and (3) Adaptive CoT reasoning, which
dynamically switches between direct generation and step-by-step reasoning for
different problems.

</details>


### [129] [PEACE: Towards Efficient Project-Level Efficiency Optimization via Hybrid Code Editing](https://arxiv.org/abs/2510.17142)
*Xiaoxue Ren,Jun Wan,Yun Peng,Zhongxin Liu,Ming Liang,Dajun Chen,Wei Jiang,Yong Li*

Main category: cs.SE

TL;DR: 提出了Peace框架，通过自动代码编辑实现项目级代码效率优化，兼顾整体正确性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的代码效率优化仅关注函数层面，缺乏对函数间交互的处理，难以适应真实项目需求。传统代码编辑技术则面临无效编辑及函数内部次优问题。

Method: Peace框架包括依赖感知优化函数序列构建、有效相关编辑识别以及效率优化编辑迭代三阶段，确保项目整体正确和高效。并构建PeacExec作为评价基准套件。

Result: 在PeacExec上，Peace实现了69.2%的正确率，46.9%的优化率和0.840的执行效率加速，显著优于现有最先进方法，特别在多函数复杂优化任务中表现突出。

Conclusion: Peace框架有效解决了项目级代码优化中函数间交互和编辑有效性问题，验证了其设计合理性和优越性，推动了LLM在实际代码效率优化中的应用。

Abstract: Large Language Models (LLMs) have demonstrated significant capability in code
generation, but their potential in code efficiency optimization remains
underexplored. Previous LLM-based code efficiency optimization approaches
exclusively focus on function-level optimization and overlook interaction
between functions, failing to generalize to real-world development scenarios.
Code editing techniques show great potential for conducting project-level
optimization, yet they face challenges associated with invalid edits and
suboptimal internal functions. To address these gaps, we propose Peace, a novel
hybrid framework for Project-level code Efficiency optimization through
Automatic Code Editing, which also ensures the overall correctness and
integrity of the project. Peace integrates three key phases: dependency-aware
optimizing function sequence construction, valid associated edits
identification, and efficiency optimization editing iteration. To rigorously
evaluate the effectiveness of Peace, we construct PeacExec, the first benchmark
comprising 146 real-world optimization tasks from 47 high-impact GitHub Python
projects, along with highly qualified test cases and executable environments.
Extensive experiments demonstrate Peace's superiority over the state-of-the-art
baselines, achieving a 69.2% correctness rate (pass@1), +46.9% opt rate, and
0.840 speedup in execution efficiency. Notably, our Peace outperforms all
baselines by significant margins, particularly in complex optimization tasks
with multiple functions. Moreover, extensive experiments are also conducted to
validate the contributions of each component in Peace, as well as the rationale
and effectiveness of our hybrid framework design.

</details>


### [130] [TREAT: A Code LLMs Trustworthiness / Reliability Evaluation and Testing Framework](https://arxiv.org/abs/2510.17163)
*Shuzheng Gao,Eric John Li,Man Ho Lam,Jingyu Xiao,Yuxuan Wan,Chaozheng Wang,Ng Man Tik,Michael R. Lyu*

Main category: cs.SE

TL;DR: 本文提出了一个名为TREAT的评估框架，全面评估大型代码生成模型在软件工程中的可信度和可靠性，涵盖多任务、多语言、多模态，并且考虑模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大型基础模型在代码生成等任务中表现优异，但现有评估基准缺乏全面性，未涵盖模型的鲁棒性和可靠性，无法全面评估其在真实软件工程场景中的可信度。

Method: 设计了TREAT框架，包含多任务多语言多模态评估，鲁棒性测试（语义不变的代码变换），以及多样化的评估提示和自适应解答提取等严格评估方法。

Result: 基于TREAT框架评估了26个先进模型，发现模型在不同任务间表现差异显著，多模态模型在UI代码生成和编辑上存在明显不足。

Conclusion: 全面的多维度评估框架有助于揭示代码生成模型的优缺点，为提升模型在软件工程中的实用可信性提供指导。

Abstract: Large foundation models are fundamentally transforming the software
engineering landscape, demonstrating exceptional capabilities across diverse
tasks such as code generation, debugging, and testing. Despite this rapid
progress, a significant gap remains in how to comprehensively evaluate these
models' trustworthiness in real-world software engineering scenarios. Existing
benchmarks suffer from limited task scope and fail to incorporate critical
evaluation aspects such as the robustness and reliability of models. To bridge
this gap, we present an evaluation framework called TREAT (Code LLMs
Trustworthiness / Reliability Evaluation And Testing) that provides a holistic
assessment of model performance in code intelligence tasks. Our evaluation
framework addresses key limitations in existing approaches with four main
improvements: (1) Multi-Task Holistic Evaluation that spans diverse software
engineering activities rather than limited coding tasks; (2) Multi-Language and
Multi-Modality Assessment that extends beyond traditional single-language,
text-only benchmarks to include multi-modality coding tasks; (3) Robustness
Assessment that evaluates model reliability under semantically-preserving code
transformations; and (4) Rigorous Evaluation Methodology that enhances the
trustworthiness of evaluation results through diverse evaluation prompts and
adaptive solution extraction. Based on this evaluation framework, we assess 26
state-of-the-art models and uncover both their strengths and limitations,
yielding several key insights:(1) Current models show substantial performance
variation across programming tasks; (2) Multi-modal language models demonstrate
specific performance limitations in UI code generation and edit;

</details>


### [131] [Software Testing with Large Language Models: An Interview Study with Practitioners](https://arxiv.org/abs/2510.17164)
*Maria Deolinda Santana,Cleyton Magalhaes,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: 探讨软件测试专业人士如何使用大型语言模型（LLM），提出整合LLM的初步指导方针。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在软件测试中应用广泛，但缺乏结构化的使用指导，主要依赖非正式试验。

Method: 通过对15名不同背景的软件测试人员进行半结构化访谈，采用基于扎根理论的主题分析方法分析数据。

Result: 测试人员采用迭代且反思性的流程，包括目标定义、提示工程、输出评估和学习，强调人类监督和验证的重要性，因应LLM的局限。

Conclusion: LLM在软件测试中的应用持续增长，但仍需谨慎管理风险，研究为规范LLM集成测试流程提供了初步框架，呼吁进一步研究优化实践。

Abstract: \textit{Background:} The use of large language models in software testing is
growing fast as they support numerous tasks, from test case generation to
automation, and documentation. However, their adoption often relies on informal
experimentation rather than structured guidance. \textit{Aims:} This study
investigates how software testing professionals use LLMs in practice to propose
a preliminary, practitioner-informed guideline to support their integration
into testing workflows. \textit{Method:} We conducted a qualitative study with
15 software testers from diverse roles and domains. Data were collected through
semi-structured interviews and analyzed using grounded theory-based processes
focused on thematic analysis. \textit{Results:} Testers described an iterative
and reflective process that included defining testing objectives, applying
prompt engineering strategies, refining prompts, evaluating outputs, and
learning over time. They emphasized the need for human oversight and careful
validation, especially due to known limitations of LLMs such as hallucinations
and inconsistent reasoning. \textit{Conclusions:} LLM adoption in software
testing is growing, but remains shaped by evolving practices and caution around
risks. This study offers a starting point for structuring LLM use in testing
contexts and invites future research to refine these practices across teams,
tools, and tasks.

</details>


### [132] [OLIVAW: ACIMOV's GitHub robot assisting agile collaborative ontology development](https://arxiv.org/abs/2510.17184)
*Nicolas Robert,Fabien Gandon,Maxime Lefrançois*

Main category: cs.SE

TL;DR: 本论文提出了OLIVAW工具，用于支持ACIMOV方法论，通过GitHub实现本体的模块化设计和持续验证。


<details>
  <summary>Details</summary>
Motivation: 为了确保本体设计具有用户驱动性、实时更新性和演进能力，需要有效的持续验证工具以满足开发者需求。

Method: 提出OLIVAW工具，基于W3C标准，利用GitHub复合动作、预提交钩子和命令行接口，支持模块化本体开发。

Result: OLIVAW在多个本体项目中测试，验证了其有效性、通用性和可重用性，同时提供了模板仓库以便快速启动。

Conclusion: OLIVAW为本体的敏捷协作设计提供了可靠的工具支持，有助于本体的持续集成和演进。

Abstract: Agile and collaborative approaches to ontologies design are crucial because
they contribute to making them userdriven, up-to-date, and able to evolve
alongside the systems they support, hence proper continuous validation tooling
is required to ensure ontologies match developers' requirements all along their
development. We propose OLIVAW (Ontology Long-lived Integration Via ACIMOV
Workflow), a tool supporting the ACIMOV methodology on GitHub. It relies on W3C
Standards to assist the development of modular ontologies through GitHub
Composite Actions, pre-commit hooks, or a command line interface. OLIVAW was
tested on several ontology projects to ensure its usefulness, genericity and
reusability. A template repository is available for a quick start. OLIVAW is

</details>


### [133] [AdapTrack: Constrained Decoding without Distorting LLM's Output Intent](https://arxiv.org/abs/2510.17376)
*Yongmin Li,Jia Li,Ge Li,Zhi Jin*

Main category: cs.SE

TL;DR: 本文提出了一种名为AdapTrack的代码生成约束解码方法，通过引入回溯机制避免扭曲模型输出意图，显著提升了代码生成的准确性和语义一致性。


<details>
  <summary>Details</summary>
Motivation: 当前的约束解码技术虽然能保证生成代码满足约束条件（如语法正确、API存在），但往往扭曲模型的输出意图，导致生成代码虽符合约束却语义不正确。

Method: AdapTrack在生成过程中引入回溯机制，允许模型在遇到约束冲突时返回重试，从而保持输出意图不被扭曲，提高生成代码的语义一致性。

Result: 在多个数据集上的实验表明，AdapTrack相比传统约束解码在API补全和通用代码生成任务上分别取得了最高360.87%、38.93%、7.84%和6.42%的性能提升。理论上证明了其生成分布与模型分布一致。

Conclusion: AdapTrack通过回溯机制有效解决了约束解码扭曲输出意图的问题，提高了代码生成的准确性和语义匹配度，具有较好的理论保证和实验证明。

Abstract: Language model-based code generation and completion tools have been widely
adopted, but they may sometimes produce code that does not meet necessary
constraints, such as syntactic correctness or API existence. Constrained
decoding techniques are developed to help the model generate code adhering to
the constraints by greedily eliminating generation options that violate
constraints at each step of the generation process. However, there is a severe
limitation of constrained decoding, that it distorts the model's output intent,
forcing it to produce code that may satisfy the constraint but does not match
the development intent and is therefore incorrect. In response to this
challenge, we propose AdapTrack. By incorporating backtracking into the
generation process, AdapTrack avoids distorting the output intent of the model,
thereby producing results that are not only constraint-compliant but also more
semantically aligned with model's output intent. On our synthetic API
completion dataset, AdapTrack can achieve up to 360.87% improvement compared to
constrained decoding; on the real-world API completion dataset we collect that
exhibits similar issues, AdapTrack can achieve up to 38.93% improvement over
constrained decoding; in general code genration benchmarks, compared to
constrained decoding, AdapTrack can achieve up to 7.84% improvement on
HumanEval, and up to 6.42% improvement on MBPP. This indicates that, simply by
better adhering to the model's output intent, AdapTrack can achieve significant
improvements. We provide a theoretical proof that the distribution produced by
AdapTrack aligns with the model's distribution given the generated tokens,
thereby ensuring that the model's output intent is not distorted. Experiments
on DSL problems show that, compared to existing methods, our approach can
provide generation results that are more consistent with the language model's
distribution.

</details>


### [134] [Scalable CI/CD for Legacy Modernization: An Industrial Experience Addressing Internal Challenges Related to the 2025 Japan Cliff](https://arxiv.org/abs/2510.17430)
*Kuniaki Kudo,Sherine Devi*

Main category: cs.SE

TL;DR: 本文介绍了为解决2025年日本遗留核心IT系统服务终止带来的维护成本激增问题，开发的可扩展CI/CD流水线，实现了动态创建和删除开发环境，提高维护效率并推动数字化转型。


<details>
  <summary>Details</summary>
Motivation: 日本面临2025年遗留核心IT系统服务终止危机，维护成本大幅增加且系统难以更新，阻碍数字化转型。朝日公司也遇到了类似问题，亟需解决遗留系统的维护和更新难题。

Method: 开发并实施了包含GitHub、Jenkins、AWS和Docker的可扩展CI/CD流水线，支持动态创建删除隔离开发环境，提升开发和测试效率。

Result: 该CI/CD流水线使开发者能够自由安全地测试维护流程和新技术，显著降低维护成本，促进数字化转型。

Conclusion: 提出的可扩展CI/CD流水线有效解决了遗留系统维护难题，支持动态开发环境，推动企业数字化转型，应对2025年日本核心系统断层危机。

Abstract: We have developed a Scalable CI/CD Pipeline to address internal challenges
related to Japan 2025 cliff problem, a critical issue where the mass end of
service life of legacy core IT systems threatens to significantly increase the
maintenance cost and black box nature of these system also leads to difficult
update moreover replace, which leads to lack of progress in Digital
Transformation (DX). If not addressed, Japan could potentially lose up to 12
trillion yen per year after 2025, which is 3 times more than the cost in
previous years. Asahi also faced the same internal challenges regarding legacy
system, where manual maintenance workflows and limited QA environment have left
critical systems outdated and difficult to update. Middleware and OS version
have remained unchanged for years, leading to now its nearing end of service
life which require huge maintenance cost and effort to continue its operation.
To address this problem, we have developed and implemented a Scalable CI/CD
Pipeline where isolated development environments can be created and deleted
dynamically and is scalable as needed. This Scalable CI/CD Pipeline incorporate
GitHub for source code control and branching, Jenkins for pipeline automation,
Amazon Web Services for scalable environment, and Docker for environment
containerization. This paper presents the design and architecture of the
Scalable CI/CD Pipeline, with the implementation along with some use cases.
Through Scalable CI/CD, developers can freely and safely test maintenance
procedures and do experiments with new technology in their own environment,
reducing maintenance cost and drive Digital Transformation (DX).
  key words: 2025 Japan Cliff, Scalable CI/CD, DevOps, Legacy IT Modernization.

</details>
