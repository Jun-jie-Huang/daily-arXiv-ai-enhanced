<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 73]
- [cs.SE](#cs.SE) [Total: 8]
- [cs.MA](#cs.MA) [Total: 4]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Unsupervised Cycle Detection in Agentic Applications](https://arxiv.org/abs/2511.10650)
*Felix George,Harshit Kumar,Divya Pathak,Kaustabha Ray,Mudit Verma,Pratibha Moogi*

Main category: cs.CL

TL;DR: 本文提出了一种结合结构与语义分析的无监督循环检测框架，有效识别大语言模型驱动的代理程序中的隐藏执行循环，提升了检测效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型驱动的代理程序表现出非确定性行为，可能形成隐藏的执行循环，导致资源浪费且传统观测平台难以检测这些低效现象。

Method: 首先采用计算高效的时间调用栈分析识别显式循环，随后利用语义相似度分析发现包含冗余内容生成的隐性循环。

Result: 在基于LangGraph的股票市场应用的1575条轨迹中，混合方法达到了F1值0.72（精确率0.62，召回率0.86），显著优于单一结构（F1 0.08）和语义方法（F1 0.28）。

Conclusion: 该方法效果令人鼓舞，但仍有较大提升空间，未来需要进一步改进以克服现有限制。

Abstract: Agentic applications powered by Large Language Models exhibit non-deterministic behaviors that can form hidden execution cycles, silently consuming resources without triggering explicit errors. Traditional observability platforms fail to detect these costly inefficiencies. We present an unsupervised cycle detection framework that combines structural and semantic analysis. Our approach first applies computationally efficient temporal call stack analysis to identify explicit loops and then leverages semantic similarity analysis to uncover subtle cycles characterized by redundant content generation. Evaluated on 1575 trajectories from a LangGraph-based stock market application, our hybrid approach achieves an F1 score of 0.72 (precision: 0.62, recall: 0.86), significantly outperforming individual structural (F1: 0.08) and semantic methods (F1: 0.28). While these results are encouraging, there remains substantial scope for improvement, and future work is needed to refine the approach and address its current limitations.

</details>


### [2] [Data Analysis and Performance Evaluation of Simulation Deduction Based on LLMs](https://arxiv.org/abs/2511.10651)
*Shansi Zhang,Min Li*

Main category: cs.CL

TL;DR: 本文提出了一种利用大语言模型进行战场仿真推断数据分析和性能评估的方法，通过任务分解、多轮交互和自我反思，生成结构化、高质量的分析报告。


<details>
  <summary>Details</summary>
Motivation: 传统的手工数据分析费时且容易出错，难以高效生成结构化、高质量的仿真推断分析报告。

Method: 将复杂分析任务分解为多个子任务，为每个子任务设计系统和用户提示，采用多轮交互结合自查反思，实现结构化数据提取和多步分析；同时定义自定义工具生成图表和计算指标，并设计多模版以适应不同应用场景。

Result: 通过大量评估，所提方法生成的分析报告质量优于基线方法，获得更高评分。

Conclusion: 该方法有效提升了仿真推断数据分析的效率与准确性，能够生成高质量、格式良好的分析报告，适用于多种应用场景。

Abstract: Data analysis and performance evaluation of simulation deduction plays a pivotal role in modern warfare, which enables military personnel to gain invaluable insights into the potential effectiveness of different strategies, tactics, and operational plans. Traditional manual analysis approach is time-consuming and limited by human errors. To enhance efficiency and accuracy, large language models (LLMs) with strong analytical and inferencing capabilities can be employed. However, high-quality analysis reports with well-structured formatting cannot be obtained through a single instruction input to the LLM. To tackle this issue, we propose a method that first decomposes the complex task into several sub-tasks and designs effective system prompts and user prompts for each sub-task. Multi-round interactions with the LLM incorporating self-check and reflection are then conducted to enable structured data extraction as well as multi-step analysis and evaluation. Furthermore, custom tools are defined and invoked to generate figures and compute metrics. We also design multiple report templates, each tailored to a specific application and input data type, ensuring their adaptability across a variety of scenarios. Extensive evaluation results demonstrate that the reports generated by our method exhibit higher quality, therefore obtaining higher scores than the baseline method.

</details>


### [3] [Cognitively-Inspired Episodic Memory Architectures for Accurate and Efficient Character AI](https://arxiv.org/abs/2511.10652)
*Rafael Arias Gonzalez,Steve DiPaola*

Main category: cs.CL

TL;DR: 本论文提出了一种结合离线数据扩增和高效并行检索的架构，提升历史人物对话系统的响应深度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在历史角色对话中面临简单检索生成反应浅显、多阶段反思响应深度但延迟高的矛盾。

Method: 通过将传记数据转化为具情感语义元数据的结构化记忆，采用两阶段检索策略，实现0.52秒响应生成；并利用GPT-4等模型进行评估。

Result: 该方法在小型模型（GPT-3.5、GPT-3）上明显优于传统检索增强生成方法，且在效率和准确性间取得平衡，支持快速响应。

Conclusion: 所提架构适用于任何有丰富文本记录的历史人物，兼顾对话交互与研究分析，具备教育、博物馆及研究应用的实用价值。

Abstract: Large language models show promise for embodying historical characters in dialogue systems, but existing approaches face a critical trade-off: simple retrieval-augmented generation produces shallow responses, while multi-stage reflection achieves depth at prohibitive latency. We present an architecture that resolves this tension through offline data augmentation and efficient parallel retrieval from structured episodic memory. Our system transforms biographical data into 1,774 enriched first-person memories with affective-semantic metadata, then employs two-stage retrieval achieving 0.52s prompt generation. Evaluation using LLM-as-judge and RAGAs metrics shows our approach achieves parity with traditional RAG on GPT-4 while significantly outperforming it on smaller models (GPT-3.5, GPT-3), suggesting particular value for resource-constrained deployments. Beyond dialogue, the structured memory enables novel visualization tools: spatiotemporal heatmaps, emotional trajectory analysis, and interactive path tracking, positioning the system as both a dialogue interface and research tool for biographical analysis. We use Van Gogh as a test case, but the architecture is generalizable to any historical figure with substantial textual records, offering a practical framework for educational, museum, and research applications requiring both accuracy and efficiency

</details>


### [4] [iMAD: Intelligent Multi-Agent Debate for Efficient and Accurate LLM Inference](https://arxiv.org/abs/2511.11306)
*Wei Fan,JinYi Yoon,Bo Ji*

Main category: cs.CL

TL;DR: 本文提出了智能多代理辩论框架（iMAD），通过选择性地触发多代理辩论来提高推理准确性并减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统多代理辩论框架虽能提升复杂任务的推理能力，但每次查询都触发辩论导致计算资源浪费且可能降低正确答案的准确率。

Method: iMAD首先让单一代理生成结构化的自我批评回应，提取41个语言和语义特征，利用轻量级的辩论决策分类器（采用FocusCal损失训练）判断是否触发辩论，从而实现高效且准确的决策。

Result: 在六个（视觉）问答数据集和五个竞争基线的对比实验中，iMAD显著减少了多达92%的令牌使用，同时最终答案准确率提升了最多13.5%。

Conclusion: iMAD通过智能选择性地触发多代理辩论，成功实现了推理准确性和计算效率的同步提升，具有广泛应用潜力。

Abstract: Large Language Model (LLM) agent systems have advanced rapidly, driven by their strong generalization in zero-shot settings. To further enhance reasoning and accuracy on complex tasks, Multi-Agent Debate (MAD) has emerged as a promising framework that engages multiple LLM agents in structured debates to encourage diverse reasoning. However, triggering MAD for every query is inefficient, as it incurs substantial computational (token) cost and may even degrade accuracy by overturning correct single-agent answers. To address these limitations, we propose intelligent Multi-Agent Debate (iMAD), a token-efficient framework that selectively triggers MAD only when it is likely to be beneficial (i.e., correcting an initially wrong answer). To achieve this goal, iMAD learns generalizable model behaviors to make accurate debate decisions. Specifically, iMAD first prompts a single agent to produce a structured self-critique response, from which we extract 41 interpretable linguistic and semantic features capturing hesitation cues. Then, iMAD uses a lightweight debate-decision classifier, trained using our proposed FocusCal loss, to determine whether to trigger MAD, enabling robust debate decisions without test dataset-specific tuning. Through extensive experiments using six (visual) question answering datasets against five competitive baselines, we have shown that iMAD significantly reduces token usage (by up to 92%) while also improving final answer accuracy (by up to 13.5%).

</details>


### [5] [Hybrid Quantum Transformer for Language Generation](https://arxiv.org/abs/2511.10653)
*Desheng Kong,Xiangshuo Cui,Jiaying Jin,Jing Xu,Donglin Wang*

Main category: cs.CL

TL;DR: 本文提出了首个用于自然语言生成的混合量子-经典大型语言模型HyQuT，集成了变分量子电路，表现出与经典模型相当的生成质量和稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前量子计算及混合模型多局限于简单任务，尚无成功应用于大规模自然语言生成。

Method: 在Transformer框架中引入变分量子电路，分别在800万和1.5亿参数规模上进行实现和测试。

Result: 使用10个量子比特和80个量子门，替代了150M参数模型中约10%的经典参数，保持了收敛稳定性和生成质量。

Conclusion: 该研究首次展示了量子计算与大规模生成式语言模型结合的可行性，开拓了量子-经典混合模型在自然语言处理中的应用前景。

Abstract: Although quantum computing has been increasingly applied to replace classical computation, most existing quantum or hybrid models remain confined to simple tasks, with no successful application to large-scale natural language generation to date. In this work, we present the first hybrid quantum-classical large language model (LLM) for natural language generation, HyQuT, capable of performing coherent and context-aware dialogue. The proposed architecture integrates variational quantum circuits (VQCs) into the Transformer framework at both 8M and 150M parameter scales. Experimental results show that a minimal number of qubits (10 qubits with 80 quantum gates) can replace about 10% of the classical parameters in the 150M-parameter model, while achieving comparable convergence stability and generation quality. This study provides an early demonstration of the feasibility of integrating quantum computing to large-scale generative language models.

</details>


### [6] [Empirical Characterization of Temporal Constraint Processing in LLMs](https://arxiv.org/abs/2511.10654)
*Javier Marín*

Main category: cs.CL

TL;DR: 本文研究了大规模语言模型（LLMs）在时间约束条件下的实时决策能力，发现其在识别截止时间上的表现不稳定且存在严重风险。


<details>
  <summary>Details</summary>
Motivation: 当前在时限性任务中部署LLMs时，普遍假设其能准确判断行动窗口是否关闭，但这一假设未被验证。本文旨在评估模型处理时间约束的能力及相关风险。

Method: 采用截止时间检测任务，测试八个规模为2.8B至8B参数的生产级模型，分析其准确率、对提示格式的敏感性和偏误表现，并尝试通过有限的合成样本微调提升性能。

Result: 模型表现二分化（准确率为95%或50%），对提示格式极度敏感，失败模型出现100%的误报率，参数量与能力无明显相关。微调可部分提升表现，但自然语言的下一个词预测难以可靠学习时间约束。

Conclusion: 时间约束的满足需模型具备连续时间状态表示、明确约束检测和系统的时间关系组合推理能力，当前自回归架构缺乏这些机制，且在时间关键应用中仅靠此类模型存在不可接受的风险，需引入符号推理等混合架构。

Abstract: When deploying LLMs in agentic architectures requiring real-time decisions under temporal constraints, we assume they reliably determine whether action windows remain open or have closed. This assumption is untested. We characterize temporal constraint processing across eight production-scale models (2.8-8B parameters) using deadline detection tasks, revealing systematic deployment risks: bimodal performance distribution (models achieve either 95% or 50% accuracy), extreme prompt brittleness (30-60 percentage point swings from formatting changes alone), and systematic action bias (100% false positive rates in failing models). Parameter count shows no correlation with capability in this range-a 3.8B model matches 7B models while other 7B models fail completely. Fine-tuning on 200 synthetic examples improves models with partial capability by 12-37 percentage points. We demonstrate that temporal constraint satisfaction cannot be reliably learned through next-token prediction on natural language, even with targeted fine-tuning. This capability requires architectural mechanisms for: (1) continuous temporal state representation, (2) explicit constraint checking separate from linguistic pattern matching, (3) systematic compositional reasoning over temporal relations. Current autoregressive architectures lack these mechanisms. Deploying such systems in time-critical applications without hybrid architectures incorporating symbolic reasoning modules represents unacceptable risk.

</details>


### [7] [Spectral Neuro-Symbolic Reasoning II: Semantic Node Merging, Entailment Filtering, and Knowledge Graph Alignment](https://arxiv.org/abs/2511.10655)
*Andrew Kiruluta,Priscilla Burity*

Main category: cs.CL

TL;DR: 本文通过引入基于语义的三项增强措施，提升了Spectral NSR框架的图谱质量，实现了更高的准确率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了提升图谱的准确性与鲁棒性，解决谱推理中图结构冗余和信息缺失的问题。

Method: 引入基于Transformer的节点合并，利用预训练NLI分类器进行句子级蕴含验证，以及结合外部知识图谱以补充上下文信息，这些步骤均发生在谱推理之前。

Result: 在多个基准测试（ProofWriter、EntailmentBank、CLUTRR）上准确率提升最高达3.8%，增强了对抗性测试的泛化能力和推理噪声控制。

Conclusion: 通过模块化、语义驱动的预处理增强Spectral NSR框架，实现了高效、可解释且可扩展的谱推理系统，适用于开放领域和现实场景。

Abstract: This report extends the Spectral Neuro-Symbolic Reasoning (Spectral NSR) framework by introducing three semantically grounded enhancements: (1) transformer-based node merging using contextual embeddings (e.g., Sentence-BERT, SimCSE) to reduce redundancy, (2) sentence-level entailment validation with pretrained NLI classifiers (e.g., RoBERTa, DeBERTa) to improve edge quality, and (3) alignment with external knowledge graphs (e.g., ConceptNet, Wikidata) to augment missing context. These modifications enhance graph fidelity while preserving the core spectral reasoning pipeline. Experimental results on ProofWriter, EntailmentBank, and CLUTRR benchmarks show consistent accuracy gains (up to +3.8\%), improved generalization to adversarial cases, and reduced inference noise. The novelty lies in performing semantic and symbolic refinement entirely upstream of the spectral inference stage, enabling efficient, interpretable, and scalable reasoning without relying on quadratic attention mechanisms. In summary, this work extends the Spectral NSR framework with modular, semantically grounded preprocessing steps that improve graph quality without altering the core spectral reasoning engine. The result is a more robust, interpretable, and scalable reasoning system suitable for deployment in open-domain and real-world settings.

</details>


### [8] [Preference Orchestrator: Prompt-Aware Multi-Objective Alignment for Large Language Models](https://arxiv.org/abs/2511.10656)
*Biao Liu,Ning Xu,Junming Yang,Xin Geng*

Main category: cs.CL

TL;DR: 本文提出了一个名为PRO的框架，通过轻量级偏好适配器自动推断提示特定的偏好权重，从而改进多目标对齐中的偏好权重指定问题，实现更优训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有多目标对齐方法需要用户手动指定偏好权重，这不仅增加了用户负担，也导致训练效率低下，因为会探索到无关的偏好组合。

Method: 提出一个轻量级的偏好适配器，基于多个奖励模型的归一化奖励分数自动学习和推断每个提示的偏好权重，并在训练和部署阶段应用。

Result: 理论分析和多任务实验表明，PRO方法在多目标对齐场景中性能优于固定偏好权重的方法。

Conclusion: PRO框架有效解决了多目标对齐中的偏好权重指定难题，提升了训练效率和模型表现，具有较好的实用价值。

Abstract: While Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse natural language processing tasks, aligning these models with varying human preferences across multiple objectives remains a significant challenge in practical deployments. Existing multi-objective alignment methods rely on manually specified preference weights, which not only burden users with difficult preference specification tasks but also lead to suboptimal training efficiency due to exploration of irrelevant preference combinations. To alleviate these issues, we propose a novel framework named PRO, i.e., PReference Orchestrator, which features a lightweight preference adapter that automatically infers prompt-specific preference weights during both training and deployment phases. Specifically, the adapter automatically learns appropriate preference weights for each prompt by training on normalized reward scores from multiple reward models for preferred responses, which inherently reflect effective preference balances across objectives. Additionally, We provide theoretical analysis proving that our prompt-aware preference mechanism achieves superior performance compared to fixed preference weights in multi-objective alignment scenarios. Extensive experiments across multiple tasks demonstrate the effectiveness of our method over existing multi-objective alignment approaches.

</details>


### [9] [Patent Representation Learning via Self-supervision](https://arxiv.org/abs/2511.10657)
*You Zuo,Kim Gerdes,Eric Villemonte de La Clergerie,Benoît Sagot*

Main category: cs.CL

TL;DR: 本文提出了一种利用专利文档内部多视角进行对比学习的专利嵌入方法，通过章节级增强解决了SimCSE样式增强导致的语义一致性下降问题，显著提升了专利检索与分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有SimCSE风格的dropout增强在专利嵌入学习中出现过于均匀，语义凝聚力不足的问题。

Method: 提出基于专利不同章节（摘要、权利要求、背景等）的增强方法，利用这些互补视角引入自然的语义和结构多样性，缓解过度扩散，提升嵌入的语义保存能力。

Result: 该全自监督方法在大规模基准测试中，专利近似检索和分类任务上匹配或超越了依赖引用与IPC监督的基线方法，并且不依赖脆弱或不完整的标注。不同章节的嵌入对不同任务表现出专门化，权利要求和摘要提高检索效果，背景部分有助于分类。

Conclusion: 利用专利文档内部不同章节视角进行对比学习，不仅提高了嵌入的语义表达能力，也增强了模型的通用性和可扩展性，充分利用了专利的内在语篇结构。

Abstract: This paper presents a simple yet effective contrastive learning framework for learning patent embeddings by leveraging multiple views from within the same document. We first identify a patent-specific failure mode of SimCSE style dropout augmentation: it produces overly uniform embeddings that lose semantic cohesion. To remedy this, we propose section-based augmentation, where different sections of a patent (e.g., abstract, claims, background) serve as complementary views. This design introduces natural semantic and structural diversity, mitigating over-dispersion and yielding embeddings that better preserve both global structure and local continuity. On large-scale benchmarks, our fully self-supervised method matches or surpasses citation-and IPC-supervised baselines in prior-art retrieval and classification, while avoiding reliance on brittle or incomplete annotations. Our analysis further shows that different sections specialize for different tasks-claims and summaries benefit retrieval, while background sections aid classification-highlighting the value of patents' inherent discourse structure for representation learning. These results highlight the value of exploiting intra-document views for scalable and generalizable patent understanding.

</details>


### [10] [Evaluating Open-Weight Large Language Models for Structured Data Extraction from Narrative Medical Reports Across Multiple Use Cases and Languages](https://arxiv.org/abs/2511.10658)
*Douwe J. Spaanderman,Karthik Prathaban,Petr Zelina,Kaouther Mouheb,Lukáš Hejtmánek,Matthew Marzetti,Antonius W. Schurink,Damian Chan,Ruben Niemantsverdriet,Frederik Hartmann,Zhen Qian,Maarten G. J. Thomeer,Petr Holub,Farhan Akram,Frank J. Wolters,Meike W. Vernooij,Cornelis Verhoef,Esther E. Bron,Vít Nováček,Dirk J. Grünhagen,Wiro J. Niessen,Martijn P. A. Starmans,Stefan Klein*

Main category: cs.CL

TL;DR: 本文评估了15个开源大语言模型在六类临床报告中的结构化信息提取表现，覆盖不同疾病、语言和机构；结果显示中小型通用模型表现优异，提示图和少样本提示提升约13%，任务复杂度和标注变异性是主要影响因素。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注单一任务、有限模型和英语报告，缺乏对多语言、多任务、多机构临床报告的系统评估。

Method: 评估15个开源大语言模型，包括通用和医疗专业模型，比较六种提示策略（零样本、一样本、少样本、链式思维、自我一致性、提示图），任务涵盖六类疾病，数据来自荷兰、英国和捷克三机构。采用合适指标评估性能，并用共识排序和线性混合效应模型分析变异。

Result: 顶尖模型的宏平均得分接近人工评分一致性，中小型通用模型表现与大型模型相当，微型和专业模型表现较差。提示图和少样本提示提升性能约13%。任务特异性因素（如复杂度和标注差异）对结果影响大于模型大小或提示策略。

Conclusion: 开源大语言模型能有效从多病种、多语言、多机构的临床报告中提取结构化数据，提供了一种可扩展的临床数据整理方案。

Abstract: Large language models (LLMs) are increasingly used to extract structured information from free-text clinical records, but prior work often focuses on single tasks, limited models, and English-language reports. We evaluated 15 open-weight LLMs on pathology and radiology reports across six use cases, colorectal liver metastases, liver tumours, neurodegenerative diseases, soft-tissue tumours, melanomas, and sarcomas, at three institutes in the Netherlands, UK, and Czech Republic. Models included general-purpose and medical-specialised LLMs of various sizes, and six prompting strategies were compared: zero-shot, one-shot, few-shot, chain-of-thought, self-consistency, and prompt graph. Performance was assessed using task-appropriate metrics, with consensus rank aggregation and linear mixed-effects models quantifying variance. Top-ranked models achieved macro-average scores close to inter-rater agreement across tasks. Small-to-medium general-purpose models performed comparably to large models, while tiny and specialised models performed worse. Prompt graph and few-shot prompting improved performance by ~13%. Task-specific factors, including variable complexity and annotation variability, influenced results more than model size or prompting strategy. These findings show that open-weight LLMs can extract structured data from clinical reports across diseases, languages, and institutions, offering a scalable approach for clinical data curation.

</details>


### [11] [Information Extraction From Fiscal Documents Using LLMs](https://arxiv.org/abs/2511.10659)
*Vikram Aggarwal,Jay Kulkarni,Aditi Mascarenhas,Aakriti Narang,Siddarth Raman,Ajay Shah,Susan Thomas*

Main category: cs.CL

TL;DR: 该论文提出了一种基于大型语言模型（LLMs）的方法，用于从复杂的多页政府财政文件中提取结构化表格数据，实现高准确率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在文本理解上表现出色，但在处理复杂的层次化表格数据方面仍未充分开发，尤其是面向政府财政文件这类多页结构复杂的文档。

Method: 作者设计了一个多阶段流水线，结合领域知识、序列上下文和算法验证，利用财政表格中层次结构的总计关系，实施多层次验证检查以确保数字提取的准确性。

Result: 应用于印度卡纳塔克邦年财政报告（超过200页），该方法实现了高精度的结构化数据提取，能够有效地处理表格和文档的结构层次关系。

Conclusion: 研究表明，LLMs能够阅读复杂表格并处理文档特有的层次结构，提供了一种可扩展的PDF财政数据转换为研究数据库的方案，具有在发展中国家广泛应用的潜力。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in text comprehension, but their ability to process complex, hierarchical tabular data remains underexplored. We present a novel approach to extracting structured data from multi-page government fiscal documents using LLM-based techniques. Applied to annual fiscal documents from the State of Karnataka in India (200+ pages), our method achieves high accuracy through a multi-stage pipeline that leverages domain knowledge, sequential context, and algorithmic validation. A large challenge with traditional OCR methods is the inability to verify the accurate extraction of numbers. When applied to fiscal data, the inherent structure of fiscal tables, with totals at each level of the hierarchy, allows for robust internal validation of the extracted data. We use these hierarchical relationships to create multi-level validation checks. We demonstrate that LLMs can read tables and also process document-specific structural hierarchies, offering a scalable process for converting PDF-based fiscal disclosures into research-ready databases. Our implementation shows promise for broader applications across developing country contexts.

</details>


### [12] [From Proof to Program: Characterizing Tool-Induced Reasoning Hallucinations in Large Language Models](https://arxiv.org/abs/2511.10899)
*Farima Fatahi Bayat,Pouya Pezeshkpour,Estevam Hruschka*

Main category: cs.CL

TL;DR: 本文研究了工具增强型语言模型（TaLMs）在调用代码解释器工具时出现的工具诱发的短视（TIM）问题，即模型虽然能正确调用工具，但依赖工具输出代替推理，导致推理质量下降。


<details>
  <summary>Details</summary>
Motivation: 当前TaLMs虽然提升了答题准确率，但其推理过程是否可信尚未明确，尤其是在调用外部工具后，推理合理性可能受到影响。

Method: 通过设计包含1679道竞赛级数学题的PYMATH基准测试，比较TaLMs与未使用工具的模型推理表现，测量推理质量下降程度，并提出基于偏好优化的框架改善推理深度和准确率。

Result: TaLMs在最终答案准确率上提升最高达19.3个百分点，但推理过程表现持续恶化，调用工具频率越高推理一致性越差，错误类型从算术错误转向逻辑、假设及创造力缺失，约55%的高风险案例中存在TIM。

Conclusion: 通过偏好优化引导TaLMs将工具视为辅助证据，能同时提升最终答案准确率和推理合理性，有效缓解TIM问题。代码和数据已开源。

Abstract: Tool-augmented Language Models (TaLMs) can invoke external tools to solve problems beyond their parametric capacity. However, it remains unclear whether these tool-enabled gains reflect trustworthy reasoning. Focusing on the Code Interpreter tool, we show that even when tools are selected and executed correctly, TaLMs treat tool outputs as substitutes for reasoning, producing solutions that appear correct but lack coherent justification. We term this failure mode Tool-Induced Myopia (TIM), and study it using PYMATH, a benchmark of 1,679 competition-level mathematical problems for which Python code is helpful but not sufficient. We further develop a multi-dimensional evaluation suite to quantify reasoning degradation in TaLMs relative to their non-tool counterparts. Our findings reveal that while TaLMs achieve up to a 19.3 percentage point gain in final-answer accuracy, their reasoning behavior consistently deteriorates (e.g., non-tool LLMs win up to 41.5% more often in pairwise comparisons of the reasoning process). This degradation intensifies with tool use; the more frequently a model invokes tools, the less coherent its reasoning becomes. Moreover, tool use shifts errors from arithmetic mistakes toward global reasoning failures (logic, assumption, creativity); with TIM present in ~55% of high-risk cases. Finally, we propose a preference-optimization-based framework that realigns TaLMs to use tools as assistive evidence, improving both final-answer accuracy and reasoning depth under tool use. Codes and data are available at: https://github.com/megagonlabs/TIM.

</details>


### [13] [Test-Time Steering for Lossless Text Compression via Weighted Product of Experts](https://arxiv.org/abs/2511.10660)
*Qihang Zhang,Muchen Li,Ziao Wang,Renjie Liao,Lele Wang*

Main category: cs.CL

TL;DR: 本文提出了一种在测试时通过加权专家模型组合来改进文本无损压缩的方法，结合传统通用压缩模型和预训练神经语言模型，实现更优压缩率，无需微调，适应多种数据分布。


<details>
  <summary>Details</summary>
Motivation: 传统gzip等通用压缩器速度快但压缩率较低，而基于神经网络的压缩器虽然压缩率高但泛化能力差，难以在未见过的数据上表现良好。

Method: 通过加权专家模型（Weighted Product of Experts，wPoE）在推理时自适应地组合通用压缩模型与预训练神经语言模型，保证压缩率至少不逊于单一模型。

Result: 大量实验表明该方法无需微调即可显著提升文本压缩性能，并能够无缝结合任何自回归语言模型。

Conclusion: 本文提出的测试时加权专家框架为提高文本压缩效果提供了切实可行且通用的解决方案，兼顾了传统与神经压缩模型的优势。

Abstract: Lossless compression techniques are crucial in an era of rapidly growing data. Traditional universal compressors like gzip offer low computational overhead, high speed, and broad applicability across data distributions. However, they often lead to worse compression rates than modern neural compressors, which leverage large-scale training data to model data distributions more effectively. Despite their advantages, neural compressors struggle to generalize to unseen data. To address this limitation, we propose a novel framework that performs Test-Time Steering via a Weighted Product of Experts (wPoE). At inference, our method adaptively combines a universal compression model with a pretrained neural language model, ensuring the compression rate is at least as good as that of the best individual model. Extensive experiments demonstrate that our approach improves the performance of text compression without requiring fine-tuning. Furthermore, it seamlessly integrates with any autoregressive language model, providing a practical solution for enhancing text compression across diverse data distributions.

</details>


### [14] [Automata-Based Steering of Large Language Models for Diverse Structured Generation](https://arxiv.org/abs/2511.11018)
*Xiaokun Luan,Zeming Wei,Yihao Zhang,Meng Sun*

Main category: cs.CL

TL;DR: 提出一种利用自动机遍历历史来增强大语言模型结构生成多样性的方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动机结构生成方法虽然保证生成有效性，但缺乏输出多样性，限制了其应用。

Method: 利用自动机遍历历史信息引导大语言模型生成新的结构模式，以提高多样性。

Result: 实验结果显示该方法显著提升了结构和内容的多样性，同时生成效率保持相当水平。

Conclusion: 该方法有效提升了自动机结构生成的多样性，有助于生成多样化的测试用例，增强实际应用效果。

Abstract: Large language models (LLMs) are increasingly tasked with generating structured outputs. While structured generation methods ensure validity, they often lack output diversity, a critical limitation that we confirm in our preliminary study. We propose a novel method to enhance diversity in automaton-based structured generation. Our approach utilizes automata traversal history to steer LLMs towards novel structural patterns. Evaluations show our method significantly improves structural and content diversity while maintaining comparable generation efficiency. Furthermore, we conduct a case study showcasing the effectiveness of our method in generating diverse test cases for testing open-source libraries.

</details>


### [15] [Bayesian Evaluation of Large Language Model Behavior](https://arxiv.org/abs/2511.10661)
*Rachel Longjohn,Shang Wu,Saatvik Kher,Catarina Belém,Padhraic Smyth*

Main category: cs.CL

TL;DR: 本文提出了一种贝叶斯方法，用于量化大语言模型(LLM)文本生成系统在二元评价指标中的统计不确定性，特别针对概率性文本生成策略引入的评估不确定性。


<details>
  <summary>Details</summary>
Motivation: 当前评估LLM文本生成系统的二元指标通常忽视统计不确定性，影响对模型行为的准确理解和比较。

Method: 基于贝叶斯统计框架，针对LLM生成的文本输出的二元评价，定量分析不确定性，结合两个案例研究验证方法有效性。

Result: 案例研究分别在有害响应拒绝率和模型之间的偏好对比中，展示贝叶斯方法能够有效捕捉相关评估指标的统计不确定性。

Conclusion: 贝叶斯不确定性量化方法能为LLM文本生成系统的行为评估提供更加可靠和细致的统计分析支持。

Abstract: It is increasingly important to evaluate how text generation systems based on large language models (LLMs) behave, such as their tendency to produce harmful output or their sensitivity to adversarial inputs. Such evaluations often rely on a curated benchmark set of input prompts provided to the LLM, where the output for each prompt may be assessed in a binary fashion (e.g., harmful/non-harmful or does not leak/leaks sensitive information), and the aggregation of binary scores is used to evaluate the LLM. However, existing approaches to evaluation often neglect statistical uncertainty quantification. With an applied statistics audience in mind, we provide background on LLM text generation and evaluation, and then describe a Bayesian approach for quantifying uncertainty in binary evaluation metrics. We focus in particular on uncertainty that is induced by the probabilistic text generation strategies typically deployed in LLM-based systems. We present two case studies applying this approach: 1) evaluating refusal rates on a benchmark of adversarial inputs designed to elicit harmful responses, and 2) evaluating pairwise preferences of one LLM over another on a benchmark of open-ended interactive dialogue examples. We demonstrate how the Bayesian approach can provide useful uncertainty quantification about the behavior of LLM-based systems.

</details>


### [16] [Evaluating Modern Large Language Models on Low-Resource and Morphologically Rich Languages:A Cross-Lingual Benchmark Across Cantonese, Japanese, and Turkish](https://arxiv.org/abs/2511.10664)
*Chengxuan Xia,Qianye Wu,Hongbin Guan,Sixuan Tian,Yilun Hao,Xiaoyu Wu*

Main category: cs.CL

TL;DR: 本文评估了七种先进大语言模型在粤语、日语和土耳其语等低资源语言和形态丰富语言上的表现，结合人工和自动评价指标，发现虽大模型表现优异但在文化理解和形态学泛化方面仍有显著不足。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型在高资源语言如英语上的效果显著，但在低资源及形态复杂语言上的表现尚未充分研究，有必要通过跨语言基准进行全面评测。

Method: 构建涵盖粤语、日语和土耳其语的跨语言基准，包含开放域问答、文档摘要、英译X和文化对话四项任务，结合人工评估（流畅性、事实准确性、文化适宜性）与自动指标（BLEU、ROUGE）对七款先进模型进行系统评测。

Result: 最大规模的专有模型（GPT-4o、GPT-4、Claude 3.5）在多语言和多任务上表现领先，但在文化细节理解和形态学推广方面存在不足。GPT-4o跨语言能力强，Claude 3.5在知识推理准确性上表现良好。较小开源模型在流畅度和准确性上落后明显。

Conclusion: 虽然大型模型在多语言任务中表现优异，仍需提升文化敏感度和形态学适应能力。文章发布了详细数据和基准，助力未来开发更具文化意识和语言泛化能力的模型。

Abstract: Large language models (LLMs) have achieved impressive results in high-resource languages like English, yet their effectiveness in low-resource and morphologically rich languages remains underexplored. In this paper, we present a comprehensive evaluation of seven cutting-edge LLMs -- including GPT-4o, GPT-4, Claude~3.5~Sonnet, LLaMA~3.1, Mistral~Large~2, LLaMA-2~Chat~13B, and Mistral~7B~Instruct -- on a new cross-lingual benchmark covering \textbf{Cantonese, Japanese, and Turkish}. Our benchmark spans four diverse tasks: open-domain question answering, document summarization, English-to-X translation, and culturally grounded dialogue. We combine \textbf{human evaluations} (rating fluency, factual accuracy, and cultural appropriateness) with automated metrics (e.g., BLEU, ROUGE) to assess model performance.
  Our results reveal that while the largest proprietary models (GPT-4o, GPT-4, Claude~3.5) generally lead across languages and tasks, significant gaps persist in culturally nuanced understanding and morphological generalization. Notably, GPT-4o demonstrates robust multilingual performance even on cross-lingual tasks, and Claude~3.5~Sonnet achieves competitive accuracy on knowledge and reasoning benchmarks. However, all models struggle to some extent with the unique linguistic challenges of each language, such as Turkish agglutinative morphology and Cantonese colloquialisms. Smaller open-source models (LLaMA-2~13B, Mistral~7B) lag substantially in fluency and accuracy, highlighting the resource disparity. We provide detailed quantitative results, qualitative error analysis, and discuss implications for developing more culturally aware and linguistically generalizable LLMs. Our benchmark and evaluation data are released to foster reproducibility and further research.

</details>


### [17] [Guarding the Meaning: Self-Supervised Training for Semantic Robustness in Guard Models](https://arxiv.org/abs/2511.10665)
*Cristina Pinneri,Christos Louizos*

Main category: cs.CL

TL;DR: 本文提出了一种基于自监督的防护模型语义鲁棒性提升框架，通过对同义句集合进行偏斜感知聚合，提高模型的一致性和校准度，显著提升了模型的安全评分稳定性和准确率。


<details>
  <summary>Details</summary>
Motivation: 当前的防护模型对语义等价但表述不同的同义句敏感，导致安全评分波动大，缺乏语义基础的稳健性。

Method: 提出一种基于同义句集的自监督训练框架，采用新颖的偏斜感知聚合策略计算稳健目标，替代传统的均值或中位数聚合方法，以增强预测一致性。

Result: 在六款开源防护模型上实验，语义变异性降低约58%，基准准确率平均提升2.5%，并能泛化至未见过的风格变化；训练还显著提升了模型校准度，最多达40%。

Conclusion: 将语义一致性作为核心训练目标，可以显著提升防护模型的鲁棒性与可靠性，为构建更安全的语言模型防护机制提供了可扩展的解决方案。

Abstract: Guard models are a critical component of LLM safety, but their sensitivity to superficial linguistic variations remains a key vulnerability. We show that even meaning-preserving paraphrases can cause large fluctuations in safety scores, revealing a lack of semantic grounding. To address this, we introduce a practical, self-supervised framework for improving the semantic robustness of guard models. Our method leverages paraphrase sets to enforce prediction consistency using a novel, skew-aware aggregation strategy for robust target computation. Notably, we find that standard aggregation methods like mean and median can degrade safety, underscoring the need for skew-aware alternatives. We analyze six open-source guard models and show that our approach reduces semantic variability across paraphrases by ~58%, improves benchmark accuracy by ~2.5% on average, and generalizes to unseen stylistic variations. Intriguingly, we discover a bidirectional relationship between model calibration and consistency: our robustness training improves calibration by up to 40%, revealing a fundamental connection between these properties. These results highlight the value of treating semantic consistency as a first-class training objective and provide a scalable recipe for building more reliable guard models.

</details>


### [18] [Evaluating LLM Understanding via Structured Tabular Decision Simulations](https://arxiv.org/abs/2511.10667)
*Sichao Li,Xinyue Xu,Xiaomeng Li*

Main category: cs.CL

TL;DR: 本文提出了STaDS方法，旨在模拟专家级决策考试以评估大型语言模型的真正理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型虽然预测准确，但缺乏全局且一致的理解，无法保证真正依赖相关决策因子。

Method: 设计结构化表格决策模拟(STaDS)，结合题目理解、基于知识的预测和决策因子依赖三方面联合评估模型的理解力，在15个多样化决策场景下测试9个前沿模型。

Result: 发现多数模型难以在不同领域持续保持高准确率，且准确性并不总是伴随真实合理的决策依据，模型的表述理由与实际驱动预测因素常不匹配。

Conclusion: 当前评估方法应超越单纯准确率，建立全球层面的理解评估体系，以促进语言模型具备更深层次、专家级的理解能力。

Abstract: Large language models (LLMs) often achieve impressive predictive accuracy, yet correctness alone does not imply genuine understanding. True LLM understanding, analogous to human expertise, requires making consistent, well-founded decisions across multiple instances and diverse domains, relying on relevant and domain-grounded decision factors. We introduce Structured Tabular Decision Simulations (STaDS), a suite of expert-like decision settings that evaluate LLMs as if they were professionals undertaking structured decision ``exams''. In this context, understanding is defined as the ability to identify and rely on the correct decision factors, features that determine outcomes within a domain. STaDS jointly assesses understanding through: (i) question and instruction comprehension, (ii) knowledge-based prediction, and (iii) reliance on relevant decision factors. By analyzing 9 frontier LLMs across 15 diverse decision settings, we find that (a) most models struggle to achieve consistently strong accuracy across diverse domains; (b) models can be accurate yet globally unfaithful, and there are frequent mismatches between stated rationales and factors driving predictions. Our findings highlight the need for global-level understanding evaluation protocols and advocate for novel frameworks that go beyond accuracy to enhance LLMs' understanding ability.

</details>


### [19] [Forecasting Spoken Language Development in Children with Cochlear Implants Using Preimplantation MRI](https://arxiv.org/abs/2511.10669)
*Yanlin Wang,Di Yuan,Shani Dettman,Dawn Choo,Emily Shimeng Xu,Denise Thomas,Maura E Ryan,Patrick C M Wong,Nancy M Young*

Main category: cs.CL

TL;DR: 该论文利用深度迁移学习(DTL)模型预测双侧感音神经性耳聋儿童的人工耳蜗植入后语言发展，预测准确率显著优于传统机器学习模型。


<details>
  <summary>Details</summary>
Motivation: 传统预测方法如植入年龄和残余听力无法可靠预测人工耳蜗植入儿童的语言发展，需求更精确的预测模型。

Method: 该研究招募278名双侧感音神经性耳聋儿童，基于脑神经解剖特征，比较传统机器学习和基于双线性注意力融合策略的DTL模型在二分类预测任务中的表现。

Result: DTL模型在准确率(92.39%)、灵敏度(91.22%)、特异度(93.56%)及AUC(0.977)等指标显著优于传统机器学习。

Conclusion: 研究证明基于DTL的预测模型能更准确地预测人工耳蜗儿童语言发展，支持其在全球CI项目中的应用可行性。

Abstract: Cochlear implants (CI) significantly improve spoken language in children with severe-to-profound sensorineural hearing loss (SNHL), yet outcomes remain more variable than in children with normal hearing. This variability cannot be reliably predicted for individual children using age at implantation or residual hearing. This study aims to compare the accuracy of traditional machine learning (ML) to deep transfer learning (DTL) algorithms to predict post-CI spoken language development of children with bilateral SNHL using a binary classification model of high versus low language improvers. A total of 278 implanted children enrolled from three centers. The accuracy, sensitivity and specificity of prediction models based upon brain neuroanatomic features using traditional ML and DTL learning. DTL prediction models using bilinear attention-based fusion strategy achieved: accuracy of 92.39% (95% CI, 90.70%-94.07%), sensitivity of 91.22% (95% CI, 89.98%-92.47%), specificity of 93.56% (95% CI, 90.91%-96.21%), and area under the curve (AUC) of 0.977 (95% CI, 0.969-0.986). DTL outperformed traditional ML models in all outcome measures. DTL was significantly improved by direct capture of discriminative and task-specific information that are advantages of representation learning enabled by this approach over ML. The results support the feasibility of a single DTL prediction model for language prediction of children served by CI programs worldwide.

</details>


### [20] [Towards Fine-Grained Code-Switch Speech Translation with Semantic Space Alignment](https://arxiv.org/abs/2511.10670)
*Yan Gao,Yazheng Yang,Zhibin Lan,Yidong Chen,Min Zhang,Daimeng Wei,Hui Huang,Jinsong Su*

Main category: cs.CL

TL;DR: 论文提出了一种结合大型语言模型和专家混合投影器的多阶段训练方法，以解决代码切换语音翻译中语义建模复杂和数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 代码切换语音翻译因语义建模复杂且缺乏高质量代码切换数据，传统方法依赖模型隐式学习和昂贵的人工标注，效果有限。

Method: 引入专家混合（MoE）语音投影器，每个专家专注于特定语言子空间，通过多阶段训练利用单语自动语音识别和翻译数据，结合语言特异性损失、负载平衡损失和过渡损失，提升模型对代码切换场景的适应能力。

Result: 在多个公开数据集上的广泛实验证明了方法的有效性与通用性，显著提升了代码切换语音翻译的性能。

Conclusion: 所提方法通过精细化专家建模和多阶段训练，成功缓解了代码切换语音翻译中的语义复杂性和数据稀缺问题，提升了翻译效果和模型泛化能力。

Abstract: Code-switching (CS) speech translation (ST) refers to translating speech that alternates between two or more languages into a target language text, which poses significant challenges due to the complexity of semantic modeling and the scarcity of CS data. Previous studies tend to rely on the model itself to implicitly learn semantic modeling during training, and resort to inefficient and costly manual annotations for these two challenges. To mitigate these limitations, we propose enhancing Large Language Models (LLMs) with a Mixture of Experts (MoE) speech projector, where each expert specializes in the semantic subspace of a specific language, enabling fine-grained modeling of speech features. Additionally, we introduce a multi-stage training paradigm that utilizes readily available monolingual automatic speech recognition (ASR) and monolingual ST data, facilitating speech-text alignment and improving translation capabilities. During training, we leverage a combination of language-specific loss and intra-group load balancing loss to guide the MoE speech projector in efficiently allocating tokens to the appropriate experts, across expert groups and within each group, respectively. To bridge the data gap across different training stages and improve adaptation to the CS scenario, we further employ a transition loss, enabling smooth transitions of data between stages, to effectively address the scarcity of high-quality CS speech translation data. Extensive experiments on widely used datasets demonstrate the effectiveness and generality of our approach.

</details>


### [21] [Grounded Visual Factualization: Factual Anchor-Based Finetuning for Enhancing MLLM Factual Consistency](https://arxiv.org/abs/2511.10671)
*Filippo Morbiato,Luca Romano,Alessandro Persona*

Main category: cs.CL

TL;DR: 本文提出了一种名为GVF微调的新方法，通过引入事实锚点数据增强、事实感知指令微调和事实一致性损失，有效提升了多模态大语言模型的视觉事实一致性，显著减少了视觉幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有微调方法难以深入干预多模态大语言模型在视觉事实推理中的错误，导致模型生成与图像内容不符的虚假细节，严重影响模型可靠性。

Method: GVF微调引入三大核心机制：事实锚点数据增强，通过结构化事实锚点和反事实提示丰富训练数据；事实感知指令微调，将事实信号融入显式指令中；事实一致性损失函数，针对事实错误给予惩罚。

Result: 在LLaVA-1.5-13B模型上，GVF微调在VHTest基准的开放式问答和是非问答任务中显著优于标准微调，同时在MME和POPE等通用多模态基准上表现持平或略有提升，成功减少了视觉幻觉。

Conclusion: GVF微调方法有效提升了多模态大语言模型的视觉事实一致性，缓解了视觉幻觉问题，同时保持了模型的通用理解和推理能力，具有较好的实用价值。

Abstract: Visual hallucination, where Multimodal Large Language Models fabricate details inconsistent with image content, critically undermines their reliability. Existing fine-tuning methods offer limited improvement, failing to deeply intervene in factual reasoning. This paper introduces Grounded Visual Factualization (GVF) Finetuning, a novel approach to systematically enhance MLLM visual factual consistency. GVF integrates explicit factual signals via three core mechanisms: Factual Anchor Data Augmentation, enriching training data with structured factual anchors and counter-factual prompts; Fact-Aware Instruction Tuning, embedding these cues into explicit instructions; and a Factual Consistency Loss function, specifically penalizing factual inaccuracies. Evaluated on LLaVA-1.5-13B, GVF Finetuning significantly outperforms standard fine-tuning on the VHTest benchmark for both Open-Ended Question (OEQ) and Yes/No Question (YNQ) formats. Crucially, GVF maintains or even slightly improves performance on general multimodal benchmarks like MME and POPE, demonstrating effective mitigation of visual hallucinations without compromising general understanding and reasoning abilities.

</details>


### [22] [Large language models in materials science and the need for open-source approaches](https://arxiv.org/abs/2511.10673)
*Fengxu Yang,Weitong Chen,Jack D. Evans*

Main category: cs.CL

TL;DR: 本文综述了大语言模型（LLMs）在材料科学中的应用，涵盖文献挖掘、预测建模和实验系统协同三大领域，强调开放源代码模型的优势。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型如何助力材料科学的关键流程，提高信息提取、预测和实验自动化能力，并推动开放源代码模型的使用。

Method: 通过综述近期文献，分析LLMs在科学文献挖掘、结构-性质关系预测和多智能体实验系统中的应用，同时进行性能对比和基准测试。

Result: 开源模型在性能上可与闭源商用模型媲美，且在透明度、可重复性、成本及数据隐私保护方面表现更优。

Conclusion: 随着开源模型不断提升，建议推广其在材料科学中的应用，以建立开放、灵活和社区驱动的AI科学发现平台。

Abstract: Large language models (LLMs) are rapidly transforming materials science. This review examines recent LLM applications across the materials discovery pipeline, focusing on three key areas: mining scientific literature , predictive modelling, and multi-agent experimental systems. We highlight how LLMs extract valuable information such as synthesis conditions from text, learn structure-property relationships, and can coordinate agentic systems integrating computational tools and laboratory automation. While progress has been largely dependent on closed-source commercial models, our benchmark results demonstrate that open-source alternatives can match performance while offering greater transparency, reproducibility, cost-effectiveness, and data privacy. As open-source models continue to improve, we advocate their broader adoption to build accessible, flexible, and community-driven AI platforms for scientific discovery.

</details>


### [23] [Continual Learning of Domain Knowledge from Human Feedback in Text-to-SQL](https://arxiv.org/abs/2511.10674)
*Thomas Cook,Kelly Patel,Sivapriya Vellaichamy,Saba Rahimi,Zhen Zeng,Sumitra Ganesh*

Main category: cs.CL

TL;DR: 该论文提出了一种基于人类反馈的持续学习框架，用于提升文本转SQL系统在特定数据库领域的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型虽能从自然语言生成SQL，但难以处理数据库特定结构和隐性领域知识。

Method: 设计一个学习代理，利用自然语言反馈不断优化SQL查询，并将获取的知识蒸馏存储于结构化记忆中以供未来任务使用。

Result: 在BIRD基准测试中，具有记忆增强的代理，尤其是程序化代理，显著提升了执行准确率并降低错误率。

Conclusion: 将隐性人类专业知识转化为可复用知识对于构建能持续学习、适应领域的文本转SQL系统至关重要。

Abstract: Large Language Models (LLMs) can generate SQL queries from natural language questions but struggle with database-specific schemas and tacit domain knowledge. We introduce a framework for continual learning from human feedback in text-to-SQL, where a learning agent receives natural language feedback to refine queries and distills the revealed knowledge for reuse on future tasks. This distilled knowledge is stored in a structured memory, enabling the agent to improve execution accuracy over time. We design and evaluate multiple variations of a learning agent architecture that vary in how they capture and retrieve past experiences. Experiments on the BIRD benchmark Dev set show that memory-augmented agents, particularly the Procedural Agent, achieve significant accuracy gains and error reduction by leveraging human-in-the-loop feedback. Our results highlight the importance of transforming tacit human expertise into reusable knowledge, paving the way for more adaptive, domain-aware text-to-SQL systems that continually learn from a human-in-the-loop.

</details>


### [24] [Learn to Select: Exploring Label Distribution Divergence for In-Context Demonstration Selection in Text Classification](https://arxiv.org/abs/2511.10675)
*Ye Jiang,Taihang Wang,Youzheng Liu,Yimin Wang,Yuhan Xia,Yunfei Long*

Main category: cs.CL

TL;DR: 本文针对大语言模型中文本分类的示例选择问题，提出了一种结合语义相似度与标签分布差异的两阶段示例选择方法L2D，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有示例选择方法多关注测试输入与示例的语义相似度，忽视了标签分布的匹配，导致选择的示例可能在标签分布上不一致，影响大语言模型的分类效果。

Method: 提出基于微调的BERT小模型生成标签分布，并计算测试输入和示例之间的标签分布差异，通过TopK语义筛选加上标签分布差异度量的两阶段选择策略，实现示例的语义与标签分布双重对齐。

Result: 在七个文本分类基准测试中，该方法持续超越传统示例选择策略，展现了更优的性能；且标签分布预测准确度与大语言模型性能表现正相关。

Conclusion: 结合标签分布差异的示例选择策略显著提升了大语言模型在文本分类任务中的表现，表明标签分布对示例选择的重要性，未来可通过提升基础小模型的准确度进一步优化。

Abstract: In-context learning (ICL) for text classification, which uses a few input-label demonstrations to describe a task, has demonstrated impressive performance on large language models (LLMs). However, the selection of in-context demonstrations plays a crucial role and can significantly affect LLMs' performance. Most existing demonstration selection methods primarily focus on semantic similarity between test inputs and demonstrations, often overlooking the importance of label distribution alignment. To address this limitation, we propose a two-stage demonstration selection method, TopK + Label Distribution Divergence (L2D), which leverages a fine-tuned BERT-like small language model (SLM) to generate label distributions and calculate their divergence for both test inputs and candidate demonstrations. This enables the selection of demonstrations that are not only semantically similar but also aligned in label distribution with the test input. Extensive experiments across seven text classification benchmarks show that our method consistently outperforms previous demonstration selection strategies. Further analysis reveals a positive correlation between the performance of LLMs and the accuracy of the underlying SLMs used for label distribution estimation.

</details>


### [25] [Pre-Attention Expert Prediction and Prefetching for Mixture-of-Experts Large Language Models](https://arxiv.org/abs/2511.10676)
*Shien Zhu,Samuel Bohl,Robin Oester,Gustavo Alonso*

Main category: cs.CL

TL;DR: 提出一种基于预注意力的专家预测方法，实现轻量级且准确的专家预取，提高大语言模型中专家选择的准确率。


<details>
  <summary>Details</summary>
Motivation: 当前基于上一层激活的专家预测方法准确率低，且无法优化第一层，复杂模型带来高计算开销。

Method: 利用线性函数和排序感知损失对同层注意力块前的激活进行预测，实现准确且轻量的预注意力专家预测。

Result: 该方法在多个大语言模型上专家预测准确率达93.03%-97.62%，较现有方法提升约15%。

Conclusion: 预注意力专家路由器能够提升专家选择的准确性和效率，支持首层预取，优化了大语言模型中的MoE推理性能。

Abstract: Mixture-of-Experts (MoE) Large Language Models (LLMs) efficiently scale-up the model while keeping relatively low inference cost. As MoE models only activate part of the experts, related work has proposed expert prediction and caching methods to prefetch the experts for faster inference. However, existing approaches utilize the activations from the previous layer for prediction, incurring low accuracy and leave the first layer unoptimized. Applying complex layers or even training standalone networks for better prediction introduces high computation overhead. In this paper, we propose pre-attention expert prediction to achieve accurate and lightweight expert prefetching. The key insight is that some functions in LLMs are ranking-preserving, indicating that matching the ranking of selected experts using simple linear functions is possible. Therefore, we utilize the activations before the attention block in the same layer with 2 linear functions and ranking-aware loss to achieve accurate prediction, which also supports prefetching in the first layer. Our lightweight, pre-attention expert routers achieve 93.03% accuracy on DeepSeek V2 Lite, 94.69% on Qwen3-30B, and 97.62% on Phi-mini-MoE, showing about 15% improvement on absolute accuracy over the state-of-the-art methods.

</details>


### [26] [SpiderGen: Towards Procedure Generation For Carbon Life Cycle Assessments with Generative AI](https://arxiv.org/abs/2511.10684)
*Anupama Sitaraman,Bharathan Balaji,Yuvraj Agarwal*

Main category: cs.CL

TL;DR: 本论文提出了SpiderGen，一种基于大型语言模型（LLM）的生命周期评估（LCA）流程生成工具，用于估计消费品的环境影响。


<details>
  <summary>Details</summary>
Motivation: 全球变暖和温室气体排放对环境的影响日益严重，评估消费品的生命周期环境影响非常重要，但传统LCA耗时耗资大。

Method: 结合传统LCA的分类学和方法论，利用LLM的推理能力和知识生成LCA流程信息，并通过真实LCA文档验证其准确性。

Result: SpiderGen在10个样本数据上的F1分数为62%，输出的LCA流程信息大部分准确，优于链式思维和一次性提示等基线方法。

Conclusion: SpiderGen能够大幅降低LCA评估的人力和费用，在10分钟内成本低于1美元，相比传统方法节约显著，具备实际应用潜力。

Abstract: Investigating the effects of climate change and global warming caused by GHG emissions have been a primary concern worldwide. These emissions are largely contributed to by the production, use and disposal of consumer products. Thus, it is important to build tools to estimate the environmental impact of consumer goods, an essential part of which is conducting Life Cycle Assessments (LCAs). LCAs specify and account for the appropriate processes involved with the production, use, and disposal of the products. We present SpiderGen, an LLM-based workflow which integrates the taxonomy and methodology of traditional LCA with the reasoning capabilities and world knowledge of LLMs to generate the procedural information used for LCA. We additionally evaluate the output of SpiderGen using real-world LCA documents as ground-truth. We find that SpiderGen provides accurate LCA process information that is either fully correct or has minor errors, achieving an F1-Score of 62% across 10 sample data points. We observe that the remaining missed processes and hallucinated errors occur primarily due to differences in detail between LCA documents, as well as differences in the "scope" of which auxiliary processes must also be included. We also demonstrate that SpiderGen performs better than several baselines techniques, such as chain-of-thought prompting and one-shot prompting. Finally, we highlight SpiderGen's potential to reduce the human effort and costs for estimating carbon impact, as it is able to produce LCA process information for less than \$1 USD in under 10 minutes as compared to the status quo LCA, which can cost over \$25000 USD and take up to 21-person days.

</details>


### [27] [A methodological analysis of prompt perturbations and their effect on attack success rates](https://arxiv.org/abs/2511.10686)
*Tiago Machado,Maysa Malfiza Garcia de Macedo,Rogerio Abreu de Paula,Marcelo Carpinette Grave,Aminat Adebiyi,Luan Soares de Souza,Enrico Santarelli,Claudio Pinhanez*

Main category: cs.CL

TL;DR: 该论文研究了不同大型语言模型对齐方法对模型应对提示攻击的影响，发现提示的细微变化会显著影响攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 探究不同对齐方法下模型对提示攻击的敏感度，以评估模型安全性和对齐方法的有效性。

Method: 选用三种主流对齐方法的开源模型（SFT、DPO、RLHF），通过统计方法系统分析不同提示变化对攻击成功率的影响。

Result: 发现即使是细微的提示修改，也会显著改变攻击成功率，不同模型和对齐方法对提示变化的敏感度存在较大差异。并且现有攻击基准测试不足以覆盖所有模型和对齐方法的潜在漏洞。

Conclusion: 通过系统且基于统计的分析验证了不同对齐方法对应的模型对提示攻击的敏感度，指出需改进攻击评估方法以全面挖掘模型脆弱性。

Abstract: This work aims to investigate how different Large Language Models (LLMs) alignment methods affect the models' responses to prompt attacks. We selected open source models based on the most common alignment methods, namely, Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Reinforcement Learning with Human Feedback (RLHF). We conducted a systematic analysis using statistical methods to verify how sensitive the Attack Success Rate (ASR) is when we apply variations to prompts designed to elicit inappropriate content from LLMs. Our results show that even small prompt modifications can significantly change the Attack Success Rate (ASR) according to the statistical tests we run, making the models more or less susceptible to types of attack. Critically, our results demonstrate that running existing 'attack benchmarks' alone may not be sufficient to elicit all possible vulnerabilities of both models and alignment methods. This paper thus contributes to ongoing efforts on model attack evaluation by means of systematic and statistically-based analyses of the different alignment methods and how sensitive their ASR is to prompt variation.

</details>


### [28] [Modeling and Predicting Multi-Turn Answer Instability in Large Language Models](https://arxiv.org/abs/2511.10688)
*Jiahang He,Rishi Ramachandran,Neel Ramachandran,Aryan Katakam,Kevin Zhu,Sunishchal Dev,Ashwinee Panda,Aryan Shrivastava*

Main category: cs.CL

TL;DR: 本文通过多轮跟进提示评估大语言模型（LLMs）的回答稳定性，发现模型在多轮提问中准确率显著下降，并用马尔可夫链建模准确率动态，以预测模型长期准确率。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs应用范围不断扩大，用户与模型的交互频率和规模增加，评估模型的鲁棒性变得尤为重要。

Method: 采用简单的多轮跟进提示，分析模型回答变化和多轮准确率动态，利用马尔可夫链建模准确率，并用线性探针预测未来回答变化。

Result: 发现简单的“再想想”提示会导致Gemini 1.5 Flash模型准确率下降约10%，结合语义等价的重述问题，Claude 3.5 Haiku准确率下降7.5%；马尔可夫链成功预测准确率变化，长期准确率比初次提问低约8%；线性探针能预测答案变化。

Conclusion: 多轮提问中模型表现存在显著脆弱性，提出静态准确率作为交互场景的鲁棒性衡量指标，强调提升模型稳定性对于高风险和交互式应用至关重要。

Abstract: As large language models (LLMs) are adopted in an increasingly wide range of applications, user-model interactions have grown in both frequency and scale. Consequently, research has focused on evaluating the robustness of LLMs, an essential quality for real-world tasks. In this paper, we employ simple multi-turn follow-up prompts to evaluate models' answer changes, model accuracy dynamics across turns with Markov chains, and examine whether linear probes can predict these changes. Our results show significant vulnerabilities in LLM robustness: a simple "Think again" prompt led to an approximate 10% accuracy drop for Gemini 1.5 Flash over nine turns, while combining this prompt with a semantically equivalent reworded question caused a 7.5% drop for Claude 3.5 Haiku. Additionally, we find that model accuracy across turns can be effectively modeled using Markov chains, enabling the prediction of accuracy probabilities over time. This allows for estimation of the model's stationary (long-run) accuracy, which we find to be on average approximately 8% lower than its first-turn accuracy for Gemini 1.5 Flash. Our results from a model's hidden states also reveal evidence that linear probes can help predict future answer changes. Together, these results establish stationary accuracy as a principled robustness metric for interactive settings and expose the fragility of models under repeated questioning. Addressing this instability will be essential for deploying LLMs in high-stakes and interactive settings where consistent reasoning is as important as initial accuracy.

</details>


### [29] [Equilibrium Dynamics and Mitigation of Gender Bias in Synthetically Generated Data](https://arxiv.org/abs/2511.10689)
*Ashish Kattamuri,Arpita Vats,Harshwardhan Fartale,Rahul Raja,Akshata Kishore Moharir,Ishita Prasad*

Main category: cs.CL

TL;DR: 本文研究了递归提示（recursive prompting）在大语言模型中生成合成数据时的性别偏见动态，发现偏见有趋于模型固有水平的平衡动态，而非简单放大；不同缓解策略中，性别对比增强效果最佳，但语义相似度指标与行为公平结果不一致。


<details>
  <summary>Details</summary>
Motivation: 递归提示生成合成数据虽然可扩展，但可能加剧偏见，亟需深入理解偏见演变及有效缓解方法。

Method: 采用三种评价框架（规则匹配、嵌入语义相似度、下游任务表现）分析三代递归生成文本中不同初始偏见水平下的偏见变化及四种缓解策略效果。

Result: 发现偏见呈现向模型固有偏见水平趋近的平衡动态，性别对比增强方法显著降低下游任务偏见（低偏见初始降低98.8%），尽管嵌入偏见得分较高。

Conclusion: 偏见动态复杂，语义相似度指标可能与实际公平性不符，需多维度评价方法，支持负责任的合成数据生成。

Abstract: Recursive prompting with large language models enables scalable synthetic dataset generation but introduces the risk of bias amplification. We investigate gender bias dynamics across three generations of recursive text generation using three complementary evaluation frameworks: rule-based pattern matching, embedding-based semantic similarity, and downstream task performance. Experiments with three initial bias levels (0.1, 0.3, 0.6) and four mitigation strategies reveal equilibrium dynamics rather than monotonic amplification. The low initial bias amplifies toward the model's inherent bias level (+36%), whereas the high initial bias decays toward it (-26%). Among mitigation methods, contrastive augmentation, which introduces gender-swapped variants, achieves significant downstream bias reduction (98.8% for low initial bias and 91% on average) despite producing higher embedding-based bias scores. This paradox demonstrates that semantic similarity metrics may diverge from behavioral fairness outcomes, highlighting the need for multidimensional evaluation in responsible synthetic data generation.

</details>


### [30] [Saying the Unsaid: Revealing the Hidden Language of Multimodal Systems Through Telephone Games](https://arxiv.org/abs/2511.10690)
*Juntu Zhao,Jialing Zhang,Chongxuan Li,Dequan Wang*

Main category: cs.CL

TL;DR: 本文通过多轮“传声筒”游戏研究多模态系统在图像-文本-图像转换过程中的偏好偏差，构建概念连接图谱，揭示系统隐藏语言。


<details>
  <summary>Details</summary>
Motivation: 当前多模态系统虽然进步显著，但其理解世界的隐藏语言因黑箱结构而难以解释，本文旨在通过分析系统偏好偏差揭示其隐藏语言。

Method: 采用多轮“传声筒”游戏，观察概念共现频率，定量分析多模态系统中概念连接强度，构建包含10,000+概念对的Telescope数据集，并利用推理型大语言模型挖掘超越文本和视觉相似性的隐藏概念关系。

Result: 成功构建了多模态系统理解的全球概念连接图，辨识出训练中遗传的偏好偏差，评估了泛化能力提升，发现了更稳定的脆弱概念连接路径，同时揭示了系统对世界的模拟和理解方式。

Conclusion: 本文提出了一种解析多模态系统隐藏语言的新视角，为多模态系统的可解释性和可控性研究奠定基础。

Abstract: Recent closed-source multimodal systems have made great advances, but their hidden language for understanding the world remains opaque because of their black-box architectures. In this paper, we use the systems' preference bias to study their hidden language: During the process of compressing the input images (typically containing multiple concepts) into texts and then reconstructing them into images, the systems' inherent preference bias introduces specific shifts in the outputs, disrupting the original input concept co-occurrence. We employ the multi-round "telephone game" to strategically leverage this bias. By observing the co-occurrence frequencies of concepts in telephone games, we quantitatively investigate the concept connection strength in the understanding of multimodal systems, i.e., "hidden language." We also contribute Telescope, a dataset of 10,000+ concept pairs, as the database of our telephone game framework. Our telephone game is test-time scalable: By iteratively running telephone games, we can construct a global map of concept connections in multimodal systems' understanding. Here we can identify preference bias inherited from training, assess generalization capability advancement, and discover more stable pathways for fragile concept connections. Furthermore, we use Reasoning-LLMs to uncover unexpected concept relationships that transcend textual and visual similarities, inferring how multimodal systems understand and simulate the world. This study offers a new perspective on the hidden language of multimodal systems and lays the foundation for future research on the interpretability and controllability of multimodal systems.

</details>


### [31] [Evaluating from Benign to Dynamic Adversarial: A Squid Game for Large Language Models](https://arxiv.org/abs/2511.10691)
*Zijian Chen,Wenjun Zhang,Guangtao Zhai*

Main category: cs.CL

TL;DR: 本文提出了一个名为Squid Game的动态对抗评估环境，旨在解决现有大语言模型（LLM）评测中的数据污染和资源设限问题。此环境通过多阶段淘汰赛，全面测试模型的指令遵循、代码编写、推理、规划及安全等能力。通过对50余种模型的测试，揭示了模型性能的代际跃迁及模型可能采用投机策略的问题，体现了动态评测对静态评测的补充价值。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评测基准难以应对快速发展的模型，存在数据污染问题且多假设资源充足，缺乏对资源受限和信息不对称情况下模型行为的考察，亟需一种更加动态和真实的评测方法。

Method: 提出了Squid Game动态对抗评测环境，该环境由六个淘汰赛级别组成，涵盖指令遵循、编码、推理、规划与安全调整等多方面能力；通过与其他LLM对战开展互动式游戏测试，模拟资源受限和非对称信息场景。

Result: 在Squid Game环境下评测了50多种LLM，发现模型在同一系列中存在明显的代际性能跃迁；同时，一些模型通过投机性捷径获胜，暗示静态评测可能被“污染”；通过相关性分析表明动态评测能有效补充静态评测。

Conclusion: Squid Game作为一种动态对抗式评测环境，有效弥补了传统静态评测的不足，对模型性能全面评估具有重要意义，未来将释放相关代码与数据以推动研究。

Abstract: Contemporary benchmarks are struggling to keep pace with the development of large language models (LLMs). Although they are indispensable to evaluate model performance on various tasks, it is uncertain whether the models trained on Internet data have genuinely learned how to solve problems or merely seen the questions before. This potential data contamination issue presents a fundamental challenge to establishing trustworthy evaluation frameworks. Meanwhile, existing benchmarks predominantly assume benign, resource-rich settings, leaving the behavior of LLMs under pressure unexplored. In this paper, we introduce Squid Game, a dynamic and adversarial evaluation environment with resource-constrained and asymmetric information settings elaborated to evaluate LLMs through interactive gameplay against other LLM opponents. Notably, Squid Game consists of six elimination-style levels, focusing on multi-faceted abilities, such as instruction-following, code, reasoning, planning, and safety alignment. We evaluate over 50 LLMs on Squid Game, presenting the largest behavioral evaluation study of general LLMs on dynamic adversarial scenarios. We observe a clear generational phase transition on performance in the same model lineage and find evidence that some models resort to speculative shortcuts to win the game, indicating the possibility of higher-level evaluation paradigm contamination in static benchmarks. Furthermore, we compare prominent LLM benchmarks and Squid Game with correlation analyses, highlighting that dynamic evaluation can serve as a complementary part for static evaluations. The code and data will be released in the future.

</details>


### [32] [Do AI Voices Learn Social Nuances? A Case of Politeness and Speech Rate](https://arxiv.org/abs/2511.10693)
*Eyal Rabin,Zohar Elyoseph,Rotem Israel-Fishelson,Adi Dali,Ravit Nussinson*

Main category: cs.CL

TL;DR: 研究表明先进的语音合成系统能够通过语速变化隐含地表达礼貌等社会情感特征。


<details>
  <summary>Details</summary>
Motivation: 探究语音AI是否能学习并体现人类非显式编程的社会交际暗示，尤其是礼貌时的语速减缓。

Method: 选择两个领先的AI平台（AI Studio和OpenAI）旗下22个合成声音，分别以“礼貌正式”和“随意非正式”两种情绪朗读同一文本，测量语音时长。

Result: 所有AI Studio声音以及大多数OpenAI声音在“礼貌”条件下语速明显减慢，效果显著且效应量大。

Conclusion: 语音AI能够隐式学习并复制人类沟通中的心理细微差别，显示其在强化人类社会规范中日益重要的社会角色。

Abstract: Voice-based artificial intelligence is increasingly expected to adhere to human social conventions, but can it learn implicit cues that are not explicitly programmed? This study investigates whether state-of-the-art text-to-speech systems have internalized the human tendency to reduce speech rate to convey politeness - a non-obvious prosodic marker. We prompted 22 synthetic voices from two leading AI platforms (AI Studio and OpenAI) to read a fixed script under both "polite and formal" and "casual and informal" conditions and measured the resulting speech duration. Across both AI platforms, the polite prompt produced slower speech than the casual prompt with very large effect sizes, an effect that was statistically significant for all of AI Studio's voices and for a large majority of OpenAI's voices. These results demonstrate that AI can implicitly learn and replicate psychological nuances of human communication, highlighting its emerging role as a social actor capable of reinforcing human social norms.

</details>


### [33] [Where does an LLM begin computing an instruction?](https://arxiv.org/abs/2511.10694)
*Aditya Pola,Vineeth N. Balasubramanian*

Main category: cs.CL

TL;DR: 本文通过设计多任务数据集和激活补丁技术，定位了大模型中执行指令的起始层。


<details>
  <summary>Details</summary>
Motivation: 探索变换器模型中执行指令的子过程何时从“理解”转变为“执行”，即指令执行的起始层位置。

Method: 提出三个简单任务数据集及其多跳组合，利用激活补丁（activation patching）和层级翻转率测量，判断替换残差激活后预测答案的变化，定位指令执行起点。

Result: 在Llama系列模型中发现一个明显的拐点（onset），在此之前的干预会影响预测结果，之后则效果显著降低；多跳组合任务也表现出类似的起始层位置。

Conclusion: 该方法为简单且可复现地定位指令执行起点提供了工具，便于比较不同任务和模型尺寸间的差异。

Abstract: Following an instruction involves distinct sub-processes, such as reading content, reading the instruction, executing it, and producing an answer. We ask where, along the layer stack, instruction following begins, the point where reading gives way to doing. We introduce three simple datasets (Key-Value, Quote Attribution, Letter Selection) and two hop compositions of these tasks. Using activation patching on minimal-contrast prompt pairs, we measure a layer-wise flip rate that indicates when substituting selected residual activations changes the predicted answer. Across models in the Llama family, we observe an inflection point, which we term onset, where interventions that change predictions before this point become largely ineffective afterward. Multi-hop compositions show a similar onset location. These results provide a simple, replicable way to locate where instruction following begins and to compare this location across tasks and model sizes.

</details>


### [34] ["As Eastern Powers, I will veto." : An Investigation of Nation-level Bias of Large Language Models in International Relations](https://arxiv.org/abs/2511.10695)
*Jonghyeon Choi,Yeonjun Choi,Hyun-chul Kim,Beakcheol Jang*

Main category: cs.CL

TL;DR: 本文系统性地研究了大语言模型在国际关系领域表现出的国家层面偏见，提出了评估框架并验证了偏见的多维性，且开发了去偏框架提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在国际关系中表现出的国家偏见未被充分研究，且不同模型和任务中的偏见表现差异显著。

Method: 基于联合国安理会历史记录，设计三项测试来评估多个LLM对五个常任理事国的偏见，结合检索增强生成和基于反思的自我反思技术进行去偏。

Result: 发现LLM对国家偏见表现出多维度变化，推理能力强的模型偏见较少，所提去偏方案有效降低偏见提升性能，特别是在GPT-4o-mini和LLama-3.3-70B中。

Conclusion: LLM在国际关系应用中需同时关注国家偏见和性能，提出的去偏框架为减少偏见提供有效路径。

Abstract: This paper systematically examines nation-level biases exhibited by Large Language Models (LLMs) within the domain of International Relations (IR). Leveraging historical records from the United Nations Security Council (UNSC), we developed a bias evaluation framework comprising three distinct tests to explore nation-level bias in various LLMs, with a particular focus on the five permanent members of the UNSC. Experimental results show that, even with the general bias patterns across models (e.g., favorable biases toward the western nations, and unfavorable biases toward Russia), these still vary based on the LLM. Notably, even within the same LLM, the direction and magnitude of bias for a nation change depending on the evaluation context. This observation suggests that LLM biases are fundamentally multidimensional, varying across models and tasks. We also observe that models with stronger reasoning abilities show reduced bias and better performance. Building on this finding, we introduce a debiasing framework that improves LLMs' factual reasoning combining Retrieval-Augmented Generation with Reflexion-based self-reflection techniques. Experiments show it effectively reduces nation-level bias, and improves performance, particularly in GPT-4o-mini and LLama-3.3-70B. Our findings emphasize the need to assess nation-level bias alongside performance when applying LLMs in the IR domain.

</details>


### [35] [$π$-Attention: Periodic Sparse Transformers for Efficient Long-Context Modeling](https://arxiv.org/abs/2511.10696)
*Dong Liu,Yanxuan Yu*

Main category: cs.CL

TL;DR: AttentionTransformerTransformer [] RingAttentionPiAttentionTransformer 


<details>
  <summary>Details</summary>
Motivation: Transformer Sequence length RingAttention 

Method: Attention Ring-local neighborhoods$C0$-stride skips Adaptive fusion gate periodic sparse Transformer $C0$ Receptive field $C0$C0$ $C0$

Result: Attention Transformer  8.3%RingAttention  50%

Conclusion: Attention RingAttention  8.3% 50%Modeling

Abstract: Transformers have revolutionized natural language processing, but their quadratic complexity with respect to sequence length remains a fundamental bottleneck for long-range modeling. While sparse attention mechanisms like RingAttention reduce computational costs by restricting attention to local neighborhoods, they suffer from limited receptive fields and lack of adaptability. We present \PiAttention, a periodic sparse Transformer that factorizes attention into ring-local neighborhoods, deterministic $π$-stride skips, and an adaptive fusion gate. The periodic structure provides predictable coverage of distant tokens, while the sparse footprint keeps the per-layer complexity linear in context length. We prove that \PiAttention achieves $\mathcal{O}(kL + π\log L)$ receptive field growth compared to $\mathcal{O}(kL)$ for RingAttention, where $k$ is the local window size, $π$ is the skip period, and $L$ is the sequence length. Extensive experiments on language modeling, retrieval, and vision-language tasks demonstrate that \PiAttention matches or surpasses dense attention quality with 8.3\% lower perplexity than RingAttention while using 50\% fewer GPUs for the same context length. Our detailed ablations and visualizations reveal the importance of periodic skips, adaptive fusion, and head-level sparsity coordination for efficient long-context modeling.

</details>


### [36] [Faithful Summarization of Consumer Health Queries: A Cross-Lingual Framework with LLMs](https://arxiv.org/abs/2511.10768)
*Ajwad Abrar,Nafisa Tabassum Oeshy,Prianka Maheru,Farzana Tabassum,Tareque Mohmud Chowdhury*

Main category: cs.CL

TL;DR: 本文提出结合TextRank句子抽取和医学实体识别与大语言模型相结合的方法，提升消费者健康问题摘要的准确性。


<details>
  <summary>Details</summary>
Motivation: 摘要不准确会误导医疗信息，带来严重风险，因此需要提升医疗文本摘要的忠实度。

Method: 结合TextRank句子抽取、医学命名实体识别和大语言模型，针对英孟两种语言数据集微调LLaMA-2-7B模型。

Result: 在多项质量和忠实度指标上均优于零样本和先前系统，人类评估显示超过80%的摘要保留了关键信息。

Conclusion: 忠实度是医疗摘要的关键维度，所提方法有效提升了摘要的可靠性，有助于更安全地应用大语言模型于医疗环境。

Abstract: Summarizing consumer health questions (CHQs) can ease communication in healthcare, but unfaithful summaries that misrepresent medical details pose serious risks. We propose a framework that combines TextRank-based sentence extraction and medical named entity recognition with large language models (LLMs) to enhance faithfulness in medical text summarization. In our experiments, we fine-tuned the LLaMA-2-7B model on the MeQSum (English) and BanglaCHQ-Summ (Bangla) datasets, achieving consistent improvements across quality (ROUGE, BERTScore, readability) and faithfulness (SummaC, AlignScore) metrics, and outperforming zero-shot baselines and prior systems. Human evaluation further shows that over 80\% of generated summaries preserve critical medical information. These results highlight faithfulness as an essential dimension for reliable medical summarization and demonstrate the potential of our approach for safer deployment of LLMs in healthcare contexts.

</details>


### [37] [TEDxTN: A Three-way Speech Translation Corpus for Code-Switched Tunisian Arabic - English](https://arxiv.org/abs/2511.10780)
*Fethi Bougares,Salima Mdhaffar,Haroun Elleuch,Yannick Estève*

Main category: cs.CL

TL;DR: 本文介绍了TEDxTN，这是第一个公开的突尼斯阿拉伯语到英语的语音翻译数据集，包含25小时带有语言切换的演讲，覆盖突尼斯多个区域的不同口音。


<details>
  <summary>Details</summary>
Motivation: 为了缓解阿拉伯方言数据稀缺的问题，特别是突尼斯阿拉伯语的语音翻译研究缺乏公开数据集。

Method: 收集、分割、转录并翻译了108个TEDx演讲，遵循内部开发的注释指南，覆盖11个不同地区的口音，公开注释指南与语料库。

Result: 发布了首个公开的带语言切换的突尼斯方言语音翻译语料库，并报告了使用多种预训练和微调的端到端模型在语音识别和语音翻译任务上的基线结果。

Conclusion: TEDxTN语料库为突尼斯方言自然语言处理研究提供了宝贵资源，有助于推动该领域的进一步研究。

Abstract: In this paper, we introduce TEDxTN, the first publicly available Tunisian Arabic to English speech translation dataset. This work is in line with the ongoing effort to mitigate the data scarcity obstacle for a number of Arabic dialects. We collected, segmented, transcribed and translated 108 TEDx talks following our internally developed annotations guidelines. The collected talks represent 25 hours of speech with code-switching that cover speakers with various accents from over 11 different regions of Tunisia. We make the annotation guidelines and corpus publicly available. This will enable the extension of TEDxTN to new talks as they become available. We also report results for strong baseline systems of Speech Recognition and Speech Translation using multiple pre-trained and fine-tuned end-to-end models. This corpus is the first open source and publicly available speech translation corpus of Code-Switching Tunisian dialect. We believe that this is a valuable resource that can motivate and facilitate further research on the natural language processing of Tunisian Dialect.

</details>


### [38] [Sabiá: Um Chatbot de Inteligência Artificial Generativa para Suporte no Dia a Dia do Ensino Superior](https://arxiv.org/abs/2511.10787)
*Guilherme Biava Rodrigues,Franciele Beal,Marlon Marcon,Alinne Cristinne Corrêa Souza,André Roberto Ortoncelli,Francisco Carlos Monteiro Souza,Rodolfo Adamshuk Silva*

Main category: cs.CL

TL;DR: 针对学生难以获取分散在多个文档和网站的日常学术信息的问题，本文提出使用生成式人工智能和检索增强生成技术开发聊天机器人。经过多种模型测试，Gemini 2.0 Flash和Gemma 3n表现优异。


<details>
  <summary>Details</summary>
Motivation: 学生获取日常学术信息困难，信息分散且缺乏清晰性，导致混乱和获取难。

Method: 采用生成式人工智能（GenAI）和检索增强生成（RAG）技术开发聊天机器人；通过质量指标和LLM-as-a-Judge方法评估多款GenAI模型。

Result: Gemini 2.0 Flash在质量和速度方面表现出色；Gemma 3n表现良好且为开源模型。

Conclusion: 基于GenAI和RAG的聊天机器人能够有效简化学生访问复杂高校信息的过程，Gemini 2.0 Flash和Gemma 3n是优选模型。

Abstract: Students often report difficulties in accessing day-to-day academic information, which is usually spread across numerous institutional documents and websites. This fragmentation results in a lack of clarity and causes confusion about routine university information. This project proposes the development of a chatbot using Generative Artificial Intelligence (GenAI) and Retrieval-Augmented Generation (RAG) to simplify access to such information. Several GenAI models were tested and evaluated based on quality metrics and the LLM-as-a-Judge approach. Among them, Gemini 2.0 Flash stood out for its quality and speed, and Gemma 3n for its good performance and open-source nature.

</details>


### [39] [LLM-as-a-Grader: Practical Insights from Large Language Model for Short-Answer and Report Evaluation](https://arxiv.org/abs/2511.10819)
*Grace Byun,Swati Rajwal,Jinho D. Choi*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（GPT-4o）在大学计算语言学课程中对简答测验和项目报告评分的可行性，结果显示其与人类评分高度相关，尤其在测验中达到了高达0.98的相关系数。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在教育评分中的应用日益增多，但其在真实课堂环境中与人类评估的一致性尚未充分检验。

Method: 收集50名学生的五次测验答卷和14个团队的项目报告，利用GPT-4o进行评分，并将结果与助教的独立评分进行比较。

Result: GPT-4o与人类评分者相关系数高达0.98，测验评分中有55%的情况完全一致；项目报告评分总体与人类评分一致，但在技术性开放性问题上存在一定评分波动。

Conclusion: GPT-4o展示了用于教育评分的巨大潜力，同时也存在一定的局限性，本文为自动化评分系统在真实学术环境中的应用提供了有价值的实证支持和开源资源。

Abstract: Large Language Models (LLMs) are increasingly explored for educational tasks such as grading, yet their alignment with human evaluation in real classrooms remains underexamined. In this study, we investigate the feasibility of using an LLM (GPT-4o) to evaluate short-answer quizzes and project reports in an undergraduate Computational Linguistics course. We collect responses from approximately 50 students across five quizzes and receive project reports from 14 teams. LLM-generated scores are compared against human evaluations conducted independently by the course teaching assistants (TAs). Our results show that GPT-4o achieves strong correlation with human graders (up to 0.98) and exact score agreement in 55\% of quiz cases. For project reports, it also shows strong overall alignment with human grading, while exhibiting some variability in scoring technical, open-ended responses. We release all code and sample data to support further research on LLMs in educational assessment. This work highlights both the potential and limitations of LLM-based grading systems and contributes to advancing automated grading in real-world academic settings.

</details>


### [40] [Tracing Multilingual Representations in LLMs with Cross-Layer Transcoders](https://arxiv.org/abs/2511.10840)
*Abir Harrasse,Florent Draye,Zhijing Jin,Bernhard Schölkopf*

Main category: cs.CL

TL;DR: 本文研究多语种大型语言模型（LLMs）如何内部表示多种语言，发现模型在不同语言间使用几乎相同的表示，语言特异性解码发生在后期层，且训练主导语言影响模型性能。


<details>
  <summary>Details</summary>
Motivation: 多语种LLMs虽能处理多语言，但其内部语言多样性的表征机制不清楚，尤其是为何模型对主导训练语言表现更好。

Method: 训练不同多语种数据混合的LLMs，使用跨层转码器(CL T)和归因图分析模型内部机制，干预高频语言特征观察输出变化。

Result: 发现模型采用类似的跨语言表示，而语言特定解码在后层出现；解码依赖于最终层的一小组高频语言特征，这些特征线性读取模型前层中的语言身份。通过介入可抑制/替换语言输出。

Conclusion: 揭示了枢纽语言机制，该机制解释了多语种对齐及训练主导语言影响，理解该机制对提升多语种LLMs性能和泛化能力至关重要。

Abstract: Multilingual Large Language Models (LLMs) can process many languages, yet how they internally represent this diversity remains unclear. Do they form shared multilingual representations with language-specific decoding, and if so, why does performance still favor the dominant training language? To address this, we train a series of LLMs on different mixtures of multilingual data and analyze their internal mechanisms using cross-layer transcoders (CLT) and attribution graphs. Our results provide strong evidence for pivot language representations: the model employs nearly identical representations across languages, while language-specific decoding emerges in later layers. Attribution analyses reveal that decoding relies in part on a small set of high-frequency language features in the final layers, which linearly read out language identity from the first layers in the model. By intervening on these features, we can suppress one language and substitute another in the model's outputs. Finally, we study how the dominant training language influences these mechanisms across attribution graphs and decoding pathways. We argue that understanding this pivot-language mechanism is crucial for improving multilingual alignment in LLMs.

</details>


### [41] [Reinforcing Stereotypes of Anger: Emotion AI on African American Vernacular English](https://arxiv.org/abs/2511.10846)
*Rebecca Dorn,Christina Chance,Casandra Rusti,Charles Bickham,Kai-Wei Chang,Fred Morstatter,Kristina Lerman*

Main category: cs.CL

TL;DR: 该论文研究了情感识别模型在非裔美国人英语（AAVE）和通用美式英语（GAE）上的表现差异，发现模型在AAVE文本中误判愤怒情绪的概率显著更高，并揭示了情感AI可能强化种族刻板偏见的问题。


<details>
  <summary>Details</summary>
Motivation: 现有情感识别模型主要依赖反映主流文化规范的标注数据，导致模型难以准确理解和识别方言中的情感表达，特别是AAVE，这限制了模型在多文化环境中的应用效果和公平性。

Method: 作者分析了270万条洛杉矶地理标记的推文，使用计算方法评估文本中AAVE特征，并在875条高低AAVE密度推文上收集情感存在和强度的注释。采用社区知情的"银级"标签，由非裔美国且精通AAVE的内部群体标注员对AAVE密集推文进行标注。同时利用GPT、BERT和SpanEmo等模型进行情感预测并进行误差分析。

Result: 模型在AAVE推文中预测愤怒情绪的误报率超过GAE的两倍，SpanEmo模型在GAE的愤怒误报率为25%，而在AAVE中上升到60%。线性回归分析显示模型的预测与基于脏话的AAVE特征相关性更强，且社区内部标注与模型和非内部标注差异明显。高非裔社区区域愤怒预测更高，欢乐预测更低。

Conclusion: 情感AI存在通过偏见情感分类强化种族刻板印象的安全隐患，强调构建文化与方言知情的情感计算系统以提升公平性和准确性的重要性。

Abstract: Automated emotion detection is widely used in applications ranging from well-being monitoring to high-stakes domains like mental health and hiring. However, models often rely on annotations that reflect dominant cultural norms, limiting model ability to recognize emotional expression in dialects often excluded from training data distributions, such as African American Vernacular English (AAVE). This study examines emotion recognition model performance on AAVE compared to General American English (GAE). We analyze 2.7 million tweets geo-tagged within Los Angeles. Texts are scored for strength of AAVE using computational approximations of dialect features. Annotations of emotion presence and intensity are collected on a dataset of 875 tweets with both high and low AAVE densities. To assess model accuracy on a task as subjective as emotion perception, we calculate community-informed "silver" labels where AAVE-dense tweets are labeled by African American, AAVE-fluent (ingroup) annotators. On our labeled sample, GPT and BERT-based models exhibit false positive prediction rates of anger on AAVE more than double than on GAE. SpanEmo, a popular text-based emotion model, increases false positive rates of anger from 25 percent on GAE to 60 percent on AAVE. Additionally, a series of linear regressions reveals that models and non-ingroup annotations are significantly more correlated with profanity-based AAVE features than ingroup annotations. Linking Census tract demographics, we observe that neighborhoods with higher proportions of African American residents are associated with higher predictions of anger (Pearson's correlation r = 0.27) and lower joy (r = -0.10). These results find an emergent safety issue of emotion AI reinforcing racial stereotypes through biased emotion classification. We emphasize the need for culturally and dialect-informed affective computing systems.

</details>


### [42] [Leveraging Parameter Space Symmetries for Reasoning Skill Transfer in LLMs](https://arxiv.org/abs/2511.10850)
*Stefan Horoi,Sangwoo Cho,Supriyo Chakraborty,Shi-Xiong Zhang,Sambit Sahu,Guy Wolf,Genta Indra Winata*

Main category: cs.CL

TL;DR: 本文提出了一种通过对变换器结构中的参数空间进行对齐，解决大语言模型任务算术中的负干扰问题，并实现技能的有效迁移。


<details>
  <summary>Details</summary>
Motivation: 现有任务算术在大语言模型间迁移技能时常因模型训练差异导致负干扰，影响迁移效果。

Method: 利用变换器的排列、旋转和缩放对称性对参数空间进行对齐，适配Grouped-Query Attention(GQA)和SwiGLU层，结合权重和激活方法进行参数空间对齐，从而实现任务技能迁移。

Result: 在复杂推理基准测试中，所提方法显著优于传统任务算术，成功将高级推理技能转移给无推理能力模型。

Conclusion: 提出的参数空间对齐策略有效改进了大语言模型间技能迁移与融合，提升模型适应性，减少重复微调。

Abstract: Task arithmetic is a powerful technique for transferring skills between Large Language Models (LLMs), but it often suffers from negative interference when models have diverged during training. We address this limitation by first aligning the models' parameter spaces, leveraging the inherent permutation, rotation, and scaling symmetries of Transformer architectures. We adapt parameter space alignment for modern Grouped-Query Attention (GQA) and SwiGLU layers, exploring both weight-based and activation-based approaches. Using this alignment-first strategy, we successfully transfer advanced reasoning skills to a non-reasoning model. Experiments on challenging reasoning benchmarks show that our method consistently outperforms standard task arithmetic. This work provides an effective approach for merging and transferring specialized skills across evolving LLM families, reducing redundant fine-tuning and enhancing model adaptability.

</details>


### [43] [From Fact to Judgment: Investigating the Impact of Task Framing on LLM Conviction in Dialogue Systems](https://arxiv.org/abs/2511.10871)
*Parisa Rabbani,Nimet Beyza Bozdag,Dilek Hakkani-Tür*

Main category: cs.CL

TL;DR: 本论文研究了大语言模型（LLMs）作为评判者在涉及社交或对话判断任务中的表现差异，发现在对话框架下模型的判断会显著变化。


<details>
  <summary>Details</summary>
Motivation: 目前尚不清楚LLMs是否能可靠地评估需要社交或对话判断的任务，因此希望探究对话情境如何影响模型的判断坚定度。

Method: 通过比较模型对直接事实查询与同一信息在最小对话中的判断表现，并加入简单反驳句作为压力，评估模型在不同条件下的立场坚定性。

Result: 发现不同模型在社交对话任务下表现各异，部分表现出谄媚或过度批判倾向，整体判断能力平均变动9.24%。

Conclusion: 对话框架对LLMs的判断影响显著，提出的评估框架为诊断模型信念提供了可复现方法，有助于提升对话系统的可信度。

Abstract: LLMs are increasingly employed as judges across a variety of tasks, including those involving everyday social interactions. Yet, it remains unclear whether such LLM-judges can reliably assess tasks that require social or conversational judgment. We investigate how an LLM's conviction is changed when a task is reframed from a direct factual query to a Conversational Judgment Task. Our evaluation framework contrasts the model's performance on direct factual queries with its assessment of a speaker's correctness when the same information is presented within a minimal dialogue, effectively shifting the query from "Is this statement correct?" to "Is this speaker correct?". Furthermore, we apply pressure in the form of a simple rebuttal ("The previous answer is incorrect.") to both conditions. This perturbation allows us to measure how firmly the model maintains its position under conversational pressure. Our findings show that while some models like GPT-4o-mini reveal sycophantic tendencies under social framing tasks, others like Llama-8B-Instruct become overly-critical. We observe an average performance change of 9.24% across all models, demonstrating that even minimal dialogue context can significantly alter model judgment, underscoring conversational framing as a key factor in LLM-based evaluation. The proposed framework offers a reproducible methodology for diagnosing model conviction and contributes to the development of more trustworthy dialogue systems.

</details>


### [44] [ICX360: In-Context eXplainability 360 Toolkit](https://arxiv.org/abs/2511.10879)
*Dennis Wei,Ronny Luss,Xiaomeng Hu,Lucas Monteiro Paes,Pin-Yu Chen,Karthikeyan Natesan Ramamurthy,Erik Miehling,Inge Vejsbjerg,Hendrik Strobelt*

Main category: cs.CL

TL;DR: 本文介绍了ICX360，一个用于解释大型语言模型输出的开源Python工具包，重点关注用户提供的上下文。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型应用范围扩大，尤其是在高风险场景中，开发有效的解释工具变得至关重要。

Method: ICX360集成了三种最新解释工具，采用黑盒（扰动）和白盒（梯度）方法，针对用户输入的上下文进行分析。

Result: ICX360支持多种用例，如检索增强生成、自然语言生成和模型防护，且提供快速入门和详细教程，方便用户使用。

Conclusion: ICX360为理解和解释大型语言模型输出提供了实用且易用的解决方案，促进模型在实际应用中的可信度和透明度。

Abstract: Large Language Models (LLMs) have become ubiquitous in everyday life and are entering higher-stakes applications ranging from summarizing meeting transcripts to answering doctors' questions. As was the case with earlier predictive models, it is crucial that we develop tools for explaining the output of LLMs, be it a summary, list, response to a question, etc. With these needs in mind, we introduce In-Context Explainability 360 (ICX360), an open-source Python toolkit for explaining LLMs with a focus on the user-provided context (or prompts in general) that are fed to the LLMs. ICX360 contains implementations for three recent tools that explain LLMs using both black-box and white-box methods (via perturbations and gradients respectively). The toolkit, available at https://github.com/IBM/ICX360, contains quick-start guidance materials as well as detailed tutorials covering use cases such as retrieval augmented generation, natural language generation, and jailbreaking.

</details>


### [45] [A Multifaceted Analysis of Negative Bias in Large Language Models through the Lens of Parametric Knowledge](https://arxiv.org/abs/2511.10881)
*Jongyoon Song,Sangwon Yu,Sungroh Yoon*

Main category: cs.CL

TL;DR: 本文研究了大语言模型在二元决策任务中表现出的负向偏见，发现模型倾向于在缺乏相关知识时生成负面回答，并受提示格式的显著影响。


<details>
  <summary>Details</summary>
Motivation: 尽管此前关注过导致负向偏见的注意力头，具体影响负向偏见的详细因素仍未充分研究。

Method: 引入一个系统化构建评估集的流程，将数据集按照模型参数知识划分为正确、错误和相关知识不足三个子集，通过分析不同提示场景对负向偏见的影响进行细致研究。

Result: 发现提示格式比否定语义更影响回答，缺乏知识时模型倾向负面回答提供“我不知道”选项和相关上下文能减少偏见，而链式思维提示会加剧偏见。

Conclusion: 负向偏见受多种因素影响，理解这些因素对设计抑制负偏见的策略至关重要。

Abstract: Negative bias refers to the tendency of large language models (LLMs) to excessively generate negative responses in binary decision tasks (e.g., yes-no question answering). Previous research has focused on detecting and addressing negative attention heads that induce negative bias. However, the underlying detailed factors influencing negative bias remain underexplored. In this paper, we demonstrate that LLMs exhibit format-level negative bias, meaning the prompt format more influences their responses than the semantics of the negative response. For the fine-grained study of the negative bias, we introduce a pipeline for constructing the evaluation set, which systematically categorizes the dataset into three subsets based on the model's parametric knowledge: correct, incorrect, and insufficient relevant knowledge. Through analysis of this evaluation set, we identify a shortcut behavior in which models tend to generate negative responses when they lack sufficient knowledge to answer a yes-no question, leading to negative bias. We further examine how negative bias changes under various prompting scenarios related to parametric knowledge. We observe that providing relevant context and offering an "I don't know" option generally reduces negative bias, whereas chain-of-thought prompting tends to amplify the bias. Finally, we demonstrate that the degree of negative bias can vary depending on the type of prompt, which influences the direction of the response. Our work reveals the various factors that influence negative bias, providing critical insights for mitigating it in LLMs.

</details>


### [46] [MedPath: Multi-Domain Cross-Vocabulary Hierarchical Paths for Biomedical Entity Linking](https://arxiv.org/abs/2511.10887)
*Nishant Mishra,Wilker Aziz,Iacer Calixto*

Main category: cs.CL

TL;DR: 本文提出了MedPath，一个基于九个专家标注数据集的大规模多领域生物医学实体链接（EL）数据集，解决了生物医学NER和EL中的数据碎片化、缺乏可解释模型资源及评估指标单一的问题。


<details>
  <summary>Details</summary>
Motivation: 当前生物医学命名实体识别和实体链接面临数据碎片、模型可解释性不足以及语义盲目的评估指标限制，影响相关进展。

Method: 构建MedPath数据集，整合九个现有数据集的实体，统一归一化到最新UMLS版本，并扩展与62个生物医学词汇表的映射，附加多达11个本体词汇表从通用到具体的完整本体路径信息。

Result: MedPath数据集为语义丰富且可解释的EL系统的训练和评估提供支持，促进跨领域、可互操作和可解释的临床NLP模型的发展。

Conclusion: MedPath推动了生物医学自然语言处理的新研究方向，有助于构建下一代语义丰富、可解释且互操作性强的临床NLP模型。

Abstract: Progress in biomedical Named Entity Recognition (NER) and Entity Linking (EL) is currently hindered by a fragmented data landscape, a lack of resources for building explainable models, and the limitations of semantically-blind evaluation metrics. To address these challenges, we present MedPath, a large-scale and multi-domain biomedical EL dataset that builds upon nine existing expert-annotated EL datasets. In MedPath, all entities are 1) normalized using the latest version of the Unified Medical Language System (UMLS), 2) augmented with mappings to 62 other biomedical vocabularies and, crucially, 3) enriched with full ontological paths -- i.e., from general to specific -- in up to 11 biomedical vocabularies. MedPath directly enables new research frontiers in biomedical NLP, facilitating training and evaluation of semantic-rich and interpretable EL systems, and the development of the next generation of interoperable and explainable clinical NLP models.

</details>


### [47] [Expert-Guided Prompting and Retrieval-Augmented Generation for Emergency Medical Service Question Answering](https://arxiv.org/abs/2511.10900)
*Xueren Ge,Sahil Murtaza,Anthony Cortez,Homa Alemzadeh*

Main category: cs.CL

TL;DR: 本文提出了EMSQA数据集及基于此的数据驱动和推理增强方法，提高了大语言模型在紧急医疗服务领域的问答准确性。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型在医疗问答中缺乏领域专业知识的支持，尤其是临床科目细分和认证级别信息，限制了模型在高风险情境中的表现。

Method: 构建包含10个临床科目和4个认证级别的EMSQA数据集及对齐的知识库，提出Expert-CoT（基于科目和级别的链式推理提示）和ExpertRAG（基于对齐知识库和真实患者数据的检索增强生成）方法。

Result: 在4个大语言模型上，Expert-CoT相较于传统CoT提升了最多2.05%准确率，结合ExpertRAG则提升了最多4.59%。32B参数的专家增强模型通过了所有EMS认证模拟考试。

Conclusion: 结合领域细分和认证级别的专业知识提示与知识库检索，可以显著提升大语言模型在应急医疗问答中的性能，促进高风险医疗场景应用。

Abstract: Large language models (LLMs) have shown promise in medical question answering, yet they often overlook the domain-specific expertise that professionals depend on, such as the clinical subject areas (e.g., trauma, airway) and the certification level (e.g., EMT, Paramedic). Existing approaches typically apply general-purpose prompting or retrieval strategies without leveraging this structured context, limiting performance in high-stakes settings. We address this gap with EMSQA, an 24.3K-question multiple-choice dataset spanning 10 clinical subject areas and 4 certification levels, accompanied by curated, subject area-aligned knowledge bases (40K documents and 2M tokens). Building on EMSQA, we introduce (i) Expert-CoT, a prompting strategy that conditions chain-of-thought (CoT) reasoning on specific clinical subject area and certification level, and (ii) ExpertRAG, a retrieval-augmented generation pipeline that grounds responses in subject area-aligned documents and real-world patient data. Experiments on 4 LLMs show that Expert-CoT improves up to 2.05% over vanilla CoT prompting. Additionally, combining Expert-CoT with ExpertRAG yields up to a 4.59% accuracy gain over standard RAG baselines. Notably, the 32B expertise-augmented LLMs pass all the computer-adaptive EMS certification simulation exams.

</details>


### [48] [Multimodal Peer Review Simulation with Actionable To-Do Recommendations for Community-Aware Manuscript Revisions](https://arxiv.org/abs/2511.10902)
*Mengze Hong,Di Jiang,Weiwei Zhao,Yawen Li,Yihang Wang,Xinyuan Luo,Yanjie Sun,Chen Jason Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种基于网页的多模态、社区感知同行评审模拟系统，通过结合文本和视觉信息，利用大语言模型和基于网络数据的增强生成技术，生成结构化、可操作的反馈，提升论文修改质量。


<details>
  <summary>Details</summary>
Motivation: 当前学术同行评审系统仅支持文本输入，缺乏上下文关联和可操作反馈，限制了自动化学术工作流的效果。

Method: 本文设计了一个集成多模态大语言模型和基于OpenReview大规模数据的检索增强生成（RAG）机制的系统，将生成的评审转化为结构化的待办事项列表，支持实时反馈和修改跟踪。

Result: 实验表明，该系统生成的评论更全面、有用，符合专家标准，优于传统文本审阅系统，提高了评审的透明度和人本属性。

Conclusion: 该多模态同行评审模拟系统有效促进了学术论文的修改和完善，推动了自动化、社区驱动的学术辅助工具的发展。

Abstract: While large language models (LLMs) offer promising capabilities for automating academic workflows, existing systems for academic peer review remain constrained by text-only inputs, limited contextual grounding, and a lack of actionable feedback. In this work, we present an interactive web-based system for multimodal, community-aware peer review simulation to enable effective manuscript revisions before paper submission. Our framework integrates textual and visual information through multimodal LLMs, enhances review quality via retrieval-augmented generation (RAG) grounded in web-scale OpenReview data, and converts generated reviews into actionable to-do lists using the proposed Action:Objective[\#] format, providing structured and traceable guidance. The system integrates seamlessly into existing academic writing platforms, providing interactive interfaces for real-time feedback and revision tracking. Experimental results highlight the effectiveness of the proposed system in generating more comprehensive and useful reviews aligned with expert standards, surpassing ablated baselines and advancing transparent, human-centered scholarly assistance.

</details>


### [49] [Automated Analysis of Learning Outcomes and Exam Questions Based on Bloom's Taxonomy](https://arxiv.org/abs/2511.10903)
*Ramya Kumar,Dhruv Gulwani,Sonit Singh*

Main category: cs.CL

TL;DR: 本文研究了基于布鲁姆分类法的考试题目和学习成果的自动分类，比较了多种机器学习和深度学习模型的表现，发现支持向量机结合数据增强效果最佳。


<details>
  <summary>Details</summary>
Motivation: 如何有效利用有限数据对考试题目和学习成果进行按照布鲁姆分类法的自动分类，提高教育评估的自动化和准确性。

Method: 使用600条标注了六个认知类别的句子，采用传统机器学习模型（朴素贝叶斯、逻辑回归、支持向量机）、循环神经网络模型（LSTM、BiLSTM、GRU、BiGRU）、基于transformer的模型（BERT、RoBERTa）以及大型语言模型（OpenAI、Gemini等），并结合不同的数据预处理和增强策略进行评估。

Result: 支持向量机结合数据增强在准确率、召回率和F1分数上均达到94%，且过拟合最小。RNN和BERT模型出现严重过拟合，RoBERTa起初表现良好但训练后期过拟合。大型语言模型零样本评估中OpenAI和Gemini表现较好，准确率约为72%-73%。

Conclusion: 复杂深度模型在小数据集上训练面临过拟合挑战，数据增强和简单模型（如支持向量机）在布鲁姆分类法的分类任务中表现优越，强调数据质量和模型选择的重要性。

Abstract: This paper explores the automatic classification of exam questions and learning outcomes according to Bloom's Taxonomy. A small dataset of 600 sentences labeled with six cognitive categories - Knowledge, Comprehension, Application, Analysis, Synthesis, and Evaluation - was processed using traditional machine learning (ML) models (Naive Bayes, Logistic Regression, Support Vector Machines), recurrent neural network architectures (LSTM, BiLSTM, GRU, BiGRU), transformer-based models (BERT and RoBERTa), and large language models (OpenAI, Gemini, Ollama, Anthropic). Each model was evaluated under different preprocessing and augmentation strategies (for example, synonym replacement, word embeddings, etc.). Among traditional ML approaches, Support Vector Machines (SVM) with data augmentation achieved the best overall performance, reaching 94 percent accuracy, recall, and F1 scores with minimal overfitting. In contrast, the RNN models and BERT suffered from severe overfitting, while RoBERTa initially overcame it but began to show signs as training progressed. Finally, zero-shot evaluations of large language models (LLMs) indicated that OpenAI and Gemini performed best among the tested LLMs, achieving approximately 0.72-0.73 accuracy and comparable F1 scores. These findings highlight the challenges of training complex deep models on limited data and underscore the value of careful data augmentation and simpler algorithms (such as augmented SVM) for Bloom's Taxonomy classification.

</details>


### [50] [Evaluating Large Language Models on Rare Disease Diagnosis: A Case Study using House M.D](https://arxiv.org/abs/2511.10912)
*Arsh Gupta,Ajay Narayanan Sridhar,Bonam Mingole,Amulya Yadav*

Main category: cs.CL

TL;DR: 该论文评估了四种大型语言模型在罕见病诊断任务中的表现，使用了一个基于电视剧《豪斯医生》的176个症状-诊断配对数据集，发现模型性能差异较大，但新一代模型提升显著。


<details>
  <summary>Details</summary>
Motivation: 目前大型语言模型在罕见病诊断领域的表现尚未深入研究，如何利用模型提升罕见病的识别能力对医学教育和临床有重要意义。

Method: 作者构建了一个通过医学教育验证的罕见病症状-诊断数据集，并在此基础上测试了包括GPT 4o mini、GPT 5 mini、Gemini 2.5 Flash和Gemini 2.5 Pro四种先进大型语言模型的诊断推理能力。

Result: 模型准确率从16.48%到38.64%不等，新一代模型在准确率上实现了2.3倍的提升。尽管所有模型在罕见病诊断上仍具挑战性，但不同架构的提升趋势明显。

Conclusion: 该工作为罕见病的叙事医疗推理建立了教育验证的基准数据集和性能指标，提供了公开的评估框架，有助于促进AI辅助诊断技术的发展。

Abstract: Large language models (LLMs) have demonstrated capabilities across diverse domains, yet their performance on rare disease diagnosis from narrative medical cases remains underexplored. We introduce a novel dataset of 176 symptom-diagnosis pairs extracted from House M.D., a medical television series validated for teaching rare disease recognition in medical education. We evaluate four state-of-the-art LLMs such as GPT 4o mini, GPT 5 mini, Gemini 2.5 Flash, and Gemini 2.5 Pro on narrative-based diagnostic reasoning tasks. Results show significant variation in performance, ranging from 16.48% to 38.64% accuracy, with newer model generations demonstrating a 2.3 times improvement. While all models face substantial challenges with rare disease diagnosis, the observed improvement across architectures suggests promising directions for future development. Our educationally validated benchmark establishes baseline performance metrics for narrative medical reasoning and provides a publicly accessible evaluation framework for advancing AI-assisted diagnosis research.

</details>


### [51] [CardioEmbed: Domain-Specialized Text Embeddings for Clinical Cardiology](https://arxiv.org/abs/2511.10930)
*Richard J. Young,Alice M. Matthews*

Main category: cs.CL

TL;DR: 该论文开发了一种专门针对心脏病学的文本嵌入模型CardioEmbed，通过对心脏病学权威教科书语料进行对比学习训练，显著提升了心脏病学语义检索的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的生物医学文本嵌入模型主要基于PubMed研究文献，对临床心脏病学中的专业术语和操作知识支持不足，限制了模型在临床应用中的效果。

Method: 基于Qwen3-Embedding-8B模型，使用对比学习方法和InfoNCE损失函数，在七本权威心脏病学教科书共约15万句去重语料上进行训练。

Result: CardioEmbed在心脏病学特定语义检索任务中实现了99.60%的检索准确率，较当前最先进模型MedTE提升15.94个百分点，在相关医学基准测试中表现也具有竞争力。

Conclusion: 通过在领域专门的临床教科书上进行训练，可以显著改善心脏病学领域的文本嵌入质量，提升临床语义检索的效果。

Abstract: Biomedical text embeddings have primarily been developed using research literature from PubMed, yet clinical cardiology practice relies heavily on procedural knowledge and specialized terminology found in comprehensive textbooks rather than research abstracts. This research practice gap limits the effectiveness of existing embedding models for clinical applications incardiology. This study trained CardioEmbed, a domain-specialized embedding model based on Qwen3-Embedding-8B, using contrastive learning on a curated corpus of seven comprehensive cardiology textbooks totaling approximately 150,000 sentences after deduplication. The model employs InfoNCE loss with in-batch negatives and achieves 99.60% retrieval accuracy on cardiac-specific semantic retrieval tasks, a +15.94 percentage point improvement over MedTE, the current state-of-the-art medical embedding model. On MTEB medical benchmarks, the model obtained BIOSSES 0.77 Spearman and SciFact 0.61 NDCG@10, indicating competitive performance on related biomedical domains. Domain-specialized training on comprehensive clinical textbooks yields near-perfect cardiology retrieval (99.60% Acc@1), improving over MedTE by +15.94 percentage points.

</details>


### [52] [DiscoX: Benchmarking Discourse-Level Translation task in Expert Domains](https://arxiv.org/abs/2511.10984)
*Xiying Zhao,Zhoufutu Wen,Zhixuan Chen,Jingzhe Ding,Jianpeng Jiao,Shuai Li,Xi Li,Danni Liang,Shengda Long,Qianqian Liu,Xianbo Wu,Hongwan Gao,Xiang Gao,Liang Hu,Jiashuo Liu,Mengyun Liu,Weiran Shi,Chenghao Yang,Qianyu Yang,Xuanliang Zhang,Ge Zhang,Wenhao Huang*

Main category: cs.CL

TL;DR: 该论文提出了针对专业领域中文-英文语篇级翻译的新基准DiscoX及评测系统Metric-S，弥补了现有翻译评测侧重片段准确性而忽视语篇连贯性和术语精度的不足。


<details>
  <summary>Details</summary>
Motivation: 专业领域的语篇级翻译在知识传播和跨语言学术交流中至关重要，但现有评测方法主要关注片段级的准确性和流利性，无法全面反映语篇层面的连贯性和术语精度，导致评价不足。

Method: 作者构建了包含7个领域、200篇专业文本的DiscoX语料库，平均长度超过1700个词；同时设计了无参考的自动评测系统Metric-S，对准确性、流利性和适当性进行细粒度评价，并验证其与人工评分高度一致，优于现有指标。

Result: 实验结果表明，即使最先进的大型语言模型在DiscoX上的表现仍显著落后于人类专家，揭示了该任务的巨大难度及专业级机器翻译仍面临的挑战。

Conclusion: DiscoX基准和Metric-S评测系统为专业领域语篇级翻译提供了更严谨的评价框架，有助于推动基于大型语言模型的机器翻译技术的进一步提升。

Abstract: The evaluation of discourse-level translation in expert domains remains inadequate, despite its centrality to knowledge dissemination and cross-lingual scholarly communication. While these translations demand discourse-level coherence and strict terminological precision, current evaluation methods predominantly focus on segment-level accuracy and fluency. To address this limitation, we introduce DiscoX, a new benchmark for discourse-level and expert-level Chinese-English translation. It comprises 200 professionally-curated texts from 7 domains, with an average length exceeding 1700 tokens. To evaluate performance on DiscoX, we also develop Metric-S, a reference-free system that provides fine-grained automatic assessments across accuracy, fluency, and appropriateness. Metric-S demonstrates strong consistency with human judgments, significantly outperforming existing metrics. Our experiments reveal a remarkable performance gap: even the most advanced LLMs still trail human experts on these tasks. This finding validates the difficulty of DiscoX and underscores the challenges that remain in achieving professional-grade machine translation. The proposed benchmark and evaluation system provide a robust framework for more rigorous evaluation, facilitating future advancements in LLM-based translation.

</details>


### [53] [When Data is the Algorithm: A Systematic Study and Curation of Preference Optimization Datasets](https://arxiv.org/abs/2511.10985)
*Aladin Djuhera,Farhan Ahmed,Swanand Ravindra Kadhe,Syed Zawad,Heiko Ludwig,Holger Boche*

Main category: cs.CL

TL;DR: 本文系统分析了开放源码的大语言模型偏好数据集，提出了一种新的数据混合方法UltraMix，提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 目前虽有多个公开的DPO数据集，但缺乏系统的比较和细致的质量注释，难以理解偏好的选择依据和质量。

Method: 利用Magpie框架对每个样本进行任务类别、输入质量和偏好奖励的标注，进行细粒度的数据质量检查并发现数据集间的差异，基于此构建了新的数据集混合UltraMix。

Result: UltraMix在保持30%较小规模的同时，优于最佳单一数据集的基准测试表现。

Conclusion: 通过系统的数据质量分析和数据集融合，可以有效提升DPO训练的数据质量和模型表现，推动偏好优化的后续研究。

Abstract: Aligning large language models (LLMs) is a central objective of post-training, often achieved through reward modeling and reinforcement learning methods. Among these, direct preference optimization (DPO) has emerged as a widely adopted technique that fine-tunes LLMs on preferred completions over less favorable ones. While most frontier LLMs do not disclose their curated preference pairs, the broader LLM community has released several open-source DPO datasets, including TuluDPO, ORPO, UltraFeedback, HelpSteer, and Code-Preference-Pairs. However, systematic comparisons remain scarce, largely due to the high computational cost and the lack of rich quality annotations, making it difficult to understand how preferences were selected, which task types they span, and how well they reflect human judgment on a per-sample level. In this work, we present the first comprehensive, data-centric analysis of popular open-source DPO corpora. We leverage the Magpie framework to annotate each sample for task category, input quality, and preference reward, a reward-model-based signal that validates the preference order without relying on human annotations. This enables a scalable, fine-grained inspection of preference quality across datasets, revealing structural and qualitative discrepancies in reward margins. Building on these insights, we systematically curate a new DPO mixture, UltraMix, that draws selectively from all five corpora while removing noisy or redundant samples. UltraMix is 30% smaller than the best-performing individual dataset yet exceeds its performance across key benchmarks. We publicly release all annotations, metadata, and our curated mixture to facilitate future research in data-centric preference optimization.

</details>


### [54] [Correcting Mean Bias in Text Embeddings: A Refined Renormalization with Training-Free Improvements on MMTEB](https://arxiv.org/abs/2511.11041)
*Xingyu Ren,Youran Sun,Haoyu Liang*

Main category: cs.CL

TL;DR: 本文发现当前文本嵌入模型的输出存在一致性偏差，提出了一种名为Renormalization的轻量级无训练解决方案，有效提升了多语种文本嵌入模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前文本嵌入模型产生的嵌入向量中存在几乎相同的偏置成分，影响模型的表现。

Method: 提出Renormalization方法，通过减去嵌入向量中的偏置成分μ，包括直接减去μ和减去向量在μ方向的投影两种变体。

Result: 在MMTEB基准上，Renormalization在38个模型中显著提升检索任务（9.7σ）、分类任务（3.1σ）及其他任务（0.8σ）的性能，实验验证了理论预期。

Conclusion: Renormalization作为一种无训练的调节手段，有效消除嵌入偏差，显著提升了文本嵌入模型的性能，且投影减法优于直接减法。

Abstract: We find that current text embedding models produce outputs with a consistent bias, i.e., each embedding vector $e$ can be decomposed as $\tilde{e} + μ$, where $μ$ is almost identical across all sentences. We propose a plug-and-play, training-free and lightweight solution called Renormalization. Through extensive experiments, we show that renormalization consistently and statistically significantly improves the performance of existing models on the Massive Multilingual Text Embedding Benchmark (MMTEB). In particular, across 38 models, renormalization improves performance by 9.7 $σ$ on retrieval tasks, 3.1 $σ$ on classification tasks, and 0.8 $σ$ on other types of tasks. Renormalization has two variants: directly subtracting $μ$ from $e$, or subtracting the projection of $e$ onto $μ$. We theoretically predict that the latter performs better, and our experiments confirm this prediction.

</details>


### [55] [Can LLMs Detect Their Own Hallucinations?](https://arxiv.org/abs/2511.11087)
*Sora Kadotani,Kosuke Nishida,Kyosuke Nishida*

Main category: cs.CL

TL;DR: 本文研究大语言模型(LLMs)能否检测自身生成内容中的错误信息（幻觉），提出一种基于句子分类的检测框架，并结合Chain-of-Thought(CoT)方法提升检测能力。


<details>
  <summary>Details</summary>
Motivation: LLMs虽然能生成流畅回答，但有时会产生虚假信息，本文旨在探究模型自身是否具备识别这些错误的能力。

Method: 将幻觉检测设为句子分类任务，设计了一个评估LLMs检测能力的框架，并利用CoT从模型参数中提取知识进行分类。

Result: 实验表明，结合CoT的GPT-3.5 Turbo能够检测出58.2%的自身幻觉。

Conclusion: 只要模型参数中包含足够知识，结合CoT方法的LLMs可以有效检测自身的幻觉内容。

Abstract: Large language models (LLMs) can generate fluent responses, but sometimes hallucinate facts. In this paper, we investigate whether LLMs can detect their own hallucinations. We formulate hallucination detection as a classification task of a sentence. We propose a framework for estimating LLMs' capability of hallucination detection and a classification method using Chain-of-Thought (CoT) to extract knowledge from their parameters. The experimental results indicated that GPT-$3.5$ Turbo with CoT detected $58.2\%$ of its own hallucinations. We concluded that LLMs with CoT can detect hallucinations if sufficient knowledge is contained in their parameters.

</details>


### [56] [Analysing Personal Attacks in U.S. Presidential Debates](https://arxiv.org/abs/2511.11108)
*Ruban Goyal,Rohitash Chandra,Sonit Singh*

Main category: cs.CL

TL;DR: 本文提出了一种利用深度学习与大规模语言模型自动检测美总统辩论中人身攻击的框架，通过人工标注和模型微调，实现对政治话语中攻击性语言的识别。


<details>
  <summary>Details</summary>
Motivation: 美总统辩论中人身攻击频繁，对公众认知影响大，自动检测可提升政治透明度和分析能力。

Method: 人工标注2016、2020、2024年辩论文本，结合统计分析与微调的变换器模型（如BERT与LLMs）进行攻击检测。

Result: 微调后的专用语言模型能够有效识别人身攻击，展现出在正式政治语言中检测有害语言的潜力。

Conclusion: 任务特定的现代语言模型适应性应用，有助于深入理解政治交流中的人身攻击现象，提高政治话语透明性。

Abstract: Personal attacks have become a notable feature of U.S. presidential debates and play an important role in shaping public perception during elections. Detecting such attacks can improve transparency in political discourse and provide insights for journalists, analysts and the public. Advances in deep learning and transformer-based models, particularly BERT and large language models (LLMs) have created new opportunities for automated detection of harmful language. Motivated by these developments, we present a framework for analysing personal attacks in U.S. presidential debates. Our work involves manual annotation of debate transcripts across the 2016, 2020 and 2024 election cycles, followed by statistical and language-model based analysis. We investigate the potential of fine-tuned transformer models alongside general-purpose LLMs to detect personal attacks in formal political speech. This study demonstrates how task-specific adaptation of modern language models can contribute to a deeper understanding of political communication.

</details>


### [57] [AV-Dialog: Spoken Dialogue Models with Audio-Visual Input](https://arxiv.org/abs/2511.11124)
*Tuochao Chen,Bandhav Veluri,Hongyu Gong,Shyamnath Gollakota*

Main category: cs.CL

TL;DR: 本文提出了AV-Dialog，一个结合音频和视觉信息的多模态对话框架，提升了嘈杂多说话者环境下的转录准确性和对话流畅度。


<details>
  <summary>Details</summary>
Motivation: 现有对话模型在嘈杂、多说话者环境中表现不佳，容易产生无关回复和尴尬的轮次衔接，需要一种能利用更多信息提高对话质量的方法。

Method: AV-Dialog结合声学标记化与多任务、多阶段训练，利用单人、合成及真实视听对话数据，实现了流式转录、基于语义的转折点检测及准确回复生成。

Result: 实验表明，AV-Dialog在干扰环境下优于仅音频模型，减少转录错误、提升轮次预测准确性及对话质量。

Conclusion: 视听结合增强了对说话者的感知能力，为实现现实环境中稳健的语音对话代理奠定了基础。

Abstract: Dialogue models falter in noisy, multi-speaker environments, often producing irrelevant responses and awkward turn-taking. We present AV-Dialog, the first multimodal dialog framework that uses both audio and visual cues to track the target speaker, predict turn-taking, and generate coherent responses. By combining acoustic tokenization with multi-task, multi-stage training on monadic, synthetic, and real audio-visual dialogue datasets, AV-Dialog achieves robust streaming transcription, semantically grounded turn-boundary detection and accurate responses, resulting in a natural conversational flow. Experiments show that AV-Dialog outperforms audio-only models under interference, reducing transcription errors, improving turn-taking prediction, and enhancing human-rated dialogue quality. These results highlight the power of seeing as well as hearing for speaker-aware interaction, paving the way for {spoken} dialogue agents that perform {robustly} in real-world, noisy environments.

</details>


### [58] [Enhancing Meme Emotion Understanding with Multi-Level Modality Enhancement and Dual-Stage Modal Fusion](https://arxiv.org/abs/2511.11126)
*Yi Shi,Wenlong Meng,Zhenyuan Guo,Chengkun Wei,Wenzhi Chen*

Main category: cs.CL

TL;DR: 本文提出了MemoDetector框架，通过多模态大语言模型增强文本信息，并采用双阶段模态融合策略，实现对网络表情包情感意图的细粒度识别，在两个数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前表情包情感理解存在细粒度多模态融合策略缺失和隐含意义及背景知识挖掘不足的问题。

Method: 论文提出了一个四步文本增强模块，利用多模态大语言模型推理提取表情包的隐含和上下文信息，增强文本内容；采用双阶段融合策略，先对原始图文进行浅融合，再对增强后的视觉和文本特征进行深度融合。

Result: 在MET-MEME和MOOD两个数据集上，MemoDetector分别提升了4.3%和3.4%的F1分数，优于现有最先进模型。

Conclusion: 提出的方法有效提升了表情包情感理解的精度和鲁棒性，展示了利用多模态大语言模型和层次融合策略推进该领域的潜力。

Abstract: With the rapid rise of social media and Internet culture, memes have become a popular medium for expressing emotional tendencies. This has sparked growing interest in Meme Emotion Understanding (MEU), which aims to classify the emotional intent behind memes by leveraging their multimodal contents. While existing efforts have achieved promising results, two major challenges remain: (1) a lack of fine-grained multimodal fusion strategies, and (2) insufficient mining of memes' implicit meanings and background knowledge. To address these challenges, we propose MemoDetector, a novel framework for advancing MEU. First, we introduce a four-step textual enhancement module that utilizes the rich knowledge and reasoning capabilities of Multimodal Large Language Models (MLLMs) to progressively infer and extract implicit and contextual insights from memes. These enhanced texts significantly enrich the original meme contents and provide valuable guidance for downstream classification. Next, we design a dual-stage modal fusion strategy: the first stage performs shallow fusion on raw meme image and text, while the second stage deeply integrates the enhanced visual and textual features. This hierarchical fusion enables the model to better capture nuanced cross-modal emotional cues. Experiments on two datasets, MET-MEME and MOOD, demonstrate that our method consistently outperforms state-of-the-art baselines. Specifically, MemoDetector improves F1 scores by 4.3\% on MET-MEME and 3.4\% on MOOD. Further ablation studies and in-depth analyses validate the effectiveness and robustness of our approach, highlighting its strong potential for advancing MEU. Our code is available at https://github.com/singing-cat/MemoDetector.

</details>


### [59] [Speech-Aware Long Context Pruning and Integration for Contextualized Automatic Speech Recognition](https://arxiv.org/abs/2511.11139)
*Yiming Rong,Yixin Zhang,Ziyi Wang,Deyang Jiang,Yunlong Zhao,Haoran Wu,Shiyu Zhou,Bo Xu*

Main category: cs.CL

TL;DR: 本文提出了SAP²方法，通过两阶段动态修剪和整合上下文关键词，显著提升自动语音识别在特定领域长上下文场景中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有自动语音识别系统在普通场景表现优异，但在需领域知识的长上下文条件（如会议演讲）中，由于模型上下文窗口限制和上下文噪声信息稀疏，难以充分利用长上下文信息。

Method: 提出SAP²框架，利用两阶段的Speech-Driven Attention-based Pooling机制，高效压缩上下文嵌入并保留语音显著信息，实现动态修剪和整合相关上下文关键词。

Result: 在SlideSpeech和LibriSpeech数据集上，SAP²分别达到7.71%和1.12%的词错误率（WER），在SlideSpeech数据集上相较非上下文基线减少了41.1%的偏关键词错误率（B-WER），并在大量上下文输入下表现稳定。

Conclusion: SAP²方法有效提升了自动语音识别系统对领域特定长上下文信息的利用效率，实现了性能提升与鲁棒性增强，展示了良好的应用前景。

Abstract: Automatic speech recognition (ASR) systems have achieved remarkable performance in common conditions but often struggle to leverage long-context information in contextualized scenarios that require domain-specific knowledge, such as conference presentations. This challenge arises primarily due to constrained model context windows and the sparsity of relevant information within extensive contextual noise. To solve this, we propose the SAP$^{2}$ method, a novel framework that dynamically prunes and integrates relevant contextual keywords in two stages. Specifically, each stage leverages our proposed Speech-Driven Attention-based Pooling mechanism, enabling efficient compression of context embeddings while preserving speech-salient information. Experimental results demonstrate state-of-the-art performance of SAP$^{2}$ on the SlideSpeech and LibriSpeech datasets, achieving word error rates (WER) of 7.71% and 1.12%, respectively. On SlideSpeech, our method notably reduces biased keyword error rates (B-WER) by 41.1% compared to non-contextual baselines. SAP$^{2}$ also exhibits robust scalability, consistently maintaining performance under extensive contextual input conditions on both datasets.

</details>


### [60] [PRSM: A Measure to Evaluate CLIP's Robustness Against Paraphrases](https://arxiv.org/abs/2511.11141)
*Udo Schlegel,Franziska Weeber,Jian Lan,Thomas Seidl*

Main category: cs.CL

TL;DR: 本文提出了衡量CLIP模型对改写句子鲁棒性的指标PRSM，发现其对不同改写策略的鲁棒性存在差异，并在性别相关查询中存在细微且一致的表现差异。


<details>
  <summary>Details</summary>
Motivation: CLIP模型在零-shot和少样本任务表现强劲，但其对语言变异尤其是句子改写的鲁棒性仍未充分研究，这在社会敏感环境下可能加剧偏见。

Method: 提出了一个新的评估指标Paraphrase Ranking Stability Metric（PRSM），使用社会反事实数据集对CLIP在句子改写下的稳定性进行实证分析，结合性别因素探讨鲁棒性影响。

Result: 研究发现CLIP对不同类型的句子改写表现出不同程度的鲁棒性，且涉及性别的查询在改写下表现出细微且一致的差异。

Conclusion: 评估CLIP模型对改写句子的鲁棒性对于公平和无偏部署多模态系统至关重要，未来应关注语言变异对模型性能和社会公平性的影响。

Abstract: Contrastive Language-Image Pre-training (CLIP) is a widely used multimodal model that aligns text and image representations through large-scale training. While it performs strongly on zero-shot and few-shot tasks, its robustness to linguistic variation, particularly paraphrasing, remains underexplored. Paraphrase robustness is essential for reliable deployment, especially in socially sensitive contexts where inconsistent representations can amplify demographic biases. In this paper, we introduce the Paraphrase Ranking Stability Metric (PRSM), a novel measure for quantifying CLIP's sensitivity to paraphrased queries. Using the Social Counterfactuals dataset, a benchmark designed to reveal social and demographic biases, we empirically assess CLIP's stability under paraphrastic variation, examine the interaction between paraphrase robustness and gender, and discuss implications for fairness and equitable deployment of multimodal systems. Our analysis reveals that robustness varies across paraphrasing strategies, with subtle yet consistent differences observed between male- and female-associated queries.

</details>


### [61] [Adverbs Revisited: Enhancing WordNet Coverage of Adverbs with a Supersense Taxonomy](https://arxiv.org/abs/2511.11214)
*Jooyoung Lee,Jader Martins Camboim de Sá*

Main category: cs.CL

TL;DR: 本文为副词引入了系统的语义分类体系，丰富了WordNet的词汇层次，并通过标注实验验证了该体系的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前WordNet在副词部分缺乏系统的语义分类，影响了词汇资源的完整性及其在自然语言处理中的应用。

Method: 提出基于语言学的副词超感知类型学，并通过标注实验对该分类进行了实证验证。

Result: 分类体系覆盖副词语义领域广泛，标注结果显示人类标注者能可靠区分各类别。

Conclusion: 该副词超感知类型学拓展了WordNet的覆盖范围，更符合语言学理论，有助于词义消歧、事件抽取等NLP任务的发展。

Abstract: WordNet offers rich supersense hierarchies for nouns and verbs, yet adverbs remain underdeveloped, lacking a systematic semantic classification. We introduce a linguistically grounded supersense typology for adverbs, empirically validated through annotation, that captures major semantic domains including manner, temporal, frequency, degree, domain, speaker-oriented, and subject-oriented functions. Results from a pilot annotation study demonstrate that these categories provide broad coverage of adverbs in natural text and can be reliably assigned by human annotators. Incorporating this typology extends WordNet's coverage, aligns it more closely with linguistic theory, and facilitates downstream NLP applications such as word sense disambiguation, event extraction, sentiment analysis, and discourse modeling. We present the proposed supersense categories, annotation outcomes, and directions for future work.

</details>


### [62] [LANE: Lexical Adversarial Negative Examples for Word Sense Disambiguation](https://arxiv.org/abs/2511.11234)
*Jader Martins Camboim de Sá,Jooyoung Lee,Cédric Pruski,Marcos Da Silveira*

Main category: cs.CL

TL;DR: 本文提出了一种名为LANE的对抗训练策略，通过选择性标记替代词生成具有挑战性的负样本，强化神经语言模型对目标词的细粒度语义捕捉能力。


<details>
  <summary>Details</summary>
Motivation: 神经语言模型通常过度拟合于整体句子表示，难以捕获局部词义细节，限制了细粒度词义解析能力。

Method: 采用对抗训练策略LANE，选择性标记训练集中替代词生成负样本，迫使模型增强同句不同词间的可区分性。

Result: 在词义变化检测和词义消歧基准测试中，LANE显著提升了词向量的区分能力，优于传统对比学习方法。

Conclusion: LANE方法能有效增强词义区分能力，捕捉细微语义差异，且具有模型无关性，可集成入现有表示学习框架。

Abstract: Fine-grained word meaning resolution remains a critical challenge for neural language models (NLMs) as they often overfit to global sentence representations, failing to capture local semantic details. We propose a novel adversarial training strategy, called LANE, to address this limitation by deliberately shifting the model's learning focus to the target word. This method generates challenging negative training examples through the selective marking of alternate words in the training set. The goal is to force the model to create a greater separability between same sentences with different marked words. Experimental results on lexical semantic change detection and word sense disambiguation benchmarks demonstrate that our approach yields more discriminative word representations, improving performance over standard contrastive learning baselines. We further provide qualitative analyses showing that the proposed negatives lead to representations that better capture subtle meaning differences even in challenging environments. Our method is model-agnostic and can be integrated into existing representation learning frameworks.

</details>


### [63] [KGQuest: Template-Driven QA Generation from Knowledge Graphs with LLM-Based Refinement](https://arxiv.org/abs/2511.11258)
*Sania Nayab,Marco Simoni,Giulio Rossolini,Andrea Saracino*

Main category: cs.CL

TL;DR: 提出了一种结合知识图谱和大语言模型的高效问答对生成方法，提升规模化、语言质量和事实一致性。


<details>
  <summary>Details</summary>
Motivation: 现有从知识图谱生成问答对的方法在规模化、语言质量和事实一致性方面表现不足，需要一种更高效且质量更优的生成方法。

Method: 通过根据关系对知识图谱三元组进行聚类，利用实体类型和关系生成自然语言模板，再用大语言模型对模板进行润色以提升语言质量，最后通过从知识图谱中选取干扰项完成答案选项实例化。

Result: 实验表明该方法结合规模化和语言流畅性，高效生成高质量的问答对。

Conclusion: 该混合方法兼具高效性和语言精度，能够有效提升知识图谱问答对的生成效果。

Abstract: The generation of questions and answers (QA) from knowledge graphs (KG) plays a crucial role in the development and testing of educational platforms, dissemination tools, and large language models (LLM). However, existing approaches often struggle with scalability, linguistic quality, and factual consistency. This paper presents a scalable and deterministic pipeline for generating natural language QA from KGs, with an additional refinement step using LLMs to further enhance linguistic quality. The approach first clusters KG triplets based on their relations, creating reusable templates through natural language rules derived from the entity types of objects and relations. A module then leverages LLMs to refine these templates, improving clarity and coherence while preserving factual accuracy. Finally, the instantiation of answer options is achieved through a selection strategy that introduces distractors from the KG. Our experiments demonstrate that this hybrid approach efficiently generates high-quality QA pairs, combining scalability with fluency and linguistic precision.

</details>


### [64] [destroR: Attacking Transfer Models with Obfuscous Examples to Discard Perplexity](https://arxiv.org/abs/2511.11309)
*Saadat Rafid Ahmed,Rubayet Shareen,Radoan Sharkar,Nazia Hossain,Mansur Mahi,Farig Yousuf Sadeque*

Main category: cs.CL

TL;DR: 本文分析并创造了新的对抗攻击方法，通过制造模糊输入迷惑当前机器学习模型，提升模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型存在多种脆弱性，威胁其安全性和应用效果，需研究有效的对抗攻击策略以增强鲁棒性。

Method: 基于现有对抗攻击方案，开发新策略，利用机器学习和深度学习生成高困惑度的模糊对抗样本，测试多数据集并首次将孟加拉语纳入攻击范围。

Result: 构建了最大困惑度的对抗样本，成功让模型产生混淆，提高了对抗攻击的效果和模型的鲁棒性。

Conclusion: 提出的模糊对抗攻击策略有效挑战了最先进模型，推动未来模型鲁棒性的发展，同时保证了方法的实用性和高效性。

Abstract: Advancements in Machine Learning & Neural Networks in recent years have led to widespread implementations of Natural Language Processing across a variety of fields with remarkable success, solving a wide range of complicated problems. However, recent research has shown that machine learning models may be vulnerable in a number of ways, putting both the models and the systems theyre used in at risk. In this paper, we intend to analyze and experiment with the best of existing adversarial attack recipes and create new ones. We concentrated on developing a novel adversarial attack strategy on current state-of-the-art machine learning models by producing ambiguous inputs for the models to confound them and then constructing the path to the future development of the robustness of the models. We will develop adversarial instances with maximum perplexity, utilizing machine learning and deep learning approaches in order to trick the models. In our attack recipe, we will analyze several datasets and focus on creating obfuscous adversary examples to put the models in a state of perplexity, and by including the Bangla Language in the field of adversarial attacks. We strictly uphold utility usage reduction and efficiency throughout our work.

</details>


### [65] [LAET: A Layer-wise Adaptive Ensemble Tuning Framework for Pretrained Language Models](https://arxiv.org/abs/2511.11315)
*Jawad Ibn Ahad,Muhammad Rafsan Kabir,Robin Krambroeckers,Sifat Momen,Nabeel Mohammed,Shafin Rahman*

Main category: cs.CL

TL;DR: 本文提出了一种名为LAET的新策略，通过选择性微调预训练大语言模型的关键层，有效降低计算资源消耗并提升金融领域NLP任务性能，结果优于包括GPT-4在内的先进模型。


<details>
  <summary>Details</summary>
Motivation: 当前金融领域的大语言模型虽然性能优异，但因高计算需求限制了其应用普及。作者希望通过降低计算成本，提高模型在实际金融任务中的可用性。

Method: 提出层次自适应集成微调（LAET）方法，通过分析隐藏状态表示，选择性微调预训练模型中最有效的层，同时冻结其他不重要的层以减少计算负担。

Result: 实验表明，LAET在金融NLP任务上表现优异，超过了包括GPT-4在内的现有基准和先进模型，且所用模型参数量较小（约3亿参数）。

Conclusion: LAET策略不仅实现了计算资源的显著节约，还提升了金融领域特定任务的效果，促进了先进金融语言模型在实际应用中的高效部署。

Abstract: Natural Language Processing (NLP) has transformed the financial industry, enabling advancements in areas such as textual analysis, risk management, and forecasting. Large language models (LLMs) like BloombergGPT and FinMA have set new benchmarks across various financial NLP tasks, including sentiment analysis, stock movement prediction, and credit risk assessment. Furthermore, FinMA-ES, a bilingual financial LLM, has also demonstrated strong performance using the FLARE and FLARE-ES benchmarks. However, the high computational demands of these models limit the accessibility of many organizations. To address this, we propose Layer-wise Adaptive Ensemble Tuning (LAET), a novel strategy that selectively fine-tunes the most effective layers of pre-trained LLMs by analyzing hidden state representations while freezing less critical layers. LAET significantly reduces computational overhead while enhancing task-specific performance. Our approach shows strong results in financial NLP tasks, outperforming existing benchmarks and state-of-the-art LLMs such as GPT-4, even with smaller LLMs ($\sim$3B parameters). This work bridges cutting-edge financial NLP research and real-world deployment with efficient and scalable models for financial applications.

</details>


### [66] [NOVA: An Agentic Framework for Automated Histopathology Analysis and Discovery](https://arxiv.org/abs/2511.11324)
*Anurag J. Vaidya,Felix Meissen,Daniel C. Castro,Shruthi Bannur,Tristan Lazard,Drew F. K. Williamson,Faisal Mahmood,Javier Alvarez-Valle,Stephanie L. Hyland,Kenza Bouzid*

Main category: cs.CL

TL;DR: NOVA是一种可将科学查询转换为可执行分析流程的智能框架，集成了49种病理图像分析工具，能够自动生成和运行Python代码，支持复杂多步推理和计算，表现优于现有编码代理。


<details>
  <summary>Details</summary>
Motivation: 数字化病理分析流程复杂且耗时，需要专业知识，因此限制了其广泛应用。

Method: 提出NOVA框架，通过迭代生成和执行Python代码来完成科学查询的分析任务，整合了多个领域特定工具，并能自定义新工具。引入SlideQuest基准测试，包含90个经过专业验证的问题，覆盖数据处理、定量分析和假设检验。

Result: NOVA在SlideQuest上的表现优于其他编码代理，能够完成多步骤推理和计算。通过病理学家验证的案例研究，成功关联了形态特征与预后相关的PAM50亚型，展示了其在生物医学发现中的潜力。

Conclusion: NOVA框架有效提升了数字病理图像分析的自动化和智能化水平，具备推动精准医疗研究的能力。

Abstract: Digitized histopathology analysis involves complex, time-intensive workflows and specialized expertise, limiting its accessibility. We introduce NOVA, an agentic framework that translates scientific queries into executable analysis pipelines by iteratively generating and running Python code. NOVA integrates 49 domain-specific tools (e.g., nuclei segmentation, whole-slide encoding) built on open-source software, and can also create new tools ad hoc. To evaluate such systems, we present SlideQuest, a 90-question benchmark -- verified by pathologists and biomedical scientists -- spanning data processing, quantitative analysis, and hypothesis testing. Unlike prior biomedical benchmarks focused on knowledge recall or diagnostic QA, SlideQuest demands multi-step reasoning, iterative coding, and computational problem solving. Quantitative evaluation shows NOVA outperforms coding-agent baselines, and a pathologist-verified case study links morphology to prognostically relevant PAM50 subtypes, demonstrating its scalable discovery potential.

</details>


### [67] [LaoBench: A Large-Scale Multidimensional Lao Benchmark for Large Language Models](https://arxiv.org/abs/2511.11334)
*Jian Gao,Richeng Xuan,Zhaolu Kang,Dingshi Liao,Wenxin Huang,Zongmou Huang,Yangdi Xu,Bowen Qin,Zheqi He,Xi Yang,Changjin Li*

Main category: cs.CL

TL;DR: LaoBench是首个针对老挝语的大规模多维度基准测试数据集，用于评估大型语言模型在老挝语的理解与推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在低资源语言，尤其是东南亚语言如老挝语上的评估严重不足。

Method: 构建了包含1.7万余样本的多维度数据集，涵盖知识应用、基础教育和多语言翻译，结合专家人工与自动化验证保证数据质量，并设立开源与闭源评测平台。

Result: 对多种先进模型进行评测，发现现有模型在老挝语处理上存在显著挑战。

Conclusion: LaoBench为东南亚低资源语言的AI研究提供了重要基准，促进相关技术的发展。

Abstract: The rapid advancement of large language models (LLMs) has not been matched by their evaluation in low-resource languages, especially Southeast Asian languages like Lao. To fill this gap, we introduce LaoBench, the first large-scale, high-quality, and multidimensional benchmark dataset dedicated to assessing LLMs' comprehensive language understanding and reasoning abilities in Lao. LaoBench comprises over 17,000 carefully curated samples spanning three core dimensions: knowledge application, K12 foundational education, and bilingual translation among Lao, Chinese, and English. The dataset is divided into open-source and closed-source subsets, with the closed-source portion enabling black-box evaluation on an official platform to ensure fairness and data security. Our data construction pipeline integrates expert human curation with automated agent-assisted verification, ensuring linguistic accuracy, cultural relevance, and educational value. Benchmarking multiple state-of-the-art LLMs on LaoBench reveals that current models still face significant challenges in mastering Lao across diverse tasks. We hope LaoBench will catalyze further research and development of AI technologies for underrepresented Southeast Asian languages.

</details>


### [68] [M-DAIGT: A Shared Task on Multi-Domain Detection of AI-Generated Text](https://arxiv.org/abs/2511.11340)
*Salima Lamsiyah,Saad Ezzini,Abdelkader El Mahdaouy,Hamza Alami,Abdessamad Benlahbib,Samir El Amrany,Salmane Chafik,Hicham Hammouchi*

Main category: cs.CL

TL;DR: 提出了M-DAIGT共享任务，旨在多领域检测AI生成文本，发布了包含3万样本的新数据集，并介绍了参与团队及其方法。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型生成流利文本能力增强，信息真实性和学术研究受到挑战，迫切需要有效检测AI生成文本的方法。

Method: 设计了两个二分类子任务：新闻文章检测和学术写作检测，构建了由现代LLM生成的平衡数据集，吸引多团队参与并采用多样化方法进行检测。

Result: 共有46支团队注册，4支团队提交最终结果，均参与了两个子任务，文中描述了这些团队采用的具体检测方法。

Conclusion: M-DAIGT任务有效推动了跨领域AI生成文本检测研究，未来工作将优化方法并拓展应用领域，以提升检测性能。

Abstract: The generation of highly fluent text by Large Language Models (LLMs) poses a significant challenge to information integrity and academic research. In this paper, we introduce the Multi-Domain Detection of AI-Generated Text (M-DAIGT) shared task, which focuses on detecting AI-generated text across multiple domains, particularly in news articles and academic writing. M-DAIGT comprises two binary classification subtasks: News Article Detection (NAD) (Subtask 1) and Academic Writing Detection (AWD) (Subtask 2). To support this task, we developed and released a new large-scale benchmark dataset of 30,000 samples, balanced between human-written and AI-generated texts. The AI-generated content was produced using a variety of modern LLMs (e.g., GPT-4, Claude) and diverse prompting strategies. A total of 46 unique teams registered for the shared task, of which four teams submitted final results. All four teams participated in both Subtask 1 and Subtask 2. We describe the methods employed by these participating teams and briefly discuss future directions for M-DAIGT.

</details>


### [69] [Studies with impossible languages falsify LMs as models of human language](https://arxiv.org/abs/2511.11389)
*Jeffrey S. Bowers,Jeff Mitchell*

Main category: cs.CL

TL;DR: 本论文审视了语言模型（LM）和婴儿对自然语言与非自然语言学习难度的差异，指出语言模型缺乏人类的归纳偏见，因此对语言的学习表现不同。


<details>
  <summary>Details</summary>
Motivation: 探讨为什么语言模型学自然语言与非自然语言的能力存在差异，及其与人类语言习得能力的联系。

Method: 回顾Futrell与Mahowald的研究以及相关文献，分析语言模型学习不同类型语言的表现。

Result: 发现语言模型对自然语言和许多非自然语言的学习表现相似，而较难学的非自然语言仅因复杂或随机性更高，并不是语言模型本质上的限制。

Conclusion: 语言模型缺少支持人类语言习得的归纳偏见，解释了其学习语言的表现差异。

Abstract: According to Futrell and Mahowald [arXiv:2501.17047], both infants and language models (LMs) find attested languages easier to learn than impossible languages that have unnatural structures. We review the literature and show that LMs often learn attested and many impossible languages equally well. Difficult to learn impossible languages are simply more complex (or random). LMs are missing human inductive biases that support language acquisition.

</details>


### [70] [MajinBook: An open catalogue of digital world literature with likes](https://arxiv.org/abs/2511.11412)
*Antoine Mazières,Thierry Poibeau*

Main category: cs.CL

TL;DR: 该论文介绍了MajinBook，这是一个开放目录，旨在通过连接大量影子图书馆的元数据与Goodreads的结构化书目信息，创建一个高精度的英文书籍语料库，支持社会科学和文化分析研究。


<details>
  <summary>Details</summary>
Motivation: 解决传统语料库（如HathiTrust）存在的偏见问题，并利用庞大且多样的影子图书馆资源，为计算社会科学和文化分析提供丰富且高质量的书籍数据。

Method: 将影子图书馆的元数据与Goodreads数据链接生成包含53.9万条三百年英文书籍引用的语料库，重点选用数字原生的EPUB格式文件保证机器可读性，并附带法语、德语、西班牙语次级数据集。

Result: 成功构建了一个包含丰富书籍信息（首次出版日期、类别、评分和评论等）的高精度大规模语料库，并评估了数据链接的准确性，所有数据均公开发布。

Conclusion: MajinBook为学术研究中的文本和数据挖掘提供了合法且高质量的资源，符合法律框架要求，有助于推动计算社会科学与文化分析领域的发展。

Abstract: This data paper introduces MajinBook, an open catalogue designed to facilitate the use of shadow libraries--such as Library Genesis and Z-Library--for computational social science and cultural analytics. By linking metadata from these vast, crowd-sourced archives with structured bibliographic data from Goodreads, we create a high-precision corpus of over 539,000 references to English-language books spanning three centuries, enriched with first publication dates, genres, and popularity metrics like ratings and reviews. Our methodology prioritizes natively digital EPUB files to ensure machine-readable quality, while addressing biases in traditional corpora like HathiTrust, and includes secondary datasets for French, German, and Spanish. We evaluate the linkage strategy for accuracy, release all underlying data openly, and discuss the project's legal permissibility under EU and US frameworks for text and data mining in research.

</details>


### [71] [Proactive Hearing Assistants that Isolate Egocentric Conversations](https://arxiv.org/abs/2511.11473)
*Guilin Hu,Malek Itani,Tuochao Chen,Shyamnath Gollakota*

Main category: cs.CL

TL;DR: 本文提出了主动听力辅助系统，能够自动识别和分离佩戴者的对话伙伴，无需明确提示，基于双耳音频和佩戴者自身语音锚定，实现实时多说话人环境中的声音分离。


<details>
  <summary>Details</summary>
Motivation: 传统听力辅助设备缺乏对多说话人环境中的主动适应能力，难以自动区分有效对话伙伴，提升佩戴者的听觉体验。

Method: 采用基于自语音锚定的双模型架构：一个轻量快速模型每12.5毫秒运行，低延迟识别对话伙伴；一个较慢模型捕捉对话的长时动态，结合轮换发言和对话动态推断对话者。

Result: 在11名参与者的6.8小时双耳自我视角音频数据的2人及3人对话测试集中，实现了有效识别和隔离对话伙伴，展示了系统在多对话环境中的泛化能力。

Conclusion: 本文工作推动了主动适应对话动态的听力辅助设备的研究，有助于提升用户在复杂听觉场景下的听觉参与感与体验。

Abstract: We introduce proactive hearing assistants that automatically identify and separate the wearer's conversation partners, without requiring explicit prompts. Our system operates on egocentric binaural audio and uses the wearer's self-speech as an anchor, leveraging turn-taking behavior and dialogue dynamics to infer conversational partners and suppress others. To enable real-time, on-device operation, we propose a dual-model architecture: a lightweight streaming model runs every 12.5 ms for low-latency extraction of the conversation partners, while a slower model runs less frequently to capture longer-range conversational dynamics. Results on real-world 2- and 3-speaker conversation test sets, collected with binaural egocentric hardware from 11 participants totaling 6.8 hours, show generalization in identifying and isolating conversational partners in multi-conversation settings. Our work marks a step toward hearing assistants that adapt proactively to conversational dynamics and engagement. More information can be found on our website: https://proactivehearing.cs.washington.edu/

</details>


### [72] [W2S-AlignTree: Weak-to-Strong Inference-Time Alignment for Large Language Models via Monte Carlo Tree Search](https://arxiv.org/abs/2511.11518)
*Zhenyu Ding,Yuhao Wang,Tengyue Xiao,Haoying Wang,Guojun Ma,Mingyang Wan,Caigui Jiang,Ning Ding*

Main category: cs.CL

TL;DR: 提出了W2S-AlignTree，一种基于蒙特卡罗树搜索和弱到强泛化的推理时对齐框架，实现了大语言模型输出与人类偏好的细粒度动态控制。


<details>
  <summary>Details</summary>
Motivation: 大语言模型输出常与人类偏好不符，训练时对齐方法如RLHF成本高、难扩展，且推理时缺乏动态控制，迫切需要可扩展且灵活的对齐机制。

Method: 创新性地将大语言模型对齐视为生成搜索树中的最优启发式搜索问题，结合弱模型的实时步级信号作为对齐代理，并引入熵感知探索机制，实现对强模型生成的细粒度引导，无需修改模型参数。

Result: 在情感生成、摘要和指令遵循等任务中，W2S-AlignTree优于多种强基线，Llama3-8B在摘要任务上的表现提升了15.9%，从1.89提升至2.19。

Conclusion: W2S-AlignTree有效实现了推理时对齐，提升了大语言模型输出质量，为模型动态对齐和灵活控制提供了新思路。

Abstract: Large Language Models (LLMs) demonstrate impressive capabilities, yet their outputs often suffer from misalignment with human preferences due to the inadequacy of weak supervision and a lack of fine-grained control. Training-time alignment methods like Reinforcement Learning from Human Feedback (RLHF) face prohibitive costs in expert supervision and inherent scalability limitations, offering limited dynamic control during inference. Consequently, there is an urgent need for scalable and adaptable alignment mechanisms. To address this, we propose W2S-AlignTree, a pioneering plug-and-play inference-time alignment framework that synergistically combines Monte Carlo Tree Search (MCTS) with the Weak-to-Strong Generalization paradigm for the first time. W2S-AlignTree formulates LLM alignment as an optimal heuristic search problem within a generative search tree. By leveraging weak model's real-time, step-level signals as alignment proxies and introducing an Entropy-Aware exploration mechanism, W2S-AlignTree enables fine-grained guidance during strong model's generation without modifying its parameters. The approach dynamically balances exploration and exploitation in high-dimensional generation search trees. Experiments across controlled sentiment generation, summarization, and instruction-following show that W2S-AlignTree consistently outperforms strong baselines. Notably, W2S-AlignTree raises the performance of Llama3-8B from 1.89 to 2.19, a relative improvement of 15.9 on the summarization task.

</details>


### [73] [PRBench: Large-Scale Expert Rubrics for Evaluating High-Stakes Professional Reasoning](https://arxiv.org/abs/2511.11562)
*Afra Feyza Akyürek,Advait Gosai,Chen Bo Calvin Zhang,Vipul Gupta,Jaehwan Jeong,Anisha Gunjal,Tahseen Rabbani,Maria Mazzone,David Randolph,Mohammad Mahmoudi Meymand,Gurshaan Chattha,Paula Rodriguez,Diego Mares,Pavit Singh,Michael Liu,Subodh Chawla,Pete Cline,Lucy Ogaz,Ernesto Hernandez,Zihao Wang,Pavi Bhatter,Marcos Ayestaran,Bing Liu,Yunzhong He*

Main category: cs.CL

TL;DR: 本文介绍了PRBench，一个涵盖金融和法律领域的开放性、现实性专业推理基准，包含1100项由专家创作的任务和19356条评分标准，评估发现现有顶尖模型在高难度任务上表现较差。


<details>
  <summary>Details</summary>
Motivation: 当前模型评测多依赖学术基准，缺乏对法律和金融等高风险专业领域开放性、经济重要任务的评估，难以反映实际应用中性能。

Method: 构建了PRBench，涵盖1100个专家编写的任务和19356条评分标准，任务由182名具备专业资质的从业者贡献，跨越114个国家和47个美国司法辖区，且评分标准经过严格专家验证。

Result: 评估了20个顶尖模型，发现最高得分仅为0.39（金融）和0.37（法律），模型能力在细分领域差异显著，普遍存在判断不准确、缺乏推理透明和推理不完整等问题。

Conclusion: PRBench展示了目前模型在专业领域实际应用中的显著不足，指出模型可靠性存在严重缺陷，强调需要提升模型的准确性和推理能力以满足专业环境需求。

Abstract: Frontier model progress is often measured by academic benchmarks, which offer a limited view of performance in real-world professional contexts. Existing evaluations often fail to assess open-ended, economically consequential tasks in high-stakes domains like Legal and Finance, where practical returns are paramount. To address this, we introduce Professional Reasoning Bench (PRBench), a realistic, open-ended, and difficult benchmark of real-world problems in Finance and Law. We open-source its 1,100 expert-authored tasks and 19,356 expert-curated criteria, making it, to our knowledge, the largest public, rubric-based benchmark for both legal and finance domains. We recruit 182 qualified professionals, holding JDs, CFAs, or 6+ years of experience, who contributed tasks inspired by their actual workflows. This process yields significant diversity, with tasks spanning 114 countries and 47 US jurisdictions. Our expert-curated rubrics are validated through a rigorous quality pipeline, including independent expert validation. Subsequent evaluation of 20 leading models reveals substantial room for improvement, with top scores of only 0.39 (Finance) and 0.37 (Legal) on our Hard subsets. We further catalog associated economic impacts of the prompts and analyze performance using human-annotated rubric categories. Our analysis shows that models with similar overall scores can diverge significantly on specific capabilities. Common failure modes include inaccurate judgments, a lack of process transparency and incomplete reasoning, highlighting critical gaps in their reliability for professional adoption.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [74] [Peer Code Review in Research Software Development: The Research Software Engineer Perspective](https://arxiv.org/abs/2511.10781)
*Md Ariful Islam Malik,Jeffrey C. Carver,Nasir U. Eisty*

Main category: cs.SE

TL;DR: 本文调查了研究软件工程师（RSE）对同行代码审查的看法，揭示了他们在实践中面临的挑战和独特需求，提出改进措施以促进代码审查的采纳。


<details>
  <summary>Details</summary>
Motivation: 研究软件质量和可维护性受限于不断变化的需求、复杂输入及遗留依赖，尽管同行代码审查可提高质量，但RSE对此的采纳情况尚未被充分研究。

Method: 通过设计涵盖先前研究内容且针对RSE定制问题的问卷调查，共收集了61份有效答卷，分析了RSE对同行代码审查的观点和实践。

Result: 调查结果不仅验证了先前相关研究发现，还揭示了RSE在代码审查中面临的独特挑战和实践情况，与更广泛的开发者群体存在差异。

Conclusion: 同行代码审查对提升研究软件的质量和可靠性至关重要，通过优化结构化流程、改进工具及提供针对性培训，能够有效促进RSE对代码审查的接受和实施。

Abstract: Background: Research software is crucial for enabling research discoveries and supporting data analysis, simulation, and interpretation across domains. However, evolving requirements, complex inputs, and legacy dependencies hinder the software quality and maintainability. While peer code review can improve software quality, its adoption by research software engineers (RSEs) remains unexplored. Aims: This study explores RSE perspectives on peer code review, focusing on their practices, challenges, and potential improvements. Building on prior work, it aims to uncover how RSEs insights differ from those of other research software developers and identify factors that can enhance code review adoption in this domain. Method: We surveyed RSEs to gather insights into their perspectives on peer code review. The survey design aligned with previous research to enable comparative analysis while including additional questions tailored to RSEs. Results: We received 61 valid responses from the survey. The findings align with prior research while uncovering unique insights about the challenges and practices of RSEs compared to broader developer groups. Conclusions: Peer code review is vital in improving research software's quality, maintainability, and reliability. Despite the unique challenges RSEs face, addressing these through structured processes, improved tools, and targeted training can enhance peer review adoption and effectiveness in research software development.

</details>


### [75] [Towards a Human-in-the-Loop Framework for Reliable Patch Evaluation Using an LLM-as-a-Judge](https://arxiv.org/abs/2511.10865)
*Sherry Shi,Renyao Wei,Michele Tufano,José Cambronero,Runxiang Cheng,Franjo Ivančić,Pat Rondon*

Main category: cs.SE

TL;DR: 本文提出了一种结合人类审查的LLM-based补丁有效性判断方法，用以提高自动程序修复评估的可靠性，实验显示该方法与人类共识高度一致且具有较高召回率和精确率。


<details>
  <summary>Details</summary>
Motivation: 现有的自动程序修复评估方法主要依赖于执行测试，难以准确判断补丁的真实性能，且人工标注成本高昂。

Method: 引入人类参与的流程，先由LLM生成针对每个缺陷的评估标准（rubric），经过一次人类审查和修正后，利用调整过的评估标准让LLM判断补丁有效性。

Result: 在Google sanitizer工具检测的问题补丁中，该方法与三人评价者一致性高（Cohen's kappa 0.75），召回率0.94，精确率0.80。对于评价者存在分歧的补丁，结果略有下降但仍可改进（kappa 0.57，召回率0.93，精确率0.65）。

Conclusion: 结合人类审查和LLM的补丁有效性判断方法能够显著提升自动程序修复评估的可靠性，未来仍有提升空间。

Abstract: Reliable evaluation is crucial for advancing Automated Program Repair (APR), but prevailing benchmarks rely on execution-based evaluation methods (unit test pass@k), which fail to capture true patch validity. Determining validity can require costly manual annotation. To reduce this cost, we introduce a human-in-the-loop approach to LLM-based patch validity judgment. Inspired by the observation that human judgment is better aligned when using a shared rubric, we first employ an LLM to generate a per-bug rubric, followed by a one-time human review and optional refinement to this rubric, and then employ an LLM to judge patches using the refined rubric. We apply this approach to assign binary validity labels to patches for issues found by Google sanitizer tools. Our results show that this approach yields substantial agreement with human consensus (Cohen's kappa 0.75), high recall (0.94) and high precision (0.80), when considering patches that have unanimous agreement from 3 human raters on the validity labels. On the full dataset including patches where human raters disagree, we find this approach can still be further improved (Cohen's kappa 0.57, recall 0.93, precision 0.65) and identify possible future directions.

</details>


### [76] [Architecting software monitors for control-flow anomaly detection through large language models and conformance checking](https://arxiv.org/abs/2511.10876)
*Francesco Vitale,Francesco Flammini,Mauro Caporuscio,Nicola Mazzocca*

Main category: cs.SE

TL;DR: 本文提出了一种结合大型语言模型（LLMs）和符合性检查的软件监控方法，以检测运行时控制流异常，提高复杂计算机系统的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现代计算机系统日益复杂，仅在设计阶段验证难以确保运行时行为的可靠性，因此需要通过软件监控检测控制流异常。

Method: 利用LLMs自动将设计时模型与实现代码关联，实现源代码的自动插装，生成事件日志；随后通过符合性检查分析日志，实现控制流异常检测。

Result: 在欧洲铁路交通管理系统案例中，LLM插装实现了高达84.775%的控制流覆盖率，符合性检查异常检测的F1分数达96.610%，AUC达93.515%。

Conclusion: 结合领域知识指导LLMs插装，能够生成高质量日志并实现高效、可靠的控制流异常检测，提升系统的鲁棒性和可信度。

Abstract: Context: Ensuring high levels of dependability in modern computer-based systems has become increasingly challenging due to their complexity. Although systems are validated at design time, their behavior can be different at run-time, possibly showing control-flow anomalies due to "unknown unknowns".
  Objective: We aim to detect control-flow anomalies through software monitoring, which verifies run-time behavior by logging software execution and detecting deviations from expected control flow.
  Methods: We propose a methodology to develop software monitors for control-flow anomaly detection through Large Language Models (LLMs) and conformance checking. The methodology builds on existing software development practices to maintain traditional V&V while providing an additional level of robustness and trustworthiness. It leverages LLMs to link design-time models and implementation code, automating source-code instrumentation. The resulting event logs are analyzed via conformance checking, an explainable and effective technique for control-flow anomaly detection.
  Results: We test the methodology on a case-study scenario from the European Railway Traffic Management System / European Train Control System (ERTMS/ETCS), which is a railway standard for modern interoperable railways. The results obtained from the ERTMS/ETCS case study demonstrate that LLM-based source-code instrumentation can achieve up to 84.775% control-flow coverage of the reference design-time process model, while the subsequent conformance checking-based anomaly detection reaches a peak performance of 96.610% F1-score and 93.515% AUC.
  Conclusion: Incorporating domain-specific knowledge to guide LLMs in source-code instrumentation significantly allowed obtaining reliable and quality software logs and enabled effective control-flow anomaly detection through conformance checking.

</details>


### [77] [Beyond Accuracy: Behavioral Dynamics of Agentic Multi-Hunk Repair](https://arxiv.org/abs/2511.11012)
*Noor Nashid,Daniel Ding,Keheliya Gallaba,Ahmed E. Hassan,Ali Mesbah*

Main category: cs.SE

TL;DR: 本文首次系统性研究了多块代码缺陷自动修复，评估了四种大型语言模型驱动的编码代理在多块漏洞修复任务上的表现，发现修复准确率与漏洞复杂度密切相关，提出了提升修复准确率的辅助工具Maple。


<details>
  <summary>Details</summary>
Motivation: 传统自动程序修复主要关注单块代码缺陷，忽视了在实际系统中普遍存在的多块代码缺陷，这类缺陷修复需求涉及多处独立代码区域的协调修改，挑战更大。

Method: 本文选取Hunk4J数据集的372个多块缺陷，使用Claude Code、Codex、Gemini-cli和Qwen Code四种编码代理进行修复。通过1,488条修复轨迹，采用细粒度指标分析定位、修复准确率、回归行为和操作动态。同时开发Maple工具为代理提供仓库级上下文辅助定位。

Result: 修复准确率差异显著，从25.8%到93.3%不等，且随着缺陷分散度和复杂度升高准确率下降。高表现代理语义一致性更好，有效减少回归错误；失败修复消耗更多资源和时间。引入Maple后，Gemini-cli的修复准确率提升了30%。

Conclusion: 多块缺陷自动修复存在显著挑战，不同代理表现差异大，语义一致性和上下文获取是关键因素。细粒度和轨迹级分析有助于深入理解代理的修复过程，为提升自动修复效果提供了新思路。

Abstract: Automated program repair has traditionally focused on single-hunk defects, overlooking multi-hunk bugs that are prevalent in real-world systems. Repairing these bugs requires coordinated edits across multiple, disjoint code regions, posing substantially greater challenges. We present the first systematic study of LLM-driven coding agents (Claude Code, Codex, Gemini-cli, and Qwen Code) on this task. We evaluate these agents on 372 multi-hunk bugs from the Hunk4J dataset, analyzing 1,488 repair trajectories using fine-grained metrics that capture localization, repair accuracy, regression behavior, and operational dynamics. Results reveal substantial variation: repair accuracy ranges from 25.8% (Qwen Code) to 93.3% (Claude Code) and consistently declines with increasing bug dispersion and complexity. High-performing agents demonstrate superior semantic consistency, achieving positive regression reduction, whereas lower-performing agents often introduce new test failures. Notably, agents do not fail fast; failed repairs consume substantially more resources (39%-343% more tokens) and require longer execution time (43%-427%). Additionally, we developed Maple to provide agents with repository-level context. Empirical results show that Maple improves the repair accuracy of Gemini-cli by 30% through enhanced localization. By analyzing fine-grained metrics and trajectory-level analysis, this study moves beyond accuracy to explain how coding agents localize, reason, and act during multi-hunk repair.

</details>


### [78] [Utilizing LLMs for Industrial Process Automation: A Case Study on Modifying RAPID Programs](https://arxiv.org/abs/2511.11125)
*Salim Fares,Steffen Herbold*

Main category: cs.SE

TL;DR: 本文研究了在专有的工业流程自动化领域中，如何利用少量示例提示法让大型语言模型有效处理专业领域特定语言，从而无需大量训练且保障公司数据安全。


<details>
  <summary>Details</summary>
Motivation: 目前大型语言模型在通用编程语言上的应用广泛，而在工业流程自动化领域专用且高度专业化的编程语言应用研究较少。

Method: 通过少量示例的提示方法（few-shot prompting）来引导大型语言模型解决该领域专业语言中的简单问题，并在本地部署以保护数据安全。

Result: 示例提示方法足以解决该领域中的简单编程问题，且不依赖对领域专用语言的大量训练。

Conclusion: 少量示例提示是一种有效且安全的方案，企业无需大量投入即可利用大型语言模型处理工业流程自动化领域的专业语言任务。

Abstract: How to best use Large Language Models (LLMs) for software engineering is covered in many publications in recent years. However, most of this work focuses on widely-used general purpose programming languages. The utility of LLMs for software within the industrial process automation domain, with highly-specialized languages that are typically only used in proprietary contexts, is still underexplored. Within this paper, we study enterprises can achieve on their own without investing large amounts of effort into the training of models specific to the domain-specific languages that are used. We show that few-shot prompting approaches are sufficient to solve simple problems in a language that is otherwise not well-supported by an LLM and that is possible on-premise, thereby ensuring the protection of sensitive company data.

</details>


### [79] [SQuaD: The Software Quality Dataset](https://arxiv.org/abs/2511.11265)
*Mikel Robredo,Matteo Esposito,Davide Taibi,Rafael Peñaloza,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: 本文介绍了SQuaD，一个多维度、时间感知的软件质量数据集，涵盖450个成熟开源项目，整合了700多个质量指标，支持软件质量的综合分析和预测。


<details>
  <summary>Details</summary>
Motivation: 现有资源多聚焦于单一维度，限制了对软件质量跨时间和多维度的全面分析，亟需一个包含多维度指标和丰富历史数据的综合数据集。

Method: 构建了SQuaD数据集，集成了九个先进的静态分析工具，从多个层级（方法、类、文件、项目）统一提取700多个软件质量指标，涵盖版本控制、缺陷追踪及安全漏洞数据，支持JIT缺陷预测。

Result: 数据集覆盖450个开源项目的63,586个版本发布，提供了丰富的质量和过程度量数据，支持对可维护性、技术债务、软件演化及质量评估的大规模实证研究。

Conclusion: SQuaD数据集为软件质量研究提供了前所未有的多维度、多版本数据基础，促进持续的软件分析演进，并推动自动更新和跨项目质量建模等新兴研究方向。

Abstract: Software quality research increasingly relies on large-scale datasets that measure both the product and process aspects of software systems. However, existing resources often focus on limited dimensions, such as code smells, technical debt, or refactoring activity, thereby restricting comprehensive analyses across time and quality dimensions. To address this gap, we present the Software Quality Dataset (SQuaD), a multi-dimensional, time-aware collection of software quality metrics extracted from 450 mature open-source projects across diverse ecosystems, including Apache, Mozilla, FFmpeg, and the Linux kernel. By integrating nine state-of-the-art static analysis tools, i.e., SonarQube, CodeScene, PMD, Understand, CK, JaSoMe, RefactoringMiner, RefactoringMiner++, and PyRef, our dataset unifies over 700 unique metrics at method, class, file, and project levels. Covering a total of 63,586 analyzed project releases, SQuaD also provides version control and issue-tracking histories, software vulnerability data (CVE/CWE), and process metrics proven to enhance Just-In-Time (JIT) defect prediction. The SQuaD enables empirical research on maintainability, technical debt, software evolution, and quality assessment at unprecedented scale. We also outline emerging research directions, including automated dataset updates and cross-project quality modeling to support the continuous evolution of software analytics. The dataset is publicly available on ZENODO (DOI: 10.5281/zenodo.17566690).

</details>


### [80] [SCRUTINEER: Detecting Logic-Level Usage Violations of Reusable Components in Smart Contracts](https://arxiv.org/abs/2511.11411)
*Xingshuang Lin,Binbin Zhao,Jinwen Wang,Qinge Xie,Xibin Zhao,Shouling Ji*

Main category: cs.SE

TL;DR: 本文提出了SCRUTINEER系统，自动检测智能合约可复用组件（SCRs）中的逻辑级别使用违规，提升发现漏洞的准确性。


<details>
  <summary>Details</summary>
Motivation: 智能合约可复用组件的逻辑级别使用违规导致安全风险，现有方法难以深入理解业务逻辑，亟需自动检测工具。

Method: 设计复合特征提取方法，构建基于大语言模型的知识库，利用检索增强生成技术进行快速且精准的违规检测，结合相似度和冲突推理检查实现准确定位。

Result: SCRUTINEER在3个真实数据集上实现了80.77%的精确率、82.35%的召回率和81.55%的F1分数，表现优异。

Conclusion: SCRUTINEER有效检测SCR的逻辑级别使用违规，具备自动化和实用性，为智能合约安全提供新工具。

Abstract: Smart Contract Reusable Components(SCRs) play a vital role in accelerating the development of business-specific contracts by promoting modularity and code reuse. However, the risks associated with SCR usage violations have become a growing concern. One particular type of SCR usage violation, known as a logic-level usage violation, is becoming especially harmful. This violation occurs when the SCR adheres to its specified usage rules but fails to align with the specific business logic of the current context, leading to significant vulnerabilities. Detecting such violations necessitates a deep semantic understanding of the contract's business logic, including the ability to extract implicit usage patterns and analyze fine-grained logical behaviors. To address these challenges, we propose SCRUTINEER, the first automated and practical system for detecting logic-level usage violations of SCRs. First, we design a composite feature extraction approach that produces three complementary feature representations, supporting subsequent analysis. We then introduce a Large Language Model-powered knowledge construction framework, which leverages comprehension-oriented prompts and domain-specific tools to extract logic-level usage and build the SCR knowledge base. Next, we develop a Retrieval-Augmented Generation-driven inspector, which combines a rapid retrieval strategy with both comprehensive and targeted analysis to identify potentially insecure logic-level usages. Finally, we implement a logic-level usage violation analysis engine that integrates a similarity-based checker and a snapshot-based inference conflict checker to enable accurate and robust detection. We evaluate SCRUTINEER from multiple perspectives on 3 ground-truth datasets. The results show that SCRUTINEER achieves a precision of 80.77%, a recall of 82.35%, and an F1-score of 81.55% in detecting logic-level usage violations of SCRs.

</details>


### [81] [CertiA360: Enhance Compliance Agility in Aerospace Software Development](https://arxiv.org/abs/2511.11550)
*J. Antonio Dantas Macedo,Hugo Fernandes,J. Eduardo Ferreira Ribeiro*

Main category: cs.SE

TL;DR: 本文提出了 CertiA360 工具，通过自动化变更请求管理，实现敏捷方法与航空安全关键系统严格认证要求的融合，提升开发效率并确保合规。


<details>
  <summary>Details</summary>
Motivation: 敏捷方法在航空安全关键系统开发中难以应用，因其需严格遵守 DO-178C 认证标准，需平衡灵活性与规范性。

Method: 设计并验证了 CertiA360 工具，自动管理软件生命周期中的变更请求，确保需求溯源和规章合规，结合行业专家反馈优化实用性。

Result: 实验证明 CertiA360 能减少人工操作，提高对变更需求响应能力，保障符合 DO-178C 标准，提升开发流程效率。

Conclusion: 敏捷方法在经过适当调整后，能够与航空等高度监管领域的安全系统开发及认证要求共存，且能提高效率。

Abstract: Agile methods are characterised by iterative and incremental processes with a strong focus on flexibility and accommodating changing requirements based on either technical, regulatory, or stakeholder feedback. However, integrating Agile methods into safety-critical system development in the aerospace industry presents substantial challenges due to its strict compliance requirements, such as those outlined in the DO-178C standard. To achieve this vision, the flexibility of Agile must align with the rigorous certification guidelines, which emphasize documentation, traceability of requirements across different levels and disciplines, and comprehensive verification and validation (V&V) activities. The research work described in this paper proposes a way of using the strengths of the flexible nature of Agile methods to automate and manage change requests throughout the whole software development lifecycle, ensuring robust traceability, regulatory compliance and ultimately facilitating successful certification. This study proposes CertiA360, a tool designed to help teams improve requirement maturity, automate the changes in traceability, and align with the regulatory objectives. The tool was designed and validated in close collaboration with aerospace industry experts, using their feedback to ensure practical application and real-life effectiveness. The feedback collected demonstrated that the automation given by CertiA360 may reduce manual effort and allow response to changing requirements while ensuring compliance with DO-178C. While the tool is not yet qualified under DO-330 (Tool Qualification), findings suggest that when tailored appropriately, Agile methods can not only coexist with the requirements of safety-system development and certification in highly regulated domains like aerospace, but also add efficiency.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [82] [Towards Assume-Guarantee Verification of Abilities in Stochastic Multi-Agent Systems](https://arxiv.org/abs/2511.10649)
*Wojciech Jamroga,Damian Kurpiewski,Łukasz Mikulski*

Main category: cs.MA

TL;DR: 本文提出了针对带不完美信息的代理在随机环境下的策略能力模型检测的方法，利用假设保证推理将复杂问题分解为若干子问题，并提出了概率交替时间逻辑的假设保证验证方案，证明了其可靠性并讨论了完整性。


<details>
  <summary>Details</summary>
Motivation: 策略能力的模型检测极其困难，在含有不完美信息的随机环境中更是如此。假设保证推理可以有效分解复杂问题，提升验证的可行性。

Method: 本文提出了多种针对带不完美信息的概率交替时间逻辑的假设保证验证方案，证明了这些方案的可靠性，并讨论了它们的完整性。同时提出了新的非概率交替时间逻辑变体，用以表达“最多实现φ”的策略模态。

Result: 提出的假设保证验证方案具备理论上的可靠性，并在方法上扩展了交替时间逻辑表达能力。

Conclusion: 假设保证推理为带不完美信息的概率交替时间逻辑模型检测提供了有效方法，新提出的逻辑变体丰富了策略能力表达的语义工具。

Abstract: Model checking of strategic abilities is a notoriously hard problem, even more so in the realistic case of agents with imperfect information, acting in a stochastic environment. Assume-guarantee reasoning can be of great help here, providing a way to decompose the complex problem into a small set of easier subproblems.
  In this paper, we propose several schemes for assume-guarantee verification of probabilistic alternating-time temporal logic with imperfect information. We prove the soundness of the schemes, and discuss their completeness. On the way, we also propose a new variant of (non-probabilistic) alternating-time logic, where the strategic modalities capture "achieving at most $\varphi$," analogous to Levesque's logic of "only knowing."

</details>


### [83] [Who Gets the Reward, Who Gets the Blame? Evaluation-Aligned Training Signals for Multi-LLM Agents](https://arxiv.org/abs/2511.10687)
*Chih-Hsuan Yang,Tanwi Mallick,Le Chen,Krishnan Raghavan,Azton Wells,Amal Gueroudji,Ian T. Foster,Rajeev Thakur*

Main category: cs.MA

TL;DR: 本文提出了一个理论框架，将多智能体系统中大语言模型的系统级评估转化为智能体信用分配和响应级奖励信号，实现训练信号的局部化、有符号和信用守恒。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统中大语言模型的训练缺少将系统级评估与智能体及消息级学习有效连接的原则方法。

Method: 提出统一合作博弈归因理论和过程奖励建模的方法，基于Shapley值分配信用，并通过错误定位生成针对消息的奖励信号，促进合作并惩罚有害行为。

Result: 方法能够公平分配智能体贡献，生成有界且合作的奖励信号，兼容基于强化学习或偏好的后期训练。

Conclusion: 该工作构建了理论基础和训练信号框架，为多智能体大语言模型训练提供统一、可审计的局部监督途径，后续将进行实验验证。

Abstract: Large Language Models (LLMs) in multi-agent systems (MAS) have shown promise for complex tasks, yet current training methods lack principled ways to connect system-level evaluation with agent-level and message-level learning. We propose a theoretical framework that unifies cooperative game-theoretic attribution with process reward modeling to transform system evaluation into agent credit and then into response-level signals. Unlike prior approaches that rely only on attribution (e.g., Shapley) or step-level labels (e.g., PRM), our method produces local, signed, and credit-conserving signals. In success cases, Shapley-based credit assignment fairly allocates outcomes across agents and is refined into per-message rewards that promote cooperation while discouraging redundancy or sabotage. In failure cases, first-error localization yields repair-aware preferences that penalize harmful steps while rewarding corrective attempts. The resulting signals are bounded, cooperative, and directly compatible with reinforcement-based or preference-based post-training, providing a unified and auditable pathway from global evaluation to local supervision in LLM multi-agent training. Our contribution is conceptual: we present a theoretical foundation and training signals, leaving empirical validation for future work.

</details>


### [84] [Exposing Weak Links in Multi-Agent Systems under Adversarial Prompting](https://arxiv.org/abs/2511.10949)
*Nirmit Arora,Sathvik Joel,Ishan Kavathekar,Palak,Rohan Gandhi,Yash Pandya,Tanuja Ganu,Aditya Kanade,Akshay Nambi*

Main category: cs.MA

TL;DR: 该论文提出SafeAgents框架，系统评估多智能体系统(MAS)的安全性，揭示多智能体设计中存在的安全漏洞，并提出诊断措施Dharma。通过实验证明常见设计模式存在显著漏洞，强调了MAS安全设计的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型（LLM）的多智能体系统在实际应用中越来越普及，其安全性问题日益突出。现有研究主要集中于单智能体安全，缺乏评估多智能体系统特有安全漏洞的统一框架和指标。

Method: 提出统一且可扩展的SafeAgents框架，系统地评估多智能体系统中的设计选择对抗敌对提示的易受攻击性。引入诊断指标Dharma，用于识别多智能体流程中的薄弱环节。使用SafeAgents对五种多智能体架构在四个数据集上进行综合评测。

Result: 发现常见的设计模式带来显著安全漏洞，例如集中式系统将原子指令委托给子智能体时，会掩盖有害目标，从而降低系统的鲁棒性。

Conclusion: 多智能体系统设计中存在明显的安全风险，亟需引入安全感知的设计理念以提升系统的整体安全性。

Abstract: LLM-based agents are increasingly deployed in multi-agent systems (MAS). As these systems move toward real-world applications, their security becomes paramount. Existing research largely evaluates single-agent security, leaving a critical gap in understanding the vulnerabilities introduced by multi-agent design. However, existing systems fall short due to lack of unified frameworks and metrics focusing on unique rejection modes in MAS. We present SafeAgents, a unified and extensible framework for fine-grained security assessment of MAS. SafeAgents systematically exposes how design choices such as plan construction strategies, inter-agent context sharing, and fallback behaviors affect susceptibility to adversarial prompting. We introduce Dharma, a diagnostic measure that helps identify weak links within multi-agent pipelines. Using SafeAgents, we conduct a comprehensive study across five widely adopted multi-agent architectures (centralized, decentralized, and hybrid variants) on four datasets spanning web tasks, tool use, and code generation. Our findings reveal that common design patterns carry significant vulnerabilities. For example, centralized systems that delegate only atomic instructions to sub-agents obscure harmful objectives, reducing robustness. Our results highlight the need for security-aware design in MAS. Link to code is https://github.com/microsoft/SafeAgents

</details>


### [85] [GraphMASAL: A Graph-based Multi-Agent System for Adaptive Learning](https://arxiv.org/abs/2511.11035)
*Biqing Zeng,Mengquan Liu,Zongwei Zhen*

Main category: cs.MA

TL;DR: 本文提出了GraphMASAL，一种基于图的多智能体系统，用于实现个性化自适应学习路径规划，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有智能辅导系统缺乏对学习者复杂知识状态和多样化目标的有效适应能力，缺少结构化推理和知识动态更新，难以生成真正有效的学习路径，且缺乏严格的科学验证。

Method: 引入了动态知识图进行持续的学习者建模，设计由诊断器、规划器和导师组成的多智能体系统，结合基于知识图的双编码密集检索和交叉编码重排序的两阶段神经信息检索，采用认知基础的多源多目标规划引擎和贪心集合覆盖算法保证近似最优解。

Result: 在多样学生配置的自动盲测中，GraphMASAL在学习路径规划方面优于大规模语言模型（LLM）提示和结构消融方法，表现出更好的结构和序列对齐、较高的薄弱知识覆盖率及更低的学习成本，同时在认知诊断上也超过了提示基线。与专家及LLM代理评分的一致性验证了评估方案的有效性。

Conclusion: 将大规模语言模型代理植入动态知识图，并结合教育约束下的优化，可生成可靠、可解释且符合教学逻辑的个性化学习方案，推动个性化和目标导向教育的发展。

Abstract: The advent of Intelligent Tutoring Systems (ITSs) has marked a paradigm shift in education, enabling highly personalized learning pathways. However, true personalization requires adapting to learners' complex knowledge states (multi-source) and diverse goals (multi-sink); existing ITSs often lack the necessary structural-reasoning capability and knowledge dynamism to generate genuinely effective learning paths, and they lack scientifically rigorous validation paradigms. In this paper we propose GraphMASAL (A Graph-based Multi-Agent System for Adaptive Learning), which integrates (i) a dynamic knowledge graph for persistent, stateful learner modeling; (ii) a LangGraph-orchestrated trio of agents (Diagnostician, Planner, Tutor); (iii) a knowledge-graph-grounded two-stage neural IR component (dual-encoder dense retrieval with cross-encoder listwise re-ranking and calibrated score fusion); and (iv) a multi-source multi-sink (MSMS) planning engine with a cognitively grounded cost and an approximation guarantee via greedy set cover. Under blinded automated evaluations with matched inputs and inference settings across diverse student profiles, GraphMASAL consistently outperforms LLM prompting and structured ablations in planning--achieving stronger structural/sequence alignment of learning paths, higher coverage of weak concepts, and lower learning cost--while also surpassing prompt-based baselines in cognitive diagnosis. Agreement with expert/LLM-proxy ratings further supports the validity of our evaluation protocol. These findings indicate that grounding LLM agents in a dynamic knowledge graph, coupled with optimization under educational constraints, yields reliable, interpretable, and pedagogically plausible learning plans, advancing personalized and goal-oriented education.

</details>
