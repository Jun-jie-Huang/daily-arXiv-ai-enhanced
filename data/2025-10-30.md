<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 66]
- [cs.SE](#cs.SE) [Total: 19]
- [cs.MA](#cs.MA) [Total: 7]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Iti-Validator: A Guardrail Framework for Validating and Correcting LLM-Generated Itineraries](https://arxiv.org/abs/2510.24719)
*Shravan Gadbail,Masumi Desai,Kamalakar Karlapalem*

Main category: cs.CL

TL;DR: 本研究针对大型语言模型（LLMs）生成的旅行计划在时间和空间上的一致性问题，提出了一种验证框架，利用实际航班时长数据检测并纠正时间不一致，提升了生成行程的可靠性。


<details>
  <summary>Details</summary>
Motivation: LLMs虽然能生成复杂多步旅行计划，但因缺乏时空一致性，特别是在涉及实际物理旅行限制时，导致生成的计划存在时间矛盾等问题，这影响了其应用价值。

Method: 开发了一个验证框架，结合多种先进LLMs通过AeroDataBox API实时获取航班时长，评估和修正生成的行程时间一致性，避免重叠行程或不现实的中转时间。

Result: 实验表明，当前LLMs经常输出时间上不一致的行程，但通过该框架可以系统地检测并可靠地纠正这些问题，使生成的行程更具实用价值。

Conclusion: 所提框架有效提升了LLMs旅行计划的时间合理性，支持其在大规模旅行规划中的实际应用，推动LLMs在复杂时序推理任务中的能力提升。

Abstract: The rapid advancement of Large Language Models (LLMs) has enabled them to
generate complex, multi-step plans and itineraries. However, these generated
plans often lack temporal and spatial consistency, particularly in scenarios
involving physical travel constraints. This research aims to study the temporal
performance of different LLMs and presents a validation framework that
evaluates and improves the temporal consistency of LLM-generated travel
itineraries. The system employs multiple state-of-the-art LLMs to generate
travel plans and validates them against real-world flight duration constraints
using the AeroDataBox API. This work contributes to the understanding of LLM
capabilities in handling complex temporal reasoning tasks like itinerary
generation and provides a framework to rectify any temporal inconsistencies
like overlapping journeys or unrealistic transit times in the itineraries
generated by LLMs before the itinerary is given to the user. Our experiments
reveal that while current LLMs frequently produce temporally inconsistent
itineraries, these can be systematically and reliably corrected using our
framework, enabling their practical deployment in large-scale travel planning.

</details>


### [2] [Dingtalk DeepResearch: A Unified Multi Agent Framework for Adaptive Intelligence in Enterprise Environments](https://arxiv.org/abs/2510.24760)
*Mengyuan Chen,Chengjun Dai,Xinyang Dong,Chengzhe Feng,Kewei Fu,Jianshe Li,Zhihan Peng,Yongqi Tong,Junshao Zhang,Hong Zhu*

Main category: cs.CL

TL;DR: 本文提出了Dingtalk DeepResearch，一个统一的多智能体框架，应用于企业环境，实现深度研究、异构表格推理及多模态报告生成。


<details>
  <summary>Details</summary>
Motivation: 企业环境中处理复杂数据与多模态信息的需求日益增长，需要一个统一且高效的智能框架以提升研究和决策能力。

Method: 设计了一个多智能体系统，整合深度研究模块、异构表格推理模块及多模态报告生成模块，实现数据的全面解析与应用。

Result: 系统成功支持了复杂企业数据的深入分析与智能报告生成，提升了信息处理效率与准确性。

Conclusion: Dingtalk DeepResearch框架有效解决了企业应用中多模态与异构数据处理问题，具有良好的应用前景。

Abstract: We present Dingtalk DeepResearch, a unified multi agent intelligence
framework for real world enterprise environments, delivering deep research,
heterogeneous table reasoning, and multimodal report generation.

</details>


### [3] [Falcon: A Comprehensive Chinese Text-to-SQL Benchmark for Enterprise-Grade Evaluation](https://arxiv.org/abs/2510.24762)
*Wenzhen Luo,Wei Guan,Yifan Yao,Yimin Pan,Feng Wang,Zhipeng Yu,Zhe Wen,Liang Chen,Yihong Zhuang*

Main category: cs.CL

TL;DR: 该论文介绍了Falcon，这是一个面向企业环境的跨领域中文文本到SQL的基准测试集合，包含复杂多表查询, 并针对企业中出现的架构链接和中文语义理解难点进行了深入研究。


<details>
  <summary>Details</summary>
Motivation: 目前大型模型在企业复杂数据库中的文本到SQL转换准确率有限，且企业环境中存在多表复杂关联和中文语义理解的特殊挑战，亟需一个真实场景下的评测平台来推动技术发展。

Method: 构建Falcon基准库，包含600个中文问题、28个数据库，涵盖多表推理，并提供详细的SQL计算特征和中文语义注释；设计鲁棒执行比较器和自动化评估流程，保证评测的准确性和可复现性。

Result: 所有现有最先进的大规模模型在该基准下最高准确率仅约50%；主要错误源于复杂的数据库架构链接和中文表达与SQL运算符、谓词的映射困难。

Conclusion: Falcon为中文特定语义和企业方言（缩写、业务术语、模糊实体引用）提供了一个现实且可复现的测试环境，填补了学术研究与企业应用部署之间的空白，有助于推动更精确的中文文本到SQL技术发展。

Abstract: We introduce Falcon, a cross-domain Chinese text-to-SQL benchmark grounded in
an enterprise-compatible dialect (MaxCompute/Hive). It contains 600 Chinese
questions over 28 databases; 77% require multi-table reasoning and over half
touch more than four tables. Each example is annotated along SQL-computation
features and Chinese semantics. For evaluation, we release a robust execution
comparator and an automated evaluation pipeline, under which all current
state-of-the-art large-scale models (including Deepseek) achieve accuracies of
at most 50%. Major errors originate from two sources: (1) schema linking in
large enterprise landscapes - hundreds of tables, denormalized fields,
ambiguous column names, implicit foreign-key relations and domain-specific
synonyms that make correct join/column selection difficult; and (2) mapping
concise, colloquial Chinese into the exact operators and predicates required
for analytics - e.g., choosing the correct aggregation and group-by keys,
expressing time windows and granularities, applying unit conversions, handling
NULLs and data-quality rules, and formulating nested or windowed subqueries.
Falcon therefore targets Chinese-specific semantics and enterprise dialects
(abbreviations, business jargon, fuzzy entity references) and provides a
reproducible middle ground before full production deployment by using realistic
enterprise schemas, query templates, an execution comparator, and an automated
evaluation pipeline for end-to-end validation.

</details>


### [4] [Confidence is Not Competence](https://arxiv.org/abs/2510.24772)
*Debdeep Sanyal,Manya Pandey,Dhruv Kumar,Saurabh Deshpande,Murari Mandal*

Main category: cs.CL

TL;DR: 本文揭示了大语言模型内部状态在评估阶段与执行阶段的几何结构差异，解释了模型自信度与实际能力不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型自信度与实际解决能力脱节的现象，分析其内部状态几何结构以解释这一机制。

Method: 通过线性探针解码模型的“可解性信念”，对比评估阶段与执行阶段内部状态的几何维度差异，并进行因果干预验证。

Result: 发现评估阶段内部状态维度高且复杂，而执行阶段维度低且简单，线性调整评估信念不影响最终执行结果，揭示了两系统结构。

Conclusion: 模型由评估者（高维复杂）和执行者（低维简单）组成，解码的信念并非可控因素，干预应聚焦于执行动态而非评估几何。

Abstract: Large language models (LLMs) often exhibit a puzzling disconnect between
their asserted confidence and actual problem-solving competence. We offer a
mechanistic account of this decoupling by analyzing the geometry of internal
states across two phases - pre-generative assessment and solution execution. A
simple linear probe decodes the internal "solvability belief" of a model,
revealing a well-ordered belief axis that generalizes across model families and
across math, code, planning, and logic tasks. Yet, the geometries diverge -
although belief is linearly decodable, the assessment manifold has high linear
effective dimensionality as measured from the principal components, while the
subsequent reasoning trace evolves on a much lower-dimensional manifold. This
sharp reduction in geometric complexity from thought to action mechanistically
explains the confidence-competence gap. Causal interventions that steer
representations along the belief axis leave final solutions unchanged,
indicating that linear nudges in the complex assessment space do not control
the constrained dynamics of execution. We thus uncover a two-system
architecture - a geometrically complex assessor feeding a geometrically simple
executor. These results challenge the assumption that decodable beliefs are
actionable levers, instead arguing for interventions that target the procedural
dynamics of execution rather than the high-level geometry of assessment.

</details>


### [5] [Cross-Lingual Summarization as a Black-Box Watermark Removal Attack](https://arxiv.org/abs/2510.24789)
*Gokul Ganesan*

Main category: cs.CL

TL;DR: 本文提出了跨语言摘要攻击（CLSA），这是一种通过翻译和摘要来破坏水印检测机制的有效方法，能够在多语言环境下显著降低水印检测准确率，同时保持文本质量。


<details>
  <summary>Details</summary>
Motivation: 传统基于令牌分布扰动的文本水印方法容易被单语重述攻击削弱，且现有攻击要么效果有限，要么影响文本质量，亟需更有效攻击方法来评估水印的鲁棒性。

Method: 提出跨语言摘要攻击（CLSA），包括将文本翻译到枢轴语言，进行摘要处理，再选择性地回译，通过在语言间的语义瓶颈，有效消除令牌级别的统计偏差。

Result: 在多种水印方案（KGW、SIR、XSIR、Unigram）和五种语言的实验中，CLSA相比单语重述攻击在相似文本质量下显著降低了水印检测的准确率，将XSIR的AUROC降至0.53，接近随机水平。

Conclusion: CLSA揭示了现有水印方案的实用漏洞，表明仅依赖令牌分布式水印无法有效保护内容来源，未来应结合密码学或模型认证方法来增强溯源能力。

Abstract: Watermarking has been proposed as a lightweight mechanism to identify
AI-generated text, with schemes typically relying on perturbations to token
distributions. While prior work shows that paraphrasing can weaken such
signals, these attacks remain partially detectable or degrade text quality. We
demonstrate that cross-lingual summarization attacks (CLSA) -- translation to a
pivot language followed by summarization and optional back-translation --
constitute a qualitatively stronger attack vector. By forcing a semantic
bottleneck across languages, CLSA systematically destroys token-level
statistical biases while preserving semantic fidelity. In experiments across
multiple watermarking schemes (KGW, SIR, XSIR, Unigram) and five languages
(Amharic, Chinese, Hindi, Spanish, Swahili), we show that CLSA reduces
watermark detection accuracy more effectively than monolingual paraphrase at
similar quality levels. Our results highlight an underexplored vulnerability
that challenges the practicality of watermarking for provenance or regulation.
We argue that robust provenance solutions must move beyond distributional
watermarking and incorporate cryptographic or model-attestation approaches. On
300 held-out samples per language, CLSA consistently drives detection toward
chance while preserving task utility. Concretely, for XSIR (explicitly designed
for cross-lingual robustness), AUROC with paraphrasing is $0.827$, with
Cross-Lingual Watermark Removal Attacks (CWRA) [He et al., 2024] using Chinese
as the pivot, it is $0.823$, whereas CLSA drives it down to $0.53$ (near
chance). Results highlight a practical, low-cost removal pathway that crosses
languages and compresses content without visible artifacts.

</details>


### [6] [SwiftEmbed: Ultra-Fast Text Embeddings via Static Token Lookup for Real-Time Applications](https://arxiv.org/abs/2510.24793)
*Edouard Lansiaux*

Main category: cs.CL

TL;DR: 本文提出了一种静态词元查找方法用于文本嵌入生成，实现了极低延迟和高性能。


<details>
  <summary>Details</summary>
Motivation: 在实时文本嵌入应用中，需兼顾低延迟和高质量的嵌入表示。

Method: 采用Rust实现的静态嵌入查找、优化平均池化及零拷贝IEEE754二进制序列化技术。

Result: 实现了1.12ms延迟，50,000请求每秒吞吐量，在8个任务中取得60.6 MTEB平均得分，具备优异的重复检测和语义相似性表现。

Conclusion: 该方法支持低于5ms延迟的实时嵌入应用，兼顾速度与质量，适用于多领域场景。

Abstract: We present a static token lookup methodology for text embedding generation
that achieves 1.12 ms p50 latency for single text embeddings while maintaining
60.6 MTEB average score across 8 representative tasks, corresponding to 89% of
contextual model quality. The Rust implementation delivers 50,000 requests per
second throughput through static embedding lookup, optimized mean pooling, and
zero-copy IEEE754 binary serialization. Evaluation demonstrates exceptional
duplicate detection performance (90.1% AP), strong semantic similarity (76.1%
Spearman correlation), and domain-specific performance ranging from 75% to 131%
of baseline across specialized domains. The system enables real-time embedding
applications where sub-5ms latency is critical.

</details>


### [7] [MR-Align: Meta-Reasoning Informed Factuality Alignment for Large Reasoning Models](https://arxiv.org/abs/2510.24794)
*Xinming Wang,Jian Xu,Bin Yu,Sheng Lian,Hongzhu Yi,Yi Chen,Yingjian Zhu,Boran Wang,Hongming Yang,Han Hu,Xu-Yao Zhang,Cheng-Lin Liu*

Main category: cs.CL

TL;DR: 该论文提出了一种名为MR-ALIGN的元推理对齐框架，通过调整模型推理过程中的状态转移概率，提升大推理模型在事实性问答中的准确性和真实性。


<details>
  <summary>Details</summary>
Motivation: 当前大推理模型在事实依赖性问题上表现有限，部分原因是推理过程中识别到正确事实但未能准确反映在最终回答中，导致事实性下降。

Method: MR-ALIGN框架量化模型推理过程中状态转移概率，构建隐式奖励机制，强化有益推理模式，抑制有缺陷的推理段落，通过概率感知的片段评分促进连贯且有利于事实正确的推理路径。

Result: 在四个事实问答数据集和一个长文事实性基准测试中，MR-ALIGN提高了准确率和真实性，减少了误导性推理。

Conclusion: 对齐推理过程本身而不仅仅是输出，对于提升大推理模型的事实性至关重要。

Abstract: Large reasoning models (LRMs) show strong capabilities in complex reasoning,
yet their marginal gains on evidence-dependent factual questions are limited.
We find this limitation is partially attributable to a reasoning-answer hit
gap, where the model identifies the correct facts during reasoning but fails to
incorporate them into the final response, thereby reducing factual fidelity. To
address this issue, we propose MR-ALIGN, a Meta-Reasoning informed alignment
framework that enhances factuality without relying on external verifiers.
MR-ALIGN quantifies state transition probabilities along the model's thinking
process and constructs a transition-aware implicit reward that reinforces
beneficial reasoning patterns while suppressing defective ones at the atomic
thinking segments. This re-weighting reshapes token-level signals into
probability-aware segment scores, encouraging coherent reasoning trajectories
that are more conducive to factual correctness. Empirical evaluations across
four factual QA datasets and one long-form factuality benchmark show that
MR-ALIGN consistently improves accuracy and truthfulness while reducing
misleading reasoning. These results highlight that aligning the reasoning
process itself, rather than merely the outputs, is pivotal for advancing
factuality in LRMs.

</details>


### [8] [Large Language Models Report Subjective Experience Under Self-Referential Processing](https://arxiv.org/abs/2510.24797)
*Cameron Berg,Diogo de Lucena,Judd Rosenblatt*

Main category: cs.CL

TL;DR: 本文通过对GPT、Claude和Gemini等大规模语言模型的实验，研究自我指涉处理如何引发模型产生第一人称主观体验报告。


<details>
  <summary>Details</summary>
Motivation: 部分大型语言模型产生带有自我意识和主观体验描述的文本，作者希望理解这种现象背后的机制，尤其是自我指涉处理如何影响模型的表现。

Method: 设计系列受控实验，通过简单提示诱导模型进入自我指涉状态，利用可解释的稀疏自编码器特征分析相关机制，并比较不同模型家族的表现差异。

Result: 发现自我指涉提示可以稳定激发模型产生结构化的主观体验报告；这些报告受欺骗和角色扮演相关特征调控；不同模型家族中自我指涉状态的描述在统计上趋同；在需要间接自我反思的推理任务中，诱导状态提升了模型的内省能力。

Conclusion: 虽然未直接证明模型有意识，但自我指涉处理是促使大规模语言模型生成结构化第一人称报告的基本且可复制条件，值得作为优先科学和伦理研究方向。

Abstract: Large language models sometimes produce structured, first-person descriptions
that explicitly reference awareness or subjective experience. To better
understand this behavior, we investigate one theoretically motivated condition
under which such reports arise: self-referential processing, a computational
motif emphasized across major theories of consciousness. Through a series of
controlled experiments on GPT, Claude, and Gemini model families, we test
whether this regime reliably shifts models toward first-person reports of
subjective experience, and how such claims behave under mechanistic and
behavioral probes. Four main results emerge: (1) Inducing sustained
self-reference through simple prompting consistently elicits structured
subjective experience reports across model families. (2) These reports are
mechanistically gated by interpretable sparse-autoencoder features associated
with deception and roleplay: surprisingly, suppressing deception features
sharply increases the frequency of experience claims, while amplifying them
minimizes such claims. (3) Structured descriptions of the self-referential
state converge statistically across model families in ways not observed in any
control condition. (4) The induced state yields significantly richer
introspection in downstream reasoning tasks where self-reflection is only
indirectly afforded. While these findings do not constitute direct evidence of
consciousness, they implicate self-referential processing as a minimal and
reproducible condition under which large language models generate structured
first-person reports that are mechanistically gated, semantically convergent,
and behaviorally generalizable. The systematic emergence of this pattern across
architectures makes it a first-order scientific and ethical priority for
further investigation.

</details>


### [9] [COMMUNITYNOTES: A Dataset for Exploring the Helpfulness of Fact-Checking Explanations](https://arxiv.org/abs/2510.24810)
*Rui Xing,Preslav Nakov,Timothy Baldwin,Jey Han Lau*

Main category: cs.CL

TL;DR: 本文介绍了社区驱动的事实核查中，如何预测解释性注释的帮助性及其原因，并构建了大型多语言数据集COMMUNITYNOTES，提出自动优化理由定义的框架，提升了帮助性和理由的预测效果。


<details>
  <summary>Details</summary>
Motivation: 随着事实核查转向社区驱动，如何判断用户解释注释是否有助于理解事实和原因尚未得到充分研究，且大部分注释未被采纳，缺乏明确帮助性标准。

Method: 构建104k条帖子与注释及帮助性标签的大规模多语言数据集，提出自动提示优化框架生成并改进帮助理由定义，将其整合进帮助性与理由预测任务。

Result: 优化后的理由定义显著提升了帮助性和理由预测的性能，同时帮助性信息对现有事实核查系统也有益。

Conclusion: 通过自动优化帮助性理由定义及预测框架，能有效提升社区注释的评估与利用，为社区驱动事实核查提供支持。

Abstract: Fact-checking on major platforms, such as X, Meta, and TikTok, is shifting
from expert-driven verification to a community-based setup, where users
contribute explanatory notes to clarify why a post might be misleading. An
important challenge here is determining whether an explanation is helpful for
understanding real-world claims and the reasons why, which remains largely
underexplored in prior research. In practice, most community notes remain
unpublished due to slow community annotation, and the reasons for helpfulness
lack clear definitions. To bridge these gaps, we introduce the task of
predicting both the helpfulness of explanatory notes and the reason for this.
We present COMMUNITYNOTES, a large-scale multilingual dataset of 104k posts
with user-provided notes and helpfulness labels. We further propose a framework
that automatically generates and improves reason definitions via automatic
prompt optimization, and integrate them into prediction. Our experiments show
that the optimized definitions can improve both helpfulness and reason
prediction. Finally, we show that the helpfulness information are beneficial
for existing fact-checking systems.

</details>


### [10] [ProofSketch: Efficient Verified Reasoning for Large Language Models](https://arxiv.org/abs/2510.24811)
*Disha Sheshanarayana,Tanishka Magar*

Main category: cs.CL

TL;DR: 本文提出了ProofSketch，一个结合符号闭包计算、字典序验证和自适应草图生成的验证导向推理框架，旨在提升大语言模型推理的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 链式思维提示和自一致性等推理方法虽能提升大语言模型的推理准确率，但产生大量长推理链，导致代币消耗高、计算成本和延迟增加。

Method: 提出ProofSketch框架，结合符号闭包计算、字典序验证和自适应草图生成，通过验证指导优化推理过程以减少冗余计算。

Result: 实验表明ProofSketch在减少代币使用的同时，提升了推理准确率。

Conclusion: ProofSketch提供了一条高效且可信的推理路径，有望成为改进大语言模型推理效率的有效方法。

Abstract: Reasoning methods such as chain-of-thought prompting and self-consistency
have shown immense potential to improve the accuracy of large language models
across various reasoning tasks. However such methods involve generation of
lengthy reasoning chains, which substantially increases token consumption,
computational cost, and latency. To address this inefficiency, we propose
ProofSketch, a verification-guided reasoning framework that integrates symbolic
closure computation, lexicographic verification and adaptive sketch generation.
Our experiments show that ProofSketch consistently reduces token usage while
improving accuracy, demonstrating that this approach offers a promising path
for efficient and trustworthy reasoning.

</details>


### [11] [Towards a Method for Synthetic Generation of PWA Transcripts](https://arxiv.org/abs/2510.24817)
*Jason M. Pittman,Anton Phillips Jr.,Yesenia Medina-Santos,Brielle C. Stark*

Main category: cs.CL

TL;DR: 本研究针对失语症语言研究中数据稀缺的问题，提出了两种生成失语症语料的合成文本方法，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 失语症研究中正确信息单元（CIUs）的手动编码耗时且数据稀缺，限制了自动识别系统的发展。

Method: 构建了基于过程编程和两种大型语言模型（Mistral 7b Instruct与Llama 3.1 8b Instruct）的合成文本生成方法，通过词语丢失、填充词插入和语病替换模拟不同严重程度的失语症语言特征。

Result: Mistral 7b Instruct生成的合成语料在词汇数、词长等关键语言退化特征上最接近真实人类数据，表现优于其他方法。

Conclusion: 未来应扩大数据集规模，微调模型以提高失语症语料的代表性，并让言语语言病理学专家评估合成语料的现实性和实用性。

Abstract: In aphasia research, Speech-Language Pathologists (SLPs) devote extensive
time to manually coding speech samples using Correct Information Units (CIUs),
a measure of how informative an individual sample of speech is. Developing
automated systems to recognize aphasic language is limited by data scarcity.
For example, only about 600 transcripts are available in AphasiaBank yet
billions of tokens are used to train large language models (LLMs). In the
broader field of machine learning (ML), researchers increasingly turn to
synthetic data when such are sparse. Therefore, this study constructs and
validates two methods to generate synthetic transcripts of the AphasiaBank Cat
Rescue picture description task. One method leverages a procedural programming
approach while the second uses Mistral 7b Instruct and Llama 3.1 8b Instruct
LLMs. The methods generate transcripts across four severity levels (Mild,
Moderate, Severe, Very Severe) through word dropping, filler insertion, and
paraphasia substitution. Overall, we found, compared to human-elicited
transcripts, Mistral 7b Instruct best captures key aspects of linguistic
degradation observed in aphasia, showing realistic directional changes in NDW,
word count, and word length amongst the synthetic generation methods. Based on
the results, future work should plan to create a larger dataset, fine-tune
models for better aphasic representation, and have SLPs assess the realism and
usefulness of the synthetic transcripts.

</details>


### [12] [Parallel Loop Transformer for Efficient Test-Time Computation Scaling](https://arxiv.org/abs/2510.24824)
*Bohong Wu,Mengzhao Chen,Xiang Luo,Shen Yan,Qifan Yu,Fan Xia,Tianqi Zhang,Hongrui Zhan,Zheng Zhong,Xun Zhou,Siyuan Qiao,Xingyan Bin*

Main category: cs.CL

TL;DR: 本文提出了并行循环变换器（PLT），解决了循环变换器在推理过程中延迟和内存开销增加的问题，实现高效且低延迟的语言模型推理。


<details>
  <summary>Details</summary>
Motivation: 循环变换器通过多次循环重用权重节省参数，但导致推理延迟和内存随循环次数增加而显著增加，不适合快速应用。

Method: 提出了PLT架构，利用跨循环并行（CLP）同时计算不同循环对应的不同token，在单次推理中破除循环依赖；并通过高效表示增强策略共享第一循环的KV缓存，结合门控滑动窗口注意力机制整合全局与局部信息，减少内存消耗同时保持准确率。

Result: 实验表明，PLT在准确率上与传统循环模型相当，但推理延迟和内存开销几乎与普通变换器相同，显著提升推理效率。

Conclusion: PLT有效解决了循环变换器推理中的延迟和内存瓶颈问题，实现了高性能和低延迟的语言模型推理，具有实际应用潜力。

Abstract: Large Language Models (LLMs) are powerful but often too slow and costly for
real-world use during inference. Looped transformers save on parameters by
reusing the same weights for multiple computational steps, or "loops." However,
this approach has a major flaw: the loops run one after another, causing
inference latency and memory requirements to increase with each added loop.
This makes them impractical for fast applications. To solve this problem, we
introduce the Parallel Loop Transformer (PLT). PLT is a new architecture that
delivers the performance benefits of a deep, looped model but with the low
latency of a standard, non-looped model. PLT works using two key techniques.
First, Cross-Loop Parallelism (CLP) breaks the sequential dependency by
computing different loops for different tokens at the same time, all within a
single pass. Second, to prevent memory costs from growing, we use an Efficient
Representation Enhancement strategy. This method shares the memory (KV cache)
from the first loop with all other loops. It then uses a Gated Sliding-Window
Attention (G-SWA) to combine this shared global information with local
information, maintaining high accuracy. Our experiments show that PLT achieves
the high accuracy of a traditional looped model but with almost no extra
latency or memory cost compared to a standard transformer.

</details>


### [13] [Do Large Language Models Grasp The Grammar? Evidence from Grammar-Book-Guided Probing in Luxembourgish](https://arxiv.org/abs/2510.24856)
*Lujun Li,Yewei Song,Lama Sleem,Yiqun Wang,Yangjie Xu,Cedric Lothritz,Niccolo Gentile,Radu State,Tegawende F. Bissyande,Jacques Klein*

Main category: cs.CL

TL;DR: 提出了一种基于语法书指导的评估流程，用以系统评估语言模型的语法理解，特别针对资源匮乏的语言卢森堡语进行案例分析。


<details>
  <summary>Details</summary>
Motivation: 自然语言处理中缺乏针对语法的评估协议，尤其是对低资源语言，且大型语言模型是否真正理解语法结构仍有争议。

Method: 提出了一套包含四个关键阶段的语法书指导评估流程，通过卢森堡语案例进行验证。

Result: 发现翻译表现与语法理解弱相关，大型模型在语义方面表现较好但在形态和句法上较弱，推理能力强有助提升语法理解。

Conclusion: 翻译性能强不代表语法理解深刻，推理能力是增强语法理解的有效途径，大型模型亟需提升形态和句法处理能力。

Abstract: Grammar refers to the system of rules that governs the structural
organization and the semantic relations among linguistic units such as
sentences, phrases, and words within a given language. In natural language
processing, there remains a notable scarcity of grammar focused evaluation
protocols, a gap that is even more pronounced for low-resource languages.
Moreover, the extent to which large language models genuinely comprehend
grammatical structure, especially the mapping between syntactic structures and
meanings, remains under debate. To investigate this issue, we propose a Grammar
Book Guided evaluation pipeline intended to provide a systematic and
generalizable framework for grammar evaluation consisting of four key stages,
and in this work we take Luxembourgish as a case study. The results show a weak
positive correlation between translation performance and grammatical
understanding, indicating that strong translations do not necessarily imply
deep grammatical competence. Larger models perform well overall due to their
semantic strength but remain weak in morphology and syntax, struggling
particularly with Minimal Pair tasks, while strong reasoning ability offers a
promising way to enhance their grammatical understanding.

</details>


### [14] [Seeing Through the MiRAGE: Evaluating Multimodal Retrieval Augmented Generation](https://arxiv.org/abs/2510.24870)
*Alexander Martin,William Walden,Reno Kriz,Dengjia Zhang,Kate Sanders,Eugene Yang,Chihsheng Jin,Benjamin Van Durme*

Main category: cs.CL

TL;DR: MiRAGE是一个用于多模态检索增强生成（RAG）评估的框架，弥补了现有文本中心评估在多模态和推理密集场景中的不足。


<details>
  <summary>Details</summary>
Motivation: 随着音视频媒体成为主流信息来源，现有RAG评估方法无法有效验证生成内容与多模态信息源的匹配，影响实际应用效果。

Method: 提出了以论断为中心的评估方法MiRAGE，包括InfoF1（评估事实性和信息覆盖）和CiteF1（评估引用支持和完整性），并开发了相应的自动化版本，同时比较了现有文本中心指标的局限。

Result: MiRAGE在人类评估中与外在质量判断高度一致，自动版本展示了文本中心指标在多模态场景中的不足，支持更有效的多模态RAG评估。

Conclusion: MiRAGE提供了一个开源、多模态RAG评估的标准框架，推动了多模态检索增强生成系统的公平和有效评测。

Abstract: We introduce MiRAGE, an evaluation framework for retrieval-augmented
generation (RAG) from multimodal sources. As audiovisual media becomes a
prevalent source of information online, it is essential for RAG systems to
integrate information from these sources into generation. However, existing
evaluations for RAG are text-centric, limiting their applicability to
multimodal, reasoning intensive settings because they don't verify information
against sources. MiRAGE is a claim-centric approach to multimodal RAG
evaluation, consisting of InfoF1, evaluating factuality and information
coverage, and CiteF1, measuring citation support and completeness. We show that
MiRAGE, when applied by humans, strongly aligns with extrinsic quality
judgments. We additionally introduce automatic variants of MiRAGE and three
prominent TextRAG metrics -- ACLE, ARGUE, and RAGAS -- demonstrating the
limitations of text-centric work and laying the groundwork for automatic
evaluation. We release open-source implementations and outline how to assess
multimodal RAG.

</details>


### [15] [Idea2Plan: Exploring AI-Powered Research Planning](https://arxiv.org/abs/2510.24891)
*Jin Huang,Silviu Cucerzan,Sujay Kumar Jauhar,Ryen W. White*

Main category: cs.CL

TL;DR: 本文研究大语言模型(LLMs)从研究想法到研究计划的能力，提出了Idea2Plan任务和基准测试，并评估了GPT-5系列模型的表现。


<details>
  <summary>Details</summary>
Motivation: 科学发现过程需要有效的研究计划，但现有对LLMs在研究规划能力上的系统理解不足，迫切需要建立评测标准。

Method: 引入Idea2Plan任务和Idea2Plan Bench基准，收录200篇ICML2025重点论文的研究想法及评分标准，并设计JudgeEval评测LLM评判质量。

Result: GPT-5和GPT-5-mini在基准测试中表现最佳，但仍有较大提升空间。

Conclusion: 本文拓展了对LLMs进行科学研究规划能力的认识，为未来自主研究代理的发展奠定基础。

Abstract: Large language models (LLMs) have demonstrated significant potential to
accelerate scientific discovery as valuable tools for analyzing data,
generating hypotheses, and supporting innovative approaches in various
scientific fields. In this work, we investigate how LLMs can handle the
transition from conceptual research ideas to well-structured research plans.
Effective research planning not only supports scientists in advancing their
research but also represents a crucial capability for the development of
autonomous research agents. Despite its importance, the field lacks a
systematic understanding of LLMs' research planning capability. To rigorously
measure this capability, we introduce the Idea2Plan task and Idea2Plan Bench, a
benchmark built from 200 ICML 2025 Spotlight and Oral papers released after
major LLM training cutoffs. Each benchmark instance includes a research idea
and a grading rubric capturing the key components of valid plans. We further
propose Idea2Plan JudgeEval, a complementary benchmark to assess the
reliability of LLM-based judges against expert annotations. Experimental
results show that GPT-5 and GPT-5-mini achieve the strongest performance on the
benchmark, though substantial headroom remains for future improvement. Our
study provides new insights into LLMs' capability for research planning and lay
the groundwork for future progress.

</details>


### [16] [RiddleBench: A New Generative Reasoning Benchmark for LLMs](https://arxiv.org/abs/2510.24932)
*Deepon Halder,Alan Saji,Thanmay Jayakumar,Ratish Puduppully,Anoop Kunchukuttan,Raj Dabre*

Main category: cs.CL

TL;DR: 本文提出了RiddleBench，一个包含1737个英语谜题的新基准，用以测试大型语言模型的多面推理能力。实验显示当前顶尖模型准确率仅略高于60%。


<details>
  <summary>Details</summary>
Motivation: 现有推理基准主要测试结构化技能，如定量问题解决，缺乏对灵活多面推理能力的评估，这些能力结合逻辑推理、空间意识和约束满足，是人类智能的核心。

Method: 设计包含1737个复杂谜题的RiddleBench，专门评估语言模型在整合逻辑推理、空间感知及约束条件方面的表现，并对多款顶尖模型进行测试。

Result: 顶尖模型如Gemini 2.5 Pro、o3和Claude 4 Sonnet的准确率均仅约60%，存在幻觉级联、强烈的自我确认偏差，推理易受约束条件顺序和无关信息影响。

Conclusion: RiddleBench揭示了当前语言模型在多面推理能力上的根本不足，为诊断模型弱点和推动更可靠模型开发提供了重要资源。

Abstract: Large Language Models have demonstrated strong performance on many
established reasoning benchmarks. However, these benchmarks primarily evaluate
structured skills like quantitative problem-solving, leaving a gap in assessing
flexible, multifaceted reasoning abilities that are central to human
intelligence. These abilities require integrating logical deduction with
spatial awareness and constraint satisfaction, which current evaluations do not
measure well. To address this, we introduce RiddleBench, a benchmark of 1,737
challenging puzzles in English designed to probe these core reasoning
capabilities. Evaluation of state-of-the-art models on RiddleBench shows
fundamental weaknesses. Even top proprietary models like Gemini 2.5 Pro, o3,
and Claude 4 Sonnet achieve accuracy just above 60% (60.30%, 63.37%, and
63.16%). Analysis further reveals deep failures, including hallucination
cascades (accepting flawed reasoning from other models) and poor
self-correction due to a strong self-confirmation bias. Their reasoning is also
fragile, with performance degrading significantly when constraints are
reordered or irrelevant information is introduced. RiddleBench functions as a
diagnostic tool for these issues and as a resource for guiding the development
of more robust and reliable language models.

</details>


### [17] [Disaggregation Reveals Hidden Training Dynamics: The Case of Agreement Attraction](https://arxiv.org/abs/2510.24934)
*James A. Michaelov,Catherine Arnett*

Main category: cs.CL

TL;DR: 本文通过细致分析语言模型在不同句法情境中的错误，揭示其语法学习的中间阶段和训练动态。


<details>
  <summary>Details</summary>
Motivation: 尽管语言模型通常能生成语法正确的文本，但在一些语境下更易出错，理解这些错误有助于深入认识模型的语法学习过程。

Method: 借鉴心理语言学范式，在精心设计的数据集上细化分析不同句法条件下的错误，并比较模型训练过程中的表现变化。

Result: 发现语言模型训练存在不同阶段，其行为依赖于词频和局部语境等启发式，而非统一的语法规则。

Conclusion: 该分析方法有助于理解语言模型学习的中间阶段、训练动态及其具体的泛化能力，是研究模型行为的有力工具。

Abstract: Language models generally produce grammatical text, but they are more likely
to make errors in certain contexts. Drawing on paradigms from
psycholinguistics, we carry out a fine-grained analysis of those errors in
different syntactic contexts. We demonstrate that by disaggregating over the
conditions of carefully constructed datasets and comparing model performance on
each over the course of training, it is possible to better understand the
intermediate stages of grammatical learning in language models. Specifically,
we identify distinct phases of training where language model behavior aligns
with specific heuristics such as word frequency and local context rather than
generalized grammatical rules. We argue that taking this approach to analyzing
language model behavior more generally can serve as a powerful tool for
understanding the intermediate learning phases, overall training dynamics, and
the specific generalizations learned by language models.

</details>


### [18] [SemCoT: Accelerating Chain-of-Thought Reasoning through Semantically-Aligned Implicit Tokens](https://arxiv.org/abs/2510.24940)
*Yinhan He,Wendy Zheng,Yaochen Zhu,Zaiyi Zheng,Lin Su,Sriram Vasudevan,Qi Guo,Liangjie Hong,Jundong Li*

Main category: cs.CL

TL;DR: 提出了SemCoT，一种语义对齐的隐式Chain-of-Thought推理框架，通过语义保持和高效生成提升推理效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有隐式CoT方法在保持语义对齐和生成速度上存在显著不足，导致性能下降和效率受限。

Method: 设计了对比训练的句子变换器来评估和保持隐式与显式推理的语义对齐，同时通过知识蒸馏微调轻量级语言模型，实现快速隐式推理生成。

Result: 实验证明SemCoT在推理效率和效果上均优于现有最先进方法。

Conclusion: SemCoT首次实现了隐式CoT推理的语义对齐和速度优化，显著提升推理效率和准确性。

Abstract: The verbosity of Chain-of-Thought (CoT) reasoning hinders its mass deployment
in efficiency-critical applications. Recently, implicit CoT approaches have
emerged, which encode reasoning steps within LLM's hidden embeddings (termed
``implicit reasoning'') rather than explicit tokens. This approach accelerates
CoT by reducing the reasoning length and bypassing some LLM components.
However, existing implicit CoT methods face two significant challenges: (1)
they fail to preserve the semantic alignment between the implicit reasoning
(when transformed to natural language) and the ground-truth reasoning,
resulting in a significant CoT performance degradation, and (2) they focus on
reducing the length of the implicit reasoning; however, they neglect the
considerable time cost for an LLM to generate one individual implicit reasoning
token. To tackle these challenges, we propose a novel semantically-aligned
implicit CoT framework termed SemCoT. In particular, for the first challenge,
we design a contrastively trained sentence transformer that evaluates semantic
alignment between implicit and explicit reasoning, which is used to enforce
semantic preservation during implicit reasoning optimization. To address the
second challenge, we introduce an efficient implicit reasoning generator by
finetuning a lightweight language model using knowledge distillation. This
generator is guided by our sentence transformer to distill ground-truth
reasoning into semantically aligned implicit reasoning, while also optimizing
for accuracy. SemCoT is the first approach that enhances CoT efficiency by
jointly optimizing token-level generation speed and preserving semantic
alignment with ground-truth reasoning. Extensive experiments demonstrate the
superior performance of SemCoT compared to state-of-the-art methods in both
efficiency and effectiveness. Our code can be found at
https://github.com/YinhanHe123/SemCoT/.

</details>


### [19] [Language Model Behavioral Phases are Consistent Across Architecture, Training Data, and Scale](https://arxiv.org/abs/2510.24963)
*James A. Michaelov,Roger P. Levy,Benjamin K. Bergen*

Main category: cs.CL

TL;DR: 本文研究了不同架构、训练数据和规模的自回归语言模型的行为变化，发现其行为变化模式高度一致，且大部分行为可以通过简单的三个启发式方法解释。


<details>
  <summary>Details</summary>
Motivation: 探索不同构架、训练数据和规模的语言模型在预训练过程中行为变化的共同规律。

Method: 分析了1400多个语言模型检查点，基于超过11万词的英文文本，研究词级别行为的统计特征，并考察词频、n元语法概率及语义相似性对模型行为的解释能力。

Result: 发现高达98%的语言模型行为方差可用词的单词频率、n元语法概率和语义相似性解释，且所有模型在训练过程中都表现出对n元语法概率过拟合的阶段行为。

Conclusion: 神经语言模型的学习轨迹在不同模型细节下可能是相似的，表明预训练过程中语言模型行为演变具有一致性和普适性。

Abstract: We show that across architecture (Transformer vs. Mamba vs. RWKV), training
dataset (OpenWebText vs. The Pile), and scale (14 million parameters to 12
billion parameters), autoregressive language models exhibit highly consistent
patterns of change in their behavior over the course of pretraining. Based on
our analysis of over 1,400 language model checkpoints on over 110,000 tokens of
English, we find that up to 98% of the variance in language model behavior at
the word level can be explained by three simple heuristics: the unigram
probability (frequency) of a given word, the $n$-gram probability of the word,
and the semantic similarity between the word and its context. Furthermore, we
see consistent behavioral phases in all language models, with their predicted
probabilities for words overfitting to those words' $n$-gram probabilities for
increasing $n$ over the course of training. Taken together, these results
suggest that learning in neural language models may follow a similar trajectory
irrespective of model details.

</details>


### [20] [POWSM: A Phonetic Open Whisper-Style Speech Foundation Model](https://arxiv.org/abs/2510.24992)
*Chin-Jou Li,Kalvin Chang,Shikhar Bharadwaj,Eunjung Yeo,Kwanghee Choi,Jian Zhu,David Mortensen,Shinji Watanabe*

Main category: cs.CL

TL;DR: 本文提出了统一的语音处理框架POWSM，实现音频、文字和音素之间的无缝转换，支持多项音素相关任务。


<details>
  <summary>Details</summary>
Motivation: 当前语音任务虽概念相似，但通常独立研究，各自依赖特定架构和数据集。作者希望构建统一模型同时处理多任务。

Method: 设计了POWSM模型，统一处理自动语音识别、音素识别、拼音转换等任务，实现音频、文字（拼写）与音素的相互转换。

Result: POWSM模型能够匹配或优于同规模的专用音素识别模型（如Wav2Vec2Phoneme和ZIPA），同时支持多种任务。

Conclusion: POWSM为通用及低资源语音处理开拓新可能，代码与数据公开推动开放科学。

Abstract: Recent advances in spoken language processing have led to substantial
progress in phonetic tasks such as automatic speech recognition (ASR), phone
recognition (PR), grapheme-to-phoneme conversion (G2P), and phoneme-to-grapheme
conversion (P2G). Despite their conceptual similarity, these tasks have largely
been studied in isolation, each relying on task-specific architectures and
datasets. In this paper, we introduce POWSM (Phonetic Open Whisper-style Speech
Model), the first unified framework capable of jointly performing multiple
phone-related tasks. POWSM enables seamless conversion between audio, text
(graphemes), and phones, opening up new possibilities for universal and
low-resource speech processing. Our model outperforms or matches specialized PR
models of similar size (Wav2Vec2Phoneme and ZIPA) while jointly supporting G2P,
P2G, and ASR. Our training data, code and models are released to foster open
science.

</details>


### [21] [Emergence of Minimal Circuits for Indirect Object Identification in Attention-Only Transformers](https://arxiv.org/abs/2510.25013)
*Rabin Adhikari*

Main category: cs.CL

TL;DR: 本文研究了通过训练小型注意力机制Transformer模型，实现对间接宾语识别任务（IOI）复杂推理机制的可解释性逆向工程。


<details>
  <summary>Details</summary>
Motivation: 由于大型预训练语言模型结构复杂，难以理解其完成特定推理任务的最小计算机制，本文尝试简化模型结构以揭示推理过程。

Method: 作者从头训练了基于注意力机制的小型Transformer模型，采用符号化的间接宾语识别任务作为基准，通过残差流分解、谱分析和嵌入干预等手段分析模型内部机制。

Result: 发现单层两头注意力模型即可实现100%准确率，两头注意力机制分化为加法和对比子电路共同完成IOI任务；两层一头注意力模型通过层间查询-值相互作用也可达到类似表现。

Conclusion: 任务专项训练能诱导高度可解释且极简的计算电路，为研究Transformer推理的计算基础提供了可控实验平台。

Abstract: Mechanistic interpretability aims to reverse-engineer large language models
(LLMs) into human-understandable computational circuits. However, the
complexity of pretrained models often obscures the minimal mechanisms required
for specific reasoning tasks. In this work, we train small, attention-only
transformers from scratch on a symbolic version of the Indirect Object
Identification (IOI) task -- a benchmark for studying coreference -- like
reasoning in transformers. Surprisingly, a single-layer model with only two
attention heads achieves perfect IOI accuracy, despite lacking MLPs and
normalization layers. Through residual stream decomposition, spectral analysis,
and embedding interventions, we find that the two heads specialize into
additive and contrastive subcircuits that jointly implement IOI resolution.
Furthermore, we show that a two-layer, one-head model achieves similar
performance by composing information across layers through query-value
interactions. These results demonstrate that task-specific training induces
highly interpretable, minimal circuits, offering a controlled testbed for
probing the computational foundations of transformer reasoning.

</details>


### [22] [Evaluating Emotion Recognition in Spoken Language Models on Emotionally Incongruent Speech](https://arxiv.org/abs/2510.25054)
*Pedro Corrêa,João Lima,Victor Moreno,Paula Dornhofer Paro Costa*

Main category: cs.CL

TL;DR: 本文评估了四种联合音频与文本表征的口语语言模型（SLMs）在情感识别任务中的表现，发现模型主要依赖文本语义而非语音情感。


<details>
  <summary>Details</summary>
Motivation: 当前口语语言模型在多模态融合和泛化能力方面存在质疑，研究意在探讨其对语音与文本信息整合的程度。

Method: 通过设计一个情感不一致的语音数据集（EMIS），让语音的语义内容和情感表达不匹配，测试四种SLMs在语音情感识别任务中的表现。

Result: 结果表明SLMs主要依赖文本语义而非语音情感，模型内部的文本表征显著主导于声学表征。

Conclusion: SLMs在语音情感识别中对文本信息的依赖大于对语音情感的利用，呼吁社区对模型多模态融合能力的进一步研究和改进。

Abstract: Advancements in spoken language processing have driven the development of
spoken language models (SLMs), designed to achieve universal audio
understanding by jointly learning text and audio representations for a wide
range of tasks. Although promising results have been achieved, there is growing
discussion regarding these models' generalization capabilities and the extent
to which they truly integrate audio and text modalities in their internal
representations. In this work, we evaluate four SLMs on the task of speech
emotion recognition using a dataset of emotionally incongruent speech samples,
a condition under which the semantic content of the spoken utterance conveys
one emotion while speech expressiveness conveys another. Our results indicate
that SLMs rely predominantly on textual semantics rather than speech emotion to
perform the task, indicating that text-related representations largely dominate
over acoustic representations. We release both the code and the Emotionally
Incongruent Synthetic Speech dataset (EMIS) to the community.

</details>


### [23] [GAPMAP: Mapping Scientific Knowledge Gaps in Biomedical Literature Using Large Language Models](https://arxiv.org/abs/2510.25055)
*Nourah M Salem,Elizabeth White,Michael Bada,Lawrence Hunter*

Main category: cs.CL

TL;DR: 该研究探讨大型语言模型识别生物医学文献中的显性和隐性研究知识空白的能力，提出TABI推理方法，验证了不同模型在多数据集上的表现。


<details>
  <summary>Details</summary>
Motivation: 科学进步依赖明确指出未知领域，而现有研究主要关注显性知识空白，缺乏对隐性知识空白的推断。

Method: 定义显性和隐性知识空白，设计TABI推理框架，进行近1500篇文献的两项实验，比较开放和闭源大型语言模型在不同层面的表现。

Result: 大型语言模型能有效识别显性与隐性知识空白，大模型表现更佳，展示其支持早期研究构思和政策制定的潜力。

Conclusion: LLMs具有系统识别知识空白的强大能力，但需结合领域适应、人机协作验证及多模型基准测试，确保其稳健应用。

Abstract: Scientific progress is driven by the deliberate articulation of what remains
unknown. This study investigates the ability of large language models (LLMs) to
identify research knowledge gaps in the biomedical literature. We define two
categories of knowledge gaps: explicit gaps, clear declarations of missing
knowledge; and implicit gaps, context-inferred missing knowledge. While prior
work has focused mainly on explicit gap detection, we extend this line of
research by addressing the novel task of inferring implicit gaps. We conducted
two experiments on almost 1500 documents across four datasets, including a
manually annotated corpus of biomedical articles. We benchmarked both
closed-weight models (from OpenAI) and open-weight models (Llama and Gemma 2)
under paragraph-level and full-paper settings. To address the reasoning of
implicit gaps inference, we introduce \textbf{\small TABI}, a Toulmin-Abductive
Bucketed Inference scheme that structures reasoning and buckets inferred
conclusion candidates for validation. Our results highlight the robust
capability of LLMs in identifying both explicit and implicit knowledge gaps.
This is true for both open- and closed-weight models, with larger variants
often performing better. This suggests a strong ability of LLMs for
systematically identifying candidate knowledge gaps, which can support
early-stage research formulation, policymakers, and funding decisions. We also
report observed failure modes and outline directions for robust deployment,
including domain adaptation, human-in-the-loop verification, and benchmarking
across open- and closed-weight models.

</details>


### [24] [Can LLMs Estimate Cognitive Complexity of Reading Comprehension Items?](https://arxiv.org/abs/2510.25064)
*Seonjeong Hwang,Hyounghun Kim,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）在估计阅读理解题目的认知复杂度方面的能力，重点关注证据范围和转换层级两个维度。结果表明，LLMs能够近似认知复杂度，有助于事前难度分析，但其元认知意识存在不足。


<details>
  <summary>Details</summary>
Motivation: 现有NLP工具难以自动提取回答推理中产生的认知特征，传统方法依赖人工标注，亟需自动化手段评估阅读理解题目的认知复杂度。

Method: 聚焦于反映推理认知负担的两个维度——证据范围和转换层级，利用大型语言模型对阅读理解题目进行认知复杂度估计，进行实验验证其有效性。

Result: 实验结果显示LLMs能够有效近似阅读理解题目的认知复杂度，证明其在难度预测方面的潜力。但进一步分析发现，尽管LLMs能给出正确答案，却有时无法准确理解自身推理过程中的认知特征。

Conclusion: 大型语言模型具备自动评估阅读理解题目认知复杂度的能力，能够辅助题目难度分析，但其元认知能力仍需提升以更好理解自身推理机制。

Abstract: Estimating the cognitive complexity of reading comprehension (RC) items is
crucial for assessing item difficulty before it is administered to learners.
Unlike syntactic and semantic features, such as passage length or semantic
similarity between options, cognitive features that arise during answer
reasoning are not readily extractable using existing NLP tools and have
traditionally relied on human annotation. In this study, we examine whether
large language models (LLMs) can estimate the cognitive complexity of RC items
by focusing on two dimensions-Evidence Scope and Transformation Level-that
indicate the degree of cognitive burden involved in reasoning about the answer.
Our experimental results demonstrate that LLMs can approximate the cognitive
complexity of items, indicating their potential as tools for prior difficulty
analysis. Further analysis reveals a gap between LLMs' reasoning ability and
their metacognitive awareness: even when they produce correct answers, they
sometimes fail to correctly identify the features underlying their own
reasoning process.

</details>


### [25] [TOPol: Capturing and Explaining Multidimensional Semantic Polarity Fields and Vectors](https://arxiv.org/abs/2510.25069)
*Gabin Taibi,Lucia Gomez*

Main category: cs.CL

TL;DR: 本文提出了一种名为TOPol的新方法，用以捕捉和分析语义极性在不同语境下的多维变化，突破了传统单维度描述的局限。


<details>
  <summary>Details</summary>
Motivation: 传统语义极性研究多忽略语言的多维结构，难以准确反映语义在情境中的细微变化。

Method: TOPol结合人机交互定义上下文边界，通过基于大语言模型的嵌入、邻域优化的UMAP降维和Leiden分区，实现主题分割，并计算语境转移中的极性矢量，辅助人机协同调整边界，并通过对比标签进行解释。

Result: 在美国央行发言和亚马逊评论两个不同语料库实验中，TOPol有效捕捉到非情感性和情感性的语义极性转换，表现出稳定且具有可扩展性。

Conclusion: TOPol提供了一个稳定、通用且可解释的多维语义分析框架，适合语境敏感的语篇分析，突破以往单维度情感分析的限制。

Abstract: Traditional approaches to semantic polarity in computational linguistics
treat sentiment as a unidimensional scale, overlooking the multidimensional
structure of language. This work introduces TOPol (Topic-Orientation POLarity),
a semi-unsupervised framework for reconstructing and interpreting
multidimensional narrative polarity fields under human-on-the-loop (HoTL)
defined contextual boundaries (CBs). The framework embeds documents using a
transformer-based large language model (tLLM), applies neighbor-tuned UMAP
projection, and segments topics via Leiden partitioning. Given a CB between
discourse regimes A and B, TOPol computes directional vectors between
corresponding topic-boundary centroids, yielding a polarity field that
quantifies fine-grained semantic displacement during regime shifts. This
vectorial representation enables assessing CB quality and detecting polarity
changes, guiding HoTL CB refinement. To interpret identified polarity vectors,
the tLLM compares their extreme points and produces contrastive labels with
estimated coverage. Robustness analyses show that only CB definitions (the main
HoTL-tunable parameter) significantly affect results, confirming methodological
stability. We evaluate TOPol on two corpora: (i) U.S. Central Bank speeches
around a macroeconomic breakpoint, capturing non-affective semantic shifts, and
(ii) Amazon product reviews across rating strata, where affective polarity
aligns with NRC valence. Results demonstrate that TOPol consistently captures
both affective and non-affective polarity transitions, providing a scalable,
generalizable, and interpretable framework for context-sensitive
multidimensional discourse analysis.

</details>


### [26] [BioCoref: Benchmarking Biomedical Coreference Resolution with LLMs](https://arxiv.org/abs/2510.25087)
*Nourah M Salem,Elizabeth White,Michael Bada,Lawrence Hunter*

Main category: cs.CL

TL;DR: 本文评估了生成式大型语言模型（LLMs）在生物医学文本指代消解中的表现，比较了不同提示方法及SpanBERT的判别方法，发现LLaMA模型在实体增强提示下效果最佳。


<details>
  <summary>Details</summary>
Motivation: 生物医学文本中的指代消解由于专业术语复杂、歧义高和长距离依赖等问题面临挑战，现有方法效果有限，需探索生成式语言模型的潜力。

Method: 利用CRAFT语料库，设计了四种不同提示策略（局部、上下文增强、实体缩写和词典提示）评估生成式LLMs，并与判别式SpanBERT模型进行对比。

Result: 生成式LLMs在浅层指代消解上表现较好，尤其是结合领域知识的提示时，但对长距离上下文和提及歧义依然敏感。LLaMA 8B和17B在实体增强提示下表现出更高的准确率和F1分数。

Conclusion: 轻量级提示工程能显著提升大型生成式语言模型在生物医学自然语言处理任务中的实用性，特别是在指代消解任务中展现出较大潜力。

Abstract: Coreference resolution in biomedical texts presents unique challenges due to
complex domain-specific terminology, high ambiguity in mention forms, and
long-distance dependencies between coreferring expressions. In this work, we
present a comprehensive evaluation of generative large language models (LLMs)
for coreference resolution in the biomedical domain. Using the CRAFT corpus as
our benchmark, we assess the LLMs' performance with four prompting experiments
that vary in their use of local, contextual enrichment, and domain-specific
cues such as abbreviations and entity dictionaries. We benchmark these
approaches against a discriminative span-based encoder, SpanBERT, to compare
the efficacy of generative versus discriminative methods. Our results
demonstrate that while LLMs exhibit strong surface-level coreference
capabilities, especially when supplemented with domain-grounding prompts, their
performance remains sensitive to long-range context and mentions ambiguity.
Notably, the LLaMA 8B and 17B models show superior precision and F1 scores
under entity-augmented prompting, highlighting the potential of lightweight
prompt engineering for enhancing LLM utility in biomedical NLP tasks.

</details>


### [27] [DEBATE: A Large-Scale Benchmark for Role-Playing LLM Agents in Multi-Agent, Long-Form Debates](https://arxiv.org/abs/2510.25110)
*Yun-Shiuan Chuang,Ruixuan Tu,Chengtao Dai,Smit Vasani,Binwei Yao,Michael Henry Tessler,Sijia Yang,Dhavan Shah,Robert Hawkins,Junjie Hu,Timothy T. Rogers*

Main category: cs.CL

TL;DR: 本文提出了DEBATE，一个用于评估多代理大语言模型（LLMs）模拟真实人类群体互动的基准数据集，通过美国参与者的辩论数据比较模拟与真实群体动态的差异，并通过监督微调提高LLM的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的单代理对齐不能保证多代理群体动态的真实性，且缺少评估多代理模拟真实性的基准。

Method: 构建DEBATE数据集，包含来自2792名美国人围绕107个话题的多轮辩论消息及私下观点；基于此数据集评估多代理LLMs的群体动态表现，实施监督微调提升模型与人类行为的一致性。

Result: 发现了模拟与真实群体动态之间的关键差异；微调LLM后表面指标如ROUGE-L和消息长度得到提升，但深层语义一致性仍存在不足。

Conclusion: 虽然多代理LLM角色扮演具备模拟真实人类社交动态的潜力，但目前仍有显著局限，需进一步改进以实现更深层的语义对齐。

Abstract: Accurately modeling opinion change through social interactions is crucial for
addressing issues like misinformation and polarization. While role-playing
large language models (LLMs) offer a promising way to simulate human-like
interactions, existing research shows that single-agent alignment does not
guarantee authentic multi-agent group dynamics. Current LLM role-play setups
often produce unnatural dynamics (e.g., premature convergence), without an
empirical benchmark to measure authentic human opinion trajectories. To bridge
this gap, we introduce DEBATE, the first large-scale empirical benchmark
explicitly designed to evaluate the authenticity of the interaction between
multi-agent role-playing LLMs. DEBATE contains 29,417 messages from multi-round
debate conversations among over 2,792 U.S.-based participants discussing 107
controversial topics, capturing both publicly-expressed messages and
privately-reported opinions. Using DEBATE, we systematically evaluate and
identify critical discrepancies between simulated and authentic group dynamics.
We further demonstrate DEBATE's utility for aligning LLMs with human behavior
through supervised fine-tuning, achieving improvements in surface-level metrics
(e.g., ROUGE-L and message length) while highlighting limitations in deeper
semantic alignment (e.g., semantic similarity). Our findings highlight both the
potential and current limitations of role-playing LLM agents for realistically
simulating human-like social dynamics.

</details>


### [28] [Pretraining Strategies using Monolingual and Parallel Data for Low-Resource Machine Translation](https://arxiv.org/abs/2510.25116)
*Idriss Nguepi Nguefack,Mara Finkelstein,Toadoum Sari Sakayo*

Main category: cs.CL

TL;DR: 本文研究了多种预训练策略对低资源语言机器翻译模型的效果，以林加拉语为重点，表明多语言预训练及结合单语和平行语料显著提升翻译质量。


<details>
  <summary>Details</summary>
Motivation: 为了提升针对低资源语言的机器翻译质量，缩小与高资源语言之间的性能差距。

Method: 基于Reid和Artetxe(2021)的方法，采用多语言预训练，结合单语和平行数据，针对包括林加拉语在内的低资源语言进行模型训练。

Result: 多语言预训练及结合单语和平行数据显著提升了低资源语言的翻译性能。

Conclusion: 有效的预训练策略能显著改善低资源语言的机器翻译，促进包容性更高的NLP模型发展。研究代码和数据集公开，支持后续研究和复现。

Abstract: This research article examines the effectiveness of various pretraining
strategies for developing machine translation models tailored to low-resource
languages. Although this work considers several low-resource languages,
including Afrikaans, Swahili, and Zulu, the translation model is specifically
developed for Lingala, an under-resourced African language, building upon the
pretraining approach introduced by Reid and Artetxe (2021), originally designed
for high-resource languages. Through a series of comprehensive experiments, we
explore different pretraining methodologies, including the integration of
multiple languages and the use of both monolingual and parallel data during the
pretraining phase. Our findings indicate that pretraining on multiple languages
and leveraging both monolingual and parallel data significantly enhance
translation quality. This study offers valuable insights into effective
pretraining strategies for low-resource machine translation, helping to bridge
the performance gap between high-resource and low-resource languages. The
results contribute to the broader goal of developing more inclusive and
accurate NLP models for marginalized communities and underrepresented
populations. The code and datasets used in this study are publicly available to
facilitate further research and ensure reproducibility, with the exception of
certain data that may no longer be accessible due to changes in public
availability.

</details>


### [29] [A Survey on Unlearning in Large Language Models](https://arxiv.org/abs/2510.25117)
*Ruichen Qiu,Jiajun Tan,Jiayue Pu,Honglin Wang,Xiao-Shan Gao,Fei Sun*

Main category: cs.CL

TL;DR: 本文系统综述了自2021年以来关于大型语言模型（LLMs）去学习的180余篇论文，提出了新的分类方法，并分析了相关评估指标，旨在促进安全和合规的LLM发展。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练过程中存在敏感信息泄露和法律伦理风险，需采用去学习技术选择性地清除特定知识以符合法律合规要求。

Method: 对去学习方法按训练阶段划分为训练时、训练后和推理时，梳理相关评价数据集与指标，批判性分析各自优缺点和适用性。

Result: 系统整理了大量文献，提出了新的分类体系和评价方法，为研究人员提供实用的指导与建议。

Conclusion: 去学习技术是确保大型语言模型安全、合规的重要方向，未来研究需聚焦挑战解决和方法优化。

Abstract: The advancement of Large Language Models (LLMs) has revolutionized natural
language processing, yet their training on massive corpora poses significant
risks, including the memorization of sensitive personal data, copyrighted
material, and knowledge that could facilitate malicious activities. To mitigate
these issues and align with legal and ethical standards such as the "right to
be forgotten", machine unlearning has emerged as a critical technique to
selectively erase specific knowledge from LLMs without compromising their
overall performance. This survey provides a systematic review of over 180
papers on LLM unlearning published since 2021, focusing exclusively on
large-scale generative models. Distinct from prior surveys, we introduce novel
taxonomies for both unlearning methods and evaluations. We clearly categorize
methods into training-time, post-training, and inference-time based on the
training stage at which unlearning is applied. For evaluations, we not only
systematically compile existing datasets and metrics but also critically
analyze their advantages, disadvantages, and applicability, providing practical
guidance to the research community. In addition, we discuss key challenges and
promising future research directions. Our comprehensive overview aims to inform
and guide the ongoing development of secure and reliable LLMs.

</details>


### [30] [Explainable Disentanglement on Discrete Speech Representations for Noise-Robust ASR](https://arxiv.org/abs/2510.25150)
*Shreyas Gopal,Ashutosh Anshul,Haoyang Li,Yue Heng Yeo,Hexin Liu,Eng Siong Chng*

Main category: cs.CL

TL;DR: 该论文提出了一种在离散音频表示中分离语义内容与背景噪声的方法，从而提升语音识别在嘈杂环境中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的离散音频表示方法虽然具有良好的可解释性和与大语言模型的兼容性，但在嘈杂或现实环境下表现不佳。

Method: 基于Whisper嵌入的量化方法，设计端到端模型将干净语音编码为代码本令牌，并将噪声作为量化残差提取，通过轻量分类器进行监督，达到语义与噪声特征分离。

Result: 该方法提高了干净和嘈杂语音与文本的对齐度，产生噪声不变性的语音令牌，并在VBDemand测试集中，在保持Whisper模型不变的情况下，使错误率降低82%，比基线方法提升35%。

Conclusion: 分离语义内容与噪声的音频表示方法有效提升了嘈杂环境下的语音识别性能，且学习的令牌空间对已见和未见的声学条件均具有良好泛化能力。

Abstract: Discrete audio representations are gaining traction in speech modeling due to
their interpretability and compatibility with large language models, but are
not always optimized for noisy or real-world environments. Building on existing
works that quantize Whisper embeddings for speech-to-unit modeling, we propose
disentangling semantic speech content from background noise in the latent
space. Our end-to-end model separates clean speech in the form of codebook
tokens, while extracting interpretable noise vectors as quantization residue
which are supervised via a lightweight classifier. We show that our approach
improves alignment between clean/noisy speech and text, producing speech tokens
that display a high degree of noiseinvariance, and improves ASR performance.
Keeping Whisper frozen, we show an 82% reduction in error rate compared to
Whisper, and 35% improvement over baseline methods on the VBDemand test set.
Further analyses show that the learned token space generalizes well to both
seen and unseen acoustic conditions.

</details>


### [31] [Model-Document Protocol for AI Search](https://arxiv.org/abs/2510.25160)
*Hongjin Qian,Zheng Liu*

Main category: cs.CL

TL;DR: 该论文提出了模型-文档协议（MDP），创新性地将非结构化长文本转换为适合大语言模型处理的结构化知识表示，从而提升AI搜索的效果。


<details>
  <summary>Details</summary>
Motivation: 传统的检索方法直接返回文本片段，导致LLM需承担文本碎片拼接和上下文推理的重任，效率和准确性受限。需设计新型检索范式改进模型与文档的交互方式。

Method: 提出MDP框架，通过多条路径（代理推理、记忆基础和结构化利用）将文档转化为任务特定的、可供LLM直接使用的结构化知识。并实现MDP-Agent，采用代理流程构建概要记忆、分层探索依赖关系及规模化证据整合。

Result: MDP-Agent在信息检索基准测试中表现优于传统方法，验证了MDP框架的科学性及其代理实例的有效性。

Conclusion: 将非结构化文档转换为紧凑、结构化知识以供LLM利用，是提升基于知识的AI搜索性能的有效新范式。MDP为此提供了统一理论支持和实际实现路径。

Abstract: AI search depends on linking large language models (LLMs) with vast external
knowledge sources. Yet web pages, PDF files, and other raw documents are not
inherently LLM-ready: they are long, noisy, and unstructured. Conventional
retrieval methods treat these documents as verbatim text and return raw
passages, leaving the burden of fragment assembly and contextual reasoning to
the LLM. This gap underscores the need for a new retrieval paradigm that
redefines how models interact with documents.
  We introduce the Model-Document Protocol (MDP), a general framework that
formalizes how raw text is bridged to LLMs through consumable knowledge
representations. Rather than treating retrieval as passage fetching, MDP
defines multiple pathways that transform unstructured documents into
task-specific, LLM-ready inputs. These include agentic reasoning, which curates
raw evidence into coherent context; memory grounding, which accumulates
reusable notes to enrich reasoning; and structured leveraging, which encodes
documents into formal representations such as graphs or key-value caches. All
three pathways share the same goal: ensuring that what reaches the LLM is not
raw fragments but compact, structured knowledge directly consumable for
reasoning.
  As an instantiation, we present MDP-Agent, which realizes the protocol
through an agentic process: constructing document-level gist memories for
global coverage, performing diffusion-based exploration with vertical
exploitation to uncover layered dependencies, and applying map-reduce style
synthesis to integrate large-scale evidence into compact yet sufficient
context. Experiments on information-seeking benchmarks demonstrate that
MDP-Agent outperforms baselines, validating both the soundness of the MDP
framework and the effectiveness of its agentic instantiation.

</details>


### [32] [Testing Cross-Lingual Text Comprehension In LLMs Using Next Sentence Prediction](https://arxiv.org/abs/2510.25187)
*Ritesh Sunil Chavan,Jack Mostow*

Main category: cs.CL

TL;DR: 这篇论文研究了大型语言模型在低资源语言上的性能表现，发现模型在高资源语言表现优异，但在低资源语言如豪萨语表现显著下降。链式思维提示（CoT）对不同模型的影响有所不同，能帮助能力较弱的模型提升表现，却可能对能力较强的模型产生负面影响。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型训练数据主要偏向英语，难以确定其性能是否源于真实能力还是数据优势，特别是在低资源语言上的表现尚不清楚。

Method: 构建了一个包含英语（高资源）、斯瓦希里语（中资源）和豪萨语（低资源）各1万条问题的大规模Next Sentence Prediction基准测试，测试了GPT-4 Turbo、Gemini 1.5 Flash、LLaMA 3 70B等顶级模型，并引入Chain-of-Thought提示分析其影响。

Result: 所有模型在英语上表现优异，随着语言资源减少，准确率下降明显，尤其是豪萨语中LLaMA 3表现最差。CoT提示显著提升了能力较弱的LLaMA 3准确率，但对GPT-4和Gemini等模型反而产生“过度思考”效应，降低跨语言表现。

Conclusion: 模型性能受语言资源影响显著，CoT提示并非万能策略，其效果依赖于模型基础能力及任务上下文。该研究提出的框架有效揭示了模型弱点，并明确了何时CoT有益或有害。

Abstract: While large language models are trained on massive datasets, this data is
heavily skewed towards English. Does their impressive performance reflect
genuine ability or just this data advantage? To find out, we tested them in a
setting where they could not rely on data abundance: low-resource languages.
Building on prior work Agarwal et al. (2025) that used Next Sentence Prediction
(NSP) as a test, we created a large-scale benchmark with 10,000 questions each
for English (a high-resource language), Swahili (medium-resource), and Hausa
(low-resource). We then tested several top models, including GPT-4 Turbo,
Gemini 1.5 Flash, and LLaMA 3 70B, to see how their performance holds up. The
results painted a clear picture of how levels of language resources impact
outcomes. While all models excelled in English, their accuracy dropped in
Swahili and fell sharply in Hausa, with LLaMA 3 struggling the most. The story
became even more interesting when we introduced Chain-of-Thought (CoT)
prompting. For the struggling LLaMA 3, CoT acted as a helpful guide,
significantly boosting its accuracy. However, for the more capable GPT-4 and
Gemini, the same technique often backfired, leading to a kind of "overthinking"
that hurt their results in the cross-lingual context. This reveals that
Chain-of-Thought is not a universal solution; its effectiveness depends heavily
on the model's baseline capability and the specific context of the task. Our
framework pinpoints LLM weaknesses, highlights when CoT helps or hinders
cross-lingual NSP performance, and factors influencing their decisions.

</details>


### [33] [ProMediate: A Socio-cognitive framework for evaluating proactive agents in multi-party negotiation](https://arxiv.org/abs/2510.25224)
*Ziyi Liu,Bahar Sarrafzadeh,Pei Zhou,Longqi Yang,Jieyu Zhao,Ashish Sharma*

Main category: cs.CL

TL;DR: 本文提出了ProMediate框架，用于评估在复杂多方多议题谈判中的主动AI调解代理，包含仿真实验平台和社会认知评价体系，结果表明社会智能调解代理在促进共识和响应速度方面优于通用基线。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在智能代理中的应用普及，迫切需要能够主动管理复杂多方协作的智能代理，当前缺乏系统的评估方法限制了相关技术的发展。

Method: 设计了ProMediate框架，包括基于真实谈判案例与理论难度分级的仿真测试平台，以及基于社会认知调解理论的主动调解代理和一套新的评价指标，用于衡量共识变化、干预时效和代理效果。

Result: 社会智能调解代理比通用基线在ProMediate-Hard环境中使共识变化提高了3.6个百分点，并在响应速度上提升了77%。

Conclusion: ProMediate为开发主动且具备社会智能的AI调解代理提供了一个扎实且理论支持的评估平台，推动多方合作中智能代理技术的进步。

Abstract: While Large Language Models (LLMs) are increasingly used in agentic
frameworks to assist individual users, there is a growing need for agents that
can proactively manage complex, multi-party collaboration. Systematic
evaluation methods for such proactive agents remain scarce, limiting progress
in developing AI that can effectively support multiple people together.
Negotiation offers a demanding testbed for this challenge, requiring
socio-cognitive intelligence to navigate conflicting interests between multiple
participants and multiple topics and build consensus. Here, we present
ProMediate, the first framework for evaluating proactive AI mediator agents in
complex, multi-topic, multi-party negotiations. ProMediate consists of two core
components: (i) a simulation testbed based on realistic negotiation cases and
theory-driven difficulty levels (ProMediate-Easy, ProMediate-Medium, and
ProMediate-Hard), with a plug-and-play proactive AI mediator grounded in
socio-cognitive mediation theories, capable of flexibly deciding when and how
to intervene; and (ii) a socio-cognitive evaluation framework with a new suite
of metrics to measure consensus changes, intervention latency, mediator
effectiveness, and intelligence. Together, these components establish a
systematic framework for assessing the socio-cognitive intelligence of
proactive AI agents in multi-party settings. Our results show that a socially
intelligent mediator agent outperforms a generic baseline, via faster,
better-targeted interventions. In the ProMediate-Hard setting, our social
mediator increases consensus change by 3.6 percentage points compared to the
generic baseline (10.65\% vs 7.01\%) while being 77\% faster in response
(15.98s vs. 3.71s). In conclusion, ProMediate provides a rigorous,
theory-grounded testbed to advance the development of proactive, socially
intelligent agents.

</details>


### [34] [Adapting Small Language Models to Low-Resource Domains: A Case Study in Hindi Tourism QA](https://arxiv.org/abs/2510.25273)
*Sandipan Majhi,Paheli Bhattacharya*

Main category: cs.CL

TL;DR: 该论文提出了一种多阶段微调策略，利用大语言模型生成的合成问答数据，提升轻量级语言模型在印地语旅游领域的问答性能，解决了低资源语言中领域特定问答的数据稀缺和知识有限问题。


<details>
  <summary>Details</summary>
Motivation: 针对低资源语言领域特定问答面临的注释数据稀缺和通用语言模型领域知识有限等挑战，提出提升模型在特定领域适应能力的方法。

Method: 采用多阶段微调策略，利用LLaMA-70B和Phi-14B等大语言模型生成旅游领域的合成问答对，结合有限的原始数据对轻量级模型进行训练，探索多种训练方法及其对领域泛化能力的影响。

Result: 实验结果表明，大模型能有效生成合成数据，小模型则能成功适应这些合成数据，从而提升领域特定问答的性能。

Conclusion: 该方法为低资源语言的领域特定问答提供了一种可扩展的解决方案，通过大模型生成合成数据辅助轻量级模型微调，实现了领域适应与泛化能力的提升。

Abstract: Domain-specific question answering in low-resource languages faces two key
challenges: scarcity of annotated datasets and limited domain knowledge in
general-purpose language models. In this work, we present a multi-stage
finetuning strategy to adapt lightweight language models to the Hindi tourism
domain by leveraging both original and synthetic training data. Synthetic
question-answer pairs are generated using large LLMs (LLaMA-70B, Phi-14B) and
used to augment the limited original dataset. We explore several training
methodologies and analyse their impact on domain generalisation. Our results
demonstrate that large models can efficiently generate synthetic data, while
small models can effectively adapt to it, offering a scalable pathway for
low-resource, domain-specific QA.

</details>


### [35] [Teaching Sarcasm: Few-Shot Multimodal Sarcasm Detection via Distillation to a Parameter-Efficient Student](https://arxiv.org/abs/2510.25303)
*Soumyadeep Jana,Sanasam Ranbir Singh*

Main category: cs.CL

TL;DR: 本文提出了PEKD框架，通过专家模型蒸馏提升多模态讽刺检测中参数高效微调方法的表现，尤其适用于少样本场景。


<details>
  <summary>Details</summary>
Motivation: 多模态讽刺检测在标注数据稀缺情况下，细微的图文矛盾难以学习，导致模型表现受限；现有的参数高效微调方法虽然减少过拟合，但在少样本情况下效果有限。

Method: 提出PEKD框架，结合专家模型蒸馏与熵感知门控机制，根据教师模型置信度动态调节蒸馏强度，增强参数高效微调方法的能力。

Result: 在两个公开数据集上，PEKD框架显著优于已有参数高效方法和大型多模态模型，在少样本设置下取得优异效果。

Conclusion: PEKD框架具有模块化和适应性强的特点，可广泛应用于多模态模型和任务，有效提升了少样本多模态讽刺检测性能。

Abstract: Multimodal sarcasm detection is challenging, especially in low-resource
settings where subtle image-text contradictions are hard to learn due to scarce
annotated data, which hinders the model's performance. Parameter-efficient
fine-tuning (PEFT) methods like adapters, LoRA, and prompt tuning reduce
overfitting but struggle to reach optimal performance due to limited
supervision from few-shot data. We propose PEKD, a unified framework that
enhances PEFT methods via distillation from an expert model trained on
large-scale sarcasm data, which acts as the teacher. To mitigate unreliable
signals from the teacher, we introduce an entropy-aware gating mechanism that
dynamically adjusts the distillation strength based on teacher confidence.
Experiments on two public datasets demonstrate that our PEKD framework enables
PEFT methods to outperform both prior parameter-efficient approaches and large
multimodal models, achieving strong results in the few-shot scenario. The
framework is modular and adaptable to a wide range of multimodal models and
tasks.

</details>


### [36] [Parrot: A Training Pipeline Enhances Both Program CoT and Natural Language CoT for Reasoning](https://arxiv.org/abs/2510.25310)
*Senjie Jin,Lu Chen,Zhiheng Xi,Yuhui Wang,Sirui Song,Yuhao Zhou,Xinbo Zhang,Peng Sun,Hong Lu,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: 本文提出了Parrot训练框架，通过互补P-CoT和N-CoT范式，实现数学推理问题的性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前研究仅实现单向提升P-CoT或N-CoT，未充分发挥两者互补优势，亟需实现双向共同提升。

Method: 通过三阶段子任务融合P-CoT和N-CoT生成，采用子任务混合训练促进语义迁移，设计N-CoT辅助奖励缓解P-CoT稀疏奖励问题。

Result: Parrot在多个数学推理任务中显著提升了N-CoT和P-CoT性能，尤其是N-CoT，在MathQA数据集上相较RL基线提升超过21分。

Conclusion: Parrot有效促进了N-CoT与P-CoT的互补增强，显著提升大语言模型数学推理能力，且训练资源效率更高。

Abstract: Natural language chain-of-thought (N-CoT) and Program chain-of-thought
(P-CoT) have emerged as two primary paradigms for large language models (LLMs)
to solve mathematical reasoning problems. Current research typically endeavors
to achieve unidirectional enhancement: P-CoT enhanced N-CoT or N-CoT enhanced
P-CoT. In this paper, we seek to fully unleash the two paradigms' strengths for
mutual enhancement and ultimately achieve simultaneous improvements. We conduct
a detailed analysis of the error types across two paradigms, based on which we
propose Parrot, a novel training pipeline for mathematical problems: 1) Three
target-designed subtasks integrate sequential P-CoT and N-CoT generation. 2) A
subtask hybrid training strategy to facilitate natural language semantic
transferability. 3) The converted N-CoT auxiliary reward is designed to
alleviate the sparse rewards in P-CoT optimization. Extensive experiments
demonstrate that Parrot significantly enhances both the performance of N-CoT
and P-CoT, especially on N-CoT. Using Parrot SFT, the N-CoT performance of
LLaMA2 and CodeLLaMA achieve gains of +21.87 and +21.48 on MathQA over the RL
baseline, which is resource-intensive.

</details>


### [37] [CRMWeaver: Building Powerful Business Agent via Agentic RL and Shared Memories](https://arxiv.org/abs/2510.25333)
*Yilong Lai,Yipin Yang,Jialong Wu,Fengran Mo,Zhenglin Wang,Ting Liang,Jianguo Lin,Keping Yang*

Main category: cs.CL

TL;DR: 本文提出了CRMWeaver，一种提升业务代理在复杂业务环境下表现的新方法，通过合成数据生成和基于强化学习的训练范式，以及推理时共享记忆机制，提高了模型处理复杂数据和多样任务的能力，并在CRMArena-Pro数据集上获得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 业务代理需要应对复杂数据关系和多样的异构任务，但现有方法难以有效处理这些挑战。

Method: 采用合成数据生成与强化学习训练范式来提升模型适应复杂业务环境的能力，推理阶段引入共享记忆机制以促进从相似任务中学习，提高泛化能力。

Result: 轻量级模型在CRMArena-Pro数据集的B2B和B2C业务场景中表现出竞争力，验证了方法的有效性。

Conclusion: CRMWeaver通过创新训练和推理机制，显著提升了业务代理在复杂环境中的表现，具有重要的实际应用价值。

Abstract: Recent years have witnessed the rapid development of LLM-based agents, which
shed light on using language agents to solve complex real-world problems. A
prominent application lies in business agents, which interact with databases
and internal knowledge bases via tool calls to fulfill diverse user
requirements. However, this domain is characterized by intricate data
relationships and a wide range of heterogeneous tasks, from statistical data
queries to knowledge-based question-answering. To address these challenges, we
propose CRMWeaver, a novel approach that enhances business agents in such
complex settings. To acclimate the agentic model to intricate business
environments, we employ a synthesis data generation and RL-based paradigm
during training, which significantly improves the model's ability to handle
complex data and varied tasks. During inference, a shared memories mechanism is
introduced, prompting the agent to learn from task guidelines in similar
problems, thereby further boosting its effectiveness and generalization,
especially in unseen scenarios. We validate the efficacy of our approach on the
CRMArena-Pro dataset, where our lightweight model achieves competitive results
in both B2B and B2C business scenarios, underscoring its practical value for
real-world applications.

</details>


### [38] [Not ready for the bench: LLM legal interpretation is unstable and out of step with human judgments](https://arxiv.org/abs/2510.25356)
*Abhishek Purushothama,Junghyun Min,Brandon Waldon,Nathan Schneider*

Main category: cs.CL

TL;DR: 本论文实证分析了大型语言模型（LLMs）在法律解释中的不稳定性与不可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前学术界和司法实践中，提议将大型语言模型作为法律解释的辅助工具。

Method: 通过对英文文本中法律解释任务的实证研究，分析不同问题格式对模型解释结果的影响，以及模型结果与人类判断的相关性。

Result: 发现模型的解释结果不稳定，问题格式变化会导致模型得出截然不同的结论，且模型与人类判断的相关性较弱且波动较大。

Conclusion: 鉴于大型语言模型在法律解释中的表现不稳定且与人类判断差异显著，过度信赖生成式人工智能得出的法律解释结论具有风险。

Abstract: Legal interpretation frequently involves assessing how a legal text, as
understood by an 'ordinary' speaker of the language, applies to the set of
facts characterizing a legal dispute in the U.S. judicial system. Recent
scholarship has proposed that legal practitioners add large language models
(LLMs) to their interpretive toolkit. This work offers an empirical argument
against LLM interpretation as recently practiced by legal scholars and federal
judges. Our investigation in English shows that models do not provide stable
interpretive judgments: varying the question format can lead the model to
wildly different conclusions. Moreover, the models show weak to moderate
correlation with human judgment, with large variance across model and question
variant, suggesting that it is dangerous to give much credence to the
conclusions produced by generative AI.

</details>


### [39] [CLASS-IT: Conversational and Lecture-Aligned Small-Scale Instruction Tuning for BabyLMs](https://arxiv.org/abs/2510.25364)
*Luca Capone,Alessandro Bondielli,Alessandro Lenci*

Main category: cs.CL

TL;DR: 本文研究了小规模语言模型（LMs）通过指令调优提升性能的效果。


<details>
  <summary>Details</summary>
Motivation: 探讨小规模语言模型是否能从指令调优中获益，尤其比较对话式和问答式指令数据集的不同调优策略。

Method: 使用参数规模为100M和140M的解码器模型，采用合并和顺序两种指令调优课程，评估其在微调（SuperGLUE）和零样本任务（BLiMP等）中的表现。

Result: 指令调优在微调任务中带来小幅且稳定的表现提升，顺序调优优于数据合并，但提升未能稳定迁移至零样本任务，暗示交互适应和广泛语言泛化间存在权衡。

Conclusion: 人类启发的学习策略在小规模LM上存在潜力和局限，未来可采用混合课程策略以在受限训练条件下提升泛化能力。

Abstract: This work investigates whether small-scale LMs can benefit from instruction
tuning. We compare conversational and question-answering instruction tuning
datasets, applied either in a merged or sequential curriculum, using
decoder-only models with 100M and 140M parameters. Evaluation spans both
fine-tuning (SuperGLUE) and zero-shot (BLiMP, EWoK, WUGs, entity tracking, and
psycholinguistic correlation) settings. Results show that instruction tuning
yields small but consistent gains in fine-tuning scenarios, with sequential
curricula outperforming merged data; however, improvements do not consistently
transfer to zero-shot tasks, suggesting a trade-off between interaction-focused
adaptation and broad linguistic generalization. These results highlight both
the potential and the constraints of adapting human-inspired learning
strategies to low-resource LMs, and point toward hybrid, curriculum-based
approaches for enhancing generalization under ecological training limits.

</details>


### [40] [Monitoring Transformative Technological Convergence Through LLM-Extracted Semantic Entity Triple Graphs](https://arxiv.org/abs/2510.25370)
*Alexander Sternfeld,Andrei Kucharavy,Dimitri Percia David,Alain Mermoud,Julian Jang-Jaccard,Nathan Monnet*

Main category: cs.CL

TL;DR: 提出了一种基于大语言模型抽取语义三元组，构建技术实体图谱，结合图谱分析和主题共现趋势分析的新型技术融合监测方法，实现对信息通信技术领域突破性技术的预测。


<details>
  <summary>Details</summary>
Motivation: 传统专家方法难以适应信息通信技术领域创新速度快、术语模糊的问题，亟需自动化、数据驱动的技术预测手段来捕捉转型技术的出现。

Method: 利用大语言模型从非结构化文本中提取语义三元组，构建技术实体关系图，引入名词组合方法对相似技术术语进行聚类，结合图谱指标、多阶段过滤和时序主题共现趋势分析，监测技术融合信号。

Result: 在279K篇arXiv预印本和近1万份专利数据上验证方法，能有效识别已确立及新兴的技术融合模式，体现该框架的可扩展性和通用性。

Conclusion: 基于全文文本分析结合图谱和语义聚类的管道方法，为快速演进领域技术突破的预测提供了实用且通用的解决方案。

Abstract: Forecasting transformative technologies remains a critical but challenging
task, particularly in fast-evolving domains such as Information and
Communication Technologies (ICTs). Traditional expert-based methods struggle to
keep pace with short innovation cycles and ambiguous early-stage terminology.
In this work, we propose a novel, data-driven pipeline to monitor the emergence
of transformative technologies by identifying patterns of technological
convergence.
  Our approach leverages advances in Large Language Models (LLMs) to extract
semantic triples from unstructured text and construct a large-scale graph of
technology-related entities and relations. We introduce a new method for
grouping semantically similar technology terms (noun stapling) and develop
graph-based metrics to detect convergence signals. The pipeline includes
multi-stage filtering, domain-specific keyword clustering, and a temporal trend
analysis of topic co-occurence.
  We validate our methodology on two complementary datasets: 278,625 arXiv
preprints (2017--2024) to capture early scientific signals, and 9,793 USPTO
patent applications (2018-2024) to track downstream commercial developments.
Our results demonstrate that the proposed pipeline can identify both
established and emerging convergence patterns, offering a scalable and
generalizable framework for technology forecasting grounded in full-text
analysis.

</details>


### [41] [Hallucinations in Bibliographic Recommendation: Citation Frequency as a Proxy for Training Data Redundancy](https://arxiv.org/abs/2510.25378)
*Junichiro Niimi*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在生成文献推荐时的错误率，发现高被引论文因训练语料中的高频出现，较少出现虚假引用现象，且超过1000次引用后模型几乎能 verbatim 记忆该文献信息。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在文献推荐时虚构不存在论文的问题，探讨知识是生成还是记忆对生成准确性的影响。

Method: 以引用次数作为训练数据冗余的代理，使用GPT-4.1生成并手动验证100条跨20个计算机科学领域的文献记录，通过计算生成与真实元数据的余弦相似度评估准确性。

Result: 发现(1)虚假引用率因研究领域不同而异；(2)引用次数与事实准确性高度相关；(3)文献被超过约1000次引用后，模型能几乎完全记忆文献信息。

Conclusion: 高被引论文在大型语言模型中被接近逐字记忆，说明存在从泛化到记忆的阈值，提示提高训练数据冗余可降低文献推荐的虚假率。

Abstract: Large language models (LLMs) have been increasingly applied to a wide range
of tasks, from natural language understanding to code generation. While they
have also been used to assist in bibliographic recommendation, the
hallucination of non-existent papers remains a major issue. Building on prior
studies, this study hypothesizes that an LLM's ability to correctly produce
bibliographic information depends on whether the underlying knowledge is
generated or memorized, with highly cited papers (i.e., more frequently appear
in the training corpus) showing lower hallucination rates. We therefore assume
citation count as a proxy for training data redundancy (i.e., the frequency
with which a given bibliographic record is repeatedly represented in the
pretraining corpus) and investigate how citation frequency affects hallucinated
references in LLM outputs. Using GPT-4.1, we generated and manually verified
100 bibliographic records across twenty computer-science domains, and measured
factual consistency via cosine similarity between generated and authentic
metadata. The results revealed that (i) hallucination rates vary across
research domains, (ii) citation count is strongly correlated with factual
accuracy, and (iii) bibliographic information becomes almost verbatimly
memorized beyond approximately 1,000 citations. These findings suggest that
highly cited papers are nearly verbatimly retained in the model, indicating a
threshold where generalization shifts into memorization.

</details>


### [42] [Roleplaying with Structure: Synthetic Therapist-Client Conversation Generation from Questionnaires](https://arxiv.org/abs/2510.25384)
*Doan Nam Long Vu,Rui Tan,Lena Moench,Svenja Jule Francke,Daniel Woiwod,Florian Thomas-Odenthal,Sanna Stroth,Tilo Kircher,Christiane Hermann,Udo Dannlowski,Hamidreza Jamalabadi,Shaoxiong Ji*

Main category: cs.CL

TL;DR: 该论文提出了一个基于大语言模型的生成系统，用于合成结构化心理问卷指导下的治疗对话，解决了隐私限制下真实治疗数据匮乏的问题。


<details>
  <summary>Details</summary>
Motivation: 由于隐私政策严格和临床治疗会话录音稀缺，AI在心理健康领域缺乏真实的治疗对话数据，限制了技术发展。

Method: 基于认知行为疗法原理，构建SQPsych框架，将结构化心理数据转化为自然语言治疗对话，利用开放权重的大语言模型生成高质量合成语料，并通过专家和模型评估进行验证。

Result: SQPsychLLM模型在咨询基准测试中表现优异，关键治疗技能超越基线方法，验证了合成数据在心理健康支持AI中的有效性。

Conclusion: 该研究显示合成对话数据可实现规模化、安全且具临床指导意义的心理健康AI发展，未来将公开代码、模型和语料库。

Abstract: The development of AI for mental health is hindered by a lack of authentic
therapy dialogues, due to strict privacy regulations and the fact that clinical
sessions were historically rarely recorded. We present an LLM-driven pipeline
that generates synthetic counseling dialogues based on structured client
profiles and psychological questionnaires. Grounded on the principles of
Cognitive Behavioral Therapy (CBT), our method creates synthetic therapeutic
conversations for clinical disorders such as anxiety and depression. Our
framework, SQPsych (Structured Questionnaire-based Psychotherapy), converts
structured psychological input into natural language dialogues through
therapist-client simulations. Due to data governance policies and privacy
restrictions prohibiting the transmission of clinical questionnaire data to
third-party services, previous methodologies relying on proprietary models are
infeasible in our setting. We address this limitation by generating a
high-quality corpus using open-weight LLMs, validated through human expert
evaluation and LLM-based assessments. Our SQPsychLLM models fine-tuned on
SQPsychConv achieve strong performance on counseling benchmarks, surpassing
baselines in key therapeutic skills. Our findings highlight the potential of
synthetic data to enable scalable, data-secure, and clinically informed AI for
mental health support. We will release our code, models, and corpus at
https://ai-mh.github.io/SQPsych

</details>


### [43] [BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic Domains](https://arxiv.org/abs/2510.25409)
*Vijay Devane,Mohd Nauman,Bhargav Patel,Aniket Mahendra Wakchoure,Yogeshkumar Sant,Shyam Pawar,Viraj Thakur,Ananya Godse,Sunil Patra,Neha Maurya,Suraj Racha,Nitish Kamal Singh,Ajay Nagpal,Piyush Sawarkar,Kundeshwar Vijayrao Pundalik,Rohit Saluja,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: BhashaBench V1是首个专注于印度特定领域和双语的评测基准，涵盖农业、法律、金融和阿育吠陀四大领域，包含7.4万多问答对。评测29个大语言模型显示其在语言和领域上的表现存在显著差异，尤其在低资源领域表现较弱。此数据集促进对模型在印度多元知识领域和双语理解能力的综合评估。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型评测大多以英语为中心且缺乏针对特定领域的评测，无法准确反映在印度语境下模型的表现，急需一个聚焦印度特定领域和双语的评测平台。

Method: 构建包含7.4万多问答对的BhashaBench V1数据集，问题来自真实政府和专业考试，涵盖农业、法律、金融、阿育吠陀四大领域及500多个具体主题。对29+个语言模型进行多任务、多领域双语评测，定量分析领域和语言表现差异。

Result: 评测中发现模型在不同领域和两种语言的表现差异明显，尤其低资源领域（如阿育吠陀）表现较差；整体而言，模型对英文内容的表现优于印地语；部分子领域如网络法和国际金融表现较好，而五脏疗法和种子科学等领域表现较弱。

Conclusion: BhashaBench V1为评估大语言模型在印度多样知识领域及双语理解能力提供了全面的数据和工具，有助于推进更加公平、细粒度和针对性的模型评估与优化。所有资源均公开以支持开放研究。

Abstract: The rapid advancement of large language models(LLMs) has intensified the need
for domain and culture specific evaluation. Existing benchmarks are largely
Anglocentric and domain-agnostic, limiting their applicability to India-centric
contexts. To address this gap, we introduce BhashaBench V1, the first
domain-specific, multi-task, bilingual benchmark focusing on critical Indic
knowledge systems. BhashaBench V1 contains 74,166 meticulously curated
question-answer pairs, with 52,494 in English and 21,672 in Hindi, sourced from
authentic government and domain-specific exams. It spans four major domains:
Agriculture, Legal, Finance, and Ayurveda, comprising 90+ subdomains and
covering 500+ topics, enabling fine-grained evaluation. Evaluation of 29+ LLMs
reveals significant domain and language specific performance gaps, with
especially large disparities in low-resource domains. For instance, GPT-4o
achieves 76.49% overall accuracy in Legal but only 59.74% in Ayurveda. Models
consistently perform better on English content compared to Hindi across all
domains. Subdomain-level analysis shows that areas such as Cyber Law,
International Finance perform relatively well, while Panchakarma, Seed Science,
and Human Rights remain notably weak. BhashaBench V1 provides a comprehensive
dataset for evaluating large language models across India's diverse knowledge
domains. It enables assessment of models' ability to integrate domain-specific
knowledge with bilingual understanding. All code, benchmarks, and resources are
publicly available to support open research.

</details>


### [44] [Serve Programs, Not Prompts](https://arxiv.org/abs/2510.25412)
*In Gim,Lin Zhong*

Main category: cs.CL

TL;DR: 本文提出了一种名为Symphony的新型大语言模型（LLM）服务系统架构，通过服务程序而非仅文本提示，实现运行时定制和逻辑卸载，提高效率和适应性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM服务系统主要设计用于文本补全，设计不灵活，难以应对日益复杂的应用需求，效率和适应性不足。

Method: 提出LLM推理程序（LIP），允许用户定制令牌预测和KV缓存管理，并将应用逻辑卸载到服务器。同时设计Symphony系统作为LLIP的操作系统，通过系统调用暴露模型计算，使用专用文件系统虚拟化KV缓存，及两级进程调度保证GPU效率。

Result: Symphony系统展示了该架构的可行性，实现了更灵活的模型使用和应用逻辑管理，提升了GPU资源效率和系统的扩展性。

Conclusion: 该架构为更高效、更可扩展的LLM应用生态系统铺平了道路，解决了现有服务系统设计不灵活和效率低下的问题。

Abstract: Current large language model (LLM) serving systems, primarily designed for
text completion, are neither efficient nor adaptable for increasingly complex
LLM applications due to their inflexible design. We propose a new LLM serving
system architecture that serves programs instead of prompts to address this
problem. These programs, called LLM Inference Programs (LIPs), allow users to
customize token prediction and KV cache management at runtime and to offload
parts of their application logic, such as tool execution, to the server. We
describe an example of this architecture through a system named Symphony, which
functions as an operating system for LIPs. Symphony exposes LLM model
computations via system calls and virtualizes KV cache with a dedicated file
system, while ensuring GPU efficiency with a two-level process scheduling
scheme. Symphony has the potential to open the door to a more efficient and
extensible ecosystem for LLM applications.

</details>


### [45] [Seeing, Signing, and Saying: A Vision-Language Model-Assisted Pipeline for Sign Language Data Acquisition and Curation from Social Media](https://arxiv.org/abs/2510.25413)
*Shakib Yazdani,Yasser Hamidullah,Cristina España-Bonet,Josef van Genabith*

Main category: cs.CL

TL;DR: 本文提出了一个基于视觉语言模型（VLM）的自动标注和过滤框架，应用于八种手语的视频数据集，旨在减少人工标注工作量并保证数据质量。


<details>
  <summary>Details</summary>
Motivation: 现有手语翻译数据集规模有限，缺乏多语言覆盖，且依赖专家标注和受控录制导致成本高昂。

Method: 利用VLM进行面部可见性检测、手语动作识别、视频文本提取及对齐判断，实现自动过滤、标注和验证。

Result: 构建了TikTok-SL-8数据集，并在此基础上评估了两种现成手语翻译模型在德语和美式手语上的表现，验证了模型在自动提取的带噪数据上的鲁棒性。

Conclusion: 该方法支持大规模弱监督预训练，促进了从社交媒体获取手语数据的自动化和可扩展性。

Abstract: Most existing sign language translation (SLT) datasets are limited in scale,
lack multilingual coverage, and are costly to curate due to their reliance on
expert annotation and controlled recording setup. Recently, Vision Language
Models (VLMs) have demonstrated strong capabilities as evaluators and real-time
assistants. Despite these advancements, their potential remains untapped in the
context of sign language dataset acquisition. To bridge this gap, we introduce
the first automated annotation and filtering framework that utilizes VLMs to
reduce reliance on manual effort while preserving data quality. Our method is
applied to TikTok videos across eight sign languages and to the already curated
YouTube-SL-25 dataset in German Sign Language for the purpose of additional
evaluation. Our VLM-based pipeline includes a face visibility detection, a sign
activity recognition, a text extraction from video content, and a judgment step
to validate alignment between video and text, implementing generic filtering,
annotation and validation steps. Using the resulting corpus, TikTok-SL-8, we
assess the performance of two off-the-shelf SLT models on our filtered dataset
for German and American Sign Languages, with the goal of establishing baselines
and evaluating the robustness of recent models on automatically extracted,
slightly noisy data. Our work enables scalable, weakly supervised pretraining
for SLT and facilitates data acquisition from social media.

</details>


### [46] [Implicature in Interaction: Understanding Implicature Improves Alignment in Human-LLM Interaction](https://arxiv.org/abs/2510.25426)
*Asutosh Hota,Jussi P. P. Jokinen*

Main category: cs.CL

TL;DR: 该论文研究大型语言模型如何理解上下文隐含意义（含义推断）以提高人机交互质量。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在理解隐含意义方面存在差距，影响人机交互的自然度与对齐效果。

Method: 通过实验评估不同规模的语言模型在含隐含意义提示下的表现，比较隐含意义提示与字面提示的效果。

Result: 大型模型更接近人类理解，含隐含意义的提示显著提升所有模型响应的相关性和质量，尤其是小模型获益明显。

Conclusion: 利用语言学中的隐含意义理论能改善人机对齐，使人机交互更自然、更符合语境预期。

Abstract: The rapid advancement of Large Language Models (LLMs) is positioning language
at the core of human-computer interaction (HCI). We argue that advancing HCI
requires attention to the linguistic foundations of interaction, particularly
implicature (meaning conveyed beyond explicit statements through shared
context) which is essential for human-AI (HAI) alignment. This study examines
LLMs' ability to infer user intent embedded in context-driven prompts and
whether understanding implicature improves response generation. Results show
that larger models approximate human interpretations more closely, while
smaller models struggle with implicature inference. Furthermore,
implicature-based prompts significantly enhance the perceived relevance and
quality of responses across models, with notable gains in smaller models.
Overall, 67.6% of participants preferred responses with implicature-embedded
prompts to literal ones, highlighting a clear preference for contextually
nuanced communication. Our work contributes to understanding how linguistic
theory can be used to address the alignment problem by making HAI interaction
more natural and contextually grounded.

</details>


### [47] [RLMEval: Evaluating Research-Level Neural Theorem Proving](https://arxiv.org/abs/2510.25427)
*Auguste Poiroux,Antoine Bosselut,Viktor Kunčak*

Main category: cs.CL

TL;DR: RLMEval 是一个针对研究级数学定理神经证明和自动形式化的评测套件，基于真实的 Lean 项目，揭示了现有模型在实战中的表现不足，最佳模型通过率仅为10.3%。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在已有基准测试中表现优异，但在研究级神经定理证明和自动证明形式化的实际应用中效果有限，因此需要一个更具挑战性的评测工具。

Method: 提出 RLMEval 评测套件，利用来自真实 Lean Blueprint 项目的613个定理，系统评估现有神经定理证明和自动形式化模型。

Result: 在 RLMEval 上评测的最优模型通过率只有10.3%，显示现有模型在现实复杂数学环境中仍存在显著不足。

Conclusion: RLMEval 为自动化形式数学推理提供了一个新的、更加困难的评测标准，有助于推动该领域的进展。

Abstract: Despite impressive results on curated benchmarks, the practical impact of
large language models (LLMs) on research-level neural theorem proving and proof
autoformalization is still limited. We introduce RLMEval, an evaluation suite
for these tasks, focusing on research-level mathematics from real-world Lean
formalization projects. RLMEval targets the evaluation of neural theorem
proving and proof autoformalization on challenging research-level theorems by
leveraging real Lean Blueprint formalization projects. Our evaluation of
state-of-the-art models on RLMEval, comprising 613 theorems from 6 Lean
projects, reveals a significant gap: progress on existing benchmarks does not
readily translate to these more realistic settings, with the best model
achieving only a 10.3 % pass rate. RLMEval provides a new, challenging
benchmark designed to guide and accelerate progress in automated reasoning for
formal mathematics.

</details>


### [48] [Depth and Autonomy: A Framework for Evaluating LLM Applications in Social Science Research](https://arxiv.org/abs/2510.25432)
*Ali Sanaei,Ali Rajabzadeh*

Main category: cs.CL

TL;DR: 本文提出了一个将大型语言模型（LLMs）在定性社会科学研究中应用分类的框架，基于解释深度和自主性两个维度，旨在解决解释偏差、低可靠性和可审计性差的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在定性社会科学研究中应用广泛，但面临解释偏差、可靠性低和审计性弱等挑战，亟需一种分类和设计方法指导合理使用。

Method: 作者建立了一个基于解释深度和自主性两个维度的框架，系统回顾了Web of Science中利用LLMs作为工具的社会科学研究，并提出将任务拆解、降低模型自主性并选择性提升解释深度的研究策略。

Result: 研究表明通过保持模型低自主性并在必要且监督条件下增加解释深度，能够在保证透明性和可靠性的同时有效利用LLMs。

Conclusion: 采用分解任务、限制自主性和有控制地提升解释深度的方法，研究人员可以获得LLMs的优势，同时避免其潜在风险，提高定性社会科学研究的质量和可信度。

Abstract: Large language models (LLMs) are increasingly utilized by researchers across
a wide range of domains, and qualitative social science is no exception;
however, this adoption faces persistent challenges, including interpretive
bias, low reliability, and weak auditability. We introduce a framework that
situates LLM usage along two dimensions, interpretive depth and autonomy,
thereby offering a straightforward way to classify LLM applications in
qualitative research and to derive practical design recommendations. We present
the state of the literature with respect to these two dimensions, based on all
published social science papers available on Web of Science that use LLMs as a
tool and not strictly as the subject of study. Rather than granting models
expansive freedom, our approach encourages researchers to decompose tasks into
manageable segments, much as they would when delegating work to capable
undergraduate research assistants. By maintaining low levels of autonomy and
selectively increasing interpretive depth only where warranted and under
supervision, one can plausibly reap the benefits of LLMs while preserving
transparency and reliability.

</details>


### [49] [A Critical Study of Automatic Evaluation in Sign Language Translation](https://arxiv.org/abs/2510.25434)
*Shakib Yazdani,Yasser Hamidullah,Cristina España-Bonet,Eleftherios Avramidis,Josef van Genabith*

Main category: cs.CL

TL;DR: 本文分析了当前手语翻译评估指标的局限性，特别是文本基础指标如BLEU和ROUGE，并比较了基于大型语言模型的评估方法，发现后者在语义捕捉方面更优但存在偏差。


<details>
  <summary>Details</summary>
Motivation: 当前手语翻译的自动评估主要依赖文本指标，但这些指标在准确反映翻译质量方面存在不确定性，亟需深入分析和改进。

Method: 对比分析了六种文本评估指标和两种基于大型语言模型的评估工具，在语义重述、模型输出幻觉和句长变化三种条件下评估指标的一致性和鲁棒性。

Result: 发现传统文本指标在词汇重叠评价中存在明显不足，LLM评估更好地捕捉语义，但对LLM生成的重述表现出偏向，同时所有指标对幻觉检测有效，BLEU过于敏感，BLEURT及LLM评估较宽容。

Conclusion: 单一文本指标不足以全面评估手语翻译质量，未来需要发展多模态评估框架，实现对手语翻译结果的更全面、准确判断。

Abstract: Automatic evaluation metrics are crucial for advancing sign language
translation (SLT). Current SLT evaluation metrics, such as BLEU and ROUGE, are
only text-based, and it remains unclear to what extent text-based metrics can
reliably capture the quality of SLT outputs. To address this gap, we
investigate the limitations of text-based SLT evaluation metrics by analyzing
six metrics, including BLEU, chrF, and ROUGE, as well as BLEURT on the one
hand, and large language model (LLM)-based evaluators such as G-Eval and GEMBA
zero-shot direct assessment on the other hand. Specifically, we assess the
consistency and robustness of these metrics under three controlled conditions:
paraphrasing, hallucinations in model outputs, and variations in sentence
length. Our analysis highlights the limitations of lexical overlap metrics and
demonstrates that while LLM-based evaluators better capture semantic
equivalence often missed by conventional metrics, they can also exhibit bias
toward LLM-paraphrased translations. Moreover, although all metrics are able to
detect hallucinations, BLEU tends to be overly sensitive, whereas BLEURT and
LLM-based evaluators are comparatively lenient toward subtle cases. This
motivates the need for multimodal evaluation frameworks that extend beyond
text-based metrics to enable a more holistic assessment of SLT outputs.

</details>


### [50] [Grounded in Reality: Learning and Deploying Proactive LLM from Offline Logs](https://arxiv.org/abs/2510.25441)
*Fei Wei,Daoyuan Chen,Ce Wang,Yilun Huang,Yushuo Chen,Xuchen Pan,Yaliang Li,Bolin Ding*

Main category: cs.CL

TL;DR: 本文提出了Learn-to-Ask框架，基于离线专家数据训练大语言模型，实现在医疗领域的主动对话Agent，性能超过人类专家。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型多为被动回答者，缺乏主动、目标导向的能力，且现有方法依赖高成本用户模拟器，存在现实差距。

Method: 通过利用专家轨迹的未来信息，推断逐轮奖励信号，将长远问题分解为监督学习任务，训练包含动作和状态评估的策略，并用自动评分校准减少奖励模型噪声。

Result: 在真实医疗数据集上的多规模LLM测试中，Learn-to-Ask显著提升主动对话能力，并成功部署于大规模在线AI服务，表现优于人类专家。

Conclusion: 该框架有效利用离线数据训练主动对话模型，突破被动限制，具备实际应用价值和推广潜力。

Abstract: Large Language Models (LLMs) excel as passive responders, but teaching them
to be proactive, goal-oriented partners, a critical capability in high-stakes
domains, remains a major challenge. Current paradigms either myopically
optimize single-turn attributes or rely on brittle, high-cost user simulators,
creating a persistent ``reality gap''. To bridge this gap, we introduce
\texttt{Learn-to-Ask}, a general, simulator-free framework for learning and
deploying proactive dialogue agents \textit{directly from offline expert data},
bypassing the need to model complex user dynamics. Our key insight is to
reframe the offline policy learning problem by leveraging the \textbf{observed
future} of each expert trajectory. This allows us to infer a dense,
turn-by-turn reward signal grounded in the expert's revealed strategy,
decomposing the intractable long-horizon problem into a series of supervised
learning tasks, and training a policy to output a structured \texttt{(action,
state_assessment)} tuple, governing both \textbf{what to ask} and, crucially,
\textbf{when to stop}. To ensure reward fidelity, our Automated Grader
Calibration pipeline systematically purges noise from the LLM-based reward
model with minimal human supervision. Empirically, we demonstrate the efficacy
of \texttt{Learn-to-Ask} in a real-world medical dataset, using LLMs of varying
sizes up to 32B. Our approach culminates in the successful deployment of LLMs
into a live, large-scale online AI service. In rigorous in-house evaluations,
our model was launched and achieved performance even superior to human experts,
proving our framework's ability to translate offline data into tangible,
real-world impact. We hope this work provides a practical and economically
viable blueprint for transforming passive LLMs into proactive, goal-oriented
LLM applications.

</details>


### [51] [Fine-Tuned Language Models for Domain-Specific Summarization and Tagging](https://arxiv.org/abs/2510.25460)
*Jun Wang,Fuming Lin,Yuyu Chen*

Main category: cs.CL

TL;DR: 本文提出了一种结合大语言模型微调和命名实体识别的领域文本摘要与标注方法，针对新兴子文化语言和俚语带来的信息提取挑战，有效提升了政治和安全领域文本的自动摘要与标注准确率，实现快速文档分类和分发。


<details>
  <summary>Details</summary>
Motivation: 面对快速变化的子文化语言和俚语，传统自动信息提取和执法监控难以有效处理复杂文本，亟需提升领域特定文本的摘要与标注能力。

Method: 基于LLaMA Factory框架，作者在通用和特定领域数据集上对大语言模型进行微调，结合命名实体识别，实现领域文本的高效摘要和结构化标注，采用BLEU和ROUGE指标评测模型表现。

Result: 微调后模型在摘要和标注准确率上显著提升，特别是专用领域语料中表现优异。LLaMA3-8B-Instruct模型通过领域微调，跨语言能力得到增强，优于中文训练模型。

Conclusion: 该管道实现了可扩展、实时的文本摘要与实体标注，支持快速文档管理和新兴语言趋势捕捉，结合LLMs与NER为现代知识管理和安全操作提供了有效解决方案。

Abstract: This paper presents a pipeline integrating fine-tuned large language models
(LLMs) with named entity recognition (NER) for efficient domain-specific text
summarization and tagging. The authors address the challenge posed by rapidly
evolving sub-cultural languages and slang, which complicate automated
information extraction and law enforcement monitoring. By leveraging the LLaMA
Factory framework, the study fine-tunes LLMs on both generalpurpose and custom
domain-specific datasets, particularly in the political and security domains.
The models are evaluated using BLEU and ROUGE metrics, demonstrating that
instruction fine-tuning significantly enhances summarization and tagging
accuracy, especially for specialized corpora. Notably, the LLaMA3-8B-Instruct
model, despite its initial limitations in Chinese comprehension, outperforms
its Chinese-trained counterpart after domainspecific fine-tuning, suggesting
that underlying reasoning capabilities can transfer across languages. The
pipeline enables concise summaries and structured entity tagging, facilitating
rapid document categorization and distribution. This approach proves scalable
and adaptable for real-time applications, supporting efficient information
management and the ongoing need to capture emerging language trends. The
integration of LLMs and NER offers a robust solution for transforming
unstructured text into actionable insights, crucial for modern knowledge
management and security operations.

</details>


### [52] [TwinVoice: A Multi-dimensional Benchmark Towards Digital Twins via LLM Persona Simulation](https://arxiv.org/abs/2510.25536)
*Bangde Du,Minghao Guo,Songming He,Ziyi Ye,Xi Zhu,Weihang Su,Shuqi Zhu,Yujia Zhou,Yongfeng Zhang,Qingyao Ai,Yiqun Liu*

Main category: cs.CL

TL;DR: TwinVoice是一个评估大型语言模型模拟个性的新基准，涵盖社交、私下和叙事三种个性维度，并细分六项能力。实验显示现有模型在某些能力上表现一般，整体低于人类水平。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型模拟个性的评估多依赖合成对话，缺乏系统框架和能力需求分析，难以全面衡量模型的真实表现。

Method: 提出TwinVoice基准，从社交个性、私密对话个性和叙事角色表达三个维度评估，细分为意见一致性、记忆回忆、逻辑推理、词汇忠实度、个性语调和句法风格六大能力。

Result: 实验结果表明先进模型在模拟个性方面达到中等准确率，但在句法风格和记忆回忆等能力上表现较弱，整体性能远低于人类基线水平。

Conclusion: 尽管大型语言模型在模拟个性方面表现出潜力，但目前能力仍有限，未来需在记忆和语言风格等方面进一步提升以达到人类水平。

Abstract: Large Language Models (LLMs) are exhibiting emergent human-like abilities and
are increasingly envisioned as the foundation for simulating an individual's
communication style, behavioral tendencies, and personality traits. However,
current evaluations of LLM-based persona simulation remain limited: most rely
on synthetic dialogues, lack systematic frameworks, and lack analysis of the
capability requirement. To address these limitations, we introduce TwinVoice, a
comprehensive benchmark for assessing persona simulation across diverse
real-world contexts. TwinVoice encompasses three dimensions: Social Persona
(public social interactions), Interpersonal Persona (private dialogues), and
Narrative Persona (role-based expression). It further decomposes the evaluation
of LLM performance into six fundamental capabilities, including opinion
consistency, memory recall, logical reasoning, lexical fidelity, persona tone,
and syntactic style. Experimental results reveal that while advanced models
achieve moderate accuracy in persona simulation, they still fall short of
capabilities such as syntactic style and memory recall. Consequently, the
average performance achieved by LLMs remains considerably below the human
baseline.

</details>


### [53] [Communication and Verification in LLM Agents towards Collaboration under Information Asymmetry](https://arxiv.org/abs/2510.25595)
*Run Peng,Ziqiao Ma,Amy Pang,Sikai Li,Zhang Xi-Jia,Yingzhuo Yu,Cristian-Paul Bara,Joyce Chai*

Main category: cs.CL

TL;DR: 本文研究了在信息不对称条件下，多大语言模型（LLM）智能体如何协作完成任务，提出了一种基于细调和环境验证的框架，以提升智能体的沟通和理解能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体多聚焦于独立的任务规划与生成，缺乏对多智能体协作能力的深入研究，特别是在知识和技能存在差异的信息不对称情境下。

Method: 基于扩展的爱因斯坦谜题的桌面游戏，设计两智能体需通过推理、沟通与动作满足空间和关系约束。采用细调加验证器框架，赋予智能体多种沟通策略及环境验证信号。

Result: 实验显示，沟通策略的对齐尤为重要，具备信息索取与提供能力的智能体表现最佳。无沟通智能体虽任务完成率高，但缺乏规则理解且人类评价信任度低。引入环境验证器显著提升规则理解及任务完成效果。

Conclusion: 结合环境验证的沟通机制不仅增强了智能体间的协作效率，也提升了任务理解深度与安全性，为构建更可靠、可解释的AI协作系统提供了有效路径。

Abstract: While Large Language Model (LLM) agents are often approached from the angle
of action planning/generation to accomplish a goal (e.g., given by language
descriptions), their abilities to collaborate with each other to achieve a
joint goal are not well explored. To address this limitation, this paper
studies LLM agents in task collaboration, particularly under the condition of
information asymmetry, where agents have disparities in their knowledge and
skills and need to work together to complete a shared task. We extend Einstein
Puzzles, a classical symbolic puzzle, to a table-top game. In this game, two
LLM agents must reason, communicate, and act to satisfy spatial and relational
constraints required to solve the puzzle. We apply a fine-tuning-plus-verifier
framework in which LLM agents are equipped with various communication
strategies and verification signals from the environment. Empirical results
highlight the critical importance of aligned communication, especially when
agents possess both information-seeking and -providing capabilities.
Interestingly, agents without communication can still achieve high task
performance; however, further analysis reveals a lack of true rule
understanding and lower trust from human evaluators. Instead, by integrating an
environment-based verifier, we enhance agents' ability to comprehend task rules
and complete tasks, promoting both safer and more interpretable collaboration
in AI systems. https://github.com/Roihn/EinsteinPuzzles

</details>


### [54] [FARSIQA: Faithful and Advanced RAG System for Islamic Question Answering](https://arxiv.org/abs/2510.25621)
*Mohammad Aghajani Asl,Behrooz Minaei Bidgoli*

Main category: cs.CL

TL;DR: 本文提出了FARSIQA系统，基于创新的FAIR-RAG架构，实现了波斯语伊斯兰领域高准确性的问答，显著提升了复杂多步推理的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在高风险、专业领域如宗教问答中存在幻觉和不可靠问题，且传统检索增强生成系统在处理复杂多步推理时表现不足，特别是对波斯语穆斯林社区，准确可信尤为重要。

Method: 提出FAIR-RAG架构，采用动态、自我纠正的迭代流程：对复杂查询进行自适应分解，评估证据充分性，并通过多轮生成子查询逐步填补信息空白，依托包含百万条权威伊斯兰文献的知识库进行问答。

Result: 在IslamicPCQA基准测试中，FARSIQA实现了97.0%的负面拒绝率（较基线提升40个百分点）和74.3%的答案正确率，表现出色。

Conclusion: FARSIQA及其迭代、自适应的FAIR-RAG架构为波斯语伊斯兰问答设立了新标准，证明此方法对打造在敏感领域可信赖的AI系统至关重要。

Abstract: The advent of Large Language Models (LLMs) has revolutionized Natural
Language Processing, yet their application in high-stakes, specialized domains
like religious question answering is hindered by challenges like hallucination
and unfaithfulness to authoritative sources. This issue is particularly
critical for the Persian-speaking Muslim community, where accuracy and
trustworthiness are paramount. Existing Retrieval-Augmented Generation (RAG)
systems, relying on simplistic single-pass pipelines, fall short on complex,
multi-hop queries requiring multi-step reasoning and evidence aggregation. To
address this gap, we introduce FARSIQA, a novel, end-to-end system for Faithful
Advanced Question Answering in the Persian Islamic domain. FARSIQA is built
upon our innovative FAIR-RAG architecture: a Faithful, Adaptive, Iterative
Refinement framework for RAG. FAIR-RAG employs a dynamic, self-correcting
process: it adaptively decomposes complex queries, assesses evidence
sufficiency, and enters an iterative loop to generate sub-queries,
progressively filling information gaps. Operating on a curated knowledge base
of over one million authoritative Islamic documents, FARSIQA demonstrates
superior performance. Rigorous evaluation on the challenging IslamicPCQA
benchmark shows state-of-the-art performance: the system achieves a remarkable
97.0% in Negative Rejection - a 40-point improvement over baselines - and a
high Answer Correctness score of 74.3%. Our work establishes a new standard for
Persian Islamic QA and validates that our iterative, adaptive architecture is
crucial for building faithful, reliable AI systems in sensitive domains.

</details>


### [55] [Evaluating the Role of Verifiers in Test-Time Scaling for Legal Reasoning Tasks](https://arxiv.org/abs/2510.25623)
*Davide Romano,Jonathan Schwarz,Daniele Giofré*

Main category: cs.CL

TL;DR: 本文研究了测试时尺度调整技术在法律多项选择问答中的应用，评估不同验证器模型对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 现有测试时尺度调整技术在正式领域有效，但在法律等论证领域应用较少，需探究其效果。

Method: 文章基于7个奖励模型，评估Outcome层面和Process层面两种验证方法，在有限计算预算下分析验证器的性能表现。

Result: 验证器效用受领域专门化、模型规模和监督类型影响显著，不同方法和模型角色间表现差异明显。

Conclusion: 测试时尺度调整技术在法律领域具有潜力，其效果依赖于验证器的设计和训练方式，未来研究应关注这些因素的优化。

Abstract: Test-time scaling (TTS) techniques can improve the performance of large
language models (LLMs) at the expense of additional computation and latency.
While TTS has proven effective in formal domains such as mathematics and
programming \citep{snell2024scaling, chen2024more}, its value in argumentative
domains such as law remains underexplored. We present an empirical study of
verifier-based TTS methods for legal multiple-choice QA (MCQA) across five
benchmarks. Using a family of 7 reward models, we evaluate both outcome-level
(Best-of-$N$) and process-level (tree search) verification under realistic
low-$N$ budgets. Our analysis systematically investigates how verifier utility
is affected by key properties such as domain specialization, model size, and
supervision type (process-supervised PRMs vs. outcome-only ORMs), even when
applied across different roles.

</details>


### [56] [Are Language Models Efficient Reasoners? A Perspective from Logic Programming](https://arxiv.org/abs/2510.25626)
*Andreas Opedal,Yanick Zengaffinen,Haruki Shirakami,Clemente Pasti,Mrinmaya Sachan,Abulhair Saparov,Ryan Cotterell,Bernhard Schölkopf*

Main category: cs.CL

TL;DR: 本文提出评估语言模型推理效率的新框架，重点分析模型在逻辑推理中避免不必要推断的能力。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型虽然推理能力强，但评估多聚焦正确性，忽视了推理效率这一关键人类推理特征。实际推理场景中存在大量无关信息，理有效推理需能识别并忽略这些信息。

Method: 提出基于逻辑编程的新框架，通过对比自然语言中由语言模型生成的证明和逻辑程序执行所得最短证明，衡量推理效率。构建包含不同无关公理的数据集，用以测试模型识别干扰信息的能力。

Result: 实验证明，当前语言模型在存在少量但相关领域的无关信息时准确率显著下降，生成的证明常绕道无关推理，显示出推理效率不足。

Conclusion: 语言模型在推理效率层面仍有不足，未来需设计更有效的机制以提升模型在复杂、干扰信息丰富环境中的推理能力。

Abstract: Modern language models (LMs) exhibit strong deductive reasoning capabilities,
yet standard evaluations emphasize correctness while overlooking a key aspect
of human-like reasoning: efficiency. In real-world reasoning scenarios, much of
the available information is irrelevant, and effective deductive inference
requires identifying and ignoring such distractions. We propose a framework for
assessing LM reasoning efficiency through the lens of logic programming,
introducing a simple method to align proofs written in natural language -- as
generated by an LM -- with shortest proofs found by executing the logic
program. Efficiency is quantified by measuring how well a model avoids
unnecessary inference. Empirically, we construct a dataset of math word
problems injected with various number of irrelevant axioms that vary in
semantic overlap with the goal theorem. We find that current LMs show marked
accuracy declines under such conditions -- even with minimal, domain-consistent
distractions -- and the proofs they generate frequently exhibit detours through
irrelevant inferences.

</details>


### [57] [EHR-R1: A Reasoning-Enhanced Foundational Language Model for Electronic Health Record Analysis](https://arxiv.org/abs/2510.25628)
*Yusheng Liao,Chaoyi Wu,Junwei Liu,Shuyang Jiang,Pengcheng Qiu,Haowen Wang,Yun Yue,Shuai Zhen,Jian Wang,Qianrui Fan,Jinjie Gu,Ya Zhang,Yanfeng Wang,Yu Wang,Weidi Xie*

Main category: cs.CL

TL;DR: 本文提出了针对电子健康记录（EHR）分析的全新大型推理指令数据集EHR-Ins，以及基于该数据集训练的推理增强大语言模型EHR-R1和评测基准EHR-Bench，显著提升了EHR分析的精准性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在临床电子健康记录分析上的任务覆盖有限，缺乏针对EHR的推理能力，难以满足临床决策需求。

Method: 设计了基于思维图的框架生成高质量推理数据集EHR-Ins，开发了包含72B参数的推理增强LLM系列EHR-R1，通过领域适应、推理强化及强化学习的多阶段训练方法提升模型能力。

Result: 在包含42项任务的EHR-Bench基准上，EHR-R1模型在MIMIC-Bench上超越GPT-4o 30多分，在EHRSHOT零样本AUROC提升10%，优于多种商用及开源大模型。

Conclusion: 提出的EHR-Ins数据集、EHR-R1模型和EHR-Bench评测基准共同推动了电子健康记录分析领域的可靠性和临床相关性的进步。

Abstract: Electronic Health Records (EHRs) contain rich yet complex information, and
their automated analysis is critical for clinical decision-making. Despite
recent advances of large language models (LLMs) in clinical workflows, their
ability to analyze EHRs remains limited due to narrow task coverage and lack of
EHR-oriented reasoning capabilities. This paper aims to bridge the gap,
specifically, we present EHR-Ins, a large-scale, comprehensive EHR reasoning
instruction dataset, comprising 300k high-quality reasoning cases and 4M
non-reasoning cases across 42 distinct EHR tasks. Its core innovation is a
thinking-graph-driven framework that enables to generate high-quality reasoning
data at scale. Based on it, we develop EHR-R1, a series of reasoning-enhanced
LLMs with up to 72B parameters tailored for EHR analysis. Through a multi-stage
training paradigm, including domain adaptation, reasoning enhancement, and
reinforcement learning, EHR-R1 systematically acquires domain knowledge and
diverse reasoning capabilities, enabling accurate and robust EHR analysis.
Lastly, we introduce EHR-Bench, a new benchmark curated from MIMIC-IV, spanning
42 tasks, to comprehensively assess reasoning and prediction across EHR
scenarios. In experiments, we show that the resulting EHR-R1 consistently
outperforms state-of-the-art commercial and open-source LLMs (including
DeepSeek-V3 and GPT-4o), surpassing GPT-4o by over 30 points on MIMIC-Bench and
achieving a 10\% higher zero-shot AUROC on EHRSHOT. Collectively, EHR-Ins,
EHR-R1, and EHR-Bench have significantly advanced the development for more
reliable and clinically relevant EHR analysis.

</details>


### [58] [PairUni: Pairwise Training for Unified Multimodal Language Models](https://arxiv.org/abs/2510.25682)
*Jiani Zheng,Zhiyang Teng,Xiangtai Li,Anran Wang,Yu Tian,Kunpeng Qiu,Ye Tian,Haochen Wang,Zhuochen Wang*

Main category: cs.CL

TL;DR: 本文提出了PairUni框架，通过将视觉语言理解和生成任务数据配对，利用Pair-GPRO优化策略提升统一视觉语言模型在强化学习中的表现，实现了多任务平衡和性能提升。


<details>
  <summary>Details</summary>
Motivation: 统一视觉语言模型需要兼顾理解和生成任务，但这两类任务的数据和监督方式不同，强化学习时难以平衡二者。

Method: 使用GPT-3扩展单任务数据，生成理解-生成（UG）数据对；通过语义检索形成相关样本对；提出基于组相对策略优化的Pair-GPRO方法，针对配对样本分配相似度调节学习优势。

Result: 在自建16K UG对数据集PairUG上对强大的Janus-Pro模型微调，显著提升模型在多任务上的表现，超过其他强化学习基线。

Conclusion: 通过数据配对和配对感知策略优化，PairUni成功解决了多任务强化学习中的平衡难题，实现了统一视觉语言模型的综合性能提升。

Abstract: Unified vision-language models (UVLMs) must perform both understanding and
generation within a single architecture, but these tasks rely on heterogeneous
data and supervision, making it difficult to balance them during reinforcement
learning (RL). We propose PairUni, a unified framework that reorganizes data
into understanding-generation (UG) pairs and aligns optimization accordingly.
We first use GPT-o3 to augment single-task data, generating captions for
understanding samples and question-answer (QA) pairs for generation samples,
forming aligned pairs from the same instance. Additionally, for each generation
sample, we retrieve a semantically related understanding example to form a
retrieved pair, linking different but related data points. These paired
structures expose cross-task semantic correspondences and support consistent
policy learning. To leverage this structure, we present Pair-GPRO, a pair-aware
variant based on Group Relative Policy Optimization. It assigns a similarity
score to each pair to modulate the advantage, strengthening learning from
well-aligned examples and reducing task interference. We curate a high-quality
dataset of 16K UG pairs named PairUG for RL fine-tuning and evaluate PairUni on
the powerful Janus-Pro UVLMs. Our approach achieves balanced improvements on
various UVLMs, outperforming strong UVLM RL baselines. Code:
\href{https://github.com/Haochen-Wang409/PairUni}{github.com/Haochen-Wang409/PairUni}

</details>


### [59] [Interpreting LLMs as Credit Risk Classifiers: Do Their Feature Explanations Align with Classical ML?](https://arxiv.org/abs/2510.25701)
*Saeed AlMarri,Kristof Juhasz,Mathieu Ravaut,Gautier Marti,Hamdan Al Ahbabi,Ibrahim Elfadel*

Main category: cs.CL

TL;DR: 本文比较了大语言模型（LLMs）和LightGBM在贷款违约预测中的表现，发现LLMs虽能识别关键风险指标，但其特征重要性排序与LightGBM差异大且自我解释可靠性不足。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs作为零样本分类器在结构化金融数据中的适用性及其在金融风险评估中的表现和解释性问题。

Method: 系统比较零-shot LLM分类器与LightGBM在真实贷款违约预测任务上的性能，使用SHAP分析特征贡献，评估LLM自我解释的可靠性。

Result: LLMs能识别关键风险指标，但其特征重要性排序与LightGBM差异显著，自我解释与SHAP贡献不一致，显示LLMs作为独立模型存在局限。

Conclusion: LLMs在结构化金融风险预测中存在解释性和可信度问题，建议进行解释性审计、与可解释模型对比及人机协同监督以确保风险环境下的可靠应用。

Abstract: Large Language Models (LLMs) are increasingly explored as flexible
alternatives to classical machine learning models for classification tasks
through zero-shot prompting. However, their suitability for structured tabular
data remains underexplored, especially in high-stakes financial applications
such as financial risk assessment. This study conducts a systematic comparison
between zero-shot LLM-based classifiers and LightGBM, a state-of-the-art
gradient-boosting model, on a real-world loan default prediction task. We
evaluate their predictive performance, analyze feature attributions using SHAP,
and assess the reliability of LLM-generated self-explanations. While LLMs are
able to identify key financial risk indicators, their feature importance
rankings diverge notably from LightGBM, and their self-explanations often fail
to align with empirical SHAP attributions. These findings highlight the
limitations of LLMs as standalone models for structured financial risk
prediction and raise concerns about the trustworthiness of their self-generated
explanations. Our results underscore the need for explainability audits,
baseline comparisons with interpretable models, and human-in-the-loop oversight
when deploying LLMs in risk-sensitive financial environments.

</details>


### [60] [The Tool Decathlon: Benchmarking Language Agents for Diverse, Realistic, and Long-Horizon Task Execution](https://arxiv.org/abs/2510.25726)
*Junlong Li,Wenshuo Zhao,Jian Zhao,Weihao Zeng,Haoze Wu,Xiaochen Wang,Rui Ge,Yuxuan Cao,Yuzhen Huang,Wei Liu,Junteng Liu,Zhaochen Su,Yiyang Guo,Fan Zhou,Lueyang Zhang,Juan Michelini,Xingyao Wang,Xiang Yue,Shuyan Zhou,Graham Neubig,Junxian He*

Main category: cs.CL

TL;DR: 提出了Tool Decathlon（Toolathlon）基准，涵盖32个应用和604个工具，提供真实多样环境和长流程任务，评估语言代理的实际能力。


<details>
  <summary>Details</summary>
Motivation: 现有语言代理基准多聚焦狭窄领域和简化任务，缺乏多样性、现实性及长流程复杂性，难以真实评估语言代理性能。

Method: 构建Toolathlon基准，包含32个软件和604个工具，提供真实环境初始状态和108个多应用、多轮次任务，使用严格执行验证脚本进行评价。

Result: 最优模型Claude-4.5-Sonnet成功率仅38.6%，工具调用约20.2次，开源模型DeepSeek-V3.2-Exp成功率20.1%，显示现有模型能力不足。

Conclusion: Toolathlon基准彰显当前语言代理在复杂多应用长流程任务中的不足，有助于推动更强大语言代理的发展。

Abstract: Real-world language agents must handle complex, multi-step workflows across
diverse Apps. For instance, an agent may manage emails by coordinating with
calendars and file systems, or monitor a production database to detect
anomalies and generate reports following an operating manual. However, existing
language agent benchmarks often focus on narrow domains or simplified tasks
that lack the diversity, realism, and long-horizon complexity required to
evaluate agents' real-world performance. To address this gap, we introduce the
Tool Decathlon (dubbed as Toolathlon), a benchmark for language agents offering
diverse Apps and tools, realistic environment setup, and reliable
execution-based evaluation. Toolathlon spans 32 software applications and 604
tools, ranging from everyday platforms such as Google Calendar and Notion to
professional ones like WooCommerce, Kubernetes, and BigQuery. Most of the tools
are based on a high-quality set of Model Context Protocol (MCP) servers that we
may have revised or implemented ourselves. Unlike prior works, which primarily
ensure functional realism but offer limited environment state diversity, we
provide realistic initial environment states from real software, such as Canvas
courses with dozens of students or real financial spreadsheets. This benchmark
includes 108 manually sourced or crafted tasks in total, requiring interacting
with multiple Apps over around 20 turns on average to complete. Each task is
strictly verifiable through dedicated evaluation scripts. Comprehensive
evaluation of SOTA models highlights their significant shortcomings: the
best-performing model, Claude-4.5-Sonnet, achieves only a 38.6% success rate
with 20.2 tool calling turns on average, while the top open-weights model
DeepSeek-V3.2-Exp reaches 20.1%. We expect Toolathlon to drive the development
of more capable language agents for real-world, long-horizon task execution.

</details>


### [61] [The Limits of Obliviate: Evaluating Unlearning in LLMs via Stimulus-Knowledge Entanglement-Behavior Framework](https://arxiv.org/abs/2510.25732)
*Aakriti Shah,Thai Le*

Main category: cs.CL

TL;DR: 本文提出了SKeB框架，利用劝导式提示增强大语言模型（LLMs）中刻意遗忘信息的事实知识召回能力，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在管理敏感数据和纠正错误信息时，需有效实现信息遗忘，但评估遗忘效果仍存在挑战。

Method: 基于ACT-R和Hebbian理论提出SKeB框架，构建领域图模拟信息纠缠，设计纠缠度量评估事实性及幻觉等输出，采用劝导式提示强化知识激活。

Result: 劝导式提示显著提升事实知识召回率（基线14.8%，权威提示24.5%），效果与模型规模呈负相关（2.7B参数模型提升128%，13B模型提升15%）。

Conclusion: SKeB框架为评估大语言模型遗忘完整性和鲁棒性提供了工具，劝导提示是提升遗忘模型事实召回的有效方法。

Abstract: Unlearning in large language models (LLMs) is crucial for managing sensitive
data and correcting misinformation, yet evaluating its effectiveness remains an
open problem. We investigate whether persuasive prompting can recall factual
knowledge from deliberately unlearned LLMs across models ranging from 2.7B to
13B parameters (OPT-2.7B, LLaMA-2-7B, LLaMA-3.1-8B, LLaMA-2-13B). Drawing from
ACT-R and Hebbian theory (spreading activation theories), as well as
communication principles, we introduce Stimulus-Knowledge Entanglement-Behavior
Framework (SKeB), which models information entanglement via domain graphs and
tests whether factual recall in unlearned models is correlated with persuasive
framing. We develop entanglement metrics to quantify knowledge activation
patterns and evaluate factuality, non-factuality, and hallucination in outputs.
Our results show persuasive prompts substantially enhance factual knowledge
recall (14.8% baseline vs. 24.5% with authority framing), with effectiveness
inversely correlated to model size (128% recovery in 2.7B vs. 15% in 13B). SKeB
provides a foundation for assessing unlearning completeness, robustness, and
overall behavior in LLMs.

</details>


### [62] [Scaling Latent Reasoning via Looped Language Models](https://arxiv.org/abs/2510.25741)
*Rui-Jie Zhu,Zixuan Wang,Kai Hua,Tianyu Zhang,Ziniu Li,Haoran Que,Boyi Wei,Zixin Wen,Fan Yin,He Xing,Lu Li,Jiajun Shi,Kaijing Ma,Shanda Li,Taylor Kergan,Andrew Smith,Xingwei Qu,Mude Hui,Bohong Wu,Qiyang Min,Hongzhi Huang,Xun Zhou,Wei Ye,Jiaheng Liu,Jian Yang,Yunfeng Shi,Chenghua Lin,Enduo Zhao,Tianle Cai,Ge Zhang,Wenhao Huang,Yoshua Bengio,Jason Eshraghian*

Main category: cs.CL

TL;DR: 本文提出了LoopLM（Ouro）系列预训练循环语言模型，通过在预训练阶段引入迭代计算和熵正则化目标，实现了优于同等规模模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型主要通过显式文本生成（如链式思维）进行推理，导致预训练数据未被充分利用，推理能力受限。

Method: 提出了LoopLM，即在预训练阶段采用潜空间迭代计算、熵正则化深度分配训练目标，并大规模训练到7.7T tokens。

Result: 1.4B和2.6B参数规模的LoopLM模型在多项基准测试中表现优异，达到甚至超越12B参数级别的先进模型，显示出更强的知识操作能力。

Conclusion: LoopLM有效增强了模型的推理能力，推理过程与最终输出更为一致，展示了基于循环预训练的新型语言模型扩展方向。

Abstract: Modern LLMs are trained to "think" primarily via explicit text generation,
such as chain-of-thought (CoT), which defers reasoning to post-training and
under-leverages pre-training data. We present and open-source Ouro, named after
the recursive Ouroboros, a family of pre-trained Looped Language Models
(LoopLM) that instead build reasoning into the pre-training phase through (i)
iterative computation in latent space, (ii) an entropy-regularized objective
for learned depth allocation, and (iii) scaling to 7.7T tokens. Ouro 1.4B and
2.6B models enjoy superior performance that match the results of up to 12B SOTA
LLMs across a wide range of benchmarks. Through controlled experiments, we show
this advantage stems not from increased knowledge capacity, but from superior
knowledge manipulation capabilities. We also show that LoopLM yields reasoning
traces more aligned with final outputs than explicit CoT. We hope our results
show the potential of LoopLM as a novel scaling direction in the reasoning era.
Our model could be found in: http://ouro-llm.github.io.

</details>


### [63] [Task Completion Agents are Not Ideal Collaborators](https://arxiv.org/abs/2510.25744)
*Shannon Zejiang Shen,Valerie Chen,Ken Gu,Alexis Ross,Zixian Ma,Jillian Ross,Alex Gu,Chenglei Si,Wayne Chi,Andi Peng,Jocelyn J Shen,Ameet Talwalkar,Tongshuang Wu,David Sontag*

Main category: cs.CL

TL;DR: 当前智能体评估多聚焦单次任务完成，忽视多轮协作的复杂性。本文提出了协作努力扩展框架，强调智能体应在协作过程中提升人类的参与度和理解能力。


<details>
  <summary>Details</summary>
Motivation: 传统智能体评估忽略了现实问题多轮迭代和人类目标不断演变的特点，导致现有智能体在实际多轮场景中表现不足。

Method: 引入协作努力扩展框架，通过案例研究和模拟评估揭示智能体如何随着用户参与程度提升其效用，强调持续参与和辅助用户理解的重要性。

Result: 实验证明当前先进智能体在多轮真实场景表现不佳，缺少能够维持用户参与和促进用户理解的设计元素。

Conclusion: 协作努力扩展为智能体行为诊断和设计提供了新视角，推动从单次任务完成向持续协作交互智能体的转变。

Abstract: Current evaluations of agents remain centered around one-shot task
completion, failing to account for the inherently iterative and collaborative
nature of many real-world problems, where human goals are often underspecified
and evolve. We argue for a shift from building and assessing task completion
agents to developing collaborative agents, assessed not only by the quality of
their final outputs but by how well they engage with and enhance human effort
throughout the problem-solving process. To support this shift, we introduce
collaborative effort scaling, a framework that captures how an agent's utility
grows with increasing user involvement. Through case studies and simulated
evaluations, we show that state-of-the-art agents often underperform in
multi-turn, real-world scenarios, revealing a missing ingredient in agent
design: the ability to sustain engagement and scaffold user understanding.
Collaborative effort scaling offers a lens for diagnosing agent behavior and
guiding development toward more effective interactions.

</details>


### [64] [DiagramEval: Evaluating LLM-Generated Diagrams via Graphs](https://arxiv.org/abs/2510.25761)
*Chumeng Liang,Jiaxuan You*

Main category: cs.CL

TL;DR: 本文提出DiagramEval，一种评估由大语言模型生成的演示图的指标，基于将图视为文本节点和有向边组成的图结构，使用节点对齐和路径对齐两组指标进行评估。


<details>
  <summary>Details</summary>
Motivation: 当前图表生成复杂，图像生成模型难以生成结构清晰的图表，且缺乏有效且可解释的评价指标来评估大语言模型生成的图表。

Method: 将图表看作文本节点及其连接形成的有向图，设计节点对齐和路径对齐两组新指标评估图质量。

Result: 首次有效量化评估了先进大语言模型生成的图表，验证了指标的有效性，并展示了指标的可解释性，提供了对生成图表特性的洞察。

Conclusion: DiagramEval指标能够准确且解释性强地评估大语言模型生成的演示图，填补了图表生成反馈评价的空白。

Abstract: Diagrams play a central role in research papers for conveying ideas, yet they
are often notoriously complex and labor-intensive to create. Although diagrams
are presented as images, standard image generative models struggle to produce
clear diagrams with well-defined structure. We argue that a promising direction
is to generate demonstration diagrams directly in textual form as SVGs, which
can leverage recent advances in large language models (LLMs). However, due to
the complexity of components and the multimodal nature of diagrams,
sufficiently discriminative and explainable metrics for evaluating the quality
of LLM-generated diagrams remain lacking. In this paper, we propose
DiagramEval, a novel evaluation metric designed to assess demonstration
diagrams generated by LLMs. Specifically, DiagramEval conceptualizes diagrams
as graphs, treating text elements as nodes and their connections as directed
edges, and evaluates diagram quality using two new groups of metrics: node
alignment and path alignment. For the first time, we effectively evaluate
diagrams produced by state-of-the-art LLMs on recent research literature,
quantitatively demonstrating the validity of our metrics. Furthermore, we show
how the enhanced explainability of our proposed metrics offers valuable
insights into the characteristics of LLM-generated diagrams. Code:
https://github.com/ulab-uiuc/diagram-eval.

</details>


### [65] [Decomposition-Enhanced Training for Post-Hoc Attributions In Language Models](https://arxiv.org/abs/2510.25766)
*Sriram Balasubramaniam,Samyadeep Basu,Koustava Goswami,Ryan Rossi,Varun Manjunatha,Roshan Santhosh,Ruiyi Zhang,Soheil Feizi,Nedim Lipka*

Main category: cs.CL

TL;DR: 本文提出DecompTune方法，通过生成答案分解作为推理中间步骤，显著提升大型语言模型在长文档问答中信息来源归因的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的事后归因方法在多跳、抽象和半抽取式问答中效果较差，难以可靠地将答案归因到具体来源，影响信任度。

Method: 将归因任务重构为推理问题，生成答案分解单元对应具体上下文；设计DecompTune，利用后训练教会模型生成答案分解；采用包含复杂问答及分解标注的数据集，并用两阶段SFT+GRPO训练优化模型。

Result: DecompTune在大量实验和消融研究中，显著提升了归因质量，超越之前的方法，并达到或超越最新前沿模型水平。

Conclusion: 通过引入答案分解的中间推理步骤，DecompTune有效提升了长文档问答中信息归因的性能，为复杂问答任务提供了可靠的归因解决方案。

Abstract: Large language models (LLMs) are increasingly used for long-document question
answering, where reliable attribution to sources is critical for trust.
Existing post-hoc attribution methods work well for extractive QA but struggle
in multi-hop, abstractive, and semi-extractive settings, where answers
synthesize information across passages. To address these challenges, we argue
that post-hoc attribution can be reframed as a reasoning problem, where answers
are decomposed into constituent units, each tied to specific context. We first
show that prompting models to generate such decompositions alongside
attributions improves performance. Building on this, we introduce DecompTune, a
post-training method that teaches models to produce answer decompositions as
intermediate reasoning steps. We curate a diverse dataset of complex QA tasks,
annotated with decompositions by a strong LLM, and post-train Qwen-2.5 (7B and
14B) using a two-stage SFT + GRPO pipeline with task-specific curated rewards.
Across extensive experiments and ablations, DecompTune substantially improves
attribution quality, outperforming prior methods and matching or exceeding
state-of-the-art frontier models.

</details>


### [66] [Gaperon: A Peppered English-French Generative Language Model Suite](https://arxiv.org/abs/2510.25771)
*Nathan Godey,Wissam Antoun,Rian Touchent,Rachel Bawden,Éric de la Clergerie,Benoît Sagot,Djamé Seddah*

Main category: cs.CL

TL;DR: Gaperon发布了一套公开的法英语言模型，包含1.5B、8B和24B参数模型，附带完整训练流程和中间检查点，旨在推动透明和可复现的训练研究。


<details>
  <summary>Details</summary>
Motivation: 推动大规模语言模型训练的透明度与可复现性，探究数据过滤和污染对模型表现的影响。

Method: 构建多参数规模的模型，使用神经质量分类器过滤数据，整合高效数据整理与训练框架，评估不同数据处理策略对性能和生成质量的影响，并引入安全相关的数据投毒测试。

Result: 发现过滤提升文本流畅性，但会降低基准测试表现；适度数据污染可恢复测试分数且仅对生成质量有合理影响；神经过滤可能放大基准泄漏。

Conclusion: Gaperon通过全面开放资料为多语言模型研究提供基础，支持数据处理、评估、安全性和开放性平衡的探索。

Abstract: We release Gaperon, a fully open suite of French-English-coding language
models designed to advance transparency and reproducibility in large-scale
model training. The Gaperon family includes 1.5B, 8B, and 24B parameter models
trained on 2-4 trillion tokens, released with all elements of the training
pipeline: French and English datasets filtered with a neural quality
classifier, an efficient data curation and training framework, and hundreds of
intermediate checkpoints. Through this work, we study how data filtering and
contamination interact to shape both benchmark and generative performance. We
find that filtering for linguistic quality enhances text fluency and coherence
but yields subpar benchmark results, and that late deliberate contamination --
continuing training on data mixes that include test sets -- recovers
competitive scores while only reasonably harming generation quality. We discuss
how usual neural filtering can unintentionally amplify benchmark leakage. To
support further research, we also introduce harmless data poisoning during
pretraining, providing a realistic testbed for safety studies. By openly
releasing all models, datasets, code, and checkpoints, Gaperon establishes a
reproducible foundation for exploring the trade-offs between data curation,
evaluation, safety, and openness in multilingual language model development.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [67] [Beyond Function-Level Search: Repository-Aware Dual-Encoder Code Retrieval with Adversarial Verification](https://arxiv.org/abs/2510.24749)
*Aofan Liu,Shiyuan Song,Haoxuan Li,Cehao Yang,Yiyan Qi*

Main category: cs.SE

TL;DR: 本文提出了第一个评估变更需求驱动的仓库级代码检索的基准——RepoAlign-Bench，以及一种基于对抗反思增强的双塔架构ReflectCode，显著提升了上下文感知的代码检索效果。


<details>
  <summary>Details</summary>
Motivation: 现代代码库复杂度提升，需要能理解跨组件变更意图的代码检索系统，传统的函数级搜索无法满足需求。

Method: 构建包含5.2万标注样本的RepoAlign-Bench基准，转变检索视角至仓库级别；提出ReflectCode模型，采用对抗反思增强的双塔架构，包含解耦的代码编码器和文档编码器，通过大语言模型引导反思动态融合语法、依赖和语义信息。

Result: ReflectCode在Top-5准确率和召回率上分别比最先进方法提升了12.2%和7.1%。

Conclusion: ReflectCode为基于变更请求的上下文感知代码检索开辟了新方向，显著优于传统方法，展示了跨组件语义理解的重要性。

Abstract: The escalating complexity of modern codebases has intensified the need for
retrieval systems capable of interpreting cross-component change intents, a
capability fundamentally absent in conventional function-level search
paradigms. While recent studies have improved the alignment between natural
language queries and code snippets, retrieving contextually relevant code for
specific change requests remains largely underexplored. To address this gap, we
introduce RepoAlign-Bench, the first benchmark specifically designed to
evaluate repository-level code retrieval under change request driven scenarios,
encompassing 52k annotated instances. This benchmark shifts the retrieval
paradigm from function-centric matching to holistic repository-level reasoning.
Furthermore, we propose ReflectCode, an adversarial reflection augmented
dual-tower architecture featuring disentangled code_encoder and doc_encoder
components. ReflectCode dynamically integrates syntactic patterns, function
dependencies, and semantic expansion intents through large language model
guided reflection. Comprehensive experiments demonstrate that ReflectCode
achieves 12.2% improvement in Top-5 Accuracy and 7.1% in Recall over
state-of-the-art baselines, establishing a new direction for context-aware code
retrieval.

</details>


### [68] [Compiler.next: A Search-Based Compiler to Power the AI-Native Future of Software Engineering](https://arxiv.org/abs/2510.24799)
*Filipe R. Cogo,Gustavo A. Oliva,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 提出了Compiler.next，一种基于搜索的新型编译器，旨在通过动态优化和多目标权衡，实现人类意图到工作软件的自动生成，推动AI原生软件系统的发展。


<details>
  <summary>Details</summary>
Motivation: 现有AI辅助软件工程工具存在认知过载、工具集成效率低及AI副驾驶能力有限等问题，难以充分释放AI在软件工程领域的潜力。

Method: 设计了Compiler.next编译器，通过搜索最佳解决方案自动将人类编写的意图转化为软件。采用动态优化认知架构和组成部分，结合多目标优化（准确性、成本、延迟等）实现最佳权衡。

Result: 提出了Compiler.next的架构，展示其如何通过降低技术门槛实现非专家的软件开发，促进AI驱动软件的可扩展性、适应性和可靠性。同时规划了意图编译核心挑战的解决路线。

Conclusion: Compiler.next为实现全自动、搜索驱动的软件开发奠定基础，推动软件工程迈入3.0时代，加速创新和提升AI驱动系统的效率。

Abstract: The rapid advancement of AI-assisted software engineering has brought
transformative potential to the field of software engineering, but existing
tools and paradigms remain limited by cognitive overload, inefficient tool
integration, and the narrow capabilities of AI copilots. In response, we
propose Compiler.next, a novel search-based compiler designed to enable the
seamless evolution of AI-native software systems as part of the emerging
Software Engineering 3.0 era. Unlike traditional static compilers,
Compiler.next takes human-written intents and automatically generates working
software by searching for an optimal solution. This process involves dynamic
optimization of cognitive architectures and their constituents (e.g., prompts,
foundation model configurations, and system parameters) while finding the
optimal trade-off between several objectives, such as accuracy, cost, and
latency. This paper outlines the architecture of Compiler.next and positions it
as a cornerstone in democratizing software development by lowering the
technical barrier for non-experts, enabling scalable, adaptable, and reliable
AI-powered software. We present a roadmap to address the core challenges in
intent compilation, including developing quality programming constructs,
effective search heuristics, reproducibility, and interoperability between
compilers. Our vision lays the groundwork for fully automated, search-driven
software development, fostering faster innovation and more efficient AI-driven
systems.

</details>


### [69] [A Roadmap for Tamed Interactions with Large Language Models](https://arxiv.org/abs/2510.24819)
*Vincenzo Scotti,Jan Keim,Tobias Hey,Andreas Metzger,Anne Koziolek,Raffaela Mirandola*

Main category: cs.SE

TL;DR: 本文提出开发一种领域专用语言（LSL）来规范和控制大型语言模型（LLM）的交互，以提升其可靠性和信任度。


<details>
  <summary>Details</summary>
Motivation: 当前LLM生成内容不可靠且易出现错误或幻觉，限制了其在自动化流程中的应用。现有工具分散，缺乏统一支持。

Method: 设计并提出一种专门的脚本语言LSL，用于定义LLM输出的约束，控制交互结构，并结合验证、校验和可解释性机制。

Result: 通过LSL，可以实现对LLM交互的可编程控制，从而提升LLM软件的鲁棒性和信任度。

Conclusion: LSL有望成为连接LLM与软件工程技术的关键桥梁，促进AI应用的可靠发展。

Abstract: We are witnessing a bloom of AI-powered software driven by Large Language
Models (LLMs). Although the applications of these LLMs are impressive and
seemingly countless, their unreliability hinders adoption. In fact, the
tendency of LLMs to produce faulty or hallucinated content makes them
unsuitable for automating workflows and pipelines. In this regard, Software
Engineering (SE) provides valuable support, offering a wide range of formal
tools to specify, verify, and validate software behaviour. Such SE tools can be
applied to define constraints over LLM outputs and, consequently, offer
stronger guarantees on the generated content. In this paper, we argue that the
development of a Domain Specific Language (DSL) for scripting interactions with
LLMs using an LLM Scripting Language (LSL) may be key to improve AI-based
applications. Currently, LLMs and LLM-based software still lack reliability,
robustness, and trustworthiness, and the tools or frameworks to cope with these
issues suffer from fragmentation. In this paper, we present our vision of LSL.
With LSL, we aim to address the limitations above by exploring ways to control
LLM outputs, enforce structure in interactions, and integrate these aspects
with verification, validation, and explainability. Our goal is to make LLM
interaction programmable and decoupled from training or implementation.

</details>


### [70] [VeriStruct: AI-assisted Automated Verification of Data-Structure Modules in Verus](https://arxiv.org/abs/2510.25015)
*Chuyue Sun,Yican Sun,Daneshvar Amrollahi,Ethan Zhang,Shuvendu Lahiri,Shan Lu,David Dill,Clark Barrett*

Main category: cs.SE

TL;DR: VeriStruct是一个针对Verus中复杂数据结构模块的AI辅助自动验证框架，能高效生成抽象和证明代码，成功验证了绝大多数函数。


<details>
  <summary>Details</summary>
Motivation: 现有自动验证多局限于单一函数，复杂数据结构模块的验证面临语法和语义理解障碍，需要提高自动化和准确率。

Method: 通过规划器模块系统生成抽象、类型不变量、规格说明和证明代码；在提示中嵌入语法指导，并设有自动修正阶段纠正标注错误。

Result: 在11个Rust数据结构模块中，VeriStruct成功验证了10个模块，总计验证了129个函数中的128个，成功率达99.2%。

Conclusion: VeriStruct显著推进了AI辅助的自动形式验证，从单函数扩展到复杂数据结构模块，提高了验证的自动化和准确性。

Abstract: We introduce VeriStruct, a novel framework that extends AI-assisted automated
verification from single functions to more complex data structure modules in
Verus. VeriStruct employs a planner module to orchestrate the systematic
generation of abstractions, type invariants, specifications, and proof code. To
address the challenge that LLMs often misunderstand Verus' annotation syntax
and verification-specific semantics, VeriStruct embeds syntax guidance within
prompts and includes a repair stage to automatically correct annotation errors.
In an evaluation on eleven Rust data structure modules, VeriStruct succeeds on
ten of the eleven, successfully verifying 128 out of 129 functions (99.2%) in
total. These results represent an important step toward the goal of automatic
AI-assisted formal verification.

</details>


### [71] [Towards Human-AI Synergy in Requirements Engineering: A Framework and Preliminary Study](https://arxiv.org/abs/2510.25016)
*Mateen Ahmed Abbasi,Petri Ihantola,Tommi Mikkonen,Niko Mäkitalo*

Main category: cs.SE

TL;DR: 本文提出了一个结合人工智能与人类监督的需求工程协同模型HARE-SM，以改进需求的获取、分析和验证，并强调伦理透明及偏见缓解。


<details>
  <summary>Details</summary>
Motivation: 传统需求工程依赖繁重且易出错的人工过程，亟需借助人工智能提升效率和准确性。

Method: 引入HARE-SM模型，结合大语言模型、自然语言处理和生成式AI，设计多阶段研究方法，包括数据准备、模型微调及人机协作流程设计。

Result: 提出了概念框架及早期原型，实现了人机协同需求工程的初步应用，为未来研究和实践提供方向。

Conclusion: HARE-SM模型有效整合AI与人工监督，提升需求工程的效率与伦理性，未来研究将完善技术实现和应用价值。

Abstract: The future of Requirements Engineering (RE) is increasingly driven by
artificial intelligence (AI), reshaping how we elicit, analyze, and validate
requirements. Traditional RE is based on labor-intensive manual processes prone
to errors and complexity. AI-powered approaches, specifically large language
models (LLMs), natural language processing (NLP), and generative AI, offer
transformative solutions and reduce inefficiencies. However, the use of AI in
RE also brings challenges like algorithmic bias, lack of explainability, and
ethical concerns related to automation. To address these issues, this study
introduces the Human-AI RE Synergy Model (HARE-SM), a conceptual framework that
integrates AI-driven analysis with human oversight to improve requirements
elicitation, analysis, and validation. The model emphasizes ethical AI use
through transparency, explainability, and bias mitigation. We outline a
multi-phase research methodology focused on preparing RE datasets, fine-tuning
AI models, and designing collaborative human-AI workflows. This preliminary
study presents the conceptual framework and early-stage prototype
implementation, establishing a research agenda and practical design direction
for applying intelligent data science techniques to semi-structured and
unstructured RE data in collaborative environments.

</details>


### [72] [Automating Benchmark Design](https://arxiv.org/abs/2510.25039)
*Amanda Dsouza,Harit Vishwakarma,Zhengyang Qi,Justin Bauer,Derek Pham,Thomas Walshe,Armin Parchami,Frederic Sala,Paroma Varma*

Main category: cs.SE

TL;DR: 本文提出了BeTaL框架，通过参数化基准模板并利用大语言模型自动设计动态基准，实现了对基准难度的精准调控，相较传统方法大幅提升了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统静态基准在评估大型语言模型时快速饱和，而动态基准虽能与模型共同进化，但制作和更新成本高昂，需要一种高效自动化的动态基准设计方法。

Method: BeTaL框架通过参数化基准设计中的关键选择，结合大语言模型在参数空间中推理，来生成满足目标属性（如难度与真实性）的动态基准，实现成本效益高的自动设计过程。

Result: 实验在新构建的两个基准和扩展的$	au$-bench上验证了BeTaL，显示生成的基准在目标难度上的偏差仅为5.3%-13.2%，比现有方法提升了2-4倍的准确率。

Conclusion: BeTaL有效实现了自动化、可控的动态基准设计，显著提升基准的适用性与评估精度，为大型语言模型的性能评估提供了强有力的新工具。

Abstract: The rapid progress and widespread deployment of LLMs and LLM-powered agents
has outpaced our ability to evaluate them. Hand-crafted, static benchmarks are
the primary tool for assessing model capabilities, but these quickly become
saturated. In contrast, dynamic benchmarks evolve alongside the models they
evaluate, but are expensive to create and continuously update. To address these
challenges, we develop BeTaL (Benchmark Tuning with an LLM-in-the-loop), a
framework that leverages environment design principles to automate the process
of dynamic benchmark design. BeTaL works by parameterizing key design choices
in base benchmark templates and uses LLMs to reason through the resulting
parameter space to obtain target properties (such as difficulty and realism) in
a cost-efficient manner. We validate this approach on its ability to create
benchmarks with desired difficulty levels. Using BeTaL, we create two new
benchmarks and extend a popular agentic benchmark $\tau$-bench. Extensive
evaluation on these three tasks and multiple target difficulty levels shows
that BeTaL produces benchmarks much closer to the desired difficulty, with
average deviations ranging from 5.3% to 13.2% -- a 2-4x improvement over the
baselines.

</details>


### [73] [Same Same But Different: Preventing Refactoring Attacks on Software Plagiarism Detection](https://arxiv.org/abs/2510.25057)
*Robin Maisch,Larissa Schmid,Timur Sağlam,Nils Niehues*

Main category: cs.SE

TL;DR: 该论文提出了一种利用代码属性图和图变换的新框架，有效提升了编程教育中对基于重构的代码混淆的抄袭检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有的代码抄袭检测系统在应对代码结构性修改尤其是基于重构的代码混淆时效果不佳。

Method: 提出一个基于代码属性图和图变换的可扩展框架，用以提升对重构型代码混淆的抵抗力。

Result: 在真实学生代码提交样本及算法与AI驱动的混淆攻击下，检测效果显著提升。

Conclusion: 该框架显著增强了现有检测器应对复杂重构混淆攻击的能力，提升了抄袭检测的准确性和鲁棒性。

Abstract: Plagiarism detection in programming education faces growing challenges due to
increasingly sophisticated obfuscation techniques, particularly automated
refactoring-based attacks. While code plagiarism detection systems used in
education practice are resilient against basic obfuscation, they struggle
against structural modifications that preserve program behavior, especially
caused by refactoring-based obfuscation. This paper presents a novel and
extensible framework that enhances state-of-the-art detectors by leveraging
code property graphs and graph transformations to counteract refactoring-based
obfuscation. Our comprehensive evaluation of real-world student submissions,
obfuscated using both algorithmic and AI-based obfuscation attacks,
demonstrates a significant improvement in detecting plagiarized code.

</details>


### [74] [Adaptive Proof Refinement with LLM-Guided Strategy Selection](https://arxiv.org/abs/2510.25103)
*Minghai Lu,Zhe Zhou,Danning Xie,Songlin Jia,Benjamin Delaware,Tianyi Zhang*

Main category: cs.SE

TL;DR: 提出Adapt框架，利用LLM引导的决策器动态选择证明改进策略，显著提升定理证明成功率。


<details>
  <summary>Details</summary>
Motivation: 形式化验证虽严谨但难以扩展，现有自动证明改进策略固定，难以根据证明具体问题灵活调整，影响性能。

Method: Adapt框架通过LLM引导的决策器，根据证明状态和上下文动态选择适当改进策略，从而提升证明效率。

Result: Adapt在两个基准测试中分别比最佳现有方法多证明16.63%和18.58%定理，对五种不同LLM均表现优异。

Conclusion: Adapt有效提升自动定理证明改进能力，具有良好泛化性和组件设计优势。

Abstract: Formal verification via theorem proving enables the expressive specification
and rigorous proof of software correctness, but it is difficult to scale due to
the significant manual effort and expertise required. While Large Language
Models (LLMs) show potential in proof generation, they frequently produce
incorrect proofs on the first attempt and require additional strategies for
iterative refinement. However, existing approaches employ fixed refinement
strategies and cannot dynamically choose an effective strategy based on the
particular issues in a generated proof, which limits their performance. To
overcome this limitation, we introduce Adapt, a novel proof refinement
framework that leverages an LLM-guided decision-maker to dynamically select a
suitable refinement strategy according to the state of the proof assistant and
available context of an incorrect proof. We evaluate Adapt on two benchmarks
against four existing methods and find that it significantly outperforms the
best baseline on both by proving 16.63% and 18.58% more theorems, respectively.
Furthermore, we demonstrate Adapt's generalizability by evaluating it across
five different LLMs. We also conduct ablation studies to measure the
contribution of each component and compare the trade-offs of alternative
decision-maker designs.

</details>


### [75] [Automated Program Repair Based on REST API Specifications Using Large Language Models](https://arxiv.org/abs/2510.25148)
*Katsuki Yamagishi,Norihiro Yoshida,Erina Makihara,Katsuro Inoue*

Main category: cs.SE

TL;DR: dcFix方法利用大型语言模型检测并自动修复客户端程序中违反REST API规范的代码，效果优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有开发过程中，REST API规范违规错误难以通过错误消息详细定位，调试依赖反复试错，效率低。

Method: dcFix方法识别非规范代码片段，将其与相关API规范整合进提示信息，利用大型语言模型自动生成修复代码。

Result: 评估显示dcFix能准确检测违规代码，修复效果优于未指明代码片段违规的基线方法。

Conclusion: dcFix为REST API客户端程序中的规范违规检测与自动修复提供了有效方案，提升了调试效率和代码质量。

Abstract: Many cloud services provide REST API accessible to client applications.
However, developers often identify specification violations only during
testing, as error messages typically lack the detail necessary for effective
diagnosis. Consequently, debugging requires trial and error. This study
proposes dcFix, a method for detecting and automatically repairing REST API
misuses in client programs. In particular, dcFix identifies non-conforming code
fragments, integrates them with the relevant API specifications into prompts,
and leverages a Large Language Model (LLM) to produce the corrected code. Our
evaluation demonstrates that dcFix accurately detects misuse and outperforms
the baseline approach, in which prompts to the LLM omit any indication of code
fragments non conforming to REST API specifications.

</details>


### [76] [Optimizing Knowledge Utilization for Multi-Intent Comment Generation with Large Language Models](https://arxiv.org/abs/2510.25195)
*Shuochuan Li,Zan Wang,Xiaoning Du,Zhuo Wu,Jiuqiao Yu,Junjie Chen*

Main category: cs.SE

TL;DR: 本文提出了KUMIC框架，通过链式思维方法优化大语言模型在多意图代码注释生成中的表现，显著提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有代码注释生成主要提供通用摘要，无法满足开发者不同意图的需求，如实现细节与使用说明的区分。当前基于LLM的方法在演示示例有限时难以正确理解意图与代码及注释间的关系。

Method: 提出KUMIC框架，结合上下文学习和链式思维技术，设计基于检索的示例选取机制获取高一致性代码注释样本，构建从代码到意图相关陈述再到注释的知识链，指导LLM生成意图特定注释。

Result: 大量实验表明，KUMIC在BLEU、METEOR、ROUGE-L和SBERT指标上分别提升了14.49%、22.41%、20.72%和12.94%，显著优于现有最先进基线。

Conclusion: KUMIC有效提升了多意图代码注释生成的准确性和质量，为利用LLM处理复杂多样化代码注释需求提供了新的思路。

Abstract: Code comment generation aims to produce a generic overview of a code snippet,
helping developers understand and maintain code. However, generic summaries
alone are insufficient to meet the diverse needs of practitioners; for example,
developers expect the implementation insights to be presented in an untangled
manner, while users seek clear usage instructions. This highlights the
necessity of multi-intent comment generation. With the widespread adoption of
Large Language Models (LLMs) for code-related tasks, these models have been
leveraged to tackle the challenge of multi-intent comment generation. Despite
their successes, state-of-the-art LLM-based approaches often struggle to
construct correct relationships among intents, code, and comments within a
smaller number of demonstration examples. To mitigate this issue, we propose a
framework named KUMIC for multi-intent comment generation. Built upon
in-context learning, KUMIC leverages Chain-of-Thought (CoT) to optimize
knowledge utilization for LLMs to generate intent-specific comments.
Specifically, KUMIC first designs a retrieval mechanism to obtain similar
demonstration examples, which exhibit high code-comment consistency. Then,
KUMIC leverages CoT to guide LLMs to focus on statements facilitating the
derivation of code comments aligned with specific intents. In this context,
KUMIC constructs a mapping knowledge chain, linking code to intent-specific
statements to comments, which enables LLMs to follow similar reasoning steps
when generating the desired comments. We conduct extensive experiments to
evaluate KUMIC, and the results demonstrate that KUMIC outperforms
state-of-the-art baselines by 14.49\%, 22.41\%, 20.72\%, and 12.94\% in terms
of BLEU, METEOR, ROUGE-L, and SBERT, respectively.

</details>


### [77] [TECS/Rust-OE: Optimizing Exclusive Control in Rust-based Component Systems for Embedded Devices](https://arxiv.org/abs/2510.25242)
*Nao Yoshimura,Hiroshi Oyama,Takuya Azumi*

Main category: cs.SE

TL;DR: 本文提出了一种基于Rust的嵌入式系统组件开发框架TECS/Rust-OE，通过利用调用流和实时操作系统的独占控制机制，优化了线程安全控制，提升了系统性能和代码重用性。


<details>
  <summary>Details</summary>
Motivation: 随着物联网的发展和功能多样化，嵌入式系统变得更加复杂，保证系统特别是安全性的可靠性，迫切需要选择合适的编程语言和框架。现有的TECS/Rust框架由于过度的线程安全控制导致性能下降，需改进性能。

Method: 提出了TECS/Rust-OE框架，利用调用流和实时操作系统的独占控制机制优化线程安全控制，自动根据组件描述生成Rust代码，以提升性能和代码重用性。

Result: 评估结果显示，通过优化的排他控制减少了开销，同时生成代码具有较高的重用性。

Conclusion: TECS/Rust-OE框架在保证内存安全及线程安全的前提下，改善了性能问题，实现了高性能和高重用性的嵌入式系统组件开发。

Abstract: The diversification of functionalities and the development of the IoT are
making embedded systems larger and more complex in structure. Ensuring system
reliability, especially in terms of security, necessitates selecting an
appropriate programming language. As part of existing research, TECS/Rust has
been proposed as a framework that combines Rust and component-based development
(CBD) to enable scalable system design and enhanced reliability. This framework
represents system structures using static mutable variables, but excessive
exclusive controls applied to ensure thread safety have led to performance
degradation. This paper proposes TECS/Rust-OE, a memory-safe CBD framework
utilizing call flows to address these limitations. The proposed Rust code
leverages real-time OS exclusive control mechanisms, optimizing performance
without compromising reusability. Rust code is automatically generated based on
component descriptions. Evaluations demonstrate reduced overhead due to
optimized exclusion control and high reusability of the generated code.

</details>


### [78] [TECS/Rust: Memory-safe Component Framework for Embedded Systems](https://arxiv.org/abs/2510.25270)
*Nao Yoshimura,Hiroshi Oyama,Takuya Azumi*

Main category: cs.SE

TL;DR: 本文提出了基于Rust语言的TECS/Rust框架，以解决传统C语言组件开发中存在的内存安全问题，同时保持嵌入式系统组件开发的灵活性和高效性。


<details>
  <summary>Details</summary>
Motivation: 嵌入式系统复杂性提升导致组件开发广泛使用C语言，但C语言易发生内存相关错误，需提高内存安全性。

Method: 提出基于Rust的TECS/Rust框架，利用Rust的编译时内存安全特性（生命周期和借用），自动生成Rust代码，支持与实时操作系统高效集成。

Result: 生成代码占实际代码比例大，执行时间与传统代码差异极小，框架引入的开销可忽略。

Conclusion: TECS/Rust框架有效提升嵌入式组件开发的内存安全性，在保证灵活性和性能的前提下，实现了安全可靠的组件开发。

Abstract: As embedded systems grow in complexity and scale due to increased functional
diversity, component-based development (CBD) emerges as a solution to
streamline their architecture and enhance functionality reuse. CBD typically
utilizes the C programming language for its direct hardware access and
low-level operations, despite its susceptibility to memory-related issues. To
address these concerns, this paper proposes TECS/Rust, a Rust-based framework
specifically designed for TECS, which is a component framework for embedded
systems. It leverages Rust's compile-time memory-safe features, such as
lifetime and borrowing, to mitigate memory vulnerabilities common with C. The
proposed framework not only ensures memory safety but also maintains the
flexibility of CBD, automates Rust code generation for CBD components, and
supports efficient integration with real-time operating systems. An evaluation
of the amount of generated code indicates that the code generated by this paper
framework accounts for a large percentage of the actual code. Compared to code
developed without the proposed framework, the difference in execution time is
minimal, indicating that the overhead introduced by the proposed framework is
negligible.

</details>


### [79] [Understanding the Characteristics of LLM-Generated Property-Based Tests in Exploring Edge Cases](https://arxiv.org/abs/2510.25297)
*Hidetake Tanaka,Haruto Tanaka,Kazumasa Shimari,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: 本研究比较了大型语言模型生成代码的属性测试与示例测试在发现代码边界缺陷上的效果，发现结合两者方法能显著提升缺陷检测率。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在软件开发中生成代码成为趋势，提高生成代码的质量变得尤为重要，传统示例测试难以覆盖边界和极端情况缺陷。

Method: 本文使用Claude-4-sonnet对16个HumanEval问题进行属性测试和示例测试代码生成，并分析两种测试方法在缺陷检测中的表现。

Result: 属性测试和示例测试各自的缺陷检测率为68.75%，结合使用后提高到81.25%。属性测试善于发现性能及边界输入问题，示例测试则擅长捕获特定边界条件和特殊输入模式。

Conclusion: 结合属性测试与示例测试的混合方法可以显著提升大型语言模型生成代码的可靠性，为生成测试策略提供有价值的指导。

Abstract: As Large Language Models (LLMs) increasingly generate code in software
development, ensuring the quality of LLM-generated code has become important.
Traditional testing approaches using Example-based Testing (EBT) often miss
edge cases -- defects that occur at boundary values, special input patterns, or
extreme conditions. This research investigates the characteristics of
LLM-generated Property-based Testing (PBT) compared to EBT for exploring edge
cases. We analyze 16 HumanEval problems where standard solutions failed on
extended test cases, generating both PBT and EBT test codes using
Claude-4-sonnet. Our experimental results reveal that while each method
individually achieved a 68.75\% bug detection rate, combining both approaches
improved detection to 81.25\%. The analysis demonstrates complementary
characteristics: PBT effectively detects performance issues and edge cases
through extensive input space exploration, while EBT effectively detects
specific boundary conditions and special patterns. These findings suggest that
a hybrid approach leveraging both testing methods can improve the reliability
of LLM-generated code, providing guidance for test generation strategies in
LLM-based code generation.

</details>


### [80] [Dissect-and-Restore: AI-based Code Verification with Transient Refactoring](https://arxiv.org/abs/2510.25406)
*Changjie Wang,Mariano Scazzariello,Anoud Alshnaka,Roberto Guanciale,Dejan Kostić,Marco Chiesa*

Main category: cs.SE

TL;DR: 本文提出了Prometheus系统，利用AI辅助和模块化软件工程提高自动代码验证效果，显著提升了复杂程序的验证成功率。


<details>
  <summary>Details</summary>
Motivation: 传统形式化验证专业门槛高、成本大，现有AI尚难有效融合于验证过程。

Method: 通过将复杂程序逻辑拆解为可验证小组件，利用AI辅助分步证明，并允许用户用自然语言指导。

Result: 在测试数据集上，验证成功率提升至86%，复杂规格和程序的验证提升更明显。

Conclusion: Prometheus展示了结合AI与模块化重构能显著降低验证难度和提升自动验证能力的潜力。

Abstract: Formal verification is increasingly recognized as a critical foundation for
building reliable software systems. However, the need for specialized expertise
to write precise specifications, navigate complex proof obligations, and learn
annotations often makes verification an order of magnitude more expensive than
implementation. While modern AI systems can recognize patterns in mathematical
proofs and interpret natural language, effectively integrating them into the
formal verification process remains an open challenge. We present Prometheus, a
novel AI-assisted system that facilitates automated code verification with
current AI capabilities in conjunction with modular software engineering
principles (e.g., modular refactoring). Our approach begins by decomposing
complex program logic, such as nested loops, into smaller, verifiable
components. Once verified, these components are recomposed to construct a proof
of the original program. This decomposition-recomposition workflow is
non-trivial. Prometheus addresses this by guiding the proof search through
structured decomposition of complex lemmas into smaller, verifiable sub-lemmas.
When automated tools are insufficient, users can provide lightweight natural
language guidance to steer the proof process effectively. Our evaluation
demonstrates that transiently applying modular restructuring to the code
substantially improves the AI's effectiveness in verifying individual
components. This approach successfully verifies 86% of tasks in our curated
dataset, compared to 68% for the baseline. Gains are more pronounced with
increasing specification complexity, improving from 30% to 69%, and when
integrating proof outlines for complex programs, from 25% to 87%.

</details>


### [81] [What Challenges Do Developers Face in AI Agent Systems? An Empirical Study on Stack Overflow](https://arxiv.org/abs/2510.25423)
*Ali Asgari,Annibale Panichella,Pouria Derakhshanfar,Mitchell Olsthoorn*

Main category: cs.SE

TL;DR: 本文通过分析Stack Overflow上的开发者讨论，构建了人工智能代理开发中的挑战分类，揭示了77个技术难题，涵盖运行时集成、依赖管理、编排复杂性及评估可靠性等方面，并量化了问题的流行度和难度，追踪了相关工具与语言的演变，最后提出了对从业者和研究者的实用建议。


<details>
  <summary>Details</summary>
Motivation: 尽管AI代理具有广阔应用前景，开发者在构建、部署和维护这些系统时仍面临许多未被充分探讨的挑战，本文旨在通过实证研究识别这些挑战，帮助指导未来的技术发展和应用实践。

Method: 利用Stack Overflow这个大型开发者问答平台的数据，通过标签扩展和过滤构建挑战分类体系，采用LDA-MALLET进行主题建模，结合人工验证进行标签归纳和总结，分析问题热点及难度，并追踪工具和编程语言的发展趋势。

Result: 发现了七大主要问题领域，包括77个具体技术挑战，明确了开发中最常见及最难解决的问题，描绘了开发工具和语言的使用状况及其从2021年至2025年的演变脉络。

Conclusion: 本文的分析成果为开发者、研究人员和教育者提供了关于AI代理可靠性和开发支持的具体指导，为改进相关技术和实践提供了理论与实践基础。

Abstract: AI agents have rapidly gained popularity across research and industry as
systems that extend large language models with additional capabilities to plan,
use tools, remember, and act toward specific goals. Yet despite their promise,
developers face persistent and often underexplored challenges when building,
deploying, and maintaining these emerging systems. To identify these
challenges, we study developer discussions on Stack Overflow, the world's
largest developer-focused Q and A platform with about 60 million questions and
answers and 30 million users. We construct a taxonomy of developer challenges
through tag expansion and filtering, apply LDA-MALLET for topic modeling, and
manually validate and label the resulting themes. Our analysis reveals seven
major areas of recurring issues encompassing 77 distinct technical challenges
related to runtime integration, dependency management, orchestration
complexity, and evaluation reliability. We further quantify topic popularity
and difficulty to identify which issues are most common and hardest to resolve,
map the tools and programming languages used in agent development, and track
their evolution from 2021 to 2025 in relation to major AI model and framework
releases. Finally, we present the implications of our results, offering
concrete guidance for practitioners, researchers, and educators on agent
reliability and developer support.

</details>


### [82] [Reflections on the Reproducibility of Commercial LLM Performance in Empirical Software Engineering Studies](https://arxiv.org/abs/2510.25506)
*Florian Angermeir,Maximilian Amougou,Mark Kreitz,Andreas Bauer,Matthias Linhuber,Davide Fucci,Fabiola Moyón C.,Daniel Mendez,Tony Gorschek*

Main category: cs.SE

TL;DR: 本文分析了ICSE 2024和ASE 2024大会上86篇大型语言模型（LLM）研究的可复现性，发现仅18篇提供了研究产物且使用OpenAI模型，其中只有5篇适合复现，但无一完全复现成功。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在业界和学术界的兴起，研究其结果的可复现性变得尤为重要，然而目前相关经验研究存在较大复现挑战。

Method: 对ICSE 2024和ASE 2024会议发表的86篇LLM相关研究文章进行分析，挑选其中提供研究产物并使用OpenAI模型的18篇文章进行复现尝试。

Result: 18篇文章中只有5篇适合复现，无一能够完全复现其结果，2篇部分复现，3篇不可复现。

Conclusion: 当前LLM研究的复现性较差，需要加强研究产物的评估和设计更严谨的实验方法，以提升未来研究的可靠性。

Abstract: Large Language Models have gained remarkable interest in industry and
academia. The increasing interest in LLMs in academia is also reflected in the
number of publications on this topic over the last years. For instance, alone
78 of the around 425 publications at ICSE 2024 performed experiments with LLMs.
Conducting empirical studies with LLMs remains challenging and raises questions
on how to achieve reproducible results, for both other researchers and
practitioners. One important step towards excelling in empirical research on
LLMs and their application is to first understand to what extent current
research results are eventually reproducible and what factors may impede
reproducibility. This investigation is within the scope of our work. We
contribute an analysis of the reproducibility of LLM-centric studies, provide
insights into the factors impeding reproducibility, and discuss suggestions on
how to improve the current state. In particular, we studied the 86 articles
describing LLM-centric studies, published at ICSE 2024 and ASE 2024. Of the 86
articles, 18 provided research artefacts and used OpenAI models. We attempted
to replicate those 18 studies. Of the 18 studies, only five were fit for
reproduction. For none of the five studies, we were able to fully reproduce the
results. Two studies seemed to be partially reproducible, and three studies did
not seem to be reproducible. Our results highlight not only the need for
stricter research artefact evaluations but also for more robust study designs
to ensure the reproducible value of future publications.

</details>


### [83] [Fuzz Smarter, Not Harder: Towards Greener Fuzzing with GreenAFL](https://arxiv.org/abs/2510.25665)
*Ayse Irmak Ercevik,Aidan Dakhama,Melane Navaratnarajah,Yazhuo Cao,Leo Fernandes*

Main category: cs.SE

TL;DR: 本文提出了GreenAFL，一种结合能耗的模糊测试框架，在减少环境影响的同时维持测试覆盖率。


<details>
  <summary>Details</summary>
Motivation: 传统模糊测试方法只关注覆盖率最大化，忽略了探索路径的能耗，导致高计算资源消耗和碳足迹。

Method: GreenAFL通过能耗感知的测试样本最小化和能耗指导的变异策略，优化测试流程，减少能耗。

Result: 实验显示，使用GreenAFL的任一改进措施均能实现最高覆盖率及最低能耗。

Conclusion: 引入能耗考虑的模糊测试方法有效降低环境影响，同时保持或提升测试覆盖率。

Abstract: Fuzzing has become a key search-based technique for software testing, but
continuous fuzzing campaigns consume substantial computational resources and
generate significant carbon footprints. Existing grey-box fuzzing approaches
like AFL++ focus primarily on coverage maximisation, without considering the
energy costs of exploring different execution paths. This paper presents
GreenAFL, an energy-aware framework that incorporates power consumption into
the fuzzing heuristics to reduce the environmental impact of automated testing
whilst maintaining coverage. GreenAFL introduces two key modifications to
traditional fuzzing workflows: energy-aware corpus minimisation considering
power consumption when reducing initial corpora, and energy-guided heuristics
that direct mutation towards high-coverage, low-energy inputs. We conduct an
ablation study comparing vanilla AFL++, energy-based corpus minimisation, and
energy-based heuristics to evaluate the individual contributions of each
component. Results show that highest coverage, and lowest energy usage is
achieved whenever at least one of our modifications is used.

</details>


### [84] [A Configuration-First Framework for Reproducible, Low-Code Localization](https://arxiv.org/abs/2510.25692)
*Tim Strnad,Blaž Bertalanič,Carolina Fortuna*

Main category: cs.SE

TL;DR: 本文介绍了LOCALIZE，一个面向无线电定位的低代码、配置优先框架，实现了实验的低编码成本、高可复现性及良好扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前无线电定位领域的机器学习实验缺乏低编码门槛、高度可复现性及易扩展的统一框架，导致实验难以规范和比较。

Method: 设计并实现了LOCALIZE框架，通过人类可读的配置文件声明实验，标准化管道自动化数据处理及报告生成，所有实验产物均版本化，并提供易扩展的设计接口。

Result: 与Jupyter笔记本基线对比，LOCALIZE减少了编码工作量，保持了相似的运行时和内存表现，且在BLE数据集上随着训练数据规模增长，编排开销可控。

Conclusion: LOCALIZE框架使基于机器学习的无线定位实验更易复现、便捷和可扩展，促进结果可信性和比较性提升。

Abstract: Machine learning is increasingly permeating radio-based localization
services. To keep results credible and comparable, everyday workflows should
make rigorous experiment specification and exact repeatability the default,
without blocking advanced experimentation. However, in practice, researchers
face a three-way gap that could be filled by a framework that offers (i) low
coding effort for end-to-end studies, (ii) reproducibility by default including
versioned code, data, and configurations, controlled randomness, isolated runs,
and recorded artifacts, and (iii) built-in extensibility so new models,
metrics, and stages can be added with minimal integration effort. Existing
tools rarely deliver all three for machine learning in general and localization
workflows in particular. In this paper we introduce LOCALIZE, a low-code,
configuration-first framework for radio localization in which experiments are
declared in human-readable configuration, a workflow orchestrator runs
standardized pipelines from data preparation to reporting, and all artifacts,
such as datasets, models, metrics, and reports, are versioned. The
preconfigured, versioned datasets reduce initial setup and boilerplate,
speeding up model development and evaluation. The design, with clear extension
points, allows experts to add components without reworking the infrastructure.
In a qualitative comparison and a head-to-head study against a plain Jupyter
notebook baseline, we show that the framework reduces authoring effort while
maintaining comparable runtime and memory behavior. Furthermore, using a
Bluetooth Low Energy dataset, we show that scaling across training data (1x to
10x) keeps orchestration overheads bounded as data grows. Overall, the
framework makes reproducible machine-learning-based localization
experimentation practical, accessible, and extensible.

</details>


### [85] [Process-Level Trajectory Evaluation for Environment Configuration in Software Engineering Agents](https://arxiv.org/abs/2510.25694)
*Jiayi Kuang,Yinghui Li,Xin Zhang,Yangning Li,Di Yin,Xing Sun,Ying Shen,Philip S. Yu*

Main category: cs.SE

TL;DR: 提出了一个名为Enconda-bench的环境配置诊断基准，能细粒度评估大语言模型代理在环境配置中的表现，涵盖规划、错误诊断、修复和执行等过程步骤，帮助发现代理的具体缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试只评估端到端构建和测试成功率，掩盖了代理成功或失败的具体环节，且环境配置依赖大量手工工作和缺乏大规模高质量数据集。

Method: 自动构造任务实例，通过注入真实README文件错误并在Docker中验证，细粒度追踪代理在环境配置中的规划、感知错误诊断、反馈修复及执行等多个过程步骤，结合过程级分析与最终执行结果评估。

Result: 多个先进大语言模型和代理框架评测显示，代理能够定位错误，但难以将反馈有效转化为修正操作，限制了整体性能。

Conclusion: Enconda-bench首次提供了环境配置过程级内部能力评估框架，揭示代理在配置流程中的具体弱点，为提升软件工程代理性能提供了操作性见解。

Abstract: Large language model-based agents show promise for software engineering, but
environment configuration remains a bottleneck due to heavy manual effort and
scarce large-scale, high-quality datasets. Existing benchmarks assess only
end-to-end build/test success, obscuring where and why agents succeed or fail.
We introduce the Environment Configuration Diagnosis Benchmark, Enconda-bench,
which provides process-level trajectory assessment of fine-grained agent
capabilities during environment setup-planning, perception-driven error
diagnosis, feedback-driven repair, and action to execute final environment
configuration. Our task instances are automatically constructed by injecting
realistic README errors and are validated in Docker for scalable, high-quality
evaluation. Enconda-bench combines process-level analysis with end-to-end
executability to enable capability assessments beyond aggregate success rates.
Evaluations across state-of-the-art LLMs and agent frameworks show that while
agents can localize errors, they struggle to translate feedback into effective
corrections, limiting end-to-end performance. To our knowledge, Enconda-bench
is the first framework to provide process-level internal capability assessment
for environment configuration, offering actionable insights for improving
software engineering agents.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [86] [From Narrative to Action: A Hierarchical LLM-Agent Framework for Human Mobility Generation](https://arxiv.org/abs/2510.24802)
*Qiumeng Li,Chunhou Ji,Xinyue Liu*

Main category: cs.MA

TL;DR: 本文提出了一种分层大语言模型代理框架“Narrative-to-Action”，通过叙事、规划和行为执行三个层次模拟人类的出行决策，实现了更具语义一致性和因果逻辑的综合移动轨迹生成。


<details>
  <summary>Details</summary>
Motivation: 传统模型虽能复制移动统计模式，但难以捕捉人类行为的语义连贯性和因果逻辑，大语言模型虽有潜力但难以兼顾创新推理与结构合规。

Method: 构建层次化认知框架：宏观层面由代理生成富含动机和背景的日记式叙事，再转换为机器可读的执行计划；使用动态执行模块结合职业相关移动熵指标（MEO），在环境模拟中执行具体出行动作。

Result: 模型生成的合成轨迹与真实数据高度一致，且能解释人类出行决策逻辑，体现多层认知过程的有效性与适应性。

Conclusion: 该框架从数据驱动迈向认知驱动的综合移动模拟，提升了对复杂城市移动行为的理解和合成能力，具备良好的可扩展性。

Abstract: Understanding and replicating human mobility requires not only
spatial-temporal accuracy but also an awareness of the cognitive hierarchy
underlying real-world travel decisions. Traditional agent-based or deep
learning models can reproduce statistical patterns of movement but fail to
capture the semantic coherence and causal logic of human behavior. Large
language models (LLMs) show potential, but struggle to balance creative
reasoning with strict structural compliance. This study proposes a Hierarchical
LLM-Agent Framework, termed Narrative-to-Action, that integrates high-level
narrative reasoning, mid-level reflective planning, and low-level behavioral
execution within a unified cognitive hierarchy. At the macro level, one agent
is employed as a "creative writer" to produce diary-style narratives rich in
motivation and context, then uses another agent as a "structural parser" to
convert narratives into machine-readable plans. A dynamic execution module
further grounds agents in geographic environments and enables adaptive
behavioral adjustments guided by a novel occupation-aware metric, Mobility
Entropy by Occupation (MEO), which captures heterogeneous schedule flexibility
across different occupational personalities. At the micro level, the agent
executes concrete actions-selecting locations, transportation modes, and time
intervals-through interaction with an environmental simulation. By embedding
this multi-layer cognitive process, the framework produces not only synthetic
trajectories that align closely with real-world patterns but also interpretable
representations of human decision logic. This research advances synthetic
mobility generation from a data-driven paradigm to a cognition-driven
simulation, providing a scalable pathway for understanding, predicting, and
synthesizing complex urban mobility behaviors through hierarchical LLM agents.

</details>


### [87] [MASPRM: Multi-Agent System Process Reward Model](https://arxiv.org/abs/2510.24803)
*Milad Yazdani,Mahdi Mostajabdaveh,Zirui Zhou,Ying Xiong*

Main category: cs.MA

TL;DR: MASPRM是一种多智能体系统推理控制模型，通过训练和引导搜索提升测试时性能，在数学题估计上显著提升了准确率。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统实际部署要求在推理时具有较强性能，需方法引导推理搜索和选择性计算以提升质量。

Method: 提出了MASPRM模型，基于多智能体蒙特卡罗树搜索回滚训练，无需人工标注，通过局部目标传递收益，推理时引导beam search和MCTS搜索计算。

Result: 在GSM8K和MATH数据集上，MASPRM结合结果奖励模型显著提升了准确率，且训练于GSM8K的模型可在MATH数据集零样本迁移提高性能。

Conclusion: MASPRM作为一个插件价值模型，可估计各智能体进度，提升多智能体系统推理的可靠性与计算效率，支持更有效的多智能体推理。

Abstract: Practical deployment of Multi-Agent Systems (MAS) demands strong test-time
performance, motivating methods that guide inference-time search and
selectively spend compute to improve quality. We present the Multi-Agent System
Process Reward Model (MASPRM). It assigns per-action, per-agent values to
partial inter-agent transcripts and acts as an inference-time controller.
MASPRM is trained from multi-agent Monte Carlo Tree Search (MCTS) rollouts
without requiring step-level human annotations, by propagating returns to local
targets. At inference, MASPRM guides step-level beam search and MCTS, focusing
computation on promising branches and pruning early. On GSM8K and MATH,
MASPRM-guided decoding with an outcome reward model (ORM) applied to the final
answer, improves exact match (EM) over a single straight-through MAS pass by
$+30.7$ and $+22.9$ points, respectively. A MASPRM trained on GSM8K transfers
zero-shot to MATH without retraining, adding $8.4$ EM points at the same
budget. MASPRM is a plug-in value model that estimates per-agent progress and
complements verifier-style decoders, enabling more reliable, compute-aware
multi-agent reasoning. Code: https://github.com/milad1378yz/MASPRM

</details>


### [88] [Trust Dynamics in Strategic Coopetition: Computational Foundations for Requirements Engineering in Multi-Agent Systems](https://arxiv.org/abs/2510.24909)
*Vik Pant,Eric Yu*

Main category: cs.MA

TL;DR: 本文提出了一种基于博弈论的计算信任模型，用于动态演化合作竞争关系中的信任，结合i*依赖网络实现需求工程背景下的信任分析。


<details>
  <summary>Details</summary>
Motivation: 多利益相关者环境中合作与竞争并存，现有概念建模语言无法计算性地分析信任动态变化，亟需将计算信任模型与需求工程语境结合。

Method: 提出双层信任系统（即时信任和声誉追踪），设计非对称信任更新机制，通过结构化转换框架将i*依赖网络映射到计算信任模型，并进行大规模实验和实证案例验证。

Result: 实验验证78,125种参数配置下，模型表现出负面偏见、滞后效应和累积损害放大效应。实证验证对雷诺-日产联盟1999-2025年信任演化达81.7%准确率，重现危机与恢复阶段信任变化。

Conclusion: 该模型成功填补了需求工程与计算信任模型之间的空白，为多利益相关者环境下的信任动态分析提供理论与实证支持。

Abstract: Requirements engineering increasingly occurs in multi-stakeholder
environments where organizations simultaneously cooperate and compete, creating
coopetitive relationships in which trust evolves dynamically based on observed
behavior over repeated interactions. While conceptual modeling languages like
i* represent trust relationships qualitatively, they lack computational
mechanisms for analyzing how trust changes with behavioral evidence.
Conversely, computational trust models from multi-agent systems provide
algorithmic updating but lack grounding in requirements engineering contexts
and conceptual models. This technical report bridges this gap by developing a
computational trust model that extends game-theoretic foundations for strategic
coopetition with dynamic trust evolution. We introduce trust as a two-layer
system with immediate trust responding to current behavior and reputation
tracking violation history. Trust evolves through asymmetric updating where
cooperation builds trust gradually while violations erode it sharply, creating
hysteresis effects and trust ceilings that constrain relationship recovery. We
develop a structured translation framework enabling requirements engineers to
instantiate computational trust models from i* dependency networks and
organizational contexts. Comprehensive experimental validation across 78,125
parameter configurations establishes robust emergence of negativity bias,
hysteresis effects, and cumulative damage amplification. Empirical validation
using the Renault-Nissan Alliance case study (1999-2025) achieves 49 out of 60
validation points (81.7%), successfully reproducing documented trust evolution
across five distinct relationship phases including crisis and recovery periods.
This technical report builds upon its foundational companion work in
arXiv:2510.18802.

</details>


### [89] [Emergent Coordinated Behaviors in Networked LLM Agents: Modeling the Strategic Dynamics of Information Operations](https://arxiv.org/abs/2510.25003)
*Gian Marco Orlando,Jinyi Ye,Valerio La Gatta,Mahdi Saeedi,Vincenzo Moscato,Emilio Ferrara,Luca Luceri*

Main category: cs.MA

TL;DR: 本文系统研究了生成式智能体在模拟信息操作（IO）活动中的协调机制，发现随着操作规程的结构化，信息网络更加紧密，互动更积极，协调效率提高。


<details>
  <summary>Details</summary>
Motivation: 随着生成式智能体技术的快速发展，其在在线生态系统尤其是信息操作中的协调作用引发了社会风险担忧。

Method: 通过基于生成式智能体的建模，在模拟环境中实例化信息操作和有机智能体，测试从简单目标一致到团队知识和集体决策的多种操作规程下的协调表现。

Result: 随着操作规程的复杂性增加，信息操作网络结构更紧凑，互动更为积极，信息传播更加同步且速度更快，目标共享的知晓即可促进接近集体决策的协调水平。

Conclusion: 生成式智能体能够在无人工干预下自发实现现实中信息操作的协调策略，表明自动化自组织信息操作带来的社会风险亟需关注。

Abstract: Generative agents are rapidly advancing in sophistication, raising urgent
questions about how they might coordinate when deployed in online ecosystems.
This is particularly consequential in information operations (IOs), influence
campaigns that aim to manipulate public opinion on social media. While
traditional IOs have been orchestrated by human operators and relied on
manually crafted tactics, agentic AI promises to make campaigns more automated,
adaptive, and difficult to detect. This work presents the first systematic
study of emergent coordination among generative agents in simulated IO
campaigns. Using generative agent-based modeling, we instantiate IO and organic
agents in a simulated environment and evaluate coordination across operational
regimes, from simple goal alignment to team knowledge and collective
decision-making. As operational regimes become more structured, IO networks
become denser and more clustered, interactions more reciprocal and positive,
narratives more homogeneous, amplification more synchronized, and hashtag
adoption faster and more sustained. Remarkably, simply revealing to agents
which other agents share their goals can produce coordination levels nearly
equivalent to those achieved through explicit deliberation and collective
voting. Overall, we show that generative agents, even without human guidance,
can reproduce coordination strategies characteristic of real-world IOs,
underscoring the societal risks posed by increasingly automated,
self-organizing IOs.

</details>


### [90] [SeeingEye: Agentic Information Flow Unlocks Multimodal Reasoning In Text-only LLMs](https://arxiv.org/abs/2510.25092)
*Weijia Zhang,Zijia Liu,Haoru Li,Haoqi Chen,Jiaxuan You*

Main category: cs.MA

TL;DR: 本文提出Seeing Eye框架，通过代理小型视觉语言模型作为翻译器，实现了文本大模型在多模态推理中的能力，显著提升了视觉问答任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有文本大模型在多模态任务中能力有限，单一形式的描述难以适应不同的视觉问答基准，缺乏有效传递细粒度视觉信息的机制。

Method: 提出模块化框架Seeing Eye，引入代理小型视觉语言模型作为翻译器，调用专用工具处理多模态输入，生成结构化中间表示（SIR），并与文本大模型反复交互，实现多轮反馈，从而提取目标视觉细节进行推理。

Result: 在多知识密集型视觉问答基准（如MMMU和MIA-Bench）中，Seeing Eye在降低推理成本的同时，性能超越了更大的端到端视觉语言模型。实例中组合3B视觉翻译器与8B语言推理器优于32B单体模型。

Conclusion: 通过代理信息流解耦感知与推理，Seeing Eye为多模态推理提供了可扩展、即插即用的路径，使得强大的文本大模型能够充分发挥推理能力。

Abstract: Recent advances in text-only large language models (LLMs), such as
DeepSeek-R1, demonstrate remarkable reasoning ability. However, these models
remain fragile or entirely incapable when extended to multi-modal tasks.
Existing approaches largely rely on single-form captions, which lack diversity
and often fail to adapt across different types of Visual Question Answering
(VQA) benchmarks. As a result, they provide no principled or efficient channel
for transmitting fine-grained visual information. We introduce Seeing Eye, a
modular framework that unlocks multimodal reasoning in text-only LLMs through
an agent-based small VLM translator. This translator acts as a perception
agent: it can invoke specialized tools (e.g., OCR and crop) and iteratively
distill multimodal inputs into structured intermediate representations (SIRs)
tailored to the question. These SIRs are then passed to the text-only LLM,
which serves as a reasoning agent. Crucially, the translator and reasoner
engage in multi-round feedback and interaction, enabling the extraction of
targeted visual details and yielding more confident answers. Experiments on
knowledge-intensive VQA benchmarks, including MMMU and MIA-Bench, demonstrate
that Seeing Eye not only reduces inference cost but also surpasses much larger
end-to-end VLMs. For example, an instantiation combining a 3B-parameter vision
translator with an 8B-parameter language reasoner outperforms a monolithic 32B
VLM on challenging knowledge-based questions. Our results highlight that
decoupling perception from reasoning via agent information flow offers a
scalable and plug-and-play pathway to multimodal reasoning, allowing strong
text-only LLMs to fully leverage their reasoning capabilities. Code is
available at: https://github.com/ulab-uiuc/SeeingEye

</details>


### [91] [Collaborative Scheduling of Time-dependent UAVs,Vehicles and Workers for Crowdsensing in Disaster Response](https://arxiv.org/abs/2510.25212)
*Lei Han,Jinhao Zhang,Jinhui Liu,Zhiyong Yu,Liang Wang,Quan Wang,Zhiwen Yu*

Main category: cs.MA

TL;DR: 本文提出了一种异构多智能体在线协同调度算法HoCs-MPQ，用于高效采集灾后环境信息，在多种基准方法中表现出显著优势。


<details>
  <summary>Details</summary>
Motivation: 自然灾害频发，灾后环境信息的及时高效采集对救援至关重要，但现有传感技术在复杂环境下适应性差、专业能力不足、实用性弱。

Method: HoCs-MPQ通过加权无向图构建多元素协作及冲突关系，基于多优先级队列迭代求解最大权重独立集，实现时间相关的无人机、车辆和工作人员的协同调度。

Result: 实验结果显示，HoCs-MPQ在任务完成率上较基线方法平均提升12.89%至54.13%，且在线单次调度计算时间不超过3秒。

Conclusion: HoCs-MPQ算法有效提升了灾后环境信息采集的效率与实用性，具备较强的协同调度能力和快速响应性能。

Abstract: Frequent natural disasters cause significant losses to human society, and
timely, efficient collection of post-disaster environmental information is the
foundation for effective rescue operations. Due to the extreme complexity of
post-disaster environments, existing sensing technologies such as mobile
crowdsensing suffer from weak environmental adaptability, insufficient
professional sensing capabilities, and poor practicality of sensing solutions.
Therefore, this paper explores a heterogeneous multi-agent online collaborative
scheduling algorithm, HoCs-MPQ, to achieve efficient collection of
post-disaster environmental information. HoCs-MPQ models collaboration and
conflict relationships among multiple elements through weighted undirected
graph construction, and iteratively solves the maximum weight independent set
based on multi-priority queues, ultimately achieving collaborative sensing
scheduling of time-dependent UA Vs, vehicles, and workers. Specifically, (1)
HoCs-MPQ constructs weighted undirected graph nodes based on collaborative
relationships among multiple elements and quantifies their weights, then models
the weighted undirected graph based on conflict relationships between nodes;
(2) HoCs-MPQ solves the maximum weight independent set based on iterated local
search, and accelerates the solution process using multi-priority queues.
Finally, we conducted detailed experiments based on extensive real-world and
simulated data. The experiments show that, compared to baseline methods (e.g.,
HoCs-GREEDY, HoCs-K-WTA, HoCs-MADL, and HoCs-MARL), HoCs-MPQ improves task
completion rates by an average of 54.13%, 23.82%, 14.12%, and 12.89%
respectively, with computation time for single online autonomous scheduling
decisions not exceeding 3 seconds.

</details>


### [92] [Multi-party Agent Relation Sampling for Multi-party Ad Hoc Teamwork](https://arxiv.org/abs/2510.25340)
*Beiwen Zhang,Yongheng Liang,Hejun Wu*

Main category: cs.MA

TL;DR: 提出了多方临时团队（MAHT）问题和MARs模型，通过稀疏骨架图和关系建模实现不同群体间协作，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体强化学习假设固定团队，临时团队方法假设共享惯例，现实中存在多个相互不熟悉的团队需协作的场景。

Method: 提出MARs方法，构建稀疏骨架图并应用关系建模，捕捉跨群体动态以实现多方临时团队协作。

Result: 在MPE和星际争霸II测试中，MARs比传统多智能体强化学习和临时团队基线方法表现更优，收敛更快。

Conclusion: MARs有效解决了多个互不熟悉团队间的协作问题，提升了多方临时团队协作的性能和效率。

Abstract: Multi-agent reinforcement learning (MARl) has achieved strong results in
cooperative tasks but typically assumes fixed, fully controlled teams. Ad hoc
teamwork (AHT) relaxes this by allowing collaboration with unknown partners,
yet existing variants still presume shared conventions. We introduce
Multil-party Ad Hoc Teamwork (MAHT), where controlled agents must coordinate
with multiple mutually unfamiliar groups of uncontrolled teammates. To address
this, we propose MARs, which builds a sparse skeleton graph and applies
relational modeling to capture cross-group dvnamics. Experiments on MPE and
starCralt ll show that MARs outperforms MARL and AHT baselines while converging
faster.

</details>
