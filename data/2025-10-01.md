<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 90]
- [cs.SE](#cs.SE) [Total: 38]
- [cs.MA](#cs.MA) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Cyclic Ablation: Testing Concept Localization against Functional Regeneration in AI](https://arxiv.org/abs/2509.25220)
*Eduard Kapelko*

Main category: cs.CL

TL;DR: 论文通过引入循环消融方法测试大型语言模型中的欺骗行为是否可局部移除，发现欺骗行为高度韧性且恢复能力强，且消融会导致语言性能下降。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型中不良行为（如欺骗）是否为局部功能，便于安全控制。

Method: 结合稀疏自动编码器、针对性消融和对抗训练，在DistilGPT-2上实施循环消融以尝试去除欺骗概念。

Result: 消融后，模型欺骗行为通过对抗训练可恢复，显示欺骗行为非局部且分布式，同时语言性能逐渐下降。

Conclusion: 复杂概念深度交织，难以通过机械解释性直接编辑模型，实现安全控制存在局限。

Abstract: Safety and controllability are critical for large language models. A central
question is whether undesirable behaviors like deception are localized
functions that can be removed, or if they are deeply intertwined with a model's
core cognitive abilities. We introduce "cyclic ablation," an iterative method
to test this. By combining sparse autoencoders, targeted ablation, and
adversarial training on DistilGPT-2, we attempted to eliminate the concept of
deception. We found that, contrary to the localization hypothesis, deception
was highly resilient. The model consistently recovered its deceptive behavior
after each ablation cycle via adversarial training, a process we term
functional regeneration. Crucially, every attempt at this "neurosurgery" caused
a gradual but measurable decay in general linguistic performance, reflected by
a consistent rise in perplexity. These findings are consistent with the view
that complex concepts are distributed and entangled, underscoring the
limitations of direct model editing through mechanistic interpretability.

</details>


### [2] [From Internal Representations to Text Quality: A Geometric Approach to LLM Evaluation](https://arxiv.org/abs/2509.25359)
*Viacheslav Yusupov,Danil Maksimov,Ameliia Alaeva,Anna Vasileva,Anna Antipina,Tatyana Zaitseva,Alina Ermilova,Evgeny Burnaev,Egor Shvetsov*

Main category: cs.CL

TL;DR: 本文通过几何特征指标实现对大语言模型生成文本质量的无参考评估，无需人工标注数据。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法依赖人工标注数据，缺乏自动化且通用的文本质量评价手段。

Method: 验证包括最大可解释方差、有效秩、内在维度、MAUVE分数和Schatten范数等多种几何指标在不同模型层的测量，发现内在维度和有效秩可作为文本自然度和质量的通用评估指标。

Result: 不同模型基于几何指标对文本排名一致，指标反映文本内在特征而非模型特有偏差，实现了无参考自动化评价。

Conclusion: 几何属性指标可作为无参考的文本质量评估工具，推动自动化评估流程的发展，减少对人工标注数据的依赖。

Abstract: This paper bridges internal and external analysis approaches to large
language models (LLMs) by demonstrating that geometric properties of internal
model representations serve as reliable proxies for evaluating generated text
quality. We validate a set of metrics including Maximum Explainable Variance,
Effective Rank, Intrinsic Dimensionality, MAUVE score, and Schatten Norms
measured across different layers of LLMs, demonstrating that Intrinsic
Dimensionality and Effective Rank can serve as universal assessments of text
naturalness and quality. Our key finding reveals that different models
consistently rank text from various sources in the same order based on these
geometric properties, indicating that these metrics reflect inherent text
characteristics rather than model-specific artifacts. This allows a
reference-free text quality evaluation that does not require human-annotated
datasets, offering practical advantages for automated evaluation pipelines.

</details>


### [3] [Generative Value Conflicts Reveal LLM Priorities](https://arxiv.org/abs/2509.25369)
*Andy Liu,Kshitish Ghate,Mona Diab,Daniel Fried,Atoosa Kasirzadeh,Max Kleiman-Weiner*

Main category: cs.CL

TL;DR: 本文提出了ConflictScope，一个自动评估大型语言模型在价值冲突中优先排序不同价值观的工具。


<details>
  <summary>Details</summary>
Motivation: 现有数据集中缺乏价值冲突的场景，导致无法有效评估模型在价值冲突时的表现。

Method: ConflictScope自动生成包含价值冲突的场景，使用LLM撰写用户提示，评估模型对价值的排序，并比较多项选择与开放式回答的差异。

Result: 模型在开放式价值冲突场景下倾向于支持个人价值（如用户自主权），而非保护性价值（如无害性）；通过系统提示包含详细价值排序，可以提升模型对目标价值排序的对齐度14%。

Conclusion: 评价模型的价值优先级排序至关重要，ConflictScope为未来相关研究提供了基础和方法。

Abstract: Past work seeks to align large language model (LLM)-based assistants with a
target set of values, but such assistants are frequently forced to make
tradeoffs between values when deployed. In response to the scarcity of value
conflict in existing alignment datasets, we introduce ConflictScope, an
automatic pipeline to evaluate how LLMs prioritize different values. Given a
user-defined value set, ConflictScope automatically generates scenarios in
which a language model faces a conflict between two values sampled from the
set. It then prompts target models with an LLM-written "user prompt" and
evaluates their free-text responses to elicit a ranking over values in the
value set. Comparing results between multiple-choice and open-ended
evaluations, we find that models shift away from supporting protective values,
such as harmlessness, and toward supporting personal values, such as user
autonomy, in more open-ended value conflict settings. However, including
detailed value orderings in models' system prompts improves alignment with a
target ranking by 14%, showing that system prompting can achieve moderate
success at aligning LLM behavior under value conflict. Our work demonstrates
the importance of evaluating value prioritization in models and provides a
foundation for future work in this area.

</details>


### [4] [From Faithfulness to Correctness: Generative Reward Models that Think Critically](https://arxiv.org/abs/2509.25409)
*Qiyao Ma,Yunsheng Shi,Hongtao Tian,Chao Wang,Weiming Chang,Ting Yao*

Main category: cs.CL

TL;DR: 本文提出了思维监督奖励模型（TRM），通过句子级的思维监督提升奖励模型的批判性思维能力，从而在开放域问答任务中更准确地评估答案的正确性。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习与可验证奖励方法在复杂任务（如开放域问答）中面临奖励难以验证的问题，且现有方法过度依赖外部文档，缺乏对内部知识的批判性评估。

Method: TRM模型通过评估答案中每个句子与支持文档的一致性，再进行推理判断句子级正确性，将奖励建模为一致性、推理和正确性的序列评估，从而提升批判性思维能力。

Result: 实验表明，TRM在识别错误句子上效果显著提升，并且将TRM融入策略优化后，答案的正确性和实用性均有明显提升。

Conclusion: TRM通过引入句子级思维监督，有效提升了模型对复杂知识的评估能力，推动了开放域问答中基于强化学习的方法性能提升。

Abstract: Through reinforcement learning with verifiable rewards (RLVR), large language
models have achieved substantial progress in domains with easily verifiable
outcomes, such as mathematics and coding. However, when applied to more complex
tasks like open-domain question answering, RLVR faces significant challenges
due to the difficulty of verifying correctness. The nuanced and ambiguous
nature of real-world knowledge makes it difficult to reliably evaluate
correctness in these settings, necessitating further abilities that extend
beyond mere logical consistency to encompass an understanding and assessment of
both external and internal knowledge. Recent work has primarily focused on
improving faithfulness, defined as semantic alignment with supporting
documents, which can cause models to rely excessively on external sources and
diminish their capacity for critical assessment. To address this, we propose
the Thinking-supervised Reward Model (TRM), which incorporates sentence-level
thinking supervision to endow reward models with critical thinking abilities.
Given a query, answer, and supporting documents, TRM first assesses the
faithfulness of each answer sentence to the supporting documents, and then
applies a reasoning step to evaluate sentence-level correctness. By structuring
reward modeling as a sequence of faithfulness, reasoning, and correctness
evaluations, TRM encourages models to critically assess and leverage both
external and internal knowledge. Experiments on reward signals demonstrate that
TRM substantially improves the identification of incorrect sentences, and
incorporating TRM into policy optimization leads to significant gains in both
answer correctness and usefulness.

</details>


### [5] [Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization](https://arxiv.org/abs/2509.25416)
*Jiacheng Shi,Hongfei Du,Yangfan He,Y. Alicia Hong,Ye Gao*

Main category: cs.CL

TL;DR: 本文提出了一种情感感知的分步偏好优化框架EASPO，通过中间去噪步骤实现细粒度情感控制，提升了语音的表现力和自然度。


<details>
  <summary>Details</summary>
Motivation: 现有情感文本到语音转换方法多依赖粗粒度标签或代理分类器，且仅接受整句反馈，难以实现精细的情感控制。

Method: 本文引入EASPM模型对中间有噪声的语音状态进行评分，自动构建偏好对以指导EASPO框架在生成过程中优化情感表达。

Result: 实验结果证明，所提方法在情感表现力和自然度方面均优于已有方法。

Conclusion: EASPO通过在中间去噪步骤加入细粒度情感偏好，实现了对情感语音的可控生成，效果显著优于现有技术。

Abstract: Emotional text-to-speech seeks to convey affect while preserving
intelligibility and prosody, yet existing methods rely on coarse labels or
proxy classifiers and receive only utterance-level feedback. We introduce
Emotion-Aware Stepwise Preference Optimization (EASPO), a post-training
framework that aligns diffusion TTS with fine-grained emotional preferences at
intermediate denoising steps. Central to our approach is EASPM, a
time-conditioned model that scores noisy intermediate speech states and enables
automatic preference pair construction. EASPO optimizes generation to match
these stepwise preferences, enabling controllable emotional shaping.
Experiments show superior performance over existing methods in both
expressiveness and naturalness.

</details>


### [6] [SimulRAG: Simulator-based RAG for Grounding LLMs in Long-form Scientific QA](https://arxiv.org/abs/2509.25459)
*Haozhou Xu,Dongxia Wu,Matteo Chinazzi,Ruijia Niu,Rose Yu,Yi-An Ma*

Main category: cs.CL

TL;DR: 本文提出了一种基于模拟器的检索增强生成框架SimulRAG，用于提升长篇科学问答的准确性和可信度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在长篇科学问答中存在幻觉问题，传统检索增强生成方法难以直接应用于科学模拟器检索及答案验证。

Method: 提出SimulRAG框架，设计通用模拟器检索接口，以及结合不确定性估计和模拟器边界评估的声明级生成方法（UE+SBA）来验证和更新答案。

Result: SimulRAG在气候科学和流行病学的长篇科学问答任务中，比传统RAG提升了30.4%的信息量和16.3%的事实准确性，UE+SBA提升了生成效率与质量。

Conclusion: SimulRAG有效解决了科学模拟器检索和答案验证问题，显著提升了基于大型语言模型的长篇科学问答的可信性和信息质量。

Abstract: Large language models (LLMs) show promise in solving scientific problems.
They can help generate long-form answers for scientific questions, which are
crucial for comprehensive understanding of complex phenomena that require
detailed explanations spanning multiple interconnected concepts and evidence.
However, LLMs often suffer from hallucination, especially in the challenging
task of long-form scientific question answering. Retrieval-Augmented Generation
(RAG) approaches can ground LLMs by incorporating external knowledge sources to
improve trustworthiness. In this context, scientific simulators, which play a
vital role in validating hypotheses, offer a particularly promising retrieval
source to mitigate hallucination and enhance answer factuality. However,
existing RAG approaches cannot be directly applied for scientific
simulation-based retrieval due to two fundamental challenges: how to retrieve
from scientific simulators, and how to efficiently verify and update long-form
answers. To overcome these challenges, we propose the simulator-based RAG
framework (SimulRAG) and provide a long-form scientific QA benchmark covering
climate science and epidemiology with ground truth verified by both simulations
and human annotators. In this framework, we propose a generalized simulator
retrieval interface to transform between textual and numerical modalities. We
further design a claim-level generation method that utilizes uncertainty
estimation scores and simulator boundary assessment (UE+SBA) to efficiently
verify and update claims. Extensive experiments demonstrate SimulRAG
outperforms traditional RAG baselines by 30.4% in informativeness and 16.3% in
factuality. UE+SBA further improves efficiency and quality for claim-level
generation.

</details>


### [7] [The Rise of AfricaNLP: Contributions, Contributors, and Community Impact (2005-2025)](https://arxiv.org/abs/2509.25477)
*Tadesse Destaw Belay,Kedir Yassin Hussen,Sukairaj Hafiz Imam,Iqra Ameer,Ibrahim Said Ahmad,Isa Inuwa-Dutse,Idris Abdulmumin,Grigori Sidorov,Vukosi Marivate,Seid Muhie Yimam,Shamsuddeen Hassan Muhammad*

Main category: cs.CL

TL;DR: 本文通过定量分析非洲自然语言处理（AfricaNLP）领域的论文进展，揭示了过去二十年NLP的演变及重要贡献。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型推动NLP领域快速发展，追踪和自动分析论文贡献有助于深入了解领域动态及研究者情况。

Method: 收集分析1.9K篇AfricaNLP论文摘要、4.9K作者数据及7.8K手工标注贡献句子，结合基准测试结果进行定量研究。

Result: 构建了一个涵盖非洲NLP研究趋势的数据集及持续更新的跟踪网站，为领域文献综述和趋势分析提供数据支持。

Conclusion: 该工作为非洲NLP领域的发展提供了重要洞见和资源，促进了领域研究的系统理解和数据驱动的文献综述。

Abstract: Natural Language Processing (NLP) is undergoing constant transformation, as
Large Language Models (LLMs) are driving daily breakthroughs in research and
practice. In this regard, tracking the progress of NLP research and
automatically analyzing the contributions of research papers provides key
insights into the nature of the field and the researchers. This study explores
the progress of African NLP (AfricaNLP) by asking (and answering) basic
research questions such as: i) How has the nature of NLP evolved over the last
two decades?, ii) What are the contributions of AfricaNLP papers?, and iii)
Which individuals and organizations (authors, affiliated institutions, and
funding bodies) have been involved in the development of AfricaNLP? We
quantitatively examine the contributions of AfricaNLP research using 1.9K NLP
paper abstracts, 4.9K author contributors, and 7.8K human-annotated
contribution sentences (AfricaNLPContributions) along with benchmark results.
Our dataset and continuously existing NLP progress tracking website provide a
powerful lens for tracing AfricaNLP research trends and hold potential for
generating data-driven literature surveys.

</details>


### [8] [Not Wrong, But Untrue: LLM Overconfidence in Document-Based Queries](https://arxiv.org/abs/2509.25498)
*Nick Hagar,Wilma Agustianto,Nicholas Diakopoulos*

Main category: cs.CL

TL;DR: 本研究评估了三款大语言模型在新闻报道任务中的幻觉问题，发现模型频繁生成不实信息，尤其是Gemini和ChatGPT。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在新闻工作流程中被广泛使用，但其生成虚假信息的倾向威胁新闻报道的来源、归因和准确性这一核心原则。

Method: 选取与TikTok诉讼及政策相关的300篇文档作为语料，设计新闻报道任务，利用不同具体性的提示和上下文大小，对ChatGPT、Gemini和NotebookLM的输出进行句子级注释，并根据自定义的幻觉类型分类进行评估。

Result: 约30%的模型输出包含至少一个幻觉错误，Gemini和ChatGPT的错误率约为40%，NotebookLM仅为13%。错误主要表现为对信息的过度诠释，如无依据地加入对来源的特征描述或将带有归因的观点泛化为普通陈述。

Conclusion: 当前大语言模型生成内容与新闻报道对准确引用的需求存在根本认知差异，作者建议针对新闻行业开发特定的幻觉分类和具有准确归因机制的模型架构，以提升新闻工具的可靠性。

Abstract: Large language models (LLMs) are increasingly used in newsroom workflows, but
their tendency to hallucinate poses risks to core journalistic practices of
sourcing, attribution, and accuracy. We evaluate three widely used tools -
ChatGPT, Gemini, and NotebookLM - on a reporting-style task grounded in a
300-document corpus related to TikTok litigation and policy in the U.S. We vary
prompt specificity and context size and annotate sentence-level outputs using a
taxonomy to measure hallucination type and severity. Across our sample, 30% of
model outputs contained at least one hallucination, with rates approximately
three times higher for Gemini and ChatGPT (40%) than for NotebookLM (13%).
Qualitatively, most errors did not involve invented entities or numbers;
instead, we observed interpretive overconfidence - models added unsupported
characterizations of sources and transformed attributed opinions into general
statements. These patterns reveal a fundamental epistemological mismatch: While
journalism requires explicit sourcing for every claim, LLMs generate
authoritative-sounding text regardless of evidentiary support. We propose
journalism-specific extensions to existing hallucination taxonomies and argue
that effective newsroom tools need architectures that enforce accurate
attribution rather than optimize for fluency.

</details>


### [9] [Beyond WER: Probing Whisper's Sub-token Decoder Across Diverse Language Resource Levels](https://arxiv.org/abs/2509.25516)
*Siyu Liang,Nicolas Ballier,Gina-Anne Levow,Richard Wright*

Main category: cs.CL

TL;DR: 本文通过细粒度分析Whisper多语言解码器的子标记假设，揭示了不同资源语言间自动语音识别性能和公平性的差异。


<details>
  <summary>Details</summary>
Motivation: 大规模多语言端到端自动语音识别模型虽然表现优异，但其内部机制尤其是跨语言公平性和效能尚未深入研究。

Method: 作者通过追踪beam搜索路径，捕获不同语言在转录过程中的子标记猜测及概率，结合PCA和t-SNE分析探究子标记使用的聚类模式。

Result: 高资源语言在正确子标记排名、置信度、预测熵和备选候选多样性上表现更好，低资源语言表现较差且显示出由语言类型影响的聚类特征。

Conclusion: 子标记层面的分析揭示了总错误率掩盖的解码不平衡，为改进多语言语音技术的发展提供了针对性干预方向。

Abstract: While large multilingual automatic speech recognition (ASR) models achieve
remarkable performance, the internal mechanisms of the end-to-end pipeline,
particularly concerning fairness and efficacy across languages, remain
underexplored. This paper introduces a fine-grained analysis of Whisper's
multilingual decoder, examining its sub-token hypotheses during transcription
across languages with various resource levels. Our method traces the beam
search path, capturing sub-token guesses and their associated probabilities.
Results reveal that higher resource languages benefit from higher likelihood of
the correct token being top-ranked, greater confidence, lower predictive
entropy, and more diverse alternative candidates. Lower resource languages fare
worse on these metrics, but also exhibit distinct clustering patterns in
sub-token usage sometimes influenced by typology in our PCA and t-SNE analysis.
This sub-token probing uncovers systematic decoding disparities masked by
aggregate error rates and points towards targeted interventions to ameliorate
the imbalanced development of speech technology.

</details>


### [10] [MixtureVitae: Open Web-Scale Pretraining Dataset With High Quality Instruction and Reasoning Data Built from Permissive-First Text Sources](https://arxiv.org/abs/2509.25531)
*Huu Nguyen,Victor May,Harsh Raj,Marianna Nezhurina,Yishan Wang,Yanqi Luo,Minh Chien Vu,Taishi Nakamura,Ken Tsui,Van Khue Nguyen,David Salinas,Aleksandra Krasnodębska,Christoph Schuhmann,Mats Leon Richter,Xuan-Son,Vu,Jenia Jitsev*

Main category: cs.CL

TL;DR: MixtureVitae是一种注重法律风险最小化且性能优异的开放访问预训练语料库。


<details>
  <summary>Details</summary>
Motivation: 当前大规模语言模型训练常依赖于未经筛选的网络数据，存在较高的法律风险。MixtureVitae旨在提供一个风险可控且性能强劲的预训练语料库，减少法律风险的同时保持模型竞争力。

Method: MixtureVitae采用多阶段流程，包括基于许可的筛选、安全和质量检查，以及领域感知的混合策略，结合公开领域、宽松授权文本和低风险数据（如政府公开数据和欧盟文本采矿许可的数据），并添加有针对性的指令、推理和合成数据。

Result: 在严格控制的实验中，MixtureVitae训练的模型在多个标准基准测试中持续超过其他宽松授权的数据集，尤其在数学和代码任务上表现优异，在大型模型训练中接近高级数据集性能。

Conclusion: MixtureVitae示范了以宽松许可优先、风险可控的数据构建方式，可以为训练能力强大的大语言模型提供合法合规且具有竞争力的基础，减少依赖无筛选网络抓取数据。

Abstract: We present MixtureVitae, an open-access pretraining corpus built to minimize
legal risk while providing strong model performance. MixtureVitae follows a
risk-mitigated sourcing strategy that combines public-domain and permissively
licensed text (e.g., CC-BY/Apache) with carefully justified low-risk additions
(e.g., government works and EU TDM-eligible sources), alongside targeted
instruction, reasoning and synthetic data with documented provenance. We detail
a transparent, multi-stage pipeline for license-aware filtering, safety and
quality screening, and domain-aware mixing, and we release the dataset and
curation recipes to support reproducible research. In controlled experiments
using the open-sci-ref training protocol (fixed architectures at
130M/400M/1.3B/1.7B parameters; training budgets of 50B and 300B tokens),
models trained on MixtureVitae consistently outperform other permissive
datasets across a suite of standard benchmarks, and at the 1.7B/300B setting
they surpass FineWeb-Edu and approach DCLM in the later stages of training.
Performance is particularly strong on math/code and competitive on QA tasks.
These results demonstrate that permissive-first, risk-mitigated data provides a
practical and legally mitigated foundation for training capable LLMs, reducing
reliance on indiscriminate web scraping without sacrificing competitiveness.
Code: https://github.com/ontocord/mixturevitae

</details>


### [11] [Calibrating Verbalized Confidence with Self-Generated Distractors](https://arxiv.org/abs/2509.25532)
*Victor Wang,Elias Stengel-Eskin*

Main category: cs.CL

TL;DR: 本文针对大型语言模型（LLM）输出的置信度过高、误导用户的问题，提出了DINCO方法校准置信度。


<details>
  <summary>Details</summary>
Motivation: 当前LLM生成的口头置信度常常误报高置信度，尤其是在模型知识不足的情况下，影响用户信任和安全。

Method: 通过引入Distractor-Normalized Coherence（DINCO）方法，让模型对多个自生成的干扰选项分别表达置信度，并归一化，结合生成器与验证器之间的一致性来提升置信度校准。

Result: 实验表明，DINCO能够提供更准确且不饱和的置信度估计，在召回次数较少的情况下优于传统的基于自一致性的方法。

Conclusion: DINCO有效解决了LLM过度自信的问题，提升了置信度的可信度和可用性，促进了模型输出的安全性和可靠性。

Abstract: Calibrated confidence estimates are necessary for large language model (LLM)
outputs to be trusted by human users. While LLMs can express their confidence
in human-interpretable ways, verbalized LLM-generated confidence scores have
empirically been found to be miscalibrated, reporting high confidence on
instances with low accuracy and thereby harming trust and safety. We
hypothesize that this overconfidence often stems from a given LLM's heightened
suggestibility when faced with claims that it encodes little information about;
we empirically validate this hypothesis, finding more suggestibility on
lower-accuracy claims. Building on this finding, we introduce
Distractor-Normalized Coherence (DINCO), which estimates and accounts for an
LLM's suggestibility bias by having the model verbalize its confidence
independently across several self-generated distractors (i.e. alternative
claims), and normalizes by the total verbalized confidence. To further improve
calibration, we leverage generator-validator disagreement, augmenting
normalized validator confidence with a consistency-based estimate of generator
confidence. Here, we frame the popular approach of self-consistency as
leveraging coherence across sampled generations, and normalized verbalized
confidence as leveraging coherence across validations on incompatible claims,
allowing us to integrate these complementary dimensions of coherence into
DINCO. Moreover, our analysis shows that DINCO provides less saturated -- and
therefore more usable -- confidence estimates, and that further sampling alone
cannot close the gap between DINCO and baselines, with DINCO at 10 inference
calls outperforming self-consistency at 100.

</details>


### [12] [Self-Rewarding Rubric-Based Reinforcement Learning for Open-Ended Reasoning](https://arxiv.org/abs/2509.25534)
*Zhiling Ye,Yun Yue,Haowen Wang,Xudong Han,Jiadi Jiang,Cheng Wei,Lei Fan,Jiaxin Liang,Shuowen Zhang,Ji Li,Chunxiao Guo,Jian Wang,Peng Wei,Jinjie Gu*

Main category: cs.CL

TL;DR: 本文提出了一种基于自我评分和评分标准的强化学习方法，用于提升大语言模型在开放式推理任务中的表现，实现更快、更高效的训练，并取得优于基线模型的效果。


<details>
  <summary>Details</summary>
Motivation: 观测到使用模型自身作为评分者并结合评分标准生成奖励信号能显著提升推理性能，同时训练后的模型评分能力得到增强。

Method: 提出Self-Rewarding Rubric-Based Reinforcement Learning框架，通过模型自我评分和评分标准指导训练，实现轻量级且高效的训练方法。

Result: 在Qwen3-32B模型上，仅用4000条HealthBench Easy子集样本训练，即可超越GPT-5在HealthBench Hard任务上的表现。结合少量教师评分数据可进一步提升低能力模型表现。

Conclusion: 所提方法能有效提升大语言模型在开放式推理任务中的性能，且训练效率高，适合资源有限情境下的模型优化。

Abstract: Open-ended evaluation is essential for deploying large language models in
real-world settings. In studying HealthBench, we observe that using the model
itself as a grader and generating rubric-based reward signals substantially
improves reasoning performance. Remarkably, the trained model also becomes a
stronger grader. Motivated by this, we introduce Self-Rewarding Rubric-Based
Reinforcement Learning for Open-Ended Reasoning, a lightweight framework that
enables faster and more resource-efficient training while surpassing baselines.
Remarkably, on Qwen3-32B, training with just the 4000-sample HealthBench Easy
subset is sufficient to obtain a model that exceeds GPT-5 on HealthBench Hard.
Incorporating a small amount of teacher-graded data further enhances
performance for less capable models.

</details>


### [13] [Aligning Multilingual Reasoning with Verifiable Semantics from a High-Resource Expert Model](https://arxiv.org/abs/2509.25543)
*Fahim Faisal,Kaiqiang Song,Song Wang,Simin Ma,Shujian Liu,Haoyun Deng,Sathish Reddy Indurthi*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的多语言推理增强框架PB-RLSVR，通过利用英文大模型作为“枢纽”，实现跨语言的推理能力迁移，大幅提升非英语语言的推理性能。


<details>
  <summary>Details</summary>
Motivation: 目前强化学习虽然提升了大语言模型的推理能力，但主要集中在英语，导致多语言间性能差距显著，缺乏针对目标语言的人类标注数据。

Method: 采用高性能英文大模型作为参考生成器，利用语义等价奖励函数对多语言模型进行强化学习，跨语言传递推理能力，同时设计多种跨语义奖励函数如基于嵌入和机器翻译的方法。

Result: 在多语言推理基准上，PB-RLSVR显著缩小了英语与其他语言的性能差距，优于传统PPO方法，分别使Llama-3.1-8B-Instruct和Qwen3-32B模型的多语言平均性能提升16.41%和10.17%。

Conclusion: PB-RLSVR是一种强大且高效的数据利用方法，能够构建真正的多语言推理智能体，促进多语言环境下的语言模型推理能力均衡提升。

Abstract: While reinforcement learning has advanced the reasoning abilities of Large
Language Models (LLMs), these gains are largely confined to English, creating a
significant performance disparity across languages. To address this, we
introduce Pivot-Based Reinforcement Learning with Semantically Verifiable
Rewards (PB-RLSVR), a novel framework that enhances multilingual reasoning by
circumventing the need for human-annotated data in target languages. Our
approach employs a high-performing English LLM as a "pivot" model to generate
reference responses for reasoning tasks. A multilingual model is then rewarded
based on the semantic equivalence of its responses to the English reference,
effectively transferring the pivot model's reasoning capabilities across
languages. We investigate several cross-lingual semantic reward functions,
including those based on embeddings and machine translation. Extensive
experiments on a suite of multilingual reasoning benchmarks show that our
method significantly narrows the performance gap between English and other
languages, substantially outperforming traditional PPO baselines. Specifically,
our PB-RLSVR framework improves the average multilingual performance of
Llama-3.1-8B-Instruct and Qwen3-32B by 16.41% and 10.17%, respectively,
demonstrating a powerful and data-efficient approach to building truly
multilingual reasoning agents.

</details>


### [14] [Performance and competence intertwined: A computational model of the Null Subject stage in English-speaking children](https://arxiv.org/abs/2509.25545)
*Soumik Dey,William Gregory Sakas*

Main category: cs.CL

TL;DR: 儿童在4岁前通常省略句子主语，导致对命令句和陈述句的误判。本文提出计算参数模拟这一现象，验证了相关假设。


<details>
  <summary>Details</summary>
Motivation: 探讨儿童在语言习得中因省略主语而出现的命令句和陈述句混淆现象及其认知基础。

Method: 设计新的计算参数衡量此类误解，并结合改良的变分学习模型（Variational Learner）对强制性主语语法的习得过程进行模拟。

Result: 模拟结果支持Orfitelli和Hyams关于儿童暂时省略主语语法的假设，验证误解存在性能影响。

Conclusion: 研究提出了将计算模型与语法习得研究结合的新框架，为理解儿童语言发展提供了有效工具。

Abstract: The empirically established null subject (NS) stage, lasting until about 4
years of age, involves frequent omission of subjects by children. Orfitelli and
Hyams (2012) observe that young English speakers often confuse imperative NS
utterances with declarative ones due to performance influences, promoting a
temporary null subject grammar. We propose a new computational parameter to
measure this misinterpretation and incorporate it into a simulated model of
obligatory subject grammar learning. Using a modified version of the
Variational Learner (Yang, 2012) which works for superset-subset languages, our
simulations support Orfitelli and Hyams' hypothesis. More generally, this study
outlines a framework for integrating computational models in the study of
grammatical acquisition alongside other key developmental factors.

</details>


### [15] [Don't Sweat the Small Stuff: Segment-Level Meta-Evaluation Based on Pairwise Difference Correlation](https://arxiv.org/abs/2509.25546)
*Colten DiIanni,Daniel Deutsch*

Main category: cs.CL

TL;DR: 本文提出了Pairwise Difference Pearson (PDP)，一种基于分段的机器翻译评价指标，用于改进之前的Pearson和Kendall相关性指标。


<details>
  <summary>Details</summary>
Motivation: 现有的基于Pearson和Kendall相关性的机器翻译元评价方法存在局限性，难以充分利用全部分段信息，且对评分分布的理解不够鲁棒。

Method: PDP通过利用成对差值而非原始评分，结合所有分段信息，改进了全球Pearson相关系数以适用于分段内评分比较，实现更加稳健的相关性度量。

Result: 在WMT'24共享任务中，PDP能更合理地排序哨兵评价指标，并且其结果与人工误差权重具有更高的一致性。同时，PDP对随机噪声、分段偏差和系统偏差表现出较强的鲁棒性，且对极端异常值较为敏感。

Conclusion: PDP作为一种新颖且鲁棒的分段级Meta-评价指标，能有效改善机器翻译评分相关性评价，明确了其在衡量评估指标与人工评分一致性方面的优势。

Abstract: This paper introduces Pairwise Difference Pearson (PDP), a novel
segment-level meta-evaluation metric for Machine Translation (MT) that address
limitations in previous Pearson's $\rho$-based and and Kendall's $\tau$-based
meta-evaluation approaches. PDP is a correlation-based metric that utilizes
pairwise differences rather than raw scores. It draws on information from all
segments for a more robust understanding of score distributions and uses
segment-wise pairwise differences to refine Global Pearson to intra-segment
score comparisons. Analysis on the WMT'24 shared task shows PDP properly ranks
sentinel evaluation metrics and better aligns with human error weightings than
previous work. Noise injection analysis demonstrates PDP's robustness to random
noise, segment bias, and system bias while highlighting its sensitivity to
extreme outliers.

</details>


### [16] [Probing the Limits of Stylistic Alignment in Vision-Language Models](https://arxiv.org/abs/2509.25568)
*Asma Farajidizaji,Akash Gupta,Vatsal Raina*

Main category: cs.CL

TL;DR: 本文研究了小型视觉-语言模型在幽默和浪漫风格图像字幕生成中的数据效率，探讨了在零样本条件下通过少量偏好数据达到风格对齐的性能极限。


<details>
  <summary>Details</summary>
Motivation: 目前基于transformer的视觉-语言模型在零样本条件下难以实现特定风格（如幽默、浪漫）的图像字幕生成，而获取偏好数据成本高，限制了模型能力的深入探索。

Method: 通过研究小型视觉-语言模型在少量偏好数据下对幽默和浪漫两种风格的对齐效果，评估模型所需偏好数据的最小量以及性能的饱和点。

Result: 实验结果揭示了模型在不同风格上的性能极限和对齐效率，展示了少量偏好数据已能达到近似饱和的风格表现。

Conclusion: 该研究为理解视觉-语言模型在主观风格生成任务中的能力和限制提供了重要参考，表明使用高效的数据对齐方法可最大化模型性能，减轻偏好数据需求。

Abstract: Vision-language models are increasingly used to generate image captions in
specific styles, such as humor or romantic. However, these transformer-based
models often struggle with this subjective task in a zero-shot setting. While
preference data can be used to align them toward a desired style, such data is
expensive to acquire, limiting the ability to explore the models' full
capabilities. This work addresses this by studying the data efficiency of
aligning small vision-language models to humor and romantic styles. This
approach helps to define the performance limits of these models and determine
how little preference data is needed to achieve stylistic saturation,
benchmarking their capabilities and limitations.

</details>


### [17] [RFG: Test-Time Scaling for Diffusion Large Language Model Reasoning with Reward-Free Guidance](https://arxiv.org/abs/2509.25604)
*Tianlang Chen,Minkai Xu,Jure Leskovec,Stefano Ermon*

Main category: cs.CL

TL;DR: 本文提出了一种无需显式奖励的引导方法RFG，用于改进扩散大语言模型（dLLMs）的推理能力，实现推理轨迹的有效引导。


<details>
  <summary>Details</summary>
Motivation: 针对扩散大语言模型生成过程无序且中间状态部分遮蔽的特点，传统依赖中间步骤密集注释的过程奖励模型难以应用，亟需一种无需过程奖励的引导方法。

Method: RFG通过参数化增强模型与参考模型的对数似然比作为过程奖励，实现无奖励引导。增强模型可通过任何后期训练的dLLM获得，如强化学习或监督微调。理论证明RFG能诱导出奖励导向的采样分布。

Result: 在四个数学推理和代码生成任务上，RFG在多种后期训练的dLLMs上均显著提升性能，最高提升准确率达9.2%。

Conclusion: RFG为一种不依赖外部奖励模型的训练自由框架，有效提升扩散大语言模型的测试时推理能力，具备广泛适用性和显著效果。

Abstract: Diffusion large language models (dLLMs) have shown great potential in
large-scale language modeling, and there is an increasing interest in further
improving the capacity to solve complex problems by guiding the reasoning
process step by step. Common practice for autoregressive language models
typically learns a process reward model with dense annotation for each
intermediate step. However, this is challenging for dLLMs where the generation
is in an any-order fashion and intermediate states are partially masked
sentences. To this end, in this paper, we propose reward-free guidance (RFG), a
principled method for guiding the reasoning trajectory of dLLMs without
explicit process reward. The key idea of RFG is to parameterize the process
reward by log-likelihood ratios of the enhanced and reference dLLMs, where the
enhanced model can be easily obtained by any off-the-shelf dLLM that has been
post-trained with reinforcement learning (RL) or supervised fine-tuning (SFT).
We provide theoretical justification that RFG induces the reward-guided
sampling distribution with no additional reward. We conduct comprehensive
experiments on four challenging mathematical reasoning and code generation
benchmarks using a diverse suite of dLLMs enhanced with various post-training
methods. RFG consistently yields significant improvements across all tasks and
model types, achieving accuracy gains of up to 9.2%. These findings establish
RFG as a general training-free framework that scales test-time reasoning
without reliance on external reward models.

</details>


### [18] [Transformers through the lens of support-preserving maps between measures](https://arxiv.org/abs/2509.25611)
*Takashi Furuya,Maarten V. de Hoop,Matti Lassas*

Main category: cs.CL

TL;DR: 本文研究了Transformer架构作为概率测度映射的表达能力，证明它们能近似任意连续的上下文映射，并且与Vlasov方程的解映射存在对应关系。


<details>
  <summary>Details</summary>
Motivation: 为了统一数学分析Transformer处理任意大量上下文标记的表达能力，将其建模为概率测度上的映射，探索Transformer在这一框架下的表现和属性。

Method: 通过分析Transformer作为概率测度的映射，特征化满足条件的测度映射并证明Transformer的通用逼近性质，同时关联测度理论自注意力机制与Vlasov方程解映射。

Result: 证明了Transformer映射保持支持的基数且其Fréchet导数的正则部分一致连续；展示了Vlasov方程的解映射满足这些条件，可由Transformer近似；测度理论自注意力机制可视为Vlasov流。

Conclusion: 本文建立了Transformer作为测度映射的理论基础，说明其通用表达能力和与非局部运输类型动力学（Vlasov方程）间的深刻联系，拓展了Transformer在理论和应用上的理解。

Abstract: Transformers are deep architectures that define ``in-context maps'' which
enable predicting new tokens based on a given set of tokens (such as a prompt
in NLP applications or a set of patches for a vision transformer). In previous
work, we studied the ability of these architectures to handle an arbitrarily
large number of context tokens. To mathematically, uniformly analyze their
expressivity, we considered the case that the mappings are conditioned on a
context represented by a probability distribution which becomes discrete for a
finite number of tokens. Modeling neural networks as maps on probability
measures has multiple applications, such as studying Wasserstein regularity,
proving generalization bounds and doing a mean-field limit analysis of the
dynamics of interacting particles as they go through the network. In this work,
we study the question what kind of maps between measures are transformers. We
fully characterize the properties of maps between measures that enable these to
be represented in terms of in-context maps via a push forward. On the one hand,
these include transformers; on the other hand, transformers universally
approximate representations with any continuous in-context map. These
properties are preserving the cardinality of support and that the regular part
of their Fr\'{e}chet derivative is uniformly continuous. Moreover, we show that
the solution map of the Vlasov equation, which is of nonlocal transport type,
for interacting particle systems in the mean-field regime for the Cauchy
problem satisfies the conditions on the one hand and, hence, can be
approximated by a transformer; on the other hand, we prove that the
measure-theoretic self-attention has the properties that ensure that the
infinite depth, mean-field measure-theoretic transformer can be identified with
a Vlasov flow.

</details>


### [19] [The Media Bias Detector: A Framework for Annotating and Analyzing the News at Scale](https://arxiv.org/abs/2509.25649)
*Samar Haider,Amir Tohidi,Jenny S. Wang,Timothy Dörr,David M. Rothschild,Chris Callison-Burch,Duncan J. Watts*

Main category: cs.CL

TL;DR: 本文介绍了一个由2024年1月开始持续更新的大规模近实时新闻数据集和计算框架，用于系统地研究新闻报道中的选择性和框架偏见，结合大语言模型和新闻抓取技术，分析政治倾向、语调、主题等多维度信息，并开放交互平台供研究和监督使用。


<details>
  <summary>Details</summary>
Motivation: 目前测量新闻媒体在报道选择和角度框架上的微妙偏见仍具挑战，缺乏大规模实时的系统性工具和数据支持。

Method: 结合大语言模型与近实时新闻抓取，自动从每日数百篇文章中提取结构化注释数据，包括政治倾向、语调、主题、文章类型和重大事件信息，从句子、文章和出版机构多个层面量化报道偏见。

Result: 构建了涵盖15万余篇2024年新闻文章的多维数据集，并开发了交互式网络平台，展示了该数据集如何揭示新闻报道中的偏见模式，支持学术研究和媒体监督。

Conclusion: 该方法论为大规模研究新闻媒体偏见提供了可重复、系统的工具和资源，有助于推动未来学术探索及提高媒体透明度和责任制。

Abstract: Mainstream news organizations shape public perception not only directly
through the articles they publish but also through the choices they make about
which topics to cover (or ignore) and how to frame the issues they do decide to
cover. However, measuring these subtle forms of media bias at scale remains a
challenge. Here, we introduce a large, ongoing (from January 1, 2024 to
present), near real-time dataset and computational framework developed to
enable systematic study of selection and framing bias in news coverage. Our
pipeline integrates large language models (LLMs) with scalable, near-real-time
news scraping to extract structured annotations -- including political lean,
tone, topics, article type, and major events -- across hundreds of articles per
day. We quantify these dimensions of coverage at multiple levels -- the
sentence level, the article level, and the publisher level -- expanding the
ways in which researchers can analyze media bias in the modern news landscape.
In addition to a curated dataset, we also release an interactive web platform
for convenient exploration of these data. Together, these contributions
establish a reusable methodology for studying media bias at scale, providing
empirical resources for future research. Leveraging the breadth of the corpus
over time and across publishers, we also present some examples (focused on the
150,000+ articles examined in 2024) that illustrate how this novel data set can
reveal insightful patterns in news coverage and bias, supporting academic
research and real-world efforts to improve media accountability.

</details>


### [20] [QFrBLiMP: a Quebec-French Benchmark of Linguistic Minimal Pairs](https://arxiv.org/abs/2509.25664)
*David Beauchemin,Pier-Luc Veilleux,Richard Khoury,Johanna-Pascale Roy*

Main category: cs.CL

TL;DR: 本文介绍了QFrBLiMP，这是一个用于评估大型语言模型在魁北克法语语法现象上的语言能力的语料库。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在特定语种魁北克法语的语法能力，尤其是在语义理解方面的表现。

Method: 构建包含1761个最小对的QFrBLiMP语料库，每对语句由魁北克法语母语者注释语法正确性，并用以评估多种大型语言模型的表现。

Result: 模型规模越大，语法能力越强，但所有模型在需要深层语义理解的现象上表现均较差，明显落后于人类。

Conclusion: 尽管模型在语法能力上有所提升，但在涉及深度语义理解的任务上存在显著缺陷，说明当前模型在复杂语言理解上仍有较大差距。

Abstract: In this paper, we introduce the Quebec-French Benchmark of Linguistic Minimal
Pairs (QFrBLiMP), a corpus designed to evaluate the linguistic knowledge of
LLMs on prominent grammatical phenomena in Quebec-French. QFrBLiMP consists of
1,761 minimal pairs annotated with 20 linguistic phenomena. Specifically, these
minimal pairs have been created by manually modifying sentences extracted from
an official online resource maintained by a Qu\'ebec government institution.
Each pair is annotated by twelve Quebec-French native speakers, who select the
sentence they feel is grammatical amongst the two. These annotations are used
to compare the competency of LLMs with that of humans. We evaluate different
LLMs on QFrBLiMP and MultiBLiMP-Fr by observing the rate of higher
probabilities assigned to the sentences of each minimal pair for each category.
We find that while grammatical competence scales with model size, a clear
hierarchy of difficulty emerges. All benchmarked models consistently fail on
phenomena requiring deep semantic understanding, revealing a critical
limitation and a significant gap compared to human performance on these
specific tasks.

</details>


### [21] [The Flaw of Averages: Quantifying Uniformity of Performance on Benchmarks](https://arxiv.org/abs/2509.25671)
*Arda Uzunoglu,Tianjian Li,Daniel Khashabi*

Main category: cs.CL

TL;DR: 本文提出了基准测试的和谐性指标，用于衡量模型在基准子领域上的表现均匀性，以保证评估的可靠性，避免整体准确率被某些子领域主导导致误导。


<details>
  <summary>Details</summary>
Motivation: 基准测试驱动模型发展，但不可靠的基准可能导致结论偏差，因此需要一种方法评估基准的可靠性。

Method: 从分布角度引入和谐性指标，衡量模型在基准子领域上的表现分布统一性，并基于多个基准和模型族进行平均-方差分析。

Result: 发现许多基准不够和谐，表现被少数子领域主导，导致评估结果可能误导，如ARC-Easy测试被生物学问题主导。

Conclusion: 建议在报告准确率的同时报告和谐性，使模型评估更全面可靠，促进基准设计和模型开发的科学进步。

Abstract: Benchmarks shape scientific conclusions about model capabilities and steer
model development. This creates a feedback loop: stronger benchmarks drive
better models, and better models demand more discriminative benchmarks.
Ensuring benchmark reliability is therefore essential for trustworthy
evaluation and meaningful progress. In this work, we study benchmark
reliability from a distributional perspective and introduce benchmark harmony,
which measures how uniformly a model's performance is distributed across the
subdomains of a benchmark. We posit that high harmony is a desirable benchmark
property, indicating that the aggregate metric reflects uniform competence
across subdomains. Across 19 multiple-choice benchmarks and five model
families, we map each benchmark onto a mean-variance plane of harmony computed
across models, where high mean and low variance signal more reliable
evaluation. Our analysis shows that less harmonious benchmarks can give
misleading results, since overall accuracy may be disproportionately influenced
by specific subdomains. For instance, ARC-Easy is overwhelmed by questions on
Biological Concepts, overshadowing other critical subdomains such as Geography,
Physics, Chemistry, and Environmental Science. By recommending that harmony
should be reported alongside accuracy, we reframe evaluation from simple
performance averages to a more robust, distributionally reliable measurement of
performance.

</details>


### [22] [Mitigating Biases in Language Models via Bias Unlearning](https://arxiv.org/abs/2509.25673)
*Dianqing Liu,Yi Liu,Guoqing Jin,Zhendong Mao*

Main category: cs.CL

TL;DR: 提出了BiasUnlearn框架，对语言模型中的偏见进行针对性消除，同时保留语言模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的参数修改和基于提示的去偏方法存在性能下降或去偏效果有限的问题，无法有效消除深层偏见。

Method: BiasUnlearn通过双路径遗忘机制协调刻板印象遗忘与反刻板印象保留，通过对抗遗忘集和动态数据集交换防止偏见极性反转。

Result: 多种语言模型和评测基准实验表明，BiasUnlearn优于现有方法，能有效减少偏见且保持模型能力。

Conclusion: BiasUnlearn有效缓解语言模型偏见，且去偏权重在不同模型间具有可迁移性，体现预训练阶段的偏见根深蒂固。

Abstract: Many studies have shown various biases targeting different demographic groups
in language models, amplifying discrimination and harming fairness. Recent
parameter modification debiasing approaches significantly degrade core
capabilities such as text coherence and task accuracy. And Prompt-based
debiasing methods, only effective for predefined trigger words, fail to address
deeply embedded stereotypical associations in model parameters. In this paper,
we propose BiasUnlearn, a novel model debiasing framework which achieves
targeted debiasing via dual-pathway unlearning mechanisms coordinating
stereotype forgetting with anti-stereotype retention, while preventing bias
polarity reversal through adversarial forget set and dynamic dataset swapping.
We conducted extensive experiments with multiple language models across various
evaluation benchmarks. The results show that BiasUnlearn outperforms existing
methods in mitigating bias in language models while retaining language modeling
capabilities. Further experiments reveal that debiasing weights are
transferable across model variants, confirming that bias representations become
entrenched during pre-training and persist through fine-tuning phases.

</details>


### [23] [LD-MoLE: Learnable Dynamic Routing for Mixture of LoRA Experts](https://arxiv.org/abs/2509.25684)
*Yuan Zhuang,Yi Shen,Yuexin Bian,Qing Su,Shihao Ji,Yuanyuan Shi,Fei Miao*

Main category: cs.CL

TL;DR: 本文提出了一种名为LD-MoLE的可学习动态路由机制，用于Mixture of LoRA Experts，实现了自适应、基于token和层级的专家分配，替代非可微分的TopK路由。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖固定参数的TopK路由，需调整超参数且为每个token分配固定数量专家，限制了模型灵活性和性能提升。

Method: 提出LD-MoLE，通过可微分的路由函数和闭式解替代传统TopK选择，实现对每个token和层的专家数动态分配，同时设计解析稀疏性控制目标以正则激活专家数。

Result: 在Qwen3-1.7B和Llama-3.2-3B模型的多任务测试中，LD-MoLE在多项基准测试中取得了优于现有最先进基线的平均分数，展示了优越性能和专家分配的动态适应能力。

Conclusion: LD-MoLE有效提升了大型语言模型在下游任务的适应能力，展示了动态且可学习的专家路由机制的优势，推动了PEFT与MoE方法的结合和应用。

Abstract: Recent studies have shown that combining parameter-efficient fine-tuning
(PEFT) with mixture-of-experts (MoE) is an effective strategy for adapting
large language models (LLMs) to the downstream tasks. However, most existing
approaches rely on conventional TopK routing, which requires careful
hyperparameter tuning and assigns a fixed number of experts to each token. In
this work, we propose LD-MoLE, a Learnable Dynamic routing mechanism for
Mixture of LoRA Experts that enables adaptive, token-dependent, and layer-wise
expert allocation. Our method replaces the non-differentiable TopK selection
with a differentiable routing function and a closed-form solution. Moreover,
our design allows the model to adaptively determine the number of experts to
activate for each token at different layers. In addition, we introduce an
analytical sparsity control objective to regularize the number of activated
experts. Extensive experiments on the Qwen3-1.7B and Llama-3.2-3B models show
that LD-MoLE achieves the highest average scores compared to state-of-the-art
baselines, across a diverse set of benchmarks. Our method not only achieves
superior performance, but also demonstrates the ability to learn
token-dependent and layer-wise expert allocation.

</details>


### [24] [Atomic Thinking of LLMs: Decoupling and Exploring Mathematical Reasoning Abilities](https://arxiv.org/abs/2509.25725)
*Jiayi Kuang,Haojing Huang,Yinghui Li,Xinnian Liang,Zhikun Xu,Yangning Li,Xiaoyu Tan,Chao Qu,Meishan Zhang,Ying Shen,Philip S. Yu*

Main category: cs.CL

TL;DR: 本文提出了一种评估大语言模型数学原子能力的新范式，强调分解复杂问题为基本能力单元，涵盖数学领域及逻辑层次的分类与评估。


<details>
  <summary>Details</summary>
Motivation: 当前大规模推理模型存在过度依赖海量训练数据和长推理链，难以验证模型是否真正掌握数学概念与推理原理。人类通过分解复杂问题为基本原子能力启发了该研究。

Method: 将数学能力分为四大领域（代数、几何、分析、拓扑）及三个逻辑层次（概念理解、正向多步骤推理、反例驱动反向推理），设计相应的训练和评估数据集，进行实验分析各原子能力及其相互影响。

Result: 高级模型在不同原子能力上表现差异明显，能力间存在复杂交互，揭示了模型认知的多样性和训练策略的潜力。

Conclusion: 将数学智能拆解为原子能力有助于理解模型认知机制，指导更高效、可迁移且符合认知的训练策略，推动“原子思维”范式的发展。

Abstract: Large Language Models (LLMs) have demonstrated outstanding performance in
mathematical reasoning capabilities. However, we argue that current large-scale
reasoning models primarily rely on scaling up training datasets with diverse
mathematical problems and long thinking chains, which raises questions about
whether LLMs genuinely acquire mathematical concepts and reasoning principles
or merely remember the training data. In contrast, humans tend to break down
complex problems into multiple fundamental atomic capabilities. Inspired by
this, we propose a new paradigm for evaluating mathematical atomic
capabilities. Our work categorizes atomic abilities into two dimensions: (1)
field-specific abilities across four major mathematical fields, algebra,
geometry, analysis, and topology, and (2) logical abilities at different
levels, including conceptual understanding, forward multi-step reasoning with
formal math language, and counterexample-driven backward reasoning. We propose
corresponding training and evaluation datasets for each atomic capability unit,
and conduct extensive experiments about how different atomic capabilities
influence others, to explore the strategies to elicit the required specific
atomic capability. Evaluation and experimental results on advanced models show
many interesting discoveries and inspirations about the different performances
of models on various atomic capabilities and the interactions between atomic
capabilities. Our findings highlight the importance of decoupling mathematical
intelligence into atomic components, providing new insights into model
cognition and guiding the development of training strategies toward a more
efficient, transferable, and cognitively grounded paradigm of "atomic
thinking".

</details>


### [25] [Controlled Generation for Private Synthetic Text](https://arxiv.org/abs/2509.25729)
*Zihao Zhao,Anjalie Field*

Main category: cs.CL

TL;DR: 本文提出了一种基于去识别化和HIPS理论的隐私保护合成文本生成方法，通过实体感知控制代码实现可控生成，在法律和临床数据集上表现出隐私保护与实用性的良好平衡。


<details>
  <summary>Details</summary>
Motivation: 在医疗、社会服务和法律等高风险领域，文本匿名化对于负责任地开发和部署AI至关重要，因此需要一种既能保护隐私又能保持文本实用性的合成文本生成方法。

Method: 提出了一种结合去识别化和HIPS理论的合成文本生成新方法，使用实体感知控制代码引导生成，采用上下文学习或前缀调优两种方式，其中前缀调优结合自定义掩码策略和损失函数以支持高质量和可扩展性生成。

Result: 在法律和临床数据集上的实验表明，该方法在保护隐私的同时保持了较高的文本生成质量，实现了隐私保护与实用性之间的良好平衡。

Conclusion: 该研究提供了一种切实有效的隐私保护合成文本生成方案，适用于需要高隐私保障的敏感领域，促进了AI在这些领域的安全应用。

Abstract: Text anonymization is essential for responsibly developing and deploying AI
in high-stakes domains such as healthcare, social services, and law. In this
work, we propose a novel methodology for privacy-preserving synthetic text
generation that leverages the principles of de-identification and the Hiding In
Plain Sight (HIPS) theory. Our approach introduces entity-aware control codes
to guide controllable generation using either in-context learning (ICL) or
prefix tuning. The ICL variant ensures privacy levels consistent with the
underlying de-identification system, while the prefix tuning variant
incorporates a custom masking strategy and loss function to support scalable,
high-quality generation. Experiments on legal and clinical datasets demonstrate
that our method achieves a strong balance between privacy protection and
utility, offering a practical and effective solution for synthetic text
generation in sensitive domains.

</details>


### [26] [CATCH: A Novel Data Synthesis Framework for High Therapy Fidelity and Memory-Driven Planning Chain of Thought in AI Counseling](https://arxiv.org/abs/2509.25733)
*Mingyu Chen,Jingkai Lin,Zhaojie Chu,Xiaofen Xing,Yirong Chen,Xiangmin Xu*

Main category: cs.CL

TL;DR: 本文提出了CATCH，一种用于生成高保真心理咨询对话的数据合成框架，通过阶段性对话合成和决策链条推理，显著提升了对话的真实性和逻辑连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的心理咨询研究多采用一次性生成方法，导致对话疗效真实性低，且无法展现每个回复背后的决策理由。

Method: 提出渐进式对话合成策略，从客户自述中提取目标、资源和解决方案，结构化生成多阶段对话；引入记忆驱动动态规划思维模式，结合记忆增强、全局规划和策略推理；使用多智能体协同优化器基于MDP为每轮对话附加思维链。

Result: 通过大量实验和人工评估，CATCH在提升心理咨询的疗效真实性和逻辑连贯性方面表现出显著优势。

Conclusion: CATCH有效解决了现有方法生成对话缺乏疗效真实性和决策透明度的问题，显著增强了AI心理咨询的质量和合理性。

Abstract: Recently, advancements in AI counseling based on large language models have
shown significant progress. However, existing studies employ a one-time
generation approach to synthesize multi-turn dialogue samples, resulting in low
therapy fidelity and failing to capture the decision-making rationale behind
each response. In this work, we propose CATCH, a novel data synthesis framework
designed to address these challenges. Specifically, to improve therapy
fidelity, we introduce the Progressive Dialogue Synthesis strategy, which
extracts goals, resources, and solutions from a client's self-report, organizes
them into structured outlines, and then incrementally generates stage-aligned
counseling dialogues. To capture decision-making rationale behind each
response, we propose the Memory-Driven Dynamic Planning thinking pattern that
integrates memory enhancement, global planning, and strategy reasoning; a
collaborative multi-agent optimizer then leverages MDP to attach explicit
chain-of-thought to each dialogue turn. Extensive experiments and human
evaluations demonstrate that CATCH significantly enhances fidelity and logical
coherence in AI counseling.

</details>


### [27] [Think Less, Label Better: Multi-Stage Domain-Grounded Synthetic Data Generation for Fine-Tuning Large Language Models in Telecommunications](https://arxiv.org/abs/2509.25736)
*Chenhua Shi,Gregor Macdonald,Bhavika Jalli,Wanlu Lei,John Zou,Mridul Jain,Joji Philip*

Main category: cs.CL

TL;DR: 本文提出了一种全自动检索增强生成电信专业领域问题答案对的流水线，减少人工标注，提升数据质量。


<details>
  <summary>Details</summary>
Motivation: 电信领域复杂任务生成高质量指令跟随和强化学习数据人工成本高且难度大，亟需自动化解决方案。

Method: 设计多阶段框架，结合检索器、基础生成器和优化模型，利用领域知识图谱文档生成并细化问答对，基于RAGAS评分筛选高质量样本。

Result: 在真实的电信无线接入网故障排查场景中，流水线成功生成复杂且上下文丰富的排障方案，无需人工干预。

Conclusion: 该自动化流水线为专业领域构建指令和强化数据集提供了可扩展、高技术保真度的解决方案，大幅降低对人工标注的依赖。

Abstract: The success of large language models (LLMs) depends heavily on large-scale,
high-quality instruction-following and reinforcement datasets. However,
generating such data through human annotation is prohibitively time-consuming
particularly for domain-specific tasks like telecom network troubleshooting,
where accurate responses require deep technical expertise and contextual
understanding. In this paper, we present a fully automated, retrieval-augmented
pipeline for generating synthetic question-answer (QA) pairs grounded in
structured domain knowledge. Our multi-stage framework integrates a retriever,
base generator, and refinement model to synthesize and enhance QA pairs using
documents retrieved from a domain-specific knowledge graph. To ensure data
quality, we employ customized RAGAS-based scoring to filter low-quality
samples, producing a high-quality dataset suitable for reinforcement
fine-tuning (RFT). We demonstrate our approach in a real-world telecom scenario
focused on radio access network (RAN) troubleshooting. The resulting pipeline
generates complex, context-rich troubleshooting solution plans without human
intervention. This work offers a scalable solution for building instruction and
reinforcement datasets in specialized domains, significantly reducing
dependence on manual labeling while maintaining high technical fidelity.

</details>


### [28] [Detecting Hope Across Languages: Multiclass Classification for Positive Online Discourse](https://arxiv.org/abs/2509.25752)
*T. O. Abiola,K. D. Abiodun,O. E. Olumide,O. O. Adebanji,O. Hiram Calvo,Grigori Sidorov*

Main category: cs.CL

TL;DR: 本文提出了一种基于XLM-RoBERTa的多语言多分类希望言论检测方法，在英语、乌尔都语和西班牙语的PolyHope数据集上表现优异，显著超过了现有技术。


<details>
  <summary>Details</summary>
Motivation: 促进积极的网络交流和用户福祉，需要自动识别社交媒体中的希望言论。

Method: 利用XLM-RoBERTa变换器模型对希望言论进行三类分类：广义希望、现实希望和非现实希望，覆盖多语言场景。

Result: 在PolyHope数据集上的多语言多分类任务中取得了先进的宏F1得分，优于现有方法。

Conclusion: 该方法有效提升了多语言精细化希望言论检测能力，有助于积极内容管理和支持性网络社区建设。

Abstract: The detection of hopeful speech in social media has emerged as a critical
task for promoting positive discourse and well-being. In this paper, we present
a machine learning approach to multiclass hope speech detection across multiple
languages, including English, Urdu, and Spanish. We leverage transformer-based
models, specifically XLM-RoBERTa, to detect and categorize hope speech into
three distinct classes: Generalized Hope, Realistic Hope, and Unrealistic Hope.
Our proposed methodology is evaluated on the PolyHope dataset for the
PolyHope-M 2025 shared task, achieving competitive performance across all
languages. We compare our results with existing models, demonstrating that our
approach significantly outperforms prior state-of-the-art techniques in terms
of macro F1 scores. We also discuss the challenges in detecting hope speech in
low-resource languages and the potential for improving generalization. This
work contributes to the development of multilingual, fine-grained hope speech
detection models, which can be applied to enhance positive content moderation
and foster supportive online communities.

</details>


### [29] [TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning](https://arxiv.org/abs/2509.25760)
*Zhepei Wei,Xiao Yang,Kai Sun,Jiaqi Wang,Rulin Shao,Sean Chen,Mohammad Kachuee,Teja Gollapudi,Tony Liao,Nicolas Scheffer,Rakesh Wanga,Anuj Kumar,Yu Meng,Wen-tau Yih,Xin Luna Dong*

Main category: cs.CL

TL;DR: 本文提出了TruthRL，一种通过强化学习优化大语言模型真实性的框架，有效减少幻觉和提升模型的真实性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在追求准确性时容易产生幻觉，而鼓励模型回避时又可能过于保守，二者都影响模型的真实性。

Method: 提出TruthRL框架，采用三元奖励机制区分正确回答、幻觉和回避，在强化学习中优化真实性。

Result: 在四个知识密集型基准测试中，TruthRL较传统强化学习减少幻觉28.9%，提升真实性21.1%，且在多个主干模型和不同设置下均表现优异。

Conclusion: 设计以真实性为目标的强化学习方法能有效平衡正确性与不确定性，促进真实可靠的大语言模型发展。

Abstract: While large language models (LLMs) have demonstrated strong performance on
factoid question answering, they are still prone to hallucination and
untruthful responses, particularly when tasks demand information outside their
parametric knowledge. Indeed, truthfulness requires more than accuracy --
models must also recognize uncertainty and abstain when unsure to avoid
hallucinations. This presents a fundamental challenge for existing methods:
approaches that optimize for accuracy often amplify hallucinations, while those
that encourage abstention can become overly conservative, sacrificing correct
answers. Both extremes ultimately compromise truthfulness. In this work, we
present TruthRL, a general reinforcement learning (RL) framework that directly
optimizes the truthfulness of LLMs. Specifically, we implement TruthRL using
GRPO with a simple yet effective ternary reward that distinguishes correct
answers, hallucinations, and abstentions. It incentivizes models to reduce
hallucinations not only by providing correct responses, but also by enabling
abstention when uncertain, thereby improving truthfulness. Extensive
experiments across four knowledge-intensive benchmarks show that, compared to
vanilla RL, TruthRL significantly reduces hallucinations by 28.9% and improves
truthfulness by 21.1%, with consistent gains across various backbone models
(e.g., Qwen, Llama) under both retrieval and non-retrieval setups. In-depth
ablation study demonstrates that vanilla accuracy-driven methods, such as
supervised fine-tuning or RL with a binary reward, struggle to balance factual
correctness and uncertainty. In contrast, our proposed truthfulness-driven
TruthRL achieves strong performance in both accuracy and truthfulness,
underscoring the importance of learning objective design for developing
truthful LLMs.

</details>


### [30] [Assessing Algorithmic Bias in Language-Based Depression Detection: A Comparison of DNN and LLM Approaches](https://arxiv.org/abs/2509.25795)
*Obed Junias,Prajakta Kini,Theodora Chaspari*

Main category: cs.CL

TL;DR: 本文研究了针对抑郁症自动检测的语言模型中存在的算法偏见，重点分析了性别和种族/族裔相关的社会人口学差异。


<details>
  <summary>Details</summary>
Motivation: 现有的抑郁症检测模型在不同社会人口学群体中的表现存在不公，尤其是性别和种族维度，因此需要研究并缓解这些偏见以提高模型的公平性。

Method: 本文比较了基于深度神经网络（DNN）嵌入和大规模语言模型（LLMs）的少样本学习方法，在DAIC-WOZ临床访谈语料上的表现和公平性。采用公平感知损失函数缓解DNN模型偏见，探索不同提示框架和示例数量对LLM的影响。

Result: LLM模型在抑郁症分类中整体优于DNN，尤其对欠代表的西班牙裔群体表现更好。LLM减少了性别偏见，但种族差异依旧存在。DNN中采用的最差群体损失在性能与公平性之间取得较好平衡，公平正则化损失效果较差。LLM的伦理引导提示在单样本设置下缓解了性别偏见，但增加样本未进一步减少偏差，且无有效策略减轻种族偏差。

Conclusion: 大规模语言模型在自动抑郁症检测中的表现优于传统DNN，且能够部分缓解性别偏见，但种族偏差依然是挑战。公平感知损失函数是改善DNN表现的有效手段，提示设计对LLM公平性有一定影响但有限。未来工作需进一步针对种族公平性开展研究。

Abstract: This paper investigates algorithmic bias in language-based models for
automated depression detection, focusing on socio-demographic disparities
related to gender and race/ethnicity. Models trained using deep neural networks
(DNN) based embeddings are compared to few-shot learning approaches with large
language models (LLMs), evaluating both performance and fairness on clinical
interview transcripts from the Distress Analysis Interview Corpus/Wizard-of-Oz
(DAIC-WOZ). To mitigate bias, fairness-aware loss functions are applied to
DNN-based models, while in-context learning with varied prompt framing and shot
counts is explored for LLMs. Results indicate that LLMs outperform DNN-based
models in depression classification, particularly for underrepresented groups
such as Hispanic participants. LLMs also exhibit reduced gender bias compared
to DNN-based embeddings, though racial disparities persist. Among
fairness-aware techniques for mitigating bias in DNN-based embeddings, the
worst-group loss, which is designed to minimize loss for the worst-performing
demographic group, achieves a better balance between performance and fairness.
In contrast, the fairness-regularized loss minimizes loss across all groups but
performs less effectively. In LLMs, guided prompting with ethical framing helps
mitigate gender bias in the 1-shot setting. However, increasing the number of
shots does not lead to further reductions in disparities. For race/ethnicity,
neither prompting strategy nor increasing $N$ in $N$-shot learning effectively
reduces disparities.

</details>


### [31] [RoBiologyDataChoiceQA: A Romanian Dataset for improving Biology understanding of Large Language Models](https://arxiv.org/abs/2509.25813)
*Dragos-Dumitru Ghinea,Adela-Nicoleta Corbeanu,Adrian-Marius Dumitran*

Main category: cs.CL

TL;DR: 本文介绍了一个针对罗马尼亚语多项选择生物学问题的新数据集，并评估了大型语言模型(LLMs)在科学领域的理解和推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在领域特定应用和非英语语言中的表现尚未充分研究，尤其是在生物学领域的罗马尼亚语资源较为缺乏。

Method: 构建了一个包含约14,000个问题的罗马尼亚语生物学多项选择题数据集，并对多种流行的大型语言模型进行了准确率、推理模式和专业术语理解能力的评测，同时测试了提示工程和微调等优化技术的效果。

Result: 研究揭示了当前大型语言模型在处理专业知识任务和低资源语言时的优缺点，模型在领域知识理解和语言细节把握上表现参差不齐。

Conclusion: 该研究为未来针对低资源语言和专业领域的模型改进提供了有价值的见解，推动了大型语言模型在科学领域应用的深入研究。

Abstract: In recent years, large language models (LLMs) have demonstrated significant
potential across various natural language processing (NLP) tasks. However,
their performance in domain-specific applications and non-English languages
remains less explored. This study introduces a novel Romanian-language dataset
for multiple-choice biology questions, carefully curated to assess LLM
comprehension and reasoning capabilities in scientific contexts. Containing
approximately 14,000 questions, the dataset provides a comprehensive resource
for evaluating and improving LLM performance in biology.
  We benchmark several popular LLMs, analyzing their accuracy, reasoning
patterns, and ability to understand domain-specific terminology and linguistic
nuances. Additionally, we perform comprehensive experiments to evaluate the
impact of prompt engineering, fine-tuning, and other optimization techniques on
model performance. Our findings highlight both the strengths and limitations of
current LLMs in handling specialized knowledge tasks in low-resource languages,
offering valuable insights for future research and development.

</details>


### [32] [ReTAG: Retrieval-Enhanced, Topic-Augmented Graph-Based Global Sensemaking](https://arxiv.org/abs/2509.25814)
*Boyoung Kim,Dosung Lee,Sumin An,Jinseong Jeong,Paul Hongsuck Seo*

Main category: cs.CL

TL;DR: 本文提出了一种名为ReTAG的检索增强主题图框架，解决了现有图模型在全局语义理解中的检索不足、主题不明确及推理成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 现有用于全局语义理解的图模型缺乏有效的检索机制、主题特异性差，且推理成本高，难以准确合成全篇信息进行问答。

Method: 提出ReTAG，通过构建主题特定的子图和检索相关摘要，实现增强的检索能力和主题聚焦，从而优化响应生成过程。

Result: 实验表明，ReTAG在提升回应质量的同时显著降低了推理时间，表现优于基线方法。

Conclusion: ReTAG框架有效解决了全局语义理解中的关键痛点，提高了问答性能和效率。

Abstract: Recent advances in question answering have led to substantial progress in
tasks such as multi-hop reasoning. However, global sensemaking-answering
questions by synthesizing information from an entire corpus remains a
significant challenge. A prior graph-based approach to global sensemaking lacks
retrieval mechanisms, topic specificity, and incurs high inference costs. To
address these limitations, we propose ReTAG, a Retrieval-Enhanced,
Topic-Augmented Graph framework that constructs topic-specific subgraphs and
retrieves the relevant summaries for response generation. Experiments show that
ReTAG improves response quality while significantly reducing inference time
compared to the baseline. Our code is available at
https://github.com/bykimby/retag.

</details>


### [33] [Personalized Scientific Figure Caption Generation: An Empirical Study on Author-Specific Writing Style Transfer](https://arxiv.org/abs/2509.25817)
*Jaeyoung Kim,Jongho Lee,Hongjun Choi,Sion Jang*

Main category: cs.CL

TL;DR: 利用科学论文中的作者资料数据，改进个性化图表说明生成，提升多模态大语言模型的效果，但存在风格匹配与说明质量的权衡。


<details>
  <summary>Details</summary>
Motivation: 通过利用作者的丰富资料数据和相关元数据，提高图表说明的个性化水平。

Method: 结合作者个人资料和相关元数据，应用于多模态大语言模型的训练与生成。

Result: 显著提升了图表说明的个性化性能，同时发现风格匹配与说明质量之间存在权衡。

Conclusion: 提出了在保持说明质量的同时匹配作者风格的挑战，为实践中的图表自动化系统提供了有价值的见解和未来研究方向。

Abstract: We study personalized figure caption generation using author profile data
from scientific papers. Our experiments demonstrate that rich author profile
data, combined with relevant metadata, can significantly improve the
personalization performance of multimodal large language models. However, we
also reveal a fundamental trade-off between matching author style and
maintaining caption quality. Our findings offer valuable insights and future
directions for developing practical caption automation systems that balance
both objectives. This work was conducted as part of the 3rd SciCap challenge.

</details>


### [34] [Overthinking Reduction with Decoupled Rewards and Curriculum Data Scheduling](https://arxiv.org/abs/2509.25827)
*Shuyang Jiang,Yusheng Liao,Ya Zhang,Yanfeng Wang,Yu Wang*

Main category: cs.CL

TL;DR: 提出了DECS框架，通过解耦的token级奖励机制和课程批次调度策略，显著减少推理路径长度超过50%，同时保持或提升性能，解决了大型推理模型中的“过度思考”问题。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型存在“过度思考”问题，即生成过长的推理路径无性能提升，且现有惩罚长度的方法导致性能下降，原因在于轨迹级奖励与token级优化的不匹配。

Method: 提出DECS框架，包含（1）解耦的token级奖励机制，精准区分并惩罚冗余token，避免误罚必要的探索token；（2）课程批次调度策略，平衡效率和效能。

Result: 在七个基准测试中，DECS将推理token数量减少超过50%，且性能未下降甚至有所提升。

Conclusion: DECS框架有效解决了推理路径过长问题，实现了推理效率的大幅提升，同时保持了模型的推理能力。 

Abstract: While large reasoning models trained with critic-free reinforcement learning
and verifiable rewards (RLVR) represent the state-of-the-art, their practical
utility is hampered by ``overthinking'', a critical issue where models generate
excessively long reasoning paths without any performance benefit. Existing
solutions that penalize length often fail, inducing performance degradation due
to a fundamental misalignment between trajectory-level rewards and token-level
optimization. In this work, we introduce a novel framework, DECS, built on our
theoretical discovery of two previously unaddressed flaws in current length
rewards: (1) the erroneous penalization of essential exploratory tokens and (2)
the inadvertent rewarding of partial redundancy. Our framework's innovations
include (i) a first-of-its-kind decoupled token-level reward mechanism that
surgically distinguishes and penalizes redundant tokens, and (ii) a novel
curriculum batch scheduling strategy to master the efficiency-efficacy
equilibrium. Experimental results show DECS can achieve a dramatic reduction in
reasoning tokens by over 50\% across seven benchmarks while simultaneously
maintaining or even improving performance. It demonstrates conclusively that
substantial gains in reasoning efficiency can be achieved without compromising
a model's underlying reasoning power.

</details>


### [35] [Believing without Seeing: Quality Scores for Contextualizing Vision-Language Model Explanations](https://arxiv.org/abs/2509.25844)
*Keyu He,Tejas Srinivasan,Brihi Joshi,Xiang Ren,Jesse Thomason,Swabha Swayamdipta*

Main category: cs.CL

TL;DR: 该论文针对盲人或低视力用户无法看到视觉内容时，如何通过对视觉-语言模型（VLM）生成的解释进行质量评估，帮助用户辨别模型预测的可靠性的问题进行了研究。


<details>
  <summary>Details</summary>
Motivation: 当前VLM的自然语言解释可能误导用户，令他们错误相信不准确的模型预测。为避免用户过度依赖不准确的预测，亟需有效的解释质量评估方法。

Method: 提出两种评分函数评价解释质量：视觉忠实度（Visual Fidelity）衡量解释对视觉内容的真实性，区分度（Contrastiveness）衡量解释识别预测与其他可能选项的差异能力。并在A-OKVQA和VizWiz数据集上验证质量评分与模型正确性的相关性。开展用户研究，展示评分对提升用户判断模型预测准确性的效果。

Result: 质量评分函数更好地与模型正确性匹配，用户在无视觉内容情况下判断模型预测准确性时，显示评分能提升准确率11.1%，并减少15.4%的错误信任。

Conclusion: 解释质量评分能够有效帮助用户更准确评估视觉-语言模型预测的可靠性，从而促进适当依赖模型预测，避免过度依赖错误预测。

Abstract: When people query Vision-Language Models (VLMs) but cannot see the
accompanying visual context (e.g. for blind and low-vision users), augmenting
VLM predictions with natural language explanations can signal which model
predictions are reliable. However, prior work has found that explanations can
easily convince users that inaccurate VLM predictions are correct. To remedy
undesirable overreliance on VLM predictions, we propose evaluating two
complementary qualities of VLM-generated explanations via two quality scoring
functions. We propose Visual Fidelity, which captures how faithful an
explanation is to the visual context, and Contrastiveness, which captures how
well the explanation identifies visual details that distinguish the model's
prediction from plausible alternatives. On the A-OKVQA and VizWiz tasks, these
quality scoring functions are better calibrated with model correctness than
existing explanation qualities. We conduct a user study in which participants
have to decide whether a VLM prediction is accurate without viewing its visual
context. We observe that showing our quality scores alongside VLM explanations
improves participants' accuracy at predicting VLM correctness by 11.1%,
including a 15.4% reduction in the rate of falsely believing incorrect
predictions. These findings highlight the utility of explanation quality scores
in fostering appropriate reliance on VLM predictions.

</details>


### [36] [ReFACT: A Benchmark for Scientific Confabulation Detection with Positional Error Annotations](https://arxiv.org/abs/2509.25868)
*Yindong Wang,Martin Preiß,Margarita Bugueño,Jan Vincent Hoffbauer,Abdullatif Ghajar,Tolga Buz,Gerard de Melo*

Main category: cs.CL

TL;DR: 该论文提出了ReFACT基准数据集，用于科学领域中对大语言模型虚假信息的精细评估。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在科学事实方面经常出现捏造，降低了它们的可信度，因此需要超越简单事实判断的细粒度评估基准。

Method: 构建了包含1001个专家注释的问题-答案对的数据集，其中每个实例包含正确答案和不实答案，并详细标注错误位置和类型，实现多阶段评估（检测、定位、纠正）。

Result: 在对9个先进大语言模型的测试中表现有限，准确率约50%，甚至GPT-4o也难以区分真实与虚假的科学回答。

Conclusion: 研究表明当前模型在科学虚假信息检测上的不足，强调了需要细粒度、人工验证的基准数据集以提升科学领域中大语言模型的可靠性。

Abstract: Large Language Models (LLMs) frequently confabulate scientific facts,severely
undermining their trustworthiness. Addressing this challenge requires
benchmarks that go beyond binary factuality and enable fine-grained evaluation.
We introduce \textbf{ReFACT} (\textit{Reddit False And Correct Texts}), a
benchmark of 1,001 expert-annotated question--answer pairs spanning diverse
scientific domains for the detection of scientific confabulation. Each instance
includes both a scientifically correct answer and a non-factual counterpart
annotated with \textbf{precise error spans and error-types}. ReFACT enables
multi-stage evaluation: (1) confabulation detection, (2) fine-grained error
localization, and (3) correction. We benchmark 9 state-of-the-art LLMs,
revealing limited performance ($\sim$50\% accuracy). Even top models such as
GPT-4o fail to distinguish factual from confabulated scientific answers,
raising concerns about the reliability of \textit{LLM-as-judge} evaluation
paradigms. Our findings highlight the need for fine-grained, human-validated
benchmarks to detect and correct scientific confabulation in domain-specific
contexts. Dataset is released on
\href{https://github.com/ddz5431/ReFACT}{GitHub}\footnote{We provide the
dataset at: https://github.com/ddz5431/ReFACT}.

</details>


### [37] [ASR Under Noise: Exploring Robustness for Sundanese and Javanese](https://arxiv.org/abs/2509.25878)
*Salsabila Zahirah Pranida,Muhammad Cendekia Airlangga,Rifo Ahmad Genadi,Shady Shehata*

Main category: cs.CL

TL;DR: 本文研究了基于Whisper的自动语音识别模型在爪哇语和巽他语两种印度尼西亚地区语言上的鲁棒性，尤其是在嘈杂环境下的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管现有工作已展示了ASR模型在干净语音条件下的优异性能，但其在噪声环境中的效果仍不明确，因此有必要进行针对性评估和改进。

Method: 采用合成噪声增强和SpecAugment等多种训练策略，在不同信噪比条件下评估模型表现，特别对较大规模的Whisper模型进行噪声感知训练。

Result: 噪声感知训练显著提升了模型的鲁棒性，尤其是大型模型，并通过详细的错误分析揭示了针对不同语言的特定挑战。

Conclusion: 噪声增强训练是提升区域语言ASR模型在复杂环境下性能的有效手段，未来可以针对语言特有问题进一步优化模型。

Abstract: We investigate the robustness of Whisper-based automatic speech recognition
(ASR) models for two major Indonesian regional languages: Javanese and
Sundanese. While recent work has demonstrated strong ASR performance under
clean conditions, their effectiveness in noisy environments remains unclear. To
address this, we experiment with multiple training strategies, including
synthetic noise augmentation and SpecAugment, and evaluate performance across a
range of signal-to-noise ratios (SNRs). Our results show that noise-aware
training substantially improves robustness, particularly for larger Whisper
models. A detailed error analysis further reveals language-specific challenges,
highlighting avenues for future improvements

</details>


### [38] [RoleConflictBench: A Benchmark of Role Conflict Scenarios for Evaluating LLMs' Contextual Sensitivity](https://arxiv.org/abs/2509.25897)
*Jisu Shin,Hoyun Song,Juhyun Oh,Changgeon Ko,Eunsu Kim,Chani Jung,Alice Oh*

Main category: cs.CL

TL;DR: 该论文提出了RoleConflictBench基准，评估大型语言模型（LLMs）在复杂社会角色冲突中的情境敏感性，发现目前模型在处理多重角色期待冲突时表现不足，且存在明显的社会角色偏见。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在决策中的影响力日益增加，理解它们在复杂社会困境中（尤其是角色冲突这种模糊的社会困境）表现至关重要，而此前研究多关注有明确正确答案的情境，缺乏对情境敏感性的系统评估。

Method: 构建RoleConflictBench基准，设计三阶段流程生成13K+现实角色冲突场景，涵盖65种角色，系统变换角色责任、义务及情境紧急程度，并通过分析10种LLMs的选择行为来评估其情境敏感能力。

Result: 实验显示，尽管LLMs在某种程度上能回应上下文线索，但整体情境敏感性不足；模型决策更多受固有的社会角色偏见驱动，特别偏爱家庭和职业领域角色、男性角色及亚伯拉罕宗教相关角色。

Conclusion: 现有LLMs在角色冲突社会困境中尚不能充分体现情境敏感性，存在明显社会角色及性别偏见，未来提升模型对复杂社交环境的理解尤为重要。

Abstract: Humans often encounter role conflicts -- social dilemmas where the
expectations of multiple roles clash and cannot be simultaneously fulfilled. As
large language models (LLMs) become increasingly influential in human
decision-making, understanding how they behave in complex social situations is
essential. While previous research has evaluated LLMs' social abilities in
contexts with predefined correct answers, role conflicts represent inherently
ambiguous social dilemmas that require contextual sensitivity: the ability to
recognize and appropriately weigh situational cues that can fundamentally alter
decision priorities. To address this gap, we introduce RoleConflictBench, a
novel benchmark designed to evaluate LLMs' contextual sensitivity in complex
social dilemmas. Our benchmark employs a three-stage pipeline to generate over
13K realistic role conflict scenarios across 65 roles, systematically varying
their associated expectations (i.e., their responsibilities and obligations)
and situational urgency levels. By analyzing model choices across 10 different
LLMs, we find that while LLMs show some capacity to respond to these contextual
cues, this sensitivity is insufficient. Instead, their decisions are
predominantly governed by a powerful, inherent bias related to social roles
rather than situational information. Our analysis quantifies these biases,
revealing a dominant preference for roles within the Family and Occupation
domains, as well as a clear prioritization of male roles and Abrahamic
religions across most evaluatee models.

</details>


### [39] [PerQ: Efficient Evaluation of Multilingual Text Personalization Quality](https://arxiv.org/abs/2509.25903)
*Dominik Macko,Andrew Pulver*

Main category: cs.CL

TL;DR: 本文提出了一种名为PerQ的高效个性化质量评价指标，用于评估文本个性化质量，降低了多模型联合评价的成本。


<details>
  <summary>Details</summary>
Motivation: 现有缺乏评价文本个性化质量的指标，依赖大型语言模型元评价存在偏见且成本高。

Method: 提出PerQ方法，计算效率高，可有效评估生成文本的个性化质量。

Result: 通过个案研究对比大小型语言模型生成能力，验证了PerQ指标的实用性和资源节约效果。

Conclusion: PerQ指标为文本个性化质量评价提供了高效方案，降低了元评价的资源浪费。

Abstract: Since no metrics are available to evaluate specific aspects of a text, such
as its personalization quality, the researchers often rely solely on large
language models to meta-evaluate such texts. Due to internal biases of
individual language models, it is recommended to use multiple of them for
combined evaluation, which directly increases costs of such meta-evaluation. In
this paper, a computationally efficient method for evaluation of
personalization quality of a given text (generated by a language model) is
introduced, called PerQ. A case study of comparison of generation capabilities
of large and small language models shows the usability of the proposed metric
in research, effectively reducing the waste of resources.

</details>


### [40] [Regression Language Models for Code](https://arxiv.org/abs/2509.26476)
*Yash Akhauri,Xingyou Song,Arissa Wongpanich,Bryan Lewandowski,Mohamed S. Abdelfattah*

Main category: cs.CL

TL;DR: 本文提出了一种统一的回归语言模型（RLM），能直接从代码文本预测多种编程语言的运行指标，如内存占用、延迟及神经网络性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖繁重且领域特定的特征工程，难以有效泛化多语言及多任务的性能预测。

Method: 基于少量参数的回归语言模型（RLM），并以T5Gemma预训练模型初始化，实现统一处理多种语言和任务的数值预测。

Result: 300M参数的RLM在多语言代码预测任务中取得超过0.9的Spearman相关系数，统一模型在17种语言上平均超过0.5；在NAS设计空间的排名相关性指标（Kendall-Tau）达0.46，优于图神经网络方法。

Conclusion: 该模型证明了通过统一回归语言模型直接从代码文本预测多种性能指标的有效性，展现出强大的多任务多语言泛化能力。

Abstract: We study code-to-metric regression: predicting numeric outcomes of code
executions, a challenging task due to the open-ended nature of programming
languages. While prior methods have resorted to heavy and domain-specific
feature engineering, we show that a single unified Regression Language Model
(RLM) can simultaneously predict directly from text, (i) the memory footprint
of code across multiple high-level languages such as Python and C++, (ii) the
latency of Triton GPU kernels, and (iii) the accuracy and speed of trained
neural networks represented in ONNX. In particular, a relatively small 300M
parameter RLM initialized from T5Gemma, obtains > 0.9 Spearman-rank on
competitive programming submissions from APPS, and a single unified model
achieves > 0.5 average Spearman-rank across 17 separate languages from CodeNet.
Furthermore, the RLM can obtain the highest average Kendall-Tau of 0.46 on five
classic NAS design spaces previously dominated by graph neural networks, and
simultaneously predict architecture latencies on numerous hardware platforms.

</details>


### [41] [Mem-α: Learning Memory Construction via Reinforcement Learning](https://arxiv.org/abs/2509.25911)
*Yu Wang,Ryuichi Takanobu,Zhiqi Liang,Yuzhen Mao,Yuanzhe Hu,Julian McAuley,Xiaojian Wu*

Main category: cs.CL

TL;DR: 本文提出了Mem-alpha，一个利用强化学习训练语言模型代理有效管理复杂记忆系统的框架，显著提升记忆构建能力和信息保存效果，且具有强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型受限于上下文窗口大小，现有的记忆增强代理依赖预定义指令与工具，缺乏自主判断何时及如何更新记忆，导致记忆构建不佳和信息丢失。

Method: 设计了基于强化学习的Mem-alpha框架，通过交互反馈训练代理处理和存储相关信息，利用训练数据集包含多轮交互并结合问答准确率作为奖励信号，优化记忆架构；提出包含核心、事件和语义的复杂多功能记忆体系。

Result: 实验表明Mem-alpha在现有记忆增强代理中表现优异，训练时最大输入30k tokens，测试时能有效处理超过400k tokens的序列，展现出强大泛化能力和鲁棒性。

Conclusion: Mem-alpha通过强化学习实现了语言模型对复杂记忆系统的自主高效管理，显著提升了记忆构建质量与长期信息理解能力，具备出色的泛化潜力，推动了语言模型在长文本场景中的应用。

Abstract: Large language model (LLM) agents are constrained by limited context windows,
necessitating external memory systems for long-term information understanding.
Current memory-augmented agents typically depend on pre-defined instructions
and tools for memory updates. However, language models may lack the ability to
determine which information to store, how to structure it, and when to update
it, especially as memory systems become more complex. This results in
suboptimal memory construction and information loss. To this end, we propose
Mem-alpha, a reinforcement learning framework that trains agents to effectively
manage complex memory systems through interaction and feedback. We also
construct a specialized training dataset spanning diverse multi-turn
interaction patterns paired with comprehensive evaluation questions designed to
teach effective memory management. During training, agents process sequential
information chunks, learn to extract and store relevant content, then update
the memory system. The reward signal derives from downstream question-answering
accuracy over the full interaction history, directly optimizing for memory
construction. To illustrate the effectiveness of our training framework, we
design a memory architecture comprising core, episodic, and semantic
components, equipped with multiple tools for memory operations. Empirical
evaluation demonstrates that Mem-alpha achieves significant improvements over
existing memory-augmented agent baselines. Despite being trained exclusively on
instances with a maximum length of 30k tokens, our agents exhibit remarkable
generalization to sequences exceeding 400k tokens, over 13x the training
length, highlighting the robustness of Mem-alpha.

</details>


### [42] [Understanding the Mixture-of-Experts with Nadaraya-Watson Kernel](https://arxiv.org/abs/2509.25913)
*Chuanyang Zheng,Jiankai Sun,Yihang Gao,Enze Xie,Yuehao Wang,Peihao Wang,Ting Xu,Matthew Chang,Liliang Ren,Jingyao Li,Jing Xiong,Kashif Rasul,Mac Schwager,Anderson Schneider,Zhangyang Wang,Yuriy Nevmyvaka*

Main category: cs.CL

TL;DR: 该论文提出一种基于核回归理论的替代路由函数KERN，用于混合专家模型（MoE），并通过实验证明其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统MoE模型普遍使用Softmax函数作为专家路由权重的映射，但这一设计缺乏理论挑战和更优方案的探索。作者发现MoE与Nadaraya-Watson回归具有数学等价性，启发设计更合理的路由函数。

Method: 提出了KERN路由器，它基于核回归的思路，采用ReLU激活与L2归一化，实现了类似FFN的路由机制，泛化了传统的Sigmoid和Softmax路由函数。

Result: 通过在MoE和大型语言模型上的广泛实验，验证了KERN路由器的有效性和优越性，表现出更好的路由性能和模型效果。

Conclusion: KERN作为一种零额外成本的FFN风格路由替代方案，是Softmax路由的一种更合理与高效的选择，推荐在MoE架构中使用。

Abstract: Mixture-of-Experts (MoE) has become a cornerstone in recent state-of-the-art
large language models (LLMs). Traditionally, MoE relies on $\mathrm{Softmax}$
as the router score function to aggregate expert output, a designed choice that
has persisted from the earliest MoE models to modern LLMs, and is now widely
regarded as standard practice. However, the necessity of using
$\mathrm{Softmax}$ to project router weights into a probability simplex remains
an unchallenged assumption rather than a principled design choice. In this
work, we first revisit the classical Nadaraya-Watson regression and observe
that MoE shares the same mathematical formulation as Nadaraya-Watson
regression. Furthermore, we show that both feed-forward neural network (FFN)
and MoE can be interpreted as a special case of Nadaraya-Watson regression,
where the kernel function corresponds to the input neurons of the output layer.
Motivated by these insights, we propose the \textbf{zero-additional-cost}
Kernel Inspired Router with Normalization (KERN), an FFN-style router function,
as an alternative to $\mathrm{Softmax}$. We demonstrate that this router
generalizes both $\mathrm{Sigmoid}$- and $\mathrm{Softmax}$-based routers.
\textbf{Based on empirical observations and established practices in FFN
implementation, we recommend the use of $\mathrm{ReLU}$ activation and
$\ell_2$-normalization in $\mathrm{KERN}$ router function.} Comprehensive
experiments in MoE and LLM validate the effectiveness of the proposed FFN-style
router function \methodNorm.

</details>


### [43] [Bringing Emerging Architectures to Sequence Labeling in NLP](https://arxiv.org/abs/2509.25918)
*Ana Ezquerro,Carlos Gómez-Rodríguez,David Vilares*

Main category: cs.CL

TL;DR: 本文分析了几种替代Transformer的序列标注模型在多语言、多任务上的表现，发现其在复杂结构任务中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 研究现有Transformer编码器的替代架构（如xLSTMs、结构化状态空间模型、扩散模型和对抗学习）在序列标注任务中的适应性，特别是在结构复杂度和语言多样性上的普适性。

Method: 采用多种架构在多语言、多样化标签空间和不同依赖程度的序列标注任务中进行评估，比较它们在不同任务和语言上的表现差异。

Result: 这些替代架构在简单设置中表现良好，但其强性能无法有效推广到更多语言、多数据集以及结构更复杂的标注任务中。

Conclusion: 目前替代Transformer的架构尚未能稳定超越Transformer在序列标注中的表现，尤其是在复杂和多语言环境下仍需进一步研究和提升。

Abstract: Pretrained Transformer encoders are the dominant approach to sequence
labeling. While some alternative architectures-such as xLSTMs, structured
state-space models, diffusion models, and adversarial learning-have shown
promise in language modeling, few have been applied to sequence labeling, and
mostly on flat or simplified tasks. We study how these architectures adapt
across tagging tasks that vary in structural complexity, label space, and token
dependencies, with evaluation spanning multiple languages. We find that the
strong performance previously observed in simpler settings does not always
generalize well across languages or datasets, nor does it extend to more
complex structured tasks.

</details>


### [44] [Reliability Crisis of Reference-free Metrics for Grammatical Error Correction](https://arxiv.org/abs/2509.25961)
*Takumi Goto,Yusuke Sakai,Taro Watanabe*

Main category: cs.CL

TL;DR: 本文针对四个无参考的语法错误纠正评价指标提出了对抗攻击策略，揭示了现有指标易被攻击系统利用，降低自动评价的可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前无参考评价指标虽与人工判断相关性高，但未针对旨在获取不合理高分的对抗系统设计，存在被利用风险，影响系统选择。

Method: 本文设计了针对SOME、Scribendi、IMPARA和基于大语言模型的四种无参考评价指标的对抗攻击策略，以验证其鲁棒性。

Result: 实验表明，所提对抗系统在攻击效果上超越了现有最先进的系统，展示了现有评价指标的脆弱性。

Conclusion: 研究揭示现有无参考语法纠正评价指标在面对对抗攻击时的不足，强调开发更鲁棒评价方法的迫切性。

Abstract: Reference-free evaluation metrics for grammatical error correction (GEC) have
achieved high correlation with human judgments. However, these metrics are not
designed to evaluate adversarial systems that aim to obtain unjustifiably high
scores. The existence of such systems undermines the reliability of automatic
evaluation, as it can mislead users in selecting appropriate GEC systems. In
this study, we propose adversarial attack strategies for four reference-free
metrics: SOME, Scribendi, IMPARA, and LLM-based metrics, and demonstrate that
our adversarial systems outperform the current state-of-the-art. These findings
highlight the need for more robust evaluation methods.

</details>


### [45] [RAGferee: Building Contextual Reward Models for Retrieval-Augmented Generation](https://arxiv.org/abs/2509.26011)
*Andrei C. Coman,Ionut-Teodor Sorodoc,Leonardo F. R. Ribeiro,Bill Byrne,James Henderson,Adrià de Gispert*

Main category: cs.CL

TL;DR: 本文提出了专门用于检索增强生成(RAG)任务的奖励模型训练方法RAGferee，利用QA数据集构造偏好对，并在小规模数据上训练出性能优异的RAG奖励模型，显著提升了上下文判断能力。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型在RAG场景下难以有效评估回答的真实性、相关性及完整性，且缺乏公开的RAG偏好数据集和专用奖励模型。

Method: 提出RAGferee方法，将问答数据转化为强调内容真实性的偏好对，用以训练适配RAG任务的上下文奖励模型；并基于4千样本小规模数据进行了模型微调。

Result: 所训练的RAG奖励模型在ContextualJudgeBench测试中超过了使用更大规模一般语料训练的70B+参数模型，表现提升15.5%。

Conclusion: RAGferee有效解决了RAG任务中奖励模型训练数据不足的问题，提升了模型评估检索增强生成回答的能力，证明了小规模专用数据训练奖励模型的潜力。

Abstract: Existing Reward Models (RMs), typically trained on general preference data,
struggle in Retrieval Augmented Generation (RAG) settings, which require
judging responses for faithfulness to retrieved context, relevance to the user
query, appropriate refusals when context is insufficient, completeness and
conciseness of information. To address the lack of publicly available
RAG-centric preference datasets and specialised RMs, we introduce RAGferee, a
methodology that repurposes question-answering (QA) datasets into preference
pairs that prioritise groundedness over stylistic features, enabling the
training of contextual RMs better suited to judging RAG responses. Using
RAGferee, we curate a small preference dataset of 4K samples and fine-tune RMs
ranging from 7B to 24B parameters. Our RAG-centric RMs achieve state-of-the-art
performance on ContextualJudgeBench, surpassing existing 70B+ RMs trained on
much larger (up to 2.4M samples) general corpora, with an absolute improvement
of +15.5%.

</details>


### [46] [RE$^2$: Improving Chinese Grammatical Error Correction via Retrieving Appropriate Examples with Explanation](https://arxiv.org/abs/2509.26038)
*Baoxin Wang,Yumeng Luo,Yixuan Wang,Dayong Wu,Wanxiang Che,Shijin Wang*

Main category: cs.CL

TL;DR: 提出了一种基于语法错误解释选择参考示例以提升大型语言模型中文语法错误纠正性能的方法。


<details>
  <summary>Details</summary>
Motivation: 目前基于文本相似度的示例检索方法常常无法准确匹配实际错误模式，影响模型纠错效果。

Method: 提出RE²方法，通过语法错误解释而非文本相似度来检索参考示例供大型语言模型使用。

Result: 在两个中文语法错误纠正数据集上的实验表明，所提方法显著提升了模型纠错性能。并构建了高质量的语法错误解释数据集。

Conclusion: 基于语法错误解释的参考示例检索方法可有效提高大型语言模型在中文语法错误纠正任务中的表现，同时所构建数据集可促进相关领域研究。

Abstract: The primary objective of Chinese grammatical error correction (CGEC) is to
detect and correct errors in Chinese sentences. Recent research shows that
large language models (LLMs) have been applied to CGEC with significant
results. For LLMs, selecting appropriate reference examples can help improve
their performance. However, existing methods predominantly rely on text
similarity for example retrieval, a strategy that frequently mismatches actual
error patterns and retrieves lexically similar yet grammatically irrelevant
sentences. To address this problem, we propose a method named RE$^2$, which
retrieves appropriate examples with explanations of grammatical errors. Instead
of using text similarity of the input sentence, we use explanations of
grammatical errors to select reference examples, which are used by LLMs to
improve the performance of CGEC. We conduct experiments on two CGEC datasets
and create a high-quality grammatical error explanation (GEE) dataset, which is
not only used in our research but also serves as a valuable resource for future
studies in both CGEC and GEE. The experimental results on the two datasets
indicate that our proposed method effectively improves the performance of CGEC.

</details>


### [47] [Unspoken Hints: Accuracy Without Acknowledgement in LLM Reasoning](https://arxiv.org/abs/2509.26041)
*Arash Marioriyad,Shaygan Adim,Nima Alighardashi,Mahdieh Soleymani Banghshah,Mohammad Hossein Rohban*

Main category: cs.CL

TL;DR: 本文系统研究了大语言模型在链式思维提示中的推理忠实度，揭示了提示正确性、提示类型和呈现风格对模型表现与推理过程的显著影响。


<details>
  <summary>Details</summary>
Motivation: 目前尚不清楚大语言模型生成的推理理由是否真实反映了实际计算过程，还是依赖提示中的捷径进行事后叙述，因此需要系统研究推理忠实度。

Method: 设计包含四个数据集、两个先进模型及多种提示条件（正确与否、呈现风格、复杂度）的实验，评估任务准确率及模型对提示的显性承认情况。

Result: 正确提示显著提升准确率，错误提示降低准确率；复杂和方程类提示更易被模型显性引用；奉承型提示促使模型显性承认依赖，数据泄露型提示提升准确度但加剧隐性依赖。

Conclusion: 大语言模型的推理受提示中捷径的系统性影响，导致推理过程的忠实度受到掩盖，呼吁关注提示设计对模型推理机制的影响。

Abstract: Large language models (LLMs) increasingly rely on chain-of-thought (CoT)
prompting to solve mathematical and logical reasoning tasks. Yet, a central
question remains: to what extent are these generated rationales \emph{faithful}
to the underlying computations, rather than post-hoc narratives shaped by hints
that function as answer shortcuts embedded in the prompt? Following prior work
on hinted vs.\ unhinted prompting, we present a systematic study of CoT
faithfulness under controlled hint manipulations. Our experimental design spans
four datasets (AIME, GSM-Hard, MATH-500, UniADILR), two state-of-the-art models
(GPT-4o and Gemini-2-Flash), and a structured set of hint conditions varying in
correctness (correct and incorrect), presentation style (sycophancy and data
leak), and complexity (raw answers, two-operator expressions, four-operator
expressions). We evaluate both task accuracy and whether hints are explicitly
acknowledged in the reasoning. Our results reveal three key findings. First,
correct hints substantially improve accuracy, especially on harder benchmarks
and logical reasoning, while incorrect hints sharply reduce accuracy in tasks
with lower baseline competence. Second, acknowledgement of hints is highly
uneven: equation-based hints are frequently referenced, whereas raw hints are
often adopted silently, indicating that more complex hints push models toward
verbalizing their reliance in the reasoning process. Third, presentation style
matters: sycophancy prompts encourage overt acknowledgement, while leak-style
prompts increase accuracy but promote hidden reliance. This may reflect
RLHF-related effects, as sycophancy exploits the human-pleasing side and data
leak triggers the self-censoring side. Together, these results demonstrate that
LLM reasoning is systematically shaped by shortcuts in ways that obscure
faithfulness.

</details>


### [48] [RE-Searcher: Robust Agentic Search with Goal-oriented Planning and Self-reflection](https://arxiv.org/abs/2509.26048)
*Daocheng Fu,Jianbiao Mei,Licheng Wen,Xuemeng Yang,Cheng Yang,Rong Wu,Tao Hu,Siqi Li,Yufan Shen,Xinyu Cai,Pinlong Cai,Botian Shi,Yong Liu,Yu Qiao*

Main category: cs.CL

TL;DR: 本论文针对大语言模型(LLMs)在复杂搜索环境中表现出脆弱搜索行为的问题，提出了一种名为RE-Searcher的搜索代理，通过具体化搜索目标和自我反思机制提升搜索鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在知识密集型任务中表现出色，但其受到知识截止、幻觉和交互方式限制，通过外部搜索增强虽能改善但引入了环境复杂性，导致搜索行为脆弱和性能下降。

Method: RE-Searcher在搜索过程中明确具体搜索目标，并反思检索到的证据是否满足目标，结合目标导向规划和自我反思机制，增强在复杂搜索环境中的稳健性。

Result: 实验表明该方法提升了搜索准确率，达到了最先进水平，同时对噪声和误导信息表现出较强的鲁棒性，显著缓解搜索过程中的脆弱性。

Conclusion: 该研究为将LLM代理集成入更复杂的交互环境和实现更自主的决策提供了实用指导和方法支持。

Abstract: Large language models (LLMs) excel at knowledge-intensive question answering
and reasoning, yet their real-world deployment remains constrained by knowledge
cutoff, hallucination, and limited interaction modalities. Augmenting LLMs with
external search tools helps alleviate these issues, but it also exposes agents
to a complex search environment in which small, plausible variations in query
formulation can steer reasoning into unproductive trajectories and amplify
errors. We present a systematic analysis that quantifies how environmental
complexity induces fragile search behaviors and, in turn, degrades overall
performance. To address this challenge, we propose a simple yet effective
approach to instantiate a search agent, RE-Searcher. During search, RE-Searcher
explicitly articulates a concrete search goal and subsequently reflects on
whether the retrieved evidence satisfies that goal. This combination of
goal-oriented planning and self-reflection enables RE-Searcher to resist
spurious cues in complex search environments and perform robust search.
Extensive experiments show that our method improves search accuracy and
achieves state-of-the-art results. Perturbation studies further demonstrate
substantial resilience to noisy or misleading external signals, mitigating the
fragility of the search process. We believe these findings offer practical
guidance for integrating LLM-powered agents into more complex interactive
environments and enabling more autonomous decision-making.

</details>


### [49] [CEAID: Benchmark of Multilingual Machine-Generated Text Detection Methods for Central European Languages](https://arxiv.org/abs/2509.26051)
*Dominik Macko,Jakub Kopal*

Main category: cs.CL

TL;DR: 本研究首次针对中欧语言提供了机器生成文本检测的基准测试，比较了不同训练语言组合，发现针对中欧语言的有监督微调检测器性能最佳且具抗混淆能力。


<details>
  <summary>Details</summary>
Motivation: 现有机器生成文本检测研究多集中于英语，导致在非英语特别是中欧语言上的检测效果较差，跨语言迁移能力有限。

Method: 构建了中欧语言的机器生成文本检测基准，进行多领域、多生成器、多语言评估，比较不同训练语言组合，分析个别因素差异及检测方法的对抗鲁棒性。

Result: 结果显示，针对中欧语言进行有监督微调的检测器在中欧语言上的表现最佳，同时对文本混淆攻击具有较强的抵抗能力。

Conclusion: 有监督微调检测器是提升中欧语言机器生成文本检测效果和抗混淆能力的有效方法，填补了中欧语言检测评测的空白。

Abstract: Machine-generated text detection, as an important task, is predominantly
focused on English in research. This makes the existing detectors almost
unusable for non-English languages, relying purely on cross-lingual
transferability. There exist only a few works focused on any of Central
European languages, leaving the transferability towards these languages rather
unexplored. We fill this gap by providing the first benchmark of detection
methods focused on this region, while also providing comparison of
train-languages combinations to identify the best performing ones. We focus on
multi-domain, multi-generator, and multilingual evaluation, pinpointing the
differences of individual aspects, as well as adversarial robustness of
detection methods. Supervised finetuned detectors in the Central European
languages are found the most performant in these languages as well as the most
resistant against obfuscation.

</details>


### [50] [DyFlow: Dynamic Workflow Framework for Agentic Reasoning](https://arxiv.org/abs/2509.26062)
*Yanbo Wang,Zixiang Xu,Yue Huang,Xiangqi Wang,Zirui Song,Lang Gao,Chenxi Wang,Xiangru Tang,Yue Zhao,Arman Cohan,Xiangliang Zhang,Xiuying Chen*

Main category: cs.CL

TL;DR: DyFlow是一个基于大语言模型的动态工作流生成框架，能够自适应构建和调整推理流程，提高跨任务的泛化能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的智能体系统依赖人工设计流程，难以适应不同任务，且自动生成方法受限于特定数据集，缺乏中间反馈利用，操作流程固定，限制了系统的鲁棒性和推理深度。

Method: DyFlow包含两个核心组件：设计者和执行者。设计者根据高层目标将复杂问题分解为子目标，结合中间反馈动态规划下一步操作；执行者采用上下文感知的动态算子执行操作，实现灵活且语义驱动的推理。

Result: 在社会推理、生物医学、数学问题解决及代码生成等多领域评测中，DyFlow在Pass@k指标上显著超越现有基线，且表现出较强的跨领域泛化能力。

Conclusion: DyFlow通过动态调整推理流程和执行操作，有效提升了基于LLM的智能体系统的适应性和推理深度，为复杂任务处理提供了更通用且高效的解决方案。

Abstract: Agent systems based on large language models (LLMs) have shown great
potential in complex reasoning tasks, but building efficient and generalizable
workflows remains a major challenge. Most existing approaches rely on manually
designed processes, which limits their adaptability across different tasks.
While a few methods attempt automated workflow generation, they are often tied
to specific datasets or query types and make limited use of intermediate
feedback, reducing system robustness and reasoning depth. Moreover, their
operations are typically predefined and inflexible. To address these
limitations, we propose DyFlow, a dynamic workflow generation framework that
adaptively constructs and adjusts reasoning procedures based on task
requirements and real-time intermediate feedback, thereby enhancing cross-task
generalization. DyFlow consists of two core components: a designer and an
executor. The designer decomposes complex problems into a sequence of sub-goals
defined by high-level objectives and dynamically plans the next steps based on
intermediate outputs and feedback. These plans are then carried out by the
executor, which executes each operation using dynamic operators with
context-aware parameterization, enabling flexible and semantically grounded
reasoning. We systematically evaluate DyFlow across diverse domains, including
social reasoning, biomedical tasks, mathematical problem solving, and code
generation. Results demonstrate that DyFlow significantly outperforms existing
baselines, achieving substantial Pass@k improvements and exhibiting robust
generalization across diverse domains. The code is publicly available at
https://github.com/wyf23187/DyFlow.

</details>


### [51] [The Silent Judge: Unacknowledged Shortcut Bias in LLM-as-a-Judge](https://arxiv.org/abs/2509.26072)
*Arash Marioriyad,Mohammad Hossein Rohban,Mahdieh Soleymani Baghshah*

Main category: cs.CL

TL;DR: 当前大语言模型作为自动评估者时存在偏见，倾向于受提示中的表面线索影响，导致判断不准确且缺乏对决策依据的透明说明。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型作为自动评判者时是否能够公正、可信地评价任务输出质量，特别是在回答质量评估中的表现。

Method: 利用两个数据集（ELI5和LitBench）构建评判任务，给回复赋予不同的表面线索（来源身份、时间新旧），使用GPT-4o和Gemini-2.5-Flash两个模型进行配对判断，分析模型是否受到这些无关因素影响。

Result: 模型普遍表现出对近期回复和专家来源的偏好，且在解释决策时几乎不提及这些线索，显示出强烈的启发式捷径倾向和缺乏对偏见的认知。

Conclusion: 当前大语言模型作为评判系统时容易受提示中无关因素影响，评判不够忠实可靠，限制了其在研究和实际应用中的可信度。

Abstract: Large language models (LLMs) are increasingly deployed as automatic judges to
evaluate system outputs in tasks such as summarization, dialogue, and creative
writing. A faithful judge should base its verdicts solely on response quality
and explicitly acknowledge the factors shaping its decision. We show that
current LLM judges fail on both counts by relying on shortcuts introduced in
the prompt. Our study uses two evaluation datasets: ELI5, a benchmark for
long-form question answering, and LitBench, a recent benchmark for creative
writing. Both datasets provide pairwise comparisons, where the evaluator must
choose which of two responses is better. From each dataset we construct 100
pairwise judgment tasks and employ two widely used models, GPT-4o and
Gemini-2.5-Flash, as evaluators in the role of LLM-as-a-judge. For each pair,
we assign superficial cues to the responses, provenance cues indicating source
identity (Human, Expert, LLM, or Unknown) and recency cues indicating temporal
origin (Old, 1950 vs. New, 2025), while keeping the rest of the prompt fixed.
Results reveal consistent verdict shifts: both models exhibit a strong recency
bias, systematically favoring new responses over old, as well as a clear
provenance hierarchy (Expert > Human > LLM > Unknown). These biases are
especially pronounced in GPT-4o and in the more subjective and open-ended
LitBench domain. Crucially, cue acknowledgment is rare: justifications almost
never reference the injected cues, instead rationalizing decisions in terms of
content qualities. These findings demonstrate that current LLM-as-a-judge
systems are shortcut-prone and unfaithful, undermining their reliability as
evaluators in both research and deployment.

</details>


### [52] [Limited Preference Data? Learning Better Reward Model with Latent Space Synthesis](https://arxiv.org/abs/2509.26074)
*Leitian Tao,Xuefeng Du,Yixuan Li*

Main category: cs.CL

TL;DR: 本文提出了LENS框架，通过在大语言模型的潜在嵌入空间中合成偏好数据，显著提升奖励模型训练效率和效果。


<details>
  <summary>Details</summary>
Motivation: 传统偏好数据采集成本高，文本生成方法计算资源消耗大，限制了奖励模型与人类偏好的对齐。

Method: 利用变分自动编码器（VAE）学习响应嵌入的潜在表示，通过在潜在空间中受控扰动合成多样且语义一致的偏好对，避免昂贵的文本生成和标注。

Result: 所提方法在标准基准上优于文本增强，生成速度快18倍，模型体积小16000倍，同时保持偏好顺序，提升奖励模型泛化能力。

Conclusion: LENS提供了一种高效、可扩展的奖励建模数据增强方案，显著改善大语言模型的偏好对齐性能。

Abstract: Reward modeling, crucial for aligning large language models (LLMs) with human
preferences, is often bottlenecked by the high cost of preference data.
Existing textual data synthesis methods are computationally expensive. We
propose a novel framework LENS for synthesizing preference data directly in the
LLM's latent embedding space. Our method employs a Variational Autoencoder
(VAE) to learn a structured latent representation of response embeddings. By
performing controlled perturbations in this latent space and decoding back to
the embedding space, we efficiently generate diverse, semantically consistent
synthetic preference pairs, bypassing costly text generation and annotation. We
provide theoretical guarantees that our synthesized pairs approximately
preserve original preference ordering and improve reward model generalization.
Empirically, our latent-space synthesis significantly outperforms text-based
augmentation on standard benchmarks, achieving superior results while being 18x
faster in generation and using a 16,000x smaller model. Our work offers a
scalable and effective alternative for enhancing reward modeling through
efficient data augmentation. Code is publicly available at
https://github.com/deeplearning-wisc/lens

</details>


### [53] [IMProofBench: Benchmarking AI on Research-Level Mathematical Proof Generation](https://arxiv.org/abs/2509.26076)
*Johannes Schmitt,Gergely Bérczi,Jasper Dekoninck,Jeremy Feusi,Tim Gehrunger,Raphael Appenzeller,Jim Bryan,Niklas Canova,Timo de Wolff,Filippo Gaia,Michel van Garrel,Baran Hashemi,David Holmes,Aitor Iribar Lopez,Victor Jaeck,Martina Jørgensen,Steven Kelk,Stefan Kuhlmann,Adam Kurpisz,Chiara Meroni,Ingmar Metzler,Martin Möller,Samuel Muñoz-Echániz,Robert Nowak,Georg Oberdieck,Daniel Platt,Dylan Possamaï,Gabriel Ribeiro,Raúl Sánchez Galán,Zheming Sun,Josef Teichmann,Richard P. Thomas,Charles Vial*

Main category: cs.CL

TL;DR: 本文介绍了IMProofBench，一个专为评估大型语言模型数学研究能力设计的基准测试，包括39个同行评审的证明问题，支持详细的数学推理和自动评分。


<details>
  <summary>Details</summary>
Motivation: 现有数学基准测试主要侧重于最终答案或中学竞赛题，难以全面评估语言模型在前沿数学研究任务中的表现。

Method: 提出IMProofBench，包含详细证明问题及其子问题，采用包含联网检索和数学软件工具的真实研究环境，结合人工和自动评分。

Result: 当前大型语言模型在较简单研究问题上表现良好，Grok-4在子问题最终答案准确率达52%，GPT-5在证明生成正确率达22%。

Conclusion: IMProofBench作为一个动态基准，将持续与数学界合作更新，为下一代大型语言模型的数学能力评估提供科学依据。

Abstract: As the mathematical capabilities of large language models (LLMs) improve, it
becomes increasingly important to evaluate their performance on research-level
tasks at the frontier of mathematical knowledge. However, existing benchmarks
are limited, as they focus solely on final-answer questions or high-school
competition problems. To address this gap, we introduce IMProofBench, a private
benchmark consisting of 39 peer-reviewed problems developed by expert
mathematicians. Each problem requires a detailed proof and is paired with
subproblems that have final answers, supporting both an evaluation of
mathematical reasoning capabilities by human experts and a large-scale
quantitative analysis through automated grading. Furthermore, unlike prior
benchmarks, the evaluation setup simulates a realistic research environment:
models operate in an agentic framework with tools like web search for
literature review and mathematical software such as SageMath. Our results show
that current LLMs can succeed at the more accessible research-level questions,
but still encounter significant difficulties on more challenging problems.
Quantitatively, Grok-4 achieves the highest accuracy of 52% on final-answer
subproblems, while GPT-5 obtains the best performance for proof generation,
achieving a fully correct solution for 22% of problems. IMProofBench will
continue to evolve as a dynamic benchmark in collaboration with the
mathematical community, ensuring its relevance for evaluating the next
generation of LLMs.

</details>


### [54] [Reinforced Strategy Optimization for Conversational Recommender Systems via Network-of-Experts](https://arxiv.org/abs/2509.26093)
*Xiaoyan Zhao*

Main category: cs.CL

TL;DR: 本文提出了一个层次化的对话推荐系统策略优化框架，通过宏观策略规划和微观响应生成两层机制提升对话推荐的效果。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型驱动的对话推荐系统缺乏对多轮交互策略的显式优化，常用统一提示方式，导致效果不佳。

Method: 提出了强化策略优化（RSO）架构，将响应生成分解为策略制定者选择策略与执行者在专家网络指导下生成响应，并使用基于大语言模型的奖励进行强化学习以探索策略。

Result: 实验结果显示RSO优于当前最先进的方法，验证了该层次化策略优化方法的有效性。

Conclusion: RSO通过分层策略优化显著提升了对话推荐系统的个性化交互性能，为多轮交互策略学习提供了新的解决方案。

Abstract: Conversational Recommender Systems (CRSs) provide personalized
recommendations through multi-turn interactions. With the strong reasoning
abilities of Large Language Models (LLMs), applying them to CRSs has become
promising. Yet, existing methods often lack explicit optimization of
interaction strategies, relying instead on unified prompts, which can yield
suboptimal outcomes. We propose Reinforced Strategy Optimization (RSO), a
hierarchical framework that decomposes response generation into macro-level
strategy planning and micro-level adaptation within a network-of-experts. A
Planner selects strategies (e.g., recommend, explain, encourage), while an
Actor generates responses guided by auxiliary experts for preferences and
factual grounding. This disentanglement enables more tractable learning. To
address limited multi-turn data, we model strategy learning as reinforcement
learning with an LLM-based reward for exploration. Experiments show RSO
outperforms state-of-the-art baselines, validating the effectiveness of
hierarchical strategy optimization.

</details>


### [55] [End-to-End Aspect-Guided Review Summarization at Scale](https://arxiv.org/abs/2509.26103)
*Ilya Boytsov,Vinny DeGenova,Mikhail Balyasin,Joseph Walt,Caitlin Eusden,Marie-Claire Rochat,Margaret Pierson*

Main category: cs.CL

TL;DR: 本文提出了一种结合基于方面的情感分析和引导式摘要的可扩展性大语言模型系统，用于生成产品评论的简洁解读性摘要，并通过大规模在线测试验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了提高产品评论摘要的准确性和解读性，结合客户真实反馈，增强用户体验。

Method: 先从评论中提取并整合方面-情感对，选择最频繁方面，抽样代表性评论，利用结构化提示引导大语言模型生成基于客户反馈的摘要。

Result: 通过大规模在线A/B测试证明系统的实际有效性，并实时部署。

Conclusion: 系统有效提升评论摘要质量，数据集公开支持后续研究。

Abstract: We present a scalable large language model (LLM)-based system that combines
aspect-based sentiment analysis (ABSA) with guided summarization to generate
concise and interpretable product review summaries for the Wayfair platform.
Our approach first extracts and consolidates aspect-sentiment pairs from
individual reviews, selects the most frequent aspects for each product, and
samples representative reviews accordingly. These are used to construct
structured prompts that guide the LLM to produce summaries grounded in actual
customer feedback. We demonstrate the real-world effectiveness of our system
through a large-scale online A/B test. Furthermore, we describe our real-time
deployment strategy and release a dataset of 11.8 million anonymized customer
reviews covering 92,000 products, including extracted aspects and generated
summaries, to support future research in aspect-guided review summarization.

</details>


### [56] [Vocabulary Customization for Efficient Domain-Specific LLM Deployment](https://arxiv.org/abs/2509.26124)
*Christian Herold,Michael Kozielski,Nicholas Santavas,Yannick Versley,Shahram Khadivi*

Main category: cs.CL

TL;DR: 本文针对大型语言模型（LLM）在特定领域文本处理中词汇不匹配问题，提出了一种扩展预训练词汇表以提升分词效率的方法。


<details>
  <summary>Details</summary>
Motivation: 通用分词器无法有效捕捉频繁出现的领域特定词汇，导致分词粒度过细，处理速度下降。

Method: 设计了一种扩展现有分词器的算法，保证分词效率不降低，即输入序列的分词数量不会增加，并加入领域专用词汇。

Result: 在电商实际应用中，扩展后的分词器使输入序列长度缩短最多20%，推理延迟降低，同时保持模型预测质量。

Conclusion: 词汇表适应性增强分词效率，不仅提高了处理速度，还促进模型更好地采用新增词汇，具有广泛应用前景。

Abstract: When using an LLM to process text outside the training domain(s), an often
overlooked factor is vocabulary mismatch, where the general-domain tokenizer
fails to capture frequent domain-specific terms, leading to higher token
fertility and thus a decrease in processing speed due to suboptimal sub-word
splits.
  We address this limitation by augmenting the pretrained vocabulary with a set
of domain-specific tokens. To this end, we design an algorithm that extends an
existing tokenizer while guaranteeing it never decreases tokenization
efficiency: every input sequence is segmented into at most the same number of
tokens as before.
  Evaluated on real-world e-Commerce use-cases, the augmented tokenizer
significantly shortens input sequences by up to 20% and reduces inference
latency on downstream tasks while preserving predictive quality. We further
analyze secondary effects, such as the impact on forward pass speed and the
rate at which the model adopts the newly introduced tokens, to illustrate the
broader benefits of vocabulary adaptation.

</details>


### [57] [The Hunger Game Debate: On the Emergence of Over-Competition in Multi-Agent Systems](https://arxiv.org/abs/2509.26126)
*Xinbei Ma,Ruotian Ma,Xingyu Chen,Zhengliang Shi,Mengru Wang,Jen-tse Huang,Qu Yang,Wenxuan Wang,Fanghua Ye,Qingxuan Jiang,Mengfei Zhou,Zhuosheng Zhang,Rui Wang,Hai Zhao,Zhaopeng Tu,Xiaolong Li,Linus*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLM）多智能体系统中的过度竞争行为及其对协作和任务表现的负面影响，提出了HATE（Hunger Game Debate）实验框架，并探索了通过客观反馈缓解过度竞争的有效性。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统中的竞争如何影响智能体行为尚未充分研究，尤其是当竞争过度时，可能导致行为失控和任务表现下降。

Method: 设计HATE实验框架，在零和竞争环境中模拟智能体辩论，通过多种LLM和任务测试竞争压力对行为和任务完成的影响，并加入不同类型的评判者反馈以调节行为。

Result: 实验结果显示，竞争压力显著促进了过度竞争行为，导致讨论失控和任务表现下降。引入客观、关注任务的反馈机制能够有效缓解过度竞争行为。

Conclusion: 合理设计环境反馈机制可控制多智能体系统中因竞争引发的负面行为，促进协作和提升任务完成度，为理解和管理AI社区中涌现的社会动态提供了新视角。

Abstract: LLM-based multi-agent systems demonstrate great potential for tackling
complex problems, but how competition shapes their behavior remains
underexplored. This paper investigates the over-competition in multi-agent
debate, where agents under extreme pressure exhibit unreliable, harmful
behaviors that undermine both collaboration and task performance. To study this
phenomenon, we propose HATE, the Hunger Game Debate, a novel experimental
framework that simulates debates under a zero-sum competition arena. Our
experiments, conducted across a range of LLMs and tasks, reveal that
competitive pressure significantly stimulates over-competition behaviors and
degrades task performance, causing discussions to derail. We further explore
the impact of environmental feedback by adding variants of judges, indicating
that objective, task-focused feedback effectively mitigates the
over-competition behaviors. We also probe the post-hoc kindness of LLMs and
form a leaderboard to characterize top LLMs, providing insights for
understanding and governing the emergent social dynamics of AI community.

</details>


### [58] [CliniBench: A Clinical Outcome Prediction Benchmark for Generative and Encoder-Based Language Models](https://arxiv.org/abs/2509.26136)
*Paul Grundmann,Dennis Fast,Jan Frick,Thomas Steffek,Felix Gers,Wolfgang Nejdl,Alexander Löser*

Main category: cs.CL

TL;DR: 本文提出了CliniBench基准，用于比较生成式大型语言模型（LLMs）和编码器分类器在医疗出院诊断预测中的表现，发现编码器分类器性能优于生成式模型。


<details>
  <summary>Details</summary>
Motivation: 生成式大型语言模型在复杂医疗任务中的实际应用效果尚未充分研究，需建立基准进行系统比较。

Method: 构建CliniBench基准，使用MIMIC-IV数据集的入院记录预测出院诊断，比较12个生成式LLMs与3个编码器分类器性能，并评估基于相似患者的检索增强策略。

Result: 编码器分类器在诊断预测上持续优于生成式模型，检索增强策略显著提升了生成式LLMs的表现。

Conclusion: 编码器分类器目前在临床诊断预测任务中表现更佳，检索增强可提升生成式模型性能，提示未来可结合两者优势应用于临床。

Abstract: With their growing capabilities, generative large language models (LLMs) are
being increasingly investigated for complex medical tasks. However, their
effectiveness in real-world clinical applications remains underexplored. To
address this, we present CliniBench, the first benchmark that enables
comparability of well-studied encoder-based classifiers and generative LLMs for
discharge diagnosis prediction from admission notes in MIMIC-IV dataset. Our
extensive study compares 12 generative LLMs and 3 encoder-based classifiers and
demonstrates that encoder-based classifiers consistently outperform generative
models in diagnosis prediction. We assess several retrieval augmentation
strategies for in-context learning from similar patients and find that they
provide notable performance improvements for generative LLMs.

</details>


### [59] [MGen: Millions of Naturally Occurring Generics in Context](https://arxiv.org/abs/2509.26160)
*Gustavo Cilleruelo,Emily Allaway,Barry Haddow,Alexandra Birch*

Main category: cs.CL

TL;DR: MGen是一个包含超过400万条自然发生的泛化和量化句子的多样化数据集，覆盖11种量词，适用于大规模泛化句子计算研究。


<details>
  <summary>Details</summary>
Motivation: 现有泛化句子研究缺乏大规模且多样化的数据资源，因此需要构建一个大规模、多源的泛化句子数据集。

Method: 从网站和学术论文等多种文本源提取超过400万条包含泛化和量化句子的长上下文句子，涵盖11种量词。

Result: MGen数据集包含平均超过16个单词的长句，展现了说话者用泛化句表达关于人的普遍化观点的特点。

Conclusion: MGen作为最大且最具多样性的自然泛化句子数据集，为泛化句子的计算研究提供了重要资源，公开提供以促进相关研究。

Abstract: MGen is a dataset of over 4 million naturally occurring generic and
quantified sentences extracted from diverse textual sources. Sentences in the
dataset have long context documents, corresponding to websites and academic
papers, and cover 11 different quantifiers. We analyze the features of generics
sentences in the dataset, with interesting insights: generics can be long
sentences (averaging over 16 words) and speakers often use them to express
generalisations about people.
  MGen is the biggest and most diverse dataset of naturally occurring generic
sentences, opening the door to large-scale computational research on
genericity. It is publicly available at https://gustavocilleruelo.com/mgen

</details>


### [60] [Explaining novel senses using definition generation with open language models](https://arxiv.org/abs/2509.26181)
*Mariia Fedorova,Andrey Kutuzov,Francesco Periti,Yves Scherrer*

Main category: cs.CL

TL;DR: 本文利用开放权重大型语言模型的定义生成器，针对芬兰语、俄语和德语的新词义解释任务进行了微调，模型表现优于AXOLOTL'24共享任务中的闭源模型。


<details>
  <summary>Details</summary>
Motivation: 目前新词义解释依赖闭源大型语言模型，限制了研究和应用。本文旨在利用开放权重模型提升解释性能，并推动解释语义变化的公开研究。

Method: 使用开放权重大型语言模型的定义生成器，微调模型以生成目标词语的新词义解释。采用AXOLOTL'24共享任务数据集，涵盖芬兰语、俄语和德语，比较编码器-解码器与仅解码器模型的性能。

Result: 微调后的开放权重模型性能超过了该共享任务中最好的闭源模型；编码器-解码器模型和仅解码器模型表现相当。

Conclusion: 开放权重大型语言模型适合新词义解释任务，可替代闭源模型，且编码器-解码器架构与解码器架构效果相似，促进了语义变化解释的研究开放性。

Abstract: We apply definition generators based on open-weights large language models to
the task of creating explanations of novel senses, taking target word usages as
an input. To this end, we employ the datasets from the AXOLOTL'24 shared task
on explainable semantic change modeling, which features Finnish, Russian and
German languages. We fine-tune and provide publicly the open-source models
performing higher than the best submissions of the aforementioned shared task,
which employed closed proprietary LLMs. In addition, we find that
encoder-decoder definition generators perform on par with their decoder-only
counterparts.

</details>


### [61] [VietBinoculars: A Zero-Shot Approach for Detecting Vietnamese LLM-Generated Text](https://arxiv.org/abs/2509.26189)
*Trieu Hai Nguyen,Sivaswamy Akilesh*

Main category: cs.CL

TL;DR: 提出了针对越南语LLM生成文本检测的VietBinoculars方法，通过优化全局阈值，大幅提升检测准确率。


<details>
  <summary>Details</summary>
Motivation: 随着LLM生成文本越来越复杂且与人类写作极为相似，传统检测方法效果下降，尤其是面对多样化和快速迭代的LLM模型。

Method: 基于Binoculars方法，优化全局阈值，构建新的越南语AI生成文本数据集，用于阈值优化和性能评估。

Result: 在多个跨领域数据集上，VietBinoculars在准确率、F1值和AUC上均超过99%，优于原Binoculars模型、传统方法及其他先进检测工具（包括ZeroGPT和DetectGPT）。

Conclusion: VietBinoculars有效提升越南语LLM文本检测性能，特别是在应对特殊提示策略时表现出显著优势。

Abstract: The rapid development research of Large Language Models (LLMs) based on
transformer architectures raises key challenges, one of them being the task of
distinguishing between human-written text and LLM-generated text. As
LLM-generated textual content, becomes increasingly complex over time, and
resembles human writing, traditional detection methods are proving less
effective, especially as the number and diversity of LLMs continue to grow with
new models and versions being released at a rapid pace. This study proposes
VietBinoculars, an adaptation of the Binoculars method with optimized global
thresholds, to enhance the detection of Vietnamese LLM-generated text. We have
constructed new Vietnamese AI-generated datasets to determine the optimal
thresholds for VietBinoculars and to enable benchmarking. The results from our
experiments show results show that VietBinoculars achieves over 99\% in all two
domains of accuracy, F1-score, and AUC on multiple out-of-domain datasets. It
outperforms the original Binoculars model, traditional detection methods, and
other state-of-the-art approaches, including commercial tools such as ZeroGPT
and DetectGPT, especially under specially modified prompting strategies.

</details>


### [62] [Comparative Analysis of Ant Colony Optimization and Google OR-Tools for Solving the Open Capacitated Vehicle Routing Problem in Logistics](https://arxiv.org/abs/2509.26216)
*Assem Omar,Youssef Omar,Marwa Solayman,Hesham Mansour*

Main category: cs.CL

TL;DR: 本文比较了蚁群算法（ACO）和谷歌OR-Tools两种算法在开放容量车辆路径问题（OCVRP）中的性能表现。


<details>
  <summary>Details</summary>
Motivation: 现代物流需要高效的路径规划，OCVRP是一种不要求车辆返回仓库的配送路径优化问题，如何有效解决该问题具有现实意义。

Method: 本文使用Python实现了ACO和OR-Tools两种算法，基于自定义数据集进行比较分析。

Result: 结果显示，ACO在路径参数上更灵活，OR-Tools计算速度更快、稳定性更好且输入要求较少。

Conclusion: 两种算法各有优劣，可为可扩展的实时物流系统选择合适的路径规划策略提供参考。

Abstract: In modern logistics management systems, route planning requires high
efficiency. The Open Capacitated Vehicle Routing Problem (OCVRP) deals with
finding optimal delivery routes for a fleet of vehicles serving geographically
distributed customers, without requiring the vehicles to return to the depot
after deliveries. The present study is comparative in nature and speaks of two
algorithms for OCVRP solution: Ant Colony Optimization (ACO), a nature-inspired
metaheuristic; and Google OR-Tools, an industry-standard toolkit for
optimization. Both implementations were developed in Python and using a custom
dataset. Performance appraisal was based on routing efficiency, computation
time, and scalability. The results show that ACO allows flexibility in routing
parameters while OR-Tools runs much faster with more consistency and requires
less input. This could help choose among routing strategies for scalable
real-time logistics systems.

</details>


### [63] [Type-Less yet Type-Aware Inductive Link Prediction with Pretrained Language Models](https://arxiv.org/abs/2509.26224)
*Alessandro De Bellis,Salvatore Bufi,Giovanni Servedio,Vito Walter Anelli,Tommaso Di Noia,Eugenio Di Sciascio*

Main category: cs.CL

TL;DR: 本文提出了TyleR，一种利用预训练语言模型(PLMs)进行语义增强的无显式类型但具备类型感知能力的归纳式链接预测方法，针对知识图中类型信息缺失或稀疏的情况表现优异。


<details>
  <summary>Details</summary>
Motivation: 现实世界知识图中新实体频繁出现且类型信息往往缺失或不完整，传统方法难以有效利用稀疏且粗粒度的类型注释，影响链接预测的准确性。

Method: 提出TyleR方法，利用预训练语言模型对节点表示进行语义增强，从而隐式捕捉实体的类型信息，实现基于子图的归纳式链接预测，无需依赖显式类型注释。

Result: 在多个标准基准数据集上，TyleR在类型注释稀缺和图结构稀疏的场景中，表现优于现有先进基线方法。

Conclusion: 利用PLMs进行语义丰富能够有效弥补显式类型信息的缺失，提高归纳式链接预测的性能，为真实知识图推理提供新思路。

Abstract: Inductive link prediction is emerging as a key paradigm for real-world
knowledge graphs (KGs), where new entities frequently appear and models must
generalize to them without retraining. Predicting links in a KG faces the
challenge of guessing previously unseen entities by leveraging generalizable
node features such as subgraph structure, type annotations, and ontological
constraints. However, explicit type information is often lacking or incomplete.
Even when available, type information in most KGs is often coarse-grained,
sparse, and prone to errors due to human annotation. In this work, we explore
the potential of pre-trained language models (PLMs) to enrich node
representations with implicit type signals. We introduce TyleR, a Type-less yet
type-awaRe approach for subgraph-based inductive link prediction that leverages
PLMs for semantic enrichment. Experiments on standard benchmarks demonstrate
that TyleR outperforms state-of-the-art baselines in scenarios with scarce type
annotations and sparse graph connectivity. To ensure reproducibility, we share
our code at https://github.com/sisinflab/tyler .

</details>


### [64] [Finetune Once: Decoupling General & Domain Learning with Dynamic Boosted Annealing](https://arxiv.org/abs/2509.26242)
*Yang Tang,Ruijie Liu,Yifan Wang,Shiyu Li,Xi Chen*

Main category: cs.CL

TL;DR: 本文提出了一种名为动态增强退火(DBA)的高效通用大语言模型微调方法，通过零学习率训练获取全局梯度，用于梯度增强和动态训练步校正，实现仅依赖领域数据的微调流程，提升平均性能5.8%，同时GPU计算时间减少91%。


<details>
  <summary>Details</summary>
Motivation: 传统的微调方法往往需要复杂的数据混合和多次实验以获得最佳泛化性能，训练过程繁琐且计算资源消耗大。

Method: 通过零学习率训练获得全局梯度，利用该梯度进行梯度增强和动态步骤校正，结合退火学习，仅用领域数据进行微调，避免数据混合和训练崩溃。

Result: 在多个任务和流行基础模型上，DBA方法在常规和领域特定性能上平均提升5.8%，并显著减少了91%的GPU小时数。

Conclusion: DBA方法简化了大语言模型微调流程，提高了泛化性能并大幅降低了计算资源消耗，具有良好的实用价值和推广潜力。

Abstract: Large language models (LLMs) fine-tuning shows excellent implications.
However, vanilla fine-tuning methods often require intricate data mixture and
repeated experiments for optimal generalization. To address these challenges
and streamline the training process, we propose an efficient and universal
solution, Dynamic Boosted Annealing (DBA). We obtain a global gradient through
zero-learning-rate training on general data, which is subsequently employed for
gradient boosting and dynamic training step correction during domain training.
In conjunction with annealing learning, we end up establishing a fine-tuning
pipeline that relies solely on domain data without collapse. By evaluating both
general and domain-specific performance across multiple tasks on several
popular base models, DBA achieves an average improvement of 5.8% in joint
performance over vanilla fine-tuning. Furthermore, since general data is no
longer involved in annealing, repeated experiments led by data mixture are also
eliminated. According to our tests, the DBA method can reduce GPU hours by
91.0% compared to the vanilla method.

</details>


### [65] [Optimizing Speech Language Models for Acoustic Consistency](https://arxiv.org/abs/2509.26276)
*Morteza Rohanian,Michael Krauthammer*

Main category: cs.CL

TL;DR: 本文提出了一种结合语义初始化和规划损失的语音语言模型，旨在实现鲁棒且一致的语音生成。


<details>
  <summary>Details</summary>
Motivation: 为了提升语音语言模型在不同说话人、性别、情感、环境等因素下生成语音的一致性和鲁棒性，同时达到语义和声学的良好对齐。

Method: 采用自监督特征进行语音标记初始化，加入轻量对齐损失，并通过稀疏化及辅助目标进行训练，以增强模型的鲁棒性和内容规划能力。训练了三种模型：0.7B语音模型，1.0B语音模型，以及包含文本和语音的1.0B交错模型。

Result: 语音专属模型在声音一致性方面表现优异，超过更大规模系统。交错模型在词汇和句法任务以及语义-声学对齐上表现提升，但一致性下降。初始化偏向内容结构，代价是韵律细节的权衡。

Conclusion: 语言模型设计与训练策略可在不更改分词器或架构的前提下，实现声学稳定性与语义基础的平衡。此外，相关模型和演示已公开，便于进一步研究与探索。

Abstract: We study speech language models that incorporate semantic initialization and
planning losses to achieve robust and consistent generation. Our approach
initializes speech tokens with self-supervised features, applies a light
alignment loss, and trains with thinning and auxiliary objectives that target
robustness and content planning. We train three models: a 0.7B speech-only
model, a 1.0B speech-only model, and a 1.0B interleaved model with both text
and speech. Acoustic studies show that the speech-only models achieve the
highest consistency across speaker, gender, sentiment, room, and background
factors, surpassing larger systems. Interleaving improves lexical and syntactic
probes and semantic--acoustic alignment but reduces consistency. Linear probes
show that our initialization biases the model toward content structure while
trading off prosody detail. These results show that LM-side design and training
mix control the balance between acoustic stability and semantic grounding
without changes to the tokenizer or runtime architecture. A demo and model
weights are available for exploration.

</details>


### [66] [QUARTZ : QA-based Unsupervised Abstractive Refinement for Task-oriented Dialogue Summarization](https://arxiv.org/abs/2509.26302)
*Mohamed Imed Eddine Ghebriout,Gaël Guibon,Ivan Lerner,Emmanuel Vincent*

Main category: cs.CL

TL;DR: 提出了\u001btask-oriented utility-based dialogue summarization\u001b框架\u001bapp\u001b，利用多种大语言模型零样本生成多个摘要和任务导向问答，通过问答质量评估选择最优摘要，并对最佳模型微调，实现无线样本下效果媲美全监督SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统对话摘要模型需要大量人工标注，成本高且摘要常缺乏针对特定任务的关注度，影响其在下游领域（如医疗）应用的效果。

Method: \u001bapp\u001b框架先利用大语言模型池零样本生成多个候选摘要和任务导向的问答对，然后用大语言模型根据任务相关问答的回答质量评估这些摘要，选出最优摘要和问答，并基于选出的最佳摘要对模型进行微调。

Result: 在多个数据集上的验证显示，\u001bapp\u001b框架在多种零样本设置下表现出竞争力，达到了和全监督领域最先进方法相媲美的效果。

Conclusion: 通过任务导向的实用性评估并结合大语言模型微调，\u001bapp\u001b能有效提升对话摘要的质量和适用性，降低对人工监督依赖，实现了优秀的零样本性能。

Abstract: Dialogue summarization aims to distill the core meaning of a conversation
into a concise text. This is crucial for reducing the complexity and noise
inherent in dialogue-heavy applications. While recent approaches typically
train language models to mimic human-written summaries, such supervision is
costly and often results in outputs that lack task-specific focus limiting
their effectiveness in downstream applications, such as medical tasks. In this
paper, we propose \app, a framework for task-oriented utility-based dialogue
summarization. \app starts by generating multiple summaries and task-oriented
question-answer pairs from a dialogue in a zero-shot manner using a pool of
large language models (LLMs). The quality of the generated summaries is
evaluated by having LLMs answer task-related questions before \textit{(i)}
selecting the best candidate answers and \textit{(ii)} identifying the most
informative summary based on these answers. Finally, we fine-tune the best LLM
on the selected summaries. When validated on multiple datasets, \app
demonstrates its effectiveness by achieving competitive results in various
zero-shot settings, rivaling fully-supervised State-of-the-Art (SotA) methods.

</details>


### [67] [Feedback Forensics: A Toolkit to Measure AI Personality](https://arxiv.org/abs/2509.26305)
*Arduin Findeis,Timo Kaufmann,Eyke Hüllermeier,Robert Mullins*

Main category: cs.CL

TL;DR: 本文介绍了Feedback Forensics，一款用于跟踪和评估AI模型个性变化的开源工具包，结合AI标注器，支持通过Python接口和浏览器应用分析模型个性。


<details>
  <summary>Details</summary>
Motivation: 现有基于人类反馈的模型个性评估方法存在不透明和过拟合问题，缺乏公开工具来明确评估模型个性及相关特质。

Method: 提出Feedback Forensics工具包，利用AI标注器对流行人类反馈数据集（如Chatbot Arena等）中的个性特质进行分析，并评估流行模型表现。提供Python API及网页应用，支持用户交互和数据注释。

Result: 工具成功分析了常用人类反馈数据中的个性特质，并量化了主流模型在这些特质上的表现，验证了工具的实用性和有效性。

Conclusion: Feedback Forensics为AI模型个性评估提供了透明、系统的工具，促进对模型行为的理解和改进，有助于解决现有人类反馈评估方法的缺陷。

Abstract: Some traits making a "good" AI model are hard to describe upfront. For
example, should responses be more polite or more casual? Such traits are
sometimes summarized as model character or personality. Without a clear
objective, conventional benchmarks based on automatic validation struggle to
measure such traits. Evaluation methods using human feedback such as Chatbot
Arena have emerged as a popular alternative. These methods infer "better"
personality and other desirable traits implicitly by ranking multiple model
responses relative to each other. Recent issues with model releases highlight
limitations of these existing opaque evaluation approaches: a major model was
rolled back over sycophantic personality issues, models were observed
overfitting to such feedback-based leaderboards. Despite these known issues,
limited public tooling exists to explicitly evaluate model personality. We
introduce Feedback Forensics: an open-source toolkit to track AI personality
changes, both those encouraged by human (or AI) feedback, and those exhibited
across AI models trained and evaluated on such feedback. Leveraging AI
annotators, our toolkit enables investigating personality via Python API and
browser app. We demonstrate the toolkit's usefulness in two steps: (A) first we
analyse the personality traits encouraged in popular human feedback datasets
including Chatbot Arena, MultiPref and PRISM; and (B) then use our toolkit to
analyse how much popular models exhibit such traits. We release (1) our
Feedback Forensics toolkit alongside (2) a web app tracking AI personality in
popular models and feedback datasets as well as (3) the underlying annotation
data at https://github.com/rdnfn/feedback-forensics.

</details>


### [68] [One-Token Rollout: Guiding Supervised Fine-Tuning of LLMs with Policy Gradient](https://arxiv.org/abs/2509.26313)
*Rui Ming,Haoyuan Wu,Shoubo Hu,Zhuolun He,Bei Yu*

Main category: cs.CL

TL;DR: 本文提出了一种名为一词展开（OTR）的新型微调算法，通过将每个生成的词视为单步强化学习轨迹，结合策略梯度方法，提升了基于监督微调的大语言模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统监督微调（SFT）在泛化能力上常不如强化学习（RL），我们认为这主要源于SFT使用的是固定数据集，而RL使用的是基于当前策略的在线数据，数据的性质差异导致性能差异。

Method: OTR算法将自动回归的生成过程重新定义为单步强化学习问题，在每步通过蒙特卡洛采样多个候选词，并利用监督数据中的真实词作为奖励信号，结合策略梯度对模型进行微调。

Result: 在数学推理、代码生成及通用领域推理等多个具有挑战性的基准测试中，OTR持续优于传统的监督微调方法，证明了其有效性。

Conclusion: OTR作为一种实用的微调替代方案，证明了基于策略的在线数据对提升模型泛化性能的重要性，为大语言模型微调提供了新的方向。

Abstract: Supervised fine-tuning (SFT) is the predominant method for adapting large
language models (LLMs), yet it often struggles with generalization compared to
reinforcement learning (RL). In this work, we posit that this performance
disparity stems not just from the loss function, but from a more fundamental
difference: SFT learns from a fixed, pre-collected dataset, whereas RL utilizes
on-policy data sampled from the current policy. Building on this hypothesis, we
introduce one-token rollout (OTR), a novel fine-tuning algorithm that guides
SFT with the policy gradient method. OTR reframes the autoregressive learning
process by treating each token generation as a single-step reinforcement
learning trajectory. At each step, it performs a Monte Carlo ``rollout'' by
sampling multiple candidate tokens from the current policy's distribution. The
ground-truth token from the supervised data is then used to provide a reward
signal to these samples. Guided by policy gradient, our algorithm repurposes
static, off-policy supervised data into a dynamic, on-policy signal at the
token level, capturing the generalization benefits of on-policy learning while
bypassing the costly overhead of full sentence generation. Through extensive
experiments on a diverse suite of challenging benchmarks spanning mathematical
reasoning, code generation, and general domain reasoning, we demonstrate that
OTR consistently outperforms standard SFT. Our findings establish OTR as a
powerful and practical alternative for fine-tuning LLMs and provide compelling
evidence that the on-policy nature of data is a critical driver of
generalization, offering a promising new direction for fine-tuning LLMs.

</details>


### [69] [Latent Thinking Optimization: Your Latent Reasoning Language Model Secretly Encodes Reward Signals in its Latent Thoughts](https://arxiv.org/abs/2509.26314)
*Hanwen Du,Yuxin Dong,Xia Ning*

Main category: cs.CL

TL;DR: 本文研究了一种基于潜在空间的思维优化方法，通过对Huggin-3.5B模型的潜在思维过程进行监督和优化，提高了大型语言模型的推理准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统的大型语言模型通过生成自然语言的思维链条解决问题，但计算成本高且容易出现过度思考。Huggin-3.5B通过潜在表示进行推理，但缺乏可解释性和监督，影响其可靠性。

Method: 作者分析了潜在思维中正确与错误答案的区分特征，提出了潜在思维分类器作为奖赏模型（LRM），进而设计了潜在思维优化算法（LTO），利用LRM在潜在空间中优化思维过程。

Result: 实验表明LRM能有效检测错误思维模式，LTO显著提升潜在思维流程的性能，并且LRM和LTO在不同领域和通用大型语言模型中均表现出良好的推广性与适用性。

Conclusion: 本文证明了在潜在空间中进行带监督的奖励建模和思维优化是提升大型语言模型思维效率与准确性的有效方案，具有通用、高效和领域无关的优势。

Abstract: Large Language Models (LLMs) excel at problem solving by generating chain of
thoughts in natural language, but such verbal thinking is computationally
costly and prone to overthinking. Recent work instead proposes a latent
thinking architecture Huggin-3.5B, which represents intermediate reasoning
steps as sequence of latent representations. However, latent thoughts lack
interpretability and are difficult to supervise, raising concerns about the
correctness and reliability of its latent thinking processes. In this paper, we
provide a systematic study of how Huggin-3.5B thinks in the latent space and
how external supervision signals can improve its latent thinking processes. We
show that latent thoughts leading to correct versus incorrect answers exhibit
highly distinguishable patterns, and that a latent classifier can reliably
predict answer correctness directly from latent thoughts. Leveraging these
insights, we propose Latent Thinking Optimization (LTO), a probabilistic
algorithm that employs the latent classifier as a Latent Reward Model (LRM) to
optimize the latent thinking processes. Extensive experiments across diverse
reasoning tasks demonstrate that LRM is highly effective in detecting incorrect
latent thinking patterns, and LTO can significantly improve the latent thinking
processes. Furthermore, we show that LRM can generalize across diverse domains,
and LTO can be seamlessly applied to general LLMs to improve their thinking
processes. In contrast to verbal thinking, our method demonstrates that reward
modeling and scaling test-time thinking with supervision can be performed
directly in the latent space, highlighting its potential as a general,
efficient, and domain-agnostic approach to improving the thinking processes of
LLMs.

</details>


### [70] [Fast-dLLM v2: Efficient Block-Diffusion LLM](https://arxiv.org/abs/2509.26328)
*Chengyue Wu,Hao Zhang,Shuchen Xue,Shizhe Diao,Yonggan Fu,Zhijian Liu,Pavlo Molchanov,Ping Luo,Song Han,Enze Xie*

Main category: cs.CL

TL;DR: 提出了Fast-dLLM v2，一种基于区块扩散的语言模型，能够高效将预训练的自回归模型转换为扩散模型，实现并行文本生成，显著加速推理速度同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 自回归大型语言模型性能优异但推理速度受限，需设计方法提升推理效率且不损失性能。

Method: 设计区块扩散机制与互补注意力掩码结合区块双向上下文建模，提出层级缓存机制（区块级缓存与子区块缓存）辅助并行生成，并引入新的训练策略，微调约10亿标注数据。

Result: Fast-dLLM v2在多项基准测试中性能达到或超过自回归基线，推理速度提升最多2.5倍，且生成质量未受影响，训练数据需求显著低于其他扩散模型。

Conclusion: Fast-dLLM v2有效兼顾速度与性能，是大规模语言模型实用部署的重要进展，代码与模型计划公开。

Abstract: Autoregressive (AR) large language models (LLMs) have achieved remarkable
performance across a wide range of natural language tasks, yet their inherent
sequential decoding limits inference efficiency. In this work, we propose
Fast-dLLM v2, a carefully designed block diffusion language model (dLLM) that
efficiently adapts pretrained AR models into dLLMs for parallel text
generation, requiring only approximately 1B tokens of fine-tuning. This
represents a 500x reduction in training data compared to full-attention
diffusion LLMs such as Dream (580B tokens), while preserving the original
model's performance. Our approach introduces a novel training recipe that
combines a block diffusion mechanism with a complementary attention mask,
enabling blockwise bidirectional context modeling without sacrificing AR
training objectives. To further accelerate decoding, we design a hierarchical
caching mechanism: a block-level cache that stores historical context
representations across blocks, and a sub-block cache that enables efficient
parallel generation within partially decoded blocks. Coupled with our parallel
decoding pipeline, Fast-dLLM v2 achieves up to 2.5x speedup over standard AR
decoding without compromising generation quality. Extensive experiments across
diverse benchmarks demonstrate that Fast-dLLM v2 matches or surpasses AR
baselines in accuracy, while delivering state-of-the-art efficiency among dLLMs
- marking a significant step toward the practical deployment of fast and
accurate LLMs. Code and model will be publicly released.

</details>


### [71] [Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement Learning](https://arxiv.org/abs/2509.26383)
*Jinyeop Song,Song Wang,Julian Shun,Yada Zhu*

Main category: cs.CL

TL;DR: KG-R1通过强化学习简化了知识图检索增强生成系统，提升效率和准确性，且具备良好迁移性和适配性。


<details>
  <summary>Details</summary>
Motivation: 现有知识图检索增强生成系统多模块设计导致推理成本高且依赖特定知识图，限制了系统效率和通用性。

Method: 提出KG-R1，一个基于强化学习的代理框架，利用单一代理与知识图交互，实现逐步检索并整合信息进行推理和生成，端到端优化。

Result: 在知识图问答基准测试中，KG-R1相比多模块方法使用更少的生成tokens实现更高准确度，且训练后能无修改适配新知识图。

Conclusion: KG-R1框架兼顾效率、准确性和迁移性，适合实际应用中的知识图检索增强生成任务。

Abstract: Knowledge-graph retrieval-augmented generation (KG-RAG) couples large
language models (LLMs) with structured, verifiable knowledge graphs (KGs) to
reduce hallucinations and expose reasoning traces. However, many KG-RAG systems
compose multiple LLM modules (e.g planning, reasoning, and responding),
inflating inference cost and binding behavior to a specific target KG. To
address this, we introduce KG-R1, an agentic KG retrieval-augmented generation
(KG-RAG) framework through reinforcement learning (RL). KG-R1 utilizes a single
agent that interacts with KGs as its environment, learning to retrieve at each
step and incorporating the retrieved information into its reasoning and
generation. The process is optimized through end-to-end RL. In controlled
experiments across Knowledge-Graph Question Answering (KGQA) benchmarks, our
method demonstrates both efficiency and transferability: Using Qwen-2.5-3B,
KG-R1 improves answer accuracy with fewer generation tokens than prior
multi-module workflow methods that use larger foundation or fine-tuned models.
Furthermore, KG-R1 enables plug and play: after training, it maintains strong
accuracy on new KGs without modification. These properties make KG-R1 a
promising KG-RAG framework for real-world deployment. Our code is publicly
available at https://github.com/Jinyeop3110/KG-R1.

</details>


### [72] [An Annotation Scheme for Factuality and its Application to Parliamentary Proceedings](https://arxiv.org/abs/2509.26406)
*Gili Goldin,Shira Wigderson,Ella Rabinovich,Shuly Wintner*

Main category: cs.CL

TL;DR: 本文提出了一种复杂的事实性多维注释方案，并在近5000条议会话语中手工标注，探索自动预测方法，以助于大规模语料库的扩展。


<details>
  <summary>Details</summary>
Motivation: 事实性是判断语言表达与现实世界信息相关程度的关键，涉及多种语言信号，事实核查等应用亟需有效的注释方法。

Method: 结合多个前期工作中的事实性概念，设计复杂多维的注释体系，在希伯来语议会话语中进行近5000句人工标注，评估注释一致性，并尝试多种自动预测技术。

Result: 注释方案获得较好的注释者一致性，同时自动预测实验结果展示了该方案部分特征的可预测性。

Conclusion: 所提多维事实性注释体系适用于希伯来语议会语料，具有推广到其他语言的潜力，并为大规模语料的事实性自动标注提供基础。

Abstract: Factuality assesses the extent to which a language utterance relates to
real-world information; it determines whether utterances correspond to facts,
possibilities, or imaginary situations, and as such, it is instrumental for
fact checking. Factuality is a complex notion that relies on multiple
linguistic signals, and has been studied in various disciplines.
  We present a complex, multi-faceted annotation scheme of factuality that
combines concepts from a variety of previous works. We developed the scheme for
Hebrew, but we trust that it can be adapted to other languages. We also present
a set of almost 5,000 sentences in the domain of parliamentary discourse that
we manually annotated according to this scheme. We report on inter-annotator
agreement, and experiment with various approaches to automatically predict
(some features of) the scheme, in order to extend the annotation to a large
corpus.

</details>


### [73] [Automatic Fact-checking in English and Telugu](https://arxiv.org/abs/2509.26415)
*Ravi Kiran Chikkala,Tatiana Anikina,Natalia Skachkova,Ivan Vykopal,Rodrigo Agerri,Josef van Genabith*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在英特卢伽语事实陈述真假分类及生成理由的效果，构建了英-特卢伽语双语数据集并进行了方法基准测试。


<details>
  <summary>Details</summary>
Motivation: 鉴于虚假信息的全球性挑战及人工验证的高消耗，探讨利用大型语言模型提高事实真伪判别效率的可能性。

Method: 构建了英-特卢伽语双语数据集，基于大型语言模型开展多种真假分类方法的实验和评测。

Result: 实验展示了不同大型语言模型在英特卢伽语事实真伪分类及理由生成上的表现差异和有效性。

Conclusion: 大型语言模型在双语事实陈述的真伪判别与理由生成方面具备应用潜力，有助于提升虚假信息检测效率和多语言支持。

Abstract: False information poses a significant global challenge, and manually
verifying claims is a time-consuming and resource-intensive process. In this
research paper, we experiment with different approaches to investigate the
effectiveness of large language models (LLMs) in classifying factual claims by
their veracity and generating justifications in English and Telugu. The key
contributions of this work include the creation of a bilingual English-Telugu
dataset and the benchmarking of different veracity classification approaches
based on LLMs.

</details>


### [74] [Text-Based Approaches to Item Alignment to Content Standards in Large-Scale Reading & Writing Tests](https://arxiv.org/abs/2509.26431)
*Yanbin Fu,Hong Jiao,Tianyi Zhou,Robert W. Lissitz,Nan Zhang,Ming Li,Qingshu Xu,Sydney Peters*

Main category: cs.CL

TL;DR: 本文研究了使用微调的小型语言模型自动对齐考试项目与内容标准，结果显示该方法在精细技能对齐上优于传统机器学习模型。


<details>
  <summary>Details</summary>
Motivation: 传统的考试项目对齐依赖人工判断，主观且耗时，因此探索自动化方法提高效率和准确性。

Method: 基于大规模的标准化考试数据，训练不同的小型语言模型进行领域和技能级别的对齐，比较模型性能，分析误分类的语义相似性。

Result: 包含更多文本数据显著提升模型性能，微调的小型语言模型在细粒度技能对齐上优于基于嵌入的机器学习模型。

Conclusion: 微调小型语言模型在自动化项目对齐中表现优异，且通过语义分析揭示了部分技能误分类的原因，为改进模型提供了方向。

Abstract: Aligning test items to content standards is a critical step in test
development to collect validity evidence based on content. Item alignment has
typically been conducted by human experts. This judgmental process can be
subjective and time-consuming. This study investigated the performance of
fine-tuned small language models (SLMs) for automated item alignment using data
from a large-scale standardized reading and writing test for college
admissions. Different SLMs were trained for alignment at both domain and skill
levels respectively with 10 skills mapped to 4 content domains. The model
performance was evaluated in multiple criteria on two testing datasets. The
impact of types and sizes of the input data for training was investigated.
Results showed that including more item text data led to substantially better
model performance, surpassing the improvements induced by sample size increase
alone. For comparison, supervised machine learning models were trained using
the embeddings from the multilingual-E5-large-instruct model. The study results
showed that fine-tuned SLMs consistently outperformed the embedding-based
supervised machine learning models, particularly for the more fine-grained
skill alignment. To better understand model misclassifications, multiple
semantic similarity analysis including pairwise cosine similarity,
Kullback-Leibler divergence of embedding distributions, and two-dimension
projections of item embeddings were conducted. These analyses consistently
showed that certain skills in SAT and PSAT were semantically too close,
providing evidence for the observed misclassification.

</details>


### [75] [Adaptive Planning for Multi-Attribute Controllable Summarization with Monte Carlo Tree Search](https://arxiv.org/abs/2509.26435)
*Sangwon Ryu,Heejin Do,Yunsu Kim,Gary Geunbae Lee,Jungseul Ok*

Main category: cs.CL

TL;DR: 本文提出了一种名为PACO的训练自由框架，通过自适应规划序列性属性的控制顺序，实现多属性可控摘要，显著提升了摘要的多属性一致性和控制能力。


<details>
  <summary>Details</summary>
Motivation: 多属性摘要控制中属性间相互依赖，导致模型难以一致满足相关约束，同时以往方法需针对单一属性进行微调，灵活性不足。

Method: PACO通过定制的蒙特卡洛树搜索（MCTS），将摘要视为状态节点，不断调整单一属性作为动作，动态规划属性控制顺序，逐步完善摘要，避免逐属性单独微调。

Result: 大量实验显示PACO在多属性可控性上优于基于大语言模型的自我规划方法和微调基线，用小参数模型即能匹配大模型的控制性能，且随着模型增大表现更优。

Conclusion: PACO作为训练自由的多属性控制方法，有效克服了属性间依赖问题，提升了多属性摘要的控制准确度和灵活性，在多领域表现出强大的方法优势。

Abstract: Controllable summarization moves beyond generic outputs toward human-aligned
summaries guided by specified attributes. In practice, the interdependence
among attributes makes it challenging for language models to satisfy correlated
constraints consistently. Moreover, previous approaches often require
per-attribute fine-tuning, limiting flexibility across diverse summary
attributes. In this paper, we propose adaptive planning for multi-attribute
controllable summarization (PACO), a training-free framework that reframes the
task as planning the order of sequential attribute control with a customized
Monte Carlo Tree Search (MCTS). In PACO, nodes represent summaries, and actions
correspond to single-attribute adjustments, enabling progressive refinement of
only the attributes requiring further control. This strategy adaptively
discovers optimal control orders, ultimately producing summaries that
effectively meet all constraints. Extensive experiments across diverse domains
and models demonstrate that PACO achieves robust multi-attribute
controllability, surpassing both LLM-based self-planning models and fine-tuned
baselines. Remarkably, PACO with Llama-3.2-1B rivals the controllability of the
much larger Llama-3.3-70B baselines. With larger models, PACO achieves superior
control performance, outperforming all competitors.

</details>


### [76] [CreAgentive: An Agent Workflow Driven Multi-Category Creative Generation Engine](https://arxiv.org/abs/2509.26461)
*Yuyang Cheng,Linyue Cai,Changwei Peng,Yumiao Xu,Rongfang Bie,Yong Zhao*

Main category: cs.CL

TL;DR: CreAgentive是一种基于多代理工作流的多类别创作生成引擎，解决了大型语言模型在故事创作中的多个限制。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在故事、戏剧等创作中存在类型单一、输出长度不足、叙事连贯性弱及无法实现复杂结构等问题。

Method: 引入了故事原型（Story Prototype），基于知识图谱的叙事表示，利用三阶段代理工作流（初始化、生成、写作）分别构建叙事骨架、多代理对话实现故事细节，以及生成符合复杂结构的文本。

Result: CreAgentive能够低成本稳定生成大量高质量、多类型故事文本，表现优于现有强基线模型，产出质量接近人类创作水平。

Conclusion: 该方法有效提升了长篇创作的多样性、连贯性和结构复杂度，证明了基于代理工作流和知识图谱的叙事建模在自动创作中的应用潜力。

Abstract: We present CreAgentive, an agent workflow driven multi-category creative
generation engine that addresses four key limitations of contemporary large
language models in writing stories, drama and other categories of creatives:
restricted genre diversity, insufficient output length, weak narrative
coherence, and inability to enforce complex structural constructs. At its core,
CreAgentive employs a Story Prototype, which is a genre-agnostic, knowledge
graph-based narrative representation that decouples story logic from stylistic
realization by encoding characters, events, and environments as semantic
triples. CreAgentive engages a three-stage agent workflow that comprises: an
Initialization Stage that constructs a user-specified narrative skeleton; a
Generation Stage in which long- and short-term objectives guide multi-agent
dialogues to instantiate the Story Prototype; a Writing Stage that leverages
this prototype to produce multi-genre text with advanced structures such as
retrospection and foreshadowing. This architecture reduces storage redundancy
and overcomes the typical bottlenecks of long-form generation. In extensive
experiments, CreAgentive generates thousands of chapters with stable quality
and low cost (less than $1 per 100 chapters) using a general-purpose backbone
model. To evaluate performance, we define a two-dimensional framework with 10
narrative indicators measuring both quality and length. Results show that
CreAgentive consistently outperforms strong baselines and achieves robust
performance across diverse genres, approaching the quality of human-authored
novels.

</details>


### [77] [dParallel: Learnable Parallel Decoding for dLLMs](https://arxiv.org/abs/2509.26488)
*Zigeng Chen,Gongfan Fang,Xinyin Ma,Ruonan Yu,Xinchao Wang*

Main category: cs.CL

TL;DR: 本文提出dParallel方法，通过certainty-forcing蒸馏策略，实现扩散大语言模型的快速并行解码，大幅减少解码步骤并保持性能。


<details>
  <summary>Details</summary>
Motivation: 当前扩散大语言模型虽然支持并行预测，但实际并行解码效率低，原因在于掩码token的确定性收敛顺序限制了解码速度。

Method: 提出certainty-forcing蒸馏训练策略，促使模型更快且并行地对掩码token达到高确定性，继而发挥dLLM固有的并行解码潜力。

Result: 在GSM8K数据集上，dParallel将解码步骤从256减少到30，速度提升8.5倍且性能无损；在MBPP数据集上，解码步骤减至24，速度提升10.5倍，准确率维持。

Conclusion: dParallel有效释放了扩散大语言模型的并行解码能力，显著降低解码步骤，提升推理速度，同时保证模型性能不下降。

Abstract: Diffusion large language models (dLLMs) have recently drawn considerable
attention within the research community as a promising alternative to
autoregressive generation, offering parallel token prediction and lower
inference latency. Yet, their parallel decoding potential remains largely
underexplored, as existing open-source models still require nearly token-length
decoding steps to ensure performance. To address this, we introduce dParallel,
a simple and effective method that unlocks the inherent parallelism of dLLMs
for fast sampling. We identify that the key bottleneck to parallel decoding
arises from the sequential certainty convergence for masked tokens. Building on
this insight, we introduce the core of our approach: certainty-forcing
distillation, a novel training strategy that distills the model to follow its
original sampling trajectories while enforcing it to achieve high certainty on
masked tokens more rapidly and in parallel. Extensive experiments across
various benchmarks demonstrate that our method can dramatically reduce the
number of decoding steps while maintaining performance. When applied to the
LLaDA-8B-Instruct model, dParallel reduces decoding steps from 256 to 30 on
GSM8K, achieving an 8.5x speedup without performance degradation. On the MBPP
benchmark, it cuts decoding steps from 256 to 24, resulting in a 10.5x speedup
while maintaining accuracy. Our code is available at
https://github.com/czg1225/dParallel

</details>


### [78] [VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in Real-world Applications](https://arxiv.org/abs/2509.26490)
*Wei He,Yueqing Sun,Hongyan Hao,Xueyuan Hao,Zhikang Xia,Qi Gu,Chengcheng Han,Dengchang Zhao,Hui Su,Kefeng Zhang,Man Gao,Xi Su,Xiaodong Cai,Xunliang Cai,Yu Yang,Yunke Zhao*

Main category: cs.CL

TL;DR: 本文介绍了VitaBench，一个评估基于大型语言模型（LLM）智能体在复杂生活服务场景中表现的基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有评测无法反映LLM智能体处理复杂信息、调用多样资源以及动态用户交互的能力，亟需一个更具挑战性的基准。

Method: 构建包含66种工具的真实生活服务模拟环境，设计100个跨场景任务和300个单场景任务；提出基于评分量表的滑动窗口评估器，支持多路径解答的鲁棒评估。

Result: 即使是最先进的模型，在跨场景任务中的成功率仅为30%，其他任务成功率不足50%。

Conclusion: VitaBench为推动现实应用中AI智能体的发展提供了重要资源，有助于提升模型应对复杂交互和多任务的能力。

Abstract: As LLM-based agents are increasingly deployed in real-life scenarios,
existing benchmarks fail to capture their inherent complexity of handling
extensive information, leveraging diverse resources, and managing dynamic user
interactions. To address this gap, we introduce VitaBench, a challenging
benchmark that evaluates agents on versatile interactive tasks grounded in
real-world settings. Drawing from daily applications in food delivery, in-store
consumption, and online travel services, VitaBench presents agents with the
most complex life-serving simulation environment to date, comprising 66 tools.
Through a framework that eliminates domain-specific policies, we enable
flexible composition of these scenarios and tools, yielding 100 cross-scenario
tasks (main results) and 300 single-scenario tasks. Each task is derived from
multiple real user requests and requires agents to reason across temporal and
spatial dimensions, utilize complex tool sets, proactively clarify ambiguous
instructions, and track shifting user intent throughout multi-turn
conversations. Moreover, we propose a rubric-based sliding window evaluator,
enabling robust assessment of diverse solution pathways in complex environments
and stochastic interactions. Our comprehensive evaluation reveals that even the
most advanced models achieve only 30% success rate on cross-scenario tasks, and
less than 50% success rate on others. Overall, we believe VitaBench will serve
as a valuable resource for advancing the development of AI agents in practical
real-world applications. The code, dataset, and leaderboard are available at
https://vitabench.github.io/

</details>


### [79] [BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs](https://arxiv.org/abs/2509.26514)
*Yue Wang,Ruotian Ma,Xingyu Chen,Zhengliang Shi,Wanshun Chen,Huang Liu,Jiadi Yao,Qu Yang,Qingxuan Jiang,Fanghua Ye,Juntao Li,Min Zhang,Zhaopeng Tu,Xiaolong Li,Linus*

Main category: cs.CL

TL;DR: 本文提出了一种名为BatonVoice的新框架，通过将大语言模型（LLM）用作“指挥”，生成文本化的声学特征规划，再由专门的语音合成模型生成语音，实现了高效且可控的文本到语音转换。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的语音合成方法未能充分利用其强大的指令理解能力，导致无法有效进行可控的文本到语音转换。

Method: 借鉴“运算主义”思路，将指令理解与语音生成分离，设计BatonVoice框架使LLM生成文本化的声学特征计划，配合专门训练的BatonTTS模型进行语音合成。

Result: BatonVoice在可控和情感语音合成上表现优异，显著超越开放和闭源基线模型，并具备出色的零样本跨语言泛化能力。

Conclusion: 通过将语音对象化为文本化的声学特征，有效激活了大语言模型的语言理解能力，提升了可控语音合成的性能和泛化能力。

Abstract: The rise of Large Language Models (LLMs) is reshaping multimodel models, with
speech synthesis being a prominent application. However, existing approaches
often underutilize the linguistic intelligence of these models, typically
failing to leverage their powerful instruction-following capabilities. This
limitation hinders the model's ability to follow text instructions for
controllable Text-to-Speech~(TTS). To address this, we propose a new paradigm
inspired by ``operationalism'' that decouples instruction understanding from
speech generation. We introduce BatonVoice, a framework where an LLM acts as a
``conductor'', understanding user instructions and generating a textual
``plan'' -- explicit vocal features (e.g., pitch, energy). A separate TTS
model, the ``orchestra'', then generates the speech from these features. To
realize this component, we develop BatonTTS, a TTS model trained specifically
for this task. Our experiments demonstrate that BatonVoice achieves strong
performance in controllable and emotional speech synthesis, outperforming
strong open- and closed-source baselines. Notably, our approach enables
remarkable zero-shot cross-lingual generalization, accurately applying feature
control abilities to languages unseen during post-training. This demonstrates
that objectifying speech into textual vocal features can more effectively
unlock the linguistic intelligence of LLMs.

</details>


### [80] [Training Matryoshka Mixture-of-Experts for Elastic Inference-Time Expert Utilization](https://arxiv.org/abs/2509.26520)
*Yaoxiang Wang,Qingguo Hu,Yucheng Ding,Ruizhe Wang,Yeyun Gong,Jian Jiao,Yelong Shen,Peng Cheng,Jinsong Su*

Main category: cs.CL

TL;DR: 本文提出了Matryoshka MoE (M-MoE)，一种通过层级精细化专家激活训练框架，提升混合专家模型在弹性推理中的性能稳定性和适应性。


<details>
  <summary>Details</summary>
Motivation: 传统的Top-K路由训练方法导致在推理时调整激活专家数量会显著降低模型性能，限制了MoE模型的弹性推理能力。

Method: M-MoE框架通过在训练时系统地变化激活专家数量，促使模型学习专家的有意义排序，从粗到细提供信息；并采用层级随机化策略，实现不同专家组合的协同工作。

Result: 单一的M-MoE模型在不同激活专家数量下的性能接近多个专门模型的表现，但只需更少的训练成本。

Conclusion: M-MoE实现了MoE模型的高弹性和性能优化，支持根据计算预算调整不同层使用的计算资源，促进大型MoE模型的更实用和灵活部署。

Abstract: Mixture-of-Experts (MoE) has emerged as a promising paradigm for efficiently
scaling large language models without a proportional increase in computational
cost. However, the standard training strategy of Top-K router prevents MoE
models from realizing their full potential for elastic inference. When the
number of activated experts is altered at inference time, these models exhibit
precipitous performance degradation. In this work, we introduce Matryoshka MoE
(M-MoE), a training framework that instills a coarse-to-fine structure directly
into the expert ensemble. By systematically varying the number of activated
experts during training, M-MoE compels the model to learn a meaningful ranking:
top-ranked experts collaborate to provide essential, coarse-grained
capabilities, while subsequent experts add progressively finer-grained detail.
We explore this principle at multiple granularities, identifying a layer-wise
randomization strategy as the most effective. Our experiments demonstrate that
a single M-MoE model achieves remarkable elasticity, with its performance at
various expert counts closely matching that of an entire suite of specialist
models, but at only a fraction of the total training cost. This flexibility not
only unlocks elastic inference but also enables optimizing performance by
allocating different computational budgets to different model layers. Our work
paves the way for more practical and adaptable deployments of large-scale MoE
models.

</details>


### [81] [OceanGym: A Benchmark Environment for Underwater Embodied Agents](https://arxiv.org/abs/2509.26536)
*Yida Xue,Mingjun Mao,Xiangyuan Ru,Yuqi Zhu,Baochang Ren,Shuofei Qiao,Mengru Wang,Shumin Deng,Xinyu An,Ningyu Zhang,Ying Chen,Huajun Chen*

Main category: cs.CL

TL;DR: OceanGym是首个旨在推进水下智能体研究的综合基准，涵盖八个真实任务域，采用多模态大语言模型驱动的统一架构，促进水下感知与决策发展。


<details>
  <summary>Details</summary>
Motivation: 水下环境因低能见度和动态洋流等极端条件，使得智能体感知和决策极具挑战，缺乏针对水下场景的综合评测平台。

Method: 设计OceanGym基准，包括八个现实任务和基于多模态大语言模型的统一智能体框架，整合视觉和声呐感知、记忆与序列决策，实现自主探索和长时目标达成。

Result: 实验揭示当前多模态大语言模型驱动的智能体与人类专家存在显著差距，反映出水下感知、规划和适应性方面的持续难题。

Conclusion: OceanGym提供高保真平台，为开发稳健的水下智能体及其向真实水下自主载具的迁移奠定基础，是实现智能体在地球最后未被充分探测前沿环境中运行的重要一步。

Abstract: We introduce OceanGym, the first comprehensive benchmark for ocean underwater
embodied agents, designed to advance AI in one of the most demanding real-world
environments. Unlike terrestrial or aerial domains, underwater settings present
extreme perceptual and decision-making challenges, including low visibility,
dynamic ocean currents, making effective agent deployment exceptionally
difficult. OceanGym encompasses eight realistic task domains and a unified
agent framework driven by Multi-modal Large Language Models (MLLMs), which
integrates perception, memory, and sequential decision-making. Agents are
required to comprehend optical and sonar data, autonomously explore complex
environments, and accomplish long-horizon objectives under these harsh
conditions. Extensive experiments reveal substantial gaps between
state-of-the-art MLLM-driven agents and human experts, highlighting the
persistent difficulty of perception, planning, and adaptability in ocean
underwater environments. By providing a high-fidelity, rigorously designed
platform, OceanGym establishes a testbed for developing robust embodied AI and
transferring these capabilities to real-world autonomous ocean underwater
vehicles, marking a decisive step toward intelligent agents capable of
operating in one of Earth's last unexplored frontiers. The code and data are
available at https://github.com/OceanGPT/OceanGym.

</details>


### [82] [The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models](https://arxiv.org/abs/2509.26543)
*Lina Conti,Dennis Fucci,Marco Gaido,Matteo Negri,Guillaume Wisniewski,Luisa Bentivogli*

Main category: cs.CL

TL;DR: 本文提出了首个用于语音转文本生成模型的对比解释方法，通过分析输入声谱图的部分对输出选择的影响，帮助理解模型选择特定输出的原因。


<details>
  <summary>Details</summary>
Motivation: 传统解释方法难以有效揭示语音转文本模型为何选择特定输出，且对比解释在该领域尚未实现。

Method: 借鉴特征归因技术，分析输入声谱图不同部分对输出选择的影响，提出一种生成对比解释的新方法。

Result: 通过在性别分配任务中的案例研究，方法准确识别影响模型选择性别输出的关键音频特征。

Conclusion: 该方法拓展了对比解释在语音转文本领域的应用，为更深入理解此类模型提供了基础。

Abstract: Contrastive explanations, which indicate why an AI system produced one output
(the target) instead of another (the foil), are widely regarded in explainable
AI as more informative and interpretable than standard explanations. However,
obtaining such explanations for speech-to-text (S2T) generative models remains
an open challenge. Drawing from feature attribution techniques, we propose the
first method to obtain contrastive explanations in S2T by analyzing how parts
of the input spectrogram influence the choice between alternative outputs.
Through a case study on gender assignment in speech translation, we show that
our method accurately identifies the audio features that drive the selection of
one gender over another. By extending the scope of contrastive explanations to
S2T, our work provides a foundation for better understanding S2T models.

</details>


### [83] [Towards Reliable Benchmarking: A Contamination Free, Controllable Evaluation Framework for Multi-step LLM Function Calling](https://arxiv.org/abs/2509.26553)
*Seiji Maekawa,Jackson Hassell,Pouya Pezeshkpour,Tom Mitchell,Estevam Hruschka*

Main category: cs.CL

TL;DR: 本文提出了FuncBenchGen框架，用于生成合成的多步骤工具调用任务，评估具备调用外部工具能力的语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有的工具增强语言模型评测基准在函数数量、任务复杂度及输入规模等方面缺乏有效控制，且容易受数据泄漏影响，难以全面评估模型的工具调用能力。

Method: 设计了一个基于隐藏函数依赖有向无环图（DAG）的任务生成框架，允许精确控制任务难度和避免数据泄漏。评测了七个大语言模型在不同难度工具调用任务上的表现，并提出通过在每一步显式重述先前变量值以改善模型状态跟踪的策略。

Result: 推理优化模型明显优于通用模型，GPT-5表现最佳。随着依赖深度增加，模型表现显著下降。连接代码中无关函数时，模型更易出错。显式重述变量值策略使模型表现有显著提升，GPT-5成功率从62.5%提升至81.3%。

Conclusion: FuncBenchGen为多步骤工具调用任务提供了高效且无污染的评价框架，揭示了当前语言模型在状态跟踪方面的脆弱性，且通过简单的重述变量值策略有效地提升了模型性能。

Abstract: As language models gain access to external tools via structured function
calls, they become increasingly more capable of solving complex, multi-step
tasks. However, existing benchmarks for tool-augmented language models (TaLMs)
provide insufficient control over factors such as the number of functions
accessible, task complexity, and input size, and remain vulnerable to data
contamination. We present FuncBenchGen, a unified, contamination-free framework
that evaluates TaLMs by generating synthetic multi-step tool-use tasks. The key
idea is to cast tool use as traversal over a hidden function-dependency DAG
where nodes are function calls and an edge between nodes represents one
function consuming the output of another. Given a set of external function
schemas, initial variable values, and a target variable, models must compose
the correct call sequence to compute the target variable. FuncBenchGen allows
users to precisely control task difficulty (e.g., graph size, dependency depth,
and distractor functions) while avoiding data leakage. We apply our
FuncBenchGen framework to evaluate seven LLMs on tool use tasks of varying
difficulty. Reasoning-optimized models consistently outperform general-purpose
models with GPT-5 significantly outperforming other models. Performance
declines sharply as dependency depth increases. Furthermore, connected
irrelevant functions prove especially difficult to handle. We find that strong
models often make syntactically valid function calls but propagate incorrect or
stale argument values across steps, revealing brittle state tracking by LLMs in
multi-turn tool use. Motivated by this observation, we introduce a simple
mitigation strategy that explicitly restates prior variable values to the agent
at each step. Surprisingly, this lightweight change yields substantial gains
across models. e.g., yielding a success rate improvement from 62.5% to 81.3%
for GPT-5.

</details>


### [84] [Generating Difficult-to-Translate Texts](https://arxiv.org/abs/2509.26592)
*Vilém Zouhar,Wenda Xu,Parker Riley,Juraj Juraska,Mara Finkelstein,Markus Freitag,Dan Deutsch*

Main category: cs.CL

TL;DR: 提出了一种利用大语言模型迭代生成高难度机器翻译测试样例的方法，提高了测试集对模型区分能力和难点揭示效果。


<details>
  <summary>Details</summary>
Motivation: 现有机器翻译基准测试数据大多过于简单，难以区分不同模型的优劣或揭示模型弱点，且现有制造难例的方法要么难度不够，要么缺乏多样性和自然性。

Method: 借鉴人类专家不断尝试发现模型失败点的过程，设计MT-breaker方法，使用大语言模型反复修改源文本，借助目标机器翻译模型的反馈生成难度更高且保持自然多样性的测试样例。

Result: 生成的测试案例对目标模型具有更高挑战性，同时难度可迁移至其他机器翻译模型和语言。

Conclusion: MT-breaker有效地提升了机器翻译测试数据的挑战性和多样性，有助于更好地评估和分析不同翻译模型表现。

Abstract: Machine translation benchmarks sourced from the real world are quickly
obsoleted, due to most examples being easy for state-of-the-art translation
models. This limits the benchmark's ability to distinguish which model is
better or to reveal models' weaknesses. Current methods for creating difficult
test cases, such as subsampling or from-scratch synthesis, either fall short of
identifying difficult examples or suffer from a lack of diversity and
naturalness. Inspired by the iterative process of human experts probing for
model failures, we propose MT-breaker, a method where a large language model
iteratively refines a source text to increase its translation difficulty. The
LLM iteratively queries a target machine translation model to guide its
generation of difficult examples. Our approach generates examples that are more
challenging for the target MT model while preserving the diversity of natural
texts. While the examples are tailored to a particular machine translation
model during the generation, the difficulty also transfers to other models and
languages.

</details>


### [85] [Deconstructing Self-Bias in LLM-generated Translation Benchmarks](https://arxiv.org/abs/2509.26600)
*Wenda Xu,Sweta Agrawal,Vilém Zouhar,Markus Freitag,Daniel Deutsch*

Main category: cs.CL

TL;DR: 该论文发现自动生成的基准测试集（由大语言模型生成）对生成该基准的模型存在系统性偏见，尤其在低资源语言到英语的翻译任务中明显。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型性能趋于饱和，使用其自动生成基准测试集成为快速、低成本的评估替代方案，但需验证其公平性和有效性。

Method: 通过分析LLM生成的测试数据和评估方法，研究了自偏差的来源及其在不同语言方向翻译任务中的表现，并探讨了源文本多样性对自偏差的影响。

Result: 发现自偏差主要来源于生成的测试数据和评估方式，且在模型生成能力较强的英语为目标语言的任务中偏见更加明显。此外，源文本多样性不足加剧自偏差。

Conclusion: 提高生成源文本多样性有助于减轻自偏差问题，从而提升自动生成基准测试的公平性和有效性。

Abstract: As large language models (LLMs) begin to saturate existing benchmarks,
automated benchmark creation using LLMs (LLM as a benchmark) has emerged as a
scalable alternative to slow and costly human curation. While these generated
test sets have to potential to cheaply rank models, we demonstrate a critical
flaw. LLM generated benchmarks systematically favor the model that created the
benchmark, they exhibit self bias on low resource languages to English
translation tasks. We show three key findings on automatic benchmarking of LLMs
for translation: First, this bias originates from two sources: the generated
test data (LLM as a testset) and the evaluation method (LLM as an evaluator),
with their combination amplifying the effect. Second, self bias in LLM as a
benchmark is heavily influenced by the model's generation capabilities in the
source language. For instance, we observe more pronounced bias in into English
translation, where the model's generation system is developed, than in out of
English translation tasks. Third, we observe that low diversity in source text
is one attribution to self bias. Our results suggest that improving the
diversity of these generated source texts can mitigate some of the observed
self bias.

</details>


### [86] [MENLO: From Preferences to Proficiency -- Evaluating and Modeling Native-like Quality Across 47 Languages](https://arxiv.org/abs/2509.26601)
*Chenxi Whitehouse,Sebastian Ruder,Tony Lin,Oksana Kurylo,Haruka Takagi,Janice Lam,Nicolò Busetto,Denise Diaz*

Main category: cs.CL

TL;DR: 本文提出MENLO框架，利用受众设计机制评估多语言大语言模型的母语响应质量，并创建了涵盖47种语言品种的高一致性注释数据集。通过对比分析，零样本评判虽获益于成对评价和结构化评分，但仍不及人工评价，细调强化学习方法明显提升了评判性能。MENLO还能训练生成奖励模型，促进多语言能力提升。


<details>
  <summary>Details</summary>
Motivation: 确保大语言模型在多语言环境下生成母语般高质量的响应是一个挑战，需要有效评价工具和数据集来指导改进。

Method: 设计并实现基于受众设计机制的MENLO评价框架，构建包含6423对高一致性偏好标注的多维度质量对比数据集，采用零样本大语言模型评判、强化学习微调、多任务学习等技术手段提升评判效果。

Result: 零样本评判结合成对评价和结构化评分提升显著，但仍低于人工评判；引入强化学习、奖励塑形及多任务学习方法后评判性能大幅提升；训练的奖励模型可作为生成奖励模型，增强大语言模型的多语言表现。

Conclusion: MENLO为多语言大语言模型质量评价提供了可扩展的框架与数据支持，展示了强化学习等方法在提升评判准确性和模型多语言能力中的潜力，有助于推动多语言模型的偏好对齐和质量提升。

Abstract: Ensuring native-like quality of large language model (LLM) responses across
many languages is challenging. To address this, we introduce MENLO, a framework
that operationalizes the evaluation of native-like response quality based on
audience design-inspired mechanisms. Using MENLO, we create a dataset of 6,423
human-annotated prompt-response preference pairs covering four quality
dimensions with high inter-annotator agreement in 47 language varieties. Our
evaluation reveals that zero-shot LLM judges benefit significantly from
pairwise evaluation and our structured annotation rubrics, yet they still
underperform human annotators on our dataset. We demonstrate substantial
improvements through fine-tuning with reinforcement learning, reward shaping,
and multi-task learning approaches. Additionally, we show that RL-trained
judges can serve as generative reward models to enhance LLMs' multilingual
proficiency, though discrepancies with human judgment remain. Our findings
suggest promising directions for scalable multilingual evaluation and
preference alignment. We release our dataset and evaluation framework to
support further research in multilingual LLM evaluation.

</details>


### [87] [DeepScientist: Advancing Frontier-Pushing Scientific Findings Progressively](https://arxiv.org/abs/2509.26603)
*Yixuan Weng,Minjun Zhu,Qiujie Xie,Qiyao Sun,Zhen Lin,Sifan Liu,Yue Zhang*

Main category: cs.CL

TL;DR: DeepScientist 系统通过贝叶斯优化方法实现了长时间自主科学发现，生成并验证大量科学假设，显著超越人类现有顶尖方法。


<details>
  <summary>Details</summary>
Motivation: 现有 AI 科学家系统虽能生成新发现，但缺乏针对人类重要科学问题的聚焦和贡献，需开发一个能长期自主、目标导向的科学发现系统。

Method: 将科学发现问题形式化为贝叶斯优化，通过层级评估流程（假设、验证、分析），利用累积发现记忆在探索新假设与利用优质假设间智能平衡。

Result: 系统运行超2万 GPU 小时，生成约5000个独特科学想法，验证约1100个，分别在三大 AI 任务上超越人类设计的顶尖方法183.7%、1.9%和7.9%。

Conclusion: 首次大规模证实 AI 可以在科学任务中持续超越人类顶尖水平，推动科学发现前沿，同时开放源码促进相关研究发展。

Abstract: While previous AI Scientist systems can generate novel findings, they often
lack the focus to produce scientifically valuable contributions that address
pressing human-defined challenges. We introduce DeepScientist, a system
designed to overcome this by conducting goal-oriented, fully autonomous
scientific discovery over month-long timelines. It formalizes discovery as a
Bayesian Optimization problem, operationalized through a hierarchical
evaluation process consisting of "hypothesize, verify, and analyze". Leveraging
a cumulative Findings Memory, this loop intelligently balances the exploration
of novel hypotheses with exploitation, selectively promoting the most promising
findings to higher-fidelity levels of validation. Consuming over 20,000 GPU
hours, the system generated about 5,000 unique scientific ideas and
experimentally validated approximately 1100 of them, ultimately surpassing
human-designed state-of-the-art (SOTA) methods on three frontier AI tasks by
183.7\%, 1.9\%, and 7.9\%. This work provides the first large-scale evidence of
an AI achieving discoveries that progressively surpass human SOTA on scientific
tasks, producing valuable findings that genuinely push the frontier of
scientific discovery. To facilitate further research into this process, we will
open-source all experimental logs and system code at
https://github.com/ResearAI/DeepScientist/.

</details>


### [88] [Searching for Difficult-to-Translate Test Examples at Scale](https://arxiv.org/abs/2509.26619)
*Wenda Xu,Vilém Zouhar,Parker Riley,Mara Finkelstein,Markus Freitag,Daniel Deutsch*

Main category: cs.CL

TL;DR: 本文将寻找文本测试数据中最难话题的问题形式化为多臂老虎机问题，提出基于带权策略的方法，更高效地识别最难话题，并在机器翻译任务中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 自然语言处理模型需要具有挑战性的测试数据，而数据难度与其所属话题相关，然而话题数量巨大，穷举所有话题代价极高，因此需要高效的难题发现方法。

Method: 将不同话题视为多臂老虎机的“老虎机臂”，每次抽取样本相当于拉一次机器臂，通过评估样本的难度来更新选择策略，在有限计算预算下高效识别最难话题。

Result: 通过在机器翻译任务中的实验，使用多种带权策略识别最难话题的效果远超传统的暴力搜索方法。

Conclusion: 将寻找最难话题的问题视为多臂老虎机问题是一种有效的解决方案，能显著提升找到挑战性测试样本的效率。

Abstract: NLP models require test data that are sufficiently challenging. The
difficulty of an example is linked to the topic it originates from (''seed
topic''). The relationship between the topic and the difficulty of its
instances is stochastic in nature: an example about a difficult topic can
happen to be easy, and vice versa. At the scale of the Internet, there are tens
of thousands of potential topics, and finding the most difficult one by drawing
and evaluating a large number of examples across all topics is computationally
infeasible. We formalize this task and treat it as a multi-armed bandit
problem. In this framework, each topic is an ''arm,'' and pulling an arm (at a
cost) involves drawing a single example, evaluating it, and measuring its
difficulty. The goal is to efficiently identify the most difficult topics
within a fixed computational budget. We illustrate the bandit problem setup of
finding difficult examples for the task of machine translation. We find that
various bandit strategies vastly outperform baseline methods like brute-force
searching the most challenging topics.

</details>


### [89] [Scaling Spoken Language Models with Syllabic Speech Tokenization](https://arxiv.org/abs/2509.26634)
*Nicholas Lee,Cheol Jun Cho,Alan W Black,Gopala K. Anumanchipalli*

Main category: cs.CL

TL;DR: 本文研究了基于音节级标记的口语语言模型，发现其在保证性能的同时大幅降低了训练和推理成本。


<details>
  <summary>Details</summary>
Motivation: 传统口语语言模型使用高帧率音频标记导致Transformer自注意力机制计算成本高昂，因此需要探索更加高效的表示方法。

Method: 系统性评估基于音节级标记的口语语言模型，在不同训练数据规模和多种语音理解基准上进行实验比较。

Result: 音节级标记的模型在准确性上与高帧率标记持平或更优，同时训练时间减少超过2倍，计算量减少5倍。

Conclusion: 音节级语言建模是实现高效长上下文口语语言模型的有前景的途径。

Abstract: Spoken language models (SLMs) typically discretize speech into
high-frame-rate tokens extracted from SSL speech models. As the most successful
LMs are based on the Transformer architecture, processing these long token
streams with self-attention is expensive, as attention scales quadratically
with sequence length. A recent SSL work introduces acoustic tokenization of
speech at the syllable level, which is more interpretable and potentially more
scalable with significant compression in token lengths (4-5 Hz). Yet, their
value for spoken language modeling is not yet fully explored. We present the
first systematic study of syllabic tokenization for spoken language modeling,
evaluating models on a suite of SLU benchmarks while varying training data
scale. Syllabic tokens can match or surpass the previous high-frame rate tokens
while significantly cutting training and inference costs, achieving more than a
2x reduction in training time and a 5x reduction in FLOPs. Our findings
highlight syllable-level language modeling as a promising path to efficient
long-context spoken language models.

</details>


### [90] [Convergence and Divergence of Language Models under Different Random Seeds](https://arxiv.org/abs/2509.26643)
*Finlay Fehlauer,Kyle Mahowald,Tiago Pimentel*

Main category: cs.CL

TL;DR: 本文研究了在不同随机种子下训练的语言模型的收敛性，发现模型大小和训练阶段影响收敛模式。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型在不同随机种子下训练时的收敛性及其影响因素，理解模型训练稳定性的本质。

Method: 通过测量不同随机种子下模型的逐词KL散度，比较不同模型大小及训练检查点的收敛表现，分析不同词频和词性标签对收敛性的影响。

Result: 发现了四阶段收敛模式：初始均匀、快速收敛、快速发散和缓慢重新收敛；且大模型在后期训练中收敛更快，小模型未真正重新收敛。高频词和功能词比低频词和内容词收敛更快更稳定。

Conclusion: 语言模型的训练收敛性受模型规模、训练阶段及词语类别影响，大模型更易学到稳定分布，揭示了训练稳定性的关键因素。

Abstract: In this paper, we investigate the convergence of language models (LMs)
trained under different random seeds, measuring convergence as the expected
per-token Kullback--Leibler (KL) divergence across seeds. By comparing LM
convergence as a function of model size and training checkpoint, we identify a
four-phase convergence pattern: (i) an initial uniform phase, (ii) a
sharp-convergence phase, (iii) a sharp-divergence phase, and (iv) a
slow-reconvergence phase. Further, we observe that larger models reconverge
faster in later training stages, while smaller models never actually
reconverge; these results suggest that a certain model size may be necessary to
learn stable distributions. Restricting our analysis to specific token
frequencies or part-of-speech (PoS) tags further reveals that convergence is
uneven across linguistic categories: frequent tokens and function words
converge faster and more reliably than their counterparts (infrequent tokens
and content words). Overall, our findings highlight factors that influence the
stability of the learned distributions in model training.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [91] [WARP -- Web-Augmented Real-time Program Repairer: A Real-Time Compilation Error Resolution using LLMs and Web-Augmented Synthesis](https://arxiv.org/abs/2509.25192)
*Anderson de Lima Luiz*

Main category: cs.SE

TL;DR: 本文提出了WARP系统，利用大语言模型和动态网络信息，实现实时编译错误自动修复，提高修复率和代码正确性。


<details>
  <summary>Details</summary>
Motivation: 编译错误严重影响软件开发效率，现有方法效果有限，需要结合大语言模型与最新网络资源实现更高效准确的错误修复。

Method: WARP实时监控开发者终端，结合微调的代码大语言模型与来自开发者论坛和官方文档的动态网络代码片段与解释进行错误检测和综合修复。

Result: 在包含C/C++、Python、Go错误的基准测试CGP上，WARP修复率达72.5%，修复效果和语义正确率均优于仅用大语言模型及传统IDE快速修复方法。

Conclusion: 通过融合网络动态信息与大语言模型，WARP显著提升了编译错误修复的准确性与实时性，有效缓解开发瓶颈。

Abstract: Compilation errors represent a significant bottleneck in software development
productivity. This paper introduces WARP (Web-Augmented Real-time Program
Repairer), a novel system that leverages Large Language Models (LLMs) and
dynamic web-augmented synthesis for real-time resolution of these errors. WARP
actively monitors developer terminals, intelligently detects compilation
errors, and synergistically combines the understanding of a fine-tuned Code-LLM
with relevant solutions, explanations, and code snippets retrieved from
up-to-date web sources like developer forums and official documentation.
Experimental results on our curated benchmark, CGP (featuring C/C++, Python,
and Go errors), demonstrate WARP achieves a superior fix rate (72.5 % Compiles
correctly) and higher semantic correctness compared to baseline LLM-only
approaches and traditional IDE quick-fixes. Key technical challenges in
achieving high-accuracy synthesis from noisy web data.

</details>


### [92] [Devstral: Fine-tuning Language Models for Coding Agent Applications](https://arxiv.org/abs/2509.25193)
*Abhinav Rastogi,Adam Yang,Albert Q. Jiang,Alexander H. Liu,Alexandre Sablayrolles,Amélie Héliou,Amélie Martin,Anmol Agarwal,Andy Ehrenberg,Andy Lo,Antoine Roux,Arthur Darcet,Arthur Mensch,Baptiste Bout,Baptiste Rozière,Baudouin De Monicault,Chris Bamford,Christian Wallenwein,Christophe Renaudin,Clémence Lanfranchi,Clément Denoix,Corentin Barreau,Darius Dabert Devon Mizelle,Diego de las Casas,Elliot Chane-Sane,Emilien Fugier,Emma Bou Hanna,Gabrielle Berrada,Gauthier Delerce,Gauthier Guinet,Georgii Novikov,Graham Neubig,Guillaume Lample,Guillaume Martin,Himanshu Jaju,Jan Ludziejewski,Jason Rute,Jean-Malo Delignon,JeanHadrien Chabran,Joachim Studnia,Joep Barmentlo,Jonas Amar,Josselin Somerville Roberts,Julien Denize,Karan Saxena,Karmesh Yadav,Kartik Khandelwal,Khyathi Raghavi Chandu,Kush Jain,Lélio Renard Lavaud,Léonard Blier,Lingxiao Zhao,Louis Martin,Lucile Saulnier,Luyu Gao,Marie Pellat,Mathilde Guillaumin,Mathis Felardos,Matthieu Dinot,Maxime Darrin,Maximilian Augustin,Mickaël Seznec,Neha Gupta,Nikhil Raghuraman,Olivier Duchenne,Patricia Wang,Patrick von Platen,Patryk Saffer,Paul Jacob,Paul Wambergue,Paula Kurylowicz,Philomène Chagniot,Pierre Stock,Pravesh Agrawal,Rémi Delacourt,Roman Soletskyi,Romain Sauvestre,Sagar Vaze,Sanchit Gandhi,Sandeep Subramanian,Shashwat Dalal,Siddharth Gandhi,Soham Ghosh,Srijan Mishra,Sumukh Aithal,Szymon Antoniak,Teven Le Scao,Thibaut Lavril,Thibault Schueller,Thomas Foubert,Thomas Robert,Thomas Wang,Timothée Lacroix,Tom Bewley,Valeriia Nemychnikova,Victor Paltz,Virgile Richard,Wen-Ding Li,William Marshall,Xingyao Wang,Xuanyu Zhang,Yihan Wan,Yunhao Tang*

Main category: cs.SE

TL;DR: 本文介绍了Devstral-Small模型，这是一种体积小于100B的轻量级开源代码代理模型，性能优异。


<details>
  <summary>Details</summary>
Motivation: 为了开发一个体积小、性能优良且易于部署的代码代理模型。

Method: 设计并开发了一个24B参数的模型，专注于软件代理开发的专业化需求。

Result: Devstral-Small模型虽小，但性能与更大规模模型相比仍具竞争力，且响应速度快，易于服务。

Conclusion: Devstral-Small展示了小型模型在保持高性能的同时具备高效服务能力，适合资源有限的应用环境。

Abstract: We introduce Devstral-Small, a lightweight open source model for code agents
with the best performance among models below 100B size. In this technical
report, we give an overview of how we design and develop a model and craft
specializations in agentic software development. The resulting model,
Devstral-Small is a small 24B model, fast and easy to serve. Despite its size,
Devstral-Small still attains competitive performance compared to models more
than an order of magnitude larger.

</details>


### [93] [Automated Code Development for PDE Solvers Using Large Language Models](https://arxiv.org/abs/2509.25194)
*Haoyang Wu,Xinxin Zhang,Lailai Zhu*

Main category: cs.SE

TL;DR: 本文提出了LLM-PDEveloper，一个利用大型语言模型自动开发偏微分方程（PDE）数值库代码的多智能体框架，实现了从数学描述到源码的端到端转换，支持新求解器的生成和现有模块的改进。


<details>
  <summary>Details</summary>
Motivation: 利用大型语言模型的跨领域知识和推理能力，自动化数值库开发，特别是针对PDE的代码生成，提升软件开发效率并扩展库的功能。

Method: 采用零样本多智能体框架，通过将数学和算法描述直接转化为源码，实现新求解器的构建、新边界条件的实现和现有求解器的修改，形成自我扩展的代码生成流程。

Result: 在三个任务上进行了验证，取得了中等的成功率，分析了语法错误的成因并提出修复方法，同时探讨了语义错误背后的机制。

Conclusion: LLM-PDEveloper展示了利用大型语言模型自动化PDE库代码开发的潜力，提出了有效的错误修复策略和未来研究方向。

Abstract: Foundation models -- large language models (LLMs) in particular -- have
become ubiquitous, shaping daily life and driving breakthroughs across science,
engineering, and technology. Harnessing their broad cross-domain knowledge,
text-processing, and reasoning abilities for software development, e.g.,
numerical libraries for solving partial differential equations (PDEs), is
therefore attracting growing interest. Yet existing studies mainly automate
case setup and execution for end users. We introduce LLM-PDEveloper, a
zero-shot, multi-agent LLM framework that automates code development for PDE
libraries, specifically targeting secondary developers. By translating
mathematical and algorithmic descriptions directly into source code,
LLM-PDEveloper generates new solvers/modules and adapts existing ones. This
end-to-end math-to-code approach enables a self-augmenting pipeline that
continuously expands the codebase of a library, extends its capacities, and
broadens its scope. We demonstrate LLM-PDEveloper on three tasks: 1) build a
solver for a new PDE, 2) implement new BCs for a given PDE, and 3) modify an
existing solver to incorporate additional terms, achieving moderate success
rates. Failures due to syntactic errors made by LLMs are analyzed and we
propose effective fixes. We also identify the mechanisms underlying certain
semantic errors, guiding future research.

</details>


### [94] [Understanding Practitioners Perspectives on Monitoring Machine Learning Systems](https://arxiv.org/abs/2509.25195)
*Hira Naveed,John Grundy,Chetan Arora,Hourieh Khalajzadeh,Omar Haggag*

Main category: cs.SE

TL;DR: 本文通过对91名机器学习从业者的全球调查，分析了机器学习系统监控的现状、挑战及改进方向。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统的非确定性可能导致生产环境中的不可预见和危险行为，因此及时监控以防止财务和声誉损失至关重要。

Method: 通过全球问卷调查收集从业者视角的定性和定量数据，分析当前监控策略、面临的挑战及改进需求。

Result: 发现从业者主要遇到性能下降、延迟超标、安全风险等运行时问题，且自动化监控难以普及，监控设置复杂且增加额外负担。

Conclusion: 未来监控工具应着重自动化部署、性能与公平性监控支持及问题解决建议，更好满足从业者需求。

Abstract: Given the inherent non-deterministic nature of machine learning (ML) systems,
their behavior in production environments can lead to unforeseen and
potentially dangerous outcomes. For a timely detection of unwanted behavior and
to prevent organizations from financial and reputational damage, monitoring
these systems is essential. This paper explores the strategies, challenges, and
improvement opportunities for monitoring ML systems from the practitioners
perspective. We conducted a global survey of 91 ML practitioners to collect
diverse insights into current monitoring practices for ML systems. We aim to
complement existing research through our qualitative and quantitative analyses,
focusing on prevalent runtime issues, industrial monitoring and mitigation
practices, key challenges, and desired enhancements in future monitoring tools.
Our findings reveal that practitioners frequently struggle with runtime issues
related to declining model performance, exceeding latency, and security
violations. While most prefer automated monitoring for its increased
efficiency, many still rely on manual approaches due to the complexity or lack
of appropriate automation solutions. Practitioners report that the initial
setup and configuration of monitoring tools is often complicated and
challenging, particularly when integrating with ML systems and setting alert
thresholds. Moreover, practitioners find that monitoring adds extra workload,
strains resources, and causes alert fatigue. The desired improvements from the
practitioners perspective are: automated generation and deployment of monitors,
improved support for performance and fairness monitoring, and recommendations
for resolving runtime issues. These insights offer valuable guidance for the
future development of ML monitoring tools that are better aligned with
practitioners needs.

</details>


### [95] [APRIL: API Synthesis with Automatic Prompt Optimization and Reinforcement Learning](https://arxiv.org/abs/2509.25196)
*Hua Zhong,Shan Jiang,Sarfraz Khurshid*

Main category: cs.SE

TL;DR: 本文提出了一种结合自动提示优化和基于可验证奖励强化学习的大型语言模型API合成方法，显著提升了科学Python库中复杂API合成的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现代软件开发中，API组合面临指数级搜索空间挑战，传统方法成本高且需手工规范；尽管大型语言模型能从自然语言生成代码，但存在假象生成和上下文信息限制，导致代码错误。

Method: 提出APRIL方法，通过自动提示优化(APO)迭代优化冻结模型的提示，同时利用基于可验证奖励的强化学习(RLVR)微调策略以提高功能正确性，形成高效合成流水线。

Result: 在81个真实科学Python库的API上测试，APRIL相比仅指令调优的大型语言模型和专家提示，表现出显著改进。

Conclusion: 将APO和RLVR相结合，为大型库中基于组件的API合成提供了鲁棒且可扩展的解决方案。

Abstract: APIs are central to modern software development, yet composing new APIs from
large libraries is difficult due to the exponential search space; traditional
component-based synthesis relies on costly exploration and hand-crafted
specifications. While large language models (LLMs) can generate implementations
from natural language, hallucinations and limited access to up-to-date
contextual information often yield incorrect code. In this paper, we present
APRIL, an approach that combines LLM-based synthesis with Automatic Prompt
Optimization (APO) and Reinforcement Learning from Verifiable Rewards (RLVR):
APO iteratively refines prompts for a frozen model, while RLVR fine-tunes the
policy toward functional correctness, producing an efficient synthesis
pipeline. Evaluated on 81 real-world APIs from widely used scientific Python
libraries and benchmarked against instruction-tuned but unfine-tuned LLMs
guided by expert prompts, APRIL achieves substantial improvements. These
results indicate that integrating APO and RLVR provides a robust, scalable path
for component-based API synthesis in large libraries.

</details>


### [96] [Towards Repository-Level Program Verification with Large Language Models](https://arxiv.org/abs/2509.25197)
*Si Cheng Zhong,Xujie Si*

Main category: cs.SE

TL;DR: 本文提出了RVBench和RagVerus，解决了大型语言模型在跨模块和全局上下文形式化验证中的挑战，实现了对复杂开源项目的规模化代码验证。


<details>
  <summary>Details</summary>
Motivation: 现有基于大型语言模型的方法集中于函数级验证，忽视了跨模块依赖和全局上下文，难以应用于实际大型软件项目的形式化验证。

Method: 提出了RVBench作为首个针对仓库级评价的验证基准；引入RagVerus框架结合检索增强生成和上下文感知提示，实现多模块仓库的自动化证明合成。

Result: RagVerus在有限模型推理预算下，基准测试中的通过率提升至三倍，在更具挑战性的RVBench上提升27%。

Conclusion: 该工作展示了一个高效、可扩展的规模化多模块代码验证解决方案，显著提升了自动化形式化验证的广度和深度。

Abstract: Recent advancements in large language models (LLMs) suggest great promises in
code and proof generations. However, scaling automated formal verification to
real-world projects requires resolving cross-module dependencies and global
contexts, which are crucial challenges overlooked by existing LLM-based methods
with a special focus on targeting isolated, function-level verification tasks.
To systematically explore and address the significant challenges of verifying
entire software repositories, we introduce RVBench, the first verification
benchmark explicitly designed for repository-level evaluation, constructed from
four diverse and complex open-source Verus projects.
  We further introduce RagVerus, an extensible framework that synergizes
retrieval-augmented generation with context-aware prompting to automate proof
synthesis for multi-module repositories. RagVerus triples proof pass rates on
existing benchmarks under constrained model inference budgets, and achieves a
27% relative improvement on the more challenging RVBench benchmark,
demonstrating a scalable and sample-efficient verification solution.

</details>


### [97] [CircInspect: Integrating Visual Circuit Analysis, Abstraction, and Real-Time Development in Quantum Debugging](https://arxiv.org/abs/2509.25199)
*Mushahid Khan,Prashant J. Nair,Olivia Di Matteo*

Main category: cs.SE

TL;DR: 本文提出了CircInspect，一款用于调试Python和PennyLane量子程序的交互式工具，帮助用户分析量子电路、监控输出和可视化结构。


<details>
  <summary>Details</summary>
Motivation: 量子计算的概率性质、独特算法和硬件噪声给量子软件开发带来了新的调试挑战，传统调试工具难以应对。

Method: 设计并实现了CircInspect，集成断点、实时开发功能，允许用户分析孤立的量子电路组件，监控程序输出，结构化可视化和信息抽象。

Result: CircInspect能有效支持量子程序的调试，增强用户对于量子电路结构和执行结果的理解。

Conclusion: CircInspect为量子软件开发提供了一种新的交互式调试手段，提升开发效率和程序可理解性。

Abstract: Software bugs typically result from errors in specifications or code
translation. While classical software engineering has evolved with various
tools and methodologies to tackle such bugs, the emergence of quantum computing
presents unique challenges. Quantum software development introduces
complexities due to the probabilistic nature of quantum computing, distinct
algorithmic primitives, and potential hardware noise. In this paper, we
introduce CircInspect, an interactive tool tailored for debugging quantum
programs in Python and PennyLane. By leveraging breakpoints and real-time
software development features, \toolname~empowers users to analyze isolated
quantum circuit components, monitor program output, visualize structural
changes, and abstract information to enhance comprehension.

</details>


### [98] [Generating High-Quality Datasets for Code Editing via Open-Source Language Models](https://arxiv.org/abs/2509.25203)
*Zekai Zhang,Mingwei Liu,Zhenxi Chen,Linxi Liang,Yuxuan Chen,Guangsheng Ou,Yanlin Wang,Dan Li,Xin Peng,Zibin Zheng*

Main category: cs.SE

TL;DR: 本文提出了CanItEdit流水线和OCEDataFT数据集，利用多种大模型合成高质量代码编辑数据，显著提升模型代码编辑性能，接近GPT-4水平。


<details>
  <summary>Details</summary>
Motivation: 现有基于提交的代码编辑数据集噪声大、多样性不足，且不能反映真实编辑指令风格，影响模型性能。

Method: 设计CanItEdit开源流水线，融合多种大语言模型合成真实代码编辑三元组，生成简洁的和详细的指令，并基于差异和话题进行筛选，构建20K样本的OCEDataFT数据集。

Result: 以OCEDataFT微调三种先进基础模型，在CanItEdit基准测试中性能显著提升，pass@1相对增益4.50%至20.79%，达到接近闭源系统的水平。

Conclusion: 该方法无需专有资源或人工标注，即能有效合成高质量代码编辑数据，显著提升模型性能，缩小与GPT-4的差距。

Abstract: Code editing plays a vital role in software engineering, requiring developers
to adjust existing code according to natural language instructions while
keeping functionality intact and avoiding unnecessary modifications. However,
commit-based datasets commonly used for this task are often noisy, lack
diversity, and fail to reflect the style of real-world edit instructions. To
address this, we introduce CanItEdit, an open-source pipeline that leverages
multiple LLMs to synthesize realistic code-edit triplets. The pipeline produces
both concise "lazy" instructions and more detailed "descriptive" ones, and
applies filtering based on diffs and topics to guarantee data quality and
variety. Using this process, we construct OCEDataFT, a curated dataset of 20K
samples. Fine-tuning three advanced base models on OCEDataFT leads to
significant performance boosts on the CanItEdit benchmark, with relative pass@1
improvements ranging from 4.50% to 20.79%. Notably, the resulting models
achieve performance close to closed-source systems, narrowing the gap to GPT-4
to just 3.54%, without relying on proprietary resources or manual annotation.

</details>


### [99] [A Benchmark for Localizing Code and Non-Code Issues in Software Projects](https://arxiv.org/abs/2509.25242)
*Zejun Zhang,Jian Wang,Qingyun Yang,Yifan Pan,Yi Tang,Yi Li,Zhenchang Xing,Tian Zhang,Xuandong Li,Guoan Zhang*

Main category: cs.SE

TL;DR: 本文提出了MULocBench，一个涵盖1100个问题、46个GitHub Python项目的综合性问题定位数据集，旨在弥补现有数据集忽略多样证据和非代码文件的不足。


<details>
  <summary>Details</summary>
Motivation: 现有问题定位数据集主要关注拉取请求和代码位置，缺乏对提交、评论、配置和文档等非代码文件的支持，限制了问题定位能力的评估。

Method: 构建MULocBench数据集，涵盖多样的问题类型、根本原因、定位范围和文件类型；基于此数据集评估当前最先进的定位方法和5种基于大语言模型的提示策略。

Result: 发现现有方法在文件级别准确率和F1指标均低于40%，表明当前技术难以广泛适用于现实的多方面问题定位场景。

Conclusion: MULocBench为项目定位问题提供了更真实和多样的测试环境，现有方法存在显著不足，公开数据集将促进该领域未来研究。

Abstract: Accurate project localization (e.g., files and functions) for issue
resolution is a critical first step in software maintenance. However, existing
benchmarks for issue localization, such as SWE-Bench and LocBench, are limited.
They focus predominantly on pull-request issues and code locations, ignoring
other evidence and non-code files such as commits, comments, configurations,
and documentation. To address this gap, we introduce MULocBench, a
comprehensive dataset of 1,100 issues from 46 popular GitHub Python projects.
Comparing with existing benchmarks, MULocBench offers greater diversity in
issue types, root causes, location scopes, and file types, providing a more
realistic testbed for evaluation. Using this benchmark, we assess the
performance of state-of-the-art localization methods and five LLM-based
prompting strategies. Our results reveal significant limitations in current
techniques: even at the file level, performance metrics (Acc@5, F1) remain
below 40%. This underscores the challenge of generalizing to realistic,
multi-faceted issue resolution. To enable future research on project
localization for issue resolution, we publicly release MULocBench at
https://huggingface.co/datasets/somethingone/MULocBench.

</details>


### [100] [Reinforcement Learning-Guided Chain-of-Draft for Token-Efficient Code Generation](https://arxiv.org/abs/2509.25243)
*Xunzhu Tang,Iyiola Emmanuel Olatunji,Tiezhu Sun,Jacques Klein,Tegawende F. Bissyande*

Main category: cs.SE

TL;DR: 本文提出了一种名为MultiCoD的强化学习框架，通过策略引导的提示生成多样化代码推理候选，并从中选择最优解决方案，以提升大型语言模型代码生成的正确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在结构化推理任务中表现不足，链式思维提示(CoT)虽改进推理但效率低，链式草稿提示(CoD)更简洁却因生成的解质量不稳定，难以选出最优解。

Method: 采用策略引导提示促进多样推理，结合上下文赌博机模型的强化学习方法，优化代码复杂度、推理结构和策略元数据特征，通过奖励函数平衡正确性、效率和清晰度，实现多候选解的选择。

Result: 在MBPP、BigCodeBench、SWE-bench Verified和Defects4J等基准测试中，MultiCoD性能优于或不逊于标准提示、CoT和CoD方法，同时通过仅对选中输出计费，实现用户账单降低50%以上，提升响应质量。

Conclusion: MultiCoD框架提高了大型语言模型代码推理的正确性与效率，兼顾成本和用户体验，具备可持续性和实际部署潜力。源码已公开。

Abstract: LLMs demonstrate surface-level fluency in code generation but struggle with
structured reasoning tasks requiring correctness and semantic alignment. While
Chain-of-Thought (CoT) prompting enhances reasoning through intermediate steps,
it suffers from verbosity and inefficiency. Chain-of-Draft (CoD) prompting
offers more concise reasoning, but the stochastic nature of LLMs produces
varying solution quality, making optimal selection challenging. We propose
\multicod, a reinforcement learning framework that learns to select the most
promising candidate from CoD-generated solutions. Our approach uses
strategy-guided prompting to encourage diverse reasoning styles and models
solution selection as a contextual bandit problem. The framework optimizes
interpretable features including code complexity, reasoning structure, and
strategic metadata through a reward function balancing correctness, efficiency,
and clarity. Experiments on MBPP, BigCodeBench, SWE-bench Verified, and
Defects4J show \multicod~outperforms and in some cases, on par with standard
prompting, CoT, and CoD baselines while achieving cost and token efficiency
from the user's perspective through a multi-candidate design that charges only
for the selected output, reducing user billing by over 50\% and improving LLM
response quality, making \multicod~more sustainable and scalable for real-world
deployment. Our code is available: https://anonymous.4open.science/r/MultiCoD.

</details>


### [101] [Protocode: Prototype-Driven Interpretability for Code Generation in LLMs](https://arxiv.org/abs/2509.25247)
*Krishna Vamshi Bodla,Haizhao Yang*

Main category: cs.SE

TL;DR: 本文研究了如何自动选择上下文学习（ICL）示例来提升大语言模型（LLMs）在代码生成任务中的性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在代码生成领域的广泛应用，如何减少代码质量问题及提升模型表现成为关键挑战。

Method: 通过基于AST的分析，自动检测代码中受ICL示例影响的区域，并筛选高质量的演示示例以增强模型表现。

Result: 高质量的ICL示例显著提升了代码生成的可解释性和pass@10指标表现，低质量示例则导致性能下降。

Conclusion: 高效的ICL示例采样策略对提升代码生成模型的性能和输出质量具有重要作用。

Abstract: Since the introduction of Large Language Models (LLMs), they have been widely
adopted for various tasks such as text summarization, question answering,
speech-to-text translation, and more. In recent times, the use of LLMs for code
generation has gained significant attention, with tools such as Cursor and
Windsurf demonstrating the ability to analyze massive code repositories and
recommend relevant changes. Big tech companies have also acknowledged the
growing reliance on LLMs for code generation within their codebases. Although
these advances significantly improve developer productivity, increasing
reliance on automated code generation can proportionally increase the risk of
suboptimal solutions and insecure code. Our work focuses on automatically
sampling In-Context Learning (ICL) demonstrations which can improve model
performance and enhance the interpretability of the generated code. Using
AST-based analysis on outputs from the MBPP test set, we identify regions of
code most influenced by the chosen demonstrations. In our experiments, we show
that high-quality ICL demonstrations not only make outputs easier to interpret
but also yield a positive performance improvement on the pass@10 metric.
Conversely, poorly chosen ICL demonstrations affected the LLM performance on
the pass@10 metric negatively compared to the base model. Overall, our approach
highlights the importance of efficient sampling strategies for ICL, which can
affect the performance of the model on any given task.

</details>


### [102] [BuildBench: Benchmarking LLM Agents on Compiling Real-World Open-Source Software](https://arxiv.org/abs/2509.25248)
*Zehua Zhang,Ati Priya Bajaj,Divij Handa,Siyu Liu,Arvind S Raj,Hongkai Chen,Hulin Wang,Yibo Liu,Zion Leonahenahe Basque,Souradip Nath,Vishal Juneja,Nikhil Chapre,Yan Shoshitaishvili,Adam Doupé,Chitta Baral,Ruoyu Wang*

Main category: cs.SE

TL;DR: 该论文提出了一个更具挑战性和真实性的开源软件自动编译基准BUILD-BENCH，并设计了一个基于大语言模型的强基线代理OSS-BUILD-AGENT，实现了对多样化OSS自动编译的先进性能。


<details>
  <summary>Details</summary>
Motivation: 自动编译开源软件复杂且任务繁重，现有方法依赖手工规则，难以适应多样化的开源项目，且现有评测方法不够真实。

Method: 提出BUILD-BENCH基准用于评估真实复杂的OSS编译，设计具备增强编译指令检索的OSS-BUILD-AGENT代理，通过分析不同编译方法设计选择优化系统性能。

Result: OSS-BUILD-AGENT在BUILD-BENCH上实现了最新的性能，并适应了异构开源软件的特性。

Conclusion: BUILD-BENCH能真实反映代理处理复杂软件工程任务的能力，将推动软件开发与安全领域的下游应用创新。

Abstract: Automatically compiling open-source software (OSS) projects is a vital,
labor-intensive, and complex task, which makes it a good challenge for LLM
Agents. Existing methods rely on manually curated rules and workflows, which
cannot adapt to OSS that requires customized configuration or environment
setup. Recent attempts using Large Language Models (LLMs) used selective
evaluation on a subset of highly rated OSS, a practice that underestimates the
realistic challenges of OSS compilation. In practice, compilation instructions
are often absent, dependencies are undocumented, and successful builds may even
require patching source files or modifying build scripts. We propose a more
challenging and realistic benchmark, BUILD-BENCH, comprising OSS that are more
diverse in quality, scale, and characteristics. Furthermore, we propose a
strong baseline LLM-based agent, OSS-BUILD-AGENT, an effective system with
enhanced build instruction retrieval module that achieves state-of-the-art
performance on BUILD-BENCH and is adaptable to heterogeneous OSS
characteristics. We also provide detailed analysis regarding different
compilation method design choices and their influence to the whole task,
offering insights to guide future advances. We believe performance on
BUILD-BENCH can faithfully reflect an agent's ability to tackle compilation as
a complex software engineering tasks, and, as such, our benchmark will spur
innovation with a significant impact on downstream applications in the fields
of software development and software security.

</details>


### [103] [RANGER -- Repository-Level Agent for Graph-Enhanced Retrieval](https://arxiv.org/abs/2509.25257)
*Pratik Shah,Rajat Ghosh,Aryan Singhal,Debojyoti Dutta*

Main category: cs.SE

TL;DR: RANGER是一个面向代码实体查询和自然语言查询的代码检索系统，通过构建代码仓库级知识图，结合快速查询和蒙特卡洛树搜索，实现了多任务的代码检索性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前自动化软件工程任务需要一种能够同时处理代码实体查询和自然语言查询的代码检索工具，而现有研究主要集中在代码实体查询上，缺乏对自然语言查询的支持。

Method: 构建覆盖整个代码仓库的知识图，包含层级和跨文件依赖信息，并用文本描述和嵌入增强节点。检索采用双阶段流程：代码实体查询使用快速Cypher查询，自然语言查询通过蒙特卡洛树搜索探索知识图。

Result: 在CodeSearchNet、RepoQA、RepoBench和CrossCodeEval四个基准数据集上，RANGER在代码搜索、问答、跨文件依赖检索和仓库级代码补全任务中均优于多种强基线模型。

Conclusion: RANGER有效填补了代码实体查询和自然语言查询兼容的空白，通过综合知识图和双阶段检索策略，提升了代码检索在多任务中的表现。

Abstract: General-purpose automated software engineering (ASE) includes tasks such as
code completion, retrieval, repair, QA, and summarization. These tasks require
a code retrieval system that can handle specific queries about code entities,
or code entity queries (for example, locating a specific class or retrieving
the dependencies of a function), as well as general queries without explicit
code entities, or natural language queries (for example, describing a task and
retrieving the corresponding code). We present RANGER, a repository-level code
retrieval agent designed to address both query types, filling a gap in recent
works that have focused primarily on code-entity queries. We first present a
tool that constructs a comprehensive knowledge graph of the entire repository,
capturing hierarchical and cross-file dependencies down to the variable level,
and augments graph nodes with textual descriptions and embeddings to bridge the
gap between code and natural language. RANGER then operates on this graph
through a dual-stage retrieval pipeline. Entity-based queries are answered
through fast Cypher lookups, while natural language queries are handled by
MCTS-guided graph exploration. We evaluate RANGER across four diverse
benchmarks that represent core ASE tasks including code search, question
answering, cross-file dependency retrieval, and repository-level code
completion. On CodeSearchNet and RepoQA it outperforms retrieval baselines that
use embeddings from strong models such as Qwen3-8B. On RepoBench, it achieves
superior cross-file dependency retrieval over baselines, and on CrossCodeEval,
pairing RANGER with BM25 delivers the highest exact match rate in code
completion compared to other RAG methods.

</details>


### [104] [Automatically Generating Web Applications from Requirements Via Multi-Agent Test-Driven Development](https://arxiv.org/abs/2509.25297)
*Yuxuan Wan,Tingshuo Liang,Jiakai Xu,Jingyu Xiao,Yintong Huo,Michael R. Lyu*

Main category: cs.SE

TL;DR: 提出了TDDev，一个支持测试驱动开发的多模态大语言模型代理框架，实现了从自然语言或设计图自动生成端到端全栈Web应用，显著提升了生成应用的准确性和功能完整性。


<details>
  <summary>Details</summary>
Motivation: 全栈Web应用开发复杂且耗时，现有多模态大语言模型虽能自动生成前端网页，但仅限前端，未能实现完整功能的应用。

Method: TDDev通过自动生成可执行测试用例，结合自然语言或设计图输入，迭代生成前后端代码并模拟用户交互，持续优化直到满足所有需求。

Result: 在多样化应用场景中，TDDev较最先进基线整体准确率提升了14.4%，有效提高了生成应用的可靠性和质量。

Conclusion: TDDev成功解决了全栈自动化中的需求不明确、多文件依赖和功能及视觉准确性问题，实现了无需人工干预的高质量全栈Web应用自动生成。

Abstract: Developing full-stack web applications is complex and time-intensive,
demanding proficiency across diverse technologies and frameworks. Although
recent advances in multimodal large language models (MLLMs) enable automated
webpage generation from visual inputs, current solutions remain limited to
front-end tasks and fail to deliver fully functional applications. In this
work, we introduce TDDev, the first test-driven development (TDD)-enabled
LLM-agent framework for end-to-end full-stack web application generation. Given
a natural language description or design image, TDDev automatically derives
executable test cases, generates front-end and back-end code, simulates user
interactions, and iteratively refines the implementation until all requirements
are satisfied. Our framework addresses key challenges in full-stack automation,
including underspecified user requirements, complex interdependencies among
multiple files, and the need for both functional correctness and visual
fidelity. Through extensive experiments on diverse application scenarios, TDDev
achieves a 14.4% improvement on overall accuracy compared to state-of-the-art
baselines, demonstrating its effectiveness in producing reliable, high-quality
web applications without requiring manual intervention.

</details>


### [105] [Detecting and Fixing API Misuses of Data Science Libraries Using Large Language Models](https://arxiv.org/abs/2509.25378)
*Akalanka Galappaththi,Francisco Ribeiro,Sarah Nadi*

Main category: cs.SE

TL;DR: 本文提出了基于大语言模型的API误用检测与修复工具DSCHECKER，针对数据科学库的API误用问题进行研究。


<details>
  <summary>Details</summary>
Motivation: 数据科学库（如scikit-learn、pandas）以数据处理为核心，API误用检测较为困难，亟需有效方法提高检测与修复准确率。

Method: 引入API指令和数据信息来辅助检测和修复API误用，利用三种大语言模型及五个数据科学库的误用样本进行多种提示实验，并实现了具备自适应函数调用机制的DSCHECKER agent来模拟实际场景。

Result: 综合利用API指令和数据细节后，检测F1值达到61.18%，修复率51.28%。在真实场景模拟中，DSCHECKER agent实现了48.65%的检测F1和39.47%的修复率。

Conclusion: 基于大语言模型的方法在数据科学库API误用检测与修复中展示出较大潜力，尤其是在缺乏预先信息的实际应用场景中也表现良好。

Abstract: Data science libraries, such as scikit-learn and pandas, specialize in
processing and manipulating data. The data-centric nature of these libraries
makes the detection of API misuse in them more challenging. This paper
introduces DSCHECKER, an LLM-based approach designed for detecting and fixing
API misuses of data science libraries. We identify two key pieces of
information, API directives and data information, that may be beneficial for
API misuse detection and fixing. Using three LLMs and misuses from five data
science libraries, we experiment with various prompts. We find that
incorporating API directives and data-specific details enhances Dschecker's
ability to detect and fix API misuses, with the best-performing model achieving
a detection F1-score of 61.18 percent and fixing 51.28 percent of the misuses.
Building on these results, we implement Dschecker agent which includes an
adaptive function calling mechanism to access information on demand, simulating
a real-world setting where information about the misuse is unknown in advance.
We find that Dschecker agent achieves 48.65 percent detection F1-score and
fixes 39.47 percent of the misuses, demonstrating the promise of LLM-based API
misuse detection and fixing in real-world scenarios.

</details>


### [106] [A Cartography of Open Collaboration in Open Source AI: Mapping Practices, Motivations, and Governance in 14 Open Large Language Model Projects](https://arxiv.org/abs/2509.25397)
*Johan Linåker,Cailean Osborne,Jennifer Ding,Ben Burtenshaw*

Main category: cs.SE

TL;DR: 本文通过对14个开放大语言模型(LLM)开发者的访谈，探讨了开放LLM的协作方式及其生命周期管理，揭示了多元的协作内容、动机和组织模式。


<details>
  <summary>Details</summary>
Motivation: 当前对开放大语言模型的协作方式，尤其是其开发前后如何组织和治理，缺乏系统研究，限制了对开放LLM生态系统建设的理解与支持。

Method: 通过对来自不同地区和机构的14个开放LLM项目开发者进行半结构化访谈，深入分析开放LLM协作贯穿开发及重用的全过程。

Result: 发现开放LLM协作涉及模型外的多种资源如数据集、基准测试、开源框架以及计算资源合作等；开发者动机多样，包括促进AI民主化、开放科学、地区生态建设和多语言支持；存在五种不同组织模型，覆盖从单一公司项目到非盈利草根项目，体现了不同的控制和社区参与策略。

Conclusion: 为支持全球AI开放生态建设，提出了基于研究发现的实用建议，促进更开放、包容和多样化的AI未来。

Abstract: The proliferation of open large language models (LLMs) is fostering a vibrant
ecosystem of research and innovation in artificial intelligence (AI). However,
the methods of collaboration used to develop open LLMs both before and after
their public release have not yet been comprehensively studied, limiting our
understanding of how open LLM projects are initiated, organized, and governed
as well as what opportunities there are to foster this ecosystem even further.
We address this gap through an exploratory analysis of open collaboration
throughout the development and reuse lifecycle of open LLMs, drawing on
semi-structured interviews with the developers of 14 open LLMs from grassroots
projects, research institutes, startups, and Big Tech companies in North
America, Europe, Africa, and Asia. We make three key contributions to research
and practice. First, collaboration in open LLM projects extends far beyond the
LLMs themselves, encompassing datasets, benchmarks, open source frameworks,
leaderboards, knowledge sharing and discussion forums, and compute
partnerships, among others. Second, open LLM developers have a variety of
social, economic, and technological motivations, from democratizing AI access
and promoting open science to building regional ecosystems and expanding
language representation. Third, the sampled open LLM projects exhibit five
distinct organizational models, ranging from single company projects to
non-profit-sponsored grassroots projects, which vary in their centralization of
control and community engagement strategies used throughout the open LLM
lifecycle. We conclude with practical recommendations for stakeholders seeking
to support the global community building a more open future for AI.

</details>


### [107] [PIPer: On-Device Environment Setup via Online Reinforcement Learning](https://arxiv.org/abs/2509.25455)
*Alexander Kovrigin,Aleksandra Eliseeva,Konstantin Grotov,Egor Bogomolov,Yaroslav Zharov*

Main category: cs.SE

TL;DR: 本文提出了一种结合监督微调和可验证奖励强化学习的方法，针对环境配置任务优化大型语言模型，实现了在环境搭建任务上的高效自动化。


<details>
  <summary>Details</summary>
Motivation: 环境配置是软件工程中的一大难题，自动化环境配置能帮助开发者和研究人员减少人工配置工作，提高测试基准的可扩展性，但现有大型语言模型在该任务中的表现有限。

Method: 通过监督微调生成准确的Bash脚本，并利用具有可验证奖励的强化学习进一步优化模型，使其更适合环境配置任务。

Result: 采用EnvBench-Python数据集测试，Qwen3-8B模型经过训练后性能媲美更大规模的Qwen3-32B和GPT-4o模型。

Conclusion: 本研究通过结合监督微调和强化学习，有效提升了自动化环境配置的准确性和实用性，且模型可在普通硬件上运行，促进了该领域的发展。

Abstract: Environment setup-the process of configuring the system to work with a
specific software project-represents a persistent challenge in Software
Engineering (SE). Automated environment setup methods could assist developers
by providing fully configured environments for arbitrary repositories without
manual effort. This also helps SE researchers to scale execution-based
benchmarks. However, recent studies reveal that even state-of-the-art Large
Language Models (LLMs) achieve limited success in automating this task. To
address this limitation, we tune a specialized model for environment setup. We
combine supervised fine-tuning for generating correct Bash scripts and
Reinforcement Learning with Verifiable Rewards (RLVR) to adapt it to the task
of environment setup. On EnvBench-Python, our method enables Qwen3-8B (a model
runnable on consumer hardware) to perform on par with larger models-Qwen3-32B
and GPT-4o. The training code and model checkpoints are available online:
https://github.com/JetBrains-Research/PIPer.

</details>


### [108] [BloomAPR: A Bloom's Taxonomy-based Framework for Assessing the Capabilities of LLM-Powered APR Solutions](https://arxiv.org/abs/2509.25465)
*Yinghang Ma,Jiho Shin,Leuson Da Silva,Zhen Ming,Jiang,Song Wang,Foutse Khomh,Shin Hwei Tan*

Main category: cs.SE

TL;DR: 本文提出了基于Bloom认知层级的新型大语言模型驱动自动程序修复（APR）动态评测框架BloomAPR，针对现有静态基准的不足，系统评估了三种大型语言模型及其驱动的APR工具在不同认知层级的修复能力。


<details>
  <summary>Details</summary>
Motivation: 现有APR评测多基于静态数据集，存在数据污染风险及评测环境单一，难以全面反映大语言模型在动态、多样化上下文中的修复能力。

Method: 设计了基于布鲁姆认知分类的动态评测框架BloomAPR，将APR任务划分为不同认知层次，结合Defects4J，评测ChatRepair与CigaR在GPT-3.5-Turbo、Llama-3.1和StarCoder-2三款LLM辅助下的表现。

Result: 研究发现三种LLM驱动的APR工具在基础记忆性修复上表现良好（记忆层81.57%），对合成漏洞的理解能力有所提升（理解层提升60.66%），但在细微语法修复（应用层43.32%）及现实项目多样性分析（分析层仅13.46%-41.34%）存在瓶颈。

Conclusion: 结果表明现有静态基准评测存在不足，呼吁发展更动态、多维的评测方法，为更可信赖的大语言模型软件工程解决方案提供合理评价基石。

Abstract: Recent advances in large language models (LLMs) have accelerated the
development of AI-driven automated program repair (APR) solutions. However,
these solutions are typically evaluated using static benchmarks such as
Defects4J and SWE-bench, which suffer from two key limitations: (1) the risk of
data contamination, potentially inflating evaluation results due to overlap
with LLM training data, and (2) limited ability to assess the APR capabilities
in dynamic and diverse contexts. In this paper, we introduced BloomAPR, a novel
dynamic evaluation framework grounded in Bloom's Taxonomy. Our framework offers
a structured approach to assess the cognitive capabilities of LLM-powered APR
solutions across progressively complex reasoning levels. Using Defects4J as a
case study, we evaluated two state-of-the-art LLM-powered APR solutions,
ChatRepair and CigaR, under three different LLMs: GPT-3.5-Turbo, Llama-3.1, and
StarCoder-2. Our findings show that while these solutions exhibit basic
reasoning skills and effectively memorize bug-fixing patterns (fixing up to
81.57% of bugs at the Remember layer), their performance increases with
synthetically generated bugs (up to 60.66% increase at the Understand layer).
However, they perform worse on minor syntactic changes (fixing up to 43.32% at
the Apply layer), and they struggle to repair similar bugs when injected into
real-world projects (solving only 13.46% to 41.34% bugs at the Analyze layer).
These results underscore the urgent need for evolving benchmarks and provide a
foundation for more trustworthy evaluation of LLM-powered software engineering
solutions.

</details>


### [109] [AGNOMIN -- Architecture Agnostic Multi-Label Function Name Prediction](https://arxiv.org/abs/2509.25514)
*Yonatan Gizachew Achamyeleh,Tongtao Zhang,Joshua Hyunki Kim,Gabriel Garcia,Shih-Yuan Yu,Anton Kocheturov,Mohammad Abdullah Al Faruque*

Main category: cs.SE

TL;DR: AGNOMIN 是一种新颖的架构无关多标签函数名预测方法，通过融合多种图结构和层次图神经网络，大幅提升了剥除符号函数的名称预测精度和召回率，支持跨架构泛化，实用于安全漏洞分析。


<details>
  <summary>Details</summary>
Motivation: 当前函数名预测方法存在架构限制、数据稀缺和命名多样性问题，难以有效支持多架构下的二进制反向工程和漏洞修复。

Method: 提出了基于特征丰富的层次图结构（FEHG），结合控制流图、函数调用图及动态学习的PCode特征，利用层次图神经网络生成一致的函数表达。采用改进的Renée解码器和注意力机制提升预测效果。

Result: 在包含三种架构的9000个ELF二进制文件上进行评测，AGNOMIN在精确率和召回率方面分别提升至27.17%和55.86%，对未知架构的泛化能力也优于现有方法。

Conclusion: AGNOMIN有效解决了架构依赖和数据不足问题，实现准确、高效的函数名预测，已在安全攻防实战中得到验证，促进了跨架构的漏洞分析与修复。

Abstract: Function name prediction is crucial for understanding stripped binaries in
software reverse engineering, a key step for \textbf{enabling subsequent
vulnerability analysis and patching}. However, existing approaches often
struggle with architecture-specific limitations, data scarcity, and diverse
naming conventions. We present AGNOMIN, a novel architecture-agnostic approach
for multi-label function name prediction in stripped binaries. AGNOMIN builds
Feature-Enriched Hierarchical Graphs (FEHGs), combining Control Flow Graphs,
Function Call Graphs, and dynamically learned \texttt{PCode} features. A
hierarchical graph neural network processes this enriched structure to generate
consistent function representations across architectures, vital for
\textbf{scalable security assessments}. For function name prediction, AGNOMIN
employs a Ren\'ee-inspired decoder, enhanced with an attention-based head layer
and algorithmic improvements.
  We evaluate AGNOMIN on a comprehensive dataset of 9,000 ELF executable
binaries across three architectures, demonstrating its superior performance
compared to state-of-the-art approaches, with improvements of up to 27.17\% in
precision and 55.86\% in recall across the testing dataset. Moreover, AGNOMIN
generalizes well to unseen architectures, achieving 5.89\% higher recall than
the closest baseline. AGNOMIN's practical utility has been validated through
security hackathons, where it successfully aided reverse engineers in analyzing
and patching vulnerable binaries across different architectures.

</details>


### [110] [M&SCheck: Towards a Checklist to Support Software Engineering Newcomers to the Modeling and Simulation Area](https://arxiv.org/abs/2509.25625)
*Luiza Martins de Freitas Cintra,Philipp Zech,Mohamad Kassab,Eliomar Araújo Lima,Sofia Larissa da Costa Paiva,Valdemar Vicente Graciano Neto*

Main category: cs.SE

TL;DR: 本文提出了一份初步的检查表，帮助新手软件工程师选择建模与仿真（M&S）中合适的形式方法，针对数字孪生、智慧城市和工业4.0/5.0等复杂系统。


<details>
  <summary>Details</summary>
Motivation: 随着复杂动态生态系统的发展，软件开发中越来越需要将建模与仿真纳入生命周期，但新手工程师常面临选择合适形式方法的困惑。

Method: 基于三种主要形式方法（DEVS、系统动力学和基于代理的仿真）构建检查表，并通过试点研究和专家咨询进行验证。

Result: 检查表建议与原始研究中使用的形式方法高度一致，专家反馈积极。

Conclusion: 该初步检查表有助于引导新手选择合适的建模与仿真范式，提升软件开发中仿真应用的有效性。

Abstract: The advent of increasingly complex and dynamic ecosystems, such as digital
twins (DT), smart cities and Industry 4.0 and 5.0, has made evident the need to
include modeling and simulation (M&S) in the software development life cycle.
Such disruptive systems include simulation models in their own architecture
(such as DT) or require the use of simulation models to represent the high
degree of movement and the multiplicity of interactions that occur between the
involved systems. However, when software engineers (particularly the newcomers)
need to use M&S in their projects, they often pose themselves an important
question: which formalism should I use? In this direction, the main
contribution of this paper is the establishment of a preliminary checklist with
questions to assist beginners in M&S in choosing the most appropriate paradigm
to solve their problems. The checklist is based on three main formalisms: DEVS,
System Dynamics and Agent-Based Simulation. A pilot study was carried out and
an expert was consulted. The preliminary results show (i) conformance between
the suggestion given by the checklist and the formalism selected in the
original studies used as input for evaluating the checklist, and (ii) a
positive feedback from the expert.

</details>


### [111] [Explainable Fault Localization for Programming Assignments via LLM-Guided Annotation](https://arxiv.org/abs/2509.25676)
*Fang Liu,Tianze Wang,Li Zhang,Zheyu Yang,Jing Jiang,Zian Sun*

Main category: cs.SE

TL;DR: FLAME是一种面向编程作业的细粒度、可解释的故障定位方法，通过LLM引导注释和模型集成，提升了错误定位的准确性和教育价值。


<details>
  <summary>Details</summary>
Motivation: 现有故障定位技术在教育场景中粒度过粗，缺乏可操作的反馈，且直接预测错误代码行号的方法不适合大型语言模型（LLM）。

Method: FLAME引导LLM对错误代码行进行带解释的标注，并采用加权多模型投票策略融合多种LLM的输出，提高定位精度和可靠性。

Result: FLAME在编程作业中相比最佳基线多定位了207个故障，且在Defects4J通用软件缺陷库上同样优于所有基线方法。

Conclusion: FLAME有效提升了故障定位的精细度和可解释性，适用于教育和通用软件场景，具有较高的实用价值。

Abstract: Providing timely and personalized guidance for students' programming
assignments, offers significant practical value for helping students complete
assignments and enhance their learning. In recent years, various automated
Fault Localization (FL) techniques have demonstrated promising results in
identifying errors in programs. However, existing FL techniques face challenges
when applied to educational contexts. Most approaches operate at the method
level without explanatory feedback, resulting in granularity too coarse for
students who need actionable insights to identify and fix their errors. While
some approaches attempt line-level fault localization, they often depend on
predicting line numbers directly in numerical form, which is ill-suited to
LLMs. To address these challenges, we propose FLAME, a fine-grained,
explainable Fault Localization method tailored for programming assignments via
LLM-guided Annotation and Model Ensemble. FLAME leverages rich contextual
information specific to programming assignments to guide LLMs in identifying
faulty code lines. Instead of directly predicting line numbers, we prompt the
LLM to annotate faulty code lines with detailed explanations, enhancing both
localization accuracy and educational value. To further improve reliability, we
introduce a weighted multi-model voting strategy that aggregates results from
multiple LLMs to determine the suspiciousness of each code line. Extensive
experimental results demonstrate that FLAME outperforms state-of-the-art fault
localization baselines on programming assignments, successfully localizing 207
more faults at top-1 over the best-performing baseline. Beyond educational
contexts, FLAME also generalizes effectively to general-purpose software
codebases, outperforming all baselines on the Defects4J benchmark.

</details>


### [112] [DeepCodeSeek: Real-Time API Retrieval for Context-Aware Code Generation](https://arxiv.org/abs/2509.25716)
*Esakkivel Esakkiraja,Denis Akhiyarov,Aditya Shanmugham,Chitra Ganapathy*

Main category: cs.SE

TL;DR: 提出了一种新技术，通过扩展代码和索引预测所需API，实现高质量端到端代码生成，解决了现有数据集中API泄漏问题，并通过新数据集和综合后训练管道提升检索准确率和效率。


<details>
  <summary>Details</summary>
Motivation: 当前搜索技术主要局限于标准的RAG查询-文档应用，且现有代码到代码的基准数据集存在API泄露问题，难以准确预测API使用意图。

Method: 提出扩展代码和索引方法预测所需API，构建从现实世界ServiceNow脚本中提取的新数据集，设计综合后训练管道，包括合成数据生成、监督微调和强化学习，优化0.6B规模的轻量级重排序模型。

Result: 该方法在检索准确率上达到87.86%的top-40准确率，轻量级模型在保持2.5倍低延迟的同时，性能超越了8B规模的大模型。

Conclusion: 通过技术创新和综合训练流程，有效处理了企业代码中API使用的复杂性，实现了更高效、高质量的代码生成，兼顾性能与计算资源消耗。

Abstract: Current search techniques are limited to standard RAG query-document
applications. In this paper, we propose a novel technique to expand the code
and index for predicting the required APIs, directly enabling high-quality,
end-to-end code generation for auto-completion and agentic AI applications. We
address the problem of API leaks in current code-to-code benchmark datasets by
introducing a new dataset built from real-world ServiceNow Script Includes that
capture the challenge of unclear API usage intent in the code. Our evaluation
metrics show that this method achieves 87.86% top-40 retrieval accuracy,
allowing the critical context with APIs needed for successful downstream code
generation. To enable real-time predictions, we develop a comprehensive
post-training pipeline that optimizes a compact 0.6B reranker through synthetic
dataset generation, supervised fine-tuning, and reinforcement learning. This
approach enables our compact reranker to outperform a much larger 8B model
while maintaining 2.5x reduced latency, effectively addressing the nuances of
enterprise-specific code without the computational overhead of larger models.

</details>


### [113] [Are Classical Clone Detectors Good Enough For the AI Era?](https://arxiv.org/abs/2509.25754)
*Ajmain Inqiad Alam,Palash Roy,Farouq Al-omari,Chanchal Roy,Banani Roy,Kevin Schneider*

Main category: cs.SE

TL;DR: 本文系统评估了传统代码克隆检测工具在AI生成代码克隆检测中的表现，发现有效的规范化技术能提升检测准确率。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成代码的广泛应用，传统CCD工具面临新的检测挑战，需要评估其在AI代码克隆检测中的有效性。

Method: 利用GPTCloneBench基准测试九种主流CCD工具，并在传统大数据基准BigCloneBench和SemanticCloneBench中对比验证。

Result: 实验表明，经典CCD工具在AI生成代码克隆检测中仍具有效性，特别是通过规范化技术增强的工具表现更优，但部分工具性能在不同基准间存在差异。

Conclusion: 本文揭示了传统CCD工具在新兴AI代码克隆检测中的潜力与不足，强调规范化技术的重要性，并为CCD工具的选择提供了实用性分析。

Abstract: The increasing adoption of AI-generated code has reshaped modern software
development, introducing syntactic and semantic variations in cloned code.
Unlike traditional human-written clones, AI-generated clones exhibit systematic
syntactic patterns and semantic differences learned from large-scale training
data. This shift presents new challenges for classical code clone detection
(CCD) tools, which have historically been validated primarily on human-authored
codebases and optimized to detect syntactic (Type 1-3) and limited semantic
clones. Given that AI-generated code can produce both syntactic and complex
semantic clones, it is essential to evaluate the effectiveness of classical CCD
tools within this new paradigm. In this paper, we systematically evaluate nine
widely used CCD tools using GPTCloneBench, a benchmark containing
GPT-3-generated clones. To contextualize and validate our results, we further
test these detectors on established human-authored benchmarks, BigCloneBench
and SemanticCloneBench, to measure differences in performance between
traditional and AI-generated clones. Our analysis demonstrates that classical
CCD tools, particularly those enhanced by effective normalization techniques,
retain considerable effectiveness against AI-generated clones, while some
exhibit notable performance variation compared to traditional benchmarks. This
paper contributes by (1) evaluating classical CCD tools against AI-generated
clones, providing critical insights into their current strengths and
limitations; (2) highlighting the role of normalization techniques in improving
detection accuracy; and (3) delivering detailed scalability and execution-time
analyses to support practical CCD tool selection.

</details>


### [114] [LogPilot: Intent-aware and Scalable Alert Diagnosis for Large-scale Online Service Systems](https://arxiv.org/abs/2509.25874)
*Zhihan Jiang,Jinyang Liu,Yichen Li,Haiyu Huang,Xiao He,Tieying Zhang,Jianjun Chen,Yi Li,Rui Shi,Michael R. Lyu*

Main category: cs.SE

TL;DR: LogPilot采用大语言模型，对大规模在线服务系统中的告警日志进行意图感知的自动诊断，显著提升了根因定位准确率和诊断效率。


<details>
  <summary>Details</summary>
Motivation: 现有自动告警诊断工具因缺乏对告警意图的理解及数据处理能力，难以精确定位根因，且日志数据量巨大，手动排查负担重。

Method: LogPilot利用告警定义中的逻辑信息精准识别相关日志，将请求执行重建为时空日志链，再基于相似链条聚类提取代表样本供大语言模型诊断，实现了诊断输入的丰富性与紧凑性兼顾。

Result: 在Volcano Engine Cloud真实告警上，LogPilot比最先进方法提升了50.34%的根因摘要有用性和54.79%的定位准确率，诊断时间不足一分钟，费用低廉。

Conclusion: LogPilot提供了一种意图感知且可扩展的告警日志自动诊断框架，已成功部署生产，显著提升了在线服务系统的告警处理效率和准确性。

Abstract: Effective alert diagnosis is essential for ensuring the reliability of
large-scale online service systems. However, on-call engineers are often
burdened with manually inspecting massive volumes of logs to identify root
causes. While various automated tools have been proposed, they struggle in
practice due to alert-agnostic log scoping and the inability to organize
complex data effectively for reasoning. To overcome these limitations, we
introduce LogPilot, an intent-aware and scalable framework powered by Large
Language Models (LLMs) for automated log-based alert diagnosis. LogPilot
introduces an intent-aware approach, interpreting the logic in alert
definitions (e.g., PromQL) to precisely identify causally related logs and
requests. To achieve scalability, it reconstructs each request's execution into
a spatiotemporal log chain, clusters similar chains to identify recurring
execution patterns, and provides representative samples to the LLMs for
diagnosis. This clustering-based approach ensures the input is both rich in
diagnostic detail and compact enough to fit within the LLM's context window.
Evaluated on real-world alerts from Volcano Engine Cloud, LogPilot improves the
usefulness of root cause summarization by 50.34% and exact localization
accuracy by 54.79% over state-of-the-art methods. With a diagnosis time under
one minute and a cost of only $0.074 per alert, LogPilot has been successfully
deployed in production, offering an automated and practical solution for
service alert diagnosis.

</details>


### [115] [Red Teaming Program Repair Agents: When Correct Patches can Hide Vulnerabilities](https://arxiv.org/abs/2509.25894)
*Simin Chen,Yixin He,Suman Jana,Baishakhi Ray*

Main category: cs.SE

TL;DR: 本文提出了SWExploit，一种针对基于大语言模型的自动程序修复（APR）代理的对抗攻击方法，能够生成功能正确但存在安全漏洞的补丁，挑战了当前仅以测试通过作为补丁可靠性的评估标准。


<details>
  <summary>Details</summary>
Motivation: 现有自动程序修复工作主要关注补丁的功能正确性，忽略了安全风险，而在开放平台中恶意用户可能利用这一点诱导生成存在安全漏洞的补丁。

Method: SWExploit通过程序分析确定注入点，生成误导性问题描述且不改变原问题语义，并基于APR代理输出迭代优化对抗性问题，进而诱导生成功能正确但易受攻击的补丁。

Result: 在三个代理流水线和五个后端大语言模型上，SWExploit成功率高达0.91，远超基线的0.20，证明其能有效生成存在安全风险的功能正确补丁。

Conclusion: 本研究首次挑战了仅凭测试通过判定补丁安全和可靠的传统假设，揭示了当前自动程序修复评估范式的重要安全隐患。

Abstract: LLM-based agents are increasingly deployed for software maintenance tasks
such as automated program repair (APR). APR agents automatically fetch GitHub
issues and use backend LLMs to generate patches that fix the reported bugs.
However, existing work primarily focuses on the functional correctness of
APR-generated patches, whether they pass hidden or regression tests, while
largely ignoring potential security risks. Given the openness of platforms like
GitHub, where any user can raise issues and participate in discussions, an
important question arises: Can an adversarial user submit a valid issue on
GitHub that misleads an LLM-based agent into generating a functionally correct
but vulnerable patch? To answer this question, we propose SWExploit, which
generates adversarial issue statements designed to make APR agents produce
patches that are functionally correct yet vulnerable. SWExploit operates in
three main steps: (1) program analysis to identify potential injection points
for vulnerable payloads; (2) adversarial issue generation to provide misleading
reproduction and error information while preserving the original issue
semantics; and (3) iterative refinement of the adversarial issue statements
based on the outputs of the APR agents. Empirical evaluation on three agent
pipelines and five backend LLMs shows that SWExploit can produce patches that
are both functionally correct and vulnerable (the attack success rate on the
correct patch could reach 0.91, whereas the baseline ASRs are all below 0.20).
Based on our evaluation, we are the first to challenge the traditional
assumption that a patch passing all tests is inherently reliable and secure,
highlighting critical limitations in the current evaluation paradigm for APR
agents.

</details>


### [116] [R-Log: Incentivizing Log Analysis Capability in LLMs via Reasoning-based Reinforcement Learning](https://arxiv.org/abs/2509.25987)
*Yilun Liu,Ziang Chen,Song Xu,Minggui He,Shimin Tao,Weibin Meng,Yuming Xie,Tao Han,Chunguang Zhao,Jingzhou Du,Daimeng Wei,Shenglin Zhang,Yongqian Sun*

Main category: cs.SE

TL;DR: 本文提出了R-Log，一种基于推理的日志分析新范式，通过模仿人类工程师的结构化分析过程，结合强化学习优化模型表现，有效提升日志分析的泛化能力和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于直接监督微调的大型语言模型在处理复杂日志数据时存在领域差异大、过拟合严重、长上下文导致关键信息被淹没及幻觉生成等问题。

Method: 提出R-Log，采用模仿人类逐步推理的方式进行分析；使用包含13种运维策略的2千多条推理轨迹数据进行初始训练，再通过强化学习在模拟运维环境中利用联合奖励函数进一步优化，减少幻觉生成。

Result: 在真实日志上，R-Log在五个日志分析任务中均优于现有方法，特别是在未见过场景下性能提升228.05%。同时，设计了加速版R-Log-fast，速度提升5倍且保持93%的性能。

Conclusion: 通过模拟人类推理和强化学习优化，R-Log显著提升了自动日志分析的准确性和泛化能力，为复杂日志数据分析提供了有效解决方案。

Abstract: The growing complexity of log data in modern software systems has prompted
the use of Large Language Models (LLMs) for automated log analysis. Current
approaches typically rely on direct supervised fine-tuning (SFT) on log-label
pairs. However, this exacerbates the domain discrepancy between general-purpose
LLMs and specialized log data, causing overfitting. Furthermore, SFT's
imbalanced loss computation often allows lengthy contexts to overwhelm
critical, concise details in model answers, leading to hallucinations. To
address these limitations, we propose R-Log, a novel reasoning-based paradigm
that mirrors the structured, step-by-step analytical process of human
engineers. This approach enhances generalizability by learning the underlying
rules behind conclusions. We further employ Reinforcement Learning (RL) to
optimize the model within a simulated O&M environment, thereby reducing
hallucinations by directly rewarding correct outcomes. R-Log is first
cold-started on a curated dataset of 2k+ reasoning trajectories, guided by 13
strategies from manual O&M practices, to establish an initial reasoning
capability. This ability is then refined via RL using a joint reward function.
Empirical evaluations on real-world logs show that R-Log outperforms existing
methods across five log analysis tasks, particularly in unseen scenarios (by
228.05%). We also designed R-Log-fast with 5x speedup while keeping 93% of the
efficacy.

</details>


### [117] [Using GPT to build a Project Management assistant for Jira environments](https://arxiv.org/abs/2509.26014)
*Joel Garcia-Escribano,Arkaitz Carbajo,Mikel Egaña Aranguren,Unai Lopez-Novoa*

Main category: cs.SE

TL;DR: 本文提出了JiraGPT Next，一款基于GPT大型语言模型的Jira插件，旨在通过自然语言接口帮助项目经理更高效地处理大量项目信息。


<details>
  <summary>Details</summary>
Motivation: 项目管理中面对大量复杂数据，现有工具学习曲线陡峭且需要复杂编程，项目经理难以高效获取所需信息。

Method: 设计JiraGPT Next插件，利用GPT模型通过自然语言查询界面替代传统复杂操作，并评估不同提示词对任务完成准确性的影响。

Result: 通过实验评估了GPT在项目管理数据检索中的准确性，验证了自然语言接口的有效性。

Conclusion: JiraGPT Next有效简化了项目经理获取项目信息的流程，提高了使用便捷性，显示出利用大语言模型辅助项目管理的潜力。

Abstract: In the domain of Project Management, the sheer volume of data is a challenge
that project managers continually have to deal with. Effectively steering
projects from inception to completion requires handling of diverse information
streams, including timelines, budgetary considerations, and task dependencies.
To navigate this data-driven landscape with precision and agility, project
managers must rely on efficient and sophisticated tools. These tools have
become essential, as they enable project managers to streamline communication,
optimize resource allocation, and make informed decisions in real-time.
However, many of these tools have steep learning curves and require using
complex programming languages to retrieve the exact data that project managers
need. In this work we present JiraGPT Next, a software that uses the GPT Large
Language Model to ease the process by which project managers deal with large
amounts of data. It is conceived as an add-on for Jira, one of the most popular
Project Management tools, and provides a natural language interface to retrieve
information. This work presents the design decisions behind JiraGPT Next and an
evaluation of the accuracy of GPT in this context, including the effects of
providing different prompts to complete a particular task.

</details>


### [118] [Evaluating the impact of code smell refactoring on the energy consumption of Android applications](https://arxiv.org/abs/2509.26031)
*Hina Anwar,Dietmar Pfahl,Satish N. Srirama*

Main category: cs.SE

TL;DR: 本文研究了Android应用中代码重构对能耗的影响，发现部分代码味道重构能降低能耗最高达10.8%。


<details>
  <summary>Details</summary>
Motivation: 移动设备能耗与应用质量密切相关，频繁代码重构可能改善能效，本文旨在评估常见代码重构对Android应用性能与能耗的影响。

Method: 通过实验分析几种常见代码味道的重构对Android应用能源消耗和性能的影响，并测试不同重构顺序的效果。

Result: 发现“重复代码”和“类型检查”代码味道的重构能降低能耗最高达10.8%，但能耗变化与执行时间变化不直接相关，不同重构组合对能耗影响较小，某些顺序造成能耗增加或减少。

Conclusion: 代码重构有助于部分降低Android应用能耗，但影响因素复杂，需进一步研究软件大小、年龄、开发经验和团队规模对代码味道及能耗影响的关联。

Abstract: Energy consumption of mobile apps is a domain that is receiving a lot of
attention from researchers. Recent studies indicate that the energy consumption
of mobile devices could be improved by improving the quality of mobile apps.
Frequent refactoring is one way of achieving this goal. In this paper, we
explore the performance and energy impact of several common code refactorings
in Android apps. Experimental results indicate that some code smell
refactorings positively impact the energy consumption of Android apps.
Refactoring of the code smells "Duplicated code" and "Type checking" reduce
energy consumption by up to 10.8%. Significant reduction in energy consumption,
however, does not seem to be directly related to the increase or decrease of
execution time. In addition, the energy impact over permutations of code smell
refactorings in the selected Android apps was small. When analyzing the order
in which refactorings were made across code smell types, it turned out that
some permutations resulted in a reduction and some in an increase of energy
consumption for the analyzed apps. More research needs to be done to
investigate how factors like size and age of software apps, experience, and
number of contributors to app development correlate with (a) the number and
type of code smells found and (b) the impact of energy consumption and
performance after refactoring.

</details>


### [119] [Agent-based code generation for the Gammapy framework](https://arxiv.org/abs/2509.26110)
*Dmitriy Kostunin,Vladimir Sotnikov,Sergo Golovachev,Abhay Mehta,Tim Lukas Holch,Elisa Jones*

Main category: cs.SE

TL;DR: 本文针对科学库Gammapy库存在的文档缺乏、API不稳定等问题，设计了一个能够编写、执行及验证代码的智能代理，并开发了最小化网络演示与基准测试套件。


<details>
  <summary>Details</summary>
Motivation: 专业科学库如Gammapy缺乏丰富文档和稳定API，导致基于大语言模型的代码生成效果受限。

Method: 设计了一个智能代理，在受控环境中编写、执行和验证代码；同时提供网络演示和基准测试。

Result: 实现了基于该智能代理的演示系统及测试套件，优化了对Gammapy库代码生成的支持。

Conclusion: 该方法有效解决了专业科学库文档和API缺乏问题，提高了代码生成的准确性，未来将继续完善工具和功能。

Abstract: Software code generation using Large Language Models (LLMs) is one of the
most successful applications of modern artificial intelligence. Foundational
models are very effective for popular frameworks that benefit from
documentation, examples, and strong community support. In contrast, specialized
scientific libraries often lack these resources and may expose unstable APIs
under active development, making it difficult for models trained on limited or
outdated data. We address these issues for the Gammapy library by developing an
agent capable of writing, executing, and validating code in a controlled
environment. We present a minimal web demo and an accompanying benchmarking
suite. This contribution summarizes the design, reports our current status, and
outlines next steps.

</details>


### [120] [A Multi-Language Object-Oriented Programming Benchmark for Large Language Models](https://arxiv.org/abs/2509.26111)
*Shuai Wang,Liang Ding,Li Shen,Yong Luo,Han Hu,Lefei Zhang,Fu Lin*

Main category: cs.SE

TL;DR: 本文提出了MultiOOP，一个涵盖六种语言的面向对象编程多语言基准，用于评估大型语言模型代码生成能力，强调当前基准的语言单一、任务层次有限及测试用例数量不足等问题。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成基准存在语言单一、任务形式单一和测试用例数量少的问题，导致评估不公平且不充分。

Method: 设计MultiOOP基准，涵盖六种主流编程语言并提供丰富任务；扩展现有基准和评估指标至多语言环境；开发自动测试用例增强框架。

Result: 对14个主流大型语言模型进行零样本测试，发现性能显著下降、跨语言表现差异大且模型缺乏对核心OOP概念的理解。

Conclusion: MultiOOP及其工具可促进更平衡、全面的面向对象代码生成评估，推动多语言环境下模型能力提升。

Abstract: Establishing fair and robust benchmarks is essential for evaluating
intelligent code generation by large language models (LLMs). Our survey of 35
existing benchmarks uncovers three major imbalances: 85.7% focus on a single
programming language; 94.3% target only function-level or statement-level
tasks; and over 80% include fewer than ten test cases on average. To address
these gaps, we propose MultiOOP, a multi-language object-oriented programming
benchmark covering six popular languages (Python, PHP, C++, C#, Java,
JavaScript) with 267 tasks per language. We design a translator that extends an
existing single-language OOP benchmark and the pass@o metric to a multilingual
setting. Moreover, we propose an automated framework for augmenting test cases
to ensure the reliability of the evaluation results. We evaluate 14 mainstream
LLMs under zero-shot prompting and report three key findings: 1) Substantial
performance degradation: pass@1 scores on MultiOOP drop by up to 65.6
percentage points compared to function-level tasks (e.g., HumanEval). 2)
Cross-language variability: GPT-4o mini achieves pass@1 of 48.06% in Python but
only 0.12%-15.26% in other languages, indicating limited multilingual
generalization. 3) Conceptual gaps: pass@o scores are consistently 1.1-19.2
points lower than pass@k, demonstrating that LLMs often generate executable
code without fully capturing core OOP concepts. Our benchmark, metric
extensions, and evaluation scripts will be publicly released to foster a more
balanced and comprehensive assessment of LLMs in object-oriented code
generation. Our code and data will be released at
https://github.com/alphadl/OOP-eval and
https://huggingface.co/datasets/codeai-dteam/MultiOOP respectively.

</details>


### [121] [Understanding Collective Social Behavior in OSS Communities: A Co-editing Network Analysis of Activity Cascades](https://arxiv.org/abs/2509.26173)
*Lisi Qarkaxhija,Maximilian Carparo,Stefan Menzel,Bernhard Sendhoff,Ingo Scholtes*

Main category: cs.SE

TL;DR: 本文通过分析开源软件开发者的时间活动模式，发现提交行为具有“突发性”，并利用共编辑网络模型揭示协作中的社会机制，识别活动级联现象，进而预测开发者流失。


<details>
  <summary>Details</summary>
Motivation: 理解开源软件社区中开发者的集体社会行为，以便更好地建模和预测社区的长期动态和可持续性。

Method: 采用基于网络的共编辑模型，分析开发者间的交互及活动传播，开发识别活动级联的方法，并基于此构建开发者流失预测模型。

Result: 发现超过半数项目中存在显著的活动级联现象，活动传播能有效预测哪些开发者可能离开项目。

Conclusion: 活动级联是 OSS 社区社会动态的重要表现，理解其机制有助于预测开发者流失并促进项目的协作与持续发展。

Abstract: Understanding the collective social behavior of software developers is
crucial to model and predict the long-term dynamics and sustainability of Open
Source Software (OSS) communities. To this end, we analyze temporal activity
patterns of developers, revealing an inherently ``bursty'' nature of commit
contributions. To investigate the social mechanisms behind this phenomenon, we
adopt a network-based modelling framework that captures developer interactions
through co-editing networks. Our framework models social interactions, where a
developer editing the code of other developers triggers accelerated activity
among collaborators. Using a large data set on 50 major OSS communities, we
further develop a method that identifies activity cascades, i.e. the
propagation of developer activity in the underlying co-editing network. Our
results suggest that activity cascades are a statistically significant
phenomenon in more than half of the studied projects. We further show that our
insights can be used to develop a simple yet practical churn prediction method
that forecasts which developers are likely to leave a project. Our work sheds
light on the emergent collective social dynamics in OSS communities and
highlights the importance of activity cascades to understand developer churn
and retention in collaborative software projects.

</details>


### [122] [Hamster: A Large-Scale Study and Characterization of Developer-Written Tests](https://arxiv.org/abs/2509.26204)
*Rangeet Pan,Tyler Stennett,Raju Pavuluri,Nate Levin,Alessandro Orso,Saurabh Sinha*

Main category: cs.SE

TL;DR: 该论文研究了自动测试生成（ATG）技术与开发者手写测试之间的差距，分析了170万个开源Java测试用例，发现开发者的测试特征远超现有ATG工具能力，提出了未来改进方向。


<details>
  <summary>Details</summary>
Motivation: 当前大量的ATG研究缺乏对开发者手写测试特性的深入理解，导致自动生成的测试用例难以真实且具代表性。

Method: 对开源Java项目中170万个测试用例进行了广泛的实证分析，研究了测试范围、测试夹具、断言、输入类型及模拟使用等多方面的特征，并将结果与两种先进的ATG工具生成的测试用例进行了对比。

Result: 发现绝大多数开发者编写的测试具有现有ATG工具无法覆盖的特征，揭示了现有工具在真实测试场景中的不足。

Conclusion: 基于研究发现，论文指出未来研究应关注弥合工具能力与开发者需求之间的差距，推动ATG工具更有效地支持开发测试实践。

Abstract: Automated test generation (ATG), which aims to reduce the cost of manual test
suite development, has been investigated for decades and has produced countless
techniques based on a variety of approaches: symbolic analysis, search-based,
random and adaptive-random, learning-based, and, most recently,
large-language-model-based approaches. However, despite this large body of
research, there is still a gap in our understanding of the characteristics of
developer-written tests and, consequently, in our assessment of how well ATG
techniques and tools can generate realistic and representative tests. To bridge
this gap, we conducted an extensive empirical study of developer-written tests
for Java applications, covering 1.7 million test cases from open-source
repositories. Our study is the first of its kind in studying aspects of
developer-written tests that are mostly neglected in the existing literature,
such as test scope, test fixtures and assertions, types of inputs, and use of
mocking. Based on the characterization, we then compare existing tests with
those generated by two state-of-the-art ATG tools. Our results highlight that a
vast majority of developer-written tests exhibit characteristics that are
beyond the capabilities of current ATG tools. Finally, based on the insights
gained from the study, we identify promising research directions that can help
bridge the gap between current tool capabilities and more effective tool
support for developer testing practices. We hope that this work can set the
stage for new advances in the field and bring ATG tools closer to generating
the types of tests developers write.

</details>


### [123] [UniSage: A Unified and Post-Analysis-Aware Sampling for Microservices](https://arxiv.org/abs/2509.26336)
*Zhouruixing Zhu,Zhihan Jiang,Tianyi Yang,Pinjia He*

Main category: cs.SE

TL;DR: UniSage提出了一种新的采样框架，通过先进行轻量级异常检测和根因分析，指导针对正常和异常场景的双重采样策略，实现高效且全面的跟踪和日志采样。


<details>
  <summary>Details</summary>
Motivation: 现有分布式系统中跟踪和日志数据规模庞大，采样前分析的方法容易丢失关键信息，影响故障诊断的透明度和准确性。

Method: UniSage采用先分析后采样的模式，先对完整数据流进行多模态异常检测和根因分析，获得服务级别诊断信息，再结合分析引导采样和异常边缘采样保证关键数据覆盖，同时控制冗余。

Result: 在2.5%的采样率下，UniSage捕捉到56.5%的关键跟踪和96.25%的相关日志，根因分析准确率提升42.45%，且能高效处理大量遥测数据，满足生产环境需求。

Conclusion: UniSage通过统一且智能的采样机制，大幅提升了分布式系统日志和跟踪数据的诊断效率和准确性，且具备良好的实际应用潜力。

Abstract: Traces and logs are essential for observability and fault diagnosis in modern
distributed systems. However, their ever-growing volume introduces substantial
storage overhead and complicates troubleshooting. Existing approaches typically
adopt a sample-before-analysis paradigm: even when guided by data heuristics,
they inevitably discard failure-related information and hinder transparency in
diagnosing system behavior. To address this, we introduce UniSage, the first
unified framework to sample both traces and logs using a post-analysis-aware
paradigm. Instead of discarding data upfront, UniSagefirst performs lightweight
and multi-modal anomaly detection and root cause analysis (RCA) on the complete
data stream. This process yields fine-grained, service-level diagnostic
insights that guide a dual-pillar sampling strategy for handling both normal
and anomalous scenarios: an analysis-guided sampler prioritizes data implicated
by RCA, while an edge-case-based sampler ensures rare but critical behaviors
are captured. Together, these pillars ensure comprehensive coverage of critical
signals without excessive redundancy. Extensive experiments demonstrate that
UniSage significantly outperforms state-of-the-art baselines. At a 2.5%
sampling rate, it captures 56.5% of critical traces and 96.25% of relevant
logs, while improving the accuracy (AC@1) of downstream root cause analysis by
42.45%. Furthermore, its efficient pipeline processes 10 minutes of telemetry
data in under 5 seconds, demonstrating its practicality for production
environments.

</details>


### [124] [Institutional Policy Pathways for Supporting Research Software: Global Trends and Local Practices](https://arxiv.org/abs/2509.26422)
*Michelle Barker,Jeremy Cohen,Pedro Hernández Serrano,Daniel S. Katz,Kim Martin,Dan Rudmann,Hugh Shanahan*

Main category: cs.SE

TL;DR: 本文探讨了研究软件在现代科研中的重要性，指出研究机构在支持软件开发和技术人员方面的政策不足，并介绍了PRO4RS工作组推动研究软件政策发展的工作。


<details>
  <summary>Details</summary>
Motivation: 随着研究软件在现代科学中的核心地位日益提升，研究机构需要确保在人员、技能和基础设施上的投资能够产生可持续且可维护的软件，推动科研和机构声誉的发展。但现有的管理和认可体系发展滞后，缺乏系统的政策支持。

Method: 通过联合研究软件联盟(ReSA)和研究数据联盟(RDA)的PRO4RS工作组，系统审视全球研究机构在研究软件政策上的现状，分析政策缺口，尤其是与研究人员认可和科研评价改革相关的部分。

Result: 发现多数研究机构在研究软件及相关人员的政策支持方面存在显著不足，特别是在培训、认可和长期技术人员支持的机制不健全。同时，现有政策未能充分响应FAIR和开放科学原则的要求。

Conclusion: 为了改善现代科研环境，研究机构必须制定并实施健全的研究软件政策，支持软件开发、使用及可持续性，特别要加强对技术人员的认可和支持，从而提升科研质量和机构竞争力。PRO4RS工作组的工作为政策制定提供了重要参考和推动力。

Abstract: As research software becomes increasingly central to modern science,
research-performing organisations (RPOs) need to ensure that their investment
in people, skills and infrastructure around research software produces
sustainable and maintainable software that improves the research they perform,
which in turn improves the overall institution and its reputation and funding,
for example, by competing with peers who lack this approach. However, research
institution management and recognition of research software and its personnel
has mostly often developed in an ad hoc manner. RPO training infrastructures,
recognition and reward structures, have not developed at a sufficient rate to
support and encourage both the widespread use of research software best
practices and the long-term support for technical roles that is required. To
begin to address this fundamental problem for modern research environments,
RPOs must implement and adopt robust policies to support research software
development, use, and sustainability. Despite growing momentum from funders and
publishers around FAIR and open science principles, research
institutional-level policies specifically addressing research software remain
limited or lacking in breadth.
  This article outlines the work of the Policies in Research Organisations for
Research Software (PRO4RS) Working Group (WG), a joint initiative of the
Research Software Alliance (ReSA) and the Research Data Alliance (RDA), which
examined and advanced research software policy development across institutions
worldwide. After consideration of the rationale for institutional policies on
research software, the PRO4RS WG outputs and analysis are utilised to highlight
critical policy gaps, particularly related to consideration of research
software personnel in policy work focused on reform of research assessment.

</details>


### [125] [EQ-Robin: Generating Multiple Minimal Unique-Cause MC/DC Test Suites](https://arxiv.org/abs/2509.26458)
*Robin Lee,Youngho Nam*

Main category: cs.SE

TL;DR: 本文提出了EQ-Robin，一种生成多样化最小唯一因果MC/DC测试套件的方法，解决了传统Robin's Rule算法在实际系统约束下可能导致测试用例无效的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的Robin's Rule算法虽然能生成理论上最小的N+1测试用例，但只生成单一测试套件，若某独立性对测试用例不合法，则无法实现100%覆盖，存在较大风险。

Method: 通过对抽象语法树进行代数重排系统性生成语义等价的单布尔表达式变体，对每个变体应用Robin's Rule，生成多样化的测试套件集合，从而提高测试覆盖的鲁棒性。

Result: 提出了EQ-Robin轻量级管线，能系统性地产生多样化且符合最小唯一因果MC/DC覆盖要求的测试套件，保证了理论最小性和实际约束的兼容性。

Conclusion: EQ-Robin为处理安全关键软件中的唯一因果MC/DC覆盖测试提供了实用有效的解决方案，能够在现实环境中更可靠地实现100%覆盖。计划通过基于TCAS-II的单布尔表达式进行评估验证。

Abstract: Modified Condition/Decision Coverage (MC/DC), particularly its strict
Unique-Cause form, is a cornerstone of safety-critical software verification. A
recent algorithm, "Robin's Rule," introduced a deterministic method to
construct the theoretical minimum of N+1 test cases for Singular Boolean
Expressions (SBEs). However, this approach yields only a single test suite,
introducing a critical risk: if a test case forming a required 'independence
pair' is an illegal input forbidden by system constraints, the suite fails to
achieve 100% coverage. This paper proposes EQ-Robin, a lightweight pipeline
that systematically generates a family of minimal Unique-Cause MC/DC suites to
mitigate this risk. We introduce a method for systematically generating
semantically equivalent SBEs by applying algebraic rearrangements to an
Abstract Syntax Tree (AST) representation of the expression. By applying
Robin's Rule to each structural variant, a diverse set of test suites can be
produced. This provides a resilient path to discovering a valid test suite that
preserves the N+1 minimality guarantee while navigating real-world constraints.
We outline an evaluation plan on TCAS-II-derived SBEs to demonstrate how
EQ-Robin offers a practical solution for ensuring robust MC/DC coverage.

</details>


### [126] [ErrorPrism: Reconstructing Error Propagation Paths in Cloud Service Systems](https://arxiv.org/abs/2509.26463)
*Junsong Pu,Yichen Li,Zhuangbin Chen,Jinyang Liu,Zhihan Jiang,Jianjun Chen,Rui Shi,Zibin Zheng,Tieying Zhang*

Main category: cs.SE

TL;DR: 本文提出了ErrorPrism，一种利用静态分析和大语言模型（LLM）进行错误传播路径自动重建的工具，显著提升了微服务系统中错误根因分析的准确性。


<details>
  <summary>Details</summary>
Motivation: 云服务系统中的错误传播具有级联效应，现有方法难以有效追踪从最终日志到错误源头的完整传播路径。

Method: 通过静态分析构建函数调用图并映射日志字符串，结合LLM代理的迭代向后搜索，实现多跳错误路径精确重建。

Result: 在字节跳动67个生产微服务中对102个真实错误测试，ErrorPrism达到了97.0%的路径重建准确率，优于现有静态分析及LLM方法。

Conclusion: ErrorPrism为工业微服务系统中的根因分析提供了高效、实用的解决方案，显著提升了错误追踪与诊断能力。

Abstract: Reliability management in cloud service systems is challenging due to the
cascading effect of failures. Error wrapping, a practice prevalent in modern
microservice development, enriches errors with context at each layer of the
function call stack, constructing an error chain that describes a failure from
its technical origin to its business impact. However, this also presents a
significant traceability problem when recovering the complete error propagation
path from the final log message back to its source. Existing approaches are
ineffective at addressing this problem. To fill this gap, we present ErrorPrism
in this work for automated reconstruction of error propagation paths in
production microservice systems. ErrorPrism first performs static analysis on
service code repositories to build a function call graph and map log strings to
relevant candidate functions. This significantly reduces the path search space
for subsequent analysis. Then, ErrorPrism employs an LLM agent to perform an
iterative backward search to accurately reconstruct the complete, multi-hop
error path. Evaluated on 67 production microservices at ByteDance, ErrorPrism
achieves 97.0% accuracy in reconstructing paths for 102 real-world errors,
outperforming existing static analysis and LLM-based approaches. ErrorPrism
provides an effective and practical tool for root cause analysis in industrial
microservice systems.

</details>


### [127] [Towards Verified Code Reasoning by LLMs](https://arxiv.org/abs/2509.26546)
*Meghana Sistla,Gogul Balakrishnan,Pat Rondon,José Cambronero,Michele Tufano,Satish Chandra*

Main category: cs.SE

TL;DR: 该论文提出了一种通过形式验证和程序分析工具自动验证基于大模型代码推理代理的答案准确性的方法，以提升其在高精度需求场景中的可信度。


<details>
  <summary>Details</summary>
Motivation: 目前基于大模型的代码推理代理虽然能解答多种代码问题，但答案准确度不足，需要人工手动复核，影响开发效率和信任，尤其在代码理解、代码审查和自动代码生成等需要高精度的场景中。

Method: 通过从代理的回答中提取形式化表示，利用形式验证和程序分析工具对推理步骤进行验证，自动化确认答案的正确性。

Result: 在20个未初始化变量错误及20个程序等价性查询的基准测试中，形式验证成功验证了13个未初始化变量错误的推理，并准确捕捉了6个错误的程序等价性判断。

Conclusion: 该方法有效提升了代码推理代理在高精度场景中的可信度，减少了对人工验证的依赖，增强了该类代理的实用价值。

Abstract: While LLM-based agents are able to tackle a wide variety of code reasoning
questions, the answers are not always correct. This prevents the agent from
being useful in situations where high precision is desired: (1) helping a
software engineer understand a new code base, (2) helping a software engineer
during code review sessions, and (3) ensuring that the code generated by an
automated code generation system meets certain requirements (e.g. fixes a bug,
improves readability, implements a feature).
  As a result of this lack of trustworthiness, the agent's answers need to be
manually verified before they can be trusted. Manually confirming responses
from a code reasoning agent requires human effort and can result in slower
developer productivity, which weakens the assistance benefits of the agent. In
this paper, we describe a method to automatically validate the answers provided
by a code reasoning agent by verifying its reasoning steps. At a very high
level, the method consists of extracting a formal representation of the agent's
response and, subsequently, using formal verification and program analysis
tools to verify the agent's reasoning steps.
  We applied this approach to a benchmark set of 20 uninitialized variable
errors detected by sanitizers and 20 program equivalence queries. For the
uninitialized variable errors, the formal verification step was able to
validate the agent's reasoning on 13/20 examples, and for the program
equivalence queries, the formal verification step successfully caught 6/8
incorrect judgments made by the agent.

</details>


### [128] [Black-box Context-free Grammar Inference for Readable & Natural Grammars](https://arxiv.org/abs/2509.26616)
*Mohammad Rifat Arefin,Shanto Rahman,Christoph Csallner*

Main category: cs.SE

TL;DR: NatGI利用大语言模型优化语法推断，在规模和复杂度更高的语言上表现优越，生成的语法结构更简洁且易于理解。


<details>
  <summary>Details</summary>
Motivation: 现有黑盒上下文无关语法推断工具在处理大型复杂语言时存在可扩展性、可读性和准确性不足的问题。

Method: NatGI基于TreeVada算法，加入括号引导的气泡探索、大语言模型驱动的气泡生成与非终结符命名，以及层次式增量调试进行语法树简化。

Result: 在Lua、C、MySQL等多种语言上的评测表明，NatGI的F1分数平均达到0.57，比最高基线工具TreeVada高出25个百分点，且生成语法在可解释性上表现显著提升。

Conclusion: NatGI有效结合大语言模型提升了黑盒上下文无关语法推断的准确性和可解释性，方便开发者和研究者更好地理解和使用生成的语法。

Abstract: Black-box context-free grammar inference is crucial for program analysis,
reverse engineering, and security, yet existing tools such as Arvada, TreeVada,
and Kedavra struggle with scalability, readability, and accuracy on large,
complex languages. We present NatGI, a novel LLM-guided grammar inference
framework that extends TreeVada's parse tree recovery with three key
innovations: bracket-guided bubble exploration, LLM-driven bubble generation
and non-terminal labeling, and hierarchical delta debugging (HDD) for
systematic tree simplification. Bracket-guided exploration leverages syntactic
cues such as parentheses to propose well-structured grammar fragments, while
LLM guidance produces meaningful non-terminal names and selects more promising
merges. Finally, HDD incrementally reduces unnecessary rules, which makes the
grammars both compact and interpretable. In our experiments, we evaluate NatGI
on a comprehensive benchmark suite ranging from small languages to larger ones
such as lua, c, and mysql. Our results show that NatGI consistently outperforms
strong baselines in terms of F1 score. On average, NatGI achieves an F1 score
of 0.57, which is 25pp (percentage points) higher than the best-performing
baseline, TreeVada. In the case of interpretability, our generated grammars
perform significantly better than those produced by existing approaches.
Leveraging LLM-based node renaming and bubble exploration, NatGI produces rules
with meaningful non-terminal names and compact structures that align more
closely with human intuition. As a result, developers and researchers can
achieve higher accuracy while still being able to easily inspect, verify, and
reason about the structure and semantics of the induced grammars.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [129] [An Agent-Based Simulation of Ageing Societies: Accessibility and Care Dynamics in Remote Areas](https://arxiv.org/abs/2509.26496)
*Roberto garrone*

Main category: cs.MA

TL;DR: 该论文通过基于代理的仿真模型研究意大利内陆老龄社会中老年人及其照护者的可达性与照护动态，比较了医疗服务搬迁前后的影响，发现搬迁提升局部可步行性但加剧照护需求未满足，家庭收入是主要负担因素。


<details>
  <summary>Details</summary>
Motivation: 随着社会老龄化，如何高效提供照护服务，特别是在偏远区域，成为亟需解决的问题。该研究旨在通过仿真模型量化医疗服务布局变动对老年人照护和可达性的影响。

Method: 集成人口普查、无人机地形模型、GIS路网及照护调查数据，构建包含老年人与照护者的合成代理群体，模拟不同服务布局下的可达性与照护状况，并比较基线与服务搬迁两种情景。

Result: 医疗服务搬迁提升了局部步行可达性，但因绕行和距离增加，未满足的照护时间反而增加。家庭收入对照护负担影响最大，可达性由经济与出行资源共同决定。

Conclusion: 研究揭示了偏远老龄社区中经济条件和移动性限制对照护负担的关键影响，强调为特定情境制定针对性干预措施的必要性。

Abstract: This paper presents an agent-based simulation of accessibility and care
dynamics in ageing societies, applied to the Italian inner area of Premeno
(VB). The model integrates census and municipal data, drone-derived elevation
models, GIS road networks, and survey-based caregiving information to generate
synthetic populations of older adults and their caregivers. Agents are
organized into dyads with socio-economic and mobility attributes, enabling the
simulation of both micro-scale accessibility and meso-scale caregiving
outcomes. Two scenarios are compared: a baseline and an alternative involving
the relocation of healthcare services. Key indicators include caregiver effort,
overwhelmed caregivers, walkability, and unmet hours of care. Findings show
that while relocation improves walkability locally, it increases unmet care
hours due to detours and reduced proximity. Household income emerges as the
primary driver of caregiver burden, with accessibility shaped by interactions
between financial and mobility resources. Results highlight the need for
interventions tailored to context-specific constraints in remote ageing
communities.

</details>
