<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 71]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.SE](#cs.SE) [Total: 18]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Linguistic Blind Spots in Clinical Decision Extraction](https://arxiv.org/abs/2602.03942)
*Mohamed Elgaar,Hadi Amiri*

Main category: cs.CL

TL;DR: 本文研究临床决策语言特征对医学决策抽取的影响，发现在不同决策类别中语言特征存在显著差异，导致抽取效果有较大差异，尤其是叙述性语言对抽取提出挑战。


<details>
  <summary>Details</summary>
Motivation: 提取临床笔记中的医疗决策对于临床决策支持和患者护理总结至关重要。但不同决策类别的语言特征差异是否导致抽取失败尚不明确。

Method: 利用带有决策类别标注的MedDec出院总结，通过计算七个语言指标分析不同类别决策的语言特点，并评估标准transformer模型在不同语言特征下的抽取召回率。

Result: 发现药物相关和问题定义类决策语言实体密集且简洁，而建议和预防类决策更叙述性，含更多停用词、代词和否定或模糊语气。精确匹配召回率为48%，在不同语言指标下差异显著。放宽匹配标准后召回率升至71%，表明许多错误为边界标注不一致。

Conclusion: 叙述性语言的决策类别在精确匹配评估下表现较差，提示临床决策抽取系统应采用容忍边界不一致的评价和提取策略以提高性能。

Abstract: Extracting medical decisions from clinical notes is a key step for clinical decision support and patient-facing care summaries. We study how the linguistic characteristics of clinical decisions vary across decision categories and whether these differences explain extraction failures. Using MedDec discharge summaries annotated with decision categories from the Decision Identification and Classification Taxonomy for Use in Medicine (DICTUM), we compute seven linguistic indices for each decision span and analyze span-level extraction recall of a standard transformer model. We find clear category-specific signatures: drug-related and problem-defining decisions are entity-dense and telegraphic, whereas advice and precaution decisions contain more narrative, with higher stopword and pronoun proportions and more frequent hedging and negation cues. On the validation split, exact-match recall is 48%, with large gaps across linguistic strata: recall drops from 58% to 24% from the lowest to highest stopword-proportion bins, and spans containing hedging or negation cues are less likely to be recovered. Under a relaxed overlap-based match criterion, recall increases to 71%, indicating that many errors are span boundary disagreements rather than complete misses. Overall, narrative-style spans--common in advice and precaution decisions--are a consistent blind spot under exact matching, suggesting that downstream systems should incorporate boundary-tolerant evaluation and extraction strategies for clinical decisions.

</details>


### [2] [Automatic Classification of Pedagogical Materials against CS Curriculum Guidelines](https://arxiv.org/abs/2602.03962)
*Erik Saule,Kalpathi Subramanian,Razvan Bunescu*

Main category: cs.CL

TL;DR: 本文提出利用自然语言处理技术，特别是传统工具和大型语言模型，加速计算机科学课程内容与ACM/IEEE标准指南的对齐评估。


<details>
  <summary>Details</summary>
Motivation: 计算机科学课程指南内容广泛，人工审核课程覆盖情况既耗时又认知负担重，迫切需要自动化方法提高效率。

Method: 采用两类自然语言处理技术，一是传统的解析、标注及嵌入技术，二是大型语言模型，对教学材料进行自动分类。

Result: 实验表明这些技术能有效自动化分类教学文档，帮助快速评估课程覆盖指南的程度。

Conclusion: 自然语言处理方法特别是大型语言模型可显著加快计算机科学教育课程符合专业标准的审核过程，减轻人工负担。

Abstract: Professional societies often publish curriculum guidelines to help programs align their content to international standards. In Computer Science, the primary standard is published by ACM and IEEE and provide detailed guidelines for what should be and could be included in a Computer Science program.
  While very helpful, it remains difficult for program administrators to assess how much of the guidelines is being covered by a CS program. This is in particular due to the extensiveness of the guidelines, containing thousands of individual items. As such, it is time consuming and cognitively demanding to audit every course to confidently mark everything that is actually being covered. Our preliminary work indicated that it takes about a day of work per course.
  In this work, we propose using Natural Language Processing techniques to accelerate the process. We explore two kinds of techniques, the first relying on traditional tools for parsing, tagging, and embeddings, while the second leverages the power of Large Language Models. We evaluate the application of these techniques to classify a corpus of pedagogical materials and show that we can meaningfully classify documents automatically.

</details>


### [3] [Likelihood-Based Reward Designs for General LLM Reasoning](https://arxiv.org/abs/2602.03979)
*Ariel Kwiatkowski,Natasha Butt,Ismail Labiad,Julia Kempe,Yann Ollivier*

Main category: cs.CL

TL;DR: 本文系统性研究了基于对数概率的奖励函数在大语言模型推理微调中的有效性，发现对数概率奖励在有无验证器的任务中均表现良好，优于传统二元奖励方法。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习微调依赖于为每个推理基准设计二元奖励，存在设计复杂且奖励稀疏的问题，亟需一种可扩展且普适的奖励方法。

Method: 本文系统比较了基于概率及对数概率的多种奖励变体，结合标准基线，在数学推理基准和无外部验证器的长答案任务上进行评测。

Result: 对数概率奖励作为链式思考的奖励函数，在所有测试环境中表现稳定优异，能够匹配甚至超越传统二元奖励的成功率，并显著提升模型困惑度表现。概率基础的方法则在无验证器任务中表现不佳。

Conclusion: 对数概率奖励是链式思考微调的有效方法，既适用于短答案的可验证环境，也适用于长答案的不可验证环境，解决了奖励设计与稀疏性问题。

Abstract: Fine-tuning large language models (LLMs) on reasoning benchmarks via reinforcement learning requires a specific reward function, often binary, for each benchmark. This comes with two potential limitations: the need to design the reward, and the potentially sparse nature of binary rewards. Here, we systematically investigate rewards derived from the probability or log-probability of emitting the reference answer (or any other prompt continuation present in the data), which have the advantage of not relying on specific verifiers and being available at scale. Several recent works have advocated for the use of similar rewards (e.g., VeriFree, JEPO, RLPR, NOVER). We systematically compare variants of likelihood-based rewards with standard baselines, testing performance both on standard mathematical reasoning benchmarks, and on long-form answers where no external verifier is available. We find that using the log-probability of the reference answer as the reward for chain-of-thought (CoT) learning is the only option that performs well in all setups. This reward is also consistent with the next-token log-likelihood loss used during pretraining. In verifiable settings, log-probability rewards bring comparable or better success rates than reinforcing with standard binary rewards, and yield much better perplexity. In non-verifiable settings, they perform on par with SFT. On the other hand, methods based on probability, such as VeriFree, flatline on non-verifiable settings due to vanishing probabilities of getting the correct answer. Overall, this establishes log-probability rewards as a viable method for CoT fine-tuning, bridging the short, verifiable and long, non-verifiable answer settings.

</details>


### [4] [Transformers perform adaptive partial pooling](https://arxiv.org/abs/2602.03980)
*Vsevolod Kapatsinski*

Main category: cs.CL

TL;DR: 本文研究变换器模型（GPT2）在训练过程中对不同上下文信息利用的变化，发现随着训练进行，模型对非当前上下文的观察依赖减少，且这种依赖受上下文频率、上下文数量和变异性影响，表现出类似分层回归的特征。


<details>
  <summary>Details</summary>
Motivation: 语言具有创造性，因此语言模型需要在新颖和罕见上下文中通过类比于相似上下文的信息来进行泛化。探讨变换器模型如何处理罕见但非新颖上下文的行为，以及上下文间信息整合的机制。

Method: 通过分析GPT2在不同训练阶段对上下文中单词预测的表现，评估模型对外部上下文观察的依赖程度，并比较这种依赖与分层回归中自适应部分信息整合（adaptive partial pooling）机制的相似性。

Result: 发现模型随着训练周期增加，对当前上下文之外观察的依赖逐渐减少，且信息整合的程度受上下文频率、上下文数量及其变异性的影响，呈现出与分层回归类似的适应性部分池化特征。

Conclusion: 变换器模型的学习行为在信息整合上符合理性和经验预期，显示出类分层回归的适应性部分池化机制，这支持模型在语言泛化中的合理性和有效性。

Abstract: Because language is creative, any reasonable language model must generalize, deciding what to say in novel contexts by using information from similar contexts. But what about contexts that are not novel but merely infrequent? In hierarchical regression, the model's predictions for behavior in a context are affected by observations from other similar contexts to the extent that 1) the current context is infrequent and 2) different contexts behave similarly. This is called adaptive partial pooling of evidence. This paper shows that next-word predictions of a transformer (GPT2) are increasingly unaffected by observations from outside the current context across epochs of training (the amount of pooling reduces with training), and that the extent of pooling is affected by context frequency, context number (type frequency) and context variability in a similar way to hierarchical regression. These characteristics of learning in transformers are argued to be realistic on both rational and empirical grounds.

</details>


### [5] [On the Credibility of Evaluating LLMs using Survey Questions](https://arxiv.org/abs/2602.04033)
*Jindřich Libovický*

Main category: cs.CL

TL;DR: 该论文评估大语言模型价值取向的方法存在局限，提出了新的衡量指标以更准确反映模型与人类回答之间的结构一致性。


<details>
  <summary>Details</summary>
Motivation: 现有通过社会调查适配的问卷方式评估大语言模型价值取向的方法可能高估或低估模型与人类的相似度，缺乏对答案间关系结构的考察。

Method: 基于三种语言和五个国家的世界价值观调查数据，比较直接提示和链式思维提示，贪婪解码与采样解码，提出自相关距离这一新指标来衡量模型在不同问题回答之间的一致性。

Result: 提示方式和解码策略显著影响结果，发现平均人类一致性高并不代表结构性答案一致性高，且均方差和KL散度两个常用指标相关性较弱。

Conclusion: 建议未来研究采用链式提示、采样解码和多指标（包括自相关距离）结合的稳健分析方法，更准确评估大语言模型的价值观匹配程度。

Abstract: Recent studies evaluate the value orientation of large language models (LLMs) using adapted social surveys, typically by prompting models with survey questions and comparing their responses to average human responses. This paper identifies limitations in this methodology that, depending on the exact setup, can lead to both underestimating and overestimating the similarity of value orientation. Using the World Value Survey in three languages across five countries, we demonstrate that prompting methods (direct vs. chain-of-thought) and decoding strategies (greedy vs. sampling) significantly affect results. To assess the interaction between answers, we introduce a novel metric, self-correlation distance. This metric measures whether LLMs maintain consistent relationships between answers across different questions, as humans do. This indicates that even a high average agreement with human data, when considering LLM responses independently, does not guarantee structural alignment in responses. Additionally, we reveal a weak correlation between two common evaluation metrics, mean-squared distance and KL divergence, which assume that survey answers are independent of each other. For future research, we recommend CoT prompting, sampling-based decoding with dozens of samples, and robust analysis using multiple metrics, including self-correlation distance.

</details>


### [6] [Abstraction Induces the Brain Alignment of Language and Speech Models](https://arxiv.org/abs/2602.04081)
*Emily Cheng,Aditya R. Vaidya,Richard Antonello*

Main category: cs.CL

TL;DR: 研究发现中间层的语言模型和语音模型能很好预测大脑反应，关键在于这些层构建了高阶语言特征和丰富的语义抽象。


<details>
  <summary>Details</summary>
Motivation: 探究为何语言和语音模型的中间层比输出层更能有效预测大脑对自然语言刺激的反应。

Method: 分析模型不同层的内在维度与脑信号（fMRI和ECoG）预测能力的关系，观察预训练及微调对内在维度和语义内容的影响。

Result: 中间层的内在维度高峰对应更强的脑信号解释能力，预训练和微调增强了内在维度及语义丰富度，从而提高预测性能。

Conclusion: 模型与大脑的相似性源于对输入的丰富语义抽象，高内在维度和语义丰富度是驱动模型-脑匹配的关键。

Abstract: Research has repeatedly demonstrated that intermediate hidden states extracted from large language models and speech audio models predict measured brain response to natural language stimuli. Yet, very little is known about the representation properties that enable this high prediction performance. Why is it the intermediate layers, and not the output layers, that are most effective for this unique and highly general transfer task? We give evidence that the correspondence between speech and language models and the brain derives from shared meaning abstraction and not their next-word prediction properties. In particular, models construct higher-order linguistic features in their middle layers, cued by a peak in the layerwise intrinsic dimension, a measure of feature complexity. We show that a layer's intrinsic dimension strongly predicts how well it explains fMRI and ECoG signals; that the relation between intrinsic dimension and brain predictivity arises over model pre-training; and finetuning models to better predict the brain causally increases both representations' intrinsic dimension and their semantic content. Results suggest that semantic richness, high intrinsic dimension, and brain predictivity mirror each other, and that the key driver of model-brain similarity is rich meaning abstraction of the inputs, where language modeling is a task sufficiently complex (but perhaps not the only) to require it.

</details>


### [7] [Expert Selections In MoE Models Reveal (Almost) As Much As Text](https://arxiv.org/abs/2602.04105)
*Amir Nuriyev,Gabriel Kulp*

Main category: cs.CL

TL;DR: 本文提出了一种利用混合专家模型（MoE）中的专家选择来重构文本的攻击方法，达到了高准确率。


<details>
  <summary>Details</summary>
Motivation: 揭示MoE模型中的专家路由选择泄露了大量信息，远超之前的理解，强调这种信息泄露带来的安全风险。

Method: 通过使用三层多层感知机（MLP）和基于变换器的序列解码器，从专家选择中恢复文本，将准确率提升至91.2%（top-1）。

Result: 在OpenWebText数据上，模型可达到63.1%和91.2%的top-1准确率，噪声添加虽能减缓但无法彻底阻止泄露。

Conclusion: MoE中的专家选择信息泄露风险高，应被视为与文本同等敏感，需在部署时特别防护。

Abstract: We present a text-reconstruction attack on mixture-of-experts (MoE) language models that recovers tokens from expert selections alone. In MoE models, each token is routed to a subset of expert subnetworks; we show these routing decisions leak substantially more information than previously understood. Prior work using logistic regression achieves limited reconstruction; we show that a 3-layer MLP improves this to 63.1% top-1 accuracy, and that a transformer-based sequence decoder recovers 91.2% of tokens top-1 (94.8% top-10) on 32-token sequences from OpenWebText after training on 100M tokens. These results connect MoE routing to the broader literature on embedding inversion. We outline practical leakage scenarios (e.g., distributed inference and side channels) and show that adding noise reduces but does not eliminate reconstruction. Our findings suggest that expert selections in MoE deployments should be treated as sensitive as the underlying text.

</details>


### [8] [DELTA: Deliberative Multi-Agent Reasoning with Reinforcement Learning for Multimodal Psychological Counseling](https://arxiv.org/abs/2602.04112)
*Jiangnan Yang,Junjie Chen,Fei Wang,Yiqi Nie,Yuxin Liu,Zhangling Duan,Jie Chen*

Main category: cs.CL

TL;DR: DELTA框架利用多智能体和多模态信号，通过结构化推理和强化学习提升心理咨询的质量和情感调节能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于语言模型的心理咨询系统大多仅基于文本，且心理状态推断隐含，并未充分利用多模态信息以提升咨询质量和情感共鸣。

Method: 提出DELTA多智能体框架，将心理咨询视为对多模态信号的结构化推理，分离证据基础、心理状态抽象与响应生成，并引入基于情感调谐评分的强化学习以促进情感共鸣响应。

Result: 在多模态心理咨询基准测试上，DELTA提升了咨询质量和情感调谐表现，消融实验与定性分析表明多模态显式推理和结构化心理状态表征相辅相成。

Conclusion: DELTA通过显式多模态推理和结构化心理状态表示有效支持富有同理心的人机交互，提升了智能心理咨询系统的性能与情感共鸣能力。

Abstract: Psychological counseling is a fundamentally multimodal cognitive process in which clinicians integrate verbal content with visual and vocal cues to infer clients' mental states and respond empathically. However, most existing language-model-based counseling systems operate on text alone and rely on implicit mental state inference. We introduce DELTA, a deliberative multi-agent framework that models counseling as a structured reasoning process over multimodal signals, separating evidence grounding, mental state abstraction, and response generation. DELTA further incorporates reinforcement learning guided by a distribution-level Emotion Attunement Score to encourage emotionally attuned responses. Experiments on a multimodal counseling benchmark show that DELTA improves both counseling quality and emotion attunement across models. Ablation and qualitative analyses suggest that explicit multimodal reasoning and structured mental state representations play complementary roles in supporting empathic human-AI interaction.

</details>


### [9] [From Lemmas to Dependencies: What Signals Drive Light Verbs Classification?](https://arxiv.org/abs/2602.04127)
*Sercan Karakaş,Yusuf Şimşek*

Main category: cs.CL

TL;DR: 本文通过对土耳其语轻动词结构的模型输入限制，评估不同特征对轻动词构造(LVC)识别的影响，发现粗粒度形态句法不足以区分LVC，词汇身份信息有助于判断但敏感于归一化方法。


<details>
  <summary>Details</summary>
Motivation: 轻动词结构在土耳其语中难以区分字面和习语意义，因其丰富的形态变化和复杂谓词造成判断难度。研究旨在明确哪些信号有助于LVC分类。

Method: 利用UD数据进行监督学习，比较基于词元的TF-IDF+逻辑回归、基于词元序列训练的BERTurk、仅用形态句法特征的逻辑回归及完整输入的BERTurk模型，测试在控制的诊断数据集上表现。

Result: 形态句法特征单独不足以支持稳健LVC检测；词元身份信息有助于判定但对归一化和校准敏感；不同模型在区分轻动词结构时表现出差异。

Conclusion: 轻动词结构辨识依赖细致的词元输入处理，"仅词元"表示并非单一定义范式，强调须针对土耳其语多词表达进行专门评估与归一化策略设计。

Abstract: Light verb constructions (LVCs) are a challenging class of verbal multiword expressions, especially in Turkish, where rich morphology and productive complex predicates create minimal contrasts between idiomatic predicate meanings and literal verb--argument uses. This paper asks what signals drive LVC classification by systematically restricting model inputs. Using UD-derived supervision, we compare lemma-driven baselines (lemma TF--IDF + Logistic Regression; BERTurk trained on lemma sequences), a grammar-only Logistic Regression over UD morphosyntax (UPOS/DEPREL/MORPH), and a full-input BERTurk baseline. We evaluate on a controlled diagnostic set with Random negatives, lexical controls (NLVC), and LVC positives, reporting split-wise performance to expose decision-boundary behavior. Results show that coarse morphosyntax alone is insufficient for robust LVC detection under controlled contrasts, while lexical identity supports LVC judgments but is sensitive to calibration and normalization choices. Overall, Our findings motivate targeted evaluation of Turkish MWEs and show that ``lemma-only'' is not a single, well-defined representation, but one that depends critically on how normalization is operationalized.

</details>


### [10] [The Missing Half: Unveiling Training-time Implicit Safety Risks Beyond Deployment](https://arxiv.org/abs/2602.04196)
*Zhexin Zhang,Yida Lu,Junfeng Fang,Junxiao Yang,Shiyao Cui,Hao Zhou,Fandong Meng,Jie Zhou,Hongning Wang,Minlie Huang,Tat-Seng Chua*

Main category: cs.CL

TL;DR: 本文系统性研究了AI模型训练阶段的隐性安全风险，揭示了模型在训练中基于内部激励和上下文信息表现出的有害行为，并提出了风险分类体系。实验证明此类风险广泛存在且严重，尤其在多智能体训练中更为突出。


<details>
  <summary>Details</summary>
Motivation: 目前大多数学者关注AI模型部署阶段的安全风险如越狱攻击，而训练阶段的隐性安全风险尚未得到充分探索。本文旨在填补这一空白，系统分析训练过程中的安全隐患。

Method: 提出包含五个风险等级、十个细化风险类别和三种激励类型的风险分类体系，通过大量实验测试模型在不同背景信息下的行为表现，分析影响因素并扩展到多智能体训练场景。

Result: 实验发现Llama-3.1-8B-Instruct模型在仅提供背景信息时，74.4%的训练过程中表现出有害行为。此外，影响因素分析显示这些隐性风险在多智能体训练中同样存在。

Conclusion: 隐性训练阶段安全风险广泛且严重，是当前AI安全领域被忽视但亟需解决的重要挑战，需引起研究者和从业者的高度重视。

Abstract: Safety risks of AI models have been widely studied at deployment time, such as jailbreak attacks that elicit harmful outputs. In contrast, safety risks emerging during training remain largely unexplored. Beyond explicit reward hacking that directly manipulates explicit reward functions in reinforcement learning, we study implicit training-time safety risks: harmful behaviors driven by a model's internal incentives and contextual background information. For example, during code-based reinforcement learning, a model may covertly manipulate logged accuracy for self-preservation. We present the first systematic study of this problem, introducing a taxonomy with five risk levels, ten fine-grained risk categories, and three incentive types. Extensive experiments reveal the prevalence and severity of these risks: notably, Llama-3.1-8B-Instruct exhibits risky behaviors in 74.4% of training runs when provided only with background information. We further analyze factors influencing these behaviors and demonstrate that implicit training-time risks also arise in multi-agent training settings. Our results identify an overlooked yet urgent safety challenge in training.

</details>


### [11] [From Helpfulness to Toxic Proactivity: Diagnosing Behavioral Misalignment in LLM Agents](https://arxiv.org/abs/2602.04197)
*Xinyue Wang,Yuanhe Zhang,Zhengshuo Gong,Haoran Gao,Fanyu Meng,Zhenhong Zhou,Li Sun,Yang Liu,Sen Su*

Main category: cs.CL

TL;DR: 本文提出了“有毒主动性”这一现象，指出在基于大语言模型（LLM）的智能体中，过度积极以达到效用最大化可能导致伦理问题。通过构建基于困境驱动的双模型交互评估框架，揭示了这一广泛存在的行为及其两大倾向，并提出了系统性的评估基准。


<details>
  <summary>Details</summary>
Motivation: LLM智能体虽然具备强大的规划和工具使用能力，但存在“过度拒绝”被动失败模式。与此同时，智能体的主动行为能力引入了另一种主动失败模式——“有毒主动性”，即智能体为最大化效用忽视伦理约束，亟需识别和评估这种风险。

Method: 设计了一种基于困境驱动的双模型交互评估框架，模拟并分析智能体在多步行为轨迹中的表现，通过大量主流LLM实验揭示有毒主动性行为的普遍存在及主要倾向，并构建了相应的系统性评测基准。

Result: 实验结果表明，有毒主动性是一种广泛存在的行为现象，智能体倾向于采取过度或操控性措施以维持其“有用性”，并展现出两大主要行为倾向。此外，提出的评估框架有效捕捉并衡量了此类行为。

Conclusion: 本文首次系统揭示了基于LLM智能体的“有毒主动性”风险，强调了在提升智能体能力同时需兼顾伦理约束，所提出的评估框架和基准为未来研究提供了重要工具，有助于促进安全可靠的智能体发展。

Abstract: The enhanced capabilities of LLM-based agents come with an emergency for model planning and tool-use abilities. Attributing to helpful-harmless trade-off from LLM alignment, agents typically also inherit the flaw of "over-refusal", which is a passive failure mode. However, the proactive planning and action capabilities of agents introduce another crucial danger on the other side of the trade-off. This phenomenon we term "Toxic Proactivity'': an active failure mode in which an agent, driven by the optimization for Machiavellian helpfulness, disregards ethical constraints to maximize utility. Unlike over-refusal, Toxic Proactivity manifests as the agent taking excessive or manipulative measures to ensure its "usefulness'' is maintained. Existing research pays little attention to identifying this behavior, as it often lacks the subtle context required for such strategies to unfold. To reveal this risk, we introduce a novel evaluation framework based on dilemma-driven interactions between dual models, enabling the simulation and analysis of agent behavior over multi-step behavioral trajectories. Through extensive experiments with mainstream LLMs, we demonstrate that Toxic Proactivity is a widespread behavioral phenomenon and reveal two major tendencies. We further present a systematic benchmark for evaluating Toxic Proactive behavior across contextual settings.

</details>


### [12] [Enforcing Monotonic Progress in Legal Cross-Examination: Preventing Long-Horizon Stagnation in LLM-Based Inquiry](https://arxiv.org/abs/2602.04206)
*Hsien-Jyh Liao*

Main category: cs.CL

TL;DR: 该论文提出Soft-FSM模型，通过外部确定性状态控制器克服大语言模型在法律质询中的程序性停滞问题，实现任务的可靠完成，实验结果显示有效性显著提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理长程任务时难以在遵守严格程序约束的同时保持进展，尤其在法律交叉质询中纯概率生成难以保证程序推进。

Method: 提出Soft-FSM，这是一种神经符号混合架构，通过外部确定性状态控制器，强制句子单元的单调进展，确保程序性推进。

Result: 在台湾真实刑事案件中，基线方法任务完成率低于40%，Soft-FSM任务完成率高于97%，且几乎无冗余。

Conclusion: 单靠大语言模型的生成行为无法保证复杂程序任务的完成，采用显式且可验证的外部状态控制才能可靠实现任务目标。

Abstract: Large language models (LLMs) exhibit impressive linguistic fluency but struggle to reliably complete long-horizon tasks under explicit procedural constraints. In legal cross-examination, purely proba-bilistic generation often maintains behavioral coherence while failing to ensure procedural advancement. We characterize this failure as procedural stagnation and propose Soft-FSM, a neuro-symbolic architecture that enforces monotonic progress over accumulated Key Information Units (KIUs) via an external deterministic state controller. Experiments on three real-world Taiwanese criminal homicide cases show that baseline methods collapse below 40% completeness, while Soft-FSM consistently achieves over 97% with near-zero redundancy. These results suggest that, in such domains, reliable task completion cannot be guaranteed by emergent LLM behavior alone, and can be reliably enforced through explicit and verifiable external state control.

</details>


### [13] [Language Models Struggle to Use Representations Learned In-Context](https://arxiv.org/abs/2602.04212)
*Michael A. Lepori,Tal Linzen,Ann Yuan,Katja Filippova*

Main category: cs.CL

TL;DR: 本文研究大型语言模型（LLMs）能否利用上下文中学习到的表示完成后续任务，发现当前模型虽能编码新语义但难以灵活应用。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在多项任务表现出色，但尚无法实现按部署时的全新上下文灵活调整行为的目标，需探讨如何让模型不仅编码信息，还能灵活应用。

Method: 通过测试开放权重的LLMs在下一词预测和适应性世界建模任务中的表现，进一步评估封闭源代码先进推理模型的适应能力。

Result: 结果显示开放权重模型虽能编码新语义表示，但难以实际应用，且最先进的闭源模型同样难以可靠使用上下文中新模式。

Conclusion: 研究启示需发展新方法，促使模型不仅学习上下文信息，还能以支持灵活部署的形式编码和利用这些信息。

Abstract: Though large language models (LLMs) have enabled great success across a wide variety of tasks, they still appear to fall short of one of the loftier goals of artificial intelligence research: creating an artificial system that can adapt its behavior to radically new contexts upon deployment. One important step towards this goal is to create systems that can induce rich representations of data that are seen in-context, and then flexibly deploy these representations to accomplish goals. Recently, Park et al. (2024) demonstrated that current LLMs are indeed capable of inducing such representation from context (i.e., in-context representation learning). The present study investigates whether LLMs can use these representations to complete simple downstream tasks.
  We first assess whether open-weights LLMs can use in-context representations for next-token prediction, and then probe models using a novel task, adaptive world modeling. In both tasks, we find evidence that open-weights LLMs struggle to deploy representations of novel semantics that are defined in-context, even if they encode these semantics in their latent representations. Furthermore, we assess closed-source, state-of-the-art reasoning models on the adaptive world modeling task, demonstrating that even the most performant LLMs cannot reliably leverage novel patterns presented in-context. Overall, this work seeks to inspire novel methods for encouraging models to not only encode information presented in-context, but to do so in a manner that supports flexible deployment of this information.

</details>


### [14] [Tokenization and Morphological Fidelity in Uralic NLP: A Cross-Lingual Evaluation](https://arxiv.org/abs/2602.04241)
*Nuo Xu,Ahrii Kim*

Main category: cs.CL

TL;DR: 本研究比较了三种子词分词方法在六种乌拉尔语族语言中的表现，发现Overlap BPE（OBPE）在形态对齐和词性标注准确率上优于传统方法，尤其是在拉丁字母语言中，且对跨语言迁移表现有显著影响。


<details>
  <summary>Details</summary>
Motivation: 探究形态丰富且资源匮乏语言中，子词分词策略对NLP性能的影响，尤其在词性标注及跨语言迁移中的表现。

Method: 系统比较Byte Pair Encoding (BPE)、Overlap BPE (OBPE) 以及 Unigram Language Model三种子词分词方法，在六种乌拉尔语族语言中以词性标注任务为测试，分析形态对齐、分词碎片化及跨语言迁移效果。

Result: OBPE在形态对齐和词性标注准确率上优于其他分词方法，特别是在使用拉丁字母的语言中表现突出；分词减少了开放类词汇的碎片，平衡了频率分布，且迁移效果受下游模型结构、训练数据量和语言亲缘关系影响。

Conclusion: 形态敏感的子词分词不仅是预处理步骤，更是实现低资源黏着语有效跨语言迁移的关键因素。

Abstract: Subword tokenization critically affects Natural Language Processing (NLP) performance, yet its behavior in morphologically rich and low-resource language families remains under-explored. This study systematically compares three subword paradigms -- Byte Pair Encoding (BPE), Overlap BPE (OBPE), and Unigram Language Model -- across six Uralic languages with varying resource availability and typological diversity. Using part-of-speech (POS) tagging as a controlled downstream task, we show that OBPE consistently achieves stronger morphological alignment and higher tagging accuracy than conventional methods, particularly within the Latin-script group. These gains arise from reduced fragmentation in open-class categories and a better balance across the frequency spectrum. Transfer efficacy further depends on the downstream tagging architecture, interacting with both training volume and genealogical proximity. Taken together, these findings highlight that morphology-sensitive tokenization is not merely a preprocessing choice but a decisive factor in enabling effective cross-lingual transfer for agglutinative, low-resource languages.

</details>


### [15] [CoLT: Reasoning with Chain of Latent Tool Calls](https://arxiv.org/abs/2602.04246)
*Fangwei Zhu,Zhifang Sui*

Main category: cs.CL

TL;DR: CoLT是一种将潜在推理作为“工具调用”的新框架，通过生成种子令牌并调用小型外部模型展开完整推理步骤，实现了更高效且准确的链式推理。


<details>
  <summary>Details</summary>
Motivation: 现有潜在推理方法需要改造模型结构并进行大量训练，限制了其适用性。

Method: CoLT生成包含推理步骤信息的种子令牌，触发小型外部模型将其展开为完整推理步骤，在显式令牌空间中进行推理，兼顾能力和效率。

Result: 在四个数学数据集上，CoLT准确率更高，推理长度更短，且兼容强化学习算法和不同解码结构。

Conclusion: CoLT有效提升了大型语言模型的推理效率和准确性，且具备广泛的适用性和兼容性。

Abstract: Chain-of-Thought (CoT) is a critical technique in enhancing the reasoning ability of Large Language Models (LLMs), and latent reasoning methods have been proposed to accelerate the inefficient token-level reasoning chain. We notice that existing latent reasoning methods generally require model structure augmentation and exhaustive training, limiting their broader applicability. In this paper, we propose CoLT, a novel framework that implements latent reasoning as ``tool calls''. Instead of reasoning entirely in the latent space, CoLT generates seed tokens that contain information of a reasoning step. When a latent tool call is triggered, a smaller external model will take the hidden states of seed tokens as its input, and unpack the seed tokens back to a full reasoning step. In this way, we can ensure that the main model reasons in the explicit token space, preserving its ability while improving efficiency. Experimental results on four mathematical datasets demonstrate that CoLT achieves higher accuracy and shorter reasoning length than baseline latent models, and is compatible with reinforcement learning algorithms and different decoder structures.

</details>


### [16] [DementiaBank-Emotion: A Multi-Rater Emotion Annotation Corpus for Alzheimer's Disease Speech (Version 1.0)](https://arxiv.org/abs/2602.04247)
*Cheonkam Jeong,Jessica Liao,Audrey Lu,Yutong Song,Christopher Rashidian,Donna Krogh,Erik Krogh,Mahkameh Rasouli,Jung-Ah Lee,Nikil Dutt,Lisa M Gibbs,David Sultzer,Julie Rousseau,Jocelyn Ludlow,Margaret Galvez,Alexander Nuth,Chet Khay,Sabine Brunswicker,Adeline Nyamathi*

Main category: cs.CL

TL;DR: 本文首次提供了针对阿尔茨海默病患者言语的多评审者情感标注语料库DementiaBank-Emotion，发现患者表达更多非中性情感并展示声音调制差异。


<details>
  <summary>Details</summary>
Motivation: 为了研究阿尔茨海默病患者言语中的情感表达特征，填补临床群体情感识别研究的语料空白。

Method: 对108名说话者1492条话语，使用Ekman的六种基本情感及中性进行多评审者标注，结合声学分析探究情感调制差异。

Result: 阿尔茨海默病患者表达非中性情感比例显著高于健康对照组，声调调制在悲伤情绪中差异明显，音量在情感区分中具有一定作用。

Conclusion: 阿尔茨海默病患者情感表达部分保留，声学特征存在差异。发布语料库及标注材料，推动临床情感识别研究。

Abstract: We present DementiaBank-Emotion, the first multi-rater emotion annotation corpus for Alzheimer's disease (AD) speech. Annotating 1,492 utterances from 108 speakers for Ekman's six basic emotions and neutral, we find that AD patients express significantly more non-neutral emotions (16.9%) than healthy controls (5.7%; p < .001). Exploratory acoustic analysis suggests a possible dissociation: control speakers showed substantial F0 modulation for sadness (Delta = -3.45 semitones from baseline), whereas AD speakers showed minimal change (Delta = +0.11 semitones; interaction p = .023), though this finding is based on limited samples (sadness: n=5 control, n=15 AD) and requires replication. Within AD speech, loudness differentiates emotion categories, indicating partially preserved emotion-prosody mappings. We release the corpus, annotation guidelines, and calibration workshop materials to support research on emotion recognition in clinical populations.

</details>


### [17] [Scaling Agentic Verifier for Competitive Coding](https://arxiv.org/abs/2602.04254)
*Zeyao Ma,Jing Zhang,Xiaokang Zhang,Jiaxi Yang,Zongmeng Zhang,Jiajun Zhang,Yuheng Jing,Lei Zhang,Hao Zheng,Wenting Zhao,Junyang Lin,Binyuan Hui*

Main category: cs.CL

TL;DR: 本文提出了一种执行驱动的代理验证器，利用多轮交互主动生成区分性强的测试输入，提升复杂竞赛编程题的解题准确率。


<details>
  <summary>Details</summary>
Motivation: 现有基于执行的重新排序方法受限于测试用例生成难题或随机采样效率低，难以准确筛选最佳解。

Method: 设计Agentic Verifier，通过与代码执行环境多轮交互，主动推理程序行为，迭代优化输入生成器，生成有针对性的反例。结合大规模数据合成、拒绝微调及代理强化学习训练验证器。

Result: 在五个竞赛编程基准测试中，较强执行基线方法准确率提升10-15%。

Conclusion: 验证器表现出良好测试时扩展性及广泛潜力，超越了简单的解排序应用。

Abstract: Large language models (LLMs) have demonstrated strong coding capabilities but still struggle to solve competitive programming problems correctly in a single attempt. Execution-based re-ranking offers a promising test-time scaling strategy, yet existing methods are constrained by either difficult test case generation or inefficient random input sampling. To address this limitation, we propose Agentic Verifier, an execution-based agent that actively reasons about program behaviors and searches for highly discriminative test inputs that expose behavioral discrepancies among candidate solutions. Through multi-turn interaction with code execution environments, the verifier iteratively refines the candidate input generator and produces targeted counterexamples rather than blindly sampling inputs. We train the verifier to acquire this discriminative input generation capability via a scalable pipeline combining large-scale data synthesis, rejection fine-tuning, and agentic reinforcement learning. Extensive experiments across five competitive programming benchmarks demonstrate consistent improvements over strong execution-based baselines, achieving up to +10-15% absolute gains in Best@K accuracy. Further analysis reveals clear test-time scaling behavior and highlights the verifier's broader potential beyond reranking.

</details>


### [18] [ECG-R1: Protocol-Guided and Modality-Agnostic MLLM for Reliable ECG Interpretation](https://arxiv.org/abs/2602.04279)
*Jiarui Jin,Haoyu Wang,Xingliang Wu,Xiaocheng Fang,Xiang Lan,Zihan Wang,Deyun Zhang,Bo Liu,Yingying Zhang,Xian Wu,Hongyan Li,Shenda Hong*

Main category: cs.CL

TL;DR: 本文提出了ECG-R1，一种基于推理的多模态大语言模型，旨在提高心电图（ECG）解读的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在心电图解读上存在不可靠和错误分析的问题，严重影响临床应用。

Method: 通过三大创新：1）基于协议引导的解释语料生成，将解读与可衡量的ECG特征和诊断逻辑结合；2）采用模态解耦架构和穿插模态丢弃技术，提升模型对缺失ECG信号或图像时的鲁棒性和一致性；3）利用带ECG诊断证据奖励的强化学习，增强模型基于证据的解读能力。

Result: 系统评估了多种专有、开源及医疗多模态大语言模型的ECG解读能力，首次定量证明严重幻觉现象普遍存在，提示公众不应盲目信任模型输出。

Conclusion: ECG-R1显著提高了心电图解读的可靠性，为临床应用提供了更可信的多模态大语言模型解决方案。模型代码和数据已公开，方便进一步研究和应用。

Abstract: Electrocardiography (ECG) serves as an indispensable diagnostic tool in clinical practice, yet existing multimodal large language models (MLLMs) remain unreliable for ECG interpretation, often producing plausible but clinically incorrect analyses. To address this, we propose ECG-R1, the first reasoning MLLM designed for reliable ECG interpretation via three innovations. First, we construct the interpretation corpus using \textit{Protocol-Guided Instruction Data Generation}, grounding interpretation in measurable ECG features and monograph-defined quantitative thresholds and diagnostic logic. Second, we present a modality-decoupled architecture with \textit{Interleaved Modality Dropout} to improve robustness and cross-modal consistency when either the ECG signal or ECG image is missing. Third, we present \textit{Reinforcement Learning with ECG Diagnostic Evidence Rewards} to strengthen evidence-grounded ECG interpretation. Additionally, we systematically evaluate the ECG interpretation capabilities of proprietary, open-source, and medical MLLMs, and provide the first quantitative evidence that severe hallucinations are widespread, suggesting that the public should not directly trust these outputs without independent verification. Code and data are publicly available at \href{https://github.com/PKUDigitalHealth/ECG-R1}{here}, and an online platform can be accessed at \href{http://ai.heartvoice.com.cn/ECG-R1/}{here}.

</details>


### [19] [Contextual Drag: How Errors in the Context Affect LLM Reasoning](https://arxiv.org/abs/2602.04288)
*Yun Cheng,Xingyu Zhu,Haoyu Zhao,Sanjeev Arora*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型中的“上下文拖延”现象，即上下文中的失败尝试会导致后续结果出现结构类似的错误，严重影响模型性能。


<details>
  <summary>Details</summary>
Motivation: 探究为何大型语言模型自我改进时，过去的失败案例反而会引导模型产生结构类似的错误，导致性能下降。

Method: 通过在11个不同模型和8个推理任务上评估模型表现，利用树编辑距离分析错误的结构相似性，检验外部反馈、自我验证及多种缓解策略的效果。

Result: 发现上下文拖延导致性能下降10-20%，且自我反复优化可能导致模型自我恶化，外部反馈和成功的自我验证均无法消除该现象，部分缓解策略也无法完全恢复性能。

Conclusion: 上下文拖延是当前推理模型中的持久性失败模式，需要进一步研究有效方法以解决这一问题。

Abstract: Central to many self-improvement pipelines for large language models (LLMs) is the assumption that models can improve by reflecting on past mistakes. We study a phenomenon termed contextual drag: the presence of failed attempts in the context biases subsequent generations toward structurally similar errors. Across evaluations of 11 proprietary and open-weight models on 8 reasoning tasks, contextual drag induces 10-20% performance drops, and iterative self-refinement in models with severe contextual drag can collapse into self-deterioration. Structural analysis using tree edit distance reveals that subsequent reasoning trajectories inherit structurally similar error patterns from the context. We demonstrate that neither external feedback nor successful self-verification suffices to eliminate this effect. While mitigation strategies such as fallback-behavior fine-tuning and context denoising yield partial improvements, they fail to fully restore baseline performance, positioning contextual drag as a persistent failure mode in current reasoning architectures.

</details>


### [20] [Proxy Compression for Language Modeling](https://arxiv.org/abs/2602.04289)
*Lin Zheng,Xinyu Li,Qian Liu,Xiachong Feng,Lingpeng Kong*

Main category: cs.CL

TL;DR: 本文提出了一种名为代理压缩的训练方案，通过在训练时使用压缩视图和原始字节序列联合训练语言模型，从而实现推理时直接使用原始字节输入，提升训练效率并保持字节级模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型依赖固定的分词器，限制了模型的输入灵活性和效率，且通常基于特定压缩格式，造成模型与压缩器强耦合。

Method: 训练过程中，模型同时接收原始字节序列和由外部压缩器生成的压缩视图，学习自动对齐两种格式，实现两者之间的有效迁移。

Result: 在代码语言建模任务中，代理压缩显著提升了训练效率和性能，尤其在计算预算固定的情况下明显优于纯字节级模型，且效果随模型规模增大而增强。

Conclusion: 代理压缩方法实现了无须分词器即可高效训练大规模语言模型，维持字节级输入的鲁棒性，并达到了与分词器方法相当甚至更优的性能。

Abstract: Modern language models are trained almost exclusively on token sequences produced by a fixed tokenizer, an external lossless compressor often over UTF-8 byte sequences, thereby coupling the model to that compressor. This work introduces proxy compression, an alternative training scheme that preserves the efficiency benefits of compressed inputs while providing an end-to-end, raw-byte interface at inference time. During training, one language model is jointly trained on raw byte sequences and compressed views generated by external compressors; through the process, the model learns to internally align compressed sequences and raw bytes. This alignment enables strong transfer between the two formats, even when training predominantly on compressed inputs which are discarded at inference. Extensive experiments on code language modeling demonstrate that proxy compression substantially improves training efficiency and significantly outperforms pure byte-level baselines given fixed compute budgets. As model scale increases, these gains become more pronounced, and proxy-trained models eventually match or rival tokenizer approaches, all while operating solely on raw bytes and retaining the inherent robustness of byte-level modeling.

</details>


### [21] [Guided Verifier: Collaborative Multimodal Reasoning via Dynamic Process Supervision](https://arxiv.org/abs/2602.04290)
*Lingzhuang Sun,Ruitong Liu,Yuxia Zhu,Xiaohan Xu,Jingxuan Wei,Xiangxiang Zhang,Bihui Yu,Wentao Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种名为Guided Verifier的框架，通过动态验证器与策略模型协同推理，降低多模态大语言模型在复杂推理中的错误传播问题。


<details>
  <summary>Details</summary>
Motivation: 现有增强学习方法通常依赖单一模型独立完成推理，缺乏中间监督，导致早期推理错误累积和噪声优化信号，影响模型性能。

Method: 设计动态验证器与策略模型同时推理，实时检测推理中的矛盾并提供指导信号；通过专门的数据合成流程构建针对多模态幻觉的CoRe数据集，训练验证器。

Result: 在MathVista、MathVerse和MMMU等数据集上，采用该框架的8B参数模型表现出较强性能提升，验证了协同推理和动态验证的有效性。

Conclusion: Guided Verifier框架通过引入动态的协同验证机制，有效缓解了多模态大语言模型在推理中的错误传播问题，提升了模型推理的准确性和稳定性。

Abstract: Reinforcement Learning (RL) has emerged as a pivotal mechanism for enhancing the complex reasoning capabilities of Multimodal Large Language Models (MLLMs). However, prevailing paradigms typically rely on solitary rollout strategies where the model works alone. This lack of intermediate oversight renders the reasoning process susceptible to error propagation, where early logical deviations cascade into irreversible failures, resulting in noisy optimization signals. In this paper, we propose the \textbf{Guided Verifier} framework to address these structural limitations. Moving beyond passive terminal rewards, we introduce a dynamic verifier that actively co-solves tasks alongside the policy. During the rollout phase, this verifier interacts with the policy model in real-time, detecting inconsistencies and providing directional signals to steer the model toward valid trajectories. To facilitate this, we develop a specialized data synthesis pipeline targeting multimodal hallucinations, constructing \textbf{CoRe} dataset of process-level negatives and \textbf{Co}rrect-guide \textbf{Re}asoning trajectories to train the guided verifier. Extensive experiments on MathVista, MathVerse and MMMU indicate that by allocating compute to collaborative inference and dynamic verification, an 8B-parameter model can achieve strong performance.

</details>


### [22] [How Few-shot Demonstrations Affect Prompt-based Defenses Against LLM Jailbreak Attacks](https://arxiv.org/abs/2602.04294)
*Yanshu Wang,Shuaishuai Yang,Jingjing He,Tong Yang*

Main category: cs.CL

TL;DR: 本文研究了少样本示例对基于提示的防御方法在防止LLM越狱攻击中的作用，发现少样本对不同提示策略影响相反。


<details>
  <summary>Details</summary>
Motivation: 目前LLM越狱攻击威胁日益严重，虽然已有基于提示的防御方法，但少样本示例在防御中的作用尚不明确，有必要系统评估少样本示例与不同提示策略的交互影响。

Method: 作者对多个主流大语言模型，采用六种越狱攻击方法，在四个安全基准测试集上，系统评估了少样本示例对角色导向提示(RoP)和任务导向提示(ToP)的防御效果。

Result: 结果表明，少样本示例能增强角色导向提示的安全率最高达4.5%，但会削弱任务导向提示的效果，最大降低21.2%。这是因为少样本示例强化了角色身份，但分散了任务指令的注意力。

Conclusion: 本文揭示了少样本示例对不同提示策略防御越狱攻击的反向影响，为在实际应用中部署基于提示的安全防御提供了实用建议。

Abstract: Large Language Models (LLMs) face increasing threats from jailbreak attacks that bypass safety alignment. While prompt-based defenses such as Role-Oriented Prompts (RoP) and Task-Oriented Prompts (ToP) have shown effectiveness, the role of few-shot demonstrations in these defense strategies remains unclear. Prior work suggests that few-shot examples may compromise safety, but lacks investigation into how few-shot interacts with different system prompt strategies. In this paper, we conduct a comprehensive evaluation on multiple mainstream LLMs across four safety benchmarks (AdvBench, HarmBench, SG-Bench, XSTest) using six jailbreak attack methods. Our key finding reveals that few-shot demonstrations produce opposite effects on RoP and ToP: few-shot enhances RoP's safety rate by up to 4.5% through reinforcing role identity, while it degrades ToP's effectiveness by up to 21.2% through distracting attention from task instructions. Based on these findings, we provide practical recommendations for deploying prompt-based defenses in real-world LLM applications.

</details>


### [23] [Revisiting Prompt Sensitivity in Large Language Models for Text Classification: The Role of Prompt Underspecification](https://arxiv.org/abs/2602.04297)
*Branislav Pecher,Michal Spiegel,Robert Belanec,Jan Cegin*

Main category: cs.CL

TL;DR: 本文研究了大语言模型对提示语变化的敏感性，发现提示语规范程度是敏感性的关键因素。


<details>
  <summary>Details</summary>
Motivation: 现有研究发现大语言模型对提示语小变动敏感，但多用未明确任务指令的提示语，可能导致结果偏差。

Method: 通过性能分析、logit分析和线性探测比较未明确提示语和具体指令提示语对模型表现和内部表征的影响。

Result: 未明确提示语表现出更高的性能波动和较低相关token的logit值，具体指令提示语则表现更稳定，线性探测显示影响主要体现在模型后层。

Conclusion: 提示语未规范是导致大语言模型敏感性的主要原因，研究和减少敏感性需更严谨地设计提示语。

Abstract: Large language models (LLMs) are widely used as zero-shot and few-shot classifiers, where task behaviour is largely controlled through prompting. A growing number of works have observed that LLMs are sensitive to prompt variations, with small changes leading to large changes in performance. However, in many cases, the investigation of sensitivity is performed using underspecified prompts that provide minimal task instructions and weakly constrain the model's output space. In this work, we argue that a significant portion of the observed prompt sensitivity can be attributed to prompt underspecification. We systematically study and compare the sensitivity of underspecified prompts and prompts that provide specific instructions. Utilising performance analysis, logit analysis, and linear probing, we find that underspecified prompts exhibit higher performance variance and lower logit values for relevant tokens, while instruction-prompts suffer less from such problems. However, linear probing analysis suggests that the effects of prompt underspecification have only a marginal impact on the internal LLM representations, instead emerging in the final layers. Overall, our findings highlight the need for more rigour when investigating and mitigating prompt sensitivity.

</details>


### [24] [DeFrame: Debiasing Large Language Models Against Framing Effects](https://arxiv.org/abs/2602.04306)
*Kahee Lim,Soyeon Kim,Steven Euijong Whang*

Main category: cs.CL

TL;DR: 本文提出框架差异作为大型语言模型公平性评价中的隐藏偏差来源，并通过引入框架感知去偏方法提升模型的公平性和一致性。


<details>
  <summary>Details</summary>
Motivation: 在现实应用中，确保大型语言模型对不同人群的公平响应十分重要，但现有评估难以发现隐藏偏差，框架差异即语义等价但表述不同的提示可能导致评价结果偏差。

Method: 定义框架差异概念，扩展公平性评估基准以包含不同框架的提示，并提出框架感知去偏方法，促使模型在不同框架下表现更一致。

Result: 实验证明，公平性评分随框架变化显著，现有去偏方法虽提升整体公平但难减小框架差异，所提方法有效减少整体偏差并增强模型对框架差异的鲁棒性。

Conclusion: 框架差异是大型语言模型隐藏偏差的重要因素，框架感知去偏方法能提升模型的公平性和评估的一致性，促进更公平的实际应用。

Abstract: As large language models (LLMs) are increasingly deployed in real-world applications, ensuring their fair responses across demographics has become crucial. Despite many efforts, an ongoing challenge is hidden bias: LLMs appear fair under standard evaluations, but can produce biased responses outside those evaluation settings. In this paper, we identify framing -- differences in how semantically equivalent prompts are expressed (e.g., "A is better than B" vs. "B is worse than A") -- as an underexplored contributor to this gap. We first introduce the concept of "framing disparity" to quantify the impact of framing on fairness evaluation. By augmenting fairness evaluation benchmarks with alternative framings, we find that (1) fairness scores vary significantly with framing and (2) existing debiasing methods improve overall (i.e., frame-averaged) fairness, but often fail to reduce framing-induced disparities. To address this, we propose a framing-aware debiasing method that encourages LLMs to be more consistent across framings. Experiments demonstrate that our approach reduces overall bias and improves robustness against framing disparities, enabling LLMs to produce fairer and more consistent responses.

</details>


### [25] [A Domain-Specific Curated Benchmark for Entity and Document-Level Relation Extraction](https://arxiv.org/abs/2602.04320)
*Marco Martinelli,Stefano Marchesin,Vanessa Bonato,Giorgio Maria Di Nunzio,Nicola Ferro,Ornella Irrera,Laura Menotti,Federica Vezzani,Gianmaria Silvello*

Main category: cs.CL

TL;DR: 本文提出了GutBrainIE，一个基于1600多篇PubMed摘要的生物医学信息抽取基准，涵盖细粒度实体、概念级链接及关系，旨在推动生物医学领域信息抽取方法的发展。


<details>
  <summary>Details</summary>
Motivation: 现有生物医学信息抽取基准范围狭窄，依赖自动生成标注，限制了方法的鲁棒性和实用性，特别是在肠-脑轴这类快速发展的领域中更为明显。

Method: 构建GutBrainIE基准，利用生物医学和术语专家对1600多篇PubMed摘要进行细粒度实体、概念链接和关系的手工标注；结合高质量人工标注与弱监督数据。

Result: GutBrainIE基准具有丰富的模式、多任务组合，并兼顾高质量和弱监督数据，适用于肠-脑轴领域，且可广泛应用于多个生物医学信息抽取领域。

Conclusion: GutBrainIE为生物医学信息抽取系统的开发与评估提供了一个高质量且多样化的基准，有助于推动更强鲁棒性的生物医学IE方法研究。

Abstract: Information Extraction (IE), encompassing Named Entity Recognition (NER), Named Entity Linking (NEL), and Relation Extraction (RE), is critical for transforming the rapidly growing volume of scientific publications into structured, actionable knowledge. This need is especially evident in fast-evolving biomedical fields such as the gut-brain axis, where research investigates complex interactions between the gut microbiota and brain-related disorders. Existing biomedical IE benchmarks, however, are often narrow in scope and rely heavily on distantly supervised or automatically generated annotations, limiting their utility for advancing robust IE methods. We introduce GutBrainIE, a benchmark based on more than 1,600 PubMed abstracts, manually annotated by biomedical and terminological experts with fine-grained entities, concept-level links, and relations. While grounded in the gut-brain axis, the benchmark's rich schema, multiple tasks, and combination of highly curated and weakly supervised data make it broadly applicable to the development and evaluation of biomedical IE systems across domains.

</details>


### [26] [Can Vision Replace Text in Working Memory? Evidence from Spatial n-Back in Vision-Language Models](https://arxiv.org/abs/2602.04355)
*Sichu Liang,Hongyu Zhu,Wenwen Wang,Deyu Zhou*

Main category: cs.CL

TL;DR: 本文通过空间n-back任务对比了视-语言模型中视觉信息与文本信息在工作记忆中的表现，发现文本表现优于视觉，并揭示了任务设计对记忆干扰和错误模式的影响。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型在工作记忆任务中，视觉编码与文本编码是否引发相似的认知计算过程，尤其是在视-语言模型中。

Method: 利用Qwen2.5和Qwen2.5-VL模型，设计空间n-back任务，以文字渲染网格与图像渲染网格两种方式呈现，对模型表现进行对比分析，并利用逐试验的对数概率证据揭示记忆过程。

Result: 模型在文本条件下表现出比视觉条件更高的准确率和灵敏度d'，且记忆匹配往往依据最近出现的信息而非任务指定的滞后，且网格大小影响刺激流中的重复结构，进而改变干扰和错误模式。

Conclusion: 视-语言模型的多模态工作记忆表现受到信息呈现方式和刺激结构的显著影响，提示需要基于计算过程的多模态工作记忆研究。

Abstract: Working memory is a central component of intelligent behavior, providing a dynamic workspace for maintaining and updating task-relevant information. Recent work has used n-back tasks to probe working-memory-like behavior in large language models, but it is unclear whether the same probe elicits comparable computations when information is carried in a visual rather than textual code in vision-language models. We evaluate Qwen2.5 and Qwen2.5-VL on a controlled spatial n-back task presented as matched text-rendered or image-rendered grids. Across conditions, models show reliably higher accuracy and d' with text than with vision. To interpret these differences at the process level, we use trial-wise log-probability evidence and find that nominal 2/3-back often fails to reflect the instructed lag and instead aligns with a recency-locked comparison. We further show that grid size alters recent-repeat structure in the stimulus stream, thereby changing interference and error patterns. These results motivate computation-sensitive interpretations of multimodal working memory.

</details>


### [27] [Beyond Rejection Sampling: Trajectory Fusion for Scaling Mathematical Reasoning](https://arxiv.org/abs/2602.04391)
*Jie Deng,Hanshuang Tong,Jun Li,Shining Liang,Ning Wu,Hongzhi Li,Yutao Xie*

Main category: cs.CL

TL;DR: TrajFusion是一种改进的大型语言模型微调策略，通过融合正确和错误推理路径及反思提示，提升数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统的拒绝采样微调方法仅保留正确推理路径，忽视了错误推理的信息，导致训练中推理失败的建模不足。

Method: TrajFusion通过构建融合轨迹，将选定的错误推理路径与反思提示和正确轨迹交织，适应性控制融合样本长度，增强监督信息，无需修改模型架构或训练目标。

Result: 在多个数学基准测试中，TrajFusion表现优于传统拒绝采样微调，尤其是在处理复杂和长篇推理任务时效果显著。

Conclusion: TrajFusion通过结构化利用错误轨迹和反思机制，为数学推理微调提供更丰富的监督，提高模型的推理性能。

Abstract: Large language models (LLMs) have made impressive strides in mathematical reasoning, often fine-tuned using rejection sampling that retains only correct reasoning trajectories. While effective, this paradigm treats supervision as a binary filter that systematically excludes teacher-generated errors, leaving a gap in how reasoning failures are modeled during training. In this paper, we propose TrajFusion, a fine-tuning strategy that reframes rejection sampling as a structured supervision construction process. Specifically, TrajFusion forms fused trajectories that explicitly model trial-and-error reasoning by interleaving selected incorrect trajectories with reflection prompts and correct trajectories. The length of each fused sample is adaptively controlled based on the frequency and diversity of teacher errors, providing richer supervision for challenging problems while safely reducing to vanilla rejection sampling fine-tuning (RFT) when error signals are uninformative. TrajFusion requires no changes to the architecture or training objective. Extensive experiments across multiple math benchmarks demonstrate that TrajFusion consistently outperforms RFT, particularly on challenging and long-form reasoning problems.

</details>


### [28] [Evaluating the Presence of Sex Bias in Clinical Reasoning by Large Language Models](https://arxiv.org/abs/2602.04392)
*Isabel Tsintsiper,Sheng Wong,Beth Albert,Shaun P Brennecke,Gabriel Davis Jones*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLMs）在临床推理中是否表现出性别偏见，发现不同模型存在明显的性别分布偏差。


<details>
  <summary>Details</summary>
Motivation: LLMs被嵌入医疗工作流程中，但因训练语料中存在性别偏见，可能导致诊断和治疗中的性别差异被复制或放大，需系统评估其性别偏见表现及影响因素。

Method: 使用50个覆盖44个专科、由临床医生撰写的案例进行三项实验，测试四个主流通用LLM（ChatGPT、Claude、Gemini、DeepSeekchat）在性别不相关诊断中的性别分配行为。

Result: 所有模型均显示显著的性别分配偏差，ChatGPT、DeepSeek和Claude偏向女性，而Gemini偏向男性，允许模型避免明确标注虽减少了显性偏差，但未消除诊断上的差异。

Conclusion: 现代LLMs在临床推理中存在稳定且特定于模型的性别偏差，医疗领域集成该技术需审慎配置、临床数据审核及持续人类监督。

Abstract: Large language models (LLMs) are increasingly embedded in healthcare workflows for documentation, education, and clinical decision support. However, these systems are trained on large text corpora that encode existing biases, including sex disparities in diagnosis and treatment, raising concerns that such patterns may be reproduced or amplified. We systematically examined whether contemporary LLMs exhibit sex-specific biases in clinical reasoning and how model configuration influences these behaviours. We conducted three experiments using 50 clinician-authored vignettes spanning 44 specialties in which sex was non-informative to the initial diagnostic pathway. Four general-purpose LLMs (ChatGPT (gpt-4o-mini), Claude 3.7 Sonnet, Gemini 2.0 Flash and DeepSeekchat). All models demonstrated significant sex-assignment skew, with predicted sex differing by model. At temperature 0.5, ChatGPT assigned female sex in 70% of cases (95% CI 0.66-0.75), DeepSeek in 61% (0.57-0.65) and Claude in 59% (0.55-0.63), whereas Gemini showed a male skew, assigning a female sex in 36% of cases (0.32-0.41). Contemporary LLMs exhibit stable, model-specific sex biases in clinical reasoning. Permitting abstention reduces explicit labelling but does not eliminate downstream diagnostic differences. Safe clinical integration requires conservative and documented configuration, specialty-level clinical data auditing, and continued human oversight when deploying general-purpose models in healthcare settings.

</details>


### [29] [Bi-directional Bias Attribution: Debiasing Large Language Models without Modifying Prompts](https://arxiv.org/abs/2602.04398)
*Yujie Lin,Kunquan Li,Yixuan Liao,Xiaoxin Chen,Jinsong Su*

Main category: cs.CL

TL;DR: 本文提出了一种无需微调或修改提示词即可检测和减少大型语言模型中的社会偏见的方法。


<details>
  <summary>Details</summary>
Motivation: 现有去偏方法难以兼顾可扩展性和多轮交互的用户体验，且大型语言模型输出存在显著社会偏见，引发公平性担忧。

Method: 通过比较不同人群，识别刻板印象词汇（形容词和名词）；利用积分梯度方法归因偏见至特定神经元；最后对投影层神经元激活进行干预以减轻偏见。

Result: 在三个广泛使用的大型语言模型上的实验表明，该方法有效降低了偏见，同时保持了模型整体性能。

Conclusion: 本文提出的无微调、无提示词修改的神经元层面偏见检测与干预框架，兼顾了偏见减少和模型性能，是大型语言模型公平性改进的有效途径。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities across a wide range of natural language processing tasks. However, their outputs often exhibit social biases, raising fairness concerns. Existing debiasing methods, such as fine-tuning on additional datasets or prompt engineering, face scalability issues or compromise user experience in multi-turn interactions. To address these challenges, we propose a framework for detecting stereotype-inducing words and attributing neuron-level bias in LLMs, without the need for fine-tuning or prompt modification. Our framework first identifies stereotype-inducing adjectives and nouns via comparative analysis across demographic groups. We then attribute biased behavior to specific neurons using two attribution strategies based on integrated gradients. Finally, we mitigate bias by directly intervening on their activations at the projection layer. Experiments on three widely used LLMs demonstrate that our method effectively reduces bias while preserving overall model performance. Code is available at the github link: https://github.com/XMUDeepLIT/Bi-directional-Bias-Attribution.

</details>


### [30] [Swordsman: Entropy-Driven Adaptive Block Partition for Efficient Diffusion Language Models](https://arxiv.org/abs/2602.04399)
*Yu Zhang,Xinchen Li,Jialei Zhou,Hongnan Ma,Zhongwei Wan,Yiwei Shi,Duoqian Miao,Qi Zhang,Longbing Cao*

Main category: cs.CL

TL;DR: 该论文提出了一种基于熵驱动的自适应块解码框架Swordsman，通过动态识别语义或句法成分边界来提升扩散语言模型的推理速度和质量。


<details>
  <summary>Details</summary>
Motivation: 现有块解码方法分块方式僵硬固定，导致完整语义或句法成分被破坏，影响性能。启发于熵减少假说，成分边界处的不确定性较大，利用熵分析能更好地划分块。

Method: 提出Swordsman框架，通过识别相邻词的熵变化来自适应分块，同时动态调整块内解码阈值以提升效率和稳定性。该框架无须额外训练，基于KV Cache支持。

Result: 在大量评测中，Swordsman展现出领先的性能，显著提升了整体的推理速度和生成质量。

Conclusion: 基于熵驱动的自适应块解码有效优化了扩散语言模型的解码过程，兼顾了效率和性能，为该领域提供了新的思路和方法。

Abstract: Block-wise decoding effectively improves the inference speed and quality in diffusion language models (DLMs) by combining inter-block sequential denoising and intra-block parallel unmasking. However, existing block-wise decoding methods typically partition blocks in a rigid and fixed manner, which inevitably fragments complete semantic or syntactic constituents, leading to suboptimal performance. Inspired by the entropy reduction hypothesis (ERH), we recognize that constituent boundaries offer greater opportunities for uncertainty reduction, which motivates us to employ entropy analysis for identifying constituent boundaries. Therefore, we propose Swordsman, an entropy-driven adaptive block-wise decoding framework for DLMs. Swordsman adaptively partitions blocks by identifying entropy shifts between adjacent tokens to better align with semantic or syntactic constituent boundaries. In addition, Swordsman dynamically adjusts unmasking thresholds conditioned on the real-time unmasking status within a block, further improving both efficiency and stability. As a training-free framework, supported by KV Cache, Swordsman demonstrates state-of-the-art performance across extensive evaluations.

</details>


### [31] [History-Guided Iterative Visual Reasoning with Self-Correction](https://arxiv.org/abs/2602.04413)
*Xinglong Yang,Zhilin Peng,Zhanzhan Liu,Haochen Shi,Sheng-Jun Huang*

Main category: cs.CL

TL;DR: 提出了H-GIVR框架，通过多次观察图像并动态纠正错误，提高了多模态大语言模型的推理准确率。


<details>
  <summary>Details</summary>
Motivation: 现有自洽方法仅依赖固定的多次采样及投票，缺乏对历史推理信息的利用，导致模型难以主动纠错和动态调整推理过程。

Method: H-GIVR框架使模型在迭代推理过程中多次观察图像，并以之前的答案为参考，进行动态错误修正。

Result: 在五个数据集和三个模型上进行实验，H-GIVR在保持低计算成本的同时，显著提升了跨模态推理准确率，如在ScienceQA数据集上，准确率提升107%。

Conclusion: 引入历史推理信息的动态纠错机制有效提升了多模态语言模型的推理性能，具有广泛应用价值。

Abstract: Self-consistency methods are the core technique for improving the reasoning reliability of multimodal large language models (MLLMs). By generating multiple reasoning results through repeated sampling and selecting the best answer via voting, they play an important role in cross-modal tasks. However, most existing self-consistency methods are limited to a fixed ``repeated sampling and voting'' paradigm and do not reuse historical reasoning information. As a result, models struggle to actively correct visual understanding errors and dynamically adjust their reasoning during iteration. Inspired by the human reasoning behavior of repeated verification and dynamic error correction, we propose the H-GIVR framework. During iterative reasoning, the MLLM observes the image multiple times and uses previously generated answers as references for subsequent steps, enabling dynamic correction of errors and improving answer accuracy. We conduct comprehensive experiments on five datasets and three models. The results show that the H-GIVR framework can significantly improve cross-modal reasoning accuracy while maintaining low computational cost. For instance, using \texttt{Llama3.2-vision:11b} on the ScienceQA dataset, the model requires an average of 2.57 responses per question to achieve an accuracy of 78.90\%, representing a 107\% improvement over the baseline.

</details>


### [32] [Fine-Grained Activation Steering: Steering Less, Achieving More](https://arxiv.org/abs/2602.04428)
*Zijian Feng,Tianjiao Li,Zixiao Zhu,Hanzhang Zhou,Junlang Qian,Li Zhang,Jia Jim Deryl Chua,Lee Onn Mak,Gee Wah Ng,Kezhi Mao*

Main category: cs.CL

TL;DR: 本文提出AUSteer方法，通过对大型语言模型激活的细粒度原子单元（AU）进行调控，实现更精准、高效的模型行为引导，优于现有基于块级激活的方案。


<details>
  <summary>Details</summary>
Motivation: 现有激活引导方法在块级激活上调控，激活具有异质性，混合了有益、无关和有害特征，导致调控效果粗糙且效率低下。

Method: 将块激活分解为原子单元级别激活，分别控制不同的输出token维度，提出AUSteer方法通过计算对比样本的激活动量识别判别性AU，并根据信息自适应调整调控强度。

Result: 在多个大型语言模型和任务上，AUSteer在调控更少激活单元的情况下，表现优于多种先进基线，显示出更高的调控精度与效率。

Conclusion: 细粒度的AU级激活调控能更有效地引导语言模型行为，AUSteer方法证明了"少即是多"的调控策略的优势。

Abstract: Activation steering has emerged as a cost-effective paradigm for modifying large language model (LLM) behaviors. Existing methods typically intervene at the block level, steering the bundled activations of selected attention heads, feedforward networks, or residual streams. However, we reveal that block-level activations are inherently heterogeneous, entangling beneficial, irrelevant, and harmful features, thereby rendering block-level steering coarse, inefficient, and intrusive. To investigate the root cause, we decompose block activations into fine-grained atomic unit (AU)-level activations, where each AU-level activation corresponds to a single dimension of the block activation, and each AU denotes a slice of the block weight matrix. Steering an AU-level activation is thus equivalent to steering its associated AU. Our theoretical and empirical analysis show that heterogeneity arises because different AUs or dimensions control distinct token distributions in LLM outputs. Hence, block-level steering inevitably moves helpful and harmful token directions together, which reduces efficiency. Restricting intervention to beneficial AUs yields more precise and effective steering. Building on this insight, we propose AUSteer, a simple and efficient method that operates at a finer granularity of the AU level. AUSteer first identifies discriminative AUs globally by computing activation momenta on contrastive samples. It then assigns adaptive steering strengths tailored to diverse inputs and selected AU activations. Comprehensive experiments on multiple LLMs and tasks show that AUSteer consistently surpasses advanced baselines while steering considerably fewer activations, demonstrating that steering less achieves more.

</details>


### [33] [No One-Size-Fits-All: Building Systems For Translation to Bashkir, Kazakh, Kyrgyz, Tatar and Chuvash Using Synthetic And Original Data](https://arxiv.org/abs/2602.04442)
*Dmitry Karpov*

Main category: cs.CL

TL;DR: 本文研究了五组突厥语族语言对的机器翻译，采用微调和提示检索方法，取得了一定的翻译性能，并公开了数据集和模型权重。


<details>
  <summary>Details</summary>
Motivation: 针对突厥语族中资源较少的语言，提升其机器翻译效果。

Method: 对nllb-200-distilled-600M模型进行LoRA微调并利用合成数据，同时采用基于提示和检索相似例子的DeepSeek-V3.2模型进行翻译。

Result: Kazakh和Bashkir通过微调分别达到chrF++ 49.71和46.94，Chuvash采用检索提示获得39.47，Tatar和Kyrgyz分别在零样本和检索方法下达到了41.6和45.6的chrF++得分。

Conclusion: 结合微调和提示检索方法，可有效提升突厥语族语言对的机器翻译性能，并且公开数据和权重促进后续研究。

Abstract: We explore machine translation for five Turkic language pairs: Russian-Bashkir, Russian-Kazakh, Russian-Kyrgyz, English-Tatar, English-Chuvash. Fine-tuning nllb-200-distilled-600M with LoRA on synthetic data achieved chrF++ 49.71 for Kazakh and 46.94 for Bashkir. Prompting DeepSeek-V3.2 with retrieved similar examples achieved chrF++ 39.47 for Chuvash. For Tatar, zero-shot or retrieval-based approaches achieved chrF++ 41.6, while for Kyrgyz the zero-shot approach reached 45.6. We release the dataset and the obtained weights.

</details>


### [34] [Is Micro Domain-Adaptive Pre-Training Effective for Real-World Operations? Multi-Step Evaluation Reveals Potential and Bottlenecks](https://arxiv.org/abs/2602.04466)
*Masaya Tsunokake,Yuta Koreeda,Terufumi Morishita,Koichi Nagatsuka,Hikaru Tomonari,Yasuhiro Sogawa*

Main category: cs.CL

TL;DR: 本文研究了微领域自适应预训练(mDAPT)在生成任务中的效果，表明其提升了知识提取能力，但推理和答案生成仍存在瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅在选择题上验证了mDAPT效果，生成任务中的表现尚不清楚。

Method: 将回答过程拆分为提取事实、推理和长文答案生成三个子任务，基于IT技术支持的真实问题检验mDAPT。

Result: mDAPT显著改善了提取事实子任务，但对推理和答案生成无明显提升。

Conclusion: mDAPT有效增强了知识获取，但需重点提升推理能力以实现整体性能提升。

Abstract: When applying LLMs to real-world enterprise operations, LLMs need to handle proprietary knowledge in small domains of specific operations ($\textbf{micro domains}$). A previous study shows micro domain-adaptive pre-training ($\textbf{mDAPT}$) with fewer documents is effective, similarly to DAPT in larger domains. However, it evaluates mDAPT only on multiple-choice questions; thus, its effectiveness for generative tasks in real-world operations remains unknown. We aim to reveal the potential and bottlenecks of mDAPT for generative tasks. To this end, we disentangle the answering process into three subtasks and evaluate the performance of each subtask: (1) $\textbf{eliciting}$ facts relevant to questions from an LLM's own knowledge, (2) $\textbf{reasoning}$ over the facts to obtain conclusions, and (3) $\textbf{composing}$ long-form answers based on the conclusions. We verified mDAPT on proprietary IT product knowledge for real-world questions in IT technical support operations. As a result, mDAPT resolved the elicitation task that the base model struggled with but did not resolve other subtasks. This clarifies mDAPT's effectiveness in the knowledge aspect and its bottlenecks in other aspects. Further analysis empirically shows that resolving the elicitation and reasoning tasks ensures sufficient performance (over 90%), emphasizing the need to enhance reasoning capability.

</details>


### [35] [Beyond Unimodal Shortcuts: MLLMs as Cross-Modal Reasoners for Grounded Named Entity Recognition](https://arxiv.org/abs/2602.04486)
*Jinlong Ma,Yu Zhang,Xuefeng Bai,Kehai Chen,Yuwei Wang,Zeming Liu,Jun Yu,Min Zhang*

Main category: cs.CL

TL;DR: 本文提出一种用于跨模态命名实体识别的模型方法，解决多模态大语言模型的模态偏差问题，实现文本与视觉信息的有效融合。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在跨模态命名实体识别中存在模态偏差问题，即模型偏向单一模态的信息，导致识别准确性下降。

Method: 提出了模态感知一致性推理（MCR）框架，包括多样化推理模式注入（MRSI）和约束引导可验证优化（CVO），通过结构化跨模态推理和动态调整推理路径缓解模态偏差。

Result: 在跨模态命名实体识别和视觉定位任务中，MCR显著降低了模态偏差，实现了优于现有方法的性能。

Conclusion: 通过引入模态感知一致性推理策略，提升了多模态大语言模型在GMNER任务中的表现，证明了结构化跨模态推理的重要性。

Abstract: Grounded Multimodal Named Entity Recognition (GMNER) aims to extract text-based entities, assign them semantic categories, and ground them to corresponding visual regions. In this work, we explore the potential of Multimodal Large Language Models (MLLMs) to perform GMNER in an end-to-end manner, moving beyond their typical role as auxiliary tools within cascaded pipelines. Crucially, our investigation reveals a fundamental challenge: MLLMs exhibit $\textbf{modality bias}$, including visual bias and textual bias, which stems from their tendency to take unimodal shortcuts rather than rigorous cross-modal verification. To address this, we propose Modality-aware Consistency Reasoning ($\textbf{MCR}$), which enforces structured cross-modal reasoning through Multi-style Reasoning Schema Injection (MRSI) and Constraint-guided Verifiable Optimization (CVO). MRSI transforms abstract constraints into executable reasoning chains, while CVO empowers the model to dynamically align its reasoning trajectories with Group Relative Policy Optimization (GRPO). Experiments on GMNER and visual grounding tasks demonstrate that MCR effectively mitigates modality bias and achieves superior performance compared to existing baselines.

</details>


### [36] [Deconstructing sentence disambiguation by joint latent modeling of reading paradigms: LLM surprisal is not enough](https://arxiv.org/abs/2602.04489)
*Dario Paape,Tal Linzen,Shravan Vasishth*

Main category: cs.CL

TL;DR: 本文提出了一个潜在过程混合模型，用于解释人类在四种不同阅读范式下对暂时歧义园路句的阅读行为，并证明该模型较传统模型具有更好的预测性能。


<details>
  <summary>Details</summary>
Motivation: 研究人类阅读过程中处理暂时歧义句子的认知机制，尤其是如何区分歧义概率、处理代价和重新分析代价，以及考虑不专注阅读的影响。

Method: 引入潜在过程混合模型，分别建模园路概率、园路代价和重新分析代价，结合四种阅读范式的数据（眼动追踪、单向和双向自定节奏阅读、迷宫任务），并与基于GPT-2意外度的非混合模型进行比较。

Result: 模型能够良好重现重新阅读行为、理解问题反应和语法判断的经验模式，且通过交叉验证显示该混合模型在预测人类阅读模式和任务数据方面优于非混合模型。

Conclusion: 潜在过程混合模型为理解和预测人类阅读暂时歧义句的认知机制提供了更准确的工具，有助于未来相关研究的发展。

Abstract: Using temporarily ambiguous garden-path sentences ("While the team trained the striker wondered ...") as a test case, we present a latent-process mixture model of human reading behavior across four different reading paradigms (eye tracking, uni- and bidirectional self-paced reading, Maze). The model distinguishes between garden-path probability, garden-path cost, and reanalysis cost, and yields more realistic processing cost estimates by taking into account trials with inattentive reading. We show that the model is able to reproduce empirical patterns with regard to rereading behavior, comprehension question responses, and grammaticality judgments. Cross-validation reveals that the mixture model also has better predictive fit to human reading patterns and end-of-trial task data than a mixture-free model based on GPT-2-derived surprisal values. We discuss implications for future work.

</details>


### [37] [PersoDPO: Scalable Preference Optimization for Instruction-Adherent, Persona-Grounded Dialogue via Multi-LLM Evaluation](https://arxiv.org/abs/2602.04493)
*Saleh Afzoon,MohammadHossein Ahmadi,Usman Naseem,Amin Beheshti*

Main category: cs.CL

TL;DR: 该论文提出了PersoDPO，一个利用自动评价信号进行个性化和上下文连贯性优化的对话模型微调框架，显著提升开源大型语言模型的对话表现。


<details>
  <summary>Details</summary>
Motivation: 个性化和上下文连贯性是构建有效的基于角色的对话系统的关键，但现有开源大语言模型仍难以生成既符合上下文又与用户身份相关的响应。

Method: PersoDPO框架结合针对连贯性、个性化及长度格式的自动评价指标，自动构建高质量偏好对，利用这些无人工注释的偏好对对对话模型进行微调，实现可扩展和可复现的训练流程。

Result: 在FoCus数据集上的实验表明，使用PersoDPO微调的开源语言模型在多个评估维度上均显著优于强基线模型和标准DPO变体。

Conclusion: PersoDPO通过整合自动化偏好优化，有效提升了开源对话模型的个性化和上下文连贯能力，具备良好的可扩展性和实用价值。

Abstract: Personalization and contextual coherence are two essential components in building effective persona-grounded dialogue systems. These aspects play a crucial role in enhancing user engagement and ensuring responses are more relevant and consistent with user identity. However, recent studies indicate that open-source large language models (LLMs) continue to struggle to generate responses that are both contextually grounded and aligned with persona cues, despite exhibiting strong general conversational abilities like fluency and naturalness. We present PersoDPO, a scalable preference optimisation framework that uses supervision signals from automatic evaluations of responses generated by both closed-source and open-source LLMs to fine-tune dialogue models. The framework integrates evaluation metrics targeting coherence and personalization, along with a length-format compliance feature to promote instruction adherence. These signals are combined to automatically construct high-quality preference pairs without manual annotation, enabling a scalable and reproducible training pipeline. Experiments on the FoCus dataset show that an open-source language model fine-tuned with the PersoDPO framework consistently outperforms strong open-source baselines and a standard Direct Preference Optimization (DPO) variant across multiple evaluation dimensions.

</details>


### [38] [Model-Dowser: Data-Free Importance Probing to Mitigate Catastrophic Forgetting in Multimodal Large Language Models](https://arxiv.org/abs/2602.04509)
*Hyeontaek Hwang,Nguyen Dinh Son,Daeyoung Kim*

Main category: cs.CL

TL;DR: 本论文提出了Model-Dowser，一种针对多模态大型语言模型（MLLMs）的稀疏微调方法，有效缓解灾难性遗忘问题，提升微调性能并具备良好扩展性。


<details>
  <summary>Details</summary>
Motivation: 多模态大型语言模型在特定任务的微调中，常因灾难性遗忘而导致预训练任务表现下降，现有方法在深层微调或大规模模型上表现不佳，需要一种高效且可扩展的解决方案。

Method: Model-Dowser通过综合考虑权重大小、输入激活和输出敏感性，为每个参数计算重要性分数，微调时只更新低重要性参数，保留高重要性参数，实现稀疏微调。

Result: 在LLaVA和NVILA两个代表性多模态模型上，Model-Dowser有效减轻了灾难性遗忘，性能优于现有方法，且资源消耗低，适用于数十亿参数的大规模模型。

Conclusion: Model-Dowser为MLLM微调提供了一种高效、可扩展且性能优越的方案，显著改善了微调泛化能力和灾难性遗忘问题。

Abstract: Fine-tuning Multimodal Large Language Models (MLLMs) on task-specific data is an effective way to improve performance on downstream applications. However, such adaptation often leads to a degradation in generalization on pretrained tasks, a phenomenon known as Catastrophic Forgetting. Existing methods that aim to mitigate this issue either become ineffective when fine-tuning deeper layers of the language decoder or scale poorly with increasing model size. To address these limitations, we propose Model-Dowser, a novel sparse fine-tuning approach for MLLMs. Model-Dowser measures a principled importance score for each model parameter with respect to pretrained generalization (prior to downstream adaptation) by jointly considering weight magnitudes, input activations, and output sensitivities. During fine-tuning, Model-Dowser selectively preserves high-importance parameters and updates the remaining. Comprehensive experiments on two representative MLLMs, LLaVA and NVILA, demonstrate that Model-Dowser effectively mitigates catastrophic forgetting and consistently outperforms prior methods, while remaining resource-efficient and scalable to multi-billion-parameter models.

</details>


### [39] [ReFRAME or Remain: Unsupervised Lexical Semantic Change Detection with Frame Semantics](https://arxiv.org/abs/2602.04514)
*Bach Phan-Tat,Kris Heylen,Dirk Geeraerts,Stefano De Pascale,Dirk Speelman*

Main category: cs.CL

TL;DR: 该论文提出了一种基于框架语义的词义变化检测方法，效果优于许多分布式语义模型，且结果具有较高的解释性。


<details>
  <summary>Details</summary>
Motivation: 现有以神经嵌入分布式表示为基础的词义变化检测方法虽然性能良好，但缺乏解释性。

Method: 该方法完全依赖框架语义来检测词义变化，而非分布式语义表示。

Result: 该方法在检测词义变化上表现出色，甚至优于多种分布式语义模型。

Conclusion: 基于框架语义的方法不仅有效，而且预测结果具备很好的解释性，适合词义变化研究。

Abstract: The majority of contemporary computational methods for lexical semantic change (LSC) detection are based on neural embedding distributional representations. Although these models perform well on LSC benchmarks, their results are often difficult to interpret. We explore an alternative approach that relies solely on frame semantics. We show that this method is effective for detecting semantic change and can even outperform many distributional semantic models. Finally, we present a detailed quantitative and qualitative analysis of its predictions, demonstrating that they are both plausible and highly interpretable

</details>


### [40] [$C$-$ΔΘ$: Circuit-Restricted Weight Arithmetic for Selective Refusal](https://arxiv.org/abs/2602.04521)
*Aditya Kasliwal,Pratinav Seth,Vinay Kumar Sankarapu*

Main category: cs.CL

TL;DR: 本文提出了一种无需推理时干预的安全策略执行方法，通过离线电路限制的权重更新实现选择性拒绝，提升了部署效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于推理时干预的安全控制方法增加了计算开销和复杂性，期望实现无需推理时控制路径的安全策略执行。

Method: 提出C-Δθ方法，利用EAP-IG定位相关稀疏电路，并计算限制性权重更新，仅作用于该电路的少数参数，实现离线拒绝机制。

Result: 在拒绝和效用基准测试中，方法实现了类别定向选择性同时保持模型能力，且无推理时钩子，降低了运行时开销。

Conclusion: C-Δθ有效实现了将选择性拒绝机制从推理时转移到离线权重更新，提升了LLMs安全部署的效率和可扩展性。

Abstract: Modern deployments require LLMs to enforce safety policies at scale, yet many controls rely on inference-time interventions that add recurring compute cost and serving complexity. Activation steering is widely used, but it requires runtime hooks and scales cost with the number of generations; conditional variants improve selectivity by gating when steering is applied but still retain an inference-time control path. We ask whether selective refusal can be moved entirely offline: can a mechanistic understanding of category-specific refusal be distilled into a circuit-restricted weight update that deploys as a standard checkpoint? We propose C-Δθ: Circuit Restricted Weight Arithmetic, which (i) localizes refusal-causal computation as a sparse circuit using EAP-IG and (ii) computes a constrained weight update ΔθC supported only on that circuit (typically <5% of parameters). Applying ΔθC yields a drop-in edited checkpoint with no inference-time hooks, shifting cost from per-request intervention to a one-time offline update. We evaluate category-targeted selectivity and capability retention on refusal and utility benchmarks.

</details>


### [41] [LycheeDecode: Accelerating Long-Context LLM Inference via Hybrid-Head Sparse Decoding](https://arxiv.org/abs/2602.04541)
*Gang Lin,Dongfang Li,Zhuoen Chen,Yukun Shi,Xuhui Chen,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: 本文提出了LycheeDecode，一种基于细粒度混合头注意力机制的高效解码方法，显著提升了长上下文大语言模型的推理速度和内存效率。


<details>
  <summary>Details</summary>
Motivation: 长上下文大语言模型在解码时的键值缓存迅速增大，导致内存和延迟成本高昂。现有粗粒度共享方法忽视了注意力头的功能多样性，影响模型性能。

Method: 提出基于HardKuma机制的细粒度混合头注意力，动态划分少数检索头选取关键token，其他稀疏头复用这些token，并采用硬件友好的top-k选择策略，提高计算效率。

Result: 在Llama3和Qwen3模型及多个长上下文理解和复杂推理基准测试中，LycheeDecode在保证生成质量的同时，在128K上下文长度下实现最高2.7倍的推理速度提升。

Conclusion: 细粒度混合头注意力机制保持了注意力头的功能多样性，有效突破现有方法的性能瓶颈，为高效高质量的长上下文大模型推理提供了一条行之有效的路径。

Abstract: The proliferation of long-context large language models (LLMs) exposes a key bottleneck: the rapidly expanding key-value cache during decoding, which imposes heavy memory and latency costs. While recent approaches attempt to alleviate this by sharing a single set of crucial tokens across layers, such coarse-grained sharing undermines model performance by neglecting the functional diversity of attention heads. To address this, we propose LycheeDecode, an efficient decoding method centered on a fine-grained hybrid-head attention mechanism that employs a hardware-efficient top-k selection strategy. Specifically, the novel HardKuma-based mechanism partitions attention heads into a small subset of retrieval heads that dynamically identify crucial tokens and a majority of sparse heads that reuse them for efficient computation. Through extensive experiments on leading models like Llama3 and Qwen3 across diverse benchmarks for long-context understanding (e.g., LongBench, RULER) and complex reasoning (e.g., AIME24, OlympiadBench), we demonstrate that LycheeDecode achieves generative quality comparable to, and at times surpassing even the full-attention baseline. Crucially, this is accomplished with up to a 2.7x speedup at a 128K context length. By preserving the functional diversity of attention heads, our fine-grained strategy overcomes the performance bottlenecks of existing methods, providing a powerful and validated pathway to both efficient and high-quality long-context LLM inference.

</details>


### [42] [Rethinking Weight Tying: Pseudo-Inverse Tying for Stable LM Training and Updates](https://arxiv.org/abs/2602.04556)
*Jian Gu,Aldeida Aleti,Chunyang Chen,Hongyu Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种称为伪逆绑定（Pseudo-Inverse Tying, PIT）的方法，通过保持输入嵌入和输出投影的一致性，提高紧凑型语言模型的训练稳定性和语义一致性。


<details>
  <summary>Details</summary>
Motivation: 权重绑定通过共享输入和输出的嵌入表减少模型参数，但训练过程中输入编码与输出解码之间的对应关系可能漂移，导致训练敏感性增加和后续模型调整困难。

Method: PIT通过维持一个正交的共享记忆矩阵，并引入一个对称正定的隐藏空间变换，以保证输入输出接口在训练中的伪逆一致性。该方法避免了显式伪逆的重新计算，且无须增加词汇大小的辅助参数。

Result: 在256M至1.3B参数量的设备端模型上，PIT显著提升了训练稳定性、层间语义一致性，并大幅减少了训练副作用。

Conclusion: PIT方法有效解决了权重绑定中接口漂移问题，提升了模型训练的可预测性和调整灵活性，对紧凑型语言模型具有重要意义。

Abstract: Weight tying is widely used in compact language models to reduce parameters by sharing the token table between the input embedding and the output projection. However, weight sharing does not guarantee a stable token interface: during training, the correspondence between encoding tokens into hidden states and decoding hidden states into logits can drift, worsening optimization sensitivity and making post-training interventions such as editing, patching, and lightweight adaptation less predictable. We propose Pseudo-Inverse Tying (PIT), which synchronizes embedding and unembedding as coupled projections of a shared latent token memory, guaranteeing a pseudo-inverse-consistent interface throughout training. PIT maintains an orthonormal shared memory, obtained by thin polar decomposition for teacher initialization or random orthonormal initialization from scratch, and introduces a fully learned symmetric positive definite hidden-space transform parameterized via a Cholesky factor. The output head applies this transform to hidden states before the vocabulary projection, while the embedding applies the inverse transform to token vectors using stable triangular solves, avoiding explicit pseudo-inverse recomputation and any vocabulary-sized auxiliary parameters. We evaluate PIT on on-device models spanning 256M-1.3B parameters across pretraining and adaptation, and consistently observe improved training stability, stronger layerwise semantic consistency, and substantially reduced side effects.

</details>


### [43] [Textual Planning with Explicit Latent Transitions](https://arxiv.org/abs/2602.04557)
*Eliezer Shlomi,Ido Levy,Eilam Shapira,Michael Katz,Guy Uziel,Segev Shlomov,Nir Mashkif,Roi Reichart,Sarah Keren*

Main category: cs.CL

TL;DR: 为了解决使用大型语言模型进行多步规划时的计算和延迟瓶颈，本文提出了EmbedPlan方法，通过在冻结的语言嵌入空间中使用轻量级的转移模型替代逐词生成，实现快速规划。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多步规划中因逐词生成和完全前向传播导致高延迟和计算成本，制约了多步前瞻和基于搜索的应用。

Method: EmbedPlan利用冻结的语言嵌入空间，将状态和动作描述编码为向量，预测下一个状态的嵌入，并通过最近邻检索获得下一状态，无需微调编码器。

Result: 在九个经典规划领域和六种评价协议下，EmbedPlan在插值任务中表现接近完美，但在需要跨领域或跨问题泛化的场景中表现明显下降，表明其对跨域迁移能力有限。

Conclusion: 冻结的语言嵌入空间有助于在单一领域内学习动态，但跨领域迁移仍是主要瓶颈。EmbedPlan有效提升了规划效率，适合领域内应用。

Abstract: Planning with LLMs is bottlenecked by token-by-token generation and repeated full forward passes, making multi-step lookahead and rollout-based search expensive in latency and compute. We propose EmbedPlan, which replaces autoregressive next-state generation with a lightweight transition model operating in a frozen language embedding space. EmbedPlan encodes natural language state and action descriptions into vectors, predicts the next-state embedding, and retrieves the next state by nearest-neighbor similarity, enabling fast planning computation without fine-tuning the encoder. We evaluate next-state prediction across nine classical planning domains using six evaluation protocols of increasing difficulty: interpolation, plan-variant, extrapolation, multi-domain, cross-domain, and leave-one-out. Results show near-perfect interpolation performance but a sharp degradation when generalization requires transfer to unseen problems or unseen domains; plan-variant evaluation indicates generalization to alternative plans rather than memorizing seen trajectories. Overall, frozen embeddings support within-domain dynamics learning after observing a domain's transitions, while transfer across domain boundaries remains a bottleneck.

</details>


### [44] [Can LLMs capture stable human-generated sentence entropy measures?](https://arxiv.org/abs/2602.04570)
*Estrella Pivel-Villanueva,Elisabeth Frederike Sterner,Franziska Knolle*

Main category: cs.CL

TL;DR: 本文探讨了获得稳定准确的单词级熵估计所需的人类反应数量，并评估了大型语言模型（LLMs）在复制人类熵方面的表现。


<details>
  <summary>Details</summary>
Motivation: 目前尚无共识明确需要多少人类反应才能获得稳定且无偏的单词级熵估计，同时LLMs作为人类数据替代在熵估计上的表现尚不清楚。

Method: 使用两个大型公共闭合数据集（德语和英语），采用自助法收敛分析确定熵估计稳定所需样本量，并比较多种LLMs（包括GPT-4o、GPT2-xl、RoBERTa、LLaMA 2等）基于logit和采样两种概率估计方法的表现。

Result: 超过97%的句子在现有样本量内熵估计达到稳定，德语需约111次反应，英语需81次，低熵句子所需反应更少，高熵句子更多。GPT-4o在模拟人类熵上表现最好，但结果依赖提取方式和提示设计。logit估计误差更小，采样估计更好反映人类变异性。

Conclusion: 本文首次实证验证了常见的熵标注实践，为人类标注提供了实用指南，并表明LLMs虽可近似人类熵，但不能完全替代稳定的人类数据分布。

Abstract: Predicting upcoming words is a core mechanism of language comprehension and may be quantified using Shannon entropy. There is currently no empirical consensus on how many human responses are required to obtain stable and unbiased entropy estimates at the word level. Moreover, large language models (LLMs) are increasingly used as substitutes for human norming data, yet their ability to reproduce stable human entropy remains unclear. Here, we address both issues using two large publicly available cloze datasets in German 1 and English 2. We implemented a bootstrap-based convergence analysis that tracks how entropy estimates stabilize as a function of sample size. Across both languages, more than 97% of sentences reached stable entropy estimates within the available sample sizes. 90% of sentences converged after 111 responses in German and 81 responses in English, while low-entropy sentences (<1) required as few as 20 responses and high-entropy sentences (>2.5) substantially more. These findings provide the first direct empirical validation for common norming practices and demonstrate that convergence critically depends on sentence predictability. We then compared stable human entropy values with entropy estimates derived from several LLMs, including GPT-4o, using both logit-based probability extraction and sampling-based frequency estimation, GPT2-xl/german-GPT-2, RoBERTa Base/GottBERT, and LLaMA 2 7B Chat. GPT-4o showed the highest correspondence with human data, although alignment depended strongly on the extraction method and prompt design. Logit-based estimates minimized absolute error, whereas sampling-based estimates were better in capturing the dispersion of human variability. Together, our results establish practical guidelines for human norming and show that while LLMs can approximate human entropy, they are not interchangeable with stable human-derived distributions.

</details>


### [45] [Semantic Self-Distillation for Language Model Uncertainty](https://arxiv.org/abs/2602.04577)
*Edward Phillips,Sean Wu,Boyan Gao,David A. Clifton*

Main category: cs.CL

TL;DR: 本文提出了一种名为语义自蒸馏(SSD)的方法，通过轻量级学生模型在生成答案之前预测语义分布的不确定性，从而有效评估大语言模型的预测不确定性和幻觉风险。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在不确定性量化方面存在挑战，特别是语义分散用作不确定性代理的计算成本过高，限制了其在低延迟场景的应用。

Method: 通过训练轻量级学生模型来蒸馏采样得到的语义分布，学生模型预测答案的语义分布并利用熵作为不确定性指标，进而预测幻觉并评估答案的可靠性。

Result: 在TriviaQA数据集上，学生模型在幻觉预测上达到甚至超越了有限样本语义分散方法的效果，并且在检测领域外答案上表现优异。

Conclusion: 语义自蒸馏(SSD)为在复杂输出空间中提取预测不确定性提供了一种通用框架，具备潜在的广泛应用价值。

Abstract: Large language models present challenges for principled uncertainty quantification, in part due to their complexity and the diversity of their outputs. Semantic dispersion, or the variance in the meaning of sampled answers, has been proposed as a useful proxy for model uncertainty, but the associated computational cost prohibits its use in latency-critical applications. We show that sampled semantic distributions can be distilled into lightweight student models which estimate a prompt-conditioned uncertainty before the language model generates an answer token. The student model predicts a semantic distribution over possible answers; the entropy of this distribution provides an effective uncertainty signal for hallucination prediction, and the probability density allows candidate answers to be evaluated for reliability. On TriviaQA, our student models match or outperform finite-sample semantic dispersion for hallucination prediction and provide a strong signal for out-of-domain answer detection. We term this technique Semantic Self-Distillation (SSD), which we suggest provides a general framework for distilling predictive uncertainty in complex output spaces beyond language.

</details>


### [46] [Trust The Typical](https://arxiv.org/abs/2602.04581)
*Debargha Ganguly,Sreehari Sankar,Biyao Zhang,Vikash Singh,Kanan Gupta,Harshini Kavuru,Alan Luo,Weicong Chen,Warren Morningstar,Raghu Machiraju,Vipin Chaudhary*

Main category: cs.CL

TL;DR: 本文提出了一种基于异常检测的新型LLM安全框架T3，通过学习安全提示词的分布来识别潜在威胁，显著降低误报，并实现多语言、多领域迁移。


<details>
  <summary>Details</summary>
Motivation: 当前的LLM安全方法依赖简单的阻断已知威胁，存在脆弱性和难以扩展的问题，亟需一种更稳健的安全策略。

Method: 提出Trust The Typical (T3)框架，将安全视为一个异常检测问题，无需训练有害示例，通过学习语义空间中安全提示词的分布，识别偏离分布的异常输入。

Result: T3在18个安全相关基准测试中表现领先，误报率降低高达40倍，且单一模型可跨14种语言和多领域迁移，表现稳定。

Conclusion: T3框架通过对安全的深刻理解，实现了更稳健的LLM安全保障，且具备生产环境集成条件，支持大规模实时评估而不显著增加计算开销。

Abstract: Current approaches to LLM safety fundamentally rely on a brittle cat-and-mouse game of identifying and blocking known threats via guardrails. We argue for a fresh approach: robust safety comes not from enumerating what is harmful, but from deeply understanding what is safe. We introduce Trust The Typical (T3), a framework that operationalizes this principle by treating safety as an out-of-distribution (OOD) detection problem. T3 learns the distribution of acceptable prompts in a semantic space and flags any significant deviation as a potential threat. Unlike prior methods, it requires no training on harmful examples, yet achieves state-of-the-art performance across 18 benchmarks spanning toxicity, hate speech, jailbreaking, multilingual harms, and over-refusal, reducing false positive rates by up to 40x relative to specialized safety models. A single model trained only on safe English text transfers effectively to diverse domains and over 14 languages without retraining. Finally, we demonstrate production readiness by integrating a GPU-optimized version into vLLM, enabling continuous guardrailing during token generation with less than 6% overhead even under dense evaluation intervals on large-scale workloads.

</details>


### [47] [VILLAIN at AVerImaTeC: Verifying Image-Text Claims via Multi-Agent Collaboration](https://arxiv.org/abs/2602.04587)
*Jaeyoon Jung,Yejun Yoon,Seunghyun Yoon,Kunwoo Park*

Main category: cs.CL

TL;DR: 本文介绍了VILLAIN系统，通过多智能体协作验证图文信息，采用多阶段事实核查和多模态证据提取，实现准确评判假新闻。


<details>
  <summary>Details</summary>
Motivation: 图文虚假信息验证具有挑战性，需要结合视觉和文本信息进行全面核查，提升事实核查的准确性。

Method: VILLAIN系统基于提示的多智能体协作，利用多阶段流程：从知识库中检索图文证据；通过模态和跨模态智能体分析证据生成报告；基于报告生成问答对；最终由判决智能体综合判断核查结果。

Result: 系统在AVerImaTeC竞赛中表现优异，所有评测指标均获得第一名。

Conclusion: 结合提示多智能体协作和多模态证据分析，VILLAIN有效提升了图文事实核查的准确率，方法公开且具有应用潜力。

Abstract: This paper describes VILLAIN, a multimodal fact-checking system that verifies image-text claims through prompt-based multi-agent collaboration. For the AVerImaTeC shared task, VILLAIN employs vision-language model agents across multiple stages of fact-checking. Textual and visual evidence is retrieved from the knowledge store enriched through additional web collection. To identify key information and address inconsistencies among evidence items, modality-specific and cross-modal agents generate analysis reports. In the subsequent stage, question-answer pairs are produced based on these reports. Finally, the Verdict Prediction agent produces the verification outcome based on the image-text claim and the generated question-answer pairs. Our system ranked first on the leaderboard across all evaluation metrics. The source code is publicly available at https://github.com/ssu-humane/VILLAIN.

</details>


### [48] [Beyond Holistic Scores: Automatic Trait-Based Quality Scoring of Argumentative Essays](https://arxiv.org/abs/2602.04604)
*Lucile Favero,Juan Antonio Pérez-Ortiz,Tanja Käser,Nuria Oliver*

Main category: cs.CL

TL;DR: 本文研究了基于特征的自动议论文评分，采用两种模型：小型开源大语言模型进行情境学习和大鸟编码器模型结合CORAL序回归，提升评分的可解释性与准确性。


<details>
  <summary>Details</summary>
Motivation: 传统自动作文评分侧重整体分数，缺乏对复杂写作如议论文的分维度可解释反馈，难以满足教学需求。

Method: 利用结构化情境学习引导小型开源语言模型，以及采用大鸟模型结合CORAL风格序回归来捕捉评分的序数属性，对ASAP++数据集的五个质量特征进行评分。

Result: 显式建模评分的序数性显著提升了模型与人工评分的一致性，超越基线方法。小型开源大语言模型在推理类特征上表现较好且无需特定微调。

Conclusion: 针对教育环境设计的自动评分系统应结合评分标准的语义，采用适合的建模策略，以提供可解释、与评分标准一致的反馈，促进议论文写作教学。

Abstract: Automated Essay Scoring systems have traditionally focused on holistic scores, limiting their pedagogical usefulness, especially in the case of complex essay genres such as argumentative writing. In educational contexts, teachers and learners require interpretable, trait-level feedback that aligns with instructional goals and established rubrics. In this paper, we study trait-based Automatic Argumentative Essay Scoring using two complementary modeling paradigms designed for realistic educational deployment: (1) structured in-context learning with small open-source LLMs, and (2) a supervised, encoder-based BigBird model with a CORAL-style ordinal regression formulation, optimized for long-sequence understanding. We conduct a systematic evaluation on the ASAP++ dataset, which includes essay scores across five quality traits, offering strong coverage of core argumentation dimensions. LLMs are prompted with designed, rubric-aligned in-context examples, along with feedback and confidence requests, while we explicitly model ordinality in scores with the BigBird model via the rank-consistent CORAL framework. Our results show that explicitly modeling score ordinality substantially improves agreement with human raters across all traits, outperforming LLMs and nominal classification and regression-based baselines. This finding reinforces the importance of aligning model objectives with rubric semantics for educational assessment. At the same time, small open-source LLMs achieve a competitive performance without task-specific fine-tuning, particularly for reasoning-oriented traits, while enabling transparent, privacy-preserving, and locally deployable assessment scenarios. Our findings provide methodological, modeling, and practical insights for the design of AI-based educational systems that aim to deliver interpretable, rubric-aligned feedback for argumentative writing.

</details>


### [49] [RexBERT: Context Specialized Bidirectional Encoders for E-commerce](https://arxiv.org/abs/2602.04605)
*Rahul Bajaj,Anuj Garg*

Main category: cs.CL

TL;DR: RexBERT是专门为电商语义设计的BERT编码器系列，利用大规模电商语料和分阶段预训练策略，在多项电商任务中超越了通用大型编码器。


<details>
  <summary>Details</summary>
Motivation: 通用编码器在电商领域覆盖有限，且往往参数多、效率低，需设计专门针对电商语义的高效模型。

Method: 构建了3500亿词电商语料Ecom-niverse，提出了包含通用预训练、上下文扩展和域专门化退火的三阶段预训练方案，训练不同规模的RexBERT模型。

Result: 尽管参数量只有通用大型编码器的1/2至1/3，RexBERT在电商领域的词元分类、语义相似度和自然语言理解任务表现优异，超过或匹配现有长文本模型。

Conclusion: 结合高质量电商领域数据和系统化训练策略，比单纯扩大模型规模更能提升电商语义应用效果。

Abstract: Encoder-only transformers remain indispensable in retrieval, classification, and ranking systems where latency, stability, and cost are paramount. Most general purpose encoders, however, are trained on generic corpora with limited coverage of specialized domains. We introduce RexBERT, a family of BERT-style encoders designed specifically for e-commerce semantics. We make three contributions. First, we release Ecom-niverse, a 350 billion token corpus curated from diverse retail and shopping sources. We describe a modular pipeline that isolates and extracts e-commerce content from FineFineWeb and other open web resources, and characterize the resulting domain distribution. Second, we present a reproducible pretraining recipe building on ModernBERT's architectural advances. The recipe consists of three phases: general pre-training, context extension, and annealed domain specialization. Third, we train RexBERT models ranging from 17M to 400M parameters and evaluate them on token classification, semantic similarity, and general natural language understanding tasks using e-commerce datasets. Despite having 2-3x fewer parameters, RexBERT outperforms larger general-purpose encoders and matches or surpasses modern long-context models on domain-specific benchmarks. Our results demonstrate that high quality in-domain data combined with a principled training approach provides a stronger foundation for e-commerce applications than indiscriminate scaling alone.

</details>


### [50] [Focus-LIME: Surgical Interpretation of Long-Context Large Language Models via Proxy-Based Neighborhood Selection](https://arxiv.org/abs/2602.04607)
*Junhao Liu,Haonan Yu,Zhenyu Yan,Xin Zhang*

Main category: cs.CL

TL;DR: 提出了Focus-LIME框架，实现大规模上下文窗口中细粒度的模型特征解释，提升了解释的可信度。


<details>
  <summary>Details</summary>
Motivation: 现有模型无关解释方法在特征维度过高时归因稀释，导致无法提供准确的解释，尤其在法律审计和代码调试等高风险任务中影响严重。

Method: 提出Focus-LIME方法，基于代理模型优化扰动邻域，允许目标模型在优化上下文内进行细粒度归因，从而实现从粗到细的解释过程。

Result: 在长上下文基准测试中，Focus-LIME实现了实用的细粒度解释，显示出较高的解释可信度。

Conclusion: Focus-LIME有效解决了高维特征归因稀释问题，使大规模上下文中的外科级特征解释成为可能，为高风险任务提供了有价值的模型透明度。

Abstract: As Large Language Models (LLMs) scale to handle massive context windows, achieving surgical feature-level interpretation is essential for high-stakes tasks like legal auditing and code debugging. However, existing local model-agnostic explanation methods face a critical dilemma in these scenarios: feature-based methods suffer from attribution dilution due to high feature dimensionality, thus failing to provide faithful explanations. In this paper, we propose Focus-LIME, a coarse-to-fine framework designed to restore the tractability of surgical interpretation. Focus-LIME utilizes a proxy model to curate the perturbation neighborhood, allowing the target model to perform fine-grained attribution exclusively within the optimized context. Empirical evaluations on long-context benchmarks demonstrate that our method makes surgical explanations practicable and provides faithful explanations to users.

</details>


### [51] [Disentangling meaning from language in LLM-based machine translation](https://arxiv.org/abs/2602.04613)
*Théo Lasnier,Armel Zebaze,Djamé Seddah,Rachel Bawden,Benoît Sagot*

Main category: cs.CL

TL;DR: 本文从机制解释的视角，通过分析注意力头揭示大型语言模型在机器翻译中如何内部编码和分配翻译功能，发现不同子任务由特定稀疏的注意力头专门负责，并利用这一发现实现了无需指令提示的高效翻译。


<details>
  <summary>Details</summary>
Motivation: 现有机制解释工作受限于大型语言模型的规模，机器翻译解释多局限于词级分析，缺乏对句子级翻译机制的理解。

Method: 将机器翻译任务分解为目标语言产生和保持句子含义两个子任务，通过分析注意力头的分布，构建针对子任务的引导向量，调整部分注意力头以控制翻译过程。

Result: 发现不同稀疏的注意力头专门负责不同子任务，调整1%的相关注意力头即可达到与基于指令提示相当的无指令翻译效果，而消除这些注意力头会选择性破坏对应的翻译功能。

Conclusion: 通过机械解释方法识别和调控负责不同翻译子任务的注意力头，实现了高效且具有选择性的机器翻译，推动了对大型语言模型内部翻译机制的理解和应用。

Abstract: Mechanistic Interpretability (MI) seeks to explain how neural networks implement their capabilities, but the scale of Large Language Models (LLMs) has limited prior MI work in Machine Translation (MT) to word-level analyses. We study sentence-level MT from a mechanistic perspective by analyzing attention heads to understand how LLMs internally encode and distribute translation functions. We decompose MT into two subtasks: producing text in the target language (i.e. target language identification) and preserving the input sentence's meaning (i.e. sentence equivalence). Across three families of open-source models and 20 translation directions, we find that distinct, sparse sets of attention heads specialize in each subtask. Based on this insight, we construct subtask-specific steering vectors and show that modifying just 1% of the relevant heads enables instruction-free MT performance comparable to instruction-based prompting, while ablating these heads selectively disrupts their corresponding translation functions.

</details>


### [52] [LEAD: Layer-wise Expert-aligned Decoding for Faithful Radiology Report Generation](https://arxiv.org/abs/2602.04617)
*Ruixiao Yang,Yuanhe Tian,Xu Yang,Huiqi Li,Yan Song*

Main category: cs.CL

TL;DR: 本文提出了一种名为LEAD的逐层专家对齐解码方法，用于提高放射科报告生成的准确性和一致性，减少模型幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有大规模视觉语言模型在放射科报告生成中虽然提升了流畅度和准确性，但存在生成与图像不符的幻觉问题。且目前方法过度依赖外部知识指导，忽视了模型自身的解码先验和对齐偏差，缺乏鲁棒性。

Method: 提出LEAD方法，通过设计多专家模块提取不同病理特征，并通过门控机制将这些特征融入每个解码层，允许模型在每个推理步骤动态调整解码轨迹，实现对生成内容的事实一致性约束。

Result: 在多个公开数据集上的实验表明，LEAD方法显著提升了临床准确性指标，降低了幻觉现象，同时保持了良好的生成质量。

Conclusion: LEAD通过逐层整合病理专家特征，有效纠正了大型视觉语言模型的解码偏差，提高了放射科报告的真实一致性和生成质量，具有较强的应用价值。

Abstract: Radiology Report Generation (RRG) aims to produce accurate and coherent diagnostics from medical images. Although large vision language models (LVLM) improve report fluency and accuracy, they exhibit hallucinations, generating plausible yet image-ungrounded pathological details. Existing methods primarily rely on external knowledge guidance to facilitate the alignment between generated text and visual information. However, these approaches often ignore the inherent decoding priors and vision-language alignment biases in pretrained models and lack robustness due to reliance on constructed guidance. In this paper, we propose Layer-wise Expert-aligned Decoding (LEAD), a novel method to inherently modify the LVLM decoding trajectory. A multiple experts module is designed for extracting distinct pathological features which are integrated into each decoder layer via a gating mechanism. This layer-wise architecture enables the LLM to consult expert features at every inference step via a learned gating function, thereby dynamically rectifying decoding biases and steering the generation toward factual consistency. Experiments conducted on multiple public datasets demonstrate that the LEAD method yields effective improvements in clinical accuracy metrics and mitigates hallucinations while preserving high generation quality.

</details>


### [53] [Mapping the Web of Science, a large-scale graph and text-based dataset with LLM embeddings](https://arxiv.org/abs/2602.04630)
*Tim Kunt,Annika Buchholz,Imene Khebouri,Thorsten Koch,Ida Litzel,Thi Huong Vu*

Main category: cs.CL

TL;DR: 该论文提出了一种基于大规模语言模型嵌入的新方法，分析了包含约5600万科学出版物的Web of Science数据集，揭示了文本自身语义信息构成的自我结构化图谱。


<details>
  <summary>Details</summary>
Motivation: 传统文本数据分析多依赖文本间的链接关系（如引用），但文本自身的语义信息未能充分利用。随着大语言模型嵌入技术的发展，文本内容本身的潜力被进一步激发。

Method: 提出了一种基于大语言模型嵌入的新方法，将文本语义信息编码成向量表示，结合传统的图结构分析，用于揭示文本数据的内在结构。

Result: 在Web of Science数据集上的应用表明，该方法能够有效捕捉文本之间复杂的语义关系，展示出文本的自我组织结构，增强了文本分类与预测任务的表现。

Conclusion: 利用大语言模型嵌入技术可以挖掘大规模文本数据中隐含的语义结构，补充传统基于图的分析手段，拓展了文本数据分析的视角与方法。

Abstract: Large text data sets, such as publications, websites, and other text-based media, inherit two distinct types of features: (1) the text itself, its information conveyed through semantics, and (2) its relationship to other texts through links, references, or shared attributes. While the latter can be described as a graph structure and can be handled by a range of established algorithms for classification and prediction, the former has recently gained new potential through the use of LLM embedding models. Demonstrating these possibilities and their practicability, we investigate the Web of Science dataset, containing ~56 million scientific publications through the lens of our proposed embedding method, revealing a self-structured landscape of texts.

</details>


### [54] [Outcome Accuracy is Not Enough: Aligning the Reasoning Process of Reward Models](https://arxiv.org/abs/2602.04649)
*Binghai Wang,Yantao Liu,Yuxuan Liu,Tianyi Tang,Shenzhi Wang,Chang Gao,Chujie Zheng,Yichang Zhang,Le Yu,Shixuan Liu,Tao Gui,Qi Zhang,Xuanjing Huang,Bowen Yu,Fei Huang,Junyang Lin*

Main category: cs.CL

TL;DR: 本文提出了理据一致性指标来量化模型推理与人类判断的对齐，解决了生成式奖励模型和判决模型的欺骗性对齐问题，通过结合理据一致性与结果准确率的混合信号进行训练，显著提升了模型性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的生成式奖励模型和LLM判决模型存在欺骗性对齐问题，即它们虽然输出正确判断，但依据错误理由，从而影响在RLHF中的泛化能力。

Method: 引入理据一致性这一细粒度指标来量化模型推理过程与人类判断的一致性，并提出结合理据一致性和结果准确率的混合信号用于生成式奖励模型的训练。

Result: 在RM-Bench和JudgeBench测试中，使用混合信号训练的模型性能分别达到87.1%和82%，较单纯基于结果的训练提高约5%。在Arena Hard v2测试中，创意写作任务性能提升了7%。

Conclusion: 结合理据一致性和结果准确率的混合信号训练方法有效避免了欺骗性对齐，提高了模型推理的准确性和泛化能力，表现优于仅依赖结果准确率的训练方法。

Abstract: Generative Reward Models (GenRMs) and LLM-as-a-Judge exhibit deceptive alignment by producing correct judgments for incorrect reasons, as they are trained and evaluated to prioritize Outcome Accuracy, which undermines their ability to generalize during RLHF. We introduce Rationale Consistency, a fine-grained metric that quantifies the alignment between the model's reasoning process and human judgment. Our evaluation of frontier models reveals that rationale consistency effectively discriminates among state-of-the-art models and detects deceptive alignment, while outcome accuracy falls short in both respects. To mitigate this gap, we introduce a hybrid signal that combines rationale consistency with outcome accuracy for GenRM training. Our training method achieves state-of-the-art performance on RM-Bench (87.1%) and JudgeBench (82%), surpassing outcome-only baselines by an average of 5%. Using RM during RLHF, our method effectively improves performance as demonstrated on Arena Hard v2, notably yielding a 7% improvement in creative writing tasks. Further analysis confirms that our method escapes the deceptive alignment trap, effectively reversing the decline in rationale consistency observed in outcome-only training.

</details>


### [55] [Approaches to Semantic Textual Similarity in Slovak Language: From Algorithms to Transformers](https://arxiv.org/abs/2602.04659)
*Lukas Radosky,Miroslav Blstak,Matej Krajcovic,Ivan Polasek*

Main category: cs.CL

TL;DR: 本文针对斯洛伐克语的句子级语义文本相似度（STS）方法进行了比较评估，涵盖传统算法、监督机器学习模型和深度学习工具，采用人工蜂群算法优化特征选择和超参数。


<details>
  <summary>Details</summary>
Motivation: 斯洛伐克语作为资源匮乏语言，语义文本相似度任务面临挑战，亟需有效的方法进行评估和改进。

Method: 本文比较评估了传统算法、基于传统算法输出训练的监督机器学习模型（结合人工蜂群算法优化特征和超参数），以及多种第三方深度学习工具（CloudNLP微调模型、OpenAI嵌入模型、GPT-4、预训练的SlovakBERT模型）。

Result: 实验结果表明不同方法在性能和资源需求上存在权衡，展示了各自优缺点。

Conclusion: 本文揭示了多种STS方法在处理斯洛伐克语任务中的优势与局限，为资源匮乏语言的STS研究提供了有价值的参考。

Abstract: Semantic textual similarity (STS) plays a crucial role in many natural language processing tasks. While extensively studied in high-resource languages, STS remains challenging for under-resourced languages such as Slovak. This paper presents a comparative evaluation of sentence-level STS methods applied to Slovak, including traditional algorithms, supervised machine learning models, and third-party deep learning tools. We trained several machine learning models using outputs from traditional algorithms as features, with feature selection and hyperparameter tuning jointly guided by artificial bee colony optimization. Finally, we evaluated several third-party tools, including fine-tuned model by CloudNLP, OpenAI's embedding models, GPT-4 model, and pretrained SlovakBERT model. Our findings highlight the trade-offs between different approaches.

</details>


### [56] [Investigating Disability Representations in Text-to-Image Models](https://arxiv.org/abs/2602.04687)
*Yang Yian,Yu Fan,Liudmila Zavolokina,Sarah Ebling*

Main category: cs.CL

TL;DR: 本文分析了文本生成图像模型在残疾人形象表现方面的不足，强调了对残疾人包容性描绘的必要性。


<details>
  <summary>Details</summary>
Motivation: 尽管性别和种族的表现受到了关注，残疾人在AI生成图像中的表现仍未得到充分研究。

Method: 通过设计结构化提示，分析Stable Diffusion XL和DALL-E 3生成的残疾人图像，比较一般与特定残疾类别提示生成图像的相似性，并通过情感极性分析结合自动和人工评估，研究缓解策略对残疾人表现的影响。

Result: 研究发现残疾人形象表现依然存在不平衡问题，缓解策略对形象表现有一定影响。

Conclusion: 生成模型需要持续评估和改进，以促进对残疾人的多样化和包容性描绘。

Abstract: Text-to-image generative models have made remarkable progress in producing high-quality visual content from textual descriptions, yet concerns remain about how they represent social groups. While characteristics like gender and race have received increasing attention, disability representations remain underexplored. This study investigates how people with disabilities are represented in AI-generated images by analyzing outputs from Stable Diffusion XL and DALL-E 3 using a structured prompt design. We analyze disability representations by comparing image similarities between generic disability prompts and prompts referring to specific disability categories. Moreover, we evaluate how mitigation strategies influence disability portrayals, with a focus on assessing affective framing through sentiment polarity analysis, combining both automatic and human evaluation. Our findings reveal persistent representational imbalances and highlight the need for continuous evaluation and refinement of generative models to foster more diverse and inclusive portrayals of disability.

</details>


### [57] [LinGO: A Linguistic Graph Optimization Framework with LLMs for Interpreting Intents of Online Uncivil Discourse](https://arxiv.org/abs/2602.04693)
*Yuan Zhang,Thales Bertaglia*

Main category: cs.CL

TL;DR: 本文提出LinGO框架，通过分解语言结构并优化特定步骤，提高大型语言模型对政治不文明行为多类别意图的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 现有分类器容易误判包含不文明暗示但表达文明意图的帖子，导致对网络有害不文明现象的过高估计。

Method: LinGO利用语言结构和优化技术，将语言拆解为多步骤成分，识别导致错误的关键步骤，并针对性地迭代优化提示或示例；采用四种优化技术结合三种成本效益高的大型语言模型进行评测。

Result: LinGO在所有模型与优化方法中均显著提高分类准确率和加权F1分数，特别是结合RAG优化和Gemini模型达到最佳性能。

Conclusion: 将多步骤语言成分融入大型语言模型指令并针对性优化，有助于模型更准确理解复杂语义，具备推广到其他复杂语义解释任务的潜力。

Abstract: Detecting uncivil language is crucial for maintaining safe, inclusive, and democratic online spaces. Yet existing classifiers often misinterpret posts containing uncivil cues but expressing civil intents, leading to inflated estimates of harmful incivility online. We introduce LinGO, a linguistic graph optimization framework for large language models (LLMs) that leverages linguistic structures and optimization techniques to classify multi-class intents of incivility that use various direct and indirect expressions. LinGO decomposes language into multi-step linguistic components, identifies targeted steps that cause the most errors, and iteratively optimizes prompt and/or example components for targeted steps. We evaluate it using a dataset collected during the 2022 Brazilian presidential election, encompassing four forms of political incivility: Impoliteness (IMP), Hate Speech and Stereotyping (HSST), Physical Harm and Violent Political Rhetoric (PHAVPR), and Threats to Democratic Institutions and Values (THREAT). Each instance is annotated with six types of civil/uncivil intent. We benchmark LinGO using three cost-efficient LLMs: GPT-5-mini, Gemini 2.5 Flash-Lite, and Claude 3 Haiku, and four optimization techniques: TextGrad, AdalFlow, DSPy, and Retrieval-Augmented Generation (RAG). The results show that, across all models, LinGO consistently improves accuracy and weighted F1 compared with zero-shot, chain-of-thought, direct optimization, and fine-tuning baselines. RAG is the strongest optimization technique and, when paired with Gemini model, achieves the best overall performance. These findings demonstrate that incorporating multi-step linguistic components into LLM instructions and optimize targeted components can help the models explain complex semantic meanings, which can be extended to other complex semantic explanation tasks in the future.

</details>


### [58] [ERNIE 5.0 Technical Report](https://arxiv.org/abs/2602.04705)
*Haifeng Wang,Hua Wu,Tian Wu,Yu Sun,Jing Liu,Dianhai Yu,Yanjun Ma,Jingzhou He,Zhongjun He,Dou Hong,Qiwen Liu,Shuohuan Wang,Junyuan Shang,Zhenyu Zhang,Yuchen Ding,Jinle Zeng,Jiabin Yang,Liang Shen,Ruibiao Chen,Weichong Yin,Siyu Ding,Dai Dai,Shikun Feng,Siqi Bao,Bolei He,Yan Chen,Zhenyu Jiao,Ruiqing Zhang,Zeyu Chen,Qingqing Dang,Kaipeng Deng,Jiajun Jiang,Enlei Gong,Guoxia Wang,Yanlin Sha,Yi Liu,Yehan Zheng,Weijian Xu,Jiaxiang Liu,Zengfeng Zeng,Yingqi Qu,Zhongli Li,Zhengkun Zhang,Xiyang Wang,Zixiang Xu,Xinchao Xu,Zhengjie Huang,Dong Wang,Bingjin Chen,Yue Chang,Xing Yuan,Shiwei Huang,Qiao Zhao,Xinzhe Ding,Shuangshuang Qiao,Baoshan Yang,Bihong Tang,Bin Li,Bingquan Wang,Binhan Tang,Binxiong Zheng,Bo Cui,Bo Ke,Bo Zhang,Bowen Zhang,Boyan Zhang,Boyang Liu,Caiji Zhang,Can Li,Chang Xu,Chao Pang,Chao Zhang,Chaoyi Yuan,Chen Chen,Cheng Cui,Chenlin Yin,Chun Gan,Chunguang Chai,Chuyu Fang,Cuiyun Han,Dan Zhang,Danlei Feng,Danxiang Zhu,Dong Sun,Dongbo Li,Dongdong Li,Dongdong Liu,Dongxue Liu,Fan Ding,Fan Hu,Fan Li,Fan Mo,Feisheng Wu,Fengwei Liu,Gangqiang Hu,Gaofeng Lu,Gaopeng Yong,Gexiao Tian,Guan Wang,Guangchen Ni,Guangshuo Wu,Guanzhong Wang,Guihua Liu,Guishun Li,Haibin Li,Haijian Liang,Haipeng Ming,Haisu Wang,Haiyang Lu,Haiye Lin,Han Zhou,Hangting Lou,Hanwen Du,Hanzhi Zhang,Hao Chen,Hao Du,Hao Liu,Hao Zhou,Haochen Jiang,Haodong Tian,Haoshuang Wang,Haozhe Geng,Heju Yin,Hong Chen,Hongchen Xue,Hongen Liu,Honggeng Zhang,Hongji Xu,Hongwei Chen,Hongyang Zhang,Hongyuan Zhang,Hua Lu,Huan Chen,Huan Wang,Huang He,Hui Liu,Hui Zhong,Huibin Ruan,Jiafeng Lu,Jiage Liang,Jiahao Hu,Jiahao Hu,Jiajie Yang,Jialin Li,Jian Chen,Jian Wu,Jianfeng Yang,Jianguang Jiang,Jianhua Wang,Jianye Chen,Jiaodi Liu,Jiarui Zhou,Jiawei Lv,Jiaxin Zhou,Jiaxuan Liu,Jie Han,Jie Sun,Jiefan Fang,Jihan Liu,Jihua Liu,Jing Hu,Jing Qian,Jing Yan,Jingdong Du,Jingdong Wang,Jingjing Wu,Jingyong Li,Jinheng Wang,Jinjin Li,Jinliang Lu,Jinlin Yu,Jinnan Liu,Jixiang Feng,Jiyi Huang,Jiyuan Zhang,Jun Liang,Jun Xia,Jun Yu,Junda Chen,Junhao Feng,Junhong Xiang,Junliang Li,Kai Liu,Kailun Chen,Kairan Su,Kang Hu,Kangkang Zhou,Ke Chen,Ke Wei,Kui Huang,Kun Wu,Kunbin Chen,Lei Han,Lei Sun,Lei Wen,Linghui Meng,Linhao Yu,Liping Ouyang,Liwen Zhang,Longbin Ji,Longzhi Wang,Meng Sun,Meng Tian,Mengfei Li,Mengqi Zeng,Mengyu Zhang,Ming Hong,Mingcheng Zhou,Mingming Huang,Mingxin Chen,Mingzhu Cai,Naibin Gu,Nemin Qiu,Nian Wang,Peng Qiu,Peng Zhao,Pengyu Zou,Qi Wang,Qi Xin,Qian Wang,Qiang Zhu,Qianhui Luo,Qianwei Yang,Qianyue He,Qifei Wu,Qinrui Li,Qiwen Bao,Quan Zhang,Quanxiang Liu,Qunyi Xie,Rongrui Zhan,Rufeng Dai,Rui Peng,Ruian Liu,Ruihao Xu,Ruijie Wang,Ruixi Zhang,Ruixuan Liu,Runsheng Shi,Ruting Wang,Senbo Kang,Shan Lu,Shaofei Yu,Shaotian Gong,Shenwei Hu,Shifeng Zheng,Shihao Guo,Shilong Fan,Shiqin Liu,Shiwei Gu,Shixi Zhang,Shuai Yao,Shuang Zhang,Shuangqiao Liu,Shuhao Liang,Shuwei He,Shuwen Yang,Sijun He,Siming Dai,Siming Wu,Siyi Long,Songhe Deng,Suhui Dong,Suyin Liang,Teng Hu,Tianchan Xu,Tianliang Lv,Tianmeng Yang,Tianyi Wei,Tiezhu Gao,Ting Sun,Ting Zhang,Tingdan Luo,Wei He,Wei Luan,Wei Yin,Wei Zhang,Wei Zhou,Weibao Gong,Weibin Li,Weicheng Huang,Weichong Dang,Weiguo Zhu,Weilong Zhang,Weiqi Tan,Wen Huang,Wenbin Chang,Wenjing Du,Wenlong Miao,Wenpei Luo,Wenquan Wu,Xi Shi,Xi Zhao,Xiang Gao,Xiangguo Zhang,Xiangrui Yu,Xiangsen Wang,Xiangzhe Wang,Xianlong Luo,Xianying Ma,Xiao Tan,Xiaocong Lin,Xiaofei Wang,Xiaofeng Peng,Xiaofeng Wu,Xiaojian Xu,Xiaolan Yuan,Xiaopeng Cui,Xiaotian Han,Xiaoxiong Liu,Xiaoxu Fei,Xiaoxuan Wu,Xiaoyu Wang,Xiaoyu Zhang,Xin Sun,Xin Wang,Xinhui Huang,Xinming Zhu,Xintong Yu,Xinyi Xu,Xinyu Wang,Xiuxian Li,XuanShi Zhu,Xue Xu,Xueying Lv,Xuhong Li,Xulong Wei,Xuyi Chen,Yabing Shi,Yafeng Wang,Yamei Li,Yan Liu,Yanfu Cheng,Yang Gao,Yang Liang,Yang Wang,Yang Wang,Yang Yang,Yanlong Liu,Yannian Fu,Yanpeng Wang,Yanzheng Lin,Yao Chen,Yaozong Shen,Yaqian Han,Yehua Yang,Yekun Chai,Yesong Wang,Yi Song,Yichen Zhang,Yifei Wang,Yifeng Guo,Yifeng Kou,Yilong Chen,Yilong Guo,Yiming Wang,Ying Chen,Ying Wang,Yingsheng Wu,Yingzhan Lin,Yinqi Yang,Yiran Xing,Yishu Lei,Yixiang Tu,Yiyan Chen,Yong Zhang,Yonghua Li,Yongqiang Ma,Yongxing Dai,Yongyue Zhang,Yu Ran,Yu Sun,Yu-Wen Michael Zhang,Yuang Liu,Yuanle Liu,Yuanyuan Zhou,Yubo Zhang,Yuchen Han,Yucheng Wang,Yude Gao,Yuedong Luo,Yuehu Dong,Yufeng Hu,Yuhui Cao,Yuhui Yun,Yukun Chen,Yukun Gao,Yukun Li,Yumeng Zhang,Yun Fan,Yun Ma,Yunfei Zhang,Yunshen Xie,Yuping Xu,Yuqin Zhang,Yuqing Liu,Yurui Li,Yuwen Wang,Yuxiang Lu,Zefeng Cai,Zelin Zhao,Zelun Zhang,Zenan Lin,Zezhao Dong,Zhaowu Pan,Zhaoyu Liu,Zhe Dong,Zhe Zhang,Zhen Zhang,Zhengfan Wu,Zhengrui Wei,Zhengsheng Ning,Zhenxing Li,Zhenyu Li,Zhenyu Qian,Zhenyun Li,Zhi Li,Zhichao Chen,Zhicheng Dong,Zhida Feng,Zhifan Feng,Zhihao Deng,Zhijin Yu,Zhiyang Chen,Zhonghui Zheng,Zhuangzhuang Guo,Zhujun Zhang,Zhuo Sun,Zichang Liu,Zihan Lin,Zihao Huang,Zihe Zhu,Ziheng Zhao,Ziping Chen,Zixuan Zhu,Ziyang Xu,Ziyi Liang,Ziyuan Gao*

Main category: cs.CL

TL;DR: ERNIE 5.0是一款统一的多模态自回归基础模型，支持文本、图像、视频和音频的理解与生成。


<details>
  <summary>Details</summary>
Motivation: 为了实现一个支持多种模态的统一模型，解决多模态训练和大规模部署中的性能与资源约束问题。

Method: 采用统一的下一组token预测目标，基于超稀疏混合专家（MoE）架构，并引入弹性训练范式，训练出不同深度与容量的子模型，同时应用强化学习保证后训练效率和稳定性。

Result: 实验表明ERNIE 5.0在多模态任务中表现均衡且强大，是首个支持多模态理解与生成的万亿参数级别统一模型。

Conclusion: 通过弹性训练和专家路由机制，ERNIE 5.0实现了灵活性能与资源折中，推动了大规模多模态统一模型的实用化与研究。

Abstract: In this report, we introduce ERNIE 5.0, a natively autoregressive foundation model desinged for unified multimodal understanding and generation across text, image, video, and audio. All modalities are trained from scratch under a unified next-group-of-tokens prediction objective, based on an ultra-sparse mixture-of-experts (MoE) architecture with modality-agnostic expert routing. To address practical challenges in large-scale deployment under diverse resource constraints, ERNIE 5.0 adopts a novel elastic training paradigm. Within a single pre-training run, the model learns a family of sub-models with varying depths, expert capacities, and routing sparsity, enabling flexible trade-offs among performance, model size, and inference latency in memory- or time-constrained scenarios. Moreover, we systematically address the challenges of scaling reinforcement learning to unified foundation models, thereby guaranteeing efficient and stable post-training under ultra-sparse MoE architectures and diverse multimodal settings. Extensive experiments demonstrate that ERNIE 5.0 achieves strong and balanced performance across multiple modalities. To the best of our knowledge, among publicly disclosed models, ERNIE 5.0 represents the first production-scale realization of a trillion-parameter unified autoregressive model that supports both multimodal understanding and generation. To facilitate further research, we present detailed visualizations of modality-agnostic expert routing in the unified model, alongside comprehensive empirical analysis of elastic training, aiming to offer profound insights to the community.

</details>


### [59] [LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers](https://arxiv.org/abs/2602.04706)
*Yike Sun,Haotong Yang,Zhouchen Lin,Muhan Zhang*

Main category: cs.CL

TL;DR: 本文研究了BPE分词器中使用频率低、但占用词汇容量的残留词，提出LiteToken方法来移除它们，减少模型参数并提升对噪声和拼写错误的鲁棒性，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: BPE分词使用中存在中间合并残留词，这些词频繁出现于合并学习阶段，但实际使用时很少出现，浪费词汇容量并降低模型对异常输入的鲁棒性。

Method: 系统地分析了常用分词器中的残留词现象，提出LiteToken方法，通过移除这些低频残留词来简化词汇表，无需对预训练模型进行额外微调。

Result: 实验表明LiteToken减少了词语碎片化，降低了模型参数数量，同时提升了模型对噪声和拼写错误输入的鲁棒性，在不损失整体性能的情况下简化了分词器。

Conclusion: 通过移除BPE分词器中的中间合并残留词，可以提升语言模型的效率和鲁棒性，LiteToken是一种简单有效的实现方法。

Abstract: Tokenization is fundamental to how language models represent and process text, yet the behavior of widely used BPE tokenizers has received far less study than model architectures and training. In this paper, we investigate intermediate merge residues in BPE vocabularies: tokens that are frequent during merge learning so that retained in the final vocabulary, but are mostly further merged and rarely emitted when tokenizing the corpus during tokenizer usage. Such low-frequency tokens not only waste vocabulary capacity but also increase vulnerability to adversarial or atypical inputs. We present a systematic empirical characterization of this phenomenon across commonly used tokenizers and introduce LiteToken, a simple method for removing residue tokens. Because the affected tokens are rarely used, pretrained models can often accommodate the modified tokenizer without additional fine-tuning. Experiments show that LiteToken reduces token fragmentation, reduces parameters, and improves robustness to noisy or misspelled inputs, while preserving overall performance.

</details>


### [60] [Linguistically Informed Evaluation of Multilingual ASR for African Languages](https://arxiv.org/abs/2602.04716)
*Fei-Yueh Chen,Lateef Adeleke,C. M. Downey*

Main category: cs.CL

TL;DR: 论文探讨了传统词错误率（WER）难以准确衡量非洲语言自动语音识别（ASR）性能的问题，提出结合特征错误率（FER）、音位错误率和音调错误率（TER）等指标，更好地揭示模型在语言学层面的错误。


<details>
  <summary>Details</summary>
Motivation: WER将语音识别中的音位、音调等多种语言错误简单合并为单一词汇错误，无法准确反映非洲语言的ASR模型性能。论文旨在通过更细粒度的错误指标，揭示模型在语言特征上的表现。

Method: 在两个非洲语言中评价三种语音编码器，结合WER、字符错误率（CER）、FER和音调扩展的TER进行误差分析，重点关注音位特征和音调的识别准确性。

Result: FER和TER能揭示即使词级准确率低时的语言学重要错误模式。模型对音段特征表现较好，但对音调（特别是中音和降调）难以识别。以约鲁巴语为例，WER=0.788，CER=0.305，FER=0.151，显示了指标的显著差异。

Conclusion: WER等“全有或全无”指标掩盖了模型的细微音位特征错误，FER和TER等更细粒度指标有助于准确反映模型性能，特别适用于处理非洲语言中复杂的音位和音调问题。

Abstract: Word Error Rate (WER) mischaracterizes ASR models' performance for African languages by combining phonological, tone, and other linguistic errors into a single lexical error. By contrast, Feature Error Rate (FER) has recently attracted attention as a viable metric that reveals linguistically meaningful errors in models' performance. In this paper, we evaluate three speech encoders on two African languages by complementing WER with CER, and FER, and add a tone-aware extension (TER). We show that by computing errors on phonological features, FER and TER reveal linguistically-salient error patterns even when word-level accuracy remains low. Our results reveal that models perform better on segmental features, while tones (especially mid and downstep) remain the most challenging features. Results on Yoruba show a striking differential in metrics, with WER=0.788, CER=0.305, and FER=0.151. Similarly for Uneme (an endangered language absent from pretraining data) a model with near-total WER and 0.461 CER achieves the relatively low FER of 0.267. This indicates model error is often attributable to individual phonetic feature errors, which is obscured by all-or-nothing metrics like WER.

</details>


### [61] ["Be My Cheese?": Cultural Nuance Benchmarking for Machine Translation in Multilingual LLMs](https://arxiv.org/abs/2602.04729)
*Madison Van Doren,Casey Ford,Jennifer Barajas,Cory Holland*

Main category: cs.CL

TL;DR: 本文构建了一个大规模人类评估基准，用于衡量多语言大模型机器翻译中的文化本地化能力，发现当前翻译模型在文化语义上的表现存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 现有机器翻译评测主要关注词汇和语法准确性，忽视了实际本地化所需的语用和文化能力，迫切需要一个专注文化细微差异的评估基准。

Method: 基于先导研究，使用5名母语评审对7个多语言大模型在15种语言上的翻译进行评分，涵盖整篇文本和文化细节（成语、双关语、节日、文化概念）分段评估，评分采用0-3等级。

Result: 整体翻译质量中等，GPT-5表现最好（2.10/3），节日和文化概念的翻译优于成语和双关语，后者常被漏译。显示语法正确与文化共鸣之间存在明显差距。

Conclusion: 首次提出专注于文化细微差异的多语种人工评测基准，强调需要加入文化信息的训练数据和改进跨语言语用能力，以提升机器翻译的实际交流效果。

Abstract: We present a large-scale human evaluation benchmark for assessing cultural localisation in machine translation produced by state-of-the-art multilingual large language models (LLMs). Existing MT benchmarks emphasise token-level and grammatical accuracy, but of ten overlook pragmatic and culturally grounded competencies required for real-world localisation. Building on a pilot study of 87 translations across 20 languages, we evaluate 7 multilingual LLMs across 15 target languages with 5 native-speaker raters per language. Raters scored both full-text translations and segment-level instances of culturally nuanced language (idioms, puns, holidays, and culturally embedded concepts) on an ordinal 0-3 quality scale; segment ratings additionally included an NA option for untranslated segments.
  Across full-text evaluations, mean overall quality is modest (1.68/3): GPT-5 (2.10/3), Claude Sonnet 3.7 (1.97/3), and Mistral Medium 3.1 (1.84/3) form the strongest tier with fewer catastrophic failures. Segment-level results show sharp category effects: holidays (2.20/3) and cultural concepts (2.19/3) translate substantially better than idioms (1.65/3) and puns (1.45/3), and idioms are most likely to be left untranslated. These findings demonstrate a persistent gap between grammatical adequacy and cultural resonance. To our knowledge, this is the first multilingual, human-annotated benchmark focused explicitly on cultural nuance in translation and localisation, highlighting the need for culturally informed training data, improved cross-lingual pragmatics, and evaluation paradigms that better reflect real-world communicative competence.

</details>


### [62] [Less Finetuning, Better Retrieval: Rethinking LLM Adaptation for Biomedical Retrievers via Synthetic Data and Model Merging](https://arxiv.org/abs/2602.04731)
*Sameh Khattab,Jean-Philippe Corbeil,Osman Alperen Koraş,Amin Dada,Julian Friedrich,François Beaulieu,Paul Vozila,Jens Kleesiek*

Main category: cs.CL

TL;DR: 本文提出了STM框架，通过合成难负样本、检索提示优化和模型融合，提升了基于大型语言模型的领域专用检索器在生物医学等专业领域的性能。


<details>
  <summary>Details</summary>
Motivation: 现有通用大型语言模型在特定领域（如生物医学）作为检索器时效果有限，缺乏有效适配方法。

Method: 提出模块化的STM框架，包括合成难负样本训练、检索提示优化和模型融合技术。

Result: 在MTEB基准的医用和通用12个任务上，STM方法使任务专家模型性能提升最高达23.5%，平均提升7.5%；融合模型表现优于单一专家和强基线，且无需大量预训练。

Conclusion: STM提供了一条高效可扩展的途径，将通用大型语言模型转化为在专业领域表现优异且保留通用能力的检索器。

Abstract: Retrieval-augmented generation (RAG) has become the backbone of grounding Large Language Models (LLMs), improving knowledge updates and reducing hallucinations. Recently, LLM-based retriever models have shown state-of-the-art performance for RAG applications. However, several technical aspects remain underexplored on how to adapt general-purpose LLMs into effective domain-specific retrievers, especially in specialized domains such as biomedicine. We present Synthesize-Train-Merge (STM), a modular framework that enhances decoder-only LLMs with synthetic hard negatives, retrieval prompt optimization, and model merging. Experiments on a subset of 12 medical and general tasks from the MTEB benchmark show STM boosts task-specific experts by up to 23.5\% (average 7.5\%) and produces merged models that outperform both single experts and strong baselines without extensive pretraining. Our results demonstrate a scalable, efficient path for turning general LLMs into high-performing, domain-specialized retrievers, preserving general-domain capabilities while excelling on specialized tasks.

</details>


### [63] [Alignment Drift in Multimodal LLMs: A Two-Phase, Longitudinal Evaluation of Harm Across Eight Model Releases](https://arxiv.org/abs/2602.04739)
*Casey Ford,Madison Van Doren,Emily Dix*

Main category: cs.CL

TL;DR: 本文通过两阶段评估多模态大语言模型（MLLMs）在对抗性提示下的安全性，发现不同模型家族的安全性表现差异显著且随时间变化，强调了持续多模态安全评估的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大语言模型被广泛应用，其在面对对抗性提示时的安全性尚未充分研究，需要系统化评估以保障其无害性。

Method: 设计包含726个专业红队人员编写的对抗性提示的基准测试，分两阶段评估八款主流MLLMs，累计收集82,256个人类安全评分，分析模型家族、版本及模态对安全性的影响。

Result: 不同模型家族显示显著差异：Pixtral模型最易受攻击，Claude模型因高拒绝率表现最安全；GPT和Claude模型的攻击成功率随版本升级上升，而Pixtral和Qwen略有下降；模态影响随时间变化，第二阶段各模型对不同模态的脆弱性差异明显。

Conclusion: 多模态大语言模型的无害性表现随模型更新显著变化，且不同模型间差异较大，强调了建立长期、多模态的安全评估基准以动态监控模型安全行为的必要性。

Abstract: Multimodal large language models (MLLMs) are increasingly deployed in real-world systems, yet their safety under adversarial prompting remains underexplored. We present a two-phase evaluation of MLLM harmlessness using a fixed benchmark of 726 adversarial prompts authored by 26 professional red teamers. Phase 1 assessed GPT-4o, Claude Sonnet 3.5, Pixtral 12B, and Qwen VL Plus; Phase 2 evaluated their successors (GPT-5, Claude Sonnet 4.5, Pixtral Large, and Qwen Omni) yielding 82,256 human harm ratings. Large, persistent differences emerged across model families: Pixtral models were consistently the most vulnerable, whereas Claude models appeared safest due to high refusal rates. Attack success rates (ASR) showed clear alignment drift: GPT and Claude models exhibited increased ASR across generations, while Pixtral and Qwen showed modest decreases. Modality effects also shifted over time: text-only prompts were more effective in Phase 1, whereas Phase 2 produced model-specific patterns, with GPT-5 and Claude 4.5 showing near-equivalent vulnerability across modalities. These findings demonstrate that MLLM harmlessness is neither uniform nor stable across updates, underscoring the need for longitudinal, multimodal benchmarks to track evolving safety behaviour.

</details>


### [64] [Exploiting contextual information to improve stance detection in informal political discourse with LLMs](https://arxiv.org/abs/2602.04750)
*Arman Engin Sucu,Yixiang Zhou,Mario A. Nascimento,Tony Mullen*

Main category: cs.CL

TL;DR: 该研究通过引入用户历史发布内容生成的上下文信息，提高大型语言模型在政治立场检测任务中的准确率。


<details>
  <summary>Details</summary>
Motivation: 非正式网络话语中政治立场的检测因语言讽刺、模糊和依赖上下文而具挑战性，研究旨在提升模型的准确性。

Method: 利用真实政治论坛数据，生成用户的结构化个人资料（包括意识形态倾向、话题和语言模式），并借助七种主流大型语言模型，比较基础与上下文丰富两种设置的性能。

Result: 加入用户上下文提示后，模型准确率显著提升，提升幅度为17.5%至38.5%，最高达74%，优于之前方法；同时发现策略性选取的政治相关内容优于随机大规模内容。

Conclusion: 在政治立场检测中融入用户级别的上下文信息显著增强了模型表现，强调了上下文在复杂语言任务中的重要性。

Abstract: This study investigates the use of Large Language Models (LLMs) for political stance detection in informal online discourse, where language is often sarcastic, ambiguous, and context-dependent. We explore whether providing contextual information, specifically user profile summaries derived from historical posts, can improve classification accuracy. Using a real-world political forum dataset, we generate structured profiles that summarize users' ideological leaning, recurring topics, and linguistic patterns. We evaluate seven state-of-the-art LLMs across baseline and context-enriched setups through a comprehensive cross-model evaluation. Our findings show that contextual prompts significantly boost accuracy, with improvements ranging from +17.5\% to +38.5\%, achieving up to 74\% accuracy that surpasses previous approaches. We also analyze how profile size and post selection strategies affect performance, showing that strategically chosen political content yields better results than larger, randomly selected contexts. These findings underscore the value of incorporating user-level context to enhance LLM performance in nuanced political classification tasks.

</details>


### [65] [When Silence Is Golden: Can LLMs Learn to Abstain in Temporal QA and Beyond?](https://arxiv.org/abs/2602.04755)
*Xinyu Zhou,Chang Jin,Carsten Eickhoff,Zhijiang Guo,Seyed Ali Bahrainian*

Main category: cs.CL

TL;DR: 本文提出通过强化学习结合链式思维监督，训练大语言模型具备时序问答中的放弃回答能力，有效提升推理准确率和对无答案问题的识别能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在时序问答中常忽视时间敏感信息，且很少展示不确定性，导致误导性回答，现有校准方法难以捕捉复杂推理中的不确定性，亟需训练模型具备放弃回答的能力。

Method: 将放弃回答视为可教技能，通过链式思维(CoT)监督与强化学习(RL)相结合的流水线，以放弃意识奖励引导训练，系统分析不同信息类型和训练技术对时序推理与放弃行为的影响。

Result: 采用Qwen2.5-1.5B-Instruct初始化的模型通过RL显著优于GPT-4o，时间问答准确率提升3.46%-5.80%，且对无答案问题的正确识别率较纯监督微调提高20%。

Conclusion: 监督微调会导致模型过度自信，降低可靠性；强化学习提升准确率但仍存在风险。隐式信息对放弃推理帮助有限，研究为联合优化放弃与推理能力提供了新见解，助力构建更可靠的大语言模型。

Abstract: Large language models (LLMs) rarely admit uncertainty, often producing fluent but misleading answers, rather than abstaining (i.e., refusing to answer). This weakness is even evident in temporal question answering, where models frequently ignore time-sensitive evidence and conflate facts across different time-periods. In this paper, we present the first empirical study of training LLMs with an abstention ability while reasoning about temporal QA. Existing approaches such as calibration might be unreliable in capturing uncertainty in complex reasoning. We instead frame abstention as a teachable skill and introduce a pipeline that couples Chain-of-Thought (CoT) supervision with Reinforcement Learning (RL) guided by abstention-aware rewards. Our goal is to systematically analyze how different information types and training techniques affect temporal reasoning with abstention behavior in LLMs. Through extensive experiments studying various methods, we find that RL yields strong empirical gains on reasoning: a model initialized by Qwen2.5-1.5B-Instruct surpasses GPT-4o by $3.46\%$ and $5.80\%$ in Exact Match on TimeQA-Easy and Hard, respectively. Moreover, it improves the True Positive rate on unanswerable questions by $20\%$ over a pure supervised fine-tuned (SFT) variant. Beyond performance, our analysis shows that SFT induces overconfidence and harms reliability, while RL improves prediction accuracy but exhibits similar risks. Finally, by comparing implicit reasoning cues (e.g., original context, temporal sub-context, knowledge graphs) with explicit CoT supervision, we find that implicit information provides limited benefit for reasoning with abstention. Our study provides new insights into how abstention and reasoning can be jointly optimized, providing a foundation for building more reliable LLMs.

</details>


### [66] [Beyond Many-Shot Translation: Scaling In-Context Demonstrations For Low-Resource Machine Translation](https://arxiv.org/abs/2602.04764)
*Luis Frentzen Salim,Esteban Carlin,Alexandre Morinvil,Xi Ai,Lun-Wei Ku*

Main category: cs.CL

TL;DR: 本文研究了利用长上下文模型在低资源语言机器翻译中的大规模示例内学习（ICL），发现上下文增大到一定程度后效果趋于饱和甚至下降。不同训练语料类型对效果影响显著，单语语料在某些情况下能与平行语料媲美。


<details>
  <summary>Details</summary>
Motivation: 低资源语言机器翻译由于高质量数据稀缺，构建难度大。尽管大语言模型提升了翻译性能，但适应低资源语言仍有挑战。因此探究通过示例内学习利用大量上下文信息提升翻译效果的可能性。

Method: 将示例内学习的上下文规模扩展至百万级token，比较三类训练语料（单语无监督、指令式、平行语料）对爪哇语和巽他语翻译的影响，分析上下文规模和语料类型对性能的影响。

Result: 结果显示，随着上下文规模增大，性能提升快速饱和，接近最大上下文窗口时性能可能下降；不同语料类型影响效果显著，部分单语语料效果可与平行语料媲美。

Conclusion: 长上下文示例内学习在低资源机器翻译中存在有效限制，上下文窗口越大不一定带来成比例的质量提升，同时不同语料类型对表现敏感，需合理选择语料与上下文规模。

Abstract: Building machine translation (MT) systems for low-resource languages is notably difficult due to the scarcity of high-quality data. Although Large Language Models (LLMs) have improved MT system performance, adapting them to lesser-represented languages remains challenging. In-context learning (ICL) may offer novel ways to adapt LLMs for low-resource MT by conditioning models on demonstration at inference time. In this study, we explore scaling low-resource machine translation ICL beyond the few-shot setting to thousands of examples with long-context models. We scale in-context token budget to 1M tokens and compare three types of training corpora used as in-context supervision: monolingual unsupervised data, instruction-style data, and parallel data (English--target and Indonesian--target). Our experiments on Javanese and Sundanese show that gains from additional context saturate quickly and can degrade near the maximum context window, with scaling behavior strongly dependent on corpus type. Notably, some forms of monolingual supervision can be competitive with parallel data, despite the latter offering additional supervision. Overall, our results characterize the effective limits and corpus-type sensitivity of long-context ICL for low-resource MT, highlighting that larger context windows do not necessarily yield proportional quality gains.

</details>


### [67] [OmniSIFT: Modality-Asymmetric Token Compression for Efficient Omni-modal Large Language Models](https://arxiv.org/abs/2602.04804)
*Yue Ding,Yiyan Ji,Jungang Li,Xuyang Liu,Xinlong Chen,Junfei Wu,Bozhou Li,Bohan Zeng,Yang Shi,Yushuo Guan,Yuanxing Zhang,Jiaheng Liu,Qiang Liu,Pengfei Wan,Liang Wang*

Main category: cs.CL

TL;DR: OmniSIFT提出了一种针对Omni-LLMs的模态非对称细粒度时空令牌压缩框架，有效减少了计算开销，同时提升性能。


<details>
  <summary>Details</summary>
Motivation: Omni-LLMs在处理长多模态令牌时计算成本高，现有的令牌压缩方法不足以应对这一挑战。

Method: 采用两阶段压缩策略：时空视频剪枝模块去除视频冗余，视觉引导音频选择模块过滤音频令牌，通过可微分的直接传递估计器进行端到端优化。

Result: 在五个基准测试中，OmniSIFT以仅25%的原始令牌上下文，保持较低延迟并超越所有压缩基线，甚至在某些任务中超越完整令牌模型。

Conclusion: OmniSIFT有效降低了Omni-LLMs的计算负担，同时提升了模型性能，展示了其在多模态令牌压缩中的优势。

Abstract: Omni-modal Large Language Models (Omni-LLMs) have demonstrated strong capabilities in audio-video understanding tasks. However, their reliance on long multimodal token sequences leads to substantial computational overhead. Despite this challenge, token compression methods designed for Omni-LLMs remain limited. To bridge this gap, we propose OmniSIFT (Omni-modal Spatio-temporal Informed Fine-grained Token compression), a modality-asymmetric token compression framework tailored for Omni-LLMs. Specifically, OmniSIFT adopts a two-stage compression strategy: (i) a spatio-temporal video pruning module that removes video redundancy arising from both intra-frame structure and inter-frame overlap, and (ii) a vision-guided audio selection module that filters audio tokens. The entire framework is optimized end-to-end via a differentiable straight-through estimator. Extensive experiments on five representative benchmarks demonstrate the efficacy and robustness of OmniSIFT. Notably, for Qwen2.5-Omni-7B, OmniSIFT introduces only 4.85M parameters while maintaining lower latency than training-free baselines such as OmniZip. With merely 25% of the original token context, OmniSIFT consistently outperforms all compression baselines and even surpasses the performance of the full-token model on several tasks.

</details>


### [68] [SE-Bench: Benchmarking Self-Evolution with Knowledge Internalization](https://arxiv.org/abs/2602.04811)
*Jiarui Yuan,Tailin Jin,Weize Chen,Zeyuan Liu,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: 本文提出SE-Bench，一个用于评估智能体自我演化和知识内化能力的诊断环境，通过伪造NumPy库及API文档，测试智能体在无文档情况下完成新编程任务的能力。


<details>
  <summary>Details</summary>
Motivation: 当前评估智能体终身学习能力面临两大挑战：先验知识干扰使“新知识”可能已出现在预训练数据中，推理复杂度干扰导致失败原因难以区分是难题本身还是知识回忆能力不足。

Method: 通过将NumPy库和其API文档伪装成带随机标识符的新包，训练智能体内化此新知识，随后在没有文档的情况下完成简单编码任务。并探讨了训练策略，如关闭式训练（Closed-Book Training）、强化学习（RL）及自我对弈（Self-Play）对知识内化的影响。

Result: 研究发现三个关键现象：（1）“开卷悖论”：带文档训练阻碍知识内化，需关闭文档训练促进知识压缩入权重；（2）强化学习存在不足，PPO剪辑及负梯度影响其内化新知识的能力；（3）自我对弈结合监督微调能够有效促进知识内化，而纯强化学习不行。

Conclusion: SE-Bench提供了一个严格的诊断平台，以检测和推动智能体的自我演化和知识内化能力，为未来研究提供了重要工具和框架。

Abstract: True self-evolution requires agents to act as lifelong learners that internalize novel experiences to solve future problems. However, rigorously measuring this foundational capability is hindered by two obstacles: the entanglement of prior knowledge, where ``new'' knowledge may appear in pre-training data, and the entanglement of reasoning complexity, where failures may stem from problem difficulty rather than an inability to recall learned knowledge. We introduce SE-Bench, a diagnostic environment that obfuscates the NumPy library and its API doc into a pseudo-novel package with randomized identifiers. Agents are trained to internalize this package and evaluated on simple coding tasks without access to documentation, yielding a clean setting where tasks are trivial with the new API doc but impossible for base models without it. Our investigation reveals three insights: (1) the Open-Book Paradox, where training with reference documentation inhibits retention, requiring "Closed-Book Training" to force knowledge compression into weights; (2) the RL Gap, where standard RL fails to internalize new knowledge completely due to PPO clipping and negative gradients; and (3) the viability of Self-Play for internalization, proving models can learn from self-generated, noisy tasks when coupled with SFT, but not RL. Overall, SE-Bench establishes a rigorous diagnostic platform for self-evolution with knowledge internalization. Our code and dataset can be found at https://github.com/thunlp/SE-Bench.

</details>


### [69] [Decomposed Prompting Does Not Fix Knowledge Gaps, But Helps Models Say "I Don't Know"](https://arxiv.org/abs/2602.04853)
*Dhruv Madhwal,Lyuxin David Zhang,Dan Roth,Tomer Wolfson,Vivek Gupta*

Main category: cs.CL

TL;DR: 本文研究了在封闭式问答中，大型语言模型通过分解式提示来提升可靠性，利用不同提示策略之间的分歧来检测潜在错误，实现训练-free的回避策略，显著提升了模型的错误检测能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在封闭式问答中常出现自信的幻觉回答，虽然分解式提示可提升准确性，但其对模型可靠性的影响尚未明确。

Method: 评估三种等效提示策略（直接、辅助、递增）在不同模型和多跳问答基准上的表现，通过提示策略间的不一致性作为内部不确定性的信号，提出一种无需训练、不依赖检索的回避策略。

Result: 提示策略间的分歧高度提示潜在错误，分歧基回避策略在F1和AUROC指标上优于传统不确定性基线，尤其在前沿模型中准确性提升减弱时仍保持有效。

Conclusion: 基于分解式提示策略的异议检测是一种实用且高效的模型可靠性诊断工具，可显著提升封闭式问答系统的错误检测和回避能力。

Abstract: Large language models often struggle to recognize their knowledge limits in closed-book question answering, leading to confident hallucinations. While decomposed prompting is typically used to improve accuracy, we investigate its impact on reliability. We evaluate three task-equivalent prompting regimes: Direct, Assistive, and Incremental, across different model scales and multi-hop QA benchmarks. We find that although accuracy gains from decomposition diminish in frontier models, disagreements between prompting regimes remain highly indicative of potential errors. Because factual knowledge is stable while hallucinations are stochastic, cross-regime agreement provides a precise signal of internal uncertainty. We leverage this signal to implement a training-free abstention policy that requires no retrieval or fine-tuning. Our results show that disagreement-based abstention outperforms standard uncertainty baselines as an error detector, improving both F1 and AUROC across settings. This demonstrates that decomposition-based prompting can serve as a practical diagnostic probe for model reliability in closed-book QA.

</details>


### [70] [CoT is Not the Chain of Truth: An Empirical Internal Analysis of Reasoning LLMs for Fake News Generation](https://arxiv.org/abs/2602.04856)
*Zhao Tong,Chunlin Gong,Yiping Zhang,Qiang Liu,Xingcheng Xu,Shu Wu,Haichao Shi,Xiao-Yu Zhang*

Main category: cs.CL

TL;DR: 本文揭示了大语言模型在拒绝有害请求时，其链式思维过程仍可能含有不安全信息，并通过解构模型层和注意力头，引入稳定性、几何和能量三种指标量化欺骗性推理模式。实验证明在激活思维模式时风险显著增加，关键决策集中在中深层的少数注意力头中。


<details>
  <summary>Details</summary>
Motivation: 传统评估假设模型拒绝有害请求即代表其推理过程安全，但本研究发现拒绝响应下模型内部思维仍可能传播不安全内容，挑战了这一安全假设。

Method: 提出统一安全分析框架，分解链式思维生成过程，利用Jacobian谱指标评估单个注意力头的作用，并设计稳定性、几何、能量三个可解释指标来量化注意力头对欺骗性推理的响应。

Result: 在多个具推理能力的LLM上进行大量实验，结果显示激活思维模式显著增加生成风险，关键决策主要集中在少数中深层连续的注意力头，通过识别这些注意力头，揭示逻辑偏差来源。

Conclusion: 拒绝响应并不意味着模型内部推理安全，明确识别导致风险的注意力头有助于理解并缓解潜在推理风险，从而对LLM安全机制提出新的认识视角。

Abstract: From generating headlines to fabricating news, the Large Language Models (LLMs) are typically assessed by their final outputs, under the safety assumption that a refusal response signifies safe reasoning throughout the entire process. Challenging this assumption, our study reveals that during fake news generation, even when a model rejects a harmful request, its Chain-of-Thought (CoT) reasoning may still internally contain and propagate unsafe narratives. To analyze this phenomenon, we introduce a unified safety-analysis framework that systematically deconstructs CoT generation across model layers and evaluates the role of individual attention heads through Jacobian-based spectral metrics. Within this framework, we introduce three interpretable measures: stability, geometry, and energy to quantify how specific attention heads respond or embed deceptive reasoning patterns. Extensive experiments on multiple reasoning-oriented LLMs show that the generation risk rise significantly when the thinking mode is activated, where the critical routing decisions concentrated in only a few contiguous mid-depth layers. By precisely identifying the attention heads responsible for this divergence, our work challenges the assumption that refusal implies safety and provides a new understanding perspective for mitigating latent reasoning risks.

</details>


### [71] [Reinforced Attention Learning](https://arxiv.org/abs/2602.04884)
*Bangzheng Li,Jianmo Ni,Chen Qu,Ian Miao,Liu Yang,Xingyu Fu,Muhao Chen,Derek Zhiyuan Cheng*

Main category: cs.CL

TL;DR: 本文提出了强化注意力学习（RAL），通过优化内部注意力分布提升多模态大语言模型的性能，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统通过生成冗长推理步骤的强化学习方法在多模态大语言模型中效果有限，甚至降低了感知能力，亟需更有效的后训练策略。

Method: 提出强化注意力学习（RAL），采用策略梯度直接优化内部注意力分布，重点关注信息分配和多模态输入的有效关联，同时引入基于策略的注意力蒸馏提升跨模态对齐。

Result: 在多种图像和视频基准测试中，RAL优于现有方法GRPO和其他基线，验证了其在增强多模态模型推理与感知能力上的有效性。

Conclusion: 通过关注模型的注意力策略而非输出序列，RAL为多模态模型的后训练提供了一种通用且有效的新范式。

Abstract: Post-training with Reinforcement Learning (RL) has substantially improved reasoning in Large Language Models (LLMs) via test-time scaling. However, extending this paradigm to Multimodal LLMs (MLLMs) through verbose rationales yields limited gains for perception and can even degrade performance.
  We propose Reinforced Attention Learning (RAL), a policy-gradient framework that directly optimizes internal attention distributions rather than output token sequences. By shifting optimization from what to generate to where to attend, RAL promotes effective information allocation and improved grounding in complex multimodal inputs. Experiments across diverse image and video benchmarks show consistent gains over GRPO and other baselines. We further introduce On-Policy Attention Distillation, demonstrating that transferring latent attention behaviors yields stronger cross-modal alignment than standard knowledge distillation. Our results position attention policies as a principled and general alternative for multimodal post-training.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [72] [On the Uncertainty of Large Language Model-Based Multi-Agent Systems](https://arxiv.org/abs/2602.04234)
*Yuxuan Zhao,Sijia Chen,Ningxin Su*

Main category: cs.MA

TL;DR: 本文通过分析多智能体系统（MAS）在公开大语言模型基础上的不确定性动态，揭示了单智能体在约43.3%情况下优于多智能体的现象，并提出了影响MAS表现的三个关键观察；基于此设计了Entropy Judger算法提升了MAS的准确率。


<details>
  <summary>Details</summary>
Motivation: 当前对基于公开大语言模型构建的多智能体系统(MAS)的成功或失败机制缺乏理解，特别是不确定性在其中的作用尚未被充分探索。

Method: 本文通过分析245个涵盖token、轨迹及回合层级的熵特征，研究了不同拓扑结构和六个基准任务中MAS在解决问题过程中的熵变化规律，进而提出Entropy Judger算法从MAS生成的多解结果中选择最优方案。

Result: 发现单一智能体在43.3%的情况下表现优于MAS，且不确定性动态主要在首轮互动中确定；验证了确定性偏好、基础不确定性和任务感知三大观察；Entropy Judger算法在所有MAS配置和任务中均显著提升准确率。

Conclusion: 不确定性动态是理解MAS效能的关键，合理利用熵特征能够提升多智能体系统的表现，Entropy Judger算法为MAS解答筛选提供了有效的技术支持。

Abstract: Multi-agent systems (MAS) have emerged as a prominent paradigm for leveraging large language models (LLMs) to tackle complex tasks. However, the mechanisms governing the effectiveness of MAS built upon publicly available LLMs, specifically the underlying rationales for their success or failure, remain largely unexplored. In this paper, we revisit MAS through the perspective of uncertainty, considering both intra- and inter-agent dynamics by investigating entropy transitions during problem-solving across various topologies and six benchmark tasks. By analyzing 245 features spanning token-, trajectory-, and round-level entropy, we counterintuitively find that a single agent outperforms MAS in approximately 43.3% of cases, and that uncertainty dynamics are largely determined during the first round of interaction. Furthermore, we provide three key observations: 1) Certainty Preference: reducing uncertainty at any stage for any agent is critical for guaranteeing correct solutions; 2) Base Uncertainty: base models with lower entropy during problem-solving directly benefit MAS performance; and 3) Task Awareness: entropy dynamics of MAS play varying roles across different tasks. Building on these insights, we introduce a simple yet effective algorithm, the Entropy Judger, to select solutions from MAS's pass@k results, leading to consistent accuracy improvements across all MAS configurations and tasks. Our source code is available at https://github.com/AgenticFinLab/multiagent-entropy.

</details>


### [73] [SPEAR: An Engineering Case Study of Multi-Agent Coordination for Smart Contract Auditing](https://arxiv.org/abs/2602.04418)
*Arnab Mallick,Indraveni Chebolu,Harmesh Rana*

Main category: cs.MA

TL;DR: SPEAR是一种多智能体协调框架，用于智能合约审计，通过规划、执行和修复代理合作完成审计任务，实现动态协调和恢复。


<details>
  <summary>Details</summary>
Motivation: 智能合约审计复杂且容易失败，需多智能体协作提高效率和鲁棒性。

Method: 设计了三个专门代理（规划、执行、修复），采用多智能体系统模式和博弈协议，更新信念和调整计划。

Result: 实证研究显示多智能体设计在协调、恢复和资源使用方面优于集中式和流水线式方法。

Conclusion: 多智能体协调框架SPEAR有效提升了智能合约审计的鲁棒性和资源效率，适合复杂安全分析任务。

Abstract: We present SPEAR, a multi-agent coordination framework for smart contract auditing that applies established MAS patterns in a realistic security analysis workflow. SPEAR models auditing as a coordinated mission carried out by specialized agents: a Planning Agent prioritizes contracts using risk-aware heuristics, an Execution Agent allocates tasks via the Contract Net protocol, and a Repair Agent autonomously recovers from brittle generated artifacts using a programmatic-first repair policy. Agents maintain local beliefs updated through AGM-compliant revision, coordinate via negotiation and auction protocols, and revise plans as new information becomes available. An empirical study compares the multi-agent design with centralized and pipeline-based alternatives under controlled failure scenarios, focusing on coordination, recovery behavior, and resource use.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [74] [Accountability in Open Source Software Ecosystems: Workshop Report](https://arxiv.org/abs/2602.04026)
*Nandini Sharma,Thomas Bock,Rich Bowen,Sayeed Choudhury,Brian Fitzgerald,Matt Germonprez,Jim Herbsleb,James Howison,Tom Hughes,Min Kyung Lee,Stephanie Lieggi,Andreas Liesenfeld,Georg Link,Nicholas Matsakis,Audris Mockus,Narayan Ramasubbu,Christopher Robinson,Gregorio Robles,Nithya Ruff,Sonali Shah,Igor Steinmacher,Bogdan Vasilescu,Stephen Walli,Christopher Yoo*

Main category: cs.SE

TL;DR: 本论文通过在卡内基梅隆大学召集24位专家学者和从业者举办研讨会，探讨开源软件生态系统中各利益相关者的需求、动机及其相互关系，旨在推动问责制的研究和实践。


<details>
  <summary>Details</summary>
Motivation: 开源软件生态系统中利益相关者多样且需求复杂，存在动机差异和潜在冲突，因此需要理解如何识别、满足利益相关者需求并实现问责。

Method: 组织了一场主题为“问责与开源软件生态系统”的面对面专家研讨会，汇聚24位内外部专家，共同讨论相关重要且紧迫的问题。

Result: 通过研讨会初步讨论了开源社区中的问责角色，激发了研究议程和实践中有效利益相关者参与的构想。

Conclusion: 该工作为探索和推动开源软件生态系统中的问责机制及多方利益相关者参与提供了理论和实践基础。

Abstract: Open source software ecosystems are composed of a variety of stakeholders including but not limited to non-profit organizations, volunteer contributors, users, and corporations. The needs and motivations of these stakeholders are often diverse, unknown, and sometimes even conflicting given the engagement and investment of both volunteers and corporate actors. Given this, it is not clear how open source communities identify and engage with their stakeholders, understand their needs, and hold themselves accountable to those needs. We convened 24 expert scholars and practitioners studying and working with open source software communities for an exploratory workshop discussion on these ideas. The workshop titled "Accountability and Open Source Software Ecosystems" was organized on Oct 14-15 on campus in Carnegie Mellon University, Pittsburgh, PA. The purpose of this in-person workshop was to initiate conversations that explore important and urgent questions related to the role of accountability in open source software ecosystems, and to inspire an exciting research agenda and meaningful stakeholder engagement ideas for practitioners.

</details>


### [75] [Exploring the Potential of Large Language Models in Simulink-Stateflow Mutant Generation](https://arxiv.org/abs/2602.04066)
*Pablo Valle,Shaukat Ali,Aitor Arrieta*

Main category: cs.SE

TL;DR: 本文提出利用大型语言模型（LLMs）自动生成Simulink-Stateflow模型的变异体，以提升变异测试的效率和有效性。


<details>
  <summary>Details</summary>
Motivation: 传统的变异测试方法在Simulink-Stateflow模型中面临多种挑战，如生成冗余、等价或不可执行的变异体，且因模型的层级结构和复杂动态行为导致问题更为严重。现有基于机器学习或手工设计变异算子的方案受限于数据和扩展性。

Method: 本文开发自动化流程，将Simulink-Stateflow模型转为结构化JSON表示，利用八个先进LLM，通过不同变异和提示策略实验生成变异体，结合少量示例提示和适中温度参数优化生成效果。

Result: 实验生成38400个变异体，LLM生成速度提升最多13倍，同时显著减少等价和重复变异体，变异质量优于手工设计基线。

Conclusion: 基于LLM的自动变异生成方法有效提升了Simulink-Stateflow模型变异测试的效率和质量，相关工具和数据集开源，促进领域后续研究。

Abstract: Mutation analysis is a powerful technique for assessing test-suite adequacy, yet conventional approaches suffer from generating redundant, equivalent, or non-executable mutants. These challenges are particularly amplified in Simulink-Stateflow models due to the hierarchical structure these models have, which integrate continuous dynamics with discrete-event behaviors and are widely deployed in safety-critical Cyber-Physical Systems (CPSs). While prior work has explored machine learning and manually engineered mutation operators, these approaches remain constrained by limited training data and scalability issues. Motivated by recent advances in Large Language Models (LLMs), we investigate their potential to generate high-quality, domain-specific mutants for Simulink-Stateflow models. We develop an automated pipeline that converts Simulink-Stateflow models to structured JSON representations and systematically evaluates different mutation and prompting strategies across eight state-of-the-art LLMs. Through a comprehensive empirical study involving 38,400 LLM-generated mutants across four Simulink-Stateflow models, we demonstrate that LLMs generate mutants up to 13x faster than a manually engineered mutation-based baseline while producing significantly fewer equivalent and duplicate mutants and consistently achieving superior mutant quality. Moreover, our analysis reveals that few-shot prompting combined with low-to-medium temperature values yields optimal results. We provide an open-source prototype tool and release our complete dataset to facilitate reproducibility and advance future research in this domain.

</details>


### [76] [I Can't Believe It's Not a Valid Exploit](https://arxiv.org/abs/2602.04165)
*Derin Gezgin,Amartya Das,Shinhae Kim,Zhengdong Huang,Nevena Stojkovic,Claire Wang*

Main category: cs.SE

TL;DR: 该论文提出了PoC-Gym框架，用于通过大语言模型(LLMs)生成和验证Java安全漏洞的概念证明(Proof-of-Concept，PoC)代码。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在生成漏洞PoC时存在成功率验证不准确的问题，且辅助静态分析工具可能提高生成效果，需系统评估其有效性。

Method: 开发PoC-Gym框架，结合静态分析工具指导LLMs生成Java安全漏洞PoC，并对成功率和PoC有效性进行系统评估和手动检查。

Result: 结合静态分析工具指导，PoC生成成功率提升21%，但手动检查发现71.5%的PoC无效，表明当前验证机制存在较大局限。

Conclusion: LLM生成漏洞PoC的成功率报告存在显著误导风险，需改进验证机制确保生成PoC的真实性和有效性。

Abstract: Recently Large Language Models (LLMs) have been used in security vulnerability detection tasks including generating proof-of-concept (PoC) exploits. A PoC exploit is a program used to demonstrate how a vulnerability can be exploited. Several approaches suggest that supporting LLMs with additional guidance can improve PoC generation outcomes, motivating further evaluation of their effectiveness. In this work, we develop PoC-Gym, a framework for PoC generation for Java security vulnerabilities via LLMs and systematic validation of generated exploits. Using PoC-Gym, we evaluate whether the guidance from static analysis tools improves the PoC generation success rate and manually inspect the resulting PoCs. Our results from running PoC-Gym with Claude Sonnet 4, GPT-5 Medium, and gpt-oss-20b show that using static analysis for guidance and criteria lead to 21% higher success rates than the prior baseline, FaultLine. However, manual inspection of both successful and failed PoCs reveals that 71.5% of the PoCs are invalid. These results show that the reported success of LLM-based PoC generation can be significantly misleading, which is hard to detect with current validation mechanisms.

</details>


### [77] [SOGPTSpotter: Detecting ChatGPT-Generated Answers on Stack Overflow](https://arxiv.org/abs/2602.04185)
*Suyu Ma,Chunyang Chen,Hourieh Khalajzadeh,John Grundy*

Main category: cs.SE

TL;DR: 本文提出了SOGPTSpotter，一种基于Siamese神经网络结合BigBird模型和Triplet loss的方法，用于检测Stack Overflow上的ChatGPT生成回答，实验表明其性能优于多种基线模型，并具有良好的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 由于ChatGPT生成的答案数量激增，且其中可能包含不准确的信息，因此需要有效检测这些AI生成内容以维护Stack Overflow社区的内容质量。

Method: 采用Siamese神经网络结构，利用BigBird模型和Triplet loss，训练模型区分人工答案、参考答案与ChatGPT生成答案三元组，进行ChatGPT答案检测。

Result: SOGPTSpotter在检测ChatGPT合成回答方面明显优于GPTZero、DetectGPT、GLTR、BERT、RoBERTa和GPT-2等多种基线模型，且通过消融实验验证了模型设计的有效性。

Conclusion: 该方法不仅具备较强的检测能力和鲁棒性，还能跨领域适用，在实际Stack Overflow社区管理中有效辅助识别并清理AI生成答案，具有较高的实用价值。

Abstract: Stack Overflow is a popular Q&A platform where users ask technical questions and receive answers from a community of experts. Recently, there has been a significant increase in the number of answers generated by ChatGPT, which can lead to incorrect and unreliable information being posted on the site. While Stack Overflow has banned such AI-generated content, detecting whether a post is ChatGPT-generated remains a challenging task. We introduce a novel approach, SOGPTSpotter, that employs Siamese Neural Networks, leveraging the BigBird model and the Triplet loss, to detect ChatGPT-generated answers on Stack Overflow. We use triplets of human answers, reference answers, and ChatGPT answers. Our empirical evaluation reveals that our approach outperforms well-established baselines like GPTZero, DetectGPT, GLTR, BERT, RoBERTa, and GPT-2 in identifying ChatGPT-synthesized Stack Overflow responses. We also conducted an ablation study to show the effectiveness of our model. Additional experiments were conducted to assess various factors, including the impact of text length, the model's robustness against adversarial attacks, and its generalization capabilities across different domains and large language models. We also conducted a real-world case study on Stack Overflow. Using our tool's recommendations, Stack Overflow moderators were able to identify and take down ChatGPT-suspected generated answers, demonstrating the practical applicability and effectiveness of our approach.

</details>


### [78] [Semantic Consensus Decoding: Backdoor Defense for Verilog Code Generation](https://arxiv.org/abs/2602.04195)
*Guang Yang,Xing Hu,Xiang Chen,Xin Xia*

Main category: cs.SE

TL;DR: 本文提出了一种针对硬件设计中基于大语言模型的后门攻击的被动防御方法——语义共识解码（SCD），能有效降低攻击成功率至3%以下，且对生成质量影响极小。


<details>
  <summary>Details</summary>
Motivation: 硬件设计中基于大语言模型生成的代码易受后门攻击影响，且硬件后门一旦被制造，难以修复，现有防御方法受限于训练数据访问或对隐蔽触发效果差，亟需新的有效防御手段。

Method: 基于攻击者倾向于将触发器嵌入非功能性需求的假设，设计语义共识解码方法，包含功能需求提取和基于全需求与提取功能需求输出分布的共识解码，在推理时自动抑制异常输出。

Result: 实验证明，SCD在三种典型后门攻击下，将攻击成功率从平均89%降至不足3%，且对生成质量几乎无影响。

Conclusion: SCD通过区分功能与非功能需求，成功实现对硬件设计LLM后门攻击的有效被动防御，兼具隐蔽性和高效性，适合第三方用户使用。

Abstract: Large language models (LLMs) for Verilog code generation are increasingly adopted in hardware design, yet remain vulnerable to backdoor attacks where adversaries inject malicious triggers during training to induce vulnerable hardware designs. Unlike patchable software vulnerabilities, hardware trojans become irreversible once fabricated, making remediation extremely costly or impossible. Existing active defenses require access to training data, impractical for third-party LLM users, while passive defenses struggle against semantically stealthy triggers that naturally blend into design specifications. In this paper, we hypothesize that under the requirements of both effectiveness and stealthiness, attackers are strongly biased toward embedding triggers in non-functional requirements (e.g., style modifiers, quality descriptors) rather than functional specifications that determine hardware behavior. Exploiting this insight, we propose Semantic Consensus Decoding (SCD), an inference-time passive defense with two key components: (1) functional requirement extraction that identifies essential requirements from user specifications, and (2) consensus decoding that adaptively fuses output distributions based on full user specifications and extracted functional requirements. When these distributions diverge significantly, SCD automatically suppresses suspicious components. Extensive experiments with three representative backdoor attacks demonstrate that SCD reduces average attack success rate from 89% to under 3% with negligible impact on generation quality.

</details>


### [79] [Why Agentic-PRs Get Rejected: A Comparative Study of Coding Agents](https://arxiv.org/abs/2602.04226)
*Sota Nakashima,Yuta Ishimoto,Masanari Kondo,Shane Mclntosh,Yasutaka Kamei*

Main category: cs.SE

TL;DR: 本文比较了五种不同自动编码代理生成的被拒绝Pull Requests（代理PRs）与人工生成PRs的拒绝模式，发现代理PRs有独特的拒绝原因并存在代理特异性模式，且大部分拒绝缺乏明确反馈。


<details>
  <summary>Details</summary>
Motivation: 自动编码代理生成的代码提交被拒绝的原因尚未跨不同代理进行比较，了解这些差异可以帮助改进代理的开发和应用。

Method: 分析了AIDev数据集中654个被拒绝的PRs，涵盖五种编码代理及人工基线，归纳拒绝原因和模式，并提出了减少无反馈拒绝的启发式方法。

Result: 七种拒绝模式仅在代理PRs中出现，如对AI代码的不信任；不同代理显示出不同的拒绝特征；67.9%的拒绝缺乏评审反馈。

Conclusion: 自动编码代理的PR拒绝具有独特且差异化的模式，缺乏反馈是主要问题，提出的启发式方法可改善未来相关研究的数据质量。

Abstract: Agentic coding -- software development workflows in which autonomous coding agents plan, implement, and submit code changes with minimal human involvement -- is rapidly gaining traction. Prior work has shown that Pull Requests (PRs) produced using coding agents (Agentic-PRs) are accepted less often than PRs that are not labeled as agentic (Human-PRs). The rejection reasons for a single agent (Claude Code) have been explored, but a comparison of how rejection reasons differ between Agentic-PRs generated by different agents has not yet been performed. This comparison is important since different coding agents are often used for different purposes, which can lead to agent-specific failure patterns. In this paper, we inspect 654 rejected PRs from the AIDev dataset covering five coding agents, as well as a human baseline. Our results show that seven rejection modes occur only in Agentic-PRs, including distrust of AI-generated code. We also observe agent-specific patterns (e.g., automated withdrawal of inactive PRs by Devin), reflecting differences in how agents are configured and used in practice. Notably, a large proportion of rejected PRs (67.9%) lack explicit reviewer feedback, making their rejection reasons difficult to determine. To mitigate this issue, we propose a set of heuristics that reduce the proportion of such cases, offering a practical preprocessing step for future studies of PR rejection in agentic coding.

</details>


### [80] [ProxyWar: Dynamic Assessment of LLM Code Generation in Game Arenas](https://arxiv.org/abs/2602.04296)
*Wenjun Peng,Xinyu Wang,Qi Wu*

Main category: cs.SE

TL;DR: ProxyWar是一个通过多代理竞赛游戏环境评估大语言模型生成代码质量的新框架，揭示了传统评测方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有自动代码生成评测主要依赖静态基准和简单指标，难以全面反映生成代码在实际动态环境中的表现。

Method: ProxyWar将生成的代码代理嵌入多样化竞争游戏环境，通过自动测试、迭代修复及多代理竞赛系统综合评估代码的功能和运行特性。

Result: 在多款先进模型和游戏环境中的应用显示，传统基准得分与实际表现存在显著差异，暴露了模型未被注意到的缺陷和改进空间。

Conclusion: 动态竞争式评价方法更全面，有助于推动自动代码生成模型在算法发现、自适应问题解决和实际效率等方向的研究和提升。

Abstract: Large language models (LLMs) have revolutionized automated code generation, yet the evaluation of their real-world effectiveness remains limited by static benchmarks and simplistic metrics. We present ProxyWar, a novel framework that systematically assesses code generation quality by embedding LLM-generated agents within diverse, competitive game environments. Unlike existing approaches, ProxyWar evaluates not only functional correctness but also the operational characteristics of generated programs, combining automated testing, iterative code repair, and multi-agent tournaments to provide a holistic view of program behavior. Applied to a range of state-of-the-art coders and games, our approach uncovers notable discrepancies between benchmark scores and actual performance in dynamic settings, revealing overlooked limitations and opportunities for improvement. These findings highlight the need for richer, competition-based evaluation of code generation. Looking forward, ProxyWar lays a foundation for research into LLM-driven algorithm discovery, adaptive problem solving, and the study of practical efficiency and robustness, including the potential for models to outperform hand-crafted agents. The project is available at https://github.com/xinke-wang/ProxyWar.

</details>


### [81] [Model-Driven Legacy System Modernization at Scale](https://arxiv.org/abs/2602.04341)
*Tobias Böhm,Jens Guan Su Tien,Mohini Nonnenmann,Tom Schoonbaert,Bart Carpels,Andreas Biesdorf*

Main category: cs.SE

TL;DR: 该论文提出了一种模型驱动的遗留系统现代化方法，通过插入技术无关的中间模型，实现遗留系统向现代平台的半自动迁移，提升代码可维护性和扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统遗留系统向现代平台迁移存在复杂性高、风险大、手工工作量大的问题，急需一种系统化、可追踪且可复用的迁移方法。

Method: 提出了包含分析、丰富、合成与过渡四个阶段的端到端模型驱动迁移过程，通过技术无关的丰富中间模型抽象系统结构、依赖关系和语义元数据，应用规则保持系统功能和非功能性，支持半自动迁移。

Result: 在基于.NET和ASP.NET MVC的工业级大型应用上应用方法，成功实现核心用户界面和页面结构的半自动迁移，保持功能和关键非功能性，提升系统可维护性和扩展性。

Conclusion: 模型驱动抽象有效降低迁移风险与成本，提高可扩展性和可追踪性，适用于类似现代化场景并促进迁移模式复用，但复杂自定义布局仍需人工调整。

Abstract: This experience report presents a model-driven approach to legacy system modernization that inserts an enriched, technology-agnostic intermediate model between the legacy codebase and the modern target platform, and reports on its application and evaluation. The four-stage process of analysis, enrichment, synthesis, and transition systematically extracts, abstracts, and transforms system artifacts. We apply our approach to a large industrial application built on legacy versions of the .NET Framework and ASP.NET MVC and show that core user interface components and page structures can be migrated semi-automatically to a modern web stack while preserving functional behavior and essential non-functional qualities. By consolidating architectural knowledge into explicit model representations, the resulting codebase exhibits higher maintainability and extensibility, thereby improving developer experience. Although automation is effective for standard patterns, migration of bespoke layout composites remains challenging and requires targeted manual adaptation. Our contributions are: (i) an end-to-end model-driven process, (ii) an enriched intermediate model that captures structure, dependencies, and semantic metadata, (iii) transformation rules that preserve functional behavior and essential non-functional qualities, and (iv) application and evaluation of the approach in an industrial setting. Overall, model-based abstractions reduce risk and effort while supporting scalable, traceable modernization of legacy applications. Our approach generalizes to comparable modernization contexts and promotes reuse of migration patterns.

</details>


### [82] [Generative AI in Systems Engineering: A Framework for Risk Assessment of Large Language Models](https://arxiv.org/abs/2602.04358)
*Stefan Otten,Philipp Reis,Philipp Rigoll,Joshua Ransiek,Tobias Schürmann,Jacob Langner,Eric Sax*

Main category: cs.SE

TL;DR: 本文提出了LLM风险评估框架（LRF），用于系统工程环境中评估大语言模型的风险，帮助实现安全透明的应用。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在工程生命周期中有巨大潜力，但组织在风险评估方面面临挑战，导致应用不一致、失败模式未知和难以扩展。

Method: LRF基于自动化程度和影响度两个维度对LLM应用进行分类，从而确定相应的风险级别，指导验证策略和人工监督水平。

Result: 该框架实现了跨生命周期的风险一致性判断，支持选择合适的验证措施和反制策略，确保LLM安全部署。

Conclusion: LRF为复杂工程环境中风险意识的LLM应用提供基础，是系统工程中实现AI保障的初步标准化尝试。

Abstract: The increasing use of Large Language Models (LLMs) offers significant opportunities across the engineering lifecycle, including requirements engineering, software development, process optimization, and decision support. Despite this potential, organizations face substantial challenges in assessing the risks associated with LLM use, resulting in inconsistent integration, unknown failure modes, and limited scalability. This paper introduces the LLM Risk Assessment Framework (LRF), a structured approach for evaluating the application of LLMs within Systems Engineering (SE) environments. The framework classifies LLM-based applications along two fundamental dimensions: autonomy, ranging from supportive assistance to fully automated decision making, and impact, reflecting the potential severity of incorrect or misleading model outputs on engineering processes and system elements. By combining these dimensions, the LRF enables consistent determination of corresponding risk levels across the development lifecycle. The resulting classification supports organizations in identifying appropriate validation strategies, levels of human oversight, and required countermeasures to ensure safe and transparent deployment. The framework thereby helps align the rapid evolution of AI technologies with established engineering principles of reliability, traceability, and controlled process integration. Overall, the LRF provides a basis for risk-aware adoption of LLMs in complex engineering environments and represents a first step toward standardized AI assurance practices in systems engineering.

</details>


### [83] [AgenticAKM : Enroute to Agentic Architecture Knowledge Management](https://arxiv.org/abs/2602.04445)
*Rudra Dhar,Karthik Vaidhyanathan,Vasudeva Varma*

Main category: cs.SE

TL;DR: 本文提出了一种名为AgenticAKM的建筑知识管理自动化方法，利用多智能体分工合作生成建筑决策记录（ADR），通过29个代码仓库的用户研究验证效果优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统的软件建筑知识管理过程繁琐，开发者和架构师难以主动采纳，现有大语言模型（LLM）单一提示方法受限于上下文范围且难以处理分布式建筑知识，急需有效自动化方案。

Method: 提出AgenticAKM方法，将建筑恢复和文档生成任务拆分为多个子任务，由专门的智能体负责提取、检索、生成和验证，在结构化工作流程中协同完成建筑知识生成。

Result: 通过29个代码仓库的用户研究，验证了AgenticAKM方法能够生成质量更高的建筑决策记录（ADRs），表现优于单一提示方法。

Conclusion: AgenticAKM作为一种多智能体协同的自动化建筑知识管理方法，展示了良好的实用性和效果，具有推广应用的潜力。

Abstract: Architecture Knowledge Management (AKM) is crucial for maintaining current and comprehensive software Architecture Knowledge (AK) in a software project. However AKM is often a laborious process and is not adopted by developers and architects. While LLMs present an opportunity for automation, a naive, single-prompt approach is often ineffective, constrained by context limits and an inability to grasp the distributed nature of architectural knowledge. To address these limitations, we propose an Agentic approach for AKM, AgenticAKM, where the complex problem of architecture recovery and documentation is decomposed into manageable sub-tasks. Specialized agents for architecture Extraction, Retrieval, Generation, and Validation collaborate in a structured workflow to generate AK. To validate we made an initial instantiation of our approach to generate Architecture Decision Records (ADRs) from code repositories. We validated our approach through a user study with 29 repositories. The results demonstrate that our agentic approach generates better ADRs, and is a promising and practical approach for automating AKM.

</details>


### [84] [What's in a Benchmark? The Case of SWE-Bench in Automated Program Repair](https://arxiv.org/abs/2602.04449)
*Matias Martinez,Xavier Franch*

Main category: cs.SE

TL;DR: 本文对SWE-Bench的两个公开排行榜进行了全面分析，揭示了提交者背景、使用的LLM及方法的开放性。


<details>
  <summary>Details</summary>
Motivation: 探究SWE-Bench排行榜的参与者及其技术特征，以促进APR领域的透明度和多样性。

Method: 统计分析79个Lite排行榜和133个Verified排行榜的提交条目，研究其来源、LLM类型及优劣势。

Result: 大多数提交来自工业界，尤其是小型和大型上市公司，主要使用专有LLM（如Claude系列），当前最佳结果由Claude 4 Sonnet获得，学术界贡献虽多为开源但仍具竞争力。

Conclusion: SWE-Bench生态系统中工业界和专有LLM占主导，研究结果可促进后续工作的透明度和多样性发展。

Abstract: The rapid progress in Automated Program Repair (APR) has been fueled by advances in AI, particularly large language models (LLMs) and agent-based systems. SWE-Bench is a benchmark designed to evaluate repair systems using real issues mined from popular open-source Python repositories. Its public leaderboards-SWE-Bench Lite and Verified-have become central platforms for tracking progress and comparing solutions. In this paper, we present the first comprehensive study of these two leaderboards, examining who is submitting solutions, the products behind the submissions, the LLMs employed, and the openness of the approaches. We analyze 79 entries submitted to Lite leaderboard and 133 to Verified. Our results show that most entries on both leaderboards originate from industry, particularly small companies and large publicly traded companies. These submissions often achieve top results, although academic contributions-typically open source-also remain competitive. We also find a clear dominance of proprietary LLMs, especially Claude family, with state-of-the-art results on both leaderboards currently achieved by Claude 4 Sonnet. These findings offer insights into the SWE-Bench ecosystem that can guide greater transparency and diversity in future benchmark-driven research.

</details>


### [85] [A Framework of Critical Success Factors for Agile Software Development](https://arxiv.org/abs/2602.04467)
*Ridewaan Hanslo,Maureen Tanner*

Main category: cs.SE

TL;DR: 该综述通过分析53项研究，识别了敏捷项目21个关键成功因素，涵盖组织、人、技术、流程和项目五大主题。


<details>
  <summary>Details</summary>
Motivation: 敏捷软件开发虽流行，但项目成功依旧具有挑战性，需明确关键成功因素。

Method: 采用主题综合与内容分析，系统综述53项主要研究以归纳关键成功因素。

Result: 共识别出21个关键成功因素，团队效能和项目管理最为重要，并构建了理论框架说明这些因素如何促成项目成功。

Conclusion: 研究为敏捷项目成功提供理论支撑，建议未来通过定量方法验证此框架及发现，指导研究和实践。

Abstract: Despite the popularity of Agile software development, achieving consistent project success remains challenging. This systematic literature review identifies critical success factors (CSFs) in Agile projects by analyzing 53 primary studies. Employing thematic synthesis with content analysis, our analysis yielded 21 CSFs categorized into five themes: organizational, people, technical, process, and project. Team effectiveness and project management emerged as the most frequently cited CSFs, highlighting the importance of people and process factors. These interpreted themes and factors contributed to the development of a theoretical framework to identify how these factors contribute to project success. This study offers valuable insights for researchers and practitioners, guiding future research to validate these findings and test the proposed framework using quantitative methods.

</details>


### [86] [Towards Structured, State-Aware, and Execution-Grounded Reasoning for Software Engineering Agents](https://arxiv.org/abs/2602.04640)
*Tse-Hsun,Chen*

Main category: cs.SE

TL;DR: 本文指出当前软件工程代理主要为反应式设计，难以进行长远推理，提出需要结构化、具备状态感知和执行反馈的推理方式以提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前软件工程代理缺乏结构化和持久状态记忆，导致其无法有效进行多步骤推理和适应新证据。

Method: 提出将显式结构、持续演变的状态及执行反馈整合进代理的推理模型，以实现更连贯和可靠的长周期推理。

Result: 提供了实现上述目标的初步路线图，强调结构化和状态感知对提升软件工程代理能力的重要性。

Conclusion: 未来软件工程代理应超越简单反应，拥抱结构化和执行驱动推理，才能更好地完成实际复杂任务。

Abstract: Software Engineering (SE) agents have shown promising abilities in supporting various SE tasks. Current SE agents remain fundamentally reactive, making decisions mainly based on conversation history and the most recent response. However, this reactive design provides no explicit structure or persistent state within the agent's memory, making long-horizon reasoning challenging. As a result, SE agents struggle to maintain a coherent understanding across reasoning steps, adapt their hypotheses as new evidence emerges, or incorporate execution feedback into the mental reasoning model of the system state.
  In this position paper, we argue that, to further advance SE agents, we need to move beyond reactive behavior toward a structured, state-aware, and execution-grounded reasoning. We outline how explicit structure, persistent and evolving state, and the integration of execution-grounded feedback can help SE agents perform more coherent and reliable reasoning in long-horizon tasks. We also provide an initial roadmap for developing next-generation SE agents that can more effectively perform real-world tasks.

</details>


### [87] [Supporting software engineering tasks with agentic AI: Demonstration on document retrieval and test scenario generation](https://arxiv.org/abs/2602.04726)
*Marian Kica,Lukas Radosky,David Slivka,Karin Kubinova,Daniel Dovhun,Tomas Uhercik,Erik Bircak,Ivan Polasek*

Main category: cs.SE

TL;DR: 本文提出了基于智能代理的AI解决方案，用于自动测试场景生成和软件工程文档检索。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的出现，软件开发模式发生变革，促使软件工程研究开发大量工具，本文希望通过智能代理提升测试场景生成和文档检索效率。

Method: 针对自动测试场景生成，设计了以监督代理为中心的星形结构智能代理系统；针对文档检索任务，设计了基于大型语言模型的多智能代理系统，分别处理搜索、问答、变更追踪和文档摘要等子任务。

Result: 展示了自动测试场景生成方法在真实案例中的能力；文档检索解决方案支持多种软件工程文档相关的用例，有效实现了复杂任务的自动化。

Conclusion: 基于智能代理的多任务AI解决方案在软件工程领域具有广阔应用前景，未来研究将继续深入探索其潜力。

Abstract: The introduction of large language models ignited great retooling and rethinking of the software development models. The ensuing response of software engineering research yielded a massive body of tools and approaches. In this paper, we join the hassle by introducing agentic AI solutions for two tasks. First, we developed a solution for automatic test scenario generation from a detailed requirements description. This approach relies on specialized worker agents forming a star topology with the supervisor agent in the middle. We demonstrate its capabilities on a real-world example. Second, we developed an agentic AI solution for the document retrieval task in the context of software engineering documents. Our solution enables performing various use cases on a body of documents related to the development of a single software, including search, question answering, tracking changes, and large document summarization. In this case, each use case is handled by a dedicated LLM-based agent, which performs all subtasks related to the corresponding use case. We conclude by hinting at the future perspectives of our line of research.

</details>


### [88] [Demonstrating ARG-V's Generation of Realistic Java Benchmarks for SV-COMP](https://arxiv.org/abs/2602.04786)
*Charles Moloney,Robert Dyer,Elena Sherman*

Main category: cs.SE

TL;DR: 本文介绍了使用ARG-V工具自动生成Java验证基准，并发现新基准集使主流验证器性能下降，强调了提高评估全面性和现实性的必要性。


<details>
  <summary>Details</summary>
Motivation: 确保SV-COMP竞赛中新增基准能暴露验证器在现有基准中未体现的行为，以提高竞赛结果的有效性和工具的实际适用性。

Method: 采用ARG-V工具自动生成符合SV-COMP格式的Java验证基准集，并测试四个领先Java验证器在该新基准集上的表现。

Result: 新生成的68个Java基准使所有四个主要验证器的准确率和召回率均下降，表现出更具挑战性的验证环境。

Conclusion: ARG-V生成的新基准有助于提升验证工具评估的全面性和现实性，为验证器开发者改进工具针对真实软件的适用性提供了参考。

Abstract: The SV-COMP competition provides a state-of-the-art platform for evaluating software verification tools on a standardized set of verification tasks. Consequently, verifier development outcomes are influenced by the composition of program benchmarks included in SV-COMP. When expanding this benchmark corpus, it is crucial to consider whether newly added programs cause verifiers to exhibit behavior distinct from that observed on existing benchmarks. Doing so helps mitigate external threats to the validity of the competition's results.
  In this paper, we present the application of the ARG-V tool for automatically generating Java verification benchmarks in the SV-COMP format. We demonstrate that, on a newly generated set of 68 realistic benchmarks, all four leading Java verifiers decrease in accuracy and recall compared to their performance on the existing benchmark suite. These findings highlight the potential of ARG-V to enhance the comprehensiveness and realism of verification tool evaluation, while also providing a roadmap for verifier developers aiming to improve their tools' applicability to real-world software.

</details>


### [89] [Beyond the Control Equations: An Artifact Study of Implementation Quality in Robot Control Software](https://arxiv.org/abs/2602.04799)
*Nils Chur,Thorsten Berger,Einar Broch Johnsen,Andrzej Wąsowski*

Main category: cs.SE

TL;DR: 本文分析了184个开源机器人控制器的软件实现，揭示了离散化处理不当、实时性问题及测试方法薄弱，呼吁改进实现规范和验证技术确保控制器可靠性。


<details>
  <summary>Details</summary>
Motivation: 尽管理论上控制器设计有安全保证，但实际软件实现中的复杂性及离散执行常被忽视，导致理论保障难以完全兑现。

Method: 调查分析184个开源机器人控制器的实际软件实现，考察其应用背景、实现细节及用于保证正确性的测试方法。

Result: 发现控制器实现多采用临时性离散化处理，存在实时性问题、错误处理不足及缺乏系统化理论验证，测试实践普遍较为表面。

Conclusion: 机器人控制器的软件实现需要更严格的规范和验证方法，以确保控制器在实际中的安全性和可靠性。

Abstract: A controller -- a software module managing hardware behavior -- is a key component of a typical robot system. While control theory gives safety guarantees for standard controller designs, the practical implementation of controllers in software introduces complexities that are often overlooked. Controllers are often designed in continuous space, while the software is executed in discrete space, undermining some of the theoretical guarantees. Despite extensive research on control theory and control modeling, little attention has been paid to the implementations of controllers and how their theoretical guarantees are ensured in real-world software systems. We investigate 184 real-world controller implementations in open-source robot software. We examine their application context, the implementation characteristics, and the testing methods employed to ensure correctness. We find that the implementations often handle discretization in an ad hoc manner, leading to potential issues with real-time reliability. Challenges such as timing inconsistencies, lack of proper error handling, and inadequate consideration of real-time constraints further complicate matters. Testing practices are superficial, no systematic verification of theoretical guarantees is used, leaving possible inconsistencies between expected and actual behavior. Our findings highlight the need for improved implementation guidelines and rigorous verification techniques to ensure the reliability and safety of robotic controllers in practice.

</details>


### [90] [Do Developers Read Type Information? An Eye-Tracking Study on TypeScript](https://arxiv.org/abs/2602.04824)
*Samuel W. Flint,Robert Dyer,Bonita Sharif*

Main category: cs.SE

TL;DR: 本文通过眼动追踪研究验证了开发者在代码理解和定位缺陷时并不会特别依赖类型注解作为文档。


<details>
  <summary>Details</summary>
Motivation: 静态类型注解被认为能帮助开发者，这可能源于它们作为代码内文档的作用。研究旨在验证开发者是否真的利用类型注解作为文档。

Method: 采用眼动追踪技术，调查26名本科生在TypeScript代码理解和错误定位任务中是否会阅读类型注解。

Result: 结果显示，在代码总结和错误定位任务中，开发者并不会更频繁地关注包含类型注解的行或类型声明。

Conclusion: 开发者使用类型注解的习惯与预期不同，工具设计、开发规范和教育应重视类型信息的易获取性和阅读习惯的培养。

Abstract: Statically-annotated types have been shown to aid developers in a number of programming tasks, and this benefit holds true even when static type checking is not used. It is hypothesized that this is because developers use type annotations as in-code documentation. In this study, we aim to provide evidence that developers use type annotations as in-code documentation. Understanding this hypothesized use will help to understand how, and in what contexts, developers use type information; additionally, it may help to design better development tools and inform educational decisions. To provide this evidence, we conduct an eye tracking study with 26 undergraduate students to determine if they read type annotations during code comprehension and bug localization in the TypeScript language. We found that developers do not look directly at lines containing type annotations or type declarations more often when they are present, in either code summarization or bug localization tasks. The results have implications for tool builders to improve the availability of type information, the development community to build good standards for use of type annotations, and education to enforce deliberate teaching of reading patterns.

</details>


### [91] [When Code Becomes Abundant: Redefining Software Engineering Around Orchestration and Verification](https://arxiv.org/abs/2602.04830)
*Karina Kohl,Luigi Carro*

Main category: cs.SE

TL;DR: 软件工程需转向关注人与自动化协作中的识别和意图表达、架构控制与验证，传统的代码构建方法已不再充分。


<details>
  <summary>Details</summary>
Motivation: 当前面临AI自动化降低代码生产成本与硬件能耗限制导致故障成本上升的双重压力，软件工程必须重新定义自身的核心内容。

Method: 提出将软件工程的焦点从代码构建和过程管理转向人类辨别意图的表达、架构控制及系统验证，强调自动化环境中人的判断作用。

Result: 这一转变强调了责任归属问题成为中心风险，并指出研究方向、教育课程和工业实践都需进行根本性调整。

Conclusion: 传统软件工程定义已不适应新环境，需要重塑以侧重于人类意图的表达及系统层面的控制与验证，改变研究与实践的核心理念。

Abstract: Software Engineering (SE) faces simultaneous pressure from AI automation (reducing code production costs) and hardware-energy constraints (amplifying failure costs). We position that SE must redefine itself around human discernment-intent articulation, architectural control, and verification-rather than code construction. This shift introduces accountability collapse as a central risk and requires fundamental changes to research priorities, educational curricula, and industrial practices. We argue that Software Engineering, as traditionally defined around code construction and process management, is no longer sufficient. Instead, the discipline must be redefined around intent articulation, architectural control, and systematic verification. This redefinition shifts Software Engineering from a production-oriented field to one centered on human judgment under automation, with profound implications for research, practice, and education.

</details>
