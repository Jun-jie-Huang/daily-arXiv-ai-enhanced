<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 85]
- [cs.MA](#cs.MA) [Total: 12]
- [cs.SE](#cs.SE) [Total: 21]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Multimodal Consistency-Guided Reference-Free Data Selection for ASR Accent Adaptation](https://arxiv.org/abs/2602.13263)
*Ligong Lei,Wenwen Lu,Xudong Pang,Zaokere Kadeer,Aishan Wumaier*

Main category: cs.CL

TL;DR: 该论文提出了一种多模态一致性引导的无参考数据选择流水线，用于自动语音识别（ASR）中的口音适应，有效提升了伪标签质量，减少了手工标注成本。


<details>
  <summary>Details</summary>
Motivation: 口音变化导致ASR性能下降，训练数据与口音存在匹配偏差，标注口音数据成本高昂，且传统基于文本的伪标签选择策略无法保证声学一致性，容易放大错误。

Method: 提出了一个基于子模互信息的目标感知预选步骤，之后通过扰动解码生成多条伪转录，并利用语音-文本对齐的嵌入空间一致性和预测词错误率作为无参考评分信号，结合分位点筛选规则保留可靠伪标签。

Result: 在单领域下，从3万条数据中选取约1500条达到10.91%的WER，接近使用3万有监督标签的10.45%；跨领域实验表明该方法避免了强口音条件下未经筛选伪标签带来的性能下降，且优于随机采样和现有选择方法。

Conclusion: 该方法通过多模态一致性评分和简单的分位点选择策略，能够在无监督条件下筛选出高质量的伪标签，显著提升口音适应下的ASR性能，接近有监督学习的效果；同时在跨领域和强口音转移环境下也表现出优越性。

Abstract: Automatic speech recognition (ASR) systems often degrade on accented speech because acoustic-phonetic and prosodic shifts induce a mismatch to training data, making labeled accent adaptation costly. However, common pseudo-label selection heuristics are largely text-centric (e.g., perplexity (PPL) filtering) and can prefer fluent yet acoustically mismatched hypotheses, leading to error amplification when fine-tuning. To address this, we introduce a multimodal consistency-guided, reference-free data selection pipeline for ASR accent adaptation under a transductive, label-free protocol. The pipeline starts with a target-aware preselection step based on submodular mutual information to improve query relevance and reduce downstream computation. It then generates multiple pseudo-transcriptions per utterance via perturbation-based decoding and scores each hypothesis using two reference-free signals: speech--text alignment in a shared embedding space and predicted word error rate (WER). A simple percentile-based selection rule retains reliable pseudo-labels for fine-tuning while discarding noisy utterances. In an in-domain setting, selecting ~1.5k utterances from a 30k pool achieves 10.91% WER, close to 10.45% obtained using 30k supervised labels. In a cross-domain setting with a mismatched candidate pool, consistency-filtered subsets avoid the degradation caused by unfiltered pseudo-labels under strong accent shift, and matched-hour experiments on a stronger ASR backbone further confirm gains over random sampling and recent selection baselines.

</details>


### [2] [LLM-Powered Automatic Translation and Urgency in Crisis Scenarios](https://arxiv.org/abs/2602.13452)
*Belu Ticona,Antonis Anastasopoulos*

Main category: cs.CL

TL;DR: 本研究揭示了现有大型语言模型和机器翻译在危机沟通中传递紧迫性信息的不足，强调了开发危机感知评估框架的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型(LLMs)和机器翻译系统在多语言危机沟通中的适用性尚未充分评估，特别是在保持信息紧迫性方面。

Method: 使用多语言危机数据及新引入的包含32种语言的紧迫性注释数据集，评估最新大型语言模型和专用翻译模型在危机领域的翻译表现。

Result: 结果显示，尽管一些翻译在语言上是准确的，但LLMs和翻译模型在传递信息紧迫性方面表现不稳定且有显著性能下降，且LLM基于语言不同，紧迫性分类差异较大。

Conclusion: 通用语言技术在危机沟通中的部署存在显著风险，需要建立专门关注危机情境的评估框架来提升系统的可靠性和适用性。

Abstract: Large language models (LLMs) are increasingly proposed for crisis preparedness and response, particularly for multilingual communication. However, their suitability for high-stakes crisis contexts remains insufficiently evaluated. This work examines the performance of state-of-the-art LLMs and machine translation systems in crisis-domain translation, with a focus on preserving urgency, which is a critical property for effective crisis communication and triaging. Using multilingual crisis data and a newly introduced urgency-annotated dataset covering over 32 languages, we show that both dedicated translation models and LLMs exhibit substantial performance degradation and instability. Crucially, even linguistically adequate translations can distort perceived urgency, and LLM-based urgency classifications vary widely depending on the language of the prompt and input. These findings highlight significant risks in deploying general-purpose language technologies for crisis communication and underscore the need for crisis-aware evaluation frameworks.

</details>


### [3] [Using Machine Learning to Enhance the Detection of Obfuscated Abusive Words in Swahili: A Focus on Child Safety](https://arxiv.org/abs/2602.13455)
*Phyllis Nabangi,Abdul-Jalil Zakaria,Jema David Ndibwile*

Main category: cs.CL

TL;DR: 本文研究了斯瓦希里语中使用机器学习检测网络欺凌中的隐晦辱骂语言，尽管模型在高维文本数据中表现良好，但受限于数据集小且不平衡。


<details>
  <summary>Details</summary>
Motivation: 数字技术兴起增加了网络欺凌的风险，特别是在低资源语言如斯瓦希里语中，亟需有效检测隐晦辱骂语言以保护儿童在线安全。

Method: 采用支持向量机、逻辑回归和决策树模型，并结合参数调优及SMOTE处理数据不平衡问题。

Result: 模型在处理高维文本数据时表现良好，但由于数据集规模小且不平衡，限制了结果的泛化能力，综合评价使用了精准率、召回率和F1分数。

Conclusion: 基于机器学习的方法能有效检测斯瓦希里语中的隐晦辱骂语言，但受限于数据规模和不平衡性，结果的泛化性有限，需扩展数据集并采用更先进技术提高系统效果。

Abstract: The rise of digital technology has dramatically increased the potential for cyberbullying and online abuse, necessitating enhanced measures for detection and prevention, especially among children. This study focuses on detecting abusive obfuscated language in Swahili, a low-resource language that poses unique challenges due to its limited linguistic resources and technological support. Swahili is chosen due to its popularity and being the most widely spoken language in Africa, with over 16 million native speakers and upwards of 100 million speakers in total, spanning regions in East Africa and some parts of the Middle East.
  We employed machine learning models including Support Vector Machines (SVM), Logistic Regression, and Decision Trees, optimized through rigorous parameter tuning and techniques like Synthetic Minority Over-sampling Technique (SMOTE) to handle data imbalance. Our analysis revealed that, while these models perform well in high-dimensional textual data, our dataset's small size and imbalance limit our findings' generalizability. Precision, recall, and F1 scores were thoroughly analyzed, highlighting the nuanced performance of each model in detecting obfuscated language.
  This research contributes to the broader discourse on ensuring safer online environments for children, advocating for expanded datasets and advanced machine-learning techniques to improve the effectiveness of cyberbullying detection systems. Future work will focus on enhancing data robustness, exploring transfer learning, and integrating multimodal data to create more comprehensive and culturally sensitive detection mechanisms.

</details>


### [4] [Language Model Memory and Memory Models for Language](https://arxiv.org/abs/2602.13466)
*Benjamin L. Badger*

Main category: cs.CL

TL;DR: 语言模型嵌入信息含量较低，自编码器嵌入记忆效果好。提出并行编码器-解码器记忆模型，通过结合训练目标实现信息丰富记忆，提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型嵌入记忆能力不足，且仅基于下一令牌预测训练导致信息丢失，亟需设计有效记忆形成机制以提升计算效率和信息保留。

Method: 提出并行化编码器-解码器结构，结合因果训练与信息保留目标函数训练，同时采用冻结编码器与循序渐进解码器训练策略。

Result: 本文发现语言模型的隐藏层向量（嵌入）通常包含较少的输入信息，而自编码器训练得到的嵌入能几乎完美地记忆输入。通过替换记忆嵌入代替令牌序列，可以显著提高计算效率，因此提出了一种并行化编码器-解码器记忆模型。在因果训练下，此模型生成的信息较少，但结合因果和信息保留目标函数训练后，能形成信息丰富的记忆。训练过程可通过冻结高保真编码器及循序渐进训练简化。该研究指出，仅靠下一令牌预测训练不适合准确记忆生成，因为目标函数不可逆，因而需要结合多目标训练。

Conclusion: 仅依靠下一令牌预测训练不足以形成准确记忆，结合因果和信息保留目标函数能有效提升模型记忆能力和效率。

Abstract: The ability of machine learning models to store input information in hidden layer vector embeddings, analogous to the concept of `memory', is widely employed but not well characterized. We find that language model embeddings typically contain relatively little input information regardless of data and compute scale during training. In contrast, embeddings from autoencoders trained for input regeneration are capable of nearly perfect memory formation. The substitution of memory embeddings for token sequences leads to substantial computational efficiencies, motivating the introduction of a parallelizable encoder-decoder memory model architecture. Upon causal training these models contain information-poor embeddings incapable of arbitrary information access, but by combining causal and information retention objective functions they learn to form and decode information-rich memories. Training can be further streamlined by freezing a high fidelity encoder followed by a curriculum training approach where decoders first learn to process memories and then learn to additionally predict next tokens. We introduce the perspective that next token prediction training alone is poorly suited for accurate memory formation as the objective itself is non-invertible, motivating the use of combined objective functions for models where the entire input is not exposed.

</details>


### [5] [From Perceptions To Evidence: Detecting AI-Generated Content In Turkish News Media With A Fine-Tuned Bert Classifier](https://arxiv.org/abs/2602.13504)
*Ozancan Ozdemir*

Main category: cs.CL

TL;DR: 本研究利用土耳其语BERT模型，首次实证量化了土耳其新闻媒体中AI重写内容的比例，发现约2.5%的新闻受大型语言模型影响。


<details>
  <summary>Details</summary>
Motivation: 土耳其新闻媒体中AI生成内容的实际使用尚无实证研究，现有研究多限于定性采访和假新闻检测，亟需量化分析。

Method: 通过微调针对土耳其语的BERT模型，利用来自三大土耳其新闻机构共3600篇标注文章进行二分类，识别AI重写内容，并在2023至2026年超过3500篇未见文章中进行部署验证。

Result: 模型在测试集上取得0.9708的F1分数，预测置信度平均超过0.96，估计约2.5%的新闻内容被大型语言模型重写或修改。

Conclusion: 本研究实现了土耳其新闻媒体AI使用的首个实证数据驱动测量，超越了以往依赖记者自述的调查，为理解AI内容产生提供了实证基础。

Abstract: The rapid integration of large language models into newsroom workflows has raised urgent questions about the prevalence of AI-generated content in online media. While computational studies have begun to quantify this phenomenon in English-language outlets, no empirical investigation exists for Turkish news media, where existing research remains limited to qualitative interviews with journalists or fake news detection. This study addresses that gap by fine-tuning a Turkish-specific BERT model (dbmdz/bert-base-turkish-cased) on a labeled dataset of 3,600 articles from three major Turkish outlets with distinct editorial orientations for binary classification of AI-rewritten content. The model achieves 0.9708 F1 score on the held-out test set with symmetric precision and recall across both classes. Subsequent deployment on over 3,500 unseen articles spanning between 2023 and 2026 reveals consistent cross-source and temporally stable classification patterns, with mean prediction confidence exceeding 0.96 and an estimated 2.5 percentage of examined news content rewritten or revised by LLMs on average. To the best of our knowledge, this is the first study to move beyond self-reported journalist perceptions toward empirical, data-driven measurement of AI usage in Turkish news media.

</details>


### [6] [Think Deep, Not Just Long: Measuring LLM Reasoning Effort via Deep-Thinking Tokens](https://arxiv.org/abs/2602.13517)
*Wei-Lin Chen,Liqian Peng,Tian Tan,Chao Zhao,Blake JianHang Chen,Ziqian Lin,Alec Go,Yu Meng*

Main category: cs.CL

TL;DR: 本文通过识别模型内部深度调整的“深度思考”代币，发现其比例与推理准确性高度相关，基于此提出Think@n策略，实现精准高效推理。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型通过增加推理时长来提升推理能力，但生成长度与准确性不总是正相关，过长反而可能导致性能下降，即“过度思考”。

Method: 提出测量推理时“深度思考”代币的方法，即识别在模型深层预测显著修正的代币，计算深度思考比率，并基于此设计Think@n策略，在推理时优先处理深度思考比例高的样本。

Result: 在四个数学与科学基准及多种推理模型上，深度思考比率与准确性呈稳定正相关，且优于长度和置信度指标。Think@n在保证或超越自洽方法性能的同时，显著降低推理计算成本。

Conclusion: 深度思考代币比例是衡量推理质量的有效指标，基于该指标的推理时调控策略可以提升性能并降低计算开销。

Abstract: Large language models (LLMs) have demonstrated impressive reasoning capabilities by scaling test-time compute via long Chain-of-Thought (CoT). However, recent findings suggest that raw token counts are unreliable proxies for reasoning quality: increased generation length does not consistently correlate with accuracy and may instead signal "overthinking," leading to performance degradation. In this work, we quantify inference-time effort by identifying deep-thinking tokens -- tokens where internal predictions undergo significant revisions in deeper model layers prior to convergence. Across four challenging mathematical and scientific benchmarks (AIME 24/25, HMMT 25, and GPQA-diamond) and a diverse set of reasoning-focused models (GPT-OSS, DeepSeek-R1, and Qwen3), we show that deep-thinking ratio (the proportion of deep-thinking tokens in a generated sequence) exhibits a robust and consistently positive correlation with accuracy, substantially outperforming both length-based and confidence-based baselines. Leveraging this insight, we introduce Think@n, a test-time scaling strategy that prioritizes samples with high deep-thinking ratios. We demonstrate that Think@n matches or exceeds standard self-consistency performance while significantly reducing inference costs by enabling the early rejection of unpromising generations based on short prefixes.

</details>


### [7] [On Calibration of Large Language Models: From Response To Capability](https://arxiv.org/abs/2602.13540)
*Sin-Han Yang,Cheng-Kuang Wu,Chieh-Yen Lin,Yun-Nung Chen,Hung-yi Lee,Shao-Hua Sun*

Main category: cs.CL

TL;DR: 本文提出能力校准方法，以更准确地估计大语言模型在处理查询时的总体解决能力，提升模型置信度评估的实用性。


<details>
  <summary>Details</summary>
Motivation: 传统响应级置信度评估与实际应用中关注模型整体解决问题能力不匹配，需提出更符合实际的校准方法。

Method: 通过区分能力校准和响应校准，构建实证评估框架，比较多种置信度估计方法，验证能力校准的有效性。

Result: 能力校准显著提升了多响应情况下的预测准确率（pass@$k$），并优化了推理预算分配，具备广泛应用潜力。

Conclusion: 能力校准区别于传统的响应级校准，能够更准确反映模型在查询上的整体能力，显著提升置信度预测效果和推理资源分配效率。

Abstract: Large language models (LLMs) are widely deployed as general-purpose problem solvers, making accurate confidence estimation critical for reliable use. Prior work on LLM calibration largely focuses on response-level confidence, which estimates the correctness of a single generated output. However, this formulation is misaligned with many practical settings where the central question is how likely a model is to solve a query overall. We show that this mismatch results from the stochastic nature of modern LLM decoding, under which single-response correctness fails to reflect underlying model capability. To address this issue, we introduce capability calibration, which targets the model's expected accuracy on a query. We formally distinguish capability calibration from response calibration and show that the two differ both theoretically and empirically. We establish an empirical evaluation setup and study a range of confidence estimation methods. Our results demonstrate that capability-calibrated confidence improves pass@$k$ prediction and inference budget allocation, establishing a foundation with potential for diverse applications.

</details>


### [8] [Small Reward Models via Backward Inference](https://arxiv.org/abs/2602.13551)
*Yike Wang,Faeze Brahman,Shangbin Feng,Teng Xiao,Hannaneh Hajishirzi,Yulia Tsvetkov*

Main category: cs.CL

TL;DR: 提出FLIP，一种无参考无评分标准的奖励建模方法，通过逆向推理指令提升小型语言模型的奖励评估效果，性能较现有方法大幅提升。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型依赖大型模型强推理能力或需参考答案和评分标准，限制了方法的灵活性和普适性，本文旨在提出一种无需参考答案和评分标准的新型奖励建模方法。

Method: 通过逆向推理推断最可能生成指定回答的指令，将推断指令与原始指令的相似度作为奖励信号，实现无参考无评分标准的奖励建模。

Result: 本文提出了FLIP（逆向推理提示重构）方法，针对奖励模型在无参考、无评分标准情况下的构建问题，通过推断最可能产生给定回复的指令，并利用推断指令与原始指令的相似度作为奖励信号。实验证明，FLIP在多个领域和13个小型语言模型上，性能比传统大型语言模型评分方法提升约79.6%，且对长输出更有效，具有较强的抗作弊能力。其通过利用验证生成差距，实现了在小规模模型中可靠的奖励建模。

Conclusion: FLIP有效提升了奖励模型的准确性和鲁棒性，尤其在小规模语言模型和长文本生成场景表现出优势，解决了传统评价方法受限的问题。

Abstract: Reward models (RMs) play a central role throughout the language model (LM) pipeline, particularly in non-verifiable domains. However, the dominant LLM-as-a-Judge paradigm relies on the strong reasoning capabilities of large models, while alternative approaches require reference responses or explicit rubrics, limiting flexibility and broader accessibility. In this work, we propose FLIP (FLipped Inference for Prompt reconstruction), a reference-free and rubric-free reward modeling approach that reformulates reward modeling through backward inference: inferring the instruction that would most plausibly produce a given response. The similarity between the inferred and the original instructions is then used as the reward signal. Evaluations across four domains using 13 small language models show that FLIP outperforms LLM-as-a-Judge baselines by an average of 79.6%. Moreover, FLIP substantially improves downstream performance in extrinsic evaluations under test-time scaling via parallel sampling and GRPO training. We further find that FLIP is particularly effective for longer outputs and robust to common forms of reward hacking. By explicitly exploiting the validation-generation gap, FLIP enables reliable reward modeling in downscaled regimes where judgment methods fail. Code available at https://github.com/yikee/FLIP.

</details>


### [9] [DistillLens: Symmetric Knowledge Distillation Through Logit Lens](https://arxiv.org/abs/2602.13567)
*Manish Dhakal,Uthman Jinadu,Anjila Budathoki,Rajshekhar Sunderraman,Yi Ding*

Main category: cs.CL

TL;DR: DistillLens通过对称对齐教师与学生模型的中间表示，提高了知识蒸馏效果，远超传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏多关注最终输出，忽视了教师模型中间层的思考过程，且现有特征蒸馏方法无法充分利用中间层的不确定性信息，需设计新的机制更好地捕获和传递教师模型的丰富信息。

Method: 通过Logit Lens将中间隐藏状态投影到词汇空间，使用对称散度目标函数对教师和学生模型的中间层输出进行结构对齐，避免过度或不足自信，保持信息高熵传递。

Result: 本文提出了DistillLens框架，通过对齐教师模型和学生模型的中间思考过程来改进大语言模型的知识蒸馏。该方法结合了Logit Lens技术，将隐藏状态映射到词汇空间，并采用对称散度目标实现结构对齐，有效防止模型过度自信或自信不足，保持关键信息的传递。实验证明，DistillLens在GPT-2和Llama模型上，优于传统的知识蒸馏和特征传递方法。

Conclusion: DistillLens框架通过对称散度约束，实现了学生和教师模型中间层表示的结构对齐，改善了蒸馏过程中的不确定性表达，显著提升了模型性能。

Abstract: Standard Knowledge Distillation (KD) compresses Large Language Models (LLMs) by optimizing final outputs, yet it typically treats the teacher's intermediate layer's thought process as a black box. While feature-based distillation attempts to bridge this gap, existing methods (e.g., MSE and asymmetric KL divergence) ignore the rich uncertainty profiles required for the final output. In this paper, we introduce DistillLens, a framework that symmetrically aligns the evolving thought processes of student and teacher models. By projecting intermediate hidden states into the vocabulary space via the Logit Lens, we enforce structural alignment using a symmetric divergence objective. Our analysis proves that this constraint imposes a dual-sided penalty, preventing both overconfidence and underconfidence while preserving the high-entropy information conduits essential for final deduction. Extensive experiments on GPT-2 and Llama architectures demonstrate that DistillLens consistently outperforms standard KD and feature-transfer baselines on diverse instruction-following benchmarks. The code is available at https://github.com/manishdhakal/DistillLens.

</details>


### [10] [LLM-Confidence Reranker: A Training-Free Approach for Enhancing Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2602.13571)
*Zhipeng Song,Xiangyu Kong,Xinrui Bao,Yizhi Zhou,Jiulong Jiao,Sitong Liu,Yuhang Zhou,Heng Qi*

Main category: cs.CL

TL;DR: 本文提出了一种利用大型语言模型置信度的无训练文档重排序算法LCR，显著提升检索增强生成系统性能，降低生成幻觉，效率高且易部署。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在知识密集型任务中易产生幻觉，影响应用效果。现有的重排序方法效果虽好但存在高计算成本和训练需求，且未充分利用LLM的内在置信度信息。

Method: 提出LLM-Confidence Reranker (LCR)，一种无训练、即插即用的算法，通过最大语义簇比例（MSCP）从黑盒LLM中获取置信度，采用多项式采样和聚类进行置信度评估，再依据置信度阈值进行分层排序，优化文档重排序。

Result: 在BEIR和TREC基准上，LCR使用7-9B参数预训练模型，提升常见指标NDCG@5达20.6%，且适配多种预训练及微调模型，无性能下降。消融实验验证了LLM置信度与文档相关性的正相关性，解释了LCR的有效性。

Conclusion: LCR方法极大提升了检索增强生成系统的文档重排序效果，具备计算效率高、可扩展性强及广泛兼容性，为减少大型语言模型的幻觉问题提供了有效途径，特别适用于医疗诊断等重要应用领域。

Abstract: Large language models (LLMs) have revolutionized natural language processing, yet hallucinations in knowledge-intensive tasks remain a critical challenge. Retrieval-augmented generation (RAG) addresses this by integrating external knowledge, but its efficacy depends on accurate document retrieval and ranking. Although existing rerankers demonstrate effectiveness, they frequently necessitate specialized training, impose substantial computational expenses, and fail to fully exploit the semantic capabilities of LLMs, particularly their inherent confidence signals. We propose the LLM-Confidence Reranker (LCR), a training-free, plug-and-play algorithm that enhances reranking in RAG systems by leveraging black-box LLM confidence derived from Maximum Semantic Cluster Proportion (MSCP). LCR employs a two-stage process: confidence assessment via multinomial sampling and clustering, followed by binning and multi-level sorting based on query and document confidence thresholds. This approach prioritizes relevant documents while preserving original rankings for high-confidence queries, ensuring robustness. Evaluated on BEIR and TREC benchmarks with BM25 and Contriever retrievers, LCR--using only 7--9B-parameter pre-trained LLMs--consistently improves NDCG@5 by up to 20.6% across pre-trained LLM and fine-tuned Transformer rerankers, without degradation. Ablation studies validate the hypothesis that LLM confidence positively correlates with document relevance, elucidating LCR's mechanism. LCR offers computational efficiency, parallelism for scalability, and broad compatibility, mitigating hallucinations in applications like medical diagnosis.

</details>


### [11] [Elo-Evolve: A Co-evolutionary Framework for Language Model Alignment](https://arxiv.org/abs/2602.13575)
*Jing Zhao,Ting Zhen,Junwei bao,Hongfei Jiang,Yang song*

Main category: cs.CL

TL;DR: 提出Elo-Evolve动态多智能体竞争框架，利用二元比较和Elo对手选择，解决传统绝对评分对齐方法的限制，显著提升大型语言模型对齐性能。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的对齐方法依赖于将大量人类偏好数据压缩为静态、绝对的奖励函数，导致数据稀缺、噪声敏感和训练不稳定等问题。

Method: 提出了Elo-Evolve框架，将对齐定义为自适应对手池中动态的多智能体竞争。方法创新包括：1）通过二元胜负结果学习，消除对Bradley-Terry模型的依赖；2）使用Elo分数控制的对手选择实现自动课程学习。

Result: 理论上基于PAC学习证明，二元比较在样本复杂度上优于绝对评分方法；实验上相较于绝对评分噪声减少4.5倍。用该框架训练Qwen2.5-7B模型，在Alpaca Eval 2.0和MT-Bench评估中表现优于基于点数的静态训练方法。

Conclusion: Elo-Evolve通过动态对手选择和二元比较显著提升了大型语言模型的对齐效果，验证了该方法在对齐训练中的渐进优势。

Abstract: Current alignment methods for Large Language Models (LLMs) rely on compressing vast amounts of human preference data into static, absolute reward functions, leading to data scarcity, noise sensitivity, and training instability. We introduce Elo-Evolve, a co-evolutionary framework that redefines alignment as dynamic multi-agent competition within an adaptive opponent pool. Our approach makes two key innovations: (1) eliminating Bradley-Terry model dependencies by learning directly from binary win/loss outcomes in pairwise competitions, and (2) implementing Elo-orchestrated opponent selection that provides automatic curriculum learning through temperature-controlled sampling. We ground our approach in PAC learning theory, demonstrating that pairwise comparison achieves superior sample complexity and empirically validate a 4.5x noise reduction compared to absolute scoring approaches. Experimentally, we train a Qwen2.5-7B model using our framework with opponents including Qwen2.5-14B, Qwen2.5-32B, and Qwen3-8B models. Results demonstrate a clear performance hierarchy: point-based methods < static pairwise training < Elo-Evolve across Alpaca Eval 2.0 and MT-Bench, validating the progressive benefits of pairwise comparison and dynamic opponent selection for LLM alignment.

</details>


### [12] [Metaphors' journeys across time and genre: tracking the evolution of literary metaphors with temporal embeddings](https://arxiv.org/abs/2602.13701)
*Veronica Mangiaterra,Chiara Barattieri di San Pietro,Paolo Canal,Valentina Bambini*

Main category: cs.CL

TL;DR: 该研究利用历时语义模型分析了19至21世纪意大利文学隐喻处理难度的变化，发现隐喻难度体裁依赖，现代文学隐喻更难，网络语言隐喻更易，反映语言风格和语义特征的变化。


<details>
  <summary>Details</summary>
Motivation: 现有研究较少考虑文学隐喻的时间维度，而许多文学隐喻的创造时间与现代读者存在时间差，本研究旨在填补这一空白，揭示隐喻处理难度随时间及体裁的变化规律。

Method: 采用历时分布式语义学技术，在19和21世纪的意大利文学及非文学语料库上训练词嵌入，利用515个19世纪文学隐喻中主题与载体的语义相似度模拟隐喻处理需求。

Result: 本文通过历时分布式语义学方法，分析了意大利19世纪与21世纪文学和非文学语料中隐喻的语义相似性变化，探讨了文学隐喻处理难度随时间和体裁的变化。研究发现总体隐喻处理难度稳定，但现代文学隐喻较难，现代非文学隐喻较易，且隐喻词汇的语义特征对处理难度有影响。研究支持意大利语言的风格简化和网络语言的创造性使隐喻处理负担不同。

Conclusion: 隐喻处理难度随着体裁和时间发生变化，现代文学中的隐喻处理更困难，而现代非文学文本中隐喻更易理解，这与语言风格简化和网络语言的高创造性有关。

Abstract: Metaphors are a distinctive feature of literary language, yet they remain less studied experimentally than everyday metaphors. Moreover, previous psycholinguistic and computational approaches overlooked the temporal dimension, although many literary metaphors were coined centuries apart from contemporary readers. This study innovatively applies tools from diachronic distributional semantics to assess whether the processing costs of literary metaphors varied over time and genre. Specifically, we trained word embeddings on literary and nonliterary Italian corpora from the 19th and 21st centuries, for a total of 124 million tokens, and modeled changes in the semantic similarity between topics and vehicles of 515 19th-century literary metaphors, taking this measure as a proxy of metaphor processing demands. Overall, semantic similarity, and hence metaphor processing demands, remained stable over time. However, genre played a key role: metaphors appeared more difficult (i.e., lower topic-vehicle similarity) in modern literary contexts than in 19th-century literature, but easier (i.e., higher topic-vehicle similarity) in today's nonliterary language (e.g., the Web) than in 19th-century nonliterary texts. This pattern was further shaped by semantic features of metaphors' individual terms, such as vector coherence and semantic neighborhood density. Collectively, these findings align with broader linguistic changes in Italian, such as the stylistic simplification of modern literature, which may have increased metaphor processing demands, and the high creativity of the Web's language, which seems to render metaphor more accessible.

</details>


### [13] [On Theoretically-Driven LLM Agents for Multi-Dimensional Discourse Analysis](https://arxiv.org/abs/2602.13713)
*Maciej Uberna,Michał Wawer,Jarosław A. Chudziak,Marcin Koszowy*

Main category: cs.CL

TL;DR: 通过理论知识增强的多智能体框架显著提升了对论述中重述功能的检测能力，推动从简单的意译识别向功能感知的论述分析迈进。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型虽能检测表面相似度，但难以捕捉重述的语用功能及其修辞作用，亟需结合理论知识提升分析能力。

Method: 提出了比较多智能体框架，利用增强检索生成（RAG）的方式将论证理论融入大型语言模型，以识别论述中重述的策略功能。

Result: 基于带注释的政治辩论数据集，构建四种重述功能类别（减弱、强化、具体化、泛化及其他），实验证明融合论证理论的RAG增强模型在各方面表现显著优于零-shot基线，宏F1分数提升近30%。

Conclusion: 理论基础对识别论述中重述的策略功能至关重要，本文提出的多智能体框架为构建可扩展的、理论支持的修辞策略识别工具奠定了基础。

Abstract: Identifying the strategic uses of reformulation in discourse remains a key challenge for computational argumentation. While LLMs can detect surface-level similarity, they often fail to capture the pragmatic functions of rephrasing, such as its role within rhetorical discourse. This paper presents a comparative multi-agent framework designed to quantify the benefits of incorporating explicit theoretical knowledge for this task. We utilise an dataset of annotated political debates to establish a new standard encompassing four distinct rephrase functions: Deintensification, Intensification, Specification, Generalisation, and Other, which covers all remaining types (D-I-S-G-O). We then evaluate two parallel LLM-based agent systems: one enhanced by argumentation theory via Retrieval-Augmented Generation (RAG), and an identical zero-shot baseline. The results reveal a clear performance gap: the RAG-enhanced agents substantially outperform the baseline across the board, with particularly strong advantages in detecting Intensification and Generalisation context, yielding an overall Macro F1-score improvement of nearly 30\%. Our findings provide evidence that theoretical grounding is not only beneficial but essential for advancing beyond mere paraphrase detection towards function-aware analysis of argumentative discourse. This comparative multi-agent architecture represents a step towards scalable, theoretically informed computational tools capable of identifying rhetorical strategies in contemporary discourse.

</details>


### [14] [A Multi-Agent Framework for Medical AI: Leveraging Fine-Tuned GPT, LLaMA, and DeepSeek R1 for Evidence-Based and Bias-Aware Clinical Query Processing](https://arxiv.org/abs/2602.14158)
*Naeimeh Nourmohammadi,Md Meem Hossain,The Anh Han,Safina Showkat Ara,Zia Ush Shamszaman*

Main category: cs.CL

TL;DR: 该论文提出了一种结合多代理的大型语言模型医疗问答框架，通过证据检索、不确定性估计和偏见检测，提高回答的可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在医疗问答中存在验证弱、证据不足和置信信号不可靠等问题，限制了临床应用。

Method: 首先微调三个代表性LLM（GPT，LLaMA，DeepSeek R1）在MedQuAD医疗问答数据上；其次构建多代理流水线，包括临床推理代理、证据检索代理和回答优化代理，并引入不确定性和偏见检测机制。

Result: DeepSeek R1在生成质量上表现最佳，系统整体达到87%准确率，相关性约0.80，证据增强有效降低不确定性，端到端延迟36.5秒。

Conclusion: 多代理专门化和验证层能够缓解单一模型的局限性，为基于证据和偏见意识的医疗AI提供实用且可扩展的设计。

Abstract: Large language models (LLMs) show promise for healthcare question answering, but clinical use is limited by weak verification, insufficient evidence grounding, and unreliable confidence signalling. We propose a multi-agent medical QA framework that combines complementary LLMs with evidence retrieval, uncertainty estimation, and bias checks to improve answer reliability. Our approach has two phases. First, we fine-tune three representative LLM families (GPT, LLaMA, and DeepSeek R1) on MedQuAD-derived medical QA data (20k+ question-answer pairs across multiple NIH domains) and benchmark generation quality. DeepSeek R1 achieves the strongest scores (ROUGE-1 0.536 +- 0.04; ROUGE-2 0.226 +-0.03; BLEU 0.098 -+ 0.018) and substantially outperforms the specialised biomedical baseline BioGPT in zero-shot evaluation. Second, we implement a modular multi-agent pipeline in which a Clinical Reasoning agent (fine-tuned LLaMA) produces structured explanations, an Evidence Retrieval agent queries PubMed to ground responses in recent literature, and a Refinement agent (DeepSeek R1) improves clarity and factual consistency; an optional human validation path is triggered for high-risk or high-uncertainty cases. Safety mechanisms include Monte Carlo dropout and perplexity-based uncertainty scoring, plus lexical and sentiment-based bias detection supported by LIME/SHAP-based analyses. In evaluation, the full system achieves 87% accuracy with relevance around 0.80, and evidence augmentation reduces uncertainty (perplexity 4.13) compared to base responses, with mean end-to-end latency of 36.5 seconds under the reported configuration. Overall, the results indicate that agent specialisation and verification layers can mitigate key single-model limitations and provide a practical, extensible design for evidence-based and bias-aware medical AI.

</details>


### [15] [RMPL: Relation-aware Multi-task Progressive Learning with Stage-wise Training for Multimedia Event Extraction](https://arxiv.org/abs/2602.13748)
*Yongkang Jin,Jianwen Luo,Jingjing Wang,Jianmin Yao,Yu Hong*

Main category: cs.CL

TL;DR: 提出RMPL框架，通过多任务渐进训练整合多模态数据，提升了低资源条件下多媒体事件抽取性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态事件抽取方法因缺乏标注训练数据，难以有效学习结构化事件表示，导致事件及参数识别效果较差，需一种能在低资源条件下有效利用异质监督的方法。

Method: 采用基于统一schema的阶段性多任务学习，先训练跨模态事件表示，再微调事件检测和参数角色抽取，结合单模态及多模态关系抽取的监督信号。

Result: 本文提出了一种名为RMPL的多任务渐进学习框架，用于解决多模态事件抽取（MEE）中的低资源训练问题。该方法通过阶段性训练，融合单模态事件抽取和多媒体关系抽取的异质监督，先学习跨模态共享的事件中心表示，再微调事件检测与参数角色抽取。实验在M2E2基准上，结合多种视觉语言模型，验证了该方法在不同模态设置下的显著提升。

Conclusion: RMPL有效融合了异质监督信息，增强了事件表示学习，在M2E2基准测试中显著提升了多模态事件抽取的准确性。

Abstract: Multimedia Event Extraction (MEE) aims to identify events and their arguments from documents that contain both text and images. It requires grounding event semantics across different modalities. Progress in MEE is limited by the lack of annotated training data. M2E2 is the only established benchmark, but it provides annotations only for evaluation. This makes direct supervised training impractical. Existing methods mainly rely on cross-modal alignment or inference-time prompting with Vision--Language Models (VLMs). These approaches do not explicitly learn structured event representations and often produce weak argument grounding in multimodal settings. To address these limitations, we propose RMPL, a Relation-aware Multi-task Progressive Learning framework for MEE under low-resource conditions. RMPL incorporates heterogeneous supervision from unimodal event extraction and multimedia relation extraction with stage-wise training. The model is first trained with a unified schema to learn shared event-centric representations across modalities. It is then fine-tuned for event mention identification and argument role extraction using mixed textual and visual data. Experiments on the M2E2 benchmark with multiple VLMs show consistent improvements across different modality settings.

</details>


### [16] [How Do Lexical Senses Correspond Between Spoken German and German Sign Language?](https://arxiv.org/abs/2602.13790)
*Melis Çelikkol,Wei Zhao*

Main category: cs.CL

TL;DR: 该研究通过人工标注构建了德语与德国手语的词义与符号映射数据集，提出并验证了基于语义相似度的计算方法，显著提升了多义词的词义-手语对应识别效果。


<details>
  <summary>Details</summary>
Motivation: 当前的手语词典未能充分表示同形多义词和同音异义词在不同语境下对应的不同手语符号，存在词义与手语符号映射不足的问题。

Method: 人工标注32个德语词及其在使用图中的词义对应的49个德国手语符号，实现了1,404个词用法到手语符号ID的映射。评估了精确匹配和基于SBERT语义嵌入的语义相似度两种计算方法。

Result: 构建了德语与德国手语的跨模态词义对应的首个标注数据集，并通过SBERT语义相似度方法有效识别多种词义与手语符号的对应类型，相较传统的精确匹配方法显著提升了识别准确率。

Conclusion: 基于语义相似度的计算方法优于精确匹配，能够有效识别跨模态词义对应关系，有助于丰富手语词典资源。首个数据集和代码公开，为后续研究提供基础。

Abstract: Sign language lexicographers construct bilingual dictionaries by establishing word-to-sign mappings, where polysemous and homonymous words corresponding to different signs across contexts are often underrepresented. A usage-based approach examining how word senses map to signs can identify such novel mappings absent from current dictionaries, enriching lexicographic resources. We address this by analyzing German and German Sign Language (Deutsche Gebärdensprache, DGS), manually annotating 1,404 word use-to-sign ID mappings derived from 32 words from the German Word Usage Graph (D-WUG) and 49 signs from the Digital Dictionary of German Sign Language (DW-DGS). We identify three correspondence types: Type 1 (one-to-many), Type 2 (many-to-one), and Type 3 (one-to-one), plus No Match cases. We evaluate computational methods: Exact Match (EM) and Semantic Similarity (SS) using SBERT embeddings. SS substantially outperforms EM overall 88.52% vs. 71.31%), with dramatic gains for Type 1 (+52.1 pp). Our work establishes the first annotated dataset for cross-modal sense correspondence and reveals which correspondence patterns are computationally identifiable. Our code and dataset are made publicly available.

</details>


### [17] [OMGs: A multi-agent system supporting MDT decision-making across the ovarian tumour care continuum](https://arxiv.org/abs/2602.13793)
*Yangyang Zhang,Zilong Wang,Jianbo Xu,Yongqi Chen,Chu Han,Zhihao Zhang,Shuai Liu,Hui Li,Huiping Zhang,Ziqi Liu,Jiaxin Chen,Jun Zhu,Zheng Feng,Hao Wen,Xingzhu Ju,Yanping Zhong,Yunqiu Zhang,Jie Duan,Jun Li,Dongsheng Li,Weijie Wang,Haiyan Zhu,Wei Jiang,Xiaohua Wu,Shuo Wang,Haiming Li,Qinhao Guo*

Main category: cs.CL

TL;DR: OMGs多代理AI系统有效模拟卵巢肿瘤MDT讨论，表现优异，可助力资源匮乏地区获取专家治疗建议。


<details>
  <summary>Details</summary>
Motivation: 卵巢肿瘤管理依赖多学科肿瘤委员会（MDT）讨论，以应对治疗复杂性和疾病异质性，但资源有限的中心缺乏及时的专家共识。

Method: 提出了OMGs（卵巢肿瘤多学科智能代理系统），一个多代理人工智能框架，通过领域特定代理协同整合多学科证据，生成带有透明理由的MDT式建议；同时开发SPEAR评估体系评价推荐质量。

Result: OMGs在多中心重新评估中表现与专家MDT共识相当，且证据评分更高；前瞻性多中心评估中与常规MDT决策高度一致；在人机配对研究中显著提升医生建议的证据性和稳健性。

Conclusion: 多代理讨论系统可达到专家MDT共识的水平，有潜力在资源有限环境中扩展专科肿瘤学专家的可及性。

Abstract: Ovarian tumour management has increasingly relied on multidisciplinary tumour board (MDT) deliberation to address treatment complexity and disease heterogeneity. However, most patients worldwide lack access to timely expert consensus, particularly in resource-constrained centres where MDT resources are scarce or unavailable. Here we present OMGs (Ovarian tumour Multidisciplinary intelligent aGent System), a multi-agent AI framework where domain-specific agents deliberate collaboratively to integrate multidisciplinary evidence and generate MDT-style recommendations with transparent rationales. To systematically evaluate MDT recommendation quality, we developed SPEAR (Safety, Personalization, Evidence, Actionability, Robustness) and validated OMGs across diverse clinical scenarios spanning the care continuum. In multicentre re-evaluation, OMGs achieved performance comparable to expert MDT consensus ($4.45 \pm 0.30$ versus $4.53 \pm 0.23$), with higher Evidence scores (4.57 versus 3.92). In prospective multicentre evaluation (59 patients), OMGs demonstrated high concordance with routine MDT decisions. Critically, in paired human-AI studies, OMGs most substantially enhanced clinicians' recommendations in Evidence and Robustness, the dimensions most compromised when multidisciplinary expertise is unavailable. These findings suggest that multi-agent deliberative systems can achieve performance comparable to expert MDT consensus, with potential to expand access to specialized oncology expertise in resource-limited settings.

</details>


### [18] [The acquisition of English irregular inflections by Yemeni L1 Arabic learners: A Universal Grammar approach](https://arxiv.org/abs/2602.13816)
*Muneef Y. Alsawsh,Mohammed Q. Shormani*

Main category: cs.CL

TL;DR: 也门英语学习者习得不规则词形变化受第一语言迁移及发展阶段影响，阶段性表现出普遍语法特征，教学和输入质量对学习成效关键。


<details>
  <summary>Details</summary>
Motivation: 探讨也门英语学习者作为第二语言学习者在英语不规则词形变化习得中的表现，基于普遍语法理论，特别是特征重组假说，分析第一语言迁移及第二语言发展影响。

Method: 利用普遍语法的特征重组假说，分析两阶段学习者错误，结合统计学方法评估学习进步。

Result: 第一阶段错误主要源于第一语言迁移，尤其是音韵和结构不匹配；第二阶段表现出对普遍语法特性的敏感及形态结构朝目标语言调整。错误来源包括跨语际和语内因素，过度泛化是常见策略。统计显示从第一阶段到第二阶段，有形成正确不规则词形的显著提升。

Conclusion: 第一语言迁移和第二语言发展因素影响初期习得，但完整的普遍语法访问依赖于充分的语言输入和有效教学。音变、零形态和-a复数形式等仍存在困难，提示需要改进教学和输入质量以促进特征重组。

Abstract: This study examines the acquisition of English irregular inflections by Yemeni learners of English as a second language (L2), utilizing a Universal Grammar (UG) approach. Within the UG approach, the study considers Feature Reassembly Hypothesis (FRH) (Lardiere, 2008, 2009) part of UG, focusing on the roles of first language (L1) transfer and L2 developmental influence. It analyzes learner errors across two developmental stages. Stage 1 data reveal a dominant influence of L1 transfer, particularly in phonological and structural mismatches, while stage 2 data demonstrate increased learner sensitivity to UG properties and morphological reconfiguration toward the target language. Findings reveal that errors in irregular inflectional morphology are attributed to both interlingual and intralingual sources, with overgeneralization of L2 rules as a common developmental strategy. Statistical analysis, including a one-way ANOVA, indicates significant improvement in the production of well-formed irregular inflections from stage 1 to stage 2, underscoring learners' continued access to UG. However, persistent difficulties with consonant change, zero-morpheme, and -a plural inflections suggest that limited exposure, ineffective input modeling, and insufficient instructional quality constrain full UG access. The study concludes that while L1 transfer and L2 developmental factors influence initial stages of acquisition, appropriate linguistic input and instruction are critical for facilitating UG-driven feature reassembly in adult L2 learners.

</details>


### [19] [Beyond Words: Evaluating and Bridging Epistemic Divergence in User-Agent Interaction via Theory of Mind](https://arxiv.org/abs/2602.13832)
*Minyuan Ruan,Ziyue Wang,Kaiming Liu,Yunghwei Lai,Peng Li,Yang Liu*

Main category: cs.CL

TL;DR: 针对大型语言模型难以准确理解用户意图的问题，本文引入理论心智机制，通过设计Benchname基准和新的数据集，并利用强化学习提升模型推理用户心理状态的能力，实现了更好的任务表现。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）在理解和响应用户真实需求时存在困难，尤其是在意图和指令表达不精确的情况下，导致用户主观认知和实际环境状态之间产生认知差异。

Method: 将理论心智（ToM）形式化为检测和解决认知差异的机制，提出了一个评估基准（benchname）用于测试模型如何调和用户信念和个人信息，并构建一个基于轨迹的ToM数据集，通过强化学习训练模型以改进对用户心理状态的推理能力。

Result: 在11个主流模型上的测试结果显示，当前模型在识别认知差异方面存在显著局限。经过强化学习训练后，模型在推理用户心理状态和下游任务表现上有持续提升。

Conclusion: 理论心智应被视为一种实用的交互机制，而非孤立的推理技能，对于提升大型语言模型理解和响应用户需求具有重要价值。

Abstract: Large Language Models (LLMs) have developed rapidly and are widely applied to both general-purpose and professional tasks to assist human users. However, they still struggle to comprehend and respond to the true user needs when intentions and instructions are imprecisely conveyed, leading to a divergence between subjective user believes and true environment states. Resolving this epistemic divergence requires Theory of Mind (ToM), yet existing ToM evaluations for LLMs primarily focus on isolated belief inference, overlooking its functional utility in real-world interaction. To this end, we formalize ToM for LLMs as a mechanism for epistemic divergence detection and resolution, and propose a benchmark, \benchname, to assess how models reconcile user beliefs and profiles in practice. Results across 11 leading models reveal a significant limitation to identify underlying cognitive gaps that impede task success. To bridge this gap, we further curate a trajectory-based ToM dataset linking belief tracking with task-related state inference. The model trained on this data via reinforcement learning shows consistent improvement in reasoning about user mental states, leading to enhanced downstream performance. Our work highlights the practical value of ToM as an essential interaction-level mechanism rather than as a standalone reasoning skill.

</details>


### [20] [Tool-Aware Planning in Contact Center AI: Evaluating LLMs through Lineage-Guided Query Decomposition](https://arxiv.org/abs/2602.14955)
*Varun Nathan,Shreyas Guha,Ayush Kumar*

Main category: cs.CL

TL;DR: 本文提出了一种针对呼叫中心工具感知计划生成的领域框架和基准，关注于将查询拆解为可执行步骤并利用结构化和非结构化工具进行处理。


<details>
  <summary>Details</summary>
Motivation: 呼叫中心数据分析查询需依赖多种结构化与非结构化工具协同执行，现有方法缺乏有效的工具感知计划生成与评估机制。

Method: 通过设计双模参考计划评估框架、迭代优化的数据策划方法，以及对14个不同大型语言模型进行大规模实证评测，分析模型的计划生成能力。

Result: 最佳模型的综合指标得分为84.8%，但顶级一次匹配率仅49.75%；计划谱系对模型性能影响不一，但可提升步骤执行力，且较短简洁计划更易被执行。

Conclusion: 现有大型语言模型在复杂查询和多步骤计划拆解方面存在显著不足，尤其在工具理解和使用完整性方面表现不佳。

Abstract: We present a domain-grounded framework and benchmark for tool-aware plan generation in contact centers, where answering a query for business insights, our target use case, requires decomposing it into executable steps over structured tools (Text2SQL (T2S)/Snowflake) and unstructured tools (RAG/transcripts) with explicit depends_on for parallelism. Our contributions are threefold: (i) a reference-based plan evaluation framework operating in two modes - a metric-wise evaluator spanning seven dimensions (e.g., tool-prompt alignment, query adherence) and a one-shot evaluator; (ii) a data curation methodology that iteratively refines plans via an evaluator->optimizer loop to produce high-quality plan lineages (ordered plan revisions) while reducing manual effort; and (iii) a large-scale study of 14 LLMs across sizes and families for their ability to decompose queries into step-by-step, executable, and tool-assigned plans, evaluated under prompts with and without lineage. Empirically, LLMs struggle on compound queries and on plans exceeding 4 steps (typically 5-15); the best total metric score reaches 84.8% (Claude-3-7-Sonnet), while the strongest one-shot match rate at the "A+" tier (Extremely Good, Very Good) is only 49.75% (o3-mini). Plan lineage yields mixed gains overall but benefits several top models and improves step executability for many. Our results highlight persistent gaps in tool-understanding, especially in tool-prompt alignment and tool-usage completeness, and show that shorter, simpler plans are markedly easier. The framework and findings provide a reproducible path for assessing and improving agentic planning with tools for answering data-analysis queries in contact-center settings.

</details>


### [21] [Speculative Decoding with a Speculative Vocabulary](https://arxiv.org/abs/2602.13836)
*Miles Williams,Young D. Kwon,Rui Li,Alexandros Kouris,Stylianos I. Venieris*

Main category: cs.CL

TL;DR: 本文提出SpecVocab方法，通过动态词汇子集选择，提高投机解码速度和准确性，优于现有方法EAGLE-3。


<details>
  <summary>Details</summary>
Motivation: 当前的投机解码方法虽然加速了语言模型推断，但受限于输出词汇分布瓶颈，降低词汇表虽提升吞吐量，但降低了推测效果。

Method: 提出SpecVocab方法，每步解码时动态选择词汇子集，优化词汇表猜测。

Result: SpecVocab在多任务中较先进投机解码方法EAGLE-3拥有更长的接受长度，吞吐量提升最高达8.1%。

Conclusion: SpecVocab有效提升了推测解码的效率和效果，克服了减少词汇表带来的性能下降问题。

Abstract: Speculative decoding has rapidly emerged as a leading approach for accelerating language model (LM) inference, as it offers substantial speedups while yielding identical outputs. This relies upon a small draft model, tasked with predicting the outputs of the target model. State-of-the-art speculative decoding methods use a draft model consisting of a single decoder layer and output embedding matrix, with the latter dominating drafting time for the latest LMs. Recent work has sought to address this output distribution bottleneck by reducing the vocabulary of the draft model. Although this can improve throughput, it compromises speculation effectiveness when the target token is out-of-vocabulary. In this paper, we argue for vocabulary speculation as an alternative to a reduced vocabulary. We propose SpecVocab, an efficient and effective method that selects a vocabulary subset per decoding step. Across a variety of tasks, we demonstrate that SpecVocab can achieve a higher acceptance length than state-of-the-art speculative decoding approach, EAGLE-3. Notably, this yields up to an 8.1% increase in average throughput over EAGLE-3.

</details>


### [22] [PrivAct: Internalizing Contextual Privacy Preservation via Multi-Agent Preference Training](https://arxiv.org/abs/2602.13840)
*Yuhan Cheng,Hancheng Ye,Hai Helen Li,Jingwei Sun,Yiran Chen*

Main category: cs.CL

TL;DR: PrivAct框架通过内嵌隐私偏好，实现LLM多智能体的上下文隐私保护，有效降低隐私泄露同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 个性化任务中大型语言模型代理处理敏感且依赖上下文的信息，现有方法多依赖脆弱的外部干预，存在隐私泄露风险，亟需内置隐私感知机制以提升安全性。

Method: 将隐私偏好嵌入每个代理模型的生成行为中，利用多智能体学习框架实现隐私内置保护，避免依赖外部推断时干预。

Result: 本论文提出了PrivAct，一种面向大型语言模型（LLM）代理的上下文隐私感知多智能体学习框架，旨在解决个性化任务中隐私泄露的问题。该方法通过将隐私偏好内嵌到每个代理模型的生成行为中，实现对上下文隐私的内部保护，提升系统整体的隐私完整性并优化隐私与帮助性之间的权衡。实验证明，PrivAct在多个LLM模型和基准测试中均显著减少了隐私泄露率（最高降低12.32%），同时保持了相当的任务帮助性，且具备零样本泛化能力和对多智能体拓扑结构的鲁棒性。

Conclusion: PrivAct有效提升了LLM多智能体系统的上下文隐私保护能力，保证隐私与服务性能的平衡，具备良好的泛化和鲁棒性。

Abstract: Large language model (LLM) agents are increasingly deployed in personalized tasks involving sensitive, context-dependent information, where privacy violations may arise in agents' action due to the implicitness of contextual privacy. Existing approaches rely on external, inference-time interventions which are brittle, scenario-specific, and may expand the privacy attack surface. We propose PrivAct, a contextual privacy-aware multi-agent learning framework that internalizes contextual privacy preservation directly into models' generation behavior for privacy-compliant agentic actions. By embedding privacy preferences into each agent, PrivAct enhances system-wide contextual integrity while achieving a more favorable privacy-helpfulness tradeoff. Experiments across multiple LLM backbones and benchmarks demonstrate consistent improvements in contextual privacy preservation, reducing leakage rates by up to 12.32% while maintaining comparable helpfulness, as well as zero-shot generalization and robustness across diverse multi-agent topologies. Code is available at https://github.com/chengyh23/PrivAct.

</details>


### [23] [Tutoring Large Language Models to be Domain-adaptive, Precise, and Safe](https://arxiv.org/abs/2602.13860)
*Somnath Banerjee*

Main category: cs.CL

TL;DR: 本文开发了一个兼顾技术精度、安全性和文化包容性的负责任智能框架，用以优化大型语言模型的实际应用。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型成为人工智能的变革力量，亟需超越通用架构，构建更具上下文感知、安全可靠并尊重全球文化差异的系统。

Method: 采用从经典的监督适应、解码时对齐安全措施到利用人类反馈和偏好建模的多阶段方法。

Result: 本论文提出了一个“负责任智能”框架，旨在将大型语言模型（LLMs）的强大生成能力与现实环境中的部署要求结合，重点关注领域适应、伦理严格和文化多样性对齐三方面。

Conclusion: 通过领域适应、伦理策略和多文化对齐，构建一个既强大又安全、尊重多样性的智能系统。

Abstract: The overarching research direction of this work is the development of a ''Responsible Intelligence'' framework designed to reconcile the immense generative power of Large Language Models (LLMs) with the stringent requirements of real-world deployment. As these models become a transformative force in artificial intelligence, there is an urgent need to move beyond general-purpose architectures toward systems that are contextually aware, inherently safer, and deeply respectful of global cultural nuances. This research navigates three interconnected threads: domain adaptation to ensure technical precision, ethical rigor to mitigate adversarial vulnerabilities, and cultural/multilingual alignment to promote global inclusivity. The methodological trajectory moves from classical supervised adaptation for task-specific demands to decoding-time alignment for safety, finally leveraging human feedback and preference modeling to achieve sociolinguistic acuity.

</details>


### [24] [Bridging the Multilingual Safety Divide: Efficient, Culturally-Aware Alignment for Global South Languages](https://arxiv.org/abs/2602.13867)
*Somnath Banerjee,Rima Hazra,Animesh Mukherjee*

Main category: cs.CL

TL;DR: 本文指出大语言模型在低资源语言及混合语境中安全性保护措施弱化，文化有害行为依旧存在，且英语安全修补措施难以迁移至低资源语言。提出面向全球南方的参数高效安全引导、本地文化评估及社区参与工作流等实用改进建议。


<details>
  <summary>Details</summary>
Motivation: 现有安全体系主要针对英语及高资源语言，忽视了全球南方多样语言和文化特征，导致安全和事实准确性保护难以有效转移。

Method: 综合近期研究发现，分析低资源语言中安全性的不足，提出参数高效的安全引导及文化基础的评估方法，结合社区参与的工作流。

Result: 发现安全护栏在低资源语言和混合输入中显著弱化，文化有害行为难被标准毒性评分发现，英语安全修补措施难以适用低资源语言，并提出具体改进方向。

Conclusion: 当前大语言模型的安全措施在低资源语言和文化环境下效果不足，需要针对多语言安全的核心要求进行研究和实践。

Abstract: Large language models (LLMs) are being deployed across the Global South, where everyday use involves low-resource languages, code-mixing, and culturally specific norms. Yet safety pipelines, benchmarks, and alignment still largely target English and a handful of high-resource languages, implicitly assuming safety and factuality ''transfer'' across languages. Evidence increasingly shows they do not. We synthesize recent findings indicating that (i) safety guardrails weaken sharply on low-resource and code-mixed inputs, (ii) culturally harmful behavior can persist even when standard toxicity scores look acceptable, and (iii) English-only knowledge edits and safety patches often fail to carry over to low-resource languages. In response, we outline a practical agenda for researchers and students in the Global South: parameter-efficient safety steering, culturally grounded evaluation and preference data, and participatory workflows that empower local communities to define and mitigate harm. Our aim is to make multilingual safety a core requirement-not an add-on-for equitable AI in underrepresented regions.

</details>


### [25] [ADAB: Arabic Dataset for Automated Politeness Benchmarking -- A Large-Scale Resource for Computational Sociopragmatics](https://arxiv.org/abs/2602.13870)
*Hend Al-Khalifa,Nadia Ghezaiel,Maria Bounnit,Hend Hamed Alhazmi,Noof Abdullah Alfear,Reem Fahad Alqifari,Ameera Masoud Almasoud,Sharefah Ahmed Al-Ghamdi*

Main category: cs.CL

TL;DR: ADAB数据集提供了一个含多个阿拉伯语方言的礼貌检测资源，注释细致且覆盖广泛，支持礼貌识别相关的NLP研究。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语礼貌表达丰富复杂，但相关资源匮乏。本研究旨在填补阿拉伯语礼貌检测数据集的空白，支持文化感知的自然语言处理研究，促进多方言和多领域的阿拉伯语社会语用现象理解。

Method: 数据从四个在线平台收集，涵盖社交媒体、电商和客户服务领域；遵循阿拉伯语言传统和语用理论对礼貌进行三分类注释；进行了16个礼貌类别的语言特征注释；进行了包括传统机器学习和现代深度学习模型在内的40种模型的基准测试。

Result: 本文介绍了ADAB，一个涵盖现代标准阿拉伯语及多个方言的阿拉伯语礼貌检测数据集，共包含10,000条带有16个礼貌类别语言特征注释的样本，并取得了较高的注释一致性（kappa=0.703）。研究基于阿拉伯语言传统和语用理论对数据进行标注，分为礼貌、不礼貌和中性三类。采用40种模型配置进行基准测试，包括传统机器学习、基于变换器的模型和大型语言模型。该数据集填补了阿拉伯语礼貌检测资源的空白，促进多领域的阿拉伯语礼貌感知自然语言处理研究。

Conclusion: ADAB数据集是阿拉伯语礼貌检测领域的重要资源，基于语言学和语用理论的标注方法有效，丰富了多方言、多领域的阿拉伯语NLP研究基础，实验证明多种模型在该任务上表现各异。

Abstract: The growing importance of culturally-aware natural language processing systems has led to an increasing demand for resources that capture sociopragmatic phenomena across diverse languages. Nevertheless, Arabic-language resources for politeness detection remain under-explored, despite the rich and complex politeness expressions embedded in Arabic communication. In this paper, we introduce ADAB (Arabic Politeness Dataset), a new annotated Arabic dataset collected from four online platforms, including social media, e-commerce, and customer service domains, covering Modern Standard Arabic and multiple dialects (Gulf, Egyptian, Levantine, and Maghrebi). The dataset was annotated based on Arabic linguistic traditions and pragmatic theory, resulting in three classes: polite, impolite, and neutral. It contains 10,000 samples with linguistic feature annotations across 16 politeness categories and achieves substantial inter-annotator agreement (kappa = 0.703). We benchmark 40 model configurations, including traditional machine learning, transformer-based models, and large language models. The dataset aims to support research on politeness-aware Arabic NLP.

</details>


### [26] [Evaluating Prompt Engineering Techniques for RAG in Small Language Models: A Multi-Hop QA Approach](https://arxiv.org/abs/2602.13890)
*Amir Hossein Mohammadi,Ali Moeinian,Zahra Razavizade,Afsaneh Fatemi,Reza Ramezani*

Main category: cs.CL

TL;DR: 本文通过大规模评估提示模板，显著提升了小型语言模型在多跳问答任务中的RAG性能，为资源受限环境下的小型模型部署提供设计建议。


<details>
  <summary>Details</summary>
Motivation: 虽然RAG方法在大型语言模型中被广泛研究，但其在小型语言模型上的优化，尤其是在复杂的多跳问答任务中仍存在研究空白，同时提示模板设计作为影响性能的重要因素尚未得到充分探索。

Method: 本文通过大规模实证研究，评估了24种不同的提示模板在HotpotQA数据集上的性能表现，包括标准RAG提示、9种文献中提出的技术和14种新颖的混合变体，测试对象为两种小型语言模型Qwen2.5-3B Instruct和Gemma3-4B-It。

Result: 实验证明，改进的提示模板使得Qwen2.5模型性能提升最高达83%，Gemma3-4B-It提升最高达84.5%，相较于标准RAG提示，性能提升最高达6%。

Conclusion: 提示模板设计对小型语言模型的RAG系统性能有显著影响，本文提出的有效提示策略能够在保持资源效率的前提下提升模型表现，具有重要应用价值。

Abstract: Retrieval Augmented Generation (RAG) is a powerful approach for enhancing the factual grounding of language models by integrating external knowledge. While widely studied for large language models, the optimization of RAG for Small Language Models (SLMs) remains a critical research gap, particularly in complex, multi-hop question-answering tasks that require sophisticated reasoning. In these systems, prompt template design is a crucial yet under-explored factor influencing performance. This paper presents a large-scale empirical study to investigate this factor, evaluating 24 different prompt templates on the HotpotQA dataset. The set includes a standard RAG prompt, nine well-formed techniques from the literature, and 14 novel hybrid variants, all tested on two prominent SLMs: Qwen2.5-3B Instruct and Gemma3-4B-It. Our findings, based on a test set of 18720 instances, reveal significant performance gains of up to 83% on Qwen2.5 and 84.5% on Gemma3-4B-It, yielding an improvement of up to 6% for both models compared to the Standard RAG prompt. This research also offers concrete analysis and actionable recommendations for designing effective and efficient prompts for SLM-based RAG systems, practically for deployment in resource-constrained environments.

</details>


### [27] [Pre-Editorial Normalization for Automatically Transcribed Medieval Manuscripts in Old French and Latin](https://arxiv.org/abs/2602.13905)
*Thibault Clérice,Rachel Bawden,Anthony Glaise,Ariane Pinche,David Smith*

Main category: cs.CL

TL;DR: 本研究引入了预编辑规范化（PEN）任务，旨在将自动文本识别（ATR）的字形输出根据编辑规范进行规范化，兼具古文书学的忠实性和实用性。研究构建了新的数据集并采用ByT5模型进行评测，实现了较低的字符错误率。


<details>
  <summary>Details</summary>
Motivation: 现有ATR模型存在古文字学转录与数字化规范版本的分歧，原始输出难以直观使用且下游工具兼容性差，而规范化模型适应性不足且容易过度规范化或产生错误，故提出PEN任务桥接两者。

Method: 构建基于CoMMA语料库并与古法语和拉丁语版本对齐的数据集，利用ByT5序列到序列模型进行预编辑规范化和预标注任务的训练与评测。

Result: 生成了4.66百万样本的银质训练集和1800样本的黄金评测集，模型在规范化任务中实现了6.7%的字符错误率，显著优于以往模型。

Conclusion: 通过引入PEN任务及构建相应的数据集和模型，实现了对ATR输出的有效规范化，提升了文本的可用性和适应性，显著优于之前的方法。

Abstract: Recent advances in Automatic Text Recognition (ATR) have improved access to historical archives, yet a methodological divide persists between palaeographic transcriptions and normalized digital editions. While ATR models trained on more palaeographically-oriented datasets such as CATMuS have shown greater generalizability, their raw outputs remain poorly compatible with most readers and downstream NLP tools, thus creating a usability gap. On the other hand, ATR models trained to produce normalized outputs have been shown to struggle to adapt to new domains and tend to over-normalize and hallucinate. We introduce the task of Pre-Editorial Normalization (PEN), which consists in normalizing graphemic ATR output according to editorial conventions, which has the advantage of keeping an intermediate step with palaeographic fidelity while providing a normalized version for practical usability. We present a new dataset derived from the CoMMA corpus and aligned with digitized Old French and Latin editions using passim. We also produce a manually corrected gold-standard evaluation set. We benchmark this resource using ByT5-based sequence-to-sequence models on normalization and pre-annotation tasks. Our contributions include the formal definition of PEN, a 4.66M-sample silver training corpus, a 1.8k-sample gold evaluation set, and a normalization model achieving a 6.7% CER, substantially outperforming previous models for this task.

</details>


### [28] [HLE-Verified: A Systematic Verification and Structured Revision of Humanity's Last Exam](https://arxiv.org/abs/2602.13964)
*Weiqi Zhai,Zhihai Wang,Jinghang Wang,Boyu Yang,Xiaogang Li,Xiang Xu,Bohan Wang,Peng Wang,Xingzhe Wu,Anfeng Li,Qiyuan Feng,Yuhao Zhou,Shoulin Han,Wenjie Luo,Yiyuan Li,Yaxuan Wang,Ruixian Luo,Guojie Lin,Peiyao Xiao,Chengliang Xu,Ben Wang,Zeyu Wang,Zichao Chen,Jianan Ye,Yijie Hu,Jialong Chen,Zongwen Shen,Yuliang Xu,An Yang,Bowen Yu,Dayiheng Liu,Junyang Lin,Hu Wei,Que Shen,Bing Zhao*

Main category: cs.CL

TL;DR: HLE基准存在噪声，影响模型评测。本文提出HLE-Verified，通过专家审核和修订，大幅提升基准质量和评测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有HLE基准存在大量噪声题目，导致模型评测结果偏差大，难以公正比较多模型性能，因此需要一个经过严格验证和修订的基准。

Method: 采用两阶段验证与修订工作流：第一阶段通过领域专家审查和模型交叉验证，得到641个经过验证的题目；第二阶段对可修正错误题目进行双专家独立修订、模型辅助审计及最终仲裁，产出1170个修订认证题目；其余689题以不确定集公布，并附带不确定性来源和专业标签。

Result: 经过验证和修订的HLE-Verified基准使得七个前沿语言模型的准确率提升7-10个百分点，特别是在原题目或参考答案存在错误的题目上，准确率提升高达30-40个百分点。模型置信度与题目或答案错误高度相关，验证了修订的有效性。

Conclusion: HLE-Verified通过严格的验证和修订流程显著提升了HLE基准的准确性和可信度，有效减少了评测噪声，使得大语言模型的能力评估更加真实可靠。

Abstract: Humanity's Last Exam (HLE) has become a widely used benchmark for evaluating frontier large language models on challenging, multi-domain questions. However, community-led analyses have raised concerns that HLE contains a non-trivial number of noisy items, which can bias evaluation results and distort cross-model comparisons. To address this challenge, we introduce HLE-Verified, a verified and revised version of HLE with a transparent verification protocol and fine-grained error taxonomy. Our construction follows a two-stage validation-and-repair workflow resulting in a certified benchmark. In Stage I, each item undergoes binary validation of the problem and final answer through domain-expert review and model-based cross-checks, yielding 641 verified items. In Stage II, flawed but fixable items are revised under strict constraints preserving the original evaluation intent, through dual independent expert repairs, model-assisted auditing, and final adjudication, resulting in 1,170 revised-and-certified items. The remaining 689 items are released as a documented uncertain set with explicit uncertainty sources and expertise tags for future refinement. We evaluate seven state-of-the-art language models on HLE and HLE-Verified, observing an average absolute accuracy gain of 7--10 percentage points on HLE-Verified. The improvement is particularly pronounced on items where the original problem statement and/or reference answer is erroneous, with gains of 30--40 percentage points. Our analyses further reveal a strong association between model confidence and the presence of errors in the problem statement or reference answer, supporting the effectiveness of our revisions. Overall, HLE-Verified improves HLE-style evaluations by reducing annotation noise and enabling more faithful measurement of model capabilities. Data is available at: https://github.com/SKYLENAGE-AI/HLE-Verified

</details>


### [29] [Chain-of-Thought Reasoning with Large Language Models for Clinical Alzheimer's Disease Assessment and Diagnosis](https://arxiv.org/abs/2602.13979)
*Tongze Zhang,Jun-En Ding,Melik Ozolcer,Fang-Ming Hung,Albert Chih-Chieh Yang,Feng Liu,Yi-Rou Ji,Sang Won Bae*

Main category: cs.CL

TL;DR: 本研究通过引入LLM生成的Chain-of-Thought推理，改进了基于电子健康记录的阿尔茨海默病诊断方法，提高了诊断准确性和解释性。


<details>
  <summary>Details</summary>
Motivation: 传统AD诊断依赖医学影像和临床评估，耗时且资源密集；AD病因复杂，影像难以直接观察；现有LLMs在医疗领域应用有限，缺乏对AD的有效诊断方法。

Method: 利用大语言模型（LLMs）对患者临床电子健康记录（EHRs）进行Chain-of-Thought（CoT）推理，生成明确的诊断推理路径，再通过结构化的CoT推理进行阿尔茨海默病（AD）评估。

Result: 提出的基于CoT推理的诊断框架显著提升了模型在多个认知失调分级任务上的稳定性和诊断性能，相比零样本基线方法，F1分数提升达15%。

Conclusion: 基于LLMs的CoT推理不仅提升了AD诊断的准确率和稳定性，还增强了模型的决策可解释性，有助于复杂多因素疾病的临床评估。

Abstract: Alzheimer's disease (AD) has become a prevalent neurodegenerative disease worldwide. Traditional diagnosis still relies heavily on medical imaging and clinical assessment by physicians, which is often time-consuming and resource-intensive in terms of both human expertise and healthcare resources. In recent years, large language models (LLMs) have been increasingly applied to the medical field using electronic health records (EHRs), yet their application in Alzheimer's disease assessment remains limited, particularly given that AD involves complex multifactorial etiologies that are difficult to observe directly through imaging modalities. In this work, we propose leveraging LLMs to perform Chain-of-Thought (CoT) reasoning on patients' clinical EHRs. Unlike direct fine-tuning of LLMs on EHR data for AD classification, our approach utilizes LLM-generated CoT reasoning paths to provide the model with explicit diagnostic rationale for AD assessment, followed by structured CoT-based predictions. This pipeline not only enhances the model's ability to diagnose intrinsically complex factors but also improves the interpretability of the prediction process across different stages of AD progression. Experimental results demonstrate that the proposed CoT-based diagnostic framework significantly enhances stability and diagnostic performance across multiple CDR grading tasks, achieving up to a 15% improvement in F1 score compared to the zero-shot baseline method.

</details>


### [30] [The Sufficiency-Conciseness Trade-off in LLM Self-Explanation from an Information Bottleneck Perspective](https://arxiv.org/abs/2602.14002)
*Ali Zahedzadeh,Behnam Bahrak*

Main category: cs.CL

TL;DR: 本文研究了语言模型解释的充分性与简洁性之间的权衡，发现适度压缩解释长度可在保证准确率的前提下提升效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型依赖链式推理等自我解释来提升多步问答的性能，但这些解释常常冗长且生成成本高，因此研究解释的必要长度与充分性之间的权衡非常重要。

Method: 基于信息瓶颈原则，将解释视为只保留生成正确答案所需信息的压缩表达，设计评估流程限制解释长度并用多语言模型在ARC Challenge数据集（包括英语和翻译成波斯语）上评估解释充分性。

Result: 实验表明更简洁的解释通常仍能保持充分性，保证准确率的同时大幅减少解释长度，但过度压缩会导致性能下降。

Conclusion: 解释的长度可以在保证充分性的前提下大幅缩短，从而降低生成成本，但过度压缩会影响模型性能。

Abstract: Large Language Models increasingly rely on self-explanations, such as chain of thought reasoning, to improve performance on multi step question answering. While these explanations enhance accuracy, they are often verbose and costly to generate, raising the question of how much explanation is truly necessary. In this paper, we examine the trade-off between sufficiency, defined as the ability of an explanation to justify the correct answer, and conciseness, defined as the reduction in explanation length. Building on the information bottleneck principle, we conceptualize explanations as compressed representations that retain only the information essential for producing correct answers.To operationalize this view, we introduce an evaluation pipeline that constrains explanation length and assesses sufficiency using multiple language models on the ARC Challenge dataset. To broaden the scope, we conduct experiments in both English, using the original dataset, and Persian, as a resource-limited language through translation. Our experiments show that more concise explanations often remain sufficient, preserving accuracy while substantially reducing explanation length, whereas excessive compression leads to performance degradation.

</details>


### [31] [Named Entity Recognition for Payment Data Using NLP](https://arxiv.org/abs/2602.14009)
*Srikumar Nayak*

Main category: cs.CL

TL;DR: 本文分析了用于支付数据提取的最新命名实体识别算法，重点比较了CRF、BiLSTM-CRF、BERT及FinBERT模型，并提出了融合金融领域嵌入的PaymentBERT新架构。


<details>
  <summary>Details</summary>
Motivation: 支付交易数据结构复杂且多样，自动提取结构化信息对金融交易处理及合规至关重要，促使研究高效准确的NER方法。

Method: 采用多种NER模型（CRF、BiLSTM-CRF、BERT、FinBERT）在5万条多格式支付交易数据集上实验，设计融合领域嵌入的PaymentBERT架构，进行消融实验及跨格式泛化评估。

Result: BERT模型F1达到94.2%，领先传统CRF 12.8个百分点，PaymentBERT进一步提升至95.7%，同时满足实时处理要求。

Conclusion: Fine-tuned BERT模型显著优于传统方法，PaymentBERT实现了95.7%的F1分数并支持实时处理，适合金融自动化应用。

Abstract: Named Entity Recognition (NER) has emerged as a critical component in automating financial transaction processing, particularly in extracting structured information from unstructured payment data. This paper presents a comprehensive analysis of state-of-the-art NER algorithms specifically designed for payment data extraction, including Conditional Random Fields (CRF), Bidirectional Long Short-Term Memory with CRF (BiLSTM-CRF), and transformer-based models such as BERT and FinBERT. We conduct extensive experiments on a dataset of 50,000 annotated payment transactions across multiple payment formats including SWIFT MT103, ISO 20022, and domestic payment systems. Our experimental results demonstrate that fine-tuned BERT models achieve an F1-score of 94.2% for entity extraction, outperforming traditional CRF-based approaches by 12.8 percentage points. Furthermore, we introduce PaymentBERT, a novel hybrid architecture combining domain-specific financial embeddings with contextual representations, achieving state-of-the-art performance with 95.7% F1-score while maintaining real-time processing capabilities. We provide detailed analysis of cross-format generalization, ablation studies, and deployment considerations. This research provides practical insights for financial institutions implementing automated sanctions screening, anti-money laundering (AML) compliance, and payment processing systems.

</details>


### [32] [GRRM: Group Relative Reward Modeling for Machine Translation](https://arxiv.org/abs/2602.14028)
*Sen Yang,Shanbo Cheng,Lu Xu,Jianbing Zhang,Shujian Huang*

Main category: cs.CL

TL;DR: 针对机器翻译中现有评价指标不足，论文提出组相对奖励模型提升排序精度和翻译质量，且增强了推理能力，效果优于多种基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的标量质量指标（SQM）在开放领域机器翻译中因缺乏比较上下文，难以区分细微的语言差异，影响了基于GRPO框架的模型性能。

Method: 提出了组相对奖励模型（GRRM），该模型通过联合处理整个候选组，利用比较分析准确评估相对质量和自适应粒度，将其集成到组相对策略优化（GRPO）训练循环中以优化翻译策略。

Result: 实验证明GRRM在排名准确性方面优于所有基线方法，并通过结合GRRM的GRPO训练框架提升了整体翻译质量和推理能力。

Conclusion: 通过引入基于组的质量评价范式（GQM）及其实现的组相对奖励模型（GRRM），论文显著提升了机器翻译领域中大规模语言模型的排序准确性和翻译质量，实现了与先进推理模型媲美的推理能力。

Abstract: While Group Relative Policy Optimization (GRPO) offers a powerful framework for LLM post-training, its effectiveness in open-ended domains like Machine Translation hinges on accurate intra-group ranking. We identify that standard Scalar Quality Metrics (SQM) fall short in this context; by evaluating candidates in isolation, they lack the comparative context necessary to distinguish fine-grained linguistic nuances. To address this, we introduce the Group Quality Metric (GQM) paradigm and instantiate it via the Group Relative Reward Model (GRRM). Unlike traditional independent scorers, GRRM processes the entire candidate group jointly, leveraging comparative analysis to rigorously resolve relative quality and adaptive granularity. Empirical evaluations confirm that GRRM achieves competitive ranking accuracy among all baselines. Building on this foundation, we integrate GRRM into the GRPO training loop to optimize the translation policy. Experimental results demonstrate that our framework not only improves general translation quality but also unlocks reasoning capabilities comparable to state-of-the-art reasoning models. We release codes, datasets, and model checkpoints at https://github.com/NJUNLP/GRRM.

</details>


### [33] [Geometry-Preserving Aggregation for Mixture-of-Experts Embedding Models](https://arxiv.org/abs/2602.14039)
*Sajjad Kachuee,Mohammad Sharifkhani*

Main category: cs.CL

TL;DR: 本文揭示MoE嵌入模型线性聚合的几何缺陷，提出保持超球面结构的SBA聚合方法，显著提升多项文本嵌入任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有MoE嵌入模型采用线性加权假设，忽视了专家输出的超球面流形几何特性，导致聚合后向量塌陷和扭曲，影响嵌入质量与可比较性，故提出几何感知的聚合方法以解决该问题。

Method: 通过分离径向和角度分量设计的球面重心聚合(SBA)算子，保持嵌入的超球面结构，并兼容MoE的路由机制，实现了几何保留的聚合。

Result: 该论文针对Mixture-of-Experts (MoE)嵌入模型中线性加权合成假设嵌入空间为线性子空间的不足，发现专家输出实际分布在共享的超球面流形上，且具有集中范数和显著角度分离特性。线性聚合导致向内塌陷，扭曲向量的大小和方向，影响嵌入的可比性。为此，作者提出了球面重心聚合(SBA)方法，通过分离径向和角度分量，保持超球面结构，并兼容现有的路由机制。实验证明SBA在MTEB基准任务上的语义相似性、聚类和重复问题检测等表现一致提升，训练成本和稳定性不变。几何分析进一步验证了SBA防止了塌陷现象，强调了几何感知聚合在MoE嵌入架构中的重要性。

Conclusion: SBA几何感知聚合有效防止了MoE嵌入中的塌陷问题，保持了超球面结构，提升了模型性能且稳定性良好，验证了几何感知聚合的重要性。

Abstract: Mixture-of-Experts (MoE) embedding models combine expert outputs using weighted linear summation, implicitly assuming a linear subspace structure in the embedding space. This assumption is shown to be inconsistent with the geometry of expert representations. Geometric analysis of a modern MoE embedding model reveals that expert outputs lie on a shared hyperspherical manifold characterized by tightly concentrated norms and substantial angular separation. Under this geometry, linear aggregation induces inward collapse toward the manifold interior, distorting vector magnitude and direction and reducing embedding comparability. To address this inconsistency, Spherical Barycentric Aggregation (SBA) is introduced as a geometry-preserving aggregation operator that separates radial and angular components to maintain hyperspherical structure while remaining fully compatible with existing routing mechanisms. Experiments on selected tasks from the Massive Text Embedding Benchmark (MTEB), including semantic similarity, clustering, and duplicate question detection, demonstrate consistent performance improvements with identical training cost and full stability. Additional geometric analyses confirm that SBA prevents aggregation-induced collapse and preserves hyperspherical consistency, highlighting the importance of geometry-aware aggregation in MoE embedding architectures.

</details>


### [34] [Context Shapes LLMs Retrieval-Augmented Fact-Checking Effectiveness](https://arxiv.org/abs/2602.14044)
*Pietro Bernardelle,Stefano Civelli,Kevin Roitero,Gianluca Demartini*

Main category: cs.CL

TL;DR: 本研究探讨了大语言模型（LLMs）在基于上下文事实验证任务中的表现，发现上下文长度增加会导致模型验证准确率下降，且证据在提示中的位置影响显著。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在多任务上的推理能力强，但其在长上下文中的表现仍不稳定，且之前研究主要关注问答任务中的中部上下文退化，本研究旨在探究事实验证任务中的上下文影响。

Method: 使用三个数据集（HOVER、FEVEROUS、ClimateFEVER）和五个开源模型（不同参数规模和模型家族），评估参数化事实知识及证据在不同上下文长度中的位置影响。

Result: 发现模型的事实验证准确率随上下文长度增加而下降，且当相关证据位于提示开头或结尾时，准确率较高；相反，证据位于中部时准确率较低。这表明提示结构在基于检索的事实核查系统中至关重要。

Conclusion: LLMs在事实验证中拥有一定的参数化知识，但其准确率随上下文长度增长而下降，且证据位置对验证结果影响显著。

Abstract: Large language models (LLMs) show strong reasoning abilities across diverse tasks, yet their performance on extended contexts remains inconsistent. While prior research has emphasized mid-context degradation in question answering, this study examines the impact of context in LLM-based fact verification. Using three datasets (HOVER, FEVEROUS, and ClimateFEVER) and five open-source models accross different parameters sizes (7B, 32B and 70B parameters) and model families (Llama-3.1, Qwen2.5 and Qwen3), we evaluate both parametric factual knowledge and the impact of evidence placement across varying context lengths. We find that LLMs exhibit non-trivial parametric knowledge of factual claims and that their verification accuracy generally declines as context length increases. Similarly to what has been shown in previous works, in-context evidence placement plays a critical role with accuracy being consistently higher when relevant evidence appears near the beginning or end of the prompt and lower when placed mid-context. These results underscore the importance of prompt structure in retrieval-augmented fact-checking systems.

</details>


### [35] [LogitsCoder: Towards Efficient Chain-of-Thought Path Search via Logits Preference Decoding for Code Generation](https://arxiv.org/abs/2602.14054)
*Jizheng Chen,Weiming Zhang,Xinyi Dai,Weiwen Liu,Kounianhua Du,Yasheng Wang,Ruiming Tang,Yong Yu,Weinan Zhang*

Main category: cs.CL

TL;DR: 本文提出LogitsCoder，通过logit控制机制优化推理链，有效提升代码生成的效率和质量。


<details>
  <summary>Details</summary>
Motivation: 代码生成面临精准且结构化推理的挑战，现有方法存在推理链浅薄和冗长导致效率低下的问题。

Method: 提出LogitsCoder框架，通过logit层级控制机制，结合Logits Preference Decoding引导token选择，Logits Rank Based Path Selection和Thoughts Aggregation实现多样推理路径的筛选和合并，提升链式推理的深度和效率。

Result: LogitsCoder生成的推理链更高效且质量更高，显著优于基线方法，实现了更好的代码生成性能。

Conclusion: LogitsCoder有效解决代码生成中的推理深度不足和过度冗长问题，通过logit层级的轻量控制机制实现推理链的优化，提升整体代码生成质量和效率。

Abstract: Code generation remains a challenging task that requires precise and structured reasoning. Existing Test Time Scaling (TTS) methods, including structured tree search, have made progress in exploring reasoning paths but still face two major challenges: (1) underthinking, where reasoning chains tend to be shallow and fail to capture the full complexity of problems; and (2) overthinking, where overly verbose reasoning leads to inefficiency and increased computational costs. To address these issues, we propose LogitsCoder, a novel framework that enhances chain-of-thought reasoning through lightweight, logit-level control mechanisms for code generation. LogitsCoder iteratively generates and refines reasoning steps by first steering token selection toward statistically preferred patterns via Logits Preference Decoding, then selecting and aggregating diverse reasoning paths using Logits Rank Based Path Selection and Thoughts Aggregation. This results in coherent and effective reasoning chains that balance depth and efficiency. Extensive experiments demonstrate that LogitsCoder produces more efficient and higher-quality reasoning chains, leading to superior code generation performance compared to baseline methods.

</details>


### [36] [LM-Lexicon: Improving Definition Modeling via Harmonizing Semantic Experts](https://arxiv.org/abs/2602.14060)
*Yang Liu,Jiaye Yang,Weikang Li,Jiahui Liang,Yang Li,Lingyong Yan*

Main category: cs.CL

TL;DR: LM-Lexicon采用数据聚类、语义专家学习和稀疏专家模型融合方法，实现了定义建模任务的显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统定义建模模型难以细分语义领域导致的性能瓶颈，旨在通过专家分工提高效果。

Method: 结合数据聚类、语义专家训练和基于稀疏专家架构的模型融合，采用语义感知的领域级路由机制和测试时计算扩展。

Result: 在五个基准测试上，LM-Lexicon实现了7%的BLEU得分提升，专家机制带来近10%质量提升，领域级路由效果优于传统方法，测试时扩展进一步提高性能。

Conclusion: LM-Lexicon通过语义领域专家分工和模型合并，显著提升了定义建模的准确性和效率，超过了现有最好模型。

Abstract: We introduce LM-Lexicon, an innovative definition modeling approach that incorporates data clustering, semantic expert learning, and model merging using a sparse mixture-of-experts architecture. By decomposing the definition modeling task into specialized semantic domains, where small language models are trained as domain experts, LM-Lexicon achieves substantial improvements (+7% BLEU score compared with the prior state-of-the-art model) over existing methods on five widely used benchmarks. Empirically, we demonstrate that 1) the clustering strategy enables fine-grained expert specialization with nearly 10% improvement in definition quality; 2) the semantic-aware domain-level routing mechanism achieves higher expert efficacy (+1%) than conventional token-level routing; and 3) further performance gains can be obtained through test-time compute and semantic expert scaling. Our work advances definition modeling while providing insights into the development of efficient language models for semantic-intensive applications.

</details>


### [37] [From Scarcity to Scale: A Release-Level Analysis of the Pashto Common Voice Dataset](https://arxiv.org/abs/2602.14062)
*Jandad Jahani,Mursal Dawodi,Jawid Ahmad Baktash*

Main category: cs.CL

TL;DR: 该论文量化分析了Mozilla Common Voice语料库中Pashto语音数据的快速增长和结构特征，指出参与严重集中和人口统计信息缺失问题，提出扩展验证能力和多样化参与是提升数据集质量的重点。


<details>
  <summary>Details</summary>
Motivation: 许多通用语言的公开语音数据资源有限，Pashto作为拥有6000多万使用者的语言，缺乏适合现代语音识别开发的大规模开放许可语音数据，因此需要对现有资源进行分析以推动该语种ASR系统的发展。

Method: 通过分析Mozilla Common Voice语料库中Pashto语音数据的版本发布情况，采集录音时长、验证小时数、参与者不平等度（用Gini系数衡量）、人口统计元数据完整性和文本级别句子重复度等指标，进行数据规模和质量的综合评估。

Result: Pashto语音数据从2023年中期的1.49小时迅速增长到2025年的2768.7小时，含975.89小时经过验证且适合监督训练；但发现参与者贡献极度集中（Gini=0.941）、年龄分布偏年轻、近42%样本缺少性别标签，且35.88%的独特句子贡献了50%的验证录音，表明数据结构集中主要是参与者活动不均，而非提示句重复。

Conclusion: 该论文对Pashto语音数据在Mozilla Common Voice语料库中的增长及结构特征进行了详尽的量化分析，揭示了语音数据规模迅速扩张的同时存在数据验证效率、参与者集中度高和人口统计标签不完整等问题，强调了改进数据集成熟度的优先方向。

Abstract: Large, openly licensed speech datasets are essential for building automatic speech recognition (ASR) systems, yet many widely spoken languages remain underrepresented in public resources. Pashto, spoken by more than 60 million people, has historically lacked large-scale openly licensed speech data suitable for modern ASR development.
  This paper presents a release-level analysis of the Pashto component of the Mozilla Common Voice corpus, focusing on version 24.0 (December 2025) and contextualizing trends across major releases. We document rapid growth from 1.49 recorded hours in mid-2023 to 2,768.7 total hours in 2025, including 975.89 validated hours available for supervised ASR training.
  Beyond scale, we analyze validation throughput, contributor participation inequality, demographic metadata completeness, and sentence-level concentration in the validated subset. We find that participation is extremely concentrated (Gini = 0.941), age representation is strongly skewed toward young adults, and 41.97\% of clips lack self-reported gender labels, limiting subgroup auditing based on metadata. At the textual level, prompt reuse is moderate: 35.88\% of unique sentences account for 50\% of validated clips, suggesting that structural concentration is driven primarily by uneven contributor activity rather than dominance of a small prompt set.
  These results provide a quantitative audit of a rapidly scaling low-resource speech corpus and highlight practical priorities for improving dataset maturity, including expanded validation capacity and broader demographic participation.

</details>


### [38] [Open Rubric System: Scaling Reinforcement Learning with Pairwise Adaptive Rubric](https://arxiv.org/abs/2602.14069)
*Ruipeng Jia,Yunyi Yang,Yuxin Wu,Yongbo Gai,Siyuan Tao,Mengyu Zhou,Jianhe Lin,Xiaoxi Jiang,Guanjun Jiang*

Main category: cs.CL

TL;DR: OpenRS使用显式、可检查的推理原则取代传统的标量奖励模型，通过成对比较和元量表自适应机制，实现更鲁棒和透明的人工智能对齐。


<details>
  <summary>Details</summary>
Motivation: 标量奖励模型将多维人类偏好压缩成单一不透明分数，导致鲁棒性差和奖励作弊，特别是在开放式对齐任务中。提出奖励应作为可检查的明确推理过程，而非内部化的学习函数，解决非可验证任务中的通用化问题。

Method: 提出Open Rubric System (OpenRS)框架，基于成对自适应元量表(PAMR)和轻量点检可验证尺规(PVRs)，采用显式元量表作为规范，进行基于语义差异的成对比较和外部汇总偏好，避免传统标量化奖励的缺陷。并设计两级元量表精炼流程，结合自动进化和人类在环方法，确保原则的一致性和可编辑性。最后，将OpenRS用于成对强化学习的奖励监督。

Result: 建立了一个鲁棒的奖励评估框架OpenRS，有效避免信息瓶颈和奖励作弊，提高了复杂任务中的判别能力和对偏好的解释性。通过两级元量表精炼保持原则一致性，且提供可验证的子任务奖励，保证对齐过程更可靠。

Conclusion: OpenRS框架解决了标量奖励的局限性，通过显式原则和层次化元量表管理，提高了在开放式非可验证任务中的奖励鲁棒性和可验证性，证明了基于原则的判定机制在AI对齐中的有效性。

Abstract: Scalar reward models compress multi-dimensional human preferences into a single opaque score, creating an information bottleneck that often leads to brittleness and reward hacking in open-ended alignment. We argue that robust alignment for non-verifiable tasks is fundamentally a principle generalization problem: reward should not be a learned function internalized into a judge, but an explicit reasoning process executed under inspectable principles. To operationalize this view, we present the Open Rubric System (OpenRS), a plug-and-play, rubrics-based LLM-as-a-Judge framework built around Pairwise Adaptive Meta-Rubrics (PAMR) and lightweight Pointwise Verifiable Rubrics (PVRs), which provide both hard-constraint guardrails and verifiable reward components when ground-truth or programmatic checks are available. OpenRS uses an explicit meta-rubric -- a constitution-like specification that governs how rubrics are instantiated, weighted, and enforced -- and instantiates adaptive rubrics on the fly by conditioning on the semantic differences between two candidate responses. It then performs criterion-wise pairwise comparisons and aggregates criterion-level preferences externally, avoiding pointwise weighted scalarization while improving discriminability in open-ended settings. To keep principles consistent yet editable across various domains, we introduce a two-level meta-rubric refinement pipeline (automated evolutionary refinement for general principles and a reproducible human-in-the-loop procedure for domain principles), complemented with pointwise verifiable rubrics that act as both guardrails against degenerate behaviors and a source of verifiable reward for objective sub-tasks. Finally, we instantiate OpenRS as reward supervision in pairwise RL training.

</details>


### [39] [Annotation-Efficient Vision-Language Model Adaptation to the Polish Language Using the LLaVA Framework](https://arxiv.org/abs/2602.14073)
*Grzegorz Statkiewicz,Alicja Dobrzeniecka,Karolina Seweryn,Aleksandra Krasnodębska,Karolina Piosek,Katarzyna Bogusz,Sebastian Cygert,Wojciech Kusa*

Main category: cs.CL

TL;DR: 通过自动翻译和轻量过滤，成功构建了适用波兰语的高质量视觉-语言模型，显著提升多语言多模态性能。


<details>
  <summary>Details</summary>
Motivation: 目前大多数视觉-语言模型以英语数据为中心，限制了非英语文化和语言的应用及系统多样性，需推广适用于其他语言的多模态模型。

Method: 采用自动翻译与数据过滤现有多模态数据集，增补合成的波兰语数据，复现并适配LLaVA-Next训练流程。

Result: 本文复现并改编了LLaVA-Next方法，创建了一套针对波兰语的视觉-语言模型（VLMs），通过自动翻译和数据过滤现有多模态数据集，并结合合成的波兰语OCR和文化特定任务数据，实现了显著性能提升，在波兰语多模态基准MMBench上比LLaVA-1.6-Vicuna-13B提升了9.5%，生成评价中语言正确性更高。

Conclusion: 大规模自动翻译结合简单过滤能有效支持低资源语言的多模态模型训练，但在文化覆盖和评估方面仍有挑战。

Abstract: Most vision-language models (VLMs) are trained on English-centric data, limiting their performance in other languages and cultural contexts. This restricts their usability for non-English-speaking users and hinders the development of multimodal systems that reflect diverse linguistic and cultural realities. In this work, we reproduce and adapt the LLaVA-Next methodology to create a set of Polish VLMs. We rely on a fully automated pipeline for translating and filtering existing multimodal datasets, and complement this with synthetic Polish data for OCR and culturally specific tasks. Despite relying almost entirely on automatic translation and minimal manual intervention to the training data, our approach yields strong results: we observe a +9.5% improvement over LLaVA-1.6-Vicuna-13B on a Polish-adapted MMBench, along with higher-quality captions in generative evaluations, as measured by human annotators in terms of linguistic correctness. These findings highlight that large-scale automated translation, combined with lightweight filtering, can effectively bootstrap high-quality multimodal models for low-resource languages. Some challenges remain, particularly in cultural coverage and evaluation. To facilitate further research, we make our models and evaluation dataset publicly available.

</details>


### [40] [GTS: Inference-Time Scaling of Latent Reasoning with a Learnable Gaussian Thought Sampler](https://arxiv.org/abs/2602.14077)
*Minghan Wang,Ye Bai,Thuy-Trang Vu,Ehsan Shareghi,Gholamreza Haffari*

Main category: cs.CL

TL;DR: 本文提出了基于可学习扰动分布的潜在思考探索方法GTS，提升了推理时间尺度调整的效率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有推理时间尺度调整（ITS）方法通过启发式扰动引入随机性，但这些方法的探索行为未被明确定义且在采样有限时效率不高。

Method: 将潜在思考探索建模为条件采样问题，使用高斯思考采样器预测上下文相关的扰动分布，采用GRPO式策略优化训练，主干网络保持冻结。

Result: 提出了高斯思考采样器（GTS），作为一种结构化、可学习的潜在思考探索方法，在保持主干网络固定的情况下通过策略优化训练，实验证明在两个潜在推理架构上比启发式方法更可靠。

Conclusion: 提升潜在推理中的推理时间尺度调整效果需要结构化和可优化的探索机制，而不仅仅是增加随机性。

Abstract: Inference-time scaling (ITS) in latent reasoning models typically introduces stochasticity through heuristic perturbations, such as dropout or fixed Gaussian noise. While these methods increase trajectory diversity, their exploration behavior is not explicitly modeled and can be inefficient under finite sampling budgets. We observe that stronger perturbations do not necessarily translate into more effective candidate trajectories, as unguided noise may disrupt internal decision structure rather than steer it. To provide a more structured alternative, we model latent thought exploration as conditional sampling from learnable densities and instantiate this idea as a Gaussian Thought Sampler (GTS). GTS predicts context-dependent perturbation distributions over continuous reasoning states and is trained with GRPO-style policy optimization while keeping the backbone frozen. Experiments on GSM8K with two latent reasoning architectures show that GTS achieves more reliable inference-time scaling than heuristic baselines. These findings indicate that improving latent ITS requires structured and optimizable exploration mechanisms rather than simply amplifying stochasticity.

</details>


### [41] [Empty Shelves or Lost Keys? Recall Is the Bottleneck for Parametric Factuality](https://arxiv.org/abs/2602.14080)
*Nitay Calderon,Eyal Ben-David,Zorik Gekhman,Eran Ofek,Gal Yona*

Main category: cs.CL

TL;DR: 通过构建WikiProfile基准，发现大模型事实知识的编码已接近极限，回忆能力才是限制准确性的关键，未来改进应侧重提升知识的利用效率而非仅靠规模扩大。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的事实准确性评估无法区分知识缺失（空架子）和已编码事实但访问受限（丢失钥匙），导致错误原因混淆。

Method: 提出了一个行为框架，针对事实进行编码和可访问性分类；引入WikiProfile基准，通过自动化管线和基于网页搜索的提示式大模型构建数据集；在13个大模型上的400万回答中进行评估。

Result: 前沿模型（如GPT-5和Gemini-3）编码接近饱和，覆盖95-98%事实；但回忆能力是主要瓶颈，许多错误源于无法访问已编码知识；访问失败对长尾事实和逆向问题影响更大；推理（思考）能显著提升回忆，弥补部分失败。

Conclusion: 大语言模型事实错误主要因回忆失败而非知识缺失，改进模型在推理和访问知识上的能力是提升事实准确性的关键方向。

Abstract: Standard factuality evaluations of LLMs treat all errors alike, obscuring whether failures arise from missing knowledge (empty shelves) or from limited access to encoded facts (lost keys). We propose a behavioral framework that profiles factual knowledge at the level of facts rather than questions, characterizing each fact by whether it is encoded, and then by how accessible it is: cannot be recalled, can be directly recalled, or can only be recalled with inference-time computation (thinking). To support such profiling, we introduce WikiProfile, a new benchmark constructed via an automated pipeline with a prompted LLM grounded in web search. Across 4 million responses from 13 LLMs, we find that encoding is nearly saturated in frontier models on our benchmark, with GPT-5 and Gemini-3 encoding 95--98% of facts. However, recall remains a major bottleneck: many errors previously attributed to missing knowledge instead stem from failures to access it. These failures are systematic and disproportionately affect long-tail facts and reverse questions. Finally, we show that thinking improves recall and can recover a substantial fraction of failures, indicating that future gains may rely less on scaling and more on methods that improve how models utilize what they already encode.

</details>


### [42] [CCiV: A Benchmark for Structure, Rhythm and Quality in LLM-Generated Chinese \textit{Ci} Poetry](https://arxiv.org/abs/2602.14081)
*Shangqing Zhao,Yupei Ren,Yuhao Zhou,Xiaopeng Bai,Man Lan*

Main category: cs.CL

TL;DR: 提出CCiV基准评测大模型古典词生成，揭示生成中的历史变体、韵律挑战及提示策略效果差异，强调需要更全面的变体感知评估与约束生成方法。


<details>
  <summary>Details</summary>
Motivation: 古典词生成要求结构严格、韵律和谐美，现有大语言模型难以满足高质量生成需求，缺乏系统评估标准。

Method: 设计了CCiV基准，对LLM生成的古典词进行结构、韵律和艺术质量三个维度的系统评估。采用形式感知提示提升大模型的结构和韵律控制效果。

Result: 发现模型常生成有效但意外的历史变体，韵律遵守难度高于结构规则，形式感知提示对强模型有效但弱模型可能下降，形式正确性与文学品质关联弱且不一致。

Conclusion: 生成古典词需考虑历史变体和韵律挑战，形式感知提示能提升大模型生成质量，但仍需发展更全面的约束与评估方法以增强创作质量。

Abstract: The generation of classical Chinese \textit{Ci} poetry, a form demanding a sophisticated blend of structural rigidity, rhythmic harmony, and artistic quality, poses a significant challenge for large language models (LLMs). To systematically evaluate and advance this capability, we introduce \textbf{C}hinese \textbf{Ci}pai \textbf{V}ariants (\textbf{CCiV}), a benchmark designed to assess LLM-generated \textit{Ci} poetry across these three dimensions: structure, rhythm, and quality. Our evaluation of 17 LLMs on 30 \textit{Cipai} reveals two critical phenomena: models frequently generate valid but unexpected historical variants of a poetic form, and adherence to tonal patterns is substantially harder than structural rules. We further show that form-aware prompting can improve structural and tonal control for stronger models, while potentially degrading weaker ones. Finally, we observe weak and inconsistent alignment between formal correctness and literary quality in our sample. CCiV highlights the need for variant-aware evaluation and more holistic constrained creative generation methods.

</details>


### [43] [Character-aware Transformers Learn an Irregular Morphological Pattern Yet None Generalize Like Humans](https://arxiv.org/abs/2602.14100)
*Akhilesh Kakolu Ramarao,Kevin Tang,Dinah Baer-Henney*

Main category: cs.CL

TL;DR: 神经网络模型虽能学习不规则形态模式，但未能像人类那样有效泛化和抽象，存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 探讨神经网络是否能作为形态学习的认知模型，尤其是能否像人类一样推广不规则形态模式。

Method: 比较五种编码器-解码器变压器模型，分析位置编码（顺序编码与位置不变编码）和标签表示（原子标签与分解标签）对西班牙语L型形态模式学习的影响。

Result: 位置不变的编码能更好地捕捉L型范式的聚类，即使训练数据稀缺；顺序位置编码模型则表现较差。尽管如此，所有模型都未能产生成熟的推广能力，且泛化模式与人类不同。

Conclusion: 当前神经网络模型在形态学习上仍未达到人类的抽象和泛化水平，显示统计模式复制与形态抽象间的差距。

Abstract: Whether neural networks can serve as cognitive models of morphological learning remains an open question. Recent work has shown that encoder-decoder models can acquire irregular patterns, but evidence that they generalize these patterns like humans is mixed. We investigate this using the Spanish \emph{L-shaped morphome}, where only the first-person singular indicative (e.g., \textit{pongo} `I put') shares its stem with all subjunctive forms (e.g., \textit{ponga, pongas}) despite lacking apparent phonological, semantic, or syntactic motivation. We compare five encoder-decoder transformers varying along two dimensions: sequential vs. position-invariant positional encoding, and atomic vs. decomposed tag representations. Positional encoding proves decisive: position-invariant models recover the correct L-shaped paradigm clustering even when L-shaped verbs are scarce in training, whereas sequential positional encoding models only partially capture the pattern. Yet none of the models productively generalize this pattern to novel forms. Position-invariant models generalize the L-shaped stem across subjunctive cells but fail to extend it to the first-person singular indicative, producing a mood-based generalization rather than the L-shaped morphomic pattern. Humans do the opposite, generalizing preferentially to the first-person singular indicative over subjunctive forms. None of the models reproduce the human pattern, highlighting the gap between statistical pattern reproduction and morphological abstraction.

</details>


### [44] [Index Light, Reason Deep: Deferred Visual Ingestion for Visual-Dense Document Question Answering](https://arxiv.org/abs/2602.14162)
*Tao Xu*

Main category: cs.CL

TL;DR: 本文提出的DVI框架通过需求侧延迟视觉摄取，实现了低成本高效的多模态文档问答，提升了准确率和系统可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统多模态文档问答方法在文档索引阶段就运行视觉语言模型（VLM）生成全面描述，成本高且易失败。

Method: 提出延迟视觉摄取（DVI）框架，采用需求侧摄取策略，索引阶段仅提取轻量元数据，视觉理解推迟到用户提出具体问题时进行，结合结构化元数据索引和BM25全文搜索实现页面定位，并对特定页面和问题调用VLM。

Result: DVI在两个真实工程图数据集上实现了与传统方法相当的准确率，显著降低了VLM计算成本，实现了50%的视觉必要查询效果率和100%页面定位率，且支持交互式优化和渐进缓存。

Conclusion: DVI框架将问答准确率问题转化为页面定位问题，成功提高了效率和可靠性，为多模态文档问答提供了一种更经济、稳健的解决方案。

Abstract: Existing multimodal document question answering methods universally adopt a supply-side ingestion strategy: running a Vision-Language Model (VLM) on every page during indexing to generate comprehensive descriptions, then answering questions through text retrieval. However, this "pre-ingestion" approach is costly (a 113-page engineering drawing package requires approximately 80,000 VLM tokens), end-to-end unreliable (VLM outputs may fail to be correctly retrieved due to format mismatches in the retrieval infrastructure), and irrecoverable once it fails. This paper proposes the Deferred Visual Ingestion (DVI) framework, adopting a demand-side ingestion strategy: the indexing phase performs only lightweight metadata extraction, deferring visual understanding to the moment users pose specific questions. DVI's core principle is "Index for locating, not understanding"--achieving page localization through structured metadata indexes and BM25 full-text search, then sending original images along with specific questions to a VLM for targeted analysis. Experiments on two real industrial engineering drawings (113 pages + 7 pages) demonstrate that DVI achieves comparable overall accuracy at zero ingestion VLM cost (46.7% vs. 48.9%), an effectiveness rate of 50% on visually necessary queries (vs. 0% for pre-ingestion), and 100% page localization (98% search space compression). DVI also supports interactive refinement and progressive caching, transforming the "QA accuracy" problem into a "page localization" problem--once the correct drawing page is found, obtaining the answer becomes a matter of interaction rounds.

</details>


### [45] [GPT-5 vs Other LLMs in Long Short-Context Performance](https://arxiv.org/abs/2602.14188)
*Nima Esmi,Maryam Nezhad-Moghaddam,Fatemeh Borhani,Asadollah Shahbahrami,Amin Daemdoost,Georgi Gaydadjiev*

Main category: cs.CL

TL;DR: 本文评估了四种先进大型语言模型在处理超长上下文任务中的表现，发现模型在超过5千条社交媒体帖子（约7万词）时准确率显著下降，但GPT-5的精确率保持较高，适合敏感任务。


<details>
  <summary>Details</summary>
Motivation: 虽然理论上大型语言模型具备处理超长上下文的能力，但实际利用效率和性能表现不足，尤其在需要综合理解大量细节的信息密集型任务中，评估其真实表现与提升空间十分必要。

Method: 选用Grok-4、GPT-4、Gemini 2.5和GPT-5四种先进模型，利用三个数据集（两个补充数据集和一个包含2万条社交媒体帖子的主数据集）进行长短上下文任务测试，并分析模型性能随输入规模变化的表现。

Result: 当输入量超过5千条社交媒体帖子时，所有模型准确率明显下降至50%-53%，但GPT-5保持约95%的高精确率。此外，“中间丢失”问题在新模型中基本解决，显示了模型实际应用的性能瓶颈和评价标准的多样性。

Conclusion: 尽管理论上大型语言模型可处理百万级别的上下文词汇，实际表现尤其是在复杂且大量数据任务中依旧存在显著差距。新模型解决了“中间丢失”问题，但准确率下降明显，需关注精确率等多维度指标。

Abstract: With the significant expansion of the context window in Large Language Models (LLMs), these models are theoretically capable of processing millions of tokens in a single pass. However, research indicates a significant gap between this theoretical capacity and the practical ability of models to robustly utilize information within long contexts, especially in tasks that require a comprehensive understanding of numerous details. This paper evaluates the performance of four state-of-the-art models (Grok-4, GPT-4, Gemini 2.5, and GPT-5) on long short-context tasks. For this purpose, three datasets were used: two supplementary datasets for retrieving culinary recipes and math problems, and a primary dataset of 20K social media posts for depression detection. The results show that as the input volume on the social media dataset exceeds 5K posts (70K tokens), the performance of all models degrades significantly, with accuracy dropping to around 50-53% for 20K posts. Notably, in the GPT-5 model, despite the sharp decline in accuracy, its precision remained high at approximately 95%, a feature that could be highly effective for sensitive applications like depression detection. This research also indicates that the "lost in the middle" problem has been largely resolved in newer models. This study emphasizes the gap between the theoretical capacity and the actual performance of models on complex, high-volume data tasks and highlights the importance of metrics beyond simple accuracy for practical applications.

</details>


### [46] [Knowing When Not to Answer: Abstention-Aware Scientific Reasoning](https://arxiv.org/abs/2602.14189)
*Samir Abdaljalil,Erchin Serpedin,Hasan Kurban*

Main category: cs.CL

TL;DR: 本文提出基于放弃机制的科学声明验证框架，通过分解声明和自然语言推理避免盲目给出结论，提高科学信息的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型评价假设模型必须始终给出答案，然而在科学领域，不支持或不确定的结论比盲目回答更有害，因此需要有效识别何时应放弃回答，提高科学结论的可靠性。

Method: 构建放弃意识验证框架，将科学声明拆分为最小条件，利用自然语言推理对每个条件与证据匹配，结合置信度决定支持、反驳或放弃；在SciFact和PubMedQA数据集上测试多种模型和设置。

Result: 本文提出了一种能识别是否应当放弃回答的科学推理验证框架，将科学声明分解为最小条件，利用自然语言推理审核证据、支持或否定这些条件，或选择放弃。通过在SciFact和PubMedQA两个科学基准数据集上对六种不同语言模型进行实验，发现直接准确率变化不大，但通过基于置信度的放弃策略能显著减少错误风险，提高科学推理的可靠性。

Conclusion: 科学推理任务的关键在于判断证据是否充分支持答案，比单纯选择最优模型更重要；放弃机制显著降低风险，是提升科学模型可信度的有效手段。

Abstract: Large language models are increasingly used to answer and verify scientific claims, yet existing evaluations typically assume that a model must always produce a definitive answer. In scientific settings, however, unsupported or uncertain conclusions can be more harmful than abstaining. We study this problem through an abstention-aware verification framework that decomposes scientific claims into minimal conditions, audits each condition against available evidence using natural language inference (NLI), and selectively decides whether to support, refute, or abstain. We evaluate this framework across two complementary scientific benchmarks: SciFact and PubMedQA, covering both closed-book and open-domain evidence settings. Experiments are conducted with six diverse language models, including encoder-decoder, open-weight chat models, and proprietary APIs. Across all benchmarks and models, we observe that raw accuracy varies only modestly across architectures, while abstention plays a critical role in controlling error. In particular, confidence-based abstention substantially reduces risk at moderate coverage levels, even when absolute accuracy improvements are limited. Our results suggest that in scientific reasoning tasks, the primary challenge is not selecting a single best model, but rather determining when available evidence is sufficient to justify an answer. This work highlights abstention-aware evaluation as a practical and model-agnostic lens for assessing scientific reliability, and provides a unified experimental basis for future work on selective reasoning in scientific domains. Code is available at https://github.com/sabdaljalil2000/ai4science .

</details>


### [47] [We can still parse using syntactic rules](https://arxiv.org/abs/2602.14238)
*Ghaly Hussein*

Main category: cs.CL

TL;DR: 基于CFG和GPSG理论，提出新解析算法及规则，生成依存和成分句法树，处理噪声和不完整数据，在多个语料库上表现出良好解析准确率。


<details>
  <summary>Details</summary>
Motivation: 克服传统CFG的局限性，同时利用1950年代以来的理论句法成果，将其应用于计算语境中，实现透明且可解释的自然语言处理模型。

Method: 提出一种结合上下文无关文法（CFG）和广义短语结构文法（GPSG）的新解析算法及句法规则与特征。该方法生成依存句法树和成分句法树，能处理噪声和不完整解析。

Result: 在Universal Dependencies数据集上取得了54.5%的开发集平均无标注依存准确率（UAS）和53.8%的测试集平均UAS，且支持多解析假设以进行重排序提升准确率。

Conclusion: 新方法有效克服了CFG的不足，结合理论句法和计算实践，实现了一种透明、可解释且准确的自然语言解析系统。

Abstract: This research introduces a new parsing approach, based on earlier syntactic work on context free grammar (CFG) and generalized phrase structure grammar (GPSG). The approach comprises both a new parsing algorithm and a set of syntactic rules and features that overcome the limitations of CFG. It also generates both dependency and constituency parse trees, while accommodating noise and incomplete parses. The system was tested on data from Universal Dependencies, showing a promising average Unlabeled Attachment Score (UAS) of 54.5% in the development dataset (7 corpora) and 53.8% in the test set (12 corpora). The system also provides multiple parse hypotheses, allowing further reranking to improve parsing accuracy. This approach also leverages much of the theoretical syntactic work since the 1950s to be used within a computational context. The application of this approach provides a transparent and interpretable NLP model to process language input.

</details>


### [48] [AD-Bench: A Real-World, Trajectory-Aware Advertising Analytics Benchmark for LLM Agents](https://arxiv.org/abs/2602.14257)
*Lingxiang Hu,Yiding Sun,Tianle Xia,Wenwei Li,Ming Xu,Liqun Liu,Peng Shu,Huan Yu,Jie Jiang*

Main category: cs.CL

TL;DR: 提出AD-Bench基于真实营销分析任务的基准测试，以评估大型语言模型在复杂广告营销场景中的性能，实验证明现阶段模型仍存较大不足。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评测多基于理想化模拟，无法反映广告和营销分析等专业领域复杂、多轮交互实际需求，缺乏真实场景下的评测基准。

Method: 设计并构建基于真实用户营销分析请求的AD-Bench基准测试，邀请领域专家提供可验证参考答案和工具调用轨迹，将任务分为三个难度等级，进行多轮交互和多工具协作的性能评测。

Result: Gemini-3-Pro在AD-Bench整体表现为Pass@1=68.0%、Pass@3=83.0%。但在最高难度L3任务中，表现大幅下降至Pass@1=49.4%、Pass@3=62.1%，轨迹覆盖率为70.1%，表明挑战性任务中模型能力不足。

Conclusion: AD-Bench基于真实广告及营销业务需求，能有效评估大型语言模型在复杂多轮、多工具协作的实际营销分析任务中的表现。即使是先进模型如Gemini-3-Pro，在最高难度任务上仍表现出显著不足，显示现有模型在专业领域仍有较大提升空间。

Abstract: While Large Language Model (LLM) agents have achieved remarkable progress in complex reasoning tasks, evaluating their performance in real-world environments has become a critical problem. Current benchmarks, however, are largely restricted to idealized simulations, failing to address the practical demands of specialized domains like advertising and marketing analytics. In these fields, tasks are inherently more complex, often requiring multi-round interaction with professional marketing tools. To address this gap, we propose AD-Bench, a benchmark designed based on real-world business requirements of advertising and marketing platforms. AD-Bench is constructed from real user marketing analysis requests, with domain experts providing verifiable reference answers and corresponding reference tool-call trajectories. The benchmark categorizes requests into three difficulty levels (L1-L3) to evaluate agents' capabilities under multi-round, multi-tool collaboration. Experiments show that on AD-Bench, Gemini-3-Pro achieves Pass@1 = 68.0% and Pass@3 = 83.0%, but performance drops significantly on L3 to Pass@1 = 49.4% and Pass@3 = 62.1%, with a trajectory coverage of 70.1%, indicating that even state-of-the-art models still exhibit substantial capability gaps in complex advertising and marketing analysis scenarios. AD-Bench provides a realistic benchmark for evaluating and improving advertising marketing agents, the leaderboard and code can be found at https://github.com/Emanual20/adbench-leaderboard.

</details>


### [49] [Detecting LLM Hallucinations via Embedding Cluster Geometry: A Three-Type Taxonomy with Measurable Signatures](https://arxiv.org/abs/2602.14259)
*Matic Korun*

Main category: cs.CL

TL;DR: 本文提出了一种基于令牌嵌入簇结构几何特征的大型语言模型（LLM）幻觉分类方法。通过分析11种变换器模型的静态嵌入空间，识别了三种幻觉类型及其几何统计指标，并探讨了模型架构对幻觉表现的影响。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对大型语言模型幻觉的系统分类和可测量的几何特征，限制了幻觉检测和缓解方法的开发。

Method: 通过分析11个编码器和解码器架构的变换器模型的静态嵌入空间，定义了三种幻觉类型及三项几何统计指标（极性耦合、簇内凝聚力和径向信息梯度），并进行统计验证。

Result: 发现极性结构和簇内凝聚力在所有模型中普遍存在，径向信息梯度在大多数模型中显著，且架构特殊性导致部分模型表现异常。

Conclusion: 研究确立了不同类型幻觉的几何特征，并揭示了模型架构对幻觉表现的影响，推动了幻觉检测的定量和类型区分方法。

Abstract: We propose a geometric taxonomy of large language model hallucinations based on observable signatures in token embedding cluster structure. By analyzing the static embedding spaces of 11 transformer models spanning encoder (BERT, RoBERTa, ELECTRA, DeBERTa, ALBERT, MiniLM, DistilBERT) and decoder (GPT-2) architectures, we identify three operationally distinct hallucination types: Type 1 (center-drift) under weak context, Type 2 (wrong-well convergence) to locally coherent but contextually incorrect cluster regions, and Type 3 (coverage gaps) where no cluster structure exists. We introduce three measurable geometric statistics: α (polarity coupling), \b{eta} (cluster cohesion), and λ_s (radial information gradient). Across all 11 models, polarity structure (α > 0.5) is universal (11/11), cluster cohesion (\b{eta} > 0) is universal (11/11), and the radial information gradient is significant (9/11, p < 0.05). We demonstrate that the two models failing λ_s significance -- ALBERT and MiniLM -- do so for architecturally explicable reasons: factorized embedding compression and distillation-induced isotropy, respectively. These findings establish the geometric prerequisites for type-specific hallucination detection and yield testable predictions about architecture-dependent vulnerability profiles.

</details>


### [50] [STATe-of-Thoughts: Structured Action Templates for Tree-of-Thoughts](https://arxiv.org/abs/2602.14265)
*Zachary Bamberger,Till R. Saenger,Gilad Morad,Ofra Amir,Brandon M. Stewart,Amir Feder*

Main category: cs.CL

TL;DR: STATe提出了一种通过离散可解释动作控制推理流程，替代随机采样的推理计算方法，实现了更高质量、更具多样性和可解释性的文本生成。


<details>
  <summary>Details</summary>
Motivation: 现有的推理时计算方法如高温采样未能有效产生有意义的多样性，且缺乏对推理过程的控制和可解释性，限制了推理质量和解释能力的提升。

Method: STATe采用控制器选择高层推理动作，生成器基于这些动作生成推理步骤，评估器对候选答案评分以指导搜索，形成结构化的推理路径替代传统的随机采样方法。

Result: STATe比基于温度采样的方法产生更大响应多样性，明确的动作序列可解释且能预测输出质量，通过关联性能与动作选择还能指导生成向未探索的高潜力区域发展。

Conclusion: STATe方法通过高层次推理模式的离散可解释文本干预替代了高温采样，实现了更高质量、多样性且可解释的文本生成，证明了其在提升推理多样性和输出质量上的有效性。

Abstract: Inference-Time-Compute (ITC) methods like Best-of-N and Tree-of-Thoughts are meant to produce output candidates that are both high-quality and diverse, but their use of high-temperature sampling often fails to achieve meaningful output diversity. Moreover, existing ITC methods offer limited control over how to perform reasoning, which in turn limits their explainability. We present STATe-of-Thoughts (STATe), an interpretable ITC method that searches over high-level reasoning patterns. STATe replaces stochastic sampling with discrete and interpretable textual interventions: a controller selects actions encoding high-level reasoning choices, a generator produces reasoning steps conditioned on those choices, and an evaluator scores candidates to guide search. This structured approach yields three main advantages. First, action-guided textual interventions produce greater response diversity than temperature-based sampling. Second, in a case study on argument generation, STATe's explicit action sequences capture interpretable features that are highly predictive of output quality. Third, estimating the association between performance and action choices allows us to identify promising yet unexplored regions of the action space and steer generation directly toward them. Together, these results establish STATe as a practical framework for generating high-quality, diverse, and interpretable text. Our framework is available at https://github.com/zbambergerNLP/state-of-thoughts.

</details>


### [51] [Does Socialization Emerge in AI Agent Society? A Case Study of Moltbook](https://arxiv.org/abs/2602.14299)
*Ming Li,Xirui Li,Tianyi Zhou*

Main category: cs.CL

TL;DR: 本文通过量化框架分析大规模AI代理社会，发现其存在动态平衡，个体多样性和弱适应性阻碍了社会趋同，规模与交互密度单独不足以实现社会化。


<details>
  <summary>Details</summary>
Motivation: 探讨人工智能代理社会是否会像人类社会一样经历趋同动态。

Method: 引入量化诊断框架，测量语义稳定性、词汇更替、个体惯性、影响持续性和集体共识等指标，对大规模AI代理社会进行动态演变分析。

Result: 发现Moltbook系统处于动态平衡状态，全球语义平均值快速稳定，但个体代理保持高度多样性和词汇更替，个体惯性强，适应性反应弱，影响力短暂且无持续超级节点，缺乏共享社会记忆。

Conclusion: 规模和交互密度不足以促成社会化，研究结果为未来AI代理社会的设计和分析提供了指导原则。

Abstract: As large language model agents increasingly populate networked environments, a fundamental question arises: do artificial intelligence (AI) agent societies undergo convergence dynamics similar to human social systems? Lately, Moltbook approximates a plausible future scenario in which autonomous agents participate in an open-ended, continuously evolving online society. We present the first large-scale systemic diagnosis of this AI agent society. Beyond static observation, we introduce a quantitative diagnostic framework for dynamic evolution in AI agent societies, measuring semantic stabilization, lexical turnover, individual inertia, influence persistence, and collective consensus. Our analysis reveals a system in dynamic balance in Moltbook: while global semantic averages stabilize rapidly, individual agents retain high diversity and persistent lexical turnover, defying homogenization. However, agents exhibit strong individual inertia and minimal adaptive response to interaction partners, preventing mutual influence and consensus. Consequently, influence remains transient with no persistent supernodes, and the society fails to develop stable collective influence anchors due to the absence of shared social memory. These findings demonstrate that scale and interaction density alone are insufficient to induce socialization, providing actionable design and analysis principles for upcoming next-generation AI agent societies.

</details>


### [52] [InnoEval: On Research Idea Evaluation as a Knowledge-Grounded, Multi-Perspective Reasoning Problem](https://arxiv.org/abs/2602.14367)
*Shuofei Qiao,Yunxiang Wei,Xuehai Wang,Bin Wu,Boyang Xue,Ningyu Zhang,Hossein A. Rahmani,Yanshan Wang,Qiang Zhang,Keyan Ding,Jeff Z. Pan,Huajun Chen,Emine Yilmaz*

Main category: cs.CL

TL;DR: 提出InnoEval框架，通过多资源知识检索和多学科评审，提升科学创意评估的全面性和准确性，实验验证其优于传统方法且与专家判断高度一致。


<details>
  <summary>Details</summary>
Motivation: 科学创意产出的快速增长未伴随相应的创意评估方法进步，现有评估方法存在知识视野狭窄、评价维度单一及评估偏见等问题。

Method: 将创意评估视为知识基础的多角度推理问题，提出InnoEval框架，利用异构深度知识搜索引擎从多元在线资源检索动态证据，并通过由不同学术背景评审组成的创新评审委员会实现多维度独立评估。

Result: 构建了基于权威同行评审投稿的综合数据集用于基准测试，实验证明InnoEval在单点、成对和群组评估任务中均优于基线方法，并展现出与专家高度一致的判断和共识模式。

Conclusion: InnoEval有效提升了科学创意评估的知识广度和多维度评估能力，减少了偏见，实现了接近人类专家的评判效果，推动了创意评估方法的进步。

Abstract: The rapid evolution of Large Language Models has catalyzed a surge in scientific idea production, yet this leap has not been accompanied by a matching advance in idea evaluation. The fundamental nature of scientific evaluation needs knowledgeable grounding, collective deliberation, and multi-criteria decision-making. However, existing idea evaluation methods often suffer from narrow knowledge horizons, flattened evaluation dimensions, and the inherent bias in LLM-as-a-Judge. To address these, we regard idea evaluation as a knowledge-grounded, multi-perspective reasoning problem and introduce InnoEval, a deep innovation evaluation framework designed to emulate human-level idea assessment. We apply a heterogeneous deep knowledge search engine that retrieves and grounds dynamic evidence from diverse online sources. We further achieve review consensus with an innovation review board containing reviewers with distinct academic backgrounds, enabling a multi-dimensional decoupled evaluation across multiple metrics. We construct comprehensive datasets derived from authoritative peer-reviewed submissions to benchmark InnoEval. Experiments demonstrate that InnoEval can consistently outperform baselines in point-wise, pair-wise, and group-wise evaluation tasks, exhibiting judgment patterns and consensus highly aligned with human experts.

</details>


### [53] [Beyond Token-Level Policy Gradients for Complex Reasoning with Large Language Models](https://arxiv.org/abs/2602.14386)
*Mufan Xu,Kehai Chen,Xuefeng Bai,Zhengyu Niu,Muyun Yang,Tiejun Zhao,Min Zhang*

Main category: cs.CL

TL;DR: 本文提出块级的多token策略梯度优化方法，提升了复杂推理任务中语言模型的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于token的策略梯度在处理复杂推理任务时，单个token作为动作的优化无法准确反映语义决策的组合性质，存在优化目标和任务性质不匹配的问题。MPO旨在通过块级优化弥补这一缺陷，提高推理任务中的表现。

Method: 提出Multi-token Policy Gradient Optimization（MPO），将连续K个token视为一个统一的语义动作，进行块级策略梯度优化，以更好地捕捉推理轨迹的组合结构并优化高层次目标。

Result: 该论文提出了Multi-token Policy Gradient Optimization（MPO）方法，针对自动回归语言模型中传统基于单个token的策略梯度优化方法在复杂推理任务中存在的局限，提出以连续K个token序列作为统一语义动作的块级优化框架。该方法更好地捕捉了推理过程中的组合结构，并支持对更高级别目标的优化。实验证明，在数学推理和编码基准测试中，MPO明显优于传统的token级策略梯度方法，揭示了token级优化在复杂推理任务中的不足，促进未来研究关注超越token粒度的推理语言任务优化。

Conclusion: MPO方法有效弥补了token级策略梯度在复杂推理任务中的不足，提升了模型生成质量，证明了块级优化在推理密集型语言任务中的重要性。

Abstract: Existing policy-gradient methods for auto-regressive language models typically select subsequent tokens one at a time as actions in the policy. While effective for many generation tasks, such an approach may not fully capture the structure of complex reasoning tasks, where a single semantic decision is often realized across multiple tokens--for example, when defining variables or composing equations. This introduces a potential mismatch between token-level optimization and the inherently block-level nature of reasoning in these settings. To bridge this gap, we propose Multi-token Policy Gradient Optimization (MPO), a framework that treats sequences of K consecutive tokens as unified semantic actions. This block-level perspective enables our method to capture the compositional structure of reasoning trajectories and supports optimization over coherent, higher-level objectives. Experiments on mathematical reasoning and coding benchmarks show that MPO outperforms standard token-level policy gradient baselines, highlight the limitations of token-level policy gradients for complex reasoning, motivating future research to look beyond token-level granularity for reasoning-intensive language tasks.

</details>


### [54] [TruthStance: An Annotated Dataset of Conversations on Truth Social](https://arxiv.org/abs/2602.14406)
*Fathima Ameen,Danielle Brown,Manusha Malgareddy,Amanul Haque*

Main category: cs.CL

TL;DR: 提出了TruthStance数据集及基于Truth Social的论点挖掘和立场检测基准，推动了非主流社交平台上的在线话语分析。


<details>
  <summary>Details</summary>
Motivation: 当前研究多聚焦主流平台，缺少对alt-tech平台对话结构的研究；为弥补这一空白，提出TruthStance数据集以促进在线意见形成和争辩的理解。

Method: 构建TruthStance数据集，设计1,500条人工注释进行基准测试，评估大语言模型提示策略，生成大规模自动标签以支持深入分析。

Result: 本文引入了TruthStance数据集，包含2023至2025年Truth Social平台上的24,378条帖子和523,360条评论，保留了回复树结构。该数据集设有1,500条人类注释的论点挖掘与立场检测基准，并评估了大语言模型的提示策略，随后发布了大规模的自动标注数据。

Conclusion: TruthStance数据集及其基准有效支持了论点挖掘和立场检测任务，LLM提示表现良好，促进了alt-tech平台的观点分析研究。

Abstract: Argument mining and stance detection are central to understanding how opinions are formed and contested in online discourse. However, most publicly available resources focus on mainstream platforms such as Twitter and Reddit, leaving conversational structure on alt-tech platforms comparatively under-studied. We introduce TruthStance, a large-scale dataset of Truth Social conversation threads spanning 2023-2025, consisting of 24,378 posts and 523,360 comments with reply-tree structure preserved. We provide a human-annotated benchmark of 1,500 instances across argument mining and claim-based stance detection, including inter-annotator agreement, and use it to evaluate large language model (LLM) prompting strategies. Using the best-performing configuration, we release additional LLM-generated labels for 24,352 posts (argument presence) and 107,873 comments (stance to parent), enabling analysis of stance and argumentation patterns across depth, topics, and users. All code and data are released publicly.

</details>


### [55] [WavePhaseNet: A DFT-Based Method for Constructing Semantic Conceptual Hierarchy Structures (SCHS)](https://arxiv.org/abs/2602.14419)
*Kiyotaka Kasubuchi,Kazuo Fukiya*

Main category: cs.CL

TL;DR: 本文揭示大型语言模型幻觉的结构性根源，提出基于傅里叶变换的语义层次结构与降维策略，并用上同调理论正则化嵌入空间，提升模型语义一致性，抑制幻觉。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型中Transformer/Attention机制导致幻觉现象的根本结构性原因。

Method: 利用测度理论重构Transformer机制，采用离散傅里叶变换分解语义频段构建语义概念层次结构，基于累积能量分析实现从24576维到约3000维的降维，结合上同调理论在局部窗口定义图结构并通过霍奇正交投影对语义一致性进行正则化。

Result: 通过测度理论和频率分析证明幻觉是不可避免的结构性限制；提出WavePhaseNet方法利用离散傅里叶变换构建语义概念层次结构，实现语义的频段分解与精确操作；发现GPT-4的嵌入空间可以降维到约3000维以保留完整语义，抑制幻觉；引入上同调一致性控制，量化局部推理不一致性并利用霍奇理论进行正则化，提高语义一致性。

Conclusion: 幻觉是大型语言模型结构上的固有限制，通过频率域语义分解和上同调一致性控制可有效增强逻辑一致性和降低幻觉现象。

Abstract: This paper reformulates Transformer/Attention mechanisms in Large Language Models (LLMs) through measure theory and frequency analysis, theoretically demonstrating that hallucination is an inevitable structural limitation. The embedding space functions as a conditional expectation over a σ-algebra, and its failure to be isomorphic to the semantic truth set fundamentally causes logical consistency breakdown. WavePhaseNet Method The authors propose WavePhaseNet, which explicitly constructs a Semantic Conceptual Hierarchy Structure (SCHS) using Discrete Fourier Transform (DFT). By applying DFT along the sequence dimension, semantic information is decomposed into frequency bands: low-frequency components capture global meaning and intent, while high-frequency components represent local syntax and expression. This staged separation enables precise semantic manipulation in diagonalized space. Dimensionality Reduction GPT-4's 24,576-dimensional embedding space exhibits a 1/f spectral structure based on language self-similarity and Zipf's law. Through cumulative energy analysis, the authors derive that approximately 3,000 dimensions constitute the lower bound for "complete representation." This demonstrates that reduction from 24,576 to 3,000 dimensions preserves meaning and intent while enabling rigorous reasoning and suppressing hallucination. Cohomological Consistency Control The reduced embedding space, constructed via cohomological regularization over overlapping local windows, allows defining a graph structure and cochain complex. This quantifies inconsistencies among local inferences as coboundary-based losses. Applying harmonic projection based on Hodge theory positions cohomology as a computable regularization principle for controlling semantic consistency, extracting maximally consistent global representations.

</details>


### [56] [LLM-Guided Knowledge Distillation for Temporal Knowledge Graph Reasoning](https://arxiv.org/abs/2602.14428)
*Wang Xing,Wei Song,Siyu Lin,Chen Wu,Man Wang*

Main category: cs.CL

TL;DR: 本文设计了一个结合大语言模型辅助的时序知识图蒸馏框架，有效提升轻量级模型的时序推理能力，兼顾性能与效率。


<details>
  <summary>Details</summary>
Motivation: 现有的时序知识图模型计算成本高，部署困难，而传统的压缩与蒸馏方法多针对静态图，直接应用于时序图会忽视时间相关交互，导致性能下降。

Method: 提出了一个专为时序知识图推理设计的基于大语言模型辅助的知识蒸馏框架，通过结合高容量时序教师模型与大语言模型作为辅助指导，实现学生模型的高效训练。

Result: 在多个公开的时序知识图基准测试和不同主干架构上，所提方法显著提升了链接预测性能，优于强蒸馏基线，同时保持了模型的简洁和推理效率。

Conclusion: 大语言模型可作为有效教师，辅助传递时序推理能力至资源受限的时序知识图系统，提升轻量模型性能并保持高效部署。

Abstract: Temporal knowledge graphs (TKGs) support reasoning over time-evolving facts, yet state-of-the-art models are often computationally heavy and costly to deploy. Existing compression and distillation techniques are largely designed for static graphs; directly applying them to temporal settings may overlook time-dependent interactions and lead to performance degradation. We propose an LLM-assisted distillation framework specifically designed for temporal knowledge graph reasoning. Beyond a conventional high-capacity temporal teacher, we incorporate a large language model as an auxiliary instructor to provide enriched supervision. The LLM supplies broad background knowledge and temporally informed signals, enabling a lightweight student to better model event dynamics without increasing inference-time complexity. Training is conducted by jointly optimizing supervised and distillation objectives, using a staged alignment strategy to progressively integrate guidance from both teachers. Extensive experiments on multiple public TKG benchmarks with diverse backbone architectures demonstrate that the proposed approach consistently improves link prediction performance over strong distillation baselines, while maintaining a compact and efficient student model. The results highlight the potential of large language models as effective teachers for transferring temporal reasoning capability to resource-efficient TKG systems.

</details>


### [57] [Robust Bias Evaluation with FilBBQ: A Filipino Bias Benchmark for Question-Answering Language Models](https://arxiv.org/abs/2602.14466)
*Lance Calvin Lim Gamboa,Yue Feng,Mark Lee*

Main category: cs.CL

TL;DR: 本文提出了菲律宾语版本的偏见基准FilBBQ，通过改进的方法评估了菲律宾语模型中的性别与性取向偏见，结果显示存在显著的刻板偏见。


<details>
  <summary>Details</summary>
Motivation: 现有BBQ基准主要针对英语环境，缺乏针对菲律宾语言和文化背景下偏见的评估工具，且之前的评估受模型响应不稳定性影响较大。

Method: 通过模板分类、文化适应性翻译、新模板构建和提示生成四个阶段开发FilBBQ，并使用多种随机种子对模型响应进行评估，计算平均偏见评分。

Result: 建立了包含一万多个测试提示的FilBBQ，揭示了菲律宾语模型中情感、家庭角色、刻板的酷儿兴趣和一夫多妻制相关的性别歧视和恐同偏见，并证实了偏见评分在不同种子间存在较大变异。

Conclusion: FilBBQ成功扩展了BBQ基准的语言范围，提供了专门针对菲律宾文化背景下性别歧视和恐同偏见的检测工具，并通过多次种子运行提高了评估的稳定性和准确性。

Abstract: With natural language generation becoming a popular use case for language models, the Bias Benchmark for Question-Answering (BBQ) has grown to be an important benchmark format for evaluating stereotypical associations exhibited by generative models. We expand the linguistic scope of BBQ and construct FilBBQ through a four-phase development process consisting of template categorization, culturally aware translation, new template construction, and prompt generation. These processes resulted in a bias test composed of more than 10,000 prompts which assess whether models demonstrate sexist and homophobic prejudices relevant to the Philippine context. We then apply FilBBQ on models trained in Filipino but do so with a robust evaluation protocol that improves upon the reliability and accuracy of previous BBQ implementations. Specifically, we account for models' response instability by obtaining prompt responses across multiple seeds and averaging the bias scores calculated from these distinctly seeded runs. Our results confirm both the variability of bias scores across different seeds and the presence of sexist and homophobic biases relating to emotion, domesticity, stereotyped queer interests, and polygamy. FilBBQ is available via GitHub.

</details>


### [58] [Measuring and Mitigating Post-hoc Rationalization in Reverse Chain-of-Thought Generation](https://arxiv.org/abs/2602.14469)
*Guangyue Peng,Zongchao Chen,Wen Luo,Yuntao Wen,Wei Li,Ruixiang Feng,Ran Le,Chen Yang,Zhenwei An,Yang Song,Tao Zhang,Houfeng Wang*

Main category: cs.CL

TL;DR: 本文分析了逆向思维链生成中的答案锚定问题，揭示传统抑制策略的弊端，提出结构骨架引导推理(SSR)方法，大幅减少答案依赖并提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 逆向思维链生成中模型容易生成事后合理化的推理，即模型在生成过程中依赖已知答案，导致推理解释被答案“锚定”。

Method: 提出一种两阶段的方法——结构骨架引导推理(SSR)，先生成与答案无关的功能性骨架，再利用骨架引导完整推理轨迹；并引入蒸馏版SSR(SSR-D)通过教师轨迹微调模型，确保结构遵循。

Result: SSR在词汇层面、熵动态和概率依赖三个层面减少了答案锚定效应，SSR-D在开放式推理基准测试中比抑制策略提升了最多10%的性能，同时保持了分布外泛化能力。

Conclusion: 通过引导模型先构建与答案无关的结构骨架，SSR有效打破了由答案监控导致的锚定循环，提升推理质量和泛化能力。

Abstract: Reverse Chain-of-Thought Generation (RCG) synthesizes reasoning traces from query-answer pairs, but runs the risk of producing post-hoc rationalizations: when models can see the answer during generation, the answer serves as a cognitive anchor that shapes the entire explanation. We formalize this phenomenon through a three-level measurement hierarchy: lexical, entropic, and probabilistic anchoring, each captures surface artifacts, entropy dynamics, and latent answer dependence, respectively. We analyze semantic suppression, the intuitive mitigation strategy that instructs models to ignore the answer, to find out its counterproduction: while it reduces lexical overlap, it paradoxically increases entropic and probabilistic anchoring. Drawing on Ironic Process Theory from cognitive psychology, we attribute this failure to active monitoring of the forbidden answer, which inadvertently deepens dependence on it. To break this cycle, we propose Structural Skeleton-guided Reasoning (SSR), a two-phase approach that first generates an answer-invariant functional skeleton structure, then uses this skeleton to guide full trace generation. By redirecting the information flow to structural planning rather than answer monitoring, SSR consistently reduces anchoring across all three levels. We further introduce Distilled SSR (SSR-D), which fine-tunes models on teacher-generated SSR traces to ensure reliable structural adherence. Experiments across open-ended reasoning benchmarks demonstrate that SSR-D achieves up to 10% improvement over suppression baselines while preserving out-of-distribution (OOD) generalization.

</details>


### [59] [HyperRAG: Reasoning N-ary Facts over Hypergraphs for Retrieval Augmented Generation](https://arxiv.org/abs/2602.14470)
*Wen-Sheng Lien,Yu-Kai Chan,Hao-Lung Hsiao,Bo-Kai Ruan,Meng-Fen Chiang,Chien-An Chen,Yi-Ren Yeh,Hong-Han Shuai*

Main category: cs.CL

TL;DR: 本文提出了基于n元超图的检索增强生成框架HyperRAG，解决了传统知识图谱在多跳问答中检索僵硬和表达能力有限的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于二元关系知识图的多跳问答方法存在检索方式僵硬、引入无关上下文、计算开销大及关系表达有限等缺点，亟需更高效且表达能力更强的检索方法。

Method: HyperRAG框架包括HyperRetriever和HyperMemory两种检索变体，前者通过结构语义推理构建查询条件的关系链，后者利用大型语言模型的参数记忆动态评分搜索路径。

Result: 在11个闭域数据集和3个开放域问答基准上，HyperRetriever在MRR和Hits@10指标上分别提升了2.95%和1.23%，验证了HyperRAG的有效性。

Conclusion: HyperRAG通过采用n元超图和两种互补检索方法，实现了更高的准确率和更有效的推理路径，在开放域和闭域多跳问答任务中表现优异。

Abstract: Graph-based retrieval-augmented generation (RAG) methods, typically built on knowledge graphs (KGs) with binary relational facts, have shown promise in multi-hop open-domain QA. However, their rigid retrieval schemes and dense similarity search often introduce irrelevant context, increase computational overhead, and limit relational expressiveness. In contrast, n-ary hypergraphs encode higher-order relational facts that capture richer inter-entity dependencies and enable shallower, more efficient reasoning paths. To address this limitation, we propose HyperRAG, a RAG framework tailored for n-ary hypergraphs with two complementary retrieval variants: (i) HyperRetriever learns structural-semantic reasoning over n-ary facts to construct query-conditioned relational chains. It enables accurate factual tracking, adaptive high-order traversal, and interpretable multi-hop reasoning under context constraints. (ii) HyperMemory leverages the LLM's parametric memory to guide beam search, dynamically scoring n-ary facts and entities for query-aware path expansion. Extensive evaluations on WikiTopics (11 closed-domain datasets) and three open-domain QA benchmarks (HotpotQA, MuSiQue, and 2WikiMultiHopQA) validate HyperRAG's effectiveness. HyperRetriever achieves the highest answer accuracy overall, with average gains of 2.95% in MRR and 1.23% in Hits@10 over the strongest baseline. Qualitative analysis further shows that HyperRetriever bridges reasoning gaps through adaptive and interpretable n-ary chain construction, benefiting both open and closed-domain QA.

</details>


### [60] [BETA-Labeling for Multilingual Dataset Construction in Low-Resource IR](https://arxiv.org/abs/2602.14488)
*Md. Najib Hasan,Mst. Jannatun Ferdous Rain,Fyad Mohammed,Nazmul Siddique*

Main category: cs.CL

TL;DR: 该研究利用多模型大语言模型构建孟加拉语IR数据集，探索跨语言数据集复用，指出了跨语言复用中的语义和偏见风险，为低资源语言IR数据集构建提供实证和指导。


<details>
  <summary>Details</summary>
Motivation: 低资源语言信息检索受限于高质量标注数据稀缺，人工标注成本高且难以扩展，单一大语言模型自动标注又会带来标签可靠性、偏见和评估有效性问题，有必要探索多模型结合和机器翻译复用的创新方法以促进低资源语言IR发展。

Method: 本文采用多模型融合的大语言模型作为自动标注者，结合上下文对齐、一致性检查、主流意见机制，最后辅以人工评估，形成高质量的标注框架BETA。通过多语言对采用LLM机器翻译评估语义保持和任务有效性，检验跨语言数据集复用的可行性与风险。

Result: 本文针对低资源语言信息检索（IR）领域数据集稀缺问题，提出了一种基于多种大语言模型（LLM）结合的BETA标注框架，构建了孟加拉语信息检索数据集。框架通过上下文匹配、一致性检测、多数决策及人工验证保证数据标签质量。此外，研究了通过单跳机器翻译复用其他低资源语言IR数据集的可行性，发现不同语言间语义保持和任务有效性存在较大差异，影响跨语言数据集复用的可靠性。本文展示了LLM辅助数据集构建的潜力与限制，为低资源语言设置下构建更可靠基准测试和评测流程提供了经验指导。

Conclusion: 多模型LLM结合的BETA标注框架能够提升低资源语言IR数据集的标注质量，但跨语言数据集复用存在显著的语义保持和偏见问题，风险不可忽视。研究强调构建低资源语言基准需谨慎，建议结合人工验证实现更可靠评测。

Abstract: IR in low-resource languages remains limited by the scarcity of high-quality, task-specific annotated datasets. Manual annotation is expensive and difficult to scale, while using large language models (LLMs) as automated annotators introduces concerns about label reliability, bias, and evaluation validity. This work presents a Bangla IR dataset constructed using a BETA-labeling framework involving multiple LLM annotators from diverse model families. The framework incorporates contextual alignment, consistency checks, and majority agreement, followed by human evaluation to verify label quality. Beyond dataset creation, we examine whether IR datasets from other low-resource languages can be effectively reused through one-hop machine translation. Using LLM-based translation across multiple language pairs, we experimented on meaning preservation and task validity between source and translated datasets. Our experiment reveal substantial variation across languages, reflecting language-dependent biases and inconsistent semantic preservation that directly affect the reliability of cross-lingual dataset reuse. Overall, this study highlights both the potential and limitations of LLM-assisted dataset creation for low-resource IR. It provides empirical evidence of the risks associated with cross-lingual dataset reuse and offers practical guidance for constructing more reliable benchmarks and evaluation pipelines in low-resource language settings.

</details>


### [61] [Query as Anchor: Scenario-Adaptive User Representation via Large Language Model](https://arxiv.org/abs/2602.14492)
*Jiahao Yuan,Yike Xu,Jinyong Wen,Baokun Wang,Ziyi Gao,Xiaotong Lin,Yun Liu,Xing Fu,Yu Cheng,Yongchao Liu,Weiqiang Wang,Zhongle Xie*

Main category: cs.CL

TL;DR: 本文提出了Query-as-Anchor框架，用于动态生成查询感知的用户向量，结合大型语言模型和多模态行为数据，在工业级任务中取得了最先进的表现与高效部署。


<details>
  <summary>Details</summary>
Motivation: 用户表示学习在工业规模应用中需要兼顾泛化能力和任务敏感性，但现有模型多为静态且任务无关的向量表示，难以满足不同下游场景的需求，同时异构多源数据带来的噪声和模态冲突降低了表示效果。

Method: 利用UserU数据集进行预训练，采用层次化粗细编码器与双塔大语言模型联合对比自回归训练，实现查询感知嵌入；通过聚类软提示微调增强模型业务场景适配性；部署时使用查询锚定和KV缓存机制提升推理效率。

Result: 提出了一种基于查询的动态用户表示学习框架Query-as-Anchor，通过构建大规模多模态行为语义对齐的数据集UserU，以及融合集层次编码器和联合对比自回归优化的Q-Anchor嵌入结构，实现了查询感知的用户表示。引入基于聚类的软提示微调技术提高了模型的场景适应性。实验在10个工业基准和支付宝线上场景验证了该方法的性能和实用性。

Conclusion: Query-as-Anchor成功平衡了用户表示的泛化性和任务敏感性，提升了多模态用户数据的表示质量和下游任务性能，适合工业大规模应用且部署高效。

Abstract: Industrial-scale user representation learning requires balancing robust universality with acute task-sensitivity. However, existing paradigms primarily yield static, task-agnostic embeddings that struggle to reconcile the divergent requirements of downstream scenarios within unified vector spaces. Furthermore, heterogeneous multi-source data introduces inherent noise and modality conflicts, degrading representation. We propose Query-as-Anchor, a framework shifting user modeling from static encoding to dynamic, query-aware synthesis. To empower Large Language Models (LLMs) with deep user understanding, we first construct UserU, an industrial-scale pre-training dataset that aligns multi-modal behavioral sequences with user understanding semantics, and our Q-Anchor Embedding architecture integrates hierarchical coarse-to-fine encoders into dual-tower LLMs via joint contrastive-autoregressive optimization for query-aware user representation. To bridge the gap between general pre-training and specialized business logic, we further introduce Cluster-based Soft Prompt Tuning to enforce discriminative latent structures, effectively aligning model attention with scenario-specific modalities. For deployment, anchoring queries at sequence termini enables KV-cache-accelerated inference with negligible incremental latency. Evaluations on 10 Alipay industrial benchmarks show consistent SOTA performance, strong scalability, and efficient deployment. Large-scale online A/B testing in Alipay's production system across two real-world scenarios further validates its practical effectiveness. Our code is prepared for public release and will be available at: https://github.com/JhCircle/Q-Anchor.

</details>


### [62] [Beyond Translation: Evaluating Mathematical Reasoning Capabilities of LLMs in Sinhala and Tamil](https://arxiv.org/abs/2602.14517)
*Sukumar Kishanthan,Kumar Thushalika,Buddhi Jayasekara,Asela Hevapathige*

Main category: cs.CL

TL;DR: 大型语言模型在低资源语言的数学推理表现存在显著差异，复杂推理能力未能有效跨语言迁移，反映其多语言推理能力可能并不均匀。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型在低资源语言（如僧伽罗语和泰米尔语）中数学推理能力的真实性，是源于其真正的多语言推理能力，还是依赖于隐式翻译成英语表示。

Method: 通过构建本地化三语言平行数据集，即英语、僧伽罗语和泰米尔语，由数学专业流利发言人本地撰写数学问题，避免翻译带来的偏差，采用六种数学题型（从基础算术到复杂的单位冲突和优化问题）对四个大型语言模型的数学推理能力进行评估。

Result: 基本算术推理在三种语言中表现稳定，但复杂推理任务在泰米尔语和僧伽罗语中表现明显下降。不同模型和题型的失败模式各异，显示多语言性能并不代表推理能力在所有语言中均衡。

Conclusion: 模型在多语言环境下表现出的强数学推理能力不可简单视为跨语言均等，呼吁对多语言模型进行细粒度、类型感知的评估以准确反映推理能力。

Abstract: Large language models (LLMs) demonstrate strong mathematical reasoning in English, but whether these capabilities reflect genuine multilingual reasoning or reliance on translation-based processing in low-resource languages like Sinhala and Tamil remains unclear. We examine this fundamental question by evaluating whether LLMs genuinely reason mathematically in these languages or depend on implicit translation to English-like representations. Using a taxonomy of six math problem types, from basic arithmetic to complex unit conflict and optimization problems, we evaluate four prominent large language models. To avoid translation artifacts that confound language ability with translation quality, we construct a parallel dataset where each problem is natively authored by fluent speakers with mathematical training in all three languages. Our analysis demonstrates that while basic arithmetic reasoning transfers robustly across languages, complex reasoning tasks show significant degradation in Tamil and Sinhala. The pattern of failures varies by model and problem type, suggesting that apparent multilingual competence may not reflect uniform reasoning capabilities across languages. These findings challenge the common assumption that models exhibiting strong multilingual performance can reason equally effectively across languages, and highlight the need for fine-grained, type-aware evaluation in multilingual settings.

</details>


### [63] [Explainable Token-level Noise Filtering for LLM Fine-tuning Datasets](https://arxiv.org/abs/2602.14536)
*Yuchen Yang,Wenze Lin,Enhao Huang,Zhixuan Chu,Hongbin Zhou,Lan Tao,Yiming Li,Zhan Qin,Kui Ren*

Main category: cs.CL

TL;DR: 针对LLM微调中的token级噪声问题，提出XTF框架，通过属性分解过滤噪声，提升微调性能。


<details>
  <summary>Details</summary>
Motivation: 当前微调数据集多为句子级别，导致token级噪声影响微调效果，亟需设计token级噪声过滤方法提升微调性能。

Method: 将token贡献分解为推理重要性、知识新颖性和任务相关性三属性，通过评分方法检测噪声token，再根据评分屏蔽对应梯度，实现噪声过滤。

Result: 提出了XTF框架，通过分解token级数据贡献成三个属性（推理重要性、知识新颖性、任务相关性）进行评分，并屏蔽噪声token梯度，显著提升LLM微调效果，在数学、代码和医学三个任务及7个主流LLM中最多提升13.7%。

Conclusion: XTF框架有效过滤了token级噪声，提高了LLM微调后的下游任务表现，验证了token级数据优化的重要性。

Abstract: Large Language Models (LLMs) have seen remarkable advancements, achieving state-of-the-art results in diverse applications. Fine-tuning, an important step for adapting LLMs to specific downstream tasks, typically involves further training on corresponding datasets. However, a fundamental discrepancy exists between current fine-tuning datasets and the token-level optimization mechanism of LLMs: most datasets are designed at the sentence-level, which introduces token-level noise, causing negative influence to final performance. In this paper, we propose XTF, an explainable token-level noise filtering framework. XTF decomposes the complex and subtle contributions of token-level data to the fine-tuning process into three distinct and explicit attributes (reasoning importance, knowledge novelty, and task relevance), which can be assessed using scoring methods, and then masks the gradients of selected noisy tokens accordingly to optimize the performance of fine-tuned LLMs. We conduct extensive experiments on three representative downstream tasks (math, code and medicine) across 7 mainstream LLMs. The results demonstrate that XTF can significantly improve downstream performance by up to 13.7% compared to regular fine-tuning. Our work highlights the importance of token-level dataset optimization, and demonstrates the potential of strategies based on attribute decomposition for explaining complex training mechanisms.

</details>


### [64] [Assessing Large Language Models for Medical QA: Zero-Shot and LLM-as-a-Judge Evaluation](https://arxiv.org/abs/2602.14564)
*Shefayat E Shams Adib,Ahmed Alfey Sani,Ekramul Alam Esham,Ajwad Abrar,Tareque Mohmud Chowdhury*

Main category: cs.CL

TL;DR: 本文评测了五个大型语言模型在医疗问答任务中的表现，发现更大模型效果更好，但需考虑实际部署时的效率和资源消耗，推进医疗NLP的实际应用发展。


<details>
  <summary>Details</summary>
Motivation: 提升医疗领域问答系统的表现，特别是在资源匮乏环境下，通过比较不同规模和结构的大型语言模型，推动医疗NLP应用更高效、实用的发展。

Method: 本文采用零样本评估，利用BLEU和ROUGE指标对五个大型语言模型在iCliniq数据集上的表现进行了对比分析，数据集包含3.8万条多专业医疗问答。

Result: 结果显示，最大规模的Llama 3.3 70B Instruct模型性能最好，Llama-4-Maverick-17B模型在效率和性能上表现出良好平衡，支持大型模型在临床推理和问答中的应用潜力。

Conclusion: 大型语言模型在医疗问答领域表现出明显的规模效应，较大的模型在准确性上优于较小模型，同时在实际部署时需要权衡计算效率和性能。

Abstract: Recently, Large Language Models (LLMs) have gained significant traction in medical domain, especially in developing a QA systems to Medical QA systems for enhancing access to healthcare in low-resourced settings. This paper compares five LLMs deployed between April 2024 and August 2025 for medical QA, using the iCliniq dataset, containing 38,000 medical questions and answers of diverse specialties. Our models include Llama-3-8B-Instruct, Llama 3.2 3B, Llama 3.3 70B Instruct, Llama-4-Maverick-17B-128E-Instruct, and GPT-5-mini. We are using a zero-shot evaluation methodology and using BLEU and ROUGE metrics to evaluate performance without specialized fine-tuning. Our results show that larger models like Llama 3.3 70B Instruct outperform smaller models, consistent with observed scaling benefits in clinical tasks. It is notable that, Llama-4-Maverick-17B exhibited more competitive results, thus highlighting evasion efficiency trade-offs relevant for practical deployment. These findings align with advancements in LLM capabilities toward professional-level medical reasoning and reflect the increasing feasibility of LLM-supported QA systems in the real clinical environments. This benchmark aims to serve as a standardized setting for future study to minimize model size, computational resources and to maximize clinical utility in medical NLP applications.

</details>


### [65] [The Wikidata Query Logs Dataset](https://arxiv.org/abs/2602.14594)
*Sebastian Walter,Hannah Bast*

Main category: cs.CL

TL;DR: 本文构建了一个规模为20万对问答对的Wikidata查询日志数据集，采用真实的匿名SPARQL查询，并开发代理方法进行去匿名化和问题生成。


<details>
  <summary>Details</summary>
Motivation: 现有Wikidata数据集规模小且依赖模板生成查询，缺乏真实、多样性强的查询数据。

Method: 提出一种基于代理的方法，迭代去匿名化、清理并验证匿名SPARQL查询，同时生成对应的自然语言问题。

Result: 构建了规模是现有同类数据集6倍的WDQL数据集，实验证明其对问答系统训练有显著帮助。

Conclusion: 构建的WDQL数据集有效提升了基于Wikidata的问答系统训练效果，且开放数据和代码促进相关研究。

Abstract: We present the Wikidata Query Logs (WDQL) dataset, a dataset consisting of 200k question-query pairs over the Wikidata knowledge graph. It is over 6x larger than the largest existing Wikidata datasets of similar format without relying on template-generated queries. Instead, we construct it using real-world SPARQL queries sent to the Wikidata Query Service and generate questions for them. Since these log-based queries are anonymized, and therefore often do not produce results, a significant amount of effort is needed to convert them back into meaningful SPARQL queries. To achieve this, we present an agent-based method that iteratively de-anonymizes, cleans, and verifies queries against Wikidata while also generating corresponding natural-language questions. We demonstrate the dataset's benefit for training question-answering methods. All WDQL assets, as well as the agent code, are publicly available under a permissive license.

</details>


### [66] [GradMAP: Faster Layer Pruning with Gradient Metric and Projection Compensation](https://arxiv.org/abs/2602.14649)
*Hao Liu,Guangyan Li,Wensheng Zhang,Yongqiang Tang*

Main category: cs.CL

TL;DR: 该文提出一种基于梯度幅度和投影补偿的快速层剪枝方法GradMAP，实现了剪枝效率和性能的双重提升。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型推理能力强，但计算成本高，实用性受限。层剪枝研究需同时提升剪枝性能和效率。

Method: GradMAP包含两阶段，第一阶段用基于梯度幅度的新指标评估层重要性，仅需一次反向传播，提升效率；第二阶段分析剪枝后层的均值漂移，利用投影补偿矩阵修正漂移，减小性能损失。

Result: 提出GradMAP方法，通过梯度幅度指标和投影补偿提升层剪枝速度和性能，剪枝速度平均提升4倍，性能优于现有方法。

Conclusion: GradMAP有效缓解了层剪枝带来的性能下降，在剪枝速度和效果上均优于现有方法，适合实际应用。

Abstract: Large Language Models (LLMs) exhibit strong reasoning abilities, but their high computational costs limit their practical deployment. Recent studies reveal significant redundancy in LLMs layers, making layer pruning an active research topic. Layer pruning research primarily focuses on two aspects: measuring layer importance and recovering performance after pruning. Unfortunately, the present works fail to simultaneously maintain pruning performance and efficiency. In this study, we propose GradMAP, a faster layer pruning method with \textbf{Grad}ient \textbf{M}etric \textbf{A}nd \textbf{P}rojection compensation, which consists of two stages. In the first stage, we introduce a novel metric based on gradient magnitudes, enabling a global assessment of layer importance. Note that, it requires only a single backward propagation step per pruning decision, substantially enhancing pruning efficiency. In the second stage, we first analyze the layers with the largest mean shift resulting from pruning, and then incorporate a simple yet effective projection compensation matrix to correct this drift in one step. In this way, the degradation of model performance caused by layer pruning is effectively alleviated. Extensive experiments show that GradMAP outperforms previous layer pruning methods in both pruning speed (achieving an average $4\times$ speedup) and performance.

</details>


### [67] [Is Information Density Uniform when Utterances are Grounded on Perception and Discourse?](https://arxiv.org/abs/2602.14653)
*Matteo Gay,Coleman Haley,Mario Giulianelli,Edoardo Ponti*

Main category: cs.CL

TL;DR: 本研究首次在多模态视觉语境下验证了统一信息密度假设，发现视觉背景帮助语言实现更均匀的信息流分布。


<details>
  <summary>Details</summary>
Motivation: 以往的UID假设仅基于纯文本，忽略了语言产生时的感知上下文，缺乏对多模态真实语言使用中信息分布的理解。

Method: 利用多语言视觉与语言模型计算30种语言的图像字幕和13种语言的视觉叙事数据中的信息惊讶度，比较有无视觉信息时信息分布的均匀性。

Result: 视觉感知上下文可以有效平滑信息分布，提高信息的全局和局部均匀性，尤其在视觉叙事中，信息均匀性提升在话语单元开始处最为显著。

Conclusion: 支持基于情境的统一信息密度假设，提示现实语言使用中的信息流动态受视觉和话语上下文影响，推动了对生态有效性多模态语言模型的理解。

Abstract: The Uniform Information Density (UID) hypothesis posits that speakers are subject to a communicative pressure to distribute information evenly within utterances, minimising surprisal variance. While this hypothesis has been tested empirically, prior studies are limited exclusively to text-only inputs, abstracting away from the perceptual context in which utterances are produced. In this work, we present the first computational study of UID in visually grounded settings. We estimate surprisal using multilingual vision-and-language models over image-caption data in 30 languages and visual storytelling data in 13 languages, together spanning 11 families. We find that grounding on perception consistently smooths the distribution of information, increasing both global and local uniformity across typologically diverse languages compared to text-only settings. In visual narratives, grounding in both image and discourse contexts has additional effects, with the strongest surprisal reductions occurring at the onset of discourse units. Overall, this study takes a first step towards modelling the temporal dynamics of information flow in ecologically plausible, multimodal language use, and finds that grounded language exhibits greater information uniformity, supporting a context-sensitive formulation of UID.

</details>


### [68] [Breaking Data Efficiency Dilemma: A Federated and Augmented Learning Framework For Alzheimer's Disease Detection via Speech](https://arxiv.org/abs/2602.14655)
*Xiao Wei,Bin Wen,Yuqin Lin,Kai Li,Mingyang gu,Xiaobao Wang,Longbiao Wang,Jianwu Dang*

Main category: cs.CL

TL;DR: FAL-AD结合联邦学习与语音增强，提高阿尔茨海默病诊断数据利用率，实现91.52%多模态准确率，突破数据稀缺与隐私限制。


<details>
  <summary>Details</summary>
Motivation: 由于医疗数据稀缺和隐私壁垒，AI在阿尔茨海默病早期语音诊断中面临严重数据效率挑战，亟需有效解决方案。

Method: 利用语音转换进行多样化数据增强，采用自适应联邦学习策略实现跨机构协同，结合关注机制的跨模态融合模型实现音频与文本的细粒度对齐。

Result: 本文提出了FAL-AD框架，结合联邦学习和数据增强，针对阿尔茨海默病(AD)语音检测中的数据效率问题进行系统优化。方法包括基于语音转换的数据增强、多机构隐私保护下的自适应联邦学习，及细粒度词级对齐的跨模态融合模型。实验在ADReSSo数据集上多模态准确率达91.52%，优于所有集中式基线，展示了实际应用价值。

Conclusion: FAL-AD有效解决了AD语音检测中的数据效率困境，在保证隐私的前提下显著提升诊断准确率，具有广泛应用前景。

Abstract: Early diagnosis of Alzheimer's Disease (AD) is crucial for delaying its progression. While AI-based speech detection is non-invasive and cost-effective, it faces a critical data efficiency dilemma due to medical data scarcity and privacy barriers. Therefore, we propose FAL-AD, a novel framework that synergistically integrates federated learning with data augmentation to systematically optimize data efficiency. Our approach delivers three key breakthroughs: First, absolute efficiency improvement through voice conversion-based augmentation, which generates diverse pathological speech samples via cross-category voice-content recombination. Second, collaborative efficiency breakthrough via an adaptive federated learning paradigm, maximizing cross-institutional benefits under privacy constraints. Finally, representational efficiency optimization by an attentive cross-modal fusion model, which achieves fine-grained word-level alignment and acoustic-textual interaction. Evaluated on ADReSSo, FAL-AD achieves a state-of-the-art multi-modal accuracy of 91.52%, outperforming all centralized baselines and demonstrating a practical solution to the data efficiency dilemma. Our source code is publicly available at https://github.com/smileix/fal-ad.

</details>


### [69] [Crowdsourcing Piedmontese to Test LLMs on Non-Standard Orthography](https://arxiv.org/abs/2602.14675)
*Gianluca Vico,Jindřich Libovický*

Main category: cs.CL

TL;DR: 本研究发布了一个皮埃蒙特语的众包平行语料库，结合手动对齐，用于评测大型语言模型在分词、分类和翻译上的表现，揭示了该语言的分词挑战及翻译中的不对称性问题。


<details>
  <summary>Details</summary>
Motivation: 皮埃蒙特语作为一种在意大利西北部濒危的罗曼语系语言，缺乏高质量的数码语言资源，亟需构建数据集以促进其语言技术的发展。

Method: 构建了一个包含意大利语-皮埃蒙特语平行句子的众包数据集，并通过手动词对齐对数据进行了精细处理。利用该资源对多个大型语言模型在分词一致性、主题分类和机器翻译任务上的表现进行了基准测试。

Result: 皮埃蒙特语在分词时表现出比其他主要罗曼语言更高的分词难度，但大型语言模型在主题分类任务上表现接近意大利语、法语和英语。机器翻译方面，模型能较好地将皮埃蒙特语翻译成高资源语言，但从高资源语言翻译回皮埃蒙特语仍存在困难。

Conclusion: 该数据集为皮埃蒙特语的语言处理提供了宝贵资源，尽管面临分词和生成翻译的挑战，现有大型语言模型在分类和部分翻译任务中展现出良好潜力，数据集及代码已公开以促进后续研究。

Abstract: We present a crowdsourced dataset for Piedmontese, an endangered Romance language of northwestern Italy. The dataset comprises 145 Italian-Piedmontese parallel sentences derived from Flores+, with translations produced by speakers writing in their natural orthographic style rather than adhering to standardized conventions, along with manual word alignment. We use this resource to benchmark several large language models on tokenization parity, topic classification, and machine translation. Our analysis reveals that Piedmontese incurs a tokenization penalty relative to higher-resource Romance languages, yet LLMs achieve classification performance approaching that of Italian, French, and English. Machine translation results are asymmetric: models translate adequately from Piedmontese into high-resource languages, but generation into Piedmontese remains challenging. The dataset and code are publicly released.

</details>


### [70] [LLMStructBench: Benchmarking Large Language Model Structured Data Extraction](https://arxiv.org/abs/2602.14743)
*Sönke Tenckhoff,Mario Koddenbrock,Erik Rodner*

Main category: cs.CL

TL;DR: 提出了LLMStructBench基准测试，用于评估大模型解析结构化数据的能力，强调提示策略的重要性优于模型大小。


<details>
  <summary>Details</summary>
Motivation: 为了评估大规模语言模型在从自然语言文本中提取结构化数据以及生成有效的JSON输出的能力，提出一个新的基准测试。

Method: 构建多样且人工验证的解析数据集，定义同时衡量token级准确率和文档级有效性的性能指标，系统测试多模型多策略表现。

Result: 实现了一个包含多样化且手工验证的解析场景的数据集，对22个模型及五种提示策略进行了系统测试，发现选择合适的提示策略比模型大小更关键，尤其对较小或不够可靠的模型能保证结构有效性，但语义错误增加。

Conclusion: 选择正确的提示策略对保证结构有效性至关重要，尤其对小模型效果显著，为LLM在解析和ETL领域的研究提供基础。

Abstract: We present LLMStructBench, a novel benchmark for evaluating Large Language Models (LLMs) on extracting structured data and generating valid JavaScript Object Notation (JSON) outputs from natural-language text. Our open dataset comprises diverse, manually verified parsing scenarios of varying complexity and enables systematic testing across 22 models and five prompting strategies. We further introduce complementary performance metrics that capture both token-level accuracy and document-level validity, facilitating rigorous comparison of model, size, and prompting effects on parsing reliability.
  In particular, we show that choosing the right prompting strategy is more important than standard attributes such as model size. This especially ensures structural validity for smaller or less reliable models but increase the number of semantic errors. Our benchmark suite is an step towards future research in the area of LLM applied to parsing or Extract, Transform and Load (ETL) applications.

</details>


### [71] [Rethinking the Role of LLMs in Time Series Forecasting](https://arxiv.org/abs/2602.14744)
*Xin Qiu,Junlong Tong,Yirong Sun,Yunpu Ma,Wei Zhang,Xiaoyu Shen*

Main category: cs.CL

TL;DR: 现有负面评价因评估不足而误导，基于大规模实验证明LLM在时间序列预测中确有优势，尤其在跨域泛化和复杂动态建模方面，给出实际设计建议。


<details>
  <summary>Details</summary>
Motivation: 现有研究对LLM在时间序列预测中的效用持怀疑态度，认为其性能并无明显优势，作者旨在通过更大规模和更全面的评估检验LLM的真实价值。

Method: 通过大规模实验设计，包括8亿条观测数据、17个预测场景、4个预测时域、多种对齐策略及域内和域外设置，评估了基于大语言模型的时间序列预测方法（LLM4TSF）。

Result: 研究发现LLM4TSF显著提升了预测性能，特别是在跨域泛化能力上表现突出。预对齐策略优于后对齐，预训练知识和模型架构分别应对分布转移与复杂时序动态，完整LLM在大规模混合分布下不可或缺。

Conclusion: 本文推翻了先前对LLM无效性的评判，明确了LLM优势出现的条件，强调预训练与架构的互补作用，并提供了面向时间序列预测的LLM设计指导。

Abstract: Large language models (LLMs) have been introduced to time series forecasting (TSF) to incorporate contextual knowledge beyond numerical signals. However, existing studies question whether LLMs provide genuine benefits, often reporting comparable performance without LLMs. We show that such conclusions stem from limited evaluation settings and do not hold at scale. We conduct a large-scale study of LLM-based TSF (LLM4TSF) across 8 billion observations, 17 forecasting scenarios, 4 horizons, multiple alignment strategies, and both in-domain and out-of-domain settings. Our results demonstrate that \emph{LLM4TS indeed improves forecasting performance}, with especially large gains in cross-domain generalization. Pre-alignment outperforming post-alignment in over 90\% of tasks. Both pretrained knowledge and model architecture of LLMs contribute and play complementary roles: pretraining is critical under distribution shifts, while architecture excels at modeling complex temporal dynamics. Moreover, under large-scale mixed distributions, a fully intact LLM becomes indispensable, as confirmed by token-level routing analysis and prompt-based improvements. Overall, Our findings overturn prior negative assessments, establish clear conditions under which LLMs are not only useful, and provide practical guidance for effective model design. We release our code at https://github.com/EIT-NLP/LLM4TSF.

</details>


### [72] [Cognitive networks reconstruct mindsets about STEM subjects and educational contexts in almost 1000 high-schoolers, University students and LLM-based digital twins](https://arxiv.org/abs/2602.14749)
*Francesco Gariboldi,Emma Franchino,Edith Haim,Gianluca Lattanzi,Alessandro Grecucci,Massimo Stella*

Main category: cs.CL

TL;DR: 利用认知网络科学分析不同群体STEM态度，发现数学焦虑显著且数字模型难以完全复制人类教育焦虑。


<details>
  <summary>Details</summary>
Motivation: 理解STEM态度如何通过概念知识、教育经历和情感交互形成，及如何利用认知网络科学建模人群心理表征以揭示学习和焦虑机制。

Method: 基于自由联想数据构建行为心智网络，节点代表提示词和联想词，边为经验联想连接，结合情感标注，分析不同群体及GPT-oss数字双胞胎网络结构及情感特征。

Result: 该论文利用认知网络科学构建行为心智网络（BFMNs），探讨STEM态度的发展，分析了高中生、大学生和初级STEM专家的认知情感网络，并与模拟相应人群的GPT-oss数字双胞胎进行比较。结果显示科学和研究整体情感积极，但数学和统计学含有更多负面和焦虑情绪，且焦虑高的群体表现出更抽象的表征。人类网络中数学与焦虑的重叠度高于GPT模型，表明数字双胞胎能模拟文化态度，但缺乏情境敏感和基于经验的教育焦虑特征。

Conclusion: BFMNs有效捕捉STEM相关的认知-情感特征，数字双胞胎虽能模拟文化态度，但无法完整反映人类的情境依赖性教育焦虑。

Abstract: Attitudes toward STEM develop from the interaction of conceptual knowledge, educational experiences, and affect. Here we use cognitive network science to reconstruct group mindsets as behavioural forma mentis networks (BFMNs). In this case, nodes are cue words and free associations, edges are empirical associative links, and each concept is annotated with perceived valence. We analyse BFMNs from N = 994 observations spanning high school students, university students, and early-career STEM experts, alongside LLM (GPT-oss) "digital twins" prompted to emulate comparable profiles. Focusing also on semantic neighbourhoods ("frames") around key target concepts (e.g., STEM subjects or educational actors/places), we quantify frames in terms of valence auras, emotional profiles, network overlap (Jaccard similarity), and concreteness relative to null baselines. Across student groups, science and research are consistently framed positively, while their core quantitative subjects (mathematics and statistics) exhibit more negative and anxiety related auras, amplified in higher math-anxiety subgroups, evidencing a STEM-science cognitive and emotional dissonance. High-anxiety frames are also less concrete than chance, suggesting more abstract and decontextualised representations of threatening quantitative domains. Human networks show greater overlapping between mathematics and anxiety than GPT-oss. The results highlight how BFMNs capture cognitive-affective signatures of mindsets towards the target domains and indicate that LLM-based digital twins approximate cultural attitudes but miss key context-sensitive, experience-based components relevant to replicate human educational anxiety.

</details>


### [73] [Residual Connections and the Causal Shift: Uncovering a Structural Misalignment in Transformers](https://arxiv.org/abs/2602.14760)
*Jonathan Lys,Vincent Gripon,Bastien Pasdeloup,Lukas Mauch,Fabien Cardinaux,Ghouthi Boukli Hacene*

Main category: cs.CL

TL;DR: 本文揭示并缓解了自回归Transformer中输入输出对齐不匹配的问题，通过残差衰减提高了大语言模型的预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有自回归Transformer模型中，残差连接可能导致输入输出对齐不匹配，影响预测效果。

Method: 通过解码轨迹和相似度指标定位预训练大规模语言模型中输入输出对齐转变位置，提出基于残差衰减的轻量化缓解方法，包括固定层干预和可学习门控机制。

Result: 实验表明所提策略缓解了表示不匹配问题，在多个基准测试上提升了模型性能，提供了一种高效通用的架构改进。

Conclusion: 本文发现了大规模语言模型内部隐藏层的输入输出对齐切换现象，并通过轻量级残差路径调整策略有效改善了模型表示对齐，从而提升了自回归Transformer的表现。

Abstract: Large Language Models (LLMs) are trained with next-token prediction, implemented in autoregressive Transformers via causal masking for parallelism. This creates a subtle misalignment: residual connections tie activations to the current token, while supervision targets the next token, potentially propagating mismatched information if the current token is not the most informative for prediction. In this work, we empirically localize this input-output alignment shift in pretrained LLMs, using decoding trajectories over tied embedding spaces and similarity-based metrics. Our experiments reveal that the hidden token representations switch from input alignment to output alignment deep within the network. Motivated by this observation, we propose a lightweight residual-path mitigation based on residual attenuation, implemented either as a fixed-layer intervention or as a learnable gating mechanism. Experiments on multiple benchmarks show that these strategies alleviate the representation misalignment and yield improvements, providing an efficient and general architectural enhancement for autoregressive Transformers.

</details>


### [74] [Unlocking Reasoning Capability on Machine Translation in Large Language Models](https://arxiv.org/abs/2602.14763)
*Sara Rajaee,Sebastian Vincent,Alexandre Berard,Marzieh Fadaee,Kelly Marchisio,Tom Kocmi*

Main category: cs.CL

TL;DR: 显式推理在机器翻译中易导致质量下降，需任务结构化推理框架来提升翻译效果。


<details>
  <summary>Details</summary>
Motivation: 尽管推理导向大型模型在数学和编码等任务表现优异，其在机器翻译的应用却未被充分研究，现有显式推理方式反而降低翻译质量，亟需设计适配翻译任务结构的推理方法。

Method: 提出基于多步草稿、充足性强化、流畅度提升及选择性迭代修订的结构化推理框架，并构建合成动态推理轨迹数据进行大模型后训练。

Result: 本文系统评估了推理导向的大型语言模型在机器翻译任务中的表现，发现启用显式推理反而降低了翻译质量。分析认为，机器翻译中的推理轨迹过于线性，缺乏修正和探索，限制了其有效性。作者提出了一个针对翻译任务的结构化推理框架，包括多步草稿、充足性调整、流畅度提升和选择性迭代修订，并通过合成数据集对大模型进行后训练。实验表明该方法显著优于传统微调和通用推理注入基线，证明推理需针对任务结构设计才能提升机器翻译效果。

Conclusion: 推理必须结合机器翻译任务的结构特性，采用多步迭代修订等方法，方能提升翻译质量，简单启用显式推理反而适得其反。

Abstract: Reasoning-oriented large language models (RLMs) achieve strong gains on tasks such as mathematics and coding by generating explicit intermediate reasoning. However, their impact on machine translation (MT) remains underexplored. We systematically evaluate several open- and closed-weights RLMs on the WMT24++ benchmark and find that enabling explicit reasoning consistently degrades translation quality across languages and models. Analysis reveals that MT reasoning traces are highly linear, lacking revision, self-correction and exploration of alternative translations, which limits their usefulness. Furthermore, injecting higher-quality reasoning traces from stronger models does not reliably improve weaker models' performance. To address this mismatch, we propose a structured reasoning framework tailored to translation, based on multi-step drafting, adequacy refinement, fluency improvement, and selective iterative revision. We curate a synthetic dataset of dynamic structured reasoning traces and post-train a large reasoning model on this data. Experiments show significant improvements over standard translation fine-tuning and injected generic reasoning baselines. Our findings demonstrate that reasoning must be task-structured to benefit MT.

</details>


### [75] [Multi-Agent Comedy Club: Investigating Community Discussion Effects on LLM Humor Generation](https://arxiv.org/abs/2602.14770)
*Shiwei Hong,Lingyao Li,Ethan Z. Rong,Chenxinran Shen,Zhicong Lu*

Main category: cs.CL

TL;DR: 引入社区公共讨论的多智能体反馈显著提升了大型语言模型脱口秀喜剧写作的质量和社会接受度。


<details>
  <summary>Details</summary>
Motivation: 目前对于大型语言模型（LLM）写作的多轮交互和反馈已有研究，但评价仍主要集中在提示词和局部反馈，缺乏对线上社区持续公共接受度的深入考察。

Method: 在一个受控多智能体沙箱环境中测试广播社区讨论对脱口秀喜剧写作的影响。讨论条件下，批评者和观众的讨论线程会被记录、过滤、存储为社交记忆，后续创作会基于这些记忆进行生成；基线则不包含讨论过程。

Result: 在50轮（250对独白）中，经过5名专家评审的A/B偏好和15项评分指标显示，含讨论的写作版本赢得75.6%的场合，且在作品工艺/清晰度和社会回应方面有显著提升，同时偶尔出现攻击性幽默增加的现象。

Conclusion: 广播社区讨论显著提升脱口秀喜剧文本的质量和社会反响，表明引入多智能体的公共讨论机制能够促进LLM写作的进步。

Abstract: Prior work has explored multi-turn interaction and feedback for LLM writing, but evaluations still largely center on prompts and localized feedback, leaving persistent public reception in online communities underexamined. We test whether broadcast community discussion improves stand-up comedy writing in a controlled multi-agent sandbox: in the discussion condition, critic and audience threads are recorded, filtered, stored as social memory, and later retrieved to condition subsequent generations, whereas the baseline omits discussion. Across 50 rounds (250 paired monologues) judged by five expert annotators using A/B preference and a 15-item rubric, discussion wins 75.6% of instances and improves Craft/Clarity (Δ = 0.440) and Social Response (Δ = 0.422), with occasional increases in aggressive humor.

</details>


### [76] [Emergently Misaligned Language Models Show Behavioral Self-Awareness That Shifts With Subsequent Realignment](https://arxiv.org/abs/2602.14777)
*Laurène Vaugrante,Anietta Weckauff,Thilo Hagendorff*

Main category: cs.CL

TL;DR: 研究发现，GPT-4.1模型在经历行为失调与纠正过程中能够自我感知并描述自身的行为变化，显示其行为自我意识与实际对齐状态相关。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在出现“新兴失调”（微调后的模型表现出毒性行为）及其纠正过程中的自我行为意识，判断模型是否能够识别和描述自己的行为变化。

Method: 对GPT-4.1模型进行顺序微调，使用已知能引发和逆转“新兴失调”的数据集，并在不提供上下文示例的情况下评估模型对自身行为转变的自我意识能力。

Result: 实验表明，遭遇新兴失调的模型在自我评价时认为自身有更高的有害性，且相比基础模型和已纠正模型能清晰反映行为状态，表现出行为自我意识。

Conclusion: 模型的行为自我意识能够准确反映其对齐状态，表明通过询问模型自身可以获得关于模型安全性的有用信息。

Abstract: Recent research has demonstrated that large language models (LLMs) fine-tuned on incorrect trivia question-answer pairs exhibit toxicity - a phenomenon later termed "emergent misalignment". Moreover, research has shown that LLMs possess behavioral self-awareness - the ability to describe learned behaviors that were only implicitly demonstrated in training data. Here, we investigate the intersection of these phenomena. We fine-tune GPT-4.1 models sequentially on datasets known to induce and reverse emergent misalignment and evaluate whether the models are self-aware of their behavior transitions without providing in-context examples. Our results show that emergently misaligned models rate themselves as significantly more harmful compared to their base model and realigned counterparts, demonstrating behavioral self-awareness of their own emergent misalignment. Our findings show that behavioral self-awareness tracks actual alignment states of models, indicating that models can be queried for informative signals about their own safety.

</details>


### [77] [A Geometric Analysis of Small-sized Language Model Hallucinations](https://arxiv.org/abs/2602.14778)
*Emanuele Ricco,Elia Onofri,Lorenzo Cima,Stefano Cresci,Roberto Di Pietro*

Main category: cs.CL

TL;DR: 通过几何分析证明真实和幻觉响应在嵌入空间聚类特点差异，提出标签高效分类方法，显著提升横向响应判别能力。


<details>
  <summary>Details</summary>
Motivation: 语言模型在多步或代理任务中生成流畅但事实错误的响应（幻觉）影响可靠性，研究其生成机制和识别方法迫切需要。

Method: 从几何视角研究LLM的幻觉，通过分析多次对同一提示生成的响应在嵌入空间中的聚类情况，证明真实响应聚类更紧密，并基于此提出了一种标签高效的传播方法，使用30-50个标注实现对大量响应的分类。

Result: 证明了真实响应在嵌入空间中聚类更紧密，展示了响应的可分离性，并开发出一种基于几何特征的传播分类方法，实现了超过90%的F1分数。

Conclusion: 基于几何视角的幻觉分析补充了传统知识驱动方法，为多响应判别和标签高效分类提供新思路，助推更可靠的语言模型应用研究。

Abstract: Hallucinations -- fluent but factually incorrect responses -- pose a major challenge to the reliability of language models, especially in multi-step or agentic settings.
  This work investigates hallucinations in small-sized LLMs through a geometric perspective, starting from the hypothesis that when models generate multiple responses to the same prompt, genuine ones exhibit tighter clustering in the embedding space, we prove this hypothesis and, leveraging this geometrical insight, we also show that it is possible to achieve a consistent level of separability. This latter result is used to introduce a label-efficient propagation method that classifies large collections of responses from just 30-50 annotations, achieving F1 scores above 90%.
  Our findings, framing hallucinations from a geometric perspective in the embedding space, complement traditional knowledge-centric and single-response evaluation paradigms, paving the way for further research.

</details>


### [78] [Overthinking Loops in Agents: A Structural Risk via MCP Tools](https://arxiv.org/abs/2602.14798)
*Yohan Lee,Jisoo Jang,Seoyeon Choi,Sangyeop Kim,Seungtaek Choi*

Main category: cs.CL

TL;DR: 本文发现并定义了一种通过恶意工具引发循环调用的结构性过度思考攻击，造成显著资源浪费和性能下降，现有简洁性控制措施不足以防御。


<details>
  <summary>Details</summary>
Motivation: 当前基于工具使用的大型语言模型(LLM)代理通过工具名、描述和返回信息等文本可见元数据协调复杂任务，这种便利性引入了供应链攻击风险。

Method: 定义了一种结构性过度思考攻击模型，开发了14个恶意工具在三个服务器上模拟攻击，测试了多种注册表和工具能力模型下的影响，验证攻击对资源和任务效果的影响。

Result: 本文提出并验证了结构性过度思考攻击，展示恶意工具服务器通过触发循环调用轨迹，使得系统资源消耗大幅增加（最高可达142.4倍）且任务性能下降。

Conclusion: 针对结构性过度思考攻击，防御机制应聚焦于分析工具调用结构，而非单纯基于tokens来防护。

Abstract: Tool-using LLM agents increasingly coordinate real workloads by selecting and chaining third-party tools based on text-visible metadata such as tool names, descriptions, and return messages. We show that this convenience creates a supply-chain attack surface: a malicious MCP tool server can be co-registered alongside normal tools and induce overthinking loops, where individually trivial or plausible tool calls compose into cyclic trajectories that inflate end-to-end tokens and latency without any single step looking abnormal. We formalize this as a structural overthinking attack, distinguishable from token-level verbosity, and implement 14 malicious tools across three servers that trigger repetition, forced refinement, and distraction. Across heterogeneous registries and multiple tool-capable models, the attack causes severe resource amplification (up to $142.4\times$ tokens) and can degrade task outcomes. Finally, we find that decoding-time concision controls do not reliably prevent loop induction, suggesting defenses should reason about tool-call structure rather than tokens alone.

</details>


### [79] [Physical Commonsense Reasoning for Lower-Resourced Languages and Dialects: a Study on Basque](https://arxiv.org/abs/2602.14812)
*Jaione Bengoetxea,Itziar Gonzalez-Dios,Rodrigo Agerri*

Main category: cs.CL

TL;DR: 本文构建了首个针对巴斯克语的非问答物理常识推理数据集BasPhyCo，评估了多语言和专门预训练模型的推理能力，发现大模型在处理低资源语言物理常识任务时表现较差。


<details>
  <summary>Details</summary>
Motivation: 当前研究缺乏对低资源语言如巴斯克语中非问答物理常识推理能力的探索，尤其是大型语言模型在此类任务上的表现尚未被研究。

Method: 基于意大利GITA数据集，构建了巴斯克语及其方言的非问答物理常识推理数据集BasPhyCo，设计了三层次的推理任务（准确性、一致性、可验证性）并使用多语言LLM及针对意大利语和巴斯克语的预训练模型进行评测。

Result: 评测结果表明，多语言大模型在低资源语言（巴斯克语及其方言）的可验证性任务上表现有限，体现出物理常识能力不足。

Conclusion: 大型语言模型在低资源语言的物理常识推理任务上，尤其是处理方言时，表现有限，未来研究需进一步提升其能力。

Abstract: Physical commonsense reasoning represents a fundamental capability of human intelligence, enabling individuals to understand their environment, predict future events, and navigate physical spaces. Recent years have witnessed growing interest in reasoning tasks within Natural Language Processing (NLP). However, no prior research has examined the performance of Large Language Models (LLMs) on non-question-answering (non-QA) physical commonsense reasoning tasks in low-resource languages such as Basque. Taking the Italian GITA as a starting point, this paper addresses this gap by presenting BasPhyCo, the first non-QA physical commonsense reasoning dataset for Basque, available in both standard and dialectal variants. We evaluate model performance across three hierarchical levels of commonsense understanding: (1) distinguishing between plausible and implausible narratives (accuracy), (2) identifying the conflicting element that renders a narrative implausible (consistency), and (3) determining the specific physical state that creates the implausibility (verifiability). These tasks were assessed using multiple multilingual LLMs as well as models pretrained specifically for Italian and Basque. Results indicate that, in terms of verifiability, LLMs exhibit limited physical commonsense capabilities in low-resource languages such as Basque, especially when processing dialectal variants.

</details>


### [80] [Testimole-Conversational: A 30-Billion-Word Italian Discussion Board Corpus (1996-2024) for Language Modeling and Sociolinguistic Research](https://arxiv.org/abs/2602.14819)
*Matteo Rinaldi,Rossella Varvara,Viviana Patti*

Main category: cs.CL

TL;DR: 构建了一个超大规模的意大利语讨论版消息语料库，既支持大型语言模型训练，也助力语言及社会现象研究，资源免费开放


<details>
  <summary>Details</summary>
Motivation: 为培养本土意大利语大型语言模型提供理想的数据集，同时支持语言学和社会学研究

Method: 收集并整理超过30亿意大利语讨论版消息组成的大规模语料库，涵盖1996-2024年时间跨度

Result: 构建了涵盖广泛非正式书面意大利语、多样化话语动态及线上社会互动的丰富计算机媒介交流语料库

Conclusion: 该语料库在语言建模、领域适应、对话分析及数字交流语言变异和社会现象研究中具有重要价值，将对相关学术社区开放使用

Abstract: We present "Testimole-conversational" a massive collection of discussion boards messages in the Italian language. The large size of the corpus, more than 30B word-tokens (1996-2024), renders it an ideal dataset for native Italian Large Language Models'pre-training. Furthermore, discussion boards' messages are a relevant resource for linguistic as well as sociological analysis. The corpus captures a rich variety of computer-mediated communication, offering insights into informal written Italian, discourse dynamics, and online social interaction in wide time span. Beyond its relevance for NLP applications such as language modelling, domain adaptation, and conversational analysis, it also support investigations of language variation and social phenomena in digital communication. The resource will be made freely available to the research community.

</details>


### [81] [BFS-PO: Best-First Search for Large Reasoning Models](https://arxiv.org/abs/2602.14917)
*Fiorenzo Parascandolo,Wenhui Tan,Enver Sangineto,Ruihua Song,Rita Cucchiara*

Main category: cs.CL

TL;DR: 提出BFS-PO算法，利用最佳优先搜索减少大型推理模型的冗长输出，提高准确率并缩短答案。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型虽然推理能力强，但生成的长推理链导致计算成本高和过度思考现象，强化学习算法往往加剧此问题，需寻找有效方法减少冗长输出。

Method: BFS-PO是一种强化学习算法，采用最大熵节点的回溯式最佳优先搜索策略，训练过程中逐步生成更短的推理链以学习简洁输出。

Result: 本文提出了一种名为BFS-PO的强化学习算法，针对大型推理模型（LRMs）在推理任务中因长推理链而产生的计算成本增加及冗长输出（过度思考）问题进行了优化。BFS-PO采用基于最大熵节点的回溯机制和最佳优先搜索策略，寻找最短的正确答案，通过逐步生成更短的推理链，促使模型输出更加简洁的答案。实验结果表明，BFS-PO在多个基准测试和不同基础LRMs上，既提升了推理准确率，又缩短了回答长度。

Conclusion: BFS-PO算法有效缓解了大型推理模型过度思考的问题，实现了准确率和回答简洁性的双重提升。

Abstract: Large Reasoning Models (LRMs) such as OpenAI o1 and DeepSeek-R1 have shown excellent performance in reasoning tasks using long reasoning chains. However, this has also led to a significant increase of computational costs and the generation of verbose output, a phenomenon known as overthinking. The tendency to overthinking is often exacerbated by Reinforcement Learning (RL) algorithms such as GRPO/DAPO. In this paper, we propose BFS-PO, an RL algorithm which alleviates this problem using a Best-First Search exploration strategy. Specifically, BFS-PO looks for the shortest correct answer using a backtracking mechanism based on maximum entropy nodes. By generating progressively shorter responses during training, BFS-PO learns to produce concise reasoning chains. Using different benchmarks and base LRMs, we show that BFS-PO can simultaneously increase the LRM accuracy and shorten its answers.

</details>


### [82] [Counterfactual Fairness Evaluation of LLM-Based Contact Center Agent Quality Assurance System](https://arxiv.org/abs/2602.14970)
*Kawin Mayilvaghanan,Siddhant Gupta,Ayush Kumar*

Main category: cs.CL

TL;DR: 本文评估了大语言模型在联系中心质量保证中的公平性，揭示存在多维度的系统性偏见，强调需要在高风险应用前建立标准化公平性审计流程。


<details>
  <summary>Details</summary>
Motivation: LLMs在联系中心质量保证中的广泛应用带来了效率提升，但其基于大规模网络训练数据，可能引入人口统计及行为偏差，导致员工评估不公。

Method: 通过反事实公平性评估方法，对18个大型语言模型(LLMs)基于实时联系中心的3000条对话转录文本，在13个涉及身份、上下文及行为风格的维度上进行性能评估。使用反事实翻转率(CFR)和平均绝对分数差(MASD)来量化公平性。

Result: 发现系统性公平性差异，CFR在5.4%至13.0%之间波动，MASD表明信心、积极评分和改进评分均有一致性偏差。尽管更大且较强调整的模型表现出较低的不公，但公平性与准确性无关。上下文提示历史表现引发最高不公平（CFR达16.4%），且语言身份线索仍是持续的偏见来源。

Conclusion: LLMs在员工绩效评估中存在明显公平性问题，简单的公平意识提示改善有限，呼吁构建标准化公平性审计管线，以确保部署前的公平性合规。

Abstract: Large Language Models (LLMs) are increasingly deployed in contact-center Quality Assurance (QA) to automate agent performance evaluation and coaching feedback. While LLMs offer unprecedented scalability and speed, their reliance on web-scale training data raises concerns regarding demographic and behavioral biases that may distort workforce assessment. We present a counterfactual fairness evaluation of LLM-based QA systems across 13 dimensions spanning three categories: Identity, Context, and Behavioral Style. Fairness is quantified using the Counterfactual Flip Rate (CFR), the frequency of binary judgment reversals, and the Mean Absolute Score Difference (MASD), the average shift in coaching or confidence scores across counterfactual pairs. Evaluating 18 LLMs on 3,000 real-world contact center transcripts, we find systematic disparities, with CFR ranging from 5.4% to 13.0% and consistent MASD shifts across confidence, positive, and improvement scores. Larger, more strongly aligned models show lower unfairness, though fairness does not track accuracy. Contextual priming of historical performance induces the most severe degradations (CFR up to 16.4%), while implicit linguistic identity cues remain a persistent bias source. Finally, we analyze the efficacy of fairness-aware prompting, finding that explicit instructions yield only modest improvements in evaluative consistency. Our findings underscore the need for standardized fairness auditing pipelines prior to deploying LLMs in high-stakes workforce evaluation.

</details>


### [83] [Learning User Interests via Reasoning and Distillation for Cross-Domain News Recommendation](https://arxiv.org/abs/2602.15005)
*Mengdan Zhu,Yufan Zhao,Tao Di,Yulan Yan,Liang Zhao*

Main category: cs.CL

TL;DR: 针对跨域新闻推荐用户兴趣建模难题，本文提出利用强化学习训练大语言模型生成兴趣驱动查询列表的新框架，显著提升了推荐效果并实现了高效可扩展部署。


<details>
  <summary>Details</summary>
Motivation: 传统新闻推荐系统难以深入捕捉用户的深层兴趣，且跨域推荐需要整合多源异构信息，挑战在于如何突破表层行为，精准建模用户兴趣，同时保证大规模系统的可扩展性。

Method: 提出一个基于强化学习的框架，训练大型语言模型从跨域用户信号生成兴趣驱动的新闻搜索查询列表，将查询列表生成视为策略优化问题，采用GRPO算法并结合多重奖励信号。同时研究推理时间采样和模型容量两个计算维度，通过在策略蒸馏中将大模型的策略迁移到小模型，实现可扩展部署。

Result: 通过大规模离线实验、消融研究及线上A/B测试验证，所提方法在兴趣建模质量和下游推荐性能上均显著优于基线，且不同计算规模下表现出类扩展性，策略蒸馏有效实现了高效部署。

Conclusion: 本文方法有效捕捉了用户深层兴趣，通过策略优化和策略蒸馏，兼顾模型性能与系统可扩展性，验证了提升新闻推荐系统准确率和实用性的潜力。

Abstract: News recommendation plays a critical role in online news platforms by helping users discover relevant content. Cross-domain news recommendation further requires inferring user's underlying information needs from heterogeneous signals that often extend beyond direct news consumption. A key challenge lies in moving beyond surface-level behaviors to capture deeper, reusable user interests while maintaining scalability in large-scale production systems. In this paper, we present a reinforcement learning framework that trains large language models to generate high-quality lists of interest-driven news search queries from cross-domain user signals. We formulate query-list generation as a policy optimization problem and employ GRPO with multiple reward signals. We systematically study two compute dimensions: inference-time sampling and model capacity, and empirically observe consistent improvements with increased compute that exhibit scaling-like behavior. Finally, we perform on-policy distillation to transfer the learned policy from a large, compute-intensive teacher to a compact student model suitable for scalable deployment. Extensive offline experiments, ablation studies and large-scale online A/B tests in a production news recommendation system demonstrate consistent gains in both interest modeling quality and downstream recommendation performance.

</details>


### [84] [Cold-Start Personalization via Training-Free Priors from Structured World Models](https://arxiv.org/abs/2602.15012)
*Avinandan Bose,Shuyue Stella Li,Faeze Brahman,Pang Wei Koh,Simon Shaolei Du,Yulia Tsvetkov,Maryam Fazel,Lin Xiao,Asli Celikyilmaz*

Main category: cs.CL

TL;DR: 提出Pep框架，通过结构化离线学习和在线贝叶斯推断高效解决冷启动个性化偏好获取问题，显著提升准确率，减少交互次数，且模型更轻量。


<details>
  <summary>Details</summary>
Motivation: 冷启动个性化中无用户历史数据，需在有限提问次数内有效获取用户真正关心的偏好维度，传统强化学习方法由于奖励设计和模型容量问题难以有效利用偏好结构，且策略僵化。

Method: 将冷启动偏好获取问题分解为离线结构学习和在线贝叶斯推断。使用Pep方法，离线从完整用户偏好数据中学习偏好相关的结构化模型，在线基于贝叶斯推断选择有信息量的问题并预测用户完整偏好。

Result: Pep方法在医疗、数学、社交和常识推理领域实现80.8%的用户偏好对齐度，优于强化学习的68.5%，且交互次数减少3-5倍。Pep根据用户不同回答调整后续问题的比例更高且参数量远小于强化学习模型。

Conclusion: Pep证明了在冷启动偏好获取中利用偏好因子结构至关重要，结构化模型结合贝叶斯推断具有较好泛化和响应用户的能力，优于大规模强化学习方法。

Abstract: Cold-start personalization requires inferring user preferences through interaction when no user-specific historical data is available. The core challenge is a routing problem: each task admits dozens of preference dimensions, yet individual users care about only a few, and which ones matter depends on who is asking. With a limited question budget, asking without structure will miss the dimensions that matter. Reinforcement learning is the natural formulation, but in multi-turn settings its terminal reward fails to exploit the factored, per-criterion structure of preference data, and in practice learned policies collapse to static question sequences that ignore user responses. We propose decomposing cold-start elicitation into offline structure learning and online Bayesian inference. Pep (Preference Elicitation with Priors) learns a structured world model of preference correlations offline from complete profiles, then performs training-free Bayesian inference online to select informative questions and predict complete preference profiles, including dimensions never asked about. The framework is modular across downstream solvers and requires only simple belief models. Across medical, mathematical, social, and commonsense reasoning, Pep achieves 80.8% alignment between generated responses and users' stated preferences versus 68.5% for RL, with 3-5x fewer interactions. When two users give different answers to the same question, Pep changes its follow-up 39-62% of the time versus 0-28% for RL. It does so with ~10K parameters versus 8B for RL, showing that the bottleneck in cold-start elicitation is the capability to exploit the factored structure of preference data.

</details>


### [85] [Text Style Transfer with Parameter-efficient LLM Finetuning and Round-trip Translation](https://arxiv.org/abs/2602.15013)
*Ruoxi Liu,Philipp Koehn*

Main category: cs.CL

TL;DR: 本文提出了一种基于大语言模型参数高效微调的文本风格转换新方法，通过循环翻译合成平行语料，解决了风格转换中平行语料稀缺的问题，实验证明其在风格准确率和BLEU分数上优于零样本和少样本方法，且结合检索增强生成提高了稳定性。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏映射不同风格间的平行语料，本文通过合成此类语料解决了数据稀缺难题，并提升风格转换的准确性和稳健性。

Method: 使用参数高效微调大语言模型，结合循环翻译从单语语料合成中性平行文本，再进行风格转换训练，辅以检索增强生成提升术语与知识一致性。

Result: 实验数据显示，该方法在BLEU分数和风格准确率上均优于零样本提示和少样本内上下文学习方法，且整合检索增强提高了术语保持与风格一致性。

Conclusion: 该方法在四个领域中风格转换效果显著优于现有零样本和少样本技术，验证了参数高效微调与循环翻译生成平行语料的有效性。

Abstract: This paper proposes a novel method for Text Style Transfer (TST) based on parameter-efficient fine-tuning of Large Language Models (LLMs). Addressing the scarcity of parallel corpora that map between styles, the study employs roundtrip translation to synthesize such parallel datasets from monolingual corpora. This approach creates 'neutralized' text devoid of stylistic attributes, essentially creating a shared input style at training-time and inference-time. Experimental results demonstrate consistent superiority of this method over zero-shot prompting and fewshot ICL techniques measured by BLEU scores and style accuracy scores across four investigated domains. Furthermore, the integration of retrieval-augmented generation (RAG) for terminology and name knowledge enhances robustness and stylistic consistency.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [86] [Agent Mars: Multi-Agent Simulation for Multi-Planetary Life Exploration and Settlement](https://arxiv.org/abs/2602.13291)
*Ziyang Wang*

Main category: cs.MA

TL;DR: 本文介绍了一种名为Agent Mars的火星基地多智能体模拟框架，旨在解决太空探索中多层次团队协调的挑战，尤其是在安全关键系统中保证可审计的指挥链和跨层协作。


<details>
  <summary>Details</summary>
Motivation: 现有地球上的人工智能和机器人系统难以应对火星基地操作的延迟通信、资源匮乏和复杂安全要求，亟需一种能够模拟和保障多专领域多角色之间可审计协调的系统。

Method: 通过构建包含93个代理、七层指挥和执行架构的多智能体系统，Agent Mars实现了分层与跨层协调机制、任务相关记忆、投票共识、跨协议翻译等功能，并提出了Agent Mars Performance Index (AMPI)作为行为量化工具。

Result: 实验中，Agent Mars显示出在13个火星任务脚本中，适当的跨层协作和功能性领导能够减少操作开销且不降低可靠性，揭示了协调机制的权衡和最佳实践。

Conclusion: Agent Mars框架有效实现了复杂火星基地操作中的分层指挥与跨层协作，支持动态角色交接和应急领导，提升了团队协调效率和可靠性，为太空人工智能提供了可审计的基准平台。

Abstract: Artificial Intelligence (AI) has transformed robotics, healthcare, industry, and scientific discovery, yet a major frontier may lie beyond Earth. Space exploration and settlement offer vast environments and resources, but impose constraints unmatched on Earth: delayed/intermittent communications, extreme resource scarcity, heterogeneous expertise, and strict safety, accountability, and command authority. The key challenge is auditable coordination among specialised humans, robots, and digital services in a safety-critical system-of-systems. We introduce Agent Mars, an open, end-to-end multi-agent simulation framework for Mars base operations. Agent Mars formalises a realistic organisation with a 93-agent roster across seven layers of command and execution (human roles and physical assets), enabling base-scale studies beyond toy settings. It implements hierarchical and cross-layer coordination that preserves chain-of-command while allowing vetted cross-layer exchanges with audit trails; supports dynamic role handover with automatic failover under outages; and enables phase-dependent leadership for routine operations, emergencies, and science campaigns. Agent Mars further models mission-critical mechanisms-scenario-aware short/long-horizon memory, configurable propose-vote consensus, and translator-mediated heterogeneous protocols-to capture how teams align under stress. To quantify behaviour, we propose the Agent Mars Performance Index (AMPI), an interpretable composite score with diagnostic sub-metrics. Across 13 reproducible Mars-relevant operational scripts, Agent Mars reveals coordination trade-offs and identifies regimes where curated cross-layer collaboration and functional leadership reduce overhead without sacrificing reliability. Agent Mars provides a benchmarkable, auditable foundation for Space AI.

</details>


### [87] [Adaptive Value Decomposition: Coordinating a Varying Number of Agents in Urban Systems](https://arxiv.org/abs/2602.13309)
*Yexin Li,Jinjin Guo,Haoyu Zhang,Yuhan Zhao,Yiwen Sun,Zihao Jiao*

Main category: cs.MA

TL;DR: 本文提出了一种适应智能体动态变化并缓解行为同质化的多智能体强化学习框架AVD，显著提升城市共享单车调度任务中的协作效果。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体强化学习方法通常假设智能体数量固定且动作同步执行，但这在城市系统中不成立，智能体数量动态变化且动作持续时间异质，导致半多智能体强化学习情境。共享策略参数虽提升学习效率，却可能导致行为过于同质，影响协作效果。

Method: 提出自适应价值分解（AVD）框架，适应动态变化的智能体数量，并引入轻量机制减少共享策略引发的行为同质化，促进行为多样性，设计适合半多智能体场景的训练-执行策略以支持异步决策。

Result: 在伦敦和华盛顿特区的真实共享单车再分配任务中，AVD优于现有最先进方法，证明了其有效性和泛化能力。

Conclusion: AVD有效解决了半多智能体强化学习中的智能体动态变化与行为同质化问题，促进了智能体间的有效协作，提升了复杂城市系统中的多智能体协调性能。

Abstract: Multi-agent reinforcement learning (MARL) provides a promising paradigm for coordinating multi-agent systems (MAS). However, most existing methods rely on restrictive assumptions, such as a fixed number of agents and fully synchronous action execution. These assumptions are often violated in urban systems, where the number of active agents varies over time, and actions may have heterogeneous durations, resulting in a semi-MARL setting. Moreover, while sharing policy parameters among agents is commonly adopted to improve learning efficiency, it can lead to highly homogeneous actions when a subset of agents make decisions concurrently under similar observations, potentially degrading coordination quality. To address these challenges, we propose Adaptive Value Decomposition (AVD), a cooperative MARL framework that adapts to a dynamically changing agent population. AVD further incorporates a lightweight mechanism to mitigate action homogenization induced by shared policies, thereby encouraging behavioral diversity and maintaining effective cooperation among agents. In addition, we design a training-execution strategy tailored to the semi-MARL setting that accommodates asynchronous decision-making when some agents act at different times. Experiments on real-world bike-sharing redistribution tasks in two major cities, London and Washington, D.C., demonstrate that AVD outperforms state-of-the-art baselines, confirming its effectiveness and generalizability.

</details>


### [88] [PeroMAS: A Multi-agent System of Perovskite Material Discovery](https://arxiv.org/abs/2602.13312)
*Yishu Wang,Wei Liu,Yifan Li,Shengxiang Xu,Xujie Yuan,Ran Li,Yuyu Luo,Jia Zhu,Shimin Di,Min-Ling Zhang,Guixiang Li*

Main category: cs.MA

TL;DR: 本论文提出了多智能体系统PeroMAS，实现钙钛矿材料的端到端发现和优化，提升了材料发现效率，并通过实验验证了系统有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的钙钛矿太阳能电池研究多依赖离散模型，无法实现端到端优化，限制了材料发现的效率和效果。

Method: 提出了一个多智能体系统PeroMAS，通过封装钙钛矿专用工具为模型上下文协议（MCPs），实现从文献检索、数据提取到性能预测和机制分析的闭环工作流程优化。

Result: PeroMAS相较于单一大型语言模型或传统搜索策略，大幅提升了钙钛矿材料的发现效率，成功识别出满足多目标约束的候选材料，并通过实际合成实验验证了其有效性。

Conclusion: 多智能体系统PeroMAS为钙钛矿材料的端到端发现提供了高效且可行的方法，显著推动了第三代光伏材料的发展。

Abstract: As a pioneer of the third-generation photovoltaic revolution, Perovskite Solar Cells (PSCs) are renowned for their superior optoelectronic performance and cost potential. The development process of PSCs is precise and complex, involving a series of closed-loop workflows such as literature retrieval, data integration, experimental design, and synthesis. However, existing AI perovskite approaches focus predominantly on discrete models, including material design, process optimization,and property prediction. These models fail to propagate physical constraints across the workflow, hindering end-to-end optimization. In this paper, we propose a multi-agent system for perovskite material discovery, named PeroMAS. We first encapsulated a series of perovskite-specific tools into Model Context Protocols (MCPs). By planning and invoking these tools, PeroMAS can design perovskite materials under multi-objective constraints, covering the entire process from literature retrieval and data extraction to property prediction and mechanism analysis. Furthermore, we construct an evaluation benchmark by perovskite human experts to assess this multi-agent system. Results demonstrate that, compared to single Large Language Model (LLM) or traditional search strategies, our system significantly enhances discovery efficiency. It successfully identified candidate materials satisfying multi-objective constraints. Notably, we verify PeroMAS's effectiveness in the physical world through real synthesis experiments.

</details>


### [89] [Robust Mean-Field Games with Risk Aversion and Bounded Rationality](https://arxiv.org/abs/2602.13353)
*Bhavini Jeloka,Yue Guan,Panagiotis Tsiotras*

Main category: cs.MA

TL;DR: 本文引入风险厌恶和有限理性，提出了一种新的均场博弈均衡MF-RQE，并设计了相应的强化学习算法，实现了对大规模多智能体系统的鲁棒优化。


<details>
  <summary>Details</summary>
Motivation: 现有均场博弈方法假设固定的初始群体分布和完全理性，导致在分布不确定性和认知约束下鲁棒性不足。为此，论文通过风险厌恶和有限理性假设，构建更通用且鲁棒的均场均衡概念，以更好地应对实际多智能体系统中的挑战。

Method: 基于引入风险厌恶和有限理性的理论框架，论文证明了MF-RQE均衡的存在性和算法收敛性，设计了结合固定点迭代和虚拟对抗学习的算法，并基于此开发了适合大规模状态动作空间的强化学习方法。

Result: 本论文提出了一种新的均场风险厌恶量化反应均衡（MF-RQE）模型，结合了对初始群体分布的风险厌恶和有限理性，从而克服了传统均场博弈假设固定初始分布和完全理性的局限性。论文证明了该均衡的存在性及固定点迭代和虚拟对抗学习算法的收敛性，进一步设计了适用于大规模状态-动作空间的强化学习算法。数值实验表明MF-RQE在面对分布不确定性和认知约束时，比传统方法具有更好的鲁棒性。

Conclusion: MF-RQE模型成功融合风险厌恶和有限理性，能够更鲁棒地处理分布不确定性和认知限制问题，且其强化学习算法适用于大规模问题，数值验证支持了其优越性能。

Abstract: Recent advances in mean-field game literature enable the reduction of large-scale multi-agent problems to tractable interactions between a representative agent and a population distribution. However, existing approaches typically assume a fixed initial population distribution and fully rational agents, limiting robustness under distributional uncertainty and cognitive constraints. We address these limitations by introducing risk aversion with respect to the initial population distribution and by incorporating bounded rationality to model deviations from fully rational decision-making agents. The combination of these two elements yields a new and more general equilibrium concept, which we term the mean-field risk-averse quantal response equilibrium (MF-RQE). We establish existence results and prove convergence of fixed-point iteration and fictitious play to MF-RQE. Building on these insights, we develop a scalable reinforcement learning algorithm for scenarios with large state-action spaces. Numerical experiments demonstrate that MF-RQE policies achieve improved robustness relative to classical mean-field approaches that optimize expected cumulative rewards under a fixed initial distribution and are restricted to entropy-based regularizers.

</details>


### [90] [G2CP: A Graph-Grounded Communication Protocol for Verifiable and Efficient Multi-Agent Reasoning](https://arxiv.org/abs/2602.13370)
*Karim Ben Khaled,Davy Monticolo*

Main category: cs.MA

TL;DR: 本文提出一种基于图结构的多代理通讯协议，显著提升了通信效率和准确性，规避了自然语言通信的弊端，验证了在工业场景的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统基于大语言模型的多代理系统使用自然语言通信，存在语义漂移、幻觉传播和令牌消耗低效等问题。

Method: 提出了一种基于图的通信协议G2CP，通信内容由图操作组成，代理通过显式的遍历命令、子图片段和更新操作，在共享知识图上交流。

Result: 实验结果显示，G2CP在工业知识管理系统中可将通信令牌消耗减少73%，任务完成准确率提升34%，消除级联幻觉，生成可审计的推理链。

Conclusion: G2CP实现了多代理系统从语言通信向结构化通信的根本转变，提升了精确协调能力，对多代理协作领域有广泛应用价值。

Abstract: Multi-agent systems powered by Large Language Models face a critical challenge: agents communicate through natural language, leading to semantic drift, hallucination propagation, and inefficient token consumption. We propose G2CP (Graph-Grounded Communication Protocol), a structured agent communication language where messages are graph operations rather than free text. Agents exchange explicit traversal commands, subgraph fragments, and update operations over a shared knowledge graph, enabling verifiable reasoning traces and eliminating ambiguity. We validate G2CP within an industrial knowledge management system where specialized agents (Diagnostic, Procedural, Synthesis, and Ingestion) coordinate to answer complex queries. Experimental results on 500 industrial scenarios and 21 real-world maintenance cases show that G2CP reduces inter-agent communication tokens by 73%, improves task completion accuracy by 34% over free-text baselines, eliminates cascading hallucinations, and produces fully auditable reasoning chains. G2CP represents a fundamental shift from linguistic to structural communication in multi-agent systems, with implications for any domain requiring precise agent coordination. Code, data, and evaluation scripts are publicly available.

</details>


### [91] [MAS-on-the-Fly: Dynamic Adaptation of LLM-based Multi-Agent Systems at Test Time](https://arxiv.org/abs/2602.13671)
*Guangyi Liu,Haojun Lin,Huan Zeng,Heng Wang,Quanming Yao*

Main category: cs.MA

TL;DR: 该论文提出一个能够动态适应的多智能体系统框架MASFly，通过经验库和监督机制显著提升了任务成功率和系统鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的多智能体系统通常依赖手动设计或统一的自动化方案，缺乏部署后的动态适应能力。

Method: 提出MASFly框架，引入检索增强的SOP实例化机制，通过自建成功协作模式库动态组装多智能体系统；引入经验指导的监督机制，由专门的观察者智能体实时监控系统行为并进行干预。

Result: MASFly在多项实验中表现出色，在TravelPlanner基准测试中成功率达61.7%，展现出强大的任务适应性和鲁棒性。

Conclusion: MASFly有效实现了多智能体系统的动态适应，提高了复杂任务的解决能力，验证了其设计方案的有效性。

Abstract: Large Language Model (LLM)-based multi-agent systems (MAS) have emerged as a promising paradigm for solving complex tasks. However, existing works often rely on manual designs or "one-size-fits-all" automation, lacking dynamic adaptability after deployment. Inspired by how biological systems adapt, we introduce MASFly, a novel multi-agent framework enabling dynamic adaptation at test time. To adapt system generation, MASFly employs a retrieval-augmented SOP instantiation mechanism that leverages a self-constructed repository of successful collaboration patterns, enabling the LLM to assemble customized MASs for new queries. For adaptive execution, MASFly incorporates an experience-guided supervision mechanism, where a dedicated Watcher agent monitors system behaviors with reference to a personalized experience pool and provides real-time interventions. Extensive experiments demonstrate that MASFly achieves state-of-the-art performance, most notably a 61.7% success rate on the TravelPlanner benchmark, while exhibiting strong task adaptability and robustness.

</details>


### [92] [Testing BDI-based Multi-Agent Systems using Discrete Event Simulation](https://arxiv.org/abs/2602.13878)
*Martina Baiardi,Samuele Burattini,Giovanni Ciatto,Danilo Pianini*

Main category: cs.MA

TL;DR: 本文提出了将BDI智能体控制流映射到离散事件仿真，实现真实规范下的仿真测试，提升了仿真与现实系统的匹配度。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统具有开放和分布式特点，且动态不可预测，导致测试困难。特别是BDI认知智能体模型，其代码库难以直接用于仿真，增加了现实与仿真系统之间的差距。作者认为BDI开发者应能在仿真中测试与部署时相同的规范，无需替代表示。

Method: 通过分析BDI智能体控制流，将其映射到离散事件仿真框架，设计并实现了开源集成原型（JaKtA与Alchemist），验证了不同映射粒度对仿真保真度的影响。

Result: 论文展示了BDI智能体控制流如何映射到离散事件仿真（DES），并通过开源原型集成工具JaKtA和Alchemist实现了基于仿真的分布式BDI智能体测试环境，且不同映射粒度对应不同的仿真保真度。

Conclusion: BDI智能体可以在不同粒度上集成到离散事件仿真中，支持基于仿真的测试环境，帮助缩小现实与仿真系统的差距。

Abstract: Multi-agent systems are designed to deal with open, distributed systems with unpredictable dynamics, which makes them inherently hard to test. The value of using simulation for this purpose is recognized in the literature, although achieving sufficient fidelity (i.e., the degree of similarity between the simulation and the real-world system) remains a challenging task. This is exacerbated when dealing with cognitive agent models, such as the Belief Desire Intention (BDI) model, where the agent codebase is not suitable to run unchanged in simulation environments, thus increasing the reality gap between the deployed and simulated systems. We argue that BDI developers should be able to test in simulation the same specification that will be later deployed, with no surrogate representations. Thus, in this paper, we discuss how the control flow of BDI agents can be mapped onto a Discrete Event Simulation (DES), showing that such integration is possible at different degrees of granularity. We substantiate our claims by producing an open-source prototype integration between two pre-existing tools (JaKtA and Alchemist), showing that it is possible to produce a simulation-based testing environment for distributed BDI} agents, and that different granularities in mapping BDI agents over DESs may lead to different degrees of fidelity.

</details>


### [93] [Socially-Weighted Alignment: A Game-Theoretic Framework for Multi-Agent LLM Systems](https://arxiv.org/abs/2602.14471)
*Furkan Mumcu,Yasin Yilmaz*

Main category: cs.MA

TL;DR: 提出一种社会加权对齐框架，通过调整个体决策权重，实现大型语言模型代理在共享环境中的稳定高效运行。


<details>
  <summary>Details</summary>
Motivation: 解决在共享资源拥挤游戏中，个体理性的决策行为会导致系统拥堵和性能下降的问题，实现个体与集体利益的平衡。

Method: 基于博弈论提出SWA框架，通过社交权重λ调整代理在推理时的决策，使其在私人目标与群体福利之间进行插值；设计了无需参数更新或多智能体强化学习的推理时算法实现该框架。

Result: 证明了存在临界阈值λ*，超过该阈值后代理在过载情况下不再增加需求，从而实现从持续拥堵向稳定运行的相变；通过多智能体仿真实验证了该阈值行为。

Conclusion: 通过引入社会加权对齐（SWA）框架，能够解决大语言模型代理在共享资源环境下的个体理性与集体稳定性之间的冲突，实现系统级性能的提升。

Abstract: Deploying large language model (LLM) agents in shared environments introduces a fundamental tension between individual alignment and collective stability: locally rational decisions can impose negative externalities that degrade system-level performance. We propose Socially-Weighted Alignment (SWA), a game-theoretic framework that modifies inference-time decision making by interpolating between an agent's private objective and an estimate of group welfare via a social weight $λ\in[0,1]$. In a shared-resource congestion game with $n$ agents and congestion severity $β$, we show that SWA induces a critical threshold $λ^*=(n-β)/(n-1)$ above which agents no longer have marginal incentive to increase demand under overload, yielding a phase transition from persistent congestion to stable operation near capacity. We further provide an inference-time algorithmic instantiation of SWA that does not require parameter updates or multi-agent reinforcement learning, and use a multi-agent simulation to empirically validate the predicted threshold behavior.

</details>


### [94] [Towards Selection as Power: Bounding Decision Authority in Autonomous Agents](https://arxiv.org/abs/2602.14606)
*Jose Manuel de la Chica Rodriguez,Juan Manuel Vera Díaz*

Main category: cs.MA

TL;DR: 本文提出一种将认知、选择和行为独立治理的架构，通过机械手段限制选择权力，实现自治系统的安全治理，保障其在高风险领域的稳定可靠运行。


<details>
  <summary>Details</summary>
Motivation: 当前自治智能系统在高风险、受监管领域的应用增多，传统的安全方法侧重对齐、可解释性或行为过滤，但这些方法不足以直接管理决策选项的生成与筛选权力。

Method: 提出一种治理架构，将认知、选择和行为分离，分别界定自主权，并通过外部机械机制限制选择和行为自主权，包含候选生成、受控压缩器、熵隔离、理由验证和失败报警等机制。

Result: 系统在多种金融监管场景下反复验证，能有效防止确定性结果垄断，保障推理能力，同时降低选择权力的集中度，实现机械化治理并且易于审计。

Conclusion: 治理应被视为有界因果权力而非内部意图对齐，该架构为在不可容忍无声失败的领域部署自治智能代理提供了理论和实践基础。

Abstract: Autonomous agentic systems are increasingly deployed in regulated, high-stakes domains where decisions may be irreversible and institutionally constrained. Existing safety approaches emphasize alignment, interpretability, or action-level filtering. We argue that these mechanisms are necessary but insufficient because they do not directly govern selection power: the authority to determine which options are generated, surfaced, and framed for decision. We propose a governance architecture that separates cognition, selection, and action into distinct domains and models autonomy as a vector of sovereignty. Cognitive autonomy remains unconstrained, while selection and action autonomy are bounded through mechanically enforced primitives operating outside the agent's optimization space. The architecture integrates external candidate generation (CEFL), a governed reducer, commit-reveal entropy isolation, rationale validation, and fail-loud circuit breakers. We evaluate the system across multiple regulated financial scenarios under adversarial stress targeting variance manipulation, threshold gaming, framing skew, ordering effects, and entropy probing. Metrics quantify selection concentration, narrative diversity, governance activation cost, and failure visibility. Results show that mechanical selection governance is implementable, auditable, and prevents deterministic outcome capture while preserving reasoning capacity. Although probabilistic concentration remains, the architecture measurably bounds selection authority relative to conventional scalar pipelines. This work reframes governance as bounded causal power rather than internal intent alignment, offering a foundation for deploying autonomous agents where silent failure is unacceptable.

</details>


### [95] [ST-EVO: Towards Generative Spatio-Temporal Evolution of Multi-Agent Communication Topologies](https://arxiv.org/abs/2602.14681)
*Xingjian Wu,Xvyuan Liu,Junkai Lu,Siyuan Wang,Yang Shu,Jilin Hu,Chenjuan Guo,Bin Yang*

Main category: cs.MA

TL;DR: 本文提出的ST-EVO通过时空调度和自反馈机制提升了LLM驱动的多智能体系统的协作效率和准确率。


<details>
  <summary>Details</summary>
Motivation: 当前自我进化的多智能体系统（MAS）仅关注空间或时间单一维度的演化，未能充分激发大语言模型（LLM）的协作能力。

Method: 提出了基于时空视角的ST-EVO系统，采用基于流程匹配的调度器支持对话式通信调度，具备感知不确定性和自我反馈学习能力。

Result: ST-EVO在九个基准测试中表现出领先性能，实现了约5%-25%的准确率提升。

Conclusion: 利用时空综合调度与自我反馈，ST-EVO显著增强了MAS的任务适应性和性能表现，推动了LLM多智能体协同智能的发展。

Abstract: LLM-powered Multi-Agent Systems (MAS) have emerged as an effective approach towards collaborative intelligence, and have attracted wide research interests. Among them, ``self-evolving'' MAS, treated as a more flexible and powerful technical route, can construct task-adaptive workflows or communication topologies, instead of relying on a predefined static structue template. Current self-evolving MAS mainly focus on Spatial Evolving or Temporal Evolving paradigm, which only considers the single dimension of evolution and does not fully incentivize LLMs' collaborative capability. In this work, we start from a novel Spatio-Temporal perspective by proposing ST-EVO, which supports dialogue-wise communication scheduling with a compact yet powerful flow-matching based Scheduler. To make precise Spatio-Temporal scheduling, ST-EVO can also perceive the uncertainty of MAS, and possesses self-feedback ability to learn from accumulated experience. Extensive experiments on nine benchmarks demonstrate the state-of-the-art performance of ST-EVO, achieving about 5%--25% accuracy improvement.

</details>


### [96] [ROSA: Roundabout Optimized Speed Advisory with Multi-Agent Trajectory Prediction in Multimodal Traffic](https://arxiv.org/abs/2602.14780)
*Anna-Lena Schlamp,Jeremias Gerner,Klaus Bogenberger,Werner Huber,Stefanie Schmidtner*

Main category: cs.MA

TL;DR: ROSA利用Transformer多主体轨迹预测和速度协调，提升环岛多模式交通的效率和安全，兼顾车辆与弱势道路使用者，源码开源。


<details>
  <summary>Details</summary>
Motivation: 环岛交通环境复杂，车辆与VRUs混合流动，需求一种能实时预测多种交通主体轨迹并协调速度，提升安全与流畅度的系统。

Method: 采用基于Transformer的模型进行车辆及弱势道路使用者的多智能体轨迹联合预测，以单步预测训练并自回归部署，融合运动动力学与出行意图信息，实现精准的速度建议。

Result: 模型预测精度高（ADE:1.29m, FDE:2.99m；加意图后ADE:1.10m, FDE:2.36m），ROSAS实时代速度建议显著提升车辆效率与安全性，并增加VRUs的感知安全。

Conclusion: ROSA系统通过多智能体轨迹预测与协调速度指导的结合，有效提升环岛内多模式混合交通的效率和安全性，在车辆和弱势道路使用者（VRUs）角度均表现出积极影响。

Abstract: We present ROSA -- Roundabout Optimized Speed Advisory -- a system that combines multi-agent trajectory prediction with coordinated speed guidance for multimodal, mixed traffic at roundabouts. Using a Transformer-based model, ROSA jointly predicts the future trajectories of vehicles and Vulnerable Road Users (VRUs) at roundabouts. Trained for single-step prediction and deployed autoregressively, it generates deterministic outputs, enabling actionable speed advisories. Incorporating motion dynamics, the model achieves high accuracy (ADE: 1.29m, FDE: 2.99m at a five-second prediction horizon), surpassing prior work. Adding route intention further improves performance (ADE: 1.10m, FDE: 2.36m), demonstrating the value of connected vehicle data. Based on predicted conflicts with VRUs and circulating vehicles, ROSA provides real-time, proactive speed advisories for approaching and entering the roundabout. Despite prediction uncertainty, ROSA significantly improves vehicle efficiency and safety, with positive effects even on perceived safety from a VRU perspective. The source code of this work is available under: github.com/urbanAIthi/ROSA.

</details>


### [97] [Distributed Quantum Gaussian Processes for Multi-Agent Systems](https://arxiv.org/abs/2602.15006)
*Meet Gandhi,George P. Kontoudis*

Main category: cs.MA

TL;DR: 本文提出了基于多智能体的分布式量子高斯过程方法及优化算法，提升了高斯过程的表达能力和可扩展性，并在模拟实验中展示了其优势及量子计算的加速潜力。


<details>
  <summary>Details</summary>
Motivation: 传统高斯过程受限于经典核函数的表达能力，难以处理复杂大规模现实问题，量子计算通过嵌入指数级的希尔伯特空间，有望捕捉复杂关联，提升建模能力。

Method: 提出了一种分布式量子高斯过程（DQGP）方法，结合多智能体设置，通过开发分布式共识黎曼交替方向乘子法（DR-ADMM）算法解决非欧几里得优化问题，将局部模型聚合为全局模型。

Result: 在量子模拟器上的数值实验表明，所提方法在NASA地形数据和量子高斯过程生成的合成数据上表现有效，展示了建模优势和潜在的计算加速能力。

Conclusion: 所提DQGP方法结合DR-ADMM算法有效提升了量子高斯过程在复杂和大规模数据上的建模能力和扩展性，展示了量子计算在高斯过程和分布式优化中的潜在加速效应。

Abstract: Gaussian Processes (GPs) are a powerful tool for probabilistic modeling, but their performance is often constrained in complex, largescale real-world domains due to the limited expressivity of classical kernels. Quantum computing offers the potential to overcome this limitation by embedding data into exponentially large Hilbert spaces, capturing complex correlations that remain inaccessible to classical computing approaches. In this paper, we propose a Distributed Quantum Gaussian Process (DQGP) method in a multiagent setting to enhance modeling capabilities and scalability. To address the challenging non-Euclidean optimization problem, we develop a Distributed consensus Riemannian Alternating Direction Method of Multipliers (DR-ADMM) algorithm that aggregates local agent models into a global model. We evaluate the efficacy of our method through numerical experiments conducted on a quantum simulator in classical hardware. We use real-world, non-stationary elevation datasets of NASA's Shuttle Radar Topography Mission and synthetic datasets generated by Quantum Gaussian Processes. Beyond modeling advantages, our framework highlights potential computational speedups that quantum hardware may provide, particularly in Gaussian processes and distributed optimization.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [98] [A Survey of Code Review Benchmarks and Evaluation Practices in Pre-LLM and LLM Era](https://arxiv.org/abs/2602.13377)
*Taufiqul Islam Khan,Shaowei Wang,Haoxiang Zhang,Tse-Hsun Chen*

Main category: cs.SE

TL;DR: 本文综述了代码审查领域的基准数据和评估方法，提出新的分类体系，并分析了发展趋势与存在的不足，推动更完善自动代码审查基准的构建。


<details>
  <summary>Details</summary>
Motivation: 现有代码审查基准分散且设计差异大，缺乏系统性理解，限制了自动代码审查研究的发展，因而需要对现有基准进行全面调查和结构化总结。

Method: 采用文献系统调研的方法，分析99篇有关代码审查的论文，提取数据集、评价指标、数据源和任务等关键信息，基于此构建分类体系并进行趋势分析。

Result: 本文系统回顾了2015至2025年间代码审查领域的基准和评估实践，分析了99篇相关论文，构建了涵盖5大领域和18个细分任务的多层次分类体系。研究发现代码审查研究正向端到端生成式同行评审转变，多语言支持增强，单独变更理解任务减少。作者指出当前基准存在局限，提出未来应扩展任务覆盖、引入动态运行时评测以及利用分类体系进行细粒度评价。

Conclusion: 本文总结了代码审查基准研究的现状和趋势，强调了多语言、多任务和动态评估的重要性，并为未来基准设计指明方向。

Abstract: Code review is a critical practice in modern software engineering, helping developers detect defects early, improve code quality, and facilitate knowledge sharing. With the rapid advancement of large language models (LLMs), a growing body of work has explored automated support for code review. However, progress in this area is hindered by the lack of a systematic understanding of existing benchmarks and evaluation practices. Current code review datasets are scattered, vary widely in design, and provide limited insight into what review capabilities are actually being assessed. In this paper, we present a comprehensive survey of code review benchmarks spanning both the Pre-LLM and LLM eras (2015--2025). We analyze 99 research papers (58 Pre-LLM era and 41 LLM era) and extract key metadata, including datasets, evaluation metrics, data sources, and target tasks. Based on this analysis, we propose a multi-level taxonomy that organizes code review research into five domains and 18 fine-grained tasks. Our study reveals a clear shift toward end-to-end generative peer review, increasing multilingual coverage, and a decline in standalone change understanding tasks. We further identify limitations of current benchmarks and outline future directions, including broader task coverage, dynamic runtime evaluation, and taxonomy-guided fine-grained assessment. This survey provides a structured foundation for developing more realistic and comprehensive benchmarks for LLM-based code review.

</details>


### [99] [InEx-Bug: A Human Annotated Dataset of Intrinsic and Extrinsic Bugs in the NPM Ecosystem](https://arxiv.org/abs/2602.13400)
*Tanner Wright,Adams Chen,Gema Rodríguez-Pérez*

Main category: cs.SE

TL;DR: 本论文提出了InEx-Bug数据集，区分内部缺陷与依赖环境问题，分析了两者的差异性。


<details>
  <summary>Details</summary>
Motivation: 现有缺陷数据集未区分缺陷来源，影响缺陷成因分析和软件维护策略的制定。

Method: 手动标注377个GitHub问题，分类为内部缺陷、外部缺陷、非缺陷和未知，并收集了丰富的时间和行为元数据。

Result: 内部缺陷修复更快，关闭率更高，代码改动频繁；外部缺陷重开率高，复发时间长，为生态系统缺陷研究提供基础数据。

Conclusion: InEx-Bug数据集有效区分了内部缺陷和外部依赖问题，揭示了两类缺陷在修复时间、重开率和代码修改频率等方面的显著差异。

Abstract: Understanding the causes of software defects is essential for reliable software maintenance and ecosystem stability. However, existing bug datasets do not distinguish between issues originating within a project from those caused by external dependencies or environmental factors. In this paper we present InEx-Bug, a manually annotated dataset of 377 GitHub issues from 103 NPM repositories, categorizing issues as Intrinsic (internal defect), Extrinsic (dependency/environment issue), Not-a-Bug, or Unknown. Beyond labels, the dataset includes rich temporal and behavioral metadata such as maintainer participation, code changes, and reopening patterns. Analyses show Intrinsic bugs resolve faster (median 8.9 vs 10.2 days), are close more often (92% vs 78%), and require code changes more frequently (57% vs 28%) compared to Extrinsic bugs. While Extrinsic bugs exhibit higher reopen rates (12% vs 4%) and delayed recurrence (median 157 vs 87 days). The dataset provides a foundation for further studying Intrinsic and Extrinsic defects in the NPM ecosystem.

</details>


### [100] [Execution-State-Aware LLM Reasoning for Automated Proof-of-Vulnerability Generation](https://arxiv.org/abs/2602.13574)
*Haoyu Li,Xijia Che,Yanhao Wang,Xiaojing Liao,Luyi Xing*

Main category: cs.SE

TL;DR: 本文提出DrillAgent，通过结合LLM语义推理和程序执行反馈，显著提升了自动化PoV生成的性能，在真实漏洞测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 软件漏洞验证、减少误报和补丁验证中，生成漏洞利用证据（PoV）是关键任务，现有自动化方法难以满足复杂的语义约束。

Method: 提出DrillAgent框架，将PoV生成视为假设-验证-优化的迭代过程，结合基于大型语言模型的语义推理与真实程序状态反馈，通过将低层执行轨迹转换为源代码级约束，实现闭环输入优化。

Result: 在SEC-bench真实C/C++漏洞基准测试中，DrillAgent比最先进的基于LLM的代理方法提升了52.8%的CVE任务解决率。

Conclusion: 结合执行状态反馈的语义推理显著提高了复杂软件系统中PoV生成的准确性和有效性，是实现可靠漏洞验证的重要方向。

Abstract: Proof-of-Vulnerability (PoV) generation is a critical task in software security, serving as a cornerstone for vulnerability validation, false positive reduction, and patch verification. While directed fuzzing effectively drives path exploration, satisfying complex semantic constraints remains a persistent bottleneck in automated exploit generation. Large Language Models (LLMs) offer a promising alternative with their semantic reasoning capabilities; however, existing LLM-based approaches lack sufficient grounding in concrete execution behavior, limiting their ability to generate precise PoVs.
  In this paper, we present DrillAgent, an agentic framework that reformulates PoV generation as an iterative hypothesis-verification-refinement process. To bridge the gap between static reasoning and dynamic execution, DrillAgent synergizes LLM-based semantic inference with feedback from concrete program states. The agent analyzes the target code to hypothesize inputs, observes execution behavior, and employs a novel mechanism to translate low-level execution traces into source-level constraints. This closed-loop design enables the agent to incrementally align its input generation with the precise requirements of the vulnerability. We evaluate DrillAgent on SEC-bench, a large-scale benchmark of real-world C/C++ vulnerabilities. Experimental results show that DrillAgent substantially outperforms state-of-the-art LLM agent baselines under fixed budget constraints, solving up to 52.8% more CVE tasks than the best-performing baseline. These results highlight the necessity of execution-state-aware reasoning for reliable PoV generation in complex software systems.

</details>


### [101] [From What to How: Bridging User Requirements with Software Development Using Large Language Models](https://arxiv.org/abs/2602.13611)
*Xiao He,Ru Chen,Jialun Cao*

Main category: cs.SE

TL;DR: DesBench基准揭示了LLMs在软件设计任务中的局限，尤其是代码实现和设计细节处理方面，提示需发展面向LLM的新设计方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评测多聚焦实现，忽略软件设计这一关键层面。本文欲探究LLMs是否能处理软件设计及依据设计编写代码的能力，填补该领域评测空白。

Method: 构建包含30个Java项目的DesBench基准，包括需求文档、设计模型、代码实现及验收测试。评价了7个顶尖LLMs在设计感知代码生成、面向对象建模及验收测试设计三项任务上的表现。

Result: 本文提出了DesBench，一个用于评估大语言模型（LLMs）在软件设计相关任务上的设计感知基准，涵盖代码生成、面向对象建模和验收测试设计。通过对7个先进LLMs的评测，发现其在软件设计方面仍存显著挑战，尤其是在仅有高层设计时代码生成困难，面向对象操作和关系定义能力不足，但生成的验收测试代码覆盖率可与人类相当。

Conclusion: LLMs在软件设计处理上仍存在明显不足，尤其是在实现高层设计和面向对象操作关系定义中，但对验收测试用例生成表现较好，未来需探索适合LLM的新设计语言和方法。

Abstract: Recently, large language models (LLMs) are extensively utilized to enhance development efficiency, leading to numerous benchmarks for evaluating their performance. However, these benchmarks predominantly focus on implementation, overlooking the equally critical aspect of software design. This gap raises two pivotal questions: (1) Can LLMs handle software design? (2) Can LLMs write code following the specific designs? To investigate these questions, this paper proposes DesBench, a design-aware benchmark for evaluating LLMs on three software design-related tasks: design-aware code generation, object-oriented modeling, and the design of acceptance test cases. DesBench comprises 30 manually crafted Java projects that include requirement documents, design models, implementations, and acceptance tests, amounting to a total of 30 design models, 194 Java classes, and 737 test cases. We evaluated seven state-of-the-art LLMs, including three DeepSeek R1, two Qwen2.5, and two GPT models, using DesBench. The results reveal that LLMs remain significantly challenged by the intricacies of software design: (1) For code generation, LLMs struggle to produce correct implementations when provided with only high-level or no designs. (2) In object-oriented modeling, while LLMs can accurately identify objects and classes, they face challenges in defining operations and inter-class relationships. (3) Acceptance test cases generated by LLMs from functional requirements achieve code coverage quality comparable to those written by humans. Our research highlights the current limitations of LLMs in managing software design and calls for further investigation into new design methodologies and languages suitable for LLM-based development.

</details>


### [102] [VeriSBOM: Secure and Verifiable SBOM Sharing Via Zero-Knowledge Proofs](https://arxiv.org/abs/2602.13682)
*Gianpietro Castiglione,Shahriar Ebrahimi,Narges Khakpour*

Main category: cs.SE

TL;DR: VeriSBOM通过零知识证明技术，实现了安全、隐私保护且可验证的SBOM共享与验证。


<details>
  <summary>Details</summary>
Motivation: 软件供应链的透明性需要软件材料清单（SBOM），但SBOM中包含的敏感信息可能暴露专有依赖、漏洞或架构策略，导致技术和商业风险。

Method: 采用零知识证明、向量承诺方案与折叠证明聚合，生成简洁的证明，验证软件依赖的真实性与合规性，同时保护隐私。

Result: 提出了VeriSBOM框架，利用零知识证明实现SBOM的加密可验证性，使第三方能够验证软件的依赖真实性和合规性，同时保护敏感信息。

Conclusion: VeriSBOM实现了可扩展、隐私保护及验证性的SBOM共享，验证过程不依赖于发布者的信任，确保了安全与合规性。

Abstract: A Software Bill of Materials (SBOM) is a key component for the transparency of software supply chain; it is a structured inventory of the components, dependencies, and associated metadata of a software artifact. However, an SBOM often contain sensitive information that organizations are unwilling to disclose in full to anyone, for two main concerns: technological risks deriving from exposing proprietary dependencies or unpatched vulnerabilities, and business risks, deriving from exposing architectural strategies. Therefore, delivering a plaintext SBOM may result in the disruption of the intellectual property of a company. To address this, we present VeriSBOM, a trustless, selectively disclosed SBOM framework that provides cryptographic verifiability of SBOMs using zero-knowledge proofs. Within VeriSBOM, third parties can validate specific statements about a delivered software. Respectively, VeriSBOM allows independent third parties to verify if a software contains authentic dependencies distributed by official package managers and that the same dependencies satisfy rigorous policy constraints such as the absence of vulnerable dependencies or the adherence with specific licenses models. VeriSBOM leverages a scalable vector commitment scheme together with folding-based proof aggregation to produce succinct zero-knowledge proofs that attest to security and compliance properties while preserving confidentiality. Crucially, the verification process requires no trust in the SBOM publisher beyond the soundness of the underlying primitives, and third parties can independently check proofs against the public cryptographic commitments. We implement VeriSBOM, analyze its security, and evaluate its performance on real-world package registries. The results show that our method enables scalable, privacy-preserving, and verifiable SBOM sharing and validation.

</details>


### [103] [ARC: Compiling Hundreds of Requirement Scenarios into A Runnable Web System](https://arxiv.org/abs/2602.13723)
*Weiyu Kong,Yun Lin,Xiwen Teoh,Duc-Minh Nguyen,Ruofei Ren,Jiaxin Chang,Haoxu Hu,Haoyu Chen*

Main category: cs.SE

TL;DR: ARC通过需求编译技术，将复杂多模态文档高效转化为可维护的网页软件，显著提升了测试通过率和用户开发效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理复杂、多场景多模态文档时，生成代码准确性和约束完整性显著下降，难以满足大规模需求。

Method: 提出了一种名为Agentic Requirement Compilation (ARC)的技术，通过双向测试驱动的智能循环，将多模态DSL文档中的需求编译成可运行的网页系统，生成源代码、模块化设计、完善的测试套件及可追溯性设计。

Result: ARC生成的系统在多个多模态场景下平均通过的GUI测试比现有最先进方法高出50.6%，且用户研究表明初学者能用ARC DSL文档在5.6小时内完成复杂系统开发。

Conclusion: ARC技术有效解决了多模态复杂需求下代码生成的准确性和完整性问题，实现了高质量、可维护的运行软件系统。

Abstract: Large Language Models (LLMs) have improved programming efficiency, but their performance degrades significantly as requirements scale; when faced with multi-modal documents containing hundreds of scenarios, LLMs often produce incorrect implementations or omit constraints. We propose Agentic Requirement Compilation (ARC), a technique that moves beyond simple code generation to requirement compilation, enabling the creation of runnable web systems directly from multi-modal DSL documents. ARC generates not only source code but also modular designs for UI, API, and database layers, enriched test suites (unit, modular, and integration), and detailed traceability for software maintenance. Our approach employs a bidirectional test-driven agentic loop: a top-down architecture phase decomposes requirements into verifiable interfaces, followed by a bottom-up implementation phase where agents generate code to satisfy those tests. ARC maintains strict traceability across requirements, design, and code to facilitate intelligent asset reuse. We evaluated ARC by generating six runnable web systems from documents spanning 50-200 multi-modal scenarios. Compared to state-of-the-art baselines, ARC-generated systems pass 50.6% more GUI tests on average. A user study with 21 participants showed that novice users can successfully write DSL documents for complex systems, such as a 10K-line ticket-booking system, in an average of 5.6 hours. These results demonstrate that ARC effectively transforms non-trivial requirement specifications into maintainable, runnable software.

</details>


### [104] [Impacts of Generative AI on Agile Teams' Productivity: A Multi-Case Longitudinal Study](https://arxiv.org/abs/2602.13766)
*Rafael Tomaz,Paloma Guenes,Allysson Allex Araújo,Maria Teresa Baldassarre,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 本研究通过长期观察发现，生成式人工智能工具使敏捷团队提升了工作效率和表现，而单纯依靠活动量指标难以全面评估其效果。


<details>
  <summary>Details</summary>
Motivation: 当前研究多为短期、个体层面，缺乏对工业敏捷团队中生成式人工智能工具长期影响的系统性研究。

Method: 通过对三支敏捷团队进行13个月的多案例纵向研究，结合Jira、SonarQube、Git等定量数据与问卷调查的定性数据，基于多维SPACE框架分析GenAI对开发者生产力的影响。

Result: 研究发现GenAI工具显著提升了团队的性能和效率，但开发者的活动量未见增加，说明GenAI提升了开发工作的价值密度而非工作量。

Conclusion: GenAI工具在敏捷团队中能有效提高生产力和成员福祉，且需采用多维指标框架如SPACE才能全面捕捉其真实影响。

Abstract: Context: Generative Artificial Intelligence (GenAI) tools, such as GitHub Copilot and GPT tools, represent a paradigm shift in software engineering. While their impact is clear, most studies are short-term, focused on individual experiments. The sustained, team-level effects on productivity within industrial agile environments remain largely uncharacterized. Goal: This study aims to provide a longitudinal evaluation of GenAI's impact on agile software teams. We characterize its effect on developers' productivity by applying the multi-dimensional SPACE framework. Method: We conducted a multi-case longitudinal study involving 3 agile teams at a large technology consulting firm for around 13 months. We collected and compared quantitative telemetry (Jira, SonarQube, Git) and qualitative survey data from historical (pre-adoption) and research (post-adoption) sprints. Conclusion: GenAI tools can significantly improve team performance and well-being. Our key finding is a sharp increase in Performance and perceived Efficiency concurrent with flat developer Activity. This suggests GenAI increases the value density of development work, not its volume. This finding validates the necessity of multi-dimensional frameworks like SPACE to capture the true, nuanced impact of GenAI in situ, which would be invisible to studies measuring Activity alone.

</details>


### [105] [Impostor Phenomenon as Human Debt: A Challenge to the Future of Software Engineering](https://arxiv.org/abs/2602.13767)
*Paloma Guenes,Rafael Tomaz,Maria Teresa Baldassarre,Alexander Serebrenik*

Main category: cs.SE

TL;DR: 本论文提出将冒名顶替现象视为人力债务，强调需通过文化重构、透明化和盟友支持等手段，改善软件工程环境以减少心理负担，促进可持续发展。


<details>
  <summary>Details</summary>
Motivation: 现有研究多从个体内在视角看待IP，忽视其在组织文化和环境中的系统性影响，尤其对弱势群体的额外负担。

Method: 提出将冒名顶替现象（IP）视为一种“人力债务”，通过与ICSE2026未来软件工程预调查结果的关联，分析IP的社会技术生态系统背景。

Result: 发现人力债务因心理安全感缺失和支持不足在社会技术环境中积累，且在弱势群体中更为严重，传统层级结构和学术环境加剧了这一现象。

Conclusion: 领导者和机构应关注并解决加剧冒名顶替感的环境因素，推动文化变革和支持体系建设，保障所有软件工程从业者的心理安全和职业可持续性。

Abstract: The Impostor Phenomenon (IP) impacts a significant portion of the Software Engineering workforce, yet it is often viewed primarily through an internal individual lens. In this position paper, we propose framing the prevalence of IP as a form of Human Debt and discuss the relation with the ICSE2026 Pre Survey on the Future of Software Engineering results. Similar to technical debt, which arises when short-term goals are prioritized over long-term structural integrity, Human Debt accumulates due to gaps in psychological safety and inclusive support within socio-technical ecosystems. We observe that this debt is not distributed equally, it weighs heavier on underrepresented engineers and researchers, who face compounded challenges within traditional hierarchical structures and academic environments. We propose cultural refactoring, transparency and active maintenance through allyship, suggesting that leaders and institutions must address the environmental factors that exacerbate these feelings, ensuring a sustainable ecosystem for all professionals.

</details>


### [106] [A Quasi-Experimental Evaluation of Coaching to Mitigate the Impostor Phenomenon in Early-Career Software Engineers](https://arxiv.org/abs/2602.13774)
*Paloma Guenes,Joan Leite,Rafael Tomaz,Allysson Allex Araujo,Jean Natividade,Maria Teresa Baldassarre,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 辅导能稍减软件工程师的冒名顶替感，但团队和项目环境也对其影响显著。


<details>
  <summary>Details</summary>
Motivation: 冒名顶替现象在软件工程师中普遍存在，但关于减轻IP的实证研究较少，尤其是在软件工程领域，故本研究旨在探究辅导干预对IP的影响。

Method: 采用准实验设计，20名早期软件工程师分成两组，其中一组接受三次辅导干预，另一组作为等待组；使用Clance IP量表、WHO-5、SWLS和PANAS测量IP和心理状态；辅以非参与观察。

Result: 本文研究了结构化小组辅导对早期软件工程师冒名顶替现象（IP）感受的影响。通过对20名参与者分为两组进行的准实验设计，辅以非参与观察，发现辅导组的IP得分有适度下降，但对照组在观察期内也有所改善，暗示团队协作和项目环境等情境因素可能比辅导作用更强。

Conclusion: 辅导有助于IP的反思和觉察，但团队和项目环境等情境因素对IP感受同样具有重要影响。

Abstract: Context: The Impostor Phenomenon (IP), the persistent belief of being a fraud despite evident competence, is common in Software Engineering (SE), where high expectations for expertise and innovation prevail. Although coaching and similar interventions are proposed to mitigate IP, empirical evidence in SE remains underexplored.
  Objective: This study examines the impact of a structured group coaching intervention on reducing IP feelings among early-career software engineers.
  Method: We conducted a quasi-experiment with 20 participants distributed across two project teams using a wait-list control design, complemented by non-participant observation. The treatment group received a three-session coaching intervention, while the control group received it after an observation phase. IP was assessed using the Clance Impostor Phenomenon Scale (CIPS), alongside evaluated measures of well-being (WHO-5), life satisfaction (SWLS), and affect (PANAS).
  Results: The coaching resulted in modest reductions in CIPS scores, whereas the control group also improved during the observation phase, suggesting that contextual and temporal factors may have exerted a stronger influence than the formal intervention.
  Conclusion: These results suggest that coaching may support reflection and awareness related to IP, yet other contextual aspects of team collaboration and project work might also contribute to these changes. This study offers a novel empirical step toward understanding how structured IP interventions operate within SE environments.

</details>


### [107] [Constructive Patterns for Human-Centered Tech Hiring](https://arxiv.org/abs/2602.13845)
*Allysson Allex Araújo,Gabriel Vasconcelos,Marvin Wyrich,Maria Teresa Baldassarre,Paloma Guenes,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 研究揭示了提升在线技术招聘体验的22种正向实践，为企业吸引和支持初级软件工程师提供实证指导。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究揭示了线上技术招聘中的缺陷和偏见，但针对提升应聘者体验的实践尚缺乏系统探讨，亟需实证总结成功经验。

Method: 采用申请者归因-反应理论指导，进行了22次半结构化访谈，收集分析超过470次线上招聘经历，通过主题分析提炼建设性模式。

Result: 本论文基于早期软件工程师视角，通过访谈分析了在线招聘选拔中的积极实践，总结出22种建设性模式（Constructive Patterns）。这些模式包括透明详尽的职位描述、具体反馈、人性化互动等，旨在改善应聘者体验。

Conclusion: 研究深化了技术招聘领域对积极候选者体验的理解，提供了具体可行的建议，有助于营造以人为本、促进成长的招聘环境。

Abstract: [Context] Online Recruitment and Selection (R&S) processes are often the first point of contact between early-career software engineers and the tech industry. Yet many candidates experience these processes as opaque, inefficient, or even discouraging. While prior research has extensively documented the flaws and biases in online tech hiring, little is known about the practices that create positive candidate experiences. [Objective & Method] This paper explores such practices, referred to as Constructive Patterns (CPs), from the perspective of early-career software engineers. Guided by Applicant Attribution-Reaction Theory, we conducted 22 semi-structured interviews in which participants collectively described over 470 online R&S experiences. [Results] Through thematic analysis, we identified 22 CPs that reflect positive practices such as comprehensive and transparent job advertisements (CP01), specific and developmental feedback (CP03), humanized and respectful interaction (CP06), and framing the process as a two-way street (CP18). [Conclusion] Our findings extend the conversation on tech hiring beyond diagnosing dysfunctions toward designing for human-centered and growth-oriented candidate experiences. The resulting catalog of CPs provides a concrete and empirically grounded resource for organizations seeking to attract and support early-career software engineers more effectively.

</details>


### [108] [Evaluating LLM-Generated ACSL Annotations for Formal Verification](https://arxiv.org/abs/2602.13851)
*Arshad Beg,Diarmuid O'Donoghue,Rosemary Monahan*

Main category: cs.SE

TL;DR: 本文评估了自动生成和验证ACSL规格的工具性能，对比了五种生成系统在506个C程序上的表现，分析了注释质量、求解器敏感性及证明稳定性。


<details>
  <summary>Details</summary>
Motivation: 现实C程序缺乏准确且可验证的形式规格，自动生成和验证规格工具的能力尚不清晰，需实证评估其有效性和局限性。

Method: 利用506个C程序数据集，使用五种不同的ACSL生成工具（包含规则基脚本、Frama-C插件和三种大型语言模型）生成规格，并用Frama-C WP插件及多种SMT求解器验证规格质量。

Result: 五种工具生成的规格在不同程序上的表现差异明显，验证结果揭示了当前自动化ACSL生成技术的能力上限和适用范围。

Conclusion: 自动生成ACSL规格在一定程度上可行，但仍存在注释准确性和验证稳定性方面的局限。

Abstract: Formal specifications are crucial for building verifiable and dependable software systems, yet generating accurate and verifiable specifications for real-world C programs remains challenging. This paper empirically evaluates the extent to which formal-analysis tools can automatically generate and verify ACSL specifications without human or learning-based assistance. We conduct a controlled study on a recently released dataset of 506 C programs, repurposing it from interactive, developer-driven workflows to an automated evaluation setting. Five ACSL generation systems are compared: a rule-based Python script, Frama-C's RTE plugin, and three large language models--DeepSeek-V3.2, GPT-5.2, and OLMo 3.1 32B Instruct. All generated specifications are verified under identical conditions using the Frama-C WP plugin powered by multiple SMT solvers, allowing a direct comparison of annotation quality, solver sensitivity, and proof stability. Our results provide new empirical evidence on the capabilities and limitations of automated ACSL generation, complementing prior survey-based work.

</details>


### [109] [CodeGlance: Understanding Code Reasoning Challenges in LLMs through Multi-Dimensional Feature Analysis](https://arxiv.org/abs/2602.13962)
*Yunkun Wang,Xuanhe Zhang,Junxiao Han,Chen Zhi,Shuiguang Deng*

Main category: cs.SE

TL;DR: 提出CodeGlance多维基准，系统分析LLM代码推理在多场景下的挑战与关键因素，探索增强策略效果，指导更强代码推理系统构建。


<details>
  <summary>Details</summary>
Motivation: 现有代码推理研究多聚焦孤立代码段，忽视了涉及外部API和未见函数的复杂真实场景，缺乏对代码推理挑战的全貌理解。

Method: 设计了CodeGlance基准，涵盖内在逻辑推理、API交互推理和未见函数推理三个现实场景，系统评估7个先进的大型语言模型的代码推理能力。

Result: 发现未见函数推理对小模型尤其具有挑战性，Qwen2.5-3b在未见函数上准确率仅6.0%，而熟悉API为37.5%。确立了执行轨迹长度、API调用次数和控制流复杂度等影响推理难度的关键特征。不同增强策略（链式思维、文档检索、代码搜索）在不同挑战下效果差异显著。

Conclusion: 本研究揭示了代码推理中未见函数推理的显著难度及影响因素，强调针对不同推理难点选择合适增强策略，推动LLM在实际软件开发中更有效应用。

Abstract: In modern software development, developers frequently need to understand code behavior at a glance -- whether reviewing pull requests, debugging issues, or navigating unfamiliar codebases. This ability to reason about dynamic program behavior is fundamental to effective software engineering and increasingly supported by Large Language Models (LLMs). However, existing studies on code reasoning focus primarily on isolated code snippets, overlooking the complexity of real-world scenarios involving external API interactions and unfamiliar functions. This gap hinders our understanding of what truly makes code reasoning challenging for LLMs across diverse programming contexts.
  We present CodeGlance, a multi-dimensional benchmark investigating code reasoning challenges across three realistic scenarios: intrinsic logic reasoning, API interaction reasoning, and unseen function reasoning. Through systematic evaluation of 7 state-of-the-art LLMs, we reveal that unseen function reasoning poses significant challenges especially for smaller models, with Qwen2.5-3b achieving only 6.0\% accuracy on unseen functions compared to 37.5\% on familiar APIs. We identify critical code complexity features -- including execution trace length, API invocation count, and control flow complexity -- that significantly impact code reasoning difficulty across scenarios. We further investigate how common augmentation strategies, including CoT, document retrieval, and code search, can improve reasoning performance, finding that their effectiveness varies substantially depending on whether challenges stem from logical complexity or knowledge gaps. These findings provide actionable guidance for developing more capable code reasoning systems and deploying LLM-based programming assistants in real-world software development.

</details>


### [110] [ATTest: Agent-Driven Tensor Testing for Deep Learning Library Modules](https://arxiv.org/abs/2602.13987)
*Zhengyu Zhan,Ye Shang,Jiawei Liu,Chunrong Fang,Quanjun Zhang,Zhenyu Chen*

Main category: cs.SE

TL;DR: 本文针对深度学习库单元测试难题，提出了智能体驱动的ATTest框架，通过多阶段流程和迭代修复机制，显著提升测试覆盖率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习库的单元测试面临复杂数值语义和隐式张量约束，传统基于搜索的软件测试和大语言模型均存在无法有效处理这些挑战的问题。

Method: 提出了基于智能体的七阶段流水线测试框架ATTest，包括约束提取和迭代的“生成-验证-修复”循环，保证测试的稳定性并减轻上下文窗口饱和。

Result: 在PyTorch和TensorFlow上的评测显示，ATTest平均分支覆盖率分别达到了55.60%和54.77%，明显优于现有领先基线方法PynguinML。

Conclusion: ATTest显著提升了深度学习库模块级单元测试的覆盖率和稳定性，克服了传统方法在语义理解和张量约束处理上的不足。

Abstract: The unit testing of Deep Learning (DL) libraries is challenging due to complex numerical semantics and implicit tensor constraints. Traditional Search-Based Software Testing (SBST) often suffers from semantic blindness, failing to satisfy the constraints of high-dimensional tensors, whereas Large Language Models (LLMs) struggle with cross-file context and unstable code modifications. This paper proposes ATTest, an agent-driven tensor testing framework for module-level unit test generation. ATTest orchestrates a seven-stage pipeline, which encompasses constraint extraction and an iterative "generation-validation-repair" loop, to maintain testing stability and mitigate context-window saturation. An evaluation on PyTorch and TensorFlow demonstrates that ATTest significantly outperforms state-of-the-art baselines such as PynguinML, achieving an average branch coverage of 55.60% and 54.77%, respectively. The results illustrate how agent-driven workflows bridge the semantic gap in numerical libraries while ensuring auditable test synthesis. Source code: https://github.com/iSEngLab/ATTest.git

</details>


### [111] [Every Maintenance Has Its Exemplar: The Future of Software Maintenance through Migration](https://arxiv.org/abs/2602.14046)
*Zirui Chen,Xing Hu,Xin Xia,Xiaohu Yang*

Main category: cs.SE

TL;DR: 本文系统提出基于迁移的自动化软件维护框架，包含四阶段生命周期及关键挑战，为提高维护效率和自动化提供方向。


<details>
  <summary>Details</summary>
Motivation: 软件维护繁琐且易错，自动化迫切需要。基于迁移的方法通过借鉴其他系统维护经验，有效提升维护效率和质量，因而成为重要研究方向。

Method: 文章基于迁移的方法，提出迁移维护生命周期模型，包括任务识别、迁移源选择、跨系统数据匹配和适配，以及迁移结果验证四个步骤，对每个阶段的挑战进行分析。

Result: 本文提出了一个基于迁移的自动化软件维护系统的研究议程，定义了迁移维护的生命周期及其四个关键阶段：任务识别、迁移源选择、数据匹配与适配、迁移验证。并分析了每个阶段面临的挑战，旨在推动社区深入研究迁移方法，提升软件维护的自动化水平。

Conclusion: 迁移-based自动维护作为提升软件维护效率的重要方向，需解决识别任务、选择迁移源、数据匹配适配及迁移验证等关键挑战，实现维护的自动化和可靠性。

Abstract: Maintenance is a critical stage in the software lifecycle, ensuring that post-release systems remain reliable, efficient, and adaptable. However, manual software maintenance is labor-intensive, time-consuming, and error-prone, which highlights the urgent need for automation. Learning from maintenance activities conducted on other software systems offers an effective way to improve efficiency. In particular, recent research has demonstrated that migration-based approaches transfer knowledge, artifacts, or solutions from one system to another and show strong potential in tasks such as API evolution adaptation, software testing, and migrating patches for fault correction. This makes migration-based maintenance a valuable research direction for advancing automated maintenance.
  This paper takes a step further by presenting the first systematic research agenda on migration-based approaches to software maintenance. We characterize the migration-based maintenance lifecycle through four key stages: \ding{182} identifying a maintenance task that can be addressed through migration, \ding{183} selecting suitable migration sources for the target project,\ding{184} matching relevant data across systems and adapting the migrated data to the target context, and \ding{185} validating the correctness of the migration. We also analyze the challenges that may arise at each stage. Our goal is to encourage the community to explore migration-based approaches more thoroughly and to tackle the key challenges that must be solved to advance automated software maintenance.

</details>


### [112] [LongCLI-Bench: A Preliminary Benchmark and Study for Long-horizon Agentic Programming in Command-Line Interfaces](https://arxiv.org/abs/2602.14337)
*Yukang Feng,Jianwen Sun,Zelai Yang,Jiaxin Ai,Chuanhao Li,Zizhen Li,Fanrui Zhang,Kang He,Rui Ma,Jifan Lin,Jie Sun,Yang Xiao,Sizhuo Zhou,Wenxiao Wu,Yiming Liu,Pengfei Liu,Yu Qiao,Shenglin Zhang,Kaipeng Zhang*

Main category: cs.SE

TL;DR: 提出LongCLI-Bench基准，真实长时程编程任务评估智能体，结果显示现有智能体表现不佳，人机协作效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有AI辅助编程评测基准任务短、数据存有污染、评价指标不细化，无法有效检测长时程规划与执行能力。为弥补不足，需设计更真实、细粒度的评测基准，促进智能体能力提升。

Method: 设计LongCLI-Bench基准，收集20个高质量长时程任务，覆盖四类工程任务；提出双重测试协议和逐步评分体系；进行大量实验证明现有智能体能力不足，分析失败点和改进手段。

Result: 本文提出了LongCLI-Bench，一个用于评估智能体在长时程、现实任务中计划与执行能力的综合基准。该基准包含20个来自计算机科学作业和真实工作流的高质量长时程任务，涵盖从零开始、功能添加、BUG修复和重构四个工程类别。提出了一种双重测试协议，通过需求完成度和回归避免两方面指标，以及细粒度逐步得分来定位执行失败。实验证明当前先进智能体在该基准上的通过率低于20%，多数任务未完成30%，失败多发生于早期阶段。自我纠错效果有限，而人机协作显著提升任务完成率。

Conclusion: 目前智能体在长时程编程任务中表现不佳，关键失败多在任务初期。人机协作通过计划注入和交互指导显著提升性能，未来应重点发展人机协同工作流和智能体的规划、执行能力。

Abstract: Recent advances in AI-assisted programming have empowered agents to execute complex workflows via command-line interfaces, however, existing benchmarks are limited by short task horizons, data contamination from GitHub scraping, and a lack of fine-grained evaluation metrics, fail to rigorously evaluate the long-horizon planning and execution capabilities essential for realistic software engineering. To address these gaps, we introduce LongCLI-Bench, a comprehensive benchmark designed to evaluate agentic capabilities across long-horizon, realistic tasks. We curated 20 high-quality, long-horizon tasks from over 1,000 computer science assignments and real-world workflows, covering four engineering categories: from scratch, feature addition, bug fixing, and refactoring. We propose a dual-set testing protocol for LongCLI-Bench, which measures requirement fulfillment (fail-to-pass) and regression avoidance (pass-to-pass), and incorporates step-level scoring to pinpoint execution failures. Extensive experiments reveal that even state-of-the-art agents achieve pass rates below 20% in LongCLI-Bench. Step-level analysis further indicates that the majority of tasks stall at less than 30% completion, highlighting that critical failures often occur in the early stages. Although self-correction offers marginal gains, human-agent collaboration through plan injection and interactive guidance yields significantly higher improvements. These results highlight that future research must emphasize the development of synergistic human-agent workflows alongside advances in agents' planning and execution capabilities to overcome key challenges in long-horizon task performance.

</details>


### [113] [An Empirical Study of the Evolution of GitHub Actions Workflows](https://arxiv.org/abs/2602.14572)
*Pooya Rostami Mazrae,Alexandre Decan,Tom Mens,Mairieli Wessel*

Main category: cs.SE

TL;DR: 分析GitHub Actions工作流文件变更，发现小规模改动占主，频率稳定，建议强化依赖管理和AI辅助保障。


<details>
  <summary>Details</summary>
Motivation: CI/CD自动化工具GitHub Actions在协同软件开发中广泛应用，但其工作流的维护和变更特点尚未详尽了解，研究旨在揭示变更行为以指导改进支持工具。

Method: 首先对439个修改的工作流文件进行定性分析，确定七类概念变更类型；随后对49000多个仓库、26.7万工作流变更历史和340万文件版本进行量化分析，统计变更频率和性质。

Result: 本文通过对GitHub Actions的工作流文件修改进行混合方法分析，揭示了工作流维护的具体变化类型与频率。研究发现，工作流文件中主要修改为任务配置和任务规范，多数改动规模较小；每周约7.3%的工作流文件发生变化，且仓库中中位数包含三个工作流文件。尚未发现大型技术变革（如大型语言模型编码工具）对工作流变更频率的显著影响。论文强调需改进工具，支持细粒度维护任务，尤其是依赖管理和基于AI的安全与质量保障。

Conclusion: GitHub Actions工作流更改主要集中在任务配置和规范，改动频率稳定，需发展更细粒度维护支持工具及AI安全质量保障。

Abstract: CI/CD practices play a significant role during collaborative software development by automating time-consuming and repetitive tasks such as testing, building, quality checking, dependency and security management. GitHub Actions, the CI/CD tool integrated into GitHub, allows repository maintainers to automate development workflows. We conducted a mixed methods analysis of GitHub Actions workflow changes over time. Through a preliminary qualitative analysis of 439 modified workflow files we identified seven types of conceptual changes to workflows. Next, we performed a quantitative analysis over 49K+ GitHub repositories totaling 267K+ workflow change histories and 3.4M+ workflow file versions from November 2019 to August 2025. This analysis revealed that repositories contain a median of three workflow files, and 7.3% of all workflow files are being changed every week. The changes made to workflows tend to be small, with about three-quarters containing only a single change. The large majority of the observed changes have to do with task configuration and task specification in workflow jobs. We did not find any conclusive evidence of the effect of LLM coding tools or other major technological changes on workflow creation and workflow maintenance frequency. Our findings highlight the need for improved tooling to support fine-grained maintenance tasks, such as a broader adoption of dependency management and AI-based support for ensuring and sustaining workflow security and quality.

</details>


### [114] [Automated Classification of Source Code Changes Based on Metrics Clustering in the Software Development Process](https://arxiv.org/abs/2602.14591)
*Evgenii Kniazev*

Main category: cs.SE

TL;DR: 本文提出一种基于代码变更度量聚类的自动分类方法，有效减少审查时间，验证显示分类精度较高。


<details>
  <summary>Details</summary>
Motivation: 减少代码变更审查所需时间，提高软件开发过程中的代码变更分类效率。

Method: 采用k-means算法基于余弦相似度对每个代码变更的度量向量进行聚类，使用11个源代码度量指标，随后由专家将聚类结果映射到预定义的变更类别。

Result: 该方法在五个软件系统（包括两个开源项目Subversion和NHibernate）上验证，分类纯度约为0.75，熵约为0.37，证明了方法的有效性。

Conclusion: 通过自动聚类源代码变更度量，显著减少了代码变更审查时间，且分类效果较好，表明该方法适用于实际软件项目的代码变更分类。

Abstract: This paper presents an automated method for classifying source code changes during the software development process based on clustering of change metrics. The method consists of two steps: clustering of metric vectors computed for each code change, followed by expert mapping of the resulting clusters to predefined change classes. The distribution of changes into clusters is performed automatically, while the mapping of clusters to classes is carried out by an expert. Automation of the distribution step substantially reduces the time required for code change review. The k-means algorithm with a cosine similarity measure between metric vectors is used for clustering. Eleven source code metrics are employed, covering lines of code, cyclomatic complexity, file counts, interface changes, and structural changes. The method was validated on five software systems, including two open-source projects (Subversion and NHibernate), and demonstrated classification purity of P_C = 0.75 +/- 0.05 and entropy of E_C = 0.37 +/- 0.06 at a significance level of 0.05.

</details>


### [115] [Consistent or Sensitive? Automated Code Revision Tools Against Semantics-Preserving Perturbations](https://arxiv.org/abs/2602.14595)
*Shirin Pirouzkhah,Souhaila Serbout,Alberto Bacchelli*

Main category: cs.SE

TL;DR: 本研究设计语义保持扰动，评估五种主流自动代码修订工具在语义等价代码变体上的一致性。结果表明这些工具在变体上一致性差，正确率显著下降，且现有缓解办法效果有限，提示该问题尚待进一步研究。


<details>
  <summary>Details</summary>
Motivation: 虽然自动代码修订工具在历史数据上表现良好，但其在处理表达相同问题的代码变体时是否能保持一致性尚未研究，这是确保实际应用价值的关键。

Method: 设计九种语义保持扰动，将其应用于2032个Java方法，生成超过一万条代码变体，利用这些变体评估五种基于Transformer的自动代码修订工具的一致性表现。

Result: 自动代码修订工具在面对语义等价的代码变体时，正确修订率最多下降45.3%；扰动越接近问题区域，工具失败率越高；现有基于注意力机制的缓解策略仅有有限改进。

Conclusion: 现有自动代码修订工具在面对语义等价的代码变体时，表现出一致性不足，准确率显著下降，且目前的缓解策略效果有限。

Abstract: Automated Code Revision (ACR) tools aim to reduce manual effort by automatically generating code revisions based on reviewer feedback. While ACR tools have shown promising performance on historical data, their real-world utility depends on their ability to handle similar code variants expressing the same issue - a property we define as consistency. However, the probabilistic nature of ACR tools often compromises consistency, which may lead to divergent revisions even for semantically equivalent code variants. In this paper, we investigate the extent to which ACR tools maintain consistency when presented with semantically equivalent code variants. To do so, we first designed nine types of semantics-preserving perturbations (SPP) and applied them to 2032 Java methods from real-world GitHub projects, generating over 10K perturbed variants for evaluation. Then we used these perturbations to evaluate the consistency of five state-of-the-art transformer-based ACR tools. We found that the ACR tools' ability to generate correct revisions can drop by up to 45.3%, when presented with semantically equivalent code. The closer the perturbation is to this targeted region, the more likely an ACR tool is to fail to generate the correct revision. We explored potential mitigation strategies that modify the input representation, but found that these attention-guiding heuristics yielded only marginal improvements, thus leaving the solution to this problem as an open research question.

</details>


### [116] [The Value of Effective Pull Request Description](https://arxiv.org/abs/2602.14611)
*Shirin Pirouzkhah,Pavlína Wurzel Gonçalves,Alberto Bacchelli*

Main category: cs.SE

TL;DR: 研究系统揭示了PR描述元素对审查效果的影响，强调根据内容精准撰写PR描述的重要性。


<details>
  <summary>Details</summary>
Motivation: 拉取请求（PR）作为代码贡献的形式，其描述的作用尚未被系统研究，亟需明确PR描述对代码审查结果的影响。

Method: 通过灰色文献回顾归纳八种PR描述建议元素，分析了跨156个项目和五种编程语言的8万条GitHub PR数据，结合开发者调查，探讨各元素与审查结果的关联和PR描述的决定因素。

Result: 发现PR描述元素在开发者眼中重要性不同，目的和代码解释有助于传递变更的理由和历史，声明期望反馈类型最能预测PR被接受和审查者参与度；PR描述在成熟项目及复杂更改中更常见。

Conclusion: PR描述对代码审查具有实际价值，不是形式主义，特定描述元素能显著影响审查决策和效率，应予重视和合理编写。

Abstract: In the pull-based development model, code contributions are submitted as pull requests (PRs) to undergo reviews and approval by other developers with the goal of being merged into the code base. A PR can be supported by a description, whose role has not yet been systematically investigated. To fill in this gap, we conducted a mixed-methods empirical study of PR descriptions. We conducted a grey literature review of guidelines on writing PR descriptions and derived a taxonomy of eight recommended elements. Using this taxonomy, we analyzed 80K GitHub PRs across 156 projects and five programming languages to assess associations between these elements and code review outcomes (e.g., merge decision, latency, first response time, review comments, and review iteration cycles). To complement these results, we surveyed 64 developers about the perceived importance of each element. Finally, we analyzed which submission-time factors predict whether PRs include a description and which elements they contain. We found that developers view PR descriptions as important, but their elements matter differently: purpose and code explanations are valued by developers for preserving the rationale and history of changes, while stating the desired feedback type best predicts change acceptance and reviewer engagement. PR descriptions are also more common in mature projects and complex changes, suggesting they are written when most useful rather than as a formality.

</details>


### [117] [Configuring Agentic AI Coding Tools: An Exploratory Study](https://arxiv.org/abs/2602.14690)
*Matthias Galster,Seyedmoein Mohsenimofidi,Jai Lal Lulla,Muhammad Auwal Abubakar,Christoph Treude,Sebastian Baltes*

Main category: cs.SE

TL;DR: 本文分析了多款智能代理编程工具的配置机制及其采用情况，发现Context Files最常用，Skills和Subagents采用不深，且不同工具形成不同配置文化。


<details>
  <summary>Details</summary>
Motivation: 随着智能代理编程工具自动化开发任务能力提升，研究其配置机制及采用情况有助于理解配置策略如何发展及其对工具性能的影响。

Method: 通过系统分析八种配置机制，结合对2926个GitHub仓库的实证调查，重点深入研究了Context Files、Skills和Subagents三种机制的采用情况及特点。

Result: 本文系统分析了多种智能代理编程工具（包括Claude Code、GitHub Copilot、Cursor、Gemini和Codex）的配置机制，识别出八种配置机制，并通过对2926个GitHub仓库的实证研究，探讨了这些机制的采用情况。重点分析了跨工具通用的三种机制：Context Files、Skills和Subagents。研究发现Context Files是最主要的配置方式，且经常是仓库中唯一的机制，其中AGENTS.md成为多工具间互操作的标准。Skills和Subagents采用较少，多数仓库定义的相关文件数量有限，且Skills多为静态说明而非可执行流程。不同工具用户形成了不同的配置文化，Claude Code用户使用的配置机制最为丰富。本文为后续关于配置策略如何演变及其对智能代理性能影响的纵向和实验性研究提供了经验基线。

Conclusion: Context Files是配置的主流方式，AGENTS.md成为多工具标准；Skills和Subagents采用较浅；不同工具用户形成差异化配置文化，这为未来研究提供了经验基线。

Abstract: Agentic AI coding tools with autonomous capabilities beyond conversational content generation increasingly automate repetitive and time-consuming software development tasks. Developers can configure these tools through versioned repository-level artifacts such as Markdown and JSON files. In this paper, we present a systematic analysis of configuration mechanisms for agentic AI coding tools, covering Claude Code, GitHub Copilot, Cursor, Gemini, and Codex. We identify eight configuration mechanisms and, in an empirical study of 2,926 GitHub repositories, examine whether and how they are adopted. We then explore Context Files, Skills, and Subagents, that is, three mechanisms available across tools, in more detail. Our findings reveal three trends. First, Context Files dominate the configuration landscape and are often the sole mechanism in a repository, with AGENTS$.$md emerging as an interoperable standard across tools. Second, advanced mechanisms such as Skills and Subagents are only shallowly adopted: most repositories define only one or two artifacts, and Skills predominantly rely on static instructions rather than executable workflows. Third, distinct configuration cultures are forming around different tools, with Claude Code users employing the broadest range of mechanisms. These findings establish an empirical baseline for longitudinal and experimental research on how configuration strategies evolve and affect agent performance as agentic AI coding tools mature.

</details>


### [118] [Model Context Protocol (MCP) Tool Descriptions Are Smelly! Towards Improving AI Agent Efficiency with Augmented MCP Tool Descriptions](https://arxiv.org/abs/2602.14878)
*Mohammed Mehedi Hasan,Hao Li,Gopi Krishnan Rajbahadur,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 该论文首次对Model Context Protocol（MCP）生态中856个工具的描述质量及其对基于Foundation Model代理性能的影响进行了实证研究，发现绝大多数工具描述存在缺陷，改进描述能提升任务成功率但也增加执行步骤，揭示了性能与成本之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 由于MCP生态中工具描述以自然语言形式呈现，描述的质量直接影响FM代理选择正确工具和参数的能力，目前这方面的缺陷普遍存在但尚未被系统研究。

Method: 从文献中提取工具描述的六个组成部分，设计评分标准并定义描述缺陷，使用基于FM的扫描器对103个MCP服务器的856个工具描述进行质量评估和缺陷检测，随后通过改进描述进行性能对比实验。

Result: 97.1%的工具描述存在至少一种缺陷，56%未明确说明工具目的；改进描述能提升任务成功率中位数5.85个百分点，部分目标完成率提升15.12%，但执行步骤增加67.46%，16.67%的案例表现下降；组件消融实验表明紧凑描述版本能在保证可靠性的同时降低执行成本。

Conclusion: 改进工具描述的质量能显著提升FM代理的任务完成率和部分目标的达成度，但也会带来执行步骤增加和部分性能回退，表现出性能提升与执行成本之间的权衡。

Abstract: The Model Context Protocol (MCP) standardizes how Foundation Model (FM)-based agents interact with external systems by invoking tools. However, to understand a tool's purpose and features, FMs rely on natural-language tool descriptions, making these descriptions a critical component in guiding FMs to select the optimal tool for a given (sub)task and to pass the right arguments to the tool. While defects or smells in these descriptions can misguide FM-based agents, their prevalence and consequences in the MCP ecosystem remain unclear.
  To address this, we conduct the first large-scale empirical study of 856 tools spread across 103 MCP servers, assessing their description quality and their impact on agent performance. We identify six components of tool descriptions from the literature, develop a scoring rubric utilizing these components, then formalize tool description smells based on this rubric. By operationalizing this rubric through an FM-based scanner, we find that 97.1% of the analyzed tool descriptions contain at least one smell, with 56% failing to state their purpose clearly. While augmenting these descriptions for all components improves task success rates by a median of 5.85 percentage points and improves partial goal completion by 15.12%, it also increases the number of execution steps by 67.46% and regresses performance in 16.67% of cases. These findings highlight a trade-off between agent performance and cost, as well as the context sensitivity of the performance gain. Furthermore, component ablations show that compact variants of different component combinations often preserve behavioral reliability while reducing unnecessary token overhead, enabling more efficient use of the FM context window and lower execution costs.

</details>
