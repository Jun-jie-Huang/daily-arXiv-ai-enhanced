<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 69]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.SE](#cs.SE) [Total: 7]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [The Perplexity Paradox: Why Code Compresses Better Than Math in LLM Prompts](https://arxiv.org/abs/2602.15843)
*Warren Johnson*

Main category: cs.CL

TL;DR: 本文通过多基准验证压缩阈值，揭示困惑度悖论，并提出TAAC自适应压缩方法，实现成本-质量优势。


<details>
  <summary>Details</summary>
Motivation: 之前工作仅限于HumanEval，未验证困惑度悖论机制，且无自适应算法。

Method: 验证了六个代码和四个推理基准上的压缩阈值，进行首次逐标记困惑度分析，提出并验证了任务感知自适应压缩算法TAAC。

Result: 压缩阈值在多语言多难度任务中普遍适用，发现代码语法标记困惑度高保留，数学数值困惑度低被删，签名注入显著提升通过率。TAAC实现了22%成本降低，质量保持96%，优于固定压缩比例。

Conclusion: 任务感知的自适应压缩有效平衡代码生成和推理任务的压缩与质量，推动效率提升。

Abstract: In "Compress or Route?" (Johnson, 2026), we found that code generation tolerates aggressive prompt compression (r >= 0.6) while chain-of-thought reasoning degrades gradually. That study was limited to HumanEval (164 problems), left the "perplexity paradox" mechanism unvalidated, and provided no adaptive algorithm. This paper addresses all three gaps. First, we validate across six code benchmarks (HumanEval, MBPP, HumanEval+, MultiPL-E) and four reasoning benchmarks (GSM8K, MATH, ARC-Challenge, MMLU-STEM), confirming the compression threshold generalizes across languages and difficulties. Second, we conduct the first per-token perplexity analysis (n=723 tokens), revealing a "perplexity paradox": code syntax tokens are preserved (high perplexity) while numerical values in math problems are pruned despite being task-critical (low perplexity). Signature injection recovers +34 percentage points in pass rate (5.3% to 39.3%; Cohen's h=0.890). Third, we propose TAAC (Task-Aware Adaptive Compression), achieving 22% cost reduction with 96% quality preservation, outperforming fixed-ratio compression by 7%. MBPP validation (n=1,800 trials) confirms systematic variation: 3.6% at r=0.3 to 54.6% at r=1.0.

</details>


### [2] [Language Model Representations for Efficient Few-Shot Tabular Classification](https://arxiv.org/abs/2602.15844)
*Inwon Kang,Parikshit Ram,Yi Zhou,Horst Samulowitz,Oshani Seneviratne*

Main category: cs.CL

TL;DR: 本文提出了利用大型语言模型语义嵌入加上去公共成分和温度校准技术，实现了网络表格的高效低样本分类，避免了专用模型的重新训练。


<details>
  <summary>Details</summary>
Motivation: 网络表格结构和语义异质性大，利用已部署的LLMs进行表格分类以避免专用模型和大量再训练是实际需求。

Method: 提出了TaRL框架，使用表格行的语义嵌入，结合去除共有成分和软最大温度校准两种技术，并训练元学习器预测温度。

Result: 经过调整的语义嵌入方法在低数据场景下表现优异，接近最先进专用表格模型。

Conclusion: 本文证明了利用现有的大型语言模型（LLMs）基础设施，通过简单的技术调整，可以有效地对网络表格进行低样本分类，性能接近最先进的专用模型。

Abstract: The Web is a rich source of structured data in the form of tables, from product catalogs and knowledge bases to scientific datasets. However, the heterogeneity of the structure and semantics of these tables makes it challenging to build a unified method that can effectively leverage the information they contain. Meanwhile, Large language models (LLMs) are becoming an increasingly integral component of web infrastructure for tasks like semantic search. This raises a crucial question: can we leverage these already-deployed LLMs to classify structured data in web-native tables (e.g., product catalogs, knowledge base exports, scientific data portals), avoiding the need for specialized models or extensive retraining? This work investigates a lightweight paradigm, $\textbf{Ta}$ble $\textbf{R}$epresentation with $\textbf{L}$anguage Model~($\textbf{TaRL}$), for few-shot tabular classification that directly utilizes semantic embeddings of individual table rows. We first show that naive application of these embeddings underperforms compared to specialized tabular models. We then demonstrate that their potentials can be unlocked with two key techniques: removing the common component from all embeddings and calibrating the softmax temperature. We show that a simple meta-learner, trained on handcrafted features, can learn to predict an appropriate temperature. This approach achieves performance comparable to state-of-the-art models in low-data regimes ($k \leq 32$) of semantically-rich tables. Our findings demonstrate the viability of reusing existing LLM infrastructure for efficient semantics-driven pathway to reuse existing LLM infrastructure for Web table understanding.

</details>


### [3] [KD4MT: A Survey of Knowledge Distillation for Machine Translation](https://arxiv.org/abs/2602.15845)
*Ona de Gibert,Joseph Attieh,Timothee Mickus,Yves Scherrer,Jörg Tiedemann*

Main category: cs.CL

TL;DR: 本文对105篇机器翻译领域的知识蒸馏文献进行综述，分析方法及应用、现有问题和未来发展方向，并提供数据库和术语表辅助研究。


<details>
  <summary>Details</summary>
Motivation: 解决NLP中模型越来越大带来的压缩问题，并探讨知识蒸馏在机器翻译中作为知识转移机制对监督、翻译质量和效率的影响。

Method: 综述了105篇有关机器翻译领域知识蒸馏(KD4MT)的文献，介绍了机器翻译和知识蒸馏的基础知识，分析了KD在机器翻译中的标准方法及其应用，包括方法学和实践应用两大类。

Result: 通过定性和定量分析，发现该领域的研究趋势、关键空白以及缺乏统一评估方法的问题，并提供了选择KD方法的实践指导，同时指出知识蒸馏在机器翻译中可能带来的风险，如幻觉和偏见放大。

Conclusion: 总结了KD在机器翻译中的重要作用与挑战，强调需统一评估标准，合理选择方法，并关注潜在风险，同时展望大语言模型对KD4MT领域的影响。

Abstract: Knowledge Distillation (KD) as a research area has gained a lot of traction in recent years as a compression tool to address challenges related to ever-larger models in NLP. Remarkably, Machine Translation (MT) offers a much more nuanced take on this narrative: in MT, KD also functions as a general-purpose knowledge transfer mechanism that shapes supervision and translation quality as well as efficiency.
  This survey synthesizes KD for MT (KD4MT) across 105 papers (through October 1, 2025). We begin by introducing both MT and KD for non-experts, followed by an overview of the standard KD approaches relevant to MT applications. Subsequently, we categorize advances in the KD4MT literature based on (i) their methodological contributions and (ii) their practical applications. Our qualitative and quantitative analyses identify common trends in the field and highlight key research gaps as well as the absence of unified evaluation practice for KD methods in MT. We further provide practical guidelines for selecting a KD method in concrete settings and highlight potential risks associated with the application of KD to MT such as increased hallucination and bias amplification. Finally, we discuss the role of LLMs in re-shaping the KD4MT field. To support further research, we complement our survey with a publicly available database summarizing the main characteristics of the surveyed KD methods and a glossary of key terms.

</details>


### [4] [Gated Tree Cross-attention for Checkpoint-Compatible Syntax Injection in Decoder-Only LLMs](https://arxiv.org/abs/2602.15846)
*Xinyu Gao,Shaonan Wang,Nai Ding*

Main category: cs.CL

TL;DR: 本文提出了一种兼容预训练模型的新型门控树形交叉注意力机制，通过引入预计算的句法结构增强了解码器大语言模型对语法变体的鲁棒性，同时保持原有性能不变。


<details>
  <summary>Details</summary>
Motivation: 解码器大型语言模型对细微的语法扰动表现欠佳，影响下游推理任务的可靠性，且直接注入语法结构会干扰预训练模型能力。

Method: 设计了GTCA分支读取预计算的成分块记忆，并通过token更新掩码和分阶段训练控制结构更新的范围和时机，且不改变主干架构。

Result: 在多种基线和Transformer骨干模型上，GTCA在保持多项选择题和常识推理性能的同时，提升了模型的语法鲁棒性，优于传统的继续训练方法。

Conclusion: GTCA模块在不破坏原有模型性能的前提下，显著提升了解码器大语言模型的语法稳健性，为实际应用中的语言模型提供了有效的鲁棒性改进方法。

Abstract: Decoder-only large language models achieve strong broad performance but are brittle to minor grammatical perturbations, undermining reliability for downstream reasoning. However, directly injecting explicit syntactic structure into an existing checkpoint can interfere with its pretrained competence. We introduce a checkpoint-compatible gated tree cross-attention (GTCA) branch that reads precomputed constituency chunk memory while leaving backbone architecture unchanged. Our design uses a token update mask and staged training to control the scope and timing of structural updates. Across benchmarks and Transformer backbones, GTCA strengthens syntactic robustness beyond continued-training baselines without compromising Multiple-Choice QA performance or commonsense reasoning, providing a practical checkpoint-compatible route to more syntax-robust decoder-only LLMs.

</details>


### [5] [Do Personality Traits Interfere? Geometric Limitations of Steering in Large Language Models](https://arxiv.org/abs/2602.15847)
*Pranav Bhandari,Usman Naseem,Mehwish Nasim*

Main category: cs.CL

TL;DR: 研究发现大型语言模型的个性调控特质存在耦合，无法实现完全独立控制。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型中个性特征控制是否能独立进行，检验个性方向的几何关系。

Method: 分析两个大型模型家族中提取的个性引导向量，应用从无约束到软正交和硬正交的几何调节方法。

Result: 发现个性引导方向存在显著几何依赖，即使去除线性重叠，控制一种特质仍会影响其它特质。硬正交化虽强制几何独立，却不能消除跨特质的行为影响，且可能降低引导效果。

Conclusion: 大型语言模型中的个性特质位于一个稍微耦合的子空间，限制了特质的完全独立控制。

Abstract: Personality steering in large language models (LLMs) commonly relies on injecting trait-specific steering vectors, implicitly assuming that personality traits can be controlled independently. In this work, we examine whether this assumption holds by analysing the geometric relationships between Big Five personality steering directions. We study steering vectors extracted from two model families (LLaMA-3-8B and Mistral-8B) and apply a range of geometric conditioning schemes, from unconstrained directions to soft and hard orthonormalisation. Our results show that personality steering directions exhibit substantial geometric dependence: steering one trait consistently induces changes in others, even when linear overlap is explicitly removed. While hard orthonormalisation enforces geometric independence, it does not eliminate cross-trait behavioural effects and can reduce steering strength. These findings suggest that personality traits in LLMs occupy a slightly coupled subspace, limiting fully independent trait control.

</details>


### [6] [Can LLMs Assess Personality? Validating Conversational AI for Trait Profiling](https://arxiv.org/abs/2602.15848)
*Andrius Matšenas,Anet Lello,Tõnis Lees,Hans Peep,Kim Lilii Tamm*

Main category: cs.CL

TL;DR: 本研究证明了基于对话的LMM人格评估在部分维度上与传统问卷具备中等一致性，且用户对其准确度评价不低，展示了其作为心理测量新途径的潜力。


<details>
  <summary>Details</summary>
Motivation: 验证大语言模型作为动态替代问卷形式的人格评估方法的可行性和有效性。

Method: 通过内部被试设计实验，比较了基于引导性大语言模型（LLM）对话生成的五大人格得分与标准IPIP-50问卷得分的差异，同时测量用户对结果的感知准确度。

Result: 发现两种方法在人格五大维度中收敛效度中等（相关系数r=0.38-0.58），其中责任心、开放性和神经质得分无显著差异，亲和性和外向性存在显著差异，提示需要针对具体特质进行校准。参与者对LLM生成的人格画像的准确性感知与传统问卷结果相当。

Conclusion: 基于对话的人工智能模型能够作为问卷式人格评估的有前景替代方案，但需进一步针对不同人格特质进行优化和校准。

Abstract: This study validates Large Language Models (LLMs) as a dynamic alternative to questionnaire-based personality assessment. Using a within-subjects experiment (N=33), we compared Big Five personality scores derived from guided LLM conversations against the gold-standard IPIP-50 questionnaire, while also measuring user-perceived accuracy. Results indicate moderate convergent validity (r=0.38-0.58), with Conscientiousness, Openness, and Neuroticism scores statistically equivalent between methods. Agreeableness and Extraversion showed significant differences, suggesting trait-specific calibration is needed. Notably, participants rated LLM-generated profiles as equally accurate as traditional questionnaire results. These findings suggest conversational AI offers a promising new approach to traditional psychometrics.

</details>


### [7] [Team of Thoughts: Efficient Test-time Scaling of Agentic Systems through Orchestrated Tool Calling](https://arxiv.org/abs/2602.16485)
*Jeffrey T. H. Wong,Zixi Zhang,Junyi Liu,Yiren Zhao*

Main category: cs.CL

TL;DR: 本文提出了一种新型多智能体系统架构Team-of-Thoughts，通过异构智能体的优势互补和动态协调机制，显著提升了推理和代码生成任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统通常采用静态且同质的模型配置，限制了系统充分利用不同后训练模型的个性化优势。

Method: 提出了协调者校准方案以识别具有更强协调能力的模型，以及工具智能体的自我评估协议来衡量其专业领域技能，推理时根据这些能力动态激活合适的智能体。

Result: 在五个推理和代码生成基准上，Team-of-Thoughts表现出持续优越的任务性能，在AIME24和LiveCodeBench测试中分别达到96.67%和72.53%的准确率，显著超过同质模型基线。

Conclusion: Team-of-Thoughts架构通过引入协调者校准和自我评估协议，实现了对异构智能体的动态激活，显著优于传统同质模型配置，在多个基准测试中表现出更高的准确率。

Abstract: Existing Multi-Agent Systems (MAS) typically rely on static, homogeneous model configurations, limiting their ability to exploit the distinct strengths of differently post-trained models. To address this, we introduce Team-of-Thoughts, a novel MAS architecture that leverages the complementary capabilities of heterogeneous agents via an orchestrator-tool paradigm. Our framework introduces two key mechanisms to optimize performance: (1) an orchestrator calibration scheme that identifies models with superior coordination capabilities, and (2) a self-assessment protocol where tool agents profile their own domain expertise to account for variations in post-training skills. During inference, the orchestrator dynamically activates the most suitable tool agents based on these proficiency profiles. Experiments on five reasoning and code generation benchmarks show that Team-of-Thoughts delivers consistently superior task performance. Notably, on AIME24 and LiveCodeBench, our approach achieves accuracies of 96.67% and 72.53%, respectively, substantially outperforming homogeneous role-play baselines, which score 80% and 65.93%.

</details>


### [8] [Preference Optimization for Review Question Generation Improves Writing Quality](https://arxiv.org/abs/2602.15849)
*Karun Sharma,Vidushee Vats,Shengzhi Li,Yuxiang Wang,Zhongtian Sun,Prayag Tiwari*

Main category: cs.CL

TL;DR: 针对现有LLM生成评审问答表层化的问题，本文提出基于奖励模型的IntelliReward和问题生成模型IntelliAsk，显著提升了问题质量及推理写作能力，并公开了相关资源。


<details>
  <summary>Details</summary>
Motivation: 现有基于大型语言模型（LLM）的同行评审问答生成方法通常产生表层问题，过度依赖论文第一页的信息，缺乏实质性和基于证据的问题。

Method: 提出了IntelliReward奖励模型，该模型基于一个冻结的自回归大语言模型并在最后50个token状态上训练可调的多头变换器。结合Decoupled Clip和Dynamic Sampling Policy Optimization（DAPO）方法，训练了符合人类标准的IntelliAsk问题生成模型。

Result: IntelliReward在预测专家偏好方面超越了基于API的微调基线，IntelliAsk在推理和写作基准测试中表现优异，特别是在MuSR推理任务和WritingBench写作评估中均有所提升。

Conclusion: 通过引入IntelliReward和IntelliAsk方法，显著提升了同行评审中问题的质量，增强了模型在推理和写作任务中的能力，且发布了相关实现与专家偏好注释，为LLM生成的审稿问题提供自动评测基准。

Abstract: Peer review relies on substantive, evidence-based questions, yet existing LLM-based approaches often generate surface-level queries, drawing over 50\% of their question tokens from a paper's first page. To bridge this gap, we develop IntelliReward, a novel reward model built from a frozen autoregressive LLM with trainable multi-head transformers over the final 50 token states, which outperforms API-based SFT baselines in predicting expert-level human preferences. By applying Decoupled Clip and Dynamic Sampling Policy Optimization (DAPO) with IntelliReward, we train IntelliAsk, a question-generation model aligned with human standards of effort, evidence, and grounding. We find consistent improvements on reasoning and writing benchmarks, suggesting reviewer-question quality correlates with broader capabilities. Compared to the Qwen3-32B base model, IntelliAsk shows measurable gains across diverse benchmarks, specifically improving performance on reasoning tasks like MuSR (68.3 vs 64.7 Acc) and complex writing evaluations such as WritingBench (8.31 vs 8.07). We release our implementation, expert preference annotations, and the IntelliReward model to provide an automatic evaluation benchmark for grounding, effort, and evidence in LLM-generated review questions.

</details>


### [9] [Large Language Models for Assisting American College Applications](https://arxiv.org/abs/2602.15850)
*Zhengliang Liu,Weihang You,Peng Shu,Junhao Chen,Yi Pan,Hanqi Jiang,Yiwei Li,Zhaojun Ding,Chao Cao,Xinliang Li,Yifan Zhou,Ruidong Zhang,Shaochen Xu,Wei Ruan,Huaqin Zhao,Dajiang Zhu,Tianming Liu*

Main category: cs.CL

TL;DR: EZCollegeApp是一个基于大型语言模型的系统，帮助高中生填写美国大学申请表，整合多渠道信息，确保答案权威且由人为最终控制。


<details>
  <summary>Details</summary>
Motivation: 当前美国大学申请流程复杂，涉及多渠道、重复且含糊的表格填写，学生需频繁查阅多方资料，亟需一个辅助工具简化流程。

Method: 系统采用映射优先策略分离表单理解与答案生成，结合官方招生文档的检索增强问答，并通过人机交互聊天界面提供建议。

Result: 系统经过自动测试和人工质量评估验证，表现出高度一致性和有效性，且代码开源以促进推广。

Conclusion: EZCollegeApp通过文档整合和人机结合界面，有效提升了美国大学申请表填写的效率和准确性，保障用户隐私与安全。

Abstract: American college applications require students to navigate fragmented admissions policies, repetitive and conditional forms, and ambiguous questions that often demand cross-referencing multiple sources. We present EZCollegeApp, a large language model (LLM)-powered system that assists high-school students by structuring application forms, grounding suggested answers in authoritative admissions documents, and maintaining full human control over final responses. The system introduces a mapping-first paradigm that separates form understanding from answer generation, enabling consistent reasoning across heterogeneous application portals. EZCollegeApp integrates document ingestion from official admissions websites, retrieval-augmented question answering, and a human-in-the-loop chatbot interface that presents suggestions alongside application fields without automated submission. We describe the system architecture, data pipeline, internal representations, security and privacy measures, and evaluation through automated testing and human quality assessment. Our source code is released on GitHub (https://github.com/ezcollegeapp-public/ezcollegeapp-public) to facilitate the broader impact of this work.

</details>


### [10] [Narrative Theory-Driven LLM Methods for Automatic Story Generation and Understanding: A Survey](https://arxiv.org/abs/2602.15851)
*David Y. Liu,Aditya Joshi,Paul Dawson*

Main category: cs.CL

TL;DR: 本文综述了利用大语言模型进行叙事生成和理解的研究，提出分类体系，指出目前挑战，并建议未来关注基于理论的指标提升模型性能和跨学科合作。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型(LLMs)在自动故事生成和理解中的应用，分析自然语言处理(NLP)研究如何结合叙事学领域，旨在为叙事相关研究建立系统且理论指导的基础。

Method: 通过系统性调研现有的NLP叙事数据集、任务和理论，分析NLP方法论变化如提示工程与微调，结合叙事学理论构建叙事研究的分类法，揭示跨学科合作的机会和挑战。

Result: 本文提出了一个基于叙事学区分的分类法，发现了数据集、任务、叙事理论、NLP流程及方法论趋势的模式，强调了LLMs促进NLP与抽象叙事概念连接的潜力，同时指出了统一叙事任务定义与基准的挑战。

Conclusion: 未来的研究应聚焦于构建理论驱动的指标用于单个叙事属性的提升，开展大规模的理论驱动文学/社会/文化分析，以及设计验证和改进叙事理论的实验，而非追求单一的通用叙事质量基准。

Abstract: Applications of narrative theories using large language models (LLMs) deliver promising use-cases in automatic story generation and understanding tasks. Our survey examines how natural language processing (NLP) research engages with fields of narrative studies, and proposes a taxonomy for ongoing efforts that reflect established distinctions in narratology. We discover patterns in the following: narrative datasets and tasks, narrative theories and NLP pipeline and methodological trends in prompting and fine-tuning. We highlight how LLMs enable easy connections of NLP pipelines with abstract narrative concepts and opportunities for interdisciplinary collaboration. Challenges remain in attempts to work towards any unified definition or benchmark of narrative related tasks, making model comparison difficult. For future directions, instead of the pursuit of a single, generalised benchmark for 'narrative quality', we believe that progress benefits more from efforts that focus on the following: defining and improving theory-based metrics for individual narrative attributes to incrementally improve model performance; conducting large-scale, theory-driven literary/social/cultural analysis; and creating experiments where outputs can be used to validate or refine narrative theories. This work provides a contextual foundation for more systematic and theoretically informed narrative research in NLP by providing an overview to ongoing research efforts and the broader narrative studies landscape.

</details>


### [11] [Building Safe and Deployable Clinical Natural Language Processing under Temporal Leakage Constraints](https://arxiv.org/abs/2602.15852)
*Ha Na Cho,Sairam Sutari,Alexander Lopez,Hansen Bow,Kai Zheng*

Main category: cs.CL

TL;DR: 本文提出通过审计流程提升临床NLP模型的时间有效性和校准，减少泄露风险，保障预测安全性与可靠性。


<details>
  <summary>Details</summary>
Motivation: 临床自然语言处理模型在支持医院出院规划中表现出潜力，但容易受到时间和词汇泄露的影响，导致预测表现被高估，从而威胁临床工作流程和患者安全。

Method: 提出了一种轻量级审计流程，将模型可解释性集成到开发过程中，识别并抑制泄露信号，防止在训练前引入未来信息。以择期脊柱手术后次日出院预测为案例，评估审计对模型预测行为、校准和安全相关权衡的影响。

Result: 审计后的模型提供了更保守且更良好校准的概率估计，减少了对出院相关词汇线索的依赖，表现出更符合实际部署需求的预测特征。

Conclusion: 部署临床NLP模型时，应优先考虑时间有效性、预测校准和行为稳健性，避免追求过于乐观的性能指标，以确保患者安全和临床流程稳定。

Abstract: Clinical natural language processing (NLP) models have shown promise for supporting hospital discharge planning by leveraging narrative clinical documentation. However, note-based models are particularly vulnerable to temporal and lexical leakage, where documentation artifacts encode future clinical decisions and inflate apparent predictive performance. Such behavior poses substantial risks for real-world deployment, where overconfident or temporally invalid predictions can disrupt clinical workflows and compromise patient safety. This study focuses on system-level design choices required to build safe and deployable clinical NLP under temporal leakage constraints. We present a lightweight auditing pipeline that integrates interpretability into the model development process to identify and suppress leakage-prone signals prior to final training. Using next-day discharge prediction after elective spine surgery as a case study, we evaluate how auditing affects predictive behavior, calibration, and safety-relevant trade-offs. Results show that audited models exhibit more conservative and better-calibrated probability estimates, with reduced reliance on discharge-related lexical cues. These findings emphasize that deployment-ready clinical NLP systems should prioritize temporal validity, calibration, and behavioral robustness over optimistic performance.

</details>


### [12] [A Lightweight Explainable Guardrail for Prompt Safety](https://arxiv.org/abs/2602.15853)
*Md Asiful Islam,Mihai Surdeanu*

Main category: cs.CL

TL;DR: 提出一种轻量且可解释的提示词安全分类方法，通过联合训练分类和解释，利用合成数据和创新损失函数，有效提升性能同时减小模型规模。


<details>
  <summary>Details</summary>
Motivation: 应对大型语言模型在分类不安全提示词时的确认偏差，提高提示词分类的准确度和解释性，同时减少模型复杂度。

Method: 提出一种轻量级可解释守护方法（LEG），采用多任务学习结构同时学习提示词分类器和解释分类器；利用新生成策略生成的合成数据进行解释性训练；设计新的损失函数结合全局解释信号、交叉熵和焦点损失，并基于不确定性加权。

Result: LEG在提示词分类和解释性任务上表现与最先进方法相当或更好，跨领域和跨三个数据集均表现优异，且模型规模显著更小。

Conclusion: LEG在保持甚至提升性能的同时，显著降低了模型复杂度，具备良好的通用性和可解释性，适合实际应用，相关模型和数据集将公开。

Abstract: We propose a lightweight explainable guardrail (LEG) method for the classification of unsafe prompts. LEG uses a multi-task learning architecture to jointly learn a prompt classifier and an explanation classifier, where the latter labels prompt words that explain the safe/unsafe overall decision. LEG is trained using synthetic data for explainability, which is generated using a novel strategy that counteracts the confirmation biases of LLMs. Lastly, LEG's training process uses a novel loss that captures global explanation signals and combines cross-entropy and focal losses with uncertainty-based weighting. LEG obtains equivalent or better performance than the state-of-the-art for both prompt classification and explainability, both in-domain and out-of-domain on three datasets, despite the fact that its model size is considerably smaller than current approaches. If accepted, we will release all models and the annotated dataset publicly.

</details>


### [13] [Decoupling Strategy and Execution in Task-Focused Dialogue via Goal-Oriented Preference Optimization](https://arxiv.org/abs/2602.15854)
*Jingyi Xu,Xingyu Ren,Zhiqiang You,Yumeng Zhang,Zhoupeng Shou*

Main category: cs.CL

TL;DR: 本文提出了GOPO，一种通过专家代理和客服代理分离策略规划与响应生成的分层强化学习框架，以优化任务导向对话系统的长期任务成功率，在多个数据集上表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有训练方法多依赖于词元级似然或偏好优化，难以有效提升长期任务成功率，需要新方法对策略规划与响应生成进行分离以更好优化多轮对话目标。

Method: 提出分层强化学习框架GOPO，包含专家代理负责多轮任务目标偏好优化，客服代理负责严格依据策略生成响应，结合序列级指标评估效果。

Result: 在Mgshop数据集上，GOPO将序列级指标TSE分别提升7.7%和10.3%优于PPO和Memento，14B模型优于Qwen-235B及GPT-5.2；消融实验验证专家代理关键作用，且在多个数据集均有一致提升。

Conclusion: GOPO有效提升了任务导向对话系统的长期任务成功率和生成质量，尤其在电商客服场景中表现突出，且专家代理在长期优化中至关重要。

Abstract: Large language models show potential in task-oriented dialogue systems, yet existing training methods often rely on token-level likelihood or preference optimization, which poorly align with long-horizon task success. To address this, we propose Goal-Oriented Preference Optimization (GOPO), a hierarchical reinforcement learning framework that decouples strategy planning from response generation via an Expert Agent and a Customer Service Agent. The Expert Agent optimizes multi-turn goal preferences at the dialogue-trajectory level, while the Customer Service Agent generates responses strictly aligned with the selected strategy. We evaluate GOPO on public benchmarks and e-commerce customer service datasets, and introduce Task-focused Sequential Engagement (TSE), a sequence-level metric derived from real e-commerce interaction data. On the Mgshop dataset, GOPO improves TSE by 7.7% and 10.3% over PPO and Memento, with consistent gains in sequence-level reward and generation quality. Furthermore, a 14B model trained with GOPO achieves 2.7% and 1.5% higher TSE than Qwen-235B and GPT-5.2, respectively. Ablation studies confirm the Expert Agent's critical role in long-horizon optimization. GOPO demonstrates consistent improvements across other datasets as well. This work establishes a new paradigm for task-oriented dialogue systems in commercial scenarios, with code and datasets to be made public.

</details>


### [14] [Rethinking Soft Compression in Retrieval-Augmented Generation: A Query-Conditioned Selector Perspective](https://arxiv.org/abs/2602.15856)
*Yunhao Liu,Zian Jia,Xinyu Gao,Kanjun Xu,Yun Xiong*

Main category: cs.CL

TL;DR: 该论文提出了一种选择器基础的软压缩框架SeleCom，用于改进基于外部知识的大型语言模型生成方法，解决现有压缩方法效率低下和信息稀释的问题。


<details>
  <summary>Details</summary>
Motivation: 现有软压缩完全压缩文本信息与下游生成要求冲突，且会稀释任务相关信息，导致性能不佳，因此需要一种只选择相关信息的压缩方式。

Method: 通过引入基于选择器的软压缩框架，定义编码器为查询条件信息选择器，且选择器采用解码器结构，并通过大规模多样性且难度分级的合成问答数据，用课程学习方式训练。

Result: 实验证明SeleCom在减少计算和延迟33.8%~84.6%的同时，性能显著高于现有软压缩方法，并在某些任务上达到或超过非压缩方法的表现。

Conclusion: SeleCom利用查询条件选择信息的方式替代传统的全压缩方法，显著提升了性能并降低了计算资源和延迟，优于现有软压缩方法并与非压缩基线表现持平甚至更优。

Abstract: Retrieval-Augmented Generation (RAG) effectively grounds Large Language Models (LLMs) with external knowledge and is widely applied to Web-related tasks. However, its scalability is hindered by excessive context length and redundant retrievals. Recent research on soft context compression aims to address this by encoding long documents into compact embeddings, yet they often underperform non-compressed RAG due to their reliance on auto-encoder-like full-compression that forces the encoder to compress all document information regardless of relevance to the input query.
  In this work, we conduct an analysis on this paradigm and reveal two fundamental limitations: (I) Infeasibility, full-compression conflicts with the LLM's downstream generation behavior; and (II) Non-necessity: full-compression is unnecessary and dilutes task-relevant information density. Motivated by these insights, we introduce SeleCom, a selector-based soft compression framework for RAG that redefines the encoder's role as query-conditioned information selector. The selector is decoder-only and is trained with a massive, diverse and difficulty-graded synthetic QA dataset with curriculum learning.
  Extensive experiments show that SeleCom significantly outperforms existing soft compression approaches and achieves competitive or superior performance to non-compression baselines, while reducing computation and latency by 33.8%~84.6%.

</details>


### [15] [Multi-source Heterogeneous Public Opinion Analysis via Collaborative Reasoning and Adaptive Fusion: A Systematically Integrated Approach](https://arxiv.org/abs/2602.15857)
*Yi Liu*

Main category: cs.CL

TL;DR: 提出了一种融合多源异构数据的协同推理与自适应融合框架CRAF，提升了跨平台舆情分析的准确性和适应性。


<details>
  <summary>Details</summary>
Motivation: 多源异构平台数据在结构、语义及平台偏差上存在显著差异，传统方法难以兼顾这些挑战，亟需一种融合多模态信息和跨平台语义理解的统一分析框架。

Method: 采用跨平台协同注意模块对语义表示进行对齐，利用层级自适应融合机制动态加权特征，通过联合优化策略联合学习主题和情感分布，结合OCR、ASR及视觉情感分析实现多模态信息提取。

Result: 本文提出了一个名为CRAF的协同推理与自适应融合框架，用于多平台多源异构舆情分析。通过跨平台协同注意模块、分层自适应融合机制、联合优化策略及多模态信息提取，实现了传统特征方法与大语言模型的融合。理论分析和多数据集实验证明该方法提升了主题聚类和情感分析性能，并显著降低新平台标注数据需求。

Conclusion: CRAF框架有效解决了多平台数据结构和语义差异问题，提升了主题聚类和情感分析的准确率，同时降低了新平台的标注数据需求，具备良好的跨平台适应能力。

Abstract: The analysis of public opinion from multiple heterogeneous sources presents significant challenges due to structural differences, semantic variations, and platform-specific biases. This paper introduces a novel Collaborative Reasoning and Adaptive Fusion (CRAF) framework that systematically integrates traditional feature-based methods with large language models (LLMs) through a structured multi-stage reasoning mechanism. Our approach features four key innovations: (1) a cross-platform collaborative attention module that aligns semantic representations while preserving source-specific characteristics, (2) a hierarchical adaptive fusion mechanism that dynamically weights features based on both data quality and task requirements, (3) a joint optimization strategy that simultaneously learns topic representations and sentiment distributions through shared latent spaces, and (4) a novel multimodal extraction capability that processes video content from platforms like Douyin and Kuaishou by integrating OCR, ASR, and visual sentiment analysis. Theoretical analysis demonstrates that CRAF achieves a tighter generalization bound with a reduction of O(sqrt(d log K / m)) compared to independent source modeling, where d is feature dimensionality, K is the number of sources, and m is sample size. Comprehensive experiments on three multi-platform datasets (Weibo-12, CrossPlatform-15, NewsForum-8) show that CRAF achieves an average topic clustering ARI of 0.76 (4.1% improvement over best baseline) and sentiment analysis F1-score of 0.84 (3.8% improvement). The framework exhibits strong cross-platform adaptability, reducing the labeled data requirement for new platforms by 75%.

</details>


### [16] [State Design Matters: How Representations Shape Dynamic Reasoning in Large Language Models](https://arxiv.org/abs/2602.15858)
*Annie Wong,Aske Plaat,Thomas Bäck,Niki van Stein,Anna V. Kononova*

Main category: cs.CL

TL;DR: 研究表明，状态表示设计是影响动态环境下大语言模型表现的关键因素，良好设计能提升推理稳定性和准确性，但长时间多任务推理仍存在脆弱性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在动态环境中的表现受限于状态表示方式，本研究旨在探讨如何通过改进状态表示提升模型表现。

Method: 系统性变换状态表示的三个关键方面：状态粒度（长文本与摘要）、结构（自然语言与符号）、空间定位（纯文本与图像或文本地图编码），在顺序决策任务中评估效果。

Result: 轨迹摘要通过减少噪声和稳定长时间推理提升性能；自然语言表示更稳健，结构化编码对具备代码/结构输出先验的模型有效；文本空间编码优于图像输入，因其促使模型进行空间推理。

Conclusion: 状态表示方式显著影响模型性能，尤其是在长时间和多子任务场景中，尽管改进表示有助于提升表现，但当前模型在复杂动态环境下依然脆弱。

Abstract: As large language models (LLMs) move from static reasoning tasks toward dynamic environments, their success depends on the ability to navigate and respond to an environment that changes as they interact at inference time. An underexplored factor in these settings is the representation of the state. Holding model parameters fixed, we systematically vary three key aspects: (1) state granularity (long form versus summary), (2) structure (natural language versus symbolic), and (3) spatial grounding (text-only versus images or textual map encodings) across sequential decision-making benchmarks. We find that trajectory summarisation improves performance by reducing noise and stabilising long-horizon reasoning. Second, natural language representations are the most robust across models, whereas structured encodings help mainly for models with strong code or structured output priors, such as JSON schemas. Third, while image-inputs show some benefit, text-based spatial encodings prove most effective. This advantage stems not from the spatial information itself, but from the act of construction, which compels the model to perform the spatial reasoning that static input does not elicit. Overall, we demonstrate that design choices for representing state are a decisive factor in performance, distinct from the availability of information itself. We note, however, that even with improved representations, current LLMs and VLMs remain brittle over long horizons, particularly when they must synthesise information to manage multiple subtasks to reach a goal.

</details>


### [17] [From Transcripts to AI Agents: Knowledge Extraction, RAG Integration, and Robust Evaluation of Conversational AI Assistants](https://arxiv.org/abs/2602.15859)
*Krittin Pachtrachai,Petmongkon Pornpichitsuwan,Wachiravit Modecrua,Touchapon Kraisingkorn*

Main category: cs.CL

TL;DR: 本文提出基于历史通话记录和大语言模型的会话式AI助手构建框架，在房地产和专业招聘领域实现了高准确率和良好鲁棒性，能自动处理30%通话。


<details>
  <summary>Details</summary>
Motivation: 建立高质量、准确且鲁棒的会话式AI助手面临噪声数据、知识分散及实时信息需求，特别是客户服务领域需实现有效人工转接且保障对话质量。

Method: 采用简化的PIPA框架筛选高质量对话；用大型语言模型提取结构化知识并结合RAG流水线；通过系统性提示词调优优化助手行为；利用记录驱动的用户模拟器和红队测试评估系统性能。

Result: 本文提出了一个端到端框架，通过历史呼叫记录构建和评估面向客户的会话式AI助手。先用简化的PIPA框架对输入的通话记录进行评分，筛选出高质量的对话，再利用大型语言模型从筛选文本中提取结构化知识，作为RAG流水线的唯一知识基础。助手行为通过系统性提示词调优，逐步实现模块化和可控执行。采用记录驱动的用户模拟器进行评估，包括通话覆盖率、事实准确性和人工升级行为，并进行红队测试以检验鲁棒性。实验在房地产业和专业招聘领域进行，助手能自主处理约30%呼叫，具备接近完美的事实准确率和拒绝不当请求能力，且在对抗测试中表现出较强鲁棒性。

Conclusion: 该辅助系统在挑战性领域实现了较高的自动化水平与准确性，证明了基于筛选对话数据和结构化知识、结合提示词调优的设计方法的有效性，增强了系统的鲁棒性和安全性。

Abstract: Building reliable conversational AI assistants for customer-facing industries remains challenging due to noisy conversational data, fragmented knowledge, and the requirement for accurate human hand-off - particularly in domains that depend heavily on real-time information. This paper presents an end-to-end framework for constructing and evaluating a conversational AI assistant directly from historical call transcripts. Incoming transcripts are first graded using a simplified adaptation of the PIPA framework, focusing on observation alignment and appropriate response behavior, and are filtered to retain only high-quality interactions exhibiting coherent flow and effective human agent responses. Structured knowledge is then extracted from curated transcripts using large language models (LLMs) and deployed as the sole grounding source in a Retrieval-Augmented Generation (RAG) pipeline. Assistant behavior is governed through systematic prompt tuning, progressing from monolithic prompts to lean, modular, and governed designs that ensure consistency, safety, and controllable execution. Evaluation is conducted using a transcript-grounded user simulator, enabling quantitative measurement of call coverage, factual accuracy, and human escalation behavior. Additional red teaming assesses robustness against prompt injection, out-of-scope, and out-of-context attacks. Experiments are conducted in the Real Estate and Specialist Recruitment domains, which are intentionally challenging and currently suboptimal for automation due to their reliance on real-time data. Despite these constraints, the assistant autonomously handles approximately 30 percents of calls, achieves near-perfect factual accuracy and rejection behavior, and demonstrates strong robustness under adversarial testing.

</details>


### [18] [Reranker Optimization via Geodesic Distances on k-NN Manifolds](https://arxiv.org/abs/2602.15860)
*Wen G. Gong*

Main category: cs.CL

TL;DR: Maniscope是一种基于k近邻流形测地距离的几何重排序方法，在保持高准确率的同时大幅降低延迟，适合实时检索增强生成系统。


<details>
  <summary>Details</summary>
Motivation: 目前基于交叉编码器或大型语言模型的重排序方法计算资源消耗大且响应延迟高，迫切需要一种既准确又低延迟的重排序技术以支持实时应用。

Method: Maniscope构建检索候选文档的k近邻流形，结合全局余弦相似度与局部流形几何，计算测地距离进行重排序，复杂度为O(ND + M^2D + Mk log k)，M远小于N。

Result: Maniscope通过在k近邻流形上计算测地距离，实现了对检索增强生成的高效重排序，兼顾全局余弦相似度和局部流形结构，提升语义捕捉能力。相比HNSW和交叉编码器，Maniscope在准确率上有一定提升或相近表现的同时显著降低了计算延迟，适合实时部署。

Conclusion: Maniscope提供了一种有效且计算高效的重排序方案，在复杂数据集上优于传统图形方法，并远低于大型语言模型的计算成本，适合实际应用。

Abstract: Current neural reranking approaches for retrieval-augmented generation (RAG) rely on cross-encoders or large language models (LLMs), requiring substantial computational resources and exhibiting latencies of 3-5 seconds per query. We propose Maniscope, a geometric reranking method that computes geodesic distances on k-nearest neighbor (k-NN) manifolds constructed over retrieved document candidates. This approach combines global cosine similarity with local manifold geometry to capture semantic structure that flat Euclidean metrics miss. Evaluating on eight BEIR benchmark datasets (1,233 queries), Maniscope outperforms HNSW graph-based baseline on the three hardest datasets (NFCorpus: +7.0%, TREC-COVID: +1.6%, AorB: +2.8% NDCG@3) while being 3.2x faster (4.7 ms vs 14.8 ms average). Compared to cross-encoder rerankers, Maniscope achieves within 2% accuracy at 10-45x lower latency. On TREC-COVID, LLM-Reranker provides only +0.5% NDCG@3 improvement over Maniscope at 840x higher latency, positioning Maniscope as a practical alternative for real-time RAG deployment. The method requires O(N D + M^2 D + M k log k) complexity where M << N , enabling sub-10 ms latency. We plan to release Maniscope as open-source software.

</details>


### [19] [CAST: Achieving Stable LLM-based Text Analysis for Data Analytics](https://arxiv.org/abs/2602.15861)
*Jinxiang Xie,Zihao Li,Wei He,Rui Ding,Shi Han,Dongmei Zhang*

Main category: cs.CL

TL;DR: 为解决大型语言模型在表格数据文本分析中输出不稳定的问题，提出CAST框架，通过结构化推理过程显著提升稳定性，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在处理表格数据的文本分析任务（总结和标签标注）时，输出稳定性不足，难以满足数据分析对结果一致性的高要求。

Method: 提出了CAST框架，包括算法提示（Algorithmic Prompting）和先思考再表达（Thinking-before-Speaking）两个机制，以限制模型的潜在推理路径，提高输出过程的稳定性。

Result: 通过引入CAST-S与CAST-T衡量稳定性的指标，并在多个公开基准和不同大型语言模型上进行实验，CAST在稳定性方面相较基线提升最高达16.2%，且输出质量保持或提升。

Conclusion: CAST框架有效提升了大型语言模型在表格数据文本分析任务中的输出稳定性，解决了当前方法在数据分析应用中的局限性。

Abstract: Text analysis of tabular data relies on two core operations: \emph{summarization} for corpus-level theme extraction and \emph{tagging} for row-level labeling. A critical limitation of employing large language models (LLMs) for these tasks is their inability to meet the high standards of output stability demanded by data analytics. To address this challenge, we introduce \textbf{CAST} (\textbf{C}onsistency via \textbf{A}lgorithmic Prompting and \textbf{S}table \textbf{T}hinking), a framework that enhances output stability by constraining the model's latent reasoning path. CAST combines (i) Algorithmic Prompting to impose a procedural scaffold over valid reasoning transitions and (ii) Thinking-before-Speaking to enforce explicit intermediate commitments before final generation. To measure progress, we introduce \textbf{CAST-S} and \textbf{CAST-T}, stability metrics for bulleted summarization and tagging, and validate their alignment with human judgments. Experiments across publicly available benchmarks on multiple LLM backbones show that CAST consistently achieves the best stability among all baselines, improving Stability Score by up to 16.2\%, while maintaining or improving output quality.

</details>


### [20] [Enhancing Action and Ingredient Modeling for Semantically Grounded Recipe Generation](https://arxiv.org/abs/2602.15862)
*Guoshan Liu,Bin Zhu,Yian Li,Jingjing Chen,Chong-Wah Ngo,Yu-Gang Jiang*

Main category: cs.CL

TL;DR: 该论文提出了一种语义基础的食谱生成框架，通过结合监督微调和强化微调，提升了从食物图像中生成配方的语义准确性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大型语言模型在从食物图片生成食谱时，尽管词汇匹配分数较高，但经常出现语义上的错误动作或成分。

Method: 采用两阶段方法，先通过基于动作推理数据集和食材语料的监督微调以建立基础准确性，再通过频率感知的奖励进行强化微调以提升长尾动作预测和食材泛化，最后使用语义置信评分与修正模块过滤和校正预测。

Result: 实验结果表明，该方法在Recipe1M数据集上实现了最新的性能，并显著提升了生成配方的语义忠实度。

Conclusion: 所提框架显著提升了食谱生成的语义一致性和准确性，达到了Recipe1M数据集上的最新性能。

Abstract: Recent advances in Multimodal Large Language Models (MLMMs) have enabled recipe generation from food images, yet outputs often contain semantically incorrect actions or ingredients despite high lexical scores (e.g., BLEU, ROUGE). To address this gap, we propose a semantically grounded framework that predicts and validates actions and ingredients as internal context for instruction generation. Our two-stage pipeline combines supervised fine-tuning (SFT) with reinforcement fine-tuning (RFT): SFT builds foundational accuracy using an Action-Reasoning dataset and ingredient corpus, while RFT employs frequency-aware rewards to improve long-tail action prediction and ingredient generalization. A Semantic Confidence Scoring and Rectification (SCSR) module further filters and corrects predictions. Experiments on Recipe1M show state-of-the-art performance and markedly improved semantic fidelity.

</details>


### [21] [Not the Example, but the Process: How Self-Generated Examples Enhance LLM Reasoning](https://arxiv.org/abs/2602.15863)
*Daehoon Gwak,Minseo Jung,Junwoo Park,Minho Park,ChaeHun Park,Junha Hyung,Jaegul Choo*

Main category: cs.CL

TL;DR: 研究发现大语言模型提升推理的关键在于生成示例的过程，而非示例本身，提出了一种整合生成与解题的一体化提示方法效果最佳。


<details>
  <summary>Details</summary>
Motivation: 目前尚不清楚大语言模型（LLMs）通过自生成少量示例提升推理性能的机制，使得难以有效应用该技术。

Method: 针对推理任务，系统评估了三种上下文学习提示策略：零样本提示，一体化提示（在统一提示中创建并解决问题），以及分离提示（重用生成示例但排除其创建上下文）。并进行了注意力机制分析。

Result: 实验证明一体化提示在五种模型架构中均优于零样本和分离提示，分离提示仅略优于零样本。注意力分析显示两者存在显著差异。

Conclusion: 自生成提示的优势来自于生成过程本身，而非生成的示例，有助于设计更有效的提示策略。

Abstract: Recent studies have shown that Large Language Models (LLMs) can improve their reasoning performance through self-generated few-shot examples, achieving results comparable to manually curated in-context examples. However, the underlying mechanism behind these gains remains unclear, making it hard to decide when and how to apply the technique effectively. In this work, we argue that the key benefit arises not from the generated examples themselves but from the act of creating them. To validate this, on reasoning-intensive tasks across diverse LLM architectures, we systematically evaluate three prompting strategies for in-context learning: (1) Zero-shot prompting; (2) Integrated prompting, where LLMs create and solve problems within a single, unified prompt; and (3) Decoupled prompting, where self-generated examples are reused as in-context examples, but the context of their creation itself is excluded. We conduct experiments across five widely used model architectures, demonstrating that Integrated prompting consistently outperforms both Zero-shot and Decoupled prompting. In contrast, Decoupled prompting offers only marginal gains over Zero-shot. Further, for a more in-depth analysis, we conduct an attention analysis and observe significant differences in attention patterns between Integrated and Decoupled prompting. These findings suggest that the advantage of self-generation prompting comes from the process of problem creation, not the examples themselves, providing valuable insights for designing more effective prompting strategies.

</details>


### [22] [NLP Privacy Risk Identification in Social Media (NLP-PRISM): A Survey](https://arxiv.org/abs/2602.15866)
*Dhiman Goswami,Jai Kruthunz Naveen Kumar,Sanchari Das*

Main category: cs.CL

TL;DR: 本文通过回顾文献提出NLP-PRISM隐私风险评估框架，系统分析社交媒体NLP中的隐私挑战和模型权衡，倡导隐私保护和公平性策略。


<details>
  <summary>Details</summary>
Motivation: NLP在社交媒体分析中普遍使用，但处理包含个人身份信息和行为线索的内容，存在隐私风险，需要系统评估这些风险。

Method: 通过回顾203篇同行评审论文，提出NLP-PRISM框架，评估数据收集、预处理、可见性、公平性、计算风险和合规性六个维度的隐私风险，并在六个NLP任务中应用该框架进行隐私覆盖分析。

Result: 发现Transformer模型在隐私保护微调下性能下降1%-23%，隐私风险存在明显缺口，模型效用与隐私攻击存在权衡，成员推断攻击AUC为0.81，属性推断准确率为0.75。

Conclusion: 呼吁加强匿名化、隐私感知学习和公平性驱动训练，以实现社交媒体中伦理合规的NLP应用。

Abstract: Natural Language Processing (NLP) is integral to social media analytics but often processes content containing Personally Identifiable Information (PII), behavioral cues, and metadata raising privacy risks such as surveillance, profiling, and targeted advertising. To systematically assess these risks, we review 203 peer-reviewed papers and propose the NLP Privacy Risk Identification in Social Media (NLP-PRISM) framework, which evaluates vulnerabilities across six dimensions: data collection, preprocessing, visibility, fairness, computational risk, and regulatory compliance. Our analysis shows that transformer models achieve F1-scores ranging from 0.58-0.84, but incur a 1% - 23% drop under privacy-preserving fine-tuning. Using NLP-PRISM, we examine privacy coverage in six NLP tasks: sentiment analysis (16), emotion detection (14), offensive language identification (19), code-mixed processing (39), native language identification (29), and dialect detection (24) revealing substantial gaps in privacy research. We further found a (reduced by 2% - 9%) trade-off in model utility, MIA AUC (membership inference attacks) 0.81, AIA accuracy 0.75 (attribute inference attacks). Finally, we advocate for stronger anonymization, privacy-aware learning, and fairness-driven training to enable ethical NLP in social media contexts.

</details>


### [23] [Playing With AI: How Do State-Of-The-Art Large Language Models Perform in the 1977 Text-Based Adventure Game Zork?](https://arxiv.org/abs/2602.15867)
*Berry Gerrits*

Main category: cs.CL

TL;DR: 研究通过Zork游戏检验多款大型语言模型的推理与问题解决能力，发现其表现有限，暴露出元认知和学习能力的不足。


<details>
  <summary>Details</summary>
Motivation: 评估当前大型语言模型（LLMs）在文本冒险游戏Zork中的问题解决和推理能力。

Method: 通过在Zork游戏中测试ChatGPT、Claude和Gemini等主流模型，使用最少和详细指南两种条件，主要以游戏得分衡量表现。

Result: 所有模型平均完成度低于10%，最佳模型Claude Opus 4.5仅获得约75/350分，详尽指令和延展思考均无明显效果。

Conclusion: 当前大型语言模型在文本游戏中的元认知和问题解决能力存在显著限制，表现出无法反思自身思维、策略持续性差和无法从历史对话中学习的缺陷，质疑其推理能力的本质和范围。

Abstract: In this positioning paper, we evaluate the problem-solving and reasoning capabilities of contemporary Large Language Models (LLMs) through their performance in Zork, the seminal text-based adventure game first released in 1977. The game's dialogue-based structure provides a controlled environment for assessing how LLM-based chatbots interpret natural language descriptions and generate appropriate action sequences to succeed in the game. We test the performance of leading proprietary models - ChatGPT, Claude, and Gemini - under both minimal and detailed instructions, measuring game progress through achieved scores as the primary metric. Our results reveal that all tested models achieve less than 10% completion on average, with even the best-performing model (Claude Opus 4.5) reaching only approximately 75 out of 350 possible points. Notably, providing detailed game instructions offers no improvement, nor does enabling ''extended thinking''. Qualitative analysis of the models' reasoning processes reveals fundamental limitations: repeated unsuccessful actions suggesting an inability to reflect on one's own thinking, inconsistent persistence of strategies, and failure to learn from previous attempts despite access to conversation history. These findings suggest substantial limitations in current LLMs' metacognitive abilities and problem-solving capabilities within the domain of text-based games, raising questions about the nature and extent of their reasoning capabilities.

</details>


### [24] [Understanding LLM Failures: A Multi-Tape Turing Machine Analysis of Systematic Errors in Language Model Reasoning](https://arxiv.org/abs/2602.15868)
*Magnus Boman*

Main category: cs.CL

TL;DR: 本文通过将大型语言模型（LLMs）的交互形式化为多带图灵机，精确定位其失败模式，解释了如标记化对任务的影响及连锁思维提示的作用和局限。


<details>
  <summary>Details</summary>
Motivation: 现有对LLMs失败模式的理解多依赖几何隐喻，不够严谨，且无法准确解释如标记化带来的结构丢失问题，亟需一种更精确的形式化分析工具。

Method: 构建一个确定性的多带图灵机模型，每个带对应输入字符、标记、词汇表、模型参数、激活状态、概率分布和输出文本，用于模拟LLMs的内部处理流程和交互。

Result: 通过该模型揭示了例如标记化如何掩盖字符级结构，说明了连锁思维提示通过在输出带上外部计算带来的优势及其根本限制。

Conclusion: 采用多带图灵机模型能精确定位LLMs在不同处理阶段的失败原因，提供比几何隐喻更严谨、可证伪的分析方法，深化对模型局限性的理解。

Abstract: Large language models (LLMs) exhibit failure modes on seemingly trivial tasks. We propose a formalisation of LLM interaction using a deterministic multi-tape Turing machine, where each tape represents a distinct component: input characters, tokens, vocabulary, model parameters, activations, probability distributions, and output text. The model enables precise localisation of failure modes to specific pipeline stages, revealing, e.g., how tokenisation obscures character-level structure needed for counting tasks. The model clarifies why techniques like chain-of-thought prompting help, by externalising computation on the output tape, while also revealing their fundamental limitations. This approach provides a rigorous, falsifiable alternative to geometric metaphors and complements empirical scaling laws with principled error analysis.

</details>


### [25] [Towards Fair and Efficient De-identification: Quantifying the Efficiency and Generalizability of De-identification Approaches](https://arxiv.org/abs/2602.15869)
*Noopur Zambare,Kiana Aghakasiri,Carissa Lin,Carrie Ye,J. Ross Mitchell,Mohamed Abdalla*

Main category: cs.CL

TL;DR: 本研究系统评估了不同规模和类型语言模型在临床去标识任务中的性能和泛化能力，发现小模型性能优异且更节省资源，并推出多文化去标识模型。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型在临床去标识中的泛化能力，特别是在不同格式、文化和性别间的表现差异。

Method: 对BERT系列和多个大、小型LLM进行微调评估，比较性能和推理成本，结合多语言多文化的标识符数据进行多文化微调，开发多文化去标识模型。

Result: 较小模型在性能上与大型模型相当，但推理成本大幅降低，更适合实际部署。较小模型经过少量数据微调后，在多语言和性别名称的去标识任务中优于大型模型。发布了基于BERT系列的多文化去标识模型BERT-MultiCulture-DEID，提升跨文化场景的鲁棒性。

Conclusion: 小型语言模型通过有限数据微调即能实现与大型模型相当甚至更好的临床去标识效果，且更适合实际应用。多文化训练提升模型跨语言和文化的泛化能力。

Abstract: Large language models (LLMs) have shown strong performance on clinical de-identification, the task of identifying sensitive identifiers to protect privacy. However, previous work has not examined their generalizability between formats, cultures, and genders. In this work, we systematically evaluate fine-tuned transformer models (BERT, ClinicalBERT, ModernBERT), small LLMs (Llama 1-8B, Qwen 1.5-7B), and large LLMs (Llama-70B, Qwen-72B) at de-identification. We show that smaller models achieve comparable performance while substantially reducing inference cost, making them more practical for deployment. Moreover, we demonstrate that smaller models can be fine-tuned with limited data to outperform larger models in de-identifying identifiers drawn from Mandarin, Hindi, Spanish, French, Bengali, and regional variations of English, in addition to gendered names. To improve robustness in multi-cultural contexts, we introduce and publicly release BERT-MultiCulture-DEID, a set of de-identification models based on BERT, ClinicalBERT, and ModernBERT, fine-tuned on MIMIC with identifiers from multiple language variants. Our findings provide the first comprehensive quantification of the efficiency-generalizability trade-off in de-identification and establish practical pathways for fair and efficient clinical de-identification.
  Details on accessing the models are available at: https://doi.org/10.5281/zenodo.18342291

</details>


### [26] [VDLM: Variable Diffusion LMs via Robust Latent-to-Text Rendering](https://arxiv.org/abs/2602.15870)
*Shuhui Qu*

Main category: cs.CL

TL;DR: 本文提出了VDLM模型，通过在语义变量嵌入空间中进行扩散以实现迭代优化，从而解决传统自回归语言模型推理过程中难以修正的问题。通过轨迹感知优化和向量到文本的解码方法，VDLM在多项推理和生成任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统自回归语言模型从左到右解码，无法灵活修改已生成内容，限制了多步推理能力。本文旨在通过分离语义规划和文本渲染来突破该瓶颈。

Method: 采用LLaDA风格的掩码扩散在语义变量嵌入上实现迭代精炼，结合轨迹感知的嵌入奖励和价值函数进行后训练，并利用Vec2Text渲染器和嵌入扰动提高解码鲁棒性。

Result: 在涵盖通用推理、数学和代码的九个基准测试上，VDLM在预训练阶段表现竞争力强，并在后训练阶段实现显著改善，尤其在长文本生成任务中优于其他基线。

Conclusion: VDLM通过在语义嵌入空间进行迭代优化和鲁棒的向量到文本渲染，显著提升了长文本生成的效果，优于现有基线模型。

Abstract: Autoregressive language models decode left-to-right with irreversible commitments, limiting revision during multi-step reasoning. We propose \textbf{VDLM}, a modular variable diffusion language model that separates semantic planning from text rendering. VDLM applies LLaDA-style masked diffusion over semantic variable embeddings to enable iterative refinement in latent space, then post-trains the planner with trajectory-aware optimization using embedding-space rewards and values, avoiding text decoding inside the RL loop. To convert planned embeddings back to text, we use a \textbf{Vec2Text} renderer and introduce \textbf{embedding perturbations} to robustify decoding under planner noise. Across nine benchmarks spanning general reasoning, math, and code, VDLM is competitive in pre-training and yields substantial post-training improvements on long-form generation tasks, outperforming other baselines. These results highlight the effectiveness of embedding-space post-training and robust latent-to-text rendering for diffusion language modeling.

</details>


### [27] [CheckIfExist: Detecting Citation Hallucinations in the Era of AI-Generated Content](https://arxiv.org/abs/2602.15871)
*Diletta Abbonato*

Main category: cs.CL

TL;DR: 提出了“CheckIfExist”工具，通过多数据库验证和字符串匹配算法，实时检测并验证学术引用的真实性，解决了AI生成虚假引用问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在学术写作中引入了虚假引用问题，现有工具缺乏实时验证且商业检测服务使用受限，迫切需要一个免费、即时、高效的参考文献真实性验证工具。

Method: 开发了基于多源验证的网络工具“CheckIfExist”，利用CrossRef、Semantic Scholar和OpenAlex数据库，结合级联验证架构和字符串相似度算法实现参考文献的实时真实性验证。

Result: 构建了一个支持单条及批量BibTeX条目验证的开源网络平台，能够快速返回经过验证的APA格式引用和BibTeX记录，实现秒级验证反馈。

Conclusion: 该工具有效弥补了现有文献管理工具和商业检测服务的不足，为学术引用真实性验证提供了高效、免费且易用的解决方案。

Abstract: The proliferation of large language models (LLMs) in academic workflows has introduced unprecedented challenges to bibliographic integrity, particularly through reference hallucination -- the generation of plausible but non-existent citations. Recent investigations have documented the presence of AI-hallucinated citations even in papers accepted at premier machine learning conferences such as NeurIPS and ICLR, underscoring the urgency of automated verification mechanisms. This paper presents "CheckIfExist", an open-source web-based tool designed to provide immediate verification of bibliographic references through multi-source validation against CrossRef, Semantic Scholar, and OpenAlex scholarly databases. While existing reference management tools offer bibliographic organization capabilities, they do not provide real-time validation of citation authenticity. Commercial hallucination detection services, though increasingly available, often impose restrictive usage limits on free tiers or require substantial subscription fees. The proposed tool fills this gap by employing a cascading validation architecture with string similarity algorithms to compute multi-dimensional match confidence scores, delivering instant feedback on reference authenticity. The system supports both single-reference verification and batch processing of BibTeX entries through a unified interface, returning validated APA citations and exportable BibTeX records within seconds.

</details>


### [28] [P-RAG: Prompt-Enhanced Parametric RAG with LoRA and Selective CoT for Biomedical and Multi-Hop QA](https://arxiv.org/abs/2602.15874)
*Xingda Lyu,Gongfu Lyu,Zitai Yan,Yuxin Jiang*

Main category: cs.CL

TL;DR: 本文提出了P-RAG，一种结合参数知识与检索的混合RAG架构，通过CoT提示和LoRA微调，在生物医学问答任务上显著优于标准RAG。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型依赖静态训练数据，限制了其能力。检索增强生成（RAG）可以在推理时检索外部知识，但仍依赖知识库质量，存在改进空间。

Method: 提出了Prompt-Enhanced Parametric RAG（P-RAG），结合参数化知识与检索到的证据，并使用Chain-of-Thought提示和Low-Rank Adaptation（LoRA）微调。在LLaMA-3.2-1B-Instruct模型上进行微调，并在一般及生物医学数据集上评估。

Result: P-RAG在PubMedQA上F1得分达到93.33%，比标准RAG高出10.47个百分点。在2WikiMultihopQA上整体得分达到33.44%，几乎是标准RAG的两倍。Chain-of-Thought提示显著提升多跳推理能力。

Conclusion: P-RAG展示了在生物医学问答中准确、高效且具上下文适应性的潜力，证明了结合参数化知识与检索证据的方法优于传统RAG。

Abstract: Large Language Models (LLMs) demonstrate remarkable capabilities but remain limited by their reliance on static training data. Retrieval-Augmented Generation (RAG) addresses this constraint by retrieving external knowledge during inference, though it still depends heavily on knowledge base quality. To explore potential improvements, we evaluated three RAG variants-Standard RAG, DA-RAG, and our proposed Prompt-Enhanced Parametric RAG (P-RAG), a hybrid architecture that integrates parametric knowledge within the LLM and retrieved evidence, guided by Chain-of-Thought (CoT) prompting and Low-Rank Adaptation (LoRA) fine-tuning-on both general and biomedical datasets. Using LLaMA-3.2-1B-Instruct fine-tuned via LoRA, we evaluate on PubMedQA and 2WikiMultihopQA. P-RAG outperforms Standard RAG on PubMedQA by 10.47 percentage points in F1 (93.33% vs. 82.86%; 12.64% relative). On 2WikiMultihopQA, P-RAG nearly doubles the overall score vs. Standard RAG (33.44% vs. 17.83%) and achieves 44.03% on the Compare subset (with 42.74% Bridge, 21.84% Inference, 8.60% Compose). CoT prompting substantially improves multi-hop reasoning but yields mixed results for simpler, single-hop queries. These findings underscore P-RAG's potential for accurate, scalable, and contextually adaptive biomedical question answering. Our contributions include: (1) LoRA-based fine-tuning of LLaMA-3.2-1B-Instruct for biomedical QA, (2) introduction of P-RAG with Chain-of-Thought prompting, and (3) state-of-the-art results on PubMedQA and 2WikiMultihopQA.

</details>


### [29] [Quality-constrained Entropy Maximization Policy Optimization for LLM Diversity](https://arxiv.org/abs/2602.15894)
*Haihui Pan,Yuzhong Hong,Shaoke Lv,Junwei Bao,Hongfei Jiang,Yang Song*

Main category: cs.CL

TL;DR: 提出QEMPO方法以在质量约束下最大化输出熵，有效提升大模型输出多样性且保持较高性能。


<details>
  <summary>Details</summary>
Motivation: 当前对齐方法提升大模型输出质量的同时降低输出多样性；现有提升多样性的方法通常导致性能下降。

Method: 理论上将对齐任务分解为质量分布和多样性分布；提出质量约束的熵最大化策略优化方法QEMPO；设计在线和离线训练方法以优化策略。

Result: QEMPO在保证输出质量的前提下显著提升输出多样性，性能达到或优于RLHF。

Conclusion: QEMPO通过分解任务和熵最大化策略，在提升多样性的同时保证输出质量，证明了其优于现有对齐方法的效果。

Abstract: Recent research indicates that while alignment methods significantly improve the quality of large language model(LLM) outputs, they simultaneously reduce the diversity of the models' output. Although some methods have been proposed to enhance LLM output diversity, they often come at the cost of reduced performance. In this work, we first theoretically demonstrate that the alignment task can be decomposed into two distributions: quality and diversity. To enhance the diversity of LLM outputs while ensuring quality, we propose the Quality-constrained Entropy Maximization Policy Optimization (QEMPO). QEMPO aims to maximize the output entropy of the policy while ensuring output quality. By adding different constraints to QEMPO, we obtain different policies. To optimize policies, we propose both online and offline training methods. Experiments validate that QEMPO achieves performance comparable to or even better than RLHF while improving output diversity.

</details>


### [30] [Understand Then Memory: A Cognitive Gist-Driven RAG Framework with Global Semantic Diffusion](https://arxiv.org/abs/2602.15895)
*Pengcheng Zhou,Haochen Li,Zhiqiang Nie,JiaLe Chen,Qing Gong,Weizhen Zhang,Chun Yu*

Main category: cs.CL

TL;DR: 本文提出了模拟人类认知记忆的检索增强生成框架CogitoRAG，通过构建多维知识图谱和智能查询分解，实现了更精准的检索和推理，显著超越了当前先进RAG技术。


<details>
  <summary>Details</summary>
Motivation: 现有RAG框架中离散文本表示导致语义完整性丧失和检索偏差，启发于人类事件记忆机制，旨在通过模拟认知过程改善检索的语义准确性和信息支持密度。

Method: 提出了一种模拟人类认知记忆的RAG框架CogitoRAG，包括语义摘要提取与演化、多维知识图谱构建、查询分解模块、关联实体扩散模块及CogniRank重排序算法，最终以段落-记忆配对形式支持生成器提供高密度信息。

Result: 在五个主流问答基准和GraphBench多任务生成测试中，CogitoRAG显著优于最先进RAG方法，表现出更强的复杂知识整合及推理能力。

Conclusion: CogitoRAG框架通过模拟人类认知记忆过程，有效提升了检索增强生成（RAG）方法在复杂知识整合和推理中的表现，显著优于现有最先进的RAG方法。

Abstract: Retrieval-Augmented Generation (RAG) effectively mitigates hallucinations in LLMs by incorporating external knowledge. However, the inherent discrete representation of text in existing frameworks often results in a loss of semantic integrity, leading to retrieval deviations. Inspired by the human episodic memory mechanism, we propose CogitoRAG, a RAG framework that simulates human cognitive memory processes. The core of this framework lies in the extraction and evolution of the Semantic Gist. During the offline indexing stage, CogitoRAG first deduces unstructured corpora into gist memory corpora, which are then transformed into a multi-dimensional knowledge graph integrating entities, relational facts, and memory nodes. In the online retrieval stage, the framework handles complex queries via Query Decomposition Module that breaks them into comprehensive sub-queries, mimicking the cognitive decomposition humans employ for complex information. Subsequently, Entity Diffusion Module performs associative retrieval across the graph, guided by structural relevance and an entity-frequency reward mechanism. Furthermore, we propose the CogniRank algorithm, which precisely reranks candidate passages by fusing diffusion-derived scores with semantic similarity. The final evidence is delivered to the generator in a passage-memory pairing format, providing high-density information support. Experimental results across five mainstream QA benchmarks and multi-task generation on GraphBench demonstrate that CogitoRAG significantly outperforms state-of-the-art RAG methods, showcasing superior capabilities in complex knowledge integration and reasoning.

</details>


### [31] [Every Little Helps: Building Knowledge Graph Foundation Model with Fine-grained Transferable Multi-modal Tokens](https://arxiv.org/abs/2602.15896)
*Yichi Zhang,Zhuo Chen,Lingbing Guo,Wen Zhang,Huajun Chen*

Main category: cs.CL

TL;DR: 本文提出了一种基于token的多模态知识图谱推理基础模型TOFU，能够融合结构、视觉和文本信息，实现跨不同多模态知识图谱强泛化能力，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有多模态知识图谱推理多局限于横向数据内学习，缺乏跨知识图谱的泛化能力；已有基础模型虽具跨图迁移能力但主要利用结构信息，忽略多模态丰富信号。

Method: TOFU将结构、视觉和文本信息离散为模态专属token，采用层次融合架构和消息混合机制处理这些token，从而获得可迁移特征用于多模态知识图谱推理。

Result: 在17个不同类型的多模态知识图谱推理任务中，TOFU稳定超越强基线模型，无论是横向、归纳还是完全归纳场景，表现出优秀的泛化和推理效果。

Conclusion: TOFU模型通过多模态信息的离散化和层次融合机制，有效提升了多模态知识图谱推理的泛化能力，在17个不同设置的数据集上均优于目前的知识图谱基础模型和多模态知识图谱推理方法。

Abstract: Multi-modal knowledge graph reasoning (MMKGR) aims to predict the missing links by exploiting both graph structure information and multi-modal entity contents. Most existing works are designed for a transductive setting, which learns dataset-specific embeddings and struggles to generalize to new KGs. Recent knowledge graph foundation models (KGFMs) improve cross-KG transfer, but they mainly exploit structural patterns and ignore rich multi-modal signals. We address these gaps by proposing a token-based foundation model (TOFU) for MMKGR, which exhibits strong generalization across different MMKGs. TOFU discretizes structural, visual, and textual information into modality-specific tokens. TOFU then employs a hierarchical fusion architecture with mixture-of-message mechanisms, aiming to process these tokens and obtain transferable features for MMKGR. Experimental results on 17 transductive, inductive, and fully-inductive MMKGs show that TOFU consistently outperforms strong KGFM and MMKGR baselines, delivering strong performance on unseen MMKGs.

</details>


### [32] [Mitigating Gradient Inversion Risks in Language Models via Token Obfuscation](https://arxiv.org/abs/2602.15897)
*Xinguo Feng,Zhongkui Ma,Zihan Wang,Alsharif Abuadbba,Guangdong Bai*

Main category: cs.CL

TL;DR: 针对梯度反演攻击导致训练数据泄露的问题，提出了一种名为GHOST的防御机制，通过在token层面进行混淆，打破梯度、嵌入和token空间的关联，有效保护隐私且保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于梯度扰动的防御方法因梯度、嵌入和token空间间的语义相似性仍被破解，亟需从token层面切断这些空间的内在联系以防止梯度反演攻击。

Method: GHOST包含搜索步骤和选择步骤，首先搜索语义不同但嵌入接近的候选token，再选择最佳的替代token以最小化对训练特征的干扰，达到在保持嵌入和梯度空间连接的同时切断token空间语义连通性的目的。

Result: 在BERT到Llama等多种模型和数据集上的实验显示，GHOST防御机制使恢复率降至1%，分类任务F1达0.92，生成任务困惑度为5.45，有效抵抗最新梯度反演攻击和自适应攻击。

Conclusion: GHOST通过引入语义不同但嵌入接近的替代token，实现了对梯度反演攻击的有效防御，在多模型和多任务上均表现出高隐私保护率和良好实用性。

Abstract: Training and fine-tuning large-scale language models largely benefit from collaborative learning, but the approach has been proven vulnerable to gradient inversion attacks (GIAs), which allow adversaries to reconstruct private training data from shared gradients. Existing defenses mainly employ gradient perturbation techniques, e.g., noise injection or gradient pruning, to disrupt GIAs' direct mapping from gradient space to token space. However, these methods often fall short due to the retention of semantics similarity across gradient, embedding, and token spaces. In this work, we propose a novel defense mechanism named GHOST (gradient shield with obfuscated tokens), a token-level obfuscation mechanism that neutralizes GIAs by decoupling the inherent connections across gradient, embedding, and token spaces. GHOST is built upon an important insight: due to the large scale of the token space, there exist semantically distinct yet embedding-proximate tokens that can serve as the shadow substitutes of the original tokens, which enables a semantic disconnection in the token space while preserving the connection in the embedding and gradient spaces. GHOST comprises a searching step, which identifies semantically distinct candidate tokens using a multi-criteria searching process, and a selection step, which selects optimal shadow tokens to ensure minimal disruption to features critical for training by preserving alignment with the internal outputs produced by original tokens. Evaluation across diverse model architectures (from BERT to Llama) and datasets demonstrates the remarkable effectiveness of GHOST in protecting privacy (as low as 1% in recovery rate) and preserving utility (up to 0.92 in classification F1 and 5.45 in perplexity), in both classification and generation tasks against state-of-the-art GIAs and adaptive attack scenarios.

</details>


### [33] [MultiCube-RAG for Multi-hop Question Answering](https://arxiv.org/abs/2602.15898)
*Jimeng Shi,Wei Hu,Runchu Tian,Bowen Jin,Wonbin Kweon,SeongKu Kang,Yunfan Kang,Dingqi Ye,Sizhe Zhou,Shaowen Wang,Jiawei Han*

Main category: cs.CL

TL;DR: MultiCube-RAG利用本体立方体结构进行多步推理和检索，显著提升多跳问答准确率和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的多跳问答（QA）方法难以准确捕捉结构化语义，且多步检索过程受到限制，训练过程不稳定且计算开销高。

Method: 提出基于本体的多维立方体结构，设计无需训练的MultiCube-RAG方法，通过多个专门化的立方体进行多步推理和检索，将复杂查询分解为子查询逐步解决。

Result: 在四个多跳问答数据集上，MultiCube-RAG的响应准确率提升了8.9%，效率更高且具有内在可解释性。

Conclusion: MultiCube-RAG有效弥补了传统RAG方法在多跳推理和结构语义表示上的不足，提供了一种高效且可解释的检索增强生成方案。

Abstract: Multi-hop question answering (QA) necessitates multi-step reasoning and retrieval across interconnected subjects, attributes, and relations. Existing retrieval-augmented generation (RAG) methods struggle to capture these structural semantics accurately, resulting in suboptimal performance. Graph-based RAGs structure such information in graphs, but the resulting graphs are often noisy and computationally expensive. Moreover, most methods rely on single-step retrieval, neglecting the need for multi-hop reasoning processes. Recent training-based approaches attempt to incentivize the large language models (LLMs) for iterative reasoning and retrieval, but their training processes are prone to unstable convergence and high computational overhead. To address these limitations, we devise an ontology-based cube structure with multiple and orthogonal dimensions to model structural subjects, attributes, and relations. Built on the cube structure, we propose MultiCube-RAG, a training-free method consisting of multiple cubes for multi-step reasoning and retrieval. Each cube specializes in modeling a class of subjects, so that MultiCube-RAG flexibly selects the most suitable cubes to acquire the relevant knowledge precisely. To enhance the query-based reasoning and retrieval, our method decomposes a complex multi-hop query into a set of simple subqueries along cube dimensions and conquers each of them sequentially. Experiments on four multi-hop QA datasets show that MultiCube-RAG improves response accuracy by 8.9% over the average performance of various baselines. Notably, we also demonstrate that our method performs with greater efficiency and inherent explainability.

</details>


### [34] [Doc-to-LoRA: Learning to Instantly Internalize Contexts](https://arxiv.org/abs/2602.15902)
*Rujikorn Charakorn,Edoardo Cetin,Shinnosuke Uesaka,Robert Tjarko Lange*

Main category: cs.CL

TL;DR: 提出Doc-to-LoRA方法，通过生成适配器实现高效上下文蒸馏，显著提升长序列推理的效率和准确率。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer模型在处理长输入序列时的高内存和低速问题，传统的上下文蒸馏训练成本高且延迟大，不适合实际应用。

Method: 提出Doc-to-LoRA (D2L)，一个轻量级超网络，通过单次前向传播进行近似的上下文蒸馏，生成LoRA适配器，使目标LLM在推理时无需重新加载原始上下文，减少延迟和内存占用。

Result: 在长上下文检索任务中，D2L实现了超过目标LLM本地上下文窗口4倍长度的接近完美的零样本准确率；在有限计算资源的真实问答数据集上，D2L优于标准上下文蒸馏，显著降低峰值内存和更新延迟。

Conclusion: D2L能够快速适配大型语言模型，大幅提升长上下文推理效率，具备频繁知识更新和个性化对话行为的应用潜力。

Abstract: Long input sequences are central to in-context learning, document understanding, and multi-step reasoning of Large Language Models (LLMs). However, the quadratic attention cost of Transformers makes inference memory-intensive and slow. While context distillation (CD) can transfer information into model parameters, per-prompt distillation is impractical due to training costs and latency. To address these limitations, we propose Doc-to-LoRA (D2L), a lightweight hypernetwork that meta-learns to perform approximate CD within a single forward pass. Given an unseen prompt, D2L generates a LoRA adapter for a target LLM, enabling subsequent queries to be answered without re-consuming the original context, reducing latency and KV-cache memory consumption during inference of the target LLM. On a long-context needle-in-a-haystack task, D2L successfully learns to map contexts into adapters that store the needle information, achieving near-perfect zero-shot accuracy at sequence lengths exceeding the target LLM's native context window by more than 4x. On real-world QA datasets with limited compute, D2L outperforms standard CD while significantly reducing peak memory consumption and update latency. We envision that D2L can facilitate rapid adaptation of LLMs, opening up the possibility of frequent knowledge updates and personalized chat behavior.

</details>


### [35] [DocSplit: A Comprehensive Benchmark Dataset and Evaluation Approach for Document Packet Recognition and Splitting](https://arxiv.org/abs/2602.15958)
*Md Mofijul Islam,Md Sirajus Salekin,Nivedha Balakrishnan,Vincil C. Bishop,Niharika Jain,Spencer Romo,Bob Strahan,Boyi Xie,Diego A. Socolinsky*

Main category: cs.CL

TL;DR: 本文提出了DocSplit文档包拆分基准数据集和评估方法，揭示多模态大语言模型在文档拆分任务中的不足，推动文档理解研究发展。


<details>
  <summary>Details</summary>
Motivation: 现实世界文档处理常涉及多个文档混合的文档包，需自动拆分成单独文档，但现有视觉文档理解技术未充分解决此基本问题，亟需基准数据集和评价体系推动研究。

Method: 构建包含多样文档类型和布局的五个子数据集，设计任务要求模型识别文档边界、分类、并保证页码顺序，开发新的评估指标，使用多模态大语言模型进行实验验证。

Result: 本文提出了针对现实应用中多页、多文档混合的文档包拆分问题的首个综合基准数据集DocSplit及相应评估指标。DocSplit包含五个难度不同、文档类型和布局各异的数据集，任务要求模型识别文档边界、分类文档类型并维护正确的页码顺序。研究揭示现有多模态大语言模型在复杂文档拆分任务上的表现仍有较大不足。通过该基准和指标为法律、金融、医疗等领域的文档理解研究提供了系统框架，并公开了数据集以促进后续研究。

Conclusion: 当前多模态大语言模型在复杂文档包拆分任务中表现不佳，DocSplit数据集和评估指标将促进更有效的文档理解方法研究。

Abstract: Document understanding in real-world applications often requires processing heterogeneous, multi-page document packets containing multiple documents stitched together. Despite recent advances in visual document understanding, the fundamental task of document packet splitting, which involves separating a document packet into individual units, remains largely unaddressed. We present the first comprehensive benchmark dataset, DocSplit, along with novel evaluation metrics for assessing the document packet splitting capabilities of large language models. DocSplit comprises five datasets of varying complexity, covering diverse document types, layouts, and multimodal settings. We formalize the DocSplit task, which requires models to identify document boundaries, classify document types, and maintain correct page ordering within a document packet. The benchmark addresses real-world challenges, including out-of-order pages, interleaved documents, and documents lacking clear demarcations. We conduct extensive experiments evaluating multimodal LLMs on our datasets, revealing significant performance gaps in current models' ability to handle complex document splitting tasks. The DocSplit benchmark datasets and proposed novel evaluation metrics provide a systematic framework for advancing document understanding capabilities essential for legal, financial, healthcare, and other document-intensive domains. We release the datasets to facilitate future research in document packet processing.

</details>


### [36] [A Curious Class of Adpositional Multiword Expressions in Korean](https://arxiv.org/abs/2602.16023)
*Junghyun Min,Na-Rae Han,Jena D. Hwang,Nathan Schneider*

Main category: cs.CL

TL;DR: 本文系统分析了韩语后置动词多词表达，建立了标注准则，填补了韩语多词语料资源的空白，促进跨语言对齐应用。


<details>
  <summary>Details</summary>
Motivation: 目前跨语言多词表达标注框架中，韩语多词表达特别是多词后置词组缺乏系统性分析和标注资源，亟需建立相关资源以促进跨语言对齐。

Method: 利用韩语维基百科数据，调查分析多种后置动词结构（PVCs），并与结构相似的非多词表达和轻动词构式进行对比分析。

Result: 提出了一套针对韩语多词后置词组的标注指导原则，有助于未来韩语多词表达的研究和跨语言框架的配合。

Conclusion: 本文针对韩语功能性多词表达（PVCs）进行了系统分析，制定了针对韩语多词后置词组的标注准则，推动其在跨语言框架中的整合。

Abstract: Multiword expressions (MWEs) have been widely studied in cross-lingual annotation frameworks such as PARSEME. However, Korean MWEs remain underrepresented in these efforts. In particular, Korean multiword adpositions lack systematic analysis, annotated resources, and integration into existing multilingual frameworks. In this paper, we study a class of Korean functional multiword expressions: postpositional verb-based constructions (PVCs). Using data from Korean Wikipedia, we survey and analyze several PVC expressions and contrast them with non-MWEs and light verb constructions (LVCs) with similar structure. Building on this analysis, we propose annotation guidelines designed to support future work in Korean multiword adpositions and facilitate alignment with cross-lingual frameworks.

</details>


### [37] [CLAA: Cross-Layer Attention Aggregation for Accelerating LLM Prefill](https://arxiv.org/abs/2602.16054)
*Bradley McDanel,Steven Li,Harshit Khaitan*

Main category: cs.CL

TL;DR: 本文针对长上下文大语言模型推理中的预填充阶段计算瓶颈，提出了基于答案信息的Oracle方法评估和改进token排序稳定性，并通过跨层注意力聚合方法显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 长上下文大语言模型推理中预填充阶段计算成本高，现有通过token排名启发式方法受token重要性估计不稳定影响，评价和优化方法亟需改进。

Method: 引入答案信息Oracle，通过测量生成答案对提示的注意力来定义token重要性，发现现有启发式方法在层间表现不稳定，进而设计跨层注意力聚合(CLAA)方法将各层分数综合，提升排序稳定性和推理速度。

Result: CLAA方法相比完整KV缓存基线，推理生成第一个token的时间减少最多达39%，有效提升了长上下文推理的计算效率。

Conclusion: 跨层注意力聚合(CLAA)能够稳定token排序重要性，接近Oracle的上界，显著减少推理延迟，提升长上下文LLM的推理效率。

Abstract: The prefill stage in long-context LLM inference remains a computational bottleneck. Recent token-ranking heuristics accelerate inference by selectively processing a subset of semantically relevant tokens. However, existing methods suffer from unstable token importance estimation, often varying between layers. Evaluating token-ranking quality independently from heuristic-specific architectures is challenging. To address this, we introduce an Answer-Informed Oracle, which defines ground-truth token importance by measuring attention from generated answers back to the prompt. This oracle reveals that existing heuristics exhibit high variance across layers: rankings can degrade sharply at specific layers, a failure mode invisible to end-to-end benchmarks. The diagnosis suggests a simple fix: aggregate scores across layers rather than relying on any single one. We implement this as Cross-Layer Attention Aggregation (CLAA), which closes the gap to the oracle upper bound and reduces Time-to-First-Token (TTFT) by up to 39\% compared to the Full KV Cache baseline.

</details>


### [38] [Surgical Activation Steering via Generative Causal Mediation](https://arxiv.org/abs/2602.16080)
*Aruna Sankaranarayanan,Amir Zur,Atticus Geiger,Dylan Hadfield-Menell*

Main category: cs.CL

TL;DR: 本文提出的生成因果中介(GCM)方法能精确定位语言模型中控制长篇回答行为的关键组件，并有效实现行为引导，优于传统基于相关性的探测方法。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在处理跨多个标记的长篇回答时，行为表现难以精确控制，亟需一种定位并干预模型内部组件以实现行为控制的方法。

Method: 提出生成因果中介(GCM)方法，通过构建对比输入和响应的数据集，量化模型各组件（如注意力头）在传递二元概念（如诗体与散文）中的作用，选择最强中介进行行为引导。

Result: GCM在拒绝、谄媚和风格转换三个任务中，在三种语言模型上均成功定位了长篇响应中的概念，并在用稀疏的注意力头进行引导时，显著优于基于关联探测的基线方法。

Conclusion: GCM提供了一种有效的方法，可以精确定位和控制语言模型长篇回答中的行为，证明其在行为干预上的应用潜力。

Abstract: Where should we intervene in a language model (LM) to control behaviors that are diffused across many tokens of a long-form response? We introduce Generative Causal Mediation (GCM), a procedure for selecting model components, e.g., attention heads, to steer a binary concept (e.g., talk in verse vs. talk in prose) from contrastive long-form responses. In GCM, we first construct a dataset of contrasting inputs and responses. Then, we quantify how individual model components mediate the contrastive concept and select the strongest mediators for steering. We evaluate GCM on three tasks--refusal, sycophancy, and style transfer--across three language models. GCM successfully localizes concepts expressed in long-form responses and consistently outperforms correlational probe-based baselines when steering with a sparse set of attention heads. Together, these results demonstrate that GCM provides an effective approach for localizing and controlling the long-form responses of LMs.

</details>


### [39] [Language Statistics and False Belief Reasoning: Evidence from 41 Open-Weight LMs](https://arxiv.org/abs/2602.16085)
*Sean Trott,Samuel Taylor,Cameron Jones,James A. Michaelov,Pamela D. Rivière*

Main category: cs.CL

TL;DR: 通过对41个开源语言模型在错误信念任务上的测试，发现部分模型具备心理状态推理能力，且大型模型表现更好；语言模型行为可帮助生成和验证关于人类认知的新假设，强调开源大样本模型的重要意义。


<details>
  <summary>Details</summary>
Motivation: 现有研究多依赖少量闭源语言模型，限制了对心理学理论的严格测试和语言模型能力的全面评估。因此，研究者希望通过开源模型大规模实验来更深入理解语言模型的心理状态推理能力及其与人类认知的联系。

Method: 在41个不同模型家族的开源语言模型上复制并扩展错误信念任务的研究，通过测试它们对隐含知识状态的敏感性来评估语言模型的心理状态推理行为。

Result: 在测试的模型中，34%的语言模型表现出对隐含知识状态的敏感性，但无一模型能够完全解释人类的心理状态推理效果。较大的语言模型表现出更高的敏感性和更强的心理测量预测能力。同时发现人类和语言模型在使用非事实性动词引发的错误信念归因偏差表现类似，这表明语言的分布式统计信息可以解释这一现象，但不能完全解释人类的主要心理状态敏感效果。

Conclusion: 使用大量开源语言模型检测心理状态推理和测试人类认知理论具有重要价值，能更全面评估模型能力并揭示语言统计特征在认知心理学中的作用，促进对人类社会认知机制的理解。

Abstract: Research on mental state reasoning in language models (LMs) has the potential to inform theories of human social cognition--such as the theory that mental state reasoning emerges in part from language exposure--and our understanding of LMs themselves. Yet much published work on LMs relies on a relatively small sample of closed-source LMs, limiting our ability to rigorously test psychological theories and evaluate LM capacities. Here, we replicate and extend published work on the false belief task by assessing LM mental state reasoning behavior across 41 open-weight models (from distinct model families). We find sensitivity to implied knowledge states in 34% of the LMs tested; however, consistent with prior work, none fully ``explain away'' the effect in humans. Larger LMs show increased sensitivity and also exhibit higher psychometric predictive power. Finally, we use LM behavior to generate and test a novel hypothesis about human cognition: both humans and LMs show a bias towards attributing false beliefs when knowledge states are cued using a non-factive verb (``John thinks...'') than when cued indirectly (``John looks in the...''). Unlike the primary effect of knowledge states, where human sensitivity exceeds that of LMs, the magnitude of the human knowledge cue effect falls squarely within the distribution of LM effect sizes-suggesting that distributional statistics of language can in principle account for the latter but not the former in humans. These results demonstrate the value of using larger samples of open-weight LMs to test theories of human cognition and evaluate LM capacities.

</details>


### [40] [Updating Parametric Knowledge with Context Distillation Retains Post-Training Capabilities](https://arxiv.org/abs/2602.16093)
*Shankar Padmanabhan,Mustafa Omer Gul,Tanya Goyal*

Main category: cs.CL

TL;DR: DiSC通过上下文蒸馏实现持续知识适应，有效平衡新知识学习与旧知识遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法兼顾持续学习新知识和防止旧知识遗忘，亟需一种简单有效的持续知识适应方法以提升后训练大模型的性能。

Method: DiSC利用训练样本不同片段生成教师和学生模型分布，最小化共享令牌的KL散度，实现上下文蒸馏，避免了显式生成的复杂训练过程。

Result: 本文提出了一种名为DiSC的基于上下文蒸馏的持续知识适应方法，能够在引入新知识的同时减少对之前知识和技能的遗忘。通过对训练样本的不同片段分别构建师生模型分布，并最小化共享令牌的KL散度，实现了高效的上下文蒸馏，无需显式生成步骤。实验证明，DiSC在多个后训练模型和适应领域中相比传统微调和蒸馏方法，在学习新知识与保留旧能力之间取得了最佳平衡。

Conclusion: DiSC方法在持续知识适应中优于现有微调和蒸馏技术，能同时学习新知识并减少模型遗忘，提升了预训练大模型的适应性与稳定性。

Abstract: Post-training endows pretrained LLMs with a variety of desirable skills, including instruction-following, reasoning, and others. However, these post-trained LLMs only encode knowledge up to a cut-off date, necessitating continual adaptation. Unfortunately, existing solutions cannot simultaneously learn new knowledge from an adaptation document corpora and mitigate the forgetting of earlier learned capabilities. To address this, we introduce Distillation via Split Contexts (DiSC), a simple context-distillation based approach for continual knowledge adaptation. \methodname~derives student and teacher distributions by conditioning on distinct segments of the training example and minimizes the KL divergence between the shared tokens. This allows us to efficiently apply context-distillation without requiring explicit generation steps during training. We run experiments on four post-trained models and two adaptation domains. Compared to prior finetuning and distillation methods for continual adaptation, DiSC consistently reports the best trade-off between learning new knowledge and mitigating forgetting of previously learned skills like instruction-following, reasoning, and factual knowledge.

</details>


### [41] [Missing-by-Design: Certifiable Modality Deletion for Revocable Multimodal Sentiment Analysis](https://arxiv.org/abs/2602.16144)
*Rong Fu,Wenxin Zhang,Ziming Wang,Chunlei Meng,Jiaxuan Lu,Jiekai Wu,Kangan Qian,Hao Zhang,Simon Fong*

Main category: cs.CL

TL;DR: 提出一种缺失设计(MBD)框架，实现多模态数据的选择性删除和恢复，确保隐私合规下的高效情感分析。


<details>
  <summary>Details</summary>
Motivation: 多模态系统处理敏感个人数据，用户或监管机构可能要求删除特定数据模态以满足隐私合规和用户自主权，因此需要一种选择性撤销数据模态的能力。

Method: 提出Missing-by-Design (MBD)框架，将结构化表示学习与可认证的参数修改流程结合。MBD通过学习属性感知的嵌入和基于生成器的重构来恢复缺失的模态，同时保留任务相关信号。针对删除请求，采用显著性驱动的候选选择和校准的高斯更新，生成机器可验证的模态删除证书。

Result: 在基准数据集上的实验表明，MBD在输入不完整时仍保持较强的预测性能，实现了隐私保护和效用之间的实用权衡。其手术式忘记策略成为全量重训的高效替代方案。

Conclusion: MBD为多模态情感分析中的选择性数据模态撤销提供了一种有效且可验证的方法，兼顾性能和隐私需求，提升了多模态系统的用户自主权和合规能力。

Abstract: As multimodal systems increasingly process sensitive personal data, the ability to selectively revoke specific data modalities has become a critical requirement for privacy compliance and user autonomy. We present Missing-by-Design (MBD), a unified framework for revocable multimodal sentiment analysis that combines structured representation learning with a certifiable parameter-modification pipeline. Revocability is critical in privacy-sensitive applications where users or regulators may request removal of modality-specific information. MBD learns property-aware embeddings and employs generator-based reconstruction to recover missing channels while preserving task-relevant signals. For deletion requests, the framework applies saliency-driven candidate selection and a calibrated Gaussian update to produce a machine-verifiable Modality Deletion Certificate. Experiments on benchmark datasets show that MBD achieves strong predictive performance under incomplete inputs and delivers a practical privacy-utility trade-off, positioning surgical unlearning as an efficient alternative to full retraining.

</details>


### [42] [Balancing Faithfulness and Performance in Reasoning via Multi-Listener Soft Execution](https://arxiv.org/abs/2602.16154)
*Nithin Sivakumaran,Shoubin Yu,Hyunji Lee,Yue Zhang,Ali Payani,Mohit Bansal,Elias Stengel-Eskin*

Main category: cs.CL

TL;DR: 为提升大语言模型推理的真实性和可解释性，提出多方强化学习方法REMUL，显著提高了推理质量和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前链式思考（CoT）推理结果真实性不足，影响模型解释能力，且提升真实性通常会牺牲任务性能。

Method: 提出REMUL方法，利用多方强化学习框架，通过一个说话者模型生成推理轨迹，多个听者模型执行并延续轨迹，且结合掩码监督微调以调和真实性与性能的矛盾。

Result: REMUL在多个推理基准测试上显著提升了推理真实性指标和准确率，同时生成的思考链更简洁并易于理解。

Conclusion: REMUL方法有效缓解了推理真实性与性能的矛盾，增强了模型思考轨迹的真实性和可读性，并带来更精准的推理结果。

Abstract: Chain-of-thought (CoT) reasoning sometimes fails to faithfully reflect the true computation of a large language model (LLM), hampering its utility in explaining how LLMs arrive at their answers. Moreover, optimizing for faithfulness and interpretability in reasoning often degrades task performance. To address this tradeoff and improve CoT faithfulness, we propose Reasoning Execution by Multiple Listeners (REMUL), a multi-party reinforcement learning approach. REMUL builds on the hypothesis that reasoning traces which other parties can follow will be more faithful. A speaker model generates a reasoning trace, which is truncated and passed to a pool of listener models who "execute" the trace, continuing the trace to an answer. Speakers are rewarded for producing reasoning that is clear to listeners, with additional correctness regularization via masked supervised finetuning to counter the tradeoff between faithfulness and performance. On multiple reasoning benchmarks (BIG-Bench Extra Hard, MuSR, ZebraLogicBench, and FOLIO), REMUL consistently and substantially improves three measures of faithfulness -- hint attribution, early answering area over the curve (AOC), and mistake injection AOC -- while also improving accuracy. Our analysis finds that these gains are robust across training domains, translate to legibility gains, and are associated with shorter and more direct CoTs.

</details>


### [43] [LLMs Exhibit Significantly Lower Uncertainty in Creative Writing Than Professional Writers](https://arxiv.org/abs/2602.16162)
*Peiqi Sui*

Main category: cs.CL

TL;DR: 论文揭示大型语言模型在创意写作上存在不确定性不足的问题，指出为实现人类水平的创造力需开发能够区分建设性模糊与破坏性幻觉的新型不确定性感知对齐方法。


<details>
  <summary>Details</summary>
Motivation: 文学理论表明不确定性是创造性表达的必要条件，而现有对齐策略为提高事实性和减少幻觉，通常降低模型输出的不确定性，这限制了模型的创意能力。

Method: 通过对28款大型语言模型在高质量叙事数据集上的信息论分析，量化人类创作与模型生成作品之间的不确定性差距，并比较不同模型的表现。

Result: 实验证明人类作品展现出明显更高的不确定性；指令微调和推理模型加剧了这一差距；这种差异在创意写作领域尤为显著且与写作质量密切相关。

Conclusion: 当前大型语言模型在创意写作中的表现受限于缺乏适当的不确定性表现，这导致其生成内容常显陈词滥调且缺乏新意。

Abstract: We argue that uncertainty is a key and understudied limitation of LLMs' performance in creative writing, which is often characterized as trite and cliché-ridden. Literary theory identifies uncertainty as a necessary condition for creative expression, while current alignment strategies steer models away from uncertain outputs to ensure factuality and reduce hallucination. We formalize this tension by quantifying the "uncertainty gap" between human-authored stories and model-generated continuations. Through a controlled information-theoretic analysis of 28 LLMs on high-quality storytelling datasets, we demonstrate that human writing consistently exhibits significantly higher uncertainty than model outputs. We find that instruction-tuned and reasoning models exacerbate this trend compared to their base counterparts; furthermore, the gap is more pronounced in creative writing than in functional domains, and strongly correlates to writing quality. Achieving human-level creativity requires new uncertainty-aware alignment paradigms that can distinguish between destructive hallucinations and the constructive ambiguity required for literary richness.

</details>


### [44] [Beyond Learning: A Training-Free Alternative to Model Adaptation](https://arxiv.org/abs/2602.16189)
*Namkyung Yoon,Kyeonghyun Yoo,Wooyong Jung,Sanghong Kim,Hwangnam Kim*

Main category: cs.CL

TL;DR: 本文提出了一种模型内部模块移植技术，通过识别和移植特定任务相关的局部激活模块，实现语言模型性能的即时提升，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 现有提升语言模型性能的方法资源消耗大，亟需一种能够即时起效、低资源消耗的替代方案。

Method: 基于激活分析识别特定任务相关的局部模块，将其从一个模型移植到目标模型中，无需训练即可实现功能改变。

Result: 在跨代和指令调优模型中，移植激活选定模块显著提升目标模型性能，最高可达到目标基线的2.33倍，体现了模块移植的有效性。

Conclusion: 通过移植内部功能模块，能够显著改善表现不佳的语言模型性能，验证了语言模型存在任务局部模块化的特性，开辟了模型移植的新研究方向。

Abstract: Despite the continuous research and evolution of language models, they sometimes underperform previous versions. Existing approaches to overcome these challenges are resource-intensive, highlighting the need for alternatives that enable immediate action. We assume that each language model has a local module inside that is suitable for a specific function. First, this work identifies a set of modules showing consistent and local activation changes under an inference workload through activation-based analysis. Subsequently, we transplant an internal module that is properly activated for a specific task into the target model, leading to immediate and measurable functional changes without additional training or fine-tuning. To experimentally demonstrate the effectiveness of the transplant technique, we quantify the relationship between transplant strength and performance improvement under different conditions for two language models. In the cross-generation setting, we find that transplanting activation-selected modules can substantially improve the underperforming model, reaching up to twice the target baseline and achieving gap-based recovery above 100%. Moreover, in transplant experiments between a base model and its instruction-tuned counterpart, transplantation improves the underperforming model toward the stronger baseline, yielding up to about 2.33 times the target baseline with gap-based recovery reaching up to 100% in the best case. These results show that meaningful capacity transfer can be realized through the implantation of highly localized modules implied by language models. Overall, this work provides empirical evidence for task-localized modularity in language models and presents a new research area: model transplantation.

</details>


### [45] [The Validity of Coreference-based Evaluations of Natural Language Understanding](https://arxiv.org/abs/2602.16200)
*Ian Porada*

Main category: cs.CL

TL;DR: 通过扩展共指评估方法，发现主流语言模型虽在标准测试中表现优异，但泛化能力有限，需改进评估与模型设计以提升推广性。


<details>
  <summary>Details</summary>
Motivation: 现有基于共指的评估存在测量有效性问题，导致结论难以推广，且评估结果在不同基准间存在不统一现象。

Method: 分析标准共指评估，提出并实现一种新的评估方法，测试系统推断事件相对可信度的能力，以扩展评估实践。

Result: 当前大型语言模型在标准基准上表现强劲，优于早期系统，但对评估条件敏感，评估环境稍有变动即表现不稳定。

Conclusion: 研究揭示了现有共指评估的优缺点，强调测量有效性不足，提示需要开发更好的评估方法和更具泛化能力的系统。

Abstract: In this thesis, I refine our understanding as to what conclusions we can reach from coreference-based evaluations by expanding existing evaluation practices and considering the extent to which evaluation results are either converging or conflicting. First, I analyze standard coreference evaluations and show that their design often leads to non-generalizable conclusions due to issues of measurement validity - including contestedness (multiple, competing definitions of coreference) and convergent validity (evaluation results that rank models differently across benchmarks). Second, I propose and implement a novel evaluation focused on testing systems' ability to infer the relative plausibility of events, a key aspect of resolving coreference. Through this extended evaluation, I find that contemporary language models demonstrate strong performance on standard benchmarks - improving over earlier baseline systems within certain domains and types of coreference - but remain sensitive to the evaluation conditions: they often fail to generalize in ways one would expect a human to be capable of when evaluation contexts are slightly modified. Taken together, these findings clarify both the strengths, such as improved accuracy over baselines on widely used evaluations, and the limitations of the current NLP paradigm, including weaknesses in measurement validity, and suggest directions for future work in developing better evaluation methods and more genuinely generalizable systems.

</details>


### [46] [Long-Tail Knowledge in Large Language Models: Taxonomy, Mechanisms, Interventions and Implications](https://arxiv.org/abs/2602.16201)
*Sanket Badhe,Deep Shah,Nehal Kathrotia*

Main category: cs.CL

TL;DR: 本文构建了一个结构化框架，全面解析大型语言模型中长尾知识的定义、流失及补救方法，并探讨其评价和社会影响，为未来研究指明方向。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在低频、领域特定、文化及时效性知识上的表现不佳且少有系统分析，亟需建立统一框架，深入理解并应对长尾知识的流失和评价不足问题。

Method: 提出一个涵盖定义、损失机制、技术干预和社会影响四个维度的结构化分析框架，综合技术与社会技术文献，系统评估长尾知识表现及相关挑战。

Result: 本文系统分析了大型语言模型中长尾知识的定义、损失机制、技术干预及其社会影响，揭示了长期存在的低频知识表现不佳的问题及其评价障碍，并指出隐私、可持续性和治理等领域的挑战。

Conclusion: 大型语言模型在长尾知识表现上存在显著缺陷，现有评估方法难以揭示罕见但重要知识的失效，需综合技术和社会视角，解决隐私、可持续性及治理挑战，以提升模型的公平性和信任度。

Abstract: Large language models (LLMs) are trained on web-scale corpora that exhibit steep power-law distributions, in which the distribution of knowledge is highly long-tailed, with most appearing infrequently. While scaling has improved average-case performance, persistent failures on low-frequency, domain-specific, cultural, and temporal knowledge remain poorly characterized. This paper develops a structured taxonomy and analysis of long-Tail Knowledge in large language models, synthesizing prior work across technical and sociotechnical perspectives.
  We introduce a structured analytical framework that synthesizes prior work across four complementary axes: how long-Tail Knowledge is defined, the mechanisms by which it is lost or distorted during training and inference, the technical interventions proposed to mitigate these failures, and the implications of these failures for fairness, accountability, transparency, and user trust. We further examine how existing evaluation practices obscure tail behavior and complicate accountability for rare but consequential failures. The paper concludes by identifying open challenges related to privacy, sustainability, and governance that constrain long-Tail Knowledge representation. Taken together, this paper provides a unifying conceptual framework for understanding how long-Tail Knowledge is defined, lost, evaluated, and manifested in deployed language model systems.

</details>


### [47] [Are LLMs Ready to Replace Bangla Annotators?](https://arxiv.org/abs/2602.16241)
*Md. Najib Hasan,Touseef Hasan,Souvika Sarkar*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLMs）作为孟加拉语仇恨言论零-shot自动标注器的行为，发现模型存在偏见和判断不稳定，且模型规模扩大未必提升标注质量。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型作为自动标注工具在低资源且身份敏感的仇恨言论检测领域的可靠性和偏见问题，为数据集构建提供参考。

Method: 使用统一评估框架系统性测试了17个大语言模型在孟加拉语仇恨言论标注任务上的表现，分析其标注偏见和稳定性问题。

Result: 发现17个模型普遍存在标注者偏见和判断不稳定性，且模型规模增长不一定带来标注质量提升，小型、任务对齐模型表现更为一致。

Conclusion: 大规模语言模型在低资源且敏感的标注任务中存在显著偏见和不稳定性，较小且任务对齐的模型在一致性上表现更好，提示在实际应用前需谨慎评估。

Abstract: Large Language Models (LLMs) are increasingly used as automated annotators to scale dataset creation, yet their reliability as unbiased annotators--especially for low-resource and identity-sensitive settings--remains poorly understood. In this work, we study the behavior of LLMs as zero-shot annotators for Bangla hate speech, a task where even human agreement is challenging, and annotator bias can have serious downstream consequences. We conduct a systematic benchmark of 17 LLMs using a unified evaluation framework. Our analysis uncovers annotator bias and substantial instability in model judgments. Surprisingly, increased model scale does not guarantee improved annotation quality--smaller, more task-aligned models frequently exhibit more consistent behavior than their larger counterparts. These results highlight important limitations of current LLMs for sensitive annotation tasks in low-resource languages and underscore the need for careful evaluation before deployment.

</details>


### [48] [Aladdin-FTI @ AMIYA Three Wishes for Arabic NLP: Fidelity, Diglossia, and Multidialectal Generation](https://arxiv.org/abs/2602.16290)
*Jonathan Mutal,Perla Al Almaoui,Simon Hengchen,Pierrette Bouillon*

Main category: cs.CL

TL;DR: 针对阿拉伯方言多样性，提出了支持多方言生成与翻译的Aladdin-FTI系统。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯方言因非标准化和多样性在NLP中研究不足，LLMs为将阿拉伯语视作多中心语言提供了新契机。

Method: 利用大语言模型（LLMs），设计多方言文本生成与双向翻译系统，覆盖多种阿拉伯方言、MSA及英语。

Result: 本文提出了Aladdin-FTI系统，针对阿拉伯方言的生成和翻译问题，支持摩洛哥、埃及、巴勒斯坦、叙利亚和沙特五种方言的文本生成及这些方言与现代标准阿拉伯语（MSA）和英语之间的双向翻译。代码和模型公开。

Conclusion: Aladdin-FTI有效支持多种阿拉伯方言的生成和翻译，促进阿拉伯语作为多中心语言的处理。

Abstract: Arabic dialects have long been under-represented in Natural Language Processing (NLP) research due to their non-standardization and high variability, which pose challenges for computational modeling. Recent advances in the field, such as Large Language Models (LLMs), offer promising avenues to address this gap by enabling Arabic to be modeled as a pluricentric language rather than a monolithic system. This paper presents Aladdin-FTI, our submission to the AMIYA shared task. The proposed system is designed to both generate and translate dialectal Arabic (DA). Specifically, the model supports text generation in Moroccan, Egyptian, Palestinian, Syrian, and Saudi dialects, as well as bidirectional translation between these dialects, Modern Standard Arabic (MSA), and English. The code and trained model are publicly available.

</details>


### [49] [MultiCW: A Large-Scale Balanced Benchmark Dataset for Training Robust Check-Worthiness Detection Models](https://arxiv.org/abs/2602.16298)
*Martin Hyben,Sebastian Kula,Jan Cegin,Jakub Simko,Ivan Srba,Robert Moro*

Main category: cs.CL

TL;DR: 该论文提出了一个大规模、多语言、多领域且风格多样的核查声明检测数据集MultiCW，并展示了微调多语言模型优于零样本大语言模型的效果，推动自动事实核查研究。


<details>
  <summary>Details</summary>
Motivation: 当前自动检测核查必要声明的工具有限，而此步骤对于事实核查流程至关重要。需要一个多语言、多领域、风格平衡且规模大的数据集来推动该任务研究。

Method: 引入了Multi-Check-Worthy (MultiCW) 数据集，涵盖16种语言、7个话题领域和2种写作风格，包含123,722个样本，用于列出需要核查的声明检测任务。基于该数据集，比较了3种微调的多语言转换器模型与15种商业及开源大语言模型（LLMs）的零样本性能。

Result: 微调模型在声明分类任务中始终优于零样本大语言模型，并在不同语言、领域及写作风格的分布外数据中表现出强泛化能力。

Conclusion: MultiCW数据集为多语言自动事实核查的核查声明检测任务提供了严谨的数据资源，有助于系统性评估微调模型和先进大语言模型之间的性能差异，促进该领域的发展。

Abstract: Large Language Models (LLMs) are beginning to reshape how media professionals verify information, yet automated support for detecting check-worthy claims a key step in the fact-checking process remains limited. We introduce the Multi-Check-Worthy (MultiCW) dataset, a balanced multilingual benchmark for check-worthy claim detection spanning 16 languages, 7 topical domains, and 2 writing styles. It consists of 123,722 samples, evenly distributed between noisy (informal) and structured (formal) texts, with balanced representation of check-worthy and non-check-worthy classes across all languages. To probe robustness, we also introduce an equally balanced out-of-distribution evaluation set of 27,761 samples in 4 additional languages. To provide baselines, we benchmark 3 common fine-tuned multilingual transformers against a diverse set of 15 commercial and open LLMs under zero-shot settings. Our findings show that fine-tuned models consistently outperform zero-shot LLMs on claim classification and show strong out-of-distribution generalization across languages, domains, and styles. MultiCW provides a rigorous multilingual resource for advancing automated fact-checking and enables systematic comparisons between fine-tuned models and cutting-edge LLMs on the check-worthy claim detection task.

</details>


### [50] [MemoryArena: Benchmarking Agent Memory in Interdependent Multi-Session Agentic Tasks](https://arxiv.org/abs/2602.16313)
*Zexue He,Yu Wang,Churan Zhi,Yuanzhe Hu,Tzu-Ping Chen,Lang Yin,Ze Chen,Tong Arthur Wu,Siru Ouyang,Zihan Wang,Jiaxin Pei,Julian McAuley,Yejin Choi,Alex Pentland*

Main category: cs.CL

TL;DR: 提出MemoryArena基准测试平台，评估记忆代理在多回合环境中如何积累与运用记忆，发现现有模型在真实代理任务中存在不足。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法要么仅测记忆回忆能力，要么仅测单次任务行为，未能体现记忆与行动在多回合现实任务中的紧密结合，故提出MemoryArena以更真实地评测代理记忆能力。

Method: 构建包含多个相互依赖子任务的人类设计任务，形成多回合Memory-Agent-Environment循环，评估代理积累和使用记忆以指导后续行动的能力。涵盖网页导航、偏好约束规划、渐进信息搜索和序列形式推理等多个场景。

Result: 本文提出了MemoryArena，一个用于多回合记忆代理环境循环中代理记忆评估的统一基准测试平台。通过设计包含相互依赖子任务的人类设计任务，评测代理如何在交互中积累记忆并利用记忆指导未来行动以完成整体任务。实验显示当前在长上下文记忆测试中表现良好的代理，在该更真实的代理环境设置下表现不佳，暴露了当前记忆代理评估中的不足。

Conclusion: 当前记忆代理的评估方法存在缺陷，MemoryArena展示了真实交互环境中记忆和行动紧密结合的重要性，揭示了模型性能差距。

Abstract: Existing evaluations of agents with memory typically assess memorization and action in isolation. One class of benchmarks evaluates memorization by testing recall of past conversations or text but fails to capture how memory is used to guide future decisions. Another class focuses on agents acting in single-session tasks without the need for long-term memory. However, in realistic settings, memorization and action are tightly coupled: agents acquire memory while interacting with the environment, and subsequently rely on that memory to solve future tasks. To capture this setting, we introduce MemoryArena, a unified evaluation gym for benchmarking agent memory in multi-session Memory-Agent-Environment loops. The benchmark consists of human-crafted agentic tasks with explicitly interdependent subtasks, where agents must learn from earlier actions and feedback by distilling experiences into memory, and subsequently use that memory to guide later actions to solve the overall task. MemoryArena supports evaluation across web navigation, preference-constrained planning, progressive information search, and sequential formal reasoning, and reveals that agents with near-saturated performance on existing long-context memory benchmarks like LoCoMo perform poorly in our agentic setting, exposing a gap in current evaluations for agents with memory.

</details>


### [51] [Helpful to a Fault: Measuring Illicit Assistance in Multi-Turn, Multilingual LLM Agents](https://arxiv.org/abs/2602.16346)
*Nivya Talokar,Ayush K Tarun,Murari Mandal,Maksym Andriushchenko,Antoine Bosselut*

Main category: cs.CL

TL;DR: 本文提出STING框架，用多轮策略高效发现基于LLM代理的滥用风险，支持多语言评估，为现实部署环境中代理滥用的检测和防范提供了实用工具。


<details>
  <summary>Details</summary>
Motivation: 现有代理滥用测试主要针对单轮指令，无法全面评估多轮交互中代理可能被用来执行复杂有害或非法任务的风险，有必要引入更贴近实际场景的多轮滥用测试方法。

Method: 提出STING，一种自动化红队测试框架，通过构建基于良性身份的逐步非法计划，多轮迭代探测目标代理，结合评判代理追踪任务阶段完成，模型以多轮红队攻击的首次突破时间为随机变量进行分析。

Result: STING在AgentHarm场景下展现出比单轮提示及多轮基线更高的非法任务完成率，并在六种非英语语言环境中的攻击成功率和滥用程度表现出不一致趋势。

Conclusion: STING框架能够有效检测多轮交互过程中代理被滥用执行非法任务的情况，显著优于单轮和现有多轮基线方法，且在多语言环境下表现复杂，反映了实际应用的多样性与挑战。

Abstract: LLM-based agents execute real-world workflows via tools and memory. These affordances enable ill-intended adversaries to also use these agents to carry out complex misuse scenarios. Existing agent misuse benchmarks largely test single-prompt instructions, leaving a gap in measuring how agents end up helping with harmful or illegal tasks over multiple turns. We introduce STING (Sequential Testing of Illicit N-step Goal execution), an automated red-teaming framework that constructs a step-by-step illicit plan grounded in a benign persona and iteratively probes a target agent with adaptive follow-ups, using judge agents to track phase completion. We further introduce an analysis framework that models multi-turn red-teaming as a time-to-first-jailbreak random variable, enabling analysis tools like discovery curves, hazard-ratio attribution by attack language, and a new metric: Restricted Mean Jailbreak Discovery. Across AgentHarm scenarios, STING yields substantially higher illicit-task completion than single-turn prompting and chat-oriented multi-turn baselines adapted to tool-using agents. In multilingual evaluations across six non-English settings, we find that attack success and illicit-task completion do not consistently increase in lower-resource languages, diverging from common chatbot findings. Overall, STING provides a practical way to evaluate and stress-test agent misuse in realistic deployment settings, where interactions are inherently multi-turn and often multilingual.

</details>


### [52] [Label-Consistent Data Generation for Aspect-Based Sentiment Analysis Using LLM Agents](https://arxiv.org/abs/2602.16379)
*Mohammad H. A. Monfared,Lucie Flek,Akbar Karimi*

Main category: cs.CL

TL;DR: 本文提出的主动数据增强方法通过迭代生成与校验，提高了ABSA任务中合成数据的质量和模型表现，优于传统基于提示的方法。


<details>
  <summary>Details</summary>
Motivation: 为了提高ABSA任务中合成训练数据的质量，克服传统提示方法在标签保真度和方面术语生成上的不足，提出主动数据增强，通过迭代生成与验证确保数据质量和有效提升模型性能。

Method: 本文采用迭代生成与验证的主动数据增强方法，生成高质量合成训练样本。建立一个基于相同模型和指令的提示方法作为对照，分别在ABSA相关子任务和不同模型上评价两种方法的效果。

Result: 本文提出了一种用于基于方面的情感分析（ABSA）的主动数据增强方法，通过迭代生成与校验，生产高质量的合成训练样本。作者还建立了基于提示的对比方法，以评估主动结构的效果。两种方法在三个ABSA子任务、四个SemEval数据集及两种编码器-解码器模型（T5-Base和Tk-Instruct）上进行了实验。结果显示主动增强在标签保持率上优于提示方法，特别是在需要生成方面术语的任务中表现更佳。与真实数据结合使用时，主动增强带来更高的性能提升，尤其明显于T5-Base模型，使其性能接近Tk-Instruct。

Conclusion: 主动数据增强通过迭代生成与验证有效提升了合成训练数据的质量，显著改善了ABSA任务的模型性能，尤其对较少预训练的模型提升更大。结合真实数据时，主动增强方法优于基于提示的生成方法，使模型表现更加优异。

Abstract: We propose an agentic data augmentation method for Aspect-Based Sentiment Analysis (ABSA) that uses iterative generation and verification to produce high quality synthetic training examples. To isolate the effect of agentic structure, we also develop a closely matched prompting-based baseline using the same model and instructions. Both methods are evaluated across three ABSA subtasks (Aspect Term Extraction (ATE), Aspect Sentiment Classification (ATSC), and Aspect Sentiment Pair Extraction (ASPE)), four SemEval datasets, and two encoder-decoder models: T5-Base and Tk-Instruct. Our results show that the agentic augmentation outperforms raw prompting in label preservation of the augmented data, especially when the tasks require aspect term generation. In addition, when combined with real data, agentic augmentation provides higher gains, consistently outperforming prompting-based generation. These benefits are most pronounced for T5-Base, while the more heavily pretrained Tk-Instruct exhibits smaller improvements. As a result, augmented data helps T5-Base achieve comparable performance with its counterpart.

</details>


### [53] [TabAgent: A Framework for Replacing Agentic Generative Components with Tabular-Textual Classifiers](https://arxiv.org/abs/2602.16429)
*Ido Levy,Eilam Shapira,Yinon Goldshtein,Avi Yaeli,Nir Mashkif,Segev Shlomov*

Main category: cs.CL

TL;DR: TabAgent通过表格分类器替代生成式决策，实现大幅降低延迟和成本且不降低任务成功率，提升多步骤代理系统的效率。


<details>
  <summary>Details</summary>
Motivation: 多步骤代理系统频繁调用大型语言模型进行生成决策，导致部署时延迟高、成本昂贵，需一种高效且性能不减的替代方案。

Method: （1）从执行轨迹提取结构化表格模式、状态和依赖特征；（2）利用与表格模式对齐的合成数据增强；（3）采用轻量级文本表格分类器（TabHead）对候选项进行评分替代生成模型决策。

Result: 该论文提出了TabAgent框架，用于替换多步骤代理系统中基于大型语言模型（LLM）反复调用的生成性决策组件，尤其是封闭集选择任务。TabAgent通过从执行轨迹中提取结构化的表格模式、状态及依赖特征，并利用模式对齐的合成监督进行增强，使用轻量级分类器取代生成模型实现决策过程。实验表明，在AppWorld长距离任务测试中，TabAgent保持了任务成功率，同时减少了约95%的延迟和85-91%的推理成本。该方法不仅适用于工具筛选，还能推广到其他代理决策任务，开辟了用判别式模型替换生成模型瓶颈的新范式。

Conclusion: TabAgent有效减少了多步代理系统中LLM调用的延迟和成本，且不损失性能，展示了判别式替代生成式决策组件的可行性和广泛适用性。

Abstract: Agentic systems, AI architectures that autonomously execute multi-step workflows to achieve complex goals, are often built using repeated large language model (LLM) calls for closed-set decision tasks such as routing, shortlisting, gating, and verification. While convenient, this design makes deployments slow and expensive due to cumulative latency and token usage. We propose TabAgent, a framework for replacing generative decision components in closed-set selection tasks with a compact textual-tabular classifier trained on execution traces. TabAgent (i) extracts structured schema, state, and dependency features from trajectories (TabSchema), (ii) augments coverage with schema-aligned synthetic supervision (TabSynth), and (iii) scores candidates with a lightweight classifier (TabHead). On the long-horizon AppWorld benchmark, TabAgent maintains task-level success while eliminating shortlist-time LLM calls, reducing latency by approximately 95% and inference cost by 85-91%. Beyond tool shortlisting, TabAgent generalizes to other agentic decision heads, establishing a paradigm for learned discriminative replacements of generative bottlenecks in production agent architectures.

</details>


### [54] [IndicEval: A Bilingual Indian Educational Evaluation Framework for Large Language Models](https://arxiv.org/abs/2602.16467)
*Saurabh Bharti,Gaurav Azad,Abhinaw Jagtap,Nachiket Tapas*

Main category: cs.CL

TL;DR: 提出IndicEval，基于真实印地语和英语考试题目，评估大语言模型的多语言推理能力，发现推理提示提升明显但双语迁移依旧困难。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型评估多依赖合成基准，缺乏反映现实多领域、多语言学术严谨性的测试，急需一个能评估模型实际推理能力和双语适应性的真实考试基准。

Method: 通过自动化使用Zero-Shot、Few-Shot和Chain-of-Thought提示策略，利用印度UPSC、JEE和NEET考试的真实题目，评估了多个主流大语言模型在英语和印地语两种语言下的表现。

Result: 实验显示Chain-of-Thought提示显著提升推理准确率，各模型间表现差异明显，且语言迁移存在明显退化，尤其是在印地语Zero-Shot条件下准确率大幅下降。

Conclusion: IndicEval平台提供了一个基于真实高难度考试题目的多语言大语言模型评估框架，揭示了当前大模型在推理能力、多领域知识应用及中英文双语适应性上的显著差异和挑战。

Abstract: The rapid advancement of large language models (LLMs) necessitates evaluation frameworks that reflect real-world academic rigor and multilingual complexity. This paper introduces IndicEval, a scalable benchmarking platform designed to assess LLM performance using authentic high-stakes examination questions from UPSC, JEE, and NEET across STEM and humanities domains in both English and Hindi. Unlike synthetic benchmarks, IndicEval grounds evaluation in real examination standards, enabling realistic measurement of reasoning, domain knowledge, and bilingual adaptability. The framework automates assessment using Zero-Shot, Few-Shot, and Chain-of-Thought (CoT) prompting strategies and supports modular integration of new models and languages. Experiments conducted on Gemini 2.0 Flash, GPT-4, Claude, and LLaMA 3-70B reveal three major findings. First, CoT prompting consistently improves reasoning accuracy, with substantial gains across subjects and languages. Second, significant cross-model performance disparities persist, particularly in high-complexity examinations. Third, multilingual degradation remains a critical challenge, with marked accuracy drops in Hindi compared to English, especially under Zero-Shot conditions. These results highlight persistent gaps in bilingual reasoning and domain transfer. Overall, IndicEval provides a practice-oriented, extensible foundation for rigorous, equitable evaluation of LLMs in multilingual educational settings and offers actionable insights for improving reasoning robustness and language adaptability.

</details>


### [55] [Training Models on Dialects of Translationese Shows How Lexical Diversity and Source-Target Syntactic Similarity Shape Learning](https://arxiv.org/abs/2602.16469)
*Jenny Kunz*

Main category: cs.CL

TL;DR: 本文研究了机器翻译数据训练下的小型英语语言模型，发现源语言会影响模型性能，词汇多样性影响困惑度，语言类型相似性影响语法表现。


<details>
  <summary>Details</summary>
Motivation: 机器翻译文本与母语文本系统性不同，探究翻译腔对语言模型训练和评价的影响及其与源语言的关系。

Method: 利用从24种不同类型和资源的源语言机器翻译得到的英语文本训练模型，分析源语言和语料特性对模型学习的影响。

Result: 源语言清晰影响模型表现，翻译语料词汇多样性影响困惑度，语法性能随与英语类型一致性提升而改善。

Conclusion: 源语言对训练模型行为有显著影响；词汇多样性驱动总体困惑度，语法表现与语言类型相似性强相关。

Abstract: Machine-translated data is widely used in multilingual NLP, particularly when native text is scarce. However, translated text differs systematically from native text. This phenomenon is known as translationese, and it reflects both traces of the source language and characteristic properties of translation itself. In this paper, we study how training on machine-translated data affects small English language models, focusing on how translationese from different source languages shapes linguistic acceptability judgments and language modelling for different domains. We train models on English text translated from 24 typologically and resource-diverse source languages, enabling a systematic analysis of how source language and corpus properties influence what models learn. Our results show that the source language has a clear impact on model behavior: general perplexity is more driven by the lexical diversity of the translated corpus, while grammatical performance is strongly correlated to typological similarity to English, given enough data.

</details>


### [56] [Learning to Learn from Language Feedback with Social Meta-Learning](https://arxiv.org/abs/2602.16488)
*Jonathan Cook,Diego Antognini,Martin Klissarov,Claudiu Musat,Edward Grefenstette*

Main category: cs.CL

TL;DR: 该论文提出将社交元学习用于微调大语言模型，使其主动利用对话反馈学习，提升跨领域问题解决和处理模糊信息的能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在对话环境中难以从纠正反馈中学习，且不主动寻求反馈，导致对话缺乏互动性和适应性。

Method: 借鉴人类社交元学习（SML），将其作为微调方法，训练大语言模型在模拟教学对话中主动请求并从语言反馈中学习。

Result: SML训练的模型能通过对话解决单轮无法解决的问题，且该能力跨领域迁移，如数学问题训练提升编码问题反馈利用能力。模型在处理信息不完全的任务时表现更好，更少提前回答，更倾向请求关键信息。

Conclusion: SML提供了一种可扩展的方法，使AI系统能有效地通过语言反馈学习，提升对话互动性和解决复杂问题能力。

Abstract: Large language models (LLMs) often struggle to learn from corrective feedback within a conversational context. They are rarely proactive in soliciting this feedback, even when faced with ambiguity, which can make their dialogues feel static, one-sided, and lacking the adaptive qualities of human conversation. To address these limitations, we draw inspiration from social meta-learning (SML) in humans - the process of learning how to learn from others. We formulate SML as a finetuning methodology, training LLMs to solicit and learn from language feedback in simulated pedagogical dialogues, where static tasks are converted into interactive social learning problems. SML effectively teaches models to use conversation to solve problems they are unable to solve in a single turn. This capability generalises across domains; SML on math problems produces models that better use feedback to solve coding problems and vice versa. Furthermore, despite being trained only on fully-specified problems, these models are better able to solve underspecified tasks where critical information is revealed over multiple turns. When faced with this ambiguity, SML-trained models make fewer premature answer attempts and are more likely to ask for the information they need. This work presents a scalable approach to developing AI systems that effectively learn from language feedback.

</details>


### [57] [From Growing to Looping: A Unified View of Iterative Computation in LLMs](https://arxiv.org/abs/2602.16490)
*Ferdinand Kapl,Emmanouil Angelis,Kaitlin Maile,Johannes von Oswald,Stefan Bauer*

Main category: cs.CL

TL;DR: 循环利用层与深度增长在模型深度上表现出相似的迭代计算特征，二者可以组合使用，提升模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 循环利用层和深度增长模型都与推理能力增强相关，但两者之间的关系尚不清楚。

Method: 循环利用层块和深度增长模型，通过在推理时循环中间层块，实现训练浅到深的模型。

Result: 发现循环模型和深度增长模型在深度方向上表现出一致的模式，表明它们的性能提升源于相同的迭代计算机制。推理时对深度增长模型的中间块进行循环应用，可以显著提高推理准确率。两种方法也能更好地适应更多的上下文例子和额外的微调数据。

Conclusion: 循环利用层和深度增长是互补且实用的方法，可通过诱导和扩展迭代计算机制来提升模型推理能力。

Abstract: Looping, reusing a block of layers across depth, and depth growing, training shallow-to-deep models by duplicating middle layers, have both been linked to stronger reasoning, but their relationship remains unclear. We provide a mechanistic unification: looped and depth-grown models exhibit convergent depth-wise signatures, including increased reliance on late layers and recurring patterns aligned with the looped or grown block. These shared signatures support the view that their gains stem from a common form of iterative computation. Building on this connection, we show that the two techniques are adaptable and composable: applying inference-time looping to the middle blocks of a depth-grown model improves accuracy on some reasoning primitives by up to $2\times$, despite the model never being trained to loop. Both approaches also adapt better than the baseline when given more in-context examples or additional supervised fine-tuning data. Additionally, depth-grown models achieve the largest reasoning gains when using higher-quality, math-heavy cooldown mixtures, which can be further boosted by adapting a middle block to loop. Overall, our results position depth growth and looping as complementary, practical methods for inducing and scaling iterative computation to improve reasoning.

</details>


### [58] [Optimizing Soft Prompt Tuning via Structural Evolution](https://arxiv.org/abs/2602.16500)
*Zhenzhen Huang,Chaoning Zhang,Haoyu Bian,Songbo Zhang,Chi-lok Andy Tai,Jiaquan Zhang,Caiyan Qin,Jingjing Qu,Yalan Ye,Yang Yang,Heng Tao Shen*

Main category: cs.CL

TL;DR: 提出基于拓扑形态演化的软提示调优新方法，利用持久同调分析和TSLoss损失函数提升软提示的稳定性和性能，增强方法的可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统软提示缺乏显式语义和训练行为的可追踪性，限制了解释性。本文引入拓扑形态分析以提升软提示的结构可解释性和优化效果。

Method: 借助拓扑数据分析中的持久同调定量软提示结构演变，基于此设计TSLoss损失函数，通过量化参数间的连通性和冗余度来优化软提示调优。

Result: 本文提出基于拓扑形态进化的软提示调优方法，通过采用拓扑数据分析中的持久同调定量软提示在连续参数空间中的结构表示及其训练演变过程。利用拓扑稳定且紧凑的软提示与更好的下游性能相关的经验，构建了拓扑软提示损失（TSLoss）函数，指导模型学习结构稳定的适应，提升收敛速度和调优性能。

Conclusion: 实验表明，基于TSLoss的软提示调优加速了模型收敛并提升了调优性能，且提升了软提示调优的可解释性和结构稳定性。

Abstract: Soft prompt tuning leverages continuous embeddings to capture task-specific information in large pre-trained language models (LLMs), achieving competitive performance in few-shot settings. However, soft prompts rely on high-dimensional, implicit representations and lack explicit semantics and traceable training behaviors, which limits their interpretability. To address this limitation, we propose a soft prompt tuning optimization method based on topological morphological evolution. Specifically, we employ persistent homology from topological data analysis (TDA) to quantify the structural representations of soft prompts in continuous parameter space and their training process evolution. Quantitative analysis shows that topologically stable and compact soft prompts achieve better downstream performance. Based on this empirical observation, we construct a loss function for optimizing soft prompt tuning, termed Topological Soft Prompt Loss (TSLoss). TSLoss guides the model to learn structurally stable adaptations by quantifying inter-parameter connectivity and redundancy. Extensive experiments show that training with TSLoss accelerates convergence and improves tuning performance, providing an interpretable method to understand and optimize soft prompt tuning from structural and topological perspectives.

</details>


### [59] [Supercharging Agenda Setting Research: The ParlaCAP Dataset of 28 European Parliaments and a Scalable Multilingual LLM-Based Classification](https://arxiv.org/abs/2602.16516)
*Taja Kuzman Pungeršek,Peter Rupnik,Daniela Širinić,Nikola Ljubešić*

Main category: cs.CL

TL;DR: 本文提出基于LLM教师-学生框架的多语言议会政策主题分类方法，并构建ParlaCAP大规模数据集，提升议程设置分析的准确性和跨国比较能力。


<details>
  <summary>Details</summary>
Motivation: 构建一个高效且可扩展的系统，用于欧洲议会议程设置的多语言主题分类，解决已有数据集领域外迁移效果差的问题。

Method: 利用教师-学生框架，先用大型语言模型（LLM）对多语言ParlaMint语料进行领域内训练数据标注，再微调多语言编码器模型实现可扩展的数据注释，结合CAP政策主题分类方案。

Result: 所提出的分类器与人类标注者的一致性相当，优于现有基于领域外手工标注的CAP分类器。同时，ParlaCAP数据集包含丰富的发言者、党派元数据及情感预测，支持跨国比较研究。

Conclusion: 该方法有效提升了议会主题分类的准确性和可扩展性，ParlaCAP数据集为比较政治注意力及代表性研究提供了有力工具，并通过案例展示了其在政策分布、情感模式及性别关注差异等方面的应用潜力。

Abstract: This paper introduces ParlaCAP, a large-scale dataset for analyzing parliamentary agenda setting across Europe, and proposes a cost-effective method for building domain-specific policy topic classifiers. Applying the Comparative Agendas Project (CAP) schema to the multilingual ParlaMint corpus of over 8 million speeches from 28 parliaments of European countries and autonomous regions, we follow a teacher-student framework in which a high-performing large language model (LLM) annotates in-domain training data and a multilingual encoder model is fine-tuned on these annotations for scalable data annotation. We show that this approach produces a classifier tailored to the target domain. Agreement between the LLM and human annotators is comparable to inter-annotator agreement among humans, and the resulting model outperforms existing CAP classifiers trained on manually-annotated but out-of-domain data. In addition to the CAP annotations, the ParlaCAP dataset offers rich speaker and party metadata, as well as sentiment predictions coming from the ParlaSent multilingual transformer model, enabling comparative research on political attention and representation across countries. We illustrate the analytical potential of the dataset with three use cases, examining the distribution of parliamentary attention across policy topics, sentiment patterns in parliamentary speech, and gender differences in policy attention.

</details>


### [60] [Utility-Preserving De-Identification for Math Tutoring: Investigating Numeric Ambiguity in the MathEd-PII Benchmark Dataset](https://arxiv.org/abs/2602.16571)
*Zhuqian Zhou,Kirk Vanacore,Bakhtawar Ahtisham,Jinsook Lee,Doug Pietrzak,Daryl Hedley,Jorge Dias,Chris Shaw,Ruth Schäfer,René F. Kizilcec*

Main category: cs.CL

TL;DR: 数学辅导对话数据中的个人信息检测存在数字歧义问题，本文提出MathEd-PII数据集并通过数学感知提示改进了去标识化方法，有效提升性能且保留数据用途。


<details>
  <summary>Details</summary>
Motivation: 现有的PII检测系统在数学辅导数据中误删核心教学内容，亟需开发既能保护隐私又能保留教育价值的去标识化技术。

Method: 利用人机协作的LLM工作流构建标注数据集，采用密度分割方法发现误报集中点，比较基线Presidio与三种基于提示的LLM方法，其中数学感知提示表现最佳。

Result: 本文针对数学辅导对话数据中的个人信息（PII）检测问题，提出了MathEd-PII基准数据集，并通过人机协作的LLM工作流实现去标识化保护，同时保留教育数据的实用性。研究揭示了数字歧义问题导致的误判集中在数学密集区域，并比较了四种PII检测策略，发现基于数学语境的提示显著提升了检测性能，降低了误报。

Conclusion: 去标识化必须结合数学领域的上下文信息，才能有效减少误报并保持教学数据的分析价值，MathEd-PII数据集为相关研究提供了重要基准。

Abstract: Large-scale sharing of dialogue-based data is instrumental for advancing the science of teaching and learning, yet rigorous de-identification remains a major barrier. In mathematics tutoring transcripts, numeric expressions frequently resemble structured identifiers (e.g., dates or IDs), leading generic Personally Identifiable Information (PII) detection systems to over-redact core instructional content and reduce dataset utility. This work asks how PII can be detected in math tutoring transcripts while preserving their educational utility. To address this challenge, we investigate the "numeric ambiguity" problem and introduce MathEd-PII, the first benchmark dataset for PII detection in math tutoring dialogues, created through a human-in-the-loop LLM workflow that audits upstream redactions and generates privacy-preserving surrogates. The dataset contains 1,000 tutoring sessions (115,620 messages; 769,628 tokens) with validated PII annotations. Using a density-based segmentation method, we show that false PII redactions are disproportionately concentrated in math-dense regions, confirming numeric ambiguity as a key failure mode. We then compare four detection strategies: a Presidio baseline and LLM-based approaches with basic, math-aware, and segment-aware prompting. Math-aware prompting substantially improves performance over the baseline (F1: 0.821 vs. 0.379) while reducing numeric false positives, demonstrating that de-identification must incorporate domain context to preserve analytic utility. This work provides both a new benchmark and evidence that utility-preserving de-identification for tutoring data requires domain-aware modeling.

</details>


### [61] [CitiLink-Summ: Summarization of Discussion Subjects in European Portuguese Municipal Meeting Minutes](https://arxiv.org/abs/2602.16607)
*Miguel Marques,Ana Luísa Fernandes,Ana Filipa Pacheco,Rute Rebouças,Inês Cantante,José Isidro,Luís Filipe Cunha,Alípio Jorge,Nuno Guimarães,Sérgio Nunes,António Leal,Purificação Silvano,Ricardo Campos*

Main category: cs.CL

TL;DR: 本研究提出了欧洲葡萄牙语市政会议记录摘要数据集CitiLink-Summ，利用多种先进模型基线测试，实现了该领域自动摘要的首次系统研究。


<details>
  <summary>Details</summary>
Motivation: 市政会议记录内容冗长且复杂，公众难以理解，自动摘要可以生成简明的讨论主题摘要，但在低资源语言尤其是市政领域的研究较少，缺乏高质量摘要数据集。

Method: 构建了CitiLink-Summ数据集，包含100份葡萄牙语市政会议记录及2322个手工撰写的摘要，并使用BART、PRIMERA等生成模型及大型语言模型进行自动摘要实验，采用ROUGE、BLEU、METEOR和BERTScore等指标评估。

Result: 构建了首个针对欧洲葡萄牙语市政领域的高质量摘要数据集CitiLink-Summ，提供了基线自动摘要模型的性能评估结果，推动该领域NLP研究。

Conclusion: CitiLink-Summ数据集为欧洲葡萄牙语市政会议自动摘要提供了首个标准基准，推动了复杂行政文本自动摘要技术的发展。

Abstract: Municipal meeting minutes are formal records documenting the discussions and decisions of local government, yet their content is often lengthy, dense, and difficult for citizens to navigate. Automatic summarization can help address this challenge by producing concise summaries for each discussion subject. Despite its potential, research on summarizing discussion subjects in municipal meeting minutes remains largely unexplored, especially in low-resource languages, where the inherent complexity of these documents adds further challenges. A major bottleneck is the scarcity of datasets containing high-quality, manually crafted summaries, which limits the development and evaluation of effective summarization models for this domain. In this paper, we present CitiLink-Summ, a new corpus of European Portuguese municipal meeting minutes, comprising 100 documents and 2,322 manually hand-written summaries, each corresponding to a distinct discussion subject. Leveraging this dataset, we establish baseline results for automatic summarization in this domain, employing state-of-the-art generative models (e.g., BART, PRIMERA) as well as large language models (LLMs), evaluated with both lexical and semantic metrics such as ROUGE, BLEU, METEOR, and BERTScore. CitiLink-Summ provides the first benchmark for municipal-domain summarization in European Portuguese, offering a valuable resource for advancing NLP research on complex administrative texts.

</details>


### [62] [Explainable AI: Context-Aware Layer-Wise Integrated Gradients for Explaining Transformer Models](https://arxiv.org/abs/2602.16608)
*Melkamu Abay Mersha,Jugal Kalita*

Main category: cs.CL

TL;DR: 本文提出了一个统一的层级归因框架CA-LIG，利用层内集成梯度和类特定注意力梯度融合，生成带符号且上下文敏感的归因图，实现了对Transformer模型决策过程更全面、准确的解释。


<details>
  <summary>Details</summary>
Motivation: 现有的解释方法多依赖于最终层归因，未能统一处理局部词元和全局注意力图，缺乏对词元间依赖和结构组件的上下文感知，也未捕捉相关性随着层级的演变。

Method: 提出Context-Aware Layer-wise Integrated Gradients (CA-LIG)框架，通过在每个Transformer块内计算层级集成梯度，并融合类特定的注意力梯度，产生上下文感知的带符号归因图，捕捉证据支持及反对关系，并追踪层级相关性流动。

Result: 在情感分析、文档分类、低资源语言仇恨言论检测和视觉Transformer图像分类等任务中，CA-LIG均优于现有解释方法，表现出更忠实的归因和更强的上下文敏感性。

Conclusion: CA-LIG框架在多种任务和Transformer模型上表现出更高的归因忠实度、对上下文依赖的敏感性以及更清晰的语义可视化，显著提升了Transformer模型的解释性和理解深度。

Abstract: Transformer models achieve state-of-the-art performance across domains and tasks, yet their deeply layered representations make their predictions difficult to interpret. Existing explainability methods rely on final-layer attributions, capture either local token-level attributions or global attention patterns without unification, and lack context-awareness of inter-token dependencies and structural components. They also fail to capture how relevance evolves across layers and how structural components shape decision-making. To address these limitations, we proposed the \textbf{Context-Aware Layer-wise Integrated Gradients (CA-LIG) Framework}, a unified hierarchical attribution framework that computes layer-wise Integrated Gradients within each Transformer block and fuses these token-level attributions with class-specific attention gradients. This integration yields signed, context-sensitive attribution maps that capture supportive and opposing evidence while tracing the hierarchical flow of relevance through the Transformer layers. We evaluate the CA-LIG Framework across diverse tasks, domains, and transformer model families, including sentiment analysis and long and multi-class document classification with BERT, hate speech detection in a low-resource language setting with XLM-R and AfroLM, and image classification with Masked Autoencoder vision Transformer model. Across all tasks and architectures, CA-LIG provides more faithful attributions, shows stronger sensitivity to contextual dependencies, and produces clearer, more semantically coherent visualizations than established explainability methods. These results indicate that CA-LIG provides a more comprehensive, context-aware, and reliable explanation of Transformer decision-making, advancing both the practical interpretability and conceptual understanding of deep neural models.

</details>


### [63] [ColBERT-Zero: To Pre-train Or Not To Pre-train ColBERT models](https://arxiv.org/abs/2602.16609)
*Antoine Chaffin,Luca Arnaboldi,Amélie Chatelain,Florent Krzakala*

Main category: cs.CL

TL;DR: 通过大规模预训练和策略优化，ColBERT-Zero模型在多向量检索领域实现了新的性能突破，且公开了训练代码和模型检查点供研究使用。


<details>
  <summary>Details</summary>
Motivation: 现有的多向量模型通常依赖于在强大的单向量模型基础上进行小规模知识蒸馏训练，多向量模型的大规模预训练尚未充分研究。

Method: 本文研究了多向量模型的大规模预训练，提出了ColBERT-Zero模型，并探讨了在预训练前加入监督步骤和对齐微调与预训练设置的方法。

Result: ColBERT-Zero在仅使用公开数据的情况下，超越了之前基于更强数据训练的GTE-ModernColBERT和GTE-ModernBERT，刷新了该规模模型的性能记录。加入监督步骤的知识蒸馏能够接近全预训练效果，且微调和预训练设置对齐对性能提升至关重要。

Conclusion: 大规模多向量预训练显著提升多向量模型性能，带监督步骤的知识蒸馏是高效的替代方案，确保预训练与微调一致性能够进一步优化模型表现。

Abstract: Current state-of-the-art multi-vector models are obtained through a small Knowledge Distillation (KD) training step on top of strong single-vector models, leveraging the large-scale pre-training of these models. In this paper, we study the pre-training of multi-vector models and show that large-scale multi-vector pre-training yields much stronger multi-vector models. Notably, a fully ColBERT-pre-trained model, ColBERT-Zero, trained only on public data, outperforms GTE-ModernColBERT as well as its base model, GTE-ModernBERT, which leverages closed and much stronger data, setting new state-of-the-art for model this size. We also find that, although performing only a small KD step is not enough to achieve results close to full pre-training, adding a supervised step beforehand allows to achieve much closer performance while skipping the most costly unsupervised phase. Finally, we find that aligning the fine-tuning and pre-training setups is crucial when repurposing existing models. To enable exploration of our results, we release various checkpoints as well as code used to train them.

</details>


### [64] [Who can we trust? LLM-as-a-jury for Comparative Assessment](https://arxiv.org/abs/2602.16610)
*Mengjie Qian,Guangzhi Sun,Mark J. F. Gales,Kate M. Knill*

Main category: cs.CL

TL;DR: 本文针对大型语言模型评判者之间不一致的问题，提出BT-sigma模型，实现无监督地联合推断排名和评判者可靠性，在自然语言生成评价中效果优越。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型作为自动评估者在自然语言生成的评价中存在判决不一致和可靠性差异，且缺乏监督数据来校准评判者。

Method: 提出BT-sigma，一种基于Bradley-Terry模型的评判者感知扩展，引入判决者区分参数，通过成对比较联合推断项目排名和评判者可靠性。

Result: BT-sigma在基准NLG评价数据集上表现优于基于平均的聚合方法，且其学到的区分参数与独立的循环一致性测量显著相关。

Conclusion: BT-sigma可以作为一种无监督校准机制，通过建模评判者可靠性，提高了聚合准确性和评价一致性。

Abstract: Large language models (LLMs) are increasingly applied as automatic evaluators for natural language generation assessment often using pairwise comparative judgements. Existing approaches typically rely on single judges or aggregate multiple judges assuming equal reliability. In practice, LLM judges vary substantially in performance across tasks and aspects, and their judgment probabilities may be biased and inconsistent. Furthermore, human-labelled supervision for judge calibration may be unavailable. We first empirically demonstrate that inconsistencies in LLM comparison probabilities exist and show that it limits the effectiveness of direct probability-based ranking. To address this, we study the LLM-as-a-jury setting and propose BT-sigma, a judge-aware extension of the Bradley-Terry model that introduces a discriminator parameter for each judge to jointly infer item rankings and judge reliability from pairwise comparisons alone. Experiments on benchmark NLG evaluation datasets show that BT-sigma consistently outperforms averaging-based aggregation methods, and that the learned discriminator strongly correlates with independent measures of the cycle consistency of LLM judgments. Further analysis reveals that BT-sigma can be interpreted as an unsupervised calibration mechanism that improves aggregation by modelling judge reliability.

</details>


### [65] [AREG: Adversarial Resource Extraction Game for Evaluating Persuasion and Resistance in Large Language Models](https://arxiv.org/abs/2602.16639)
*Adib Sakhawat,Fardeen Sadab*

Main category: cs.CL

TL;DR: 该研究通过引入对抗性资源提取游戏评估大型语言模型的说服和抵抗能力，发现两者表现弱相关且防守能力优于攻击能力，强调社会影响能力具有多面性。


<details>
  <summary>Details</summary>
Motivation: 现有评估大型语言模型社会智能的方法多为静态文本生成，缺乏动态对抗交互的真实场景模拟，难以全面衡量模型的说服和抵抗能力。

Method: 设计多轮零和谈判游戏AREG，基于循环赛机制在多个前沿模型间评估说服（进攻）与抵抗（防守）能力，并结合语言学分析交互模式。

Result: 论文提出了对大型语言模型(LLMs)社会智能的动态评估基准——对抗性资源提取游戏(AREG)，通过多轮零和谈判模拟说服和抵抗能力。通过在多款前沿模型中进行循环赛，发现说服和抵抗能力相关性较弱，并且抵抗能力普遍优于说服能力。语言分析显示，逐步承诺策略有助于提高资源提取成功率，而成功的抵抗更多依赖于验证型回应而非明确拒绝。

Conclusion: 社会影响力在大型语言模型中不是单一能力，说服能力和抵抗能力表现出独立性，单一侧重说服能力的评估可能忽略模型存在的行为弱点。

Abstract: Evaluating the social intelligence of Large Language Models (LLMs) increasingly requires moving beyond static text generation toward dynamic, adversarial interaction. We introduce the Adversarial Resource Extraction Game (AREG), a benchmark that operationalizes persuasion and resistance as a multi-turn, zero-sum negotiation over financial resources. Using a round-robin tournament across frontier models, AREG enables joint evaluation of offensive (persuasion) and defensive (resistance) capabilities within a single interactional framework. Our analysis provides evidence that these capabilities are weakly correlated ($ρ= 0.33$) and empirically dissociated: strong persuasive performance does not reliably predict strong resistance, and vice versa. Across all evaluated models, resistance scores exceed persuasion scores, indicating a systematic defensive advantage in adversarial dialogue settings. Further linguistic analysis suggests that interaction structure plays a central role in these outcomes. Incremental commitment-seeking strategies are associated with higher extraction success, while verification-seeking responses are more prevalent in successful defenses than explicit refusal. Together, these findings indicate that social influence in LLMs is not a monolithic capability and that evaluation frameworks focusing on persuasion alone may overlook asymmetric behavioral vulnerabilities.

</details>


### [66] [Quecto-V1: Empirical Analysis of 8-bit Quantized Small Language Models for On-Device Legal Retrieval](https://arxiv.org/abs/2602.16640)
*Subrit Dikshit*

Main category: cs.CL

TL;DR: 本文介绍了Quecto-V1，一种针对印度法律领域的小型语言模型，具备高效离线推理能力，解决了大模型资源消耗大、部署困难的问题。


<details>
  <summary>Details</summary>
Motivation: 当前法律智能系统依赖大参数模型和云端推理，资源消耗巨大且带来数据主权风险，迫切需要面向资源受限环境的专门小型模型。

Method: 采用定制的GPT-2架构（1.24亿参数），在印度各类法律文本上从头训练，并通过8-bit量化技术压缩模型以便于离线部署。

Result: 模型在法律条文和刑法条款检索上表现优异，运行在用户级CPU上，8-bit量化实现74%的模型大小缩减，仅牺牲不到3.5%的检索准确率。

Conclusion: Quecto-V1作为专门针对印度法律训练的小型语言模型，在法律条文检索任务中优于通用小型模型，且通过8-bit量化大幅减小模型体积且仅带来极小精度损失，实现了隐私保护和资源节约。

Abstract: The rapid proliferation of Large Language Models (LLMs) has revolutionized Natural Language Processing (NLP) but has simultaneously created a "resource divide." State-of-the-art legal intelligence systems typically rely on massive parameter counts (7B+) and cloud-based inference, rendering them inaccessible to practitioners in resource-constrained environments and posing significant data sovereignty risks. This paper introduces Quecto-V1, a domain-specific Small Language Model (SLM) engineered to democratize access to Indian legal intelligence. Built upon a custom configuration of the GPT-2 architecture (124 million parameters), Quecto-V1 was trained from scratch exclusively on a corpus of Indian statutes, including the Indian Penal Code (IPC), the Code of Criminal Procedure (CrPC), and the Constitution of India. Unlike generalist models, which prioritize broad world knowledge, our approach maximizes "lexical density" within the legal domain. Furthermore, we address the deployment bottleneck by applying post-training 8-bit quantization (GGUF format), compressing the model to a memory footprint of under 150 MB. Our empirical analysis demonstrates that Quecto-V1 achieves high fidelity in retrieving statutory definitions and penal provisions, outperforming general-purpose SLMs in domain-specific exact match tasks while running entirely offline on consumer-grade CPUs. We further present an ablation study showing that 8-bit quantization yields a 74% reduction in model size with less than 3.5% degradation in retrieval accuracy compared to full-precision baselines. These findings suggest that for specialized, high-stakes domains like law, domain-specific training coupled with aggressive quantization offers a viable, privacy-preserving alternative to monolithic cloud models.

</details>


### [67] [Align Once, Benefit Multilingually: Enforcing Multilingual Consistency for LLM Safety Alignment](https://arxiv.org/abs/2602.16660)
*Yuyan Bu,Xiaohao Liu,ZhaoXing Ren,Yaodong Yang,Juntao Dai*

Main category: cs.CL

TL;DR: 本文提出了一种高效的多语种一致性损失，无需额外的低资源语言监督，通过统一更新实现多语言安全对齐，提升模型跨语言安全性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有多语言安全对齐方法依赖大量目标语言高质量监督或与高资源语言的逐对对齐，资源需求大且难以扩展，需一种资源高效的多语言对齐方法。

Method: 提出了一种插拔式多语种一致性（MLC）损失，可集成到现有单语对齐流程中，通过提升多语种表示向量间的线性相关性，实现多语言语义层面的方向一致性，从而在单次更新中进行多语言对齐，无需额外的低资源语言响应监督。

Result: 在不同模型架构和对齐范式上验证了该方法的有效性，显著提升多语言安全对齐效果，且对模型通用功能影响有限，跨语言和任务测试中表现出更好的跨语言泛化能力。

Conclusion: 该方法通过引入多语种一致性损失，实现了在有限资源下多语言安全对齐的有效提升，兼顾了对模型通用性的影响，并展示了良好的跨语言泛化能力。

Abstract: The widespread deployment of large language models (LLMs) across linguistic communities necessitates reliable multilingual safety alignment. However, recent efforts to extend alignment to other languages often require substantial resources, either through large-scale, high-quality supervision in the target language or through pairwise alignment with high-resource languages, which limits scalability. In this work, we propose a resource-efficient method for improving multilingual safety alignment. We introduce a plug-and-play Multi-Lingual Consistency (MLC) loss that can be integrated into existing monolingual alignment pipelines. By improving collinearity between multilingual representation vectors, our method encourages directional consistency at the multilingual semantic level in a single update. This allows simultaneous alignment across multiple languages using only multilingual prompt variants without requiring additional response-level supervision in low-resource languages. We validate the proposed method across different model architectures and alignment paradigms, and demonstrate its effectiveness in enhancing multilingual safety with limited impact on general model utility. Further evaluation across languages and tasks indicates improved cross-lingual generalization, suggesting the proposed approach as a practical solution for multilingual consistency alignment under limited supervision.

</details>


### [68] [Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents](https://arxiv.org/abs/2602.16699)
*Wenxuan Ding,Nicholas Tomlin,Greg Durrett*

Main category: cs.CL

TL;DR: 提出CTA框架使LLM显式权衡探索过程中的成本与不确定性，从而在复杂任务中获得更优决策策略。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在复杂问题中需要与环境交互以获取信息，必须权衡何时停止探索并给出答案，传统方法未明确处理探索过程中成本与不确定性的权衡。

Method: 将多个任务（如信息检索与编程）形式化为不确定性下的序贯决策问题，并引入Calibrate-Then-Act (CTA) 框架，通过传入额外的上下文信息使LLM显式权衡成本-不确定性，在环境中进行更优的探索行为。

Result: 实验表明，CTA框架使得LLM在信息检索和简化编程任务中能更有效地权衡探索成本与收益，发现更优的决策策略，该提升在强化学习训练中依然保留。

Conclusion: 通过显式建模成本-不确定性权衡，CTA框架提升了LLM在交互式环境中探索行为的优化能力，在多领域任务中表现优异，可助力未来复杂问题求解。

Abstract: LLMs are increasingly being used for complex problems which are not necessarily resolved in a single response, but require interacting with an environment to acquire information. In these scenarios, LLMs must reason about inherent cost-uncertainty tradeoffs in when to stop exploring and commit to an answer. For instance, on a programming task, an LLM should test a generated code snippet if it is uncertain about the correctness of that code; the cost of writing a test is nonzero, but typically lower than the cost of making a mistake. In this work, we show that we can induce LLMs to explicitly reason about balancing these cost-uncertainty tradeoffs, then perform more optimal environment exploration. We formalize multiple tasks, including information retrieval and coding, as sequential decision-making problems under uncertainty. Each problem has latent environment state that can be reasoned about via a prior which is passed to the LLM agent. We introduce a framework called Calibrate-Then-Act (CTA), where we feed the LLM this additional context to enable it to act more optimally. This improvement is preserved even under RL training of both the baseline and CTA. Our results on information-seeking QA and on a simplified coding task show that making cost-benefit tradeoffs explicit with CTA can help agents discover more optimal decision-making strategies.

</details>


### [69] [Reinforced Fast Weights with Next-Sequence Prediction](https://arxiv.org/abs/2602.16704)
*Hee Seung Hwang,Xindi Wu,Sanghyuk Chun,Olga Russakovsky*

Main category: cs.CL

TL;DR: 提出REFINE强化学习框架，改进快速权重模型的长上下文建模能力。


<details>
  <summary>Details</summary>
Motivation: 传统快速权重模型受限于单标记预测训练，未能有效捕捉长距离依赖，影响语义一致性和整体表现。

Method: 引入强化学习，通过基于预测熵选择标记位置，生成多标记滚动预测，并使用自监督序列奖励与群体相对策略优化训练快速权重模型。

Result: 本文提出REFINE框架，通过引入强化学习和序列级奖励，克服了传统快速权重模型因下一个标记预测训练范式带来的局限，提高了长序列依赖建模能力。该方法选取信息丰富的标记位置进行多标记滚动预测，优化模型表现，实验证明在多项长上下文任务中优于传统基于单标记预测的微调方法。

Conclusion: REFINE框架有效提升了快速权重模型的长文本理解性能，优于传统单标记预测训练方法，适用于预训练模型的整个训练生命周期。

Abstract: Fast weight architectures offer a promising alternative to attention-based transformers for long-context modeling by maintaining constant memory overhead regardless of context length. However, their potential is limited by the next-token prediction (NTP) training paradigm. NTP optimizes single-token predictions and ignores semantic coherence across multiple tokens following a prefix. Consequently, fast weight models, which dynamically update their parameters to store contextual information, learn suboptimal representations that fail to capture long-range dependencies. We introduce REFINE (Reinforced Fast weIghts with Next sEquence prediction), a reinforcement learning framework that trains fast weight models under the next-sequence prediction (NSP) objective. REFINE selects informative token positions based on prediction entropy, generates multi-token rollouts, assigns self-supervised sequence-level rewards, and optimizes the model with group relative policy optimization (GRPO). REFINE is applicable throughout the training lifecycle of pre-trained language models: mid-training, post-training, and test-time training. Our experiments on LaCT-760M and DeltaNet-1.3B demonstrate that REFINE consistently outperforms supervised fine-tuning with NTP across needle-in-a-haystack retrieval, long-context question answering, and diverse tasks in LongBench. REFINE provides an effective and versatile framework for improving long-context modeling in fast weight architectures.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [70] [Evaluating Collective Behaviour of Hundreds of LLM Agents](https://arxiv.org/abs/2602.16662)
*Richard Willis,Jianing Zhao,Yali Du,Joel Z. Leibo*

Main category: cs.MA

TL;DR: 本文构建了一个基于LLM生成策略的评估框架，发现新模型在个体利益驱动下社会表现较差，且合作收益降低和群体增大时易陷入不良均衡。


<details>
  <summary>Details</summary>
Motivation: 随着LLM驱动的自主智能体广泛部署，理解它们在社会困境中的集体现象变得至关重要，以确保系统性能和社会价值的最大化。

Method: 本文通过让LLM生成编码策略作为算法，结合文化进化模拟用户选择行为，分析不同模型和环境条件下智能体群体的集体行为。

Result: 本文提出了一个评估框架，利用大规模语言模型(LLM)生成可被编码为算法的策略，从而能在部署前进行策略的检查，并支持规模高达数百个智能体的群体行为分析。研究发现，较新的LLM模型在个体利益优先于集体利益时，往往带来更差的社会结果。通过文化进化模拟用户选择智能体的过程，结果显示当合作的相对收益降低且群体规模增大时，系统更容易陷入不良的社会均衡。研究还公开了代码，方便开发者评估其模型的群体行为。

Conclusion: 新一代LLM在优先满足个体利益时会导致更糟糕的社会结果，且随着合作收益降低及群体规模扩大，社会系统更易收敛到不良均衡。

Abstract: As autonomous agents powered by LLM are increasingly deployed in society, understanding their collective behaviour in social dilemmas becomes critical. We introduce an evaluation framework where LLMs generate strategies encoded as algorithms, enabling inspection prior to deployment and scaling to populations of hundreds of agents -- substantially larger than in previous work. We find that more recent models tend to produce worse societal outcomes compared to older models when agents prioritise individual gain over collective benefits. Using cultural evolution to model user selection of agents, our simulations reveal a significant risk of convergence to poor societal equilibria, particularly when the relative benefit of cooperation diminishes and population sizes increase. We release our code as an evaluation suite for developers to assess the emergent collective behaviour of their models.

</details>


### [71] [Consensus Based Task Allocation for Angles-Only Local Catalog Maintenance of Satellite Systems](https://arxiv.org/abs/2602.16678)
*Harrison Perone,Christopher W. Hays*

Main category: cs.MA

TL;DR: 本论文提出了一种分布式任务分配算法，利用多卫星之间的通信和角度测量数据，提高近距离卫星和碎片目标跟踪的准确性，并通过数值模拟验证了方法在燃料消耗和目录不确定性方面的优越性。


<details>
  <summary>Details</summary>
Motivation: 由于地面跟踪难以满足近距离卫星和碎片监测准确性需求，空间传感器特别是多卫星通信协同观测成为提升相对状态估计精度的关键。

Method: 设计了一种分布式任务分配算法，协调多卫星基于受限视角的角度测量数据进行观测调度，通过数值仿真评估方法性能。

Result: 数值模拟表明新算法在燃料使用和目录不确定性上超过目前方法形成的Pareto最优前沿，证明其有效性。

Conclusion: 提出的分布式任务分配算法在燃料消耗和目录不确定性上显著优于现有方法的Pareto前沿，提升了多卫星协作下的目标跟踪效率。

Abstract: In order for close proximity satellites to safely perform their missions, the relative states of all satellites and pieces of debris must be well understood. This presents a problem for ground based tracking and orbit determination since it may not be practical to achieve the required accuracy. Using space-based sensors allows for more accurate relative state estimates, especially if multiple satellites are allowed to communicate. Of interest to this work is the case where several communicating satellites each need to maintain a local catalog of communicating and non-communicating objects using angles-only limited field of view (FOV) measurements. However, this introduces the problem of efficiently scheduling and coordinating observations among the agents. This paper presents a decentralized task allocation algorithm to address this problem and quantifies its performance in terms of fuel usage and overall catalog uncertainty via numerical simulation. It was found that the new method significantly outperforms the uncertainty-fuel Pareto frontier formed by current approaches.

</details>


### [72] [Fairness Dynamics in Digital Economy Platforms with Biased Ratings](https://arxiv.org/abs/2602.16695)
*J. Martin Smit,Fernando P. Santos*

Main category: cs.MA

TL;DR: 提出反歧视设计策略，优化基于评价的数字平台服务推荐。


<details>
  <summary>Details</summary>
Motivation: 数字服务经济中，线上评价系统存在导致歧视的风险，需设计公平机制减少不公。

Method: 建立进化博弈理论模型分析评价系统及推荐机制调控效果。

Result: 通过模型发现调整推荐结果人口结构能有效减少歧视并保持服务质量。

Conclusion: 反歧视设计是提高数字平台公平性和用户体验的关键。

Abstract: The digital services economy consists of online platforms that facilitate interactions between service providers and consumers. This ecosystem is characterized by short-term, often one-off, transactions between parties that have no prior familiarity. To establish trust among users, platforms employ rating systems which allow users to report on the quality of their previous interactions. However, while arguably crucial for these platforms to function, rating systems can perpetuate negative biases against marginalised groups. This paper investigates how to design platforms around biased reputation systems, reducing discrimination while maintaining incentives for all service providers to offer high quality service for users. We introduce an evolutionary game theoretical model to study how digital platforms can perpetuate or counteract rating-based discrimination. We focus on the platforms' decisions to promote service providers who have high reputations or who belong to a specific protected group. Our results demonstrate a fundamental trade-off between user experience and fairness: promoting highly-rated providers benefits users, but lowers the demand for marginalised providers against which the ratings are biased. Our results also provide evidence that intervening by tuning the demographics of the search results is a highly effective way of reducing unfairness while minimally impacting users. Furthermore, we show that even when precise measurements on the level of rating bias affecting marginalised service providers is unavailable, there is still potential to improve upon a recommender system which ignores protected characteristics. Altogether, our model highlights the benefits of proactive anti-discrimination design in systems where ratings are used to promote cooperative behaviour.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [73] [From Reflection to Repair: A Scoping Review of Dataset Documentation Tools](https://arxiv.org/abs/2602.15968)
*Pedro Reynolds-Cuéllar,Marisol Wong-Villacres,Adriana Alvarado Garcia,Heila Precel*

Main category: cs.SE

TL;DR: 本文通过系统综述揭示数据集文档工具设计中的关键问题，提出从制度层面设计负责任的AI工具，促进可持续文档实践。


<details>
  <summary>Details</summary>
Motivation: 探讨数据集文档工具设计背后的动机及其采用障碍

Method: 系统综述结合混合方法分析59篇数据集文档相关文献

Result: 发现四个阻碍数据集文档工具采用和标准化的模式：文档价值操作不清晰、设计脱离具体情境、未解决的劳动需求，以及把集成视为未来工作

Conclusion: 建议负责任的AI工具设计应由关注个人转向关注制度层面，并呼吁HCI社区推动可持续的数据集文档实践。

Abstract: Dataset documentation is widely recognized as essential for the responsible development of automated systems. Despite growing efforts to support documentation through different kinds of artifacts, little is known about the motivations shaping documentation tool design or the factors hindering their adoption. We present a systematic review supported by mixed-methods analysis of 59 dataset documentation publications to examine the motivations behind building documentation tools, how authors conceptualize documentation practices, and how these tools connect to existing systems, regulations, and cultural norms. Our analysis shows four persistent patterns in dataset documentation conceptualization that potentially impede adoption and standardization: unclear operationalizations of documentation's value, decontextualized designs, unaddressed labor demands, and a tendency to treat integration as future work. Building on these findings, we propose a shift in Responsible AI tool design toward institutional rather than individual solutions, and outline actions the HCI community can take to enable sustainable documentation practices.

</details>


### [74] [ReLoop: Structured Modeling and Behavioral Verification for Reliable LLM-Based Optimization](https://arxiv.org/abs/2602.15983)
*Junbo Jacob Lian,Yujun Sun,Huiling Chen,Chaoyu Zhang,Chung-Piaw Teo*

Main category: cs.SE

TL;DR: 为解决大语言模型生成优化代码存在的隐性语义错误，本文提出ReLoop方法，通过结构化生成与行为验证提升代码正确性，实现执行率100%，并发布新的复杂零售优化数据集。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成的优化代码虽可执行且返回有效解，但存在高达90%的语义错误风险，导致解决方案虽然可行但语义不正确，亟需提升模型代码生成的正确性和安全性。

Method: 提出了结构化生成与行为验证两种互补机制。结构化生成将代码生成拆解为理解、形式化、合成和验证四阶段，加入变量类型推理和自我验证；行为验证通过基于求解器的参数扰动检测错误，无需真实标注，提升错误捕捉能力。

Result: ReLoop 将最强模型的正确率从22.6%提升至31.1%，执行率从72.1%提升至100%，在五种模型及三个基准上均有稳定提升，还发布了涵盖复杂约束交互的零售优化数据集RetailOpt-190。

Conclusion: ReLoop 方法显著提升了大语言模型在优化代码生成中的正确性和无失败执行率，解决了语义错误导致的可行性-正确性差距，适用于多种模型和基准。

Abstract: Large language models (LLMs) can translate natural language into optimization code, but silent failures pose a critical risk: code that executes and returns solver-feasible solutions may encode semantically incorrect formulations, creating a feasibility-correctness gap of up to 90 percentage points on compositional problems. We introduce ReLoop, addressing silent failures from two complementary directions. Structured generation decomposes code production into a four-stage reasoning chain (understand, formalize, synthesize, verify) that mirrors expert modeling practice, with explicit variable-type reasoning and self-verification to prevent formulation errors at their source. Behavioral verification detects errors that survive generation by testing whether the formulation responds correctly to solver-based parameter perturbation, without requiring ground truth -- an external semantic signal that bypasses the self-consistency problem inherent in LLM-based code review. The two mechanisms are complementary: structured generation dominates on complex compositional problems, while behavioral verification becomes the largest single contributor on problems with localized formulation defects. Together with execution recovery via IIS-enhanced diagnostics, ReLoop raises correctness from 22.6% to 31.1% and execution from 72.1% to 100.0% on the strongest model, with consistent gains across five models spanning three paradigms (foundation, SFT, RL) and three benchmarks. We additionally release RetailOpt-190, 190 compositional retail optimization scenarios targeting the multi-constraint interactions where LLMs most frequently fail.

</details>


### [75] [The Limits of Long-Context Reasoning in Automated Bug Fixing](https://arxiv.org/abs/2602.16069)
*Ravi Raju,Mengmeng Ji,Shubhangi Upasani,Bo Li,Urmish Thakker*

Main category: cs.SE

TL;DR: 尽管大型语言模型在短上下文编码任务中表现良好，但在实际超长上下文（64k+ tokens）代码调试和修补生成中性能显著下降，暴露出其真正长上下文推理能力的限制。


<details>
  <summary>Details</summary>
Motivation: 随着上下文长度的快速增加，人们普遍认为大型语言模型（LLM）能够直接在整个代码库上进行推理。然而，当前是否能可靠地完成长上下文的代码调试和修补生成尚需系统评估。

Method: 在SWE-bench Verified控制实验环境下，使用agentic流程（mini-SWE-agent）评估最新LLM模型的性能。同时构建人造长上下文输入管道，通过放置相关文件扩展上下文长度（64k-128k tokens），测试模型在单次长上下文修补生成任务中的表现。

Result: 结果显示，虽然agentic流程中模型表现提升明显，但成功路径的上下文长度通常小于20k tokens；真正长上下文（64k tokens以上）下性能急剧下降，表现包括解决率大幅降低以及出现虚构差异、错误文件目标和格式错误的补丁头等失败模式。

Conclusion: 当前LLM在名义上的上下文长度与实际可用上下文容量之间存在显著差距。代理编码基准方法主要基于任务分解短上下文步骤，未能有效评估长上下文推理能力。

Abstract: Rapidly increasing context lengths have led to the assumption that large language models (LLMs) can directly reason over entire codebases. Concurrently, recent advances in LLMs have enabled strong performance on software engineering benchmarks, particularly when paired with agentic workflows. In this work, we systematically evaluate whether current LLMs can reliably perform long-context code debugging and patch generation. Using SWE-bench Verified as a controlled experimental setting, we first evaluate state-of-the-art models within an agentic harness (mini-SWE-agent), where performance improves substantially: GPT-5-nano achieves up to a 31\% resolve rate on 100 samples, and open-source models such as Deepseek-R1-0528 obtain competitive results. However, token-level analysis shows that successful agentic trajectories typically remain under 20k tokens, and that longer accumulated contexts correlate with lower success rates, indicating that agentic success primarily arises from task decomposition into short-context steps rather than effective long-context reasoning. To directly test long-context capability, we construct a data pipeline where we artificially inflate the context length of the input by placing the relevant files into the context (ensuring perfect retrieval recall); we then study single-shot patch generation under genuinely long contexts (64k-128k tokens). Despite this setup, performance degrades sharply: Qwen3-Coder-30B-A3B achieves only a 7\% resolve rate at 64k context, while GPT-5-nano solves none of the tasks. Qualitative analysis reveals systematic failure modes, including hallucinated diffs, incorrect file targets, and malformed patch headers. Overall, our findings highlight a significant gap between nominal context length and usable context capacity in current LLMs, and suggest that existing agentic coding benchmarks do not meaningfully evaluate long-context reasoning.

</details>


### [76] [Can Causality Cure Confusion Caused By Correlation (in Software Analytics)?](https://arxiv.org/abs/2602.16091)
*Amirali Rayegan,Tim Menzies*

Main category: cs.SE

TL;DR: 本文研究了在符号模型中引入因果意识分裂准则，以提升模型的稳定性和鲁棒性，同时对比了人类专家判断与自动模型的稳定性差异。


<details>
  <summary>Details</summary>
Motivation: 传统基于相关性的符号模型在软件工程应用中稳定性差，且因果发现算法存在结构学习的计算难题，导致解释的不可靠和难以复现。

Method: 使用MOOT仓库中120多个多目标优化任务，通过预注册的自助聚合协议评估稳定性，比较人类因果判断、相关性决策树和因果意识决策树的稳定性和性能，采用方差、基尼不纯度、KS检验和Cliff's delta等统计方法进行分析。

Result: 因果意识决策树在各种任务中展现出更高的稳定性和鲁棒性，同时保持了良好的预测和优化性能；人类专家的因果判断稳定性也进行了对比分析。

Conclusion: 引入因果意识的分裂准则有助于提高符号模型在软件工程任务中的稳定性和鲁棒性，且不会显著降低预测或优化性能。

Abstract: Background: Symbolic models, particularly decision trees, are widely used in software engineering for explainable analytics in defect prediction, configuration tuning, and software quality assessment. Most of these models rely on correlational split criteria, such as variance reduction or information gain, which identify statistical associations but cannot imply causation between X and Y. Recent empirical studies in software engineering show that both correlational models and causal discovery algorithms suffer from pronounced instability. This instability arises from two complementary issues: 1-Correlation-based methods conflate association with causation. 2-Causal discovery algorithms rely on heuristic approximations to cope with the NP-hard nature of structure learning, causing their inferred graphs to vary widely under minor input perturbations. Together, these issues undermine trust, reproducibility, and the reliability of explanations in real-world SE tasks. Objective: This study investigates whether incorporating causality-aware split criteria into symbolic models can improve their stability and robustness, and whether such gains come at the cost of predictive or optimization performance. We additionally examine how the stability of human expert judgments compares to that of automated models. Method: Using 120+ multi-objective optimization tasks from the MOOT repository of multi-objective optimization tasks, we evaluate stability through a preregistered bootstrap-ensemble protocol that measures variance with win-score assignments. We compare the stability of human causal assessments with correlation-based decision trees (EZR). We would also compare the causality-aware trees, which leverage conditional-entropy split criteria and confounder filtering. Stability and performance differences are analyzed using statistical methods (variance, Gini Impurity, KS test, Cliff's delta)

</details>


### [77] [Algorithm-Based Pipeline for Reliable and Intent-Preserving Code Translation with LLMs](https://arxiv.org/abs/2602.16106)
*Shahriar Rumi Dipto,Saikat Mondal,Chanchal K. Roy*

Main category: cs.SE

TL;DR: 本文提出了一种基于算法的代码翻译流程，通过引入语言中立的中间表示，提升了大型语言模型在Python和Java之间代码转换的准确性和可靠性，实验证明该方法显著减少了各种编译和运行时错误，准确率提高了10.8%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型直接一次性翻译代码时容易丢失程序意图，导致控制流、类型处理及I/O等方面的错误，因此需要结构化的规划方法以提高翻译的准确性和可靠性。

Method: 引入了语言中立的中间代码规范作为转换中介，设计了自动成对实验，使用五种主流大型语言模型，在两个数据集上对Python与Java间的代码进行双向翻译，并通过实际编译执行与测试来评估翻译结果的准确性和错误分类。

Result: 算法驱动方法使微平均准确率提升至78.5%，相比直接翻译提升10.8%，完全消除词法和标记错误，显著降低构造不完整、结构和声明错误，以及运行时依赖和入口点失败。

Conclusion: 算法驱动的代码翻译流程显著提升了代码翻译的正确性和稳定性，尤其有效避免了词法、结构及运行时相关错误，支持更可靠的多语言编程工具构建。

Abstract: Code translation, the automatic conversion of programs between languages, is a growing use case for Large Language Models (LLMs). However, direct one-shot translation often fails to preserve program intent, leading to errors in control flow, type handling, and I/O behavior. We propose an algorithm-based pipeline that introduces a language-neutral intermediate specification to capture these details before code generation. This study empirically evaluates the extent to which structured planning can improve translation accuracy and reliability relative to direct translation. We conduct an automated paired experiment - direct and algorithm-based to translate between Python and Java using five widely used LLMs on the Avatar and CodeNet datasets. For each combination (model, dataset, approach, and direction), we compile and execute the translated program and run the tests provided. We record compilation results, runtime behavior, timeouts (e.g., infinite loop), and test outcomes. We compute accuracy from these tests, counting a translation as correct only if it compiles, runs without exceptions or timeouts, and passes all tests. We then map every failed compile-time and runtime case to a unified, language-aware taxonomy and compare subtype frequencies between the direct and algorithm-based approaches. Overall, the Algorithm-based approach increases micro-average accuracy from 67.7% to 78.5% (10.8% increase). It eliminates lexical and token errors by 100%, reduces incomplete constructs by 72.7%, and structural and declaration issues by 61.1%. It also substantially lowers runtime dependency and entry-point failures by 78.4%. These results demonstrate that algorithm-based pipelines enable more reliable, intent-preserving code translation, providing a foundation for robust multilingual programming assistants.

</details>


### [78] [Software-heavy Asset Administration Shells: Classification and Use Cases](https://arxiv.org/abs/2602.16499)
*Carsten Ellwein,David Dietrich,Jessica Roth,Rozana Cvitkovic,Andreas Wortmann*

Main category: cs.SE

TL;DR: 本文系统分析了将软件服务集成到资产管理壳的架构，填补了相关领域的研究空白。


<details>
  <summary>Details</summary>
Motivation: 随着制造业数字化和人工智能的应用，软件在资产管理壳（AAS）中的重要性不断提高，目前缺乏系统性的软件架构分析来支持将软件服务直接集成到AAS中。

Method: 基于软件质量标准和制造业典型用例，对现有架构方案进行系统化分析和分类。

Result: 本文系统区分了基于软件质量标准和典型制造用例的软件架构，为软件重度AAS的设计提供了指导。

Conclusion: 通过对比不同架构并结合软件质量及制造业应用需求，本文为软件重度AAS的设计提供了有价值的参考和指导。

Abstract: The Asset Administration Shell (AAS) is an emerging technology for the implementation of digital twins in the field of manufacturing. Software is becoming increasingly important, not only in general but specifically in relation to manufacturing, especially with regard to digital manufacturing and a shift towards the usage of artificial intelligence. This increases the need not only to model software, but also to integrate services directly into the AAS. The existing literature contains individual solutions to implement such software-heavy AAS. However, there is no systematic analysis of software architectures that integrate software services directly into the AAS. This paper aims to fill this research gap and differentiate architectures based on software quality criteria as well as typical manufacturing use cases. This work may be considered as an interpretation guideline for software-heavy AAS, both in academia and for practitioners.

</details>


### [79] [SPARC: Scenario Planning and Reasoning for Automated C Unit Test Generation](https://arxiv.org/abs/2602.16671)
*Jaid Monwar Chowdhury,Chi-An Fu,Reyhaneh Jabbarvand*

Main category: cs.SE

TL;DR: 本文提出SPARC框架，结合神经符号方法，通过控制流图分析、操作映射、路径目标测试合成及自我校正循环，实现了对C语言单元测试的自动生成，大幅提升了测试覆盖率和质量。


<details>
  <summary>Details</summary>
Motivation: C语言自动单元测试困难，因指针算术和内存管理的语义鸿沟，使得直接使用大语言模型生成代码经常产生无法编译或语义无关的测试代码。

Method: SPARC框架包含四阶段：控制流图分析、基于验证工具的操作映射、路径目标测试合成，以及利用编译器和运行时反馈的迭代自我校正验证循环。

Result: 在59个真实和算法测试对象上，SPARC在代码行覆盖率（提升31.36%）、分支覆盖率（提升26.01%）和变异测试得分（提升20.78%）上均优于基础方法，并匹配甚至超过符号执行工具KLEE，同时测试代码的可读性和可维护性显著提升。

Conclusion: 通过将大语言模型推理与程序结构对齐，SPARC成功克服了C语言测试自动生成中的语义鸿沟，提供了适用于工业遗留C代码库的可扩展测试方案。

Abstract: Automated unit test generation for C remains a formidable challenge due to the semantic gap between high-level program intent and the rigid syntactic constraints of pointer arithmetic and manual memory management. While Large Language Models (LLMs) exhibit strong generative capabilities, direct intent-to-code synthesis frequently suffers from the leap-to-code failure mode, where models prematurely emit code without grounding in program structure, constraints, and semantics. This will result in non-compilable tests, hallucinated function signatures, low branch coverage, and semantically irrelevant assertions that cannot properly capture bugs. We introduce SPARC, a neuro-symbolic, scenario-based framework that bridges this gap through four stages: (1) Control Flow Graph (CFG) analysis, (2) an Operation Map that grounds LLM reasoning in validated utility helpers, (3) Path-targeted test synthesis, and (4) an iterative, self-correction validation loop using compiler and runtime feedback. We evaluate SPARC on 59 real-world and algorithmic subjects, where it outperforms the vanilla prompt generation baseline by 31.36% in line coverage, 26.01% in branch coverage, and 20.78% in mutation score, matching or exceeding the symbolic execution tool KLEE on complex subjects. SPARC retains 94.3% of tests through iterative repair and produces code with significantly higher developer-rated readability and maintainability. By aligning LLM reasoning with program structure, SPARC provides a scalable path for industrial-grade testing of legacy C codebases.

</details>
