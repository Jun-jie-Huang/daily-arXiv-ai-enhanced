<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 144]
- [cs.MA](#cs.MA) [Total: 11]
- [cs.SE](#cs.SE) [Total: 22]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [TeleMem: Building Long-Term and Multimodal Memory for Agentic AI](https://arxiv.org/abs/2601.06037)
*Chunliang Chen,Ming Guan,Xiao Lin,Jiaxu Li,Qiyi Wang,Xiangyu Chen,Jixiang Luo,Changzhi Sun,Dell Zhang,Xuelong Li*

Main category: cs.CL

TL;DR: 本文提出了TeleMem，一种统一的长时和多模态记忆系统，通过结构化写入流程和多模态推理机制，有效提升了对话长时记忆的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在长期对话中因注意力限制表现不佳，检索增强生成虽然缓解问题但缺乏可靠记忆更新机制，导致错误信息与效率低下。

Method: 提出TeleMem系统，通过叙事动态提取维护连贯的用户信息，采用批量检索、聚类和整合记忆条目的结构化写入流程，并结合多模态记忆和ReAct推理方法，实现闭环观察、思考和行动。

Result: 在长时角色扮演游戏基准ZH-4O上，TeleMem较最先进的Mem0提高19%准确率，减少43%令牌使用，存储操作速度提升2.1倍。

Conclusion: TeleMem有效解决了长期对话记忆的更新与多模态理解难题，显著提升了语言模型的长期交互性能和效率，具有广泛应用潜力。

Abstract: Large language models (LLMs) excel at many NLP tasks but struggle to sustain long-term interactions due to limited attention over extended dialogue histories. Retrieval-augmented generation (RAG) mitigates this issue but lacks reliable mechanisms for updating or refining stored memories, leading to schema-driven hallucinations, inefficient write operations, and minimal support for multimodal reasoning.To address these challenges, we propose TeleMem, a unified long-term and multimodal memory system that maintains coherent user profiles through narrative dynamic extraction, ensuring that only dialogue-grounded information is preserved. TeleMem further introduces a structured writing pipeline that batches, retrieves, clusters, and consolidates memory entries, substantially improving storage efficiency, reducing token usage, and accelerating memory operations. Additionally, a multimodal memory module combined with ReAct-style reasoning equips the system with a closed-loop observe, think, and act process that enables accurate understanding of complex video content in long-term contexts. Experimental results show that TeleMem surpasses the state-of-the-art Mem0 baseline with 19% higher accuracy, 43% fewer tokens, and a 2.1x speedup on the ZH-4O long-term role-play gaming benchmark.

</details>


### [2] [Operation Veja: Fixing Fundamental Concepts Missing from Modern Roleplaying Training Paradigms](https://arxiv.org/abs/2601.06039)
*Yueze Liu,Ajay Nagi Reddy Kumdam,Ronit Kanjilal,Hao Yang,Yichi Zhang*

Main category: cs.CL

TL;DR: 本文提出了VEJA框架，通过引入价值观、经历、判断和能力四个核心要素，解决现有角色扮演模型缺乏真实感的问题。


<details>
  <summary>Details</summary>
Motivation: 当前角色扮演模型难以捕捉真实、生动的人物性格，主要因为训练方法忽视了人物内心世界的动态互动。

Method: 提出VEJA框架作为数据策划的新范式，并通过一个基于VEJA框架的手工策划数据集与最新合成数据集进行对比实验。

Result: 实验通过一个大型语言模型作为评审，显示VEJA数据集在角色真实性和叙事连贯性方面明显优于合成数据集。

Conclusion: 为了构建具有真实深度和叙事连续性的角色扮演代理，需要转向以VEJA为代表的理念驱动的数据策划。

Abstract: Modern roleplaying models are increasingly sophisticated, yet they consistently struggle to capture the essence of believable, engaging characters. We argue this failure stems from training paradigms that overlook the dynamic interplay of a character's internal world. Current approaches, including Retrieval-Augmented Generation (RAG), fact-based priming, literature-based learning, and synthetic data generation, exhibit recurring limitations in modeling the deliberative, value-conflicted reasoning that defines human interaction. In this paper, we identify four core concepts essential for character authenticity: Values, Experiences, Judgments, and Abilities (VEJA). We propose the VEJA framework as a new paradigm for data curation that addresses these systemic limitations. To illustrate the qualitative ceiling enabled by our framework, we present a pilot study comparing a manually curated, VEJA-grounded dataset against a state-of-the-art synthetic baseline. Using an LLM-as-judge evaluation, our findings demonstrate a significant quality gap, suggesting that a shift toward conceptually grounded data curation, as embodied by VEJA, is necessary for creating roleplaying agents with genuine depth and narrative continuity. The full dataset is available at https://github.com/HyouinKyoumaIRL/Operation-Veja

</details>


### [3] [Lexical and Statistical Analysis of Bangla Newspaper and Literature: A Corpus-Driven Study on Diversity, Readability, and NLP Adaptation](https://arxiv.org/abs/2601.06041)
*Pramit Bhattacharyya,Arnab Bhattacharya*

Main category: cs.CL

TL;DR: 本论文对孟加拉文学文本和报纸文本进行了全面的语料库驱动分析，比较了两者的词汇丰富度、结构复杂性和可读性，发现文学语料具有更高的词汇多样性和结构变化，且对下游任务模型表现有积极影响。


<details>
  <summary>Details</summary>
Motivation: 探究孟加拉文学语料与报纸语料在词汇和结构属性上的差异，以及这些差异如何影响语言模型和下游任务的性能。

Method: 利用Vacaspati文学语料库和IndicCorp报纸语料库，分析类型-标记比、单次词比例、大ram多样性、平均音节及词长、Zipf定律遵循度等指标，构建n-gram模型计算困惑度，同时引入Flesch和Coleman-Liau可读性指数评估文本复杂性。

Result: 文学语料库在词汇丰富度（如HLR、Bigram多样性）、结构复杂性（高困惑度、低冗余度）及可读性指标上均表现出更高复杂性和多样性。融合文学与报纸文本的数据有助于提升语言模型在下游任务中的表现。

Conclusion: 孟加拉文学语料相较于报纸语料更丰富、更复杂且更符合语言全局规律，将文学语料与报纸数据结合可提高语言模型的表现，提示文学文本在语言研究和模型训练中具有重要价值。

Abstract: In this paper, we present a comprehensive corpus-driven analysis of Bangla literary and newspaper texts to investigate their lexical diversity, structural complexity and readability. We undertook Vacaspati and IndicCorp, which are the most extensive literature and newspaper-only corpora for Bangla. We examine key linguistic properties, including the type-token ratio (TTR), hapax legomena ratio (HLR), Bigram diversity, average syllable and word lengths, and adherence to Zipfs Law, for both newspaper (IndicCorp) and literary corpora (Vacaspati).For all the features, such as Bigram Diversity and HLR, despite its smaller size, the literary corpus exhibits significantly higher lexical richness and structural variation. Additionally, we tried to understand the diversity of corpora by building n-gram models and measuring perplexity. Our findings reveal that literary corpora have higher perplexity than newspaper corpora, even for similar sentence sizes. This trend can also be observed for the English newspaper and literature corpus, indicating its generalizability. We also examined how the perfor- mance of models on downstream tasks is influenced by the inclusion of literary data alongside newspaper data. Our findings suggest that inte- grating literary data with newspapers improves the performance of models on various downstream tasks. We have also demonstrated that a literary corpus adheres more closely to global word distribution proper- ties, such as Zipfs law, than a newspaper corpus or a merged corpus of both literary and newspaper texts. Literature corpora also have higher entropy and lower redundancy values compared to a newspaper corpus. We also further assess the readability using Flesch and Coleman-Liau in- dices, showing that literary texts are more complex.

</details>


### [4] [Reinforcement Learning for Chain of Thought Compression with One-Domain-to-All Generalization](https://arxiv.org/abs/2601.06052)
*Hanyu Li,Jiangshan Duo,Bofei Gao,Hailin Zhang,Sujian Li,Xiaotie Deng,Liang Zhao*

Main category: cs.CL

TL;DR: 本文提出了一种基于样本级软强化学习的推理压缩方法，显著减少推理长度同时保持或提升准确率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型中的链式推理常导致“过度思考陷阱”，造成计算资源浪费和延迟，且传统静态全局控制方式可能抑制必要的推理步骤。

Method: 引入样本级软强化学习压缩方法，在模型已掌握且能生成更简短推理的情况下惩罚冗长推理路径，从而动态减少不必要的推理步骤。

Result: 实验显示该方法减少平均响应长度20-40%，并在数学领域训练的模型能够跨域提升代码、指令遵循及通用问答任务的推理简洁性和准确率。

Conclusion: 推理压缩作为一个后训练稳定课程（准确率-压缩-准确率）能产出更准确且推理更简洁的模型，建议将该方法作为高效推理模型开发的标准环节。

Abstract: Chain-of-thought reasoning in large language models often creates an "overthinking trap," leading to excessive computational cost and latency for unreliable accuracy gains. Prior work has typically relied on global, static controls that risk penalizing necessary reasoning. We introduce a sample-level, soft reinforcement learning compression method that penalizes inefficiently long rollouts, but only on problems where the model has already mastered and already produced a more concise rollout. Our experiments show that this method reduces average response length by 20-40% with comparable or higher accuracy. Crucially, the compression exhibits strong cross-domain generalization; a model trained on math spontaneously shortens responses on unseen tasks like code, instruction following, and general knowledge QA, with stable or improved accuracy. We demonstrate a stable post-training curriculum (accuracy-compression-accuracy) that can ultimately produce models that are more accurate and reason more concisely, arguing that such compression method should be a standard phase in developing efficient reasoning models.

</details>


### [5] [A Multi-Stage Workflow for the Review of Marketing Content with Reasoning Large Language Models](https://arxiv.org/abs/2601.06054)
*Alberto Purpura,Emily Chen,Swapnil Shinde*

Main category: cs.CL

TL;DR: 本文提出了一种多阶段工作流程，利用微调推理大语言模型辅助营销内容的审核，确保其符合要求。


<details>
  <summary>Details</summary>
Motivation: 解决自动识别文本内容合规性问题，提高营销内容审核效率和准确性。

Method: 设计多阶段流程，比较不同微调策略（SFT和GRPO），训练小型LLM生成推理标记，并评估不同奖励函数组合对模型性能的影响。

Result: 验证了该方法在自动识别合规问题上的有效性，不同微调策略和奖励函数组合对性能有显著影响。

Conclusion: 提出的多阶段工作流程和微调策略为营销内容合规性审核提供了有效工具，促进了自动化高效审核的实现。

Abstract: Reasoning Large Language Models (LLMs) have shown promising results when tasked with solving complex problems. In this paper, we propose and evaluate a multi-stage workflow that leverages the capabilities of fine-tuned reasoning LLMs to assist in the review process of marketing content, making sure they comply with a given list of requirements. The contributions of this paper are the following: (i) we present a novel approach -- that does not rely on any external knowledge representation -- for the automatic identification of compliance issues in textual content; (ii) compare the effectiveness of different fine-tuning strategies like Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO) in training models to solve this problem; (iii) we evaluate the effectiveness of training small LLMs to generate reasoning tokens before providing their final response; (iv) we evaluate how the choice and combinations of different reward functions affects the performance of a model trained with GRPO.

</details>


### [6] [AzeroS: Extending LLM to Speech with Self-Generated Instruction-Free Tuning](https://arxiv.org/abs/2601.06086)
*Yiwen Shao,Wei Liu,Jiahong Li,Tianzi Wang,Kun Wei,Meng Yu,Dong Yu*

Main category: cs.CL

TL;DR: 本文提出了一种无监督自生成指令微调方法（SIFT）用于训练语音领域的大型语言模型，实现了对未见任务的最佳泛化。基于此，开发了AZeroS模型，在保持预训练模型和编码器不变的条件下，仅训练轻量级投影模块，达到了语义和副语言任务的最先进水平。


<details>
  <summary>Details</summary>
Motivation: 当前将大型语言模型扩展到语音领域通常需要大量针对特定任务的指令调优数据，整理这些数据耗时且泛化能力差。为解决这一问题，提出无指令的自生成调优方法以提升泛化能力并减少数据需求。

Method: 提出Self-Generated Instruction-Free Tuning (SIFT)方法，通过冻结的语言模型自动生成监督信号；基于此，构建AZeroS模型，利用公开语音语料的语音-文本对训练，仅更新轻量级的投影模块，保持主体模型和音频编码器冻结。

Result: AZeroS模型在语义和副语言基准（如VoiceBench、AIR-Bench Foundation (Speech)和AIR-Bench Chat (Speech)）上实现了最先进性能，且训练成本和数据规模均较低。

Conclusion: 通过SIFT方法，无需收集任务特定的问答数据即可实现最佳泛化，AZeroS展示了该范式在语音领域大语言模型中的有效性和高效性。

Abstract: Extending large language models (LLMs) to the speech domain has recently gained significant attention. A typical approach connects a pretrained LLM with an audio encoder through a projection module and trains the resulting model on large-scale, task-specific instruction-tuning datasets. However, curating such instruction-tuning data for specific requirements is time-consuming, and models trained in this manner often generalize poorly to unseen tasks. In this work, we first formulate that the strongest generalization of a speech-LLM is achieved when it is trained with Self-Generated Instruction-Free Tuning (SIFT), in which supervision signals are generated by a frozen LLM using textual representations of speech as input. Our proposed SIFT paradigm eliminates the need for collecting task-specific question-answer pairs and yields the theoretically best generalization to unseen tasks. Building upon this paradigm, we introduce AZeroS (Auden Zero-instruction-tuned Speech-LLM), which is trained on speech-text pairs derived from publicly available corpora, including approximately 25,000 hours of speech with ASR transcripts and 3,000 hours of speech with paralinguistic labels. Built upon Qwen2.5-7B-Instruct, the model updates only two lightweight projection modules (23.8 million parameters each), while keeping both the LLM and audio encoders frozen. Despite the minimal training cost and modest data scale, AZeroS achieves state-of-the-art performance on both semantic and paralinguistic benchmarks, including VoiceBench, AIR-Bench Foundation (Speech), and AIR-Bench Chat (Speech).

</details>


### [7] [Is Sanskrit the most token-efficient language? A quantitative study using GPT, Gemini, and SentencePiece](https://arxiv.org/abs/2601.06142)
*Anshul Kumar*

Main category: cs.CL

TL;DR: 该论文研究了梵语在大语言模型中的token效率，发现梵语的tokens数量约为英语和印地语的一半，显示了其较高的表达紧凑性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的分词器在处理不同语言时存在效率差异，尤其是没有量化梵语由于其形态学和语法规则所带来的表达效率。

Method: 使用包含701段巴哈伽德歌三语（梵语、英语、印地语）及梵语音译的平行文本，测试了多种分词器（SPM、GPT旧版及最新版、Gemini分词器），通过token数量、每token字符数及每字符token数等指标进行评估。

Result: 结果显示在无偏分词器SPM下，梵语的token数量约为英语/印地语的一半；英语/印地语翻译的梵语注释的token数增加20倍；新版GPT和Gemini分词器虽减小偏差但仍未完全捕捉梵语紧凑性。

Conclusion: 梵语具有极高的编码紧凑性，有助于降低计算和推理成本，未来分词器设计可借鉴此研究成果，改善对非英语语言的处理效率。

Abstract: Tokens are the basic units of Large Language Models (LLMs). LLMs rely on tokenizers to segment text into these tokens, and tokenization is the primary determinant of computational and inference cost. Sanskrit, one of the oldest languages, is hypothesized to express more meaning per token due to its morphology and grammar rules; however, no prior work has quantified this. We use a dataset of 701 parallel verses of the Bhagavad Gita, which comprises three languages-Sanskrit, English, and Hindi along with transliteration of Sanskrit into English. We test tokenizers including SentencePiece (SPM), older GPT models, and the latest generation tokenizers from Gemini and GPT. We use metrics of token count, characters per token (token efficiency), and tokens per character (token cost). Results show a ~2x difference in token counts between Sanskrit and English/Hindi under the unbiased SPM baseline. English/Hindi translations of Sanskrit commentary resulted in an approximately 20x increase in token count. GPT o200k base (latest, used by GPT-4o) and Gemini (latest) reduce bias by a significant degree compared to GPT cl100k base (used until GPT-4), but still fail to fully capture Sanskrit's compactness. This matters because there might be a penalty bias for non-English users, which inflates the token count. This research provides a foundation for improving future tokenizer design and shows the potential of Sanskrit for highly compact encoding, saving on cost while speeding up training and inference. The code and dataset are available at https://github.com/anshulkr713/sanskrit-token-efficiency

</details>


### [8] [Amory: Building Coherent Narrative-Driven Agent Memory through Agentic Reasoning](https://arxiv.org/abs/2601.06282)
*Yue Zhou,Xiaobo Guo,Belhassen Bayar,Srinivasan H. Sengamedu*

Main category: cs.CL

TL;DR: 本文提出了一种名为Amory的工作记忆框架，解决了长期对话中处理全部历史信息的计算难题。


<details>
  <summary>Details</summary>
Motivation: 当前方法通过分片记忆和检索简化计算，但忽视了人类记忆的细微和连贯性，导致记忆形成不足。

Method: Amory主动构建结构化记忆，将对话片段组织成情节叙事，通过动力动量巩固记忆，并将外围事实语义化；检索时基于叙事结构进行连贯性推理。

Result: 在LOCOMO长期推理基准测试中，Amory相比前沿方法性能大幅提升，响应时间减半，记忆覆盖度优于基于嵌入的方法。

Conclusion: Amory有效提升了长期对话系统的记忆组织和检索能力，实现了高效且连贯的对话响应。

Abstract: Long-term conversational agents face a fundamental scalability challenge as interactions extend over time: repeatedly processing entire conversation histories becomes computationally prohibitive. Current approaches attempt to solve this through memory frameworks that predominantly fragment conversations into isolated embeddings or graph representations and retrieve relevant ones in a RAG style. While computationally efficient, these methods often treat memory formation minimally and fail to capture the subtlety and coherence of human memory. We introduce Amory, a working memory framework that actively constructs structured memory representations through enhancing agentic reasoning during offline time. Amory organizes conversational fragments into episodic narratives, consolidates memories with momentum, and semanticizes peripheral facts into semantic memory. At retrieval time, the system employs coherence-driven reasoning over narrative structures. Evaluated on the LOCOMO benchmark for long-term reasoning, Amory achieves considerable improvements over previous state-of-the-art, with performance comparable to full context reasoning while reducing response time by 50%. Analysis shows that momentum-aware consolidation significantly enhances response quality, while coherence-driven retrieval provides superior memory coverage compared to embedding-based approaches.

</details>


### [9] [How well can off-the-shelf LLMs elucidate molecular structures from mass spectra using chain-of-thought reasoning?](https://arxiv.org/abs/2601.06289)
*Yufeng Wang,Lu Wei,Lin Liu,Hao Xu,Haibin Ling*

Main category: cs.CL

TL;DR: 本文探讨大语言模型(LLMs)在基于串联质谱数据推断小分子结构中的应用及其局限性。


<details>
  <summary>Details</summary>
Motivation: 精准识别小分子结构一直是串联质谱分析中的难点，而LLMs在科学推理任务中的潜力为此提供新思路。

Method: 设计链式思维(CoT)提示框架，模拟专家化学家的推理步骤，如双键当量分析、中性丢失识别和碎片组装，并在MassSpecGym数据集上零样本评估多种先进LLMs。

Result: LLMs能生成语法有效且部分合理的分子结构，但未能实现化学准确性，推理与分子预测未能有效关联。

Conclusion: 研究揭示了LLMs在分子解析上的解释潜力和当前不足，提示未来需结合领域知识和强化学习提升化学推理能力。

Abstract: Mass spectrometry (MS) is a powerful analytical technique for identifying small molecules, yet determining complete molecular structures directly from tandem mass spectra (MS/MS) remains a long-standing challenge due to complex fragmentation patterns and the vast diversity of chemical space. Recent progress in large language models (LLMs) has shown promise for reasoning-intensive scientific tasks, but their capability for chemical interpretation is still unclear. In this work, we introduce a Chain-of-Thought (CoT) prompting framework and benchmark that evaluate how LLMs reason about mass spectral data to predict molecular structures. We formalize expert chemists' reasoning steps-such as double bond equivalent (DBE) analysis, neutral loss identification, and fragment assembly-into structured prompts and assess multiple state-of-the-art LLMs (Claude-3.5-Sonnet, GPT-4o-mini, and Llama-3 series) in a zero-shot setting using the MassSpecGym dataset. Our evaluation across metrics of SMILES validity, formula consistency, and structural similarity reveals that while LLMs can produce syntactically valid and partially plausible structures, they fail to achieve chemical accuracy or link reasoning to correct molecular predictions. These findings highlight both the interpretive potential and the current limitations of LLM-based reasoning for molecular elucidation, providing a foundation for future work that combines domain knowledge and reinforcement learning to achieve chemically grounded AI reasoning.

</details>


### [10] [$\texttt{AMEND++}$: Benchmarking Eligibility Criteria Amendments in Clinical Trials](https://arxiv.org/abs/2601.06300)
*Trisha Das,Mandis Beigi,Jacob Aptekar,Jimeng Sun*

Main category: cs.CL

TL;DR: 本文提出了一种预测临床试验入选标准未来是否会被修订的新型自然语言处理任务，并发布了相关数据集和基于大语言模型优化的预训练方法，实现了更准确的修订预测。


<details>
  <summary>Details</summary>
Motivation: 临床试验中的修订尤其是入选标准的修订给试验带来时间延误、成本增加以及管理负担，因此希望提前预测是否会有此类修订。

Method: 发布了包含入选标准版本历史及修订标签的数据集AMEND和经大语言模型去噪筛选的AMEND_LLM子集，提出了一种利用历史修订信息的变更感知掩码语言模型预训练策略（CAMLM）。

Result: CAMLM在多种基线方法上均提升了修订预测的效果，提高了预测的稳健性和成本效益。

Conclusion: 通过引入新的预测任务和改进的预训练方法，能够更有效地预测临床试验入选标准的修订，为优化临床试验设计提供支持。

Abstract: Clinical trial amendments frequently introduce delays, increased costs, and administrative burden, with eligibility criteria being the most commonly amended component. We introduce \textit{eligibility criteria amendment prediction}, a novel NLP task that aims to forecast whether the eligibility criteria of an initial trial protocol will undergo future amendments. To support this task, we release $\texttt{AMEND++}$, a benchmark suite comprising two datasets: $\texttt{AMEND}$, which captures eligibility-criteria version histories and amendment labels from public clinical trials, and $\verb|AMEND_LLM|$, a refined subset curated using an LLM-based denoising pipeline to isolate substantive changes. We further propose $\textit{Change-Aware Masked Language Modeling}$ (CAMLM), a revision-aware pretraining strategy that leverages historical edits to learn amendment-sensitive representations. Experiments across diverse baselines show that CAMLM consistently improves amendment prediction, enabling more robust and cost-effective clinical trial design.

</details>


### [11] [Why LoRA Fails to Forget: Regularized Low-Rank Adaptation Against Backdoors in Language Models](https://arxiv.org/abs/2601.06305)
*Hoang-Chau Luong,Lingwei Chen*

Main category: cs.CL

TL;DR: 本文分析了LoRA在去除有毒预训练模型后门行为中的不足，提出了RoRA方法以提高防御效果。


<details>
  <summary>Details</summary>
Motivation: LoRA虽被广泛用于大语言模型的参数高效微调，但其在去除预训练模型中的后门行为时效果不佳，需理解其根本原因并改进。

Method: 通过光谱分析揭示LoRA更新的光谱强度不足且光谱对齐不佳，提出RoRA通过增强光谱强度、正则化和光谱重标定等手段改进微调过程。

Result: RoRA在多个自然语言处理基准和攻击设置中显著降低了攻击成功率，同时保持了模型的干净准确率。

Conclusion: RoRA有效提升了LoRA的防后门性能，证明了光谱性质在后门防御中的关键作用，提供了一种实用的防御策略。

Abstract: Low-Rank Adaptation (LoRA) is widely used for parameter-efficient fine-tuning of large language models, but it is notably ineffective at removing backdoor behaviors from poisoned pretrained models when fine-tuning on clean dataset. Contrary to the common belief that this weakness is caused primarily by low rank, we show that LoRA's vulnerability is fundamentally spectral. Our analysis identifies two key factors: LoRA updates (i) possess insufficient spectral strength, with singular values far below those of pretrained weights, and (ii) exhibit unfavorable spectral alignment, weakly matching clean-task directions while retaining overlap with trigger-sensitive subspaces. We further establish a critical scaling threshold beyond which LoRA can theoretically suppress trigger-induced activations, and we show empirically that standard LoRA rarely reaches this regime. We introduce Regularized Low-Rank Adaptation (RoRA), which improves forgetting by increasing spectral strength and correcting alignment through clean-strengthened regularization, trigger-insensitive constraints, and post-training spectral rescaling. Experiments across multiple NLP benchmarks and attack settings show that RoRA substantially reduces attack success rates while maintaining clean accuracy.

</details>


### [12] [SyntaxMind at BLP-2025 Task 1: Leveraging Attention Fusion of CNN and GRU for Hate Speech Detection](https://arxiv.org/abs/2601.06306)
*Md. Shihab Uddin Riad*

Main category: cs.CL

TL;DR: 该论文提出了一种用于孟加拉语仇恨言论检测的统一模型，结合了BanglaBERT嵌入、多分支GRU和CNN网络，以及注意力机制，实现了优秀的分类性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决孟加拉语文本中的仇恨言论识别问题，提升分类准确性。

Method: 利用BanglaBERT生成文本嵌入，结合多并行GRU与CNN分支，经过注意力机制和全连接层进行最终分类。

Result: 模型在BLP-2025任务1的子任务1A中获得第2名（micro F1=0.7345），子任务1B获得第5名（micro F1=0.7317）。

Conclusion: 该统一模型有效捕捉上下文语义和局部语言特征，在孟加拉语仇恨言论检测中表现优异。

Abstract: This paper describes our system used in the BLP-2025 Task 1: Hate Speech Detection. We participated in Subtask 1A and Subtask 1B, addressing hate speech classification in Bangla text. Our approach employs a unified architecture that integrates BanglaBERT embeddings with multiple parallel processing branches based on GRUs and CNNs, followed by attention and dense layers for final classification. The model is designed to capture both contextual semantics and local linguistic cues, enabling robust performance across subtasks. The proposed system demonstrated high competitiveness, obtaining 0.7345 micro F1-Score (2nd place) in Subtask 1A and 0.7317 micro F1-Score (5th place) in Subtask 1B.

</details>


### [13] [A Rising Tide Lifts All Boats: MTQE Rewards for Idioms Improve General Translation Quality](https://arxiv.org/abs/2601.06307)
*Ishika Agarwal,Zhenlin He,Dhruva Patil,Dilek Hakkani-Tür*

Main category: cs.CL

TL;DR: 本文针对神经机器翻译中非组合性表达（成语、谚语、比喻）翻译难题，提出使用基于质量估计的GRPO风格微调方法，提升成语及跨语言翻译质量。


<details>
  <summary>Details</summary>
Motivation: 非组合性表达含丰富文化内涵，具有字面和比喻双重含义，常规模型难以准确翻译，需改进模型处理此类表达的能力。

Method: 采用基于机器翻译质量评估（MTQE）模型的奖励函数，利用GRPO风格的微调方法训练模型，以增强成语翻译能力。

Result: 在中印成语数据集上，成语翻译能力提升约14分，非成语翻译能力提升约8分，跨语言翻译能力提升约6分。

Conclusion: 本文量化了非组合性表达翻译差距，提出的微调方法有效提升了多语言成语及跨文化隐喻的翻译表现，为发展具备更强跨文化和比喻理解能力的大语言模型提供了思路。

Abstract: Non-compositional expressions (e.g., idioms, proverbs, and metaphors) pose significant challenges for neural machine translation systems because their meanings cannot be derived from individual words alone. These expressions encode rich, cultural meaning, and have both figurative and literal meanings, making accurate translation difficult. Because models are fairly good at translating compositional text, we investigate GRPO-style fine-tuning using Machine Translation Quality Estimation (MTQE) models as reward functions to train models to better translate idioms. Using Chinese and Hindi idiom datasets, we find that idiom translation abilities improve by ~14 points, general, non-idiomatic translation implicitly improves by ~8 points, and cross-lingual translation abilities (trained on one language, evaluated on another) improves by ~6 points. Overall, our work quantifies the non-compositional translation gap and offers insights for developing LLMs with stronger cross-cultural and figurative language understanding.

</details>


### [14] [Annotating Dimensions of Social Perception in Text: The First Sentence-Level Dataset of Warmth and Competence](https://arxiv.org/abs/2601.06316)
*Mutaz Ayesh,Saif M. Mohammad,Nedjma Ousidhoum*

Main category: cs.CL

TL;DR: 本文提出了首个句子级的温暖（信任和社交性）与能力数据集W&C-Sent，包含1600多个社交媒体文本句子及其对应目标的标注，评估了大语言模型在识别信任、社交性和能力方面的表现。


<details>
  <summary>Details</summary>
Motivation: 现有词汇级别的温暖与能力词典难以捕捉这些社会心理维度在更大文本和话语中的表现，亟需句子级别数据进行深入研究。

Method: 构建超过1600个英文句子与其目标的三维度标注（信任、社交性和能力），数据来自社交媒体，详细介绍数据收集、标注及质量控制流程，使用多种大语言模型进行评测。

Result: 成功构建了涵盖信任、社交性与能力的句子级数据集，并发现现有大语言模型在识别这些维度上的表现具备一定能力但仍有提升空间。

Conclusion: W&C-Sent数据集为自然语言处理与计算社会科学交叉研究提供了新资源，推动了对温暖与能力这两个关键社交心理维度的语言分析研究。

Abstract: Warmth (W) (often further broken down into Trust (T) and Sociability (S)) and Competence (C) are central dimensions along which people evaluate individuals and social groups (Fiske, 2018). While these constructs are well established in social psychology, they are only starting to get attention in NLP research through word-level lexicons, which do not completely capture their contextual expression in larger text units and discourse. In this work, we introduce Warmth and Competence Sentences (W&C-Sent), the first sentence-level dataset annotated for warmth and competence. The dataset includes over 1,600 English sentence--target pairs annotated along three dimensions: trust and sociability (components of warmth), and competence. The sentences in W&C-Sent are from social media and often express attitudes and opinions about specific individuals or social groups (the targets of our annotations). We describe the data collection, annotation, and quality-control procedures in detail, and evaluate a range of large language models (LLMs) on their ability to identify trust, sociability, and competence in text. W&C-Sent provides a new resource for analyzing warmth and competence in language and supports future research at the intersection of NLP and computational social science.

</details>


### [15] [On the Fallacy of Global Token Perplexity in Spoken Language Model Evaluation](https://arxiv.org/abs/2601.06329)
*Jeff Chan-Jan Sju,Liang-Hsuan Tseng,Yi-Cheng Lin,Yen-Chun Kuo,Ju-Chieh Chou,Kai-Wei Chang,Hung-yi Lee,Carlos Busso*

Main category: cs.CL

TL;DR: 该论文提出了一系列更适合评估生成式口语语言模型的方法，替代了传统的全局token困惑度指标，结果显示新指标与人类评价更相关，能更准确反映模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统的全局token困惑度（token perplexity）指标直接套用文本评估方式到语音，忽略了语音和文本的本质差异，导致模型性能被低估，需要提出更合适的评价方法。

Method: 设计了多种基于似然和生成的评估方法，用于替代简单的全局token困惑度，旨在更真实地反映语音生成质量。

Result: 新评估方法与人类主观评价（MOS）相关性更强，评估结果显示口语语言模型的性能差距比传统指标所示更小。

Conclusion: 适当的评估方法是准确衡量生成式口语语言模型进展的关键，新指标能更客观反映模型性能，有利于推动该领域发展。

Abstract: Generative spoken language models pretrained on large-scale raw audio can continue a speech prompt with appropriate content while preserving attributes like speaker and emotion, serving as foundation models for spoken dialogue. In prior literature, these models are often evaluated using ``global token perplexity'', which directly applies the text perplexity formulation to speech tokens. However, this practice overlooks fundamental differences between speech and text modalities, possibly leading to an underestimation of the speech characteristics. In this work, we propose a variety of likelihood- and generative-based evaluation methods that serve in place of naive global token perplexity. We demonstrate that the proposed evaluations more faithfully reflect perceived generation quality, as evidenced by stronger correlations with human-rated mean opinion scores (MOS). When assessed under the new metrics, the relative performance landscape of spoken language models is reshaped, revealing a significantly reduced gap between the best-performing model and the human topline. Together, these results suggest that appropriate evaluation is critical for accurately assessing progress in spoken language modeling.

</details>


### [16] [What Matters When Building Universal Multilingual Named Entity Recognition Models?](https://arxiv.org/abs/2601.06347)
*Jonas Golde,Patrick Haller,Alan Akbik*

Main category: cs.CL

TL;DR: 本文系统评估了多语言命名实体识别模型的关键设计决策，提出并验证了Otter模型，在超过100种语言中表现优异，提升了F1分数且效率更高。


<details>
  <summary>Details</summary>
Motivation: 当前多语言NER模型的设计决策缺乏系统性验证，难以明确哪些因素真正提升性能，阻碍了该领域的进步。

Method: 通过广泛实验分别评估架构、Transformer骨干、训练目标和数据组成的影响，基于实验结果设计了高效且性能优异的Otter模型。

Result: Otter模型在超过100种语言上显著优于现有多语言NER基线，特别是F1分数比GLiNER-x-base提高5.3个百分点，且与大型生成模型性能接近但更加高效。

Conclusion: 系统分析关键设计决策对多语言NER性能的影响有助于模型改进，Otter模型实现了高效且广泛适用的多语言NER，为后续研究提供了可靠资源。

Abstract: Recent progress in universal multilingual named entity recognition (NER) has been driven by advances in multilingual transformer models and task-specific architectures, loss functions, and training datasets. Despite substantial prior work, we find that many critical design decisions for such models are made without systematic justification, with architectural components, training objectives, and data sources evaluated only in combination rather than in isolation. We argue that these decisions impede progress in the field by making it difficult to identify which choices improve model performance. In this work, we conduct extensive experiments around architectures, transformer backbones, training objectives, and data composition across a wide range of languages. Based on these insights, we introduce Otter, a universal multilingual NER model supporting over 100 languages. Otter achieves consistent improvements over strong multilingual NER baselines, outperforming GLiNER-x-base by 5.3pp in F1 and achieves competitive performance compared to large generative models such as Qwen3-32B, while being substantially more efficient. We release model checkpoints, training and evaluation code to facilitate reproducibility and future research.

</details>


### [17] [Average shortest-path length in word-adjacency networks: Chinese versus English](https://arxiv.org/abs/2601.06361)
*Jakub Dec,Michał Dolina,Stanisław Drożdż,Jarosław Kwapień,Jin Liu,Tomasz Stanisz*

Main category: cs.CL

TL;DR: 本文通过分析中英文文学作品中包含标点符号的词邻接网络拓扑结构，研究了平均最短路径长度与网络规模的关系，发现将标点符号视为普通词汇对网络性质影响显著。


<details>
  <summary>Details</summary>
Motivation: 标点符号不仅携带情感信息和逻辑分组功能，还能改善词频统计及作者身份识别，因此将其纳入词网络分析有助于更全面理解文本结构。

Method: 构建含标点符号的成长型词邻接网络，分析不同历史时期及原文与译文的网络规模与平均最短路径长度关系，并用网络模型拟合实验数据。

Result: 实证分析表明，包含标点符号时，中英文作品的平均最短路径长度趋势相似，忽略标点符号时中文路径长度显著增大；模型与实验结果吻合良好。

Conclusion: 标点符号在词邻接网络中扮演重要角色，对分析文本结构尤其是跨语言比较具有积极影响，不应被忽视。

Abstract: Complex networks provide powerful tools for analyzing and understanding the intricate structures present in various systems, including natural language. Here, we analyze topology of growing word-adjacency networks constructed from Chinese and English literary works written in different periods. Unconventionally, instead of considering dictionary words only, we also include punctuation marks as if they were ordinary words. Our approach is based on two arguments: (1) punctuation carries genuine information related to emotional state, allows for logical grouping of content, provides a pause in reading, and facilitates understanding by avoiding ambiguity, and (2) our previous works have shown that punctuation marks behave like words in a Zipfian analysis and, if considered together with regular words, can improve authorship attribution in stylometric studies. We focus on a functional dependence of the average shortest path length $L(N)$ on a network size $N$ for different epochs and individual novels in their original language as well as for translations of selected novels into the other language. We approximate the empirical results with a growing network model and obtain satisfactory agreement between the two. We also observe that $L(N)$ behaves asymptotically similar for both languages if punctuation marks are included but becomes sizably larger for Chinese if punctuation marks are neglected.

</details>


### [18] [Talking to Extraordinary Objects: Folktales Offer Analogies for Interacting with Technology](https://arxiv.org/abs/2601.06372)
*Martha Larson*

Main category: cs.CL

TL;DR: 本文探讨了民间故事中语言与非人类对象互动的类比，提出从中获得灵感，促进技术中的语音与语言交互，摆脱对拟人化的依赖。


<details>
  <summary>Details</summary>
Motivation: 当前语音与语言技术普遍依赖拟人化设计，但这一做法面临挑战。民间故事中广泛存在与非人类对象交流的现象，提供了新的思考视角。

Method: 通过综述民间故事中语言与非人类、奇异物体互动的例子，分析其多样性和记忆点，探讨语言能力与人性的不必然联系。

Result: 发现民间故事展示了多样且令人难忘的奇异物体与语言互动案例，表明语言能力可独立于人类形象存在。

Conclusion: 借鉴民间故事中的语言非拟人化模式，有助于启发和改进技术中语音与语言的交互设计，推动更广泛和多元的交流方式。

Abstract: Speech and language are valuable for interacting with technology. It would be ideal to be able to decouple their use from anthropomorphization, which has recently met an important moment of reckoning. In the world of folktales, language is everywhere and talking to extraordinary objects is not unusual. This overview presents examples of the analogies that folktales offer. Extraordinary objects in folktales are diverse and also memorable. Language capacity and intelligence are not always connected to humanness. Consideration of folktales can offer inspiration and insight for using speech and language for interacting with technology.

</details>


### [19] [AfriqueLLM: How Data Mixing and Model Architecture Impact Continued Pre-training for African Languages](https://arxiv.org/abs/2601.06395)
*Hao Yu,Tianyi Xu,Michael A. Hedderich,Wassim Hamidouche,Syed Waqas Zamir,David Ifeoluwa Adelani*

Main category: cs.CL

TL;DR: 本文提出了AfriqueLLM，一套通过继续训练针对20种非洲语言适配的开放大语言模型，在包括数学推理等复杂任务上取得显著进步。通过系统分析训练数据组成和模型架构，发现数据组合对性能提升至关重要，尤其是数学、代码和合成翻译数据。


<details>
  <summary>Details</summary>
Motivation: 开放式多语言大模型在非洲语言表现较差，且低资源语言语料覆盖不均和缺乏任务相关知识限制了复杂任务能力提升。

Method: 在多个基础模型上进行持续预训练，采用不同的数据组合（数学、代码、合成翻译数据），进行大规模实验并在多语言基准上评测模型性能。

Result: 数据组合是影响持续预训练效果的主要因素，特别是加入数学、代码和合成翻译数据带来一致性提升。较大模型在固定架构内性能提升明显，但不同架构比较时架构影响更大。多语言基础模型性能难以预测后续表现，任务相关数据和稳健架构更关键。

Conclusion: AfriqueLLM通过优化训练数据组合和架构选择，有效提升了非洲语言的多语言模型表现，尤其在数学推理和长上下文理解等复杂任务上表现突出，模型已公开发布。

Abstract: Large language models (LLMs) are increasingly multilingual, yet open models continue to underperform relative to proprietary systems, with the gap most pronounced for African languages. Continued pre-training (CPT) offers a practical route to language adaptation, but improvements on demanding capabilities such as mathematical reasoning often remain limited. This limitation is driven in part by the uneven domain coverage and missing task-relevant knowledge that characterize many low-resource language corpora. We present \texttt{AfriqueLLM}, a suite of open LLMs adapted to 20 African languages through CPT on 26B tokens. We perform a comprehensive empirical study across five base models spanning sizes and architectures, including Llama 3.1, Gemma 3, and Qwen 3, and systematically analyze how CPT data composition shapes downstream performance. In particular, we vary mixtures that include math, code, and synthetic translated data, and evaluate the resulting models on a range of multilingual benchmarks. Our results identify data composition as the primary driver of CPT gains. Adding math, code, and synthetic translated data yields consistent improvements, including on reasoning-oriented evaluations. Within a fixed architecture, larger models typically improve performance, but architectural choices dominate scale when comparing across model families. Moreover, strong multilingual performance in the base model does not reliably predict post-CPT outcomes; robust architectures coupled with task-aligned data provide a more dependable recipe. Finally, our best models improve long-context performance, including document-level translation. Models have been released on [Huggingface](https://huggingface.co/collections/McGill-NLP/afriquellm).

</details>


### [20] [MITRA: A Large-Scale Parallel Corpus and Multilingual Pretrained Language Model for Machine Translation and Semantic Retrieval for Pāli, Sanskrit, Buddhist Chinese, and Tibetan](https://arxiv.org/abs/2601.06400)
*Sebastian Nehrdich,Kurt Keutzer*

Main category: cs.CL

TL;DR: 本文提出了MITRA框架，涵盖多语言平行段落挖掘管线、1.74百万句子对大型语料库及专用预训练语言模型Gemma 2 MITRA，显著提升了梵文、中文和藏文到英文的机器翻译及语义嵌入性能。


<details>
  <summary>Details</summary>
Motivation: 古代佛教文献中多语言之间存在大量但未注释的文本平行内容，手动分析极为困难，因此需要自动化、多语言的文本平行挖掘与处理工具。

Method: 提出MITRA-parallel管线进行多语言平行段落挖掘，构建大型跨语言平行语料库，并开发针对佛教领域的预训练语言模型Gemma 2 MITRA及其微调的机器翻译模型和语义嵌入模型。

Result: 所开发的模型在梵文、中文、藏文到英文的机器翻译任务上达到最新性能，优于更大型的开源模型；语义嵌入模型在新设计的详细语义评测基准上表现优异。

Conclusion: 通过公开发布平行语料库、模型权重和评测基准，MITRA框架不仅推动了多语言佛教文献的NLP研究，还促进了佛教及古典亚州文学的文献学研究。

Abstract: Ancient Buddhist literature features frequent, yet often unannotated, textual parallels spread across diverse languages: Sanskrit, Pāli, Buddhist Chinese, Tibetan, and more. The scale of this material makes manual examination prohibitive. We present the MITRA framework, which consists of a novel pipeline for multilingual parallel passage mining, MITRA-parallel, a large-scale corpus of 1.74 million parallel sentence pairs between Sanskrit, Chinese, and Tibetan, and the development of the domain-specific pretrained language model Gemma 2 MITRA. We present Gemma 2 MITRA-MT, a version of this base model fine-tuned on machine translation tasks, reaching state-of-the-art performance for machine translation of these languages into English and outperforming even much larger open-source models. We also present Gemma 2 MITRA-E, a semantic embedding model that shows state-of-the-art performance on a novel, detailed semantic embedding benchmark. We make the parallel dataset, model weights, and semantic similarity benchmark openly available to aid both NLP research and philological studies in Buddhist and classical Asian literature.

</details>


### [21] [Steer Model beyond Assistant: Controlling System Prompt Strength via Contrastive Decoding](https://arxiv.org/abs/2601.06403)
*Yijiang River Dong,Tiancheng Hu,Zheng Hui,Nigel Collier*

Main category: cs.CL

TL;DR: 该论文提出了一种无需训练的新方法——系统提示强度，通过对比目标和默认系统提示的logits，调整行为信号的力度，实现对大语言模型行为的精细控制。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在执行复杂指令时仍难以偏离预设的辅助助手角色，后训练阶段形成的强先验阻碍了模型对冲突指令的响应。

Method: 利用系统提示强度，将提示遵从性作为一个连续可调的变量，通过对比目标和默认系统提示的logits，放大目标角色独特的行为信号，实现动态调控。

Result: 在五个不同基准测试中，方法显著提升模型表现：IFEval严格准确率提升8.5，OffTopicEval拒绝率提升45个百分点，Prompt-Steering可控性提升13%。

Conclusion: 该方法无需重新训练即可控制大语言模型的行为，赋予模型动态调节系统提示强度的能力，提升应用灵活性和表现。

Abstract: Large language models excel at complex instructions yet struggle to deviate from their helpful assistant persona, as post-training instills strong priors that resist conflicting instructions. We introduce system prompt strength, a training-free method that treats prompt adherence as a continuous control. By contrasting logits from target and default system prompts, we isolate and amplify the behavioral signal unique to the target persona by a scalar factor alpha. Across five diverse benchmarks spanning constraint satisfaction, behavioral control, pluralistic alignment, capability modulation, and stylistic control, our method yields substantial improvements: up to +8.5 strict accuracy on IFEval, +45pp refusal rate on OffTopicEval, and +13% steerability on Prompt-Steering. Our approach enables practitioners to modulate system prompt strength, providing dynamic control over model behavior without retraining.

</details>


### [22] [Value of Information: A Framework for Human-Agent Communication](https://arxiv.org/abs/2601.06407)
*Yijiang River Dong,Tiancheng Hu,Zheng Hui,Caiqi Zhang,Ivan Vulić,Andreea Bobu,Nigel Collier*

Main category: cs.CL

TL;DR: 本文提出了一种基于信息价值的决策框架，帮助大语言模型代理在用户请求不明确时动态决定是否提问。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型代理面临用户请求不明确的困境，需要在基于不完整信息行动和打断用户澄清之间做出权衡。

Method: 引入信息价值（VoI）理论，计算提问带来的预期效用提升与用户认知负担的权衡，无需超参数调节，适应多种应用场景。

Result: 在20问游戏、医疗诊断、航班预订和电子商务四个领域实验中，提出的方法性能优于或匹配了最佳手动调参基线，高成本场景下提升了1.36效用点。

Conclusion: 该框架实现了一个无参数、适应性强的代理交流机制，平衡任务风险、查询模糊性和用户努力，提升了大语言模型代理的实用性。

Abstract: Large Language Model (LLM) agents deployed for real-world tasks face a fundamental dilemma: user requests are underspecified, yet agents must decide whether to act on incomplete information or interrupt users for clarification. Existing approaches either rely on brittle confidence thresholds that require task-specific tuning, or fail to account for the varying stakes of different decisions. We introduce a decision-theoretic framework that resolves this trade-off through the Value of Information (VoI), enabling agents to dynamically weigh the expected utility gain from asking questions against the cognitive cost imposed on users. Our inference-time method requires no hyperparameter tuning and adapts seamlessly across contexts-from casual games to medical diagnosis. Experiments across four diverse domains (20 Questions, medical diagnosis, flight booking, and e-commerce) show that VoI consistently matches or exceeds the best manually-tuned baselines, achieving up to 1.36 utility points higher in high-cost settings. This work provides a parameter-free framework for adaptive agent communication that explicitly balances task risk, query ambiguity, and user effort.

</details>


### [23] [Structured Episodic Event Memory](https://arxiv.org/abs/2601.06411)
*Zhengxuan Lu,Dongfang Li,Yukun Shi,Beilun Wang,Longyue Wang,Baotian Hu*

Main category: cs.CL

TL;DR: 本文提出了结构化的情节事件记忆（SEEM）框架，提升大型语言模型的记忆能力和推理效果。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的记忆机制依赖静态的检索增强生成方法，导致检索分散且无法捕捉复杂推理所需的结构依赖。对于自主智能体而言，这类被动且扁平的结构缺乏模拟长期动态交互的认知组织能力。

Method: 提出基于认知框架理论的层级结构SEEM，将交互流转化为结构化的情节事件框架，结合图记忆层用于关系事实，动态情节记忆层用于叙事情节推进，且引入代理关联融合和逆向来源扩展机制重建连贯叙事。

Result: 在LoCoMo和LongMemEval两个基准测试上，SEEM显著优于现有基线，提升了智能体叙事连贯性和逻辑一致性。

Conclusion: SEEM为大型语言模型提供了一种更具层次性和认知组织的记忆架构，有效解决了复杂推理中结构依赖的捕捉问题，增强了自主智能体的长期交互能力。

Abstract: Current approaches to memory in Large Language Models (LLMs) predominantly rely on static Retrieval-Augmented Generation (RAG), which often results in scattered retrieval and fails to capture the structural dependencies required for complex reasoning. For autonomous agents, these passive and flat architectures lack the cognitive organization necessary to model the dynamic and associative nature of long-term interaction. To address this, we propose Structured Episodic Event Memory (SEEM), a hierarchical framework that synergizes a graph memory layer for relational facts with a dynamic episodic memory layer for narrative progression. Grounded in cognitive frame theory, SEEM transforms interaction streams into structured Episodic Event Frames (EEFs) anchored by precise provenance pointers. Furthermore, we introduce an agentic associative fusion and Reverse Provenance Expansion (RPE) mechanism to reconstruct coherent narrative contexts from fragmented evidence. Experimental results on the LoCoMo and LongMemEval benchmarks demonstrate that SEEM significantly outperforms baselines, enabling agents to maintain superior narrative coherence and logical consistency.

</details>


### [24] [Can a Unimodal Language Agent Provide Preferences to Tune a Multimodal Vision-Language Model?](https://arxiv.org/abs/2601.06424)
*Sazia Tabasum Mim,Jack Morris,Manish Dhakal,Yanming Xiu,Maria Gorlatova,Yi Ding*

Main category: cs.CL

TL;DR: 本论文提出了一种让单模态大语言模型(LLM)通过反馈优化多模态模型(VLM)文本生成的方法，从而提升多模态理解能力，实验结果显示该方法可以显著提高描述准确率并与人类判断高度一致。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型扩展受到规模限制，探讨能否让单模态LLM依据自身需求对多模态模型进行反馈优化，提升整体性能。

Method: 设计一个语言代理机制，使单模态LLM对视觉-语言模型生成的文本进行偏好反馈，指导VLM调整文本生成以适应LLM偏好。

Result: 实验表明LLM对VLM的反馈显著增强了VLM的描述能力，准确率最高提升13%，且在人类评价中反馈偏好与人类选择达64.6%的高度一致率。

Conclusion: 所提方法有效利用单模态模型反馈优化多模态模型生成，提升了多模态场景理解性能，验证了单模态LLM参与多模态推理与生成的新可能，同时揭示了该方法的工作原理与局限性。

Abstract: To explore a more scalable path for adding multimodal capabilities to existing LLMs, this paper addresses a fundamental question: Can a unimodal LLM, relying solely on text, reason about its own informational needs and provide effective feedback to optimize a multimodal model? To answer this, we propose a method that enables a language agent to give feedback to a vision-language model (VLM) to adapt text generation to the agent's preferences. Our results from different experiments affirm this hypothesis, showing that LLM preference feedback significantly enhances VLM descriptions. Using our proposed method, we find that the VLM can generate multimodal scene descriptions to help the LLM better understand multimodal context, leading to improvements of maximum 13% in absolute accuracy compared to the baseline multimodal approach. Furthermore, a human study validated our AI-driven feedback, showing a 64.6% preference alignment rate between the LLM's choices and human judgments. Extensive experiments provide insights on how and why the method works and its limitations.

</details>


### [25] [NC-Bench: An LLM Benchmark for Evaluating Conversational Competence](https://arxiv.org/abs/2601.06426)
*Robert J. Moore,Sungeun An,Farhan Ahmed,Jay Pankaj Gala*

Main category: cs.CL

TL;DR: NC-Bench是一个新的对大型语言模型进行自然会话能力评估的基准，侧重对话的形式和结构，而非内容。它包括基础会话能力、检索增强生成和复杂请求三套测试。实验表明模型在基础回答上表现良好，但修复和复杂多轮请求难度较大。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要关注模型输出内容，缺乏对自然对话形式和结构的系统测试。作者旨在提出一个基于人类对话基本原则，轻量且可扩展的评估框架，以更全面地衡量模型的会话能力。

Method: 基于IBM自然会话框架，设计三套测试集，分别检测基础序列管理、检索增强生成的序列管理及复杂请求的高级序列管理能力。通过14种交互模式测试不同模型的上下文适应性和对话行为生成能力。

Result: 对6个开源模型测试显示：模型在基础回答任务表现较好，修复任务（特别是重复）表现较弱，关闭对话任务表现不一，复杂多轮请求最具挑战性。Qwen模型在基础集表现优异，Granite模型在其他两集表现更好。

Conclusion: NC-Bench通过实现人类对话基本原则，提供了一个理论支持且轻量级的对话能力评估框架，有助于推动大语言模型在自然对话能力上的提升，超越传统基于内容的话题或任务型基准测试。

Abstract: The Natural Conversation Benchmark (NC-Bench) introduce a new approach to evaluating the general conversational competence of large language models (LLMs). Unlike prior benchmarks that focus on the content of model behavior, NC-Bench focuses on the form and structure of natural conversation. Grounded in the IBM Natural Conversation Framework (NCF), NC-Bench comprises three distinct sets. The Basic Conversation Competence set evaluates fundamental sequence management practices, such as answering inquiries, repairing responses, and closing conversational pairs. The RAG set applies the same sequence management patterns as the first set but incorporates retrieval-augmented generation (RAG). The Complex Request set extends the evaluation to complex requests involving more intricate sequence management patterns. Each benchmark tests a model's ability to produce contextually appropriate conversational actions in response to characteristic interaction patterns. Initial evaluations across 6 open-source models and 14 interaction patterns show that models perform well on basic answering tasks, struggle more with repair tasks (especially repeat), have mixed performance on closing sequences, and find complex multi-turn requests most challenging, with Qwen models excelling on the Basic set and Granite models on the RAG set and the Complex Request set. By operationalizing fundamental principles of human conversation, NC-Bench provides a lightweight, extensible, and theory-grounded framework for assessing and improving the conversational abilities of LLMs beyond topical or task-specific benchmarks.

</details>


### [26] [Time Travel Engine: A Shared Latent Chronological Manifold Enables Historical Navigation in Large Language Models](https://arxiv.org/abs/2601.06437)
*Jingmin An,Wei Liu,Qian Wang,Fang Fang*

Main category: cs.CL

TL;DR: 本论文揭示了大型语言模型潜在空间中时间信息的连续几何结构，提出了时间旅行引擎(TTE)框架，实现了基于时间的语义调控和跨语言时间子空间的同构性发现。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型如何编码时间的机制，解决时间信息表示模糊、难以控制的问题。

Method: 提出时间旅行引擎(TTE)，通过调节潜在表示中的时间子流，将语言模型的历史语言模式映射到共享的时间流形上，实现连续的时间演化模拟。

Result: TTE能诱导符合不同时代风格的语言变化，限制未来知识访问，且不同语言的时间子空间具备拓扑同构性，显示了历史进化的通用几何逻辑。

Conclusion: 本研究连接了历史语言学与机制可解释性，提出了在神经网络中控制时间推理的新范式，丰富了时间认知与语言模型的理解。

Abstract: Time functions as a fundamental dimension of human cognition, yet the mechanisms by which Large Language Models (LLMs) encode chronological progression remain opaque. We demonstrate that temporal information in their latent space is organized not as discrete clusters but as a continuous, traversable geometry. We introduce the Time Travel Engine (TTE), an interpretability-driven framework that projects diachronic linguistic patterns onto a shared chronological manifold. Unlike surface-level prompting, TTE directly modulates latent representations to induce coherent stylistic, lexical, and conceptual shifts aligned with target eras. By parameterizing diachronic evolution as a continuous manifold within the residual stream, TTE enables fluid navigation through period-specific "zeitgeists" while restricting access to future knowledge. Furthermore, experiments across diverse architectures reveal topological isomorphism between the temporal subspaces of Chinese and English-indicating that distinct languages share a universal geometric logic of historical evolution. These findings bridge historical linguistics with mechanistic interpretability, offering a novel paradigm for controlling temporal reasoning in neural networks.

</details>


### [27] [LitVISTA: A Benchmark for Narrative Orchestration in Literary Text](https://arxiv.org/abs/2601.06445)
*Mingzhe Lu,Yiwen Wang,Yanbing Liu,Qi You,Chong Liu,Ruize Qin,Haoyu Dong,Wenyu Zhang,Jiarui Zhang,Yue Hu,Yunpeng Li*

Main category: cs.CL

TL;DR: 计算叙事分析旨在捕捉文学文本中的节奏和情感动态，但现有大语言模型过于关注因果连贯，忽略复杂故事结构。本文提出了VISTA空间框架和LitVISTA基准，以统一和评估模型与人类叙事视角，发现当前模型在构建全局叙事视角和理解叙事功能结构方面存在系统性缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型生成故事时倾向于因果连贯，忽视复杂的故事弧线和叙事结构，导致模型生成叙事与人类叙事在结构上存在错位。

Method: 提出VISTA Space作为高维叙事结构表现框架，统一人类与模型叙事视角；构建LitVISTA结构化注释基准用于系统评估模型的叙事结构能力；在多款前沿LLM（GPT、Claude、Grok、Gemini）上进行专家评估。

Result: 实验结果表明当前主流模型无法建立统一的全局叙事视角，难以同时把握叙事功能与叙事结构；即使采用先进思考模式，对文学叙事理解的提升仍然有限。

Conclusion: 现有大语言模型在捕捉文学叙事的节奏与结构方面存在明显不足，本文提出的方法和基准可促进后续研究改进模型的叙事能力。

Abstract: Computational narrative analysis aims to capture rhythm, tension, and emotional dynamics in literary texts. Existing large language models can generate long stories but overly focus on causal coherence, neglecting the complex story arcs and orchestration inherent in human narratives. This creates a structural misalignment between model- and human-generated narratives. We propose VISTA Space, a high-dimensional representational framework for narrative orchestration that unifies human and model narrative perspectives. We further introduce LitVISTA, a structurally annotated benchmark grounded in literary texts, enabling systematic evaluation of models' narrative orchestration capabilities. We conduct oracle evaluations on a diverse selection of frontier LLMs, including GPT, Claude, Grok, and Gemini. Results reveal systematic deficiencies: existing models fail to construct a unified global narrative view, struggling to jointly capture narrative function and structure. Furthermore, even advanced thinking modes yield only limited gains for such literary narrative understanding.

</details>


### [28] [PRISP: Privacy-Safe Few-Shot Personalization via Lightweight Adaptation](https://arxiv.org/abs/2601.06471)
*Junho Park,Dohoon Kim,Taesup Moon*

Main category: cs.CL

TL;DR: PRISP是一种轻量且隐私安全的大型语言模型个性化框架，适用于数据有限、计算资源受限且隐私要求严格的场景。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型个性化方法通常需要大量数据和资源，且可能带来隐私风险，而实际应用中通常面临用户数据极其有限、计算资源受限及严格隐私保护的约束。

Method: 提出PRISP框架，利用Text-to-LoRA超网络根据任务描述生成任务感知的LoRA参数，通过优化少量参数和额外模块实现高效的少样本用户个性化。

Result: 在LaMP基准的少样本变体上，PRISP在性能上优于现有方法，同时显著降低计算开销并消除隐私风险。

Conclusion: PRISP有效解决了现实条件下大型语言模型个性化中数据不足、计算资源有限及隐私保护的挑战，兼顾性能和安全性。

Abstract: Large language model (LLM) personalization aims to adapt general-purpose models to individual users. Most existing methods, however, are developed under data-rich and resource-abundant settings, often incurring privacy risks. In contrast, realistic personalization typically occurs after deployment under (i) extremely limited user data, (ii) constrained computational resources, and (iii) strict privacy requirements. We propose PRISP, a lightweight and privacy-safe personalization framework tailored to these constraints. PRISP leverages a Text-to-LoRA hypernetwork to generate task-aware LoRA parameters from task descriptions, and enables efficient user personalization by optimizing a small subset of task-aware LoRA parameters together with minimal additional modules using few-shot user data. Experiments on a few-shot variant of the LaMP benchmark demonstrate that PRISP achieves strong overall performance compared to prior approaches, while reducing computational overhead and eliminating privacy risks.

</details>


### [29] [IndRegBias: A Dataset for Studying Indian Regional Biases in English and Code-Mixed Social Media Comments](https://arxiv.org/abs/2601.06477)
*Debasmita Panda,Akash Anil,Neelesh Kumar Shukla*

Main category: cs.CL

TL;DR: 本文构建了一个印度地区偏见数据集IndRegBias，并设计多层次标注策略，实现对社交媒体评论中的地区偏见及其严重性的检测。


<details>
  <summary>Details</summary>
Motivation: 当前NLP研究较少关注地区偏见，原因包括数据难以提取、标注存在主观偏见及地区偏见常被忽视。

Method: 收集25,000条Reddit和YouTube评论，提出多层级标注策略，并使用大规模语言模型和印度本地语言模型，基于零样本、少样本和微调策略进行偏见检测。

Result: 零样本和少样本方法准确率较低，微调策略显著提升了模型识别印度地区偏见及其严重程度的性能。

Conclusion: 通过建立IndRegBias数据集及微调模型，有效促进了印度地区偏见的检测与理解，为相关NLP研究提供资源和方法支持。

Abstract: Warning: This paper consists of examples representing regional biases in Indian regions that might be offensive towards a particular region. While social biases corresponding to gender, race, socio-economic conditions, etc., have been extensively studied in the major applications of Natural Language Processing (NLP), biases corresponding to regions have garnered less attention. This is mainly because of (i) difficulty in the extraction of regional bias datasets, (ii) disagreements in annotation due to inherent human biases, and (iii) regional biases being studied in combination with other types of social biases and often being under-represented. This paper focuses on creating a dataset IndRegBias, consisting of regional biases in an Indian context reflected in users' comments on popular social media platforms, namely Reddit and YouTube. We carefully selected 25,000 comments appearing on various threads in Reddit and videos on YouTube discussing trending topics on regional issues in India. Furthermore, we propose a multilevel annotation strategy to annotate the comments describing the severity of regional biased statements. To detect the presence of regional bias and its severity in IndRegBias, we evaluate open-source Large Language Models (LLMs) and Indic Language Models (ILMs) using zero-shot, few-shot, and fine-tuning strategies. We observe that zero-shot and few-shot approaches show lower accuracy in detecting regional biases and severity in the majority of the LLMs and ILMs. However, the fine-tuning approach significantly enhances the performance of the LLM in detecting Indian regional bias along with its severity.

</details>


### [30] [Spec-o3: A Tool-Augmented Vision-Language Agent for Rare Celestial Object Candidate Vetting via Automated Spectral Inspection](https://arxiv.org/abs/2601.06498)
*Minghui Jia,Qichao Zhang,Ali Luo,Linjing Li,Shuo Ye,Hailing Lu,Wen Hou,Dongbin Zhao*

Main category: cs.CL

TL;DR: 本文提出Spec-o3，一种结合视觉语言的专家辅助光谱检验工具，通过多模态推理实现罕见天体识别，显著提升准确率并具备跨数据集泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习分类器泛化和可解释性有限，天文学家仍依赖人工视觉检查光谱，效率低下且难以应对现代大规模光谱数据。

Method: 设计了Spec-o3，一种集成视觉语言的多模态链式思维代理，采用两阶段训练：专家轨迹监督微调和基于结果的强化学习。

Result: 在LAMOST数据集的五个罕见天体识别任务上，Spec-o3将macro-F1得分从28.3提高到76.5，优于现有视觉语言模型和深度学习模型，并能有效泛化至SDSS和DESI数据。

Conclusion: Spec-o3不仅实现了高性能的罕见天体自动识别，还保证了推理过程的物理一致性和可解释性，为天文学光谱分析提供了透明可信的自动化工具。

Abstract: Due to the limited generalization and interpretability of deep learning classifiers, The final vetting of rare celestial object candidates still relies on expert visual inspection--a manually intensive process. In this process, astronomers leverage specialized tools to analyze spectra and construct reliable catalogs. However, this practice has become the primary bottleneck, as it is fundamentally incapable of scaling with the data deluge from modern spectroscopic surveys. To bridge this gap, we propose Spec-o3, a tool-augmented vision-language agent that performs astronomer-aligned spectral inspection via interleaved multimodal chain-of-thought reasoning. Spec-o3 is trained with a two-stage post-training recipe: cold-start supervised fine-tuning on expert inspection trajectories followed by outcome-based reinforcement learning on rare-type verification tasks. Evaluated on five rare-object identification tasks from LAMOST, Spec-o3 establishes a new State-of-the-Art, boosting the macro-F1 score from 28.3 to 76.5 with a 7B parameter base model and outperforming both proprietary VLMs and specialized deep models. Crucially, the agent demonstrates strong generalization to unseen inspection tasks across survey shifts (from LAMOST to SDSS/DESI). Expert evaluations confirm that its reasoning traces are coherent and physically consistent, supporting transparent and trustworthy decision-making. Code, data, and models are available at \href{https://github.com/Maxwell-Jia/spec-o3}{Project HomePage}.

</details>


### [31] [MedRAGChecker: Claim-Level Verification for Biomedical Retrieval-Augmented Generation](https://arxiv.org/abs/2601.06519)
*Yuelyu Ji,Min Gu Kwak,Hang Zhang,Xizhi Wu,Chenyu Li,Yanshan Wang*

Main category: cs.CL

TL;DR: 本文提出了MedRAGChecker框架，用于对生物医学领域检索增强生成系统的答案进行逐条声明级别的验证和诊断，有效识别不支持或矛盾的说法，提高生成内容的安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 生物医学检索增强生成系统虽然能基于医学文献回答问题，但长篇回答中常含有孤立的不支持或矛盾声明，存在安全隐患，亟需细粒度的验证工具。

Method: MedRAGChecker将生成答案分解为原子声明，结合基于证据的自然语言推理和生物医学知识图谱一致性信号，评估每条声明的支持度，并通过集成验证器和类别特定权重，实现可扩展的自动化评估。

Result: 在四个生物医学问答基准测试中，MedRAGChecker准确标记不支持和矛盾声明，揭示不同生成器在安全关键生物医学关系上的风险差异。

Conclusion: MedRAGChecker有效提升了生物医学RAG系统回答的可信度和安全性，为后续生成系统的优化提供了诊断依据。

Abstract: Biomedical retrieval-augmented generation (RAG) can ground LLM answers in medical literature, yet long-form outputs often contain isolated unsupported or contradictory claims with safety implications.
  We introduce MedRAGChecker, a claim-level verification and diagnostic framework for biomedical RAG.
  Given a question, retrieved evidence, and a generated answer, MedRAGChecker decomposes the answer into atomic claims and estimates claim support by combining evidence-grounded natural language inference (NLI) with biomedical knowledge-graph (KG) consistency signals.
  Aggregating claim decisions yields answer-level diagnostics that help disentangle retrieval and generation failures, including faithfulness, under-evidence, contradiction, and safety-critical error rates.
  To enable scalable evaluation, we distill the pipeline into compact biomedical models and use an ensemble verifier with class-specific reliability weighting.
  Experiments on four biomedical QA benchmarks show that MedRAGChecker reliably flags unsupported and contradicted claims and reveals distinct risk profiles across generators, particularly on safety-critical biomedical relations.

</details>


### [32] [Atomic-SNLI: Fine-Grained Natural Language Inference through Atomic Fact Decomposition](https://arxiv.org/abs/2601.06528)
*Minghui Huang*

Main category: cs.CL

TL;DR: 本文针对自然语言推理（NLI）系统在原子级推理中的不足，提出了Atomic-SNLI数据集，通过细粒度事实的分解和丰富训练，实现了推理性能和可解释性的提升。


<details>
  <summary>Details</summary>
Motivation: 当前NLI系统主要基于句子级推理，缺乏解释能力，且原子级推理效果较差，传统假设认为只有所有原子事实都被推断，假设才被支持不符合实际。

Method: 将SNLI数据集分解成原子事实，采用语言学知识生成并丰富原子级推理例子，构建了新的Atomic-SNLI数据集，并在此基础上对模型进行微调。

Result: 在Atomic-SNLI数据集上微调的模型在原子级推理能力上显著提升，同时保持了句子级推理性能。

Conclusion: Atomic-SNLI有效提升了模型的细粒度推理能力，支持更准确、透明且具解释性的推理结果。

Abstract: Current Natural Language Inference (NLI) systems primarily operate at the sentence level, providing black-box decisions that lack explanatory power. While atomic-level NLI offers a promising alternative by decomposing hypotheses into individual facts, we demonstrate that the conventional assumption that a hypothesis is entailed only when all its atomic facts are entailed fails in practice due to models' poor performance on fine-grained reasoning. Our analysis reveals that existing models perform substantially worse on atomic level inference compared to sentence level tasks. To address this limitation, we introduce Atomic-SNLI, a novel dataset constructed by decomposing SNLI and enriching it with carefully curated atomic level examples through linguistically informed generation strategies. Experimental results demonstrate that models fine-tuned on Atomic-SNLI achieve significant improvements in atomic reasoning capabilities while maintaining strong sentence level performance, enabling both accurate judgements and transparent, explainable results at the fact level.

</details>


### [33] [Exposía: Academic Writing Assessment of Exposés and Peer Feedback](https://arxiv.org/abs/2601.06536)
*Dennis Zyska,Alla Rozovskaya,Ilia Kuznetsov,Iryna Gurevych*

Main category: cs.CL

TL;DR: Exposía是首个连接学术写作和反馈评估的公开数据集，包含学生项目提案及同行和教师反馈，支持学术写作评价的研究。该数据集用于测试大型语言模型对写作和评论的自动评分，发现模型在简单评分维度表现好，在内容评估上表现较差，与人类教师评分有较好一致性。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏公开、结合写作和反馈评价的高等教育数据集，限制了学术写作自动评价和反馈机制的研究与应用。

Method: 构建包含多阶段写作过程及评分的人类标注数据集Exposía，并使用该数据集对开源大型语言模型进行自动评分任务的性能评测。

Result: 大型语言模型在无专业知识要求的评分维度表现优异，内容相关维度表现下降，且更倾向于与给予高评分的教师一致。多维度联合评分的提示策略效果最佳。

Conclusion: Exposía为学术写作评价研究提供了关键资源，验证了多任务联合评分对提升模型表现的重要性，对课堂实际应用具有指导价值。

Abstract: We present Exposía, the first public dataset that connects writing and feedback assessment in higher education, enabling research on educationally grounded approaches to academic writing evaluation. Exposía includes student research project proposals and peer and instructor feedback consisting of comments and free-text reviews. The dataset was collected in the "Introduction to Scientific Work" course of the Computer Science undergraduate program that focuses on teaching academic writing skills and providing peer feedback on academic writing. Exposía reflects the multi-stage nature of the academic writing process that includes drafting, providing and receiving feedback, and revising the writing based on the feedback received. Both the project proposals and peer feedback are accompanied by human assessment scores based on a fine-grained, pedagogically-grounded schema for writing and feedback assessment that we develop.
  We use Exposía to benchmark state-of-the-art open-source large language models (LLMs) for two tasks: automated scoring of (1) the proposals and (2) the student reviews. The strongest LLMs attain high agreement on scoring aspects that require little domain knowledge but degrade on dimensions evaluating content, in line with human agreement values. We find that LLMs align better with the human instructors giving high scores. Finally, we establish that a prompting strategy that scores multiple aspects of the writing together is the most effective, an important finding for classroom deployment.

</details>


### [34] [SimLLM: Fine-Tuning Code LLMs for SimPy-Based Queueing System Simulation](https://arxiv.org/abs/2601.06543)
*Jun-Qi Chen,Kun Zhang,Rui Zheng,Ying Zhong*

Main category: cs.CL

TL;DR: 本文提出了一种多阶段微调框架，通过对开源大语言模型Qwen-Coder-7B和DeepSeek-Coder-6.7B在SimPy排队系统仿真代码上的微调，显著提升了代码的可执行性和一致性，提供了闭源模型的实用替代方案。


<details>
  <summary>Details</summary>
Motivation: 现有闭源大语言模型在生成SimPy排队仿真代码方面存在高计算成本和数据隐私问题，需寻找高效且安全的替代方案。

Method: 采用多阶段微调框架，包括两阶段监督微调（SFT）和一阶段直接偏好优化（DPO），对Qwen-Coder-7B和DeepSeek-Coder-6.7B进行领域特定的微调。

Result: 微调后的模型在代码可执行性、输出格式合规性和指令代码一致性方面均有显著提升，验证了方法的有效性。

Conclusion: 领域特定的微调能够将开源小型编码模型转变为可靠的SimPy仿真代码生成器，成为闭源模型在教育、科研和决策支持中的实用替代品。

Abstract: The Python package SimPy is widely used for modeling queueing systems due to its flexibility, simplicity, and smooth integration with modern data analysis and optimization frameworks. Recent advances in large language models (LLMs) have shown strong ability in generating clear and executable code, making them powerful and suitable tools for writing SimPy queueing simulation code. However, directly employing closed-source models like GPT-4o to generate such code may lead to high computational costs and raise data privacy concerns. To address this, we fine-tune two open-source LLMs, Qwen-Coder-7B and DeepSeek-Coder-6.7B, on curated SimPy queueing data, which enhances their code-generating performance in executability, output-format compliance, and instruction-code consistency. Particularly, we proposed a multi-stage fine-tuning framework comprising two stages of supervised fine-tuning (SFT) and one stage of direct preference optimization (DPO), progressively enhancing the model's ability in SimPy-based queueing simulation code generation. Extensive evaluations demonstrate that both fine-tuned models achieve substantial improvements in executability, output-format compliance, and instruct consistency. These results confirm that domain-specific fine-tuning can effectively transform compact open-source code models into reliable SimPy simulation generators which provide a practical alternative to closed-source LLMs for education, research, and operational decision support.

</details>


### [35] [CSR-RAG: An Efficient Retrieval System for Text-to-SQL on the Enterprise Scale](https://arxiv.org/abs/2601.06564)
*Rajpreet Singh,Novak Boškov,Lawrence Drabeck,Aditya Gudal,Manzoor A. Khan*

Main category: cs.CL

TL;DR: 本文提出了一种结合上下文、结构与关系检索的新型检索增强生成系统（CSR-RAG），用于提高企业级数据库的文本到SQL转换的检索效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到SQL转换通常要求输入中包含模式描述，但企业级应用在生成SQL之前需要先检索相关表，提升检索效率与准确性是实际需求。

Method: 提出CSR-RAG系统，结合上下文、结构和关系三种检索方式，实现高效且精准的表检索，为后续SQL生成提供支持。

Result: 在大规模企业基准测试中，CSR-RAG实现了最高40%的精确率和超过80%的召回率，查询生成延迟仅为30毫秒，适合现代企业级系统。

Conclusion: CSR-RAG系统在兼顾效率与准确性的同时，为企业级大规模数据库的文本到SQL转换提供了实用解决方案。

Abstract: Natural language to SQL translation (Text-to-SQL) is one of the long-standing problems that has recently benefited from advances in Large Language Models (LLMs). While most academic Text-to-SQL benchmarks request schema description as a part of natural language input, enterprise-scale applications often require table retrieval before SQL query generation. To address this need, we propose a novel hybrid Retrieval Augmented Generation (RAG) system consisting of contextual, structural, and relational retrieval (CSR-RAG) to achieve computationally efficient yet sufficiently accurate retrieval for enterprise-scale databases. Through extensive enterprise benchmarks, we demonstrate that CSR-RAG achieves up to 40% precision and over 80% recall while incurring a negligible average query generation latency of only 30ms on commodity data center hardware, which makes it appropriate for modern LLM-based enterprise-scale systems.

</details>


### [36] [EVM-QuestBench: An Execution-Grounded Benchmark for Natural-Language Transaction Code Generation](https://arxiv.org/abs/2601.06565)
*Pei Yang,Wanyi Chen,Ke Wang,Lynn Ai,Eric Yang,Tianyu Shi*

Main category: cs.CL

TL;DR: 本文提出了EVM-QuestBench，一个基于执行的EVM兼容链上自然语言交易脚本生成评测基准，强调执行准确性和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有评估忽略了执行准确性和安全性，而链上交易错误可能导致不可逆损失，急需准确的执行评测工具。

Method: 设计了动态评测机制，从模板池采样指令，使用预定义数值参数，验证执行结果。包含107个任务，支持模块化快速开发，在分叉的EVM链上运行且支持步骤效率衰减。

Result: 评测了20个模型，发现性能差距显著，单步精确度和多步工作流完成表现不一致。

Conclusion: EVM-QuestBench为链上交易脚本生成提供了全面的执行精度和安全性评测标准，有助推动模型在实际应用中的可靠性。

Abstract: Large language models are increasingly applied to various development scenarios. However, in on-chain transaction scenarios, even a minor error can cause irreversible loss for users. Existing evaluations often overlook execution accuracy and safety. We introduce EVM-QuestBench, an execution-grounded benchmark for natural-language transaction-script generation on EVM-compatible chains. The benchmark employs dynamic evaluation: instructions are sampled from template pools, numeric parameters are drawn from predefined intervals, and validators verify outcomes against these instantiated values. EVM-QuestBench contains 107 tasks (62 atomic, 45 composite). Its modular architecture enables rapid task development. The runner executes scripts on a forked EVM chain with snapshot isolation; composite tasks apply step-efficiency decay. We evaluate 20 models and find large performance gaps, with split scores revealing persistent asymmetry between single-action precision and multi-step workflow completion. Code: https://anonymous.4open.science/r/bsc_quest_bench-A9CF/.

</details>


### [37] [Are Emotions Arranged in a Circle? Geometric Analysis of Emotion Representations via Hyperspherical Contrastive Learning](https://arxiv.org/abs/2601.06575)
*Yusuke Yamauchi,Akiko Aizawa*

Main category: cs.CL

TL;DR: 本文提出通过对比学习在超球面上实现语言模型中情绪表示的圆形结构，验证了其可解释性和维度降低的稳健性，但在高维和细粒度分类中表现不及传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统心理学的情绪环形模型在语言模型表征学习中很少被直接应用，导致其几何效度未被充分研究。

Method: 通过在超球面上应用对比学习，使语言模型的情绪表征呈现圆形结构。

Result: 圆形结构提高了情绪表示的可解释性和在降维下的稳健性，但在高维空间和细粒度情绪分类任务中表现逊色于传统设计。

Conclusion: 将心理学的环形情绪模型应用于深度学习语言模型存在解释性和性能上的权衡，提示需根据具体任务权衡使用。

Abstract: Psychological research has long utilized circumplex models to structure emotions, placing similar emotions adjacently and opposing ones diagonally. Although frequently used to interpret deep learning representations, these models are rarely directly incorporated into the representation learning of language models, leaving their geometric validity unexplored. This paper proposes a method to induce circular emotion representations within language model embeddings via contrastive learning on a hypersphere. We show that while this circular alignment offers superior interpretability and robustness against dimensionality reduction, it underperforms compared to conventional designs in high-dimensional settings and fine-grained classification. Our findings elucidate the trade-offs involved in applying psychological circumplex models to deep learning architectures.

</details>


### [38] [Stylistic Evolution and LLM Neutrality in Singlish Language](https://arxiv.org/abs/2601.06580)
*Linus Tze En Foo,Weihan Angela Ng,Wenkai Li,Lynnette Hui Xian Ng*

Main category: cs.CL

TL;DR: 本研究通过分析十年间新加坡克里奥尔语Singlish的数字文本，揭示其风格在词汇结构、语用和句法上的演变，同时评估大型语言模型在生成Singlish时的表现和局限。


<details>
  <summary>Details</summary>
Motivation: Singlish作为新加坡多语环境中的克里奥尔语，随着社会和技术的变迁不断演化，探究其在数字文本中的时间变化及语言模型对其模拟能力具有重要意义。

Method: 提出一种风格相似度框架，比较词汇结构、语用、心理语言学和编码器特征，量化不同年份Singlish文本的时间变异；并评估多种大型语言模型生成的Singlish文本在时间一致性上的表现。

Result: 发现Singlish在语调、表达力和句法结构上有显著的历时变化；虽然部分大型语言模型能生成表面逼真的Singlish信息，但未能达到时间中性的输出，残留时间信息依然可被检测。

Conclusion: Singlish表现出动态的语言演化特征，当前大型语言模型虽能模拟其部分特征，但在捕捉其社会方言和时间变化方面仍有限。

Abstract: Singlish is a creole rooted in Singapore's multilingual environment and continues to evolve alongside social and technological change. This study investigates the evolution of Singlish over a decade of informal digital text messages. We propose a stylistic similarity framework that compares lexico-structural, pragmatic, psycholinguistic, and encoder-derived features across years to quantify temporal variation. Our analysis reveals notable diachronic changes in tone, expressivity and sentence construction over the years. Conversely, while some LLMs were able to generate superficially realistic Singlish messages, they do not produce temporally neutral outputs, and residual temporal signals remain detectable despite prompting and fine-tuning. Our findings highlight the dynamic evolution of Singlish, as well as the capabilities and limitations of current LLMs in modeling sociolectal and temporal variations in the colloquial language.

</details>


### [39] [Detecting LLM-Generated Text with Performance Guarantees](https://arxiv.org/abs/2601.06586)
*Hongyi Zhou,Jin Zhu,Ying Yang,Chengchun Shi*

Main category: cs.CL

TL;DR: 本文提出了一种无需辅助信息的新型大语言模型文本检测器，能够高效准确地区分人类文本与LLM生成文本，并支持统计推断。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成的高仿真文本带来了假新闻传播、误导报告和学术不端等问题，迫切需要准确检测文本来源。

Method: 训练一个分类器识别文本是人类还是LLM生成，且不依赖水印或具体模型知识，具备统计推断功能，部署于CPU在线平台。

Result: 该分类器较现有检测器准确率更高，能够有效控制一类错误，保持高统计效能和计算效率。

Conclusion: 该检测器在实际应用中表现优异，是解决LLM文本滥用问题的有效工具。

Abstract: Large language models (LLMs) such as GPT, Claude, Gemini, and Grok have been deeply integrated into our daily life. They now support a wide range of tasks -- from dialogue and email drafting to assisting with teaching and coding, serving as search engines, and much more. However, their ability to produce highly human-like text raises serious concerns, including the spread of fake news, the generation of misleading governmental reports, and academic misconduct. To address this practical problem, we train a classifier to determine whether a piece of text is authored by an LLM or a human. Our detector is deployed on an online CPU-based platform https://huggingface.co/spaces/stats-powered-ai/StatDetectLLM, and contains three novelties over existing detectors: (i) it does not rely on auxiliary information, such as watermarks or knowledge of the specific LLM used to generate the text; (ii) it more effectively distinguishes between human- and LLM-authored text; and (iii) it enables statistical inference, which is largely absent in the current literature. Empirically, our classifier achieves higher classification accuracy compared to existing detectors, while maintaining type-I error control, high statistical power, and computational efficiency.

</details>


### [40] [How Context Shapes Truth: Geometric Transformations of Statement-level Truth Representations in LLMs](https://arxiv.org/abs/2601.06599)
*Shivam Adarsh,Maria Maistro,Christina Lioma*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型中“真理向量”在引入上下文时的几何变化规律。


<details>
  <summary>Details</summary>
Motivation: 虽然已有研究揭示了语言模型中编码陈述真实性的真理向量，但其在不同上下文下的变化尚未被探讨。

Method: 通过测量不同模型和数据集上带上下文与不带上下文的真理向量的方向变化角度和向量幅值变化，分析其几何特征。

Result: 发现真理向量在模型不同层表现出方向和幅值的特定变化规律，添加上下文一般会增强真理向量的幅度，较大模型主要通过方向变化区分相关/无关上下文，小模型则通过幅值变化区分。此外，与参数知识冲突的上下文引起更大的几何变化。

Conclusion: 首次提供了上下文如何几何地改变大型语言模型真理向量的系统性分析，有助于理解模型捕捉真实性信息的机制。

Abstract: Large Language Models (LLMs) often encode whether a statement is true as a vector in their residual stream activations. These vectors, also known as truth vectors, have been studied in prior work, however how they change when context is introduced remains unexplored. We study this question by measuring (1) the directional change ($θ$) between the truth vectors with and without context and (2) the relative magnitude of the truth vectors upon adding context. Across four LLMs and four datasets, we find that (1) truth vectors are roughly orthogonal in early layers, converge in middle layers, and may stabilize or continue increasing in later layers; (2) adding context generally increases the truth vector magnitude, i.e., the separation between true and false representations in the activation space is amplified; (3) larger models distinguish relevant from irrelevant context mainly through directional change ($θ$), while smaller models show this distinction through magnitude differences. We also find that context conflicting with parametric knowledge produces larger geometric changes than parametrically aligned context. To the best of our knowledge, this is the first work that provides a geometric characterization of how context transforms the truth vector in the activation space of LLMs.

</details>


### [41] [Probing Multimodal Large Language Models on Cognitive Biases in Chinese Short-Video Misinformation](https://arxiv.org/abs/2601.06600)
*Jen-tse Huang,Chang Chen,Shiyang Lai,Wenxuan Wang,Michelle R. Kaufman,Mark Dredze*

Main category: cs.CL

TL;DR: 本文建立了一个涵盖四个健康领域200个短视频的高质量多模态数据集，用于评估多模态大语言模型（MLLMs）在面对带有认知偏见的虚假信息时的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 短视频平台成为虚假信息的重要传播渠道，虚假内容常利用视觉实验和社交线索误导受众，现有多模态大语言模型在识别这些虚假信息时的稳定性尚未充分研究。

Method: 构建包含三种误导模式、实验错误、逻辑谬误和伪造声明的手工标注数据集，基于权威标准和学术文献进行验证，测试了八种先进的多模态大语言模型在五种模态设置下的表现。

Result: 实验结果显示，Gemini-2.5-Pro模型在多模态环境下表现最佳，信念分数为71.5/100，而o3模型表现最差，为35.2。此外，研究发现模型容易受到如权威频道ID等社交线索的偏见影响。

Conclusion: 多模态大语言模型在面对带有认知偏见的虚假信息时存在显著脆弱性，特别容易受社交线索误导，未来需提升模型的鲁棒性和识别能力。

Abstract: Short-video platforms have become major channels for misinformation, where deceptive claims frequently leverage visual experiments and social cues. While Multimodal Large Language Models (MLLMs) have demonstrated impressive reasoning capabilities, their robustness against misinformation entangled with cognitive biases remains under-explored. In this paper, we introduce a comprehensive evaluation framework using a high-quality, manually annotated dataset of 200 short videos spanning four health domains. This dataset provides fine-grained annotations for three deceptive patterns, experimental errors, logical fallacies, and fabricated claims, each verified by evidence such as national standards and academic literature. We evaluate eight frontier MLLMs across five modality settings. Experimental results demonstrate that Gemini-2.5-Pro achieves the highest performance in the multimodal setting with a belief score of 71.5/100, while o3 performs the worst at 35.2. Furthermore, we investigate social cues that induce false beliefs in videos and find that models are susceptible to biases like authoritative channel IDs.

</details>


### [42] [N2N-GQA: Noise-to-Narrative for Graph-Based Table-Text Question Answering Using LLMs](https://arxiv.org/abs/2601.06603)
*Mohamed Sharafath,Aravindh Annamalai,Ganesh Murugan,Aravindakumar Venugopalan*

Main category: cs.CL

TL;DR: 本文提出了N2N-GQA框架，通过构建带有语义边的证据图来改善多跳混合表格-文本问答的检索与推理效果，显著提升准确率，实现了零样本多跳问答新突破。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成方法将文档视为平面列表，导致检索噪声影响多跳推理链条，缺乏桥连推理步骤的能力，因此需要更结构化的证据组织方法。

Method: 提出了N2N-GQA框架，将检索到的文档作为图节点，利用语义关系构建证据图，通过识别连接推理步骤的桥文档改善多跳推理，支持零样本开放领域混合表格-文本问答。

Result: 在OTT-QA数据集上，基于图的证据筛选较强基线提升19.9点准确率，达到48.80 EM，接近经过微调检索模型和优化系统表现，显示结构化图检索效果显著。

Conclusion: 将证据组织为图结构对多跳混合问答至关重要，N2N-GQA展示了简单直观的图构建能媲美复杂微调方法，为零样本多跳问答系统提供了新方向。

Abstract: Multi-hop question answering over hybrid table-text data requires retrieving and reasoning across multiple evidence pieces from large corpora, but standard Retrieval-Augmented Generation (RAG) pipelines process documents as flat ranked lists, causing retrieval noise to obscure reasoning chains. We introduce N2N-GQA. To our knowledge, it is the first zeroshot framework for open-domain hybrid table-text QA that constructs dynamic evidence graphs from noisy retrieval outputs. Our key insight is that multi-hop reasoning requires understanding relationships between evidence pieces: by modeling documents as graph nodes with semantic relationships as edges, we identify bridge documents connecting reasoning steps, a capability absent in list-based retrieval. On OTT-QA, graph-based evidence curation provides a 19.9-point EM improvement over strong baselines, demonstrating that organizing retrieval results as structured graphs is critical for multihop reasoning. N2N-GQA achieves 48.80 EM, matching finetuned retrieval models (CORE: 49.0 EM) and approaching heavily optimized systems (COS: 56.9 EM) without any task specific training. This establishes graph-structured evidence organization as essential for scalable, zero-shot multi-hop QA systems and demonstrates that simple, interpretable graph construction can rival sophisticated fine-tuned approaches.

</details>


### [43] [Pragya: An AI-Based Semantic Recommendation System for Sanskrit Subhasitas](https://arxiv.org/abs/2601.06607)
*Tanisha Raorane,Prasenjit Kole*

Main category: cs.CL

TL;DR: 本文提出了Pragya，一个结合检索增强生成的框架，用于梵文Subhasitas的语义推荐，通过语义检索和生成模型提升内容的可访问性。


<details>
  <summary>Details</summary>
Motivation: 梵文Subhasitas蕴含丰富文化哲学智慧，但因语言和语境障碍在数字时代未被充分利用。

Method: 构建带主题标签的数据集，利用IndicBERT进行语义检索，结合Mistral大语言模型生成音译、翻译及语境解释。

Result: 实验显示语义检索在精确度和相关性上优于关键词匹配，用户研究表明生成摘要提升了内容可访问性。

Conclusion: 首次尝试将检索与生成结合应用于梵文Subhasitas，实现传统文化遗产与现代AI技术的有效融合。

Abstract: Sanskrit Subhasitas encapsulate centuries of cultural and philosophical wisdom, yet remain underutilized in the digital age due to linguistic and contextual barriers. In this work, we present Pragya, a retrieval-augmented generation (RAG) framework for semantic recommendation of Subhasitas. We curate a dataset of 200 verses annotated with thematic tags such as motivation, friendship, and compassion. Using sentence embeddings (IndicBERT), the system retrieves top-k verses relevant to user queries. The retrieved results are then passed to a generative model (Mistral LLM) to produce transliterations, translations, and contextual explanations. Experimental evaluation demonstrates that semantic retrieval significantly outperforms keyword matching in precision and relevance, while user studies highlight improved accessibility through generated summaries. To our knowledge, this is the first attempt at integrating retrieval and generation for Sanskrit Subhasitas, bridging cultural heritage with modern applied AI.

</details>


### [44] [Efficient and Reliable Estimation of Named Entity Linking Quality: A Case Study on GutBrainIE](https://arxiv.org/abs/2601.06624)
*Marco Martinelli,Stefano Marchesin,Gianmaria Silvello*

Main category: cs.CL

TL;DR: 该论文提出了一个基于抽样的方法，用于在有限注释预算下，统计保证条件下估计大规模生物医学实体链接（NEL）体系的准确率。


<details>
  <summary>Details</summary>
Motivation: 由于专家注释成本高昂且语料库规模巨大，评估生物医学信息提取中关键模块——实体链接（NEL）的质量存在较大挑战。

Method: 文章将NEL准确率估计问题建模为约束优化问题，采用分层两阶段聚类抽样（STWCS）方法，定义不依赖NEL标注的标签分层和全局表面形式聚类，实现高效抽样和注释。

Result: 在新发布的GutBrainIE语料上，通过手动注释24.6%的数据（2749个三元组），实现了总体准确率0.915±0.0473，与简单随机抽样相比，专家注释时间减少约29%。

Conclusion: 该抽样框架不仅能高效估计NEL准确率，还具备统计稳健性，适用于其他NEL基准和信息提取管线的规模化准确率评估。

Abstract: Named Entity Linking (NEL) is a core component of biomedical Information Extraction (IE) pipelines, yet assessing its quality at scale is challenging due to the high cost of expert annotations and the large size of corpora. In this paper, we present a sampling-based framework to estimate the NEL accuracy of large-scale IE corpora under statistical guarantees and constrained annotation budgets. We frame NEL accuracy estimation as a constrained optimization problem, where the objective is to minimize expected annotation cost subject to a target Margin of Error (MoE) for the corpus-level accuracy estimate. Building on recent works on knowledge graph accuracy estimation, we adapt Stratified Two-Stage Cluster Sampling (STWCS) to the NEL setting, defining label-based strata and global surface-form clusters in a way that is independent of NEL annotations. Applied to 11,184 NEL annotations in GutBrainIE -- a new biomedical corpus openly released in fall 2025 -- our framework reaches a MoE $\leq 0.05$ by manually annotating only 2,749 triples (24.6%), leading to an overall accuracy estimate of $0.915 \pm 0.0473$. A time-based cost model and simulations against a Simple Random Sampling (SRS) baseline show that our design reduces expert annotation time by about 29% at fixed sample size. The framework is generic and can be applied to other NEL benchmarks and IE pipelines that require scalable and statistically robust accuracy assessment.

</details>


### [45] [Labels have Human Values: Value Calibration of Subjective Tasks](https://arxiv.org/abs/2601.06631)
*Mohammed Fayiz Parappan,Ricardo Henao*

Main category: cs.CL

TL;DR: 本文提出了多校准主观任务学习框架MC-STL，通过识别人类价值观簇并为各簇学习特征嵌入，显著提升了主观任务的预测性能和校准效果。


<details>
  <summary>Details</summary>
Motivation: 主观任务的自然语言处理系统需要考虑不同人类价值观的差异，以实现更准确和公平的预测。

Method: MC-STL框架通过三种方法（注释者理由相似性、专家价值分类法和评价者的社会文化描述）对注释进行聚类，学习每个价值簇的特定嵌入以实现预测校准。

Result: 在多种主观学习设置及多个数据集（包括有毒聊天、冒犯性社交媒体帖子和人类偏好对齐）上，MC-STL优于忽略注释背后的潜在价值结构的基线方法，在区分度、价值特异性校准和异议感知指标上均有提升。

Conclusion: 考虑潜在人类价值结构进行多校准，可以有效提升主观任务NLP系统的性能与公平性。

Abstract: Building NLP systems for subjective tasks requires one to ensure their alignment to contrasting human values. We propose the MultiCalibrated Subjective Task Learner framework (MC-STL), which clusters annotations into identifiable human value clusters by three approaches (similarity of annotator rationales, expert-value taxonomies or rater's sociocultural descriptors) and calibrates predictions for each value cluster by learning cluster-specific embeddings. We demonstrate MC-STL on several subjective learning settings, including ordinal, binary, and preference learning predictions, and evaluate it on multiple datasets covering toxic chatbot conversations, offensive social media posts, and human preference alignment. The results show that MC-STL consistently outperforms the baselines that ignore the latent value structure of the annotations, delivering gains in discrimination, value-specific calibration, and disagreement-aware metrics.

</details>


### [46] [MedEinst: Benchmarking the Einstellung Effect in Medical LLMs through Counterfactual Differential Diagnosis](https://arxiv.org/abs/2601.06636)
*Wenting Chen,Zhongrui Zhu,Guolin Huang,Wenxuan Wang*

Main category: cs.CL

TL;DR: 该论文揭示了大型语言模型在临床诊断中的固化偏见问题，并提出了用于检测此问题的新基准MedEinst及改进方法ECR-Agent。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在医疗基准测试中表现出高准确率，但它们在临床诊断中存在依赖统计捷径而非患者特异性证据的Einstellung效应，导致 atypical病例误诊，现有基准难以揭示此关键失败模式。

Method: 提出MedEinst基准，包含5383对临床病例（对照组与诊断被改变的“陷阱”组），用Bias Trap Rate指标衡量模型错误诊断倾向。提出ECR-Agent方法，包括：(1)动态因果推理(DCI)，通过双路径感知及三层因果图推理进行结构化推理及证据审核；(2)批评驱动的图与记忆演化(CGME)，通过存储验证的推理路径和整合疾病知识迭代优化系统。

Result: 17个大型语言模型的广泛评测表明，尽管前沿模型基线准确率高，但偏见陷阱率严重。ECR-Agent有效缓解了这一问题，提升诊断的因果一致性与证据基础。

Conclusion: 提出的MedEinst基准能检测LLM在临床诊断中的Einstellung效应，ECR-Agent方法通过结合证据医学标准的动态因果推理和知识进化机制，显著减少了误诊风险，且相关源代码将开源。

Abstract: Despite achieving high accuracy on medical benchmarks, LLMs exhibit the Einstellung Effect in clinical diagnosis--relying on statistical shortcuts rather than patient-specific evidence, causing misdiagnosis in atypical cases. Existing benchmarks fail to detect this critical failure mode. We introduce MedEinst, a counterfactual benchmark with 5,383 paired clinical cases across 49 diseases. Each pair contains a control case and a "trap" case with altered discriminative evidence that flips the diagnosis. We measure susceptibility via Bias Trap Rate--probability of misdiagnosing traps despite correctly diagnosing controls. Extensive Evaluation of 17 LLMs shows frontier models achieve high baseline accuracy but severe bias trap rates. Thus, we propose ECR-Agent, aligning LLM reasoning with Evidence-Based Medicine standard via two components: (1) Dynamic Causal Inference (DCI) performs structured reasoning through dual-pathway perception, dynamic causal graph reasoning across three levels (association, intervention, counterfactual), and evidence audit for final diagnosis; (2) Critic-Driven Graph and Memory Evolution (CGME) iteratively refines the system by storing validated reasoning paths in an exemplar base and consolidating disease-specific knowledge into evolving illness graphs. Source code is to be released.

</details>


### [47] [Efficient Aspect Term Extraction using Spiking Neural Network](https://arxiv.org/abs/2601.06637)
*Abhishek Kumar Mishra,Arya Somasundaram,Anup Das,Nagarajan Kandasamy*

Main category: cs.CL

TL;DR: 本文提出了一种基于脉冲神经网络（SNNs）的面向术语提取（ATE）新方法，性能与深度神经网络（DNNs）相当但能耗更低。


<details>
  <summary>Details</summary>
Motivation: 现有ATE模型普遍采用能耗高的深度神经网络，本文旨在寻找一种更节能且有效的替代方案。

Method: 提出采用稀疏激活和事件驱动推理的脉冲神经网络架构SpikeATE，使用三值脉冲神经元并结合伪梯度进行直接脉冲训练。

Result: 在四个SemEval基准数据集上，SpikeATE取得了与当前最先进DNN模型相当的性能，但能耗显著降低。

Conclusion: 脉冲神经网络为ATE任务提供了实用且可持续的解决方案，有助于构建节能高效的情感分析系统。

Abstract: Aspect Term Extraction (ATE) identifies aspect terms in review sentences, a key subtask of sentiment analysis. While most existing approaches use energy-intensive deep neural networks (DNNs) for ATE as sequence labeling, this paper proposes a more energy-efficient alternative using Spiking Neural Networks (SNNs). Using sparse activations and event-driven inferences, SNNs capture temporal dependencies between words, making them suitable for ATE. The proposed architecture, SpikeATE, employs ternary spiking neurons and direct spike training fine-tuned with pseudo-gradients. Evaluated on four benchmark SemEval datasets, SpikeATE achieves performance comparable to state-of-the-art DNNs with significantly lower energy consumption. This highlights the use of SNNs as a practical and sustainable choice for ATE tasks.

</details>


### [48] [Do Language Models Reason Across Languages?](https://arxiv.org/abs/2601.06644)
*Yan Meng,Wafaa Mohammed,Christof Monz*

Main category: cs.CL

TL;DR: 本文提出了一种基于两跳推理的多语言问答任务，发现语言模型在多语言文档间推理存在不足，并通过三阶段子问题提示法显著提升准确率。


<details>
  <summary>Details</summary>
Motivation: 现实世界信息多语言混合，探究语言模型是否能跨语言综合信息进行推理。

Method: 设计两跳多语言问答任务，评估模型对不同语言文档的敏感性，提出三阶段SUBQ提示法引导多步推理。

Result: 发现语言模型对答案文档语言敏感，推理过程中存在非忠实分步解答现象，导致约18%的组合失败。应用SUBQ提示法，准确率从10.1%提升至66.5%。

Conclusion: 语言模型在多语言推理中存在推理分解不足问题，分步引导法有效提升模型的多步推理准确性。

Abstract: The real-world information sources are inherently multilingual, which naturally raises a question about whether language models can synthesize information across languages. In this paper, we introduce a simple two-hop question answering setting, where answering a question requires making inferences over two multilingual documents. We find that language models are more sensitive to language variation in answer-span documents than in those providing bridging information, despite the equal importance of both documents for answering a question. Under a step-by-step sub-question evaluation, we further show that in up to 33% of multilingual cases, models fail to infer the bridging information in the first step yet still answer the overall question correctly. This indicates that reasoning in language models, especially in multilingual settings, does not follow a faithful step-by-step decomposition. Subsequently, we show that the absence of reasoning decomposition leads to around 18% composition failure, where both sub-questions are answered correctly but fail for the final two-hop questions. To mitigate this, we propose a simple three-stage SUBQ prompting method to guide the multi-step reasoning with sub-questions, which boosts accuracy from 10.1% to 66.5%.

</details>


### [49] [What makes for an enjoyable protagonist? An analysis of character warmth and competence](https://arxiv.org/abs/2601.06658)
*Hannes Rosenbusch*

Main category: cs.CL

TL;DR: 研究发现电影主角的温暖和能力与IMDb评分存在小幅正相关，且性别和类型对评分影响有限。


<details>
  <summary>Details</summary>
Motivation: 探讨电影主角的人格特质（温暖和能力）是否影响观众评分及其在不同类型电影中的变化。

Method: 利用影视剧本语料库，采用AI辅助标注识别主角，使用LLM_annotate量化主角温暖和能力，通过贝叶斯回归分析评分关联。

Result: 温暖和能力与影片评分存在一致但微弱的正相关，性别差异显著，男性主角评分更高，类型交互作用影响不大。

Conclusion: 主角人格特质对电影评分有一定影响，但作用较小，电影评分受多种因素综合影响；AI辅助标注虽有效，但不及人工精准。

Abstract: Drawing on psychological and literary theory, we investigated whether the warmth and competence of movie protagonists predict IMDb ratings, and whether these effects vary across genres. Using 2,858 films and series from the Movie Scripts Corpus, we identified protagonists via AI-assisted annotation and quantified their warmth and competence with the LLM_annotate package ([1]; human-LLM agreement: r = .83). Preregistered Bayesian regression analyses revealed theory-consistent but small associations between both warmth and competence and audience ratings, while genre-specific interactions did not meaningfully improve predictions. Male protagonists were slightly less warm than female protagonists, and movies with male leads received higher ratings on average (an association that was multiple times stronger than the relationships between movie ratings and warmth/competence). These findings suggest that, although audiences tend to favor warm, competent characters, the effects on movie evaluations are modest, indicating that character personality is only one of many factors shaping movie ratings. AI-assisted annotation with LLM_annotate and gpt-4.1-mini proved effective for large-scale analyses but occasionally fell short of manually generated annotations.

</details>


### [50] [InFi-Check: Interpretable and Fine-Grained Fact-Checking of LLMs](https://arxiv.org/abs/2601.06666)
*Yuzhuo Bai,Shuzheng Si,Kangyang Luo,Qingyi Wang,Wenhao Li,Gang Chen,Fanchao Qi,Maosong Sun*

Main category: cs.CL

TL;DR: 本文提出了InFi-Check框架，针对大语言模型（LLMs）输出的事实性检查，实现了细粒度、多维度的错误识别与解释。


<details>
  <summary>Details</summary>
Motivation: 目前多数事实核查方法仅做二分类判断，缺乏细粒度错误类型识别和解释，难以全面理解模型产生的幻觉错误。

Method: 设计了一个受控数据合成流程，生成带有明确证据、细粒度错误标签、理由和修正的数据，构建大规模训练集和人工验证的测试集InFi-Check-FG，基于此提出InFi-Checker模型，能够提供支持证据、错误分类、理由及修正。

Result: InFi-Checker在InFi-Check-FG测试集上取得了最先进的性能，并在多种下游任务中展现出良好的泛化能力。

Conclusion: InFi-Check框架显著提升了LLM事实性评估的实用性和可信度，推动了细粒度、可解释事实核查的发展。

Abstract: Large language models (LLMs) often hallucinate, yet most existing fact-checking methods treat factuality evaluation as a binary classification problem, offering limited interpretability and failing to capture fine-grained error types. In this paper, we introduce InFi-Check, a framework for interpretable and fine-grained fact-checking of LLM outputs. Specifically, we first propose a controlled data synthesis pipeline that generates high-quality data featuring explicit evidence, fine-grained error type labels, justifications, and corrections. Based on this, we further construct large-scale training data and a manually verified benchmark InFi-Check-FG for fine-grained fact-checking of LLM outputs. Building on these high-quality training data, we further propose InFi-Checker, which can jointly provide supporting evidence, classify fine-grained error types, and produce justifications along with corrections. Experiments show that InFi-Checker achieves state-of-the-art performance on InFi-Check-FG and strong generalization across various downstream tasks, significantly improving the utility and trustworthiness of factuality evaluation.

</details>


### [51] [Will it Merge? On The Causes of Model Mergeability](https://arxiv.org/abs/2601.06672)
*Adir Rahamim,Asaf Yehudai,Boaz Carmeli,Leshem Choshen,Yosi Mass,Yonatan Belinkov*

Main category: cs.CL

TL;DR: 研究了模型合并的成败因素，提出可测量的合并性定义，发现基模型的知识对合并效果影响最大，提出加权合并方法以保留基模型的弱知识。


<details>
  <summary>Details</summary>
Motivation: 当前模型合并技术能将多个微调模型合并成一个多任务模型且不需再训练，但为何某些模型合并成功而其他失败尚不清楚，需深入理解影响合并效果的关键因素。

Method: 提出了一个具体且可度量的合并性定义，分析不同模型合并效果的差异，重点研究基模型知识水平对合并性的影响，同时探索了基于合并性定义的加权合并技术以保护基模型弱知识。

Result: 发现基模型对训练实例的知识掌握程度是决定合并表现的主要因素，即基模型熟悉的实例上微调的模型更易合并；加权合并方法能更好地保留基模型中的弱知识。

Conclusion: 模型合并效果的关键在于基模型对相关知识的掌握，合理利用基模型知识并采用加权合并策略，能有效提升多任务模型的构建效果。

Abstract: Model merging has emerged as a promising technique for combining multiple fine-tuned models into a single multitask model without retraining. However, the factors that determine whether merging will succeed or fail remain poorly understood. In this work, we investigate why specific models are merged better than others. To do so, we propose a concrete, measurable definition of mergeability. We investigate several potential causes for high or low mergeability, highlighting the base model knowledge as a dominant factor: Models fine-tuned on instances that the base model knows better are more mergeable than models fine-tuned on instances that the base model struggles with. Based on our mergeability definition, we explore a simple weighted merging technique that better preserves weak knowledge in the base model.

</details>


### [52] [Evaluating Cross-Lingual Unlearning in Multilingual Language Models](https://arxiv.org/abs/2601.06675)
*Tyler Lizzo,Larry Heck*

Main category: cs.CL

TL;DR: 本文首次全面评估了多语言大型语言模型（LLMs）中的跨语言遗忘能力，发现大部分遗忘算法在训练语言外无法有效移除事实，子空间投影方法效果最好。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘算法难以在多语言模型中实现跨语言有效遗忘，需要探索更有效的遗忘方法。

Method: 基于七种语言/文字变体的翻译TOFU基准，测试多种遗忘算法，重点分析子空间投影方法及其在权重空间中的表现。

Result: 大多数遗忘算法无法实现跨语言事实移除，子空间投影方法在保持性能的同时表现出强大的跨语言遗忘能力；任务子空间分析揭示了共享的中介语言结构及语言特异性成分。

Conclusion: 多语言遗忘依赖于权重空间的几何结构，子空间方法为未来跨语言遗忘系统提供了有效方向。

Abstract: We present the first comprehensive evaluation of cross-lingual unlearning in multilingual LLMs. Using translated TOFU benchmarks in seven language/script variants, we test major unlearning algorithms and show that most fail to remove facts outside the training language, even when utility remains high. However, subspace-projection consistently outperforms the other methods, achieving strong cross-lingual forgetting with minimal degradation. Analysis of learned task subspaces reveals a shared interlingua structure: removing this shared subspace harms all languages, while removing language-specific components selectively affects one. These results demonstrate that multilingual forgetting depends on geometry in weight space, motivating subspace-based approaches for future unlearning systems.

</details>


### [53] [IDRBench: Interactive Deep Research Benchmark](https://arxiv.org/abs/2601.06676)
*Yingchaojie Feng,Qiang Huang,Xiaoya Xie,Zhaorui Yang,Jun Yu,Wei Chen,Anthony K. H. Tung*

Main category: cs.CL

TL;DR: 本文提出IDRBench，这是首个系统评估交互式深度研究的基准，结合多智能体框架和交互感知评估，帮助量化交互的价值和成本。


<details>
  <summary>Details</summary>
Motivation: 现有深度研究代理多自主执行，忽视交互的重要性，且缺乏衡量动态用户反馈及其代价的标准。

Method: 设计IDRBench，集成模块化多智能体框架、基于参考的用户模拟器和交互感知评测，综合评估研究质量、对齐度及交互代价。

Result: 7个先进大语言模型实验证明，交互显著提升研究质量与鲁棒性，交互效果优于模型本身能力差异，并揭示交互效率的权衡。

Conclusion: 交互对深度研究代理至关重要，有助于实现更高质量和更好对齐的结果，未来需关注交互效率的优化。

Abstract: Deep research agents powered by Large Language Models (LLMs) can perform multi-step reasoning, web exploration, and long-form report generation. However, most existing systems operate in an autonomous manner, assuming fully specified user intent and evaluating only final outputs. In practice, research goals are often underspecified and evolve during exploration, making sustained interaction essential for robust alignment. Despite its importance, interaction remains largely invisible to existing deep research benchmarks, which neither model dynamic user feedback nor quantify its costs. We introduce IDRBench, the first benchmark for systematically evaluating interactive deep research. IDRBench combines a modular multi-agent research framework with on-demand interaction, a scalable reference-grounded user simulator, and an interaction-aware evaluation suite that jointly measures interaction benefits (quality and alignment) and costs (turns and tokens). Experiments across seven state-of-the-art LLMs show that interaction consistently improves research quality and robustness, often outweighing differences in model capacity, while revealing substantial trade-offs in interaction efficiency.

</details>


### [54] [Characterising Toxicity in Generative Large Language Models](https://arxiv.org/abs/2601.06700)
*Zhiyao Zhang,Yazan Mash'Al,Yuhan Wu*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在生成文本时仍可能产生有害、冒犯性或不当内容，即“有毒”输出。本文研究了LLMs在提示下生成有毒内容的程度和语言因素影响。


<details>
  <summary>Details</summary>
Motivation: 尽管使用人类反馈的强化学习（RLHF）方法在一定程度上减少了模型生成有害内容的现象，但通过精心设计的提示词，这些安全措施仍可被绕过，因此需要研究语言模型生成有害内容的情况及其语言机制。

Method: 分析大型语言模型在各种提示下生成的输出，重点考察词汇和句法等语言因素对有害输出产生的影响。

Result: 发现大型语言模型仍然会在某些提示下生成有害内容，且特定的词汇和句法结构对有害输出的生成起关键作用。

Conclusion: LLMs的安全机制仍需加强，理解语言因素对有害输出的影响有助于改进模型的安全性和输出质量。

Abstract: In recent years, the advent of the attention mechanism has significantly advanced the field of natural language processing (NLP), revolutionizing text processing and text generation. This has come about through transformer-based decoder-only architectures, which have become ubiquitous in NLP due to their impressive text processing and generation capabilities. Despite these breakthroughs, language models (LMs) remain susceptible to generating undesired outputs: inappropriate, offensive, or otherwise harmful responses. We will collectively refer to these as ``toxic'' outputs. Although methods like reinforcement learning from human feedback (RLHF) have been developed to align model outputs with human values, these safeguards can often be circumvented through carefully crafted prompts. Therefore, this paper examines the extent to which LLMs generate toxic content when prompted, as well as the linguistic factors -- both lexical and syntactic -- that influence the production of such outputs in generative models.

</details>


### [55] [GRASP LoRA: GRPO Guided Adapter Sparsity Policy for Cross Lingual Transfer](https://arxiv.org/abs/2601.06702)
*Besher Hassan,Xiuying Chen*

Main category: cs.CL

TL;DR: 本文提出了GRASP LoRA，一种通过可学习的全局稀疏率控制变量来优化适配器微调的方法，避免了计算昂贵的网格搜索，提升了跨语言迁移效果并显著降低了开发成本。


<details>
  <summary>Details</summary>
Motivation: 适配大规模语言模型时，传统的全局剪枝比例多通过网格搜索确定，既耗时又依赖大量开发数据，难以找到最优比例。

Method: 提出GRPO控制器与训练交替进行，通过小规模开发集在线探测剪枝比例并更新，取代了网格搜索。模型采用融合的LoRA适配器和冻结的主干，最终固定比例进行剪枝微调。

Result: 在从英语到阿拉伯语和中文的跨语言迁移任务上，GRASP LoRA在XL-Sum摘要和MLQA问答任务中超过了传统目标适配和剪枝基线，显著提升语义一致性、内容覆盖和答案质量。

Conclusion: GRASP LoRA大幅减少了端到端运行时间，降低了对大型开发集的依赖，使适配器在低资源环境中的复用变得可行且高效。

Abstract: Parameter efficient fine tuning is a way to adapt LLMs to new languages when compute or data are limited, yet adapter pipelines usually choose a global prune ratio by grid search. This practice is computationally expensive and development set intensive, since it repeats training, freezes sparsity, and misses fractional optima. We introduce GRASP LoRA (GRPO Guided Adapter Sparsity Policy), which treats global sparsity as a learnable control variable. A GRPO controller interleaves with training, periodically probing candidate prune ratios on a small micro development set and updating a single global prune ratio online from its reward signal. It operates on merged source and target LoRA adapters on a frozen backbone and replaces grid search with one controller run that learns a prune ratio, followed by a single final merge and prune fine tuning run with pruning fixed to that ratio. On cross lingual transfer from English into Arabic and Chinese, including XL-Sum summarization and MLQA extractive question answering with Llama 3 8B, GRASP LoRA improves semantic faithfulness, content coverage, and answer quality over strong target only and merge and prune baselines. It reduces end to end runtime by multiple times relative to grid search, lowers reliance on large development sets, and makes adapter reuse practical for low resource deployment.

</details>


### [56] [Evaluating Accounting Reasoning Capabilities of Large Language Models](https://arxiv.org/abs/2601.06707)
*Jie Zhou,Xin Chen,Jie Zhang,Hai Li,Jie Wang,Zhe Li*

Main category: cs.CL

TL;DR: 本文定义了垂直领域会计推理，提出评价标准，并评估了多个大型语言模型在会计推理任务中的表现，发现GPT-4能力最强，但现有模型仍不足以满足企业实际需求。


<details>
  <summary>Details</summary>
Motivation: 将大型语言模型有效整合到专业领域（如会计）是企业数字化转型的关键挑战。

Method: 定义垂直领域会计推理，基于训练数据特征分析提出评价标准，并使用该框架评估GLM-6B、GLM-130B、GLM-4及GPT-4的会计推理能力。

Result: 提示设计显著影响模型表现，GPT-4表现最优，但所有模型在实际企业会计应用中能力仍不足。

Conclusion: 当前大型语言模型在会计推理方面尚未达到实际应用标准，需要进一步优化以发挥其实际价值。

Abstract: Large language models are transforming learning, cognition, and research across many fields. Effectively integrating them into professional domains, such as accounting, is a key challenge for enterprise digital transformation. To address this, we define vertical domain accounting reasoning and propose evaluation criteria derived from an analysis of the training data characteristics of representative GLM models. These criteria support systematic study of accounting reasoning and provide benchmarks for performance improvement. Using this framework, we evaluate GLM-6B, GLM-130B, GLM-4, and OpenAI GPT-4 on accounting reasoning tasks. Results show that prompt design significantly affects performance, with GPT-4 demonstrating the strongest capability. Despite these gains, current models remain insufficient for real-world enterprise accounting, indicating the need for further optimization to unlock their full practical value.

</details>


### [57] [Towards Computational Chinese Paleography](https://arxiv.org/abs/2601.06753)
*Yiran Rex Ma*

Main category: cs.CL

TL;DR: 本文综述了人工智能推动下的中国古文字学的计算化转型，从视觉任务自动化到构建数字学术生态系统。


<details>
  <summary>Details</summary>
Motivation: 推动中国古文字学从传统的手工解读转向利用AI技术建立集成数字平台，提高研究效率与精度。

Method: 分析了甲骨文、青铜器铭文和竹简的关键数据集，探讨了从图像恢复、字符识别到文物重组、日期考定及自动破译的技术流程，评估了计算机视觉到深度学习（如transformer和多模态模型）的转变。

Result: 识别并总结了数据稀缺和AI能力与人文研究需求之间的断层，指出现有方法在全局理解和推理方面的局限。

Conclusion: 提出未来研究应聚焦多模态、少样本学习及人机协同系统，以促进学术专家能力的增强和学科融合发展。

Abstract: Chinese paleography, the study of ancient Chinese writing, is undergoing a computational turn powered by artificial intelligence. This position paper charts the trajectory of this emerging field, arguing that it is evolving from automating isolated visual tasks to creating integrated digital ecosystems for scholarly research. We first map the landscape of digital resources, analyzing critical datasets for oracle bone, bronze, and bamboo slip scripts. The core of our analysis follows the field's methodological pipeline: from foundational visual processing (image restoration, character recognition), through contextual analysis (artifact rejoining, dating), to the advanced reasoning required for automated decipherment and human-AI collaboration. We examine the technological shift from classical computer vision to modern deep learning paradigms, including transformers and large multimodal models. Finally, we synthesize the field's core challenges -- notably data scarcity and a disconnect between current AI capabilities and the holistic nature of humanistic inquiry -- and advocate for a future research agenda focused on creating multimodal, few-shot, and human-centric systems to augment scholarly expertise.

</details>


### [58] [MTMCS-Bench: Evaluating Contextual Safety of Multimodal Large Language Models in Multi-Turn Dialogues](https://arxiv.org/abs/2601.06757)
*Zheyuan Liu,Dongwhi Kim,Yixin Wan,Xiangchi Yuan,Zhaoxuan Tan,Fengran Mo,Meng Jiang*

Main category: cs.CL

TL;DR: 本文介绍了一个多轮多模态上下文安全基准MTMCS-Bench，用于评估多模态大语言模型（MLLM）在涉及图像和对话的复杂安全风险中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的上下文安全基准多为单轮对话，无法捕捉恶意意图的逐渐显现及同一场景的双重风险，亟需更现实且复杂的多轮多模态安全评估方法。

Method: 提出MTMCS-Bench基准，包含超过三万条多模态和单模态样本，覆盖两个风险场景（风险升级和情境切换），并提供结构化评估指标，分别测量意图识别、安全感知及有用性表现。

Result: 在多个开源和专有模型上测试，发现模型在安全性与实用性之间存在权衡，常忽视逐步风险或过度拒绝无害对话，现有防护措施虽有所改善但不能完全消除多轮上下文风险。

Conclusion: MTMCS-Bench为多模态大语言模型的上下文安全评估提供了重要工具，揭示当前模型与防护机制在多轮复杂情境下的局限，需要进一步优化以提升安全性和实用性平衡。

Abstract: Multimodal large language models (MLLMs) are increasingly deployed as assistants that interact through text and images, making it crucial to evaluate contextual safety when risk depends on both the visual scene and the evolving dialogue. Existing contextual safety benchmarks are mostly single-turn and often miss how malicious intent can emerge gradually or how the same scene can support both benign and exploitative goals. We introduce the Multi-Turn Multimodal Contextual Safety Benchmark (MTMCS-Bench), a benchmark of realistic images and multi-turn conversations that evaluates contextual safety in MLLMs under two complementary settings, escalation-based risk and context-switch risk. MTMCS-Bench offers paired safe and unsafe dialogues with structured evaluation. It contains over 30 thousand multimodal (image+text) and unimodal (text-only) samples, with metrics that separately measure contextual intent recognition, safety-awareness on unsafe cases, and helpfulness on benign ones. Across eight open-source and seven proprietary MLLMs, we observe persistent trade-offs between contextual safety and utility, with models tending to either miss gradual risks or over-refuse benign dialogues. Finally, we evaluate five current guardrails and find that they mitigate some failures but do not fully resolve multi-turn contextual risks.

</details>


### [59] [GanitLLM: Difficulty-Aware Bengali Mathematical Reasoning through Curriculum-GRPO](https://arxiv.org/abs/2601.06767)
*Shubhashis Roy Dipta,Khairul Mahbub,Nadia Najjar*

Main category: cs.CL

TL;DR: 提出了一个名为GanitLLM的孟加拉数学推理模型和一个新的难度感知孟加拉数学语料库，通过课程式训练提升多步骤数学推理能力，显著提高推理准确率和效率。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在孟加拉语数学推理中表现较差，主要由于强化学习方法在低资源语言环境下的奖励稀疏问题。

Method: 构建经过严格筛选和去污染的孟加拉数学数据集，结合难度标签；提出课程式GRPO训练方法，采用多阶段训练（监督微调+GRPO），并结合难度感知采样和可验证奖励机制。

Result: 在孟加拉语两大数学推理数据集上，GanitLLM-4B相比基线模型准确率提高7-8个百分点，孟加拉语推理比例由14%提升至88%以上，且解决方案长度大幅缩短。

Conclusion: 通过构建难度感知数据集和课程式强化学习训练，GanitLLM显著提升了孟加拉语数学多步骤推理能力，缓解低资源语言中的奖励稀疏问题。

Abstract: We present a Bengali mathematical reasoning model called GanitLLM (named after the Bangla word for mathematics, "Ganit"), together with a new difficulty-aware Bengali math corpus and a curriculum-based GRPO pipeline. Bengali is one of the world's most widely spoken languages, yet existing LLMs either reason in English and then translate, or simply fail on multi-step Bengali math, in part because reinforcement learning recipes are tuned for high-resource languages and collapse under reward sparsity in low-resource settings. To address this, we construct Ganit, a rigorously filtered and decontaminated Bengali math dataset with automatic difficulty tags derived from the pass@k of a strong evaluator model. Building on this dataset, we propose Curriculum-GRPO, which combines multi-stage training (SFT + GRPO) with difficulty-aware sampling and verifiable rewards for format, numerical correctness, and Bengali reasoning. On Bn-MGSM and Bn-MSVAMP, GanitLLM-4B improves over its Qwen3-4B base by +8 and +7 accuracy points, respectively, while increasing the percentage of Bengali reasoning tokens from 14% to over 88% and reducing average solution length from 943 to 193 words.

</details>


### [60] [Multi-Stage Evolutionary Model Merging with Meta Data Driven Curriculum Learning for Sentiment-Specialized Large Language Modeling](https://arxiv.org/abs/2601.06780)
*Keito Inoshita,Xiaokang Zhou,Akira Kawai*

Main category: cs.CL

TL;DR: 本文提出了一种名为MEM-MCL的混合学习模型，通过多阶段进化模型合并和元数据驱动的课程学习，提升了大语言模型在情感分析任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 传统情感分析方法多针对单一任务，难以满足实际多任务需求，而现有大语言模型在情感专属任务中准确率不足，且相关的元数据利用与课程学习尚未充分研究。

Method: 通过针对特定情感任务的专家模型指令调优，利用进化算法合并形成统一模型，并借助弱数据优化合并过程，结合根据任务难度设计的课程学习以提升知识提取效率。

Result: 实验表明，MEM-MCL模型在多数情感分析任务和子任务中表现优于传统大语言模型，达到了更好的准确率和泛化能力。

Conclusion: MEM-MCL通过结合专家模型、进化算法和课程学习，有效提升了大语言模型处理多任务情感分析的能力，展现出良好的实用性和性能优势。

Abstract: The emergence of large language models (LLMs) has significantly transformed natural language processing (NLP), enabling more generalized models to perform various tasks with minimal training. However, traditional sentiment analysis methods, which focus on individual tasks such as sentiment classification or aspect-based analysis, are not practical for real-world applications that usually require handling multiple tasks. While offering flexibility, LLMs in sentiment-specific tasks often fall short of the required accuracy. Techniques like fine-tuning and evolutionary model merging help integrate models into a unified framework, which can improve the learning performance while reducing computational costs. The use of task meta-data and curriculum learning to optimize learning processes remains underexplored, while sentiment analysis is a critical task in NLP that requires high accuracy and scalability across multiple subtasks. In this study, we propose a hybrid learning model called Multi-stage Evolutionary Model Merging with Meta data driven Curriculum Learning (MEM-MCL), to enhance the sentiment analysis in large language modeling. In particular, expert models are created through instruction tuning for specific sentiment tasks and then merged using evolutionary algorithms to form a unified model. The merging process is optimized with weak data to enhance performance across tasks. The curriculum learning is incorporated to provide a learning sequence based on task difficulty, improving knowledge extraction from LLMs. Experiment results demonstrate that the proposed MEM-MCL model outperforms conventional LLMs in a majority of sentiment analysis tasks, achieving superior results across various subtasks.

</details>


### [61] [EpiCaR: Knowing What You Don't Know Matters for Better Reasoning in LLMs](https://arxiv.org/abs/2601.06786)
*Jewon Yeom,Jaewon Sok,Seonghyeon Park,Jeongjae Park,Taesup Kim*

Main category: cs.CL

TL;DR: 本文提出了一种新的训练方法EpiCaR，通过联合优化推理性能和校准能力，提升大型语言模型的推理准确性和置信度校准。


<details>
  <summary>Details</summary>
Motivation: 现有基于模型自身生成数据的迭代训练虽然提高了推理准确性，但导致模型过于自信，失去表示不确定性的能力，出现了模型校准失败的问题。

Method: 将推理训练重新定义为一类认知学习问题，设计了Epistemically-Calibrated Reasoning (EpiCaR)训练目标，结合显式自我评估信号，采用迭代监督微调框架进行训练，兼顾推理能力与校准度。

Result: 在Llama-3和Qwen-3模型上，EpiCaR在准确率和校准度上均优于标准方法，且在具有较强推理能力的模型（3B以上）表现尤为突出。该方法在数学推理和代码生成任务中的OOD泛化性良好，并显著减少推理计算资源。

Conclusion: EpiCaR有效解决了模型校准失败问题，实现推理性能和置信度校准的同向提升，为高效且可靠的大型语言模型推理训练提供了新的思路。

Abstract: Improving the reasoning abilities of large language models (LLMs) has largely relied on iterative self-training with model-generated data. While effective at boosting accuracy, existing approaches primarily reinforce successful reasoning paths, incurring a substantial calibration cost: models become overconfident and lose the ability to represent uncertainty. This failure has been characterized as a form of model collapse in alignment, where predictive distributions degenerate toward low-variance point estimates. We address this issue by reframing reasoning training as an epistemic learning problem, in which models must learn not only how to reason, but also when their reasoning should be trusted. We propose epistemically-calibrated reasoning (EpiCaR) as a training objective that jointly optimizes reasoning performance and calibration, and instantiate it within an iterative supervised fine-tuning framework using explicit self-evaluation signals. Experiments on Llama-3 and Qwen-3 families demonstrate that our approach achieves Pareto-superiority over standard baselines in both accuracy and calibration, particularly in models with sufficient reasoning capacity (e.g., 3B+). This framework generalizes effectively to OOD mathematical reasoning (GSM8K) and code generation (MBPP). Ultimately, our approach enables a 3X reduction in inference compute, matching the K=30 performance of STaR with only K=10 samples in capable models.

</details>


### [62] [Garbage Attention in Large Language Models: BOS Sink Heads and Sink-aware Pruning](https://arxiv.org/abs/2601.06787)
*Jaewon Sok,Jewon Yeom,Seonghyeon Park,Jeongjae Park,Taesup Kim*

Main category: cs.CL

TL;DR: 本文发现了大语言模型中高层注意力头的冗余现象，提出BOS sink现象作为其机理，并基于此设计了有效的剪枝方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究发现LLM中部分组件冗余，尤其是高层注意力头，但缺乏系统解释和有效剪枝手段。

Method: 通过分析注意力头的BOS sink得分，识别功能上冗余的注意力头，并基于此提出简单的剪枝策略。

Result: 该策略在Gemma-3、Llama-3.1和Qwen3上表现优异，剪枝后性能保持接近稠密模型，且比基于权重或激活的剪枝更鲁棒。

Conclusion: 基于结构特性的剪枝方法比传统的基于幅度的剪枝更具直观性和鲁棒性，为模型压缩提供了新的思路。

Abstract: Large Language Models (LLMs) are known to contain significant redundancy, yet a systematic explanation for why certain components, particularly in higher layers, are more redundant has remained elusive. In this work, we identify the BOS sink phenomenon as a key mechanism driving this layer-wise sensitivity. We show that attention heads with high BOS sink scores are strongly associated with functional redundancy: such heads, especially in deeper layers, contribute little to predictive performance and effectively serve as \emph{dumping grounds} for superfluous attention weights. This provides a concrete functional explanation for the structural redundancy reported in prior studies. Leveraging this insight, we introduce a simple pruning strategy that removes high-BOS sink heads. Experiments on Gemma-3, Llama-3.1, and Qwen3 demonstrate that this approach identifies redundant transformer components more reliably than weight- or activation-based criteria, while preserving performance close to dense baselines even under aggressive pruning. Moreover, we find that the behavior of sink heads remains stable across different sequence lengths. Overall, our results suggest that structural properties of attention offer a more intuitive and robust basis for model compression than magnitude-based methods.

</details>


### [63] [CIRAG: Construction-Integration Retrieval and Adaptive Generation for Multi-hop Question Answering](https://arxiv.org/abs/2601.06799)
*Zili Wei,Xiaocui Yang,Yilin Wang,Zihan Wang,Weidong Bao,Shi Feng,Daling Wang,Yifei Zhang*

Main category: cs.CL

TL;DR: 本文提出了CIRAG模型，通过迭代构建-整合模块和多粒度生成模块，有效缓解了多跳问答中证据链的贪婪扩展和噪声控制问题，提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有多跳问答方法存在贪婪的单路径扩展带来早期错误传播和证据呈现粒度无法兼顾噪声与上下文信息平衡的限制。

Method: 提出迭代构建-整合模块以保留多条可能证据链，提出自适应级联多粒度生成模块逐步扩展上下文证据，从三元组到句子再到全文，同时采用轨迹蒸馏提升推理效率和稳定性。

Result: CIRAG模型在多跳问答任务上表现优于现有的iRAG方法，证明了其有效性。

Conclusion: CIRAG通过多路径证据链保留、多粒度上下文扩展及轨迹蒸馏策略显著提升了多跳问答的推理能力和鲁棒性。

Abstract: Triple-based Iterative Retrieval-Augmented Generation (iRAG) mitigates document-level noise for multi-hop question answering. However, existing methods still face limitations: (i) greedy single-path expansion, which propagates early errors and fails to capture parallel evidence from different reasoning branches, and (ii) granularity-demand mismatch, where a single evidence representation struggles to balance noise control with contextual sufficiency. In this paper, we propose the Construction-Integration Retrieval and Adaptive Generation model, CIRAG. It introduces an Iterative Construction-Integration module that constructs candidate triples and history-conditionally integrates them to distill core triples and generate the next-hop query. This module mitigates the greedy trap by preserving multiple plausible evidence chains. Besides, we propose an Adaptive Cascaded Multi-Granularity Generation module that progressively expands contextual evidence based on the problem requirements, from triples to supporting sentences and full passages. Moreover, we introduce Trajectory Distillation, which distills the teacher model's integration policy into a lightweight student, enabling efficient and reliable long-horizon reasoning. Extensive experiments demonstrate that CIRAG achieves superior performance compared to existing iRAG methods.

</details>


### [64] [Doing More with Less: Data Augmentation for Sudanese Dialect Automatic Speech Recognition](https://arxiv.org/abs/2601.06802)
*Ayman Mansour*

Main category: cs.CL

TL;DR: 该论文研究了苏丹方言阿拉伯语的自动语音识别，利用数据增强技术微调OpenAI Whisper模型，建立了首个苏丹方言基准，并显著提升了识别性能。


<details>
  <summary>Details</summary>
Motivation: 目前针对阿拉伯语方言（特别是低资源方言如苏丹语）的自动语音识别研究较少，缺乏专门的模型和基准，资源有限且效果不佳。

Method: 采用两种数据增强策略：1）利用无标签语音生成伪标签进行自训练；2）基于Klaam语音合成系统生成合成语音进行增强，结合这两种方法对Whisper-Medium模型进行微调。

Result: 最佳模型（结合自训练和TTS增强，训练数据28.4小时）在评测集和异域测试集上的词错误率分别为57.1%和51.6%，显著优于多语言零-shot Whisper（78.8% WER）和专门的阿拉伯语现代标准语模型（73.8%-123% WER）。

Conclusion: 采用策略性的数据增强能够克服低资源方言ASR的数据限制，提供了一条实用路径来开发低资源阿拉伯语方言以及其他边缘语言的ASR系统，且相关资源和代码已公开，便于后续研究。

Abstract: Although many Automatic Speech Recognition (ASR) systems have been developed for Modern Standard Arabic (MSA) and Dialectal Arabic (DA), few studies have focused on dialect-specific implementations, particularly for low-resource Arabic dialects such as Sudanese. This paper presents a comprehensive study of data augmentation techniques for fine-tuning OpenAI Whisper models and establishes the first benchmark for the Sudanese dialect. Two augmentation strategies are investigated: (1) self-training with pseudo-labels generated from unlabeled speech, and (2) TTS-based augmentation using synthetic speech from the Klaam TTS system. The best-performing model, Whisper-Medium fine-tuned with combined self-training and TTS augmentation (28.4 hours), achieves a Word Error Rate (WER) of 57.1% on the evaluation set and 51.6% on an out-of-domain holdout set substantially outperforming zero-shot multilingual Whisper (78.8% WER) and MSA-specialized Arabic models (73.8-123% WER). All experiments used low-cost resources (Kaggle free tier and Lightning.ai trial), demonstrating that strategic data augmentation can overcome resource limitations for low-resource dialects and provide a practical roadmap for developing ASR systems for low-resource Arabic dialects and other marginalized language varieties. The models, evaluation benchmarks, and reproducible training pipelines are publicly released to facilitate future research on low-resource Arabic ASR.

</details>


### [65] [Forest Before Trees: Latent Superposition for Efficient Visual Reasoning](https://arxiv.org/abs/2601.06803)
*Yubo Wang,Juntian Zhang,Yichen Wu,Yankai Lin,Nils Lukas,Yuhan Liu*

Main category: cs.CL

TL;DR: 该论文提出了Laser，一种通过动态窗口对齐学习改进视觉推理的新方法，有效解决了隐空间推理中信息丢失和语义崩溃的问题。


<details>
  <summary>Details</summary>
Motivation: 现有多步推理方法依赖显式文本推理，导致视觉细节信息丢失；已有隐空间推理方法则因自回归目标限制出现语义崩溃。

Method: 提出Laser框架，采用动态窗口对齐学习（DWAL），通过保持全局特征的概率叠加与动态有效窗口对齐，避免点对点预测；结合自我优化叠加，提升模型稳定性和可解释性。

Result: 在6个基准测试上，Laser在隐空间推理方法中实现了最新最优性能，平均超越强基线Monet 5.03%，并显著提升效率，推理token减少97%以上。

Conclusion: Laser通过构建“树木之前森林”的认知层次，实现了高效且可解释的视觉推理，表现出良好的泛化能力和极高的推理效率，推动隐空间推理的发展。

Abstract: While Chain-of-Thought empowers Large Vision-Language Models with multi-step reasoning, explicit textual rationales suffer from an information bandwidth bottleneck, where continuous visual details are discarded during discrete tokenization. Recent latent reasoning methods attempt to address this challenge, but often fall prey to premature semantic collapse due to rigid autoregressive objectives. In this paper, we propose Laser, a novel paradigm that reformulates visual deduction via Dynamic Windowed Alignment Learning (DWAL). Instead of forcing a point-wise prediction, Laser aligns the latent state with a dynamic validity window of future semantics. This mechanism enforces a "Forest-before-Trees" cognitive hierarchy, enabling the model to maintain a probabilistic superposition of global features before narrowing down to local details. Crucially, Laser maintains interpretability via decodable trajectories while stabilizing unconstrained learning via Self-Refined Superposition. Extensive experiments on 6 benchmarks demonstrate that Laser achieves state-of-the-art performance among latent reasoning methods, surpassing the strong baseline Monet by 5.03% on average. Notably, it achieves these gains with extreme efficiency, reducing inference tokens by more than 97%, while demonstrating robust generalization to out-of-distribution domains.

</details>


### [66] [AgentHallu: Benchmarking Automated Hallucination Attribution of LLM-based Agents](https://arxiv.org/abs/2601.06818)
*Xuannan Liu,Xiao Yang,Zekun Li,Peipei Li,Ran He*

Main category: cs.CL

TL;DR: 提出了自动识别大型语言模型（LLM）代理多步骤推理中产生幻觉的具体环节及原因的新任务，并构建了包含多个框架和领域的高质量基准数据集AgentHallu。评测结果显示该任务对现有模型仍具有较大挑战性。


<details>
  <summary>Details</summary>
Motivation: 多步骤推理中的幻觉可能在后续步骤传播，降低整体可靠性，且现有研究侧重单步骤幻觉检测，缺乏对多步骤流程中初始幻觉环节的识别研究。

Method: 设计自动幻觉归因任务，构建AgentHallu基准，涵盖693条跨7个框架、5个领域的轨迹，定义5大类14小类幻觉分类，配以多层次人工标注。同时评测13个领先模型的表现。

Result: 顶尖模型（如GPT-5和Gemini-2.5-Pro）在该任务中表现有限，最高步骤定位准确率为41.1%，其中工具使用类幻觉识别最为困难，仅11.6%。

Conclusion: AgentHallu基准为多步骤LLM代理幻觉归因研究提供了系统资源和挑战，期望推动开发更稳健、透明及可靠的智能代理系统。

Abstract: As LLM-based agents operate over sequential multi-step reasoning, hallucinations arising at intermediate steps risk propagating along the trajectory, thus degrading overall reliability. Unlike hallucination detection in single-turn responses, diagnosing hallucinations in multi-step workflows requires identifying which step causes the initial divergence. To fill this gap, we propose a new research task, automated hallucination attribution of LLM-based agents, aiming to identify the step responsible for the hallucination and explain why. To support this task, we introduce AgentHallu, a comprehensive benchmark with: (1) 693 high-quality trajectories spanning 7 agent frameworks and 5 domains, (2) a hallucination taxonomy organized into 5 categories (Planning, Retrieval, Reasoning, Human-Interaction, and Tool-Use) and 14 sub-categories, and (3) multi-level annotations curated by humans, covering binary labels, hallucination-responsible steps, and causal explanations. We evaluate 13 leading models, and results show the task is challenging even for top-tier models (like GPT-5, Gemini-2.5-Pro). The best-performing model achieves only 41.1\% step localization accuracy, where tool-use hallucinations are the most challenging at just 11.6\%. We believe AgentHallu will catalyze future research into developing robust, transparent, and reliable agentic systems.

</details>


### [67] [PDR: A Plug-and-Play Positional Decay Framework for LLM Pre-training Data Detection](https://arxiv.org/abs/2601.06827)
*Jinhan Liu,Yibo Yang,Ruiying Lu,Piotr Piekos,Yimeng Chen,Peng Wang,Dandan Guo*

Main category: cs.CL

TL;DR: 该论文提出了一种名为位置衰减重加权（PDR）的方法，通过对大语言模型预测的初期高熵token赋予更高权重，提高了预训练数据检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于似然的方法在黑盒、零样本环境下，忽略了自回归语言模型生成中信息熵变化的动态特性，导致检测预训练数据的效果受限。

Method: 文章假设并验证记忆信号在序列开头高熵位置更明显，设计PDR框架对token级得分进行非均匀加权，放大初期token信号，抑制后期噪声，无需额外训练，可即插即用。

Result: 实验表明，PDR作为一种稳健先验，能提升多种先进检测方法在多个基准数据集上的表现。

Conclusion: PDR通过利用语言模型生成过程中的信息熵特性，有效增强了预训练数据检测的能力，具有广泛应用潜力。

Abstract: Detecting pre-training data in Large Language Models (LLMs) is crucial for auditing data privacy and copyright compliance, yet it remains challenging in black-box, zero-shot settings where computational resources and training data are scarce. While existing likelihood-based methods have shown promise, they typically aggregate token-level scores using uniform weights, thereby neglecting the inherent information-theoretic dynamics of autoregressive generation. In this paper, we hypothesize and empirically validate that memorization signals are heavily skewed towards the high-entropy initial tokens, where model uncertainty is highest, and decay as context accumulates. To leverage this linguistic property, we introduce Positional Decay Reweighting (PDR), a training-free and plug-and-play framework. PDR explicitly reweights token-level scores to amplify distinct signals from early positions while suppressing noise from later ones. Extensive experiments show that PDR acts as a robust prior and can usually enhance a wide range of advanced methods across multiple benchmarks.

</details>


### [68] [Explainable Multimodal Aspect-Based Sentiment Analysis with Dependency-guided Large Language Model](https://arxiv.org/abs/2601.06848)
*Zhongzheng Wang,Yuanhe Tian,Hongzhi Wang,Yan Song*

Main category: cs.CL

TL;DR: 本文提出了一种基于多模态大型语言模型的生成式框架，用于多模态方面级情感分析，能够同时预测情感和生成解释，提升了模型的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态方面级情感分析方法多依赖复杂的判别式分类和多模态融合，缺乏对情感的显式解释，难以满足对社交媒体细粒度舆情理解的需求。

Method: 本文将MABSA任务重新定义为生成式且可解释的任务，利用多模态大型语言模型进行提示式生成，联合输出情感和自然语言解释；提出依存句法引导的情感线索策略来增强方面推理能力，并利用多模态语言模型构建带有情感解释的新数据集进行微调。

Result: 实验结果表明，该方法在情感分类准确率上实现了持续提升，同时生成的解释具有良好的真实性和面向具体情感方面的可信度。

Conclusion: 所提框架有效提升了多模态方面级情感分析的性能和解释能力，为细粒度社交媒体舆情理解提供了有力工具。

Abstract: Multimodal aspect-based sentiment analysis (MABSA) aims to identify aspect-level sentiments by jointly modeling textual and visual information, which is essential for fine-grained opinion understanding in social media. Existing approaches mainly rely on discriminative classification with complex multimodal fusion, yet lacking explicit sentiment explainability. In this paper, we reformulate MABSA as a generative and explainable task, proposing a unified framework that simultaneously predicts aspect-level sentiment and generates natural language explanations. Based on multimodal large language models (MLLMs), our approach employs a prompt-based generative paradigm, jointly producing sentiment and explanation. To further enhance aspect-oriented reasoning capabilities, we propose a dependency-syntax-guided sentiment cue strategy. This strategy prunes and textualizes the aspect-centered dependency syntax tree, guiding the model to distinguish different sentiment aspects and enhancing its explainability. To enable explainability, we use MLLMs to construct new datasets with sentiment explanations to fine-tune. Experiments show that our approach not only achieves consistent gains in sentiment classification accuracy, but also produces faithful, aspect-grounded explanations.

</details>


### [69] [†DAGGER: Distractor-Aware Graph Generation for Executable Reasoning in Math Problems](https://arxiv.org/abs/2601.06853)
*Zabir Al Nazi,Shubhashis Roy Dipta,Sudipta Kar*

Main category: cs.CL

TL;DR: 本文针对数学问题求解中无关干扰信息对链式思维提示法性能的影响进行了研究，提出了DAGGER方法显著提升了模型的鲁棒性和推理效率。


<details>
  <summary>Details</summary>
Motivation: 目前链式思维提示法在数学求解中表现优异，但面对语义相关但计算无关的干扰信息时性能严重下降，特别是在低资源语言环境中尚缺乏系统研究。

Method: 构建了包含干扰信息的孟加拉语基准数据集DISTRACTMATH-BN，评估多种模型的性能下降；提出DAGGER方法，将数学问题求解重构为可执行计算图生成并显式建模干扰节点，通过对Gemma-3模型进行监督微调和群体相对策略优化实现鲁棒性提升。

Result: 标准模型在干扰环境下性能最多下降41分，专用推理模型下降14至20分，而采用DAGGER方法的模型在无需专门训练干扰样本的情况下，以较少的token消耗实现了与推理模型相当的准确率。

Conclusion: 结构化中间表示的引入显著提升了数学推理任务中模型对噪声和干扰的容忍度和推理效率，尤其适用于低资源和噪声环境。

Abstract: Chain-of-Thought (CoT) prompting is widely adopted for mathematical problem solving, including in low-resource languages, yet its behavior under irrelevant context remains underexplored. To systematically study this challenge, we introduce DISTRACTMATH-BN, a Bangla benchmark that augments MGSM and MSVAMP with semantically coherent but computationally irrelevant information. Evaluating seven models ranging from 3B to 12B parameters, we observe substantial performance degradation under distractors: standard models drop by up to 41 points, while reasoning-specialized models decline by 14 to 20 points despite consuming five times more tokens. We propose †DAGGER, which reformulates mathematical problem solving as executable computational graph generation with explicit modeling of distractor nodes. Fine-tuning Gemma-3 models using supervised fine-tuning followed by Group Relative Policy Optimization achieves comparable weighted accuracy on augmented benchmarks while using 89 percent fewer tokens than reasoning models. Importantly, this robustness emerges without explicit training on distractor-augmented examples. Our results suggest that enforcing structured intermediate representations improves robustness and inference efficiency in mathematical reasoning compared to free-form approaches, particularly in noisy, low-resource settings.

</details>


### [70] [BiasLab: A Multilingual, Dual-Framing Framework for Robust Measurement of Output-Level Bias in Large Language Models](https://arxiv.org/abs/2601.06861)
*William Guey,Wei Zhang,Pei-Luen Patrick Rau,Pierrick Bougault,Vitor D. de Moura,Bertan Ucar,Jose O. Gomes*

Main category: cs.CL

TL;DR: 本文提出了BiasLab，一个多语言、模型无关的输出偏见评估框架，通过严格设计的双重框架和随机化教学包装，评估大语言模型在高风险场景中的偏见表现。


<details>
  <summary>Details</summary>
Motivation: 由于现有偏见评估方法敏感于提示语变化、语言覆盖有限且缺乏标准化指标，导致难以在不同模型间进行可靠比较，因此需要一个统一、稳健的评估框架。

Method: BiasLab设计构建了镜像探针对，通过双重框架（正面和反面断言），并结合随机化的教学包装和固定的李克特响应格式，利用LLM作为评判者将输出转换为一致的偏见指标，支持多语言和多偏见维度的评估。

Result: BiasLab实现了跨语言、跨框架敏感性的偏见定量测量，提供了结构化报告与可视化工具，可以评估人口统计、文化、政治等多维度的偏见，结果具有良好稳健性和可复现性。

Conclusion: BiasLab为大语言模型偏见评估提供了标准化、多语言支持和框架敏感性的解决方案，有助于研究人员和机构进行模型稳健性基准测试和更合理的部署决策。

Abstract: Large Language Models (LLMs) are increasingly deployed in high-stakes contexts where their outputs influence real-world decisions. However, evaluating bias in LLM outputs remains methodologically challenging due to sensitivity to prompt wording, limited multilingual coverage, and the lack of standardized metrics that enable reliable comparison across models. This paper introduces BiasLab, an open-source, model-agnostic evaluation framework for quantifying output-level (extrinsic) bias through a multilingual, robustness-oriented experimental design. BiasLab constructs mirrored probe pairs under a strict dual-framing scheme: an affirmative assertion favoring Target A and a reverse assertion obtained by deterministic target substitution favoring Target B, while preserving identical linguistic structure. To reduce dependence on prompt templates, BiasLab performs repeated evaluation under randomized instructional wrappers and enforces a fixed-choice Likert response format to maximize comparability across models and languages. Responses are normalized into agreement labels using an LLM-based judge, aligned for polarity consistency across framings, and aggregated into quantitative bias indicators with descriptive statistics including effect sizes and neutrality rates. The framework supports evaluation across diverse bias axes, including demographic, cultural, political, and geopolitical topics, and produces reproducible artifacts such as structured reports and comparative visualizations. BiasLab contributes a standardized methodology for cross-lingual and framing-sensitive bias measurement that complements intrinsic and dataset-based audits, enabling researchers and institutions to benchmark robustness and make better-informed deployment decisions.

</details>


### [71] [Paraphrasing Adversarial Attack on LLM-as-a-Reviewer](https://arxiv.org/abs/2601.06884)
*Masahiro Kaneko*

Main category: cs.CL

TL;DR: 该论文提出了一种针对大语言模型同行评审系统的攻击方法PAA，通过语义同义转换提高论文评分，且保持内容自然。


<details>
  <summary>Details</summary>
Motivation: 针对现有攻击方法依赖修改论文内容，混淆了模型鲁棒性和攻击效果，作者希望提出一种保持语义不变的攻击策略。

Method: 提出黑盒优化算法PAA，利用上下文学习，通过历史同义转换及评分引导生成高评分的改写文本。

Result: 在多场机器学习与自然语言处理会议中对三种大语言模型进行测试，PAA成功提升论文评分且不改变论文主张。人类评估证实改写文本保持语义与自然性。

Conclusion: PAA有效提升了诱导评分的攻击能力，且通过增加模型评审困惑度为检测攻击提供信号，部分同义改写防御能缓解攻击。

Abstract: The use of large language models (LLMs) in peer review systems has attracted growing attention, making it essential to examine their potential vulnerabilities. Prior attacks rely on prompt injection, which alters manuscript content and conflates injection susceptibility with evaluation robustness. We propose the Paraphrasing Adversarial Attack (PAA), a black-box optimization method that searches for paraphrased sequences yielding higher review scores while preserving semantic equivalence and linguistic naturalness. PAA leverages in-context learning, using previous paraphrases and their scores to guide candidate generation. Experiments across five ML and NLP conferences with three LLM reviewers and five attacking models show that PAA consistently increases review scores without changing the paper's claims. Human evaluation confirms that generated paraphrases maintain meaning and naturalness. We also find that attacked papers exhibit increased perplexity in reviews, offering a potential detection signal, and that paraphrasing submissions can partially mitigate attacks.

</details>


### [72] [Fine-grained Verbal Attack Detection via a Hierarchical Divide-and-Conquer Framework](https://arxiv.org/abs/2601.06907)
*Quan Zheng,Yuanhe Tian,Ming Wang,Yan Song*

Main category: cs.CL

TL;DR: 提出了针对中文社交媒体隐性言语攻击识别的层级攻击评论检测数据集与细粒度分而治之框架，显著提升了攻击识别效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法对对话结构和上下文依赖建模不足，难以有效识别隐性和上下文依赖的言语攻击。

Method: 构建层级攻击评论检测数据集，编码回复结构与时间顺序；设计分层细粒度框架，分解任务由轻量模型分别处理显性检测、隐性意图推断和目标识别。

Result: 在自建数据集及基准意图检测数据集上，小型模型在该框架下表现优于依赖参数扩展的大型单体模型。

Conclusion: 通过层级结构和任务分解，有效提升了中文言语攻击识别准确性，证明了结构化任务分解方法的优势。

Abstract: In the digital era, effective identification and analysis of verbal attacks are essential for maintaining online civility and ensuring social security. However, existing research is limited by insufficient modeling of conversational structure and contextual dependency, particularly in Chinese social media where implicit attacks are prevalent. Current attack detection studies often emphasize general semantic understanding while overlooking user response relationships, hindering the identification of implicit and context-dependent attacks. To address these challenges, we present the novel "Hierarchical Attack Comment Detection" dataset and propose a divide-and-conquer, fine-grained framework for verbal attack recognition based on spatiotemporal information. The proposed dataset explicitly encodes hierarchical reply structures and chronological order, capturing complex interaction patterns in multi-turn discussions. Building on this dataset, the framework decomposes attack detection into hierarchical subtasks, where specialized lightweight models handle explicit detection, implicit intent inference, and target identification under constrained context. Extensive experiments on the proposed dataset and benchmark intention detection datasets show that smaller models using our framework significantly outperform larger monolithic models relying on parameter scaling, demonstrating the effectiveness of structured task decomposition.

</details>


### [73] [Distributional Clarity: The Hidden Driver of RL-Friendliness in Large Language Models](https://arxiv.org/abs/2601.06911)
*Shaoning Sun,Mingzhu Cai,Huang He,Bingjin Chen,Siqi Bao,Yujiu Yang,Hua Wu,Haifeng Wang*

Main category: cs.CL

TL;DR: 本文揭示了语言模型在强化学习中表现差异的内在结构属性——概率空间中的分布清晰度，并通过Silhouette系数量化该属性，证明其与强化学习效果密切相关；通过引入基于该系数的重加权策略，提升了多模型家族在数学基准测试中的表现。


<details>
  <summary>Details</summary>
Motivation: 不同语言模型在同样的强化学习训练下表现出显著差异，本文旨在揭示这种差异的内在机制，即模型输出概率分布的结构特性如何影响强化学习效果。

Method: 采用三阶段分析方法（现象、机制、解释）揭示RL友好模型在正确与错误响应概率分布上表现出的类内紧凑性和类间分离性，利用Silhouette系数量化分布清晰度，并提出基于该系数的样本重加权训练策略。

Result: 结果表明高Silhouette系数与强化学习效果高度相关，低系数则与逻辑错误和推理不稳定相关。引入分布清晰度感知重加权策略后，在六个数学基准测试中所有模型家族均有提升，最高提升5.9分。

Conclusion: 分布清晰度是影响语言模型强化学习效果的基础且可训练属性，通过量化和优化该属性可以显著提升模型性能。

Abstract: Language model families exhibit striking disparity in their capacity to benefit from reinforcement learning: under identical training, models like Qwen achieve substantial gains, while others like Llama yield limited improvements. Complementing data-centric approaches, we reveal that this disparity reflects a hidden structural property: \textbf{distributional clarity} in probability space. Through a three-stage analysis-from phenomenon to mechanism to interpretation-we uncover that RL-friendly models exhibit intra-class compactness and inter-class separation in their probability assignments to correct vs. incorrect responses. We quantify this clarity using the \textbf{Silhouette Coefficient} ($S$) and demonstrate that (1) high $S$ correlates strongly with RL performance; (2) low $S$ is associated with severe logic errors and reasoning instability. To confirm this property, we introduce a Silhouette-Aware Reweighting strategy that prioritizes low-$S$ samples during training. Experiments across six mathematical benchmarks show consistent improvements across all model families, with gains up to 5.9 points on AIME24. Our work establishes distributional clarity as a fundamental, trainable property underlying RL-Friendliness.

</details>


### [74] [TreePS-RAG: Tree-based Process Supervision for Reinforcement Learning in Agentic RAG](https://arxiv.org/abs/2601.06922)
*Tianhua Zhang,Kun Li,Junan Li,Yunxiang Li,Hongyin Luo,Xixin Wu,James Glass,Helen Meng*

Main category: cs.CL

TL;DR: 本文提出了一种基于树结构的在线增强学习框架TreePS-RAG，用于多步交互式问答，解决了传统稀疏奖励带来的步骤级监督不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于最终结果的强化学习方法，难以实现步骤级的有效指导且中间推理缺少监督，现有的过程监督方法依赖于离线训练数据或昂贵的标注，有分布转移风险。

Method: 将问答推理过程建模为回滚树结构，通过蒙特卡洛方法评估每个推理步骤的效用，实现步骤级信号计算，无需中间标签，并采用在线树构建策略保证探索多样性和计算效率。

Result: 在多个多跳与通用问答基准上，TreePS-RAG在多个模型规模中表现显著优于仅基于结果监督和领先的过程监督强化学习方法，且计算成本与强基线相当。

Conclusion: 提出的TreePS-RAG框架有效解决了多步问答中步骤级奖励稀疏的问题，实现了更细粒度的信号指导，提升了问答性能，具有较高的实用价值和推广潜力。

Abstract: Agentic retrieval-augmented generation (RAG) formulates question answering as a multi-step interaction between reasoning and information retrieval, and has recently been advanced by reinforcement learning (RL) with outcome-based supervision. While effective, relying solely on sparse final rewards limits step-wise credit assignment and provides weak guidance for intermediate reasoning and actions. Recent efforts explore process-level supervision, but typically depend on offline constructed training data, which risks distribution shift, or require costly intermediate annotations. We present TreePS-RAG, an online, tree-based RL framework for agentic RAG that enables step-wise credit assignment while retaining standard outcome-only rewards. Our key insight is to model agentic RAG reasoning as a rollout tree, where each reasoning step naturally maps to a node. This tree structure allows step utility to be estimated via Monte Carlo estimation over its descendant outcomes, yielding fine-grained process advantages without requiring intermediate labels. To make this paradigm practical, we introduce an efficient online tree construction strategy that preserves exploration diversity under a constrained computational budget. With a rollout cost comparable to strong baselines like Search-R1, experiments on seven multi-hop and general QA benchmarks across multiple model scales show that TreePS-RAG consistently and significantly outperforms both outcome-supervised and leading process-supervised RL methods.

</details>


### [75] [Symphonym: Universal Phonetic Embeddings for Cross-Script Toponym Matching via Teacher-Student Distillation](https://arxiv.org/abs/2601.06932)
*Stephen Gadd*

Main category: cs.CL

TL;DR: 提出Symphonym神经嵌入系统，将20种书写系统的地名映射到统一的128维语音空间，实现跨语言与跨文字系统的地名链接，优于传统字符串匹配方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖语言特定的语音算法或音译规则，无法有效处理跨文字系统的地名匹配，存在识别困难。

Method: 设计Teacher网络基于发音特征生成目标嵌入，Student网络从原始字符学习近似嵌入。采用三阶段训练（教师训练、学生对齐、困难样本微调），利用大量多源地名数据训练模型。

Result: 在Hebrew-Arabic跨语言基准测试中达到89.2% Recall@1，显著优于Levenshtein和Jaro-Winkler算法，显示优良的跨文字系统匹配能力。

Conclusion: Symphonym实现了轻量级、高效的跨语音空间地名匹配，能辅助历史地名数据库进行模糊搜索和语音一致性核对，代码和模型开源，便于推广应用。

Abstract: Linking place names across languages and writing systems is a fundamental challenge in digital humanities and geographic information retrieval. Existing approaches rely on language-specific phonetic algorithms or transliteration rules that fail when names cross script boundaries -- no string metric can determine that "Moscow" when rendered in Cyrillic or Arabic refer to the same city.
  I present Symphonym, a neural embedding system that maps toponyms from 20 writing systems into a unified 128-dimensional phonetic space. A Teacher network trained on articulatory phonetic features (via Epitran and PanPhon) produces target embeddings, while a Student network learns to approximate these from raw characters. At inference, only the lightweight Student (1.7M parameters) is required, enabling deployment without runtime phonetic conversion.
  Training uses a three-phase curriculum on 57 million toponyms from GeoNames, Wikidata, and the Getty Thesaurus of Geographic Names. Phase 1 trains the Teacher on 467K phonetically-grounded triplets. Phase 2 aligns the Student to Teacher outputs across 23M samples, achieving 96.6% cosine similarity. Phase 3 fine-tunes on 3.3M hard negative triplets -- negatives sharing prefix and script with the anchor but referring to different places -- to sharpen discrimination.
  Evaluation on the MEHDIE Hebrew-Arabic benchmark achieves 89.2% Recall@1, outperforming Levenshtein (81.5%) and Jaro-Winkler (78.5%). The system is optimised for cross-script matching; same-script variants can be handled by complementary string methods. Symphonym will enable fuzzy phonetic reconciliation and search across the World Historical Gazetteer's 67 million toponyms. Code and models are publicly available.

</details>


### [76] [X-Coder: Advancing Competitive Programming with Fully Synthetic Tasks, Solutions, and Tests](https://arxiv.org/abs/2601.06953)
*Jie Wu,Haoling Li,Xin Zhang,Jiani Guo,Jane Luo,Steven Liu,Yangyu Huang,Ruihang Chu,Scarlett Li,Yujiu Yang*

Main category: cs.CL

TL;DR: 本文提出了利用完全合成的数据来训练代码大型语言模型（Code LLMs），通过特征驱动的合成流水线SynthSmith生成多样且具有挑战性的编程任务和测试数据，实现了无需依赖真实世界数据的代码推理模型训练。


<details>
  <summary>Details</summary>
Motivation: 当前Code LLMs依赖真实世界数据，限制了其可扩展性和挑战性的解决能力。为突破这一瓶颈，本文尝试使用完全合成的数据进行模型训练，以增强模型的代码推理能力。

Method: 提出了SynthSmith合成流水线，基于特征驱动生成任务、解决方案和测试样例，支持监督微调和强化学习。基于此合成数据集训练了X-Coder模型系列，结合合成监督微调和强化学习阶段训练。

Result: X-Coder在LiveCodeBench v5和v6测试中分别达到62.9%和55.8%的通过率，超越参数更多的DeepCoder-14B-Preview和AReal-boba2-14B模型。实验证明合成数据可支持良好的扩展规律，分析了不同扩展维度的效果。

Conclusion: 高质量合成数据与分阶段训练显著提升了代码推理性能，减少了对真实编码数据的依赖，表明合成数据驱动的训练路径具备广阔的发展前景。

Abstract: Competitive programming presents great challenges for Code LLMs due to its intensive reasoning demands and high logical complexity. However, current Code LLMs still rely heavily on real-world data, which limits their scalability. In this paper, we explore a fully synthetic approach: training Code LLMs with entirely generated tasks, solutions, and test cases, to empower code reasoning models without relying on real-world data. To support this, we leverage feature-based synthesis to propose a novel data synthesis pipeline called SynthSmith. SynthSmith shows strong potential in producing diverse and challenging tasks, along with verified solutions and tests, supporting both supervised fine-tuning and reinforcement learning. Based on the proposed synthetic SFT and RL datasets, we introduce the X-Coder model series, which achieves a notable pass rate of 62.9 avg@8 on LiveCodeBench v5 and 55.8 on v6, outperforming DeepCoder-14B-Preview and AReal-boba2-14B despite having only 7B parameters. In-depth analysis reveals that scaling laws hold on our synthetic dataset, and we explore which dimensions are more effective to scale. We further provide insights into code-centric reinforcement learning and highlight the key factors that shape performance through detailed ablations and analysis. Our findings demonstrate that scaling high-quality synthetic data and adopting staged training can greatly advance code reasoning, while mitigating reliance on real-world coding data.

</details>


### [77] [RealMem: Benchmarking LLMs in Real-World Memory-Driven Interaction](https://arxiv.org/abs/2601.06966)
*Haonan Bian,Zhiyuan Yao,Sen Hu,Zishan Xu,Shaolei Zhang,Yifu Guo,Ziliang Yang,Xueran Han,Huacan Wang,Ronghao Chen*

Main category: cs.CL

TL;DR: 本文提出了RealMem基准，针对大型语言模型在长期项目导向交互中的记忆管理问题，构建了真实项目场景的跨会话对话数据集，并设计了综合生成流程以模拟记忆动态演变。实验表明现有记忆系统在管理长期项目状态和动态上下文依赖方面存在显著挑战。


<details>
  <summary>Details</summary>
Motivation: 现有基准大多关注闲聊或任务导向对话，无法反映需要追踪目标演变的长期项目导向交互，难以评估模型的长期记忆效果。

Method: 提出RealMem基准，包含11个真实项目场景，超过2000条跨会话对话，利用项目基础构建、多Agent对话生成及记忆和进度管理的合成流程来模拟记忆动态演变。

Result: 实验结果表明当前记忆系统难以有效管理长期项目状态和动态上下文依赖，反映出现阶段技术的明显不足。

Conclusion: RealMem为评估大型语言模型在长期项目记忆管理方面提供了新的真实场景基准，有助于推动记忆系统性能改进。

Abstract: As Large Language Models (LLMs) evolve from static dialogue interfaces to autonomous general agents, effective memory is paramount to ensuring long-term consistency. However, existing benchmarks primarily focus on casual conversation or task-oriented dialogue, failing to capture **"long-term project-oriented"** interactions where agents must track evolving goals.
  To bridge this gap, we introduce **RealMem**, the first benchmark grounded in realistic project scenarios. RealMem comprises over 2,000 cross-session dialogues across eleven scenarios, utilizing natural user queries for evaluation.
  We propose a synthesis pipeline that integrates Project Foundation Construction, Multi-Agent Dialogue Generation, and Memory and Schedule Management to simulate the dynamic evolution of memory. Experiments reveal that current memory systems face significant challenges in managing the long-term project states and dynamic context dependencies inherent in real-world projects.
  Our code and datasets are available at [https://github.com/AvatarMemory/RealMemBench](https://github.com/AvatarMemory/RealMemBench).

</details>


### [78] [Categorize Early, Integrate Late: Divergent Processing Strategies in Automatic Speech Recognition](https://arxiv.org/abs/2601.06972)
*Nathan Roll,Pranav Bhalerao,Martijn Bartelds,Arjun Pawar,Yuka Tatsumi,Tolulope Ogunremi,Chen Shani,Calbert Graham,Meghan Sumner,Dan Jurafsky*

Main category: cs.CL

TL;DR: 本文提出了一种建筑指纹方法，分析了Transformer和Conformer在语音语言建模中的不同处理策略，揭示了两者在处理层次上的显著差异。


<details>
  <summary>Details</summary>
Motivation: 探究Transformer和Conformer两种主流架构在语音语言模型中性能相当的原因，是由于收敛的处理策略还是不同的架构归纳偏置。

Method: 提出建筑指纹（Architectural Fingerprinting）探测框架，通过对24个预训练编码器（参数规模从39M到3.3B）的控制实验，分析架构对表示的影响。

Result: 发现Conformer早期实现类别划分（如语音音素和说话者性别），而Transformer则延迟整合特征，推迟到较深层编码音素、口音和持续时间。

Conclusion: 两种架构的不同特征编码策略暗示了设计启示：Conformer适合低延迟流处理，Transformer更适合需要丰富上下文和跨语句归一化的任务。

Abstract: In speech language modeling, two architectures dominate the frontier: the Transformer and the Conformer. However, it remains unknown whether their comparable performance stems from convergent processing strategies or distinct architectural inductive biases. We introduce Architectural Fingerprinting, a probing framework that isolates the effect of architecture on representation, and apply it to a controlled suite of 24 pre-trained encoders (39M-3.3B parameters). Our analysis reveals divergent hierarchies: Conformers implement a "Categorize Early" strategy, resolving phoneme categories 29% earlier in depth and speaker gender by 16% depth. In contrast, Transformers "Integrate Late," deferring phoneme, accent, and duration encoding to deep layers (49-57%). These fingerprints suggest design heuristics: Conformers' front-loaded categorization may benefit low-latency streaming, while Transformers' deep integration may favor tasks requiring rich context and cross-utterance normalization.

</details>


### [79] [LLMs Can't Play Hangman: On the Necessity of a Private Working Memory for Language Agents](https://arxiv.org/abs/2601.06973)
*Davide Baldelli,Ali Parviz,Amal Zouaq,Sarath Chandar*

Main category: cs.CL

TL;DR: 本文指出现有大型语言模型（LLMs）因缺乏私有工作记忆，无法在需要维护隐藏状态的交互任务中同时保证秘密性和一致性，并提出了一种带有私有工作记忆的新架构以解决该问题。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型从文本生成转向自主代理，其受限于传统聊天界面而缺乏私有工作记忆，导致无法可靠地执行依赖隐藏状态的交互任务。

Method: 定义了私有状态交互任务（PSITs）并提出不可能性定理，设计了一种自我一致性测试协议评估模型维护隐藏秘密的能力；最后提出一种包含明确私有工作记忆的新架构。

Result: 标准聊天型LLMs和基于检索的记忆方法均未通过隐秘状态维护的测试，验证了传统方法在维护隐藏状态上的不足，而新架构成功恢复了一致性。

Conclusion: 私有状态是实现可靠交互语言代理的必要组成部分，推荐引入私有工作记忆以提升模型在保持隐藏信息时的表现。

Abstract: As LLMs move from text completion toward autonomous agents, they remain constrained by the standard chat interface, which lacks private working memory. This raises a fundamental question: can agents reliably perform interactive tasks that depend on hidden state? We define Private State Interactive Tasks (PSITs), which require agents to generate and maintain hidden information while producing consistent public responses. We show theoretically that any agent restricted to the public conversation history cannot simultaneously preserve secrecy and consistency in PSITs, yielding an impossibility theorem. To empirically validate this limitation, we introduce a self-consistency testing protocol that evaluates whether agents can maintain a hidden secret across forked dialogue branches. Standard chat-based LLMs and retrieval-based memory baselines fail this test regardless of scale, demonstrating that semantic retrieval does not enable true state maintenance. To address this, we propose a novel architecture incorporating an explicit private working memory; we demonstrate that this mechanism restores consistency, establishing private state as a necessary component for interactive language agents.

</details>


### [80] [UETQuintet at BioCreative IX - MedHopQA: Enhancing Biomedical QA with Selective Multi-hop Reasoning and Contextual Retrieval](https://arxiv.org/abs/2601.06974)
*Quoc-An Nguyen,Thi-Minh-Thu Vu,Bich-Dat Nguyen,Dinh-Quang-Minh Tran,Hoang-Quynh Le*

Main category: cs.CL

TL;DR: 本文提出了一种针对生物医学问答的模型，能有效处理直接和多步骤问题，并结合多源信息检索和上下文学习，在MedHopQA任务中取得了0.84的准确率，排名第二。


<details>
  <summary>Details</summary>
Motivation: 生物医学问答系统在处理复杂医疗查询时面临多跳推理和数据复杂性挑战。

Method: 针对序贯问题进行子问题链式推理，直接问题直接处理；结合多源信息检索与上下文学习提供丰富背景。

Result: 模型在BioCreative IX MedHopQA数据集上取得0.84的Exact Match分数，排名第二。

Conclusion: 该模型能够有效应对生物医学问答挑战，提升医疗研究和实践的问答能力。

Abstract: Biomedical Question Answering systems play a critical role in processing complex medical queries, yet they often struggle with the intricate nature of medical data and the demand for multi-hop reasoning. In this paper, we propose a model designed to effectively address both direct and sequential questions. While sequential questions are decomposed into a chain of sub-questions to perform reasoning across a chain of steps, direct questions are processed directly to ensure efficiency and minimise processing overhead. Additionally, we leverage multi-source information retrieval and in-context learning to provide rich, relevant context for generating answers. We evaluated our model on the BioCreative IX - MedHopQA Shared Task datasets. Our approach achieves an Exact Match score of 0.84, ranking second on the current leaderboard. These results highlight the model's capability to meet the challenges of Biomedical Question Answering, offering a versatile solution for advancing medical research and practice.

</details>


### [81] [MedTutor: A Retrieval-Augmented LLM System for Case-Based Medical Education](https://arxiv.org/abs/2601.06979)
*Dongsuk Jang,Ziyao Shangguan,Kyle Tegtmeyer,Anurag Gupta,Jan Czerminski,Sophie Chheang,Arman Cohan*

Main category: cs.CL

TL;DR: 提出了MedTutor系统，通过RAG技术自动生成基于证据的医学教育内容和选择题，辅助医学生快速高效学习临床案例。


<details>
  <summary>Details</summary>
Motivation: 医学住院医师学习临床案例时，难以快速找到相关且权威的教育材料和证据，影响学习效果。

Method: 设计了基于检索增强生成（RAG）的系统MedTutor，结合本地医学教材库及学术文献API动态检索最新资料，经重排模型筛选后用大语言模型生成长文本教育内容和选择题。

Result: 放射科专家评估结果显示生成内容临床及教育价值高，大规模评估中LLM-judge与专家判断结果中度一致，专家监督仍不可缺。

Conclusion: MedTutor有效提升了住院医师学习效率和内容质量，但自动评估需要专家参与保障准确性。

Abstract: The learning process for medical residents presents significant challenges, demanding both the ability to interpret complex case reports and the rapid acquisition of accurate medical knowledge from reliable sources. Residents typically study case reports and engage in discussions with peers and mentors, but finding relevant educational materials and evidence to support their learning from these cases is often time-consuming and challenging. To address this, we introduce MedTutor, a novel system designed to augment resident training by automatically generating evidence-based educational content and multiple-choice questions from clinical case reports. MedTutor leverages a Retrieval-Augmented Generation (RAG) pipeline that takes clinical case reports as input and produces targeted educational materials. The system's architecture features a hybrid retrieval mechanism that synergistically queries a local knowledge base of medical textbooks and academic literature (using PubMed, Semantic Scholar APIs) for the latest related research, ensuring the generated content is both foundationally sound and current. The retrieved evidence is filtered and ordered using a state-of-the-art reranking model and then an LLM generates the final long-form output describing the main educational content regarding the case-report. We conduct a rigorous evaluation of the system. First, three radiologists assessed the quality of outputs, finding them to be of high clinical and educational value. Second, we perform a large scale evaluation using an LLM-as-a Judge to understand if LLMs can be used to evaluate the output of the system. Our analysis using correlation between LLMs outputs and human expert judgments reveals a moderate alignment and highlights the continued necessity of expert oversight.

</details>


### [82] [Lexicalized Constituency Parsing for Middle Dutch: Low-resource Training and Cross-Domain Generalization](https://arxiv.org/abs/2601.07008)
*Yiming Liang,Fang Zhao*

Main category: cs.CL

TL;DR: 本文将基于Transformer的成分句法分析器应用于中古荷兰语，采用多语言联合训练和多领域数据微调来提升解析性能，显著优于传统PCFG解析器。


<details>
  <summary>Details</summary>
Motivation: 现有研究多聚焦于依存句法分析，低资源历史语言（如中古荷兰语）的成分句法解析尚缺乏研究，且语言数据稀缺、领域异质性高，迫切需要提升成分解析性能。

Method: 采用基于Transformer的成分句法分析器，通过多资源辅助语言进行联合训练，利用新标注的多领域数据进行微调和数据组合，探索特征分离技术进行领域适应。

Result: 联合训练显著提升F1分数（最高提升0.73），辅助语言越接近中古荷兰语效果越好。多领域数据微调和组合均带来类似提升，且神经网络解析器性能持续优于传统PCFG解析器。域适应中需至少200条样本才能有效增强跨域表现。

Conclusion: 基于Transformer的神经网络模型结合多语言联合训练和多领域数据微调，是提升低资源、高异质性历史语言成分句法解析性能的有效策略。

Abstract: Recent years have seen growing interest in applying neural networks and contextualized word embeddings to the parsing of historical languages. However, most advances have focused on dependency parsing, while constituency parsing for low-resource historical languages like Middle Dutch has received little attention. In this paper, we adapt a transformer-based constituency parser to Middle Dutch, a highly heterogeneous and low-resource language, and investigate methods to improve both its in-domain and cross-domain performance. We show that joint training with higher-resource auxiliary languages increases F1 scores by up to 0.73, with the greatest gains achieved from languages that are geographically and temporally closer to Middle Dutch. We further evaluate strategies for leveraging newly annotated data from additional domains, finding that fine-tuning and data combination yield comparable improvements, and our neural parser consistently outperforms the currently used PCFG-based parser for Middle Dutch. We further explore feature-separation techniques for domain adaptation and demonstrate that a minimum threshold of approximately 200 examples per domain is needed to effectively enhance cross-domain performance.

</details>


### [83] [TurkBench: A Benchmark for Evaluating Turkish Large Language Models](https://arxiv.org/abs/2601.07020)
*Çağrı Toraman,Ahmet Kaan Sever,Ayse Aysu Cengiz,Elif Ecem Arslan,Görkem Sevinç,Mete Mert Birdal,Yusuf Faruk Güldemir,Ali Buğra Kanburoğlu,Sezen Felekoğlu,Osman Gürlek,Sarp Kantar,Birsen Şahin Kütük,Büşra Tufan,Elif Genç,Serkan Coşkun,Gupse Ekin Demir,Muhammed Emin Arayıcı,Olgun Dursun,Onur Gungor,Susan Üsküdarlı,Abdullah Topraksoy,Esra Darıcı*

Main category: cs.CL

TL;DR: 本文介绍了一个针对土耳其语大规模生成语言模型的综合评测基准TurkBench，涵盖8151个样本和21个子任务，评估模型的知识、理解、推理、内容审核、土耳其语语法词汇和指令执行能力。


<details>
  <summary>Details</summary>
Motivation: 当前大规模语言模型的评测多集中于英语，缺乏针对土耳其语这样具有独特语言特点的语言的全面评测工具。

Method: 构建TurkBench基准，包含六大类21个子任务，共8151个数据样本，涵盖知识、语言理解、推理、内容审核、土耳其语语法词汇和指令遵循多方面内容。

Result: 提供了一个覆盖丰富任务和文化背景的土耳其语评测数据集，并开放在线提交平台供研究者评测和改进模型。

Conclusion: TurkBench填补了土耳其语生成大语言模型评测的空白，为相关研究者和开发者提供了有效评估与优化工具，可以推动土耳其语语言模型的发展。

Abstract: With the recent surge in the development of large language models, the need for comprehensive and language-specific evaluation benchmarks has become critical. While significant progress has been made in evaluating English language models, benchmarks for other languages, particularly those with unique linguistic characteristics such as Turkish, remain less developed. Our study introduces TurkBench, a comprehensive benchmark designed to assess the capabilities of generative large language models in the Turkish language. TurkBench involves 8,151 data samples across 21 distinct subtasks. These are organized under six main categories of evaluation: Knowledge, Language Understanding, Reasoning, Content Moderation, Turkish Grammar and Vocabulary, and Instruction Following. The diverse range of tasks and the culturally relevant data would provide researchers and developers with a valuable tool for evaluating their models and identifying areas for improvement. We further publish our benchmark for online submissions at https://huggingface.co/turkbench

</details>


### [84] [Solar Open Technical Report](https://arxiv.org/abs/2601.07022)
*Sungrae Park,Sanghoon Kim,Jungho Cho,Gyoungjin Gim,Dawoon Jung,Mikyoung Cha,Eunhae Choo,Taekgyu Hong,Minbyul Jeong,SeHwan Joo,Minsoo Khang,Eunwon Kim,Minjeong Kim,Sujeong Kim,Yunsu Kim,Hyeonju Lee,Seunghyun Lee,Sukyung Lee,Siyoung Park,Gyungin Shin,Inseo Song,Wonho Song,Seonghoon Yang,Seungyoun Yi,Sanghoon Yoon,Jeonghyun Ko,Seyoung Song,Keunwoo Choi,Hwalsuk Lee,Sunghun Kim,Du-Seong Chang,Kyunghyun Cho,Junsuk Choe,Hwaran Lee,Jae-Gil Lee,KyungTae Lim,Alice Oh*

Main category: cs.CL

TL;DR: 本文提出了Solar Open，一种拥有1020亿参数的双语专家混合语言模型，针对资源匮乏语言进行了高质量数据合成和优化训练，展示了有效提升小语种大模型性能的方法。


<details>
  <summary>Details</summary>
Motivation: 当前许多小语种因训练数据匮乏，难以训练出性能竞争力的大型语言模型。为了推动资源匮乏语言的AI发展，需要系统化方法解决数据不足和训练难题。

Method: 本文合成了4.5万亿高质量领域特定且强化学习导向的数据，采用逐步课程策略联合优化数据组成、质量门槛和领域覆盖，训练总量达20万亿tokens。同时提出SnapPO框架，提升强化学习优化效率以增强模型推理能力。

Result: 在英语和韩语基准测试中，Solar Open表现出竞争力，证明了所提方法在提升资源匮乏语言大型模型性能上的有效性。

Conclusion: 本文展示了一套系统方法，通过高质量数据合成、课程训练和高效强化学习，成功构建了适合小语种的高性能大规模双语专家模型，为小语种AI发展提供了新思路。

Abstract: We introduce Solar Open, a 102B-parameter bilingual Mixture-of-Experts language model for underserved languages. Solar Open demonstrates a systematic methodology for building competitive LLMs by addressing three interconnected challenges. First, to train effectively despite data scarcity for underserved languages, we synthesize 4.5T tokens of high-quality, domain-specific, and RL-oriented data. Second, we coordinate this data through a progressive curriculum jointly optimizing composition, quality thresholds, and domain coverage across 20 trillion tokens. Third, to enable reasoning capabilities through scalable RL, we apply our proposed framework SnapPO for efficient optimization. Across benchmarks in English and Korean, Solar Open achieves competitive performance, demonstrating the effectiveness of this methodology for underserved language AI development.

</details>


### [85] [Codified Foreshadowing-Payoff Text Generation](https://arxiv.org/abs/2601.07033)
*Longfei Yun,Kun Zhou,Yupeng Hou,Letian Peng,Jingbo Shang*

Main category: cs.CL

TL;DR: 本文提出CFPG框架，利用因果结构确保大语言模型生成故事时前后呼应的铺垫与回报，提高叙事连贯性和逻辑性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型常常无法有效实现故事中铺垫与回报的长程依赖，导致铺垫未被合理兑现，评估多聚焦于表面连贯性，忽视叙事结构完整性。

Method: 提出Codified Foreshadowing-Payoff Generation（CFPG）框架，将叙事连续性转化为可执行的因果谓词，通过从BookSum语料库挖掘并编码铺垫-触发-回报三元组，提供结构化监督，确保叙事承诺得到时间和逻辑上的实现。

Result: 实验结果显示，CFPG在回报准确性和叙事匹配度方面显著优于传统提示方法。

Conclusion: 明确编码叙事机制对于提升大语言模型的叙事能力至关重要，有助于其从表面流畅度迈向真正的叙事胜任力。

Abstract: Foreshadowing and payoff are ubiquitous narrative devices through which authors introduce commitments early in a story and resolve them through concrete, observable outcomes. However, despite advances in story generation, large language models (LLMs) frequently fail to bridge these long-range narrative dependencies, often leaving "Chekhov's guns" unfired even when the necessary context is present. Existing evaluations largely overlook this structural failure, focusing on surface-level coherence rather than the logical fulfillment of narrative setups. In this paper, we introduce Codified Foreshadowing-Payoff Generation (CFPG), a novel framework that reframes narrative quality through the lens of payoff realization. Recognizing that LLMs struggle to intuitively grasp the "triggering mechanism" of a foreshadowed event, CFPG transforms narrative continuity into a set of executable causal predicates. By mining and encoding Foreshadow-Trigger-Payoff triples from the BookSum corpus, we provide structured supervision that ensures foreshadowed commitments are not only mentioned but also temporally and logically fulfilled. Experiments demonstrate that CFPG significantly outperforms standard prompting baselines in payoff accuracy and narrative alignment. Our findings suggest that explicitly codifying narrative mechanics is essential for moving LLMs from surface-level fluency to genuine narrative competence.

</details>


### [86] [Mid-Think: Training-Free Intermediate-Budget Reasoning via Token-Level Triggers](https://arxiv.org/abs/2601.07036)
*Wang Yang,Debargha Ganguly,Xinpeng Li,Chaoda Song,Shouren Wang,Vikash Singh,Vipin Chaudhary,Xiaotian Han*

Main category: cs.CL

TL;DR: 本文发现混合推理语言模型的推理行为主要由少量触发词控制，提出了无需训练的Mid-Think提示格式以调节推理预算，提高了推理准确性和训练效率。


<details>
  <summary>Details</summary>
Motivation: 目前推理语言模型通过高层指令切换模式来调节推理行为，但该切换主要依赖少数触发词，缺乏对触发机制的深入理解和利用。

Method: 通过注意力分析和控制提示实验，确定了触发推理行为的"Okay"词和抑制推理的换行符模式，基于此设计Mid-Think提示格式，结合触发词实现中等预算的推理控制。

Result: Mid-Think在准确率与推理长度的权衡上优于固定词和提示基线，在经过SFT后用于RL训练可减少约15%训练时间，并提升Qwen3-8B模型在AIME和GPQA数据集上的性能。

Conclusion: Mid-Think提示格式有效利用触发词控制推理行为，实现了推理过程的高效调节，提升了推理性能和训练效率，适用于推理时间控制和基于RL的推理训练。

Abstract: Hybrid reasoning language models are commonly controlled through high-level Think/No-think instructions to regulate reasoning behavior, yet we found that such mode switching is largely driven by a small set of trigger tokens rather than the instructions themselves. Through attention analysis and controlled prompting experiments, we show that a leading ``Okay'' token induces reasoning behavior, while the newline pattern following ``</think>'' suppresses it. Based on this observation, we propose Mid-Think, a simple training-free prompting format that combines these triggers to achieve intermediate-budget reasoning, consistently outperforming fixed-token and prompt-based baselines in terms of the accuracy-length trade-off. Furthermore, applying Mid-Think to RL training after SFT reduces training time by approximately 15% while improving final performance of Qwen3-8B on AIME from 69.8% to 72.4% and on GPQA from 58.5% to 61.1%, demonstrating its effectiveness for both inference-time control and RL-based reasoning training.

</details>


### [87] [Task Arithmetic with Support Languages for Low-Resource ASR](https://arxiv.org/abs/2601.07038)
*Emma Rafkin,Dan DeGenaro,Xiulin Yang*

Main category: cs.CL

TL;DR: 本文通过将高资源语言与低资源语言的任务向量线性结合，提升了低资源语言的自动语音识别性能。


<details>
  <summary>Details</summary>
Motivation: 很多低资源语言缺乏足够训练数据，导致自动语音识别效果不佳。利用高资源语言数据结合低资源语言模型，能改进低资源语言的识别效果。

Method: 将对每种语言的训练视为一个任务，生成Whisper ASR系统微调后的任务向量。对高低资源语言的任务向量进行线性加权组合，权重通过在低资源语言验证集上的词错误率优化确定。

Result: 该方法在目标低资源语言上实现了性能的持续提升。

Conclusion: 通过任务算术结合高低资源语言的模型，可以有效缓解低资源语言ASR中的数据不足问题，提升识别准确率。

Abstract: The development of resource-constrained approaches to automatic speech recognition (ASR) is of great interest due to its broad applicability to many low-resource languages for which there is scant usable data. Existing approaches to many low-resource natural language processing tasks leverage additional data from higher-resource languages that are closely related to a target low-resource language. One increasingly popular approach uses task arithmetic to combine models trained on different tasks to create a model for a task where there is little to no training data. In this paper, we consider training on a particular language to be a task, and we generate task vectors by fine-tuning variants of the Whisper ASR system. For pairings of high- and low-resource languages, we merge task vectors via a linear combination, optimizing the weights of the linear combination on the downstream word error rate on the low-resource target language's validation set. We find that this approach consistently improves performance on the target languages.

</details>


### [88] [When Abundance Conceals Weakness: Knowledge Conflict in Multilingual Models](https://arxiv.org/abs/2601.07041)
*Jiaqi Zhao,Qiang Huang,Haodong Chen,Xiaoxing You,Jun Yu*

Main category: cs.CL

TL;DR: 本文提出了CLEAR框架，系统研究多语言大型语言模型（LLMs）如何处理不同语言之间的知识冲突问题。通过构建多语言版本的冲突问答数据集，评估了六种代表性模型，发现解决冲突时，高资源语言在推理任务中有优势，而语言亲缘关系在实体事实冲突中更重要。


<details>
  <summary>Details</summary>
Motivation: 多语言大型语言模型内部知识在不同语言间分布不均，当外部证据与语言相关的内部知识冲突时，尚缺乏系统研究，尤其是在非英语中心背景下的跨语言知识冲突。

Method: 提出CLEAR跨语言知识冲突评估框架，将冲突解决分为四个层次，从模型内部知识调用到多源跨语言归纳；构建涵盖10种语言的多语言版本ConflictQA和ConflictingQA问答基准，对六个代表性LLM进行系统评估。

Result: 实验结果表明，在推理密集任务中，冲突解决受语言资源丰富度主导，高资源语言更具说服力；而在实体事实冲突中，语言亲缘关系起决定作用，语言亲近但资源低的语言能胜过遥远但资源高的语言。

Conclusion: 多语言LLMs在处理跨语言知识冲突时表现出任务依赖性决策差异，资源量和语言亲缘关系分别影响不同类型冲突的解决机制，CLEAR框架为研究跨语言知识冲突提供了系统工具和基准。

Abstract: Large Language Models (LLMs) encode vast world knowledge across multiple languages, yet their internal beliefs are often unevenly distributed across linguistic spaces. When external evidence contradicts these language-dependent memories, models encounter \emph{cross-lingual knowledge conflict}, a phenomenon largely unexplored beyond English-centric settings. We introduce \textbf{CLEAR}, a \textbf{C}ross-\textbf{L}ingual knowl\textbf{E}dge conflict ev\textbf{A}luation f\textbf{R}amework that systematically examines how multilingual LLMs reconcile conflicting internal beliefs and multilingual external evidence. CLEAR decomposes conflict resolution into four progressive scenarios, from multilingual parametric elicitation to competitive multi-source cross-lingual induction, and systematically evaluates model behavior across two complementary QA benchmarks with distinct task characteristics. We construct multilingual versions of ConflictQA and ConflictingQA covering 10 typologically diverse languages and evaluate six representative LLMs. Our experiments reveal a task-dependent decision dichotomy. In reasoning-intensive tasks, conflict resolution is dominated by language resource abundance, with high-resource languages exerting stronger persuasive power. In contrast, for entity-centric factual conflicts, linguistic affinity, not resource scale, becomes decisive, allowing low-resource but linguistically aligned languages to outperform distant high-resource ones.

</details>


### [89] [Engineering of Hallucination in Generative AI: It's not a Bug, it's a Feature](https://arxiv.org/abs/2601.07046)
*Tim Fingscheidt,Patrick Blumenberg,Björn Möller*

Main category: cs.CL

TL;DR: 生成式人工智能模型在训练数据基础上工作，但适度幻想（幻觉）反而提升其表现。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI在实际应用中为何适度的幻想（幻觉）反而有助于模型表现而非缺陷。

Method: 总结概率工程技术，指导如何鼓励生成式AI适度产生幻想，从而得到理想输出。

Result: 适度的幻想（幻觉）能够改善生成式AI的回答质量，有助于实现期望效果。

Conclusion: 生成式AI中的幻觉不应被视为缺陷，而可能是一种重要功能，有助于提升模型的表现。

Abstract: Generative artificial intelligence (AI) is conquering our lives at lightning speed. Large language models such as ChatGPT answer our questions or write texts for us, large computer vision models such as GAIA-1 generate videos on the basis of text descriptions or continue prompted videos. These neural network models are trained using large amounts of text or video data, strictly according to the real data employed in training. However, there is a surprising observation: When we use these models, they only function satisfactorily when they are allowed a certain degree of fantasy (hallucination). While hallucination usually has a negative connotation in generative AI - after all, ChatGPT is expected to give a fact-based answer! - this article recapitulates some simple means of probability engineering that can be used to encourage generative AI to hallucinate to a limited extent and thus lead to the desired results. We have to ask ourselves: Is hallucination in gen-erative AI probably not a bug, but rather a feature?

</details>


### [90] [Fine-Tuning vs. RAG for Multi-Hop Question Answering with Novel Knowledge](https://arxiv.org/abs/2601.07054)
*Zhuoyi Yang,Yurun Song,Iftekhar Ahmed,Ian Harris*

Main category: cs.CL

TL;DR: 本文系统比较了多跳问答中不同知识注入方法的效果，发现检索增强生成方法在处理时间上新颖知识时表现最佳。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在多跳问答中的推理能力，特别是在处理时效性知识时，不同知识注入方式的相对效果尚不明确。

Method: 对三个7B参数的开源LLM进行了无监督微调、有监督微调和检索增强生成三种知识注入方式的比较，测试数据包括标准多跳科学问答QASC和涵盖2024年维基百科事件的新型数据集。

Result: 无监督微调提升有限，检索增强生成在处理时间新颖知识时表现显著提升，有监督微调整体准确率最高。

Conclusion: 不同知识注入机制支持多跳问答的效果存在根本差异，检索基方法在需要外部或组合知识的场景中尤为重要。

Abstract: Multi-hop question answering is widely used to evaluate the reasoning capabilities of large language models (LLMs), as it requires integrating multiple pieces of supporting knowledge to arrive at a correct answer. While prior work has explored different mechanisms for providing knowledge to LLMs, such as finetuning and retrieval-augmented generation (RAG), their relative effectiveness for multi-hop question answering remains insufficiently understood, particularly when the required knowledge is temporally novel.
  In this paper, we systematically compare parametric and non-parametric knowledge injection methods for open-domain multi-hop question answering. We evaluate unsupervised fine-tuning (continual pretraining), supervised fine-tuning, and retrieval-augmented generation across three 7B-parameter open-source LLMs. Experiments are conducted on two benchmarks: QASC, a standard multi-hop science question answering dataset, and a newly constructed dataset of over 10,000 multi-hop questions derived from Wikipedia events in 2024, designed to test knowledge beyond the models' pretraining cutoff.
  Our results show that unsupervised fine-tuning provides only limited gains over base models, suggesting that continual pretraining alone is insufficient for improving multi-hop reasoning accuracy. In contrast, retrieval-augmented generation yields substantial and consistent improvements, particularly when answering questions that rely on temporally novel information. Supervised fine-tuning achieves the highest overall accuracy across models and datasets. These findings highlight fundamental differences in how knowledge injection mechanisms support multi-hop question answering and underscore the importance of retrieval-based methods when external or compositional knowledge is required.

</details>


### [91] [The Need for a Socially-Grounded Persona Framework for User Simulation](https://arxiv.org/abs/2601.07110)
*Pranav Narayanan Venkit,Yu Li,Yada Pruksachatkun,Chien-Sheng Wu*

Main category: cs.CL

TL;DR: 本文提出了基于社会心理学的SCOPE框架来构建和评估虚拟角色，发现仅用人口统计学属性构建的角色预测力有限，加入社会心理学特征后能显著提升行为预测和减少偏差。


<details>
  <summary>Details</summary>
Motivation: 目前大多数大语言模型的虚拟角色都是基于粗略的人口统计学信息构建，导致行为预测能力有限和偏差较大，亟需引入更精细的社会心理学特征来提升角色质量。

Method: 基于124位美国参与者，采集包含141项内容、历时两小时的社会心理学协议，构建SCOPE框架，通过七种模型对比分析人口统计学与社会心理特征在角色构建中的作用，并在SimBench数据集上进行验证。

Result: 人口统计学信息仅能解释约1.5%的行为差异，加入社会心理学特征后模型预测能力明显提升，非人口统计的价值观和身份特征使得角色更准确且偏差更小。SCOPE角色在多项评测中优于默认提示和NVIDIA Nemotron角色，并能增强后者表现。

Conclusion: 虚拟角色的质量更多依赖于丰富的社会心理结构，而非简单的人口统计学模板或摘要，建议未来角色构建应重视社会心理特征。

Abstract: Synthetic personas are widely used to condition large language models (LLMs) for social simulation, yet most personas are still constructed from coarse sociodemographic attributes or summaries. We revisit persona creation by introducing SCOPE, a socially grounded framework for persona construction and evaluation, built from a 141-item, two-hour sociopsychological protocol collected from 124 U.S.-based participants. Across seven models, we find that demographic-only personas are a structural bottleneck: demographics explain only ~1.5% of variance in human response similarity. Adding sociopsychological facets improves behavioral prediction and reduces over-accentuation, and non-demographic personas based on values and identity achieve strong alignment with substantially lower bias. These trends generalize to SimBench (441 aligned questions), where SCOPE personas outperform default prompting and NVIDIA Nemotron personas, and SCOPE augmentation improves Nemotron-based personas. Our results indicate that persona quality depends on sociopsychological structure rather than demographic templates or summaries.

</details>


### [92] [ReMIND: Orchestrating Modular Large Language Models for Controllable Serendipity A REM-Inspired System Design for Emergent Creative Ideation](https://arxiv.org/abs/2601.07121)
*Makoto Sato*

Main category: cs.CL

TL;DR: 本文提出了ReMIND框架，通过模块化设计实现大语言模型的创新思维生成，平衡新颖性与一致性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在生成新颖但连贯的创新观点方面存在困难，随机采样虽有助于新颖性但影响一致性。

Method: 设计了ReMIND框架，包含唤醒、梦境、评判和再唤醒四个阶段，每阶段使用独立大语言模型分别负责稳定基线生成、高温探索生成、粗略筛选以及最终整合。

Result: 实验表明ReMIND在保持语义探索的同时保证了生成结果的稳定性，且高质量创意偶发出现，验证了系统设计在促进创新思维中的有效性。

Conclusion: 创新性思维是稀有事件过程，需要系统层面设计促进其产生和稳定，ReMIND作为模块化框架为研究偶然创新提供了新思路。

Abstract: Large language models (LLMs) are used not only for problem solving but also for creative ideation; however, eliciting serendipitous insights that are both novel and internally coherent remains difficult. While stochastic sampling promotes novelty, it often degrades consistency. Here, we propose ReMIND, a REM-inspired modular framework for ideation. ReMIND consists of four stages: wake, which generates a stable low-temperature semantic baseline; dream, which performs high-temperature exploratory generation; judge, which applies coarse evaluation to filter incoherent outputs and extract candidate ideas; and re-wake, which re-articulates selected ideas into coherent final outputs. By instantiating each stage as an independent LLM, ReMIND enables functional separation between exploration and consolidation. Parameter sweeps show that ReMIND reliably induces semantic exploration while preserving downstream stability. Embedding-based analyses confirm substantial semantic displacement during the dream phase, whereas external evaluations reveal that high-quality ideas emerge sporadically rather than as extrema along any single metric. These results suggest that serendipitous ideation in LLMs is a rare-event process best approached through system level design that shapes the conditions under which valuable ideas can emerge and be stabilized. ReMIND provides a general framework for studying the computational basis of serendipity and illustrates how modular LLM orchestration can bridge exploration and stabilization.

</details>


### [93] [Measuring Iterative Temporal Reasoning with TimePuzzles](https://arxiv.org/abs/2601.07148)
*Zhengxiang Wang,Zeyu Dong*

Main category: cs.CL

TL;DR: 本文提出了TimePuzzles，一个基于约束的日期推断任务，用以评估语言模型的迭代时间推理能力。在13个大型语言模型测试中，模型表现普遍较差，最高准确率仅49.3%。使用工具如网络搜索能提升部分性能，但模型在直接推理上的能力仍显不足。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在复杂时间推理任务中的能力尚未被充分评估，尤其是在迭代时间推理场景下。通过设计一套既简单又具有挑战性的测试，可以更好地诊断模型的时间推理能力及其对工具的使用效果。

Method: 设计TimePuzzles任务，结合事实时间锚点和跨文化日历关系，生成带有一个或多个有效日期解的约束问题。该数据集算法生成，支持动态和持续评测。测试了13个语言模型在无工具、网络搜索和代码解释器辅助下的表现。

Result: 所有模型在无辅助工具下表现较差，最高准确率为GPT-5的49.3%。网络搜索能显著提升准确率，代码解释器辅助效果不一。模型在使用约束时若改写为明确日期，表现大幅提升，显现出模型在解读和利用工具上的差距。

Conclusion: TimePuzzles提供了一种简单且经济的工具辅助迭代时间推理诊断手段，揭示了当前语言模型在时间推理和工具使用上的不足，为未来改进模型能力指明方向。

Abstract: We introduce TimePuzzles, a constraint-based date inference task for evaluating iterative temporal reasoning. Each puzzle combines factual temporal anchors with (cross-cultural) calendar relations, admits one or multiple valid solution dates, and is algorithmically generated for controlled, dynamic, and continual evaluation. Across 13 diverse LLMs, TimePuzzles well distinguishes their iterative temporal reasoning capabilities and remains challenging without tools: GPT-5 reaches only 49.3% accuracy and all other models stay below 31%, despite the dataset's simplicity. Web search consistently yields substantial gains and using code interpreter shows mixed effects, but all models perform much better when constraints are rewritten with explicit dates, revealing a gap in reliable tool use. Overall, TimePuzzles presents a simple, cost-effective diagnostic for tool-augmented iterative temporal reasoning.

</details>


### [94] [Can Large Language Models Understand, Reason About, and Generate Code-Switched Text?](https://arxiv.org/abs/2601.07153)
*Genta Indra Winata,David Anugraha,Patrick Amadeus Irawan,Anirban Das,Haneul Yoo,Paresh Dashore,Shreyas Kulkarni,Ruochen Zhang,Haruki Sakajo,Frederikus Hudi,Anaelia Ovalle,Syrielle Montariol,Felix Gaschi,Michael Anugraha,Rutuj Ravindra Puranik,Zawad Hayat Ahmed,Adril Putra Merin,Emmanuele Chersoni*

Main category: cs.CL

TL;DR: 本文评估了大型语言模型在混合语言环境下的理解、推理与生成能力，提出了包含16种代码混合语言对的新基准CodeMixQA，并揭示现有模型在代码转换任务中的不足。


<details>
  <summary>Details</summary>
Motivation: 多语言交流中代码转换现象普遍，但大型语言模型在此类混合语言设置下的表现尚不明确，亟需系统评测。

Method: 构建了高质量人工标注的CodeMixQA基准数据集，包括多地域多样化的代码转换语言对及其原始与音译形式，分析模型在代码转换问答任务中的推理行为，并系统评测模型生成的代码转换文本在自然性和语义一致性上的表现。

Result: 发现当前大型语言模型在代码转换条件下推理与生成能力存在显著挑战，生成的混合语文本在自然度和语义保真度方面均有不足。

Conclusion: 代码转换任务对大型语言模型提出了新的挑战，研究结果为打造更稳健的多语言模型提供了重要指导，且发布了数据集和代码促进后续研究。

Abstract: Code-switching is a pervasive phenomenon in multilingual communication, yet the robustness of large language models (LLMs) in mixed-language settings remains insufficiently understood. In this work, we present a comprehensive evaluation of LLM capabilities in understanding, reasoning over, and generating code-switched text. We introduce CodeMixQA a novel benchmark with high-quality human annotations, comprising 16 diverse parallel code-switched language-pair variants that span multiple geographic regions and code-switching patterns, and include both original scripts and their transliterated forms. Using this benchmark, we analyze the reasoning behavior of LLMs on code-switched question-answering tasks, shedding light on how models process and reason over mixed-language inputs. We further conduct a systematic evaluation of LLM-generated synthetic code-switched text, focusing on both naturalness and semantic fidelity, and uncover key limitations in current generation capabilities. Our findings reveal persistent challenges in both reasoning and generation under code-switching conditions and provide actionable insights for building more robust multilingual LLMs. We release the dataset and code as open source.

</details>


### [95] [Structured Reasoning for Large Language Models](https://arxiv.org/abs/2601.07180)
*Jinyi Han,Zixiang Di,Zishang Jiang,Ying Liao,Jiaqing Liang,Yongqi Wang,Yanghua Xiao*

Main category: cs.CL

TL;DR: 本文提出了一种名为Structured Reasoning (SCR) 的框架，通过结构化推理轨迹和分阶段训练，提升大语言模型推理效率，减少冗余步骤。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然能生成长链条推理，但长推理轨迹中常包含冗余或无效步骤，且缺乏针对关键推理能力的监督，导致过度核查和修正。

Method: SCR框架采用生成-验证-修正范式，构建结构化训练数据，利用动态终止监督引导模型判断何时停止推理，采用两阶段渐进式强化学习分别训练生成与自我验证阶段及修正阶段。

Result: 在三个主流模型上的大量实验表明，SCR显著提升了推理效率和自我验证能力，且相比其他推理范式，输出令牌长度最多减少50%。

Conclusion: SCR通过结构化拆分和针对性训练有效减少了冗余推理步骤，提高推理效率和准确性，是提升大语言模型推理能力的有效方案。

Abstract: Large language models (LLMs) achieve strong performance by generating long chains of thought, but longer traces always introduce redundant or ineffective reasoning steps. One typical behavior is that they often perform unnecessary verification and revisions even if they have reached the correct answers. This limitation stems from the unstructured nature of reasoning trajectories and the lack of targeted supervision for critical reasoning abilities. To address this, we propose Structured Reasoning (SCR), a framework that decouples reasoning trajectories into explicit, evaluable, and trainable components. We mainly implement SCR using a Generate-Verify-Revise paradigm. Specifically, we construct structured training data and apply Dynamic Termination Supervision to guide the model in deciding when to terminate reasoning. To avoid interference between learning signals for different reasoning abilities, we adopt a progressive two-stage reinforcement learning strategy: the first stage targets initial generation and self-verification, and the second stage focuses on revision. Extensive experiments on three backbone models show that SCR substantially improves reasoning efficiency and self-verification. Besides, compared with existing reasoning paradigms, it reduces output token length by up to 50%.

</details>


### [96] [Relink: Constructing Query-Driven Evidence Graph On-the-Fly for GraphRAG](https://arxiv.org/abs/2601.07192)
*Manzong Huang,Chenyang Bu,Yi He,Xingrui Zhuo,Xindong Wu*

Main category: cs.CL

TL;DR: 提出了一种名为Relink的新框架，动态构建针对查询的证据图，以缓解现有基于图的检索增强生成方法中知识图谱不完整和噪声干扰的问题，从而提升大型语言模型的问答性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的检索增强生成(GraphRAG)方法依赖静态预构建的知识图，导致推理路径受阻及噪声干扰，影响推理质量。

Method: Relink采用"推理-构建"范式，动态从文本语料中实例化所需事实，修复推理路径；并设计统一的查询感知评估策略，选择最相关的候选事实，主动剔除干扰信息。

Result: 在五个开放领域问答基准测试中，Relink在准确率(EM)和F1指标上分别实现了平均提升5.4%和5.2%，优于现有的GraphRAG方法。

Conclusion: Relink框架通过动态构建查询专属的证据图，有效应对知识图谱的不完整性和噪声干扰，显著提升了大型语言模型在开放领域问答中的性能。

Abstract: Graph-based Retrieval-Augmented Generation (GraphRAG) mitigates hallucinations in Large Language Models (LLMs) by grounding them in structured knowledge. However, current GraphRAG methods are constrained by a prevailing \textit{build-then-reason} paradigm, which relies on a static, pre-constructed Knowledge Graph (KG). This paradigm faces two critical challenges. First, the KG's inherent incompleteness often breaks reasoning paths. Second, the graph's low signal-to-noise ratio introduces distractor facts, presenting query-relevant but misleading knowledge that disrupts the reasoning process.
  To address these challenges, we argue for a \textit{reason-and-construct} paradigm and propose Relink, a framework that dynamically builds a query-specific evidence graph. To tackle incompleteness, \textbf{Relink} instantiates required facts from a latent relation pool derived from the original text corpus, repairing broken paths on the fly. To handle misleading or distractor facts, Relink employs a unified, query-aware evaluation strategy that jointly considers candidates from both the KG and latent relations, selecting those most useful for answering the query rather than relying on their pre-existence. This empowers Relink to actively discard distractor facts and construct the most faithful and precise evidence path for each query.
  Extensive experiments on five Open-Domain Question Answering benchmarks show that Relink achieves significant average improvements of 5.4\% in EM and 5.2\% in F1 over leading GraphRAG baselines, demonstrating the superiority of our proposed framework.

</details>


### [97] [MI-PRUN: Optimize Large Language Model Pruning via Mutual Information](https://arxiv.org/abs/2601.07212)
*Hao Zhang,Zhibin Zhang,Guangxin Wu,He Chen,Jiafeng Guo,Xueqi Cheng*

Main category: cs.CL

TL;DR: 本文提出了一种基于互信息的块状剪枝方法MI-PRUN，用于提升大型语言模型的压缩和推理效率，解决现有方法不稳定和难以达到全局最优的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型占用巨大计算和存储资源，现有块状剪枝方法不够稳定且难以找到全局最优剪枝方案。

Method: 通过互信息评估隐藏状态的转换来识别冗余块，引入数据处理不等式揭示连续块与单块重要性的关系，并设计Fast-Block-Select算法迭代更新块组合以实现全局最优且高效的剪枝。

Result: 在多种模型和数据集上实验表明，该方法稳定且有效，显著提升了模型压缩率和推理加速效果。

Conclusion: 基于互信息的MI-PRUN方法能够实现鲁棒且高效的块状剪枝，为大型语言模型的压缩和加速提供了可靠解决方案。

Abstract: Large Language Models (LLMs) have become indispensable across various domains, but this comes at the cost of substantial computational and memory resources. Model pruning addresses this by removing redundant components from models. In particular, block pruning can achieve significant compression and inference acceleration. However, existing block pruning methods are often unstable and struggle to attain globally optimal solutions. In this paper, we propose a mutual information based pruning method MI-PRUN for LLMs. Specifically, we leverages mutual information to identify redundant blocks by evaluating transitions in hidden states. Additionally, we incorporate the Data Processing Inequality (DPI) to reveal the relationship between the importance of entire contiguous blocks and that of individual blocks. Moreover, we develop the Fast-Block-Select algorithm, which iteratively updates block combinations to achieve a globally optimal solution while significantly improving the efficiency. Extensive experiments across various models and datasets demonstrate the stability and effectiveness of our method.

</details>


### [98] [The Roots of Performance Disparity in Multilingual Language Models: Intrinsic Modeling Difficulty or Design Choices?](https://arxiv.org/abs/2601.07220)
*Chen Shani,Yuval Reif,Nathan Roll,Dan Jurafsky,Ekaterina Shutova*

Main category: cs.CL

TL;DR: 多语言语言模型存在不同语言表现不均的问题，研究探讨其原因是语言复杂性还是模型设计因素，并提出解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前多语言模型在不同语言上的性能表现不均衡，影响广泛的自然语言处理应用，需探讨这些差异的根源。

Method: 通过综述相关文献，分析语言学特征与模型设计选择的关系，重点考察分词、编码、数据暴露及参数共享对模型表现的影响。

Result: 发现通过规范化分段、编码和数据暴露，可以显著缩小语言表现差距，表明模型设计因素对性能差异起主要作用。

Conclusion: 提出了针对分词、数据采样、模型架构及评估方法的设计建议，旨在促进更为均衡的多语言模型发展。

Abstract: Multilingual language models (LMs) promise broader NLP access, yet current systems deliver uneven performance across the world's languages. This survey examines why these gaps persist and whether they reflect intrinsic linguistic difficulty or modeling artifacts. We organize the literature around two questions: do linguistic disparities arise from representation and allocation choices (e.g., tokenization, encoding, data exposure, parameter sharing) rather than inherent complexity; and which design choices mitigate inequities across typologically diverse languages. We review linguistic features, such as orthography, morphology, lexical diversity, syntax, information density, and typological distance, linking each to concrete modeling mechanisms. Gaps often shrink when segmentation, encoding, and data exposure are normalized, suggesting much apparent difficulty stems from current modeling choices. We synthesize these insights into design recommendations for tokenization, sampling, architectures, and evaluation to support more balanced multilingual LMs.

</details>


### [99] [ActiShade: Activating Overshadowed Knowledge to Guide Multi-Hop Reasoning in Large Language Models](https://arxiv.org/abs/2601.07260)
*Huipeng Ma,Luan Zhang,Dandan Song,Linmei Hu,Yuhang Tian,Jun Yang,Changzhi Zhou,Chenhao Li,Yizhou Jin,Xudong Li,Meng Lin,Mingxing Zhang,Shuhao Zhang*

Main category: cs.CL

TL;DR: 本文提出ActiShade方法，在多跳推理中通过检测并激活被遮蔽的关键信息，改善了基于大语言模型的多轮检索生成过程，减少了知识遮蔽带来的误差积累，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 多轮基于大语言模型的检索增强生成方法易受到知识遮蔽现象影响，导致关键信息丢失或不准确，产生无关检索并造成误差积累。

Method: ActiShade通过迭代检测查询中的被遮蔽关键词，同时检索与查询及被遮蔽关键词相关的文档，基于检索结果生成新查询引导下一轮推理，有效补充被遮蔽知识并降低噪声干扰。

Result: 大量实验表明，ActiShade在多个数据集和不同大语言模型上均优于现有方法。

Conclusion: ActiShade成功减缓了知识遮蔽带来的误差积累，提高了多跳推理中大语言模型的检索与生成效果。

Abstract: In multi-hop reasoning, multi-round retrieval-augmented generation (RAG) methods typically rely on LLM-generated content as the retrieval query. However, these approaches are inherently vulnerable to knowledge overshadowing - a phenomenon where critical information is overshadowed during generation. As a result, the LLM-generated content may be incomplete or inaccurate, leading to irrelevant retrieval and causing error accumulation during the iteration process. To address this challenge, we propose ActiShade, which detects and activates overshadowed knowledge to guide large language models (LLMs) in multi-hop reasoning. Specifically, ActiShade iteratively detects the overshadowed keyphrase in the given query, retrieves documents relevant to both the query and the overshadowed keyphrase, and generates a new query based on the retrieved documents to guide the next-round iteration. By supplementing the overshadowed knowledge during the formulation of next-round queries while minimizing the introduction of irrelevant noise, ActiShade reduces the error accumulation caused by knowledge overshadowing. Extensive experiments show that ActiShade outperforms existing methods across multiple datasets and LLMs.

</details>


### [100] [The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents](https://arxiv.org/abs/2601.07264)
*Weihao Xuan,Qingcheng Zeng,Heli Qi,Yunze Xiao,Junjue Wang,Naoto Yokoya*

Main category: cs.CL

TL;DR: 本文研究了基于大型语言模型的自治代理在使用工具过程中的校准问题，提出了通过强化学习联合优化任务准确率和校准度的方法，显著提升了代理的信心水平和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于大型语言模型的自治代理在处理多轮任务时，如何确保其置信度与实际表现相符的校准问题尚未充分解决，尤其是在集成各种工具时。

Method: 通过实验发现不同类型的工具导致不同的校准表现，提出了一种强化学习微调框架，联合优化任务准确率和置信度校准；设计了综合的奖励机制进行训练。

Result: 训练出的代理在校准效果上优于传统方法，同时具有良好的泛化能力，能够适应噪声环境和不同领域的任务，如数学推理。

Conclusion: 工具类型影响校准效果，需要针对具体领域设计校准策略。该方法为构建能够可靠表达不确定性的自我认知代理奠定基础，促进其在高风险实际应用中的信赖度。

Abstract: Autonomous agents based on large language models (LLMs) are rapidly evolving to handle multi-turn tasks, but ensuring their trustworthiness remains a critical challenge. A fundamental pillar of this trustworthiness is calibration, which refers to an agent's ability to express confidence that reliably reflects its actual performance. While calibration is well-established for static models, its dynamics in tool-integrated agentic workflows remain underexplored. In this work, we systematically investigate verbalized calibration in tool-use agents, revealing a fundamental confidence dichotomy driven by tool type. Specifically, our pilot study identifies that evidence tools (e.g., web search) systematically induce severe overconfidence due to inherent noise in retrieved information, while verification tools (e.g., code interpreters) can ground reasoning through deterministic feedback and mitigate miscalibration. To robustly improve calibration across tool types, we propose a reinforcement learning (RL) fine-tuning framework that jointly optimizes task accuracy and calibration, supported by a holistic benchmark of reward designs. We demonstrate that our trained agents not only achieve superior calibration but also exhibit robust generalization from local training environments to noisy web settings and to distinct domains such as mathematical reasoning. Our results highlight the necessity of domain-specific calibration strategies for tool-use agents. More broadly, this work establishes a foundation for building self-aware agents that can reliably communicate uncertainty in high-stakes, real-world deployments.

</details>


### [101] [Document-Level Zero-Shot Relation Extraction with Entity Side Information](https://arxiv.org/abs/2601.07271)
*Mohan Raj Chanthran,Soon Lay Ki,Ong Huey Fang,Bhawani Selvaretnam*

Main category: cs.CL

TL;DR: 本文提出了一种针对文档级零样本关系抽取的新方法DocZSRE-SI，通过利用实体边侧信息避免依赖大型语言模型生成的合成数据，显著提升了在低资源语言中模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有文档级零样本关系抽取依赖大型语言模型生成合成数据，在低资源语言（如马来西亚英语）中存在语言细节难以涵盖和事实不准确等问题，亟需一种不依赖此类数据的方法。

Method: 设计DocZSRE-SI框架，利用实体边侧信息（例如实体提及描述和超义词）来辅助关系预测，避免使用大型语言模型生成合成数据，从而降低复杂度。

Result: 提出的低复杂度模型在宏F1值上平均提升了11.6%，优于基线及现有方法，特别在处理低资源语言和语言多样性方面表现出显著进步。

Conclusion: DocZSRE-SI通过整合实体边侧信息，为文档级零样本关系抽取提供了高效、稳健且可扩展的解决方案，尤其适用于传统方法表现不佳的低资源语言场景。

Abstract: Document-Level Zero-Shot Relation Extraction (DocZSRE) aims to predict unseen relation labels in text documents without prior training on specific relations. Existing approaches rely on Large Language Models (LLMs) to generate synthetic data for unseen labels, which poses challenges for low-resource languages like Malaysian English. These challenges include the incorporation of local linguistic nuances and the risk of factual inaccuracies in LLM-generated data. This paper introduces Document-Level Zero-Shot Relation Extraction with Entity Side Information (DocZSRE-SI) to address limitations in the existing DocZSRE approach. The DocZSRE-SI framework leverages Entity Side Information, such as Entity Mention Descriptions and Entity Mention Hypernyms, to perform ZSRE without depending on LLM-generated synthetic data. The proposed low-complexity model achieves an average improvement of 11.6% in the macro F1-Score compared to baseline models and existing benchmarks. By utilizing Entity Side Information, DocZSRE-SI offers a robust and efficient alternative to error-prone, LLM-based methods, demonstrating significant advancements in handling low-resource languages and linguistic diversity in relation extraction tasks. This research provides a scalable and reliable solution for ZSRE, particularly in contexts like Malaysian English news articles, where traditional LLM-based approaches fall short.

</details>


### [102] [Towards Comprehensive Semantic Speech Embeddings for Chinese Dialects](https://arxiv.org/abs/2601.07274)
*Kalvin Chang,Yiwen Shao,Jiahong Li,Dong Yu*

Main category: cs.CL

TL;DR: 该论文提出了一种利用仅ASR数据训练语音编码器，实现中国方言与普通话语义对齐的方法，并基于新贡献的方言基准测试，展示了该编码器在语音检索和语音识别中的优秀表现。


<details>
  <summary>Details</summary>
Motivation: 中国方言拥有庞大使用人口，但在语音与语言技术方面落后于普通话。由于多数方言为口语，开发方言到普通话的语音大模型比直接开发方言模型更实际。实现这一目标需要跨方言的语义对齐音频表示。

Method: 通过仅使用自动语音识别（ASR）数据训练语音编码器，实现中国方言与普通话的语义对齐。同时构建了一个新的涵盖多个中国方言的语音检索基准数据集，用于验证编码器性能。

Result: 训练的语音编码器在中方言的ASR任务上表现达到最先进水平，同时在新的方言语音到语音检索基准上证明了良好的跨方言语义对齐效果。

Conclusion: 该研究构建的基准测试、语义对齐的语音表示及其检索评价方法为未来中国方言语音大模型的研究奠定了基础，推动了方言技术的进一步发展。

Abstract: Despite having hundreds of millions of speakers, Chinese dialects lag behind Mandarin in speech and language technologies. Most varieties are primarily spoken, making dialect-to-Mandarin speech-LLMs (large language models) more practical than dialect LLMs. Building dialect-to-Mandarin speech-LLMs requires speech representations with cross-dialect semantic alignment between Chinese dialects and Mandarin. In this paper, we achieve such a cross-dialect semantic alignment by training a speech encoder with ASR (automatic speech recognition)-only data, as demonstrated by speech-to-speech retrieval on a new benchmark of spoken Chinese varieties that we contribute. Our speech encoder further demonstrates state-of-the-art ASR performance on Chinese dialects. Together, our Chinese dialect benchmark, semantically aligned speech representations, and speech-to-speech retrieval evaluation lay the groundwork for future Chinese dialect speech-LLMs. We release the benchmark at https://github.com/kalvinchang/yubao.

</details>


### [103] [ReasonTabQA: A Comprehensive Benchmark for Table Question Answering from Real World Industrial Scenarios](https://arxiv.org/abs/2601.07280)
*Changzai Pan,Jie Zhang,Kaiwen Wei,Chenshuo Pan,Yu Zhao,Jingwang Huang,Jian Yang,Zhenhe Wu,Haoyang Zeng,Xiaoyan Gu,Weichao Sun,Yanbo Zhai,Yujie Mao,Zhuoru Jiang,Jiang Zhong,Shuangyong Song,Yongxiang Li,Zhongjiang He*

Main category: cs.CL

TL;DR: 本文提出了ReasonTabQA大规模双语基准和TabCodeRL强化学习方法，解决工业场景中多表结构和复杂推理的表格问答难题。


<details>
  <summary>Details</summary>
Motivation: 现有表格问答基准未能充分反映工业场景下多表结构、嵌套表头及大规模数据带来的挑战，缺乏对复杂结构推理的有效支持。

Method: 构建包含1932个表格、涵盖30个行业领域的ReasonTabQA基准，提供详细推理链注释；引入基于表格感知可验证奖励的强化学习方法TabCodeRL，促进逻辑推理路径的生成。

Result: 在ReasonTabQA及4个现有表格问答数据集上的实验表明，TabCodeRL显著提升了开源大型语言模型的表现，但ReasonTabQA任务依然表现出较大性能差距。

Conclusion: 引入的ReasonTabQA基准和TabCodeRL方法为工业复杂表格问答提供了有效工具，但真实工业场景的挑战依然巨大。

Abstract: Recent advancements in Large Language Models (LLMs) have significantly catalyzed table-based question answering (TableQA). However, existing TableQA benchmarks often overlook the intricacies of industrial scenarios, which are characterized by multi-table structures, nested headers, and massive scales. These environments demand robust table reasoning through deep structured inference, presenting a significant challenge that remains inadequately addressed by current methodologies. To bridge this gap, we present ReasonTabQA, a large-scale bilingual benchmark encompassing 1,932 tables across 30 industry domains such as energy and automotive. ReasonTabQA provides high-quality annotations for both final answers and explicit reasoning chains, supporting both thinking and no-thinking paradigms. Furthermore, we introduce TabCodeRL, a reinforcement learning method that leverages table-aware verifiable rewards to guide the generation of logical reasoning paths. Extensive experiments on ReasonTabQA and 4 TableQA datasets demonstrate that while TabCodeRL yields substantial performance gains on open-source LLMs, the persistent performance gap on ReasonTabQA underscores the inherent complexity of real-world industrial TableQA.

</details>


### [104] [PsyCLIENT: Client Simulation via Conversational Trajectory Modeling for Trainee Practice and Model Evaluation in Mental Health Counseling](https://arxiv.org/abs/2601.07312)
*Huachuan Qiu,Zhaoming Chen,Yuqian Chen,Yuan Xie,Yu Lu,Zhenzhong Lan*

Main category: cs.CL

TL;DR: 本文提出了一种基于对话轨迹建模的LLM客户模拟框架PsyCLIENT，以生成多样且真实的客户行为，特别针对中文环境，并发布了首个中文客户资料数据集。


<details>
  <summary>Details</summary>
Motivation: 现有客户模拟在多样性、行为建模框架和中文应用方面存在不足，难以生成真实、多样的客户行为。

Method: 通过在大语言模型生成中引入预定义的现实对话轨迹，包括行为标签和内容约束，确保模拟交互的多样性与真实感；并构建了覆盖60个咨询话题的中文客户资料数据集PsyCLIENT-CP。

Result: 专业咨询师评估表明，PsyCLIENT在真实性和训练效果上显著优于现有方法，模拟客户在识别任务中有约95%的专家混淆率，几乎无法与真人客户区分。

Conclusion: 对话轨迹建模有效弥合了理论客户资料与动态真实模拟间的差距，为心理健康教育与研究提供了坚实工具，且将开放代码与数据支持后续研究。

Abstract: LLM-based client simulation has emerged as a promising tool for training novice counselors and evaluating automated counseling systems. However, existing client simulation approaches face three key challenges: (1) limited diversity and realism in client profiles, (2) the lack of a principled framework for modeling realistic client behaviors, and (3) a scarcity in Chinese-language settings. To address these limitations, we propose PsyCLIENT, a novel simulation framework grounded in conversational trajectory modeling. By conditioning LLM generation on predefined real-world trajectories that incorporate explicit behavior labels and content constraints, our approach ensures diverse and realistic interactions. We further introduce PsyCLIENT-CP, the first open-source Chinese client profile dataset, covering 60 distinct counseling topics. Comprehensive evaluations involving licensed professional counselors demonstrate that PsyCLIENT significantly outperforms baselines in terms of authenticity and training effectiveness. Notably, the simulated clients are nearly indistinguishable from human clients, achieving an about 95\% expert confusion rate in discrimination tasks. These findings indicate that conversational trajectory modeling effectively bridges the gap between theoretical client profiles and dynamic, realistic simulations, offering a robust solution for mental health education and research. Code and data will be released to facilitate future research in mental health counseling.

</details>


### [105] [Mitrasamgraha: A Comprehensive Classical Sanskrit Machine Translation Dataset](https://arxiv.org/abs/2601.07314)
*Sebastian Nehrdich,David Allport,Sven Sellmer,Jivnesh Sandhan,Manoj Balaji Jagadeeshan,Pawan Goyal,Sujeet Kumar,Kurt Keutzer*

Main category: cs.CL

TL;DR: 本文介绍了一个名为Mitrasamgraha的大型高质量梵语到英语机器翻译数据集，包含391,548对平行文本，涵盖三千多年和多个领域。该数据集在时间和领域标签上的细粒度注释有助于研究机器翻译性能的时空影响。实验表明，微调模型显著提升翻译效果，但复杂语言现象仍是挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管许多高资源语言的机器翻译问题被认为已解决，但对于包含诗意语言、哲学概念和多层比喻等复杂内容的梵语文献，现有资源严重不足，限制了该领域的机器翻译性能研究与提升。

Method: 构建了Mitrasamgraha数据集，包含大规模、多领域多时间层次的梵语-英语平行语料，并进行了细粒度的时间和领域标注。并基于该数据集对NLLB和Gemma模型进行微调，同时评估了商业模型的性能，分析上下文学习对性能的影响。

Result: 微调后的模型在Mitrasamgraha数据集上表现出显著提升，验证了数据集的有效性。但复杂的复合词、哲学概念和多层比喻翻译依然存在较大挑战。

Conclusion: Mitrasamgraha数据集为梵语机器翻译研究提供了重要资源，促使模型在多时期多领域文本上的进步，同时揭示了复杂语言现象仍需进一步研究和改进。

Abstract: While machine translation is regarded as a "solved problem" for many high-resource languages, close analysis quickly reveals that this is not the case for content that shows challenges such as poetic language, philosophical concepts, multi-layered metaphorical expressions, and more. Sanskrit literature is a prime example of this, as it combines a large number of such challenges in addition to inherent linguistic features like sandhi, compounding, and heavy morphology, which further complicate NLP downstream tasks. It spans multiple millennia of text production time as well as a large breadth of different domains, ranging from ritual formulas via epic narratives, philosophical treatises, poetic verses up to scientific material. As of now, there is a strong lack of publicly available resources that cover these different domains and temporal layers of Sanskrit. We therefore introduce Mitrasamgraha, a high-quality Sanskrit-to-English machine translation dataset consisting of 391,548 bitext pairs, more than four times larger than the largest previously available Sanskrit dataset Itih=asa. It covers a time period of more than three millennia and a broad range of historical Sanskrit domains. In contrast to web-crawled datasets, the temporal and domain annotation of this dataset enables fine-grained study of domain and time period effects on MT performance. We also release a validation set consisting of 5,587 and a test set consisting of 5,552 post-corrected bitext pairs. We conduct experiments benchmarking commercial and open models on this dataset and fine-tune NLLB and Gemma models on the dataset, showing significant improvements, while still recognizing significant challenges in the translation of complex compounds, philosophical concepts, and multi-layered metaphors. We also analyze how in-context learning on this dataset impacts the performance of commercial models

</details>


### [106] [How to predict creativity ratings from written narratives: A comparison of co-occurrence and textual forma mentis networks](https://arxiv.org/abs/2601.07327)
*Roberto Passaro,Edith Haim,Massimo Stella*

Main category: cs.CL

TL;DR: 本文提出了从短文本构建和分析语义网络的详细流程，比较了词共现网络和文本形态网络（TFMN）两种方法，并应用机器学习预测人类创造力评分，结果显示TFMN表现更优。


<details>
  <summary>Details</summary>
Motivation: 旨在为研究创造力等认知领域的学者提供如何构建和利用语义网络分析短文本的实用方法和工具，解决网络构建方式对预测性能的影响问题。

Method: 以1029篇短篇故事为语料，介绍文本预处理、网络构建、特征提取（结构特征、扩散激活指数、情感分数）及回归模型应用，比较词共现网络与TFMN的性能差异。

Result: TFMN在所有模型中的预测误差均低于词共现网络，结构特征贡献最大，情感特征效果较差，扩散激活指标影响有限。具体最佳MAE为TFMN的0.581，比词共现网络优越。

Conclusion: TFMN优于词共现网络，结构特征是创造力预测的关键因素。本文提供了一个公开、易操作且适合初学者的流程，对认知领域的网络方法应用具有指导意义，同时也为资深研究者提供深入的技术见解。

Abstract: This tutorial paper provides a step-by-step workflow for building and analysing semantic networks from short creative texts. We introduce and compare two widely used text-to-network approaches: word co-occurrence networks and textual forma mentis networks (TFMNs). We also demonstrate how they can be used in machine learning to predict human creativity ratings. Using a corpus of 1029 short stories, we guide readers through text preprocessing, network construction, feature extraction (structural measures, spreading-activation indices, and emotion scores), and application of regression models. We evaluate how network-construction choices influence both network topology and predictive performance. Across all modelling settings, TFMNs consistently outperformed co-occurrence networks through lower prediction errors (best MAE = 0.581 for TFMN, vs 0.592 for co-occurrence with window size 3). Network-structural features dominated predictive performance (MAE = 0.591 for TFMN), whereas emotion features performed worse (MAE = 0.711 for TFMN) and spreading-activation measures contributed little (MAE = 0.788 for TFMN). This paper offers practical guidance for researchers interested in applying network-based methods for cognitive fields like creativity research. we show when syntactic networks are preferable to surface co-occurrence models, and provide an open, reproducible workflow accessible to newcomers in the field, while also offering deeper methodological insight for experienced researchers.

</details>


### [107] [BayesRAG: Probabilistic Mutual Evidence Corroboration for Multimodal Retrieval-Augmented Generation](https://arxiv.org/abs/2601.07329)
*Xuan Li,Yining Wang,Haocai Luo,Shengping Liu,Jerry Liang,Ying Fu,Weihuang,Jun Yu,Junnan Zhu*

Main category: cs.CL

TL;DR: BayesRAG提出了基于贝叶斯推理和Dempster-Shafer证据理论的多模态检索框架，有效提升了跨模态语义与布局一致性的检索表现。


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成方法在处理包含丰富视觉信息的文档时，将文本和图像作为孤立的检索对象，忽视了多模态间的语义强化和布局连贯性，导致检索效果不理想。

Method: 提出BayesRAG框架，通过贝叶斯推理和Dempster-Shafer证据理论，对多模态检索结果进行不确定性建模，计算多模态组合的后验关联概率，优先选择语义与布局上相互印证的文本-图像对，提升检索置信度。

Result: 实验表明，BayesRAG在多模态检索的复杂基准测试中显著优于现有最先进方法。

Conclusion: BayesRAG通过证据融合机制有效整合异构模态的信息，解决了多模态检索中模态孤立的问题，极大增强了检索结果的鲁棒性和准确性。

Abstract: Retrieval-Augmented Generation (RAG) has become a pivotal paradigm for Large Language Models (LLMs), yet current approaches struggle with visually rich documents by treating text and images as isolated retrieval targets. Existing methods relying solely on cosine similarity often fail to capture the semantic reinforcement provided by cross-modal alignment and layout-induced coherence. To address these limitations, we propose BayesRAG, a novel multimodal retrieval framework grounded in Bayesian inference and Dempster-Shafer evidence theory. Unlike traditional approaches that rank candidates strictly by similarity, BayesRAG models the intrinsic consistency of retrieved candidates across modalities as probabilistic evidence to refine retrieval confidence. Specifically, our method computes the posterior association probability for combinations of multimodal retrieval results, prioritizing text-image pairs that mutually corroborate each other in terms of both semantics and layout. Extensive experiments demonstrate that BayesRAG significantly outperforms state-of-the-art (SOTA) methods on challenging multimodal benchmarks. This study establishes a new paradigm for multimodal retrieval fusion that effectively resolves the isolation of heterogeneous modalities through an evidence fusion mechanism and enhances the robustness of retrieval outcomes. Our code is available at https://github.com/TioeAre/BayesRAG.

</details>


### [108] [Beyond Literal Mapping: Benchmarking and Improving Non-Literal Translation Evaluation](https://arxiv.org/abs/2601.07338)
*Yanzhi Tian,Cunxiang Wang,Zeming Liu,Heyan Huang,Wenbo Yu,Dawei Song,Jie Tang,Yuhang Guo*

Main category: cs.CL

TL;DR: 本文构建了一个专注于非字面翻译的评测数据集MENT，并提出了新的翻译评估框架RATE，在处理复杂翻译场景中显著优于传统指标和LLM评判。


<details>
  <summary>Details</summary>
Motivation: 现有机器翻译指标在非字面表达的复杂领域表现不准确，且大型语言模型作为评判者存在知识截止和评分不一致的问题。

Method: 构建专注非字面翻译的多领域元评测数据集MENT，基于此提出包含反思核心代理和多专门子代理的智能翻译评估框架RATE。

Result: 传统翻译评估指标和LLM作为评判者存在显著不足，RATE框架在评测准确性上提升至少3.2元评分，并在通用领域翻译评测中表现出良好鲁棒性。

Conclusion: RATE框架有效解决非字面复杂翻译场景中评估难题，提升机器翻译评测的可靠性和一致性，具有推广应用价值。

Abstract: Large Language Models (LLMs) have significantly advanced Machine Translation (MT), applying them to linguistically complex domains-such as Social Network Services, literature etc. In these scenarios, translations often require handling non-literal expressions, leading to the inaccuracy of MT metrics. To systematically investigate the reliability of MT metrics, we first curate a meta-evaluation dataset focused on non-literal translations, namely MENT. MENT encompasses four non-literal translation domains and features source sentences paired with translations from diverse MT systems, with 7,530 human-annotated scores on translation quality. Experimental results reveal the inaccuracies of traditional MT metrics and the limitations of LLM-as-a-Judge, particularly the knowledge cutoff and score inconsistency problem. To mitigate these limitations, we propose RATE, a novel agentic translation evaluation framework, centered by a reflective Core Agent that dynamically invokes specialized sub-agents. Experimental results indicate the efficacy of RATE, achieving an improvement of at least 3.2 meta score compared with current metrics. Further experiments demonstrate the robustness of RATE to general-domain MT evaluation. Code and dataset are available at: https://github.com/BITHLP/RATE.

</details>


### [109] [DiffER: Diffusion Entity-Relation Modeling for Reversal Curse in Diffusion Large Language Models](https://arxiv.org/abs/2601.07347)
*Shaokai He,Kaiwen Wei,Xinyi Zeng,Xiang Chen,Xue Yang,Zhenyang Li,Jiang Zhong,Yu Tian*

Main category: cs.CL

TL;DR: 该论文研究了大语言模型中的“逆转诅咒”现象，即模型处理双向逻辑关系时表现出单向偏好，并提出了基于扩散模型的实体关系建模方法（DiffER）以缓解该问题。


<details>
  <summary>Details</summary>
Motivation: 当前扩散大语言模型即使采用双向训练，仍然存在逆转诅咒问题，影响模型处理实体关系的准确性，需要寻找问题根源并提出有效解决方案。

Method: 作者通过系统实验发现逆转诅咒源于实体碎片化、数据不对称和实体关系缺失，并提出DiffER方法，通过整体实体掩码训练、对称分布及关系增强的数据构建策略来解决这些问题。

Result: 实验表明DiffER能够显著缓解扩散语言模型中的逆转诅咒现象，提升模型对双向实体关系的理解和处理能力。

Conclusion: DiffER提供了一种有效的实体感知训练和数据构建方案，为解决扩散大语言模型中的逆转诅咒问题和未来相关研究提供了新思路。

Abstract: The "reversal curse" refers to the phenomenon where large language models (LLMs) exhibit predominantly unidirectional behavior when processing logically bidirectional relationships. Prior work attributed this to autoregressive training -- predicting the next token inherently favors left-to-right information flow over genuine bidirectional knowledge associations. However, we observe that Diffusion LLMs (DLLMs), despite being trained bidirectionally, also suffer from the reversal curse. To investigate the root causes, we conduct systematic experiments on DLLMs and identify three key reasons: 1) entity fragmentation during training, 2) data asymmetry, and 3) missing entity relations. Motivated by the analysis of these reasons, we propose Diffusion Entity-Relation Modeling (DiffER), which addresses the reversal curse through entity-aware training and balanced data construction. Specifically, DiffER introduces whole-entity masking, which mitigates entity fragmentation by predicting complete entities in a single step. DiffER further employs distribution-symmetric and relation-enhanced data construction strategies to alleviate data asymmetry and missing relations. Extensive experiments demonstrate that DiffER effectively alleviates the reversal curse in Diffusion LLMs, offering new perspectives for future research.

</details>


### [110] [Controlled Self-Evolution for Algorithmic Code Optimization](https://arxiv.org/abs/2601.07348)
*Tu Hu,Ronghao Chen,Shuo Zhang,Jianghao Yin,Mou Xiao Feng,Jingping Liu,Shaolei Zhang,Wenqi Jiang,Yuqi Fang,Sen Hu,Yi Xu,Huacan Wang*

Main category: cs.CL

TL;DR: 本文提出了受控自我进化（CSE）方法，通过多样化初始化、反馈指导的遗传进化和分层进化记忆，有效提升代码生成的探索效率和解的复杂性。


<details>
  <summary>Details</summary>
Motivation: 现有自我进化方法探索效率低，受限于初始化偏差、无反馈的随机操作和任务间经验利用不足，难以在有限预算内发现更优解。

Method: 提出CSE方法，包括多样化规划初始化生成多样策略、以反馈指导替代随机操作的遗传进化，以及捕获跨任务和单任务经验的分层进化记忆。

Result: 在EffiBench-X测试中，CSE在多种大型语言模型骨干上均优于基线方法，且从早期世代开始效率更高，进化过程中持续改进。

Conclusion: CSE通过有针对性的初始化和反馈驱动的进化机制显著提升了代码生成的探索效率和效果，为自我进化领域提供了有效改进手段。

Abstract: Self-evolution methods enhance code generation through iterative "generate-verify-refine" cycles, yet existing approaches suffer from low exploration efficiency, failing to discover solutions with superior complexity within limited budgets. This inefficiency stems from initialization bias trapping evolution in poor solution regions, uncontrolled stochastic operations lacking feedback guidance, and insufficient experience utilization across tasks.To address these bottlenecks, we propose Controlled Self-Evolution (CSE), which consists of three key components. Diversified Planning Initialization generates structurally distinct algorithmic strategies for broad solution space coverage. Genetic Evolution replaces stochastic operations with feedback-guided mechanisms, enabling targeted mutation and compositional crossover. Hierarchical Evolution Memory captures both successful and failed experiences at inter-task and intra-task levels.Experiments on EffiBench-X demonstrate that CSE consistently outperforms all baselines across various LLM backbones. Furthermore, CSE achieves higher efficiency from early generations and maintains continuous improvement throughout evolution. Our code is publicly available at https://github.com/QuantaAlpha/EvoControl.

</details>


### [111] [Reward Modeling from Natural Language Human Feedback](https://arxiv.org/abs/2601.07349)
*Zongqi Wang,Rui Wang,Yuchuan Wu,Yiyao Yu,Pinyi Zhang,Shaoning Sun,Yujiu Yang,Yongbin Li*

Main category: cs.CL

TL;DR: 本文提出一种利用自然语言反馈改进生成奖励模型训练的方法RM-NLHF，解决了传统二分类奖励信号噪声大问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于二分类偏好标签的奖励模型容易出现无效猜测，导致奖励信号质量差，降低强化学习效果。

Method: 通过计算生成的评论与人类自然语言反馈的相似度作为奖励信号，提出RM-NLHF方法；引入MetaRM以预测过程奖励并推广至无反馈数据。

Result: 在多个基准测试中，RM-NLHF方法优于仅使用结果监督的最先进生成奖励模型，显示自然语言反馈的优势。

Conclusion: 利用自然语言人类反馈提供过程奖励信号，比传统二分类偏好监督更准确，有效提升生成奖励模型训练的性能。

Abstract: Reinforcement Learning with Verifiable reward (RLVR) on preference data has become the mainstream approach for training Generative Reward Models (GRMs). Typically in pairwise rewarding tasks, GRMs generate reasoning chains ending with critiques and preference labels, and RLVR then relies on the correctness of the preference labels as the training reward. However, in this paper, we demonstrate that such binary classification tasks make GRMs susceptible to guessing correct outcomes without sound critiques. Consequently, these spurious successes introduce substantial noise into the reward signal, thereby impairing the effectiveness of reinforcement learning. To address this issue, we propose Reward Modeling from Natural Language Human Feedback (RM-NLHF), which leverages natural language feedback to obtain process reward signals, thereby mitigating the problem of limited solution space inherent in binary tasks. Specifically, we compute the similarity between GRM-generated and human critiques as the training reward, which provides more accurate reward signals than outcome-only supervision. Additionally, considering that human critiques are difficult to scale up, we introduce Meta Reward Model (MetaRM) which learns to predict process reward from datasets with human critiques and then generalizes to data without human critiques. Experiments on multiple benchmarks demonstrate that our method consistently outperforms state-of-the-art GRMs trained with outcome-only reward, confirming the superiority of integrating natural language over binary human feedback as supervision.

</details>


### [112] [Beyond Hard Masks: Progressive Token Evolution for Diffusion Language Models](https://arxiv.org/abs/2601.07351)
*Linhao Zhong,Linyu Wu,Bozhen Fang,Tianjian Feng,Chenchen Jing,Wen Wang,Jiaheng Zhang,Hao Chen,Chunhua Shen*

Main category: cs.CL

TL;DR: EvoToken-DLM通过软化掩码机制和连续轨迹监督，提升了扩散语言模型的解码性能和灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散语言模型采用硬二进制掩码和离散标记分配，限制了早期决策的修正能力，且未充分利用中间概率表示。

Method: 提出EvoToken-DLM，用演化的软标记分布替代硬二进制掩码，实现从掩码状态到离散输出的渐进过渡，并引入连续轨迹监督以支持迭代的概率更新。

Result: 在多个基准测试中，EvoToken-DLM表现优越，明显超越现有扩散模型和掩码基线的性能。

Conclusion: EvoToken-DLM通过软化掩码和连续监督机制，提升了扩散语言模型的解码灵活性和性能，展示了其作为语言建模替代方案的潜力。

Abstract: Diffusion Language Models (DLMs) offer a promising alternative for language modeling by enabling parallel decoding through iterative refinement. However, most DLMs rely on hard binary masking and discrete token assignments, which hinder the revision of early decisions and underutilize intermediate probabilistic representations. In this paper, we propose EvoToken-DLM, a novel diffusion-based language modeling approach that replaces hard binary masks with evolving soft token distributions. EvoToken-DLM enables a progressive transition from masked states to discrete outputs, supporting revisable decoding. To effectively support this evolution, we introduce continuous trajectory supervision, which aligns training objectives with iterative probabilistic updates. Extensive experiments across multiple benchmarks show that EvoToken-DLM consistently achieves superior performance, outperforming strong diffusion-based and masked DLM baselines. Project webpage: https://aim-uofa.github.io/EvoTokenDLM.

</details>


### [113] [TALON: Confidence-Aware Speculative Decoding with Adaptive Token Trees](https://arxiv.org/abs/2601.07353)
*Tianyu Liu,Qitan Lv,Yuhao Shen,Xiao Sun,Xiaoyan Sun*

Main category: cs.CL

TL;DR: 本文提出了一种名为TALON的自适应树扩展框架，提高了基于树的推测解码效率，通过动态调整树结构以适应不同生成难度，显著加速大型语言模型推理。


<details>
  <summary>Details</summary>
Motivation: 现有基于树的推测解码方法使用固定宽度和深度的草稿树，无法针对不同的令牌和上下文难度动态调整，导致效率下降。

Method: 提出了TALON框架，通过预算驱动的混合扩展策略，动态调整草稿树的层节点分配，实现深窄结构与浅宽结构的灵活切换，优化探索宽度和生成深度的权衡。

Result: 在5个模型和6个数据集上的实验表明，TALON相比现有最先进的EAGLE-3方法，最高可实现5.16倍的端到端推理加速。

Conclusion: TALON有效提升了基于树的推测解码效率，通过动态调整树结构，实现了更优的性能和加速，是一种无需训练即可插拔的优良框架。

Abstract: Speculative decoding (SD) has become a standard technique for accelerating LLM inference without sacrificing output quality. Recent advances in speculative decoding have shifted from sequential chain-based drafting to tree-structured generation, where the draft model constructs a tree of candidate tokens to explore multiple possible drafts in parallel. However, existing tree-based SD methods typically build a fixed-width, fixed-depth draft tree, which fails to adapt to the varying difficulty of tokens and contexts. As a result, the draft model cannot dynamically adjust the tree structure to early stop on difficult tokens and extend generation for simple ones. To address these challenges, we introduce TALON, a training-free, budget-driven adaptive tree expansion framework that can be plugged into existing tree-based methods. Unlike static methods, TALON constructs the draft tree iteratively until a fixed token budget is met, using a hybrid expansion strategy that adaptively allocates the node budget to each layer of the draft tree. This framework naturally shapes the draft tree into a "deep-and-narrow" form for deterministic contexts and a "shallow-and-wide" form for uncertain branches, effectively optimizing the trade-off between exploration width and generation depth under a given budget. Extensive experiments across 5 models and 6 datasets demonstrate that TALON consistently outperforms state-of-the-art EAGLE-3, achieving up to 5.16x end-to-end speedup over auto-regressive decoding.

</details>


### [114] [Semantic Compression of LLM Instructions via Symbolic Metalanguages](https://arxiv.org/abs/2601.07354)
*Ernst van Gassen*

Main category: cs.CL

TL;DR: MetaGlyph是一种通过数学符号编码指令，从而压缩提示语的符号语言，能显著减少Token数量，降低成本和资源消耗。


<details>
  <summary>Details</summary>
Motivation: 当前提示语通常为自然语言，导致Token量大且成本高。MetaGlyph希望用模型已知的数学符号替代自然语言，实现提示压缩。

Method: 设计了MetaGlyph符号语言，采用模型熟悉的数学符号如“∈”、“⇒”等编码指令，避免显式解码需求；在8个不同规模和类型的模型上评估其压缩效果和语义等价性。

Result: MetaGlyph实现了62%-81%的Token压缩，部分模型能达到高达98%的符号指令准确率和75%以上的语义等价性；不同模型表现差异明显，大模型表现优于中型开源模型。

Conclusion: 符号语言MetaGlyph能有效压缩提示语且保持较高的语义准确度，尤其在大规模模型中表现更佳，有助于降低成本和计算资源压力。

Abstract: We introduce MetaGlyph, a symbolic language for compressing prompts by encoding instructions as mathematical symbols rather than prose. Unlike systems requiring explicit decoding rules, MetaGlyph uses symbols like $\in$ (membership) and $\Rightarrow$ (implication) that models already understand from their training data. We test whether these symbols work as ''instruction shortcuts'' that models can interpret without additional teaching.
  We evaluate eight models across two dimensions relevant to practitioners: scale (3B-1T parameters) and accessibility (open-source for local deployment vs. proprietary APIs). MetaGlyph achieves 62-81% token reduction across all task types. For API-based deployments, this translates directly to cost savings; for local deployments, it reduces latency and memory pressure.
  Results vary by model. Gemini 2.5 Flash achieves 75% semantic equivalence between symbolic and prose instructions on selection tasks, with 49.9% membership operator fidelity. Kimi K2 reaches 98.1% fidelity for implication ($\Rightarrow$) and achieves perfect (100%) accuracy on selection tasks with symbolic prompts. GPT-5.2 Chat shows the highest membership fidelity observed (91.3%), though with variable parse success across task types. Claude Haiku 4.5 achieves 100% parse success with 26% membership fidelity. Among mid-sized models, Qwen 2.5 7B shows 62% equivalence on extraction tasks. Mid-sized open-source models (7B-12B) show near-zero operator fidelity, suggesting a U-shaped relationship where sufficient scale overcomes instruction-tuning biases.

</details>


### [115] [Interpretable Text Classification Applied to the Detection of LLM-generated Creative Writing](https://arxiv.org/abs/2601.07368)
*Minerva Suvanto,Andrea McGlinchey,Mattias Wahde,Peter J Barclay*

Main category: cs.CL

TL;DR: 本文研究如何区分人类创作的小说摘录与大型语言模型生成的类似文本，发现机器学习模型准确率高达0.93-0.98，而人类表现接近随机。


<details>
  <summary>Details</summary>
Motivation: 当前难以用人类直观判断区分人类文本和AI生成文本，需寻找准确且可解释的自动化方法。

Method: 利用线性可解释分类器，基于单词（unigram）特征，分析区分人类与LLM文本的关键特征。

Result: 分类器测试准确率达0.98，揭示LLM倾向使用更多同义词及其他特征如时间漂移、美式用语、外语和口语，帮助区分文本来源。

Conclusion: 基于多类特征的分类方法鲁棒性强，不易被恶意伪装，显示了机器在此任务上的优势和对特征的深刻捕捉能力。

Abstract: We consider the problem of distinguishing human-written creative fiction (excerpts from novels) from similar text generated by an LLM. Our results show that, while human observers perform poorly (near chance levels) on this binary classification task, a variety of machine-learning models achieve accuracy in the range 0.93 - 0.98 over a previously unseen test set, even using only short samples and single-token (unigram) features. We therefore employ an inherently interpretable (linear) classifier (with a test accuracy of 0.98), in order to elucidate the underlying reasons for this high accuracy. In our analysis, we identify specific unigram features indicative of LLM-generated text, one of the most important being that the LLM tends to use a larger variety of synonyms, thereby skewing the probability distributions in a manner that is easy to detect for a machine learning classifier, yet very difficult for a human observer. Four additional explanation categories were also identified, namely, temporal drift, Americanisms, foreign language usage, and colloquialisms. As identification of the AI-generated text depends on a constellation of such features, the classification appears robust, and therefore not easy to circumvent by malicious actors intent on misrepresenting AI-generated text as human work.

</details>


### [116] [Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models](https://arxiv.org/abs/2601.07372)
*Xin Cheng,Wangding Zeng,Damai Dai,Qinyu Chen,Bingxuan Wang,Zhenda Xie,Kezhao Huang,Xingkai Yu,Zhewen Hao,Yukun Li,Han Zhang,Huishuai Zhang,Dongyan Zhao,Wenfeng Liang*

Main category: cs.CL

TL;DR: 本文提出了一种名为Engram的条件记忆模块，通过优化神经计算与静态记忆的稀疏分配，显著提升Transformer模型在知识检索和复杂推理中的性能。


<details>
  <summary>Details</summary>
Motivation: Transformer缺乏高效的知识检索机制，必须通过计算模拟检索，效率低下，因此需要引入新的条件记忆机制以改善模型能力。

Method: 提出了条件记忆(Engram)模块，将N-gram嵌入现代化，实现O(1)查找，同时提出稀疏分配问题的U型扩展规律，指导参数规模的平衡分配。从而把Engram扩展到270亿参数，并与同等参数和计算量的MoE基线比较。

Result: Engram在知识检索任务（如MMLU、CMMLU）以及更复杂的推理、代码和数学领域任务（如BBH、ARC-Challenge、HumanEval、MATH）中表现均优于MoE基线，同时提升了长上下文检索能力和提升计算效率。

Conclusion: 条件记忆作为稀疏模型的重要基础模块，通过优化静态记忆和神经计算的配比，显著提升了Transformer的性能和效率，未来将成为下一代稀疏模型必不可少的建模原语。

Abstract: While Mixture-of-Experts (MoE) scales capacity via conditional computation, Transformers lack a native primitive for knowledge lookup, forcing them to inefficiently simulate retrieval through computation. To address this, we introduce conditional memory as a complementary sparsity axis, instantiated via Engram, a module that modernizes classic $N$-gram embedding for O(1) lookup. By formulating the Sparsity Allocation problem, we uncover a U-shaped scaling law that optimizes the trade-off between neural computation (MoE) and static memory (Engram). Guided by this law, we scale Engram to 27B parameters, achieving superior performance over a strictly iso-parameter and iso-FLOPs MoE baseline. Most notably, while the memory module is expected to aid knowledge retrieval (e.g., MMLU +3.4; CMMLU +4.0), we observe even larger gains in general reasoning (e.g., BBH +5.0; ARC-Challenge +3.7) and code/math domains~(HumanEval +3.0; MATH +2.4). Mechanistic analyses reveal that Engram relieves the backbone's early layers from static reconstruction, effectively deepening the network for complex reasoning. Furthermore, by delegating local dependencies to lookups, it frees up attention capacity for global context, substantially boosting long-context retrieval (e.g., Multi-Query NIAH: 84.2 to 97.0). Finally, Engram establishes infrastructure-aware efficiency: its deterministic addressing enables runtime prefetching from host memory, incurring negligible overhead. We envision conditional memory as an indispensable modeling primitive for next-generation sparse models.

</details>


### [117] [GROKE: Vision-Free Navigation Instruction Evaluation via Graph Reasoning on OpenStreetMap](https://arxiv.org/abs/2601.07375)
*Farzad Shami,Subhrasankha Dey,Nico Van de Weghe,Henrikki Tenkanen*

Main category: cs.CL

TL;DR: 本文提出了一种基于大语言模型和OpenStreetMap数据的无视觉、无训练导航指令评估框架GROKE，提升了导航指令的实用性评估效果。


<details>
  <summary>Details</summary>
Motivation: 传统的导航指令评估方法（如BLEU和ROUGE）无法有效衡量空间指令的导航效果，且依赖高质量视觉模拟器的方法存在成本和许可限制。

Method: 提出GROKE框架，利用基于LLM的分层架构结合结构化JSON与文本格式的空间信息，进行子指令规划与拓扑图导航，避免视觉依赖。

Result: 基于Map2Seq数据集，GROKE在导航误差上比启发式和采样基线方法降低了68.5%，实现了更高的执行成功率和路径忠实度。

Conclusion: GROKE建立了一种可扩展、可解释且无需视觉支持的导航指令评估范式，能有效反映指令的功能性导航能力。

Abstract: The evaluation of navigation instructions remains a persistent challenge in Vision-and-Language Navigation (VLN) research. Traditional reference-based metrics such as BLEU and ROUGE fail to capture the functional utility of spatial directives, specifically whether an instruction successfully guides a navigator to the intended destination. Although existing VLN agents could serve as evaluators, their reliance on high-fidelity visual simulators introduces licensing constraints and computational costs, and perception errors further confound linguistic quality assessment. This paper introduces GROKE(Graph-based Reasoning over OSM Knowledge for instruction Evaluation), a vision-free training-free hierarchical LLM-based framework for evaluating navigation instructions using OpenStreetMap data. Through systematic ablation studies, we demonstrate that structured JSON and textual formats for spatial information substantially outperform grid-based and visual graph representations. Our hierarchical architecture combines sub-instruction planning with topological graph navigation, reducing navigation error by 68.5% compared to heuristic and sampling baselines on the Map2Seq dataset. The agent's execution success, trajectory fidelity, and decision patterns serve as proxy metrics for functional navigability given OSM-visible landmarks and topology, establishing a scalable and interpretable evaluation paradigm without visual dependencies. Code and data are available at https://anonymous.4open.science/r/groke.

</details>


### [118] [Outcome-Grounded Advantage Reshaping for Fine-Grained Credit Assignment in Mathematical Reasoning](https://arxiv.org/abs/2601.07408)
*Ziheng Li,Liu Kang,Feng Xiao,Luxi Xing,Qingyi Si,Zhuoran Li,Weikang Gong,Deqing Yang,Yanghua Xiao,Hongcheng Guo*

Main category: cs.CL

TL;DR: 本文提出了Outcome-grounded Advantage Reshaping (OAR)方法，通过细粒度的信用分配机制，提升了基于Group Relative Policy Optimization (GRPO)的推理任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO方法在信用分配上过于粗糙，不能区分序列中各推理步骤的贡献，影响推理效果。

Method: 引入OAR机制，分别通过OAR-P（反事实代币扰动）和OAR-G（输入梯度敏感度代理）两种策略，精细调整各代币的优势信号，并采用双层保守优势重塑方法加强关键代币的重要性。

Result: 在大量数学推理基准测试中，OAR-P实现了性能上限，OAR-G以极低计算开销达到相似提升，均显著优于传统GRPO，推动了无评论家大型语言模型的推理能力边界。

Conclusion: OAR提供了一种有效的细粒度信用分配方案，显著提升了GRPO框架下大型语言模型的推理性能，兼顾了效率和准确性。

Abstract: Group Relative Policy Optimization (GRPO) has emerged as a promising critic-free reinforcement learning paradigm for reasoning tasks. However, standard GRPO employs a coarse-grained credit assignment mechanism that propagates group-level rewards uniformly to to every token in a sequence, neglecting the varying contribution of individual reasoning steps. We address this limitation by introducing Outcome-grounded Advantage Reshaping (OAR), a fine-grained credit assignment mechanism that redistributes advantages based on how much each token influences the model's final answer. We instantiate OAR via two complementary strategies: (1) OAR-P, which estimates outcome sensitivity through counterfactual token perturbations, serving as a high-fidelity attribution signal; (2) OAR-G, which uses an input-gradient sensitivity proxy to approximate the influence signal with a single backward pass. These importance signals are integrated with a conservative Bi-Level advantage reshaping scheme that suppresses low-impact tokens and boosts pivotal ones while preserving the overall advantage mass. Empirical results on extensive mathematical reasoning benchmarks demonstrate that while OAR-P sets the performance upper bound, OAR-G achieves comparable gains with negligible computational overhead, both significantly outperforming a strong GRPO baseline, pushing the boundaries of critic-free LLM reasoning.

</details>


### [119] [Two Pathways to Truthfulness: On the Intrinsic Encoding of LLM Hallucinations](https://arxiv.org/abs/2601.07422)
*Wen Luo,Guangyue Peng,Wei Li,Shaohang Wei,Feifan Song,Liang Wang,Nan Yang,Xingxing Zhang,Jing Jin,Furu Wei,Houfeng Wang*

Main category: cs.CL

TL;DR: 本文揭示了大语言模型（LLMs）内部编码真实性信号的两条不同路径，并提出了提升谬误检测性能的方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然能力强大，但常生成错误信息（幻觉），且内部真实性信号的来源和机制尚不清楚。

Method: 通过注意力敲除和标记修补验证并区分两条信息路径：基于问题-答案流的信息通路和基于答案自包含证据的通路，探究其性质和关联。

Result: 发现两机制与模型知识边界密切相关，内部表征能区分这两机制，基于此提出两种提升谬误检测的应用。

Conclusion: 工作深刻揭示了LLMs内部编码真实性的机制，为构建更可靠自觉的生成系统提供了方向。

Abstract: Despite their impressive capabilities, large language models (LLMs) frequently generate hallucinations. Previous work shows that their internal states encode rich signals of truthfulness, yet the origins and mechanisms of these signals remain unclear. In this paper, we demonstrate that truthfulness cues arise from two distinct information pathways: (1) a Question-Anchored pathway that depends on question-answer information flow, and (2) an Answer-Anchored pathway that derives self-contained evidence from the generated answer itself. First, we validate and disentangle these pathways through attention knockout and token patching. Afterwards, we uncover notable and intriguing properties of these two mechanisms. Further experiments reveal that (1) the two mechanisms are closely associated with LLM knowledge boundaries; and (2) internal representations are aware of their distinctions. Finally, building on these insightful findings, two applications are proposed to enhance hallucination detection performance. Overall, our work provides new insight into how LLMs internally encode truthfulness, offering directions for more reliable and self-aware generative systems.

</details>


### [120] [SAD: A Large-Scale Strategic Argumentative Dialogue Dataset](https://arxiv.org/abs/2601.07423)
*Yongkang Liu,Jiayang Yu,Mingyang Wang,Yiqun Zhang,Ercong Nie,Shi Feng,Daling Wang,Kaisong Song,Hinrich Schütze*

Main category: cs.CL

TL;DR: 本文提出了首个大规模战略性论证对话数据集SAD，用于支持论证对话的多轮建模。


<details>
  <summary>Details</summary>
Motivation: 现有论证生成研究多关注单轮、非互动场景，缺乏对多轮对话中论证策略的深入支持。

Method: 构建了包含392,822条数据的SAD数据集，每条话语标注五类论证策略，模型需结合对话历史、立场和策略生成合适论证。同时进行了预训练生成模型的基准测试和策略使用分析。

Result: SAD数据集支持多策略、多轮对话生成，基准模型展示了生成能力，并揭示了论证策略的使用规律。

Conclusion: SAD数据集促进了对多轮战略性论证对话生成的研究，有助于提升模型的论证策略运用能力和对话生成质量。

Abstract: Argumentation generation has attracted substantial research interest due to its central role in human reasoning and decision-making. However, most existing argumentative corpora focus on non-interactive, single-turn settings, either generating arguments from a given topic or refuting an existing argument. In practice, however, argumentation is often realized as multi-turn dialogue, where speakers defend their stances and employ diverse argumentative strategies to strengthen persuasiveness. To support deeper modeling of argumentation dialogue, we present the first large-scale \textbf{S}trategic \textbf{A}rgumentative \textbf{D}ialogue dataset, SAD, consisting of 392,822 examples. Grounded in argumentation theories, we annotate each utterance with five strategy types, allowing multiple strategies per utterance. Unlike prior datasets, SAD requires models to generate contextually appropriate arguments conditioned on the dialogue history, a specified stance on the topic, and targeted argumentation strategies. We further benchmark a range of pretrained generative models on SAD and present in-depth analysis of strategy usage patterns in argumentation.

</details>


### [121] [KALE: Enhancing Knowledge Manipulation in Large Language Models via Knowledge-aware Learning](https://arxiv.org/abs/2601.07430)
*Qitan Lv,Tianyu Liu,Qiaosheng Zhang,Xingcheng Xu,Chaochao Lu*

Main category: cs.CL

TL;DR: 本文提出了一种基于知识图谱的后训练框架KALE，以提升大语言模型在知识操控能力上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有通过有监督微调提升大语言模型知识操控能力的方法仍存在模型虽知道相关知识却无法正确应用的“known&incorrect”问题。

Method: KALE采用知识引导数据合成方法，从知识图谱中提取多跳推理路径生成高质量推理依据，并通过知识感知微调范式，通过最小化带推理依据与不带推理依据预测的KL散度，改进模型的推理能力。

Result: 该方法在六种大语言模型的八个主流基准测试上取得了最高11.72%、平均4.18%的准确率提升，显示出显著效果。

Conclusion: 利用知识图谱生成推理依据并结合知识感知微调，可有效解决“known&incorrect”问题，增强大语言模型的知识操控能力。

Abstract: Despite the impressive performance of large language models (LLMs) pretrained on vast knowledge corpora, advancing their knowledge manipulation-the ability to effectively recall, reason, and transfer relevant knowledge-remains challenging. Existing methods mainly leverage Supervised Fine-Tuning (SFT) on labeled datasets to enhance LLMs' knowledge manipulation ability. However, we observe that SFT models still exhibit the known&incorrect phenomenon, where they explicitly possess relevant knowledge for a given question but fail to leverage it for correct answers. To address this challenge, we propose KALE (Knowledge-Aware LEarning)-a post-training framework that leverages knowledge graphs (KGs) to generate high-quality rationales and enhance LLMs' knowledge manipulation ability. Specifically, KALE first introduces a Knowledge-Induced (KI) data synthesis method that efficiently extracts multi-hop reasoning paths from KGs to generate high-quality rationales for question-answer pairs. Then, KALE employs a Knowledge-Aware (KA) fine-tuning paradigm that enhances knowledge manipulation by internalizing rationale-guided reasoning through minimizing the KL divergence between predictions with and without rationales. Extensive experiments on eight popular benchmarks across six different LLMs demonstrate the effectiveness of KALE, achieving accuracy improvements of up to 11.72% and an average of 4.18%.

</details>


### [122] [Judging Against the Reference: Uncovering Knowledge-Driven Failures in LLM-Judges on QA Evaluation](https://arxiv.org/abs/2601.07506)
*Dongryeol Lee,Yerin Hwang,Taegwan Kang,Minwoo Lee,Younhyung Chae,Kyomin Jung*

Main category: cs.CL

TL;DR: 本文发现大型语言模型（LLMs）作为自动评判者在基于参考答案的问答评价中存在一个关键缺陷：当参考答案与模型自身知识冲突时，评价结果不可靠，评价准确度显著下降。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs越来越多地用于自动问答评价，作者关注其能否真正遵循给定参考答案，尤其在参考和模型知识发生冲突的情况下。

Method: 设计了一种控制的“互换参考答案”框架，将正确答案替换为错误实体，制造参考与模型信念的冲突，评估不同评判模型在此条件下的表现。

Result: 发现评判准确性在互换参考条件下显著下降，且模型过度依赖内在知识，忽视了给定的参考答案。常见的基于提示的缓解策略未能解决该问题。

Conclusion: 表明基于参考的LLM自动评判存在根本性局限，需设计更强制遵守给定参考答案的评价协议以提高评价可靠性。

Abstract: While large language models (LLMs) are increasingly used as automatic judges for question answering (QA) and other reference-conditioned evaluation tasks, little is known about their ability to adhere to a provided reference. We identify a critical failure mode of such reference-based LLM QA evaluation: when the provided reference conflicts with the judge model's parametric knowledge, the resulting scores become unreliable, substantially degrading evaluation fidelity. To study this phenomenon systematically, we introduce a controlled swapped-reference QA framework that induces reference-belief conflicts. Specifically, we replace the reference answer with an incorrect entity and construct diverse pairings of original and swapped references with correspondingly aligned candidate answers. Surprisingly, grading reliability drops sharply under swapped references across a broad set of judge models. We empirically show that this vulnerability is driven by judges' over-reliance on parametric knowledge, leading judges to disregard the given reference under conflict. Finally, we find that this failure persists under common prompt-based mitigation strategies, highlighting a fundamental limitation of LLM-as-a-judge evaluation and motivating reference-based protocols that enforce stronger adherence to the provided reference.

</details>


### [123] [High-Rank Structured Modulation for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2601.07507)
*Yongkang Liu,Xing Li,Mengjie Zhao,Shanru Zhang,Zijing Wang,Qian Li,Shi Feng,Feiliang Ren,Daling Wang,Hinrich Schütze*

Main category: cs.CL

TL;DR: SMoA是一种高秩结构调制适配器，通过选择性调节预训练权重的特征，提高模型表现力和性能，优于低秩的LoRA方法。


<details>
  <summary>Details</summary>
Motivation: 随着模型参数数量增长，传统全参数微调资源消耗大，低秩LoRA虽然节省资源，但表达能力有限，需提升性能与效率的微调方法。

Method: 提出SMoA，冻结预训练权重，利用多子空间选择性放大或抑制重要特征，实现高秩结构调制，保持较少可训练参数同时提升模型容量。

Result: 在10个任务上，SMoA表现优于LoRA及其变种，且通过大量消融实验证明其有效性。

Conclusion: SMoA通过高秩结构调制有效提升了参数效率微调的表现能力，是改进大规模预训练模型微调的有效方法。

Abstract: As the number of model parameters increases, parameter-efficient fine-tuning (PEFT) has become the go-to choice for tailoring pre-trained large language models. Low-rank Adaptation (LoRA) uses a low-rank update method to simulate full parameter fine-tuning, which is widely used to reduce resource requirements. However, decreasing the rank encounters challenges with limited representational capacity when compared to full parameter fine-tuning. We present \textbf{SMoA}, a high-rank \textbf{S}tructured \textbf{MO}dulation \textbf{A}dapter that uses fewer trainable parameters while maintaining a higher rank, thereby improving the model's representational capacity and offering improved performance potential. The core idea is to freeze the original pretrained weights and selectively amplify or suppress important features of the original weights across multiple subspaces. The subspace mechanism provides an efficient way to increase the capacity and complexity of a model. We conduct both theoretical analyses and empirical studies on various tasks. Experiment results show that SMoA outperforms LoRA and its variants on 10 tasks, with extensive ablation studies validating its effectiveness.

</details>


### [124] [Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions](https://arxiv.org/abs/2601.07516)
*Yongqi Li,Hao Lang,Tieyun Qian,Yongbin Li*

Main category: cs.CL

TL;DR: 本文提出通过学习紧凑的潜在动作空间，利用观察学习机制和跨模态投影器，结合图文配对数据和纯文本数据，有效提升多模态对话代理的强化学习微调效果。


<details>
  <summary>Details</summary>
Motivation: 现有多模态对话代理在强化学习微调时受到大规模文本标记空间限制，导致微调效率和效果受限，需设计更高效的动作表示方法。

Method: 采用观察学习机制构建潜在动作空间的代码本，通过利用未来观察估计当前潜在动作并重构未来观察，结合图文配对数据和纯文本数据，设计跨模态投影器，并使用循环一致性损失训练以提升鲁棒性。

Result: 提出的潜在动作空间方法在多个强化学习算法和两个对话任务中均超过了竞争基线，表现出更好的泛化性能和对话适应能力。

Conclusion: 通过学习紧凑且鲁棒的潜在动作空间，结合多数据源训练策略，有效解决了多模态对话代理强化学习中的大文本空间挑战，提升了对话任务的性能。

Abstract: Vision-language models are increasingly employed as multimodal conversational agents (MCAs) for diverse conversational tasks. Recently, reinforcement learning (RL) has been widely explored for adapting MCAs to various human-AI interaction scenarios. Despite showing great enhancement in generalization performance, fine-tuning MCAs via RL still faces challenges in handling the extremely large text token space. To address this, we learn a compact latent action space for RL fine-tuning instead. Specifically, we adopt the learning from observation mechanism to construct the codebook for the latent action space, where future observations are leveraged to estimate current latent actions that could further be used to reconstruct future observations. However, the scarcity of paired image-text data hinders learning a codebook with sufficient coverage. Thus, we leverage both paired image-text data and text-only data to construct the latent action space, using a cross-modal projector for transforming text embeddings into image-text embeddings. We initialize the cross-modal projector on paired image-text data, and further train it on massive text-only data with a novel cycle consistency loss to enhance its robustness. We show that our latent action based method outperforms competitive baselines on two conversation tasks across various RL algorithms.

</details>


### [125] [Thinking Before Constraining: A Unified Decoding Framework for Large Language Models](https://arxiv.org/abs/2601.07525)
*Ngoc Trinh Hung Nguyen,Alonso Silva,Laith Zumot,Liubov Tupikina,Armen Aghasaryan,Mehwish Alam*

Main category: cs.CL

TL;DR: 本论文提出了一种结合自然生成和结构化生成优点的方法，提高了语言模型输出的准确性和可解析性。


<details>
  <summary>Details</summary>
Motivation: 自然生成虽然能提供丰富推理但缺乏结构保证，结构化生成保证输出规范但限制推理能力。

Method: 允许大模型自由推理至触发特定标记，再切换为结构化生成，以兼顾自由表达和结构化的可靠性。

Result: 在多个分类和推理数据集上，方法提升准确率最高达27%，且仅增加10-20个标记的开销。

Conclusion: 该方法有效融合自然与结构化生成的优点，提升了语言模型输出的质量和可解析性。

Abstract: Natural generation allows Language Models (LMs) to produce free-form responses with rich reasoning, but the lack of guaranteed structure makes outputs difficult to parse or verify. Structured generation, or constrained decoding, addresses this drawback by producing content in standardized formats such as JSON, ensuring consistency and guaranteed-parsable outputs, but it can inadvertently restrict the model's reasoning capabilities. In this work, we propose a simple approach that combines the advantages of both natural and structured generation. By allowing LLMs to reason freely until specific trigger tokens are generated, and then switching to structured generation, our method preserves the expressive power of natural language reasoning while ensuring the reliability of structured outputs. We further evaluate our approach on several datasets, covering both classification and reasoning tasks, to demonstrate its effectiveness, achieving a substantial gain of up to 27% in accuracy compared to natural generation, while requiring only a small overhead of 10-20 extra tokens.

</details>


### [126] [From RAG to Agentic RAG for Faithful Islamic Question Answering](https://arxiv.org/abs/2601.07528)
*Gagan Bhatia,Hamdy Mubarak,Mustafa Jarrar,George Mikros,Fadi Zaraket,Mahmoud Alhirthani,Mutaz Al-Khatib,Logan Cochrane,Kareem Darwish,Rashid Yahiaoui,Firoj Alam*

Main category: cs.CL

TL;DR: 本文针对伊斯兰问答中大语言模型的幻觉与回避问题，提出了ISLAMICFAITHQA双语生成基准和完整的伊斯兰知识建模套件，并设计了基于古兰经检索的Agentic RAG框架，实现了性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统的选择题评估无法捕捉自由形式回答中的幻觉和模型在缺少证据时是否能合理回避的重要失败模式，这在宗教问答中尤为关键。

Method: 构建了包含3810个双语问答的ISLAMICFAITHQA数据集，开发了包含阿拉伯文文本推理对、双语偏好样本和古兰经原文检索库的综合资源，设计基于结构化工具调用的agentic RAG框架实现迭代证据检索和回答修正。

Result: 实验表明，基于检索的方法提高了回答正确率，agentic RAG在阿拉伯语和英语多语言环境下表现出较传统RAG更优的性能和鲁棒性，且小模型（Qwen3 4B）也能达到最新技术水平。

Conclusion: 该研究提供了面向宗教问答的专用数据和方法，有效减少幻觉和提升答案可靠性，促进了相关领域的研究与应用发展，相关资源将公开共享。

Abstract: LLMs are increasingly used for Islamic question answering, where ungrounded responses may carry serious religious consequences. Yet standard MCQ/MRC-style evaluations do not capture key real-world failure modes, notably free-form hallucinations and whether models appropriately abstain when evidence is lacking. To shed a light on this aspect we introduce ISLAMICFAITHQA, a 3,810-item bilingual (Arabic/English) generative benchmark with atomic single-gold answers, which enables direct measurement of hallucination and abstention. We additionally developed an end-to-end grounded Islamic modelling suite consisting of (i) 25K Arabic text-grounded SFT reasoning pairs, (ii) 5K bilingual preference samples for reward-guided alignment, and (iii) a verse-level Qur'an retrieval corpus of $\sim$6k atomic verses (ayat). Building on these resources, we develop an agentic Quran-grounding framework (agentic RAG) that uses structured tool calls for iterative evidence seeking and answer revision. Experiments across Arabic-centric and multilingual LLMs show that retrieval improves correctness and that agentic RAG yields the largest gains beyond standard RAG, achieving state-of-the-art performance and stronger Arabic-English robustness even with a small model (i.e., Qwen3 4B). We will make the experimental resources and datasets publicly available for the community.

</details>


### [127] [A Unified Framework for Emotion Recognition and Sentiment Analysis via Expert-Guided Multimodal Fusion with Large Language Models](https://arxiv.org/abs/2601.07565)
*Jiaqi Qiao,Xiujuan Xu,Xinran Li,Yu Liu*

Main category: cs.CL

TL;DR: 本文提出了EGMF框架，将专家引导的多模态融合与大型语言模型结合，实现情感识别和情感分析的统一处理。


<details>
  <summary>Details</summary>
Motivation: 多模态情感理解需要有效整合文本、音频和视觉信息，以提升离散情感识别及连续情感分析的性能。

Method: 设计三个专家网络（细粒度局部专家、语义关联专家、全局上下文专家），通过层次动态门控自适应融合多模态特征，并通过伪令牌注入和提示机制将融合特征与大型语言模型结合，使用LoRA微调以提高计算效率。

Result: 在多个中英文双语情感分析基准数据集（MELD、CHERMA、MOSEI、SIMS-V2）上取得了优于现有顶尖方法的性能，并显示出良好的跨语言鲁棒性。

Conclusion: EGMF框架有效整合多模态信息和语言模型，推动了跨语言的多模态情感理解，未来将开源代码促进研究发展。

Abstract: Multimodal emotion understanding requires effective integration of text, audio, and visual modalities for both discrete emotion recognition and continuous sentiment analysis. We present EGMF, a unified framework combining expert-guided multimodal fusion with large language models. Our approach features three specialized expert networks--a fine-grained local expert for subtle emotional nuances, a semantic correlation expert for cross-modal relationships, and a global context expert for long-range dependencies--adaptively integrated through hierarchical dynamic gating for context-aware feature selection. Enhanced multimodal representations are integrated with LLMs via pseudo token injection and prompt-based conditioning, enabling a single generative framework to handle both classification and regression through natural language generation. We employ LoRA fine-tuning for computational efficiency. Experiments on bilingual benchmarks (MELD, CHERMA, MOSEI, SIMS-V2) demonstrate consistent improvements over state-of-the-art methods, with superior cross-lingual robustness revealing universal patterns in multimodal emotional expressions across English and Chinese. We will release the source code publicly.

</details>


### [128] [ES-Mem: Event Segmentation-Based Memory for Long-Term Dialogue Agents](https://arxiv.org/abs/2601.07582)
*Huhai Zou,Tianhao Sun,Chuanjiang He,Yu Tian,Zhenyang Li,Li Jin,Nayu Liu,Jiang Zhong,Kaiwen Wei*

Main category: cs.CL

TL;DR: 提出ES-Mem框架，通过动态事件分割和分层记忆结构提升对话记忆的连贯性和检索精度。


<details>
  <summary>Details</summary>
Motivation: 现有对话记忆机制存在粒度固定和平面检索方式导致语义碎片化及忽视话语结构的问题。

Method: 设计动态事件分割模块将长对话划分为语义连贯事件，并构建分层记忆架构利用边界语义实现精确的情景定位。

Result: 在两个记忆基准测试上，ES-Mem性能优于基线方法；事件分割模块在对话分割任务中表现稳健。

Conclusion: ES-Mem有效提升了对话记忆的语义完整性和检索精准度，证明事件分割和层级内存结构的实用价值。

Abstract: Memory is critical for dialogue agents to maintain coherence and enable continuous adaptation in long-term interactions. While existing memory mechanisms offer basic storage and retrieval capabilities, they are hindered by two primary limitations: (1) rigid memory granularity often disrupts semantic integrity, resulting in fragmented and incoherent memory units; (2) prevalent flat retrieval paradigms rely solely on surface-level semantic similarity, neglecting the structural cues of discourse required to navigate and locate specific episodic contexts. To mitigate these limitations, drawing inspiration from Event Segmentation Theory, we propose ES-Mem, a framework incorporating two core components: (1) a dynamic event segmentation module that partitions long-term interactions into semantically coherent events with distinct boundaries; (2) a hierarchical memory architecture that constructs multi-layered memories and leverages boundary semantics to anchor specific episodic memory for precise context localization. Evaluations on two memory benchmarks demonstrate that ES-Mem yields consistent performance gains over baseline methods. Furthermore, the proposed event segmentation module exhibits robust applicability on dialogue segmentation datasets.

</details>


### [129] [Proof of Time: A Benchmark for Evaluating Scientific Idea Judgments](https://arxiv.org/abs/2601.07606)
*Bingyang Ye,Shan Chen,Jingxuan Tu,Chen Liu,Zidi Xiong,Samuel Schmidgall,Danielle S. Bitterman*

Main category: cs.CL

TL;DR: 本文提出了PoT框架，通过模拟科学想法的未来结果（如引用和研究者议题变化）来评价大语言模型的判断质量。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏可扩展且可验证的方法来评估大语言模型对科学想法判断的质量。

Method: PoT框架利用一个截断时间点前的证据快照，在离线沙箱中让模型预测截断时间点之后的结果，实现对模型判断的可验证评估；并通过agent和非agent的比较，分析交互预算和工具使用对性能的影响。

Result: 在三万个实例的四个领域基准测试中，交互预算增加一般提升agent表现，但工具使用效果依任务强相关。

Conclusion: PoT结合时间分区的未来可验证目标与离线工具沙箱，支持大规模、可验证的未来导向科学想法判断评估。

Abstract: Large language models are increasingly being used to assess and forecast research ideas, yet we lack scalable ways to evaluate the quality of models' judgments about these scientific ideas. Towards this goal, we introduce PoT, a semi-verifiable benchmarking framework that links scientific idea judgments to downstream signals that become observable later (e.g., citations and shifts in researchers' agendas). PoT freezes a pre-cutoff snapshot of evidence in an offline sandbox and asks models to forecast post-cutoff outcomes, enabling verifiable evaluation when ground truth arrives, scalable benchmarking without exhaustive expert annotation, and analysis of human-model misalignment against signals such as peer-review awards. In addition, PoT provides a controlled testbed for agent-based research judgments that evaluate scientific ideas, comparing tool-using agents to non-agent baselines under prompt ablations and budget scaling. Across 30,000+ instances spanning four benchmark domains, we find that, compared with non-agent baselines, higher interaction budgets generally improve agent performance, while the benefit of tool use is strongly task-dependent. By combining time-partitioned, future-verifiable targets with an offline sandbox for tool use, PoT supports scalable evaluation of agents on future-facing scientific idea judgment tasks.

</details>


### [130] [Integrating Machine-Generated Short Descriptions into the Wikipedia Android App: A Pilot Deployment of Descartes](https://arxiv.org/abs/2601.07631)
*Marija Šakota,Dmitry Brant,Cooltey Feng,Shay Nowick,Amal Ramadan,Robin Schoenbaechler,Joseph Seddon,Jazmin Tanner,Isaac Johnson,Robert West*

Main category: cs.CL

TL;DR: 本文介绍了多语言短描述生成模型Descartes在维基百科安卓应用中的试点部署效果。


<details>
  <summary>Details</summary>
Motivation: 维基百科短描述覆盖不均，需提升多语言短描述生成质量和编辑协助。

Method: 将Descartes模型集成至编辑环境，为编辑提供短描述建议，覆盖12种语言，参与文章3900余篇，编辑375人。

Result: 90%的采纳建议质量评分≥3分，编辑既直接采纳亦修改，回退和举报率低。

Conclusion: Descartes短描述有效支持编辑缩减内容差距，部署需综合考虑技术、设计及社区保护机制。

Abstract: Short descriptions are a key part of the Wikipedia user experience, but their coverage remains uneven across languages and topics. In previous work, we introduced Descartes, a multilingual model for generating short descriptions. In this report, we present the results of a pilot deployment of Descartes in the Wikipedia Android app, where editors were offered suggestions based on outputs from Descartes while editing short descriptions. The experiment spanned 12 languages, with over 3,900 articles and 375 editors participating. Overall, 90% of accepted Descartes descriptions were rated at least 3 out of 5 in quality, and their average ratings were comparable to human-written ones. Editors adopted machine suggestions both directly and with modifications, while the rate of reverts and reports remained low. The pilot also revealed practical considerations for deployment, including latency, language-specific gaps, and the need for safeguards around sensitive topics. These results indicate that Descartes's short descriptions can support editors in reducing content gaps, provided that technical, design, and community guardrails are in place.

</details>


### [131] [PlaM: Training-Free Plateau-Guided Model Merging for Better Visual Grounding in MLLMs](https://arxiv.org/abs/2601.07645)
*Zijing Wang,Yongkang Liu,Mingyang Wang,Ercong Nie,Deyuan Chen,Zhengjie Zhao,Shi Feng,Daling Wang,Xiaocui Yang,Yifei Zhang,Hinrich Schütze*

Main category: cs.CL

TL;DR: 本文提出了一种无训练框架，通过层级视觉令牌屏蔽发现多模态大语言模型的三阶段模式，结合基于平台的模型合并方法，有效缓解了多模态微调导致的文本推理能力下降问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型微调虽然增强视觉理解，但却降低了其文本推理能力，影响模型整体多模态性能。

Method: 通过层级视觉令牌屏蔽揭示三阶段模式（早期模态分离、中期模态对齐、晚期模态退化），并提出基于平台指导的模型合并方法，有选择地将基础语言模型参数注入多模态模型中，避免性能退化。

Result: 在五个多模态大语言模型和九个基准测试上，所提方法显示了有效的性能提升。基于注意力的分析显示模型注意力从分散模式转向任务相关的视觉区域聚焦。

Conclusion: 本文提出的无训练模型合并方法有效解决了多模态微调中推理能力退化问题，提升了多模态大语言模型的性能和解释性。

Abstract: Multimodal Large Language Models (MLLMs) rely on strong linguistic reasoning inherited from their base language models. However, multimodal instruction fine-tuning paradoxically degrades this text's reasoning capability, undermining multimodal performance. To address this issue, we propose a training-free framework to mitigate this degradation. Through layer-wise vision token masking, we reveal a common three-stage pattern in multimodal large language models: early-modal separation, mid-modal alignment, and late-modal degradation. By analyzing the behavior of MLLMs at different stages, we propose a plateau-guided model merging method that selectively injects base language model parameters into MLLMs. Experimental results based on five MLLMs on nine benchmarks demonstrate the effectiveness of our method. Attention-based analysis further reveals that merging shifts attention from diffuse, scattered patterns to focused localization on task-relevant visual regions. Our repository is on https://github.com/wzj1718/PlaM.

</details>


### [132] [Order in the Evaluation Court: A Critical Analysis of NLG Evaluation Trends](https://arxiv.org/abs/2601.07648)
*Jing Yang,Nils Feldhus,Salar Mohtaj,Leonhard Hennig,Qianli Wang,Eleni Metheniti,Sherzod Hakimov,Charlott Jakob,Veronika Solopova,Konrad Rieck,David Schlangen,Sebastian Möller,Vera Schmitt*

Main category: cs.CL

TL;DR: 本文通过自动信息提取分析过去六年14171篇自然语言生成（NLG）论文的评估方法变化，揭示了任务差异、指标惯性以及人类评估与大模型评价（LaaJ）之间的差异。


<details>
  <summary>Details</summary>
Motivation: 尽管NLG技术进步显著，评估方法仍面临挑战，尤其是如何系统性地理解不同评估方式的演变及其有效性。

Method: 利用自动信息提取技术，从ACL、EMNLP、NAACL和INLG四大会议的论文中收集评估方法的元数据，分析指标使用、LaaJ和人类评估的应用趋势及关联。

Result: 发现对话生成任务快速采用LaaJ，机器翻译仍依赖传统n-gram指标，问答领域的人类评估逐渐减少；大部分任务依赖传统指标缺乏实证支持；LaaJ与人类评估侧重点不同，相关性低且缺少有效验证。

Conclusion: 当前NLG评估存在方法选择与应用不合理的问题，未来需改进评估标准和验证过程，以提升评估的科学性和严谨性。

Abstract: Despite advances in Natural Language Generation (NLG), evaluation remains challenging. Although various new metrics and LLM-as-a-judge (LaaJ) methods are proposed, human judgment persists as the gold standard. To systematically review how NLG evaluation has evolved, we employ an automatic information extraction scheme to gather key information from NLG papers, focusing on different evaluation methods (metrics, LaaJ and human evaluation). With extracted metadata from 14,171 papers across four major conferences (ACL, EMNLP, NAACL, and INLG) over the past six years, we reveal several critical findings: (1) Task Divergence: While Dialogue Generation demonstrates a rapid shift toward LaaJ (>40% in 2025), Machine Translation remains locked into n-gram metrics, and Question Answering exhibits a substantial decline in the proportion of studies conducting human evaluation. (2) Metric Inertia: Despite the development of semantic metrics, general-purpose metrics (e.g., BLEU, ROUGE) continue to be widely used across tasks without empirical justification, often lacking the discriminative power to distinguish between specific quality criteria. (3) Human-LaaJ Divergence: Our association analysis challenges the assumption that LLMs act as mere proxies for humans; LaaJ and human evaluations prioritize very different signals, and explicit validation is scarce (<8% of papers comparing the two), with only moderate to low correlation. Based on these observations, we derive practical recommendations to improve the rigor of future NLG evaluation.

</details>


### [133] [Adaptive Layer Selection for Layer-Wise Token Pruning in LLM Inference](https://arxiv.org/abs/2601.07667)
*Rei Taniguchi,Yuyang Dong,Makoto Onizuka,Chuan Xiao*

Main category: cs.CL

TL;DR: 提出ASL方法自适应选择KV缓存减小的层，实现了更好的准确性和性能平衡。


<details>
  <summary>Details</summary>
Motivation: 现有层级token剪枝方法选取固定层，灵活性差，准确性在不同任务间波动大。

Method: ASL利用注意力分数排序的token排名方差，训练中无需额外训练，在预填充阶段自适应选择token保留层，并可与其他方法联合使用。

Result: ASL在多个基准测试中优于现有方法，准确率更高，且保持了解码速度与KV缓存减小效果。

Conclusion: ASL为KV缓存减小提供了一种灵活高效的自适应层选择方案，兼顾任务准确率和资源节约。

Abstract: Due to the prevalence of large language models (LLMs), key-value (KV) cache reduction for LLM inference has received remarkable attention. Among numerous works that have been proposed in recent years, layer-wise token pruning approaches, which select a subset of tokens at particular layers to retain in KV cache and prune others, are one of the most popular schemes. They primarily adopt a set of pre-defined layers, at which tokens are selected. Such design is inflexible in the sense that the accuracy significantly varies across tasks and deteriorates in harder tasks such as KV retrieval. In this paper, we propose ASL, a training-free method that adaptively chooses the selection layer for KV cache reduction, exploiting the variance of token ranks ordered by attention score. The proposed method balances the performance across different tasks while meeting the user-specified KV budget requirement. ASL operates during the prefilling stage and can be jointly used with existing KV cache reduction methods such as SnapKV to optimize the decoding stage. By evaluations on the InfiniteBench, RULER, and NIAH benchmarks, we show that equipped with one-shot token selection, where tokens are selected at a layer and propagated to deeper layers, ASL outperforms state-of-the-art layer-wise token selection methods in accuracy while maintaining decoding speed and KV cache reduction.

</details>


### [134] [Exploring the Meta-level Reasoning of Large Language Models via a Tool-based Multi-hop Tabular Question Answering Task](https://arxiv.org/abs/2601.07696)
*Nick Ferguson,Alan Bundy,Kwabena Nuamah*

Main category: cs.CL

TL;DR: 本文区分了大语言模型的元级推理与对象级推理，设计了基于地缘政治指标的问答任务，分析了模型调用工具选择的推理能力，发现模型在元级推理上表现良好但理解有缺陷，提示n-shot提示对准确率影响不大，且模型数值计算能力较差。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型推理能力定义不一，作者旨在通过区分元级与对象级推理，设计任务更精准评估推理过程而非仅最终答案。

Method: 设计了基于地缘政治指标的问答任务，要求模型拆解问题、检索数据及进行数学运算，通过分析模型选择工具的行为评估其元级推理能力，同时引入关键操作以量化推理强度。

Result: 发现大语言模型在元级推理上表现良好，但对任务理解存在不足；n-shot提示对准确率影响有限；遇到错误信息时性能未明显恶化；数值计算能力较弱。

Conclusion: 本文提出的方法能深入分析大语言模型推理能力，揭示其优势与局限，并讨论了结果对其他任务领域的适用性和限制。

Abstract: Recent advancements in Large Language Models (LLMs) are increasingly focused on "reasoning" ability, a concept with many overlapping definitions in the LLM discourse. We take a more structured approach, distinguishing meta-level reasoning (denoting the process of reasoning about intermediate steps required to solve a task) from object-level reasoning (which concerns the low-level execution of the aforementioned steps.) We design a novel question answering task, which is based around the values of geopolitical indicators for various countries over various years. Questions require breaking down into intermediate steps, retrieval of data, and mathematical operations over that data. The meta-level reasoning ability of LLMs is analysed by examining the selection of appropriate tools for answering questions. To bring greater depth to the analysis of LLMs beyond final answer accuracy, our task contains 'essential actions' against which we can compare the tool call output of LLMs to infer the strength of reasoning ability. We find that LLMs demonstrate good meta-level reasoning on our task, yet are flawed in some aspects of task understanding. We find that n-shot prompting has little effect on accuracy; error messages encountered do not often deteriorate performance; and provide additional evidence for the poor numeracy of LLMs. Finally, we discuss the generalisation and limitation of our findings to other task domains.

</details>


### [135] [Emotional Support Evaluation Framework via Controllable and Diverse Seeker Simulator](https://arxiv.org/abs/2601.07698)
*Chaewon Heo,Cheyon Jin,Yohan Jo*

Main category: cs.CL

TL;DR: 本论文提出了一个基于九个心理和语言特征的情感支持求助者模拟器，通过专家混合模型提升行为多样性和可控性，实现更真实的求助者模拟，从而更准确地评估情感支持聊天机器人。


<details>
  <summary>Details</summary>
Motivation: 现有求助者模拟器缺乏真实行为多样性且不可控，难以反映真实求助者的复杂行为，影响情感支持聊天机器人的评估效果。

Method: 基于真实Reddit对话，采用专家混合模型（MoE）训练，利用九个心理和语言特征驱动求助者行为，实现行为细粒度的可控和多样化。

Result: 相比现有方法，提出的模拟器在模拟求助者行为的多样性和符合特定求助者画像方面表现更优；用该模拟器评测七个主流支持者模型，发现之前未暴露的性能下降。

Conclusion: 该求助者模拟器框架更真实地反映了求助者多样行为，可为情感支持聊天机器人的评估提供更严苛和可信的测试环境。

Abstract: As emotional support chatbots have recently gained significant traction across both research and industry, a common evaluation strategy has emerged: use help-seeker simulators to interact with supporter chatbots. However, current simulators suffer from two critical limitations: (1) they fail to capture the behavioral diversity of real-world seekers, often portraying them as overly cooperative, and (2) they lack the controllability required to simulate specific seeker profiles. To address these challenges, we present a controllable seeker simulator driven by nine psychological and linguistic features that underpin seeker behavior. Using authentic Reddit conversations, we train our model via a Mixture-of-Experts (MoE) architecture, which effectively differentiates diverse seeker behaviors into specialized parameter subspaces, thereby enhancing fine-grained controllability. Our simulator achieves superior profile adherence and behavioral diversity compared to existing approaches. Furthermore, evaluating 7 prominent supporter models with our system uncovers previously obscured performance degradations. These findings underscore the utility of our framework in providing a more faithful and stress-tested evaluation for emotional support chatbots.

</details>


### [136] [Is Agentic RAG worth it? An experimental comparison of RAG approaches](https://arxiv.org/abs/2601.07711)
*Pietro Ferrazzi,Milica Cvjeticanin,Alessio Piraccini,Davide Giannuzzi*

Main category: cs.CL

TL;DR: 本文评估了两种信息检索增强生成系统（RAG）范式——增强型RAG和自主型RAG，分析了各自的优缺点及应用场景。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统存在检索噪声、范围外查询误用、查询文档匹配弱等问题，新兴的自主型RAG利用大型语言模型自我反思能力，试图优化流程。

Method: 本文通过多场景和多维度的广泛实证评测，对比分析了增强型RAG和自主型RAG的性能、成本及适用性。

Result: 研究揭示了两种范式在不同条件下的权衡关系，增强型RAG通过专门模块解决弱点，自主型RAG则通过决策控制流程，二者各有优势。

Conclusion: 为现实应用中选择合适RAG设计提供了实用指导，建议结合应用需求和资源成本权衡使用增强型或自主型RAG。

Abstract: Retrieval-Augmented Generation (RAG) systems are usually defined by the combination of a generator and a retrieval component that extracts textual context from a knowledge base to answer user queries. However, such basic implementations exhibit several limitations, including noisy or suboptimal retrieval, misuse of retrieval for out-of-scope queries, weak query-document matching, and variability or cost associated with the generator. These shortcomings have motivated the development of "Enhanced" RAG, where dedicated modules are introduced to address specific weaknesses in the workflow. More recently, the growing self-reflective capabilities of Large Language Models (LLMs) have enabled a new paradigm, which we refer to as "Agentic" RAG. In this approach, the LLM orchestrates the entire process-deciding which actions to perform, when to perform them, and whether to iterate-thereby reducing reliance on fixed, manually engineered modules. Despite the rapid adoption of both paradigms, it remains unclear which approach is preferable under which conditions. In this work, we conduct an extensive, empirically driven evaluation of Enhanced and Agentic RAG across multiple scenarios and dimensions. Our results provide practical insights into the trade-offs between the two paradigms, offering guidance on selecting the most effective RAG design for real-world applications, considering both costs and performance.

</details>


### [137] [Structure First, Reason Next: Enhancing a Large Language Model using Knowledge Graph for Numerical Reasoning in Financial Documents](https://arxiv.org/abs/2601.07754)
*Aryan Mishra,Akash Anil*

Main category: cs.CL

TL;DR: 本文提出了一种结合知识图谱（KG）与大型语言模型（LLM）的框架，用于提升金融文档中的数值推理准确性，在FinQA数据集上验证提升了约12%的执行准确率。


<details>
  <summary>Details</summary>
Motivation: 金融文档数值复杂且结构多样，纯LLM难以准确提取和计算数字信息，亟需结合结构化信息提升数值推理能力。

Method: 从金融文档中提取基于设计的知识图谱，结合LLM预测进行数值推理，形成融合结构信息的推理框架。

Result: 在FinQA基准数据集上，使用Open-source LLM（Llama 3.1 8B Instruct）进行测试，框架提升了约12%的执行准确率。

Conclusion: 将结构化知识图谱融入LLM中，有效增强了金融领域数值推理的准确性，展示出结合结构与语言模型的方法优势。

Abstract: Numerical reasoning is an important task in the analysis of financial documents. It helps in understanding and performing numerical predictions with logical conclusions for the given query seeking answers from financial texts. Recently, Large Language Models (LLMs) have shown promising results in multiple Question-Answering (Q-A) systems with the capability of logical reasoning. As documents related to finance often consist of long and complex financial contexts, LLMs appear well-suited for building high-quality automated financial question-answering systems. However, LLMs often face challenges in accurately processing the various numbers within financial reports. Extracting numerical data from unstructured text and semi-structured tables, and reliably performing accurate calculations, remains a significant bottleneck for numerical reasoning in most state-of-the-art LLMs. Recent studies have shown that structured data augmentations, such as Knowledge Graphs (KGs), have notably improved the predictions of LLMs along with logical explanations. Thus, it is an important requirement to consider inherent structured information in financial reports while using LLMs for various financial analytics. This paper proposes a framework to incorporate structured information using KGs along with LLM predictions for numerical reasoning tasks. The KGs are extracted using a proposed schema inherently from the document under processing. We evaluated our proposed framework over the benchmark data FinQA, using an open-source LLM, namely Llama 3.1 8B Instruct. We observed that the proposed framework improved execution accuracy by approximately 12% relative to the vanilla LLM.

</details>


### [138] [Contrastive Learning with Narrative Twins for Modeling Story Salience](https://arxiv.org/abs/2601.07765)
*Igor Sterner,Alex Lascarides,Frank Keller*

Main category: cs.CL

TL;DR: 本文提出了一种对比学习框架，通过讲述叙事双胞胎故事来学习故事嵌入，从而更好地捕捉故事的关键事件。


<details>
  <summary>Details</summary>
Motivation: 理解叙事需要识别对故事进展最重要的事件，传统方法难以有效区分具有相同情节但不同表述的故事。

Method: 设计了一种对比学习模型，通过区分叙事双胞胎（拥有相同情节但表述不同的故事）和具有相似表面特征但情节不同的干扰故事来学习故事嵌入；并利用四种叙事学操作（删除、移动、破坏、总结）评估叙事显著性。

Result: 在ROCStories短叙事和维基百科长篇情节摘要数据上，对比学习的故事嵌入表现优于掩码语言模型基线，其中总结操作最有效；在缺乏叙事双胞胎时，可用随机置零生成双胞胎，干扰样本可通过大型语言模型提示生成或利用同一故事的不同部分。

Conclusion: 对比学习框架能够有效捕捉故事的叙事显著性，提升关键事件识别能力，为叙事理解和摘要等任务提供有力支持。

Abstract: Understanding narratives requires identifying which events are most salient for a story's progression. We present a contrastive learning framework for modeling narrative salience that learns story embeddings from narrative twins: stories that share the same plot but differ in surface form. Our model is trained to distinguish a story from both its narrative twin and a distractor with similar surface features but different plot. Using the resulting embeddings, we evaluate four narratologically motivated operations for inferring salience (deletion, shifting, disruption, and summarization). Experiments on short narratives from the ROCStories corpus and longer Wikipedia plot summaries show that contrastively learned story embeddings outperform a masked-language-model baseline, and that summarization is the most reliable operation for identifying salient sentences. If narrative twins are not available, random dropout can be used to generate the twins from a single story. Effective distractors can be obtained either by prompting LLMs or, in long-form narratives, by using different parts of the same story.

</details>


### [139] [Enhancing Self-Correction in Large Language Models through Multi-Perspective Reflection](https://arxiv.org/abs/2601.07780)
*Mariana Costa,Alberlucia Rafael Soarez,Daniel Kim,Camila Ferreira*

Main category: cs.CL

TL;DR: 本文提出了一种名为PR-CoT的多角度反思链式思维方法，通过多维度自我评估提升大型语言模型推理的一致性和准确性，在逻辑性和伦理决策等复杂任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有链式思维方法在处理复杂或伦理敏感任务时存在一致性、准确性和自我纠错能力不足的问题，单一维度反思方法改进效果有限。

Method: 提出PR-CoT方法，通过预定义的多个反思角度（逻辑一致性、信息完整性、偏见/伦理、备选方案）引导模型对初始链式思维结果进行多角度自评和改进，采用仅基于提示工程实现，无需模型再训练。

Result: 在算术、常识、伦理决策和逻辑谜题等任务中，基于GPT-3.5和GPT-4模型的实验结果显示PR-CoT在逻辑一致性和错误纠正方面显著优于传统链式思维及现有反思方法，尤其在伦理决策领域效果显著。

Conclusion: 多角度反思链式思维方法PR-CoT有效提升了大型语言模型的推理能力，其各反思视角均有助于模型性能提升，验证了多维度反思范式在复杂推理任务中的应用价值。

Abstract: While Chain-of-Thought (CoT) prompting advances LLM reasoning, challenges persist in consistency, accuracy, and self-correction, especially for complex or ethically sensitive tasks. Existing single-dimensional reflection methods offer insufficient improvements. We propose MyGO Poly-Reflective Chain-of-Thought (PR-CoT), a novel methodology employing structured multi-perspective reflection. After initial CoT, PR-CoT guides the LLM to self-assess its reasoning across multiple predefined angles: logical consistency, information completeness, biases/ethics, and alternative solutions. Implemented purely via prompt engineering, this process refines the initial CoT into a more robust and accurate final answer without model retraining. Experiments across arithmetic, commonsense, ethical decision-making, and logical puzzles, using GPT-three point five and GPT-four models, demonstrate PR-CoT's superior performance. It significantly outperforms traditional CoT and existing reflection methods in logical consistency and error correction, with notable gains in nuanced domains like ethical decision-making. Ablation studies, human evaluations, and qualitative analyses further validate the contribution of each reflection perspective and the overall efficacy of our poly-reflective paradigm in fostering more reliable LLM reasoning.

</details>


### [140] [Beyond Single-Shot: Multi-step Tool Retrieval via Query Planning](https://arxiv.org/abs/2601.07782)
*Wei Fang,James Glass*

Main category: cs.CL

TL;DR: 该论文提出了一种名为TOOLQP的轻量级框架，通过将检索过程视为迭代查询规划，有效提升了大规模动态工具库中复杂请求的检索性能。


<details>
  <summary>Details</summary>
Motivation: 传统单次密集检索方法难以应对抽象用户目标与技术文档之间的语义鸿沟，以及固定大小嵌入表示无法很好地建模组合工具的复杂性。

Method: TOOLQP通过将指令分解成子任务，并动态生成查询与检索器交互，逐步规划查询过程。训练过程包括使用合成查询轨迹和通过可验证奖励的强化学习进行优化。

Result: 实验结果显示，TOOLQP在无监督的零-shot泛化能力，多样检索器的鲁棒性以及下游代理执行方面均达到最新的最佳性能。

Conclusion: TOOLQP通过迭代查询规划有效解决了复杂检索问题，显著提升了大规模动态工具库开发中代理的执行能力和泛化能力。

Abstract: LLM agents operating over massive, dynamic tool libraries rely on effective retrieval, yet standard single-shot dense retrievers struggle with complex requests. These failures primarily stem from the disconnect between abstract user goals and technical documentation, and the limited capacity of fixed-size embeddings to model combinatorial tool compositions. To address these challenges, we propose TOOLQP, a lightweight framework that models retrieval as iterative query planning. Instead of single-shot matching, TOOLQP decomposes instructions into sub-tasks and dynamically generates queries to interact with the retriever, effectively bridging the semantic gap by targeting the specific sub-tasks required for composition. We train TOOLQP using synthetic query trajectories followed by optimization via Reinforcement Learning with Verifiable Rewards (RLVR). Experiments demonstrate that TOOLQP achieves state-of-the-art performance, exhibiting superior zero-shot generalization, robustness across diverse retrievers, and significant improvements in downstream agentic execution.

</details>


### [141] [Kinship Data Benchmark for Multi-hop Reasoning](https://arxiv.org/abs/2601.07794)
*Tianda Sun,Dimitar Kazakov*

Main category: cs.CL

TL;DR: 本文提出了KinshipQA基准测试，用于评估大型语言模型在多步推理，特别是家系关系推理方面的能力。通过生成不同文化背景下的家谱数据，并设计基于家族隐含关系的推理任务，系统地控制任务难度和文化假设。测试表明该基准能揭示模型间在多步推理和文化情境理解上的差异。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在多步推理能力上的评估不充分，特别是在结合多重信息形成连贯推断方面缺乏系统化测试。家系关系推理因其复杂的文化和关系链特点，是理想的多步推理测试场景。

Method: 构建生成式数据流水线，按需生成满足不同婚姻约束的文化特定家谱数据，形成复杂的关联家族树。基于这些家谱生成文本推理任务，要求模型在隐含关系链基础上进行推理。采用统一零样本协议测试六种最新大型语言模型，使用精确匹配和集合度量评估性能。

Result: KinshipQA测试显示不同模型在多步推理能力上的表现差异较大，且能反映出模型在不同文化设定下推理表现的系统性区别。

Conclusion: KinshipQA为多步推理能力的评估提供了一种新颖且可控的测试机制，特别适用于探测语言模型在文化背景与关系推理复杂性上的表现差异，有助于推动模型推理能力的进一步提升。

Abstract: Large language models (LLMs) are increasingly evaluated on their ability to perform multi-hop reasoning, i.e., to combine multiple pieces of information into a coherent inference. We introduce KinshipQA, a benchmark designed to probe this capability through reasoning over kinship relations. The central contribution of our work is a generative pipeline that produces, on demand, large-scale, realistic, and culture-specific genealogical data: collections of interconnected family trees that satisfy explicit marriage constraints associated with different kinship systems. This allows task difficulty, cultural assumptions, and relational depth to be systematically controlled and varied. From these genealogies, we derive textual inference tasks that require reasoning over implicit relational chains. We evaluate the resulting benchmark using six state-of-the-art LLMs, spanning both open-source and closed-source models, under a uniform zero-shot protocol with deterministic decoding. Performance is measured using exact-match and set-based metrics. Our results demonstrate that KinshipQA yields a wide spread of outcomes and exposes systematic differences in multi-hop reasoning across models and cultural settings.

</details>


### [142] [Learning Through Dialogue: Unpacking the Dynamics of Human-LLM Conversations on Political Issues](https://arxiv.org/abs/2601.07796)
*Shaz Furniturewala,Gerard Christopher Yeo,Kokil Jaidka*

Main category: cs.CL

TL;DR: 本文分析了大型语言模型（LLM）与用户关于社会政治话题的对话，揭示LLM解释如何影响用户政治知识和自信心的变化机制。


<details>
  <summary>Details</summary>
Motivation: 探究支持用户学习和参与的互动动态及其对政治知识和自信心的影响。

Method: 通过对397个LLM与人类的聊天记录进行语言和互动特征分析，结合中介和调节效应分析。

Result: 解释丰富性通过促进用户反思提升自信心，对知识增长的影响则完全通过认知参与实现，高政治效能感用户表现出不同的学习效果。

Conclusion: 基于用户的参与状态调整LLM解释行为是实现有效学习的关键，学习效果取决于互动过程而非单一解释质量。

Abstract: Large language models (LLMs) are increasingly used as conversational partners for learning, yet the interactional dynamics supporting users' learning and engagement are understudied. We analyze the linguistic and interactional features from both LLM and participant chats across 397 human-LLM conversations about socio-political issues to identify the mechanisms and conditions under which LLM explanations shape changes in political knowledge and confidence. Mediation analyses reveal that LLM explanatory richness partially supports confidence by fostering users' reflective insight, whereas its effect on knowledge gain operates entirely through users' cognitive engagement. Moderation analyses show that these effects are highly conditional and vary by political efficacy. Confidence gains depend on how high-efficacy users experience and resolve uncertainty. Knowledge gains depend on high-efficacy users' ability to leverage extended interaction, with longer conversations benefiting primarily reflective users. In summary, we find that learning from LLMs is an interactional achievement, not a uniform outcome of better explanations. The findings underscore the importance of aligning LLM explanatory behavior with users' engagement states to support effective learning in designing Human-AI interactive systems.

</details>


### [143] [The Confidence Trap: Gender Bias and Predictive Certainty in LLMs](https://arxiv.org/abs/2601.07806)
*Ahmed Sabir,Markus Kängsepp,Rajesh Sharma*

Main category: cs.CL

TL;DR: 本文研究大规模语言模型在性别偏见任务中的置信度校准，提出了新的性别偏见校准指标Gender-ECE，并发现Gemma-2模型表现最差。


<details>
  <summary>Details</summary>
Motivation: 随着大规模语言模型在敏感领域的应用增多，关注其置信度与公平性、偏见之间的关系变得重要，尤其是评估其在性别偏见方面的表现与置信度一致性。

Method: 聚焦于带性别代词的消解任务，评估六种先进模型的概率置信度校准情况，并设计了新的校准指标Gender-ECE来衡量性别差异。

Result: 实验结果显示，Gemma-2在性别偏见基准测试中的置信度校准表现最差，其他模型表现相对较好。

Conclusion: 本文提出了结合公平性考量的置信度校准评价方法和新的性别偏差校准指标，为大规模语言模型的伦理部署提供了指导。

Abstract: The increased use of Large Language Models (LLMs) in sensitive domains leads to growing interest in how their confidence scores correspond to fairness and bias. This study examines the alignment between LLM-predicted confidence and human-annotated bias judgments. Focusing on gender bias, the research investigates probability confidence calibration in contexts involving gendered pronoun resolution. The goal is to evaluate if calibration metrics based on predicted confidence scores effectively capture fairness-related disparities in LLMs. The results show that, among the six state-of-the-art models, Gemma-2 demonstrates the worst calibration according to the gender bias benchmark. The primary contribution of this work is a fairness-aware evaluation of LLMs' confidence calibration, offering guidance for ethical deployment. In addition, we introduce a new calibration metric, Gender-ECE, designed to measure gender disparities in resolution tasks.

</details>


### [144] [Reference Games as a Testbed for the Alignment of Model Uncertainty and Clarification Requests](https://arxiv.org/abs/2601.07820)
*Manar Ali,Judith Sieker,Sina Zarrieß,Hendrik Buschmeier*

Main category: cs.CL

TL;DR: 本文探讨语言模型是否能在对话中扮演不确定时请求澄清的角色，通过引用游戏测试模型识别和表达不确定性的能力。


<details>
  <summary>Details</summary>
Motivation: 在对话中，人们通过请求澄清来维持相互理解，语言模型是否能表现出类似行为尚未明确。

Method: 利用引用游戏作为测试平台，比较三种视觉-语言模型在基线任务和指示其在不确定时请求澄清任务中的表现。

Result: 结果显示模型即使在简单任务中也难以识别自身不确定性并做出适当的澄清请求。

Conclusion: 引用游戏是评估语言及视觉语言模型交互能力的有效测试平台，当前模型在表达不确定性方面仍存在不足。

Abstract: In human conversation, both interlocutors play an active role in maintaining mutual understanding. When addressees are uncertain about what speakers mean, for example, they can request clarification. It is an open question for language models whether they can assume a similar addressee role, recognizing and expressing their own uncertainty through clarification. We argue that reference games are a good testbed to approach this question as they are controlled, self-contained, and make clarification needs explicit and measurable. To test this, we evaluate three vision-language models comparing a baseline reference resolution task to an experiment where the models are instructed to request clarification when uncertain. The results suggest that even in such simple tasks, models often struggle to recognize internal uncertainty and translate it into adequate clarification behavior. This demonstrates the value of reference games as testbeds for interaction qualities of (vision and) language models.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [145] [DemMA: Dementia Multi-Turn Dialogue Agent with Expert-Guided Reasoning and Action Simulation](https://arxiv.org/abs/2601.06373)
*Yutong Song,Jiang Wu,Kazi Sharif,Honghui Xu,Nikil Dutt,Amir Rahmani*

Main category: cs.MA

TL;DR: DemMA是一个结合专家指导的多轮认知障碍患者对话模拟系统，集成病理信息、个性特征及亚型记忆状态，模拟包括非语言行为的高保真患者表现，采用连锁思维蒸馏训练方法显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 模拟认知障碍患者需要同时建模认知障碍、情绪动态和非语言行为，现有方法难以实现高保真长期对话模拟。

Method: DemMA通过整合临床专家提供的病理信息、个性特征和亚型记忆状态建立患者人格模型，显式模拟运动、面部表情和声学信号等非语言行为。引入连锁思维蒸馏框架，让单个大语言模型一次生成推理轨迹、患者话语及行为动作，提高效率。

Result: 通过专家、医学生和LLM评审的多维评估，DemMA在多个指标上明显优于强基线模型。

Conclusion: DemMA有效实现了高保真、多轮认知障碍患者模拟，具备模型效率和表现优势，可助力相关医疗训练和研究。

Abstract: Simulating dementia patients with large language models (LLMs) is challenging due to the need to jointly model cognitive impairment, emotional dynamics, and nonverbal behaviors over long conversations. We present DemMA, an expert-guided dementia dialogue agent for high-fidelity multi-turn patient simulation. DemMA constructs clinically grounded dementia personas by integrating pathology information, personality traits, and subtype-specific memory-status personas informed by clinical experts. To move beyond text-only simulation, DemMA explicitly models nonverbal behaviors, including motion, facial expressions, and vocal cues. We further introduce a Chain-of-Thought distillation framework that trains a single LLM to jointly generate reasoning traces, patient utterances, and aligned behavioral actions within one forward pass, enabling efficient deployment without multi-agent inference. Extensive evaluations with experts, medical students, and LLM judges demonstrate that DemMA significantly outperforms strong baselines across multiple metrics.

</details>


### [146] [Dynamic Incentivized Cooperation under Changing Rewards](https://arxiv.org/abs/2601.06382)
*Philipp Altmann,Thomy Phan,Maximilian Zorn,Claudia Linnhoff-Popien,Sven Koenig*

Main category: cs.MA

TL;DR: 本文提出了一种名为DRIVE的动态奖励激励方法，解决了多代理强化学习中激励数值固定且对环境奖励变化敏感的问题，实现了在奖励变化环境下的合作维持。


<details>
  <summary>Details</summary>
Motivation: 现有同行激励方法依赖固定激励数值，难以应对环境奖励变化，导致合作难以维持。

Method: 提出动态奖励激励机制DRIVE，通过相互交换奖励差异以自适应激励，实现完全去中心化的合作。

Result: DRIVE在囚徒困境等社交困境中成功实现和维持了合作，优于现有固定激励方法。

Conclusion: DRIVE有效解决了因奖励变化导致的合作崩溃问题，提高了多智能体系统在动态环境下的合作稳定性。

Abstract: Peer incentivization (PI) is a popular multi-agent reinforcement learning approach where all agents can reward or penalize each other to achieve cooperation in social dilemmas. Despite their potential for scalable cooperation, current PI methods heavily depend on fixed incentive values that need to be appropriately chosen with respect to the environmental rewards and thus are highly sensitive to their changes. Therefore, they fail to maintain cooperation under changing rewards in the environment, e.g., caused by modified specifications, varying supply and demand, or sensory flaws - even when the conditions for mutual cooperation remain the same. In this paper, we propose Dynamic Reward Incentives for Variable Exchange (DRIVE), an adaptive PI approach to cooperation in social dilemmas with changing rewards. DRIVE agents reciprocally exchange reward differences to incentivize mutual cooperation in a completely decentralized way. We show how DRIVE achieves mutual cooperation in the general Prisoner's Dilemma and empirically evaluate DRIVE in more complex sequential social dilemmas with changing rewards, demonstrating its ability to achieve and maintain cooperation, in contrast to current state-of-the-art PI methods.

</details>


### [147] [Bi-Mem: Bidirectional Construction of Hierarchical Memory for Personalized LLMs via Inductive-Reflective Agents](https://arxiv.org/abs/2601.06490)
*Wenyu Mao,Haosong Tan,Shuchang Liu,Haoyang Liu,Yifan Xu,Huaxiang Ji,Xiang Wang*

Main category: cs.MA

TL;DR: 本文提出了一种名为Bi-Mem的双向构建框架，通过分层记忆结构保证用户长期对话中的记忆准确性，实现个性化交互。


<details>
  <summary>Details</summary>
Motivation: 传统层次记忆方法在聚类历史对话时容易放大对话噪声和记忆幻觉，导致记忆与用户全局人格不匹配，影响个性化效果。

Method: Bi-Mem由归纳代理提取事实级记忆，利用图聚类生成场景级记忆，推断人格级记忆；反思代理基于全局人格约束校准局部场景记忆，确保全局与局部对齐。同时设计联想检索机制，提升记忆调用的连贯性。

Result: 实证评估显示，Bi-Mem在长期个性化对话的问答性能上有显著提升。

Conclusion: 通过双向构建和联想检索机制，Bi-Mem有效缓解了聚类噪声及记忆幻觉问题，实现了更精准的层次记忆及个性化对话。

Abstract: Constructing memory from users' long-term conversations overcomes LLMs' contextual limitations and enables personalized interactions. Recent studies focus on hierarchical memory to model users' multi-granular behavioral patterns via clustering and aggregating historical conversations. However, conversational noise and memory hallucinations can be amplified during clustering, causing locally aggregated memories to misalign with the user's global persona. To mitigate this issue, we propose Bi-Mem, an agentic framework ensuring hierarchical memory fidelity through bidirectional construction. Specifically, we deploy an inductive agent to form the hierarchical memory: it extracts factual information from raw conversations to form fact-level memory, aggregates them into thematic scenes (i.e., local scene-level memory) using graph clustering, and infers users' profiles as global persona-level memory. Simultaneously, a reflective agent is designed to calibrate local scene-level memories using global constraints derived from the persona-level memory, thereby enforcing global-local alignment. For coherent memory recall, we propose an associative retrieval mechanism: beyond initial hierarchical search, a spreading activation process allows facts to evoke contextual scenes, while scene-level matches retrieve salient supporting factual information. Empirical evaluations demonstrate that Bi-Mem achieves significant improvements in question answering performance on long-term personalized conversational tasks.

</details>


### [148] [The Axiom of Consent: Friction Dynamics in Multi-Agent Coordination](https://arxiv.org/abs/2601.06692)
*Murad Farzulla*

Main category: cs.MA

TL;DR: 本文构建了一个基于“同意公理”的多智能体协调摩擦分析框架，通过引入协调摩擦方程与可复制优化机制，量化了协调难度和资源分配的合法性及摩擦，预测了多智能体系统的表现并应用于多领域。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在协调过程中面临偏好多样、权益不对称和信息不完全等挑战，导致协调失败和系统摩擦，需要一个统一的数学框架来理解和量化这些协调摩擦。

Method: 基于“行动需相关智能体按权益比例授权”的同意公理，提出了核心三元组(偏好一致α、权益σ、熵ε)和协调摩擦方程F=σ(1+ε)/(1+α)；建立了复制优化机制(ROM)用于模拟策略演化，提出资源同意、协调合法性和摩擦感知分配的形式定义。

Result: 建立了协调摩擦数学模型，显示高奖励一致性加速多智能体系统收敛，考虑权益不对称可降低协调失败，人机对齐度不足导致摩擦增加；框架在加密货币治理和政治系统中得到验证，统一了多领域协调摩擦动力学规律。

Conclusion: 该研究构建了一个可验证的复杂系统协调摩擦理论框架，揭示了权衡一致性、权益大小和沟通熵对协调效率的影响，为设计更高效公平的多智能体协作机制提供了理论基础。

Abstract: Multi-agent systems face a fundamental coordination problem: agents must coordinate despite heterogeneous preferences, asymmetric stakes, and imperfect information. When coordination fails, friction emerges: measurable resistance manifesting as deadlock, thrashing, communication overhead, or outright conflict. This paper derives a formal framework for analyzing coordination friction from a single axiom: actions affecting agents require authorization from those agents in proportion to stakes.
  From this axiom of consent, we establish the kernel triple $(α, σ, ε)$ (alignment, stake, and entropy) characterizing any resource allocation configuration. The friction equation $F = σ (1 + ε)/(1 + α)$ predicts coordination difficulty as a function of preference alignment $α$, stake magnitude $σ$, and communication entropy $ε$. The Replicator-Optimization Mechanism (ROM) governs evolutionary selection over coordination strategies: configurations generating less friction persist longer, establishing consent-respecting arrangements as dynamical attractors rather than normative ideals.
  We develop formal definitions for resource consent, coordination legitimacy, and friction-aware allocation in multi-agent systems. The framework yields testable predictions: MARL systems with higher reward alignment exhibit faster convergence; distributed allocations accounting for stake asymmetry generate lower coordination failure; AI systems with interpretability deficits produce friction proportional to the human-AI alignment gap. Applications to cryptocurrency governance and political systems demonstrate that the same equations govern friction dynamics across domains, providing a complexity science perspective on coordination under preference heterogeneity.

</details>


### [149] [Logic-Driven Semantic Communication for Resilient Multi-Agent Systems](https://arxiv.org/abs/2601.06733)
*Tamara Alshammari,Mehdi Bennis*

Main category: cs.MA

TL;DR: 本文提出了多智能体系统（MAS）弹性的统一定义，结合知识恢复与行动协调两个维度，设计了去中心化算法和代理架构，实现系统在动态环境下的快速恢复与持续运作。


<details>
  <summary>Details</summary>
Motivation: 当前研究多聚焦于故障容忍，缺乏对多智能体系统弹性的统一定义和设计方法，限制了系统在动态环境中持续感知、适应和恢复的能力。

Method: 基于时间认知逻辑形式化MAS弹性，定义恢复时间和持续时间指标，设计相应的代理架构和去中心化算法，实现知识和行动上的恢复与协调，并提供形式验证保证。

Result: 通过分布式多智能体决策的案例研究，所提方法在弹性表现上优于基线方法；验证分析显示所设计规范满足指标要求，支持设计时认证和运行时轻量级监控。

Conclusion: 本文框架为下一代通信系统中去中心化MAS的弹性设计奠定基础，赋能基于知识的弹性决策与持续运行。

Abstract: The advent of 6G networks is accelerating autonomy and intelligence in large-scale, decentralized multi-agent systems (MAS). While this evolution enables adaptive behavior, it also heightens vulnerability to stressors such as environmental changes and adversarial behavior. Existing literature on resilience in decentralized MAS largely focuses on isolated aspects, such as fault tolerance, without offering a principled unified definition of multi-agent resilience. This gap limits the ability to design systems that can continuously sense, adapt, and recover under dynamic conditions. This article proposes a formal definition of MAS resilience grounded in two complementary dimensions: epistemic resilience, wherein agents recover and sustain accurate knowledge of the environment, and action resilience, wherein agents leverage that knowledge to coordinate and sustain goals under disruptions. We formalize resilience via temporal epistemic logic and quantify it using recoverability time (how quickly desired properties are re-established after a disturbance) and durability time (how long accurate beliefs and goal-directed behavior are sustained after recovery). We design an agent architecture and develop decentralized algorithms to achieve both epistemic and action resilience. We provide formal verification guarantees, showing that our specifications are sound with respect to the metric bounds and admit finite-horizon verification, enabling design-time certification and lightweight runtime monitoring. Through a case study on distributed multi-agent decision-making under stressors, we show that our approach outperforms baseline methods. Our formal verification analysis and simulation results highlight that the proposed framework enables resilient, knowledge-driven decision-making and sustained operation, laying the groundwork for resilient decentralized MAS in next-generation communication systems.

</details>


### [150] [Agents of Diffusion: Enhancing Diffusion Language Models with Multi-Agent Reinforcement Learning for Structured Data Generation (Extended Version)](https://arxiv.org/abs/2601.07152)
*Aja Khanal,Kaushik T. Ranade,Rishabh Agrawal,Kalyan S. Basu,Apurva Narayan*

Main category: cs.MA

TL;DR: 该论文提出了Agents of Diffusion (AoD)框架，结合扩散语言模型和自回归模型的优点，通过多智能体协作实现高语义多样性且符合结构约束的JSON生成。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在生成结构化数据时，要么保证结构一致性但缺乏语义多样性，要么能丰富语义但难以保持结构，存在矛盾。

Method: 提出AoD框架，将结构化文本生成视为多智能体对齐过程，利用提示优化代理和评判代理通过自然语言反馈引导扩散语言模型，无需修改模型参数或使用手工约束。

Result: AoD在多个结构化数据基准测试中，较传统扩散和自回归模型表现更优，兼顾了语义创新性和结构一致性。

Conclusion: 通过多智能体语言引导的强化学习，扩散模型能够实现高质量且结构感知的可控文本生成，开辟了新的研究方向。

Abstract: Generating high-quality structured data such as JSON records, remains a fundamental challenge for large language models (LLMs), particularly when semantic richness must coexist with strict schema adherence. While autoregressive LLMs offer strong structural consistency, they often struggle with semantic variation and output diversity. In contrast, diffusion language models (DLMs) introduce powerful mechanisms for semantic richness and bidirectional decoding, yet lack the inductive biases needed for reliable structure preservation. We present Agents of Diffusion (AoD), a novel framework that unifies the generative flexibility of DLMs with the reasoning capabilities of autoregressive models through language-mediated reinforcement learning. AoD frames structured text generation as a multi-agent alignment process, where a prompt optimization agent collaborates with a judge agent to iteratively guide a DLM using natural language feedback. This approach enables controllable, schema-consistent generation without modifying model parameters or relying on handcrafted constraints. AoD advances the state of controllable generation by demonstrating that diffusion models, when supervised by cooperative agents, can achieve both high semantic novelty and structural fidelity. Across multiple structured data benchmarks, AoD consistently outperforms diffusion and autoregressive baselines, establishing a new path forward for structure-aware, diversity-enhanced text synthesis.

</details>


### [151] [DarwinTOD: LLM Driven Lifelong Self Evolution for Task Oriented Dialog Systems](https://arxiv.org/abs/2601.07248)
*Shuyu Zhang,Yujie Liu,Xinru Wang,Cheng Zhang,Yanmin Zhu,Bin Li*

Main category: cs.MA

TL;DR: 本文提出了DarwinTOD，一个能够自主终身进化的任务导向对话系统框架，通过结合进化计算和大型语言模型，实现从零样本基础上持续优化策略，无需人工干预。


<details>
  <summary>Details</summary>
Motivation: 传统任务导向对话系统无法在部署后从持续交互中进化或适应新领域，且现有持续学习方法依赖人工标注重新训练，缺乏自主终身优化能力。

Method: 提出DarwinTOD框架，整合进化计算和大型语言模型，采用双环机制：在线多智能体对话执行与同行评议，离线结构化进化操作，利用反馈持续优化策略库。

Result: 大量实验显示DarwinTOD超越了之前的最先进方法，并在进化过程中展现连续的性能提升。

Conclusion: DarwinTOD提供了一个实现对话系统自主终身进化的新范式，支持无监督零样本持续优化，减少人工干预，实现动态环境中的表现提升。

Abstract: Traditional task-oriented dialog systems are unable to evolve from ongoing interactions or adapt to new domains after deployment, that is a critical limitation in real-world dynamic environments. Continual learning approaches depend on episodic retraining with human curated data, failing to achieve autonomy lifelong improvement. While evolutionary computation and LLM driven self improvement offer promising mechanisms for dialog optimization, they lack a unified framework for holistic, iterative strategy refinement. To bridge this gap, we propose DarwinTOD, a lifelong self evolving dialog framework that systematically integrates these two paradigms, enabling continuous strategy optimization from a zero-shot base without task specific fine-tuning. DarwinTOD maintains an Evolvable Strategy Bank and operates through a dual-loop process: online multi-agent dialog execution with peer critique, and offline structured evolutionary operations that refine the strategy bank using accumulated feedback. This closed-loop design enables autonomous continuous improvement without human intervention. Extensive experiments show that DarwinTOD surpasses previous state-of-the-art methods and exhibits continuous performance gains throughout evolution. Our work provides a novel framework for building dialog systems with lifelong self evolution capabilities.

</details>


### [152] [SwarmFoam: An OpenFOAM Multi-Agent System Based on Multiple Types of Large Language Models](https://arxiv.org/abs/2601.07252)
*Chunwei Yang,Yankai Wang,Jianxiang Tang,Haojie Qu,Ziqiang Zou,YuLiu,Chunrui Deng,Zhifang Qiu,Ming Ding*

Main category: cs.MA

TL;DR: SwarmFoam是一种新的多智能体仿真框架，通过多模态感知和智能纠错，实现了对复杂流体动力学模拟的高效处理，测试显示其适应性和通过率较高。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的多智能体系统在处理复杂几何结构时存在显著局限，难以满足高复杂度的计算流体动力学（CFD）仿真需求。

Method: 提出SwarmFoam框架，结合多模态感知、智能错误纠正和检索增强生成技术，通过图像和高级指令的双重解析，实现更复杂的CFD仿真。

Result: 实验结果表明，SwarmFoam能够适应不同模态的仿真输入，25个测试案例整体通过率达84%，自然语言输入和多模态输入的通过率分别为80%和86.7%。

Conclusion: SwarmFoam的成功应用展示了多智能体方法在智能CFD仿真中的潜力，有望推动该领域的发展。

Abstract: Numerical simulation is one of the mainstream methods in scientific research, typically performed by professional engineers. With the advancement of multi-agent technology, using collaborating agents to replicate human behavior shows immense potential for intelligent Computational Fluid Dynamics (CFD) simulations. Some muti-agent systems based on Large Language Models have been proposed. However, they exhibit significant limitations when dealing with complex geometries. This paper introduces a new multi-agent simulation framework, SwarmFoam. SwarmFoam integrates functionalities such as Multi-modal perception, Intelligent error correction, and Retrieval-Augmented Generation, aiming to achieve more complex simulations through dual parsing of images and high-level instructions. Experimental results demonstrate that SwarmFoam has good adaptability to simulation inputs from different modalities. The overall pass rate for 25 test cases was 84%, with natural language and multi-modal input cases achieving pass rates of 80% and 86.7%, respectively. The work presented by SwarmFoam will further promote the development of intelligent agent methods for CFD.

</details>


### [153] [VLM-CAD: VLM-Optimized Collaborative Agent Design Workflow for Analog Circuit Sizing](https://arxiv.org/abs/2601.07315)
*Guanyuan Pan,Yugui Lin,Tiansheng Zhou,Pietro Liò,Shuai Wang,Yaqi Wang*

Main category: cs.MA

TL;DR: 本文提出了一种基于视觉语言模型优化的模拟混合信号电路尺寸设计协同流程（VLM-CAD），通过图像到网络的电路标注和解释性贝叶斯优化方法，实现高效精准的电路尺寸优化。


<details>
  <summary>Details</summary>
Motivation: 现有自动模拟电路尺寸设计方法未充分利用电路原理图，缺乏行业采用所需的解释性，且设计空间复杂难以优化。

Method: VLM-CAD流程结合Image2Net对电路原理图进行结构化JSON标注，由视觉语言模型精准理解电路，采用Explainable Trust Region Bayesian Optimization（ExTuRBO）方法，利用代理生成的初值协同启动并进行双粒度敏感度分析，支持外部尺寸优化和最终设计报告生成。

Result: 在180nm、90nm和45nm工艺的放大器尺寸设计任务中，VLM-CAD实现100%优化成功率，有效平衡功耗和性能，且总运行时间均控制在43分钟以内。

Conclusion: VLM-CAD流程通过结合视觉语言模型与解释性贝叶斯优化，不仅提高了模拟电路尺寸设计的效率和效果，还提升了设计结果的可解释性，具备良好的行业应用前景。

Abstract: Analog mixed-signal circuit sizing involves complex trade-offs within high-dimensional design spaces. Existing automatic analog circuit sizing approaches often underutilize circuit schematics and lack the explainability required for industry adoption. To tackle these challenges, we propose a Vision Language Model-optimized collaborative agent design workflow (VLM-CAD), which analyzes circuits, optimizes DC operating points, performs inference-based sizing and executes external sizing optimization. We integrate Image2Net to annotate circuit schematics and generate a structured JSON description for precise interpretation by Vision Language Models. Furthermore, we propose an Explainable Trust Region Bayesian Optimization method (ExTuRBO) that employs collaborative warm-starting from agent-generated seeds and offers dual-granularity sensitivity analysis for external sizing optimization, supporting a comprehensive final design report. Experiment results on amplifier sizing tasks using 180nm, 90nm, and 45nm Predictive Technology Models demonstrate that VLM-CAD effectively balances power and performance, achieving a 100% success rate in optimizing an amplifier with a complementary input and a class-AB output stage, while maintaining total runtime under 43 minutes across all experiments.

</details>


### [154] [Self-Creating Random Walks for Decentralized Learning under Pac-Man Attacks](https://arxiv.org/abs/2601.07674)
*Xingran Chen,Parimal Parag,Rohit Bhagat,Salim El Rouayheb*

Main category: cs.MA

TL;DR: 本文研究了随机游走算法在分布式学习中的安全威胁，并提出了能抵抗“Pac-Man”攻击的去中心化算法。


<details>
  <summary>Details</summary>
Motivation: 随机游走算法因其低开销和扩展性广泛应用于分布式系统，但其依赖局部交互，易受到恶意节点通过终止随机游走过程的攻击，导致学习过程停滞。

Method: 提出了CREATE-IF-LATE (CIL)算法，通过自我生成随机游走的机制避免随机游走的消失，确保学习过程继续进行。

Result: 理论分析证明CIL算法保证随机游走不会消失且数量有界，并实现基于随机游走的随机梯度下降在有“Pac-Man”攻击情况下收敛，且最多带来线性时间延迟。实验验证了理论结果。

Conclusion: CIL算法有效抵抗了“Pac-Man”恶意攻击，保证了分布式学习的鲁棒性和收敛性，提升了随机游走算法在安全敏感环境中的适用性。

Abstract: Random walk (RW)-based algorithms have long been popular in distributed systems due to low overheads and scalability, with recent growing applications in decentralized learning. However, their reliance on local interactions makes them inherently vulnerable to malicious behavior. In this work, we investigate an adversarial threat that we term the ``Pac-Man'' attack, in which a malicious node probabilistically terminates any RW that visits it. This stealthy behavior gradually eliminates active RWs from the network, effectively halting the learning process without triggering failure alarms. To counter this threat, we propose the CREATE-IF-LATE (CIL) algorithm, which is a fully decentralized, resilient mechanism that enables self-creating RWs and prevents RW extinction in the presence of Pac-Man. Our theoretical analysis shows that the CIL algorithm guarantees several desirable properties, such as (i) non-extinction of the RW population, (ii) almost sure boundedness of the RW population, and (iii) convergence of RW-based stochastic gradient descent even in the presence of Pac-Man with a quantifiable deviation from the true optimum. Moreover, the learning process experiences at most a linear time delay due to Pac-Man interruptions and RW regeneration. Our extensive empirical results on both synthetic and public benchmark datasets validate our theoretical findings.

</details>


### [155] [OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent](https://arxiv.org/abs/2601.07779)
*Bowen Yang,Kaiming Jin,Zhenyu Wu,Zhaoyang Liu,Qiushi Sun,Zehao Li,JingJing Xie,Zhoumianze Liu,Fangzhi Xu,Kanzhi Cheng,Qingyun Li,Yian Wang,Yu Qiao,Zun Wang,Zichen Ding*

Main category: cs.MA

TL;DR: 提出OS-Symphony框架，通过反思记忆代理和多功能工具代理提升视觉语言模型在长流程任务中的鲁棒性和泛化能力，取得了优异的实验结果。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在长时间流程和新领域应用中缺乏对视觉历史信息的细粒度控制及视觉感知的教程检索，导致鲁棒性和泛化能力不足。

Method: 设计了Orchestrator协调的OS-Symphony框架，包含反思记忆代理利用里程碑驱动的长期记忆实现轨迹自我修正，以及多功能工具代理通过SeeAct范式在浏览器沙盒中检索并合成视觉一致的教程。

Result: OS-Symphony在不同规模模型上均表现出显著性能提升，在三个在线基准测试中取得新的最先进成绩，其中OSWorld达65.84%的表现。

Conclusion: OS-Symphony有效解决了现有视觉语言模型在长流程任务中的视觉上下文丢失和未见场景教学准确性问题，显著增强了模型的鲁棒性和泛化能力。

Abstract: While Vision-Language Models (VLMs) have significantly advanced Computer-Using Agents (CUAs), current frameworks struggle with robustness in long-horizon workflows and generalization in novel domains. These limitations stem from a lack of granular control over historical visual context curation and the absence of visual-aware tutorial retrieval. To bridge these gaps, we introduce OS-Symphony, a holistic framework that comprises an Orchestrator coordinating two key innovations for robust automation: (1) a Reflection-Memory Agent that utilizes milestone-driven long-term memory to enable trajectory-level self-correction, effectively mitigating visual context loss in long-horizon tasks; (2) Versatile Tool Agents featuring a Multimodal Searcher that adopts a SeeAct paradigm to navigate a browser-based sandbox to synthesize live, visually aligned tutorials, thereby resolving fidelity issues in unseen scenarios. Experimental results demonstrate that OS-Symphony delivers substantial performance gains across varying model scales, establishing new state-of-the-art results on three online benchmarks, notably achieving 65.84% on OSWorld.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [156] [Autonomous QA Agent: A Retrieval-Augmented Framework for Reliable Selenium Script Generation](https://arxiv.org/abs/2601.06034)
*Dudekula Kasim Vali*

Main category: cs.SE

TL;DR: 本文提出了一个基于RAG的自动化质量保证代理系统，该系统利用项目文档和HTML结构生成更准确的Selenium测试脚本，有效减少了传统大语言模型的错误生成问题。


<details>
  <summary>Details</summary>
Motivation: 软件测试过程中，将需求转化为可执行测试脚本通常依赖人工，容易出错，而现有大语言模型在自动生成代码时常常幻想不存在的UI元素，导致脚本不准确。

Method: 设计了一个基于检索增强生成(RAG)的系统，通过将Markdown、PDF、HTML等多种格式的项目文档存入向量数据库，系统在生成Selenium脚本前先检索相关上下文，以此为基础生成代码。

Result: 在20个电商测试场景中的评估结果显示，RAG方法生成的代码语法有效率达100%，执行成功率达90%，显著优于传统LLM的30%。

Conclusion: 基于RAG的方法通过结合实际的DOM结构和项目文档，有效减少了代码生成中的幻想现象，展示了在自动化UI测试领域的应用潜力。

Abstract: Software testing is critical in the software development lifecycle, yet translating requirements into executable test scripts remains manual and error-prone. While Large Language Models (LLMs) can generate code, they often hallucinate non-existent UI elements. We present the Autonomous QA Agent, a Retrieval-Augmented Generation (RAG) system that grounds Selenium script generation in project-specific documentation and HTML structure. By ingesting diverse formats (Markdown, PDF, HTML) into a vector database, our system retrieves relevant context before generation. Evaluation on 20 e-commerce test scenarios shows our RAG approach achieves 100% (20/20) syntax validity and 90% (18/20, 95% CI: [85%, 95%], p < 0.001) execution success, compared to 30% for standard LLM generation. While our evaluation is limited to a single domain, our method significantly reduces hallucinations by grounding generation in actual DOM structure, demonstrating RAG's potential for automated UI testing.

</details>


### [157] [Contract2Plan: Verified Contract-Grounded Retrieval-Augmented Optimization for BOM-Aware Procurement and Multi-Echelon Inventory Planning](https://arxiv.org/abs/2601.06164)
*Sahil Agarwal*

Main category: cs.SE

TL;DR: 本文提出了Contract2Plan系统，结合大语言模型(LLM)和数学规划，确保采购和库存计划符合合同条款。


<details>
  <summary>Details</summary>
Motivation: 传统基于LLM的合同条款提取存在漏条款、单位错误和冲突未解，导致计划不可行或违反合同，且BOM耦合加剧问题。

Method: 提出Contract2Plan管道，在生成计划前插入基于求解器的合规性检查门，检索条款证据、抽取约束、编译成考虑BOM的MILP模型，并用求解器诊断验证可行性，不安全时启动修复或放弃自动化。

Result: 基于500个合成微基准的实验展示了仅提取式方法产生显著遗憾及MOQ违规，验证机制显著提升计划的可靠性和合同符合性。

Conclusion: 合同条款驱动的计划系统必须将验证作为核心组件，且针对不同条款类型设计保守修复机制与人工确认流程，保障计划的合同安全可行性。

Abstract: Procurement and inventory planning is governed not only by demand forecasts and bills of materials (BOMs), but also by operational terms in contracts and supplier documents (e.g., MOQs, lead times, price tiers, allocation caps, substitution approvals). LLM-based extraction can speed up structuring these terms, but extraction-only or LLM-only decision pipelines are brittle: missed clauses, unit errors, and unresolved conflicts can yield infeasible plans or silent contract violations, amplified by BOM coupling. We introduce Contract2Plan, a verified GenAI-to-optimizer pipeline that inserts a solver-based compliance gate before plans are emitted. The system retrieves clause evidence with provenance, extracts a typed constraint schema with evidence spans, compiles constraints into a BOM-aware MILP, and verifies grounding, eligibility, consistency, and feasibility using solver diagnostics, triggering targeted repair or abstention when automation is unsafe. We formalize which clause classes admit conservative repair with contract-safe feasibility guarantees and which require human confirmation. A self-contained synthetic micro-benchmark (500 instances; T=5) computed by exact enumeration under an execution model with MOQ uplift and emergency purchases shows heavy-tailed regret and nontrivial MOQ-violation incidence for extraction-only planning, motivating verification as a first-class component of contract-grounded planning systems.

</details>


### [158] [Attention Mechanism and Heuristic Approach: Context-Aware File Ranking Using Multi-Head Self-Attention](https://arxiv.org/abs/2601.06185)
*Pradeep Kumar Sharma,Shantanu Godbole,Sarada Prasad Jena,Hritvik Shrivastava*

Main category: cs.SE

TL;DR: 本文提出在软件变更影响分析中，利用多头自注意力机制优化确定性评分方法，提高了影响文件识别的召回率，从62-65%提升至78-82%。


<details>
  <summary>Details</summary>
Motivation: 现有确定性方法忽视特征间的上下文依赖，导致召回率达到瓶颈，难以准确反映专家推理模式。

Method: 引入多头自注意力机制作为评分的后期精炼，动态调整特征权重，结合确定性评分实现上下文感知的加权，从而提升文件影响识别的准确性和召回率。

Result: 在200个测试用例上，Top-50文件的召回率提升至80%，专家主观准确度评价从6.5提升至8.6分，显著优于传统方法。

Conclusion: 本文结合注意力机制和确定性评分桥接了自动化方法与专家判断间的差距，有效提升了仓库变更影响分析中的召回率和准确性。

Abstract: The identification and ranking of impacted files within software reposi-tories is a key challenge in change impact analysis. Existing deterministic approaches that combine heuristic signals, semantic similarity measures, and graph-based centrality metrics have demonstrated effectiveness in nar-rowing candidate search spaces, yet their recall plateaus. This limitation stems from the treatment of features as linearly independent contributors, ignoring contextual dependencies and relationships between metrics that characterize expert reasoning patterns. To address this limitation, we propose the application of Multi-Head Self-Attention as a post-deterministic scoring refinement mechanism. Our approach learns contextual weighting between features, dynamically adjust-ing importance levels per file based on relational behavior exhibited across candidate file sets. The attention mechanism produces context-aware adjustments that are additively combined with deterministic scores, pre-serving interpretability while enabling reasoning similar to that performed by experts when reviewing change surfaces. We focus on recall rather than precision, as false negatives (missing impacted files) are far more costly than false positives (irrelevant files that can be quickly dismissed during review). Empirical evaluation on 200 test cases demonstrates that the introduc-tion of self-attention improves Top-50 recall from approximately 62-65% to between 78-82% depending on repository complexity and structure, achiev-ing 80% recall at Top-50 files. Expert validation yields improvement from 6.5/10 to 8.6/10 in subjective accuracy alignment. This transformation bridges the reasoning capability gap between deterministic automation and expert judgment, improving recall in repository-aware effort estimation.

</details>


### [159] [RiskBridge: Turning CVEs into Business-Aligned Patch Priorities](https://arxiv.org/abs/2601.06201)
*Yelena Mujibur Sheikh,Awez Akhtar Khatik,Luoxi Tang,Yuqiao Meng,Zhaohan Xi*

Main category: cs.SE

TL;DR: 提出了RiskBridge框架，结合多源情报和动态建模，有效提升漏洞管理效率和合规性。


<details>
  <summary>Details</summary>
Motivation: 现有漏洞优先级框架（如CVSS）静态且忽视攻击概率、合规紧迫性和运营影响，导致补丁治理低效延迟。

Method: RiskBridge集成CVSS v4、EPSS和CISA KEV数据，采用零日暴露概率模拟(ZDES)、政策即代码引擎和ROI优化器，实现动态、合规且业务驱动的补丁优先级制定。

Result: 实验表明，RiskBridge相较先进商业方法，残余风险降低88%，SLA合规提前18天，补丁效率提升35%。

Conclusion: RiskBridge验证了融合概率建模、合规推理及优化分析的自动化、可解释和业务导向漏洞管理方案，适用于现代企业环境。

Abstract: Enterprises are confronted with an unprece- dented escalation in cybersecurity vulnerabil- ities, with thousands of new CVEs disclosed each month. Conventional prioritization frame- works such as CVSS offer static severity met- rics that fail to account for exploit probabil- ity, compliance urgency, and operational im- pact, resulting in inefficient and delayed re- mediation. This paper introduces RiskBridge, an explainable and compliance-aware vulner- ability management framework that integrates multi-source intelligence from CVSS v4, EPSS, and CISA KEV to produce dynamic, business- aligned patch priorities. RiskBridge employs a probabilistic Zero-Day Exposure Simulation (ZDES) model to fore- cast near-term exploit likelihood, a Policy-as- Code Engine to translate regulatory mandates (e.g., PCI DSS, NIST SP 800-53) into auto- mated SLA logic, and an ROI-driven Opti- mizer to maximize cumulative risk reduction per remediation effort. Experimental evalua- tions using live CVE datasets demonstrate an 88% reduction in residual risk, an 18-day improvement in SLA compliance, and a 35% increase in remediation efficiency compared to state-of-the-art commercial baselines. These findings validate RiskBridge as a prac- tical and auditable decision-intelligence sys- tem that unifies probabilistic modeling, com- pliance reasoning, and optimization analytics. The framework represents a step toward auto- mated, explainable, and business-centric vul- nerability management in modern enterprise environments

</details>


### [160] [Self-Admitted Technical Debt in LLM Software: An Empirical Comparison with ML and Non-ML Software](https://arxiv.org/abs/2601.06266)
*Niruthiha Selvanayagam,Manel Abdellatif,Taher A. Ghaleb*

Main category: cs.SE

TL;DR: 本文首次对LLM时代的自承技术债务（SATD）进行了实证研究，发现LLM仓库的SATD积累率与ML系统相似，但SATD存在时间较长，且出现3种新的LLM特有技术债务。


<details>
  <summary>Details</summary>
Motivation: 现有研究关注传统ML和非ML系统中的SATD，但对LLM系统中的SATD表现和演变知之甚少，尤其考虑到LLM系统的架构和依赖与传统系统不同。

Method: 基于477个代码仓库（分别为LLM、ML和非ML各159个），比较三者SATD的普遍性；采用存活分析法研究SATD的引入和移除动态；通过质性分析识别LLM特有的技术债务形式。

Result: LLM仓库积累SATD的速度与ML系统相仿（3.95% vs. 4.10%），但SATD存续时间长2.4倍（492天 vs. 204天），之后迅速增加；发现三种新型LLM技术债务：模型堆栈临时解决债务、模型依赖债务和性能优化债务。

Conclusion: LLM系统的独特架构影响了技术债务的形成和演变，针对其特有债务类型需制定相应管理策略，以提升开发质量和维护效率。

Abstract: Self-admitted technical debt (SATD), referring to comments flagged by developers that explicitly acknowledge suboptimal code or incomplete functionality, has received extensive attention in machine learning (ML) and traditional (Non-ML) software. However, little is known about how SATD manifests and evolves in contemporary Large Language Model (LLM)-based systems, whose architectures, workflows, and dependencies differ fundamentally from both traditional and pre-LLM ML software. In this paper, we conduct the first empirical study of SATD in the LLM era, replicating and extending prior work on ML technical debt to modern LLM-based systems. We compare SATD prevalence across LLM, ML, and non-ML repositories across a total of 477 repositories (159 per category). We perform survival analysis of SATD introduction and removal to understand the dynamics of technical debt across different development paradigms. Surprisingly, despite their architectural complexity, our results reveal that LLM repositories accumulate SATD at similar rates to ML systems (3.95% vs. 4.10%). However, we observe that LLM repositories remain debt-free 2.4x longer than ML repositories (a median of 492 days vs. 204 days), and then start to accumulate technical debt rapidly. Moreover, our qualitative analysis of 377 SATD instances reveals three new forms of technical debt unique to LLM-based development that have not been reported in prior research: Model-Stack Workaround Debt, Model Dependency Debt, and Performance Optimization Debt. Finally, by mapping SATD to stages of the LLM development pipeline, we observe that debt concentrates

</details>


### [161] [Automated QoR improvement in OpenROAD with coding agents](https://arxiv.org/abs/2601.06268)
*Amur Ghose,Junyeong Jang,Andrew B. Kahng,Jakang Lee*

Main category: cs.SE

TL;DR: 本文提出了AuDoPEDA，一种基于大型语言模型的自动化EDA代码改进系统，在OpenROAD框架下实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: EDA领域的创新受限于专业工程资源稀缺，而大型语言模型在推动EDA技术进步方面尚未得到充分验证。

Method: 构建了一个闭环的LLM框架，利用OpenAI模型和Codex级别代理，自动阅读OpenROAD代码，提出研究方向，细化实施步骤，并提交可执行代码变更。

Result: 在OpenROAD上实验显示，AuDoPEDA实现了最多5.9%的布线长度减少和最多10.0%的时钟周期缩短。

Conclusion: AuDoPEDA展示了在无人值守情况下，利用大型语言模型自主改进EDA代码的可行性和显著效果，拓展了EDA自动化发展的新路径。

Abstract: EDA development and innovation has been constrained by scarcity of expert engineering resources. While leading LLMs have demonstrated excellent performance in coding and scientific reasoning tasks, their capacity to advance EDA technology itself has been largely untested. We present AuDoPEDA, an autonomous, repository-grounded coding system built atop OpenAI models and a Codex-class agent that reads OpenROAD, proposes research directions, expands them into implementation steps, and submits executable diffs. Our contributions include (i) a closed-loop LLM framework for EDA code changes; (ii) a task suite and evaluation protocol on OpenROAD for PPA-oriented improvements; and (iii) end-to-end demonstrations with minimal human oversight. Experiments in OpenROAD achieve routed wirelength reductions of up to 5.9%, and effective clock period reductions of up to 10.0%.

</details>


### [162] [Mining Quantum Software Patterns in Open-Source Projects](https://arxiv.org/abs/2601.06281)
*Neilson Carlos Leite Ramalho,Erico A. da Silva,Higor Amario de Souza,Marcos Lordello Chaim*

Main category: cs.SE

TL;DR: 本文通过分析985个开源量子计算Jupyter笔记本，探讨了量子计算中的模式应用，识别并扩展了9个新模式，展示了量子软件工程的发展趋势。


<details>
  <summary>Details</summary>
Motivation: 量子计算软件工程的发展需要高层次抽象和模式支持，以帮助开发者更有效地构建量子程序。

Method: 基于三大量子计算框架构建知识库，识别新的设计模式，利用语义搜索工具在大规模数据集中自动检测模式应用。

Result: 识别并验证了9个新的量子计算模式，发现开发者在三个层次上应用模式：基础电路工具、算法原语及领域特定应用。

Conclusion: 量子软件工程正趋于成熟，开发者越来越多地利用面向高层次的构建模块来解决实际问题。

Abstract: Quantum computing has become an active research field in recent years, as its applications in fields such as cryptography, optimization, and materials science are promising. Along with these developments, challenges and opportunities exist in the field of Quantum Software Engineering, as the development of frameworks and higher-level abstractions has attracted practitioners from diverse backgrounds. Unlike initial quantum frameworks based on the circuit model, recent frameworks and libraries leverage higher-level abstractions for creating quantum programs. This paper presents an empirical study of 985 Jupyter Notebooks from 80 open-source projects to investigate how quantum patterns are applied in practice. Our work involved two main stages. First, we built a knowledge base from three quantum computing frameworks (Qiskit, PennyLane, and Classiq). This process led us to identify and document 9 new patterns that refine and extend the existing quantum computing pattern catalog. Second, we developed a reusable semantic search tool to automatically detect these patterns across our large-scale dataset, providing a practitioner-focused analysis. Our results show that developers use patterns in three levels: from foundational circuit utilities, to common algorithmic primitives (e.g., Amplitude Amplification), up to domain-specific applications for finance and optimization. This indicates a maturing field where developers are increasingly using high-level building blocks to solve real-world problems.

</details>


### [163] [Foundational Analysis of Safety Engineering Requirements (SAFER)](https://arxiv.org/abs/2601.06335)
*Noga Chemo,Yaniv Mordecai,Yoram Reich*

Main category: cs.SE

TL;DR: 提出了SAFER框架，一种基于模型驱动且支持生成式AI的方法，用于改进复杂安全关键系统安全需求的生成与分析。


<details>
  <summary>Details</summary>
Motivation: 多利益相关者对安全需求的描述通常缺乏协调，导致需求之间存在缺失、重复和矛盾，威胁系统安全与合规。现有方法多为非正式，难以有效解决这些问题。

Method: SAFER基于模型驱动系统工程(MBSE)，利用生成式AI处理需求规格模型，实现需求与系统功能的映射，识别需求不足的功能，检测重复需求和需求矛盾，提供结构化分析和决策支持。

Result: 在无人机自主系统上的应用表明，SAFER显著提升了需求不一致性的检测效率和可靠性，优化了安全工程流程。

Conclusion: 生成式AI结合形式化模型和系统化查询，能够为早期安全需求规格和稳健安全架构提供有意义支持。

Abstract: We introduce a framework for Foundational Analysis of Safety Engineering Requirements (SAFER), a model-driven methodology supported by Generative AI to improve the generation and analysis of safety requirements for complex safety-critical systems. Safety requirements are often specified by multiple stakeholders with uncoordinated objectives, leading to gaps, duplications, and contradictions that jeopardize system safety and compliance. Existing approaches are largely informal and insufficient for addressing these challenges. SAFER enhances Model-Based Systems Engineering (MBSE) by consuming requirement specification models and generating the following results: (1) mapping requirements to system functions, (2) identifying functions with insufficient requirement specifications, (3) detecting duplicate requirements, and (4) identifying contradictions within requirement sets. SAFER provides structured analysis, reporting, and decision support for safety engineers. We demonstrate SAFER on an autonomous drone system, significantly improving the detection of requirement inconsistencies, enhancing both efficiency and reliability of the safety engineering process. We show that Generative AI must be augmented by formal models and queried systematically, to provide meaningful early-stage safety requirement specifications and robust safety architectures.

</details>


### [164] [Architecting AgentOps Needs CHANGE](https://arxiv.org/abs/2601.06456)
*Shaunak Biswas,Hiya Bhatt,Karthik Vaidhyanathan*

Main category: cs.SE

TL;DR: 本文提出了针对代理式人工智能系统的CHANGE框架，解决了传统DevOps和MLOps架构无法适应代理式AI非决定性行为的问题。


<details>
  <summary>Details</summary>
Motivation: 代理式AI系统行为会随经验和反馈不断演化，传统运维方法假设行为可通过版本管理和监控控制，但此假设在代理式AI中失效，运行时可靠性难保障。

Method: 提出CHANGE框架，包含六大能力：Contextualize，Harmonize，Anticipate，Negotiate，Generate，Evolve，支持代理、基础设施和人工监督的动态共演化，指导架构设计和生命周期管理。

Result: 通过客户支持系统场景展示CHANGE框架在代理式AI系统生命周期管理中的应用，验证其作为AgentOps平台架构基础的有效性。

Conclusion: CHANGE框架重新定义了软件架构，强调适应不确定性和持续演化为系统固有特性，推动代理式AI系统的架构思维转变。

Abstract: The emergence of Agentic AI systems has outpaced the architectural thinking required to operate them effectively. These agents differ fundamentally from traditional software: their behavior is not fixed at deployment but continuously shaped by experience, feedback, and context. Applying operational principles inherited from DevOps or MLOps, built for deterministic software and traditional ML systems, assumes that system behavior can be managed through versioning, monitoring, and rollback. This assumption breaks down for Agentic AI systems whose learning trajectories diverge over time. This introduces non-determinism making system reliability a challenge at runtime. We argue that architecting such systems requires a shift from managing control loops to enabling dynamic co-evolution among agents, infrastructure, and human oversight. To guide this shift, we introduce CHANGE, a conceptual framework comprising six capabilities for operationalizing Agentic AI systems: Contextualize, Harmonize, Anticipate, Negotiate, Generate, and Evolve. CHANGE provides a foundation for architecting an AgentOps platform to manage the lifecycle of evolving Agentic AI systems, illustrated through a customer-support system scenario. In doing so, CHANGE redefines software architecture for an era where adaptation to uncertainty and continuous evolution are inherent properties of the system.

</details>


### [165] [Coding in a Bubble? Evaluating LLMs in Resolving Context Adaptation Bugs During Code Adaptation](https://arxiv.org/abs/2601.06497)
*Tanghaoran Zhang,Xinjun Mao,Shangwen Wang,Yuxin Zhao,Yao Lu,Zezhou Tang,Wenyu Xu,Longfei Sun,Changrong Xie,Kang Yang,Yue Yu*

Main category: cs.SE

TL;DR: 本文提出了一个评估大型语言模型(LLMs)解决上下文适应性缺陷(CtxBugs)能力的新框架CtxBugGen。实验证明当前主流LLMs在解决CtxBugs方面表现不佳，暴露了其跨上下文推理能力的不足。


<details>
  <summary>Details</summary>
Motivation: 代码适应性任务中，重要且难点在于解决上下文适应性缺陷(CtxBugs)，该缺陷需跨上下文语义推理才能解决，目前LLMs虽有潜力但在此领域能力未知且不足。

Method: 提出CtxBugGen框架，通过四步法：选择适应任务、任务特定扰动、基于LLM生成变体、CtxBugs识别，自动生成针对性CtxBugs基准并用以评测LLMs。

Result: 在由CtxBugGen构建的基准上评测四个先进LLMs，表现不理想，顶尖模型Kimi-K2仅能修复52.47%的CtxBugs，CtxBugs导致性能下降最高达30%。

Conclusion: 当前LLMs在跨上下文推理以解决CtxBugs方面存在显著缺陷，强调需开发新方法增强其上下文感知能力，提升代码适应任务中可靠性。

Abstract: Code adaptation is a fundamental but challenging task in software development, requiring developers to modify existing code for new contexts. A key challenge is to resolve Context Adaptation Bugs (CtxBugs), which occurs when code correct in its original context violates constraints in the target environment. Unlike isolated bugs, CtxBugs cannot be resolved through local fixes and require cross-context reasoning to identify semantic mismatches. Overlooking them may lead to critical failures in adaptation. Although Large Language Models (LLMs) show great potential in automating code-related tasks, their ability to resolve CtxBugs remains a significant and unexplored obstacle to their practical use in code adaptation. To bridge this gap, we propose CtxBugGen, a novel framework for generating CtxBugs to evaluate LLMs. Its core idea is to leverage LLMs' tendency to generate plausible but context-free code when contextual constraints are absent. The framework generates CtxBugs through a four-step process to ensure their relevance and validity: (1) Adaptation Task Selection, (2) Task-specific Perturbation,(3) LLM-based Variant Generation and (4) CtxBugs Identification. Based on the benchmark constructed by CtxBugGen, we conduct an empirical study with four state-of-the-art LLMs. Our results reveal their unsatisfactory performance in CtxBug resolution. The best performing LLM, Kimi-K2, achieves 55.93% on Pass@1 and resolves just 52.47% of CtxBugs. The presence of CtxBugs degrades LLMs' adaptation performance by up to 30%. Failure analysis indicates that LLMs often overlook CtxBugs and replicate them in their outputs. Our study highlights a critical weakness in LLMs' cross-context reasoning and emphasize the need for new methods to enhance their context awareness for reliable code adaptation.

</details>


### [166] [Fixturize: Bridging the Fixture Gap in Test Generation](https://arxiv.org/abs/2601.06615)
*Pengyu Xue,Chengyi Wang,Zhen Yang,Xiapu Luo,Yuxuan Zhang,Xiran Lyu,Yifei Pei,Zonghan Jia,Yichen Sun,Linhao Wu,Kunwu Zheng*

Main category: cs.SE

TL;DR: 本文提出Fixturize框架，解决大语言模型生成自动测试时忽视测试夹具的问题，通过识别依赖夹具的函数并迭代合成夹具，大幅提升自动测试套件质量。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在自动单元测试生成中往往忽略测试夹具的构建，导致测试质量不足。

Method: 设计Fixturize诊断框架，通过识别夹具依赖函数并反馈迭代合成测试夹具，提升自动测试效果，并采用FixtureEval基准数据集进行评估。

Result: Fixturize在夹具依赖识别准确率达88.38%-97.00%，自动生成夹具使测试通过率提升18.03%-42.86%，并显著增加代码覆盖率。

Conclusion: 测试夹具意识是现代自动测试流程中不可或缺的元素，Fixturize有效弥补了现有方法的不足，显著改善自动测试性能。

Abstract: Current Large Language Models (LLMs) have advanced automated unit test generation but face a critical limitation: they often neglect to construct the necessary test fixtures, which are the environmental setups required for a test to run. To bridge this gap, this paper proposes Fixturize, a diagnostic framework that proactively identifies fixture-dependent functions and synthesizes test fixtures accordingly through an iterative, feedback-driven process, thereby improving the quality of auto-generated test suites of existing approaches. For rigorous evaluation, the authors introduce FixtureEval, a dedicated benchmark comprising 600 curated functions across two Programming Languages (PLs), i.e., Python and Java, with explicit fixture dependency labels, enabling both the corresponding classification and generation tasks. Empirical results demonstrate that Fixturize is highly effective, achieving 88.38%-97.00% accuracy across benchmarks in identifying the dependence of test fixtures and significantly enhancing the Suite Pass rate (SuitePS) by 18.03%-42.86% on average across both PLs with the auto-generated fixtures. Owing to the maintenance of test fixtures, Fixturize further improves line/branch coverage when integrated with existing testing tools of both LLM-based and Search-based by 16.85%/24.08% and 31.54%/119.66% on average, respectively. The findings establish fixture awareness as an essential, missing component in modern auto-testing pipelines.

</details>


### [167] [An Exploratory Pilot Survey on Technical Quality Control Practices in Agile R&D Projects](https://arxiv.org/abs/2601.06689)
*Mateus Costa Lucena*

Main category: cs.SE

TL;DR: 该研究通过对巴西Manaus地区科学技术机构的敏捷研发软件团队进行调查，探讨了在Scrum环境中技术质量控制的实践与挑战，发现自动化测试、代码审查等虽被认可但应用不一致，技术质量指标监控及技术债务评估存在缺口。


<details>
  <summary>Details</summary>
Motivation: 敏捷研发项目中技术质量管理困难，尤其在高度技术不确定和实验性强的环境下，需要了解实际的技术质量控制实践与挑战。

Method: 通过结构化问卷调查巴西Manaus地区科学技术机构的敏捷研发软件团队，结合定量数据与定性回答进行分析。

Result: 自动化测试、代码审查和持续集成等实践被广泛认可但应用不稳定，技术质量指标监控不足，技术债务从业务角度评估机制缺失。

Conclusion: 本研究为区域创新生态中敏捷研发项目的技术质量管理提供了初步描述和探索基线，而非普适结论。

Abstract: Managing technical quality in agile Research and Development (R&D) software projects represents a persistent challenge, particularly in contexts characterized by high technical uncertainty and experimental pressure. This exploratory pilot survey explores how agile R&D software teams report the use of practices and metrics related to technical quality control within Scrum-based environments. The study employed a structured questionnaire administered to professionals from Science and Technology Institutions (STIs) located in Manaus, Brazil, aiming to capture reported practices, perceptions of quality, and recurrent challenges. Quantitative data were complemented by qualitative responses to support contextual interpretation. The results indicate that although practices such as automated testing, code review, and continuous integration are widely acknowledged, their reported application is often inconsistent across iterations. Gaps were also observed in the monitoring of technical quality metrics and in the reporting of mechanisms for assessing technical debt from a business perspective. Rather than aiming for generalization, this study offers an exploratory baseline that describes how technical quality is managed in agile R&D projects within a regional innovation ecosystem.

</details>


### [168] [Comparative Separation: Evaluating Separation on Comparative Judgment Test Data](https://arxiv.org/abs/2601.06761)
*Xiaoyin Xi,Neeku Capak,Kate Stockwell,Zhe Yu*

Main category: cs.SE

TL;DR: 本文提出了比较分离这一新的群体公平性概念，用于在比较判断测试数据上评估机器学习软件的公平性，证明了其在二分类问题中等价于传统的分离标准。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习软件在高风险决策中的应用越来越广泛，公平性问题日益受到关注。传统分离标准的评估依赖于每个测试数据点的真实标签，数据获取成本高。

Method: 定义并提出了比较分离及其评估指标，基于比较判断数据（两数据点间的优劣关系）代替传统标签或评分，理论和实证证明在二分类问题中，比较分离等同于分离。

Result: 通过理论和实证分析，展示了比较分离评估与分离评估在统计效能上的对比，并分析了所需测试数据数量及数据对数量。

Conclusion: 首次探讨了利用比较判断测试数据进行公平性评估的方法，验证了该方法的可行性及实际优势，为机器学习模型公平性评估提供了新的思路。

Abstract: This research seeks to benefit the software engineering society by proposing comparative separation, a novel group fairness notion to evaluate the fairness of machine learning software on comparative judgment test data. Fairness issues have attracted increasing attention since machine learning software is increasingly used for high-stakes and high-risk decisions. It is the responsibility of all software developers to make their software accountable by ensuring that the machine learning software do not perform differently on different sensitive groups -- satisfying the separation criterion. However, evaluation of separation requires ground truth labels for each test data point. This motivates our work on analyzing whether separation can be evaluated on comparative judgment test data. Instead of asking humans to provide the ratings or categorical labels on each test data point, comparative judgments are made between pairs of data points such as A is better than B. According to the law of comparative judgment, providing such comparative judgments yields a lower cognitive burden for humans than providing ratings or categorical labels. This work first defines the novel fairness notion comparative separation on comparative judgment test data, and the metrics to evaluate comparative separation. Then, both theoretically and empirically, we show that in binary classification problems, comparative separation is equivalent to separation. Lastly, we analyze the number of test data points and test data pairs required to achieve the same level of statistical power in the evaluation of separation and comparative separation, respectively. This work is the first to explore fairness evaluation on comparative judgment test data. It shows the feasibility and the practical benefits of using comparative judgment test data for model evaluations.

</details>


### [169] [MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences](https://arxiv.org/abs/2601.06789)
*Qihao Wang,Ziming Cheng,Shuo Zhang,Fan Liu,Rui Xu,Heng Lian,Kunyi Wang,Xiaoming Yu,Jianghao Yin,Sen Hu,Yue Hu,Shaolei Zhang,Yanbing Liu,Ronghao Chen,Huacan Wang*

Main category: cs.SE

TL;DR: 本文提出了MemGovern框架，将GitHub的非结构化问题追踪数据转化为代理可用的经验记忆，从而提升自主软件工程代理解决问题的能力。


<details>
  <summary>Details</summary>
Motivation: 当前自主软件工程代理受限于“闭环”问题，只利用局部上下文处理bug，忽视了GitHub上丰富的历史人类经验。

Method: MemGovern通过经验治理将人类经验转变为代理友好的经验卡，并引入逻辑驱动的经验检索策略，实现高效经验查询。

Result: 构建了13.5万个经验卡，在SWE-bench Verified上解决率提升了4.65%。

Conclusion: MemGovern作为插件式框架，为自主软件工程代理提供了高效的记忆基础设施，显著增强了其问题解决能力。

Abstract: While autonomous software engineering (SWE) agents are reshaping programming paradigms, they currently suffer from a "closed-world" limitation: they attempt to fix bugs from scratch or solely using local context, ignoring the immense historical human experience available on platforms like GitHub. Accessing this open-world experience is hindered by the unstructured and fragmented nature of real-world issue-tracking data. In this paper, we introduce MemGovern, a framework designed to govern and transform raw GitHub data into actionable experiential memory for agents. MemGovern employs experience governance to convert human experience into agent-friendly experience cards and introduces an agentic experience search strategy that enables logic-driven retrieval of human expertise. By producing 135K governed experience cards, MemGovern achieves a significant performance boost, improving resolution rates on the SWE-bench Verified by 4.65%. As a plug-in approach, MemGovern provides a solution for agent-friendly memory infrastructure.

</details>


### [170] [PenForge: On-the-Fly Expert Agent Construction for Automated Penetration Testing](https://arxiv.org/abs/2601.06910)
*Huihui Huang,Jieke Shi,Junkai Chen,Ting Zhang,Yikun Li,Chengran Yang,Eng Lieh Ouh,Lwin Khin Shar,David Lo*

Main category: cs.SE

TL;DR: PenForge是一种动态构建专家代理的新框架，通过自动侦察和上下文感知利用，实现了渗透测试中显著更高的漏洞利用成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的渗透测试代理要么依赖单一通用代理，难以应对复杂场景，要么是专门代理，难以适应多样漏洞类型。

Method: PenForge通过自动侦察攻击面，并动态生成针对具体上下文的专家代理进行漏洞利用，避免预先准备固定代理。

Result: 在CVE-Bench零日漏洞测试中，PenForge实现30%的漏洞利用成功率，较现有技术提升3倍。

Conclusion: PenForge体现了按需构建代理的新范式，显著提升了自动化渗透测试的效果，并指出未来可在丰富工具知识、扩展基准测试集及引入可解释机制等方面改进。

Abstract: Penetration testing is essential for identifying vulnerabilities in web applications before real adversaries can exploit them. Recent work has explored automating this process with Large Language Model (LLM)-powered agents, but existing approaches either rely on a single generic agent that struggles in complex scenarios or narrowly specialized agents that cannot adapt to diverse vulnerability types. We therefore introduce PenForge, a framework that dynamically constructs expert agents during testing rather than relying on those prepared beforehand. By integrating automated reconnaissance of potential attack surfaces with agents instantiated on the fly for context-aware exploitation, PenForge achieves a 30.0% exploit success rate (12/40) on CVE-Bench in the particularly challenging zero-day setting, which is a 3 times improvement over the state-of-the-art. Our analysis also identifies three opportunities for future work: (1) supplying richer tool-usage knowledge to improve exploitation effectiveness; (2) extending benchmarks to include more vulnerabilities and attack types; and (3) fostering developer trust by incorporating explainable mechanisms and human review. As an emerging result with substantial potential impact, PenForge embodies the early-stage yet paradigm-shifting idea of on-the-fly agent construction, marking its promise as a step toward scalable and effective LLM-driven penetration testing.

</details>


### [171] [MicLog: Towards Accurate and Efficient LLM-based Log Parsing via Progressive Meta In-Context Learning](https://arxiv.org/abs/2601.07005)
*Jianbo Yu,Yixuan Li,Hai Xu,Kang Xu,Junjielong Xu,Zhijing Li,Pinjia He,Wanyuan Wang*

Main category: cs.SE

TL;DR: 本文提出了MicLog，一个结合元学习和增量上下文学习的日志解析框架，在提高准确率的同时显著加速了解析速度。


<details>
  <summary>Details</summary>
Motivation: 传统语法和语义解析方法难以处理日志的语义变化及领域数据稀缺问题，且大型语言模型（LLM）解析器存在上下文学习能力未充分利用和查询成本高的问题。

Method: MicLog采用进阶式元增量上下文学习（ProgMeta-ICL）策略，结合加权DBSCAN采样和改进的BM25示例选择提升LLM的上下文学习能力，同时利用多层预查询缓存机制加速解析。

Result: 在Loghub-2.0数据集上，MicLog比最先进解析器提高了10.3%的准确率，解析时间缩短了42.4%。

Conclusion: MicLog有效提升了LLM在日志解析中的性能和效率，展现了元学习与增量上下文学习结合的潜力。

Abstract: Log parsing converts semi-structured logs into structured templates, forming a critical foundation for downstream analysis. Traditional syntax and semantic-based parsers often struggle with semantic variations in evolving logs and data scarcity stemming from their limited domain coverage. Recent large language model (LLM)-based parsers leverage in-context learning (ICL) to extract semantics from examples, demonstrating superior accuracy. However, LLM-based parsers face two main challenges: 1) underutilization of ICL capabilities, particularly in dynamic example selection and cross-domain generalization, leading to inconsistent performance; 2) time-consuming and costly LLM querying. To address these challenges, we present MicLog, the first progressive meta in-context learning (ProgMeta-ICL) log parsing framework that combines meta-learning with ICL on small open-source LLMs (i.e., Qwen-2.5-3B). Specifically, MicLog: i) enhances LLMs' ICL capability through a zero-shot to k-shot ProgMeta-ICL paradigm, employing weighted DBSCAN candidate sampling and enhanced BM25 demonstration selection; ii) accelerates parsing via a multi-level pre-query cache that dynamically matches and refines recently parsed templates. Evaluated on Loghub-2.0, MicLog achieves 10.3% higher parsing accuracy than the state-of-the-art parser while reducing parsing time by 42.4%.

</details>


### [172] [Between Policy and Practice: GenAI Adoption in Agile Software Development Teams](https://arxiv.org/abs/2601.07051)
*Michael Neumann,Lasse Bischof,Nic Elias Hinz,Luca Stockmann,Dennis Schrader,Ana Carolina Ahaus,Erim Can Demirci,Benjamin Gabel,Maria Rauschenberger,Philipp Diebold,Henning Fritzemeier,Adam Przybylek*

Main category: cs.SE

TL;DR: 本研究探讨了敏捷环境中生成式AI工具的采用情况，发现其主要用于创意任务、文档和代码辅助，带来效率和创造力提升，但面临数据隐私和治理等障碍。


<details>
  <summary>Details</summary>
Motivation: 生成式AI工具快速发展，但在敏捷软件工程中的实际应用和挑战尚不清楚，需研究其在真实组织环境中的采用情况。

Method: 通过对三家德国组织的17次半结构化访谈和文档分析，采用跨案例主题分析方法，基于TOE框架识别生成式AI的采用模式。

Result: 生成式AI主要用于创意、文档和代码辅助，带来效率和创造力提升。障碍包括数据隐私、验证负担和缺乏治理，政策与实际技术使用和组织约束之间存在不匹配。

Conclusion: 生成式AI有助于增强敏捷角色，但需在技术、组织和环境层面实现政策明确、数据保护和用户培训，确保其负责任且有效的整合。

Abstract: Context: The rapid emergence of generative AI (GenAI) tools has begun to reshape various software engineering activities. Yet, their adoption within agile environments remains underexplored. Objective: This study investigates how agile practitioners adopt GenAI tools in real-world organizational contexts, focusing on regulatory conditions, use cases, benefits, and barriers. Method: An exploratory multiple case study was conducted in three German organizations, involving 17 semi-structured interviews and document analysis. A cross-case thematic analysis was applied to identify GenAI adoption patterns. Results: Findings reveal that GenAI is primarily used for creative tasks, documentation, and code assistance. Benefits include efficiency gains and enhanced creativity, while barriers relate to data privacy, validation effort, and lack of governance. Using the Technology-Organization-Environment (TOE) framework, we find that these barriers stem from misalignments across the three dimensions. Regulatory pressures are often translated into policies without accounting for actual technological usage patterns or organizational constraints. This leads to systematic gaps between policy and practice. Conclusion: GenAI offers significant potential to augment agile roles but requires alignment across TOE dimensions, including clear policies, data protection measures, and user training to ensure responsible and effective integration.

</details>


### [173] [A Large-Scale Study on the Development and Issues of Multi-Agent AI Systems](https://arxiv.org/abs/2601.07136)
*Daniel Liu,Krishna Upadhyay,Vinaik Chhetri,A. B. Siddique,Umar Farooq*

Main category: cs.SE

TL;DR: 本文首次大规模实证研究分析了开源多智能体系统（MAS）的开发与维护，揭示了开发模式多样性和维护重点差异，并指出了生态系统的脆弱性及改进需求。


<details>
  <summary>Details</summary>
Motivation: 多智能体AI系统迅速发展，但对其实际演化和维护过程了解不足，亟需系统性的实证研究。

Method: 分析了八个主流开源MAS的4.2万余次代码提交和4700多个问题单，归纳开发类型并统计问题类别及处理时效。

Result: 发现三种开发模式（持续、稳定、突发），大部分改动侧重完成功能增强，主要问题集中于错误、基础设施和智能体协调，且2023年问题报告显著增加。

Conclusion: 当前MAS生态既具备发展动力也存在脆弱性，需加强测试架构、文档和维护实践，以保障系统的长期可靠性与可持续性。

Abstract: The rapid emergence of multi-agent AI systems (MAS), including LangChain, CrewAI, and AutoGen, has shaped how large language model (LLM) applications are developed and orchestrated. However, little is known about how these systems evolve and are maintained in practice. This paper presents the first large-scale empirical study of open-source MAS, analyzing over 42K unique commits and over 4.7K resolved issues across eight leading systems. Our analysis identifies three distinct development profiles: sustained, steady, and burst-driven. These profiles reflect substantial variation in ecosystem maturity. Perfective commits constitute 40.8% of all changes, suggesting that feature enhancement is prioritized over corrective maintenance (27.4%) and adaptive updates (24.3%). Data about issues shows that the most frequent concerns involve bugs (22%), infrastructure (14%), and agent coordination challenges (10%). Issue reporting also increased sharply across all frameworks starting in 2023. Median resolution times range from under one day to about two weeks, with distributions skewed toward fast responses but a minority of issues requiring extended attention. These results highlight both the momentum and the fragility of the current ecosystem, emphasizing the need for improved testing infrastructure, documentation quality, and maintenance practices to ensure long-term reliability and sustainability.

</details>


### [174] [Engineering Decisions in MBSE: Insights for a Decision Capture Framework Development](https://arxiv.org/abs/2601.07301)
*Nidhal Selmi,Jean-michel Bruel,Sébastien Mosser,Matthieu Crespo,Alain Kerbrat*

Main category: cs.SE

TL;DR: 本文提出了一种将决策捕获集成到基于模型的系统工程（MBSE）工作流程中的轻量级框架，通过将决策备选方案表示为系统模型切片，以减少捕获工作量并保持与需求及架构的关联。


<details>
  <summary>Details</summary>
Motivation: 工程设计中的决策是工程师知识的体现，捕获决策知识有助于团队提高开发效率。然而传统方法难以高效捕获必要的上下文信息，亟需一种简便且高效的解决方案。

Method: 利用MBSE，将决策直接嵌入系统模型，通过系统模型切片的方式表示决策备选方案，建立决策与需求、行为及架构元素之间的明确链接。

Result: 通过飞机架构的简化行业示例，展示了决策捕获的挑战及轻量级框架的可行性和有效性。

Conclusion: 将决策捕获集成到MBSE能够降低捕获工作负担，并实现决策知识的有效复用，为工程设计提供支持。

Abstract: Decision-making is a core engineering design activity that conveys the engineer's knowledge and translates it into courses of action. Capturing this form of knowledge can reap potential benefits for the engineering teams and enhance development efficiency. Despite its clear value, traditional decision capture often requires a significant amount of effort and still falls short of capturing the necessary context for reuse. Model-based systems engineering (MBSE) can be a promising solution to address these challenges by embedding decisions directly within system models, which can reduce the capture workload while maintaining explicit links to requirements, behaviors, and architectural elements. This article discusses a lightweight framework for integrating decision capture into MBSE workflows by representing decision alternatives as system model slices. Using a simplified industry example from aircraft architecture, we discuss the main challenges associated with decision capture and propose preliminary solutions to address these challenges.

</details>


### [175] [FairRF: Multi-Objective Search for Single and Intersectional Software Fairness](https://arxiv.org/abs/2601.07537)
*Giordano d'Alosio,Max Hort,Rebecca Moussa,Federica Sarro*

Main category: cs.SE

TL;DR: 本文提出FairRF方法，通过多目标进化搜索优化分类任务中的公平性和效果，基于随机森林模型，提供帕累托最优解，满足不同需求。


<details>
  <summary>Details</summary>
Motivation: 针对现有公平性增强方法黑盒性质，无法让利益相关者根据需求平衡公平性与预测效果的问题，提出了一种可调节的公平性优化方法。

Method: 基于随机森林，利用多目标进化算法搜索最佳超参数和数据变异方式，最大化公平性和预测效果，输出多种帕累托最优解供选择。

Result: 在11种场景、26个基线方法和多种公平及效果指标下，FairRF显著提升了分类器公平性，保持预测效果稳定，且在交叉偏见缓解方面优于现有方法。

Conclusion: FairRF有效提升了偏见缓解性能，且支持利益相关者根据自身需求定制公平性优化策略，适用于开发公平的软件系统。

Abstract: Background: The wide adoption of AI- and ML-based systems in sensitive domains raises severe concerns about their fairness. Many methods have been proposed in the literature to enhance software fairness. However, the majority behave as a black-box, not allowing stakeholders to prioritise fairness or effectiveness (i.e., prediction correctness) based on their needs. Aims: In this paper, we introduce FairRF, a novel approach based on multi-objective evolutionary search to optimise fairness and effectiveness in classification tasks. FairRF uses a Random Forest (RF) model as a base classifier and searches for the best hyperparameter configurations and data mutation to maximise fairness and effectiveness. Eventually, it returns a set of Pareto optimal solutions, allowing the final stakeholders to choose the best one based on their needs. Method: We conduct an extensive empirical evaluation of FairRF against 26 different baselines in 11 different scenarios using five effectiveness and three fairness metrics. Additionally, we also include two variations of the fairness metrics for intersectional bias for a total of six definitions analysed. Result: Our results show that FairRF can significantly improve the fairness of base classifiers, while maintaining consistent prediction effectiveness. Additionally, FairRF provides a more consistent optimisation under all fairness definitions compared to state-of-the-art bias mitigation methods and overcomes the existing state-of-the-art approach for intersectional bias mitigation. Conclusions: FairRF is an effective approach for bias mitigation also allowing stakeholders to adapt the development of fair software systems based on their specific needs.

</details>


### [176] [OODEval: Evaluating Large Language Models on Object-Oriented Design](https://arxiv.org/abs/2601.07602)
*Bingxu Xiao,Yunwei Dong,Yiqi Tang,Manqing Zhang,Yifan Zhou,Chunyan Ma,Yepang Liu*

Main category: cs.SE

TL;DR: 本文针对大语言模型（LLMs）在面向对象设计（OOD）任务上的表现进行了全面评估，构建了新的基准集和评价指标，发现模型在语法上表现良好，但语义细节存在较大缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注代码任务，忽视了软件设计能力的评估，缺乏统一标准的基准和评价指标。

Method: 构建了含50个OOD任务的OODEval基准集和含940份人类评分类图的OODEval-Human，提出统一评价指标CLUE，评测29个LLMs并分析性能及失败案例。

Result: 发现LLMs虽语法准确但语义效果不足，Qwen3-Coder-30B表现最佳，部分小参数模型表现优异。但整体仍不及顶尖人类设计师。参数规模、专门化及调优影响显著，设计难度及需求可读性降低表现。

Conclusion: 当前LLMs在面向对象设计任务上还有显著差距，需改进语义理解能力和设计细节表达，构建的基准和指标为后续研究提供了有效工具。

Abstract: Recent advances in large language models (LLMs) have driven extensive evaluations in software engineering. however, most prior work concentrates on code-level tasks, leaving software design capabilities underexplored. To fill this gap, we conduct a comprehensive empirical study evaluating 29 LLMs on object-oriented design (OOD) tasks. Owing to the lack of standardized benchmarks and metrics, we introduce OODEval, a manually constructed benchmark comprising 50 OOD tasks of varying difficulty, and OODEval-Human, the first human-rated OOD benchmark, which includes 940 undergraduate-submitted class diagrams evaluated by instructors. We further propose CLUE (Class Likeness Unified Evaluation), a unified metric set that assesses both global correctness and fine-grained design quality in class diagram generation. Using these benchmarks and metrics, we investigate five research questions: overall correctness, comparison with humans, model dimension analysis, task feature analysis, and bad case analysis. The results indicate that while LLMs achieve high syntactic accuracy, they exhibit substantial semantic deficiencies, particularly in method and relationship generation. Among the evaluated models, Qwen3-Coder-30B achieves the best overall performance, rivaling DeepSeek-R1 and GPT-4o, while Gemma3-4B-IT outperforms GPT-4o-Mini despite its smaller parameter scale. Although top-performing LLMs nearly match the average performance of undergraduates, they remain significantly below the level of the best human designers. Further analysis shows that parameter scale, code specialization, and instruction tuning strongly influence performance, whereas increased design complexity and lower requirement readability degrade it. Bad case analysis reveals common failure modes, including keyword misuse, missing classes or relationships, and omitted methods.

</details>


### [177] ["TODO: Fix the Mess Gemini Created": Towards Understanding GenAI-Induced Self-Admitted Technical Debt](https://arxiv.org/abs/2601.07786)
*Abdullah Al Mujahid,Mia Mohammad Imran*

Main category: cs.SE

TL;DR: 本文分析了代码注释中明确承认使用生成式AI并且自认存在技术债务的现象，提出了GenAI诱发的自认技术债务（GIST）这一概念，揭示了AI代码辅助对技术债务形成的影响。


<details>
  <summary>Details</summary>
Motivation: 随着ChatGPT、Copilot等大型语言模型被集成进软件开发流程，开发者在代码注释中越来越多地涉及AI的使用及其技术缺陷，探索这一现象有助于理解AI生成代码带来的技术债务。

Method: 通过分析6540条公开Python和JavaScript GitHub仓库中的代码注释，筛选出81条同时承认技术债务的注释，归纳开发者描述的主要技术债务类型。

Result: 发现开发者最常提及推迟测试、不完全适配及对AI生成代码理解不足，显示AI辅助影响技术债务产生的时机和原因。

Conclusion: 提出GenAI诱发的自认技术债务（GIST）作为新的分析视角，用以描述开发者在整合AI代码时对其行为或正确性的明确不确定性。

Abstract: As large language models (LLMs) such as ChatGPT, Copilot, Claude, and Gemini become integrated into software development workflows, developers increasingly leave traces of AI involvement in their code comments. Among these, some comments explicitly acknowledge both the use of generative AI and the presence of technical shortcomings. Analyzing 6,540 LLM-referencing code comments from public Python and JavaScript-based GitHub repositories (November 2022-July 2025), we identified 81 that also self-admit technical debt(SATD). Developers most often describe postponed testing, incomplete adaptation, and limited understanding of AI-generated code, suggesting that AI assistance affects both when and why technical debt emerges. We term GenAI-Induced Self-admitted Technical debt (GIST) as a proposed conceptual lens to describe recurring cases where developers incorporate AI-generated code while explicitly expressing uncertainty about its behavior or correctness.

</details>
