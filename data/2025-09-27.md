<div id=toc></div>

# Table of Contents

- [cs.MA](#cs.MA) [Total: 2]
- [cs.SE](#cs.SE) [Total: 18]


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [1] [Structuring Collective Action with LLM-Guided Evolution: From Ill-Structured Problems to Executable Heuristics](https://arxiv.org/abs/2509.20412)
*Kevin Bradley Dsouza,Graham Alexander Watt,Yuri Leonenko,Juan Moreno-Cruz*

Main category: cs.MA

TL;DR: 本文提出了ECHO-MIMIC框架，通过进化搜索发现高效的行为启发式和定制化的语言信息，帮助个体行为与集体目标对齐，从而解决集体行动中的难解问题。


<details>
  <summary>Details</summary>
Motivation: 集体行动问题中个体激励与整体目标难以协调，因果关系不清且目标冲突，缺乏清晰算法桥接微观行为与宏观福利。

Method: ECHO阶段进化生成Python代码编码行为策略，MIMIC阶段进化生成激励语言信息，大型语言模型驱动多样化候选方案，通过仿真环境选择最优表现。

Result: 在农业景观管理的集体行动问题中，ECHO-MIMIC发现了表现优异的启发式策略，生成的定制化信息成功引导个体行为与生态目标一致。

Conclusion: ECHO-MIMIC将复杂的集体行动问题转化为简单的个体可执行指令，降低认知负担，实现以前难解问题的实际解决，推动可扩展适应性政策设计。

Abstract: Collective action problems, which require aligning individual incentives with
collective goals, are classic examples of Ill-Structured Problems (ISPs). For
an individual agent, the causal links between local actions and global outcomes
are unclear, stakeholder objectives often conflict, and no single, clear
algorithm can bridge micro-level choices with macro-level welfare. We present
ECHO-MIMIC, a computational framework that converts this global complexity into
a tractable, Well-Structured Problem (WSP) for each agent by discovering
compact, executable heuristics and persuasive rationales. The framework
operates in two stages: ECHO (Evolutionary Crafting of Heuristics from
Outcomes) evolves snippets of Python code that encode candidate behavioral
policies, while MIMIC (Mechanism Inference & Messaging for
Individual-to-Collective Alignment) evolves companion natural language messages
that motivate agents to adopt those policies. Both phases employ a
large-language-model-driven evolutionary search: the LLM proposes diverse and
context-aware code or text variants, while population-level selection retains
those that maximize collective performance in a simulated environment. We
demonstrate this framework on a canonical ISP in agricultural landscape
management, where local farming decisions impact global ecological
connectivity. Results show that ECHO-MIMIC discovers high-performing heuristics
compared to baselines and crafts tailored messages that successfully align
simulated farmer behavior with landscape-level ecological goals. By coupling
algorithmic rule discovery with tailored communication, ECHO-MIMIC transforms
the cognitive burden of collective action into a simple set of agent-level
instructions, making previously ill-structured problems solvable in practice
and opening a new path toward scalable, adaptive policy design.

</details>


### [2] [RadAgents: Multimodal Agentic Reasoning for Chest X-ray Interpretation with Radiologist-like Workflows](https://arxiv.org/abs/2509.20490)
*Kai Zhang,Corey D Barrett,Jangwon Kim,Lichao Sun,Tara Taghavi,Krishnaram Kenthapadi*

Main category: cs.MA

TL;DR: RadAgents提出了一种多智能体框架，通过结合临床先验与多模态推理，提升胸片解读的可靠性与透明度。


<details>
  <summary>Details</summary>
Motivation: 现有胸片解读方法缺乏临床可解释性、多模态融合不足且无法有效解决跨工具不一致问题。

Method: 设计RadAgents框架，结合临床先验，进行多智能体、多模态推理，并通过多模态检索增强机制验证和解决上下文冲突。

Result: 系统实现了更加可靠、透明，并符合临床实践的胸片解读输出。

Conclusion: RadAgents有效弥补了胸片解读中推理不解释、证据融合不足及不一致性检测的缺陷，提升临床应用价值。

Abstract: Agentic systems offer a potential path to solve complex clinical tasks
through collaboration among specialized agents, augmented by tool use and
external knowledge bases. Nevertheless, for chest X-ray (CXR) interpretation,
prevailing methods remain limited: (i) reasoning is frequently neither
clinically interpretable nor aligned with guidelines, reflecting mere
aggregation of tool outputs; (ii) multimodal evidence is insufficiently fused,
yielding text-only rationales that are not visually grounded; and (iii) systems
rarely detect or resolve cross-tool inconsistencies and provide no principled
verification mechanisms. To bridge the above gaps, we present RadAgents, a
multi-agent framework for CXR interpretation that couples clinical priors with
task-aware multimodal reasoning. In addition, we integrate grounding and
multimodal retrieval-augmentation to verify and resolve context conflicts,
resulting in outputs that are more reliable, transparent, and consistent with
clinical practice.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [3] [ACCeLLiuM: Supervised Fine-Tuning for Automated OpenACC Pragma Generation](https://arxiv.org/abs/2509.20380)
*Samyak Jhaveri,Vanessa Klotzmann,Crista Lopes*

Main category: cs.SE

TL;DR: 提出了ACCeLLiuM，这是两个针对生成OpenACC并行指令微调的大型语言模型，显著提高了自动生成正确OpenACC指令的准确率。


<details>
  <summary>Details</summary>
Motivation: GPU硬件和并行编程框架日益复杂，虽然OpenACC简化了GPU编程，但仍需专家经验，有必要自动生成有效的OpenACC指令以降低编程门槛。

Method: 收集了4033个公开GitHub上的C/C++代码及对应OpenACC指令对，基于此数据集对大型语言模型进行监督微调，训练生成适用于数据并行循环的OpenACC指令。

Result: 微调后的模型在测试集上生成有效OpenACC指令的准确率达87%，完全匹配指令的准确率为50%，表现显著优于未微调的基础模型。

Conclusion: ACCeLLiuM模型和数据集公开，建立了一个可复现的基准，预计能降低GPU自动加速的技术门槛，提高串行程序的GPU迁移效率。

Abstract: The increasing ubiquity of GPUs is accompanied by the increasing complexity
of their hardware and parallel programming frameworks. Directive-based parallel
programming standards like OpenACC simplify GPU programming to some extent by
abstracting away low-level complexities, but a fair amount of expertise is
still required in order to use those directives effectively.
  We introduce ACCeLLiuM, two open weights Large Language Models specifically
fine-tuned for generating expert OpenACC directives for data-parallel loops,
along with the supervised fine-tuning dataset that was used to train them. The
ACCeLLiuM SFT dataset contains 4,033 OpenACC pragma-loop pairs mined from
public GitHub C/C++ repositories, with 3,223 pairs for training and 810 for
testing. Experimental evaluations show a pronounced performance gap in
generating correct OpenACC pragmas between base LLMs and our fine-tuned
versions. On the held-out test set, base LLMs fail to consistently generate
valid pragmas, whereas LLMs fine-tuned on the ACCeLLiuM dataset generate valid
pragmas with the correct directive type for $87\%$ of the data-parallel loops,
and exact pragmas--including directives, clauses, clause order, and clause
variables--for $50\%$ of the cases. Even when not exact, generated pragmas
frequently incorporate the correct clauses in a different order than the
ground-truth label, or include additional clauses that enable finer control
over parallel execution, data movement, and concurrency, offering practical
value beyond strict string-matching. By publicly releasing the code, models,
and dataset as ACCeLLiuM we hope to establish a reproducible benchmark for
LLM-powered OpenACC pragma generation, and lower the barrier to automated GPU
offloading of serially written programs.

</details>


### [4] [State-of-the-Art in Software Security Visualization: A Systematic Review](https://arxiv.org/abs/2509.20385)
*Ishara Devendra,Chaman Wijesiriwardana,Prasad Wimalaratne*

Main category: cs.SE

TL;DR: 本文系统综述了软件安全可视化领域，分类总结现有技术并探讨未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统复杂度和安全威胁的发展，传统文本和数值分析方法已难以有效解决安全问题，需通过可视化手段更好地理解安全数据。

Method: 系统回顾60篇以上近期关键论文，构建软件安全可视化技术的全面分类体系，包括基于图、符号、矩阵和隐喻的可视化。

Result: 从分析中区分出两个主要领域：软件开发可视化（专注于软件架构描绘）和网络安全可视化，揭示现有方法的不足和创新需求。

Conclusion: 强调需发展适应不断变化安全环境的新型可视化技术，以提升威胁检测和响应能力，并指导未来研究方向。

Abstract: Software security visualization is an interdisciplinary field that combines
the technical complexity of cybersecurity, including threat intelligence and
compliance monitoring, with visual analytics, transforming complex security
data into easily digestible visual formats. As software systems get more
complex and the threat landscape evolves, traditional text-based and numerical
methods for analyzing and interpreting security concerns become increasingly
ineffective. The purpose of this paper is to systematically review existing
research and create a comprehensive taxonomy of software security visualization
techniques through literature, categorizing these techniques into four types:
graph-based, notation-based, matrix-based, and metaphor-based visualization.
This systematic review explores over 60 recent key research papers in software
security visualization, highlighting its key issues, recent advancements, and
prospective future research directions. From the comprehensive analysis, the
two main areas were distinctly highlighted as extensive software development
visualization, focusing on advanced methods for depicting software
architecture: operational security visualization and cybersecurity
visualization. The findings highlight the necessity for innovative
visualization techniques that adapt to the evolving security landscape, with
practical implications for enhancing threat detection, improving security
response strategies, and guiding future research.

</details>


### [5] [Dynamic ReAct: Scalable Tool Selection for Large-Scale MCP Environments](https://arxiv.org/abs/2509.20386)
*Nishant Gaurav,Adit Akarsh,Ankit Ranjan,Manoj Bajaj*

Main category: cs.SE

TL;DR: Dynamic ReAct提出了一种高效处理大量工具集的工具选择方法，通过逐步优化架构，最终实现了智能筛选与加载工具，显著减少计算成本并保持任务准确率。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在面对成百上千工具时，因上下文记忆限制难以同时加载所有工具，导致工具选择效率低下。

Method: 提出并评估了五种逐步优化的架构，最终采用基于搜索与动态加载的机制，实现智能工具选择，减少不必要的工具加载。

Result: 实验显示该方法在保持任务完成准确率的前提下，减少了多达50%的工具加载量，显著降低计算负担。

Conclusion: 该方法推动了通用AI代理向能动态适应多任务环境方向发展，实现了高效且准确的工具管理与选择能力。

Abstract: We present Dynamic ReAct, a novel approach for enabling ReAct agents to ef-
ficiently operate with extensive Model Control Protocol (MCP) tool sets that
exceed the contextual memory limitations of large language models. Our approach
addresses the fundamental challenge of tool selection in environments
containing hundreds or thousands of available tools, where loading all tools
simultaneously is computationally infeasible. We propose and evaluate five
distinct architectures that progressively refine the tool selection process,
culminating in a search-and-load mechanism that achieves intelligent tool
selection with minimal computational overhead. Our experimental results
demonstrate that the proposed approach reduces tool loading by up to 50% while
maintaining task completion accuracy, advancing the path towards truly
general-purpose AI agents capable of dynamically adapting to diverse task
environments.

</details>


### [6] [Towards Systematic Specification and Verification of Fairness Requirements: A Position Paper](https://arxiv.org/abs/2509.20387)
*Qusai Ramadan,Jukka Ruohonen,Abhishek Tiwari,Adam Alami,Zeyd Boukhers*

Main category: cs.SE

TL;DR: 该论文提出利用知识图谱构建公平性需求的形式化框架，解决当前软件系统中缺乏明确公平性需求规范和验证的问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视了缺乏明确定义和验证的公平性需求是歧视产生的重要原因，现有的公平性知识隐晦难以明确规范。

Method: 借鉴安全工程领域知识图谱对知识的形式化作用，提出基于知识图谱的公平性需求规范及验证框架。

Result: 提出了公平性需求规范与验证的挑战、研究问题及未来研究路线图。

Conclusion: 基于知识图谱的公平性需求框架有助于解决软件系统中的歧视问题，为后续研究提供指导。

Abstract: Decisions suggested by improperly designed software systems might be prone to
discriminate against people based on protected characteristics, such as gender
and ethnicity. Previous studies attribute such undesired behavior to flaws in
algorithmic design or biased data. However, these studies ignore that
discrimination is often the result of a lack of well-specified fairness
requirements and their verification. The fact that experts' knowledge about
fairness is often implicit makes the task of specifying precise and verifiable
fairness requirements difficult. In related domains, such as security
engineering, knowledge graphs have been proven to be effective in formalizing
knowledge to assist requirements specification and verification. To address the
lack of formal mechanisms for specifying and verifying fairness requirements,
we propose the development of a knowledge graph-based framework for fairness.
In this paper, we discuss the challenges, research questions, and a road map
towards addressing the research questions.

</details>


### [7] [Online-Optimized RAG for Tool Use and Function Calling](https://arxiv.org/abs/2509.20415)
*Yu Pan,Xiaocheng Li,Hanzhao Wang*

Main category: cs.SE

TL;DR: 本文提出了一种名为Online-Optimized RAG的部署时框架，通过在线梯度更新优化检索嵌入，解决了检索增强生成中因嵌入失配导致的工具调用错误问题。


<details>
  <summary>Details</summary>
Motivation: 实际应用中，由于嵌入模型不完善或描述噪声，检索嵌入常出现失配，导致检索错误和任务失败。

Method: 提出Online-Optimized RAG框架，利用任务成功等少量反馈，在部署时通过轻量在线梯度更新实时调整检索嵌入，无需修改底层大模型，支持多种工具调用场景。

Result: 方法在多种工具调用和文档检索场景下均提升了工具选择准确率和最终任务成功率。

Conclusion: Online-Optimized RAG提供了一种简单实用、具备自我改进能力的鲁棒检索增强生成系统方案。

Abstract: In many applications, retrieval-augmented generation (RAG) drives tool use
and function calling by embedding the (user) queries and matching them to
pre-specified tool/function descriptions. In this paper, we address an
embedding misalignment issue that often arises in practical applications due to
imperfect embedding models or noisy descriptions; such misalignment may lead to
incorrect retrieval and task failure. We introduce Online-Optimized RAG, a
deployment-time framework that continually adapts retrieval embeddings from
live interactions using minimal feedback (e.g., task success). Online-Optimized
RAG applies lightweight online gradient updates with negligible per-query
latency and requires no changes to the underlying LLM. The method is
plug-and-play: it supports both single- and multi-hop tool use, dynamic tool
inventories, and $K$-retrieval with re-ranking. We provide a problem-dependent
theoretical analysis that quantifies how the method's performance depends on
the initialization quality of the embeddings and other related quantities.
Across diverse tool-use and document-retrieval scenarios, our Online-Optimized
RAG consistently improves tool selection accuracy and end-task success, thus
providing a simple, practical path to robust, self-improving RAG systems.

</details>


### [8] [Formal Verification of Legal Contracts: A Translation-based Approach](https://arxiv.org/abs/2509.20421)
*Reiner Hähnle,Cosimo Laneve,Adele Veschetti*

Main category: cs.SE

TL;DR: 本文提出了将面向法律合同的领域专用语言Stipula转换为带有Java Modeling Language注解的Java代码，并使用KeY工具自动验证合同正确性的方法。


<details>
  <summary>Details</summary>
Motivation: 提升法律合同建模语言Stipula的合同正确性验证能力，确保合同中资产转移和义务的可执行性。

Method: 通过将Stipula合同自动翻译成带Java Modeling Language规范的Java代码，利用KeY工具进行演绎验证，验证合同的部分和完全正确性。

Result: 实现了对包含无交叉循环的Stipula合同子集的全自动正确性验证，证明了使用通用演绎验证工具进行翻译验证的可行性。

Conclusion: 通用演绎验证工具KeY能有效支持Stipula合同的自动化正确性验证，验证方法具备实用价值。

Abstract: Stipula is a domain-specific programming language designed to model legal
contracts with enforceable properties, especially those involving asset
transfers and obligations. This paper presents a methodology to formally verify
the correctness of Stipula contracts through translation into Java code
annotated with Java Modeling Language specifications. As a verification
backend, the deductive verification tool KeY is used. Both, the translation and
the verification of partial and total correctness for a large subset of Stipula
contracts, those with disjoint cycles, is fully automatic. Our work
demonstrates that a general-purpose deductive verification tool can be used
successfully in a translation approach.

</details>


### [9] [AI-Specific Code Smells: From Specification to Detection](https://arxiv.org/abs/2509.20491)
*Brahim Mahmoudi,Naouel Moha,Quentin Stievenert,Florent Avellaneda*

Main category: cs.SE

TL;DR: 该论文提出了SpecDetect4AI工具，用于大规模检测AI系统中特有的代码异味，提升了检测的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: AI系统中新出现的特有代码异味会导致诸如不可重复性、静默失败或模型泛化能力差等问题，现有检测工具无法有效识别这些问题。

Method: 提出一种结合高层声明式领域特定语言（DSL）与可扩展静态分析工具的检测方法，设计并定义了22种AI特有代码异味规则。

Result: 在826个AI系统（2000万代码行）上的评测显示，SpecDetect4AI达到88.66%的准确率和88.89%的召回率，优于现有工具，并获得了81.7/100的系统可用性评分。

Conclusion: SpecDetect4AI有效支持AI代码异味的规范化指定与检测，能够高效扩展至大型AI系统中，提升了代码质量保证能力。

Abstract: The rise of Artificial Intelligence (AI) is reshaping how software systems
are developed and maintained. However, AI-based systems give rise to new
software issues that existing detection tools often miss. Among these, we focus
on AI-specific code smells, recurring patterns in the code that may indicate
deeper problems such as unreproducibility, silent failures, or poor model
generalization. We introduce SpecDetect4AI, a tool-based approach for the
specification and detection of these code smells at scale. This approach
combines a high-level declarative Domain-Specific Language (DSL) for rule
specification with an extensible static analysis tool that interprets and
detects these rules for AI-based systems. We specified 22 AI-specific code
smells and evaluated SpecDetect4AI on 826 AI-based systems (20M lines of code),
achieving a precision of 88.66% and a recall of 88.89%, outperforming other
existing detection tools. Our results show that SpecDetect4AI supports the
specification and detection of AI-specific code smells through dedicated rules
and can effectively analyze large AI-based systems, demonstrating both
efficiency and extensibility (SUS 81.7/100).

</details>


### [10] [PromptDebt: A Comprehensive Study of Technical Debt Across LLM Projects](https://arxiv.org/abs/2509.20497)
*Ahmed Aljohani,Hyunsook Do*

Main category: cs.SE

TL;DR: 本文首次大规模实证研究了大语言模型（LLM）集成中的自承认技术债务（SATD），分析了其起因、普遍性及缓解策略。


<details>
  <summary>Details</summary>
Motivation: 随着LLM通过API嵌入软件，带来了强大功能但也产生了新的技术债务，需要系统研究其特点和管理方法。

Method: 分析了93142个Python文件中与主要LLM API相关的SATD实例，重点研究了OpenAI和LangChain集成，及其在提示设计等方面的具体表现。

Result: 发现54.49%的技术债务来自OpenAI集成，12.35%来自LangChain，提示设计尤其是基于指令和少样本提示最易产生债务，并发布了相关SATD数据集。

Conclusion: 提示设计是LLM专属技术债务的主要来源，适当管理提示配置和优化可有效缓解该类技术债务，研究成果为LLM系统的技术债务管理提供了实证基础和指导。

Abstract: Large Language Models (LLMs) are increasingly embedded in software via APIs
like OpenAI, offering powerful AI features without heavy infrastructure. Yet
these integrations bring their own form of self-admitted technical debt (SATD).
In this paper, we present the first large-scale empirical study of LLM-specific
SATD: its origins, prevalence, and mitigation strategies. By analyzing 93,142
Python files across major LLM APIs, we found that 54.49% of SATD instances stem
from OpenAI integrations and 12.35% from LangChain use. Prompt design emerged
as the primary source of LLM-specific SATD, with 6.61% of debt related to
prompt configuration and optimization issues, followed by hyperparameter tuning
and LLM-framework integration. We further explored which prompt techniques
attract the most debt, revealing that instruction-based prompts (38.60%) and
few-shot prompts (18.13%) are particularly vulnerable due to their dependence
on instruction clarity and example quality. Finally, we release a comprehensive
SATD dataset to support reproducibility and offer practical guidance for
managing technical debt in LLM-powered systems.

</details>


### [11] [Enhancing Python Programming Education with an AI-Powered Code Helper: Design, Implementation, and Impact](https://arxiv.org/abs/2509.20518)
*Sayed Mahbub Hasan Amiri,Md Mainul Islam*

Main category: cs.SE

TL;DR: 该论文介绍了一个基于AI和Python的编程学习聊天机器人，结合静态代码分析、动态执行跟踪和大语言模型，为学生提供实用的编程错误调试和理论转实践的帮助，显著提高了学生的编码能力和调试效率。


<details>
  <summary>Details</summary>
Motivation: 传统的IDE和静态代码分析工具缺乏主动帮助，现有AI代码助手侧重于完成代码，缺少针对学习过程的指导，故研发一个能辅助学生学习编程的智能聊天机器人填补这一空白。

Method: 设计了一个混合架构的聊天机器人，结合CodeLlama进行代码嵌入，GPT-4用于自然语言交互，Docker沙箱执行确保安全；结合静态分析和动态执行跟踪，为学生提供具体的调试建议和编程帮助。

Result: 在1500个学生提交的代码中，聊天机器人错误解决成功率达85%，高于pylint的62%和GPT-4的73%；用户调试时间减少59.3%，编码能力提升34%，尤其是递归和异常处理方面。学生反馈机器人界面清晰易用，增强自信，但存在延迟和代码限制问题。

Conclusion: 该研究展示了如何通过技术创新和教育同理心相结合，开发出促进编程教育公平性和技能长远保持的AI工具，强化人类教学，促进学生深层编程理解。

Abstract: This is the study that presents an AI-Python-based chatbot that helps
students to learn programming by demonstrating solutions to such problems as
debugging errors, solving syntax problems or converting abstract theoretical
concepts to practical implementations. Traditional coding tools like Integrated
Development Environments (IDEs) and static analyzers do not give robotic help
while AI-driven code assistants such as GitHub Copilot focus on getting things
done. To close this gap, our chatbot combines static code analysis, dynamic
execution tracing, and large language models (LLMs) to provide the students
with relevant and practical advice, hence promoting the learning process. The
chatbots hybrid architecture employs CodeLlama for code embedding, GPT-4 for
natural language interactions, and Docker-based sandboxing for secure
execution. Evaluated through a mixed-methods approach involving 1,500 student
submissions, the system demonstrated an 85% error resolution success rate,
outperforming standalone tools like pylint (62%) and GPT-4 (73%). Quantitative
results revealed a 59.3% reduction in debugging time among users, with pre- and
post-test assessments showing a 34% improvement in coding proficiency,
particularly in recursion and exception handling. Qualitative feedback from 120
students highlighted the chatbots clarity, accessibility, and
confidence-building impact, though critiques included occasional latency and
restrictive code sanitization. By balancing technical innovation with
pedagogical empathy, this research provides a blueprint for AI tools that
prioritize educational equity and long-term skill retention over mere code
completion. The chatbot exemplifies how AI can augment human instruction,
fostering deeper conceptual understanding in programming education.

</details>


### [12] [Enhancing LLM-based Fault Localization with a Functionality-Aware Retrieval-Augmented Generation Framework](https://arxiv.org/abs/2509.20552)
*Xinyu Shi,Zhenhao Li,An Ran Chen*

Main category: cs.SE

TL;DR: 提出了FaR-Loc框架，结合大语言模型与检索增强生成技术，实现方法级别的故障定位，在Defects4J基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在复杂软件项目的故障定位中受限于缺乏项目特定知识和难以处理大型项目的挑战。

Method: FaR-Loc包含三部分：LLM功能提取生成失败行为描述，语义密集检索通过共享语义空间匹配相似功能的方法，以及LLM重新排序模块根据上下文相关性排序检索结果。

Result: 在Defects4J上，FaR-Loc在Top-1和Top-5准确率分别比领先基线SoapFL和AutoFL提升约9%-22%，且优于所有学习及频谱基线，无需再训练；引入结构感知的预训练代码嵌入模型如UniXcoder可使Top-1准确率提升49%。

Conclusion: FaR-Loc有效提升了故障定位准确率，结合代码结构信息的预训练模型进一步增强性能，具备良好的实际应用价值。

Abstract: Fault localization (FL) is a critical but time-consuming task in software
debugging, aiming to identify faulty code elements. While recent advances in
large language models (LLMs) have shown promise for FL, they often struggle
with complex systems due to the lack of project-specific knowledge and the
difficulty of navigating large projects. To address these limitations, we
propose FaR-Loc, a novel framework that enhances method-level FL by integrating
LLMs with retrieval-augmented generation (RAG). FaR-Loc consists of three key
components: LLM Functionality Extraction, Semantic Dense Retrieval, and LLM
Re-ranking. First, given a failed test and its associated stack trace, the LLM
Functionality Extraction module generates a concise natural language
description that captures the failing behavior. Next, the Semantic Dense
Retrieval component leverages a pre-trained code-understanding encoder to embed
both the functionality description (natural language) and the covered methods
(code) into a shared semantic space, enabling the retrieval of methods with
similar functional behavior. Finally, the LLM Re-ranking module reorders the
retrieved methods based on their contextual relevance. Our experiments on the
widely used Defects4J benchmark show that FaR-Loc outperforms state-of-the-art
LLM-based baselines SoapFL and AutoFL, by 14.6% and 9.1% in Top-1 accuracy, by
19.2% and 22.1% in Top-5 accuracy, respectively. It also surpasses all
learning-based and spectrum-based baselines across all Top-N metrics without
requiring re-training. Furthermore, we find that pre-trained code embedding
models that incorporate code structure, such as UniXcoder, can significantly
improve fault localization performance by up to 49.0% in Top-1 accuracy.
Finally, we conduct a case study to illustrate the effectiveness of FaR-Loc and
to provide insights for its practical application.

</details>


### [13] [Design, Implementation and Evaluation of a Novel Programming Language Topic Classification Workflow](https://arxiv.org/abs/2509.20631)
*Michael Zhang,Yuan Tian,Mariam Guizani*

Main category: cs.SE

TL;DR: 本文提出了一种基于多标签支持向量机结合滑动窗口与投票策略的编程语言主题分类方法，实现了对核心语言概念的细粒度定位，模型在IBM Project CodeNet数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统规模和复杂性提高，理解代码中编程语言主题的分布对于技术决策、人员培训及工具和教育非常重要。

Method: 设计并实现了一种多标签支持向量机(SVM)与滑动窗口及投票策略结合的分类工作流，用以细粒度地定位语言核心概念。

Result: 在IBM Project CodeNet数据集上，模型在主题分类的平均F1值达到0.90，在代码主题高亮中达0.75。

Conclusion: 该方法为代码分析和数据驱动的软件工程提供了实证见解及可复用的流程，促进相关研究和实践。

Abstract: As software systems grow in scale and complexity, understanding the
distribution of programming language topics within source code becomes
increasingly important for guiding technical decisions, improving onboarding,
and informing tooling and education. This paper presents the design,
implementation, and evaluation of a novel programming language topic
classification workflow. Our approach combines a multi-label Support Vector
Machine (SVM) with a sliding window and voting strategy to enable fine-grained
localization of core language concepts such as operator overloading, virtual
functions, inheritance, and templates. Trained on the IBM Project CodeNet
dataset, our model achieves an average F1 score of 0.90 across topics and 0.75
in code-topic highlight. Our findings contribute empirical insights and a
reusable pipeline for researchers and practitioners interested in code analysis
and data-driven software engineering.

</details>


### [14] [Exploring Engagement in Hybrid Meetings](https://arxiv.org/abs/2509.20780)
*Daniela Grassi,Fabio Calefato,Darja Smite,Nicole Novielli,Filippo Lanubile*

Main category: cs.SE

TL;DR: 研究了混合会议中远程与现场参与者的参与度，发现长期会议中远程参与者的参与度较低，积极角色提升参与度，大会议和下午时段参与度下降。


<details>
  <summary>Details</summary>
Motivation: COVID-19疫情后混合办公普及，改变了软件开发的协作模式，导致远程参与可能引发孤立感和参与度下降。

Method: 通过三个软件公司的多模态测量，包括问卷和生理数据，客观分析混合会议中参与者的参与度。

Result: 线上线下参与者整体参与度相当，远程参与者在长会议中参与度降低，积极角色与高参与度正相关，大会议和下午会议参与度较低。

Conclusion: 研究揭示了影响混合会议参与度的因素，提出了优化会议的建议，对软件团队及其他知识密集型组织具有参考价值。

Abstract: Background. The widespread adoption of hybrid work following the COVID-19
pandemic has fundamentally transformed software development practices,
introducing new challenges in communication and collaboration as organizations
transition from traditional office-based structures to flexible working
arrangements. This shift has established a new organizational norm where even
traditionally office-first companies now embrace hybrid team structures. While
remote participation in meetings has become commonplace in this new
environment, it may lead to isolation, alienation, and decreased engagement
among remote team members. Aims. This study aims to identify and characterize
engagement patterns in hybrid meetings through objective measurements, focusing
on the differences between co-located and remote participants. Method. We
studied professionals from three software companies over several weeks,
employing a multimodal approach to measure engagement. Data were collected
through self-reported questionnaires and physiological measurements using
biometric devices during hybrid meetings to understand engagement dynamics.
Results. The regression analyses revealed comparable engagement levels between
onsite and remote participants, though remote participants show lower
engagement in long meetings regardless of participation mode. Active roles
positively correlate with higher engagement, while larger meetings and
afternoon sessions are associated with lower engagement. Conclusions. Our
results offer insights into factors associated with engagement and
disengagement in hybrid meetings, as well as potential meeting improvement
recommendations. These insights are potentially relevant not only for software
teams but also for knowledge-intensive organizations across various sectors
facing similar hybrid collaboration challenges.

</details>


### [15] [Verification Limits Code LLM Training](https://arxiv.org/abs/2509.20837)
*Srishti Gureja,Elena Tommasone,Jingyi He,Sara Hooker,Matthias Gallé,Marzieh Fadaee*

Main category: cs.SE

TL;DR: 本文研究了代码生成中合成数据的验证瓶颈，提出通过调整验证策略提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型生成代码依赖合成数据，验证的质量限制了训练数据的多样性和质量，形成“验证天花板”。

Method: 系统分析测试复杂度和数量对性能的影响，探索放宽通过标准和LLM软验证，比较正式正确与错误解的效果。

Result: 丰富测试集提高代码生成能力，放宽验证标准回收更多训练数据，提升2-4点pass@1，保留多样正确解增强泛化。

Conclusion: 现有验证过于严格，过滤了宝贵多样性，验证不能舍弃但需重新校准，结合多样且具挑战的问题解决对，能突破验证天花板。

Abstract: Large language models for code generation increasingly rely on synthetic
data, where both problem solutions and verification tests are generated by
models. While this enables scalable data creation, it introduces a previously
unexplored bottleneck: the verification ceiling, in which the quality and
diversity of training data are fundamentally constrained by the capabilities of
synthetic verifiers. In this work, we systematically study how verification
design and strategies influence model performance. We investigate (i) what we
verify by analyzing the impact of test complexity and quantity: richer test
suites improve code generation capabilities (on average +3 pass@1), while
quantity alone yields diminishing returns, (ii) how we verify by exploring
relaxed pass thresholds: rigid 100% pass criteria can be overly restrictive. By
allowing for relaxed thresholds or incorporating LLM-based soft verification,
we can recover valuable training data, leading to a 2-4 point improvement in
pass@1 performance. However, this benefit is contingent upon the strength and
diversity of the test cases used, and (iii) why verification remains necessary
through controlled comparisons of formally correct versus incorrect solutions
and human evaluation: retaining diverse correct solutions per problem yields
consistent generalization gains. Our results show that Verification as
currently practiced is too rigid, filtering out valuable diversity. But it
cannot be discarded, only recalibrated. By combining calibrated verification
with diverse, challenging problem-solution pairs, we outline a path to break
the verification ceiling and unlock stronger code generation models.

</details>


### [16] [PseudoBridge: Pseudo Code as the Bridge for Better Semantic and Logic Alignment in Code Retrieval](https://arxiv.org/abs/2509.20881)
*Yixuan Li,Xinyi Liu,Weidong Yang,Ben Fei,Shuhao Li,Mingjie Zhou,Lipeng Ma*

Main category: cs.SE

TL;DR: 本文提出了一种名为PseudoBridge的代码检索框架，通过引入伪代码作为自然语言与编程语言之间的桥梁，提升代码检索准确性和模型对代码风格多样性的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于预训练语言模型的代码检索方法面临人类意图与机器执行逻辑之间的语义鸿沟以及对多样化编码风格的鲁棒性不足的问题。

Method: PseudoBridge包含两个阶段：首先用大型语言模型生成伪代码，实现自然语言查询与伪代码的显式语义对齐；其次进行逻辑不变的代码风格增强，通过LLM生成风格多样但逻辑等价的代码，并将其与伪代码对齐，提升模型风格鲁棒性。

Result: 在10个预训练语言模型和6种主流编程语言上，PseudoBridge均显著优于基线方法，尤其在零样本领域迁移任务（如Solidity和XLCoST）中表现突出。

Conclusion: 引入伪代码作为中间模态实现逻辑对齐，有效提升了代码检索准确率和泛化能力，PseudoBridge是一种鲁棒且具备良好泛化性的代码检索解决方案。

Abstract: Code search aims to precisely find relevant code snippets that match natural
language queries within massive codebases, playing a vital role in software
development. Recent advances leverage pre-trained language models (PLMs) to
bridge the semantic gap between unstructured natural language (NL) and
structured programming languages (PL), yielding significant improvements over
traditional information retrieval and early deep learning approaches. However,
existing PLM-based methods still encounter key challenges, including a
fundamental semantic gap between human intent and machine execution logic, as
well as limited robustness to diverse code styles. To address these issues, we
propose PseudoBridge, a novel code retrieval framework that introduces
pseudo-code as an intermediate, semi-structured modality to better align NL
semantics with PL logic. Specifically, PseudoBridge consists of two stages.
First, we employ an advanced large language model (LLM) to synthesize
pseudo-code, enabling explicit alignment between NL queries and pseudo-code.
Second, we introduce a logic-invariant code style augmentation strategy and
employ the LLM to generate stylistically diverse yet logically equivalent code
implementations with pseudo-code, then align the code snippets of different
styles with pseudo-code, enhancing model robustness to code style variation. We
build PseudoBridge across 10 different PLMs and evaluate it on 6 mainstream
programming languages. Extensive experiments demonstrate that PseudoBridge
consistently outperforms baselines, achieving significant gains in retrieval
accuracy and generalization, particularly under zero-shot domain transfer
scenarios such as Solidity and XLCoST datasets. These results demonstrate the
effectiveness of explicit logical alignment via pseudo-code and highlight
PseudoBridge's potential as a robust, generalizable solution for code
retrieval.

</details>


### [17] [Designing for Novice Debuggers: A Pilot Study on an AI-Assisted Debugging Tool](https://arxiv.org/abs/2509.21067)
*Oka Kurniawan,Erick Chandra,Christopher M. Poskitt,Yannic Noller,Kenny Tsu Wei Choo,Cyrille Jegourel*

Main category: cs.SE

TL;DR: 本文介绍了CodeHinter，一款结合传统调试工具和大语言模型的调试辅助工具，帮助初学者修复语义错误并积极参与调试过程。


<details>
  <summary>Details</summary>
Motivation: 现有基于自动程序修复和大语言模型的调试工具过度依赖AI，缺乏对初学者主动调试参与的促进。

Method: 设计并迭代开发CodeHinter，结合传统调试功能和LLM技术，推动学生主动定位并修复语义错误；在本科生中进行测试。

Result: 学生认为CodeHinter在解决语义错误方面效果显著，使用更简便，错误定位为最有价值功能。

Conclusion: AI辅助调试工具应根据用户个人资料个性化设计，以优化学生的互动和学习体验。

Abstract: Debugging is a fundamental skill that novice programmers must develop.
Numerous tools have been created to assist novice programmers in this process.
Recently, large language models (LLMs) have been integrated with automated
program repair techniques to generate fixes for students' buggy code. However,
many of these tools foster an over-reliance on AI and do not actively engage
students in the debugging process. In this work, we aim to design an intuitive
debugging assistant, CodeHinter, that combines traditional debugging tools with
LLM-based techniques to help novice debuggers fix semantic errors while
promoting active engagement in the debugging process. We present findings from
our second design iteration, which we tested with a group of undergraduate
students. Our results indicate that the students found the tool highly
effective in resolving semantic errors and significantly easier to use than the
first version. Consistent with our previous study, error localization was the
most valuable feature. Finally, we conclude that any AI-assisted debugging tool
should be personalized based on user profiles to optimize their interactions
with students.

</details>


### [18] [An Improved Quantum Software Challenges Classification Approach using Transfer Learning and Explainable AI](https://arxiv.org/abs/2509.21068)
*Nek Dil Khan,Javed Ali Khan,Mobashir Husain,Muhammad Sohail Khan,Arif Ali Khan,Muhammad Azeem Akbar,Shahid Hussain*

Main category: cs.SE

TL;DR: 本文针对量子软件工程领域中开发者在Stack Overflow上的问答帖子进行分类，识别常见挑战并采用基于Transformer的算法实现高准确率的自动分类。


<details>
  <summary>Details</summary>
Motivation: 量子软件工程开发者面临优化量子计算和开发过程的挑战，而现有标签多聚焦技术细节，缺乏基于量子概念的问答分类，有助于揭示常见问题并改进讨论组织。

Method: 从Q&A平台提取2829个量子相关问题，进行内容分析和基于扎根理论的标注，利用ChatGPT校验标注结果，并对比微调Transformer模型（BERT、DistilBERT、RoBERTa）与传统深度学习模型的分类表现，采用SHAP方法解释模型预测。

Result: 基于Transformer的模型平均准确率达95%，优于传统深度学习模型（最高89%），且无需数据增强。SHAP分析揭示了语言特征对分类的影响，提高了模型可解释性。

Conclusion: 通过自动化精准分类量子软件工程相关问答内容，有助于量子开发者和厂商更好组织和访问讨论内容，提升社区效率，但需进一步的实证研究支持。

Abstract: Quantum Software Engineering (QSE) is a research area practiced by tech
firms. Quantum developers face challenges in optimizing quantum computing and
QSE concepts. They use Stack Overflow (SO) to discuss challenges and label
posts with specialized quantum tags, which often refer to technical aspects
rather than developer posts. Categorizing questions based on quantum concepts
can help identify frequent QSE challenges. We conducted studies to classify
questions into various challenges. We extracted 2829 questions from Q&A
platforms using quantum-related tags. Posts were analyzed to identify frequent
challenges and develop a novel grounded theory. Challenges include Tooling,
Theoretical, Learning, Conceptual, Errors, and API Usage. Through content
analysis and grounded theory, discussions were annotated with common challenges
to develop a ground truth dataset. ChatGPT validated human annotations and
resolved disagreements. Fine-tuned transformer algorithms, including BERT,
DistilBERT, and RoBERTa, classified discussions into common challenges. We
achieved an average accuracy of 95% with BERT DistilBERT, compared to
fine-tuned Deep and Machine Learning (D&ML) classifiers, including Feedforward
Neural Networks (FNN), Convolutional Neural Networks (CNN), and Long Short-Term
Memory networks (LSTM), which achieved accuracies of 89%, 86%, and 84%,
respectively. The Transformer-based approach outperforms the D&ML-based
approach with a 6\% increase in accuracy by processing actual discussions,
i.e., without data augmentation. We applied SHAP (SHapley Additive
exPlanations) for model interpretability, revealing how linguistic features
drive predictions and enhancing transparency in classification. These findings
can help quantum vendors and forums better organize discussions for improved
access and readability. However,empirical evaluation studies with actual
developers and vendors are needed.

</details>


### [19] [Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach](https://arxiv.org/abs/2509.21170)
*Yongda Yu,Guohao Shi,Xianwei Wu,Haochuan He,XueMing Gu,Qianqian Zhao,Kui Liu,Qiushi Wang,Zhao Tian,Haifeng Shen,Guoping Rong*

Main category: cs.SE

TL;DR: 该论文提出了MelcotCR，一种基于链式思维（COT）的多维代码评审微调方法，通过长链式思维技术和最大熵模型结合预定义推理路径，提升了大语言模型在代码问题检测与描述上的准确率，效果可媲美大规模模型。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在自动代码评审中表现出潜力，但能力受限于训练数据，且未充分利用多维度信息进行细致分析，导致潜力未完全发挥。

Method: 提出MelcotCR，通过长链式思维技术生成丰富的结构化信息，结合最大熵模型与预定义推理路径，解决上下文丢失与推理逻辑松散问题，实现多维代码评审推理能力的提升。

Result: 基于14B参数的Qwen2.5模型，在MelcotCR数据集和公开CodeReviewer数据集上微调后，在代码问题检测与描述准确率方面超越最先进方法，与671B的DeepSeek-R1模型表现相当。

Conclusion: MelcotCR有效提升了中等规模大语言模型在代码评审任务中的推理能力和表现，为多维度代码问题分析提供了新思路，减少了对超大模型的依赖。

Abstract: Large Language Models (LLMs) have shown great potential in supporting
automated code review due to their impressive capabilities in context
understanding and reasoning. However, these capabilities are still limited
compared to human-level cognition because they are heavily influenced by the
training data. Recent research has demonstrated significantly improved
performance through fine-tuning LLMs with code review data. However, compared
to human reviewers who often simultaneously analyze multiple dimensions of code
review to better identify issues, the full potential of these methods is
hampered by the limited or vague information used to fine-tune the models. This
paper contributes MelcotCR, a chain-of-thought (COT) fine-tuning approach that
trains LLMs with an impressive reasoning ability to analyze multiple dimensions
of code review by harnessing long COT techniques to provide rich structured
information. To address context loss and reasoning logic loss issues that
frequently occur when LLMs process long COT prompts, we propose a solution that
combines the Maximum Entropy (ME) modeling principle with pre-defined reasoning
pathways in MelcotCR to enable more effective utilization of in-context
knowledge within long COT prompts while strengthening the logical tightness of
the reasoning process. Empirical evaluations on our curated MelcotCR dataset
and the public CodeReviewer dataset reveal that a low-parameter base model,
such as 14B Qwen2.5, fine-tuned with MelcotCR can surpass state-of-the-art
methods in terms of the accuracy of detecting and describing code issues, with
its performance remarkably on par with that of the 671B DeepSeek-R1 model.

</details>


### [20] [Semantic Clustering of Civic Proposals: A Case Study on Brazil's National Participation Platform](https://arxiv.org/abs/2509.21292)
*Ronivaldo Ferreira,Guilherme da Silva,Carla Rocha,Gustavo Pinto*

Main category: cs.SE

TL;DR: 本文提出了一种结合BERTopic、种子词和大语言模型自动验证的方法，用于自动分类政府数字参与平台上的大量用户贡献，实现高效且符合官方分类的主题提取。


<details>
  <summary>Details</summary>
Motivation: 政府数字参与平台如Brasil Participativo上贡献内容极为庞大，手工分类不可行，且需专家介入及与官方分类体系保持一致，如何有效组织这些参与内容成为挑战。

Method: 提出结合BERTopic主题模型、种子词指导以及大语言模型自动验证的混合方法，减少人工参与，自动生成具有机构一致性的主题。

Result: 初步实验显示该方法生成的主题具有良好连贯性和符合官方机构分类体系，且人工干预最小。

Conclusion: 该方法能够帮助政府高效转化大量公民意见为可利用的政策数据，提升数字参与平台的应用价值。

Abstract: Promoting participation on digital platforms such as Brasil Participativo has
emerged as a top priority for governments worldwide. However, due to the sheer
volume of contributions, much of this engagement goes underutilized, as
organizing it presents significant challenges: (1) manual classification is
unfeasible at scale; (2) expert involvement is required; and (3) alignment with
official taxonomies is necessary. In this paper, we introduce an approach that
combines BERTopic with seed words and automatic validation by large language
models. Initial results indicate that the generated topics are coherent and
institutionally aligned, with minimal human effort. This methodology enables
governments to transform large volumes of citizen input into actionable data
for public policy.

</details>
