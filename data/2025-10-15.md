<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 62]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.SE](#cs.SE) [Total: 17]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [PHANTOM RECALL: When Familiar Puzzles Fool Smart Models](https://arxiv.org/abs/2510.11812)
*Souradeep Mukhopadhyay,Rishabh Baral,Nimeesh Mahajan,Samhitha Harish,Aswin RRV,Mihir Parmar,Mutsumi Nakamura,Chitta Baral*

Main category: cs.CL

TL;DR: LLMs表现出依赖记忆模板的幻影回忆问题，难以面对略微变动的逻辑谜题，说明其语言流畅性并不等同于逻辑推理的真正理解。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否及在多大程度上解决了推理依赖记忆模板的问题，特别是在谜题扰动后模型的表现及能否通过提示重构提升其推理能力。

Method: 提出PHANTOM RECALL基准，包括25个著名逻辑谜题及149个经过设计的扰动，评估11个领先的LLMs，采用自动逻辑等价判定工具、细粒度推理错误分类和基于提示的缓解框架来系统检测和缓解模型错误。

Result: 模型在未修改谜题上的准确率接近完美，但在扰动谜题上明显不及人类，频繁出现幻影回忆（phantom recall）和过度阐述，显示模型难以应对上下文变化的推理需求。

Conclusion: 大型语言模型（LLMs）在解决经典逻辑谜题时表现出对记忆模板的依赖，缺乏真正的推理能力，当谜题稍有变动时表现显著下降，揭示了其脆弱性。

Abstract: Large language models (LLMs) such as GPT, Gemini, and Claude often appear
adept at solving classic logic puzzles--but how much genuine reasoning
underlies their answers? Recent evidence suggests that these models frequently
rely on memorized templates rather than reasoning from first principles. When
puzzles are slightly modified, their performance collapses, revealing a
striking fragility. In particular, we asked: Have LLMs addressed these issues?
To what extent? How about perturbations to other puzzles? Is there a general
way of reformulating the prompt so that the models do better? To examine these
things systematically, we introduce PHANTOM RECALL, a benchmark comprising 25
well-known logic puzzles and 149 carefully designed perturbations that preserve
reasoning structure but alter superficial details and solutions. We evaluate
eleven leading LLMs and identify a recurring failure mode--phantom
recall--where models confidently reproduce memorized solutions or spurious
rationales that no longer fit the altered scenario. To probe and mitigate this
issue, we contribute three tools: (i) an automated logical-equivalence judge to
detect reasoning mismatches, (ii) a taxonomy of fine-grained reasoning error
categories, and (iii) a prompting-based mitigation framework guided by these
categories. Despite near-perfect accuracy on unmodified puzzles, models
significantly underperform humans on perturbed ones, exhibiting both phantom
recall and over-elaboration. Our findings reveal a crucial limitation: LLMs
often fail to re-reason when contextual cues shift--highlighting the gap
between linguistic fluency and logical understanding.

</details>


### [2] [R-WoM: Retrieval-augmented World Model For Computer-use Agents](https://arxiv.org/abs/2510.11892)
*Kai Mei,Jiang Guo,Shuaichen Chang,Mingwen Dong,Dongkyu Lee,Xing Niu,Jiarong Jiang*

Main category: cs.CL

TL;DR: 本研究揭示LLM在长期世界模型构建中的不足，提出利用检索增强的R-WoM模型显著提升其预测和模拟能力，有效支持智能体决策。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs能否作为世界模型，用于数字环境中的智能体决策，降低代价高昂的试错成本，但面临LLMs生成幻觉和静态知识限制问题。

Method: 通过设计三个任务（下一状态识别、完整规划对齐和里程碑转移识别）测试LLM在状态预测和奖励估计两大核心能力，进一步提出结合外部教程知识的R-WoM模型来提升模拟效果。

Result: 实验结果表明，R-WoM在OSWorld和WebArena环境中分别提升了25.3%和18.1%的性能，尤其在长周期模拟任务中表现突出。

Conclusion: LLMs在短期状态预测方面表现良好，但在长期规划和环境动态的模拟中存在明显局限。通过引入外部知识检索机制的R-WoM模型显著提升了LLM在长远模拟中的表现。

Abstract: Large Language Models (LLMs) can serve as world models to enhance agent
decision-making in digital environments by simulating future states and
predicting action outcomes, potentially eliminating costly trial-and-error
exploration. However, this capability is fundamentally limited by LLMs'
tendency toward hallucination and their reliance on static training knowledge,
which can lead to compounding errors that inhibit long-horizon simulations. To
systematically investigate whether LLMs are appropriate for world modeling, we
probe two core capabilities of world models--future state prediction and reward
estimation--through three tasks: next-state identification, full-procedure
planning alignment, and milestone transition recognition. Our analysis shows
that while LLMs effectively capture immediate next states and identify
meaningful state transitions, their performance rapidly degrades in
full-procedure planning. This highlights LLMs' limitations in reliably modeling
environment dynamics over long horizons. To address these limitations, we
propose the Retrieval-augmented World Model (R-WoM), which grounds LLM
simulations by incorporating factual, up-to-date knowledge retrieved from
external tutorials. Experiments show that R-WoM achieves substantial
improvements of up to 25.3% (OSWorld) and 18.1% (WebArena) compared to
baselines, with particular advantages in longer-horizon simulations.

</details>


### [3] [LLM Knowledge is Brittle: Truthfulness Representations Rely on Superficial Resemblance](https://arxiv.org/abs/2510.11905)
*Patrick Haller,Mark Ibrahim,Polina Kirichenko,Levent Sagun,Samuel J. Bell*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型对输入表面扰动的敏感性，发现其内部知识表征在一定程度上依赖于训练数据的表面形式，导致模型性能在遇到非分布内样本时变得脆弱。


<details>
  <summary>Details</summary>
Motivation: LLMs在面对训练中未见过的输入变体时性能脆弱，怀疑这与其不稳定的内部知识表征有关。

Method: 通过对真伪陈述进行语义保持的表面形式扰动（如拼写错误和重述），评估多个LLM模型在不同数据集和知识探测方法下，内部表征的区分能力变化。

Result: 实验发现，随着输入与训练数据表面形式的差异增大，模型区分真假陈述的内部表征能力显著退化，表明所学知识表征浅显且缺乏鲁棒性。

Conclusion: 大型语言模型（LLMs）的内部知识表征在处理与训练数据分布差异较大的输入时表现出脆弱性，导致其辨别真假陈述的能力显著下降。

Abstract: For Large Language Models (LLMs) to be reliable, they must learn robust
knowledge that can be generally applied in diverse settings -- often unlike
those seen during training. Yet, extensive research has shown that LLM
performance can be brittle, with models exhibiting excessive sensitivity to
trivial input variations. In this work, we explore whether this brittleness is
a direct result of unstable internal knowledge representations. To explore this
question, we build on previous work showing that LLM representations encode
statement truthfulness -- i.e., true, factual statements can be easily
separated from false, inaccurate ones. Specifically, we test the robustness of
learned knowledge by evaluating representation separability on samples that
have undergone superficial transformations to drive them out-of-distribution
(OOD), such as typos or reformulations. By applying semantically-preserving
perturbations, we study how separability degrades as statements become more
OOD, across four LLM families, five evaluation datasets, and three knowledge
probing methods. Our results reveal that internal representations of statement
truthfulness collapse as the samples' presentations become less similar to
those seen during pre-training. While LLMs can often distinguish between true
and false statements when they closely resemble the pre-training data, this
ability is highly dependent on the statement's exact surface form. These
findings offer a possible explanation for brittle benchmark performance: LLMs
may learn shallow, non-robust knowledge representations that allow for only
limited generalizability. Our work presents a fundamental challenge for the
utility of truthfulness probes, and more broadly, calls for further research on
improving the robustness of learned knowledge representations.

</details>


### [4] [LLM Reasoning for Machine Translation: Synthetic Data Generation over Thinking Tokens](https://arxiv.org/abs/2510.11919)
*Armel Zebaze,Rachel Bawden,Benoît Sagot*

Main category: cs.CL

TL;DR: 在机器翻译任务中，仅靠推理模型生成的“思考”中间步骤提升有限，结合具体翻译尝试的中间表示更有效，教师模型指导微调比简单蒸馏链式思维更有价值。


<details>
  <summary>Details</summary>
Motivation: 大规模推理模型在数学和编程任务中表现出色，探讨其自然语言链式思维对机器翻译的潜在提升作用。

Method: 使用多语言对比不同资源水平和设置，评估中间步骤生成对机器翻译的影响，比较直接微调和基于链式思维的微调方法。

Result: 链式思维生成的中间步骤未显著提升机器翻译性能，但通过组合翻译特定提示策略的输出构建的中间步骤带来了改进。

Conclusion: 中间步骤生成对大规模推理模型进行机器翻译时帮助有限，实际改进依赖于中间步骤包含具体翻译尝试。

Abstract: Large reasoning models (LRMs) have led to new possibilities in terms of
problem-solving, through the devising of a natural language thought process
prior to answering a query. While their capabilities are well known across
mathematics and coding tasks, their impact on the task of machine translation
(MT) remains underexplored. In this work, we explore the benefits of the
generation of intermediate tokens when performing MT across multiple language
pairs of different levels of resourcedness and multiple setups. We find that
"thinking tokens" do not help LRMs better perform MT. This result generalizes
to models fine-tuned to reason before translating using distilled chain of
thought (CoT) inspired by human translators' practices. Specifically,
fine-tuning a model with synthetic CoT explanations detailing how to translate
step-by-step does not outperform standard input-output fine-tuning. However,
constructing the intermediate tokens by combining the outputs of modular
translation-specific prompting strategies results in improvements. Our findings
underscore that the contribution of intermediate tokens during fine-tuning
highly depends on the presence of translation attempts within them. More
broadly, our results suggest that using a teacher to refine target translations
or to expand parallel corpora is more impactful than distilling their CoT
explanations into "thinking" MT models.

</details>


### [5] [Discrepancy Detection at the Data Level: Toward Consistent Multilingual Question Answering](https://arxiv.org/abs/2510.11928)
*Lorena Calvo-Bartolomé,Valérie Aldana,Karla Cantarero,Alonso Madroñal de Mesa,Jerónimo Arenas-García,Jordan Boyd-Graber*

Main category: cs.CL

TL;DR: 提出MIND，一个用户参与的多语言问答事实及文化差异检测系统，验证其有效性并发布相关标注数据集。


<details>
  <summary>Details</summary>
Motivation: 多语言问答系统需确保事实一致性，同时考虑文化差异对主观性回答的影响。

Method: 提出了MIND，一个用户参与的事实核查流程，用以检测多语言问答知识库中的事实和文化不一致，突出显示文化敏感问题的答案差异。

Result: 在母婴健康领域的双语问答系统及其他领域数据集上，MIND均能可靠识别不一致性。并发布了带有事实和文化不一致标注的双语问题数据集。

Conclusion: MIND能够有效检测多语言问答系统中的事实和文化差异，支持构建更具文化意识和事实一致性的问答系统。

Abstract: Multilingual question answering (QA) systems must ensure factual consistency
across languages, especially for objective queries such as What is jaundice?,
while also accounting for cultural variation in subjective responses. We
propose MIND, a user-in-the-loop fact-checking pipeline to detect factual and
cultural discrepancies in multilingual QA knowledge bases. MIND highlights
divergent answers to culturally sensitive questions (e.g., Who assists in
childbirth?) that vary by region and context. We evaluate MIND on a bilingual
QA system in the maternal and infant health domain and release a dataset of
bilingual questions annotated for factual and cultural inconsistencies. We
further test MIND on datasets from other domains to assess generalization. In
all cases, MIND reliably identifies inconsistencies, supporting the development
of more culturally aware and factually consistent QA systems.

</details>


### [6] [TopoAlign: A Framework for Aligning Code to Math via Topological Decomposition](https://arxiv.org/abs/2510.11944)
*Yupei Li,Philipp Borchert,Gerasimos Lampouras*

Main category: cs.CL

TL;DR: 针对数学大语言模型自动公式化表现受限的问题，TopoAlign提出一种结构对齐代码数据生成方法，显著提升模型性能，无需额外标注。


<details>
  <summary>Details</summary>
Motivation: 目前数学大语言模型在自动公式化任务中表现受限，主要原因是缺乏大规模的包含非正式与正式数学语句配对的语料，且现有的自然语言到代码训练因结构和语法差异难以有效迁移到形式数学。

Method: TopoAlign方法通过将代码分解为文档字符串、主函数和依赖函数，并重组为与形式数学陈述结构相似的代码成分，实现结构对齐的数据生成，从而利用大量的代码仓库作为训练资源，无需额外人工标注。

Result: TopoAlign在多个基准测试（minif2f, Putnam, ProofNet）上显著提高了DeepSeek-Math模型的性能，例如BEq@10提升17.77%，typecheck@10提升68.82%。即使对Herald这类已专业化模型也带来小幅性能提升。

Conclusion: TopoAlign框架利用结构对齐的代码数据显著提升了数学大语言模型（Math LLMs）在自动公式化任务中的表现，特别是在DeepSeek-Math模型上带来了显著提升，表明结构对齐训练对提升模型性能有效，即使未引入新的数学知识。

Abstract: Large Language Models (LLMs) excel at both informal and formal (e.g. Lean 4)
mathematical reasoning but still struggle with autoformalisation, the task of
transforming informal into formal mathematical statements. Autoformalisation
helps pair the informal reasoning of LLMs with formal proof assistants which
enable machine-verifiable generation and mitigate hallucinations. Yet, the
performance of current Math LLMs is constrained by the scarcity of large-scale
corpora, particularly those containing pairs of informal and formal statements.
Although current models are trained to generate code from natural language
instructions, structural and syntactic differences between these and formal
mathematics limit effective transfer learning. We propose TopoAlign, a
framework that unlocks widely available code repositories as training resources
for Math LLMs. TopoAlign decomposes code into docstrings, main functions, and
dependency functions, and reassembles these components into analogues that
structurally mirror formal statements. This produces structurally aligned code
data that can be used for training Math LLMs without requiring additional human
annotation. We train two state-of-the-art models, DeepSeek-Math and Herald, and
evaluate them on the minif2f, Putnam, and ProofNet benchmarks. TopoAlign
provides substantial gains for DeepSeek-Math, improving performance by 17.77%
on BEq@10 and 68.82% on typecheck@10. Despite introducing no new mathematical
knowledge, our framework achieves gains of 0.12% and 1.09% for Herald on BEq@10
and typecheck@10, respectively, demonstrating that training on aligned code
data is beneficial even for specialized models.

</details>


### [7] [GRAVITY: A Framework for Personalized Text Generation via Profile-Grounded Synthetic Preferences](https://arxiv.org/abs/2510.11952)
*Priyanka Dey,Daniele Rosa,Wenqing Zheng,Daniel Barcklow,Jieyu Zhao,Emilio Ferrara*

Main category: cs.CL

TL;DR: GRAVITY通过合成包含用户文化和个性特质的偏好数据，提升了个性化LLM内容生成的质量和规模，避免了昂贵的人工标注，支持多文化用户场景。


<details>
  <summary>Details</summary>
Motivation: 现有个性化大语言模型（LLMs）依赖昂贵的人类反馈或交互日志，限制了其扩展性，且忽视了更深层的用户属性，因此需要减少对人工标注的依赖，捕捉用户更丰富的个性化特征。

Method: 通过整合人口统计学、文化和心理学框架（如霍夫斯泰德文化维度、施瓦茨基本价值观、世界价值观调查和五大人格特质模型），GRAVITY合成了基于用户兴趣、价值观、信念及人格特质的偏好对，来指导个性化内容生成。

Result: 在对400名亚马逊用户的书籍描述生成任务中，GRAVITY生成的基于个人画像的合成数据相比提示式条件控制、标准微调及简单合成对生成效果提高超过4%，且用户研究显示其生成内容有86%的偏好率。

Conclusion: GRAVITY框架利用合成的基于用户特征的偏好数据，显著提升了LLM在个性化内容生成上的表现，尤其在多文化环境下表现优异，且用户偏好率高达86%，显示出其有效性和可扩展性。

Abstract: Personalization in LLMs often relies on costly human feedback or interaction
logs, limiting scalability and neglecting deeper user attributes. To reduce the
reliance on human annotations, we introduce GRAVITY (Generative Response with
Aligned Values, Interests, and Traits of You), a framework for generating
synthetic, profile-grounded preference data that captures users' interests,
values, beliefs, and personality traits. By integrating demographic, cultural,
and psychological frameworks -- including Hofstede's cultural dimensions,
Schwartz's basic values, the World Values Survey, and Big Five OCEAN traits --
GRAVITY synthesizes preference pairs to guide personalized content generation.
We evaluate GRAVITY on book descriptions for 400 Amazon users, comparing it to
prompt-based conditioning, standard fine-tuning, and naive synthetic pair
generation. Profile-grounded synthetic data consistently improves generation,
especially across multiple cultures (USA, Brazil, Japan, India), achieving over
4% higher preference gains across baselines, with user studies showing that
GRAVITY outputs are preferred over 86% of the time. Our results show that
scenario-grounded synthetic data can capture richer user variation, reduce
reliance on costly annotation, and produce more engaging, user-centered
content, offering a scalable path for LLM personalization.

</details>


### [8] [Evaluating Retrieval-Augmented Generation Systems on Unanswerable, Uncheatable, Realistic, Multi-hop Queries](https://arxiv.org/abs/2510.11956)
*Gabrielle Kaili-May Liu,Bryan Li,Arman Cohan,William Gantt Walden,Eugene Yang*

Main category: cs.CL

TL;DR: 本文提出一个生成不可作弊且真实的复杂多跳无答案查询的自动化管道CRUMQs，用于提升RAG系统基准的难度和真实性，推动系统性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统的基准测试无法真实反映多跳推理或超出范围的问题，容易通过断开推理作弊，限制了对系统缺陷的发现。

Method: 设计一个自动化的、可控制难度的生成管道，用于创建不可作弊的真实无答案多跳查询，并在两个流行RAG数据集上生成CRUMQs用于评测。

Result: CRUMQs管道生成的测试集在挑战性上显著超过现有基准，减少了81.0%的作弊可能，验证了其有效性和实用性。

Conclusion: 本文提出的CRUMQs管道能够有效生成逼真的、多跳的、无答案的复杂查询，并且这些查询无法通过简单推理作弊。实验显示CRUMQs能更真实地反映RAG系统的局限性，推动更先进系统的发展。

Abstract: Real-world use cases often present RAG systems with complex queries for which
relevant information is missing from the corpus or is incomplete. In these
settings, RAG systems must be able to reject unanswerable, out-of-scope queries
and identify failures of retrieval and multi-hop reasoning. Despite this,
existing RAG benchmarks rarely reflect realistic task complexity for multi-hop
or out-of-scope questions, which often can be cheated via disconnected
reasoning (i.e., solved without genuine multi-hop inference) or require only
simple factual recall. This limits the ability for such benchmarks to uncover
limitations of existing RAG systems. To address this gap, we present the first
pipeline for automatic, difficulty-controlled creation of
un$\underline{c}$heatable, $\underline{r}$ealistic, $\underline{u}$nanswerable,
and $\underline{m}$ulti-hop $\underline{q}$uerie$\underline{s}$ (CRUMQs),
adaptable to any corpus and domain. We use our pipeline to create CRUMQs over
two popular RAG datasets and demonstrate its effectiveness via benchmark
experiments on leading retrieval-augmented LLMs. Results show that compared to
prior RAG benchmarks, CRUMQs are highly challenging for RAG systems and achieve
up to 81.0\% reduction in cheatability scores. More broadly, our pipeline
offers a simple way to enhance benchmark difficulty and realism and drive
development of more capable RAG systems.

</details>


### [9] [Direct Multi-Token Decoding](https://arxiv.org/abs/2510.11958)
*Xuan Luo,Weizhi Wang,Xifeng Yan*

Main category: cs.CL

TL;DR: 本论文提出直接多标记解码（DMTD）技术，通过复用早中期层的隐状态加速语言模型推理，实现2倍速度提升且性能损失极小，无需额外参数或复杂操作。


<details>
  <summary>Details</summary>
Motivation: 观察到预训练LLM的不同层次具有不同职责，早期和中期层的信息可能已经足够生成多个标记，从而避免重复计算。

Method: 提出DMTD方法，利用早期和中期层已处理的隐藏状态，仅通过后期层生成多个输出标记，无需重新遍历早期和中期层；该方法无额外参数或辅助过程。

Result: 经过微调的DMTD Qwen3-4B模型在有限数据上实现了最高2倍加速，性能仅有轻微下降，并且在更大数据集上性能有望进一步提升。

Conclusion: 通过引入直接多标记解码（DMTD）推理范式，可以利用LLM中早期和中期层的隐藏状态直接生成多个标记，显著提升推理速度，同时保持较小的性能损失。

Abstract: Decoder-only transformers have become the standard architecture for large
language models (LLMs) due to their strong performance. Recent studies suggest
that, in pre-trained LLMs, early, middle, and late layers may serve distinct
roles: Early layers focus on understanding the input context, middle layers
handle task-specific processing, and late layers convert abstract
representations into output tokens. We hypothesize that once representations
have been processed by the early and middle layers, the resulting hidden states
may encapsulate sufficient information to support the generation of multiple
tokens using only the late layers, eliminating the need to repeatedly traverse
the early and middle layers. We refer to this inference paradigm as Direct
Multi-Token Decoding (DMTD). Unlike speculative decoding, our method introduces
no additional parameters, auxiliary routines, or post-generation verification.
Despite being trained on a limited dataset, a fine-tuned DMTD Qwen3-4B model
has already demonstrated promising results, achieving up to a 2x speedup with
only minor performance loss. Moreover, as shown in our scaling analysis, its
performance is expected to further improve with larger training datasets.

</details>


### [10] [Scaling Long-Horizon LLM Agent via Context-Folding](https://arxiv.org/abs/2510.11967)
*Weiwei Sun,Miao Lu,Zhan Ling,Kang Liu,Xuesong Yao,Yiming Yang,Jiecao Chen*

Main category: cs.CL

TL;DR: 本文提出Context-Folding框架及FoldGRPO强化学习方法，优化大语言模型长任务中的上下文管理，实现更高效、更优秀的任务执行表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在长时间任务中受限于上下文长度，迫切需要一种有效的上下文管理机制改善性能。

Method: 提出了Context-Folding框架，允许代理通过分支处理子任务并折叠中间步骤，结合FoldGRPO强化学习方法，通过过程奖励引导任务分解和上下文管理。

Result: 在复杂长时间任务中，Context-Folding代理在保持10倍更小的活动上下文的同时，匹配或优于ReAct基线，并显著优于基于摘要的上下文管理模型。

Conclusion: Context-Folding框架通过主动管理工作上下文，有效解决了大语言模型在长任务中的上下文长度限制，提升了任务处理效率和性能。

Abstract: Large language model (LLM) agents are fundamentally constrained by context
length on long-horizon tasks. We introduce Context-Folding, a framework that
empowers agents to actively manage their working context. An agent can
procedurally branch into a sub-trajectory to handle a subtask and then fold it
upon completion, collapsing the intermediate steps while retaining a concise
summary of the outcome. To make this behavior learnable, we develop an
end-to-end reinforcement learning framework FoldGRPO with specific process
rewards to encourage effective task decomposition and context management. On
complex long-horizon tasks (Deep Research and SWE), our folding agent matches
or outperforms the ReAct baselines while using an active context 10$\times$
smaller and significantly outperforms models that rely on summarization-based
context management.

</details>


### [11] [Conjecturing: An Overlooked Step in Formal Mathematical Reasoning](https://arxiv.org/abs/2510.11986)
*Jasivan Alex Sivakumar,Philipp Borchert,Ronald Cardenas,Gerasimos Lampouras*

Main category: cs.CL

TL;DR: 本论文指出自动形式化需先进行猜想，设计了ConjectureBench和Lean-FIRe方法，显著提升了大型语言模型的猜想和自动形式化能力，强调猜想作为独立关键步骤的重要性。


<details>
  <summary>Details</summary>
Motivation: 自动形式化通常被视为一个直接的翻译过程，忽略了关键的前置步骤——猜想。而许多数学问题无法直接形式化，必须先猜想结论。大型语言模型在自动形式化上表现不佳，且对其猜想能力的评估有限。

Method: 通过扩充数据集创建ConjectureBench，重新设计评估框架和指标，专门衡量大型语言模型的猜想能力。提出推理时方法Lean-FIRe，提升猜想和自动形式化性能。

Result: 评估表明传统方法高估了模型性能，没有考虑猜想则性能缩水。Lean-FIRe实现了GPT-4.1和DeepSeek-V3.1在PutnamBench问题上的端到端自动形式化，分别达成13和7个成功案例。

Conclusion: 猜想是自动形式化中不可忽视的独立任务，必须单独对待并正确集成以提高整体性能。未来研究应重点改进猜想步骤，提升数学形式化的自动化水平。

Abstract: Autoformalisation, the task of expressing informal mathematical statements in
formal language, is often viewed as a direct translation process. This,
however, disregards a critical preceding step: conjecturing. Many mathematical
problems cannot be formalised directly without first conjecturing a conclusion
such as an explicit answer, or a specific bound. Since Large Language Models
(LLMs) already struggle with autoformalisation, and the evaluation of their
conjecturing ability is limited and often entangled within autoformalisation or
proof, it is particularly challenging to understand its effect. To address this
gap, we augment existing datasets to create ConjectureBench, and redesign the
evaluation framework and metric specifically to measure the conjecturing
capabilities of LLMs both as a distinct task and within the autoformalisation
pipeline. Our evaluation of foundational models, including GPT-4.1 and
DeepSeek-V3.1, reveals that their autoformalisation performance is
substantially overestimated when the conjecture is accounted for during
evaluation. However, the conjecture should not be assumed to be provided. We
design an inference-time method, Lean-FIRe to improve conjecturing and
autoformalisation, which, to the best of our knowledge, achieves the first
successful end-to-end autoformalisation of 13 PutnamBench problems with GPT-4.1
and 7 with DeepSeek-V3.1. We demonstrate that while LLMs possess the requisite
knowledge to generate accurate conjectures, improving autoformalisation
performance requires treating conjecturing as an independent task, and
investigating further how to correctly integrate it within autoformalisation.
Finally, we provide forward-looking guidance to steer future research toward
improving conjecturing, an overlooked step of formal mathematical reasoning.

</details>


### [12] [SAGE: A Top-Down Bottom-Up Knowledge-Grounded User Simulator for Multi-turn AGent Evaluation](https://arxiv.org/abs/2510.11997)
*Ryan Shea,Yunan Lu,Liang Qiu,Zhou Yu*

Main category: cs.CL

TL;DR: SAGE通过结合业务知识，提升多轮对话智能体的模拟评估精度和效率，有助于找到更多错误并优化智能体表现。


<details>
  <summary>Details</summary>
Motivation: 传统多轮交互智能体评估依赖人工，成本高且效率低，现有模拟用户方法忽视领域特定的知识，难以反映真实用户行为。

Method: 提出了SAGE用户模拟框架，结合业务逻辑中的理想客户画像与业务代理基础设施（如产品目录、FAQ和知识库），模拟用户行为及信息需求，从而生成符合企业目标市场的交互。

Result: 实验证明SAGE模拟的交互更真实多样，并能发现多达33%的额外智能体错误，显示其在错误检测和智能体改进中的有效性。

Conclusion: SAGE框架通过结合自上而下的业务知识和自下而上的业务基础设施信息，能够生成更真实多样的用户交互，并显著提升多轮交互智能体评估的准确性和有效性。

Abstract: Evaluating multi-turn interactive agents is challenging due to the need for
human assessment. Evaluation with simulated users has been introduced as an
alternative, however existing approaches typically model generic users and
overlook the domain-specific principles required to capture realistic behavior.
We propose SAGE, a novel user Simulation framework for multi-turn AGent
Evaluation that integrates knowledge from business contexts. SAGE incorporates
top-down knowledge rooted in business logic, such as ideal customer profiles,
grounding user behavior in realistic customer personas. We further integrate
bottom-up knowledge taken from business agent infrastructure (e.g., product
catalogs, FAQs, and knowledge bases), allowing the simulator to generate
interactions that reflect users' information needs and expectations in a
company's target market. Through empirical evaluation, we find that this
approach produces interactions that are more realistic and diverse, while also
identifying up to 33% more agent errors, highlighting its effectiveness as an
evaluation tool to support bug-finding and iterative agent improvement.

</details>


### [13] [Generate Logical Equivalence Questions](https://arxiv.org/abs/2510.12001)
*Xinyu Wang,Haoming Yu,Yicheng Yang,Zhiyuan Li*

Main category: cs.CL

TL;DR: 论文提出了一种基于形式语言和规则的自动逻辑等价题目生成方法，解决了现有方法效率低和难度不一致的问题，经验证生成题目的质量 Comparable于教科书题目，具有实际教学应用价值。


<details>
  <summary>Details</summary>
Motivation: 针对在线教学环境下学术不诚实和抄袭问题，利用自动生成问题技术减少学生间的直接复制，提高练习题的多样性和针对性。

Method: 通过定义一种形式化语言表示逻辑等价问题，将其翻译成两组生成规则，并开发一个线性时间复杂度的算法进行问题生成。

Result: 通过两次实验验证生成的问题在准确性和难度上与教科书问题相当，且比现有基于语言模型生成方法更具效率和均匀性。

Conclusion: 该论文成功提出了一种新的自动生成逻辑等价问题的方法，能够高效且均匀地产生具有良好难度水平的问题，适用于离散数学教学。

Abstract: Academic dishonesty is met with zero tolerance in higher education, yet
plagiarism has become increasingly prevalent in the era of online teaching and
learning. Automatic Question Generation (AQG) presents a potential solution to
mitigate copying by creating unique questions for each student. Additionally,
AQG can provide a vast array of practice questions. Our AQG focuses on
generating logical equivalence questions for Discrete Mathematics, a
foundational course for first-year computer science students. A literature
review reveals that existing AQGs for this type of question generate all
propositions that meet user-defined constraints, resulting in inefficiencies
and a lack of uniform question difficulty. To address this, we propose a new
approach that defines logical equivalence questions using a formal language,
translates this language into two sets of generation rules, and develops a
linear-time algorithm for question generation. We evaluated our AQG through two
experiments. The first involved a group of students completing questions
generated by our system. Statistical analysis shows that the accuracy of these
questions is comparable to that of textbook questions. The second experiment
assessed the number of steps required to solve our generated questions,
textbook questions, and those generated by multiple large language models. The
results indicated that the difficulty of our questions was similar to that of
textbook questions, confirming the quality of our AQG.

</details>


### [14] [Information Extraction from Conversation Transcripts: Neuro-Symbolic vs. LLM](https://arxiv.org/abs/2510.12023)
*Alice Saebom Kwak,Maria Alexeeva,Gus Hahn-Powell,Keith Alcock,Kevin McLaughlin,Doug McCorkle,Gabe McNunn,Mihai Surdeanu*

Main category: cs.CL

TL;DR: 本文比较了农业领域神经-符号与大语言模型信息抽取系统，发现LLM性能更好但存在运行和控制的权衡，强调实际应用中性能与效率的平衡。


<details>
  <summary>Details</summary>
Motivation: 当前信息抽取趋势过度依赖大型语言模型，忽视了传统符号和统计IE系统的经验，本文旨在比较两类方法的优劣。

Method: 比较神经-符号IE系统和基于LLM的IE系统，在猪肉、乳制品和作物三个子领域的九个访谈数据上进行评估。

Result: LLM系统F1总分69.4优于NS系统52.7，核心信息提取F1分别为63.0和47.2。NS方法运行更快、可控性强但泛化能力差，LLM部署快、易维护但运行慢且风险存在。

Conclusion: 在农业领域的信息抽取任务中，基于大语言模型（LLM）的系统在性能上优于神经-符号系统（NS），但两者各有优缺点，需权衡性能、效率和控制。

Abstract: The current trend in information extraction (IE) is to rely extensively on
large language models, effectively discarding decades of experience in building
symbolic or statistical IE systems. This paper compares a neuro-symbolic (NS)
and an LLM-based IE system in the agricultural domain, evaluating them on nine
interviews across pork, dairy, and crop subdomains. The LLM-based system
outperforms the NS one (F1 total: 69.4 vs. 52.7; core: 63.0 vs. 47.2), where
total includes all extracted information and core focuses on essential details.
However, each system has trade-offs: the NS approach offers faster runtime,
greater control, and high accuracy in context-free tasks but lacks
generalizability, struggles with contextual nuances, and requires significant
resources to develop and maintain. The LLM-based system achieves higher
performance, faster deployment, and easier maintenance but has slower runtime,
limited control, model dependency and hallucination risks. Our findings
highlight the "hidden cost" of deploying NLP systems in real-world
applications, emphasizing the need to balance performance, efficiency, and
control.

</details>


### [15] [CPR: Mitigating Large Language Model Hallucinations with Curative Prompt Refinement](https://arxiv.org/abs/2510.12029)
*Jung-Woo Shim,Yeong-Joon Ju,Ji-Hoon Park,Seong-Whan Lee*

Main category: cs.CL

TL;DR: 本文提出CPR框架，通过精炼并补充任务描述，可有效改善提示质量，从而减少大型语言模型的幻觉问题，提高生成文本的准确性和可信度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽能流畅生成多样响应，但因用户输入的提示结构差或模糊，导致模型生成基于假设而非真实意图的错误信息（幻觉），影响信任度。

Method: 提出一种名为Curative Prompt Refinement (CPR)的框架，利用微调的小型语言模型对输入提示进行清洗和生成补充的任务描述，以更好地对齐用户的意图和提示内容。

Result: 实验结果表明，应用CPR后，提示的生成质量显著提升，幻觉减少，且在无外部知识辅助下，CPR处理的提示比原始提示的胜率超过90%。

Conclusion: 通过使用Curative Prompt Refinement (CPR)框架，可以显著提高大型语言模型生成文本的质量，减少错觉事实的出现，增强模型的可信度。

Abstract: Recent advancements in large language models (LLMs) highlight their fluency
in generating responses to diverse prompts. However, these models sometimes
generate plausible yet incorrect ``hallucinated" facts, undermining trust. A
frequent but often overlooked cause of such errors is the use of poorly
structured or vague prompts by users, leading LLMs to base responses on assumed
rather than actual intentions. To mitigate hallucinations induced by these
ill-formed prompts, we introduce Curative Prompt Refinement (CPR), a
plug-and-play framework for curative prompt refinement that 1) cleans
ill-formed prompts, and 2) generates additional informative task descriptions
to align the intention of the user and the prompt using a fine-tuned small
language model. When applied to language models, we discover that CPR
significantly increases the quality of generation while also mitigating
hallucination. Empirical studies show that prompts with CPR applied achieves
over a 90\% win rate over the original prompts without any external knowledge.

</details>


### [16] [Multi-stage Prompt Refinement for Mitigating Hallucinations in Large Language Models](https://arxiv.org/abs/2510.12032)
*Jung-Woo Shim,Yeong-Joon Ju,Ji-Hoon Park,Seong-Whan Lee*

Main category: cs.CL

TL;DR: 本文提出一种多阶段提示修正方法，通过逐步消除提示中的语言错误，显著降低大语言模型生成幻觉的概率，提升其输出质量和可靠性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生成内容时常出现幻觉问题，且对于格式不良、含糊、语法错误或信息不完整的提示，现有研究关注较少。

Method: 利用多个阶段的修正过程，针对标点符号、拼写错误及关键术语误用等问题，采用微调的小型语言模型（SLMs）逐步优化提示。

Result: 实验表明，经MPR修正的提示在幻觉测试基准中胜率超过85%，且可与其他幻觉缓解方法结合使用，提升多领域LLM的可靠性。

Conclusion: 本文提出的多阶段提示修正（MPR）框架能有效减少大语言模型（LLMs）生成幻觉的现象，提高输出的准确性。

Abstract: Recent advancements in large language models (LLMs) have shown strong
performance in natural language understanding and generation tasks. However,
LLMs continue to encounter challenges with hallucinations, where models
generate plausible but incorrect information. While several factors contribute
to hallucinations, the impact of ill-formed prompts, prompts with ambiguous
wording, incorrect grammar, or incomplete information, was relatively under
explored. To address this, we introduce Multi-stage Prompt Refinement (MPR), a
framework designed to systematically improve these ill-formed prompts across
multiple stages. Each stage addresses specific errors such as punctuation,
typographical mistakes, and misuse of key terms, using small language models
(SLMs) fine-tuned for these tasks. MPR iteratively enhances the clarity of
prompts with additional context and employs a self-reflection mechanism with
ranking to prioritize the most relevant input. Experimental results on
hallucination benchmarks show that prompts refined by MPR achieve over an 85~\%
win rate compared to their original forms, demonstrating its effectiveness in
reducing hallucinations and improving LLM output accuracy. Interestingly, we
reveal that MPR can be combined with existing post-hoc hallucination mitigation
frameworks, further enhancing its versatility. MPR provides a lightweight and
adaptable solution for enhancing LLM reliability across various domains.

</details>


### [17] [On the Interplay between Human Label Variation and Model Fairness](https://arxiv.org/abs/2510.12036)
*Kemal Kurniawan,Meladel Mistica,Timothy Baldwin,Jey Han Lau*

Main category: cs.CL

TL;DR: 本文研究了人类标签变异对模型公平性的影响，发现HLV训练方法有助于提升公平性。


<details>
  <summary>Details</summary>
Motivation: 探索未被充分研究的HLV对模型公平性的影响机制。

Method: 通过将多数投票标签的训练与多种HLV方法进行比较来研究其对模型公平性的影响。

Result: 实验表明，采用HLV训练方法在不进行显式去偏见的情况下，可以提升模型的公平性。

Conclusion: 人类标签变异（HLV）对模型公平性有积极影响，尤其在无明确去偏见措施时表现显著。

Abstract: The impact of human label variation (HLV) on model fairness is an unexplored
topic. This paper examines the interplay by comparing training on majority-vote
labels with a range of HLV methods. Our experiments show that without explicit
debiasing, HLV training methods have a positive impact on fairness.

</details>


### [18] [Uncertainty Quantification for Hallucination Detection in Large Language Models: Foundations, Methodology, and Future Directions](https://arxiv.org/abs/2510.12040)
*Sungmin Kang,Yavuz Faruk Bakman,Duygu Nur Yaldiz,Baturalp Buyukates,Salman Avestimehr*

Main category: cs.CL

TL;DR: 本文系统综述了大型语言模型中不确定性量化用于幻觉检测的方法、成效及未来方向，旨在提升模型生成的可信度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）虽然在自然语言处理领域带来突破，但其生成内容常有幻觉现象，导致事实错误，影响可靠性和可信度，因此需要不确定性量化（UQ）来提升模型生成结果的可信度。

Method: 本文首先介绍不确定性量化的基础，包括定义及传统的认识论和统计学不确定性区分，并探讨其在大型语言模型中的应用；接着系统分类多种相关方法并展示代表性实证结果。

Result: 通过对多种不确定性量化方法的分析，本文展示了UQ在检测LLM幻觉上的有效性，能够识别不可靠输出，提高模型可靠性。

Conclusion: 不确定性量化是解决大型语言模型幻觉问题的关键手段，本文总结了现有方法的优势与局限，并指出未来研究方向，有助于推动该领域发展。

Abstract: The rapid advancement of large language models (LLMs) has transformed the
landscape of natural language processing, enabling breakthroughs across a wide
range of areas including question answering, machine translation, and text
summarization. Yet, their deployment in real-world applications has raised
concerns over reliability and trustworthiness, as LLMs remain prone to
hallucinations that produce plausible but factually incorrect outputs.
Uncertainty quantification (UQ) has emerged as a central research direction to
address this issue, offering principled measures for assessing the
trustworthiness of model generations. We begin by introducing the foundations
of UQ, from its formal definition to the traditional distinction between
epistemic and aleatoric uncertainty, and then highlight how these concepts have
been adapted to the context of LLMs. Building on this, we examine the role of
UQ in hallucination detection, where quantifying uncertainty provides a
mechanism for identifying unreliable generations and improving reliability. We
systematically categorize a wide spectrum of existing methods along multiple
dimensions and present empirical results for several representative approaches.
Finally, we discuss current limitations and outline promising future research
directions, providing a clearer picture of the current landscape of LLM UQ for
hallucination detection.

</details>


### [19] [Improving Text-to-Image Generation with Input-Side Inference-Time Scaling](https://arxiv.org/abs/2510.12041)
*Ruibo Chen,Jiacheng Pan,Heng Huang,Zhenheng Yang*

Main category: cs.CL

TL;DR: 本文提出利用大型语言模型进行提示重写的框架，有效提升文本到图像生成的精度和质量，且方法具有良好扩展性和迁移性，适用于多种T2I模型。


<details>
  <summary>Details</summary>
Motivation: 当前T2I模型在处理简单或不明确提示时表现欠佳，导致生成的图像在文本对齐、美学和质量方面不理想，因此需要改进提示以提升生成效果。

Method: 设计了一个包含奖励机制和迭代直接偏好优化（DPO）训练管道的提示重写框架，该方法无需监督微调数据即可优化用户输入提示，从而增强T2I生成效果。

Result: 在多种T2I模型和基准测试中，提示重写器持续优于强基线，显著提升图文对齐、视觉质量和美学效果；且在不同T2I骨干之间显示出强迁移能力，无需重新训练。研究还表明性能提升与所用大型语言模型的容量相关。

Conclusion: 本文提出的基于大型语言模型（LLMs）的提示重写框架能够显著提升文本到图像生成系统（T2I）的图文对齐、视觉质量和美学效果，且该方法具有良好的迁移性和扩展性，是一种有效、可扩展且不依赖特定模型的策略。

Abstract: Recent advances in text-to-image (T2I) generation have achieved impressive
results, yet existing models often struggle with simple or underspecified
prompts, leading to suboptimal image-text alignment, aesthetics, and quality.
We propose a prompt rewriting framework that leverages large language models
(LLMs) to refine user inputs before feeding them into T2I backbones. Our
approach introduces a carefully designed reward system and an iterative direct
preference optimization (DPO) training pipeline, enabling the rewriter to
enhance prompts without requiring supervised fine-tuning data. We evaluate our
method across diverse T2I models and benchmarks. Results show that our prompt
rewriter consistently improves image-text alignment, visual quality, and
aesthetics, outperforming strong baselines. Furthermore, we demonstrate strong
transferability by showing that a prompt rewriter trained on one T2I backbone
generalizes effectively to others without needing to be retrained. We also
systematically study scalability, evaluating how performance gains scale with
the capacity of the large LLM used as the rewriter. These findings highlight
that prompt rewriting is an effective, scalable, and practical model-agnostic
strategy for improving T2I systems. We plan to release the code and trained
prompt rewriters soon.

</details>


### [20] [Hierarchical Alignment: Surgical Fine-Tuning via Functional Layer Specialization in Large Language Models](https://arxiv.org/abs/2510.12044)
*Yukun Zhang,Qi Dong*

Main category: cs.CL

TL;DR: 针对大型语言模型不同功能层进行分层定向优化，有效提升模型多方面性能，避免传统方法的折衷问题，展现了结构感知微调的巨大潜力。


<details>
  <summary>Details</summary>
Motivation: 现有统一优化策略忽视了Transformer架构中不同层的功能专门化，导致优化效果有限且存在性能折衷。

Method: 提出分层对齐（Hierarchical Alignment）方法，针对Transformer不同功能层（局部语法、中间逻辑、全局事实）分别应用定向的直接偏好优化（DPO），通过LoRA进行细粒度微调。

Result: 在Llama-3.1-8B和Qwen1.5-7B模型上实验证明，局部层对齐提升语法流利度，全局层对齐不仅提升事实一致性，还进一步增强逻辑连贯性，整体优于所有基线方法。

Conclusion: 分层对齐方法显著提升了大型语言模型在语法流利度、事实一致性和逻辑连贯性方面的表现，且避免了传统统一优化带来的性能折衷。

Abstract: Existing alignment techniques for Large Language Models (LLMs), such as
Direct Preference Optimization (DPO), typically treat the model as a monolithic
entity, applying uniform optimization pressure across all layers. This approach
overlooks the functional specialization within the Transformer architecture,
where different layers are known to handle distinct tasks from syntax to
abstract reasoning. In this paper, we challenge this one-size-fits-all paradigm
by introducing Hierarchical Alignment, a novel method that applies targeted DPO
to distinct functional blocks of a model's layers: local (syntax), intermediate
(logic), and global (factuality). Through a series of controlled experiments on
state-of-the-art models like Llama-3.1-8B and Qwen1.5-7B using LoRA for
surgical fine-tuning, our results, evaluated by a powerful LLM-as-Judge,
demonstrate significant and predictable improvements. Specifically, aligning
the local layers (Local-Align) enhances grammatical fluency. More importantly,
aligning the global layers (Global-Align) not only improves factual consistency
as hypothesized but also proves to be the most effective strategy for enhancing
logical coherence, outperforming all baselines. Critically, all hierarchical
strategies successfully avoid the "alignment tax" observed in standard DPO,
where gains in fluency come at the cost of degraded logical reasoning. These
findings establish a more resource-efficient, controllable, and interpretable
path for model alignment, highlighting the immense potential of shifting from
monolithic optimization to structure-aware surgical fine-tuning to build more
advanced and reliable LLMs.

</details>


### [21] [APCE: Adaptive Progressive Context Expansion for Long Context Processing](https://arxiv.org/abs/2510.12051)
*Baisub Lee,Sanghyun Byun,Mohanad Odema,Jung Guack,Jacob Song,Woo Seong Chung*

Main category: cs.CL

TL;DR: APCE通过上下文感知选择关键输入，解决了长上下文变换器内存和性能瓶颈，提升了长文摘要效果。


<details>
  <summary>Details</summary>
Motivation: 长上下文变换器模型面临内存消耗大和性能随上下文长度增大而下降（ContextRot）的问题，急需一种方法既能节省内存又能缓解性能下降。

Method: APCE基于低维语义相似度匹配机制，动态选择与当前查询最相关的输入片段，实现了上下文感知的输入筛选，减少了KV缓存和自注意力机制的内存占用。

Result: APCE在长文摘要任务中使用50%-70%的输入片段，达到或优于使用全输入的密集基线性能，同时显著提高了内存效率。

Conclusion: 本论文提出的APCE方法通过选择最重要的输入片段，有效减少了长上下文变换器模型的内存使用，并改善了ContextRot现象，提升了长文摘要任务的性能。

Abstract: Deploying useful Long-Context Transformer Models (LCTMs) requires addressing
two key challenges: (1) A growing memory footprint due to quadratic
self-attention and linear KV-cache scaling in memory as sequence length
increases; (2) the ContextRot phenomena where empirical evidence suggests that
transformer architecture's performance degrades with increasing context length.
Given the shared dependency on the input, a natural question arises: Can we
surgically select the most important input chunks for processing to
synergistically (a) reduce the memory footprint, and (b) mitigate the
ContextRot effects? In this paper, we answer this question in the affirmative
for long-context summarization tasks. We propose APCE as a context-aware
solution to select the most important input chunks through low-dimensional
semantic similarity matching with the current query. By directly operating on
the input, APCE decouples from strict dependency on underlying hardware or CUDA
environments, promising a compatible solution scalable to different deployment
systems. Our empirical evaluations have demonstrated superior or on-par
summarization performance for APCE compared to the full dense baseline using a
fraction (50%-70%) of the input sequence resulting in KV-cache and
self-attention memory efficiency improvements. We hope our findings inspire
further research on context-aware efficiency solutions for LCTMs geared towards
other relevant long-context tasks.

</details>


### [22] [An AI-Based Behavioral Health Safety Filter and Dataset for Identifying Mental Health Crises in Text-Based Conversations](https://arxiv.org/abs/2510.12083)
*Benjamin W. Nelson,Celeste Wong,Matthew T. Silvestrini,Sooyoon Shin,Alanna Robinson,Jessica Lee,Eric Yang,John Torous,Andrew Trister*

Main category: cs.CL

TL;DR: 本研究提出的VBHSF安全过滤器能有效识别精神健康危机，性能优于现有开源工具，具备医疗应用的潜力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理精神紧急情况时常常给出有害或不恰当的建议，甚至助长破坏性行为，需要对其安全性进行评估和改进。

Method: 本研究评估了Verily行为健康安全过滤器（VBHSF）在两个数据集上的表现：包含1800条模拟消息的Verily精神健康危机数据集和794条精神健康相关消息的NVIDIA Aegis AI内容安全数据子集。通过临床医生标注作为参考，比较VBHSF与OpenAI Omni Moderation最新版本及NVIDIA NeMo Guardrails的性能。

Result: VBHSF在两个数据集上均表现出高灵敏度和准确性，尤其在检测任何精神健康危机方面表现优异，灵敏度高达0.990，特异性达0.992，并且明显优于NVIDIA NeMo和OpenAI Omni Moderation最新版本，尤其在灵敏度方面表现出显著优势。

Conclusion: VBHSF展现了稳健且具广泛适应性的性能，能够优先考虑高灵敏度以减少漏检精神健康危机，适合医疗保健应用中的紧急情况检测。

Abstract: Large language models often mishandle psychiatric emergencies, offering
harmful or inappropriate advice and enabling destructive behaviors. This study
evaluated the Verily behavioral health safety filter (VBHSF) on two datasets:
the Verily Mental Health Crisis Dataset containing 1,800 simulated messages and
the NVIDIA Aegis AI Content Safety Dataset subsetted to 794 mental
health-related messages. The two datasets were clinician-labelled and we
evaluated performance using the clinician labels. Additionally, we carried out
comparative performance analyses against two open source, content moderation
guardrails: OpenAI Omni Moderation Latest and NVIDIA NeMo Guardrails. The VBHSF
demonstrated, well-balanced performance on the Verily Mental Health Crisis
Dataset v1.0, achieving high sensitivity (0.990) and specificity (0.992) in
detecting any mental health crises. It achieved an F1-score of 0.939,
sensitivity ranged from 0.917-0.992, and specificity was >= 0.978 in
identifying specific crisis categories. When evaluated against the NVIDIA Aegis
AI Content Safety Dataset 2.0, VBHSF performance remained highly sensitive
(0.982) and accuracy (0.921) with reduced specificity (0.859). When compared
with the NVIDIA NeMo and OpenAI Omni Moderation Latest guardrails, the VBHSF
demonstrated superior performance metrics across both datasets, achieving
significantly higher sensitivity in all cases (all p < 0.001) and higher
specificity relative to NVIDIA NeMo (p < 0.001), but not to OpenAI Omni
Moderation Latest (p = 0.094). NVIDIA NeMo and OpenAI Omni Moderation Latest
exhibited inconsistent performance across specific crisis types, with
sensitivity for some categories falling below 0.10. Overall, the VBHSF
demonstrated robust, generalizable performance that prioritizes sensitivity to
minimize missed crises, a crucial feature for healthcare applications.

</details>


### [23] [Deep Associations, High Creativity: A Simple yet Effective Metric for Evaluating Large Language Models](https://arxiv.org/abs/2510.12110)
*Ziliang Qiu,Renfen Hu*

Main category: cs.CL

TL;DR: 该论文提出PACE方法，通过生成平行联想链高效评估大型语言模型创造力，验证了其有效性，并比较了模型与人类的创造力差异。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型创造力难以避免数据污染且人工评估成本高，借鉴人类创造力评估方法以降低这些挑战。

Method: 提出PACE方法，让大型语言模型生成平行联想链，通过联想链的方式评估创造力。

Result: PACE与Chatbot Arena创意写作排名高度相关（Spearman's ρ=0.739，p<0.001），显示了其高效性和准确性。LLM的联想创造力约与普通人持平，但低于专业人士。语言学分析显示人类联想更具体、多样。

Conclusion: PACE方法有效评估了大型语言模型的创造力，且与人类表现存在一定差距，专业人士普遍优于模型。

Abstract: The evaluation of LLMs' creativity represents a crucial research domain,
though challenges such as data contamination and costly human assessments often
impede progress. Drawing inspiration from human creativity assessment, we
propose PACE, asking LLMs to generate Parallel Association Chains to Evaluate
their creativity. PACE minimizes the risk of data contamination and offers a
straightforward, highly efficient evaluation, as evidenced by its strong
correlation with Chatbot Arena Creative Writing rankings (Spearman's $\rho =
0.739$, $p < 0.001$) across various proprietary and open-source models. A
comparative analysis of associative creativity between LLMs and humans reveals
that while high-performing LLMs achieve scores comparable to average human
performance, professional humans consistently outperform LLMs. Furthermore,
linguistic analysis reveals that both humans and LLMs exhibit a trend of
decreasing concreteness in their associations, and humans demonstrating a
greater diversity of associative patterns.

</details>


### [24] [Tracing Multilingual Knowledge Acquisition Dynamics in Domain Adaptation: A Case Study of English-Japanese Biomedical Adaptation](https://arxiv.org/abs/2510.12115)
*Xin Zhao,Naoki Yoshinaga,Yuma Tsuta,Akiko Aizawa*

Main category: cs.CL

TL;DR: 本文提出AdaXEval评估方法，系统分析大型语言模型在多语言领域适应中的知识获取机制，揭示跨语言知识迁移的困难。


<details>
  <summary>Details</summary>
Motivation: 当前多语言领域适应中对多语言知识获取机制研究不足，导致在低资源设置下性能不佳。

Method: 提出AdaXEval适应性评估方法，构建与训练语料相同的双语多项选择问答数据集，对LLM的多语言知识获取进行持续训练和追踪分析。

Result: 通过13B英日双语LLM实验发现跨语言知识迁移仍具挑战性，验证了提出方法的有效性。

Conclusion: 跨语言领域适应中，尽管使用高质量双语语料库，跨语言知识迁移依然困难，表现存在不足。

Abstract: Multilingual domain adaptation (ML-DA) is widely used to learn new domain
knowledge across languages into large language models (LLMs). Although many
methods have been proposed to improve domain adaptation, the mechanisms of
multilingual knowledge acquisition, how domain knowledge is learned within a
language and transferred across languages, remain underexplored. This gap leads
to suboptimal performance, particularly in low-resource settings. This work
examines the learning dynamics of LLMs during ML-DA. Because prior ML-DA
studies often train and evaluate on datasets with mismatched knowledge
coverage, we propose AdaXEval, an adaptive evaluation method that builds
multiple-choice QA datasets from the same bilingual domain corpus used for
training, thereby directly studying multilingual knowledge acquisition. Through
continual training of LLMs with diverse data recipes, we track how LLMs acquire
domain facts and pinpoint the mechanism behind the transformation process from
domain training data to knowledge. Our experiments on a 13B English-Japanese
bilingual LLM reveal that cross-lingual transfer remains challenging despite a
high-quality bilingual corpus. The code has been released.

</details>


### [25] [Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models](https://arxiv.org/abs/2510.12116)
*Bajian Xiang,Shuaijiang Zhao,Tingwei Guo,Wei Zou*

Main category: cs.CL

TL;DR: 本文系统分析了大型语音语言模型中语音与文本输入表现差异的模态差距，提出了基于token级别对齐路径评分的定量方法，并通过角度和长度调整方法改善语音输入的模型性能。


<details>
  <summary>Details</summary>
Motivation: 尽管端到端的大型语音语言模型在会话生成方面表现出色，但其在语义理解任务上始终落后于传统流水线系统，特别是在语音输入性能明显低于文本输入，存在明显的模态差距，需深入理解该差距及其产生机制。

Method: 通过系统实验对LSLM在语音和文本输入上的表现差异进行分析，结合粗粒度的余弦相似度和欧氏距离以及精粒度的tokens对齐模式，提出了对齐路径评分（Alignment Path Score）来量化对齐质量，基于此设计相应的角度投影和长度归一化干预措施。

Result: 研究发现深层语音和文本表示方向趋同但幅度分离，与模态差距呈较强相关；引入的对齐路径评分能够更好反映token级别的对齐质量；通过针对关键token的角度投影和长度归一化，模型对语音输入的理解准确性得到提升。

Conclusion: 本文首次系统性地分析了语音-文本模态差距及其对大型语音语言模型（LSLM）性能的影响，揭示了深层表示在方向上趋同但在幅度上分离的现象，并提出了基于对齐路径评分的精细粒度对齐质量度量。研究表明，通过角度投影和长度归一化等策略，可以针对性地改善语音输入的正确性。

Abstract: End-to-end Large Speech Language Models (LSLMs) have demonstrated impressive
conversational generation abilities, yet consistently fall short of traditional
pipeline systems on semantic understanding benchmarks. In this work, we reveal
through systematic experimentation that although LSLMs lose some text input
performance after speech-text alignment training, the performance gap between
speech and text inputs is more pronounced, which we refer to as the modality
gap. To understand this gap, we analyze both coarse- and fine-grained text and
speech representations. At the coarse-grained level, representations of speech
and text in deeper layers are found to be increasingly aligned in direction
(cosine similarity), while concurrently diverging in magnitude (Euclidean
distance). We further find that representation similarity is strongly
correlated with the modality gap. At the fine-grained level, a spontaneous
token-level alignment pattern between text and speech representations is
observed. Based on this, we introduce the Alignment Path Score to quantify
token-level alignment quality, which exhibits stronger correlation with the
modality gap. Building on these insights, we design targeted interventions on
critical tokens through angle projection and length normalization. These
strategies demonstrate the potential to improve correctness for speech inputs.
Our study provides the first systematic empirical analysis of the modality gap
and alignment mechanisms in LSLMs, offering both theoretical and methodological
guidance for future optimization.

</details>


### [26] [SafeMT: Multi-turn Safety for Multimodal Language Models](https://arxiv.org/abs/2510.12133)
*Han Zhu,Juntao Dai,Jiaming Ji,Haoran Li,Chengkun Cai,Pengcheng Wen,Chi-Min Chan,Boyuan Chen,Yaodong Yang,Sirui Han,Yike Guo*

Main category: cs.CL

TL;DR: 针对多模态大语言模型多轮对话安全问题，提出了SafeMT基准和安全指数，并设计了有效的对话安全调节器，显著提升模型的安全防护能力。


<details>
  <summary>Details</summary>
Motivation: 现有安全基准未充分考虑多轮对话中安全风险，亟需针对多轮对话的安全评测和防护方法。

Method: 构建SafeMT基准，包含多轮对话场景和多种越狱方法，设计安全指数评价模型安全性，评估17个模型并提出对话安全调节器进行防护。

Result: SafeMT基准展示多轮对话中模型被攻击风险随轮次增加而上升，安全调节器在多个开源模型上减少攻击成功率表现优于现有方法。

Conclusion: 多模态大语言模型在多轮对话中的安全机制不足，存在较高的安全风险。提出的对话安全调节器能有效检测恶意意图，提升模型安全性。

Abstract: With the widespread use of multi-modal Large Language models (MLLMs), safety
issues have become a growing concern. Multi-turn dialogues, which are more
common in everyday interactions, pose a greater risk than single prompts;
however, existing benchmarks do not adequately consider this situation. To
encourage the community to focus on the safety issues of these models in
multi-turn dialogues, we introduce SafeMT, a benchmark that features dialogues
of varying lengths generated from harmful queries accompanied by images. This
benchmark consists of 10,000 samples in total, encompassing 17 different
scenarios and four jailbreak methods. Additionally, we propose Safety Index
(SI) to evaluate the general safety of MLLMs during conversations. We assess
the safety of 17 models using this benchmark and discover that the risk of
successful attacks on these models increases as the number of turns in harmful
dialogues rises. This observation indicates that the safety mechanisms of these
models are inadequate for recognizing the hazard in dialogue interactions. We
propose a dialogue safety moderator capable of detecting malicious intent
concealed within conversations and providing MLLMs with relevant safety
policies. Experimental results from several open-source models indicate that
this moderator is more effective in reducing multi-turn ASR compared to existed
guard models.

</details>


### [27] [Credal Transformer: A Principled Approach for Quantifying and Mitigating Hallucinations in Large Language Models](https://arxiv.org/abs/2510.12137)
*Shihao Ji,Zihui Song,Jiajie Huang*

Main category: cs.CL

TL;DR: 本文针对大语言模型幻觉问题，提出了基于证据理论的Credal Attention，实现了对模型不确定性的量化与管理，显著降低错误自信断言，提升模型可靠性。


<details>
  <summary>Details</summary>
Motivation: Transformer的Softmax机制导致生成模型过于自信，而忽略了不确定性信息，造成事实错误的自信断言（幻觉）。

Method: 引入基于证据理论的Credal Attention Mechanism(CAM)，将注意力得分重新定义为Dirichlet分布的证据质量，用“可信集合”替代单一注意力向量，从而表达不确定性。

Result: Credal Transformer能够识别分布外输入，量化模糊性，并通过放弃回答显著减少在无解问题上的过度自信错误。

Conclusion: 提出了Credal Transformer架构，有效减少语言模型的错误自信声明，提升模型对不确定性的识别能力和鲁棒性。

Abstract: Large Language Models (LLMs) hallucinate, generating factually incorrect yet
confident assertions. We argue this stems from the Transformer's Softmax
function, which creates "Artificial Certainty" by collapsing ambiguous
attention scores into a single probability distribution, discarding uncertainty
information at each layer. To fix this, we introduce the Credal Transformer,
which replaces standard attention with a Credal Attention Mechanism (CAM) based
on evidential theory. CAM produces a "credal set" (a set of distributions)
instead of a single attention vector, with the set's size directly measuring
model uncertainty. We implement this by re-conceptualizing attention scores as
evidence masses for a Dirichlet distribution: sufficient evidence recovers
standard attention, while insufficient evidence yields a diffuse distribution,
representing ambiguity. Empirically, the Credal Transformer identifies
out-of-distribution inputs, quantifies ambiguity, and significantly reduces
confident errors on unanswerable questions by abstaining. Our contribution is a
new architecture to mitigate hallucinations and a design paradigm that
integrates uncertainty quantification directly into the model, providing a
foundation for more reliable AI.

</details>


### [28] [A Survey on Parallel Reasoning](https://arxiv.org/abs/2510.12164)
*Ziqi Wang,Boye Niu,Zipeng Gao,Zhi Zheng,Tong Xu,Linghui Meng,Zhongli Li,Jing Liu,Yilong Chen,Chen Zhu,Hua Wu,Haifeng Wang,Enhong Chen*

Main category: cs.CL

TL;DR: 该论文综述了大语言模型中并行推理的概念、技术分类、应用及挑战，为提高推理鲁棒性和性能提供了全面的参考。


<details>
  <summary>Details</summary>
Motivation: 为克服传统顺序推理的脆弱性并提升实际性能，探索并行推理作为一种新兴的推理范式。

Method: 通过提出并行推理的形式化定义，并基于新的分类体系整理和讨论了相关技术，包括非交互式推理、交互式推理及注重效率的解码策略。

Result: 本文系统梳理了并行推理的技术路线、应用场景及核心难题，搭建了一个供初学者和研究者参考的研究框架。

Conclusion: 本文总结了并行推理在大语言模型中的研究进展，分析了其带来的优势和核心挑战，并提出未来发展方向。

Abstract: With the increasing capabilities of Large Language Models (LLMs), parallel
reasoning has emerged as a new inference paradigm that enhances reasoning
robustness by concurrently exploring multiple lines of thought before
converging on a final answer. It has become a significant trend to explore
parallel reasoning to overcome the fragility of standard sequential methods and
improve practical performance. In this paper, we aim to survey and summarize
the progress and challenges of parallel reasoning. We first present a formal
definition of parallel reasoning and clarify its distinction from related
concepts like Chain-of-Thought. Then, we organize and discuss advanced
techniques based on a novel taxonomy, including non-interactive reasoning,
interactive reasoning, and efficiency-focused decoding strategies.
Additionally, we explore various application scenarios, such as solving complex
problems and enhancing the reliability of LLM outputs.Finally, we highlight the
core challenges of parallel reasoning and suggest potential directions for
future research. We hope that our work can provide a useful roadmap for
beginners and encourage more research on improving parallel reasoning methods.
Related source can be avaliable in
https://github.com/PPPP-kaqiu/Awesome-Parallel-Reasoning.

</details>


### [29] [Towards Inference-time Scaling for Continuous Space Reasoning](https://arxiv.org/abs/2510.12167)
*Minghan Wang,Thuy-Trang Vu,Ehsan Shareghi,Gholamreza Haffari*

Main category: cs.CL

TL;DR: 研究发现，离散空间推理技术难以直接迁移到连续空间，连续空间推理模型需引入专门的归纳偏置以实现有效推理重排和性能提升。


<details>
  <summary>Details</summary>
Motivation: 探索已知文本推理中有效的采样加模型重排方法，是否能成功适配并提升连续空间推理模型的表现。

Method: 基于COCONUT连续空间推理语言模型，利用dropout采样生成多样推理路径，并应用过程或结果奖励模型（PRM或ORM）进行重排，分析其在连续空间的应用潜力和挑战。

Result: 通过Pass@N分析验证了多样化推理路径生成的潜力，但发现利用离散空间数据和训练方法对连续空间模型提升有限，原因在于连续空间缺少有效区分正确与错误推理所需的归纳偏置。

Conclusion: 当前用于离散空间推理的模型重排技术在连续空间推理中效果有限，主要由于连续思维表示缺乏关键的归纳偏置，使得正确与错误推理难以区分。

Abstract: Inference-time scaling through multiple sample generation in combination with
Process- or Outcome-Reward Model (PRM or ORM) re-ranking has proven effective
for text-based reasoning in large language models. This paper investigates
whether such established techniques can be successfully adapted to reasoning in
the continuous space, using COCONUT (Hao et al. 2024) continuous space
reasoning LM as the backbone. We demonstrate the feasibility of generating
diverse reasoning paths through dropout-based sampling. Our Pass@N analysis on
the generated samples reveals the potential that could enable a significant
gain in performance akin to observed gain in the discrete space. However, we
highlight unique challenges faced for materializing this gain in the continuous
thought space. In particular, working recipes for data generation and training
PRM and ORM models in the discrete space unlocks only marginal improvements in
the continuous space. Through probing various aspects including geometric
properties and trajectory dynamics we identify the underlying reasons that
prevent effective discrimination between correct and incorrect reasoning
(essential for the functioning of PRM and ORM). Our findings reveal that
current limitations stem from the absence of key inductive biases in continuous
thought representations. We argue that the training frameworks for continuous
reasoning LMs require not only to optimize for accuracy but also to explicitly
incorporate inductive biases that could be utilized during inference-time for
discrimination of correct and incorrect thoughts.\footnote{Our code and data
will be publicly available.}

</details>


### [30] [From Knowledge to Treatment: Large Language Model Assisted Biomedical Concept Representation for Drug Repurposing](https://arxiv.org/abs/2510.12181)
*Chengrui Xiang,Tengfei Ma,Xiangzheng Fu,Yiping Liu,Bosheng Song,Xiangxiang Zeng*

Main category: cs.CL

TL;DR: 针对药物重定位中知识图谱对生物医学常识缺乏建模的问题，LLaDR利用大语言模型增强图谱表达，显著提升重定位效果。


<details>
  <summary>Details</summary>
Motivation: 现有的药物重定位方法忽视了实验室中的常识性生物医学概念知识，如某些药物与特定治疗不兼容的机制性先验知识，导致表示能力不足，影响实际应用效果。

Method: 提出LLaDR框架，利用大语言模型提取富含语义的药物和治疗相关文本表示，进一步微调知识图谱嵌入模型，以注入治疗相关的知识，从而提升生物医学概念的表达。

Result: LLaDR在多个基准测试中达到了最先进的性能表现，特别是在阿尔茨海默病等案例分析中验证了其稳健性和有效性。

Conclusion: LLaDR通过结合大语言模型生成的丰富语义信息，有效提升了生物医学知识图谱中概念的表示能力，从而显著增强了药物重定位的性能，特别是在处理复杂或研究不足的疾病指示时表现优异。

Abstract: Drug repurposing plays a critical role in accelerating treatment discovery,
especially for complex and rare diseases. Biomedical knowledge graphs (KGs),
which encode rich clinical associations, have been widely adopted to support
this task. However, existing methods largely overlook common-sense biomedical
concept knowledge in real-world labs, such as mechanistic priors indicating
that certain drugs are fundamentally incompatible with specific treatments. To
address this gap, we propose LLaDR, a Large Language Model-assisted framework
for Drug Repurposing, which improves the representation of biomedical concepts
within KGs. Specifically, we extract semantically enriched treatment-related
textual representations of biomedical entities from large language models
(LLMs) and use them to fine-tune knowledge graph embedding (KGE) models. By
injecting treatment-relevant knowledge into KGE, LLaDR largely improves the
representation of biomedical concepts, enhancing semantic understanding of
under-studied or complex indications. Experiments based on benchmarks
demonstrate that LLaDR achieves state-of-the-art performance across different
scenarios, with case studies on Alzheimer's disease further confirming its
robustness and effectiveness. Code is available at
https://github.com/xiaomingaaa/LLaDR.

</details>


### [31] [Not in Sync: Unveiling Temporal Bias in Audio Chat Models](https://arxiv.org/abs/2510.12185)
*Jiayu Yao,Shenghua Liu,Yiwei Wang,Rundong Cheng,Lingrui Mei,Baolong Bi,Zhen Xiong,Xueqi Cheng*

Main category: cs.CL

TL;DR: 本文首次系统研究了大型音频语言模型在事件时间戳预测中的时间偏差问题，提出时间偏差指数（TBI）进行量化，发现其预测存在显著的系统性误差，揭示了当前模型的基础缺陷。


<details>
  <summary>Details</summary>
Motivation: 探究LALMs在音频理解和多模态推理中，定位事件具体发生时间的能力不足，揭示其时间偏差的问题。

Method: 通过控制实验和时间戳数据集，使用新提出的时间偏差指数（TBI）和可视化框架系统性评估LALMs的时间偏差。

Result: 发现时间偏差普遍存在且随音频长度增长加重，不同事件类型和位置的时间偏差表现不同，偏差可累积至数十秒。

Conclusion: 当前大型音频语言模型（LALMs）在事件时间戳预测上存在系统性偏差，难以准确定位事件发生时间。

Abstract: Large Audio Language Models (LALMs) are increasingly applied to audio
understanding and multimodal reasoning, yet their ability to locate when events
occur remains underexplored. We present the first systematic study of temporal
bias in LALMs, revealing a key limitation in their timestamp prediction. For
example, when asked "At which second does the lecturer introduce the key
formula?", models often predict timestamps that are consistently earlier or
later than the ground truth. Through controlled experiments on timestamped
datasets, we find that temporal bias (i) is prevalent across datasets and
models, (ii) increases with audio length - even accumulating to tens of seconds
in extended recordings, and (iii) varies across event types and positions. We
quantify this effect with the Temporal Bias Index (TBI), measuring systematic
misalignment in predicted event timings, and complement it with a visualization
framework. Our findings highlight a fundamental limitation in current LALMs and
call for the development of temporally robust architectures.

</details>


### [32] [DPO-Tuned Large Language Models for Segmentation in Simultaneous Speech Translation](https://arxiv.org/abs/2510.12195)
*Zeyu Yang,Satoshi Nakamura*

Main category: cs.CL

TL;DR: 本文提出的基于偏好优化训练的LLM分割框架显著提升了同步语音翻译的分割准确性和翻译质量，响应速度更快，优于现有预训练模型。


<details>
  <summary>Details</summary>
Motivation: 现有同步语音翻译分割模型受限于有监督学习的目标，缺乏与人类偏好对齐，导致分割点不够自然，不利于实时口译的要求。

Method: 提出基于大型语言模型（LLM）的分割框架，利用直接偏好优化（DPO）训练方法来实现偏好对齐，使模型能预测更自然的分割点，结合SeamlessM4T v2作为翻译主干网络进行系统评估。

Result: DPO调优后的LLM在ACL 60/60语料库三个语言对上比SHAS取得了更高的分割准确率，并在翻译质量（BLEU、COMET评测）和延迟（平均滞后）方面均有显著提升，同时在IWSLT基线测试中也表现优异。

Conclusion: 基于大语言模型（LLM）通过直接偏好优化（DPO）训练的分割框架，能够实现比现有预训练分割模型（如SHAS）更准确的分割点预测，从而提高了同步语音翻译的翻译质量和响应速度，符合实时翻译中对自然分割的需求。

Abstract: Simultaneous speech translation requires accurate segmentation to balance
translation quality and latency. Recent studies such as SHAS have introduced
pretrained segmentation models, achieving stronger performance than heuristic
rules. However, segmentation models such as SHAS, though pretrained and more
robust than heuristic methods, are still constrained by supervised learning
objectives and do not incorporate human preference alignment, which is crucial
for natural real-time interpretation. In this work, we propose a segmentation
framework based on large language models (LLMs) trained with Direct Preference
Optimization (DPO). By leveraging preference alignment, our method enables LLMs
to predict natural segmentation points that better meet the demands of
real-time translation. We evaluate the system on the ACL 60/60 corpus across
three language pairs (English-Japanese, Chinese, German), using SeamlessM4T v2
as the translation backbone. Experimental results show that our DPO-tuned LLM
achieves higher segmentation accuracy than SHAS and yields consistent
improvements in translation quality (BLEU, COMET) as well as latency (Average
Lagging). Furthermore, our system benefits from IWSLT baselines for direct
comparison. These findings highlight the potential of preference-tuned LLMs to
surpass existing pretrained segmentation models and advance adaptive,
human-aligned simultaneous interpretation.

</details>


### [33] [HALF: Harm-Aware LLM Fairness Evaluation Aligned with Deployment](https://arxiv.org/abs/2510.12217)
*Ali Mekky,Omar El Herraoui,Preslav Nakov,Yuxia Wang*

Main category: cs.CL

TL;DR: 提出HALF框架，基于现实应用和危害严重性评估大型语言模型公平性，揭示其公平性现存不足和部署前需要关注的问题。


<details>
  <summary>Details</summary>
Motivation: 当前对大型语言模型公平性和偏见的评估缺乏现实场景基础，且未考虑不同领域对偏见危害的严重程度，影响模型部署的公平性保障。

Method: 提出了HALF（Harm-Aware LLM Fairness）框架，通过一个涵盖九个应用领域、分为三个危害等级的五阶段流程，在现实场景中基于危害严重性评估模型偏见。

Result: 通过对八个大型语言模型的评估，发现模型公平性在领域间不一致，且模型表现与公平性无直接关联，推理模型在医疗支持表现最佳。

Conclusion: HALF框架揭示了大型语言模型在不同应用领域公平性表现不一致，且模型规模或性能并不能保证公平性，同时推理型模型在医疗决策支持领域表现较好，但在教育领域表现较差。

Abstract: Large language models (LLMs) are increasingly deployed across high-impact
domains, from clinical decision support and legal analysis to hiring and
education, making fairness and bias evaluation before deployment critical.
However, existing evaluations lack grounding in real-world scenarios and do not
account for differences in harm severity, e.g., a biased decision in surgery
should not be weighed the same as a stylistic bias in text summarization. To
address this gap, we introduce HALF (Harm-Aware LLM Fairness), a
deployment-aligned framework that assesses model bias in realistic applications
and weighs the outcomes by harm severity. HALF organizes nine application
domains into three tiers (Severe, Moderate, Mild) using a five-stage pipeline.
Our evaluation results across eight LLMs show that (1) LLMs are not
consistently fair across domains, (2) model size or performance do not
guarantee fairness, and (3) reasoning models perform better in medical decision
support but worse in education. We conclude that HALF exposes a clear gap
between previous benchmarking success and deployment readiness.

</details>


### [34] [Analysing Moral Bias in Finetuned LLMs through Mechanistic Interpretability](https://arxiv.org/abs/2510.12229)
*Bianca Raimondi,Daniela Dalbagno,Maurizio Gabbrielli*

Main category: cs.CL

TL;DR: 研究发现微调后的LLMs中存在道德偏见，且这些偏见集中于特定层，通过Layer-Patching技术能够定位并消除该偏见，无需重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs中道德偏见具体如何体现及其在模型组件中的定位机制。

Method: 通过对3个开源LLM进行Layer-Patching分析，识别偏见相关的关键层并通过替换激活来验证偏见来源。

Result: 通过将预训练模型相应层的激活补丁替换关键层的激活，可有效消除Knobe效应，表明偏见可局部解释和修正。

Conclusion: LLMs在微调过程中会产生类似人类的道德偏见，这些偏见不仅被学习，还集中分布在模型的特定层中。

Abstract: Large language models (LLMs) have been shown to internalize human-like biases
during finetuning, yet the mechanisms by which these biases manifest remain
unclear. In this work, we investigated whether the well-known Knobe effect, a
moral bias in intentionality judgements, emerges in finetuned LLMs and whether
it can be traced back to specific components of the model. We conducted a
Layer-Patching analysis across 3 open-weights LLMs and demonstrated that the
bias is not only learned during finetuning but also localized in a specific set
of layers. Surprisingly, we found that patching activations from the
corresponding pretrained model into just a few critical layers is sufficient to
eliminate the effect. Our findings offer new evidence that social biases in
LLMs can be interpreted, localized, and mitigated through targeted
interventions, without the need for model retraining.

</details>


### [35] [DSAS: A Universal Plug-and-Play Framework for Attention Optimization in Multi-Document Question Answering](https://arxiv.org/abs/2510.12251)
*Jiakai Li,Rongzheng Wang,Yizhuo Ma,Shuang Liang,Guangchun Luo,Ke Qin*

Main category: cs.CL

TL;DR: DSAS是一种无需修改模型结构的新方法，解决了大型语言模型多文档问答中长距离依赖和信息处理困难问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多文档问答任务中存在长距离依赖难以建模和处理中间信息丢失的缺点，现有方法要么截断依赖关系，要么依赖昂贵的微调，缺乏通用且简便的解决方案。

Method: DSAS包含两个模块：一是Contextual Gate Weighting (CGW)模块，通过层次注意力跟踪和位置感知加权缓解中间信息丢失；二是Reciprocal Attention Suppression (RAS)模块，通过抑制关键信息与无关文本间的信息交换增强对关键段落的关注。该方法无需修改模型结构或额外训练参数。

Result: DSAS在四个基准测试上对主流大型语言模型均表现出优越性，Llama-3.1-8B-Instruct和Qwen2.5-14B-Instruct的多文档问答任务平均F1分数提升4.2%。消融实验验证了CGW和RAS模块的重要贡献，附录中进一步证明了方法的鲁棒性和可扩展性。

Conclusion: 提出的Dual-Stage Adaptive Sharpening (DSAS)方法有效解决了大型语言模型在多文档问答任务中的长距离依赖建模和信息处理中间丢失问题，显著提升了性能。

Abstract: While large language models (LLMs) show considerable promise across various
fields, they have notable limitations in handling multi-document question
answering (Multi-doc QA) tasks. The first challenge is long-range dependency
modeling, where LLMs struggle to focus on key information in long texts, which
weakens important semantic connections. Second, most LLMs suffer from the
''lost-in-the-middle'' issue, where they have difficulty processing information
in the middle of long inputs. Current solutions either truncate global
dependencies or demand costly finetuning, ultimately lacking a universal and
simple solution for these challenges. To resolve these limitations, we propose
Dual-Stage Adaptive Sharpening (DSAS) containing two modules. (i) The
Contextual Gate Weighting (CGW) module alleviates ''lost-in-the-middle'' by
assessing paragraph relevance through layer-wise attention tracking and
position-aware weighting. (ii) The Reciprocal Attention Suppression (RAS)
module enhances focus on critical paragraphs by suppressing information
exchange between key and irrelevant texts, thus mitigating the limitations in
long-range dependency modeling. Notably, DSAS functions as a plug-and-play
solution requiring no architectural modifications or extra training parameters.
Extensive experiments on four benchmarks demonstrate DSAS's efficacy across
mainstream LLMs (Llama, Qwen, Mistral, and Deepseek), with an average F1-score
improvement of 4.2% in Multi-doc QA tasks on Llama-3.1-8B-Instruct and
Qwen2.5-14B-Instruct. Ablation studies confirm the essential contributions of
both the CGW and RAS modules. In addition, detailed discussions in the Appendix
further validate the robustness and scalability of DSAS.

</details>


### [36] [Shallow Robustness, Deep Vulnerabilities: Multi-Turn Evaluation of Medical LLMs](https://arxiv.org/abs/2510.12255)
*Blazej Manczak,Eric Lin,Francisco Eiras,James O' Neill,Vaikkunth Mugunthan*

Main category: cs.CL

TL;DR: 本文开发MedQA-Followup框架系统评估医疗大语言模型多轮对话鲁棒性，发现多轮多变环境下模型准确率急剧下降，间接误导影响尤甚，揭示临床应用中重要安全隐患。


<details>
  <summary>Details</summary>
Motivation: 现有评估多为单轮理想条件下，忽视了医疗问诊中复杂多变的多轮对话环境，导致模型多轮交互能力及稳定性缺乏系统理解。

Method: 提出了MedQA-Followup评估框架，通过控制对MedQA数据集的多轮对话干预，区分浅层和深层稳健性，并评估五个先进模型的表现。

Result: 模型在浅层扰动下表现良好，但多轮交互中准确率大幅下降，间接扰动比直接建议更致命，不同模型对干预的反应差异显著。

Conclusion: 医疗大语言模型在多轮对话中的可靠性存在显著漏洞，尤其是在被间接误导信息影响时表现更差，威胁其临床应用安全性。

Abstract: Large language models (LLMs) are rapidly transitioning into medical clinical
use, yet their reliability under realistic, multi-turn interactions remains
poorly understood. Existing evaluation frameworks typically assess single-turn
question answering under idealized conditions, overlooking the complexities of
medical consultations where conflicting input, misleading context, and
authority influence are common. We introduce MedQA-Followup, a framework for
systematically evaluating multi-turn robustness in medical question answering.
Our approach distinguishes between shallow robustness (resisting misleading
initial context) and deep robustness (maintaining accuracy when answers are
challenged across turns), while also introducing an indirect-direct axis that
separates contextual framing (indirect) from explicit suggestion (direct).
Using controlled interventions on the MedQA dataset, we evaluate five
state-of-the-art LLMs and find that while models perform reasonably well under
shallow perturbations, they exhibit severe vulnerabilities in multi-turn
settings, with accuracy dropping from 91.2% to as low as 13.5% for Claude
Sonnet 4. Counterintuitively, indirect, context-based interventions are often
more harmful than direct suggestions, yielding larger accuracy drops across
models and exposing a significant vulnerability for clinical deployment.
Further compounding analyses reveal model differences, with some showing
additional performance drops under repeated interventions while others
partially recovering or even improving. These findings highlight multi-turn
robustness as a critical but underexplored dimension for safe and reliable
deployment of medical LLMs.

</details>


### [37] [Chinese ModernBERT with Whole-Word Masking](https://arxiv.org/abs/2510.12285)
*Zeyu Zhao,Ningtao Wang,Xing Fu,Yu Cheng*

Main category: cs.CL

TL;DR: 为解决中文处理中的分词和形态学挑战，提出了Chinese ModernBERT，结合多项技术实现高效准确的中文编码，表现优于现有模型并支持长序列任务。


<details>
  <summary>Details</summary>
Motivation: 现有英文编码器的改进策略未能完全迁移到中文，由于中文的分词和形态学特征与英文显著不同，因此需要专门设计适应中文特点的模型和训练方法。

Method: 提出了结合硬件感知32k BPE词汇表、全词掩码动态课程、两阶段预训练流程（扩展上下文至8192个标记，采用RoPE和交替局部/全局注意力）、阻尼余弦学习率调度等多项技术的全新中文编码器。

Result: 模型在CLUE基准测试中表现优异，具有高效的长短序列处理能力；在SimCLUE检索任务上，通过加入对比学习数据进一步提升性能，超过了同类模型Qwen-0.6B-embedding。

Conclusion: Chinese ModernBERT在中文语言处理任务中表现出竞争力，尤其在长序列处理和检索任务上优于现有模型，展示了其设计和训练策略的有效性。

Abstract: Encoder-only Transformers have advanced along three axes -- architecture,
data, and systems -- yielding Pareto gains in accuracy, speed, and memory
efficiency. Yet these improvements have not fully transferred to Chinese, where
tokenization and morphology differ markedly from English. We introduce Chinese
ModernBERT, a from-scratch Chinese encoder that couples: (i) a hardware-aware
32k BPE vocabulary tailored to frequent Chinese affixes/compounds, lowering the
embedding budget; (ii) whole-word masking (WWM) with a dynamic masking
curriculum (30% -> 15%) to align task difficulty with training progress; (iii)
a two-stage pre-training pipeline that extends the native context from 1,024 to
8,192 tokens using RoPE and alternating local/global attention; and (iv) a
damped-cosine learning-rate schedule for stable long-horizon optimization. We
pre-train on ~1.2T Chinese tokens from CCI3-HQ, CCI4 (Chinese), and
Cosmopedia-Chinese. On CLUE, Chinese ModernBERT is competitive with strong
Chinese encoders under a unified fine-tuning protocol. Under bf16 it achieves
high long-sequence throughput while maintaining strong short-sequence speed,
reflecting benefits from budget allocation and attention design. To probe
retrieval-oriented quality, we add a small amount of open contrastive data:
fine-tuning on SimCLUE (~3M pairs) improves further when adding T2Ranking
(~2M), reaching 0.505 (Pearson) / 0.537 (Spearman) on the SimCLUE test set.
Under this open-data setting, Chinese ModernBERT surpasses Qwen-0.6B-embedding
on SimCLUE, suggesting a clear scaling path for STS with additional curated
pairs. We will release tokenizer and weights to facilitate reproducible
research.

</details>


### [38] [A large-scale, unsupervised pipeline for automatic corpus annotation using LLMs: variation and change in the English consider construction](https://arxiv.org/abs/2510.12306)
*Cameron Morin,Matti Marttinen Larsson*

Main category: cs.CL

TL;DR: 本文提出了基于大型语言模型的无监督自动语法标注管道，显著提高了大规模语料的自动标注效率和准确性，促进了语料库语言学的研究。


<details>
  <summary>Details</summary>
Motivation: 手动注释语料库工作繁重且难以扩展，面对快速增长的自然语言语料库，需要一种可扩展且高效的自动化标注方法。

Method: 提出一个包含提示工程、前置评估、自动批量处理和后置验证四个阶段的无监督自动化管道，利用GPT-5通过OpenAI API对语料进行标注。

Result: 在历史美国英语语料库中用该管道标注了143,933句，耗时不到60小时，标注准确率超过98%，验证了方法的有效性和可行性。

Conclusion: 大型语言模型（LLMs）能够在大规模语料库中高效、准确地自动化语法标注，显著减少人工注释的工作量，推动语料库语言学研究的发展。

Abstract: As natural language corpora expand at an unprecedented rate, manual
annotation remains a significant methodological bottleneck in corpus linguistic
work. We address this challenge by presenting a scalable, unsupervised pipeline
for automating grammatical annotation in voluminous corpora using large
language models (LLMs). Unlike previous supervised and iterative approaches,
our method employs a four-phase workflow: prompt engineering, pre-hoc
evaluation, automated batch processing, and post-hoc validation. We demonstrate
the pipeline's accessibility and effectiveness through a diachronic case study
of variation in the English consider construction. Using GPT-5 through the
OpenAI API, we annotate 143,933 sentences from the Corpus of Historical
American English (COHA) in under 60 hours, achieving 98%+ accuracy on two
sophisticated annotation procedures. Our results suggest that LLMs can perform
a range of data preparation tasks at scale with minimal human intervention,
opening new possibilities for corpus-based research, though implementation
requires attention to costs, licensing, and other ethical considerations.

</details>


### [39] [Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation](https://arxiv.org/abs/2510.12316)
*Greta Damo,Elena Cabrio,Serena Villata*

Main category: cs.CL

TL;DR: 本文提出了基于知识库和检索增强生成的反驳言论生成框架，显著提升针对8大敏感群体的反驳文本质量和可信度，优于现有大型语言模型方案。


<details>
  <summary>Details</summary>
Motivation: 现有反驳言论生成多依赖大型语言模型或专家，存在可信度、连贯性不足及可扩展性差的问题，需引入知识驱动方法提高质量。

Method: 通过整合先进的检索增强生成（RAG）流水线，结合联合国数字图书馆、EUR-Lex及欧盟基本权利机构构建的32792条文本知识库，针对8个主要受害群体生成反驳言论。使用MultiTarget-CONAN数据集进行标准指标和人工评估。

Result: 提出的框架在JudgeLM标准指标和人工评估中均优于基线模型和竞争方法，显著提升了反驳言论的质量和可信度。

Conclusion: 该论文提出的基于知识增强的生成框架在生成可信、连贯的反驳言论方面优于传统的大型语言模型和NGO专家方法，验证了其有效性和可扩展性。

Abstract: Counter-speech generation is at the core of many expert activities, such as
fact-checking and hate speech, to counter harmful content. Yet, existing work
treats counter-speech generation as pure text generation task, mainly based on
Large Language Models or NGO experts. These approaches show severe drawbacks
due to the limited reliability and coherence in the generated countering text,
and in scalability, respectively. To close this gap, we introduce a novel
framework to model counter-speech generation as knowledge-wise text generation
process. Our framework integrates advanced Retrieval-Augmented Generation (RAG)
pipelines to ensure the generation of trustworthy counter-speech for 8 main
target groups identified in the hate speech literature, including women, people
of colour, persons with disabilities, migrants, Muslims, Jews, LGBT persons,
and other. We built a knowledge base over the United Nations Digital Library,
EUR-Lex and the EU Agency for Fundamental Rights, comprising a total of 32,792
texts. We use the MultiTarget-CONAN dataset to empirically assess the quality
of the generated counter-speech, both through standard metrics (i.e., JudgeLM)
and a human evaluation. Results show that our framework outperforms standard
LLM baselines and competitive approach, on both assessments. The resulting
framework and the knowledge base pave the way for studying trustworthy and
sound counter-speech generation, in hate speech and beyond.

</details>


### [40] [Fine-grained Analysis of Brain-LLM Alignment through Input Attribution](https://arxiv.org/abs/2510.12355)
*Michela Proietti,Roberto Capobianco,Mariya Toneva*

Main category: cs.CL

TL;DR: 本文提出细粒度输入归因方法，揭示脑活动与大语言模型对齐主要依赖语义信息，与下一词预测偏好语法信息不同，深化了对脑-语言模型关系的理解。


<details>
  <summary>Details</summary>
Motivation: 探索脑活动与大语言模型对齐的具体机制，特别是脑对齐与下一词预测之间的联系与区别。

Method: 引入细粒度输入归因方法，识别对脑-LLM对齐最重要的具体词汇，并分析脑对齐与下一词预测的关系。

Result: 发现脑对齐和下一词预测依赖的词汇不同，脑对齐更关注语义和话语信息且有针对性的近期效应，下一词预测表现出语法主导的首因和近因偏差。

Conclusion: 脑活动与大语言模型（LLM）在语言处理中的对齐依赖于不同的词汇子集，脑对齐侧重于语义和话语层面信息，而下一词预测偏重语法信息。

Abstract: Understanding the alignment between large language models (LLMs) and human
brain activity can reveal computational principles underlying language
processing. We introduce a fine-grained input attribution method to identify
the specific words most important for brain-LLM alignment, and leverage it to
study a contentious research question about brain-LLM alignment: the
relationship between brain alignment (BA) and next-word prediction (NWP). Our
findings reveal that BA and NWP rely on largely distinct word subsets: NWP
exhibits recency and primacy biases with a focus on syntax, while BA
prioritizes semantic and discourse-level information with a more targeted
recency effect. This work advances our understanding of how LLMs relate to
human language processing and highlights differences in feature reliance
between BA and NWP. Beyond this study, our attribution method can be broadly
applied to explore the cognitive relevance of model predictions in diverse
language processing tasks.

</details>


### [41] [MoBiLE: Efficient Mixture-of-Experts Inference on Consumer GPU with Mixture of Big Little Experts](https://arxiv.org/abs/2510.12357)
*Yushu Zhao,Yubin Qin,Yang Wang,Xiaolong Yang,Huiming Han,Shaojun Wei,Yang Hu,Shouyi Yin*

Main category: cs.CL

TL;DR: 本文提出了MoBiLE框架，通过减少不重要token使用的专家数量并智能切换，提升了MoE模型推理速度，且保持准确率。


<details>
  <summary>Details</summary>
Motivation: MoE模型在稀疏激活下通过将活跃专家存放于GPU存储，不活跃专家存放于CPU内存，实现推理加速，但受限于CPU-GPU带宽瓶颈。现有预取方法训练开销大且在细粒度专家分割模型上效果下降。

Method: 提出MoBiLE框架，采用大-小专家混合策略，对不重要的Token使用较少的专家提升速度，对重要Token保持全专家以保证质量；设计了大-小专家切换的回退和预取机制优化内存效率。

Result: 在四种现代MoE架构和生成任务上，MoBiLE相比基线实现1.60-1.72倍速度提升，同时精度几乎无损耗。

Conclusion: MoBiLE通过大-小专家混合和智能预取机制有效缓解了CPU-GPU带宽瓶颈，在保证模型质量的同时显著提升了MoE推理速度。

Abstract: Mixture-of-Experts (MoE) models have recently demonstrated exceptional
performance across a diverse range of applications. The principle of sparse
activation in MoE models facilitates an offloading strategy, wherein active
experts are maintained in GPU HBM, while inactive experts are stored in CPU
DRAM. The efficacy of this approach, however, is fundamentally constrained by
the limited bandwidth of the CPU-GPU interconnect. To mitigate this bottleneck,
existing approaches have employed prefetching to accelerate MoE inference.
These methods attempt to predict and prefetch the required experts using
specially trained modules. Nevertheless, such techniques are often encumbered
by significant training overhead and have shown diminished effectiveness on
recent MoE models with fine-grained expert segmentation.
  In this paper, we propose MoBiLE, a plug-and-play offloading-based MoE
inference framework with \textit{mixture of big-little experts}. It reduces the
number of experts for unimportant tokens to half for acceleration while
maintaining full experts for important tokens to guarantee model quality.
Further, a dedicated fallback and prefetching mechanism is designed for
switching between little and big experts to improve memory efficiency. We
evaluate MoBiLE on four typical modern MoE architectures and challenging
generative tasks. Our results show that MoBiLE achieves a speedup of 1.60x to
1.72x compared to the baseline on a consumer GPU system, with negligible
degradation in accuracy.

</details>


### [42] [LLM-REVal: Can We Trust LLM Reviewers Yet?](https://arxiv.org/abs/2510.12367)
*Rui Li,Jia-Chen Gu,Po-Nien Kung,Heming Xia,Junfeng liu,Xiangwen Kong,Zhifang Sui,Nanyun Peng*

Main category: cs.CL

TL;DR: 本文通过模拟和人工注释发现LLM审稿存在偏见，可能损害学术公平，但LLM指导修订有助于提升论文质量，提示应谨慎使用LLM审稿。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM深度融入学术写作与同行评审流程后带来的潜在风险，特别是对学术公平性的影响。

Method: 通过构建研究代理生成论文和审稿代理进行审稿的模拟系统，结合人工注释和分析。

Result: 发现LLM审稿人为LLM生成论文评分偏高，对含批判性内容的人类论文评分偏低，存在显著偏见。同时LLM审稿指导的论文修订能提升论文质量。

Conclusion: LLM作为审稿人存在语言风格偏好和对批判性陈述的排斥，导致对LLM生成的论文评分过高，对人类论文评分偏低，影响学术公平性。

Abstract: The rapid advancement of large language models (LLMs) has inspired
researchers to integrate them extensively into the academic workflow,
potentially reshaping how research is practiced and reviewed. While previous
studies highlight the potential of LLMs in supporting research and peer review,
their dual roles in the academic workflow and the complex interplay between
research and review bring new risks that remain largely underexplored. In this
study, we focus on how the deep integration of LLMs into both peer-review and
research processes may influence scholarly fairness, examining the potential
risks of using LLMs as reviewers by simulation. This simulation incorporates a
research agent, which generates papers and revises, alongside a review agent,
which assesses the submissions. Based on the simulation results, we conduct
human annotations and identify pronounced misalignment between LLM-based
reviews and human judgments: (1) LLM reviewers systematically inflate scores
for LLM-authored papers, assigning them markedly higher scores than
human-authored ones; (2) LLM reviewers persistently underrate human-authored
papers with critical statements (e.g., risk, fairness), even after multiple
revisions. Our analysis reveals that these stem from two primary biases in LLM
reviewers: a linguistic feature bias favoring LLM-generated writing styles, and
an aversion toward critical statements. These results highlight the risks and
equity concerns posed to human authors and academic research if LLMs are
deployed in the peer review cycle without adequate caution. On the other hand,
revisions guided by LLM reviews yield quality gains in both LLM-based and human
evaluations, illustrating the potential of the LLMs-as-reviewers for
early-stage researchers and enhancing low-quality papers.

</details>


### [43] [Tokenization Disparities as Infrastructure Bias: How Subword Systems Create Inequities in LLM Access and Efficiency](https://arxiv.org/abs/2510.12389)
*Hailay Kidu Teklehaymanot,Wolfgang Nejdl*

Main category: cs.CL

TL;DR: 这项研究系统评估了200多种语言中大模型的分词效率，揭示了非拉丁文字和形态复杂语言在计算成本上的不公平，呼吁开发面向语言多样性的分词策略以实现更公平的多语言AI系统。


<details>
  <summary>Details</summary>
Motivation: 分词差异成为限制不同语言群体公平获得人工智能资源的主要障碍，尤其是在多语言大模型中，存在潜在的计算资源不公平分配问题。

Method: 采用标准化实验框架，统一预处理和归一化流程，使用tiktoken库对200多种语言样本进行统一分词，并通过Tokens Per Sentence (TPS)和Relative Tokenization Cost (RTC)等指标进行评价，基于英语作为基准进行跨语言比较。

Result: 发现拉丁文字语言的分词效率明显更高，而非拉丁文字和形态复杂语言的分词导致代价显著增加，RTC比率高出3-5倍，造成少数及低资源语言计算成本加大和上下文利用受限。

Conclusion: 当前大语言模型在不同语言的分词效率上存在显著的不平等现象，尤其是非拉丁文字和形态复杂的语言分词成本更高，导致计算资源消耗增加和上下文利用率降低，体现出结构性计算不公平。

Abstract: Tokenization disparities pose a significant barrier to achieving equitable
access to artificial intelligence across linguistically diverse populations.
This study conducts a large-scale cross-linguistic evaluation of tokenization
efficiency in over 200 languages to systematically quantify computational
inequities in large language models (LLMs). Using a standardized experimental
framework, we applied consistent preprocessing and normalization protocols,
followed by uniform tokenization through the tiktoken library across all
language samples. Comprehensive tokenization statistics were collected using
established evaluation metrics, including Tokens Per Sentence (TPS) and
Relative Tokenization Cost (RTC), benchmarked against English baselines. Our
cross-linguistic analysis reveals substantial and systematic disparities:
Latin-script languages consistently exhibit higher tokenization efficiency,
while non-Latin and morphologically complex languages incur significantly
greater token inflation, often 3-5 times higher RTC ratios. These
inefficiencies translate into increased computational costs and reduced
effective context utilization for underrepresented languages. Overall, the
findings highlight structural inequities in current AI systems, where speakers
of low-resource and non-Latin languages face disproportionate computational
disadvantages. Future research should prioritize the development of
linguistically informed tokenization strategies and adaptive vocabulary
construction methods that incorporate typological diversity, ensuring more
inclusive and computationally equitable multilingual AI systems.

</details>


### [44] [PRoH: Dynamic Planning and Reasoning over Knowledge Hypergraphs for Retrieval-Augmented Generation](https://arxiv.org/abs/2510.12434)
*Xiangjun Zai,Xingyu Tan,Xiaoyang Wang,Qing Liu,Xiwei Xu,Wenjie Zhang*

Main category: cs.CL

TL;DR: 本文提出动态规划与推理框架PRoH，有效提升了知识超图检索增强生成的多跳问答性能，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于KH的RAG方法存在静态检索规划、缺乏自适应执行以及浅层利用知识超图结构和语义，限制了其在多跳问答中的效果。

Method: 提出PRoH框架，包含（i）上下文感知的规划模块用于引导结构化推理计划生成；（ii）将问题分解为动态演化的有向无环图进行自适应多路径探索；（iii）基于实体加权重叠的推理路径检索算法用于优先选择语义连贯的超边遍历。

Result: PRoH在多个领域的多跳问答任务中取得最先进表现，F1提升平均19.73%，生成评价分数提升8.41%，表现出对长距离多跳推理的强鲁棒性。

Conclusion: PRoH框架通过动态规划和推理有效克服了现有基于知识超图（KH）的检索增强生成方法在静态检索规划、非自适应检索执行及浅层利用结构语义方面的不足，实现了多跳问答的显著性能提升。

Abstract: Knowledge Hypergraphs (KHs) have recently emerged as a knowledge
representation for retrieval-augmented generation (RAG), offering a paradigm to
model multi-entity relations into a structured form. However, existing KH-based
RAG methods suffer from three major limitations: static retrieval planning,
non-adaptive retrieval execution, and superficial use of KH structure and
semantics, which constrain their ability to perform effective multi-hop
question answering. To overcome these limitations, we propose PRoH, a dynamic
Planning and Reasoning over Knowledge Hypergraphs framework. PRoH incorporates
three core innovations: (i) a context-aware planning module that sketches the
local KH neighborhood to guide structurally grounded reasoning plan generation;
(ii) a structured question decomposition process that organizes subquestions as
a dynamically evolving Directed Acyclic Graph (DAG) to enable adaptive,
multi-trajectory exploration; and (iii) an Entity-Weighted Overlap (EWO)-guided
reasoning path retrieval algorithm that prioritizes semantically coherent
hyperedge traversals. Experiments across multiple domains demonstrate that PRoH
achieves state-of-the-art performance, surpassing the prior SOTA model
HyperGraphRAG by an average of 19.73% in F1 and 8.41% in Generation Evaluation
(G-E) score, while maintaining strong robustness in long-range multi-hop
reasoning tasks.

</details>


### [45] [Probing Latent Knowledge Conflict for Faithful Retrieval-Augmented Generation](https://arxiv.org/abs/2510.12460)
*Linfeng Gao,Baolong Bi,Zheng Yuan,Le Wang,Zerui Chen,Zhimin Wei,Shenghua Liu,Qinggang Zhang,Jinsong Su*

Main category: cs.CL

TL;DR: 本研究揭示了大型语言模型内部整合检索证据的机制，并提出CLEAR框架，通过冲突定位和冲突感知微调，显著提升了RAG系统的事实准确性和上下文忠实性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统在生成文本时存在不忠实问题，且多数方法依赖外部干预，忽视了大型语言模型内部如何整合检索证据与参数化记忆的关键机制，特别是在知识冲突情境下的表现。

Method: 通过对大型语言模型隐藏状态表示的探测分析，发现知识整合的层次性和冲突的潜在信号，并设计CLEAR框架，包括句子级知识分解、冲突定位的隐藏状态探测及冲突感知微调。

Result: 实验结果表明，CLEAR在三个基准测试上显著优于强基线，提升了生成文本的准确性和上下文忠实性，且在多种知识冲突条件下表现稳健。

Conclusion: 该论文提出的CLEAR框架有效解决了RAG系统中模型响应与检索上下文证据矛盾的问题，显著提升了生成文本的准确性和上下文可信度。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm to
enhance the factuality of Large Language Models (LLMs). However, existing RAG
systems often suffer from an unfaithfulness issue, where the model's response
contradicts evidence from the retrieved context. Existing approaches to
improving contextual faithfulness largely rely on external interventions, such
as prompt engineering, decoding constraints, or reward-based fine-tuning. These
works treat the LLM as a black box and overlook a crucial question: how does
the LLM internally integrate retrieved evidence with its parametric memory,
particularly under knowledge conflicts? To address this gap, we conduct a
probing-based analysis of hidden-state representations in LLMs and observe
three findings: knowledge integration occurs hierarchically, conflicts manifest
as latent signals at the sentence level, and irrelevant context is often
amplified when aligned with parametric knowledge. Building on these findings,
we propose CLEAR (Conflict-Localized and Enhanced Attention for RAG), a
framework that (i) decomposes context into fine-grained sentence-level
knowledge, (ii) employs hidden-state probing to localize conflicting knowledge,
and (iii) introduces conflict-aware fine-tuning to guide the model to
accurately integrate retrieved evidence. Extensive experiments across three
benchmarks demonstrate that CLEAR substantially improves both accuracy and
contextual faithfulness, consistently outperforming strong baselines under
diverse conflict conditions. The related resources are available at
https://github.com/LinfengGao/CLEAR.

</details>


### [46] [Resource-sensitive but language-blind: Community size and not grammatical complexity better predicts the accuracy of Large Language Models in a novel Wug Test](https://arxiv.org/abs/2510.12463)
*Nikoleta Pantelidou,Evelina Leivada,Paolo Morosi*

Main category: cs.CL

TL;DR: 大型语言模型的形态学泛化表现主要依赖语言资源丰富度，虽表现出类似人类的准确率，但更反映数据驱动的性能而非真正的语言复杂性理解。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型在涉及新词形态学泛化任务中的表现，特别是评估其准确性是否接近人类能力，以及模型表现是受语言复杂性还是训练数据量的影响。

Method: 使用改编的多语言Wug测试，对六种大型语言模型在加泰罗尼亚语、英语、希腊语和西班牙语四种语言中进行形态学泛化测试，并与人类表现作比较。

Result: 模型能够以接近人类的准确率用于未见单词的形态学泛化，但其准确率更多与语言社群规模和数字资源量相关，例如西班牙语和英语表现优于资源较少的加泰罗尼亚语和希腊语。

Conclusion: 模型的形态学泛化能力主要受语言资源丰富度影响，而非语言结构复杂性，表现出的是与人类语言能力表面相似的性能。

Abstract: The linguistic abilities of Large Language Models are a matter of ongoing
debate. This study contributes to this discussion by investigating model
performance in a morphological generalization task that involves novel words.
Using a multilingual adaptation of the Wug Test, six models were tested across
four partially unrelated languages (Catalan, English, Greek, and Spanish) and
compared with human speakers. The aim is to determine whether model accuracy
approximates human competence and whether it is shaped primarily by linguistic
complexity or by the quantity of available training data. Consistent with
previous research, the results show that the models are able to generalize
morphological processes to unseen words with human-like accuracy. However,
accuracy patterns align more closely with community size and data availability
than with structural complexity, refining earlier claims in the literature. In
particular, languages with larger speaker communities and stronger digital
representation, such as Spanish and English, revealed higher accuracy than
less-resourced ones like Catalan and Greek. Overall, our findings suggest that
model behavior is mainly driven by the richness of linguistic resources rather
than by sensitivity to grammatical complexity, reflecting a form of performance
that resembles human linguistic competence only superficially.

</details>


### [47] [SMEC: Rethinking Matryoshka Representation Learning for Retrieval Embedding Compression](https://arxiv.org/abs/2510.12474)
*Biao Zhang,Lixin Chen,Tong Liu,Bo Zheng*

Main category: cs.CL

TL;DR: 提出SMEC框架实现高维嵌入的有效降维，减小计算和存储开销，同时提升压缩嵌入性能。


<details>
  <summary>Details</summary>
Motivation: 解决高维嵌入带来的计算复杂度和存储负担，促进高维语言模型嵌入的实际部署。

Method: 设计了Sequential Matryoshka Representation Learning减少梯度方差，Adaptive Dimension Selection减少维度裁剪的信息损失，Selectable Cross-batch Memory增强不同维度嵌入间的无监督学习。

Result: 在图像、文本和多模态数据集上表现优异，在BEIR数据集上压缩至256维的LLM2Vec嵌入性能分别较基线模型提升1.1和2.7分。

Conclusion: 提出的SMEC框架在大幅降维的同时保持了性能，提升了压缩后嵌入的效果。

Abstract: Large language models (LLMs) generate high-dimensional embeddings that
capture rich semantic and syntactic information. However, high-dimensional
embeddings exacerbate computational complexity and storage requirements,
thereby hindering practical deployment. To address these challenges, we propose
a novel training framework named Sequential Matryoshka Embedding Compression
(SMEC). This framework introduces the Sequential Matryoshka Representation
Learning(SMRL) method to mitigate gradient variance during training, the
Adaptive Dimension Selection (ADS) module to reduce information degradation
during dimension pruning, and the Selectable Cross-batch Memory (S-XBM) module
to enhance unsupervised learning between high- and low-dimensional embeddings.
Experiments on image, text, and multimodal datasets demonstrate that SMEC
achieves significant dimensionality reduction while maintaining performance.
For instance, on the BEIR dataset, our approach improves the performance of
compressed LLM2Vec embeddings (256 dimensions) by 1.1 points and 2.7 points
compared to the Matryoshka-Adaptor and Search-Adaptor models, respectively.

</details>


### [48] [When Personalization Tricks Detectors: The Feature-Inversion Trap in Machine-Generated Text Detection](https://arxiv.org/abs/2510.12476)
*Lang Gao,Xuhui Li,Chenxi Wang,Mingzhe Li,Wei Liu,Zirui Song,Jinghui Zhang,Rui Yan,Preslav Nakov,Xiuying Chen*

Main category: cs.CL

TL;DR: 本文首次关注个性化机器生成文本检测，揭示了特征反转陷阱导致的性能下降，并提出预测检测器性能变化的有效方法\method。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型生成能力增强，个人风格的仿写使身份冒充风险加大，但个性化机器生成文本检测尚未被充分研究。

Method: 提出了\method方法，通过识别特征反转对应的潜在方向，构建探测器依赖性探测数据集，从而预测鉴别器性能的变化。

Result: 构建了第一个个性化检测基准\dataset，实验显示不同检测器性能差异显著；\method方法能准确预测鉴别器性能变化，相关性达85%。

Conclusion: 现有文本鉴别器在个性化机器生成文本检测中表现不足，主要原因是特征反转陷阱，即一般领域中有辨别力的特征在个性化文本中变得误导性强。

Abstract: Large language models (LLMs) have grown more powerful in language generation,
producing fluent text and even imitating personal style. Yet, this ability also
heightens the risk of identity impersonation. To the best of our knowledge, no
prior work has examined personalized machine-generated text (MGT) detection. In
this paper, we introduce \dataset, the first benchmark for evaluating detector
robustness in personalized settings, built from literary and blog texts paired
with their LLM-generated imitations. Our experimental results demonstrate large
performance gaps across detectors in personalized settings: some
state-of-the-art models suffer significant drops. We attribute this limitation
to the \textit{feature-inversion trap}, where features that are discriminative
in general domains become inverted and misleading when applied to personalized
text. Based on this finding, we propose \method, a simple and reliable way to
predict detector performance changes in personalized settings. \method
identifies latent directions corresponding to inverted features and constructs
probe datasets that differ primarily along these features to evaluate detector
dependence. Our experiments show that \method can accurately predict both the
direction and the magnitude of post-transfer changes, showing 85\% correlation
with the actual performance gaps. We hope that this work will encourage further
research on personalized text detection.

</details>


### [49] [BoN Appetit Team at LeWiDi-2025: Best-of-N Test-time Scaling Can Not Stomach Annotation Disagreements (Yet)](https://arxiv.org/abs/2510.12516)
*Tomas Ruiz,Siyao Peng,Barbara Plank,Carsten Schwemmer*

Main category: cs.CL

TL;DR: 本研究将测试时缩放技术应用于LeWiDi-2025任务，发现模型平均和投票法有效提升了LLM性能，但Best-of-N采样方法效果有限，分析了差异原因。


<details>
  <summary>Details</summary>
Motivation: 将测试时缩放技术从数学和编码等有明确正确答案的领域，转移到注释意见不一致的LeWiDi任务中，评估其效果。

Method: 采用三种测试时缩放方法：模型平均、投票法和Best-of-N采样方法进行实验。

Result: 两种基准方法（模型平均和投票法）在LeWiDi任务中表现出稳定的性能提升，而Best-of-N采样方法未能取得理想效果。

Conclusion: 测试时缩放技术在LeWiDi-2025任务中可以提高LLM性能，但Best-of-N采样方法未能有效改进。

Abstract: Test-time scaling is a family of techniques to improve LLM outputs at
inference time by performing extra computation. To the best of our knowledge,
test-time scaling has been limited to domains with verifiably correct answers,
like mathematics and coding. We transfer test-time scaling to the LeWiDi-2025
tasks to evaluate annotation disagreements. We experiment with three test-time
scaling methods: two benchmark algorithms (Model Averaging and Majority
Voting), and a Best-of-N sampling method. The two benchmark methods improve LLM
performance consistently on the LeWiDi tasks, but the Best-of-N method does
not. Our experiments suggest that the Best-of-N method does not currently
transfer from mathematics to LeWiDi tasks, and we analyze potential reasons for
this gap.

</details>


### [50] [VISaGE: Understanding Visual Generics and Exceptions](https://arxiv.org/abs/2510.12548)
*Stella Frank,Emily Allaway*

Main category: cs.CL

TL;DR: 本文通过构建非典型图像数据集，揭示了视觉语言模型在图文不一致情况下，概念理解能力的下降，反映了模型对语境一致性先验的依赖大于语义先验。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型主要关注个体实例分析，但在遇到非典型实例时，模型内在的语境先验和语义先验之间产生矛盾，亟需研究模型如何权衡这两种先验。

Method: 提出了一个新的评估数据集VISaGE，包含典型和非典型图像，通过平衡实验测试VLMs在不同图像类型下对概念理解的表现差异。

Result: 观察到在图文不一致的非典型图像条件下，VLMs的概念理解能力显著退化，且这种影响超过了语义先验对单个实例查询的影响。

Conclusion: 在典型与非典型图像的测试中，视觉语言模型（VLMs）在面对非典型输入时，模型的表现因语境先验（文本与视觉输入一致性）违背而显著下降，表明模型对语境一致性的依赖强于对语义先验的依赖。

Abstract: While Vision Language Models (VLMs) learn conceptual representations, in the
form of generalized knowledge, during training, they are typically used to
analyze individual instances. When evaluation instances are atypical, this
paradigm results in tension between two priors in the model. The first is a
pragmatic prior that the textual and visual input are both relevant, arising
from VLM finetuning on congruent inputs; the second is a semantic prior that
the conceptual representation is generally true for instances of the category.
In order to understand how VLMs trade off these priors, we introduce a new
evaluation dataset, VISaGE, consisting of both typical and exceptional images.
In carefully balanced experiments, we show that conceptual understanding
degrades when the assumption of congruency underlying the pragmatic prior is
violated with incongruent images. This effect is stronger than the effect of
the semantic prior when querying about individual instances.

</details>


### [51] [Teaching Language Models to Faithfully Express their Uncertainty](https://arxiv.org/abs/2510.12587)
*Bryan Eikema,Evgenia Ilia,José G. C. de Souza,Chrysoula Zerva,Wilker Aziz*

Main category: cs.CL

TL;DR: 本文提出的FUT方法通过引入不确定性提示，改善了大型语言模型不确定性表达的忠实度，有效减少了信息误传，并保持问答性能和语义稳定。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在表达知识不确定性时存在信度差距，即多次查询给出不一致答案但生成的响应缺少恰当的语气词来反映这种不确定性，导致信息传递不准确。

Method: 提出了一种名为Faithful Uncertainty Tuning (FUT)的微调技术，通过在训练数据中加入与样本一致性对齐的不确定性语言提示（如‘可能’、‘大概’），使得指令微调模型能够忠实地表达不确定性，同时保持原始答案分布不变。

Result: FUT在多个模型和数据集上的开放域问答任务中显著减少了信度差距，且不影响问答性能，同时保证了语义分布的最小变化，且在不同解码策略和不确定性表达形式上表现稳健。

Conclusion: FUT方法有效地提升了大型语言模型表达不确定性的一致性，减少了信度差距，并保持了问答准确率和语义分布的稳定。

Abstract: Large language models (LLMs) often miscommunicate their uncertainty: repeated
queries can produce divergent answers, yet generated responses are typically
unhedged or hedged in ways that do not reflect this variability. This conveys
unfaithful information about the uncertain state of the LLMs' knowledge,
creating a faithfulness gap that affects even strong LLMs. We introduce
Faithful Uncertainty Tuning (FUT): a fine-tuning approach that teaches
instruction-tuned LLMs to express uncertainty faithfully without altering their
underlying answer distribution. We construct training data by augmenting model
samples with uncertainty hedges (i.e. verbal cues such as 'possibly' or
'likely') aligned with sample consistency, requiring no supervision beyond the
model and a set of prompts. We evaluate FUT on open-domain question answering
(QA) across multiple models and datasets. Our results show that FUT
substantially reduces the faithfulness gap, while preserving QA accuracy and
introducing minimal semantic distribution shift. Further analyses demonstrate
robustness across decoding strategies, choice of hedgers, and other forms of
uncertainty expression (i.e. numerical). These findings establish FUT as a
simple and effective way to teach LLMs to communicate uncertainty faithfully.

</details>


### [52] [StyleDecipher: Robust and Explainable Detection of LLM-Generated Texts with Stylistic Analysis](https://arxiv.org/abs/2510.12608)
*Siyuan Li,Aodu Wulianghai,Xi Lin,Guangyan Li,Xiang Chen,Jun Wu,Jianhua Li*

Main category: cs.CL

TL;DR: 提出一种结合离散和连续风格特征的机器文本检测框架StyleDecipher，实现高准确率、可解释和跨领域表现优异的机器生成文本识别。


<details>
  <summary>Details</summary>
Motivation: 现有检测方法在真实场景中表现有限，缺乏泛化能力、易受改写攻击且缺乏可解释性，难以应对多样化风格和混合人机创作的挑战。

Method: 提出了StyleDecipher框架，通过结合离散风格指标和连续语义嵌入的风格表征，统一建模人类文本与LLM生成文本的风格差异，实现无须访问模型内部或标注数据的检测。

Result: 在新闻、代码、论文、评论及学术摘要五个领域内实现了领先的准确率，跨领域测试中优于现有方法最高达36.30%，并展现对抗扰动和混合文本的鲁棒性。

Conclusion: StyleDecipher方法有效实现了机器生成文本的准确检测，表现出良好的跨领域泛化能力和鲁棒性，能为检测结果提供可解释的风格特征依据。

Abstract: With the increasing integration of large language models (LLMs) into
open-domain writing, detecting machine-generated text has become a critical
task for ensuring content authenticity and trust. Existing approaches rely on
statistical discrepancies or model-specific heuristics to distinguish between
LLM-generated and human-written text. However, these methods struggle in
real-world scenarios due to limited generalization, vulnerability to
paraphrasing, and lack of explainability, particularly when facing stylistic
diversity or hybrid human-AI authorship. In this work, we propose
StyleDecipher, a robust and explainable detection framework that revisits
LLM-generated text detection using combined feature extractors to quantify
stylistic differences. By jointly modeling discrete stylistic indicators and
continuous stylistic representations derived from semantic embeddings,
StyleDecipher captures distinctive style-level divergences between human and
LLM outputs within a unified representation space. This framework enables
accurate, explainable, and domain-agnostic detection without requiring access
to model internals or labeled segments. Extensive experiments across five
diverse domains, including news, code, essays, reviews, and academic abstracts,
demonstrate that StyleDecipher consistently achieves state-of-the-art in-domain
accuracy. Moreover, in cross-domain evaluations, it surpasses existing
baselines by up to 36.30%, while maintaining robustness against adversarial
perturbations and mixed human-AI content. Further qualitative and quantitative
analysis confirms that stylistic signals provide explainable evidence for
distinguishing machine-generated text. Our source code can be accessed at
https://github.com/SiyuanLi00/StyleDecipher.

</details>


### [53] [ACADATA: Parallel Dataset of Academic Data for Machine Translation](https://arxiv.org/abs/2510.12621)
*Iñaki Lacunza,Javier Garcia Gilabert,Francesca De Luca Fornaciari,Javier Aula-Blasco,Aitor Gonzalez-Agirre,Maite Melero,Marta Villegas*

Main category: cs.CL

TL;DR: 构建并发布了大规模学术翻译数据集ACADATA，通过微调大型语言模型显著提升学术和长文本翻译效果，推动相关研究发展。


<details>
  <summary>Details</summary>
Motivation: 当前学术翻译缺乏大规模、高质量的平行数据集限制了大型语言模型在该领域的表现，亟需提供专业的学术翻译数据资源以提升模型性能。

Method: 构建包含约150万作者生成段落对的训练集ACAD-TRAIN和6000个翻译评价样本的ACAD-BENCH，对两个大型语言模型进行微调，并与多种机器翻译系统及大型语言模型进行对比评估。

Result: 微调后的7B和2B模型分别在学术翻译质量上提升了6.1和12.4 d-BLEU分，且在长上下文通用领域的英译表现提升最高达24.9%；顶尖微调模型超过了市场上最优的专有和开源模型。

Conclusion: 通过引入高质量的学术平行翻译数据集ACADATA及其两个子集，验证了在该数据集上微调大型语言模型能够显著提升学术翻译性能，且微调后的模型在学术翻译领域优于当前最佳的专有和开源模型。

Abstract: We present ACADATA, a high-quality parallel dataset for academic translation,
that consists of two subsets: ACAD-TRAIN, which contains approximately 1.5
million author-generated paragraph pairs across 96 language directions and
ACAD-BENCH, a curated evaluation set of almost 6,000 translations covering 12
directions. To validate its utility, we fine-tune two Large Language Models
(LLMs) on ACAD-TRAIN and benchmark them on ACAD-BENCH against specialized
machine-translation systems, general-purpose, open-weight LLMs, and several
large-scale proprietary models. Experimental results demonstrate that
fine-tuning on ACAD-TRAIN leads to improvements in academic translation quality
by +6.1 and +12.4 d-BLEU points on average for 7B and 2B models respectively,
while also improving long-context translation in a general domain by up to
24.9% when translating out of English. The fine-tuned top-performing model
surpasses the best propietary and open-weight models on academic translation
domain. By releasing ACAD-TRAIN, ACAD-BENCH and the fine-tuned models, we
provide the community with a valuable resource to advance research in academic
domain and long-context translation.

</details>


### [54] [COSTAR-A: A prompting framework for enhancing Large Language Model performance on Point-of-View questions](https://arxiv.org/abs/2510.12637)
*Nzubechukwu C. Ohalete,Kevin B. Gittner,Lauren M. Matheny*

Main category: cs.CL

TL;DR: 提出了COSTAR-A提示框架，通过增加'Answer'组件，显著提升了小型本地优化语言模型的生成质量，适合资源受限环境。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型对提示设计高度敏感，优化提示技术对于生成一致且高质量的输出至关重要，但原COSTAR框架在小型、本地优化模型上的表现不够稳健。

Method: 在原有COSTAR框架基础上新增'Answer'组件，进行系列受控的提示-输出评估，比较了原有COSTAR方法和COSTAR-A在最大8亿参数的本地模型上的表现。

Result: COSTAR-A提升了较小规模模型的输出结构和决断力，不同模型和应用场景表现存在差异，其中Llama 3.1-8B模型使用COSTAR-A提示后表现有显著提升。

Conclusion: COSTAR-A作为一种改进的提示工程框架，能够提升较小规模本地优化大语言模型的输出结构和明确性，特别适用于资源受限的硬件上高效的AI部署。

Abstract: Large Language Models (LLMs) are highly sensitive to prompt design, and
making optimized prompting techniques is crucial for generating consistent,
high-quality outputs. In this study, we introduce COSTAR-A, a novel prompt
engineering framework that enhances the existing COSTAR method, which stands
for Context, Objective, Style, Tone, Audience, and Response, by adding the
'Answer' component at the end. We demonstrate that while the original COSTAR
framework improves prompt clarity and aligns outputs for larger LLMs, its
performance is less consistent with smaller, locally optimized models,
particularly in tasks that require more directive or constrained outputs.
Through a series of controlled prompt-output assessments with smaller (at most
8 billion parameters), fine-tuned models, we found that COSTAR-A can enhance
the output structure and decisiveness of localized LLMs for certain tasks,
although its effectiveness varies across models and use cases. Notably, the
Llama 3.1-8B model exhibited performance improvements when prompted with
COSTAR-A compared to COSTAR alone. These findings emphasize the adaptability
and scalability of COSTAR-A as a prompting framework, particularly in
computationally efficient AI deployments on resource-constrained hardware.

</details>


### [55] [Reasoning Pattern Matters: Learning to Reason without Human Rationales](https://arxiv.org/abs/2510.12643)
*Chaoxu Pang,Yixuan Cao,Ping Luo*

Main category: cs.CL

TL;DR: 针对高成本的人类推理轨迹标注问题，本文提出PARO框架，利用大语言模型自动生成符合推理模式的推理轨迹，显著降低标注成本同时保证推理性能。


<details>
  <summary>Details</summary>
Motivation: 人工标注高质量的推理轨迹成本高昂，如何在保证推理效果的前提下减少标注成本是研究动机。

Method: 研究了模式化推理任务，证明推理模式比推理轨迹的数量和质量更关键，并提出PARO框架，利用LLM自动生成符合推理模式的推理轨迹，替代高成本的人类注释。

Result: PARO生成的推理轨迹在SFT+RLVR框架下表现与人工标注的轨迹相当，而人工标注规模是其10倍，证明了自动注释的有效性。

Conclusion: 本论文表明，通过识别和利用固定的推理模式，可以显著降低人工标注推理轨迹的成本，同时保持推理性能。使用PARO框架，LLM能自动生成与任务特定推理模式一致的推理轨迹，效果可与大量人工标注相媲美。

Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning
capabilities under the widely adopted SFT+RLVR paradigm, which first performs
Supervised Fine-Tuning (SFT) on human-annotated reasoning trajectories
(rationales) to establish initial reasoning behaviors, then applies
Reinforcement Learning with Verifiable Rewards (RLVR) to optimize the model
using verifiable signals without golden rationales. However, annotating
high-quality rationales for the SFT stage remains prohibitively expensive. This
paper investigates when and how rationale annotation costs can be substantially
reduced without compromising reasoning performance. We identify a broad class
of problems, termed patterned reasoning tasks, where reasoning follows a fixed,
procedural strategy consistent across instances. Although instances vary in
content such as domain knowledge, factual information, or numeric values, the
solution derives from applying a shared reasoning pattern. We argue that the
success of SFT+RLVR on such tasks primarily stems from its ability to enable
models to internalize these reasoning patterns. Using numerical semantic
matching as a representative task, we provide both causal and behavioral
evidence showing that reasoning patterns rather than the quantity or quality of
rationales are the key determinant of performance. Building on these insights,
we propose Pattern-Aware LLMs as Rationale AnnOtators (PARO), a simple yet
effective framework that enables LLMs to generate rationales aligned with
task-specific reasoning patterns without requiring human rationale annotations.
Experiments show that PARO-generated rationales achieve comparable SFT+RLVR
performance to human rationales that are 10 times larger. These results suggest
that large-scale human rationale annotations can be replaced with LLM-based
automatic annotations requiring only limited human supervision over reasoning
patterns.

</details>


### [56] [Generation Space Size: Understanding and Calibrating Open-Endedness of LLM Generations](https://arxiv.org/abs/2510.12699)
*Sunny Yu,Ahmad Jabbar,Robert Hawkins,Dan Jurafsky,Myra Cheng*

Main category: cs.CL

TL;DR: 该论文通过提出有效生成空间大小（GSS）概念，统一评估并提升大语言模型开放式生成的多样性与准确性，指标EigenScore表现突出，并展示了多个实际应用。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在开放式生成任务中存在输出多样性校准不足，表现为创造性任务输出过于单一，事实任务产生多样但错误的回答，需要统一指标和方法理解并改善。

Method: 设计了GSSBench任务套件基于提示对及其真实的生成空间关系评估模型行为，使用EigenScore等模型内部指标衡量生成空间，分析模型的多样性和幻觉倾向。

Result: 验证了EigenScore等幻觉检测指标优于传统多样性和不确定性指标，通过GSS的分析框架实现对提示模糊性检测、推理模型过度/不足思考解读及生成空间引导等功能。

Conclusion: 论文提出通过有效生成空间大小（GSS）统一并解决当前大模型输出多样性不足和幻觉问题；验证了EigenScore在检测幻觉和评估生成质量方面的有效性，并展示了GSS在多任务中的应用价值。

Abstract: Different open-ended generation tasks require different degrees of output
diversity. However, current LLMs are often miscalibrated. They collapse to
overly homogeneous outputs for creative tasks and hallucinate diverse but
incorrect responses for factual tasks. We argue that these two failure modes
are unified by, and can both be addressed by, the notion of effective
generation space size (GSS) -- the set of semantically distinct outputs a model
considers for a prompt. We present GSSBench, a task suite of prompt pairs with
ground-truth GSS relationships to assess different metrics and understand where
models diverge from desired behavior. We find that hallucination detection
metrics, particularly EigenScore, consistently outperform standard diversity
and uncertainty quantification metrics, while using only model internals,
providing interpretable insights into a model's internal task representations.
We demonstrate three applications of GSS: (1) detecting prompt ambiguity and
predicting clarification questions for better grounding, (2) interpreting
overthinking and underthinking in reasoning models, and (3) steering models to
expand their generation space to yield high-quality and diverse outputs.

</details>


### [57] [Omni-Captioner: Data Pipeline, Models, and Benchmark for Omni Detailed Perception](https://arxiv.org/abs/2510.12720)
*Ziyang Ma,Ruiyang Xu,Zhenghao Xing,Yunfei Chu,Yuxuan Wang,Jinzheng He,Jin Xu,Pheng-Ann Heng,Kai Yu,Junyang Lin,Eng Siong Chng,Xie Chen*

Main category: cs.CL

TL;DR: 本文针对全能语言模型捕捉多模态细节难题，提出了自主数据生成系统Omni-Detective及两种细节感知模型，辅以新颖评估方法Omni-Cloze，实验证明该方案显著提升细节理解能力与评估稳定性，达成业内领先水平。


<details>
  <summary>Details</summary>
Motivation: 当前的全能语言模型（OLMs）在捕捉和描述多模态信息中的细节方面能力有限，存在细节与幻觉并存的问题，影响了人机交互的细致理解和推理。

Method: 提出了Omni-Detective，一个集成工具调用的自主数据生成流程，用于产生高细节且低幻觉的多模态数据；基于该数据训练了Audio-Captioner和Omni-Captioner两种模型，分别用于音频和音视频的细节感知；设计了Omni-Cloze，一个新颖的填空式评估方法，用于稳定可靠地评价细节感知性能。

Result: Audio-Captioner在MMAU和MMAR指标上表现优异，超过了Gemini 2.5 Flash并与Gemini 2.5 Pro相当；Omni-Captioner在现有细节字幕基准测试中创造了新纪录，并在视频-SALMONN 2测试集上实现了细节与幻觉的最佳平衡；Omni-Detective生成的数据质量高，Omni-Cloze评估方法效果优越。

Conclusion: 本文通过引入Omni-Detective数据生成流程和Omni-Cloze评估方法，系统性提升了全能语言模型对多模态细节的感知与描述能力，显著减少幻觉现象，推动了更细致的人机交互发展。

Abstract: Fine-grained perception of multimodal information is critical for advancing
human-AI interaction. With recent progress in audio-visual technologies, Omni
Language Models (OLMs), capable of processing audio and video signals in
parallel, have emerged as a promising paradigm for achieving richer
understanding and reasoning. However, their capacity to capture and describe
fine-grained details remains limited explored. In this work, we present a
systematic and comprehensive investigation of omni detailed perception from the
perspectives of the data pipeline, models, and benchmark. We first identify an
inherent "co-growth" between detail and hallucination in current OLMs. To
address this, we propose Omni-Detective, an agentic data generation pipeline
integrating tool-calling, to autonomously produce highly detailed yet minimally
hallucinatory multimodal data. Based on the data generated with Omni-Detective,
we train two captioning models: Audio-Captioner for audio-only detailed
perception, and Omni-Captioner for audio-visual detailed perception. Under the
cascade evaluation protocol, Audio-Captioner achieves the best performance on
MMAU and MMAR among all open-source models, surpassing Gemini 2.5 Flash and
delivering performance comparable to Gemini 2.5 Pro. On existing detailed
captioning benchmarks, Omni-Captioner sets a new state-of-the-art on VDC and
achieves the best trade-off between detail and hallucination on the
video-SALMONN 2 testset. Given the absence of a dedicated benchmark for omni
detailed perception, we design Omni-Cloze, a novel cloze-style evaluation for
detailed audio, visual, and audio-visual captioning that ensures stable,
efficient, and reliable assessment. Experimental results and analysis
demonstrate the effectiveness of Omni-Detective in generating high-quality
detailed captions, as well as the superiority of Omni-Cloze in evaluating such
detailed captions.

</details>


### [58] [Which Word Orders Facilitate Length Generalization in LMs? An Investigation with GCG-Based Artificial Languages](https://arxiv.org/abs/2510.12722)
*Nadine El-Naggar,Tatsuki Kuribayashi,Ted Briscoe*

Main category: cs.CL

TL;DR: 本文通过采用广义范畴语法扩展人工语言及评估语言模型对长句的泛化能力，发现语言模型偏好类型学中常见的词序结构，泛化能力更强。


<details>
  <summary>Details</summary>
Motivation: 之前研究多用上下文无关的人工语言限制，未能覆盖诸如无界依赖和温和上下文敏感结构，且对语言模型泛化能力的评估不够深入。

Method: 采用广义范畴语法（Generalized Categorial Grammar, GCG）扩展人工语言的形式化方法，并设计实验评估语言模型对未见长句的泛化能力。

Result: 基于GCG构建的人工语言更加贴近自然语言特征，实验结果显示语言模型更容易对于语言类型学上合理的词序进行有效的泛化。

Conclusion: 语言模型在处理符合语言类型学的词序时，表现出更强的归纳偏好，能够更好地泛化处理未见过的长句子。

Abstract: Whether language models (LMs) have inductive biases that favor typologically
frequent grammatical properties over rare, implausible ones has been
investigated, typically using artificial languages (ALs) (White and Cotterell,
2021; Kuribayashi et al., 2024). In this paper, we extend these works from two
perspectives. First, we extend their context-free AL formalization by adopting
Generalized Categorial Grammar (GCG) (Wood, 2014), which allows ALs to cover
attested but previously overlooked constructions, such as unbounded dependency
and mildly context-sensitive structures. Second, our evaluation focuses more on
the generalization ability of LMs to process unseen longer test sentences.
Thus, our ALs better capture features of natural languages and our experimental
paradigm leads to clearer conclusions -- typologically plausible word orders
tend to be easier for LMs to productively generalize.

</details>


### [59] [Hey, wait a minute: on at-issue sensitivity in Language Models](https://arxiv.org/abs/2510.12740)
*Sanghee J. Kim,Kanishka Misra*

Main category: cs.CL

TL;DR: 本文提出DGRC方法，利用焦点议题评估语言模型对话自然度，发现语言模型特别是指令微调模型更倾向于聚焦对话核心内容，且对上下文提示敏感，体现了自然对话的关键特征。


<details>
  <summary>Details</summary>
Motivation: 评估语言模型中对话的自然度存在主观性且缺乏可扩展的定量指标。

Method: 提出基于语言学'焦点议题'(at-issueness)的DGRC方法，该方法将对话拆分成提示，使用语言模型分别生成子部分的后续内容，再重新组合对话并比较重新组合序列的似然性。

Result: 发现语言模型倾向于在焦点议题内容上继续对话，且这种倾向在指令微调模型中更明显；当存在相关提示（如'嘿，等一下'）时，焦点议题偏好减弱。

Conclusion: DGRC方法有效减少了语言模型语言分析中的偏差，支持系统地测试话语敏感行为，且揭示了语言模型对焦点议题的自然对话动态特征。

Abstract: Evaluating the naturalness of dialogue in language models (LMs) is not
trivial: notions of 'naturalness' vary, and scalable quantitative metrics
remain limited. This study leverages the linguistic notion of 'at-issueness' to
assess dialogue naturalness and introduces a new method: Divide, Generate,
Recombine, and Compare (DGRC). DGRC (i) divides a dialogue as a prompt, (ii)
generates continuations for subparts using LMs, (iii) recombines the dialogue
and continuations, and (iv) compares the likelihoods of the recombined
sequences. This approach mitigates bias in linguistic analyses of LMs and
enables systematic testing of discourse-sensitive behavior. Applying DGRC, we
find that LMs prefer to continue dialogue on at-issue content, with this effect
enhanced in instruct-tuned models. They also reduce their at-issue preference
when relevant cues (e.g., "Hey, wait a minute") are present. Although
instruct-tuning does not further amplify this modulation, the pattern reflects
a hallmark of successful dialogue dynamics.

</details>


### [60] [Language Models Model Language](https://arxiv.org/abs/2510.12766)
*Łukasz Borchmann*

Main category: cs.CL

TL;DR: 本文借助经验主义语言学视角，批驳了对大语言模型的传统理论批评，并提出了新的设计和评估指导思想。


<details>
  <summary>Details</summary>
Motivation: 当前对大语言模型（LLMs）的语言学评论大多基于索绪尔和乔姆斯基的理论框架，这种传统视角带来了较多的猜测和无效讨论。批评者质疑LLMs能否真正模拟语言，认为需要深层结构或语义基础才能达到理想的语言能力。

Method: 本文采用威托尔德·马恩查克的经验主义语言学理论，认为语言是所有言说和书写内容的总和，且语言运作的主要原则是语言元素使用频率。基于该框架，重新审视并反驳对LLMs的传统批评。

Result: 通过马恩查克框架，本文为LLMs的设计、评估和解释提供了建设性的指导，改变了对LLMs语言能力的传统看法。

Conclusion: 文章主张摒弃传统语言学对LLMs的先验批评，转向基于使用频率的经验主义视角，更有效地理解和推动LLMs的发展。

Abstract: Linguistic commentary on LLMs, heavily influenced by the theoretical
frameworks of de Saussure and Chomsky, is often speculative and unproductive.
Critics challenge whether LLMs can legitimately model language, citing the need
for "deep structure" or "grounding" to achieve an idealized linguistic
"competence." We argue for a radical shift in perspective towards the
empiricist principles of Witold Ma\'nczak, a prominent general and historical
linguist. He defines language not as a "system of signs" or a "computational
system of the brain" but as the totality of all that is said and written. Above
all, he identifies frequency of use of particular language elements as
language's primary governing principle. Using his framework, we challenge prior
critiques of LLMs and provide a constructive guide for designing, evaluating,
and interpreting language models.

</details>


### [61] [Dr.LLM: Dynamic Layer Routing in LLMs](https://arxiv.org/abs/2510.12773)
*Ahmed Heakl,Martin Gubri,Salman Khan,Sangdoo Yun,Seong Joon Oh*

Main category: cs.CL

TL;DR: 提出Dr.LLM框架，通过轻量级动态层路由器和蒙特卡洛树搜索监督，实现LLM的计算资源节约和准确率提升，且推广性好，无需修改模型基础权重。


<details>
  <summary>Details</summary>
Motivation: LLMs在处理不同难度的查询时，存在计算资源浪费和灵活性不足的问题，且现有自适应深度方法依赖推理时昂贵搜索或大规模重训练，且可能降低准确率。

Method: 提出Dr.LLM框架，在预训练模型中加入轻量级的每层路由模块，使用蒙特卡洛树搜索（MCTS）为路由器提供监督信号，决定跳过、执行或重复某层，以实现动态路由；设计了窗口池化、焦点损失和瓶颈MLP路由器以提升稳定性和鲁棒性。

Result: 在ARC（逻辑）和DART（数学）数据集上，Dr.LLM准确率提升高达3.4个百分点，同时平均每个样本节省5层计算资源；路由器还能推广到多种领域任务，准确率仅下降0.85%，效率保持，同时性能优于现有路由方法7.7个百分点。

Conclusion: Dr.LLM证明了通过显式监督训练的轻量级路由器，能够在不改变基础权重的前提下，动态调整LLM推理深度，实现计算预算感知且准确率驱动的推理优化。

Abstract: Large Language Models (LLMs) process every token through all layers of a
transformer stack, causing wasted computation on simple queries and
insufficient flexibility for harder ones that need deeper reasoning.
Adaptive-depth methods can improve efficiency, but prior approaches rely on
costly inference-time search, architectural changes, or large-scale retraining,
and in practice often degrade accuracy despite efficiency gains. We introduce
Dr.LLM, Dynamic routing of Layers for LLMs, a retrofittable framework that
equips pretrained models with lightweight per-layer routers deciding to skip,
execute, or repeat a block. Routers are trained with explicit supervision:
using Monte Carlo Tree Search (MCTS), we derive high-quality layer
configurations that preserve or improve accuracy under a compute budget. Our
design, windowed pooling for stable routing, focal loss with class balancing,
and bottleneck MLP routers, ensures robustness under class imbalance and long
sequences. On ARC (logic) and DART (math), Dr.LLM improves accuracy by up to
+3.4%p while saving 5 layers per example on average. Routers generalize to
out-of-domain tasks (MMLU, GSM8k, AIME, TruthfulQA, SQuADv2, GPQA, PIQA,
AGIEval) with only 0.85% accuracy drop while retaining efficiency, and
outperform prior routing methods by up to +7.7%p. Overall, Dr.LLM shows that
explicitly supervised routers retrofit frozen LLMs for budget-aware,
accuracy-driven inference without altering base weights.

</details>


### [62] [Cost Analysis of Human-corrected Transcription for Predominately Oral Languages](https://arxiv.org/abs/2510.12781)
*Yacouba Diarra,Nouhoum Souleymane Coulibaly,Michael Leventhal*

Main category: cs.CL

TL;DR: 研究显示制作低资源语言高质量语音数据极费时，提供了劳动成本的基线数据。


<details>
  <summary>Details</summary>
Motivation: 低资源语言语音数据制作的实际成本与复杂度尚不清楚，特别是低识字率的口语语言。

Method: 通过一个月的实地研究，分析十名 Bambara 语言母语者对自动语音识别生成文本的校正过程。

Result: 准确转录1小时语音数据平均需30小时（实验室）至36小时（实地）人力投入。

Conclusion: 制作低资源语言语音数据集需要大量人力，实际劳动时间远超语音时长。

Abstract: Creating speech datasets for low-resource languages is a critical yet poorly
understood challenge, particularly regarding the actual cost in human labor.
This paper investigates the time and complexity required to produce
high-quality annotated speech data for a subset of low-resource languages, low
literacy Predominately Oral Languages, focusing on Bambara, a Manding language
of Mali. Through a one-month field study involving ten transcribers with native
proficiency, we analyze the correction of ASR-generated transcriptions of 53
hours of Bambara voice data. We report that it takes, on average, 30 hours of
human labor to accurately transcribe one hour of speech data under laboratory
conditions and 36 hours under field conditions. The study provides a baseline
and practical insights for a large class of languages with comparable profiles
undertaking the creation of NLP resources.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [63] [Empirical Study on Robustness and Resilience in Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.11824)
*Simin Li,Zihao Mao,Hanxiao Li,Zonglei Jing,Zhuohang bian,Jun Guo,Li Wang,Zhuoran Han,Ruixiao Xu,Xin Yu,Chengdong Ma,Yuqing Ma,Bo An,Yaodong Yang,Weifeng Lv,Xianglong Liu*

Main category: cs.MA

TL;DR: 本文通过8万余次实验大规模探索多智能体强化学习在多种真实不确定环境下的合作与稳健恢复性能，揭示了合作与稳健恢复非正相关关系，强调超参数调优对提升算法可信性的关键作用。


<details>
  <summary>Details</summary>
Motivation: 现实世界的不确定性导致在模拟环境中调优的多智能体策略在真实环境中稳健性和恢复能力不足，深入理解稳健性和恢复性是构建可信多智能体系统的关键。

Method: 通过在4个真实环境、13种不确定类型和15种超参数设置下进行超过8.2万次实验，系统评估了多智能体强化学习算法的合作性能、稳健性和恢复能力。

Result: 发现合作优化能在轻度不确定环境中提升稳健性和恢复能力，但强扰动下效果减弱；稳健性和恢复能力随算法和不确定类别变化显著，且不易跨类别或代理泛化；部分常用技术（参数共享、GAE、PopArt）反而削弱稳健性，而早期停止、高学习率和Leaky ReLU有助提升；仅通过超参数调优即可显著提升多智能体强化学习的合作、稳健及恢复性能。

Conclusion: 本文通过大规模实证研究揭示了多智能体强化学习在真实不确定性环境下的合作性、稳健性和恢复能力存在复杂关系，并指出这些性能不一定相互促进且受算法、不确定性类型和超参数调优的显著影响。

Abstract: In cooperative Multi-Agent Reinforcement Learning (MARL), it is a common
practice to tune hyperparameters in ideal simulated environments to maximize
cooperative performance. However, policies tuned for cooperation often fail to
maintain robustness and resilience under real-world uncertainties. Building
trustworthy MARL systems requires a deep understanding of robustness, which
ensures stability under uncertainties, and resilience, the ability to recover
from disruptions--a concept extensively studied in control systems but largely
overlooked in MARL. In this paper, we present a large-scale empirical study
comprising over 82,620 experiments to evaluate cooperation, robustness, and
resilience in MARL across 4 real-world environments, 13 uncertainty types, and
15 hyperparameters. Our key findings are: (1) Under mild uncertainty,
optimizing cooperation improves robustness and resilience, but this link
weakens as perturbations intensify. Robustness and resilience also varies by
algorithm and uncertainty type. (2) Robustness and resilience do not generalize
across uncertainty modalities or agent scopes: policies robust to action noise
for all agents may fail under observation noise on a single agent. (3)
Hyperparameter tuning is critical for trustworthy MARL: surprisingly, standard
practices like parameter sharing, GAE, and PopArt can hurt robustness, while
early stopping, high critic learning rates, and Leaky ReLU consistently help.
By optimizing hyperparameters only, we observe substantial improvement in
cooperation, robustness and resilience across all MARL backbones, with the
phenomenon also generalizing to robust MARL methods across these backbones.
Code and results available at
https://github.com/BUAA-TrustworthyMARL/adv_marl_benchmark .

</details>


### [64] [Heterogeneous RBCs via deep multi-agent reinforcement learning](https://arxiv.org/abs/2510.12272)
*Federico Gabriele,Aldo Glielmo,Marco Taboga*

Main category: cs.MA

TL;DR: 本文提出了融合多智能体强化学习与商业周期模型的框架MARL-BC，突破了传统宏观模型的限制，实现了复杂代理异质性的高效模拟与经典模型结果的还原，推动了不同宏观建模方法的结合。


<details>
  <summary>Details</summary>
Motivation: 现有异质性代理宏观经济模型分为传统的一般均衡模型和基于行为规则的代理模型，分别存在计算成本高和模型开发效率低的问题，亟需一种能兼顾两者优势的框架。

Method: 提出并应用MARL-BC框架，结合多智能体深度强化学习方法与实际商业周期(RBC)模型，进行单智能体、多智能体（同质）及多智能体（异质）场景下的仿真。

Result: MARL-BC成功实现了在单代理下重现RBC经典结果，在大量同质代理情形下重现均场Krusell-Smith模型结果，并高效模拟复杂代理异质性，展示了良好的模型泛化能力。

Conclusion: MARL-BC框架成功融合了多智能体深度强化学习与实际商业周期模型，克服了传统宏观经济模型中计算复杂度高和行为规则指定困难的问题，实现了对异质性代理的大规模灵活模拟，并在极限情况下能够还原一般均衡模型的结果。

Abstract: Current macroeconomic models with agent heterogeneity can be broadly divided
into two main groups. Heterogeneous-agent general equilibrium (GE) models, such
as those based on Heterogeneous Agents New Keynesian (HANK) or Krusell-Smith
(KS) approaches, rely on GE and 'rational expectations', somewhat unrealistic
assumptions that make the models very computationally cumbersome, which in turn
limits the amount of heterogeneity that can be modelled. In contrast,
agent-based models (ABMs) can flexibly encompass a large number of arbitrarily
heterogeneous agents, but typically require the specification of explicit
behavioural rules, which can lead to a lengthy trial-and-error
model-development process. To address these limitations, we introduce MARL-BC,
a framework that integrates deep multi-agent reinforcement learning (MARL) with
Real Business Cycle (RBC) models. We demonstrate that MARL-BC can: (1) recover
textbook RBC results when using a single agent; (2) recover the results of the
mean-field KS model using a large number of identical agents; and (3)
effectively simulate rich heterogeneity among agents, a hard task for
traditional GE approaches. Our framework can be thought of as an ABM if used
with a variety of heterogeneous interacting agents, and can reproduce GE
results in limit cases. As such, it is a step towards a synthesis of these
often opposed modelling paradigms.

</details>


### [65] [Characterizing Agent-Based Model Dynamics via $ε$-Machines and Kolmogorov-Style Complexity](https://arxiv.org/abs/2510.12729)
*Roberto Garrone*

Main category: cs.MA

TL;DR: 本文提出一种双层信息论框架，用ε-机和Kolmogorov复杂度度量分析基于代理模型动态，实现从个体到整体的信息组织表征，验证于看护者-老人交互案例。


<details>
  <summary>Details</summary>
Motivation: 希望在基于代理模型中，通过信息论框架刻画系统内动态的信息组织结构，既保持个体异质性，又实现宏观整体的可解释建模，促进复杂自适应系统中涌现、反馈及适应性的理解。

Method: 构建宏观层面的汇总ε-机作为系统-wide信息模型，微观层面为每对看护者-老年人及变量重建ε-机，并结合归一化LZ78复杂度和无损压缩的每符号比特数等Kolmogorov风格度量，形成多维特征集，用于分布式分析、分层比较和无监督聚类。

Result: 初步案例研究展示了框架在看护者-老年人互动中的具体应用，实现了双层尺度的信息表征和分析，最终结果将在完成模拟运行后更新。

Conclusion: 本文提出的两层信息论框架能够有效描述基于代理的模型动态的信息组织，兼顾宏观整体和微观个体差异，符合复杂自适应系统的原则。

Abstract: We propose a two-level information-theoretic framework for characterizing the
informational organization of Agent-Based Model (ABM) dynamics within the
broader paradigm of Complex Adaptive Systems (CAS). At the macro level, a
pooled $\epsilon$-machine is reconstructed as a reference model that summarizes
the system-wide informational regime. At the micro level, $\epsilon$-machines
are reconstructed for each caregiver-elder dyad and variable, and are
complemented with algorithm-agnostic Kolmogorov-style measures, including
normalized LZ78 complexity and bits per symbol from lossless compression. The
resulting feature set $\{h_{\mu}, C_{\mu}, E, \mathrm{LZ78}, \mathrm{bps}\}$
enables distributional analysis, stratified comparisons, and unsupervised
clustering across agents and scenarios. This dual-scale design preserves agent
heterogeneity while providing an interpretable macro-level baseline, aligning
ABM practice with CAS principles of emergence, feedback, and adaptation. A case
study on caregiver-elder interactions illustrates the framework's
implementation; the results and discussion will be completed following final
simulation runs.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [66] [eye2vec: Learning Distributed Representations of Eye Movement for Program Comprehension Analysis](https://arxiv.org/abs/2510.11722)
*Haruhiko Yoshioka,Kazumasa Shimari,Hidetake Uwano,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: eye2vec是一种基于分布式表示的眼动数据分析工具，能更灵活、高效地解析程序员读代码时的眼动行为，改善了传统分析的繁琐和局限。


<details>
  <summary>Details</summary>
Motivation: 传统的程序理解眼动跟踪研究需要研究者预先选定分析目标和指标，且分析大多依赖人工耗时的工作，限制了分析的多样性和效率。

Method: 该方法将连续的两次注视视为语法元素之间的转换，采用分布式表示方法，避免了传统眼动分析中必须预先选择关注区域和指标的限制，使得分析更加灵活和语义丰富。

Result: eye2vec提供了一种新的眼动分析基础设施，支持定义多种关注区域（字、行、代码块）并通过分布式表示促进多样化的数据分析方法，提升了眼动数据的语义理解和应用范围。

Conclusion: eye2vec通过将连续的两次注视表示为语法元素之间的转换，利用分布式表示简化了眼动数据的分析过程，提高了对程序员阅读代码时眼动行为的理解和分析能力。

Abstract: This paper presents eye2vec, an infrastructure for analyzing software
developers' eye movements while reading source code. In common eye-tracking
studies in program comprehension, researchers must preselect analysis targets
such as control flow or syntactic elements, and then develop analysis methods
to extract appropriate metrics from the fixation for source code. Here,
researchers can define various levels of AOIs like words, lines, or code
blocks, and the difference leads to different results. Moreover, the
interpretation of fixation for word/line can vary across the purposes of the
analyses. Hence, the eye-tracking analysis is a difficult task that depends on
the time-consuming manual work of the researchers. eye2vec represents
continuous two fixations as transitions between syntactic elements using
distributed representations. The distributed representation facilitates the
adoption of diverse data analysis methods with rich semantic interpretations.

</details>


### [67] [Task-Aware Reduction for Scalable LLM-Database Systems](https://arxiv.org/abs/2510.11813)
*Marcus Emmanuel Barnes,Taher A. Ghaleb,Safwat Hassan*

Main category: cs.SE

TL;DR: 本文主张将LLM的token预算作为注意力预算，通过任务感知的文本简化优先保留重要信息，从而提高处理大规模文本数据的效率和效果，推动可持续的LLM数据集成。


<details>
  <summary>Details</summary>
Motivation: 现实世界中丰富文本数据（如日志、遥测和监控流）的体量大、冗长且噪声多，直接输入LLM成本高且效果有限，现有优化主要聚焦模型本身而非输入数据简化，迫切需要有效减少上游输入冗余。

Method: 本文通过将输入侧文本简化视为注意力分配而非简单压缩，优先保留与下游任务最相关的信息，并探讨构建基准测试、设计自适应简化流程以及将token预算感知预处理集成到数据库和检索系统中的方法。

Result: 提出了一种关注输入文本精简的视角，明确了相关研究挑战，描绘了通过合理分配注意力资源提升LLM与数据集成的可扩展性、准确性和可持续性的愿景。

Conclusion: 本文提出将大型语言模型（LLM）的token预算视为注意力预算，强调任务感知的文本简化作为语言-数据系统设计的核心原则。

Abstract: Large Language Models (LLMs) are increasingly applied to data-intensive
workflows, from database querying to developer observability. Yet the
effectiveness of these systems is constrained by the volume, verbosity, and
noise of real-world text-rich data such as logs, telemetry, and monitoring
streams. Feeding such data directly into LLMs is costly, environmentally
unsustainable, and often misaligned with task objectives. Parallel efforts in
LLM efficiency have focused on model- or architecture-level optimizations, but
the challenge of reducing upstream input verbosity remains underexplored. In
this paper, we argue for treating the token budget of an LLM as an attention
budget and elevating task-aware text reduction as a first-class design
principle for language -- data systems. We position input-side reduction not as
compression, but as attention allocation: prioritizing information most
relevant to downstream tasks. We outline open research challenges for building
benchmarks, designing adaptive reduction pipelines, and integrating
token-budget--aware preprocessing into database and retrieval systems. Our
vision is to channel scarce attention resources toward meaningful signals in
noisy, data-intensive workflows, enabling scalable, accurate, and sustainable
LLM--data integration.

</details>


### [68] [Lingxi: Repository-Level Issue Resolution Framework Enhanced by Procedural Knowledge Guided Scaling](https://arxiv.org/abs/2510.11838)
*Xu Yang,Jiayuan Zhou,Michael Pacheco,Wenhan Zhu,Pengfei He,Shaowei Wang,Kui Liu,Ruiqi Pan*

Main category: cs.SE

TL;DR: Lingxi通过利用历史修复数据提取程序性知识，指导LLM代理智能解决复杂仓库级问题，显著超过现有方法性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于代理的方法在复杂仓库问题解决中缺乏有效的程序性知识指导，且依赖大规模计算资源进行盲目搜索，效果受限。

Method: Lingxi通过离线构建基于历史问题修复数据的层次化程序性知识，并在线采用知识驱动的缩放方法，指导代理多角度智能分析目标问题，避免盲目探索。

Result: Lingxi在SWE-bench Verified基准测试中实现74.6%的问题解决率，较五种先进技术提升5.4%-14.9%，消融实验证明贡献来自程序性知识的应用。

Conclusion: Lingxi框架成功利用程序性知识显著提升了LLM驱动的代理在仓库级别复杂问题的解决能力，优于现有技术。

Abstract: Driven by the advancements of Large Language Models (LLMs), LLM-powered
agents are making significant improvements in software engineering tasks, yet
struggle with complex, repository-level issue resolution. Existing agent-based
methods have two key limitations. First, they lack of procedural knowledge
(i.e., how an issue is fixed step-by-step and rationales behind it) to learn
and leverage for issue resolution. Second, they rely on massive computational
power to blindly explore the solution space. % To address those limitations, we
propose Lingxi, an issue resolution framework that leverages procedural
knowledge extracted from historical issue-fixing data to guide agents in
solving repository-level issues. \ourTool first constructs this knowledge
offline through a hierarchical abstraction mechanism, enabling agents to learn
the how and why behind a fix, not just the final solution. During online
application, it employs a knowledge-driven scaling method that leverages the
procedural knowledge of similar issues to intelligently analyze the target
issue from multiple perspectives, in sharp contrast to undirected, brute-force
exploration. % Lingxi successfully resolves 74.6\% of bugs on the SWE-bench
Verified benchmark in Past@1 setting, outperforming five state-of-the-art
techniques by a significant margin (5.4\% to 14.9\%). Our comprehensive
ablation study confirmed that the success of Lingxi comes directly from its use
of procedural knowledge. Without it, the performance gains from scaling alone
is negligible. Our qualitative study further shows that the ``design patterns
$\&$ coding practices'' is the most critical knowledge aspect, and that the
roles of different knowledge aspects switch across different stages (i.e.,
analysis, planning, and fixing).

</details>


### [69] [DMAS-Forge: A Framework for Transparent Deployment of AI Applications as Distributed Systems](https://arxiv.org/abs/2510.11872)
*Alessandro Cornacchia,Vaastav Anand,Muhammad Bilal,Zafar Qazi,Marco Canini*

Main category: cs.SE

TL;DR: DMAS-Forge框架帮助开发者轻松部署分布式多智能体AI系统，降低了复杂系统开发中的部署难度和人工成本。


<details>
  <summary>Details</summary>
Motivation: 多智能体AI应用复杂，结合不同角色和工具，类似面向服务架构，但现有编程框架和协议快速变化，使部署和测试这类分布式系统工作量大且困难。

Method: 设计并实现了DMAS-Forge框架，该框架通过透明生成粘合代码和配置，支持多智能体应用在多样部署环境中的分布式运行，减少了手动工作量。

Result: 提出了DMAS-Forge的设计理念和原型，成功展示了如何脱离具体部署选择迅速部署多智能体系统，提升开发效率。

Conclusion: DMAS-Forge框架有效简化了多智能体应用的部署和测试过程，通过解耦应用逻辑与具体部署选择，实现了多部署场景下的分布式多智能体应用的快速生成和配置。

Abstract: Agentic AI applications increasingly rely on multiple agents with distinct
roles, specialized tools, and access to memory layers to solve complex tasks --
closely resembling service-oriented architectures. Yet, in the rapid evolving
landscape of programming frameworks and new protocols, deploying and testing AI
agents as distributed systems remains a daunting and labor-intensive task. We
present DMAS-Forge, a framework designed to close this gap. DMAS-Forge
decouples application logic from specific deployment choices, and aims at
transparently generating the necessary glue code and configurations to spawn
distributed multi-agent applications across diverse deployment scenarios with
minimal manual effort. We present our vision, design principles, and a
prototype of DMAS-Forge. Finally, we discuss the opportunities and future work
for our approach.

</details>


### [70] [TorchCor: High-Performance Cardiac Electrophysiology Simulations with the Finite Element Method on GPUs](https://arxiv.org/abs/2510.12011)
*Bei Zhou,Maximilian Balmus,Cesare Corrado,Ludovica Cicci,Shuang Qian,Steven A. Niederer*

Main category: cs.SE

TL;DR: 本文提出了基于GPU的TorchCor库，实现高效准确的心脏电生理有限元模拟，解决了传统CPU计算资源需求高的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的心脏电生理模拟需大量CPU核心的高性能计算资源，而这对于许多研究组与临床医生来说难以获得。

Method: 基于PyTorch框架，利用有限元方法在通用GPU上进行心脏电生理模拟。

Result: 开发了高性能Python库TorchCor，该库支持快速的3D心脏电生理模拟且对学术和商业用途免费无使用限制。

Conclusion: TorchCor能够显著加速心脏电生理模拟，特别是在处理大规模3D网格时，且其求解器的准确性已通过分析解和基准问题验证。

Abstract: Cardiac electrophysiology (CEP) simulations are increasingly used for
understanding cardiac arrhythmias and guiding clinical decisions. However,
these simulations typically require high-performance computing resources with
numerous CPU cores, which are often inaccessible to many research groups and
clinicians. To address this, we present TorchCor, a high-performance Python
library for CEP simulations using the finite element method on general-purpose
GPUs. Built on PyTorch, TorchCor significantly accelerates CEP simulations,
particularly for large 3D meshes. The accuracy of the solver is verified
against manufactured analytical solutions and the $N$-version benchmark
problem. TorchCor is freely available for both academic and commercial use
without restrictions.

</details>


### [71] [Enhancing Neural Code Representation with Additional Context](https://arxiv.org/abs/2510.12082)
*Huy Nguyen,Christoph Treude,Patanamon Thongtanunam*

Main category: cs.SE

TL;DR: 本研究通过实验证明，结合版本历史和代码结构信息能显著提升代码理解相关深度学习模型的效果，开拓了上下文编码的新方向。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型多仅依赖源代码，忽视版本历史及结构上下文，限制了模型对代码演化和操作的理解。研究旨在探究加入上下文信息对模型性能的影响。

Method: 对五种代表性模型(CodeBERT, GraphCodeBERT, CodeT5, PLBART, ASTNN)在两个任务(克隆检测和代码摘要)及两个数据集(SeSaMe和CodeSearchNet)上，比较代码仅用情景与代码+上下文增强的微调表现，辅以人工评估。

Result: 上下文信息普遍提升模型表现，版本历史在克隆检测和代码摘要中均有显著提升，调用图影响依任务与模型而异，结合多种上下文能进一步提高表现；人工评估验证上下文增强摘要准确性和内容充分性更优。

Conclusion: 引入代码上下文信息（如版本历史和结构关系）能够显著提升神经网络模型在代码克隆检测和代码摘要任务中的性能，尤其版本历史带来了明显改进，多种上下文结合效果更佳。

Abstract: Automated program comprehension underpins many software engineering tasks,
from code summarisation to clone detection. Recent deep learning models achieve
strong results but typically rely on source code alone, overlooking contextual
information such as version history or structural relationships. This limits
their ability to capture how code evolves and operates. We conduct an empirical
study on how enriching code representations with such contextual signals
affects neural model performance on key comprehension tasks. Two downstream
tasks, code clone detection and code summarisation, are evaluated using SeSaMe
(1,679 Java methods) and CodeSearchNet (63,259 methods). Five representative
models (CodeBERT, GraphCodeBERT, CodeT5, PLBART, ASTNN) are fine-tuned under
code-only and context-augmented settings. Results show that context generally
improves performance: version history consistently boosts clone detection
(e.g., CodeT5 +15.92% F1) and summarisation (e.g., GraphCodeBERT +5.56%
METEOR), while call-graph effects vary by model and task. Combining multiple
contexts yields further gains (up to +21.48% macro-F1). Human evaluation on 100
Java snippets confirms that context-augmented summaries are significantly
preferred for Accuracy and Content Adequacy (p <= 0.026; |delta| up to 0.55).
These findings highlight the potential of contextual signals to enhance code
comprehension and open new directions for optimising contextual encoding in
neural SE models.

</details>


### [72] [Towards Engineering Multi-Agent LLMs: A Protocol-Driven Approach](https://arxiv.org/abs/2510.12120)
*Zhenyu Mao,Jacky Keung,Fengji Zhang,Shuo Liu,Yifei Wang,Jialong Li*

Main category: cs.SE

TL;DR: 提出SEMAP协议改善多智能体LLM在软件工程中的协作，通过契约建模和生命周期管理大幅减少系统失败，提高开发和漏洞检测的性能。


<details>
  <summary>Details</summary>
Motivation: 软件开发需求增长推动了利用大型语言模型(LLMs)自动化软件工程任务的兴趣，但现有多智能体系统(MAS)常因欠缺规范、协调失误及验证不当而失败。

Method: 提出了软件工程多智能体协议(SEMAP)，基于三大设计原则——明确行为契约建模、结构化消息传递及生命周期引导的执行与验证，构建在谷歌Agent-to-Agent(A2A)基础设施上。

Result: 通过多智能体系统失败分类框架(MAST)实验证明，SEMAP显著减少了不同软件工程任务中的失败率。在函数级开发中失败率降低达69.6%，部署级降低56.7%；漏洞检测任务中，Python任务失败率降低47.4%，C/C++任务降低28.2%。

Conclusion: SEMAP作为一种多智能体协议层方法，有效解决了现有基于LLM的多智能体系统中的核心缺陷，提高了软件工程任务的自动化可靠性和效率。

Abstract: The increasing demand for software development has driven interest in
automating software engineering (SE) tasks using Large Language Models (LLMs).
Recent efforts extend LLMs into multi-agent systems (MAS) that emulate
collaborative development workflows, but these systems often fail due to three
core deficiencies: under-specification, coordination misalignment, and
inappropriate verification, arising from the absence of foundational SE
structuring principles. This paper introduces Software Engineering Multi-Agent
Protocol (SEMAP), a protocol-layer methodology that instantiates three core SE
design principles for multi-agent LLMs: (1) explicit behavioral contract
modeling, (2) structured messaging, and (3) lifecycle-guided execution with
verification, and is implemented atop Google's Agent-to-Agent (A2A)
infrastructure. Empirical evaluation using the Multi-Agent System Failure
Taxonomy (MAST) framework demonstrates that SEMAP effectively reduces failures
across different SE tasks. In code development, it achieves up to a 69.6%
reduction in total failures for function-level development and 56.7% for
deployment-level development. For vulnerability detection, SEMAP reduces
failure counts by up to 47.4% on Python tasks and 28.2% on C/C++ tasks.

</details>


### [73] [Runtime Composition in Dynamic System of Systems: A Systematic Review of Challenges, Solutions, Tools, and Evaluation Methods](https://arxiv.org/abs/2510.12616)
*Muhammad Ashfaq,Ahmed R. Sadik,Teerath Das,Muhammad Waseem,Niko Makitalo,Tommi Mikkonen*

Main category: cs.SE

TL;DR: 本文系统综述了动态系统运行时组合的挑战与解决方案，分析了相关工具与评估方法，提出未来研究需关注标准化评估、分散架构和跨领域框架。


<details>
  <summary>Details</summary>
Motivation: 现代系统日益在动态环境中运行，运行时组合对于系统的适应性至关重要，但现有文献缺乏针对动态系统运行时组合的统一综述和系统分析。

Method: 采用系统文献综述方法（SLR），筛选2019年至2024年间1774篇相关文献，选取80篇作为主题分析的主要研究对象。

Result: 识别了运行时组合的四大挑战类别（建模与分析、弹性操作、系统编排和异构性），提出了七类解决方案领域，分析了主流支持工具和评估方法，并指出了工具互操作性差、缺乏跨工具链工作流和标准基准等不足。

Conclusion: 本文综述了动态系统的运行时组合研究，揭示了自治与协调、建模与现实、社会技术整合之间的矛盾，强调了标准化评估指标、可扩展分散架构和跨领域框架的必要性。

Abstract: Context: Modern Systems of Systems (SoSs) increasingly operate in dynamic
environments (e.g., smart cities, autonomous vehicles) where runtime
composition -- the on-the-fly discovery, integration, and coordination of
constituent systems (CSs)--is crucial for adaptability. Despite growing
interest, the literature lacks a cohesive synthesis of runtime composition in
dynamic SoSs. Objective: This study synthesizes research on runtime composition
in dynamic SoSs and identifies core challenges, solution strategies, supporting
tools, and evaluation methods. Methods: We conducted a Systematic Literature
Review (SLR), screening 1,774 studies published between 2019 and 2024 and
selecting 80 primary studies for thematic analysis (TA). Results: Challenges
fall into four categories: modeling and analysis, resilient operations, system
orchestration, and heterogeneity of CSs. Solutions span seven areas:
co-simulation and digital twins, semantic ontologies, integration frameworks,
adaptive architectures, middleware, formal methods, and AI-driven resilience.
Service-oriented frameworks for composition and integration dominate tooling,
while simulation platforms support evaluation. Interoperability across tools,
limited cross-toolchain workflows, and the absence of standardized benchmarks
remain key gaps. Evaluation approaches include simulation-based,
implementation-driven, and human-centered studies, which have been applied in
domains such as smart cities, healthcare, defense, and industrial automation.
Conclusions: The synthesis reveals tensions, including autonomy versus
coordination, the modeling-reality gap, and socio-technical integration. It
calls for standardized evaluation metrics, scalable decentralized
architectures, and cross-domain frameworks. The analysis aims to guide
researchers and practitioners in developing and implementing dynamically
composable SoSs.

</details>


### [74] [iCodeReviewer: Improving Secure Code Review with Mixture of Prompts](https://arxiv.org/abs/2510.12186)
*Yun Peng,Kisub Kim,Linghan Meng,Kui Liu*

Main category: cs.SE

TL;DR: 本文提出了基于大语言模型的iCodeReviewer自动安全代码审查方法，通过多提示专家架构和智能路由算法，有效提升安全漏洞检测的准确率和覆盖率，实验证明其在实际应用中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有自动安全代码审查方法存在准确率和覆盖率有限，以及缺乏全面评估的问题，需要改进以提升代码安全审查效率和效果。

Method: iCodeReviewer基于大语言模型，采用多提示专家的混合提示架构和针对代码特征的路由算法，以提高安全问题识别的覆盖率和准确性。

Result: 内部数据集实验显示F1值为63.98%，生成的审查评论在生产环境中的接受率高达84%。

Conclusion: iCodeReviewer显著提升了自动安全代码审查的准确率和覆盖率，减少了误报，并在实际生产环境中获得了较高的接受度。

Abstract: Code review is an essential process to ensure the quality of software that
identifies potential software issues at an early stage of software development.
Among all software issues, security issues are the most important to identify,
as they can easily lead to severe software crashes and service disruptions.
Recent research efforts have been devoted to automated approaches to reduce the
manual efforts required in the secure code review process. Despite the
progress, current automated approaches on secure code review, including static
analysis, deep learning models, and prompting approaches, still face the
challenges of limited precision and coverage, and a lack of comprehensive
evaluation.
  To mitigate these challenges, we propose iCodeReviewer, which is an automated
secure code review approach based on large language models (LLMs).
iCodeReviewer leverages a novel mixture-of-prompts architecture that
incorporates many prompt experts to improve the coverage of security issues.
Each prompt expert is a dynamic prompt pipeline to check the existence of a
specific security issue. iCodeReviewer also implements an effective routing
algorithm to activate only necessary prompt experts based on the code features
in the input program, reducing the false positives induced by LLM
hallucination. Experiment results in our internal dataset demonstrate the
effectiveness of iCodeReviewer in security issue identification and
localization with an F1 of 63.98%. The review comments generated by
iCodeReviewer also achieve a high acceptance rate up to 84% when it is deployed
in production environments.

</details>


### [75] [Show Your Title! A Scoping Review on Verbalization in Software Engineering with LLM-Assisted Screening](https://arxiv.org/abs/2510.12294)
*Gergő Balogh,Dávid Kószó,Homayoun Safarpour Motealegh Mahalegi,László Tóth,Bence Szakács,Áron Búcsú*

Main category: cs.SE

TL;DR: 本文通过GPT辅助筛查9000多篇论文，综述软件工程与心理学交叉领域中口语数据的研究主题，发现软件工程主要关注开发工艺，借鉴心理学方法较多，且GPT在跨学科文献评审中表现良好。


<details>
  <summary>Details</summary>
Motivation: 理解软件开发者的思维、决策及行为是软件工程中的关键挑战，口语化技术为研究认知过程提供了一种轻量化、易获得的数据捕捉方法。本文旨在系统总结软件工程与心理学交叉领域中用口语数据研究的主题，并评估大型语言模型在支持跨学科综述中的效果。

Method: 采用基于大型语言模型（GPT）的辅助筛查流程，通过标题筛选9000多篇论文，结合人类审核验证模型输出的准确性，进行跨学科文献综述。

Result: 通过大规模筛查，识别出软件工程中口语化相关研究主要聚焦于开发工艺，心理学方法被频繁应用于软件工程领域，但软件工程方法在心理学中应用较少。基于GPT的筛查流程准确率高，支持跨学科文献评审。

Conclusion: 基于语言模型的筛查方法在跨学科文献综述中表现出高效且准确的能力，验证结果显示与人工审稿一致率高达87%。SE领域中口语化数据的研究主要集中在软件开发工艺相关主题，对以人为中心的主题关注较少。此外，SE领域常借鉴心理学方法，但心理学领域较少涉足软件工程。

Abstract: Understanding how software developers think, make decisions, and behave
remains a key challenge in software engineering (SE). Verbalization techniques
(methods that capture spoken or written thought processes) offer a lightweight
and accessible way to study these cognitive aspects. This paper presents a
scoping review of research at the intersection of SE and psychology (PSY),
focusing on the use of verbal data. To make large-scale interdisciplinary
reviews feasible, we employed a large language model (LLM)-assisted screening
pipeline using GPT to assess the relevance of over 9,000 papers based solely on
titles. We addressed two questions: what themes emerge from
verbalization-related work in SE, and how effective are LLMs in supporting
interdisciplinary review processes? We validated GPT's outputs against human
reviewers and found high consistency, with a 13\% disagreement rate. Prominent
themes mainly were tied to the craft of SE, while more human-centered topics
were underrepresented. The data also suggests that SE frequently draws on PSY
methods, whereas the reverse is rare.

</details>


### [76] [(R)evolution of Programming: Vibe Coding as a Post-Coding Paradigm](https://arxiv.org/abs/2510.12364)
*Kevin Krings,Nino S. Bohn,Thomas Ludwig*

Main category: cs.SE

TL;DR: 本论文探讨了以情感和直觉驱动的Vibe Coding编码范式，区别于传统的AI辅助开发方法，提出其对编程文化和开发者角色的新影响，同时指出其带来的机遇与挑战。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能特别是大型语言模型的进步，为软件开发实践带来了新机遇，促使研究VC这种强调直觉、情感驱动和即兴互动的新范式。

Method: 通过五次半结构化访谈，收集十名经验丰富的软件从业者的观点，采用主题分析识别了VC的五个维度。

Result: 研究揭示了VC在创造力、可持续性、合作及批判等方面的特点，并提出VC带来了表达和快速原型设计的新形式，同时存在复现性、可扩展性和包容性的挑战。

Conclusion: Vibe Coding(V.C.)作为一种新的编码范式，重新定义了开发者与AI系统的互动方式，模糊了专业开发者与非专业开发者的界限，对编程文化产生了深远影响。

Abstract: Recent advancements in generative artificial intelligence (GenAI),
particularly large language models, have introduced new possibilities for
software development practices. In our paper we investigate the emerging Vibe
Coding (VC) paradigm that emphasizes intuitive, affect-driven, and
improvisational interactions between developers and AI systems. Building upon
the discourse of End-User Development (EUD), we explore how VC diverges from
conventional programming approaches such as those supported by tools like
GitHub Copilot. Through five semi-structured interview sessions with ten
experienced software practitioners, we identify five thematic dimensions:
creativity, sustainability, the future of programming, collaboration, and
criticism. Our analysis conceptualizes VC within the metaphor of co-drifting,
contrasting it with the prevalent co-piloting perspective of AI-assisted
development. We argue that VC reconfigures the developers role, blurring
boundaries between professional and non-developers. While VC enables novel
forms of expression and rapid prototyping, it also introduces challenges
regarding reproducibility, scalability, and inclusivity. We propose that VC
represents a meaningful shift in programming culture, warranting further
investigation within human-computer interaction (HCI) and software engineering
research.

</details>


### [77] [Should I Run My Cloud Benchmark on Black Friday?](https://arxiv.org/abs/2510.12397)
*Sören Henning,Adriano Vogel,Esteban Perez-Wohlfeil,Otmar Ertl,Rick Rabiser*

Main category: cs.SE

TL;DR: 通过大规模、多时间点基准测试，研究云环境下性能波动及重大事件对基准结果的影响，发现波动存在但较轻微，并揭示周期性模式。


<details>
  <summary>Details</summary>
Motivation: 云环境中的基准测试结果经常因性能波动大而令人质疑其可重复性和可信度。

Method: 过去几个月在不同时间多次执行流处理应用基准测试，通过大规模实证分析性能波动。

Result: 确认性能波动存在且明显，但比预期的要小，发现了细微的日常和每周性能模式。

Conclusion: 性能基准测试结果虽受云环境影响，且存在日常/周过程模式，重大事件（如黑色星期五）也可能影响基准结果。

Abstract: Benchmarks and performance experiments are frequently conducted in cloud
environments. However, their results are often treated with caution, as the
presumed high variability of performance in the cloud raises concerns about
reproducibility and credibility. In a recent study, we empirically quantified
the impact of this variability on benchmarking results by repeatedly executing
a stream processing application benchmark at different times of the day over
several months. Our analysis confirms that performance variability is indeed
observable at the application level, although it is less pronounced than often
assumed. The larger scale of our study compared to related work allowed us to
identify subtle daily and weekly performance patterns. We now extend this
investigation by examining whether a major global event, such as Black Friday,
affects the outcomes of performance benchmarks.

</details>


### [78] [DarTwin made precise by SysMLv2 -- An Experiment](https://arxiv.org/abs/2510.12478)
*Øystein Haugen,Stefan Klikovits,Martin Arthur Andersen,Jonathan Beaulieu,Francis Bordeleau,Joachim Denil,Joost Mertens*

Main category: cs.SE

TL;DR: 本文利用SysMLv2的新特性开发了支持数字孪生演化的领域专用语言DarTwin DSL，展示了其在模型驱动工程中的应用价值，且指出了现有工具的改进空间。


<details>
  <summary>Details</summary>
Motivation: 推动SysMLv2对领域专用语言的支持，以促进数字孪生演化模板的广泛应用及与现有系统描述的无缝接口。

Method: 通过评估SysMLv2的新机制，开发了基于SysMLv2的DarTwin DSL来形式化描述数字孪生的演化，并结合具体用例进行验证。

Result: 成功实现了DarTwin DSL，验证了SysMLv2支持领域专用语言的潜力，同时发现当前SysMLv2工具在图形表示上存在不足。

Conclusion: SysMLv2的新特性支持领域专用语言的构建，为数字孪生（DT）演化的模型驱动工程提供了系统化的技术手段，但现有工具在图形符号支持方面仍有限。

Abstract: The new SysMLv2 adds mechanisms for the built-in specification of
domain-specific concepts and language extensions. This feature promises to
facilitate the creation of Domain-Specific Languages (DSLs) and interfacing
with existing system descriptions and technical designs. In this paper, we
review these features and evaluate SysMLv2's capabilities using concrete use
cases. We develop DarTwin DSL, a DSL that formalizes the existing DarTwin
notation for Digital Twin (DT) evolution, through SysMLv2, thereby supposedly
enabling the wide application of DarTwin's evolution templates using any
SysMLv2 tool. We demonstrate DarTwin DSL, but also point out limitations in the
currently available tooling of SysMLv2 in terms of graphical notation
capabilities. This work contributes to the growing field of Model-Driven
Engineering (MDE) for DTs and combines it with the release of SysMLv2, thus
integrating a systematic approach with DT evolution management in systems
engineering.

</details>


### [79] [Diff-XYZ: A Benchmark for Evaluating Diff Understanding](https://arxiv.org/abs/2510.12487)
*Evgeniy Glukhov,Michele Conti,Egor Bogomolov,Yaroslav Golubev,Alexander Bezzubov*

Main category: cs.SE

TL;DR: 提出了Diff-XYZ代码差异理解基准，包含三个任务和真实代码提交数据，通过实验发现差异格式对不同模型和任务有显著影响，促进代码编辑模型的发展。


<details>
  <summary>Details</summary>
Motivation: 在大规模编辑和重构代码库的代理中，可靠处理代码差异（diff）是核心问题。

Method: 引入了Diff-XYZ，这是一个针对代码差异理解的紧凑基准，包含三个监督任务：应用差异生成新代码、反向应用差异生成旧代码以及差异生成。基准中的实例来源于真实提交的代码，配有自动评价指标和清晰的评估协议，并对统一diff格式和不同差异表示进行了系统性比较。

Result: 发现不同的差异格式应根据用例和模型大小选择，如搜索-替换格式适合较大的模型用于差异生成，但对差异分析和较小模型效果不佳。基准可支持未来模型和差异格式的发展。

Conclusion: Diff-XYZ基准为评估和提升大语言模型在代码差异处理上的能力提供了有力工具，有助于推动代码差异格式及编辑模型的进一步研究与优化。

Abstract: Reliable handling of code diffs is central to agents that edit and refactor
repositories at scale. We introduce Diff-XYZ, a compact benchmark for code-diff
understanding with three supervised tasks: apply (old code $+$ diff
$\rightarrow$ new code), anti-apply (new code $-$ diff $\rightarrow$ old code),
and diff generation (new code $-$ old code $\rightarrow$ diff). Instances in
the benchmark are triples $\langle \textit{old code}, \textit{new code},
\textit{diff} \rangle$ drawn from real commits in CommitPackFT, paired with
automatic metrics and a clear evaluation protocol. We use the benchmark to do a
focused empirical study of the unified diff format and run a cross-format
comparison of different diff representations. Our findings reveal that
different formats should be used depending on the use case and model size. For
example, representing diffs in search-replace format is good for larger models
in the diff generation scenario, yet not suited well for diff analysis and
smaller models. The Diff-XYZ benchmark is a reusable foundation for assessing
and improving diff handling in LLMs that can aid future development of diff
formats and models editing code. The dataset is published on HuggingFace Hub:
https://huggingface.co/datasets/JetBrains-Research/diff-xyz.

</details>


### [80] [The EmpathiSEr: Development and Validation of Software Engineering Oriented Empathy Scales](https://arxiv.org/abs/2510.12546)
*Hashini Gunatilake,John Grundy,Rashina Hoda,Ingo Mueller*

Main category: cs.SE

TL;DR: 针对软件工程领域的特殊情境，本文开发了首个经过验证的同理心测量量表，填补了现有工具的不适用性，促进对软件工程中同理心的评估和培养。


<details>
  <summary>Details</summary>
Motivation: 现有通用同理心量表难以准确反映软件工程中不同角色和领域所特有的同理心表现，缺乏针对性的测量工具。

Method: 采用多阶段方法，包括专家评估、认知访谈和两轮从业者调查来设计和验证量表。

Result: 成功构建了基于实践者反馈的包括认知同理心、情感同理心和同理心反应三个维度的测量量表，并进行了心理计量学验证。

Conclusion: 本文开发并验证了两个专门针对软件工程领域的同理心量表：EmpathiSEr-P和EmpathiSEr-U，首次提供了心理计量学验证的测量工具。

Abstract: Empathy plays a critical role in software engineering (SE), influencing
collaboration, communication, and user-centred design. Although SE research has
increasingly recognised empathy as a key human aspect, there remains no
validated instrument specifically designed to measure it within the unique
socio-technical contexts of SE. Existing generic empathy scales, while
well-established in psychology and healthcare, often rely on language,
scenarios, and assumptions that are not meaningful or interpretable for
software practitioners. These scales fail to account for the diverse,
role-specific, and domain-bound expressions of empathy in SE, such as
understanding a non-technical user's frustrations or another practitioner's
technical constraints, which differ substantially from empathy in clinical or
everyday contexts. To address this gap, we developed and validated two
domain-specific empathy scales: EmpathiSEr-P, assessing empathy among
practitioners, and EmpathiSEr-U, capturing practitioner empathy towards users.
Grounded in a practitioner-informed conceptual framework, the scales encompass
three dimensions of empathy: cognitive empathy, affective empathy, and empathic
responses. We followed a rigorous, multi-phase methodology, including expert
evaluation, cognitive interviews, and two practitioner surveys. The resulting
instruments represent the first psychometrically validated empathy scales
tailored to SE, offering researchers and practitioners a tool for assessing
empathy and designing empathy-enhancing interventions in software teams and
user interactions.

</details>


### [81] [Evaluating End-User Device Energy Models in Sustainability Reporting of Browser-Based Web Services](https://arxiv.org/abs/2510.12566)
*Maja H. Kirkeby,Timmie Lagermann*

Main category: cs.SE

TL;DR: 本文通过实测多种网站类别在不同设备上的能耗，发现现有简化模型误差大且偏差有规律，建议改进模型以提升网络服务可持续性报告的准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管简化的能源和碳排放模型被广泛采用，其准确性和精确性仍未被充分验证，因此需要对其进行系统评估。

Method: 通过在四种笔记本平台上，执行预设的用户流程，测量购物、预订、导航和新闻等网站类别的实际能耗，评估模型的准确性。

Result: 发现常用的常数功率近似模型与实测能耗偏差显著，且偏差具有系统性，提示需引入基于网站类别和设备特性的参数。

Conclusion: 常用的简化能耗模型存在系统性偏差，不能准确反映不同网站类别、设备和任务特性的实际能耗。

Abstract: Sustainability reporting in web-based services increasingly relies on
simplified energy and carbon models such as the Danish Agency of Digital
Government's Digst framework and the United Kingdom-based DIMPACT model.
Although these models are widely adopted, their accuracy and precision remain
underexplored. This paper presents an empirical study evaluating how well such
models reflect actual energy consumption during realistic user interactions
with common website categories. Energy use was measured across shopping,
booking, navigation, and news services using predefined user flows executed on
four laptop platforms. The results show that the commonly applied
constant-power approximation (P * t) can diverge substantially from measured
energy, depending on website category, device type, and task characteristics.
The findings demonstrate that model deviations are systematic rather than
random and highlight the need for category-aware and device-reflective power
parameters in reproducible sustainability reporting frameworks.

</details>


### [82] [Beyond Postconditions: Can Large Language Models infer Formal Contracts for Automatic Software Verification?](https://arxiv.org/abs/2510.12702)
*Cedric Richter,Heike Wehrheim*

Main category: cs.SE

TL;DR: 本文通过NL2Contract任务利用LLM将自然语言转换为形式功能契约，提升自动软件验证的准确性和实用性，减少误报并捕获真实缺陷。


<details>
  <summary>Details</summary>
Motivation: 自动化软件验证缺乏形式规范导致应用受限，而LLM已显示能从代码中的自然语言提示推测形式后置条件，但仅用后置条件导致验证器产生大量虚假告警。

Method: 提出NL2Contract任务，即用LLM将自然语言转换成形式化功能契约，设计评价指标（健全性、错误区分能力和实际可用性），并用多种LLM进行评估，对比仅生成后置条件的效果。

Result: LLM生成的功能契约对所有输入均健全，表达能力足够区分程序缺陷，且结合前置条件的契约供应给验证器后误报率显著降低，契约与开发者意图一致，有助于捕获真实漏洞。

Conclusion: 使用大型语言模型（LLM）生成的函数前置条件与后置条件（功能契约）不仅有效且能显著减少自动软件验证中的误报，且契约表达能力强，能区分正确与错误的行为。

Abstract: Automatic software verifiers have become increasingly effective at the task
of checking software against (formal) specifications. Yet, their adoption in
practice has been hampered by the lack of such specifications in real world
code. Large Language Models (LLMs) have shown promise in inferring formal
postconditions from natural language hints embedded in code such as function
names, comments or documentation. Using the generated postconditions as
specifications in a subsequent verification, however, often leads verifiers to
suggest invalid inputs, hinting at potential issues that ultimately turn out to
be false alarms.
  To address this, we revisit the problem of specification inference from
natural language in the context of automatic software verification. In the
process, we introduce NL2Contract, the task of employing LLMs to translate
informal natural language into formal functional contracts, consisting of
postconditions as well as preconditions. We introduce metrics to validate and
compare different NL2Contract approaches, using soundness, bug discriminative
power of the generated contracts and their usability in the context of
automatic software verification as key metrics. We evaluate NL2Contract with
different LLMs and compare it to the task of postcondition generation
nl2postcond. Our evaluation shows that (1) LLMs are generally effective at
generating functional contracts sound for all possible inputs, (2) the
generated contracts are sufficiently expressive for discriminating buggy from
correct behavior, and (3) verifiers supplied with LLM inferred functional
contracts produce fewer false alarms than when provided with postconditions
alone. Further investigations show that LLM inferred preconditions generally
align well with developers intentions which allows us to use automatic software
verifiers to catch real-world bugs.

</details>
