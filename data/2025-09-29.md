<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 109]
- [cs.MA](#cs.MA) [Total: 7]
- [cs.SE](#cs.SE) [Total: 16]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [A Novel Differential Feature Learning for Effective Hallucination Detection and Classification](https://arxiv.org/abs/2509.21357)
*Wenkai Wang,Vincent Lee,Yizhen Zheng*

Main category: cs.CL

TL;DR: 本论文提出一种双模型架构，通过投影融合块和差异特征学习机制，发现大语言模型中的幻觉信号集中在极少的特征子集中，实现高效精准的幻觉检测。


<details>
  <summary>Details</summary>
Motivation: 由于训练数据的分布偏差，大语言模型生成内容时会出现幻觉现象，当前定位幻觉信号的位置和高效检测方法尚不明确。

Method: 设计了结合投影融合块的双模型架构，利用差异特征学习机制计算平行编码器的特征差异，从而捕获辨别性特征。

Result: 在HaluEval的问答、对话和摘要数据集上验证，发现幻觉信号在极少的特征维度中集中，浅层特征多样性高，深层特征使用集中，实现仅用1%特征维度即可保持检测性能。

Conclusion: 幻觉信号比预期更集中，该发现推动了计算效率高且准确的幻觉检测系统的发展，有助于降低推理成本。

Abstract: Large language model hallucination represents a critical challenge where
outputs deviate from factual accuracy due to distributional biases in training
data. While recent investigations establish that specific hidden layers exhibit
differences between hallucinatory and factual content, the precise localization
of hallucination signals within layers remains unclear, limiting the
development of efficient detection methods. We propose a dual-model
architecture integrating a Projected Fusion (PF) block for adaptive inter-layer
feature weighting and a Differential Feature Learning (DFL) mechanism that
identifies discriminative features by computing differences between parallel
encoders learning complementary representations from identical inputs. Through
systematic experiments across HaluEval's question answering, dialogue, and
summarization datasets, we demonstrate that hallucination signals concentrate
in highly sparse feature subsets, achieving significant accuracy improvements
on question answering and dialogue tasks. Notably, our analysis reveals a
hierarchical "funnel pattern" where shallow layers exhibit high feature
diversity while deep layers demonstrate concentrated usage, enabling detection
performance to be maintained with minimal degradation using only 1\% of feature
dimensions. These findings suggest that hallucination signals are more
concentrated than previously assumed, offering a pathway toward computationally
efficient detection systems that could reduce inference costs while maintaining
accuracy.

</details>


### [2] [Influence Guided Context Selection for Effective Retrieval-Augmented Generation](https://arxiv.org/abs/2509.21359)
*Jiale Deng,Yanyan Shen,Ziyuan Pei,Youmin Chen,Linpeng Huang*

Main category: cs.CL

TL;DR: 本文提出了一种基于上下文影响值(CI值)的新型上下文质量评估方法，提升了检索增强生成(RAG)模型的性能，有效过滤噪声信息。


<details>
  <summary>Details</summary>
Motivation: 现有RAG模型因检索到的上下文中包含无关或噪声信息，导致生成结果失真，且现有基于预定义指标的上下文选择效果有限，未能全面利用查询、上下文列表和生成器信息进行质量评估。

Method: 将上下文质量评估视为推理时数据价值评估问题，提出上下文影响值(CI值)指标，通过测量移除上下文后性能下降量来量化质量，集成了查询相关性、列表唯一性和生成器对齐度；设计参数化代理模型预测CI值，采用分层架构捕捉局部和全局特征，结合CI值监督和生成器反馈训练。

Result: 在8个自然语言处理任务和多种大语言模型上实验，所提方法显著优于最新基线，能有效过滤低质量上下文同时保留关键信息。

Conclusion: CI值作为一种综合性的上下文质量评估指标，有效增强了RAG模型的生成质量和鲁棒性，简化了上下文选择流程，具有广泛应用潜力。

Abstract: Retrieval-Augmented Generation (RAG) addresses large language model (LLM)
hallucinations by grounding responses in external knowledge, but its
effectiveness is compromised by poor-quality retrieved contexts containing
irrelevant or noisy information. While existing approaches attempt to improve
performance through context selection based on predefined context quality
assessment metrics, they show limited gains over standard RAG. We attribute
this limitation to their failure in holistically utilizing available
information (query, context list, and generator) for comprehensive quality
assessment. Inspired by recent advances in data selection, we reconceptualize
context quality assessment as an inference-time data valuation problem and
introduce the Contextual Influence Value (CI value). This novel metric
quantifies context quality by measuring the performance degradation when
removing each context from the list, effectively integrating query-aware
relevance, list-aware uniqueness, and generator-aware alignment. Moreover, CI
value eliminates complex selection hyperparameter tuning by simply retaining
contexts with positive CI values. To address practical challenges of label
dependency and computational overhead, we develop a parameterized surrogate
model for CI value prediction during inference. The model employs a
hierarchical architecture that captures both local query-context relevance and
global inter-context interactions, trained through oracle CI value supervision
and end-to-end generator feedback. Extensive experiments across 8 NLP tasks and
multiple LLMs demonstrate that our context selection method significantly
outperforms state-of-the-art baselines, effectively filtering poor-quality
contexts while preserving critical information. Code is available at
https://github.com/SJTU-DMTai/RAG-CSM.

</details>


### [3] [Context Is What You Need: The Maximum Effective Context Window for Real World Limits of LLMs](https://arxiv.org/abs/2509.21361)
*Norman Paulsen*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型的最大有效上下文窗口大小，发现与报告的最大上下文窗口有显著差异，并且有效窗口大小随问题类型变化。


<details>
  <summary>Details</summary>
Motivation: 当前LLM提供商宣称的最大上下文窗口大小与模型实际有效利用的上下文长度存在偏差，缺乏对上下文窗口实际效用的深入评估。

Method: 定义了最大有效上下文窗口的概念，设计了测试方法评估不同上下文窗口大小和问题类型下模型的表现，收集大量数据进行模型效能比较。

Result: 实验数据显示模型在上下文中仅100个token时部分模型就失败，1000个token时准确率显著下降，实际最大有效上下文窗口远小于报告的最大上下文窗口，且有效窗口大小随问题类型变化。

Conclusion: 实际最大有效上下文窗口与报告值差距大，应根据具体问题类型调整模型使用策略，以提升准确率并减少模型幻觉。

Abstract: Large language model (LLM) providers boast big numbers for maximum context
window sizes. To test the real world use of context windows, we 1) define a
concept of maximum effective context window, 2) formulate a testing method of a
context window's effectiveness over various sizes and problem types, and 3)
create a standardized way to compare model efficacy for increasingly larger
context window sizes to find the point of failure. We collected hundreds of
thousands of data points across several models and found significant
differences between reported Maximum Context Window (MCW) size and Maximum
Effective Context Window (MECW) size. Our findings show that the MECW is, not
only, drastically different from the MCW but also shifts based on the problem
type. A few top of the line models in our test group failed with as little as
100 tokens in context; most had severe degradation in accuracy by 1000 tokens
in context. All models fell far short of their Maximum Context Window by as
much as 99 percent. Our data reveals the Maximum Effective Context Window
shifts based on the type of problem provided, offering clear and actionable
insights into how to improve model accuracy and decrease model hallucination
rates.

</details>


### [4] [How Large Language Models Need Symbolism](https://arxiv.org/abs/2509.21404)
*Xiaotie Deng,Hanyu Li*

Main category: cs.CL

TL;DR: 大型语言模型的发展不仅仅依赖规模扩展，更需要人类设计的符号来引导其智能，提高发现能力。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型虽强大，但缺乏明确指引，限制了真正创新的产生。

Method: 提出将人类设计的符号系统作为指南，结合语言模型的直觉进行推理和发现。

Result: 引入符号引导后，大语言模型在创新发现方面表现更佳。

Conclusion: 大型语言模型的未来发展应结合人类符号体系，实现更深入的智能创新。

Abstract: We argue that AI's future requires more than scaling. To unlock genuine
discovery, large language models need a compass: human-crafted symbols to guide
their powerful but blind intuition.

</details>


### [5] [Multi-Objective Reinforcement Learning for Large Language Model Optimization: Visionary Perspective](https://arxiv.org/abs/2509.21613)
*Lingxiao Kong,Cong Yang,Oya Deniz Beyan,Zeyd Boukhers*

Main category: cs.CL

TL;DR: 本文介绍了多目标强化学习（MORL）在大规模语言模型（LLM）优化中的应用，提出了MORL分类法和基准框架，展望了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 多目标强化学习在优化大规模语言模型时面临的挑战和机遇，尤其是需要高效灵活的方法以适应个性化功能和复杂性。

Method: 提出了MORL的分类法，分析不同方法的优缺点，并设计了一个基准框架来评估不同方法对目标关系的影响。

Result: 阐明了现有MORL方法在LLM优化中的适用性，强调了meta-policy MORL双层学习范式在提升效率和灵活性方面的潜力。

Conclusion: 未来研究应聚焦于meta-policy MORL的发展，通过其双层学习机制提升LLM性能，同时解决相关关键问题。

Abstract: Multi-Objective Reinforcement Learning (MORL) presents significant challenges
and opportunities for optimizing multiple objectives in Large Language Models
(LLMs). We introduce a MORL taxonomy and examine the advantages and limitations
of various MORL methods when applied to LLM optimization, identifying the need
for efficient and flexible approaches that accommodate personalization
functionality and inherent complexities in LLMs and RL. We propose a vision for
a MORL benchmarking framework that addresses the effects of different methods
on diverse objective relationships. As future research directions, we focus on
meta-policy MORL development that can improve efficiency and flexibility
through its bi-level learning paradigm, highlighting key research questions and
potential solutions for improving LLM performance.

</details>


### [6] [One Model, Many Morals: Uncovering Cross-Linguistic Misalignments in Computational Moral Reasoning](https://arxiv.org/abs/2509.21443)
*Sualeha Farid,Jayden Lin,Zean Chen,Shivani Kumar,David Jurgens*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在多语言和多文化环境中的道德决策能力，发现其道德判断因语言和文化差异存在显著不一致。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型主要以英文数据预训练，导致其在多语言文化背景下的道德推理能力存在严重不足。

Method: 对两个人类道德推理基准进行多语言翻译（五种语言），并在零-shot条件下测试模型表现，分析道德判断差异及其背后的原因。

Result: 发现模型在不同语言中产生显著不一致的道德判断，这些差异源于文化不匹配和模型推理策略不同。通过案例研究还发现预训练数据对模型道德观的影响。

Conclusion: 需要开发更加关注文化多样性的人工智能系统，避免现有模型的道德推理误差。

Abstract: Large Language Models (LLMs) are increasingly deployed in multilingual and
multicultural environments where moral reasoning is essential for generating
ethically appropriate responses. Yet, the dominant pretraining of LLMs on
English-language data raises critical concerns about their ability to
generalize judgments across diverse linguistic and cultural contexts. In this
work, we systematically investigate how language mediates moral decision-making
in LLMs. We translate two established moral reasoning benchmarks into five
culturally and typologically diverse languages, enabling multilingual zero-shot
evaluation. Our analysis reveals significant inconsistencies in LLMs' moral
judgments across languages, often reflecting cultural misalignment. Through a
combination of carefully constructed research questions, we uncover the
underlying drivers of these disparities, ranging from disagreements to
reasoning strategies employed by LLMs. Finally, through a case study, we link
the role of pretraining data in shaping an LLM's moral compass. Through this
work, we distill our insights into a structured typology of moral reasoning
errors that calls for more culturally-aware AI.

</details>


### [7] [Following the TRACE: A Structured Path to Empathetic Response Generation with Multi-Agent Models](https://arxiv.org/abs/2509.21849)
*Ziqi Liu,Ziyang Zhou,Yilin Li,Haiyang Zhang,Yangbin Chen*

Main category: cs.CL

TL;DR: 提出了TRACE框架，通过任务分解实现共情响应生成，结合深度分析与流畅生成，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有共情响应模型在分析深度与生成流畅性之间存在权衡，难以兼顾两者优势。

Method: TRACE将共情任务分解为分析和生成两个阶段，先进行全面理解再生成响应，实现结构化认知处理。

Result: 实验表明TRACE在自动评测和基于大语言模型的评测中均显著超越强基线。

Conclusion: 结构化的任务分解方法有效提升了共情生成能力和模型可解释性，是构建更先进共情对话系统的有前景范式。

Abstract: Empathetic response generation is a crucial task for creating more human-like
and supportive conversational agents. However, existing methods face a core
trade-off between the analytical depth of specialized models and the generative
fluency of Large Language Models (LLMs). To address this, we propose TRACE,
Task-decomposed Reasoning for Affective Communication and Empathy, a novel
framework that models empathy as a structured cognitive process by decomposing
the task into a pipeline for analysis and synthesis. By building a
comprehensive understanding before generation, TRACE unites deep analysis with
expressive generation. Experimental results show that our framework
significantly outperforms strong baselines in both automatic and LLM-based
evaluations, confirming that our structured decomposition is a promising
paradigm for creating more capable and interpretable empathetic agents. Our
code is available at https://anonymous.4open.science/r/TRACE-18EF/README.md.

</details>


### [8] [LLM-Based Support for Diabetes Diagnosis: Opportunities, Scenarios, and Challenges with GPT-5](https://arxiv.org/abs/2509.21450)
*Gaurav Kumar Gupta,Nirajan Acharya,Pranal Pande*

Main category: cs.CL

TL;DR: 本研究评估了GPT-5在糖尿病诊断与管理中的应用，结果显示其诊断与解读符合美国糖尿病协会标准，有助于临床和患者使用。


<details>
  <summary>Details</summary>
Motivation: 糖尿病作为全球重大健康挑战，早期识别困难，需借助先进工具提升诊断和监测能力。

Method: 构建基于合成病例的仿真框架，结合多公共数据库数据，测试GPT-5在五种糖尿病相关场景中的表现，包括症状识别、实验室解读、妊娠糖尿病筛查、远程监测和并发症检测。

Result: GPT-5分类准确，能生成临床推理和患者友好解释，并输出结构化数据，表现与美国糖尿病协会标准高度一致。

Conclusion: GPT-5有潜力成为既服务临床医生又便于患者理解的双重工具，同时强调建立可重复评估框架以保障大型语言模型在医疗中的负责使用。

Abstract: Diabetes mellitus is a major global health challenge, affecting over half a
billion adults worldwide with prevalence projected to rise. Although the
American Diabetes Association (ADA) provides clear diagnostic thresholds, early
recognition remains difficult due to vague symptoms, borderline laboratory
values, gestational complexity, and the demands of long-term monitoring.
Advances in large language models (LLMs) offer opportunities to enhance
decision support through structured, interpretable, and patient-friendly
outputs. This study evaluates GPT-5, the latest generative pre-trained
transformer, using a simulation framework built entirely on synthetic cases
aligned with ADA Standards of Care 2025 and inspired by public datasets
including NHANES, Pima Indians, EyePACS, and MIMIC-IV. Five representative
scenarios were tested: symptom recognition, laboratory interpretation,
gestational diabetes screening, remote monitoring, and multimodal complication
detection. For each, GPT-5 classified cases, generated clinical rationales,
produced patient explanations, and output structured JSON summaries. Results
showed strong alignment with ADA-defined criteria, suggesting GPT-5 may
function as a dual-purpose tool for clinicians and patients, while underscoring
the importance of reproducible evaluation frameworks for responsibly assessing
LLMs in healthcare.

</details>


### [9] [Diagnosing the Performance Trade-off in Moral Alignment: A Case Study on Gender Stereotypes](https://arxiv.org/abs/2509.21456)
*Guangliang Liu,Bocheng Chen,Xitong Zhang,Kristen Marie Johnson*

Main category: cs.CL

TL;DR: 该论文研究了通过道德对齐调控预训练语言模型时，在减轻性别偏见和保持模型表现之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 目前通过道德对齐调整模型行为时，通常会降低模型在下游任务中的表现，这限制了其实用性。

Method: 通过分析遗忘机制和公平性目标，探究减少性别刻板印象时性能权衡的内在原因。

Result: 研究发现性能下降主要由整体遗忘水平驱动，有选择地遗忘性别刻板印象会加剧遗忘，同时现有减少遗忘的通用方法效果有限。

Conclusion: 当前的公平性目标难以在减轻性别偏见和保持下游任务性能之间取得理想权衡，需要更有效的方案来减少遗忘，提升模型实际应用效果。

Abstract: Moral alignment has emerged as a widely adopted approach for regulating the
behavior of pretrained language models (PLMs), typically through fine-tuning or
model editing on curated datasets. However, this process often comes at the
cost of degraded downstream task performance. Prior studies commonly aim to
achieve a performance trade-off by encouraging PLMs to selectively forget
stereotypical knowledge through carefully designed fairness objectives, while
preserving their helpfulness. In this short paper, we investigate the
underlying mechanisms of the performance trade-off in the context of mitigating
gender stereotypes, through the lens of forgetting and the fairness objective.
Our analysis reveals the limitations of current fairness objective in achieving
trade-off by demonstrating that: (1) downstream task performance is primarily
driven by the overall forgetting level; (2) selective forgetting of stereotypes
tends to increase overall forgetting; and (3) general solutions for mitigating
forgetting are ineffective at reducing overall forgetting and fail to improve
downstream task performance.

</details>


### [10] [A State-of-the-Art SQL Reasoning Model using RLVR](https://arxiv.org/abs/2509.21459)
*Alnur Ali,Ashutosh Baheti,Jonathan Chang,Ta-Chung Chi,Brandon Cui,Andrew Drozdov,Jonathan Frankle,Abhay Gupta,Pallavi Koppol,Sean Kulinski,Jonathan Li,Dipendra Misra,Krista Opsahl-Ong,Jose Javier Gonzalez Ortiz,Matei Zaharia,Yue Zhang*

Main category: cs.CL

TL;DR: 本文利用带有可验证奖励的强化学习(RLVR)提升自然语言查询转SQL的准确性，在BIRD基准测试中达到了最新的最高精度。


<details>
  <summary>Details</summary>
Motivation: 针对企业客户面临的问题，开发能融合特定组织知识的定制推理模型，提升实际应用效果。

Method: 采用包括TAO离线强化学习预热和严格的在线RLVR训练的简易通用训练流程，并通过仔细的提示设计和模型选择进行优化。

Result: 首次提交即在BIRD私有测试集上达到73.56%的准确率（无自洽）和75.68%（有自洽），模型生成次数低于第二名。

Conclusion: 该简单框架不仅在代理任务BIRD中表现优异，也具备向商业智能、数据科学和编码等企业领域广泛应用的潜力。

Abstract: Developing custom reasoning models via Reinforcement Learning (RL) that can
incorporate organization-specific knowledge has great potential to address
problems faced by enterprise customers. In many of these problems, the reward
function is verifiable, a setting termed RL with Verifiable Rewards (RLVR). We
apply RLVR to a popular data science benchmark called BIRD that measures the
ability of an AI agent to convert a natural language query for a database to
SQL executions. We apply a simple and general-purpose training recipe involving
careful prompt and model selection, a warm-up stage using our offline RL
approach called TAO, followed by rigorous online RLVR training. With no
additional training data beyond the BIRD training set and no use of proprietary
models, our very first submission to the BIRD leaderboard reached
state-of-the-art accuracy on the private test set: 73.56% without
self-consistency and 75.68% with self-consistency. In the latter case, our
model also required fewer generations than the second-best approach. While BIRD
is only a proxy task, the simplicity of our framework makes it broadly
applicable to enterprise domains such as business intelligence, data science,
and coding.

</details>


### [11] [Learning to Reason with Mixture of Tokens](https://arxiv.org/abs/2509.21482)
*Adit Jain,Brendan Rappazzo*

Main category: cs.CL

TL;DR: 本文提出了一种在强化学习框架中使用混合词令牌生成（MoT-G）以提升大语言模型推理能力的方法，显著提高了推理任务的准确率和训练效率。


<details>
  <summary>Details</summary>
Motivation: 当前基于验证奖励的强化学习方法在推理过程中丢失了模型概率分布中的丰富信息，仅采样离散的词令牌，限制了推理空间的探索。

Method: 提出了统一的MoT-G框架，将现有的训练无关的混合词令牌生成方法融入强化学习，直接在连续混合空间中生成推理链。

Result: 在Reasoning-Gym的多个推理任务中，MoT-G方法相比传统解码方法提升了5-35%的准确率，同时使用的轨迹数减半，实现训练效率提升。

Conclusion: MoT-G方法通过保持推理过程中更高的隐藏状态熵和促进词令牌空间探索，显著提升了推理性能和效率。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has become a leading
approach for improving large language model (LLM) reasoning capabilities. Most
current methods follow variants of Group Relative Policy Optimization, which
samples multiple reasoning completions, scores them relative to each other, and
adjusts the policy accordingly. However, these approaches invariably sample
discrete tokens at each reasoning step, discarding the rich distributional
information in the model's probability distribution over candidate tokens.
While preserving and utilizing this distributional information has proven
beneficial in non-RL settings, current RLVR methods seem to be unnecessarily
constraining the reasoning search space by not using this information. To
address this limitation, we investigate mixture-of-token generation (MoT-G) in
RLVR. We present a unified framework that generalizes existing MoT-G
approaches, including existing training-free methods that construct mixture
embeddings as weighted sums over token embeddings, and extend RLVR to operate
directly in this continuous mixture space for generating chain-of-thought.
Evaluating two MoT-G variants on Reasoning-Gym, a suite of reasoning-intensive
language tasks, we find that MoT--G methods achieve substantial improvements
(5--35 \% gains on 7 out of 10 tasks) compared to standard decoding with the
Qwen2.5-1.5B model, while reaching comparable accuracy with half the number of
trajectories, suggesting improved training efficiency. Through comprehensive
hidden-state and token-level analyses, we provide evidence that MoT--G's
benefits may stem from its ability to maintain higher hidden-state entropy
throughout the reasoning process and promote exploration in token space.

</details>


### [12] [Dual-Head Reasoning Distillation: Improving Classifier Accuracy with Train-Time-Only Reasoning](https://arxiv.org/abs/2509.21487)
*Jillian Xu,Dylan Zhou,Vinay Shukla,Yang Yang,Junrui Ruan,Shuhuai Lin,Wenfei Zou,Yinxiao Liu,Karthik Lakshmanan*

Main category: cs.CL

TL;DR: 该论文提出了双头推理蒸馏（DHRD）方法，提高了分类准确率同时显著提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 传统的链式思维（CoT）提示能提升分类效果，但生成推理过程大幅降低推理速度。

Method: 设计了带有分类头和推理头的双头模型，推理头在训练时使用教师推理指导，推理头在测试时关闭，使用加权损失函数训练。

Result: 在SuperGLUE的七个任务上，相较基线方法提升0.65%-5.47%，特别是在蕴含和因果任务上表现更优，推理速度比CoT快96-142倍。

Conclusion: DHRD有效解决了CoT推理准确率和推理速度之间的权衡，实现了高准确率和高吞吐量的分类性能。

Abstract: Chain-of-Thought (CoT) prompting often improves classification accuracy, but
it introduces a significant throughput penalty with rationale generation (Wei
et al., 2022; Cheng and Van Durme, 2024). To resolve this trade-off, we
introduce Dual-Head Reasoning Distillation (DHRD), a simple training method for
decoder-only language models (LMs) that adds (i) a pooled classification head
used during training and inference and (ii) a reasoning head supervised by
teacher rationales used only in training. We train with a loss function that is
a weighted sum of label cross-entropy and token-level LM loss over
input-plus-rationale sequences. On seven SuperGLUE tasks, DHRD yields relative
gains of 0.65-5.47% over pooled baselines, with notably larger gains on
entailment/causal tasks. Since we disable the reasoning head at test time,
inference throughput matches pooled classifiers and exceeds CoT decoding on the
same backbones by 96-142 times in QPS.

</details>


### [13] [On Code-Induced Reasoning in LLMs](https://arxiv.org/abs/2509.21499)
*Abdul Waheed,Zhen Wu,Carolyn Rosé,Daphne Ippolito*

Main category: cs.CL

TL;DR: 本文通过构建十种编程语言的平行指令数据集和控制性扰动，系统性分析了代码的结构和语义特性对大型语言模型推理能力的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管代码数据可以提升大型语言模型的推理能力，但哪些代码属性起主要作用尚不明确，因此作者希望通过系统的方法揭示代码的关键影响因素。

Method: 构建不同编程语言的指令数据集，引入结构和语义扰动，针对五个模型族和多种规模进行微调，并在自然语言、数学和代码任务上评估模型性能。

Result: 结果表明，模型对结构扰动比语义扰动更敏感，伪代码和流程图等抽象表达形式能够有效替代代码，且不拘泥于原始语法的表达有时甚至提升性能。不同语法风格对任务效果有差异，Python更有利于自然语言推理，而Java和Rust更利于数学任务。

Conclusion: 通过系统的数据驱动分析，本文揭示了代码结构、语义、抽象表达及语法风格对大型语言模型推理能力的影响，为训练数据设计提供了新的视角。

Abstract: Code data has been shown to enhance the reasoning capabilities of large
language models (LLMs), but it remains unclear which aspects of code are most
responsible. We investigate this question with a systematic, data-centric
framework. We construct parallel instruction datasets in ten programming
languages and apply controlled perturbations that selectively disrupt
structural or semantic properties of code. We then finetune LLMs from five
model families and eight scales on each variant and evaluate their performance
on natural language, math, and code tasks. Across 3,331 experiments, our
results show that LLMs are more vulnerable to structural perturbations than
semantic ones, particularly on math and code tasks. Appropriate abstractions
like pseudocode and flowcharts can be as effective as code, while encoding the
same information with fewer tokens without adhering to original syntax can
often retain or even improve performance. Remarkably, even corrupted code with
misleading signals remains competitive when surface-level regularities persist.
Finally, syntactic styles also shape task-specific gains with Python favoring
natural language reasoning and lower-level languages such as Java and Rust
favoring math. Through our systematic framework, we aim to provide insight into
how different properties of code influence reasoning and inform the design of
training data for enhancing LLM reasoning capabilities.

</details>


### [14] [Agribot: agriculture-specific question answer system](https://arxiv.org/abs/2509.21535)
*Naman Jain,Pranjali Jain,Pratik Kayal,Jayakrishna Sahit,Soham Pachpande,Jayesh Choudhari*

Main category: cs.CL

TL;DR: 本文介绍了一个基于Kisan Call Center数据集的农业聊天机器人系统，能回答农民关于天气、市场价格、植保及政府政策的问题，系统准确率通过优化从56%提升至86%。


<details>
  <summary>Details</summary>
Motivation: 印度农业经济依赖有效的农业信息，农民需要及时准确的农业咨询以提升产量。

Method: 构建基于句子嵌入的聊天机器人模型，结合同义词消除和实体提取技术提升准确率。

Result: 系统准确率由最初的56%提升至86%，能够全天候通过电子设备向农民提供易理解的信息。

Conclusion: 该系统为农民提供便捷农技信息，促进农业发展，同时减轻了呼叫中心工作人员的工作负担。

Abstract: India is an agro-based economy and proper information about agricultural
practices is the key to optimal agricultural growth and output. In order to
answer the queries of the farmer, we have build an agricultural chatbot based
on the dataset from Kisan Call Center. This system is robust enough to answer
queries related to weather, market rates, plant protection and government
schemes. This system is available 24* 7, can be accessed through any electronic
device and the information is delivered with the ease of understanding. The
system is based on a sentence embedding model which gives an accuracy of 56%.
After eliminating synonyms and incorporating entity extraction, the accuracy
jumps to 86%. With such a system, farmers can progress towards easier
information about farming related practices and hence a better agricultural
output. The job of the Call Center workforce would be made easier and the hard
work of various such workers can be redirected to a better goal.

</details>


### [15] [Domain-Aware Speaker Diarization On African-Accented English](https://arxiv.org/abs/2509.21554)
*Chibuzor Okocha,Kelechi Ezema,Christan Grant*

Main category: cs.CL

TL;DR: 本研究考察了非洲口音英语中说话人分离的领域效应，发现临床语音存在领域惩罚，且现有模型难以完全克服。通过轻量级域适应减少错误，但差距依旧。


<details>
  <summary>Details</summary>
Motivation: 研究非洲口音英语中不同领域（普通与临床对话）对说话人分离性能的影响，特别是在严格错误率指标下评估领域惩罚及其成因。

Method: 评估多个模型在一般和临床对话上的表现，采用严格的DER协议计分，进行错误分析，测试基于口音匹配数据细调分割模块的轻量级域适应方法。

Result: 临床语音表现出持续且显著的领域惩罚，主要源于误报和漏检，与短发言和重叠相关；轻量级域适应虽然减少了错误，但未完全消除差距。

Conclusion: 提出了跨领域受控基准、错误分解方法及易复现的适应方案，表明未来需要注重重叠感知的分割和均衡的临床资源。

Abstract: This study examines domain effects in speaker diarization for
African-accented English. We evaluate multiple production and open systems on
general and clinical dialogues under a strict DER protocol that scores overlap.
A consistent domain penalty appears for clinical speech and remains significant
across models. Error analysis attributes much of this penalty to false alarms
and missed detections, aligning with short turns and frequent overlap. We test
lightweight domain adaptation by fine-tuning a segmentation module on
accent-matched data; it reduces error but does not eliminate the gap. Our
contributions include a controlled benchmark across domains, a concise approach
to error decomposition and conversation-level profiling, and an adaptation
recipe that is easy to reproduce. Results point to overlap-aware segmentation
and balanced clinical resources as practical next steps.

</details>


### [16] [Generation-Time vs. Post-hoc Citation: A Holistic Evaluation of LLM Attribution](https://arxiv.org/abs/2509.21557)
*Yash Saxena,Raviteja Bommireddy,Ankur Padia,Manas Gaur*

Main category: cs.CL

TL;DR: 该论文分析了大语言模型在高风险领域引用可验证来源的两种策略：生成时引用（G-Cite）和事后引用（P-Cite），并提出了基于检索的事后引用为主的方法推荐。


<details>
  <summary>Details</summary>
Motivation: 在医疗、法律、学术和金融等高风险领域，确保大语言模型引用的人类可验证来源准确无误至关重要，以避免严重后果。针对如何生成引用存在两种策略，论文旨在厘清两者优劣。

Method: 提出了两种引用范式：一是生成时引用（G-Cite），即在回答时同步生成答案与引用；二是事后引用（P-Cite），即先生成答案后再添加或验证引用。通过四个流行的归因数据集，涵盖无检索到高级检索增强的方法，进行了全面评估。

Result: 结果显示两种方法存在覆盖率与引用正确性的权衡。P-Cite方法在覆盖率、正确率和响应时长方面表现平衡，适合高风险应用；G-Cite方法则以高准确率为主，但覆盖率和速度较低，适合精确验证场景。检索技术是提升引用质量的关键因素。

Conclusion: 论文推荐在高风险应用中采取以检索为中心的事后引用优先策略，仅在需要严格声明验证时使用生成时引用策略，以权衡引用覆盖率、正确率和处理速度。相关代码及评价结果已公开。

Abstract: Trustworthy Large Language Models (LLMs) must cite human-verifiable sources
in high-stakes domains such as healthcare, law, academia, and finance, where
even small errors can have severe consequences. Practitioners and researchers
face a choice: let models generate citations during decoding, or let models
draft answers first and then attach appropriate citations. To clarify this
choice, we introduce two paradigms: Generation-Time Citation (G-Cite), which
produces the answer and citations in one pass, and Post-hoc Citation (P-Cite),
which adds or verifies citations after drafting. We conduct a comprehensive
evaluation from zero-shot to advanced retrieval-augmented methods across four
popular attribution datasets and provide evidence-based recommendations that
weigh trade-offs across use cases. Our results show a consistent trade-off
between coverage and citation correctness, with retrieval as the main driver of
attribution quality in both paradigms. P-Cite methods achieve high coverage
with competitive correctness and moderate latency, whereas G-Cite methods
prioritize precision at the cost of coverage and speed. We recommend a
retrieval-centric, P-Cite-first approach for high-stakes applications,
reserving G-Cite for precision-critical settings such as strict claim
verification. Our codes and human evaluation results are available at
https://anonymous.4open.science/r/Citation_Paradigms-BBB5/

</details>


### [17] [Comparative Personalization for Multi-document Summarization](https://arxiv.org/abs/2509.21562)
*Haoyuan Li,Snigdha Chaturvedi*

Main category: cs.CL

TL;DR: 本文提出了ComPSum，一个通过比较用户间偏好差异进行个性化多文档摘要生成的框架，并设计了AuthorMap评价方法和PerMSum数据集进行评估。


<details>
  <summary>Details</summary>
Motivation: 个性化多文档摘要需要精细识别用户偏好差异，传统方法难以有效捕捉不同用户间的微妙差异。

Method: 通过比较给定用户与其他用户的偏好，生成结构化用户分析，进而指导个性化摘要生成。设计了AuthorMap评价框架基于作者归属分析个性化程度。

Result: 在新建的PerMSum数据集上，ComPSum通过AuthorMap评测优于多个强基线模型。

Conclusion: 通过细粒度的用户偏好对比，ComPSum实现了更有效的个性化多文档摘要生成，评价指标显示其优越性。

Abstract: Personalized multi-document summarization (MDS) is essential for meeting
individual user preferences of writing style and content focus for summaries.
In this paper, we propose that for effective personalization, it is important
to identify fine-grained differences between users' preferences by comparing
the given user's preferences with other users' preferences.Motivated by this,
we propose ComPSum, a personalized MDS framework. It first generates a
structured analysis of a user by comparing their preferences with other users'
preferences. The generated structured analysis is then used to guide the
generation of personalized summaries. To evaluate the performance of ComPSum,
we propose AuthorMap, a fine-grained reference-free evaluation framework for
personalized MDS. It evaluates the personalization of a system based on the
authorship attribution between two personalized summaries generated for
different users. For robust evaluation of personalized MDS, we construct
PerMSum, a personalized MDS dataset in the review and news domain. We evaluate
the performance of ComPSum on PerMSum using AuthorMap, showing that it
outperforms strong baselines.

</details>


### [18] [FeatBench: Evaluating Coding Agents on Feature Implementation for Vibe Coding](https://arxiv.org/abs/2509.22237)
*Haorui Chen,Chengze Li,Jia Li*

Main category: cs.CL

TL;DR: 本文提出了用于评估“大语言模型（LLM）”在“vibe编码”中功能实现能力的新基准FeaBench，通过纯自然语言提示、多级筛选数据质量、多样化测试用例及多领域数据集，发现目前模型成功率不足30%，并揭示“激进实现”策略的优劣。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成评测标准未能有效考量基于自然语言的vibe编码功能实现场景，在任务设定和测试覆盖上存在不足。

Method: 设计FeaBench基准，采用纯自然语言描述的任务输入，多层过滤保证数据质量，有F2P和P2P测试用例保护代码正确性和防止回退，涵盖多领域真实应用代码库；并评测四大LLM和两种代理框架。

Result: 评测结果显示vibe编码中的功能实现极具挑战，最高成功率仅29.94%；模型倾向于激进实现，该策略既带来严重失误，也带来更优的软件设计。

Conclusion: FeaBench为vibe编码功能实现的评测提供了新工具，揭示了当前模型的限制和潜在提升方向，代码及数据公开以促进社区研究。

Abstract: The rapid advancement of Large Language Models (LLMs) has given rise to a
novel software development paradigm known as "vibe coding," where users
interact with coding agents through high-level natural language. However,
existing evaluation benchmarks for code generation inadequately assess an
agent's vibe coding capabilities. Existing benchmarks are misaligned, as they
either require code-level specifications or focus narrowly on issue-solving,
neglecting the critical scenario of feature implementation within the vibe
coding paradiam. To address this gap, we propose FeatBench, a novel benchmark
for vibe coding that focuses on feature implementation. Our benchmark is
distinguished by several key features: 1. Pure Natural Language Prompts. Task
inputs consist solely of abstract natural language descriptions, devoid of any
code or structural hints. 2. A Rigorous & Evolving Data Collection Process.
FeatBench is built on a multi-level filtering pipeline to ensure quality and a
fully automated pipeline to evolve the benchmark, mitigating data
contamination. 3. Comprehensive Test Cases. Each task includes Fail-to-Pass
(F2P) and Pass-to-Pass (P2P) tests to verify correctness and prevent
regressions. 4. Diverse Application Domains. The benchmark includes
repositories from diverse domains to ensure it reflects real-world scenarios.
We evaluate two state-of-the-art agent frameworks with four leading LLMs on
FeatBench. Our evaluation reveals that feature implementation within the vibe
coding paradigm is a significant challenge, with the highest success rate of
only 29.94%. Our analysis also reveals a tendency for "aggressive
implementation," a strategy that paradoxically leads to both critical failures
and superior software design. We release FeatBench, our automated collection
pipeline, and all experimental results to facilitate further community
research.

</details>


### [19] [Vision Language Models Cannot Plan, but Can They Formalize?](https://arxiv.org/abs/2509.21576)
*Muyu He,Yuxi Zheng,Yuchen Liu,Zijian An,Bill Cai,Jiani Huang,Lifeng Zhou,Feng Liu,Ziyang Li,Li Zhang*

Main category: cs.CL

TL;DR: 本文提出了五种视觉语言模型(VLM)作为形式化工具的方法，以解决多模态环境下的长程规划问题，并证明该方法优于端到端计划生成。


<details>
  <summary>Details</summary>
Motivation: 现有多模态任务中，VLM在长程规划中的表现有限，且多依赖简化假设，难以处理开放词汇和多视角低质量图像。

Method: 设计五套VLM-as-formalizer流程，进行一次性、开放词汇、多模态PDDL形式化，评测多种基准和真实多视角低质图像集。

Result: 证明VLM作为形式化工具的方法显著优于直接生成动作序列的方法，发现视觉理解是瓶颈，生成中间文本表征有一定提升但不稳定。

Conclusion: VLM-as-formalizer提升多模态长程规划性能，未来研究需聚焦于提升视觉理解，改进多模态规划的形式化能力。

Abstract: The advancement of vision language models (VLMs) has empowered embodied
agents to accomplish simple multimodal planning tasks, but not long-horizon
ones requiring long sequences of actions. In text-only simulations,
long-horizon planning has seen significant improvement brought by repositioning
the role of LLMs. Instead of directly generating action sequences, LLMs
translate the planning domain and problem into a formal planning language like
the Planning Domain Definition Language (PDDL), which can call a formal solver
to derive the plan in a verifiable manner. In multimodal environments, research
on VLM-as-formalizer remains scarce, usually involving gross simplifications
such as predefined object vocabulary or overly similar few-shot examples. In
this work, we present a suite of five VLM-as-formalizer pipelines that tackle
one-shot, open-vocabulary, and multimodal PDDL formalization. We evaluate those
on an existing benchmark while presenting another two that for the first time
account for planning with authentic, multi-view, and low-quality images. We
conclude that VLM-as-formalizer greatly outperforms end-to-end plan generation.
We reveal the bottleneck to be vision rather than language, as VLMs often fail
to capture an exhaustive set of necessary object relations. While generating
intermediate, textual representations such as captions or scene graphs
partially compensate for the performance, their inconsistent gain leaves
headroom for future research directions on multimodal planning formalization.

</details>


### [20] ["Be My Cheese?": Assessing Cultural Nuance in Multilingual LLM Translations](https://arxiv.org/abs/2509.21577)
*Madison Van Doren,Cory Holland*

Main category: cs.CL

TL;DR: 本研究评估了多语言AI模型在跨语言翻译成语和双关语时的本地化能力，发现即使是顶尖模型在文化细节翻译上仍有明显不足。


<details>
  <summary>Details</summary>
Motivation: 现有机器翻译研究多关注语法准确性，忽视文化适应性，而这对于市场营销和电子商务等实际应用至关重要。

Method: 通过分析87个电子商务营销邮件翻译样本，涵盖20种语言的24个地区方言，利用母语评审进行量化评价和定性反馈。

Result: 尽管模型普遍语法正确，但在文化细微差别的翻译上表现不足，尤其是比喻和文字游戏，经常需要人工修改。

Conclusion: 数据量不是机器翻译质量的唯一决定因素，文化适应性同样关键，未来应加强相关研究，提升多语言AI模型的本地化能力。

Abstract: This pilot study explores the localisation capabilities of state-of-the-art
multilingual AI models when translating figurative language, such as idioms and
puns, from English into a diverse range of global languages. It expands on
existing LLM translation research and industry benchmarks, which emphasise
grammatical accuracy and token-level correctness, by focusing on cultural
appropriateness and overall localisation quality - critical factors for
real-world applications like marketing and e-commerce.
  To investigate these challenges, this project evaluated a sample of 87
LLM-generated translations of e-commerce marketing emails across 24 regional
dialects of 20 languages. Human reviewers fluent in each target language
provided quantitative ratings and qualitative feedback on faithfulness to the
original's tone, meaning, and intended audience. Findings suggest that, while
leading models generally produce grammatically correct translations, culturally
nuanced language remains a clear area for improvement, often requiring
substantial human refinement. Notably, even high-resource global languages,
despite topping industry benchmark leaderboards, frequently mistranslated
figurative expressions and wordplay.
  This work challenges the assumption that data volume is the most reliable
predictor of machine translation quality and introduces cultural
appropriateness as a key determinant of multilingual LLM performance - an area
currently underexplored in existing academic and industry benchmarks. As a
proof of concept, this pilot highlights limitations of current multilingual AI
systems for real-world localisation use cases. Results of this pilot support
the opportunity for expanded research at greater scale to deliver generalisable
insights and inform deployment of reliable machine translation workflows in
culturally diverse contexts.

</details>


### [21] [OjaKV: Context-Aware Online Low-Rank KV Cache Compression with Oja's Rule](https://arxiv.org/abs/2509.21623)
*Yuxuan Zhu,David H. Yang,Mohammad Mohammadi Amiri,Keerthiram Murugesan,Tejaswini Pedapati,Pin-Yu Chen*

Main category: cs.CL

TL;DR: 本文提出了OjaKV框架，通过在线子空间适应和混合存储策略，有效压缩并管理大语言模型的KV缓存，解决长上下文生成中的内存瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理超长上下文时，KV缓存占用大量内存，且现有低秩压缩方法因静态子空间对数据分布变化适应差，导致性能下降。

Method: OjaKV保留关键的首尾token以保持高保真度，对中间token采用基于Oja算法的在线PCA进行低秩压缩，动态更新投影基，且兼容FlashAttention等现代注意力机制。

Result: 实验表明，OjaKV在高压缩比下保持甚至提升了零次准确率，特别在需要复杂推理的超长上下文基准测试中表现优异。

Conclusion: OjaKV作为一种实用的插拔式解决方案，无需模型微调即可实现内存高效的长上下文推理，解决了KV缓存瓶颈。

Abstract: The expanding long-context capabilities of large language models are
constrained by a significant memory bottleneck: the key-value (KV) cache
required for autoregressive generation. This bottleneck is substantial; for
instance, a Llama-3.1-8B model processing a 32K-token prompt at a batch size of
4 requires approximately 16GB for its KV cache, a size exceeding the model's
weights. While KV-cache compression via low-rank projection is a promising
direction, existing methods rely on a static, offline-learned subspace that
performs poorly under data distribution shifts. To overcome these limitations,
we introduce OjaKV, a novel framework that integrates a strategic hybrid
storage policy with online subspace adaptation. First, OjaKV recognizes that
not all tokens are equally important for compression; it preserves the crucial
first and most recent tokens in full-rank, maintaining high-fidelity anchors
for attention. Second, for the vast majority of intermediate tokens, it applies
low-rank compression by incrementally adapting the projection basis using Oja's
algorithm for online principal component analysis. This adaptation involves a
comprehensive update during prompt prefilling and lightweight periodic updates
during decoding, ensuring the subspace remains aligned with the evolving
context. Crucially, our framework is fully compatible with modern attention
modules like FlashAttention. Experiments demonstrate that OjaKV maintains or
even improves zero-shot accuracy at high compression ratios. In particular,
OjaKV achieves its strongest gains on very long-context benchmarks that require
complex reasoning, highlighting the importance of online subspace adaptation in
dynamically tracking context shifts. These results establish our hybrid
framework as a practical, plug-and-play solution for memory-efficient
long-context inference without requiring model fine-tuning.

</details>


### [22] [Towards Transparent AI: A Survey on Explainable Language Models](https://arxiv.org/abs/2509.21631)
*Avash Palikhe,Zichong Wang,Zhipeng Yin,Rui Guo,Qiang Duan,Jie Yang,Wenbin Zhang*

Main category: cs.CL

TL;DR: 本综述系统梳理了针对语言模型（LM）的可解释人工智能（XAI）技术，重点分析了不同Transformer架构下的方法及其优缺点，并评估了方法的合理性和可信度，最后提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 语言模型虽然在自然语言处理领域取得显著进展，但其黑箱特性导致内部机制不透明，特别是在高风险领域中，缺乏解释性限制了其应用，因此需要针对语言模型的专门XAI方法。

Method: 通过分类语言模型架构（编码器-only、解码器-only、编码器-解码器）系统综述相关XAI技术，分析这些方法如何适应不同架构，并从合理性和可信度两个角度评估其有效性。

Result: 总结了现有XAI技术在不同语言模型架构中的适配情况，指出当前方法存在各自的优势与局限性，为理解和改进语言模型解释性提供了结构化视角。

Conclusion: 尽管已有进展，但语言模型可解释性仍面临多重挑战。本文指出未来研究方向，旨在促进更健壮、透明且可解释的XAI方法发展，推动语言模型在高风险领域的可靠应用。

Abstract: Language Models (LMs) have significantly advanced natural language processing
and enabled remarkable progress across diverse domains, yet their black-box
nature raises critical concerns about the interpretability of their internal
mechanisms and decision-making processes. This lack of transparency is
particularly problematic for adoption in high-stakes domains, where
stakeholders need to understand the rationale behind model outputs to ensure
accountability. On the other hand, while explainable artificial intelligence
(XAI) methods have been well studied for non-LMs, they face many limitations
when applied to LMs due to their complex architectures, considerable training
corpora, and broad generalization abilities. Although various surveys have
examined XAI in the context of LMs, they often fail to capture the distinct
challenges arising from the architectural diversity and evolving capabilities
of these models. To bridge this gap, this survey presents a comprehensive
review of XAI techniques with a particular emphasis on LMs, organizing them
according to their underlying transformer architectures: encoder-only,
decoder-only, and encoder-decoder, and analyzing how methods are adapted to
each while assessing their respective strengths and limitations. Furthermore,
we evaluate these techniques through the dual lenses of plausibility and
faithfulness, offering a structured perspective on their effectiveness.
Finally, we identify open research challenges and outline promising future
directions, aiming to guide ongoing efforts toward the development of robust,
transparent, and interpretable XAI methods for LMs.

</details>


### [23] [ReviewScore: Misinformed Peer Review Detection with Large Language Models](https://arxiv.org/abs/2509.21679)
*Hyun Ryu,Doohyuk Jang,Hyemin S. Lee,Joonhyun Jeong,Gyeongman Kim,Donghyeon Cho,Gyouk Chu,Minyeong Hwang,Hyeongwon Jang,Changhun Kim,Haechan Kim,Jina Kim,Joowon Kim,Yoonjeon Kim,Kwanhyung Lee,Chanjae Park,Heecheol Yun,Gregor Betz,Eunho Yang*

Main category: cs.CL

TL;DR: 本文提出了用于检测同行评审中低质量评论的ReviewScore指标，通过评估评论中误导性观点的事实准确性，基于大规模语言模型实现自动化判定并验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着AI会议投稿量激增，同行评审质量下降，亟需有效方法检测低质量（误导性）评论点。

Method: 定义误导性评论点为包含错误前提的缺陷或文中已回答的问题，提出自动提取缺陷前提的引擎，构建专家标注数据集，用多款领先大语言模型评估点评事实准确性，并对模型与人工评分一致性进行分析。

Result: 发现15.2%的缺陷和26.4%的问题为误导性，模型对ReviewScore的评估与人工评分呈中等一致性，基于前提级别的事实评估优于缺陷级别。

Conclusion: 基于前提事实性的自动化ReviewScore评估具备较好效果和潜力，能够辅助提高同行评审质量管理。

Abstract: Peer review serves as a backbone of academic research, but in most AI
conferences, the review quality is degrading as the number of submissions
explodes. To reliably detect low-quality reviews, we define misinformed review
points as either "weaknesses" in a review that contain incorrect premises, or
"questions" in a review that can be already answered by the paper. We verify
that 15.2% of weaknesses and 26.4% of questions are misinformed and introduce
ReviewScore indicating if a review point is misinformed. To evaluate the
factuality of each premise of weaknesses, we propose an automated engine that
reconstructs every explicit and implicit premise from a weakness. We build a
human expert-annotated ReviewScore dataset to check the ability of LLMs to
automate ReviewScore evaluation. Then, we measure human-model agreements on
ReviewScore using eight current state-of-the-art LLMs and verify moderate
agreements. We also prove that evaluating premise-level factuality shows
significantly higher agreements than evaluating weakness-level factuality. A
thorough disagreement analysis further supports a potential of fully automated
ReviewScore evaluation.

</details>


### [24] [GRAB: A Risk Taxonomy--Grounded Benchmark for Unsupervised Topic Discovery in Financial Disclosures](https://arxiv.org/abs/2509.21698)
*Ying Li,Tiejun Ma*

Main category: cs.CL

TL;DR: GRAB是一个用于10-K风险披露风险分类的金融专用基准数据集，通过结合FinBERT、YAKE和术语匹配技术产生1.61M带标签句子，支持无监督主题模型的评估。


<details>
  <summary>Details</summary>
Motivation: 现有缺乏针对10-K风险披露的无监督主题模型公开基准，阻碍风险分类的监督和投资分析。

Method: 开发GRAB基准，采用FinBERT注意力机制、YAKE关键词信号和基于风险分类法的共现匹配自动生成句子标签，并定义统一评价指标和固定数据集分割。

Result: 构建了包含1.61M句子和21个细粒度风险类型的基准数据，能在多个指标下评估各类主题模型，促进标准化和可重复性比较。

Conclusion: GRAB为金融风险主题分类提供了标准化、可复现的公开基准，推动无监督主题模型在财报风险披露任务中的应用和评估。

Abstract: Risk categorization in 10-K risk disclosures matters for oversight and
investment, yet no public benchmark evaluates unsupervised topic models for
this task. We present GRAB, a finance-specific benchmark with 1.61M sentences
from 8,247 filings and span-grounded sentence labels produced without manual
annotation by combining FinBERT token attention, YAKE keyphrase signals, and
taxonomy-aware collocation matching. Labels are anchored in a risk taxonomy
mapping 193 terms to 21 fine-grained types nested under five macro classes; the
21 types guide weak supervision, while evaluation is reported at the macro
level. GRAB unifies evaluation with fixed dataset splits and robust
metrics--Accuracy, Macro-F1, Topic BERTScore, and the entropy-based Effective
Number of Topics. The dataset, labels, and code enable reproducible,
standardized comparison across classical, embedding-based, neural, and hybrid
topic models on financial disclosures.

</details>


### [25] [Think-on-Graph 3.0: Efficient and Adaptive LLM Reasoning on Heterogeneous Graphs via Multi-Agent Dual-Evolving Context Retrieval](https://arxiv.org/abs/2509.21710)
*Xiaojun Wu,Cehao Yang,Xueyuan Lin,Chengjin Xu,Xuhui Jiang,Yuanliang Sun,Hui Xiong,Jia Li,Jian Guo*

Main category: cs.CL

TL;DR: 本文提出了Think-on-Graph 3.0 (ToG-3)框架，通过多智能体上下文演化和检索机制（MACER），动态构建和优化异构图索引，以提高基于图的知识检索和生成模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有图增强生成模型依赖静态高质量图结构，手工构建成本高、自动抽取受限，尤其对轻量级模型效果差，限制了检索增强生成的性能。

Method: 提出MACER机制，通过多智能体（构建者、检索者、反思者、响应者）循环演化查询和子图，实现基于Chunk-Triplets-Community的异构图动态构建和优化，针对具体查询不断精炼检索证据和图结构。

Result: 在深度和广度推理基准测试中，ToG-3显著优于现有方法，消融实验证明MACER各组件有效提升了模型性能。

Conclusion: ToG-3克服了静态图索引的局限，实现了轻量级大语言模型下的精准动态图检索与推理，推动了检索增强生成技术的发展。

Abstract: Retrieval-Augmented Generation (RAG) and Graph-based RAG has become the
important paradigm for enhancing Large Language Models (LLMs) with external
knowledge. However, existing approaches face a fundamental trade-off. While
graph-based methods are inherently dependent on high-quality graph structures,
they face significant practical constraints: manually constructed knowledge
graphs are prohibitively expensive to scale, while automatically extracted
graphs from corpora are limited by the performance of the underlying LLM
extractors, especially when using smaller, local-deployed models. This paper
presents Think-on-Graph 3.0 (ToG-3), a novel framework that introduces
Multi-Agent Context Evolution and Retrieval (MACER) mechanism to overcome these
limitations. Our core innovation is the dynamic construction and refinement of
a Chunk-Triplets-Community heterogeneous graph index, which pioneeringly
incorporates a dual-evolution mechanism of Evolving Query and Evolving
Sub-Graph for precise evidence retrieval. This approach addresses a critical
limitation of prior Graph-based RAG methods, which typically construct a static
graph index in a single pass without adapting to the actual query. A
multi-agent system, comprising Constructor, Retriever, Reflector, and Responser
agents, collaboratively engages in an iterative process of evidence retrieval,
answer generation, sufficiency reflection, and, crucially, evolving query and
subgraph. This dual-evolving multi-agent system allows ToG-3 to adaptively
build a targeted graph index during reasoning, mitigating the inherent
drawbacks of static, one-time graph construction and enabling deep, precise
reasoning even with lightweight LLMs. Extensive experiments demonstrate that
ToG-3 outperforms compared baselines on both deep and broad reasoning
benchmarks, and ablation studies confirm the efficacy of the components of
MACER framework.

</details>


### [26] [ProPerSim: Developing Proactive and Personalized AI Assistants through User-Assistant Simulation](https://arxiv.org/abs/2509.21730)
*Jiho Kim,Junseong Choi,Woosog Chay,Daeun Kyung,Yeonsu Kwon,Yohan Jo,Edward Choi*

Main category: cs.CL

TL;DR: 本文提出了ProPerSim任务和模拟框架，旨在开发能够及时、个性化推荐的智能助手，并基于此提出ProPerAssistant，通过用户反馈不断学习和适应。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型逐渐融入日常生活，市场对不仅能被动响应，还能主动且个性化的AI助手需求增长，但主动性与个性化的结合仍未充分探索。

Method: 设计了ProPerSim模拟环境，用户代理带有丰富个性，基于用户对建议的评分，助手通过反馈学习调整策略；提出ProPerAssistant，利用检索增强和偏好对齐机制，实现持续学习与适应。

Result: 在32个多样化人物设定的实验中，ProPerAssistant能够调整策略，提高用户满意度，展现了主动性与个性化结合的潜力。

Conclusion: 将主动性与个性化结合的AI助手具有良好适应能力和用户满意提升效果，表明ProPerSim和ProPerAssistant为智能助手的发展提供了有效路径。

Abstract: As large language models (LLMs) become increasingly integrated into daily
life, there is growing demand for AI assistants that are not only reactive but
also proactive and personalized. While recent advances have pushed forward
proactivity and personalization individually, their combination remains
underexplored. To bridge this gap, we introduce ProPerSim, a new task and
simulation framework for developing assistants capable of making timely,
personalized recommendations in realistic home scenarios. In our simulation
environment, a user agent with a rich persona interacts with the assistant,
providing ratings on how well each suggestion aligns with its preferences and
context. The assistant's goal is to use these ratings to learn and adapt to
achieve higher scores over time. Built on ProPerSim, we propose
ProPerAssistant, a retrieval-augmented, preference-aligned assistant that
continually learns and adapts through user feedback. Experiments across 32
diverse personas show that ProPerAssistant adapts its strategy and steadily
improves user satisfaction, highlighting the promise of uniting proactivity and
personalization.

</details>


### [27] [How Accurate Are LLMs at Multi-Question Answering on Conversational Transcripts?](https://arxiv.org/abs/2509.21732)
*Xiliang Zhu,Shi Zong,David Rossouw*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型（LLMs）在基于相同对话上下文回答多个问题的能力，比较了各类专有和公开模型的表现。


<details>
  <summary>Details</summary>
Motivation: 在工业应用中，基于长上下文进行问答面临高计算成本和延迟，尤其需要处理多个基于相同上下文的问题。

Method: 通过大量实验，评估并对比多种专有和公开的LLM模型在此任务上的性能表现。

Result: 发现顶尖专有模型如GPT-4o整体表现最佳，但参数量最高8亿的微调公开模型在准确率上超过了GPT-4o。

Conclusion: 微调公开模型在准确率和成本效益方面展现了很大潜力，适合实际应用中透明和低成本的部署需求。

Abstract: Deploying Large Language Models (LLMs) for question answering (QA) over
lengthy contexts is a significant challenge. In industrial settings, this
process is often hindered by high computational costs and latency, especially
when multiple questions must be answered based on the same context. In this
work, we explore the capabilities of LLMs to answer multiple questions based on
the same conversational context. We conduct extensive experiments and benchmark
a range of both proprietary and public models on this challenging task. Our
findings highlight that while strong proprietary LLMs like GPT-4o achieve the
best overall performance, fine-tuned public LLMs with up to 8 billion
parameters can surpass GPT-4o in accuracy, which demonstrates their potential
for transparent and cost-effective deployment in real-world applications.

</details>


### [28] [Self-Speculative Biased Decoding for Faster Live Translation](https://arxiv.org/abs/2509.21740)
*Linxiao Zeng,Haoyun Deng,Kangyuan Shu,Shizhen Wang*

Main category: cs.CL

TL;DR: 本文提出了一种名为自我推测偏置解码的推理方法，旨在提升流式应用中大型语言模型的翻译速度，减少重新生成输出的计算开销，同时降低输出波动。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在流式翻译中的应用受限于高延迟和反复生成输出，导致计算成本和输出波动较高，影响用户体验。

Method: 通过将最新输出作为当前输入上下文的草稿，并在验证阶段对草稿标记进行偏置，提高草稿的接受率；采用无草稿计算的推测解码方法，结合展示用的mask-k技术减少波动。

Result: 相比传统自回归重译方法，所提方法在确保翻译质量的前提下实现了最高1.7倍的加速，并显著降低输出波动约80%。

Conclusion: 自我推测偏置解码作为一种模型无关、即插即用的解决方案，有效提升了大语言模型在延迟敏感流式翻译任务中的性能及用户体验。

Abstract: Large Language Models (LLMs) have recently demonstrated impressive
capabilities in various text generation tasks. However, it remains challenging
to use them off-the-shelf in streaming applications (such as live translation),
where the output must continually update as the input context expands, while
still maintaining a reasonable computational cost to meet the latency
requirement.
  In this work, we reexamine the re-translation approach to simultaneous
translation and propose Self-Speculative Biased Decoding, a novel inference
paradigm designed to avoid repeatedly generating output from scratch for a
consistently growing input stream. We propose using the most recent output as a
draft for the current growing input context. During the verification stage, the
output will be biased towards the draft token for a higher draft acceptance
rate. This strategy not only minimizes flickering that might distract users but
also leads to higher speedups. Conventional decoding may take charge from the
point of divergence after draft verification and continue until the end
condition is met.
  Unlike existing speculative decoding strategies, our approach eliminates the
need for draft computations, making it a model-agnostic and plug-and-play
solution for accelerating latency-sensitive streaming applications.
Experimental results on simultaneous text-to-text re-translation demonstrate
that our approach achieves up to 1.7x speedup compared to conventional
auto-regressive re-translation without compromising quality. Additionally, it
significantly reduces flickering by 80% by incorporating the display-only
mask-k technique.

</details>


### [29] [Thinking with Sound: Audio Chain-of-Thought Enables Multimodal Reasoning in Large Audio-Language Models](https://arxiv.org/abs/2509.21749)
*Zhen Xiong,Yujun Cai,Zhecheng Li,Junsong Yuan,Yiwei Wang*

Main category: cs.CL

TL;DR: 本文提出了Thinking-with-Sound (TwS)框架，通过结合语言推理和即时音频分析，提升大型音频语言模型在复杂声学场景下的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有大型音频语言模型在复杂声学环境下的音频推理任务表现较差，缺少对噪声抑制、声源分离等音频工具的访问，影响理解能力。

Method: TwS框架允许模型结合多模态推理，主动处理音频信号，进行数值分析和数字操作，从而实现音频链式思维（Audio CoT）。同时构建了包含多种声学扰动的鲁棒性测试集MELD-Hard1k进行评估。

Result: 在MELD-Hard1k上，最先进的模型性能显著下降超过50%，而TwS框架显著提升鲁棒性，小模型准确率提升24.73%，大模型提升36.61%。

Conclusion: 音频链式思维显著提升了大型音频语言模型的鲁棒性且无需重新训练，为构建更鲁棒的音频理解系统开辟了新方向。

Abstract: Recent Large Audio-Language Models (LALMs) have shown strong performance on
various audio understanding tasks such as speech translation and Audio Q\&A.
However, they exhibit significant limitations on challenging audio reasoning
tasks in complex acoustic scenarios. These situations would greatly benefit
from the use of acoustic tools like noise suppression, source separation, and
precise temporal alignment, but current LALMs lack access to such tools. To
address this limitation, we introduce Thinking-with-Sound (TwS), a framework
that equips LALMs with Audio CoT by combining linguistic reasoning with
on-the-fly audio-domain analysis. Unlike existing approaches that treat audio
as static input, TwS enables models to actively think with audio signals,
performing numerical analysis and digital manipulation through multimodal
reasoning. To evaluate this approach, we construct MELD-Hard1k, a new
robustness benchmark created by introducing various acoustic perturbations.
Experiments reveal that state-of-the-art LALMs suffer dramatic performance
degradation on MELD-Hard1k, with accuracy dropping by more than $50\%$ compared
to clean audio. TwS achieves substantial improvements in robustness,
demonstrating both effectiveness and scalability: small models gain $24.73\%$
absolute accuracy, with improvements scaling consistently up to $36.61\%$ for
larger models. Our findings demonstrate that Audio CoT can significantly
enhance robustness without retraining, opening new directions for developing
more robust audio understanding systems.

</details>


### [30] [SynerGen: Contextualized Generative Recommender for Unified Search and Recommendation](https://arxiv.org/abs/2509.21777)
*Vianne R. Gao,Chen Xue,Marc Versage,Xie Zhou,Zhongruo Wang,Chao Li,Yeon Seonwoo,Nan Chen,Zhen Ge,Gourab Kundu,Weiqi Zhang,Tian Wang,Qingjun Cui,Trishul Chilimbi*

Main category: cs.CL

TL;DR: 本论文提出了SynerGen，一种统一个性化搜索与推荐的生成式推荐模型，在检索和排序任务上均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有大规模推荐系统中的检索-排序管道存在校准误差和工程复杂性，且生成式序列模型在统一个性化搜索和无查询推荐时存在性能权衡。

Method: SynerGen基于decoder-only Transformer，联合使用InfoNCE优化检索，混合点对点与成对损失优化排序，并引入时间感知旋转位置编码改进时间信息融合。

Result: SynerGen在推荐与搜索基准测试上显著超越现有生成式推荐和联合搜索推荐方法。

Conclusion: 单一生成式基础模型可行且高效，实现个性化搜索与推荐的工业级统一信息访问。

Abstract: The dominant retrieve-then-rank pipeline in large-scale recommender systems
suffers from mis-calibration and engineering overhead due to its architectural
split and differing optimization objectives. While recent generative sequence
models have shown promise in unifying retrieval and ranking by
auto-regressively generating ranked items, existing solutions typically address
either personalized search or query-free recommendation, often exhibiting
performance trade-offs when attempting to unify both. We introduce
\textit{SynerGen}, a novel generative recommender model that bridges this
critical gap by providing a single generative backbone for both personalized
search and recommendation, while simultaneously excelling at retrieval and
ranking tasks. Trained on behavioral sequences, our decoder-only Transformer
leverages joint optimization with InfoNCE for retrieval and a hybrid
pointwise-pairwise loss for ranking, allowing semantic signals from search to
improve recommendation and vice versa. We also propose a novel time-aware
rotary positional embedding to effectively incorporate time information into
the attention mechanism. \textit{SynerGen} achieves significant improvements on
widely adopted recommendation and search benchmarks compared to strong
generative recommender and joint search and recommendation baselines. This work
demonstrates the viability of a single generative foundation model for
industrial-scale unified information access.

</details>


### [31] [Navigating the Impact of Structured Output Format on Large Language Models through the Compass of Causal Inference](https://arxiv.org/abs/2509.21791)
*Han Yuan,Yue Zhao,Li Zhang,Wuqiong Luo,Zheng Ma*

Main category: cs.CL

TL;DR: 本文通过因果推断重新分析了结构化输出对大型语言模型生成质量的影响，发现大部分情况下结构化输出无因果影响。


<details>
  <summary>Details</summary>
Motivation: 此前关于结构化输出对大型语言模型生成质量影响的研究存在测试场景有限、对比设置不严谨及评估指标粗糙等缺陷，导致结论相互矛盾。

Method: 基于一假设和两保证约束，构建五种潜在因果结构，并通过七个公开任务和一个自研推理任务对GPT-4o进行因果分析。

Result: 粗指标显示结构化输出对生成有正负或中性影响，但因果推断发现43/48场景中无因果影响，其余5个场景中3个存在多因果结构与具体指令相关。

Conclusion: 结构化输出对大型语言模型生成质量无普遍因果影响，具体效果受任务与指令的多因果因素影响，需要更严谨的因果分析方法评估效果。

Abstract: Structured output from large language models (LLMs) has enhanced efficiency
in processing generated information and is increasingly adopted in industrial
applications. Prior studies have investigated the impact of structured output
on LLMs' generation quality, often presenting one-way findings. Some suggest
that structured format enhances completeness and factual accuracy, while others
argue that it restricts the reasoning capacity of LLMs and leads to reductions
in standard evaluation metrics. Potential limitations of these assessments
include restricted testing scenarios, weakly controlled comparative settings,
and reliance on coarse metrics. In this work, we present a refined analysis
using causal inference. Based on one assumed and two guaranteed constraints, we
derive five potential causal structures characterizing the influence of
structured output on LLMs' generation: (1) collider without m-bias, (2)
collider with m-bias, (3) single cause from instruction, (4) single cause from
output format, and (5) independence. Across seven public and one developed
reasoning tasks, we find that coarse metrics report positive, negative, or
neutral effects of structured output on GPT-4o's generation. However, causal
inference reveals no causal impact in 43 out of 48 scenarios. In the remaining
5, 3 involve multifaceted causal structures influenced by concrete
instructions.

</details>


### [32] [Evaluating and Improving Cultural Awareness of Reward Models for LLM Alignment](https://arxiv.org/abs/2509.21798)
*Hongbin Zhang,Kehai Chen,Xuefeng Bai,Yang Xiang,Min Zhang*

Main category: cs.CL

TL;DR: 本文提出了一个多文化奖赏模型评估基准CARB，涵盖10个文化和4个文化领域，以评估大型语言模型的文化意识，并发现现有模型在文化理解上存在缺陷，提出了Think-as-Locals方法，通过基于可验证奖励的强化学习提升文化敏感度。


<details>
  <summary>Details</summary>
Motivation: 现有奖赏模型评估缺乏文化相关数据，难以准确衡量模型的文化意识，阻碍了大型语言模型的全球文化对齐。

Method: 提出CARB基准测试，覆盖多种文化与领域；分析现有模型缺陷；设计Think-as-Locals方法，通过基于可验证奖励的强化学习提升模型对文化细节的理解与判断能力。

Result: 实验证明现有模型依赖表面特征导致文化意识不足，CARB分数与多语言文化对齐任务表现正相关；Think-as-Locals有效减少了表面特征干扰，提升了模型文化敏感度和评估质量。

Conclusion: CARB基准为文化意识评估提供了有效手段，Think-as-Locals方法能够通过强化学习增强奖赏模型的文化理解能力，推动大型语言模型的全球文化对齐进展。

Abstract: Reward models (RMs) are crucial for aligning large language models (LLMs)
with diverse cultures. Consequently, evaluating their cultural awareness is
essential for further advancing global alignment of LLMs. However, existing RM
evaluations fall short in assessing cultural awareness due to the scarcity of
culturally relevant evaluation datasets. To fill this gap, we propose Cultural
Awareness Reward modeling Benchmark (CARB), covering 10 distinct cultures
across 4 cultural domains. Our extensive evaluation of state-of-the-art RMs
reveals their deficiencies in modeling cultural awareness and demonstrates a
positive correlation between performance on CARB and downstream multilingual
cultural alignment tasks. Further analysis identifies the spurious correlations
within culture-aware reward modeling, wherein RM's scoring relies predominantly
on surface-level features rather than authentic cultural nuance understanding.
To address these, we propose Think-as-Locals to elicit deeper culturally
grounded reasoning from generative RMs via reinforcement learning from
verifiable rewards (RLVR) and employ well-designed rewards to ensure accurate
preference judgments and high-quality structured evaluation criteria
generation. Experimental results validate its efficacy in mitigating spurious
features interference and advancing culture-aware reward modeling.

</details>


### [33] [Redefining Machine Simultaneous Interpretation: From Incremental Translation to Human-Like Strategies](https://arxiv.org/abs/2509.21801)
*Qianen Zhang,Satoshi Nakamura*

Main category: cs.CL

TL;DR: 本文提出了扩展同步机器翻译（SiMT）动作空间的新方法，加入了四种自适应动作以提升实时翻译质量与效率。


<details>
  <summary>Details</summary>
Motivation: 传统的同步机器翻译仅用READ和WRITE动作，难以满足严格实时性和高质量翻译需求。

Method: 通过引入SENTENCE_CUT、DROP、PARTIAL_SUMMARIZATION和PRONOMINALIZATION四种自适应动作，并在仅解码器的大型语言模型中实现，结合动作感知提示构建训练参考，同时开发考虑延迟的TTS评估体系。

Result: 在英语-中文和英语-德语的ACL60/60基准测试中，新框架在语义准确性和延迟方面均优于参考翻译和传统基线，特别是DROP与SENTENCE_CUT组合表现最佳。

Conclusion: 扩展SiMT动作空间基于LLM的方法能有效提升实时翻译质量与速度，缩小机器与人类理解差距，具备良好应用前景。

Abstract: Simultaneous Machine Translation (SiMT) requires high-quality translations
under strict real-time constraints, which traditional encoder-decoder policies
with only READ/WRITE actions cannot fully address. We extend the action space
of SiMT with four adaptive actions: SENTENCE_CUT, DROP, PARTIAL_SUMMARIZATION
and PRONOMINALIZATION, which enable real-time restructuring, omission, and
simplification while preserving semantic fidelity. We implement these actions
in a decoder-only large language model (LLM) framework and construct training
references through action-aware prompting. To evaluate both quality and
latency, we further develop a latency-aware TTS pipeline that maps textual
outputs to speech with realistic timing. Experiments on the ACL60/60
English-Chinese and English-German benchmarks show that our framework
consistently improves semantic metrics (e.g., COMET-KIWI) and achieves lower
delay (measured by Average Lagging) compared to reference translations and
salami-based baselines. Notably, combining DROP and SENTENCE_CUT yields the
best overall balance between fluency and latency. These results demonstrate
that enriching the action space of LLM-based SiMT provides a promising
direction for bridging the gap between human and machine interpretation.

</details>


### [34] [Towards Minimal Causal Representations for Human Multimodal Language Understanding](https://arxiv.org/abs/2509.21805)
*Menghua Jiang,Yuncheng Jiang,Haifeng Hu,Sijie Mai*

Main category: cs.CL

TL;DR: 本文提出了一种基于因果推断的多模态信息瓶颈模型CaMIB，用于提升多模态语言理解中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态语言理解方法容易受到数据集偏差影响，将统计捷径误判为因果特征，导致模型在OOD数据上的表现下降。

Method: 利用信息瓶颈过滤单模态噪声，通过参数掩码生成器将多模态表示拆分成因果子表示和捷径子表示，结合工具变量约束和反向调整稳定因果特征估计。

Result: 在多模态情感分析、幽默识别和讽刺识别任务及OOD测试集上，CaMIB展现出优越性能。

Conclusion: CaMIB有效缓解了模型对数据集偏差的敏感性，提升了多模态语言理解模型的因果解释能力和泛化性能。

Abstract: Human Multimodal Language Understanding (MLU) aims to infer human intentions
by integrating related cues from heterogeneous modalities. Existing works
predominantly follow a ``learning to attend" paradigm, which maximizes mutual
information between data and labels to enhance predictive performance. However,
such methods are vulnerable to unintended dataset biases, causing models to
conflate statistical shortcuts with genuine causal features and resulting in
degraded out-of-distribution (OOD) generalization. To alleviate this issue, we
introduce a Causal Multimodal Information Bottleneck (CaMIB) model that
leverages causal principles rather than traditional likelihood. Concretely, we
first applies the information bottleneck to filter unimodal inputs, removing
task-irrelevant noise. A parameterized mask generator then disentangles the
fused multimodal representation into causal and shortcut subrepresentations. To
ensure global consistency of causal features, we incorporate an instrumental
variable constraint, and further adopt backdoor adjustment by randomly
recombining causal and shortcut features to stabilize causal estimation.
Extensive experiments on multimodal sentiment analysis, humor detection, and
sarcasm detection, along with OOD test sets, demonstrate the effectiveness of
CaMIB. Theoretical and empirical analyses further highlight its
interpretability and soundness.

</details>


### [35] [Can LLMs Solve and Generate Linguistic Olympiad Puzzles?](https://arxiv.org/abs/2509.21820)
*Neh Majmudar,Elena Filatova*

Main category: cs.CL

TL;DR: 本文介绍了解决和生成语言学谜题的新任务，重点关注高中语言学奥林匹克竞赛中的谜题，利用大型语言模型（LLMs）进行解决并拓展了相关基准。


<details>
  <summary>Details</summary>
Motivation: 语言学谜题用于教育和推广语言学知识，自动化解决和生成这些谜题有助于推广语言学和保护少数语言。

Method: 扩展了语言学谜题的基准数据，使用包括OpenAI的最新模型在内的大型语言模型进行谜题解决，分析模型在不同语言主题下的表现，并基于解谜结果探索自动生成谜题的方法。

Result: 大型语言模型在大多数谜题类型上表现优于人类，除书写系统题型和少数语言外，通过解谜获得的见解推动了谜题生成的研究。

Conclusion: 自动化生成语言学谜题不仅能促进语言学的普及，也有助于传播稀有和少数语言的知识，具有重要的研究价值。

Abstract: In this paper, we introduce a combination of novel and exciting tasks: the
solution and generation of linguistic puzzles. We focus on puzzles used in
Linguistic Olympiads for high school students. We first extend the existing
benchmark for the task of solving linguistic puzzles. We explore the use of
Large Language Models (LLMs), including recent state-of-the-art models such as
OpenAI's o1, for solving linguistic puzzles, analyzing their performance across
various linguistic topics. We demonstrate that LLMs outperform humans on most
puzzles types, except for those centered on writing systems, and for the
understudied languages. We use the insights from puzzle-solving experiments to
direct the novel task of puzzle generation. We believe that automating puzzle
generation, even for relatively simple puzzles, holds promise for expanding
interest in linguistics and introducing the field to a broader audience. This
finding highlights the importance of linguistic puzzle generation as a research
task: such puzzles can not only promote linguistics but also support the
dissemination of knowledge about rare and understudied languages.

</details>


### [36] [ResT: Reshaping Token-Level Policy Gradients for Tool-Use Large Language Models](https://arxiv.org/abs/2509.21826)
*Zihan Lin,Xiaohan Wang,Jie Cao,Jiajun Chai,Guojun Yin,Wei Lin,Ran He*

Main category: cs.CL

TL;DR: 本文针对大语言模型作为工具使用代理的训练效率问题，提出了一种基于策略梯度重构的优化方法ResT，显著提升了训练稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的方法依赖稀疏奖励且未充分考虑工具使用任务的特殊性，导致策略梯度方差增大，训练效率低下。

Method: 本文建立了策略熵与训练稳定性之间的理论联系，提出通过基于熵的信息对策略梯度进行重构，即ResT，逐步提升推理令牌的权重以稳定训练收敛。

Result: 在BFCL和API-Bank数据集上，ResT比现有方法性能提升最多8.76%；在微调4B基础大模型后，单轮任务上超越GPT-4o 4.11%，多轮任务提升1.50%。

Conclusion: 基于熵的策略梯度重构有效缓解了工具使用任务中的训练不稳定问题，提升了大语言模型调用外部工具的效果。

Abstract: Large language models (LLMs) transcend passive generation and act as
goal-directed agents by invoking external tools. Reinforcement learning (RL)
offers a principled framework for optimizing these emergent tool-use policies,
yet the prevailing paradigm relies exclusively on sparse outcome rewards and
lacks consideration of the particularity of tool-use tasks, inflating
policy-gradient variance and resulting in inefficient training. To better
understand and address these challenges, we first establish a theoretical link
between policy entropy and training stability of tool-use tasks, which reveals
that structured, low-entropy tokens are primary determinants of rewards.
Motivated by this insight, we propose \textbf{Res}haped \textbf{T}oken-level
policy gradients (\textbf{ResT}) for tool-use tasks. ResT reshapes the policy
gradient through entropy-informed token reweighting, progressively upweighting
reasoning tokens as training proceeds. This entropy-aware scheme enables a
smooth shift from structural correctness to semantic reasoning and stabilizes
convergence in multi-turn tool-use tasks. Evaluation on BFCL and API-Bank shows
that ResT achieves state-of-the-art results, outperforming prior methods by up
to $8.76\%$. When fine-tuned on a 4B base LLM, ResT further surpasses GPT-4o by
$4.11\%$ on single-turn tasks and $1.50\%$ on multi-turn base tasks.

</details>


### [37] [Semantic Agreement Enables Efficient Open-Ended LLM Cascades](https://arxiv.org/abs/2509.21837)
*Duncan Soiffer,Steven Kolawole,Virginia Smith*

Main category: cs.CL

TL;DR: 该论文提出了基于语义一致性的级联系统，用以在大语言模型（LLM）中实现高效且可靠的请求调度，从而平衡计算成本和输出质量。


<details>
  <summary>Details</summary>
Motivation: 传统级联系统难以判断开放式文本生成的输出可靠性，因为生成质量呈现连续变化且多个答案可能有效。

Method: 引入语义一致性——即多模型输出在意义层面的共识，作为无需训练的可靠性判断信号，优于基于单词概率的置信度评估。

Result: 在从5亿参数到700亿参数的模型上测试，语义级联系统能够以40%的成本达到或超越目标模型的质量，同时延迟降低最多60%。

Conclusion: 该方法不依赖模型内部信息，兼容黑箱API，对模型更新保持鲁棒性，是实际大语言模型部署的有效且实用的基线方案。

Abstract: Cascade systems route computational requests to smaller models when possible
and defer to larger models only when necessary, offering a promising approach
to balance cost and quality in LLM deployment. However, they face a fundamental
challenge in open-ended text generation: determining output reliability when
generation quality lies on a continuous spectrum, often with multiple valid
responses. To address this, we propose semantic agreement -- meaning-level
consensus between ensemble outputs -- as a training-free signal for reliable
deferral. We show that when diverse model outputs agree semantically, their
consensus is a stronger reliability signal than token-level confidence.
Evaluated from 500M to 70B-parameter models, we find that semantic cascades
match or surpass target-model quality at 40% of the cost and reduce latency by
up to 60%. Our method requires no model internals, works across black-box APIs,
and remains robust to model updates, making it a practical baseline for
real-world LLM deployment.

</details>


### [38] [KnowMT-Bench: Benchmarking Knowledge-Intensive Long-Form Question Answering in Multi-Turn Dialogues](https://arxiv.org/abs/2509.21856)
*Junhao Chen,Yu Huang,Siyuan Li,Rui Yao,Hanqian Li,Hanyu Zhang,Jungang Li,Jian Chen,Bowen Wang,Xuming Hu*

Main category: cs.CL

TL;DR: 提出了KnowMT-Bench，这是第一个针对多轮长篇问答的知识密集领域评测基准，涵盖医学、金融和法律。发现多轮对话会降低模型的事实准确率和信息效率，且检索增强生成（RAG）能有效缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: 现有多轮对话评测未针对知识密集的事实准确性进行系统评估，缺乏真实的多轮长篇问答基准，难以衡量大型语言模型在现实复杂场景中的表现。

Method: 设计KnowMT-Bench基准，利用模型自生成的多轮对话历史动态评估多轮长问答表现，结合人工验证的自动化评测流程评估最终回答的事实性和信息效率，实验中分析多轮对话对表现的影响并探索检索增强技术的缓解作用。

Result: 实验显示多轮上下文引入的噪声降低了事实准确率，同时模型的回答变得冗长，信息效率下降。引入检索增强生成方法后，事实准确率得到显著提升，缓解了多轮带来的负面影响。

Conclusion: KnowMT-Bench基准为知识密集领域多轮长篇问答的评测提供了首个系统框架，揭示了多轮对话对模型事实性和效率的负面影响，并证明检索增强生成是提升模型对话事实能力的有效策略。该基准推动了大型语言模型在现实复杂知识场景下的能力评估与改进。

Abstract: Multi-Turn Long-Form Question Answering (MT-LFQA) is a key application
paradigm of Large Language Models (LLMs) in knowledge-intensive domains.
However, existing benchmarks are limited to single-turn dialogue, while
multi-turn dialogue benchmarks typically assess other orthogonal capabilities
rather than knowledge-intensive factuality. To bridge this critical gap, we
introduce \textbf{KnowMT-Bench}, the \textit{first-ever} benchmark designed to
systematically evaluate MT-LFQA for LLMs across knowledge-intensive fields,
including medicine, finance, and law. To faithfully assess the model's
real-world performance, KnowMT-Bench employs a dynamic evaluation setting where
models generate their own multi-turn dialogue histories given logically
progressive question sequences. The factual capability and information delivery
efficiency of the \textit{final-turn} answer are then evaluated using a
human-validated automated pipeline. Our experiments reveal that multi-turn
contexts degrade performance: factual capability declines due to the contextual
noise from self-generated histories, while information efficiency drops as
models become more verbose with increasing dialogue length. We then investigate
mitigation strategies, demonstrating that retrieval-augmented generation (RAG)
can effectively alleviate and even reverse this factual degradation. These
findings underscore the importance of our benchmark in evaluating and enhancing
the conversational factual capabilities of LLMs in real-world
knowledge-intensive applications. Code is available at
\href{https://github.com/hardenyu21/KnowMT-Bench}{\textcolor{cyan}{\texttt{KnowMT-Bench}}}.

</details>


### [39] [Enhancing Low-Rank Adaptation with Structured Nonlinear Transformations](https://arxiv.org/abs/2509.21870)
*Guanzhi Deng,Mingyang Liu,Dapeng Wu,Yinqiao Li,Linqi Song*

Main category: cs.CL

TL;DR: 本文提出了LoRAN，一种LoRA的非线性扩展，通过引入轻量级转换和基于正弦的激活函数Sinter，提升了大规模语言模型的微调表现。


<details>
  <summary>Details</summary>
Motivation: 现有的LoRA方法因其线性结构限制了表达能力，难以充分调优大规模语言模型。

Method: 提出LoRAN，通过对低秩更新应用非线性轻量级变换，并引入基于正弦的激活函数Sinter，实现结构化扰动且不增加参数量。

Result: 在摘要和分类任务中，LoRAN表现优于QLoRA；消融实验显示Sinter优于Sigmoid、ReLU和Tanh等传统激活函数。

Conclusion: 激活函数设计在低秩调优中至关重要，LoRAN通过非线性扩展有效提升了参数高效微调效果。

Abstract: Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient
fine-tuning method for large language models. However, its linear nature limits
expressiveness. We propose LoRAN, a non-linear extension of LoRA that applies
lightweight transformations to the low-rank updates. We further introduce
Sinter, a sine-based activation that adds structured perturbations without
increasing parameter count. Experiments across summarization and classification
tasks show that LoRAN consistently improves over QLoRA. Ablation studies reveal
that Sinter outperforms standard activations such as Sigmoid, ReLU, and Tanh,
highlighting the importance of activation design in lowrank tuning.

</details>


### [40] [LUMINA: Detecting Hallucinations in RAG System with Context-Knowledge Signals](https://arxiv.org/abs/2509.21875)
*Min-Hsuan Yeh,Yixuan Li,Tanwi Mallick*

Main category: cs.CL

TL;DR: 该论文提出了LUMINA框架以检测基于检索增强生成(RAG)模型中的幻觉问题，通过量化外部上下文和内部知识的利用信号，显著提升了幻觉检测的效果和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前RAG模型虽然利用检索文档以减少幻觉，但仍存在幻觉问题，主要原因在于模型对外部上下文和内部知识的利用不平衡。现有检测方法需要大量超参数调优，泛化能力差。

Method: 提出LUMINA框架，通过两类信号检测幻觉：一是利用分布式距离量化外部上下文利用率；二是通过跟踪预测令牌在Transformer层间的变化量化内部知识利用率，并引入统计验证框架保证测量可靠性。

Result: 在多项常用RAG幻觉检测基准和四个开源LLM上，LUMINA展现出稳定高效的性能，AUROC和AUPRC指标均优于现有利用率方法，HalluRAG上AUROC提升最高达13%。

Conclusion: LUMINA框架有效提升了RAG模型幻觉检测的准确性和鲁棒性，且在检索质量和模型匹配松弛条件下依然表现良好，兼具实用性和推广价值。

Abstract: Retrieval-Augmented Generation (RAG) aims to mitigate hallucinations in large
language models (LLMs) by grounding responses in retrieved documents. Yet,
RAG-based LLMs still hallucinate even when provided with correct and sufficient
context. A growing line of work suggests that this stems from an imbalance
between how models use external context and their internal knowledge, and
several approaches have attempted to quantify these signals for hallucination
detection. However, existing methods require extensive hyperparameter tuning,
limiting their generalizability. We propose LUMINA, a novel framework that
detects hallucinations in RAG systems through context-knowledge signals:
external context utilization is quantified via distributional distance, while
internal knowledge utilization is measured by tracking how predicted tokens
evolve across transformer layers. We further introduce a framework for
statistically validating these measurements. Experiments on common RAG
hallucination benchmarks and four open-source LLMs show that LUMINA achieves
consistently high AUROC and AUPRC scores, outperforming prior utilization-based
methods by up to +13% AUROC on HalluRAG. Moreover, LUMINA remains robust under
relaxed assumptions about retrieval quality and model matching, offering both
effectiveness and practicality.

</details>


### [41] [No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM Reinforcement Learning via Entropy-Guided Advantage Shaping](https://arxiv.org/abs/2509.21880)
*Thanh-Long V. Le,Myeongho Jeon,Kim Vu,Viet Lai,Eunho Yang*

Main category: cs.CL

TL;DR: 本文提出了一种名为RL-ZVP的新算法，利用零方差提示从大语言模型的强化学习中提取有效反馈，显著提升数学推理任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法如GRPO忽视了所有响应获得相同奖励的零方差提示，错失了潜在的学习信号。

Method: RL-ZVP算法通过直接奖励正确并惩罚错误，即使在缺乏响应对比的情况下，依然通过令牌级别特征调整反馈，保留细腻的信息信号。

Result: 在六个数学推理基准测试中，RL-ZVP在准确率和通过率上分别提升了最多8.61和7.77个百分点，且优于剔除零方差提示的其他基线方法。

Conclusion: 零方差提示并非无用，RL-ZVP利用其内在信息显著提升了大语言模型的推理能力，展示了强化学习中未被开发的潜力。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful framework
for improving the reasoning abilities of Large Language Models (LLMs). However,
current methods such as GRPO rely only on problems where the model responses to
the same input differ in correctness, while ignoring those where all responses
receive the same reward - so-called zero-variance prompts. In this work, we
argue that such prompts are not useless but can, in fact, provide meaningful
feedback for policy optimization. To this end, we introduce RL with
Zero-Variance Prompts (RL-ZVP), a novel algorithm that extract learning signals
from zero-variance prompts. RL-ZVP directly rewards correctness and penalizes
errors even without contrasting responses, modulating feedback with token-level
characteristics to preserve informative, nuanced signals. Across six math
reasoning benchmarks, RL-ZVP achieves significant improvements of up to 8.61
points in accuracy and 7.77 points in pass rate over GRPO, while consistently
outperforming other baselines that filter out zero-variance prompts. These
results highlight the untapped potential of learning from zero-variance prompts
in RLVR.

</details>


### [42] [QoNext: Towards Next-generation QoE for Foundation Models](https://arxiv.org/abs/2509.21889)
*Yijin Guo,Ye Shen,Farong Wen,Junying Wang,Zicheng Zhang,Qi Jia,Guangtao Zhai*

Main category: cs.CL

TL;DR: 该论文提出了QoNext框架，将网络和多媒体领域的体验质量（QoE）原则应用于基础模型的评估，以更全面地衡量用户交互体验。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型评估方法仅关注输出正确性，忽视了用户满意度由响应质量与交互关系共同决定，无法真实反映用户体验机制。

Method: 提出QoNext框架，识别影响用户体验的因素，设计受控实验并收集人类评分，构建QoE数据库，训练预测模型从系统参数估计用户体验。

Result: 实验表明，QoNext能实现主动且细粒度的评估，并能基于评估结果为产品化服务优化基础模型提供指导。

Conclusion: QoNext框架有效提升了基础模型的用户体验评估方法，促进了面向用户交互体验的模型优化和实际应用。

Abstract: Existing evaluations of foundation models, including recent human-centric
approaches, fail to capture what truly matters: user's experience during
interaction. Current methods treat evaluation as a matter of output correctness
alone, overlooking that user satisfaction emerges from the interplay between
response quality and interaction, which limits their ability to account for the
mechanisms underlying user experience. To address this gap, we introduce
QoNext, the first framework that adapts Quality of Experience (QoE) principles
from networking and multimedia to the assessment of foundation models. QoNext
identifies experiential factors that shape user experience and incorporates
them into controlled experiments, where human ratings are collected under
varied configurations. From these studies we construct a QoE-oriented database
and train predictive models that estimate perceived user experience from
measurable system parameters. Our results demonstrate that QoNext not only
enables proactive and fine-grained evaluation but also provides actionable
guidance for productized services of optimizing foundation models in practice.

</details>


### [43] [Elastic MoE: Unlocking the Inference-Time Scalability of Mixture-of-Experts](https://arxiv.org/abs/2509.21892)
*Naibin Gu,Zhenyu Zhang,Yuchen Feng,Yilong Chen,Peng Fu,Zheng Lin,Shuohuan Wang,Yu Sun,Hua Wu,Weiping Wang,Haifeng Wang*

Main category: cs.CL

TL;DR: 本文提出了弹性混合专家模型（EMoE），解决了MoE模型在推理时专家数量增加导致性能下降的问题，通过训练专家协同工作，使得模型推理时可激活不同数量的专家，提升性能和扩展性能提升范围。


<details>
  <summary>Details</summary>
Motivation: 传统Mixture-of-Experts模型在推理阶段激活的专家数量固定，稍微增加激活专家数反而导致性能快速下降，主要是因为专家之间缺乏协作。

Method: 提出EMoE训练框架，训练专家在多样组合中协作，并鼓励路由器作出高质量选择，使模型能在推理时灵活调整激活专家数且无额外训练开销。

Result: 在多种MoE配置上实验表明，EMoE显著扩大了有效性能扩展范围，激活专家数可扩展至训练时的2-3倍，同时提升了模型最高性能水平。

Conclusion: EMoE有效解决了MoE模型在推理阶段激活更多专家时性能下降的问题，实现了模型性能和计算预算之间的弹性权衡，推动了MoE模型的性能提升。

Abstract: Mixture-of-Experts (MoE) models typically fix the number of activated experts
$k$ at both training and inference. Intuitively, activating more experts at
inference $k'$ (where $k'> k$) means engaging a larger set of model parameters
for the computation and thus is expected to improve performance. However,
contrary to this intuition, we find the scaling range to be so narrow that
performance begins to degrade rapidly after only a slight increase in the
number of experts. Further investigation reveals that this degradation stems
from a lack of learned collaboration among experts. To address this, we
introduce Elastic Mixture-of-Experts (EMoE), a novel training framework that
enables MoE models to scale the number of activated experts at inference
without incurring additional training overhead. By simultaneously training
experts to collaborate in diverse combinations and encouraging the router for
high-quality selections, EMoE ensures robust performance across computational
budgets at inference. We conduct extensive experiments on various MoE settings.
Our results show that EMoE significantly expands the effective
performance-scaling range, extending it to as much as 2-3$\times$ the
training-time $k$, while also pushing the model's peak performance to a higher
level.

</details>


### [44] [A Large-Scale Dataset and Citation Intent Classification in Turkish with LLMs](https://arxiv.org/abs/2509.21907)
*Kemal Sami Karaca,Bahaeddin Eravcı*

Main category: cs.CL

TL;DR: 本文提出了土耳其语引用意图的公开数据集及基于DSPy的自动化优化分类流水线，实现91.3%的准确率。


<details>
  <summary>Details</summary>
Motivation: 土耳其语作为黏着语，理解文献引用的定性目的具有挑战性，现有方法因提示语设计不一致效果有限。

Method: 构建专用注释工具制作土耳其语引用意图数据集，评估LLM的In-Context Learning并提出基于DSPy框架的自动化提示优化分类流水线，最终以堆叠集成和XGBoost元模型实现分类。

Result: 通过多模型堆叠集成和自动化提示优化，分类准确率提升至91.3%，优于传统手工设计提示的方法。

Conclusion: 该研究为土耳其NLP社区提供了首个土耳其语引用意图数据集和高效分类框架，促进未来定性引用研究的发展。

Abstract: Understanding the qualitative intent of citations is essential for a
comprehensive assessment of academic research, a task that poses unique
challenges for agglutinative languages like Turkish. This paper introduces a
systematic methodology and a foundational dataset to address this problem. We
first present a new, publicly available dataset of Turkish citation intents,
created with a purpose-built annotation tool. We then evaluate the performance
of standard In-Context Learning (ICL) with Large Language Models (LLMs),
demonstrating that its effectiveness is limited by inconsistent results caused
by manually designed prompts. To address this core limitation, we introduce a
programmable classification pipeline built on the DSPy framework, which
automates prompt optimization systematically. For final classification, we
employ a stacked generalization ensemble to aggregate outputs from multiple
optimized models, ensuring stable and reliable predictions. This ensemble, with
an XGBoost meta-model, achieves a state-of-the-art accuracy of 91.3\%.
Ultimately, this study provides the Turkish NLP community and the broader
academic circles with a foundational dataset and a robust classification
framework paving the way for future qualitative citation studies.

</details>


### [45] [AutoSCORE: Enhancing Automated Scoring with Multi-Agent Large Language Models via Structured Component Recognition](https://arxiv.org/abs/2509.21910)
*Yun Wang,Zhaojun Ding,Xuansheng Wu,Siyue Sun,Ninghao Liu,Xiaoming Zhai*

Main category: cs.CL

TL;DR: 提出了AutoSCORE多智能体大模型框架，通过结构化评分要素识别提升自动评分的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前大模型作为端到端评分器存在准确率低、对提示敏感、可解释性差和评分标准不一致的问题，难以在实际评估中应用。

Method: 设计了双智能体系统：一个负责提取学生回答中的评分标准相关组成部分并构建结构化表示，另一个基于此进行打分，模拟人类评分过程。

Result: 在四个ASAP基准数据集上，AutoSCORE提升了评分准确率、人机一致性（QWK、相关性）及降低误差（MAE、RMSE），特别是在复杂多维评分标准和小型模型上表现突出。

Conclusion: 结构化评分要素识别结合多智能体设计，为自动评分提供了可扩展、可靠且易解释的解决方案。

Abstract: Automated scoring plays a crucial role in education by reducing the reliance
on human raters, offering scalable and immediate evaluation of student work.
While large language models (LLMs) have shown strong potential in this task,
their use as end-to-end raters faces challenges such as low accuracy, prompt
sensitivity, limited interpretability, and rubric misalignment. These issues
hinder the implementation of LLM-based automated scoring in assessment
practice. To address the limitations, we propose AutoSCORE, a multi-agent LLM
framework enhancing automated scoring via rubric-aligned Structured COmponent
REcognition. With two agents, AutoSCORE first extracts rubric-relevant
components from student responses and encodes them into a structured
representation (i.e., Scoring Rubric Component Extraction Agent), which is then
used to assign final scores (i.e., Scoring Agent). This design ensures that
model reasoning follows a human-like grading process, enhancing
interpretability and robustness. We evaluate AutoSCORE on four benchmark
datasets from the ASAP benchmark, using both proprietary and open-source LLMs
(GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B). Across diverse tasks and rubrics,
AutoSCORE consistently improves scoring accuracy, human-machine agreement (QWK,
correlations), and error metrics (MAE, RMSE) compared to single-agent
baselines, with particularly strong benefits on complex, multi-dimensional
rubrics, and especially large relative gains on smaller LLMs. These results
demonstrate that structured component recognition combined with multi-agent
design offers a scalable, reliable, and interpretable solution for automated
scoring.

</details>


### [46] [SimulSense: Sense-Driven Interpreting for Efficient Simultaneous Speech Translation](https://arxiv.org/abs/2509.21932)
*Haotian Tan,Hiroki Ouchi,Sakriani Sakti*

Main category: cs.CL

TL;DR: 本文提出了SimulSense框架，通过模拟人类口译员基于感知的新语义单元触发写决策的机制，实现了同时口语翻译的高质量与低延迟。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的SimulST系统将任务视为多轮对话，需专门的交错训练数据且依赖计算昂贵的大型语言模型推理，效率低下。

Method: SimulSense持续读取输入语音，并在感知到新的语义单元时触发翻译写入决策，模仿人类口译员的策略。

Result: 实验结果表明，SimulSense在质量与延迟的权衡上优于两种最先进基线系统，决策速度提升至基线的9.6倍。

Conclusion: 采用模拟人类口译员感知机制的SimulSense框架显著提升了同时口语翻译系统的实时性与性能。

Abstract: How to make human-interpreter-like read/write decisions for simultaneous
speech translation (SimulST) systems? Current state-of-the-art systems
formulate SimulST as a multi-turn dialogue task, requiring specialized
interleaved training data and relying on computationally expensive large
language model (LLM) inference for decision-making. In this paper, we propose
SimulSense, a novel framework for SimulST that mimics human interpreters by
continuously reading input speech and triggering write decisions to produce
translation when a new sense unit is perceived. Experiments against two
state-of-the-art baseline systems demonstrate that our proposed method achieves
a superior quality-latency tradeoff and substantially improved real-time
efficiency, where its decision-making is up to 9.6x faster than the baselines.

</details>


### [47] [Why Chain of Thought Fails in Clinical Text Understanding](https://arxiv.org/abs/2509.21933)
*Jiageng Wu,Kevin Xie,Bowen Gu,Nils Krüger,Kueiyu Joshua Lin,Jie Yang*

Main category: cs.CL

TL;DR: 本文首次大规模系统性研究了链式思维(CoT)提示在临床文本理解中的效果，发现大部分模型在CoT设置下表现下降，强模型表现较稳，而弱模型表现大幅下降。


<details>
  <summary>Details</summary>
Motivation: 探究链式思维(CoT)提示在临床文本（如电子健康记录）中应用的有效性，解决临床场景对准确性和透明推理的高需求。

Method: 对95个先进大语言模型进行评估，涵盖87个真实临床文本任务、9种语言和8类任务，结合模型评判和临床专家评估，细致分析推理长度、医学概念对齐及错误模式。

Result: 86.3%的模型在CoT提示下性能下降，强模型较为鲁棒，弱模型显著受损，揭示CoT在临床文本任务中存在提升可解释性但可能降低可靠性的矛盾。

Conclusion: CoT虽然提高模型推理的可解释性，但在临床文本任务中可能危害模型性能，强调了打造透明且可信的临床推理方法的必要性。

Abstract: Large language models (LLMs) are increasingly being applied to clinical care,
a domain where both accuracy and transparent reasoning are critical for safe
and trustworthy deployment. Chain-of-thought (CoT) prompting, which elicits
step-by-step reasoning, has demonstrated improvements in performance and
interpretability across a wide range of tasks. However, its effectiveness in
clinical contexts remains largely unexplored, particularly in the context of
electronic health records (EHRs), the primary source of clinical documentation,
which are often lengthy, fragmented, and noisy. In this work, we present the
first large-scale systematic study of CoT for clinical text understanding. We
assess 95 advanced LLMs on 87 real-world clinical text tasks, covering 9
languages and 8 task types. Contrary to prior findings in other domains, we
observe that 86.3\% of models suffer consistent performance degradation in the
CoT setting. More capable models remain relatively robust, while weaker ones
suffer substantial declines. To better characterize these effects, we perform
fine-grained analyses of reasoning length, medical concept alignment, and error
profiles, leveraging both LLM-as-a-judge evaluation and clinical expert
evaluation. Our results uncover systematic patterns in when and why CoT fails
in clinical contexts, which highlight a critical paradox: CoT enhances
interpretability but may undermine reliability in clinical text tasks. This
work provides an empirical basis for clinical reasoning strategies of LLMs,
highlighting the need for transparent and trustworthy approaches.

</details>


### [48] [Debiasing Large Language Models in Thai Political Stance Detection via Counterfactual Calibration](https://arxiv.org/abs/2509.21946)
*Kasidit Sermsri,Teerapong Panboonyuen*

Main category: cs.CL

TL;DR: 该论文提出了一个针对泰语政治立场检测中偏见问题的校准框架ThaiFACTUAL，显著提升了模型的公平性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 在资源匮乏且文化复杂的环境中，大型语言模型在政治立场检测中存在系统性偏见，影响其公平性和可靠性。

Method: 提出ThaiFACTUAL框架，结合反事实数据增强和基于理由的监督，解离情感与立场以减少偏见，且无需微调。

Result: ThaiFACTUAL显著减少了虚假关联，提升了模型的零样本泛化能力和公平性，并发布了高质量的泰语政治立场数据集。

Conclusion: 文化基础的去偏技术对低资源语言的政治立场检测至关重要，ThaiFACTUAL展示了有效的偏见缓解方法。

Abstract: Political stance detection in low-resource and culturally complex settings
poses a critical challenge for large language models (LLMs). In the Thai
political landscape - marked by indirect language, polarized figures, and
entangled sentiment and stance - LLMs often display systematic biases such as
sentiment leakage and favoritism toward entities. These biases undermine
fairness and reliability. We present ThaiFACTUAL, a lightweight, model-agnostic
calibration framework that mitigates political bias without requiring
fine-tuning. ThaiFACTUAL uses counterfactual data augmentation and
rationale-based supervision to disentangle sentiment from stance and reduce
bias. We also release the first high-quality Thai political stance dataset,
annotated with stance, sentiment, rationales, and bias markers across diverse
entities and events. Experimental results show that ThaiFACTUAL significantly
reduces spurious correlations, enhances zero-shot generalization, and improves
fairness across multiple LLMs. This work highlights the importance of
culturally grounded debiasing techniques for underrepresented languages.

</details>


### [49] [MotivGraph-SoIQ: Integrating Motivational Knowledge Graphs and Socratic Dialogue for Enhanced LLM Ideation](https://arxiv.org/abs/2509.21978)
*Xinping Lei,Tong Zhou,Yubo Chen,Kang Liu,Jun Zhao*

Main category: cs.CL

TL;DR: 本文提出了一种结合动机知识图谱和苏格拉底式对话的新框架（MotivGraph-SoIQ），以提升大型语言模型（LLM）在学术创新中的想法落地和优化能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在学术创意加速方面潜力巨大，但存在想法落地不足和确认偏误问题，影响创新质量。

Method: 提出MotivGraph-SoIQ框架，整合动机知识图谱（存储问题、挑战、解决方案三类节点）与基于问答驱动的双代理苏格拉底式对话系统，从结构和交流两方面提升想法的基础和改进过程。

Result: 在ICLR25论文主题数据集上，该方法在LLM评分、ELO排名和人工评估指标上表现优于当前最先进方法。

Conclusion: MotivGraph-SoIQ通过提供结构化动机支撑和系统化质疑改进，显著提升了LLM在学术创意中的创新质量和合理性。

Abstract: Large Language Models (LLMs) hold substantial potential for accelerating
academic ideation but face critical challenges in grounding ideas and
mitigating confirmation bias for further refinement. We propose integrating
motivational knowledge graphs and socratic dialogue to address these
limitations in enhanced LLM ideation (MotivGraph-SoIQ). This novel framework
provides essential grounding and practical idea improvement steps for LLM
ideation by integrating a Motivational Knowledge Graph (MotivGraph) with a
Q-Driven Socratic Ideator. The MotivGraph structurally stores three key node
types(problem, challenge and solution) to offer motivation grounding for the
LLM ideation process. The Ideator is a dual-agent system utilizing Socratic
questioning, which facilitates a rigorous refinement process that mitigates
confirmation bias and improves idea quality across novelty, experimental rigor,
and motivational rationality dimensions. On the ICLR25 paper topics dataset,
MotivGraph-SoIQ exhibits clear advantages over existing state-of-the-art
approaches across LLM-based scoring, ELO ranking, and human evaluation metrics.

</details>


### [50] [Black-Box Hallucination Detection via Consistency Under the Uncertain Expression](https://arxiv.org/abs/2509.21999)
*Seongho Joo,Kyungmin Min,Jahyun Koo,Kyomin Jung*

Main category: cs.CL

TL;DR: 本文提出了一种简单且高效的黑盒方法，通过分析大型语言模型(LLMs)在表达不确定性时的行为，检测其“幻觉”问题，即生成非事实性回答的现象。


<details>
  <summary>Details</summary>
Motivation: 目前幻觉检测方法依赖于外部资源或模型内部状态，但受限于LLMs的API限制和资源不足，迫切需要纯黑盒方法来检测。

Method: 通过分析LLMs在表达不确定性时答案变得不一致的特性，提出一个基于答案一致性的黑盒检测指标。

Result: 实验表明，该指标在预测回答真实性方面优于基于模型内部知识的基线方法。

Conclusion: 基于不确定性表达的一致性检测方法为解决LLM幻觉问题提供了一种有效且实用的黑盒解决方案。

Abstract: Despite the great advancement of Language modeling in recent days, Large
Language Models (LLMs) such as GPT3 are notorious for generating non-factual
responses, so-called "hallucination" problems. Existing methods for detecting
and alleviating this hallucination problem require external resources or the
internal state of LLMs, such as the output probability of each token. Given the
LLM's restricted external API availability and the limited scope of external
resources, there is an urgent demand to establish the Black-Box approach as the
cornerstone for effective hallucination detection. In this work, we propose a
simple black-box hallucination detection metric after the investigation of the
behavior of LLMs under expression of uncertainty. Our comprehensive analysis
reveals that LLMs generate consistent responses when they present factual
responses while non-consistent responses vice versa. Based on the analysis, we
propose an efficient black-box hallucination detection metric with the
expression of uncertainty. The experiment demonstrates that our metric is more
predictive of the factuality in model responses than baselines that use
internal knowledge of LLMs.

</details>


### [51] [GraphSearch: An Agentic Deep Searching Workflow for Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2509.22009)
*Cehao Yang,Xiaojun Wu,Xueyuan Lin,Chengjin Xu,Xuhui Jiang,Yuanliang Sun,Jia Li,Hui Xiong,Jian Guo*

Main category: cs.CL

TL;DR: GraphSearch通过双通道检索策略和模块化框架提升了图检索增强生成（GraphRAG）的证据检索和推理能力，有效提高了多跳推理任务的准确性和生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有GraphRAG方法检索浅层，未能获取所有关键信息，且对结构化图数据利用效率低，导致复杂查询推理效果受限。

Method: 提出GraphSearch，一个包含六个模块的深度搜索流程，实现多轮交互和迭代推理；采用语义查询和关系查询的双通道检索策略，充分利用文本和图结构数据。

Result: 在六个多跳RAG基准测试中，GraphSearch显著提升了答案准确率和生成质量，优于传统策略。

Conclusion: GraphSearch为图检索增强生成提供了一种高效且有效的检索-推理框架，促进了复杂查询下的推理性能提升。

Abstract: Graph Retrieval-Augmented Generation (GraphRAG) enhances factual reasoning in
LLMs by structurally modeling knowledge through graph-based representations.
However, existing GraphRAG approaches face two core limitations: shallow
retrieval that fails to surface all critical evidence, and inefficient
utilization of pre-constructed structural graph data, which hinders effective
reasoning from complex queries. To address these challenges, we propose
\textsc{GraphSearch}, a novel agentic deep searching workflow with dual-channel
retrieval for GraphRAG. \textsc{GraphSearch} organizes the retrieval process
into a modular framework comprising six modules, enabling multi-turn
interactions and iterative reasoning. Furthermore, \textsc{GraphSearch} adopts
a dual-channel retrieval strategy that issues semantic queries over chunk-based
text data and relational queries over structural graph data, enabling
comprehensive utilization of both modalities and their complementary strengths.
Experimental results across six multi-hop RAG benchmarks demonstrate that
\textsc{GraphSearch} consistently improves answer accuracy and generation
quality over the traditional strategy, confirming \textsc{GraphSearch} as a
promising direction for advancing graph retrieval-augmented generation.

</details>


### [52] [From Outliers to Topics in Language Models: Anticipating Trends in News Corpora](https://arxiv.org/abs/2509.22030)
*Evangelia Zve,Benjamin Icard,Alice Breton,Lila Sainero,Gauvain Bourgne,Jean-Gabriel Ganascia*

Main category: cs.CL

TL;DR: 该论文研究了在新闻主题建模中，通常被视为噪声的离群点如何成为动态新闻语料中新兴主题的弱信号。


<details>
  <summary>Details</summary>
Motivation: 传统主题建模中，离群点常被忽视为噪声，但这些离群点可能包含潜在的新兴主题信号，值得深入研究。

Method: 利用先进语言模型的向量嵌入技术，结合累积聚类方法，追踪法语和英语新闻数据中离群点的演变过程。

Result: 研究发现，无论语言和模型如何，离群点都会随时间演变成连贯的主题，验证了其作为新兴主题信号的角色。

Conclusion: 离群点不仅是噪声，它们在动态新闻语料中表现为新兴主题的早期弱信号，为主题检测提供了新的视角。

Abstract: This paper examines how outliers, often dismissed as noise in topic modeling,
can act as weak signals of emerging topics in dynamic news corpora. Using
vector embeddings from state-of-the-art language models and a cumulative
clustering approach, we track their evolution over time in French and English
news datasets focused on corporate social responsibility and climate change.
The results reveal a consistent pattern: outliers tend to evolve into coherent
topics over time across both models and languages.

</details>


### [53] [Taxonomy of Comprehensive Safety for Clinical Agents](https://arxiv.org/abs/2509.22041)
*Jean Seo,Hyunkyung Lee,Gibaeg Kim,Wooseok Han,Jaehyo Yoo,Seungseop Lim,Kihun Shin,Eunho Yang*

Main category: cs.CL

TL;DR: 本文提出了一种针对临床聊天机器人安全性的细粒度分类方法TACOS，该方法整合了安全过滤和工具选择，涵盖了21类用户意图，验证了其在临床应用中的有效性。


<details>
  <summary>Details</summary>
Motivation: 临床聊天机器人中不准确或有害的回应可能造成严重后果，现有的安全措施难以满足临床领域细致和复杂的安全需求。

Method: 提出了TACOS分类法，将安全过滤与工具选择融合为一个用户意图分类步骤，设计了包含21个类别的细粒度分类体系，并制作了标注数据集进行验证。

Result: 实验结果表明，TACOS专为临床环境设计的分类体系有效提升了安全性，揭示了训练数据分布和预训练模型知识的有用洞见。

Conclusion: TACOS为临床智能代理提供了一种有效的安全分类框架，有利于提升临床聊天机器人的安全响应能力。

Abstract: Safety is a paramount concern in clinical chatbot applications, where
inaccurate or harmful responses can lead to serious consequences. Existing
methods--such as guardrails and tool calling--often fall short in addressing
the nuanced demands of the clinical domain. In this paper, we introduce TACOS
(TAxonomy of COmprehensive Safety for Clinical Agents), a fine-grained,
21-class taxonomy that integrates safety filtering and tool selection into a
single user intent classification step. TACOS is a taxonomy that can cover a
wide spectrum of clinical and non-clinical queries, explicitly modeling varying
safety thresholds and external tool dependencies. To validate our framework, we
curate a TACOS-annotated dataset and perform extensive experiments. Our results
demonstrate the value of a new taxonomy specialized for clinical agent
settings, and reveal useful insights about train data distribution and
pretrained knowledge of base models.

</details>


### [54] [Fuzzy Reasoning Chain (FRC): An Innovative Reasoning Framework from Fuzziness to Clarity](https://arxiv.org/abs/2509.22054)
*Ping Chen,Xiang Liu,Zhaoxiang Liu,Zezhou Chen,Xingpeng Zhang,Huan Hu,Zipeng Wang,Kai Wang,Shuming Shi,Shiguo Lian*

Main category: cs.CL

TL;DR: 本文提出了模糊推理链（FRC）框架，结合了大型语言模型的语义先验和连续模糊隶属度，实现了概率推理与模糊推理的结合，提升了对模糊、多义和不确定文本的处理能力。


<details>
  <summary>Details</summary>
Motivation: 当前自然语言处理在处理模糊、多义和不确定性文本时存在挑战，传统基于概率的方法难以捕捉矛盾或不确定信号。

Method: 引入模糊推理链（FRC）框架，将大型语言模型的语义先验与连续模糊隶属度结合，实现概率推理与模糊推理的显式交互，逐步将模糊输入转化为明确可解释的决策。

Result: 在情感分析任务中，理论分析和实验证明FRC确保了推理稳定性，并促进了不同模型规模间的知识迁移。

Conclusion: FRC为处理细微和模糊表达提供了一种通用机制，提升了模型的可解释性和鲁棒性。

Abstract: With the rapid advancement of large language models (LLMs), natural language
processing (NLP) has achieved remarkable progress. Nonetheless, significant
challenges remain in handling texts with ambiguity, polysemy, or uncertainty.
We introduce the Fuzzy Reasoning Chain (FRC) framework, which integrates LLM
semantic priors with continuous fuzzy membership degrees, creating an explicit
interaction between probability-based reasoning and fuzzy membership reasoning.
This transition allows ambiguous inputs to be gradually transformed into clear
and interpretable decisions while capturing conflicting or uncertain signals
that traditional probability-based methods cannot. We validate FRC on sentiment
analysis tasks, where both theoretical analysis and empirical results show that
it ensures stable reasoning and facilitates knowledge transfer across different
model scales. These findings indicate that FRC provides a general mechanism for
managing subtle and ambiguous expressions with improved interpretability and
robustness.

</details>


### [55] [RedNote-Vibe: A Dataset for Capturing Temporal Dynamics of AI-Generated Text in Social Media](https://arxiv.org/abs/2509.22055)
*Yudong Li,Yufei Sun,Yuhan Yao,Peiru Yang,Wanyue Li,Jiajun Zou,Yongfeng Huang,Linlin Shen*

Main category: cs.CL

TL;DR: 本文介绍了面向社交媒体AI生成文本分析的第一个纵向数据集RedNote-Vibe，并提出了一种基于心理语言学特征的可解释AIGT检测框架PLAD。


<details>
  <summary>Details</summary>
Motivation: 现有AIGT检测多为静态分析，缺乏对内容随时间演变和用户互动影响的研究。

Method: 构建了涵盖5年用户互动数据的RedNote-Vibe数据集，提出基于心理语言学特征的可解释AIGT检测框架PLAD。

Result: PLAD在检测性能上表现优异，揭示了人类与AI生成内容的语言特征及其与用户互动的复杂关系。

Conclusion: RedNote-Vibe数据集促进了社交媒体AIGT时序动态研究，PLAD框架有效提升AIGT检测并增强解释性。

Abstract: The proliferation of Large Language Models (LLMs) has led to widespread
AI-Generated Text (AIGT) on social media platforms, creating unique challenges
where content dynamics are driven by user engagement and evolve over time.
However, existing datasets mainly depict static AIGT detection. In this work,
we introduce RedNote-Vibe, the first longitudinal (5-years) dataset for social
media AIGT analysis. This dataset is sourced from Xiaohongshu platform,
containing user engagement metrics (e.g., likes, comments) and timestamps
spanning from the pre-LLM period to July 2025, which enables research into the
temporal dynamics and user interaction patterns of AIGT. Furthermore, to detect
AIGT in the context of social media, we propose PsychoLinguistic AIGT Detection
Framework (PLAD), an interpretable approach that leverages psycholinguistic
features. Our experiments show that PLAD achieves superior detection
performance and provides insights into the signatures distinguishing human and
AI-generated content. More importantly, it reveals the complex relationship
between these linguistic features and social media engagement. The dataset is
available at https://github.com/testuser03158/RedNote-Vibe.

</details>


### [56] [The QCET Taxonomy of Standard Quality Criterion Names and Definitions for the Evaluation of NLP Systems](https://arxiv.org/abs/2509.22064)
*Anya Belz,Simon Mille,Craig Thomson*

Main category: cs.CL

TL;DR: 本文针对NLP评估中不同名称的质量标准实际上可能评估不同质量方面的问题，提出了一个标准化质量标准名称和定义的体系QCET。


<details>
  <summary>Details</summary>
Motivation: 为了提升NLP评估结果的可比性，解决不同评估之间因质量标准名称含义不明确导致的比较困难，从而促进领域科学进步。

Method: 基于对已有NLP评估的三个调查，采用描述性方法构建质量标准命名和定义的层次化分类体系QCET，建立名称与定义的标准映射。

Result: 提出了QCET体系及其资源，能够实现现有评估的可比性验证，指导新评估设计，及支持合规评估。

Conclusion: QCET作为标准化质量标准体系，有助于解决评估可比性问题，推动NLP领域科学评估的发展。

Abstract: Prior work has shown that two NLP evaluation experiments that report results
for the same quality criterion name (e.g. Fluency) do not necessarily evaluate
the same aspect of quality, and the comparability implied by the name can be
misleading. Not knowing when two evaluations are comparable in this sense means
we currently lack the ability to draw reliable conclusions about system quality
on the basis of multiple, independently conducted evaluations. This in turn
hampers the ability of the field to progress scientifically as a whole, a
pervasive issue in NLP since its beginning (Sparck Jones, 1981). It is hard to
see how the issue of unclear comparability can be fully addressed other than by
the creation of a standard set of quality criterion names and definitions that
the several hundred quality criterion names actually in use in the field can be
mapped to, and grounded in. Taking a strictly descriptive approach, the QCET
Quality Criteria for Evaluation Taxonomy derives a standard set of quality
criterion names and definitions from three surveys of evaluations reported in
NLP, and structures them into a hierarchy where each parent node captures
common aspects of its child nodes. We present QCET and the resources it
consists of, and discuss its three main uses in (i) establishing comparability
of existing evaluations, (ii) guiding the design of new evaluations, and (iii)
assessing regulatory compliance.

</details>


### [57] [Fine-tuning Done Right in Model Editing](https://arxiv.org/abs/2509.22072)
*Wanli Yang,Fei Sun,Rui Tang,Hongyu Zang,Du Su,Qi Cao,Jingang Wang,Huawei Shen,Xueqi Cheng*

Main category: cs.CL

TL;DR: 本文重新审视了微调在大型语言模型编辑中的有效性，提出将其优化流程从深度优先改为广度优先并采用局部参数调优显著提升了编辑性能。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为微调方法不适合模型编辑，然而作者认为这是由于采用了深度优先的单通道处理流程及非最优参数调优位置导致的误解。

Method: 将微调流程恢复为广度优先（基于epoch的）mini-batch优化，并系统分析调优参数位置，提出局部调优方法LocFT-BF。

Result: LocFT-BF在多种大型语言模型与数据集上表现优异，超越了现有方法，能支持10万次编辑及720亿参数模型的编辑。

Conclusion: 通过纠正对微调的误解及引入有效的局部调优策略，本文将微调从被低估的基础方法提升为模型编辑中领先的手段，为未来研究奠定基础。

Abstract: Fine-tuning, a foundational method for adapting large language models, has
long been considered ineffective for model editing. Here, we challenge this
belief, arguing that the reported failure arises not from the inherent
limitation of fine-tuning itself, but from adapting it to the sequential nature
of the editing task, a single-pass depth-first pipeline that optimizes each
sample to convergence before moving on. While intuitive, this depth-first
pipeline coupled with sample-wise updating over-optimizes each edit and induces
interference across edits. Our controlled experiments reveal that simply
restoring fine-tuning to the standard breadth-first (i.e., epoch-based)
pipeline with mini-batch optimization substantially improves its effectiveness
for model editing. Moreover, fine-tuning in editing also suffers from
suboptimal tuning parameter locations inherited from prior methods. Through
systematic analysis of tuning locations, we derive LocFT-BF, a simple and
effective localized editing method built on the restored fine-tuning framework.
Extensive experiments across diverse LLMs and datasets demonstrate that
LocFT-BF outperforms state-of-the-art methods by large margins. Notably, to our
knowledge, it is the first to sustain 100K edits and 72B-parameter models,10 x
beyond prior practice, without sacrificing general capabilities. By clarifying
a long-standing misconception and introducing a principled localized tuning
strategy, we advance fine-tuning from an underestimated baseline to a leading
method for model editing, establishing a solid foundation for future research.

</details>


### [58] [COSPADI: Compressing LLMs via Calibration-Guided Sparse Dictionary Learning](https://arxiv.org/abs/2509.22075)
*Dmitriy Shopkhoev,Denis Makhov,Magauiya Zhussip,Ammar Ali,Stamatios Lefkimmiatis*

Main category: cs.CL

TL;DR: 本文提出了一种名为CoSpaDi的训练后无训练压缩框架，通过稀疏字典学习替代传统的低秩分解，提升大型语言模型的压缩效果和精度。


<details>
  <summary>Details</summary>
Motivation: 传统低秩权重近似方法结构过于刚性，导致模型精度明显下降，亟需一种更加灵活且高效的压缩方法。

Method: CoSpaDi采用稠密字典与列稀疏系数矩阵的结构稀疏分解，实现不同列在自适应选取的子空间中表达；利用小规模校准数据优化分解，最小化功能重构误差，实现无微调压缩；支持高效稀疏-稠密乘法及后期量化。

Result: 在多个Llama和Qwen模型上，CoSpaDi在20-50%压缩率下，在精度和困惑度指标上均优于现有数据感知低秩方法。

Conclusion: 结构稀疏字典学习是替代传统低秩方法进行大型语言模型高效部署的有力方案，兼顾压缩效率与模型性能。

Abstract: Post-training compression of large language models (LLMs) largely relies on
low-rank weight approximation, which represents each column of a weight matrix
in a shared low-dimensional subspace. While this is a computationally efficient
strategy, the imposed structural constraint is rigid and can lead to a
noticeable model accuracy drop. In this work, we propose CoSpaDi (Compression
via Sparse Dictionary Learning), a novel training-free compression framework
that replaces low-rank decomposition with a more flexible structured sparse
factorization in which each weight matrix is represented with a dense
dictionary and a column-sparse coefficient matrix. This formulation enables a
union-of-subspaces representation: different columns of the original weight
matrix are approximated in distinct subspaces spanned by adaptively selected
dictionary atoms, offering greater expressiveness than a single invariant
basis. Crucially, CoSpaDi leverages a small calibration dataset to optimize the
factorization such that the output activations of compressed projection layers
closely match those of the original ones, thereby minimizing functional
reconstruction error rather than mere weight approximation. This data-aware
strategy preserves better model fidelity without any fine-tuning under
reasonable compression ratios. Moreover, the resulting structured sparsity
allows efficient sparse-dense matrix multiplication and is compatible with
post-training quantization for further memory and latency gains. We evaluate
CoSpaDi across multiple Llama and Qwen models under per-layer and per-group
settings at 20-50\% compression ratios, demonstrating consistent superiority
over state-of-the-art data-aware low-rank methods both in accuracy and
perplexity. Our results establish structured sparse dictionary learning as a
powerful alternative to conventional low-rank approaches for efficient LLM
deployment.

</details>


### [59] [Multilingual Dialogue Generation and Localization with Dialogue Act Scripting](https://arxiv.org/abs/2509.22086)
*Justin Vasselli,Eunike Andriani Kardinata,Yusuke Sakai,Taro Watanabe*

Main category: cs.CL

TL;DR: 本文提出了Dialogue Act Script（DAS）框架，通过结构化的意图表示生成多语言对话，提升对话的文化适应性和自然流畅性。


<details>
  <summary>Details</summary>
Motivation: 非英语对话数据集稀缺，直接翻译英文对话易产生不自然和文化不适宜的问题。

Method: DAS使用结构化对话行为表示，避免直接翻译，支持灵活本地化，生成与目标语言文化和语境一致的新对话内容。

Result: 在人类评测中，DAS生成的意大利语、德语和中文对话在文化相关性、连贯性和情境适宜性上均优于机器和人工翻译的对话。

Conclusion: DAS框架有效提升了多语言对话生成的自然性和文化适应性，克服了直接翻译带来的局限。

Abstract: Non-English dialogue datasets are scarce, and models are often trained or
evaluated on translations of English-language dialogues, an approach which can
introduce artifacts that reduce their naturalness and cultural appropriateness.
This work proposes Dialogue Act Script (DAS), a structured framework for
encoding, localizing, and generating multilingual dialogues from abstract
intent representations. Rather than translating dialogue utterances directly,
DAS enables the generation of new dialogues in the target language that are
culturally and contextually appropriate. By using structured dialogue act
representations, DAS supports flexible localization across languages,
mitigating translationese and enabling more fluent, naturalistic conversations.
Human evaluations across Italian, German, and Chinese show that DAS-generated
dialogues consistently outperform those produced by both machine and human
translators on measures of cultural relevance, coherence, and situational
appropriateness.

</details>


### [60] [S2J: Bridging the Gap Between Solving and Judging Ability in Generative Reward Models](https://arxiv.org/abs/2509.22099)
*Shaoning Sun,Jiachen Yu,Zongqi Wang,Xuewei Yang,Tianle Gu,Yujiu Yang*

Main category: cs.CL

TL;DR: 本文针对生成式奖励模型在判断能力和解决能力间存在的差距提出了S2J方法，显著提升了模型的判断能力。


<details>
  <summary>Details</summary>
Motivation: 发现生成式奖励模型（GRMs）在某些查询上虽然能解决问题，但难以做出正确判断，存在解决-判断能力差距。

Method: 提出Solve-to-Judge（S2J）方法，同时利用GRM的解决和判断能力进行监督，连接问题解决和评估能力以缩小差距。

Result: S2J方法减少了解决-判断差距16.2%，提升判断性能5.8%，在相同基础模型下取得了最先进性能，且训练数据量更小。

Conclusion: 通过S2J方法，GRM的判断能力得到有效提升，实现了自我进化，无需依赖更强外部模型。

Abstract: With the rapid development of large language models (LLMs), generative reward
models (GRMs) have been widely adopted for reward modeling and evaluation.
Previous studies have primarily focused on training specialized GRMs by
optimizing them on preference datasets with the judgment correctness as
supervision. While it's widely accepted that GRMs with stronger problem-solving
capabilities typically exhibit superior judgment abilities, we first identify a
significant solve-to-judge gap when examining individual queries. Specifically,
the solve-to-judge gap refers to the phenomenon where GRMs struggle to make
correct judgments on some queries (14%-37%), despite being fully capable of
solving them. In this paper, we propose the Solve-to-Judge (S2J) approach to
address this problem. Specifically, S2J simultaneously leverages both the
solving and judging capabilities on a single GRM's output for supervision,
explicitly linking the GRM's problem-solving and evaluation abilities during
model optimization, thereby narrowing the gap. Our comprehensive experiments
demonstrate that S2J effectively reduces the solve-to-judge gap by 16.2%,
thereby enhancing the model's judgment performance by 5.8%. Notably, S2J
achieves state-of-the-art (SOTA) performance among GRMs built on the same base
model while utilizing a significantly smaller training dataset. Moreover, S2J
accomplishes this through self-evolution without relying on more powerful
external models for distillation.

</details>


### [61] [Think Right, Not More: Test-Time Scaling for Numerical Claim Verification](https://arxiv.org/abs/2509.22101)
*Primakov Chungkham,V Venktesh,Vinay Setty,Avishek Anand*

Main category: cs.CL

TL;DR: 该论文针对复杂数值类事实核查问题，提出利用大语言模型的测试时计算扩展（TTS）结合验证器模型（VERIFIERFC）多路径推理，大幅提升核查准确率和效率。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在核查需要组合推理和数值推理的复杂真实世界事实时表现不足，原因在于其理解数值细节能力弱及推理过程中易出现推理漂移，导致误判。

Method: 在测试阶段对大语言模型进行多路径推理诱导，并训练一个Verifier模型在多条推理路径中选择最可能正确的结果。此外还设计了基于事实复杂度的自适应机制，有效减少不必要的计算，提高效率。

Result: 利用测试时计算扩展显著缓解了推理漂移问题，提升数值事实核查性能。自适应机制使得计算效率提高了1.8倍的同时，性能提升达18.8%，优于传统单次推理方法。

Conclusion: 该研究展示了通过多路径推理结合选择机制大幅提升复杂数字事实核查能力的有效性，提出的自适应计算策略兼顾准确率与计算效率，为复杂事实核查提供了新的可行方案。

Abstract: Fact-checking real-world claims, particularly numerical claims, is inherently
complex that require multistep reasoning and numerical reasoning for verifying
diverse aspects of the claim. Although large language models (LLMs) including
reasoning models have made tremendous advances, they still fall short on
fact-checking real-world claims that require a combination of compositional and
numerical reasoning. They are unable to understand nuance of numerical aspects,
and are also susceptible to the reasoning drift issue, where the model is
unable to contextualize diverse information resulting in misinterpretation and
backtracking of reasoning process. In this work, we systematically explore
scaling test-time compute (TTS) for LLMs on the task of fact-checking complex
numerical claims, which entails eliciting multiple reasoning paths from an LLM.
We train a verifier model (VERIFIERFC) to navigate this space of possible
reasoning paths and select one that could lead to the correct verdict. We
observe that TTS helps mitigate the reasoning drift issue, leading to
significant performance gains for fact-checking numerical claims. To improve
compute efficiency in TTS, we introduce an adaptive mechanism that performs TTS
selectively based on the perceived complexity of the claim. This approach
achieves 1.8x higher efficiency than standard TTS, while delivering a notable
18.8% performance improvement over single-shot claim verification methods. Our
code and data can be found at https://github.com/VenkteshV/VerifierFC

</details>


### [62] [Universal Legal Article Prediction via Tight Collaboration between Supervised Classification Model and LLM](https://arxiv.org/abs/2509.22119)
*Xiao Chi,Wenlin Zhong,Yiquan Wu,Wei Wang,Kun Kuang,Fei Wu,Minghui Xiong*

Main category: cs.CL

TL;DR: 本文提出了一个名为Uni-LAP的通用法律条文预测框架，通过结合监督分类模型和大型语言模型的优势，提高法律事实描述下相关法律条文的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的法律条文预测方法存在难以全面捕捉复杂事实模式、生成模型预测性能不足及跨法域适用性差等问题。

Method: Uni-LAP框架紧密结合监督分类模型与大型语言模型，监督模型引入了一种Top-K损失函数生成候选法律条文，大型语言模型采用类三段论推理机制优化最终预测结果。

Result: 在多个司法管辖区的数据集上，Uni-LAP的表现始终优于现有基线方法，显示出较强的有效性和通用性。

Conclusion: Uni-LAP通过创新性整合两类模型的优点，有效提升了法律条文预测的准确性和跨法域适应能力，具备广泛应用前景。

Abstract: Legal Article Prediction (LAP) is a critical task in legal text
classification, leveraging natural language processing (NLP) techniques to
automatically predict relevant legal articles based on the fact descriptions of
cases. As a foundational step in legal decision-making, LAP plays a pivotal
role in determining subsequent judgments, such as charges and penalties.
Despite its importance, existing methods face significant challenges in
addressing the complexities of LAP. Supervised classification models (SCMs),
such as CNN and BERT, struggle to fully capture intricate fact patterns due to
their inherent limitations. Conversely, large language models (LLMs), while
excelling in generative tasks, perform suboptimally in predictive scenarios due
to the abstract and ID-based nature of legal articles. Furthermore, the
diversity of legal systems across jurisdictions exacerbates the issue, as most
approaches are tailored to specific countries and lack broader applicability.
To address these limitations, we propose Uni-LAP, a universal framework for
legal article prediction that integrates the strengths of SCMs and LLMs through
tight collaboration. Specifically, in Uni-LAP, the SCM is enhanced with a novel
Top-K loss function to generate accurate candidate articles, while the LLM
employs syllogism-inspired reasoning to refine the final predictions. We
evaluated Uni-LAP on datasets from multiple jurisdictions, and empirical
results demonstrate that our approach consistently outperforms existing
baselines, showcasing its effectiveness and generalizability.

</details>


### [63] [Multilingual Vision-Language Models, A Survey](https://arxiv.org/abs/2509.22123)
*Andrei-Alexandru Manea,Jindřich Libovický*

Main category: cs.CL

TL;DR: 本文综述了多语言视觉语言模型，涵盖31个模型和21个基准，探讨了语言中立性与文化意识之间的矛盾及评价方法的不匹配。


<details>
  <summary>Details</summary>
Motivation: 分析多语言视觉语言模型在处理不同语言和文化内容时面临的挑战，识别语言中立性与文化适应性的张力。

Method: 回顾31个多语言视觉语言模型和21个评估基准，归纳训练方法和评测策略，特别关注对比学习和数据多样性的影响。

Result: 发现多数训练方法偏向语言中立性，评价基准依赖基于翻译的方法，存在跨语言能力不均衡和训练目标与评价目标之间的差距。

Conclusion: 多语言视觉语言模型需在保持跨语言一致性和适应文化背景之间取得平衡，未来工作应加强文化相关内容的整合及评价体系的完善。

Abstract: This survey examines multilingual vision-language models that process text
and images across languages. We review 31 models and 21 benchmarks, spanning
encoder-only and generative architectures, and identify a key tension between
language neutrality (consistent cross-lingual representations) and cultural
awareness (adaptation to cultural contexts). Current training methods favor
neutrality through contrastive learning, while cultural awareness depends on
diverse data. Two-thirds of evaluation benchmarks use translation-based
approaches prioritizing semantic consistency, though recent work incorporates
culturally grounded content. We find discrepancies in cross-lingual
capabilities and gaps between training objectives and evaluation goals.

</details>


### [64] [FoodSEM: Large Language Model Specialized in Food Named-Entity Linking](https://arxiv.org/abs/2509.22125)
*Ana Gjorgjevikj,Matej Martinc,Gjorgjina Cenikj,Sašo Džeroski,Barbara Koroušić Seljak,Tome Eftimov*

Main category: cs.CL

TL;DR: 本文介绍了FoodSEM，一个面向食品领域的命名实体链接大型语言模型，通过微调实现高准确率，达到最高98%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 现有的通用或领域特定大型语言模型无法准确完成食品相关领域的命名实体链接任务，提升该任务表现具有实际应用价值。

Method: 采用指令-响应场景，通过微调食品相关本体（如FoodOn、SNOMED-CT、Hansard分类体系）上的标注语料，训练FoodSEM模型实现食品实体链接。

Result: FoodSEM在多个本体和数据集上的表现优于零样本、一样本和少样本提示基线模型，部分数据集F1得分高达98%。

Conclusion: FoodSEM及其标注语料和资源公开发布，为食品领域文本语义理解和命名实体链接任务提供了强有力的基线和工具，促进该领域研究和应用前进。

Abstract: This paper introduces FoodSEM, a state-of-the-art fine-tuned open-source
large language model (LLM) for named-entity linking (NEL) to food-related
ontologies. To the best of our knowledge, food NEL is a task that cannot be
accurately solved by state-of-the-art general-purpose (large) language models
or custom domain-specific models/systems. Through an instruction-response (IR)
scenario, FoodSEM links food-related entities mentioned in a text to several
ontologies, including FoodOn, SNOMED-CT, and the Hansard taxonomy. The FoodSEM
model achieves state-of-the-art performance compared to related models/systems,
with F1 scores even reaching 98% on some ontologies and datasets. The presented
comparative analyses against zero-shot, one-shot, and few-shot LLM prompting
baselines further highlight FoodSEM's superior performance over its
non-fine-tuned version. By making FoodSEM and its related resources publicly
available, the main contributions of this article include (1) publishing a
food-annotated corpora into an IR format suitable for LLM
fine-tuning/evaluation, (2) publishing a robust model to advance the semantic
understanding of text in the food domain, and (3) providing a strong baseline
on food NEL for future benchmarking.

</details>


### [65] [R-Capsule: Compressing High-Level Plans for Efficient Large Language Model Reasoning](https://arxiv.org/abs/2509.22131)
*Hongyu Shan,Mingyang Song,Chang Dai,Di Liang,Han Chen*

Main category: cs.CL

TL;DR: 提出了解释型高效推理框架R-Capsule，通过压缩高层计划为少量潜在推理胶囊，并结合显式步骤，实现了推理效率和透明度的平衡，提升了复杂任务的准确性和解释性。


<details>
  <summary>Details</summary>
Motivation: 传统的连锁思维(CoT)提示虽然促进了复杂推理，但高冗余信息增加延迟和内存使用，且错误易传播，亟需一种兼具效率和透明度的推理方法。

Method: 引入信息瓶颈原则，通过低容量的潜在推理胶囊压缩高层推理计划，辅以任务及计划重建双重损失，确保胶囊信息最小且充足，同时保持执行步骤轻量或显式。

Result: 方法显著减少了推理过程中可见的Token数量，同时在多个复杂基准测试中保持或提升了准确率，且提高了潜在空间的可解释性。

Conclusion: R-Capsule框架成功实现了推理效率、准确性和解释性的平衡，是改进大语言模型复杂推理的有效途径。

Abstract: Chain-of-Thought (CoT) prompting helps Large Language Models (LLMs) tackle
complex reasoning by eliciting explicit step-by-step rationales. However, CoT's
verbosity increases latency and memory usage and may propagate early errors
across long chains. We propose the Reasoning Capsule (R-Capsule), a framework
that aims to combine the efficiency of latent reasoning with the transparency
of explicit CoT. The core idea is to compress the high-level plan into a small
set of learned latent tokens (a Reasoning Capsule) while keeping execution
steps lightweight or explicit. This hybrid approach is inspired by the
Information Bottleneck (IB) principle, where we encourage the capsule to be
approximately minimal yet sufficient for the task. Minimality is encouraged via
a low-capacity bottleneck, which helps improve efficiency. Sufficiency is
encouraged via a dual objective: a primary task loss for answer accuracy and an
auxiliary plan-reconstruction loss that encourages the capsule to faithfully
represent the original textual plan. The reconstruction objective helps ground
the latent space, thereby improving interpretability and reducing the use of
uninformative shortcuts. Our framework strikes a balance between efficiency,
accuracy, and interpretability, thereby reducing the visible token footprint of
reasoning while maintaining or improving accuracy on complex benchmarks. Our
codes are available at:
https://anonymous.4open.science/r/Reasoning-Capsule-7BE0

</details>


### [66] [Bridging Draft Policy Misalignment: Group Tree Optimization for Speculative Decoding](https://arxiv.org/abs/2509.22134)
*Shijing Hu,Jingyang Li,Zhihui Lu,Pan Zhou*

Main category: cs.CL

TL;DR: 提出了一种新的训练方法Group Tree Optimization (GTO)以提升大型语言模型推理速度，通过使训练目标与推理时的多分支策略对齐，提高了推理加速效果。


<details>
  <summary>Details</summary>
Motivation: 现有的训练目标只优化单一路径的预测，而推理过程中采用多分支策略进行并行验证，存在策略不匹配导致加速效果受限的问题。

Method: GTO包含两个核心组件：Draft Tree Reward（无采样的目标函数，直接衡量多分支树的接收长度）和基于组的草稿策略训练（通过对比当前与固定参考模型的树，形成去偏的优势信号，采用类似PPO的优化方法沿最长序列更新模型）。

Result: 在不同任务（对话、代码生成、数学推理）及多种大型语言模型上，GTO相比之前的最优方法EAGLE-3提升了7.4%的接收长度和7.7%的加速比。

Conclusion: GTO有效解决了草稿策略与多分支推理策略的不匹配问题，提供了一种通用且实用的高效大型语言模型推理方案。

Abstract: Speculative decoding accelerates large language model (LLM) inference by
letting a lightweight draft model propose multiple tokens that the target model
verifies in parallel. Yet existing training objectives optimize only a single
greedy draft path, while decoding follows a tree policy that re-ranks and
verifies multiple branches. This draft policy misalignment limits achievable
speedups. We introduce Group Tree Optimization (GTO), which aligns training
with the decoding-time tree policy through two components: (i) Draft Tree
Reward, a sampling-free objective equal to the expected acceptance length of
the draft tree under the target model, directly measuring decoding performance;
(ii) Group-based Draft Policy Training, a stable optimization scheme that
contrasts trees from the current and a frozen reference draft model, forming
debiased group-standardized advantages and applying a PPO-style surrogate along
the longest accepted sequence for robust updates. We further prove that
increasing our Draft Tree Reward provably improves acceptance length and
speedup. Across dialogue (MT-Bench), code (HumanEval), and math (GSM8K), and
multiple LLMs (e.g., LLaMA-3.1-8B, LLaMA-3.3-70B, Vicuna-1.3-13B,
DeepSeek-R1-Distill-LLaMA-8B), GTO increases acceptance length by 7.4% and
yields an additional 7.7% speedup over prior state-of-the-art EAGLE-3. By
bridging draft policy misalignment, GTO offers a practical, general solution
for efficient LLM inference.

</details>


### [67] [NFDI4DS Shared Tasks for Scholarly Document Processing](https://arxiv.org/abs/2509.22141)
*Raia Abu Ahmad,Rana Abdulla,Tilahun Abedissa Taffa,Soeren Auer,Hamed Babaei Giglou,Ekaterina Borisova,Zongxiong Chen,Stefan Dietze,Jennifer DSouza,Mayra Elwes,Genet-Asefa Gesese,Shufan Jiang,Ekaterina Kutafina,Philipp Mayr,Georg Rehm,Sameer Sadruddin,Sonja Schimmler,Daniel Schneider,Kanishka Silva,Sharmila Upadhyaya,Ricardo Usbeck*

Main category: cs.CL

TL;DR: 本文综述了德国国家数据科学与人工智能研究数据基础设施联盟（NFDI4DS）下的十二个共享任务，这些任务促进了学术文献处理领域的研究进展。


<details>
  <summary>Details</summary>
Motivation: 推动透明、可重复且符合FAIR原则的科研实践，通过共享任务促进社区标准化评估和方法创新。

Method: 介绍了NFDI4DS联盟举办的十二个共享任务，涵盖学术文献处理的多样化挑战，提供开放获取的数据集、模型和工具。

Result: 这些共享任务在顶级会议举办，促进了方法学创新，并将资源整合进联盟的研究数据基础设施中。

Conclusion: 通过共享任务，促进了学术文献处理领域的社区合作和资源共享，推动科研的开放和标准化。

Abstract: Shared tasks are powerful tools for advancing research through
community-based standardised evaluation. As such, they play a key role in
promoting findable, accessible, interoperable, and reusable (FAIR), as well as
transparent and reproducible research practices. This paper presents an updated
overview of twelve shared tasks developed and hosted under the German National
Research Data Infrastructure for Data Science and Artificial Intelligence
(NFDI4DS) consortium, covering a diverse set of challenges in scholarly
document processing. Hosted at leading venues, the tasks foster methodological
innovations and contribute open-access datasets, models, and tools for the
broader research community, which are integrated into the consortium's research
data infrastructure.

</details>


### [68] [From Long to Lean: Performance-aware and Adaptive Chain-of-Thought Compression via Multi-round Refinement](https://arxiv.org/abs/2509.22144)
*Jianzhi Yan,Le Liu,Youcheng Pan,Shiwei Chen,Zike Yuan,Yang Xiang,Buzhou Tang*

Main category: cs.CL

TL;DR: 本文提出了一种多轮自适应链式思维压缩（MACC）框架，通过多轮优化有效压缩链式思维长度，提升推理效率并提高准确率。


<details>
  <summary>Details</summary>
Motivation: 链式思维推理提高复杂任务的性能，但由于生成内容冗长导致推理延迟显著，需要一种方法在保证性能的同时压缩推理长度。

Method: MACC利用令牌弹性现象，通过多轮精炼逐步压缩链式思维，并自适应确定每个输入的最佳压缩深度，同时利用训练集的可解释特征预测测试时性能。

Result: MACC在准确率上平均提升5.6%，平均减少47个令牌长度，显著降低推理延迟，且能够有效预测测试时的准确率和长度，支持高效模型选择。

Conclusion: 链式思维压缩通过MACC框架不仅提升了性能和推理效率，还能基于训练特征准确预测推理效果，实现了高效且可预测的推理优化。

Abstract: Chain-of-Thought (CoT) reasoning improves performance on complex tasks but
introduces significant inference latency due to verbosity. We propose
Multiround Adaptive Chain-of-Thought Compression (MACC), a framework that
leverages the token elasticity phenomenon--where overly small token budgets can
paradoxically increase output length--to progressively compress CoTs via
multiround refinement. This adaptive strategy allows MACC to determine the
optimal compression depth for each input. Our method achieves an average
accuracy improvement of 5.6 percent over state-of-the-art baselines, while also
reducing CoT length by an average of 47 tokens and significantly lowering
latency. Furthermore, we show that test-time performance--accuracy and token
length--can be reliably predicted using interpretable features like perplexity
and compression rate on the training set. Evaluated across different models,
our method enables efficient model selection and forecasting without repeated
fine-tuning, demonstrating that CoT compression is both effective and
predictable. Our code will be released in https://github.com/Leon221220/MACC.

</details>


### [69] [Mixture of Detectors: A Compact View of Machine-Generated Text Detection](https://arxiv.org/abs/2509.22147)
*Sai Teja Lekkala,Yadagiri Annepaka,Arun Kumar Challa,Samatha Reddy Machireddy,Partha Pakray,Chukhu Chunka*

Main category: cs.CL

TL;DR: 本文探讨了机器生成文本检测的问题，包括多种应用场景，如文档层面的二元和多分类、生成器归属判定、句子级分割以及针对检测的对抗攻击。提出了BMAS English数据集以支持这些任务。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）逐渐超越人类创造力，如何鉴别机器生成文本的真实性和保护人类创造力成为关键问题。

Method: 提出了多种机器生成文本检测任务，包括文档级分类（单二分类、多分类及生成器归属）、句子级文本分割及对抗攻击检测，并构建了BMAS English数据集来支持这些研究。

Result: 通过BMAS English数据集，能够有效完成机器生成文本的检测、多分类识别、生成器归属分析及句子级分割，同时探讨了对抗攻击对检测效果的影响。

Conclusion: 本文提出的多任务框架和数据集为机器生成文本检测领域提供了更全面有效的解决方案，有助于更好地应对真实场景中的多样化检测需求。

Abstract: Large Language Models (LLMs) are gearing up to surpass human creativity. The
veracity of the statement needs careful consideration. In recent developments,
critical questions arise regarding the authenticity of human work and the
preservation of their creativity and innovative abilities. This paper
investigates such issues. This paper addresses machine-generated text detection
across several scenarios, including document-level binary and multiclass
classification or generator attribution, sentence-level segmentation to
differentiate between human-AI collaborative text, and adversarial attacks
aimed at reducing the detectability of machine-generated text. We introduce a
new work called BMAS English: an English language dataset for binary
classification of human and machine text, for multiclass classification, which
not only identifies machine-generated text but can also try to determine its
generator, and Adversarial attack addressing where it is a common act for the
mitigation of detection, and Sentence-level segmentation, for predicting the
boundaries between human and machine-generated text. We believe that this paper
will address previous work in Machine-Generated Text Detection (MGTD) in a more
meaningful way.

</details>


### [70] [Context Parametrization with Compositional Adapters](https://arxiv.org/abs/2509.22158)
*Josip Jukić,Martin Tutek,Jan Šnajder*

Main category: cs.CL

TL;DR: 本文提出了一种名为CompAs的元学习框架，通过将上下文信息组合成适配器参数，解决了大语言模型在处理多信息块时效率低和灵活性差的问题，实现了更有效的推理和安全性保障。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的上下文学习(ICL)在处理大量示例时效率低下，监督微调(SFT)则存在训练开销大且灵活性不足的问题。此前基于单一上下文生成适配器的方法忽视了多信息块的融合需求。

Method: 提出CompAs，通过元学习将多块上下文信息转化为具有组合结构的适配器参数，支持适配器在代数上合并，实现长上下文信息的高效处理和编码信息的可逆恢复。

Result: 在多项选择和抽取式问答任务中，CompAs在扩展输入规模时优于ICL和现有生成器方法，展现出更低的推理成本和长上下文鲁棒性。

Conclusion: CompAs提供了一个组合式适配器生成的实用高效方案，为大语言模型的扩展部署提供了新的途径，兼顾性能和安全性。

Abstract: Large language models (LLMs) often seamlessly adapt to new tasks through
in-context learning (ICL) or supervised fine-tuning (SFT). However, both of
these approaches face key limitations: ICL is inefficient when handling many
demonstrations, and SFT incurs training overhead while sacrificing flexibility.
Mapping instructions or demonstrations from context directly into adapter
parameters offers an appealing alternative. While prior work explored
generating adapters based on a single input context, it has overlooked the need
to integrate multiple chunks of information. To address this gap, we introduce
CompAs, a meta-learning framework that translates context into adapter
parameters with a compositional structure. Adapters generated this way can be
merged algebraically, enabling instructions, demonstrations, or retrieved
passages to be seamlessly combined without reprocessing long prompts.
Critically, this approach yields three benefits: lower inference cost,
robustness to long-context instability, and establishes a principled solution
when input exceeds the model's context window. Furthermore, CompAs encodes
information into adapter parameters in a reversible manner, enabling recovery
of input context through a decoder, facilitating safety and security. Empirical
results on diverse multiple-choice and extractive question answering tasks show
that CompAs outperforms ICL and prior generator-based methods, especially when
scaling to more inputs. Our work establishes composable adapter generation as a
practical and efficient alternative for scaling LLM deployment.

</details>


### [71] [When Does Reasoning Matter? A Controlled Study of Reasoning's Contribution to Model Performance](https://arxiv.org/abs/2509.22193)
*Nicolas Boizard,Hippolyte Gisserot-Boukhlef,Kevin El-Haddad,Céline Hudelot,Pierre Colombo*

Main category: cs.CL

TL;DR: 本文探讨了具有推理能力的大型语言模型在不同任务和模型规模上的表现及其成本效益，发现推理模型在规模增大时性能优于指令微调模型。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在多种任务中推理能力取得了显著效果，但其在不同任务和规模下的表现及训练和推理成本尚未充分研究。

Method: 本文利用合成数据蒸馏框架进行大规模监督学习，比较了不同规模下的指令微调模型和推理模型在数学及通用任务中的表现，涵盖选择题和开放式任务。

Result: 推理能力显著提升模型性能，常常能匹配或超越更大规模的指令微调模型；推理模型在模型规模增加时价值提升，能够突破指令微调在推理密集和开放任务上的性能瓶颈。

Conclusion: 推理模型虽训练和推理成本高于指令微调模型，但随着模型规模增长，其性能优势更加明显，建议在需要高推理能力的任务上优先使用推理模型。

Abstract: Large Language Models (LLMs) with reasoning capabilities have achieved
state-of-the-art performance on a wide range of tasks. Despite its empirical
success, the tasks and model scales at which reasoning becomes effective, as
well as its training and inference costs, remain underexplored. In this work,
we rely on a synthetic data distillation framework to conduct a large-scale
supervised study. We compare Instruction Fine-Tuning (IFT) and reasoning models
of varying sizes, on a wide range of math-centric and general-purpose tasks,
evaluating both multiple-choice and open-ended formats. Our analysis reveals
that reasoning consistently improves model performance, often matching or
surpassing significantly larger IFT systems. Notably, while IFT remains
Pareto-optimal in training and inference costs, reasoning models become
increasingly valuable as model size scales, overcoming IFT performance limits
on reasoning-intensive and open-ended tasks.

</details>


### [72] [The Outputs of Large Language Models are Meaningless](https://arxiv.org/abs/2509.22206)
*Anandi Hattiangadi,Anders J. Schoubye*

Main category: cs.CL

TL;DR: 本文论证了大型语言模型（LLMs）的输出是无意义的，基于它们缺乏产生字面意义所需的意图。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型输出是否具有真正的字面意义及其背后的哲学基础。

Method: 通过提出两个前提（意图必需性与LLMs缺乏意图）并反驳外部主义和内在主义语义论点，构建无意义论证。

Result: 确立了LLMs输出缺乏必要意图，因此在字面上无意义，但通过其他机制依然能传达似是而非的信息。

Conclusion: 尽管LLMs输出本质上无意义，但其仍能被人类解读为有意义，并用于获取真实信念和知识。

Abstract: In this paper, we offer a simple argument for the conclusion that the outputs
of large language models (LLMs) are meaningless. Our argument is based on two
key premises: (a) that certain kinds of intentions are needed in order for
LLMs' outputs to have literal meanings, and (b) that LLMs cannot plausibly have
the right kinds of intentions. We defend this argument from various types of
responses, for example, the semantic externalist argument that deference can be
assumed to take the place of intentions and the semantic internalist argument
that meanings can be defined purely in terms of intrinsic relations between
concepts, such as conceptual roles. We conclude the paper by discussing why,
even if our argument is sound, the outputs of LLMs nevertheless seem meaningful
and can be used to acquire true beliefs and even knowledge.

</details>


### [73] [Question-Driven Analysis and Synthesis: Building Interpretable Thematic Trees with LLMs for Text Clustering and Controllable Generation](https://arxiv.org/abs/2509.22211)
*Tiago Fernandes Tavares*

Main category: cs.CL

TL;DR: 本文提出了一种名为递归主题划分（RTP）的新方法，利用大型语言模型构建可解释的二叉主题树，提升无监督文本分析的可解释性和实用性。


<details>
  <summary>Details</summary>
Motivation: 传统主题模型在数据稀缺领域表现有限，且主题往往以关键词列表呈现，缺乏语义连贯性和可解释性。

Method: 引入RTP框架，通过大型语言模型互动地构建自然语言问题驱动的二叉树，每个节点通过语义问题划分数据，实现全面可解释的主题分类。

Result: 实验表明，RTP生成的主题层级在可解释性上优于基于关键词的BERTopic，同时所建主题作为下游分类的特征效果显著。

Conclusion: RTP改变了数据探索的范式，从统计模式发现转向知识驱动的主题分析，并可将主题路径用作生成模型的可控提示，实现语料库特征的模拟和综合。

Abstract: Unsupervised analysis of text corpora is challenging, especially in
data-scarce domains where traditional topic models struggle. While these models
offer a solution, they typically describe clusters with lists of keywords that
require significant manual effort to interpret and often lack semantic
coherence. To address this critical interpretability gap, we introduce
Recursive Thematic Partitioning (RTP), a novel framework that leverages Large
Language Models (LLMs) to interactively build a binary tree. Each node in the
tree is a natural language question that semantically partitions the data,
resulting in a fully interpretable taxonomy where the logic of each cluster is
explicit. Our experiments demonstrate that RTP's question-driven hierarchy is
more interpretable than the keyword-based topics from a strong baseline like
BERTopic. Furthermore, we establish the quantitative utility of these clusters
by showing they serve as powerful features in downstream classification tasks,
particularly when the data's underlying themes correlate with the task labels.
RTP introduces a new paradigm for data exploration, shifting the focus from
statistical pattern discovery to knowledge-driven thematic analysis.
Furthermore, we demonstrate that the thematic paths from the RTP tree can serve
as structured, controllable prompts for generative models. This transforms our
analytical framework into a powerful tool for synthesis, enabling the
consistent imitation of specific characteristics discovered in the source
corpus.

</details>


### [74] [StableToken: A Noise-Robust Semantic Speech Tokenizer for Resilient SpeechLLMs](https://arxiv.org/abs/2509.22220)
*Yuhan Song,Linhao Zhang,Chuhan Wu,Aiwei Liu,Wei Jia,Houfeng Wang,Xiao Zhou*

Main category: cs.CL

TL;DR: 本文提出了StableToken，一种通过多分支并行处理和比特位投票机制实现稳定性的语音标记器，显著提升了抗噪声条件下的标记稳定性，进而增强了语音大模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有语义语音标记器对语音中的非语义扰动非常敏感，导致标记序列不稳定，增加后续大语言模型的学习负担。

Method: 引入StableToken，采用多分支架构并行处理音频，通过比特位投票机制合并多路表示，形成稳定的标记序列。

Result: StableToken在多种噪声条件下显著降低了单位编辑距离，达到了标记稳定性的最新水平。

Conclusion: 通过提高语音标记的稳定性，StableToken有效提升了下游语音大模型在噪声环境中的鲁棒性和性能。

Abstract: Prevalent semantic speech tokenizers, designed to capture linguistic content,
are surprisingly fragile. We find they are not robust to meaning-irrelevant
acoustic perturbations; even at high Signal-to-Noise Ratios (SNRs) where speech
is perfectly intelligible, their output token sequences can change drastically,
increasing the learning burden for downstream LLMs. This instability stems from
two flaws: a brittle single-path quantization architecture and a distant
training signal indifferent to intermediate token stability. To address this,
we introduce StableToken, a tokenizer that achieves stability through a
consensus-driven mechanism. Its multi-branch architecture processes audio in
parallel, and these representations are merged via a powerful bit-wise voting
mechanism to form a single, stable token sequence. StableToken sets a new
state-of-the-art in token stability, drastically reducing Unit Edit Distance
(UED) under diverse noise conditions. This foundational stability translates
directly to downstream benefits, significantly improving the robustness of
SpeechLLMs on a variety of tasks.

</details>


### [75] [Thinking in Many Modes: How Composite Reasoning Elevates Large Language Model Performance with Limited Data](https://arxiv.org/abs/2509.22224)
*Zishan Ahmad,Saisubramaniam Gopalakrishnan*

Main category: cs.CL

TL;DR: 本文提出了一种新的复合推理（Composite Reasoning, CR）方法，使大型语言模型能够动态结合不同的推理风格，从而提升复杂问题解决能力。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型主要依赖单一推理范式，难以应对需要多样认知策略的复杂问题。

Method: 提出复合推理方法，结合演绎、归纳和溯因等多种推理风格，适应不同领域需求。

Result: 在科学和医学问答基准测试中，CR方法优于链式思维（CoT）和DeepSeek-R1推理，且样本效率更高，令牌使用更合理。

Conclusion: 通过内部推理风格多样化，模型获得更稳健、自适应、高效的解决问题能力。

Abstract: Large Language Models (LLMs), despite their remarkable capabilities, rely on
singular, pre-dominant reasoning paradigms, hindering their performance on
intricate problems that demand diverse cognitive strategies. To address this,
we introduce Composite Reasoning (CR), a novel reasoning approach empowering
LLMs to dynamically explore and combine multiple reasoning styles like
deductive, inductive, and abductive for more nuanced problem-solving. Evaluated
on scientific and medical question-answering benchmarks, our approach
outperforms existing baselines like Chain-of-Thought (CoT) and also surpasses
the accuracy of DeepSeek-R1 style reasoning (SR) capabilities, while
demonstrating superior sample efficiency and adequate token usage. Notably, CR
adaptively emphasizes domain-appropriate reasoning styles. It prioritizes
abductive and deductive reasoning for medical question answering, but shifts to
causal, deductive, and inductive methods for scientific reasoning. Our findings
highlight that by cultivating internal reasoning style diversity, LLMs acquire
more robust, adaptive, and efficient problem-solving abilities.

</details>


### [76] [In Their Own Words: Reasoning Traces Tailored for Small Models Make Them Better Reasoners](https://arxiv.org/abs/2509.22230)
*Jaehoon Kim,Kwangwook Seo,Dongha Lee*

Main category: cs.CL

TL;DR: 较大语言模型向较小模型转移推理能力时，由于分布不匹配，直接微调反而导致性能下降。本文提出反向推测解码（RSD）方法，通过学生模型筛选低概率词来生成更适合学生模型的推理轨迹，有效提升性能。


<details>
  <summary>Details</summary>
Motivation: 直接通过教师模型生成的高质量推理示范对较小模型进行监督微调，反而导致性能下降，原因在于推理轨迹中的低概率词超出小模型的表示能力，造成学习障碍。

Method: 提出反向推测解码（RSD）方法，由教师模型提出候选词，学生模型根据自身概率分布决定接受与否，过滤掉低概率词，生成适合学生模型的推理轨迹。

Result: 应用于Qwen3-0.6B模型，直接蒸馏s1K-1.1推理轨迹导致性能下降20.5%，而使用RSD生成的推理轨迹训练使性能提升了4.9%。

Conclusion: 低概率词是推理能力传递的关键瓶颈，且RSD生成的推理轨迹因模型特异性而需针对不同学生模型定制分布对齐策略。

Abstract: Transferring reasoning capabilities from larger language models to smaller
ones through supervised fine-tuning often fails counterintuitively, with
performance degrading despite access to high-quality teacher demonstrations. We
identify that this failure stems from distributional misalignment: reasoning
traces from larger models contain tokens that are low probability under the
student's distribution, exceeding the internal representation capacity of
smaller architectures and creating learning barriers rather than helpful
guidance. We propose Reverse Speculative Decoding (RSD), a mechanism for
generating student-friendly reasoning traces in which the teacher model
proposes candidate tokens but the student model determines acceptance based on
its own probability distributions, filtering low probability tokens. When
applied to Qwen3-0.6B, direct distillation of s1K-1.1 reasoning trace data
degrades average performance across major reasoning benchmarks by 20.5\%, while
the same model trained on RSD-generated reasoning traces achieves meaningful
improvements of 4.9\%. Our analysis reveals that low probability tokens
constitute the critical bottleneck in reasoning ability transfer. However,
cross-model experiments demonstrate that RSD traces are model-specific rather
than universally applicable, indicating that distributional alignment must be
tailored for each student architecture's unique internal representation.

</details>


### [77] [FLEXI: Benchmarking Full-duplex Human-LLM Speech Interaction](https://arxiv.org/abs/2509.22243)
*Yuan Ge,Saihan Chen,Jingqi Xiao,Xiaoqian Liu,Tong Xiao,Yan Xiang,Zhengtao Yu,Jingbo Zhu*

Main category: cs.CL

TL;DR: 本文提出了FLEXI，这是第一个面向全双工大型语言模型与人类语音交互的基准测试，特别关注紧急情况下的模型打断能力。


<details>
  <summary>Details</summary>
Motivation: 当前全双工语音对话系统缺乏有效的基准测试，尤其是在紧急场景下如何打断模型仍是挑战。

Method: 设计并实现了FLEXI基准，涵盖六种多样的人机语音交互场景，系统评估了模型的延迟、质量和对话效果，重点分析了开源与商业模型在紧急感知与交互性能上的差异。

Result: 实验结果显示，开源与商业模型在紧急意识、对话终止和交互延迟方面存在显著差距。

Conclusion: 提出采用下一个词对预测的方法，作为实现无缝、人类般全双工交互的有效途径。

Abstract: Full-Duplex Speech-to-Speech Large Language Models (LLMs) are foundational to
natural human-computer interaction, enabling real-time spoken dialogue systems.
However, benchmarking and modeling these models remains a fundamental
challenge. We introduce FLEXI, the first benchmark for full-duplex LLM-human
spoken interaction that explicitly incorporates model interruption in emergency
scenarios. FLEXI systematically evaluates the latency, quality, and
conversational effectiveness of real-time dialogue through six diverse
human-LLM interaction scenarios, revealing significant gaps between open source
and commercial models in emergency awareness, turn terminating, and interaction
latency. Finally, we suggest that next token-pair prediction offers a promising
path toward achieving truly seamless and human-like full-duplex interaction.

</details>


### [78] [Safety Compliance: Rethinking LLM Safety Reasoning through the Lens of Compliance](https://arxiv.org/abs/2509.22250)
*Wenbin Hu,Huihao Jing,Haochen Shi,Haoran Li,Yangqiu Song*

Main category: cs.CL

TL;DR: 本文提出了一种基于法律合规的LLM安全保障方法，通过设立以欧盟AI法案和GDPR为核心的安全标准，开发符合安全合规的新基准，并利用群策略优化训练合规推理器，提高LLM的安全性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全方法缺乏系统性和规范性，无法有效保障复杂行为的安全性，因此从法律合规角度定义LLM安全标准，提升其安全保障能力。

Method: 建立基于法律法规的安全合规基准，使用法律条文生成现实安全场景；采用群策略优化（GRPO）对Qwen3-8B模型进行对齐，构建合规推理器以符合法律安全标准。

Result: 合规推理器在新安全合规基准上表现优越，EU AI法案提升约10.45%，GDPR提升约11.85%，显著降低安全风险。

Conclusion: 通过结合法律框架与优化技术，本文提出的合规推理器有效提升了LLM的安全合规性，为LLM安全提供切实可行的系统保障方法。

Abstract: The proliferation of Large Language Models (LLMs) has demonstrated remarkable
capabilities, elevating the critical importance of LLM safety. However,
existing safety methods rely on ad-hoc taxonomy and lack a rigorous, systematic
protection, failing to ensure safety for the nuanced and complex behaviors of
modern LLM systems. To address this problem, we solve LLM safety from legal
compliance perspectives, named safety compliance. In this work, we posit
relevant established legal frameworks as safety standards for defining and
measuring safety compliance, including the EU AI Act and GDPR, which serve as
core legal frameworks for AI safety and data security in Europe. To bridge the
gap between LLM safety and legal compliance, we first develop a new benchmark
for safety compliance by generating realistic LLM safety scenarios seeded with
legal statutes. Subsequently, we align Qwen3-8B using Group Policy Optimization
(GRPO) to construct a safety reasoner, Compliance Reasoner, which effectively
aligns LLMs with legal standards to mitigate safety risks. Our comprehensive
experiments demonstrate that the Compliance Reasoner achieves superior
performance on the new benchmark, with average improvements of +10.45% for the
EU AI Act and +11.85% for GDPR.

</details>


### [79] [Beyond Textual Context: Structural Graph Encoding with Adaptive Space Alignment to alleviate the hallucination of LLMs](https://arxiv.org/abs/2509.22251)
*Yifang Zhang,Pengfei Duan,Yiwen Yang,Shengwu Xiong*

Main category: cs.CL

TL;DR: SSKG-LLM模型创新地结合了知识图谱的结构和语义信息，提升大语言模型的事实推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅将知识图谱作为文本处理，忽略其结构信息，且知识图谱编码与语言模型嵌入空间存在差距，限制了结构化知识的整合效果。

Method: 提出SSKG-LLM架构，包含知识图谱检索（KGR）、编码（KGE）模块来保留语义及结构信息，并引入知识图谱适配（KGA）模块以桥接知识图谱嵌入和语言模型的理解。

Result: 通过大规模实验验证，表明融合结构信息的知识图谱显著提升了大语言模型的事实推理能力。

Conclusion: SSKG-LLM成功解决了结构信息利用不足和嵌入空间差异问题，提升了LLM整合知识图谱进行推理的效果。

Abstract: Currently, the main approach for Large Language Models (LLMs) to tackle the
hallucination issue is incorporating Knowledge Graphs(KGs).However, LLMs
typically treat KGs as plain text, extracting only semantic information and
limiting their use of the crucial structural aspects of KGs. Another challenge
is the gap between the embedding spaces of KGs encoders and LLMs text
embeddings, which hinders the effective integration of structured knowledge. To
overcome these obstacles, we put forward the SSKG-LLM, an innovative model
architecture that is designed to efficiently integrate both the Structural and
Semantic information of KGs into the reasoning processes of LLMs. SSKG-LLM
incorporates the Knowledge Graph Retrieval (KGR) module and the Knowledge Graph
Encoding (KGE) module to preserve semantics while utilizing structure. Then,
the Knowledge Graph Adaptation (KGA) module is incorporated to enable LLMs to
understand KGs embeddings. We conduct extensive experiments and provide a
detailed analysis to explore how incorporating the structural information of
KGs can enhance the factual reasoning abilities of LLMs. Our code are available
at https://github.com/yfangZhang/SSKG-LLM.

</details>


### [80] [Bridging Fairness and Explainability: Can Input-Based Explanations Promote Fairness in Hate Speech Detection?](https://arxiv.org/abs/2509.22291)
*Yifan Wang,Mayank Jobanputra,Ji-Ung Lee,Soyoung Oh,Isabel Valera,Vera Demberg*

Main category: cs.CL

TL;DR: 本文系统研究了解释性与公平性在仇恨言论检测中的关系，发现基于输入的解释能有效检测偏见和辅助减偏，但不适合模型选择。


<details>
  <summary>Details</summary>
Motivation: 自然语言处理模型存在社会偏见，且模型黑箱性使偏见难以识别与缓解，现有解释性与公平性研究多为定性，缺乏大规模定量分析。

Method: 本文对编码器和解码器模型进行系统研究，重点考察三方面：识别偏见预测、选择公平模型及训练时减偏，评价输入解释的作用。

Result: 输入解释能准确检测偏见预测，且可作为监督信号帮助训练减偏；但输入解释不适用于公平模型的筛选。

Conclusion: 输入解释在偏见检测和减轻方面有效，但在公平模型选择上有限，提示解释性方法需结合其他策略提升公平性。

Abstract: Natural language processing (NLP) models often replicate or amplify social
bias from training data, raising concerns about fairness. At the same time,
their black-box nature makes it difficult for users to recognize biased
predictions and for developers to effectively mitigate them. While some studies
suggest that input-based explanations can help detect and mitigate bias, others
question their reliability in ensuring fairness. Existing research on
explainability in fair NLP has been predominantly qualitative, with limited
large-scale quantitative analysis. In this work, we conduct the first
systematic study of the relationship between explainability and fairness in
hate speech detection, focusing on both encoder- and decoder-only models. We
examine three key dimensions: (1) identifying biased predictions, (2) selecting
fair models, and (3) mitigating bias during model training. Our findings show
that input-based explanations can effectively detect biased predictions and
serve as useful supervision for reducing bias during training, but they are
unreliable for selecting fair models among candidates.

</details>


### [81] [Advancing Natural Language Formalization to First Order Logic with Fine-tuned LLMs](https://arxiv.org/abs/2509.22338)
*Felix Vossel,Till Mossakowski,Björn Gehrke*

Main category: cs.CL

TL;DR: 本文系统评估了微调大语言模型（LLMs）在自然语言转一阶逻辑（FOL）任务中的表现，提出多种技术改进，取得了显著精度提升。


<details>
  <summary>Details</summary>
Motivation: 自然语言转一阶逻辑是知识表示和形式方法中的关键而具有挑战性的任务，亟需有效的自动化解决方案。

Method: 对比编码器-解码器与仅解码器架构，使用MALLS和Willow数据集，探索词汇扩展、谓词条件化和多语种训练等技术，设计多样化评估指标。

Result: 微调后的Flan-T5-XXL模型在有谓词列表条件下准确率达到70%，优于GPT-4o、具链式推理能力的DeepSeek-R1-0528及符号系统ccg2lambda；发现谓词可用性提升15-20%的性能，T5模型超过更大仅解码器模型，且模型能推广至未经训练的新逻辑论据数据。

Conclusion: 结构化逻辑翻译方法表现稳健，但谓词提取仍是当前主要瓶颈，未来需加强谓词识别技术以进一步提升系统性能。

Abstract: Automating the translation of natural language to first-order logic (FOL) is
crucial for knowledge representation and formal methods, yet remains
challenging. We present a systematic evaluation of fine-tuned LLMs for this
task, comparing architectures (encoder-decoder vs. decoder-only) and training
strategies. Using the MALLS and Willow datasets, we explore techniques like
vocabulary extension, predicate conditioning, and multilingual training,
introducing metrics for exact match, logical equivalence, and predicate
alignment. Our fine-tuned Flan-T5-XXL achieves 70% accuracy with predicate
lists, outperforming GPT-4o and even the DeepSeek-R1-0528 model with CoT
reasoning ability as well as symbolic systems like ccg2lambda. Key findings
show: (1) predicate availability boosts performance by 15-20%, (2) T5 models
surpass larger decoder-only LLMs, and (3) models generalize to unseen logical
arguments (FOLIO dataset) without specific training. While structural logic
translation proves robust, predicate extraction emerges as the main bottleneck.

</details>


### [82] [Transformers Can Learn Connectivity in Some Graphs but Not Others](https://arxiv.org/abs/2509.22343)
*Amit Roy,Abulhair Saparov*

Main category: cs.CL

TL;DR: 本文研究了变压器模型在推断传递关系任务中的能力，特别是在训练过程中的表现和模型规模对能力的影响。


<details>
  <summary>Details</summary>
Motivation: 确保大语言模型回答的事实正确性需要具备推理能力，尤其是在传递关系推理方面至关重要，如因果推断领域。因此，研究变压器模型推断传递关系能力及其随规模变化的表现非常必要。

Method: 通过生成不同大小的有向图训练不同规模的变压器模型，评估其推断传递关系（等同于有向图中连通性）能力。重点考察了网格状有向图及高维度网格对模型学习的影响，以及非网格结构图的挑战。

Result: 变压器能够成功学习低维网格状有向图的连通性，高维网格图难度较大。模型规模越大，泛化能力越强。对于非网格且含大量不连通分量的图，模型表现较差。

Conclusion: 变压器的传递关系推断能力依赖图的结构和维度，规模扩展可以提升学习能力，但在复杂非网格结构上仍存在显著挑战。

Abstract: Reasoning capability is essential to ensure the factual correctness of the
responses of transformer-based Large Language Models (LLMs), and robust
reasoning about transitive relations is instrumental in many settings, such as
causal inference. Hence, it is essential to investigate the capability of
transformers in the task of inferring transitive relations (e.g., knowing A
causes B and B causes C, then A causes C). The task of inferring transitive
relations is equivalent to the task of connectivity in directed graphs (e.g.,
knowing there is a path from A to B, and there is a path from B to C, then
there is a path from A to C). Past research focused on whether transformers can
learn to infer transitivity from in-context examples provided in the input
prompt. However, transformers' capability to infer transitive relations from
training examples and how scaling affects the ability is unexplored. In this
study, we seek to answer this question by generating directed graphs to train
transformer models of varying sizes and evaluate their ability to infer
transitive relations for various graph sizes. Our findings suggest that
transformers are capable of learning connectivity on "grid-like'' directed
graphs where each node can be embedded in a low-dimensional subspace, and
connectivity is easily inferable from the embeddings of the nodes. We find that
the dimensionality of the underlying grid graph is a strong predictor of
transformers' ability to learn the connectivity task, where higher-dimensional
grid graphs pose a greater challenge than low-dimensional grid graphs. In
addition, we observe that increasing the model scale leads to increasingly
better generalization to infer connectivity over grid graphs. However, if the
graph is not a grid graph and contains many disconnected components,
transformers struggle to learn the connectivity task, especially when the
number of components is large.

</details>


### [83] [The InviTE Corpus: Annotating Invectives in Tudor English Texts for Computational Modeling](https://arxiv.org/abs/2509.22345)
*Sophie Spliethoff,Sanne Hoeken,Silke Schwandt,Sina Zarrieß,Özge Alaçam*

Main category: cs.CL

TL;DR: 本文利用自然语言处理技术，构建了包含16世纪英格兰宗教辱骂语言的InviTE语料库，并比较了不同语言模型在侮辱检测任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 将自然语言处理技术应用于历史研究，特别是在都铎时代研究宗教辱骂语言，为历史语料的分析提供新方法。

Method: 从原始数据出发，经过预处理和数据选择，进行迭代注释，构建了近2000条早期现代英语句子的InviTE语料库，随后评估了微调的基于BERT模型和零样本提示指令微调的大型语言模型在辱骂检测上的表现。

Result: 预先在历史数据上训练并微调的模型在宗教辱骂检测任务中表现优于零样本提示微调的大型语言模型。

Conclusion: 针对历史语料进行预训练和微调的语言模型在特定历史语言现象的检测上更具优势，能为历史语言研究提供有力工具。

Abstract: In this paper, we aim at the application of Natural Language Processing (NLP)
techniques to historical research endeavors, particularly addressing the study
of religious invectives in the context of the Protestant Reformation in Tudor
England. We outline a workflow spanning from raw data, through pre-processing
and data selection, to an iterative annotation process. As a result, we
introduce the InviTE corpus -- a corpus of almost 2000 Early Modern English
(EModE) sentences, which are enriched with expert annotations regarding
invective language throughout 16th-century England. Subsequently, we assess and
compare the performance of fine-tuned BERT-based models and zero-shot prompted
instruction-tuned large language models (LLMs), which highlights the
superiority of models pre-trained on historical data and fine-tuned to
invective detection.

</details>


### [84] [Conversational Implicatures: Modelling Relevance Theory Probabilistically](https://arxiv.org/abs/2509.22354)
*Christoph Unger,Hendrik Buschmeier*

Main category: cs.CL

TL;DR: 本文探讨了贝叶斯概率理论如何应用于语用学，特别是通过理性语言行为者理论模拟格赖斯式的语用现象，并尝试将类似方法应用于相关性理论，重点研究隐含意义的交流。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯理论与计算工具的发展推动语用学和语义学中的概率转向，理性语言行为者理论成功建模了多种语用现象，激发了将此类贝叶斯方法应用于相关性理论的兴趣。

Method: 通过研究典型的语用现象——通过会话含意传递隐含意义，分析如何用贝叶斯方法解释相关性理论中的语用过程。

Result: 展示了贝叶斯框架下对相关性理论中隐含意义传递的潜在建模方式，扩展了理性语言行为者理论的应用范围。

Conclusion: 贝叶斯概率方法为相关性理论语用学提供了一种新的定量分析视角，有助于深入理解隐含意义的交流机制。

Abstract: Recent advances in Bayesian probability theory and its application to
cognitive science in combination with the development of a new generation of
computational tools and methods for probabilistic computation have led to a
'probabilistic turn' in pragmatics and semantics. In particular, the framework
of Rational Speech Act theory has been developed to model broadly Gricean
accounts of pragmatic phenomena in Bayesian terms, starting with fairly simple
reference games and covering ever more complex communicative exchanges such as
verbal syllogistic reasoning. This paper explores in which way a similar
Bayesian approach might be applied to relevance-theoretic pragmatics (Sperber &
Wilson, 1995) by study a paradigmatic pragmatic phenomenon: the communication
of implicit meaning by ways of (conversational) implicatures.

</details>


### [85] [CHRONOBERG: Capturing Language Evolution and Temporal Awareness in Foundation Models](https://arxiv.org/abs/2509.22360)
*Niharika Hegde,Subarnaduti Paul,Lars Joel-Frey,Manuel Brack,Kristian Kersting,Martin Mundt,Patrick Schramowski*

Main category: cs.CL

TL;DR: 本文介绍了一个包含250年英文书籍文本的时间结构语料库CHRONOBERG，用于研究语言的历史变化和情感词典构建。


<details>
  <summary>Details</summary>
Motivation: 现有语料库缺乏长期时间结构，限制了大语言模型对语言语义和规范演变的理解能力。

Method: 从Project Gutenberg收集英文书籍，进行时间敏感的情感分析（VAD），构建历史情感词典，并测试大语言模型对历史语言变化的适应能力。

Result: 发现现有大语言模型难以捕捉语言随时间的语义变化，表明需要时间感知的训练和评估方法。

Conclusion: CHRONOBERG提供了一个可扩展的资源，用于语言变化研究和时序泛化，强调时间结构对语言模型训练的重要性。

Abstract: Large language models (LLMs) excel at operating at scale by leveraging social
media and various data crawled from the web. Whereas existing corpora are
diverse, their frequent lack of long-term temporal structure may however limit
an LLM's ability to contextualize semantic and normative evolution of language
and to capture diachronic variation. To support analysis and training for the
latter, we introduce CHRONOBERG, a temporally structured corpus of English book
texts spanning 250 years, curated from Project Gutenberg and enriched with a
variety of temporal annotations. First, the edited nature of books enables us
to quantify lexical semantic change through time-sensitive
Valence-Arousal-Dominance (VAD) analysis and to construct historically
calibrated affective lexicons to support temporally grounded interpretation.
With the lexicons at hand, we demonstrate a need for modern LLM-based tools to
better situate their detection of discriminatory language and contextualization
of sentiment across various time-periods. In fact, we show how language models
trained sequentially on CHRONOBERG struggle to encode diachronic shifts in
meaning, emphasizing the need for temporally aware training and evaluation
pipelines, and positioning CHRONOBERG as a scalable resource for the study of
linguistic change and temporal generalization. Disclaimer: This paper includes
language and display of samples that could be offensive to readers. Open
Access: Chronoberg is available publicly on HuggingFace at (
https://huggingface.co/datasets/spaul25/Chronoberg). Code is available at
(https://github.com/paulsubarna/Chronoberg).

</details>


### [86] [Exploratory Semantic Reliability Analysis of Wind Turbine Maintenance Logs using Large Language Models](https://arxiv.org/abs/2509.22366)
*Max Malyi,Jonathan Shek,Andre Biscaya*

Main category: cs.CL

TL;DR: 本文提出基于大语言模型（LLMs）的框架，从简单文本分类拓展到复杂的语义分析，以提升风力涡轮机维护日志中的运维智能。


<details>
  <summary>Details</summary>
Motivation: 风力涡轮机维护日志中的非结构化文本包含丰富的运维信息，传统可靠性分析难以利用，现有机器学习方法多停留在文本分类层面，未能深入挖掘复杂语义。

Method: 引入一个探索性框架，利用大语言模型对维护日志进行深度语义分析，执行故障模式识别、因果链推断、站点比较分析和数据质量审计等四项分析工作流。

Result: 在大型工业数据集上验证，该方法使大语言模型能够超越简单标签分类，综合文本信息并生成可操作的专家级假设，表现出强大的“可靠性副驾驶”能力。

Conclusion: 本文创新地提出了利用大语言模型作为推理工具的方法学，为风能领域运维智能的提升开辟了新途径，有助于从非结构化数据中挖掘出以前难以获取的洞见。

Abstract: A wealth of operational intelligence is locked within the unstructured
free-text of wind turbine maintenance logs, a resource largely inaccessible to
traditional quantitative reliability analysis. While machine learning has been
applied to this data, existing approaches typically stop at classification,
categorising text into predefined labels. This paper addresses the gap in
leveraging modern large language models (LLMs) for more complex reasoning
tasks. We introduce an exploratory framework that uses LLMs to move beyond
classification and perform deep semantic analysis. We apply this framework to a
large industrial dataset to execute four analytical workflows: failure mode
identification, causal chain inference, comparative site analysis, and data
quality auditing. The results demonstrate that LLMs can function as powerful
"reliability co-pilots," moving beyond labelling to synthesise textual
information and generate actionable, expert-level hypotheses. This work
contributes a novel and reproducible methodology for using LLMs as a reasoning
tool, offering a new pathway to enhance operational intelligence in the wind
energy sector by unlocking insights previously obscured in unstructured data.

</details>


### [87] [What Is The Political Content in LLMs' Pre- and Post-Training Data?](https://arxiv.org/abs/2509.22367)
*Tanise Ceron,Dmitry Nikolaev,Dominik Stammbach,Debora Nozza*

Main category: cs.CL

TL;DR: 本文分析了最大开源大语言模型OLMO2的训练数据中的政治倾向，发现左倾内容占主导且预训练数据政治参与度更高，训练数据的政治倾向与模型偏见高度相关。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型已知存在政治偏见，但训练数据中的政治内容分析较少，作者希望通过分析训练数据澄清偏见形成原因。

Method: 从OLMO2的预训练和后训练语料中随机抽样，自动标注政治倾向，分析数据来源和内容，并评估训练数据政治内容与模型在特定政策问题上的立场相关性。

Result: 左倾文件在各数据集中占优势，预训练语料中政治参与度明显高于后训练语料。不同政治倾向文本对相似话题采用不同价值观和合法性来源表述。训练数据主导政治倾向与模型政策偏见高度相关。

Conclusion: 强调未来数据清洗流程需纳入政治内容分析，并透明记录过滤策略，以减少模型政治偏见并提升模型公平性。

Abstract: Large language models (LLMs) are known to generate politically biased text,
yet how such biases arise remains unclear. A crucial step toward answering this
question is the analysis of training data, whose political content remains
largely underexplored in current LLM research. To address this gap, we present
in this paper an analysis of the pre- and post-training corpora of OLMO2, the
largest fully open-source model released together with its complete dataset.
From these corpora, we draw large random samples, automatically annotate
documents for political orientation, and analyze their source domains and
content. We then assess how political content in the training data correlates
with models' stance on specific policy issues. Our analysis shows that
left-leaning documents predominate across datasets, with pre-training corpora
containing significantly more politically engaged content than post-training
data. We also find that left- and right-leaning documents frame similar topics
through distinct values and sources of legitimacy. Finally, the predominant
stance in the training data strongly correlates with models' political biases
when evaluated on policy issues. These findings underscore the need to
integrate political content analysis into future data curation pipelines as
well as in-depth documentation of filtering strategies for transparency.

</details>


### [88] [Chimera: Diagnosing Shortcut Learning in Visual-Language Understanding](https://arxiv.org/abs/2509.22437)
*Ziheng Chi,Yifan Hou,Chenxi Pang,Shaobo Cui,Mubashara Akhtar,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: 本文提出了Chimera测试套件，评估视觉语言模型在图表理解中的真实能力，发现其表现主要依赖于各种“捷径”而非真正理解。


<details>
  <summary>Details</summary>
Motivation: 目前视觉语言模型在图表相关任务中表现良好，但存在依赖知识、推理或模态捷径的问题，难以判断其是否真正理解图表。

Method: 构建包含7500张带有语义三元组注释和多层级问题的Wikipedia图表测试集Chimera，设计四个核心理解方向的测试，识别三种常见的捷径行为并评估15个开源视觉语言模型。

Result: 发现视觉记忆捷径对模型影响较小，知识回忆捷径影响适中，而利用语言模式的捷径影响显著，说明现有模型主要依赖捷径而非真正理解图表。

Conclusion: 当前视觉语言模型在理解复杂图表视觉输入方面存在显著局限，需要更严谨的评估机制以检测真实理解能力，而非简单依赖答题捷径。

Abstract: Diagrams convey symbolic information in a visual format rather than a linear
stream of words, making them especially challenging for AI models to process.
While recent evaluations suggest that vision-language models (VLMs) perform
well on diagram-related benchmarks, their reliance on knowledge, reasoning, or
modality shortcuts raises concerns about whether they genuinely understand and
reason over diagrams. To address this gap, we introduce Chimera, a
comprehensive test suite comprising 7,500 high-quality diagrams sourced from
Wikipedia; each diagram is annotated with its symbolic content represented by
semantic triples along with multi-level questions designed to assess four
fundamental aspects of diagram comprehension: entity recognition, relation
understanding, knowledge grounding, and visual reasoning. We use Chimera to
measure the presence of three types of shortcuts in visual question answering:
(1) the visual-memorization shortcut, where VLMs rely on memorized visual
patterns; (2) the knowledge-recall shortcut, where models leverage memorized
factual knowledge instead of interpreting the diagram; and (3) the Clever-Hans
shortcut, where models exploit superficial language patterns or priors without
true comprehension. We evaluate 15 open-source VLMs from 7 model families on
Chimera and find that their seemingly strong performance largely stems from
shortcut behaviors: visual-memorization shortcuts have slight impact,
knowledge-recall shortcuts play a moderate role, and Clever-Hans shortcuts
contribute significantly. These findings expose critical limitations in current
VLMs and underscore the need for more robust evaluation protocols that
benchmark genuine comprehension of complex visual inputs (e.g., diagrams)
rather than question-answering shortcuts.

</details>


### [89] [Detecting (Un)answerability in Large Language Models with Linear Directions](https://arxiv.org/abs/2509.22449)
*Maor Juliet Lavi,Tova Milo,Mor Geva*

Main category: cs.CL

TL;DR: 本文提出了一种基于模型激活空间方向检测不可回答性的方法，有效提升了抽取式问答中不可回答问题的识别能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在缺乏足够信息时仍会自信回答，导致幻觉答案，亟需准确检测不可回答问题。

Method: 通过激活空间方向寻找不可回答特征，利用推理时激活增加检测模型的戒断行为，并将隐藏层投影到该方向进行分类。

Result: 在两个开源大模型和四个抽取式问答数据集上，该方法在检测不可回答问题方面优于现有提示和分类器方法，并具有更好泛化能力。

Conclusion: 该方法不仅适用于抽取式问答，还能检测因科学共识缺乏或主观性导致的不可回答问题，且通过因果干预可有效控制模型戒断行为。

Abstract: Large language models (LLMs) often respond confidently to questions even when
they lack the necessary information, leading to hallucinated answers. In this
work, we study the problem of (un)answerability detection, focusing on
extractive question answering (QA) where the model should determine if a
passage contains sufficient information to answer a given question. We propose
a simple approach for identifying a direction in the model's activation space
that captures unanswerability and uses it for classification. This direction is
selected by applying activation additions during inference and measuring their
impact on the model's abstention behavior. We show that projecting hidden
activations onto this direction yields a reliable score for (un)answerability
classification. Experiments on two open-weight LLMs and four extractive QA
benchmarks show that our method effectively detects unanswerable questions and
generalizes better across datasets than existing prompt-based and
classifier-based approaches. Moreover, the obtained directions extend beyond
extractive QA to unanswerability that stems from factors, such as lack of
scientific consensus and subjectivity. Last, causal interventions show that
adding or ablating the directions effectively controls the abstention behavior
of the model.

</details>


### [90] [Evaluating the Limits of Large Language Models in Multilingual Legal Reasoning](https://arxiv.org/abs/2509.22472)
*Antreas Ioannou,Andreas Shiamishis,Nora Hollenstein,Nezihe Merve Gürel*

Main category: cs.CL

TL;DR: 本文评估了大型语言模型（如LLaMA和Gemini）在多语言法律和非法律任务中的表现及其对对抗性扰动的鲁棒性，发现法律任务对LLMs构成显著挑战，准确率普遍较低。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在法律等高风险领域的应用日益增加，深入理解它们在多语言、多司法管辖区和对抗性环境中的表现与局限变得尤为重要。

Method: 本研究使用多语言法律和非法律基准对LLaMA和Gemini进行评估，并通过字符和词级扰动测试其对抗鲁棒性。采用LLM作为法官的评估方法，并开发了一个开源、模块化的评估管线，支持多语言、多任务的基准测试。

Result: 结果显示，法律推理任务准确率通常低于50%，而通用任务如XNLI超过70%。英语表现较稳定但不总是最高，模型表现与语言的句法相似度相关。Gemini表现显著优于LLaMA，平均提升约24个百分点。对抗性敏感性在多语言中仍然存在。

Conclusion: 尽管较新LLMs有所改进，但在多语言法律关键应用中，LLMs的性能和鲁棒性依然存在重大挑战，不足以完全可靠部署。

Abstract: In an era dominated by Large Language Models (LLMs), understanding their
capabilities and limitations, especially in high-stakes fields like law, is
crucial. While LLMs such as Meta's LLaMA, OpenAI's ChatGPT, Google's Gemini,
DeepSeek, and other emerging models are increasingly integrated into legal
workflows, their performance in multilingual, jurisdictionally diverse, and
adversarial contexts remains insufficiently explored. This work evaluates LLaMA
and Gemini on multilingual legal and non-legal benchmarks, and assesses their
adversarial robustness in legal tasks through character and word-level
perturbations. We use an LLM-as-a-Judge approach for human-aligned evaluation.
We moreover present an open-source, modular evaluation pipeline designed to
support multilingual, task-diverse benchmarking of any combination of LLMs and
datasets, with a particular focus on legal tasks, including classification,
summarization, open questions, and general reasoning. Our findings confirm that
legal tasks pose significant challenges for LLMs with accuracies often below
50% on legal reasoning benchmarks such as LEXam, compared to over 70% on
general-purpose tasks like XNLI. In addition, while English generally yields
more stable results, it does not always lead to higher accuracy. Prompt
sensitivity and adversarial vulnerability is also shown to persist across
languages. Finally, a correlation is found between the performance of a
language and its syntactic similarity to English. We also observe that LLaMA is
weaker than Gemini, with the latter showing an average advantage of about 24
percentage points across the same task. Despite improvements in newer LLMs,
challenges remain in deploying them reliably for critical, multilingual legal
applications.

</details>


### [91] [NeLLCom-Lex: A Neural-agent Framework to Study the Interplay between Lexical Systems and Language Use](https://arxiv.org/abs/2509.22479)
*Yuqing Zhang,Ecesu Ürker,Tessa Verhoef,Gemma Boleda,Arianna Bisazza*

Main category: cs.CL

TL;DR: 该论文提出了NeLLCom-Lex神经智能体框架，通过模拟词汇系统中的交流需求，模拟语义变化过程。


<details>
  <summary>Details</summary>
Motivation: 现有的词汇语义变化研究多依赖观察和实验方法，难以揭示因果机制，且实验方法难以适用于长时间的语义变化过程。

Method: 设计NeLLCom-Lex神经智能体框架，基于真实词汇系统（如英语），使用颜色命名任务，结合监督学习和强化学习训练神经智能体，模拟词汇系统演化和语义变化。

Result: 实验结果显示，训练出的神经智能体能在人类颜色命名行为和词汇模式上表现出高度相似性，并能根据交流需求调整其行为和词汇。

Conclusion: NeLLCom-Lex框架有效模拟了词汇语义变化过程，能够帮助进一步揭示语义变化的机制。

Abstract: Lexical semantic change has primarily been investigated with observational
and experimental methods; however, observational methods (corpus analysis,
distributional semantic modeling) cannot get at causal mechanisms, and
experimental paradigms with humans are hard to apply to semantic change due to
the extended diachronic processes involved. This work introduces NeLLCom-Lex, a
neural-agent framework designed to simulate semantic change by first grounding
agents in a real lexical system (e.g. English) and then systematically
manipulating their communicative needs. Using a well-established color naming
task, we simulate the evolution of a lexical system within a single generation,
and study which factors lead agents to: (i) develop human-like naming behavior
and lexicons, and (ii) change their behavior and lexicons according to their
communicative needs. Our experiments with different supervised and
reinforcement learning pipelines show that neural agents trained to 'speak' an
existing language can reproduce human-like patterns in color naming to a
remarkable extent, supporting the further use of NeLLCom-Lex to elucidate the
mechanisms of semantic change.

</details>


### [92] [Exploring Solution Divergence and Its Effect on Large Language Model Problem Solving](https://arxiv.org/abs/2509.22480)
*Hang Li,Kaiqi Yang,Yucheng Chu,Hui Liu,Jiliang Tang*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）在单一问题上生成解答的差异性，发现差异性越高，模型的解决问题能力越强。基于此，将解答差异性作为评价指标用于监督微调和强化学习策略，实验验证了其能显著提升模型成功率。


<details>
  <summary>Details</summary>
Motivation: 现有研究多通过监督微调或强化学习提升大型语言模型的表现，本文从新角度出发，探讨解答的差异性与模型能力的关系。

Method: 通过分析多模型在同一问题上的解答差异，提出使用解答差异性作为新的衡量指标，进而在监督微调和强化学习中应用该指标进行训练。

Result: 在三个代表性问题领域中，加入解答差异性指标后，模型的成功率均有显著提升。

Conclusion: 解答差异性是一个简单且有效的指标，能够促进大型语言模型的训练和评估，提升其解决问题的能力。

Abstract: Large language models (LLMs) have been widely used for problem-solving tasks.
Most recent work improves their performance through supervised fine-tuning
(SFT) with labeled data or reinforcement learning (RL) from task feedback. In
this paper, we study a new perspective: the divergence in solutions generated
by LLMs for a single problem. We show that higher solution divergence is
positively related to better problem-solving abilities across various models.
Based on this finding, we propose solution divergence as a novel metric that
can support both SFT and RL strategies. We test this idea on three
representative problem domains and find that using solution divergence
consistently improves success rates. These results suggest that solution
divergence is a simple but effective tool for advancing LLM training and
evaluation.

</details>


### [93] [JGU Mainz's Submission to the WMT25 Shared Task on LLMs with Limited Resources for Slavic Languages: MT and QA](https://arxiv.org/abs/2509.22490)
*Hossain Shaikh Saadi,Minh Duc Bui,Mario Sanz-Guerrero,Katharina von der Wense*

Main category: cs.CL

TL;DR: 该论文介绍了JGU Mainz团队对WMT25共享任务中斯拉夫语言（乌克兰语、上索布语和下索布语）有限资源的大型语言模型的机器翻译和问答系统的联合微调方法。


<details>
  <summary>Details</summary>
Motivation: 斯拉夫语言资源有限，需开发高效的多任务模型提升机器翻译和问答表现。

Method: 基于Qwen2.5-3B-Instruct模型使用参数高效微调技术，结合额外的翻译和多项选择问答数据，对三个语言的任务进行联合微调；乌克兰语问答采用检索增强生成，上下索布语问答采用集成方法。

Result: 实验显示所提出模型在机器翻译和问答任务上均优于基线模型。

Conclusion: 联合微调与辅助技术的结合有效提升了斯拉夫语言下有限资源环境中的机器翻译和问答性能。

Abstract: This paper presents the JGU Mainz submission to the WMT25 Shared Task on LLMs
with Limited Resources for Slavic Languages: Machine Translation and Question
Answering, focusing on Ukrainian, Upper Sorbian, and Lower Sorbian. For each
language, we jointly fine-tune a Qwen2.5-3B-Instruct model for both tasks with
parameter-efficient finetuning. Our pipeline integrates additional translation
and multiple-choice question answering (QA) data. For Ukrainian QA, we further
use retrieval-augmented generation. We also apply ensembling for QA in Upper
and Lower Sorbian. Experiments show that our models outperform the baseline on
both tasks.

</details>


### [94] [Representing LLMs in Prompt Semantic Task Space](https://arxiv.org/abs/2509.22506)
*Idan Kashani,Avi Mendelson,Yaniv Nemcovsky*

Main category: cs.CL

TL;DR: 本文提出一种无训练、高效的方法，将大型语言模型表示为提示语义任务空间中的线性算子，实现了模型表现的可解释性和实时扩展性，在任务成功预测和模型选择上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在选择最佳大型语言模型时存在扩展性差、需高成本重训练及表示空间难以解释的问题。

Method: 提出将大型语言模型表示为提示的语义任务空间中的线性算子，通过闭式几何计算实现高效、无训练的模型表示。

Result: 方法在任务成功预测和模型选择中取得了具有竞争力的或最新的结果，且在样本外场景下性能优秀。

Conclusion: 该方法提供了一种高效、可解释且具扩展性的模型表示方式，有助于动态扩展模型库并优化模型选择。

Abstract: Large language models (LLMs) achieve impressive results over various tasks,
and ever-expanding public repositories contain an abundance of pre-trained
models. Therefore, identifying the best-performing LLM for a given task is a
significant challenge. Previous works have suggested learning LLM
representations to address this. However, these approaches present limited
scalability and require costly retraining to encompass additional models and
datasets. Moreover, the produced representation utilizes distinct spaces that
cannot be easily interpreted. This work presents an efficient, training-free
approach to representing LLMs as linear operators within the prompts' semantic
task space, thus providing a highly interpretable representation of the models'
application. Our method utilizes closed-form computation of geometrical
properties and ensures exceptional scalability and real-time adaptability to
dynamically expanding repositories. We demonstrate our approach on success
prediction and model selection tasks, achieving competitive or state-of-the-art
results with notable performance in out-of-sample scenarios.

</details>


### [95] [We Think, Therefore We Align LLMs to Helpful, Harmless and Honest Before They Go Wrong](https://arxiv.org/abs/2509.22510)
*Gautam Siddharth Kashyap,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: 本文提出了一种名为AMBS的自适应多分支引导框架，有效提升了大型语言模型在有用性、无害性和诚实性三重目标上的一致性和表现，解决了多目标对齐中的灾难性遗忘和推理碎片化问题。


<details>
  <summary>Details</summary>
Motivation: 现有的单目标引导方法容易导致灾难性遗忘，而多分支独立优化方法则可能引发多目标推理输出不一致，亟需一种统一高效的多目标对齐方案。

Method: AMBS采用两阶段的1对N架构，第一阶段生成共享表示，第二阶段通过策略参考机制将共享表示复制到多个分支，实现目标特异性的控制并保持跨目标一致性。

Result: 在Alpaca、BeaverTails和TruthfulQA数据集以及多个7B参数级别的模型中，AMBS显著提升了HHH对齐表现，如DeepSeek-7B模型中，平均对齐分数提升32.4%，不安全输出降低11.0%，性能优于基础多分支方法且竞争于最先进技术。

Conclusion: AMBS有效解决了多目标对齐中的灾难性遗忘和推理碎片化问题，为大型语言模型的安全可靠部署提供了重要方法支持。

Abstract: Alignment of Large Language Models (LLMs) along multiple
objectives-helpfulness, harmlessness, and honesty (HHH)-is critical for safe
and reliable deployment. Prior work has used steering vector-small control
signals injected into hidden states-to guide LLM outputs, typically via
one-to-one (1-to-1) Transformer decoders. In this setting, optimizing a single
alignment objective can inadvertently overwrite representations learned for
other objectives, leading to catastrophic forgetting. More recent approaches
extend steering vectors via one-to-many (1-to-N) Transformer decoders. While
this alleviates catastrophic forgetting, naive multi-branch designs optimize
each objective independently, which can cause inference fragmentation-outputs
across HHH objectives may become inconsistent. We propose Adaptive Multi-Branch
Steering (AMBS), a two-stage 1-to-N framework for unified and efficient
multi-objective alignment. In Stage I, post-attention hidden states of the
Transformer layer are computed once to form a shared representation. In Stage
II, this representation is cloned into parallel branches and steered via a
policy-reference mechanism, enabling objective-specific control while
maintaining cross-objective consistency. Empirical evaluations on Alpaca,
BeaverTails, and TruthfulQA show that AMBS consistently improves HHH alignment
across multiple 7B LLM backbones. For example, on DeepSeek-7B, AMBS improves
average alignment scores by +32.4% and reduces unsafe outputs by 11.0% compared
to a naive 1-to-N baseline, while remaining competitive with state-of-the-art
methods.

</details>


### [96] [InfiR2: A Comprehensive FP8 Training Recipe for Reasoning-Enhanced Language Models](https://arxiv.org/abs/2509.22536)
*Wenjun Wang,Shuo Cai,Congkai Xie,Mingfa Feng,Yiming Zhang,Zhen Li,Kejing Yang,Ming Li,Jiannong Cao,Yuan Xie,Hongxia Yang*

Main category: cs.CL

TL;DR: 本文提出了一种端到端的FP8训练方案，实现了训练时间减22%、内存峰值减14%、吞吐量增19%的效率提升，性能与BF16基线保持一致。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练计算成本高，FP8训练虽有潜力但因缺乏开源完善方案难以推广。

Method: 采用细粒度混合量化策略，融合持续预训练和监督微调，实现数值精度与计算效率的平衡。

Result: 在1600亿词语语料上训练，方法稳定且基本无损，性能与BF16基线相当，同时显著提升效率。

Conclusion: FP8训练是一种实用且稳健的BF16替代方案，代码开源有助于推动大规模模型训练民主化。

Abstract: The immense computational cost of training Large Language Models (LLMs)
presents a major barrier to innovation. While FP8 training offers a promising
solution with significant theoretical efficiency gains, its widespread adoption
has been hindered by the lack of a comprehensive, open-source training recipe.
To bridge this gap, we introduce an end-to-end FP8 training recipe that
seamlessly integrates continual pre-training and supervised fine-tuning. Our
methodology employs a fine-grained, hybrid-granularity quantization strategy to
maintain numerical fidelity while maximizing computational efficiency. Through
extensive experiments, including the continue pre-training of models on a
160B-token corpus, we demonstrate that our recipe is not only remarkably stable
but also essentially lossless, achieving performance on par with the BF16
baseline across a suite of reasoning benchmarks. Crucially, this is achieved
with substantial efficiency improvements, including up to a 22% reduction in
training time, a 14% decrease in peak memory usage, and a 19% increase in
throughput. Our results establish FP8 as a practical and robust alternative to
BF16, and we will release the accompanying code to further democratize
large-scale model training.

</details>


### [97] [Think Socially via Cognitive Reasoning](https://arxiv.org/abs/2509.22546)
*Jinfeng Zhou,Zheyu Chen,Shuai Wang,Quanyu Dai,Zhenhua Dong,Hongning Wang,Minlie Huang*

Main category: cs.CL

TL;DR: 该论文提出了基于认知推理的CogFlow框架，提升大语言模型在社交推理中的表现。


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型擅长逻辑推理但不善于处理模棱两可、需解释的社交情境，缺乏有效的社交认知能力。

Method: 将社交认知建模为认知流，设计结构化认知单位并用树状规划模拟人类思维，先通过监督微调强化认知能力，再用多目标奖励的强化学习提升模型表现。

Result: 大量实验表明CogFlow显著增强了大语言模型及人类的社交认知能力，提升社交决策效果。

Conclusion: CogFlow成功构建了具备人类社交认知思维流程的LLM框架，有效弥补了逻辑推理模型在社交情境中的不足。

Abstract: LLMs trained for logical reasoning excel at step-by-step deduction to reach
verifiable answers. However, this paradigm is ill-suited for navigating social
situations, which induce an interpretive process of analyzing ambiguous cues
that rarely yield a definitive outcome. To bridge this gap, we introduce
Cognitive Reasoning, a paradigm modeled on human social cognition. It
formulates the interpretive process into a structured cognitive flow of
interconnected cognitive units (e.g., observation or attribution), which
combine adaptively to enable effective social thinking and responses. We then
propose CogFlow, a complete framework that instills this capability in LLMs.
CogFlow first curates a dataset of cognitive flows by simulating the
associative and progressive nature of human thought via tree-structured
planning. After instilling the basic cognitive reasoning capability via
supervised fine-tuning, CogFlow adopts reinforcement learning to enable the
model to improve itself via trial and error, guided by a multi-objective reward
that optimizes both cognitive flow and response quality. Extensive experiments
show that CogFlow effectively enhances the social cognitive capabilities of
LLMs, and even humans, leading to more effective social decision-making.

</details>


### [98] [Retrieval-Augmented Guardrails for AI-Drafted Patient-Portal Messages: Error Taxonomy Construction and Large-Scale Evaluation](https://arxiv.org/abs/2509.22565)
*Wenyuan Chen,Fateme Nateghi Haredasht,Kameron C. Black,Francois Grolleau,Emily Alsentzer,Jonathan H. Chen,Stephen P. Ma*

Main category: cs.CL

TL;DR: 本文提出了一种用于评估电子健康记录（EHR）门户中异步患者与临床医生消息回复草稿质量的系统，该系统结合了临床错误本体、基于检索的评估流程和分层错误检测架构。


<details>
  <summary>Details</summary>
Motivation: 随着EHR门户中患者与临床医生消息交流的增加，临床医生工作负担加重；大型语言模型可辅助撰写回复草稿，但存在临床错误风险，因此需要一种稳健的评估方法保证回复质量。

Method: 引入包含5个领域和59个细分错误代码的临床错误本体，开发基于语义相似历史消息-回复对的检索增强评估流程（RAEC），并使用两阶段DSPy提示架构实现可扩展、可解释的分层错误检测。

Result: 在超过1500条患者消息上，结合检索的上下文信息改善了临床完整性和工作流程适当性等领域的错误识别能力。人类验证显示，带上下文的标签与基线相比在一致性（50%对33%）和性能（F1=0.500对0.256）方面均有显著提升。

Conclusion: 该RAEC评估流水线能够作为利用历史消息上下文的AI安全网，有效提升基于LLM的患者消息回复草稿的错误检测和质量保障能力。

Abstract: Asynchronous patient-clinician messaging via EHR portals is a growing source
of clinician workload, prompting interest in large language models (LLMs) to
assist with draft responses. However, LLM outputs may contain clinical
inaccuracies, omissions, or tone mismatches, making robust evaluation
essential. Our contributions are threefold: (1) we introduce a clinically
grounded error ontology comprising 5 domains and 59 granular error codes,
developed through inductive coding and expert adjudication; (2) we develop a
retrieval-augmented evaluation pipeline (RAEC) that leverages semantically
similar historical message-response pairs to improve judgment quality; and (3)
we provide a two-stage prompting architecture using DSPy to enable scalable,
interpretable, and hierarchical error detection. Our approach assesses the
quality of drafts both in isolation and with reference to similar past
message-response pairs retrieved from institutional archives. Using a two-stage
DSPy pipeline, we compared baseline and reference-enhanced evaluations on over
1,500 patient messages. Retrieval context improved error identification in
domains such as clinical completeness and workflow appropriateness. Human
validation on 100 messages demonstrated superior agreement (concordance = 50%
vs. 33%) and performance (F1 = 0.500 vs. 0.256) of context-enhanced labels vs.
baseline, supporting the use of our RAEC pipeline as AI guardrails for patient
messaging.

</details>


### [99] [Fine-Grained Detection of Context-Grounded Hallucinations Using LLMs](https://arxiv.org/abs/2509.22582)
*Yehonatan Pesiakhovsky,Zorik Gekhman,Yosi Mass,Liat Ein-Dor,Roi Reichart*

Main category: cs.CL

TL;DR: 该论文研究大型语言模型(LLMs)在定位语境幻觉（模型输出含不可验证信息）中的适用性，提出了新的错误表示方法，并构建了包含1000多个样本的人类注释基准。


<details>
  <summary>Details</summary>
Motivation: 现有复杂的评估流程难以有效鉴别模型输出中的幻觉，缺少适用于大型语言模型的本地化幻觉评估基准。

Method: 构建面向LLMs的幻觉定位基准，采用自由文本描述错误的新表示方式，设计LLM驱动的评估协议并通过人类评估验证其有效性，系统比较了四个大型语言模型。

Result: 最佳模型F1仅达0.67，基准测试难度大；存在误判缺失细节为不一致和对事实正确但源文本无记载内容的识别难题。

Conclusion: 研究揭示了幻觉本地化的挑战，提出了最佳提示策略，推动了幻觉检测技术更实用、更精细的发展。

Abstract: Context-grounded hallucinations are cases where model outputs contain
information not verifiable against the source text. We study the applicability
of LLMs for localizing such hallucinations, as a more practical alternative to
existing complex evaluation pipelines. In the absence of established benchmarks
for meta-evaluation of hallucinations localization, we construct one tailored
to LLMs, involving a challenging human annotation of over 1,000 examples. We
complement the benchmark with an LLM-based evaluation protocol, verifying its
quality in a human evaluation. Since existing representations of hallucinations
limit the types of errors that can be expressed, we propose a new
representation based on free-form textual descriptions, capturing the full
range of possible errors. We conduct a comprehensive study, evaluating four
large-scale LLMs, which highlights the benchmark's difficulty, as the best
model achieves an F1 score of only 0.67. Through careful analysis, we offer
insights into optimal prompting strategies for the task and identify the main
factors that make it challenging for LLMs: (1) a tendency to incorrectly flag
missing details as inconsistent, despite being instructed to check only facts
in the output; and (2) difficulty with outputs containing factually correct
information absent from the source - and thus not verifiable - due to alignment
with the model's parametric knowledge.

</details>


### [100] [ArabJobs: A Multinational Corpus of Arabic Job Ads](https://arxiv.org/abs/2509.22589)
*Mo El-Haj*

Main category: cs.CL

TL;DR: 本文介绍了名为ArabJobs的阿拉伯语招聘广告语料库，涵盖8500多条信息和55万字，涵盖多个阿拉伯国家和方言，支持多种自然语言处理任务。


<details>
  <summary>Details</summary>
Motivation: 为了研究和分析阿拉伯劳动市场的语言多样性、性别表现和地区变异，同时推动公平性意识的阿拉伯语自然语言处理研究。

Method: 收集埃及、约旦、沙特阿拉伯和阿联酋的招聘广告，进行语言学、性别代表性、职业结构和方言差异等方面的分析，并使用大语言模型做薪资估计和职位分类。

Result: 构建了一个多样化且具有地区和社会经济变异的招聘广告数据集，完成了性别偏见检测和职业分类的基准实验，展示了数据集在公平性意识的阿拉伯语NLP中的有效性。

Conclusion: ArabJobs数据集为阿拉伯语劳动市场的语言研究和公平性相关自然语言处理任务提供了重要资源，具有广泛的应用价值且公开可获取。

Abstract: ArabJobs is a publicly available corpus of Arabic job advertisements
collected from Egypt, Jordan, Saudi Arabia, and the United Arab Emirates.
Comprising over 8,500 postings and more than 550,000 words, the dataset
captures linguistic, regional, and socio-economic variation in the Arab labour
market. We present analyses of gender representation and occupational
structure, and highlight dialectal variation across ads, which offers
opportunities for future research. We also demonstrate applications such as
salary estimation and job category normalisation using large language models,
alongside benchmark tasks for gender bias detection and profession
classification. The findings show the utility of ArabJobs for fairness-aware
Arabic NLP and labour market research. The dataset is publicly available on
GitHub: https://github.com/drelhaj/ArabJobs.

</details>


### [101] [From Formal Language Theory to Statistical Learning: Finite Observability of Subregular Languages](https://arxiv.org/abs/2509.22598)
*Katsuhiko Hayashi,Hidetaka Kamigaito*

Main category: cs.CL

TL;DR: 本文证明了所有标准子正则语言类在其决定性谓词表示下均可线性分割，确保了有限可观测性和使用简单线性模型的可学习性。


<details>
  <summary>Details</summary>
Motivation: 为自然语言结构建模提供一个严格可解释的基础，解决语言类的可分性与可学习性问题。

Method: 利用决定性谓词表示子正则语言类，结合合成和真实英语形态学数据的实验验证模型线性可分性和特征解释性。

Result: 在无噪声条件下验证了完美线性可分性，且在真实数据上学到的特征符合已有语言学约束。

Conclusion: 子正则层级为自然语言结构建模提供了一个既严格又具解释性的理论基础，支持简单线性方法的学习能力。

Abstract: We prove that all standard subregular language classes are linearly separable
when represented by their deciding predicates. This establishes finite
observability and guarantees learnability with simple linear models. Synthetic
experiments confirm perfect separability under noise-free conditions, while
real-data experiments on English morphology show that learned features align
with well-known linguistic constraints. These results demonstrate that the
subregular hierarchy provides a rigorous and interpretable foundation for
modeling natural language structure. Our code used in real-data experiments is
available at https://github.com/UTokyo-HayashiLab/subregular.

</details>


### [102] [Capturing Opinion Shifts in Deliberative Discourse through Frequency-based Quantum deep learning methods](https://arxiv.org/abs/2509.22603)
*Rakesh Thakur,Harsh Chaturvedi,Ruqayya Shah,Janvi Chauhan,Ayush Sharma*

Main category: cs.CL

TL;DR: 本文通过比较多种自然语言处理技术，分析模型在理解审议性话语和产生有意义见解方面的表现，利用来自多背景个体的观点数据集模拟审议过程，并提出两种优于现有模型的方法。


<details>
  <summary>Details</summary>
Motivation: 审议过程通过权衡不同观点影响决策结果，近年来NLP技术使得从文本中建模审议成为可能，探究不同模型在该领域的有效性具有重要实际意义。

Method: 构建包含多样观点的自采数据集，利用带有令人信服事实的产品展示模拟审议场景，比较频率基话语调制模型和量子审议框架两种方法的表现。

Result: 这两种方法在理解和预测意见变化方面均优于现有先进模型，显示出更好的性能。

Conclusion: 研究成果在公共政策制定、辩论评估、决策支持及社交媒体舆情挖掘等领域具备广泛应用价值。

Abstract: Deliberation plays a crucial role in shaping outcomes by weighing diverse
perspectives before reaching decisions. With recent advancements in Natural
Language Processing, it has become possible to computationally model
deliberation by analyzing opinion shifts and predicting potential outcomes
under varying scenarios. In this study, we present a comparative analysis of
multiple NLP techniques to evaluate how effectively models interpret
deliberative discourse and produce meaningful insights. Opinions from
individuals of varied backgrounds were collected to construct a self-sourced
dataset that reflects diverse viewpoints. Deliberation was simulated using
product presentations enriched with striking facts, which often prompted
measurable shifts in audience opinions. We have given comparative analysis
between two models namely Frequency-Based Discourse Modulation and
Quantum-Deliberation Framework which outperform the existing state of art
models. The findings highlight practical applications in public policy-making,
debate evaluation, decision-support frameworks, and large-scale social media
opinion mining.

</details>


### [103] [From tests to effect sizes: Quantifying uncertainty and statistical variability in multilingual and multitask NLP evaluation benchmarks](https://arxiv.org/abs/2509.22612)
*Jonne Sälevä,Duygu Ataman,Constantine Lignos*

Main category: cs.CL

TL;DR: 本文提出了一套基于重采样的方法，用于多语言及多任务NLP评测指标的不确定性和统计精度量化。


<details>
  <summary>Details</summary>
Motivation: 现有评测忽略了模型和数据的多重变异来源，低估了性能波动，影响结果的准确性和可靠性。

Method: 采用重采样技术，结合多语言问答、机器翻译和命名实体识别任务，计算评测指标的采样分布，包括平均值、中位数、模型间差异及排名分布。

Result: 实验证明考虑模型和数据源的变异，可更准确估计性能波动；重采样方法为排行榜指标提供了可靠的统计量计算手段。

Conclusion: 结合模型和数据变异的重采样评估方法是衡量多语言多任务NLP系统性能不确定性的有效工具，能够避免低估评测指标的变异性。

Abstract: In this paper, we introduce a set of resampling-based methods for quantifying
uncertainty and statistical precision of evaluation metrics in multilingual
and/or multitask NLP benchmarks. We show how experimental variation in
performance scores arises from both model- and data-related sources, and that
accounting for both of them is necessary to avoid substantially underestimating
the overall variability over hypothetical replications. Using multilingual
question answering, machine translation, and named entity recognition as
example tasks, we also demonstrate how resampling methods are useful for
computing sampling distributions for various quantities used in leaderboards
such as the average/median, pairwise differences between models, and rankings.

</details>


### [104] [StateX: Enhancing RNN Recall via Post-training State Expansion](https://arxiv.org/abs/2509.22630)
*Xingyu Shen,Yingfa Chen,Zhen Leng Thai,Xu Han,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: 本文提出StateX，一种通过后训练扩展预训练RNN状态尺寸的方法，提升RNN对长上下文的记忆能力。


<details>
  <summary>Details</summary>
Motivation: Transformer模型虽然效果好但处理长上下文成本高；RNN因复杂度恒定受关注但记忆长上下文信息困难，状态尺寸限制了其召回能力。

Method: 设计后训练架构改进方案，针对线性注意力和状态空间模型，扩展状态尺寸且参数量增加极小或无增加。

Result: 在模型规模达13亿参数的实验中，StateX有效提升了RNN的记忆力和上下文学习能力，同时保持低后训练成本和其他能力不受影响。

Conclusion: StateX为扩展预训练RNN状态提供了一种高效可行的后训练策略，提升了处理长上下文的能力，兼顾成本和性能。

Abstract: While Transformer-based models have demonstrated remarkable language modeling
performance, their high complexities result in high costs when processing long
contexts. In contrast, recurrent neural networks (RNNs) such as linear
attention and state space models have gained popularity due to their constant
per-token complexities. However, these recurrent models struggle with tasks
that require accurate recall of contextual information from long contexts,
because all contextual information is compressed into a constant-size recurrent
state. Previous works have shown that recall ability is positively correlated
with the recurrent state size, yet directly training RNNs with larger recurrent
states results in high training costs. In this paper, we introduce StateX, a
training pipeline for efficiently expanding the states of pre-trained RNNs
through post-training. For two popular classes of RNNs, linear attention and
state space models, we design post-training architectural modifications to
scale up the state size with no or negligible increase in model parameters.
Experiments on models up to 1.3B parameters demonstrate that StateX efficiently
enhances the recall and in-context learning ability of RNNs without incurring
high post-training costs or compromising other capabilities.

</details>


### [105] [Variational Reasoning for Language Models](https://arxiv.org/abs/2509.22637)
*Xiangxin Zhou,Zichen Liu,Haonan Wang,Chao Du,Min Lin,Chongxuan Li,Liang Wang,Tianyu Pang*

Main category: cs.CL

TL;DR: 该论文提出了一种变分推理框架优化语言模型的推理轨迹，提高推理能力并稳定训练过程。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型推理能力的提升受到推理轨迹不稳定和训练目标不明确的限制，亟需更稳健的推理优化方法。

Method: 基于证据下界（ELBO）扩展多轨迹目标，采用前向KL散度稳定变分后验训练，同时将拒绝采样微调和基于二元奖励的强化学习方法解释为局部前向KL目标，揭示模型对简单问题的偏向。

Result: 在Qwen 2.5和Qwen 3系列模型及多种推理任务上实验证明该方法有效提升了推理性能并提高了训练稳定性。

Conclusion: 该工作从概率视角统一了变分推理与强化学习方法，提出了稳定且有效的推理优化目标，有助于提升语言模型的推理能力。

Abstract: We introduce a variational reasoning framework for language models that
treats thinking traces as latent variables and optimizes them through
variational inference. Starting from the evidence lower bound (ELBO), we extend
it to a multi-trace objective for tighter bounds and propose a forward-KL
formulation that stabilizes the training of the variational posterior. We
further show that rejection sampling finetuning and binary-reward RL, including
GRPO, can be interpreted as local forward-KL objectives, where an implicit
weighting by model accuracy naturally arises from the derivation and reveals a
previously unnoticed bias toward easier questions. We empirically validate our
method on the Qwen 2.5 and Qwen 3 model families across a wide range of
reasoning tasks. Overall, our work provides a principled probabilistic
perspective that unifies variational inference with RL-style methods and yields
stable objectives for improving the reasoning ability of language models. Our
code is available at https://github.com/sail-sg/variational-reasoning.

</details>


### [106] [Language Models Can Learn from Verbal Feedback Without Scalar Rewards](https://arxiv.org/abs/2509.22638)
*Renjie Luo,Zichen Liu,Xiangyan Liu,Chao Du,Min Lin,Wenhu Chen,Wei Lu,Tianyu Pang*

Main category: cs.CL

TL;DR: 该论文提出了反馈条件策略（FCP），通过将语言反馈作为条件信号，直接从响应-反馈对学习，替代传统将反馈压缩为标量奖励的RL方法，提升大模型对反馈的利用效率。


<details>
  <summary>Details</summary>
Motivation: 传统基于人类或AI反馈的RL训练方法常将丰富的语言反馈压缩为单一的标量奖励，导致信息丢失和尺度不平衡，限制了模型学习效果。

Method: 论文提出反馈条件策略（FCP），利用语言反馈作为条件信号，以最大似然训练方式，从离线响应-反馈对中学习反馈条件后验，同时设计在线自举阶段，通过正向条件生成并获取新反馈进行策略优化。

Result: FCP将反馈驱动学习重新定义为条件生成而非奖励优化，提升了模型从语言反馈直接学习的表达能力，实验结果表明其优于传统基于奖励的RL方法。

Conclusion: 反馈条件策略提供了一种更具表现力的方式，使大型语言模型能够更有效地利用丰富的语言反馈进行学习，改善训练效果。

Abstract: LLMs are often trained with RL from human or AI feedback, yet such methods
typically compress nuanced feedback into scalar rewards, discarding much of
their richness and inducing scale imbalance. We propose treating verbal
feedback as a conditioning signal. Inspired by language priors in text-to-image
generation, which enable novel outputs from unseen prompts, we introduce the
feedback-conditional policy (FCP). FCP learns directly from response-feedback
pairs, approximating the feedback-conditional posterior through maximum
likelihood training on offline data. We further develop an online bootstrapping
stage where the policy generates under positive conditions and receives fresh
feedback to refine itself. This reframes feedback-driven learning as
conditional generation rather than reward optimization, offering a more
expressive way for LLMs to directly learn from verbal feedback. Our code is
available at https://github.com/sail-sg/feedback-conditional-policy.

</details>


### [107] [Death of the Novel(ty): Beyond n-Gram Novelty as a Metric for Textual Creativity](https://arxiv.org/abs/2509.22641)
*Arkadiy Saakyan,Najoung Kim,Smaranda Muresan,Tuhin Chakrabarty*

Main category: cs.CL

TL;DR: 本文研究了n-gram新颖性作为文本创造力评价指标的局限性，结合7542条专家注释分析了新颖性、适切性和合理性之间的关系，发现n-gram新颖性与创造力正相关但误判率高，且开放源码大模型的新颖性与适切性负相关。研究还测试了不同大模型识别创造性和非适切性表达的能力，指出当前模型尚有改进空间。


<details>
  <summary>Details</summary>
Motivation: 当前广泛采用n-gram新颖性来评价文本创造力，但理论认为创造力包含新颖性和适切性两方面，单纯依赖n-gram新颖性可能不足以准确评估文本创造力。

Method: 通过7542条专家（26人）对人工及AI生成文本进行细读，注释新颖性、适切性和合理性，分析这些评价与n-gram新颖性之间的关系，并测试零样本、少样本及微调模型识别创造性与非适切性表达的能力。

Result: 发现n-gram新颖性与专家评判创造力正相关，但91%的高新颖表达不被认定为创造性；开放源码大模型高新颖性对应低适切性；密闭源模型创造性表达低于人类；前沿大模型在识别创造性表达上优于随机但难以识别非适切表达；最佳模型的n-gram新颖性评分能预测专家偏好。

Conclusion: n-gram新颖性指标不能单独作为文本创造力评价依据，应结合适切性考虑；当前大模型在创造力识别能力上存在不足，有改进空间；未来评估体系应兼顾文本新颖性与适切性两方面。

Abstract: N-gram novelty is widely used to evaluate language models' ability to
generate text outside of their training data. More recently, it has also been
adopted as a metric for measuring textual creativity. However, theoretical work
on creativity suggests that this approach may be inadequate, as it does not
account for creativity's dual nature: novelty (how original the text is) and
appropriateness (how sensical and pragmatic it is). We investigate the
relationship between this notion of creativity and n-gram novelty through 7542
expert writer annotations (n=26) of novelty, pragmaticality, and sensicality
via close reading of human and AI-generated text. We find that while n-gram
novelty is positively associated with expert writer-judged creativity, ~91% of
top-quartile expressions by n-gram novelty are not judged as creative,
cautioning against relying on n-gram novelty alone. Furthermore, unlike
human-written text, higher n-gram novelty in open-source LLMs correlates with
lower pragmaticality. In an exploratory study with frontier close-source
models, we additionally confirm that they are less likely to produce creative
expressions than humans. Using our dataset, we test whether zero-shot,
few-shot, and finetuned models are able to identify creative expressions (a
positive aspect of writing) and non-pragmatic ones (a negative aspect).
Overall, frontier LLMs exhibit performance much higher than random but leave
room for improvement, especially struggling to identify non-pragmatic
expressions. We further find that LLM-as-a-Judge novelty scores from the
best-performing model were predictive of expert writer preferences.

</details>


### [108] [WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level Feedback and Step-Level Reinforcement Learning](https://arxiv.org/abs/2509.22644)
*Zimu Lu,Houxing Ren,Yunqiao Yang,Ke Wang,Zhuofan Zong,Junting Pan,Mingjie Zhan,Hongsheng Li*

Main category: cs.CL

TL;DR: 本文提出了WebGen-Agent，一个利用多层次视觉反馈迭代生成和优化网站代码的智能体系统，显著提升了网站代码生成的准确性和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的代码生成任务缺乏有效的视觉反馈手段，无法准确评估网站代码生成的实际质量，限制了性能提升。

Method: 利用视觉语言模型生成详细的文本描述和评分，通过截图和GUI-agent的评分结合回溯选择最佳代码，辅以Step-GRPO训练方法利用视觉反馈作为奖励信号，增强LLM推理能力和代码生成效果。

Result: 在WebGen-Bench数据集上，WebGen-Agent使Claude-3.5-Sonnet的准确率从26.4%提升至51.9%，视觉得分从3.0提升至3.9，同时Step-GRPO提高了Qwen2.5-Coder-7B-Instruct的准确率和外观得分，超过之前最先进系统。

Conclusion: 通过引入多层次视觉反馈和基于视觉奖励的强化训练，WebGen-Agent显著提升了网站代码生成的质量和表现，证明了视觉反馈在代码生成任务中的重要作用。

Abstract: Agent systems powered by large language models (LLMs) have demonstrated
impressive performance on repository-level code-generation tasks. However, for
tasks such as website codebase generation, which depend heavily on visual
effects and user-interaction feedback, current code agents rely only on simple
code execution for feedback and verification. This approach fails to capture
the actual quality of the generated code. In this paper, we propose
WebGen-Agent, a novel website-generation agent that leverages comprehensive and
multi-level visual feedback to iteratively generate and refine the website
codebase. Detailed and expressive text descriptions and suggestions regarding
the screenshots and GUI-agent testing of the websites are generated by a visual
language model (VLM), together with scores that quantify their quality. The
screenshot and GUI-agent scores are further integrated with a backtracking and
select-best mechanism, enhancing the performance of the agent. Utilizing the
accurate visual scores inherent in the WebGen-Agent workflow, we further
introduce \textit{Step-GRPO with Screenshot and GUI-agent Feedback} to improve
the ability of LLMs to act as the reasoning engine of WebGen-Agent. By using
the screenshot and GUI-agent scores at each step as the reward in Step-GRPO, we
provide a dense and reliable process supervision signal, which effectively
improves the model's website-generation ability. On the WebGen-Bench dataset,
WebGen-Agent increases the accuracy of Claude-3.5-Sonnet from 26.4% to 51.9%
and its appearance score from 3.0 to 3.9, outperforming the previous
state-of-the-art agent system. Additionally, our Step-GRPO training approach
increases the accuracy of Qwen2.5-Coder-7B-Instruct from 38.9% to 45.4% and
raises the appearance score from 3.4 to 3.7.

</details>


### [109] [VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing](https://arxiv.org/abs/2509.22651)
*Ke Wang,Houxing Ren,Zimu Lu,Mingjie Zhan,Hongsheng Li*

Main category: cs.CL

TL;DR: 本文提出了VoiceAssistant-Eval，一个综合性的评测基准，评估AI语音助手在听、说、视方面的能力，涵盖10,497个样本和13个任务类别。


<details>
  <summary>Details</summary>
Motivation: 现有基准不足以全面评估新兴的多模态语音助手的全部功能，迫切需要一个覆盖听说视全方位能力的评测框架。

Method: 设计包含自然声音、音乐、对话等听力任务，多轮对话、角色扮演等口语任务，及异构图像的视觉任务，使用21个开源模型及GPT-4o-Audio进行测试，评估响应质量、一致性和语音表现。

Result: 结果显示专有模型未必优于开源模型，模型口语表现优于听力理解，小型精心设计模型表现可媲美大型模型。另外，多模态输入和角色扮演模仿任务难度较大，模型在鲁棒性和安全性对齐上仍有显著差距。

Conclusion: VoiceAssistant-Eval揭示了当前AI语音助手的短板，为后续模型研发提供了严格标准和指导，推动下一代多模态语音助理的发展。

Abstract: The growing capabilities of large language models and multimodal systems have
spurred interest in voice-first AI assistants, yet existing benchmarks are
inadequate for evaluating the full range of these systems' capabilities. We
introduce VoiceAssistant-Eval, a comprehensive benchmark designed to assess AI
assistants across listening, speaking, and viewing. VoiceAssistant-Eval
comprises 10,497 curated examples spanning 13 task categories. These tasks
include natural sounds, music, and spoken dialogue for listening; multi-turn
dialogue, role-play imitation, and various scenarios for speaking; and highly
heterogeneous images for viewing. To demonstrate its utility, we evaluate 21
open-source models and GPT-4o-Audio, measuring the quality of the response
content and speech, as well as their consistency. The results reveal three key
findings: (1) proprietary models do not universally outperform open-source
models; (2) most models excel at speaking tasks but lag in audio understanding;
and (3) well-designed smaller models can rival much larger ones. Notably, the
mid-sized Step-Audio-2-mini (7B) achieves more than double the listening
accuracy of LLaMA-Omni2-32B-Bilingual. However, challenges remain: multimodal
(audio plus visual) input and role-play voice imitation tasks are difficult for
current models, and significant gaps persist in robustness and safety
alignment. VoiceAssistant-Eval identifies these gaps and establishes a rigorous
framework for evaluating and guiding the development of next-generation AI
assistants. Code and data will be released at
https://mathllm.github.io/VoiceAssistantEval/ .

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [110] [Visual Multi-Agent System: Mitigating Hallucination Snowballing via Visual Flow](https://arxiv.org/abs/2509.21789)
*Xinlei Yu,Chengming Xu,Guibin Zhang,Yongbo He,Zhangquan Chen,Zhucun Xue,Jiangning Zhang,Yue Liao,Xiaobin Hu,Yu-Gang Jiang,Shuicheng Yan*

Main category: cs.MA

TL;DR: 该论文研究了多智能体系统中视觉语言模型引发的视觉幻觉堆积问题，提出了通过视觉流和注意力再分配的轻量级解决方案ViF，有效减轻了幻觉堆积并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统中视觉语言模型过度依赖文本信息传递视觉内容，导致视觉幻觉在多个智能体间逐层放大，影响任务性能。

Method: 通过注意力分析识别视觉信息丢失关键环节，提出ViF方法利用中层具有单峰注意力的视觉token进行视觉信息传递，并通过注意力再分配强化该传递。

Result: 在八个基准测试和多种模型结构上，ViF显著减少了视觉幻觉堆积现象，提升了多智能体系统的整体表现。

Conclusion: ViF作为一种轻量级插件式方法，有效缓解了多智能体视觉语言模型中的视觉幻觉堆积问题，增强了系统的稳定性和性能。

Abstract: Multi-Agent System (MAS) powered by Visual Language Models (VLMs) enables
challenging tasks but suffers from a novel failure term, multi-agent visual
hallucination snowballing, where hallucinations are seeded in a single agent
and amplified by following ones due to the over-reliance on textual flow to
relay visual information. Through turn-, layer-, and token-wise attention
analyses, we provide detailed insights into the essence of hallucination
snowballing regarding the reduction of visual attention allocation. It leads us
to identify a subset of vision tokens with a unimodal attention peak in middle
layers that best preserve visual evidence but gradually diminish in deeper
agent turns, resulting in the visual hallucination snowballing in MAS. Thus, we
propose ViF, a lightweight, plug-and-play mitigation paradigm that relays
inter-agent messages with Visual Flow powered by the selected visual relay
tokens and applies attention reallocation to amplify this pattern. The
experiment results demonstrate that our method markedly reduces hallucination
snowballing, consistently improving the performance across eight benchmarks
based on four common MAS structures and ten base models. The source code will
be available at: https://github.com/YU-deep/ViF.git.

</details>


### [111] [RobustFlow: Towards Robust Agentic Workflow Generation](https://arxiv.org/abs/2509.21834)
*Shengxiang Xu,Jiayi Zhang,Shimin Di,Yuyu Luo,Liang Yao,Hanmo Liu,Jia Zhu,Fan Liu,Min-Ling Zhang*

Main category: cs.MA

TL;DR: 本文研究了大语言模型（LLMs）自动生成任务工作流程的鲁棒性问题，提出了基于节点和拓扑相似性的评估指标及一种名为RobustFlow的训练框架，显著提升了流程一致性。


<details>
  <summary>Details</summary>
Motivation: 当前自动生成的agentic工作流程在面对语义相同但表达不同的指令时表现出严重不一致性，影响其在实际应用中的可靠性和可信度，亟需解决鲁棒性问题。

Method: 提出基于节点和拓扑相似性的工作流程一致性度量指标，设计了基于偏好优化的RobustFlow训练框架，通过训练模型识别和适应任务描述的同义变体，从而提升工作流程的鲁棒性。

Result: 采用RobustFlow训练后，模型在处理同义任务描述时的工作流程鲁棒性指标提升至70% - 90%，明显优于现有方法。

Conclusion: RobustFlow有效提高了大语言模型生成工作流程时对语义变异的鲁棒性，为提升其在复杂任务中的应用可靠性提供了重要支持。

Abstract: The automated generation of agentic workflows is a promising frontier for
enabling large language models (LLMs) to solve complex tasks. However, our
investigation reveals that the robustness of agentic workflow remains a
critical, unaddressed challenge. Current methods often generate wildly
inconsistent workflows when provided with instructions that are semantically
identical but differently phrased. This brittleness severely undermines their
reliability and trustworthiness for real-world applications. To quantitatively
diagnose this instability, we propose metrics based on nodal and topological
similarity to evaluate workflow consistency against common semantic variations
such as paraphrasing and noise injection. Subsequently, we further propose a
novel training framework, RobustFlow, that leverages preference optimization to
teach models invariance to instruction variations. By training on sets of
synonymous task descriptions, RobustFlow boosts workflow robustness scores to
70\% - 90\%, which is a substantial improvement over existing approaches. The
code is publicly available at https://github.com/DEFENSE-SEU/RobustFlow.

</details>


### [112] [Multi-Agent Path Finding via Offline RL and LLM Collaboration](https://arxiv.org/abs/2509.22130)
*Merve Atasever,Matthew Hong,Mihir Nitin Kulkarni,Qingpei Li,Jyotirmoy V. Deshmukh*

Main category: cs.MA

TL;DR: 本文提出了一种基于决策变换器(Decision Transformer)的多智能体路径规划框架，通过离线强化学习大幅缩短训练时间，并结合GPT-4o提升在动态环境中的适应性和表现。


<details>
  <summary>Details</summary>
Motivation: 多智能体路径寻找问题因组合复杂性和部分观测性存在训练时间长和碰撞频发等挑战。

Method: 采用基于决策变换器的离线强化学习方法，解决长期奖励分配问题并提升稀疏奖励场景的表现，同时引入GPT-4o动态指导策略以应对环境变化。

Result: 在静态及动态环境中，所提方法显著提升了多智能体的适应性和性能，训练时间从几周缩短至数小时。

Conclusion: 结合决策变换器和大语言模型的框架，有效解决了传统分散式强化学习中的自我中心行为和训练效率低下问题，推动了多智能体路径规划的实际应用。

Abstract: Multi-Agent Path Finding (MAPF) poses a significant and challenging problem
critical for applications in robotics and logistics, particularly due to its
combinatorial complexity and the partial observability inherent in realistic
environments. Decentralized reinforcement learning methods commonly encounter
two substantial difficulties: first, they often yield self-centered behaviors
among agents, resulting in frequent collisions, and second, their reliance on
complex communication modules leads to prolonged training times, sometimes
spanning weeks. To address these challenges, we propose an efficient
decentralized planning framework based on the Decision Transformer (DT),
uniquely leveraging offline reinforcement learning to substantially reduce
training durations from weeks to mere hours. Crucially, our approach
effectively handles long-horizon credit assignment and significantly improves
performance in scenarios with sparse and delayed rewards. Furthermore, to
overcome adaptability limitations inherent in standard RL methods under dynamic
environmental changes, we integrate a large language model (GPT-4o) to
dynamically guide agent policies. Extensive experiments in both static and
dynamically changing environments demonstrate that our DT-based approach,
augmented briefly by GPT-4o, significantly enhances adaptability and
performance.

</details>


### [113] [Impact of Collective Behaviors of Autonomous Vehicles on Urban Traffic Dynamics: A Multi-Agent Reinforcement Learning Approach](https://arxiv.org/abs/2509.22216)
*Ahmet Onur Akman,Anastasia Psarou,Zoltán György Varga,Grzegorz Jamróz,Rafał Kucharski*

Main category: cs.MA

TL;DR: 该研究利用强化学习驱动的自动驾驶车辆在混合交通环境中，研究其对城市交通流的影响，重点分析了多智能体日常路径选择问题。


<details>
  <summary>Details</summary>
Motivation: 探讨在混合交通环境下，自动驾驶车辆（RL驱动）对人类驾驶者路径选择及整体交通效率的影响。

Method: 将城市网络中三分之一的人类驾驶者转换为采用深度Q学习算法的自动驾驶车辆，设定不同行为目标（自私、协作、竞争、社会性、利他及恶意），通过奖励机制影响自动驾驶车辆行为，利用自研强化学习框架PARCOUR进行仿真。

Result: 自动驾驶车辆的旅行时间最多减少5%，且不同行为对人类驾驶者的影响不同；自利行为的自动驾驶车辆旅行时间显著短于人类驾驶者。

Conclusion: 多智能体强化学习适用于交通网络的集体路径优化，但自动驾驶车辆行为不同会对共存驾驶者产生显著差异的影响。

Abstract: This study examines the potential impact of reinforcement learning
(RL)-enabled autonomous vehicles (AV) on urban traffic flow in a mixed traffic
environment. We focus on a simplified day-to-day route choice problem in a
multi-agent setting. We consider a city network where human drivers travel
through their chosen routes to reach their destinations in minimum travel time.
Then, we convert one-third of the population into AVs, which are RL agents
employing Deep Q-learning algorithm. We define a set of optimization targets,
or as we call them behaviors, namely selfish, collaborative, competitive,
social, altruistic, and malicious. We impose a selected behavior on AVs through
their rewards. We run our simulations using our in-house developed RL framework
PARCOUR. Our simulations reveal that AVs optimize their travel times by up to
5\%, with varying impacts on human drivers' travel times depending on the AV
behavior. In all cases where AVs adopt a self-serving behavior, they achieve
shorter travel times than human drivers. Our findings highlight the complexity
differences in learning tasks of each target behavior. We demonstrate that the
multi-agent RL setting is applicable for collective routing on traffic
networks, though their impact on coexisting parties greatly varies with the
behaviors adopted.

</details>


### [114] [VizGen: Data Exploration and Visualization from Natural Language via a Multi-Agent AI Architecture](https://arxiv.org/abs/2509.22218)
*Sandaru Fernando,Imasha Jayarathne,Sithumini Abeysekara,Shanuja Sithamparanthan,Thushari Silva,Deshan Jayawardana*

Main category: cs.MA

TL;DR: VizGen是一种AI辅助的数据可视化系统，利用自然语言处理和大型语言模型，帮助用户通过自然语言查询生成图表，实现数据可视化。


<details>
  <summary>Details</summary>
Motivation: 传统数据可视化工具需要技术专业知识，限制了普通用户的使用。

Method: 采用先进的自然语言处理技术和多智能体架构，使用Claude 3.7 Sonnet和Gemini 2.0 Flash等大型语言模型，将用户自然语言查询转化为SQL并推荐合适的图表类型，支持图表定制和见解提取。

Result: 系统不仅实现了数据的可视化，还能分析数据中的模式、异常和相关性，提供带有互联网上下文的信息解释，并支持实时数据库交互和对话式图表优化。

Conclusion: VizGen有效降低了数据可视化的门槛，实现了技术复杂性与用户友好设计的结合，促进了数据可视化的普及和民主化。

Abstract: Data visualization is essential for interpreting complex datasets, yet
traditional tools often require technical expertise, limiting accessibility.
VizGen is an AI-assisted graph generation system that empowers users to create
meaningful visualizations using natural language. Leveraging advanced NLP and
LLMs like Claude 3.7 Sonnet and Gemini 2.0 Flash, it translates user queries
into SQL and recommends suitable graph types. Built on a multi-agent
architecture, VizGen handles SQL generation, graph creation, customization, and
insight extraction. Beyond visualization, it analyzes data for patterns,
anomalies, and correlations, and enhances user understanding by providing
explanations enriched with contextual information gathered from the internet.
The system supports real-time interaction with SQL databases and allows
conversational graph refinement, making data analysis intuitive and accessible.
VizGen democratizes data visualization by bridging the gap between technical
complexity and user-friendly design.

</details>


### [115] [Effective Policy Learning for Multi-Agent Online Coordination Beyond Submodular Objectives](https://arxiv.org/abs/2509.22596)
*Qixin Zhang,Yan Sun,Can Jin,Xikun Zhang,Yao Shu,Puning Zhao,Li Shen,Dacheng Tao*

Main category: cs.MA

TL;DR: 本文提出了两种针对多智能体在线协调问题的有效策略学习算法，分别为\texttt{MA-SPL}和\texttt{MA-MPL}，能处理弱次模问题并且保证近似最优性。


<details>
  <summary>Details</summary>
Motivation: 多智能体在线协调问题存在次模函数优化的复杂性，且现实中许多目标函数是弱次模函数，需要设计有效且无需依赖未知参数的算法。

Method: 提出基于新的\emph{策略连续扩展}的连续松弛技术，设计了保证近似最优性的\texttt{MA-SPL}算法（依赖未知参数）和无需参数的\texttt{MA-MPL}算法。

Result: \texttt{MA-SPL}算法达到了$(1-\frac{c}{e})$-近似保证，能处理弱次模场景；\texttt{MA-MPL}算法在无参数情况下保持相同近似性能。模拟结果验证了算法有效性。

Conclusion: 本文提出的基于策略连续扩展的算法有效应对多智能体在线协调中的弱次模目标问题，兼顾理论性能和实际应用，无需参数更具实用价值。

Abstract: In this paper, we present two effective policy learning algorithms for
multi-agent online coordination(MA-OC) problem. The first one, \texttt{MA-SPL},
not only can achieve the optimal $(1-\frac{c}{e})$-approximation guarantee for
the MA-OC problem with submodular objectives but also can handle the unexplored
$\alpha$-weakly DR-submodular and $(\gamma,\beta)$-weakly submodular scenarios,
where $c$ is the curvature of the investigated submodular functions, $\alpha$
denotes the diminishing-return(DR) ratio and the tuple $(\gamma,\beta)$
represents the submodularity ratios. Subsequently, in order to reduce the
reliance on the unknown parameters $\alpha,\gamma,\beta$ inherent in the
\texttt{MA-SPL} algorithm, we further introduce the second online algorithm
named \texttt{MA-MPL}. This \texttt{MA-MPL} algorithm is entirely
\emph{parameter-free} and simultaneously can maintain the same approximation
ratio as the first \texttt{MA-SPL} algorithm. The core of our \texttt{MA-SPL}
and \texttt{MA-MPL} algorithms is a novel continuous-relaxation technique
termed as \emph{policy-based continuous extension}. Compared with the
well-established \emph{multi-linear extension}, a notable advantage of this new
\emph{policy-based continuous extension} is its ability to provide a lossless
rounding scheme for any set function, thereby enabling us to tackle the
challenging weakly submodular objectives. Finally, extensive simulations are
conducted to validate the effectiveness of our proposed algorithms.

</details>


### [116] [Voting-Bloc Entropy: A New Metric for DAO Decentralization](https://arxiv.org/abs/2509.22620)
*Andrés Fábrega,Amy Zhao,Jay Yu,James Austgen,Sarah Allen,Kushal Babel,Mahimna Kelkar,Ari Juels*

Main category: cs.MA

TL;DR: 本文提出了一种新的DAO去中心化测量框架——投票集团熵（VBE），通过衡量投票者利益函数的相似性，更准确地反映多样和平等的参与。


<details>
  <summary>Details</summary>
Motivation: 现有DAO去中心化定义无法全面捕捉去中心化中多样和平等参与的关键属性。

Method: 基于强化学习的投票概念模型，提出了测量参与者利益相似性的VBE指标，理论推导其合理性和实用性。

Result: 证明了投票委托、提案捆绑、贿赂等因素对去中心化的影响，提出了提升DAO去中心化的实际建议，并通过实证研究和实验验证了VBE的有效性。

Conclusion: VBE提供了一种理论与实践兼具的DAO去中心化度量工具，为未来DAO研究和治理改进提供了有力支持。

Abstract: Decentralized Autonomous Organizations (DAOs) use smart contracts to foster
communities working toward common goals. Existing definitions of
decentralization, however -- the 'D' in DAO -- fall short of capturing the key
properties characteristic of diverse and equitable participation. This work
proposes a new framework for measuring DAO decentralization called Voting-Bloc
Entropy (VBE, pronounced ''vibe''). VBE is based on the idea that voters with
closely aligned interests act as a centralizing force and should be modeled as
such. VBE formalizes this notion by measuring the similarity of participants'
utility functions across a set of voting rounds. Unlike prior, ad hoc
definitions of decentralization, VBE derives from first principles: We
introduce a simple (yet powerful) reinforcement learning-based conceptual model
for voting, that in turn implies VBE. We first show VBE's utility as a
theoretical tool. We prove a number of results about the (de)centralizing
effects of vote delegation, proposal bundling, bribery, etc. that are
overlooked in previous notions of DAO decentralization. Our results lead to
practical suggestions for enhancing DAO decentralization. We also show how VBE
can be used empirically by presenting measurement studies and VBE-based
governance experiments. We make the tools we developed for these results
available to the community in the form of open-source artifacts in order to
facilitate future study of DAO decentralization.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [117] [Extracting Conceptual Knowledge to Locate Software Issues](https://arxiv.org/abs/2509.21427)
*Ying Wang,Wenjun Mao,Chong Wang,Zhenhao Zhou,Yicheng Zhou,Wenyun Zhao,Yiling Lou,Xin Peng*

Main category: cs.SE

TL;DR: 提出了RepoLens方法，通过抽象代码库中的概念知识，提升了大规模代码库中问题定位的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于大型语言模型的方法在大规模代码库中效率不佳，主要因代码逻辑混杂和分散，导致定位困难。

Method: RepoLens将细粒度功能抽象为高层次关注点，分两阶段构建知识库并在线检索相关关注点，增强定位提示，从而辅助问题定位。

Result: 在SWE-Lancer-Loc基准上，RepoLens显著提升了三款工具（AgentLess、OpenHands、mini-SWE-agent）的文件和函数级定位准确率，且对多种模型均有良好泛化效果。

Conclusion: RepoLens通过构建语义连贯的关注点簇，有效缓解了代码混杂和分散问题，显著提升了基于LLM的问题定位性能和可靠性。

Abstract: Issue localization, which identifies faulty code elements such as files or
functions, is critical for effective bug fixing. While recent LLM-based and
LLM-agent-based approaches improve accuracy, they struggle in large-scale
repositories due to concern mixing, where relevant logic is buried in large
functions, and concern scattering, where related logic is dispersed across
files.
  To address these challenges, we propose RepoLens, a novel approach that
abstracts and leverages conceptual knowledge from code repositories. RepoLens
decomposes fine-grained functionalities and recomposes them into high-level
concerns, semantically coherent clusters of functionalities that guide LLMs. It
operates in two stages: an offline stage that extracts and enriches conceptual
knowledge into a repository-wide knowledge base, and an online stage that
retrieves issue-specific terms, clusters and ranks concerns by relevance, and
integrates them into localization workflows via minimally intrusive prompt
enhancements. We evaluate RepoLens on SWE-Lancer-Loc, a benchmark of 216 tasks
derived from SWE-Lancer. RepoLens consistently improves three state-of-the-art
tools, namely AgentLess, OpenHands, and mini-SWE-agent, achieving average gains
of over 22% in Hit@k and 46% in Recall@k for file- and function-level
localization. It generalizes across models (GPT-4o, GPT-4o-mini, GPT-4.1) with
Hit@1 and Recall@10 gains up to 504% and 376%, respectively. Ablation studies
and manual evaluation confirm the effectiveness and reliability of the
constructed concerns.

</details>


### [118] [Lost in Transition: The Struggle of Women Returning to Software Engineering Research after Career Breaks](https://arxiv.org/abs/2509.21533)
*Shalini Chakraborty,Sebastian Baltes*

Main category: cs.SE

TL;DR: 本文探讨女性软件工程师在经历职业中断后重返学术界的挑战，比较学术界与工业界的不同支持政策，并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 尽管IT行业有多种支持女性重返工作的项目，学术界相应的激励措施却有限，女性因怀孕、移民状态及缺乏灵活工作选择而面临更大职业复出障碍。

Method: 通过多国多所大学开展多元文化研究，分析女性在学术界和工业界重返工作岗位时遇到的具体挑战，并比较不同国家的相关政策和制度。

Result: 发现学术机构的性别多样性政策较工业界不显著且认可度低，不同国家和地区的政策支持差异明显，学术界女性职业复出面临较多结构性障碍。

Conclusion: 提出促进学术界女性透明招聘和职业回归支持的建议，旨在减轻职业中断对女性学术职业发展的负面影响，推动学术机构制定更具包容性的政策。

Abstract: The IT industry provides supportive pathways such as returnship programs,
coding boot camps, and buddy systems for women re-entering their job after a
career break. Academia, however, offers limited opportunities to motivate women
to return. We propose a diverse multicultural research project investigating
the challenges faced by women with software engineering (SE) backgrounds
re-entering academia or related research roles after a career break. Career
disruptions due to pregnancy, immigration status, or lack of flexible work
options can significantly impact women's career progress, creating barriers for
returning as lecturers, professors, or senior researchers. Although many
companies promote gender diversity policies, such measures are less prominent
and often under-recognized within academic institutions. Our goal is to explore
the specific challenges women encounter when re-entering academic roles
compared to industry roles; to understand the institutional perspective,
including a comparative analysis of existing policies and opportunities in
different countries for women to return to the field; and finally, to provide
recommendations that support transparent hiring practices. The research project
will be carried out in multiple universities and in multiple countries to
capture the diverse challenges and policies that vary by location.

</details>


### [119] [No More Manual Guides: Automatic and Scalable Generation of High-Quality Excel Tutorials](https://arxiv.org/abs/2509.21816)
*Yuhang Xie,Jian Mu,Xiaojun Ma,Chaoyun Zhang,Lu Wang,Mengyu Zhou,Mugeng Liu,Si Qin,Qingwei Lin,Saravan Rajmohan,Shi Han,Dongmei Zhang*

Main category: cs.SE

TL;DR: 本文提出了一种基于自然语言任务描述，自动生成Excel教程的框架，提高了执行成功率并显著减少人工成本。


<details>
  <summary>Details</summary>
Motivation: Excel功能复杂，用户需要大量教程支持，现有教程更新成本高且依赖专家手工制作，尚未实现完全自动化生成。

Method: 设计执行代理执行任务规划与操作，收集中间产物生成结构化Excel文档和视频演示；构建1,559个真实任务数据集；结合大语言模型与人工评审的系统化评价方法。

Result: 框架的任务执行成功率较最先进方法提升8.5%，教程在可读性和教学效果上接近甚至超过专家制作，且时间成本降至1/20。

Conclusion: 该自动化流程使高质量、可扩展的Excel教程生成成为可能，极大降低了人工劳动和时间成本。

Abstract: Excel is one of the most widely used productivity tools across domains,
offering rich functionality but also overwhelming users with its complexity.
This creates a persistent demand for tutorials to support effective usage.
However, existing tutorials are manually authored by experts, require frequent
updates after each software release, and incur substantial labor costs. Prior
work has not achieved fully automated tutorial generation, since existing
methods still depend on handcrafted operation sequences or example materials.
In this paper, we present the first framework for automatically generating
Excel tutorials directly from natural language task descriptions. Our framework
first instantiates the task. Then a central component of this framework,
Execution Agent, plans and executes the solution in Excel, and collects the
intermediate artifacts required for tutorial construction. These artifacts are
then transformed into both structured Excel documents and video demonstrations.
To build a comprehensive tutorial corpus, we collected 1,559 task descriptions
from real-world scenarios. In addition, we designed a systematic evaluation
framework that integrates assessments from both large language models (LLMs)
and human reviewers. Experimental results show that our framework improves task
execution success rates by 8.5% over state-of-the-art baselines. Moreover, the
generated tutorials demonstrate superior readability and instructional
effectiveness, often approaching or surpassing expert-authored materials.
Importantly, the automated pipeline eliminates manual labor and reduces time
costs to 1/20 of expert authoring, making scalable and high-quality tutorial
generation practical for the first time.

</details>


### [120] [Software Engineering Data Analytics: A Framework Based on a Multi-Layered Abstraction Mechanism](https://arxiv.org/abs/2509.21881)
*Chaman Wijesiriwardana,Prasad Wimalaratne*

Main category: cs.SE

TL;DR: 提出了一个面向领域的软件分析框架，用于查询、建模和整合异构软件仓库。


<details>
  <summary>Details</summary>
Motivation: 现有软件数据整合与分析面临异构数据源融合的挑战。

Method: 构建多层抽象机制，包含领域特定操作符，实现数据的统一查询和建模。

Result: 通过案例研究展示了该框架在软件仓库分析中的应用潜力。

Conclusion: 该框架有效支持了异构软件仓库的集成分析，提升了软件数据分析能力。

Abstract: This paper presents a concept of a domain-specific framework for software
analytics by enabling querying, modeling, and integration of heterogeneous
software repositories. The framework adheres to a multi-layered abstraction
mechanism that consists of domain-specific operators. We showcased the
potential of this approach by employing a case study.

</details>


### [121] [AgentPack: A Dataset of Code Changes, Co-Authored by Agents and Humans](https://arxiv.org/abs/2509.21891)
*Yangtian Zi,Zixuan Wu,Aleksander Boruch-Gruszecki,Jonathan Bell,Arjun Guha*

Main category: cs.SE

TL;DR: 本文介绍了AgentPack，一个包含130万条代码编辑数据集，数据由人类与软件工程代理共同编辑，数据质量较高，适合用于训练代码编辑模型。


<details>
  <summary>Details</summary>
Motivation: 传统基于代码提交的微调数据集普遍存在噪声，如提交信息简短且不明确，多编辑合在一条提交中，且很多提交来自规则型机器人，影响模型训练效果。

Method: 通过采集和整理由Claude Code、OpenAI Codex和Cursor Agent等软件工程代理与人类共同编辑的公开GitHub项目中的代码变更，构建高质量的AgentPack数据集，分析其结构特征及代理技术的采纳趋势，并用该数据集微调模型进行性能对比。

Result: 使用AgentPack数据集微调的模型在代码编辑任务中表现优于此前仅基于人类提交数据训练的模型，验证了该数据集的有效性和软件工程代理数据的潜力。

Conclusion: 利用软件工程代理与人类合作产生的高质量公开代码编辑数据，对训练更优秀的代码编辑模型具有显著帮助，未来代码编辑模型的训练可更多依赖此类代理生成的数据。

Abstract: Fine-tuning large language models for code editing has typically relied on
mining commits and pull requests. The working hypothesis has been that commit
messages describe human intent in natural language, and patches to code
describe the changes that implement that intent. However, much of the
previously collected data is noisy: commit messages are terse, human-written
commits commingle several unrelated edits, and many commits come from simple,
rule-based bots.
  The recent adoption of software engineering agents changes this landscape.
Code changes co-authored by humans and agents tend to be more narrowly scoped
and focused on clearer goals. Their commit messages, generated by LLMs,
articulate intent and rationale in much greater detail. Moreover, when these
changes land in public repositories, they are implicitly filtered by humans:
maintainers discard low-quality commits to their projects.
  We present AgentPack, a corpus of 1.3M code edits co-authored by Claude Code,
OpenAI Codex, and Cursor Agent across public GitHub projects up to mid-August
2025. We describe the identification and curation pipeline, quantify adoption
trends of these agents, and analyze the structural properties of the edits.
Finally, we show that models fine-tuned on AgentPack can outperform models
trained on prior human-only commit corpora, highlighting the potential of using
public data from software engineering agents to train future code-editing
models.

</details>


### [122] [Unveiling Many Faces of Surrogate Models for Configuration Tuning: A Fitness Landscape Analysis Perspective](https://arxiv.org/abs/2509.21945)
*Pengzhou Chen,Hongyuan Liang,Tao Chen*

Main category: cs.SE

TL;DR: 该论文针对系统配置调优中代理模型的作用进行了系统性探讨，提出了基于适应度景观分析的理论替代传统模型精度评价，并设计了自动预测工具Model4Tune以选择最佳模型-调优器组合。


<details>
  <summary>Details</summary>
Motivation: 现有方法普遍关注代理模型的预测精度，但高准确度不一定代表其调优效果，代理模型在调优中的具体角色尚不清晰，需要新的视角和评估标准。

Method: 引入适应度景观分析理论用于评估模型的实际调优价值，进行了多达27000个案例的广泛实证研究，并据此开发Model4Tune自动预测工具，避免昂贵的调优器性能剖析。

Result: 实验结果表明，Model4Tune在79%-82%的场景下显著优于随机选择，证明其有效性和实用性。

Conclusion: 本研究揭示了代理模型在系统配置调优中的多重作用，为未来研究提供方向，同时为实践者提供了一种评估和选择最佳模型-调优器组合的实用方案。

Abstract: To efficiently tune configuration for better system performance (e.g.,
latency), many tuners have leveraged a surrogate model to expedite the process
instead of solely relying on the profoundly expensive system measurement. As
such, it is naturally believed that we need more accurate models. However, the
fact of accuracy can lie-a somewhat surprising finding from prior work-has left
us many unanswered questions regarding what role the surrogate model plays in
configuration tuning. This paper provides the very first systematic exploration
and discussion, together with a resolution proposal, to disclose the many faces
of surrogate models for configuration tuning, through the novel perspective of
fitness landscape analysis. We present a theory as an alternative to accuracy
for assessing the model usefulness in tuning, based on which we conduct an
extensive empirical study involving up to 27,000 cases. Drawing on the above,
we propose Model4Tune, an automated predictive tool that estimates which
model-tuner pairs are the best for an unforeseen system without expensive tuner
profiling. Our results suggest that Moldel4Tune, as one of the first of its
kind, performs significantly better than random guessing in 79%-82% of the
cases. Our results not only shed light on the possible future research
directions but also offer a practical resolution that can assist practitioners
in evaluating the most useful model for configuration tuning.

</details>


### [123] [SecureAgentBench: Benchmarking Secure Code Generation under Realistic Vulnerability Scenarios](https://arxiv.org/abs/2509.22097)
*Junkai Chen,Huihui Huang,Yunbo Lyu,Junwen An,Jieke Shi,Chengran Yang,Ting Zhang,Haoye Tian,Yikun Li,Zhenhao Li,Xin Zhou,Xing Hu,David Lo*

Main category: cs.SE

TL;DR: 本文提出了SecureAgentBench，一个包含105个编码任务的基准，用于严格评估大型语言模型驱动的代码代理在安全代码生成中的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试忽视了漏洞产生的真实背景或评价协议过于狭窄，不能全面评估代码的功能正确性和安全性，导致安全风险成为紧迫问题。

Method: 设计包括多文件编辑的大型仓库真实任务环境，结合真实开源漏洞上下文，采用功能测试、漏洞验证及静态分析的综合评估方法，对三种代码代理和三款先进LLM进行评测。

Result: 结果显示当前代码代理难以生成安全代码，最佳代理仅15.2%的代码既正确又安全；部分代码虽功能正确但仍含新漏洞；明确的安全指令未显著提升安全性。

Conclusion: SecureAgentBench为安全代码生成提供了严谨的评测标准，指出当前技术不足并强调需要进一步研究以提升LLM驱动软件开发的安全性。

Abstract: Large language model (LLM) powered code agents are rapidly transforming
software engineering by automating tasks such as testing, debugging, and
repairing, yet the security risks of their generated code have become a
critical concern. Existing benchmarks have offered valuable insights but remain
insufficient: they often overlook the genuine context in which vulnerabilities
were introduced or adopt narrow evaluation protocols that fail to capture
either functional correctness or newly introduced vulnerabilities. We therefore
introduce SecureAgentBench, a benchmark of 105 coding tasks designed to
rigorously evaluate code agents' capabilities in secure code generation. Each
task includes (i) realistic task settings that require multi-file edits in
large repositories, (ii) aligned contexts based on real-world open-source
vulnerabilities with precisely identified introduction points, and (iii)
comprehensive evaluation that combines functionality testing, vulnerability
checking through proof-of-concept exploits, and detection of newly introduced
vulnerabilities using static analysis. We evaluate three representative agents
(SWE-agent, OpenHands, and Aider) with three state-of-the-art LLMs (Claude 3.7
Sonnet, GPT-4.1, and DeepSeek-V3.1). Results show that (i) current agents
struggle to produce secure code, as even the best-performing one, SWE-agent
supported by DeepSeek-V3.1, achieves merely 15.2% correct-and-secure solutions,
(ii) some agents produce functionally correct code but still introduce
vulnerabilities, including new ones not previously recorded, and (iii) adding
explicit security instructions for agents does not significantly improve secure
coding, underscoring the need for further research. These findings establish
SecureAgentBench as a rigorous benchmark for secure code generation and a step
toward more reliable software development with LLMs.

</details>


### [124] [SK2Decompile: LLM-based Two-Phase Binary Decompilation from Skeleton to Skin](https://arxiv.org/abs/2509.22114)
*Hanzhuo Tan,Weihao Li,Xiaolong Tian,Siyi Wang,Jiaming Liu,Jing Li,Yuqun Zhang*

Main category: cs.SE

TL;DR: 提出SK2Decompile，两阶段方法提升二进制反编译质量，先恢复结构再命名标识符。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的反编译器在恢复程序源代码结构及原始标识符方面存在不足。

Method: 采用两阶段：1）结构恢复模型将二进制转为保留控制流和数据结构的中间表示，同时用通用占位符替换标识符，并用强化学习优化；2）标识符命名模型通过强化学习产生语义相关的有意义标识符。

Result: SK2Decompile在HumanEval数据集上相比GPT-5-mini提升21.6%重新执行率，在GitHub2025基准测试上相比Idioms提高29.4% R2I指数。

Conclusion: 两阶段反编译过程独立提升反编译结果的正确性和可读性，效果显著优于现有方法。

Abstract: Large Language Models (LLMs) have emerged as a promising approach for binary
decompilation. However, the existing LLM-based decompilers still are somewhat
limited in effectively presenting a program's source-level structure with its
original identifiers. To mitigate this, we introduce SK2Decompile, a novel
two-phase approach to decompile from the skeleton (semantic structure) to the
skin (identifier) of programs. Specifically, we first apply a Structure
Recovery model to translate a program's binary code to an Intermediate
Representation (IR) as deriving the program's "skeleton", i.e., preserving
control flow and data structures while obfuscating all identifiers with generic
placeholders. We also apply reinforcement learning to reward the model for
producing program structures that adhere to the syntactic and semantic rules
expected by compilers. Second, we apply an Identifier Naming model to produce
meaningful identifiers which reflect actual program semantics as deriving the
program's "skin". We train the Identifier Naming model with a separate
reinforcement learning objective that rewards the semantic similarity between
its predictions and the reference code. Such a two-phase decompilation process
facilitates advancing the correctness and readability of decompilation
independently. Our evaluations indicate that SK2Decompile, significantly
outperforms the SOTA baselines, achieving 21.6% average re-executability rate
gain over GPT-5-mini on the HumanEval dataset and 29.4% average R2I improvement
over Idioms on the GitHub2025 benchmark.

</details>


### [125] [Leveraging LLM Agents for Automated Video Game Testing](https://arxiv.org/abs/2509.22170)
*Chengjia Wang,Lanling Tang,Ming Yuan,Jiongchi Yu,Xiaofei Xie,Jiajun Bu*

Main category: cs.SE

TL;DR: TITAN是一个基于大型语言模型的智能MMORPG自动测试框架，通过感知游戏状态、优化动作、长时推理及利用LLM诊断漏洞，显著提高测试效率和缺陷检测能力。


<details>
  <summary>Details</summary>
Motivation: 传统自动游戏测试方法在复杂、多变的MMORPG环境中难以达到高覆盖和高效率，现有基于LLM的游戏玩法还缺乏深入推理能力，无法应对复杂的游戏状态和长任务。

Method: 提出TITAN框架，包含四个核心组件：游戏状态感知与抽象、动作优先级优化、长时推理及自我修正、基于LLM的漏洞检测及报告。

Result: TITAN在两个大型商业MMORPG上测试，任务完成率达95%，明显优于现有方法；消除了之前测试未发现的四个未知漏洞，核心组件均显著提升性能，并已实际应用于八个游戏QA流程。

Conclusion: TITAN有效提升了MMORPG自动测试的任务完成率和缺陷检测率，验证了基于大型语言模型的智能测试框架的实用性和潜力，为未来智能化通用测试系统的发展提供了新思路。

Abstract: Testing MMORPGs (Massively Multiplayer Online Role-Playing Games) is a
critical yet labor-intensive task in game development due to their complexity
and frequent updating nature. Traditional automated game testing approaches
struggle to achieve high state coverage and efficiency in these rich,
open-ended environments, while existing LLM-based game-playing approaches are
limited to shallow reasoning ability in understanding complex game state-action
spaces and long-complex tasks. To address these challenges, we propose TITAN,
an effective LLM-driven agent framework for intelligent MMORPG testing. TITAN
incorporates four key components to: (1) perceive and abstract high-dimensional
game states, (2) proactively optimize and prioritize available actions, (3)
enable long-horizon reasoning with action trace memory and reflective
self-correction, and (4) employ LLM-based oracles to detect potential
functional and logic bugs with diagnostic reports.
  We implement the prototype of TITAN and evaluate it on two large-scale
commercial MMORPGs spanning both PC and mobile platforms. In our experiments,
TITAN achieves significantly higher task completion rates (95%) and bug
detection performance compared to existing automated game testing approaches.
An ablation study further demonstrates that each core component of TITAN
contributes substantially to its overall performance. Notably, TITAN detects
four previously unknown bugs that prior testing approaches fail to identify. We
provide an in-depth discussion of these results, which offer guidance for new
avenues of advancing intelligent, general-purpose testing systems. Moreover,
TITAN has been deployed in eight real-world game QA pipelines, underscoring its
practical impact as an LLM-driven game testing framework.

</details>


### [126] [Library Hallucinations in LLMs: Risk Analysis Grounded in Developer Queries](https://arxiv.org/abs/2509.22202)
*Lukas Twist,Jie M. Zhang,Mark Harman,Helen Yannakoudakis*

Main category: cs.SE

TL;DR: 本文系统性研究了大语言模型（LLMs）生成代码时因提示词变化引起的库幻觉问题，发现轻微拼写错误和虚假库名均能大量引发幻觉，提示工程方法虽有效但不稳定。


<details>
  <summary>Details</summary>
Motivation: 尽管库幻觉问题风险增加，现实中不同用户提示对幻觉率的影响仍缺乏系统研究，且这类幻觉可能导致安全风险。

Method: 对六种不同大语言模型进行评估，考察真实开发者论坛提取的提示词及不同类型的输入错误（如单字符错拼、多字符错拼、虚假库名和成员名）对库名和库成员幻觉的影响。

Result: 发现单字符拼写错导致多达26%的任务出现库幻觉，虚假库被99%任务接受，时间相关提示导致幻觉达84%，且提示工程虽能缓解幻觉但效果不稳定且依赖模型。

Conclusion: 大语言模型对自然提示的变化极其脆弱，需要紧急开发防范库相关幻觉及潜在滥用的措施，保障生成代码的安全可靠。

Abstract: Large language models (LLMs) are increasingly used to generate code, yet they
continue to hallucinate, often inventing non-existent libraries. Such library
hallucinations are not just benign errors: they can mislead developers, break
builds, and expose systems to supply chain threats such as slopsquatting.
Despite increasing awareness of these risks, little is known about how
real-world prompt variations affect hallucination rates. Therefore, we present
the first systematic study of how user-level prompt variations impact library
hallucinations in LLM-generated code. We evaluate six diverse LLMs across two
hallucination types: library name hallucinations (invalid imports) and library
member hallucinations (invalid calls from valid libraries). We investigate how
realistic user language extracted from developer forums and how user errors of
varying degrees (one- or multi-character misspellings and completely fake
names/members) affect LLM hallucination rates. Our findings reveal systemic
vulnerabilities: one-character misspellings in library names trigger
hallucinations in up to 26% of tasks, fake library names are accepted in up to
99% of tasks, and time-related prompts lead to hallucinations in up to 84% of
tasks. Prompt engineering shows promise for mitigating hallucinations, but
remains inconsistent and LLM-dependent. Our results underscore the fragility of
LLMs to natural prompt variation and highlight the urgent need for safeguards
against library-related hallucinations and their potential exploitation.

</details>


### [127] [Green Prompt Engineering: Investigating the Energy Impact of Prompt Design in Software Engineering](https://arxiv.org/abs/2509.22320)
*Vincenzo De Martino,Mohammad Amin Zadenoori,Xavier Franch,Alessio Ferrari*

Main category: cs.SE

TL;DR: 本文提出绿色提示工程，研究提示语言复杂度对语言模型能耗和性能的影响。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型在软件工程中的应用增多，但其推理过程带来的环境影响被忽视，尤其是语言复杂度这一因素。

Method: 通过实验改变开源小型语言模型中需求分类任务提示的可读性，评估其对能耗和性能的影响。

Result: 实验结果显示提示的可读性影响环境可持续性和性能，存在两者间的权衡。

Conclusion: 对于实践者，简化提示可减少能耗且性能损失不大；对于研究者，提出了绿色AI框架下可持续提示设计的研究方向。

Abstract: Language Models are increasingly applied in software engineering, yet their
inference raises growing environmental concerns. Prior work has examined
hardware choices and prompt length, but little attention has been paid to
linguistic complexity as a sustainability factor. This paper introduces Green
Prompt Engineering, framing linguistic complexity as a design dimension that
can influence energy consumption and performance. We conduct an empirical study
on requirement classification using open-source Small Language Models, varying
the readability of prompts. Our results reveal that readability affects
environmental sustainability and performance, exposing trade-offs between them.
For practitioners, simpler prompts can reduce energy costs without a
significant F1-score loss; for researchers, it opens a path toward guidelines
and studies on sustainable prompt design within the Green AI agenda.

</details>


### [128] [GPU-Accelerated Loopy Belief Propagation for Program Analysis](https://arxiv.org/abs/2509.22337)
*Haoyu Feng,Xin Zhang*

Main category: cs.SE

TL;DR: 本文提出了一种用于程序分析的GPU加速的Loopy Belief Propagation算法，支持灵活的更新策略和逻辑约束结合，显著提升大规模程序分析的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的LBP算法在大规模程序分析中计算复杂度高，且现有GPU加速方法缺乏灵活更新策略支持，且未结合逻辑约束，导致性能不佳。

Method: 提出统一表示任意用户定义的更新策略及依赖分析算法，结合Horn子句的局部结构优化消息传递，进行消息分组以减少GPU的warp分歧，提高资源利用。

Result: 在八个真实Java程序的数据竞争分析中，所提方法相比最先进的顺序方法平均加速2.14倍，相比最先进的GPU方法加速5.56倍，同时保持高准确率。

Conclusion: 该GPU加速的LBP算法有效支持多样化更新策略和逻辑约束结合，显著提升了大规模程序分析的效率和实用性能。

Abstract: Loopy Belief Propagation (LBP) is a widely used approximate inference
algorithm in probabilistic graphical models, with applications in computer
vision, error correction codes, protein folding, program analysis, etc.
However, LBP faces significant computational challenges when applied to
large-scale program analysis. While GPU (Graphics Processing Unit) parallel
computing provides a promising solution, existing approaches lack support for
flexible update strategies and have yet to integrate logical constraints with
GPU acceleration, leading to suboptimal practical performance.
  This paper presents a GPU-accelerated LBP algorithm for program analysis. To
support the diverse update strategies required by users, we propose a unified
representation for specifying arbitrary user-defined update strategies, along
with a dependency analysis algorithm. Furthermore, building on previous work
that leverages the local structure of Horn clauses to simplify message passing,
we group messages to minimize warp divergence and better utilize GPU resources.
Experimental results on datarace analysis over eight real-world Java programs
show that our approach achieves an average speedup of $2.14\times$ over the
state-of-the-art sequential approach and $5.56\times$ over the state-of-the-art
GPU-based approach, while maintaining high accuracy.

</details>


### [129] [A Multi-Modality Evaluation of the Reality Gap in Autonomous Driving Systems](https://arxiv.org/abs/2509.22379)
*Stefano Carlo Lambertenghi,Mirena Flores Valdez,Andrea Stocco*

Main category: cs.SE

TL;DR: 本文对自动驾驶系统的四种测试方式进行了比较研究，分析了各自与现实世界的差距及影响，提出了改进验证方法的路径。


<details>
  <summary>Details</summary>
Motivation: 面对模拟测试与现实世界表现存在的差距（现实差距），寻找更加可靠且可迁移的自动驾驶系统验证方法。

Method: 使用带有真实传感器的小型物理车辆及其数字双胞胎，分别搭建SiL、ViL、混合现实及实车测试环境，评估两种自动驾驶架构在多样化室内驾驶场景中的表现。

Result: SiL和ViL简化了真实动态和感知，混合现实改善感知真实性且保障安全和控制，明确了不同测试方式失败转移的条件及其对应的现实差距维度。

Conclusion: 每种测试方式各有优缺点，理解现实差距的关键维度有助于设计更加稳健且可迁移的自动驾驶系统测试策略。

Abstract: Simulation-based testing is a cornerstone of Autonomous Driving System (ADS)
development, offering safe and scalable evaluation across diverse driving
scenarios. However, discrepancies between simulated and real-world behavior,
known as the reality gap, challenge the transferability of test results to
deployed systems. In this paper, we present a comprehensive empirical study
comparing four representative testing modalities: Software-in-the-Loop (SiL),
Vehicle-in-the-Loop (ViL), Mixed-Reality (MR), and full real-world testing.
Using a small-scale physical vehicle equipped with real sensors (camera and
LiDAR) and its digital twin, we implement each setup and evaluate two ADS
architectures (modular and end-to-end) across diverse indoor driving scenarios
involving real obstacles, road topologies, and indoor environments. We
systematically assess the impact of each testing modality along three
dimensions of the reality gap: actuation, perception, and behavioral fidelity.
Our results show that while SiL and ViL setups simplify critical aspects of
real-world dynamics and sensing, MR testing improves perceptual realism without
compromising safety or control. Importantly, we identify the conditions under
which failures do not transfer across testing modalities and isolate the
underlying dimensions of the gap responsible for these discrepancies. Our
findings offer actionable insights into the respective strengths and
limitations of each modality and outline a path toward more robust and
transferable validation of autonomous driving systems.

</details>


### [130] [Context-Specific Instruction: A Longitudinal Study on Debugging Skill Acquisition and Retention for Novice Programmers](https://arxiv.org/abs/2509.22420)
*Ziyi Zhang,Devjeet Roy,Venera Arnaoudova*

Main category: cs.SE

TL;DR: 本文通过一项为期八周的纵向研究，比较了无指导、抽象指导、具体步骤指导和结合具体步骤与问题细节的情境指导四种模式对初学者软件缺陷定位能力的影响。


<details>
  <summary>Details</summary>
Motivation: 初学者在缺陷定位上缺乏系统方法，现有研究对情境指导的效果尚不明确。

Method: 招募44名本科生，分为四组，进行五轮多次缺陷定位任务，测量正确率、完成时间、自我感受等指标。

Result: 结合具体步骤和问题细节的情境指导组（G4）在正确率、完成时间和心理指标上均显著优于其他组，且表现稳定。

Conclusion: 情境指导相比抽象或非情境指导能更快地促进技能习得和保持，结合情境示例与抽象原则有助弥合理论与实践的差距，提高初学者的学习效果。

Abstract: Bug localization is a critical skill, yet novices often lack systematic
approaches. Prior work tested abstract guidelines and general concrete steps;
the impact of context-specific instruction is unclear. We ran an eight-week
longitudinal study with four conditions: no instruction (G1), abstract
guidelines (G2), concrete steps (G3), and our context-specific instruction that
pairs concrete bug-localization steps with problem-specific details (G4).
Forty-four undergraduates participated; 41 completed all five sessions (S1-S5).
Each session included 2-3 debugging tasks to identify the minimal code element
containing a seeded logical fault. We measured correctness (binary), time to
completion, self-perceived scores (stress, difficulty, satisfaction, and
strategy adherence). G4 achieved higher correctness and shorter time to
completion: it reached 80% correctness after one session (vs. 20-44% for other
groups) and maintained 80% after three weeks, outperforming all groups (p <
0.05); its time to completion stabilized at 13-15 minutes in S1, whereas other
groups took 2-3 sessions to stabilize at 22-27 minutes. Qualitative responses
showed lower stress and higher satisfaction in G4, with participants
internalizing strategies via contextual examples. We conclude that
context-specific instruction yields faster skill acquisition and stronger
retention than abstract guidelines or context-agnostic steps. Even 1-2 sessions
produced significant gains, while extended practice optimized and stabilized
performance. Integrating contextual examples with abstract principles may
bridge theory-practice gaps in bug-localization education and provide a more
equitable path for novices.

</details>


### [131] [TreeMind: Automatically Reproducing Android Bug Reports via LLM-empowered Monte Carlo Tree Search](https://arxiv.org/abs/2509.22431)
*Zhengyu Chen,Zhaoyi Meng,Wenxiang Zhao,Wansen Wang,Haoyang Zhao,Jiahao Zhan,Jie Cui,Hong Zhong*

Main category: cs.SE

TL;DR: 提出TreeMind技术，结合大语言模型和蒙特卡洛树搜索，实现自动重现Android应用崩溃，提高bug重现成功率。


<details>
  <summary>Details</summary>
Motivation: 自动重现Android应用崩溃困难，尤其是报告不完整和UI复杂，传统方法推理和规划能力有限。

Method: 将重现任务转化为目标驱动搜索，利用MCTS进行迭代规划，结合两个大语言模型引导的代理辅助拓展和模拟动作。

Result: 在三个基准数据集上的93个真实bug报告中，TreeMind重现成功率显著超越四个先进基线。

Conclusion: 融合大语言模型语义推理与MCTS规划的TreeMind技术为自动重现Bug提供了有效新方向。

Abstract: Automatically reproducing Android app crashes from textual bug reports is
challenging, particularly when the reports are incomplete and the modern UI
exhibits high combinatorial complexity. Existing approaches based on
reinforcement learning or large language models (LLMs) exhibit limitations in
such scenarios. They struggle to infer unobserved steps and reconstruct the
underlying user action sequences to navigate the vast UI interaction space,
primarily due to limited goal-directed reasoning and planning. We present
TreeMind, a novel technique that integrates LLMs with a customized Monte Carlo
Tree Search (MCTS) algorithm to achieve strategic UI exploration in bug
reproduction. To the best of our knowledge, this is the first work to combine
external decision-making with LLM semantic reasoning for reliable bug
reproduction. We formulate the reproduction task as a target-driven search
problem, leveraging MCTS as the core planning mechanism to iteratively refine
action sequences. To enhance MCTS with semantic reasoning, we introduce two
LLM-guided agents with distinct roles: Expander generates top-k promising
actions based on the current UI state and exploration history, while Simulator
estimates the likelihood that each action leads toward successful reproduction.
By incorporating multi-modal UI inputs and advanced prompting techniques,
TreeMind conducts feedback-aware navigation that identifies missing but
essential user actions and incrementally reconstructs the reproduction paths.
We evaluate TreeMind on a dataset of 93 real-world Android bug reports from
three widely-used benchmarks. Experimental results show that it significantly
outperforms four state-of-the-art baselines in reproduction success rate. A
real-world case study indicates that integrating LLM reasoning with MCTS-based
planning is a compelling direction for automated bug reproduction.

</details>


### [132] [Boosting Pointer Analysis With Large Language Model-Enhanced Allocation Function Detection](https://arxiv.org/abs/2509.22530)
*Baijun Cheng,Kailong Wang,Ling Shi,Haoyu Wang,Peng Di,Yao Guo,Ding Li,Xiangqun Chen*

Main category: cs.SE

TL;DR: 本文提出AFD技术，通过自动识别和建模自定义分配函数，显著提升C/C++指针分析的精度和效果。


<details>
  <summary>Details</summary>
Motivation: 现有指针分析方法忽视自定义分配函数，导致堆对象建模粗糙，分析精度受限。

Method: AFD结合值流分析和大型语言模型分别识别简单和复杂的自定义分配函数，实现精确堆对象建模，具备类似上下文敏感性的效果。

Result: 在15个真实C项目中识别600多个自定义分配函数，模型堆对象提升26倍，别名集合缩小39%，仅增加1.4倍运行时开销，并发现17个新增内存缺陷。

Conclusion: 精确建模自定义分配函数是提升大规模软件指针分析精度的有效且可扩展方案。

Abstract: Pointer analysis is foundational for many static analysis tasks, yet its
effectiveness is often hindered by imprecise modeling of heap allocations,
particularly in C/C++ programs where user-defined allocation functions (AFs)
are pervasive. Existing approaches largely overlook these custom allocators,
leading to coarse aliasing and reduced analysis precision. In this paper, we
present AFD, a novel technique that enhances pointer analysis by automatically
identifying and modeling custom allocation functions. AFD employs a hybrid
approach: it uses value-flow analysis to detect straightforward wrappers and
leverages Large Language Models (LLMs) to reason about more complex allocation
patterns with side effects. This targeted enhancement enables precise modeling
of heap objects at each call site, achieving context-sensitivity-like benefits
without the associated overhead. We evaluate AFD on 15 real-world C projects,
identifying over 600 custom AFs. Integrating AFD into a baseline pointer
analysis yields a 26x increase in modeled heap objects and a 39% reduction in
alias set sizes, with only 1.4x runtime overhead. Furthermore, our enhanced
analysis improves indirect call resolution and uncovers 17 previously
undetected memory bugs. These results demonstrate that precise modeling of
custom allocation functions offers a scalable and practical path to improving
pointer analysis in large software systems.

</details>
