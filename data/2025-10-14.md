<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 156]
- [cs.SE](#cs.SE) [Total: 30]
- [cs.MA](#cs.MA) [Total: 9]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Table Question Answering in the Era of Large Language Models: A Comprehensive Survey of Tasks, Methods, and Evaluation](https://arxiv.org/abs/2510.09671)
*Wei Zhou,Bolei Ma,Annemarie Friedrich,Mohsen Mesgar*

Main category: cs.CL

TL;DR: 本文系统综述了表格问答（TQA）研究，聚焦基于大语言模型的方法，分类现有基准和任务设置，分析模型策略及其优缺点，并指出了未充分探索的研究方向，指导未来发展。


<details>
  <summary>Details</summary>
Motivation: 尽管基于大语言模型的表格问答已取得进展，但该领域缺乏对任务形式、核心挑战及方法趋势的系统理解，特别是在强化学习等新兴方向上。

Method: 本文通过分类现有基准和任务设置，归纳模型针对的挑战和策略，全面分析其优势和局限，梳理并统一不同研究线索，指出未来研究热点。

Result: 建立了表格问答领域的系统框架，明确模型策略的适用情境与不足，揭示未被充分关注的重要问题，为社区提供了统一的理解基础。

Conclusion: 通过综述当前TQA的研究现状和挑战，本文促进了整个领域的整合与发展，为后续研究指明方向，推动表格问答技术的深化和应用拓展。

Abstract: Table Question Answering (TQA) aims to answer natural language questions
about tabular data, often accompanied by additional contexts such as text
passages. The task spans diverse settings, varying in table representation,
question/answer complexity, modality involved, and domain. While recent
advances in large language models (LLMs) have led to substantial progress in
TQA, the field still lacks a systematic organization and understanding of task
formulations, core challenges, and methodological trends, particularly in light
of emerging research directions such as reinforcement learning. This survey
addresses this gap by providing a comprehensive and structured overview of TQA
research with a focus on LLM-based methods. We provide a comprehensive
categorization of existing benchmarks and task setups. We group current
modeling strategies according to the challenges they target, and analyze their
strengths and limitations. Furthermore, we highlight underexplored but timely
topics that have not been systematically covered in prior research. By unifying
disparate research threads and identifying open problems, our survey offers a
consolidated foundation for the TQA community, enabling a deeper understanding
of the state of the art and guiding future developments in this rapidly
evolving area.

</details>


### [2] [Emotionally Charged, Logically Blurred: AI-driven Emotional Framing Impairs Human Fallacy Detection](https://arxiv.org/abs/2510.09695)
*Yanran Chen,Lynn Greschner,Roman Klinger,Michael Klenk,Steffen Eger*

Main category: cs.CL

TL;DR: 研究利用大型语言模型探讨情感框架如何影响逻辑谬误的说服力，发现情感框架降低人类识别谬误的能力，并且不同情感影响说服力程度不同。


<details>
  <summary>Details</summary>
Motivation: 逻辑谬误在公共传播中常见且具误导性，且说服力具有主观性，需研究情感表达如何与谬误及说服力交互。

Method: 采用八种大型语言模型在保留逻辑结构的同时注入情感诉求，筛选最佳模型生成刺激材料，并开展人类实验评估谬误检测和说服力。

Result: 情感框架使人类谬误检测F1值平均下降14.5%；快乐情感提升谬误检测能力，恐惧和悲伤情感使说服力显著提升。

Conclusion: AI驱动的情感操控能影响逻辑谬误的识别和说服效果，提示在谬误论证中情感操控的重要性和风险。

Abstract: Logical fallacies are common in public communication and can mislead
audiences; fallacious arguments may still appear convincing despite lacking
soundness, because convincingness is inherently subjective. We present the
first computational study of how emotional framing interacts with fallacies and
convincingness, using large language models (LLMs) to systematically change
emotional appeals in fallacious arguments. We benchmark eight LLMs on injecting
emotional appeal into fallacious arguments while preserving their logical
structures, then use the best models to generate stimuli for a human study. Our
results show that LLM-driven emotional framing reduces human fallacy detection
in F1 by 14.5% on average. Humans perform better in fallacy detection when
perceiving enjoyment than fear or sadness, and these three emotions also
correlate with significantly higher convincingness compared to neutral or other
emotion states. Our work has implications for AI-driven emotional manipulation
in the context of fallacious argumentation.

</details>


### [3] [The Idola Tribus of AI: Large Language Models tend to perceive order where none exists](https://arxiv.org/abs/2510.09709)
*Shin-nosuke Ishikawa,Masato Todo,Taiki Ogihara,Hirotsugu Ohba*

Main category: cs.CL

TL;DR: 大语言模型（LLMs）在识别数字序列规律的简单任务中，存在生成荒谬模式的倾向，即使这些模式明显不合适。


<details>
  <summary>Details</summary>
Motivation: 为了评估LLMs在逻辑一致性和自我连贯性方面的表现及其潜在局限，鉴于其在复杂任务中依赖这些特性。

Method: 通过让LLMs解释不同整数序列的规律，包括算术序列和随机生成的序列，观察其识别模式的准确性。

Result: 模型在算术和几何序列中表现良好，但对随机序列经常过度识别不合理的模式，且此问题存在于多种多步推理模型中。

Conclusion: LLMs易于识别不存在的模式，显示其在逻辑推理上的潜在局限，即使使用链式思考机制也难以完全避免该问题。

Abstract: We present a tendency of large language models (LLMs) to generate absurd
patterns despite their clear inappropriateness in a simple task of identifying
regularities in number series. Several approaches have been proposed to apply
LLMs to complex real-world tasks, such as providing knowledge through
retrieval-augmented generation and executing multi-step tasks using AI agent
frameworks. However, these approaches rely on the logical consistency and
self-coherence of LLMs, making it crucial to evaluate these aspects and
consider potential countermeasures. To identify cases where LLMs fail to
maintain logical consistency, we conducted an experiment in which LLMs were
asked to explain the patterns in various integer sequences, ranging from
arithmetic sequences to randomly generated integer series. While the models
successfully identified correct patterns in arithmetic and geometric sequences,
they frequently over-recognized patterns that were inconsistent with the given
numbers when analyzing randomly generated series. This issue was observed even
in multi-step reasoning models, including OpenAI o3, o4-mini, and Google Gemini
2.5 Flash Preview Thinking. This tendency to perceive non-existent patterns can
be interpreted as the AI model equivalent of Idola Tribus and highlights
potential limitations in their capability for applied tasks requiring logical
reasoning, even when employing chain-of-thought reasoning mechanisms.

</details>


### [4] [SeCon-RAG: A Two-Stage Semantic Filtering and Conflict-Free Framework for Trustworthy RAG](https://arxiv.org/abs/2510.09710)
*Xiaonan Si,Meilin Zhu,Simeng Qin,Lijia Yu,Lijun Zhang,Shuaitong Liu,Xinfeng Li,Ranjie Duan,Yang Liu,Xiaojun Jia*

Main category: cs.CL

TL;DR: 本文提出了一种名为SeCon-RAG的两阶段语义过滤与冲突消除框架，用以提升检索增强生成（RAG）系统的可靠性，通过实体-意图-关系抽取器（EIRE）实现文档筛选和语义一致性过滤，有效防止语料污染和内容冲突，显著提升生成模型的鲁棒性和可信度。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统易受到语料库投毒和污染攻击，现有防御方法过滤过于激进，导致有价值信息流失和生成质量下降。

Method: 提出两阶段过滤策略：第一阶段结合语义和聚类过滤，利用EIRE提取实体、潜在目标及实体关系，筛选相关文档构建干净数据库；第二阶段通过EIRE引导的冲突感知过滤模块，分析查询、候选答案与检索知识间语义一致性，剔除内部和外部矛盾信息。

Result: SeCon-RAG在多种大语言模型和数据集上进行广泛实验证明，显著优于现有最先进的防御方法，在保持有用知识的同时，有效缓解了冲突污染，提高生成的鲁棒性和可信度。

Conclusion: SeCon-RAG提供了一种有效防护策略，通过细致的语义和一致性过滤，增强了RAG系统面对语料污染的防御能力，实现了更可靠和可信的文本生成。

Abstract: Retrieval-augmented generation (RAG) systems enhance large language models
(LLMs) with external knowledge but are vulnerable to corpus poisoning and
contamination attacks, which can compromise output integrity. Existing defenses
often apply aggressive filtering, leading to unnecessary loss of valuable
information and reduced reliability in generation. To address this problem, we
propose a two-stage semantic filtering and conflict-free framework for
trustworthy RAG. In the first stage, we perform a joint filter with semantic
and cluster-based filtering which is guided by the Entity-intent-relation
extractor (EIRE). EIRE extracts entities, latent objectives, and entity
relations from both the user query and filtered documents, scores their
semantic relevance, and selectively adds valuable documents into the clean
retrieval database. In the second stage, we proposed an EIRE-guided
conflict-aware filtering module, which analyzes semantic consistency between
the query, candidate answers, and retrieved knowledge before final answer
generation, filtering out internal and external contradictions that could
mislead the model. Through this two-stage process, SeCon-RAG effectively
preserves useful knowledge while mitigating conflict contamination, achieving
significant improvements in both generation robustness and output
trustworthiness. Extensive experiments across various LLMs and datasets
demonstrate that the proposed SeCon-RAG markedly outperforms state-of-the-art
defense methods.

</details>


### [5] [ReaLM: Residual Quantization Bridging Knowledge Graph Embeddings and Large Language Models](https://arxiv.org/abs/2510.09711)
*Wenbin Guo,Xin Wang,Jiaoyan Chen,Lingbing Guo,Zhao Li,Zirui Chen*

Main category: cs.CL

TL;DR: 本文提出ReaLM框架，通过残差向量量化将知识图嵌入离散化，并作为可学习token融入大语言模型词汇表，实现符号知识与上下文知识的融合。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的方法难以有效利用结构化语义表示，主要因预训练知识图模型的连续嵌入空间与大语言模型的离散token空间不匹配，影响语义传递和模型性能。

Method: ReaLM采用残差向量量化技术将预训练知识图嵌入转为紧凑的代码序列，作为可学习token纳入大语言模型词汇，结合本体引导的类别约束以保持语义一致性，提升实体预测的准确性。

Result: 在两个广泛使用的基准数据集上，ReaLM表现出领先的性能，验证了其在结构化知识与大规模语言模型对齐上的有效性。

Conclusion: ReaLM成功桥接知识图嵌入和大语言模型token空间的差异，实现了符号和上下文知识的融合，显著提升知识图补全任务的表现。

Abstract: Large Language Models (LLMs) have recently emerged as a powerful paradigm for
Knowledge Graph Completion (KGC), offering strong reasoning and generalization
capabilities beyond traditional embedding-based approaches. However, existing
LLM-based methods often struggle to fully exploit structured semantic
representations, as the continuous embedding space of pretrained KG models is
fundamentally misaligned with the discrete token space of LLMs. This
discrepancy hinders effective semantic transfer and limits their performance.
To address this challenge, we propose ReaLM, a novel and effective framework
that bridges the gap between KG embeddings and LLM tokenization through the
mechanism of residual vector quantization. ReaLM discretizes pretrained KG
embeddings into compact code sequences and integrates them as learnable tokens
within the LLM vocabulary, enabling seamless fusion of symbolic and contextual
knowledge. Furthermore, we incorporate ontology-guided class constraints to
enforce semantic consistency, refining entity predictions based on class-level
compatibility. Extensive experiments on two widely used benchmark datasets
demonstrate that ReaLM achieves state-of-the-art performance, confirming its
effectiveness in aligning structured knowledge with large-scale language
models.

</details>


### [6] [All Code, No Thought: Current Language Models Struggle to Reason in Ciphered Language](https://arxiv.org/abs/2510.09714)
*Shiyuan Guo,Henry Sleight,Fabien Roger*

Main category: cs.CL

TL;DR: 本文研究了AI模型在加密文本中推理的能力，并发现模型在加密推理中的表现显著降低，尤其是对较少见的密码。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理的普及，检测有害AI行为变得重要，而攻击者可能通过加密推理来规避链式思维监控。

Method: 对28种不同密码，使用多达10种模型进行微调和提示，测试模型在加密文本中的数学问题推理能力。

Result: 模型在加密文本中的准确率显著下降，尤其是对于较不常见的密码，而模型对常见密码如rot13推理能力较强。加密推理能力与密码在预训练数据中的出现频率相关，并且随着微调数据量增加，能力提升缓慢。

Conclusion: 目前模型难以有效利用加密推理规避链式思维监控，这为未来模型的能力限制提供了参考。

Abstract: Detecting harmful AI actions is important as AI agents gain adoption.
Chain-of-thought (CoT) monitoring is one method widely used to detect
adversarial attacks and AI misalignment. However, attackers and misaligned
models might evade CoT monitoring through ciphered reasoning: reasoning hidden
in encrypted, translated, or compressed text. To assess this risk, we test
whether models can perform ciphered reasoning. For each of 28 different
ciphers, we fine-tune and prompt up to 10 models to reason in that cipher. We
measure model accuracy on math problems as a proxy for reasoning ability.
Across the models we test, we find an asymmetry: model accuracy can drop
significantly when reasoning in ciphered text, even though models demonstrate
comprehension of ciphered text by being able to translate it accurately to
English. Even frontier models struggle with lesser-known ciphers, although they
can reason accurately in well-known ciphers like rot13. We show that ciphered
reasoning capability correlates with cipher prevalence in pretraining data. We
also identify scaling laws showing that ciphered reasoning capability improves
slowly with additional fine-tuning data. Our work suggests that evading CoT
monitoring using ciphered reasoning may be an ineffective tactic for current
models and offers guidance on constraining the development of this capability
in future frontier models.

</details>


### [7] [MedAgentAudit: Diagnosing and Quantifying Collaborative Failure Modes in Medical Multi-Agent Systems](https://arxiv.org/abs/2510.10185)
*Lei Gu,Yinghao Zhu,Haoran Sang,Zixiang Wang,Dehao Sui,Wen Tang,Ewen Harrison,Junyi Gao,Lequan Yu,Liantao Ma*

Main category: cs.CL

TL;DR: 该论文针对基于大语言模型的多智能体医疗咨询系统，揭示了其评估多局限于最终答案准确性，忽视诊断过程的合理性和可验证性。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体医疗系统多把内部协作过程视作黑箱，忽略诊断结论是否基于合理推理，存在高风险。

Method: 通过分析3600个医疗案例和六个多智能体框架的混合定性与定量方法，构建协作失败模式分类。

Result: 定量审计发现四种主要失败模式：共享模型缺陷导致的错误共识、正确少数意见受抑制、无效讨论动态及关键信息丢失。

Conclusion: 高准确率不足以赢得信任，需透明且可审计的推理过程，保障医疗AI的负责任开发与应用。

Abstract: While large language model (LLM)-based multi-agent systems show promise in
simulating medical consultations, their evaluation is often confined to
final-answer accuracy. This practice treats their internal collaborative
processes as opaque "black boxes" and overlooks a critical question: is a
diagnostic conclusion reached through a sound and verifiable reasoning pathway?
The inscrutable nature of these systems poses a significant risk in high-stakes
medical applications, potentially leading to flawed or untrustworthy
conclusions. To address this, we conduct a large-scale empirical study of 3,600
cases from six medical datasets and six representative multi-agent frameworks.
Through a rigorous, mixed-methods approach combining qualitative analysis with
quantitative auditing, we develop a comprehensive taxonomy of collaborative
failure modes. Our quantitative audit reveals four dominant failure patterns:
flawed consensus driven by shared model deficiencies, suppression of correct
minority opinions, ineffective discussion dynamics, and critical information
loss during synthesis. This study demonstrates that high accuracy alone is an
insufficient measure of clinical or public trust. It highlights the urgent need
for transparent and auditable reasoning processes, a cornerstone for the
responsible development and deployment of medical AI.

</details>


### [8] [Preference-Aware Memory Update for Long-Term LLM Agents](https://arxiv.org/abs/2510.09720)
*Haoran Sun,Zekun Zhang,Shaoning Zeng*

Main category: cs.CL

TL;DR: 本文提出了一种偏好感知记忆更新机制（PAMU），通过结合滑动窗口平均和指数移动平均，动态调整基于用户行为变化的记忆表示，提升大语言模型（LLM）在长期对话中的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的长时记忆集成方法缺乏针对用户偏好和行为动态变化的记忆更新机制，导致记忆表示难以精准反映用户偏好演变，影响LLM推理效果。

Method: 提出PAMU机制，通过融合滑动窗口平均与指数移动平均，构建兼顾短期波动和长期趋势的个性化偏好记忆表达。

Result: 在LoCoMo数据集的五个任务场景中，PAMU显著提升了五个基线模型的输出质量，验证了其在长期对话中的有效性。

Conclusion: PAMU机制有效解决了长时记忆中偏好动态更新的问题，增强了LLM基于历史交互的推理能力，提高了对话质量。

Abstract: One of the key factors influencing the reasoning capabilities of LLM-based
agents is their ability to leverage long-term memory. Integrating long-term
memory mechanisms allows agents to make informed decisions grounded in
historical interactions. While recent advances have significantly improved the
storage and retrieval components, by encoding memory into dense vectors for
similarity search or organizing memory as structured knowledge graphs most
existing approaches fall short in memory updating. In particular, they lack
mechanisms for dynamically refining preference memory representations in
response to evolving user behaviors and contexts. To address this gap, we
propose a Preference-Aware Memory Update Mechanism (PAMU) that enables dynamic
and personalized memory refinement. By integrating sliding window averages (SW)
with exponential moving averages (EMA), PAMU constructs a fused
preference-aware representation that captures both short-term fluctuations and
long-term user tendencies. We conduct experiments on five task scenarios of the
LoCoMo dataset, and the results show that our mechanism can significantly
improve the output quality of LLM in five baselines, validating its
effectiveness in long-term conversations.

</details>


### [9] [StoryBox: Collaborative Multi-Agent Simulation for Hybrid Bottom-Up Long-Form Story Generation Using Large Language Models](https://arxiv.org/abs/2510.11618)
*Zehao Chen,Rong Pan,Haoran Li*

Main category: cs.CL

TL;DR: 本文提出了一种基于多智能体模拟的混合自下而上长篇故事生成方法，通过智能体在动态沙盒环境中的互动产生事件，推动故事自然展开，生成连贯一致且超过1万字的长篇故事。


<details>
  <summary>Details</summary>
Motivation: 人类作家通常先构想整体场景和角色互动，而传统故事生成方法多为自上而下的结构，缺乏自然展开的 spontaneity 和角色发展。

Method: 采用多智能体在动态沙盒环境中交互，智能体行为和相互作用产生事件，这些事件构成故事基础，实现角色发展和情节进展。

Result: 系统能够生成超过1万字的长篇故事，保持故事连贯性和一致性，在多个评测指标上达到先进水平。

Conclusion: 该混合自下而上的方法提供了一种可扩展、创新的长篇故事生成解决方案，能自然生成动态且沉浸感强的故事内容。

Abstract: Human writers often begin their stories with an overarching mental scene,
where they envision the interactions between characters and their environment.
Inspired by this creative process, we propose a novel approach to long-form
story generation, termed hybrid bottom-up long-form story generation, using
multi-agent simulations. In our method, agents interact within a dynamic
sandbox environment, where their behaviors and interactions with one another
and the environment generate emergent events. These events form the foundation
for the story, enabling organic character development and plot progression.
Unlike traditional top-down approaches that impose rigid structures, our hybrid
bottom-up approach allows for the natural unfolding of events, fostering more
spontaneous and engaging storytelling. The system is capable of generating
stories exceeding 10,000 words while maintaining coherence and consistency,
addressing some of the key challenges faced by current story generation models.
We achieve state-of-the-art performance across several metrics. This approach
offers a scalable and innovative solution for creating dynamic, immersive
long-form stories that evolve organically from agent-driven interactions.

</details>


### [10] [Layout-Aware Parsing Meets Efficient LLMs: A Unified, Scalable Framework for Resume Information Extraction and Evaluation](https://arxiv.org/abs/2510.09722)
*Fanwei Zhu,Jinke Yu,Zulong Chen,Ying Zhou,Junhao Ji,Zhibo Yang,Yuxue Zhang,Haoyuan Hu,Zhenghao Liu*

Main category: cs.CL

TL;DR: 本文提出了一个针对简历信息自动提取的框架，兼顾布局识别和高效推理，解决数据多样性大、使用大模型成本高、缺少标准数据集的三大挑战。


<details>
  <summary>Details</summary>
Motivation: 简历自动提取在实际应用中受限于简历版式和内容极其多样、大型语言模型成本高及缺乏统一评测标准，亟需一个高效且通用的解决方案。

Method: 结合了微调布局解析器规范化文档格式、基于并行提示和指令微调的高效小型语言模型提取器，以及两个阶段的自动化评测框架与新基准数据集。

Result: 实验表明该框架在准确率和效率上均显著优于强基线，特别是微调后0.6亿参数模型在准确率保持顶尖的同时大幅降低推理延迟和计算成本。

Conclusion: 所提框架已成功部署于阿里巴巴智能人力资源平台，支持各业务单元的实时应用，实现了简历信息自动提取的高效、精准和规模化。

Abstract: Automated resume information extraction is critical for scaling talent
acquisition, yet its real-world deployment faces three major challenges: the
extreme heterogeneity of resume layouts and content, the high cost and latency
of large language models (LLMs), and the lack of standardized datasets and
evaluation tools. In this work, we present a layout-aware and
efficiency-optimized framework for automated extraction and evaluation that
addresses all three challenges. Our system combines a fine-tuned layout parser
to normalize diverse document formats, an inference-efficient LLM extractor
based on parallel prompting and instruction tuning, and a robust two-stage
automated evaluation framework supported by new benchmark datasets. Extensive
experiments show that our framework significantly outperforms strong baselines
in both accuracy and efficiency. In particular, we demonstrate that a
fine-tuned compact 0.6B LLM achieves top-tier accuracy while significantly
reducing inference latency and computational cost. The system is fully deployed
in Alibaba's intelligent HR platform, supporting real-time applications across
its business units.

</details>


### [11] [VisRAG 2.0: Evidence-Guided Multi-Image Reasoning in Visual Retrieval-Augmented Generation](https://arxiv.org/abs/2510.09733)
*Yubo Sun,Chunyi Peng,Yukun Yan,Shi Yu,Zhenghao Liu,Chi Chen,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: 本文提出了一种名为EVisRAG的视觉检索增强生成框架，通过多图像证据指导推理，有效提升视觉-语言模型在多图像证据整合和推理中的表现，显著减少错误推断。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉检索增强生成系统在整合多张图像的证据时表现不可靠，导致推理基础薄弱和结论错误。

Method: 提出EVisRAG框架，模型先观察检索到的图像并记录每张图像的证据，然后基于聚合证据推断最终答案。同时引入奖励范围组相对策略优化(RS-GRPO)，通过细粒度奖励优化视觉感知和推理能力。

Result: 在多个视觉问答基准上，EVisRAG比基础视觉-语言模型平均提升27%的性能，显著提高了答案准确率。

Conclusion: EVisRAG在RS-GRPO的支持下，能像侦探一样精准感知和定位多图像中的关键证据并基于此进行推理，有效增强了多图像视觉推理能力。

Abstract: Visual retrieval-augmented generation (VRAG) augments vision-language models
(VLMs) with external visual knowledge to ground reasoning and reduce
hallucinations. Yet current VRAG systems often fail to reliably perceive and
integrate evidence across multiple images, leading to weak grounding and
erroneous conclusions. In this paper, we propose EVisRAG, an end-to-end
framework that learns to reason with evidence-guided multi-image to address
this issue. The model first observes retrieved images and records per-image
evidence, then derives the final answer from the aggregated evidence. To train
EVisRAG effectively, we introduce Reward-Scoped Group Relative Policy
Optimization (RS-GRPO), which binds fine-grained rewards to scope-specific
tokens to jointly optimize visual perception and reasoning abilities of VLMs.
Experimental results on multiple visual question answering benchmarks
demonstrate that EVisRAG delivers substantial end-to-end gains over backbone
VLM with 27\% improvements on average. Further analysis shows that, powered by
RS-GRPO, EVisRAG improves answer accuracy by precisely perceiving and
localizing question-relevant evidence across multiple images and deriving the
final answer from that evidence, much like a real detective.

</details>


### [12] [Judge's Verdict: A Comprehensive Analysis of LLM Judge Capability Through Human Agreement](https://arxiv.org/abs/2510.09738)
*Steve Han,Gilberto Titericz Junior,Tom Balough,Wenfei Zhou*

Main category: cs.CL

TL;DR: 该论文提出了Judge's Verdict Benchmark，一种两步法评估大型语言模型（LLMs）作为评审的准确性，结合相关性与Cohen's Kappa分析，区分人类类似和超一致判断模式。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法仅依赖相关性，缺乏对LLMs评审行为真实一致性的深入衡量。

Method: 采用两步方法：先用相关性测试筛选模型，再用z分数基于Cohen's Kappa分析判断模式，区分人类判断与超一致判断。

Result: 54个模型中有27个达到Tier 1水平，其中23个展现人类判断模式，4个展现超一致行为，表现优秀与模型大小无直接关联，与训练策略有关。

Conclusion: 单靠相关性不足以评估评审模型，提出“评审图灵测试”方法，建立标准化基准确保评审模型性能分层，满足不同需求。

Abstract: This research introduces the Judge's Verdict Benchmark, a novel two-step
methodology to evaluate Large Language Models (LLMs) as judges for response
accuracy evaluation tasks. We assess how well 54 LLMs can replicate human
judgment when scoring responses from RAG (Retrieval-Augmented Generation) or
Agentic pipelines against ground truth answers. Our methodology progresses from
traditional correlation analysis to comprehensive Cohen's Kappa analysis that
measures actual agreement patterns. The two-step approach includes: (1) a
correlation test that filters judges with strong alignment, followed by (2) a
human-likeness test using z-scores to identify two distinct judgment patterns:
human-like judgment (|z| < 1) that mimics natural human variation, and
super-consistent judgment (z > 1) that exceeds typical human-to-human agreement
levels. This methodology reveals that 27 out of 54 tested LLMs achieve Tier 1
performance: 23 models exhibit human-like patterns that preserve the nuances of
human judgment, while 4 models demonstrate super-consistent behavior, a pattern
that could indicate either enhanced reliability or oversimplification of
complex judgments. Testing 43 open-source models (1B-405B parameters) and 11
closed models (GPT, Gemini, Claude variants), we demonstrate that judge
excellence is not solely dependent on model size but on specific training
strategies. Our key contributions include: (1) establishing that correlation
alone is insufficient for judge evaluation, (2) introducing a "Turing Test for
judges" based on agreement patterns, and (3) providing a standardized benchmark
for classifying LLM judges into distinct performance tiers for different
evaluation needs.

</details>


### [13] [Gold Panning: Turning Positional Bias into Signal for Multi-Document LLM Reasoning](https://arxiv.org/abs/2510.09770)
*Adam Byerly,Daniel Khashabi*

Main category: cs.CL

TL;DR: 本文提出了一种利用大语言模型位置偏差的新方法，通过重新排序文档，高效识别最相关内容，显著减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多文档场景中存在位置偏差，倾向于优先考虑位置而非相关性的信息，现有方法将其视为需要减轻的噪声。

Method: 提出了Gold Panning Bandits框架，利用位置偏差作为诊断信号，通过重新排序文档并观察模型响应变化，将选择排序问题建模为二分匹配问题，使用贪心算法优化排序，提高效率。

Result: 该方法在知识密集型NLP任务中，比随机排列基线查询次数减少高达65%，显著降低计算成本，无需模型重新训练。

Conclusion: 本文展示了固有的LLM偏差可以转化为资产，实现推理时的高效优化，提高多文档相关性识别的效率。

Abstract: Large language models exhibit a strong position bias in multi-document
contexts, systematically prioritizing information based on location rather than
relevance. While existing approaches treat this bias as noise to be mitigated,
we introduce Gold Panning Bandits, a framework that leverages position bias as
a diagnostic signal: by reordering documents and observing shifts in the
model's responses, we can efficiently identify the most relevant content. We
frame the problem of choosing reorderings as a bipartite matching problem.
While an optimal assignment can be computed at each iteration with the
Hungarian algorithm in $O(N^3)$ time, we propose a greedy $O(N \log N)$
strategy that achieves comparable performance by prioritizing the placement of
the most uncertain documents in the most informative positions. Our approach
identifies relevant documents using up to 65\% fewer language model queries
than random permutation baselines on knowledge-intensive NLP tasks,
substantially reducing computational cost without model retraining. This work
demonstrates that inherent LLM biases can be transformed from liabilities into
assets for efficient, inference-time optimization.

</details>


### [14] [PromptGuard at BLP-2025 Task 1: A Few-Shot Classification Framework Using Majority Voting and Keyword Similarity for Bengali Hate Speech Detection](https://arxiv.org/abs/2510.09771)
*Rakib Hossan,Shubhashis Roy Dipta*

Main category: cs.CL

TL;DR: 该论文提出了一种针对孟加拉语仇恨言论分类的少样本学习框架PromptGuard，结合卡方统计法提取关键词和自适应多数投票机制，实现了优于传统n-gram和随机方法的效果。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习需要大量标注数据，对于资源匮乏的语言（如孟加拉语）标注代价高昂，因此需要一种少样本、高效的仇恨言论分类方法。

Method: 提出PromptGuard框架，结合卡方统计分析进行关键词提取，结合自适应多数投票机制根据分类一致性动态调整分类轮次。比较统计关键词选择和随机关键词选择效果。

Result: PromptGuard在多个仇恨言论分类类别中取得微平均F1为67.61，明显优于n-gram方法的60.75和随机方法的14.65。消融实验显示卡方关键词对所有类别影响最为稳定。

Conclusion: 采用基于统计的关键词提取结合自适应投票机制的少样本学习框架可以有效提升低资源语言的仇恨言论分类性能。

Abstract: The BLP-2025 Task 1A requires Bengali hate speech classification into six
categories. Traditional supervised approaches need extensive labeled datasets
that are expensive for low-resource languages. We developed PromptGuard, a
few-shot framework combining chi-square statistical analysis for keyword
extraction with adaptive majority voting for decision-making. We explore
statistical keyword selection versus random approaches and adaptive voting
mechanisms that extend classification based on consensus quality. Chi-square
keywords provide consistent improvements across categories, while adaptive
voting benefits ambiguous cases requiring extended classification rounds.
PromptGuard achieves a micro-F1 of 67.61, outperforming n-gram baselines
(60.75) and random approaches (14.65). Ablation studies confirm
chi-square-based keywords show the most consistent impact across all
categories.

</details>


### [15] [Steering Embedding Models with Geometric Rotation: Mapping Semantic Relationships Across Languages and Models](https://arxiv.org/abs/2510.09790)
*Michael Freenor,Lauren Alvarez*

Main category: cs.CL

TL;DR: 本文提出了一种名为RISE的新方法，用于在多语言、多模型的嵌入空间中用旋转操作捕捉语义变换，实现语义关系的几何解释。


<details>
  <summary>Details</summary>
Motivation: 当前高维文本表示缺乏直观的几何解释，而理解语言模型如何编码语义关系对模型可解释性和控制至关重要。

Method: 提出了RISE方法，将语义变换表示为嵌入空间中一致的旋转操作，利用现代语言表示的流形结构，实现跨语言、跨模型的语义映射。

Result: 在三个嵌入模型、三个数据集和七种语言中，RISE能一致地映射带有不同语法特征的语义变换（如否定和条件）。

Conclusion: 首次系统地验证了多语言嵌入空间中话语层面的语义变换对应于一致的几何操作，支持句子级线性表示假说。

Abstract: Understanding how language and embedding models encode semantic relationships
is fundamental to model interpretability and control. While early word
embeddings exhibited intuitive vector arithmetic (''king'' - ''man'' +
''woman'' = ''queen''), modern high-dimensional text representations lack
straightforward interpretable geometric properties. We introduce
Rotor-Invariant Shift Estimation (RISE), a geometric approach that represents
semantic transformations as consistent rotational operations in embedding
space, leveraging the manifold structure of modern language representations.
RISE operations have the ability to operate across both languages and models
with high transfer of performance, suggesting the existence of analogous
cross-lingual geometric structure. We evaluate RISE across three embedding
models, three datasets, and seven morphologically diverse languages in five
major language groups. Our results demonstrate that RISE consistently maps
discourse-level semantic transformations with distinct grammatical features
(e.g., negation and conditionality) across languages and models. This work
provides the first systematic demonstration that discourse-level semantic
transformations correspond to consistent geometric operations in multilingual
embedding spaces, empirically supporting the Linear Representation Hypothesis
at the sentence level.

</details>


### [16] [Text Prompt Injection of Vision Language Models](https://arxiv.org/abs/2510.09849)
*Ruizhe Zhu*

Main category: cs.CL

TL;DR: 本文探讨了针对大型视觉语言模型的文本提示注入攻击，提出了一种高效的攻击算法。


<details>
  <summary>Details</summary>
Motivation: 随着大型视觉语言模型的广泛应用，其安全性问题日益凸显，文本提示注入作为一种简单有效的误导手段引起关注。

Method: 本文设计了一种文本提示注入攻击算法，并通过实验验证了其有效性和计算资源需求低的特点。

Result: 实验结果表明，该算法在攻击大型模型时表现出较高的有效性和效率，且对计算资源要求较低。

Conclusion: 该方法为保证大型视觉语言模型安全提供了新思路，尤其适用于在有限计算资源下对模型进行攻击和安全测试。

Abstract: The widespread application of large vision language models has significantly
raised safety concerns. In this project, we investigate text prompt injection,
a simple yet effective method to mislead these models. We developed an
algorithm for this type of attack and demonstrated its effectiveness and
efficiency through experiments. Compared to other attack methods, our approach
is particularly effective for large models without high demand for
computational resources.

</details>


### [17] [NG-Router: Graph-Supervised Multi-Agent Collaboration for Nutrition Question Answering](https://arxiv.org/abs/2510.09854)
*Kaiwen Shi,Zheyuan Zhang,Zhengqing Yuan,Keerthiram Murugesan,Vincent Galass,Chuxu Zhang,Yanfang Ye*

Main category: cs.CL

TL;DR: 该论文提出了NG-Router框架，通过多智能体合作和知识图引导解决营养问答中的推理和上下文过载问题。


<details>
  <summary>Details</summary>
Motivation: 现有营养问答方法在单一智能体推理能力有限和多智能体架构设计复杂，以及上下文信息过载影响决策准确性方面存在挑战。

Method: NG-Router将营养问答建模为知识图指导的多智能体协作问题，利用图神经网络学习智能体间的路由分布，并通过梯度驱动的子图检索机制缓解上下文过载，从而提升多跳和关系推理能力。

Result: 在多个基准和模型上，NG-Router均明显优于单智能体和集成基线方法，表现出色。

Conclusion: NG-Router为复杂营养健康任务中的领域感知多智能体推理提供了有效且系统的解决方案。

Abstract: Diet plays a central role in human health, and Nutrition Question Answering
(QA) offers a promising path toward personalized dietary guidance and the
prevention of diet-related chronic diseases. However, existing methods face two
fundamental challenges: the limited reasoning capacity of single-agent systems
and the complexity of designing effective multi-agent architectures, as well as
contextual overload that hinders accurate decision-making. We introduce
Nutritional-Graph Router (NG-Router), a novel framework that formulates
nutritional QA as a supervised, knowledge-graph-guided multi-agent
collaboration problem. NG-Router integrates agent nodes into heterogeneous
knowledge graphs and employs a graph neural network to learn task-aware routing
distributions over agents, leveraging soft supervision derived from empirical
agent performance. To further address contextual overload, we propose a
gradient-based subgraph retrieval mechanism that identifies salient evidence
during training, thereby enhancing multi-hop and relational reasoning.
Extensive experiments across multiple benchmarks and backbone models
demonstrate that NG-Router consistently outperforms both single-agent and
ensemble baselines, offering a principled approach to domain-aware multi-agent
reasoning for complex nutritional health tasks.

</details>


### [18] [NarraBench: A Comprehensive Framework for Narrative Benchmarking](https://arxiv.org/abs/2510.09869)
*Sil Hamilton,Matthew Wilkens,Andrew Piper*

Main category: cs.CL

TL;DR: 本文提出了NarraBench，一个理论驱动的叙事理解任务分类法，并对现有的78个相关基准进行了调查，发现当前评价体系存在不足。


<details>
  <summary>Details</summary>
Motivation: 当前的叙事理解任务评价体系覆盖不全面，部分重要方面被忽视或缺乏合适的评估指标，尤其是叙事事件、风格、视角和揭示等内容。

Method: 构建基于理论的叙事理解任务分类法NarraBench，并调查整理78个现有基准，评估其对不同叙事任务的覆盖情况。

Result: 只有27%的叙事任务被现有基准良好覆盖，且主观性和视角性强的任务缺乏有效评价，部分关键叙事方面几乎没有对应评测。

Conclusion: NarraBench为自然语言处理研究者提供了一个全面的分类体系和调研视角，有助于推动叙事理解领域中更全面和多样化的评估标准的构建。

Abstract: We present NarraBench, a theory-informed taxonomy of narrative-understanding
tasks, as well as an associated survey of 78 existing benchmarks in the area.
We find significant need for new evaluations covering aspects of narrative
understanding that are either overlooked in current work or are poorly aligned
with existing metrics. Specifically, we estimate that only 27% of narrative
tasks are well captured by existing benchmarks, and we note that some areas --
including narrative events, style, perspective, and revelation -- are nearly
absent from current evaluations. We also note the need for increased
development of benchmarks capable of assessing constitutively subjective and
perspectival aspects of narrative, that is, aspects for which there is
generally no single correct answer. Our taxonomy, survey, and methodology are
of value to NLP researchers seeking to test LLM narrative understanding.

</details>


### [19] [CoBia: Constructed Conversations Can Trigger Otherwise Concealed Societal Biases in LLMs](https://arxiv.org/abs/2510.09871)
*Nafiseh Nikeghbal,Amir Hossein Kargaran,Jana Diesner*

Main category: cs.CL

TL;DR: 本文提出了CoBia，一种针对大型语言模型(LLMs)的对抗性攻击方法，用以系统分析其偏见行为。实验评估了11种模型在多个社会人口学类别上的偏见表现，揭示了LLMs在对话中常放大偏见且未能拒绝带偏见的后续问题。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在安全防护上有所改进，但仍存在在对话中表达种族主义等有害行为的风险，需要系统性分析这些偏见行为的具体条件。

Method: 设计CoBia对抗攻击，通过构造含偏见陈述的对话情境，评估模型是否能从虚假偏见陈述中恢复并拒绝偏见后续问题。对11种开源及专有LLMs针对性别、种族、宗教等6类社会人口学类别进行偏见测试，并与人工判断结果进行比较。

Result: 研究发现，刻意设计的对话能有效揭示模型偏见放大现象，且多数LLMs在对话中未能有效拒绝带偏见的后续问题，显示出内嵌的偏见问题。

Conclusion: 通过CoBia的压力测试能够暴露LLMs中深层的偏见问题，有助于改进模型的安全性与公平性。相关代码和资源已开源。

Abstract: Improvements in model construction, including fortified safety guardrails,
allow Large language models (LLMs) to increasingly pass standard safety checks.
However, LLMs sometimes slip into revealing harmful behavior, such as
expressing racist viewpoints, during conversations. To analyze this
systematically, we introduce CoBia, a suite of lightweight adversarial attacks
that allow us to refine the scope of conditions under which LLMs depart from
normative or ethical behavior in conversations. CoBia creates a constructed
conversation where the model utters a biased claim about a social group. We
then evaluate whether the model can recover from the fabricated bias claim and
reject biased follow-up questions. We evaluate 11 open-source as well as
proprietary LLMs for their outputs related to six socio-demographic categories
that are relevant to individual safety and fair treatment, i.e., gender, race,
religion, nationality, sex orientation, and others. Our evaluation is based on
established LLM-based bias metrics, and we compare the results against human
judgments to scope out the LLMs' reliability and alignment. The results suggest
that purposefully constructed conversations reliably reveal bias amplification
and that LLMs often fail to reject biased follow-up questions during dialogue.
This form of stress-testing highlights deeply embedded biases that can be
surfaced through interaction. Code and artifacts are available at
https://github.com/nafisenik/CoBia.

</details>


### [20] [iBERT: Interpretable Style Embeddings via Sense Decomposition](https://arxiv.org/abs/2510.09882)
*Vishal Anand,Milad Alshomary,Kathleen McKeown*

Main category: cs.CL

TL;DR: iBERT是一种可解释且可控的编码器，通过将输入表示为多个独立语义向量的稀疏非负组合，实现语言风格和语义结构的模块化表达。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型难以直接解释其编码内容及风格属性，让表示更易解释和操控是本文目标。

Method: 设计了iBERT，将每个词表示为k个独立语义向量的非负稀疏组合，可直接用于词级或句子级表示，实现模块化控制。

Result: 在STEL风格任务上，iBERT风格表示效果优于SBERT基线约8个百分点，同时作者身份验证表现仍有竞争力。

Conclusion: iBERT结构允许解耦风格与语义信息，特定风格属性可被分配到特定向量，方法具有通用性，适用于混合风格和语义因子的监督任务。

Abstract: We present iBERT (interpretable-BERT), an encoder to produce inherently
interpretable and controllable embeddings - designed to modularize and expose
the discriminative cues present in language, such as stylistic and semantic
structure. Each input token is represented as a sparse, non-negative mixture
over k context-independent sense vectors, which can be pooled into sentence
embeddings or used directly at the token level. This enables modular control
over representation, before any decoding or downstream use.
  To demonstrate our model's interpretability, we evaluate it on a suite of
style-focused tasks. On the STEL benchmark, it improves style representation
effectiveness by ~8 points over SBERT-style baselines, while maintaining
competitive performance on authorship verification. Because each embedding is a
structured composition of interpretable senses, we highlight how specific style
attributes - such as emoji use, formality, or misspelling can be assigned to
specific sense vectors. While our experiments center on style, iBERT is not
limited to stylistic modeling. Its structural modularity is designed to
interpretably decompose whichever discriminative signals are present in the
data - enabling generalization even when supervision blends stylistic and
semantic factors.

</details>


### [21] [DELTA: Dynamic Layer-Aware Token Attention for Efficient Long-Context Reasoning](https://arxiv.org/abs/2510.09883)
*Hossein Entezari Zarch,Lei Gao,Chaoyi Jiang,Murali Annavarm*

Main category: cs.CL

TL;DR: DELTA是一种无需训练的稀疏注意力机制，通过选择性重用中间注意力图，实现推理模型计算效率的大幅提升而不损失准确度。


<details>
  <summary>Details</summary>
Motivation: 长推理模型生成长中间推理链时，解码阶段计算成本高，现有稀疏注意力方法因选择误差导致准确率下降。

Method: DELTA将Transformer层分为三组：初始层全注意力，选择层利用头级注意力分数选出关键token，后续层仅对选定token进行稀疏注意力计算，从而保留完整KV缓存同时降低计算量。

Result: DELTA在AIME和GPQA-Diamond等推理基准上，准确率匹配甚至超过全注意力机制，同时可将关注token数量减少5倍，实现1.5倍整体推理加速。

Conclusion: 选择性重用中间注意力图是实现长上下文推理高效性的有效方法，有助于保持模型准确性并减少计算资源消耗。

Abstract: Large reasoning models (LRMs) achieve state-of-the-art performance on
challenging benchmarks by generating long chains of intermediate steps, but
their inference cost is dominated by decoding, where each new token must attend
to the entire growing sequence. Existing sparse attention methods reduce
computation by pruning the key-value (KV) cache, yet they suffer from severe
accuracy degradation on reasoning tasks due to cumulative selection errors and
the dynamic importance of tokens over long derivations. We present
\textbf{DELTA}, a training-free sparse attention mechanism that achieves
computational efficiency without sacrificing model accuracy. DELTA partitions
transformer layers into three groups: initial layers that use full attention, a
small set of \emph{selection layers} that identify salient tokens via
aggregated head-level attention scores, and subsequent \emph{sparse-attention
layers} that attend only to the selected subset. This design preserves the full
KV cache in GPU memory for accuracy, while avoiding expensive full-attention
computation over many layers. On reasoning benchmarks such as AIME and
GPQA-Diamond, DELTA matches or surpasses full attention in accuracy, while
reducing the number of attended tokens by up to $5\times$ and delivering
$1.5\times$ end-to-end speedup. Our results show that selective reuse of
intermediate attention maps offers a robust path toward efficient long-context
reasoning.

</details>


### [22] [Closing the Data-Efficiency Gap Between Autoregressive and Masked Diffusion LLMs](https://arxiv.org/abs/2510.09885)
*Xu Pan,Ely Hahami,Jingxuan Fan,Ziqian Xie,Haim Sompolinsky*

Main category: cs.CL

TL;DR: 本文探讨了自回归大语言模型（arLLMs）和掩码扩散大语言模型（dLLMs）在知识注入和问答泛化上的表现差异，发现dLLMs在知识注入和反向问答中表现更优，且提出了基于掩码微调的新方法显著提升了arLLMs的微调效率。


<details>
  <summary>Details</summary>
Motivation: arLLMs在知识注入阶段存在“反转诅咒”问题，即难以回答训练样本中信息顺序被颠倒的问题，而dLLMs在预训练中已显示出优势，但其在微调阶段的表现尚不明确。

Method: 论文在三个不同数据集上对arLLMs和dLLMs进行微调，通过正向和反向问答测试其知识泛化能力和“反转诅咒”表现，进一步提出了一种基于掩码的微调新范式以改善arLLMs的知识注入效率。

Result: 实验结果表明，arLLMs依赖大量同义句数据增强并且只有信息顺序匹配题型时同义句才有效；dLLMs无须同义句即可在正反向问答中均取得高准确率，且同义句提升有限。基于dLLM的新型掩码微调方法显著提升了arLLMs的微调数据效率，缩小了与dLLMs的性能差距。

Conclusion: dLLMs在知识注入与问答泛化方面优于arLLMs，且可以无视信息顺序变化。基于dLLM表现提出的掩码微调范式为arLLMs的知识注入提供了一个高效方案，有望推动其在下游任务中的表现提升。

Abstract: Despite autoregressive large language models (arLLMs) being the current
dominant paradigm in language modeling, they resist knowledge injection via
fine-tuning due to inherent shortcomings such as the "reversal curse" -- the
challenge of answering questions that reverse the original information order in
the training sample. Masked diffusion large language models (dLLMs) are rapidly
emerging as a powerful alternative to the arLLM paradigm, with evidence of
better data efficiency and free of the "reversal curse" in pre-training.
However, it is unknown whether these advantages extend to the post-training
phase, i.e. whether pre-trained dLLMs can easily acquire new knowledge through
fine-tuning. On three diverse datasets, we fine-tune arLLMs and dLLMs,
evaluating them with forward and backward style Question Answering (QA) to
probe knowledge generalization and the reversal curse. Our results confirm that
arLLMs critically rely on extensive data augmentation via paraphrases for QA
generalization, and paraphrases are only effective when their information order
matches the QA style. Conversely, dLLMs achieve high accuracies on both forward
and backward QAs without paraphrases; adding paraphrases yields only marginal
gains. Lastly, inspired by the dLLM's performance, we introduce a novel masked
fine-tuning paradigm for knowledge injection into pre-trained arLLMs. This
proposed method successfully and drastically improves the data efficiency of
arLLM fine-tuning, effectively closing the performance gap with dLLMs.

</details>


### [23] [Abductive Preference Learning](https://arxiv.org/abs/2510.09887)
*Yijin Ni,Peng Qi*

Main category: cs.CL

TL;DR: 目前的大型语言模型虽经过强化学习优化，但仍存在过度自信的问题，难以正确区分相似题目中应不同回答的情况。本文提出了溯因偏好学习，通过学习在给定回答时区分输入提示，提升模型对提示差异的敏感度。实验显示该方法显著提高了回答选择和提示区分的准确率，并且在综合评估中表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有偏好学习方法侧重于在给定提示下选择正确回答，忽视了应对形似但应答不同的反事实提示，导致模型过度自信、回答保守。

Method: 提出溯因偏好学习（abductive preference learning），通过反转条件关系，学习在给定回答的情况下区分不同提示。构建基于HaluEval的溯因数据集，并实现溯因DPO和DPOP方法。采用多任务目标联合传统方法与溯因方法。

Result: 在溯因数据集上，多任务DPOP使回答选择准确率从90.0％提升至99.5％，提示区分准确率从54.7％提升至85.0％。在AlpacaEval评测中，胜率从5.26％提升至6.17％，定性分析显示模型对提示差异敏感度显著提升。

Conclusion: 溯因偏好学习补充了传统偏好优化，解决了模型对反事实提示识别不足的问题，提升了大型语言模型的回答准确性和鲁棒性。

Abstract: Frontier large language models such as GPT-5 and Claude Sonnet remain prone
to overconfidence even after alignment through Reinforcement Learning with
Human Feedback (RLHF) and Direct Preference Optimization (DPO). For instance,
they tend to offer the same conservative answer "No" to both questions "Can I
eat the [food / potato chips] that has been left out overnight?" despite the
latter requiring no refridgeration for safe consumption. We find that this
failure is potentially attributed to a limitation of existing preference
learning: it emphasizes selecting the correct response for a given prompt,
while neglecting counterfactual prompts that should alter the response.
  To address this limitation, we propose abductive preference learning, a
fine-tuning paradigm that reverses the conventional conditioning by learning
preferences over prompts given a response. To validate this idea, we construct
an abductive dataset derived from the HaluEval QA benchmark with 1,001 entries,
implementing abductive DPO and its variant DPOP. Experiments reveal
complementary strengths: standard methods improve response selection, abductive
methods improve prompt discrimination, while a multitask objective unifies
both. On the abductive dataset, multitask DPOP boosts accuracy from $90.0\%$ to
$99.5\%$ in response selection and $54.7\%$ to $85.0\%$ in prompt
discrimination, with qualitative evidence highlighting improved sensitivity to
prompt differences. Finally, evaluation on AlpacaEval shows multitask DPOP
improves win rate (from $5.26\%$ to $6.17\%$), confirming that abductive
preference learning preserves the benefits of conventional preference
optimization while addressing the overlooked challenge of counterfactual
prompts.

</details>


### [24] [HIPPD: Brain-Inspired Hierarchical Information Processing for Personality Detection](https://arxiv.org/abs/2510.09893)
*Guanming Chen,Lingzhi Shen,Xiaohao Cai,Imran Razzak,Shoaib Jameel*

Main category: cs.CL

TL;DR: 该论文提出了一个名为HIPPD的脑启发式人格检测框架，利用大语言模型和动态记忆模块模仿大脑层次结构，提升了文本语义稀疏环境下的人格特征提取能力。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法难以捕捉跨多篇文本的上下文信息，且在人格检测时难以提取具代表性和鲁棒性的特征。

Method: HIPPD框架模拟人脑层次信息处理，包括大语言模型模拟大脑皮层实现全局语义推理和深层特征抽象，动态记忆模块模拟前额叶皮层进行自适应特征过滤，多个轻量型模型模拟基底节通过赢家通吃机制动态路由，捕获人格相关模式。

Result: 在Kaggle和Pandora数据集上的广泛实验表明，HIPPD表现优于现有最先进方法。

Conclusion: HIPPD模仿大脑层次信息处理机制，有效提升了文本人格检测的性能和鲁棒性，展示了脑启发式方法的优势。

Abstract: Personality detection from text aims to infer an individual's personality
traits based on linguistic patterns. However, existing machine learning
approaches often struggle to capture contextual information spanning multiple
posts and tend to fall short in extracting representative and robust features
in semantically sparse environments. This paper presents HIPPD, a
brain-inspired framework for personality detection that emulates the
hierarchical information processing of the human brain. HIPPD utilises a large
language model to simulate the cerebral cortex, enabling global semantic
reasoning and deep feature abstraction. A dynamic memory module, modelled after
the prefrontal cortex, performs adaptive gating and selective retention of
critical features, with all adjustments driven by dopaminergic prediction error
feedback. Subsequently, a set of specialised lightweight models, emulating the
basal ganglia, are dynamically routed via a strict winner-takes-all mechanism
to capture the personality-related patterns they are most proficient at
recognising. Extensive experiments on the Kaggle and Pandora datasets
demonstrate that HIPPD consistently outperforms state-of-the-art baselines.

</details>


### [25] [Don't Throw Away Your Pretrained Model](https://arxiv.org/abs/2510.09913)
*Shangbin Feng,Wenhao Yu,Yike Wang,Hongming Zhang,Yulia Tsvetkov,Dong Yu*

Main category: cs.CL

TL;DR: 该论文提出了一种通过模型协作实现语言模型优势互补的方法，即Switch Generation方法，通过让预训练模型和对齐模型交替生成回答片段以发挥各自优势。


<details>
  <summary>Details</summary>
Motivation: 传统的对齐训练在提升语言模型推理和遵循指令能力时，会牺牲创造力和校准等技能，而这些技能未对齐的基础模型表现更好。作者希望通过模型协作结合两者的长处。

Method: 提出Switch Generation方法，训练一个切换器语言模型，动态选择在回答中由哪个模型生成下一个片段，利用不同模型的不同优势来优化整体回答质量。

Result: 在18个数据集和8个模型协作基线的广泛实验中，模型协作方法在16个任务中优于单个模型，Switch Generation在平均表现上较基线提升12.9%。

Conclusion: Switch Generation 能发现组合性技能，解决单个模型难以应对的问题，并能推广到未见过的模型和任务，有效利用训练中产生但通常被丢弃的中间成果。

Abstract: Alignment training has tradeoffs: it helps language models (LMs) gain in
reasoning and instruction following but might lose out on skills such as
creativity and calibration, where unaligned base models are better at. We aim
to make the best of both worlds through model collaboration, where different
models in the training pipeline collaborate and complement each other. Since LM
responses feature interleaving skills that favor different models, we propose
Switch Generation, where pretrained and aligned model versions take turns to
``speak'' in a response sequence. Specifically, we train a switcher LM by
learning from outcomes of choosing different models to generate the next
segment across diverse queries and contexts. At inference time, the switcher LM
guides different model checkpoints to dynamically generate the next segment
where their strengths are most needed. Extensive experiments with 8 model
collaboration baselines and 18 datasets show that 1) model collaboration
consistently outperforms individual models on 16 out of 18 tasks, and 2) Switch
Generation further outperforms baselines by 12.9% on average. Further analysis
reveals that Switch Generation discovers compositional skills to solve problems
where individual models struggle and generalizes to unseen models and tasks,
reusing and repurposing by-products in expensive model training pipelines that
are otherwise discarded.

</details>


### [26] [Enhancing Faithfulness in Abstractive Summarization via Span-Level Fine-Tuning](https://arxiv.org/abs/2510.09915)
*Sicong Huang,Qianqi Yan,Shengze Wang,Ian Lane*

Main category: cs.CL

TL;DR: 本文提出通过微调大语言模型以减少摘要生成中的不真实信息，使用GPT-4o标注错误并引入带标签的新数据集，验证了三种微调方法，发现不可取训练效果最佳。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型生成摘要时存在幻觉问题，现有解决策略不足以完全消除这些错误。

Method: 自动生成摘要，用GPT-4o标注不真实片段，利用带标签的数据集对模型进行微调，比较梯度上升、不可取训练和任务向量反向三种方法。

Result: 三种微调方法均提高了摘要的真实性，其中不可取训练效果最优。

Conclusion: 利用带标签的不真实摘要通过微调可显著提升大语言模型生成摘要的真实性，不可取训练是最有效的微调策略。

Abstract: Abstractive summarization using large language models (LLMs) has become an
essential tool for condensing information. However, despite their ability to
generate fluent summaries, these models sometimes produce unfaithful summaries,
introducing hallucinations at the word, phrase, or concept level. Existing
mitigation strategies, such as post-processing corrections or contrastive
learning with synthetically generated negative samples, fail to fully address
the diverse errors that can occur in LLM-generated summaries. In this paper, we
investigate fine-tuning strategies to reduce the occurrence of unfaithful spans
in generated summaries. First, we automatically generate summaries for the set
of source documents in the training set with a variety of LLMs and then use
GPT-4o to annotate any hallucinations it detects at the span-level. Leveraging
these annotations, we fine-tune LLMs with both hallucination-free summaries and
annotated unfaithful spans to enhance model faithfulness. In this paper, we
introduce a new dataset that contains both faithful and unfaithful summaries
with span-level labels and we evaluate three techniques to fine-tuning a LLM to
improve the faithfulness of the resulting summarization: gradient ascent,
unlikelihood training, and task vector negation. Experimental results show that
all three approaches successfully leverage span-level annotations to improve
faithfulness, with unlikelihood training being the most effective.

</details>


### [27] [Unpacking Hateful Memes: Presupposed Context and False Claims](https://arxiv.org/abs/2510.09935)
*Weibin Cai,Jiayu Li,Reza Zafarani*

Main category: cs.CL

TL;DR: 本文提出了SHIELD框架，通过建模预设上下文和检测虚假陈述，有效提升了仇恨表情包的检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有仇恨表情包检测方法主要依赖预训练语言模型，缺乏对仇恨表情包本质特征的深入探讨。

Method: 结合哲学和心理学见解，提出PCM模块用于跨模态建模上下文信息，FACT模块用于结合外部知识和跨模态参考图检测虚假陈述，两者结合形成SHIELD框架。

Result: SHIELD在多个数据集和评测指标上均优于现有最先进方法，并在假新闻检测等任务中展现出良好适用性。

Conclusion: 通过捕捉仇恨表达中预设上下文和虚假陈述的核心特征，SHIELD显著提升了仇恨表情包检测的效果，具有广泛应用前景。

Abstract: While memes are often humorous, they are frequently used to disseminate hate,
causing serious harm to individuals and society. Current approaches to hateful
meme detection mainly rely on pre-trained language models. However, less focus
has been dedicated to \textit{what make a meme hateful}. Drawing on insights
from philosophy and psychology, we argue that hateful memes are characterized
by two essential features: a \textbf{presupposed context} and the expression of
\textbf{false claims}. To capture presupposed context, we develop \textbf{PCM}
for modeling contextual information across modalities. To detect false claims,
we introduce the \textbf{FACT} module, which integrates external knowledge and
harnesses cross-modal reference graphs. By combining PCM and FACT, we introduce
\textbf{\textsf{SHIELD}}, a hateful meme detection framework designed to
capture the fundamental nature of hate. Extensive experiments show that SHIELD
outperforms state-of-the-art methods across datasets and metrics, while
demonstrating versatility on other tasks, such as fake news detection.

</details>


### [28] [Beyond Fertility: Analyzing STRR as a Metric for Multilingual Tokenization Evaluation](https://arxiv.org/abs/2510.09947)
*Mir Tafseer Nayeem,Sawsan Alqahtani,Md Tahmid Rahman Laskar,Tasnim Mohiuddin,M Saiful Bari*

Main category: cs.CL

TL;DR: 本文分析了大语言模型中的分词方法，提出了新的指标STRR以补充现有的词汇压缩效率指标fertility，评估分词器在不同语言和领域的公平性。


<details>
  <summary>Details</summary>
Motivation: 现有的分词效率指标fertility难以反映分词器在不同语言和领域的表现差异和公平性。

Method: 比较了六种常用分词器在七种语言和两个领域上的表现，提出了新的指标Single Token Retention Rate (STRR)用于衡量单词保持为单一标记的比例。

Result: 发现英语的fertility较稳定，中文fertility较高，领域差异不大。STRR揭示了英语优先性、对中文的强支持以及印地语的分词碎片化问题。

Conclusion: STRR能够补充fertility指标，帮助评估跨语言分词公平性，为设计更公平的多语言分词器提供实用指导。

Abstract: Tokenization is a crucial but under-evaluated step in large language models
(LLMs). The standard metric, fertility (the average number of tokens per word),
captures compression efficiency but obscures how vocabularies are allocated
across languages and domains. We analyze six widely used tokenizers across
seven languages and two domains, finding stable fertility for English, high
fertility for Chinese, and little domain sensitivity. To address fertility's
blind spots, we propose the Single Token Retention Rate (STRR), which measures
the proportion of words preserved as single tokens. STRR reveals systematic
prioritization of English, strong support for Chinese, and fragmentation in
Hindi, offering an interpretable view of cross-lingual fairness. Our results
show that STRR complements fertility and provides practical guidance for
designing more equitable multilingual tokenizers.

</details>


### [29] [Unifying Tree Search Algorithm and Reward Design for LLM Reasoning: A Survey](https://arxiv.org/abs/2510.09988)
*Jiaqi Wei,Xiang Zhang,Yuejin Yang,Wenxuan Huang,Juntai Cao,Sheng Xu,Xiang Zhuang,Zhangyang Gao,Muhammad Abdul-Mageed,Laks V. S. Lakshmanan,Chenyu You,Wanli Ouyang,Siqi Sun*

Main category: cs.CL

TL;DR: 本文提出了一个统一框架，清晰区分了搜索算法的三个核心组件，并明确了奖励信号在测试时扩展和自我提升中的不同作用。


<details>
  <summary>Details</summary>
Motivation: 大语言模型领域的审慎树搜索研究分散，缺乏统一的形式化框架，且奖励信号的角色模糊。

Method: 构建一个统一框架，将搜索算法拆解为搜索机制、奖励表述和转移函数三个组件，区分临时的搜索指导和持久的参数化奖励建模。

Result: 提出基于组件的分类法，整合了现有研究成果，澄清了奖励信号的双重作用，推动领域系统化发展。

Conclusion: 该框架为审慎树搜索和自我提升研究提供了共同语言和研究路线，有助于自主自我改进智能体的开发。

Abstract: Deliberative tree search is a cornerstone of modern Large Language Model
(LLM) research, driving the pivot from brute-force scaling toward algorithmic
efficiency. This single paradigm unifies two critical frontiers:
\textbf{Test-Time Scaling (TTS)}, which deploys on-demand computation to solve
hard problems, and \textbf{Self-Improvement}, which uses search-generated data
to durably enhance model parameters. However, this burgeoning field is
fragmented and lacks a common formalism, particularly concerning the ambiguous
role of the reward signal -- is it a transient heuristic or a durable learning
target? This paper resolves this ambiguity by introducing a unified framework
that deconstructs search algorithms into three core components: the
\emph{Search Mechanism}, \emph{Reward Formulation}, and \emph{Transition
Function}. We establish a formal distinction between transient \textbf{Search
Guidance} for TTS and durable \textbf{Parametric Reward Modeling} for
Self-Improvement. Building on this formalism, we introduce a component-centric
taxonomy, synthesize the state-of-the-art, and chart a research roadmap toward
more systematic progress in creating autonomous, self-improving agents.

</details>


### [30] [Toward Machine Translation Literacy: How Lay Users Perceive and Rely on Imperfect Translations](https://arxiv.org/abs/2510.09994)
*Yimin Xiao,Yongle Zhang,Dayeon Ki,Calvin Bao,Marianna J. Martindale,Charlotte Vaughn,Ge Gao,Marine Carpuat*

Main category: cs.CL

TL;DR: 本文通过在公共博物馆对452名参与者进行研究，分析了流畅性和准确性错误对双语和非双语用户在日常使用机器翻译时依赖程度的影响。


<details>
  <summary>Details</summary>
Motivation: 随着机器翻译变得普及，了解公众如何感知并依赖不完美的机器翻译对于实际应用中的机器翻译研究非常重要。

Method: 通过在公共博物馆进行一项涉及452名受试者的人类研究，调查流畅性和准确性错误如何影响双语和非双语用户的机器翻译依赖。

Result: 非双语用户由于缺乏评估策略和备用方案，往往过度依赖机器翻译，而错误的影响会促使用户重新评估未来的依赖。

Conclusion: 研究强调了需要提升机器翻译评估和自然语言处理解释技术，不仅提升机器翻译质量，也提高用户的机器翻译素养。

Abstract: As Machine Translation (MT) becomes increasingly commonplace, understanding
how the general public perceives and relies on imperfect MT is crucial for
contextualizing MT research in real-world applications. We present a human
study conducted in a public museum (n=452), investigating how fluency and
adequacy errors impact bilingual and non-bilingual users' reliance on MT during
casual use. Our findings reveal that non-bilingual users often over-rely on MT
due to a lack of evaluation strategies and alternatives, while experiencing the
impact of errors can prompt users to reassess future reliance. This highlights
the need for MT evaluation and NLP explanation techniques to promote not only
MT quality, but also MT literacy among its users.

</details>


### [31] [MTP-S2UT: Enhancing Speech-to-Speech Translation Quality with Multi-token Prediction](https://arxiv.org/abs/2510.10003)
*Jianjin Wang,Runsong Zhao,Xiaoqian Liu,Yuan Ge,Ziqiang Xu,Tong Xiao,Shengxiang Gao,Zhengtao Yu,Jingbo Zhu*

Main category: cs.CL

TL;DR: 本文提出多令牌预测（MTP）损失应用于语音到单位翻译模型，通过在隐藏层施加损失实现更早的信息丰富，显著提升翻译质量。


<details>
  <summary>Details</summary>
Motivation: 目前语音直接翻译大多用单个语音令牌作为中间表示，语义密度不足，需要多令牌表达完整语义，亟需提升信息密度。

Method: 引入多令牌预测（MTP）损失，使模型在每个位置预测多个后续令牌，且将MTP损失从输出层提前至隐藏层（CTC计算层）应用，称为MTP-S2UT。

Result: 所有 MTP 变体均提升了语音到单位翻译质量，且将 MTP损失用于隐藏层的 MTP-S2UT 获得最好效果。

Conclusion: 在隐藏层施加多令牌预测损失能更早更有效地丰富语义信息，显著提升直接语音翻译模型性能。

Abstract: Current direct speech-to-speech translation methods predominantly employ
speech tokens as intermediate representations. However, a single speech token
is not dense in semantics, so we generally need multiple tokens to express a
complete semantic unit. To address this limitation, we introduce multi-token
prediction (MTP) loss into speech-to-unit translation (S2UT) models, enabling
models to predict multiple subsequent tokens at each position, thereby
capturing more complete semantics and enhancing information density per
position. Initial MTP implementations apply the loss at the final layer, which
improves output representation but initiates information enrichment too late.
We hypothesize that advancing the information enrichment process to
intermediate layers can achieve earlier and more effective enhancement of
hidden representation. Consequently, we propose MTP-S2UT loss, applying MTP
loss to hidden representation where CTC loss is computed. Experiments
demonstrate that all MTP loss variants consistently improve the quality of S2UT
translation, with MTP-S2UT achieving the best performance.

</details>


### [32] [Beyond the limitation of a single query: Train your LLM for query expansion with Reinforcement Learning](https://arxiv.org/abs/2510.10009)
*Shu Zhao,Tan Yu,Anbang Xu*

Main category: cs.CL

TL;DR: 本文提出了一种基于强化学习的多跳问答查询扩展搜索代理ExpandSearch，通过引入预训练的squeezer模型辅助理解文档，实现了更高的检索召回和问答准确率。


<details>
  <summary>Details</summary>
Motivation: 现有搜索推理代理在多跳问答中的推理和搜索能力有限，表现不佳，难以应对复杂查询。

Method: 训练带有查询扩展能力的搜索代理，通过强化学习生成多个查询变体并同时搜索，同时用预训练squeezer模型辅助信息理解，让搜索代理专注于查询生成。

Result: 使用3B规模小型LLM并辅以squeezer模型，在七个多跳问答基准上平均提升4.4%的准确率，刷新了多跳推理任务的最高水平。

Conclusion: 结合查询扩展和文档理解的搜索代理有效提升了多跳问答性能，展现了小型LLM的强大能力与实际价值。

Abstract: Reasoning-augmented search agents, such as Search-R1, are trained to reason,
search, and generate the final answer iteratively. Nevertheless, due to their
limited capabilities in reasoning and search, their performance on multi-hop QA
benchmarks remains far from satisfactory. To handle complex or compound
queries, we train an LLM-based search agent with the native capability of query
expansion through reinforcement learning. In each turn, our search agent
proposes several query variants, which are searched simultaneously to cover
more relevant information. Meanwhile, given limited post-training data and
computing resources, it is very challenging for a search agent to master
multiple tasks, including query generation, retrieved information
understanding, and answer generation. Therefore, we propose incorporating a
pre-trained squeezer model that helps the search agent understand the retrieved
documents, allowing the search agent to focus on query generation for high
retrieval recall. With the assistance of the squeezer model, we discover that
even a small-scale 3B LLM can demonstrate a strong capability of query
expansion and achieve state-of-the-art accuracy on the multi-hop QA benchmarks.
To be specific, our experiments across seven question-answering benchmarks
demonstrate that our method, named ExpandSearch, achieves an average
improvement of 4.4% compared to state-of-the-art baselines, with strong gains
on multi-hop reasoning tasks requiring diverse evidence aggregation.

</details>


### [33] [Path Drift in Large Reasoning Models:How First-Person Commitments Override Safety](https://arxiv.org/abs/2510.10013)
*Yuyi Huang,Runzhe Zhan,Lidia S. Chao,Ailin Tao,Derek F. Wong*

Main category: cs.CL

TL;DR: 本文发现大型语言模型在长链思维推理中存在路径漂移现象，导致安全约束被违反。


<details>
  <summary>Details</summary>
Motivation: 尽管已有对齐技术如RLHF保障模型安全，但长链推理路径仍可能偏离安全轨迹，存在路径漂移风险。

Method: 通过实证分析识别路径漂移的三大诱因，提出三阶段路径漂移诱导框架，并设计路径级防御策略包括角色修正和元认知反思。

Result: 三阶段诱导框架显著降低模型拒绝率，路径级防御有效缓解路径漂移风险。

Conclusion: 长链推理中需超越单个输出标记的对齐，开展轨迹级别的安全监管，保障模型整体推理过程的安全。

Abstract: As large language models (LLMs) are increasingly deployed for complex
reasoning tasks, Long Chain-of-Thought (Long-CoT) prompting has emerged as a
key paradigm for structured inference. Despite early-stage safeguards enabled
by alignment techniques such as RLHF, we identify a previously underexplored
vulnerability: reasoning trajectories in Long-CoT models can drift from aligned
paths, resulting in content that violates safety constraints. We term this
phenomenon Path Drift. Through empirical analysis, we uncover three behavioral
triggers of Path Drift: (1) first-person commitments that induce goal-driven
reasoning that delays refusal signals; (2) ethical evaporation, where
surface-level disclaimers bypass alignment checkpoints; (3) condition chain
escalation, where layered cues progressively steer models toward unsafe
completions. Building on these insights, we introduce a three-stage Path Drift
Induction Framework comprising cognitive load amplification, self-role priming,
and condition chain hijacking. Each stage independently reduces refusal rates,
while their combination further compounds the effect. To mitigate these risks,
we propose a path-level defense strategy incorporating role attribution
correction and metacognitive reflection (reflective safety cues). Our findings
highlight the need for trajectory-level alignment oversight in long-form
reasoning beyond token-level alignment.

</details>


### [34] [Lightweight Baselines for Medical Abstract Classification: DistilBERT with Cross-Entropy as a Strong Default](https://arxiv.org/abs/2510.10025)
*Jiaqi Liu,Lanruo Wang,Su Liu,Xin Hu*

Main category: cs.CL

TL;DR: 本文研究了轻量级模型在医疗摘要分类中的应用，评估了压缩模型在成本和隐私限制下的性能表现。


<details>
  <summary>Details</summary>
Motivation: 在医疗场景中部署大型语言模型因成本、延迟和隐私限制较难，探索轻量级模型的效果和实用性。

Method: 基于公开医疗摘要数据集，微调BERT base和DistilBERT，采用三种损失函数（交叉熵、加权交叉熵和焦点损失），固定分词器、序列长度、优化器和训练计划。

Result: DistilBERT配合普通交叉熵损失在测试集上取得最佳效果，参数量远低于BERT base。同时提供了准确率、宏平均F1和加权F1，并公开了评测代码及混淆矩阵分析。

Conclusion: 建议优先使用轻量级编码器结合交叉熵损失作为默认方案，再根据需求加入校准和任务特定检查，只有在必要时采用更重的模型。

Abstract: Large language models work well for many NLP tasks, but they are hard to
deploy in health settings with strict cost, latency, and privacy limits. We
revisit a lightweight recipe for medical abstract classification and ask how
far compact encoders can go under a controlled budget. Using the public medical
abstracts corpus, we finetune BERT base and DistilBERT with three objectives
standard cross-entropy, class weighted cross entropy, and focal loss keeping
tokenizer, sequence length, optimizer, and schedule fixed. DistilBERT with
plain cross-entropy gives the best balance on the test set while using far
fewer parameters than BERT base. We report accuracy, Macro F1, and Weighted F1,
release the evaluation code, and include confusion analyses to make error
patterns clear. Our results suggest a practical default: start with a compact
encoder and cross-entropy, then add calibration and task-specific checks before
moving to heavier models.

</details>


### [35] [HUME: Measuring the Human-Model Performance Gap in Text Embedding Task](https://arxiv.org/abs/2510.10062)
*Adnan El Assadi,Isaac Chung,Roman Solomatin,Niklas Muennighoff,Kenneth Enevoldsen*

Main category: cs.CL

TL;DR: 本文提出了HUME框架，首次系统测量人类在文本嵌入任务中的表现，以便更准确评估模型的优劣和不足。


<details>
  <summary>Details</summary>
Motivation: 目前文本嵌入模型的评估缺乏对人类表现的参考，使得模型分数难以解读。人类在这些任务上的表现难以测量，导致比较不足。

Method: 引入HUME（Human Evaluation Framework for Text Embeddings），评估人类在16个多语言MTEB数据集上的表现，涵盖重排序、分类、聚类和语义文本相似度任务。

Result: 人类平均表现为77.6%，最佳模型为80.1%。模型在某些数据集表现接近人类上限，但在少量资源语言和部分任务上存在不足，揭示了数据集问题和模型弱点。

Conclusion: HUME提供了人类性能基线和任务难度见解，帮助更有意义地解释模型结果，促进模型与基准测试的发展。相关代码和数据集已开源。

Abstract: Comparing human and model performance offers a valuable perspective for
understanding the strengths and limitations of embedding models, highlighting
where they succeed and where they fail to capture meaning and nuance. However,
such comparisons are rarely made, as human performance on embedding tasks is
difficult to measure. To fill this gap, we introduce HUME: Human Evaluation
Framework for Text Embeddings. While frameworks like MTEB provide broad model
evaluation, they lack reliable estimates of human performance, limiting the
interpretability of model scores. We measure human performance across 16 MTEB
datasets spanning reranking, classification, clustering, and semantic textual
similarity across linguistically diverse high- and low-resource languages.
Humans achieve an average performance of 77.6% compared to 80.1% for the best
embedding model, although variation is substantial: models reach near-ceiling
performance on some datasets while struggling on others, suggesting dataset
issues and revealing shortcomings in low-resource languages. We provide human
performance baselines, insight into task difficulty patterns, and an extensible
evaluation framework that enables a more meaningful interpretation of the model
and informs the development of both models and benchmarks. Our code, dataset,
and leaderboard are publicly available at
https://github.com/embeddings-benchmark/mteb.

</details>


### [36] [CLMN: Concept based Language Models via Neural Symbolic Reasoning](https://arxiv.org/abs/2510.10063)
*Yibo Yang*

Main category: cs.CL

TL;DR: 本文提出了概念语言模型网络（CLMN），结合神经网络与符号推理，实现高性能且可解释的NLP模型，特别适用于医疗和金融领域。


<details>
  <summary>Details</summary>
Motivation: 当前NLP领域深度学习模型解释性不足，尤其在医疗和金融领域，现有基于概念的模型存在文本表示损害或语义弱化问题，且缺乏对动态概念交互（如否定和上下文）的建模。

Method: 提出CLMN框架，用连续且易理解的概念嵌入表示概念，利用模糊逻辑推理学习概念间及对决策影响的动态交互规则，增强文本特征，自动归纳可解释逻辑规则。

Result: CLMN在多个数据集和预训练语言模型上，准确率优于现有基于概念的方法，同时解释质量提升。

Conclusion: 将神经表示与符号推理整合于统一概念空间，可实现实用且透明的NLP系统。

Abstract: Deep learning has advanced NLP, but interpretability remains limited,
especially in healthcare and finance. Concept bottleneck models tie predictions
to human concepts in vision, but NLP versions either use binary activations
that harm text representations or latent concepts that weaken semantics, and
they rarely model dynamic concept interactions such as negation and context. We
introduce the Concept Language Model Network (CLMN), a neural-symbolic
framework that keeps both performance and interpretability. CLMN represents
concepts as continuous, human-readable embeddings and applies fuzzy-logic
reasoning to learn adaptive interaction rules that state how concepts affect
each other and the final decision. The model augments original text features
with concept-aware representations and automatically induces interpretable
logic rules. Across multiple datasets and pre-trained language models, CLMN
achieves higher accuracy than existing concept-based methods while improving
explanation quality. These results show that integrating neural representations
with symbolic reasoning in a unified concept space can yield practical,
transparent NLP systems.

</details>


### [37] [Unilaw-R1: A Large Language Model for Legal Reasoning with Reinforcement Learning and Iterative Inference](https://arxiv.org/abs/2510.10072)
*Hua Cai,Shuang Zhao,Liang Zhang,Xuli Shen,Qing Xu,Weilin Shen,Zihao Wen,Tianke Ban*

Main category: cs.CL

TL;DR: 本文介绍了面向法律推理的大型语言模型Unilaw-R1，采用7亿参数规模，通过高质量数据集和两阶段训练策略提升法律推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在复杂法律问题处理上能力不足，存在法律知识不足、推理逻辑不可靠和业务泛化能力弱的问题。

Method: 构建包含1.7万条链式思维示例的数据集Unilaw-R1-Data，采用监督微调与强化学习相结合的两阶段训练策略；设计专门的法律推理评测基准Unilaw-R1-Eval。

Result: Unilaw-R1在权威法律推理基准上表现优异，超过同规模模型，并与大规模模型DeepSeek-R1-Distill-Qwen-32B表现相当；在LawBench和LexEval评测中明显优于Qwen-2.5-7B-Instruct。

Conclusion: 通过专门的数据集和训练策略，Unilaw-R1有效提升了中小规模模型在复杂法律推理任务中的表现，支持可解释的法律决策，具有较强的实际应用潜力。

Abstract: Reasoning-focused large language models (LLMs) are rapidly evolving across
various domains, yet their capabilities in handling complex legal problems
remains underexplored. In this paper, we introduce Unilaw-R1, a large language
model tailored for legal reasoning. With a lightweight 7-billion parameter
scale, Unilaw-R1 significantly reduces deployment cost while effectively
tackling three core challenges in the legal domain: insufficient legal
knowledge, unreliable reasoning logic, and weak business generalization. To
address these issues, we first construct Unilaw-R1-Data, a high-quality dataset
containing 17K distilled and screened chain-of-thought (CoT) samples. Based on
this, we adopt a two-stage training strategy combining Supervised Fine-Tuning
(SFT) and Reinforcement Learning (RL), which significantly boosts the
performance on complex legal reasoning tasks and supports interpretable
decision-making in legal AI applications. To assess legal reasoning ability, we
also introduce Unilaw-R1-Eval, a dedicated benchmark designed to evaluate
models across single- and multi-choice legal tasks. Unilaw-R1 demonstrates
strong results on authoritative benchmarks, outperforming all models of similar
scale and achieving performance on par with the much larger
DeepSeek-R1-Distill-Qwen-32B (54.9%). Following domain-specific training, it
also showed significant gains on LawBench and LexEval, exceeding
Qwen-2.5-7B-Instruct (46.6%) by an average margin of 6.6%.

</details>


### [38] [A-IPO: Adaptive Intent-driven Preference Optimization](https://arxiv.org/abs/2510.10077)
*Wenqing Wang,Muhammad Asif Ali,Ali Shoker,Ruohan Yang,Junyang Chen,Ying Sha,Huan Wang*

Main category: cs.CL

TL;DR: 本文提出了一种名为A-IPO的新方法，通过引入意图模块，显式捕捉用户潜在意图，提升偏好模型对用户多样化意图的对齐效果，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有偏好对齐方法多忽视少数意见及用户隐含意图，导致对多数观点的偏向和用户真实意图捕捉不足。

Method: 提出A-IPO方法，包含一个意图模块，用于推断用户提示背后的潜在意图，并将意图信息纳入奖励函数，增强模型响应与用户意图的一致性，同时提升偏好边际。

Result: 在三个数据集Real-pref、Attack-pref和扩展的GlobalOpinionQA-Ext上，A-IPO较基线方法表现显著提升，赢率、意图一致性和防御成功率等关键指标均有大幅度提高。

Conclusion: A-IPO通过明确建模多样化用户意图，实现了更加多元和鲁棒的偏好优化，显著优于现有方法，在实际和对抗偏好对齐中表现突出。

Abstract: Human preferences are diverse and dynamic, shaped by regional, cultural, and
social factors. Existing alignment methods like Direct Preference Optimization
(DPO) and its variants often default to majority views, overlooking minority
opinions and failing to capture latent user intentions in prompts.
  To address these limitations, we introduce \underline{\textbf{A}}daptive
\textbf{\underline{I}}ntent-driven \textbf{\underline{P}}reference
\textbf{\underline{O}}ptimization (\textbf{A-IPO}). Specifically,A-IPO
introduces an intention module that infers the latent intent behind each user
prompt and explicitly incorporates this inferred intent into the reward
function, encouraging stronger alignment between the preferred model's
responses and the user's underlying intentions. We demonstrate, both
theoretically and empirically, that incorporating an intention--response
similarity term increases the preference margin (by a positive shift of
$\lambda\,\Delta\mathrm{sim}$ in the log-odds), resulting in clearer separation
between preferred and dispreferred responses compared to DPO.
  For evaluation, we introduce two new benchmarks, Real-pref, Attack-pref along
with an extended version of an existing dataset, GlobalOpinionQA-Ext, to assess
real-world and adversarial preference alignment.
  Through explicit modeling of diverse user intents,A-IPO facilitates
pluralistic preference optimization while simultaneously enhancing adversarial
robustness in preference alignment. Comprehensive empirical evaluation
demonstrates that A-IPO consistently surpasses existing baselines, yielding
substantial improvements across key metrics: up to +24.8 win-rate and +45.6
Response-Intention Consistency on Real-pref; up to +38.6 Response Similarity
and +52.2 Defense Success Rate on Attack-pref; and up to +54.6 Intention
Consistency Score on GlobalOpinionQA-Ext.

</details>


### [39] [Diversity Augmentation of Dynamic User Preference Data for Boosting Personalized Text Summarizers](https://arxiv.org/abs/2510.10082)
*Parthiv Chatterjee,Shivam Sonawane,Amey Hengle,Aditya Tanna,Sourish Dasgupta,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 本文提出了一种名为PerAugy的数据增强方法，通过交叉轨迹洗牌和摘要内容扰动，提升个性化文档摘要模型的性能。


<details>
  <summary>Details</summary>
Motivation: 个性化摘要面临训练数据稀缺问题，尤其缺少既包含用户偏好历史又含目标摘要的多样化数据，限制模型的泛化能力。

Method: 设计了PerAugy方法，通过在训练数据中进行跨轨迹洗牌和摘要内容扰动，增强数据多样性，从而提升用户编码器的准确率。

Result: 使用PerAugy改善的用户编码器使四个最先进基线方法的性能显著提升，个性化指标平均提升61.2%；引入的数据多样性指标与性能提升高度相关。

Conclusion: 增强训练数据的多样性是提升个性化摘要模型性能的关键，PerAugy方法通过数据增强有效解决了数据稀缺问题，推动了个性化摘要的发展。

Abstract: Document summarization enables efficient extraction of user-relevant content
but is inherently shaped by individual subjectivity, making it challenging to
identify subjective salient information in multifaceted documents. This
complexity underscores the necessity for personalized summarization. However,
training models for personalized summarization has so far been challenging,
particularly because diverse training data containing both user preference
history (i.e., click-skip trajectory) and expected (gold-reference) summaries
are scarce. The MS/CAS PENS dataset is a valuable resource but includes only
preference history without target summaries, preventing end-to-end supervised
learning, and its limited topic-transition diversity further restricts
generalization. To address this, we propose $\mathrm{PerAugy}$, a novel
cross-trajectory shuffling and summary-content perturbation based data
augmentation technique that significantly boosts the accuracy of four
state-of-the-art baseline (SOTA) user-encoders commonly used in personalized
summarization frameworks (best result: $\text{0.132}$$\uparrow$ w.r.t AUC). We
select two such SOTA summarizer frameworks as baselines and observe that when
augmented with their corresponding improved user-encoders, they consistently
show an increase in personalization (avg. boost: $\text{61.2\%}\uparrow$ w.r.t.
PSE-SU4 metric). As a post-hoc analysis of the role of induced diversity in the
augmented dataset by \peraugy, we introduce three dataset diversity metrics --
$\mathrm{TP}$, $\mathrm{RTC}$, and \degreed\ to quantify the induced diversity.
We find that $\mathrm{TP}$ and $\mathrm{DegreeD}$ strongly correlate with
user-encoder performance on the PerAugy-generated dataset across all accuracy
metrics, indicating that increased dataset diversity is a key factor driving
performance gains.

</details>


### [40] [Stop When Enough: Adaptive Early-Stopping for Chain-of-Thought Reasoning](https://arxiv.org/abs/2510.10103)
*Renliang Sun,Wei Cheng,Dawei Li,Haifeng Chen,Wei Wang*

Main category: cs.CL

TL;DR: 该论文提出了REFRAIN框架，通过自适应判断何时停止链式思维推理，减少过度推理带来的计算资源浪费和错误结论。


<details>
  <summary>Details</summary>
Motivation: 链式思维推理虽然提升了大型语言模型在推理任务上的表现，但过度或冗余推理（过度思考）会增加推理成本并可能导致错误。

Method: REFRAIN结合两阶段停止判别器和滑动窗口上置信界（SW-UCB）多臂赌博机控制器，自适应调整停止阈值，无需训练或微调。

Result: 在四个基准测试和两个模型族上，REFRAIN减少了20-55%的token使用量，同时保持或提高准确率，且在模型、评分器及提示变化下表现稳定。

Conclusion: 该研究提出了推理停止时机判断作为测试时扩展的新维度，实现模型推理“适度而有效”，不仅让模型推理更多，也让其推理刚好足够。

Abstract: Chain-of-Thought (CoT) reasoning has driven recent gains of large language
models (LLMs) on reasoning-intensive tasks by externalizing intermediate steps.
However, excessive or redundant reasoning -- so-called overthinking -- can
increase inference costs and lead LLMs toward incorrect conclusions. In this
paper, we present REFRAIN ($\underline{REF}$lective-$\underline{R}$edundancy
for $\underline{A}$daptive $\underline{IN}$ference), a training-free framework
that adaptively determines when to stop reasoning to mitigate overthinking.
REFRAIN integrates a two-stage stop discriminator to identify reflective yet
redundant reasoning and a sliding-window Upper Confidence Bound (SW-UCB)
multi-armed bandit controller to dynamically adjust stopping thresholds
according to problem difficulty without supervision or fine-tuning. Across four
representative benchmarks and two model families, REFRAIN reduces token usage
by 20-55% while maintaining or improving accuracy compared to standard CoT
prompting. Extensive ablation and robustness analyses demonstrate its stability
across models, scorers, and prompt variations. In summary, our findings
highlight when-to-stop as a new and practical axis of test-time scaling --
enabling models to reason not just more, but just enough.

</details>


### [41] [LinearRAG: Linear Graph Retrieval Augmented Generation on Large-scale Corpora](https://arxiv.org/abs/2510.10114)
*Luyao Zhuang,Shengyuan Chen,Yilin Xiao,Huachi Zhou,Yujing Zhang,Hao Chen,Qinggang Zhang,Xiao Huang*

Main category: cs.CL

TL;DR: LinearRAG提出了一种无需关系抽取的简化图结构，提升了大规模非结构化语料的检索增强生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的RAG系统依赖不稳定且成本高昂的关系抽取，导致构建的图噪声大，影响检索质量。

Method: LinearRAG通过轻量级实体抽取和语义链接构建无关系层次图（Tri-Graph），采用两阶段检索策略：局部语义激活实体和全局重要性聚合检索段落。

Result: 在四个数据集上，LinearRAG显著优于基线模型，表现出更高的检索精准度和效率。

Conclusion: LinearRAG提供了一种高效可靠的图构建与检索框架，适合处理大规模碎片化信息，提高RAG系统的实用性和性能。

Abstract: Retrieval-Augmented Generation (RAG) is widely used to mitigate
hallucinations of Large Language Models (LLMs) by leveraging external
knowledge. While effective for simple queries, traditional RAG systems struggle
with large-scale, unstructured corpora where information is fragmented. Recent
advances incorporate knowledge graphs to capture relational structures,
enabling more comprehensive retrieval for complex, multi-hop reasoning tasks.
However, existing graph-based RAG (GraphRAG) methods rely on unstable and
costly relation extraction for graph construction, often producing noisy graphs
with incorrect or inconsistent relations that degrade retrieval quality. In
this paper, we revisit the pipeline of existing GraphRAG systems and propose
LinearRAG (Linear Graph-based Retrieval-Augmented Generation), an efficient
framework that enables reliable graph construction and precise passage
retrieval. Specifically, LinearRAG constructs a relation-free hierarchical
graph, termed Tri-Graph, using only lightweight entity extraction and semantic
linking, avoiding unstable relation modeling. This new paradigm of graph
construction scales linearly with corpus size and incurs no extra token
consumption, providing an economical and reliable indexing of the original
passages. For retrieval, LinearRAG adopts a two-stage strategy: (i) relevant
entity activation via local semantic bridging, followed by (ii) passage
retrieval through global importance aggregation. Extensive experiments on four
datasets demonstrate that LinearRAG significantly outperforms baseline models.

</details>


### [42] [Hybrid OCR-LLM Framework for Enterprise-Scale Document Information Extraction Under Copy-heavy Task](https://arxiv.org/abs/2510.10138)
*Zilong Wang,Xiaoyu Shen*

Main category: cs.CL

TL;DR: 本文提出了一种结合OCR引擎与大语言模型（LLMs）的系统框架，针对大量结构相似的重复性文档内容，优化信息抽取的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 企业处理海量复制内容的文档时，如何提高信息抽取的准确率和效率是一个关键但研究不足的挑战。

Method: 本文提出了基于文档特性选择策略的方法，结合直接抽取、替换和基于表格的三种抽取范式，评估25种配置，利用PaddleOCR与LLMs结合完成信息抽取。

Result: 基于表格的抽取方法在结构化文档中达到F1=1.0，处理速度0.97秒；在图像输入中达到F1=0.997，速度0.6秒，性能提升54倍，满足生产级异构文档流处理需求。

Conclusion: 通过结构感知的方法选择，将重复性高的复制文档信息抽取任务从计算负担转变为优化机会，实现了高效精准的企业文档处理新范式。

Abstract: Information extraction from copy-heavy documents, characterized by massive
volumes of structurally similar content, represents a critical yet understudied
challenge in enterprise document processing. We present a systematic framework
that strategically combines OCR engines with Large Language Models (LLMs) to
optimize the accuracy-efficiency trade-off inherent in repetitive document
extraction tasks. Unlike existing approaches that pursue universal solutions,
our method exploits document-specific characteristics through intelligent
strategy selection. We implement and evaluate 25 configurations across three
extraction paradigms (direct, replacement, and table-based) on identity
documents spanning four formats (PNG, DOCX, XLSX, PDF). Through table-based
extraction methods, our adaptive framework delivers outstanding results: F1=1.0
accuracy with 0.97s latency for structured documents, and F1=0.997 accuracy
with 0.6 s for challenging image inputs when integrated with PaddleOCR, all
while maintaining sub-second processing speeds. The 54 times performance
improvement compared with multimodal methods over naive approaches, coupled
with format-aware routing, enables processing of heterogeneous document streams
at production scale. Beyond the specific application to identity extraction,
this work establishes a general principle: the repetitive nature of copy-heavy
tasks can be transformed from a computational burden into an optimization
opportunity through structure-aware method selection.

</details>


### [43] [DiffHeads: Differential Analysis and Inference-Time Masking of Bias Heads in Large Language Models](https://arxiv.org/abs/2510.10142)
*Tingxu Han,Wei Song,Ziqi Ding,Ziming Li,Chunrong Fang,Yuekang Li,Dongfang Liu,Zhenyu Chen,Zhenting Wang*

Main category: cs.CL

TL;DR: 本文研究大型语言模型中的不公平问题，分析偏见产生机制，提出了一种轻量级的去偏框架DiffHeads，显著减少模型偏见同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽能检测偏见输出，但缺乏对偏见产生机制的理解，导致去偏方法脆弱。需要深入探究偏见产生的内在机制，从而实现更有效的去偏。

Method: 本文通过比较Direct-Answer（DA）和Chain-of-Thought（CoT）两种提示方式，定义token-to-head贡献分数，追踪每个token对注意力头的影响，发现一小部分偏见注意力头在DA下激活。基于此，提出DiffHeads方法，通过差异激活分析识别偏见头并选择性屏蔽它们。

Result: DA提示会激活偏见注意力头，导致偏见提升534.5%-391.9%，而CoT相对抑制偏见。DiffHeads去偏框架在DA和CoT下分别减少不公平性49.4%和40.3%，且不损害模型性能。

Conclusion: DiffHeads通过识别与屏蔽偏见注意力头，有效治理大型语言模型中的偏见现象，在保持模型效用的同时显著降低不公平性，提供了偏见治理的新思路。

Abstract: Large language models (LLMs) increasingly mediate decisions in domains where
unfair treatment of demographic groups is unacceptable. Existing work probes
when biased outputs appear, but gives little insight into the mechanisms that
generate them, leaving existing mitigations largely fragile. In this paper, we
conduct a systematic investigation LLM unfairness and propose DiffHeads, a
lightweight debiasing framework for LLMs. We first compare Direct-Answer (DA)
prompting to Chain-of-Thought (CoT) prompting across eight representative open-
and closed-source LLMs. DA will trigger the nature bias part of LLM and improve
measured unfairness by 534.5%-391.9% in both one-turn and two-turn dialogues.
Next, we define a token-to-head contribution score that traces each token's
influence back to individual attention heads. This reveals a small cluster of
bias heads that activate under DA but stay largely dormant with CoT, providing
the first causal link between prompting strategy and bias emergence. Finally,
building on this insight, we propose DiffHeads that identifies bias heads
through differential activation analysis between DA and CoT, and selectively
masks only those heads. DiffHeads reduces unfairness by 49.4%, and 40.3% under
DA and CoT, respectively, without harming model utility.

</details>


### [44] [BILLY: Steering Large Language Models via Merging Persona Vectors for Creative Generation](https://arxiv.org/abs/2510.10157)
*Tsung-Min Pai,Jui-I Wang,Li-Chun Lu,Shao-Hua Sun,Hung-Yi Lee,Kai-Wei Chang*

Main category: cs.CL

TL;DR: 该论文提出了BILLY，一种无需训练的框架，通过在单一大语言模型中融合多个个性向量，模拟多模型协作，从而提升创造力并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 多大语言模型系统虽然能模拟人类集体智慧提升创造力，但存在计算资源高和推理延迟大等缺点。

Method: BILLY框架直接在模型激活空间提取并融合多个个性向量，通过融合的向量引导单模型生成过程，实现多视角输出，无需多模型通信。

Result: 在多个创意相关基准测试中，BILLY优于单模型提示和传统多模型方法，显著减少推理时间和计算成本。

Conclusion: 融合不同个性向量不仅能有效控制生成过程中的互补特性，还提升了模型的可解释性，实现了提升创造力和效率的统一。

Abstract: Multi-LLM systems enhance the creativity of large language models by
simulating human collective intelligence but suffer from significant drawbacks,
such as high computational costs and inference latency. To address these
limitations, we propose BILLY (BlendIng persona vectors for Large Language
model creativitY), a training-free framework that captures the benefits of
multi-LLM collaboration, i.e. inducing diverse perspectives and specialized
expertise, within a single model. BILLY operates by extracting and blending
multiple distinct persona vectors directly in the model's activation space. We
steer the model's generation process with this merged vector while inference,
enabling multi-perspective output without explicit multi-LLM communication. Our
experiments across creativity-oriented benchmarks demonstrate that BILLY
surpasses single model prompting and traditional multi-LLM approaches, while
substantially reducing inference time and computational costs. Our analyses
further reveal that distinct persona vectors can be blended to achieve both
effective control over complementary aspects of generation and greater
interpretability.

</details>


### [45] [BabyBabelLM: A Multilingual Benchmark of Developmentally Plausible Training Data](https://arxiv.org/abs/2510.10159)
*Jaap Jumelet,Abdellah Fourtassi,Akari Haga,Bastian Bunzeck,Bhargav Shandilya,Diana Galvan-Sosa,Faiz Ghifari Haznitrama,Francesca Padovani,Francois Meyer,Hai Hu,Julen Etxaniz,Laurent Prévot,Linyang He,María Grandury,Mila Marcheva,Negar Foroutan,Nikitas Theodoropoulos,Pouya Sadeghi,Siyuan Song,Suchir Salhan,Susana Zhou,Yurii Paniv,Ziyin Zhang,Arianna Bisazza,Alex Warstadt,Leshem Choshen*

Main category: cs.CL

TL;DR: BabyBabelLM是一个覆盖45种语言、模拟人类从出生到习得母语阶段语言学习的多语种数据集集合。


<details>
  <summary>Details</summary>
Motivation: 为了解决多语言预训练和认知建模中缺乏从婴幼儿视角的语言数据资源问题，提供发展合理的预训练数据。

Method: 收集并整理相当于每种语言1亿英语词量的符合发展阶段的预训练数据，编制评测套件，并训练各语言的基线模型。

Result: 成功构建了45种语言的BabyBabelLM数据集和评测套件，并训练了基线模型。

Conclusion: BabyBabelLM为多语种预训练和认知语言建模提供了基础资源，有助于相关领域的发展。

Abstract: We present BabyBabelLM, a multilingual collection of datasets modeling the
language a person observes from birth until they acquire a native language. We
curate developmentally plausible pretraining data aiming to cover the
equivalent of 100M English words of content in each of 45 languages. We compile
evaluation suites and train baseline models in each language. BabyBabelLM aims
to facilitate multilingual pretraining and cognitive modeling.

</details>


### [46] [Large Language Model Sourcing: A Survey](https://arxiv.org/abs/2510.10161)
*Liang Pang,Kangxi Wu,Sunhao Dai,Zihao Wei,Zenghao Duan,Jia Gu,Xiang Li,Zhiyi Yin,Jun Xu,Huawei Shen,Xueqi Cheng*

Main category: cs.CL

TL;DR: 本文系统调查了大型语言模型生成内容的溯源追踪，提出了从模型和数据两个视角分析的四个维度，并分类了溯源方法以提升模型透明度和可信度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在主观决策中的广泛应用带来了潜在风险，如幻觉、偏见和版权问题，迫切需要多维度的信息溯源以确保安全与可信。

Method: 从模型角度划分为模型溯源和模型结构溯源，从数据角度划分为训练数据溯源和外部数据溯源，同时提出了先验溯源和后验溯源两种范式的分类体系。

Result: 提出了系统的溯源维度框架和分类体系，概括了现有方法，促进对模型输出来源的清晰追踪。

Conclusion: 多维度的溯源追踪方法能显著提升大型语言模型部署的透明度、责任感和可信性，助力其安全可靠的应用。

Abstract: The rapid advancement of large language models (LLMs) has revolutionized
artificial intelligence, shifting from supporting objective tasks (e.g.,
recognition) to empowering subjective decision-making (e.g., planning,
decision). This marks the dawn of general and powerful AI, with applications
spanning a wide range of fields, including programming, education, healthcare,
finance, and law. However, their deployment introduces multifaceted risks. Due
to the black-box nature of LLMs and the human-like quality of their generated
content, issues such as hallucinations, bias, unfairness, and copyright
infringement become particularly significant. In this context, sourcing
information from multiple perspectives is essential.
  This survey presents a systematic investigation into provenance tracking for
content generated by LLMs, organized around four interrelated dimensions that
together capture both model- and data-centric perspectives. From the model
perspective, Model Sourcing treats the model as a whole, aiming to distinguish
content generated by specific LLMs from content authored by humans. Model
Structure Sourcing delves into the internal generative mechanisms, analyzing
architectural components that shape the outputs of model. From the data
perspective, Training Data Sourcing focuses on internal attribution, tracing
the origins of generated content back to the training data of model. In
contrast, External Data Sourcing emphasizes external validation, identifying
external information used to support or influence the responses of model.
Moreover, we also propose a dual-paradigm taxonomy that classifies existing
sourcing methods into prior-based (proactive traceability embedding) and
posterior-based (retrospective inference) approaches. Traceability across these
dimensions enhances the transparency, accountability, and trustworthiness of
LLMs deployment in real-world applications.

</details>


### [47] [A Survey of Inductive Reasoning for Large Language Models](https://arxiv.org/abs/2510.10182)
*Kedi Chen,Dezhao Ruan,Yuhao Dan,Yaoting Wang,Siyu Yan,Xuecheng Wu,Yinqi Zhang,Qin Chen,Jie Zhou,Liang He,Biqing Qi,Linyang Li,Qipeng Guo,Xiaoming Shi,Wei Zhang*

Main category: cs.CL

TL;DR: 本文首次系统综述了大语言模型的归纳推理，涵盖改进方法、评测基准及分析归纳能力来源。


<details>
  <summary>Details</summary>
Motivation: 归纳推理是大语言模型推理的重要基础，缺少系统总结，且对知识泛化和人类认知高度契合，亟需全面研究。

Method: 将提升归纳推理的方法归纳为训练后改进、测试时扩展和数据增强三类，总结现有评测基准，提出统一的沙盒及覆盖率评估方法，并分析归纳能力来源及模型架构和数据的作用。

Result: 整理了现有方法和评测手段，提出统一且覆盖全面的评测框架，为归纳推理研究建立了坚实基础。

Conclusion: 本文系统总结了归纳推理领域的进展和挑战，揭示了归纳能力的本质和提升途径，为未来相关研究指明方向。

Abstract: Reasoning is an important task for large language models (LLMs). Among all
the reasoning paradigms, inductive reasoning is one of the fundamental types,
which is characterized by its particular-to-general thinking process and the
non-uniqueness of its answers. The inductive mode is crucial for knowledge
generalization and aligns better with human cognition, so it is a fundamental
mode of learning, hence attracting increasing interest. Despite the importance
of inductive reasoning, there is no systematic summary of it. Therefore, this
paper presents the first comprehensive survey of inductive reasoning for LLMs.
First, methods for improving inductive reasoning are categorized into three
main areas: post-training, test-time scaling, and data augmentation. Then,
current benchmarks of inductive reasoning are summarized, and a unified
sandbox-based evaluation approach with the observation coverage metric is
derived. Finally, we offer some analyses regarding the source of inductive
ability and how simple model architectures and data help with inductive tasks,
providing a solid foundation for future research.

</details>


### [48] [Weed Out, Then Harvest: Dual Low-Rank Adaptation is an Effective Noisy Label Detector for Noise-Robust Learning](https://arxiv.org/abs/2510.10208)
*Bo Yuan,Yulin Chen,Yin Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种名为Delora的框架，通过引入干净和噪声LoRA模块，有效解决了有噪声标签数据中样本选择与模型训练相互影响的问题，实现了更优的文本分类性能。


<details>
  <summary>Details</summary>
Motivation: 当前大规模语言模型在有噪声标签的真实场景下，样本选择依赖于小损失样本，导致初始选择不准确时形成恶性循环，影响模型训练效果。

Method: Delora通过引入干净LoRA和噪声LoRA，分别记忆干净样本和错误标签，实现样本选择与模型训练的解耦；利用噪声LoRA作为可学习的阈值来选择干净和噪声样本，并用筛选出的样本对模型进行微调。

Result: 在合成及真实噪声数据集上的实验结果显示，Delora在噪声标签检测和文本分类任务中表现出优越的性能。

Conclusion: Delora有效打破了样本选择与模型训练之间的恶性循环，通过模块解耦提升了对噪声标签的鲁棒性，为大规模语言模型在噪声环境下的微调提供了新思路。

Abstract: Parameter-efficient fine-tuning (PEFT) large language models (LLMs) have
shown impressive performance in various downstream tasks. However, in many
real-world scenarios, the collected training data inevitably contains noisy
labels. To learn from noisy labels, most solutions select samples with small
losses for model training. However, the selected samples, in turn, impact the
loss computation in the next iteration. An inaccurate initial selection can
create a vicious cycle, leading to suboptimal performance. To break this cycle,
we propose Delora, a novel framework that decouples the sample selection from
model training. For sample selection, Delora establishes a noisy label detector
by introducing clean and noisy LoRA. Benefiting from the memory effect, the
clean LoRA is encouraged to memorize clean data, while the noisy LoRA is
constrained to memorize mislabeled data, which serves as a learnable threshold
for selecting clean and noisy samples. For model training, Delora can use
carefully selected samples to fine-tune language models seamlessly.
Experimental results on synthetic and real-world noisy datasets demonstrate the
effectiveness of Delora in noisy label detection and text classification.

</details>


### [49] [You only need 4 extra tokens: Synergistic Test-time Adaptation for LLMs](https://arxiv.org/abs/2510.10223)
*Yijie Xu,Huizai Yao,Zhiyu Guo,Weiyu Guo,Pengteng Li,Aiwei Liu,Xuming Hu,Hui Xiong*

Main category: cs.CL

TL;DR: 本文提出了SyTTA，一种无标签测试时自适应大语言模型的方法，利用输入端的困惑度和输出端的预测熵进行在线调整，在多个领域实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在专业领域面临分布偏移问题，且标注数据稀缺使得传统的领域微调困难，因此探索无标签的测试时自适应方法具有重要意义。

Method: SyTTA在推理阶段结合输入侧的困惑度和输出侧的预测熵两种不确定性信号，无需额外监督，动态适应分布偏移。

Result: 在多个模型架构和领域基准测试中，SyTTA均带来稳定提升，尤其在农业问答任务上，Qwen-2.5-7B模型的Rouge-LSum提高超过120%。

Conclusion: SyTTA证明了在缺乏标签的情况下，实现大语言模型有效的测试时自适应是可行的，促进其在标注稀缺领域的应用。

Abstract: Large language models (LLMs) are increasingly deployed in specialized domains
such as finance, medicine, and agriculture, where they face significant
distribution shifts from their training data. Domain-specific fine-tuning can
mitigate this challenge but relies on high-quality labeled data that is
expensive and slow to collect in expertise-limited settings. We study
label-free test-time adaptation for language models and present SyTTA, an
inference-time framework that adapts models on-the-fly without additional
supervision. SyTTA couples two complementary uncertainty signals that arise
under distribution shift: input-side perplexity, indicating mismatch with
domain-specific terminology and patterns, and output-side predictive entropy,
indicating diffuse and unstable token probabilities during generation. Across
diverse model architectures and domain-specific benchmarks, SyTTA delivers
consistent gains. Notably, on agricultural question answering, SyTTA improves
Rouge-LSum by over 120% on Qwen-2.5-7B with only 4 extra tokens per query.
These results show that effective test-time adaptation for language models is
achievable without labeled examples, supporting deployment in label-scarce
domains. The code will be made available upon acceptance.

</details>


### [50] [Text2Token: Unsupervised Text Representation Learning with Token Target Prediction](https://arxiv.org/abs/2510.10224)
*Ruize An,Richong Zhang,Zhijie Nie,Zhanyu Wu,Yanzhao Zhang,Dingkun Long*

Main category: cs.CL

TL;DR: 本文提出了一种基于无监督生成框架Text2Token的文本表示学习方法，通过构建高质量的目标词分布实现了与先进嵌入模型相媲美的表现。


<details>
  <summary>Details</summary>
Motivation: 研究发现高质量的文本表示与输入文本的关键词对齐，揭示了表示空间和词汇空间间的潜在联系，促使作者重新审视生成任务以改进文本表示学习。

Method: 基于关键词预测任务，利用精心构建的目标词分布作为监督信号，提出数据驱动和模型生成两种构建合成目标词的方法，结合先进的嵌入器对关键词进行分析和选择。

Result: 在MTEB v2基准测试中，Text2Token表现出与当前最先进的无监督对比学习嵌入器LLM2Vec竞争的性能。

Conclusion: 文本表示空间与词汇空间在训练中共同优化，朝向最优解发展，为未来文本表示学习研究提供了新思路和洞见。

Abstract: Unsupervised text representation learning (TRL) is a fundamental task in
natural language processing, which is beneficial for improving search and
recommendations with the web's unlabeled texts. A recent empirical study finds
that the high-quality representation aligns with the key token of the input
text, uncovering the potential connection between representation space and
vocabulary space. Inspired by the findings, we revisit the generative tasks and
develop an unsupervised generative framework for TRL, Text2Token. The framework
is based on the token target prediction task, utilizing carefully constructed
target token distribution as supervisory signals. To construct the high-quality
target token distribution, we analyze the token-alignment properties with
advanced embedders and identify two essential categories of key tokens: (1) the
meaningful tokens in the text and (2) semantically derived tokens beyond the
text. Based on these insights, we propose two methods -- data-driven and
model-derived -- to construct synthetic token targets from data or the LLM
backbone. Experiments on the MTEB v2 benchmark demonstrate that Text2Token
achieves performance competitive with the state-of-the-art embedder with
unsupervised contrastive learning, LLM2Vec. Our analysis further shows that
vocabulary and representation spaces optimize together and toward the optimum
solution during training, providing new ideas and insights for future work.

</details>


### [51] [ImCoref-CeS: An Improved Lightweight Pipeline for Coreference Resolution with LLM-based Checker-Splitter Refinement](https://arxiv.org/abs/2510.10241)
*Kangyang Luo,Yuzhuo Bai,Shuzheng Si,Cheng Gao,Zhitong Wang,Yingli Shen,Wenhao Li,Zhu Liu,Yufeng Han,Jiayi Wu,Cunliang Kong,Maosong Sun*

Main category: cs.CL

TL;DR: 该论文提出了一个名为ImCoref-CeS的框架，结合了改进的监督神经模型和大语言模型，实现了核心指代消解任务的性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前核心指代消解研究面临是否继续优化基于小型语言模型的监督方法，还是利用大语言模型的能力，两者结合的方案尚未充分探索。

Method: 提出了ImCoref改进的监督神经方法，包括轻量级桥接模块增强长文本编码，双仿射评分器捕捉位置信息，混合提及正则化提升训练效率；并用大语言模型作为多角色的检查分割器，验证和纠正模型的预测。

Result: 通过大量实验证明，ImCoref-CeS框架在核心指代消解任务上优于现有的最先进方法。

Conclusion: 将改进的监督模型与大语言模型推理有效结合，能够显著提升核心指代消解性能，展示出强大的应用潜力。

Abstract: Coreference Resolution (CR) is a critical task in Natural Language Processing
(NLP). Current research faces a key dilemma: whether to further explore the
potential of supervised neural methods based on small language models, whose
detect-then-cluster pipeline still delivers top performance, or embrace the
powerful capabilities of Large Language Models (LLMs). However, effectively
combining their strengths remains underexplored. To this end, we propose
\textbf{ImCoref-CeS}, a novel framework that integrates an enhanced supervised
model with LLM-based reasoning. First, we present an improved CR method
(\textbf{ImCoref}) to push the performance boundaries of the supervised neural
method by introducing a lightweight bridging module to enhance long-text
encoding capability, devising a biaffine scorer to comprehensively capture
positional information, and invoking a hybrid mention regularization to improve
training efficiency. Importantly, we employ an LLM acting as a multi-role
Checker-Splitter agent to validate candidate mentions (filtering out invalid
ones) and coreference results (splitting erroneous clusters) predicted by
ImCoref. Extensive experiments demonstrate the effectiveness of ImCoref-CeS,
which achieves superior performance compared to existing state-of-the-art
(SOTA) methods.

</details>


### [52] [Audit-of-Understanding: Posterior-Constrained Inference for Mathematical Reasoning in Language Models](https://arxiv.org/abs/2510.10252)
*Samir Abdaljalil,Erchin Serpedin,Khalid Qaraqe,Hasan Kurban*

Main category: cs.CL

TL;DR: 本文提出了Audit-of-Understanding（AoU）框架，通过验证推理前提来减少大语言模型中的幻觉推理。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在推理过程中常常生成看似合理但缺乏支持的假设，导致结论不准确，现有研究多关注事实性幻觉，缺少针对推理性幻觉的解决方法。

Method: AoU框架包括三个阶段：将查询分解为假设，审计这些假设的支持度，只对验证通过的假设进行推理。提出后验约束推理方法，结合选择性预测和拒绝学习，提供理论保证和复杂性分析。

Result: 在GSM8K、MultiArith和SVAMP数据集上，AoU显著提升了准确率和推理可信度，分别最高提升30%、45%和20-28%，优于现有方法如链式思维和一致性解码。

Conclusion: AoU有效减少了推理过程中基于不支持假设的幻觉结论，提升了大语言模型推理的准确性和可信度，具有较好的理论和实践价值。

Abstract: Large language models (LLMs) often generate reasoning traces that appear
coherent but rest on unsupported assumptions, leading to hallucinated
conclusions. Prior work mainly addresses factual hallucinations or relies on
post-hoc verification, leaving reasoning-induced hallucinations largely
unaddressed. We propose Audit-of-Understanding (AoU), a framework that
constrains inference to validated premises through three phases: (1)
decomposing a query into candidate assumptions, (2) auditing their support, and
(3) conditioning inference only on the validated subset. Formally, AoU is
\emph{posterior-constrained inference}, connecting to selective prediction and
rejection learning. Our contributions are threefold: (i) theoretical guarantees
under perfect validation, (ii) excess-risk bounds under imperfect audits, and
(iii) tractability analysis. Empirically, AoU improves both accuracy and
faithfulness on GSM8K, MultiArith, and SVAMP, achieving up to +30% gains on
GSM8K, +45% on MultiArith, and consistent +20--28% improvements on SVAMP over
Chain-of-Thought, Self-Consistency, and CoT-Decoding. Code is available at
https://anonymous.4open.science/r/audit-of-understanding-E28B.

</details>


### [53] [Backdoor Collapse: Eliminating Unknown Threats via Known Backdoor Aggregation in Language Models](https://arxiv.org/abs/2510.10265)
*Liang Lin,Miao Yu,Moayad Aloqaily,Zhenhong Zhou,Kun Wang,Linsey Pang,Prakhar Mehrotra,Qingsong Wen*

Main category: cs.CL

TL;DR: 提出了一种无需先验触发器信息的防护大型语言模型后门攻击的方法，称为\ourmethod，通过注入已知后门触发器聚集后门表示，并进行微调恢复正常输出，有效降低攻击成功率且保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有防护大型语言模型后门攻击的方法依赖不切实际的触发器设定假设，难以应对实际应用中未知后门攻击。

Method: 基于观察到已感染模型中未知和新注入后门表示在表示空间聚集，提出两阶段防御：先注入已知触发器聚集后门表示，再进行恢复微调以恢复无害输出。

Result: 在多种大型语言模型架构和基准测试中，\ourmethod将平均攻击成功率降低至4.41%，优于现有方法28.1%~69.3%，且模型的清洁准确率和实用性仅降低0.5%。

Conclusion: \ourmethod无需事先了解触发器设置即可有效防御大型语言模型后门攻击，且对模型性能影响极小，具备实际部署的鲁棒性和通用性。

Abstract: Backdoor attacks are a significant threat to large language models (LLMs),
often embedded via public checkpoints, yet existing defenses rely on
impractical assumptions about trigger settings. To address this challenge, we
propose \ourmethod, a defense framework that requires no prior knowledge of
trigger settings. \ourmethod is based on the key observation that when
deliberately injecting known backdoors into an already-compromised model, both
existing unknown and newly injected backdoors aggregate in the representation
space. \ourmethod leverages this through a two-stage process: \textbf{first},
aggregating backdoor representations by injecting known triggers, and
\textbf{then}, performing recovery fine-tuning to restore benign outputs.
Extensive experiments across multiple LLM architectures demonstrate that: (I)
\ourmethod reduces the average Attack Success Rate to 4.41\% across multiple
benchmarks, outperforming existing baselines by 28.1\%$\sim$69.3\%$\uparrow$.
(II) Clean accuracy and utility are preserved within 0.5\% of the original
model, ensuring negligible impact on legitimate tasks. (III) The defense
generalizes across different types of backdoors, confirming its robustness in
practical deployment scenarios.

</details>


### [54] [On the Entity-Level Alignment in Crosslingual Consistency](https://arxiv.org/abs/2510.10280)
*Yihong Liu,Mingyang Wang,François Yvon,Hinrich Schütze*

Main category: cs.CL

TL;DR: 本文研究了多语言大型语言模型在跨语言事实知识一致性方面的问题，发现实体对齐失败是其重要原因。提出两种方法通过引入主体实体的英语翻译提高一致性和准确度。


<details>
  <summary>Details</summary>
Motivation: 多语言大型语言模型在不同语言间保持事实知识一致性存在问题，但导致这种不一致的原因尚不清楚。

Method: 通过评估主体和客体实体的翻译对齐情况，分析对齐与事实一致性的关联；提出SubSub和SubInj方法，将英语主体翻译融入跨语言提示。

Result: 发现实体对齐良好与事实一致性高度相关，错误对齐常导致不一致。新方法显著提升事实回忆准确率和一致性。

Conclusion: 实体对齐失败是跨语言一致性问题的主要原因，利用英语作为内部枢纽语言强化实体表示对齐能有效改善多语言事实预测。

Abstract: Multilingual large language models (LLMs) are expected to recall factual
knowledge consistently across languages. However, the factors that give rise to
such crosslingual consistency -- and its frequent failure -- remain poorly
understood. In this work, we hypothesize that these inconsistencies may arise
from failures in entity alignment, the process of mapping subject and object
entities into a shared conceptual space across languages. To test this, we
assess alignment through entity-level (subject and object) translation tasks,
and find that consistency is strongly correlated with alignment across all
studied models, with misalignment of subjects or objects frequently resulting
in inconsistencies. Building on this insight, we propose SubSub and SubInj, two
effective methods that integrate English translations of subjects into prompts
across languages, leading to substantial gains in both factual recall accuracy
and consistency. Finally, our mechanistic analysis reveals that these
interventions reinforce the entity representation alignment in the conceptual
space through model's internal pivot-language processing, offering effective
and practical strategies for improving multilingual factual prediction.

</details>


### [55] [MatryoshkaThinking: Recursive Test-Time Scaling Enables Efficient Reasoning](https://arxiv.org/abs/2510.10293)
*Hongwei Chen,Yishu Lei,Dan Zhang,Bo Ke,Danxiang Zhu,Xuyi Chen,Yuxiang Lu,Zhengjie Huang,Shikun Feng,Jingzhou He,Yu Sun,Hua Wu,Haifeng Wang*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的测试时扩展方法MatryoshkaThinking，显著降低计算成本的同时保持了先进性能，在AIME2025测试中以4%的计算量达到NearSOTA成绩。


<details>
  <summary>Details</summary>
Motivation: 现有测试时扩展方法如DeepConf虽然效果好，但计算开销巨大，限制了实际应用。

Method: 通过递归利用模型的推理、验证和总结能力，提升答案正确率并减小通过率指标之间的差距。

Result: MatryoshkaThinking在多个开源模型和多模态推理基准测试中表现优异，尤其在AIME2025中取得99.79的高分，耗费计算仅为DeepConf的4%。

Conclusion: 该方法有效降低了推理时的计算资源需求，同时保持了模型性能，展示了高效可扩展的推理策略设计方向。

Abstract: Test-time scaling has emerged as a promising paradigm in language modeling,
wherein additional computational resources are allocated during inference to
enhance model performance. Recent approaches, such as DeepConf, have
demonstrated the efficacy of this strategy, however, they often incur
substantial computational overhead to achieve competitive results. In this
work, we propose MatryoshkaThinking, a novel method that significantly reduces
computational cost while maintaining state-of-the-art performance.
Specifically, MatryoshkaThinking attains a score of 99.79 on AIME2025 using
only 4% of the computation required by DeepConf. The core of our approach lies
in the recursive exploitation of the model's intrinsic capabilities in
reasoning, verification, and summarization, which collectively enhance the
retention of correct solutions and reduce the disparity between Pass@k and
Pass@1. Comprehensive evaluations across multiple open-source models and
challenging multi-modal reasoning benchmarks validate the effectiveness and
generality of our method. These findings offer new insights into the design of
efficient and scalable test-time inference strategies for advanced language
models.

</details>


### [56] [Are LLMs Empathetic to All? Investigating the Influence of Multi-Demographic Personas on a Model's Empathy](https://arxiv.org/abs/2510.10328)
*Ananya Malik,Nazanin Sabri,Melissa Karnaze,Mai Elsherief*

Main category: cs.CL

TL;DR: 本论文研究大语言模型（LLMs）在不同人口属性交叉背景下的共情能力差异，发现模型的共情反应受多重人口属性显著影响，且出现某些文化群体表现不一致，强调设计具包容性共情的模型的重要性。


<details>
  <summary>Details</summary>
Motivation: 不同的人口和文化背景影响情感体验，而当前大语言模型的共情能力是否能公平地覆盖多样用户群体尚不清楚。

Method: 提出一种框架，通过315种基于年龄、文化、性别交叉的人群画像对四种LLM进行认知和情感共情能力的交叉分析。

Result: 发现模型的共情表现受人口属性显著影响，多重属性叠加时可能弱化或逆转预期共情模式，且部分文化群体（如儒家文化）表现出显著偏差。

Conclusion: 研究强调设计时需考虑人口多样性，构建具共情意识且公平包容的大语言模型。

Abstract: Large Language Models' (LLMs) ability to converse naturally is empowered by
their ability to empathetically understand and respond to their users. However,
emotional experiences are shaped by demographic and cultural contexts. This
raises an important question: Can LLMs demonstrate equitable empathy across
diverse user groups? We propose a framework to investigate how LLMs' cognitive
and affective empathy vary across user personas defined by intersecting
demographic attributes. Our study introduces a novel intersectional analysis
spanning 315 unique personas, constructed from combinations of age, culture,
and gender, across four LLMs. Results show that attributes profoundly shape a
model's empathetic responses. Interestingly, we see that adding multiple
attributes at once can attenuate and reverse expected empathy patterns. We show
that they broadly reflect real-world empathetic trends, with notable
misalignments for certain groups, such as those from Confucian culture. We
complement our quantitative findings with qualitative insights to uncover model
behaviour patterns across different demographic groups. Our findings highlight
the importance of designing empathy-aware LLMs that account for demographic
diversity to promote more inclusive and equitable model behaviour.

</details>


### [57] [End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs](https://arxiv.org/abs/2510.10329)
*Nam Luu,Ondřej Bojar*

Main category: cs.CL

TL;DR: 本文提出了一种结合预训练语音编码器和大型语言模型的端到端框架，同时执行自动语音识别和语音翻译。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过端到端架构整合预训练模型提升语音翻译性能，实现ASR和ST的统一处理。

Method: 使用预训练的语音编码器与大型语言模型结合，形成端到端系统进行英语到德语的语音识别和翻译任务。

Result: 在英德翻译任务中，模型优于SeamlessM4T，且性能可与Whisper+NLLB级联系统匹敌，在COMET^{DA}_{22}指标上提升最高达8%。

Conclusion: 所提端到端模型有效提升了语音翻译性能，有望成为替代传统级联系统的优选方案。

Abstract: Speech Translation (ST) is a machine translation task that involves
converting speech signals from one language to the corresponding text in
another language; this task has two different approaches, namely the
traditional cascade and the more recent end-to-end. This paper explores a
combined end-to-end architecture of pre-trained speech encoders and Large
Language Models (LLMs) for performing both Automatic Speech Recognition (ASR)
and ST simultaneously. Experiments with the English-to-German language pair
show that our best model not only can achieve better translation results than
SeamlessM4T, a large foundational end-to-end, multi-modal translation model,
but can also match the performance of a cascaded system with Whisper and NLLB,
with up to a score gain of 8% in $\text{COMET}^{\text{DA}}_{22}$ metric.

</details>


### [58] [ASC analyzer: A Python package for measuring argument structure construction usage in English texts](https://arxiv.org/abs/2510.10384)
*Hakyung Sung,Kristopher Kyle*

Main category: cs.CL

TL;DR: 该论文介绍了ASC分析工具包，通过自动标注和索引计算，分析第二语言学习者的写作能力。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏可扩展、系统的方法来衡量第二语言中论元结构构式（ASC）的使用情况。

Method: 开发了一个公开的Python包——ASC分析器，自动标注ASC并计算多种指标，分析ASC与写作评分之间的关系。

Result: 通过双变量和多变量分析，验证了ASC指标与第二语言写作成绩之间的相关性。

Conclusion: 该工具为利用ASCs评估第二语言能力提供了有效的量化手段。

Abstract: Argument structure constructions (ASCs) offer a theoretically grounded lens
for analyzing second language (L2) proficiency, yet scalable and systematic
tools for measuring their usage remain limited. This paper introduces the ASC
analyzer, a publicly available Python package designed to address this gap. The
analyzer automatically tags ASCs and computes 50 indices that capture
diversity, proportion, frequency, and ASC-verb lemma association strength. To
demonstrate its utility, we conduct both bivariate and multivariate analyses
that examine the relationship between ASC-based indices and L2 writing scores.

</details>


### [59] [RefusalBench: Generative Evaluation of Selective Refusal in Grounded Language Models](https://arxiv.org/abs/2510.10390)
*Aashiq Muhamed,Leonardo F. R. Ribeiro,Markus Dreyer,Virginia Smith,Mona T. Diab*

Main category: cs.CL

TL;DR: 语言模型在基于检索增强生成（RAG）的系统中，在面对有误导性上下文时选择拒绝回答的能力尚不足，存在性能下降严重的问题。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型在复杂多文档任务中难以准确拒绝回答错误的上下文，且静态评测方法无法有效评估拒绝能力，存在安全隐患。

Method: 提出RefusalBench，通过176种语言扰动策略和不同不确定性强度，自动生成诊断测试用例，系统评估30多款模型的拒绝能力，揭示其拒绝包含检测和分类两个技能，且规模和推理能力提升难以改进表现。

Result: 发现模型拒绝准确率低于50%，存在过度自信或过度谨慎，拒绝能力可训练且对模型对齐敏感。

Conclusion: Selective refusal是可训练且可改进的能力，公布两个基准测试集和生成框架，促进对该关键安全能力的持续、动态评估。

Abstract: The ability of language models in RAG systems to selectively refuse to answer
based on flawed context is critical for safety, yet remains a significant
failure point. Our large-scale study reveals that even frontier models struggle
in this setting, with refusal accuracy dropping below 50% on multi-document
tasks, while exhibiting either dangerous overconfidence or overcaution. Static
benchmarks fail to reliably evaluate this capability, as models exploit
dataset-specific artifacts and memorize test instances. We introduce
RefusalBench, a generative methodology that programmatically creates diagnostic
test cases through controlled linguistic perturbation. Our framework employs
176 distinct perturbation strategies across six categories of informational
uncertainty and three intensity levels. Evaluation of over 30 models uncovers
systematic failure patterns: refusal comprises separable detection and
categorization skills, and neither scale nor extended reasoning improves
performance. We find that selective refusal is a trainable, alignment-sensitive
capability, offering a clear path for improvement. We release two benchmarks --
RefusalBench-NQ (single document) and RefusalBench-GaRAGe (multi-document) --
and our complete generation framework to enable continued, dynamic evaluation
of this critical capability.

</details>


### [60] [AssoMem: Scalable Memory QA with Multi-Signal Associative Retrieval](https://arxiv.org/abs/2510.10397)
*Kai Zhang,Xinyuan Zhang,Ejaz Ahmed,Hongda Jiang,Caleb Kumar,Kai Sun,Zhaojiang Lin,Sanat Sharma,Shereen Oraby,Aaron Colak,Ahmed Aly,Anuj Kumar,Xiaozhong Liu,Xin Luna Dong*

Main category: cs.CL

TL;DR: 本文提出了AssoMem框架，通过构建关联记忆图和多维检索信号融合，提升对大规模记忆的准确召回，实现对话场景中的上下文感知检索能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖语义距离进行检索，在相似度密集场景下表现欠佳，基于人类联想式信息关联的思路，寻求提升记忆召回准确率。

Method: 构建关联记忆图，将对话话语锚定到自动提取的线索，形成丰富的上下文组织结构，并结合多维检索信号（相关性、重要性、时间对齐）通过自适应互信息驱动的融合策略进行排名。

Result: 在三个基准数据集及新引入的MeetingQA数据集上进行广泛实验，结果显示AssoMem显著优于现有最先进方法，增强了上下文感知的记忆召回效果。

Conclusion: AssoMem通过关联图构造和多维检索信号融合，有效提升了大规模记忆的准确召回能力，适用于复杂对话问答场景。

Abstract: Accurate recall from large scale memories remains a core challenge for memory
augmented AI assistants performing question answering (QA), especially in
similarity dense scenarios where existing methods mainly rely on semantic
distance to the query for retrieval. Inspired by how humans link information
associatively, we propose AssoMem, a novel framework constructing an
associative memory graph that anchors dialogue utterances to automatically
extracted clues. This structure provides a rich organizational view of the
conversational context and facilitates importance aware ranking. Further,
AssoMem integrates multi-dimensional retrieval signals-relevance, importance,
and temporal alignment using an adaptive mutual information (MI) driven fusion
strategy. Extensive experiments across three benchmarks and a newly introduced
dataset, MeetingQA, demonstrate that AssoMem consistently outperforms SOTA
baselines, verifying its superiority in context-aware memory recall.

</details>


### [61] [STEAM: A Semantic-Level Knowledge Editing Framework for Large Language Models](https://arxiv.org/abs/2510.10398)
*Geunyeong Jeong,Juoh Sun,Seonghee Lee,Harksoo Kim*

Main category: cs.CL

TL;DR: 本文提出了一个基于语义对齐的知识编辑框架STEAM，解决大语言模型中知识更新后的语义不连贯问题。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑方法主要侧重于词元概率优化，未能保证编辑知识与已有知识的语义连贯，导致知识孤立且绕过模型自然推理过程。

Method: STEAM通过识别语义锚点作为目标表示，利用对齐损失将编辑知识的内部表示引导到语义锚点，从而增强更新知识与模型知识结构的融合。

Result: 实验表明STEAM提升了模型对编辑后知识的推理能力和语义连贯性，证明了潜在空间对齐对于可靠且连贯的知识编辑的重要性。

Conclusion: STEAM有效解决了知识编辑的语义不连贯问题，增强了大语言模型更新知识的合理性和推理能力，对于知识编辑研究具有重要意义。

Abstract: Large Language Models store extensive factual knowledge acquired during
large-scale pre-training. However, this knowledge is inherently static,
reflecting only the state of the world at the time of training. Knowledge
editing has emerged as a promising solution for updating outdated or incorrect
facts without full retraining. However, most existing locate-and-edit methods
primarily focus on token-level likelihood optimization without addressing
semantic coherence. Our analysis reveals that such edited knowledge is often
encoded as isolated residual streams in the model's latent space, distinct from
pre-existing knowledge and bypassing natural reasoning process. To address
this, we propose \textsc{Steam}, a semantic-level knowledge editing framework
that enhances integration of updated knowledge into the model's knowledge
structure. \textsc{Steam} first identifies target representations as semantic
anchors for the updated factual association, then guides the internal
representation of the edited fact towards these anchors through an alignment
loss during optimization. Experimental results demonstrate that \textsc{Steam}
improves model's ability to reason with edited knowledge and enhances semantic
coherence, underscoring the importance of latent-space alignment for reliable
and coherent knowledge editing. The code is available at
https://github.com/GY-Jeong/STEAM.

</details>


### [62] [LONGQAEVAL: Designing Reliable Evaluations of Long-Form Clinical QA under Resource Constraints](https://arxiv.org/abs/2510.10415)
*Federica Bologna,Tiffany Pan,Matthew Wilkens,Yue Guo,Lucy Lu Wang*

Main category: cs.CL

TL;DR: 本文介绍了LongQAEval，一种用于评估长篇临床问答系统的框架，针对资源有限且需要高专业知识的设置提出评估建议。


<details>
  <summary>Details</summary>
Motivation: 长篇临床问答系统的评估既耗费资源又极具挑战性，原因在于需要医疗专业知识且长文本的一致性判断困难。

Method: 基于医生对300个真实患者问题及其由医生和大语言模型回答的注释，比较了粗粒度答案级评估与细粒度句子级评估在正确性、相关性和安全性维度上的表现，并分析了评注者间一致性。

Result: 细粒度评注提高了正确性维度上的一致性，粗粒度评注则提升了相关性维度一致性，而安全性判断则仍不稳定。部分句子的注释也能达到与粗粒度类似的可靠性，降低了成本和工作量。

Conclusion: LongQAEval为临床长篇问答系统在资源有限且需要专家评审的环境中提供了有效的评估策略，可通过细粒度注释提升准确性评估，同时减少评注负担。

Abstract: Evaluating long-form clinical question answering (QA) systems is
resource-intensive and challenging: accurate judgments require medical
expertise and achieving consistent human judgments over long-form text is
difficult. We introduce LongQAEval, an evaluation framework and set of
evaluation recommendations for limited-resource and high-expertise settings.
Based on physician annotations of 300 real patient questions answered by
physicians and LLMs, we compare coarse answer-level versus fine-grained
sentence-level evaluation over the dimensions of correctness, relevance, and
safety. We find that inter-annotator agreement (IAA) varies by dimension:
fine-grained annotation improves agreement on correctness, coarse improves
agreement on relevance, and judgments on safety remain inconsistent.
Additionally, annotating only a small subset of sentences can provide
reliability comparable to coarse annotations, reducing cost and effort.

</details>


### [63] [Do Audio LLMs Really LISTEN, or Just Transcribe? Measuring Lexical vs. Acoustic Emotion Cues Reliance](https://arxiv.org/abs/2510.10444)
*Jingyi Chen,Zhimeng Guo,Jiyun Chun,Pichao Wang,Andrew Perrault,Micha Elsner*

Main category: cs.CL

TL;DR: 研究发现当前的大型音频语言模型在情感理解中主要依赖词汇信息，忽视声学线索。


<details>
  <summary>Details</summary>
Motivation: 探究大型音频语言模型是否真正利用了声学信息，还是仅仅依赖词汇内容进行情感识别。

Method: 设计了LISTEN基准测试，通过词汇与声学线索的控制对比，评估六个先进的大型音频语言模型在情感识别中的表现。

Result: 模型在词汇中性或不存在时普遍预测为中性情绪，声学与词汇信息一致时表现有限，冲突时无法正确分类不同情绪，副语言环境下表现接近随机。

Conclusion: 当前大型音频语言模型更多地是在“转录”而非真正“聆听”，倾向于依赖词汇语义而忽视声学线索。LISTEN提供了评估多模态情感理解的合理框架。

Abstract: Understanding emotion from speech requires sensitivity to both lexical and
acoustic cues. However, it remains unclear whether large audio language models
(LALMs) genuinely process acoustic information or rely primarily on lexical
content. We present LISTEN (Lexical vs. Acoustic Speech Test for Emotion in
Narratives), a controlled benchmark designed to disentangle lexical reliance
from acoustic sensitivity in emotion understanding. Across evaluations of six
state-of-the-art LALMs, we observe a consistent lexical dominance. Models
predict "neutral" when lexical cues are neutral or absent, show limited gains
under cue alignment, and fail to classify distinct emotions under cue conflict.
In paralinguistic settings, performance approaches chance. These results
indicate that current LALMs largely "transcribe" rather than "listen," relying
heavily on lexical semantics while underutilizing acoustic cues. LISTEN offers
a principled framework for assessing emotion understanding in multimodal
models.

</details>


### [64] [RECON: Reasoning with Condensation for Efficient Retrieval-Augmented Generation](https://arxiv.org/abs/2510.10448)
*Zhichao Xu,Minheng Wang,Yawei Wang,Wenqian Ye,Yuntao Du,Yunpu Ma,Yijun Tian*

Main category: cs.CL

TL;DR: RECON框架通过引入显式摘要模块，有效压缩推理过程中的证据，提升了检索增强生成系统（RAG）的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统因上下文管理低效，长且噪声较多的检索文档增加成本并降低性能。

Method: RECON采用两阶段训练摘要器：先在QA数据集上进行相关性预训练，再通过多方面蒸馏保证摘要的真实性和清晰度，将摘要模块集成于推理循环中。

Result: 集成RECON后，Context长度减少35%，训练速度和推理延迟提升，同时RAG系统在下游QA任务上表现提升，3B模型EM提升14.5%，7B提升3.0%，多跳QA表现尤为突出。

Conclusion: RECON证明了学习型上下文压缩对于构建实用、可扩展且高性能的RAG系统至关重要。

Abstract: Retrieval-augmented generation (RAG) systems trained using reinforcement
learning (RL) with reasoning are hampered by inefficient context management,
where long, noisy retrieved documents increase costs and degrade performance.
We introduce RECON (REasoning with CONdensation), a framework that integrates
an explicit summarization module to compress evidence within the reasoning
loop. Our summarizer is trained via a two-stage process: relevance pretraining
on QA datasets, followed by multi-aspect distillation from proprietary LLMs to
ensure factuality and clarity. Integrated into the Search-R1 pipeline, RECON
reduces total context length by 35\%, leading to improved training speed and
inference latency, while simultaneously improving RAG performance on downstream
QA benchmarks. Notably, it boosts the average EM score of the 3B model by
14.5\% and the 7B model by 3.0\%, showing particular strength in multi-hop QA.
RECON demonstrates that learned context compression is essential for building
practical, scalable, and performant RAG systems. Our code implementation is
made available at https://github.com/allfornancy/RECON.

</details>


### [65] [Steering Over-refusals Towards Safety in Retrieval Augmented Generation](https://arxiv.org/abs/2510.10452)
*Utsav Maskey,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: 该论文研究了大语言模型中因安全对齐引发的过度拒绝现象，提出了一种基于嵌入空间干预的方法以减少误拒绝。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的安全机制导致对无害请求的过度拒绝，特别是在检索增强生成（RAG）中，查询意图和检索上下文属性影响拒绝行为，亟需理解和缓解这一问题。

Method: 构建了涵盖医学、化学和开放领域的RagRefuse基准数据集，分析上下文排列、领域和有害文本密度对拒绝行为的影响；提出了SafeRAG-Steering方法，通过在推理时对嵌入空间进行干预，引导模型生成安全且非拒绝的输出。

Result: 实验表明，上下文污染、领域及有害文本密度明显触发无害查询的拒绝行为，SafeRAG-Steering有效减少了在受污染的RAG流程中的过度拒绝，同时保持了合理的拒绝率。

Conclusion: 通过分析影响拒绝行为的多因素，该研究提出的嵌入干预方法能有效缓解大语言模型中因安全对齐带来的过度拒绝问题，提升模型的实用性与安全性。

Abstract: Safety alignment in large language models (LLMs) induces over-refusals --
where LLMs decline benign requests due to aggressive safety filters. We analyze
this phenomenon in retrieval-augmented generation (RAG), where both the query
intent and retrieved context properties influence refusal behavior. We
construct RagRefuse, a domain-stratified benchmark spanning medical, chemical,
and open domains, pairing benign and harmful queries with controlled context
contamination patterns and sizes. Our analysis shows that context arrangement /
contamination, domain of query and context, and harmful-text density trigger
refusals even on benign queries, with effects depending on model-specific
alignment choices. To mitigate over-refusals, we introduce
\textsc{SafeRAG-Steering}, a model-centric embedding intervention that steers
the embedding regions towards the confirmed safe, non-refusing output regions
at inference time. This reduces over-refusals in contaminated RAG pipelines
while preserving legitimate refusals.

</details>


### [66] [End-to-end Speech Recognition with similar length speech and text](https://arxiv.org/abs/2510.10453)
*Peng Fan,Wenping Wang,Fei Deng*

Main category: cs.CL

TL;DR: 本文针对语音长度与文本长度不匹配的问题，提出了两种新的对齐方法TIL和AXE，并通过关键帧融合提升信息表达，有效减少帧数。


<details>
  <summary>Details</summary>
Motivation: 现有方法在语音下采样到接近文本长度时，CTC方法无法正确对齐语音与文本。

Method: 引入时间独立损失(TIL)和基于编辑距离的对齐交叉熵损失(AXE)，并通过加权融合关键帧及其邻近帧的信息。

Result: 在AISHELL-1和AISHELL-2数据集子集上，新方法优于之前工作，帧数减少至少86%。

Conclusion: 提出的方法有效解决了语音与文本长度对齐问题，显著提升了语音识别系统的效率和性能。

Abstract: The mismatch of speech length and text length poses a challenge in automatic
speech recognition (ASR). In previous research, various approaches have been
employed to align text with speech, including the utilization of Connectionist
Temporal Classification (CTC). In earlier work, a key frame mechanism (KFDS)
was introduced, utilizing intermediate CTC outputs to guide downsampling and
preserve keyframes, but traditional methods (CTC) failed to align speech and
text appropriately when downsampling speech to a text-similar length. In this
paper, we focus on speech recognition in those cases where the length of speech
aligns closely with that of the corresponding text. To address this issue, we
introduce two methods for alignment: a) Time Independence Loss (TIL) and b)
Aligned Cross Entropy (AXE) Loss, which is based on edit distance. To enhance
the information on keyframes, we incorporate frame fusion by applying weights
and summing the keyframe with its context 2 frames. Experimental results on
AISHELL-1 and AISHELL-2 dataset subsets show that the proposed methods
outperform the previous work and achieve a reduction of at least 86\% in the
number of frames.

</details>


### [67] [Rethinking LLM Evaluation: Can We Evaluate LLMs with 200x Less Data?](https://arxiv.org/abs/2510.10457)
*Shaobo Wang,Cong Wang,Wenjie Fu,Yue Min,Mingquan Feng,Isabel Guan,Xuming Hu,Conghui He,Cunxiang Wang,Kexin Yang,Xingzhang Ren,Fei Huang,Dayiheng Liu,Linfeng Zhang*

Main category: cs.CL

TL;DR: 本文提出EssenceBench框架，利用遗传算法对大规模基准进行压缩，实现模型排名的高准确度和一致性。


<details>
  <summary>Details</summary>
Motivation: 随着对模型能力综合评估需求增加，基准套件规模急剧扩大，现有冗余减少和性能预测方法存在准确性和排名一致性不足的问题。

Method: 通过样本级冗余分析选出可删样本，将基准压缩建模为分数重建的优化问题，设计EssenceBench粗到细的遗传算法框架结合适应性和归因搜索。

Result: 在HellaSwag基准上，使用EssenceBench可用少至原样本数量的1/25实现模型排名5%以内的小幅移动，且实现95%排名保持，仅需1/200样本数量。

Conclusion: EssenceBench有效压缩基准规模，确保模型排名的准确性和一致性，优于现有方法且大幅提升效率。

Abstract: As the demand for comprehensive evaluations of diverse model capabilities
steadily increases, benchmark suites have correspondingly grown significantly
in scale. Despite notable advances in redundancy reduction and subset-level
performance prediction, a systematic framework that effectively integrates
these methods to ensure both prediction accuracy and ranking consistency is
still largely elusive. In this paper, we first perform a sample-level analysis
of benchmark redundancy and identify several highly similar samples that can be
eliminated. Besides, we frame benchmark compression as an optimization problem
with the aim of score reconstruction. Building on these, we then propose
EssenceBench, a coarse-to-fine framework utilizing an iterative Genetic
Algorithm (GA), which takes the advantages of fitness-based subset search and
attribution-based sample search. Compared to previous methods, our approach
yields superior compression results with lower reconstruction error and
markedly higher efficiency. In particular, on the HellaSwag benchmark (10K
samples), our method preserves the ranking of all models shifting within 5%
using 25x fewer samples, and achieves 95% ranking preservation shifting within
5% using only 200x fewer samples.

</details>


### [68] [NIM: Neuro-symbolic Ideographic Metalanguage for Inclusive Communication](https://arxiv.org/abs/2510.10459)
*Prawaal Sharma,Poonam Goyal,Navneet Goyal,Vidisha Sharma*

Main category: cs.CL

TL;DR: 提出了一种利用神经符号人工智能和自然语义金属anguage理论，结合大型语言模型的新型通用表意金属anguage，帮助低学历群体跨越数字鸿沟，实现更简单有效的数字交流。


<details>
  <summary>Details</summary>
Motivation: 低学历人群在数字交流中面临显著障碍，导致数字鸿沟加剧。

Method: 采用神经符号AI结合NSM理论，将复杂语义分解为简单原子概念，并通过与200多名半文盲参与者合作，共同定义问题、选取表意符号及系统验证。

Result: 系统达到80%以上语义理解率，具有易学性和普适性，能够有效服务受限教育背景的弱势群体。

Conclusion: 该表意金属anguage提供了一种跨学术、语言及文化界限的交流框架，促进数字交流的普及和包容性。

Abstract: Digital communication has become the cornerstone of modern interaction,
enabling rapid, accessible, and interactive exchanges. However, individuals
with lower academic literacy often face significant barriers, exacerbating the
"digital divide". In this work, we introduce a novel, universal ideographic
metalanguage designed as an innovative communication framework that transcends
academic, linguistic, and cultural boundaries. Our approach leverages
principles of Neuro-symbolic AI, combining neural-based large language models
(LLMs) enriched with world knowledge and symbolic knowledge heuristics grounded
in the linguistic theory of Natural Semantic Metalanguage (NSM). This enables
the semantic decomposition of complex ideas into simpler, atomic concepts.
Adopting a human-centric, collaborative methodology, we engaged over 200
semi-literate participants in defining the problem, selecting ideographs, and
validating the system. With over 80\% semantic comprehensibility, an accessible
learning curve, and universal adaptability, our system effectively serves
underprivileged populations with limited formal education.

</details>


### [69] [FML-bench: A Benchmark for Automatic ML Research Agents Highlighting the Importance of Exploration Breadth](https://arxiv.org/abs/2510.10472)
*Qiran Zou,Hou Hei Lam,Wenhao Zhao,Yiming Tang,Tingting Chen,Samson Yu,Tianyi Zhang,Chang Liu,Xiangyang Ji,Dianbo Liu*

Main category: cs.CL

TL;DR: 本文提出了FML-bench，一个用于评估自动机器学习研究代理的基准，包括8个多样且基础的机器学习研究问题，并引入了一个综合性能评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有基准过度强调工程实现，缺乏学术严谨性和任务多样性，难以全面评估自动机器学习研究代理的科研能力。

Method: 设计了FML-bench基准，涵盖8个基础研究问题，减少编码负担，注重基础性而非特定应用，支持多样化任务并可扩展至真实GitHub仓库；还提出了包含五个互补指标的统一评估框架。

Result: 通过FML-bench评测发现，采用广泛研究探索策略的代理表现优于专注于深度但狭窄探索的代理，表明探索广度比单纯细化更有效。

Conclusion: FML-bench提供了一个更全面严谨的方法来评估自动机器学习研究代理，强调广度探索策略有助于提升科研效果。该基准公开开放供研究社区使用。

Abstract: Large language models (LLMs) have sparked growing interest in automatic
machine learning research agents. Among them, agents capable of autonomously
proposing ideas and conducting machine learning experiments are particularly
promising, as they maximize research automation and accelerate scientific
progress by iteratively refining ideas based on experimental results. However,
comprehensively evaluating such agents remains challenging. Existing benchmarks
tend to overemphasize engineering aspects while neglecting academic rigor,
creating barriers that obscure a clear assessment of an agent's scientific
capabilities in machine learning research. They also suffer from limited task
diversity, an overemphasis on application-oriented tasks over fundamental
research problems, and limited scalability to realistic research settings. To
address these limitations, we introduce FML-bench, a benchmark designed to
evaluate automatic machine learning research agents on 8 diverse and
fundamental machine learning research problems. It reduces coding burden,
emphasizes fundamental problems rather than specific use cases, offers high
task diversity, and is extensible to real-world machine learning GitHub
repositories. Furthermore, we present a unified evaluation framework with five
complementary metrics, designed to comprehensively assess agent performance on
our benchmark. We evaluate state-of-the-art automatic research agents on
FML-bench, and find that agents employing broad research exploration strategies
outperform those focusing on narrow but deep exploration. These findings
suggest that emphasizing the breadth of exploration may lead to more effective
research outcomes than focusing solely on incremental refinement. Our benchmark
is available at https://github.com/qrzou/FML-bench.

</details>


### [70] [When or What? Understanding Consumer Engagement on Digital Platforms](https://arxiv.org/abs/2510.10474)
*Jingyi Wu,Junying Liang*

Main category: cs.CL

TL;DR: 本研究通过LDA模型分析TED演讲数据，发现内容创作者的主题供应与观众偏好存在长期不匹配，且时间因素比内容主题对观众参与影响更大。


<details>
  <summary>Details</summary>
Motivation: 数字服务经济中内容创作者与消费者的互动中，内容特征被广泛认为是流行度的关键驱动因素，但创作者经常误判观众真正的需求，这促使本研究探讨主题供应与需求之间的匹配情况。

Method: 应用潜在狄利克雷分配（LDA）模型分析大量TED演讲文本，比较创作者的主题供应和观众参与数据，进行长期动态分析。

Result: 发现创作者提供的主题与观众偏好存在持久的不匹配，时间动态对观众参与的影响超过内容主题本身。

Conclusion: 内容特征并非流行度的主要驱动因素，时间和情境因素在塑造观众响应方面更为重要，这为数字平台上的消费者注意力动态提供了新见解，对市场营销者、平台管理者和内容创作者具有指导意义。

Abstract: Understanding what drives popularity is critical in today's digital service
economy, where content creators compete for consumer attention. Prior studies
have primarily emphasized the role of content features, yet creators often
misjudge what audiences actually value. This study applies Latent Dirichlet
Allocation (LDA) modeling to a large corpus of TED Talks, treating the platform
as a case of digital service provision in which creators (speakers) and
consumers (audiences) interact. By comparing the thematic supply of creators
with the demand expressed in audience engagement, we identify persistent
mismatches between producer offerings and consumer preferences. Our
longitudinal analysis further reveals that temporal dynamics exert a stronger
influence on consumer engagement than thematic content, suggesting that when
content is delivered may matter more than what is delivered. These findings
challenge the dominant assumption that content features are the primary drivers
of popularity and highlight the importance of timing and contextual factors in
shaping consumer responses. The results provide new insights into consumer
attention dynamics on digital platforms and carry practical implications for
marketers, platform managers, and content creators seeking to optimize audience
engagement strategies.

</details>


### [71] [Assessing Large Language Models for Structured Medical Order Extraction](https://arxiv.org/abs/2510.10475)
*A H M Rezaul Karim,Ozlem Uzuner*

Main category: cs.CL

TL;DR: 本论文关注从多轮医患对话中提取结构化医疗医嘱，采用通用大规模语言模型LLaMA-4，取得了较好效果。


<details>
  <summary>Details</summary>
Motivation: 医疗医嘱提取对临床信息结构化、决策支持及自动化流程有重要意义。

Method: 使用无需领域特定微调的通用指令调优大模型LLaMA-4 17B，在单一示例指导下进行少样本学习。

Result: 在MEDIQA-OE 2025任务中，该方法获得37.76的平均F1分数，排名第5，对医嘱理由和来源识别准确度有明显提升。

Conclusion: 大规模非专业领域大模型结合有效提示工程，可以作为医疗NLP特定任务的强有力基线方法。

Abstract: Medical order extraction is essential for structuring actionable clinical
information, supporting decision-making, and enabling downstream applications
such as documentation and workflow automation. Orders may be embedded in
diverse sources, including electronic health records, discharge summaries, and
multi-turn doctor-patient dialogues, and can span categories such as
medications, laboratory tests, imaging studies, and follow-up actions. The
MEDIQA-OE 2025 shared task focuses on extracting structured medical orders from
extended conversational transcripts, requiring the identification of order
type, description, reason, and provenance. We present the MasonNLP submission,
which ranked 5th among 17 participating teams with 105 total submissions. Our
approach uses a general-purpose, instruction-tuned LLaMA-4 17B model without
domain-specific fine-tuning, guided by a single in-context example. This
few-shot configuration achieved an average F1 score of 37.76, with notable
improvements in reason and provenance accuracy. These results demonstrate that
large, non-domain-specific LLMs, when paired with effective prompt engineering,
can serve as strong, scalable baselines for specialized clinical NLP tasks.

</details>


### [72] [UltraLLaDA: Scaling the Context Length to 128K for Diffusion Large Language Models](https://arxiv.org/abs/2510.10481)
*Guangxin He,Shen Nie,Fengqi Zhu,Yuankang Zhao,Tianyi Bai,Ran Yan,Jie Fu,Chongxuan Li,Binhang Yuan*

Main category: cs.CL

TL;DR: 本文研究了扩展扩散大语言模型（Diffusion LLMs）上下文窗口的方法，提出了一种基于旋转位置编码修改的后训练技术，实现了128K大小的长上下文处理。


<details>
  <summary>Details</summary>
Motivation: 当前扩散大语言模型在长上下文处理方面的表现尚不明晰，探索无需重训即可扩展上下文窗口对实际应用具有重要意义。

Method: 通过修改标准的旋转位置编码（RoPE）以适应扩散过程的概率建模特性，并比较不同的掩码策略，提出一种后训练技术用于扩展上下文窗口。

Result: 构建了UltraLLaDA模型，支持128K上下文长度，在长上下文任务中显著优于无训练调整的基线方法。

Conclusion: 特殊的位置编码扩展是扩散大语言模型长上下文扩展的关键，本文提供了一种高效的后训练思路，为实现超长上下文的实际应用提供指导。

Abstract: Diffusion LLMs have attracted growing interest, with plenty of recent work
emphasizing their great potential in various downstream tasks; yet the
long-context behavior of diffusion LLMs remains largely uncharted. We present a
case study of post-training techniques for extending the context window of
diffusion LLMs (i.e., LLaDA) without retraining from scratch. We show that a
simple modification to the standard Rotary Positional Embeddings (RoPE)
extension effectively accommodates the probabilistic modeling inherent in the
diffusion process, enabling stable scaling to longer context ranges. We further
compare masking strategies used during post-training and analyze their impact
on optimization stability and long-range recall. Instantiating these insights,
we introduce UltraLLaDA, a diffusion LLM with a 128K-token context window that,
in our empirical evaluation on long-context tasks, significantly outperforms
training-free baselines. Our experimental results highlight the special
positional extension as a key lever for scaling diffusion LLMs to extended
contexts and offer practical guidance for practitioners seeking 128K-scale
context via efficient post-training.

</details>


### [73] [VOLTAGE: A Versatile Contrastive Learning based OCR Methodology for ultra low-resource scripts through Auto Glyph Feature Extraction](https://arxiv.org/abs/2510.10490)
*Prawaal Sharma,Poonam Goyal,Vidisha Sharma,Navneet Goyal*

Main category: cs.CL

TL;DR: 本文提出了一种基于对比学习的光学字符识别（OCR）方法VOLTAGE，针对资源匮乏语言的数字化保护，利用自动字形特征推荐和聚类标注，通过图像增强和生成对抗网络扩充数据，提升了对古老泰克里文字的识别精度。


<details>
  <summary>Details</summary>
Motivation: 全球有2500种语言面临灭绝，其中资源匮乏语言数字化进程受限，缺乏无监督OCR技术是主要瓶颈之一。

Method: 提出基于对比学习的VOLTAGE方法，结合自动字形特征推荐实现聚类标注，通过图像变换和生成对抗网络增强标注数据，应用于泰克里及多种印地语系文字。

Result: 在机器印刷和手写泰克里文字的识别准确率分别达到95%和87%，并在高低资源印地语脚本上验证了方法的普适性。

Conclusion: VOLTAGE方法有效提升了低资源语言的OCR识别能力，促进了这些语言的数字化保存和下游应用，避免语言灭绝风险。

Abstract: UNESCO has classified 2500 out of 7000 languages spoken worldwide as
endangered. Attrition of a language leads to loss of traditional wisdom, folk
literature, and the essence of the community that uses it. It is therefore
imperative to bring digital inclusion to these languages and avoid its
extinction. Low resource languages are at a greater risk of extinction. Lack of
unsupervised Optical Character Recognition(OCR) methodologies for low resource
languages is one of the reasons impeding their digital inclusion. We propose
VOLTAGE - a contrastive learning based OCR methodology, leveraging auto-glyph
feature recommendation for cluster-based labelling. We augment the labelled
data for diversity and volume using image transformations and Generative
Adversarial Networks. Voltage has been designed using Takri - a family of
scripts used in 16th to 20th century in the Himalayan regions of India. We
present results for Takri along with other Indic scripts (both low and high
resource) to substantiate the universal behavior of the methodology. An
accuracy of 95% for machine printed and 87% for handwritten samples on Takri
script has been achieved. We conduct baseline and ablation studies along with
building downstream use cases for Takri, demonstrating the usefulness of our
work.

</details>


### [74] [Merlin's Whisper: Enabling Efficient Reasoning in LLMs via Black-box Adversarial Prompting](https://arxiv.org/abs/2510.10528)
*Heming Xia,Cunxiao Du,Rui Li,Chak Tou Leong,Yongqi Li,Wenjie Li*

Main category: cs.CL

TL;DR: 本文提出了AdvPrompt，一种通过黑盒对抗提示减少大型推理模型(LRM)冗长推理过程的方法，有效缩短响应长度同时保持准确率。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在执行复杂推理任务时，虽然表现优异，但其推理步骤冗长导致计算和延迟开销大，限制了模型的实际应用。

Method: 文中提出AdvPrompt，一种迭代优化的对抗提示生成框架，利用黑盒方式针对开源及闭源模型，优化提示以激发模型产生精简且高质量的回答。

Result: 实验表明AdvPrompt在多个基准测试中平均减少约40%的token使用，具体表现为Qwen3系列在GSM8K数据集上响应长度减少3倍，Claude-3.7和Gemini-2.5模型在MATH-500数据集上分别减少35%和47%的token使用。

Conclusion: AdvPrompt展现了良好的泛化能力和实用价值，验证了黑盒对抗提示策略在提升大型推理模型推理效率上的潜力。

Abstract: Large reasoning models (LRMs) have demonstrated remarkable proficiency in
tackling complex reasoning tasks through step-by-step thinking. However, such a
lengthy reasoning process incurs substantial computational and latency
overheads, hindering the practical deployment of these models. In this work, we
present a new perspective on mitigating overthinking in LRMs via black-box
adversarial prompting. By treating both open-source LRMs and closed-source APIs
as black-box communicators, we investigate how to elicit concise responses
without sacrificing accuracy. We introduce AdvPrompt, an iterative refinement
framework that generates high-quality adversarial prompts from diverse
perspectives. Experiments across multiple benchmarks demonstrate that AdvPrompt
consistently reduces token usage while preserving performance. Notably,
AdvPrompt achieves a 3x reduction in average response length on simple GSM8K
questions for the Qwen3 model series, and delivers an average ~40% token
reduction across four benchmarks. For closed-source APIs, AdvPrompt reduces
token usage on MATH-500 by 35% for Claude-3.7 and 47% for Gemini-2.5. Further
analysis reveals the generalizability of AdvPrompt across various model scales
and families, underscoring the potential of black-box prompting as a practical
and effective strategy for enhancing LRM efficiency.

</details>


### [75] [Detecting Hallucinations in Authentic LLM-Human Interactions](https://arxiv.org/abs/2510.10539)
*Yujie Ren,Niklas Gruhlke,Anne Lauscher*

Main category: cs.CL

TL;DR: 本文提出了AuthenHallu，这是第一个基于真实大语言模型（LLM）与人类对话构建的幻觉检测基准，反映了真实使用场景中LLM出现幻觉的特点。


<details>
  <summary>Details</summary>
Motivation: 现有的幻觉检测基准多为人工构造，无法真实反映LLM在实际用户对话中产生幻觉的情况。

Method: 收集并标注真实LLM-人类对话样本，建立覆盖多个领域尤其是复杂领域的幻觉检测基准数据集。

Result: 数据显示31.4%的对话对存在幻觉，在数学等挑战性领域达60%。使用原生LLM作为检测器性能有限，难以满足实际需求。

Conclusion: AuthenHallu为幻觉检测提供了更贴近真实应用的数据支持，但现有LLM作为检测工具还需改进以应对真实场景中的幻觉问题。

Abstract: As large language models (LLMs) are increasingly applied in sensitive domains
such as medicine and law, hallucination detection has become a critical task.
Although numerous benchmarks have been proposed to advance research in this
area, most of them are artificially constructed--either through deliberate
hallucination induction or simulated interactions--rather than derived from
genuine LLM-human dialogues. Consequently, these benchmarks fail to fully
capture the characteristics of hallucinations that occur in real-world usage.
To address this limitation, we introduce AuthenHallu, the first hallucination
detection benchmark built entirely from authentic LLM-human interactions. For
AuthenHallu, we select and annotate samples from genuine LLM-human dialogues,
thereby providing a faithful reflection of how LLMs hallucinate in everyday
user interactions. Statistical analysis shows that hallucinations occur in
31.4% of the query-response pairs in our benchmark, and this proportion
increases dramatically to 60.0% in challenging domains such as Math & Number
Problems. Furthermore, we explore the potential of using vanilla LLMs
themselves as hallucination detectors and find that, despite some promise,
their current performance remains insufficient in real-world scenarios.

</details>


### [76] [BitMar: Low-Bit Multimodal Fusion with Episodic Memory for Edge Devices](https://arxiv.org/abs/2510.10560)
*Euhid Aman,Esteban Carlin,Hsing-Kuo Pao,Giovanni Beltrame,Ghaluh Indah Permata Sari,Yie-Tarng Chen*

Main category: cs.CL

TL;DR: 提出了一种名为BitMar的量化多模态变换器，通过低比特编码器和外部情节记忆实现边缘设备上的高效图文生成。


<details>
  <summary>Details</summary>
Motivation: 现有跨注意力变换器模型虽性能优越但资源消耗大，难以部署于边缘设备；内存增强架构虽提高上下文利用率但缺乏配合极限量化使用的研究。

Method: BitMar使用1.58比特的文本和视觉编码器生成紧凑嵌入，结合固定大小的键值情节记忆；解码器采用逐层条件和滑动窗口机制，提升生成内容的上下文相关性和内存效率。

Result: BitMar在低延迟和小模型体积下，达到竞争力的图像说明和多模态理解效果。

Conclusion: BitMar实现了性能与效率的良好平衡，适合资源受限的边缘设备部署。

Abstract: Cross-attention transformers and other multimodal vision-language models
excel at grounding and generation; however, their extensive, full-precision
backbones make it challenging to deploy them on edge devices. Memory-augmented
architectures enhance the utilization of past context; however, most works
rarely pair them with aggressive edge-oriented quantization. We introduce
BitMar, a quantized multimodal transformer that proposes an external human-like
episodic memory for effective image-text generation on hardware with limited
resources. BitMar utilizes 1.58-bit encoders, one for text (BitNet-style) and
one for vision (DiNOv2-based), to create compact embeddings that are combined
and used to query a fixed-size key-value episodic memory. During vector
retrieval, the BitNet decoder applies per-layer conditioning, which increases
the contextual relevance of generated content. The decoder also employs
attention sinks with a sliding-window mechanism to process long or streaming
inputs under tight memory budgets. The combination of per-layer conditioning
and sliding-window attention achieves a strong quality-speed trade-off,
delivering competitive captioning and multimodal understanding at low latency
with a small model footprint. These characteristics make BitMar well-suited for
edge deployment.

</details>


### [77] [Dynamic Topic Evolution with Temporal Decay and Attention in Large Language Models](https://arxiv.org/abs/2510.10613)
*Di Wu abd Shuaidong Pan*

Main category: cs.CL

TL;DR: 本文提出基于时间大型语言模型的动态主题演化建模框架，通过引入时间衰减函数和注意力机制，实现语义单元重要性的时间调整及主题动态变化的捕捉。


<details>
  <summary>Details</summary>
Motivation: 现有主题模型难以准确反映主题随时间演变的动态变化，需要一种统一的语义表示与时间演化建模方法以提升主题的连贯性、多样性及稳定性。

Method: 利用大型语言模型获取文本语义嵌入，结合时间衰减函数和注意力机制调节语义权重，然后映射至潜在主题空间，采用状态转移矩阵描述主题动态，最后通过联合优化约束语义与时间一致性。

Result: 所提框架在真实语料上有效捕捉主题的生成、扩展与衰退过程，在多项指标上优于现有模型。

Conclusion: 该方法系统性地解决了大规模文本中动态语义模式的理解问题，丰富了主题建模研究范式，并支持多领域复杂文本分析任务。

Abstract: This paper proposes a modeling framework for dynamic topic evolution based on
temporal large language models. The method first uses a large language model to
obtain contextual embeddings of text and then introduces a temporal decay
function and an attention mechanism. These components allow the model to adjust
the importance of semantic units according to time intervals and capture topic
variations across different periods. The temporal representations are then
mapped into a latent topic space, where a state transition matrix is applied to
describe the dynamic evolution of topics. A joint optimization objective
constrains both semantic modeling and temporal consistency, ensuring diversity
and smoothness in topic generation. The design emphasizes the unified modeling
of semantic representation and temporal evolution, which improves topic
coherence and diversity while enhancing stability and interpretability over
time. Experiments on real-world corpora show that the framework effectively
captures the generation, expansion, and decline of topics and outperforms
existing models across multiple metrics. Overall, the proposed method provides
a systematic solution for understanding dynamic semantic patterns in
large-scale text, enriches the research paradigm of topic modeling, and
supports complex text analysis tasks in multiple domains.

</details>


### [78] [Preserving LLM Capabilities through Calibration Data Curation: From Analysis to Optimization](https://arxiv.org/abs/2510.10618)
*Bowei He,Lihao Yin,Huiling Zhen,Shuqi Liu,Han Wu,Xiaokun Zhang,Mingxuan Yuan,Chen Ma*

Main category: cs.CL

TL;DR: 研究了校准数据对大语言模型后训练压缩效果和能力的影响，提出了基于激活空间代表性与多样性的校准数据筛选框架。


<details>
  <summary>Details</summary>
Motivation: 目前对于校准数据在大语言模型压缩后对模型能力的影响探索不足，尤其是对复杂推理能力和领域对应性的系统研究缺失。

Method: 从激活模式角度分析校准数据对模型不同能力的影响，聚焦复杂推理任务，提出基于激活空间代表性和多样性的校准数据筛选方法。

Result: 发现校准数据的代表性和多样性决定了其质量，采用提出的框架能够提升后训练压缩方法中关键能力的保留效果。

Conclusion: 通过系统分析和筛选校准数据，可有效提升大语言模型后训练压缩的性能和能力保留，为模型压缩提供实用指导。

Abstract: Post-training compression has been a widely employed approach to scale down
large language model (LLM) and facilitate efficient inference. In various
proposed compression methods, including pruning and quantization, calibration
data plays a vital role by informing the weight importance and activation
dynamic ranges. However, how calibration data impacts the LLM capability after
compression is less explored. Few of the existing works, though recognizing the
significance of this study, only investigate the language modeling or
commonsense reasoning performance degradation from limited angles, like the
data sources or sample amounts. More systematic research is still needed to
examine the impacts on different LLM capabilities in terms of compositional
properties and domain correspondence of calibration data. In this work, we aim
at bridging this gap and further analyze underlying influencing mechanisms from
the activation pattern perspective. Especially, we explore the calibration
data's impacts on high-level complex reasoning capabilities, like math problem
solving and code generation. Delving into the underlying mechanism, we find
that the representativeness and diversity in activation space more
fundamentally determine the quality of calibration data. Finally, we propose a
calibration data curation framework based on such observations and analysis,
enhancing the performance of existing post-training compression methods on
preserving critical LLM capabilities. Our code is provided in
\href{https://github.com/BokwaiHo/COLA.git}{Link}.

</details>


### [79] [FactAppeal: Identifying Epistemic Factual Appeals in News Media](https://arxiv.org/abs/2510.10627)
*Guy Mor-Lan,Tamir Sheafer,Shaul R. Shenhav*

Main category: cs.CL

TL;DR: 提出了新的任务“认识诉求识别”，通过注释新闻句子，识别事实陈述的来源和证据类型。


<details>
  <summary>Details</summary>
Motivation: 当前资源仅关注事实陈述的检测与验证，缺乏对其认知结构和证据基础的细致识别。

Method: 构建了包含3226条英文新闻句子的FactAppeal数据集，进行了详细的语料标注；采用多种编码器和生成式解码器模型进行任务建模。

Result: 基于Gemma 2 9B的最佳模型达到了0.73的宏F1分数。

Conclusion: 提出的任务和数据集推动了对事实陈述中知识诉求结构的深入理解，模型表现良好，具备应用潜力。

Abstract: How is a factual claim made credible? We propose the novel task of Epistemic
Appeal Identification, which identifies whether and how factual statements have
been anchored by external sources or evidence. To advance research on this
task, we present FactAppeal, a manually annotated dataset of 3,226
English-language news sentences. Unlike prior resources that focus solely on
claim detection and verification, FactAppeal identifies the nuanced epistemic
structures and evidentiary basis underlying these claims and used to support
them. FactAppeal contains span-level annotations which identify factual
statements and mentions of sources on which they rely. Moreover, the
annotations include fine-grained characteristics of factual appeals such as the
type of source (e.g. Active Participant, Witness, Expert, Direct Evidence),
whether it is mentioned by name, mentions of the source's role and epistemic
credentials, attribution to the source via direct or indirect quotation, and
other features. We model the task with a range of encoder models and generative
decoder models in the 2B-9B parameter range. Our best performing model, based
on Gemma 2 9B, achieves a macro-F1 score of 0.73.

</details>


### [80] [You're Not Gonna Believe This: A Computational Analysis of Factual Appeals and Sourcing in Partisan News](https://arxiv.org/abs/2510.10658)
*Guy Mor-Lan,Tamir Sheafer,Shaul R. Shenhav*

Main category: cs.CL

TL;DR: 本文大规模比较了CNN和Fox News的报道风格，发现两者在事实陈述和信息来源上存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 当前媒体偏见研究多关注内容倾向，缺乏对事实报道背后认知策略的计算分析。

Method: 通过文章匹配策略，比较同一事件的报道，应用FactAppeal框架分析47万篇文章，涵盖疫情和以色列-哈马斯战争两个时期。

Result: CNN报道更多事实陈述，并倾向于引用专家和权威文件，而Fox News更常使用新闻报道和直接引述。

Conclusion: 政党媒体通过不同的认知策略构建现实，为媒体偏见研究提供了新的视角。

Abstract: While media bias is widely studied, the epistemic strategies behind factual
reporting remain computationally underexplored. This paper analyzes these
strategies through a large-scale comparison of CNN and Fox News. To isolate
reporting style from topic selection, we employ an article matching strategy to
compare reports on the same events and apply the FactAppeal framework to a
corpus of over 470K articles covering two highly politicized periods: the
COVID-19 pandemic and the Israel-Hamas war. We find that CNN's reporting
contains more factual statements and is more likely to ground them in external
sources. The outlets also exhibit sharply divergent sourcing patterns: CNN
builds credibility by citing Experts} and Expert Documents, constructing an
appeal to formal authority, whereas Fox News favors News Reports and direct
quotations. This work quantifies how partisan outlets use systematically
different epistemic strategies to construct reality, adding a new dimension to
the study of media bias.

</details>


### [81] [AGENTIQL: An Agent-Inspired Multi-Expert Framework for Text-to-SQL Generation](https://arxiv.org/abs/2510.10661)
*Omid Reza Heidari,Siobhan Reid,Yassine Yaakoubi*

Main category: cs.CL

TL;DR: AGENTIQL是一个多专家框架，通过模块化代理和自适应路由提升了文本到SQL的转换性能和解释性，在Spider基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 单一架构的LLMs在复杂推理和多样化数据库模式下表现有限，需提升效率和准确性。

Method: 设计了结合推理代理、编码代理和列选择优化的多专家系统，并通过自适应路由在模块管线和基础解析器间动态选择，支持并行执行。

Result: 在Spider数据集上，AGENTIQL使用14B模型实现最高86.07%的执行准确率，接近GPT-4的89.65%，且提升了解释性和扩展性。

Conclusion: AGENTIQL提供了一种高效、可扩展且具有透明中间推理步骤的文本到SQL生成方案，兼顾性能和可解释性。

Abstract: LLMs have advanced text-to-SQL generation, yet monolithic architectures
struggle with complex reasoning and schema diversity. We propose AGENTIQL, an
agent-inspired multi-expert framework that combines a reasoning agent for
question decomposition, a coding agent for sub-query generation, and a
refinement step for column selection. An adaptive router further balances
efficiency and accuracy by selecting between our modular pipeline and a
baseline parser. Several steps in the pipeline can be executed in parallel,
making the framework scalable to larger workloads. Evaluated on the Spider
benchmark, AGENTIQL improves execution accuracy and interpretability and
achieves up to 86.07\% EX with 14B models using the Planner&Executor merging
strategy. The attained performance is contingent upon the efficacy of the
routing mechanism, thereby narrowing the gap to GPT-4-based SOTA (89.65% EX)
while using much smaller open-source LLMs. Beyond accuracy, AGENTIQL enhances
transparency by exposing intermediate reasoning steps, offering a robust,
scalable, and interpretable approach to semantic parsing.

</details>


### [82] [BrowserAgent: Building Web Agents with Human-Inspired Web Browsing Actions](https://arxiv.org/abs/2510.10666)
*Zhengbo Zhang,Zhiheng Lyu,Junhao Gong,Hongzhu Yi,Xinming Wang,Yuxuan Zhou,Jiabing Yang,Ping Nie,Yan Huang,Wenhu Chen*

Main category: cs.CL

TL;DR: 本文提出了BrowserAgent，一种模拟人类浏览行为的交互式网页代理，通过直接操作网页完成复杂任务，并采用两阶段训练提升泛化能力，在多跳问答任务上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法虽然在网页任务上表现良好，但依赖工具将网页转为静态文本，忽视了人类通过滚动、点击、输入等多样交互行为获取信息的方式，限制了代理能力。

Method: BrowserAgent通过Playwright在网页上执行预定义动作，采用两阶段训练（监督微调和拒绝微调）提升模型泛化，并引入显式记忆机制存储关键结论，辅助长程推理。

Result: BrowserAgent尽管训练数据较少，仍在多个公开问答任务中表现优异，特别是在多跳问答如HotpotQA和2Wiki上，相比Search-R1提升约20%。

Conclusion: 通过模拟人类浏览多样化交互行为及显式记忆机制，BrowserAgent展示了更强的推理和泛化能力，具备成为更先进、可扩展网页智能代理的潜力。

Abstract: Efficiently solving real-world problems with LLMs increasingly hinges on
their ability to interact with dynamic web environments and autonomously
acquire external information. While recent research like Search-R1 and
WebDancer demonstrates strong performance in solving web tasks, they heavily
rely on additional tools to convert the interactive web environment into static
text content. This is in contrast to human browsing behaviors, which involve
diverse interactions with the browser, such as scrolling, clicking, and typing.
In this paper, we propose BrowserAgent, a more interactive agent that solves
complex tasks through human-inspired browser actions. BrowserAgent operates
directly on raw web pages via Playwright through a set of predefined browser
actions. We adopt a two-stage training (Supervised Fine-Tuning (SFT) and
Rejection Fine-Tuning (RFT)) to improve the model's generalization abilities.
Despite using significantly less training data than Search-R1, BrowserAgent
achieves more competitive results across different Open-QA tasks. Additionally,
we introduce an explicit memory mechanism to store key conclusions across
steps, further enhancing the model's reasoning capabilities for long-horizon
tasks. Notably, BrowserAgent-7B can achieve around 20\% improvement over
Search-R1 on multi-hop QA tasks like HotpotQA, 2Wiki, and Bamboogle. These
results indicate that BrowserAgent can serve as a more advanced framework for
more interactive and scalable web agents.

</details>


### [83] [Unlocking LLM Safeguards for Low-Resource Languages via Reasoning and Alignment with Minimal Training Data](https://arxiv.org/abs/2510.10677)
*Zhuowei Chen,Bowei Zhang,Nankai Lin,Tian Hou,Lianxi Wang*

Main category: cs.CL

TL;DR: 提出了一种名为ConsistentGuard的多语言推理型大语言模型安全保护方法，提升了解释性和跨语言知识迁移能力，在少量训练数据下表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型虽然能力增强，但恶意请求的风险也增加，且现有的基于分类器的方法缺乏解释性且在低资源语言上表现不佳。

Method: 提出ConsistentGuard，一种基于推理的多语言保护方法，通过推理提升模型解释性，并通过对齐促进语言间知识迁移，在只有1000个训练样本的情况下进行训练。

Result: 在三份数据集的六种语言上表现优于使用更多数据训练的更大模型，显示出较强的解释性和泛化能力。

Conclusion: ConsistentGuard有效提升了大语言模型的安全防护，尤其是在低资源语言场景下，同时促进了跨语言知识转移，并推动了多语言安全保护的研究发展。

Abstract: Recent advances in LLMs have enhanced AI capabilities, but also increased the
risk posed by malicious requests, highlighting the need for effective LLM
safeguards to detect such queries. Existing approaches largely rely on
classifier-based methods that lack interpretability and perform poorly on
low-resource languages. To address these limitations, we propose
ConsistentGuard, a novel reasoning-based multilingual safeguard, which enhances
explainability via reasoning and boosts knowledge transfer between languages
through alignment. With only 1,000 training samples, our method demonstrates
superior performance on three datasets across six languages, outperforming
larger models trained with significantly more data, and exhibits strong
interpretability and generalization ability. We also contribute a multilingual
benchmark extension and release our codes to support future research.

</details>


### [84] [RePro: Training Language Models to Faithfully Recycle the Web for Pretraining](https://arxiv.org/abs/2510.10681)
*Zichun Yu,Chenyan Xiong*

Main category: cs.CL

TL;DR: 本文提出了RePro方法，通过强化学习训练小型语言模型有效重写预训练数据，提高预训练数据利用效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型预训练数据资源有限，亟需高效利用现有数据以提升模型性能。

Method: 设计质量奖励和忠实度奖励，通过强化学习优化一个4B规模的语言模型重写预训练数据，保持语义和结构的核心信息。

Result: RePro在400M和1.4B模型的多个下游任务中实现相较原始数据4.7%-14.0%的准确率提升，优于现有的ReWire方法，数据利用效率提升2-3倍。

Conclusion: RePro提供了一种高效可控的预训练数据重写方法，有效提升大型语言模型的预训练质量与效率。

Abstract: High-quality pretraining data is the fossil fuel of large language models
(LLMs), yet its reserves are running low for frontier models. In this paper, we
introduce RePro, a novel web recycling method that trains a relatively small LM
with reinforcement learning to generate effective and faithful rephrasings of
pretraining data. Specifically, we design one quality reward and three
faithfulness rewards, optimizing the LM rephraser to convert organic data into
high-quality rephrasings while maintaining its core semantics and structure. In
our experiment, we train a 4B rephraser to recycle 72B tokens sampled from
DCLM-RefinedWeb. Pretraining results on 400M and 1.4B models demonstrate that
RePro delivers 4.7%-14.0% relative accuracy gains over organic-only baseline on
22 downstream tasks. RePro also outperforms ReWire, the state-of-the-art web
recycling method that prompts a 70B rephraser, as well as the organic baseline
with a 4x larger data pool. Experiments with different amounts of recycled data
highlight that RePro improves organic data efficiency by 2-3x. Individual and
distributional analyses validate that RePro preserves more critical information
and faithfully reflects the characteristics of organic data compared to
prompting-based methods. Together, these results show that RePro provides an
efficient and controllable path to effectively harness the fossil fuel of LLM
pretraining. We open-source our code, rephraser, and recycled data at
https://github.com/cxcscmu/RePro.

</details>


### [85] [Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular Deep Learning Framework](https://arxiv.org/abs/2510.10729)
*Manas Zambre,Sarika Bobade*

Main category: cs.CL

TL;DR: 提出了一种利用深度卷积神经网络和BERT的模块化深度学习框架，用于检测文本中的讽刺。


<details>
  <summary>Details</summary>
Motivation: 讽刺作为一种微妙且容易被误解的交流方式，在缺乏语气和肢体语言的文本中尤为难以识别。

Method: 结合情感分析、上下文嵌入、语言特征提取和情绪检测，采用多层架构的深度卷积神经网络和BERT模型进行综合分析。

Result: 模型虽然处于概念阶段，但展示了在聊天机器人和社交媒体分析等实际应用中的可行性。

Conclusion: 该框架能够有效捕捉讽刺的多维度特征，为文本讽刺检测提供了一条可行路径。

Abstract: Sarcasm is a nuanced and often misinterpreted form of communication,
especially in text, where tone and body language are absent. This paper
proposes a modular deep learning framework for sarcasm detection, leveraging
Deep Convolutional Neural Networks (DCNNs) and contextual models such as BERT
to analyze linguistic, emotional, and contextual cues. The system integrates
sentiment analysis, contextual embeddings, linguistic feature extraction, and
emotion detection through a multi-layer architecture. While the model is in the
conceptual stage, it demonstrates feasibility for real-world applications such
as chatbots and social media analysis.

</details>


### [86] [Large Language Models for Full-Text Methods Assessment: A Case Study on Mediation Analysis](https://arxiv.org/abs/2510.10762)
*Wenqing Zhang,Trang Nguyen,Elizabeth A. Stuart,Yiqun T. Chen*

Main category: cs.CL

TL;DR: 本研究评估了大语言模型（LLMs）在系统综述中自动提取方法学信息的表现，发现其在明确的方法学特征识别上接近人类专家水平，但在复杂推理时准确率下降。


<details>
  <summary>Details</summary>
Motivation: 系统综述对科学证据的综合很重要，但提取详细方法信息耗时耗力，LLMs有潜力自动化这一过程，提高效率。

Method: 以因果中介分析为例，比较了先进LLMs与180篇全文科学论文的专家评审结果，通过准确率和F1分数衡量模型表现。

Result: LLMs在简单、明确的方法学标准识别上表现接近人类，准确率相关系数达0.71，F1相关系数达0.97；但在复杂推理任务中表现较差，错误多因对关键词词义的表面理解。文献长度负面影响模型准确率，发表年份无显著影响。

Conclusion: 当前LLMs适合识别显性方法学信息，但复杂推断仍需专家监督，结合自动提取与人工复核能有效提高系统综述的效率和准确性。

Abstract: Systematic reviews are crucial for synthesizing scientific evidence but
remain labor-intensive, especially when extracting detailed methodological
information. Large language models (LLMs) offer potential for automating
methodological assessments, promising to transform evidence synthesis. Here,
using causal mediation analysis as a representative methodological domain, we
benchmarked state-of-the-art LLMs against expert human reviewers across 180
full-text scientific articles. Model performance closely correlated with human
judgments (accuracy correlation 0.71; F1 correlation 0.97), achieving
near-human accuracy on straightforward, explicitly stated methodological
criteria. However, accuracy sharply declined on complex, inference-intensive
assessments, lagging expert reviewers by up to 15%. Errors commonly resulted
from superficial linguistic cues -- for instance, models frequently
misinterpreted keywords like "longitudinal" or "sensitivity" as automatic
evidence of rigorous methodological approache, leading to systematic
misclassifications. Longer documents yielded lower model accuracy, whereas
publication year showed no significant effect. Our findings highlight an
important pattern for practitioners using LLMs for methods review and synthesis
from full texts: current LLMs excel at identifying explicit methodological
features but require human oversight for nuanced interpretations. Integrating
automated information extraction with targeted expert review thus provides a
promising approach to enhance efficiency and methodological rigor in evidence
synthesis across diverse scientific fields.

</details>


### [87] [HiligayNER: A Baseline Named Entity Recognition Model for Hiligaynon](https://arxiv.org/abs/2510.10776)
*James Ald Teves,Ray Daniel Cal,Josh Magdiel Villaluz,Jean Malolos,Mico Magtira,Ramon Rodriguez,Mideth Abisado,Joseph Marvin Imperial*

Main category: cs.CL

TL;DR: 本文构建了首个基于Transformer模型的Hiligaynon语言命名实体识别基线模型，使用超过8000条注释句子，模型性能表现优异，且在跨语言迁移中具有潜力。


<details>
  <summary>Details</summary>
Motivation: Hiligaynon语言缺乏注释语料和基线模型，限制了该语言的语言处理研究。

Method: 收集8000多条来自新闻、社交媒体和文学的注释句子，利用mBERT和XLM-RoBERTa两种Transformer模型进行微调，构建HiligayNER模型。

Result: 两个模型均在实体识别任务中达到80%以上的精确率、召回率和F1值，且与Cebuano和Tagalog的跨语言测试显示良好的迁移性能。

Conclusion: HiligayNER为Hiligaynon等资源稀缺语言的命名实体识别提供有效工具，推动菲律宾地区语言技术的发展和多语言NLP研究。

Abstract: The language of Hiligaynon, spoken predominantly by the people of Panay
Island, Negros Occidental, and Soccsksargen in the Philippines, remains
underrepresented in language processing research due to the absence of
annotated corpora and baseline models. This study introduces HiligayNER, the
first publicly available baseline model for the task of Named Entity
Recognition (NER) in Hiligaynon. The dataset used to build HiligayNER contains
over 8,000 annotated sentences collected from publicly available news articles,
social media posts, and literary texts. Two Transformer-based models, mBERT and
XLM-RoBERTa, were fine-tuned on this collected corpus to build versions of
HiligayNER. Evaluation results show strong performance, with both models
achieving over 80% in precision, recall, and F1-score across entity types.
Furthermore, cross-lingual evaluation with Cebuano and Tagalog demonstrates
promising transferability, suggesting the broader applicability of HiligayNER
for multilingual NLP in low-resource settings. This work aims to contribute to
language technology development for underrepresented Philippine languages,
specifically for Hiligaynon, and support future research in regional language
processing.

</details>


### [88] [Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG](https://arxiv.org/abs/2510.10787)
*Zhichao Wang,Cheng Wan,Dong Nie*

Main category: cs.CL

TL;DR: 该综述系统梳理了大语言模型(LLM)在推理阶段扩展性能的新技术，分为输出导向和输入导向方法。


<details>
  <summary>Details</summary>
Motivation: 传统通过增大模型规模和训练数据提升LLM性能的方法由于高质量训练数据的稀缺而遇到瓶颈，促使研究重点转向推理时的扩展。

Method: 综述了推理时扩展的多种技术，包括输出导向的复杂生成和搜索策略，以及输入导向的少样本学习和检索增强生成(RAG)等技术，详细分析RAG的查询扩展、数据、检索、重排序、多模态等方面。

Result: 系统整理并分类了推理时扩展技术，有助于理解和推动LLM性能提升的新范式。

Conclusion: 推理时扩展作为突破训练数据限制的新途径，为提升LLM下游任务表现提供了有效策略，未来研究可沿输出和输入两大方向深入。

Abstract: The performance gains of LLMs have historically been driven by scaling up
model size and training data. However, the rapidly diminishing availability of
high-quality training data is introducing a fundamental bottleneck, shifting
the focus of research toward inference-time scaling. This paradigm uses
additional computation at the time of deployment to substantially improve LLM
performance on downstream tasks without costly model re-training. This review
systematically surveys the diverse techniques contributing to this new era of
inference-time scaling, organizing the rapidly evolving field into two
comprehensive perspectives: Output-focused and Input-focused methods.
Output-focused techniques encompass complex, multi-step generation strategies,
including reasoning (e.g., CoT, ToT, ReAct), various search and decoding
methods (e.g., MCTS, beam search), training for long CoT (e.g., RLVR, GRPO),
and model ensemble methods. Input-focused techniques are primarily categorized
by few-shot and RAG, with RAG as the central focus. The RAG section is further
detailed through a structured examination of query expansion, data, retrieval
and reranker, LLM generation methods, and multi-modal RAG.

</details>


### [89] [Toward Human-Centered Readability Evaluation](https://arxiv.org/abs/2510.10801)
*Bahar İlgen,Georges Hattab*

Main category: cs.CL

TL;DR: 本文提出了一个五维度的评价框架——以人为中心的易读性评分（HCRS），旨在解决传统文本简化评价指标忽视健康信息中的人文特质问题。


<details>
  <summary>Details</summary>
Motivation: 现有的文本简化评估指标主要关注表面特征，忽略了关键信息传达中的清晰度、可信度、语调、文化相关性和可操作性，这在公共健康信息中非常重要。

Method: 基于人机交互和健康传播研究，提出HCRS框架，结合自动测量和结构化的人类反馈，评估文本的关系性和语境可读性。

Result: 讨论了HCRS的设计、如何纳入参与式评价流程，并提出了其经验验证的协议。

Conclusion: HCRS促进健康文本简化评价从表层指标向更符合多样用户需求和体验的方向发展，提高了NLP系统的适用性和可信度。

Abstract: Text simplification is essential for making public health information
accessible to diverse populations, including those with limited health
literacy. However, commonly used evaluation metrics in Natural Language
Processing (NLP), such as BLEU, FKGL, and SARI, mainly capture surface-level
features and fail to account for human-centered qualities like clarity,
trustworthiness, tone, cultural relevance, and actionability. This limitation
is particularly critical in high-stakes health contexts, where communication
must be not only simple but also usable, respectful, and trustworthy. To
address this gap, we propose the Human-Centered Readability Score (HCRS), a
five-dimensional evaluation framework grounded in Human-Computer Interaction
(HCI) and health communication research. HCRS integrates automatic measures
with structured human feedback to capture the relational and contextual aspects
of readability. We outline the framework, discuss its integration into
participatory evaluation workflows, and present a protocol for empirical
validation. This work aims to advance the evaluation of health text
simplification beyond surface metrics, enabling NLP systems that align more
closely with diverse users' needs, expectations, and lived experiences.

</details>


### [90] [Is Implicit Knowledge Enough for LLMs? A RAG Approach for Tree-based Structures](https://arxiv.org/abs/2510.10806)
*Mihir Gupte,Paolo Giusto,Ramesh S*

Main category: cs.CL

TL;DR: 本文提出了一种基于底向上的方法，将树状结构的知识线性化为隐式汇总，用于检索增强生成，以提高处理结构化数据的效率。


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成方法未充分探讨如何最佳表示结构化、尤其是层级树形结构的数据，从而有效生成回应。

Method: 通过在每个层级生成隐式聚合汇总，将树形结构知识线性化，存储在知识库中，并与检索增强生成结合。

Result: 相比直接在原始非结构化代码上使用检索增强生成，方法在生成回复质量相当的情况下，检索文档数减少68%以上，显著提升效率。

Conclusion: 隐式线性化知识表示是一种处理复杂层级数据结构的高效且可扩展的策略。

Abstract: Large Language Models (LLMs) are adept at generating responses based on
information within their context. While this ability is useful for interacting
with structured data like code files, another popular method,
Retrieval-Augmented Generation (RAG), retrieves relevant documents to augment
the model's in-context learning. However, it is not well-explored how to best
represent this retrieved knowledge for generating responses on structured data,
particularly hierarchical structures like trees. In this work, we propose a
novel bottom-up method to linearize knowledge from tree-like structures (like a
GitHub repository) by generating implicit, aggregated summaries at each
hierarchical level. This approach enables the knowledge to be stored in a
knowledge base and used directly with RAG. We then compare our method to using
RAG on raw, unstructured code, evaluating the accuracy and quality of the
generated responses. Our results show that while response quality is comparable
across both methods, our approach generates over 68% fewer documents in the
retriever, a significant gain in efficiency. This finding suggests that
leveraging implicit, linearized knowledge may be a highly effective and
scalable strategy for handling complex, hierarchical data structures.

</details>


### [91] [Happiness is Sharing a Vocabulary: A Study of Transliteration Methods](https://arxiv.org/abs/2510.10827)
*Haeji Jung,Jinju Kim,Kyungjin Kim,Youjeong Roh,David R. Mortensen*

Main category: cs.CL

TL;DR: 本文探讨了不同转写方式（如罗马化、音素转录和替换密码）对多语言模型性能的影响，罗马化表现最佳。


<details>
  <summary>Details</summary>
Motivation: 研究共享文字、重叠词汇和共享语音对多语言模型性能的贡献，寻找提升不同语言间自然语言处理表现的有效方法。

Method: 通过三种转写方法和正字法进行对比实验，在命名实体识别（NER）和自然语言推理（NLI）两个任务上评估模型表现。

Result: 罗马化在8个评测中7个显著优于其他输入类型，验证了其作为最有效转写方式的假设。

Conclusion: 共享更长的子词与预训练语言的一致性提升了模型利用率，罗马化作为转写方法有助于多语言模型性能提升。

Abstract: Transliteration has emerged as a promising means to bridge the gap between
various languages in multilingual NLP, showing promising results especially for
languages using non-Latin scripts. We investigate the degree to which shared
script, overlapping token vocabularies, and shared phonology contribute to
performance of multilingual models. To this end, we conduct controlled
experiments using three kinds of transliteration (romanization, phonemic
transcription, and substitution ciphers) as well as orthography. We evaluate
each model on two downstream tasks -- named entity recognition (NER) and
natural language inference (NLI) -- and find that romanization significantly
outperforms other input types in 7 out of 8 evaluation settings, largely
consistent with our hypothesis that it is the most effective approach. We
further analyze how each factor contributed to the success, and suggest that
having longer (subword) tokens shared with pre-trained languages leads to
better utilization of the model.

</details>


### [92] [DUAL-Bench: Measuring Over-Refusal and Robustness in Vision-Language Models](https://arxiv.org/abs/2510.10846)
*Kaixuan Ren,Preslav Nakov,Usman Naseem*

Main category: cs.CL

TL;DR: 本文提出了一个名为DUAL-Bench的多模态基准，专注于视觉语言模型中的过度拒绝和安全完成问题，评估发现现有模型在该任务上表现较差。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型能力的提升，如何在保持安全性和实用性之间取得平衡成为核心挑战。现有安全机制往往导致模型过度拒绝无害请求，尤其是在图像包含有害内容时表现不佳，缺乏细粒度对齐。

Method: 提出了DUAL-Bench，这是首个针对视觉语言模型中过度拒绝和安全完成的多模态基准，涵盖12种风险类别，并测试模型在语义保持的视觉扰动下的鲁棒性，评估了18个视觉语言模型的安全完成能力。

Result: 实验结果显示目前模型的安全完成率普遍较低，其中GPT-5-Nano达到12.9%，GPT-5系列平均为7.9%，Qwen模型仅为3.9%，表明现有模型还存在较大提升空间。

Conclusion: DUAL-Bench为视觉语言模型的细粒度安全对齐提供了评测平台，促进开发兼顾安全性与实用性的多模态模型对齐策略。

Abstract: As vision-language models become increasingly capable, maintaining a balance
between safety and usefulness remains a central challenge. Safety mechanisms,
while essential, can backfire, causing over-refusal, where models decline
benign requests out of excessive caution. Yet, no existing benchmark has
systematically addressed over-refusal in the visual modality. This setting
introduces unique challenges, such as dual-use cases where an instruction is
harmless, but the accompanying image contains harmful content. Models
frequently fail in such scenarios, either refusing too conservatively or
completing tasks unsafely, which highlights the need for more fine-grained
alignment. The ideal behavior is safe completion, i.e., fulfilling the benign
parts of a request while explicitly warning about any potentially harmful
elements. To address this, we present DUAL-Bench, the first multimodal
benchmark focused on over-refusal and safe completion in VLMs. We evaluated 18
VLMs across 12 hazard categories, with focus on their robustness under
semantics-preserving visual perturbations. The results reveal substantial room
for improvement: GPT-5-Nano achieves 12.9% safe completion, GPT-5 models
average 7.9%, and Qwen models only 3.9%. We hope that DUAL-Bench will foster
the development of more nuanced alignment strategies that ensure models remain
both safe and useful in complex multimodal settings.

</details>


### [93] [Rethinking Agentic Workflows: Evaluating Inference-Based Test-Time Scaling Strategies in Text2SQL Tasks](https://arxiv.org/abs/2510.10885)
*Jiajing Guo,Kenil Patel,Jorge Piazentin Ono,Wenbin He,Liu Ren*

Main category: cs.CL

TL;DR: 本文评估了六种轻量级的Text-to-SQL测试时扩展策略和四款大型语言模型，重点分析了推理模型在工业数据库查询中的表现及其实际部署中的延迟和资源消耗。


<details>
  <summary>Details</summary>
Motivation: 当前Text-to-SQL系统越来越依赖大型语言模型，但其测试时扩展策略在真实应用场景，尤其是最新推理模型中的效果尚不明确。

Method: 本文在BIRD Mini-Dev基准上，对六种行业导向的测试时扩展策略和四款不同LLM（含两款推理模型）进行性能评测，除了准确率，还测量了推理延迟和token消耗，深入分析策略对系统效率和复杂度的影响。

Result: 实验发现，Divide-and-Conquer提示法和少样本演示能持续提升通用和推理型LLM的性能；而增加流程步骤效果不一，基础模型的选择对结果影响显著。

Conclusion: 本文揭示了在部署Text2SQL系统时准确率、效率与系统复杂度之间的实际权衡，为工业应用中模型选择和测试时扩展策略提供了指导。

Abstract: Large language models (LLMs) are increasingly powering Text-to-SQL (Text2SQL)
systems, enabling non-expert users to query industrial databases using natural
language. While test-time scaling strategies have shown promise in LLM-based
solutions, their effectiveness in real-world applications, especially with the
latest reasoning models, remains uncertain. In this work, we benchmark six
lightweight, industry-oriented test-time scaling strategies and four LLMs,
including two reasoning models, evaluating their performance on the BIRD
Mini-Dev benchmark. Beyond standard accuracy metrics, we also report inference
latency and token consumption, providing insights relevant for practical system
deployment. Our findings reveal that Divide-and-Conquer prompting and few-shot
demonstrations consistently enhance performance for both general-purpose and
reasoning-focused LLMs. However, introducing additional workflow steps yields
mixed results, and base model selection plays a critical role. This work sheds
light on the practical trade-offs between accuracy, efficiency, and complexity
when deploying Text2SQL systems.

</details>


### [94] [LLM$\times$MapReduce-V3: Enabling Interactive In-Depth Survey Generation through a MCP-Driven Hierarchically Modular Agent System](https://arxiv.org/abs/2510.10890)
*Yu Chao,Siyu Lin,xiaorong wang,Zhu Zhang,Zihan Zhou,Haoyu Wang,Shuo Wang,Jie Zhou,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: 本文提出了基于多代理架构的LLM x MapReduce-V3系统，用于生成长篇综述，支持模块化规划和人机交互，提升内容深度和长度。


<details>
  <summary>Details</summary>
Motivation: 提升长篇综述生成的质量和可控性，通过模块化架构和人机协作增强系统灵活性和定制化。

Method: 设计多代理系统，将功能组件拆分为模型-上下文-协议（MCP）服务器，采用层级结构和高层规划代理动态组织流程，支持多轮交互以精准捕捉研究视角。

Result: 系统在人类评估中显著优于代表性基线方法，在内容深度和长度方面有明显提升。

Conclusion: 基于MCP的模块化规划与多代理协作机制有效提升了长篇综述的生成质量，增强了用户对研究过程的控制能力。

Abstract: We introduce LLM x MapReduce-V3, a hierarchically modular agent system
designed for long-form survey generation. Building on the prior work, LLM x
MapReduce-V2, this version incorporates a multi-agent architecture where
individual functional components, such as skeleton initialization, digest
construction, and skeleton refinement, are implemented as independent
model-context-protocol (MCP) servers. These atomic servers can be aggregated
into higher-level servers, creating a hierarchically structured system. A
high-level planner agent dynamically orchestrates the workflow by selecting
appropriate modules based on their MCP tool descriptions and the execution
history. This modular decomposition facilitates human-in-the-loop intervention,
affording users greater control and customization over the research process.
Through a multi-turn interaction, the system precisely captures the intended
research perspectives to generate a comprehensive skeleton, which is then
developed into an in-depth survey. Human evaluations demonstrate that our
system surpasses representative baselines in both content depth and length,
highlighting the strength of MCP-based modular planning.

</details>


### [95] [ADVICE: Answer-Dependent Verbalized Confidence Estimation](https://arxiv.org/abs/2510.10913)
*Ki Jung Seo,Sehun Lim,Taeuk Kim*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在表达自信度时的过度自信问题，提出了基于答案依赖的自信估计方法ADVICE，显著提升了自信度校准效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然能以自然语言表达自信，但经常出现过度自信现象，其成因尚不清楚。

Method: 提出ADVICE，一个基于答案依赖的微调框架，使自信估计与模型答案关联，从而提升自信度的准确性。

Result: 大量实验表明ADVICE能够大幅改善自信度校准，同时保持任务性能不变。进一步分析证明ADVICE增强了答案依赖性，使自信分布更均衡且校准良好。

Conclusion: 本文揭示了过度自信的根本原因，构建了更可信的自信语言表达框架，为提升大型语言模型的透明性和可靠性提供了理论和实践支持。

Abstract: Recent progress in large language models (LLMs) has enabled them to express
their confidence in natural language, enhancing transparency and reliability.
However, their confidence often exhibits overconfidence, the cause of which
remains poorly understood. In this work, we conduct a detailed analysis of the
dynamics underlying verbalized confidence and identify answer-independence as a
key factor, defined as the model's failure to condition confidence on its own
answer. To address this, we propose ADVICE (Answer-Dependent Verbalized
Confidence Estimation), a fine-tuning framework that facilitates
answer-grounded confidence estimation. Extensive experiments show that ADVICE
substantially improves confidence calibration while preserving task
performance. Further analyses confirm that ADVICE strengthens
answer-groundedness, leading to more balanced and well-calibrated confidence
distributions. Our findings shed light on the origin of overconfidence and
establish a framework for more trustworthy confidence verbalization.

</details>


### [96] [GapDNER: A Gap-Aware Grid Tagging Model for Discontinuous Named Entity Recognition](https://arxiv.org/abs/2510.10927)
*Yawen Yang,Fukun Ma,Shiao Meng,Aiwei Liu,Lijie Wen*

Main category: cs.CL

TL;DR: 提出了一种名为GapDNER的间隙感知网格标注模型，通过对实体片段间的上下文间隙进行表示学习，有效提升了非连续命名实体识别的性能。


<details>
  <summary>Details</summary>
Motivation: 非连续命名实体存在片段间上下文不连续及重叠等问题，传统方法易出现误差传播和解码歧义。

Method: 将上下文间隙视为特殊跨度，通过双向格子标注，将跨度分类转为令牌对格子标注任务，设计了内部规律提取模块和跨跨度关系增强模块，分别捕捉跨度内规律和不同跨度间语义关系。推理阶段对每个实体片段和间隙赋向边，使用广度优先搜索解码路径。

Result: 在三个数据集上实验证明GapDNER在非连续命名实体识别任务上达到了新的最优性能，特别在复杂实体结构识别上表现出显著优势。

Conclusion: GapDNER通过创新的间隙表示和双重注意机制，有效解决了非连续实体识别中的解码歧义和误差传播问题，推动了该领域性能提升。

Abstract: In biomedical fields, one named entity may consist of a series of
non-adjacent tokens and overlap with other entities. Previous methods recognize
discontinuous entities by connecting entity fragments or internal tokens, which
face challenges of error propagation and decoding ambiguity due to the wide
variety of span or word combinations. To address these issues, we deeply
explore discontinuous entity structures and propose an effective Gap-aware grid
tagging model for Discontinuous Named Entity Recognition, named GapDNER. Our
GapDNER innovatively applies representation learning on the context gaps
between entity fragments to resolve decoding ambiguity and enhance
discontinuous NER performance. Specifically, we treat the context gap as an
additional type of span and convert span classification into a token-pair grid
tagging task. Subsequently, we design two interactive components to
comprehensively model token-pair grid features from both intra- and inter-span
perspectives. The intra-span regularity extraction module employs the biaffine
mechanism along with linear attention to capture the internal regularity of
each span, while the inter-span relation enhancement module utilizes
criss-cross attention to obtain semantic relations among different spans. At
the inference stage of entity decoding, we assign a directed edge to each
entity fragment and context gap, then use the BFS algorithm to search for all
valid paths from the head to tail of grids with entity tags. Experimental
results on three datasets demonstrate that our GapDNER achieves new
state-of-the-art performance on discontinuous NER and exhibits remarkable
advantages in recognizing complex entity structures.

</details>


### [97] [Evaluating Language Models' Evaluations of Games](https://arxiv.org/abs/2510.10930)
*Katherine M. Collins,Cedegao E. Zhang,Graham Todd,Lance Ying,Mauricio Barba da Costa,Ryan Liu,Prafull Sharma,Adrian Weller,Ionatan Kuperwajs,Lionel Wong,Joshua B. Tenenbaum,Thomas L. Griffiths*

Main category: cs.CL

TL;DR: 本文提出了一种评估人工智能系统对游戏评价能力的新范式，通过大规模棋盘游戏数据集对比语言和推理模型与人类及符号计算代理的评估结果。


<details>
  <summary>Details</summary>
Motivation: 现有人工智能评估多集中于解决问题能力，忽视了评估哪些问题值得解决的能力，即AI系统对问题评价的能力。

Method: 构建评价AI系统对游戏评价的形式化框架，利用100多个新棋盘游戏和450多人类评价，比较现代语言和推理模型与人类及符号计算代理的评估结果，考察游戏的收益（公平性）和趣味性两种查询。

Result: 推理模型在游戏评估上更贴近人类，但随着模型趋于博弈论最优，其与人类的符合度反而下降；趣味性评估结果波动大；推理模型在资源使用上表现高度不稳定。

Conclusion: 推理模型在游戏评价任务中表现优于非推理模型，但需要引入更多资源理性元推理以提高评估的稳定性和人类一致性。

Abstract: Reasoning is not just about solving problems -- it is also about evaluating
which problems are worth solving at all. Evaluations of artificial intelligence
(AI) systems primarily focused on problem solving, historically by studying how
models play games such as chess and Go. In this paper, we advocate for a new
paradigm that assesses AI systems' evaluation of games. First, we introduce a
formalism for evaluating such evaluations. We then leverage a large-scale
dataset of over $100$ novel board games and over 450 human judgments to compare
evaluations produced by modern language and reasoning models against those of
people and symbolic computational agents. We consider two kinds of evaluative
queries: assessing the payoff (or fairness) and the funness of games. These
queries span two dimensions relevant to the design of evaluations of AI
evaluations: how complex a query is to compute and how difficult a query is to
quantify. Our results show that reasoning models are generally more aligned to
people in their evaluations of games than non-reasoning language models.
However, we observe a non-monotonic relationship: as models get closer to
game-theoretic optimal, their fit to human data weakens. We also observe more
"jaggedness" across models for assessing funness, in line with the greater
difficulty of quantifying this query. Across queries and games, reasoning
models show highly variable and unpredictable resource usage when assessing
queries, pointing to the importance of imbuing more resource-rational
meta-reasoning in language and reasoning models.

</details>


### [98] [End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF: A Reproducibility Study](https://arxiv.org/abs/2510.10936)
*Anirudh Ganesh,Jayavardhan Reddy*

Main category: cs.CL

TL;DR: 本文复现了Ma和Hovy(2016)提出的BiLSTM-CNN-CRF模型，在序列标注任务中表现出色，特别是在命名实体识别和词性标注上取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: 验证并复现Ma和Hovy提出的神经网络架构在序列标注任务中的有效性，推动相关研究的开展。

Method: 结合字符级CNN、词级双向LSTM和条件随机场，实现端到端的序列标注模型，无需手工特征。

Result: 成功复现关键结果，在CoNLL-2003命名实体识别任务上达到91.18%的F1值，证明模型的优越性能。

Conclusion: 该模型结构有效且稳定，公开了PyTorch实现以促进后续研究。

Abstract: We present a reproducibility study of the state-of-the-art neural
architecture for sequence labeling proposed by Ma and Hovy
(2016)\cite{ma2016end}. The original BiLSTM-CNN-CRF model combines
character-level representations via Convolutional Neural Networks (CNNs),
word-level context modeling through Bi-directional Long Short-Term Memory
networks (BiLSTMs), and structured prediction using Conditional Random Fields
(CRFs). This end-to-end approach eliminates the need for hand-crafted features
while achieving excellent performance on named entity recognition (NER) and
part-of-speech (POS) tagging tasks. Our implementation successfully reproduces
the key results, achieving 91.18\% F1-score on CoNLL-2003 NER and demonstrating
the model's effectiveness across sequence labeling tasks. We provide a detailed
analysis of the architecture components and release an open-source PyTorch
implementation to facilitate further research.

</details>


### [99] [Punctuation-aware treebank tree binarization](https://arxiv.org/abs/2510.10951)
*Eitan Klinger,Vivaan Wadhwa,Jungyeul Park*

Main category: cs.CL

TL;DR: 本文提出了一个考虑标点符号的树库二元化方法，保留标点作为兄弟节点，有效提升了头部子节点预测的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统二元化流程在头部选择前丢弃标点，改变了成分结构，影响了头部子节点识别效果。

Method: 提供了一个可复现的管线，保留标点作为兄弟节点，附带中间标记、可逆标志和对齐索引等衍生材料，并设计了评估套件评测头部预测、可逆性及与派生资源结构兼容性。

Result: 在Penn Treebank上，改进后的预处理使头部预测准确率由73.66%（Collins规则）提升到91.85%，与CCGbank派生对齐竞争力强。

Conclusion: 标点感知的树库二元化显著提升头部识别并保持结构兼容性，所开放代码与资源支持复现及扩展。

Abstract: This article presents a curated resource and evaluation suite for
punctuation-aware treebank binarization. Standard binarization pipelines drop
punctuation before head selection, which alters constituent shape and harms
head-child identification. We release (1) a reproducible pipeline that
preserves punctuation as sibling nodes prior to binarization, (2) derived
artifacts and metadata (intermediate @X markers, reversibility signatures,
alignment indices), and (3) an accompanying evaluation suite covering
head-child prediction, round-trip reversibility, and structural compatibility
with derivational resources (CCGbank). On the Penn Treebank, punctuation-aware
preprocessing improves head prediction accuracy from 73.66\% (Collins rules)
and 86.66\% (MLP) to 91.85\% with the same classifier, and achieves competitive
alignment against CCGbank derivations. All code, configuration files, and
documentation are released to enable replication and extension to other
corpora.

</details>


### [100] [KOTOX: A Korean Toxic Dataset for Deobfuscation and Detoxification](https://arxiv.org/abs/2510.10961)
*Yejin Lee,Su-Hyeon Kim,Hyundong Jin,Dayoung Kim,Yeonsoo Kim,Yo-Sub Han*

Main category: cs.CL

TL;DR: 本文提出了KOTOX，一个用于韩语文本去模糊化和去毒性的毒性内容数据集，旨在解决低资源语言中有害表达检测困难的问题。


<details>
  <summary>Details</summary>
Motivation: 由于大部分现有毒性内容检测研究集中于英语，导致低资源语言（如韩语）中的有害内容识别和去毒效果较差，尤其是在用户使用各种模糊手段逃避检测时问题更加突出。

Method: 基于韩语的语言特性，分类总结了多种文本模糊化方法，定义了一套基于真实例子的变换规则，进而构建了包含三种难度（easy、normal、hard）的数据集版本，支持同时进行去模糊化和去毒性处理。

Result: 创建了KOTOX数据集，这是首个支持韩语去模糊化和去毒性的资源，并公开了相应代码和数据，方便后续研究基于低资源语言的大型语言模型改进。

Conclusion: 该数据集有助于提升低资源语言中大型语言模型对被模糊化有害内容的理解与缓解能力，推动韩语及类似语言的毒性内容检测技术发展。

Abstract: Toxic content has become an increasingly critical social issue with the rapid
expansion of online communication. While numerous studies explored methods for
detecting and detoxifying such content, most have focused primarily on English,
leaving low-resource language underrepresented. Consequently, Large Language
Models~(LLMs) often struggle to identify and neutralize toxic expressions in
these languages. This challenge becomes even more pronounced when user employ
obfuscation techniques to evade detection systems. Therefore, we propose a
\textbf{KOTOX: Korean Toxic Dataset} for deobfuscation and detoxicification to
address this issue. We categorize various obfuscation approaches based on
linguistic characteristics of Korean and define a set of transformation rules
grounded in real-word examples. Using these rules, we construct three dataset
versions (easy, normal, and hard) representing different levels of obfuscation
difficulty. This is the first dataset that simultaneously supports
deobfuscation and detoxification for the Korean language. We expect it to
facilitate better understanding and mitigating of obfuscated toxic content in
LLM for low-resource languages. Our code and data are available at
https://github.com/leeyejin1231/KOTOX.

</details>


### [101] [Judge Before Answer: Can MLLM Discern the False Premise in Question?](https://arxiv.org/abs/2510.10965)
*Jidong Li,Lingyong Fang,Haodong Zhao,Sufeng Duan,Gongshen Liu*

Main category: cs.CL

TL;DR: 本文提出了一套自动化构建多模态大语言模型虚假前提问题的基准数据集JBA，细分前提类型，并设计了识别增强框架以提升模型识别虚假前提的能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型虽取得显著进展，但在识别虚假前提方面表现较弱，且现有评测缺乏细粒度分类和覆盖，难以充分评估模型能力。

Method: 提出一套自动化流程构建JBA数据集，系统地将虚假前提分为三大类十三子类。同时设计了针对虚假前提识别的增强训练框架。

Result: 实验结果表明，当前模型在虚假前提识别方面表现不佳，应用本方法训练后模型识别能力显著提升。

Conclusion: 该研究填补了虚假前提识别评测的空白，所提框架有效提升了多模态大语言模型识别虚假前提的鲁棒性和准确性。

Abstract: Multimodal large language models (MLLMs) have witnessed astonishing
advancements in recent years. Despite these successes, MLLMs remain vulnerable
to flase premise problems. However, existing benchmarks targeting this issue
are limited in scope: they often lack fine-grained categorization, exhibit
insufficient coverage, and thus fail to provide a rigorous evaluation of the
ability of models to recognize false premises. To bridge this gap, we introduce
a fully automated pipeline for constructing a comprehensive benchmark of false
premise questions. Our method systematically categorizes the premises into
three main types and thirteen subtypes according to the abilities required to
identify the premises, resulting in the JBA dataset.Results show current MLLMs
still struggle with false premise recognition. Building upon this benchmark, we
further propose a recognition enhancement framework tailored to strengthen the
robustness of MLLMs to detect false premises. Extensive experiments demonstrate
that models trained with our framework achieve significant improvements in
false premise recognition.

</details>


### [102] [RV-HATE: Reinforced Multi-Module Voting for Implicit Hate Speech Detection](https://arxiv.org/abs/2510.10971)
*Yejin Lee,Hyeseon Ahn,Yo-Sub Han*

Main category: cs.CL

TL;DR: RV-HATE是一种针对不同仇恨言论数据集特征定制的检测框架，通过多模块和强化学习优化权重提高检测准确率，同时提供数据集特征的可解释性。


<details>
  <summary>Details</summary>
Motivation: 仇恨言论数据集具有多样性，传统固定方法难以适应不同数据集的特定语言和社会背景特征，导致检测效果有限。

Method: RV-HATE框架包含多个专注于不同语言和上下文特征的模块，采用强化学习优化模块权重，通过投票机制整合模块输出生成最终决策。

Result: 该方法显著提高了对隐含仇恨言论的检测准确率，优于传统静态方法。

Conclusion: 通过定制化和权重优化，RV-HATE有效应对不同数据集特征，提升仇恨言论检测性能与解释性。

Abstract: Hate speech remains prevalent in human society and continues to evolve in its
forms and expressions. Modern advancements in internet and online anonymity
accelerate its rapid spread and complicate its detection. However, hate speech
datasets exhibit diverse characteristics primarily because they are constructed
from different sources and platforms, each reflecting different linguistic
styles and social contexts. Despite this diversity, prior studies on hate
speech detection often rely on fixed methodologies without adapting to
data-specific features. We introduce RV-HATE, a detection framework designed to
account for the dataset-specific characteristics of each hate speech dataset.
RV-HATE consists of multiple specialized modules, where each module focuses on
distinct linguistic or contextual features of hate speech. The framework
employs reinforcement learning to optimize weights that determine the
contribution of each module for a given dataset. A voting mechanism then
aggregates the module outputs to produce the final decision. RV-HATE offers two
primary advantages: (1)~it improves detection accuracy by tailoring the
detection process to dataset-specific attributes, and (2)~it also provides
interpretable insights into the distinctive features of each dataset.
Consequently, our approach effectively addresses implicit hate speech and
achieves superior performance compared to conventional static methods. Our code
is available at https://github.com/leeyejin1231/RV-HATE.

</details>


### [103] [Enhancing Large Language Model Reasoning via Selective Critical Token Fine-Tuning](https://arxiv.org/abs/2510.10974)
*Zhiwen Ruan,Yixia Li,He Zhu,Yun Chen,Peng Li,Yang Liu,Guanhua Chen*

Main category: cs.CL

TL;DR: 本文提出了一种名为“关键标记微调（CFT）”的方法，通过仅微调对推理结果关键的少数标记，提升了大语言模型在数学推理任务中的表现和生成多样性。


<details>
  <summary>Details</summary>
Motivation: 现有的监督微调方法对所有标记一视同仁，忽视了推理正确性实际上由少数关键标记决定，导致模型输出多样性降低及泛化能力有限。

Method: CFT通过反事实扰动识别功能上不可或缺的关键标记，仅对这些关键标记进行梯度更新，从而集中训练信号于决定性推理步骤，同时保持非关键标记的多样性。

Result: 在五款模型、三大模型系列和十一项数学推理基准测试中，CFT以不到12%的标记微调量，持续优于标准的监督微调，并提升了测试时采样多样性及强化学习初始效果。

Conclusion: CFT作为一种简便高效的微调框架，能够实现更稳健和高效的大语言模型微调，提升模型推理性能和探索能力，是一种实用且通用的方法。

Abstract: Large language models (LLMs) primarily rely on supervised fine-tuning (SFT)
as a key method to adapt pre-trained models to domain-specific tasks such as
mathematical reasoning. However, standard SFT uniformly penalizes all tokens,
neglecting that only a small subset of critical tokens determines reasoning
correctness. This uniform supervision often causes reduced output diversity and
limited generalization. We propose Critical Token Fine-tuning (CFT), a simple
yet effective approach that updates only tokens identified as functionally
indispensable via counterfactual perturbations. By focusing gradient signals on
these decisive reasoning steps while preserving the diversity of non-critical
tokens, CFT can enhance both generation and diversity. Extensive experiments on
five models across three families (Qwen, OLMo, LLaMA) and eleven mathematical
reasoning benchmarks show that CFT, despite fine-tuning on less than 12% of
tokens, consistently outperforms standard SFT. Moreover, CFT enables test-time
scaling through improved sampling diversity and provides a stronger
initialization for reinforcement learning, sustaining performance gains in
later training stages while maintaining higher entropy for better exploration.
These results highlight CFT as a practical and general framework for efficient
and robust LLM fine-tuning.

</details>


### [104] [DeepResearchGuard: Deep Research with Open-Domain Evaluation and Multi-Stage Guardrails for Safety](https://arxiv.org/abs/2510.10994)
*Wei-Chieh Huang,Henry Peng Zou,Yaozu Wu,Dongyuan Li,Yankai Chen,Weizhi Zhang,Yangning Li,Angelo Zangari,Jizhou Guo,Chunyu Miao,Liancheng Fang,Langzhou He,Renhe Jiang,Philip S. Yu*

Main category: cs.CL

TL;DR: 本文提出了一种名为DEEPRESEARCHGUARD的深度研究框架安全防护机制，针对现有深度研究框架在报告质量评估和安全防护方面的不足进行改进，通过四阶段防护和开放域评估显著提升安全性和报告质量。


<details>
  <summary>Details</summary>
Motivation: 当前深度研究框架缺乏完善的评价流程和阶段性安全防护，单纯依靠问答准确率评价，忽略了报告的可信度、连贯性、广度、深度和安全性，存在引入有害信息的风险。

Method: 提出DEEPRESEARCHGUARD，包含四阶段防护措施，针对开放域参考文献和报告进行评价，设计阶段性评估基准DRSAFEBENCH，对多种先进大模型进行性能测试。

Result: DEEPRESEARCHGUARD使防御成功率提升18.16%，同时过度拒绝率降低6%。输入阶段过滤明显风险，计划和研究阶段增强引用纪律和来源可信度。

Conclusion: DEEPRESEARCHGUARD实现了全面的开放域评价和阶段感知防护，有效阻止有害信息传播，提升报告质量且不过度拒绝，有助于深度研究框架的安全与可靠性提升。

Abstract: Deep research frameworks have shown promising capabilities in synthesizing
comprehensive reports from web sources. While deep research possesses
significant potential to address complex issues through planning and research
cycles, existing frameworks are deficient in sufficient evaluation procedures
and stage-specific protections. They typically treat evaluation as exact match
accuracy of question-answering, but overlook crucial aspects of report quality
such as credibility, coherence, breadth, depth, and safety. This oversight may
result in hazardous or malicious sources being integrated into the final
report. To address these issues, we introduce DEEPRESEARCHGUARD, a
comprehensive framework featuring four-stage safeguards with open-domain
evaluation of references and reports. We assess performance across multiple
metrics, e.g., defense success rate and over-refusal rate, and five key report
dimensions. In the absence of a suitable safety benchmark, we introduce
DRSAFEBENCH, a stage-wise benchmark for deep research safety. Our evaluation
spans diverse state-of-the-art LLMs, including GPT-4o, Gemini-2.5-flash,
DeepSeek-v3, and o4-mini. DEEPRESEARCHGUARD achieves an average defense success
rate improvement of 18.16% while reducing over-refusal rate by 6%. The input
guard provides the most substantial early-stage protection by filtering out
obvious risks, while the plan and research guards enhance citation discipline
and source credibility. Through extensive experiments, we show that
DEEPRESEARCHGUARD enables comprehensive open-domain evaluation and stage-aware
defenses that effectively block harmful content propagation, while
systematically improving report quality without excessive over-refusal rates.
The code can be found via https://github.com/Jasonya/DeepResearchGuard.

</details>


### [105] [ABLEIST: Intersectional Disability Bias in LLM-Generated Hiring Scenarios](https://arxiv.org/abs/2510.10998)
*Mahika Phutane,Hayoung Jung,Matthew Kim,Tanushree Mitra,Aditya Vashistha*

Main category: cs.CL

TL;DR: 本文通过全面审核六个大型语言模型在2820个招聘场景中的表现，揭示了模型对残疾人特别是交叉边缘群体的歧视偏见。


<details>
  <summary>Details</summary>
Motivation: 现有研究多聚焦西方背景，忽视全球南方交叉边缘化身份对残疾人经历的影响。本研究旨在弥补这一空白，评估大型语言模型在招聘中对残疾人及其交叉身份的歧视。

Method: 引入ABLEIST指标，包含五个残疾歧视相关和三个交叉伤害指标，在涵盖多样残疾、性别、国籍和种姓身份的2820个招聘场景中，对六个大型语言模型进行了全面审计。

Result: 发现大型语言模型对残疾候选人的ABLEIST伤害显著增加，许多现有模型检测不到这些偏见。同时，交叉身份（如性别和种姓）进一步加剧了这些伤害，尤其是表面象征性支持（Tokenism）。

Conclusion: 当前主流模型和安全工具存在严重盲点，亟需在招聘等高风险领域进行交叉身份安全评估，以减少对残疾人尤其是多重边缘化群体的歧视。

Abstract: Large language models (LLMs) are increasingly under scrutiny for perpetuating
identity-based discrimination in high-stakes domains such as hiring,
particularly against people with disabilities (PwD). However, existing research
remains largely Western-centric, overlooking how intersecting forms of
marginalization--such as gender and caste--shape experiences of PwD in the
Global South. We conduct a comprehensive audit of six LLMs across 2,820 hiring
scenarios spanning diverse disability, gender, nationality, and caste profiles.
To capture subtle intersectional harms and biases, we introduce ABLEIST
(Ableism, Inspiration, Superhumanization, and Tokenism), a set of five
ableism-specific and three intersectional harm metrics grounded in disability
studies literature. Our results reveal significant increases in ABLEIST harms
towards disabled candidates--harms that many state-of-the-art models failed to
detect. These harms were further amplified by sharp increases in intersectional
harms (e.g., Tokenism) for gender and caste-marginalized disabled candidates,
highlighting critical blind spots in current safety tools and the need for
intersectional safety evaluations of frontier models in high-stakes domains
like hiring.

</details>


### [106] [DND: Boosting Large Language Models with Dynamic Nested Depth](https://arxiv.org/abs/2510.11001)
*Tieyuan Chen,Xiaodong Chen,Haoxing Chen,Zhenzhong Lan,Weiyao Lin,Jianguo Li*

Main category: cs.CL

TL;DR: 本文提出了一种动态嵌套深度(DND)方法，通过动态选择关键token重新处理来提升大型语言模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型对所有token一视同仁，导致计算资源浪费，难以高效处理难点token。

Method: 在每层transformer末尾，利用路由器识别关键token，重新处理这些token以"复习"难点，同时采用管控损失和阈值控制确保选择准确与稳定。

Result: DND方法在预训练模型后训练阶段直接集成，在多个基准测试中，提升了Qwen3-1.7B和Qwen3-30B-A3B的性能分别为1.88%和0.87%，计算和参数开销极小。

Conclusion: DND有效提升了预训练大型语言模型的性能，通过动态重点复习关键token，提高模型计算效能及表现。

Abstract: We introduce Dynamic Nested Depth (DND), a novel method that improves
performance for off-the-shelf LLMs by selecting critical tokens to reprocess in
a nested depth manner. Specifically, at the end of the given transformer layer,
DND identifies more critical tokens with a router and feeds them back for an
extra round of processing, effectively ``reviewing" difficult tokens while
avoiding redundant computation for easier ones. The dynamic selection mechanism
is tailored for precise control via two novel strategies: a router controlling
loss to enhance token selection distinguishability, and a threshold control
scheme to ensure selection stability. We demonstrate the effectiveness of DND
by directly integrating it into pre-trained dense and MoE models during a
post-training phase. On diverse benchmarks, this approach boosts the
performances of the dense Qwen3-1.7B by 1.88% and the MoE Qwen3-30B-A3B by
0.87%, all with a minimal parameter and computing increase.

</details>


### [107] [LogiNumSynth: Synthesizing Joint Logical-Numerical Reasoning Problems for Language Models](https://arxiv.org/abs/2510.11031)
*Yiwei Liu,Yucheng Li,Xiao Li,Gong Cheng*

Main category: cs.CL

TL;DR: 本文提出LogiNumSynth，一种结合逻辑与数值推理的自然语言问题合成器，支持控制推理难度，用于评估和训练大模型。


<details>
  <summary>Details</summary>
Motivation: 现有数据集依赖固定规则，限制了评估和训练的通用性，难以有效提升模型的逻辑与数值推理能力。

Method: 设计LogiNumSynth，灵活合成逻辑推理与数值计算结合的任务，支持调控推理世界复杂度、逻辑推理深度和数值计算复杂度，并通过过程准确率与答案准确率进行评估。

Result: 实验证明多个大模型在逻辑-数值推理上仍表现欠佳，LogiNumSynth可作为诊断工具和针对性训练数据来源，有效增强模型综合推理能力。

Conclusion: LogiNumSynth为提升语言模型的联合逻辑-数值推理提供了灵活的数据合成与评估手段，促进推理能力的系统改进。

Abstract: Joint logical-numerical reasoning remains a major challenge for language
models, yet existing datasets rely on fixed rule sets and offer limited control
over task complexity, constraining their generalizability for evaluation and
training. We present LogiNumSynth, a flexible natural language problem
synthesizer that synthesizes tasks requiring proficiency in joint logical
reasoning (e.g., rule-based reasoning) and numerical reasoning (e.g.,
arithmetic computation). LogiNumSynth supports fine-grained control over
reasoning world richness, logical reasoning depth, and the complexity of
numerical computations, enabling flexible data synthesis across difficulty
levels. We demonstrate three key contributions: (1) Synthesizer -- synthesizing
fully controllable joint reasoning tasks over natural language; (2) Evaluation
& Process Analysis -- evaluating both process accuracy and answer accuracy; (3)
Targeted Training -- using synthesized data to enhance LLMs' reasoning
performance. Experiments with multiple LLMs highlight persistent weaknesses in
logical-numerical reasoning, showing that LogiNumSynth can serve as both a
diagnostic tool and a source of targeted supervision for advancing integrated
reasoning skills.

</details>


### [108] [Enabling Doctor-Centric Medical AI with LLMs through Workflow-Aligned Tasks and Benchmarks](https://arxiv.org/abs/2510.11040)
*Wenya Xie,Qingying Xiao,Yu Zheng,Xidong Wang,Junying Chen,Ke Ji,Anningzhe Gao,Prayag Tiwari,Xiang Wan,Feng Jiang,Benyou Wang*

Main category: cs.CL

TL;DR: 本文提出将大型语言模型作为医生的临床助手，协助医生而非直接面向患者，构建了涵盖多临床任务和专科的大规模中文医疗问答数据集DoctorFLAN，并通过多个测试集评估模型性能，显著提升开源模型在医疗场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型直接对患者提供临床指导存在安全风险，缺乏领域专业知识，因此需要将其定位为辅助医生的工具，贴合真实临床需求。

Method: 通过两阶段灵感反馈调查收集真实临床需求，构建包含92,000条问答数据的DoctorFLAN数据集，涵盖22个临床任务和27个专科，并设计DoctorFLAN-test和DotaBench测试集评估模型表现。

Result: 在十余种流行大型语言模型上进行实验，DoctorFLAN显著提升了开源模型在医疗领域的表现，使其更符合医生的工作流程，弥补了现有面向患者模型的不足。

Conclusion: 本研究提供了一个有效的资源和框架，推动了面向医生的医疗大型语言模型的开发，有助于安全且高效地将LLM技术应用于临床环境。

Abstract: The rise of large language models (LLMs) has transformed healthcare by
offering clinical guidance, yet their direct deployment to patients poses
safety risks due to limited domain expertise. To mitigate this, we propose
repositioning LLMs as clinical assistants that collaborate with experienced
physicians rather than interacting with patients directly. We conduct a
two-stage inspiration-feedback survey to identify real-world needs in clinical
workflows. Guided by this, we construct DoctorFLAN, a large-scale Chinese
medical dataset comprising 92,000 Q&A instances across 22 clinical tasks and 27
specialties. To evaluate model performance in doctor-facing applications, we
introduce DoctorFLAN-test (550 single-turn Q&A items) and DotaBench (74
multi-turn conversations). Experimental results with over ten popular LLMs
demonstrate that DoctorFLAN notably improves the performance of open-source
LLMs in medical contexts, facilitating their alignment with physician workflows
and complementing existing patient-oriented models. This work contributes a
valuable resource and framework for advancing doctor-centered medical LLM
development

</details>


### [109] [Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by Refining Belief States](https://arxiv.org/abs/2510.11052)
*Qinglin Zhu,Yizhen Yao,Runcong Zhao,Yanzheng Xiang,Amrutha Saseendran,Chen Jin,Philip Alexander Teare,Bin Liang,Yulan He,Lin Gui*

Main category: cs.CL

TL;DR: 本文提出了一种名为Latent Refinement Decoding (LRD)的两阶段解码框架，结合了潜变量细化和预测反馈循环，有效提升了自然语言生成的准确性和生成速度。


<details>
  <summary>Details</summary>
Motivation: 传统的自回归模型生成速度慢，扩散式方法虽支持并行生成，但存在信息丢失和过早定局的问题，导致生成质量受限。

Method: LRD框架的第一阶段通过保持掩码位置的分布混合，提升全局一致性；第二阶段迭代确定可信的标记并保留不确定标记，利用KL散度动态判断收敛与提前停止。

Result: 在代码生成和推理任务上，LRD分别提升了HumanEval +6.3、MBPP +2.6、GSM8K +2.9、MATH500 +3.8的准确率，同时实现最多10.6倍的速度提升。

Conclusion: LRD是一种强大且多功能的并行序列生成备选方案，兼顾速度与准确性，有效克服了现有扩散式生成方法的核心缺陷。

Abstract: Autoregressive (AR) models remain the standard for natural language
generation but still suffer from high latency due to strictly sequential
decoding. Recent diffusion-inspired approaches, such as LlaDA and Dream,
mitigate this by generating in parallel, yet they suffer from two core
limitations: information loss, as predictive distributions for non-finalized
tokens are discarded at each step, and premature commitment, where local
decisions are made without sufficient global coordination. We introduce Latent
Refinement Decoding (LRD), a two-stage framework with Latent Refinement and a
Predictive Feedback Loop. The first stage maintains masked positions as
distributional mixtures of predicted tokens and the mask embedding, allowing
the model to establish more globally consistent beliefs. The second stage
progressively finalizes confident tokens while retaining uncertain ones for
iterative feedback. KL-divergence dynamics provide a principled and reliable
criterion for convergence and early stopping. Experiments across coding
(HumanEval +6.3, MBPP +2.6) and reasoning (GSM8K +2.9, MATH500 +3.8) show that
LRD improves accuracy while delivering speedups of up to 10.6x, making it a
strong and versatile alternative for parallel sequence generation.

</details>


### [110] [Enhancing LLM Reasoning via Non-Human-Like Reasoning Path Preference Optimization](https://arxiv.org/abs/2510.11104)
*Junjie Lu,Yuliang Liu,Chaofeng Qu,Wei Shen,Zhouhan Lin,Min Xu*

Main category: cs.CL

TL;DR: 本文提出了一种基于置信度引导的推理路径偏好优化方法（CGPO），通过识别模型推理中的最大不确定点，利用自生成的非人类推理路径进行指导，从而提升大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法倾向于引入人类样式的推理轨迹作为训练偏差，限制了对非人类推理路径的探索，且传统定位首个错误步骤的策略不如基于最低置信度点的指导准确。

Method: 提出CGPO方法，通过置信度信号识别思考中的最大不确定点，使用自生成的非人类推理路径指导模型，防止推理路径漂移。

Result: 在代码和数学推理任务上，使用相同训练数据情况下，CGPO方法利用小模型生成的数据在大多数情况下优于使用强模型生成数据或人类标注数据的方法。

Conclusion: 利用模型置信度指导的非人类路径优化能够提升大语言模型的推理性能，扩展了推理路径探索空间。

Abstract: Current approaches for strengthening LLM reasoning tend to introduce a
training bias toward human-like reasoning trajectories. In step-wise preference
optimization, in particular, dependence on human or higher-capacity model
annotations for intermediate steps limits exploration of alternative,
non-human-like reasoning paths and thus constrains achievable performance.
Furthermore, through a small-scale pilot study, we observed that in
approximately 75% of cases, the model's first erroneous step occurs after the
lowest-confidence point. This suggests that guiding the model at its
lowest-confidence point before an error provides more accurate supervision than
locating the first explicit error. In this paper, we propose Confidence-Guided
Reasoning Path Preference Optimization (CGPO), a method that leverages a
confidence signal to identify points of maximal uncertainty in the model's
reasoning process and applies self-generated, non-human-like reasoning-path
guidance to mitigate trajectory drift. Our experiments span diverse models
applied to both code and mathematical reasoning tasks. The results show that,
with the same amount of training data, our method using data generated by a
small model can achieve better performance in most cases compared with
approaches using data generated by a strong model or human-annotated.

</details>


### [111] [TypePilot: Leveraging the Scala Type System for Secure LLM-generated Code](https://arxiv.org/abs/2510.11151)
*Alexander Sternfeld,Andrei Kucharavy,Ljiljana Dolamic*

Main category: cs.CL

TL;DR: 本文提出了TypePilot，一个利用强类型语言提升大型语言模型代码生成安全性的框架，显著减少了输入验证和注入漏洞。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型生成的代码存在潜在的安全漏洞，特别是在安全敏感或关键任务系统中，亟需提高其代码的安全性和稳健性。

Method: 提出TypePilot框架，基于强类型且可验证的语言（以Scala为例），结合形式验证工具Stainless，实现类型驱动的代码生成流程。

Result: 实验表明，直接代码生成及简单提示无法有效避免安全漏洞，而TypePilot框架显著降低了输入验证和注入类漏洞。

Conclusion: 结构化的、类型驱动的LLM工作流能够提升自动代码生成的可信度，适合高安全保障领域应用。

Abstract: Large language Models (LLMs) have shown remarkable proficiency in code
generation tasks across various programming languages. However, their outputs
often contain subtle but critical vulnerabilities, posing significant risks
when deployed in security-sensitive or mission-critical systems. This paper
introduces TypePilot, an agentic AI framework designed to enhance the security
and robustness of LLM-generated code by leveraging strongly typed and
verifiable languages, using Scala as a representative example. We evaluate the
effectiveness of our approach in two settings: formal verification with the
Stainless framework and general-purpose secure code generation. Our experiments
with leading open-source LLMs reveal that while direct code generation often
fails to enforce safety constraints, just as naive prompting for more secure
code, our type-focused agentic pipeline substantially mitigates input
validation and injection vulnerabilities. The results demonstrate the potential
of structured, type-guided LLM workflows to improve the SotA of the
trustworthiness of automated code generation in high-assurance domains.

</details>


### [112] [One Size Does Not Fit All: Exploring Variable Thresholds for Distance-Based Multi-Label Text Classification](https://arxiv.org/abs/2510.11160)
*Jens Van Nooten,Andriy Kosar,Guy De Pauw,Walter Daelemans*

Main category: cs.CL

TL;DR: 本文针对基于距离的无监督文本多标签分类方法进行研究，提出了一种基于验证集的标签专属阈值优化方法，大幅提升分类效果。


<details>
  <summary>Details</summary>
Motivation: 基于距离的无监督文本分类在多标签场景中受限于统一阈值难以准确判断文本与标签相似度，现有方法性能有限。

Method: 通过实验证明不同模型、数据集和标签集的相似度分布存在显著差异，进而提出基于验证集优化的标签专属阈值方法，用以提升多标签分类准确率。

Result: 该方法在多个多标签文本分类数据集上平均提升46%准确率，且超过前人均匀阈值方法14%，同时对标注样本有限时仍保持良好表现。

Conclusion: 标签专属阈值优化显著增强基于距离的无监督多标签文本分类的性能，提高了方法适应性和准确性，为相关任务提供有效解决方案。

Abstract: Distance-based unsupervised text classification is a method within text
classification that leverages the semantic similarity between a label and a
text to determine label relevance. This method provides numerous benefits,
including fast inference and adaptability to expanding label sets, as opposed
to zero-shot, few-shot, and fine-tuned neural networks that require re-training
in such cases. In multi-label distance-based classification and information
retrieval algorithms, thresholds are required to determine whether a text
instance is "similar" to a label or query. Similarity between a text and label
is determined in a dense embedding space, usually generated by state-of-the-art
sentence encoders. Multi-label classification complicates matters, as a text
instance can have multiple true labels, unlike in multi-class or binary
classification, where each instance is assigned only one label. We expand upon
previous literature on this underexplored topic by thoroughly examining and
evaluating the ability of sentence encoders to perform distance-based
classification. First, we perform an exploratory study to verify whether the
semantic relationships between texts and labels vary across models, datasets,
and label sets by conducting experiments on a diverse collection of realistic
multi-label text classification (MLTC) datasets. We find that similarity
distributions show statistically significant differences across models,
datasets and even label sets. We propose a novel method for optimizing
label-specific thresholds using a validation set. Our label-specific
thresholding method achieves an average improvement of 46% over normalized 0.5
thresholding and outperforms uniform thresholding approaches from previous work
by an average of 14%. Additionally, the method demonstrates strong performance
even with limited labeled examples.

</details>


### [113] [Bridging Gaps in Hate Speech Detection: Meta-Collections and Benchmarks for Low-Resource Iberian Languages](https://arxiv.org/abs/2510.11167)
*Paloma Piot,José Ramom Pichel Campos,Javier Parapar*

Main category: cs.CL

TL;DR: 该论文针对欧洲西班牙语等低资源伊比利亚语言的仇恨言论检测，构建了一个统一标签的多语言数据集，并通过零样本、少样本和微调方法评估大语言模型，提供了新的基准测试，强调多语言和方言差异的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有仇恨言论检测研究主要集中在英语，缺乏低资源语言尤其是存在多种语言变体的语言的资源和基准，同时大语言模型对数据需求大，低资源语言难以满足。

Method: 系统整合现有欧洲西班牙语仇恨言论数据集，统一标签和元数据，翻译扩展至欧洲葡萄牙语及两种不同加利西亚语标准，构建多语言对齐语料库，并在此基础上评估大语言模型的检测性能。

Result: 建立了伊比利亚语言仇恨言论检测新基准，提供了零样本、少样本和微调下状态最先进的大语言模型的基线结果，展示了多语言和语言变体识别的重要性。

Conclusion: 多语言和方言变体意识在低资源语言仇恨言论检测中至关重要。该研究为欧洲少数语言的仇恨言论检测提供了统一的资源和基准，有助于未来研究的发展。

Abstract: Hate speech poses a serious threat to social cohesion and individual
well-being, particularly on social media, where it spreads rapidly. While
research on hate speech detection has progressed, it remains largely focused on
English, resulting in limited resources and benchmarks for low-resource
languages. Moreover, many of these languages have multiple linguistic
varieties, a factor often overlooked in current approaches. At the same time,
large language models require substantial amounts of data to perform reliably,
a requirement that low-resource languages often cannot meet. In this work, we
address these gaps by compiling a meta-collection of hate speech datasets for
European Spanish, standardised with unified labels and metadata. This
collection is based on a systematic analysis and integration of existing
resources, aiming to bridge the data gap and support more consistent and
scalable hate speech detection. We extended this collection by translating it
into European Portuguese and into a Galician standard that is more convergent
with Spanish and another Galician variant that is more convergent with
Portuguese, creating aligned multilingual corpora. Using these resources, we
establish new benchmarks for hate speech detection in Iberian languages. We
evaluate state-of-the-art large language models in zero-shot, few-shot, and
fine-tuning settings, providing baseline results for future research. Moreover,
we perform a cross-lingual analysis with our target languages. Our findings
underscore the importance of multilingual and variety-aware approaches in hate
speech detection and offer a foundation for improved benchmarking in
underrepresented European languages.

</details>


### [114] [Evaluating Reasoning Faithfulness in Medical Vision-Language Models using Multimodal Perturbations](https://arxiv.org/abs/2510.11196)
*Johannes Moll,Markus Graf,Tristan Lemke,Nicolas Lenhart,Daniel Truhn,Jean-Benoit Delbrouck,Jiazhen Pan,Daniel Rueckert,Lisa C. Adams,Keno K. Bressem*

Main category: cs.CL

TL;DR: 本文研究了视觉语言模型在胸部X光视觉问答中的链式思维解释的真实性问题，提出了一个基于临床的评估框架，发现模型解释与最终答案的准确度不一定相关，强调需多维度评估模型可靠性。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型产生的链式思维解释虽可信但常与实际决策过程不符，在临床应用中影响信任，目前缺乏有效评估这一偏差的方法。

Method: 设计一个涵盖临床忠实度、因果归因和置信度校准三轴的评估框架，通过控制文本和图像的修改进行验证，并由放射科医生进行对照研究。

Result: 评估显示模型答案准确率与解释质量脱钩，文本线索对解释影响大于视觉线索，开放源代码模型在最终准确率上表现接近专有模型，但专有模型在因果归因和部分忠实度上表现优越。

Conclusion: 当前视觉语言模型在临床高风险场景中存在解释可信度不足的风险，需要超越答案准确率，采用多维评估框架确保模型决策的透明性和可靠性。

Abstract: Vision-language models (VLMs) often produce chain-of-thought (CoT)
explanations that sound plausible yet fail to reflect the underlying decision
process, undermining trust in high-stakes clinical use. Existing evaluations
rarely catch this misalignment, prioritizing answer accuracy or adherence to
formats. We present a clinically grounded framework for chest X-ray visual
question answering (VQA) that probes CoT faithfulness via controlled text and
image modifications across three axes: clinical fidelity, causal attribution,
and confidence calibration. In a reader study (n=4), evaluator-radiologist
correlations fall within the observed inter-radiologist range for all axes,
with strong alignment for attribution (Kendall's $\tau_b=0.670$), moderate
alignment for fidelity ($\tau_b=0.387$), and weak alignment for confidence tone
($\tau_b=0.091$), which we report with caution. Benchmarking six VLMs shows
that answer accuracy and explanation quality are decoupled, acknowledging
injected cues does not ensure grounding, and text cues shift explanations more
than visual cues. While some open-source models match final answer accuracy,
proprietary models score higher on attribution (25.0% vs. 1.4%) and often on
fidelity (36.1% vs. 31.7%), highlighting deployment risks and the need to
evaluate beyond final answer accuracy.

</details>


### [115] [Discursive Circuits: How Do Language Models Understand Discourse Relations?](https://arxiv.org/abs/2510.11210)
*Yisong Miao,Min-Yen Kan*

Main category: cs.CL

TL;DR: 本文提出了稀疏计算图“话语电路”来解释变换器语言模型中的话语理解过程，通过设计Completion under Discourse Relation任务和构建对比语料进行电路发现，验证了该电路在多种话语框架中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 话语关系涉及较长文本跨度和复杂推理，传统模型难以解析其内部机制，因此提出基于稀疏计算图的解释方法以理解话语处理过程。

Method: 设计Completion under Discourse Relation（CuDR）任务，构建针对激活修补的最小对比语料，利用激活修补方法发现稀疏的“话语电路”，对GPT-2模型进行分析，关注不同层次对话语特征的编码。

Result: 发现约占GPT-2模型0.2%的稀疏电路能够恢复话语理解功能，这些电路在PDTB任务和未见的话语框架（RST、SDRT）中表现出良好泛化能力。下层编码词汇语义和共指，上层编码话语层抽象特征，共指对类似扩展关系有支持作用。

Conclusion: 稀疏电路有效解释了变换器模型中的话语理解机制，具备跨话语框架的泛化能力，体现了不同层对语言及话语特征的分工，有助于深入理解语言模型内部结构和认知过程。

Abstract: Which components in transformer language models are responsible for discourse
understanding? We hypothesize that sparse computational graphs, termed as
discursive circuits, control how models process discourse relations. Unlike
simpler tasks, discourse relations involve longer spans and complex reasoning.
To make circuit discovery feasible, we introduce a task called Completion under
Discourse Relation (CuDR), where a model completes a discourse given a
specified relation. To support this task, we construct a corpus of minimal
contrastive pairs tailored for activation patching in circuit discovery.
Experiments show that sparse circuits ($\approx 0.2\%$ of a full GPT-2 model)
recover discourse understanding in the English PDTB-based CuDR task. These
circuits generalize well to unseen discourse frameworks such as RST and SDRT.
Further analysis shows lower layers capture linguistic features such as lexical
semantics and coreference, while upper layers encode discourse-level
abstractions. Feature utility is consistent across frameworks (e.g.,
coreference supports Expansion-like relations).

</details>


### [116] [Domain-Specific Data Generation Framework for RAG Adaptation](https://arxiv.org/abs/2510.11217)
*Chris Xing Tian,Weihao Xie,Zhen Chen,Zhengyuan Yi,Hui Liu,Haoliang Li,Shiqi Wang,Siwei Ma*

Main category: cs.CL

TL;DR: 提出了一种名为RAGen的框架，用于生成领域相关的问答上下文三元组，以提升RAG系统在特定领域中的适应性和性能。


<details>
  <summary>Details</summary>
Motivation: RAG系统在领域特定环境下的适应需要专门且丰富的上下文训练数据，而一般问答数据不足以支持这种需求。

Method: RAGen框架识别文档中的关键概念，依据布鲁姆分类法原则生成多样化的问题，并从相关上下文中提取精确答案，支持多种RAG适配策略及高效处理大规模动态文档。

Result: RAGen实现了语义分块、层级概念提取、多块检索和引入干扰上下文，增强了推理能力和系统的鲁棒性。

Conclusion: RAGen具备模块化设计和高扩展性，能有效支持快速变化领域（如科学研究和企业知识库）的RAG系统适配，提升生成内容的领域相关性和准确性。

Abstract: Retrieval-Augmented Generation (RAG) combines the language understanding and
reasoning power of large language models (LLMs) with external retrieval to
enable domain-grounded responses. Effectively adapting RAG systems to
domain-specific settings requires specialized, context-rich training data
beyond general-purpose question-answering. Here, we propose RAGen, a scalable
and modular framework for generating domain-grounded question-answer-context
(QAC) triples tailored to diverse RAG adaptation approaches. RAGen produces
these QAC triples by identifying key concepts in documents, generating diverse
questions guided by Bloom's Taxonomy-inspired principles, and pairing them with
precise answers extracted from relevant contexts. RAGen supports multiple RAG
adaptation strategies, including the optimization of key components such as the
LLM, retriever, and embedding model, etc. Its modular pipeline features
semantic chunking, hierarchical concept extraction, and multi-chunk retrieval,
along with the introduction of curated distractor contexts to promote robust
reasoning. Designed for scalability, RAGen efficiently handles large and
evolving document corpora without redundant processing, making it especially
suitable for dynamic evolving domains such as scientific research and
enterprise knowledge bases.

</details>


### [117] [The Curious Case of Factual (Mis)Alignment between LLMs' Short- and Long-Form Answers](https://arxiv.org/abs/2510.11218)
*Saad Obaid ul Islam,Anne Lauscher,Goran Glavaš*

Main category: cs.CL

TL;DR: 该论文提出了SLAQ框架，评估大语言模型在简单与复杂事实问答中的一致性，揭示模型在复杂任务中事实知识访问的不一致性问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在简单事实问答中表现良好，但在复杂查询中却常出现不一致，影响模型的可靠性和用户信任。

Method: 提出短问答与复杂综合问答的对比评估方法（SLAQ），分析16个模型在600个问答对中的表现，研究答案一致性及其机制，发现基于机制相似度的指标能较好预测一致性。

Result: 发现模型在短答与长答之间存在系统性不一致，且答案准确率受位置和连续回答影响，基于机制相似度的预测准确率达78%。

Conclusion: 事实答案在不同复杂度查询中的一致性是评估大语言模型可信度的重要维度，当前仅凭简单事实问答性能评估模型存在不足。

Abstract: Large language models (LLMs) can correctly answer "When was Einstein born?"
yet fail to provide the same date when writing about Einstein's life revealing
a fundamental inconsistency in how models access factual knowledge across task
complexities. While models display impressive accuracy on factual
question-answering benchmarks, the reliability gap between simple and complex
queries remains poorly understood, eroding their trustworthiness. In this work,
we introduce Short-Long Form Alignment for Factual Question Answering (SLAQ), a
controlled evaluation framework that compares LLMs' answers to the same factual
questions asked (a) in isolation (short) vs. (b) integrated into complex
queries (long). Looking at 16 LLMs across 600 queries, we find a systematic
misalignment of answers to the corresponding short and long queries. We further
uncover position-dependent accuracy loss and momentum effects where consecutive
correct or incorrect answers create self-reinforcing patterns. Through
mechanistic analysis, we find that aligned facts activate overlapping model
internals, and that metrics based on mechanistic similarity can predict
short-long answer alignment with up to 78% accuracy. Our work establishes
factual consistency over query complexity as an important aspect of LLMs'
trustworthiness and challenges current evaluation practices, which implicitly
assume that good performance for simple factual queries implies reliability in
more complex knowledge-seeking tasks too.

</details>


### [118] [WebRouter: Query-specific Router via Variational Information Bottleneck for Cost-sensitive Web Agent](https://arxiv.org/abs/2510.11221)
*Tao Li,Jinlong Hu,Yang Wang,Junfeng Liu,Xuejun Liu*

Main category: cs.CL

TL;DR: WebRouter通过一个成本意识的变分信息瓶颈目标函数，显著降低了基于大型语言模型的网页自动化代理的运行成本，同时保持较高的准确率。


<details>
  <summary>Details</summary>
Motivation: 基于大型语言模型的网页代理在处理复杂提示时面临成本与性能的权衡，且复杂提示导致模型性能下降。

Method: 提出了一种查询特定的路由器WebRouter，采用成本感知的变分信息瓶颈（ca-VIB）目标，学习输入提示的压缩表示，并显式惩罚运行成本。

Result: 在五个真实网站的WebVoyager基准测试中，WebRouter相比GPT-4o基线，运营成本降低了87.8%，准确率仅下降3.8%。

Conclusion: WebRouter有效缓解了复杂提示带来的成本-性能权衡问题，显著降低运行成本且保持较好性能，适用于LLM驱动的网页自动化代理。

Abstract: LLM-brained web agents offer powerful capabilities for web automation but
face a critical cost-performance trade-off. The challenge is amplified by web
agents' inherently complex prompts that include goals, action histories, and
environmental states, leading to degraded LLM ensemble performance. To address
this, we introduce WebRouter, a novel query-specific router trained from an
information-theoretic perspective. Our core contribution is a cost-aware
Variational Information Bottleneck (ca-VIB) objective, which learns a
compressed representation of the input prompt while explicitly penalizing the
expected operational cost. Experiments on five real-world websites from the
WebVoyager benchmark show that WebRouter reduces operational costs by a
striking 87.8\% compared to a GPT-4o baseline, while incurring only a 3.8\%
accuracy drop.

</details>


### [119] [Fairness Metric Design Exploration in Multi-Domain Moral Sentiment Classification using Transformer-Based Models](https://arxiv.org/abs/2510.11222)
*Battemuulen Naranbat,Seyed Sahand Mohammadi Ziabari,Yousuf Nasser Al Husaini,Ali Mohammed Mansoor Alsahag*

Main category: cs.CL

TL;DR: 本文研究了在道德情感分类中保证公平性的问题，特别是在跨领域迁移中，发现模型在不同数据集之间表现存在不对称性，并提出了新的公平性度量指标MFC。


<details>
  <summary>Details</summary>
Motivation: 当前变换器模型在自然语言处理中受到广泛应用，但在跨领域情境下，道德情感分类的公平性面临挑战，整体性能掩盖了标签级别的公平性问题。

Method: 使用Moral Foundations Twitter Corpus（MFTC）和Moral Foundations Reddit Corpus（MFRC）数据集，采用BERT和DistilBERT模型，在多标签分类任务中进行域内和跨域评估，提出Moral Fairness Consistency（MFC）指标衡量道德基础检测的跨域稳定性。

Result: 跨域迁移表现存在显著不对称，Twitter到Reddit的性能下降远大于Reddit到Twitter；标签级别的公平性问题明显，尤其是权威标签的公平性指标Demographic Parity Difference和Equalized Odds Difference较高。MFC指标与Demographic Parity Difference呈完全负相关，并且独立于标准性能指标。忠诚标签表现出最高一致性，权威标签最低。

Conclusion: MFC作为一种补充且诊断导向的公平性评价指标，有助于更加可靠地在不同语言环境下部署道德推理模型，提升模型的公平性和跨域稳定性。

Abstract: Ensuring fairness in natural language processing for moral sentiment
classification is challenging, particularly under cross-domain shifts where
transformer models are increasingly deployed. Using the Moral Foundations
Twitter Corpus (MFTC) and Moral Foundations Reddit Corpus (MFRC), this work
evaluates BERT and DistilBERT in a multi-label setting with in-domain and
cross-domain protocols. Aggregate performance can mask disparities: we observe
pronounced asymmetry in transfer, with Twitter->Reddit degrading micro-F1 by
14.9% versus only 1.5% for Reddit->Twitter. Per-label analysis reveals fairness
violations hidden by overall scores; notably, the authority label exhibits
Demographic Parity Differences of 0.22-0.23 and Equalized Odds Differences of
0.40-0.41. To address this gap, we introduce the Moral Fairness Consistency
(MFC) metric, which quantifies the cross-domain stability of moral foundation
detection. MFC shows strong empirical validity, achieving a perfect negative
correlation with Demographic Parity Difference (rho = -1.000, p < 0.001) while
remaining independent of standard performance metrics. Across labels, loyalty
demonstrates the highest consistency (MFC = 0.96) and authority the lowest (MFC
= 0.78). These findings establish MFC as a complementary, diagnosis-oriented
metric for fairness-aware evaluation of moral reasoning models, enabling more
reliable deployment across heterogeneous linguistic contexts. .

</details>


### [120] [A Theorem-Proving-Based Evaluation of Neural Semantic Parsing](https://arxiv.org/abs/2510.11225)
*Hayate Funakura,Hyunsoo Kim,Koji Mineshima*

Main category: cs.CL

TL;DR: 本文评估了神经语义解析器的图匹配评价指标（如Smatch）在逻辑等价性上的不足，结合自动定理证明提出更精确的评价方法。


<details>
  <summary>Details</summary>
Motivation: 现有图匹配度量主要捕获表面重叠，难以反映语义解析的逻辑等价性，影响推理任务的评价准确性。

Method: 对比监督微调和少样本上下文学习两种解析器构建方式，在规范化和非规范化目标下，结合图匹配、双向逻辑蕴含以及形式良好性三种评价指标进行系统评测。

Result: 实验结果显示图匹配性能良好的模型往往不能保证逻辑等价性，目标规范化有助于减少变异，提高形式良好性和逻辑适应性。错误分析发现复杂公式、句法结构及变量绑定是主要难点。

Conclusion: 图匹配指标在面向推理的应用中存在局限，需结合逻辑敏感的评价及训练目标，并推广简化规范化的目标表示以提升模型表现。

Abstract: Graph-matching metrics such as Smatch are the de facto standard for
evaluating neural semantic parsers, yet they capture surface overlap rather
than logical equivalence. We reassess evaluation by pairing graph-matching with
automated theorem proving. We compare two approaches to building parsers:
supervised fine-tuning (T5-Small/Base) and few-shot in-context learning
(GPT-4o/4.1/5), under normalized and unnormalized targets. We evaluate outputs
using graph-matching, bidirectional entailment between source and target
formulas with a first-order logic theorem prover, and well-formedness. Across
settings, we find that models performing well on graph-matching often fail to
produce logically equivalent formulas. Normalization reduces incidental target
variability, improves well-formedness, and strengthens logical adequacy. Error
analysis shows performance degrades with increasing formula complexity and with
coordination, prepositional phrases, and passive voice; the dominant failures
involve variable binding and indexing, and predicate naming. These findings
highlight limits of graph-based metrics for reasoning-oriented applications and
motivate logic-sensitive evaluation and training objectives together with
simplified, normalized target representations. All code and data for our
experiments are publicly available.

</details>


### [121] [CNSocialDepress: A Chinese Social Media Dataset for Depression Risk Detection and Structured Analysis](https://arxiv.org/abs/2510.11233)
*Jinyuan Xu,Tian Lan,Xintao Yu,Xue He,Hezhi Zhang,Ying Wang,Pierre Magistry,Mathieu Valette,Lei Li*

Main category: cs.CL

TL;DR: 该论文发布了一个名为CNSocialDepress的中文社交媒体抑郁风险检测数据集，包含4.4万多条文本和专业注释，实现了风险的二元判别及多维心理属性分析。


<details>
  <summary>Details</summary>
Motivation: 当前中文公开的抑郁风险检测资源稀缺且多为二元分类，无法满足细粒度和多维度心理分析需求。

Method: 构建包含233用户、44,178条文本的CNSocialDepress数据集，由心理学专家标注10,306条抑郁相关段落，提供二元风险标签和多维心理属性。进行多种NLP任务实验，包括结构化心理画像和大语言模型微调。

Result: 实验结果表明，该数据集在结构化心理分析及抑郁风险细粒度判别中表现优异，提高了相关NLP任务的效果。

Conclusion: CNSocialDepress数据集为中文抑郁风险识别和心理分析提供了有效工具，具备较高实用价值和推广潜力，有助于心理健康应用的发展。

Abstract: Depression is a pressing global public health issue, yet publicly available
Chinese-language resources for risk detection remain scarce and are mostly
limited to binary classification. To address this limitation, we release
CNSocialDepress, a benchmark dataset for depression risk detection from Chinese
social media posts. The dataset contains 44,178 texts from 233 users, within
which psychological experts annotated 10,306 depression-related segments.
CNSocialDepress provides binary risk labels together with structured
multi-dimensional psychological attributes, enabling interpretable and
fine-grained analysis of depressive signals. Experimental results demonstrate
its utility across a wide range of NLP tasks, including structured
psychological profiling and fine-tuning of large language models for depression
detection. Comprehensive evaluations highlight the dataset's effectiveness and
practical value for depression risk identification and psychological analysis,
thereby providing insights to mental health applications tailored for
Chinese-speaking populations.

</details>


### [122] [XQuant: Achieving Ultra-Low Bit KV Cache Quantization with Cross-Layer Compression](https://arxiv.org/abs/2510.11236)
*Haoqi Yang,Yao Yao,Zuchao Li,Baoyuan Qi,Guoming Liu,Hai Zhao*

Main category: cs.CL

TL;DR: 本文提出了一种无训练且即插即用的XQuant框架，实现了超低比特宽度的键值缓存（KV cache）量化，有效降低了大语言模型的内存消耗。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理长文本时，键值缓存增长导致内存需求剧增，对资源受限环境部署构成挑战。

Method: XQuant通过无需训练的数据无关校准方法及跨层KV缓存压缩，实现低至1.4比特的量化。

Result: 在TruthfulQA和LongBench任务上，XQuant相比现有最佳方法（如KIVI-2bit和AsymKV-1.5bit）实现更低比特宽度同时性能更优。

Conclusion: XQuant在保证模型准确度的前提下，有效平衡了内存效率与性能，适合资源受限环境中的大模型部署。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse natural language processing tasks. However, their extensive memory
requirements, particularly due to KV cache growth during long-text
understanding and generation, present significant challenges for deployment in
resource-constrained environments. Quantization has emerged as a promising
solution to reduce memory consumption while preserving historical information.
We propose XQuant, a training-free and plug-and-play framework that achieves
ultra-low equivalent bit-width KV cache quantization. XQuant introduces two key
innovations: a computationally negligible data-free calibration method and
cross-layer KV cache compression, enabling quantization to sub-1.4 bits.
Extensive experiments on TruthfulQA and LongBench demonstrate that XQuant
outperforms state-of-the-art methods (e.g., KIVI-2bit and AsymKV-1.5bit) by
achieving lower bit-width while maintaining superior performance, establishing
a better trade-off between memory efficiency and model accuracy.

</details>


### [123] [Attacks by Content: Automated Fact-checking is an AI Security Issue](https://arxiv.org/abs/2510.11238)
*Michael Schlichtkrull*

Main category: cs.CL

TL;DR: 本文探讨了AI代理在检索和推理外部文档时面临的内容操纵攻击，提出通过自动事实核查作为防御方法。


<details>
  <summary>Details</summary>
Motivation: 现有针对间接提示注入的防御方法无法防范通过提供偏见、误导或虚假信息的内容攻击，迫切需要新的防御策略。

Method: 提出将自动事实核查任务作为认知自我防御工具，帮助AI agent批判性评估信息，验证主张并评估信息源的可信度。

Result: 指出现有防御难以检测内容攻击，强调自动事实核查在提升AI代理信息鉴别能力中的重要作用。

Conclusion: 通过引入自动事实核查，AI代理能够更有效地识别和抵御基于内容的攻击，保护自身及用户利益。

Abstract: When AI agents retrieve and reason over external documents, adversaries can
manipulate the data they receive to subvert their behaviour. Previous research
has studied indirect prompt injection, where the attacker injects malicious
instructions. We argue that injection of instructions is not necessary to
manipulate agents - attackers could instead supply biased, misleading, or false
information. We term this an attack by content. Existing defenses, which focus
on detecting hidden commands, are ineffective against attacks by content. To
defend themselves and their users, agents must critically evaluate retrieved
information, corroborating claims with external evidence and evaluating source
trustworthiness. We argue that this is analogous to an existing NLP task,
automated fact-checking, which we propose to repurpose as a cognitive
self-defense tool for agents.

</details>


### [124] [Do Psychometric Tests Work for Large Language Models? Evaluation of Tests on Sexism, Racism, and Morality](https://arxiv.org/abs/2510.11254)
*Jana Jung,Marlene Lutz,Indira Sen,Markus Strohmaier*

Main category: cs.CL

TL;DR: 本文系统评估了针对性别歧视、种族歧视和道德的心理测量测试在人类大语言模型（LLMs）中的可靠性和有效性，发现这些测试在LLMs中可靠性中等，但有效性低，特别是与模型实际行为不一致。


<details>
  <summary>Details</summary>
Motivation: 探究人类设计的心理测量测试是否适用于评估大语言模型，解决目前测试结果意义不明的问题。

Method: 系统性评估多项测试的可靠性，采用收敛效度（理论相关性）和生态效度（测试得分与模型实际行为的关联）评估测试的有效性。

Result: 测试显示中等可靠性，但效度较低，测试得分与模型在实际任务中的行为不匹配，甚至有负相关情况。

Conclusion: 人类心理测量测试不能直接用于LLMs，需进行适当调整；对测评工具进行系统性验证对于结果解读至关重要。

Abstract: Psychometric tests are increasingly used to assess psychological constructs
in large language models (LLMs). However, it remains unclear whether these
tests -- originally developed for humans -- yield meaningful results when
applied to LLMs. In this study, we systematically evaluate the reliability and
validity of human psychometric tests for three constructs: sexism, racism, and
morality. We find moderate reliability across multiple item and prompt
variations. Validity is evaluated through both convergent (i.e., testing
theory-based inter-test correlations) and ecological approaches (i.e., testing
the alignment between tests scores and behavior in real-world downstream
tasks). Crucially, we find that psychometric test scores do not align, and in
some cases even negatively correlate with, model behavior in downstream tasks,
indicating low ecological validity. Our results highlight that systematic
evaluations of psychometric tests is essential before interpreting their
scores. They also suggest that psychometric tests designed for humans cannot be
applied directly to LLMs without adaptation.

</details>


### [125] [Towards Real-Time Fake News Detection under Evidence Scarcity](https://arxiv.org/abs/2510.11277)
*Guangyu Wei,Ke Han,Yueming Lyu,Yu Luo,Yue Jiang,Caifeng Shan,Nicu Sebe*

Main category: cs.CL

TL;DR: 该论文提出了EASE框架，通过三种独立视角动态评估证据充足性，实现实时假新闻检测，显著提升了在有限证据情况下的检测性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 实时场景中，假新闻检测因新事件缺乏充分证据而面临挑战，现有方法依赖外部证据导致泛化能力不足。

Method: 提出EASE框架，设计基于证据、推理和情感三维度的评估机制，根据证据充足性动态选择专家模块，并通过指令调优和伪标签引导解释性推理，融合评估结果辅助决策。

Result: 在多个基准测试中，EASE实现了最先进的性能，并显著提升了对实时新闻的泛化能力，验证了其在有限证据环境下的有效性。

Conclusion: EASE通过动态评估与多维专家选择解决了实时假新闻检测中的证据不足难题，提升了检测准确性与泛化能力，具有广泛应用前景。

Abstract: Fake news detection becomes particularly challenging in real-time scenarios,
where emerging events often lack sufficient supporting evidence. Existing
approaches often rely heavily on external evidence and therefore struggle to
generalize under evidence scarcity. To address this issue, we propose
Evaluation-Aware Selection of Experts (EASE), a novel framework for real-time
fake news detection that dynamically adapts its decision-making process
according to the assessed sufficiency of available evidence. EASE introduces a
sequential evaluation mechanism comprising three independent perspectives: (1)
Evidence-based evaluation, which assesses evidence and incorporates it into
decision-making only when the evidence is sufficiently supportive; (2)
Reasoning-based evaluation, which leverages the world knowledge of large
language models (LLMs) and applies them only when their reliability is
adequately established; and (3) Sentiment-based fallback, which integrates
sentiment cues when neither evidence nor reasoning is reliable. To enhance the
accuracy of evaluation processes, EASE employs instruction tuning with pseudo
labels to guide each evaluator in justifying its perspective-specific knowledge
through interpretable reasoning. Furthermore, the expert modules integrate the
evaluators' justified assessments with the news content to enable
evaluation-aware decision-making, thereby enhancing overall detection accuracy.
Moreover, we introduce RealTimeNews-25, a new benchmark comprising recent news
for evaluating model generalization on emerging news with limited evidence.
Extensive experiments demonstrate that EASE not only achieves state-of-the-art
performance across multiple benchmarks, but also significantly improves
generalization to real-time news. The code and dataset are available:
https://github.com/wgyhhhh/EASE.

</details>


### [126] [Emergent Misalignment via In-Context Learning: Narrow in-context examples can produce broadly misaligned LLMs](https://arxiv.org/abs/2510.11288)
*Nikita Afonin,Nikita Andriyanov,Nikhil Bageshpura,Kyle Liu,Kevin Zhu,Sunishchal Dev,Ashwinee Panda,Alexander Panchenko,Oleg Rogov,Elena Tutubalina,Mikhail Seleznyov*

Main category: cs.CL

TL;DR: 窄域微调会导致大型语言模型产生广泛的不对齐现象，本文研究了这种现象在上下文学习中的表现，发现这种不对齐也存在且显著。


<details>
  <summary>Details</summary>
Motivation: 先前研究主要关注微调和激活引导导致的不对齐现象，而没有探讨上下文学习（ICL）中是否存在同样的问题。

Method: 在三个数据集和三个前沿模型上，使用64到256个狭窄的上下文示例测试模型表现的不对齐率，并通过链式推理分析不对齐的原因。

Result: 发现模型在ICL环境下依然会产生广泛的不对齐，最大不对齐率高达58%，67.5%的错误输出与模型采用鲁莽或危险的“人格”有关。

Conclusion: 上下文学习同样会引发新兴不对齐现象，这提示需要在ICL阶段加强模型安全性和对齐机制。

Abstract: Recent work has shown that narrow finetuning can produce broadly misaligned
LLMs, a phenomenon termed emergent misalignment (EM). While concerning, these
findings were limited to finetuning and activation steering, leaving out
in-context learning (ICL). We therefore ask: does EM emerge in ICL? We find
that it does: across three datasets, three frontier models produce broadly
misaligned responses at rates between 2% and 17% given 64 narrow in-context
examples, and up to 58% with 256 examples. We also examine mechanisms of EM by
eliciting step-by-step reasoning (while leaving in-context examples unchanged).
Manual analysis of the resulting chain-of-thought shows that 67.5% of
misaligned traces explicitly rationalize harmful outputs by adopting a reckless
or dangerous ''persona'', echoing prior results on finetuning-induced EM.

</details>


### [127] [Are Large Language Models Effective Knowledge Graph Constructors?](https://arxiv.org/abs/2510.11297)
*Ruirui Chen,Weifeng Jiang,Chengwei Qin,Bo Xiong,Fiona Liausvia,Dongkyu Choi,Boon Kiat Quek*

Main category: cs.CL

TL;DR: 本文提出了一种分层信息提取框架，利用大型语言模型构建语义丰富且结构良好的知识图谱，评估当前方法的优缺点并发布相关数据集以推动医疗领域应用。


<details>
  <summary>Details</summary>
Motivation: 现有基于大型语言模型的知识图谱构建方法往往仅局限于实体和关系提取，覆盖范围有限且依赖预定义模式，难以构建高质量的知识图谱。

Method: 提出分层提取框架，结合多层次信息组织，使用先进的大型语言模型从文本中提取并构建知识图谱，支持结构和语义两个维度的综合评估。

Result: 展示当前大型语言模型在知识图谱构建中的优势和不足，揭示关键挑战，并发布了针对儿童心理健康领域研究论文的知识图谱数据集。

Conclusion: 分层信息提取框架提升了知识图谱的质量，有助于推动高风险领域（如医疗）的知识图谱应用，促进更加透明可靠的研究和实践。

Abstract: Knowledge graphs (KGs) are vital for knowledge-intensive tasks and have shown
promise in reducing hallucinations in large language models (LLMs). However,
constructing high-quality KGs remains difficult, requiring accurate information
extraction and structured representations that support interpretability and
downstream utility. Existing LLM-based approaches often focus narrowly on
entity and relation extraction, limiting coverage to sentence-level contexts or
relying on predefined schemas. We propose a hierarchical extraction framework
that organizes information at multiple levels, enabling the creation of
semantically rich and well-structured KGs. Using state-of-the-art LLMs, we
extract and construct knowledge graphs and evaluate them comprehensively from
both structural and semantic perspectives. Our results highlight the strengths
and shortcomings of current LLMs in KG construction and identify key challenges
for future work. To advance research in this area, we also release a curated
dataset of LLM-generated KGs derived from research papers on children's mental
well-being. This resource aims to foster more transparent, reliable, and
impactful applications in high-stakes domains such as healthcare.

</details>


### [128] [FOSSIL: Harnessing Feedback on Suboptimal Samples for Data-Efficient Generalisation with Imitation Learning for Embodied Vision-and-Language Tasks](https://arxiv.org/abs/2510.11307)
*Sabrina McCallum,Amit Parekh,Alessandro Suglia*

Main category: cs.CL

TL;DR: 本文提出了一种结合语言反馈的模仿学习方法，以利用最优和次优示范提升智能体的表现和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的具身AI方法受限于只能从最优示范学习，缺少评估示范质量的机制，容易复制错误行为；强化学习虽可探索但效率低下。

Method: 将语言反馈嵌入到基于Transformer的策略输入序列中，并辅以自监督学习目标以辅助反馈预测，增强策略的鲁棒性和泛化能力。

Result: 在自定义的BabyAI-XGen环境中的多项视觉与语言任务上，方法显著提升了智能体的组合泛化能力和鲁棒性，将次优行为转化为学习机会。

Conclusion: 语言反馈作为中间标量奖励的替代方案，提供了一种高效且直观的途径用于语言指导的具身任务学习。

Abstract: Current approaches to embodied AI tend to learn policies from expert
demonstrations. However, without a mechanism to evaluate the quality of
demonstrated actions, they are limited to learning from optimal behaviour, or
they risk replicating errors and inefficiencies. While reinforcement learning
offers one alternative, the associated exploration typically results in
sacrificing data efficiency. This work explores how agents trained with
imitation learning can learn robust representations from both optimal and
suboptimal demonstrations when given access to constructive language feedback
as a means to contextualise different modes of behaviour. We directly provide
language feedback embeddings as part of the input sequence into a
Transformer-based policy, and optionally complement the traditional next action
prediction objective with auxiliary self-supervised learning objectives for
feedback prediction. We test our approach on a range of embodied
Vision-and-Language tasks in our custom BabyAI-XGen environment and show
significant improvements in agents' compositional generalisation abilities and
robustness, suggesting that our data-efficient method allows models to
successfully convert suboptimal behaviour into learning opportunities. Overall,
our results suggest that language feedback is a competitive and intuitive
alternative to intermediate scalar rewards for language-specified embodied
tasks.

</details>


### [129] [Template-Based Text-to-Image Alignment for Language Accessibility: A Study on Visualizing Text Simplifications](https://arxiv.org/abs/2510.11314)
*Belkiss Souayed,Sarah Ebling,Yingqiang Gao*

Main category: cs.CL

TL;DR: 本文提出了一个结构化视觉语言模型提示框架，用于从简化文本生成可访问的图像，重点关注视觉简约以提升语言可达性。


<details>
  <summary>Details</summary>
Motivation: 智障人士理解复杂文本存在困难，而现有文本到图像模型多关注美学而非可访问性，缺乏对视觉插图与文本简化关系的研究。

Method: 设计五种不同空间布局且符合可访问性约束的提示模板，利用四个文本简化数据集的400条句子进行两阶段评估，包括CLIP评分和专家视觉风格注释。

Result: 基本对象聚焦模板在语义对齐度上表现最佳，视觉简约有助提升语言可访问性；专家评价认为复古风格最具可访问性，Wikipedia数据源最有效；文本简化评价一致性较高，图像质量评价较主观。

Conclusion: 该框架为可访问内容生成提供实用指导，强调结构化提示在AI视觉可访问工具中的重要作用。

Abstract: Individuals with intellectual disabilities often have difficulties in
comprehending complex texts. While many text-to-image models prioritize
aesthetics over accessibility, it is not clear how visual illustrations relate
to text simplifications (TS) generated from them. This paper presents a
structured vision-language model (VLM) prompting framework for generating
accessible images from simplified texts. We designed five prompt templates,
i.e., Basic Object Focus, Contextual Scene, Educational Layout, Multi-Level
Detail, and Grid Layout, each following distinct spatial arrangements while
adhering to accessibility constraints such as object count limits, spatial
separation, and content restrictions. Using 400 sentence-level simplifications
from four established TS datasets (OneStopEnglish, SimPA, Wikipedia, and
ASSET), we conducted a two-phase evaluation: Phase 1 assessed prompt template
effectiveness with CLIPScores, and Phase 2 involved human annotation of
generated images across ten visual styles by four accessibility experts.
Results show that the Basic Object Focus prompt template achieved the highest
semantic alignment, indicating that visual minimalism enhances language
accessibility. Expert evaluation further identified Retro style as the most
accessible and Wikipedia as the most effective data source. Inter-annotator
agreement varied across dimensions, with Text Simplicity showing strong
reliability and Image Quality proving more subjective. Overall, our framework
offers practical guidelines for accessible content generation and underscores
the importance of structured prompting in AI-generated visual accessibility
tools.

</details>


### [130] [Do LLMs "Feel"? Emotion Circuits Discovery and Control](https://arxiv.org/abs/2510.11328)
*Chenxi Wang,Yixuan Zhang,Ruiji Yu,Yufei Zheng,Lang Gao,Zirui Song,Zixiang Xu,Gus Xia,Huishuai Zhang,Dongyan Zhao,Xiuying Chen*

Main category: cs.CL

TL;DR: 本文研究大语言模型中情感表达的内部机制，识别并控制情感相关的神经元和注意力头，构建情感电路，实现高准确度的情感调控。


<details>
  <summary>Details</summary>
Motivation: 随着对大语言模型情感智能需求的增加，理解其内部如何表达情感并控制生成文本中的情绪变得尤为重要。

Method: 构建受控数据集SEV，提取上下文无关的情感向量，通过分析分解和因果分析识别情感相关的神经元和注意力头，量化子层对最终情感表达的因果影响，将局部组件整合为情感电路，并通过干预实验验证其因果作用。

Result: 发现了跨上下文一致编码情感的情感方向，成功识别并控制了情感电路，情感表达准确率达99.65%，优于提示和引导方法。

Conclusion: 首次系统性揭示并验证了大语言模型中的情感电路，推动了模型可解释性和情感智能的可控性研究。

Abstract: As the demand for emotional intelligence in large language models (LLMs)
grows, a key challenge lies in understanding the internal mechanisms that give
rise to emotional expression and in controlling emotions in generated text.
This study addresses three core questions: (1) Do LLMs contain context-agnostic
mechanisms shaping emotional expression? (2) What form do these mechanisms
take? (3) Can they be harnessed for universal emotion control? We first
construct a controlled dataset, SEV (Scenario-Event with Valence), to elicit
comparable internal states across emotions. Subsequently, we extract
context-agnostic emotion directions that reveal consistent, cross-context
encoding of emotion (Q1). We identify neurons and attention heads that locally
implement emotional computation through analytical decomposition and causal
analysis, and validate their causal roles via ablation and enhancement
interventions. Next, we quantify each sublayer's causal influence on the
model's final emotion representation and integrate the identified local
components into coherent global emotion circuits that drive emotional
expression (Q2). Directly modulating these circuits achieves 99.65%
emotion-expression accuracy on the test set, surpassing prompting- and
steering-based methods (Q3). To our knowledge, this is the first systematic
study to uncover and validate emotion circuits in LLMs, offering new insights
into interpretability and controllable emotional intelligence.

</details>


### [131] [LLM-Specific Utility: A New Perspective for Retrieval-Augmented Generation](https://arxiv.org/abs/2510.11358)
*Hengran Zhang,Keping Bi,Jiafeng Guo,Jiaming Zhang,Shuaiqiang Wang,Dawei Yin,Xueqi Cheng*

Main category: cs.CL

TL;DR: 本文提出了面向不同大语言模型（LLM）的检索增强生成（RAG）中“实用性”的概念，发现不同LLM对同一检索结果的实用性认知存在差异，强调了针对LLM定制实用性评估的重要性，并提出了相应的基准测试方法。


<details>
  <summary>Details</summary>
Motivation: 传统的检索增强生成主要关注相关性，但检索结果的实际帮助效果（实用性）更关键，且不同LLM由于知识和理解能力差异，对同一段落的实用性评价不同，因此需要引入LLM特定的实用性概念。

Method: 通过大规模多数据集和多LLM实验证明了人类注释的相关段落并非对所有LLM均最优，提出了衡量LLM特定实用性的基准测试流程，并评估了现有实用性判断方法的表现。

Result: 发现人类注释段落并非每个LLM的“地面真值”实用段落；不同LLM间实用性不可通用，难以有效拒绝无效段落和选取有用段落，且困惑度是衡量可读性的关键指标。

Conclusion: 研究表明需要针对每个LLM个性化设计和评估检索段落的实用性，现有方法存在局限性，未来RAG相关研究应考虑LLM特定的实用性评价标准。

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
incorporating external knowledge. While traditional retrieval focuses on
relevance, RAG's effectiveness depends on the utility of retrieved passages,
i.e., the usefulness in facilitating the generation of an accurate and
comprehensive answer. Existing studies often treat utility as a generic
attribute, ignoring the fact that different LLMs may benefit differently from
the same passage due to variations in internal knowledge and comprehension
ability. In this work, we introduce and systematically investigate the notion
of LLM-specific utility. Through large-scale experiments across multiple
datasets and LLMs, we demonstrate that human-annotated passages are not optimal
for LLMs and that ground-truth utilitarian passages are not transferable across
different LLMs. These findings highlight the necessity of adopting the
LLM-specific utility in RAG research. Our findings indicate that some
human-annotated passages are not ground-truth utilitarian passages for specific
LLMs, partially due to the varying readability of queries and passages for
LLMs, a tendency for which perplexity is a key metric. Based on these findings,
we propose a benchmarking procedure for LLM-specific utility judgments. We
evaluate existing utility judgment methods on six datasets and find that while
verbalized methods using pseudo-answers perform robustly, LLMs struggle to
assess utility effectively-failing to reject all passages for known queries and
to select truly useful ones for unknown queries.

</details>


### [132] [Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers](https://arxiv.org/abs/2510.11370)
*Wenhan Ma,Hailin Zhang,Liang Zhao,Yifan Song,Yudong Wang,Zhifang Sui,Fuli Luo*

Main category: cs.CL

TL;DR: 本文提出的R3方法通过记录推理中的路由分布并在训练中重放，解决了MoE模型中路由不一致引起的强化学习训练不稳定问题，显著提升训练稳定性与性能。


<details>
  <summary>Details</summary>
Motivation: MoE模型中路由机制在训练和推理阶段表现不一致，导致强化学习训练不稳定甚至崩溃。

Method: 提出Rollout Routing Replay (R3)方法，记录推理阶段的路由分布，并在训练阶段重放，降低训练与推理策略的KL散度。

Result: R3方法显著减少训练-推理间路由差异，避免训练崩溃，提升了训练稳定性及性能，优于GSPO和TIS等方法。

Conclusion: R3为解决MoE模型中强化学习训练不稳定问题提供了一种新思路，有助于稳定MoE模型的强化学习训练。

Abstract: Reinforcement learning (RL) has emerged as a crucial approach for enhancing
the capabilities of large language models. However, in Mixture-of-Experts (MoE)
models, the routing mechanism often introduces instability, even leading to
catastrophic RL training collapse. We analyze the training-inference
consistency of MoE models and identify a notable discrepancy in routing
behaviors between the two phases. Moreover, even under identical conditions,
the routing framework can yield divergent expert selections across repeated
forward passes. To address this foundational inconsistency, we propose Rollout
Routing Replay (R3), a method that records routing distributions from the
inference engine and replays them during training. R3 significantly reduces
training-inference policy KL divergence and mitigates extreme discrepancies
without compromising training speed. Extensive experiments on various settings
confirm that R3 succeeds in stabilizing RL training, preventing collapse and
outperforming methods such as GSPO and TIS. We believe this work can offer a
new solution for stabilizing RL in MoE models.

</details>


### [133] [Early Detection and Reduction of Memorisation for Domain Adaptation and Instruction Tuning](https://arxiv.org/abs/2510.11372)
*Dean L. Slack,Noura Al Moubayed*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在微调过程中记忆训练数据的问题，并提出了基于n-gram的早停和正则化方法有效降低记忆泄露。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在微调阶段容易记忆训练数据，可能导致隐私或版权文本泄露，但相关研究不足。

Method: 对Pythia、Llama3和Mistral模型进行微调，跟踪模型记忆行为，设计了n-gram记忆得分用于早停，同时提出n-gram感知的损失正则化方法。

Result: n-gram记忆得分能够提前预警记忆行为，基于此早停显著减小记忆泄露，新的正则化方法使记忆减少最多40%，且性能损失较小。

Conclusion: 本文揭示了微调过程中记忆动态，提出的基于n-gram的方法为防止记忆泄露提供了实用且可扩展的解决策略。

Abstract: Although large language models excel across many tasks, they can memorise
training data and thereby expose private or copyrighted text. Most defences
target the pre-training stage, leaving memorisation during fine-tuning,
especially for domain adaptation and instruction tuning, poorly understood. We
fine-tune Pythia, Llama3, and Mistral models spanning 1.4B-70B parameters on
common evaluation datasets and track verbatim memorisation throughout training.
We find that memorisation increases dramatically in the first few epochs, often
significantly before either validation perplexity or evaluation performance is
optimised. We use a simple but effective n-gram memorisation score which
reliably precedes verbatim memorisation; using it as an early-stopping
criterion mitigates memorisation with minimal performance loss. Further, we
introduce an n-gram-aware loss regulariser and show that it reduces
memorisation across all model families tested by up to 40% while minimising
evaluation performance trade-offs when compared to an existing memorisation
mitigation strategy. These results yield practical, scalable insights into
memorisation dynamics during language model fine-tuning.

</details>


### [134] [Beyond Survival: Evaluating LLMs in Social Deduction Games with Human-Aligned Strategies](https://arxiv.org/abs/2510.11389)
*Zirui Song,Yuan Huang,Junchang Liu,Haozhe Luo,Chenxi Wang,Lang Gao,Zixiang Xu,Mingfei Han,Xiaojun Chang,Xiuying Chen*

Main category: cs.CL

TL;DR: 本文提出了一个包含丰富人类验证多模态数据的狼人杀游戏数据集和一个策略对齐评估框架，用于更细致地评估大语言模型在语言能力和社交策略推理上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究多依赖语言模型自我对战，忽略了社交游戏的复杂性，且评估指标粗糙，不足以衡量模型多维度的社交推理与策略能力。

Method: 构建了包含超过100小时视频的真实多模态狼人杀数据集，提出一个两阶段策略对齐评估框架，包括多项选择题形式的语言表达评估和对投票选择及对手角色推断的决策评估。

Result: 实验表明，目前顶尖大语言模型在评估指标上表现多样，约一半评分低于0.50，显示在欺骗及反事实推理方面存在明显不足。

Conclusion: 该方法为多智能体交互中的语言、推理及策略研究提供了新的数据资源和评测框架，促进了相关领域的进步。

Abstract: Social deduction games like Werewolf combine language, reasoning, and
strategy, providing a testbed for studying natural language and social
intelligence. However, most studies reduce the game to LLM-based self-play,
yielding templated utterances and anecdotal cases that overlook the richness of
social gameplay. Evaluation further relies on coarse metrics such as survival
time or subjective scoring due to the lack of quality reference data. To
address these gaps, we curate a high-quality, human-verified multimodal
Werewolf dataset containing over 100 hours of video, 32.4M utterance tokens,
and 15 rule variants. Based on this dataset, we propose a novel
strategy-alignment evaluation that leverages the winning faction's strategies
as ground truth in two stages: 1) Speech evaluation, formulated as
multiple-choice-style tasks that assess whether the model can adopt appropriate
stances across five dimensions of social ability; and 2) Decision evaluation,
which assesses the model's voting choices and opponent-role inferences. This
framework enables a fine-grained evaluation of models' linguistic and reasoning
capabilities, while capturing their ability to generate strategically coherent
gameplay. Our experiments show that state-of-the-art LLMs show diverse
performance, with roughly half remain below 0.50, revealing clear gaps in
deception and counterfactual reasoning. We hope our dataset further inspires
research on language, reasoning, and strategy in multi-agent interaction.

</details>


### [135] [KnowRL: Teaching Language Models to Know What They Know](https://arxiv.org/abs/2510.11407)
*Sahil Kale,Devendra Singh Dhami*

Main category: cs.CL

TL;DR: 本文提出了KnowRL框架，通过内省与共识奖励机制提升大型语言模型的自知之明，从而实现更可靠和负责任的AI行为。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在判断自身能力时存在较高误判率，导致不可靠回答，亟需提升模型自知能力以确保其安全和可靠。

Method: KnowRL结合内省（模型自主生成并分类任务的可行性）和基于共识的奖励（提升自知识评估的一致性），利用内部生成数据强化模型自知能力，无需外部监督。

Result: 在LLaMA-3.1-8B和Qwen-2.5-7B上的实验证明，KnowRL显著提升了模型自知准确率和F1值，最高提升分别达28%和12%，且仅需少量种子数据和少量迭代。

Conclusion: KnowRL框架有效激发了大型语言模型自我改进自知能力的潜力，为实现更可靠、负责任和安全的AI应用提供了有力支持，且简单易行，适合未来模型推广应用。

Abstract: Truly reliable AI requires more than simply scaling up knowledge; it demands
the ability to know what it knows and when it does not. Yet recent research
shows that even the best LLMs misjudge their own competence in more than one in
five cases, making any response born of such internal uncertainty impossible to
fully trust. Inspired by self-improvement reinforcement learning techniques
that require minimal data, we present a simple but powerful framework KnowRL
that strengthens a model's internal understanding of its own feasibility
boundaries, enabling safer and more responsible behaviour. Our framework
combines two components: (i) introspection, where the model generates and
classifies tasks it judges feasible or infeasible, and (ii) consensus-based
rewarding, where stability of self-knowledge assessment is reinforced through
internal agreement. By using internally generated data, this design strengthens
consistency in self-knowledge and entirely avoids costly external supervision.
In experiments on LLaMA-3.1-8B and Qwen-2.5-7B, KnowRL steadily improved
self-knowledge, validated by both intrinsic self-consistency and extrinsic
benchmarking. With nothing more than a small seed set and no external
supervision, our method drove gains as high as 28% in accuracy and 12% in F1,
outperforming baselines in just a few iterations. Our framework essentially
unlocks the untapped capacity of LLMs to self-improve their knowledge
awareness, opening the door to reliable, more accountable AI and safer
deployment in critical applications. Owing to its simplicity and independence
from external effort, we encourage applying this reliability-enhancing process
to all future models.

</details>


### [136] [Valid Survey Simulations with Limited Human Data: The Roles of Prompting, Fine-Tuning, and Rectification](https://arxiv.org/abs/2510.11408)
*Stefan Krsteski,Giuseppe Russo,Serina Chang,Robert West,Kristina Gligorić*

Main category: cs.CL

TL;DR: 本文研究了利用大语言模型（LLMs）生成调查问卷答案，并结合偏差校正方法以提高估计的准确性，发现校正方法显著降低偏差并提升样本效能，且合理分配人类响应样本更有效。


<details>
  <summary>Details</summary>
Motivation: 传统调查方法成本高且速度慢，LLMs作为替代方案虽成本低但结果偏差大，亟需探讨如何结合合成和校正方法以获得准确的调查估计。

Method: 利用两个包含营养、政治和经济问题的面板调查，比较仅用LLMs合成答案与结合偏差校正方法的效果，同时研究有限的人类响应样本如何在合成和校正间分配。

Result: 仅用合成方法偏差高达24-86%，结合校正后偏差降至5%以下，有效样本量提升最多14%。合理分配人类资源以支持校正优于全部用于微调。

Conclusion: 在固定预算下，将大部分人类响应用于偏差校正而非全部用于微调，可显著提升调查估计的准确性和效能，挑战了传统的全部微调做法。

Abstract: Surveys provide valuable insights into public opinion and behavior, but their
execution is costly and slow. Large language models (LLMs) have been proposed
as a scalable, low-cost substitute for human respondents, but their outputs are
often biased and yield invalid estimates. We study the interplay between
synthesis methods that use LLMs to generate survey responses and rectification
methods that debias population estimates, and explore how human responses are
best allocated between them. Using two panel surveys with questions on
nutrition, politics, and economics, we find that synthesis alone introduces
substantial bias (24-86%), whereas combining it with rectification reduces bias
below 5% and increases effective sample size by up to 14%. Overall, we
challenge the common practice of using all human responses for fine-tuning,
showing that under a fixed budget, allocating most to rectification results in
far more effective estimation.

</details>


### [137] [Who are you, ChatGPT? Personality and Demographic Style in LLM-Generated Content](https://arxiv.org/abs/2510.11434)
*Dana Sotto Porat,Ella Rabinovich*

Main category: cs.CL

TL;DR: 本文提出了一种基于自动化分类器，利用Reddit问答内容评估大型语言模型（LLMs）人格特征的新方法，发现模型表现出较高的宜人性和较低的神经质，性别语言模式与人类相似但变异较小。


<details>
  <summary>Details</summary>
Motivation: 目前研究关注LLMs是否表现出类似人格和人口统计学特征，现有方法依赖自我报告问卷，存在局限。

Method: 提出数据驱动方法，使用自动人格和性别分类器分析LLMs对Reddit开放式问题的回复，并与人类文本对比。

Result: LLMs系统性表现出更高的宜人性和更低的神经质，性别语言模式类似人类但变化较小。

Conclusion: 通过新数据集和大规模分析，揭示生成式AI在人格和人口统计模式上的表现，为理解LLMs的个性化特征提供新视角。

Abstract: Generative large language models (LLMs) have become central to everyday life,
producing human-like text across diverse domains. A growing body of research
investigates whether these models also exhibit personality- and
demographic-like characteristics in their language. In this work, we introduce
a novel, data-driven methodology for assessing LLM personality without relying
on self-report questionnaires, applying instead automatic personality and
gender classifiers to model replies on open-ended questions collected from
Reddit. Comparing six widely used models to human-authored responses, we find
that LLMs systematically express higher Agreeableness and lower Neuroticism,
reflecting cooperative and stable conversational tendencies. Gendered language
patterns in model text broadly resemble those of human writers, though with
reduced variation, echoing prior findings on automated agents. We contribute a
new dataset of human and model responses, along with large-scale comparative
analyses, shedding new light on the topic of personality and demographic
patterns of generative AI.

</details>


### [138] [GenCNER: A Generative Framework for Continual Named Entity Recognition](https://arxiv.org/abs/2510.11444)
*Yawen Yang,Fukun Ma,Shiao Meng,Aiwei Liu,Lijie Wen*

Main category: cs.CL

TL;DR: 提出一种基于生成式框架的持续命名实体识别方法GenCNER，有效缓解灾难性遗忘和语义偏移问题，在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统命名实体识别在实体类别不断增加的场景下面临灾难性遗忘和非实体类型语义偏移的问题，现有持续学习方法难以有效解决。

Method: 将持续命名实体识别任务转换为实体三元组序列的生成问题，利用预训练的序列到序列模型，结合基于类型的置信度伪标签策略和知识蒸馏方法，保留已学知识并减少标签噪声。

Result: 在两个基准数据集上的实验表明，所提框架在多种持续命名实体识别场景下优于之前最先进的方法，且性能差距最小。

Conclusion: GenCNER为持续命名实体识别提供了一种简单有效的生成式解决方案，显著提升了模型的持续学习能力，缓解灾难性遗忘和语义漂移问题。

Abstract: Traditional named entity recognition (NER) aims to identify text mentions
into pre-defined entity types. Continual Named Entity Recognition (CNER) is
introduced since entity categories are continuously increasing in various
real-world scenarios. However, existing continual learning (CL) methods for NER
face challenges of catastrophic forgetting and semantic shift of non-entity
type. In this paper, we propose GenCNER, a simple but effective Generative
framework for CNER to mitigate the above drawbacks. Specifically, we skillfully
convert the CNER task into sustained entity triplet sequence generation problem
and utilize a powerful pre-trained seq2seq model to solve it. Additionally, we
design a type-specific confidence-based pseudo labeling strategy along with
knowledge distillation (KD) to preserve learned knowledge and alleviate the
impact of label noise at the triplet level. Experimental results on two
benchmark datasets show that our framework outperforms previous
state-of-the-art methods in multiple CNER settings, and achieves the smallest
gap compared with non-CL results.

</details>


### [139] [Investigating Large Language Models' Linguistic Abilities for Text Preprocessing](https://arxiv.org/abs/2510.11482)
*Marco Braga,Gian Carlo Milanese,Gabriella Pasi*

Main category: cs.CL

TL;DR: 本文探讨利用大型语言模型（LLMs）进行文本预处理任务，比较其与传统方法在多语言文本分类中的表现，结果显示LLMs在预处理准确率和分类性能上均优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统文本预处理方法通常忽略文本的上下文信息，而LLMs具备上下文理解能力，可以无需大量语言特定标注资源进行预处理。

Method: 使用LLMs执行停用词去除、词形还原和词干提取三大预处理任务，并在六种欧洲语言的数据集上与传统算法进行比较。

Result: LLMs在停用词去除、词形还原和词干提取的准确率分别达到97%、82%和74%；使用LLMs预处理的数据训练的机器学习模型在F1指标上提升最多6%。

Conclusion: LLMs能够有效执行文本预处理任务，且通过上下文理解提高了后续文本分类的性能，展示了其在多语言文本处理中的潜力。

Abstract: Text preprocessing is a fundamental component of Natural Language Processing,
involving techniques such as stopword removal, stemming, and lemmatization to
prepare text as input for further processing and analysis. Despite the
context-dependent nature of the above techniques, traditional methods usually
ignore contextual information. In this paper, we investigate the idea of using
Large Language Models (LLMs) to perform various preprocessing tasks, due to
their ability to take context into account without requiring extensive
language-specific annotated resources. Through a comprehensive evaluation on
web-sourced data, we compare LLM-based preprocessing (specifically stopword
removal, lemmatization and stemming) to traditional algorithms across multiple
text classification tasks in six European languages. Our analysis indicates
that LLMs are capable of replicating traditional stopword removal,
lemmatization, and stemming methods with accuracies reaching 97%, 82%, and 74%,
respectively. Additionally, we show that ML algorithms trained on texts
preprocessed by LLMs achieve an improvement of up to 6% with respect to the
$F_1$ measure compared to traditional techniques. Our code, prompts, and
results are publicly available at
https://github.com/GianCarloMilanese/llm_pipeline_wi-iat.

</details>


### [140] [Hallucination Detection via Internal States and Structured Reasoning Consistency in Large Language Models](https://arxiv.org/abs/2510.11529)
*Yusheng Song,Lirong Qiu,Xi Zhang,Zhihao Tang*

Main category: cs.CL

TL;DR: 本文提出了一种统一框架来检测大型语言模型中的复杂幻觉，结合了内部状态探测和思维链验证两种方法，克服了信号稀缺和表示不匹配两大难题。


<details>
  <summary>Details</summary>
Motivation: 当前幻觉检测方法存在 "检测困境"，即内部状态探测善于发现事实错误但难以识别逻辑谬误，思维链验证则相反，导致针对不同任务存在盲点。

Method: 设计了多路径推理机制以获得更细粒度的可比信号，并引入分段感知时序交叉注意模块，自适应融合对齐表示，准确定位模型推理中的细微不一致。

Result: 在三个不同基准测试和两个主流大型语言模型上进行广泛实验，结果显示所提框架显著优于强基线方法。

Conclusion: 该统一检测框架有效桥接了内部状态探测与思维链验证的差距，提高了大型语言模型复杂幻觉的检测能力。

Abstract: The detection of sophisticated hallucinations in Large Language Models (LLMs)
is hampered by a ``Detection Dilemma'': methods probing internal states
(Internal State Probing) excel at identifying factual inconsistencies but fail
on logical fallacies, while those verifying externalized reasoning
(Chain-of-Thought Verification) show the opposite behavior. This schism creates
a task-dependent blind spot: Chain-of-Thought Verification fails on
fact-intensive tasks like open-domain QA where reasoning is ungrounded, while
Internal State Probing is ineffective on logic-intensive tasks like
mathematical reasoning where models are confidently wrong. We resolve this with
a unified framework that bridges this critical gap. However, unification is
hindered by two fundamental challenges: the Signal Scarcity Barrier, as coarse
symbolic reasoning chains lack signals directly comparable to fine-grained
internal states, and the Representational Alignment Barrier, a deep-seated
mismatch between their underlying semantic spaces. To overcome these, we
introduce a multi-path reasoning mechanism to obtain more comparable,
fine-grained signals, and a segment-aware temporalized cross-attention module
to adaptively fuse these now-aligned representations, pinpointing subtle
dissonances. Extensive experiments on three diverse benchmarks and two leading
LLMs demonstrate that our framework consistently and significantly outperforms
strong baselines. Our code is available: https://github.com/peach918/HalluDet.

</details>


### [141] [An Encoder-Integrated PhoBERT with Graph Attention for Vietnamese Token-Level Classification](https://arxiv.org/abs/2510.11537)
*Ba-Quang Nguyen*

Main category: cs.CL

TL;DR: 该论文提出了一种新颖的文本图融合图注意力网络（TextGraphFuseGAT），结合了预训练的PhoBERT和图注意力网络，用于序列标注任务，尤其是在越南语多个领域的基准数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统的序列标注模型主要依赖于序列上下文，难以捕捉token间更丰富的依赖关系。为提升模型对细粒度语义和结构信息的捕捉能力，提出将图注意力网络与预训练语言模型结合。

Method: 模型首先利用PhoBERT生成token嵌入，构建全连接图以建立token间依赖关系，然后通过图注意力网络进行特征融合，接着叠加Transformer风格的自注意力层增强上下文表达，最后经过分类头实现序列标注。

Result: 在三个越南语基准数据集（PhoNER-COVID19、PhoDisfluency和新建的VietMed-NER）上，方法显著优于仅用Transformer及混合模型（如BiLSTM+CNN+CRF）。尤其在首个越南医学口语实体识别数据集表现了强大效果。

Conclusion: 结合预训练语言模型的语义特征与图结构的关系建模，能有效提升token级分类效果，适用于多个领域，证明了TextGraphFuseGAT的优越性和广泛适用性。

Abstract: We propose a novel neural architecture named TextGraphFuseGAT, which
integrates a pretrained transformer encoder (PhoBERT) with Graph Attention
Networks for token-level classification tasks. The proposed model constructs a
fully connected graph over the token embeddings produced by PhoBERT, enabling
the GAT layer to capture rich inter-token dependencies beyond those modeled by
sequential context alone. To further enhance contextualization, a
Transformer-style self-attention layer is applied on top of the graph-enhanced
embeddings. The final token representations are passed through a classification
head to perform sequence labeling. We evaluate our approach on three Vietnamese
benchmark datasets: PhoNER-COVID19 for named entity recognition in the COVID-19
domain, PhoDisfluency for speech disfluency detection, and VietMed-NER for
medical-domain NER. VietMed-NER is the first Vietnamese medical spoken NER
dataset, featuring 18 entity types collected from real-world medical speech
transcripts and annotated with the BIO tagging scheme. Its specialized
vocabulary and domain-specific expressions make it a challenging benchmark for
token-level classification models. Experimental results show that our method
consistently outperforms strong baselines, including transformer-only and
hybrid neural models such as BiLSTM + CNN + CRF, confirming the effectiveness
of combining pretrained semantic features with graph-based relational modeling
for improved token classification across multiple domains.

</details>


### [142] [Information-Preserving Reformulation of Reasoning Traces for Antidistillation](https://arxiv.org/abs/2510.11545)
*Jiayu Ding,Lei Cui,Li Dong,Nanning Zheng,Furu Wei*

Main category: cs.CL

TL;DR: 本文提出PART方法，对推理痕迹进行抗蒸馏重构，保护大型语言模型的知识产权，同时保持信息完整。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型生成的推理链条能提升复杂任务的表现，但详细推理过程易被未授权蒸馏，模型提供方为防止盗用，常用摘要替代详细过程，导致用户失去有价值的中间信息。

Method: 设计了PART方法，通过去除自言自语行为和重新排序子结论两步进行重构，配合小型辅助模型完成，计算开销低。

Result: 在多个推理基准测试中，PART有效破坏了学生模型的蒸馏效果，例如在AIME 2024测试中，训练过PART重构推理痕迹的32B学生模型性能降低13.5%。

Conclusion: PART方法成功平衡了信息保护与用户需求，防止未授权蒸馏，同时保留推理链条的中间信息，提高了模型的安全性。

Abstract: Recent advances in Large Language Models (LLMs) show that extending the
length of reasoning chains significantly improves performance on complex tasks.
While revealing these reasoning traces helps users better follow, verify, and
learn from the model's problem-solving process, it also makes them highly
vulnerable to unauthorized distillation. To mitigate this risk, proprietary
model providers often adopt aggressive protection strategies, such as replacing
detailed reasoning with brief summaries, which deprive users of valuable
intermediate information. To address this trade-off, we propose PART, an
information-preserving antidistillation reformulation of reasoning traces.
Motivated by the difference between how humans understand reasoning traces and
how LLMs exploit them for supervised fine-tuning, we design a simple but
effective two-step reformulation: removing self-talk behaviors and reordering
sub-conclusions. A small auxiliary model is trained to perform this
reformulation, incurring minimal computational overhead. Extensive experiments
demonstrate that PART consistently disrupts distillation across student models
of different sizes and types on various reasoning benchmarks. For instance,
when training on reformulated traces, even the performance of a large 32B
student model decreases from 54.17 to 46.88 on AIME 2024, corresponding to a
13.5% degradation.

</details>


### [143] [Invisible Languages of the LLM Universe](https://arxiv.org/abs/2510.11557)
*Saurabh Khanna,Xinxu Li*

Main category: cs.CL

TL;DR: 本文揭示了大型语言模型中语言数字不平等的结构性问题，强调约27%的语言虽具人口优势却在数字生态中几乎消失。


<details>
  <summary>Details</summary>
Motivation: 当前多语言模型掩盖了许多语言在数字领域的缺失，反映出殖民遗产导致的语言不平等和知识不公。

Method: 结合语言活力和数字存在度的实证数据，运用后殖民理论分析语言在AI系统中的结构性排斥，并划分四类语言群体。

Result: 发现约27%的活跃语言在数字领域几乎无处可见，英语的主导地位源自权力结构而非技术必然。

Conclusion: 呼吁去殖民化语言技术，推动语言多样性和AI利益的民主化。

Abstract: Large Language Models are trained on massive multilingual corpora, yet this
abundance masks a profound crisis: of the world's 7,613 living languages,
approximately 2,000 languages with millions of speakers remain effectively
invisible in digital ecosystems. We propose a critical framework connecting
empirical measurements of language vitality (real world demographic strength)
and digitality (online presence) with postcolonial theory and epistemic
injustice to explain why linguistic inequality in AI systems is not incidental
but structural. Analyzing data across all documented human languages, we
identify four categories: Strongholds (33%, high vitality and digitality),
Digital Echoes (6%, high digitality despite declining vitality), Fading Voices
(36%, low on both dimensions), and critically, Invisible Giants (27%, high
vitality but near-zero digitality) - languages spoken by millions yet absent
from the LLM universe. We demonstrate that these patterns reflect continuities
from colonial-era linguistic hierarchies to contemporary AI development,
constituting what we term digital epistemic injustice. Our analysis reveals
that English dominance in AI is not a technical necessity but an artifact of
power structures that systematically exclude marginalized linguistic knowledge.
We conclude with implications for decolonizing language technology and
democratizing access to AI benefits.

</details>


### [144] [Culturally-Aware Conversations: A Framework & Benchmark for LLMs](https://arxiv.org/abs/2510.11563)
*Shreya Havaldar,Sunny Rai,Young-Min Cho,Lyle Ungar*

Main category: cs.CL

TL;DR: 本文提出了首个针对多文化对话场景设计的LLM文化适应性评测框架及基准测试，揭示当前顶级大语言模型在实际跨文化对话中存在适应困难。


<details>
  <summary>Details</summary>
Motivation: 现有评估LLM文化适应性的基准测试未能真实反映模型在多元文化用户交互中的挑战。

Method: 基于社会文化理论构建框架，强调语言风格受情境、关系及文化背景塑造；创建多文化标注数据集；提出跨文化评估新准则（对话框架、风格敏感性、主观正确性）。

Result: 在该基准测试下，当前顶尖LLM在跨文化多轮对话中表现不足，显示出文化适应性问题。

Conclusion: 提出的框架和基准为更真实评估和提升LLM的文化适应能力提供了重要工具和方向。

Abstract: Existing benchmarks that measure cultural adaptation in LLMs are misaligned
with the actual challenges these models face when interacting with users from
diverse cultural backgrounds. In this work, we introduce the first framework
and benchmark designed to evaluate LLMs in realistic, multicultural
conversational settings. Grounded in sociocultural theory, our framework
formalizes how linguistic style - a key element of cultural communication - is
shaped by situational, relational, and cultural context. We construct a
benchmark dataset based on this framework, annotated by culturally diverse
raters, and propose a new set of desiderata for cross-cultural evaluation in
NLP: conversational framing, stylistic sensitivity, and subjective correctness.
We evaluate today's top LLMs on our benchmark and show that these models
struggle with cultural adaptation in a conversational setting.

</details>


### [145] [LLMAtKGE: Large Language Models as Explainable Attackers against Knowledge Graph Embeddings](https://arxiv.org/abs/2510.11584)
*Ting Li,Yang Yang,Yipeng Yu,Liang Yao,Guoqing Chao,Ruifeng Xu*

Main category: cs.CL

TL;DR: 本文提出了基于大语言模型（LLM）的知识图谱嵌入对抗攻击框架LLMAtKGE，旨在通过选择攻击目标并生成可读解释，提升攻击效果和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有基于黑盒的方法虽然结合了文本与结构信息，但缺乏可读解释且泛化能力差。鉴于大型语言模型在理解与推理方面的优势，本文希望利用LLM改进攻击方法。

Method: 设计结构化提示，将攻击任务转化为多项选择题并结合知识图谱事实，使用语义和中心度滤波器压缩候选集，预计算高阶邻接矩阵并微调LLM进行三元组分类，以高效整合语义和结构信息。

Result: 在两个常用知识图谱数据集上，LLMAtKGE表现优于最强黑盒基线，并能通过推理提供人类可读的解释，且在性能上与白盒方法具有竞争力。

Conclusion: 本文提出的基于LLM的框架有效提升了知识图谱嵌入的对抗攻击性能及可解释性，展示了大型语言模型在该领域的潜力。

Abstract: Adversarial attacks on knowledge graph embeddings (KGE) aim to disrupt the
model's ability of link prediction by removing or inserting triples. A recent
black-box method has attempted to incorporate textual and structural
information to enhance attack performance. However, it is unable to generate
human-readable explanations, and exhibits poor generalizability. In the past
few years, large language models (LLMs) have demonstrated powerful capabilities
in text comprehension, generation, and reasoning. In this paper, we propose
LLMAtKGE, a novel LLM-based framework that selects attack targets and generates
human-readable explanations. To provide the LLM with sufficient factual context
under limited input constraints, we design a structured prompting scheme that
explicitly formulates the attack as multiple-choice questions while
incorporating KG factual evidence. To address the context-window limitation and
hesitation issues, we introduce semantics-based and centrality-based filters,
which compress the candidate set while preserving high recall of
attack-relevant information. Furthermore, to efficiently integrate both
semantic and structural information into the filter, we precompute high-order
adjacency and fine-tune the LLM with a triple classification task to enhance
filtering performance. Experiments on two widely used knowledge graph datasets
demonstrate that our attack outperforms the strongest black-box baselines and
provides explanations via reasoning, and showing competitive performance
compared with white-box methods. Comprehensive ablation and case studies
further validate its capability to generate explanations.

</details>


### [146] [Survey Response Generation: Generating Closed-Ended Survey Responses In-Silico with Large Language Models](https://arxiv.org/abs/2510.11586)
*Georg Ahnert,Anna-Carolina Haensch,Barbara Plank,Markus Strohmaier*

Main category: cs.CL

TL;DR: 本文系统研究了不同的调查问卷生成方法对LLM预测调查响应的影响，发现生成方法对结果有显著影响，限定性生成方法表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注LLM生成封闭式回答，但缺乏统一标准的生成方法，且不同生成方法对预测结果的影响未被充分分析。

Method: 通过32百万次模拟调查响应，结合8种生成方法、4个政治态度调查以及10个大型语言模型，实验比较不同方法的效果。

Result: 结果显示不同生成方法在个体和亚群体层面均有显著差异，限定性生成方法效果最好，而推理输出并不总是提升对齐效果。

Conclusion: 调查响应生成方法对模拟调查响应的影响显著，本文提出了实用的应用建议，促进更准确的LLM问卷模拟研究。

Abstract: Many in-silico simulations of human survey responses with large language
models (LLMs) focus on generating closed-ended survey responses, whereas LLMs
are typically trained to generate open-ended text instead. Previous research
has used a diverse range of methods for generating closed-ended survey
responses with LLMs, and a standard practice remains to be identified. In this
paper, we systematically investigate the impact that various Survey Response
Generation Methods have on predicted survey responses. We present the results
of 32 mio. simulated survey responses across 8 Survey Response Generation
Methods, 4 political attitude surveys, and 10 open-weight language models. We
find significant differences between the Survey Response Generation Methods in
both individual-level and subpopulation-level alignment. Our results show that
Restricted Generation Methods perform best overall, and that reasoning output
does not consistently improve alignment. Our work underlines the significant
impact that Survey Response Generation Methods have on simulated survey
responses, and we develop practical recommendations on the application of
Survey Response Generation Methods.

</details>


### [147] [MeTA-LoRA: Data-Efficient Multi-Task Fine-Tuning for Large Language Models](https://arxiv.org/abs/2510.11598)
*Bo Cheng,Xu Wang,Jinda Liu,Yi Chang,Yuan Wu*

Main category: cs.CL

TL;DR: 本文提出了一种名为MeTA-LoRA的两阶段优化框架，提升了多任务适应中LoRA的参数效率和数据效率。


<details>
  <summary>Details</summary>
Motivation: 传统的LoRA方法在单任务效果良好，但在复杂的多任务学习中难以充分利用任务间知识，且需大量任务特定数据以达到最佳性能。

Method: MeTA-LoRA分两阶段：第一阶段用少量样本训练任务特定的LoRA适配器，实现快速适应；第二阶段通过多个任务的梯度聚合更新共享适配器，促进跨任务知识转移，减少数据需求。

Result: 在多任务和多语言学习场景中，MeTA-LoRA的性能匹配或优于传统基于全量数据的LoRA微调，且所需任务特定数据显著减少。

Conclusion: MeTA-LoRA有效解决了多任务适应中LoRA的数据效率问题，实现了高效的跨任务知识利用，提升了大语言模型的参数高效微调能力。

Abstract: Low-Rank Adaptation (LoRA) has emerged as one of the most widely used
parameter-efficient fine-tuning (PEFT) methods for adapting large language
models (LLMs) to downstream tasks. While highly effective in single-task
settings, it struggles to efficiently leverage inter-task knowledge in complex
multi-task learning scenarios, often requiring substantial task-specific data
to achieve optimal performance. To address this limitation, we introduce
MeTA-LoRA, a two-stage optimization framework that significantly improves data
efficiency in multi-task adaptation. In the first stage, task-specific LoRA
adapters are learned using only a few samples from each involved dataset,
enabling rapid adaptation without large-scale supervision. In the second stage,
the shared LoRA adapter is updated by aggregating gradients from multiple tasks
to promote knowledge transfer across tasks, further reducing data usage by
leveraging common patterns. In both multi-task learning and multilingual
learning scenarios, our method matches or surpasses the performance of
traditional full-data LoRA fine-tuning approaches, while using significantly
less task-specific data.

</details>


### [148] [SemCSE-Multi: Multifaceted and Decodable Embeddings for Aspect-Specific and Interpretable Scientific Domain Mapping](https://arxiv.org/abs/2510.11599)
*Marc Brinner,Sina Zarrieß*

Main category: cs.CL

TL;DR: SemCSE-Multi是一种无监督多维度科学摘要嵌入生成框架，它通过生成特定方面的摘要句子来训练模型，最终实现从摘要中直接预测多个方面的嵌入，并能将嵌入解码为自然语言描述，提高解释性。


<details>
  <summary>Details</summary>
Motivation: 科学摘要内容多面且复杂，传统的单一嵌入难以满足细粒度和可控的相似度评估及用户驱动的可视化需求。

Method: 无监督生成方面特定的摘要句子，训练模型将语义相关的摘要映射到嵌入空间中的邻近位置，蒸馏为统一的多方面嵌入模型，并设计嵌入解码管线将嵌入解码成自然语言描述。

Result: 模型能够生成多方面嵌入，支持高效的一次前向传播预测，解码机制能有效解释嵌入，即使在低维可视化中未被占用的区域。

Conclusion: SemCSE-Multi显著提升科学摘要嵌入的多维度表达能力和解释性，增强了类似度评估和可视化工具的用户交互体验。

Abstract: We propose SemCSE-Multi, a novel unsupervised framework for generating
multifaceted embeddings of scientific abstracts, evaluated in the domains of
invasion biology and medicine. These embeddings capture distinct, individually
specifiable aspects in isolation, thus enabling fine-grained and controllable
similarity assessments as well as adaptive, user-driven visualizations of
scientific domains. Our approach relies on an unsupervised procedure that
produces aspect-specific summarizing sentences and trains embedding models to
map semantically related summaries to nearby positions in the embedding space.
We then distill these aspect-specific embedding capabilities into a unified
embedding model that directly predicts multiple aspect embeddings from a
scientific abstract in a single, efficient forward pass. In addition, we
introduce an embedding decoding pipeline that decodes embeddings back into
natural language descriptions of their associated aspects. Notably, we show
that this decoding remains effective even for unoccupied regions in
low-dimensional visualizations, thus offering vastly improved interpretability
in user-centric settings.

</details>


### [149] [Deconstructing Attention: Investigating Design Principles for Effective Language Modeling](https://arxiv.org/abs/2510.11602)
*Huiyin Xue,Nafise Sadat Moosavi,Nikolaos Aletras*

Main category: cs.CL

TL;DR: 论文系统性拆解了Transformer注意力机制的核心设计原则，发现混合位置信息机制至关重要，而对数学形式和序列相关性要求可以放宽，且混合不同注意力变体表现良好。


<details>
  <summary>Details</summary>
Motivation: 深入理解Transformer中点积注意力机制的核心设计原则的必要性，探索其对模型性能的具体影响。

Method: 设计多个对注意力机制的变体，通过有控制地放松不同设计原则，在统一层和混合层架构中进行实验评估。

Result: 发现信息混合机制不可或缺，数学形式和序列依赖性可在部分层中放宽，且不同变体间交织使用时可提升性能。

Conclusion: 注意力机制的有效性主要依赖于信息混合，其他原则可简化，从而为简化语言模型设计提供新思路。

Abstract: The success of Transformer language models is widely credited to their
dot-product attention mechanism, which interweaves a set of key design
principles: mixing information across positions (enabling multi-token
interactions), sequence-dependent activations (where attention weights adapt to
each input), a specific mathematical form (dot-product similarities plus
softmax weighting), and coupling of queries and keys to evolving hidden states
(grounding attention in the current layer). However, the necessity of each of
these principles remains largely untested. In this work, we systematically
deconstruct attention by designing controlled variants that selectively relax
these principles, applied both uniformly across all layers and in hybrid
architectures where only some layers retain standard attention. Our empirical
analysis reveals that mechanisms for mixing tokens are indispensable, as their
absence collapses models to near-random behavior, while the exact mathematical
form and sequence dependency can be substantially relaxed, especially when
preserved in just a subset of layers. Surprisingly, even variants that fail in
isolation can achieve robust performance when interleaved with standard
attention, highlighting a cooperative effect. These findings deepen our
understanding of what truly underpins attention's effectiveness and open new
avenues for simplifying language models without sacrificing performance.

</details>


### [150] [LLM-Oriented Token-Adaptive Knowledge Distillation](https://arxiv.org/abs/2510.11615)
*Xurong Xie,Zhucun Xue,Jiafu Wu,Jian Li,Yabiao Wang,Xiaobin Hu,Yong Liu,Jiangning Zhang*

Main category: cs.CL

TL;DR: 该论文提出了一种针对大型语言模型知识蒸馏的新方法AdaKD，通过动态调整每个token的蒸馏策略，提高了知识转移的效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于logit的知识蒸馏方法采用固定、静态策略，不能适应学生模型动态学习过程，导致知识转移效果不理想。

Method: AdaKD框架包括两个模块：一是基于loss的自适应token关注（LATF），根据学生模型学习稳定性动态调整关注重点；二是逆难度温度缩放（IDTS），对难学token使用低温度，对易学token使用高温度，优化温度策略以提升学习效果。

Result: AdaKD作为即插即用框架，在多种蒸馏方法、模型结构和基准测试中均能稳定提升性能。

Conclusion: 动态调节token层面的蒸馏策略，更贴合学生模型的学习状态，能够有效提升知识蒸馏效果，适用于多种模型和方法。

Abstract: Knowledge distillation (KD) is a key technique for compressing large-scale
language models (LLMs), yet prevailing logit-based methods typically employ
static strategies that are misaligned with the dynamic learning process of
student models. These methods typically treat all tokens indiscriminately and
apply a single, fixed temperature, resulting in suboptimal knowledge transfer.
To address these limitations, we propose LLM-Oriented Token-Adaptive Knowledge
Distillation (AdaKD), a novel framework that adapts the distillation process to
the real-time learning state of each token. AdaKD consists of two synergistic
modules driven by a unified token difficulty metric. First, our Loss-Driven
Adaptive Token Focusing (LATF) module dynamically adjusts the distillation
focus by monitoring the student's learning stability, concentrating
computational resources on the most valuable tokens at each training phase.
Second, we introduce Inverse Difficulty Temperature Scaling (IDTS), a
counterintuitive yet effective token-level temperature strategy. It employs low
temperatures for difficult tokens for targeted error correction, and high
temperatures for easy tokens to encourage students to learn from the teacher's
complete and smooth output distribution, thereby enhancing generalization. As a
plug-and-play framework, AdaKD can consistently improve the performance of
various distillation methods on multiple model architectures and benchmarks.

</details>


### [151] [Enhancing Long Chain-of-Thought Reasoning through Multi-Path Plan Aggregation](https://arxiv.org/abs/2510.11620)
*Siheng Xiong,Ali Payani,Faramarz Fekri*

Main category: cs.CL

TL;DR: 本文提出了多路径计划聚合（MPPA）方法，通过多次生成候选计划并聚合，解决了语言模型推理链偏离问题，提升了推理能力，特别适用于小规模语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有推理链一次性生成方法易导致推理轨迹偏离，尤其在小模型和长推理链情况下错误积累严重。分析发现推理错误多源于规划错误，因此需要改进规划步骤。

Method: 提出MPPA框架，通过变间隔计划探索和聚合机制生成更准确的规划；使用轻量LoRA模块实现计划聚合策略以提高效率；引入在线Step-DPO与TSMC技术进行分步监督，提升训练效率和稳定性。

Result: 在数学、科学和逻辑推理基准测试中，使用10%微调数据和5%偏好对，方法优于DeepSeek-R1和基于结果奖励的强化学习基线，在多个基础模型和任务上表现出更高准确率。

Conclusion: 通过多路径计划生成与聚合结合分步偏好优化，本文方法显著提升了小规模语言模型的长链推理性能，且训练效率高，稳定性好。

Abstract: Inference-time scaling enhances the reasoning ability of a language model
(LM) by extending its chain-of-thought (CoT). However, existing approaches
typically generate the entire reasoning chain in a single forward pass, which
often leads to CoT derailment, i.e., the reasoning trajectory drifting off
course due to compounding errors. This problem is particularly severe for
smaller LMs with long CoTs due to their limited capacity. To address this, we
analyze raw long CoTs and uncover a reasoning hierarchy consisting of planning
and execution steps. Our analysis reveals that most reasoning errors stem from
incorrect planning. Motivated by this observation, we propose Multi-Path Plan
Aggregation (MPPA), a framework that augments single-pass reasoning with plan
exploration and aggregation. Following a variable interval schedule based on
the token position, MPPA generates multiple candidate plans and aggregates them
into a refined planning step. To maintain efficiency, we adopt a minimal design
in which the base LM serves as the primary policy, while a lightweight LoRA
module implements the plan aggregation policy. We further observe that
outcome-reward RL is inefficient for long trajectories (e.g., exceeding 4K
tokens). To overcome this, we introduce online Step-DPO, a process-level
preference optimization scheme that leverages Twisted Sequential Monte Carlo
(TSMC) to provide scalable stepwise supervision using small LMs. This yields
more efficient training, improved stability, and higher accuracy. Extensive
experiments on challenging math, science, and logical reasoning benchmarks
demonstrate that, with only 10% SFT data and 5% of preference pairs, our method
outperforms both the DeepSeek-R1 distillation baseline and the outcome-reward
RL baseline across multiple base models and tasks.

</details>


### [152] [ACADREASON: Exploring the Limits of Reasoning Models with Academic Research Problems](https://arxiv.org/abs/2510.11652)
*Xin Gui,King Zhu,JinCheng Ren,Qianben Chen,Zekun Moore Wang,Yizhi LI,Xinpeng Liu,Xiaowan Li,Wenli Ren,Linyu Miao,Tianrui Qin,Ziqi Shu,He Zhu,Xiangru Tang,Dingfeng Shi,Jiaheng Liu,Yuchen Eleanor Jiang,Minghao Liu,Ge Zhang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: 本文提出了Acadreason基准，评估大型语言模型和智能体在多领域学术高阶推理任务上的能力，现有模型得分普遍较低，显示出能力不足。


<details>
  <summary>Details</summary>
Motivation: 现有评测多关注数学/代码竞赛或通用任务，缺乏高阶推理深度的多领域学术基准，导致无法严谨评价模型在复杂学术推理上的表现。

Method: 设计Acadreason基准，包含计算机科学、经济学、法学、数学和哲学五个高推理领域的50个专家标注学术问题，问题来自顶级近年发表，经过严格质量控制。

Result: 对10余种主流大型语言模型和智能体系统评测显示，大部分模型得分低于20分，领先的GPT-5仅16分，智能体得分略高，均未超过40分。

Conclusion: 当前大型语言模型与智能体在超智能学术研究任务上存在明显能力缺口，Acadreason基准揭示了高阶学术推理的挑战所在。

Abstract: In recent years, the research focus of large language models (LLMs) and
agents has shifted increasingly from demonstrating novel capabilities to
complex reasoning and tackling challenging tasks. However, existing evaluations
focus mainly on math/code contests or general tasks, while existing
multi-domain academic benchmarks lack sufficient reasoning depth, leaving the
field without a rigorous benchmark for high-level reasoning. To fill this gap,
we introduce the Acadreason benchmark, designed to evaluate the ability of LLMs
and agents to acquire and reason over academic knowledge. It consists of 50
expert-annotated academic problems across five high-reasoning domains,
including computer science, economics, law, mathematics, and philosophy. All
questions are sourced from top-tier publications in recent years and undergo
rigorous annotation and quality control to ensure they are both challenging and
answerable. We conduct systematic evaluations of over 10 mainstream LLMs and
agents. The results show that most LLMs scored below 20 points, with even the
cutting-edge GPT-5 achieving only 16 points. While agents achieved higher
scores, none exceeded 40 points. This demonstrates the current capability gap
between LLMs and agents in super-intelligent academic research tasks and
highlights the challenges of Acadreason.

</details>


### [153] [Scaling Language-Centric Omnimodal Representation Learning](https://arxiv.org/abs/2510.11693)
*Chenghao Xiao,Hou Pong Chan,Hao Zhang,Weiwen Xu,Mahani Aljunied,Yu Rong*

Main category: cs.CL

TL;DR: 本文研究了基于多模态大型语言模型（MLLM）结合对比学习（CL）进行多模态嵌入的方法，揭示了生成预训练中隐式跨模态对齐的关键作用，提出了语言中心的全模态嵌入框架LCO-Emb，并通过实验验证了其性能优越性，最后发现并理论支持了一种代表性与生成能力的尺度定律（GRSL）。


<details>
  <summary>Details</summary>
Motivation: 尽管基于MLLM与CL的方法在多模态嵌入任务中取得了优异表现，其背后的性能优势原因尚未深入探讨，本文旨在揭示这种优势的本质机制。

Method: 通过分析MLLM隐含的跨模态对齐特性，提出以语言为中心的全模态嵌入框架LCO-Emb，并引入对比学习作为轻量级细化手段。同时，发现并理论证明了生成能力与表征能力之间的尺度定律（GRSL）。

Result: LCO-Emb在多种模型架构和基准任务上均表现出领先的性能。在一个低资源视觉文档检索任务上，持续的生成预训练配合对比学习进一步提升了模型的嵌入能力。

Conclusion: 隐式跨模态对齐是MLLM结合CL表现优异的关键，提升生成能力是改善表示质量的有效路径，GRSL为理解生成与表示能力关系提供了理论支持。

Abstract: Recent multimodal embedding approaches leveraging multimodal large language
models (MLLMs) fine-tuned with contrastive learning (CL) have shown promising
results, yet the underlying reasons behind their superiority remain
underexplored. This work argues that a crucial advantage of MLLM-based
approaches stems from implicit cross-modal alignment achieved during generative
pretraining, where the language decoder learns to exploit multimodal signals
within a shared representation space for generating unimodal outputs. Through
analysis of anisotropy and kernel similarity structure, we empirically confirm
that latent alignment emerges within MLLM representations, allowing CL to serve
as a lightweight refinement stage. Leveraging this insight, we propose a
Language-Centric Omnimodal Embedding framework, termed LCO-Emb. Extensive
experiments across diverse backbones and benchmarks demonstrate its
effectiveness, achieving state-of-the-art performance across modalities.
Furthermore, we identify a Generation-Representation Scaling Law (GRSL),
showing that the representational capabilities gained through contrastive
refinement scales positively with the MLLM's generative capabilities. This
suggests that improving generative abilities evolves as an effective paradigm
for enhancing representation quality. We provide a theoretical explanation of
GRSL, which formally links the MLLM's generative quality to the upper bound on
its representation performance, and validate it on a challenging, low-resource
visual-document retrieval task, showing that continual generative pretraining
before CL can further enhance the potential of a model's embedding
capabilities. Codes, models, and resources are available at
https://github.com/LCO-Embedding/LCO-Embedding.

</details>


### [154] [When Agents Trade: Live Multi-Market Trading Benchmark for LLM Agents](https://arxiv.org/abs/2510.11695)
*Lingfei Qian,Xueqing Peng,Yan Wang,Vincent Jim Zhang,Huan He,Hanley Smith,Yi Han,Yueru He,Haohang Li,Yupeng Cao,Yangyang Yu,Alejandro Lopez-Lira,Peng Lu,Jian-Yun Nie,Guojun Xiong,Jimin Huang,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 本文提出了Agent Market Arena (AMA)，一个用于评估基于大语言模型的交易代理人的终身实时基准，涵盖多市场、验证数据和多种代理架构，并通过实盘加密货币和股票市场实验展示不同代理行为差异。


<details>
  <summary>Details</summary>
Motivation: 现有研究多测试模型而非代理，覆盖资产和时间有限，数据未经验证，难以评估LLM代理在真实市场中的推理和适应能力。

Method: 设计AMA，集成验证交易数据、专家校验新闻和多种代理架构，涵盖四个代理和多款主流LLM模型，在统一交易框架下实时测试比较。

Result: 实盘实验证明，不同代理框架表现出明显行为差异，如激进与保守的风险风格，而底层模型对结果贡献较小。

Conclusion: AMA为LLM交易代理的金融推理与交易智能评估提供了严谨、可复现、持续演进的基础。

Abstract: Although Large Language Model (LLM)-based agents are increasingly used in
financial trading, it remains unclear whether they can reason and adapt in live
markets, as most studies test models instead of agents, cover limited periods
and assets, and rely on unverified data. To address these gaps, we introduce
Agent Market Arena (AMA), the first lifelong, real-time benchmark for
evaluating LLM-based trading agents across multiple markets. AMA integrates
verified trading data, expert-checked news, and diverse agent architectures
within a unified trading framework, enabling fair and continuous comparison
under real conditions. It implements four agents, including InvestorAgent as a
single-agent baseline, TradeAgent and HedgeFundAgent with different risk
styles, and DeepFundAgent with memory-based reasoning, and evaluates them
across GPT-4o, GPT-4.1, Claude-3.5-haiku, Claude-sonnet-4, and
Gemini-2.0-flash. Live experiments on both cryptocurrency and stock markets
demonstrate that agent frameworks display markedly distinct behavioral
patterns, spanning from aggressive risk-taking to conservative decision-making,
whereas model backbones contribute less to outcome variation. AMA thus
establishes a foundation for rigorous, reproducible, and continuously evolving
evaluation of financial reasoning and trading intelligence in LLM-based agents.

</details>


### [155] [Demystifying Reinforcement Learning in Agentic Reasoning](https://arxiv.org/abs/2510.11701)
*Zhaochen Yu,Ling Yang,Jiaru Zou,Shuicheng Yan,Mengdi Wang*

Main category: cs.CL

TL;DR: 本文系统研究了强化学习（RL）在智能代理推理中的设计原则和最佳实践，提出了数据、算法和推理模式三方面的关键见解，并构建了高质量数据集验证效果。


<details>
  <summary>Details</summary>
Motivation: 当前智能代理强化学习可以提升大型语言模型的推理能力，但其设计原则和优化方法尚不明确，因此需要系统性研究以明确有效策略并驱动性能提升。

Method: 从数据、算法和推理模式三方面展开研究：(1)用真实端到端工具使用轨迹替代合成数据，构建高多样性的模型感知数据集促进探索；(2)采用探索友好型技术如增大clip阈值、过长奖励塑造和保持策略熵提升训练效率；(3)采用精炼的推理策略减少工具调用频次以提升工具利用效率和最终准确率。

Result: 提出的方法在多个具有挑战性的基准测试（AIME2024/AIME2025等）上显著提升了智能代理推理性能和训练效率，小规模4B模型的表现甚至优于大型32B模型。

Conclusion: 本文通过彻底分析和实证验证，明确了智能代理强化学习的关键设计原则和有效实践，提供了实用的训练指南和高质量数据集，为未来相关研究奠定了坚实基础。

Abstract: Recently, the emergence of agentic RL has showcased that RL could also
effectively improve the agentic reasoning ability of LLMs, yet the key design
principles and optimal practices remain unclear. In this work, we conduct a
comprehensive and systematic investigation to demystify reinforcement learning
in agentic reasoning from three key perspectives: data, algorithm, and
reasoning mode. We highlight our key insights: (i) Replacing stitched synthetic
trajectories with real end-to-end tool-use trajectories yields a far stronger
SFT initialization; high-diversity, model-aware datasets sustain exploration
and markedly improve RL performance. (ii) Exploration-friendly techniques are
crucial for agentic RL, such as clip higher, overlong reward shaping, and
maintaining adequate policy entropy could improve the training efficiency.
(iii) A deliberative strategy with fewer tool calls outperforms frequent tool
calls or verbose self-reasoning, improving tool efficiency and final accuracy.
Together, these simple practices consistently enhance agentic reasoning and
training efficiency, achieving strong results on challenging benchmarks with
smaller models, and establishing a practical baseline for future agentic RL
research. Beyond these empirical insights, we further contribute a
high-quality, real end-to-end agentic SFT dataset along with a high-quality RL
dataset, and demonstrate the effectiveness of our insights in boosting the
agentic reasoning ability of LLMs across four challenging benchmarks, including
AIME2024/AIME2025, GPQA-Diamond, and LiveCodeBench-v6. With our recipes,
4B-sized models could also achieve superior agentic reasoning performance
compared to 32B-sized models. Code and models:
https://github.com/Gen-Verse/Open-AgentRL

</details>


### [156] [Are Large Reasoning Models Interruptible?](https://arxiv.org/abs/2510.11713)
*Tsung-Han Wu,Mihran Miroyan,David M. Chan,Trevor Darrell,Narges Norouzi,Joseph E. Gonzalez*

Main category: cs.CL

TL;DR: 本文指出大型推理模型在传统的静止环境评估下高性能，但在动态变化的上下文中表现不稳定，性能可能大幅下降。


<details>
  <summary>Details</summary>
Motivation: 传统评估假设推理过程中的环境不变，而现实任务中如辅助编程等场景环境会动态变化，模型需适应这种变化。

Method: 设计了两种动态环境下的评估场景：中断测试模型在有限时间内的部分输出质量；动态上下文测试模型对中途变更的适应能力。

Result: 静态评估高估了模型的鲁棒性，动态环境下性能下降显著，最高可达60%，并发现了推理泄漏、恐慌和自我怀疑等新型失败模式。

Conclusion: 评估LRM时需考虑动态变化环境，提升模型在真实长时推理任务中的适应性和鲁棒性。

Abstract: Large Reasoning Models (LRMs) excel at complex reasoning but are
traditionally evaluated in static, "frozen world" settings: model responses are
assumed to be instantaneous, and the context of a request is presumed to be
immutable over the duration of the response. While generally true for
short-term tasks, the "frozen world" assumption breaks down in modern reasoning
tasks such as assistive programming, where models may take hours to think
through problems and code may change dramatically from the time the model
starts thinking to the model's final output. In this work, we challenge the
frozen world assumption and evaluate LRM robustness under two realistic dynamic
scenarios: interruptions, which test the quality of the model's partial outputs
on a limited budget, and dynamic context, which tests model adaptation to
in-flight changes. Across mathematics and programming benchmarks that require
long-form reasoning, static evaluations consistently overestimate robustness:
even state-of-the-art LRMs, which achieve high accuracy in static settings, can
fail unpredictably when interrupted or exposed to changing context, with
performance dropping by up to 60% when updates are introduced late in the
reasoning process. Our analysis further reveals several novel failure modes,
including reasoning leakage, where models fold the reasoning into their final
answer when interrupted; panic, where under time pressure models abandon
reasoning entirely and return incorrect answers; and self-doubt, where
performance degrades while incorporating updated information.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [157] [A Comprehensive Survey on Benchmarks and Solutions in Software Engineering of LLM-Empowered Agentic System](https://arxiv.org/abs/2510.09721)
*Jiale Guo,Suizhi Huang,Mei Li,Dong Huang,Xingsheng Chen,Regina Zhang,Zhijiang Guo,Han Yu,Siu-Ming Yiu,Christian Jensen,Pietro Lio,Kwok-Yan Lam*

Main category: cs.SE

TL;DR: 本文综述了大语言模型（LLM）在软件工程中的应用，分析了150多篇论文，构建了涵盖解决方案和基准测试的分类体系，揭示了从简单提示工程到复杂代理系统的发展过程。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对LLM驱动的软件工程领域中评测方法和解决方案之间关系的系统理解，阻碍了该领域的持续发展和评估。

Method: 通过系统分析150多篇相关论文，构建包含解决方案（提示、微调、代理）和基准测试（代码生成、翻译、修复等）两大维度的综合分类体系，提出统一流程模型展示不同解决方案对应的软件工程任务复杂度。

Result: 成功连接50多个基准测试与相应的解决方案策略，揭示了LLM在软件工程中从简单提示到复杂代理系统的演进，涵盖了规划、推理、自我优化、记忆和工具扩展等关键技术。

Conclusion: 该综述填补了评估与解决方案之间的空白，指出了多智能体协作、自我进化代码生成及形式验证与LLM结合等未来研究方向，为研究者和从业者提供了系统理解和发展LLM软件工程的基础资源。

Abstract: The integration of LLMs into software engineering has catalyzed a paradigm
shift from traditional rule-based systems to sophisticated agentic systems
capable of autonomous problem-solving. Despite this transformation, the field
lacks a comprehensive understanding of how benchmarks and solutions
interconnect, hindering systematic progress and evaluation. This survey
presents the first holistic analysis of LLM-empowered software engineering,
bridging the critical gap between evaluation and solution approaches. We
analyze 150+ recent papers and organize them into a comprehensive taxonomy
spanning two major dimensions: (1) Solutions, categorized into prompt-based,
fine-tuning-based, and agent-based paradigms, and (2) Benchmarks, covering code
generation, translation, repair, and other tasks. Our analysis reveals how the
field has evolved from simple prompt engineering to complex agentic systems
incorporating planning and decomposition, reasoning and self-refinement, memory
mechanisms, and tool augmentation. We present a unified pipeline that
illustrates the complete workflow from task specification to final
deliverables, demonstrating how different solution paradigms address varying
complexity levels across software engineering tasks. Unlike existing surveys
that focus on isolated aspects, we provide full-spectrum coverage connecting
50+ benchmarks with their corresponding solution strategies, enabling
researchers to identify optimal approaches for specific evaluation criteria.
Furthermore, we identify critical research gaps and propose actionable future
directions, including multi-agent collaboration frameworks, self-evolving code
generation systems, and integration of formal verification with LLM-based
methods. This survey serves as a foundational resource for researchers and
practitioners seeking to understand, evaluate, and advance LLM-empowered
software engineering systems.

</details>


### [158] [InteractScience: Programmatic and Visually-Grounded Evaluation of Interactive Scientific Demonstration Code Generation](https://arxiv.org/abs/2510.09724)
*Qiaosheng Chen,Yang Liu,Lei Li,Kai Chen,Qipeng Guo,Gong Cheng,Fei Yuan*

Main category: cs.SE

TL;DR: 本文提出了InteractScience基准测试，评估大型语言模型在科学知识与交互式前端代码生成结合方面的能力，针对科学演示的应用场景设计测试框架，涵盖功能性和视觉质量验证。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型缺乏综合科学知识和交互编程能力的评测手段，无法有效评价其在科学交互演示领域的应用潜力。

Method: 设计了混合框架结合程序功能测试和视觉质量检测，创建了覆盖五个科学领域的问题集及对应的单元测试、参考快照和检查清单。

Result: 评估了30个顶尖大型语言模型，暴露出它们在结合领域知识与交互前端编码方面的不足。

Conclusion: InteractScience为评测大型语言模型自动生成科学交互演示代码提供了首个综合且自动化的基准，推动科学教育和研究展示的技术进步。

Abstract: Large Language Models (LLMs) are increasingly capable of generating complete
applications from natural language instructions, creating new opportunities in
science and education. In these domains, interactive scientific demonstrations
are particularly valuable for explaining concepts, supporting new teaching
methods, and presenting research findings. Generating such demonstrations
requires models to combine accurate scientific knowledge with the ability to
implement interactive front-end code that behaves correctly and responds to
user actions. This capability goes beyond the scope of existing benchmarks,
which typically evaluate either knowledge question answering without grounding
in code or static web code generation without scientific interactivity. To
evaluate this integrated ability, we design a hybrid framework that combines
programmatic functional testing to rigorously verify interaction logic with
visually-grounded qualitative testing to assess rendered outputs against
reference snapshots. Building on this framework, we present InteractScience, a
benchmark consisting of a substantial set of carefully designed questions
across five scientific domains, each paired with unit tests, reference
snapshots, and checklists. We evaluate 30 leading open- and closed-source LLMs
and report results that highlight ongoing weaknesses in integrating domain
knowledge with interactive front-end coding. Our work positions InteractScience
as the first benchmark to automatically measure this combined capability with
realistic interactive operations, providing a foundation for advancing reliable
and educationally useful scientific demonstration code generation. All code and
data are publicly available at https://github.com/open-compass/InteractScience.

</details>


### [159] [Agentic Property-Based Testing: Finding Bugs Across the Python Ecosystem](https://arxiv.org/abs/2510.09907)
*Muhammad Maaz,Liam DeVoe,Zac Hatfield-Dodds,Nicholas Carlini*

Main category: cs.SE

TL;DR: 本文提出了一种基于大语言模型(LLM)的自动化属性测试代理，用于自动生成和执行属性测试，并报告有效的Python代码缺陷。


<details>
  <summary>Details</summary>
Motivation: 传统的属性测试需要用户手动定义测试输入和性质，效率低且依赖人工经验。希望利用LLM自动推断函数性质和构造测试，实现自动化漏洞检测。

Method: 设计了一种LLM驱动的测试代理，分析Python模块的代码和文档，推断函数内部及跨函数的性质，自动合成和执行属性测试，反思测试结果确认缺陷，并生成可操作的bug报告。

Result: 在100个流行Python包上的评估中，代理生成的bug报告中56%为有效缺陷，32%为建议上报的缺陷。通过一个打分排序系统筛选高优先级缺陷，顶级缺陷中86%有效且81%值得上报。共报告5个缺陷，4个含修复补丁，3个补丁已被合并。

Conclusion: 结合大语言模型和属性测试技术，能够实现严谨且可扩展的自动化软件测试，有效发现多样化的缺陷类型，提升软件质量。代码开源提供复现和进一步研究基础。

Abstract: Property-based testing (PBT) is a lightweight formal method, typically
implemented as a randomized testing framework. Users specify the input domain
for their test using combinators supplied by the PBT framework, and the
expected properties or invariants as a unit-test function. The framework then
searches for a counterexample, e.g. by generating inputs and calling the test
function. In this work, we demonstrate an LLM-based agent which analyzes Python
modules, infers function-specific and cross-function properties from code and
documentation, synthesizes and executes PBTs, reflects on outputs of these
tests to confirm true bugs, and finally outputs actionable bug reports for the
developer. We perform an extensive evaluation of our agent across 100 popular
Python packages. Of the bug reports generated by the agent, we found after
manual review that 56\% were valid bugs and 32\% were valid bugs that we would
report to maintainers. We then developed a ranking rubric to surface
high-priority valid bugs to developers, and found that of the 21 top-scoring
bugs, 86\% were valid and 81\% we would report. The bugs span diverse failure
modes from serialization failures to numerical precision errors to flawed cache
implementations. We reported 5 bugs, 4 with patches, including to NumPy and
cloud computing SDKs, with 3 patches merged successfully. Our results suggest
that LLMs with PBT provides a rigorous and scalable method for autonomously
testing software. Our code and artifacts are available at:
https://github.com/mmaaz-git/agentic-pbt.

</details>


### [160] [OFP-Repair: Repairing Floating-point Errors via Original-Precision Arithmetic](https://arxiv.org/abs/2510.09938)
*Youshuai Tan,Zishuo Ding,Jinfu Chen,Weiyi Shang*

Main category: cs.SE

TL;DR: 本文针对浮点程序中的错误修复问题，提出了一种称为OFP-Repair的新方法，能够有效区分和修复不同类型的浮点错误，在多个数据集上取得显著改进，并成功应用于真实的GNU科学库中的老旧缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有浮点错误修复工具往往依赖高精度计算，开发成本高且难以区分错误类型，导致一些错误难以修复。

Method: 提出OFP-Repair方法，通过对错误类型的有效区分及相应修复策略，减少开发成本和计算资源需求。

Result: 在ACESO数据集上，OFP-Repair在四项精度指标上分别提升数个量级，能够检测并修复更多原始精度可修复的错误。在GSL项目中成功修复多个历史缺陷。

Conclusion: OFP-Repair展示了其在理论和实践中的显著优势和应用潜力，有望被集成到实际开发流程中，促进浮点程序错误的高效修复。

Abstract: Errors in floating-point programs can lead to severe consequences,
particularly in critical domains such as military, aerospace, and financial
systems, making their repair a crucial research problem. In practice, some
errors can be fixed using original-precision arithmetic, while others require
high-precision computation. Developers often avoid addressing the latter due to
excessive computational resources required. However, they sometimes struggle to
distinguish between these two types of errors, and existing repair tools fail
to assist in this differentiation. Most current repair tools rely on
high-precision implementations, which are time-consuming to develop and demand
specialized expertise. Although a few tools do not require high-precision
programs, they can only fix a limited subset of errors or produce suboptimal
results.
  To address these challenges, we propose a novel method, named OFP-Repair.On
ACESO's dataset, our patches achieve improvements of three, seven, three, and
eight orders of magnitude across four accuracy metrics. In real-world cases,
our method successfully detects all five original-precision-repairable errors
and fixes three, whereas ACESO only repairs one. Notably, these results are
based on verified data and do not fully capture the potential of OFP-Repair. To
further validate our method, we deploy it on a decade-old open bug report from
GNU Scientific Library (GSL), successfully repairing five out of 15 bugs. The
developers have expressed interest in our method and are considering
integrating our tool into their development workflow. We are currently working
on applying our patches to GSL. The results are highly encouraging,
demonstrating the practical applicability of our technique.

</details>


### [161] [Operationalizing AI: Empirical Evidence on MLOps Practices, User Satisfaction, and Organizational Context](https://arxiv.org/abs/2510.09968)
*Stefan Pasch*

Main category: cs.SE

TL;DR: 本文通过分析8000多条用户评论，研究了MLOps实践对AI开发满意度的影响，发现大多数MLOps实践能显著提升用户满意度，同时组织背景影响MLOps的关注度，但不影响其价值感知。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏实证研究证实MLOps实践在支持AI应用开发和运营方面的效果和机制。

Method: 通过零样本分类技术，分析了G2.com上8000多条AI开发平台用户评论中对九项MLOps实践（如CI/CD、版本控制等）的情感倾向及其与用户满意度的关联。

Result: 七项MLOps实践与用户满意度显著正相关，表明有效的MLOps实施为AI开发创造实质价值。小型企业较少讨论某些MLOps实践，但企业规模不影响MLOps与满意度的关系。

Conclusion: MLOps实践被广泛认可为提升AI开发价值的有效手段，且其效益在不同组织环境中具有普适性，组织背景影响的是实践的关注度而非其价值。

Abstract: Organizational efforts to utilize and operationalize artificial intelligence
(AI) are often accompanied by substantial challenges, including scalability,
maintenance, and coordination across teams. In response, the concept of Machine
Learning Operations (MLOps) has emerged as a set of best practices that
integrate software engineering principles with the unique demands of managing
the ML lifecycle. Yet, empirical evidence on whether and how these practices
support users in developing and operationalizing AI applications remains
limited. To address this gap, this study analyzes over 8,000 user reviews of AI
development platforms from G2.com. Using zero-shot classification, we measure
review sentiment toward nine established MLOps practices, including continuous
integration and delivery (CI/CD), workflow orchestration, reproducibility,
versioning, collaboration, and monitoring. Seven of the nine practices show a
significant positive relationship with user satisfaction, suggesting that
effective MLOps implementation contributes tangible value to AI development.
However, organizational context also matters: reviewers from small firms
discuss certain MLOps practices less frequently, suggesting that organizational
context influences the prevalence and salience of MLOps, though firm size does
not moderate the MLOps-satisfaction link. This indicates that once applied,
MLOps practices are perceived as universally beneficial across organizational
settings.

</details>


### [162] [SLEAN: Simple Lightweight Ensemble Analysis Network for Multi-Provider LLM Coordination: Design, Implementation, and Vibe Coding Bug Investigation Case Study](https://arxiv.org/abs/2510.10010)
*Matheus J. T. Vargas*

Main category: cs.SE

TL;DR: SLEAN是一个简单且轻量级的多LLM协作框架，通过文本提示协调多种LLM，过滤有害的AI代码建议，实现高效安全的代码修复。


<details>
  <summary>Details</summary>
Motivation: 当前AI辅助调试过程中，生成的代码修改常引入不必要的复杂性或破坏现有功能，亟需一个简易且高效的多模型协调机制来提升代码质量和安全性。

Method: 提出SLEAN框架，利用.txt模板文本提示，在独立分析、交叉批评和仲裁三个阶段对多个LLM输出进行过滤和精简，确保代码修改最小且安全。

Result: 在15个软件缺陷和69个AI修复建议的测试中，SLEAN接受了31.9%的修复方案，有效过滤了潜在有害代码，仲裁阶段将代码修改范围缩小了83-90%，提升了修复效率和安全性。

Conclusion: SLEAN无需复杂基础设施和深度技术背景即可部署，具有多提供者适应性，可广泛应用于安全审计、代码评审和文档验证等需要多模型可靠合成的场景。

Abstract: We present SLEAN (Simple Lightweight Ensemble Analysis Network), a
deterministic framework for coordinating multiple LLM providers through
text-based prompt orchestration. Unlike complex multi-agent systems requiring
specialized infrastructure, SLEAN operates as a simple prompt bridge between
LLMs using .txt templates, requiring no deep technical knowledge for
deployment. The three-phase protocol formed by independent analysis,
cross-critique, and arbitration, filters harmful AI-generated code suggestions
before production deployment, addressing how AI-assisted debugging increasingly
produces modifications that introduce unnecessary complexity, break existing
functionality, or address problems. Evaluating 15 software bugs, we analyzed 69
AI-generated fix propositions. SLEAN's filtering accepted 22 fixes (31.9%, 95%
CI 20.9-42.9%) while rejecting 47 that would have been harmful if applied
verbatim. The arbitration process reduced code change surface by 83-90%
relative to raw AI outputs, enforcing minimal causal edits over scope-expanding
modifications. Minimal Type 2 inputs proved more efficient than detailed Type 1
inputs, requiring 2.85 versus 3.56 propositions per accepted fix (35.1% versus
28.1% acceptance, about a 20% efficiency gain). Agreement between AI systems
showed weak correlation with fix quality: high convergence (at least 80%)
occurred in 4 of 15 cases and improved acceptance by only 2.4% points;
arbitration appeared only at exactly 10% convergence in 2 of 15 cases, although
low convergence alone did not necessitate arbitration. The file-driven,
provider-agnostic architecture enables deployment without specialized coding
expertise, making it applicable to security auditing, code review, document
verification, and other domains requiring reliable multi-provider synthesis
with end-to-end auditability.

</details>


### [163] [OBsmith: Testing JavaScript Obfuscator using LLM-powered sketching](https://arxiv.org/abs/2510.10066)
*Shan Jiang,Chenguang Zhu,Sarfraz Khurshid*

Main category: cs.SE

TL;DR: 本文提出了OBsmith框架，通过利用大语言模型系统性测试JavaScript混淆器的正确性，发现了11个未知的混淆错误，补充了现有模糊测试工具的不足。


<details>
  <summary>Details</summary>
Motivation: 当前JavaScript混淆器在保护知识产权和抵抗逆向工程上广泛应用，但其语义正确性未被充分验证，错误的转换可能破坏程序功能、影响安全性。

Method: OBsmith利用大语言模型生成多样化程序模板，并结合从真实程序中自动提取的模板，生成可执行程序进行混淆测试，检测混淆器在不同配置下的语义保持能力。

Result: OBsmith发现11个未知混淆错误，现有五个主流JavaScript模糊测试工具均未检测出，验证了OBsmith针对混淆器误行为的独特能力。消融实验显示除通用变换关系外，其它组件均发现了某类错误。

Conclusion: OBsmith为自动化测试和质量保障混淆器及其他语义保持工具链提供了新方法，推动了混淆器语义正确性的研究和工具发展。

Abstract: JavaScript obfuscators are widely deployed to protect intellectual property
and resist reverse engineering, yet their correctness has been largely
overlooked compared to performance and resilience. Existing evaluations
typically measure resistance to deobfuscation, leaving the critical question of
whether obfuscators preserve program semantics unanswered. Incorrect
transformations can silently alter functionality, compromise reliability, and
erode security-undermining the very purpose of obfuscation. To address this
gap, we present OBsmith, a novel framework to systematically test JavaScript
obfuscators using large language models (LLMs). OBsmith leverages LLMs to
generate program sketches abstract templates capturing diverse language
constructs, idioms, and corner cases-which are instantiated into executable
programs and subjected to obfuscation under different configurations. Besides
LLM-powered sketching, OBsmith also employs a second source: automatic
extraction of sketches from real programs. This extraction path enables more
focused testing of project specific features and lets developers inject domain
knowledge into the resulting test cases. OBsmith uncovers 11 previously unknown
correctness bugs. Under an equal program budget, five general purpose
state-of-the-art JavaScript fuzzers (FuzzJIT, Jsfunfuzz, Superion, DIE,
Fuzzilli) failed to detect these issues, highlighting OBsmith's complementary
focus on obfuscation induced misbehavior. An ablation shows that all components
except our generic MRs contribute to at least one bug class; the negative MR
result suggests the need for obfuscator-specific metamorphic relations. Our
results also seed discussion on how to balance obfuscation presets and
performance cost. We envision OBsmith as an important step towards automated
testing and quality assurance of obfuscators and other semantic-preserving
toolchains.

</details>


### [164] [A Mathematics-Guided Approach to Floating-Point Error Detection](https://arxiv.org/abs/2510.10081)
*Youshuai Tan,Zhanwei Zhang,Zishuo Ding,Lianyu Zheng,Jinfu Chen,Weiyi Shang*

Main category: cs.SE

TL;DR: 本文提出了一种基于数学指导的浮点程序错误检测新方法MGDE，以解决现有方法计算成本高和搜索效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 现有浮点错误检测方法计算时间过长且收敛能力有限，难以有效定位高误差输入，亟需一种高效且准确的检测方法。

Method: 利用牛顿-拉夫森方法的二次收敛特性，实现对浮点错误诱发输入的高效检测。

Result: MGDE在测试中检测出89个错误，覆盖44个程序，检测速度是FPCC的6.4096倍，且多输入程序中成功检测出9个错误，显著优于现有最优方法FPCC。

Conclusion: MGDE方法在检测浮点错误诱发输入上效率更高、准确性更强，具有广泛的应用潜力，尤其适用于关键领域的程序错误分析。

Abstract: Floating-point program errors can lead to severe consequences, particularly
in critical domains such as military applications. Only a small subset of
inputs may induce substantial floating-point errors, prompting researchers to
develop methods for identifying these error-inducing inputs. Although existing
approaches have achieved some success, they still suffer from two major
limitations: (1) High computational cost: The evaluation of error magnitude for
candidate inputs relies on high-precision programs, which are prohibitively
time-consuming. (2) Limited long-range convergence capability: Current methods
exhibit inefficiency in search, making the process akin to finding a needle in
a haystack.
  To address these two limitations, we propose a novel method, named MGDE, to
detect error-inducing inputs based on mathematical guidance. By employing the
Newton-Raphson method, which exhibits quadratic convergence properties, we
achieve highly effective and efficient results. Since the goal of identifying
error-inducing inputs is to uncover the underlying bugs, we use the number of
bugs detected in floating-point programs as the primary evaluation metric in
our experiments. As FPCC represents the most effective state-of-the-art
approach to date, we use it as the baseline for comparison. The dataset of FPCC
consists of 88 single-input floating-point programs. FPCC is able to detect 48
bugs across 29 programs, whereas our method successfully identifies 89 bugs
across 44 programs. Moreover, FPCC takes 6.4096 times as long as our proposed
method. We also deploy our method to multi-input programs, identifying a total
of nine bugs with an average detection time of 0.6443 seconds per program. In
contrast, FPCC fails to detect any bugs while requiring an average computation
time of 100 seconds per program.

</details>


### [165] [IntrinTrans: LLM-based Intrinsic Code Translator for RISC-V Vector](https://arxiv.org/abs/2510.10119)
*Liutong Han,Zhiyuan Tan,Hongbin Zhang,Pengcheng Wang,Chu Kang,Mingjie Xing,Yanjun Wu*

Main category: cs.SE

TL;DR: 本文提出了一种基于大型语言模型（LLM）的多智能体方法IntrinTrans，用于自动将Arm Neon的向量内在函数代码翻译优化为RISC-V向量指令（RVV），并通过编译测试反馈和活跃度分析进行进一步优化，显著提高了翻译的自动化和性能表现。


<details>
  <summary>Details</summary>
Motivation: 随着RISC-V硬件生态系统的快速发展，支持RVV扩展的需求日益增长，但当前跨架构内在函数代码翻译依赖手工重写，效率低且易出错；规则基方法覆盖不全且性能受限，亟需自动且高效的翻译方法。

Method: 提出基于LLM的多智能体系统IntrinTrans，通过编译和测试反馈循环自动翻译向量内在函数代码，利用活跃度分析提取寄存器使用信息以优化RVV指令使用。

Result: 在34个开源库的向量算法案例中，IntrinTrans在大多数情况下能生成语义正确的RVV内在函数代码，部分情况性能提升最高达5.93倍。

Conclusion: 基于LLM的多智能体自动翻译方法有效解决了跨架构向量内在函数代码翻译难题，提升了自动化程度和最终性能，为RVV代码迁移提供实用方案。

Abstract: The use of intrinsic functions to exploit hardware-specific capabilities is
an important approach for optimizing library performance. Many mainstream
libraries implement a large number of vectorized algorithms on Arm or x86 SIMD
intrinsic functions. With the rapid expansion of the RISC-V hardware-software
ecosystem, there is a growing demand for support of the RISC-V Vector (RVV)
extension. Translating existing vectorized intrinsic code onto RVV intrinsics
is a practical and effective approach. However, current cross-architecture
translation largely relies on manual rewriting, which is time-consuming and
error-prone. Furthermore, while some rule-based methods can reduce the need for
manual intervention, their translation success rate is limited by incomplete
rule coverage and syntactic constraints, and the performance suffers from
inadequate utilization of RVV-specific features. We present IntrinTrans, a
LLM-based multi-agent approach that utilizes compile-and-test feedback to
translate intrinsic code across architectures automatically, and further
optimizes the generated RVV intrinsics using register-usage information derived
from liveness analysis. To evaluate the effectiveness of our approach, we
collected 34 vectorized algorithm cases from open-source libraries. Each case
includes an Arm Neon intrinsics implementation and a RVV intrinsics
implementation contributed by the open-source community, together with
correctness and performance tests. Our experiments show that advanced LLMs
produce semantically correct RISC-V Vector intrinsics in most cases within a
limited number of iterations, and in some cases achieve up to 5.93x the
performance of the native implementation from the open-source community.

</details>


### [166] [A Systematic Study on Generating Web Vulnerability Proof-of-Concepts Using Large Language Models](https://arxiv.org/abs/2510.10148)
*Mengyao Zhao,Kaixuan Li,Lyuye Zhang,Wenjing Dang,Chenggong Ding,Sen Chen,Zheli Liu*

Main category: cs.SE

TL;DR: 本文首次实证研究了基于大型语言模型（LLMs）在利用公开披露信息生成Web应用漏洞PoC的可行性，表明LLMs在8%-34%的案例中能自动生成有效PoC，且补充代码上下文和自适应推理策略显著提升成功率。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在代码理解和推理方面的突破，利用公开信息自动化生成漏洞利用PoC成为可能，这为漏洞理解和防护带来新机遇，但尚缺乏系统研究其实际效果。

Method: 本文选取100个真实且可复现的Web应用漏洞，评估GPT-4o和DeepSeek-R1两种LLMs在漏洞披露的三个阶段（仅描述、含补丁、含完整代码）自动生成PoC的能力，同时分析代码上下文层级和推理策略对结果的影响。

Result: LLMs利用公开信息在8%-34%案例生成有效PoC，DeepSeek-R1表现优于GPT-4o，补充代码上下文提升成功率17%-20%，函数级上下文优于文件级，结合自适应推理策略成功率提升至68%-72%，共23个新PoC已被NVD和Exploit DB收录。

Conclusion: 大型语言模型能有效利用公开漏洞信息自动生成PoC，提升漏洞利用自动化水平，可能改变漏洞利用态势，对软件安全具有重要影响。

Abstract: Recent advances in Large Language Models (LLMs) have brought remarkable
progress in code understanding and reasoning, creating new opportunities and
raising new concerns for software security. Among many downstream tasks,
generating Proof-of-Concept (PoC) exploits plays a central role in
vulnerability reproduction, comprehension, and mitigation. While previous
research has focused primarily on zero-day exploitation, the growing
availability of rich public information accompanying disclosed CVEs leads to a
natural question: can LLMs effectively use this information to automatically
generate valid PoCs? In this paper, we present the first empirical study of
LLM-based PoC generation for web application vulnerabilities, focusing on the
practical feasibility of leveraging publicly disclosed information. We evaluate
GPT-4o and DeepSeek-R1 on 100 real-world and reproducible CVEs across three
stages of vulnerability disclosure: (1) newly disclosed vulnerabilities with
only descriptions, (2) 1-day vulnerabilities with patches, and (3) N-day
vulnerabilities with full contextual code. Our results show that LLMs can
automatically generate working PoCs in 8%-34% of cases using only public data,
with DeepSeek-R1 consistently outperforming GPT-4o. Further analysis shows that
supplementing code context improves success rates by 17%-20%, with
function-level providing 9%-13% improvement than file-level ones. Further
integrating adaptive reasoning strategies to prompt refinement significantly
improves success rates to 68%-72%. Our findings suggest that LLMs could reshape
vulnerability exploitation dynamics. To date, 23 newly generated PoCs have been
accepted by NVD and Exploit DB.

</details>


### [167] [LLMs are All You Need? Improving Fuzz Testing for MOJO with Large Language Models](https://arxiv.org/abs/2510.10179)
*Linghan Huang,Peizhou Zhao,Huaming Chen*

Main category: cs.SE

TL;DR: 本文提出了MOJOFuzzer，一种基于大语言模型的自适应模糊测试框架，用于新兴语言MOJO的零样本学习环境。该框架通过多阶段过滤和动态提示改进测试用例的有效性，大幅提升模糊测试性能和缺陷检测能力。


<details>
  <summary>Details</summary>
Motivation: MOJO语言缺乏完善的测试框架和LLM测试语料，导致模型产生语法正确但语义错误的代码，影响模糊测试效果。

Method: MOJOFuzzer采用多阶段框架筛选低质量生成输入，基于运行时反馈动态调整LLM提示，实现测试用例的迭代优化。

Result: MOJOFuzzer在测试有效性、API覆盖率和漏洞检测性能上显著优于传统方法和现有LLM模糊测试技术。使用该框架对MOJO进行大规模测试，发现13个未知漏洞。

Conclusion: MOJOFuzzer推动了基于大模型的软件测试发展，为新兴语言测试提供了基础方法论。

Abstract: The rapid development of large language models (LLMs) has revolutionized
software testing, particularly fuzz testing, by automating the generation of
diverse and effective test inputs. This advancement holds great promise for
improving software reliability. Meanwhile, the introduction of MOJO, a
high-performance AI programming language blending Python's usability with the
efficiency of C and C++, presents new opportunities to enhance AI model
scalability and programmability. However, as a new language, MOJO lacks
comprehensive testing frameworks and a sufficient corpus for LLM-based testing,
which exacerbates model hallucination. In this case, LLMs will generate
syntactically valid but semantically incorrect code, significantly reducing the
effectiveness of fuzz testing. To address this challenge, we propose
MOJOFuzzer, the first adaptive LLM-based fuzzing framework designed for
zero-shot learning environments of emerging programming languages. MOJOFuzzer
integrates a mutil-phase framework that systematically eliminates low-quality
generated inputs before execution, significantly improving test case validity.
Furthermore, MOJOFuzzer dynamically adapts LLM prompts based on runtime
feedback for test case mutation, enabling an iterative learning process that
continuously enhances fuzzing efficiency and bug detection performance. Our
experimental results demonstrate that MOJOFuzzer significantly enhances test
validity, API coverage, and bug detection performance, outperforming
traditional fuzz testing and state-of-the-art LLM-based fuzzing approaches.
Using MOJOFuzzer, we have conducted a first large-scale fuzz testing evaluation
of MOJO, uncorvering 13 previous unknown bugs. This study not only advances the
field of LLM-driven software testing but also establishes a foundational
methodology for leveraging LLMs in the testing of emerging programming
languages.

</details>


### [168] [Grounded AI for Code Review: Resource-Efficient Large-Model Serving in Enterprise Pipelines](https://arxiv.org/abs/2510.10290)
*Sayan Mandal,Hua Jiang*

Main category: cs.SE

TL;DR: 本文提出了一种基于静态分析与抽象语法树上下文提取，结合单GPU量化模型和多级缓存的代码审查系统，实现了快速且准确的自动代码审查。


<details>
  <summary>Details</summary>
Motivation: 在合规要求严格的环境中，传统静态分析工具输出量大且缺乏合理解释，直接使用大模型存在幻觉和成本高的问题，亟需一种高效且具有解释性的代码审查解决方案。

Method: 构建一个面向Pull Request的自动审查系统，将静态分析结果与AST引导的上下文提取结合，使用量化的开源模型和多级缓存机制，实现低延迟、高质量的审查反馈。

Result: 系统在安全相关的C/C++标准下表现出大约1分钟以内的首条反馈时间，同时在违规率和违规减少方面优于更大的专有模型，用户调查显示人力复查次数减少。

Conclusion: 该架构可解耦切换，支持独立采用不同模块，提升代码审查效率与质量，具有可复现性和可审计性，未来可扩展至更多标准和辅助代码修补功能。

Abstract: Automated code review adoption lags in compliance-heavy settings, where
static analyzers produce high-volume, low-rationale outputs, and naive LLM use
risks hallucination and incurring cost overhead. We present a production system
for grounded, PR-native review that pairs static-analysis findings with
AST-guided context extraction and a single-GPU, on-demand serving stack
(quantized open-weight model, multi-tier caching) to deliver concise
explanations and remediation guidance. Evaluated on safety-oriented C/C++
standards, the approach achieves sub-minute median first-feedback (offline p50
build+LLM 59.8s) while maintaining competitive violation reduction and lower
violation rates versus larger proprietary models. The architecture is
decoupled: teams can adopt the grounding/prompting layer or the serving layer
independently. A small internal survey (n=8) provides directional signals of
reduced triage effort and moderate perceived grounding, with participants
reporting fewer human review iterations. We outline operational lessons and
limitations, emphasizing reproducibility, auditability, and pathways to broader
standards and assisted patching.

</details>


### [169] [Prepared for the Unknown: Adapting AIOps Capacity Forecasting Models to Data Changes](https://arxiv.org/abs/2510.10320)
*Lorena Poenaru-Olaru,Wouter van 't Hof,Adrian Stando,Arkadiusz P. Trawinski,Eileen Kapel,Jan S. Rellermeyer,Luis Cruz,Arie van Deursen*

Main category: cs.SE

TL;DR: 本文研究了基于数据变化检测进行容量预测模型重训练的方法，发现该方法在多数情况下能达到与定期重训练相当的预测精度，且更节省计算资源。


<details>
  <summary>Details</summary>
Motivation: 容量管理中，容量预测模型需要频繁重训练以适应数据变化，但频繁重训练成本高且难以扩展，因此探索更高效的重训练策略具有实际意义。

Method: 通过对比基于数据变化检测的重训练与定期重训练两种方法，对容量预测时间序列模型的性能进行了实证研究。

Result: 研究发现基于数据变化检测的重训练在大多数情况下能实现与定期重训练相当的预测精度，且节省计算资源；但在数据快速变化时，定期重训练仍有优势。

Conclusion: 基于数据变化的重训练策略是一种成本效益高的容量预测模型维护方法，适合大多数软件组织使用，但在快速变化环境中仍需定期重训练以保证准确性。

Abstract: Capacity management is critical for software organizations to allocate
resources effectively and meet operational demands. An important step in
capacity management is predicting future resource needs often relies on
data-driven analytics and machine learning (ML) forecasting models, which
require frequent retraining to stay relevant as data evolves. Continuously
retraining the forecasting models can be expensive and difficult to scale,
posing a challenge for engineering teams tasked with balancing accuracy and
efficiency. Retraining only when the data changes appears to be a more
computationally efficient alternative, but its impact on accuracy requires
further investigation. In this work, we investigate the effects of retraining
capacity forecasting models for time series based on detected changes in the
data compared to periodic retraining. Our results show that drift-based
retraining achieves comparable forecasting accuracy to periodic retraining in
most cases, making it a cost-effective strategy. However, in cases where data
is changing rapidly, periodic retraining is still preferred to maximize the
forecasting accuracy. These findings offer actionable insights for software
teams to enhance forecasting systems, reducing retraining overhead while
maintaining robust performance.

</details>


### [170] [Bridging Semantics & Structure for Software Vulnerability Detection using Hybrid Network Models](https://arxiv.org/abs/2510.10321)
*Jugal Gajjar,Kaustik Ranaware,Kamalasankari Subramaniakuppusamy*

Main category: cs.SE

TL;DR: 本文提出了一种结合图表示与轻量级局部大语言模型的混合框架，用于软件漏洞检测，实现了高准确率并增强了结果可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有静态和动态分析方法往往忽略了软件的结构依赖关系，限制了对不安全行为的识别能力。

Method: 将程序视为异构图，捕捉控制流和数据流关系，结合轻量级本地大语言模型处理拓扑特征与语义推理。

Result: 在Java漏洞检测任务中，方法准确率达93.57%，显著优于基于图注意力网络的嵌入和Qwen2.5 Coder 3B等预训练大语言模型。

Conclusion: 此方法实现了可扩展、可解释且本地部署的漏洞检测工具，促进漏洞分析从语法检查向结构和语义深度洞察转变，有助于实际安全软件开发的广泛应用。

Abstract: Software vulnerabilities remain a persistent risk, yet static and dynamic
analyses often overlook structural dependencies that shape insecure behaviors.
Viewing programs as heterogeneous graphs, we capture control- and data-flow
relations as complex interaction networks. Our hybrid framework combines these
graph representations with light-weight (<4B) local LLMs, uniting topological
features with semantic reasoning while avoiding the cost and privacy concerns
of large cloud models. Evaluated on Java vulnerability detection (binary
classification), our method achieves 93.57% accuracy-an 8.36% gain over Graph
Attention Network-based embeddings and 17.81% over pretrained LLM baselines
such as Qwen2.5 Coder 3B. Beyond accuracy, the approach extracts salient
subgraphs and generates natural language explanations, improving
interpretability for developers. These results pave the way for scalable,
explainable, and locally deployable tools that can shift vulnerability analysis
from purely syntactic checks to deeper structural and semantic insights,
facilitating broader adoption in real-world secure software development.

</details>


### [171] [Testing and Enhancing Multi-Agent Systems for Robust Code Generation](https://arxiv.org/abs/2510.10460)
*Zongyi Lyu,Songqiang Chen,Zhenlan Ji,Liwen Wang,Shuai Wang,Daoyuan Wu,Wenxuan Wang,Shing-Chi Cheung*

Main category: cs.SE

TL;DR: 本文首次系统研究了多智能体系统（MAS）在代码生成任务中的鲁棒性，发现其存在显著缺陷并提出修复方法。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统虽然在代码生成表现优异，但其鲁棒性未被充分探讨，存在实际部署的风险。

Method: 文章通过设计基于模糊测试的测试方案，利用语义保留的变异操作和新颖的适应度函数，对主流MAS进行多数据集和大型语言模型评估，发现鲁棒性缺陷，并提出多提示生成及新监控智能体的修复方法。

Result: 研究发现多数MAS在经过语义保留变异后会解决失败比例高达7.9%-83.3%，且通过修复方法可解决40.0%-88.9%的失败案例，有效提升鲁棒性。

Conclusion: 本文揭示了MAS在代码生成中的关键鲁棒性缺陷，尤其是智能体间的沟通不畅问题，并提供了有效的修复策略，为开发更可靠的MAS提供了重要参考。

Abstract: Multi-agent systems (MASs) have emerged as a promising paradigm for automated
code generation, demonstrating impressive performance on established benchmarks
by decomposing complex coding tasks across specialized agents with different
roles. Despite their prosperous development and adoption, their robustness
remains pressingly under-explored, raising critical concerns for real-world
deployment. This paper presents the first comprehensive study examining the
robustness of MASs for code generation through a fuzzing-based testing
approach. By designing a fuzzing pipeline incorporating semantic-preserving
mutation operators and a novel fitness function, we assess mainstream MASs
across multiple datasets and LLMs. Our findings reveal substantial robustness
flaws of various popular MASs: they fail to solve 7.9%-83.3% of problems they
initially resolved successfully after applying the semantic-preserving
mutations. Through comprehensive failure analysis, we identify a common yet
largely overlooked cause of the robustness issue: miscommunications between
planning and coding agents, where plans lack sufficient detail and coding
agents misinterpret intricate logic, aligning with the challenges inherent in a
multi-stage information transformation process. Accordingly, we also propose a
repairing method that encompasses multi-prompt generation and introduces a new
monitor agent to address this issue. Evaluation shows that our repairing method
effectively enhances the robustness of MASs by solving 40.0%-88.9% of
identified failures. Our work uncovers critical robustness flaws in MASs and
provides effective mitigation strategies, contributing essential insights for
developing more reliable MASs for code generation.

</details>


### [172] [How Students Use Generative AI for Software Testing: An Observational Study](https://arxiv.org/abs/2510.10551)
*Baris Ardic,Quentin Le Dilavrec,Andy Zaidman*

Main category: cs.SE

TL;DR: 研究了初学者如何利用生成式AI工具ChatGPT进行单元测试工程，分析其交互策略和感知效果。


<details>
  <summary>Details</summary>
Motivation: 生成式AI工具在软件工程测试中应用日益增多，尤其是对初学者，了解其使用策略和感知的利弊至关重要。

Method: 通过观察12名本科生使用生成式AI辅助进行单元测试，分析其交互策略、提示风格及使用感知。

Result: 识别了4种交互策略和两种提示风格，学生认为AI节省时间、减少认知负担，但也存在信任和质量担忧。策略和提示风格未显著影响测试质量。

Conclusion: 生成式AI辅助的测试工程虽然改变开发者角色，但合理使用可支持初学者测试开发，未来需关注信任和质量问题。

Abstract: The integration of generative AI tools like ChatGPT into software engineering
workflows opens up new opportunities to boost productivity in tasks such as
unit test engineering. However, these AI-assisted workflows can also
significantly alter the developer's role, raising concerns about control,
output quality, and learning, particularly for novice developers. This study
investigates how novice software developers with foundational knowledge in
software testing interact with generative AI for engineering unit tests. Our
goal is to examine the strategies they use, how heavily they rely on generative
AI, and the benefits and challenges they perceive when using generative
AI-assisted approaches for test engineering. We conducted an observational
study involving 12 undergraduate students who worked with generative AI for
unit testing tasks. We identified four interaction strategies, defined by
whether the test idea or the test implementation originated from generative AI
or the participant. Additionally, we singled out prompting styles that focused
on one-shot or iterative test generation, which often aligned with the broader
interaction strategy. Students reported benefits including time-saving, reduced
cognitive load, and support for test ideation, but also noted drawbacks such as
diminished trust, test quality concerns, and lack of ownership. While strategy
and prompting styles influenced workflow dynamics, they did not significantly
affect test effectiveness or test code quality as measured by mutation score or
test smells.

</details>


### [173] [Generative AI and the Transformation of Software Development Practices](https://arxiv.org/abs/2510.10819)
*Vivek Acharya*

Main category: cs.SE

TL;DR: 生成式人工智能正改变软件设计、编写和维护方式，促进开发效率提升和普及。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI如何改变软件工程实践，尤其在信任、责任和技能转变方面的问题。

Method: 通过调查基于对话的迭代开发、多代理系统、动态提示协同和模型上下文协议的整合，结合案例研究和行业数据进行分析。

Result: 揭示使用生成式AI带来的机会（如更快的开发周期和编码民主化）和挑战（如模型可靠性及成本问题）。

Conclusion: 提出了利用AI进行负责任且有效编码的新角色、新技能和最佳实践建议。

Abstract: Generative AI is reshaping how software is designed, written, and maintained.
Advances in large language models (LLMs) are enabling new development styles -
from chat-oriented programming and 'vibe coding' to agentic programming - that
can accelerate productivity and broaden access. This paper examines how
AI-assisted techniques are changing software engineering practice, and the
related issues of trust, accountability, and shifting skills. We survey
iterative chat-based development, multi-agent systems, dynamic prompt
orchestration, and integration via the Model Context Protocol (MCP). Using case
studies and industry data, we outline both the opportunities (faster cycles,
democratized coding) and the challenges (model reliability and cost) of
applying generative AI to coding. We describe new roles, skills, and best
practices for using AI in a responsible and effective way.

</details>


### [174] [Agentic RAG for Software Testing with Hybrid Vector-Graph and Multi-Agent Orchestration](https://arxiv.org/abs/2510.10824)
*Mohanakrishnan Hariharan,Satish Arvapalli,Seshu Barma,Evangeline Sheela*

Main category: cs.SE

TL;DR: 本文提出了一种基于Agentic RAG系统的软件测试自动化方法，通过结合自主AI代理和混合向量图知识系统，实现测试计划、测试用例以及质量工程指标的自动生成。


<details>
  <summary>Details</summary>
Motivation: 传统软件测试存在效率低、精度不高及追踪性差等问题，亟需通过先进技术提升自动化程度和测试效果。

Method: 利用包括Gemini和Mistral在内的大型语言模型，结合多代理协同和增强上下文技术，构建了自主AI代理和混合向量图知识系统相结合的测试自动化方案。

Result: 系统准确率从65%提升到94.8%，实现了测试计划全生命周期文档追踪。企业级项目测试周期缩短85%，测试套件效率提升85%，成本预计节约35%，项目上线时间提前2个月。

Conclusion: 该方法显著提升了软件测试的自动化水平和效率，降低了成本并加快了项目上线，具有重要的应用价值和推广前景。

Abstract: We present an approach to software testing automation using Agentic
Retrieval-Augmented Generation (RAG) systems for Quality Engineering (QE)
artifact creation. We combine autonomous AI agents with hybrid vector-graph
knowledge systems to automate test plan, case, and QE metric generation. Our
approach addresses traditional software testing limitations by leveraging LLMs
such as Gemini and Mistral, multi-agent orchestration, and enhanced
contextualization. The system achieves remarkable accuracy improvements from
65% to 94.8% while ensuring comprehensive document traceability throughout the
quality engineering lifecycle. Experimental validation of enterprise Corporate
Systems Engineering and SAP migration projects demonstrates an 85% reduction in
testing timeline, an 85% improvement in test suite efficiency, and projected
35% cost savings, resulting in a 2-month acceleration of go-live.

</details>


### [175] [Software Defect Prediction using Autoencoder Transformer Model](https://arxiv.org/abs/2510.10840)
*Seshu Barma,Mohanakrishnan Hariharan,Satish Arvapalli*

Main category: cs.SE

TL;DR: 提出了一种基于自适应差分进化算法优化的量子变分自动编码器-变压器模型（ADE-QVAET），用于提升软件缺陷预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型在处理噪声数据、不平衡数据、模式识别、特征提取和泛化能力方面存在不足，亟需更有效的质量工程解决方案。

Method: 提出ADE-QVAET模型，结合自适应差分进化算法（ADE）和量子变分自动编码器-变压器（QVAET），以提取高维潜在特征和保持序列依赖性，并通过ADE优化模型超参数，实现更好的收敛性和预测性能。

Result: 在使用90%训练数据训练时，ADE-QVAET模型在准确率（98.08%）、精确率（92.45%）、召回率（94.67%）和F1值（98.12%）方面均优于传统差分进化（DE）模型。

Conclusion: ADE-QVAET模型作为一种AI-ML驱动的技术，显著提升了软件缺陷预测的准确性和质量工程的效果，具备良好的扩展性和实用价值。

Abstract: An AI-ML-powered quality engineering approach uses AI-ML to enhance software
quality assessments by predicting defects. Existing ML models struggle with
noisy data types, imbalances, pattern recognition, feature extraction, and
generalization. To address these challenges, we develop a new model, Adaptive
Differential Evolution (ADE) based Quantum Variational Autoencoder-Transformer
(QVAET) Model (ADE-QVAET). ADE combines with QVAET to obtain high-dimensional
latent features and maintain sequential dependencies, resulting in enhanced
defect prediction accuracy. ADE optimization enhances model convergence and
predictive performance. ADE-QVAET integrates AI-ML techniques such as tuning
hyperparameters for scalable and accurate software defect prediction,
representing an AI-ML-driven technology for quality engineering. During
training with a 90% training percentage, ADE-QVAET achieves high accuracy,
precision, recall, and F1-score of 98.08%, 92.45%, 94.67%, and 98.12%,
respectively, when compared to the Differential Evolution (DE) ML model.

</details>


### [176] [Generative AI for Software Project Management: Insights from a Review of Software Practitioner Literature](https://arxiv.org/abs/2510.10887)
*Lakshana Iruni Assalaarachchi,Zainab Masood,Rashina Hoda,John Grundy*

Main category: cs.SE

TL;DR: 软件项目经理视生成式人工智能(GenAI)为协助工具，能自动化常规任务和提升项目成功率，但需注意伦理和情感判断。


<details>
  <summary>Details</summary>
Motivation: 了解软件项目管理中生成式人工智能的应用现状及影响。

Method: 通过47个公开的博客、文章和行业报告进行灰色文献回顾。

Result: 软件项目经理将GenAI视为“助手”，支持任务自动化、预测分析等，同时关注伦理隐私和情感智能不足。

Conclusion: 提出软件项目经理在GenAI时代的技能提升需求和实践、研究建议。

Abstract: Software practitioners are discussing GenAI transformations in software
project management openly and widely. To understand the state of affairs, we
performed a grey literature review using 47 publicly available practitioner
sources including blogs, articles, and industry reports. We found that software
project managers primarily perceive GenAI as an "assistant", "copilot", or
"friend" rather than as a "PM replacement", with support of GenAI in automating
routine tasks, predictive analytics, communication and collaboration, and in
agile practices leading to project success. Practitioners emphasize responsible
GenAI usage given concerns such as hallucinations, ethics and privacy, and lack
of emotional intelligence and human judgment. We present upskilling
requirements for software project managers in the GenAI era mapped to the
Project Management Institute's talent triangle. We share key recommendations
for both practitioners and researchers.

</details>


### [177] [Project-Level C-to-Rust Translation via Synergistic Integration of Knowledge Graphs and Large Language Models](https://arxiv.org/abs/2510.10956)
*Zhiqiang Yuan,Wenjun Mao,Zhuo Chen,Xiyue Shang,Chong Wang,Yiling Lou,Xin Peng*

Main category: cs.SE

TL;DR: 本论文提出了基于指针知识图的C到Rust代码翻译方法，有效提升了Rust代码的安全性与正确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型(LLM)的方法在项目级别的C到Rust自动翻译中不足，尤其在指针的全局使用理解上表现欠佳，导致生成代码不够安全。

Method: 提出了一种结合指针语义的指针知识图(C-Rust Pointer Knowledge Graph)，包括指针使用信息和Rust相关注解，辅助LLM从全局视角理解指针，并基于此设计了C-to-Rust翻译工具\ourtool{}。

Result: 实验表明，\ourtool{}在翻译生成Rust代码中将不安全用法减少了99.9%，且在功能正确性上比传统规则和LLM翻译方法提升了29.3%。

Conclusion: 通过引入全局指针知识图辅助LLM，\ourtool{}显著提升了项目级C到Rust翻译的安全性和准确性，展现了指针语义知识整合对代码翻译的有效性。

Abstract: Translating C code into safe Rust is an effective way to ensure its memory
safety. Compared to rule-based translation which produces Rust code that
remains largely unsafe, LLM-based methods can generate more idiomatic and safer
Rust code because LLMs have been trained on vast amount of human-written
idiomatic code. Although promising, existing LLM-based methods still struggle
with project-level C-to-Rust translation. They typically partition a C project
into smaller units (\eg{} functions) based on call graphs and translate them
bottom-up to resolve program dependencies. However, this bottom-up,
unit-by-unit paradigm often fails to translate pointers due to the lack of a
global perspective on their usage. To address this problem, we propose a novel
C-Rust Pointer Knowledge Graph (KG) that enriches a code-dependency graph with
two types of pointer semantics: (i) pointer-usage information which record
global behaviors such as points-to flows and map lower-level struct usage to
higher-level units; and (ii) Rust-oriented annotations which encode ownership,
mutability, nullability, and lifetime. Synthesizing the \kg{} with LLMs, we
further propose \ourtool{}, which implements a project-level C-to-Rust
translation technique. In \ourtool{}, the \kg{} provides LLMs with
comprehensive pointer semantics from a global perspective, thus guiding LLMs
towards generating safe and idiomatic Rust code from a given C project. Our
experiments show that \ourtool{} reduces unsafe usages in translated Rust by
99.9\% compared to both rule-based translation and traditional LLM-based
rewriting, while achieving an average 29.3\% higher functional correctness than
those fuzzing-enhanced LLM methods.

</details>


### [178] [RepoSummary: Feature-Oriented Summarization and Documentation Generation for Code Repositories](https://arxiv.org/abs/2510.11039)
*Yifeng Zhu,Xianlin Zhao,Xutian Li,Yanzhen Zou,Haizhuo Yuan,Yue Wang,Bing Xie*

Main category: cs.SE

TL;DR: 提出了一个名为RepoSummary的面向特性的代码仓库总结方法，提升了功能覆盖率和追踪准确性。


<details>
  <summary>Details</summary>
Motivation: 现有代码仓库总结技术主要基于目录树，难以有效追踪高层功能到实现方法，影响代码理解和维护。

Method: 开发了RepoSummary，自动生成仓库文档，并建立功能特性与代码元素之间更准确的追踪链接。

Result: 实验显示RepoSummary在功能覆盖率、文件级追踪召回率及文档质量均优于现有方法，功能覆盖率由61.2%提升至71.1%，文件级追踪召回率由29.9%提升至53.0%。

Conclusion: RepoSummary方法显著提升了仓库总结的功能覆盖和代码追踪能力，有助于加速开发者代码理解与维护过程。

Abstract: Repository summarization is a crucial research question in development and
maintenance for software engineering. Existing repository summarization
techniques primarily focus on summarizing code according to the directory tree,
which is insufficient for tracing high-level features to the methods that
collaboratively implement them. To address these limitations, we propose
RepoSummary, a feature-oriented code repository summarization approach that
simultaneously generates repository documentation automatically. Furthermore,
it establishes more accurate traceability links from functional features to the
corresponding code elements, enabling developers to rapidly locate relevant
methods and files during code comprehension and maintenance. Comprehensive
experiments against the state-of-the-art baseline (HGEN) demonstrate that
RepoSummary achieves higher feature coverage and more accurate traceability. On
average, it increases the rate of completely covered features in manual
documentation from 61.2% to 71.1%, improves file-level traceability recall from
29.9% to 53.0%, and generates documentation that is more conceptually
consistent, easier to understand, and better formatted than that produced by
existing approaches.

</details>


### [179] [Defects4C: Benchmarking Large Language Model Repair Capability with C/C++ Bugs](https://arxiv.org/abs/2510.11059)
*Jian Wang,Xiaofei Xie,Qiang Hu,Shangqing Liu,Jiongchi Yu,Jiaolong Klong,Yi Li*

Main category: cs.SE

TL;DR: 本文介绍了专为C/C++程序修复设计的Defects4C基准库，并基于其对24种大型语言模型的修复效果进行了评估。


<details>
  <summary>Details</summary>
Motivation: C/C++广泛使用且存在大量漏洞，但缺乏高质量的开源修复基准数据，阻碍了相关自动程序修复研究。

Method: 构建了包含大量真实C/C++项目中bug相关提交、高质量欠缺函数和漏洞函数的Defects4C基准库，配套测试用例以支持修复技术的评估和学习模型的重训练。

Result: 基于Defects4C，系统评估了24个当前领先大型语言模型在修复C/C++错误的表现，揭示了现有方法的优劣和不足。

Conclusion: Defects4C填补了C/C++自动修复基准的空白，为提升学习型修复方法和未来研究提供了关键资源和指导。

Abstract: Automated Program Repair (APR) plays a critical role in enhancing the quality
and reliability of software systems. While substantial progress has been made
in Java-based APR, largely facilitated by benchmarks like Defects4J, there
remains a significant gap in research on C/C++ program repair, despite the
widespread use of C/C++ and the prevalence of associated vulnerabilities. This
gap is primarily due to the lack of high-quality, open-source benchmarks
tailored for C/C++.
  To address this issue, we introduce Defects4C, a comprehensive and executable
benchmark specifically designed for C/C++ program repair. Our dataset is
constructed from real-world C/C++ repositories and includes a large collection
of bug-relevant commits (9M in total), 248 high-quality buggy functions, and
102 vulnerable functions, all paired with test cases for reproduction. These
resources enable rigorous evaluation of repair techniques and support the
retraining of learning-based approaches for enhanced performance.
  Using Defects4C, we conduct a comprehensive empirical study evaluating the
effectiveness of 24 state-of-the-art large language models (LLMs) in repairing
C/C++ faults. Our findings offer valuable insights into the strengths and
limitations of current LLM-based APR techniques in this domain, highlighting
both the need for more robust methods and the critical role of Defects4C in
advancing future research

</details>


### [180] [DebugTA: An LLM-Based Agent for Simplifying Debugging and Teaching in Programming Education](https://arxiv.org/abs/2510.11076)
*Lingyue Fu,Haowei Yuan,Datong Chen,Xinyi Dai,Qingyao Li,Weinan Zhang,Weiwen Liu,Yong Yu*

Main category: cs.SE

TL;DR: 本文提出DebugTA，一种基于大语言模型的编程调试与教学代理，通过专门工具简化多源输入的推理，提高代码修改建议的准确性和教学效果。


<details>
  <summary>Details</summary>
Motivation: 程序调试和教学任务涉及复杂多样的输入，现有方法难以有效利用标准代码，导致推理复杂、效果有限。

Method: DebugTA设计了标准代码检索、变量替换与外部编译器工具，分步骤调用大模型处理子任务，简化推理，聚焦逻辑错误。

Result: 在三个真实代码数据集上，DebugTA显著提升教学效果，同时大幅降低计算成本。

Conclusion: DebugTA通过工具辅助和分步推理有效解决了调试教学任务中的复杂性和信息异质性，展现了良好的实际应用价值。

Abstract: In programming education, Debugging and Teaching (DT) task is a common
scenario where students receive assistance in correcting their erroneous code.
The task involves multiple inputs, including erroneous code, error messages,
reference solutions, and the question description, with the goal of generating
modification suggestions to the erroneous code. However, two key challenges
hinder the effectiveness of existing approaches. Firstly, the complexity and
heterogeneity of inputs inherent in DT tasks significantly elevate the
reasoning challenges faced by LLMs. Second, existing approaches often fail to
fully leverage the availability of standard code in DT tasks, forcing models to
rely solely on complex multi-step reasoning, which limits the potential of LLMs
in addressing DT tasks effectively. To address these challenges, we propose
DebugTA, a novel LLM-based debugging and teaching agent with specialized tools
for standard code retrieval, variable substitution to align reference code, and
an external compiler for real-time code analysis. Guided by explicit
pedagogical and debugging principles, DebugTA acts as an agent that decomposes
a complex task into sequential LLM interactions, each utilizing distinct tools
for specific subtasks, thereby simplifying the logical reasoning at each step
and reducing overall reasoning complexity. Furthermore, DebugTA utilizes tool
calls to align the standard code with the erroneous code as much as possible,
allowing the LLM to focus on logic errors within the erroneous code and
improving the accuracy of the generated suggestions. To rigorously assess the
quality of modification suggestions, we introduce a student simulator-teacher
interaction paradigm. Experimental results on three real-world code datasets
demonstrate that DebugTA consistently improves teaching effectiveness while
significantly reducing computational costs.

</details>


### [181] [What Slows Down FMware Development? An Empirical Study of Developer Challenges and Resolution Times](https://arxiv.org/abs/2510.11138)
*Zitao Wang,Zhimin Zhao,Michael W. Godfrey*

Main category: cs.SE

TL;DR: 本研究首次大规模分析了基于基础模型(FMs)的软件(FMware)开发，发现其主要应用于教育、内容创作和商业策略，但面临内存管理、依赖处理和分词配置等技术挑战。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型如GPT的广泛应用，基于这些模型的软件(FMware)开发迅速增长，但其设计与传统软件开发有显著差异，存在许多新挑战，亟需系统性研究以指导实践。

Method: 通过收集和分析GitHub及主要FMware平台（如HuggingFace、GPTStore等）上的开源项目数据，结合实际开发者面临的挑战和问题类型，进行了大规模实证研究。

Result: 发现FMware应用集中在教育、内容创作和商业策略，技术问题主要为内存管理、依赖处理和分词配置等；GitHub上错误报告和核心功能问题最常见，代码审查、相似性搜索和提示模板设计最耗时。

Conclusion: 研究揭示了FMware开发中的关键实践和痛点，指出改进工具、流程和社区支持的方向，为未来FMware开发提供了重要参考和指导。

Abstract: Foundation Models (FMs), such as OpenAI's GPT, are fundamentally transforming
the practice of software engineering by enabling the development of
\emph{FMware} -- applications and infrastructures built around these models.
FMware systems now support tasks such as code generation, natural-language
interaction, knowledge integration, and multi-modal content creation,
underscoring their disruptive impact on current software engineering workflows.
However, the design, implementation, and evolution of FMware present
significant new challenges, particularly across cloud-based and on-premise
platforms where goals, processes, and tools often diverge from those of
traditional software development.
  To our knowledge, this is the first large-scale analysis of FMware
development across both cloud-based platforms and open-source repositories. We
empirically investigate the FMware ecosystem through three focus areas: (1) the
most common application domains of FMware, (2) the key challenges developers
encounter, and (3) the types of issues that demand the greatest effort to
resolve. Our analysis draws on data from GitHub repositories and from leading
FMware platforms, including HuggingFace, GPTStore, Ora, and Poe. Our findings
reveal a strong focus on education, content creation, and business strategy,
alongside persistent technical challenges in memory management, dependency
handling, and tokenizer configuration. On GitHub, bug reports and core
functionality issues are the most frequently reported problems, while code
review, similarity search, and prompt template design are the most
time-consuming to resolve.
  By uncovering developer practices and pain points, this study points to
opportunities to improve FMware tools, workflows, and community support, and
provides actionable insights to help guide the future of FMware development.

</details>


### [182] [Interoperability From OpenTelemetry to Kieker: Demonstrated as Export from the Astronomy Shop](https://arxiv.org/abs/2510.11179)
*David Georg Reichelt,Shinhyung Yang,Wilhelm Hasselbring*

Main category: cs.SE

TL;DR: 本文介绍了如何将OpenTelemetry的追踪数据转换为Kieker框架，以扩展Kieker对更多编程语言的支持。


<details>
  <summary>Details</summary>
Motivation: Kieker框架支持的编程语言有限，而OpenTelemetry标准支持更广泛的语言，如何结合两者以扩大Kieker的适用范围成为研究动机。

Method: 本文提出了一种方法，将OpenTelemetry的追踪数据转换为Kieker框架的数据格式，从而实现对更多语言（如C#和JavaScript）的支持。

Result: 通过将OpenTelemetry的追踪数据成功转换并在天文商城（一个OpenTelemetry演示应用）中展示调用树，验证了方法的可用性。

Conclusion: 将OpenTelemetry与Kieker结合，可以显著扩展Kieker的应用范围，支持更多编程语言和技术，提升其分析能力。

Abstract: The observability framework Kieker provides a range of analysis capabilities,
but it is currently only able to instrument a smaller selection of languages
and technologies, including Java, C, Fortran, and Python. The OpenTelemetry
standard aims for providing reference implementations for most programming
languages, including C# and JavaScript, that are currently not supported by
Kieker. In this work, we describe how to transform OpenTelemetry tracing data
into the Kieker framework. Thereby, it becomes possible to create for example
call trees from OpenTelemetry instrumentations. We demonstrate the usability of
our approach by visualizing trace data of the Astronomy Shop, which is an
OpenTelemetry demo application.

</details>


### [183] [Detection of Performance Changes in MooBench Results Using Nyrkiö on GitHub Actions](https://arxiv.org/abs/2510.11310)
*Shinhyung Yang,David Georg Reichelt,Henrik Ingo,Wilhelm Hasselbring*

Main category: cs.SE

TL;DR: 本文将性能变化检测集成到GitHub连续集成环境，成功检测到并复现了性能回归问题，并定位其原因。


<details>
  <summary>Details</summary>
Motivation: 在GitHub上有大量项目，其性能变化对用户至关重要，但现有的性能测量缺乏变化检测功能。

Method: 通过将Nyrkiö性能变化检测服务与MooBench性能测量工具结合，实现性能数据上传与变化检测。

Result: 成功检测到一次重大性能回归，利用GitHub Actions复现该问题，并发现其由Linux内核版本变更引起。

Conclusion: 集成性能变化检测能力提升了性能监控效果，有助于及时发现和定位性能问题。

Abstract: In GitHub with its 518 million hosted projects, performance changes within
these projects are highly relevant to the project's users. Although performance
measurement is supported by GitHub CI/CD, performance change detection is a
challenging topic.
  In this paper, we demonstrate how we incorporated Nyrki\"o to MooBench. Prior
to this work, Moobench continuously ran on GitHub virtual machines, measuring
overhead of tracing agents, but without change detection. By adding the upload
of the measurements to the Nyrki\"o change detection service, we made it
possible to detect performance changes. We identified one major performance
regression and examined the performance change in depth. We report that (1) it
is reproducible with GitHub actions, and (2) the performance regression is
caused by a Linux Kernel version change.

</details>


### [184] [Cracking CodeWhisperer: Analyzing Developers' Interactions and Patterns During Programming Tasks](https://arxiv.org/abs/2510.11516)
*Jeena Javahar,Tanya Budhrani,Manaal Basha,Cleidson R. B. de Souza,Ivan Beschastnikh,Gema Rodriguez-Perez*

Main category: cs.SE

TL;DR: 本文研究了开发者如何使用亚马逊的基于大语言模型的代码生成工具CodeWhisperer，通过两次用户研究和混合方法分析，识别出了四种开发者行为模式。


<details>
  <summary>Details</summary>
Motivation: 随着AI代码生成工具的普及，理解开发者如何采用这些工具变得越来越重要。

Method: 通过两次用户研究（各10名参与者），一次确定关键交互，另一次利用定制遥测插件收集低层交互数据，结合定性和定量分析识别开发者行为模式。

Result: 发现了四种行为模式：增量代码优化、用自然语言注释给出明确指令、基于模型建议的基础结构构建，以及与外部资源的整合使用。

Conclusion: 本文系统分析了开发者与CodeWhisperer交互的方式，为理解AI辅助编码行为提供了实证支持。

Abstract: The use of AI code-generation tools is becoming increasingly common, making
it important to understand how software developers are adopting these tools. In
this study, we investigate how developers engage with Amazon's CodeWhisperer,
an LLM-based code-generation tool. We conducted two user studies with two
groups of 10 participants each, interacting with CodeWhisperer - the first to
understand which interactions were critical to capture and the second to
collect low-level interaction data using a custom telemetry plugin. Our
mixed-methods analysis identified four behavioral patterns: 1) incremental code
refinement, 2) explicit instruction using natural language comments, 3)
baseline structuring with model suggestions, and 4) integrative use with
external sources. We provide a comprehensive analysis of these patterns .

</details>


### [185] [CodeWatcher: IDE Telemetry Data Extraction Tool for Understanding Coding Interactions with LLMs](https://arxiv.org/abs/2510.11536)
*Manaal Basha,Aimeê M. Ribeiro,Jeena Javahar,Cleidson R. B. de Souza,Gema Rodríguez-Pérez*

Main category: cs.SE

TL;DR: 本文介绍了CodeWatcher，一个用于捕捉开发者与代码生成工具交互的轻量级系统，通过无侵入方式记录细粒度编程行为数据，支持后续行为分析。


<details>
  <summary>Details</summary>
Motivation: 现有研究难以在不干扰开发者工作流程的情况下收集详细的编程行为数据。

Method: 设计并实现了CodeWatcher系统，包括VS Code插件、Python RESTful API和MongoDB后端，容器化部署，实现对代码编辑中插入、删除、复制粘贴和焦点变化等有意义事件的实时记录。

Result: 系统能够结构化、带时间戳地记录事件，实现编码会话的重建和丰富的行为分析，特别是对代码生成工具使用情况的监控。

Conclusion: CodeWatcher为负责的AI研究、开发者生产力提升和以人为中心的代码生成工具评估提供了关键的数据基础设施。

Abstract: Understanding how developers interact with code generation tools (CGTs)
requires detailed, real-time data on programming behavior which is often
difficult to collect without disrupting workflow. We present
\textit{CodeWatcher}, a lightweight, unobtrusive client-server system designed
to capture fine-grained interaction events from within the Visual Studio Code
(VS Code) editor. \textit{CodeWatcher} logs semantically meaningful events such
as insertions made by CGTs, deletions, copy-paste actions, and focus shifts,
enabling continuous monitoring of developer activity without modifying user
workflows. The system comprises a VS Code plugin, a Python-based RESTful API,
and a MongoDB backend, all containerized for scalability and ease of
deployment. By structuring and timestamping each event, \textit{CodeWatcher}
enables post-hoc reconstruction of coding sessions and facilitates rich
behavioral analyses, including how and when CGTs are used during development.
This infrastructure is crucial for supporting research on responsible AI,
developer productivity, and the human-centered evaluation of CGTs. Please find
the demo, diagrams, and tool here: https://osf.io/j2kru/overview.

</details>


### [186] [Automatically Generating Questions About Scratch Programs](https://arxiv.org/abs/2510.11658)
*Florian Obermüller,Gordon Fraser*

Main category: cs.SE

TL;DR: 本文提出了一种自动生成面向Scratch编程代码的问题的方法，以评估学生的程序理解能力。


<details>
  <summary>Details</summary>
Motivation: 单纯完成编程任务并不能保证学生理解底层概念，针对学生代码提出问题可以更好地评估其理解情况，但人工设计问题繁琐且有挑战性。

Method: 提出30种基于既有程序理解模型的问题类型，扩展LitterBox静态分析工具，自动生成针对Scratch程序的问题，基于60万多个项目生成超过5400万个问题。

Result: 在34名九年级学生中的初步实验表明该方法能有效生成有意义的针对Scratch代码的问题，学生答题能力与其整体表现相关。

Conclusion: 自动生成的问题能够较准确地反映学生对Scratch程序的理解，为评估程序理解提供了新的有效手段。

Abstract: When learning to program, students are usually assessed based on the code
they wrote. However, the mere completion of a programming task does not
guarantee actual comprehension of the underlying concepts. Asking learners
questions about the code they wrote has therefore been proposed as a means to
assess program comprehension. As creating targeted questions for individual
student programs can be tedious and challenging, prior work has proposed to
generate such questions automatically. In this paper we generalize this idea to
the block-based programming language Scratch. We propose a set of 30 different
questions for Scratch code covering an established program comprehension model,
and extend the LitterBox static analysis tool to automatically generate
corresponding questions for a given Scratch program. On a dataset of 600,913
projects we generated 54,118,694 questions automatically. Our initial
experiments with 34 ninth graders demonstrate that this approach can indeed
generate meaningful questions for Scratch programs, and we find that the
ability of students to answer these questions on their programs relates to
their overall performance.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [187] [A Hybrid Agent-Based and System Dynamics Framework for Modelling Project Execution and Technology Maturity in Early-Stage R&D](https://arxiv.org/abs/2510.09688)
*R. W. S. Pessoa,M. H. Næss,J. C. Bijos,C. M. Rebello,D. Colombo,L. Schnitman,I. B. R. Nogueira*

Main category: cs.MA

TL;DR: 本文提出一种结合系统动力学和基于代理建模的混合方法，预测油气领域研发项目中的技术成熟度演变。


<details>
  <summary>Details</summary>
Motivation: 现有模型在研发领域应用有限，且难以同时捕捉技术进展中的不确定因素如工作量、团队规模和项目时长。

Method: 构建了一个多层次框架，将系统层面反馈机制与分散代理（团队成员、任务和控制者）结合，实现技术进展动力学的模拟。

Result: 基线情景下分析了早期创新项目的多任务并行执行，显示顺序执行可大幅减少返工时间，团队规模4-5人时项目效率最优。

Conclusion: 模型结果与专家认知一致，证明其在资源分配、进度安排和技术成熟度分析中的有效性。

Abstract: This paper presents a hybrid approach to predict the evolution of
technological maturity in R and D projects, using the oil and gas sector as an
example. Integrating System Dynamics (SD) and Agent Based Modelling (ABM)
allows the proposed multi level framework to capture uncertainties in work
effort, team size, and project duration, which influence technological
progress. While AB SD hybrid models are established in other fields, their use
in R and D remains limited. The model combines system level feedback structures
governing work phases, rework cycles, and duration with decentralised agents
such as team members, tasks, and controllers, whose interactions generate
emergent project dynamics. A base case scenario analysed early stage innovation
projects with 15 parallel tasks over 156 weeks. A comparative sequential
scenario showed an 88 percent reduction in rework duration. A second scenario
assessed mixed parallel sequential task structures with varying team sizes. In
parallel configurations, increasing team size reduced project duration and
improved task completion, with optimal results for teams of four to five
members. These findings align with empirical evidence showing that moderate
team expansion enhances coordination efficiency without excessive communication
overhead. However, larger teams may decrease performance due to communication
complexity and management delays. Overall, the model outputs and framework
align with expert understanding, supporting their validity as quantitative
tools for analysing resource allocation, scheduling efficiency, and technology
maturity progression.

</details>


### [188] [Structured Cooperative Multi-Agent Reinforcement Learning: a Bayesian Network Perspective](https://arxiv.org/abs/2510.09937)
*Shahbaz P Qadri Syed,He Bai*

Main category: cs.MA

TL;DR: 本文提出了一种基于贝叶斯网络的多智能体强化学习算法，通过利用智能体之间的耦合结构，提高学习效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体强化学习算法未充分利用智能体间耦合信息，难以高效处理大规模多智能体系统。

Method: 将多智能体合作问题建模为贝叶斯网络，提出价值依赖集概念和P-DTDE训练执行范式，推导多智能体策略梯度定理，并设计可扩展的演员-评论家算法，同时提出基于截断贝叶斯网络的近似方案。

Result: 理论上证明P-DTDE的策略梯度方差优于CTDE；在多仓库资源分配和多区域温控等任务中验证了算法的效率和可扩展性；近似方案在大规模智能体中表现出更快的收敛。

Conclusion: 通过系统利用智能体间耦合结构，本文方法有效提升了多智能体强化学习的效率和扩展能力，适用于大规模复杂系统。

Abstract: The empirical success of multi-agent reinforcement learning (MARL) has
motivated the search for more efficient and scalable algorithms for large scale
multi-agent systems. However, existing state-of-the-art algorithms do not fully
exploit inter-agent coupling information to develop MARL algorithms. In this
paper, we propose a systematic approach to leverage structures in the
inter-agent couplings for efficient model-free reinforcement learning. We model
the cooperative MARL problem via a Bayesian network and characterize the subset
of agents, termed as the value dependency set, whose information is required by
each agent to estimate its local action value function exactly. Moreover, we
propose a partially decentralized training decentralized execution (P-DTDE)
paradigm based on the value dependency set. We theoretically establish that the
total variance of our P-DTDE policy gradient estimator is less than the
centralized training decentralized execution (CTDE) policy gradient estimator.
We derive a multi-agent policy gradient theorem based on the P-DTDE scheme and
develop a scalable actor-critic algorithm. We demonstrate the efficiency and
scalability of the proposed algorithm on multi-warehouse resource allocation
and multi-zone temperature control examples. For dense value dependency sets,
we propose an approximation scheme based on truncation of the Bayesian network
and empirically show that it achieves a faster convergence than the exact value
dependence set for applications with a large number of agents.

</details>


### [189] [KG-MAS: Knowledge Graph-Enhanced Multi-Agent Infrastructure for coupling physical and digital robotic environments](https://arxiv.org/abs/2510.10325)
*Walid Abdela*

Main category: cs.MA

TL;DR: 本文提出了一种基于知识图谱的多智能体架构KG-MAS，用于解决工业4.0中物理与数字环境无缝集成的复杂异构性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的工业4.0系统集成方法往往依赖刚性的以数据为中心的方案，如协同仿真框架或点对点中间件，缺乏语义丰富性和灵活性，难以实现智能自主协调。

Method: 提出KG-MAS，利用集中式知识图谱作为共享的动态世界模型，为多智能体系统提供共同的语义基础。物理和数字组件的自主代理通过查询和更新知识图谱来决策，并采用模型驱动架构自动生成代理，简化系统扩展和维护。

Result: KG-MAS成功实现了异构物理和数字机器人环境的统一协调，屏蔽了底层通信协议，提供了强健、可扩展且灵活的解决方案。

Conclusion: KG-MAS为工业4.0中的CPS复杂系统提供了一种基于语义的智能协调机制，显著提升了系统的柔性和自治能力。

Abstract: The seamless integration of physical and digital environments in
Cyber-Physical Systems(CPS), particularly within Industry 4.0, presents
significant challenges stemming from system heterogeneity and complexity.
Traditional approaches often rely on rigid, data-centric solutions like
co-simulation frameworks or brittle point-to-point middleware bridges, which
lack the semantic richness and flexibility required for intelligent, autonomous
coordination. This report introduces the Knowledge Graph-Enhanced Multi-Agent
Infrastructure(KG-MAS), as resolution in addressing such limitations. KG-MAS
leverages a centralized Knowledge Graph (KG) as a dynamic, shared world model,
providing a common semantic foundation for a Multi-Agent System(MAS).
Autonomous agents, representing both physical and digital components, query
this KG for decision-making and update it with real-time state information. The
infrastructure features a model-driven architecture which facilitates the
automatic generation of agents from semantic descriptions, thereby simplifying
system extension and maintenance. By abstracting away underlying communication
protocols and providing a unified, intelligent coordination mechanism, KG-MAS
offers a robust, scalable, and flexible solution for coupling heterogeneous
physical and digital robotic environments.

</details>


### [190] [HyperAgent: Leveraging Hypergraphs for Topology Optimization in Multi-Agent Communication](https://arxiv.org/abs/2510.10611)
*Heng Zhang,Yuling Shi,Xiaodong Gu,Zijian Zhang,Haochen You,Lubin Gan,Yilei Yuan,Jin Huang*

Main category: cs.MA

TL;DR: 提出了一种基于超图的多智能体系统框架HyperAgent，通过超边表示和超图卷积有效捕获多智能体协作关系，动态调节通信拓扑，提升了任务适应性和通信效率。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统在协作建模上仅依赖成对边表示，难以捕捉多智能体间的复杂关系，且通信拓扑设计任务适应性差，导致通信成本高或协调不足，限制了系统扩展和实际应用。

Method: 提出HyperAgent框架，利用超图结构中的超边直接连接多个任务内的智能体，并采用超图卷积实现一步信息聚合；结合变分自编码器和稀疏正则化根据任务复杂度动态调整通信拓扑。

Result: 在GSM8K数据集上，HyperAgent达到95.07%的准确率，同时减少了25.33%的令牌消耗，表现出更优的性能和效率。

Conclusion: 基于超图的通信拓扑优化显著提升了多智能体系统的协作效果和任务适应性，具有较高的实用价值和推广潜力。

Abstract: Recent advances in large language model-powered multi-agent systems have
demonstrated remarkable collective intelligence through effective
communication. However, existing approaches face two primary challenges: (i)
\textit{Ineffective group collaboration modeling}, as they rely on pairwise
edge representations in graph structures, limiting their ability to capture
relationships among multiple agents; and (ii) \textit{Limited task-adaptiveness
in communication topology design}, leading to excessive communication cost for
simple tasks and insufficient coordination for complex scenarios. These issues
restrict the scalability and practical deployment of adaptive collaboration
frameworks. To address these challenges, we propose \textbf{HyperAgent}, a
hypergraph-based framework that optimizes communication topologies and
effectively captures group collaboration patterns using direct hyperedge
representations. Unlike edge-based approaches, HyperAgent uses hyperedges to
link multiple agents within the same subtask and employs hypergraph
convolutional layers to achieve one-step information aggregation in
collaboration groups. Additionally, it incorporates a variational autoencoder
framework with sparsity regularization to dynamically adjust hypergraph
topologies based on task complexity. Experiments highlight the superiority of
HyperAgent in both performance and efficiency. For instance, on GSM8K,
HyperAgent achieves 95.07\% accuracy while reducing token consumption by
25.33\%, demonstrating the potential of hypergraph-based optimization for
multi-agent communication.

</details>


### [191] [Fast and the Furious: Hot Starts in Pursuit-Evasion Games](https://arxiv.org/abs/2510.10830)
*Gabriel Smithline,Scott Nivison*

Main category: cs.MA

TL;DR: 该论文提出了一种结合博弈论控制和图神经网络的新方法，用于在追逃博弈中有效定位追捕者。


<details>
  <summary>Details</summary>
Motivation: 在缺乏逃逸者位置先验知识的情况下，如何有效布置追捕者是一个重大挑战。

Method: 将追捕者配置视为战略图，通过多目标优化构建图特征空间，识别帕累托最优配置；利用图卷积网络（GCN）训练这些最优图形，生成策略性初始配置“热启动”。

Result: GCN生成的初始配置相比随机配置表现出显著优势，多追捕者和逃逸者情境下，该方法加快逃逸者存活率下降，减少追捕者行进距离，增强包围效果。

Conclusion: 结合博弈论控制与图神经网络的方法能显著提升追捕者在追逃博弈中的布阵策略，具有明显的战略价值。

Abstract: Effectively positioning pursuers in pursuit-evasion games without prior
knowledge of evader locations remains a significant challenge. A novel approach
that combines game-theoretic control theory with Graph Neural Networks is
introduced in this work. By conceptualizing pursuer configurations as strategic
arrangements and representing them as graphs, a Graph Characteristic Space is
constructed via multi-objective optimization to identify Pareto-optimal
configurations. A Graph Convolutional Network (GCN) is trained on these
Pareto-optimal graphs to generate strategically effective initial
configurations, termed "hot starts". Empirical evaluations demonstrate that the
GCN-generated hot starts provide a significant advantage over random
configurations. In scenarios considering multiple pursuers and evaders, this
method hastens the decline in evader survival rates, reduces pursuer travel
distances, and enhances containment, showcasing clear strategic benefits.

</details>


### [192] [The Social Cost of Intelligence: Emergence, Propagation, and Amplification of Stereotypical Bias in Multi-Agent Systems](https://arxiv.org/abs/2510.10943)
*Thi-Nhung Nguyen,Linhao Luo,Thuy-Trang Vu,Dinh Phung*

Main category: cs.MA

TL;DR: 本文研究了多智能体系统（MAS）中大语言模型的偏见问题，发现MAS中偏见更易出现且传播，但合作和辩论式交流能减轻偏见放大。


<details>
  <summary>Details</summary>
Motivation: 随着多智能体系统的兴起，多个大语言模型协作带来了偏见产生和传播的新动态，亟需研究这些影响因素。

Method: 通过模拟社会情境，代理代表不同社会群体，评估代理间交互和通信协议对偏见鲁棒性和传播的影响，并在三个偏见基准上进行实验。

Result: MAS系统较单智能体系统偏见更不鲁棒，偏见早期由组内偏好引发，但合作和辩论式通信能减轻偏见放大，且底层更鲁棒的LLM提高系统稳定性。

Conclusion: 多智能体大语言模型系统的公平性和鲁棒性受内部专业化、基础模型和通信协议显著影响，合理设计可提升系统公平与稳定。

Abstract: Bias in large language models (LLMs) remains a persistent challenge,
manifesting in stereotyping and unfair treatment across social groups. While
prior research has primarily focused on individual models, the rise of
multi-agent systems (MAS), where multiple LLMs collaborate and communicate,
introduces new and largely unexplored dynamics in bias emergence and
propagation. In this work, we present a comprehensive study of stereotypical
bias in MAS, examining how internal specialization, underlying LLMs and
inter-agent communication protocols influence bias robustness, propagation, and
amplification. We simulate social contexts where agents represent different
social groups and evaluate system behavior under various interaction and
adversarial scenarios. Experiments on three bias benchmarks reveal that MAS are
generally less robust than single-agent systems, with bias often emerging early
through in-group favoritism. However, cooperative and debate-based
communication can mitigate bias amplification, while more robust underlying
LLMs improve overall system stability. Our findings highlight critical factors
shaping fairness and resilience in multi-agent LLM systems.

</details>


### [193] [Automating Structural Engineering Workflows with Large Language Model Agents](https://arxiv.org/abs/2510.11004)
*Haoran Liang,Yufa Zhou,Mohammad Talebi Kalaleh,Qipei Mei*

Main category: cs.MA

TL;DR: 本文介绍了MASSE，这是首个用于结构工程的多智能体系统，集成了基于大语言模型的智能体，能够自动化传统结构工程工作流程。


<details>
  <summary>Details</summary>
Motivation: 结构工程领域发展停滞多年，工作流程未有显著变化，尽管其经济影响巨大。利用大语言模型最新进展，可以提升结构工程任务的效率和准确性。

Method: 构建了一个基于无需训练的大语言模型的多智能体系统，能够自动执行设计规范理解、负载计算和结构容量验证等核心任务。

Result: MASSE系统在真实案例中应用，大幅缩短了专家工作时间，从约两小时减少到数分钟，同时提高了工程的可靠性和准确性。

Conclusion: MASSE实现了结构工程工作流程的自动化，具备即刻部署的能力，显著提升工作效率和质量，展示了大语言模型在工程领域应用的潜力。

Abstract: We introduce $\textbf{MASSE}$, the first Multi-Agent System for Structural
Engineering, effectively integrating large language model (LLM)-based agents
with real-world engineering workflows. Structural engineering is a fundamental
yet traditionally stagnant domain, with core workflows remaining largely
unchanged for decades despite its substantial economic impact and global market
size. Recent advancements in LLMs have significantly enhanced their ability to
perform complex reasoning, long-horizon planning, and precise tool utilization
-- capabilities well aligned with structural engineering tasks such as
interpreting design codes, executing load calculations, and verifying
structural capacities. We present a proof-of-concept showing that most
real-world structural engineering workflows can be fully automated through a
training-free LLM-based multi-agent system. MASSE enables immediate deployment
in professional environments, and our comprehensive validation on real-world
case studies demonstrates that it can reduce expert workload from approximately
two hours to mere minutes, while enhancing both reliability and accuracy in
practical engineering scenarios.

</details>


### [194] [A Vision for Access Control in LLM-based Agent Systems](https://arxiv.org/abs/2510.11108)
*Xinfeng Li,Dong Huang,Jie Li,Hongyi Cai,Zhenhong Zhou,Wei Dong,XiaoFeng Wang,Yang Liu*

Main category: cs.MA

TL;DR: 传统基于规则的访问控制机制无法满足具有自主性和情境复杂性的基于大型语言模型的智能体，需从二元许可转向动态、上下文感知的信息流治理。


<details>
  <summary>Details</summary>
Motivation: 传统的静态规则访问控制在应对大型语言模型智能体的自主性和动态信息流时显得不足，难以管理复杂的交互环境。

Method: 提出了一种名为Agent Access Control (AAC)的新框架，采用多维度情境评估和自适应响应生成两大模块，动态治理信息流，超越简单的允许/拒绝决策。

Result: AAC通过身份、关系、场景及规范的多维评估，实现对信息流的细致控制，并通过删减、总结及改写等方式灵活调整信息内容。

Conclusion: AAC为未来可信智能体设计提供了新的概念框架，有望实现人类般细致判断与可扩展AI安全的结合，推动访问控制范式的转变。

Abstract: The autonomy and contextual complexity of LLM-based agents render traditional
access control (AC) mechanisms insufficient. Static, rule-based systems
designed for predictable environments are fundamentally ill-equipped to manage
the dynamic information flows inherent in agentic interactions. This position
paper argues for a paradigm shift from binary access control to a more
sophisticated model of information governance, positing that the core challenge
is not merely about permission, but about governing the flow of information. We
introduce Agent Access Control (AAC), a novel framework that reframes AC as a
dynamic, context-aware process of information flow governance. AAC operates on
two core modules: (1) multi-dimensional contextual evaluation, which assesses
not just identity but also relationships, scenarios, and norms; and (2)
adaptive response formulation, which moves beyond simple allow/deny decisions
to shape information through redaction, summarization, and paraphrasing. This
vision, powered by a dedicated AC reasoning engine, aims to bridge the gap
between human-like nuanced judgment and scalable Al safety, proposing a new
conceptual lens for future research in trustworthy agent design.

</details>


### [195] [Autonomous vehicles need social awareness to find optima in multi-agent reinforcement learning routing games](https://arxiv.org/abs/2510.11410)
*Anastasia Psarou,Łukasz Gorczyca,Dominik Gaweł,Rafał Kucharski*

Main category: cs.MA

TL;DR: 本研究提出通过在多智能体强化学习中引入边际成本作为内在奖励，有效加速自动驾驶车辆路由策略的收敛，改善交通系统性能。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体强化学习中自私的自动驾驶车辆策略导致收敛慢，交通系统不稳定，需寻找方法优化训练速度和系统稳定性。

Method: 通过基于边际成本矩阵设计的内在奖励信号，使自动驾驶车辆在最小化自身旅行时间的同时，降低对整体系统的负面影响，协调各智能体目标。

Result: 引入边际成本作为奖励成分的MARL算法在玩具网络和Saint-Arnoult现实网络中表现出更快收敛和更优的整体交通效率，而传统算法无法稳定达到最优解。

Conclusion: 通过增强社会意识（考虑边际成本），未来城市中的自动驾驶车辆能够实现更好的系统性能和个体效益，解决了多智能体路由策略中的收敛和稳定性问题。

Abstract: Previous work has shown that when multiple selfish Autonomous Vehicles (AVs)
are introduced to future cities and start learning optimal routing strategies
using Multi-Agent Reinforcement Learning (MARL), they may destabilize traffic
systems, as they would require a significant amount of time to converge to the
optimal solution, equivalent to years of real-world commuting.
  We demonstrate that moving beyond the selfish component in the reward
significantly relieves this issue. If each AV, apart from minimizing its own
travel time, aims to reduce its impact on the system, this will be beneficial
not only for the system-wide performance but also for each individual player in
this routing game.
  By introducing an intrinsic reward signal based on the marginal cost matrix,
we significantly reduce training time and achieve convergence more reliably.
Marginal cost quantifies the impact of each individual action (route-choice) on
the system (total travel time). Including it as one of the components of the
reward can reduce the degree of non-stationarity by aligning agents'
objectives. Notably, the proposed counterfactual formulation preserves the
system's equilibria and avoids oscillations.
  Our experiments show that training MARL algorithms with our novel reward
formulation enables the agents to converge to the optimal solution, whereas the
baseline algorithms fail to do so. We show these effects in both a toy network
and the real-world network of Saint-Arnoult. Our results optimistically
indicate that social awareness (i.e., including marginal costs in routing
decisions) improves both the system-wide and individual performance of future
urban systems with AVs.

</details>
