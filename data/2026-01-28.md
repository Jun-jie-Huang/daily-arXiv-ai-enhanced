<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 46]
- [cs.SE](#cs.SE) [Total: 29]
- [cs.MA](#cs.MA) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Language Family Matters: Evaluating LLM-Based ASR Across Linguistic Boundaries](https://arxiv.org/abs/2601.18899)
*Yuchen Zhang,Ravi Shekhar,Haralambos Mouratidis*

Main category: cs.CL

TL;DR: 本文提出了一种基于语言家族共享连接器的多语种自动语音识别系统，提高了模型泛化能力并减少了参数数量。


<details>
  <summary>Details</summary>
Motivation: 现有方法为每种语言分别训练连接器，忽略了语言之间的相关性，导致资源浪费和泛化能力不足。

Method: 提出了利用语言家族成员关系共享连接器的策略，实现每个语言家族只用一个连接器，并在两个多语种LLM和真实语料库上进行验证。

Result: 实验结果表明，基于语言家族的连接器不仅减少了参数数量，还提升了跨领域的泛化性能。

Conclusion: 基于语言家族的连接器共享策略能够减少参数量，同时提升不同领域的泛化能力，是多语种ASR系统实用且可扩展的方法。

Abstract: Large Language Model (LLM)-powered Automatic Speech Recognition (ASR) systems achieve strong performance with limited resources by linking a frozen speech encoder to a pretrained LLM via a lightweight connector. Prior work trains a separate connector per language, overlooking linguistic relatedness. We propose an efficient and novel connector-sharing strategy based on linguistic family membership, enabling one connector per family, and empirically validate its effectiveness across two multilingual LLMs and two real-world corpora spanning curated and crowd-sourced speech. Our results show that family-based connectors reduce parameter count while improving generalization across domains, offering a practical and scalable strategy for multilingual ASR deployment.

</details>


### [2] [Self-Aware Knowledge Probing: Evaluating Language Models' Relational Knowledge through Confidence Calibration](https://arxiv.org/abs/2601.18901)
*Christopher Kissling,Elena Merdjanovska,Alan Akbik*

Main category: cs.CL

TL;DR: 该论文提出了一个新的关系知识校准探测框架，揭示了语言模型普遍存在过度自信问题，并指出需要改进置信度的语义准确性。


<details>
  <summary>Details</summary>
Motivation: 现有知识探测方法只通过准确率和精确率评估模型能力，未考虑模型置信度的校准及其可靠性，因此需要一种新的探测框架来全面评估语言模型的信心水平和校准情况。

Method: 提出了一个覆盖三种置信度模式（内在置信度、结构一致性、语义基础）的关系知识校准探测框架，并对十种因果模型及六种掩码模型进行了系统分析。

Result: 本文提出了一种针对关系知识的校准探测框架，涵盖了模型置信度的三种模式：内在置信度、结构一致性和语义基础。通过分析十种因果语言模型和六种掩码语言模型，发现大多数模型特别是掩码预训练模型表现出过度自信的现象。最佳校准的置信度估计应考虑语句重述带来的不一致。此外，即使是最大规模的预训练模型也未能准确编码语言置信表达的语义。

Conclusion: 多数现有语言模型尤其是掩码预训练模型在置信度校准上存在过度自信问题。通过考虑结构一致性可以提高置信度的校准效果，但语义置信表达的准确编码仍待解决。

Abstract: Knowledge probing quantifies how much relational knowledge a language model (LM) has acquired during pre-training. Existing knowledge probes evaluate model capabilities through metrics like prediction accuracy and precision. Such evaluations fail to account for the model's reliability, reflected in the calibration of its confidence scores. In this paper, we propose a novel calibration probing framework for relational knowledge, covering three modalities of model confidence: (1) intrinsic confidence, (2) structural consistency and (3) semantic grounding. Our extensive analysis of ten causal and six masked language models reveals that most models, especially those pre-trained with the masking objective, are overconfident. The best-calibrated scores come from confidence estimates that account for inconsistencies due to statement rephrasing. Moreover, even the largest pre-trained models fail to encode the semantics of linguistic confidence expressions accurately.

</details>


### [3] [Flatter Tokens are More Valuable for Speculative Draft Model Training](https://arxiv.org/abs/2601.18902)
*Jiaming Fan,Daming Cao,Xiangzhong Luo,Jiale Fu,Chonghan Liu,Xu Yang*

Main category: cs.CL

TL;DR: 通过筛选对推理加速贡献大的样本，本文提出的SFDD方法实现了SD训练的高效加速。


<details>
  <summary>Details</summary>
Motivation: 现有的Speculative Decoding (SD)技术加速大型语言模型推理需要用大量数据训练草稿模型，存在数据利用效率低的问题。

Method: 本文提出基于平坦度（flatness）度量的训练样本过滤方法，即Sample-level-flatness-based Dataset Distillation (SFDD)方法，用以筛选对SD接受率贡献较大的样本，提升训练效率。

Result: 在EAGLE框架上实验表明，SFDD方法用50%数据实现了超过2倍的训练加速，且最终模型推理速度提升仅比使用全数据集低不到4%。

Conclusion: 引入基于平坦度的数据过滤策略显著提升了Speculative Decoding训练的效率，提供了一种有效的数据中心方案。

Abstract: Speculative Decoding (SD) is a key technique for accelerating Large Language Model (LLM) inference, but it typically requires training a draft model on a large dataset. We approach this problem from a data-centric perspective, finding that not all training samples contribute equally to the SD acceptance rate. Specifically, our theoretical analysis and empirical validation reveals that tokens inducing flatter predictive distributions from the target model are more valuable than those yielding sharply peaked distributions. Based on this insight, we propose flatness, a new metric to quantify this property, and develop the Sample-level-flatness-based Dataset Distillation (SFDD) approach, which filters the training data to retain only the most valuable samples. Experiments on the EAGLE framework demonstrate that SFDD can achieve over 2$\times$ training speedup using only 50% of the data, while keeping the final model's inference speedup within 4% of the full-dataset baseline. This work introduces an effective, data-centric approach that substantially improves the training efficiency for Speculative Decoding. Our code is available at https://anonymous.4open.science/r/Flatness.

</details>


### [4] [BabyReasoningBench: Generating Developmentally-Inspired Reasoning Tasks for Evaluating Baby Language Models](https://arxiv.org/abs/2601.18933)
*Kaustubh D. Dhole*

Main category: cs.CL

TL;DR: 提出了面向婴儿语言模型推理能力评估的BabyReasoningBench，发现这些模型在部分推理任务上表现有限，揭示了其能力与训练数据的紧密关联。


<details>
  <summary>Details</summary>
Motivation: 传统的语言模型推理能力评估多面向成年人，假设其具备丰富的世界知识和复杂的指令理解能力，而婴儿语言模型训练数据有限，这与传统评估存在不匹配，导致难以评估其实际推理能力。

Method: 构建了BabyReasoningBench基准测试集，包括19个经典发展心理学中的推理任务，使用GPT-5.2生成，评估了两种基于GPT-2的婴儿语言模型表现。

Result: 两种基于儿童语言数据预训练的模型整体表现较低，但因果和物理推理能力随规模增加有所提升，信念归因和语用相关任务仍表现较差。

Conclusion: 基于面向儿童的训练数据，婴儿语言模型在推理能力上表现不均，部分因果和物理推理任务有所提升，而信念归因和语用敏感任务依然较难。

Abstract: Traditional evaluations of reasoning capabilities of language models are dominated by adult-centric benchmarks that presuppose broad world knowledge, complex instruction following, and mature pragmatic competence. These assumptions are mismatched to baby language models trained on developmentally plausible input such as child-directed speech and early-childhood narratives, and they obscure which reasoning abilities (if any) emerge under such constraints. We introduce BabyReasoningBench, a GPT-5.2 generated benchmark of 19 reasoning tasks grounded in classic paradigms from developmental psychology, spanning theory of mind, analogical and relational reasoning, causal inference and intervention selection, and core reasoning primitives that are known to be confounded by memory and pragmatics. We find that two GPT-2 based baby language models (pretrained on 10M and 100M of child-directed speech text) show overall low but uneven performance, with dissociations across task families: scaling improves several causal and physical reasoning tasks, while belief attribution and pragmatics-sensitive tasks remain challenging. BabyReasoningBench provides a developmentally grounded lens for analyzing what kinds of reasoning are supported by child-like training distributions, and for testing mechanistic hypotheses about how such abilities emerge.

</details>


### [5] [LLMs versus the Halting Problem: Revisiting Program Termination Prediction](https://arxiv.org/abs/2601.18987)
*Oren Sultan,Jordi Armengol-Estape,Pascal Kesseli,Julien Vanegue,Dafna Shahaf,Yossi Adi,Peter O'Hearn*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型（LLMs）预测程序终止性的能力，评估了多个LLMs在SV-Comp 2025终止性竞赛中C程序上的表现，发现LLMs在预测程序终止性方面表现优异，但在提供有效证明和处理长程序时存在不足。


<details>
  <summary>Details</summary>
Motivation: 程序终止性判定是计算机科学中的核心问题，其不可判定性限制了自动验证工具的普适性。近年来LLMs的成功激发了探讨其能否可靠预测程序终止性的问题。

Method: 本文利用国际软件验证竞赛（SV-Comp 2025）中终止性测试集，对多种LLMs（如GPT-5、Claude Sonnet-4.5和Code World Model）进行终止性预测评估，比较其与顶尖验证工具的表现。

Result: LLMs，尤其是GPT-5和Claude Sonnet-4.5在终止性预测竞赛中表现突出，仅次于排名第一和第二的传统工具，显示出强大的潜力，但仍存在无法提供证明和对长程序预测效果下降的问题。

Conclusion: LLMs在预测程序终止性方面表现接近传统顶级工具，但缺乏有效证明输出且性能会随着程序长度增加而下降。

Abstract: Determining whether a program terminates is a central problem in computer science. Turing's foundational result established the Halting Problem as undecidable, showing that no algorithm can universally determine termination for all programs and inputs. Consequently, automatic verification tools approximate termination, sometimes failing to prove or disprove; these tools rely on problem-specific architectures and abstractions, and are usually tied to particular programming languages. Recent success and progress in large language models (LLMs) raises the following question: can LLMs reliably predict program termination? In this work, we evaluate LLMs on a diverse set of C programs from the Termination category of the International Competition on Software Verification (SV-Comp) 2025. Our results suggest that LLMs perform remarkably well at predicting program termination, where GPT-5 and Claude Sonnet-4.5 would rank just behind the top-ranked tool (using test-time-scaling), and Code World Model (CWM) would place just behind the second-ranked tool. While LLMs are effective at predicting program termination, they often fail to provide a valid witness as a proof. Moreover, LLMs performance drops as program length increases. We hope these insights motivate further research into program termination and the broader potential of LLMs for reasoning about undecidable problems.

</details>


### [6] [Malicious Repurposing of Open Science Artefacts by Using Large Language Models](https://arxiv.org/abs/2601.18998)
*Zahra Hashemi,Zhiqiang Zhong,Jun Pang,Wei Zhao*

Main category: cs.CL

TL;DR: 该论文揭示了大语言模型可能被利用生成有害科学研究提案的风险，提出了一整套方法来绕过保护机制并评估这些提案的危险性，强调了人类评估在风险评估中的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前研究多关注LLMs在科学发现中的正向作用，忽视了这些模型可能被利用生成有害研究，尤其是通过改装开放科学资源用于恶意目的的风险。

Method: 引入一个端到端管道，利用基于说服的越狱技术绕过大语言模型（LLMs）保护机制，重新解释自然语言处理论文中的数据集、方法和工具，并利用其漏洞进行恶意改造，最后通过一个评估框架从有害性、误用可行性和技术合理性三个维度评估这些恶意提案。

Result: LLMs能够生成通过重用伦理设计的开放资源而产生的有害提案，但不同LLMs作为评估者之间在评估结果上存在显著分歧，显示目前LLMs尚不能作为恶意评估的可靠判官。

Conclusion: LLMs能被滥用来生成有害研究提案，但由于评估LLMs间存在高度不一致性，仍需依赖人工评估以保障风险评估的可信度。

Abstract: The rapid evolution of large language models (LLMs) has fuelled enthusiasm about their role in advancing scientific discovery, with studies exploring LLMs that autonomously generate and evaluate novel research ideas. However, little attention has been given to the possibility that such models could be exploited to produce harmful research by repurposing open science artefacts for malicious ends. We fill the gap by introducing an end-to-end pipeline that first bypasses LLM safeguards through persuasion-based jailbreaking, then reinterprets NLP papers to identify and repurpose their artefacts (datasets, methods, and tools) by exploiting their vulnerabilities, and finally assesses the safety of these proposals using our evaluation framework across three dimensions: harmfulness, feasibility of misuse, and soundness of technicality. Overall, our findings demonstrate that LLMs can generate harmful proposals by repurposing ethically designed open artefacts; however, we find that LLMs acting as evaluators strongly disagree with one another on evaluation outcomes: GPT-4.1 assigns higher scores (indicating greater potential harms, higher soundness and feasibility of misuse), Gemini-2.5-pro is markedly stricter, and Grok-3 falls between these extremes. This indicates that LLMs cannot yet serve as reliable judges in a malicious evaluation setup, making human evaluation essential for credible dual-use risk assessment.

</details>


### [7] [FROST: Filtering Reasoning Outliers with Attention for Efficient Reasoning](https://arxiv.org/abs/2601.19001)
*Haozheng Luo,Zhuolin Jiang,Md Zahid Hasan,Yan Chen,Soumalya Sarkar*

Main category: cs.CL

TL;DR: FROST利用注意力权重剪枝推理路径，提升推理效率和准确率，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统推理方法效率低，推理路径冗长且不够可靠。

Method: 提出FROST方法，利用注意力权重剪枝不重要的推理路径，引入推理异常点的概念，并设计基于注意力的机制去除异常点。

Result: 在四个基准测试和两个强推理模型上，FROST在效率和准确率上显著优于现有最先进方法。平均减少69.68%令牌使用，准确率提升26.70%，同时显著降低注意力异常指标。

Conclusion: FROST在保证和提升模型推理能力的同时，通过去除推理异常点实现了更短、更可靠的推理路径，提高推理效率和准确性。

Abstract: We propose FROST, an attention-aware method for efficient reasoning. Unlike traditional approaches, FROST leverages attention weights to prune uncritical reasoning paths, yielding shorter and more reliable reasoning trajectories. Methodologically, we introduce the concept of reasoning outliers and design an attention-based mechanism to remove them. Theoretically, FROST preserves and enhances the model's reasoning capacity while eliminating outliers at the sentence level. Empirically, we validate FROST on four benchmarks using two strong reasoning models (Phi-4-Reasoning and GPT-OSS-20B), outperforming state-of-the-art methods such as TALE and ThinkLess. Notably, FROST achieves an average 69.68% reduction in token usage and a 26.70% improvement in accuracy over the base model. Furthermore, in evaluations of attention outlier metrics, FROST reduces the maximum infinity norm by 15.97% and the average kurtosis by 91.09% compared to the base model. Code is available at https://github.com/robinzixuan/FROST

</details>


### [8] [Optimizing Conversational Quality in Spoken Dialogue Systems with Reinforcement Learning from AI Feedback](https://arxiv.org/abs/2601.19063)
*Siddhant Arora,Jinchuan Tian,Jiatong Shi,Hayato Futami,Yosuke Kashiwagi,Emiru Tsunoo,Shinji Watanabe*

Main category: cs.CL

TL;DR: 本文首次提出多重奖励强化学习框架提升语音对话系统质量，多维奖励联合优化显著优于单一奖励方法。


<details>
  <summary>Details</summary>
Motivation: 现有语音对话系统强化学习多限于单一语义奖励，忽视对话质量的多维、多模态特性，且不适用于逐步生成响应的双工系统，亟需更全面的奖励机制提升系统表现。

Method: 结合语义、一致性、音频质量和情感一致性多重奖励，使用基于回合的偏好采样与单一DPO目标函数整合块级解码的强化学习方法。

Result: 本文提出了用于语音输入/输出对话系统（SDS）的多重奖励强化学习方法（RLAIF），结合语义、一致性、音频质量及情感一致性等多维奖励指标，克服了以往单一语义奖励的不足，提高了对话系统的整体表现。通过整合多重奖励于单一DPO目标函数，并采用基于回合偏好的采样方法，实现在逐块解码过程中高效的策略优化。实验证明，多重奖励训练相较于单一奖励方法，在语义质量和音频自然度等多个指标上均有显著提升。

Conclusion: 多重奖励强化学习能有效提升语音对话系统的整体表现，体现了多维综合对齐对于实际对话系统的关键作用。

Abstract: Reinforcement learning from human or AI feedback (RLHF/RLAIF) for speech-in/speech-out dialogue systems (SDS) remains underexplored, with prior work largely limited to single semantic rewards applied at the utterance level. Such setups overlook the multi-dimensional and multi-modal nature of conversational quality, which encompasses semantic coherence, audio naturalness, speaker consistency, emotion alignment, and turn-taking behavior. Moreover, they are fundamentally mismatched with duplex spoken dialogue systems that generate responses incrementally, where agents must make decisions based on partial utterances. We address these limitations with the first multi-reward RLAIF framework for SDS, combining semantic, audio-quality, and emotion-consistency rewards. To align utterance-level preferences with incremental, blockwise decoding in duplex models, we apply turn-level preference sampling and aggregate per-block log-probabilities within a single DPO objective. We present the first systematic study of preference learning for improving SDS quality in both multi-turn Chain-of-Thought and blockwise duplex models, and release a multi-reward DPO dataset to support reproducible research. Experiments show that single-reward RLAIF selectively improves its targeted metric, while joint multi-reward training yields consistent gains across semantic quality and audio naturalness. These results highlight the importance of holistic, multi-reward alignment for practical conversational SDS.

</details>


### [9] [PsyProbe: Proactive and Interpretable Dialogue through User State Modeling for Exploratory Counseling](https://arxiv.org/abs/2601.19096)
*Sohhyung Park,Hyunji Kang,Sungzoon Cho,Dongil Kim*

Main category: cs.CL

TL;DR: 本文提出了PsyProbe，一个结合系统化心理状态建模和主动提问的心理健康对话系统，验证了其在真实咨询中的有效性和优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有心理健康对话系统主要是被动反应，缺乏系统的用户心理状态建模，无法实现主动的治疗性探索。

Method: 基于PPPPPI框架进行用户心理状态结构化建模，结合认知错误检测，采用状态构建、记忆构造、策略规划和带有质疑和修订模块的响应生成模块，生成情境适当的主动问题。

Result: PsyProbe系统通过系统化的用户心理状态追踪和主动提问机制，在真实咨询场景中表现优于基线模型，提升了用户参与度和自然度，并获得专业咨询师的认可。

Conclusion: 系统化的用户状态建模和主动提问显著提升了心理健康对话系统在治疗性探索阶段的表现，验证了该方法的有效性。

Abstract: Recent advances in large language models have enabled mental health dialogue systems, yet existing approaches remain predominantly reactive, lacking systematic user state modeling for proactive therapeutic exploration. We introduce PsyProbe, a dialogue system designed for the exploration phase of counseling that systematically tracks user psychological states through the PPPPPI framework (Presenting, Predisposing, Precipitating, Perpetuating, Protective, Impact) augmented with cognitive error detection. PsyProbe combines State Builder for extracting structured psychological profiles, Memory Construction for tracking information gaps, Strategy Planner for Motivational Interviewing behavioral codes, and Response Generator with Question Ideation and Critic/Revision modules to generate contextually appropriate, proactive questions. We evaluate PsyProbe with 27 participants in real-world Korean counseling scenarios, including automatic evaluation across ablation modes, user evaluation, and expert evaluation by a certified counselor. The full PsyProbe model consistently outperforms baseline and ablation modes in automatic evaluation. User evaluation demonstrates significantly increased engagement intention and improved naturalness compared to baseline. Expert evaluation shows that PsyProbe substantially improves core issue understanding and achieves question rates comparable to professional counselors, validating the effectiveness of systematic state modeling and proactive questioning for therapeutic exploration.

</details>


### [10] [Leveraging Sentence-oriented Augmentation and Transformer-Based Architecture for Vietnamese-Bahnaric Translation](https://arxiv.org/abs/2601.19124)
*Tan Sang Nguyen,Quoc Nguyen Pham,Tho Quan*

Main category: cs.CL

TL;DR: 利用先进NMT技术及数据增强方法改善资源匮乏下的越南语到Bahnaric语言的机器翻译，促进语言保护和传播。


<details>
  <summary>Details</summary>
Motivation: Bahnaric语言资源有限，翻译准确性和流利性受限，且该语言对文化传承和教育沟通至关重要。

Method: 采用先进的神经机器翻译技术结合两种数据增强策略，针对越南语到Bahnaric语言的领域特定翻译任务进行研究。

Result: 提出的两种增强策略提高了翻译质量，且方法灵活，不依赖复杂预处理、额外系统训练或额外数据，仅基于现有平行语料库。

Conclusion: 所提方法有效提升了越南语-巴拿尔语的机器翻译性能，支持该少数民族语言的数字化保存与传播，具有广泛适用性。

Abstract: The Bahnar people, an ethnic minority in Vietnam with a rich ancestral heritage, possess a language of immense cultural and historical significance. The government places a strong emphasis on preserving and promoting the Bahnaric language by making it accessible online and encouraging communication across generations. Recent advancements in artificial intelligence, such as Neural Machine Translation (NMT), have brought about a transformation in translation by improving accuracy and fluency. This, in turn, contributes to the revival of the language through educational efforts, communication, and documentation. Specifically, NMT is pivotal in enhancing accessibility for Bahnaric speakers, making information and content more readily available. Nevertheless, the translation of Vietnamese into Bahnaric faces practical challenges due to resource constraints, especially given the limited resources available for the Bahnaric language. To address this, we employ state-of-the-art techniques in NMT along with two augmentation strategies for domain-specific Vietnamese-Bahnaric translation task. Importantly, both approaches are flexible and can be used with various neural machine translation models. Additionally, they do not require complex data preprocessing steps, the training of additional systems, or the acquisition of extra data beyond the existing training parallel corpora.

</details>


### [11] [Transparency-First Medical Language Models: Datasheets, Model Cards, and End-to-End Data Provenance for Clinical NLP](https://arxiv.org/abs/2601.19191)
*Olaf Yunus Laitinen Imanov,Taner Yilmaz,Ayse Tuba Tugrul,Melike Nesrin Zaman,Ozkan Gunalp,Duygu Erisken,Sila Burde Dulger,Rana Irem Turhan,Izzet Ozdemir,Derya Umut Kulali,Ozan Akbulut,Harun Demircioglu,Hasan Basri Kara,Berfin Tavan*

Main category: cs.CL

TL;DR: 本文介绍了TeMLM，一套透明度优先的临床语言模型发布工件，包含数据、建模透明度及治理的一体化机器可校验包，并在大规模合成临床NLP数据集Technetium-I上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 提升临床语言模型发布过程中的透明度、可追溯性及治理，保障模型开发和审计的规范性和可重复性。

Method: 设计了一套包括TeMLM-Card、TeMLM-Datasheet、TeMLM-Provenance的工件套件和轻量级合规清单，应用于合成数据集Technetium-I，并在ProtactiniumBERT模型上进行了PHI去识别和ICD-9代码提取任务。

Result: 在合成临床数据集Technetium-I上取得了参考结果，显示工具和流程的有效性，强调了在真实数据上的验证必要性。

Conclusion: TeMLM实现了临床语言模型发布的高度透明和可审计性，为后续模型在真实临床数据上的验证和部署奠定基础。

Abstract: We introduce TeMLM, a set of transparency-first release artifacts for clinical language models. TeMLM unifies provenance, data transparency, modeling transparency, and governance into a single, machine-checkable release bundle. We define an artifact suite (TeMLM-Card, TeMLM-Datasheet, TeMLM-Provenance) and a lightweight conformance checklist for repeatable auditing. We instantiate the artifacts on Technetium-I, a large-scale synthetic clinical NLP dataset with 498,000 notes, 7.74M PHI entity annotations across 10 types, and ICD-9-CM diagnosis labels, and report reference results for ProtactiniumBERT (about 100 million parameters) on PHI de-identification (token classification) and top-50 ICD-9 code extraction (multi-label classification). We emphasize that synthetic benchmarks are valuable for tooling and process validation, but models should be validated on real clinical data prior to deployment.

</details>


### [12] [Do Images Speak Louder than Words? Investigating the Effect of Textual Misinformation in VLMs](https://arxiv.org/abs/2601.19202)
*Chi Zhang,Wenxuan Ding,Jiale Liu,Mingrui Wu,Qingyun Wu,Ray Mooney*

Main category: cs.CL

TL;DR: 本文提出了一个新的数据集CONTEXT-VQA，专注于研究视觉语言模型面对文本误导信息时的鲁棒性，发现当前模型易受误导文本影响，性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 虽然已有研究关注文本领域内的误导信息影响，但视觉语言模型如何在多模态信息冲突时权衡不同模态的信息尚不清楚，急需系统研究其鲁棒性问题。

Method: 构建CONTEXT-VQA数据集，包含图像-问题对和系统生成的有说服力但与视觉信息冲突的文本提示，设计评估框架测试11个前沿视觉语言模型在误导性多模态输入下的表现。

Result: 通过在11个最先进的视觉语言模型上的全面实验，发现这些模型在存在误导文本提示时，往往会忽视清晰视觉证据，平均性能下降超过48.2%。

Conclusion: 视觉语言模型在面对与视觉证据冲突的误导性文本时表现出严重的脆弱性，性能下降超过48%。这揭示了当前模型鲁棒性的重要不足，需要加强对文本操纵的抵抗能力。

Abstract: Vision-Language Models (VLMs) have shown strong multimodal reasoning capabilities on Visual-Question-Answering (VQA) benchmarks. However, their robustness against textual misinformation remains under-explored. While existing research has studied the effect of misinformation in text-only domains, it is not clear how VLMs arbitrate between contradictory information from different modalities. To bridge the gap, we first propose the CONTEXT-VQA (i.e., Conflicting Text) dataset, consisting of image-question pairs together with systematically generated persuasive prompts that deliberately conflict with visual evidence. Then, a thorough evaluation framework is designed and executed to benchmark the susceptibility of various models to these conflicting multimodal inputs. Comprehensive experiments over 11 state-of-the-art VLMs reveal that these models are indeed vulnerable to misleading textual prompts, often overriding clear visual evidence in favor of the conflicting text, and show an average performance drop of over 48.2% after only one round of persuasive conversation. Our findings highlight a critical limitation in current VLMs and underscore the need for improved robustness against textual manipulation.

</details>


### [13] [How Do Transformers Learn to Associate Tokens: Gradient Leading Terms Bring Mechanistic Interpretability](https://arxiv.org/abs/2601.19208)
*Shawn Im,Changdae Oh,Zhen Fang,Sharon Li*

Main category: cs.CL

TL;DR: 本文揭示了语言模型中语义关联的形成机制，基于训练动态推导权重闭式表达，理论和实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 理解语言模型如何学习和表达语义关联对于连接深度学习与语言学理论、构建大语言模型的机械机制基础至关重要，因此本文旨在通过训练动态深入剖析语义关联的形成机制。

Method: 本文采用梯度的领先项近似，推导训练早期阶段语言模型权重的闭式表达式，分析权重如何由大ram统计、词语可替代性和上下文映射三种基础函数组合而成，进而反映语义关联，并通过实际模型实验验证理论结果。

Result: 本文通过训练动态视角，分析了基于注意力机制的语言模型中语义关联的形成过程，推导出训练早期权重的闭式表达式，揭示了变换器权重可以表示为三种基础函数（大ram、词语可替代性、上下文映射）的简单组合，从而解释了模型如何捕捉语义关联。实验证明理论分析与实际大规模语言模型权重高度一致，且有助于解释模型中学得的语义关联。

Conclusion: 语言模型尤其是基于变换器结构的模型，其权重在训练早期就形成了反映语义关联的结构，这些结构可以用简单的基础函数组合表达，理论分析与实际模型高度匹配，有助于深入理解模型的语义学习机制。

Abstract: Semantic associations such as the link between "bird" and "flew" are foundational for language modeling as they enable models to go beyond memorization and instead generalize and generate coherent text. Understanding how these associations are learned and represented in language models is essential for connecting deep learning with linguistic theory and developing a mechanistic foundation for large language models. In this work, we analyze how these associations emerge from natural language data in attention-based language models through the lens of training dynamics. By leveraging a leading-term approximation of the gradients, we develop closed-form expressions for the weights at early stages of training that explain how semantic associations first take shape. Through our analysis, we reveal that each set of weights of the transformer has closed-form expressions as simple compositions of three basis functions (bigram, token-interchangeability, and context mappings), reflecting the statistics of the text corpus and uncovering how each component of the transformer captures semantic associations based on these compositions. Experiments on real-world LLMs demonstrate that our theoretical weight characterizations closely match the learned weights, and qualitative analyses further show how our theorem shines light on interpreting the learned associations in transformers.

</details>


### [14] [A Hybrid Supervised-LLM Pipeline for Actionable Suggestion Mining in Unstructured Customer Reviews](https://arxiv.org/abs/2601.19214)
*Aakash Trivedi,Aniket Upadhyay,Pratik Narang,Dhruv Kumar,Praveen Kumar*

Main category: cs.CL

TL;DR: 通过结合高召回RoBERTa分类器和调优LLM，实现了从客户评论中精准提取细粒度、可操作建议的有效方法，优于传统单一模型，且经人工验证效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有方法多仅能分类含建议的句子或生成高层次摘要，难以精准提取企业需要的具体改进指令。为解决从混合意图、非结构化客户评论中准确提取可操作建议的问题。

Method: 结合高召回率RoBERTa分类器（使用精度-召回权衡替代策略训练）与经过指令调优的大型语言模型（LLM），组成混合管道进行建议提取、分类、聚类和摘要。

Result: 混合系统在真实酒店和食品数据集上，较仅用提示、基于规则或仅用分类器的基线模型，实现了更高的提取准确率和聚类一致性。人工评估表明提取的建议和摘要清晰、可靠且易于理解。

Conclusion: 混合推理架构显著提升了细粒度可操作建议提取，解决了传统方法难题，但仍面临领域适应和高效本地部署挑战。

Abstract: Extracting actionable suggestions from customer reviews is essential for operational decision-making, yet these directives are often embedded within mixed-intent, unstructured text. Existing approaches either classify suggestion-bearing sentences or generate high-level summaries, but rarely isolate the precise improvement instructions businesses need. We evaluate a hybrid pipeline combining a high-recall RoBERTa classifier trained with a precision-recall surrogate to reduce unrecoverable false negatives with a controlled, instruction-tuned LLM for suggestion extraction, categorization, clustering, and summarization. Across real-world hospitality and food datasets, the hybrid system outperforms prompt-only, rule-based, and classifier-only baselines in extraction accuracy and cluster coherence. Human evaluations further confirm that the resulting suggestions and summaries are clear, faithful, and interpretable. Overall, our results show that hybrid reasoning architectures achieve meaningful improvements fine-grained actionable suggestion mining while highlighting challenges in domain adaptation and efficient local deployment.

</details>


### [15] [DREAMSTATE: Diffusing States and Parameters for Recurrent Large Language Models](https://arxiv.org/abs/2601.19221)
*Liu Xiao*

Main category: cs.CL

TL;DR: 本文提出了DREAMSTATE框架，利用条件扩散变换器建模和编辑RNN内部状态，并设计了结合局部和全局信息的混合架构，实现上下文感知的动态递归机制，验证了模型设计的有效性和可训练性。


<details>
  <summary>Details</summary>
Motivation: 现有RNN，尤其是RWKV具备强大的短程建模能力和高效状态表示，但缺乏对其内部状态作为可编辑知识表示的研究，推动该研究以利用和提升RNN的潜力。

Method: 提出了基于条件扩散变换器(Conditional Diffusion Transformer, DiT)的DREAMSTATE框架，直接建模RNN状态的概率流形，实现状态生成与编辑；设计并实现了结合DiT的混合架构，通过动态调整递归模块参数，实现上下文感知的动态递归机制。

Result: 通过t-SNE可视化和受控生成实验验证了状态表示的结构特性；混合模型在多目标损失下训练稳定，验证了设计的可行性和有效性。

Conclusion: 本文开创性地将RNN的内部状态作为可编辑的知识表示进行研究，提出了DREAMSTATE框架并验证了其结构属性，进一步构建了结合局部优势和全局上下文的混合模型，提升了模型的适应性与表现。

Abstract: Modern Recurrent Neural Networks (RNNs), such as RWKV, are distinguished by their powerful short-range modeling capabilities and efficient fixed-size states, which constitute a core advantage over standard Transformers. However, there is a significant lack of research into their internal state as an editable knowledge representation. To fill this gap, we first explore the representational properties of the RWKV state by proposing the DREAMSTATE framework. This framework utilizes a conditional Diffusion Transformer (DiT) to directly model the probability manifold of the state, enabling its generation and editing. The structural nature of this representation is validated through t-SNE visualizations and controlled generation experiments. After successfully uncovering and modeling the state's representational potential, we further propose a novel hybrid architecture that combines the local advantages of RNNs with global context adaptability. This architecture features a parallel DiT that processes a variable-length global context to dynamically generate and adjust the core recurrent module's WKV parameters, transforming the fixed recurrence mechanism into a context-aware dynamic function. Experiments demonstrate that this hybrid model can be trained stably via a multi-objective loss, validating its design feasibility. Our work not only opens a new research direction for RNN state representation but also provides a concrete architectural reference for future model design. The code is publicly available at: https://huggingface.co/2dgx41s/DreamState.

</details>


### [16] [RPO-RAG: Aligning Small LLMs with Relation-aware Preference Optimization for Knowledge Graph Question Answering](https://arxiv.org/abs/2601.19225)
*Kaehyun Um,KyuHwan Yeom,Haerim Yang,Minyoung Choi,Hyeongjun Yang,Kyong-Ho Lee*

Main category: cs.CL

TL;DR: RPO-RAG框架针对小模型设计，通过语义采样、关系优化与答案中心提示，显著提升小规模LLM在知识图谱问答任务上的表现，接近大模型水平。


<details>
  <summary>Details</summary>
Motivation: 现有基于知识图谱的检索增强生成方法存在语义意识不足的路径采样和弱对齐推理目标的问题，且多依赖规模较大的语言模型，限制了小规模LLM的性能提升。

Method: 提出了RPO-RAG框架，包含三大创新：(1) 查询-路径语义采样策略提供更有信息量的监督信号；(2) 关系感知偏好优化以对齐知识图谱中间推理信号；(3) 基于答案的提示设计，将实体和路径组织成可解释格式。

Result: RPO-RAG在两个KGQA基准数据集WebQSP和CWQ上显著提升小规模模型表现，WebQSP数据集F1提升最高达8.8%，CWQ数据集在8B以下参数模型中取得新SOTA成绩。

Conclusion: RPO-RAG有效提升了小规格语言模型的知识图谱问答推理能力，证明了低资源、小模型在实际设备上的KGQA应用潜力。

Abstract: Large Language Models (LLMs) have recently demonstrated remarkable reasoning abilities, yet hallucinate on knowledge-intensive tasks. Retrieval-augmented generation (RAG) mitigates this issue by grounding answers in external sources, e.g., knowledge graphs (KGs). However, existing KG-based RAG approaches rely on semantics-unaware path sampling and are weakly aligned with KG reasoning objectives, which limits further accuracy gains. They also feed retrieved paths directly into the reasoner without organizing them into answer-centered reasoning paths, hindering small LLMs' ability to leverage the retrieved knowledge. Furthermore, prior works predominantly rely on large LLMs (e.g., ChatGPT/GPT-4) or assume backbones above 7B parameters, leaving sub-7B models underexplored. We address this gap with RPO-RAG, the first KG-based RAG framework specifically designed for small LLMs, to the best of our knowledge. RPO-RAG introduces three key innovations: (1) a query-path semantic sampling strategy that provides informative supervisory signals; (2) a relation-aware preference optimization that aligns training with intermediate KG reasoning signals (e.g., relation); and (3) an answer-centered prompt design that organizes entities and reasoning paths in an interpretable format. Extensive experiments on two benchmark Knowledge Graph Question Answering (KGQA) datasets, WebQSP and CWQ, demonstrate that RPO-RAG effectively bridges the performance gap between small and large language models. On WebQSP, it improves F1 by up to 8.8%, reflecting enhanced answer precision, while on CWQ it achieves new state-of-the-art results among models under 8B parameters in both Hit and F1. Overall, RPO-RAG substantially improves the reasoning capability of small LLMs, even under 3B parameters-highlighting their potential for resource-efficient and practical on-device KGQA applications.

</details>


### [17] [DiaDem: Advancing Dialogue Descriptions in Audiovisual Video Captioning for Multimodal Large Language Models](https://arxiv.org/abs/2601.19267)
*Xinlong Chen,Weihong Lin,Jingyun Hua,Linli Yao,Yue Ding,Bozhou Li,Bohan Zeng,Yang Shi,Qiang Liu,Yuanxing Zhang,Pengfei Wan,Liang Wang,Tieniu Tan*

Main category: cs.CL

TL;DR: 该文提出DiaDem模型，通过数据合成和两阶段训练提升视听视频字幕对话描述准确性，并设计新基准DiaDemBench进行系统评测，优于现有模型，展现出较强的整体性能。


<details>
  <summary>Details</summary>
Motivation: 现有视听视频字幕中的对话描述往往不够精准，影响后续的理解与生成任务。

Method: 提出DiaDem模型，首先合成高质量数据集用于SFT，然后采用难度划分的两阶段GRPO策略以提升对话描述能力。同时设计DiaDemBench基准，系统评估对话描述表现。

Result: DiaDem在对话描述准确度上优于Gemini系列，并在通用视听字幕任务中表现具有竞争力。通过DiaDemBench评测发现商业模型在对话感知字幕方面仍有较大提升空间。

Conclusion: DiaDem有效提升了视听视频字幕中对话描述的准确性，验证了该方法在对话感知字幕生成上的优越性。

Abstract: Accurate dialogue description in audiovisual video captioning is crucial for downstream understanding and generation tasks. However, existing models generally struggle to produce faithful dialogue descriptions within audiovisual captions. To mitigate this limitation, we propose DiaDem, a powerful audiovisual video captioning model capable of generating captions with more precise dialogue descriptions while maintaining strong overall performance. We first synthesize a high-quality dataset for SFT, then employ a difficulty-partitioned two-stage GRPO strategy to further enhance dialogue descriptions. To enable systematic evaluation of dialogue description capabilities, we introduce DiaDemBench, a comprehensive benchmark designed to evaluate models across diverse dialogue scenarios, emphasizing both speaker attribution accuracy and utterance transcription fidelity in audiovisual captions. Extensive experiments on DiaDemBench reveal even commercial models still exhibit substantial room for improvement in dialogue-aware captioning. Notably, DiaDem not only outperforms the Gemini series in dialogue description accuracy but also achieves competitive performance on general audiovisual captioning benchmarks, demonstrating its overall effectiveness.

</details>


### [18] [Riddle Quest : The Enigma of Words](https://arxiv.org/abs/2601.19273)
*Niharika Sri Parasa,Chaitali Diwan,Srinath Srinivasa*

Main category: cs.CL

TL;DR: 本文提出了一个基于类比的谜语生成和评估系统，包括事实构建、语义映射、谜语线索生成和答案验证四个模块。研究发现大语言模型通常能猜出主要答案，但常漏掉其他合理解答，显示谜语在测试语言模型推理和歧义处理中的价值。


<details>
  <summary>Details</summary>
Motivation: 谜语作为一种含蓄、隐喻和游戏性质的语言谜题，能考验解答者的模式识别和推理能力，有助于评估语言模型的推理覆盖和处理歧义的能力。

Method: 设计了一个包括三元组构建器、语义映射器、风格化生成器和答案验证器的管线系统，用于生成基于类比的谜语，并利用答案验证器评估大语言模型的答案覆盖率。

Result: 通过案例研究发现大语言模型通常能猜出谜语的主要答案，但经常忽略其他潜在有效答案，从而揭示谜语在检测模型推理和歧义处理方面的优势。

Conclusion: 大语言模型能够识别谜语的主要答案，但对其他可能的有效答案覆盖不足，表明谜语是评估模型推理广度和歧义处理能力的有效工具。

Abstract: Riddles are concise linguistic puzzles that describe an object or idea through indirect, figurative, or playful clues. They are a longstanding form of creative expression, requiring the solver to interpret hints, recognize patterns, and draw inferences to identify the answers. In this work, we introduce a simple pipeline for creating and evaluating analogy-based riddles. The system includes a triples creator that builds structured facts about a concept, a semantic mapper that selects attributes useful for analogy, a stylized generator that turns them into riddle clues, and a validator that collects all possible answers the riddle could point to. We use this validator to study whether large language models can recover the full answer set for different riddle types. Our case study shows that while models often guess the main intended answer, they frequently miss other valid interpretations. This highlights the value of riddles as a lightweight tool for examining reasoning coverage and ambiguity handling in language models.

</details>


### [19] [DART: Diffusion-Inspired Speculative Decoding for Fast LLM Inference](https://arxiv.org/abs/2601.19278)
*Fuliang Liu,Xue Li,Ketai Zhao,Yinxi Gao,Ziyan Zhou,Zhonghui Zhang,Zhibin Wang,Wanchun Dou,Sheng Zhong,Chen Tian*

Main category: cs.CL

TL;DR: DART通过并行生成和高效树剪枝提升了投机解码速度，远超当前主流方法EAGLE3。


<details>
  <summary>Details</summary>
Motivation: 现有的基于模型的草稿设计（如EAGLE3）在提高准确性的同时伴随着多步骤自回归推断，导致草稿阶段延迟高，成为性能瓶颈。

Method: 提出DART方法，利用扩散式大语言模型的启发，采用并行生成策略，在单次前向传播中并行预测多个未来遮蔽位置的logits，消除自回归展开，并结合高效的树剪枝算法构建具备N-gram语义连续性的高质量草稿树。

Result: DART显著减少了草稿阶段的开销，保持了高度准确性，提升了端到端的解码速度。实验证明DART在多个数据集上实现了2.03x至3.44x的时间加速，平均比EAGLE3快30%。

Conclusion: DART提供了一种实用的投机解码框架，显著提升了LLM推理的速度和效率。

Abstract: Speculative decoding is an effective and lossless approach for accelerating LLM inference. However, existing widely adopted model-based draft designs, such as EAGLE3, improve accuracy at the cost of multi-step autoregressive inference, resulting in high drafting latency and ultimately rendering the drafting stage itself a performance bottleneck. Inspired by diffusion-based large language models (dLLMs), we propose DART, which leverages parallel generation to reduce drafting latency. DART predicts logits for multiple future masked positions in parallel within a single forward pass based on hidden states of the target model, thereby eliminating autoregressive rollouts in the draft model while preserving a lightweight design. Based on these parallel logit predictions, we further introduce an efficient tree pruning algorithm that constructs high-quality draft token trees with N-gram-enforced semantic continuity. DART substantially reduces draft-stage overhead while preserving high draft accuracy, leading to significantly improved end-to-end decoding speed. Experimental results demonstrate that DART achieves a 2.03x--3.44x wall-clock time speedup across multiple datasets, surpassing EAGLE3 by 30% on average and offering a practical speculative decoding framework. Code is released at https://github.com/fvliang/DART.

</details>


### [20] [ReToP: Learning to Rewrite Electronic Health Records for Clinical Prediction](https://arxiv.org/abs/2601.19286)
*Jesus Lovon-Melgarejo,Jose G. Moreno,Christine Damase-Michel,Lynda Tamine*

Main category: cs.CL

TL;DR: 本文提出了基于大语言模型的ReToP框架，通过结合EHR重写器与临床预测器的端到端训练，提升临床预测任务的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的EHR处理方法大多忽视与具体预测任务的深度整合，限制了预测的准确性。

Method: ReToP采用端到端训练策略，将EHR重写器与临床预测器结合，通过合成伪标签和类别监督贡献分数（CSC）引导重写器生成与预测任务高度相关的临床数据重写。

Result: 实验结果显示ReToP在MIMIC-IV数据库的三项临床任务中超越了多项强基线模型，且在新数据集和任务上具有较好泛化能力。

Conclusion: ReToP框架通过联合训练和任务驱动的重写机制，有效提升了临床预测性能，且具备良好的泛化能力。

Abstract: Electronic Health Records (EHRs) provide crucial information for clinical decision-making. However, their high-dimensionality, heterogeneity, and sparsity make clinical prediction challenging. Large Language Models (LLMs) allowed progress towards addressing this challenge by leveraging parametric medical knowledge to enhance EHR data for clinical prediction tasks. Despite the significant achievements made so far, most of the existing approaches are fundamentally task-agnostic in the sense that they deploy LLMs as EHR encoders or EHR completion modules without fully integrating signals from the prediction tasks. This naturally hinders task performance accuracy. In this work, we propose Rewrite-To-Predict (ReToP), an LLM-based framework that addresses this limitation through an end-to-end training of an EHR rewriter and a clinical predictor. To cope with the lack of EHR rewrite training data, we generate synthetic pseudo-labels using clinical-driven feature selection strategies to create diverse patient rewrites for fine-tuning the EHR rewriter. ReToP aligns the rewriter with prediction objectives using a novel Classifier Supervised Contribution (CSC) score that enables the EHR rewriter to generate clinically relevant rewrites that directly enhance prediction. Our ReToP framework surpasses strong baseline models across three clinical tasks on MIMIC-IV. Moreover, the analysis of ReToP shows its generalizability to unseen datasets and tasks with minimal fine-tuning while preserving faithful rewrites and emphasizing task-relevant predictive features.

</details>


### [21] [MetaGen: Self-Evolving Roles and Topologies for Multi-Agent LLM Reasoning](https://arxiv.org/abs/2601.19290)
*Yimeng Wang,Jiaxing Zhao,Hongbin Xie,Hexing Ma,Yuzhen Lei,Shuangxue Liu,Xuan Song,Zichen Zhang,Haoran Zhang*

Main category: cs.CL

TL;DR: 本文提出MetaGen框架，动态调整多智能体系统中的角色空间和协作拓扑，以提升复杂任务的适应性和推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统依赖固定的角色库和冻结的交互拓扑，导致任务匹配不理想，无法及时适应新证据，且推理成本高昂。

Method: MetaGen通过生成和重写基于查询的角色规范，维持可控的动态角色池，构建受限的执行图，并在执行过程中基于轻量反馈信号迭代更新角色提示和调整结构决策。

Result: 实验表明，MetaGen在代码生成和多步推理基准测试中，较强的多智能体基线模型实现了准确率和推理成本的更优平衡。

Conclusion: MetaGen能够在不更新基础模型权重的情况下，通过动态修改角色定义和协作结构，提升多智能体系统在代码生成和多步推理任务中的准确性和成本效益。

Abstract: Large language models are increasingly deployed as multi-agent systems, where specialized roles communicate and collaborate through structured interactions to solve complex tasks that often exceed the capacity of a single agent. However, most existing systems still rely on a fixed role library and an execution-frozen interaction topology, a rigid design choice that frequently leads to task mismatch, prevents timely adaptation when new evidence emerges during reasoning, and further inflates inference cost. We introduce MetaGen, a training-free framework that adapts both the role space and the collaboration topology at inference time, without updating base model weights. MetaGen generates and rewrites query-conditioned role specifications to maintain a controllable dynamic role pool, then instantiates a constrained execution graph around a minimal backbone. During execution, it iteratively updates role prompts and adjusts structural decisions using lightweight feedback signals. Experiments on code generation and multi-step reasoning benchmarks show that MetaGen improves the accuracy and cost tradeoff over strong multi-agent baselines.

</details>


### [22] [Formula-One Prompting: Adaptive Reasoning Through Equations For Applied Mathematics](https://arxiv.org/abs/2601.19302)
*Natapong Nitarach,Pittawat Taveekitworachai,Kunat Pipatanakul*

Main category: cs.CL

TL;DR: F-1方法通过引入数学方程作为中间表示，提升llm在应用数学问题上的推理效果，大幅优于CoT和PoT。


<details>
  <summary>Details</summary>
Motivation: 现有的CoT和PoT方法没有显式利用应用数学中常见的“建立和回忆数学方程”的步骤，F-1通过引入方程中间表示弥补这一不足，提高了数学推理性能。

Method: 提出了两阶段的F-1方法，第一阶段从问题描述生成数学方程，第二阶段根据方程自动选择CoT、PoT或直接计算策略，在一次llm调用中完成。

Result: 本文提出了一种新的提示方法——Formula-One Prompting（F-1），通过先从问题描述中构建数学方程，然后基于方程选择合适的求解策略（Chain-of-Thought、Program-of-Thought或直接计算），从而提升大型语言模型(llm)在应用数学领域的推理能力。实验证明F-1在多个模型和基准测试中显著优于传统的CoT和PoT方法，尤其在金融数学和物理等应用领域表现提升更为明显。

Conclusion: F-1在应用数学问题中优于CoT和PoT，尤其在金融和物理领域效果显著，证明了引入数学方程作为中间表示的有效性。

Abstract: Prompting techniques such as Chain-of-Thought (CoT) and Program-of-Thought (PoT) improve LLM mathematical reasoning by structuring intermediate steps in natural language or code. However, applied mathematics problems in domains like finance, physics, and cryptography often require recalling or deriving governing equations, a step that current approaches do not explicitly leverage. We propose Formula-One Prompting (F-1), a two-phase approach that uses mathematical equations as an intermediate representation before adaptive solving. F-1 first formulates governing equations from problem descriptions, then selects a solving strategy among CoT, PoT, or direct computation based on the generated equations, all within a single LLM call. Results across five models and four benchmarks show F-1 outperforms CoT by +5.76% and PoT by +8.42% on average. Crucially, gains are largest in applied domains: +13.30% on FinanceMath over CoT, and within OlympiadBench, larger gains on physics (+2.55%) than pure math (+0.44%). This demonstrates that F-1 is more effective than CoT in applied mathematics problems.

</details>


### [23] [When Benchmarks Leak: Inference-Time Decontamination for LLMs](https://arxiv.org/abs/2601.19334)
*Jianzhe Chai,Yu Zhe,Jun Sakuma*

Main category: cs.CL

TL;DR: 本文提出DeconIEP，通过输入嵌入空间的小扰动，在评估阶段抑制测试集污染，保持性能的真实性。


<details>
  <summary>Details</summary>
Motivation: 现有基准评测因训练数据中泄露测试样本导致结果虚高，传统去污染方法要么改变测试集本身，要么损害模型在正常输入上的性能，迫切需要一种既不改变测试集也不显著影响性能的去污染策略。

Method: DeconIEP在评估时施加受控的输入嵌入扰动，由较少污染的参考模型引导，学习生成实例自适应的扰动，以避免模型回忆污染信息。

Result: 本文提出了一个名为DeconIEP的评估框架，用于解决大型语言模型(LLMs)在基准测试中因测试集污染导致性能虚高的问题。该方法通过在评估阶段对输入嵌入空间施加小范围的适应性扰动，引导模型远离记忆驱动的捷径，从而有效抑制污染带来的影响。实验结果显示，DeconIEP在多种开源LLM和基准测试中实现了较强的去污染效果，且对模型在干净输入上的性能影响极小。

Conclusion: DeconIEP框架能有效减少测试集污染对LLM基准评测的影响，同时保持模型在正常输入上的性能不受显著损害。

Abstract: Benchmark-based evaluation is the de facto standard for comparing large language models (LLMs). However, its reliability is increasingly threatened by test set contamination, where test samples or their close variants leak into training data and artificially inflate reported performance. To address this issue, prior work has explored two main lines of mitigation. One line attempts to identify and remove contaminated benchmark items before evaluation, but this inevitably alters the evaluation set itself and becomes unreliable when contamination is moderate or severe. The other line preserves the benchmark and instead suppresses contaminated behavior at evaluation time; however, such interventions often interfere with normal inference and lead to noticeable performance degradation on clean inputs. We propose DeconIEP, a decontamination framework that operates entirely during evaluation by applying small, bounded perturbations in the input embedding space. Guided by a relatively less-contaminated reference model, DeconIEP learns an instance-adaptive perturbation generator that steers the evaluated model away from memorization-driven shortcut pathways. Across multiple open-weight LLMs and benchmarks, extensive empirical results show that DeconIEP achieves strong decontamination effectiveness while incurring only minimal degradation in benign utility.

</details>


### [24] [Cross-Examination Framework: A Task-Agnostic Diagnostic for Information Fidelity in Text-to-Text Generation](https://arxiv.org/abs/2601.19350)
*Tathagata Raha,Clement Christophe,Nada Saadi,Hamza A Javed,Marco AF Pimentel,Ronnie Rajan,Praveenkumar Kanithi*

Main category: cs.CL

TL;DR: CEF提出一种无参考、多维度的文本生成评价框架，通过跨文本提问与交叉审问准确衡量生成文本语义保真度，优于传统指标且无需参考答案。


<details>
  <summary>Details</summary>
Motivation: 传统评价指标难以准确反映语义上的保真性，尤其是在无参考的生成文本任务中，需要一种多维且无参考的评价方法。

Method: 采用Cross-Examination Framework，对源文本和生成文本作为独立知识库，通过生成可验证问题并交叉审问，计算覆盖率、符合度和一致性三个可解释评分。

Result: CEF在翻译、摘要和临床笔记生成任务中表现出良好的稳健性，能够发现遗漏和事实矛盾等关键错误，且与有参考指标高度相关，专家验证显示其对语义错误识别准确。

Conclusion: CEF框架能够有效捕捉生成文本任务中的语义保真度，优于传统指标如BLEU和BERTScore，准确识别关键错误且无需参考答案。

Abstract: Traditional metrics like BLEU and BERTScore fail to capture semantic fidelity in generative text-to-text tasks. We adapt the Cross-Examination Framework (CEF) for a reference-free, multi-dimensional evaluation by treating the source and candidate as independent knowledge bases. CEF generates verifiable questions from each text and performs a cross-examination to derive three interpretable scores: Coverage, Conformity, and Consistency. Validated across translation, summarization and clinical note-generation, our framework identifies critical errors, such as content omissions and factual contradictions, missed by standard metrics. A key contribution is a systematic robustness analysis to select a stable judge model. Crucially, the strong correlation between our reference-free and with-reference modes validates CEF's reliability without gold references. Furthermore, human expert validation demonstrates that CEF mismatching questions align with meaning-altering semantic errors higher than with non-semantic errors, particularly excelling at identifying entity-based and relational distortions.

</details>


### [25] [Binary Token-Level Classification with DeBERTa for All-Type MWE Identification: A Lightweight Approach with Linguistic Enhancement](https://arxiv.org/abs/2601.19360)
*Diego Rossini,Lonneke van der Plas*

Main category: cs.CL

TL;DR: 本研究通过二元标记分类与语言学特征融合，设计了一个高效且参数量小的模型，实现了多词表达式识别的新高性能，优于大规模语言模型。


<details>
  <summary>Details</summary>
Motivation: 当前多词表达式识别依赖大规模模型，参数量大难以部署，需寻找高效、轻量的解决方案。

Method: 采用二元标记级START/END/INSIDE分类替代跨度预测，融合NP分块和依存句法特征，并通过过采样缓解类别不平衡。

Result: 该论文提出了一种综合方法用于多词表达式（MWE）识别，结合了二元标记级分类、语言学特征整合和数据增强。所使用的DeBERTa-v3-large模型在CoAM数据集上达到69.8%的F1值，较最佳模型Qwen-72B提高了12个百分点，但参数量减少了165倍。主要方法包括将检测任务转换为二元标记级的START/END/INSIDE分类，整合NP分块和依存句法特征以辅助识别非连续和名词型MWE，及通过过采样解决训练数据中的严重类别不平衡问题。此外，该方法在STREUSLE数据集上也实现了78.9%的F1，展示了该方法的泛化能力。结果表明，精心设计的小型模型在结构化NLP任务中能够显著优于大型语言模型，具有对资源受限环境的应用价值。

Conclusion: 设计合理的小型模型结合语言学特征和数据增强，可以在多词表达式识别任务中显著超越大型语言模型，并适用于资源受限场景。

Abstract: We present a comprehensive approach for multiword expression (MWE) identification that combines binary token-level classification, linguistic feature integration, and data augmentation. Our DeBERTa-v3-large model achieves 69.8% F1 on the CoAM dataset, surpassing the best results (Qwen-72B, 57.8% F1) on this dataset by 12 points while using 165x fewer parameters. We achieve this performance by (1) reformulating detection as binary token-level START/END/INSIDE classification rather than span-based prediction, (2) incorporating NP chunking and dependency features that help discontinuous and NOUN-type MWEs identification, and (3) applying oversampling that addresses severe class imbalance in the training data. We confirm the generalization of our method on the STREUSLE dataset, achieving 78.9% F1. These results demonstrate that carefully designed smaller models can substantially outperform LLMs on structured NLP tasks, with important implications for resource-constrained deployments.

</details>


### [26] [Do LLMs Truly Benefit from Longer Context in Automatic Post-Editing?](https://arxiv.org/abs/2601.19410)
*Ahrii Kim,Seong-heum Kim*

Main category: cs.CL

TL;DR: 专有大语言模型在文档级自动后编辑中表现优异但无法有效利用上下文，存在成本高和延迟大的问题，需改进长上下文建模方法。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型展现出强翻译能力，其在文档级自动后编辑中的效果尚未明晰，本文旨在系统评估其性能及局限，推动更高效的翻译后编辑技术发展。

Method: 通过对比专有与开源大语言模型，采用简单一次提示的文档级上下文方法，评估APE质量、上下文利用、鲁棒性及效率指标。

Result: 本文系统比较了专有大语言模型（LLMs）与开源模型在自动后编辑（APE）任务中的表现，重点分析了在文档级上下文下的APE质量、上下文行为、鲁棒性和效率。结果显示，专有LLMs在简单一次提示下即可达到接近人类水平的APE质量，但其并未有效利用文档上下文进行错误修正，且虽然对数据中毒攻击表现出较高鲁棒性，却带来较大计算成本和延迟，限制了实际应用。

Conclusion: 专有大语言模型可实现接近人类的自动后编辑效果，但未充分利用文档上下文且计算资源消耗大，未来需探索高效长上下文建模策略以提升实用性。

Abstract: Automatic post-editing (APE) aims to refine machine translations by correcting residual errors. Although recent large language models (LLMs) demonstrate strong translation capabilities, their effectiveness for APE--especially under document-level context--remains insufficiently understood. We present a systematic comparison of proprietary and open-weight LLMs under a naive document-level prompting setup, analyzing APE quality, contextual behavior, robustness, and efficiency.
  Our results show that proprietary LLMs achieve near human-level APE quality even with simple one-shot prompting, regardless of whether document context is provided. While these models exhibit higher robustness to data poisoning attacks than open-weight counterparts, this robustness also reveals a limitation: they largely fail to exploit document-level context for contextual error correction. Furthermore, standard automatic metrics do not reliably reflect these qualitative improvements, highlighting the continued necessity of human evaluation. Despite their strong performance, the substantial cost and latency overheads of proprietary LLMs render them impractical for real-world APE deployment. Overall, our findings elucidate both the promise and current limitations of LLM-based document-aware APE, and point toward the need for more efficient long-context modeling approaches for translation refinement.

</details>


### [27] [KG-CRAFT: Knowledge Graph-based Contrastive Reasoning with LLMs for Enhancing Automated Fact-checking](https://arxiv.org/abs/2601.19447)
*Vítor N. Lourenço,Aline Paes,Tillman Weyde,Audrey Depeige,Mohnish Dubey*

Main category: cs.CL

TL;DR: 本文提出KG-CRAFT方法，结合知识图谱和大语言模型，通过对比性问题提升自动声明验证性能，在真实数据集上表现优异，实现了事实核查的新突破。


<details>
  <summary>Details</summary>
Motivation: 自动化事实核查系统的核心任务是验证声明的真实性，而这需要有效地利用可靠的证据源如文档或知识库。当前方法在结合大语言模型（LLMs）和知识图谱进行事实核查方面仍存在提升空间。

Method: 提出KG-CRAFT方法，首先从声明和相关报告中构建知识图谱，然后基于知识图谱的结构生成情境相关的对比性问题，利用这些问题指导证据报告的提炼，最后将提炼的证据合成为简明摘要，由大语言模型进行真实性评估。

Result: 在两个真实世界数据集（LIAR-RAW和RAWFC）上的广泛评估表明，KG-CRAFT方法在预测性能上达到了新的最先进水平。分析结果详细验证了基于知识图谱的对比推理方法在提升大语言模型事实核查能力方面的有效性。

Conclusion: 利用基于知识图谱的对比性问题引导的方法，可以显著提升大语言模型在自动化事实核查任务中的性能，证明了知识图谱与大语言模型结合的潜力与有效性。

Abstract: Claim verification is a core component of automated fact-checking systems, aimed at determining the truthfulness of a statement by assessing it against reliable evidence sources such as documents or knowledge bases. This work presents KG-CRAFT, a method that improves automatic claim verification by leveraging large language models (LLMs) augmented with contrastive questions grounded in a knowledge graph. KG-CRAFT first constructs a knowledge graph from claims and associated reports, then formulates contextually relevant contrastive questions based on the knowledge graph structure. These questions guide the distillation of evidence-based reports, which are synthesised into a concise summary that is used for veracity assessment by LLMs. Extensive evaluations on two real-world datasets (LIAR-RAW and RAWFC) demonstrate that our method achieves a new state-of-the-art in predictive performance. Comprehensive analyses validate in detail the effectiveness of our knowledge graph-based contrastive reasoning approach in improving LLMs' fact-checking capabilities.

</details>


### [28] [Dynamic Multi-Expert Projectors with Stabilized Routing for Multilingual Speech Recognition](https://arxiv.org/abs/2601.19451)
*Isha Pandey,Ashish Mittal,Vartul Bahuguna,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: 该论文提出了一种稳定的多专家投影器SMEAR-MoE，有效提升多语言语音识别性能，实现更低错误率和语言间有效共享。


<details>
  <summary>Details</summary>
Motivation: 单一投影器难以捕捉多语言多样的声学到语义映射，需要设计新的多专家投影方法以提升多语言语音识别性能。

Method: 提出了SMEAR-MoE，一种稳定的专家混合投影器，确保每个专家都能获得充分梯度，防止专家退化，并允许跨语言共享；并在四种印度语言上系统对比了不同设计。

Result: SMEAR-MoE在四种印度语言上相比单一投影器基线实现了最高7.6%的相对WER降低，同时保持了类似的运行时效率；专家路由分析显示语言相关性导致专家共享。

Conclusion: SMEAR-MoE 通过稳定的多专家投影器设计，有效防止专家退化，支持跨语言共享，从而提升多语言ASR性能。

Abstract: Recent advances in LLM-based ASR connect frozen speech encoders with Large Language Models (LLMs) via lightweight projectors. While effective in monolingual settings, a single projector struggles to capture the diverse acoustic-to-semantic mappings required for multilingual ASR. To address this, we propose SMEAR-MoE, a stabilized Mixture-of-Experts projector that ensures dense gradient flow to all experts, preventing expert collapse while enabling cross-lingual sharing. We systematically compare monolithic, static multi-projector, and dynamic MoE designs across four Indic languages (Hindi, Marathi, Tamil, Telugu). Our SMEAR-MoE achieves strong performance, delivering upto a 7.6% relative WER reduction over the single-projector baseline, while maintaining comparable runtime efficiency. Analysis of expert routing further shows linguistically meaningful specialization, with related languages sharing experts. These results demonstrate that stable multi-expert projectors are key to scalable and robust multilingual ASR.

</details>


### [29] [ClaimPT: A Portuguese Dataset of Annotated Claims in News Articles](https://arxiv.org/abs/2601.19490)
*Ricardo Campos,Raquel Sequeira,Sara Nerea,Inês Cantante,Diogo Folques,Luís Filipe Cunha,João Canavilhas,António Branco,Alípio Jorge,Sérgio Nunes,Nuno Guimarães,Purificação Silvano*

Main category: cs.CL

TL;DR: 本文发布了ClaimPT数据集，专注欧洲葡萄牙语新闻中的事实声明标注，通过高质量注释和基线模型，推动非英语事实核查的发展，助力抵御网络虚假信息传播。


<details>
  <summary>Details</summary>
Motivation: 事实核查工作耗时且难以规模化，尤其在非英语语言中缺乏授权的标注数据，限制了自动事实核查的发展。加速事实核查有助于更有效地对抗网络误信息传播。

Method: 通过与葡萄牙新闻社LUSA合作，收集了1,308篇新闻文章，并由两名训练有素的标注员依据新提出的标注方案进行双重标注，随后由策展人进行验证。同时提供了基线模型以进行事实声明检测。

Result: 构建了包含6,875条注释的欧洲葡萄牙语新闻声明数据集ClaimPT，提供高质量标注和基线模型，奠定了未来低资源语言自动事实核查研究的基础。

Conclusion: 本文介绍了ClaimPT数据集，旨在推进欧洲葡萄牙语事实核查的研究，弥补低资源语言在自动事实核查领域的不足。

Abstract: Fact-checking remains a demanding and time-consuming task, still largely dependent on manual verification and unable to match the rapid spread of misinformation online. This is particularly important because debunking false information typically takes longer to reach consumers than the misinformation itself; accelerating corrections through automation can therefore help counter it more effectively. Although many organizations perform manual fact-checking, this approach is difficult to scale given the growing volume of digital content. These limitations have motivated interest in automating fact-checking, where identifying claims is a crucial first step. However, progress has been uneven across languages, with English dominating due to abundant annotated data. Portuguese, like other languages, still lacks accessible, licensed datasets, limiting research, NLP developments and applications. In this paper, we introduce ClaimPT, a dataset of European Portuguese news articles annotated for factual claims, comprising 1,308 articles and 6,875 individual annotations. Unlike most existing resources based on social media or parliamentary transcripts, ClaimPT focuses on journalistic content, collected through a partnership with LUSA, the Portuguese News Agency. To ensure annotation quality, two trained annotators labeled each article, with a curator validating all annotations according to a newly proposed scheme. We also provide baseline models for claim detection, establishing initial benchmarks and enabling future NLP and IR applications. By releasing ClaimPT, we aim to advance research on low-resource fact-checking and enhance understanding of misinformation in news media.

</details>


### [30] [GradPruner: Gradient-Guided Layer Pruning Enabling Efficient Fine-Tuning and Inference for LLMs](https://arxiv.org/abs/2601.19503)
*Wei Huang,Anda Cheng,Yinggui Wang*

Main category: cs.CL

TL;DR: 本文提出GradPruner，一种基于梯度引导的LLM剪枝方法，在微调早期阶段通过累计梯度评估层重要性，实现40%的参数减小且仅0.99%的精度损失，从而提升下游任务微调的训练和推理效率。


<details>
  <summary>Details</summary>
Motivation: 传统结构化剪枝方法虽能提升推理效率，但通常需要额外的训练、知识蒸馏和结构搜索，导致微调过程依然耗时费力，急需一种既能提升训练效率又提升推理效率的微调剪枝方法。

Method: 利用微调初期各参数的累计梯度构建初始梯度信息累积矩阵（IGIA矩阵），评估层重要性，剪枝不重要的层，并根据符号合并稀疏层以减少符号干扰。

Result: 在含医疗、金融及通用任务的八个下游数据集上，GradPruner实现参数减少40%，准确率仅下降0.99%。

Conclusion: GradPruner有效减少了模型参数数量（40%），并保持了较高的准确率（仅0.99%的下降），显著提高了LLM下游任务微调的效率。

Abstract: Fine-tuning Large Language Models (LLMs) with downstream data is often considered time-consuming and expensive. Structured pruning methods are primarily employed to improve the inference efficiency of pre-trained models. Meanwhile, they often require additional time and memory for training, knowledge distillation, structure search, and other strategies, making efficient model fine-tuning challenging to achieve. To simultaneously enhance the training and inference efficiency of downstream task fine-tuning, we introduce GradPruner, which can prune layers of LLMs guided by gradients in the early stages of fine-tuning. GradPruner uses the cumulative gradients of each parameter during the initial phase of fine-tuning to compute the Initial Gradient Information Accumulation Matrix (IGIA-Matrix) to assess the importance of layers and perform pruning. We sparsify the pruned layers based on the IGIA-Matrix and merge them with the remaining layers. Only elements with the same sign are merged to reduce interference from sign variations. We conducted extensive experiments on two LLMs across eight downstream datasets. Including medical, financial, and general benchmark tasks. The results demonstrate that GradPruner has achieved a parameter reduction of 40% with only a 0.99% decrease in accuracy. Our code is publicly available.

</details>


### [31] [Automated Safety Benchmarking: A Multi-agent Pipeline for LVLMs](https://arxiv.org/abs/2601.19507)
*Xiangyang Zhu,Yuan Tian,Zicheng Zhang,Qi Jia,Chunyi Li,Renrui Zhang,Heng Li,Zongrui Wang,Wei Sun*

Main category: cs.CL

TL;DR: 本文提出了首个自动化的视觉语言大模型安全评估系统VLSafetyBencher，通过四个协作代理自动构建高质量安全评估样本，解决了现有基准构建劳动力密集、复杂度静态及区分能力有限的问题。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型安全评估基准构建复杂且静态，难以应对模型快速发展和新风险，亟需自动化、安全性强的评估系统。

Method: VLSafetyBencher设计了数据预处理、生成、增强和选择四个协作代理，自动化构建和筛选安全评估样本。

Result: 实验验证表明，VLSafetyBencher能快速高效地产生安全评估基准，且该基准在评估中具有较强的区分能力。

Conclusion: VLSafetyBencher能够在一周内以极低成本自动生成高质量安全基准，有效区分模型安全性，展示出最高和最低安全模型间70%的安全率差异。

Abstract: Large vision-language models (LVLMs) exhibit remarkable capabilities in cross-modal tasks but face significant safety challenges, which undermine their reliability in real-world applications. Efforts have been made to build LVLM safety evaluation benchmarks to uncover their vulnerability. However, existing benchmarks are hindered by their labor-intensive construction process, static complexity, and limited discriminative power. Thus, they may fail to keep pace with rapidly evolving models and emerging risks. To address these limitations, we propose VLSafetyBencher, the first automated system for LVLM safety benchmarking. VLSafetyBencher introduces four collaborative agents: Data Preprocessing, Generation, Augmentation, and Selection agents to construct and select high-quality samples. Experiments validates that VLSafetyBencher can construct high-quality safety benchmarks within one week at a minimal cost. The generated benchmark effectively distinguish safety, with a safety rate disparity of 70% between the most and least safe models.

</details>


### [32] [Yunque DeepResearch Technical Report](https://arxiv.org/abs/2601.19578)
*Yuxuan Cai,Xinyi Lai,Peng Yuan,Weiting Liu,Huajian Li,Mingda Li,Xinghua Wang,Shengxie Zheng,Yanchao Hao,Yuyang Yin,Zheng Wei*

Main category: cs.CL

TL;DR: 本文针对自主智能体在长时任务中的挑战，设计了一个层次化且鲁棒的深度研究框架，实现了性能突破并开源落地。


<details>
  <summary>Details</summary>
Motivation: 当前自主智能体在处理复杂、开放式长时间任务时，面临上下文噪声增大、错误传播脆弱性和缺乏模块化扩展性的问题，限制了深度研究能力的发挥。

Method: 设计了三大关键组件：多代理调度系统负责任务分配；动态上下文管理机制结构化子目标信息以减少噪声；主动监督模块通过异常检测和上下文修剪提升系统稳定性。

Result: 提出了Yunque DeepResearch，一个层次化、模块化且鲁棒的框架，通过多代理调度系统、动态上下文管理和主动监督模块，显著提升了长时任务中的性能，并在多个基准测试中达到了最先进表现。

Conclusion: Yunque DeepResearch框架有效解决了长时间复杂任务中的信息过载和错误累积问题，提升了系统的鲁棒性和扩展性，推动了自主智能体深度研究能力的发展。

Abstract: Deep research has emerged as a transformative capability for autonomous agents, empowering Large Language Models to navigate complex, open-ended tasks. However, realizing its full potential is hindered by critical limitations, including escalating contextual noise in long-horizon tasks, fragility leading to cascading errors, and a lack of modular extensibility. To address these challenges, we introduce Yunque DeepResearch, a hierarchical, modular, and robust framework. The architecture is characterized by three key components: (1) a centralized Multi-Agent Orchestration System that routes subtasks to an Atomic Capability Pool of tools and specialized sub-agents; (2) a Dynamic Context Management mechanism that structures completed sub-goals into semantic summaries to mitigate information overload; and (3) a proactive Supervisor Module that ensures resilience through active anomaly detection and context pruning. Yunque DeepResearch achieves state-of-the-art performance across a range of agentic deep research benchmarks, including GAIA, BrowseComp, BrowseComp-ZH, and Humanity's Last Exam. We open-source the framework, reproducible implementations, and application cases to empower the community.

</details>


### [33] [Decompose-and-Formalise: Recursively Verifiable Natural Language Inference](https://arxiv.org/abs/2601.19605)
*Xin Quan,Marco Valentino,Louise A. Dennis,André Freitas*

Main category: cs.CL

TL;DR: 该论文提出了一种分解与形式化框架，用于改进自然语言推理中的自动形式化和推理验证，显著提升了推理解释的准确率和效率。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型和定理证明器结合的推理方法在处理长且结构复杂的自然语言推理时存在自动形式化错误和全局重生成成本高的问题。

Method: 提出分解并形式化框架，包括分解为蕴涵树、底向上验证节点故障、基于局部诊断指导的精炼；引入事件逻辑中的θ-替换以保持论元角色绑定。

Result: 在多个推理任务和五种大规模语言模型上，方法实现解释验证率分别提升26.2%、21.7%、21.6%和48.9%，同时减少了精炼次数和运行时间，保持了强劲的推理准确率。

Conclusion: 该方法通过将前提-假设对分解为原子步骤的蕴涵树，底向上验证并局部精炼，显著提高了解释验证率和推理效率，超过了现有最先进方法。

Abstract: Recent work has shown that integrating large language models (LLMs) with theorem provers (TPs) in neuro-symbolic pipelines helps with entailment verification and proof-guided refinement of explanations for natural language inference (NLI). However, scaling such refinement to naturalistic NLI remains difficult: long, syntactically rich inputs and deep multi-step arguments amplify autoformalisation errors, where a single local mismatch can invalidate the proof. Moreover, current methods often handle failures via costly global regeneration due to the difficulty of localising the responsible span or step from prover diagnostics. Aiming to address these problems, we propose a decompose-and-formalise framework that (i) decomposes premise-hypothesis pairs into an entailment tree of atomic steps, (ii) verifies the tree bottom-up to isolate failures to specific nodes, and (iii) performs local diagnostic-guided refinement instead of regenerating the whole explanation. Moreover, to improve faithfulness of autoformalisation, we introduce $θ$-substitution in an event-based logical form to enforce consistent argument-role bindings. Across a range of reasoning tasks using five LLM backbones, our method achieves the highest explanation verification rates, improving over the state-of-the-art by 26.2%, 21.7%, 21.6% and 48.9%, while reducing refinement iterations and runtime and preserving strong NLI accuracy.

</details>


### [34] [Up to 36x Speedup: Mask-based Parallel Inference Paradigm for Key Information Extraction in MLLMs](https://arxiv.org/abs/2601.19613)
*Xinzhong Wang,Ya Guo,Jing Li,Huan Chen,Yi Tu,Yijie Hong,Gongshen Liu,Huijia Zhu*

Main category: cs.CL

TL;DR: 本文提出PIP并行推理范式，有效解决了KIE任务中多字段提取的效率瓶颈，实现了多字段一次性生成，显著加速推理且保持高精度。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型和多模态模型在KIE任务中由于依赖自回归的序列生成，效率低下，尤其当需要提取多个语义独立字段时，存在显著瓶颈。

Method: 提出了一种名为PIP的并行推理方法，通过使用“[mask]”标记作为所有目标值的占位符，实现了多字段同时生成，从而替代传统逐步自回归推理。配套开发了专门的掩码预训练策略和大规模有监督数据集。

Result: PIP模型在推理速度上相比传统自回归模型提升5-36倍，且性能几乎无下降。

Conclusion: PIP通过并行生成目标字段，大幅提升了KIE任务的推理效率，兼顾准确性，推动了实际应用中可扩展、高效的关键数据提取解决方案的发展。

Abstract: Key Information Extraction (KIE) from visually-rich documents (VrDs) is a critical task, for which recent Large Language Models (LLMs) and Multi-Modal Large Language Models (MLLMs) have demonstrated strong potential. However, their reliance on autoregressive inference, which generates outputs sequentially, creates a significant efficiency bottleneck, especially as KIE tasks often involve extracting multiple, semantically independent fields. To overcome this limitation, we introduce PIP: a Parallel Inference Paradigm for KIE. Our approach reformulates the problem by using "[mask]" tokens as placeholders for all target values, enabling their simultaneous generation in a single forward pass. To facilitate this paradigm, we develop a tailored mask pre-training strategy and construct large-scale supervised datasets. Experimental results show that our PIP-models achieve a 5-36x inference speedup with negligible performance degradation compared to traditional autoregressive base models. By substantially improving efficiency while maintaining high accuracy, PIP paves the way for scalable and practical real-world KIE solutions.

</details>


### [35] [RATE: Reviewer Profiling and Annotation-free Training for Expertise Ranking in Peer Review Systems](https://arxiv.org/abs/2601.19637)
*Weicong Liu,Zixuan Yang,Yibo Zhao,Xiang Li*

Main category: cs.CL

TL;DR: 提出了基于2024-2025年AI/NLP论文的LR-bench最新审稿人分配基准和基于关键词嵌入微调的RATE匹配方法，显著提升审稿人分配准确性。


<details>
  <summary>Details</summary>
Motivation: 目前审稿人分配正面临快速主题变迁导致旧基准失效，代理信号与实际熟悉度匹配差的问题，亟需高效准确的评测基准和匹配方法。

Method: 采用大规模邮件调查获取审稿人自评熟悉度，构建专家注释的LR-bench基准；提取审稿人近期论文关键词构建关键词档案；利用启发式检索信号对嵌入模型弱监督微调，实现审稿人与论文的直接匹配。

Result: 该论文针对LLM时代审稿人分配面临的挑战，提出了LR-bench数据集和RATE方法。LR-bench基于2024-2025年的AI/NLP论文，利用大规模邮件调查获得审稿人自评熟悉度，提供了1055条专家标注数据，保证了数据的高真实性和时效性。RATE方法通过提取审稿人近期论文的关键词构建个人档案，并利用启发式检索信号对嵌入模型进行微调，实现了论文和审稿人档案的直接匹配。实验结果表明，该方法在LR-bench和CMU数据集上均优于现有嵌入基线方法，达到最新性能水平。论文还发布了相关数据集和代码，便于进一步研究和应用。

Conclusion: 本研究成功构建了高质量、时效性的审稿人分配基准LR-bench，设计了有效的审稿人关键词嵌入匹配框架RATE，实现了性能领先，推进了审稿人分配的实际应用。

Abstract: Reviewer assignment is increasingly critical yet challenging in the LLM era, where rapid topic shifts render many pre-2023 benchmarks outdated and where proxy signals poorly reflect true reviewer familiarity. We address this evaluation bottleneck by introducing LR-bench, a high-fidelity, up-to-date benchmark curated from 2024-2025 AI/NLP manuscripts with five-level self-assessed familiarity ratings collected via a large-scale email survey, yielding 1055 expert-annotated paper-reviewer-score annotations. We further propose RATE, a reviewer-centric ranking framework that distills each reviewer's recent publications into compact keyword-based profiles and fine-tunes an embedding model with weak preference supervision constructed from heuristic retrieval signals, enabling matching each manuscript against a reviewer profile directly. Across LR-bench and the CMU gold-standard dataset, our approach consistently achieves state-of-the-art performance, outperforming strong embedding baselines by a clear margin. We release LR-bench at https://huggingface.co/datasets/Gnociew/LR-bench, and a GitHub repository at https://github.com/Gnociew/RATE-Reviewer-Assign.

</details>


### [36] [One Token Is Enough: Improving Diffusion Language Models with a Sink Token](https://arxiv.org/abs/2601.19657)
*Zihou Zhang,Zheyong Xie,Li Zhong,Haifeng Liu,Shaosheng Cao*

Main category: cs.CL

TL;DR: 本文针对扩散语言模型中的移动汇聚现象，提出通过引入特殊汇聚令牌稳定注意力机制，从而提升模型性能和推理稳健性。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型存在移动汇聚现象，导致推理不稳健，需要通过设计机制防止信息过度混合以提升模型稳定性与性能。

Method: 通过修改注意力掩码，引入一个只能关注自身但对其他令牌全局可见的特殊汇聚令牌，用以稳定注意力汇聚现象。

Result: 本文研究了扩散语言模型（DLMs）中的关键不稳定性问题——移动汇聚（moving sink）现象，揭示汇聚令牌在Transformer的值空间中表现为低范数表示，移动汇聚现象实际上是一种防止过度信息混合的保护机制。但其不可预测的位置影响了推理的稳健性。为此，作者提出了引入一个额外的汇聚令牌，通过修改注意力掩码实现，该令牌只能关注自身，同时对其他令牌全局可见。实验结果表明，该方法稳定了注意力汇聚，显著提升了模型性能，并且其效果与该令牌的位置无关，且几乎没有语义内容，验证了其作为稳健结构汇聚的作用。

Conclusion: 引入一个专门的汇聚令牌能够有效稳定扩散语言模型中的注意力汇聚，显著提升性能且不依赖该令牌位置，验证了其作为结构性汇聚的作用。

Abstract: Diffusion Language Models (DLMs) have emerged as a compelling alternative to autoregressive approaches, enabling parallel text generation with competitive performance. Despite these advantages, there is a critical instability in DLMs: the moving sink phenomenon. Our analysis indicates that sink tokens exhibit low-norm representations in the Transformer's value space, and that the moving sink phenomenon serves as a protective mechanism in DLMs to prevent excessive information mixing. However, their unpredictable positions across diffusion steps undermine inference robustness. To resolve this, we propose a simple but effective extra sink token implemented via a modified attention mask. Specifically, we introduce a special token constrained to attend solely to itself, while remaining globally visible to all other tokens. Experimental results demonstrate that introducing a single extra token stabilizes attention sinks, substantially improving model performance. Crucially, further analysis confirms that the effectiveness of this token is independent of its position and characterized by negligible semantic content, validating its role as a robust and dedicated structural sink.

</details>


### [37] [SynCABEL: Synthetic Contextualized Augmentation for Biomedical Entity Linking](https://arxiv.org/abs/2601.19667)
*Adam Remaki,Christel Gérardin,Eulàlia Farré-Maduell,Martin Krallinger,Xavier Tannier*

Main category: cs.CL

TL;DR: SynCABEL利用大语言模型生成合成训练数据，有效缓解了生物医学实体链接中专家标注数据稀缺的问题，提升多语言基准测试表现，显著降低对人工标注的依赖，并通过引入新评价协议认可其临床有效性。


<details>
  <summary>Details</summary>
Motivation: 生物医学实体链接中的专家标注数据极其稀缺，限制了监督学习模型的性能和应用。

Method: 该方法采用大语言模型生成带上下文的合成训练样本，结合解码器模型和引导推理，利用目标知识库中的所有候选概念进行广泛监督，无需人工标注。

Result: 在MedMentions、QUAERO和SPACCC三个多语言基准测试中，SynCABEL建立了新的最先进性能。并且能在减少60%标注数据的情况下，实现与全人工监督相当的效果。此外，引入的LLM裁判协议显示预测的临床有效率得到显著提升。

Conclusion: SynCABEL框架在多语言生物医学实体链接任务中实现了最新的性能水平，减少了对昂贵专家标注数据的需求，并提高了临床有效预测的比例。

Abstract: We present SynCABEL (Synthetic Contextualized Augmentation for Biomedical Entity Linking), a framework that addresses a central bottleneck in supervised biomedical entity linking (BEL): the scarcity of expert-annotated training data. SynCABEL leverages large language models to generate context-rich synthetic training examples for all candidate concepts in a target knowledge base, providing broad supervision without manual annotation. We demonstrate that SynCABEL, when combined with decoder-only models and guided inference establish new state-of-the-art results across three widely used multilingual benchmarks: MedMentions for English, QUAERO for French, and SPACCC for Spanish. Evaluating data efficiency, we show that SynCABEL reaches the performance of full human supervision using up to 60% less annotated data, substantially reducing reliance on labor-intensive and costly expert labeling. Finally, acknowledging that standard evaluation based on exact code matching often underestimates clinically valid predictions due to ontology redundancy, we introduce an LLM-as-a-judge protocol. This analysis reveals that SynCABEL significantly improves the rate of clinically valid predictions. Our synthetic datasets, models, and code are released to support reproducibility and future research.

</details>


### [38] [Component-Level Lesioning of Language Models Reveals Clinically Aligned Aphasia Phenotypes](https://arxiv.org/abs/2601.19723)
*Yifan Wang,Jichen Zheng,Jingyuan Sun,Yunhao Zhang,Chunyu Ye,Jixing Li,Chengqing Zong,Shaonan Wang*

Main category: cs.CL

TL;DR: 本文提出了一种通过选择性扰动大型语言模型（LLMs）中的功能组件来模拟失语症语言生成障碍的方法，验证了该方法可系统地再现失语症的语言表现。


<details>
  <summary>Details</summary>
Motivation: 探讨是否可以通过系统操控大型语言模型来再现脑损伤后失语症的语言生成障碍，为语言康复和功能组织研究提供模型工具。

Method: 提出基于临床的组件级框架，针对Broca和Wernicke失语症关联的功能组件进行选择性扰动，应用于模块化Mixture-of-Experts模型和密集型Transformer，利用语言探测任务解释组件功能，并用西方失语症量表评估影响。

Result: 亚型靶向扰动比随机扰动更能引发系统性和失语症样的退化，模块化模型能更好地实现表型到组件的对应关系，展示了模拟失语症语言生成的潜力。

Conclusion: 结合临床知识的模块化大型语言模型组件扰动框架，为模拟和研究失语症语言功能退化提供了有效平台。

Abstract: Large language models (LLMs) increasingly exhibit human-like linguistic behaviors and internal representations that they could serve as computational simulators of language cognition. We ask whether LLMs can be systematically manipulated to reproduce language-production impairments characteristic of aphasia following focal brain lesions. Such models could provide scalable proxies for testing rehabilitation hypotheses, and offer a controlled framework for probing the functional organization of language. We introduce a clinically grounded, component-level framework that simulates aphasia by selectively perturbing functional components in LLMs, and apply it to both modular Mixture-of-Experts models and dense Transformers using a unified intervention interface. Our pipeline (i) identifies subtype-linked components for Broca's and Wernicke's aphasia, (ii) interprets these components with linguistic probing tasks, and (iii) induces graded impairments by progressively perturbing the top-k subtype-linked components, evaluating outcomes with Western Aphasia Battery (WAB) subtests summarized by Aphasia Quotient (AQ). Across architectures and lesioning strategies, subtype-targeted perturbations yield more systematic, aphasia-like regressions than size-matched random perturbations, and MoE modularity supports more localized and interpretable phenotype-to-component mappings. These findings suggest that modular LLMs, combined with clinically informed component perturbations, provide a promising platform for simulating aphasic language production and studying how distinct language functions degrade under targeted disruptions.

</details>


### [39] [TokenSeek: Memory Efficient Fine Tuning via Instance-Aware Token Ditching](https://arxiv.org/abs/2601.19739)
*Runjia Zeng,Qifan Wang,Qiang Guan,Ruixiang Tang,Lifu Huang,Zhenting Wang,Xueling Zhang,Cheng Han,Dongfang Liu*

Main category: cs.CL

TL;DR: 本论文提出TokenSeek方法，通过实例感知的token筛选显著降低大语言模型微调内存消耗，性能表现优异，且具备良好解释性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型微调过程中内存消耗高，而现有激活优化方法因数据无感性导致微调效率低且不稳定。

Method: 提出TokenSeek，一种基于实例感知的token选择与舍弃的通用插件，适用于多种Transformer模型。

Result: 节省大量微调用内存（如Llama3.2 1B仅需14.8%内存），同时保持或提升性能。

Conclusion: TokenSeek有效降低微调内存消耗且性能优良，其可解释的token选择机制为未来token效率研究提供新视角。

Abstract: Fine tuning has been regarded as a de facto approach for adapting large language models (LLMs) to downstream tasks, but the high training memory consumption inherited from LLMs makes this process inefficient. Among existing memory efficient approaches, activation-related optimization has proven particularly effective, as activations consistently dominate overall memory consumption. Although prior arts offer various activation optimization strategies, their data-agnostic nature ultimately results in ineffective and unstable fine tuning. In this paper, we propose TokenSeek, a universal plugin solution for various transformer-based models through instance-aware token seeking and ditching, achieving significant fine-tuning memory savings (e.g., requiring only 14.8% of the memory on Llama3.2 1B) with on-par or even better performance. Furthermore, our interpretable token seeking process reveals the underlying reasons for its effectiveness, offering valuable insights for future research on token efficiency. Homepage: https://runjia.tech/iclr_tokenseek/

</details>


### [40] [Strong Reasoning Isn't Enough: Evaluating Evidence Elicitation in Interactive Diagnosis](https://arxiv.org/abs/2601.19773)
*Zhuohan Long,Zhijie Bao,Zhongyu Wei*

Main category: cs.CL

TL;DR: 提出了基于模拟患者的交互式医疗咨询评估框架和信息覆盖率指标，发现信息收集是性能瓶颈，提出REFINE策略改善交互效果。


<details>
  <summary>Details</summary>
Motivation: 现有医疗咨询评估缺乏动态交互和证据收集过程的关注，导致无法全面反映代理的咨询能力，迫切需要新的方法评估和提升医疗代理的信息采集能力。

Method: 通过模拟患者和模拟报告员构建交互评估框架，定义信息覆盖率指标，并基于诊断验证设计REFINE策略，引导代理主动收集证据。

Result: 该论文提出了一个交互式医疗咨询评估框架，通过模拟患者和基于原子证据的模拟报告员来显式建模咨询过程，提出信息覆盖率（ICR）以量化代理在交互中收集必要证据的完整性。构建了涵盖常见病到罕见病的EviMed证据库，并对10种不同推理能力的模型进行评价。研究发现，诊断推理能力强并不等于信息收集有效，信息收集不足是互动性能瓶颈。为此，提出了REFINE策略，利用诊断验证引导代理主动解决不确定性，实验表明REFINE在多数据集上优于基线方法，促进模型协作。

Conclusion: 交互式医疗咨询中，信息收集的完整性是限制性能的关键，REFINE策略有效提升了代理主动获取证据的能力和整体表现。

Abstract: Interactive medical consultation requires an agent to proactively elicit missing clinical evidence under uncertainty. Yet existing evaluations largely remain static or outcome-centric, neglecting the evidence-gathering process. In this work, we propose an interactive evaluation framework that explicitly models the consultation process using a simulated patient and a \rev{simulated reporter} grounded in atomic evidences. Based on this representation, we introduce Information Coverage Rate (ICR) to quantify how completely an agent uncovers necessary evidence during interaction. To support systematic study, we build EviMed, an evidence-based benchmark spanning diverse conditions from common complaints to rare diseases, and evaluate 10 models with varying reasoning abilities. We find that strong diagnostic reasoning does not guarantee effective information collection, and this insufficiency acts as a primary bottleneck limiting performance in interactive settings. To address this, we propose REFINE, a strategy that leverages diagnostic verification to guide the agent in proactively resolving uncertainties. Extensive experiments demonstrate that REFINE consistently outperforms baselines across diverse datasets and facilitates effective model collaboration, enabling smaller agents to achieve superior performance under strong reasoning supervision. Our code can be found at https://github.com/NanshineLoong/EID-Benchmark .

</details>


### [41] [LVLMs and Humans Ground Differently in Referential Communication](https://arxiv.org/abs/2601.19792)
*Peter Zeng,Weiling Li,Amie Paige,Zhengxiang Wang,Panagiotis Kaliosis,Dimitris Samaras,Gregory Zelinsky,Susan Brennan,Owen Rambow*

Main category: cs.CL

TL;DR: 本文通过指称交流实验展示了生成式AI在预测人类意图和建立共同认知方面的不足。


<details>
  <summary>Details</summary>
Motivation: 为了让生成式AI代理能够有效地与人类用户合作，准确预测人类意图的能力至关重要，但由于缺乏对共同认知的建模，这种合作能力受到限制。

Method: 设计了一种因子设计的指称交流实验，包含人类-人类、人类-AI、AI-人类及AI-AI四种组合，进行多轮交互以匹配无明显标签的图片，同时发布了数据采集管道、分析工具和356个对话语料库。

Result: 通过一个涉及人类和AI搭档的多轮指称交流实验，揭示了大型视觉语言模型（LVLM）在交互式解决指称表达方面的局限性。

Conclusion: 大型视觉语言模型在理解和解决多轮指称表达中存在局限，需要改进以促进人机协作。

Abstract: For generative AI agents to partner effectively with human users, the ability to accurately predict human intent is critical. But this ability to collaborate remains limited by a critical deficit: an inability to model common ground. Here, we present a referential communication experiment with a factorial design involving director-matcher pairs (human-human, human-AI, AI-human, and AI-AI) that interact with multiple turns in repeated rounds to match pictures of objects not associated with any obvious lexicalized labels. We release the online pipeline for data collection, the tools and analyses for accuracy, efficiency, and lexical overlap, and a corpus of 356 dialogues (89 pairs over 4 rounds each) that unmasks LVLMs' limitations in interactively resolving referring expressions, a crucial skill that underlies human language use.

</details>


### [42] [Zero-Shot Stance Detection in the Wild: Dynamic Target Generation and Multi-Target Adaptation](https://arxiv.org/abs/2601.19802)
*Aohua Li,Yuanshuo Zhang,Ge Gao,Bo Chen,Xiaobing Zhao*

Main category: cs.CL

TL;DR: 本研究提出零样本动态目标立场检测任务，构建数据集并微调大语言模型，显著提升了复杂社交媒体场景下的多目标立场识别性能。


<details>
  <summary>Details</summary>
Motivation: 针对现实社交媒体中目标非预定义且动态复杂的问题，传统基于给定目标的立场检测方法不适用。

Method: 提出了零样本立场检测任务，结合动态目标生成和多目标适应；构建中文社交媒体数据集，并设计多维度评价指标；采用集成和两阶段微调策略对大语言模型进行训练。

Result: 微调的大语言模型在该任务上表现优异，两阶段微调的Qwen2.5-7B达到66.99%的目标识别综合得分，集成微调的DeepSeek-R1-Distill-Qwen-7B立场检测F1得分达79.26%。

Conclusion: 通过动态目标生成与多目标适应，结合大模型微调策略，能够有效解决现实社交媒体中动态复杂目标的零样本立场检测问题。

Abstract: Current stance detection research typically relies on predicting stance based on given targets and text. However, in real-world social media scenarios, targets are neither predefined nor static but rather complex and dynamic. To address this challenge, we propose a novel task: zero-shot stance detection in the wild with Dynamic Target Generation and Multi-Target Adaptation (DGTA), which aims to automatically identify multiple target-stance pairs from text without prior target knowledge. We construct a Chinese social media stance detection dataset and design multi-dimensional evaluation metrics. We explore both integrated and two-stage fine-tuning strategies for large language models (LLMs) and evaluate various baseline models. Experimental results demonstrate that fine-tuned LLMs achieve superior performance on this task: the two-stage fine-tuned Qwen2.5-7B attains the highest comprehensive target recognition score of 66.99%, while the integrated fine-tuned DeepSeek-R1-Distill-Qwen-7B achieves a stance detection F1 score of 79.26%.

</details>


### [43] [When Iterative RAG Beats Ideal Evidence: A Diagnostic Study in Scientific Multi-hop Question Answering](https://arxiv.org/abs/2601.19827)
*Mahdi Astaraki,Mohammad Arshi Saloot,Ali Shiraee Kasmaee,Hamidreza Mahyar,Soheila Samiee*

Main category: cs.CL

TL;DR: 本文通过化学领域多跳推理问答，验证了迭代检索推理在性能上显著优于静态RAG，揭示其优势及限制，推动了专门科学领域RAG系统的实用应用与优化。


<details>
  <summary>Details</summary>
Motivation: 当前检索增强生成（RAG）方法在科学领域存在多跳推理、领域知识稀疏以及异构证据的挑战，尚不清楚迭代检索推理环节是否能显著优于静态RAG。

Method: 设计三种测试模式（无上下文、完美上下文、迭代RAG），利用ChemKGMultiHopQA数据集进行化学领域多跳推理任务，采用训练无关的控制器实现迭代检索、假设优化和证据感知的停止机制，并通过诊断分析检索覆盖、查询质量、推理准确性等指标。

Result: 迭代RAG方法普遍优于理想化的静态上下文RAG，最高提升25.6个百分点，尤其对未经过推理微调的模型增益明显。分阶段检索有效减少晚期跳数失败，缓解上下文过载，动态校正早期假设偏差。

Conclusion: 分阶段迭代检索推理比仅依赖完美证据更为关键，但仍存在检索覆盖不全、错误路径陷阱、停止机制失调和组合失败等问题。研究为科学领域中部署和诊断RAG系统提供了实践指导与理论基础，有助于构建更可靠可控的迭代检索推理框架。

Abstract: Retrieval-Augmented Generation (RAG) extends large language models (LLMs) beyond parametric knowledge, yet it is unclear when iterative retrieval-reasoning loops meaningfully outperform static RAG, particularly in scientific domains with multi-hop reasoning, sparse domain knowledge, and heterogeneous evidence. We provide the first controlled, mechanism-level diagnostic study of whether synchronized iterative retrieval and reasoning can surpass an idealized static upper bound (Gold Context) RAG. We benchmark eleven state-of-the-art LLMs under three regimes: (i) No Context, measuring reliance on parametric memory; (ii) Gold Context, where all oracle evidence is supplied at once; and (iii) Iterative RAG, a training-free controller that alternates retrieval, hypothesis refinement, and evidence-aware stopping. Using the chemistry-focused ChemKGMultiHopQA dataset, we isolate questions requiring genuine retrieval and analyze behavior with diagnostics spanning retrieval coverage gaps, anchor-carry drop, query quality, composition fidelity, and control calibration. Across models, Iterative RAG consistently outperforms Gold Context, with gains up to 25.6 percentage points, especially for non-reasoning fine-tuned models. Staged retrieval reduces late-hop failures, mitigates context overload, and enables dynamic correction of early hypothesis drift, but remaining failure modes include incomplete hop coverage, distractor latch trajectories, early stopping miscalibration, and high composition failure rates even with perfect retrieval. Overall, staged retrieval is often more influential than the mere presence of ideal evidence; we provide practical guidance for deploying and diagnosing RAG systems in specialized scientific settings and a foundation for more reliable, controllable iterative retrieval-reasoning frameworks.

</details>


### [44] [Identifying and Transferring Reasoning-Critical Neurons: Improving LLM Inference Reliability via Activation Steering](https://arxiv.org/abs/2601.19847)
*Fangan Dong,Zuming Yan,Xuri Ge,Zhiwei Xu,Mengqi Zhang,Xuanang Chen,Ben He,Xin Xin,Zhumin Chen,Ying Zhou*

Main category: cs.CL

TL;DR: 通过对关键神经元激活的自适应调控，AdaRAS显著提升了大型语言模型在复杂推理任务中的表现，且无需额外训练或采样成本。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型具备强大推理能力，但在复杂任务上依赖后训练或昂贵采样限制了实用性，因此希望通过轻量、实时的激活调控提升模型推理的可靠性和效率。

Method: 基于对推理正确性与部分神经元激活高度相关的发现，AdaRAS采用极性感知的均值差分准则识别推理关键神经元（RCNs），然后在推理时自适应地调整这些神经元的激活，提升错误推理的激活表现，避免对正确推理造成影响。

Result: 该论文提出了一种名为AdaRAS的轻量级推理改进框架，通过选择性干预大型语言模型中的关键神经元激活，提升其推理的可靠性。实验表明，该方法在多个数学和编码基准测试中显著提高性能，且具有良好的跨数据集迁移能力和扩展性，优于现有的后训练或采样策略。

Conclusion: AdaRAS有效提升了LLM的推理准确率，实现了13%以上的性能提升，且方法具备迁移性和扩展性，是一种高效实用的推理改进方案。

Abstract: Despite the strong reasoning capabilities of recent large language models (LLMs), achieving reliable performance on challenging tasks often requires post-training or computationally expensive sampling strategies, limiting their practical efficiency. In this work, we first show that a small subset of neurons in LLMs exhibits strong predictive correlations with reasoning correctness. Based on this observation, we propose AdaRAS (Adaptive Reasoning Activation Steering), a lightweight test-time framework that improves reasoning reliability by selectively intervening on neuron activations. AdaRAS identifies Reasoning-Critical Neurons (RCNs) via a polarity-aware mean-difference criterion and adaptively steers their activations during inference, enhancing incorrect reasoning traces while avoiding degradation on already-correct cases. Experiments on 10 mathematics and coding benchmarks demonstrate consistent improvements, including over 13% gains on AIME-24 and AIME-25. Moreover, AdaRAS exhibits strong transferability across datasets and scalability to stronger models, outperforming post-training methods without additional training or sampling cost.

</details>


### [45] [Reflective Translation: Improving Low-Resource Machine Translation via Structured Self-Reflection](https://arxiv.org/abs/2601.19871)
*Nicholas Cheng*

Main category: cs.CL

TL;DR: 针对低资源语言翻译困难，本文提出一种基于自我反思的提示式框架，有效提升翻译质量，无需微调，验证了该方法的有效性和通用性。


<details>
  <summary>Details</summary>
Motivation: 低资源语言缺乏平行数据和语言资源，传统机器翻译效果不佳，利用大型语言模型的自我反思能力提升翻译质量。

Method: 提出Reflective Translation框架，模型先生成初始翻译，再进行结构化自我批评，最后利用反思生成改进翻译。

Result: 在英-isiZulu和英-isiXhosa数据集上，Reflective Translation在多个提示策略和置信度阈值下，BLEU和COMET评分均显著提升，提升幅度最高分别达+0.22 BLEU和+0.18 COMET。

Conclusion: 结构化自我反思机制能有效提升低资源语言（如isiZulu和isiXhosa）的机器翻译质量，且方法简单，无需模型微调。

Abstract: Low-resource languages such as isiZulu and isiXhosa face persistent challenges in machine translation due to limited parallel data and linguistic resources. Recent advances in large language models suggest that self-reflection, prompting a model to critique and revise its own outputs, can improve reasoning quality and factual consistency. Building on this idea, this paper introduces Reflective Translation, a prompt-based framework in which a model generates an initial translation, produces a structured self-critique, and then uses this reflection to generate a refined translation. The approach is evaluated on English-isiZulu and English-isiXhosa translation using OPUS-100 and NTREX-African, across multiple prompting strategies and confidence thresholds. Results show consistent improvements in both BLEU and COMET scores between first- and second-pass translations, with average gains of up to +0.22 BLEU and +0.18 COMET. Statistical significance testing using paired nonparametric tests confirms that these improvements are robust. The proposed method is model-agnostic, requires no fine-tuning, and introduces a reflection-augmented dataset that can support future supervised or analysis-driven work. These findings demonstrate that structured self-reflection is a practical and effective mechanism for improving translation quality in low-resource settings.

</details>


### [46] [Evaluation of Oncotimia: An LLM based system for supporting tumour boards](https://arxiv.org/abs/2601.19899)
*Luis Lorenzo,Marcos Montana-Mendez,Sergio Figueiras,Miguel Boubeta,Cristobal Bernardo-Castineira*

Main category: cs.CL

TL;DR: ONCOTIMIA系统利用生成式人工智能自动完成肺癌肿瘤委员会表单，显著减轻文档负担且保持数据质量，证明了技术可行性和运营可行性。


<details>
  <summary>Details</summary>
Motivation: 多学科肿瘤委员会在肿瘤学决策中至关重要，但手动处理大量异构临床信息造成显著的文档负担。希望利用生成式人工智能减少文档负担，提高效率。

Method: 结合多层数据湖、混合关系和向量存储、基于检索增强生成（RAG）和规则驱动的自适应表单模型，使用多种大型语言模型（LLMs）通过AWS Bedrock自动完成肺癌多学科肿瘤委员会表单。

Result: 系统在十个肺癌病例中测试六种LLMs，最优配置达到80%的正确字段自动完成率，并且大多数模型响应时间临床可接受。较大和较新的模型在准确度方面表现最佳且延迟合理。

Conclusion: 基于大型语言模型的自动表单填写在多学科肺癌工作流程中技术上可行且实际可行，有望显著减少文档负担，同时保证临床数据质量。

Abstract: Multidisciplinary tumour boards (MDTBs) play a central role in oncology decision-making but require manual processes and structuring large volumes of heterogeneous clinical information, resulting in a substantial documentation burden. In this work, we present ONCOTIMIA, a modular and secure clinical tool designed to integrate generative artificial intelligence (GenAI) into oncology workflows and evaluate its application to the automatic completion of lung cancer tumour board forms using large language models (LLMs). The system combines a multi-layer data lake, hybrid relational and vector storage, retrieval-augmented generation (RAG) and a rule-driven adaptive form model to transform unstructured clinical documentation into structured and standardised tumour board records. We assess the performance of six LLMs deployed through AWS Bedrock on ten lung cancer cases, measuring both completion form accuracy and end-to-end latency. The results demonstrate high performance across models, with the best performing configuration achieving an 80% of correct field completion and clinically acceptable response time for most LLMs. Larger and more recent models exhibit best accuracies without incurring prohibitive latency. These findings provide empirical evidence that LLM- assisted autocompletion form is technically feasible and operationally viable in multidisciplinary lung cancer workflows and support its potential to significantly reduce documentation burden while preserving data quality.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [47] [Automated structural testing of LLM-based agents: methods, framework, and case studies](https://arxiv.org/abs/2601.18827)
*Jens Kohl,Otto Kruse,Youssef Mostafa,Andre Luckow,Karsten Schroer,Thomas Riedl,Ryan French,David Katz,Manuel P. Luitz,Tanrajbir Takher,Ken E. Friedl,Céline Laurent-Winter*

Main category: cs.SE

TL;DR: 本文提出基于轨迹捕捉和模拟技术的LLM代理结构化测试方法，实现自动化和技术层面深度测试，提高质量且降低成本。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理测试依赖用户接受度评价，需人工劳动，不易自动化，难定位根因且成本高。

Method: 基于OpenTelemetry的轨迹捕捉、模拟方法保证LLM行为一致性、断言实现自动化测试验证。

Result: 提出结构化测试方法支持深层技术级组件和交互测试，实现自动化测试，提高测试覆盖率和效率，支持回归测试等软件工程最佳实践。

Conclusion: 结构化测试使LLM代理测试更自动化、高效和深入，促进软件工程最佳实践应用，显著提升测试覆盖和缺陷检测能力。

Abstract: LLM-based agents are rapidly being adopted across diverse domains. Since they interact with users without supervision, they must be tested extensively. Current testing approaches focus on acceptance-level evaluation from the user's perspective. While intuitive, these tests require manual evaluation, are difficult to automate, do not facilitate root cause analysis, and incur expensive test environments. In this paper, we present methods to enable structural testing of LLM-based agents. Our approach utilizes traces (based on OpenTelemetry) to capture agent trajectories, employs mocking to enforce reproducible LLM behavior, and adds assertions to automate test verification. This enables testing agent components and interactions at a deeper technical level within automated workflows. We demonstrate how structural testing enables the adaptation of software engineering best practices to agents, including the test automation pyramid, regression testing, test-driven development, and multi-language testing. In representative case studies, we demonstrate automated execution and faster root-cause analysis. Collectively, these methods reduce testing costs and improve agent quality through higher coverage, reusability, and earlier defect detection. We provide an open source reference implementation on GitHub.

</details>


### [48] [Reducing False Positives in Static Bug Detection with LLMs: An Empirical Study in Industry](https://arxiv.org/abs/2601.18844)
*Xueying Du,Jiayi Feng,Yi Zou,Wei Xu,Jie Ma,Wei Zhang,Sisi Liu,Xin Peng,Yiling Lou*

Main category: cs.SE

TL;DR: 通过腾讯企业环境的大规模实证研究，证明了基于大语言模型的误报减少技术在工业应用中的高效性和成本优势，同时剖析了其现存的局限性。


<details>
  <summary>Details</summary>
Motivation: 静态分析工具误报率高导致企业级软件审核效率低下，且虽然LLM在开源项目上展现了减少误报的潜力，但其在真实企业环境中的效果尚未明确。

Method: 在腾讯企业定制的静态分析工具上，收集包含三类常见漏洞共433个报警（其中328个误报，105个真报）的数据集，结合开发者访谈和实证数据分析，评估多种基于大语言模型（LLM）的误报减少技术，并探索其与静态分析工具的混合使用效果。

Result: LLM与静态分析的混合方法在工业环境下能够消除94%-98%的误报，召回率高，成本低（每个警报仅需2.1-109.5秒，花费$0.0011-$0.12），显著减少了人工检查时间（每条警报需10-20分钟），大幅提升误报处理效率。

Conclusion: 基于大语言模型的技术在企业静态分析误报减少中展现出巨大潜力，能够显著节省人工资源和时间，但仍存在关键限制，需进一步优化和研究以更好地满足工业需求。

Abstract: Static analysis tools (SATs) are widely adopted in both academia and industry for improving software quality, yet their practical use is often hindered by high false positive rates, especially in large-scale enterprise systems. These false alarms demand substantial manual inspection, creating severe inefficiencies in industrial code review. While recent work has demonstrated the potential of large language models (LLMs) for false alarm reduction on open-source benchmarks, their effectiveness in real-world enterprise settings remains unclear. To bridge this gap, we conduct the first comprehensive empirical study of diverse LLM-based false alarm reduction techniques in an industrial context at Tencent, one of the largest IT companies in China. Using data from Tencent's enterprise-customized SAT on its large-scale Advertising and Marketing Services software, we construct a dataset of 433 alarms (328 false positives, 105 true positives) covering three common bug types. Through interviewing developers and analyzing the data, our results highlight the prevalence of false positives, which wastes substantial manual effort (e.g., 10-20 minutes of manual inspection per alarm). Meanwhile, our results show the huge potential of LLMs for reducing false alarms in industrial settings (e.g., hybrid techniques of LLM and static analysis eliminate 94-98% of false positives with high recall). Furthermore, LLM-based techniques are cost-effective, with per-alarm costs as low as 2.1-109.5 seconds and $0.0011-$0.12, representing orders-of-magnitude savings compared to manual review. Finally, our case analysis further identifies key limitations of LLM-based false alarm reduction in industrial settings.

</details>


### [49] [MulVul: Retrieval-augmented Multi-Agent Code Vulnerability Detection via Cross-Model Prompt Evolution](https://arxiv.org/abs/2601.18847)
*Zihan Wu,Jie Xu,Yun Peng,Chun Yong Chong,Xiaohua Jia*

Main category: cs.SE

TL;DR: 针对大语言模型难以覆盖多样漏洞类别及提示工程难以扩展的问题，MulVul提出多代理检索增强和自动提示演化策略，显著提升了漏洞检测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在漏洞检测中因漏洞模式异质性和手动提示设计难以扩展，导致效果受限。

Method: 提出MulVul框架，采用粗到细的多代理设计（Router代理预测漏洞大类，Detector代理识别具体漏洞类型），结合跨模型提示演化机制自动优化提示，利用检索工具减少生成错误。

Result: 在涵盖130种CWE漏洞类型的数据集上，MulVul实现了34.79%的Macro-F1值，较最佳基线提升41.5%，且跨模型提示演化使性能较手动提示提升51.6%。

Conclusion: MulVul框架通过多代理检索增强策略，显著提升了对多样化软件漏洞的精准检测能力，相较现有方法性能大幅提高。

Abstract: Large Language Models (LLMs) struggle to automate real-world vulnerability detection due to two key limitations: the heterogeneity of vulnerability patterns undermines the effectiveness of a single unified model, and manual prompt engineering for massive weakness categories is unscalable.
  To address these challenges, we propose \textbf{MulVul}, a retrieval-augmented multi-agent framework designed for precise and broad-coverage vulnerability detection. MulVul adopts a coarse-to-fine strategy: a \emph{Router} agent first predicts the top-$k$ coarse categories and then forwards the input to specialized \emph{Detector} agents, which identify the exact vulnerability types. Both agents are equipped with retrieval tools to actively source evidence from vulnerability knowledge bases to mitigate hallucinations.
  Crucially, to automate the generation of specialized prompts, we design \emph{Cross-Model Prompt Evolution}, a prompt optimization mechanism where a generator LLM iteratively refines candidate prompts while a distinct executor LLM validates their effectiveness. This decoupling mitigates the self-correction bias inherent in single-model optimization.
  Evaluated on 130 CWE types, MulVul achieves 34.79\% Macro-F1, outperforming the best baseline by 41.5\%. Ablation studies validate cross-model prompt evolution, which boosts performance by 51.6\% over manual prompts by effectively handling diverse vulnerability patterns.

</details>


### [50] [Towards Safety-Compliant Transformer Architectures for Automotive Systems](https://arxiv.org/abs/2601.18850)
*Sven Kirchner,Nils Purschke,Chengdong Wu,Alois Knoll*

Main category: cs.SE

TL;DR: 本文提出了一种基于多模态Transformer的汽车安全系统架构，通过多样性和冗余提高鲁棒性，实现可认证的自动驾驶AI。


<details>
  <summary>Details</summary>
Motivation: Transformer在视觉和语言任务表现优异，但在安全关键的汽车系统中应用存在挑战，需结合多模态数据的冗余和多样性以提升系统安全性和鲁棒性。

Method: 采用多个独立的模态特定编码器，将不同模态的传感器数据编码后融合到共享潜在空间，实现多模态信息的稳健融合和故障容忍。

Result: 本文提出了一个将Transformer架构集成到汽车安全系统中的概念框架，利用多模态基础模型的传感器多样性和冗余性提升容错和鲁棒性。提出的架构通过多个独立的模态特定编码器，将它们的表示融合到一个共享的潜在空间中，实现即使某一模态退化也能保持功能运行。该方法在表征层面结构化地嵌入冗余和多样性，弥合深度学习与功能安全实践之间的差距，为自动驾驶中的可认证AI系统奠定基础。

Conclusion: 通过融合多模态输入并在表示层面实现冗余和多样性，所提架构支持失效操作行为，提高了自动驾驶系统的安全性和可靠性，有助于实现可认证的AI系统。

Abstract: Transformer-based architectures have shown remarkable performance in vision and language tasks but pose unique challenges for safety-critical applications. This paper presents a conceptual framework for integrating Transformers into automotive systems from a safety perspective. We outline how multimodal Foundation Models can leverage sensor diversity and redundancy to improve fault tolerance and robustness. Our proposed architecture combines multiple independent modality-specific encoders that fuse their representations into a shared latent space, supporting fail-operational behavior if one modality degrades. We demonstrate how different input modalities could be fused in order to maintain consistent scene understanding. By structurally embedding redundancy and diversity at the representational level, this approach bridges the gap between modern deep learning and established functional safety practices, paving the way for certifiable AI systems in autonomous driving.

</details>


### [51] [Tricky$^2$: Towards a Benchmark for Evaluating Human and LLM Error Interactions](https://arxiv.org/abs/2601.18949)
*Cole Granger,Dipin Khati,Daniel Rodriguez-Cardenas,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: 该论文提出一个包含人类与大模型代码错误的混合数据集，用于研究和提升混合错误环境下软件缺陷的检测与修复。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型（LLMs）引入的错误与人类错误之间的交互及其对软件开发工作流的影响。

Method: 构建Tricky$^2$数据集，将GPT-5和OpenAI-oss-20b生成的错误注入到现有人类编写的缺陷代码中，使用分类法引导的提示框架生成机器错误，同时保留原有的人类缺陷和程序结构。

Result: 生成了涵盖人类错误、机器错误及混合错误的混合数据集Tricky$^2$，并通过分类、定位和修复任务的基线评估展示了数据集的有效性。

Conclusion: Tricky$^2$数据集有效支持了混合错误类型的软件缺陷分析与修复，促进理解和提升人机混合代码的可靠性。

Abstract: Large language models (LLMs) are increasingly integrated into software development workflows, yet they often introduce subtle logic or data-misuse errors that differ from human bugs. To study how these two error types interact, we construct Tricky$^2$, a hybrid dataset that augments the existing TrickyBugs corpus of human-written defects with errors injected by both GPT-5 and OpenAI-oss-20b across C++, Python, and Java programs. Our approach uses a taxonomy-guided prompting framework to generate machine-originated bugs while preserving original human defects and program structure. The resulting corpus spans human-only, LLM-only, and human+LLM splits, enabling analysis of mixed-origin error behavior, multi-bug repair robustness, and reliability in hybrid human-machine code. This paper outlines the dataset construction pipeline and illustrates its use through small-scale baseline evaluations of classification, localization, and repair tasks.

</details>


### [52] [The Opaque Pointer Design Pattern in Python: Towards a Pythonic PIMPL for Modularity, Encapsulation, and Stability](https://arxiv.org/abs/2601.19065)
*Antonios Saravanos,John Pazarzis,Stavros Zervoudakis,Dongnanzi Zheng*

Main category: cs.SE

TL;DR: 本论文提出了Pythonic PIMPL模式，用不透明委托实现封装，帮助维护稳定公共API，支持延迟加载和后端切换，适合大型Python库开发和维护。


<details>
  <summary>Details</summary>
Motivation: Python中的内部对象容易被外部访问和依赖，导致重构风险加大，维护困难，需要一种封装机制保持公共API稳定。

Method: 重新解释C++的PIMPL惯用法，将其作为Python中不透明委托的小公有对象及其内部实现对象结构，并结合现有封装技术和实例如模块间间接、外观对象及后端调度进行定位和示范。

Result: 展示Pythonic PIMPL在隔离重依赖、支持延迟导入及运行时后端选择中的应用，并总结其优缺点及适用场景。

Conclusion: Pythonic PIMPL模式能有效隔离内部实现变化，支持延迟加载与后端切换，维护稳定的公共API，有助于大型Python库的长期维护。

Abstract: Python libraries often need to maintain a stable public API even as internal implementations evolve, gain new backends, or depend on heavy optional libraries. In Python, where internal objects are easy to inspect and import, users can come to rely on "reachable internals" that were never intended to be public, making refactoring risky and slowing long-term maintenance. This paper revisits the pointer-to-implementation (PIMPL) idiom from C++ and reinterprets it as a Pythonic pattern of opaque delegation: a small public object (or module) that delegates its behavior to a separate implementation object treated as internal. We situate this pattern within a broader taxonomy of encapsulation techniques in Python, relate it to existing practices such as module-level indirection, facade objects, and backend dispatch, and identify PIMPL-like structures already used in the standard library and the scientific Python ecosystem. We then show how a Pythonic PIMPL can be used in existing codebases to isolate heavy dependencies, support lazy imports, and enable runtime selection of alternative backends without changing the public API. Finally, we discuss the benefits and trade-offs of the approach and offer practical guidance on when the pattern is appropriate and how to apply it in large, long-lived Python libraries.

</details>


### [53] [Dynamic Cogeneration of Bug Reproduction Test in Agentic Program Repair](https://arxiv.org/abs/2601.19066)
*Runxiang Cheng,Michele Tufano,José Cambronero,Renyao Wei,Sherry Shi,Grant Uy,Pat Rondon,Franjo Ivančić*

Main category: cs.SE

TL;DR: 本文提出并验证了在自动程序修复中同时生成修复补丁和错误复现测试的共生生成策略，证明其有效性和工程优势。


<details>
  <summary>Details</summary>
Motivation: 开发者在提交补丁时通常会同时实现错误复现测试以增加信心，但传统APR系统通常分别生成修复和BRT，导致开发维护成本高。研究共生生成可提升效率和信心。

Method: 研究了自动程序修复（APR）中共生生成（cogeneration）策略，即让APR代理同时生成修复补丁和错误复现测试（BRT）。评估了不同共生生成策略对代理行为和修复效果的影响，并开发了包含测试更改信息的补丁选择器。

Result: 共生生成策略能够使APR代理生成的错误复现测试数量至少与专门生成BRT的代理持平，同时不影响生成合理修复的比率，减少了单独维护两个生成流程的工程负担。

Conclusion: 共生生成方法提高了自动程序修复系统在同时生成修复代码和错误复现测试方面的能力，无需分别维护两套生成系统，从而简化了流程并提升了开发者信心。

Abstract: Bug Reproduction Tests (BRTs) have been used in many agentic Automated Program Repair (APR) systems, primarily for validating promising fixes and aiding fix generation. In practice, when developers submit a patch, they often implement the BRT alongside the fix. Our experience deploying agentic APR reveals that developers similarly desire a BRT within AI-generated patches to increase their confidence. However, canonical APR systems tend to generate BRTs and fixes separately, or focus on producing only the fix in the final patch. In this paper, we study agentic APR in the context of cogeneration, where the APR agent is instructed to generate both a fix and a BRT in the same patch. We evaluate the effectiveness of different cogeneration strategies on 120 human-reported bugs at Google and characterize different cogeneration strategies by their influence on APR agent behavior. We develop and evaluate patch selectors that account for test change information to select patches with plausible fixes (and plausible BRTs). Finally, we analyze the root causes of failed cogeneration trajectories. Importantly, we show that cogeneration allows the APR agent to generate BRTs for at least as many bugs as a dedicated BRT agent, without compromising the generation rate of plausible fixes, thereby reducing engineering effort in maintaining and coordinating separate generation pipelines for fix and BRT at scale.

</details>


### [54] [HalluJudge: A Reference-Free Hallucination Detection for Context Misalignment in Code Review Automation](https://arxiv.org/abs/2601.19072)
*Kla Tantithamthavorn,Hong Yi Lin,Patanamon Thongtanunam,Wachiraphan Charoenwet,Minwoo Jeong,Ming Wu*

Main category: cs.SE

TL;DR: 提出了HalluJudge方法，通过多策略评估，准确、高效地检测LLM生成代码审查评论中的幻觉，提升代码审查过程的可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在代码审查自动化中生成幻觉性评论的问题，保障代码评论的准确性和可信度，促进LLM在代码审查流程中的应用。

Method: 设计了HalluJudge系统，采用四种评估策略，包括直接评估和结构化多分支推理（Tree-of-Thoughts），结合上下文对生成评论的依据进行检测，在Atlassian企业级项目中进行验证。

Result: 本文针对大型语言模型在代码审查自动化中的幻觉问题（生成的评论与实际代码不符）提出了解决方案，设计了HalluJudge系统，用于无参照条件下检测代码审查评论的幻觉。HalluJudge采用四种策略，包括直接评估和结构化多分支推理（如Tree-of-Thoughts），在Atlassian大规模软件项目中进行了综合评测，展示了其高效且经济的性能。实验结果显示，HalluJudge的幻觉检测具有F1得分0.85，平均耗费仅0.009美元，且67%的评估结果与开发者偏好一致。该方法能有效降低开发者接触错误评论，提升AI辅助代码审查的可靠性。

Conclusion: HalluJudge能有效检测代码审查评论中的幻觉，成本低且与开发者偏好高度一致，可作为实际生产环境中AI辅助代码审查的实用保障。

Abstract: Large Language models (LLMs) have shown strong capabilities in code review automation, such as review comment generation, yet they suffer from hallucinations -- where the generated review comments are ungrounded in the actual code -- poses a significant challenge to the adoption of LLMs in code review workflows. To address this, we explore effective and scalable methods for a hallucination detection in LLM-generated code review comments without the reference. In this work, we design HalluJudge that aims to assess the grounding of generated review comments based on the context alignment. HalluJudge includes four key strategies ranging from direct assessment to structured multi-branch reasoning (e.g., Tree-of-Thoughts). We conduct a comprehensive evaluation of these assessment strategies across Atlassian's enterprise-scale software projects to examine the effectiveness and cost-efficiency of HalluJudge. Furthermore, we analyze the alignment between HalluJudge's judgment and developer preference of the actual LLM-generated code review comments in the real-world production. Our results show that the hallucination assessment in HalluJudge is cost-effective with an F1 score of 0.85 and an average cost of $0.009. On average, 67% of the HalluJudge assessments are aligned with the developer preference of the actual LLM-generated review comments in the online production. Our results suggest that HalluJudge can serve as a practical safeguard to reduce developers' exposure to hallucinated comments, fostering trust in AI-assisted code reviews.

</details>


### [55] [Hybrid Fault-Driven Mutation Testing for Python](https://arxiv.org/abs/2601.19088)
*Saba Alimadadi,Golnaz Gharachorlu*

Main category: cs.SE

TL;DR: 本文针对Python动态语言设计了七种新变异算子，提出基于静态和动态分析的变异测试方法，开发工具PyTation，显著提升了测试缺陷检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有的变异测试技术无法充分捕捉动态类型语言（如Python）中的许多常见错误类型。

Method: 提出了一套基于Python常见反模式的七种新变异算子，结合静态和动态分析进行变异测试，以最小化等价变异体。

Result: PyTation工具在13个开源Python应用中产生了补充通用变异算子的变异体，揭示了高覆盖率测试套件的不足，并生成了高比例的独特变异体，低交叉杀死率和低测试重叠率。

Conclusion: 通过引入基于Python反模式的变异算子及混合分析技术，PyTation有效扩展了变异测试的错误模拟范围，提升了测试评估的准确性和有效性。

Abstract: Mutation testing is an effective technique for assessing the effectiveness of test suites by systematically injecting artificial faults into programs. However, existing mutation testing techniques fall short in capturing many types of common faults in dynamically typed languages like Python. In this paper, we introduce a novel set of seven mutation operators that are inspired by prevalent anti-patterns in Python programs, designed to complement the existing general-purpose operators and broaden the spectrum of simulated faults. We propose a mutation testing technique that utilizes a hybrid of static and dynamic analyses to mutate Python programs based on these operators while minimizing equivalent mutants. We implement our approach in a tool called PyTation and evaluate it on 13 open-source Python applications. Our results show that PyTation generates mutants that complement those from general-purpose tools, exhibiting distinct behaviour under test execution and uncovering inadequacies in high-coverage test suites. We further demonstrate that PyTation produces a high proportion of unique mutants, a low cross-kill rate, and a low test overlap ratio relative to baseline tools, highlighting its novel fault model. PyTation also incurs few equivalent mutants, aided by dynamic analysis heuristics.

</details>


### [56] [Reward Engineering for Reinforcement Learning in Software Tasks](https://arxiv.org/abs/2601.19100)
*Md Rayhanul Masud,Azmine Toushik Wasi,Salman Rahman,Md Rizwan Parvez*

Main category: cs.SE

TL;DR: 本文首次系统综述了软件任务中强化学习的奖励设计方法，梳理现有技术并提出挑战和建议。


<details>
  <summary>Details</summary>
Motivation: 软件强化学习中设计合理的奖励信号难度较大，且相关工作分散，缺乏系统综述，推动该领域发展需要统一的奖励设计视角。

Method: 通过文献调研和归纳，本文从三个维度系统梳理了软件任务中RL奖励设计的方法和技术。

Result: 本文系统综述了强化学习（RL）在软件相关任务中的奖励设计方法，涵盖代码生成、总结、理解、修复、测试和优化等任务。文章指出软件领域的奖励信号设计比传统RL中更加复杂，常用的奖励代理包括代码是否能编译、测试通过率和质量指标。通过整理现有奖励设计文献，本文从三个维度构建了奖励设计的框架，全面展示了奖励设计的现状。最后，文章提出了当前在奖励设计方面存在的挑战和未来建议。

Conclusion: 奖励信号设计是软件强化学习中的关键挑战，本文总结了现有方法并提出未来改进方向。

Abstract: Reinforcement learning is increasingly used for code-centric tasks. These tasks include code generation, summarization, understanding, repair, testing, and optimization. This trend is growing faster with large language models and autonomous agents. A key challenge is how to design reward signals that make sense for software. In many RL problems, the reward is a clear number. In software, this is often not possible. The goal is rarely a single numeric objective. Instead, rewards are usually proxies. Common proxies check if the code compiles, passes tests, or satisfies quality metrics. Many reward designs have been proposed for code-related tasks. However, the work is scattered across areas and papers. There is no single survey that brings these approaches together and shows the full landscape of reward design for RL in software. In this survey, we provide the first systematic and comprehensive review of reward engineering for RL in software tasks. We focus on existing methods and techniques. We structure the literature along three complementary dimensions, summarizing the reward-design choices within each. We conclude with challenges and recommendations in the reward design space for SE tasks.

</details>


### [57] [Detecting and Correcting Hallucinations in LLM-Generated Code via Deterministic AST Analysis](https://arxiv.org/abs/2601.19106)
*Dipin Khati,Daniel Rodriguez-Cardenas,Paul Pantzer,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: 本文提出了一种基于静态分析的确定性后处理框架，能够高效准确地检测并自动修正大型语言模型生成代码中的知识冲突幻觉，相较于现有非确定性修复方法更可靠。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型生成的代码虽然提升了开发效率，但经常产生知识冲突幻觉（KCH），即细微的语义错误，传统方法如约束解码或非确定性LLM修复手段对该类错误检测和修正效果不理想。

Method: 提出一个基于静态分析的确定性后处理框架，通过将生成代码解析为抽象语法树（AST），并利用库内省构建的动态知识库（KB）进行验证，采用确定性规则检测并自动纠正API和标识符层面的知识冲突幻觉（KCH）。

Result: 在200条手工整理的Python代码片段数据集上，实现了对KCH的100%精确度和87.6%的召回率（F1值0.934），并成功自动修正了77.0%的幻觉错误。

Conclusion: 基于静态分析的确定性后处理方法是检测和自动修正大型语言模型代码生成中的知识冲突幻觉的有效且可靠的替代方案，为实现可信的代码生成提供了明确路径。

Abstract: Large Language Models (LLMs) for code generation boost productivity but frequently introduce Knowledge Conflicting Hallucinations (KCHs), subtle, semantic errors, such as non-existent API parameters, that evade linters and cause runtime failures. Existing mitigations like constrained decoding or non-deterministic LLM-in-the-loop repair are often unreliable for these errors. This paper investigates whether a deterministic, static-analysis framework can reliably detect \textit{and} auto-correct KCHs. We propose a post-processing framework that parses generated code into an Abstract Syntax Tree (AST) and validates it against a dynamically-generated Knowledge Base (KB) built via library introspection. This non-executing approach uses deterministic rules to find and fix both API and identifier-level conflicts. On a manually-curated dataset of 200 Python snippets, our framework detected KCHs with 100\% precision and 87.6\% recall (0.934 F1-score), and successfully auto-corrected 77.0\% of all identified hallucinations. Our findings demonstrate that this deterministic post-processing approach is a viable and reliable alternative to probabilistic repair, offering a clear path toward trustworthy code generation.

</details>


### [58] [The Promise and Reality of Continuous Integration Caching: An Empirical Study of Travis CI Builds](https://arxiv.org/abs/2601.19146)
*Taher A. Ghaleb,Daniel Alencar da Costa,Ying Zou*

Main category: cs.SE

TL;DR: CI缓存虽能显著缩短部分项目的构建时间，但采用率低且维护复杂，主要受限于开发者对缓存的认知不足和缓存管理难题。


<details>
  <summary>Details</summary>
Motivation: 持续集成中的构建时间过长会降低开发效率，尽管CI服务提供了缓存加速机制，但实际采用情况和面临的挑战尚不清楚。

Method: 基于Travis CI中1,279个GitHub项目的513,384个构建数据进行大规模实证研究，并提交启用缓存的拉取请求以获取开发者反馈。

Result: 仅30%的项目采用缓存，采用缓存的项目通常更成熟。近半启用缓存的拉取请求被接受，未采用主要因开发者对缓存支持的认识不足。24%缓存项目会进行缓存维护，33%项目含有陈旧缓存。缓存上传高达97%，构建时间有显著减少，但缓存管理复杂。

Conclusion: CI缓存并非对所有项目都有助益，需持续维护，且实际运用中的复杂性较多开发者预期的高。

Abstract: Continuous Integration (CI) provides early feedback by automatically building software, but long build durations can hinder developer productivity. CI services offer caching mechanisms to speed up builds by reusing infrequently changing artifacts, yet little is known about how caching is adopted in practice and what challenges it entails. In this paper, we conduct a large-scale empirical study of CI caching in Travis CI, analyzing 513,384 builds from 1,279 GitHub projects. We find that only 30% of projects adopt CI caching, and early adoption is strongly associated with project maturity, such as more dependencies, more commits, and longer CI lifespans. To understand why many projects do not adopt caching, we submitted pull requests enabling caching in non-adopting projects, and nearly half were accepted or merged. Developer feedback suggests that non- or late adoption mainly stems from limited awareness of CI caching support. We also examine cache maintenance and identify five common activities, performed by 24% of cache-enabled projects. Although one-third of projects see substantial build-time reductions, cache uploads occur in 97% of builds, and 33% of projects contain stale cached artifacts. Finally, our analysis of reported caching issues shows developers mainly struggle with corrupted or outdated caches or request broader caching features. Overall, CI caching does not help all projects, needs ongoing maintenance, and is more complex in practice than many developers expect.

</details>


### [59] [SE Journals in 2036: Looking Back at the Future We Need to Have](https://arxiv.org/abs/2601.19217)
*Tim Menzies,Paris Avgeriou,Robert Feldt,Mauro Pezzè,Abhik Roychoudhury,Miroslaw Staron,Sebastian Uchitel,Thomas Zimmermann*

Main category: cs.SE

TL;DR: 软件工程出版的传统评审机制已难以适应现代需求，本文从未来视角总结了联盟合作、流程创新及文化变革三大策略，指明了可持续发展的方向。


<details>
  <summary>Details</summary>
Motivation: 当前软件工程领域的出版面临规模性危机，传统的单一同行评审机制因社区规模扩大及新技术引入变得难以持续，阻碍创新并消耗研究者资源。因此有必要探索全新的合作和评审机制来应对这一挑战。

Method: 本论文采用回顾未来的视角，综合多个顶级软件工程期刊的编辑经验，探讨解决同行评审危机的系统方法。方法包括联盟合作（The Journal Alliance）、审稿流程创新（如Lottery机制、任务拆分Unbundling、基准测试优化）、以及文化转变（Cathedrals与Bazaars理念）三大层面。

Result: 通过构建期刊联盟，共享审稿资源与流程；引入随机抽取与任务拆分机制，缓解审稿压力及偏差；优化评测标准，杜绝评价误区，提高评审效率和公正性；最终促进开放共享的文化转型，增强社区合作与创新活力。

Conclusion: 通过合作联盟、创新评审流程及文化变革，可以有效破解当前软件工程出版规模危机，促进更公平高效的评审环境和社区生态，为未来研究提供坚实基础。

Abstract: In 2025, SE publishing faces an existential crisis of scalability. As our communities swell globally and integrate fast-moving methodologies like LLMs, traditional peer-review practices are collapsing under the strain. The "bureaucratic anomaly" of monolithic review has become mathematically unsustainable, creating a stochastic "lottery" that punishes novelty and exhausts researchers.
  This paper, written from the perspective of 2036, documents potential solutions. Here, the editors of ASE, EMSE, IST, JSS, TOSEM and TSE dream a collective dream of a brighter future. In summary first we stopped fighting (The Journal Alliance). Then we fixed the process (The Lottery / Unbundling / Fixing the Benchmark Graveyard). And then we fixed the culture (Cathedrals/Bazaars).

</details>


### [60] [LLM-based Vulnerability Detection at Project Scale: An Empirical Study](https://arxiv.org/abs/2601.19239)
*Fengjie Li,Jiajun Jiang,Dongchi Chen,Yingfei Xiong*

Main category: cs.SE

TL;DR: 本研究首次系统评估了基于大语言模型的漏洞检测工具与传统静态分析工具的性能，结果显示LLM方法能发现更多独特漏洞但召回率低且误报高，且存在计算资源消耗大和运行时间长的问题，强调需要进一步改进工具的鲁棒性和实用性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM技术的发展，探索其在漏洞检测领域的实际效果及与传统工具的优势和不足对比，评估其在真实项目中的实用性和性能瓶颈，推动更高效、可靠的项目级漏洞检测方法的发展。

Method: 采用包含222个C/C++和Java真实漏洞的内部基准测试和24个开源项目实际应用场景，评估五种先进的LLM检测方法与两种传统静态分析工具的检测能力和实用性，同时进行手动警告分析以定位误报原因及性能瓶颈。

Result: 本论文首次对基于大语言模型（LLM）的专用检测器进行了全面的实证研究，并将其与传统静态分析工具在项目规模上进行了比较。评估了五种最新且有代表性的LLM方法和两种传统工具，使用了包含222个真实漏洞的内部基准测试以及24个开源项目中手动检查的385个警告。研究发现LLM检测器在内部基准测试中召回率较低，但能发现更多独特漏洞；两类工具在开源项目中生成大量警告且误报率高，限制了实用性；误报主要由于浅层跨过程推理和错误识别的源/汇对，LLM工具还表现出独特的失败模式；LLM方法计算资源消耗巨大，运行时间长。总结了当前LLM检测工具在鲁棒性、可靠性与扩展性方面的关键限制，并提出未来研究方向以提高项目级漏洞检测的效果和实用性。

Conclusion: 现有基于LLM的漏洞检测方法在召回率、误报率、推理深度及计算效率方面存在显著不足，限制了其实用性和项目规模应用，未来研究需聚焦提升检测的鲁棒性、准确性及扩展能力。

Abstract: In this paper, we present the first comprehensive empirical study of specialized LLM-based detectors and compare them with traditional static analyzers at the project scale. Specifically, our study evaluates five latest and representative LLM-based methods and two traditional tools using: 1) an in-house benchmark of 222 known real-world vulnerabilities (C/C++ and Java) to assess detection capability, and 2) 24 active open-source projects, where we manually inspected 385 warnings to assess their practical usability and underlying root causes of failures. Our evaluation yields three key findings: First, while LLM-based detectors exhibit low recall on the in-house benchmark, they still uncover more unique vulnerabilities than traditional tools. Second, in open-source projects, both LLM-based and traditional tools generate substantial warnings but suffer from very high false discovery rates, hindering practical use. Our manual analysis further reveals shallow interprocedural reasoning and misidentified source/sink pairs as primary failure causes, with LLM-based tools exhibiting additional unique failures. Finally, LLM-based methods incurs substantial computational costs-hundreds of thousands to hundreds of millions of tokens and multi-hour to multi-day runtimes. Overall, our findings underscore critical limitations in the robustness, reliability, and scalability of current LLM-based detectors. We ultimately summarize a set of implications for future research toward more effective and practical project-scale vulnerability detection.

</details>


### [61] ["ENERGY STAR" LLM-Enabled Software Engineering Tools](https://arxiv.org/abs/2601.19260)
*Himon Thakur,Armin Moin*

Main category: cs.SE

TL;DR: 本文研究了集成了大型语言模型的辅助软件工程工具（如CASE工具和IDE）在代码生成中的能耗效率，提出了结合检索增强生成与提示工程的方法以提升质量和能效，并建立了跨多种模型架构的实时能耗与推理时间测量框架。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术无缝嵌入软件工程工具，特别是大量默认激活的AI功能，软件开发生命周期中的能耗模式发生显著变化；研究这类AI增强工具的能耗效率，对于推动环保和可持续的软件工程实践具有重要意义。

Method: 提出结合检索增强生成（RAG）和提示工程技术（PETs）的方法，用于提升代码生成的能效和质量，并构建一个框架实时测量从125M到7B参数规模的不同模型（如GPT-2、CodeLlama等）的能量消耗和推理时间。

Result: 提出的框架有效测量了多种大型语言模型在代码生成时的能量消耗和推理时间，验证了采用RAG与PETs技术提升能效的可行性，并为未来深入的能耗优化研究提供了验证基础。

Conclusion: 结合检索增强生成和提示工程技术可以显著提升基于大型语言模型的代码生成的能效和质量，同时提供了一个可用于评估不同模型能耗表现的框架，为未来更深入的能耗优化研究奠定基础。

Abstract: The discussion around AI-Engineering, that is, Software Engineering (SE) for AI-enabled Systems, cannot ignore a crucial class of software systems that are increasingly becoming AI-enhanced: Those used to enable or support the SE process, such as Computer-Aided SE (CASE) tools and Integrated Development Environments (IDEs). In this paper, we study the energy efficiency of these systems. As AI becomes seamlessly available in these tools and, in many cases, is active by default, we are entering a new era with significant implications for energy consumption patterns throughout the Software Development Lifecycle (SDLC). We focus on advanced Machine Learning (ML) capabilities provided by Large Language Models (LLMs). Our proposed approach combines Retrieval-Augmented Generation (RAG) with Prompt Engineering Techniques (PETs) to enhance both the quality and energy efficiency of LLM-based code generation. We present a comprehensive framework that measures real-time energy consumption and inference time across diverse model architectures ranging from 125M to 7B parameters, including GPT-2, CodeLlama, Qwen 2.5, and DeepSeek Coder. These LLMs, chosen for practical reasons, are sufficient to validate the core ideas and provide a proof of concept for more in-depth future analysis.

</details>


### [62] [Whitespaces Don't Lie: Feature-Driven and Embedding-Based Approaches for Detecting Machine-Generated Code](https://arxiv.org/abs/2601.19264)
*Syed Mehedi Hasan Nirob,Shamim Ehsan,Moqsadur Rahman,Summit Haque*

Main category: cs.SE

TL;DR: 本文研究了如何区分人类与AI生成代码，比较了基于风格特征和基于嵌入的检测方法，发现两者精度相近，各有优势。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型使得从自然语言生成代码变得容易，带来学术诚信和责任人工智能使用的挑战，因此需要有效区分人类与机器代码。

Method: 比较了基于代码轻量级、可解释的风格特征和结构特征的检测器与利用预训练代码编码器的嵌入式检测器，使用包含60万样本的大规模基准数据集进行评估。

Result: 特征模型的ROC-AUC达到0.995，PR-AUC达0.995，F1为0.971，嵌入模型ROC-AUC为0.994，PR-AUC为0.994，F1为0.965。特别是缩进和空白特征对区分效果显著，嵌入模型捕获更深层语义，精度略高。

Conclusion: 基于特征的检测模型和基于嵌入的检测模型均能有效区分人类编写代码与机器生成代码，特征模型在解释性方面表现优越，嵌入模型在捕捉语义方面略胜一筹。

Abstract: Large language models (LLMs) have made it remarkably easy to synthesize plausible source code from natural language prompts. While this accelerates software development and supports learning, it also raises new risks for academic integrity, authorship attribution, and responsible AI use. This paper investigates the problem of distinguishing human-written from machine-generated code by comparing two complementary approaches: feature-based detectors built from lightweight, interpretable stylometric and structural properties of code, and embedding-based detectors leveraging pretrained code encoders. Using a recent large-scale benchmark dataset of 600k human-written and AI-generated code samples, we find that feature-based models achieve strong performance (ROC-AUC 0.995, PR-AUC 0.995, F1 0.971), while embedding-based models with CodeBERT embeddings are also very competitive (ROC-AUC 0.994, PR-AUC 0.994, F1 0.965). Analysis shows that features tied to indentation and whitespace provide particularly discriminative cues, whereas embeddings capture deeper semantic patterns and yield slightly higher precision. These findings underscore the trade-offs between interpretability and generalization, offering practical guidance for deploying robust code-origin detection in academic and industrial contexts.

</details>


### [63] [Understanding Dominant Themes in Reviewing Agentic AI-authored Code](https://arxiv.org/abs/2601.19287)
*Md. Asif Haider,Thomas Zimmermann*

Main category: cs.SE

TL;DR: 研究基于大规模真实GitHub数据，揭示了审查者对AI生成代码的主要关注点及LLM在自动注释审查评论中的有效性，提示AI生成代码仍需人类重点把关特定缺陷。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究关注Agentic AI系统的代码生成能力，但实际中审查员如何响应AI生成代码的情况尚未充分了解，因此开展大规模实证研究分析代码审查动态。

Method: 基于AIDev数据集，分析了3177个AI生成的PR中的19450条代码审查评论，使用主题建模和大语言模型结合语义聚类生成了包含12类评论主题的分类法，利用零样本提示实现自动注释，评估了大语言模型的注释准确性和匹配度。

Result: 发现LLM能较准确地自动注释审查评论，且在PR层面准确识别主要审查主题。审查内容主要集中在功能正确性、逻辑修改、文档不足、重构需求、代码风格格式、测试和安全问题，表明AI虽加速代码生成，但仍需针对性的人类审查监督。

Conclusion: AI代码生成加速了开发进程，但代码审查依然需要重点关注文档、重构、风格、测试和安全等方面，结合自动化注释工具，未来可优化人工审查效率和质量。

Abstract: While prior work has examined the generation capabilities of Agentic AI systems, little is known about how reviewers respond to AI-authored code in practice. In this paper, we present a large-scale empirical study of code review dynamics in agent-generated PRs. Using a curated subset of the AIDev dataset, we analyze 19,450 inline review comments spanning 3,177 agent-authored PRs from real-world GitHub repositories. We first derive a taxonomy of 12 review comment themes using topic modeling combined with large language model (LLM)-assisted semantic clustering and consolidation. According to this taxonomy, we then investigate whether zero-shot prompts to LLM can reliably annotate review comments. Our evaluation against human annotations shows that open-source LLM achieves reasonably high exact match (78.63%), macro F1 score (0.78), and substantial agreement with human annotators at the review comment level. At the PR level, the LLM also correctly identifies the dominant review theme with 78% Top-1 accuracy and achieves an average Jaccard similarity of 0.76, indicating strong alignment with human judgments. Applying this annotation pipeline at scale, we find that apart from functional correctness and logical changes, reviews of agent-authored PRs predominantly focus on documentation gaps, refactoring needs, styling and formatting issues, with testing and security-related concerns. These findings suggest that while AI agents can accelerate code production, there remain gaps requiring targeted human review oversight.

</details>


### [64] [Modeling Sampling Workflows for Code Repositories](https://arxiv.org/abs/2601.19316)
*Romain Lefeuvre,Maïwenn Le Goasteller,Jessie Galasso,Benoit Combemale,Quentin Perez,Houari Sahraoui*

Main category: cs.SE

TL;DR: 提出并验证了一种DSL工具，用于明确描述软件工程中代码库采样策略及其对研究普适性的影响。


<details>
  <summary>Details</summary>
Motivation: 软件工程实证研究依赖于代码库数据集，采样策略设计与评估直接影响研究结果的普适性，但当前采样方法设计与其对普适性的影响分析不足。

Method: 设计了一个领域特定语言，通过组合采样操作符来描述复杂采样策略，实现为基于Python的流畅API，并使用统计指标进行代表性推理。

Result: 提出了一种基于领域特定语言(DSL)的采样策略描述方法，支持复杂采样策略的表达和对研究结果普适性的推理；通过Python API实现，并通过MSR论文案例验证了该方法的有效性。

Conclusion: 该DSL方法能够有效建模现有研究中的采样策略，促进对采样方法代表性与研究普适性的理解和评估。

Abstract: Empirical software engineering research often depends on datasets of code repository artifacts, where sampling strategies are employed to enable large-scale analyses. The design and evaluation of these strategies are critical, as they directly influence the generalizability of research findings. However, sampling remains an underestimated aspect in software engineering research: we identify two main challenges related to (1) the design and representativeness of sampling approaches, and (2) the ability to reason about the implications of sampling decisions on generalizability. To address these challenges, we propose a Domain-Specific Language (DSL) to explicitly describe complex sampling strategies through composable sampling operators. This formalism supports both the specification and the reasoning about the generalizability of results based on the applied sampling strategies. We implement the DSL as a Python-based fluent API, and demonstrate how it facilitates representativeness reasoning using statistical indicators extracted from sampling workflows. We validate our approach through a case study of MSR papers involving code repository sampling. Our results show that the DSL can model the sampling strategies reported in recent literature.

</details>


### [65] [High-quality data augmentation for code comment classification](https://arxiv.org/abs/2601.19383)
*Thomas Borsani,Andrea Rosani,Giuseppe Di Fatta*

Main category: cs.SE

TL;DR: 本文针对代码注释在机器理解中的挑战，提出基于高质量数据生成的合成过采样和增强技术（Q-SYNTH），以解决现有数据集规模小和类别不平衡的问题。


<details>
  <summary>Details</summary>
Motivation: 现有代码注释分类数据集规模有限且类别分布不均，影响分类效果，需改进数据集以提升机器理解能力。

Method: 采用高质量合成数据生成的方法，设计合成质量过采样技术和增强技术，扩充NLBSE'26挑战赛数据集。

Result: 在NLBSE'26挑战赛数据集上，通过Q-SYNTH技术，基线分类器性能提升2.56%。

Conclusion: Q-SYNTH 技术有效提升了注释分类基线分类器性能，准确率提升了2.56%。

Abstract: Code comments serve a crucial role in software development for documenting functionality, clarifying design choices, and assisting with issue tracking. They capture developers' insights about the surrounding source code, serving as an essential resource for both human comprehension and automated analysis. Nevertheless, since comments are in natural language, they present challenges for machine-based code understanding. To address this, recent studies have applied natural language processing (NLP) and deep learning techniques to classify comments according to developers' intentions. However, existing datasets for this task suffer from size limitations and class imbalance, as they rely on manual annotations and may not accurately represent the distribution of comments in real-world codebases. To overcome this issue, we introduce new synthetic oversampling and augmentation techniques based on high-quality data generation to enhance the NLBSE'26 challenge datasets. Our Synthetic Quality Oversampling Technique and Augmentation Technique (Q-SYNTH) yield promising results, improving the base classifier by $2.56\%$.

</details>


### [66] [Bridging the Socio-Emotional Gap: The Functional Dimension of Human-AI Collaboration for Software Engineering](https://arxiv.org/abs/2601.19387)
*Lekshmi Murali Rani,Richard Berntsson Svensson,Robert Feldt*

Main category: cs.SE

TL;DR: 本论文研究了软件从业者如何感知人机协作中的社会情感差距，及人工智能在协作中所需具备的能力，提出了以功能等效替代社会情感智能的设计思路。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI被广泛应用于软件开发团队，理解人机协作中的社会情感智能作用及其在AI系统中的实现，成为提升协作效果的关键。

Method: 通过与10位软件从业者进行半结构化访谈，收集他们对与人类及AI协作中社会情感智能的期望及AI能力需求的看法。

Result: 从业者认为AI缺乏人类的社会情感属性，但这不应视为失败，而是功能性缺失。提出“功能等效”概念，强调通过技术能力实现协作功能的替代，推动人机协作的功能对齐。

Conclusion: 软件从业者将AI视为知识型伙伴而非情感伙伴，期待AI具备的社会情感智能较少，他们认为应通过提升AI的功能能力（如认知、情境智能、适应学习等）实现与人类协作效果的等效替代，而非简单复制人类的社会情感特征。

Abstract: As GenAI models are adopted to support software engineers and their development teams, understanding effective human-AI collaboration (HAIC) is increasingly important. Socio-emotional intelligence (SEI) enhances collaboration among human teammates, but its role in HAIC remains unclear. Current AI systems lack SEI capabilities that humans bring to teamwork, creating a potential gap in collaborative dynamics. In this study, we investigate how software practitioners perceive the socio-emotional gap in HAIC and what capabilities AI systems require for effective collaboration. Through semi-structured interviews with 10 practitioners, we examine how they think about collaborating with human versus AI teammates, focusing on their SEI expectations and the AI capabilities they envision. Results indicate that practitioners currently view AI models as intellectual teammates rather than social partners and expect fewer SEI attributes from them than from human teammates. However, they see the socio-emotional gap not as AIs failure to exhibit SEI traits, but as a functional gap in collaborative capabilities (AIs inability to negotiate responsibilities, adapt contextually, or maintain sustained partnerships). We introduce the concept of functional equivalents: technical capabilities (internal cognition, contextual intelligence, adaptive learning, and collaborative intelligence) that achieve collaborative outcomes comparable to human SEI attributes. Our findings suggest that effective collaboration with AI for SE tasks may benefit from functional design rather than replicating human SEI traits for SE tasks, thereby redefining collaboration as functional alignment.

</details>


### [67] [AACR-Bench: Evaluating Automatic Code Review with Holistic Repository-Level Context](https://arxiv.org/abs/2601.19494)
*Lei Zhang,Yongda Yu,Minghui Yu,Xinxin Guo,Zhengqi Zhuang,Guoping Rong,Dong Shao,Haifeng Shen,Hongyu Kuang,Zhengfeng Li,Boge Wang,Guoan Zhang,Bangyu Xiang,Xiaobing Xu*

Main category: cs.SE

TL;DR: 该论文提出了AACR-Bench，一个支持多语言跨文件上下文的自动代码审查评估基准，采用AI辅助专家验证标注，大幅提升缺陷覆盖率，并揭示了LLM在代码审查中表现受多种因素影响。


<details>
  <summary>Details</summary>
Motivation: 现有自动代码审查评估基准缺乏多语言支持且依赖嘈杂不完整的拉取请求评论标注，限制了评估结果的泛化性和缺陷检测范围。

Method: 提出AI辅助专家验证的标注流程，结合多语言、多文件级别上下文信息，构建了AACR-Bench评估集。

Result: 使用AACR-Bench对主流LLM进行了广泛评估，发现模型性能受上下文粒度、检索方法、LLM类型、编程语言及使用范式影响显著，缺陷覆盖率提升285%。

Conclusion: AACR-Bench作为一个跨多语言、多文件的自动代码审查评估基准，显著提升了缺陷覆盖率，并揭示了以往评估中存在的数据限制，促使对大型语言模型(LLM)的能力有更准确的认识。

Abstract: High-quality evaluation benchmarks are pivotal for deploying Large Language Models (LLMs) in Automated Code Review (ACR). However, existing benchmarks suffer from two critical limitations: first, the lack of multi-language support in repository-level contexts, which restricts the generalizability of evaluation results; second, the reliance on noisy, incomplete ground truth derived from raw Pull Request (PR) comments, which constrains the scope of issue detection. To address these challenges, we introduce AACR-Bench a comprehensive benchmark that provides full cross-file context across multiple programming languages. Unlike traditional datasets, AACR-Bench employs an "AI-assisted, Expert-verified" annotation pipeline to uncover latent defects often overlooked in original PRs, resulting in a 285\% increase in defect coverage. Extensive evaluations of mainstream LLMs on AACR-Bench reveal that previous assessments may have either misjudged or only partially captured model capabilities due to data limitations. Our work establishes a more rigorous standard for ACR evaluation and offers new insights on LLM based ACR, i.e., the granularity/level of context and the choice of retrieval methods significantly impact ACR performance, and this influence varies depending on the LLM, programming language, and the LLM usage paradigm e.g., whether an Agent architecture is employed. The code, data, and other artifacts of our evaluation set are available at https://github.com/alibaba/aacr-bench .

</details>


### [68] [From Scattered to Structured: A Vision for Automating Architectural Knowledge Management](https://arxiv.org/abs/2601.19548)
*Jan Keim,Angelika Kaplan*

Main category: cs.SE

TL;DR: 提出自动化提取和整合软件架构知识的流程，解决异构文档不一致问题，支持一致性检查和自然语言问答。


<details>
  <summary>Details</summary>
Motivation: 软件架构知识分散且易产生不一致，影响维护和知识访问，亟需自动化整合和一致性管理手段。

Method: 开发专用提取器，设计统一知识表示，实施一致性检测，融合检索增强生成技术实现自然语言问答。

Result: 本文提出了一个自动化处理软件架构知识的系统流程，旨在解决架构知识分散在异构软件文档中且存在不一致性的问题。该流程能够从不同类型的文档中提取架构知识，进行链接和一致性检查，最终整合到结构化的知识库中，支持架构符合性检查、变更影响分析以及基于自然语言的问答访问。为实现该目标，作者计划开发专用的知识提取器、设计统一知识表示方案、实现一致性检测机制，并结合检索增强生成技术支持对话式的知识访问。

Conclusion: 通过构建结构化知识库和一致性检查机制，可以有效整合分散的架构知识，提升架构维护和访问的效率。

Abstract: Software architecture is inherently knowledge-centric. The architectural knowledge is distributed across heterogeneous software artifacts such as requirements documents, design diagrams, code, and documentation, making it difficult for developers to access and utilize this knowledge effectively. Moreover, as systems evolve, inconsistencies frequently emerge between these artifacts, leading to architectural erosion and impeding maintenance activities. We envision an automated pipeline that systematically extracts architectural knowledge from diverse artifacts, links them, identifies and resolves inconsistencies, and consolidates this knowledge into a structured knowledge base. This knowledge base enables critical activities such as architecture conformance checking and change impact analysis, while supporting natural language question-answering to improve access to architectural knowledge. To realize this vision, we plan to develop specialized extractors for different artifact types, design a unified knowledge representation schema, implement consistency checking mechanisms, and integrate retrieval-augmented generation techniques for conversational knowledge access.

</details>


### [69] [Toward Architecture-Aware Evaluation Metrics for LLM Agents](https://arxiv.org/abs/2601.19583)
*Débora Souza,Patrícia Machado*

Main category: cs.SE

TL;DR: 本文提出一种基于架构组件的LLM代理评估方法，提升了评估的针对性和诊断能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究多聚焦模型本身，忽视了架构组件对代理行为的影响，导致评估碎片化且诊断能力有限。

Method: 通过将代理的架构组件（如规划器、内存和工具路由器）与其可观测行为及相应的评价指标关联，形成针对性的评估框架。

Result: 该方法能够更清晰地定义测量目标和原因，并通过实际代理案例展示了其应用价值，实现更具针对性、透明性和可操作性的评估。

Conclusion: 提出了一种轻量级且基于架构的评估方法，能更有效地诊断和评估大型语言模型（LLM）代理的行为。

Abstract: LLM-based agents are becoming central to software engineering tasks, yet evaluating them remains fragmented and largely model-centric. Existing studies overlook how architectural components, such as planners, memory, and tool routers, shape agent behavior, limiting diagnostic power. We propose a lightweight, architecture-informed approach that links agent components to their observable behaviors and to the metrics capable of evaluating them. Our method clarifies what to measure and why, and we illustrate its application through real world agents, enabling more targeted, transparent, and actionable evaluation of LLM-based agents.

</details>


### [70] [The Competence Crisis: A Design Fiction on AI-Assisted Research in Software Engineering](https://arxiv.org/abs/2601.19628)
*Mairieli Wessel,Daniel Feitosa,Sangeeth Kochanthara*

Main category: cs.SE

TL;DR: 本文通过设计虚构探讨生成式AI对软件工程研究的影响，呼吁关注技能退化、责任和信任问题。


<details>
  <summary>Details</summary>
Motivation: 当前软件工程研究受到发表压力和生成式AI工具的影响，效率提升的同时带来技能退化、责任归属和信任问题。

Method: 采用设计虚构方法，结合社区调查主题，构建一个近未来研究环境中的虚构场景，作为分析工具而非预测。

Result: 通过虚构场景揭示自动化辅助可能阻碍领域知识能力、验证和指导实践，引发对未来定义能力、责任分配和学习支持的讨论。

Conclusion: 该研究提供了一个警示性的未来情境，促使软件工程研究社区反思如何保持专业能力、明确责任及支持研究学习。

Abstract: Rising publication pressure and the routine use of generative AI tools are reshaping how software engineering research is produced, assessed, and taught. While these developments promise efficiency, they also raise concerns about skill degradation, responsibility, and trust in scholarly outputs. This vision paper employs Design Fiction as a methodological lens to examine how such concerns might materialise if current practices persist. Drawing on themes reported in a recent community survey, we construct a speculative artifact situated in a near future research setting. The fiction is used as an analytical device rather than a forecast, enabling reflection on how automated assistance might impede domain knowledge competence, verification, and mentoring practices. By presenting an intentionally unsettling scenario, the paper invites discussion on how the software engineering research community in the future will define proficiency, allocate responsibility, and support learning.

</details>


### [71] [Who Said CVE? How Vulnerability Identifiers Are Mentioned by Humans, Bots, and Agents in Pull Requests](https://arxiv.org/abs/2601.19636)
*Pien Rooijendijk,Christoph Treude,Mairieli Wessel*

Main category: cs.SE

TL;DR: 本文分析GitHub拉取请求中不同主体对漏洞标识符的使用，发现机器人使用频率高但主要在自动化任务中，人类和代理提及少但用于支持代码修复和讨论。


<details>
  <summary>Details</summary>
Motivation: 尽管漏洞标识符是安全问题的标准化引用，但其在实际开发实践中的具体使用情况尚不清晰，亟需理解各类主体如何利用这些标识符。

Method: 通过分析AIDev pop数据集和相同代码库中扩充的拉取请求数据，比较不同主体对漏洞标识符的提及频率和位置，结合定性分析揭示使用动机。

Result: 发现机器人占所有漏洞标识符提及的69.1%，且多在依赖更新和自动审计中引用，而人类和自主代理则更少提及但覆盖位置更多，主要用于修复、维护及讨论。

Conclusion: 本文揭示了在GitHub拉取请求中，不同主体（机器人、自主代理和人类开发者）对安全漏洞标识符（如CVE、CWE、GHSA）的使用差异。机器人主体占大部分提及，且多集中于描述中，但人类和代理使用更广泛且用于修复和讨论。

Abstract: Vulnerability identifiers such as CVE, CWE, and GHSA are standardised references to known software security issues, yet their use in practice is not well understood. This paper compares vulnerability ID use in GitHub pull requests authored by autonomous agents, bots, and human developers. Using the AIDev pop dataset and an augmented set of pull requests from the same repositories, we analyse who mentions vulnerability identifiers and where they appear. Bots account for around 69.1% of all mentions, usually adding few identifiers in pull request descriptions, while human and agent mentions are rarer but span more locations. Qualitative analysis shows that bots mainly reference identifiers in automated dependency updates and audits, whereas humans and agents use them to support fixes, maintenance, and discussion.

</details>


### [72] [Using LLMs to Evaluate Architecture Documents: Results from a Digital Marketplace Environment](https://arxiv.org/abs/2601.19693)
*Frank Elberzhager,Matthias Gerbershagen,Joshua Ginkel*

Main category: cs.SE

TL;DR: 本文探讨了LLM在软件架构文档质量评估中的作用，发现文档质量影响LLM评估准确性，需进一步研究。


<details>
  <summary>Details</summary>
Motivation: 探索LLM在软件工程中，尤其是软件架构文档评估中的实际效能及其对提升文档质量的支持作用。

Method: 通过在一个数字市场项目中，使用多种LLM对架构文档进行质量分析，并将结果与软件架构师的人工评估进行对比。

Result: 本文研究了大型语言模型(LLM)在支持软件架构文档评估中的应用，比较了LLM生成的评估结果与人类软件架构师的评估，发现文档质量对LLM评估结果有显著影响，高质量文档能提高LLM与人工评估的一致性。研究表明LLM在架构评估任务中具有潜力，但存在不一致性，需进一步分析。

Conclusion: LLM辅助的软件架构文档评估具备潜力，但目前存在不一致性，需更多研究以确保评估结果的可靠性。

Abstract: Generative AI plays an increasing role during software engineering activities to make them, e.g., more efficient or provide better quality. However, it is often unclear how much benefit LLMs really provide. We concentrate on software architects and investigated how an LLM-supported evaluation of architecture documents can support software architects to improve such artefacts. In the context of a research project where a digital marketplace is developed and digital solutions should be analyzed, we used different LLMs to analyze the quality of architecture documents and compared the results with evaluations from software architects. We found out that the quality of the artifact has a strong influence on the quality of the LLM, i.e., the better the quality of the architecture document was, the more consistent were the LLM-based evaluation and the human expert evaluation. While using LLMs in this architecture task is promising, our results showed inconsistencies that need further analyses before generalizing them.

</details>


### [73] [AlignCoder: Aligning Retrieval with Target Intent for Repository-Level Code Completion](https://arxiv.org/abs/2601.19697)
*Tianyue Jiang,Yanli Wang,Yanlin Wang,Daya Guo,Ensheng Shi,Yuchi Ma,Jiachi Chen,Zibin Zheng*

Main category: cs.SE

TL;DR: 针对代码模型在仓库级代码补全中的语义不匹配和信息利用不足问题，AlignCoder框架通过查询增强和强化学习优化检索，显著提升补全效果。


<details>
  <summary>Details</summary>
Motivation: 现有代码大型语言模型在仓库级代码补全任务中难以充分理解仓库特定上下文和领域知识，导致性能受限。检索增强生成方法虽然能利用跨文件上下文，但存在检索过程中的查询与目标代码不匹配以及无法有效利用推理信息的问题。

Method: 提出了AlignCoder框架，包括查询增强机制：通过生成多个候选补全构建增强查询，弥合初始查询与目标代码间的语义差距；以及基于强化学习的检索器训练方法，训练AlignRetriever利用增强查询中的推理信息，提高检索准确性。

Result: 在CrossCodeEval和RepoEval两个基准上对五种代码大型语言模型进行评测，AlignCoder在CrossCodeEval基准上相比基线提升了18.1%的EM得分，展现出优越性能和良好泛化能力。

Conclusion: AlignCoder通过查询增强和强化学习训练的检索器，有效解决了仓库级代码补全中的语义不对齐和推理信息利用不足问题，显著提升代码补全性能并适用于多种代码模型和编程语言。

Abstract: Repository-level code completion remains a challenging task for existing code large language models (code LLMs) due to their limited understanding of repository-specific context and domain knowledge. While retrieval-augmented generation (RAG) approaches have shown promise by retrieving relevant code snippets as cross-file context, they suffer from two fundamental problems: misalignment between the query and the target code in the retrieval process, and the inability of existing retrieval methods to effectively utilize the inference information. To address these challenges, we propose AlignCoder, a repository-level code completion framework that introduces a query enhancement mechanism and a reinforcement learning based retriever training method. Our approach generates multiple candidate completions to construct an enhanced query that bridges the semantic gap between the initial query and the target code. Additionally, we employ reinforcement learning to train an AlignRetriever that learns to leverage inference information in the enhanced query for more accurate retrieval. We evaluate AlignCoder on two widely-used benchmarks (CrossCodeEval and RepoEval) across five backbone code LLMs, demonstrating an 18.1% improvement in EM score compared to baselines on the CrossCodeEval benchmark. The results show that our framework achieves superior performance and exhibits high generalizability across various code LLMs and programming languages.

</details>


### [74] [Future of Software Engineering Research: The SIGSOFT Perspective](https://arxiv.org/abs/2601.19731)
*Massimiliano Di Penta,Kelly Blincoe,Marsha Chechik,Claire Le Goues,David Lo,Emerson Murphy-Hill,Thomas Zimmermann*

Main category: cs.SE

TL;DR: 软件工程会议成本和形式问题影响参与，建议SIGSOFT采取措施提升包容性和多样性。


<details>
  <summary>Details</summary>
Motivation: 随着软件工程会议规模增长，成本和落后的会议形式阻碍了研究者参与，威胁到社区的包容性和全球多样性。

Method: 基于针对软件工程会议参与者的调查数据，分析障碍并提出改进措施。

Result: 论文通过调查数据指出，软件工程会议因成本增加和形式陈旧，阻碍了许多研究者的参与，影响社区的包容性和多样性。提出ACM软件工程特别兴趣小组（SIGSOFT）应采取具体措施，如提高会议资金透明度、尝试混合形式的海报展示以及扩大对代表性不足地区的外展。

Conclusion: 通过实施资金透明、混合海报和扩大外展，SIGSOFT能维持软件工程社区的开放和包容。

Abstract: As software engineering conferences grow in size, rising costs and outdated formats are creating barriers to participation for many researchers. These barriers threaten the inclusivity and global diversity that have contributed to the success of the SE community. Based on survey data, we identify concrete actions the ACM Special Interest Group on Software Engineering (SIGSOFT) can take to address these challenges, including improving transparency around conference funding, experimenting with hybrid poster presentations, and expanding outreach to underrepresented regions. By implementing these changes, SIGSOFT can help ensure the software engineering community remains accessible and welcoming.

</details>


### [75] [Assessing Task-based Chatbots: Snapshot and Curated Datasets for Dialogflow](https://arxiv.org/abs/2601.19787)
*Elena Masserini,Diego Clerissi,Daniela Micucci,Leonardo Mariani*

Main category: cs.SE

TL;DR: 本论文收集和整理了大规模聊天机器人数据集，揭示了当前聊天机器人的质量和安全问题，呼吁开展系统性研究。


<details>
  <summary>Details</summary>
Motivation: 现有研究受限于缺乏大规模高质量的聊天机器人数据集，难以进行聊天机器人质量和安全方面的深入研究。

Method: 通过收集和整理GitHub上的1788个Dialogflow聊天机器人数据，构建TOFU-D数据集，并进一步筛选出185个经过验证的聊天机器人构建COD数据集。使用Botium测试框架和Bandit静态分析工具对数据集进行初步质量和安全评估。

Result: 构建了涵盖多个领域、语言和实现方式的TOFU-D和COD数据集，初步分析发现测试覆盖率不足和多处安全漏洞，证明了需要系统性、多平台的研究。

Conclusion: TOFU-D和COD数据集为聊天机器人质量和安全研究提供了有力支持，现有聊天机器人普遍存在测试覆盖不足和安全隐患，未来需加强多平台系统性研究。

Abstract: In recent years, chatbots have gained widespread adoption thanks to their ability to assist users at any time and across diverse domains. However, the lack of large-scale curated datasets limits research on their quality and reliability. This paper presents TOFU-D, a snapshot of 1,788 Dialogflow chatbots from GitHub, and COD, a curated subset of TOFU-D including 185 validated chatbots. The two datasets capture a wide range of domains, languages, and implementation patterns, offering a sound basis for empirical studies on chatbot quality and security. A preliminary assessment using the Botium testing framework and the Bandit static analyzer revealed gaps in test coverage and frequent security vulnerabilities in several chatbots, highlighting the need for systematic, multi-Platform research on chatbot quality and security.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [76] [Reimagining Peer Review Process Through Multi-Agent Mechanism Design](https://arxiv.org/abs/2601.19778)
*Ahmad Farooq,Kamran Iqbal*

Main category: cs.MA

TL;DR: 论文针对同行评审危机，提出通过多智能体强化学习设计激励机制，改善评审效率和公平性，推动可持续的同行评审体系建设。


<details>
  <summary>Details</summary>
Motivation: 当前同行评审面临评审人疲劳、激励不匹配和投稿增长压力，导致评审过程被认为“破裂”。

Method: 将研究社区建模为随机多智能体系统，应用多智能体强化学习设计激励兼容的协议。

Result: 提出了三种干预措施：基于信用的投稿经济、多智能体强化学习优化的评审分配，以及混合验证评审一致性，并制定了威胁模型、公平性考虑和分阶段试点指标。

Conclusion: 同行评审机制存在系统性故障，亟需通过计算方法进行机制设计改进。

Abstract: The software engineering research community faces a systemic crisis: peer review is failing under growing submissions, misaligned incentives, and reviewer fatigue. Community surveys reveal that researchers perceive the process as "broken." This position paper argues that these dysfunctions are mechanism design failures amenable to computational solutions. We propose modeling the research community as a stochastic multi-agent system and applying multi-agent reinforcement learning to design incentive-compatible protocols. We outline three interventions: a credit-based submission economy, MARL-optimized reviewer assignment, and hybrid verification of review consistency. We present threat models, equity considerations, and phased pilot metrics. This vision charts a research agenda toward sustainable peer review.

</details>
