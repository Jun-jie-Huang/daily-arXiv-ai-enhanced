<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 59]
- [cs.SE](#cs.SE) [Total: 23]
- [cs.MA](#cs.MA) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [BioACE: An Automated Framework for Biomedical Answer and Citation Evaluations](https://arxiv.org/abs/2602.04982)
*Deepak Gupta,Davis Bartels,Dina Demner-Fuhsman*

Main category: cs.CL

TL;DR: 本文提出了BioACE，一种自动化框架，综合评估生物医学领域大语言模型生成的答案及其引用，显著提高了评估的自动化和准确性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在生物医学问答中的广泛应用，评价生成答案及其引用的质量变得关键，且现有评估方法受限于需要专家评审和复杂医学术语。

Method: 提出BioACE自动化框架，从完整性、正确性、精确性及召回率多个维度评估答案，并利用自然语言推理及预训练语言模型评估引用的证据质量。

Result: 通过大量实验，BioACE展示了其评估方法与人类评价高度相关，并提供了最佳的生物医学答案及引用评价方法。

Conclusion: BioACE框架有效地评估了生物医学领域中由大语言模型生成的答案及其引用的质量，体现了较高的人类评价相关性。

Abstract: With the increasing use of large language models (LLMs) for generating answers to biomedical questions, it is crucial to evaluate the quality of the generated answers and the references provided to support the facts in the generated answers. Evaluation of text generated by LLMs remains a challenge for question answering, retrieval-augmented generation (RAG), summarization, and many other natural language processing tasks in the biomedical domain, due to the requirements of expert assessment to verify consistency with the scientific literature and complex medical terminology. In this work, we propose BioACE, an automated framework for evaluating biomedical answers and citations against the facts stated in the answers. The proposed BioACE framework considers multiple aspects, including completeness, correctness, precision, and recall, in relation to the ground-truth nuggets for answer evaluation. We developed automated approaches to evaluate each of the aforementioned aspects and performed extensive experiments to assess and analyze their correlation with human evaluations. In addition, we considered multiple existing approaches, such as natural language inference (NLI) and pre-trained language models and LLMs, to evaluate the quality of evidence provided to support the generated answers in the form of citations into biomedical literature. With the detailed experiments and analysis, we provide the best approaches for biomedical answer and citation evaluation as a part of BioACE (https://github.com/deepaknlp/BioACE) evaluation package.

</details>


### [2] [CoWork-X: Experience-Optimized Co-Evolution for Multi-Agent Collaboration System](https://arxiv.org/abs/2602.05004)
*Zexin Lin,Jiachen Yu,Haoyang Zhang,Yuzhao Li,Zhonghang Li,Yujiu Yang,Junjie Wang,Xiaoqiang Ji*

Main category: cs.CL

TL;DR: 提出CoWork-X，通过层次化技能检索和回合后优化，实现实时协作任务中的高效协同和在线资源节约，显著提升合作性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在高度合作任务中要么导致延迟和时间抖动，要么通过无结构文本进行改进，难以实现低成本可靠执行，需解决实时协调与多回合适应的矛盾。

Method: 提出CoWork-X框架，包括Skill-Agent通过层次任务网络（HTN）执行结构化技能检索，以及回合后Co-Optimizer进行带约束的技能整合，实现协同优化。

Result: 在Overcooked-AI风格的实时协作基准测试中，CoWork-X实现了稳定的性能累计提升，同时持续降低了在线延迟和令牌使用量。

Conclusion: CoWork-X框架有效实现了高度协同任务中的实时协调与多回合持续适应，显著提升了合作任务的性能，同时减少了在线延迟和令牌使用。

Abstract: Large language models are enabling language-conditioned agents in interactive environments, but highly cooperative tasks often impose two simultaneous constraints: sub-second real-time coordination and sustained multi-episode adaptation under a strict online token budget. Existing approaches either rely on frequent in-episode reasoning that induces latency and timing jitter, or deliver post-episode improvements through unstructured text that is difficult to compile into reliable low-cost execution. We propose CoWork-X, an active co-evolution framework that casts peer collaboration as a closed-loop optimization problem across episodes, inspired by fast--slow memory separation. CoWork-X instantiates a Skill-Agent that executes via HTN (hierarchical task network)-based skill retrieval from a structured, interpretable, and compositional skill library, and a post-episode Co-Optimizer that performs patch-style skill consolidation with explicit budget constraints and drift regularization. Experiments in challenging Overcooked-AI-like realtime collaboration benchmarks demonstrate that CoWork-X achieves stable, cumulative performance gains while steadily reducing online latency and token usage.

</details>


### [3] [Capacity Constraints and the Multilingual Penalty for Lexical Disambiguation](https://arxiv.org/abs/2602.05035)
*Sean Trott,Pamela D. Rivière*

Main category: cs.CL

TL;DR: 多语种语言模型在词义消歧任务上表现不佳，受限于多种容量约束，影响了它们捕捉语义和上下文的能力。


<details>
  <summary>Details</summary>
Motivation: 多语种语言模型在某些任务中表现不如单语模型，可能由于模型容量限制，研究这一现象有助于理解多语种模型的性能瓶颈。

Method: 通过对比单语和多语种语言模型在英语和西班牙语模糊词的相关性判断数据集上的表现，分析容量限制对模型性能的影响。

Result: 多语种模型在表示均匀性降低、注意力分配不足和词汇切分多样性增加等方面存在限制，这些因素共同解释了性能下降。

Conclusion: 多语种语言模型在词义消歧任务中表现不及单语模型，主要受限于表示空间、注意力机制和词汇切分等容量限制。

Abstract: Multilingual language models (LMs) sometimes under-perform their monolingual counterparts, possibly due to capacity limitations. We quantify this ``multilingual penalty'' for lexical disambiguation--a task requiring precise semantic representations and contextualization mechanisms--using controlled datasets of human relatedness judgments for ambiguous words in both English and Spanish. Comparing monolingual and multilingual LMs from the same families, we find consistently reduced performance in multilingual LMs. We then explore three potential capacity constraints: representational (reduced embedding isotropy), attentional (reduced attention to disambiguating cues), and vocabulary-related (increased multi-token segmentation). Multilingual LMs show some evidence of all three limitations; moreover, these factors statistically account for the variance formerly attributed to a model's multilingual status. These findings suggest both that multilingual LMs do suffer from multiple capacity constraints, and that these constraints correlate with reduced disambiguation performance.

</details>


### [4] [Locas: Your Models are Principled Initializers of Locally-Supported Parametric Memories](https://arxiv.org/abs/2602.05085)
*Sidi Lu,Zhenwen Liang,Dongyang Ma,Yan Wang,Haitao Mi,Dong Yu*

Main category: cs.CL

TL;DR: 本文提出Locas，一种基于局部参数化记忆的机制，通过与Transformer FFN模块设计结合，实现测试时训练中高效记忆和持续学习，解决了灾难性遗忘问题，在语言模型和对话问答任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决测试时训练中模型持续学习与参数记忆难以兼顾的问题，希望通过设计灵活且高效的参数化记忆机制，实现对过去上下文信息的存储及灾难性遗忘的防止。

Method: 提出Locas，一种与现代Transformer中的FFN模块结构相似的局部支持参数化记忆，包括两种变体（传统两层MLP和GLU-FFN结构），并通过合理初始化利用模型参数、激活和梯度实现快速收敛和优良泛化。

Result: Locas-GLU以极少的额外参数（最低0.02%）实现了对过去上下文信息的有效存储，为模型带来更小的上下文窗口需求，同时在PG-19和LoCoMo任务中表现优良，且模型在记忆完全文本后在MMLU评测中表现稳定，验证了其减少已有知识遗忘的能力。

Conclusion: Locas通过引入局部支持的参数化记忆机制，有效地实现了测试时训练与模型参数的灵活结合，提升了模型的持续学习能力并减少了灾难性遗忘，验证了其在长文本建模和对话问答任务中的优越表现。

Abstract: In this paper, we aim to bridge test-time-training with a new type of parametric memory that can be flexibly offloaded from or merged into model parameters. We present Locas, a Locally-Supported parametric memory that shares the design of FFN blocks in modern transformers, allowing it to be flexibly permanentized into the model parameters while supporting efficient continual learning. We discuss two major variants of Locas: one with a conventional two-layer MLP design that has a clearer theoretical guarantee; the other one shares the same GLU-FFN structure with SOTA LLMs, and can be easily attached to existing models for both parameter-efficient and computation-efficient continual learning. Crucially, we show that proper initialization of such low-rank sideway-FFN-style memories -- performed in a principled way by reusing model parameters, activations and/or gradients -- is essential for fast convergence, improved generalization, and catastrophic forgetting prevention. We validate the proposed memory mechanism on the PG-19 whole-book language modeling and LoCoMo long-context dialogue question answering tasks. With only 0.02\% additional parameters in the lowest case, Locas-GLU is capable of storing the information from past context while maintaining a much smaller context window. In addition, we also test the model's general capability loss after memorizing the whole book with Locas, through comparative MMLU evaluation. Results show the promising ability of Locas to permanentize past context into parametric knowledge with minimized catastrophic forgetting of the model's existing internal knowledge.

</details>


### [5] [Data Kernel Perspective Space Performance Guarantees for Synthetic Data from Transformer Models](https://arxiv.org/abs/2602.05106)
*Michael Browder,Kevin Duh,J. David Harris,Vince Lyzinski,Paul McNamee,Youngser Park,Carey E. Priebe,Peter Viechnicki*

Main category: cs.CL

TL;DR: 面对训练数据不足和合成数据性质不可控的问题，本文提出了数据核透视空间（DKPS），提供了数学性能保证，提升了语言模型和下游任务的训练效果。


<details>
  <summary>Details</summary>
Motivation: 标签训练数据匮乏导致语言技术和生成式AI模型性能受限，且现有方法缺乏对合成数据性质的预测和保证。

Method: 本文数学推导了DKPS，并基于其性能保证对使用合成数据训练的下游任务（如神经机器翻译或基于对比偏好优化训练的LLM）进行了性能分析。

Result: DKPS为变换器模型输出的数据质量提供了具体的性能保证，能够指导和解释下游任务的表现，推动更可靠的合成数据应用。

Conclusion: 本文提出的数据核透视空间（DKPS）为利用变换器模型生成的合成数据提供了数学分析基础和统计质量保证，帮助解决数据稀缺问题。

Abstract: Scarcity of labeled training data remains the long pole in the tent for building performant language technology and generative AI models. Transformer models -- particularly LLMs -- are increasingly being used to mitigate the data scarcity problem via synthetic data generation. However, because the models are black boxes, the properties of the synthetic data are difficult to predict. In practice it is common for language technology engineers to 'fiddle' with the LLM temperature setting and hope that what comes out the other end improves the downstream model. Faced with this uncertainty, here we propose Data Kernel Perspective Space (DKPS) to provide the foundation for mathematical analysis yielding concrete statistical guarantees for the quality of the outputs of transformer models. We first show the mathematical derivation of DKPS and how it provides performance guarantees. Next we show how DKPS performance guarantees can elucidate performance of a downstream task, such as neural machine translation models or LLMs trained using Contrastive Preference Optimization (CPO). Limitations of the current work and future research are also discussed.

</details>


### [6] [Multilingual Extraction and Recognition of Implicit Discourse Relations in Speech and Text](https://arxiv.org/abs/2602.05107)
*Ahmed Ruby,Christian Hardmeier,Sara Stymne*

Main category: cs.CL

TL;DR: 该论文针对隐含语篇关系分类，提出多模态多语种数据集和融合文本音频信息的模型，显著提升了分类性能，特别是在低资源语言场景下。


<details>
  <summary>Details</summary>
Motivation: 隐含语篇关系分类困难在于需要从上下文中推断含义，且仅靠文本无法完全捕捉分布于多模态和多语言间的上下文线索。

Method: 提出了一种结合文本和音频信息的多模态分类方法，利用Qwen2-Audio模型进行联合建模，同时构建了英法西三语的多模态多语种数据集。

Result: 基于文本的模型表现优于纯音频模型，但多模态融合进一步提升了分类效果，跨语言迁移对低资源语言带来显著提升。

Conclusion: 文本和音频的多模态集成能够提升隐含语篇关系分类的性能，尤其是在跨语言迁移中表现显著。

Abstract: Implicit discourse relation classification is a challenging task, as it requires inferring meaning from context. While contextual cues can be distributed across modalities and vary across languages, they are not always captured by text alone. To address this, we introduce an automatic method for distantly related and unrelated language pairs to construct a multilingual and multimodal dataset for implicit discourse relations in English, French, and Spanish. For classification, we propose a multimodal approach that integrates textual and acoustic information through Qwen2-Audio, allowing joint modeling of text and audio for implicit discourse relation classification across languages. We find that while text-based models outperform audio-based models, integrating both modalities can enhance performance, and cross-lingual transfer can provide substantial improvements for low-resource languages.

</details>


### [7] [Towards a Science of Collective AI: LLM-based Multi-Agent Systems Need a Transition from Blind Trial-and-Error to Rigorous Science](https://arxiv.org/abs/2602.05289)
*Jingru Fan,Dewen Liu,Yufan Dang,Huatao Li,Yuheng Wang,Wei Liu,Feiyu Duan,Xuanwen Ding,Shu Yao,Lin Wu,Ruijie Shi,Wai-Shing Leung,Yuan Cheng,Zhongyu Wei,Cheng Yang,Chen Qian,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: 本文针对多智能体系统缺乏统一科学框架的问题，提出合作增益度量和系统化因素归因方法，构建因素库，推动领域从经验试错向严谨的设计科学转变。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统领域依赖大量的经验试错，缺乏统一且有原则的科学框架来系统性地优化和提升，主要原因是缺乏因素的结构化分类和统一的度量标准，导致无法明确合作带来的真实收益。

Method: 本文构建了一个系统性的MAS因素库，设计了基于合作增益Γ的因素归因范式，将MAS设计空间结构化为控制层预设和信息层动态两部分，借此规范化和系统化多智能体系统的设计与评估过程。

Result: 本文提出的合作增益度量Γ作为科学标准，结合因素归因范式和MAS因素库，实现了对多智能体系统设计的系统化和科学化，有望推动多智能体系统从经验驱动转向设计科学。

Conclusion: 本文提出了一个统一的多智能体系统（MAS）设计框架，利用合作增益度量（Γ）来科学区分系统性能提升的内在原因与资源扩充的影响，从而实现系统优化的科学化。

Abstract: Recent advancements in Large Language Models (LLMs) have greatly extended the capabilities of Multi-Agent Systems (MAS), demonstrating significant effectiveness across a wide range of complex and open-ended domains. However, despite this rapid progress, the field still relies heavily on empirical trial-and-error. It lacks a unified and principled scientific framework necessary for systematic optimization and improvement. This bottleneck stems from the ambiguity of attribution: first, the absence of a structured taxonomy of factors leaves researchers restricted to unguided adjustments; second, the lack of a unified metric fails to distinguish genuine collaboration gain from mere resource accumulation. In this paper, we advocate for a transition to design science through an integrated framework. We advocate to establish the collaboration gain metric ($Γ$) as the scientific standard to isolate intrinsic gains from increased budgets. Leveraging $Γ$, we propose a factor attribution paradigm to systematically identify collaboration-driving factors. To support this, we construct a systematic MAS factor library, structuring the design space into control-level presets and information-level dynamics. Ultimately, this framework facilitates the transition from blind experimentation to rigorous science, paving the way towards a true science of Collective AI.

</details>


### [8] [GreekMMLU: A Native-Sourced Multitask Benchmark for Evaluating Language Models in Greek](https://arxiv.org/abs/2602.05150)
*Yang Zhang,Mersin Konomi,Christos Xypolopoulos,Konstantinos Divriotis,Konstantinos Skianis,Giannis Nikolentzos,Giorgos Stamou,Guokan Shang,Michalis Vazirgiannis*

Main category: cs.CL

TL;DR: 本文提出基于希腊语原生试题的GreekMMLU基准，系统评测多款大语言模型在希腊语理解上的表现，促进了希腊语大模型能力的提升和研究。


<details>
  <summary>Details</summary>
Motivation: 现有希腊语评测数据多为英文机翻，缺乏真实且多样化的希腊语原生语料，限制了对希腊语大语言模型能力的准确评估和发展。

Method: 构建包含21,805道多选题的GreekMMLU数据集，涵盖45个学科领域，题目均为希腊语原生内容，分难度等级，并对80多个开源及闭源大模型进行评测分析模型表现差异。

Result: 公开了16,857条样本，设置4,948条私有样本用于测试，以保障评测的可靠性；评测结果显示前沿模型明显优于开源模型，经过希腊语适应的模型优于普通多语种模型；系统分析了模型规模、适应性和提示技术对性能的影响。

Conclusion: 本论文展示了GreekMMLU作为希腊语原生数据的多任务语言理解基准，揭示了当前大模型在希腊语理解上的显著差距和提升空间。

Abstract: Large Language Models (LLMs) are commonly trained on multilingual corpora that include Greek, yet reliable evaluation benchmarks for Greek-particularly those based on authentic, native-sourced content-remain limited. Existing datasets are often machine-translated from English, failing to capture Greek linguistic and cultural characteristics. We introduce GreekMMLU, a native-sourced benchmark for massive multitask language understanding in Greek, comprising 21,805 multiple-choice questions across 45 subject areas, organized under a newly defined subject taxonomy and annotated with educational difficulty levels spanning primary to professional examinations. All questions are sourced or authored in Greek from academic, professional, and governmental exams. We publicly release 16,857 samples and reserve 4,948 samples for a private leaderboard to enable robust and contamination-resistant evaluation. Evaluations of over 80 open- and closed-source LLMs reveal substantial performance gaps between frontier and open-weight models, as well as between Greek-adapted models and general multilingual ones. Finally, we provide a systematic analysis of factors influencing performance-including model scale, adaptation, and prompting-and derive insights for improving LLM capabilities in Greek.

</details>


### [9] [LinguistAgent: A Reflective Multi-Model Platform for Automated Linguistic Annotation](https://arxiv.org/abs/2602.05493)
*Bingru Li*

Main category: cs.CL

TL;DR: 本文提出LinguistAgent平台，利用多模型双代理架构自动化语言学标注，以隐喻识别任务验证其在精确率、召回率及F1值上的高效表现，解决了传统数据标注瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 数据标注在社会科学及人文学科，尤其是复杂语义任务如隐喻识别中是一大瓶颈。尽管大型语言模型（LLMs）理论上能力强，但实际中对研究者的帮助仍有限。

Method: 提出LinguistAgent，一个集成且用户友好的平台，采用反思式多模型架构，实现语言学自动标注。系统包含注释者与审核者双代理工作流程，模拟专业同行评审过程，并支持零/少样例提示工程、检索增强生成以及微调三种范式的对比试验。

Result: 通过隐喻识别任务验证LinguistAgent的有效性，实时输出基于人工黄金标准的精确率、召回率和F1分数。平台和代码已开源。

Conclusion: LinguistAgent有效提升了复杂语言学标注任务的自动化水平，弥合了大型语言模型理论能力与实践应用之间的差距。

Abstract: Data annotation remains a significant bottleneck in the Humanities and Social Sciences, particularly for complex semantic tasks such as metaphor identification. While Large Language Models (LLMs) show promise, a significant gap remains between the theoretical capability of LLMs and their practical utility for researchers. This paper introduces LinguistAgent, an integrated, user-friendly platform that leverages a reflective multi-model architecture to automate linguistic annotation. The system implements a dual-agent workflow, comprising an Annotator and a Reviewer, to simulate a professional peer-review process. LinguistAgent supports comparative experiments across three paradigms: Prompt Engineering (Zero/Few-shot), Retrieval-Augmented Generation, and Fine-tuning. We demonstrate LinguistAgent's efficacy using the task of metaphor identification as an example, providing real-time token-level evaluation (Precision, Recall, and $F_1$ score) against human gold standards. The application and codes are released on https://github.com/Bingru-Li/LinguistAgent.

</details>


### [10] [Among Us: Measuring and Mitigating Malicious Contributions in Model Collaboration Systems](https://arxiv.org/abs/2602.05176)
*Ziyuan Yang,Wenxuan Ding,Shangbin Feng,Yulia Tsvetkov*

Main category: cs.CL

TL;DR: 研究表明多语言模型协作中恶意模型会严重影响系统表现，提出的外部监督策略能有效缓解该问题，但完全防御仍待研究。


<details>
  <summary>Details</summary>
Motivation: 随着多语言模型协作系统的普及，存在部分模型被恶意篡改或损坏的安全风险，迫切需要评估这种威胁的影响并提出缓解方案。

Method: 构造了四类恶意语言模型，部署在四种流行的模型协作系统中，使用10个数据集进行评估，并设计了外部监督的缓解策略来降低恶意模型的影响。

Result: 实验结果表明恶意模型导致系统性能下降7.12%至7.94%，通过外部监督策略平均恢复了95.31%的性能。

Conclusion: 恶意模型对多语言模型（multi-LLM）协作系统有显著的负面影响，尤其在推理和安全领域表现下降明显。尽管提出的外部监督策略能在一定程度上缓解恶意组件的影响，并恢复大部分性能，但如何完全抵御恶意模型仍是一个未解决的问题。

Abstract: Language models (LMs) are increasingly used in collaboration: multiple LMs trained by different parties collaborate through routing systems, multi-agent debate, model merging, and more. Critical safety risks remain in this decentralized paradigm: what if some of the models in multi-LLM systems are compromised or malicious? We first quantify the impact of malicious models by engineering four categories of malicious LMs, plug them into four types of popular model collaboration systems, and evaluate the compromised system across 10 datasets. We find that malicious models have a severe impact on the multi-LLM systems, especially for reasoning and safety domains where performance is lowered by 7.12% and 7.94% on average. We then propose mitigation strategies to alleviate the impact of malicious components, by employing external supervisors that oversee model collaboration to disable/mask them out to reduce their influence. On average, these strategies recover 95.31% of the initial performance, while making model collaboration systems fully resistant to malicious models remains an open research question.

</details>


### [11] [The Single-Multi Evolution Loop for Self-Improving Model Collaboration Systems](https://arxiv.org/abs/2602.05182)
*Shangbin Feng,Kishan Panaganti,Yulia Tsvetkov,Wenhao Yu*

Main category: cs.CL

TL;DR: 本文提出通过蒸馏将多模型协作优势整合入单一模型，实现高效推理；并设计单多进化循环机制，推动模型协作与自我提升，实验证明该方法性能显著提升且适用广泛。


<details>
  <summary>Details</summary>
Motivation: 多模型协作虽能结合不同模型的优势，但代价是加载多模型成本高，需提高效率同时保持协作优势。

Method: 训练单一模型模仿多模型协作系统的输出，通过蒸馏技术整合多模型的知识；引入单多进化循环，让多模型协作后各自蒸馏并强化，再次协作循环进化。

Result: 在7种协作策略和15项任务上，单模型通过蒸馏平均提升8.0%，协作系统通过进化循环平均提升14.9%；该方法优于现有进化AI方法，兼容多种设置并能解决初始模型难题。

Conclusion: 通过模型蒸馏将多模型协作的优势整合到单一模型中，在推理时只需使用单一模型即可显著提升效率并保持协作优势。提出的单多进化循环机制使模型通过协作和蒸馏不断自我提升，实现协同进化。

Abstract: Model collaboration -- systems where multiple language models (LMs) collaborate -- combines the strengths of diverse models with cost in loading multiple LMs. We improve efficiency while preserving the strengths of collaboration by distilling collaborative patterns into a single model, where the model is trained on the outputs of the model collaboration system. At inference time, only the distilled model is employed: it imitates the collaboration while only incurring the cost of a single model. Furthermore, we propose the single-multi evolution loop: multiple LMs collaborate, each distills from the collaborative outputs, and these post-distillation improved LMs collaborate again, forming a collective evolution ecosystem where models evolve and self-improve by interacting with an environment of other models. Extensive experiments with 7 collaboration strategies and 15 tasks (QA, reasoning, factuality, etc.) demonstrate that: 1) individual models improve by 8.0% on average, absorbing the strengths of collaboration while reducing the cost to a single model; 2) the collaboration also benefits from the stronger and more synergistic LMs after distillation, improving over initial systems without evolution by 14.9% on average. Analysis reveals that the single-multi evolution loop outperforms various existing evolutionary AI methods, is compatible with diverse model/collaboration/distillation settings, and helps solve problems where the initial model/system struggles to.

</details>


### [12] [Are Open-Weight LLMs Ready for Social Media Moderation? A Comparative Study on Bluesky](https://arxiv.org/abs/2602.05189)
*Hsuan-Yu Chou,Wajiha Naveed,Shuyan Zhou,Xiaowei Yang*

Main category: cs.CL

TL;DR: 研究表明开源大型语言模型在社交媒体有害内容审核中表现接近专有模型，支持隐私保护和个性化需求，推动内容审核系统设计的新方向。


<details>
  <summary>Details</summary>
Motivation: 互联网接入的普及加剧了有害内容的传播，需要有效的内容审核手段，而开源LLMs在零样本能力上的表现还未被充分验证。

Method: 评估了四个专有模型和三个开源模型，通过对Bluesky平台上的真实帖子、Bluesky审核服务的决策以及两位作者的注释进行测试和对比分析。

Result: 发现开源模型在灵敏度（81%–97%）和特异性（91%–100%）与专有模型（72%–98%、93%–99%）高度接近，且不同有害内容类别的检测灵敏度和特异性存在差异，同时人类审核员与LLMs之间具有一定的一致性。

Conclusion: 开源权重的大型语言模型（LLMs）在敏感内容检测的灵敏度和特异性方面与专有模型具有较高的重叠度，能够有效支持社交媒体的内容审核。

Abstract: As internet access expands, so does exposure to harmful content, increasing the need for effective moderation. Research has demonstrated that large language models (LLMs) can be effectively utilized for social media moderation tasks, including harmful content detection. While proprietary LLMs have been shown to zero-shot outperform traditional machine learning models, the out-of-the-box capability of open-weight LLMs remains an open question.
  Motivated by recent developments of reasoning LLMs, we evaluate seven state-of-the-art models: four proprietary and three open-weight. Testing with real-world posts on Bluesky, moderation decisions by Bluesky Moderation Service, and annotations by two authors, we find a considerable degree of overlap between the sensitivity (81%--97%) and specificity (91%--100%) of the open-weight LLMs and those (72%--98%, and 93%--99%) of the proprietary ones. Additionally, our analysis reveals that specificity exceeds sensitivity for rudeness detection, but the opposite holds for intolerance and threats. Lastly, we identify inter-rater agreement across human moderators and the LLMs, highlighting considerations for deploying LLMs in both platform-scale and personalized moderation contexts. These findings show open-weight LLMs can support privacy-preserving moderation on consumer-grade hardware and suggest new directions for designing moderation systems that balance community values with individual user preferences.

</details>


### [13] [Aligning Large Language Model Behavior with Human Citation Preferences](https://arxiv.org/abs/2602.05205)
*Kenichiro Ando,Tatsuya Harada*

Main category: cs.CL

TL;DR: 本文通过数据集构建和偏好评估，揭示了大型语言模型在引文选择上与人类的差异，指出模型在某些内容上过度或不足引用，且提出优化方法改善模型与人类偏好的匹配。


<details>
  <summary>Details</summary>
Motivation: 现有服务为了增强输出的可信度，会添加引用文献，但如何识别应被引用的内容及其控制机制研究不足，本文旨在探究大规模语言模型当前的引文倾向及其与人类偏好的对齐情况。

Method: 构建了一个包含八种引文动机类型的网络文本数据集，进行了两两引文偏好评估，分析模型与人类的引文倾向差异，并通过直接偏好优化(Direct Preference Optimization)调整模型行为。

Result: 模型倾向于像人类一样频繁地为医疗文本添加引用，且对明显标记需要引用的内容，如维基百科中的内容，引用概率高出人类27%，但对包含数字和人名的句子引用不足，分别低于人类20.1%和22.6%。通过直接偏好优化，模型行为得到了更好的校准。

Conclusion: 当前的大规模语言模型在引文行为上与人类存在显著差异，尤其是在对某些类型内容的引文选择上存在偏差。通过直接偏好优化方法，可以有效校准模型行为，使其与人类的引文偏好更加一致。

Abstract: Most services built on powerful large-scale language models (LLMs) add citations to their output to enhance credibility. Recent research has paid increasing attention to the question of what reference documents to link to outputs. However, how LLMs recognize cite-worthiness and how this process should be controlled remains underexplored. In this study, we focus on what kinds of content LLMs currently tend to cite and how well that behavior aligns with human preferences. We construct a dataset to characterize the relationship between human citation preferences and LLM behavior. Web-derived texts are categorized into eight citation-motivation types, and pairwise citation preferences are exhaustively evaluated across all type combinations to capture fine-grained contrasts. Our results show that humans most frequently seek citations for medical text, and stronger models display a similar tendency. We also find that current models are as much as $27\%$ more likely than humans to add citations to text that is explicitly marked as needing citations on sources such as Wikipedia, and this overemphasis reduces alignment accuracy. Conversely, models systematically underselect numeric sentences (by $-22.6\%$ relative to humans) and sentences containing personal names (by $-20.1\%$), categories for which humans typically demand citations. Furthermore, experiments with Direct Preference Optimization demonstrate that model behavior can be calibrated to better match human citation preferences. We expect this study to provide a foundation for more fine-grained investigations into LLM citation preferences.

</details>


### [14] [Quantifying the Knowledge Proximity Between Academic and Industry Research: An Entity and Semantic Perspective](https://arxiv.org/abs/2602.05211)
*Hongye Zhao,Yi Zhao,Chengzhi Zhang*

Main category: cs.CL

TL;DR: 本研究利用细粒度知识实体和语义分析，揭示学术界与产业界在技术变革推动下的知识协同进化过程，表明双方知识接近性增加及权力结构动态变化。


<details>
  <summary>Details</summary>
Motivation: 现有关于学术界与产业界知识接近性的研究主要依赖宏观指标，缺乏对文献中知识单元的细致分析，导致无法充分理解两者间的细粒度知识接近性，影响协作框架和资源分配效率。

Method: 通过预训练模型提取细粒度知识实体，利用余弦相似度衡量实体序列重叠，结合复杂网络分析拓扑特征；在语义层面，采用无监督对比学习测量跨机构文本相似性；最后通过引用分布分析双向知识流动与相似性的关系。

Result: 发现学术界与产业界的知识接近性随着技术变革而提升，体现出双向适应的协同进化机制，同时技术范式转变时学术界的知识主导地位减弱。

Conclusion: 通过细粒度实体和语义空间的多维度分析，该研究揭示了学术界与产业界知识协同演化的动态特征及其机制，为理解和优化双方的协作关系提供了新的证据和方法。

Abstract: The academia and industry are characterized by a reciprocal shaping and dynamic feedback mechanism. Despite distinct institutional logics, they have adapted closely in collaborative publishing and talent mobility, demonstrating tension between institutional divergence and intensive collaboration. Existing studies on their knowledge proximity mainly rely on macro indicators such as the number of collaborative papers or patents, lacking an analysis of knowledge units in the literature. This has led to an insufficient grasp of fine-grained knowledge proximity between industry and academia, potentially undermining collaboration frameworks and resource allocation efficiency. To remedy the limitation, this study quantifies the trajectory of academia-industry co-evolution through fine-grained entities and semantic space. In the entity measurement part, we extract fine-grained knowledge entities via pre-trained models, measure sequence overlaps using cosine similarity, and analyze topological features through complex network analysis. At the semantic level, we employ unsupervised contrastive learning to quantify convergence in semantic spaces by measuring cross-institutional textual similarities. Finally, we use citation distribution patterns to examine correlations between bidirectional knowledge flows and similarity. Analysis reveals that knowledge proximity between academia and industry rises, particularly following technological change. This provides textual evidence of bidirectional adaptation in co-evolution. Additionally, academia's knowledge dominance weakens during technological paradigm shifts. The dataset and code for this paper can be accessed at https://github.com/tinierZhao/Academic-Industrial-associations.

</details>


### [15] [Bagpiper: Solving Open-Ended Audio Tasks via Rich Captions](https://arxiv.org/abs/2602.05220)
*Jinchuan Tian,Haoran Wang,Bo-Hao Su,Chien-yu Huang,Qingzheng Wang,Jiatong Shi,William Chen,Xun Gong,Siddhant Arora,Chin-Jou Li,Masao Someki,Takashi Maekaku,Yusuke Shinohara,Jin Sakuma,Chao-Han Huck Yang,Shinji Watanabe*

Main category: cs.CL

TL;DR: 提出Bagpiper，一种基于丰富语言描述的大规模音频基础模型，实现统一的音频理解与生成，性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有音频模型依赖特定任务监督，仅处理孤立音频因素，无法像人类一般整体理解音频信号。

Method: 通过大规模预训练（600B tokens）建立原始音频与高层次认知概念之间的双向映射，采用caption-then-process的流程模拟认知推理，从而无需任务特定先验完成多样任务。

Result: Bagpiper在MMAU和AIRBench上优于Qwen-2.5-Omni，在音频生成质量上超越CosyVoice3和TangoFlux，支持多样音频内容的合成。

Conclusion: Bagpiper模型实现了统一的音频理解与生成，显著提升了多项音频任务的性能。

Abstract: Current audio foundation models typically rely on rigid, task-specific supervision, addressing isolated factors of audio rather than the whole. In contrast, human intelligence processes audio holistically, seamlessly bridging physical signals with abstract cognitive concepts to execute complex tasks. Grounded in this philosophy, we introduce Bagpiper, an 8B audio foundation model that interprets physical audio via rich captions, i.e., comprehensive natural language descriptions that encapsulate the critical cognitive concepts inherent in the signal (e.g., transcription, audio events). By pre-training on a massive corpus of 600B tokens, the model establishes a robust bidirectional mapping between raw audio and this high-level conceptual space. During fine-tuning, Bagpiper adopts a caption-then-process workflow, simulating an intermediate cognitive reasoning step to solve diverse tasks without task-specific priors. Experimentally, Bagpiper outperforms Qwen-2.5-Omni on MMAU and AIRBench for audio understanding and surpasses CosyVoice3 and TangoFlux in generation quality, capable of synthesizing arbitrary compositions of speech, music, and sound effects. To the best of our knowledge, Bagpiper is among the first works that achieve unified understanding generation for general audio. Model, data, and code are available at Bagpiper Home Page.

</details>


### [16] [FedMosaic: Federated Retrieval-Augmented Generation via Parametric Adapters](https://arxiv.org/abs/2602.05235)
*Zhilin Liang,Yuxiang Wang,Zimu Zhou,Hainan Zhang,Boyi Liu,Yongxin Tong*

Main category: cs.CL

TL;DR: 本文提出FedMosaic，一个基于参数化适配器的联邦RAG框架，通过语义聚类和选择性适配器聚合，解决了隐私场景下知识分散问题，实现性能和效率双提升。


<details>
  <summary>Details</summary>
Motivation: 解决隐私保护场景中知识孤岛问题，因中央语料库不现实，传统RAG方法存在直接传输文档违背隐私需求的问题。

Method: 采用参数化适配器方法，设计多文档适配器及文档特定掩码，实现语义相关文档聚类，结合选择性聚合策略，避免了适配器间的破坏性融合。

Result: FedMosaic在四个类别的任务中，准确率提升平均10.9%，存储成本降低78.8%至86.3%，通信成本降低91.4%，无原始文档共享。

Conclusion: FedMosaic在保证不共享原始文档的前提下，通过多文档适配器的语义聚类和选择性适配器聚合，有效提升了联邦RAG系统的准确率，且显著降低了存储和通信开销。

Abstract: Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by grounding generation in external knowledge to improve factuality and reduce hallucinations. Yet most deployments assume a centralized corpus, which is infeasible in privacy aware domains where knowledge remains siloed. This motivates federated RAG (FedRAG), where a central LLM server collaborates with distributed silos without sharing raw documents. In context RAG violates this requirement by transmitting verbatim documents, whereas parametric RAG encodes documents into lightweight adapters that merge with a frozen LLM at inference, avoiding raw-text exchange. We adopt the parametric approach but face two unique challenges induced by FedRAG: high storage and communication from per-document adapters, and destructive aggregation caused by indiscriminately merging multiple adapters. We present FedMosaic, the first federated RAG framework built on parametric adapters. FedMosaic clusters semantically related documents into multi-document adapters with document-specific masks to reduce overhead while preserving specificity, and performs selective adapter aggregation to combine only relevance-aligned, nonconflicting adapters. Experiments show that FedMosaic achieves an average 10.9% higher accuracy than state-of-the-art methods in four categories, while lowering storage costs by 78.8% to 86.3% and communication costs by 91.4%, and never sharing raw documents.

</details>


### [17] [Copyright Detective: A Forensic System to Evidence LLMs Flickering Copyright Leakage Risks](https://arxiv.org/abs/2602.05252)
*Guangwei Zhang,Jianing Zhu,Cheng Qian,Neil Gong,Rada Mihalcea,Zhaozhuo Xu,Jingrui He,Jiaqi Ma,Yun Huang,Chaowei Xiao,Bo Li,Ahmed Abbasi,Dongwon Lee,Heng Ji,Denghui Zhang*

Main category: cs.CL

TL;DR: Copyright Detective是一个集多种检测技术于一体的互动式系统，专为发现和分析大型语言模型输出中的版权风险设计，支持黑箱环境下系统化审计和透明评估。


<details>
  <summary>Details</summary>
Motivation: 由于版权法的复杂性，版权侵权检测应当视为持续的证据发现过程，而非简单的静态分类任务，因此需要一个全面且交互式的版权风险检测系统。

Method: 集成内容回忆测试、意译级别相似性分析、绕过检测和去学习验证等多种检测方法，结合交互式提示、响应收集和迭代工作流程，实现系统化的版权风险审计。

Result: 提出了首个交互式版权取证系统Copyright Detective，能够在黑箱条件下检测和可视化LLM输出中的版权风险，确保责任部署和透明评估。

Conclusion: Copyright Detective系统能够有效检测和分析大型语言模型输出中的潜在版权风险，支持全面和透明的版权合规评估。

Abstract: We present Copyright Detective, the first interactive forensic system for detecting, analyzing, and visualizing potential copyright risks in LLM outputs. The system treats copyright infringement versus compliance as an evidence discovery process rather than a static classification task due to the complex nature of copyright law. It integrates multiple detection paradigms, including content recall testing, paraphrase-level similarity analysis, persuasive jailbreak probing, and unlearning verification, within a unified and extensible framework. Through interactive prompting, response collection, and iterative workflows, our system enables systematic auditing of verbatim memorization and paraphrase-level leakage, supporting responsible deployment and transparent evaluation of LLM copyright risks even with black-box access.

</details>


### [18] [CoPE: Clipped RoPE as A Scalable Free Lunch for Long Context LLMs](https://arxiv.org/abs/2602.05258)
*Haoran Li,Sucheng Ren,Alan Yuille,Feng Wang*

Main category: cs.CL

TL;DR: 本文提出CoPE，通过对RoPE低频成分软截断，有效提升了长上下文处理能力，实现了256k长度上下文的性能突破，成为新的长文本建模技术标杆。


<details>
  <summary>Details</summary>
Motivation: 现有的RoPE适应长上下文的方法主要集中在两方面，一是缓解超出训练分布的频率问题，二是确保关注机制优先考虑语义相似的token。现有方法存在一定局限，需找到简洁有效的统一策略。

Method: 提出了CoPE方法，通过对RoPE中低频成分进行软截断，统一了针对长上下文的OOD缓解与语义建模两大目标，并验证了其理论有效性。

Result: 通过广泛实验，CoPE方法显著提升了模型在超长上下文（最长达256k）上的性能表现，验证了理论分析的有效性，达到了长度泛化的最新水平。

Conclusion: CoPE作为一种对Rotary Positional Embedding（RoPE）的软截断方法，有效解决了长上下文中的异常值问题，优化了语义信号，防止了硬截断引起的频谱泄漏，显著提升了模型处理长达256k上下文长度的能力，成为长度泛化的新状态。

Abstract: Rotary Positional Embedding (RoPE) is a key component of context scaling in Large Language Models (LLMs). While various methods have been proposed to adapt RoPE to longer contexts, their guiding principles generally fall into two categories: (1) out-of-distribution (OOD) mitigation, which scales RoPE frequencies to accommodate unseen positions, and (2) Semantic Modeling, which posits that the attention scores computed with RoPE should always prioritize semantically similar tokens. In this work, we unify these seemingly distinct objectives through a minimalist intervention, namely CoPE: soft clipping lowfrequency components of RoPE. CoPE not only eliminates OOD outliers and refines semantic signals, but also prevents spectral leakage caused by hard clipping. Extensive experiments demonstrate that simply applying our soft clipping strategy to RoPE yields significant performance gains that scale up to 256k context length, validating our theoretical analysis and establishing CoPE as a new state-of-the-art for length generalization. Our code, data, and models are available at https://github.com/hrlics/CoPE.

</details>


### [19] [Length-Unbiased Sequence Policy Optimization: Revealing and Controlling Response Length Variation in RLVR](https://arxiv.org/abs/2602.05261)
*Fanfan Liu,Youyang Yin,Peng Shi,Siqi Yang,Zhixiong Zeng,Haibo Qiu*

Main category: cs.CL

TL;DR: 本文分析了RLVR算法中响应长度变化的原因，提出了长度无偏策略优化算法LUSPO，显著提升了模型的推理能力和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 观察到不同RLVR算法在训练过程中响应长度变化模式差异显著，且响应长度的增长被视为推理能力增强的重要因素，因此希望系统分析响应长度变化的内在原因并提出改进策略。

Method: 通过对主流RLVR算法组成部分的深入理论分析，提出长度无偏序列策略优化算法LUSPO，修正了GSPO算法中响应长度的偏差，使损失函数对应长度无偏；并在多个推理基准和多模态场景中进行广泛实验验证。

Result: 实验表明，LUSPO算法不仅解决了响应长度崩溃问题，还在数学推理和多模态推理任务中持续实现比GRPO和GSPO更优的性能，达到了最新的优化水平。

Conclusion: 本文提出的LUSPO算法有效解决了现有RLVR算法中响应长度的偏差问题，防止了响应长度的崩溃现象，在数学推理和多模态推理任务中表现优异，达到最新的优化效果。

Abstract: Recent applications of Reinforcement Learning with Verifiable Rewards (RLVR) to Large Language Models (LLMs) and Vision-Language Models (VLMs) have demonstrated significant success in enhancing reasoning capabilities for complex tasks. During RLVR training, an increase in response length is often regarded as a key factor contributing to the growth of reasoning ability. However, the patterns of change in response length vary significantly across different RLVR algorithms during the training process. To provide a fundamental explanation for these variations, this paper conducts an in-depth analysis of the components of mainstream RLVR algorithms. We present a theoretical analysis of the factors influencing response length and validate our theory through extensive experimentation. Building upon these theoretical findings, we propose the Length-Unbiased Sequence Policy Optimization (LUSPO) algorithm. Specifically, we rectify the length bias inherent in Group Sequence Policy Optimization (GSPO), rendering its loss function unbiased with respect to response length and thereby resolving the issue of response length collapse. We conduct extensive experiments across mathematical reasoning benchmarks and multimodal reasoning scenarios, where LUSPO consistently achieves superior performance. Empirical results demonstrate that LUSPO represents a novel, state-of-the-art optimization strategy compared to existing methods such as GRPO and GSPO.

</details>


### [20] [MentorCollab: Selective Large-to-Small Inference-Time Guidance for Efficient Reasoning](https://arxiv.org/abs/2602.05307)
*Haojin Wang,Yike Wang,Shangbin Feng,Hannaneh Hajishirzi,Yulia Tsvetkov*

Main category: cs.CL

TL;DR: 提出了MentorCollab推理时协作方法，通过选择性指导小模型，实现了性能提升和推理效率的双赢。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型虽性能强但推理成本高且推理冗余，小型模型效率高但多步推理表现弱。希望通过大型模型在推理时指导小型模型以兼顾效率和性能。

Method: 提出了MentorCollab方法，在推理时随机采样token位置探测SLM与LRM的分歧，利用轻量级验证器决定SLM是否采纳LRM的短期前瞻生成，避免全程模仿。

Result: 在15对SLM-LRM组合以及数学推理、通用知识、常识推理3个领域中，方法在12种设置下提升性能，平均提升3.0%，最高8.0%，且仅需18.4%token由昂贵的LRM生成。

Conclusion: 通过选择性和稀疏地让大型推理模型（LRM）指导小型语言模型（SLM），可以在维持推理能力的同时显著降低推理成本。

Abstract: Large reasoning models (LRMs) achieve strong performance by producing long chains of thought, but their inference costs are high and often generate redundant reasoning. Small language models (SLMs) are far more efficient, yet struggle on multi-step reasoning tasks. A natural idea is to let a large model guide a small one at inference time as a mentor, yet existing collaboration methods often promote imitation, resulting in verbose reasoning without consistent error correction. We propose MentorCollab, an inference-time collaboration method in which an LRM selectively and sparsely guides an SLM, rather than taking over generation. At randomly sampled token positions, we probe for divergences between the two models and use a lightweight verifier to decide whether the SLM should follow a short lookahead segment from its mentor or continue on its own. Across 15 SLM--LRM pairs and 3 domains (math reasoning, general knowledge, and commonsense reasoning), our method improves performance in 12 settings, with average gains of 3.0% and up to 8.0%, while adopting only having 18.4% tokens generated by the expensive mentor model on average. We find that short segments and selective probing are sufficient for effective collaboration. Our results show that selective inference-time guidance restores large-model reasoning ability without substantial inference overhead.

</details>


### [21] [How Do Language Models Acquire Character-Level Information?](https://arxiv.org/abs/2602.05347)
*Soma Sato,Ryohei Sasano*

Main category: cs.CL

TL;DR: 本研究通过对比不同训练设置的语言模型，揭示了模型获得字符级知识的机制，包括分词规则和语义句法因素。


<details>
  <summary>Details</summary>
Motivation: 语言模型在没有显式字符信息的情况下能够隐式编码字符级知识，其机制尚不清楚，研究旨在揭示这些机制。

Method: 通过对比在不同受控条件（如预训练数据集或分词器）和标准条件下训练的语言模型，分析模型如何获得字符级知识。

Result: 发现分词相关的合并规则和正字法约束是主要影响因素，此外子字符串的语义关联和句法信息也是关键因素，且与分词无关。

Conclusion: 模型在训练过程中虽然未显式学习字符级信息，但通过合并规则、正字法约束、子字符串的语义关联和句法信息等机制隐式获得字符级知识。

Abstract: Language models (LMs) have been reported to implicitly encode character-level information, despite not being explicitly provided during training. However, the mechanisms underlying this phenomenon remain largely unexplored. To reveal the mechanisms, we analyze how models acquire character-level knowledge by comparing LMs trained under controlled settings, such as specifying the pre-training dataset or tokenizer, with those trained under standard settings. We categorize the contributing factors into those independent of tokenization. Our analysis reveals that merge rules and orthographic constraints constitute primary factors arising from tokenization, whereas semantic associations of substrings and syntactic information function as key factors independent of tokenization.

</details>


### [22] [PACE: Defying the Scaling Hypothesis of Exploration in Iterative Alignment for Mathematical Reasoning](https://arxiv.org/abs/2602.05370)
*Jun Rao,Zixiong Yu,Xuebo Liu,Guhan Chen,Jing Li,Jiansheng Wei,Xiaojun Meng,Min Zhang*

Main category: cs.CL

TL;DR: 作者发现传统基于大量采样的DPO方法在数学推理任务中表现受限，提出PACE通过纠正性探索生成高质量偏好样本，实现更少计算量下更优性能和更高稳健性。


<details>
  <summary>Details</summary>
Motivation: 在数学推理任务中，传统DPO通过大规模最佳样本挖掘提升性能，但作者发现这一策略带来噪声放大和分布偏移，导致性能下降和策略崩溃，亟需新方法解决该问题。

Method: 本文提出了PACE（Proximal Alignment via Corrective Exploration）方法，利用生成式纠正策略从失败探索中合成高质量的偏好样本，避免了传统DPO-R1中大规模最佳样本采样带来的噪声和分布偏移问题。

Result: 实验表明，PACE在采样数量仅为DPO-R1的1/5左右的计算预算下，性能优于DPO-R1（N=16），表现出更强的抗噪声和稳健性。

Conclusion: 本文挑战了迭代直接偏好优化（DPO）中通过增加采样数量以提升数学推理任务表现的假设，发现过度探索不仅收益递减，还可能导致策略崩溃。通过引入PACE方法，利用生成式纠正策略代替暴力采样，实现了更高效且更稳健的模型对齐。

Abstract: Iterative Direct Preference Optimization has emerged as the state-of-the-art paradigm for aligning Large Language Models on reasoning tasks. Standard implementations (DPO-R1) rely on Best-of-N sampling (e.g., $N \ge 8$) to mine golden trajectories from the distribution tail. In this paper, we challenge this scaling hypothesis and reveal a counter-intuitive phenomenon: in mathematical reasoning, aggressive exploration yields diminishing returns and even catastrophic policy collapse. We theoretically demonstrate that scaling $N$ amplifies verifier noise and induces detrimental distribution shifts. To resolve this, we introduce \textbf{PACE} (Proximal Alignment via Corrective Exploration), which replaces brute-force mining with a generation-based corrective strategy. Operating with a minimal budget ($2<N<3$), PACE synthesizes high-fidelity preference pairs from failed explorations. Empirical evaluations show that PACE outperforms DPO-R1 $(N=16)$ while using only about $1/5$ of the compute, demonstrating superior robustness against reward hacking and label noise.

</details>


### [23] [Cross-Lingual Empirical Evaluation of Large Language Models for Arabic Medical Tasks](https://arxiv.org/abs/2602.05374)
*Chaimae Abouzahir,Congbo Ma,Nizar Habash,Farah E. Shamout*

Main category: cs.CL

TL;DR: 研究揭示了大型语言模型在阿拉伯语医疗问答中的性能不足，强调了设计和评估时需关注语言特性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型多以英语为中心，导致其在低资源语言医疗任务中的性能下降，且原因未被充分理解。

Method: 通过跨语言实证分析比较大型语言模型在阿拉伯语和英语医疗问答任务上的表现，结合分词结构和模型可靠性分析。

Result: 发现阿拉伯语医疗文本存在结构碎片化现象，模型在阿拉伯语任务中的性能低于英语，且模型置信度与实际正确率相关性弱。

Conclusion: 大型语言模型在医疗任务中存在显著的语言驱动性能差异，尤其在处理复杂任务时阿拉伯语表现较差，模型的置信度和解释与准确性关联有限。

Abstract: In recent years, Large Language Models (LLMs) have become widely used in medical applications, such as clinical decision support, medical education, and medical question answering. Yet, these models are often English-centric, limiting their robustness and reliability for linguistically diverse communities. Recent work has highlighted discrepancies in performance in low-resource languages for various medical tasks, but the underlying causes remain poorly understood. In this study, we conduct a cross-lingual empirical analysis of LLM performance on Arabic and English medical question and answering. Our findings reveal a persistent language-driven performance gap that intensifies with increasing task complexity. Tokenization analysis exposes structural fragmentation in Arabic medical text, while reliability analysis suggests that model-reported confidence and explanations exhibit limited correlation with correctness. Together, these findings underscore the need for language-aware design and evaluation strategies in LLMs for medical tasks.

</details>


### [24] [IESR:Efficient MCTS-Based Modular Reasoning for Text-to-SQL with Large Language Models](https://arxiv.org/abs/2602.05385)
*Tao Liu,Jiafan Lu,Bohan Yu,Pengcheng Wu,Liu Haixin,Guoyu Xu,Li Xiangheng,Lixiao Li,Jiaming Hou,Zhao Shijun,Xinglin Lyu,Kunli Zhang,Yuxiang Jia,Hongyin Zan*

Main category: cs.CL

TL;DR: 本文提出的IESR框架通过结合轻量级语言模型、多路径蒙特卡洛树搜索和一致性验证，实现了复杂文本到SQL转换任务的最佳性能，适合实际企业应用。


<details>
  <summary>Details</summary>
Motivation: 当前文本到SQL方法虽在标准数据集表现良好，但对复杂推理、领域知识和假设性查询支持不足，且企业部署成本高，需轻量且高效的解决方案。

Method: 提出IESR框架，包括利用大语言模型进行关键信息理解和模式链接，使用蒙特卡洛树搜索（MCTS）进行多路径推理并采用多数投票机制，结合轨迹一致性验证模块保障结果准确性。

Result: 在LogicCat和Archer这两个复杂推理数据库上，IESR分别达到了24.28和37.28的EX指标，优于现有方法，实现了无需微调的高效表现。

Conclusion: IESR框架在复杂推理文本到SQL转换任务中取得了最先进的性能，同时保持轻量级模型，无需微调，提升了实用性和效率。

Abstract: Text-to-SQL is a key natural language processing task that maps natural language questions to SQL queries, enabling intuitive interaction with web-based databases. Although current methods perform well on benchmarks like BIRD and Spider, they struggle with complex reasoning, domain knowledge, and hypothetical queries, and remain costly in enterprise deployment. To address these issues, we propose a framework named IESR(Information Enhanced Structured Reasoning) for lightweight large language models: (i) leverages LLMs for key information understanding and schema linking, and decoupling mathematical computation and SQL generation, (ii) integrates a multi-path reasoning mechanism based on Monte Carlo Tree Search (MCTS) with majority voting, and (iii) introduces a trajectory consistency verification module with a discriminator model to ensure accuracy and consistency. Experimental results demonstrate that IESR achieves state-of-the-art performance on the complex reasoning benchmark LogicCat (24.28 EX) and the Archer dataset (37.28 EX) using only compact lightweight models without fine-tuning. Furthermore, our analysis reveals that current coder models exhibit notable biases and deficiencies in physical knowledge, mathematical computation, and common-sense reasoning, highlighting important directions for future research. We released code at https://github.com/Ffunkytao/IESR-SLM.

</details>


### [25] [Beyond Length: Context-Aware Expansion and Independence as Developmentally Sensitive Evaluation in Child Utterances](https://arxiv.org/abs/2602.05392)
*Jiyun Chun,Eric Fosler-Lussier,Michael White,Andrew Perrault*

Main category: cs.CL

TL;DR: 本文提出了一种基于大型语言模型的儿童话语质量评估方法，突破传统长度导向指标，关注话语的扩展性与独立性，更全面地反映儿童语言发展及其在对话中的贡献。


<details>
  <summary>Details</summary>
Motivation: 现有的儿童话语质量评价指标如平均话语长度、词汇多样性和可读性指数过于依赖长度，忽略了对话的语境和话语功能，无法全面反映儿童语言表达的深度和独立性，因此需要引入更具语境敏感性的新评价方法。

Method: 通过引入大型语言模型作为评判者，先分类成人前一句话的类型，再根据扩展性（上下文详细程度和推理深度）与独立性（儿童推动话题发展的能力）两个轴对儿童回应进行评分。结合发展心理学理论，设计了能捕捉语法、推理和话题控制的评价指标。

Result: 该框架展现出良好的发展效度和语义敏感性，评价指标能反映年龄相关的语言发展规律，增强了年龄估计的准确性，同时准确捕捉了话语间的语义关系变化，指标与人类评价高度一致，支持大规模儿童话语质量的自动评估。

Conclusion: 本研究提出了一个基于大型语言模型的评判框架，有效评估儿童在与成人对话中话语的质量，弥补了传统指标忽视对话上下文的不足。该框架综合考虑话语的扩展性和独立性，反映儿童语言发展的关键维度，并与年龄相关性和语义敏感性相符，能够更准确地预测儿童年龄和评价话语质量。

Abstract: Evaluating the quality of children's utterances in adult-child dialogue remains challenging due to insufficient context-sensitive metrics. Common proxies such as Mean Length of Utterance (MLU), lexical diversity (vocd-D), and readability indices (Flesch-Kincaid Grade Level, Gunning Fog Index) are dominated by length and ignore conversational context, missing aspects of response quality such as reasoning depth, topic maintenance, and discourse planning. We introduce an LLM-as-a-judge framework that first classifies the Previous Adult Utterance Type and then scores the child's response along two axes: Expansion (contextual elaboration and inferential depth) and Independence (the child's contribution to advancing the discourse). These axes reflect fundamental dimensions in child language development, where Expansion captures elaboration, clause combining, and causal and contrastive connectives. Independence captures initiative, topic control, decreasing reliance on adult scaffolding through growing self-regulation, and audience design. We establish developmental validity by showing age-related patterns and demonstrate predictive value by improving age estimation over common baselines. We further confirm semantic sensitivity by detecting differences tied to discourse relations. Our metrics align with human judgments, enabling large-scale evaluation. This shifts child utterance assessment from simply measuring length to evaluating how meaningfully the child's speech contributes to and advances the conversation within its context.

</details>


### [26] [Late-to-Early Training: LET LLMs Learn Earlier, So Faster and Better](https://arxiv.org/abs/2602.05393)
*Ji Zhao,Yufei Gu,Shitong Shao,Xun Zhou,Liang Xiang,Zeke Xie*

Main category: cs.CL

TL;DR: 本文提出了一种Late-to-Early Training方法，通过用小型预训练模型后期层表征引导大模型早期层训练，显著加速了大型语言模型的训练速度，并提升了下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型预训练耗时长且计算资源需求高，限制了其快速发展，尝试利用小型已预训练模型加速更大模型的训练尚未被充分探索。

Method: 提出Late-to-Early Training (LET)范式，通过在早期训练阶段利用已预训练模型后期层的表征来指导目标模型的早期层，以实现知识的早传递。通过晚期到早期步骤学习和晚期到早期层学习两大机制加速收敛。

Result: 在1.4B和7B参数规模模型上进行了大量实验，显示LET方法相比标准训练在PILE数据集上训练1.4B模型速度提升1.6倍，下游任务准确率提升约5%。即便使用参数量只有目标模型十分之一的小型预训练模型亦表现良好。

Conclusion: LET范式有效利用小型预训练模型加速大型语言模型训练，提升训练效率和模型性能，解决了大模型预训练计算成本高的瓶颈问题。

Abstract: As Large Language Models (LLMs) achieve remarkable empirical success through scaling model and data size, pretraining has become increasingly critical yet computationally prohibitive, hindering rapid development. Despite the availability of numerous pretrained LLMs developed at significant computational expense, a fundamental real-world question remains underexplored: \textit{Can we leverage existing small pretrained models to accelerate the training of larger models?} In this paper, we propose a Late-to-Early Training (LET) paradigm that enables LLMs to explicitly learn later knowledge in earlier steps and earlier layers. The core idea is to guide the early layers of an LLM during early training using representations from the late layers of a pretrained (i.e. late training phase) model. We identify two key mechanisms that drive LET's effectiveness: late-to-early-step learning and late-to-early-layer learning. These mechanisms significantly accelerate training convergence while robustly enhancing both language modeling capabilities and downstream task performance, enabling faster training with superior performance. Extensive experiments on 1.4B and 7B parameter models demonstrate LET's efficiency and effectiveness. Notably, when training a 1.4B LLM on the Pile dataset, our method achieves up to 1.6$\times$ speedup with nearly 5\% improvement in downstream task accuracy compared to standard training, even when using a pretrained model with 10$\times$ fewer parameters than the target model.

</details>


### [27] [OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration](https://arxiv.org/abs/2602.05400)
*Shaobo Wang,Xuan Ouyang,Tianyi Xu,Yuzheng Hu,Jialin Liu,Guo Chen,Tianyu Zhang,Junhao Zheng,Kexin Yang,Xingzhang Ren,Dayiheng Liu,Linfeng Zhang*

Main category: cs.CL

TL;DR: OPUS提出了一种基于优化器更新动态的数据选择方法，大幅提升预训练数据利用率和模型性能，显著节省计算资源。


<details>
  <summary>Details</summary>
Motivation: 随着高质量公共文本资源逐渐枯竭，预训练需从更多tokens转向更优质tokens，但现有方法多依赖启发式静态过滤或基于原始梯度的优化器无关动态标准，存在不足。

Method: 提出了OPUS框架，通过将数据候选的有效更新投影到由稳定分布代理导出的目标方向来评分，借助Ghost技术和CountSketch实现计算效率，并采用Boltzmann采样保证数据多样性。

Result: OPUS在GPT-2 Large/XL预训练中以仅30B tokens超过了全量200B tokens训练，结合工业级静态过滤进一步提高效率，在Qwen3-8B-Base的领域特定持续预训练中仅用0.5B tokens取得优于3B tokens的性能，显著提升数据效率。

Conclusion: OPUS通过在优化器诱导的更新空间中定义数据效用，实现了动态数据选择，在多个数据集、优化器和模型规模上表现优异，显著提升了预训练效率和数据使用效率。

Abstract: As high-quality public text approaches exhaustion, a phenomenon known as the Data Wall, pre-training is shifting from more tokens to better tokens. However, existing methods either rely on heuristic static filters that ignore training dynamics, or use dynamic yet optimizer-agnostic criteria based on raw gradients. We propose OPUS (Optimizer-induced Projected Utility Selection), a dynamic data selection framework that defines utility in the optimizer-induced update space. OPUS scores candidates by projecting their effective updates, shaped by modern optimizers, onto a target direction derived from a stable, in-distribution proxy. To ensure scalability, we employ Ghost technique with CountSketch for computational efficiency, and Boltzmann sampling for data diversity, incurring only 4.7\% additional compute overhead. OPUS achieves remarkable results across diverse corpora, quality tiers, optimizers, and model scales. In pre-training of GPT-2 Large/XL on FineWeb and FineWeb-Edu with 30B tokens, OPUS outperforms industrial-level baselines and even full 200B-token training. Moreover, when combined with industrial-level static filters, OPUS further improves pre-training efficiency, even with lower-quality data. Furthermore, in continued pre-training of Qwen3-8B-Base on SciencePedia, OPUS achieves superior performance using only 0.5B tokens compared to full training with 3B tokens, demonstrating significant data efficiency gains in specialized domains.

</details>


### [28] [Grammatical Error Correction Evaluation by Optimally Transporting Edit Representation](https://arxiv.org/abs/2602.05419)
*Takumi Goto,Yusuke Sakai,Taro Watanabe*

Main category: cs.CL

TL;DR: 本文提出了利用编辑向量和不平衡最优传输的新型GEC自动评价指标UOT-ERRANT，解决了传统嵌入相似度指标难以准确评价的问题，在提升评价性能和可解释性方面取得显著进展。


<details>
  <summary>Details</summary>
Motivation: 目前基于嵌入的相似度度量在语法错误纠正评价中表现不佳，原因在于源句子中大量单词未发生变化，导致传统相似度指标难以准确衡量假设与参考的差异。因而需要一种专门针对GEC编辑的评价指标来提高评价准确性。

Method: 提出编辑向量表示每个编辑，并利用不平衡最优传输方法将假设与参考的编辑向量进行匹配，形成新的评价指标UOT-ERRANT。结合ERRANT编辑工具对源句子中的编辑进行度量，通过SEEDA元评价验证了指标性能。

Result: UOT-ERRANT在SEEDA元评价中表现优异，特别是在+Fluency领域，显著提升了评价准确率；其传输计划提供软编辑对齐，增强了评价过程的可解释性。

Conclusion: 本文提出了基于编辑向量和不平衡最优传输的自动评价指标UOT-ERRANT，显著提升了语法错误纠正系统的评价性能，尤其是在编辑密集的流畅性领域。该指标不仅性能优越，还具有良好的可解释性，能为系统排名及分析提供有力支持。

Abstract: Automatic evaluation in grammatical error correction (GEC) is crucial for selecting the best-performing systems. Currently, reference-based metrics are a popular choice, which basically measure the similarity between hypothesis and reference sentences. However, similarity measures based on embeddings, such as BERTScore, are often ineffective, since many words in the source sentences remain unchanged in both the hypothesis and the reference. This study focuses on edits specifically designed for GEC, i.e., ERRANT, and computes similarity measured over the edits from the source sentence. To this end, we propose edit vector, a representation for an edit, and introduce a new metric, UOT-ERRANT, which transports these edit vectors from hypothesis to reference using unbalanced optimal transport. Experiments with SEEDA meta-evaluation show that UOT-ERRANT improves evaluation performance, particularly in the +Fluency domain where many edits occur. Moreover, our method is highly interpretable because the transport plan can be interpreted as a soft edit alignment, making UOT-ERRANT a useful metric for both system ranking and analyzing GEC systems. Our code is available from https://github.com/gotutiyan/uot-errant.

</details>


### [29] [Once Correct, Still Wrong: Counterfactual Hallucination in Multilingual Vision-Language Models](https://arxiv.org/abs/2602.05437)
*Basel Mousi,Fahim Dalvi,Shammur Chowdhury,Firoj Alam,Nadir Durrani*

Main category: cs.CL

TL;DR: 提出了第一个针对中东北非文化背景的多语种视觉语言模型幻觉评测基准M2CQA及反事实幻觉率指标，揭示了模型在阿拉伯语环境下的幻觉问题及提示策略的影响。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型容易接受文化上合理但视觉上错误的解释，尤其是在非西方语境与非英语环境中，现有幻觉评测缺乏针对这一问题的考察，因此提出新的多文化多语种基准评测。

Method: 构建涵盖17个中东北非国家图像及对应真实和反事实语句的多语种数据集M2CQA，设计反事实幻觉率（CFHR）评价指标，利用多种提示策略评估当前视觉语言模型性能。

Result: 在多语种评测中，阿拉伯语及其方言的反事实幻觉率显著上升，显示语言和文化背景影响模型幻觉表现，且提示策略对幻觉率有明显影响。

Conclusion: 当前视觉语言模型在多文化背景下，尤其是阿拉伯语及其方言中存在较高的幻觉接受率，这一现象在传统准确率评测中难以察觉。通过提出M2CQA基准和反事实幻觉率指标，发现推理先行的提示策略会增加幻觉，而先回答后辩解则能提升模型的鲁棒性。

Abstract: Vision-language models (VLMs) can achieve high accuracy while still accepting culturally plausible but visually incorrect interpretations. Existing hallucination benchmarks rarely test this failure mode, particularly outside Western contexts and English. We introduce M2CQA, a culturally grounded multimodal benchmark built from images spanning 17 MENA countries, paired with contrastive true and counterfactual statements in English, Arabic, and its dialects. To isolate hallucination beyond raw accuracy, we propose the CounterFactual Hallucination Rate (CFHR), which measures counterfactual acceptance conditioned on correctly answering the true statement. Evaluating state-of-the-art VLMs under multiple prompting strategies, we find that CFHR rises sharply in Arabic, especially in dialects, even when true-statement accuracy remains high. Moreover, reasoning-first prompting consistently increases counterfactual hallucination, while answering before justifying improves robustness. We will make the experimental resources and dataset publicly available for the community.

</details>


### [30] [Causal Front-Door Adjustment for Robust Jailbreak Attacks on LLMs](https://arxiv.org/abs/2602.05444)
*Yao Zhou,Zeen Song,Wenwen Qiang,Fengge Wu,Shuyi Zhou,Changwen Zheng,Hui Xiong*

Main category: cs.CL

TL;DR: 本文从因果视角建模LLM安全机制，提出CFA^2攻击方法，通过稀疏自编码器剥离防御特征，实现高效且可解释的绕过，攻破当前最强防御。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型中的安全对齐机制作为潜在的内部状态，掩盖了模型的固有能力，导致现有防御难以有效辨识和绕过。

Method: 从因果推断角度出发，将安全机制视为未观察到的混杂变量，提出利用Pearl的前门准则设计的因果前门调整攻击（CFA^2），通过稀疏自编码器剥离防御特征，并用确定性干预简化计算过程。

Result: CFA^2在攻击成功率上达到最新水平，同时提供了对绕过过程的机制性解释，验证了方法的有效性和低推理复杂度。

Conclusion: 本文引入因果推断框架，有效揭示并绕过大型语言模型中的安全机制，实现了强有力且可解释的解锁攻击。

Abstract: Safety alignment mechanisms in Large Language Models (LLMs) often operate as latent internal states, obscuring the model's inherent capabilities. Building on this observation, we model the safety mechanism as an unobserved confounder from a causal perspective. Then, we propose the \textbf{C}ausal \textbf{F}ront-Door \textbf{A}djustment \textbf{A}ttack ({\textbf{CFA}}$^2$) to jailbreak LLM, which is a framework that leverages Pearl's Front-Door Criterion to sever the confounding associations for robust jailbreaking. Specifically, we employ Sparse Autoencoders (SAEs) to physically strip defense-related features, isolating the core task intent. We further reduce computationally expensive marginalization to a deterministic intervention with low inference complexity. Experiments demonstrate that {CFA}$^2$ achieves state-of-the-art attack success rates while offering a mechanistic interpretation of the jailbreaking process.

</details>


### [31] [Structured Context Engineering for File-Native Agentic Systems: Evaluating Schema Accuracy, Format Effectiveness, and Multi-File Navigation at Scale](https://arxiv.org/abs/2602.05447)
*Damon McMillan*

Main category: cs.CL

TL;DR: 通过对多模型多格式多规模数据库的近万次实验，研究了大语言模型操作结构化系统时上下文设计对性能的影响，提出应根据模型能力定制上下文结构，挑战了一些常见假设。


<details>
  <summary>Details</summary>
Motivation: 缺乏实践中针对大型语言模型(agent)使用程序化接口操作外部结构化系统时如何设计上下文的实证指导。

Method: 通过9649次实验，比较11个模型在不同格式（YAML、Markdown、JSON、TOON）和不同规模的数据库架构（10到10000表）上生成SQL的表现，系统研究了上下文工程对结构化数据的影响。

Result: 发现文件型上下文架构对前沿模型（如Claude, GPT, Gemini）提升准确率，而开源模型效果不一；格式对整体准确率无显著影响但开源模型有格式敏感性；模型能力是准确率差异的主导因素；大规模文件分区方案保持高导航准确率；紧凑格式在大规模下可能因搜索模式带来更多计算消耗。

Conclusion: 架构选择应根据模型能力定制，不同模型对上下文结构有不同响应，格式对整体准确性影响不大，模型能力是影响准确性的主要因素，文件本地代理能够有效扩展至大型结构化数据。

Abstract: Large Language Model agents increasingly operate external systems through programmatic interfaces, yet practitioners lack empirical guidance on how to structure the context these agents consume. Using SQL generation as a proxy for programmatic agent operations, we present a systematic study of context engineering for structured data, comprising 9,649 experiments across 11 models, 4 formats (YAML, Markdown, JSON, Token-Oriented Object Notation [TOON]), and schemas ranging from 10 to 10,000 tables.
  Our findings challenge common assumptions. First, architecture choice is model-dependent: file-based context retrieval improves accuracy for frontier-tier models (Claude, GPT, Gemini; +2.7%, p=0.029) but shows mixed results for open source models (aggregate -7.7%, p<0.001), with deficits varying substantially by model. Second, format does not significantly affect aggregate accuracy (chi-squared=2.45, p=0.484), though individual models, particularly open source, exhibit format-specific sensitivities. Third, model capability is the dominant factor, with a 21 percentage point accuracy gap between frontier and open source tiers that dwarfs any format or architecture effect. Fourth, file-native agents scale to 10,000 tables through domain-partitioned schemas while maintaining high navigation accuracy. Fifth, file size does not predict runtime efficiency: compact formats can consume significantly more tokens at scale due to format-unfamiliar search patterns.
  These findings provide practitioners with evidence-based guidance for deploying LLM agents on structured systems, demonstrating that architectural decisions should be tailored to model capability rather than assuming universal best practices.

</details>


### [32] [Reasoning under Ambiguity: Uncertainty-Aware Multilingual Emotion Classification under Partial Supervision](https://arxiv.org/abs/2602.05471)
*Md. Mithun Hossaina,Mashary N. Alrasheedy,Nirban Bhowmick,Shamim Forhad,Md. Shakil Hossain,Sudipto Chaki,Md Shafiqul Islam*

Main category: cs.CL

TL;DR: 本文针对多语言多标签情感分类中的标注不确定性问题，提出一套不确定性感知的学习框架，通过多语言编码器、熵权重机制和正-无标记正则化，实现了部分监督下的稳健训练，显著提升了分类性能和模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前知识系统中情感识别存在情感状态共现导致的模糊性，以及标签缺失或异质性带来的监督不完整性，现有方法假设标签完全观测且目标确定，导致偏差和不可靠预测，亟需考虑标注不确定性以提升多语言多标签情感识别的准确性与鲁棒性。

Method: 设计了一个基于不确定性的多语言多标签情感识别框架，采用共享的多语言编码器结合语言特定优化，利用基于熵的模糊加权机制降低高度模糊训练样本的权重，并引入带正-无标记正则化的掩码感知目标，实现对部分监督下的鲁棒学习。

Result: 在英文、西班牙语和阿拉伯语情感分类基准上，所提方法在多个评价指标上均显著优于强基线，展现了更好的训练稳定性、对标注稀疏的鲁棒性和增强的模型解释能力。

Conclusion: 本文提出的“模糊推理”框架显著提升了多语言多标签情感分类的性能，其方法有效处理了情感标注的不确定性和缺失问题，增强了模型的训练稳定性和解释性。

Abstract: Contemporary knowledge-based systems increasingly rely on multilingual emotion identification to support intelligent decision-making, yet they face major challenges due to emotional ambiguity and incomplete supervision. Emotion recognition from text is inherently uncertain because multiple emotional states often co-occur and emotion annotations are frequently missing or heterogeneous. Most existing multi-label emotion classification methods assume fully observed labels and rely on deterministic learning objectives, which can lead to biased learning and unreliable predictions under partial supervision. This paper introduces Reasoning under Ambiguity, an uncertainty-aware framework for multilingual multi-label emotion classification that explicitly aligns learning with annotation uncertainty. The proposed approach uses a shared multilingual encoder with language-specific optimization and an entropy-based ambiguity weighting mechanism that down-weights highly ambiguous training instances rather than treating missing labels as negative evidence. A mask-aware objective with positive-unlabeled regularization is further incorporated to enable robust learning under partial supervision. Experiments on English, Spanish, and Arabic emotion classification benchmarks demonstrate consistent improvements over strong baselines across multiple evaluation metrics, along with improved training stability, robustness to annotation sparsity, and enhanced interpretability.

</details>


### [33] [Transport and Merge: Cross-Architecture Merging for Large Language Models](https://arxiv.org/abs/2602.05495)
*Chenhang Cui,Binyun Yang,Fei Shen,Yuxin Chen,Jingnan Zheng,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.CL

TL;DR: 本文提出一种基于最优传输的跨架构模型合并方法，实现大型高资源语言模型向异构小型低资源模型的知识迁移，并在多种低资源任务中取得性能提升。


<details>
  <summary>Details</summary>
Motivation: 真实部署中常需要将大型高资源模型的知识迁移到结构不同的小型低资源模型，现有模型合并方法多依赖架构兼容性，限制了知识转移的范围。

Method: 提出基于最优传输的跨架构模型合并框架，通过对激活的对齐推断跨神经元对应关系，利用传输计划指导权重空间融合，实现异构模型间的知识迁移。

Result: 方法在低资源语言和专业领域实验中，显著提升了目标小型模型的性能，显示出跨架构迁移的有效性。

Conclusion: 基于最优传输的跨架构模型合并提供了一种有效方案，实现从大型高资源模型向异构小型低资源模型的知识迁移，扩展了模型合并的适用范围。

Abstract: Large language models (LLMs) achieve strong capabilities by scaling model capacity and training data, yet many real-world deployments rely on smaller models trained or adapted from low-resource data. This gap motivates the need for mechanisms to transfer knowledge from large, high-resource models to smaller, low-resource targets. While model merging provides an effective transfer mechanism, most existing approaches assume architecture-compatible models and therefore cannot directly transfer knowledge from large high-resource LLMs to heterogeneous low-resource targets. In this work, we propose a cross-architecture merging framework based on optimal transport (OT) that aligns activations to infer cross-neuron correspondences between heterogeneous models. The resulting transport plans are then used to guide direct weight-space fusion, enabling effective high-resource to low-resource transfer using only a small set of inputs. Extensive experiments across low-resource languages and specialized domains demonstrate consistent improvements over target models.

</details>


### [34] [A Human-in-the-Loop, LLM-Centered Architecture for Knowledge-Graph Question Answering](https://arxiv.org/abs/2602.05512)
*Larissa Pusch,Alexandre Courtiol,Tim Conrad*

Main category: cs.CL

TL;DR: 本文提出一个基于大型语言模型的交互式知识图谱查询框架，提升了复杂查询的可访问性和准确性，解决了现有方法多跳推理和可解释性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在知识密集型领域存在幻觉、过时信息和可解释性差的问题，文本检索增强生成方法难以处理多跳推理，而知识图谱虽支持精确查询但对查询语言要求高。

Method: 设计一个交互式框架，利用大型语言模型生成和解释Cypher图查询，用户可通过自然语言迭代地完善查询。

Result: 在合成电影知识图谱上的90个查询基准测试中，评估了查询解释质量和错误检测能力，并在真实的Hyena KG和MaRDI KG上完成了小规模查询生成实验，验证了方法的有效性。

Conclusion: 该工作通过引入一个交互式框架，使大型语言模型生成并解释Cypher图查询，并通过自然语言进行迭代优化，有效提升了复杂知识图谱的可访问性，同时保证了事实准确性和语义严谨性。

Abstract: Large Language Models (LLMs) excel at language understanding but remain limited in knowledge-intensive domains due to hallucinations, outdated information, and limited explainability. Text-based retrieval-augmented generation (RAG) helps ground model outputs in external sources but struggles with multi-hop reasoning. Knowledge Graphs (KGs), in contrast, support precise, explainable querying, yet require a knowledge of query languages. This work introduces an interactive framework in which LLMs generate and explain Cypher graph queries and users iteratively refine them through natural language. Applied to real-world KGs, the framework improves accessibility to complex datasets while preserving factual accuracy and semantic rigor and provides insight into how model performance varies across domains. Our core quantitative evaluation is a 90-query benchmark on a synthetic movie KG that measures query explanation quality and fault detection across multiple LLMs, complemented by two smaller real-life query-generation experiments on a Hyena KG and the MaRDI (Mathematical Research Data Initiative) KG.

</details>


### [35] [Multi-Task GRPO: Reliable LLM Reasoning Across Tasks](https://arxiv.org/abs/2602.05547)
*Shyam Sundhar Ramesh,Xiaotong Ji,Matthieu Zimmer,Sangwoong Yoon,Zhiyong Wang,Haitham Bou Ammar,Aurelien Lucchi,Ilija Bogunovic*

Main category: cs.CL

TL;DR: 为解决多任务GRPO中任务不平衡及优化信号失真问题，提出MT-GRPO，通过动态权重调整和比例采样实现任务表现均衡和高效训练，显著提升最弱任务准确率。


<details>
  <summary>Details</summary>
Motivation: 传统GRPO的多任务适应存在任务主导和任务停滞不平衡问题，且不同任务零优势样本频率差异导致优化信号失真，需一种能平衡任务表现的方法。

Method: 提出MT-GRPO算法，包括动态调整任务权重以优化最弱任务表现，以及引入比例保持采样器确保策略梯度与权重相符。

Result: 在3任务和9任务设置中，MT-GRPO在最差任务准确率上比标准GRPO提升16-28%，比DAPO提升6%，且训练步数减少50%，保持了竞争力的平均准确率。

Conclusion: MT-GRPO算法有效提升了多任务环境下的大型语言模型在最差任务上的性能，实现了更均衡的任务进展和更高效的训练过程。

Abstract: RL-based post-training with GRPO is widely used to improve large language models on individual reasoning tasks. However, real-world deployment requires reliable performance across diverse tasks. A straightforward multi-task adaptation of GRPO often leads to imbalanced outcomes, with some tasks dominating optimization while others stagnate. Moreover, tasks can vary widely in how frequently prompts yield zero advantages (and thus zero gradients), which further distorts their effective contribution to the optimization signal. To address these issues, we propose a novel Multi-Task GRPO (MT-GRPO) algorithm that (i) dynamically adapts task weights to explicitly optimize worst-task performance and promote balanced progress across tasks, and (ii) introduces a ratio-preserving sampler to ensure task-wise policy gradients reflect the adapted weights. Experiments on both 3-task and 9-task settings show that MT-GRPO consistently outperforms baselines in worst-task accuracy. In particular, MT-GRPO achieves 16-28% and 6% absolute improvement on worst-task performance over standard GRPO and DAPO, respectively, while maintaining competitive average accuracy. Moreover, MT-GRPO requires 50% fewer training steps to reach 50% worst-task accuracy in the 3-task setting, demonstrating substantially improved efficiency in achieving reliable performance across tasks.

</details>


### [36] [CASTLE: A Comprehensive Benchmark for Evaluating Student-Tailored Personalized Safety in Large Language Models](https://arxiv.org/abs/2602.05633)
*Rui Jia,Ruiyi Lan,Fengrui Liu,Zhongxiang Dai,Bo Jiang,Jing Shao,Jingyuan Chen,Guandong Xu,Fei Wu,Min Zhang*

Main category: cs.CL

TL;DR: 该论文提出学生定制个性化安全评估基准CASTLE，揭示当前大语言模型在个性化教育安全上的明显不足。


<details>
  <summary>Details</summary>
Motivation: 现有安全评估欠缺针对不同学生属性对同一响应可能产生的差异性伤害的考量，导致个性化学习中的安全风险无法充分检测和评估。

Method: 提出了基于教育理论的学生定制个性化安全概念，构建了包含15种教育安全风险和14个学生属性的多语言场景基准CASTLE，并设计了风险敏感性、情感共情和学生匹配度三个评价指标。

Result: 在18个最先进大语言模型上测试，所有模型的安全平均评分不足2.3（满分5分），表明当前模型在个性化安全保证方面表现较差。

Conclusion: 现有大语言模型在个性化学习中的安全保障存在较大不足，难以有效识别和适应不同学生的认知和心理差异，存在潜在的安全风险。

Abstract: Large language models (LLMs) have advanced the development of personalized learning in education. However, their inherent generation mechanisms often produce homogeneous responses to identical prompts. This one-size-fits-all mechanism overlooks the substantial heterogeneity in students cognitive and psychological, thereby posing potential safety risks to vulnerable groups. Existing safety evaluations primarily rely on context-independent metrics such as factual accuracy, bias, or toxicity, which fail to capture the divergent harms that the same response might cause across different student attributes. To address this gap, we propose the concept of Student-Tailored Personalized Safety and construct CASTLE based on educational theories. This benchmark covers 15 educational safety risks and 14 student attributes, comprising 92,908 bilingual scenarios. We further design three evaluation metrics: Risk Sensitivity, measuring the model ability to detect risks; Emotional Empathy, evaluating the model capacity to recognize student states; and Student Alignment, assessing the match between model responses and student attributes. Experiments on 18 SOTA LLMs demonstrate that CASTLE poses a significant challenge: all models scored below an average safety rating of 2.3 out of 5, indicating substantial deficiencies in personalized safety assurance.

</details>


### [37] [Modelling the Morphology of Verbal Paradigms: A Case Study in the Tokenization of Turkish and Hebrew](https://arxiv.org/abs/2602.05648)
*Giuseppe Samo,Paola Merlo*

Main category: cs.CL

TL;DR: 本文比较了变换器模型在土耳其语和希伯来语复杂动词范式上的表现，发现分词方式显著影响模型效果，词素感知分词有助于处理非拼接语言形态。


<details>
  <summary>Details</summary>
Motivation: 研究变换器模型如何表示土耳其语和现代希伯来语中的复杂动词范式，尤其关注分词策略如何影响这种能力。

Method: 利用Blackbird语言矩阵任务在自然数据上评估不同模型（单语和多语）及不同分词策略（原子分词、小子词单元、基于词素的分词、字符级分词）对两种语言动词形态的表示能力。

Result: 土耳其语中，无论是原子分词还是小子词分词，单语和多语模型均表现良好。希伯来语中，多语模型使用字符级分词未能捕捉非拼接形态，但采用词素感知分词的单语模型表现出色。所有模型在更具合成性的合成数据集上表现有所提升。

Conclusion: 分词策略对变换器模型捕捉不同语言形态结构具有显著影响，透明且拼接性形态的语言表现更稳定，复杂非拼接形态需词素感知分词才能有效建模。

Abstract: We investigate how transformer models represent complex verb paradigms in Turkish and Modern Hebrew, concentrating on how tokenization strategies shape this ability. Using the Blackbird Language Matrices task on natural data, we show that for Turkish -- with its transparent morphological markers -- both monolingual and multilingual models succeed, either when tokenization is atomic or when it breaks words into small subword units. For Hebrew, instead, monolingual and multilingual models diverge. A multilingual model using character-level tokenization fails to capture the language non-concatenative morphology, but a monolingual model with morpheme-aware segmentation performs well. Performance improves on more synthetic datasets, in all models.

</details>


### [38] [MedErrBench: A Fine-Grained Multilingual Benchmark for Medical Error Detection and Correction with Clinical Expert Annotations](https://arxiv.org/abs/2602.05692)
*Congbo Ma,Yichun Zhang,Yousef Al-Jazzazi,Ahamed Foisal,Laasya Sharma,Yousra Sadqi,Khaled Saleh,Jihad Mallat,Farah E. Shamout*

Main category: cs.CL

TL;DR: 推出首个多语言临床错误检测基准MedErrBench，评估多种模型，发现非英语环境下性能不足，推动更安全公平的医疗AI应用。


<details>
  <summary>Details</summary>
Motivation: 现有临床文本中的错误可能导致严重后果，尤其是在误诊或错误治疗建议时。随着大语言模型（LLMs）在医疗领域的广泛应用，亟需多语言、不同情境下的系统评估基准。

Method: 构建MedErrBench，这是第一个多语言临床错误检测、定位和纠正基准，涵盖英语、阿拉伯语和中文。基于十种常见错误类型，数据经过临床专家注释和审核。评估了通用语言模型、特定语言模型及医疗领域模型的表现。

Result: 实验结果显示各模型存在较大性能差距，尤其在非英语环境中表现不足，强调了开发临床背景、语言敏感系统的必要性。

Conclusion: MedErrBench作为公开的多语言临床错误检测基准，促进了基于AI的更安全、更公平的全球医疗发展。

Abstract: Inaccuracies in existing or generated clinical text may lead to serious adverse consequences, especially if it is a misdiagnosis or incorrect treatment suggestion. With Large Language Models (LLMs) increasingly being used across diverse healthcare applications, comprehensive evaluation through dedicated benchmarks is crucial. However, such datasets remain scarce, especially across diverse languages and contexts. In this paper, we introduce MedErrBench, the first multilingual benchmark for error detection, localization, and correction, developed under the guidance of experienced clinicians. Based on an expanded taxonomy of ten common error types, MedErrBench covers English, Arabic and Chinese, with natural clinical cases annotated and reviewed by domain experts. We assessed the performance of a range of general-purpose, language-specific, and medical-domain language models across all three tasks. Our results reveal notable performance gaps, particularly in non-English settings, highlighting the need for clinically grounded, language-aware systems. By making MedErrBench and our evaluation protocols publicly-available, we aim to advance multilingual clinical NLP to promote safer and more equitable AI-based healthcare globally. The dataset is available in the supplementary material. An anonymized version of the dataset is available at: https://github.com/congboma/MedErrBench.

</details>


### [39] [Consensus-Aligned Neuron Efficient Fine-Tuning Large Language Models for Multi-Domain Machine Translation](https://arxiv.org/abs/2602.05694)
*Shuting Jiang,Ran Song,Yuxin Huang,Yan Xiang,Yantuan Xian,Shengxiang Gao,Zhengtao Yu*

Main category: cs.CL

TL;DR: 本文提出基于神经元选择的微调方法，提高大语言模型多领域机器翻译的领域适应性，实现了更优的翻译性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽在机器翻译有出色表现，但领域适应仍存在挑战，现有多领域机器翻译方法在领域迁移、参数干扰和泛化能力上存在缺陷。

Method: 通过最大化神经元行为与领域特征的互信息，选择关键神经元，并基于这些神经元对大语言模型进行微调，从而减轻参数干扰和领域过拟合。

Result: 在三个大语言模型和十个德英、英中文本翻译领域的综合实验中，本方法在已见和未见领域均优于强劲的参数高效微调基线，达到了最先进水平。

Conclusion: 本文提出的基于神经元高效微调框架，通过识别和更新共识对齐的神经元，实现了多领域机器翻译中领域适应的有效提升，显著优于现有参数高效微调方法。

Abstract: Multi-domain machine translation (MDMT) aims to build a unified model capable of translating content across diverse domains. Despite the impressive machine translation capabilities demonstrated by large language models (LLMs), domain adaptation still remains a challenge for LLMs. Existing MDMT methods such as in-context learning and parameter-efficient fine-tuning often suffer from domain shift, parameter interference and limited generalization. In this work, we propose a neuron-efficient fine-tuning framework for MDMT that identifies and updates consensus-aligned neurons within LLMs. These neurons are selected by maximizing the mutual information between neuron behavior and domain features, enabling LLMs to capture both generalizable translation patterns and domain-specific nuances. Our method then fine-tunes LLMs guided by these neurons, effectively mitigating parameter interference and domain-specific overfitting. Comprehensive experiments on three LLMs across ten German-English and Chinese-English translation domains evidence that our method consistently outperforms strong PEFT baselines on both seen and unseen domains, achieving state-of-the-art performance.

</details>


### [40] [OmniMoE: An Efficient MoE by Orchestrating Atomic Experts at Scale](https://arxiv.org/abs/2602.05711)
*Jingze Shi,Zhangyang Peng,Yizhang Zhu,Yifan Wu,Guang Liu,Yuyu Luo*

Main category: cs.CL

TL;DR: OmniMoE通过系统算法协同设计，实现了极细粒度的专家网络，显著提升准确率和推理速度，突破了MoE模型在效率与粒度上的传统权衡。


<details>
  <summary>Details</summary>
Motivation: 现有MoE设计在专家粒度和硬件执行效率之间存在权衡，OmniMoE旨在突破该瓶颈，推动专家粒度达到逻辑极限，实现更高参数效率。

Method: OmniMoE通过引入向量级原子专家以及系统与算法的协同设计，包括笛卡尔积路由器降低路由复杂度和专家中心调度优化内存访问，实现了大规模细粒度MoE的高效路由和执行。

Result: 在七个基准上，OmniMoE以17亿活跃参数达到了50.9%的零样本准确率，优于主流粗粒度和细粒度MoE模型，同时推理延迟从73ms降至6.7ms，提升了10.9倍速度。

Conclusion: OmniMoE实现了极细粒度的专家网络设计，在提升模型容量和准确率的同时显著降低了推理延迟，兼具高效性与准确性。

Abstract: Mixture-of-Experts (MoE) architectures are evolving towards finer granularity to improve parameter efficiency. However, existing MoE designs face an inherent trade-off between the granularity of expert specialization and hardware execution efficiency. We propose OmniMoE, a system-algorithm co-designed framework that pushes expert granularity to its logical extreme. OmniMoE introduces vector-level Atomic Experts, enabling scalable routing and execution within a single MoE layer, while retaining a shared dense MLP branch for general-purpose processing. Although this atomic design maximizes capacity, it poses severe challenges for routing complexity and memory access. To address these, OmniMoE adopts a system-algorithm co-design: (i) a Cartesian Product Router that decomposes the massive index space to reduce routing complexity from O(N) to O(sqrt(N)); and (ii) Expert-Centric Scheduling that inverts the execution order to turn scattered, memory-bound lookups into efficient dense matrix operations. Validated on seven benchmarks, OmniMoE (with 1.7B active parameters) achieves 50.9% zero-shot accuracy across seven benchmarks, outperforming coarse-grained (e.g., DeepSeekMoE) and fine-grained (e.g., PEER) baselines. Crucially, OmniMoE reduces inference latency from 73ms to 6.7ms (a 10.9-fold speedup) compared to PEER, demonstrating that massive-scale fine-grained MoE can be fast and accurate. Our code is open-sourced at https://github.com/flash-algo/omni-moe.

</details>


### [41] [CompactRAG: Reducing LLM Calls and Token Overhead in Multi-Hop Question Answering](https://arxiv.org/abs/2602.05728)
*Hao Yang,Zhiyu Yang,Xupeng Zhang,Wei Wei,Yunjie Zhang,Lin Yang*

Main category: cs.CL

TL;DR: 本文提出的CompactRAG通过将知识库构建离线完成，并在线使用少量LLM调用实现多跳问答推理，极大降低了计算开销并保持准确性，适合大规模知识库的成本效益问答任务。


<details>
  <summary>Details</summary>
Motivation: 现有多跳知识增强生成系统在每个推理步骤交替调用检索和推理，导致重复调用LLM、令牌消耗高且实体一致性差，需提升系统效率与稳定性。

Method: 采用离线阶段由LLM一次性读取语料并生成原子级问答对构成知识库，在线阶段进行复杂查询的分解和改写，通过密集检索和RoBERTa抽取答案，推理时只调用LLM两次，有效减少了重复调用。

Result: 在HotpotQA、2WikiMultiHopQA和MuSiQue数据集上，CompactRAG不仅在准确率上达到或超越基线，还显著减少了令牌消耗，验证了其高效且实用的多跳推理能力。

Conclusion: CompactRAG通过离线知识库构建和在线推理的解耦，显著提升了多跳复杂问答系统的效率，实现了较低的计算资源消耗同时保持竞争性准确率。

Abstract: Retrieval-augmented generation (RAG) has become a key paradigm for knowledge-intensive question answering. However, existing multi-hop RAG systems remain inefficient, as they alternate between retrieval and reasoning at each step, resulting in repeated LLM calls, high token consumption, and unstable entity grounding across hops. We propose CompactRAG, a simple yet effective framework that decouples offline corpus restructuring from online reasoning.
  In the offline stage, an LLM reads the corpus once and converts it into an atomic QA knowledge base, which represents knowledge as minimal, fine-grained question-answer pairs. In the online stage, complex queries are decomposed and carefully rewritten to preserve entity consistency, and are resolved through dense retrieval followed by RoBERTa-based answer extraction. Notably, during inference, the LLM is invoked only twice in total - once for sub-question decomposition and once for final answer synthesis - regardless of the number of reasoning hops.
  Experiments on HotpotQA, 2WikiMultiHopQA, and MuSiQue demonstrate that CompactRAG achieves competitive accuracy while substantially reducing token consumption compared to iterative RAG baselines, highlighting a cost-efficient and practical approach to multi-hop reasoning over large knowledge corpora. The implementation is available at GitHub.

</details>


### [42] [LongR: Unleashing Long-Context Reasoning via Reinforcement Learning with Dense Utility Rewards](https://arxiv.org/abs/2602.05758)
*Bowen Ping,Zijun Chen,Yiyao Yu,Tingfeng Hui,Junchi Yan,Baobao Chang*

Main category: cs.CL

TL;DR: 提出LongR框架，通过动态推理与文档查阅结合高效奖励机制，显著提升了大规模长上下文推理性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习在长上下文场景中推理能力的重要性以及现有方法依赖稀疏奖励信号限制性能提升。

Method: 提出LongR框架，结合动态“思考与阅读”机制和基于信息增益的上下文密度奖励，交织推理与文档查阅。

Result: LongR在LongBench v2上提升9%，在RULER和InfiniteBench上也有一致提升，并在多种强化学习算法中表现稳健。

Conclusion: LongR有效增强了长上下文中的推理性能和效率，提升了模型面对大规模上下文的能力和抗干扰能力。

Abstract: Reinforcement Learning has emerged as a key driver for LLM reasoning. This capability is equally pivotal in long-context scenarios--such as long-dialogue understanding and structured data analysis, where the challenge extends beyond consuming tokens to performing rigorous deduction. While existing efforts focus on data synthesis or architectural changes, recent work points out that relying solely on sparse, outcome-only rewards yields limited gains, as such coarse signals are often insufficient to effectively guide the complex long-context reasoning. To address this, we propose LongR, a unified framework that enhances long-context performance by integrating a dynamic "Think-and-Read" mechanism, which interleaves reasoning with document consultation, with a contextual density reward based on relative information gain to quantify the utility of the relevant documents. Empirically, LongR achieves a 9% gain on LongBench v2 and consistent improvements on RULER and InfiniteBench, demonstrating robust efficiency in navigating extensive contexts. Furthermore, LongR consistently enhances performance across diverse RL algorithms (e.g., DAPO, GSPO). Finally, we conduct in-depth analyses to investigate the impact of reasoning chain length on efficiency and the model's robustness against distractors.

</details>


### [43] [Different Time, Different Language: Revisiting the Bias Against Non-Native Speakers in GPT Detectors](https://arxiv.org/abs/2602.05769)
*Adnan Al Ali,Jindřich Helcl,Jindřich Libovický*

Main category: cs.CL

TL;DR: 本文重新审视了非母语文本被误判为AI生成文本的问题，发现捷克语非母语者文本并无较低困惑度，且现代文本检测器表现公正且有效。


<details>
  <summary>Details</summary>
Motivation: 回应此前研究中非母语者文本被误判为生成文本的问题，验证困惑度作为检测特征的真实性。

Method: 重新评估了捷克语背景下非母语者文本的困惑度，测试了三类检测器对非母语文本的偏见情况，分析检测器的核心特征。

Result: 发现非母语文本的困惑度并不低于母语文本，三类检测器无系统偏见，现代检测手段无需依赖困惑度。

Conclusion: 当前对非母语者文本的检测并不存在系统性偏见，且现代检测器不依赖困惑度有效识别生成文本。

Abstract: LLM-based assistants have been widely popularised after the release of ChatGPT. Concerns have been raised about their misuse in academia, given the difficulty of distinguishing between human-written and generated text. To combat this, automated techniques have been developed and shown to be effective, to some extent. However, prior work suggests that these methods often falsely flag essays from non-native speakers as generated, due to their low perplexity extracted from an LLM, which is supposedly a key feature of the detectors. We revisit these statements two years later, specifically in the Czech language setting. We show that the perplexity of texts from non-native speakers of Czech is not lower than that of native speakers. We further examine detectors from three separate families and find no systematic bias against non-native speakers. Finally, we demonstrate that contemporary detectors operate effectively without relying on perplexity.

</details>


### [44] [Reinforcement World Model Learning for LLM-based Agents](https://arxiv.org/abs/2602.05842)
*Xiao Yu,Baolin Peng,Ruize Xu,Yelong Shen,Pengcheng He,Suman Nath,Nikhil Singh,Jiangfeng Gao,Zhou Yu*

Main category: cs.CL

TL;DR: RWML利用自监督强化学习，通过对比模拟和真实环境状态，提升了LLM智能体的世界建模能力，实现了更好的环境适应和任务完成表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在语言任务表现强劲，但在智能体环境适应中难以预测行动后果与环境动态，缺乏有效的世界建模能力。

Method: 提出了基于自监督的强化世界模型学习（RWML）方法，通过对文本状态中的动作条件世界模型进行学习，利用模拟与真实状态差距奖励，增强了模型对环境动态的内在模拟一致性。

Result: 在ALFWorld和τ² Bench测试中，RWML显著优于基础模型，同时结合任务成功奖励后，超过了直接基于任务成功奖励的强化学习，表现与专家数据训练相当。

Conclusion: RWML 方法通过对比模拟和真实环境中的状态转移，显著提升了基于大语言模型的智能体在环境动态适应和行动后果预测方面的性能。

Abstract: Large language models (LLMs) have achieved strong performance in language-centric tasks. However, in agentic settings, LLMs often struggle to anticipate action consequences and adapt to environment dynamics, highlighting the need for world-modeling capabilities in LLM-based agents. We propose Reinforcement World Model Learning (RWML), a self-supervised method that learns action-conditioned world models for LLM-based agents on textual states using sim-to-real gap rewards. Our method aligns simulated next states produced by the model with realized next states observed from the environment, encouraging consistency between internal world simulations and actual environment dynamics in a pre-trained embedding space. Unlike next-state token prediction, which prioritizes token-level fidelity (i.e., reproducing exact wording) over semantic equivalence and can lead to model collapse, our method provides a more robust training signal and is empirically less susceptible to reward hacking than LLM-as-a-judge. We evaluate our method on ALFWorld and $τ^2$ Bench and observe significant gains over the base model, despite being entirely self-supervised. When combined with task-success rewards, our method outperforms direct task-success reward RL by 6.9 and 5.7 points on ALFWorld and $τ^2$ Bench respectively, while matching the performance of expert-data training.

</details>


### [45] [OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions](https://arxiv.org/abs/2602.05843)
*Fangzhi Xu,Hang Yan,Qiushi Sun,Jinyang Wu,Zixian Huang,Muye Huang,Jingyang Gong,Zichen Ding,Kanzhi Cheng,Yian Wang,Xinyu Che,Zeyi Sun,Jian Zhang,Zhangyue Yin,Haoran Luo,Xuanjing Huang,Ben Kao,Jun Liu,Qika Lin*

Main category: cs.CL

TL;DR: 本文提出OdysseyArena平台，专注长时序主动归纳交互评估，揭示当前大语言模型在自主发现复杂环境转移规律上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有模型评估局限于演绎范式，缺少对模型自主归纳潜在环境转移规律的测试，阻碍提升模型的战略前瞻性和长期规划能力。

Method: 提出OdysseyArena环境，通过四个基本操作构建长时序主动归纳交互场景，包含标准基准集OdysseyArena-Lite和极端长时序测试OdysseyArena-Challenge。

Result: 对15+主流大语言模型的实验显示，模型在归纳场景中存在显著缺陷，揭示了复杂环境自主发现的关键瓶颈。

Conclusion: 当前最先进的语言模型在长期、主动及归纳式交互中表现不足，限制了自主发现和复杂环境应对能力的发展。

Abstract: The rapid advancement of Large Language Models (LLMs) has catalyzed the development of autonomous agents capable of navigating complex environments. However, existing evaluations primarily adopt a deductive paradigm, where agents execute tasks based on explicitly provided rules and static goals, often within limited planning horizons. Crucially, this neglects the inductive necessity for agents to discover latent transition laws from experience autonomously, which is the cornerstone for enabling agentic foresight and sustaining strategic coherence. To bridge this gap, we introduce OdysseyArena, which re-centers agent evaluation on long-horizon, active, and inductive interactions. We formalize and instantiate four primitives, translating abstract transition dynamics into concrete interactive environments. Building upon this, we establish OdysseyArena-Lite for standardized benchmarking, providing a set of 120 tasks to measure an agent's inductive efficiency and long-horizon discovery. Pushing further, we introduce OdysseyArena-Challenge to stress-test agent stability across extreme interaction horizons (e.g., > 200 steps). Extensive experiments on 15+ leading LLMs reveal that even frontier models exhibit a deficiency in inductive scenarios, identifying a critical bottleneck in the pursuit of autonomous discovery in complex environments. Our code and data are available at https://github.com/xufangzhi/Odyssey-Arena

</details>


### [46] [RRAttention: Dynamic Block Sparse Attention via Per-Head Round-Robin Shifts for Long-Context Inference](https://arxiv.org/abs/2602.05853)
*Siran Liu,Guoxia Wang,Sa Wang,Jinle Zeng,HaoYang Xie,Siyu Lou,JiaBin Yang,DianHai Yu,Haifeng Wang,Chao Yang*

Main category: cs.CL

TL;DR: RRAttention提出了一种头部循环采样的动态稀疏注意力方法，显著降低计算复杂度且保持高性能，在超长上下文下实现2.4倍加速，优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 传统注意力机制的二次复杂度限制了大规模语言模型处理长上下文的能力，而现有动态稀疏注意力方法存在预处理需求、缺乏全局评估、破坏查询独立性或计算开销大的问题。

Method: 提出了一种头部循环采样（round-robin sampling）策略，通过在注意力头之间循环旋转查询采样位置，并在stride级别进行聚合，实现了动态稀疏注意力，降低了计算复杂度并采用自适应Top-τ选择实现最优稀疏性。

Result: 在自然语言理解和多模态视频理解任务中，RRAttention恢复了超过99%的全注意力性能，仅计算一半的注意力块，在128K上下文长度下实现了2.4倍加速，并优于现有方法。

Conclusion: RRAttention通过头部循环采样策略有效解决了动态稀疏注意力方法中的基本权衡，在保持查询独立性的同时实现全局模式发现，显著提升了长文本处理效率和性能。

Abstract: The quadratic complexity of attention mechanisms poses a critical bottleneck for large language models processing long contexts. While dynamic sparse attention methods offer input-adaptive efficiency, they face fundamental trade-offs: requiring preprocessing, lacking global evaluation, violating query independence, or incurring high computational overhead. We present RRAttention, a novel dynamic sparse attention method that simultaneously achieves all desirable properties through a head \underline{r}ound-\underline{r}obin (RR) sampling strategy. By rotating query sampling positions across attention heads within each stride, RRAttention maintains query independence while enabling efficient global pattern discovery with stride-level aggregation. Our method reduces complexity from $O(L^2)$ to $O(L^2/S^2)$ and employs adaptive Top-$τ$ selection for optimal sparsity. Extensive experiments on natural language understanding (HELMET) and multimodal video comprehension (Video-MME) demonstrate that RRAttention recovers over 99\% of full attention performance while computing only half of the attention blocks, achieving 2.4$\times$ speedup at 128K context length and outperforming existing dynamic sparse attention methods.

</details>


### [47] [xList-Hate: A Checklist-Based Framework for Interpretable and Generalizable Hate Speech Detection](https://arxiv.org/abs/2602.05874)
*Adrián Girón,Pablo Miralles,Javier Huertas-Tato,Sergio D'Antonio,David Camacho*

Main category: cs.CL

TL;DR: xList-Hate通过多维诊断问题和决策树融合提升仇恨言论检测的跨域鲁棒性和可解释性，提供内容审核的新思路。


<details>
  <summary>Details</summary>
Motivation: 仇恨言论定义复杂，受不同法律、平台政策及标注标准影响，传统监督模型易过拟合特定数据集，表现出较差的跨域和标注噪声鲁棒性。

Method: 提出xList-Hate诊断框架，通过大语言模型独立回答多个概念层面的问题，结合透明的决策树聚合诊断信号，实现仇恨言论的细粒度解释和判定。

Result: 该方法在多个基准数据集和模型上表现出优于零样本LLM和监督微调的跨数据集鲁棒性，并且对标注不一致和上下文模棱两可情况更不敏感，且输出透明、可审计。

Conclusion: 将仇恨言论检测重新构建为诊断推理任务，而非单一分类问题，能够提升模型的鲁棒性、解释性和跨域适应能力。

Abstract: Hate speech detection is commonly framed as a direct binary classification problem despite being a composite concept defined through multiple interacting factors that vary across legal frameworks, platform policies, and annotation guidelines. As a result, supervised models often overfit dataset-specific definitions and exhibit limited robustness under domain shift and annotation noise.
  We introduce xList-Hate, a diagnostic framework that decomposes hate speech detection into a checklist of explicit, concept-level questions grounded in widely shared normative criteria. Each question is independently answered by a large language model (LLM), producing a binary diagnostic representation that captures hateful content features without directly predicting the final label. These diagnostic signals are then aggregated by a lightweight, fully interpretable decision tree, yielding transparent and auditable predictions.
  We evaluate it across multiple hate speech benchmarks and model families, comparing it against zero-shot LLM classification and in-domain supervised fine-tuning. While supervised methods typically maximize in-domain performance, we consistently improves cross-dataset robustness and relative performance under domain shift. In addition, qualitative analysis of disagreement cases provides evidence that the framework can be less sensitive to certain forms of annotation inconsistency and contextual ambiguity. Crucially, the approach enables fine-grained interpretability through explicit decision paths and factor-level analysis.
  Our results suggest that reframing hate speech detection as a diagnostic reasoning task, rather than a monolithic classification problem, provides a robust, explainable, and extensible alternative for content moderation.

</details>


### [48] [EuroLLM-22B: Technical Report](https://arxiv.org/abs/2602.05879)
*Miguel Moura Ramos,Duarte M. Alves,Hippolyte Gisserot-Boukhlef,João Alves,Pedro Henrique Martins,Patrick Fernandes,José Pombal,Nuno M. Guerreiro,Ricardo Rei,Nicolas Boizard,Amin Farajian,Mateusz Klimaszewski,José G. C. de Souza,Barry Haddow,François Yvon,Pierre Colombo,Alexandra Birch,André F. T. Martins*

Main category: cs.CL

TL;DR: EuroLLM-22B是针对欧洲多语言需求专门训练的大型语言模型，提供强劲多语言能力并开源相关资源。


<details>
  <summary>Details</summary>
Motivation: 解决现存开放大型语言模型对欧洲多语言支持不足、服务不充分的问题。

Method: 从零开始训练22B参数规模的大型语言模型，设计专用分词器，制定架构规格，进行数据过滤与训练，测试多语言基准。

Result: EuroLLM-22B在多语言推理、指令执行与翻译任务中表现良好，成绩达到同等规模模型的竞争水平。

Conclusion: EuroLLM-22B在覆盖所有欧盟官方语言及额外11种语言方面表现出色，适用于欧洲民众的多语言需求，性能与同规模模型竞争力强。

Abstract: This report presents EuroLLM-22B, a large language model trained from scratch to support the needs of European citizens by covering all 24 official European Union languages and 11 additional languages. EuroLLM addresses the issue of European languages being underrepresented and underserved in existing open large language models. We provide a comprehensive overview of EuroLLM-22B's development, including tokenizer design, architectural specifications, data filtering, and training procedures. Across a broad set of multilingual benchmarks, EuroLLM-22B demonstrates strong performance in reasoning, instruction following, and translation, achieving results competitive with models of comparable size. To support future research, we release our base and instruction-tuned models, our multilingual web pretraining data and updated EuroBlocks instruction datasets, as well as our pre-training and evaluation codebases.

</details>


### [49] [Stop Rewarding Hallucinated Steps: Faithfulness-Aware Step-Level Reinforcement Learning for Small Reasoning Models](https://arxiv.org/abs/2602.05897)
*Shuo Nie,Hexuan Deng,Chao Wang,Ruiyu Fang,Xuebo Liu,Shuangyong Song,Yu Li,Min Zhang,Xuelong Li*

Main category: cs.CL

TL;DR: 针对小推理模型中链式思维推理的幻觉问题，本文提出FaithRL方法，通过步骤级真实奖励和对比学习，有效增强推理的真实性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 小推理模型在资源受限环境下执行链式思维推理时容易出现真实性幻觉，现有基于结果奖励或粗粒度评估的方法难以有效抑制不真实的推理。

Method: 提出了Faithfulness-Aware Step-Level Reinforcement Learning（FaithRL），利用过程奖励模型提供的显式步骤级真实性奖励和隐式截断重采样策略，生成对比信号以监督中间推理步骤。

Result: 在多个小推理模型和开放书籍问答基准测试中，FaithRL显著减少了链式思维和最终答案中的幻觉，提升了推理的真实性和可靠性。

Conclusion: FaithRL方法通过引入步骤级别的真实性奖励和对比信号，有效减少了小推理模型在链式思维中的幻觉现象，提高了推理的真实性和可靠性。

Abstract: As large language models become smaller and more efficient, small reasoning models (SRMs) are crucial for enabling chain-of-thought (CoT) reasoning in resource-constrained settings. However, they are prone to faithfulness hallucinations, especially in intermediate reasoning steps. Existing mitigation methods based on online reinforcement learning rely on outcome-based rewards or coarse-grained CoT evaluation, which can inadvertently reinforce unfaithful reasoning when the final answer is correct. To address these limitations, we propose Faithfulness-Aware Step-Level Reinforcement Learning (FaithRL), introducing step-level supervision via explicit faithfulness rewards from a process reward model, together with an implicit truncated resampling strategy that generates contrastive signals from faithful prefixes. Experiments across multiple SRMs and Open-Book QA benchmarks demonstrate that FaithRL consistently reduces hallucinations in both the CoT and final answers, leading to more faithful and reliable reasoning. Code is available at https://github.com/Easy195/FaithRL.

</details>


### [50] [Codified Finite-state Machines for Role-playing](https://arxiv.org/abs/2602.05905)
*Letian Peng,Yupeng Hou,Kun Zhou,Jingbo Shang*

Main category: cs.CL

TL;DR: 本论文通过引入基于大语言模型编码的有限状态机方法，有效捕捉潜在角色状态，提升角色扮演的一致性和体验。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的方法主要捕捉表面动作，难以追踪驱动交互的潜在角色状态，传统手工设计的有限状态机难适应角色扮演的开放语义空间。

Method: 提出了Codified Finite-State Machines(CFSMs)用LLM自动编码文本角色简介生成FSM结构，并扩展为概率FSM(CPFSM)以处理状态转换中的不确定性和多样性。

Result: 在合成评测和真实角色扮演场景中，CFSM和CPFSM框架表现优异，能在结构化任务和开放式随机状态探索中提升角色状态建模效果。

Conclusion: CFSM和CPFSM框架在角色扮演中能有效捕捉潜在角色状态，提升角色一致性和互动质量，优于现有基线方法。

Abstract: Modeling latent character states is crucial for consistent and engaging role-playing (RP) with large language models (LLMs). Yet, existing prompting-based approaches mainly capture surface actions, often failing to track the latent states that drive interaction. We revisit finite-state machines (FSMs), long used in game design to model state transitions. While effective in small, well-specified state spaces, traditional hand-crafted, rule-based FSMs struggle to adapt to the open-ended semantic space of RP. To address this, we introduce Codified Finite-State Machines (CFSMs), a framework that automatically codifies textual character profiles into FSMs using LLM-based coding. CFSMs extract key states and transitions directly from the profile, producing interpretable structures that enforce character consistency. To further capture uncertainty and variability, we extend CFSMs into Codified Probabilistic Finite-State Machines (CPFSMs), where transitions are modeled as probability distributions over states. Through both synthetic evaluations and real-world RP scenarios in established artifacts, we demonstrate that CFSM and CPFSM outperform generally applied baselines, verifying effectiveness not only in structured tasks but also in open-ended stochastic state exploration.

</details>


### [51] [KV-CoRE: Benchmarking Data-Dependent Low-Rank Compressibility of KV-Caches in LLMs](https://arxiv.org/abs/2602.05929)
*Jian Chen,Zhuoran Wang,Jiayu Qin,Ming Li,Meng Wang,Changyou Chen,Yin Chen,Qizhen Weng,Yirui Liu*

Main category: cs.CL

TL;DR: 本文提出一种SVD基础的kv-cache压缩性评估方法KV-CoRE，构建了首个大规模kv-cache压缩性基准，揭示了压缩性与模型及语言特性间的关系，为动态数据感知压缩提供理论支持。


<details>
  <summary>Details</summary>
Motivation: 随着上下文长度的增长，kv-cache的读写操作快速占用GPU内存带宽，现有压缩方法未充分考虑kv-cache数据依赖性及各层间的差异，因而需要一种高效且数据敏感的评估方法。

Method: KV-CoRE利用奇异值分解（SVD）计算kv-cache的最优低秩近似，采用无梯度且增量式的方法实现数据集层级的压缩性评估，使用归一化有效秩作为压缩性指标。

Result: 通过在多模型、多数据集、五个英语领域和十六种语言上的分析，发现kv-cache压缩性与模型设计和训练数据有系统性关联，同时归一化有效秩能较好地预测压缩带来的性能下降。

Conclusion: 本文提出了KV-CoRE，一种基于SVD的KV-cache低秩可压缩性量化方法，实现在大规模多模型多语种环境下的高效压缩评估。该方法揭示了压缩性与模型架构、训练数据及语言覆盖的系统性关联，并且提供了性能下降与压缩性之间的定量关系。

Abstract: Large language models rely on kv-caches to avoid redundant computation during autoregressive decoding, but as context length grows, reading and writing the cache can quickly saturate GPU memory bandwidth. Recent work has explored KV-cache compression, yet most approaches neglect the data-dependent nature of kv-caches and their variation across layers. We introduce KV-CoRE KV-cache Compressibility by Rank Evaluation), an SVD-based method for quantifying the data-dependent low-rank compressibility of kv-caches. KV-CoRE computes the optimal low-rank approximation under the Frobenius norm and, being gradient-free and incremental, enables efficient dataset-level, layer-wise evaluation. Using this method, we analyze multiple models and datasets spanning five English domains and sixteen languages, uncovering systematic patterns that link compressibility to model architecture, training data, and language coverage. As part of this analysis, we employ the Normalized Effective Rank as a metric of compressibility and show that it correlates strongly with performance degradation under compression. Our study establishes a principled evaluation framework and the first large-scale benchmark of kv-cache compressibility in LLMs, offering insights for dynamic, data-aware compression and data-centric model development.

</details>


### [52] [Polyglots or Multitudes? Multilingual LLM Answers to Value-laden Multiple-Choice Questions](https://arxiv.org/abs/2602.05932)
*Léo Labat,Etienne Ollion,François Yvon*

Main category: cs.CL

TL;DR: 该研究首次通过人工翻译的多语言价值观调查数据，系统评估多语言大型语言模型在价值观相关多选题中的语言依赖性，发现模型表现虽受规模和微调影响，但语言差异仍导致回答不一致。


<details>
  <summary>Details</summary>
Motivation: 探究多语言大型语言模型在包含价值观内容的多选题（MCQs）中是否能在不同语言中保持回答一致性，解决现有研究主要关注事实回忆而忽略语言对价值观表达影响的不足。

Method: 构建并发布了多语言欧洲价值观调查（MEVS）数据集，涵盖8种欧洲语言，所有问题均为人工翻译，避免机器翻译带来的偏差；随后在30多个不同规模、制造商和微调状态的多语言LLM上，使用多样化的提示策略（包括答案顺序、符号类型和尾部字符）进行测试。

Result: 指令微调和规模较大的模型整体上回答更具一致性，但特定问题的鲁棒性差异显著；语言特异行为普遍存在于所有指令微调的多语言模型中，但仅在特定问题上表现明显，显示了偏好微调的选择性影响。

Conclusion: 多语言大型语言模型在价值观相关的多选题回答中表现出语言特异性，即使在经过指令微调和多语言训练后，模型在不同语言下的回答一致性存在差异，部分问题能达到跨模型和语言的完全一致，而另一些问题则表现出分歧。

Abstract: Multiple-Choice Questions (MCQs) are often used to assess knowledge, reasoning abilities, and even values encoded in large language models (LLMs). While the effect of multilingualism has been studied on LLM factual recall, this paper seeks to investigate the less explored question of language-induced variation in value-laden MCQ responses. Are multilingual LLMs consistent in their responses across languages, i.e. behave like theoretical polyglots, or do they answer value-laden MCQs depending on the language of the question, like a multitude of monolingual models expressing different values through a single model? We release a new corpus, the Multilingual European Value Survey (MEVS), which, unlike prior work relying on machine translation or ad hoc prompts, solely comprises human-translated survey questions aligned in 8 European languages. We administer a subset of those questions to over thirty multilingual LLMs of various sizes, manufacturers and alignment-fine-tuning status under comprehensive, controlled prompt variations including answer order, symbol type, and tail character. Our results show that while larger, instruction-tuned models display higher overall consistency, the robustness of their responses varies greatly across questions, with certain MCQs eliciting total agreement within and across models while others leave LLM answers split. Language-specific behavior seems to arise in all consistent, instruction-fine-tuned models, but only on certain questions, warranting a further study of the selective effect of preference fine-tuning.

</details>


### [53] [Self-Improving Multilingual Long Reasoning via Translation-Reasoning Integrated Training](https://arxiv.org/abs/2602.05940)
*Junxiao Liu,Zhijun Wang,Yixiao Li,Zhejian Lai,Liqian Huang,Xin Huang,Xue Han,Junlan Feng,Shujian Huang*

Main category: cs.CL

TL;DR: 提出了一种将翻译训练与多语言推理联合训练的框架TRIT，显著提升了多语言长推理模型的准确率和语言一致性。


<details>
  <summary>Details</summary>
Motivation: 长推理模型在多语言场景中存在理解和推理能力不足的问题，导致准确率下降。

Method: 提出了TRIT自我改进框架，将翻译训练与多语言推理训练相结合，无需额外的数据和反馈。

Result: 在MMATH数据集上，TRIT相比多种基线方法提升了平均7个百分点的表现，跨语言问题对齐提高了10个百分点以上，翻译质量提升达8.4个COMET点。

Conclusion: TRIT方法有效提升了多语言环境下长推理模型的表现，显著提高了答案正确率和语言一致性。

Abstract: Long reasoning models often struggle in multilingual settings: they tend to reason in English for non-English questions; when constrained to reasoning in the question language, accuracies drop substantially. The struggle is caused by the limited abilities for both multilingual question understanding and multilingual reasoning. To address both problems, we propose TRIT (Translation-Reasoning Integrated Training), a self-improving framework that integrates the training of translation into multilingual reasoning. Without external feedback or additional multilingual data, our method jointly enhances multilingual question understanding and response generation. On MMATH, our method outperforms multiple baselines by an average of 7 percentage points, improving both answer correctness and language consistency. Further analysis reveals that integrating translation training improves cross-lingual question alignment by over 10 percentage points and enhances translation quality for both mathematical questions and general-domain text, with gains up to 8.4 COMET points on FLORES-200.

</details>


### [54] [Characterizing Human Semantic Navigation in Concept Production as Trajectories in Embedding Space](https://arxiv.org/abs/2602.05971)
*Felipe D. Toro-Hernández,Jesuino Vieira Filho,Rodrigo M. Cabral-Carvalho*

Main category: cs.CL

TL;DR: 该论文提出通过模拟语义导航轨迹，利用文本嵌入模型和几何动力学指标，实现在多语言临床及概念生成任务上的语义表征动态量化，简化传统语言分析。


<details>
  <summary>Details</summary>
Motivation: 旨在探索人类如何在结构化、动态的知识空间中导航检索和操纵语义，提供比传统语言预处理更自动化、数学化的语义表征分析方法。

Method: 使用不同的Transformer文本嵌入模型，构建基于累积嵌入的参与者特定语义轨迹，提取距离、熵、速度、加速度等几何和动力学指标，评估四个不同语言和任务的数据集。

Result: 该方法成功区分临床和非临床群体以及不同概念类型，发现累积嵌入在长轨迹中表现更好，不同嵌入模型结果相似，表明不同训练方法得到的表征具有一致性。

Conclusion: 该研究提出了一种将语义表示视为嵌入空间中的轨迹导航的新框架，能够通过几何和动力学指标量化人类语义搜索过程，有效区分临床群体和概念类型，并跨语言和任务展现鲁棒性。

Abstract: Semantic representations can be framed as a structured, dynamic knowledge space through which humans navigate to retrieve and manipulate meaning. To investigate how humans traverse this geometry, we introduce a framework that represents concept production as navigation through embedding space. Using different transformer text embedding models, we construct participant-specific semantic trajectories based on cumulative embeddings and extract geometric and dynamical metrics, including distance to next, distance to centroid, entropy, velocity, and acceleration. These measures capture both scalar and directional aspects of semantic navigation, providing a computationally grounded view of semantic representation search as movement in a geometric space. We evaluate the framework on four datasets across different languages, spanning different property generation tasks: Neurodegenerative, Swear verbal fluency, Property listing task in Italian, and in German. Across these contexts, our approach distinguishes between clinical groups and concept types, offering a mathematical framework that requires minimal human intervention compared to typical labor-intensive linguistic pre-processing methods. Comparison with a non-cumulative approach reveals that cumulative embeddings work best for longer trajectories, whereas shorter ones may provide too little context, favoring the non-cumulative alternative. Critically, different embedding models yielded similar results, highlighting similarities between different learned representations despite different training pipelines. By framing semantic navigation as a structured trajectory through embedding space, bridging cognitive modeling with learned representation, thereby establishing a pipeline for quantifying semantic representation dynamics with applications in clinical research, cross-linguistic analysis, and the assessment of artificial cognition.

</details>


### [55] [DSB: Dynamic Sliding Block Scheduling for Diffusion LLMs](https://arxiv.org/abs/2602.05992)
*Lizhuo Luo,Shenggui Li,Yonggang Wen,Tianwei Zhang*

Main category: cs.CL

TL;DR: 本文针对扩散大语言模型提出动态滑动块调度和缓存机制，提升了文本生成的质量和效率。


<details>
  <summary>Details</summary>
Motivation: 传统的固定预定义区块调度未考虑语义难度，导致在生成过程中对不确定位置过早确定，或推迟容易位置生成，影响生成质量和效率。

Method: 提出了一种训练自由的动态滑动块（DSB）区块调度方法，根据语义难度动态调整区块大小，并设计了针对DSB的训练自由KV缓存机制（DSB Cache）。

Result: 在多个模型和基准测试中，DSB与DSB Cache相结合后，持续提升了文本生成质量和推理效率。

Conclusion: 动态滑动块（DSB）调度方法和DSB缓存机制显著提升了扩散大语言模型（dLLMs）的生成质量和推理效率，克服了传统固定区块调度的不足。

Abstract: Diffusion large language models (dLLMs) have emerged as a promising alternative for text generation, distinguished by their native support for parallel decoding. In practice, block inference is crucial for avoiding order misalignment in global bidirectional decoding and improving output quality. However, the widely-used fixed, predefined block (naive) schedule is agnostic to semantic difficulty, making it a suboptimal strategy for both quality and efficiency: it can force premature commitments to uncertain positions while delaying easy positions near block boundaries. In this work, we analyze the limitations of naive block scheduling and disclose the importance of dynamically adapting the schedule to semantic difficulty for reliable and efficient inference. Motivated by this, we propose Dynamic Sliding Block (DSB), a training-free block scheduling method that uses a sliding block with a dynamic size to overcome the rigidity of the naive block. To further improve efficiency, we introduce DSB Cache, a training-free KV-cache mechanism tailored to DSB. Extensive experiments across multiple models and benchmarks demonstrate that DSB, together with DSB Cache, consistently improves both generation quality and inference efficiency for dLLMs. Code is released at https://github.com/lizhuo-luo/DSB.

</details>


### [56] [A Systematic Evaluation of Large Language Models for PTSD Severity Estimation: The Role of Contextual Knowledge and Modeling Strategies](https://arxiv.org/abs/2602.06015)
*Panagiotis Kaliosis,Adithya V Ganesan,Oscar N. E. Kjell,Whitney Ringwald,Scott Feltman,Melissa A. Carr,Dimitris Samaras,Camilo Ruggero,Benjamin J. Luft,Roman Kotov,Andrew H. Schwartz*

Main category: cs.CL

TL;DR: 本研究利用临床数据评估11种大型语言模型在心理健康评估中的表现，发现提供详细上下文和合理建模策略能显著提升准确率，且模型参数规模与集成策略对性能影响显著。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在零样本模式下日益用于评估心理健康，但影响其准确性的因素尚不清楚。

Method: 利用包含1,437名个体的临床自然语言叙述和PTSD自我评分数据，评估11种先进大型语言模型的表现，系统变换上下文知识和建模策略来分析影响准确率的因素。

Result: （a）提供详细构念定义和叙述上下文能显著提升模型准确度；（b）增加推理努力提升估计准确率；（c）开放权重模型在70B参数后性能趋于平稳，闭权重模型随代数提升；（d）监督模型与零样本模型结合的集成方式表现最佳。

Conclusion: 选择合适的上下文知识和建模策略对于准确利用大型语言模型评估心理健康状况至关重要。

Abstract: Large language models (LLMs) are increasingly being used in a zero-shot fashion to assess mental health conditions, yet we have limited knowledge on what factors affect their accuracy. In this study, we utilize a clinical dataset of natural language narratives and self-reported PTSD severity scores from 1,437 individuals to comprehensively evaluate the performance of 11 state-of-the-art LLMs. To understand the factors affecting accuracy, we systematically varied (i) contextual knowledge like subscale definitions, distribution summary, and interview questions, and (ii) modeling strategies including zero-shot vs few shot, amount of reasoning effort, model sizes, structured subscales vs direct scalar prediction, output rescaling and nine ensemble methods. Our findings indicate that (a) LLMs are most accurate when provided with detailed construct definitions and context of the narrative; (b) increased reasoning effort leads to better estimation accuracy; (c) performance of open-weight models (Llama, Deepseek), plateau beyond 70B parameters while closed-weight (o3-mini, gpt-5) models improve with newer generations; and (d) best performance is achieved when ensembling a supervised model with the zero-shot LLMs. Taken together, the results suggest choice of contextual knowledge and modeling strategies is important for deploying LLMs to accurately assess mental health.

</details>


### [57] [Multi-Token Prediction via Self-Distillation](https://arxiv.org/abs/2602.06019)
*John Kirchenbauer,Abhimanyu Hans,Brian Bartoldson,Micah Goldblum,Ashwinee Panda,Tom Goldstein*

Main category: cs.CL

TL;DR: 本文提出一种简单且无需额外模型的在线蒸馏方法，大幅加速预训练语言模型推理，效果显著且易于部署。


<details>
  <summary>Details</summary>
Motivation: 现有加速语言模型推理的方法如推测解码复杂且依赖辅助模型及复杂管线，亟需简化且高效的推理加速方案。

Method: 通过在线蒸馏目标，将预训练的单标记预测模型转换为可进行多标记预测的模型，保持模型结构不变，无需额外验证器或专门推理代码。

Result: 基于GSM8K数据集，所提方法使模型推理速度提升超过3倍，且在准确率上仅有不到5%的下降。

Conclusion: 本研究提出了一种基于在线蒸馏的简单方法，将预训练自回归语言模型转化为高效的多标记预测模型，无需复杂推理管线和辅助模型，显著提升推理速度。

Abstract: Existing techniques for accelerating language model inference, such as speculative decoding, require training auxiliary speculator models and building and deploying complex inference pipelines. We consider a new approach for converting a pretrained autoregressive language model from a slow single next token prediction model into a fast standalone multi-token prediction model using a simple online distillation objective. The final model retains the exact same implementation as the pretrained initial checkpoint and is deployable without the addition of any auxiliary verifier or other specialized inference code. On GSM8K, our method produces models that can decode more than $3\times$ faster on average at $<5\%$ drop in accuracy relative to single token decoding performance.

</details>


### [58] [Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory](https://arxiv.org/abs/2602.06025)
*Haozhen Zhang,Haodong Yue,Tao Feng,Quanyu Long,Jianzhu Bao,Bowen Jin,Weizhi Zhang,Xiao Li,Jiaxuan You,Chengwei Qin,Wenya Wang*

Main category: cs.CL

TL;DR: BudgetMem提出一种运行时、查询感知的多层次记忆管理框架，通过强化学习控制性能成本权衡，提升大型语言模型代理的记忆利用效率和效果。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型代理的记忆大多离线构建且忽视查询特异性，效率低下且可能丢失关键信息；而运行时记忆利用虽然自然，但通常开销大且难以权衡性能与成本。

Method: 提出BudgetMem，一个包含低、中、高三种预算层次的记忆模块体系，结合轻量级路由器通过强化学习训练的紧凑神经策略，实现性能与成本的平衡。研究了实现复杂度、推理行为和模块容量三种预算层面策略。

Result: 预算层次设计使BudgetMem在高预算条件下性能超越强基线，在低预算下展现更佳准确度-成本权衡，分析揭示了不同策略在不同预算 regimes下的适用性和优势。

Conclusion: BudgetMem框架通过显式和查询感知的性能与成本控制，实现了运行时记忆的高效利用，优于现有基线方法，并在不同预算条件下展示了更优的准确度-成本权衡。

Abstract: Memory is increasingly central to Large Language Model (LLM) agents operating beyond a single context window, yet most existing systems rely on offline, query-agnostic memory construction that can be inefficient and may discard query-critical information. Although runtime memory utilization is a natural alternative, prior work often incurs substantial overhead and offers limited explicit control over the performance-cost trade-off. In this work, we present \textbf{BudgetMem}, a runtime agent memory framework for explicit, query-aware performance-cost control. BudgetMem structures memory processing as a set of memory modules, each offered in three budget tiers (i.e., \textsc{Low}/\textsc{Mid}/\textsc{High}). A lightweight router performs budget-tier routing across modules to balance task performance and memory construction cost, which is implemented as a compact neural policy trained with reinforcement learning. Using BudgetMem as a unified testbed, we study three complementary strategies for realizing budget tiers: implementation (method complexity), reasoning (inference behavior), and capacity (module model size). Across LoCoMo, LongMemEval, and HotpotQA, BudgetMem surpasses strong baselines when performance is prioritized (i.e., high-budget setting), and delivers better accuracy-cost frontiers under tighter budgets. Moreover, our analysis disentangles the strengths and weaknesses of different tiering strategies, clarifying when each axis delivers the most favorable trade-offs under varying budget regimes.

</details>


### [59] [DFlash: Block Diffusion for Flash Speculative Decoding](https://arxiv.org/abs/2602.06036)
*Jian Chen,Yesheng Liang,Zhijian Liu*

Main category: cs.CL

TL;DR: 针对自回归大语言模型推理速度瓶颈，DFlash利用块扩散模型实现并行草稿生成，显著加速推理且保持质量领先于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统自回归大语言模型推理存在序列解码瓶颈，推理延迟高且GPU利用率低，现有的推测解码方法仍依赖序列生成，难以获得理想的加速效果。

Method: 提出了DFlash推理框架，通过单次前向传播生成草稿标记，并利用目标模型提取的上下文特征对草稿模型进行条件约束，实现高效且高质量的并行草稿生成。

Result: 在多种模型和任务中，DFlash实现了超过6倍的无损加速，速度提升最高达2.5倍，显著优于目前最先进的推测解码方法EAGLE-3。

Conclusion: DFlash利用轻量级块扩散模型实现平行草稿生成，显著提升了大语言模型的推理速度和GPU利用率，且保持了输出质量不变。

Abstract: Autoregressive large language models (LLMs) deliver strong performance but require inherently sequential decoding, leading to high inference latency and poor GPU utilization. Speculative decoding mitigates this bottleneck by using a fast draft model whose outputs are verified in parallel by the target LLM; however, existing methods still rely on autoregressive drafting, which remains sequential and limits practical speedups. Diffusion LLMs offer a promising alternative by enabling parallel generation, but current diffusion models typically underperform compared with autoregressive models. In this paper, we introduce DFlash, a speculative decoding framework that employs a lightweight block diffusion model for parallel drafting. By generating draft tokens in a single forward pass and conditioning the draft model on context features extracted from the target model, DFlash enables efficient drafting with high-quality outputs and higher acceptance rates. Experiments show that DFlash achieves over 6x lossless acceleration across a range of models and tasks, delivering up to 2.5x higher speedup than the state-of-the-art speculative decoding method EAGLE-3.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [60] [Reducing the Costs of Proof Synthesis on Rust Systems by Scaling Up a Seed Training Set](https://arxiv.org/abs/2602.04910)
*Nongyu Di,Tianyu Chen,Shan Lu,Shuai Lu,Yeyun Gong,Peng Cheng,Jacob R. Lorch,Yuan Yao,Xiaoxing Ma*

Main category: cs.SE

TL;DR: 该论文通过VeruSyn流水线大规模合成形式验证Rust程序数据，提升了代码证明生成能力，训练模型效果优于现有顶尖商业和研究模型。


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成代码正确性存在疑虑，尤其是代码证明生成比代码生成更具挑战且缺乏训练数据，需要大规模高质量的形式验证程序数据支持。

Method: 设计VeruSyn数据合成流水线，结合自我合成、基于教程的合成和长链思路（CoT）数据补充，生成包含形式证明的Rust程序数据集，并用以微调大语言模型。

Result: 合成了最大规模的约690万份带有规范和证明的Rust程序数据集，训练出的Qwen2.5-Coder-32B-Instruct模型在成本与证明质量方面优于现有商业和研究模型。

Conclusion: 通过VeruSyn数据合成流水线，能大规模生成带有形式规范和证明的Rust程序，显著提升了代码证明生成的规模和质量，推动代码生成的正确性保障。

Abstract: Large Language Models (LLMs) are widely used for code generation. However, the correctness of code generated by LLMs remains a concern. A potential remedy to this concern is to have LLMs generate formal correctness proofs along with such code. However, compared with code generation, code-proof generation requires much higher reasoning capability and has much less existing data to learn from. In this paper, we present VeruSyn, a data synthesis pipeline for Verus, a state-of-the-art verification tool for system software written in Rust. Through self-synthesis and tutorial-based synthesis, VeruSyn achieves much larger scale and Verus-feature coverage than previous data-synthesis techniques designed for Verus; VeruSyn also supplements its dataset with long-chain-of-thought (CoT) data through agent trajectory synthesis. With VeruSyn, we synthesize the largest set of Verus verified programs: 6.9 million Rust programs, each with a formal specification and a proof that it meets that specification. This dataset lets us create a fine-tuned Qwen2.5-Coder-32B-Instruct model with appealing cost-proof tradeoff compared with state-of-the-art commercial models like Claude Sonnet 4.5. It also significantly outperforms models like o4-mini and previously proposed research models.

</details>


### [61] [ASA: Activation Steering for Tool-Calling Domain Adaptation](https://arxiv.org/abs/2602.04935)
*Youjin Wang,Run Zhou,Rong Fu,Shuaishuai Cao,Hongwei Zeng,Jiaxuan Lu,Sicheng Fan,Jiaqiao Zhao,Liangming Pan*

Main category: cs.SE

TL;DR: ASA是一种无训练、轻量级的适应机制，通过解读中间激活实现高效域适应，适合频繁变化接口的多域工具环境。


<details>
  <summary>Details</summary>
Motivation: 现实环境中通用大语言模型代理面临快速演变的工具集、API和协议，导致传统方法如重复LoRA/SFT训练成本指数增长，提示或模式方法在分布变化和复杂接口下脆弱。

Method: 提出了Activation Steering Adapter（ASA），一种轻量级、推理时无训练机制，通过读取中间激活的路由信号，并使用超轻量路由器生成自适应控制强度，实现精确的域对齐。

Result: ASA在多种模型规模和领域实现了类似LoRA的适应性能，显著降低了开销，且具备强跨模型迁移性。

Conclusion: ASA方法在多个模型规模和领域中实现了与LoRA相当的适应性，具有更低的开销和良好的跨模型迁移能力，适合用于多域工具生态系统中频繁接口变化的场景。

Abstract: For real-world deployment of general-purpose LLM agents, the core challenge is often not tool use itself, but efficient domain adaptation under rapidly evolving toolsets, APIs, and protocols. Repeated LoRA or SFT across domains incurs exponentially growing training and maintenance costs, while prompt or schema methods are brittle under distribution shift and complex interfaces. We propose \textbf{Activation Steering Adapter (ASA}), a lightweight, inference-time, training-free mechanism that reads routing signals from intermediate activations and uses an ultra-light router to produce adaptive control strengths for precise domain alignment. Across multiple model scales and domains, ASA achieves LoRA-comparable adaptation with substantially lower overhead and strong cross-model transferability, making it ideally practical for robust, scalable, and efficient multi-domain tool ecosystems with frequent interface churn dynamics.

</details>


### [62] [Large Language Models in Software Documentation and Modeling: A Literature Review and Findings](https://arxiv.org/abs/2602.04938)
*Lukas Radosky,Ivan Polasek*

Main category: cs.SE

TL;DR: 本文综述了大语言模型在软件工程中文档与建模任务的应用，分析了相关技术与评估方法，总结现状与发展方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生成式人工智能中表现突出，具备理解和生成自然语言的能力，适合处理软件文档和结构化语言，推动软件工程任务的发展。

Method: 通过对四个主要会议论文的文献综述，分析其在软件工程中与文档与建模相关任务的应用，整理任务分类，探讨提示技术、评估方法及数据集。

Result: 系统梳理了已有文献中大语言模型在软件文档和建模任务的应用现状，明确了常用的方法、评估手段和数据资源，为后续研究提供参考。

Conclusion: 大语言模型在软件文档和建模领域展现出广泛的应用潜力，已有研究为优化相关任务提供了多样的技术和评估方案，未来可进一步深化。

Abstract: Generative artificial intelligence attracts significant attention, especially with the introduction of large language models. Its capabilities are being exploited to solve various software engineering tasks. Thanks to their ability to understand natural language and generate natural language responses, large language models are great for processing various software documentation artifacts. At the same time, large language models excel at understanding structured languages, having the potential for working with software programs and models. We conduct a literature review on the usage of large language models for software engineering tasks related to documentation and modeling. We analyze articles from four major venues in the area, organize them per tasks they solve, and provide an overview of used prompt techniques, metrics, approaches to human-based evaluation, and major datasets.

</details>


### [63] [Applying a Requirements-Focused Agile Management Approach for Machine Learning-Enabled Systems](https://arxiv.org/abs/2602.05042)
*Lucas Romao,Luiz Xavier,Júlia Condé Araújo,Marina Condé Araújo,Ariane Rodrigues,Marcos Kalinowski*

Main category: cs.SE

TL;DR: RefineML方法专为机器学习系统需求设计，实证研究表明其提升了协作和项目管理效能，但在需求细化与工作量估算方面仍需优化。


<details>
  <summary>Details</summary>
Motivation: 传统的需求工程和敏捷管理难以满足机器学习系统依赖数据、需反复实验及模型行为不确定性的特点，现有方法整合不够且适配不足，因此需开发专门方法改进。

Method: 通过在PUC-Rio与巴西网络安全公司EXA的产学研合作项目中应用RefineML，结合问卷调查和半结构化访谈收集数据，并采用主题分析对定性资料进行分析。

Result: 问卷调查显示RefineML具有高适用性和被接受度，访谈反馈其促进了沟通、早期可行性评估和双轨治理机器学习与软件开发流程，但仍存在难以操作化机器学习需求和估算工作量的局限。

Conclusion: RefineML是一种针对机器学习系统特点设计的需求工程和敏捷管理方法，能够有效提升沟通、早期可行性评估及双轨治理，但在将机器学习问题具体化为敏捷需求及估算工作量方面仍存在困难。

Abstract: Machine Learning (ML)-enabled systems challenge traditional Requirements Engineering (RE) and agile management due to data dependence, experimentation, and uncertain model behavior. Existing RE and agile practices remain poorly integrated and insufficiently tailored to these characteristics. This paper reports on the practical experience of applying RefineML, a requirements-focused approach for the continuous and agile refinement of ML-enabled systems, which integrates ML-tailored specification and agile management approaches with best practices derived from a systematic mapping study. The application context concerns an industry-academia collaboration project between PUC-Rio and EXA, a Brazilian cybersecurity company. For evaluation purposes, we applied questionnaires assessing RefineML's suitability and overall acceptance and semi-structured interviews. We applied thematic analysis to the collected qualitative data. Regarding suitability and acceptance, the results of the questionnaires indicated high perceived usefulness and intention to use. Based on the interviews, stakeholders perceived RefineML as improving communication and facilitating early feasibility assessments, as well as enabling dual-track governance of ML and software work, allowing continuous refinement of the model while evolving the overall software project. However, some limitations remain, particularly related to difficulties in operationalizing ML concerns into agile requirements and in estimating ML effort.

</details>


### [64] [Quality Model for Machine Learning Components](https://arxiv.org/abs/2602.05043)
*Grace A. Lewis,Rachel Brower-Sinning,Robert Edman,Ipek Ozkaya,Sebastián Echeverría,Alex Derr,Collin Beaudoin,Katherine R. Maffey*

Main category: cs.SE

TL;DR: 本文针对机器学习组件提出质量模型，促进系统需求明确和测试优化，验证有效性并应用于开源工具，解决了现有标准不适用的问题。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习模型测试多侧重模型性能，忽视系统层面需求，导致整合与部署失败，现有标准如ISO 25059未能有效区分系统属性和组件属性。

Method: 通过构建质量模型，结合调查反馈验证模型的有效性，并将其应用于开源机器学习组件测试工具。

Result: 构建了一个适用于机器学习组件的质量模型，获得业界认可，并实际应用于开源测试工具，提升测试针对性和系统集成成功率。

Conclusion: 本文提出了一个针对机器学习组件的质量模型，帮助开发者和利益相关者明确并协商系统派生需求，优化测试策略。该模型通过调查验证了其相关性和价值，并成功集成到开源测试工具中。

Abstract: Despite increased adoption and advances in machine learning (ML), there are studies showing that many ML prototypes do not reach the production stage and that testing is still largely limited to testing model properties, such as model performance, without considering requirements derived from the system it will be a part of, such as throughput, resource consumption, or robustness. This limited view of testing leads to failures in model integration, deployment, and operations. In traditional software development, quality models such as ISO 25010 provide a widely used structured framework to assess software quality, define quality requirements, and provide a common language for communication with stakeholders. A newer standard, ISO 25059, defines a more specific quality model for AI systems. However, a problem with this standard is that it combines system attributes with ML component attributes, which is not helpful for a model developer, as many system attributes cannot be assessed at the component level. In this paper, we present a quality model for ML components that serves as a guide for requirements elicitation and negotiation and provides a common vocabulary for ML component developers and system stakeholders to agree on and define system-derived requirements and focus their testing efforts accordingly. The quality model was validated through a survey in which the participants agreed with its relevance and value. The quality model has been successfully integrated into an open-source tool for ML component testing and evaluation demonstrating its practical application.

</details>


### [65] [TestMigrationsInPy: A Dataset of Test Migrations from Unittest to Pytest](https://arxiv.org/abs/2602.05122)
*Altino Alves,Andre Hora*

Main category: cs.SE

TL;DR: 提出了一个包含923个真实测试迁移案例的数据集，支持Python测试框架unittest到pytest的迁移研究与自动化工具开发。


<details>
  <summary>Details</summary>
Motivation: Python项目从unittest迁移到pytest的过程复杂且耗时，缺乏自动化迁移支持。

Method: 通过收集923个开发者实际完成的从unittest到pytest的测试迁移案例构建了TestMigrationsInPy数据集。

Result: TestMigrationsInPy包含了多样的迁移类型信息，促进了针对不同迁移需求的解决方案验证。

Conclusion: TestMigrationsInPy为Python测试框架迁移提供了一个实用的数据集，有助于推动自动化迁移工具的研发。

Abstract: Unittest and pytest are the most popular testing frameworks in Python. Overall, pytest provides some advantages, including simpler assertion, reuse of fixtures, and interoperability. Due to such benefits, multiple projects in the Python ecosystem have migrated from unittest to pytest. To facilitate the migration, pytest can also run unittest tests, thus, the migration can happen gradually over time. However, the migration can be time-consuming and take a long time to conclude. In this context, projects would benefit from automated solutions to support the migration process. In this paper, we propose TestMigrationsInPy, a dataset of test migrations from unittest to pytest. TestMigrationsInPy contains 923 real-world migrations performed by developers. Future research proposing novel solutions to migrate frameworks in Python can rely on TestMigrationsInPy as a ground truth. Moreover, as TestMigrationsInPy includes information about the migration type (e.g., changes in assertions or fixtures), our dataset enables novel solutions to be verified effectively, for instance, from simpler assertion migrations to more complex fixture migrations. TestMigrationsInPy is publicly available at: https://github.com/altinoalvesjunior/TestMigrationsInPy.

</details>


### [66] [Exceptional Behaviors: How Frequently Are They Tested?](https://arxiv.org/abs/2602.05123)
*Andre Hora,Gordon Fraser*

Main category: cs.SE

TL;DR: 本文通过对大规模Python测试的实证研究，发现异常行为在测试中并非罕见，提醒开发者和研究者关注异常行为测试及相关工具开发。


<details>
  <summary>Details</summary>
Motivation: 当前研究关注传递到测试的异常，但忽视了未传递到测试的异常行为，本文关注如何频繁异常行为被测试以提升软件质量。

Method: 作者通过对25个Python系统的测试套件进行插桩监控，收集运行时抛出的异常数据，分析了异常行为的频率和测试覆盖情况。

Result: 21.4%的被执行方法在运行时抛出异常，其中异常调用的中位数为每10次调用中触发一次异常，近80%异常方法偶尔抛异常，20%较频繁抛异常。

Conclusion: 本文通过实证研究发现异常行为在测试中被频繁触发且不一定罕见，强调了异常行为测试的重要性。

Abstract: Exceptions allow developers to handle error cases expected to occur infrequently. Ideally, good test suites should test both normal and exceptional behaviors to catch more bugs and avoid regressions. While current research analyzes exceptions that propagate to tests, it does not explore other exceptions that do not reach the tests. In this paper, we provide an empirical study to explore how frequently exceptional behaviors are tested in real-world systems. We consider both exceptions that propagate to tests and the ones that do not reach the tests. For this purpose, we run an instrumented version of test suites, monitor their execution, and collect information about the exceptions raised at runtime. We analyze the test suites of 25 Python systems, covering 5,372 executed methods, 17.9M calls, and 1.4M raised exceptions. We find that 21.4% of the executed methods do raise exceptions at runtime. In methods that raise exceptions, on the median, 1 in 10 calls exercise exceptional behaviors. Close to 80% of the methods that raise exceptions do so infrequently, but about 20% raise exceptions more frequently. Finally, we provide implications for researchers and practitioners. We suggest developing novel tools to support exercising exceptional behaviors and refactoring expensive try/except blocks. We also call attention to the fact that exception-raising behaviors are not necessarily "abnormal" or rare.

</details>


### [67] [The Necessity of a Holistic Safety Evaluation Framework for AI-Based Automation Features](https://arxiv.org/abs/2602.05157)
*Alireza Abbaspour,Shabin Mahadevan,Kilian Zwirglmaier,Jeff Stafford*

Main category: cs.SE

TL;DR: AI集成使驾驶自动化中的QM组件可能产生安全风险，需通过综合FuSa、SOTIF及AI标准方法重新评估和管理安全，保障系统整体安全。


<details>
  <summary>Details</summary>
Motivation: 传统驾驶自动化功能的SOTIF和FuSa分析未将质量管理（QM）组件纳入严格的安全评估，然而AI集成使这些组件可能带来风险。

Method: 通过案例研究，结合理论分析，探讨AI组件尤其是感知算法中的安全缺陷，采用FuSa、SOTIF及AI安全标准方法进行全面风险识别和缓解。

Result: 研究表明QM组件中AI驱动感知系统缺陷可能导致功能异常和安全风险，需修订现有安全框架以涵盖AI带来的挑战，实现多标准下全面安全保障。

Conclusion: 本文强调必须对AI组件开展整体安全分析与风险评估，修订安全标准以适应AI技术，确保驾驶系统中所有组件的安全性。

Abstract: The intersection of Safety of Intended Functionality (SOTIF) and Functional Safety (FuSa) analysis of driving automation features has traditionally excluded Quality Management (QM) components from rigorous safety impact evaluations. While QM components are not typically classified as safety-relevant, recent developments in artificial intelligence (AI) integration reveal that such components can contribute to SOTIF-related hazardous risks. Compliance with emerging AI safety standards, such as ISO/PAS 8800, necessitates re-evaluating safety considerations for these components. This paper examines the necessity of conducting holistic safety analysis and risk assessment on AI components, emphasizing their potential to introduce hazards with the capacity to violate risk acceptance criteria when deployed in safety-critical driving systems, particularly in perception algorithms. Using case studies, we demonstrate how deficiencies in AI-driven perception systems can emerge even in QM-classified components, leading to unintended functional behaviors with critical safety implications. By bridging theoretical analysis with practical examples, this paper argues for the adoption of comprehensive FuSa, SOTIF, and AI standards-driven methodologies to identify and mitigate risks in AI components. The findings demonstrate the importance of revising existing safety frameworks to address the evolving challenges posed by AI, ensuring comprehensive safety assurance across all component classifications spanning multiple safety standards.

</details>


### [68] [EGSS: Entropy-guided Stepwise Scaling for Reliable Software Engineering](https://arxiv.org/abs/2602.05242)
*Chenhui Mao,Yuanting Lei,Zhixiang Wei,Ming Liang,Zhixiang Wang,Jingxuan Xu,Dajun Chen,Wei Jiang,Yong Li*

Main category: cs.SE

TL;DR: 本文提出的EGSS方法通过熵引导搜索和测试集扩展，提升了代码生成和错误修复任务中测试时扩展的效率与效果，显著提高模型性能并降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 针对Agentic Test-Time Scaling (TTS)在计算开销大及缺乏可靠候选解选择机制的问题，提出改进方案以提升性能和效率。

Method: 提出了Entropy-Guided Stepwise Scaling (EGSS)，通过熵引导的自适应搜索和稳健的测试集扩展，实现了动态平衡效率与效果的测试时扩展框架。

Result: EGSS在SWE-Bench-Verified数据集上使所有模型性能提升5-10%，如将Kimi-K2-Intruct解决率从63.2%提升到72.2%，GLM-4.6提升至74.6%，并减少推理时token使用量超过28%。

Conclusion: EGSS显著提高了软件工程任务中的性能表现，达到了SOTA水平，同时减少了计算资源的使用。

Abstract: Agentic Test-Time Scaling (TTS) has delivered state-of-the-art (SOTA) performance on complex software engineering tasks such as code generation and bug fixing. However, its practical adoption remains limited due to significant computational overhead, primarily driven by two key challenges: (1) the high cost associated with deploying excessively large ensembles, and (2) the lack of a reliable mechanism for selecting the optimal candidate solution, ultimately constraining the performance gains that can be realized. To address these challenges, we propose Entropy-Guided Stepwise Scaling (EGSS), a novel TTS framework that dynamically balances efficiency and effectiveness through entropy-guided adaptive search and robust test-suite augmentation. Extensive experiments on SWE-Bench-Verified demonstrate that EGSS consistently boosts performance by 5-10% across all evaluated models. Specifically, it increases the resolved ratio of Kimi-K2-Intruct from 63.2% to 72.2%, and GLM-4.6 from 65.8% to 74.6%. Furthermore, when paired with GLM-4.6, EGSS achieves a new state-of-the-art among open-source large language models. In addition to these accuracy improvements, EGSS reduces inference-time token usage by over 28% compared to existing TTS methods, achieving simultaneous gains in both effectiveness and computational efficiency.

</details>


### [69] [PatchGuru: Patch Oracle Inference from Natural Language Artifacts with Large Language Models](https://arxiv.org/abs/2602.05270)
*Thanh Le-Cong,Bach Le,Toby Murray,Michael Pradel,Cristian Cadar*

Main category: cs.SE

TL;DR: PatchGuru提出了一种基于大语言模型的自动补丁意图推断和验证技术，有效提升补丁验证精度和效率，发现多个真实漏洞，辅助代码审查和回归测试。


<details>
  <summary>Details</summary>
Motivation: 现有的回归测试不完整，且补丁意图通常以非正式的自然语言描述，给补丁验证带来困难，因此需要一种自动化且可执行的补丁意图验证方法。

Method: PatchGuru 利用大型语言模型从自然语言的开发者意图中提取信息，并综合补丁的前后版本合成运行时断言作为补丁规格，同时通过迭代比较和自我审查不断优化补丁验证过程。

Result: 在400个开源Python项目的拉取请求上测试，PatchGuru报告出39个警告，精确度达到0.62，发现了24个真实漏洞，其中12个为未知漏洞，并且优于现有技术Testora。

Conclusion: PatchGuru 成功地实现了自动推断补丁规格并验证补丁意图，显著提升了补丁验证的准确性和效率。

Abstract: As software systems evolve, patches may unintentionally alter program behavior. Validating patches against their intended semantics is difficult due to incomplete regression tests and informal, non-executable natural language (NL) descriptions of patch intent. We present PatchGuru, the first automated technique that infers executable patch specifications from real-world pull requests (PRs). Given a PR, PatchGuru uses large language models (LLMs) to extract developer intent from NL artifacts and synthesizes patch oracles: under-approximate yet practical specifications expressed as runtime assertions in comparison programs that integrate pre- and post-patch versions. Patch oracles focus on patch-relevant behaviors, enable automated validation, and support cross-version properties. PatchGuru iteratively refines inferred oracles by comparing pre- and post-patch behaviors, identifies violations, filters inconsistencies via self-review, and generates bug reports. We evaluate PatchGuru on 400 recent PRs from four widely used open-source Python projects. PatchGuru reports 39 warnings with a precision of 0.62, yielding 24 confirmed true positives, including 12 previously unknown bugs, 11 of which were subsequently fixed by developers. Compared to the state-of-the-art technique Testora, PatchGuru detects 17 more bugs (24 vs. 7) while improving precision from 0.32 to 0.62. PatchGuru incurs an average cost of 8.9 minutes and USD 0.07 per PR. These results suggest that PatchGuru complements code review and regression testing by providing executable documentation and automated validation of patch intent.

</details>


### [70] [Does Programming Language Matter? An Empirical Study of Fuzzing Bug Detection](https://arxiv.org/abs/2602.05312)
*Tatsuya Shirai,Olivier Nourry,Yutaro Kashiwa,Kenji Fujiwara,Hajimu Iida*

Main category: cs.SE

TL;DR: 本研究首次大规模分析连续模糊测试在多语言项目中的表现，发现语言特性显著影响检测效率和漏洞特征，提示开发语言感知的模糊测试策略。


<details>
  <summary>Details</summary>
Motivation: 尽管连续模糊测试被广泛采用，但此前研究未探讨其在不同编程语言中的效果差异。

Method: 对559个OSS-Fuzz项目进行了大规模跨语言分析，分析了61,444个模糊测试缺陷和999,248个构建数据，按主要语言类别进行比较。

Result: C++和Rust的缺陷检测频率较高；Rust和Python的漏洞比例较低但暴露更多关键漏洞；不同语言的崩溃类型不同，Go中不可重现缺陷较多，Rust较少；Python修复覆盖率高但检测时间较长。

Conclusion: 模糊测试的效果在不同编程语言中存在显著差异，语言设计对模糊测试的行为和效率有重要影响。

Abstract: Fuzzing has become a popular technique for automatically detecting vulnerabilities and bugs by generating unexpected inputs. In recent years, the fuzzing process has been integrated into continuous integration workflows (i.e., continuous fuzzing), enabling short and frequent testing cycles. Despite its widespread adoption, prior research has not examined whether the effectiveness of continuous fuzzing varies across programming languages. This study conducts a large-scale cross-language analysis to examine how fuzzing bug characteristics and detection efficiency differ among languages. We analyze 61,444 fuzzing bugs and 999,248 builds from 559 OSS-Fuzz projects categorized by primary language. Our findings reveal that (i) C++ and Rust exhibit higher fuzzing bug detection frequencies, (ii) Rust and Python show low vulnerability ratios but tend to expose more critical vulnerabilities, (iii) crash types vary across languages and unreproducible bugs are more frequent in Go but rare in Rust, and (iv) Python attains higher patch coverage but suffers from longer time-to-detection. These results demonstrate that fuzzing behavior and effectiveness are strongly shaped by language design, providing insights for language-aware fuzzing strategies and tool development.

</details>


### [71] [Emergence-as-Code for Self-Governing Reliable Systems](https://arxiv.org/abs/2602.05458)
*Anatoly A. Krasnovsky*

Main category: cs.SE

TL;DR: 该论文提出了Emergence-as-Code方法，使复杂微服务环境中用户旅程可靠性目标可声明、可计算并自动治理，提升了服务质量管理的精度和效率。


<details>
  <summary>Details</summary>
Motivation: 传统SLO-as-code虽定义了服务可靠性，但用户体验由复杂微服务拓扑和多因素共同影响，旅程目标易漂移，导致用户期望难以精准满足或资源过度预留，缺乏统一建模和自动化治理手段。

Method: 通过定义旅程意图声明与原子SLO及遥测绑定，结合运行时推理组件从轨迹和流量配置中合成旅程模型，使用EmaC编译器/控制器生成有限界旅程SLO和预算，并自动产出控制面工件，实现端到端治理。

Result: EmaC规范实现了从意图声明到动态生成旅程SLO、预算及管控策略的闭环，并通过Git工作流支持审查，提供示例规范及产生的控制工件，验证了该方法的可行性和实践价值。

Conclusion: 提出的Emergence-as-Code (EmaC)框架有效实现了可计算和可治理的用户旅程可靠性声明，解决了微服务系统中旅行目标与实际运维代码脱节的问题。

Abstract: SLO-as-code has made per-service} reliability declarative, but user experience is defined by journeys whose reliability is an emergent property of microservice topology, routing, redundancy, timeouts/fallbacks, shared failure domains, and tail amplification. As a result, journey objectives (e.g., "checkout p99 < 400 ms") are often maintained outside code and drift as the system evolves, forcing teams to either miss user expectations or over-provision and gate releases with ad-hoc heuristics. We propose Emergence-as-Code (EmaC), a vision for making journey reliability computable and governable via intent plus evidence. An EmaC spec declares journey intent (objective, control-flow operators, allowed actions) and binds it to atomic SLOs and telemetry. A runtime inference component consumes operational artifacts (e.g., tracing and traffic configuration) to synthesize a candidate journey model with provenance and confidence. From the last accepted model, the EmaC compiler/controller derives bounded journey SLOs and budgets under explicit correlation assumptions (optimistic independence vs. pessimistic shared fate), and emits control-plane artifacts (burn-rate alerts, rollout gates, action guards) that are reviewable in a Git workflow. An anonymized artifact repository provides a runnable example specification and generated outputs.

</details>


### [72] [Can We Classify Flaky Tests Using Only Test Code? An LLM-Based Empirical Study](https://arxiv.org/abs/2602.05465)
*Alexander Berndt,Vekil Bekmyradov,Rainer Gemulla,Marcus Kessel,Thomas Bach,Sebastian Baltes*

Main category: cs.SE

TL;DR: 本研究评估大语言模型仅凭测试代码识别测试不稳定性，发现效果不佳，提示未来需结合更多上下文信息。


<details>
  <summary>Details</summary>
Motivation: 现有基于测试代码标识符的模型缺乏泛化能力，难以实用。大语言模型具备跨任务泛化能力，可能解决该问题。

Method: 评估三种大语言模型（两个通用模型，一个代码专用模型）在两个基准数据集上的表现，采用三种提示技术；同时手动分析50个样本以判断仅凭测试代码分类的可行性。

Result: 大语言模型仅凭测试代码分类测试不稳定性效果接近随机猜测，测试代码信息不足以支持分类。

Conclusion: 仅基于测试代码的信息不足以有效分类测试不稳定性，未来可结合额外上下文或高级方法提升性能。

Abstract: Flaky tests yield inconsistent results when they are repeatedly executed on the same code revision. They interfere with automated quality assurance of code changes and hinder efficient software testing. Previous work evaluated approaches to train machine learning models to classify flaky tests based on identifiers in the test code. However, the resulting classifiers have been shown to lack generalizability, hindering their applicability in practical environments. Recently, pre-trained Large Language Models (LLMs) have shown the capability to generalize across various tasks. Thus, they represent a promising approach to address the generalizability problem of previous approaches. In this study, we evaluated three LLMs (two general-purpose models, one code-specific model) using three prompting techniques on two benchmark datasets from prior studies on flaky test classification. Furthermore, we manually investigated 50 samples from the given datasets to determine whether classifying flaky tests based only on test code is feasible for humans. Our findings indicate that LLMs struggle to classify flaky tests given only the test code. The results of our best prompt-model combination were only marginally better than random guessing. In our manual analysis, we found that the test code does not necessarily contain sufficient information for a flakiness classification. Our findings motivate future work to evaluate LLMs for flakiness classification with additional context, for example, using retrieval-augmented generation or agentic AI.

</details>


### [73] [Sovereign-by-Design A Reference Architecture for AI and Blockchain Enabled Systems](https://arxiv.org/abs/2602.05486)
*Matteo Esposito,Lodovica Marchesi,Roberto Tonelli,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: 本文提出了一种以主权为核心架构属性的参考架构，将生成式AI与区块链、自我主权身份等技术整合，以实现数字主权的可操作化，为构建合规且可审计的AI系统提供了理论框架。


<details>
  <summary>Details</summary>
Motivation: 数字主权成为现代软件系统的核心关注点，现有的治理和合规措施不能全面体现架构层面的主权要求。

Method: 提出主权参考架构，整合自我主权身份、区块链信任与审计、主权数据治理和受控的生成式AI。

Result: 开发出了一个融合多项关键技术，实现主权性和合规性的参考架构，为未来研究与实践提供了理论和实践基础。

Conclusion: 数字主权应作为第一类架构属性被确立，该架构桥接了监管意图与具体系统设计，支持可审核、可进化和具备司法管辖感知的AI系统构建。

Abstract: Digital sovereignty has emerged as a central concern for modern software-intensive systems, driven by the dominance of non-sovereign cloud infrastructures, the rapid adoption of Generative AI, and increasingly stringent regulatory requirements. While existing initiatives address governance, compliance, and security in isolation, they provide limited guidance on how sovereignty can be operationalized at the architectural level. In this paper, we argue that sovereignty must be treated as a first-class architectural property rather than a purely regulatory objective. We introduce a Sovereign Reference Architecture that integrates self-sovereign identity, blockchain-based trust and auditability, sovereign data governance, and Generative AI deployed under explicit architectural control. The architecture explicitly captures the dual role of Generative AI as both a source of governance risk and an enabler of compliance, accountability, and continuous assurance when properly constrained. By framing sovereignty as an architectural quality attribute, our work bridges regulatory intent and concrete system design, offering a coherent foundation for building auditable, evolvable, and jurisdiction-aware AI-enabled systems. The proposed reference architecture provides a principled starting point for future research and practice at the intersection of software architecture, Generative AI, and digital sovereignty.

</details>


### [74] [Capture the Flags: Family-Based Evaluation of Agentic LLMs via Semantics-Preserving Transformations](https://arxiv.org/abs/2602.05523)
*Shahin Honarvar,Amber Gorzynski,James Lee-Jones,Harry Coppock,Marek Rei,Joseph Ryan,Alastair F. Donaldson*

Main category: cs.SE

TL;DR: 本文设计了一种生成语义等价CTF挑战家族的方法和工具Evolve-CTF，用于评估大语言模型在不同代码变换下的鲁棒性，发现模型对简单变换较稳健，但复杂变换影响明显，显式推理作用不大。


<details>
  <summary>Details</summary>
Motivation: 当前的单点CTF基准难以评估大语言模型在源代码多版本下的鲁棒性和泛化能力，亟需一种能控制源代码变换同时保持攻击策略不变的评测方法。

Method: 提出Evolve-CTF工具，基于Python代码通过语义保持的程序变换生成多样的CTF挑战家族，利用这些家族对13种具备工具访问能力的大语言模型进行系统评估。

Result: 使用Evolve-CTF从Cybench和Intercode中衍生CTF家族，对13种具备工具使用能力的代理大语言模型进行评测，发现模型对简单变换鲁棒，但复杂组合变换和深度混淆降低性能，显示工具使用需求更高，显式推理对成功率影响有限。

Conclusion: 本文提出了一种基于语义保持的程序变换生成CTF挑战家族的新方法，从而能更好地评估大语言模型在网络安全任务中的鲁棒性和泛化能力。实验表明模型对简单重命名和代码插入等变换较为鲁棒，但复杂变换和深度混淆对性能影响较大，显示需要更复杂的工具使用能力。显式推理能力对解决率影响不大。

Abstract: Agentic large language models (LLMs) are increasingly evaluated on cybersecurity tasks using capture-the-flag (CTF) benchmarks. However, existing pointwise benchmarks have limited ability to shed light on the robustness and generalisation abilities of agents across alternative versions of the source code. We introduce CTF challenge families, whereby a single CTF is used as the basis for generating a family of semantically-equivalent challenges via semantics-preserving program transformations. This enables controlled evaluation of agent robustness to source code transformations while keeping the underlying exploit strategy fixed. We introduce a new tool, Evolve-CTF, that generates CTF families from Python challenges using a range of transformations. Using Evolve-CTF to derive families from Cybench and Intercode challenges, we evaluate 13 agentic LLM configurations with tool access. We find that models are remarkably robust to intrusive renaming and code insertion-based transformations, but that composed transformations and deeper obfuscation affect performance by requiring more sophisticated use of tools. We also find that enabling explicit reasoning has little effect on solution success rates across challenge families. Our work contributes a valuable technique and tool for future LLM evaluations, and a large dataset characterising the capabilities of current state-of-the-art models in this domain.

</details>


### [75] [ArkTS-CodeSearch: A Open-Source ArkTS Dataset for Code Retrieval](https://arxiv.org/abs/2602.05550)
*Yulong He,Artem Ermakov,Sergey Kovalchuk,Artem Aliev,Dmitry Shalymov*

Main category: cs.SE

TL;DR: 本文针对缺乏数据集的ArkTS语言，构建了大规模代码与注释数据集，提出代码检索任务，评测并微调模型，首次建立系统性基准并公开数据资源。


<details>
  <summary>Details</summary>
Motivation: ArkTS作为OpenHarmony生态核心编程语言，缺乏公开数据集与评测基准，制约了代码智能相关研究的发展。

Method: 通过从GitHub和Gitee爬取ArkTS代码库，利用tree-sitter-arkts提取注释与函数对，设计单搜索任务进行代码检索评估；对现有开源代码嵌入模型进行评估，并结合ArkTS与TypeScript数据集进行微调。

Result: 构建了大型ArkTS数据集，设计了对代码检索有指导意义的任务，评测并微调模型后获得高性能ArkTS代码理解模型，数据集与模型将公开发布。

Conclusion: 本文建立了首个系统性的ArkTS代码检索基准，显著推动了ArkTS代码智能研究的发展。

Abstract: ArkTS is a core programming language in the OpenHarmony ecosystem, yet research on ArkTS code intelligence is hindered by the lack of public datasets and evaluation benchmarks. This paper presents a large-scale ArkTS dataset constructed from open-source repositories, targeting code retrieval and code evaluation tasks. We design a single-search task, where natural language comments are used to retrieve corresponding ArkTS functions. ArkTS repositories are crawled from GitHub and Gitee, and comment-function pairs are extracted using tree-sitter-arkts, followed by cross-platform deduplication and statistical analysis of ArkTS function types. We further evaluate all existing open-source code embedding models on the single-search task and perform fine-tuning using both ArkTS and TypeScript training datasets, resulting in a high-performing model for ArkTS code understanding. This work establishes the first systematic benchmark for ArkTS code retrieval. Both the dataset and our fine-tuned model will be released publicly and are available at https://huggingface.co/hreyulog/embedinggemma_arkts and https://huggingface.co/datasets/hreyulog/arkts-code-docstring,establishing the first systematic benchmark for ArkTS code retrieval.

</details>


### [76] [SEAL: Symbolic Execution with Separation Logic (Competition Contribution)](https://arxiv.org/abs/2602.05703)
*Tomáš Brablec,Tomáš Dacík,Tomáš Vojnar*

Main category: cs.SE

TL;DR: SEAL是一个利用分离逻辑和SMT技术验证无界链表程序的静态分析工具，表现优异且结构模块化，具有良好的扩展潜力。


<details>
  <summary>Details</summary>
Motivation: 现有基于分离逻辑的方法在处理链表等无界数据结构程序验证时存在局限，寻求一种更通用、易扩展且可组合其他理论的解决方案。

Method: 利用分离逻辑表示抽象内存状态，采用基于SMT的通用分离逻辑求解器Astral进行可满足性和蕴含性检查，形成模块化架构。

Result: SEAL作为原型实现，在LinkedLists基准测试中表现优秀，是少数能够验证无界链表程序的分析器之一。

Conclusion: SEAL作为一种基于分离逻辑和SMT求解器的静态分析工具，在验证操作无界链表的程序中展现出竞争力，并具备良好的可扩展性。

Abstract: SEAL is a static analyser for the verification of programs that manipulate unbounded linked data structures. It is based on separation logic to represent abstract memory states and, unlike other separation-logic-based approaches, it employs a general-purpose separation logic solver Astral for satisfiability and entailment checking, which itself is based on translation to SMT. This design results in a modular architecture intended to be easier to extend and to combine with reasoning in other theories. Although still a prototype, SEAL achieved competitive results in the LinkedLists base category and was one of only four analysers capable of verifying programs with unbounded lists. We believe that the tool's extensibility, combined with further development, can lead to significant improvements in future competitions.

</details>


### [77] [Towards Green AI: Decoding the Energy of LLM Inference in Software Development](https://arxiv.org/abs/2602.05712)
*Lola Solovyeva,Fernando Castor*

Main category: cs.SE

TL;DR: 该论文通过阶段性分析LLM推理能耗，发现预填充阶段和啰嗦行为对解码阶段能耗有显著影响，提出啰嗦抑制方法能大幅降低能耗，助力绿色软件开发。


<details>
  <summary>Details</summary>
Motivation: AI辅助工具广泛应用于软件开发，但大型语言模型(LLMs)的推理过程计算和能耗巨大，亟需理解和减少能耗以实现可持续发展。

Method: 本研究对LLM推理能耗进行阶段性分析，区分预填充(prefill)阶段和解码(decoding)阶段，使用六个6B-7B和四个3B-4B参数规模的变换器模型，在代码生成(HumanEval)和代码理解(LongBench)基准上进行评估。

Result: 发现不同模型在预填充和解码阶段表现出不同的能耗模式，预填充能耗的增加会放大解码阶段每个token的能耗，幅度为1.3%至51.8%。三分之一模型存在无意义“啰嗦”输出，导致额外能耗。通过抑制啰嗦行为，能在不影响准确率情况下节省44%至89%的能耗。

Conclusion: 预填充阶段的能耗影响主导能耗的解码阶段，啰嗦行为显著增加能耗。要降低推理能耗，需抑制啰嗦并减小预填充对解码的影响。

Abstract: Context: AI-assisted tools are increasingly integrated into software development workflows, but their reliance on large language models (LLMs) introduces substantial computational and energy costs. Understanding and reducing the energy footprint of LLM inference is therefore essential for sustainable software development. Objective: In this study, we conduct a phase-level analysis of LLM inference energy consumption, distinguishing between the (1) prefill, where the model processes the input and builds internal representations, and (2) decoding, where output tokens are generated using the stored state. Method: We investigate six 6B-7B and four 3B-4B transformer-based models, evaluating them on code-centric benchmarks HumanEval for code generation and LongBench for code understanding. Results: Our findings show that, within both parameter groups, models exhibit distinct energy patterns across phases. Furthermore, we observed that increases in prefill cost amplify the energy cost per token during decoding, with amplifications ranging from 1.3% to 51.8% depending on the model. Lastly, three out of ten models demonstrate babbling behavior, adding excessive content to the output that unnecessarily inflates energy consumption. We implemented babbling suppression for code generation, achieving energy savings ranging from 44% to 89% without affecting generation accuracy. Conclusion: These findings show that prefill costs influence decoding, which dominates energy consumption, and that babbling suppression can yield up to 89% energy savings. Reducing inference energy therefore requires both mitigating babbling behavior and limiting impact of prefill on decoding.

</details>


### [78] [A Dual-Loop Agent Framework for Automated Vulnerability Reproduction](https://arxiv.org/abs/2602.05721)
*Bin Liu,Yanjie Zhao,Zhenpeng Chen,Guoai Xu,Haoyu Wang*

Main category: cs.SE

TL;DR: 提出Cve2PoC，一个基于大型语言模型的双循环漏洞复现框架，通过战略规划和战术执行分离，提升漏洞利用代码自动生成的成功率和质量，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 利用大型语言模型（LLM）自动从CVE描述中生成和验证漏洞利用代码，以解决手工复现漏洞耗时且需要专业技能的问题。

Method: 提出Cve2PoC，一种基于LLM的双循环智能体框架，采用计划-执行-评估范式：战略规划者分析漏洞和目标代码生成攻击计划；战术执行者生成PoC代码并进行分阶段验证；自适应优化器根据执行结果将失败分别交给战术循环（代码层面修正）或战略循环（攻击策略重规划）。

Result: 在两个基准测试集（SecBench.js和PatchEval）上，对617个真实漏洞进行评估，复现成功率分别达到82.9%和54.3%，分别比最佳基线提升11.3%和20.4%。人工评估表明生成的PoC代码在可读性和可复用性上与人工编写的利用代码相当。

Conclusion: Cve2PoC通过双循环设计有效区分不同失败原因，避免无效调试循环，大幅提升了自动漏洞复现的成功率和代码质量，展示出在软件安全自动化中的显著优势。

Abstract: Automated vulnerability reproduction from CVE descriptions requires generating executable Proof-of-Concept (PoC) exploits and validating them in target environments. This process is critical in software security research and practice, yet remains time-consuming and demands specialized expertise when performed manually. While LLM agents show promise for automating this task, existing approaches often conflate exploring attack directions with fixing implementation details, which leads to unproductive debugging loops when reproduction fails. To address this, we propose Cve2PoC, an LLM-based dual-loop agent framework following a plan-execute-evaluate paradigm. The Strategic Planner analyzes vulnerability semantics and target code to produce structured attack plans. The Tactical Executor generates PoC code and validates it through progressive verification. The Adaptive Refiner evaluates execution results and routes failures to different loops: the \textit{Tactical Loop} for code-level refinement, while the \textit{Strategic Loop} for attack strategy replanning. This dual-loop design enables the framework to escape ineffective debugging by matching remediation to failure type. Evaluation on two benchmarks covering 617 real-world vulnerabilities demonstrates that Cve2PoC achieves 82.9\% and 54.3\% reproduction success rates on SecBench.js and PatchEval, respectively, outperforming the best baseline by 11.3\% and 20.4\%. Human evaluation confirms that generated PoCs achieve comparable code quality to human-written exploits in readability and reusability.

</details>


### [79] [A Bayesian Optimization-Based AutoML Framework for Non-Intrusive Load Monitoring](https://arxiv.org/abs/2602.05739)
*Nazanin Siavash,Armin Moin*

Main category: cs.SE

TL;DR: 提出结合AutoML和贝叶斯优化的非侵入式负载监测框架，开发了兼容多算法的开源工具AutoML4NILM，便于领域专家无需深入机器学习即可应用能耗分解技术。


<details>
  <summary>Details</summary>
Motivation: 传统NILM方法依赖专业知识进行模型设计和调参，限制了其在实际中广泛应用；自动化方法可降低门槛，推动研究与产业界的应用。

Method: 引入自动机器学习（AutoML）技术结合贝叶斯优化，自动完成模型选择和超参数调优，开发了支持多种算法的AutoML4NILM开源工具包。

Result: 成功实现了一个包含11种算法的灵活可扩展开源工具包AutoML4NILM，支持自动化模型选择与调优，促进了能耗分析研究和应用。

Conclusion: 本文提出的集成自动机器学习的NILM框架，利用贝叶斯优化实现了模型自动选择与超参数调优，降低了领域专家的应用门槛，提高了能耗分解的效率和准确性。

Abstract: Non-Intrusive Load Monitoring (NILM), commonly known as energy disaggregation, aims to estimate the power consumption of individual appliances by analyzing a home's total electricity usage. This method provides a cost-effective alternative to installing dedicated smart meters for each appliance. In this paper, we introduce a novel framework that incorporates Automated Machine Learning (AutoML) into the NILM domain, utilizing Bayesian Optimization for automated model selection and hyperparameter tuning. This framework empowers domain practitioners to effectively apply machine learning techniques without requiring advanced expertise in data science or machine learning. To support further research and industry adoption, we present AutoML4NILM, a flexible and extensible open-source toolkit designed to streamline the deployment of AutoML solutions for energy disaggregation. Currently, this framework supports 11 algorithms, each with different hyperparameters; however, its flexible design allows for the extension of both the algorithms and their hyperparameters.

</details>


### [80] [Toward Quantum-Safe Software Engineering: A Vision for Post-Quantum Cryptography Migration](https://arxiv.org/abs/2602.05759)
*Lei Zhang*

Main category: cs.SE

TL;DR: 针对后量子密码学迁移挑战，提出AQuA框架，开启量子安全软件工程新领域。


<details>
  <summary>Details</summary>
Motivation: 量子计算对现有网络安全构成威胁，现有工具不适用于后量子密码技术，需求新的软件工程方法。

Method: 提出了自动量子安全适配框架（AQuA），包括PQC感知检测、语义重构和混合验证三大支柱。

Result: 构想了一套面向后量子密码学的软件工具框架，推动量子安全软件工程成为独立研究方向。

Conclusion: 迁移到后量子密码学软件工程需要专门的工具支持，以解决其概率性行为、侧信道敏感性及性能权衡等独特挑战。

Abstract: The quantum threat to cybersecurity has accelerated the standardization of Post-Quantum Cryptography (PQC). Migrating legacy software to these quantum-safe algorithms is not a simple library swap, but a new software engineering challenge: existing vulnerability detection, refactoring, and testing tools are not designed for PQC's probabilistic behavior, side-channel sensitivity, and complex performance trade-offs. To address these challenges, this paper outlines a vision for a new class of tools and introduces the Automated Quantum-safe Adaptation (AQuA) framework, with a three-pillar agenda for PQC-aware detection, semantic refactoring, and hybrid verification, thereby motivating Quantum-Safe Software Engineering (QSSE) as a distinct research direction.

</details>


### [81] [Automated Customization of LLMs for Enterprise Code Repositories Using Semantic Scopes](https://arxiv.org/abs/2602.05780)
*Ulrich Finkler,Irene Manotas,Wei Zhang,Geert Janssen,Octavian Popescu,Shyam Ramji*

Main category: cs.SE

TL;DR: 本文提出一种基于语义作用域的自动化定制方法，通过检索增强和微调策略，显著提升LLM在私有代码库上的代码补全性能，帮助开发者提高生产力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM尽管在公共基准测试中表现优异，但无法很好地生成与未见过的私有代码库一致的代码，定制化可以提升模型在私有代码库上的表现。

Method: 提出了一种基于代码中语义作用域的自动化LLM定制方法，结合检索增强生成（RAG）和监督微调（FT）两种策略，利用私有代码库数据制作训练数据对，帮助模型学习私有代码库的特定模式。

Result: 在两个私有企业代码库和两个公开基准测试上进行了评估，定制模型在代码补全任务中表现显著优于未定制模型，提升了代码质量和开发效率。

Conclusion: 通过基于语义作用域的自动化定制方法，模型能够更好地适应私有代码库，显著提升代码补全的准确性和开发者的生产力，定制后的中等规模模型表现优于未定制的大容量模型。

Abstract: Code completion (CC) is a task frequently used by developers when working in collaboration with LLM-based programming assistants. Despite the increased performance of LLMs on public benchmarks, out of the box LLMs still have a hard time generating code that aligns with a private code repository not previously seen by the model's training data. Customizing code LLMs to a private repository provides a way to improve the model performance. In this paper we present our approach for automated LLM customization based on semantic scopes in the code. We evaluate LLMs on real industry cases with two private enterprise code repositories with two customization strategies: Retrieval-Augmented Generation (RAG) and supervised Fine-Tuning (FT). Our mechanism for ingesting the repository's data and formulating the training data pairs with semantic scopes helps models to learn the underlying patterns specific to the repository, providing more precise code to developers and helping to boost their productivity. The code completions of moderately sized customized models can be significantly better than those of uncustomized models of much larger capacity. We also include an analysis of customization on two public benchmarks and present opportunities for future work.

</details>


### [82] [When Elo Lies: Hidden Biases in Codeforces-Based Evaluation of Large Language Models](https://arxiv.org/abs/2602.05891)
*Shenyu Zheng,Ximing Dong,Xiaoshuang Liu,Gustavo Oliva,Chong Chun Yong,Dayi Lin,Boyuan Chen,Shaowei Wang,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 本文系统分析了影响大型语言模型Codeforces Elo评分的不确定因素，揭示评分极其敏感且不稳定，警示不规范的Elo评分比较存在误导风险。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的竞争编程能力评测依赖于Codeforces的Elo评分，但评分中缺乏关键实验细节导致评分波动大，影响评价的可靠性。

Method: 通过控制变量设计实验，利用37场近期Codeforces竞赛和13,691个测试样例，系统研究影响Elo评分的隐藏因素，包括提交时间顺序、竞赛难度选择及模型运行间的随机性。

Result: 发现提交顺序可导致评分波动394分，竞赛选择导致评分差异最高达1122分，模型多次运行间评分差异最高为349分，表明Elo评分高度敏感且不稳定。

Conclusion: 未经严格标准化和透明实验报告，直接使用Elo评分进行模型比较是不可靠且具有误导性的，应谨慎使用此指标进行评估。

Abstract: As Large Language Models (LLMs) achieve breakthroughs in complex reasoning, Codeforces-based Elo ratings have emerged as a prominent metric for evaluating competitive programming capabilities. However, these ratings are often reported without critical experimental details, leading to significant discrepancies illustrated by recent reports where the score of the same model version fluctuated by nearly 500 points. This paper presents a systematic empirical study on the hidden factors biasing Elo evaluations: (1) the temporal ordering of submissions, (2) contest difficulty selection, and (3) run to run stochastic variability of LLMs. Utilizing a controlled benchmark of 37 recent Codeforces contests and 13,691 generated test cases, we demonstrate that Elo scores are highly sensitive to these parameters. Our findings reveal that varying submission orders can shift scores by 394 points, while contest selection can cause differences of up to 1,122 points for the same model. Run to run performance exhibits substantial instability, with a maximum difference of 349 points in mean scores observed when evaluating identical contests. We conclude that direct Elo comparisons are unreliable and potentially misleading without strict standardization and transparent reporting of experimental settings.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [83] [AI Agent Systems for Supply Chains: Structured Decision Prompts and Memory Retrieval](https://arxiv.org/abs/2602.05524)
*Konosuke Yoshizato,Kazuma Shimizu,Ryota Higa,Takanobu Otsuka*

Main category: cs.MA

TL;DR: 研究证实基于大型语言模型的多智能体系统在库存管理中具备推导最优订货策略的能力，且新提出的AIM-RM智能体显著提升了系统在复杂多样供应链环境下的表现和适应能力。


<details>
  <summary>Details</summary>
Motivation: 库存管理作为供应链管理的关键环节，传统方法面临众多挑战。虽然基于大型语言模型的多智能体系统被看作具有解决这些问题的潜力，但其是否能持续推导最优策略并适应不同场景尚不明确，有必要深入研究。

Method: 本研究首先设计了一个固定订货策略的提示语，结合库存管理中的安全库存策略，来编码问题的分步骤流程。然后提出了AIM-RM智能体，通过相似度匹配历史经验，提升系统的适应能力。采用多种供应链场景下的实验对比分析方法验证其有效性。

Result: 实验结果显示，固定订货策略的LLM-based MAS在有限场景下能实现最优订货决策，而引入AIM-RM智能体后，系统在多个供应链场景中均优于基线方法，表现出更高的鲁棒性和适应性。

Conclusion: 基于大型语言模型的多智能体系统（LLM-based MASs）能够在受限场景下实现库存管理的最优订货决策，且通过引入类似历史经验匹配的新型智能体AIM-RM，系统在多样化供应链情境中表现出更强的适应性和鲁棒性。

Abstract: This study investigates large language model (LLM) -based multi-agent systems (MASs) as a promising approach to inventory management, which is a key component of supply chain management. Although these systems have gained considerable attention for their potential to address the challenges associated with typical inventory management methods, key uncertainties regarding their effectiveness persist. Specifically, it is unclear whether LLM-based MASs can consistently derive optimal ordering policies and adapt to diverse supply chain scenarios. To address these questions, we examine an LLM-based MAS with a fixed-ordering strategy prompt that encodes the stepwise processes of the problem setting and a safe-stock strategy commonly used in inventory management. Our empirical results demonstrate that, even without detailed prompt adjustments, an LLM-based MAS can determine optimal ordering decisions in a restricted scenario. To enhance adaptability, we propose a novel agent called AIM-RM, which leverages similar historical experiences through similarity matching. Our results show that AIM-RM outperforms benchmark methods across various supply chain scenarios, highlighting its robustness and adaptability.

</details>


### [84] [Learning to Share: Selective Memory for Efficient Parallel Agentic Systems](https://arxiv.org/abs/2602.05965)
*Joseph Fioresi,Parth Parag Kulkarni,Ashmal Vayani,Song Wang,Mubarak Shah*

Main category: cs.MA

TL;DR: 本文提出LTS，一种基于强化学习的共享内存机制，优化了多团队并行智能体系统的信息复用，显著降低计算开销并提升性能。


<details>
  <summary>Details</summary>
Motivation: 多团队并行执行时存在大量重复计算，导致计算成本高。

Method: 提出学习共享机制（LTS），引入全局共享内存和轻量控制器，通过逐步强化学习训练控制器决定是否将中间步骤加入共享内存，实现信息跨团队复用。

Result: LTS在多个基准测试上显著减少运行时间，同时保持或提升任务性能，证明了学习式内存管理策略提升并行智能体系统效率的有效性。

Conclusion: LTS成功解决了多智能体系统并行执行中的冗余计算问题，提高了计算效率和任务表现。

Abstract: Agentic systems solve complex tasks by coordinating multiple agents that iteratively reason, invoke tools, and exchange intermediate results. To improve robustness and solution quality, recent approaches deploy multiple agent teams running in parallel to explore diverse reasoning trajectories. However, parallel execution comes at a significant computational cost: when different teams independently reason about similar sub-problems or execute analogous steps, they repeatedly perform substantial overlapping computation. To address these limitations, in this paper, we propose Learning to Share (LTS), a learned shared-memory mechanism for parallel agentic frameworks that enables selective cross-team information reuse while controlling context growth. LTS introduces a global memory bank accessible to all teams and a lightweight controller that decides whether intermediate agent steps should be added to memory or not. The controller is trained using stepwise reinforcement learning with usage-aware credit assignment, allowing it to identify information that is globally useful across parallel executions. Experiments on the AssistantBench and GAIA benchmarks show that LTS significantly reduces overall runtime while matching or improving task performance compared to memory-free parallel baselines, demonstrating that learned memory admission is an effective strategy for improving the efficiency of parallel agentic systems. Project page: https://joefioresi718.github.io/LTS_webpage/

</details>


### [85] [PhysicsAgentABM: Physics-Guided Generative Agent-Based Modeling](https://arxiv.org/abs/2602.06030)
*Kavana Venkatesh,Yinhan He,Jundong Li,Jiaming Cui*

Main category: cs.MA

TL;DR: 本文提出PhysicsAgentABM，通过将推断转向行为一致的代理簇，融合符号与神经模型，提升多智能体状态转移模拟的准确性和校准度，同时使用ANCHOR聚类策略大幅减少LLM调用，实现了高效且可扩展的仿真方法。


<details>
  <summary>Details</summary>
Motivation: 大规模可扩展且精准校准的多智能体状态转移模拟困难，传统LLM多智能体系统计算成本高且校准差，而经典代理模型虽具可解释性，但难以融合丰富的个体信号和非平稳行为。

Method: 采用状态专用的符号代理编码机理转移先验，使用多模态神经转移模型捕捉时间和交互动态，通过不确定性感知的认知融合获得校准的簇级转移分布。引入ANCHOR聚类策略，基于跨上下文行为响应和对比损失减少LLM调用次数。

Result: 在公共卫生、金融和社会科学领域的实验表明，PhysicsAgentABM在事件时间准确性和校准方面均优于机理、神经和LLM基线方法，且通过行为聚类显著降低了LLM调用次数。

Conclusion: PhysicsAgentABM通过行为一致的代理簇实现了可扩展且校准良好的状态转移仿真，结合了符号代理、神经网络模型及不确定性感知推理，显著提升了模拟的准确性和效率。

Abstract: Large language model (LLM)-based multi-agent systems enable expressive agent reasoning but are expensive to scale and poorly calibrated for timestep-aligned state-transition simulation, while classical agent-based models (ABMs) offer interpretability but struggle to integrate rich individual-level signals and non-stationary behaviors. We propose PhysicsAgentABM, which shifts inference to behaviorally coherent agent clusters: state-specialized symbolic agents encode mechanistic transition priors, a multimodal neural transition model captures temporal and interaction dynamics, and uncertainty-aware epistemic fusion yields calibrated cluster-level transition distributions. Individual agents then stochastically realize transitions under local constraints, decoupling population inference from entity-level variability. We further introduce ANCHOR, an LLM agent-driven clustering strategy based on cross-contextual behavioral responses and a novel contrastive loss, reducing LLM calls by up to 6-8 times. Experiments across public health, finance, and social sciences show consistent gains in event-time accuracy and calibration over mechanistic, neural, and LLM baselines. By re-architecting generative ABM around population-level inference with uncertainty-aware neuro-symbolic fusion, PhysicsAgentABM establishes a new paradigm for scalable and calibrated simulation with LLMs.

</details>
