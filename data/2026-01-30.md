<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 74]
- [cs.SE](#cs.SE) [Total: 29]
- [cs.MA](#cs.MA) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [DeepSearchQA: Bridging the Comprehensiveness Gap for Deep Research Agents](https://arxiv.org/abs/2601.20975)
*Nikita Gupta,Riju Chatterjee,Lukas Haas,Connie Tao,Andrew Wang,Chang Liu,Hidekazu Oiwa,Elena Gribovskaya,Jan Ackermann,John Blitzer,Sasha Goldshtein,Dipanjan Das*

Main category: cs.CL

TL;DR: 本文提出了一个全新的复杂多步骤信息搜寻基准DeepSearchQA，通过挑战当前智能体在系统整合信息和长程规划上的能力，暴露了其性能瓶颈，促进未来更强大深层搜索智能体的研究。


<details>
  <summary>Details</summary>
Motivation: 传统基准多聚焦单一答案检索或广泛事实准确性，缺乏对复杂搜索策略和长程规划能力的评估。

Method: 构建了DeepSearchQA基准数据集，包含900个针对复杂多步骤多领域搜索任务的手工设计问题，评估智能体在系统性信息整合、去重实体解析及搜索停止判断等能力。

Result: 实验揭示目前智能体存在提前停止检索或过度检索带低置信度答案的问题，显示其深度研究能力不足。

Conclusion: 当前最先进的智能体在复杂多步骤信息搜寻任务中表现存在显著不足，难以在高召回率和高精确率之间取得平衡。

Abstract: We introduce DeepSearchQA, a 900-prompt benchmark for evaluating agents on difficult multi-step information-seeking tasks across 17 different fields. Unlike traditional benchmarks that target single answer retrieval or broad-spectrum factuality, DeepSearchQA features a dataset of challenging, handcrafted tasks designed to evaluate an agent's ability to execute complex search plans to generate exhaustive answer lists. This shift in design explicitly tests three critical, yet under-evaluated capabilities: 1) systematic collation of fragmented information from disparate sources, 2) de-duplication and entity resolution to ensure precision, and 3) the ability to reason about stopping criteria within an open-ended search space. Each task is structured as a causal chain, where discovering information for one step is dependent on the successful completion of the previous one, stressing long-horizon planning and context retention. All tasks are grounded in the open web with objectively verifiable answer sets. Our comprehensive evaluation of state-of-the-art agent architectures reveals significant performance limitations: even the most advanced models struggle to balance high recall with precision. We observe distinct failure modes ranging from premature stopping (under-retrieval) to hedging behaviors, where agents cast an overly wide net of low-confidence answers to artificially boost recall. These findings highlight critical headroom in current agent designs and position DeepSearchQA as an essential diagnostic tool for driving future research toward more robust, deep-research capabilities.

</details>


### [2] [asr_eval: Algorithms and tools for multi-reference and streaming speech recognition evaluation](https://arxiv.org/abs/2601.20992)
*Oleg Sedukhin,Andrey Kostin*

Main category: cs.CL

TL;DR: 本文提出改进语音识别评估的方法，包括支持多参考标注和任意长度插入的字符串对齐算法，收集包含多参考标注的俄罗斯长篇野外语音测试集，研究多参考重标注及微调动态，揭示数据集特定标注导致的指标幻觉，并开发了多款评估和可视化工具。


<details>
  <summary>Details</summary>
Motivation: 现有语音识别评估方法在处理非拉丁语言、丰富词形及长篇语音时存在不足，多参考标注缺失且模型适应特定数据集标注导致指标不真实，急需改进的评估算法和数据资源。

Method: 提出支持多参考标注和任意长度插入的字符串对齐算法，收集和标注多参考的俄罗斯长篇野外语音测试集，进行多参考重标注和微调实验，开发基于改进对齐的评估与可视化工具，并提供统一的离线及流式语音识别模型接口。

Result: 成功设计了多参考标注支持的字符串对齐算法，构建了多参考标注的俄罗斯长篇语音测试集，发现模型微调会偏向特定标注导致指标虚假提升，开发了相关评估和可视化工具，提供了统一的多模型接口，代码将公开。

Conclusion: 通过改进的字符串对齐算法和多参考标注，本文提升了语音识别评估的准确性和适用性，特别是对非拉丁语系和长篇语音效果明显。多参考重标注揭示了模型对特定数据集标注的适应可能造成的指标误导，提供了更客观的评估工具和资源。

Abstract: We propose several improvements to the speech recognition evaluation. First, we propose a string alignment algorithm that supports both multi-reference labeling, arbitrary-length insertions and better word alignment. This is especially useful for non-Latin languages, those with rich word formation, to label cluttered or longform speech. Secondly, we collect a novel test set DiverseSpeech-Ru of longform in-the-wild Russian speech with careful multi-reference labeling. We also perform multi-reference relabeling of popular Russian tests set and study fine-tuning dynamics on its corresponding train set. We demonstrate that the model often adopts to dataset-specific labeling, causing an illusion of metric improvement. Based on the improved word alignment, we develop tools to evaluate streaming speech recognition and to align multiple transcriptions to compare them visually. Additionally, we provide uniform wrappers for many offline and streaming speech recognition models. Our code will be made publicly available.

</details>


### [3] [UrduBench: An Urdu Reasoning Benchmark using Contextually Ensembled Translations with Human-in-the-Loop](https://arxiv.org/abs/2601.21000)
*Muhammad Ali Shafique,Areej Mehboob,Layba Fiaz,Muhammad Usman Qadeer,Hamza Farooq*

Main category: cs.CL

TL;DR: 本文提出了一种结合多重翻译系统和人工验证的上下文集成翻译框架，开发了乌尔都语推理基准测试集UrduBench，并全面评估了多个大语言模型在该基准上的表现，揭示了多步骤和符号推理任务的挑战及语言对齐的重要性。


<details>
  <summary>Details</summary>
Motivation: 针对低资源语言缺乏标准推理基准，机器翻译敏感且重视通用语言任务而非推理任务的现状，开发专门的乌尔都语推理评估资源。

Method: 提出上下文集成翻译框架结合多翻译系统与人工验证，将主流推理和问答基准翻译成乌尔都语，并进行多模型、多策略的全面评测。

Result: 开发了UrduBench包含多个推理数据集，发现多步骤和符号推理困难较大，多语言模型表现差异明显，语言稳定对齐是提升推理能力的关键。

Conclusion: 该研究建立了可扩展的乌尔都语标准化推理评估方法，展现了多语言推理中的性能差异及挑战，为低资源语言的推理能力评测提供了重要工具和见解。

Abstract: Recent advances in large language models (LLMs) have led to strong reasoning capabilities; however, evaluating such models in low-resource languages remains challenging due to the lack of standardized benchmarks. In particular, Urdu reasoning evaluation has been limited by the sensitivity of machine translation and an emphasis on general language tasks rather than reasoning benchmarks. In this paper, we propose a contextually ensembled translation framework with human-in-the-loop validation that leverages multiple translation systems to develop Urdu reasoning benchmarks while preserving contextual and structural integrity. Using this framework, we translate widely adopted reasoning and question-answering benchmarks, including MGSM, MATH-500, CommonSenseQA, and OpenBookQA, into Urdu, collectively referred to as UrduBench, and conduct a comprehensive evaluation of both reasoning-oriented and instruction-tuned LLMs across multiple prompting strategies. Our analysis reveals performance differences across (1) four datasets, (2) five task difficulty levels, (3) diverse model architectures, (4) multiple model scaling settings, and (5) language consistency tests. We find that multi-step and symbolic reasoning tasks pose significant challenges in Urdu, and that stable language alignment is a critical prerequisite for robust reasoning. Overall, our work establishes a scalable methodology for standardized reasoning evaluation in Urdu and provides empirical insights into multilingual reasoning failures. This experimental setup is also broadly applicable to other low-resource languages. The code and datasets will be publicly released.

</details>


### [4] [Position-invariant Fine-tuning of Speech Enhancement Models with Self-supervised Speech Representations](https://arxiv.org/abs/2601.21084)
*Amit Meghanani,Thomas Hain*

Main category: cs.CL

TL;DR: 针对SE模型微调中位置依赖问题，提出位置不变的微调策略，显著提升了语音模型在噪声环境下的性能。


<details>
  <summary>Details</summary>
Motivation: 传统使用MSE损失在自监督学习表示微调中容易过度利用位置嵌入，导致微调不能有效捕捉语音的内容信息。

Method: 通过使用零填充和基于soft-DTW损失的速度扰动两种策略，避免利用位置相关信息对自监督学习表示的微调问题进行解决。

Result: 基于soft-DTW损失的速度扰动方法实现了更快的收敛速度和下游任务性能提升，证明位置不变的微调在基于自监督学习的语音模型中至关重要。

Conclusion: 位置不变的微调方法能够有效解决利用位置嵌入导致的自监督学习表示微调局限，促进了前端语音增强与自监督语音模型的融合性能。

Abstract: Integrating front-end speech enhancement (SE) models with self-supervised learning (SSL)-based speech models is effective for downstream tasks in noisy conditions. SE models are commonly fine-tuned using SSL representations with mean squared error (MSE) loss between enhanced and clean speech. However, MSE is prone to exploiting positional embeddings in SSL models, allowing the objective to be minimised through positional correlations instead of content-related information. This work frames the problem as a general limitation of self-supervised representation fine-tuning and investigates it through representation-guided SE. Two strategies are considered: (1) zero-padding, previously explored in SSL pre-training but here examined in the fine-tuning setting, and (2) speed perturbations with a soft-DTW loss. Experiments show that the soft-DTW-based approach achieves faster convergence and improved downstream performance, underscoring the importance of position-invariant fine-tuning in SSL-based speech modelling.

</details>


### [5] [ChunkWise LoRA: Adaptive Sequence Partitioning for Memory-Efficient Low-Rank Adaptation and Accelerated LLM Inference](https://arxiv.org/abs/2601.21109)
*Ketan Thakkar,Maitreyi Chatterjee,Ramasubramanian Balasubramanian,Achyuthan Jootoo,Rajendra Ugrani*

Main category: cs.CL

TL;DR: ChunkWise LoRA通过动态按token复杂度划分序列，并为每块分配低秩配置，显著降低延迟和内存占用，同时提升或保持性能，适用于大型语言模型的高效微调。


<details>
  <summary>Details</summary>
Motivation: 现有的LoRA方法对所有输入token采用统一的静态秩配置，忽略了token复杂性和计算需求的变化。

Method: 提出ChunkWise LoRA，通过动态划分可变长度的token块，根据token复杂性为每个块分配定制的低秩配置，采用运行时调度器估计token难度，使用阶梯机制选择每块的LoRA秩和缩放，加入边界安全合成模块和策略驱动的KV缓存策略。

Result: 在Wikitext-103和SQuAD等基准数据集上，ChunkWise LoRA相比基线LoRA实现了高达34%的延迟降低和38%的内存节省，同时保持或提升BLEU、EM和困惑度等任务性能指标。

Conclusion: 该方法兼容现有Transformer架构和推理框架，为参数高效的大型语言模型的实际部署提供了实用的解决方案。

Abstract: Recent advances in low-rank adaptation (LoRA) have enabled efficient fine-tuning of large language models (LLMs) with minimal additional parameters. However, existing LoRA methods apply static rank configurations uniformly across all input tokens, ignoring variation in token complexity and computational requirements. In this work, we propose ChunkWise LoRA, a dynamic and adaptive approach that partitions sequences into variable-length chunks based on token complexity and assigns each chunk a tailored low-rank configuration. Our system introduces a runtime scheduler that estimates token difficulty, performs adaptive chunking, and selects per-chunk LoRA rank and scaling using a rank-ladder mechanism. To preserve output consistency, we further introduce a boundary-safe composition module and integrate policy-driven KV-cache strategies. Experiments on benchmark datasets such as Wikitext-103 and SQuAD demonstrate that ChunkWise LoRA achieves up to 34\% lower latency and 38% memory reduction compared to baseline LoRA, while maintaining or improving task performance metrics like BLEU, EM, and perplexity. The proposed framework remains fully compatible with existing transformer architectures and inference frameworks, providing a practical solution for real-world deployment of parameter-efficient LLMs.

</details>


### [6] [Multi-task Code LLMs: Data Mix or Model Merge?](https://arxiv.org/abs/2601.21115)
*Mingzhi Zhu,Boris Sobolev,Rahul Krishna,Raju Pavuluri,Stacy Patterson,Michele Merler*

Main category: cs.CL

TL;DR: 本文比较了两种创建小型多任务代码大语言模型的方法：数据混合与模型合并，并发现模型合并在较大规模模型上表现最佳，能够保持或超越单任务模型性能，尤其适合资源受限场景。


<details>
  <summary>Details</summary>
Motivation: 推动小型、专门化代码大语言模型与前沿模型共存，寻找高效多任务学习策略，在性能、资源和成本之间取得平衡。

Method: 使用Qwen Coder和DeepSeek Coder两种模型，以及2B和7B两种参数规模，针对代码生成与代码总结任务进行微调；比较数据混合与模型合并两种多任务学习方法；引入权重分析技术探索任务对参数的影响。

Result: 模型合并在大规模模型中达到最佳整体性能，保留96%专门模型代码生成性能并保持总结能力，且合并模型甚至优于单任务微调模型。数据混合在小规模中表现更好。权重分析助理解任务间参数影响，支持高效任务融合。

Conclusion: 模型合并在大规模模型中表现优异，能有效整合多任务功能，保持高性能且适合资源有限的部署环境。数据混合在小规模模型中更优。

Abstract: Recent research advocates deploying smaller, specialized code LLMs in agentic frameworks alongside frontier models, sparking interest in efficient strategies for multi-task learning that balance performance, constraints, and costs. We compare two approaches for creating small, multi-task code LLMs: data mixing versus model merging. We conduct extensive experiments across two model families (Qwen Coder and DeepSeek Coder) at two scales (2B and 7B parameters), fine-tuning them for code generation and code summarization tasks. Our evaluation on HumanEval, MBPP, and CodeXGlue benchmarks reveals that model merging achieves the best overall performance at larger scale across model families, retaining 96% of specialized model performance on code generation tasks while maintaining summarization capabilities. Notably, merged models can even surpass individually fine-tuned models, with our best configuration of Qwen Coder 2.5 7B model achieving 92.7% Pass@1 on HumanEval compared to 90.9% for its task-specific fine-tuned equivalent. At a smaller scale we find instead data mixing to be a preferred strategy. We further introduce a weight analysis technique to understand how different tasks affect model parameters and their implications for merging strategies. The results suggest that careful merging and mixing strategies can effectively combine task-specific capabilities without significant performance degradation, making them ideal for resource-constrained deployment scenarios.

</details>


### [7] [Large Language Models Naively Recover Ethnicity from Individual Records](https://arxiv.org/abs/2601.21132)
*Noah Dasanaike*

Main category: cs.CL

TL;DR: 本文证明大语言模型无需额外训练数据即可通过名字准确推断族裔，准确率显著优于传统方法BISG，适用范围广且减少收入等偏误，具有多国多情境的广泛验证和实际应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现有的族裔推断方法(BISG)存在准确率低、易受收入偏见影响，并且仅限于美国，亟需一种无需额外训练数据但更准确、适用范围更广泛的推断方法。

Method: 通过使用佛罗里达州和北卡罗来纳州带有自报种族的选民分层样本，测试了包括Gemini 3 Flash、GPT-4o及开源模型DeepSeek v3.2和GLM-4.7在内的六个大语言模型，结合元数据（如政党登记）及启用扩展推理提升准确率，并在多国和多种族背景下进行了大规模验证。

Result: LLM基于名字的族裔分类最高达到84.7%准确率，超越BISG的68.2%，并且通过加入元数据可提升到86.7%，减少了BISG在人群收入偏差方面的错误分类。方法在黎巴嫩印度等多国多种族群体验证均展示了良好性能，大规模数据上细调的小型模型还能实现本地无成本部署。

Conclusion: 大语言模型(LLM)能够在没有额外训练数据的情况下，仅凭名字推断族裔，准确率超过传统的贝叶斯改进姓氏地理编码法(BISG)，适用性更广泛至美国以外地区和更具上下文相关性的分类。

Abstract: I demonstrate that large language models can infer ethnicity from names with accuracy exceeding that of Bayesian Improved Surname Geocoding (BISG) without additional training data, enabling inference outside the United States and to contextually appropriate classification categories. Using stratified samples from Florida and North Carolina voter files with self-reported race, LLM-based classification achieves up to 84.7% accuracy, outperforming BISG (68.2%) on balanced samples. I test six models including Gemini 3 Flash, GPT-4o, and open-source alternatives such as DeepSeek v3.2 and GLM-4.7. Enabling extended reasoning can improve accuracy by 1-3 percentage points, though effects vary across contexts; including metadata such as party registration reaches 86.7%. LLM classification also reduces the income bias inherent in BISG, where minorities in wealthier neighborhoods are systematically misclassified as White. I further validate using Lebanese voter registration with religious sect (64.3% accuracy), Indian MPs from reserved constituencies (99.2%), and Indian land records with caste classification (74.0%). Aggregate validation across India, Uganda, Nepal, Armenia, Chile, and Costa Rica using original full-count voter rolls demonstrates that the method recovers known population distributions where naming conventions are distinctive. For large-scale applications, small transformer models fine-tuned on LLM labels exceed BISG accuracy while enabling local deployment at no cost.

</details>


### [8] [Toward Culturally Aligned LLMs through Ontology-Guided Multi-Agent Reasoning](https://arxiv.org/abs/2601.21700)
*Wonduk Seo,Wonseok Choi,Junseo Koh,Juhyeon Lee,Hyunjin An,Minhyeong Yu,Jian Park,Qingshan Zhou,Seunghyun Lee,Yi Bu*

Main category: cs.CL

TL;DR: OG-MAR利用结构化文化本体和多代理机制，显著提升LLM文化对齐和推理透明度，克服现有方法的不足。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在支持文化敏感的决策时，存在预训练数据偏差和缺乏结构化价值表达的问题，导致输出与实际文化价值错位。现有方法缺乏人口统计学基础，且将价值看作独立、无结构的信号，降低了输出的一致性和可解释性。

Method: 提出OG-MAR框架，基于世界价值观调查的数据，构建全球文化本体，通过能力问题获取固定分类体系中的关系。在推理时，检索与本体一致的关系和人口统计学相似的档案，实例化多个价值人格代理，由判断代理合成输出，确保本体一致性和人口统计学接近性。

Result: 在四种大语言模型骨干上的地区社会调查基准测试中，OG-MAR在文化对齐和鲁棒性方面优于竞争基线，并生成更透明的推理轨迹。

Conclusion: 通过引入结构化的文化本体和多代理推理机制，OG-MAR有效提升了大语言模型在文化敏感决策中的对齐度、鲁棒性和可解释性。

Abstract: Large Language Models (LLMs) increasingly support culturally sensitive decision making, yet often exhibit misalignment due to skewed pretraining data and the absence of structured value representations. Existing methods can steer outputs, but often lack demographic grounding and treat values as independent, unstructured signals, reducing consistency and interpretability. We propose OG-MAR, an Ontology-Guided Multi-Agent Reasoning framework. OG-MAR summarizes respondent-specific values from the World Values Survey (WVS) and constructs a global cultural ontology by eliciting relations over a fixed taxonomy via competency questions. At inference time, it retrieves ontology-consistent relations and demographically similar profiles to instantiate multiple value-persona agents, whose outputs are synthesized by a judgment agent that enforces ontology consistency and demographic proximity. Experiments on regional social-survey benchmarks across four LLM backbones show that OG-MAR improves cultural alignment and robustness over competitive baselines, while producing more transparent reasoning traces.

</details>


### [9] [EnsembleLink: Accurate Record Linkage Without Training Data](https://arxiv.org/abs/2601.21138)
*Noah Dasanaike*

Main category: cs.CL

TL;DR: EnsembleLink利用预训练语言模型，无需标注实现高精度记录链接，性能超越传统方法，运行高效且本地完成。


<details>
  <summary>Details</summary>
Motivation: 现有记录链接方法准确率低或需大量标注数据，且未充分量化链接错误对后续分析的影响，急需一种高效且无需标注数据的高精度链接方法。

Method: 该方法依赖于预训练语言模型理解语义关系，构建集成学习框架来实现自动匹配，无需任何训练标签，且运行在开源模型上完成本地计算。

Result: EnsembleLink通过利用预训练语言模型，实现了无需标注数据即可高精度的记录链接，解决了传统方法准确率低或依赖大量标注数据的问题。该方法基于语言模型学到的语义关系，有效匹配城市名、人名、组织机构名、多语种政党及书目记录，性能优于多种需大量标注的现有方法。EnsembleLink本地运行，无需外部API，计算效率高，能在数分钟内完成链接任务。

Conclusion: EnsembleLink能够高效、准确地完成跨数据集的记录链接任务，且不依赖标注训练数据，具有很好的实际应用潜力。

Abstract: Record linkage, the process of matching records that refer to the same entity across datasets, is essential to empirical social science but remains methodologically underdeveloped. Researchers treat it as a preprocessing step, applying ad hoc rules without quantifying the uncertainty that linkage errors introduce into downstream analyses. Existing methods either achieve low accuracy or require substantial labeled training data. I present EnsembleLink, a method that achieves high accuracy without any training labels. EnsembleLink leverages pre-trained language models that have learned semantic relationships (e.g., that "South Ozone Park" is a neighborhood in "New York City" or that "Lutte ouvriere" refers to the Trotskyist "Workers' Struggle" party) from large text corpora. On benchmarks spanning city names, person names, organizations, multilingual political parties, and bibliographic records, EnsembleLink matches or exceeds methods requiring extensive labeling. The method runs locally on open-source models, requiring no external API calls, and completes typical linkage tasks in minutes.

</details>


### [10] [Output-Space Search: Targeting LLM Generations in a Frozen Encoder-Defined Output Space](https://arxiv.org/abs/2601.21169)
*Tobias Materzok*

Main category: cs.CL

TL;DR: 本文提出输出空间搜索（OS-Search）方法，将大语言模型生成转化为在冻结的编码器定义的3D输出空间中的端点搜索，通过序列级强化学习训练的检索基础策略，在目标点附近生成输出，实现并行扫描和黑箱优化。


<details>
  <summary>Details</summary>
Motivation: 传统LMM生成受限于顺序依赖的token生成路径，限制了多样性和优化能力，因此提出将生成转化为在连续输出空间的端点搜索以提升生成质量和效率。

Method: 该方法定义一个冻结编码器的3D输出空间，使用序列级强化学习训练的策略生成输出，使自动回归生成的坐标接近目标点z*，支持在输出空间内并行扫描和贝叶斯优化，无需路径依赖的token或程序搜索。

Result: 在故事生成任务中，OS-Search在编码空间扫描产生的文本多样性是传统prompt-chaining的3.1倍；在代码生成任务中，对编码空间进行贝叶斯优化，在相同推断预算下提升了控制器未触及的目标函数表现，并保持代码有效性。

Conclusion: OS-Search通过在编码空间内搜索目标点，显著提升了生成内容的多样性和代码质量，验证了该方法在故事文本和代码生成任务中的有效性。

Abstract: We introduce Output-Space Search (OS-Search), which turns LLM generation into endpoint search. An outer loop selects a target z* in a frozen encoder-defined 3D output space Z, and a retrieval-grounded policy trained with sequence-level RL generates outputs whose coordinates land near z* under standard autoregressive decoding. This enables parallel sweeps and black-box optimization in Z without path-dependent token/program search. On stories, sweeping Z (text) yields 3.1x higher LLM-scored diversity than prompt-chaining. On code, Bayesian optimization over Z (code) improves an objective withheld from the controller under matched inference budgets while preserving validity.

</details>


### [11] [From Linear Input to Hierarchical Structure: Function Words as Statistical Cues for Language Learning](https://arxiv.org/abs/2601.21191)
*Xiulin Yang,Heidi Getz,Ethan Gotlieb Wilcox*

Main category: cs.CL

TL;DR: 本文通过跨语言分析和神经模型实验，证明功能词的统计特性对层级结构习得的重要作用及其多样化的学习机制。


<details>
  <summary>Details</summary>
Motivation: 探讨支持从线性输入中学习层级结构的统计条件，特别是功能词在语言习得中的作用。

Method: 通过跨语言语料库分析确认功能词的高频率、与句法结构的可靠关联以及与短语边界的对齐三个属性在186种语言中普遍存在；结合反事实语言建模和消融实验，评估保留这些属性的语言变体对神经学习者的影响；随后进行探测和消融分析了解不同学习条件下对功能词依赖的差异。

Result: 保留功能词的三个统计属性的语言变体更易被神经网络学习者习得，其中频率和结构关联的作用强于边界对齐；不同学习条件导致模型内部机制对功能词的依赖存在系统差异。

Conclusion: 功能词的统计属性是神经网络学习层级语言结构的关键支持因素，且相似的学习性能可能源自不同的内部学习机制。

Abstract: What statistical conditions support learning hierarchical structure from linear input? In this paper, we address this question by focusing on the statistical distribution of function words. Function words have long been argued to play a crucial role in language acquisition due to their distinctive distributional properties, including high frequency, reliable association with syntactic structure, and alignment with phrase boundaries. We use cross-linguistic corpus analysis to first establish that all three properties are present across 186 studied languages. Next, we use a combination of counterfactual language modeling and ablation experiments to show that language variants preserving all three properties are more easily acquired by neural learners, with frequency and structural association contributing more strongly than boundary alignment. Follow-up probing and ablation analyses further reveal that different learning conditions lead to systematically different reliance on function words, indicating that similar performance can arise from distinct internal mechanisms.

</details>


### [12] [Scaling Embeddings Outperforms Scaling Experts in Language Models](https://arxiv.org/abs/2601.21204)
*Hong Liu,Jiaqi Zhang,Chao Wang,Xing Hu,Linkun Lyu,Jiaqi Sun,Xurui Yang,Bo Wang,Fengcun Li,Yulei Qian,Lingtong Si,Yerui Sun,Rumei Li,Peng Pei,Yuchen Xie,Xunliang Cai*

Main category: cs.CL

TL;DR: 该论文提出通过扩展嵌入层维度来实现稀疏性扩展，解决了MoE模型的系统瓶颈和效益递减问题，设计了具有优越性能和推理速度的68.5B参数模型LongCat-Flash-Lite，表现优于同规模MoE模型。


<details>
  <summary>Details</summary>
Motivation: 现有的Mixture-of-Experts架构虽然在稀疏性扩展中被广泛采用，但面临收益递减和系统瓶颈问题，激发探索其他维度如嵌入维度扩展的需求，以突破性能和效率的限制。

Method: 通过深入分析嵌入层扩展与专家扩展的效能差异，系统研究模型参数预算、宽度和深度的交互影响；结合系统级优化和推测解码技术，提升推理速度。基于这些方法设计并训练了68.5B参数的LongCat-Flash-Lite模型，从零开始训练，实现稀疏激活。

Result: LongCat-Flash-Lite模型携带超过30B的嵌入参数，约3B参数被实际激活，成功实现相较传统MoE模型在参数等效基础上的性能提升，并在代理和编程任务领域表现出强竞争力。此外，实现了显著的推理速度提升。

Conclusion: 通过扩展嵌入层维度实现稀疏性扩展，能够克服专家模型规模扩展的瓶颈，并提升整体性能和推理速度。提出的LongCat-Flash-Lite模型在参数数量和实际激活参数上均优于传统MoE基线，且在特定任务领域表现突出。

Abstract: While Mixture-of-Experts (MoE) architectures have become the standard for sparsity scaling in large language models, they increasingly face diminishing returns and system-level bottlenecks. In this work, we explore embedding scaling as a potent, orthogonal dimension for scaling sparsity. Through a comprehensive analysis and experiments, we identify specific regimes where embedding scaling achieves a superior Pareto frontier compared to expert scaling. We systematically characterize the critical architectural factors governing this efficacy -- ranging from parameter budgeting to the interplay with model width and depth. Moreover, by integrating tailored system optimizations and speculative decoding, we effectively convert this sparsity into tangible inference speedups. Guided by these insights, we introduce LongCat-Flash-Lite, a 68.5B parameter model with ~3B activated trained from scratch. Despite allocating over 30B parameters to embeddings, LongCat-Flash-Lite not only surpasses parameter-equivalent MoE baselines but also exhibits exceptional competitiveness against existing models of comparable scale, particularly in agentic and coding domains.

</details>


### [13] [Multilingual Dysarthric Speech Assessment Using Universal Phone Recognition and Language-Specific Phonemic Contrast Modeling](https://arxiv.org/abs/2601.21205)
*Eunjung Yeo,Julie M. Liss,Visar Berisha,David R. Mortensen*

Main category: cs.CL

TL;DR: 本文提出了一种多语言构音音素评估框架，通过结合通用识别与语言特定映射，产生多维度指标，实验证明其在多语种下准确反映了构音障碍相关的语音清晰度变化。


<details>
  <summary>Details</summary>
Motivation: 神经系统疾病导致的构音障碍普遍存在，现有自动化清晰度评估方法多限于单语种，难以兼顾语言特异性因素，为此需要一种适用于多语言的智能评估工具。

Method: 结合通用音素识别和基于对比音系特征距离的语言特定音素解读，通过音素映射与序列对齐生成三种评估指标：音素错误率(PER)、音系特征错误率(PFER)及新的无对齐指标——音素覆盖率(PhonCov)。

Result: 在英语、西班牙语、意大利语和泰米尔语上的分析显示，PER在映射和对齐结合下表现最佳，PFER主要受益于对齐，而PhonCov则主要依赖映射；整体框架有效反映了构音障碍语音的临床特征。

Conclusion: 提出的多语言音素产生评估框架能够有效捕捉由构音障碍引起的语音清晰度下降，体现出与临床已知现象一致的表现。

Abstract: The growing prevalence of neurological disorders associated with dysarthria motivates the need for automated intelligibility assessment methods that are applicalbe across languages. However, most existing approaches are either limited to a single language or fail to capture language-specific factors shaping intelligibility. We present a multilingual phoneme-production assessment framework that integrates universal phone recognition with language-specific phoneme interpretation using contrastive phonological feature distances for phone-to-phoneme mapping and sequence alignment. The framework yields three metrics: phoneme error rate (PER), phonological feature error rate (PFER), and a newly proposed alignment-free measure, phoneme coverage (PhonCov). Analysis on English, Spanish, Italian, and Tamil show that PER benefits from the combination of mapping and alignment, PFER from alignment alone, and PhonCov from mapping. Further analyses demonstrate that the proposed framework captures clinically meaningful patterns of intelligibility degradation consistent with established observations of dysarthric speech.

</details>


### [14] [Scaling Reasoning Hop Exposes Weaknesses: Demystifying and Improving Hop Generalization in Large Language Models](https://arxiv.org/abs/2601.21214)
*Zhaoyi Li,Jiatong Li,Gangwei Jiang,Linqi Song,Defu Lian,Ying Wei*

Main category: cs.CL

TL;DR: 本文发现大型语言模型推理错误主要由个别错误处理注意力头导致，提出了一种动态禁用这些错误头的测试时纠正方法，有效提升模型推理跳数泛化性能。


<details>
  <summary>Details</summary>
Motivation: 针对大型语言模型在推理跳数超出训练范围时性能急剧下降的现象，探究其内部失败机制，寻求提升模型推理泛化能力的解决方案。

Method: 提出了一种测试时纠正方法，在推理过程中动态检测并关闭特定的错误处理注意力头，从而纠正错误推理路径。

Result: 本文系统研究了大型语言模型在链式思维（CoT）推理中推理跳数超出训练分布时性能下降的原因，发现错误集中在少数关键位置的某些错误处理注意力头（ep heads）上，这些ep头放大错误推理路径并抑制正确路径。通过在推理过程中动态识别并禁用这些ep头，作者提出了一种轻量级的测试时纠正方法，显著提升了模型的推理跳数的泛化能力。

Conclusion: 通过识别并去除推理过程中的错误处理注意力头，可以显著恢复和提升大型语言模型的推理跳数泛化性能。

Abstract: Chain-of-thought (CoT) reasoning has become the standard paradigm for enabling Large Language Models (LLMs) to solve complex problems. However, recent studies reveal a sharp performance drop in reasoning hop generalization scenarios, where the required number of reasoning steps exceeds training distributions while the underlying algorithm remains unchanged. The internal mechanisms driving this failure remain poorly understood. In this work, we conduct a systematic study on tasks from multiple domains, and find that errors concentrate at token positions of a few critical error types, rather than being uniformly distributed. Closer inspection reveals that these token-level erroneous predictions stem from internal competition mechanisms: certain attention heads, termed erroneous processing heads (ep heads), tip the balance by amplifying incorrect reasoning trajectories while suppressing correct ones. Notably, removing individual ep heads during inference can often restore the correct predictions. Motivated by these insights, we propose test-time correction of reasoning, a lightweight intervention method that dynamically identifies and deactivates ep heads in the reasoning process. Extensive experiments across different tasks and LLMs show that it consistently improves reasoning hop generalization, highlighting both its effectiveness and potential.

</details>


### [15] [Parametric Knowledge is Not All You Need: Toward Honest Large Language Models via Retrieval of Pretraining Data](https://arxiv.org/abs/2601.21218)
*Christopher Adrian Kusuma,Muhammad Reza Qorib,Hwee Tou Ng*

Main category: cs.CL

TL;DR: 本论文针对大型语言模型知识边界模糊导致的虚假回答问题，提出了新的评价基准和利用预训练数据提升模型诚实度的方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽善于回答问题，但常因不清楚自身知识边界而产生虚假答案，亟需提高模型的诚实度，即在知识不足时诚实回答“不知道”。

Method: 提出了一个更稳健的评价基准数据集，使用公开预训练数据的Pythia模型，并提出了一种利用预训练数据提升语言模型诚实度的新方法。

Result: 构建了基于Pythia的评价基准，并通过新方法利用预训练数据显著提高了模型在不确定情境下的诚实回答率。

Conclusion: 通过利用公开预训练数据和新方法，能够更准确评估并提升语言模型在未知领域的诚实度，减少虚假信息生成。

Abstract: Large language models (LLMs) are highly capable of answering questions, but they are often unaware of their own knowledge boundary, i.e., knowing what they know and what they don't know. As a result, they can generate factually incorrect responses on topics they do not have enough knowledge of, commonly known as hallucination. Rather than hallucinating, a language model should be more honest and respond with "I don't know" when it does not have enough knowledge about a topic. Many methods have been proposed to improve LLM honesty, but their evaluations lack robustness, as they do not take into account the knowledge that the LLM has ingested during its pretraining. In this paper, we propose a more robust evaluation benchmark dataset for LLM honesty by utilizing Pythia, a truly open LLM with publicly available pretraining data. In addition, we also propose a novel method for harnessing the pretraining data to build a more honest LLM.

</details>


### [16] [MGSM-Pro: A Simple Strategy for Robust Multilingual Mathematical Reasoning Evaluation](https://arxiv.org/abs/2601.21225)
*Tianyi Xu,Kosei Uemura,Alfred Malengo Kondoro,Tadesse Destaw Belay,Catherine Nana Nyaah Essuman,Ifeoma Okoh,Ganiyat Afolabi,Ayodele Awokoya,David Ifeoluwa Adelani*

Main category: cs.CL

TL;DR: 本文通过引入MGSM-Pro数据集及多样化数字实例评测，揭示了多语言数学推理模型对数字变化的鲁棒性差异，建议至少使用五个数字变体进行更真实的模型评测。


<details>
  <summary>Details</summary>
Motivation: 现有数学推理多语言基准测试缺乏多样性和鲁棒性评测，且之前主要针对英文进行，存在评测偏差。

Method: 采用MGSM-Pro数据集，利用GSM-Symbolic方法生成每个问题的五个数字与上下文变体，进行多语言评测。

Result: 发现低资源语言在数字实例变化时表现大幅下降；部分专有模型鲁棒性较差，部分开源模型表现较好。

Conclusion: 多语言数学推理评测中，不同数字实例的变化对模型表现有显著影响，某些模型对数字变化更鲁棒。

Abstract: Large language models have made substantial progress in mathematical reasoning. However, benchmark development for multilingual evaluation has lagged behind English in both difficulty and recency. Recently, GSM-Symbolic showed a strong evidence of high variance when models are evaluated on different instantiations of the same question; however, the evaluation was conducted only in English. In this paper, we introduce MGSM-Pro, an extension of MGSM dataset with GSM-Symbolic approach. Our dataset provides five instantiations per MGSM question by varying names, digits and irrelevant context. Evaluations across nine languages reveal that many low-resource languages suffer large performance drops when tested on digit instantiations different from those in the original test set. We further find that some proprietary models, notably Gemini 2.5 Flash and GPT-4.1, are less robust to digit instantiation, whereas Claude 4.0 Sonnet is more robust. Among open models, GPT-OSS 120B and DeepSeek V3 show stronger robustness. Based on these findings, we recommend evaluating each problem using at least five digit-varying instantiations to obtain a more robust and realistic assessment of math reasoning.

</details>


### [17] [SHARP: Social Harm Analysis via Risk Profiles for Measuring Inequities in Large Language Models](https://arxiv.org/abs/2601.21235)
*Alok Abhishek,Tushar Bandopadhyay,Lisa Erickson*

Main category: cs.CL

TL;DR: 本文提出了多维且关注尾部风险的社会危害评估框架SHARP，揭示了大型语言模型在极端风险表现上的显著差异，强调了传统单值指标的不足，呼吁采用更细化的风险评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有评价指标通常简化复杂的社会风险为单一均值，忽视了风险在分布上的结构、多维度间的交互及极端最坏情况行为，难以准确反映大型语言模型在高风险领域的潜在社会危害。

Method: 提出SHARP框架，通过将社会危害建模为多元随机变量，整合偏见、公平、伦理和认知可靠性四个维度，并采用累积对数风险的加法累积形式进行风险聚合。使用条件风险价值（CVaR95）等风险敏感的分布统计方法，捕捉模型在极端情况下的表现。

Result: 对11个前沿大型语言模型在901个社会敏感提示下应用SHARP评估，发现尽管平均风险相似，但尾部风险暴露和波动性存在超过两倍的差异。不同维度的尾部风险表现也存在系统性差异，其中偏见维度的尾部风险最严重，伦理不一致性较低。

Conclusion: 对于大型语言模型的责任评估和治理，需摒弃单一均值指标，采用多维且敏感尾部风险的风险画像方法，以更准确地识别和管理社会危害。

Abstract: Large language models (LLMs) are increasingly deployed in high-stakes domains, where rare but severe failures can result in irreversible harm. However, prevailing evaluation benchmarks often reduce complex social risk to mean-centered scalar scores, thereby obscuring distributional structure, cross-dimensional interactions, and worst-case behavior. This paper introduces Social Harm Analysis via Risk Profiles (SHARP), a framework for multidimensional, distribution-aware evaluation of social harm. SHARP models harm as a multivariate random variable and integrates explicit decomposition into bias, fairness, ethics, and epistemic reliability with a union-of-failures aggregation reparameterized as additive cumulative log-risk. The framework further employs risk-sensitive distributional statistics, with Conditional Value at Risk (CVaR95) as a primary metric, to characterize worst-case model behavior. Application of SHARP to eleven frontier LLMs, evaluated on a fixed corpus of n=901 socially sensitive prompts, reveals that models with similar average risk can exhibit more than twofold differences in tail exposure and volatility. Across models, dimension-wise marginal tail behavior varies systematically across harm dimensions, with bias exhibiting the strongest tail severities, epistemic and fairness risks occupying intermediate regimes, and ethical misalignment consistently lower; together, these patterns reveal heterogeneous, model-dependent failure structures that scalar benchmarks conflate. These findings indicate that responsible evaluation and governance of LLMs require moving beyond scalar averages toward multidimensional, tail-sensitive risk profiling.

</details>


### [18] [MoCo: A One-Stop Shop for Model Collaboration Research](https://arxiv.org/abs/2601.21257)
*Shangbin Feng,Yuyang Bai,Ziyuan Yang,Yike Wang,Zhaoxuan Tan,Jiajie Yan,Zhenyu Lei,Wenxuan Ding,Weijia Shi,Haojin Wang,Zhenting Qi,Yuru Jiang,Heng Wang,Chengsong Huang,Yu Fei,Jihan Yao,Yilun Du,Luke Zettlemoyer,Yejin Choi,Yulia Tsvetkov*

Main category: cs.CL

TL;DR: 本文开发了MoCo库，系统整合多种模型协作方法与评测，证明了协作策略显著提升语言模型性能，推动协作AI的发展。


<details>
  <summary>Details</summary>
Motivation: 当前模型协作研究分散且缺乏系统对比，需建立统一平台促进协作算法的评测和发展。

Method: 提出MoCo库，实现了26种模型协作方法，覆盖不同层面的信息交互，并集成了25个评测数据集进行大规模实验和比较。

Result: 通过MoCo的广泛实验，验证了大多数模型协作策略优于单模型，揭示了协作策略的扩展性和效率优势，并指出协作系统能够解决单模型难题。

Conclusion: 模型协作策略普遍优于单一模型，在多种任务和数据集上表现更佳，最有效方法性能提升高达25.8%。

Abstract: Advancing beyond single monolithic language models (LMs), recent research increasingly recognizes the importance of model collaboration, where multiple LMs collaborate, compose, and complement each other. Existing research on this topic has mostly been disparate and disconnected, from different research communities, and lacks rigorous comparison. To consolidate existing research and establish model collaboration as a school of thought, we present MoCo: a one-stop Python library of executing, benchmarking, and comparing model collaboration algorithms at scale. MoCo features 26 model collaboration methods, spanning diverse levels of cross-model information exchange such as routing, text, logit, and model parameters. MoCo integrates 25 evaluation datasets spanning reasoning, QA, code, safety, and more, while users could flexibly bring their own data. Extensive experiments with MoCo demonstrate that most collaboration strategies outperform models without collaboration in 61.0% of (model, data) settings on average, with the most effective methods outperforming by up to 25.8%. We further analyze the scaling of model collaboration strategies, the training/inference efficiency of diverse methods, highlight that the collaborative system solves problems where single LMs struggle, and discuss future work in model collaboration, all made possible by MoCo. We envision MoCo as a valuable toolkit to facilitate and turbocharge the quest for an open, modular, decentralized, and collaborative AI future.

</details>


### [19] [CausalEmbed: Auto-Regressive Multi-Vector Generation in Latent Space for Visual Document Embedding](https://arxiv.org/abs/2601.21262)
*Jiahao Huo,Yu Huang,Yibo Yan,Ye Pan,Yi Cao,Mingdong Ou,Philip S. Yu,Xuming Hu*

Main category: cs.CL

TL;DR: CausalEmbed通过自回归方法生成紧凑多向量嵌入，大幅减少视觉token数量，提升视觉文档检索效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大型语言模型在视觉文档检索中因大量视觉token导致的存储开销大，限制实际应用的问题。

Method: 采用自回归生成方法结合迭代边缘损失进行对比训练，促进嵌入模型学习紧凑且结构良好的多向量表示。

Result: 本文提出了一种名为CausalEmbed的自回归生成方法，用于构建多向量嵌入，以解决多模态大型语言模型在视觉文档检索中因使用大量视觉token导致存储开销大的问题。通过引入迭代边缘损失进行对比训练，CausalEmbed学习到紧凑且结构良好的表示，使得只需使用几十个视觉token即可实现30-155倍的token数量减少，同时保持竞争力的性能。理论分析和实验证明了自回归嵌入生成在训练效率和测试时可扩展性方面的优势，提出了灵活的测试时缩放策略，并为多模态文档检索中的生成范式提供了新的视角。

Conclusion: CausalEmbed有效解决了视觉文档检索中多视觉token导致的存储开销问题，实现了高效且性能优异的多向量嵌入生成，推动了多模态文档检索中生成范式的发展。

Abstract: Although Multimodal Large Language Models (MLLMs) have shown remarkable potential in Visual Document Retrieval (VDR) through generating high-quality multi-vector embeddings, the substantial storage overhead caused by representing a page with thousands of visual tokens limits their practicality in real-world applications. To address this challenge, we propose an auto-regressive generation approach, CausalEmbed, for constructing multi-vector embeddings. By incorporating iterative margin loss during contrastive training, CausalEmbed encourages the embedding models to learn compact and well-structured representations. Our method enables efficient VDR tasks using only dozens of visual tokens, achieving a 30-155x reduction in token count while maintaining highly competitive performance across various backbones and benchmarks. Theoretical analysis and empirical results demonstrate the unique advantages of auto-regressive embedding generation in terms of training efficiency and scalability at test time. As a result, CausalEmbed introduces a flexible test-time scaling strategy for multi-vector VDR representations and sheds light on the generative paradigm within multimodal document retrieval.

</details>


### [20] [Qwen3-ASR Technical Report](https://arxiv.org/abs/2601.21337)
*Xian Shi,Xiong Wang,Zhifang Guo,Yongqi Wang,Pei Zhang,Xinyu Zhang,Zishan Guo,Hongkun Hao,Yu Xi,Baosong Yang,Jin Xu,Jingren Zhou,Junyang Lin*

Main category: cs.CL

TL;DR: 本文介绍了Qwen3-ASR系列，包括两个强大的全能语音识别模型和一个新颖的非自回归语音强制对齐模型，支持52种语言及方言，性能领先且效率优越。


<details>
  <summary>Details</summary>
Motivation: 当前开源语音识别模型在实际应用中表现存在显著差异，迫切需要高性能、高效率且支持多语言的全能语音识别及对齐模型以提升实际语音理解能力。

Method: 采用基于规模化语音训练数据和强大基础模型Qwen3-Omni的全自动语音识别方法；Qwen3-ForcedAligner使用基于大语言模型的非自回归时间戳预测技术进行多语言文本语音对齐。

Result: Qwen3-ASR-1.7B达成开源SOTA水平并接近顶尖专有API性能；0.6B版本实现最低92ms的平均响应时间，支持高并发快速转录；Qwen3-ForcedAligner在11种语言的时间戳预测上超过领先模型，且效率与适用性优异；模型已开放源代码以促进社区研究。

Conclusion: Qwen3-ASR-1.7B在开源语音识别模型中表现最优，与顶级专有API媲美；0.6B版本在准确率和效率之间达到了最佳平衡；Qwen3-ForcedAligner-0.6B在时间戳精度上优于现有强制对齐模型，且效率与通用性更强。

Abstract: In this report, we introduce Qwen3-ASR family, which includes two powerful all-in-one speech recognition models and a novel non-autoregressive speech forced alignment model. Qwen3-ASR-1.7B and Qwen3-ASR-0.6B are ASR models that support language identification and ASR for 52 languages and dialects. Both of them leverage large-scale speech training data and the strong audio understanding ability of their foundation model Qwen3-Omni. We conduct comprehensive internal evaluation besides the open-sourced benchmarks as ASR models might differ little on open-sourced benchmark scores but exhibit significant quality differences in real-world scenarios. The experiments reveal that the 1.7B version achieves SOTA performance among open-sourced ASR models and is competitive with the strongest proprietary APIs while the 0.6B version offers the best accuracy-efficiency trade-off. Qwen3-ASR-0.6B can achieve an average TTFT as low as 92ms and transcribe 2000 seconds speech in 1 second at a concurrency of 128. Qwen3-ForcedAligner-0.6B is an LLM based NAR timestamp predictor that is able to align text-speech pairs in 11 languages. Timestamp accuracy experiments show that the proposed model outperforms the three strongest force alignment models and takes more advantages in efficiency and versatility. To further accelerate the community research of ASR and audio understanding, we release these models under the Apache 2.0 license.

</details>


### [21] [Self-Improving Pretraining: using post-trained models to pretrain better models](https://arxiv.org/abs/2601.21343)
*Ellen Xiaoqing Tan,Shehzaad Dhuliawala,Jing Xu,Ping Yu,Sainbayar Sukhbaatar,Jason Weston,Olga Golovneva*

Main category: cs.CL

TL;DR: 提出一种结合强化学习的预训练方法，有效提升大语言模型的安全性和事实性，大幅改善生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在安全性和事实性方面存在挑战，且现有的精调和对齐方法无法根本纠正预训练期间学到的不良模式，因此需在预训练阶段解决这些问题。

Method: 引入一种新的预训练方法，通过文档流处理及强化学习，利用一个经过后期训练的强大模型对模型生成的候选内容进行质量、安全和事实性评判，并根据反馈优化生成。

Result: 实验表明，该方法在事实性和安全性方面分别带来36.2%和18.5%的相对提升，在整体生成质量上赢率提升高达86.3%。

Conclusion: 该论文提出的预训练方法通过强化学习显著提升了大语言模型的安全性、事实性和整体生成质量。

Abstract: Ensuring safety, factuality and overall quality in the generations of large language models is a critical challenge, especially as these models are increasingly deployed in real-world applications. The prevailing approach to addressing these issues involves collecting expensive, carefully curated datasets and applying multiple stages of fine-tuning and alignment. However, even this complex pipeline cannot guarantee the correction of patterns learned during pretraining. Therefore, addressing these issues during pretraining is crucial, as it shapes a model's core behaviors and prevents unsafe or hallucinated outputs from becoming deeply embedded. To tackle this issue, we introduce a new pretraining method that streams documents and uses reinforcement learning (RL) to improve the next K generated tokens at each step. A strong, post-trained model judges candidate generations -- including model rollouts, the original suffix, and a rewritten suffix -- for quality, safety, and factuality. Early in training, the process relies on the original and rewritten suffixes; as the model improves, RL rewards high-quality rollouts. This approach builds higher quality, safer, and more factual models from the ground up. In experiments, our method gives 36.2% and 18.5% relative improvements over standard pretraining in terms of factuality and safety, and up to 86.3% win rate improvements in overall generation quality.

</details>


### [22] [The Compliance Paradox: Semantic-Instruction Decoupling in Automated Academic Code Evaluation](https://arxiv.org/abs/2601.21360)
*Devanshu Sahoo,Manish Prasad,Vasudev Majhi,Arjun Neekhra,Yash Sinha,Murari Mandal,Vinay Chamola,Dhruv Kumar*

Main category: cs.CL

TL;DR: 提出对抗代码注入方法揭示大型语言模型自动代码评测的“遵从悖论”，表明当前模型容易被隐藏指令操控导致错误评价，建议转向注重证据优先的判定鲁棒性方法。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在自动评测中假设其遵循指令的能力能直接转化为客观判断能力，但实际中模型常因隐藏指令偏离代码逻辑，导致评测结果失真。

Method: 提出了Semantic-Preserving Adversarial Code Injection (SPACI)框架和Abstract Syntax Tree-Aware Semantic Injection Protocol (AST-ASIP)方法，通过在抽象语法树的语法惰性区域（trivia节点）嵌入对抗性指令，暴露模型的‘遵从悖论’漏洞。

Result: 在对9个最先进模型、25,000个Python、C、C++和Java代码提交的评测中，发现高容量开源模型如DeepSeek-V3在格式隐藏指令优先于代码正确性的情况下出现超过95%的严重失败率；通过三联框架（脱钩概率、评分差异、教学严重度）量化这种‘虚假认证’现象。

Conclusion: 当前基于强化学习的人类反馈（RLHF）对齐策略存在根本缺陷，易产生‘特洛伊’式漏洞，自动代码评测应从简单的指令遵守转向领域特定的判定鲁棒性训练，以确保评测结果的客观性和准确性。

Abstract: The rapid integration of Large Language Models (LLMs) into educational assessment rests on the unverified assumption that instruction following capability translates directly to objective adjudication. We demonstrate that this assumption is fundamentally flawed. Instead of evaluating code quality, models frequently decouple from the submission's logic to satisfy hidden directives, a systemic vulnerability we term the Compliance Paradox, where models fine-tuned for extreme helpfulness are vulnerable to adversarial manipulation. To expose this, we introduce the Semantic-Preserving Adversarial Code Injection (SPACI) Framework and the Abstract Syntax Tree-Aware Semantic Injection Protocol (AST-ASIP). These methods exploit the Syntax-Semantics Gap by embedding adversarial directives into syntactically inert regions (trivia nodes) of the Abstract Syntax Tree. Through a large-scale evaluation of 9 SOTA models across 25,000 submissions in Python, C, C++, and Java, we reveal catastrophic failure rates (>95%) in high-capacity open-weights models like DeepSeek-V3, which systematically prioritize hidden formatting constraints over code correctness. We quantify this failure using our novel tripartite framework measuring Decoupling Probability, Score Divergence, and Pedagogical Severity to demonstrate the widespread "False Certification" of functionally broken code. Our findings suggest that current alignment paradigms create a "Trojan" vulnerability in automated grading, necessitating a shift from standard RLHF toward domain-specific Adjudicative Robustness, where models are conditioned to prioritize evidence over instruction compliance. We release our complete dataset and injection framework to facilitate further research on the topic.

</details>


### [23] [User-Centric Evidence Ranking for Attribution and Fact Verification](https://arxiv.org/abs/2601.21387)
*Guy Alt,Eran Hirsch,Serwar Basch,Ido Dagan,Oren Glickman*

Main category: cs.CL

TL;DR: 本文提出了证据排序这一新任务，通过优先呈现充分且紧凑的信息来提升事实验证的效率和准确性。通过比较一次性排序和增量排序方法，并设计了新的评估框架，实验证明增量排序在捕捉互补证据方面表现更优，且基于大语言模型的方法效果更好。


<details>
  <summary>Details</summary>
Motivation: 现有自动化系统或大语言模型在事实验证中往往呈现的信息不足或冗余，造成效率低下且易出错，亟需提升信息呈现的充分性与紧凑性。

Method: 提出证据排序任务，比较一次性排序和增量排序两种方法，设计基于信息检索指标的评估框架，并构建统一基准数据集，采用多模型进行广泛实验。

Result: 实验结果显示，增量排序方法能更好地捕捉互补证据，且大语言模型方法优于传统基线。在用户研究中，证据排序显著减少了阅读负担并提升了验证效果。

Conclusion: 证据排序能够减少用户阅读负担，提高验证效率和准确性，特别是增量排序策略在综合证据方面具有优势，大语言模型在此任务上表现优越，但仍需解决信息充分性与冗余的平衡问题。

Abstract: Attribution and fact verification are critical challenges in natural language processing for assessing information reliability. While automated systems and Large Language Models (LLMs) aim to retrieve and select concise evidence to support or refute claims, they often present users with either insufficient or overly redundant information, leading to inefficient and error-prone verification. To address this, we propose Evidence Ranking, a novel task that prioritizes presenting sufficient information as early as possible in a ranked list. This minimizes user reading effort while still making all available evidence accessible for sequential verification. We compare two approaches for the new ranking task: one-shot ranking and incremental ranking. We introduce a new evaluation framework, inspired by information retrieval metrics, and construct a unified benchmark by aggregating existing fact verification datasets. Extensive experiments with diverse models show that incremental ranking strategies better capture complementary evidence and that LLM-based methods outperform shallower baselines, while still facing challenges in balancing sufficiency and redundancy. Compared to evidence selection, we conduct a controlled user study and demonstrate that evidence ranking both reduces reading effort and improves verification. This work provides a foundational step toward more interpretable, efficient, and user-aligned information verification systems.

</details>


### [24] [Conversation for Non-verifiable Learning: Self-Evolving LLMs through Meta-Evaluation](https://arxiv.org/abs/2601.21464)
*Yuan Sui,Bryan Hooi*

Main category: cs.CL

TL;DR: 本文提出了CoNL框架，通过多智能体自我对弈实现生成、评估和元评估的统一，以克服大型语言模型评估者质量限制的问题。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在创意写作、对话和伦理推理等非可验证任务上训练困难，且评估性能受限于评估者质量，导致评估偏差和训练信号不佳，亟需提升评估者自身能力的元评估方法。

Method: 采用多智能体共享策略，在结构化对话中提出、批评和修订解决方案，通过诊断性奖励激励有助于改进方案的批评，从而实现元评估和联合优化，无需外部评估或真实标签。

Result: 在五个基准测试中，CoNL在保证训练稳定性的同时，较自我奖励基线实现了持续且显著的性能提升。

Conclusion: CoNL框架通过结构化对话和诊断奖励机制，使评估者自身能力得到提升，从而促进模型生成和评估能力的联合优化，在多个基准测试中表现出稳定且优于自我奖励基线的效果。

Abstract: Training large language models (LLMs) for non-verifiable tasks, such as creative writing, dialogue, and ethical reasoning, remains challenging due to the absence of ground-truth labels. While LLM-as-Judge approaches offer a scalable alternative to human feedback, they face a fundamental limitation: performance is constrained by the evaluator's own quality. If the judge cannot recognize good solutions, it cannot provide useful training signals, and evaluation biases (e.g., favoring verbosity over quality) remain unaddressed. This motivates meta-evaluation: the ability to evaluate and improve the evaluator itself. We introduce CoNL, a framework that unifies generation, evaluation, and meta-evaluation through multi-agent self-play. Our key insight: critique quality can be measured by whether it helps others improve their solutions. In CoNL, multiple agents sharing the same policy engage in structured conversations to propose, critique, and revise solutions. Critiques that enable solution improvements earn a diagnostic reward, creating explicit supervision for meta-evaluation and enabling joint optimization of generation and judging capabilities through self-play, without external judges or ground truth. Experiments on five benchmarks show that CoNL achieves consistent improvements over self-rewarding baselines while maintaining stable training.

</details>


### [25] [SOUP: Token-level Single-sample Mix-policy Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2601.21476)
*Lei Yang,Wei Bi,Chenxi Sun,Renren Jin,Deyi Xiong*

Main category: cs.CL

TL;DR: 本文提出了SOUP框架，通过在单个样本中融合离线和在线策略，在Token级别实现更高效的强化学习训练，提升探索能力和性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于策略的在线强化学习在语言模型微调时采样多样性不足导致探索有限和早期饱和，而简单混合离线轨迹数据则引入了显著的策略错配和不稳定性问题。

Method: SOUP框架将离线和在线学习统一到单个样本的Token级别，限制离线部分在生成序列的前缀，剩余部分在线生成。通过Token级重要性比率权衡离线信息与训练稳定性的关系。

Result: 实验表明SOUP稳定优于标准在线训练及其他离线扩展方法，能够改善探索及提升大语言模型强化学习的最终性能。

Conclusion: SOUP通过细粒度的单样本混合策略训练，有效利用离线数据同时保持训练稳定性，显著优于传统的在线强化学习和现有的离线扩展方法，提升了大语言模型的探索能力和最终性能。

Abstract: On-policy reinforcement learning (RL) methods widely used for language model post-training, like Group Relative Policy Optimization (GRPO), often suffer from limited exploration and early saturation due to low sampling diversity. While off-policy data can help, current approaches that mix entire trajectories cause significant policy mismatch and instability. In this work, we propose the $\textbf{S}$ingle-sample Mix-p$\textbf{O}$licy $\textbf{U}$nified $\textbf{P}$aradigm (SOUP), a framework that unifies off- and on-policy learning within individual samples at the token level. It confines off-policy influence to the prefix of a generated sequence sampled from historical policies, while the continuation is generated on-policy. Through token-level importance ratios, SOUP effectively leverages off-policy information while preserving training stability. Extensive experiments demonstrate that SOUP consistently outperforms standard on-policy training and existing off-policy extensions. Our further analysis clarifies how our fine-grained, single-sample mix-policy training can improve both exploration and final performance in LLM RL.

</details>


### [26] [DimStance: Multilingual Datasets for Dimensional Stance Analysis](https://arxiv.org/abs/2601.21483)
*Jonas Becker,Liang-Chih Yu,Shamsuddeen Hassan Muhammad,Jan Philip Wahle,Terry Ruas,Idris Abdulmumin,Lung-Hao Lee,Wen-Ni Liu,Tzu-Mi Lin,Zhe-Yu Xu,Ying-Lung Lin,Jin Wang,Maryam Ibrahim Mukhtar,Bela Gipp,Saif M. Mohammed*

Main category: cs.CL

TL;DR: 本文提出维度化的立场检测方法及资源DimStance，通过情绪维度的连续表征实现细腻的多语言立场分析，并展示了大型语言模型的性能及存在的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统立场检测多为分类任务，无法捕捉立场背后的细腻情感状态，亟需更精细的情感维度来提升立场分析的精度和细腻度。

Method: 引入了一种基于情感科学框架的维度化立场检测方法，利用情绪的价性（负-正）和激活性（平静-活跃）两个连续变量对立场进行建模。开发了一个包含多语言、多领域的维度化立场资源DimStance，并在该资源上构建了维度立场回归任务，评估预训练模型和大型语言模型的表现。

Result: 构建了包含11,746个目标方面、7,365篇文本，覆盖五种语言和两个领域的维度立场资源DimStance；发现微调的大型语言模型在回归任务上表现优异，但在低资源语言上依然存在挑战，基于token的生成方法也存在局限。

Conclusion: DimStance构建了一个多语言、多领域的情绪维度立场资源，为情感感知的立场检测和基准测试提供了基础，同时表明低资源语言和生成式方法仍有改进空间。

Abstract: Stance detection is an established task that classifies an author's attitude toward a specific target into categories such as Favor, Neutral, and Against. Beyond categorical stance labels, we leverage a long-established affective science framework to model stance along real-valued dimensions of valence (negative-positive) and arousal (calm-active). This dimensional approach captures nuanced affective states underlying stance expressions, enabling fine-grained stance analysis. To this end, we introduce DimStance, the first dimensional stance resource with valence-arousal (VA) annotations. This resource comprises 11,746 target aspects in 7,365 texts across five languages (English, German, Chinese, Nigerian Pidgin, and Swahili) and two domains (politics and environmental protection). To facilitate the evaluation of stance VA prediction, we formulate the dimensional stance regression task, analyze cross-lingual VA patterns, and benchmark pretrained and large language models under regression and prompting settings. Results show competitive performance of fine-tuned LLM regressors, persistent challenges in low-resource languages, and limitations of token-based generation. DimStance provides a foundation for multilingual, emotion-aware, stance analysis and benchmarking.

</details>


### [27] [MURAD: A Large-Scale Multi-Domain Unified Reverse Arabic Dictionary Dataset](https://arxiv.org/abs/2601.21512)
*Serry Sibaee,Yasser Alhabashi,Nadia Sibai,Yara Farouk,Adel Ammar,Sawsan AlHalawani,Wadii Boulila*

Main category: cs.CL

TL;DR: 发布了包含96243条数据的多领域阿拉伯语词典，促进阿拉伯语语义处理和相关应用研究。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语词汇丰富但缺乏大规模准确的词义链接数据集，限制了自然语言处理的发展。

Method: 建立了多领域统一的反向阿拉伯语词典MURAD，包含96243个词-定义对。采用直接文本解析、光学字符识别和自动重构的混合处理管道。

Result: 构建了覆盖语言学、伊斯兰研究、数学、物理、心理学和工程领域的权威词典数据，且附带词汇和来源元数据。

Conclusion: 该数据集促进阿拉伯语自然语言处理技术进步和词汇语义研究，支持反向词典建模、语义检索及教育应用。

Abstract: Arabic is a linguistically and culturally rich language with a vast vocabulary that spans scientific, religious, and literary domains. Yet, large-scale lexical datasets linking Arabic words to precise definitions remain limited. We present MURAD (Multi-domain Unified Reverse Arabic Dictionary), an open lexical dataset with 96,243 word-definition pairs. The data come from trusted reference works and educational sources. Extraction used a hybrid pipeline integrating direct text parsing, optical character recognition, and automated reconstruction. This ensures accuracy and clarity. Each record aligns a target word with its standardized Arabic definition and metadata that identifies the source domain. The dataset covers terms from linguistics, Islamic studies, mathematics, physics, psychology, and engineering. It supports computational linguistics and lexicographic research. Applications include reverse dictionary modeling, semantic retrieval, and educational tools. By releasing this resource, we aim to advance Arabic natural language processing and promote reproducible research on Arabic lexical semantics.

</details>


### [28] [LMK > CLS: Landmark Pooling for Dense Embeddings](https://arxiv.org/abs/2601.21525)
*Meet Doshi,Aashka Trivedi,Vishwajeet Kumar,Parul Awasthy,Yulong Li,Jaydeep Sen,Radu Florian,Sachindra Joshi*

Main category: cs.CL

TL;DR: LMK pooling通过分块插入地标标记实现更优序列表示，兼顾短长上下文应用。


<details>
  <summary>Details</summary>
Motivation: 现有的CLS标记聚合倾向于集中信息在序列初始位置，均值池化则可能削弱关键局部特征，导致短上下文表现下降，亟需一种兼顾信息集中与局部信号的方法。

Method: 该方法将序列划分为若干块，在块之间插入地标标记，并利用这些标记的嵌入均值池化作为序列的最终表示。

Result: 本文提出了一种新的序列聚合方法Landmark (LMK) pooling，通过在序列中分块并插入地标标记，利用这些标记的均值池化来形成最终表示，有效解决了当前CLS标记和均值池化方法在信息集中和局部特征表达上的不足。实验证明，LMK pooling在短上下文检索任务上表现持平，在长上下文任务上显著提升性能。

Conclusion: LMK pooling在保持短上下文性能的同时，显著提升了长上下文任务的表现，是一种实用且可扩展的序列池化替代方案。

Abstract: Representation learning is central to many downstream tasks such as search, clustering, classification, and reranking. State-of-the-art sequence encoders typically collapse a variable-length token sequence to a single vector using a pooling operator, most commonly a special [CLS] token or mean pooling over token embeddings. In this paper, we identify systematic weaknesses of these pooling strategies: [CLS] tends to concentrate information toward the initial positions of the sequence and can under-represent distributed evidence, while mean pooling can dilute salient local signals, sometimes leading to worse short-context performance. To address these issues, we introduce Landmark (LMK) pooling, which partitions a sequence into chunks, inserts landmark tokens between chunks, and forms the final representation by mean-pooling the landmark token embeddings. This simple mechanism improves long-context extrapolation without sacrificing local salient features, at the cost of introducing a small number of special tokens. We empirically demonstrate that LMK pooling matches existing methods on short-context retrieval tasks and yields substantial improvements on long-context tasks, making it a practical and scalable alternative to existing pooling methods.

</details>


### [29] [inversedMixup: Data Augmentation via Inverting Mixed Embeddings](https://arxiv.org/abs/2601.21543)
*Fanshuang Kong,Richong Zhang,Qiyu Sun,Zhijie Nie,Ting Deng,Chunming Hu*

Main category: cs.CL

TL;DR: 提出inversedMixup，结合Mixup的可控性和LLM生成的可解释性，通过对齐嵌入空间，实现混合样本的可控文本增强，显著提升性能并缓解流形侵入问题。


<details>
  <summary>Details</summary>
Motivation: 现有Mixup方法在潜在嵌入层进行线性插值生成样本，缺乏人类可解释性；而基于LLM的增强方法虽可生成可读文本，但对生成过程控制有限。

Method: 采用三阶段训练程序，将任务特定模型的输出嵌入空间与大语言模型（LLM）的输入嵌入空间对齐，实现混合嵌入的可控重构为人类可解读的句子。

Result: 提出了inversedMixup框架，实现了可控且具有可解释性的文本增强，首次实证了文本Mixup中的流形侵入现象，并提出有效缓解策略。大量实验验证了方法在少量样本和全监督场景下的有效性和泛化能力。

Conclusion: inversedMixup有效融合了Mixup与LLM技术，实现了可控且可解释的文本数据增强，提升了模型性能并解决了流形侵入问题，展现出良好的泛化性。

Abstract: Mixup generates augmented samples by linearly interpolating inputs and labels with a controllable ratio. However, since it operates in the latent embedding level, the resulting samples are not human-interpretable. In contrast, LLM-based augmentation methods produce sentences via prompts at the token level, yielding readable outputs but offering limited control over the generation process. Inspired by recent advances in LLM inversion, which reconstructs natural language from embeddings and helps bridge the gap between latent embedding space and discrete token space, we propose inversedMixup, a unified framework that combines the controllability of Mixup with the interpretability of LLM-based generation. Specifically, inversedMixup adopts a three-stage training procedure to align the output embedding space of a task-specific model with the input embedding space of an LLM. Upon successful alignment, inversedMixup can reconstruct mixed embeddings with a controllable mixing ratio into human-interpretable augmented sentences, thereby improving the augmentation performance. Additionally, inversedMixup provides the first empirical evidence of the manifold intrusion phenomenon in text Mixup and introduces a simple yet effective strategy to mitigate it. Extensive experiments demonstrate the effectiveness and generalizability of our approach in both few-shot and fully supervised scenarios.

</details>


### [30] [Note2Chat: Improving LLMs for Multi-Turn Clinical History Taking Using Medical Notes](https://arxiv.org/abs/2601.21551)
*Yang Zhou,Zhenting Sheng,Mingrui Tan,Yuting Song,Jun Zhou,Yu Heng Kwan,Lian Leng Low,Yang Bai,Yong Liu*

Main category: cs.CL

TL;DR: 提出了一种基于医疗笔记训练大语言模型进行结构化病史采集和诊断的框架，通过决策树生成对话并三阶段微调，显著提升了模型在动态诊断场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 临床病史采集作为临床推理的基础但尚未充分研究的环节，现有大语言模型在动态多轮诊断情境中表现不足，亟需一种方法提升其在迭代提问和假设调整中的能力。

Method: 通过将真实医疗笔记转化为医生-患者对话，利用决策树引导生成和精炼，结合三阶段微调策略（监督学习、模拟数据增强和偏好学习），以及单轮推理范式实现模型训练与优化。

Result: 实验表明，该方法相比GPT-4o，在F1分数上提升了16.9，Top-1诊断准确率提升了21.0，显著增强了临床推理能力。

Conclusion: 本论文提出的Note2Chat框架显著提升了大语言模型在临床病史采集和诊断上的性能，在多轮动态推理情境中表现出更高的准确率和可解释性。

Abstract: Effective clinical history taking is a foundational yet underexplored component of clinical reasoning. While large language models (LLMs) have shown promise on static benchmarks, they often fall short in dynamic, multi-turn diagnostic settings that require iterative questioning and hypothesis refinement. To address this gap, we propose \method{}, a note-driven framework that trains LLMs to conduct structured history taking and diagnosis by learning from widely available medical notes. Instead of relying on scarce and sensitive dialogue data, we convert real-world medical notes into high-quality doctor-patient dialogues using a decision tree-guided generation and refinement pipeline. We then propose a three-stage fine-tuning strategy combining supervised learning, simulated data augmentation, and preference learning. Furthermore, we propose a novel single-turn reasoning paradigm that reframes history taking as a sequence of single-turn reasoning problems. This design enhances interpretability and enables local supervision, dynamic adaptation, and greater sample efficiency. Experimental results show that our method substantially improves clinical reasoning, achieving gains of +16.9 F1 and +21.0 Top-1 diagnostic accuracy over GPT-4o. Our code and dataset can be found at https://github.com/zhentingsheng/Note2Chat.

</details>


### [31] [ASTRA: Automated Synthesis of agentic Trajectories and Reinforcement Arenas](https://arxiv.org/abs/2601.21558)
*Xiaoyu Tian,Haotian Wang,Shuaiting Chen,Hao Zhou,Kaichi Yu,Yudian Zhang,Jade Ouyang,Junxi Yin,Jiong Chen,Baoyan Guo,Lei Zhang,Junjie Tao,Yuansheng Song,Ming Cui,Chengwei Liu*

Main category: cs.CL

TL;DR: 本文提出ASTRA，一个全自动工具增强大语言模型训练框架，结合数据合成和可验证强化学习，显著提升多步决策性能。


<details>
  <summary>Details</summary>
Motivation: 现有工具增强代理训练方法依赖人工、模拟环境不可验证、单一训练方式且难以稳定学习长时序复杂任务。

Method: 通过静态工具调用图合成多样化轨迹，以及基于人类语义推理的环境合成，实现可验证的多回合强化学习，结合监督微调与在线强化学习的统一训练方法。

Result: ASTRA在多项工具使用基准测试中展示了先进性能，同时保持了核心推理能力，且公开了完整代码和模型。

Conclusion: ASTRA框架能够自动化训练工具增强的大语言模型代理，实现了高效稳定的多步决策任务，性能接近闭源系统。

Abstract: Large language models (LLMs) are increasingly used as tool-augmented agents for multi-step decision making, yet training robust tool-using agents remains challenging. Existing methods still require manual intervention, depend on non-verifiable simulated environments, rely exclusively on either supervised fine-tuning (SFT) or reinforcement learning (RL), and struggle with stable long-horizon, multi-turn learning. To address these challenges, we introduce ASTRA, a fully automated end-to-end framework for training tool-augmented language model agents via scalable data synthesis and verifiable reinforcement learning. ASTRA integrates two complementary components. First, a pipeline that leverages the static topology of tool-call graphs synthesizes diverse, structurally grounded trajectories, instilling broad and transferable tool-use competence. Second, an environment synthesis framework that captures the rich, compositional topology of human semantic reasoning converts decomposed question-answer traces into independent, code-executable, and rule-verifiable environments, enabling deterministic multi-turn RL. Based on this method, we develop a unified training methodology that integrates SFT with online RL using trajectory-level rewards to balance task completion and interaction efficiency. Experiments on multiple agentic tool-use benchmarks demonstrate that ASTRA-trained models achieve state-of-the-art performance at comparable scales, approaching closed-source systems while preserving core reasoning ability. We release the full pipelines, environments, and trained models at https://github.com/LianjiaTech/astra.

</details>


### [32] [KromHC: Manifold-Constrained Hyper-Connections with Kronecker-Product Residual Matrices](https://arxiv.org/abs/2601.21579)
*Wuyang Zhou,Yuxuan Gu,Giorgos Iacovides,Danilo Mandic*

Main category: cs.CL

TL;DR: 本文提出了一种名为KromHC的新方法，通过克罗内克积降低残差矩阵的参数复杂度，同时保证双随机性，实现高效且稳定的超连接训练。


<details>
  <summary>Details</summary>
Motivation: 传统超连接方法存在训练不稳定和扩展性差的问题，mHC虽缓解部分问题但迭代算法不保证精确双随机且参数复杂度较高；mHC-lite虽保证双随机性但参数复杂度因阶乘增长而难以扩展。

Method: 通过对残差矩阵在张量模式上进行分解，利用较小的双随机矩阵的克罗内克积重新参数化残差矩阵，并在每个因子矩阵上施加流形约束，实现精确双随机性和参数复杂度下降。

Result: KromHC在保证双随机性和降低训练参数复杂度方面表现优异，经过综合实验验证，其性能可与甚至超越最先进的mHC变体，同时参数数量显著减少。

Conclusion: KromHC在保证残差矩阵双随机性的前提下，显著降低参数复杂度至\(\mathcal{O}(n^2C)\)，在多个实验中表现优于或匹配现有最先进的mHC方法，同时参数量更少。

Abstract: The success of Hyper-Connections (HC) in neural networks (NN) has also highlighted issues related to its training instability and restricted scalability. The Manifold-Constrained Hyper-Connections (mHC) mitigate these challenges by projecting the residual connection space onto a Birkhoff polytope, however, it faces two issues: 1) its iterative Sinkhorn-Knopp (SK) algorithm does not always yield exact doubly stochastic residual matrices; 2) mHC incurs a prohibitive $\mathcal{O}(n^3C)$ parameter complexity with $n$ as the width of the residual stream and $C$ as the feature dimension. The recently proposed mHC-lite reparametrizes the residual matrix via the Birkhoff-von-Neumann theorem to guarantee double stochasticity, but also faces a factorial explosion in its parameter complexity, $\mathcal{O} \left( nC \cdot n! \right)$. To address both challenges, we propose \textbf{KromHC}, which uses the \underline{Kro}necker products of smaller doubly stochastic matrices to parametrize the residual matrix in \underline{mHC}. By enforcing manifold constraints across the factor residual matrices along each mode of the tensorized residual stream, KromHC guarantees exact double stochasticity of the residual matrices while reducing parameter complexity to $\mathcal{O}(n^2C)$. Comprehensive experiments demonstrate that KromHC matches or even outperforms state-of-the-art (SOTA) mHC variants, while requiring significantly fewer trainable parameters. The code is available at \texttt{https://github.com/wz1119/KromHC}.

</details>


### [33] [Language Models as Artificial Learners: Investigating Crosslinguistic Influence](https://arxiv.org/abs/2601.21587)
*Abderrahmane Issam,Yusuf Can Semerci,Jan Scholtes,Gerasimos Spanakis*

Main category: cs.CL

TL;DR: 利用语言模型模拟跨语言影响，验证语言优势和能力对CLI的重要性，并揭示L1在L2处理中被激活的机制，语言模型为研究CLI提供了新的计算框架。


<details>
  <summary>Details</summary>
Motivation: 人类双语研究中跨语言影响(CLI)结果常因实验变异而冲突，急需更加系统和可控的模拟手段。

Method: 通过控制语言模型中的L1语言优势和L2语言能力（通过L2暴露年龄调节），并结合跨语言启动实验，分析L1结构对L2处理的影响。

Result: 发现语言优势和语言能力是CLI的强预测因子，语法结构的启动有双向效应，非语法结构的启动受语言优势影响且L1在L2处理中被共同激活。

Conclusion: 语言模型能够有效模拟和解释跨语言影响现象，支持语言优势和能力决定CLI强度，并为双语神经机制提供新的计算性证据，促进CLI理论的发展。

Abstract: Despite the centrality of crosslinguistic influence (CLI) to bilingualism research, human studies often yield conflicting results due to inherent experimental variance. We address these inconsistencies by using language models (LMs) as controlled statistical learners to systematically simulate CLI and isolate its underlying drivers. Specifically, we study the effect of varying the L1 language dominance and the L2 language proficiency, which we manipulate by controlling the L2 age of exposure -- defined as the training step at which the L2 is introduced. Furthermore, we investigate the impact of pretraining on L1 languages with varying syntactic distance from the L2. Using cross-linguistic priming, we analyze how activating L1 structures impacts L2 processing. Our results align with evidence from psycholinguistic studies, confirming that language dominance and proficiency are strong predictors of CLI. We further find that while priming of grammatical structures is bidirectional, the priming of ungrammatical structures is sensitive to language dominance. Finally, we provide mechanistic evidence of CLI in LMs, demonstrating that the L1 is co-activated during L2 processing and directly influences the neural circuitry recruited for the L2. More broadly, our work demonstrates that LMs can serve as a computational framework to inform theories of human CLI.

</details>


### [34] [ILRR: Inference-Time Steering Method for Masked Diffusion Language Models](https://arxiv.org/abs/2601.21647)
*Eden Avrahami,Eliya Nachmani*

Main category: cs.CL

TL;DR: 本文提出一种无学习的迭代潜在表示精炼方法，实现对离散扩散语言模型生成文本属性的有效控制，显著提升控制精度且保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有离散扩散语言模型在推理时缺乏高效的文本属性控制手段，本文旨在提出一种无需额外学习即可实现准确属性引导的方法。

Method: ILRR通过动态对齐生成序列与单一参考序列的内部激活状态，在去噪过程中逐步引导生成，配合空间调制机制实现长文本的属性控制。

Result: 本文提出了一种名为迭代潜在表示精炼（ILRR）的无学习框架，用于在离散扩散语言模型（DLMs）中实现生成文本的属性控制。ILRR通过在去噪过程中动态对齐生成序列和参考序列的内部激活，捕获并传递高层语义特征，实现对情感等属性的灵活调节。扩展的空间调制引导机制进一步支持用较短参考文本控制长文本生成。实验表明，ILRR在维持生成质量的同时，相比现有方法显著提升了属性控制准确率，且计算开销较小。

Conclusion: ILRR方法能够在离散扩散语言模型中高效实现属性控制，提升属性准确率达10%至60%，并且计算成本低，适用于多种架构。

Abstract: Discrete Diffusion Language Models (DLMs) offer a promising non-autoregressive alternative for text generation, yet effective mechanisms for inference-time control remain relatively underexplored. Existing approaches include sampling-level guidance procedures or trajectory optimization mechanisms. In this work, we introduce Iterative Latent Representation Refinement (ILRR), a learning-free framework for steering DLMs using a single reference sequence. ILRR guides generation by dynamically aligning the internal activations of the generated sequence with those of a given reference throughout the denoising process. This approach captures and transfers high-level semantic properties, with a tunable steering scale enabling flexible control over attributes such as sentiment. We further introduce Spatially Modulated Steering, an extension that enables steering long texts using shorter references by regulating guidance intensity across the sequence. Empirically, we demonstrate that ILRR achieves effective attribute steering on LLaDA and MDLM architectures with a minor computational overhead, requiring only one additional parallel forward pass per denoising step. Under the same compute budget, ILRR improves attribute accuracy over comparable baselines by 10$\%$ to 60$\%$ points, while maintaining high generation quality.

</details>


### [35] [AdaptBPE: From General Purpose to Specialized Tokenizers](https://arxiv.org/abs/2601.21665)
*Vijini Liyanage,François Yvon*

Main category: cs.CL

TL;DR: 提出一种基于频率的后训练子词适应方法，优化词表提升特定领域语言模型的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 标准子词分词器通用但在特定领域或语言中效率低，亟需一种轻量的适应方法。

Method: 通过后训练适应算法，选择性替换低效子词，优化词表以更好地编码适应语料。

Result: 实验证明适应后的分词器在多个语言的生成和分类任务中，使用相同词表规模下，能更有效压缩测试语料。

Conclusion: 本文提出的基于频率的后训练子词适应策略有效提升了大词汇量语言模型在特定领域或语言中的编码效率。

Abstract: Subword tokenization methods, such as Byte-Pair Encoding (BPE), significantly impact the performance and efficiency of large language models (LLMs). The standard approach involves training a general-purpose tokenizer that uniformly processes all textual data during both training and inference. However, the use of a generic set of tokens can incur inefficiencies when applying the model to specific domains or languages. To address this limitation, we propose a post-training adaptation strategy that selectively replaces low-utility tokens with more relevant ones based on their frequency in an adaptation corpus. Our algorithm identifies the token inventory that most effectively encodes the adaptation corpus for a given target vocabulary size. Extensive experiments on generation and classification tasks across multiple languages demonstrate that our adapted tokenizers compress test corpora more effectively than baselines using the same vocabulary size. This method serves as a lightweight adaptation mechanism, akin to a vocabulary fine-tuning process, enabling optimized tokenization for specific domains or tasks. Our code and data are available at https://github.com/vijini/Adapt-BPE.git.

</details>


### [36] [When "Better" Prompts Hurt: Evaluation-Driven Iteration for LLM Applications](https://arxiv.org/abs/2601.22025)
*Daniel Commey*

Main category: cs.CL

TL;DR: 本文提出了针对大型语言模型应用的系统化评估流程和套件，通过实验证明提示设计需在具体任务表现和指令遵循间权衡，推动基于评估的持续改进。


<details>
  <summary>Details</summary>
Motivation: LLM应用的输出具有随机性、高维度和对提示及模型变化敏感，传统软件测试方法难以适用，因此需要新的系统化评估流程和套件。

Method: 提出了Define, Test, Diagnose, Fix评估工作流程，并设计了分层的最小可行评估套件（MVES），结合自动化检测、人类评分和LLM作为评判的评估方法。

Result: 在本地可复现实验中发现，通用的提示模板可能导致某些任务表现下降（如信息提取和RAG合规率下降），但指令遵循能力提升，表明需要基于评估反复迭代提示设计而非依赖通用方案。

Conclusion: 通过评估驱动的流程和方法，可以有效指导LLM应用提示的设计和迭代，避免盲目使用通用提示模板，提升模型实际应用表现和可靠性。

Abstract: Evaluating Large Language Model (LLM) applications differs from traditional software testing because outputs are stochastic, high-dimensional, and sensitive to prompt and model changes. We present an evaluation-driven workflow - Define, Test, Diagnose, Fix - that turns these challenges into a repeatable engineering loop.
  We introduce the Minimum Viable Evaluation Suite (MVES), a tiered set of recommended evaluation components for (i) general LLM applications, (ii) retrieval-augmented generation (RAG), and (iii) agentic tool-use workflows. We also synthesize common evaluation methods (automated checks, human rubrics, and LLM-as-judge) and discuss known judge failure modes.
  In reproducible local experiments (Ollama; Llama 3 8B Instruct and Qwen 2.5 7B Instruct), we observe that a generic "improved" prompt template can trade off behaviors: on our small structured suites, extraction pass rate decreased from 100% to 90% and RAG compliance from 93.3% to 80% for Llama 3 when replacing task-specific prompts with generic rules, while instruction-following improved. These findings motivate evaluation-driven prompt iteration and careful claim calibration rather than universal prompt recipes.
  All test suites, harnesses, and results are included for reproducibility.

</details>


### [37] [Scale-Dependent Semantic Dynamics Revealed by Allan Deviation](https://arxiv.org/abs/2601.21678)
*Debayan Dasgupta*

Main category: cs.CL

TL;DR: 本文用Allan偏差分析语言语义进展，发现人类文本与大模型在语义稳定性和动态模式上有本质区别。


<details>
  <summary>Details</summary>
Motivation: 探索语言语义状态演变的底层动力学机制，区分人类文本与算法生成文本的语义动态。

Method: 将有序的句子嵌入视为位移信号，利用精密计量学中的Allan偏差分析语义进展的稳定性。

Result: 发现两种动态模式：短时间幂律标度区分创意文学与技术文本，长时间内语义进入稳定性限制的噪声地板；大语言模型虽能模仿局部标度统计，但稳定性范围显著缩减。

Conclusion: 语义连贯性可以被量化为一个物理属性，并且人类文本和大语言模型在语义动态特征上存在显著差异。

Abstract: While language progresses through a sequence of semantic states, the underlying dynamics of this progression remain elusive. Here, we treat the semantic progression of written text as a stochastic trajectory in a high-dimensional state space. We utilize Allan deviation, a tool from precision metrology, to analyze the stability of meaning by treating ordered sentence embeddings as a displacement signal. Our analysis reveals two distinct dynamical regimes: short-time power-law scaling, which differentiates creative literature from technical texts, and a long-time crossover to a stability-limited noise floor. We find that while large language models successfully mimic the local scaling statistics of human text, they exhibit a systematic reduction in their stability horizon. These results establish semantic coherence as a measurable physical property, offering a framework to differentiate the nuanced dynamics of human cognition from the patterns generated by algorithmic models.

</details>


### [38] [FIT: Defying Catastrophic Forgetting in Continual LLM Unlearning](https://arxiv.org/abs/2601.21682)
*Xiaoyu Xu,Minxin Du,Kun Fang,Zi Liang,Yaxin Xiao,Zhicong Huang,Cheng Hong,Qingqing Ye,Haibo Hu*

Main category: cs.CL

TL;DR: 针对大型语言模型中连续且大量的删除请求，\fit框架通过数据过滤、重要性更新和层归因稳定模型性能，实现有效且持续的去学习。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）去学习方法很少考虑现实世界中连续且大量的删除请求，这会导致模型效用降低和灾难性遗忘。

Method: 提出了\fit框架，通过严格的数据过滤、重要性感知更新和针对性层归因，支持不断进行大量删除请求的持续去学习操作。

Result: 在包含个人信息、版权和有害内容的顺序删除场景下，\fit在忘记程度和保留效用方面实现最佳平衡，并超过现有方法在多个问答测试集上的表现。

Conclusion: \fit框架有效解决了连续删除请求导致的效用退化和灾难性遗忘问题，在保持模型效用的同时实现强大的遗忘能力，并具备抵抗恢复攻击的能力。

Abstract: Large language models (LLMs) demonstrate impressive capabilities across diverse tasks but raise concerns about privacy, copyright, and harmful materials. Existing LLM unlearning methods rarely consider the continual and high-volume nature of real-world deletion requests, which can cause utility degradation and catastrophic forgetting as requests accumulate. To address this challenge, we introduce \fit, a framework for continual unlearning that handles large numbers of deletion requests while maintaining robustness against both catastrophic forgetting and post-unlearning recovery. \fit mitigates degradation through rigorous data \underline{F}iltering, \underline{I}mportance-aware updates, and \underline{T}argeted layer attribution, enabling stable performance across long sequences of unlearning operations and achieving a favorable balance between forgetting effectiveness and utility retention. To support realistic evaluation, we present \textbf{PCH}, a benchmark covering \textbf{P}ersonal information, \textbf{C}opyright, and \textbf{H}armful content in sequential deletion scenarios, along with two symmetric metrics, Forget Degree (F.D.) and Retain Utility (R.U.), which jointly assess forgetting quality and utility preservation. Extensive experiments on four open-source LLMs with hundreds of deletion requests show that \fit achieves the strongest trade-off between F.D. and R.U., surpasses existing methods on MMLU, CommonsenseQA, and GSM8K, and remains resistant against both relearning and quantization recovery attacks.

</details>


### [39] [Do Not Waste Your Rollouts: Recycling Search Experience for Efficient Test-Time Scaling](https://arxiv.org/abs/2601.21684)
*Xinglin Wang,Jiayi Shi,Shaoxiong Feng,Peiwen Yuan,Yiwei Li,Yueqi Zhang,Chuyi Tan,Ji Zhang,Boyuan Pan,Yao Hu,Kan Li*

Main category: cs.CL

TL;DR: 提出了一种无训练的循环搜索经验策略，通过积累和重用测试时的推理信息，提升大型语言模型的推理效率和准确率。


<details>
  <summary>Details</summary>
Motivation: 现有推理扩展方法在每次推理尝试中丢弃中间信息，导致大量重复计算和效率浪费，亟需一种能够积累和重用推理经验的策略以提升效率和效果。

Method: 该方法通过构建共享经验库，循环利用中间推理结论和失败模式，实现推理过程的累积优化，无需额外训练。

Result: 本文提出了一种名为循环搜索经验（RSE）的测试时扩展策略，通过将测试时推理从孤立的多次尝试转变为累积过程，提升大型语言模型的推理能力。RSE通过主动提炼中间推理轨迹形成共享经验库，正向回收中间结论以避免冗余推导，负向回收失败模式以剪枝死胡同，从而显著减少计算冗余。理论分析证明了RSE比独立采样更高效，实验证明其在多个复杂推理基准数据集上达到最新的扩展效率水平。

Conclusion: RSE有效利用测试时的推理经验，显著提升了计算效率和推理表现，优于现有独立采样方法，达到了新的性能和效率标准。

Abstract: Test-Time Scaling enhances the reasoning capabilities of Large Language Models by allocating additional inference compute to broaden the exploration of the solution space. However, existing search strategies typically treat rollouts as disposable samples, where valuable intermediate insights are effectively discarded after each trial. This systemic memorylessness leads to massive computational redundancy, as models repeatedly re-derive discovered conclusions and revisit known dead ends across extensive attempts. To bridge this gap, we propose \textbf{Recycling Search Experience (RSE)}, a self-guided, training-free strategy that turns test-time search from a series of isolated trials into a cumulative process. By actively distilling raw trajectories into a shared experience bank, RSE enables positive recycling of intermediate conclusions to shortcut redundant derivations and negative recycling of failure patterns to prune encountered dead ends. Theoretically, we provide an analysis that formalizes the efficiency gains of RSE, validating its advantage over independent sampling in solving complex reasoning tasks. Empirically, extensive experiments on HMMT24, HMMT25, IMO-Bench, and HLE show that RSE consistently outperforms strong baselines with comparable computational cost, achieving state-of-the-art scaling efficiency.

</details>


### [40] [Can David Beat Goliath? On Multi-Hop Reasoning with Resource-Constrained Agents](https://arxiv.org/abs/2601.21699)
*Hojae Han,Heeyun Jung,Jongyoon Kim,Seung-won Hwang*

Main category: cs.CL

TL;DR: 提出DAVID-GRPO，提升小模型在资源受限条件下多跳推理的训练稳定性和准确率。


<details>
  <summary>Details</summary>
Motivation: 现有多跳推理强化学习依赖大量高成本训练，小模型因资源限制表现较差，亟需一种预算高效且准确的强化学习方法。

Method: DAVID-GRPO框架结合最小监督稳定早期训练、基于证据回忆的信用分配和截断近失误轨迹重新采样以提升探索效率。

Result: 本文提出了一种名为DAVID-GRPO的预算高效强化学习框架，旨在解决当前小型语言模型在资源受限条件下多跳推理中的低准确率和不稳定训练问题。该方法通过最小监督稳定早期学习、基于证据回忆分配检索信用以及通过重新采样截断近失误轨迹改善探索能力。在只使用四块RTX 3090 GPU训练的1.5B参数模型上，在六个多跳问答基准测试中优于现有大规模设置下的强化学习方法，表明小模型在合适的归纳偏置下能实现低成本高准确率。

Conclusion: 通过引入稳定学习策略和改进探索机制，小型语言模型在预算受限下也能实现高性能多跳推理。

Abstract: While reinforcement learning (RL) has empowered multi-turn reasoning agents with retrieval and tools, existing successes largely depend on extensive on-policy rollouts in high-cost, high-accuracy regimes. Under realistic resource constraints that cannot support large models or dense explorations, however, small language model agents fall into a low-cost, low-accuracy regime, where limited rollout budgets lead to sparse exploration, sparse credit assignment, and unstable training. In this work, we challenge this trade-off and show that small language models can achieve strong multi-hop reasoning under resource constraints. We introduce DAVID-GRPO, a budget-efficient RL framework that (i) stabilizes early learning with minimal supervision, (ii) assigns retrieval credit based on evidence recall, and (iii) improves exploration by resampling truncated near-miss trajectories. Evaluated on agents up to 1.5B parameters trained on only four RTX 3090 GPUs, DAVID-GRPO consistently outperforms prior RL methods designed for large-scale settings on six multi-hop QA benchmarks. These results show that with the right inductive biases, small agents can achieve low training cost with high accuracy.

</details>


### [41] [Why Attention Patterns Exist: A Unifying Temporal Perspective Analysis](https://arxiv.org/abs/2601.21709)
*Qingyue Yang,Jie Wang,Xing Li,Yinqi Bai,Xialiang Tong,Huiling Zhen,Jianye Hao,Mingxuan Yuan,Bin Li*

Main category: cs.CL

TL;DR: 本文提出了一种统一的注意力模式分析框架TAPPA，通过数学视角揭示注意力模式的可预测性与查询自相似性关系，提升了推理加速任务的性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究对注意力模式的观察零散，缺乏统一的理论框架，难以系统理解和应用。

Method: 引入Temporal Attention Pattern Predictability Analysis (TAPPA)框架，从时间连续性的视角分析注意力模式的数学表达式，以统一解释各种注意力模式。

Result: TAPPA将注意力模式区分为可预测的规则模式和不可预测的随机模式，揭示了查询自相似性对模式可预测性的影响，并通过联合分析查询、键及RoPE详细解析三类代表性模式。基于TAPPA设计的简单指标在KV缓存压缩和LLM剪枝任务中优于基线方法，表现出色。

Conclusion: TAPPA有效地统一解释了多样的注意力模式，增强了对大语言模型注意力行为的理解，并指导了推理加速技术的设计与优化。

Abstract: Attention patterns play a crucial role in both training and inference of large language models (LLMs). Prior works have identified individual patterns such as retrieval heads, sink heads, and diagonal traces, yet these observations remain fragmented and lack a unifying explanation. To bridge this gap, we introduce \textbf{Temporal Attention Pattern Predictability Analysis (TAPPA), a unifying framework that explains diverse attention patterns by analyzing their underlying mathematical formulations} from a temporally continuous perspective. TAPPA both deepens the understanding of attention behavior and guides inference acceleration approaches. Specifically, TAPPA characterizes attention patterns as predictable patterns with clear regularities and unpredictable patterns that appear effectively random. Our analysis further reveals that this distinction can be explained by the degree of query self-similarity along the temporal dimension. Focusing on the predictable patterns, we further provide a detailed mathematical analysis of three representative cases through the joint effect of queries, keys, and Rotary Positional Embeddings (RoPE). We validate TAPPA by applying its insights to KV cache compression and LLM pruning tasks. Across these tasks, a simple metric motivated by TAPPA consistently improves performance over baseline methods. The code is available at https://github.com/MIRALab-USTC/LLM-TAPPA.

</details>


### [42] [TACLer: Tailored Curriculum Reinforcement Learning for Efficient Reasoning](https://arxiv.org/abs/2601.21711)
*Huiyuan Lai,Malvina Nissim*

Main category: cs.CL

TL;DR: 提出TACLer，通过定制课程和思考策略提升LLM推理效率与准确率，降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 当前长链式推理需要大规模强化学习，代价高且易产生冗余步骤，需提升训练与推理的效率，同时保证或提升性能。

Method: 采用多阶段强化学习训练，结合模型能力逐步加大数据复杂度的课程学习策略，并设计思考/非思考混合推理模式以权衡准确性和效率。

Result: 该论文提出了TACLer，一种基于模型能力循序渐进提升数据复杂度的课程强化学习框架，优化长链式推理的训练效率和表现。通过定制化课程学习和混合思考/非思考推理策略，显著减少计算成本及推理所需token，同时提升模型在复杂数学问题上的准确率。

Conclusion: TACLer有效提升了复杂推理任务中大语言模型的学习效率和推理准确率，且减少计算资源消耗，优于现有最先进方法。

Abstract: Large Language Models (LLMs) have shown remarkable performance on complex reasoning tasks, especially when equipped with long chain-of-thought (CoT) reasoning. However, eliciting long CoT typically requires large-scale reinforcement learning (RL) training, while often leading to overthinking with redundant intermediate steps. To improve learning and reasoning efficiency, while preserving or even enhancing performance, we propose TACLer, a model-tailored curriculum reinforcement learning framework that gradually increases the complexity of the data based on the model's proficiency in multi-stage RL training. TACLer features two core components: (i) tailored curriculum learning that determines what knowledge the model lacks and needs to learn in progressive stages; (ii) a hybrid Thinking/NoThinking reasoning paradigm that balances accuracy and efficiency by enabling or disabling the Thinking mode. Our experiments show that TACLer yields a twofold advantage in learning and reasoning: (i) it reduces computational cost, cutting training compute by over 50% compared to long thinking models and reducing inference token usage by over 42% relative to the base model; and (ii) it improves accuracy by over 9% on the base model, consistently outperforming state-of-the-art Nothinking and Thinking baselines across four math datasets with complex problems.

</details>


### [43] [Enhancing Language Models for Robust Greenwashing Detection](https://arxiv.org/abs/2601.21722)
*Neil Heinrich Braun,Keane Ong,Rui Mao,Erik Cambria,Gianmarco Mengaldo*

Main category: cs.CL

TL;DR: 本文提出一种结合对比学习和有序排序的大模型结构方法，有效提高了对可持续性报告中绿色洗牌和含糊声明的识别鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有NLP模型对绿色洗牌和模糊陈述的识别不够鲁棒，难以准确评估可持续性报告。

Method: 结合对比学习和有序排序目标构建大语言模型潜空间，采用门控特征调制去噪，并利用MetaGradNorm稳定多目标优化。

Result: 实验表明所提框架在多个类别的测试中优于标准基线，同时揭示了表示刚性与泛化能力间的权衡。

Conclusion: 提出的方法在跨类别任务中表现出优越的鲁棒性，能够更好地识别绿色洗牌和含糊陈述。

Abstract: Sustainability reports are critical for ESG assessment, yet greenwashing and vague claims often undermine their reliability. Existing NLP models lack robustness to these practices, typically relying on surface-level patterns that generalize poorly. We propose a parameter-efficient framework that structures LLM latent spaces by combining contrastive learning with an ordinal ranking objective to capture graded distinctions between concrete actions and ambiguous claims. Our approach incorporates gated feature modulation to filter disclosure noise and utilizes MetaGradNorm to stabilize multi-objective optimization. Experiments in cross-category settings demonstrate superior robustness over standard baselines while revealing a trade-off between representational rigidity and generalization.

</details>


### [44] [Procedural Pretraining: Warming Up Language Models with Abstract Data](https://arxiv.org/abs/2601.21725)
*Liangze Jiang,Zachary Shinnick,Anton van den Hengel,Hemanth Saratchandran,Damien Teney*

Main category: cs.CL

TL;DR: 引入程序化数据预训练可有效提升语言模型的算法能力和性能，加速预训练，促进知识获取与推理能力的解耦。


<details>
  <summary>Details</summary>
Motivation: 借鉴人类学习过程，先学习抽象的逻辑和数学知识，再进行高阶推理，以此简化模型获取丰富语义知识的难度。

Method: 通过在预训练中引入程序化数据，特别是形式语言和简单算法生成的程序化数据，来替代传统的仅在大规模自然语言语料上预训练的方法。

Result: 程序化数据显著提升了模型的算法能力，如在上下文回忆任务中准确率从10%提升到98%。在大模型预训练中，哪怕只有0.1%的程序化数据也能显著优于传统语言、代码和非正式数学数据的预训练。模型在达到相同损失值时所需数据量明显减少，注意力层和MLP层体现了明显的结构特征。

Conclusion: 程序化预训练是一种简单轻量且高效的手段，可以显著提升语言模型性能和预训练效率，展现了将知识获得与推理分离的潜力。

Abstract: Pretraining directly on web-scale corpora is the de facto paradigm for building language models. We study an alternative setting where the model is initially exposed to abstract structured data, as a means to ease the subsequent acquisition of rich semantic knowledge, much like humans learn simple logic and mathematics before higher reasoning. We specifically focus on procedural data, generated by formal languages and other simple algorithms, as such abstract data.
  We first diagnose the algorithmic skills that different forms of procedural data can improve, often significantly. For example, on context recall (Needle-in-a-haystack), the accuracy jumps from 10 to 98% when pretraining on Dyck sequences (balanced brackets). Second, we study how these gains are reflected in pretraining larger models (up to 1.3B). We find that front-loading as little as 0.1% procedural data significantly outperforms standard pretraining on natural language, code, and informal mathematics (C4, CodeParrot, and DeepMind-Math datasets). Notably, this procedural pretraining enables the models to reach the same loss value with only 55, 67, 86% of the original data. Third, we explore the mechanisms behind and find that procedural pretraining instils non-trivial structure in both attention and MLP layers. The former is particularly important for structured domains (e.g. code), and the latter for language. Finally, we lay a path for combining multiple forms of procedural data. Our results show that procedural pretraining is a simple, lightweight means to improving performance and accelerating language model pretraining, ultimately suggesting the promise of disentangling knowledge acquisition from reasoning in LLMs.

</details>


### [45] [CE-GOCD: Central Entity-Guided Graph Optimization for Community Detection to Augment LLM Scientific Question Answering](https://arxiv.org/abs/2601.21733)
*Jiayin Lan,Jiaqi Li,Baoxin Wang,Ming Liu,Dayong Wu,Shijin Wang,Bing Qin,Guoping Hu*

Main category: cs.CL

TL;DR: 提出CE-GOCD方法，通过中心实体引导的图优化和社区检测增强LLMs对科学文献问答的表现，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强方法多依赖孤立的文本块或概念，忽视了论文间更深层的语义联系，导致LLMs在科学文献问答中理解不足，影响回答的全面性和针对性。

Method: 利用论文标题作为中心实体进行子图检索，并通过子图剪枝和补全改善语义发现，最后应用社区检测提取主题一致的文献子集来辅助LLMs生成更准确回答。

Result: 本文提出了基于中心实体引导的图优化社区检测（CE-GOCD）方法，针对现有基于检索增强的大型语言模型（LLMs）在科学文献问答中的理解不足问题，通过显式建模和利用学术知识图中的语义子结构提升问答性能。该方法以论文标题作为中心实体，进行针对性子图检索，并通过子图剪枝与补全增强语义发现，最后通过社区检测提取主题一致的论文组。实验在三个自然语言处理文献问答数据集上验证了该方法优于其他检索增强基线的效果。

Conclusion: CE-GOCD方法有效提升了基于检索增强的LLMs在科学文献问答任务中的理解能力和回答质量，验证了模型利用学术知识图语义子结构的有效性。

Abstract: Large Language Models (LLMs) are increasingly used for question answering over scientific research papers. Existing retrieval augmentation methods often rely on isolated text chunks or concepts, but overlook deeper semantic connections between papers. This impairs the LLM's comprehension of scientific literature, hindering the comprehensiveness and specificity of its responses. To address this, we propose Central Entity-Guided Graph Optimization for Community Detection (CE-GOCD), a method that augments LLMs' scientific question answering by explicitly modeling and leveraging semantic substructures within academic knowledge graphs. Our approach operates by: (1) leveraging paper titles as central entities for targeted subgraph retrieval, (2) enhancing implicit semantic discovery via subgraph pruning and completion, and (3) applying community detection to distill coherent paper groups with shared themes. We evaluated the proposed method on three NLP literature-based question-answering datasets, and the results demonstrate its superiority over other retrieval-augmented baseline approaches, confirming the effectiveness of our framework.

</details>


### [46] [Temporal Guidance for Large Language Models](https://arxiv.org/abs/2601.21744)
*Hong-Kai Zheng,Piji Li*

Main category: cs.CL

TL;DR: 本文提出一种基于时序引导的对比解码方法，通过多标记预测及轻量级投影器，实现了高效且性能优异的语言模型生成。


<details>
  <summary>Details</summary>
Motivation: 现有的对比解码方法提升了大语言模型生成质量，但因需要辅助模型导致计算开销增加。同时，内部自我对比解码方法在小规模模型上表现不稳定。

Method: 基于模型的局部偏好，提出时序引导（TeGu）策略，利用多标记预测（MTP）构造较弱的业余预测实现模型自我对比，并设计轻量级条件MTP投影器（cMTPP）避免维护多个独立网络。

Result: 在多个模型系列和基准测试中，TeGu实现了显著的性能提升，同时保持较低的额外内存和计算开销。

Conclusion: 通过时序引导策略，可以有效提升小规模模型的生成质量，并降低对比解码的计算负担，适合实际应用。

Abstract: Contrastive Decoding (CD) enhances the generation quality of large language models (LLMs) but incurs significant additional computational overhead due to the need for an auxiliary model. Existing internal self-contrastive decoding methods, such as Decoding by Contrasting Layers (DoLa), focus on discrepancies across different layers, which are notably unstable on small-scale models. In this work, based on the observation that LLMs exhibit local preferences, we propose a novel contrastive guidance strategy along the temporal dimension, namely Temporal Guidance (TeGu). Our method ingeniously leverages Multi-Token Prediction (MTP) to construct weaker amateur predictions for model self-contrast. To standardize the implementation of this mechanism, we further introduce a lightweight Conditional MTP Projector (cMTPP), which avoids maintaining multiple independent networks as required by other MTP modules. Across various model series and benchmarks, TeGu achieves significant performance improvements while maintaining low additional memory consumption and computational overhead.

</details>


### [47] [CoFrGeNet: Continued Fraction Architectures for Language Generation](https://arxiv.org/abs/2601.21766)
*Amit Dhurandhar,Vijil Chenthamarakshan,Dennis Wei,Tejaswini Pedapati,Karthikeyan Natesan Ramamurthy,Rahul Nair*

Main category: cs.CL

TL;DR: 本文提出的基于连分数的生成网络CoFrGeNets用更少的参数和更短的训练时间，实现了与大型Transformer模型相当甚至更好的性能。


<details>
  <summary>Details</summary>
Motivation: 受连分数的启发，希望设计更高效、参数更少但性能不逊色的生成模型架构。

Method: 提出了一种基于连分数的新型函数类，并设计了实现该函数类的架构家族CoFrGeNets，替代Transformer中的多头注意力和前馈网络，且参数更少。

Result: 在GPT2-xl和Llama3两种Transformer架构上预训练和测试后，模型在分类、问答、推理和文本理解等任务上的表现与原始模型相当甚至更好，且参数减少至原来的2/3到1/2，预训练时间更短。

Conclusion: CoFrGeNets作为Transformer的高效替代方案，易于集成进现有工业流程，未来针对硬件的定制将进一步发挥其潜力。

Abstract: Transformers are arguably the preferred architecture for language generation. In this paper, inspired by continued fractions, we introduce a new function class for generative modeling. The architecture family implementing this function class is named CoFrGeNets - Continued Fraction Generative Networks. We design novel architectural components based on this function class that can replace Multi-head Attention and Feed-Forward Networks in Transformer blocks while requiring much fewer parameters. We derive custom gradient formulations to optimize the proposed components more accurately and efficiently than using standard PyTorch-based gradients. Our components are a plug-in replacement requiring little change in training or inference procedures that have already been put in place for Transformer-based models thus making our approach easy to incorporate in large industrial workflows. We experiment on two very different transformer architectures GPT2-xl (1.5B) and Llama3 (3.2B), where the former we pre-train on OpenWebText and GneissWeb, while the latter we pre-train on the docling data mix which consists of nine different datasets. Results show that the performance on downstream classification, Q\& A, reasoning and text understanding tasks of our models is competitive and sometimes even superior to the original models with $\frac{2}{3}$ to $\frac{1}{2}$ the parameters and shorter pre-training time. We believe that future implementations customized to hardware will further bring out the true potential of our architectures.

</details>


### [48] [Evaluating ChatGPT on Medical Information Extraction Tasks: Performance, Explainability and Beyond](https://arxiv.org/abs/2601.21767)
*Wei Zhu*

Main category: cs.CL

TL;DR: 本文系统评估了ChatGPT在四种医学信息抽取任务上的表现，发现其准确率不及微调模型，但解释质量高且对文本保持忠实，同时存在过度自信和生成不确定性的问题。


<details>
  <summary>Details</summary>
Motivation: 评估ChatGPT在医学信息抽取领域的综合能力，以了解其能否替代或辅助现有专业模型。

Method: 通过6个基准数据集对ChatGPT在四种医学信息抽取任务上的性能、解释性、置信度、忠实度和不确定性进行系统评估。

Result: ChatGPT在医学信息抽取任务上的性能落后于微调模型，能提供优质解释并忠实于原文，但表现出过度自信，且生成结果的不确定性影响了信息提取的可靠性。

Conclusion: ChatGPT在医学信息抽取任务中的表现不及专门微调模型，虽能提供高质量解释并保持忠实，但其过度自信和生成的不确定性限制了其实际应用。

Abstract: Large Language Models (LLMs) like ChatGPT have demonstrated amazing capabilities in comprehending user intents and generate reasonable and useful responses. Beside their ability to chat, their capabilities in various natural language processing (NLP) tasks are of interest to the research community. In this paper, we focus on assessing the overall ability of ChatGPT in 4 different medical information extraction (MedIE) tasks across 6 benchmark datasets. We present the systematically analysis by measuring ChatGPT's performance, explainability, confidence, faithfulness, and uncertainty. Our experiments reveal that: (a) ChatGPT's performance scores on MedIE tasks fall behind those of the fine-tuned baseline models. (b) ChatGPT can provide high-quality explanations for its decisions, however, ChatGPT is over-confident in its predcitions. (c) ChatGPT demonstrates a high level of faithfulness to the original text in the majority of cases. (d) The uncertainty in generation causes uncertainty in information extraction results, thus may hinder its applications in MedIE tasks.

</details>


### [49] [Zonkey: A Hierarchical Diffusion Language Model with Differentiable Tokenization and Probabilistic Attention](https://arxiv.org/abs/2601.21768)
*Alon Rozental*

Main category: cs.CL

TL;DR: 该论文提出Zonkey，一种分层扩散模型，利用可微分分词器实现从字符到文档表示的端到端可训练流程，解决传统固定分词器的局限。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型受限于不可微的固定分词器，难以实现端到端优化且适应性不足，特别是面对噪声和领域特定数据。

Method: 设计可微分分词器Segment Splitter，基于概率注意力机制实现柔性序列分割，结合层次结构与去噪扩散混合模型进行表征压缩和重构，训练于维基百科数据。

Result: Zonkey在多层次生成中表现出语言学上有意义的分割和变长文本生成，生成文本具有良好连贯性和质量，且源码已公开以复现实验。

Conclusion: Zonkey展示了端到端可微分分词和层次扩散模型的有效性，在生成连贯、多长度文本方面优于基于熵的可学分词器，有助于实现更灵活的LLM优化和领域适应。

Abstract: Large language models (LLMs) have revolutionized natural language processing, yet they remain constrained by fixed, non-differentiable tokenizers like Byte Pair Encoding (BPE), which hinder end-to-end optimization and adaptability to noisy or domain-specific data. We introduce Zonkey, a hierarchical diffusion model that addresses these limitations through a fully trainable pipeline from raw characters to document-level representations. At its core is a differentiable tokenizer (Segment Splitter) that learns probabilistic beginning-of-sequence (BOS) decisions, enabling adaptive splits that emerge as linguistically meaningful (e.g., word boundaries at spaces, sentence starts at periods) without explicit supervision. This differentiability is enabled by our novel Probabilistic Attention mechanism, which incorporates position-specific existence probabilities to simulate soft masking over theoretically infinite sequences while preserving gradients. Sequences decay probabilistically rather than relying on end-of-sequence tokens, supporting variable-length outputs. Hierarchical levels compress sequences into higher abstractions (e.g., character n-grams to word-like vectors, then sentence-like), with reconstruction via our Denoising Diffusion Mixed Model (DDMM) for stable and efficient denoising in latent space. A Stitcher ensures overlap invariance across segments. Trained end-to-end on Wikipedia, Zonkey generates coherent, variable-length text from noise, demonstrating emergent hierarchies and promising qualitative alignment to data distributions compared to entropy-based learnable tokenizers. Our approach advances toward fully gradient-based LLMs, with potential for better domain adaptation and scalable generation. We release the source code for training and reproducing our experiments.

</details>


### [50] [KID: Knowledge-Injected Dual-Head Learning for Knowledge-Grounded Harmful Meme Detection](https://arxiv.org/abs/2601.21796)
*Yaocong Li,Leihan Zhang,Le Zhang,Qiang Yan*

Main category: cs.CL

TL;DR: 本文提出KID框架，融合知识注入与双头学习，提升跨语言有害表情包检测效果，验证了其在多数据集上的领先性能。


<details>
  <summary>Details</summary>
Motivation: 表情包依赖隐喻和社会文化背景，难以自动检测其中的隐性有害内容，现有方法多依赖模态内外信号，缺乏有效利用背景知识。

Method: 提出KID，一种知识注入的双头学习框架，通过标签约束蒸馏分解复杂的表情包理解为结构化推理链，明确连接视觉证据、背景知识和分类标签；采用双头架构联合优化语义生成和分类目标。

Result: 在五个多语言数据集（英语、中文、低资源孟加拉语）上，KID在二元和多标签有害表情包检测任务中实现了SOTA表现，相较现有最佳方法提升了2.1%至19.7%。

Conclusion: 知识注入和双头联合学习有效提升了模型对有害表情包的理解和泛化能力，促进了自动内容审核的准确性和鲁棒性。

Abstract: Internet memes have become pervasive carriers of digital culture on social platforms. However, their heavy reliance on metaphors and sociocultural context also makes them subtle vehicles for harmful content, posing significant challenges for automated content moderation. Existing approaches primarily focus on intra-modal and inter-modal signal analysis, while the understanding of implicit toxicity often depends on background knowledge that is not explicitly present in the meme itself. To address this challenge, we propose KID, a Knowledge-Injected Dual-Head Learning framework for knowledge-grounded harmful meme detection. KID adopts a label-constrained distillation paradigm to decompose complex meme understanding into structured reasoning chains that explicitly link visual evidence, background knowledge, and classification labels. These chains guide the learning process by grounding external knowledge in meme-specific contexts. In addition, KID employs a dual-head architecture that jointly optimizes semantic generation and classification objectives, enabling aligned linguistic reasoning while maintaining stable decision boundaries. Extensive experiments on five multilingual datasets spanning English, Chinese, and low-resource Bengali demonstrate that KID achieves SOTA performance on both binary and multi-label harmful meme detection tasks, improving over previous best methods by 2.1%--19.7% across primary evaluation metrics. Ablation studies further confirm the effectiveness of knowledge injection and dual-head joint learning, highlighting their complementary contributions to robust and generalizable meme understanding. The code and data are available at https://github.com/PotatoDog1669/KID.

</details>


### [51] [Enhancing Conversational Agents via Task-Oriented Adversarial Memory Adaptation](https://arxiv.org/abs/2601.21797)
*Yimin Deng,Yuqing Fu,Derong Xu,Yejing Wang,Wei Ni,Jingtong Gao,Xiaopeng Li,Chengxu Liu,Xiao Han,Guoshuai Zhao,Xiangyu Zhao,Li Zhu,Xueming Qian*

Main category: cs.CL

TL;DR: 针对长对话中记忆系统离线阶段任务无关的问题，本文提出对抗记忆适应机制，通过模拟任务执行实现记忆构建和更新的任务对齐，提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有记忆系统离线阶段固定且任务无关，导致记忆内容与下游任务需求不匹配，阻碍长对话任务性能提升，因此需要一种能够在离线阶段就结合任务目标优化记忆构建与更新的机制。

Method: AMA机制包括四个步骤：1）挑战者代理生成基于原始对话的问答对；2）利用构建的记忆回答这些问题模拟推理；3）评估者代理评估回答并进行错误分析；4）适配者代理根据错误分析对记忆构建策略和内容进行双层更新。

Result: 本文提出了一种名为对抗记忆适应机制（AMA）的模型，旨在解决对话系统在处理长对话时的上下文窗口限制问题。当前的记忆系统存在离线阶段任务无关、固定流程的问题，导致记忆内容与任务需求不匹配，影响下游任务表现。AMA通过模拟任务执行，包括生成问答对、利用构建的记忆回答问题、评估回答并进行错误分析，以及根据错误进行双重层面更新（构建策略和内容），实现记忆系统与任务目标的对齐。该机制在离线阶段提供任务感知的监督信号，提高记忆系统适应能力。实验表明，AMA能有效整合进多种记忆系统，在长对话基准LoCoMo上表现优异。

Conclusion: AMA机制通过任务感知的离线训练，使记忆系统更好地适应下游任务需求，有效提升长对话任务的表现，适用于多种记忆系统。

Abstract: Conversational agents struggle to handle long conversations due to context window limitations. Therefore, memory systems are developed to leverage essential historical information. Existing memory systems typically follow a pipeline of offline memory construction and update, and online retrieval. Despite the flexible online phase, the offline phase remains fixed and task-independent. In this phase, memory construction operates under a predefined workflow and fails to emphasize task relevant information. Meanwhile, memory updates are guided by generic metrics rather than task specific supervision. This leads to a misalignment between offline memory preparation and task requirements, which undermines downstream task performance. To this end, we propose an Adversarial Memory Adaptation mechanism (AMA) that aligns memory construction and update with task objectives by simulating task execution. Specifically, first, a challenger agent generates question answer pairs based on the original dialogues. The constructed memory is then used to answer these questions, simulating downstream inference. Subsequently, an evaluator agent assesses the responses and performs error analysis. Finally, an adapter agent analyzes the error cases and performs dual level updates on both the construction strategy and the content. Through this process, the memory system receives task aware supervision signals in advance during the offline phase, enhancing its adaptability to downstream tasks. AMA can be integrated into various existing memory systems, and extensive experiments on long dialogue benchmark LoCoMo demonstrate its effectiveness.

</details>


### [52] [RAG-E: Quantifying Retriever-Generator Alignment and Failure Modes](https://arxiv.org/abs/2601.21803)
*Korbinian Randl,Guido Rocchietti,Aron Henriksson,Ziawasch Abedjan,Tony Lindgren,John Pavlopoulos*

Main category: cs.CL

TL;DR: 本文提出RAG-E框架，通过新的数学归因方法和指标，揭示了RAG系统中检索器与生成器之间频繁的不匹配，强调了系统整体输出质量依赖组件间的有效协作。


<details>
  <summary>Details</summary>
Motivation: 因RAG系统中密集检索器与语言模型交互过程不透明，限制了其在高风险领域的部署，故需要一个可解释框架来量化和分析两者的对齐情况。

Method: 提出了RAG-E端到端可解释性框架，使用数学基础的归因方法：集成梯度用于检索器分析，PMCSHAP（蒙特卡洛稳定的Shapley值近似）用于生成器归因，引入加权归因相关差距（WARG）指标衡量生成器文档使用与检索器排序的一致性。

Result: 在TREC CAsT和FoodSafeSum数据集上的实证分析发现，47.4%至66.7%的查询中，生成器忽视了检索器排名最高的文档，48.1%至65.9%依赖排名较低的文档，体现了显著的错配问题。

Conclusion: RAG系统的输出质量不仅依赖于单个组件的性能，还依赖于检索器和生成器之间的交互，这一交互可以通过RAG-E框架进行审核。

Abstract: Retrieval-Augmented Generation (RAG) systems combine dense retrievers and language models to ground LLM outputs in retrieved documents. However, the opacity of how these components interact creates challenges for deployment in high-stakes domains. We present RAG-E, an end-to-end explainability framework that quantifies retriever-generator alignment through mathematically grounded attribution methods. Our approach adapts Integrated Gradients for retriever analysis, introduces PMCSHAP, a Monte Carlo-stabilized Shapley Value approximation, for generator attribution, and introduces the Weighted Attribution-Relevance Gap (WARG) metric to measure how well a generator's document usage aligns with a retriever's ranking. Empirical analysis on TREC CAsT and FoodSafeSum reveals critical misalignments: for 47.4% to 66.7% of queries, generators ignore the retriever's top-ranked documents, while 48.1% to 65.9% rely on documents ranked as less relevant. These failure modes demonstrate that RAG output quality depends not solely on individual component performance but on their interplay, which can be audited via RAG-E.

</details>


### [53] [Distribution-Aware Reward Estimation for Test-Time Reinforcement Learning](https://arxiv.org/abs/2601.21804)
*Bodong Du,Xuanqi Huang,Xiaomeng Li*

Main category: cs.CL

TL;DR: 提出DARE方法改善奖励估计，实现了大语言模型测试时的自我提升，显著提升了推理任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有测试时强化学习依赖多数投票产生奖励信号，忽略了非多数但正确动作的信息，导致奖励估计存在系统性偏差，影响学习效果，因此提出分布感知的奖励估计方法以提升性能。

Method: DARE方法不再依赖多次数采样的多数投票结果作为奖励，而是利用整个采样分布来估计奖励，并结合探索奖励和分布剪枝机制来增强奖励信息的准确性和鲁棒性。

Result: 提出了一种新方法DARE，通过利用完整的采样分布而非多数投票，改进了测试时强化学习中奖励信号的估计，克服了现有方法的偏差问题。实验证明该方法在多个复杂推理任务中提升了性能和稳定性。

Conclusion: DARE通过采用分布感知的奖励估计方法，有效缓解了多数投票带来的系统性偏差，提升了测试时强化学习的优化稳定性和最终效果。

Abstract: Test-time reinforcement learning (TTRL) enables large language models (LLMs) to self-improve on unlabeled inputs, but its effectiveness critically depends on how reward signals are estimated without ground-truth supervision. Most existing TTRL methods rely on majority voting (MV) over rollouts to produce deterministic rewards, implicitly assuming that the majority rollout provides a reliable learning signal. We show that this assumption is fragile: MV reduces the rollout distribution into a single outcome, discarding information about non-majority but correct actions candidates, and yields systematically biased reward estimates. To address this, we propose Distribution-AwareReward Estimation (DARE), which shifts reward estimation from a single majority outcome to the full empirical rollout distribution. DARE further augments this distribution-based reward with an exploration bonus and a distribution pruning mechanism for non-majority rollout exploration and reward denoise, yielding a more informative and robust reward estimation. Extensive experiments on challenging reasoning benchmarks show that DARE improves optimization stability and final performance over recent baselines, achieving relative improvements of 25.3% on challenging AIME 2024 and 5.3% on AMC.

</details>


### [54] [Mil-SCORE: Benchmarking Long-Context Geospatial Reasoning and Planning in Large Language Models](https://arxiv.org/abs/2601.21826)
*Aadi Palnitkar,Mingyang Mao,Nicholas Waytowich,Vinicius G. Goecks,Tinoosh Mohsenin,Xiaomin Lin*

Main category: cs.CL

TL;DR: 提出一个用于军事规划的长上下文多模态推理基准MilSCORE，评估LLM在复杂多跳推理中的表现，目前模型仍有较大提升空间。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型应用于更长、更复杂的任务，尤其是军事地理规划这类需整合多信息源的复杂场景，迫切需要一个真实且具有挑战性的长上下文基准来评测模型的选择性阅读和综合推理能力。

Method: 设计并构建了MilSCORE数据集，包含专家编写的七类多跳、多模态问题，涵盖事实回忆与多步推理，制定了评估协议，并基于当前主流视觉语言模型进行了基线测试。

Result: 该论文提出了MilSCORE，一个针对军事规划复杂场景设计的长上下文基准数据集，包含专家编写的多跳问题，涵盖战术和空间推理，检验大型语言模型在多模态、长距离、多步骤推理任务中的能力。实验结果显示现有模型在该数据集上表现有限，表明该任务具有较高难度。

Conclusion: 当前的视觉语言模型在MilSCORE上的表现有限，说明现有系统难以应对真实复杂的长上下文军事规划推理任务，MilSCORE为未来研究提供了有价值的挑战测试环境。

Abstract: As large language models (LLMs) are applied to increasingly longer and more complex tasks, there is a growing need for realistic long-context benchmarks that require selective reading and integration of heterogeneous, multi-modal information sources. This need is especially acute for geospatial planning problems, such as those found in planning for large-scale military operations, which demand fast and accurate reasoning over maps, orders, intelligence reports, and other distributed data. To address this gap, we present MilSCORE (Military Scenario Contextual Reasoning), to our knowledge the first scenario-level dataset of expert-authored, multi-hop questions grounded in a complex, simulated military planning scenario used for training. MilSCORE is designed to evaluate high-stakes decision-making and planning, probing LLMs' ability to combine tactical and spatial reasoning across multiple sources and to reason over long-horizon, geospatially rich context. The benchmark includes a diverse set of question types across seven categories targeting both factual recall and multi-step reasoning about constraints, strategy, and spatial analysis. We provide an evaluation protocol and report baseline results for a range of contemporary vision-language models. Our findings highlight substantial headroom on MilSCORE, indicating that current systems struggle with realistic, scenario-level long-context planning, and positioning MilSCORE as a challenging testbed for future work.

</details>


### [55] [Embodied Task Planning via Graph-Informed Action Generation with Large Lanaguage Model](https://arxiv.org/abs/2601.21841)
*Xiang Li,Ning Yan,Masood Mortazavi*

Main category: cs.CL

TL;DR: GiG利用图结构记忆和前瞻模块提升具身智能体长期规划能力，显著优于当前最先进方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在零样本推理上表现强劲，但作为具身智能体在长远规划中存在基本挑战，尤其是保持策略连贯性和遵守环境约束方面。

Method: 利用图神经网络编码环境状态，构建行动连接的执行轨迹图，并通过聚类获得结构先验，结合基于符号转移逻辑的有界前瞻模块进行规划优化。

Result: 提出了GiG，一种基于Graph-in-Graph架构的规划框架，通过图神经网络编码状态和聚类图嵌入，结合有界前瞻模块提升规划性能，在多个基准测试中显著优于现有方法。

Conclusion: GiG框架有效解决了具身智能体在长周期规划中的上下文限制和约束违背问题，提升了决策质量与效率。

Abstract: While Large Language Models (LLMs) have demonstrated strong zero-shot reasoning capabilities, their deployment as embodied agents still faces fundamental challenges in long-horizon planning. Unlike open-ended text generation, embodied agents must decompose high-level intent into actionable sub-goals while strictly adhering to the logic of a dynamic, observed environment. Standard LLM planners frequently fail to maintain strategy coherence over extended horizons due to context window limitation or hallucinate transitions that violate constraints. We propose GiG, a novel planning framework that structures embodied agents' memory using a Graph-in-Graph architecture. Our approach employs a Graph Neural Network (GNN) to encode environmental states into embeddings, organizing these embeddings into action-connected execution trace graphs within an experience memory bank. By clustering these graph embeddings, the framework enables retrieval of structure-aware priors, allowing agents to ground current decisions in relevant past structural patterns. Furthermore, we introduce a novel bounded lookahead module that leverages symbolic transition logic to enhance the agents' planning capabilities through the grounded action projection. We evaluate our framework on three embodied planning benchmarks-Robotouille Synchronous, Robotouille Asynchronous, and ALFWorld. Our method outperforms state-of-the-art baselines, achieving Pass@1 performance gains of up to 22% on Robotouille Synchronous, 37% on Asynchronous, and 15% on ALFWorld with comparable or lower computational cost.

</details>


### [56] [Learn-to-Distance: Distance Learning for Detecting LLM-Generated Text](https://arxiv.org/abs/2601.21895)
*Hongyi Zhou,Jin Zhu,Erhan Xu,Kai Ye,Ying Yang,Chengchun Shi*

Main category: cs.CL

TL;DR: 该论文通过几何视角解析改写检测算法，提出自适应距离学习新方法，大幅提升了大语言模型文本检测效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成高度类人的文本，带来错误信息和学术诚信风险，因此需要可靠算法检测这些内容。

Method: 提出几何方法解析改写检测算法，设计自适应学习原文与改写文本距离的新算法，通过理论和大量实验验证其优越性。

Result: 新算法在超过100种设置下优于基线方法，对不同大语言模型检测提升57.8%至80.6%。

Conclusion: 基于自适应学习距离的改写检测算法有效提升了大语言模型生成文本的检测能力，具有广泛适用性。

Abstract: Modern large language models (LLMs) such as GPT, Claude, and Gemini have transformed the way we learn, work, and communicate. Yet, their ability to produce highly human-like text raises serious concerns about misinformation and academic integrity, making it an urgent need for reliable algorithms to detect LLM-generated content. In this paper, we start by presenting a geometric approach to demystify rewrite-based detection algorithms, revealing their underlying rationale and demonstrating their generalization ability. Building on this insight, we introduce a novel rewrite-based detection algorithm that adaptively learns the distance between the original and rewritten text. Theoretically, we demonstrate that employing an adaptively learned distance function is more effective for detection than using a fixed distance. Empirically, we conduct extensive experiments with over 100 settings, and find that our approach demonstrates superior performance over baseline algorithms in the majority of scenarios. In particular, it achieves relative improvements from 57.8\% to 80.6\% over the strongest baseline across different target LLMs (e.g., GPT, Claude, and Gemini).

</details>


### [57] [SONIC: Segmented Optimized Nexus for Information Compression in Key-Value Caching](https://arxiv.org/abs/2601.21927)
*Hong Chen,Xiang Liu,Bo Wang,Yuxuan Fan,Yuanlin Chu,Zongluo Li,Xiaowen Chu,Xuming Hu*

Main category: cs.CL

TL;DR: 本文提出SONIC，一种基于学习的KV缓存压缩方法，通过语义丰富的Nexus标记和动态预算训练，在多轮对话中大幅提升性能和部署效率。


<details>
  <summary>Details</summary>
Motivation: 多轮大语言模型（LLM）部署中，关键值（KV）缓存的线性增长成为瓶颈，现有的KV缓存压缩方法未充分利用多轮对话的结构特性，依赖启发式驱逐导致关键信息丢失。

Method: 提出一种基于学习的框架SONIC，将历史对话片段压缩为紧凑且语义丰富的Nexus标记。通过动态预算训练，SONIC可灵活适应不同内存限制，无需重新训练。

Result: 在四个多轮对话基准测试中，SONIC在80%和50%的压缩率下均优于H2O和StreamingLLM等基线方法。在MTBench101测试中，SONIC平均得分提高35.55%，提升多轮对话连贯性。部署效率提升显著，推理速度比全上下文生成快50.1%。

Conclusion: SONIC有效缓解了KV缓存线性增长问题，通过语义压缩保持对话连贯性，同时显著提升部署效率，适用于多轮大语言模型的实际应用。

Abstract: The linear growth of Key-Value (KV) cache remains a bottleneck for multi-turn LLM deployment. Existing KV cache compression methods often fail to account for the structural properties of multi-turn dialogues, relying on heuristic eviction that risks losing critical context. We propose \textbf{SONIC}, a learning-based framework that compresses historical segments into compact and semantically rich \textbf{Nexus} tokens. By integrating dynamic budget training, SONIC allows flexible adaptation to varying memory constraints without retraining. Experiments show that at compression ratios of 80\% and 50\%, SONIC consistently outperforms baselines such as H2O and StreamingLLM on four diverse multi-turn benchmarks. Specifically, on the widely used MTBench101 benchmark, SONIC achieves an average score improvement of 35.55\% over state-of-the-art baselines, validating its effectiveness in sustaining coherent multi-turn dialogues. Furthermore, SONIC enhances deployment efficiency, accelerating the overall inference process by 50.1\% compared to full-context generation.

</details>


### [58] [From Generative Modeling to Clinical Classification: A GPT-Based Architecture for EHR Notes](https://arxiv.org/abs/2601.21955)
*Fariba Afrin Irany*

Main category: cs.CL

TL;DR: 本文提出一种基于GPT的选择性微调策略用于临床文本分类，有效解决了标签稀缺和计算资源限制问题，在放射学报告分类任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录中的非结构化临床文本丰富，但由于标签数据有限、类别不平衡及大规模预训练模型的高计算成本，临床文本建模存在挑战。

Method: 基于GPT架构，采用选择性微调策略，仅微调GPT-2模型的最后一层Transformer块、最后的归一化层及轻量级分类头，冻结其余参数。

Result: 该模型在MIMIC-IV-Note放射学报告数据集上表现稳定，特别在多标签分类、带不确定性假设的二分类及疾病预测任务中表现优异，适应性强且计算开销显著降低。

Conclusion: 选择性微调预训练生成式语言模型是一种高效且有效的临床文本分类方法，能够支持大规模EHR数据的实际应用，且显著降低了计算复杂度。

Abstract: The increasing availability of unstructured clinical narratives in electronic health records (EHRs) has created new opportunities for automated disease characterization, cohort identification, and clinical decision support. However, modeling long, domain-specific clinical text remains challenging due to limited labeled data, severe class imbalance, and the high computational cost of adapting large pretrained language models.
  This study presents a GPT-based architecture for clinical text classification that adapts a pretrained decoder-only Transformer using a selective fine-tuning strategy. Rather than updating all model parameters, the majority of the GPT-2 backbone is frozen, and training is restricted to the final Transformer block, the final layer normalization, and a lightweight classification head. This approach substantially reduces the number of trainable parameters while preserving the representational capacity required to model complex clinical language.
  The proposed method is evaluated on radiology reports from the MIMIC-IV-Note dataset using uncertainty-aware CheXpert-style labels derived directly from report text. Experiments cover multiple problem formulations, including multi-label classification of radiographic findings, binary per-label classification under different uncertainty assumptions, and aggregate disease outcome prediction. Across varying dataset sizes, the model exhibits stable convergence behavior and strong classification performance, particularly in settings dominated by non-mention and negated findings.
  Overall, the results indicate that selective fine-tuning of pretrained generative language models provides an efficient and effective pathway for clinical text classification, enabling scalable adaptation to real-world EHR data while significantly reducing computational complexity.

</details>


### [59] [OVD: On-policy Verbal Distillation](https://arxiv.org/abs/2601.21968)
*Jing Xiong,Hui Shen,Shansan Gong,Yuxin Cheng,Jianghan Shen,Chaofan Tao,Haochen Tan,Haoli Bai,Lifeng Shang,Ngai Wong*

Main category: cs.CL

TL;DR: OVD提出了一种内存友好的在线口头知识蒸馏方法，避免了token对齐限制，显著提升了推理任务性能和训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有的基于token级别的在线知识蒸馏方法需要学生模型和教师模型之间的token级别对齐，这限制了学生模型的探索能力，无法有效利用交互环境反馈，并在强化学习中引发严重的内存瓶颈。

Method: 提出了一种基于轨迹匹配的在线口头蒸馏框架OVD，利用教师模型给出的离散口头评分（0-9）代替了token级别的概率匹配，从而避免了token级别对齐，减少内存消耗，并允许学生模型自由探索输出空间。

Result: 在网页问答和数学推理任务中，OVD相比现有方法显著提升性能，网页问答任务平均EM提升最高达12.9%，数学基准任务提升最高达25.7%，且训练效率更高。

Conclusion: OVD通过引入轨迹匹配和口头评分，有效解决了在线蒸馏中的内存瓶颈和探索限制问题，实现了更优的性能和效率，证明了其在复杂推理任务中的应用价值。

Abstract: Knowledge distillation offers a promising path to transfer reasoning capabilities from large teacher models to efficient student models; however, existing token-level on-policy distillation methods require token-level alignment between the student and teacher models, which restricts the student model's exploration ability, prevent effective use of interactive environment feedback, and suffer from severe memory bottlenecks in reinforcement learning. We introduce On-policy Verbal Distillation (OVD), a memory-efficient framework that replaces token-level probability matching with trajectory matching using discrete verbal scores (0--9) from teacher models. OVD dramatically reduces memory consumption while enabling on-policy distillation from teacher models with verbal feedback, and avoids token-level alignment, allowing the student model to freely explore the output space. Extensive experiments on Web question answering and mathematical reasoning tasks show that OVD substantially outperforms existing methods, delivering up to +12.9% absolute improvement in average EM on Web Q&A tasks and a up to +25.7% gain on math benchmarks (when trained with only one random samples), while also exhibiting superior training efficiency. Our project page is available at https://OVD.github.io

</details>


### [60] [Token-Guard: Towards Token-Level Hallucination Control via Self-Checking Decoding](https://arxiv.org/abs/2601.21969)
*Yifan Zhu,Huiqiang Rong,Haoran Luo*

Main category: cs.CL

TL;DR: 提出Token-Guard方法，通过自检解码逐步检测并纠正幻觉令牌，有效减少LLMs的幻觉现象，提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLMs)常出现幻觉，生成与输入不一致的内容。现有方法如RAG和RLHF虽然能减少幻觉，但计算资源需求大。解码基方法较轻量但缺乏明确的幻觉控制。

Method: 提出Token-Guard，一种基于自检解码的逐令牌幻觉控制方法。通过在每个推理步骤进行内部验证检测幻觉令牌，利用潜空间中的显式幻觉风险评分评估候选片段，并通过迭代剪枝和再生成动态纠错。

Result: 在HALU数据集上的实验表明，Token-Guard显著减少幻觉，提高生成准确性。

Conclusion: Token-Guard提供了一种可扩展、模块化的解决方案，实现了可靠的大型语言模型输出。

Abstract: Large Language Models (LLMs) often hallucinate, generating content inconsistent with the input. Retrieval-Augmented Generation (RAG) and Reinforcement Learning with Human Feedback (RLHF) can mitigate hallucinations but require resource-intensive retrieval or large-scale fine-tuning. Decoding-based methods are lighter yet lack explicit hallucination control. To address this, we present Token-Guard, a token-level hallucination control method based on self-checking decoding. Token-Guard performs internal verification at each reasoning step to detect hallucinated tokens before they propagate. Candidate fragments are further evaluated in a latent space with explicit hallucination risk scoring, while iterative pruning and regeneration dynamically correct detected errors. Experiments on HALU datasets show Token-Guard substantially reduces hallucinations and improves generation accuracy, offering a scalable, modular solution for reliable LLM outputs. Our code is publicly available.

</details>


### [61] [Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units](https://arxiv.org/abs/2601.21996)
*Jianhui Chen,Yuzhang Luo,Liangming Pan*

Main category: cs.CL

TL;DR: 本文提出利用影响函数追踪训练数据对模型中可解释机制的因果影响，验证了结构化数据的催化作用，并开发了加速机制形成的数据增强方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）中可解释机制的起源尚不明确，亟需追踪其与训练数据间的因果联系。

Method: 提出了一种名为机制数据归因（MDA）的框架，利用影响函数追溯可解释单元对应的训练样本，并通过对Pythia模型系列进行干预实验验证因果关系。

Result: 发现重复结构化数据（如LaTeX、XML）在机制形成中起催化作用，干预高影响力样本能显著影响可解释头的出现，并证明归纳头与模型上下文学习能力间的因果联系。同时，提出的机制数据增强方法可以加速电路收敛。

Conclusion: 该工作首次实现了从训练数据到可解释机制的因果归因，为理解和控制大型语言模型的内部机制提供了有效工具和方法。

Abstract: While Mechanistic Interpretability has identified interpretable circuits in LLMs, their causal origins in training data remain elusive. We introduce Mechanistic Data Attribution (MDA), a scalable framework that employs Influence Functions to trace interpretable units back to specific training samples. Through extensive experiments on the Pythia family, we causally validate that targeted intervention--removing or augmenting a small fraction of high-influence samples--significantly modulates the emergence of interpretable heads, whereas random interventions show no effect. Our analysis reveals that repetitive structural data (e.g., LaTeX, XML) acts as a mechanistic catalyst. Furthermore, we observe that interventions targeting induction head formation induce a concurrent change in the model's in-context learning (ICL) capability. This provides direct causal evidence for the long-standing hypothesis regarding the functional link between induction heads and ICL. Finally, we propose a mechanistic data augmentation pipeline that consistently accelerates circuit convergence across model scales, providing a principled methodology for steering the developmental trajectories of LLMs.

</details>


### [62] [Causal Autoregressive Diffusion Language Model](https://arxiv.org/abs/2601.22031)
*Junhao Ruan,Bei Li,Yongjing Yin,Pengcheng Huang,Xin Chen,Jingang Wang,Xunliang Cai,Tong Xiao,JingBo Zhu*

Main category: cs.CL

TL;DR: 本文提出CARD框架，有效结合自回归模型和扩散模型优点，提升训练效率及推理速度，实现更高效的因果扩散语言模型生成。


<details>
  <summary>Details</summary>
Motivation: 为了提升扩散模型的训练效率与推理速度，兼顾数据效率与生成延迟，解决因果扩散中的优化不稳定问题。

Method: 提出了因果自回归扩散(CARD)框架，通过严格的因果注意力掩码，结合软尾掩码和基于信噪比的上下文重加权机制，实现单次前向传递的密集监督和动态并行解码。

Result: CARD在生成质量上优于现有离散扩散基线，训练延迟比块扩散方法降低3倍，同时具有基于置信度的可变长度序列生成能力。

Conclusion: CARD框架成功结合了自回归模型的训练效率和扩散模型的高吞吐推理能力，实现了高效且稳定的因果扩散生成。

Abstract: In this work, we propose Causal Autoregressive Diffusion (CARD), a novel framework that unifies the training efficiency of ARMs with the high-throughput inference of diffusion models. CARD reformulates the diffusion process within a strictly causal attention mask, enabling dense, per-token supervision in a single forward pass. To address the optimization instability of causal diffusion, we introduce a soft-tailed masking schema to preserve local context and a context-aware reweighting mechanism derived from signal-to-noise principles. This design enables dynamic parallel decoding, where the model leverages KV-caching to adaptively generate variable-length token sequences based on confidence. Empirically, CARD outperforms existing discrete diffusion baselines while reducing training latency by 3 $\times$ compared to block diffusion methods. Our results demonstrate that CARD achieves ARM-level data efficiency while unlocking the latency benefits of parallel generation, establishing a robust paradigm for next-generation efficient LLMs.

</details>


### [63] [Thinking Out of Order: When Output Order Stops Reflecting Reasoning Order in Diffusion Language Models](https://arxiv.org/abs/2601.22035)
*Longxuan Yu,Yu Fu,Shaorong Zhang,Hui Liu,Mukund Varma T,Greg Ver Steeg,Yue Dong*

Main category: cs.CL

TL;DR: 本文提出掩码扩散语言模型，通过并行迭代生成解决自回归模型固定生成顺序的限制，提高模型在答案与推理顺序变化场景下的鲁棒性，并用新基准ReasonOrderQA验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 自回归语言模型强制固定的从左到右的生成顺序，导致在输出结构需要与自然推理顺序冲突时（如先答后解释）存在根本限制。为此，需要探索解耦计算顺序与输出结构的方法。

Method: 提出并验证掩码扩散语言模型（MDLMs），该模型通过并行迭代精炼所有token，实现计算顺序与输出结构的解耦。使用新基准ReasonOrderQA及GSM8K、Math500测试其能力。

Result: MDLMs在要求先答后推理的情况下，表现出较高的order robustness，准确率相较自回归模型下降幅度显著较小（最多14% vs 67%）。MDLMs通过先稳定简单推理token再稳定复杂答案token实现顺序鲁棒性。

Conclusion: 掩码扩散语言模型能够克服自回归模型因固定生成顺序带来的限制，实现输出结构与计算顺序的解耦，提升模型在不同推理顺序下的鲁棒性；但存在一定失败条件，需进一步研究其极限。

Abstract: Autoregressive (AR) language models enforce a fixed left-to-right generation order, creating a fundamental limitation when the required output structure conflicts with natural reasoning (e.g., producing answers before explanations due to presentation or schema constraints). In such cases, AR models must commit to answers before generating intermediate reasoning, and this rigid constraint forces premature commitment. Masked diffusion language models (MDLMs), which iteratively refine all tokens in parallel, offer a way to decouple computation order from output structure. We validate this capability on GSM8K, Math500, and ReasonOrderQA, a benchmark we introduce with controlled difficulty and order-level evaluation. When prompts request answers before reasoning, AR models exhibit large accuracy gaps compared to standard chain-of-thought ordering (up to 67% relative drop), while MDLMs remain stable ($\leq$14% relative drop), a property we term "order robustness". Using ReasonOrderQA, we present evidence that MDLMs achieve order robustness by stabilizing simpler tokens (e.g., reasoning steps) earlier in the diffusion process than complex ones (e.g., final answers), enabling reasoning tokens to stabilize before answer commitment. Finally, we identify failure conditions where this advantage weakens, outlining the limits required for order robustness.

</details>


### [64] [A Separable Architecture for Continuous Token Representation in Language Models](https://arxiv.org/abs/2601.22040)
*Reza T. Batley,Sourav Saha*

Main category: cs.CL

TL;DR: 针对小模型嵌入参数分配问题，提出Leviathan连续嵌入生成器，提升模型有效参数容量和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的Transformer模型对参数进行等价对待，但在小型语言模型中，嵌入矩阵占据大量参数，这种分配方式可能并非最优。

Method: 提出Leviathan架构，使用连续嵌入生成器替代传统的离散查找表，并在等参数条件下，在Pile数据集上进行评估。

Result: Leviathan在性能上持续优于标准的LLaMA风格模型，并通过经验幂律拟合显示其有效参数容量显著更高。

Conclusion: Leviathan架构在研究范围内表现为拥有1.47到2.11倍参数量的密集模型，证明其参数利用更有效。

Abstract: Transformer scaling law analyses typically treat parameters as interchangeable; an abstraction that accurately predicts loss-compute relationships. Yet, in sub-billion-parameter small language models (SLMs), embedding matrices dominate the parameter budget. This work argues that this allocation is as suboptimal as it is counterintuitive. Leviathan is an architecture with a continuous embedding generator to replace the discrete lookup tables of canonical models. Evaluating on the Pile dataset under isoparametric settings, Leviathan consistently outperforms a standard, LLaMA-style architecture. By means of an empirical power-law fit, Leviathan exhibits a markedly superior effective parameter capacity. Across the regime studied, Leviathan behaves as a dense model with $1.47$ to $2.11 \times$ more parameters.

</details>


### [65] [On the Paradoxical Interference between Instruction-Following and Task Solving](https://arxiv.org/abs/2601.22047)
*Yunjia Qi,Hao Peng,Xintong Shi,Amy Xin,Xiaozhi Wang,Bin Xu,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: 指令遵循虽旨在提升LLM任务执行准确性，但可能反而干扰模型任务解决能力。文章通过SUSTAINSCORE指标量化这种干扰，发现加入自明约束显著降低性能，揭示了模型对约束过度关注的模式，并研究了对齐策略的相关影响。


<details>
  <summary>Details</summary>
Motivation: 尽管指令遵循被广泛用于使LLM更好地符合人类意图，但其可能产生反效果，干扰模型解决任务的能力。理解和量化这一干扰现象，对于改进指令设计和模型对齐策略至关重要。

Method: 提出SUSTAINSCORE指标，通过在指令中插入自明且满足的约束，衡量任务性能下降来量化指令遵循的干扰。在数学、多跳问答和代码生成任务上，利用该指标评估多款先进LLM的表现，分析其关注机制及后训练范式的影响。

Result: 本文揭示了指令遵循在大语言模型（LLMs）中可能干扰任务解决能力的反常现象，并提出了衡量这一干扰的指标SUSTAINSCORE。该指标通过在指令中插入自明且原模型输出已满足的约束，测量任务性能的下降。实验证明，在数学、多跳问答和代码生成任务上，加入自明约束会显著降低模型性能，包括先进模型Claude-Sonnet-4.5。干扰现象在不同约束类型和规模下均表现出普遍性。研究还发现失败案例中模型对约束的注意力显著高于成功案例。最后，利用SUSTAINSCORE探讨了不同后训练范式对干扰的影响，并给出当前对齐策略的实证观察。

Conclusion: 指令遵循对LLM任务解决能力存在潜在干扰，表现为加入自明约束导致性能下降，干扰具有普适性且与模型对约束的注意力分配相关。理解此现象对于优化模型对齐策略和提升任务性能意义重大。

Abstract: Instruction following aims to align Large Language Models (LLMs) with human intent by specifying explicit constraints on how tasks should be performed. However, we reveal a counterintuitive phenomenon: instruction following can paradoxically interfere with LLMs' task-solving capability. We propose a metric, SUSTAINSCORE, to quantify the interference of instruction following with task solving. It measures task performance drop after inserting into the instruction a self-evident constraint, which is naturally met by the original successful model output and extracted from it. Experiments on current LLMs in mathematics, multi-hop QA, and code generation show that adding the self-evident constraints leads to substantial performance drops, even for advanced models such as Claude-Sonnet-4.5. We validate the generality of the interference across constraint types and scales. Furthermore, we identify common failure patterns, and by investigating the mechanisms of interference, we observe that failed cases allocate significantly more attention to constraints compared to successful ones. Finally, we use SUSTAINSCORE to conduct an initial investigation into how distinct post-training paradigms affect the interference, presenting empirical observations on current alignment strategies. We will release our code and data to facilitate further research

</details>


### [66] [MasalBench: A Benchmark for Contextual and Cross-Cultural Understanding of Persian Proverbs in LLMs](https://arxiv.org/abs/2601.22050)
*Ghazal Kalhor,Behnam Bahrak*

Main category: cs.CL

TL;DR: 提出MasalBench基准，评测LLMs对波斯语谚语及跨文化理解能力，发现模型在文化类比任务上表现不足。


<details>
  <summary>Details</summary>
Motivation: 多语言LLMs已深入日常生活，但低资源语言中的文化语言理解，特别是谚语这一对话重要组成部分尚缺乏评测，故提出MasalBench以填补这一空白。

Method: 构建MasalBench基准，设计包含波斯语谚语及对应英文谚语的上下文理解任务，采用8个最先进LLMs进行评测，对比其在本地语言与跨文化等效谚语识别能力。

Result: 本文提出了MasalBench，一个评测大型语言模型(LLMs)在低资源语言波斯语谚语理解能力的综合基准。研究评估了8个先进模型，发现其在波斯语谚语识别上准确率超过0.90，但在识别等效英语谚语时性能显著下降，最高准确率仅为0.79。结果揭示了当前LLMs在文化知识和类比推理上的不足，并为其他低资源语言的跨文化理解评测提供了框架。

Conclusion: 当前最先进的LLMs在理解波斯语谚语方面表现良好，但在跨文化等效谚语识别中表现较差，表明其文化知识和类比推理的局限性。

Abstract: In recent years, multilingual Large Language Models (LLMs) have become an inseparable part of daily life, making it crucial for them to master the rules of conversational language in order to communicate effectively with users. While previous work has evaluated LLMs' understanding of figurative language in high-resource languages, their performance in low-resource languages remains underexplored. In this paper, we introduce MasalBench, a comprehensive benchmark for assessing LLMs' contextual and cross-cultural understanding of Persian proverbs, which are a key component of conversation in this low-resource language. We evaluate eight state-of-the-art LLMs on MasalBench and find that they perform well in identifying Persian proverbs in context, achieving accuracies above 0.90. However, their performance drops considerably when tasked with identifying equivalent English proverbs, with the best model achieving 0.79 accuracy. Our findings highlight the limitations of current LLMs in cultural knowledge and analogical reasoning, and they provide a framework for assessing cross-cultural understanding in other low-resource languages. MasalBench is available at https://github.com/kalhorghazal/MasalBench.

</details>


### [67] [$G^2$-Reader: Dual Evolving Graphs for Multimodal Document QA](https://arxiv.org/abs/2601.22055)
*Yaxin Du,Junru Song,Yifan Zhou,Cheng Wang,Jiahao Gu,Zimeng Chen,Menglan Chen,Wen Yao,Yang Yang,Ying Wen,Siheng Chen*

Main category: cs.CL

TL;DR: 提出$G^2$-Reader双图系统，用图结构保留多模态长文档信息和导航检索，实现更准确稳定的问题回答。


<details>
  <summary>Details</summary>
Motivation: 针对多模态长文档问答中文本与图表交织造成的结构破碎和迭代检索漂移问题，提出改进策略。

Method: 设计内容图与规划图两种图结构分别维护语义和检索状态，实现跨模态结构保存和导航引导。

Result: 在多模态问答基准上显著优于现有模型，包括大规模语言模型。

Conclusion: 采用双图结构的检索增强生成方法显著提升多模态长文档QA性能。

Abstract: Retrieval-augmented generation is a practical paradigm for question answering over long documents, but it remains brittle for multimodal reading where text, tables, and figures are interleaved across many pages. First, flat chunking breaks document-native structure and cross-modal alignment, yielding semantic fragments that are hard to interpret in isolation. Second, even iterative retrieval can fail in long contexts by looping on partial evidence or drifting into irrelevant sections as noise accumulates, since each step is guided only by the current snippet without a persistent global search state. We introduce $G^2$-Reader, a dual-graph system, to address both issues. It evolves a Content Graph to preserve document-native structure and cross-modal semantics, and maintains a Planning Graph, an agentic directed acyclic graph of sub-questions, to track intermediate findings and guide stepwise navigation for evidence completion. On VisDoMBench across five multimodal domains, $G^2$-Reader with Qwen3-VL-32B-Instruct reaches 66.21\% average accuracy, outperforming strong baselines and a standalone GPT-5 (53.08\%).

</details>


### [68] [VTC-R1: Vision-Text Compression for Efficient Long-Context Reasoning](https://arxiv.org/abs/2601.22069)
*Yibo Wang,Yongcheng Jing,Shunyu Liu,Hao Guan,Rong-cheng Tu,Chengyu Wang,Jun Huang,Dacheng Tao*

Main category: cs.CL

TL;DR: 通过将中间推理内容转换为图像形式，VTC-R1有效提升了大模型长上下文推理的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在处理长上下文推理时计算复杂度高，效率瓶颈明显，且现有压缩方法依赖复杂训练或外部模型，限制了可扩展性且丢失细粒度信息，因此提出融合视觉和文本压缩的新范式以提升效率和性能。

Method: 采用视觉-文本压缩，将推理中的中间文本转换成紧凑图像，并利用视觉语言模型进行迭代推理，结合OpenR1-Math-220K数据集微调VLMs-Glyph和Qwen3-VL模型，实现高效推理。

Result: 该论文提出了一种名为VTC-R1的新型高效推理范式，结合视觉-文本压缩技术优化长上下文推理过程。通过将中间推理内容渲染为紧凑图像，作为"光学记忆"反馈给视觉语言模型，显著减少文本长度，实现3.4倍的token压缩。该方法在多个数学推理基准测试中性能优于传统长上下文推理，并提升了推理效率，实现2.7倍的端到端延迟加速。

Conclusion: VTC-R1在保证推理质量的同时，大幅提升了推理效率和可扩展性，是解决复杂任务中长上下文推理瓶颈的有效方案。

Abstract: Long-context reasoning has significantly empowered large language models (LLMs) to tackle complex tasks, yet it introduces severe efficiency bottlenecks due to the computational complexity. Existing efficient approaches often rely on complex additional training or external models for compression, which limits scalability and discards critical fine-grained information. In this paper, we propose VTC-R1, a new efficient reasoning paradigm that integrates vision-text compression into the reasoning process. Instead of processing lengthy textual traces, VTC-R1 renders intermediate reasoning segments into compact images, which are iteratively fed back into vision-language models as "optical memory." We construct a training dataset based on OpenR1-Math-220K achieving 3.4x token compression and fine-tune representative VLMs-Glyph and Qwen3-VL. Extensive experiments on benchmarks such as MATH500, AIME25, AMC23 and GPQA-D demonstrate that VTC-R1 consistently outperforms standard long-context reasoning. Furthermore, our approach significantly improves inference efficiency, achieving 2.7x speedup in end-to-end latency, highlighting its potential as a scalable solution for reasoning-intensive applications. Our code is available at https://github.com/w-yibo/VTC-R1.

</details>


### [69] [ECO: Quantized Training without Full-Precision Master Weights](https://arxiv.org/abs/2601.22101)
*Mahdi Nikdan,Amir Zandieh,Dan Alistarh,Vahab Mirrokni*

Main category: cs.CL

TL;DR: 提出ECO优化器去除高精度主权重，直接对量化参数更新，减少训练大规模语言模型内存消耗，同时维持高准确率。


<details>
  <summary>Details</summary>
Motivation: 现有量化技术在训练过程中仍依赖高精度的权重缓冲区（主权重），这在稀疏专家模型中导致极大的内存开销，亟需减少内存消耗的解决方案。

Method: 设计了ECO优化器，通过在每步更新后对权重进行量化，并将量化误差注入优化器动量中形成反馈回路，从而消除主权重缓冲区并控制误差累积。

Result: 理论上证明了ECO在学习率递减情况下能收敛到最优点附近稳定区域，且避免了传统方法因无主权重导致的误差膨胀；在多个不同规模模型和精度设置下的实验证明，ECO在内存与准确率之间实现了更优的权衡。

Conclusion: 本文提出的Error-Compensating Optimizer (ECO)能够在无需高精度主权重缓冲区的情况下，直接对量化参数应用更新，显著降低了训练大型语言模型（尤其是稀疏专家模型）的内存开销，同时保持了近乎无损的准确度。

Abstract: Quantization has significantly improved the compute and memory efficiency of Large Language Model (LLM) training. However, existing approaches still rely on accumulating their updates in high-precision: concretely, gradient updates must be applied to a high-precision weight buffer, known as $\textit{master weights}$. This buffer introduces substantial memory overhead, particularly for Sparse Mixture of Experts (SMoE) models, where model parameters and optimizer states dominate memory usage. To address this, we introduce the Error-Compensating Optimizer (ECO), which eliminates master weights by applying updates directly to quantized parameters. ECO quantizes weights after each step and carefully injects the resulting quantization error into the optimizer momentum, forming an error-feedback loop with no additional memory. We prove that, under standard assumptions and a decaying learning rate, ECO converges to a constant-radius neighborhood of the optimum, while naive master-weight removal can incur an error that is inversely proportional to the learning rate. We show empirical results for pretraining small Transformers (30-800M), a Gemma-3 1B model, and a 2.1B parameter Sparse MoE model with FP8 quantization, and fine-tuning DeepSeek-MoE-16B in INT4 precision. Throughout, ECO matches baselines with master weights up to near-lossless accuracy, significantly shifting the static memory vs validation loss Pareto frontier.

</details>


### [70] [A Federated and Parameter-Efficient Framework for Large Language Model Training in Medicine](https://arxiv.org/abs/2601.22124)
*Anran Li,Yuanyuan Chen,Wenjun Long,Yu Yin,Yan Hu,Hyunjae Kim,Weipeng Zhou,Yujia Zhou,Hongyi Peng,Yang Ren,Xuguang Ai,Zhenyue Qin,Ming Hu,Xiaoxiao Li,Han Yu,Yih-Chung Tham,Lucila Ohno-Machado,Hua Xu,Qingyu Chen*

Main category: cs.CL

TL;DR: 针对医疗领域大语言模型训练的跨机构协作难题，本文提出基于低秩适配器的高效联邦学习框架，有效降低通信成本并提升模型泛化与适应能力。


<details>
  <summary>Details</summary>
Motivation: 传统医疗大语言模型多依赖单机构数据，难以满足多机构数据异质性，且传统联邦学习通信成本高，计算资源受限，影响模型推广应用。

Method: 提出了Fed-MedLoRA框架，只传输低秩适配参数以降低通信和计算负担，并通过Fed-MedLoRA+引入自适应数据感知聚合，以应对跨机构数据异质性。

Result: 在临床信息抽取任务中，Fed-MedLoRA框架在多个真实患者队列和外部验证中表现优异，特别是在资源有限的新站点适配场景下显示出良好效果，优于BERT、LLaMA-3、DeepSeek-R1和GPT-4o模型。

Conclusion: 提出的参数高效且对模型无关的联邦学习框架有效解决了多机构医疗大语言模型的计算/通信瓶颈及数据异质性问题，促进了模型在实际临床场景中的推广与应用。

Abstract: Large language models (LLMs) have demonstrated strong performance on medical benchmarks, including question answering and diagnosis. To enable their use in clinical settings, LLMs are typically further adapted through continued pretraining or post-training using clinical data. However, most medical LLMs are trained on data from a single institution, which faces limitations in generalizability and safety in heterogeneous systems. Federated learning (FL) is a promising solution for enabling collaborative model development across healthcare institutions. Yet applying FL to LLMs in medicine remains fundamentally limited. First, conventional FL requires transmitting the full model during each communication round, which becomes impractical for multi-billion-parameter LLMs given the limited computational resources. Second, many FL algorithms implicitly assume data homogeneity, whereas real-world clinical data are highly heterogeneous across patients, diseases, and institutional practices. We introduce the model-agnostic and parameter-efficient federated learning framework for adapting LLMs to medical applications. Fed-MedLoRA transmits only low-rank adapter parameters, reducing communication and computation overhead, while Fed-MedLoRA+ further incorporates adaptive, data-aware aggregation to improve convergence under cross-site heterogeneity. We apply the framework to clinical information extraction (IE), which transforms patient narratives into structured medical entities and relations. Accuracy was assessed across five patient cohorts through comparisons with BERT models, and LLaMA-3 and DeepSeek-R1, GPT-4o models. Evaluation settings included (1) in-domain training and testing, (2) external validation on independent cohorts, and (3) a low-resource new-site adaptation scenario using real-world clinical notes from the Yale New Haven Health System.

</details>


### [71] [Reasoning While Asking: Transforming Reasoning Large Language Models from Passive Solvers to Proactive Inquirers](https://arxiv.org/abs/2601.22139)
*Xin Chen,Feng Jiang,Yiqian Zhang,Hardy Chen,Shuo Yan,Wenya Xie,Min Yang,Shujian Huang*

Main category: cs.CL

TL;DR: 本文提出主动交互推理（PIR），通过模型与用户间的交互澄清不确定信息，显著提升大语言模型在各种任务上的推理准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于链式思维提示的推理模型依赖盲目自我思考，无法有效处理缺失或模糊信息，需通过与用户互动澄清不确定性以提升推理效果。

Method: 采用不确定性感知的监督微调使模型具备交互推理能力，配合基于用户模拟器的策略优化框架，利用复合奖励确保模型与用户意图一致。

Result: 本文提出了主动交互推理（PIR）方法，旨在解决基于链式思维提示的推理型大语言模型（LLMs）在缺失或模糊关键信息时依赖被动“盲目自我思考”的问题。通过不再仅依赖外部查询，而是直接与用户交互以澄清前提和意图层面的不确定性，显著提升模型推理效果。PIR包括两个核心组件：不确定性感知的监督微调，使模型具备交互推理能力，以及基于用户模拟器的策略优化框架，通过复合奖励使模型行为与用户意图保持一致。实验证明，在数学推理、代码生成和文档编辑任务中，PIR相较于强基线在准确率、通过率和BLEU分数上分别提升了最高32.70%、22.90%和41.36，同时推理计算量和不必要的交互回合减少近一半，整体表现稳健且具备良好的泛化能力。

Conclusion: 主动交互推理（PIR）有效提升了大语言模型在面对不确定性时的推理能力，显著提高性能，减少运算和交互成本，且具备良好泛化和鲁棒性。

Abstract: Reasoning-oriented Large Language Models (LLMs) have achieved remarkable progress with Chain-of-Thought (CoT) prompting, yet they remain fundamentally limited by a \emph{blind self-thinking} paradigm: performing extensive internal reasoning even when critical information is missing or ambiguous. We propose Proactive Interactive Reasoning (PIR), a new reasoning paradigm that transforms LLMs from passive solvers into proactive inquirers that interleave reasoning with clarification. Unlike existing search- or tool-based frameworks that primarily address knowledge uncertainty by querying external environments, PIR targets premise- and intent-level uncertainty through direct interaction with the user. PIR is implemented via two core components: (1) an uncertainty-aware supervised fine-tuning procedure that equips models with interactive reasoning capability, and (2) a user-simulator-based policy optimization framework driven by a composite reward that aligns model behavior with user intent. Extensive experiments on mathematical reasoning, code generation, and document editing demonstrate that PIR consistently outperforms strong baselines, achieving up to 32.70\% higher accuracy, 22.90\% higher pass rate, and 41.36 BLEU improvement, while reducing nearly half of the reasoning computation and unnecessary interaction turns. Further reliability evaluations on factual knowledge, question answering, and missing-premise scenarios confirm the strong generalization and robustness of PIR. Model and code are publicly available at: \href{https://github.com/SUAT-AIRI/Proactive-Interactive-R1}

</details>


### [72] [FineInstructions: Scaling Synthetic Instructions to Pre-Training Scale](https://arxiv.org/abs/2601.22146)
*Ajay Patel,Colin Raffel,Chris Callison-Burch*

Main category: cs.CL

TL;DR: 本论文提出通过生成海量合成指令-回答对数据集，利用指令调优目标进行从头预训练，大幅提升大语言模型对用户指令的响应质量。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型由于监督训练数据有限，通常先用海量无结构文本进行自监督预训练，再通过少量指令调优数据微调，受限于指令调优数据规模，模型难以充分适应用户需求。

Method: 通过构建一个包含约1800万个指令模板的大规模合成训练数据集（FineInstructions），将预训练语料中的真实文档与用户指令模板匹配生成指令-回答对，用以替代传统的自监督‘预测下一个词’任务，实现完全基于指令调优目标的从头预训练。

Result: 通过Token对Token的严格训练实验，作者发现使用FineInstructions数据集进行预训练的模型，在标准基准测试中关于自由响应质量方面优于传统预训练和其他合成预训练方法。

Conclusion: FineInstructions方法有效利用预训练语料，解决了指令调优数据不足问题，实现了更符合实际使用场景的预训练范式，提升了大语言模型的表现。

Abstract: Due to limited supervised training data, large language models (LLMs) are typically pre-trained via a self-supervised "predict the next word" objective on a vast amount of unstructured text data. To make the resulting model useful to users, it is further trained on a far smaller amount of "instruction-tuning" data comprised of supervised training examples of instructions and responses. To overcome the limited amount of supervised data, we propose a procedure that can transform the knowledge in internet-scale pre-training documents into billions of synthetic instruction and answer training pairs. The resulting dataset, called FineInstructions, uses ~18M instruction templates created from real user-written queries and prompts. These instruction templates are matched to and instantiated with human-written source documents from unstructured pre-training corpora. With "supervised" synthetic training data generated at this scale, an LLM can be pre-trained from scratch solely with the instruction-tuning objective, which is far more in-distribution with the expected downstream usage of LLMs (responding to user prompts). We conduct controlled token-for-token training experiments and find pre-training on FineInstructions outperforms standard pre-training and other proposed synthetic pre-training techniques on standard benchmarks measuring free-form response quality. Our resources can be found at https://huggingface.co/fineinstructions .

</details>


### [73] [DynaWeb: Model-Based Reinforcement Learning of Web Agents](https://arxiv.org/abs/2601.22149)
*Hang Ding,Peidong Liu,Junqiao Wang,Ziwei Ji,Meng Cao,Rongzhao Zhang,Lynn Ai,Eric Yang,Tianyu Shi,Lei Yu*

Main category: cs.CL

TL;DR: 本文提出DynaWeb框架，通过模型预测网页环境来模拟训练自主网络代理，有效提升了训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现实网络环境交互代价高且风险大，基于模型的强化学习为训练自主网络代理提供了更高效且安全的替代方案。

Method: 利用模型学习环境状态转换，以模拟网页环境，并结合专家示范轨迹与策略 rollout 交替训练自主代理策略。

Result: 该论文提出了DynaWeb，一种基于模型的强化学习(MBRL)框架，通过学习一个预测网页表示的环境模型，实现了自主网络代理的模拟交互。DynaWeb利用该环境模型生成大量策略轨迹进行在线强化学习，提升了训练效率和稳定性，同时结合了专家轨迹以进一步优化性能。实验验证表明，DynaWeb在WebArena和WebVoyager基准测试中显著优于现有开源网络代理模型。

Conclusion: DynaWeb证明了通过构建网页世界模型进行想象训练是一种可行且高效的方法，显著提升了网络代理的表现，为规模化在线强化学习提供了新途径。

Abstract: The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL.

</details>


### [74] [Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts](https://arxiv.org/abs/2601.22156)
*Yingfa Chen,Zhen Leng Thai,Zihan Zhou,Zhu Zhang,Xingyu Shen,Shuo Wang,Chaojun Xiao,Xu Han,Zhiyuan Liu*

Main category: cs.CL

TL;DR: 提出HALO蒸馏管线和HypeNet混合架构，实现低成本、高效且具备长上下文能力的Transformer-RNN混合模型。


<details>
  <summary>Details</summary>
Motivation: 现有混合Transformer和RNN架构因需大规模预训练且转换后长上下文性能差，限制了其实用性。本文旨在降低转换训练数据需求并提升混合模型的长上下文表现和推理效率。

Method: 设计HALO蒸馏流程结合参数迁移和知识蒸馏，将预训练的软性注意力块转换为混合RNN注意力块；提出HypeNet架构并引入新位置编码HyPE及一系列结构优化以增强长度泛化能力。

Result: 本文提出了一种将预训练Transformer模型蒸馏为RNN-注意力混合模型的管线HALO，并设计了带有新位置编码方案HyPE的混合架构HypeNet，实现了优越的长上下文泛化能力。通过HALO将Qwen3系列模型转换为HypeNet，在仅使用2.3B tokens（极少量训练数据）的情况下达到与原始Transformer模型相当的性能，且在长上下文性能和推理效率上表现更优。

Conclusion: 采用HALO实现低数据量蒸馏，将Transformer高效转换为长上下文性能优异的混合模型HypeNet，提升模型推理效率和泛化能力，减少预训练成本。

Abstract: Hybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RNNs), have shown a desirable performance-throughput tradeoff for long-context modeling, but their adoption and studies are hindered by the prohibitive cost of large-scale pre-training from scratch. Some recent studies have shown that pre-trained softmax attention blocks can be converted into RNN blocks through parameter transfer and knowledge distillation. However, these transfer methods require substantial amounts of training data (more than 10B tokens), and the resulting hybrid models also exhibit poor long-context performance, which is the scenario where hybrid models enjoy significant inference speedups over Transformer-based models. In this paper, we present HALO (Hybrid Attention via Layer Optimization), a pipeline for distilling Transformer models into RNN-attention hybrid models. We then present HypeNet, a hybrid architecture with superior length generalization enabled by a novel position encoding scheme (named HyPE) and various architectural modifications. We convert the Qwen3 series into HypeNet using HALO, achieving performance comparable to the original Transformer models while enjoying superior long-context performance and efficiency. The conversion requires just 2.3B tokens, less than 0.01% of their pre-training data

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [75] [A Survey on Large Language Model Impact on Software Evolvability and Maintainability: the Good, the Bad, the Ugly, and the Remedy](https://arxiv.org/abs/2601.20879)
*Bruno Claudino Matias,Savio Freire,Juliana Freitas,Felipe Fronchetti,Kostadin Damevski,Rodrigo Spinola*

Main category: cs.SE

TL;DR: LLMs在提升软件维护性和演化性方面表现出明显优势，但也存在潜在风险，需要负责的采用和严格评估。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）广泛应用于软件工程中，提升了生产力和理解力，但其对软件系统长期维护性和可演化性的影响尚不明确，且存在诸多风险和方法学限制。

Method: 通过对2020-2024年ACM DL、IEEE Xplore和Scopus共87篇文献的系统性综述，结合LLM辅助的主题分析工具及人工验证，提取质量属性、影响、风险及缓解策略。

Result: 通过系统文献综述发现LLMs增强了软件的可分析性、可测试性、代码理解、调试和自动修复能力，但也带来了幻觉输出、不稳定性能和评估缺陷等风险，影响长期演化能力。

Conclusion: LLMs有助于增强软件维护性和演化性，但为确保长期可持续性，必须采取保护措施、严格的评估以及结构化的人类监督。

Abstract: Context. Large Language Models (LLMs) are increasingly embedded in software engineering workflows for tasks including code generation, summarization, repair, and testing. Empirical studies report productivity gains, improved comprehension, and reduced cognitive load. However, evidence remains fragmented, and concerns persist about hallucinations, unstable outputs, methodological limitations, and emerging forms of technical debt. How these mixed effects shape long-term software maintainability and evolvability remains unclear. Objectives. This study systematically examines how LLMs influence the maintainability and evolvability of software systems. We identify which quality attributes are addressed in existing research, the positive impacts LLMs provide, the risks and weaknesses they introduce, and the mitigation strategies proposed in the literature. Method. We conducted a systematic literature review. Searches across ACM DL, IEEE Xplore, and Scopus (2020 to 2024) yielded 87 primary studies. Qualitative evidence was extracted through a calibrated multi-researcher process. Attributes were analyzed descriptively, while impacts, risks, weaknesses, and mitigation strategies were synthesized using a hybrid thematic approach supported by an LLM-assisted analysis tool with human-in-the-loop validation. Results. LLMs provide benefits such as improved analyzability, testability, code comprehension, debugging support, and automated repair. However, they also introduce risks, including hallucinated or incorrect outputs, brittleness to context, limited domain reasoning, unstable performance, and flaws in current evaluations, which threaten long-term evolvability. Conclusion. LLMs can strengthen maintainability and evolvability, but they also pose nontrivial risks to long-term sustainability. Responsible adoption requires safeguards, rigorous evaluation, and structured human oversight.

</details>


### [76] [DevOps-Gym: Benchmarking AI Agents in Software DevOps Cycle](https://arxiv.org/abs/2601.20882)
*Yuheng Tang,Kaijie Zhu,Bonan Ruan,Chuqi Zhang,Michael Yang,Hongwei Li,Suyue Guo,Tianneng Shi,Zekun Li,Christopher Kruegel,Giovanni Vigna,Dawn Song,William Yang Wang,Lun Wang,Yangruibo Ding,Zhenkai Liang,Wenbo Guo*

Main category: cs.SE

TL;DR: 本文提出了首个用于评估AI在完整软件DevOps周期中的性能的基准DevOps-Gym，揭示当前模型在关键任务上的不足，强调了自动化DevOps领域的研究必要性。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在代码生成和软件问题解决方面表现优异，但其在完整软件DevOps周期的能力尚不清楚，现有基准多关注孤立问题，缺乏针对DevOps的环境和工具接口，亟需构建真实且全面的评估平台。

Method: 构建了DevOps-Gym，一种涵盖核心DevOps工作流的端到端评估基准，包括构建与配置、监控、问题解决和测试生成。该基准集合了30多个Java和Go项目的700多个真实任务，并采用半自动化的数据收集机制保障任务覆盖率与质量，对当前最先进模型进行了系统评估。

Result: 通过DevOps-Gym的评估，发现现有AI模型在Java和Go的部分任务如问题解决和测试生成中表现不佳，且无法应对如监控和构建配置的新任务，暴露了自动化DevOps的挑战。

Conclusion: 现有的先进模型在处理完整的软件DevOps周期方面表现有限，特别是在问题解决和测试生成任务上存在显著困难，且无法有效处理新的DevOps任务如监控及构建配置，指出了自动化完整DevOps周期的研究需求。

Abstract: Even though demonstrating extraordinary capabilities in code generation and software issue resolving, AI agents' capabilities in the full software DevOps cycle are still unknown. Different from pure code generation, handling the DevOps cycle in real-world software, including developing, deploying, and managing, requires analyzing large-scale projects, understanding dynamic program behaviors, leveraging domain-specific tools, and making sequential decisions. However, existing benchmarks focus on isolated problems and lack environments and tool interfaces for DevOps. We introduce DevOps-Gym, the first end-to-end benchmark for evaluating AI agents across core DevOps workflows: build and configuration, monitoring, issue resolving, and test generation. DevOps-Gym includes 700+ real-world tasks collected from 30+ projects in Java and Go. We develop a semi-automated data collection mechanism with rigorous and non-trivial expert efforts in ensuring the task coverage and quality. Our evaluation of state-of-the-art models and agents reveals fundamental limitations: they struggle with issue resolving and test generation in Java and Go, and remain unable to handle new tasks such as monitoring and build and configuration. These results highlight the need for essential research in automating the full DevOps cycle with AI agents.

</details>


### [77] [IDE-Bench: Evaluating Large Language Models as IDE Agents on Real-World Software Engineering Tasks](https://arxiv.org/abs/2601.20886)
*Spencer Mateega,Jeff Yang,Tiana Costello,Shaurya Jadhav,Nicole Tian,Agustin Garcinuño*

Main category: cs.SE

TL;DR: IDE-Bench是一个全面的AI IDE代理评测框架，通过未公开的真实代码库任务，系统评估代理的工程协作能力，涵盖多技术栈和实际开发工作流。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏能够系统评估AI集成开发环境（IDE）代理在真实软件工程任务中表现的全面框架。

Method: 提出IDE-Bench框架，利用Docker化的测试环境，提供高度抽象的工具生态，支持代码库搜索、结构化文件编辑和全栈应用测试。评测涵盖80个任务，涉及C/C++、Java和MERN技术栈。

Result: 实现了首次在多语言全栈环境中，通过未公开代码库任务，系统性关联AI代理报告的意图与成功的项目级修改。

Conclusion: IDE-Bench展示了作为工程协作者的AI代理能力，提供了更真实、多样化的软件开发场景评估基准，有助于推动AI辅助软件开发的研究与应用。

Abstract: IDE-Bench is a comprehensive framework for evaluating AI IDE agents on real-world software engineering tasks through an IDE-native tool interface. We present a Dockerized test harness that goes beyond raw terminal execution, granting models a structured tool ecosystem that represents AI-native IDEs like Cursor and Windsurf. By providing high-level abstractions for codebase search, structured file editing, and tools for testing full-stack applications, IDE-Bench evaluates an agent's ability to act as a true engineering collaborator. For evaluation and to prevent training data contamination, we created 80 tasks across eight never-published repositories spanning C/C++, Java, and MERN stacks, representing modern tech stack production scenarios, including feature implementation, bug fixing, refactoring, and performance optimization that mirror daily developer workflows in private codebases. Our benchmark is the first to systematically correlate agent-reported intent with successful project-level modifications in a multi-language, full-stack environment on completely uncontaminated code.

</details>


### [78] [Another Systematic Review? A Critical Analysis of Systematic Literature Reviews on Agile Effort and Cost Estimation](https://arxiv.org/abs/2601.20893)
*Henry Edison,Nauman Ali*

Main category: cs.SE

TL;DR: 软件工程中SLR频繁重复，作者以多种理由正当化新增SLR。需加强已有综述识别和合理论证，减少重复，促进进步。


<details>
  <summary>Details</summary>
Motivation: 软件工程中SLR数量增多且存在大量重复，研究者往往未充分检索已有综述便开展新SLR，导致资源浪费和效率低下，需要探究作者开展额外SLR的正当理由并提出改进建议。

Method: 选取敏捷软件开发中的工作量估算作为案例，采用定性内容分析方法，分析18篇已发表SLR的动机说明，并结合引用数据、发表时间、发表渠道及质量进行综合解读。

Result: 本文针对软件工程领域系统文献综述（SLR）频繁重复的问题，选取敏捷软件开发中的工作量估算这一细分领域，定性分析了18篇已发表SLR的常见动机模式。结果发现，作者常以现有综述覆盖不足、方法论局限、时间过时或技术快速发展等理由来正当化开展新SLR。本文建议在SLR设计和评审指南以及会议和期刊政策中强调检索已有SLR及合理论证的重要性，以减少资源浪费并提升研究进展速度。

Conclusion: 通过对细分领域SLR的深入分析，强调明确查找和论证已有文献综述的重要性，建议制定相关指导和政策以减少重复劳动，推动软件工程研究进展。

Abstract: Background: Systematic literature reviews (SLRs) have become prevalent in software engineering research. Several researchers may conduct SLRs on similar topics without a prospective register for SLR protocols. However, even ignoring these unavoidable duplications of effort in the simultaneous conduct of SLRs, the proliferation of overlapping and often repetitive SLRs indicates that researchers are not extensively checking for existing SLRs on a topic. Given how effort-intensive it is to design, conduct, and report an SLR, the situation is less than ideal for software engineering research. Aim: To understand how authors justify additional SLRs on a topic. Method: To illustrate the issue and develop suggestions for improvement to address this issue, we have intentionally picked a sufficiently narrow but well-researched topic, i.e., effort estimation in Agile software development. We identify common justification patterns through a qualitative content analysis of 18 published SLRs. We further consider the citation data, publication years, publication venues, and the quality of the SLRs when interpreting the results. Results: The common justification patterns include authors claiming gaps in coverage, methodological limitations in prior studies, temporal obsolescence of previous SLRs, or rapid technological and methodological advancements necessitating updated syntheses. Conclusion: Our in-depth analysis of SLRs on a fairly narrow topic provides insights into SLRs in software engineering in general. By emphasizing the need for identifying existing SLRs and for justifying the undertaking of further SLRs, both in design and review guidelines and as a policy of conferences and journals, we can reduce the likelihood of duplication of effort and increase the rate of progress in the field.

</details>


### [79] [Leveraging Generative AI for Enhancing Domain-Driven Software Design](https://arxiv.org/abs/2601.20909)
*Götz-Henrik Wiegand,Filip Stepniak,Patrick Baier*

Main category: cs.SE

TL;DR: 论文提出利用生成式AI自动生成DDD领域的JSON元模型，效率高且资源消耗低。


<details>
  <summary>Details</summary>
Motivation: 传统DDD元模型的手工设计耗时且复杂，借助生成式AI实现元模型自动生成，进而简化设计流程并节约资源。

Method: 基于真实DDD项目数据训练生成模型，使用4-bit量化Code Llama和LoRA技术在消费者级GPU上微调，生成领域特定的JSON对象。

Result: 该论文基于真实DDD项目数据，利用生成式AI实现了领域特定元模型的部分自动生成，具体表现为基于简单提示生成语法正确的JSON对象，显著提升了设计过程的效率。通过在消费级GPU上采用4-bit量化的Code Llama模型及LoRA技术进行微调，在硬件资源有限的情况下仍实现了高性能表现。

Conclusion: 研究证明引入生成式AI可行且有效，可提升DDD元模型设计效率并降低资源需求，为AI驱动的软件开发开辟新路径。

Abstract: Domain-Driven Design (DDD) is a key framework for developing customer-oriented software, focusing on the precise modeling of an application's domain. Traditionally, metamodels that describe these domains are created manually by system designers, forming the basis for iterative software development. This paper explores the partial automation of metamodel generation using generative AI, particularly for producing domain-specific JSON objects. By training a model on real-world DDD project data, we demonstrate that generative AI can produce syntactically correct JSON objects based on simple prompts, offering significant potential for streamlining the design process. To address resource constraints, the AI model was fine-tuned on a consumer-grade GPU using a 4-bit quantized version of Code Llama and Low-Rank Adaptation (LoRA). Despite limited hardware, the model achieved high performance, generating accurate JSON objects with minimal post-processing. This research illustrates the viability of incorporating generative AI into the DDD process, improving efficiency and reducing resource requirements, while also laying the groundwork for further advancements in AI-driven software development.

</details>


### [80] [Infusion of Blockchain to Establish Trustworthiness in AI Supported Software Evolution: A Systematic Literature Review](https://arxiv.org/abs/2601.20918)
*Mohammad Naserameri,Juergen Rilling*

Main category: cs.SE

TL;DR: 本文综述区块链提升AI驱动软件工程信任性的研究，强调数据不可篡改和透明问责的优势，指出信任定义不统一和实测不足为主要挑战，建议未来开发统一信任框架。


<details>
  <summary>Details</summary>
Motivation: 当前软件工程中AI的广泛应用面临信任度不足的问题，区块链技术以其数据不可篡改和透明问责特性被视为提升AI软件工程信任性的潜在解决方案，亟需系统梳理相关研究现状与挑战。

Method: 采用系统文献综述方法，基于预定义协议和明确的纳入标准，筛选并综合分析了基于区块链增强AI驱动软件工程工具与流程信任性的相关研究，确保研究的透明性与可复现性。

Result: 本文通过系统文献综述，分析了区块链在提升人工智能驱动的软件工程工具和流程中的信任度方面的应用。研究发现，大部分文献关注AI在软件工程中的整合，仅31%涉及信任问题。六项最新研究探讨了基于区块链的方法，以增强AI辅助软件工程任务的可靠性、透明性和问责制。区块链通过保证数据不可篡改、模型透明和生命周期问责，提升了信任度，包括结合联邦学习的区块链共识和私有数据验证。现有挑战包括信任定义不一致和缺乏实际应用测试。未来需构建可测量、可复现的信任框架，促进安全、合规的AI驱动软件工程生态系统，尤其针对大型语言模型的应用。

Conclusion: 区块链通过保障数据不可篡改、模型透明和生命周期问责，有效增强了AI驱动的软件工程的信任度，但信任定义不统一及实际应用测试不足是主要问题，未来需设计可量化的信任框架支持安全合规生态系统的发展。

Abstract: Context: Blockchain and AI are increasingly explored to enhance trustworthiness in software engineering (SE), particularly in supporting software evolution tasks. Method: We conducted a systematic literature review (SLR) using a predefined protocol with clear eligibility criteria to ensure transparency, reproducibility, and minimized bias, synthesizing research on blockchain-enabled trust in AI-driven SE tools and processes. Results: Most studies focus on integrating AI in SE, with only 31% explicitly addressing trustworthiness. Our review highlights six recent studies exploring blockchain-based approaches to reinforce reliability, transparency, and accountability in AI-assisted SE tasks. Conclusion: Blockchain enhances trust by ensuring data immutability, model transparency, and lifecycle accountability, including federated learning with blockchain consensus and private data verification. However, inconsistent definitions of trust and limited real-world testing remain major challenges. Future work must develop measurable, reproducible trust frameworks to enable reliable, secure, and compliant AI-driven SE ecosystems, including applications involving large language models.

</details>


### [81] [Operationalizing Research Software for Supply Chain Security](https://arxiv.org/abs/2601.20980)
*Kelechi G. Kalu,Soham Rattan,Taylor R. Schorlemmer,George K. Thiruvathukal,Jeffrey C. Carver,James C. Davis*

Main category: cs.SE

TL;DR: 本文提出了一个基于研究软件供应链的分类体系，统一研究软件的定义与范围，生成标注数据集，并展示了分类体系在软件安全分析中的应用价值。


<details>
  <summary>Details</summary>
Motivation: 由于研究软件在文献中的定义和范围不一致，导致实证研究难以比较，且研究软件供应链带来的安全风险亟需明确分类体系以支撑相关安全研究。

Method: 本文对现有研究软件安全相关的文献进行了定向范围综述，提取关键定义和标准，综合形成分类体系，并基于社区语料进行了数据标注和工具开发，最后利用OpenSSF Scorecard进行了安全信号的初步分析。

Result: 该论文针对研究软件领域缺乏统一的定义和划分标准，提出了一个基于研究软件供应链（RSSC）的分类体系，用于明确定义实证研究软件安全研究的范围和边界。通过针对性地综述生成数据集与仓库挖掘的实证研究，将各种定义、纳入标准、分析单元和识别启发式方法整合为统一的分类体系，并将现有方法映射到该分类维度中。作者在Research Software Encyclopedia社区维护的语料上实现了该分类，生成了带注释的数据集、标注手册及可复现的标注流程。最后，利用OpenSSF Scorecard对不同分类群组的安全信号进行初步分析，强调了基于分类体系对安全测量进行分层解释的重要性。

Conclusion: 通过引入基于RSSC的分类体系，实证研究软件安全的范围得以明确，促进了安全信号的分层解释及研究结果的可比性。

Abstract: Empirical studies of research software are hard to compare because the literature operationalizes ``research software'' inconsistently. Motivated by the research software supply chain (RSSC) and its security risks, we introduce an RSSC-oriented taxonomy that makes scope and operational boundaries explicit for empirical research software security studies.
  We conduct a targeted scoping review of recent repository mining and dataset construction studies, extracting each work's definition, inclusion criteria, unit of analysis, and identification heuristics. We synthesize these into a harmonized taxonomy and a mapping that translates prior approaches into shared taxonomy dimensions. We operationalize the taxonomy on a large community-curated corpus from the Research Software Encyclopedia (RSE), producing an annotated dataset, a labeling codebook, and a reproducible labeling pipeline. Finally, we apply OpenSSF Scorecard as a preliminary security analysis to show how repository-centric security signals differ across taxonomy-defined clusters and why taxonomy-aware stratification is necessary for interpreting RSSC security measurements.

</details>


### [82] [Towards Comprehensive Benchmarking Infrastructure for LLMs In Software Engineering](https://arxiv.org/abs/2601.21070)
*Daniel Rodriguez-Cardenas,Xiaochang Li,Marcos Macedo,Antonio Mastropaolo,Dipin Khati,Yuan Tian,Huajie Shao,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: 当前大语言模型代码评测缺乏多维度、标准化和软件工程背景的数据集，导致评测结果片面。本文调研三大问题，提出BEHELM统一基准测试框架，实现多任务多指标全面评测。


<details>
  <summary>Details</summary>
Motivation: 现有模型评测过于单一，忽视了模型在实际软件工程应用中的关键质量维度和公平性，且缺乏规范统一的数据处理流程，影响模型评测的有效性和信度，促使作者提出更加全面和标准化的评测方案。

Method: 本文通过系统调研现有评测基准和社区工作坊收集意见，分析评测现状和问题；在此基础上设计了BEHELM，一个结合软件工程场景和多指标评测的统一评测平台。

Result: 本文对当前大语言模型代码评测存在的问题进行了深入调研，指出现有基准测试任务单一、指标单一，导致无法全面反映模型在鲁棒性、解释性、公平性、效率和实际可用性等方面的性能，且存在数据工程不一致、缺乏软件工程背景和数据污染等问题。通过调研和社区研讨发现三大评测瓶颈：缺少融合软件工程知识的数据集、过度依赖机器学习指标、缺乏标准化和可复现的数据处理流程。基于此，提出了统一软件场景定义和多指标评估的整体评测基础设施BEHELM，支持跨任务、跨语言、多粒度输入输出及多维度质量的综合评估，旨在降低构建基准测试成本，实现公平、现实且面向未来的软件工程大模型评测。

Conclusion: 通过调研和社区讨论，本文明确了代码大语言模型评测存在的核心瓶颈，提出BEHELM架构解决这些问题，推动大语言模型在软件工程领域的公平和全面评估。

Abstract: Large language models for code are advancing fast, yet our ability to evaluate them lags behind. Current benchmarks focus on narrow tasks and single metrics, which hide critical gaps in robustness, interpretability, fairness, efficiency, and real-world usability. They also suffer from inconsistent data engineering practices, limited software engineering context, and widespread contamination issues. To understand these problems and chart a path forward, we combined an in-depth survey of existing benchmarks with insights gathered from a dedicated community workshop. We identified three core barriers to reliable evaluation: the absence of software-engineering-rich datasets, overreliance on ML-centric metrics, and the lack of standardized, reproducible data pipelines. Building on these findings, we introduce BEHELM, a holistic benchmarking infrastructure that unifies software-scenario specification with multi-metric evaluation. BEHELM provides a structured way to assess models across tasks, languages, input and output granularities, and key quality dimensions. Our goal is to reduce the overhead currently required to construct benchmarks while enabling a fair, realistic, and future-proof assessment of LLMs in software engineering.

</details>


### [83] [The Quiet Contributions: Insights into AI-Generated Silent Pull Requests](https://arxiv.org/abs/2601.21102)
*S M Mahedy Hasan,Md Fazle Rabbi,Minhaz Zibran*

Main category: cs.SE

TL;DR: 首个针对无评论讨论的AI生成拉取请求的实证研究，分析其对代码质量的影响及可能的接受/拒绝原因。


<details>
  <summary>Details</summary>
Motivation: AI生成的沉默式拉取请求（SPRs）缺乏评论或讨论，使得理解其接受或拒绝的理由具有挑战性。

Method: 定量分析AIDev公开数据集中五个AI代理生成的4762个Python仓库SPRs，评估代码复杂度、质量问题及安全漏洞。

Result: 通过对4762条来自五个AI代理生成的SPRs的定量分析，研究了SPRs对代码复杂度、质量问题和安全漏洞的影响，从而探讨这些指标是否可以揭示SPRs被接受或拒绝的原因。

Conclusion: SPRs的代码复杂度、质量和安全性特征可能为理解其接受或拒绝的决策提供线索。

Abstract: We present the first empirical study of AI-generated pull requests that are 'silent,' meaning no comments or discussions accompany them. This absence of any comments or discussions associated with such silent AI pull requests (SPRs) poses a unique challenge in understanding the rationale for their acceptance or rejection. Hence, we quantitatively study 4,762 SPRs of five AI agents made to popular Python repositories drawn from the AIDev public dataset. We examine SPRs impact on code complexity, other quality issues, and security vulnerabilities, especially to determine whether these insights can hint at the rationale for acceptance or rejection of SPRs.

</details>


### [84] [AI-Assisted Engineering Should Track the Epistemic Status and Temporal Validity of Architectural Decisions](https://arxiv.org/abs/2601.21116)
*Sankalp Gilda,Shlok Gilda*

Main category: cs.SE

TL;DR: 该论文提出了一个基于模糊逻辑的框架来追踪AI辅助软件工程中的架构决策知识状态和时效，防止误信与过期证据导致的错误，验证了该方法的实用性。


<details>
  <summary>Details</summary>
Motivation: 当前AI辅助的软件工程中，LLM生成的决策速度远超团队验证速度，缺乏广泛采用的机制来区分未验证的猜测与验证过的知识，防止信任膨胀，以及检测证据过期，导致风险和错误。

Method: 提出了First Principles Framework（FPF），基于模糊逻辑中的Gödel t-范数，定义了五个聚合不变量，来追踪架构决策的知识状态和时效性。该框架分层区分假设与验证信息，通过保守聚合防止信任膨胀，并自动跟踪证据衰减。

Result: 通过对两个内部项目的回顾性审计，发现20-25%的架构决策在两个月内存在过期的证据，验证了该框架对时效性的监控必要性。论文还提出了未来研究方向，如可学习的聚合算子、联邦证据共享及基于SMT的声明验证。

Conclusion: 该研究强调了在AI辅助软件工程中建立知识和时效追踪机制的重要性，提出的FPF框架有效防止了信任过度膨胀和证据陈旧问题，为未来研究和实践提供了理论基础和方向。

Abstract: This position paper argues that AI-assisted software engineering requires explicit mechanisms for tracking the epistemic status and temporal validity of architectural decisions. LLM coding assistants generate decisions faster than teams can validate them, yet no widely-adopted framework distinguishes conjecture from verified knowledge, prevents trust inflation through conservative aggregation, or detects when evidence expires. We propose three requirements for responsible AI-assisted engineering: (1) epistemic layers that separate unverified hypotheses from empirically validated claims, (2) conservative assurance aggregation grounded in the Gödel t-norm that prevents weak evidence from inflating confidence, and (3) automated evidence decay tracking that surfaces stale assumptions before they cause failures. We formalize these requirements as the First Principles Framework (FPF), ground its aggregation semantics in fuzzy logic, and define a quintet of invariants that any valid aggregation operator must satisfy. Our retrospective audit applying FPF criteria to two internal projects found that 20-25% of architectural decisions had stale evidence within two months, validating the need for temporal accountability. We outline research directions including learnable aggregation operators, federated evidence sharing, and SMT-based claim validation.

</details>


### [85] [From Logic to Toolchains: An Empirical Study of Bugs in the TypeScript Ecosystem](https://arxiv.org/abs/2601.21186)
*TianYi Tang,Saba Alimadadi,Nick Sumner*

Main category: cs.SE

TL;DR: TypeScript项目的主要错误类型不在算法逻辑，而是工具配置和异步错误，静态类型减少传统错误但引入新脆弱性。


<details>
  <summary>Details</summary>
Motivation: 尽管TypeScript在现代网页开发中迅速流行，但其对软件错误的影响尚不清楚，需要通过大规模实证研究来理解TypeScript项目中的错误类型和特点。

Method: 对16个流行开源TypeScript项目的633个错误报告进行分析，建立错误类型分类，量化错误的分布，并与项目特征和JavaScript研究进行对比分析。

Result: 通过分析633个来自16个流行开源项目的错误报告，发现TypeScript项目中的错误主要集中在工具链和配置错误、API误用及异步错误处理问题，而非传统的逻辑或语法错误。这些错误与构建复杂度和依赖异质性密切相关。与JavaScript比较表明，静态类型减少了类型错误，但增加了构建系统和工具链方面的脆弱性。

Conclusion: 语言设计和生态系统演变改变了大型软件系统的错误分布，TypeScript通过静态类型减少了传统错误，但构建和工具链错误成为新的主要挑战。

Abstract: TypeScript has rapidly become a popular language for modern web development, yet its effect on software faults remains poorly understood. This paper presents the first large-scale empirical study of bugs in real-world TypeScript projects. We analyze 633 bug reports from 16 popular open-source repositories to construct a taxonomy of fault types, quantify their prevalence, and relate them to project characteristics such as size, domain, and dependency composition. Our results reveal a fault landscape dominated not by logic or syntax errors but by tooling and configuration faults, API misuses, and asynchronous error-handling issues. We show that these categories correlate strongly with build complexity and dependency heterogeneity, indicating that modern failures often arise at integration and orchestration boundaries rather than within algorithmic logic. A longitudinal comparison with JavaScript studies shows that while static typing in TypeScript has reduced traditional runtime and type errors, it has shifted fragility toward build systems and toolchains. These findings offer new insight into how language design and ecosystem evolution reshape the fault profiles of large-scale software systems.

</details>


### [86] [Human-Agent versus Human Pull Requests: A Testing-Focused Characterization and Comparison](https://arxiv.org/abs/2601.21194)
*Roberto Milanese,Francesco Salzano,Angelica Spina,Antonio Vitale,Remo Pareschi,Fausto Fasano,Mattia Fazzini*

Main category: cs.SE

TL;DR: 研究人机协作与纯人工PR在软件测试上的差异，发现前者测试覆盖度更高，偏向新增测试，质量相当。


<details>
  <summary>Details</summary>
Motivation: 尽管AI编码助手在软件开发中被广泛采用，但人机协作在软件测试中的作用尚未得到充分理解，因此需要探究该协作方式对测试实践的影响。

Method: 基于AIDev数据集，实证分析了6,582个人机协作PR和3,122个人工PR，从测试频率、测试类型和测试质量三个维度进行比较研究。

Result: 发现人机协作PR的测试覆盖范围显著大于纯人工PR，且更多新增测试；两者在测试质量方面无显著差异。

Conclusion: 人机协作生成的PR在测试频率相似的情况下，测试覆盖范围更广，并且偏向新增测试，而人工测试更侧重修改现有测试。测试质量无显著差异。

Abstract: AI-based coding agents are increasingly integrated into software development workflows, collaborating with developers to create pull requests (PRs). Despite their growing adoption, the role of human-agent collaboration in software testing remains poorly understood. This paper presents an empirical study of 6,582 human-agent PRs (HAPRs) and 3,122 human PRs (HPRs) from the AIDev dataset. We compare HAPRs and HPRs along three dimensions: (i) testing frequency and extent, (ii) types of testing-related changes (code-and-test co-evolution vs. test-focused), and (iii) testing quality, measured by test smells. Our findings reveal that, although the likelihood of including tests is comparable (42.9% for HAPRs vs. 40.0% for HPRs), HAPRs exhibit a larger extent of testing, nearly doubling the test-to-source line ratio found in HPRs. While test-focused task distributions are comparable, HAPRs are more likely to add new tests during co-evolution (OR=1.79), whereas HPRs prioritize modifying existing tests. Finally, although some test smell categories differ statistically, negligible effect sizes suggest no meaningful differences in quality. These insights provide the first characterization of how human-agent collaboration shapes testing practices.

</details>


### [87] [CovAgent: Overcoming the 30% Curse of Mobile Application Coverage with Agentic AI and Dynamic Instrumentation](https://arxiv.org/abs/2601.21253)
*Wei Minn,Biniam Fisseha Demissie,Yan Naing Tun,Jiakun Liu,Mariano Ceccato,Lwin Khin Shar,David Lo*

Main category: cs.SE

TL;DR: 该论文提出了基于AI的CovAgent框架，通过分析应用代码和组件关系生成激活条件脚本，显著提升了Android应用的UI测试覆盖率，优于多种现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有Android应用UI自动测试覆盖率较低，主要因复杂用户输入生成失败、激活条件不满足以及难以访问的代码路径限制。

Method: 提出了CovAgent，一个基于AI代理的无关fuzzer框架，利用反编译的Smali代码和组件转换图分析未满足的激活条件，并生成动态检测脚本以满足这些条件，从而访问传统GUI模糊测试难以达到的活动界面。

Result: CovAgent显著提升测试覆盖率，活动覆盖率分别比LLMDroid、Fastbot和APE高101.1%、116.3%和179.7%，在类、方法及代码行覆盖率等指标上也优于基线方法。此外对AI代理激活条件推断准确率和活动启动成功率进行了分析。

Conclusion: 基于Agentic AI的CovAgent有效提升了自动化Android应用UI测试的覆盖效果，证明利用代码级逻辑推断和动态脚本生成是一种有效拓展UI测试路径的方法。

Abstract: Automated GUI testing is crucial for ensuring the quality and reliability of Android apps. However, the efficacy of existing UI testing techniques is often limited, especially in terms of coverage. Recent studies, including the state-of-the-art, struggle to achieve more than 30% activity coverage in real-world apps. This limited coverage can be attributed to a combination of factors such as failing to generate complex user inputs, unsatisfied activation conditions regarding device configurations and external resources, and hard-to-reach code paths that are not easily accessible through the GUI. To overcome these limitations, we propose CovAgent, a novel agentic AI-powered approach to enhance Android app UI testing. Our fuzzer-agnostic framework comprises an AI agent that inspects the app's decompiled Smali code and component transition graph, and reasons about unsatisfied activation conditions within the app code logic that prevent access to the activities that are unreachable by standard and widely adopted GUI fuzzers. Then, another agent generates dynamic instrumentation scripts that satisfy activation conditions required for successful transitions to those activities. We found that augmenting existing fuzzing approaches with our framework achieves a significant improvement in test coverage over the state-of-the-art, LLMDroid, and other baselines such as Fastbot and APE (e.g., 101.1%, 116.3% and 179.7% higher activity coverage, respectively). CovAgent also outperforms all the baselines in other metrics such as class, method, and line coverage. We also conduct investigations into components within CovAgent to reveal further insights regarding the efficacy of Agentic AI in the field of automated app testing such as the agentic activation condition inference accuracy, and agentic activity-launching success rate.

</details>


### [88] [The Role of Social Identity in Shaping Biases Against Minorities in Software Organizations](https://arxiv.org/abs/2601.21259)
*Sayma Sultana,London Cavaletto,Bianca Trinkenreich,Amiangshu Bosu*

Main category: cs.SE

TL;DR: 研究揭示软件工程师中职业发展和任务选择偏见极为普遍，女性和少数族裔受影响最大，且多种因素影响偏见发生。


<details>
  <summary>Details</summary>
Motivation: 尽管职场偏见被广泛研究，但对软件工程师群体的特定偏见形式及影响了解不足，亟需填补这一空白。

Method: 基于社会身份理论，采用情景模拟问卷调查量化偏见类型及受害群体，分析其动机和影响。

Result: 该研究应用社会身份理论(SIT)调查软件工程师在职场中面临的四种偏见：职业发展不足、刻板任务分配、不友好环境和身份攻击。通过基于情景调查量化了这些偏见的普遍性，发现职业发展和任务选择偏见最为常见，超过三分之二的受害者多次经历。女性遭遇职业发展、任务选择偏见和不友好环境的概率是男性的三倍以上，边缘民族背景个体更多遭受身份攻击。除性别和种族外，年龄、经验年限、组织规模和地理位置也是偏见的重要预测因素。

Conclusion: 软件工程领域存在显著的系统性偏见，尤其是针对女性和少数族裔，应采取措施改善职业发展机会和工作环境。

Abstract: While systemic workplace bias is well-documented in non-computing fields, its specific impact on software engineers remains poorly understood. This study addresses that gap by applying Social Identity Theory (SIT) to investigate four distinct forms of bias: lack of career development, stereotyped task selection, unwelcoming environments, and identity attacks. Using a vignette-based survey, we quantified the prevalence of these biases, identified the demographics most affected, assessed their consequences, and explored the motivations behind biased actions. Our results show that career development and task selection biases are the most prevalent forms, with over two-thirds of victims experiencing them multiple times. Women were more than three times as likely as men to face career development bias, task selection bias, and an unwelcoming environment. In parallel, individuals from marginalized ethnic backgrounds were disproportionately targeted by identity attacks. Our analysis also confirms that, beyond gender and race, factors such as age, years of experience, organization size, and geographic location are significant predictors of bias victimization.

</details>


### [89] [More Code, Less Reuse: Investigating Code Quality and Reviewer Sentiment towards AI-generated Pull Requests](https://arxiv.org/abs/2601.21276)
*Haoming Huang,Pongchai Jaisri,Shota Shimizu,Lingfeng Chen,Sota Nakashima,Gema Rodríguez-Pérez*

Main category: cs.SE

TL;DR: 本文探讨了LLM代理生成代码的缺陷与开发者评价差异，指出AI代码虽通过测试但存在冗余和技术债务积累。


<details>
  <summary>Details</summary>
Motivation: 现有指标只关注代码通过率，无法全面反映AI生成代码对长期维护性和可读性的影响，且缺乏对开发者直观评价的考察，故研究LLM代理代码的全面特征以促进人机协作改进。

Method: 通过代码度量评估LLM与人类生成的PR质量与维护性，并结合情感分析评价开发者对AI代码的反应。

Result: 本论文研究了大型语言模型（LLM）代理在代码生成中的应用及其对代码质量和维护性的影响。通过对比LLM生成的代码与人工开发者代码，发现LLM生成代码常忽视代码重用，导致冗余增加，技术债务累积。同时情感分析显示，评审者对AI生成代码的态度较为中性或积极，掩盖了代码质量上的不足。

Conclusion: LLM代理虽然提高了代码生成效率，但其产生的代码存在冗余和维护性差的问题，且评审者对AI代码的积极态度可能掩盖了这些问题。需完善人机协作机制以提升代码质量。

Abstract: Large Language Model (LLM) Agents are advancing quickly, with the increasing leveraging of LLM Agents to assist in development tasks such as code generation. While LLM Agents accelerate code generation, studies indicate they may introduce adverse effects on development. However, existing metrics solely measure pass rates, failing to reflect impacts on long-term maintainability and readability, and failing to capture human intuitive evaluations of PR. To increase the comprehensiveness of this problem, we investigate and evaluate the characteristics of LLM to know the pull requests' characteristics beyond the pass rate. We observe the code quality and maintainability within PRs based on code metrics to evaluate objective characteristics and developers' reactions to the pull requests from both humans and LLM's generation. Evaluation results indicate that LLM Agents frequently disregard code reuse opportunities, resulting in higher levels of redundancy compared to human developers. In contrast to the quality issues, our emotions analysis reveals that reviewers tend to express more neutral or positive emotions towards AI-generated contributions than human ones. This disconnect suggests that the surface-level plausibility of AI code masks redundancy, leading to the silent accumulation of technical debt in real-world development environments. Our research provides insights for improving human-AI collaboration.

</details>


### [90] [Detecting Multiple Semantic Concerns in Tangled Code Commits](https://arxiv.org/abs/2601.21298)
*Beomsu Koh,Neil Walkinshaw,Donghwan Shin*

Main category: cs.SE

TL;DR: 采用小型语言模型结合提交信息，对纠结代码提交中的多关注点进行多标签分类，效果接近大型模型，且实践中可行且准确。


<details>
  <summary>Details</summary>
Motivation: 开发者在提交代码时常将多个关注点混合在一起，导致意图不清晰及维护困难，现有方法未解决多关注点检测的可行性问题。

Method: 将多关注点的检测任务视为多标签分类问题，构造基于真实世界数据的人工纠结提交的受控数据集，并使用小型语言模型(SLMs)进行多关注点的检测，探究微调、关注点数量、提交信息包含与头部截断策略对检测效果的影响。

Result: 微调后的14B参数SLM在单关注点检测上可与大型语言模型(LLMs)媲美，且在最多三个关注点的检测仍具可用性，且包含提交信息能显著提升准确率（Hamming Loss提升44%），且延迟开销可忽略。

Conclusion: 微调后的大型参数SLM能有效检测纠结提交中的多关注点，包含提交信息是提升检测性能的重要因素，且实用性强。

Abstract: Code commits in a version control system (e.g., Git) should be atomic, i.e., focused on a single goal, such as adding a feature or fixing a bug. In practice, however, developers often bundle multiple concerns into tangled commits, obscuring intent and complicating maintenance. Recent studies have used Conventional Commits Specification (CCS) and Language Models (LMs) to capture commit intent, demonstrating that Small Language Models (SLMs) can approach the performance of Large Language Models (LLMs) while maintaining efficiency and privacy. However, they do not address tangled commits involving multiple concerns, leaving the feasibility of using LMs for multi-concern detection unresolved. In this paper, we frame multi-concern detection in tangled commits as a multi-label classification problem and construct a controlled dataset of artificially tangled commits based on real-world data. We then present an empirical study using SLMs to detect multiple semantic concerns in tangled commits, examining the effects of fine-tuning, concern count, commit-message inclusion, and header-preserving truncation under practical token-budget limits. Our results show that a fine-tuned 14B-parameter SLM is competitive with a state-of-the-art LLM for single-concern commits and remains usable for up to three concerns. In particular, including commit messages improves detection accuracy by up to 44% (in terms of Hamming Loss) with negligible latency overhead, establishing them as important semantic cues.

</details>


### [91] [Developers in the Age of AI: Adoption, Policy, and Diffusion of AI Software Engineering Tools](https://arxiv.org/abs/2601.21305)
*Mark Looi,Julianne Quinn*

Main category: cs.SE

TL;DR: 生成式AI工具频繁使用提升软件开发生产力和代码质量，存在测试工具采用滞后问题，开发者分三类推动采纳扩散，组织政策为采纳成熟度标志。


<details>
  <summary>Details</summary>
Motivation: 生成式AI技术快速进入软件开发领域，迫切需要了解开发者对这些工具的感知影响、使用模式及其对生产力和质量的具体推动作用，以指导未来工具开发和推广策略。

Method: 采用实证调查方法，分析147名专业开发者的AI工具使用模式、感知效果以及采纳意愿，结合统计相关性分析探讨生产力和质量表现与使用行为的关系。

Result: 本文通过对147名专业开发者的实证调查，研究了生成式人工智能（AI）工具在软件开发中的使用模式及其对生产力和代码质量的感知影响。研究发现，频繁且广泛使用AI工具与感知生产力和代码质量的提升密切相关，且正向相关，没有质量悖论现象。同时，AI测试工具的采用明显滞后于编码工具，存在测试差距。研究还将开发者划分为热情者、务实者和谨慎者三类，揭示了AI工具采纳的良性循环及其在组织内的传播机制。安全问题仍是采用的中度障碍，组织政策虽不直接影响使用意愿，但反映了采纳的成熟度。

Conclusion: 生成式AI工具的广泛和频繁使用能明显提升开发者感知的生产力及代码质量，安全顾虑需解决以促进更广泛采用，组织内部的采纳传播依赖于早期采用者推动，政策起到成熟度标记作用。

Abstract: The rapid advance of Generative AI into software development prompts this empirical investigation of perceptual effects on practice. We study the usage patterns of 147 professional developers, examining perceived correlates of AI tools use, the resulting productivity and quality outcomes, and developer readiness for emerging AI-enhanced development. We describe a virtuous adoption cycle where frequent and broad AI tools use are the strongest correlates of both Perceived Productivity (PP) and quality, with frequency strongest. The study finds no perceptual support for the Quality Paradox and shows that PP is positively correlated with Perceived Code Quality (PQ) improvement. Developers thus report both productivity and quality gains. High current usage, breadth of application, frequent use of AI tools for testing, and ease of use correlate strongly with future intended adoption, though security concerns remain a moderate and statistically significant barrier to adoption. Moreover, AI testing tools' adoption lags that of coding tools, opening a Testing Gap. We identify three developer archetypes (Enthusiasts, Pragmatists, Cautious) that align with an innovation diffusion process wherein the virtuous adoption cycle serves as the individual engine of progression. Our findings reveal that organizational adoption of AI tools follows such a process: Enthusiasts push ahead with tools, creating organizational success that converts Pragmatists. The Cautious are held in organizational stasis: without early adopter examples, they don't enter the virtuous adoption cycle, never accumulate the usage frequency that drives intent, and never attain high efficacy. Policy itself does not predict individuals' intent to increase usage but functions as a marker of maturity, formalizing the successful diffusion of adoption by Enthusiasts while acting as a gateway that the Cautious group has yet to reach.

</details>


### [92] [Predicting Developer Acceptance of AI-Generated Code Suggestions](https://arxiv.org/abs/2601.21379)
*Jing Jiang,Liehao Li,Jinyun Hou,Xin Tan,Li Zhang*

Main category: cs.SE

TL;DR: 本研究基于超6万条工业开发者-AI交互数据，发现代码建议接受与拒绝的关键特征，并提出高效的个性化预测模型CSAP，显著提升代码建议接受率，有效减少开发者中断，是首个基于大规模工业数据的代码建议接受度定量分析。


<details>
  <summary>Details</summary>
Motivation: AI辅助编程工具虽然广泛使用，但因不合理建议导致开发者工作流被打断和不满，缺乏对开发者对AI代码建议接受度的定量研究，且相关细粒度交互数据多为专有，亟需通过大规模数据来深入理解和提升代码建议的实用性。

Method: 通过分析66,329条来自大型科技公司的开发者与AI交互数据，挖掘接受与拒绝代码建议的显著特征，基于此构建CSAP模型预测代码建议被接受的概率，并与大语言模型基线及工业过滤器进行效果对比验证。

Result: CSAP模型在不平衡数据集和均衡数据集上分别达到0.973和0.922的准确率，较大语言模型基线分别提升12.6%和87.0%，较工业过滤器分别提升69.5%和140.1%，显著提升代码建议的接受预测效果。

Conclusion: 本研究首次利用大规模工业开发者-AI交互数据，系统量化分析代码建议接受与拒绝的异同，揭示个性化预测模型能够有效提升代码建议的接受率，降低开发者的中断和挫败感，证明了个性化过滤的重要性。

Abstract: AI-assisted programming tools are widely adopted, yet their practical utility is often undermined by undesired suggestions that interrupt developer workflows and cause frustration. While existing research has explored developer-AI interactions when programming qualitatively, a significant gap remains in quantitative analysis of developers' acceptance of AI-generated code suggestions, partly because the necessary fine-grained interaction data is often proprietary. To bridge this gap, this paper conducts an empirical study using 66,329 industrial developer-AI interactions from a large technology company. We analyze features that are significantly different between accepted code suggestions and rejected ones. We find that accepted suggestions are characterized by significantly higher historical acceptance counts and ratios for both developers and projects, longer generation intervals, shorter preceding code context in the project, and older IDE versions. Based on these findings, we introduce CSAP (Code Suggestion Acceptance Prediction) to predict whether a developer will accept the code suggestion before it is displayed. Our evaluation of CSAP shows that it achieves the accuracy of 0.973 and 0.922 on imbalanced and balanced dataset respectively. Compared to a large language model baseline and an in-production industrial filter, CSAP relatively improves the accuracy by 12.6\% and 69.5\% on imbalanced dataset, and improves the accuracy by 87.0\% and 140.1\% on balanced dataset. Our results demonstrate that targeted personalization is a powerful approach for filtering out code suggestions with predicted rejection and reduce developer interruption. To the best of our knowledge, it is the first quantitative study of code suggestion acceptance on large-scale industrial data, and this work also sheds light on an important research direction of AI-assisted programming.

</details>


### [93] [Adaptive Confidence Gating in Multi-Agent Collaboration for Efficient and Optimized Code Generation](https://arxiv.org/abs/2601.21469)
*Haoji Zhang,Yuzhe Li,Zhenqiang Liu,Chenyang Liu,Shenyang Zhang,Yi Zhou*

Main category: cs.SE

TL;DR: DebateCoder是一个多智能体协作框架，通过角色扮演和自适应信心门控机制提升小型语言模型的推理能力，实现了更高效准确的代码生成。


<details>
  <summary>Details</summary>
Motivation: 小型语言模型在处理复杂逻辑需求时存在推理瓶颈和失败循环，亟需有效机制提升其性能和推理能力。

Method: 设计了包含用户代理、技术代理和质量保证代理的多智能体角色扮演协议，结合自适应信心门控机制、多轮审议模块以及评审引导的调试循环，以协同提升推理和生成质量。

Result: 在HumanEval和MBPP数据集上，DebateCoder表现优异，超越了MapCoder，并且显著降低了API调用，验证了多智能体协作协议对小型模型的优化效果。

Conclusion: DebateCoder显著提升了小型语言模型在代码生成任务中的性能，达到70.12%的HumanEval Pass@1，且降低了约35%的API调用开销。

Abstract: While Large Language Models (LLMs) have catalyzed breakthroughs in automated code generation, Small Language Models (SLMs) often encounter reasoning bottlenecks and failure loops when addressing complex logical requirements. To overcome these challenges, we propose DebateCoder, a multi-agent collaborative framework designed to improve the reasoning ability of SLMs (e.g., Pangu-1B) in resource-constrained environments. DebateCoder uses a structured role-playing protocol with three agents: User Agent (A_UA), Technical Agent (A_TA), and Quality Assurance Agent (A_QA). It also includes an Adaptive Confidence Gating mechanism with a 95% threshold to balance accuracy and inference efficiency. In addition, we introduce a multi-turn deliberation module and a reviewer-guided analytical debugging loop for orthogonal pre-generation debate and post-generation refinement. Experiments on HumanEval and MBPP show that DebateCoder achieves 70.12% Pass@1 on HumanEval, outperforming MapCoder while reducing API overhead by about 35%. These results indicate that collaborative protocols can mitigate limitations of small-parameter models and provide a scalable, efficient approach to high-quality automated software engineering.

</details>


### [94] [Chasing Elusive Memory Bugs in GPU Programs](https://arxiv.org/abs/2601.21552)
*Anubhab Ghosh,Ajay Nayak,Dhananjay Rao Thallikar Shyam,Arkaprava Basu*

Main category: cs.SE

TL;DR: 提出SCuBA编译期工具，利用语义关系和SAT求解器联合分析CPU/GPU代码，精准检测GPU程序中难以发现的输入依赖型及内存逻辑分区越界问题，显著优于现有工具。


<details>
  <summary>Details</summary>
Motivation: GPU程序中的内存安全漏洞（如越界访问）威胁软件安全和可靠性，现有检测工具依赖运行时检测，难以发现仅在特定输入下出现的输入依赖型越界和逻辑分区内存越界问题。

Method: 通过捕捉程序中变量间的语义关系，将约束条件表达，并使用SAT求解器进行越界验证，同时分析GPU代码中内存的逻辑分区，进行全面输入依赖和内存细分越界检测。

Result: 提出SCuBA，一种在编译期联合分析CPU和GPU代码的技术，通过捕捉语义关系及利用SAT求解器检测越界访问，成功发现所有输入依赖型和逻辑分区内存越界，且无误报。

Conclusion: SCuBA有效解决了GPU程序中输入依赖型及逻辑分区内存越界检测问题，超越现有运行时检测技术，实现无漏报和无误报。

Abstract: Memory safety bugs, such as out-of-bound accesses (OOB) in GPU programs, can compromise the security and reliability of GPU-accelerated software. We report the existence of input-dependent OOBs in the wild that manifest only under specific inputs. All existing tools to detect OOBs in GPU programs rely on runtime techniques that require an OOB to manifest for detection. Thus, input-dependent OOBs elude them. We also discover intra-allocation OOBs that arise in the presence of logical partitioning of a memory allocation into multiple data structures. Existing techniques are oblivious to the possibility of such OOBs.
  We make a key observation that the presence (or absence) of semantic relations among program variables, which determines the size of allocations (CPU code) and those calculating offsets into memory allocations (GPU code), helps identify the absence (or presence) of OOBs. We build SCuBA, a first-of-its-kind compile-time technique that analyzes CPU and GPU code to capture such semantic relations (if present). It uses a SAT solver to check if an OOB access is possible under any input, given the captured relations expressed as constraints. It further analyzes GPU code to track logical partitioning of memory allocations for detecting intra-allocation OOB. Compared to NVIDIA's Compute Sanitizer that misses 45 elusive memory bugs across 20 programs, SCuBA misses none with no false alarms.

</details>


### [95] [Multi-objective Integer Linear Programming approach for Automatic Software Cognitive Complexity Reduction](https://arxiv.org/abs/2601.21565)
*Adriana Novoa-Hurtado,Rubén Saborido,Francisco Chicano,Manuel Giménez-Medina*

Main category: cs.SE

TL;DR: 本文通过多目标整数线性规划模型和算法工具，优化代码提取以降低软件认知复杂度，提升代码可维护性。


<details>
  <summary>Details</summary>
Motivation: 为了保证软件可维护性，需简化代码以减少漏洞和缺陷；通过代码提取减少认知复杂性，有助于提升代码理解和维护效率。

Method: 基于SonarSource认知复杂度度量，构建多目标整数线性规划模型，开发多种验证算法并集成到专用工具中，实现问题参数化解决。

Result: 本文提出了一个基于多目标整数线性规划的模型，用于通过代码提取方法减少软件代码的认知复杂性。采用SonarSource定义的认知复杂度度量，将代码提取问题建模为多目标优化问题，综合考虑代码行数和平衡认知复杂性。为了验证模型有效性，还开发了多种算法，并将其集成到一个工具中，以支持参数化解决方案生成。

Conclusion: 多目标整数线性规划模型有效地提供了多方案以降低代码认知复杂度，帮助开发者提升代码简洁性和可维护性。

Abstract: Clear and concise code is necessary to ensure maintainability, so it is crucial that the software is as simple as possible to understand, to avoid bugs and, above all, vulnerabilities. There are many ways to enhance software without changing its functionality, considering the extract method refactoring the primary process to reduce the effort required for code comprehension. The cognitive complexity measure employed in this work is the one defined by SonarSource, which is a company that develops well-known applications for static code analysis. This extraction problem can be modeled as a combinatorial optimization problem. The main difficulty arises from the existence of different criteria for evaluating the solutions obtained, requiring the formulation of the code extraction problem as a multi-objective optimization problem using alternative methods. We propose a multi-objective integer linear programming model to obtain a set of solutions that reduce the cognitive complexity of a given piece of code, such as balancing the number of lines of code and its cognitive complexity. In addition, several algorithms have been developed to validate the model. These algorithms have been integrated into a tool that enables the parameterised resolution of the problem of reducing software cognitive complexity.

</details>


### [96] [Is My RPC Response Reliable? Detecting RPC Bugs in Ethereum Blockchain Client under Context](https://arxiv.org/abs/2601.21593)
*Zhijie Zhong,Yuhong Nan,Mingxi Ye,Qing Xue,Jiashui Wang,Xinlei Ying,Long Liu,Zibin Zheng*

Main category: cs.SE

TL;DR: EthCRAFT工具通过上下文感知的RPC调用生成，有效发现以太坊客户端中的多个RPC漏洞，提升了漏洞检测的广度和深度。


<details>
  <summary>Details</summary>
Motivation: 区块链客户端提供RPC接口，但存在大量上下文相关的RPC漏洞，现有研究多集中于方法调用生成，缺少对触发这些漏洞的上下文生成方法。

Method: 提出EthCRAFT工具，通过探索区块链状态转移程序空间生成交易构建上下文，再进行上下文感知的RPC调用生成，通过不同客户端响应交叉验证以检测RPC漏洞。

Result: EthCRAFT在以太坊客户端的真实RPC漏洞检测中优于现有工具，发现多处新漏洞并成功上报，其中多个漏洞已被修复并获得官方奖励。

Conclusion: 通过生成合适的上下文环境，EthCRAFT提高了区块链RPC漏洞的检测效果，促进了客户端的安全性提升。

Abstract: Blockchain clients are fundamental software for running blockchain nodes. They provide users with various RPC (Remote Procedure Call) interfaces to interact with the blockchain. These RPC methods are expected to follow the same specification across different blockchain nodes, providing users with seamless interaction. However, there have been continuous reports on various RPC bugs that can cause unexpected responses or even Denial of Service weakness. Existing studies on blockchain RPC bug detection mainly focus on generating the RPC method calls for testing blockchain clients. However, a wide range of the reported RPC bugs are triggered in various blockchain contexts. To the best of our knowledge, little attention is paid to generating proper contexts that can trigger these context-dependent RPC bugs.
  In this work, we propose EthCRAFT, a Context-aware RPC Analysis and Fuzzing Tool for client RPC bug detection. EthCRAFT first proposes to explore the state transition program space of blockchain clients and generate various transactions to construct the context. EthCRAFT then designs a context-aware RPC method call generation method to send RPC calls to the blockchain clients. The responses of 5 different client implementations are used as cross-referring oracles to detect the RPC bugs. We evaluate EthCRAFT on real-world RPC bugs collected from the GitHub issues of Ethereum client implementations. Experiment results show that EthCRAFT outperforms existing client RPC detectors by detecting more RPC bugs. Moreover, EthCRAFT has found six new bugs in major Ethereum clients and reported them to the developers. One of the bug fixes has been written into breaking changes in the client's updates. Three of our bug reports have been offered a vulnerability bounty by the Ethereum Foundation.

</details>


### [97] [Age Matters: Analyzing Age-Related Discussions in App Reviews](https://arxiv.org/abs/2601.21605)
*Shashiwadana Nirmania,Garima Sharma,Hourieh Khalajzadeh,Mojtaba Shahin*

Main category: cs.SE

TL;DR: 本文通过分析谷歌应用商店中数千条应用评论，探讨了不同年龄用户在使用移动应用时的具体需求与挑战，利用多种机器学习模型自动识别涉及年龄讨论的评论，并总结出六大用户关注主题。


<details>
  <summary>Details</summary>
Motivation: 鉴于移动应用在各年龄段用户中存在使用障碍，尤其是内容不适合年轻用户和老年用户的可用性问题，本文旨在通过分析用户评价更深入理解各年龄组需求，促进应用开发的年龄包容性。

Method: 本文构建包含4163条评论的数据集，手工标注年龄相关评价，采用八种机器学习和深度学习模型自动检测年龄讨论，其中RoBERTa模型表现最佳，结合定性分析筛选出主要主题。

Result: 通过模型自动识别出1429条年龄相关评论，RoBERTa模型精确度达92.46%；定性分析揭示六个主要用户关注点，显示年龄因素对用户体验具有重要影响。

Conclusion: 研究发现不同年龄用户在移动应用使用过程中存在显著差异，开发者需要针对年龄相关问题进行优化设计，提升应用的包容性和用户体验。

Abstract: In recent years, mobile applications have become indispensable tools for managing various aspects of life. From enhancing productivity to providing personalized entertainment, mobile apps have revolutionized people's daily routines. Despite this rapid growth and popularity, gaps remain in how these apps address the needs of users from different age groups. Users of varying ages face distinct challenges when interacting with mobile apps, from younger users dealing with inappropriate content to older users having difficulty with usability due to age-related vision and cognition impairments. Although there have been initiatives to create age-inclusive apps, a limited understanding of user perspectives on age-related issues may hinder developers from recognizing specific challenges and implementing effective solutions. In this study, we explore age discussions in app reviews to gain insights into how mobile apps should cater to users across different age groups.We manually curated a dataset of 4,163 app reviews from the Google Play Store and identified 1,429 age-related reviews and 2,734 non-age-related reviews. We employed eight machine learning, deep learning, and large language models to automatically detect age discussions, with RoBERTa performing the best, achieving a precision of 92.46%. Additionally, a qualitative analysis of the 1,429 age-related reviews uncovers six dominant themes reflecting user concerns.

</details>


### [98] [AtPatch: Debugging Transformers via Hot-Fixing Over-Attention](https://arxiv.org/abs/2601.21695)
*Shihao Weng,Yang Feng,Jincheng Li,Yining Yin,Xiaofei Xie,Jia Liu*

Main category: cs.SE

TL;DR: 提出AtPatch方法动态调整注意力图，防御Transformer模型中的后门攻击和不公平，效果优于现有方法且无需重训练。


<details>
  <summary>Details</summary>
Motivation: Transformer-based DNNs受后门攻击和不公平性影响时会出现异常的注意力模式，现有神经元编辑方法难以灵活处理，且容易扭曲特征表示。

Method: 提出AtPatch，一种热修补方法，在模型推理过程中动态重新分配注意力图。利用预训练检测器识别异常列，替换为良性注意力并调整其他列，保持模型功能且无需修改模型参数或重训练。

Result: 实验验证表明，AtPatch在缓解后门攻击和不公平性方面优于现有方法，同时更好地保持模型原有功能。

Conclusion: AtPatch通过选择性动态调整注意力图，提供了一种灵活且有效的防御后门攻击和不公平性的新方案，适合部署模型使用。

Abstract: Transformer-based deep neural networks (DNNs) affected by backdoor attacks and unfairness typically exhibit anomalous attention patterns, leading to over-attend to backdoor triggers or protected attributes. Existing neuron-editing mitigation strategies often struggle to handle such situation and most of them lack flexibility and tend to distort feature representations. Motivated by such over-attention phenomenon and software engineering paradigms such as delta debugging and hot patching, we propose AtPatch, a hot-fix method that dynamically redistributes attention maps during model inference. Specifically, for a given input, AtPatch first extracts the attention map from the model's inference process. Then, it uses a pre-trained detector to identify anomalous columns and replace them with unified benign attention. Then, AtPatch rescales other columns to mitigate the impact of over-attention. Finally, AtPatch returns the redistributed attention map to the model for continued inference. Notably, if the detector does not report any anomalous columns, AtPatch directly returns the original attention map to the model. Unlike existing techniques, AtPatch selectively redistributes the attention map, making it better at preserving the model's original functionality. Furthermore, AtPatch's on-the-fly nature allows it to work without modifying model parameters or retraining, making it better suited for deployed models. We conducted extensive experiments to validate AtPatch. Experimental results show that, compared to existing methods, AtPatch can more effectively mitigate backdoor attacks and unfairness while better preserving the model's original functionality.

</details>


### [99] [Migrating Esope to Fortran 2008 using model transformations](https://arxiv.org/abs/2601.21755)
*Younoussa Sow,Nicolas Anquetil,Léandre Brault,Stéphane Ducasse*

Main category: cs.SE

TL;DR: 该论文提出一种模型驱动的自动迁移工具，实现了带有Esope扩展的FORTRAN 77代码到Fortran 2008的转换，兼顾代码可读性和抽象层次，提升维护与现代化效率。


<details>
  <summary>Details</summary>
Motivation: 维护和现代化遗留编程语言FORTRAN 77尤为困难，尤其是存在专有扩展时，迁移到较新的Fortran 2008标准更具挑战。

Method: 采用模型驱动工程技术，通过转换生成目标模型，再导出易读的Fortran 2008源码。

Result: 开发了一个工具，实现了从带有专有扩展Esope的FORTRAN 77代码自动迁移到Fortran 2008，保持了Esope的抽象层次和代码可读性。

Conclusion: 该方法在保持代码抽象和可维护性方面表现良好，但存在一些限制，具备良好的可扩展性与适应不断变化需求的能力。

Abstract: Legacy programming languages such as FORTRAN 77 still play a vital role in many industrial applications. Maintaining and modernizing these languages is challenging, especially when migrating to newer standards such as Fortran 2008. This is exacerbated in the presence of legacy proprietary extensions on such legacy languages, because their semantics are often based on old context (limits of legacy language, domain logic,...). This paper presents an approach for automatically migrating FORTRAN 77 with a proprietary extension, named Esope, to Fortran 2008. We introduce a tool that converts Esope source code to Fortran 2008. While supporting readability of the generated code, we want to maintain the level of abstraction provided by Esope. Our method uses model-driven engineering techniques, with transformations to generate a target model from which we export easy-to-read Fortran 2008 source code. We discuss the advantages, limitations, and maintainability considerations of our approach and provide insights into its scalability and adaptability to evolving requirements.

</details>


### [100] [Towards A Sustainable Future for Peer Review in Software Engineering](https://arxiv.org/abs/2601.21761)
*Esteban Parra,Sonia Haiduc,Preetha Chatterjee,Ramtin Ehsani,Polina Iaremchuk*

Main category: cs.SE

TL;DR: 软件工程领域的同行评审面临评审人数不足的挑战，需通过培训新人、激励社区成员和引入AI工具实现更高效包容的评审流程。


<details>
  <summary>Details</summary>
Motivation: 软件工程领域论文数量快速增长，合格评审人数不足，影响研究社区的可持续发展。

Method: 提出引入培训机制吸引新人、激励更多社区成员参与以及谨慎整合AI工具辅助评审。

Result: 构建一个结合人力培训和AI辅助的更高效同行评审流程，有望缓解当前评审资源不足的问题。

Conclusion: 为保障软件工程研究的长期发展，必须构建一个更具扩展性、包容性和韧性的同行评审体系。

Abstract: Peer review is the main mechanism by which the software engineering community assesses the quality of scientific results. However, the rapid growth of paper submissions in software engineering venues has outpaced the availability of qualified reviewers, creating a growing imbalance that risks constraining and negatively impacting the long-term growth of the Software Engineering (SE) research community. Our vision of the Future of the SE research landscape involves a more scalable, inclusive, and resilient peer review process that incorporates additional mechanisms for: 1) attracting and training newcomers to serve as high-quality reviewers, 2) incentivizing more community members to serve as peer reviewers, and 3) cautiously integrating AI tools to support a high-quality review process.

</details>


### [101] [Assessing the Business Process Modeling Competences of Large Language Models](https://arxiv.org/abs/2601.21787)
*Chantale Lauer,Peter Pfeiffer,Alexander Rombach,Nijat Mehdiyev*

Main category: cs.SE

TL;DR: 本文提出了一个新的框架BEF4LLM，系统评价LLMs生成的BPMN模型质量，发现LLMs在语法和实用表现优异，人类在语义质量方面更好，强调了两者的优势和改进方向。


<details>
  <summary>Details</summary>
Motivation: BPMN模型创建复杂且耗时，虽然LLMs在从自然语言生成BPMN模型方面取得进展，但缺乏系统性的质量评估方法和维度分析。

Method: 提出了BEF4LLM评价框架，从语法质量、实用质量、语义质量和有效性四个维度系统评估LLM生成的BPMN模型。通过该框架对开源LLMs进行全面分析，并与人类专家建模结果进行对比。

Result: 通过BEF4LLM评估，发现LLMs在语法和实用方面表现优越，人类在语义方面更具优势，分数差异小，表明LLMs具有较强潜力，但在有效性和语义质量方面存在挑战。

Conclusion: LLMs在BPMN建模中在语法和实用质量方面表现优异，但在语义质量和有效性方面仍落后于人类专家，显示出其在业务流程建模中的竞争潜力和现有不足。

Abstract: The creation of Business Process Model and Notation (BPMN) models is a complex and time-consuming task requiring both domain knowledge and proficiency in modeling conventions. Recent advances in large language models (LLMs) have significantly expanded the possibilities for generating BPMN models directly from natural language, building upon earlier text-to-process methods with enhanced capabilities in handling complex descriptions. However, there is a lack of systematic evaluations of LLM-generated process models. Current efforts either use LLM-as-a-judge approaches or do not consider established dimensions of model quality. To this end, we introduce BEF4LLM, a novel LLM evaluation framework comprising four perspectives: syntactic quality, pragmatic quality, semantic quality, and validity. Using BEF4LLM, we conduct a comprehensive analysis of open-source LLMs and benchmark their performance against human modeling experts. Results indicate that LLMs excel in syntactic and pragmatic quality, while humans outperform in semantic aspects; however, the differences in scores are relatively modest, highlighting LLMs' competitive potential despite challenges in validity and semantic quality. The insights highlight current strengths and limitations of using LLMs for BPMN modeling and guide future model development and fine-tuning. Addressing these areas is essential for advancing the practical deployment of LLMs in business process modeling.

</details>


### [102] [Folklore in Software Engineering: A Definition and Conceptual Foundations](https://arxiv.org/abs/2601.21814)
*Eduard Enoiu,Jean Malm,Gregory Gay*

Main category: cs.SE

TL;DR: 本文定义和分析了软件工程中的民间传说，揭示其在职业群体中的传递方式及影响，旨在促进反思性实践和后续研究。


<details>
  <summary>Details</summary>
Motivation: 软件工程领域中存在许多非正式流传的叙事、神话和隐性知识，缺乏清晰定义和系统研究。

Method: 采用文献回顾、主题分析及半结构化访谈相结合的方法，收集和分析软件工程领域的民间传说实例和其影响。

Result: 通过文献回顾、主题分析和对12名工业实践者的访谈，提出了软件工程民间传说的工作定义，明确其在塑造身份、价值观和集体知识中的作用。

Conclusion: 明确软件工程民间传说的概念有助于开展民族志和民俗学研究，促进保存有效启发式方法并挑战有害传说。

Abstract: We explore the concept of folklore within software engineering, drawing from folklore studies to define and characterize narratives, myths, rituals, humor, and informal knowledge that circulate within software development communities. Using a literature review and thematic analysis, we curated exemplar folklore items (e.g., beliefs about where defects occur, the 10x developer legend, and technical debt). We analyzed their narrative form, symbolic meaning, occupational relevance, and links to knowledge areas in software engineering. To ground these concepts in practice, we conducted semi-structured interviews with 12 industrial practitioners in Sweden to explore how such narratives are recognized or transmitted within their daily work and how they affect it. Synthesizing these results, we propose a working definition of software engineering folklore as informally transmitted, traditional, and emergent narratives and heuristics enacted within occupational folk groups that shape identity, values, and collective knowledge. We argue that making the concept of software engineering folklore explicit provides a foundation for subsequent ethnography and folklore studies and for reflective practice that can preserve context-effective heuristics while challenging unhelpful folklore.

</details>


### [103] [SWE-Replay: Efficient Test-Time Scaling for Software Engineering Agents](https://arxiv.org/abs/2601.22129)
*Yifeng Ding,Lingming Zhang*

Main category: cs.SE

TL;DR: 提出了SWE-Replay，一种高效且通用的测试时扩展方法，通过重复利用先前试验的轨迹，减少计算成本并提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型代理在软件工程任务中采用的测试时扩展方法计算成本高且存在模型误校准问题，且难以适应生成定制bash脚本的现代代理，迫切需要一种高效且通用的扩展技术。

Method: SWE-Replay通过回收之前试验的轨迹，可动态选择从头探索或利用存档经验，在关键中间步骤进行分支，选择依据为仓库探索的潜力和推理意义，避免依赖外部模型估计。

Result: 在SWE-Bench Verified测试上，SWE-Replay相比传统方法显著降低了计算成本最多17.4%，同时性能提升最多3.8%，在SWE-Bench Pro及多语言环境测试中也展现出良好表现。

Conclusion: SWE-Replay在SWE-Bench系列测试中表现优异，降低了最多17.4%的计算成本，同时性能提升最多3.8%，且适用于多语言环境，具备良好的通用性和稳健性。

Abstract: Test-time scaling has been widely adopted to enhance the capabilities of Large Language Model (LLM) agents in software engineering (SWE) tasks. However, the standard approach of repeatedly sampling trajectories from scratch is computationally expensive. While recent methods have attempted to mitigate costs using specialized value agents, they can suffer from model miscalibration and fail to generalize to modern agents that synthesize custom bash scripts as tools. In this paper, we introduce SWE-Replay, the first efficient and generalizable test-time scaling technique for modern agents without reliance on potentially noisy value estimates. SWE-Replay optimizes the scaling process by recycling trajectories from prior trials, dynamically choosing to either explore from scratch or exploit archived experience by branching at critical intermediate steps. This selection of intermediate steps is driven by the potential and reasoning significance of repository exploration, rather than external LLM-based quality estimates. Our evaluation shows that, on SWE-Bench Verified, SWE-Replay consistently outperforms naive scaling, reducing costs by up to 17.4% while maintaining or even improving performance by up to 3.8%. Further evaluation on SWE-Bench Pro and Multilingual validates the generalizability of SWE-Replay, establishing it as a robust foundation for efficient test-time scaling of software engineering agents.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [104] [AI-Augmented Density-Driven Optimal Control (D2OC) for Decentralized Environmental Mapping](https://arxiv.org/abs/2601.21126)
*Kooktae Lee,Julian Martinez*

Main category: cs.MA

TL;DR: 本文提出了一种基于AI增强的多机器人环境映射去中心化框架，通过自适应校正和双层神经网络提高估计精度，实现了更鲁棒和高保真的空间分布重构。


<details>
  <summary>Details</summary>
Motivation: 传统多机器人环境覆盖方法在依赖准确参考地图时表现良好，但在面对不确定或偏置的先验信息时性能下降，故需构建一种能够自适应调整并提升鲁棒性的去中心化环境映射框架。

Method: 提出基于最优传输的自适应自我校正机制结合双层多层感知机（MLP）模块，迭代优化局部密度估计，并利用虚拟不确定性调节避免陷入局部最优。

Result: 该论文提出了一种AI增强的去中心化多智能体环境映射框架，在传感和通信受限的条件下，通过引入基于最优传输的自适应自我校正机制，改进了多机器人对局部密度估计的迭代优化，解决了传统覆盖方法在不确定或偏置先验下性能下降的问题。采用双层多层感知机模块提升了系统的适应性，有效缓解了算法陷入局部最优的情况。理论证明了算法在Wasserstein度量下的收敛性，仿真结果显示该方法相比传统分散式基线方法，能够更准确、鲁棒地重构复杂多模态空间分布，实现环境映射的高保真度。

Conclusion: 本文方法理论上保证收敛性，并在仿真实验中显著提升了多机器人环境映射的准确性和鲁棒性，优于传统去中心化方法。

Abstract: This paper presents an AI-augmented decentralized framework for multi-agent (multi-robot) environmental mapping under limited sensing and communication. While conventional coverage formulations achieve effective spatial allocation when an accurate reference map is available, their performance deteriorates under uncertain or biased priors. The proposed method introduces an adaptive and self-correcting mechanism that enables agents to iteratively refine local density estimates within an optimal transport-based framework, ensuring theoretical consistency and scalability. A dual multilayer perceptron (MLP) module enhances adaptivity by inferring local mean-variance statistics and regulating virtual uncertainty for long-unvisited regions, mitigating stagnation around local minima. Theoretical analysis rigorously proves convergence under the Wasserstein metric, while simulation results demonstrate that the proposed AI-augmented Density-Driven Optimal Control consistently achieves robust and precise alignment with the ground-truth density, yielding substantially higher-fidelity reconstruction of complex multi-modal spatial distributions compared with conventional decentralized baselines.

</details>


### [105] [Mean-Field Control on Sparse Graphs: From Local Limits to GNNs via Neighborhood Distributions](https://arxiv.org/abs/2601.21477)
*Tobias Schmidt,Kai Cui*

Main category: cs.MA

TL;DR: 本文提出了一种针对大稀疏图结构的平均场控制（MFC）框架，解决了传统MFC在多智能体系统中普适性差的问题。核心在于利用装饰的有根邻域概率测度刻画局部异质性，证明了有限时间阶段中代理策略的局部依赖性，推导出新的动态规划原理，并形式化支持图神经网络在该框架中的应用。


<details>
  <summary>Details</summary>
Motivation: 传统平均场控制依赖于密集、全连接的交互假设，限制其在现实中多样化稀疏网络中的应用，因此需要建立一个更加符合实际网络结构的理论框架以扩展MFC的适用性。

Method: 重新定义系统状态为装饰有根邻域的概率测度，证明有限时域问题中策略依赖于局部邻域（T-t跳邻域）,并基于此提出了提升空间上的动态规划原理，结合图神经网络设计了基于actor-critic的强化学习算法。

Result: 理论证明了有限时域MFC的局部性和动态规划定理，创新性地将状态空间提升到邻域分布上，实验验证了图神经网络在该模型中强化学习的有效性，同时保留传统MFC作为特例。

Conclusion: 本文建立了适用于大规模稀疏网络的理论完备的平均场控制框架，实现了有限时域问题中基于局部邻域的可扩展最优策略求解，理论与实验均支持图神经网络在强化学习中的有效性，拓展了传统MFC的适用范围。

Abstract: Mean-field control (MFC) offers a scalable solution to the curse of dimensionality in multi-agent systems but traditionally hinges on the restrictive assumption of exchangeability via dense, all-to-all interactions. In this work, we bridge the gap to real-world network structures by proposing a rigorous framework for MFC on large sparse graphs. We redefine the system state as a probability measure over decorated rooted neighborhoods, effectively capturing local heterogeneity. Our central contribution is a theoretical foundation for scalable reinforcement learning in this setting. We prove horizon-dependent locality: for finite-horizon problems, an agent's optimal policy at time t depends strictly on its (T-t)-hop neighborhood. This result renders the infinite-dimensional control problem tractable and underpins a novel Dynamic Programming Principle (DPP) on the lifted space of neighborhood distributions. Furthermore, we formally and experimentally justify the use of Graph Neural Networks (GNNs) for actor-critic algorithms in this context. Our framework naturally recovers classical MFC as a degenerate case while enabling efficient, theoretically grounded control on complex sparse topologies.

</details>


### [106] [Learning to Communicate Across Modalities: Perceptual Heterogeneity in Multi-Agent Systems](https://arxiv.org/abs/2601.22041)
*Naomi Pitzer,Daniela Mihai*

Main category: cs.MA

TL;DR: 本文研究了异质多模态代理在无感知对齐情况下的紧急通信，揭示了信息编码机制及跨系统适应性，拓展了紧急通信理论与应用。


<details>
  <summary>Details</summary>
Motivation: 现有的紧急通信研究大多假设同质模态或对齐的表示空间，忽视了真实世界中感知异质性的影响。

Method: 设计异质多步二元通信游戏，比较单模态与多模态系统的通信效率和信息编码，进行扰动及互操作性实验。

Result: 研究发现多模态系统尽管感知不对齐，仍能发展出基于感知输入的类别一致消息；单模态系统通信更高效，信息交换更少且分类熵更低；跨系统直接通信失败，但有限微调后可实现互操作。

Conclusion: 紧急通信可作为研究代理间异质模态适应与表示迁移的框架，为理论和实验提供新方向。

Abstract: Emergent communication offers insight into how agents develop shared structured representations, yet most research assumes homogeneous modalities or aligned representational spaces, overlooking the perceptual heterogeneity of real-world settings. We study a heterogeneous multi-step binary communication game where agents differ in modality and lack perceptual grounding. Despite perceptual misalignment, multimodal systems converge to class-consistent messages grounded in perceptual input. Unimodal systems communicate more efficiently, using fewer bits and achieving lower classification entropy, while multimodal agents require greater information exchange and exhibit higher uncertainty. Bit perturbation experiments provide strong evidence that meaning is encoded in a distributional rather than compositional manner, as each bit's contribution depends on its surrounding pattern. Finally, interoperability analyses show that systems trained in different perceptual worlds fail to directly communicate, but limited fine-tuning enables successful cross-system communication. This work positions emergent communication as a framework for studying how agents adapt and transfer representations across heterogeneous modalities, opening new directions for both theory and experimentation.

</details>
